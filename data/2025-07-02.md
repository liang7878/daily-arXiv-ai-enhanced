<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 19]
- [cs.CE](#cs.CE) [Total: 1]
- [cs.DB](#cs.DB) [Total: 7]
- [cs.DC](#cs.DC) [Total: 11]
- [cs.DS](#cs.DS) [Total: 4]
- [cs.GT](#cs.GT) [Total: 1]
- [cs.IR](#cs.IR) [Total: 8]
- [cs.LG](#cs.LG) [Total: 99]
- [cs.NE](#cs.NE) [Total: 4]
- [cs.SE](#cs.SE) [Total: 12]
- [q-fin.ST](#q-fin.ST) [Total: 1]
- [stat.ML](#stat.ML) [Total: 5]
- [stat.CO](#stat.CO) [Total: 4]
- [cond-mat.dis-nn](#cond-mat.dis-nn) [Total: 1]
- [math.DS](#math.DS) [Total: 1]
- [cs.GR](#cs.GR) [Total: 1]
- [cs.PL](#cs.PL) [Total: 2]
- [eess.AS](#eess.AS) [Total: 2]
- [cs.SD](#cs.SD) [Total: 3]
- [cs.CV](#cs.CV) [Total: 31]
- [cs.DM](#cs.DM) [Total: 2]
- [cs.SI](#cs.SI) [Total: 1]
- [quant-ph](#quant-ph) [Total: 1]
- [eess.IV](#eess.IV) [Total: 7]
- [physics.app-ph](#physics.app-ph) [Total: 1]
- [physics.soc-ph](#physics.soc-ph) [Total: 2]
- [cs.MA](#cs.MA) [Total: 3]
- [math.DG](#math.DG) [Total: 1]
- [cs.CL](#cs.CL) [Total: 16]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [astro-ph.CO](#astro-ph.CO) [Total: 1]
- [cs.NI](#cs.NI) [Total: 1]
- [cs.CY](#cs.CY) [Total: 3]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 2]
- [cs.CR](#cs.CR) [Total: 7]
- [hep-ph](#hep-ph) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.DL](#cs.DL) [Total: 1]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [econ.GN](#econ.GN) [Total: 3]
- [cs.CC](#cs.CC) [Total: 1]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [physics.optics](#physics.optics) [Total: 1]
- [cs.HC](#cs.HC) [Total: 5]
- [physics.geo-ph](#physics.geo-ph) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [astro-ph.SR](#astro-ph.SR) [Total: 1]
- [cs.RO](#cs.RO) [Total: 9]
- [cs.MM](#cs.MM) [Total: 1]
- [q-bio.PE](#q-bio.PE) [Total: 1]
- [nlin.AO](#nlin.AO) [Total: 1]
- [eess.SY](#eess.SY) [Total: 2]
- [econ.TH](#econ.TH) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [DiMo-GUI: Advancing Test-time Scaling in GUI Grounding via Modality-Aware Visual Reasoning](https://arxiv.org/abs/2507.00008)
*Hang Wu,Hongkai Chen,Yujun Cai,Chang Liu,Qingwen Ye,Ming-Hsuan Yang,Yiwei Wang*

Main category: cs.AI

TL;DR: 本文提出无训练框架DiMo - GUI用于GUI接地，采用动态视觉接地和模态感知优化策略，在基准测试中优于基线推理管道。


<details>
  <summary>Details</summary>
Motivation: 由于GUI中视觉元素多样、空间杂乱和语言歧义，自然语言查询在GUI中接地存在独特挑战。

Method: 将输入分为文本和图标元素，用通用视觉 - 语言模型独立推理；预测模糊或错误时，动态聚焦生成候选焦点区域并细化接地结果。

Result: 在标准GUI接地基准测试中，相比基线推理管道有持续改进。

Conclusion: 结合模态分离和区域聚焦推理是有效的。

Abstract: Grounding natural language queries in graphical user interfaces (GUIs) poses
unique challenges due to the diversity of visual elements, spatial clutter, and
the ambiguity of language. In this paper, we introduce DiMo-GUI, a
training-free framework for GUI grounding that leverages two core strategies:
dynamic visual grounding and modality-aware optimization. Instead of treating
the GUI as a monolithic image, our method splits the input into textual
elements and iconic elements, allowing the model to reason over each modality
independently using general-purpose vision-language models. When predictions
are ambiguous or incorrect, DiMo-GUI dynamically focuses attention by
generating candidate focal regions centered on the model's initial predictions
and incrementally zooms into subregions to refine the grounding result. This
hierarchical refinement process helps disambiguate visually crowded layouts
without the need for additional training or annotations. We evaluate our
approach on standard GUI grounding benchmarks and demonstrate consistent
improvements over baseline inference pipelines, highlighting the effectiveness
of combining modality separation with region-focused reasoning.

</details>


### [2] [TalentMine: LLM-Based Extraction and Question-Answering from Multimodal Talent Tables](https://arxiv.org/abs/2507.00041)
*Varun Mannam,Fang Wang,Chaochun Liu,Xin Chen*

Main category: cs.AI

TL;DR: 传统语言模型处理人才管理系统中复杂表格信息检索有挑战，本文提出TalentMine框架，实验显示其性能优越，还确定Claude v3 Haiku模型最适合人才管理应用。


<details>
  <summary>Details</summary>
Motivation: 解决传统语言模型在处理人才管理系统中复杂表格信息检索的难题，以及现有表格提取方法语义理解不足的问题。

Method: 引入TalentMine框架，采用专门的多模态推理将提取的表格转换为语义丰富的表示。

Result: 在员工福利文档集合实验中，TalentMine查询回答任务准确率达100%，远高于标准AWS Textract提取（0%）和AWS Textract Visual Q&A能力（40%），Claude v3 Haiku模型对人才管理应用性能最优。

Conclusion: 论文对当前表格提取流程语义信息丢失进行系统分析，提出基于LLM的语义丰富表格表示方法，构建检索增强系统集成框架，在人才分析任务基准测试中多方面有显著改进。

Abstract: In talent management systems, critical information often resides in complex
tabular formats, presenting significant retrieval challenges for conventional
language models. These challenges are pronounced when processing Talent
documentation that requires precise interpretation of tabular relationships for
accurate information retrieval and downstream decision-making. Current table
extraction methods struggle with semantic understanding, resulting in poor
performance when integrated into retrieval-augmented chat applications. This
paper identifies a key bottleneck - while structural table information can be
extracted, the semantic relationships between tabular elements are lost,
causing downstream query failures. To address this, we introduce TalentMine, a
novel LLM-enhanced framework that transforms extracted tables into semantically
enriched representations. Unlike conventional approaches relying on CSV or text
linearization, our method employs specialized multimodal reasoning to preserve
both structural and semantic dimensions of tabular data. Experimental
evaluation across employee benefits document collections demonstrates
TalentMine's superior performance, achieving 100% accuracy in query answering
tasks compared to 0% for standard AWS Textract extraction and 40% for AWS
Textract Visual Q&A capabilities. Our comparative analysis also reveals that
the Claude v3 Haiku model achieves optimal performance for talent management
applications. The key contributions of this work include (1) a systematic
analysis of semantic information loss in current table extraction pipelines,
(2) a novel LLM-based method for semantically enriched table representation,
(3) an efficient integration framework for retrieval-augmented systems as
end-to-end systems, and (4) comprehensive benchmarks on talent analytics tasks
showing substantial improvements across multiple categories.

</details>


### [3] [A collaborative digital twin built on FAIR data and compute infrastructure](https://arxiv.org/abs/2507.00048)
*Thomas M. Deucher,Juan C. Verduzco,Michael Titus,Alejandro Strachan*

Main category: cs.AI

TL;DR: 本文提出基于nanoHUB服务的分布式SDL实现，促进科学工程领域发现与优化任务，以食物染料配色为例展示其应用及通用性。


<details>
  <summary>Details</summary>
Motivation: 结合机器学习与自动实验的SDL可加速科学工程发现与优化任务，FAIR数据基础设施支持下SDL协作更有效，因此提出分布式SDL实现。

Method: 构建基于nanoHUB服务的分布式SDL框架，地理分散的协作者将原始实验数据贡献到共享中央数据库，通过简单Web界面提交新数据点，用nanoHUB Sim2L处理，利用nanoHUB工作流进行主动学习的顺序优化。

Result: 以食物染料配色为例，研究人员和学生能设置自己的实验、共享数据、探索FAIR数据、预测性ML模型和顺序优化的结合。

Conclusion: 所介绍工具具有通用性，可轻松扩展到其他优化问题。

Abstract: The integration of machine learning with automated experimentation in
self-driving laboratories (SDL) offers a powerful approach to accelerate
discovery and optimization tasks in science and engineering applications. When
supported by findable, accessible, interoperable, and reusable (FAIR) data
infrastructure, SDLs with overlapping interests can collaborate more
effectively. This work presents a distributed SDL implementation built on
nanoHUB services for online simulation and FAIR data management. In this
framework, geographically dispersed collaborators conducting independent
optimization tasks contribute raw experimental data to a shared central
database. These researchers can then benefit from analysis tools and machine
learning models that automatically update as additional data become available.
New data points are submitted through a simple web interface and automatically
processed using a nanoHUB Sim2L, which extracts derived quantities and indexes
all inputs and outputs in a FAIR data repository called ResultsDB. A separate
nanoHUB workflow enables sequential optimization using active learning, where
researchers define the optimization objective, and machine learning models are
trained on-the-fly with all existing data, guiding the selection of future
experiments. Inspired by the concept of ``frugal twin", the optimization task
seeks to find the optimal recipe to combine food dyes to achieve the desired
target color. With easily accessible and inexpensive materials, researchers and
students can set up their own experiments, share data with collaborators, and
explore the combination of FAIR data, predictive ML models, and sequential
optimization. The tools introduced are generally applicable and can easily be
extended to other optimization problems.

</details>


### [4] [SEZ-HARN: Self-Explainable Zero-shot Human Activity Recognition Network](https://arxiv.org/abs/2507.00050)
*Devin Y. De Silva,Sandareka Wickramanayake,Dulani Meedeniya,Sanka Rasnayaka*

Main category: cs.AI

TL;DR: 提出自解释零样本人类活动识别网络SEZ - HARN，在四个基准数据集上评估，结果显示能产生可理解解释且有有竞争力的零样本识别准确率。


<details>
  <summary>Details</summary>
Motivation: 现有基于IMU的人类活动识别（HAR）因缺乏全面数据集和模型不透明受限，零样本HAR（ZS - HAR）模型决策难解释。

Method: 引入自解释零样本人类活动识别网络SEZ - HARN，可识别未遇活动并提供骨架视频解释决策过程，在四个基准数据集上评估并与三个最先进黑盒ZS - HAR模型对比。

Result: SEZ - HARN能产生现实且可理解的解释，在PAMAP2上零样本预测准确率与最佳黑盒模型相差在3%内，在其他三个数据集上表现相当。

Conclusion: SEZ - HARN模型在实现零样本活动识别的同时能提供有效解释，具有较好性能。

Abstract: Human Activity Recognition (HAR), which uses data from Inertial Measurement
Unit (IMU) sensors, has many practical applications in healthcare and assisted
living environments. However, its use in real-world scenarios has been limited
by the lack of comprehensive IMU-based HAR datasets that cover a wide range of
activities and the lack of transparency in existing HAR models. Zero-shot HAR
(ZS-HAR) overcomes the data limitations, but current models struggle to explain
their decisions, making them less transparent. This paper introduces a novel
IMU-based ZS-HAR model called the Self-Explainable Zero-shot Human Activity
Recognition Network (SEZ-HARN). It can recognize activities not encountered
during training and provide skeleton videos to explain its decision-making
process. We evaluate the effectiveness of the proposed SEZ-HARN on four
benchmark datasets PAMAP2, DaLiAc, HTD-MHAD and MHealth and compare its
performance against three state-of-the-art black-box ZS-HAR models. The
experiment results demonstrate that SEZ-HARN produces realistic and
understandable explanations while achieving competitive Zero-shot recognition
accuracy. SEZ-HARN achieves a Zero-shot prediction accuracy within 3\% of the
best-performing black-box model on PAMAP2 while maintaining comparable
performance on the other three datasets.

</details>


### [5] [Enhancing Reasoning Capabilities in SLMs with Reward Guided Dataset Distillation](https://arxiv.org/abs/2507.00054)
*Shreyansh Padarha*

Main category: cs.AI

TL;DR: 本文提出奖励引导的数据集蒸馏框架AdvDistill，提升了小语言模型在数学和复杂推理任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有知识蒸馏技术使小语言模型学习大语言模型存在局限，如泛化能力受限、推理任务表现差且计算成本高。

Method: 提出AdvDistill框架，利用教师模型对每个提示的多次生成结果，基于规则验证器分配奖励，以这些奖励作为训练学生模型的权重。

Result: 方法及其行为分析显示，学生模型在数学和复杂推理任务上的性能显著提升。

Conclusion: 在数据集蒸馏过程中引入奖励机制是有效且有益的。

Abstract: The push to compress and impart the proficiency of Large Language Models
(LLMs) into more deployable and efficient Small Language Models (SLMs) has
benefited from improvements in knowledge distillation (KD) techniques. These
techniques allow a smaller student model to learn from a more capable and
larger teacher model's responses. However, distillation often revolves around
the student model merely copying the teacher's in-distribution responses,
limiting its generalisability. This limitation is amplified on reasoning tasks
and can be computationally expensive. In this study, we propose AdvDistill, a
reward-guided dataset distillation framework. We utilise multiple generations
(responses) from a teacher for each prompt and assign rewards based on
rule-based verifiers. These varying and normally distributed rewards serve as
weights when training student models. Our methods and their subsequent
behavioural analysis demonstrate a significant improvement in student model
performance for mathematical and complex reasoning tasks, showcasing the
efficacy and benefits of incorporating a rewarding mechanism in dataset
distillation processes.

</details>


### [6] [VoyagerVision: Investigating the Role of Multi-modal Information for Open-ended Learning Systems](https://arxiv.org/abs/2507.00079)
*Ethan Smyth,Alessandro Suglia*

Main category: cs.AI

TL;DR: 本文提出多模态模型VoyagerVision，利用截图反馈在Minecraft中创建结构，扩展了模型开放性。


<details>
  <summary>Details</summary>
Motivation: 为增强模型解读空间环境的能力，提高其能成功执行的任务数量，扩展开放性潜力。

Method: 基于Voyager构建多模态模型VoyagerVision，使用截图作为视觉反馈。

Result: VoyagerVision在五十次迭代中平均创建2.75个独特结构；在平坦世界的建筑单元测试中成功率为一半，复杂结构失败较多。

Conclusion: 为模型提供视觉输入可增强其解读空间环境能力，VoyagerVision是开放性研究的新方向。

Abstract: Open-endedness is an active field of research in the pursuit of capable
Artificial General Intelligence (AGI), allowing models to pursue tasks of their
own choosing. Simultaneously, recent advancements in Large Language Models
(LLMs) such as GPT-4o [9] have allowed such models to be capable of
interpreting image inputs. Implementations such as OMNI-EPIC [4] have made use
of such features, providing an LLM with pixel data of an agent's POV to parse
the environment and allow it to solve tasks. This paper proposes that providing
these visual inputs to a model gives it greater ability to interpret spatial
environments, and as such, can increase the number of tasks it can successfully
perform, extending its open-ended potential. To this aim, this paper proposes
VoyagerVision -- a multi-modal model capable of creating structures within
Minecraft using screenshots as a form of visual feedback, building on the
foundation of Voyager. VoyagerVision was capable of creating an average of 2.75
unique structures within fifty iterations of the system, as Voyager was
incapable of this, it is an extension in an entirely new direction.
Additionally, in a set of building unit tests VoyagerVision was successful in
half of all attempts in flat worlds, with most failures arising in more complex
structures. Project website is available at
https://esmyth-dev.github.io/VoyagerVision.github.io/

</details>


### [7] [Thinking About Thinking: SAGE-nano's Inverse Reasoning for Self-Aware Language Models](https://arxiv.org/abs/2507.00092)
*Basab Jha,Firoj Paudel,Ujjwal Puri,Zhang Yuting,Choi Donghyuk,Wang Junhao*

Main category: cs.AI

TL;DR: 本文提出文本反向推理范式，使大语言模型能事后解释推理链。用SAGE - nano模型测试，该模型在推理准确性和解释质量上表现出色，还提出多个框架，为透明AI系统开辟新途径。


<details>
  <summary>Details</summary>
Motivation: 大语言模型用思维链提示解决复杂推理任务时决策过程是黑盒，需要一种方法让其能解释推理链。

Method: 引入文本反向推理范式，在SAGE - nano模型中采用元认知结构，通过注意力过程回溯来确定主要决策点并生成推理选择的解释。

Result: SAGE - nano在推理准确性（AQUA - RAT上74.6%）和解释质量（人类偏好得分92.1%）上处于前沿，性能与Claude - 3.5 Sonnet或GPT - 4o相近。

Conclusion: 提出多个框架，证明反向推理能提升推理可解释性和性能，为透明AI系统创造新途径，填补AI安全、教育和科学发现方面的差距。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities at
solving complex reasoning tasks with Chain-of-Thought (CoT) prompting, but
their decision-making processes remain somewhat blackbox. We introduce
textbfinverse reasoning, a novel paradigm enabling LLMs to decompose and
explain their own reasoning chains post-hoc. Our approach, used in SAGE-nano, a
4-billion-parameter reasoning model, employs a metacognitive structure that
reflects back via attention processes to identify major decision points and
generate explanations of reasoning choices. While typical CoT approaches are
directed towards forward reasoning generation, inverse reasoning provides
insight into why specific reasoning chains were selected over others. Through
thorough testing of logical reasoning puzzles, math problems and ethical
dilemmas from AQUA-RAT, CommonsenseQA, and customized benchmarks, we
demonstrate that SAGE-nano is at the cutting edge both on reasoning accuracy
(74.6% on AQUA-RAT) and explanation quality (92.1% human preference score) for
its task, and offers performance almost on par with models like Claude-3.5
Sonnet or GPT-4o. Our contributions are: (i) the first rigorous framework for
LLM self-reflection via inverse reasoning, (ii) a novel metalearning framework
to reverse the attention flow, (iii) comprehensive evaluation frameworks for
reasoning transparency, and (iv) evidence that increasing reasoning using
inverse reasoning improves interpretability along with reasoning performance.
Our work creates new avenues for transparent AI systems and closes significant
gaps in AI safety, education, and scientific discovery.

</details>


### [8] [BlackBoxToBlueprint: Extracting Interpretable Logic from Legacy Systems using Reinforcement Learning and Counterfactual Analysis](https://arxiv.org/abs/2507.00180)
*Vidhi Rathore*

Main category: cs.AI

TL;DR: 本文提出从黑盒遗留系统自动提取可解释决策逻辑的新管道，用RL代理探索输入空间，通过聚类和决策树提取规则，在三个虚拟遗留系统验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以捕捉遗留系统潜在决策逻辑，需要新方法解决缺乏文档和理解决策逻辑的问题。

Method: 使用强化学习代理探索输入空间，找出关键决策边界，收集反事实状态转换并用K-Means聚类，在聚类上训练决策树提取规则。

Result: 强化学习代理成功聚焦相关边界区域，提取的规则准确反映虚拟系统核心逻辑。

Conclusion: 该管道为遗留系统迁移时生成规范和测试用例提供了有前景的基础。

Abstract: Modernizing legacy software systems is a critical but challenging task, often
hampered by a lack of documentation and understanding of the original system's
intricate decision logic. Traditional approaches like behavioral cloning merely
replicate input-output behavior without capturing the underlying intent. This
paper proposes a novel pipeline to automatically extract interpretable decision
logic from legacy systems treated as black boxes. The approach uses a
Reinforcement Learning (RL) agent to explore the input space and identify
critical decision boundaries by rewarding actions that cause meaningful changes
in the system's output. These counterfactual state transitions, where the
output changes, are collected and clustered using K-Means. Decision trees are
then trained on these clusters to extract human-readable rules that approximate
the system's decision logic near the identified boundaries. I demonstrated the
pipeline's effectiveness on three dummy legacy systems with varying complexity,
including threshold-based, combined-conditional, and non-linear range logic.
Results show that the RL agent successfully focuses exploration on relevant
boundary regions, and the extracted rules accurately reflect the core logic of
the underlying dummy systems, providing a promising foundation for generating
specifications and test cases during legacy migration.

</details>


### [9] [ChatGPT produces more "lazy" thinkers: Evidence of cognitive engagement decline](https://arxiv.org/abs/2507.00181)
*Georgios P. Georgiou*

Main category: cs.AI

TL;DR: 研究探讨ChatGPT对学生学术写作认知参与度的影响，发现其会降低认知参与度，呼吁制定教学策略。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在教育中使用增多，担忧其减少深度思考和主动学习，研究ChatGPT对学生学术写作认知参与度的影响。

Method: 采用实验设计，将参与者随机分为AI辅助（ChatGPT）和非辅助（对照）组，完成写作任务后用CES - AI量表评估。

Result: ChatGPT组的认知参与度得分明显低于对照组。

Conclusion: AI辅助可能导致认知卸载，需制定教学策略促进学生积极反思性地使用AI内容，避免损害自主学习和深度认知。

Abstract: Despite the increasing use of large language models (LLMs) in education,
concerns have emerged about their potential to reduce deep thinking and active
learning. This study investigates the impact of generative artificial
intelligence (AI) tools, specifically ChatGPT, on the cognitive engagement of
students during academic writing tasks. The study employed an experimental
design with participants randomly assigned to either an AI-assisted (ChatGPT)
or a non-assisted (control) condition. Participants completed a structured
argumentative writing task followed by a cognitive engagement scale (CES), the
CES-AI, developed to assess mental effort, attention, deep processing, and
strategic thinking. The results revealed significantly lower cognitive
engagement scores in the ChatGPT group compared to the control group. These
findings suggest that AI assistance may lead to cognitive offloading. The study
contributes to the growing body of literature on the psychological implications
of AI in education and raises important questions about the integration of such
tools into academic practice. It calls for pedagogical strategies that promote
active, reflective engagement with AI-generated content to avoid compromising
self-regulated learning and deep cognitive involvement of students.

</details>


### [10] [Holistic Artificial Intelligence in Medicine; improved performance and explainability](https://arxiv.org/abs/2507.00205)
*Periklis Petridis,Georgios Margaritis,Vasiliki Stoumpou,Dimitris Bertsimas*

Main category: cs.AI

TL;DR: 提出xHAIM框架解决HAIM的局限性，评估显示其能提升AUC，让AI更具可解释性。


<details>
  <summary>Details</summary>
Motivation: HAIM以任务无关方式使用数据且缺乏可解释性，需改进。

Method: 通过四步结构化方法，利用生成式AI增强预测和可解释性，包括识别相关数据、生成患者总结、改进预测建模和提供临床解释。

Result: 在HAIM - MIMIC - MM数据集上，xHAIM将平均AUC从79.9%提升到90.3%。

Conclusion: xHAIM将AI从黑盒预测器转变为可解释的决策支持系统，连接AI进展与临床应用。

Abstract: With the increasing interest in deploying Artificial Intelligence in
medicine, we previously introduced HAIM (Holistic AI in Medicine), a framework
that fuses multimodal data to solve downstream clinical tasks. However, HAIM
uses data in a task-agnostic manner and lacks explainability. To address these
limitations, we introduce xHAIM (Explainable HAIM), a novel framework
leveraging Generative AI to enhance both prediction and explainability through
four structured steps: (1) automatically identifying task-relevant patient data
across modalities, (2) generating comprehensive patient summaries, (3) using
these summaries for improved predictive modeling, and (4) providing clinical
explanations by linking predictions to patient-specific medical knowledge.
Evaluated on the HAIM-MIMIC-MM dataset, xHAIM improves average AUC from 79.9%
to 90.3% across chest pathology and operative tasks. Importantly, xHAIM
transforms AI from a black-box predictor into an explainable decision support
system, enabling clinicians to interactively trace predictions back to relevant
patient data, bridging AI advancements with clinical utility.

</details>


### [11] [Learning for routing: A guided review of recent developments and future directions](https://arxiv.org/abs/2507.00218)
*Fangting Zhou,Attila Lischka,Balazs Kulcsar,Jiaming Wu,Morteza Haghir Chehreghani,Gilbert Laporte*

Main category: cs.AI

TL;DR: 本文回顾机器学习工具解决NP难组合优化路由问题的进展，提出分类法并提供框架指导未来研究。


<details>
  <summary>Details</summary>
Motivation: 传统精确算法求解NP难路由问题计算时间长，启发式算法无法保证最优解，而机器学习模型取得成功。

Method: 提出将基于机器学习的路由方法分为基于构造和基于改进的分类法。

Result: 对当前机器学习解决路由问题的进展进行了梳理和分类。

Conclusion: 有望将传统运筹学方法与先进机器学习技术结合，为未来研究提供结构化框架以解决新兴VRP变体问题。

Abstract: This paper reviews the current progress in applying machine learning (ML)
tools to solve NP-hard combinatorial optimization problems, with a focus on
routing problems such as the traveling salesman problem (TSP) and the vehicle
routing problem (VRP). Due to the inherent complexity of these problems, exact
algorithms often require excessive computational time to find optimal
solutions, while heuristics can only provide approximate solutions without
guaranteeing optimality. With the recent success of machine learning models,
there is a growing trend in proposing and implementing diverse ML techniques to
enhance the resolution of these challenging routing problems. We propose a
taxonomy categorizing ML-based routing methods into construction-based and
improvement-based approaches, highlighting their applicability to various
problem characteristics. This review aims to integrate traditional OR methods
with state-of-the-art ML techniques, providing a structured framework to guide
future research and address emerging VRP variants.

</details>


### [12] [ASTRO: Teaching Language Models to Reason by Reflecting and Backtracking In-Context](https://arxiv.org/abs/2507.00417)
*Joongwon Kim,Anirudh Goyal,Liang Tan,Hannaneh Hajishirzi,Srinivasan Iyer,Tianlu Wang*

Main category: cs.AI

TL;DR: 介绍ASTRO框架，通过合成数据集让语言模型内化搜索行为，应用于Llama 3模型有显著性能提升，证明搜索启发训练可增强开源大模型推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前不清楚如何提升非推理模型（如Llama 3）的推理能力，需要新方法。

Method: 通过蒙特卡罗树搜索（MCTS）在数学问题解决轨迹上生成合成数据集，将搜索轨迹转换为自然语言思维链，微调模型并通过可验证奖励的强化学习进一步提升性能。

Result: 应用ASTRO到Llama 3模型，在MATH - 500、AMC 2023和AIME 2024上分别取得16.0%、26.9%和20.0%的绝对性能提升，尤其在需要迭代修正的难题上表现出色。

Conclusion: 搜索启发训练为开源大语言模型注入强大推理能力提供了原则性方法。

Abstract: We introduce ASTRO, the "Autoregressive Search-Taught Reasoner", a framework
for training language models to reason like search algorithms, explicitly
leveraging self-reflection, backtracking, and exploration in their outputs.
Recently, training large language models (LLMs) via reinforcement learning (RL)
has led to the advent of reasoning models with greatly enhanced reasoning
capabilities. Open-source replications of reasoning models, while successful,
build upon models that already exhibit strong reasoning capabilities along with
search behavior observed even before RL. As a result, it is yet unclear how to
boost the reasoning capabilities of other non-reasoner models including Llama
3. ASTRO teaches such models to internalize structured search behavior through
a synthetic dataset derived from Monte Carlo Tree Search (MCTS) over
mathematical problem-solving trajectories. By converting search traces into
natural language chain-of-thoughts that capture both successes and recoveries
from failure, ASTRO bootstraps models with a rich prior for exploration during
RL. We finetune our models on these search-derived traces and further improve
performance via RL with verifiable rewards. We apply ASTRO to the Llama 3
family of models and achieve absolute performance gains of 16.0% on MATH-500,
26.9% on AMC 2023, and 20.0% on AIME 2024, especially improving upon
challenging problems that require iterative correction. Our results demonstrate
that search-inspired training offers a principled way to instill robust
reasoning capabilities into open LLMs.

</details>


### [13] [Does Math Reasoning Improve General LLM Capabilities? Understanding Transferability of LLM Reasoning](https://arxiv.org/abs/2507.00432)
*Maggie Huan,Yuetai Li,Tuney Zheng,Xiaoyu Xu,Seungone Kim,Minxin Du,Radha Poovendran,Graham Neubig,Xiang Yue*

Main category: cs.AI

TL;DR: 评估20多个推理调优模型，发现多数数学成功模型难迁移能力，Qwen3 - 14B实验显示RL调优模型泛化好，SFT调优常忘通用能力，需反思标准训练方法。


<details>
  <summary>Details</summary>
Motivation: 探究数学推理能力提升是反映更广泛问题解决能力还是过拟合。

Method: 评估20多个开放权重推理调优模型在多个任务上的表现，对Qwen3 - 14B模型用不同调优方法做控制实验，进行潜空间表示和令牌空间分布转移分析。

Result: 多数数学成功模型难迁移能力，RL调优模型泛化好，SFT调优常忘通用能力，SFT会导致表示和输出漂移，RL保留通用域结构。

Conclusion: 需要重新思考标准的训练后方法，特别是依赖SFT提取数据来推进推理模型的做法。

Abstract: Math reasoning has become the poster child of progress in large language
models (LLMs), with new models rapidly surpassing human-level performance on
benchmarks like MATH and AIME. But as math leaderboards improve week by week,
it is worth asking: do these gains reflect broader problem-solving ability or
just narrow overfitting? To answer this question, we evaluate over 20
open-weight reasoning-tuned models across a broad suite of tasks, including
math, scientific QA, agent planning, coding, and standard
instruction-following. We surprisingly find that most models that succeed in
math fail to transfer their gains to other domains. To rigorously study this
phenomenon, we conduct controlled experiments on Qwen3-14B models using
math-only data but different tuning methods. We find that reinforcement
learning (RL)-tuned models generalize well across domains, while supervised
fine-tuning (SFT)-tuned models often forget general capabilities. Latent-space
representation and token-space distribution shift analyses reveal that SFT
induces substantial representation and output drift, while RL preserves
general-domain structure. Our results suggest a need to rethink standard
post-training recipes, particularly the reliance on SFT-distilled data for
advancing reasoning models.

</details>


### [14] [Advancing Local Search in SMT-NRA with MCSAT Integration](https://arxiv.org/abs/2507.00557)
*Tianyi Ding,Haokun Li,Xinpeng Ni,Bican Xia,Tianqi Zhao*

Main category: cs.AI

TL;DR: 本文推进了SMT - NRA的局部搜索，提出新操作、框架和混合框架，实验显示局部搜索性能提升。


<details>
  <summary>Details</summary>
Motivation: 提升可满足性模非线性实数算术理论（SMT - NRA）局部搜索的效率。

Method: 引入2d - cell - jump操作，提出2d - LS框架，实现sample - cell projection operator技术，设计结合MCSAT、2d - LS和OpenCAD的混合框架。

Result: 实验结果表明局部搜索性能有所提高。

Conclusion: 所提出的方法对SMT - NRA的局部搜索是有效的。

Abstract: In this paper, we advance local search for Satisfiability Modulo the Theory
of Nonlinear Real Arithmetic (SMT-NRA for short). First, we introduce a
two-dimensional cell-jump move, called \emph{$2d$-cell-jump}, generalizing the
key operation, cell-jump, of the local search method for SMT-NRA. Then, we
propose an extended local search framework, named \emph{$2d$-LS} (following the
local search framework, LS, for SMT-NRA), integrating the model constructing
satisfiability calculus (MCSAT) framework to improve search efficiency. To
further improve the efficiency of MCSAT, we implement a recently proposed
technique called \emph{sample-cell projection operator} for MCSAT, which is
well suited for CDCL-style search in the real domain and helps guide the search
away from conflicting states. Finally, we design a hybrid framework for SMT-NRA
combining MCSAT, $2d$-LS and OpenCAD, to improve search efficiency through
information exchange. The experimental results demonstrate improvements in
local search performance, highlighting the effectiveness of the proposed
methods.

</details>


### [15] [Can Large Language Models Develop Strategic Reasoning? Post-training Insights from Learning Chess](https://arxiv.org/abs/2507.00726)
*Dongyoon Hwang,Hojoon Lee,Jaegul Choo,Dongmin Park,Jongho Park*

Main category: cs.AI

TL;DR: 研究大语言模型在国际象棋中通过强化学习发展战略推理能力，发现蒸馏密集奖励常优于稀疏二元奖励，但模型表现远低于专家水平，受限源于预训练模型内部对国际象棋理解不足。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在战略推理方面利用强化学习的研究较少，探究其在国际象棋中能否通过强化学习发展战略推理能力。

Method: 利用国际象棋预训练的动作价值网络，为大语言模型输出的走法质量提供密集奖励，进行知识蒸馏。

Result: 基于蒸馏的密集奖励通常优于稀疏二元奖励，但所有模型表现远低于专家水平。

Conclusion: 模型表现受限源于预训练模型内部对国际象棋理解不足，仅靠强化学习无法完全克服。

Abstract: While reinforcement learning (RL) for large language models (LLMs) has shown
promise in mathematical reasoning, strategic reasoning for LLMs using RL
remains largely unexplored. We investigate whether LLMs can develop strategic
reasoning capabilities through RL in chess. To this end, we leverage a
chess-pretrained action-value network to provide dense reward on the LLM's
output move quality, which can be seen as a form of knowledge distillation. Our
experiments show that our distillation-based dense rewards often outperform
sparse binary rewards. However, surprisingly, all models plateau far below
expert levels. We provide SFT and RL ablations on chess reasoning training and
find evidence that this limitation stems from a deficit in the pretrained
models' internal understanding of chess--a deficit which RL alone may not be
able to fully overcome.

</details>


### [16] [A Robust Algorithm for Non-IID Machine Learning Problems with Convergence Analysis](https://arxiv.org/abs/2507.00810)
*Qing Xu,Xiaohua Xuan*

Main category: cs.AI

TL;DR: 提出基于非光滑优化、二次规划和迭代过程的改进数值算法求解极小极大问题，给出收敛性证明及应用领域。


<details>
  <summary>Details</summary>
Motivation: 解决极小极大问题，提出更有效的算法。

Method: 基于非光滑优化、二次规划和迭代过程构建算法。

Result: 在梯度连续性和有界性等温和假设下，证明了算法的收敛性。

Conclusion: 该算法可广泛应用于鲁棒优化、不平衡学习等领域。

Abstract: In this paper, we propose an improved numerical algorithm for solving minimax
problems based on nonsmooth optimization, quadratic programming and iterative
process. We also provide a rigorous proof of convergence for our algorithm
under some mild assumptions, such as gradient continuity and boundedness. Such
an algorithm can be widely applied in various fields such as robust
optimization, imbalanced learning, etc.

</details>


### [17] [SafeMobile: Chain-level Jailbreak Detection and Automated Evaluation for Multimodal Mobile Agents](https://arxiv.org/abs/2507.00841)
*Siyuan Liang,Tianmeng Fang,Zhe Liu,Aishan Liu,Yan Xiao,Jinyuan He,Ee-Chien Chang,Xiaochun Cao*

Main category: cs.AI

TL;DR: 本文探讨移动多模态智能体安全问题，构建风险判别机制与自动化辅助评估方案，初步验证可提升风险行为识别并降低越狱概率。


<details>
  <summary>Details</summary>
Motivation: 多模态基础模型在智能体系统广泛应用，相关系统面临越狱风险，现有安全措施有局限且缺乏有效自动评估方法。

Method: 结合行为序列信息构建风险判别机制，基于大语言模型设计自动化辅助评估方案。

Result: 在几个代表性高风险任务初步验证，该方法能一定程度提升风险行为识别，降低智能体被越狱概率。

Conclusion: 本研究有望为多模态智能体系统安全风险建模与保护提供有价值参考。

Abstract: With the wide application of multimodal foundation models in intelligent
agent systems, scenarios such as mobile device control, intelligent assistant
interaction, and multimodal task execution are gradually relying on such large
model-driven agents. However, the related systems are also increasingly exposed
to potential jailbreak risks. Attackers may induce the agents to bypass the
original behavioral constraints through specific inputs, and then trigger
certain risky and sensitive operations, such as modifying settings, executing
unauthorized commands, or impersonating user identities, which brings new
challenges to system security. Existing security measures for intelligent
agents still have limitations when facing complex interactions, especially in
detecting potentially risky behaviors across multiple rounds of conversations
or sequences of tasks. In addition, an efficient and consistent automated
methodology to assist in assessing and determining the impact of such risks is
currently lacking. This work explores the security issues surrounding mobile
multimodal agents, attempts to construct a risk discrimination mechanism by
incorporating behavioral sequence information, and designs an automated
assisted assessment scheme based on a large language model. Through preliminary
validation in several representative high-risk tasks, the results show that the
method can improve the recognition of risky behaviors to some extent and assist
in reducing the probability of agents being jailbroken. We hope that this study
can provide some valuable references for the security risk modeling and
protection of multimodal intelligent agent systems.

</details>


### [18] [Thinking Beyond Tokens: From Brain-Inspired Intelligence to Cognitive Foundations for Artificial General Intelligence and its Societal Impact](https://arxiv.org/abs/2507.00951)
*Rizwan Qureshi,Ranjan Sapkota,Abbas Shah,Amgad Muneer,Anas Zafar,Ashmal Vayani,Maged Shoman,Abdelrahman B. M. Eldaly,Kai Zhang,Ferhat Sadak,Shaina Raza,Xinqi Fan,Ravid Shwartz-Ziv,Hong Yan,Vinjia Jain,Aman Chadha,Manoj Karkee,Jia Wu,Philip Torr,Seyedali Mirjalili*

Main category: cs.AI

TL;DR: 本文探讨AGI发展，分析通用智能基础，强调Agentic RAG框架等策略，指出智能源于记忆与推理整合，还探讨架构并识别发展挑战。


<details>
  <summary>Details</summary>
Motivation: 尽管现有模型能力提升，但因依赖token预测和缺乏实际行动能力有局限，要探索AGI发展路径。

Method: 进行跨学科综合研究，分析通用智能架构和认知基础，探讨多种策略和架构。

Result: 强调Agentic RAG框架作用，指出Vision - Language Models新角色，提出智能源于记忆与推理整合。

Conclusion: 近期架构开始缩小统计学习与目标导向认知差距，同时指出AGI发展面临科学、技术和伦理挑战。

Abstract: Can machines truly think, reason and act in domains like humans? This
enduring question continues to shape the pursuit of Artificial General
Intelligence (AGI). Despite the growing capabilities of models such as GPT-4.5,
DeepSeek, Claude 3.5 Sonnet, Phi-4, and Grok 3, which exhibit multimodal
fluency and partial reasoning, these systems remain fundamentally limited by
their reliance on token-level prediction and lack of grounded agency. This
paper offers a cross-disciplinary synthesis of AGI development, spanning
artificial intelligence, cognitive neuroscience, psychology, generative models,
and agent-based systems. We analyze the architectural and cognitive foundations
of general intelligence, highlighting the role of modular reasoning, persistent
memory, and multi-agent coordination. In particular, we emphasize the rise of
Agentic RAG frameworks that combine retrieval, planning, and dynamic tool use
to enable more adaptive behavior. We discuss generalization strategies,
including information compression, test-time adaptation, and training-free
methods, as critical pathways toward flexible, domain-agnostic intelligence.
Vision-Language Models (VLMs) are reexamined not just as perception modules but
as evolving interfaces for embodied understanding and collaborative task
completion. We also argue that true intelligence arises not from scale alone
but from the integration of memory and reasoning: an orchestration of modular,
interactive, and self-improving components where compression enables adaptive
behavior. Drawing on advances in neurosymbolic systems, reinforcement learning,
and cognitive scaffolding, we explore how recent architectures begin to bridge
the gap between statistical learning and goal-directed cognition. Finally, we
identify key scientific, technical, and ethical challenges on the path to AGI.

</details>


### [19] [Enhancing LLM Agent Safety via Causal Influence Prompting](https://arxiv.org/abs/2507.00979)
*Dongyoon Hahm,Woogyeol Jin,June Suk Choi,Sungsoo Ahn,Kimin Lee*

Main category: cs.AI

TL;DR: 提出CIP技术利用因果影响图提高大语言模型驱动的自主智能体决策安全性。


<details>
  <summary>Details</summary>
Motivation: 确保大语言模型驱动的自主智能体安全可靠行为，防止意外后果。

Method: 引入CIP技术，分三步：基于任务规范初始化CID、用CID指导智能体与环境交互、根据观察迭代优化CID。

Result: 该方法在代码执行和移动设备控制任务中有效提升安全性。

Conclusion: CIP技术可有效提高大语言模型驱动的自主智能体决策安全性。

Abstract: As autonomous agents powered by large language models (LLMs) continue to
demonstrate potential across various assistive tasks, ensuring their safe and
reliable behavior is crucial for preventing unintended consequences. In this
work, we introduce CIP, a novel technique that leverages causal influence
diagrams (CIDs) to identify and mitigate risks arising from agent
decision-making. CIDs provide a structured representation of cause-and-effect
relationships, enabling agents to anticipate harmful outcomes and make safer
decisions. Our approach consists of three key steps: (1) initializing a CID
based on task specifications to outline the decision-making process, (2)
guiding agent interactions with the environment using the CID, and (3)
iteratively refining the CID based on observed behaviors and outcomes.
Experimental results demonstrate that our method effectively enhances safety in
both code execution and mobile device control tasks.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [20] [Ensemble Kalman Filter for Data Assimilation coupled with low-resolution computations techniques applied in Fluid Dynamics](https://arxiv.org/abs/2507.00539)
*Paul Jeanney,Ashton Hetherington,Shady E. Ahmed,David Lanceta,Susana Saiz,José Miguel Perez,Soledad Le CLainche*

Main category: cs.CE

TL;DR: 本文提出用数据同化（DA）融合实验与模拟数据的降阶模型（ROM）估计流体动力学系统‘真实’状态，用低分辨率技术降低计算成本，结果显示在不影响精度下大幅减少计算时间和内存使用。


<details>
  <summary>Details</summary>
Motivation: 为更准确预测流体动力学系统状态，解决数据同化计算需求大的问题。

Method: 在降维框架内实现集合卡尔曼滤波（EnKF），采用低分辨率技术，通过下采样数据集计算并结合低成本奇异值分解（lcSVD）进行重建。

Result: 低分辨率技术显著减少计算时间和内存使用，如在湍流测试中，压缩率15.9时可加速13.7倍，内存压缩90.9%，相对均方根误差低至2.6%。

Conclusion: 该数据同化方法在流体动力学应用中有潜力，能平衡精度与低计算和内存成本，适用于大规模和实时应用。

Abstract: This paper presents an innovative Reduced-Order Model (ROM) for merging
experimental and simulation data using Data Assimilation (DA) to estimate the
"True" state of a fluid dynamics system, leading to more accurate predictions.
Our methodology introduces a novel approach implementing the Ensemble Kalman
Filter (EnKF) within a reduced-dimensional framework, grounded in a robust
theoretical foundation and applied to fluid dynamics. To address the
substantial computational demands of DA, the proposed ROM employs
low-resolution (LR) techniques to drastically reduce computational costs. This
approach involves downsampling datasets for DA computations, followed by an
advanced reconstruction technique based on low-cost Singular Value
Decomposition (lcSVD). The lcSVD method, a key innovation in this paper, has
never been applied to DA before and offers a highly efficient way to enhance
resolution with minimal computational resources. Our results demonstrate
significant reductions in both computation time and RAM usage through the LR
techniques without compromising the accuracy of the estimations. For instance,
in a turbulent test case, the LR approach with a compression rate of 15.9 can
achieve a speed-up of 13.7 and a RAM compression of 90.9% while maintaining a
low Relative Root Mean Square Error (RRMSE) of 2.6%, compared to 0.8% in the
high-resolution (HR) reference. Furthermore, we highlight the effectiveness of
the EnKF in estimating and predicting the state of fluid flow systems based on
limited observations and low-fidelity numerical data. This paper highlights the
potential of the proposed DA method in fluid dynamics applications,
particularly for improving computational efficiency in CFD and related fields.
Its ability to balance accuracy with low computational and memory costs makes
it suitable for large-scale and real-time applications, such as environmental
monitoring or aerospace.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [21] [Efficient Conformance Checking of Rich Data-Aware Declare Specifications (Extended)](https://arxiv.org/abs/2507.00094)
*Jacobo Casas-Ramos,Sarah Winkler,Alessandro Gianola,Marco Montali,Manuel Mucientes,Manuel Lama*

Main category: cs.DB

TL;DR: 本文挑战在使用支持通用数据类型和数据条件的数据感知Declare捕获参考模型时的数据感知一致性检查问题，结合A*搜索和SMT求解，提出新算法计算最优对齐，证明算法正确性并实验验证其效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于对齐的声明式过程模型一致性检查多关注纯控制流或有限数据感知扩展，计算对齐在数据依赖存在时更困难，本文旨在解决通用数据类型和数据条件下的数据感知一致性检查问题。

Method: 结合A*搜索和SMT求解，引入新算法技术，通过应用修复动作探索搜索空间，逐步解决约束违规。

Result: 证明算法正确性，实验表明该方法匹配或超越现有技术性能，支持更具表达力的数据依赖。

Conclusion: 该方法有潜力支持实际应用。

Abstract: Despite growing interest in process analysis and mining for data-aware
specifications, alignment-based conformance checking for declarative process
models has focused on pure control-flow specifications, or mild data-aware
extensions limited to numerical data and variable-to-constant comparisons. This
is not surprising: finding alignments is computationally hard, even more so in
the presence of data dependencies. In this paper, we challenge this problem in
the case where the reference model is captured using data-aware Declare with
general data types and data conditions. We show that, unexpectedly, it is
possible to compute data-aware optimal alignments in this rich setting,
enjoying at once efficiency and expressiveness. This is achieved by carefully
combining the two best-known approaches to deal with control flow and data
dependencies when computing alignments, namely A* search and SMT solving.
Specifically, we introduce a novel algorithmic technique that efficiently
explores the search space, generating descendant states through the application
of repair actions aiming at incrementally resolving constraint violations. We
prove the correctness of our algorithm and experimentally show its efficiency.
The evaluation witnesses that our approach matches or surpasses the performance
of the state of the art while also supporting significantly more expressive
data dependencies, showcasing its potential to support real-world applications.

</details>


### [22] [LIMAO: A Framework for Lifelong Modular Learned Query Optimization](https://arxiv.org/abs/2507.00188)
*Qihan Zhang,Shaolin Xie,Ibrahim Sabek*

Main category: cs.DB

TL;DR: 本文提出LIMAO框架解决现有学习型查询优化器在动态查询环境下的局限，实验表明其显著提升性能，减轻灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 多数学习型查询优化器在静态查询环境假设下运行，无法有效处理动态查询环境，且再训练会导致灾难性遗忘问题，降低泛化能力。

Method: 引入LIMAO框架，利用模块化终身学习技术、基于注意力的神经网络组合架构和高效训练范式，保留先验知识并适应新环境。

Result: 在两个学习型查询优化器中实现LIMAO，查询执行时间最多提升40%，执行时间方差最多降低60%，在部分基准测试中比Postgres快4倍。

Conclusion: LIMAO有效减轻灾难性遗忘，确保查询计划质量随时间稳定可靠，在实际查询优化中有实用优势。

Abstract: Query optimizers are crucial for the performance of database systems.
Recently, many learned query optimizers (LQOs) have demonstrated significant
performance improvements over traditional optimizers. However, most of them
operate under a limited assumption: a static query environment. This limitation
prevents them from effectively handling complex, dynamic query environments in
real-world scenarios. Extensive retraining can lead to the well-known
catastrophic forgetting problem, which reduces the LQO generalizability over
time. In this paper, we address this limitation and introduce LIMAO (Lifelong
Modular Learned Query Optimizer), a framework for lifelong learning of plan
cost prediction that can be seamlessly integrated into existing LQOs. LIMAO
leverages a modular lifelong learning technique, an attention-based neural
network composition architecture, and an efficient training paradigm designed
to retain prior knowledge while continuously adapting to new environments. We
implement LIMAO in two LQOs, showing that our approach is agnostic to
underlying engines. Experimental results show that LIMAO significantly enhances
the performance of LQOs, achieving up to a 40% improvement in query execution
time and reducing the variance of execution time by up to 60% under dynamic
workloads. By leveraging a precise and self-consistent design, LIMAO
effectively mitigates catastrophic forgetting, ensuring stable and reliable
plan quality over time. Compared to Postgres, LIMAO achieves up to a 4x speedup
on selected benchmarks, highlighting its practical advantages in real-world
query optimization.

</details>


### [23] [Meaningful Data Erasure in the Presence of Dependencies](https://arxiv.org/abs/2507.00343)
*Vishal Chakraborty,Youri Kaminsky,Sharad Mehrotra,Felix Naumann,Faisal Nawab,Primal Pappachan,Mohammad Sadoghi,Nalini Venkatasubramanian*

Main category: cs.DB

TL;DR: 针对数据擦除定义模糊问题，本文给出精确概念，设计擦除机制，探索平衡策略并验证算法实用性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 数据法规如GDPR对数据擦除定义模糊，在数据库中因数据依赖导致合规困难。

Method: 正式定义精确的数据擦除概念，设计擦除机制，探索平衡成本与吞吐量等策略。

Result: 使用真实和合成数据集证明了算法的实用性和可扩展性。

Conclusion: 所提出的概念、机制和策略能有效解决数据擦除定义模糊带来的合规难题。

Abstract: Data regulations like GDPR require systems to support data erasure but leave
the definition of "erasure" open to interpretation. This ambiguity makes
compliance challenging, especially in databases where data dependencies can
lead to erased data being inferred from remaining data. We formally define a
precise notion of data erasure that ensures any inference about deleted data,
through dependencies, remains bounded to what could have been inferred before
its insertion. We design erasure mechanisms that enforce this guarantee at
minimal cost. Additionally, we explore strategies to balance cost and
throughput, batch multiple erasures, and proactively compute data retention
times when possible. We demonstrate the practicality and scalability of our
algorithms using both real and synthetic datasets.

</details>


### [24] [Towards Robustness: A Critique of Current Vector Database Assessments](https://arxiv.org/abs/2507.00379)
*Zikai Wang,Qianxi Zhang,Baotong Lu,Qi Chen,Cheng Tan*

Main category: cs.DB

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Vector databases are critical infrastructure in AI systems, and average
recall is the dominant metric for their evaluation. Both users and researchers
rely on it to choose and optimize their systems. We show that relying on
average recall is problematic. It hides variability across queries, allowing
systems with strong mean performance to underperform significantly on hard
queries. These tail cases confuse users and can lead to failure in downstream
applications such as RAG. We argue that robustness consistently achieving
acceptable recall across queries is crucial to vector database evaluation. We
propose Robustness-$\delta$@K, a new metric that captures the fraction of
queries with recall above a threshold $\delta$. This metric offers a deeper
view of recall distribution, helps vector index selection regarding application
needs, and guides the optimization of tail performance. We integrate
Robustness-$\delta$@K into existing benchmarks and evaluate mainstream vector
indexes, revealing significant robustness differences. More robust vector
indexes yield better application performance, even with the same average
recall. We also identify design factors that influence robustness, providing
guidance for improving real-world performance.

</details>


### [25] [Zero-Knowledge Verifiable Graph Query Evaluation via Expansion-Centric Operator Decomposition](https://arxiv.org/abs/2507.00427)
*Hao Wu,Changzheng Wei,Yanhao Wang,Li Lin,Yilong Leng,Shiyu He,Minghao Zhao,Hanghang Wu,Ying Yan,Aoying Zhou*

Main category: cs.DB

TL;DR: 本文探讨图数据库零知识可验证性的可行性，提出将图查询分解为细粒度操作符的方法，实现 ZKGraph 系统，性能优于传统实现。


<details>
  <summary>Details</summary>
Motivation: 已有关系数据库零知识验证能力的探索，但图数据库因查询复杂度高，实现面临独特挑战，需研究其零知识可验证性。

Method: 将图查询分解为更细粒度的原始操作符，通过小规模电路逐步评估；设计以扩展为中心的操作符分解，构建扩展原语和相关属性的 ZKP 电路，利用 PLONKish 算术化。

Result: 实现了 ZKGraph 系统，在运行时间和内存消耗上相比图操作符的朴素电路实现有显著提升。

Conclusion: 提出的方法有效可行，能在图数据库中实现可验证查询处理并保护数据隐私，ZKGraph 系统性能良好。

Abstract: This paper investigates the feasibility of achieving zero-knowledge
verifiability for graph databases, enabling database owners to
cryptographically prove the query execution correctness without disclosing the
underlying data. Although similar capabilities have been explored for
relational databases, their implementation for graph databases presents unique
challenges. This is mainly attributed to the relatively large complexity of
queries in graph databases. When translating graph queries into arithmetic
circuits, the circuit scale can be too large to be practically evaluated. To
address this issue, we propose to break down graph queries into more
fine-grained, primitive operators, enabling a step-by-step evaluation through
smaller-scale circuits. Accordingly, the verification with ZKP circuits of
complex graph queries can be decomposed into a series of composable
cryptographic primitives, each designed to verify a fundamental structural
property such as path ordering or edge directionality. Especially, having
noticed that the graph expansion (i.e., traversing from nodes to their
neighbors along edges) operation serves as the backbone of graph query
evaluation, we design the expansion centric operator decomposition. In addition
to constructing circuits for the expansion primitives, we also design
specialized ZKP circuits for the various attributes that augment this
traversal. The circuits are meticulously designed to take advantage of PLONKish
arithmetization. By integrating these optimized circuits, we implement ZKGraph,
a system that provides verifiable query processing while preserving data
privacy. Performance evaluation indicates that ZKGraph significantly
outperforms naive in circuit implementations of graph operators, achieving
substantial improvements in both runtime and memory consumption.

</details>


### [26] [Towards Efficient Random-Order Enumeration for Join Queries](https://arxiv.org/abs/2507.00489)
*Pengyu Chen,Zizheng Guo,Jianwei Yang,Dongjing Miao*

Main category: cs.DB

TL;DR: 本文研究连接查询结果随机顺序枚举问题，提出高效算法，证明其近最优性，设计加速技术，实验显示算法优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏针对（循环）连接查询的具有最坏情况运行时间保证的高效随机顺序枚举算法，需要解决该问题。

Method: 开发高效随机顺序枚举算法，复杂度无大隐藏常数；设计两种非平凡加速技术。

Result: 算法达到预期的延迟和总运行时间，在最坏情况下近最优；实验表明增强后的算法显著优于现有方法。

Conclusion: 提出的算法和加速技术有效，能为连接查询结果随机顺序枚举提供高效解决方案。

Abstract: In many data analysis pipelines, a basic and time-consuming process is to
produce join results and feed them into downstream tasks. Numerous enumeration
algorithms have been developed for this purpose. To be a statistically
meaningful representation of the whole join result, the result tuples are
required to be enumerated in uniformly random order. However, existing studies
lack an efficient random-order enumeration algorithm with a worst-case runtime
guarantee for (cyclic) join queries. In this paper, we study the problem of
enumerating the results of a join query in random order. We develop an
efficient random-order enumeration algorithm for join queries with no large
hidden constants in its complexity, achieving expected
$O(\frac{\mathrm{AGM}(Q)}{|Res(Q)|}\log^2|Q|)$ delay,
$O(\mathrm{AGM}(Q)\log|Q|)$ total running time after $O(|Q|\log|Q|)$-time index
construction, where $|Q|$ is the size of input, $\mathrm{AGM}(Q)$ is the AGM
bound, and $|Res(Q)|$ is the size of the join result. We prove that our
algorithm is near-optimal in the worst case, under the combinatorial $k$-clique
hypothesis. Our algorithm requires no query-specific preprocessing and can be
flexibly adapted to many common database indexes with only minor modifications.
We also devise two non-trivial techniques to speed up the enumeration, and
provide an experimental study on our enumeration algorithm along with the
speed-up techniques. The experimental results show that our algorithm, enhanced
with the proposed techniques, significantly outperforms existing
state-of-the-art methods.

</details>


### [27] [RapidStore: An Efficient Dynamic Graph Storage System for Concurrent Queries](https://arxiv.org/abs/2507.00839)
*Chiyu Hao,Jixian Su,Shixuan Sun,Hao Zhang,Sen Gao,Jianwen Zhao,Chenyi Zhang,Jieru Zhao,Chen Chen,Minyi Guo*

Main category: cs.DB

TL;DR: 针对动态图存储系统并发读写处理难题，提出RapidStore方法，实验证明其能提升性能与效率。


<details>
  <summary>Details</summary>
Motivation: 现有动态图存储系统在处理并发读写操作时存在写查询干扰读效率、版本管理开销大、无法平衡性能等问题。

Method: 提出RapidStore，采用解耦系统设计，分离读写查询管理，将版本数据与图数据解耦，设计高效动态图存储与图并发控制机制配合。

Result: RapidStore可实现快速且可扩展的并发图查询，有效平衡插入、搜索和扫描性能。

Conclusion: RapidStore显著提高了动态图存储系统的效率。

Abstract: Dynamic graph storage systems are essential for real-time applications such
as social networks and recommendation, where graph data continuously evolves.
However, they face significant challenges in efficiently handling concurrent
read and write operations. We find that existing methods suffer from write
queries interfering with read efficiency, substantial time and space overhead
due to per-edge versioning, and an inability to balance performance, such as
slow searches under concurrent workloads. To address these issues, we propose
RapidStore, a holistic approach for efficient in-memory dynamic graph storage
designed for read-intensive workloads. Our key idea is to exploit the
characteristics of graph queries through a decoupled system design that
separates the management of read and write queries and decouples version data
from graph data. Particularly, we design an efficient dynamic graph store to
cooperate with the graph concurrency control mechanism. Experimental results
demonstrate that RapidStore enables fast and scalable concurrent graph queries,
effectively balancing the performance of inserts, searches, and scans, and
significantly improving efficiency in dynamic graph storage systems.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [28] [CrossPipe: Towards Optimal Pipeline Schedules for Cross-Datacenter Training](https://arxiv.org/abs/2507.00217)
*Tiancheng Chen,Ales Kubicek,Langwen Huang,Torsten Hoefler*

Main category: cs.DC

TL;DR: 提出CrossPipe框架优化跨数据中心的大语言模型训练，评估显示可减少训练时间，提升可扩展性和资源利用率。


<details>
  <summary>Details</summary>
Motivation: 训练大语言模型所需资源超单数据中心，跨数据中心策略愈发重要，需解决网络延迟和带宽限制问题。

Method: CrossPipe框架通过显式建模和缓解网络延迟及带宽限制影响，结合流水线并行和数据并行通信优化，使用求解器最优或贪心算法生成优化调度，执行引擎分离调度逻辑和通信细节。

Result: 在相同内存约束下，相比传统流水线调度，CrossPipe最多减少33.6%训练时间；放松内存约束，虽有通信延迟仍保持良好性能。

Conclusion: CrossPipe在高网络延迟或有限带宽环境下，能提升可扩展性和资源利用率。

Abstract: Training large language models (LLMs) now requires resources that exceed a
single datacenter, making cross-datacenter strategies increasingly crucial. We
present CrossPipe, a framework designed to optimize model training across
geographically distributed datacenters by explicitly modeling and mitigating
the impact of network latency and limited bandwidth. It enables unified
analysis and optimization incorporating both pipeline parallelism (PP) and
opportunities for overlapping data parallelism (DP) communication. CrossPipe
generates optimized pipeline schedules using either solver-based optimal or
fast near-optimal greedy algorithms, built upon a flexible execution engine
that separates scheduling logic from communication details. Our evaluation
shows that CrossPipe reduces training time by up to 33.6\% compared to
traditional pipeline schedules under identical memory constraints. When memory
constraints are relaxed, CrossPipe maintains strong performance despite
communication delays, approaching the efficiency of idealized schedules without
delays. CrossPipe offers improved scalability and resource utilization,
particularly in environments with high network latency or limited bandwidth.

</details>


### [29] [Serving LLMs in HPC Clusters: A Comparative Study of Qualcomm Cloud AI 100 Ultra and High-Performance GPUs](https://arxiv.org/abs/2507.00418)
*Mohammad Firas Sada,John J. Graham,Elham E Khoda,Mahidhar Tatineni,Dmitry Mishin,Rajesh K. Gupta,Rick Wagner,Larry Smarr,Thomas A. DeFanti,Frank Würthwein*

Main category: cs.DC

TL;DR: 本文对Qualcomm Cloud AI 100 Ultra加速器进行基准分析，评估其在大语言模型推理中的能效和性能，发现其能效高，为其在HPC应用提供见解。


<details>
  <summary>Details</summary>
Motivation: 评估Qualcomm Cloud AI 100 Ultra加速器在大语言模型推理中的能效和性能，并与NVIDIA和AMD的GPU对比。

Method: 使用vLLM框架服务15个开源大语言模型，在国家研究平台生态系统中进行评估。

Result: Qualcomm Cloud AI 100 Ultra推理卡能效高，在多数情况下能效指标表现良好。

Conclusion: 研究结果为Qualcomm Cloud AI 100 Ultra在国家研究平台的高性能计算应用提供了见解。

Abstract: This study presents a benchmarking analysis of the Qualcomm Cloud AI 100
Ultra (QAic) accelerator for large language model (LLM) inference, evaluating
its energy efficiency (throughput per watt) and performance against leading
NVIDIA (A100, H200) and AMD (MI300A) GPUs within the National Research Platform
(NRP) ecosystem. A total of 15 open-source LLMs, ranging from 117 million to 90
billion parameters, are served using the vLLM framework. The QAic inference
cards appears to be energy efficient and performs well in the energy efficiency
metric in most cases. The findings offer insights into the potential of the
Qualcomm Cloud AI 100 Ultra for high-performance computing (HPC) applications
within the National Research Platform (NRP).

</details>


### [30] [Real-Time In-Network Machine Learning on P4-Programmable FPGA SmartNICs with Fixed-Point Arithmetic and Taylor](https://arxiv.org/abs/2507.00428)
*Mohammad Firas Sada,John J. Graham,Mahidhar Tatineni,Dmitry Mishin,Thomas A. DeFanti,Frank Würthwein*

Main category: cs.DC

TL;DR: 探讨将P4编程范式应用于神经网络和回归模型，可在网络边缘灵活部署可重训练的机器学习模型。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习应用融入现代网络操作，对支持低延迟机器学习推理的网络可编程性需求增加，P4可编程FPGA SmartNICs是研究网络内机器学习的理想平台。

Method: 将P4编程范式应用于神经网络和回归模型，权重和偏置存储在控制平面表查找中。

Result: 实现了灵活的可编程性以及可重训练机器学习模型在网络边缘的高效部署。

Conclusion: 可在网络边缘灵活部署可重训练的机器学习模型，且与交换机层面的核心基础设施无关。

Abstract: As machine learning (ML) applications become integral to modern network
operations, there is an increasing demand for network programmability that
enables low-latency ML inference for tasks such as Quality of Service (QoS)
prediction and anomaly detection in cybersecurity. ML models provide
adaptability through dynamic weight adjustments, making Programming
Protocol-independent Packet Processors (P4)-programmable FPGA SmartNICs an
ideal platform for investigating In-Network Machine Learning (INML). These
devices offer high-throughput, low-latency packet processing and can be
dynamically reconfigured via the control plane, allowing for flexible
integration of ML models directly at the network edge. This paper explores the
application of the P4 programming paradigm to neural networks and regression
models, where weights and biases are stored in control plane table lookups.
This approach enables flexible programmability and efficient deployment of
retrainable ML models at the network edge, independent of core infrastructure
at the switch level.

</details>


### [31] [LLM-Mesh: Enabling Elastic Sharing for Serverless LLM Inference](https://arxiv.org/abs/2507.00507)
*Chuhao Xu,Zijun Li,Quan Chen,Han Zhao,Minyi Guo*

Main category: cs.DC

TL;DR: 本文提出用于中小规模大语言模型的无服务器推理方案LLM - Mesh，可跨异构硬件弹性共享资源，实验显示能提升服务容量。


<details>
  <summary>Details</summary>
Motivation: 大语言模型兴起带来私有无服务器部署需求，现有方案采用专属GPU部署，新兴带加速器的CPU架构未充分利用，且CPU和GPU可同时容纳多个大语言模型，因此探索新方案。

Method: 提出LLM - Mesh方案，解决三个关键挑战：在令牌级进行精确细粒度计算资源分配；采用协调且有前瞻性的内存扩展机制；通过主动抢占和被动装箱减少资源碎片化。

Result: 在4个32核CPU和4个A100 GPU上实验，LLM - Mesh通过共享使服务容量提升44% - 63%，利用CPU可将提升幅度提高到91% - 159%。

Conclusion: LLM - Mesh能有效实现中小规模大语言模型在异构硬件上的弹性资源共享，提升服务容量。

Abstract: The rise of LLMs has driven demand for private serverless deployments,
characterized by moderate-scale models and infrequent requests. While existing
solutions follow exclusive GPU deployment, we take a step back to explore
modern platforms and find that: Emerging CPU architectures with built-in
accelerators are capable of serving LLMs but remain underutilized, and both
CPUs and GPUs can accommodate multiple LLMs simultaneously.
  We propose LLM-Mesh, a serverless inference scheme for small-to-mid-sized
LLMs that enables elastic sharing across heterogeneous hardware. LLM-Mesh
tackles three fundamental challenges: (1) precise, fine-grained compute
resource allocation at token-level to handle fluctuating computational demands;
(2) a coordinated and forward-looking memory scaling mechanism to detect
out-of-memory hazards and reduce operational overhead; and (3) a dual approach
that reduces resource fragmentation through proactive preemption and reactive
bin-packing. Experimental results on 4 32-core CPUs and 4 A100 GPUs show that
LLM-Meshimproves service capacity by 44% - 63% through sharing, while further
leveraging CPUs boosts this to 91% - 159%.

</details>


### [32] [PANDAS: Peer-to-peer, Adaptive Networking for Data Availability Sampling within Ethereum Consensus Timebounds](https://arxiv.org/abs/2507.00824)
*Matthieu Pigaglio,Onur Ascigil,Michał Król,Sergi Rene,Felix Lange,Kaleem Peeroo,Ramin Sadre,Vladimir Stankovic,Etienne Rivière*

Main category: cs.DC

TL;DR: 现有方案无法在严格时间限制下支持以太坊Layer - 2数据的传播和采样，本文提出PANDAS方法，经评估能在4秒内完成全球规模延迟下的传播和采样。


<details>
  <summary>Details</summary>
Motivation: 以太坊Danksharding进化需选择性分发Layer - 2数据并通过DAS验证，但现有方案无法在4秒内完成数据传播和采样。

Method: 提出PANDAS方法，使用轻量级直接交换进行Layer - 2数据传播和可用性采样，考虑消息丢失、节点故障等情况。

Result: 在1000节点集群中对PANDAS原型评估，模拟最多20000个节点，能在4秒内完成全球规模延迟下的传播和采样。

Conclusion: PANDAS能在不修改以太坊共识和节点发现协议的情况下，满足Danksharding要求，实现Layer - 2数据传播和采样。

Abstract: Layer-2 protocols can assist Ethereum's limited throughput, but globally
broadcasting layer-2 data limits their scalability. The Danksharding evolution
of Ethereum aims to support the selective distribution of layer-2 data, whose
availability in the network is verified using randomized data availability
sampling (DAS). Integrating DAS into Ethereum's consensus process is
challenging, as pieces of layer-2 data must be disseminated and sampled within
four seconds of the beginning of each consensus slot. No existing solution can
support dissemination and sampling under such strict time bounds.
  We propose PANDAS, a practical approach to integrate DAS with Ethereum under
Danksharding's requirements without modifying its protocols for consensus and
node discovery. PANDAS disseminates layer-2 data and samples its availability
using lightweight, direct exchanges. Its design accounts for message loss, node
failures, and unresponsive participants while anticipating the need to scale
out the Ethereum network. Our evaluation of PANDAS's prototype in a 1,000-node
cluster and simulations for up to 20,000 peers shows that it allows layer-2
data dissemination and sampling under planetary-scale latencies within the
4-second deadline.

</details>


### [33] [Collaborative Multi-Agent Reinforcement Learning Approach for Elastic Cloud Resource Scaling](https://arxiv.org/abs/2507.00550)
*Bruce Fang,Danyi Gao*

Main category: cs.DC

TL;DR: 论文针对云计算环境挑战，提出基于多智能体系统的弹性云资源扩展优化方法，经实验验证优于现有方法，提供新解决方案。


<details>
  <summary>Details</summary>
Motivation: 应对云计算环境中资源快速变化和任务负载高度不确定的挑战。

Method: 部署多个自治智能体并行感知资源状态并做局部决策，引入协作价值函数实现全局协调；设计轻量级状态预测模型；采用集中训练、分散执行的强化学习框架进行策略训练。

Result: 所提多智能体扩展策略在资源利用率、SLA违规控制和调度延迟方面优于现有方法，展现出强适应性和智能调节能力。

Conclusion: 为复杂云平台弹性资源扩展问题提供了高效可靠的新方法。

Abstract: This paper addresses the challenges of rapid resource variation and highly
uncertain task loads in cloud computing environments. It proposes an
optimization method for elastic cloud resource scaling based on a multi-agent
system. The method deploys multiple autonomous agents to perceive resource
states in parallel and make local decisions. While maintaining the distributed
nature of the system, it introduces a collaborative value function to achieve
global coordination. This improves the responsiveness of resource scheduling
and enhances overall system performance. To strengthen system foresight, a
lightweight state prediction model is designed. It assists agents in
identifying future workload trends and optimizes the selection of scaling
actions. For policy training, the method adopts a centralized training and
decentralized execution reinforcement learning framework. This enables agents
to learn effectively and coordinate strategies under conditions of incomplete
information. The paper also constructs typical cloud scenarios, including
multi-tenancy and burst traffic, to evaluate the proposed method. The
evaluation focuses on resource isolation, service quality assurance, and
robustness. Experimental results show that the proposed multi-agent scaling
strategy outperforms existing methods in resource utilization, SLA violation
control, and scheduling latency. The results demonstrate strong adaptability
and intelligent regulation. This provides an efficient and reliable new
approach to solving the problem of elastic resource scaling in complex cloud
platforms.

</details>


### [34] [Turning AI Data Centers into Grid-Interactive Assets: Results from a Field Demonstration in Phoenix, Arizona](https://arxiv.org/abs/2507.00909)
*Philip Colangelo,Ayse K. Coskun,Jack Megrue,Ciaran Roberts,Shayan Sengupta,Varun Sivaram,Ethan Tiao,Aroon Vijaykar,Chris Williams,Daniel C. Wilson,Zack MacFarland,Daniel Dreiling,Nathan Morey,Anuja Ratnayake,Baskar Vairamohan*

Main category: cs.DC

TL;DR: 本文提出软件方法Emerald Conductor将AI数据中心转变为灵活电网资源，试验实现高峰时集群功耗降低25%，提升电网可靠性等。


<details>
  <summary>Details</summary>
Motivation: AI带来电力需求增长，威胁电网可靠性、抬高价格、阻碍AI创新，需解决数据中心与电网的适配问题。

Method: 与主要企业合作，采用软件方法Emerald Conductor，基于实时电网信号编排AI工作负载，无需硬件修改或储能。

Result: 在亚利桑那州凤凰城商业超大规模云数据中心的256-GPU集群试验中，高峰时段实现三小时集群功耗降低25%，且保证AI服务质量。

Conclusion: 该平台将数据中心重新定义为电网交互资产，可增强电网可靠性、提升经济性并加速AI发展。

Abstract: Artificial intelligence (AI) is fueling exponential electricity demand
growth, threatening grid reliability, raising prices for communities paying for
new energy infrastructure, and stunting AI innovation as data centers wait for
interconnection to constrained grids. This paper presents the first field
demonstration, in collaboration with major corporate partners, of a
software-only approach--Emerald Conductor--that transforms AI data centers into
flexible grid resources that can efficiently and immediately harness existing
power systems without massive infrastructure buildout. Conducted at a 256-GPU
cluster running representative AI workloads within a commercial, hyperscale
cloud data center in Phoenix, Arizona, the trial achieved a 25% reduction in
cluster power usage for three hours during peak grid events while maintaining
AI quality of service (QoS) guarantees. By orchestrating AI workloads based on
real-time grid signals without hardware modifications or energy storage, this
platform reimagines data centers as grid-interactive assets that enhance grid
reliability, advance affordability, and accelerate AI's development.

</details>


### [35] [DynoStore: A wide-area distribution system for the management of data over heterogeneous storage](https://arxiv.org/abs/2507.00576)
*Dante D. Sanchez-Gallegos,J. L. Gonzalez-Compean,Maxime Gonthier,Valerie Hayot-Sasson,J. Gregory Pauloski,Haochen Pan,Kyle Chard,Jesus Carretero,Ian Foster*

Main category: cs.DC

TL;DR: 论文提出DynoStore系统管理跨异构存储系统的数据，通过数据容器等实现无缝管理，评估显示性能提升、容错性强。


<details>
  <summary>Details</summary>
Motivation: 数据分布存储有优势，但因协议、认证模型等问题管理困难，需统一协调框架。

Method: 提出DynoStore系统，以数据容器为核心提供标准接口，用擦除编码保证弹性，有负载均衡算法。

Result: 相比集中式云托管系统性能提升10%，与Redis和IPFS性能相当，容错性优于传统系统。

Conclusion: DynoStore能有效管理跨异构存储系统的数据，有更好性能和容错性。

Abstract: Data distribution across different facilities offers benefits such as
enhanced resource utilization, increased resilience through replication, and
improved performance by processing data near its source. However, managing such
data is challenging due to heterogeneous access protocols, disparate
authentication models, and the lack of a unified coordination framework. This
paper presents DynoStore, a system that manages data across heterogeneous
storage systems. At the core of DynoStore are data containers, an abstraction
that provides standardized interfaces for seamless data management,
irrespective of the underlying storage systems. Multiple data container
connections create a cohesive wide-area storage network, ensuring resilience
using erasure coding policies. Furthermore, a load-balancing algorithm ensures
equitable and efficient utilization of storage resources. We evaluate DynoStore
using benchmarks and real-world case studies, including the management of
medical and satellite data across geographically distributed environments. Our
results demonstrate a 10\% performance improvement compared to centralized
cloud-hosted systems while maintaining competitive performance with
state-of-the-art solutions such as Redis and IPFS. DynoStore also exhibits
superior fault tolerance, withstanding more failures than traditional systems.

</details>


### [36] [Accelerating Loading WebGraphs in ParaGrapher](https://arxiv.org/abs/2507.00716)
*Mohsen Koohi Esfahani*

Main category: cs.DC

TL;DR: 针对ParaGrapher的局限性提出PG - Fuse和CompBin两个优化，评估显示分别达7.6和21.8倍加速。


<details>
  <summary>Details</summary>
Motivation: 之前研究发现ParaGrapher存在高效能存储利用不足和压缩比提高导致解压缩带宽降低的问题，需进行优化。

Method: 引入基于FUSE的文件系统PG - Fuse提高存储利用率；引入CSR格式的紧凑二进制表示CompBin提高解压缩带宽。

Result: 在12个真实和合成图（最多1280亿条边）上评估，PG - Fuse和CompBin分别实现最高7.6和21.8倍加速。

Conclusion: 提出的PG - Fuse和CompBin优化有效提升了ParaGrapher性能。

Abstract: ParaGrapher is a graph loading API and library that enables graph processing
frameworks to load large-scale compressed graphs with minimal overhead. This
capability accelerates the design and implementation of new high-performance
graph algorithms and their evaluation on a wide range of graphs and across
different frameworks. However, our previous study identified two major
limitations in ParaGrapher: inefficient utilization of high-bandwidth storage
and reduced decompression bandwidth due to increased compression ratios. To
address these limitations, we present two optimizations for ParaGrapher in this
paper. To improve storage utilization, particularly for high-bandwidth storage,
we introduce ParaGrapher-FUSE (PG-Fuse) a filesystem based on the FUSE
(Filesystem in User Space). PG-Fuse optimizes storage access by increasing the
size of requested blocks, reducing the number of calls to the underlying
filesystem, and caching the received blocks in memory for future calls. To
improve the decompression bandwidth, we introduce CompBin, a compact binary
representation of the CSR format. CompBin facilitates direct accesses to
neighbors while preventing storage usage for unused bytes. Our evaluation on 12
real-world and synthetic graphs with up to 128 billion edges shows that PG-Fuse
and CompBin achieve up to 7.6 and 21.8 times speedup, respectively.

</details>


### [37] [A New Family of Thread to Core Allocation Policies for an SMT ARM Processor](https://arxiv.org/abs/2507.00855)
*Marta Navarro,Josué Feliu,Salvador Petit,María E. Gómez,Julio Sahuquillo*

Main category: cs.DC

TL;DR: 本文探讨在ARM处理器构建性能栈的方法，提出ISC栈，推出SYNPA系列T2C分配策略，实验表明SYNPA4表现最佳，相关讨论可用于其他SMT处理器。


<details>
  <summary>Details</summary>
Motivation: SMT处理器因应用间干扰优化性能有挑战，当前T2C策略第一步构建性能栈有提升空间，需解决ARM PMU局限性。

Method: 探索不同构建性能栈的方法，提出ISC栈，用其作为性能预测模型输入，推出SYNPA系列T2C分配策略。

Result: SYNPA4在周转时间上比Linux高38%，是ARM处理器现有策略收益的3倍。

Conclusion: 提出的方法可帮助性能分析师在真实处理器中构建准确性能估计的性能栈，适用于不同厂商的SMT处理器。

Abstract: Modern high-performance servers commonly integrate Simultaneous
Multithreading (SMT) processors, which efficiently boosts throughput over
single-threaded cores. Optimizing performance in SMT processors faces
challenges due to the inter-application interference within each SMT core. To
mitigate the interference, thread-to-core (T2C) allocation policies play a
pivotal role. State-of-the-art T2C policies work in two steps: i) building a
per-application performance stack using performance counters and ii) building
performance prediction models to identify the best pairs of applications to run
on each core.
  This paper explores distinct ways to build the performance stack in ARM
processors and introduces the Instructions and Stalls Cycles (ISC) stack, a
novel approach to overcome ARM PMU limitations. The ISC stacks are used as
inputs for a performance prediction model to estimate the applications'
performance considering the inter-application interference. The accuracy of the
prediction model (second step) depends on the accuracy of the performance stack
(first step); thus, the higher the accuracy of the performance stack, the
higher the potential performance gains obtained by the T2C allocation policy.
  This paper presents SYNPA as a family of T2C allocation policies.
Experimental results show that $SYNPA4$, the best-performing SYNPA variant,
outperforms turnaround time by 38\% over Linux, which represents 3$\times$ the
gains achieved by the state-of-the-art policies for ARM processors.
Furthermore, the multiple discussions and refinements presented throughout this
paper can be applied to other SMT processors from distinct vendors and are
aimed at helping performance analysts build performance stacks for accurate
performance estimates in real processors.

</details>


### [38] [How Fast Can Graph Computations Go on Fine-grained Parallel Architectures](https://arxiv.org/abs/2507.00949)
*Yuqing Wang,Charles Colley,Brian Wheatman,Jiya Su,David F. Gleich,Andrew A. Chien*

Main category: cs.DC

TL;DR: 探讨细粒度架构上的图计算速度，评估UpDown架构，模拟显示其性能超先前结果。


<details>
  <summary>Details</summary>
Motivation: 大规模图问题重要但并行架构支持不足，探索细粒度架构上的图计算速度。

Method: 用PageRank和Breadth - First Search两个图基准测试评估UpDown架构，编写算法的五个变体。

Result: 对最多256个节点模拟及对16384个节点预测显示，UpDown系统在RMAT上PR达637K GTEPS，BFS达989K GTEPS，分别超先前最佳结果5倍和100倍。

Conclusion: 细粒度架构的UpDown系统在图计算性能上表现出色。

Abstract: Large-scale graph problems are of critical and growing importance and
historically parallel architectures have provided little support. In the spirit
of co-design, we explore the question, How fast can graph computing go on a
fine-grained architecture? We explore the possibilities of an architecture
optimized for fine-grained parallelism, natural programming, and the
irregularity and skew found in real-world graphs. Using two graph benchmarks,
PageRank (PR) and Breadth-First Search (BFS), we evaluate a Fine-Grained Graph
architecture, UpDown, to explore what performance codesign can achieve. To
demonstrate programmability, we wrote five variants of these algorithms.
Simulations of up to 256 nodes (524,288 lanes) and projections to 16,384 nodes
(33M lanes) show the UpDown system can achieve 637K GTEPS PR and 989K GTEPS BFS
on RMAT, exceeding the best prior results by 5x and 100x respectively.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [39] [A Simple Algorithm for Trimmed Multipoint Evaluation](https://arxiv.org/abs/2507.00196)
*Nick Fischer,Melvin Kallmayer,Leo Wennmann*

Main category: cs.DS

TL;DR: 本文提出简单递归算法解决修剪多点求值问题


<details>
  <summary>Details</summary>
Motivation: 现有修剪多点求值算法复杂，涉及大量计算机代数工具，需要简单易理解算法

Method: 提出简单递归算法，避免使用复杂计算机代数工具

Result: 得到可解决修剪多点求值问题的算法

Conclusion: 提出的算法简单，无专业背景研究者也易理解

Abstract: Evaluating a polynomial on a set of points is a fundamental task in computer
algebra. In this work, we revisit a particular variant called trimmed
multipoint evaluation: given an $n$-variate polynomial with bounded individual
degree $d$ and total degree $D$, the goal is to evaluate it on a natural class
of input points. This problem arises as a key subroutine in recent algorithmic
results [Dinur; SODA '21], [Dell, Haak, Kallmayer, Wennmann; SODA '25]. It is
known that trimmed multipoint evaluation can be solved in near-linear time [van
der Hoeven, Schost; AAECC '13] by a clever yet somewhat involved algorithm. We
give a simple recursive algorithm that avoids heavy computer-algebraic
machinery, and can be readily understood by researchers without specialized
background.

</details>


### [40] [Lazy B-Trees](https://arxiv.org/abs/2507.00277)
*Casper Moldrup Rysgaard,Sebastian Wild*

Main category: cs.DS

TL;DR: 本文设计了适用于外部内存的惰性B树，它是惰性搜索树的变体，能实现输入/输出操作的平滑插值，在作为外部内存优先队列时部分操作比已知数据结构更快。


<details>
  <summary>Details</summary>
Motivation: 将B树相对于二叉搜索树在输入/输出操作上的加速优势推广到平滑插值机制，解决缺乏外部变体的有偏搜索树这一技术难题。

Method: 给出能实现外部内存惰性搜索树的部分性能保证的构造。

Result: 设计出惰性B树，在作为外部内存优先队列时，其decrease - key和insert操作比已知数据结构更快。

Conclusion: 惰性B树是适用于外部内存的有效数据结构，在特定场景有性能优势。

Abstract: Lazy search trees (Sandlund & Wild FOCS 2020, Sandlund & Zhang SODA 2022) are
sorted dictionaries whose update and query performance smoothly interpolates
between that of efficient priority queues and binary search trees -
automatically, depending on actual use; no adjustments are necessary to the
data structure to realize the cost savings. In this paper, we design lazy
B-trees, a variant of lazy search trees suitable for external memory that
generalizes the speedup of B-trees over binary search trees wrt. input/output
operations to the same smooth interpolation regime.
  A key technical difficulty to overcome is the lack of a (fully satisfactory)
external variant of biased search trees, on which lazy search trees crucially
rely. We give a construction for a subset of performance guarantees sufficient
to realize external-memory lazy search trees, which we deem of independent
interest.
  As one special case, lazy B-trees can be used as an external-memory priority
queue, in which case they are competitive with some tailor-made heaps; indeed,
they offer faster decrease-key and insert operations than known data
structures.

</details>


### [41] [On the (In)Approximability of the Monitoring Edge Geodetic Set Problem](https://arxiv.org/abs/2507.00708)
*Davide Bilò,Giodano Colli,Luca Forlizzi,Stefano Leucci*

Main category: cs.DS

TL;DR: 本文研究最小监测边测地集问题，证明其近似算法有逼近比下界，加强了NP难结果，设计了特定图类的近似算法。


<details>
  <summary>Details</summary>
Motivation: 研究最小监测边测地集问题的近似难度和设计高效近似算法。

Method: 理论证明近似比下界，针对不同图类设计算法。

Result: 证明近似比为Ω(log n)，加强NP难结果，给出特定图类近似算法及逼近比。

Conclusion: 该问题有一定近似难度，在特定图类有较好近似算法。

Abstract: We study the minimum \emph{Monitoring Edge Geodetic Set} (\megset) problem
introduced in [Foucaud et al., CALDAM'23]: given a graph $G$, we say that an
edge is monitored by a pair $u,v$ of vertices if \emph{all} shortest paths
between $u$ and $v$ traverse $e$; the goal of the problem consists in finding a
subset $M$ of vertices of $G$ such that each edge of $G$ is monitored by at
least one pair of vertices in $M$, and $|M|$ is minimized.
  In this paper, we prove that all polynomial-time approximation algorithms for
the minimum \megset problem must have an approximation ratio of $\Omega(\log
n)$, unless \p = \np. To the best of our knowledge, this is the first
non-constant inapproximability result known for this problem. We also
strengthen the known \np-hardness of the problem on $2$-apex graphs by showing
that the same result holds for $1$-apex graphs. This leaves open the problem of
determining whether the problem remains \np-hard on planar (i.e., $0$-apex)
graphs.
  On the positive side, we design an algorithm that computes good approximate
solutions for hereditary graph classes that admit efficiently computable
balanced separators of truly sublinear size. This immediately results in
polynomial-time approximation algorithms achieving an approximation ratio of
$O(n^{\frac{1}{4}} \sqrt{\log n})$ on planar graphs, graphs with bounded genus,
and $k$-apex graphs with $k=O(n^{\frac{1}{4}})$. On graphs with bounded
treewidth, we obtain an approximation ratio of $O(\log^{3/2} n)$ for any
constant $\varepsilon > 0$. This compares favorably with the best-known
approximation algorithm for general graphs, which achieves an approximation
ratio of $O(\sqrt{n \log n})$ via a simple reduction to the \textsc{Set Cover}
problem.

</details>


### [42] [Inverse matroid optimization under subset constraints](https://arxiv.org/abs/2507.00930)
*Kristóf íBérczi,Lydia Mirabel Mendoza-Cadena,José Soto*

Main category: cs.DS

TL;DR: 本文扩展逆拟阵问题，研究六种变体并在ℓ∞ - 范数下为所有变体开发组合多项式时间算法。


<details>
  <summary>Details</summary>
Motivation: 将经典逆拟阵问题中的固定基替换为子集并施加结构约束，以拓宽可高效解决的逆优化问题范围。

Method: 提出ℓ∞ - 范数下逆拟阵的精化最小 - 最大定理。

Result: 为所有变体开发组合多项式时间算法，算法比先前方法更简单快速。

Conclusion: 工作显著拓宽了拟阵上可高效解决的逆优化问题范围。

Abstract: In the Inverse Matroid problem, we are given a matroid, a fixed basis $B$,
and an initial weight function, and the goal is to minimally modify the weights
-- measured by some function -- so that $B$ becomes a maximum-weight basis. The
problem arises naturally in settings where one wishes to explain or enforce a
given solution by minimally perturbing the input.
  We extend this classical problem by replacing the fixed basis with a subset
$S_0$ of the ground set and imposing various structural constraints on the set
of maximum-weight bases relative to $S_0$. Specifically, we study six variants:
(A) Inverse Matroid Exists, where $S_0$ must contain at least one
maximum-weight basis; (B) Inverse Matroid All, where all bases contained in
$S_0$ are maximum-weight; and (C) Inverse Matroid Only, where $S_0$ contains
exactly the maximum-weight bases, along with their natural negated
counterparts.
  For all variants, we develop combinatorial polynomial-time algorithms under
the $\ell_\infty$-norm. A key ingredient is a refined min-max theorem for
Inverse Matroid under the $\ell_\infty$-norm, which enables simpler and faster
algorithms than previous approaches and may be of independent combinatorial
interest. Our work significantly broadens the range of inverse optimization
problems on matroids that can be solved efficiently, especially those that
constrain the structure of optimal solutions through subset inclusion or
exclusion.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [43] [Horus: A Protocol for Trustless Delegation Under Uncertainty](https://arxiv.org/abs/2507.00631)
*David Shi,Kevin Joo*

Main category: cs.GT

TL;DR: 提出通过抵押索赔在递归验证游戏中执行正确性的协议，使正确性成为纳什均衡。


<details>
  <summary>Details</summary>
Motivation: 在动态低信任环境中，自主AI代理委托工作给子代理时，无法通过预先规范或集中监督确保正确性。

Method: 提出通过抵押索赔在递归验证游戏中执行正确性的协议，任务作为意图发布，求解者竞争完成，验证者事后检查，挑战者可挑战结果触发验证。

Result: 当求解者、挑战者和验证者激励一致时，伪造条件使正确性成为纳什均衡。

Conclusion: 所提出的协议能在动态低信任环境中保证AI代理工作的正确性。

Abstract: Correctness is an emergent property of systems where exposing error is
cheaper than committing it. In dynamic, low-trust environments, autonomous AI
agents benefit from delegating work to sub-agents, yet correctness cannot be
assured through upfront specification or centralized oversight. We propose a
protocol that enforces correctness through collateralized claims in a recursive
verification game. Tasks are published as intents, and solvers compete to
fulfill them. Selected solvers carry out tasks under risk, with correctness
checked post hoc by verifiers. Any challenger can challenge a result by staking
against it to trigger the verification process. Incorrect agents are slashed
and correct opposition is rewarded, with an escalation path that penalizes
erroneous verifiers themselves. When incentives are aligned across solvers,
challengers, and verifiers, falsification conditions make correctness the Nash
equilibrium.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [44] [Read the Docs Before Rewriting: Equip Rewriter with Domain Knowledge via Continual Pre-training](https://arxiv.org/abs/2507.00477)
*Qi Wang,Yixuan Cao,Yifan Liu,Jiangtao Zhao,Ping Luo*

Main category: cs.IR

TL;DR: 提出R&R重写器解决专业领域RAG问答系统查询重写难题，多数据集实验显示其在多领域专业问答中表现出色。


<details>
  <summary>Details</summary>
Motivation: 专业领域查询重写模型因领域知识有限而存在困难，需解决这一问题以提升RAG问答系统在专业领域的应用。

Method: 提出R&R重写器，进行专业文档的持续预训练，可结合有监督微调。

Result: 在多数据集实验中，R&R在多领域专业问答中表现卓越，能有效弥合查询与文档差距，在一般场景也表现良好。

Conclusion: R&R重写器推动了基于RAG的问答系统在专业领域的应用。

Abstract: A Retrieval-Augmented Generation (RAG)-based question-answering (QA) system
enhances a large language model's knowledge by retrieving relevant documents
based on user queries. Discrepancies between user queries and document
phrasings often necessitate query rewriting. However, in specialized domains,
the rewriter model may struggle due to limited domain-specific knowledge. To
resolve this, we propose the R\&R (Read the doc before Rewriting) rewriter,
which involves continual pre-training on professional documents, akin to how
students prepare for open-book exams by reviewing textbooks. Additionally, it
can be combined with supervised fine-tuning for improved results. Experiments
on multiple datasets demonstrate that R\&R excels in professional QA across
multiple domains, effectively bridging the query-document gap, while
maintaining good performance in general scenarios, thus advancing the
application of RAG-based QA systems in specialized fields.

</details>


### [45] [On Mitigating Data Sparsity in Conversational Recommender Systems](https://arxiv.org/abs/2507.00479)
*Sixiao Zhang,Mingrui Liu,Cheng Long,Wei Yuan,Hongxu Chen,Xiangyu Zhao,Hongzhi Yin*

Main category: cs.IR

TL;DR: 针对对话推荐系统的数据稀疏问题，提出DACRS模型，经实验验证其性能达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有对话推荐系统存在数据稀疏问题，且难以泛化对话表达和学习信息丰富的物品表示。

Method: 提出DACRS模型，包含对话增强、知识引导实体建模、对话 - 实体匹配三个模块。

Result: 在两个公开数据集上的大量实验表明DACRS达到了最先进的性能。

Conclusion: DACRS模型有效解决了对话推荐系统的数据稀疏问题，提升了性能。

Abstract: Conversational recommender systems (CRSs) capture user preference through
textual information in dialogues. However, they suffer from data sparsity on
two fronts: the dialogue space is vast and linguistically diverse, while the
item space exhibits long-tail and sparse distributions. Existing methods
struggle with (1) generalizing to varied dialogue expressions due to
underutilization of rich textual cues, and (2) learning informative item
representations under severe sparsity. To address these problems, we propose a
CRS model named DACRS. It consists of three modules, namely Dialogue
Augmentation, Knowledge-Guided Entity Modeling, and Dialogue-Entity Matching.
In the Dialogue Augmentation module, we apply a two-stage augmentation pipeline
to augment the dialogue context to enrich the data and improve
generalizability. In the Knowledge-Guided Entity Modeling, we propose a
knowledge graph (KG) based entity substitution and an entity similarity
constraint to enhance the expressiveness of entity embeddings. In the
Dialogue-Entity Matching module, we fuse the dialogue embedding with the
mentioned entity embeddings through a dialogue-guided attention aggregation to
acquire user embeddings that contain both the explicit and implicit user
preferences. Extensive experiments on two public datasets demonstrate the
state-of-the-art performance of DACRS.

</details>


### [46] [MassTool: A Multi-Task Search-Based Tool Retrieval Framework for Large Language Models](https://arxiv.org/abs/2507.00487)
*Jianghao Lin,Xinyuan Wang,Xinyi Dai,Menghui Zhu,Bo Chen,Ruiming Tang,Yong Yu,Weinan Zhang*

Main category: cs.IR

TL;DR: 提出MassTool框架提升大语言模型工具检索准确性，实验证明有效且代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有工具检索方法重工具表征轻查询理解，需改进。

Method: 采用双塔架构，含工具使用检测塔和工具检索塔，结合搜索式用户意图建模与自适应知识转移模块，联合优化多种损失。

Result: 大量实验表明能提高检索准确性。

Conclusion: MassTool框架有效且能精准理解查询，提升工具检索效果。

Abstract: Tool retrieval is a critical component in enabling large language models
(LLMs) to interact effectively with external tools. It aims to precisely filter
the massive tools into a small set of candidates for the downstream
tool-augmented LLMs. However, most existing approaches primarily focus on
optimizing tool representations, often neglecting the importance of precise
query comprehension. To address this gap, we introduce MassTool, a multi-task
search-based framework designed to enhance both query representation and tool
retrieval accuracy. MassTool employs a two-tower architecture: a tool usage
detection tower that predicts the need for function calls, and a tool retrieval
tower that leverages a query-centric graph convolution network (QC-GCN) for
effective query-tool matching. It also incorporates search-based user intent
modeling (SUIM) to handle diverse and out-of-distribution queries, alongside an
adaptive knowledge transfer (AdaKT) module for efficient multi-task learning.
By jointly optimizing tool usage detection loss, list-wise retrieval loss, and
contrastive regularization loss, MassTool establishes a robust dual-step
sequential decision-making pipeline for precise query understanding. Extensive
experiments demonstrate its effectiveness in improving retrieval accuracy. Our
code is available at https://github.com/wxydada/MassTool.

</details>


### [47] [\texttt{WebANNS}: Fast and Efficient Approximate Nearest Neighbor Search in Web Browsers](https://arxiv.org/abs/2507.00521)
*Mugeng Liu,Siqi Zhong,Qi Yang,Yudong Han,Xuanzhe Liu,Yun Ma*

Main category: cs.IR

TL;DR: 提出专为浏览器设计的WebANNS近似最近邻搜索（ANNS）引擎，通过多种策略提升性能，实验显示速度快且内存高效。


<details>
  <summary>Details</summary>
Motivation: 现有最先进的ANNS解决方案无法全面解决浏览器中ANNS面临的计算限制、外部存储访问和内存利用约束等独特挑战。

Method: 提出WebANNS引擎，利用WebAssembly克服计算瓶颈，设计懒加载策略优化外部存储数据检索，采用启发式方法减少内存使用。

Result: WebANNS速度快且内存高效，在第99百分位查询延迟上比现有最先进引擎提升达743.8倍，内存使用最多减少39%，将浏览器中查询时间从10秒降至10毫秒级别。

Conclusion: WebANNS使浏览器内的ANNS在用户可接受的延迟下成为现实。

Abstract: Approximate nearest neighbor search (ANNS) has become vital to modern AI
infrastructure, particularly in retrieval-augmented generation (RAG)
applications. Numerous in-browser ANNS engines have emerged to seamlessly
integrate with popular LLM-based web applications, while addressing privacy
protection and challenges of heterogeneous device deployments. However, web
browsers present unique challenges for ANNS, including computational
limitations, external storage access issues, and memory utilization
constraints, which state-of-the-art (SOTA) solutions fail to address
comprehensively.
  We propose \texttt{WebANNS}, a novel ANNS engine specifically designed for
web browsers. \texttt{WebANNS} leverages WebAssembly to overcome computational
bottlenecks, designs a lazy loading strategy to optimize data retrieval from
external storage, and applies a heuristic approach to reduce memory usage.
Experiments show that \texttt{WebANNS} is fast and memory efficient, achieving
up to $743.8\times$ improvement in 99th percentile query latency over the SOTA
engine, while reducing memory usage by up to 39\%. Note that \texttt{WebANNS}
decreases query time from 10 seconds to the 10-millisecond range in browsers,
making in-browser ANNS practical with user-acceptable latency.

</details>


### [48] [Rethinking Group Recommender Systems in the Era of Generative AI: From One-Shot Recommendations to Agentic Group Decision Support](https://arxiv.org/abs/2507.00535)
*Dietmar Jannach,Amra Delić,Francesco Ricci,Markus Zanker*

Main category: cs.IR

TL;DR: 过去二十五年多来有很多群体推荐系统算法提议，但现实应用少。文章认为常见假设和系统设计可能不符用户需求，呼吁利用生成式AI重新定位研究方向，设想让AI代理辅助群体决策。


<details>
  <summary>Details</summary>
Motivation: 现有群体推荐系统研究丰富但现实应用少，质疑学术研究常见假设。

Method: 分析现有研究与实际情况的差距，提出利用现代生成式AI助手重新定位研究方向。

Result: 提出群体推荐系统未来可让人类成员在聊天中，由AI代理辅助决策。

Conclusion: 这种新方式将营造更自然的群体决策环境，促进群体推荐系统的实际应用。

Abstract: More than twenty-five years ago, first ideas were developed on how to design
a system that can provide recommendations to groups of users instead of
individual users. Since then, a rich variety of algorithmic proposals were
published, e.g., on how to acquire individual preferences, how to aggregate
them, and how to generate recommendations for groups of users. However, despite
the rich literature on the topic, barely any examples of real-world group
recommender systems can be found. This lets us question common assumptions in
academic research, in particular regarding communication processes in a group
and how recommendation-supported decisions are made. In this essay, we argue
that these common assumptions and corresponding system designs often may not
match the needs or expectations of users. We thus call for a reorientation in
this research area, leveraging the capabilities of modern Generative AI
assistants like ChatGPT. Specifically, as one promising future direction, we
envision group recommender systems to be systems where human group members
interact in a chat and an AI-based group recommendation agent assists the
decision-making process in an agentic way. Ultimately, this shall lead to a
more natural group decision-making environment and finally to wider adoption of
group recommendation systems in practice.

</details>


### [49] [Reliable Annotations with Less Effort: Evaluating LLM-Human Collaboration in Search Clarifications](https://arxiv.org/abs/2507.00543)
*Leila Tavakoli,Hamed Zamani*

Main category: cs.IR

TL;DR: 研究聚焦搜索澄清任务标注，发现大语言模型在复杂标注任务表现不佳，提出人在回路工作流，提升标注可靠性并减少人力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在复杂、细致和多维度标注任务中的有效性研究不足，探索其在搜索澄清任务标注中的表现。

Method: 对大语言模型进行系统评估，提出使用置信阈值和模型间分歧的人在回路工作流。

Result: 大语言模型预测常不一致、校准不佳且对提示变化敏感；人在回路工作流显著提升标注可靠性，减少人力达45%。

Conclusion: 人在回路工作流为大语言模型在现实评估场景部署提供可扩展、经济且准确的途径。

Abstract: Despite growing interest in using large language models (LLMs) to automate
annotation, their effectiveness in complex, nuanced, and multi-dimensional
labelling tasks remains relatively underexplored. This study focuses on
annotation for the search clarification task, leveraging a high-quality,
multi-dimensional dataset that includes five distinct fine-grained annotation
subtasks. Although LLMs have shown impressive capabilities in general settings,
our study reveals that even state-of-the-art models struggle to replicate
human-level performance in subjective or fine-grained evaluation tasks. Through
a systematic assessment, we demonstrate that LLM predictions are often
inconsistent, poorly calibrated, and highly sensitive to prompt variations. To
address these limitations, we propose a simple yet effective human-in-the-loop
(HITL) workflow that uses confidence thresholds and inter-model disagreement to
selectively involve human review. Our findings show that this lightweight
intervention significantly improves annotation reliability while reducing human
effort by up to 45%, offering a relatively scalable and cost-effective yet
accurate path forward for deploying LLMs in real-world evaluation settings.

</details>


### [50] [EARN: Efficient Inference Acceleration for LLM-based Generative Recommendation by Register Tokens](https://arxiv.org/abs/2507.00715)
*Chaoqun Yang,Xinyu Lin,Wenjie Wang,Yongqi Li,Teng Sun,Xianjing Han,Tat-Seng Chua*

Main category: cs.IR

TL;DR: 现有LLMRec推理延迟高，提出EARN框架，实验证明其有加速、减少KV缓存和提高准确率效果。


<details>
  <summary>Details</summary>
Motivation: 解决LLMRec因计算开销和KV Cache内存压力导致的高推理延迟问题，且现有KV Cache减少方法有局限。

Method: 通过分析LLMRec注意力模式，提出EARN框架，利用早期层将信息压缩到输入序列边界的寄存器令牌，后续层仅关注这些令牌。

Result: 在三个数据集、两种LLMRec方法和两种LLM架构上实验，EARN实现高达3.79倍加速和80.8%的KV Cache减少，且准确率优于通用微调方法。

Conclusion: EARN弥合了LLMRec效率与效果的差距，在工业场景有实际部署优势。

Abstract: Large Language Model-based generative recommendation (LLMRec) has achieved
notable success, but it suffers from high inference latency due to massive
computational overhead and memory pressure of KV Cache. Existing KV Cache
reduction methods face critical limitations: cache compression offers marginal
acceleration given recommendation tasks' short decoding steps, while prompt
compression risks discarding vital interaction history. Through systematic
analysis of attention patterns in LLMRec, we uncover two pivotal insights: 1)
layer-wise attention sparsity inversion where early layers retain dense
informative patterns while later layers exhibit high redundancy, and 2) dual
attention sinks phenomenon where attention scores concentrate on both head and
tail tokens of input sequences. Motivated by these insights, we propose EARN,
an efficient inference framework that leverages the early layers to compress
information into register tokens placed at the input sequence boundaries, then
focuses solely on these tokens in the subsequent layers. Extensive experiments
on three datasets, two LLMRec methods and two LLM architectures demonstrate
EARN's superiority, achieving up to 3.79x speedup and 80.8% KV Cache reduction
with better accuracy than the general finetuning approach. Our work bridges the
efficiency-effectiveness gap in LLMRec, offering practical deployment
advantages for industrial scenarios.

</details>


### [51] [WebArXiv: Evaluating Multimodal Agents on Time-Invariant arXiv Tasks](https://arxiv.org/abs/2507.00938)
*Zihao Sun,Meng Fang,Ling Chen*

Main category: cs.IR

TL;DR: 介绍WebArXiv基准用于评估自主网页代理，指出代理常见失败模式并提出改进策略，实验验证策略有效性。


<details>
  <summary>Details</summary>
Motivation: 现有基准不稳定、不一致，评估自主网页代理有挑战，需可靠评估方法。

Method: 引入WebArXiv基准，通过行为分析识别失败模式，提出轻量级动态反思机制。

Result: 在WebArXiv上评估十个先进网页代理，显示代理性能有差异，验证反思策略有效性。

Conclusion: WebArXiv可实现可重复可靠评估，提出的反思策略有效。

Abstract: Recent progress in large language models (LLMs) has enabled the development
of autonomous web agents capable of navigating and interacting with real
websites. However, evaluating such agents remains challenging due to the
instability and inconsistency of existing benchmarks, which often rely on
dynamic content or oversimplified simulations. In this work, we introduce
WebArXiv, a static and time-invariant benchmark comprising 275 web-based tasks
grounded in the arXiv platform. WebArXiv ensures reproducible and reliable
evaluation by anchoring tasks in fixed web snapshots with deterministic ground
truths and standardized action trajectories. Through behavioral analysis, we
identify a common failure mode, Rigid History Reflection, where agents
over-rely on fixed interaction histories. To address this, we propose a
lightweight dynamic reflection mechanism that allows agents to selectively
retrieve relevant past steps during decision-making. We evaluate ten
state-of-the-art web agents on WebArXiv. Results demonstrate clear performance
differences across agents and validate the effectiveness of our proposed
reflection strategy.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [52] [A Theory of Inference Compute Scaling: Reasoning through Directed Stochastic Skill Search](https://arxiv.org/abs/2507.00004)
*Austin R. Ellis-Mohr,Anuj K. Nayak,Lav R. Varshney*

Main category: cs.LG

TL;DR: 提出DS3框架优化大语言模型推理成本，理论复现实证模式，加深对训练-推理关系理解。


<details>
  <summary>Details</summary>
Motivation: 大语言模型训练和部署资源需求大，现有计算最优性刻画可能忽略更高效运行点，推理成本负担日益显著。

Method: 引入DS3框架，将推理表示为技能图的随机遍历，推导不同推理策略的任务成功率和计算成本表达式，扩展图框架并结合实证方法。

Result: 理论上复现了实证观察到的模式，如准确率与计算量的对数线性关系等。

Conclusion: 该框架加深了理论理解，支持算法设计和资源分配。

Abstract: Large language models (LLMs) demand considerable computational, energy, and
financial resources during both training and deployment. While scaling laws for
training have guided much of the field's recent progress, inference costs now
represent a significant and growing component of the overall resource burden,
particularly for reasoning-focused models. Existing characterizations of
compute-optimality that consider model size, dataset size, and inference tokens
in isolation or in fixed combinations risk overlooking more efficient operating
points. We introduce directed stochastic skill search (DS3), a general
framework that represents inference as stochastic traversal over a learned
skill graph. From a simplified yet expressive instantiation, we derive
closed-form expressions for task success and compute cost across a wide range
of inference strategies -- including chain-of-thought (CoT) and tree-of-thought
(ToT) -- enabling comparative analysis as a function of task difficulty and
model capability. To that end, we extend a prior first-principles tripartite
graph framework of LLM training to incorporate inference, and separately bridge
DS3 with empirical methods that characterize LLM scaling behavior. We
theoretically recover empirically observed patterns, including: linear accuracy
scaling with logarithmic compute; variation in preferred inference strategies
as a function of task difficulty and model capability; emergent behavior
elicited by reasoning even when performance plateaus under parameter scaling;
and both best-of-N (BoN) and majority voting behavior captured within a unified
analytical framework. By explicitly characterizing training-inference
interdependencies, our framework deepens theoretical understanding and supports
principled algorithmic design and resource allocation.

</details>


### [53] [Hypertokens: Holographic Associative Memory in Tokenized LLMs](https://arxiv.org/abs/2507.00002)
*Christopher James Augeri*

Main category: cs.LG

TL;DR: 论文指出大语言模型存在信息传播导致的精度损失问题，引入HDRAM框架解决K:V和V:K内存问题，显著提升关联检索能力。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型存在的精度损失问题，将其视为信息论通信问题，解决K:V和V:K内存问题。

Method: 引入HDRAM框架，将变压器潜在空间视为扩频信道，基于超令牌构建，结合经典纠错码、全息计算和量子启发搜索，通过解扩恢复分布式信息。

Result: 通过结合ECC语法、压缩感知和Krylov子空间对齐，显著改善了关联检索。

Conclusion: 经典 - 全息 - 量子启发（CHQ）原则可强化变压器架构。

Abstract: Large Language Models (LLMs) exhibit remarkable capabilities but suffer from
apparent precision loss, reframed here as information spreading. This reframing
shifts the problem from computational precision to an information-theoretic
communication issue. We address the K:V and V:K memory problem in LLMs by
introducing HDRAM (Holographically Defined Random Access Memory), a symbolic
memory framework treating transformer latent space as a spread-spectrum
channel. Built upon hypertokens, structured symbolic codes integrating
classical error-correcting codes (ECC), holographic computing, and
quantum-inspired search, HDRAM recovers distributed information through
principled despreading. These phase-coherent memory addresses enable efficient
key-value operations and Grover-style search in latent space. By combining ECC
grammar with compressed sensing and Krylov subspace alignment, HDRAM
significantly improves associative retrieval without architectural changes,
demonstrating how Classical-Holographic-Quantum-inspired (CHQ) principles can
fortify transformer architectures.

</details>


### [54] [Deciding When Not to Decide: Indeterminacy-Aware Intrusion Detection with NeutroSENSE](https://arxiv.org/abs/2507.00003)
*Eyhab Al-Masri*

Main category: cs.LG

TL;DR: 本文提出NeutroSENSE框架用于物联网环境入侵检测，集成多种算法与中性逻辑，评估显示准确率达97%，表明中性逻辑可提升准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 为物联网环境提供可解释的入侵检测系统，解决不确定性量化和决策问题。

Method: 集成随机森林、XGBoost和逻辑回归，结合中性逻辑，将预测置信度分解为T、F、I分量，用阈值标记高不确定性预测。

Result: 在IoT - CAD数据集上准确率达97%，误分类样本的不确定性显著高于正确样本，验证了I分数与错误可能性的相关性。

Conclusion: 中性逻辑能增强物联网安全系统中AI的准确性和可解释性，为信任感知AI提供基础。

Abstract: This paper presents NeutroSENSE, a neutrosophic-enhanced ensemble framework
for interpretable intrusion detection in IoT environments. By integrating
Random Forest, XGBoost, and Logistic Regression with neutrosophic logic, the
system decomposes prediction confidence into truth (T), falsity (F), and
indeterminacy (I) components, enabling uncertainty quantification and
abstention. Predictions with high indeterminacy are flagged for review using
both global and adaptive, class-specific thresholds. Evaluated on the IoT-CAD
dataset, NeutroSENSE achieved 97% accuracy, while demonstrating that
misclassified samples exhibit significantly higher indeterminacy (I = 0.62)
than correct ones (I = 0.24). The use of indeterminacy as a proxy for
uncertainty enables informed abstention and targeted review-particularly
valuable in edge deployments. Figures and tables validate the correlation
between I-scores and error likelihood, supporting more trustworthy,
human-in-the-loop AI decisions. This work shows that neutrosophic logic
enhances both accuracy and explainability, providing a practical foundation for
trust-aware AI in edge and fog-based IoT security systems.

</details>


### [55] [Data Collection with Non-Uniform Axial Power for Phase II of the OECD/NEA AI/ML Critical Heat Flux Benchmark](https://arxiv.org/abs/2507.00034)
*Reece Bourisaw,Reid McCants,Jean-Marie Le Corre,Anna Iskhakova,Arsen S. Iskhakov*

Main category: cs.LG

TL;DR: 为支持OECD/NEA AI/ML CHF基准测试第二阶段，本文整理数字化CHF数据集，分析不同方法预测表现，为后续研究奠基。


<details>
  <summary>Details</summary>
Motivation: 支持OECD/NEA AI/ML CHF基准测试第二阶段，该阶段引入空间变化功率分布。

Method: 编译和数字化涵盖均匀与非均匀轴向加热条件的CHF数据集，提取加热曲线、插值、验证并编码。

Result: 经典CHF关联式在非均匀分布下误差大，现代表格法有改进但仍不完美，仅用均匀数据训练的神经网络无法推广到空间变化场景。

Conclusion: 本研究为CHF基准测试下一阶段的高级迁移学习、不确定性量化和设计优化奠定基础。

Abstract: Critical heat flux (CHF) marks the onset of boiling crisis in light-water
reactors, defining safe thermal-hydraulic operating limits. To support Phase II
of the OECD/NEA AI/ML CHF benchmark, which introduces spatially varying power
profiles, this work compiles and digitizes a broad CHF dataset covering both
uniform and non-uniform axial heating conditions. Heating profiles were
extracted from technical reports, interpolated onto a consistent axial mesh,
validated via energy-balance checks, and encoded in machine-readable formats
for benchmark compatibility.
  Classical CHF correlations exhibit substantial errors under uniform heating
and degrade markedly when applied to non-uniform profiles, while modern tabular
methods offer improved but still imperfect predictions. A neural network
trained solely on uniform data performs well in that regime but fails to
generalize to spatially varying scenarios, underscoring the need for models
that explicitly incorporate axial power distributions. By providing these
curated datasets and baseline modeling results, this study lays the groundwork
for advanced transfer-learning strategies, rigorous uncertainty quantification,
and design-optimization efforts in the next phase of the CHF benchmark.

</details>


### [56] [Novel RL approach for efficient Elevator Group Control Systems](https://arxiv.org/abs/2507.00011)
*Nathan Vaartjes,Vincent Francois-Lavet*

Main category: cs.LG

TL;DR: 为高效管理大型建筑电梯交通，将阿姆斯特丹自由大学的电梯系统建模为马尔可夫决策过程，训练端到端强化学习电梯群控系统，该系统表现优于传统规则算法。


<details>
  <summary>Details</summary>
Motivation: 大型建筑中传统基于启发式或模式检测的电梯控制器难以应对调度的随机性和组合性，需提高电梯交通管理效率，减少乘客出行时间和能耗。

Method: 将六电梯十五层系统建模为马尔可夫决策过程，采用端到端强化学习，有新的动作空间编码、引入子步骤、定制奖励信号，探索调整折扣因子，使用基于Dueling Double Deep Q - learning的强化学习架构。

Result: 所提出的基于强化学习的电梯群控系统能适应波动的交通模式，从高度随机的环境中学习。

Conclusion: 基于强化学习的电梯群控系统优于传统的基于规则的算法。

Abstract: Efficient elevator traffic management in large buildings is critical for
minimizing passenger travel times and energy consumption. Because heuristic- or
pattern-detection-based controllers struggle with the stochastic and
combinatorial nature of dispatching, we model the six-elevator, fifteen-floor
system at Vrije Universiteit Amsterdam as a Markov Decision Process and train
an end-to-end Reinforcement Learning (RL) Elevator Group Control System (EGCS).
Key innovations include a novel action space encoding to handle the
combinatorial complexity of elevator dispatching, the introduction of
infra-steps to model continuous passenger arrivals, and a tailored reward
signal to improve learning efficiency. In addition, we explore various ways to
adapt the discounting factor to the infra-step formulation. We investigate RL
architectures based on Dueling Double Deep Q-learning, showing that the
proposed RL-based EGCS adapts to fluctuating traffic patterns, learns from a
highly stochastic environment, and thereby outperforms a traditional rule-based
algorithm.

</details>


### [57] [Towards Undistillable Models by Minimizing Conditional Mutual Information](https://arxiv.org/abs/2507.00012)
*Linfeng Ye,Shayan Mohajer Hamidi,En-hui Yang*

Main category: cs.LG

TL;DR: 本文研究不可蒸馏DNN，提出CMIM训练方法，实验表明该模型不可被蒸馏且预测精度更高。


<details>
  <summary>Details</summary>
Motivation: 为保护DNN知识产权，构建不可蒸馏的DNN。

Method: 观察到不可蒸馏DNN输出概率分布簇的特性，通过条件互信息（CMI）衡量簇的集中度，提出CMI最小化（CMIM）训练方法，联合最小化传统交叉熵（CE）损失和全温度谱下所有温度缩放簇的CMI值。

Result: 大量实验表明，CMIM模型无法被现有KD方法蒸馏，蒸馏出的模型表现不如LS学生，且CMIM模型预测精度优于仅用CE损失训练的模型。

Conclusion: CMIM方法能有效构建不可蒸馏的DNN，同时提升模型预测精度。

Abstract: A deep neural network (DNN) is said to be undistillable if, when used as a
black-box input-output teacher, it cannot be distilled through knowledge
distillation (KD). In this case, the distilled student (referred to as the
knockoff student) does not outperform a student trained independently with
label smoothing (LS student) in terms of prediction accuracy. To protect
intellectual property of DNNs, it is desirable to build undistillable DNNs. To
this end, it is first observed that an undistillable DNN may have the trait
that each cluster of its output probability distributions in response to all
sample instances with the same label should be highly concentrated to the
extent that each cluster corresponding to each label should ideally collapse
into one probability distribution. Based on this observation and by measuring
the concentration of each cluster in terms of conditional mutual information
(CMI), a new training method called CMI minimized (CMIM) method is proposed,
which trains a DNN by jointly minimizing the conventional cross entropy (CE)
loss and the CMI values of all temperature scaled clusters across the entire
temperature spectrum. The resulting CMIM model is shown, by extensive
experiments, to be undistillable by all tested KD methods existing in the
literature. That is, the knockoff students distilled by these KD methods from
the CMIM model underperform the respective LS students. In addition, the CMIM
model is also shown to performs better than the model trained with the CE loss
alone in terms of their own prediction accuracy.

</details>


### [58] [GLU Attention Improve Transformer](https://arxiv.org/abs/2507.00022)
*Zehao Wang*

Main category: cs.LG

TL;DR: 介绍GLU Attention新注意力机制，实验表明其能在无额外参数和低计算成本下提升文本和视觉模型性能与收敛速度，且可与其他技术集成并已开源。


<details>
  <summary>Details</summary>
Motivation: 利用GLU增强神经网络性能的潜力，引入新的注意力机制。

Method: 引入名为GLU Attention的新注意力机制，将非线性引入注意力值。

Result: GLU Attention在文本和视觉模态中提升了模型性能和收敛速度，零额外参数且计算成本可忽略不计。

Conclusion: GLU Attention轻量级，能与其他技术无缝集成，有推广价值。

Abstract: Gated Linear Units (GLU) have shown great potential in enhancing neural
network performance. In this paper, I introduce a novel attention mechanism
called GLU Attention, which introduces nonlinearity into the values of
Attention. My experiments demonstrate that GLU Attention improves both model
performance and convergence speed across text and vision modalities with zero
additional parameters and negligible computational costs. GLU Attention is
lightweight and can seamlessly integrate with other technologies, such as Flash
Attention, Rotary Position Embedding (RoPE), and various Multi-Head Attention
(MHA) variants such as Grouped-Query Attention (GQA). This project is
open-sourced at github.

</details>


### [59] [ST-MTM: Masked Time Series Modeling with Seasonal-Trend Decomposition for Time Series Forecasting](https://arxiv.org/abs/2507.00013)
*Hyunwoo Seo,Chiehyeon Lim*

Main category: cs.LG

TL;DR: 提出带季节性趋势分解的掩码时间序列建模框架ST - MTM，实验显示其预测性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有掩码时间序列建模（MTM）简单处理原始时间序列，忽略内在语义结构，可能学习到虚假时间模式，需捕捉不同时间语义。

Method: 提出ST - MTM框架，采用针对季节性和趋势成分的新型掩码方法，引入对比学习任务。

Result: ST - MTM在预测性能上始终优于现有掩码建模、对比学习和监督预测方法。

Conclusion: ST - MTM能有效学习复杂时间变化和依赖，提升预测性能。

Abstract: Forecasting complex time series is an important yet challenging problem that
involves various industrial applications. Recently, masked time-series modeling
has been proposed to effectively model temporal dependencies for forecasting by
reconstructing masked segments from unmasked ones. However, since the semantic
information in time series is involved in intricate temporal variations
generated by multiple time series components, simply masking a raw time series
ignores the inherent semantic structure, which may cause MTM to learn spurious
temporal patterns present in the raw data. To capture distinct temporal
semantics, we show that masked modeling techniques should address entangled
patterns through a decomposition approach. Specifically, we propose ST-MTM, a
masked time-series modeling framework with seasonal-trend decomposition, which
includes a novel masking method for the seasonal-trend components that
incorporates different temporal variations from each component. ST-MTM uses a
period masking strategy for seasonal components to produce multiple masked
seasonal series based on inherent multi-periodicity and a sub-series masking
strategy for trend components to mask temporal regions that share similar
variations. The proposed masking method presents an effective pre-training task
for learning intricate temporal variations and dependencies. Additionally,
ST-MTM introduces a contrastive learning task to support masked modeling by
enhancing contextual consistency among multiple masked seasonal
representations. Experimental results show that our proposed ST-MTM achieves
consistently superior forecasting performance compared to existing masked
modeling, contrastive learning, and supervised forecasting methods.

</details>


### [60] [Best Agent Identification for General Game Playing](https://arxiv.org/abs/2507.00451)
*Matthew Stephenson,Alex Newcombe,Eric Piette,Dennis Soemers*

Main category: cs.LG

TL;DR: 提出有效通用方法在多问题领域为子任务选最优算法，在游戏领域实验显示性能提升，可改进评估程序。


<details>
  <summary>Details</summary>
Motivation: 在多问题领域准确识别每个子任务的最佳算法。

Method: 将其视为多臂老虎机的最佳臂识别问题，提出基于Wilson分数区间的乐观选择过程（Optimistic - WS）对臂排序。

Result: 与先前算法相比，在平均简单遗憾方面有显著性能提升。

Conclusion: 该方法可显著提高通用游戏框架及其他高算法运行时间的多任务领域的代理评估程序的质量和准确性。

Abstract: We present an efficient and generalised procedure to accurately identify the
best performing algorithm for each sub-task in a multi-problem domain. Our
approach treats this as a set of best arm identification problems for
multi-armed bandits, where each bandit corresponds to a specific task and each
arm corresponds to a specific algorithm or agent. We propose an optimistic
selection process based on the Wilson score interval (Optimistic-WS) that ranks
each arm across all bandits in terms of their potential regret reduction. We
evaluate the performance of Optimistic-WS on two of the most popular general
game domains, the General Video Game AI (GVGAI) framework and the Ludii general
game playing system, with the goal of identifying the highest performing agent
for each game within a limited number of trials. Compared to previous best arm
identification algorithms for multi-armed bandits, our results demonstrate a
substantial performance improvement in terms of average simple regret. This
novel approach can be used to significantly improve the quality and accuracy of
agent evaluation procedures for general game frameworks, as well as other
multi-task domains with high algorithm runtimes.

</details>


### [61] [SWE-Bench-CL: Continual Learning for Coding Agents](https://arxiv.org/abs/2507.00014)
*Thomas Joshi,Shayan Chowdhury,Fatih Uysal*

Main category: cs.LG

TL;DR: 介绍SWE - Bench - CL持续学习基准，含分析、评估框架、指标及实验协议，代码数据公开。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在静态代码生成基准表现好，但现实软件开发是动态的，需要评估模型持续学习能力。

Method: 基于SWE - Bench Verified数据集构建SWE - Bench - CL基准，进行任务结构相似性和上下文敏感性分析，使用基于LangGraph的评估框架和专门的持续学习指标，对比不同代理。

Result: 构建了SWE - Bench - CL基准，完成相应分析、评估框架和指标设定，进行了实验。

Conclusion: 公开代码和数据，为软件工程领域开发更具适应性和鲁棒性的AI代理提供可复现平台。

Abstract: Large Language Models (LLMs) have achieved impressive results on static
code-generation benchmarks, but real-world software development unfolds as a
continuous stream of evolving issues, fixes, and feature requests. We introduce
SWE-Bench-CL, a novel continual learning benchmark built on the human-verified
SWE-Bench Verified dataset introduced by OpenAI and Princeton-NLP in 2024. By
organizing GitHub issues into chronologically ordered sequences that reflect
natural repository evolution, SWE-Bench-CL enables direct evaluation of an
agent's ability to accumulate experience, transfer knowledge across tasks, and
resist catastrophic forgetting. We complement the dataset with (i) a
preliminary analysis of inter-task structural similarity and contextual
sensitivity, (ii) an interactive LangGraph-based evaluation framework augmented
with a FAISS-backed semantic memory module, and (iii) a suite of specialized
continual learning metrics -- including average accuracy, forgetting,
forward/backward transfer, tool-use efficiency, and a generalized Composite
Continual Learning Score and CL-F-beta score -- to capture the
stability-plasticity trade-off. We outline a rigorous experimental protocol
comparing memory-enabled and memory-disabled agents across diverse Python
repositories. All code and data are publicly available at
https://github.com/thomasjoshi/agents-never-forget, providing the community
with a reproducible platform for developing more adaptive and robust AI agents
in software engineering.

</details>


### [62] [Variational Autoencoder for Generating Broader-Spectrum prior Proposals in Markov chain Monte Carlo Methods](https://arxiv.org/abs/2507.00020)
*Marcio Borges,Felipe Pereira,Michel Tosin*

Main category: cs.LG

TL;DR: 研究用变分自编码器（VAE）增强马尔可夫链蒙特卡罗（McMC）方法效率与适用性，在合成地下水流动反演问题测试中表现良好，能提高贝叶斯推理效率。


<details>
  <summary>Details</summary>
Motivation: 传统方法如KLE需先验协方差函数知识，实际中常不可用，需更灵活的数据驱动方法。

Method: 使用VAE方法生成更广泛频谱的先验提议，应用于合成地下水流动反演问题。

Result: VAE参数化在相关长度已知时与KLE精度相当，偏离真实值时优于KLE，还能显著降低随机维度，提高计算效率。

Conclusion: 在McMC方法中利用深度生成模型可在高维问题中实现更灵活高效的贝叶斯推理。

Abstract: This study uses a Variational Autoencoder method to enhance the efficiency
and applicability of Markov Chain Monte Carlo (McMC) methods by generating
broader-spectrum prior proposals. Traditional approaches, such as the
Karhunen-Lo\`eve Expansion (KLE), require previous knowledge of the covariance
function, often unavailable in practical applications. The VAE framework
enables a data-driven approach to flexibly capture a broader range of
correlation structures in Bayesian inverse problems, particularly subsurface
flow modeling. The methodology is tested on a synthetic groundwater flow
inversion problem, where pressure data is used to estimate permeability fields.
Numerical experiments demonstrate that the VAE-based parameterization achieves
comparable accuracy to KLE when the correlation length is known and outperforms
KLE when the assumed correlation length deviates from the true value. Moreover,
the VAE approach significantly reduces stochastic dimensionality, improving
computational efficiency. The results suggest that leveraging deep generative
models in McMC methods can lead to more adaptable and efficient Bayesian
inference in high-dimensional problems.

</details>


### [63] [Vision Transformer with Adversarial Indicator Token against Adversarial Attacks in Radio Signal Classifications](https://arxiv.org/abs/2507.00015)
*Lu Zhang,Sangarapillai Lambotharan,Gan Zheng,Guisheng Liao,Xuekang Liu,Fabio Roli,Carsten Maple*

Main category: cs.LG

TL;DR: 本文针对基于Transformer的调制分类系统易受对抗攻击的问题，提出带对抗指示（AdvI）令牌的新型视觉Transformer（ViT）架构防御攻击，实验显示该方法在白盒攻击场景中表现出色。


<details>
  <summary>Details</summary>
Motivation: Transformer在自动调制分类应用中，基于其的无线电信号分类易受对抗攻击，需开发防御策略。

Method: 提出带AdvI令牌的ViT架构，将对抗训练方法与使用AdvI令牌的检测机制结合，统一神经网络模型；通过研究注意力机制探究方法原理。

Result: 实验表明该方法在处理包括快速梯度法、投影梯度下降攻击和基本迭代法等白盒攻击场景中优于多种竞争方法。

Conclusion: 提出的AdvI令牌是ViT中防御对抗攻击的关键元素，所提方法有效且架构复杂度低。

Abstract: The remarkable success of transformers across various fields such as natural
language processing and computer vision has paved the way for their
applications in automatic modulation classification, a critical component in
the communication systems of Internet of Things (IoT) devices. However, it has
been observed that transformer-based classification of radio signals is
susceptible to subtle yet sophisticated adversarial attacks. To address this
issue, we have developed a defensive strategy for transformer-based modulation
classification systems to counter such adversarial attacks. In this paper, we
propose a novel vision transformer (ViT) architecture by introducing a new
concept known as adversarial indicator (AdvI) token to detect adversarial
attacks. To the best of our knowledge, this is the first work to propose an
AdvI token in ViT to defend against adversarial attacks. Integrating an
adversarial training method with a detection mechanism using AdvI token, we
combine a training time defense and running time defense in a unified neural
network model, which reduces architectural complexity of the system compared to
detecting adversarial perturbations using separate models. We investigate into
the operational principles of our method by examining the attention mechanism.
We show the proposed AdvI token acts as a crucial element within the ViT,
influencing attention weights and thereby highlighting regions or features in
the input data that are potentially suspicious or anomalous. Through
experimental results, we demonstrate that our approach surpasses several
competitive methods in handling white-box attack scenarios, including those
utilizing the fast gradient method, projected gradient descent attacks and
basic iterative method.

</details>


### [64] [Generalizing to New Dynamical Systems via Frequency Domain Adaptation](https://arxiv.org/abs/2507.00025)
*Tiexin Qin,Hong Yan,Haoliang Li*

Main category: cs.LG

TL;DR: 提出FNSDA方法解决现有方法在特定领域预测和泛化问题，在四类动力系统上验证其性能优且参数成本低。


<details>
  <summary>Details</summary>
Motivation: 当前方法在特定领域可靠预测和对未知系统泛化能力受限。

Method: 提出FNSDA方法，在傅里叶空间自适应，通过自动划分傅里叶模式识别共享动力学，基于低维潜在系统参数调整特定模式实现高效泛化。

Result: 在四类动力系统评估中，FNSDA与现有方法相比，泛化性能优或相当，且显著降低参数成本。

Conclusion: FNSDA是一种参数高效的方法，能有效实现新动力学泛化。

Abstract: Learning the underlying dynamics from data with deep neural networks has
shown remarkable potential in modeling various complex physical dynamics.
However, current approaches are constrained in their ability to make reliable
predictions in a specific domain and struggle with generalizing to unseen
systems that are governed by the same general dynamics but differ in
environmental characteristics. In this work, we formulate a parameter-efficient
method, Fourier Neural Simulator for Dynamical Adaptation (FNSDA), that can
readily generalize to new dynamics via adaptation in the Fourier space.
Specifically, FNSDA identifies the shareable dynamics based on the known
environments using an automatic partition in Fourier modes and learns to adjust
the modes specific for each new environment by conditioning on low-dimensional
latent systematic parameters for efficient generalization. We evaluate our
approach on four representative families of dynamic systems, and the results
show that FNSDA can achieve superior or competitive generalization performance
compared to existing methods with a significantly reduced parameter cost. Our
code is available at https://github.com/WonderSeven/FNSDA.

</details>


### [65] [Gradient-based Fine-Tuning through Pre-trained Model Regularization](https://arxiv.org/abs/2507.00016)
*Xuanbo Liu,Liu Liu,Fuxiang Wu,Fusheng Hao,Xianglong Liu*

Main category: cs.LG

TL;DR: 提出高效梯度与正则化微调方法GRFT，更新权重矩阵行列，降存储、提效率，超现有方法。


<details>
  <summary>Details</summary>
Motivation: 大预训练模型针对特定下游任务微调需大量计算和存储资源，现有GPS方法增加了计算和存储需求。

Method: 提出GRFT方法，更新权重矩阵的行或列，理论证明平方梯度和最高的行或列更新最优，同时引入正则化增强知识迁移。

Result: GRFT取得了最优性能，在FGVC和VTAB数据集上分别仅需更新1.22%和0.30%的总参数。

Conclusion: GRFT方法高效且有效，源代码即将发布。

Abstract: Large pre-trained models have demonstrated extensive applications across
various fields. However, fine-tuning these models for specific downstream tasks
demands significant computational resources and storage. One fine-tuning
method, gradient-based parameter selection (GPS), focuses on fine-tuning only
the parameters with high gradients in each neuron, thereby reducing the number
of training parameters. Nevertheless, this approach increases computational
resource requirements and storage demands. In this paper, we propose an
efficient gradient-based and regularized fine-tuning method (GRFT) that updates
the rows or columns of the weight matrix. We theoretically demonstrate that the
rows or columns with the highest sum of squared gradients are optimal for
updating. This strategy effectively reduces storage overhead and improves the
efficiency of parameter selection. Additionally, we incorporate regularization
to enhance knowledge transfer from the pre-trained model. GRFT achieves
state-of-the-art performance, surpassing existing methods such as GPS, Adapter
Tuning, and LoRA. Notably, GRFT requires updating only 1.22% and 0.30% of the
total parameters on FGVC and VTAB datasets, respectively, demonstrating its
high efficiency and effectiveness. The source code will be released soon.

</details>


### [66] [Fractional Policy Gradients: Reinforcement Learning with Long-Term Memory](https://arxiv.org/abs/2507.00073)
*Urvi Pawar,Kunal Telangi*

Main category: cs.LG

TL;DR: 提出结合分数阶微积分的FPG强化学习框架，有理论和实证优势。


<details>
  <summary>Details</summary>
Motivation: 标准策略梯度方法受马尔可夫假设限制，存在高方差和采样低效问题。

Method: 用Caputo分数阶导数重新表述梯度，开发分数阶时间差分误差的高效递归计算技术。

Result: 理论上实现渐近方差O(t^(-alpha)) 降低且保持收敛；实证上样本效率提高35 - 68%，方差降低24 - 52%。

Conclusion: FPG是利用长程依赖且无计算开销的数学可行方法。

Abstract: We propose Fractional Policy Gradients (FPG), a reinforcement learning
framework incorporating fractional calculus for long-term temporal modeling in
policy optimization. Standard policy gradient approaches face limitations from
Markovian assumptions, exhibiting high variance and inefficient sampling. By
reformulating gradients using Caputo fractional derivatives, FPG establishes
power-law temporal correlations between state transitions. We develop an
efficient recursive computation technique for fractional temporal-difference
errors with constant time and memory requirements. Theoretical analysis shows
FPG achieves asymptotic variance reduction of order O(t^(-alpha)) versus
standard policy gradients while preserving convergence. Empirical validation
demonstrates 35-68% sample efficiency gains and 24-52% variance reduction
versus state-of-the-art baselines. This framework provides a mathematically
grounded approach for leveraging long-range dependencies without computational
overhead.

</details>


### [67] [Implicit Reward as the Bridge: A Unified View of SFT and DPO Connections](https://arxiv.org/abs/2507.00018)
*Bo Wang,Qinyuan Cheng,Runyu Peng,Rong Bao,Peiji Li,Qipeng Guo,Linyang Li,Zhiyuan Zeng,Yunhua Zhou,Xipeng Qiu*

Main category: cs.LG

TL;DR: 本文提出统一理论框架连接大语言模型后训练中的监督微调（SFT）和偏好学习，分析SFT局限，提出学习率降低方法和替代目标提升性能，还扩展了LLM对数和Q函数理论关系。


<details>
  <summary>Details</summary>
Motivation: 将预训练语言模型应用于实际任务，需要后训练过程，连接SFT和偏好学习以优化后训练。

Method: 通过严格数学推导，提出学习率降低方法，从不同f散度函数推导替代SFT目标。

Result: 学习率降低方法使指令跟随任务性能显著提升，相对增益达25%，绝对胜率提高6%，替代目标增强了后DPO模型性能，理论关系得到实验验证。

Conclusion: SFT和偏好学习在最优策略 - 奖励子空间内，SFT是隐式奖励学习特例，所提方法能有效提升大语言模型后训练性能。

Abstract: Post-training processes are essential phases in grounding pre-trained
language models to real-world tasks, with learning from demonstrations or
preference signals playing a crucial role in this adaptation. We present a
unified theoretical framework bridging Supervised Fine-Tuning (SFT) and
preference learning in Large Language Model (LLM) post-training. Through
rigorous mathematical derivation, we demonstrate that both SFT and preference
learning methods like Direct Preference Optimization (DPO) operate within the
same optimal policy-reward subspace, with SFT representing a special case of
implicit reward learning. Our analysis reveals a critical limitation in
conventional SFT: the KL divergence term in distribution matching becomes
constant with respect to the policy during optimization, failing to constrain
model updates. To address this, we propose a simple yet effective learning rate
reduction approach that yields significant performance improvements (up to
\textbf{25\%} relative gain and \textbf{6\%} absolute win rate increase in
instruction following tasks. Additionally, we derive alternative SFT objectives
from various f-divergence functions that preserve the KL term during
optimization, further enhancing post-DPO model performance. Finally, we extend
the theoretical relationship between LLM logits and Q-functions from preference
learning to the SFT context, providing mathematical derivations and
experimental validation.

</details>


### [68] [What Makes Local Updates Effective: The Role of Data Heterogeneity and Smoothness](https://arxiv.org/abs/2507.00195)
*Kumar Kshitij Patel*

Main category: cs.LG

TL;DR: 论文研究分布式和联邦优化中本地更新算法（如Local SGD），分析必要充分条件、建立上下界、提出分析框架并拓展到在线联邦学习。


<details>
  <summary>Details</summary>
Motivation: 提升对分布式和联邦优化中本地更新算法在数据异质性现实模型下的理论理解。

Method: 基于有界二阶异质性假设，采用细粒度基于共识误差的分析框架。

Result: 建立多种本地更新算法在不同机制下的上下界，刻画多问题类的最小最大复杂度，拓展到在线联邦学习给出后悔界。

Conclusion: 明确本地更新算法何时及为何有优势，为分析异质环境下Local SGD提供指南。

Abstract: This thesis contributes to the theoretical understanding of local update
algorithms, especially Local SGD, in distributed and federated optimization
under realistic models of data heterogeneity. A central focus is on the bounded
second-order heterogeneity assumption, which is shown to be both necessary and
sufficient for local updates to outperform centralized or mini-batch methods in
convex and non-convex settings. The thesis establishes tight upper and lower
bounds in several regimes for various local update algorithms and characterizes
the min-max complexity of multiple problem classes. At its core is a
fine-grained consensus-error-based analysis framework that yields sharper
finite-time convergence bounds under third-order smoothness and relaxed
heterogeneity assumptions. The thesis also extends to online federated
learning, providing fundamental regret bounds under both first-order and bandit
feedback. Together, these results clarify when and why local updates offer
provable advantages, and the thesis serves as a self-contained guide for
analyzing Local SGD in heterogeneous environments.

</details>


### [69] [Quantum Inspired Encoding Strategies for Machine Learning Models: Proposing and Evaluating Instance Level, Global Discrete, and Class Conditional Representations](https://arxiv.org/abs/2507.00019)
*Minati Rath,Hema Date*

Main category: cs.LG

TL;DR: 提出并比较三种量子启发的数据编码策略用于经典机器学习，分析其在编码效率、精度和预测性能方面的权衡。


<details>
  <summary>Details</summary>
Motivation: 降低高编码时间，确保正确编码值，分析对分类性能的影响。

Method: 提出实例级策略（ILS）、全局离散策略（GDS）和类条件值策略（CCVS）三种编码策略，并应用于分类任务。

Result: 通过应用这些策略评估了编码效率、正确性、模型准确性和计算成本。

Conclusion: 本研究为优化经典机器学习工作流中的量子启发数据转换提供了见解。

Abstract: In this study, we propose, evaluate and compare three quantum inspired data
encoding strategies, Instance Level Strategy (ILS), Global Discrete Strategy
(GDS) and Class Conditional Value Strategy (CCVS), for transforming classical
data into quantum data for use in pure classical machine learning models. The
primary objective is to reduce high encoding time while ensuring correct
encoding values and analyzing their impact on classification performance. The
Instance Level Strategy treats each row of dataset independently; mimics local
quantum states. Global Discrete Value Based encoding strategy maps all unique
feature values across the full dataset to quantum states uniformly. In
contrast, the Class conditional Value based encoding strategy encodes unique
values separately for each class, preserving class dependent information.
  We apply these encoding strategies to a classification task and assess their
impact on en-coding efficiency, correctness, model accuracy, and computational
cost. By analyzing the trade offs between encoding time, precision, and
predictive performance, this study provides insights into optimizing quantum
inspired data transformations for classical machine learning workflows.

</details>


### [70] [Exploring Large Action Sets with Hyperspherical Embeddings using von Mises-Fisher Sampling](https://arxiv.org/abs/2507.00518)
*Walid Bendada,Guillaume Salha-Galvan,Romain Hennequin,Théo Bontempelli,Thomas Bouabça,Tristan Cazenave*

Main category: cs.LG

TL;DR: 本文提出可扩展的vMF - exp方法用于强化学习中探索大动作集，理论上与B - exp有相同探索概率，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习中使用超球嵌入向量表示动作时，现有方法（如B - exp）探索大动作集的可扩展性问题。

Method: 使用von Mises - Fisher分布采样状态嵌入表示，探索其最近邻动作。

Result: 理论上vMF - exp与B - exp有相同探索概率，且实验验证了其在模拟数据、真实公共数据及音乐推荐系统中的有效性。

Conclusion: vMF - exp是使用超球嵌入探索大动作集的可扩展替代方法。

Abstract: This paper introduces von Mises-Fisher exploration (vMF-exp), a scalable
method for exploring large action sets in reinforcement learning problems where
hyperspherical embedding vectors represent these actions. vMF-exp involves
initially sampling a state embedding representation using a von Mises-Fisher
distribution, then exploring this representation's nearest neighbors, which
scales to virtually unlimited numbers of candidate actions. We show that, under
theoretical assumptions, vMF-exp asymptotically maintains the same probability
of exploring each action as Boltzmann Exploration (B-exp), a popular
alternative that, nonetheless, suffers from scalability issues as it requires
computing softmax values for each action. Consequently, vMF-exp serves as a
scalable alternative to B-exp for exploring large action sets with
hyperspherical embeddings. Experiments on simulated data, real-world public
data, and the successful large-scale deployment of vMF-exp on the recommender
system of a global music streaming service empirically validate the key
properties of the proposed method.

</details>


### [71] [Posterior Inference in Latent Space for Scalable Constrained Black-box Optimization](https://arxiv.org/abs/2507.00480)
*Kiyoung Om,Kyuil Sim,Taeyoung Yun,Hyeongyu Kang,Jinkyoo Park*

Main category: cs.LG

TL;DR: 本文提出新框架解决高维黑盒约束优化问题，通过两阶段迭代方法，在多任务上表现出色，代码公开。


<details>
  <summary>Details</summary>
Motivation: 现有贝叶斯优化和生成模型方法在解决高维黑盒约束优化问题时分别存在维度灾难、可扩展性差和模式崩溃等问题。

Method: 迭代两阶段：先训练流模型和代理模型；再将候选选择问题转化为后验推理问题，在流模型潜空间进行后验分布采样。

Result: 在多种合成和实际约束黑盒优化任务上表现优越。

Conclusion: 所提框架能有效克服现有方法的挑战，解决高维黑盒约束优化问题。

Abstract: Optimizing high-dimensional black-box functions under black-box constraints
is a pervasive task in a wide range of scientific and engineering problems.
These problems are typically harder than unconstrained problems due to
hard-to-find feasible regions. While Bayesian optimization (BO) methods have
been developed to solve such problems, they often struggle with the curse of
dimensionality. Recently, generative model-based approaches have emerged as a
promising alternative for constrained optimization. However, they suffer from
poor scalability and are vulnerable to mode collapse, particularly when the
target distribution is highly multi-modal. In this paper, we propose a new
framework to overcome these challenges. Our method iterates through two stages.
First, we train flow-based models to capture the data distribution and
surrogate models that predict both function values and constraint violations
with uncertainty quantification. Second, we cast the candidate selection
problem as a posterior inference problem to effectively search for promising
candidates that have high objective values while not violating the constraints.
During posterior inference, we find that the posterior distribution is highly
multi-modal and has a large plateau due to constraints, especially when
constraint feedback is given as binary indicators of feasibility. To mitigate
this issue, we amortize the sampling from the posterior distribution in the
latent space of flow-based models, which is much smoother than that in the data
space. We empirically demonstrate that our method achieves superior performance
on various synthetic and real-world constrained black-box optimization tasks.
Our code is publicly available \href{https://github.com/umkiyoung/CiBO}{here}.

</details>


### [72] [HelixPipe: Efficient Distributed Training of Long Sequence Transformers with Attention Parallel Pipeline Parallelism](https://arxiv.org/abs/2507.00394)
*Geng Zhang,Shenggan Cheng,Xuanlei Zhao,Ziming Liu,Yang You*

Main category: cs.LG

TL;DR: 提出HelixPipe用于长序列transformer训练，实验显示其在长序列上优势明显，能提升吞吐量和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有管道并行在transformer序列长度增长时，因二次注意力计算和大量内存开销导致性能不佳。

Method: 引入注意力并行分区减少管道气泡；采用先进后出微批次调度平衡内存使用并重叠通信与计算；利用无注意力重计算和分块MLP减轻碎片化。

Result: HelixPipe在不同管道大小、模型大小和集群配置下吞吐量和可扩展性优于现有方法，在特定条件下比基线方法提速26%。

Conclusion: HelixPipe在长序列transformer训练中有显著优势，代码开源。

Abstract: As transformer sequence lengths grow, existing pipeline parallelisms incur
suboptimal performance due to the quadratic attention computation and the
substantial memory overhead. To relieve these challenges, we propose HelixPipe,
a novel pipeline parallelism for long sequence transformer training. First,
HelixPipe introduces attention parallel partition, which schedules attention
computations of different micro batches across different pipeline stages in
parallel, reducing pipeline bubbles. Second, it employs a two-fold
first-in-last-out micro batch schedule to balance memory usage and overlap
communication with computation. Additionally, HelixPipe utilizes recomputation
without attention and chunked MLP to mitigate fragmentation and enable longer
sequences. Experiments demonstrate that HelixPipe gains increasing advantages
with longer sequence lengths, and outperforms existing methods in throughput
and scalability across varying pipeline sizes, model sizes, and cluster
configurations. Notably, it achieves a 26\% speedup over baseline methods when
training a 7B model with 128k sequence length on 64 H20 GPUs. Code is available
at https://github.com/code-tunnel/Megatron-LM/tree/dev.

</details>


### [73] [GANs Secretly Perform Approximate Bayesian Model Selection](https://arxiv.org/abs/2507.00651)
*Maurizio Filippone,Marius P. Linhard*

Main category: cs.LG

TL;DR: 本文通过将GAN解释为概率生成模型，解释其优缺点，提出正则化和优化策略，实验表明策略可提升性能并助于理解正则化策略。


<details>
  <summary>Details</summary>
Motivation: GAN优化困难且需正则化，旨在解释其优缺点并提出有效策略。

Method: 将GAN解释为部分随机的贝叶斯神经网络，建立通用近似条件，将GAN优化转化为边缘似然代理优化，利用与奥卡姆剃刀的联系定义策略。

Result: 一系列实验表明提出的策略能提升性能。

Conclusion: 提出的策略为GAN正则化策略的深入理解铺平道路。

Abstract: Generative Adversarial Networks (GANs) are popular and successful generative
models. Despite their success, optimization is notoriously challenging and they
require regularization against overfitting. In this work, we explain the
success and limitations of GANs by interpreting them as probabilistic
generative models. This interpretation enables us to view GANs as Bayesian
neural networks with partial stochasticity, allowing us to establish conditions
of universal approximation. We can then cast the adversarial-style optimization
of several variants of GANs as the optimization of a proxy for the marginal
likelihood. Taking advantage of the connection between marginal likelihood
optimization and Occam's razor, we can define regularization and optimization
strategies to smooth the loss landscape and search for solutions with minimum
description length, which are associated with flat minima and good
generalization. The results on a wide range of experiments indicate that these
strategies lead to performance improvements and pave the way to a deeper
understanding of regularization strategies for GANs.

</details>


### [74] [AIMatDesign: Knowledge-Augmented Reinforcement Learning for Inverse Materials Design under Data Scarcity](https://arxiv.org/abs/2507.00024)
*Yeyong Yu,Xilei Bian,Jie Xiong,Xing Wu,Quan Qian*

Main category: cs.LG

TL;DR: 针对机器学习驱动的材料逆向设计面临的数据与高维空间不匹配等问题，提出AIMatDesign框架，实验显示其在多方面优于传统方法，具有可靠性和应用潜力。


<details>
  <summary>Details</summary>
Motivation: 解决现有机器学习驱动的逆向设计方法在协调高维材料成分空间与有限实验数据时存在的模型可靠性低和无法有效融入领域专家知识的问题。

Method: 引入AIMatDesign框架，用基于差异的算法扩充实验数据构建可信经验池，用大语言模型引导的自动细化策略纠正预测不一致，使用基于知识的奖励函数提升训练稳定性和效率。

Result: AIMatDesign在发现效率、收敛速度和成功率上显著超越传统方法，实验合成的Zr基合金性能接近预测，能准确捕捉屈服强度随成分的变化趋势。

Conclusion: AIMatDesign框架具有可靠性，有用于闭环材料发现的潜力。

Abstract: With the growing demand for novel materials, machine learning-driven inverse
design methods face significant challenges in reconciling the high-dimensional
materials composition space with limited experimental data. Existing approaches
suffer from two major limitations: (I) machine learning models often lack
reliability in high-dimensional spaces, leading to prediction biases during the
design process; (II) these models fail to effectively incorporate domain expert
knowledge, limiting their capacity to support knowledge-guided inverse design.
To address these challenges, we introduce AIMatDesign, a reinforcement learning
framework that addresses these limitations by augmenting experimental data
using difference-based algorithms to build a trusted experience pool,
accelerating model convergence. To enhance model reliability, an automated
refinement strategy guided by large language models (LLMs) dynamically corrects
prediction inconsistencies, reinforcing alignment between reward signals and
state value functions. Additionally, a knowledge-based reward function
leverages expert domain rules to improve stability and efficiency during
training. Our experiments demonstrate that AIMatDesign significantly surpasses
traditional machine learning and reinforcement learning methods in discovery
efficiency, convergence speed, and success rates. Among the numerous candidates
proposed by AIMatDesign, experimental synthesis of representative Zr-based
alloys yielded a top-performing BMG with 1.7GPa yield strength and 10.2\%
elongation, closely matching predictions. Moreover, the framework accurately
captured the trend of yield strength variation with composition, demonstrating
its reliability and potential for closed-loop materials discovery.

</details>


### [75] [Ordinality in Discrete-level Question Difficulty Estimation: Introducing Balanced DRPS and OrderedLogitNN](https://arxiv.org/abs/2507.00736)
*Arthur Thuy,Ekaterina Loginova,Dries F. Benoit*

Main category: cs.LG

TL;DR: 研究用新指标平衡DRPS对三种模型输出进行基准测试，提出OrderedLogitNN，发现其在复杂任务表现好，平衡DRPS为QDE提供评估基础。


<details>
  <summary>Details</summary>
Motivation: 现有QDE研究忽略任务序数性质，评估指标耦合建模范式、未解决类别不平衡问题，影响跨研究可比性和评估公正性。

Method: 用平衡的离散排名概率得分（DRPS）对离散回归、分类和序数回归三种模型输出进行基准测试，提出OrderedLogitNN，在RACE++和ARC数据集上微调BERT。

Result: OrderedLogitNN在复杂任务中表现显著更好。

Conclusion: 平衡DRPS为离散级QDE提供了稳健公平的评估指标，为未来研究奠定基础。

Abstract: Recent years have seen growing interest in Question Difficulty Estimation
(QDE) using natural language processing techniques. Question difficulty is
often represented using discrete levels, framing the task as ordinal regression
due to the inherent ordering from easiest to hardest. However, the literature
has neglected the ordinal nature of the task, relying on classification or
discretized regression models, with specialized ordinal regression methods
remaining unexplored. Furthermore, evaluation metrics are tightly coupled to
the modeling paradigm, hindering cross-study comparability. While some metrics
fail to account for the ordinal structure of difficulty levels, none adequately
address class imbalance, resulting in biased performance assessments. This
study addresses these limitations by benchmarking three types of model outputs
-- discretized regression, classification, and ordinal regression -- using the
balanced Discrete Ranked Probability Score (DRPS), a novel metric that jointly
captures ordinality and class imbalance. In addition to using popular ordinal
regression methods, we propose OrderedLogitNN, extending the ordered logit
model from econometrics to neural networks. We fine-tune BERT on the RACE++ and
ARC datasets and find that OrderedLogitNN performs considerably better on
complex tasks. The balanced DRPS offers a robust and fair evaluation metric for
discrete-level QDE, providing a principled foundation for future research.

</details>


### [76] [ROSE: Toward Reality-Oriented Safety Evaluation of Large Language Models](https://arxiv.org/abs/2507.00026)
*Jiale Ding,Xiang Zheng,Cong Wang,Wei-Bin Lee,Xingjun Ma,Yu-Gang Jiang*

Main category: cs.LG

TL;DR: 提出ROSE框架用于大语言模型安全评估，实验显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型安全评估基准存在静态、更新难问题，自动对抗提示生成方法存在话题覆盖不足和与现实场景贴合度低的问题，需要更好的评估方法。

Method: 提出Reality - Oriented Safety Evaluation (ROSE)框架，用多目标强化学习微调对抗性大语言模型以生成话题多样、上下文丰富的对抗性提示。

Result: ROSE在揭示最先进大语言模型的安全漏洞方面优于现有方法，综合评估指标有显著提升。

Conclusion: ROSE朝着更实用、面向现实的大语言模型安全评估迈出了一步。

Abstract: As Large Language Models (LLMs) are increasingly deployed as black-box
components in real-world applications, evaluating their safety-especially under
adversarial prompting-has become critical. Arguably, effective safety
evaluations should be adaptive, evolving with LLM capabilities, and also cover
a broad spectrum of harmful topics and real-world scenarios to fully expose
potential vulnerabilities. Existing manual safety benchmarks, built on
handcrafted adversarial prompts, are limited by their static nature and the
intensive labor required to update them, making it difficult to keep pace with
rapidly advancing LLMs. In contrast, automated adversarial prompt generation
offers a promising path toward adaptive evaluation. However, current methods
often suffer from insufficient adversarial topic coverage (topic-level
diversity) and weak alignment with real-world contexts. These shortcomings stem
from the exploration-exploitation dilemma in black-box optimization and a lack
of real-world contextualization, resulting in adversarial prompts that are both
topically narrow and scenario-repetitive. To address these issues, we propose
Reality-Oriented Safety Evaluation (ROSE), a novel framework that uses
multi-objective reinforcement learning to fine-tune an adversarial LLM for
generating topically diverse and contextually rich adversarial prompts.
Experiments show that ROSE outperforms existing methods in uncovering safety
vulnerabilities in state-of-the-art LLMs, with notable improvements in
integrated evaluation metrics. We hope ROSE represents a step toward more
practical and reality-oriented safety evaluation of LLMs. WARNING: This paper
contains examples of potentially harmful text.

</details>


### [77] [HiT-JEPA: A Hierarchical Self-supervised Trajectory Embedding Framework for Similarity Computation](https://arxiv.org/abs/2507.00028)
*Lihuan Li,Hao Xue,Shuang Ao,Yang Song,Flora Salim*

Main category: cs.LG

TL;DR: 提出HiT - JEPA框架学习多尺度城市轨迹表示，实验证明其分层设计有效。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以在单一模型中融合轨迹细粒度细节和高级摘要，无法兼顾长期依赖和局部细微差别。

Method: 提出HiT - JEPA统一框架，采用三层层次结构逐步捕获点级细粒度细节、中间模式和高级轨迹抽象。

Result: 在多个真实数据集上的轨迹相似度计算实验表明，HiT - JEPA的分层设计能产生更丰富的多尺度表示。

Conclusion: HiT - JEPA框架有效解决了现有方法问题，可学习多尺度城市轨迹表示。

Abstract: The representation of urban trajectory data plays a critical role in
effectively analyzing spatial movement patterns. Despite considerable progress,
the challenge of designing trajectory representations that can capture diverse
and complementary information remains an open research problem. Existing
methods struggle in incorporating trajectory fine-grained details and
high-level summary in a single model, limiting their ability to attend to both
long-term dependencies while preserving local nuances. To address this, we
propose HiT-JEPA (Hierarchical Interactions of Trajectory Semantics via a Joint
Embedding Predictive Architecture), a unified framework for learning
multi-scale urban trajectory representations across semantic abstraction
levels. HiT-JEPA adopts a three-layer hierarchy that progressively captures
point-level fine-grained details, intermediate patterns, and high-level
trajectory abstractions, enabling the model to integrate both local dynamics
and global semantics in one coherent structure. Extensive experiments on
multiple real-world datasets for trajectory similarity computation show that
HiT-JEPA's hierarchical design yields richer, multi-scale representations. Code
is available at: https://anonymous.4open.science/r/HiT-JEPA.

</details>


### [78] [LoRA-Mixer: Coordinate Modular LoRA Experts Through Serial Attention Routing](https://arxiv.org/abs/2507.00029)
*Wenbing Li,Zikai Song,Hang Zhou,Yunyao Zhang,Junqing Yu,Wei Yang*

Main category: cs.LG

TL;DR: 提出LoRA - Mixer框架结合LoRA和MoE适配大语言模型，实验证明其高效且性能强。


<details>
  <summary>Details</summary>
Motivation: 现有结合低秩适应（LoRA）和专家混合（MoE）适配大语言模型的方法存在稀释参数效率和任务保真度的问题。

Method: 提出LoRA - Mixer框架，用动态路由、特定任务的LoRA专家替换注意力模块输入/输出线性层的投影矩阵，支持两种操作范式，引入自适应专业平衡损失（SBL）。

Result: 在七个基准数据集实验，如GSM8K、HumanEval和MedQA，相比基础模型有显著提升，相比最先进方法，用48%参数还有额外提升。

Conclusion: LoRA - Mixer框架有效、高效且性能强。

Abstract: Recent efforts to combine low-rank adaptation (LoRA) with mixture-of-experts
(MoE) for adapting large language models (LLMs) to multiple tasks still exhibit
prevailing limitations: they either swap entire attention/feed-forward layers
for switch experts or bolt on parallel expert branches, diluting parameter
efficiency and task fidelity. We propose the LoRA-Mixer, a modular and
lightweight MoE framework that integrates LoRA experts. Our core innovation
lies in replacing the projection matrices of the attention module's
input/output linear layers with dynamically routed, task-specific LoRA experts.
This design ensures seamless compatibility with diverse foundation models,
including transformers and state space models (SSMs), by leveraging their
inherent linear projection structures. The framework supports two operational
paradigms: (1) joint optimization of LoRA experts and routing mechanisms via a
novel hard-soft routing strategy, or (2) direct deployment of pre-trained,
frozen LoRA modules sourced from external repositories. To enable robust router
training with limited data while ensuring stable routing decisions and
maximizing expert reuse, we introduce an adaptive Specialization Balance Loss
(SBL) that jointly optimizes expert balance and task-specific alignment.
Extensive experiments on seven benchmark datasets, including MedQA, CoLA,
SST-2, GSM8K, ARC-E, ARC-C, and HumanEval, demonstrate the effectiveness of
LoRA-Mixer. On datasets such as GSM8K, HumanEval, and MedQA, LoRA-Mixer
achieves significant improvements of 7.61%, 4.88%, and 3.08% over the base
models, respectively. Compared with state-of-the-art methods, LoRA-Mixer
achieves additional improvements of 1.09%, 1.45%, and 1.68%, respectively,
using only 48% of the parameters, demonstrating its efficiency and strong
performance.

</details>


### [79] [Adaptive Action Duration with Contextual Bandits for Deep Reinforcement Learning in Dynamic Environments](https://arxiv.org/abs/2507.00030)
*Abhishek Verma,Nallarasan V,Balaraman Ravindran*

Main category: cs.LG

TL;DR: 提出将上下文老虎机与DRL结合的新范式来选择动作持续时间，实验证明在Atari 2600游戏上有性能提升，适用于实时应用。


<details>
  <summary>Details</summary>
Motivation: DRL中动作执行的时间尺度这一关键方面未充分探索，需增强策略灵活性和计算效率。

Method: 用上下文老虎机模块增强深度Q网络，基于状态上下文学习选择最佳动作重复率。

Result: 在Atari 2600游戏实验中，相比静态持续时间基线有显著性能提升。

Conclusion: 该范式为游戏和机器人等实时应用提供可扩展解决方案，动态动作持续时间很关键。

Abstract: Deep Reinforcement Learning (DRL) has achieved remarkable success in complex
sequential decision-making tasks, such as playing Atari 2600 games and
mastering board games. A critical yet underexplored aspect of DRL is the
temporal scale of action execution. We propose a novel paradigm that integrates
contextual bandits with DRL to adaptively select action durations, enhancing
policy flexibility and computational efficiency. Our approach augments a Deep
Q-Network (DQN) with a contextual bandit module that learns to choose optimal
action repetition rates based on state contexts. Experiments on Atari 2600
games demonstrate significant performance improvements over static duration
baselines, highlighting the efficacy of adaptive temporal abstractions in DRL.
This paradigm offers a scalable solution for real-time applications like gaming
and robotics, where dynamic action durations are critical.

</details>


### [80] [Enhancing Spatio-Temporal Forecasting with Spatial Neighbourhood Fusion:A Case Study on COVID-19 Mobility in Peru](https://arxiv.org/abs/2507.00031)
*Chuan Li,Jiang You,Hassine Moungla,Vincent Gauthier,Miguel Nunez-del-Prado,Hugo Alatrista-Salas*

Main category: cs.LG

TL;DR: 利用秘鲁新冠疫情期间数字接触追踪应用数据预测城市区域人流，提出SPN技术，实验显示该技术能提升预测性能，证明空间平滑稀疏移动信号对公共卫生危机时时空预测有效。


<details>
  <summary>Details</summary>
Motivation: 准确建模人类移动对理解疫情传播和及时干预至关重要，现有传统时间序列模型因空间稀疏性预测能力受限。

Method: 提出轻量级、与模型无关的空间邻域融合（SPN）技术，用相邻H3网格单元的聚合信号增强每个单元的特征，并在三种预测骨干模型上评估。

Result: SPN持续提升预测性能，测试均方误差最多降低9.85%。

Conclusion: 对稀疏移动信号进行空间平滑为公共卫生危机期间的稳健时空预测提供了简单有效的途径。

Abstract: Accurate modeling of human mobility is critical for understanding epidemic
spread and deploying timely interventions. In this work, we leverage a
large-scale spatio-temporal dataset collected from Peru's national Digital
Contact Tracing (DCT) application during the COVID-19 pandemic to forecast
mobility flows across urban regions. A key challenge lies in the spatial
sparsity of hourly mobility counts across hexagonal grid cells, which limits
the predictive power of conventional time series models. To address this, we
propose a lightweight and model-agnostic Spatial Neighbourhood Fusion (SPN)
technique that augments each cell's features with aggregated signals from its
immediate H3 neighbors. We evaluate this strategy on three forecasting
backbones: NLinear, PatchTST, and K-U-Net, under various historical input
lengths. Experimental results show that SPN consistently improves forecasting
performance, achieving up to 9.85 percent reduction in test MSE. Our findings
demonstrate that spatial smoothing of sparse mobility signals provides a simple
yet effective path toward robust spatio-temporal forecasting during public
health crises.

</details>


### [81] [IDRIFTNET: Physics-Driven Spatiotemporal Deep Learning for Iceberg Drift Forecasting](https://arxiv.org/abs/2507.00036)
*Rohan Putatunda,Sanjay Purushotham,Ratnaksha Lele,Vandana P. Janeja*

Main category: cs.LG

TL;DR: 本文提出混合IDRIFTNET模型预测冰山轨迹，在两座南极冰山数据上测试，结果显示该模型优于其他模型。


<details>
  <summary>Details</summary>
Motivation: 极地海洋中冰山轨迹准确预测因时空数据稀缺、运动复杂非线性且受环境变量影响而极具挑战，现有深度学习模型难以有效捕捉潜在动态和提供可靠预测结果。

Method: 提出结合冰山漂移物理分析公式与增强残差学习模型的物理驱动深度学习模型IDRIFTNET，学习分析解与真实观测的不匹配模式，并用旋转增强光谱神经网络捕捉数据全局和局部模式预测未来漂移位置。

Result: 在两座南极冰山A23A和B22A上与现有模型对比，IDRIFTNET在不同时间点的最终位移误差（FDE）和平均位移误差（ADE）更低。

Conclusion: IDRIFTNET在有限数据和动态环境条件下，能有效捕捉冰山复杂非线性漂移以预测轨迹。

Abstract: Drifting icebergs in the polar oceans play a key role in the Earth's climate
system, impacting freshwater fluxes into the ocean and regional ecosystems
while also posing a challenge to polar navigation. However, accurately
forecasting iceberg trajectories remains a formidable challenge, primarily due
to the scarcity of spatiotemporal data and the complex, nonlinear nature of
iceberg motion, which is also impacted by environmental variables. The iceberg
motion is influenced by multiple dynamic environmental factors, creating a
highly variable system that makes trajectory identification complex. These
limitations hinder the ability of deep learning models to effectively capture
the underlying dynamics and provide reliable predictive outcomes. To address
these challenges, we propose a hybrid IDRIFTNET model, a physics-driven deep
learning model that combines an analytical formulation of iceberg drift
physics, with an augmented residual learning model. The model learns the
pattern of mismatch between the analytical solution and ground-truth
observations, which is combined with a rotate-augmented spectral neural network
that captures both global and local patterns from the data to forecast future
iceberg drift positions. We compare IDRIFTNET model performance with
state-of-the-art models on two Antarctic icebergs: A23A and B22A. Our findings
demonstrate that IDRIFTNET outperforms other models by achieving a lower Final
Displacement Error (FDE) and Average Displacement Error (ADE) across a variety
of time points. These results highlight IDRIFTNET's effectiveness in capturing
the complex, nonlinear drift of icebergs for forecasting iceberg trajectories
under limited data and dynamic environmental conditions.

</details>


### [82] [Model Fusion via Neuron Interpolation](https://arxiv.org/abs/2507.00037)
*Phoomraphee Luenam,Andreas Spanopoulos,Amit Sant,Thomas Hofmann,Sotiris Anagnostidis,Sidak Pal Singh*

Main category: cs.LG

TL;DR: 提出以神经元为中心的模型融合算法，在多数据集实验中表现优于先前技术。


<details>
  <summary>Details</summary>
Motivation: 模型融合因内部表示差异存在困难，要开发有效融合多训练神经网络的算法。

Method: 提出以神经元为中心的模型融合算法，将父模型中间神经元分组创建目标表示，融合过程纳入神经元归因分数，可推广到任意层类型。

Result: 在多个基准数据集实验中，算法在零样本和非独立同分布融合场景中始终优于先前融合技术。

Conclusion: 所提算法能有效融合多训练神经网络，性能表现良好，代码开源。

Abstract: Model fusion aims to combine the knowledge of multiple models by creating one
representative model that captures the strengths of all of its parents.
However, this process is non-trivial due to differences in internal
representations, which can stem from permutation invariance, random
initialization, or differently distributed training data. We present a novel,
neuron-centric family of model fusion algorithms designed to integrate multiple
trained neural networks into a single network effectively regardless of
training data distribution. Our algorithms group intermediate neurons of parent
models to create target representations that the fused model approximates with
its corresponding sub-network. Unlike prior approaches, our approach
incorporates neuron attribution scores into the fusion process. Furthermore,
our algorithms can generalize to arbitrary layer types. Experimental results on
various benchmark datasets demonstrate that our algorithms consistently
outperform previous fusion techniques, particularly in zero-shot and non-IID
fusion scenarios. The code is available at
https://github.com/AndrewSpano/neuron-interpolation-model-fusion.

</details>


### [83] [Quality over Quantity: An Effective Large-Scale Data Reduction Strategy Based on Pointwise V-Information](https://arxiv.org/abs/2507.00038)
*Fei Chen,Wenchi Zhou*

Main category: cs.LG

TL;DR: 本文提出基于PVI的数据缩减策略，可提升模型性能与训练效率，还将PVI框架拓展到中文NLP任务。


<details>
  <summary>Details</summary>
Motivation: 解决在大规模数据集中选择最优实例以提高数据质量和训练效率的问题。

Method: 首先用PVI量化实例难度，过滤低难度实例；其次用渐进学习方法在按PVI升序排序的实例上训练分类器。

Result: 移除10%-30%数据，分类器性能损失仅0.0001%-0.76%；渐进学习比传统训练准确率提高0.8%；将PVI框架拓展到中文NLP任务。

Conclusion: 有效数据缩减策略能提升模型性能和训练效率，PVI框架跨语言应用有价值。

Abstract: Data reduction plays a vital role in data-centric AI by identifying the most
informative instance within large-scale datasets to enhance model training
efficiency. The core challenge lies in how to select the optimal
instances-rather than the entire datasets-to improve data quality and training
efficiency. In this paper, we propose an effective data reduction strategy
based on Pointwise V-information(PVI). First, we quantify instance difficulty
using PVI and filter out low-difficulty instances enabling a static approach.
Experiments demonstrate that removing 10%-30% of the data preserves the
classifier performance with only a 0.0001% to 0.76% loss in accuracy.Second, we
use a progressive learning approach to training the classifiers on instances
sorted by ascending PVI, accelerating convergence and achieving a 0.8% accuracy
gain over conventional training. Our results suggest that with the effective
data reduction strategy, training a classifier on the selected optimal subset
could enhance the model performance and boost training efficiency. Moreover, we
have transferred the PVI framework, which previously applied only to English
datasets, to diverse Chinese NLP tasks and base models, leading to valuable
insights for cross-lingual data reduction and faster training. The codes are
released at https://github.com/zhouwenchi/DatasetReductionStrategy.

</details>


### [84] [Pattern-Based Graph Classification: Comparison of Quality Measures and Importance of Preprocessing](https://arxiv.org/abs/2507.00039)
*Lucas Potin,Rosa Figueiredo,Vincent Labatut,Christine Largeron*

Main category: cs.LG

TL;DR: 文章对38种图分类质量度量方法进行对比分析，提出聚类预处理步骤提升性能，发现部分常用度量并非效果最佳。


<details>
  <summary>Details</summary>
Motivation: 图分类中质量度量方法众多，缺乏针对图的比较，难以选择合适方法，导致常使用最普遍的度量且缺乏充分评估。

Method: 理论上基于四个数学性质刻画38种度量，利用公开数据集构建基准并给出模式黄金标准排名方法，进行模式排名和分类性能的实证比较，提出聚类预处理步骤。

Result: 聚类预处理步骤有效，减少待处理模式数量且性能相当，部分常用度量未带来最佳结果。

Conclusion: 通过对比分析和预处理步骤，有助于为图分类选择更合适的质量度量方法。

Abstract: Graph classification aims to categorize graphs based on their structural and
attribute features, with applications in diverse fields such as social network
analysis and bioinformatics. Among the methods proposed to solve this task,
those relying on patterns (i.e. subgraphs) provide good explainability, as the
patterns used for classification can be directly interpreted. To identify
meaningful patterns, a standard approach is to use a quality measure, i.e. a
function that evaluates the discriminative power of each pattern. However, the
literature provides tens of such measures, making it difficult to select the
most appropriate for a given application. Only a handful of surveys try to
provide some insight by comparing these measures, and none of them specifically
focuses on graphs. This typically results in the systematic use of the most
widespread measures, without thorough evaluation. To address this issue, we
present a comparative analysis of 38 quality measures from the literature. We
characterize them theoretically, based on four mathematical properties. We
leverage publicly available datasets to constitute a benchmark, and propose a
method to elaborate a gold standard ranking of the patterns. We exploit these
resources to perform an empirical comparison of the measures, both in terms of
pattern ranking and classification performance. Moreover, we propose a
clustering-based preprocessing step, which groups patterns appearing in the
same graphs to enhance classification performance. Our experimental results
demonstrate the effectiveness of this step, reducing the number of patterns to
be processed while achieving comparable performance. Additionally, we show that
some popular measures widely used in the literature are not associated with the
best results.

</details>


### [85] [Leveraging Unlabeled Audio-Visual Data in Speech Emotion Recognition using Knowledge Distillation](https://arxiv.org/abs/2507.00055)
*Varsha Pendyala,Pedro Morgado,William Sethares*

Main category: cs.LG

TL;DR: 本文提出轻量级知识蒸馏框架LiSER用于语音情感识别，实验表明其可降低对大量标注数据集的依赖。


<details>
  <summary>Details</summary>
Motivation: 语音情感识别结合多模态数据有益，但收集大量标注数据成本高。

Method: 提出名为LightweightSER (LiSER) 的知识蒸馏框架，利用基于先进语音和面部表征模型构建的大型教师模型，将语音情感和面部表情知识从教师模型转移到轻量级学生模型。

Result: 在RAVDESS和CREMA - D两个基准数据集上的实验表明，LiSER可降低语音情感识别任务对大量标注数据集的依赖。

Conclusion: LiSER框架能有效降低语音情感识别对大量标注数据的依赖。

Abstract: Voice interfaces integral to the human-computer interaction systems can
benefit from speech emotion recognition (SER) to customize responses based on
user emotions. Since humans convey emotions through multi-modal audio-visual
cues, developing SER systems using both the modalities is beneficial. However,
collecting a vast amount of labeled data for their development is expensive.
This paper proposes a knowledge distillation framework called LightweightSER
(LiSER) that leverages unlabeled audio-visual data for SER, using large teacher
models built on advanced speech and face representation models. LiSER transfers
knowledge regarding speech emotions and facial expressions from the teacher
models to lightweight student models. Experiments conducted on two benchmark
datasets, RAVDESS and CREMA-D, demonstrate that LiSER can reduce the dependence
on extensive labeled datasets for SER tasks.

</details>


### [86] [Smooth-Distill: A Self-distillation Framework for Multitask Learning with Wearable Sensor Data](https://arxiv.org/abs/2507.00061)
*Hoang-Dieu Vu,Duc-Nghia Tran,Quang-Tu Pham,Hieu H. Pham,Nicolas Vuillerme,Duc-Tan Tran*

Main category: cs.LG

TL;DR: 本文提出Smooth - Distill自蒸馏框架，利用MTL - net架构处理加速度计数据进行人体活动识别和传感器放置检测，减少训练计算开销，实验效果优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 降低人体活动识别和传感器放置检测任务中知识蒸馏的训练计算开销，实现多任务学习时平衡准确性和训练效率。

Method: 提出Smooth - Distill框架，使用基于CNN的MTL - net架构，用模型自身平滑的历史版本作为教师模型；开发包含12种睡眠姿势的加速度计数据集，结合两个公开数据集。

Result: Smooth - Distill在不同评估场景中始终优于其他方法，在两项任务上有显著提升，训练收敛更稳定，过拟合更少。

Conclusion: 该框架有助于知识蒸馏在人体活动识别系统中的实际应用，减少模型训练计算成本，适用于需频繁更新模型或资源受限平台的场景。

Abstract: This paper introduces Smooth-Distill, a novel self-distillation framework
designed to simultaneously perform human activity recognition (HAR) and sensor
placement detection using wearable sensor data. The proposed approach utilizes
a unified CNN-based architecture, MTL-net, which processes accelerometer data
and branches into two outputs for each respective task. Unlike conventional
distillation methods that require separate teacher and student models, the
proposed framework utilizes a smoothed, historical version of the model itself
as the teacher, significantly reducing training computational overhead while
maintaining performance benefits. To support this research, we developed a
comprehensive accelerometer-based dataset capturing 12 distinct sleep postures
across three different wearing positions, complementing two existing public
datasets (MHealth and WISDM). Experimental results show that Smooth-Distill
consistently outperforms alternative approaches across different evaluation
scenarios, achieving notable improvements in both human activity recognition
and device placement detection tasks. This method demonstrates enhanced
stability in convergence patterns during training and exhibits reduced
overfitting compared to traditional multitask learning baselines. This
framework contributes to the practical implementation of knowledge distillation
in human activity recognition systems, offering an effective solution for
multitask learning with accelerometer data that balances accuracy and training
efficiency. More broadly, it reduces the computational cost of model training,
which is critical for scenarios requiring frequent model updates or training on
resource-constrained platforms. The code and model are available at
https://github.com/Kuan2vn/smooth\_distill.

</details>


### [87] [Theoretical Modeling of LLM Self-Improvement Training Dynamics Through Solver-Verifier Gap](https://arxiv.org/abs/2507.00075)
*Yifan Sun,Yushan Liang,Zhen Zhang,Jiaye Teng*

Main category: cs.LG

TL;DR: 本文理论建模大语言模型自提升训练动态，用早期信息预测自提升极限能力，验证模型有效性并分析外部数据影响。


<details>
  <summary>Details</summary>
Motivation: 目前大语言模型自提升过程中性能演变情况研究不足，需进行理论建模分析。

Method: 通过求解器 - 验证器差距概念对自提升训练动态进行理论建模，基于该框架用早期训练信息预测自提升极限能力，并在多种模型和数据集上验证，还分析外部数据影响。

Result: 理论模型在多种大语言模型和数据集上有效，发现有限外部数据在各阶段使用不显著影响最终性能。

Conclusion: 提出的理论模型可用于分析大语言模型自提升训练动态和外部数据影响，有限外部数据使用策略符合经验观察。

Abstract: Self-improvement is among the most prominent techniques within the realm of
large language models (LLM), aiming to enhance the LLM performance without
relying on external data. Despite its significance, generally how LLM
performances evolve during the self-improvement process remains underexplored.
In this paper, we theoretically model the training dynamics of self-improvement
via the concept of solver-verifier gap. This is inspired by the conjecture that
the performance enhancement of self-improvement stems from the gap between
LLM's solver capability and verifier capability. Based on the theoretical
framework, we further introduce how to predict the ultimate power of
self-improvement using only information from the first few training epochs. We
empirically validate the effectiveness of the theoretical model on various LLMs
and datasets. Beyond self-improvement, we extend our analysis to investigate
how external data influences these dynamics within the framework. Notably, we
find that under limited external data regimes, such external data can be
utilized at any stage without significantly affecting final performances, which
accords with the empirical observations.

</details>


### [88] [The language of time: a language model perspective on time-series foundation models](https://arxiv.org/abs/2507.00078)
*Yi Xie,Yun Xiong,Zejian Shi,Hao Niu,Zhengfu Liu*

Main category: cs.LG

TL;DR: 本文从理论和实验角度研究基于补丁的时间序列基础模型的表征学习机制和泛化能力，揭示其性能优越原因并提供理论基础。


<details>
  <summary>Details</summary>
Motivation: 时间序列基础模型在跨领域迁移上存在理论与实践的矛盾，需探究其表征学习机制和泛化能力以解决该悖论。

Method: 从理论和实验两方面进行研究，论证模型将确定性向量表征扩展为潜在概率分布形式，理论分析连续时间序列补丁可量化为离散词汇。

Result: 模型可继承大语言模型的强大表征和迁移能力，在时间任务中表现优越。

Conclusion: 为理解、评估和改进大规模时间序列基础模型的安全性和可靠性提供了严谨的理论基石。

Abstract: With the rise of large language models, the paradigm of training foundation
models with massive parameter counts on vast datasets has been adopted in
multiple domains to achieve remarkable success. Time series foundation models
represent a significant extension of this paradigm, demonstrating exceptional
expressive power, generalization, and cross-domain transferability. However,
this gives rise to a fundamental paradox: time series data reflect distinct
dynamical systems, making cross-domain transfer intuitively implausible, yet
this is contradicted by the models' empirical success. To resolve this paradox,
this paper investigates, from both theoretical and experimental perspectives,
the representation learning mechanisms and generalization capabilities of
patch-based time series foundation models. We argue that such models are not
merely applying a new architecture but are fundamentally generalizing the
representation paradigm of language models by extending deterministic
vector-based representations to latent probabilistic distributional forms. Our
theoretical analysis supports this framework by demonstrating that continuous
time-series patches can be faithfully quantized into a discrete vocabulary
whose key statistical properties are highly consistent with those of natural
language. This generalization allows time series models to inherit the robust
representation and transfer abilities of large language models, thereby
explaining their superior performance in temporal tasks. Ultimately, our work
provides a rigorous theoretical cornerstone for understanding, evaluating, and
improving the safety and reliability of large-scale time series foundation
models.

</details>


### [89] [Online Meal Detection Based on CGM Data Dynamics](https://arxiv.org/abs/2507.00080)
*Ali Tavasoli,Heman Shakeri*

Main category: cs.LG

TL;DR: 利用CGM数据的动态模式特征检测用餐事件，比传统方法更优


<details>
  <summary>Details</summary>
Motivation: 提高用餐事件检测的准确性和对潜在血糖动态的可解释性

Method: 利用从CGM数据中提取的动态模式作为特征来检测用餐事件

Result: 该方法比传统方法提高了检测准确性

Conclusion: 聚焦动态特征的方法为特征提取提供了强大框架，适用于不同数据集和实际应用

Abstract: We utilize dynamical modes as features derived from Continuous Glucose
Monitoring (CGM) data to detect meal events. By leveraging the inherent
properties of underlying dynamics, these modes capture key aspects of glucose
variability, enabling the identification of patterns and anomalies associated
with meal consumption. This approach not only improves the accuracy of meal
detection but also enhances the interpretability of the underlying glucose
dynamics. By focusing on dynamical features, our method provides a robust
framework for feature extraction, facilitating generalization across diverse
datasets and ensuring reliable performance in real-world applications. The
proposed technique offers significant advantages over traditional approaches,
improving detection accuracy,

</details>


### [90] [Federated Learning-Enabled Hybrid Language Models for Communication-Efficient Token Transmission](https://arxiv.org/abs/2507.00082)
*Faranaksadat Solat,Joohyung Lee,Mohamed Seif,Dusit Niyato,H. Vincent Poor*

Main category: cs.LG

TL;DR: 提出FedHLM框架结合不确定性感知推理与联邦学习，减少混合语言模型LLM传输，适用于边缘AI应用。


<details>
  <summary>Details</summary>
Motivation: 现有混合语言模型在带宽受限场景频繁调用大语言模型导致高通信开销。

Method: 提出FedHLM框架，协同学习token级不确定性阈值，用基于嵌入的token表示进行P2P解析，引入分层模型聚合。

Result: 在大规模新闻分类任务实验中，FedHLM减少超95%的LLM传输且精度损失可忽略。

Conclusion: FedHLM适合可扩展且高效的边缘AI应用。

Abstract: Hybrid Language Models (HLMs) combine the low-latency efficiency of Small
Language Models (SLMs) on edge devices with the high accuracy of Large Language
Models (LLMs) on centralized servers. Unlike traditional end-to-end LLM
inference, HLMs reduce latency and communication by invoking LLMs only when
local SLM predictions are uncertain, i.e., when token-level confidence is low
or entropy is high. However, ambiguous or low-confidence predictions still
require frequent offloading to the LLM, leading to significant communication
overhead in bandwidth-constrained settings. To address this, we propose FedHLM,
a communication-efficient HLM framework that integrates uncertainty-aware
inference with Federated Learning (FL). FedHLM's key innovation lies in
collaboratively learning token-level uncertainty thresholds that govern when
LLM assistance is needed. Rather than using static or manually tuned
thresholds, FedHLM employs FL to optimize these thresholds in a
privacy-preserving, distributed manner. Additionally, it leverages
embedding-based token representations for Peer-to-Peer (P2P) resolution,
enabling clients to reuse tokens inferred by semantically similar peers without
engaging the LLM. We further introduce hierarchical model aggregation: edge
servers refine local routing policies through client updates, while
cross-cluster coordination aligns global decision boundaries. This layered
design captures recurring uncertainty patterns, reducing redundant LLM queries.
Experiments on large-scale news classification tasks show that FedHLM reduces
LLM transmissions by over 95 percent with negligible accuracy loss, making it
well-suited for scalable and efficient edge-AI applications.

</details>


### [91] [Strategic Counterfactual Modeling of Deep-Target Airstrike Systems via Intervention-Aware Spatio-Causal Graph Networks](https://arxiv.org/abs/2507.00083)
*Wei Meng*

Main category: cs.LG

TL;DR: 提出IA - STGNN框架解决当前战略级模拟中战术打击与战略延迟间因果建模问题，实验显示其性能优于基线模型，可用于多种战略应用。


<details>
  <summary>Details</summary>
Motivation: 解决当前战略级模拟中战术打击行为和战略延迟之间缺乏结构化因果建模，以及捕捉‘恢复力 - 节点抑制 - 谈判窗口’链中中间变量的结构瓶颈问题。

Method: 提出Intervention - Aware Spatio - Temporal Graph Neural Network (IA - STGNN)框架，集成图注意力机制、反事实模拟单元和空间干预节点重建，用多物理模拟平台（GEANT4 + COMSOL）在NIST SP 800 - 160标准下生成训练数据。

Result: IA - STGNN显著优于基线模型，MAE降低12.8%，Top - 5准确率提高18.4%，改善因果路径一致性和干预稳定性。

Conclusion: IA - STGNN能对战略延迟进行可解释预测，支持核威慑模拟等应用，为高级政策建模提供结构化、透明的AI决策支持机制。

Abstract: This study addresses the lack of structured causal modeling between tactical
strike behavior and strategic delay in current strategic-level simulations,
particularly the structural bottlenecks in capturing intermediate variables
within the "resilience - nodal suppression - negotiation window" chain. We
propose the Intervention-Aware Spatio-Temporal Graph Neural Network (IA-STGNN),
a novel framework that closes the causal loop from tactical input to strategic
delay output. The model integrates graph attention mechanisms, counterfactual
simulation units, and spatial intervention node reconstruction to enable
dynamic simulations of strike configurations and synchronization strategies.
Training data are generated from a multi-physics simulation platform (GEANT4 +
COMSOL) under NIST SP 800-160 standards, ensuring structural traceability and
policy-level validation. Experimental results demonstrate that IA-STGNN
significantly outperforms baseline models (ST-GNN, GCN-LSTM, XGBoost),
achieving a 12.8 percent reduction in MAE and 18.4 percent increase in Top-5
percent accuracy, while improving causal path consistency and intervention
stability. IA-STGNN enables interpretable prediction of strategic delay and
supports applications such as nuclear deterrence simulation, diplomatic window
assessment, and multi-strategy optimization, providing a structured and
transparent AI decision-support mechanism for high-level policy modeling.

</details>


### [92] [A Joint Topology-Data Fusion Graph Network for Robust Traffic Speed Prediction with Data Anomalism](https://arxiv.org/abs/2507.00085)
*Ruiyuan Jiang,Dongyao Jia,Eng Gee Lim,Pengfei Fan,Yuli Zhang,Shangbo Wang*

Main category: cs.LG

TL;DR: 提出图融合增强网络（GFEN）用于交通速度预测，实验表明其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前交通预测方法难以处理交通动态的复杂性和非线性，难以融合时空特征，且处理历史数据的方法适应性差。

Method: 提出拓扑时空图融合技术提取和融合时空相关性，采用基于k阶差分的数学框架与基于注意力的深度学习结构相结合的混合方法平滑历史观测数据。

Result: GFEN在预测准确性上比现有方法提高约6.3%，收敛速度接近近期混合模型的两倍。

Conclusion: GFEN性能优越，有潜力显著提高交通预测系统的效率。

Abstract: Accurate traffic prediction is essential for Intelligent Transportation
Systems (ITS), yet current methods struggle with the inherent complexity and
non-linearity of traffic dynamics, making it difficult to integrate spatial and
temporal characteristics. Furthermore, existing approaches use static
techniques to address non-stationary and anomalous historical data, which
limits adaptability and undermines data smoothing. To overcome these
challenges, we propose the Graph Fusion Enhanced Network (GFEN), an innovative
framework for network-level traffic speed prediction. GFEN introduces a novel
topological spatiotemporal graph fusion technique that meticulously extracts
and merges spatial and temporal correlations from both data distribution and
network topology using trainable methods, enabling the modeling of multi-scale
spatiotemporal features. Additionally, GFEN employs a hybrid methodology
combining a k-th order difference-based mathematical framework with an
attention-based deep learning structure to adaptively smooth historical
observations and dynamically mitigate data anomalies and non-stationarity.
Extensive experiments demonstrate that GFEN surpasses state-of-the-art methods
by approximately 6.3% in prediction accuracy and exhibits convergence rates
nearly twice as fast as recent hybrid models, confirming its superior
performance and potential to significantly enhance traffic prediction system
efficiency.

</details>


### [93] [pUniFind: a unified large pre-trained deep learning model pushing the limit of mass spectra interpretation](https://arxiv.org/abs/2507.00087)
*Jiale Zhao,Pengzhi Mao,Kaifei Wang,Yiming Li,Yaping Peng,Ranfei Chen,Shuqi Lu,Xiaohong Ji,Jiaxiang Ding,Xin Zhang,Yucheng Liao,Weinan E,Weijie Zhang,Han Wen,Hao Chi*

Main category: cs.LG

TL;DR: 介绍蛋白质组学首个大规模多模态预训练模型pUniFind，能集成肽谱评分与从头测序，表现优于传统引擎，建立统一深度学习框架。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型多为特征提取器，缺乏统一评分框架，需新方法提升蛋白质组学分析效果。

Method: 在超1亿个开放搜索光谱上训练pUniFind，通过跨模态预测对齐光谱和肽模式，有基于深度学习的质量控制模块。

Result: 在不同数据集上优于传统引擎，免疫肽组学中鉴定肽数量增42.6%，比现有从头测序方法多鉴定60%的PSMs，质控模块额外恢复38.5%的肽。

Conclusion: pUniFind建立了统一、可扩展的蛋白质组学深度学习分析框架，提高了灵敏度、修饰覆盖率和可解释性。

Abstract: Deep learning has advanced mass spectrometry data interpretation, yet most
models remain feature extractors rather than unified scoring frameworks. We
present pUniFind, the first large-scale multimodal pre-trained model in
proteomics that integrates end-to-end peptide-spectrum scoring with open,
zero-shot de novo sequencing. Trained on over 100 million open search-derived
spectra, pUniFind aligns spectral and peptide modalities via cross modality
prediction and outperforms traditional engines across diverse datasets,
particularly achieving a 42.6 percent increase in the number of identified
peptides in immunopeptidomics. Supporting over 1,300 modifications, pUniFind
identifies 60 percent more PSMs than existing de novo methods despite a
300-fold larger search space. A deep learning based quality control module
further recovers 38.5 percent additional peptides including 1,891 mapped to the
genome but absent from reference proteomes while preserving full fragment ion
coverage. These results establish a unified, scalable deep learning framework
for proteomic analysis, offering improved sensitivity, modification coverage,
and interpretability.

</details>


### [94] [A new machine learning framework for occupational accidents forecasting with safety inspections integration](https://arxiv.org/abs/2507.00089)
*Aho Yapi,Pierre Latouche,Arnaud Guillin,Yan Bailly*

Main category: cs.LG

TL;DR: 提出短期职业事故预测通用框架，用安全检查数据，LSTM网络表现优，能转数据为风险分数辅助决策。


<details>
  <summary>Details</summary>
Motivation: 提供可靠短期职业事故预测方法，辅助决策，提高安全投资回报。

Method: 构建以安全检查为基础、将事故发生建模为二元时间序列的框架，用滑动窗口交叉验证，对比多种机器学习算法。

Result: LSTM网络表现最佳，平衡准确率达0.86，能检测高风险期。

Conclusion: 该方法可将检查数据转为风险分数，辅助决策者提前干预，获得安全投资高回报。

Abstract: We propose a generic framework for short-term occupational accident
forecasting that leverages safety inspections and models accident occurrences
as binary time series. The approach generates daily predictions, which are then
aggregated into weekly safety assessments to better inform decision making. To
ensure the reliability and operational applicability of the forecasts, we apply
a sliding-window cross-validation procedure specifically designed for time
series data, combined with an evaluation based on aggregated period-level
metrics. Several machine learning algorithms, including logistic regression,
tree-based models, and neural networks, are trained and systematically compared
within this framework. Unlike the other approaches, the long short-term memory
(LSTM) network outperforms the other approaches and detects the upcoming
high-risk periods with a balanced accuracy of 0.86, confirming the robustness
of our methodology and demonstrating that a binary time series model can
anticipate these critical periods based on safety inspections. The proposed
methodology converts routine safety inspection data into clear weekly risk
scores, detecting the periods when accidents are most likely. Decision-makers
can integrate these scores into their planning tools to classify inspection
priorities, schedule targeted interventions, and funnel resources to the sites
or shifts classified as highest risk, stepping in before incidents occur and
getting the greatest return on safety investments.

</details>


### [95] [Generating Heterogeneous Multi-dimensional Data : A Comparative Study](https://arxiv.org/abs/2507.00090)
*Corbeau Michael,Claeys Emmanuelle,Serrurier Mathieu,Zaraté Pascale*

Main category: cs.LG

TL;DR: 本文对比不同数据生成方法用于消防员干预场景资源分配模拟，并采用特定和标准指标评估合成数据质量。


<details>
  <summary>Details</summary>
Motivation: 消防员干预场景中人员和物资资源分配需模拟不同场景，需生成数据，传统评估指标不足。

Method: 比较随机采样、表格变分自编码器等不同数据生成方法，结合消防领域特定指标和Wasserstein距离评估合成数据质量。

Result: 未提及具体结果。

Conclusion: 未提及具体结论。

Abstract: Allocation of personnel and material resources is highly sensible in the case
of firefighter interventions. This allocation relies on simulations to
experiment with various scenarios. The main objective of this allocation is the
global optimization of the firefighters response. Data generation is then
mandatory to study various scenarios In this study, we propose to compare
different data generation methods. Methods such as Random Sampling, Tabular
Variational Autoencoders, standard Generative Adversarial Networks, Conditional
Tabular Generative Adversarial Networks and Diffusion Probabilistic Models are
examined to ascertain their efficacy in capturing the intricacies of
firefighter interventions. Traditional evaluation metrics often fall short in
capturing the nuanced requirements of synthetic datasets for real-world
scenarios. To address this gap, an evaluation of synthetic data quality is
conducted using a combination of domain-specific metrics tailored to the
firefighting domain and standard measures such as the Wasserstein distance.
Domain-specific metrics include response time distribution, spatial-temporal
distribution of interventions, and accidents representation. These metrics are
designed to assess data variability, the preservation of fine and complex
correlations and anomalies such as event with a very low occurrence, the
conformity with the initial statistical distribution and the operational
relevance of the synthetic data. The distribution has the particularity of
being highly unbalanced, none of the variables following a Gaussian
distribution, adding complexity to the data generation process.

</details>


### [96] [DFReg: A Physics-Inspired Framework for Global Weight Distribution Regularization in Neural Networks](https://arxiv.org/abs/2507.00101)
*Giovanni Ruggieri*

Main category: cs.LG

TL;DR: 介绍DFReg，一种受物理启发的深度神经网络正则化方法，作用于权重全局分布。


<details>
  <summary>Details</summary>
Motivation: 为深度神经网络寻找新的正则化方法，以实现全局结构规则化。

Method: 借鉴密度泛函理论（DFT），对权重施加泛函惩罚，促使权重配置平滑、多样且分布良好。

Result: 未提及具体结果。

Conclusion: 与传统技术不同，DFReg无需架构更改或随机扰动即可实现全局结构规则化。

Abstract: We introduce DFReg, a physics-inspired regularization method for deep neural
networks that operates on the global distribution of weights. Drawing from
Density Functional Theory (DFT), DFReg applies a functional penalty to
encourage smooth, diverse, and well-distributed weight configurations. Unlike
traditional techniques such as Dropout or L2 decay, DFReg imposes global
structural regularity without architectural changes or stochastic
perturbations.

</details>


### [97] [Towards transparent and data-driven fault detection in manufacturing: A case study on univariate, discrete time series](https://arxiv.org/abs/2507.00102)
*Bernd Hofmann,Patrick Bruendl,Huong Giang Nguyen,Joerg Franke*

Main category: cs.LG

TL;DR: 本文提出数据驱动且透明的工业故障检测方法，应用于压接过程，取得95.9%检测准确率，增强数据驱动故障检测的可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统质量控制方法缺乏适应性，数据驱动方法缺乏可解释性，在工业环境接受度低，需要一种兼顾数据驱动和可解释性的工业故障检测方法。

Method: 集成监督式机器学习模型进行多类故障分类，用Shapley Additive Explanations进行事后解释，用特定领域可视化技术将模型解释映射到可操作特征，还提出评估方法。

Result: 应用于压接过程，系统故障检测准确率达95.9%，定量和定性评估确认解释的相关性和可解释性。

Conclusion: 这种以人类为中心的方法增强了数据驱动故障检测的信任和可解释性，有助于工业质量控制应用系统设计。

Abstract: Ensuring consistent product quality in modern manufacturing is crucial,
particularly in safety-critical applications. Conventional quality control
approaches, reliant on manually defined thresholds and features, lack
adaptability to the complexity and variability inherent in production data and
necessitate extensive domain expertise. Conversely, data-driven methods, such
as machine learning, demonstrate high detection performance but typically
function as black-box models, thereby limiting their acceptance in industrial
environments where interpretability is paramount. This paper introduces a
methodology for industrial fault detection, which is both data-driven and
transparent. The approach integrates a supervised machine learning model for
multi-class fault classification, Shapley Additive Explanations for post-hoc
interpretability, and a do-main-specific visualisation technique that maps
model explanations to operator-interpretable features. Furthermore, the study
proposes an evaluation methodology that assesses model explanations through
quantitative perturbation analysis and evaluates visualisations by qualitative
expert assessment. The approach was applied to the crimping process, a
safety-critical joining technique, using a dataset of univariate, discrete time
series. The system achieves a fault detection accuracy of 95.9 %, and both
quantitative selectivity analysis and qualitative expert evaluations confirmed
the relevance and inter-pretability of the generated explanations. This
human-centric approach is designed to enhance trust and interpretability in
data-driven fault detection, thereby contributing to applied system design in
industrial quality control.

</details>


### [98] [Graph Neural Networks in Wind Power Forecasting](https://arxiv.org/abs/2507.00105)
*Javier Castellano,Ignacio Villanueva*

Main category: cs.LG

TL;DR: 研究GNNs在风能预测问题上的适用性，特定架构表现与CNN基准相当。


<details>
  <summary>Details</summary>
Motivation: 探究GNNs在风能预测问题中的适用性。

Method: 在三个风力发电设施上使用五年历史数据进行研究，采用数值天气预报（NWP）变量作为预测因子，在24到36小时的测试范围内评估模型。

Result: 特定的GNN架构在风能预测中的表现可与基于CNN的最佳基准相媲美。

Conclusion: GNNs在风能预测领域具有一定的应用潜力。

Abstract: We study the applicability of GNNs to the problem of wind energy forecasting.
We find that certain architectures achieve performance comparable to our best
CNN-based benchmark. The study is conducted on three wind power facilities
using five years of historical data. Numerical Weather Prediction (NWP)
variables were used as predictors, and models were evaluated on a 24 to 36 hour
ahead test horizon.

</details>


### [99] [Text-to-Level Diffusion Models With Various Text Encoders for Super Mario Bros](https://arxiv.org/abs/2507.00184)
*Jacob Schrum,Olivia Kilday,Emilio Salas,Bess Hagan,Reid Williams*

Main category: cs.LG

TL;DR: 本文探讨扩散模型用于文本到关卡生成，提出为现有关卡数据集自动分配描述性标题的策略，训练不同文本嵌入的扩散模型并评估生成关卡，结果显示简单变压器模型效果好且训练时间短，还提供GUI。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型在文本到关卡生成的应用研究不足，且创建可用模型存在实际考虑因素。

Method: 为现有关卡数据集自动分配描述性标题，使用预训练文本编码器和从头训练的简单变压器模型训练扩散模型，比较输入和输出标题的重叠度，评估生成关卡的多样性和可玩性。

Result: 最佳扩散模型使用简单变压器模型进行文本嵌入，训练时间比使用更复杂文本编码器的模型短。

Conclusion: 依赖大语言模型进行文本到关卡生成并非必要，还提供GUI帮助设计师构建长关卡。

Abstract: Recent research shows how diffusion models can unconditionally generate
tile-based game levels, but use of diffusion models for text-to-level
generation is underexplored. There are practical considerations for creating a
usable model: caption/level pairs are needed, as is a text embedding model, and
a way of generating entire playable levels, rather than individual scenes. We
present strategies to automatically assign descriptive captions to an existing
level dataset, and train diffusion models using both pretrained text encoders
and simple transformer models trained from scratch. Captions are automatically
assigned to generated levels so that the degree of overlap between input and
output captions can be compared. We also assess the diversity and playability
of the resulting levels. Results are compared with an unconditional diffusion
model and a generative adversarial network, as well as the text-to-level
approaches Five-Dollar Model and MarioGPT. Notably, the best diffusion model
uses a simple transformer model for text embedding, and takes less time to
train than diffusion models employing more complex text encoders, indicating
that reliance on larger language models is not necessary. We also present a GUI
allowing designers to construct long levels from model-generated scenes.

</details>


### [100] [Beyond Sensor Data: Foundation Models of Behavioral Data from Wearables Improve Health Predictions](https://arxiv.org/abs/2507.00191)
*Eray Erturk,Fahad Kamran,Salar Abbaspourazad,Sean Jewell,Harsh Sharma,Yujie Li,Sinead Williamson,Nicholas J Foti,Joseph Futoma*

Main category: cs.LG

TL;DR: 利用可穿戴设备数据开发行为信号基础模型，在57个健康相关任务表现良好，强调定制模型设计对可穿戴设备的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有基础模型多应用于低级别传感器数据，而行为数据在健康预测中更具信息性，故开发行为信号基础模型。

Method: 使用162K个人超25亿小时可穿戴数据，系统优化架构和分词策略。

Result: 模型在57个健康相关任务表现出色，在睡眠预测等行为驱动任务中表现优异，结合原始传感器数据表示时性能进一步提升。

Conclusion: 强调定制基础模型设计对可穿戴设备的重要性，展示了开发新健康应用的潜力。

Abstract: Wearable devices record physiological and behavioral signals that can improve
health predictions. While foundation models are increasingly used for such
predictions, they have been primarily applied to low-level sensor data, despite
behavioral data often being more informative due to their alignment with
physiologically relevant timescales and quantities. We develop foundation
models of such behavioral signals using over 2.5B hours of wearable data from
162K individuals, systematically optimizing architectures and tokenization
strategies for this unique dataset. Evaluated on 57 health-related tasks, our
model shows strong performance across diverse real-world applications including
individual-level classification and time-varying health state prediction. The
model excels in behavior-driven tasks like sleep prediction, and improves
further when combined with representations of raw sensor data. These results
underscore the importance of tailoring foundation model design to wearables and
demonstrate the potential to enable new health applications.

</details>


### [101] [PPFL-RDSN: Privacy-Preserving Federated Learning-based Residual Dense Spatial Networks for Encrypted Lossy Image Reconstruction](https://arxiv.org/abs/2507.00230)
*Peilin He,James Joshi*

Main category: cs.LG

TL;DR: 提出PPFL - RDSN框架用于有损图像重建，能保障数据安全和模型真实性，性能与集中式方法相当且降低计算负担。


<details>
  <summary>Details</summary>
Motivation: 在协作场景中，用RDSN从低分辨率输入重建高质量图像时，集中式训练存在隐私风险和高计算成本问题。

Method: 提出PPFL - RDSN框架，集成联邦学习、局部差分隐私和鲁棒模型水印技术。

Result: PPFL - RDSN性能与集中式方法相当，降低计算负担，有效缓解安全和隐私漏洞。

Conclusion: PPFL - RDSN是安全且保护隐私的协作计算机视觉应用的实用解决方案。

Abstract: Reconstructing high-quality images from low-resolution inputs using Residual
Dense Spatial Networks (RDSNs) is crucial yet challenging, particularly in
collaborative scenarios where centralized training poses significant privacy
risks, including data leakage and inference attacks, as well as high
computational costs. We propose a novel Privacy-Preserving Federated
Learning-based RDSN (PPFL-RDSN) framework specifically tailored for lossy image
reconstruction. PPFL-RDSN integrates Federated Learning (FL), local
differential privacy, and robust model watermarking techniques, ensuring data
remains secure on local devices, safeguarding sensitive information, and
maintaining model authenticity without revealing underlying data. Empirical
evaluations show that PPFL-RDSN achieves comparable performance to the
state-of-the-art centralized methods while reducing computational burdens, and
effectively mitigates security and privacy vulnerabilities, making it a
practical solution for secure and privacy-preserving collaborative computer
vision applications.

</details>


### [102] [Interpretable AI for Time-Series: Multi-Model Heatmap Fusion with Global Attention and NLP-Generated Explanations](https://arxiv.org/abs/2507.00234)
*Jiztom Kavalakkatt Francis,Matthew J Darr*

Main category: cs.LG

TL;DR: 提出融合ResNet和重构2D Transformer热力图的框架提升模型可解释性，在临床和工业数据集表现出色，还能将热力图转化为领域叙述。


<details>
  <summary>Details</summary>
Motivation: 解决现有可解释性方法中时空不对齐问题，卷积网络难捕捉全局信息，Transformer缺乏局部精度，影响安全关键领域决策。

Method: 将梯度加权激活图（ResNet）和Transformer注意力展开合并为统一可视化，保留实时性能；用NLP模块将融合热力图转化为领域叙述。

Result: 在临床和工业数据集上显著提升，混合框架在PhysioNet数据集准确率达94.1%，在UCI Energy Appliance数据集上降低回归误差，NLP模块经BLEU - 4和ROUGE - L分数验证。

Conclusion: 该方法以因果保真度和时空对齐形式定义可解释性，缩小技术输出与利益相关者理解差距，为透明、有时效性决策提供可扩展解决方案。

Abstract: In this paper, we present a novel framework for enhancing model
interpretability by integrating heatmaps produced separately by ResNet and a
restructured 2D Transformer with globally weighted input saliency. We address
the critical problem of spatial-temporal misalignment in existing
interpretability methods, where convolutional networks fail to capture global
context and Transformers lack localized precision - a limitation that impedes
actionable insights in safety-critical domains like healthcare and industrial
monitoring. Our method merges gradient-weighted activation maps (ResNet) and
Transformer attention rollout into a unified visualization, achieving full
spatial-temporal alignment while preserving real-time performance. Empirical
evaluations on clinical (ECG arrhythmia detection) and industrial (energy
consumption prediction) datasets demonstrate significant improvements: the
hybrid framework achieves 94.1% accuracy (F1 0.93) on the PhysioNet dataset and
reduces regression error to RMSE = 0.28 kWh (R2 = 0.95) on the UCI Energy
Appliance dataset-outperforming standalone ResNet, Transformer, and
InceptionTime baselines by 3.8-12.4%. An NLP module translates fused heatmaps
into domain-specific narratives (e.g., "Elevated ST-segment between 2-4 seconds
suggests myocardial ischemia"), validated via BLEU-4 (0.586) and ROUGE-L
(0.650) scores. By formalizing interpretability as causal fidelity and
spatial-temporal alignment, our approach bridges the gap between technical
outputs and stakeholder understanding, offering a scalable solution for
transparent, time-aware decision-making.

</details>


### [103] [Gym4ReaL: A Suite for Benchmarking Real-World Reinforcement Learning](https://arxiv.org/abs/2507.00257)
*Davide Salaorni,Vincenzo De Paola,Samuele Delpero,Giovanni Dispoto,Paolo Bonetti,Alessio Russo,Giuseppe Calcagno,Francesco Trovò,Matteo Papini,Alberto Maria Metelli,Marco Mussi,Marcello Restelli*

Main category: cs.LG

TL;DR: 本文介绍了Gym4ReaL，一套用于支持开发和评估能在现实场景中运行的强化学习算法的环境套件，实验显示标准RL算法有竞争力，激励新方法开发。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习在现实应用面临新挑战，但现有基准常忽略现实复杂性，需要开发新环境支持算法评估。

Method: 引入Gym4ReaL环境套件，包含多样化任务以呈现各种实际挑战。

Result: 标准RL算法在Gym4ReaL环境中面对基于规则的基准测试展现出竞争力。

Conclusion: 需要开发新方法以充分发挥强化学习应对现实任务复杂性的潜力。

Abstract: In recent years, \emph{Reinforcement Learning} (RL) has made remarkable
progress, achieving superhuman performance in a wide range of simulated
environments. As research moves toward deploying RL in real-world applications,
the field faces a new set of challenges inherent to real-world settings, such
as large state-action spaces, non-stationarity, and partial observability.
Despite their importance, these challenges are often underexplored in current
benchmarks, which tend to focus on idealized, fully observable, and stationary
environments, often neglecting to incorporate real-world complexities
explicitly. In this paper, we introduce \texttt{Gym4ReaL}, a comprehensive
suite of realistic environments designed to support the development and
evaluation of RL algorithms that can operate in real-world scenarios. The suite
includes a diverse set of tasks that expose algorithms to a variety of
practical challenges. Our experimental results show that, in these settings,
standard RL algorithms confirm their competitiveness against rule-based
benchmarks, motivating the development of new methods to fully exploit the
potential of RL to tackle the complexities of real-world tasks.

</details>


### [104] [Who Should I Listen To? Adaptive Collaboration in Personalized Federated Learning](https://arxiv.org/abs/2507.00259)
*Amr Abourayya,Jens Kleesiek,Bharat Rao,Michael Kamp*

Main category: cs.LG

TL;DR: 提出FEDMOSAIC方法解决联邦学习数据异质性问题，在多种非IID设置下优于现有方法并给出收敛保证。


<details>
  <summary>Details</summary>
Motivation: 现有个性化联邦学习（PFL）方法未超越本地或集中式基线，协作与数据结构不匹配，需解决数据异质性挑战。

Method: 提出基于自适应协作的方法，在FEDMOSAIC中实现，客户端在共享无标签数据集上交换预测，根据私有和公共数据一致性调整损失权重，按估计的示例置信度贡献全局伪标签。

Result: FEDMOSAIC在多种非IID设置下优于现有PFL方法，在标准假设下有收敛保证。

Conclusion: 数据感知协作对实现强大有效的个性化具有潜力。

Abstract: Data heterogeneity is a central challenge in federated learning, and
personalized federated learning (PFL) aims to address it by tailoring models to
each client's distribution. Yet many PFL methods fail to outperform local or
centralized baselines, suggesting a mismatch between the collaboration they
enforce and the structure of the data. We propose an approach based on adaptive
collaboration, where clients decide adaptively not only how much to rely on
others, but also whom to trust at the level of individual examples. We
instantiate this principle in FEDMOSAIC, a federated co-training method in
which clients exchange predictions over a shared unlabeled dataset. This
enables fine-grained trust decisions that are difficult to achieve with
parameter sharing alone. Each client adjusts its loss weighting based on the
agreement between private and public data, and contributes to global
pseudo-labels in proportion to its estimated per-example confidence.
Empirically, FEDMOSAIC improves upon state-of-the-art PFL methods across
diverse non-IID settings, and we provide convergence guarantees under standard
assumptions. Our results demonstrate the potential of data-aware collaboration
for robust and effective personalization.

</details>


### [105] [Examining Reject Relations in Stimulus Equivalence Simulations](https://arxiv.org/abs/2507.00265)
*Alexis Carrillo,Asieh Abolpour Mofrad,Anis Yazidi,Moises Betancort*

Main category: cs.LG

TL;DR: 研究用计算模型探究拒绝关系在刺激等价性习得中的作用，发现人工神经网络可能依赖联想策略而非刺激等价性。


<details>
  <summary>Details</summary>
Motivation: 模拟虽可探索刺激等价性，但拒绝关系对等价类形成评估的干扰存在争议，需研究拒绝关系在刺激等价性习得中的作用。

Method: 在样本匹配模拟的18种条件下，检验前馈神经网络、BERT和GPT，以概率代理为基准。条件在训练结构、关系类型和负比较选择方面有所不同。

Result: 拒绝关系影响代理表现，部分代理在等价测试中准确率高，但表现与概率代理相当。

Conclusion: 人工神经网络可能依赖联想策略，计算模型中需谨慎考虑拒绝关系并采用更严格标准。

Abstract: Simulations offer a valuable tool for exploring stimulus equivalence (SE),
yet the potential of reject relations to disrupt the assessment of equivalence
class formation is contentious. This study investigates the role of reject
relations in the acquisition of stimulus equivalence using computational
models. We examined feedforward neural networks (FFNs), bidirectional encoder
representations from transformers (BERT), and generative pre-trained
transformers (GPT) across 18 conditions in matching-to-sample (MTS)
simulations. Conditions varied in training structure (linear series,
one-to-many, and many-to-one), relation type (select-only, reject-only, and
select-reject), and negative comparison selection (standard and biased). A
probabilistic agent served as a benchmark, embodying purely associative
learning. The primary goal was to determine whether artificial neural networks
could demonstrate equivalence class formation or whether their performance
reflected associative learning. Results showed that reject relations influenced
agent performance. While some agents achieved high accuracy on equivalence
tests, particularly with reject relations and biased negative comparisons, this
performance was comparable to the probabilistic agent. These findings suggest
that artificial neural networks, including transformer models, may rely on
associative strategies rather than SE. This underscores the need for careful
consideration of reject relations and more stringent criteria in computational
models of equivalence.

</details>


### [106] [Double Q-learning for Value-based Deep Reinforcement Learning, Revisited](https://arxiv.org/abs/2507.00275)
*Prabhat Nagarajan,Martha White,Marlos C. Machado*

Main category: cs.LG

TL;DR: 研究Deep Double Q - learning (DDQL)，发现其减少高估且在57个Atari 2600游戏中总体表现优于Double DQN，无需额外超参数。


<details>
  <summary>Details</summary>
Motivation: 理解DDQL是否比Double DQN高估更少，是否存在高性能的DDQL实例。

Method: 研究适应Double Q - learning核心思想用于基于价值的深度强化学习的算法（即DDQL），还研究了DDQL的网络架构、回放比率和小批量采样策略。

Result: DDQL减少了高估，在57个Atari 2600游戏中总体表现优于Double DQN，无需额外超参数。

Conclusion: 肯定地回答了研究目标中的两个问题，即DDQL高估更少且存在高性能实例。

Abstract: Overestimation is pervasive in reinforcement learning (RL), including in
Q-learning, which forms the algorithmic basis for many value-based deep RL
algorithms. Double Q-learning is an algorithm introduced to address
Q-learning's overestimation by training two Q-functions and using both to
de-correlate action-selection and action-evaluation in bootstrap targets.
Shortly after Q-learning was adapted to deep RL in the form of deep Q-networks
(DQN), Double Q-learning was adapted to deep RL in the form of Double DQN.
However, Double DQN only loosely adapts Double Q-learning, forgoing the
training of two different Q-functions that bootstrap off one another. In this
paper, we study algorithms that adapt this core idea of Double Q-learning for
value-based deep RL. We term such algorithms Deep Double Q-learning (DDQL). Our
aim is to understand whether DDQL exhibits less overestimation than Double DQN
and whether performant instantiations of DDQL exist. We answer both questions
affirmatively, demonstrating that DDQL reduces overestimation and outperforms
Double DQN in aggregate across 57 Atari 2600 games, without requiring
additional hyperparameters. We also study several aspects of DDQL, including
its network architecture, replay ratio, and minibatch sampling strategy.

</details>


### [107] [Structure-preserving Lift & Learn: Scientific machine learning for nonlinear conservative partial differential equations](https://arxiv.org/abs/2507.00301)
*Harsh Sharma,Juan Diego Draxl Giannoni,Boris Kramer*

Main category: cs.LG

TL;DR: 提出结构保留的Lift & Learn方法学习非线性偏微分方程的降阶模型，通过三个数值例子验证其有效性和竞争力。


<details>
  <summary>Details</summary>
Motivation: 为具有守恒律的非线性偏微分方程学习结构保留的降阶模型。

Method: 采用提升变量变换，基于能量二次化策略提出混合学习方法，推导二次降阶项并构建约束优化问题。

Result: 所提方法得到的二次降阶模型计算高效，且通过数值例子表明该方法在准确性和计算效率上与现有方法有竞争力。

Conclusion: 结构保留的Lift & Learn方法具有良好的泛化性，在处理非线性偏微分方程降阶问题上表现出色。

Abstract: This work presents structure-preserving Lift & Learn, a scientific machine
learning method that employs lifting variable transformations to learn
structure-preserving reduced-order models for nonlinear partial differential
equations (PDEs) with conservation laws. We propose a hybrid learning approach
based on a recently developed energy-quadratization strategy that uses
knowledge of the nonlinearity at the PDE level to derive an equivalent
quadratic lifted system with quadratic system energy. The lifted dynamics
obtained via energy quadratization are linear in the old variables, making
model learning very effective in the lifted setting. Based on the lifted
quadratic PDE model form, the proposed method derives quadratic reduced terms
analytically and then uses those derived terms to formulate a constrained
optimization problem to learn the remaining linear reduced operators in a
structure-preserving way. The proposed hybrid learning approach yields
computationally efficient quadratic reduced-order models that respect the
underlying physics of the high-dimensional problem. We demonstrate the
generalizability of quadratic models learned via the proposed
structure-preserving Lift & Learn method through three numerical examples: the
one-dimensional wave equation with exponential nonlinearity, the
two-dimensional sine-Gordon equation, and the two-dimensional
Klein-Gordon-Zakharov equations. The numerical results show that the proposed
learning approach is competitive with the state-of-the-art structure-preserving
data-driven model reduction method in terms of both accuracy and computational
efficiency.

</details>


### [108] [MamNet: A Novel Hybrid Model for Time-Series Forecasting and Frequency Pattern Analysis in Network Traffic](https://arxiv.org/abs/2507.00304)
*Yujun Zhang,Runlong Li,Xiaoxiang Liang,Xinhao Yang,Tian Su,Bo Liu,Yan Zhou*

Main category: cs.LG

TL;DR: 本文提出MamNet模型用于网络流量预测和异常检测，结合时频域分析，实验表明其性能优于主流模型，未来可结合外部信息优化。


<details>
  <summary>Details</summary>
Motivation: 网络流量异常波动可能预示安全威胁或系统故障，需要高效的流量预测和异常检测方法保障网络安全和管理。

Method: 提出MamNet模型，通过Mamba模块进行时域建模捕捉长期依赖，用傅里叶变换进行频域特征提取识别周期性波动，在特征融合层整合多尺度信息。

Result: 在UNSW - NB15和CAIDA数据集上实验，MamNet在准确率、召回率和F1分数上优于主流模型，复杂流量模式和长期趋势检测性能提升2% - 4%。

Conclusion: MamNet能有效捕捉不同时间尺度的网络流量异常，适用于网络安全和流量管理的异常检测任务，未来可结合外部网络事件信息优化模型。

Abstract: The abnormal fluctuations in network traffic may indicate potential security
threats or system failures. Therefore, efficient network traffic prediction and
anomaly detection methods are crucial for network security and traffic
management. This paper proposes a novel network traffic prediction and anomaly
detection model, MamNet, which integrates time-domain modeling and
frequency-domain feature extraction. The model first captures the long-term
dependencies of network traffic through the Mamba module (time-domain
modeling), and then identifies periodic fluctuations in the traffic using
Fourier Transform (frequency-domain feature extraction). In the feature fusion
layer, multi-scale information is integrated to enhance the model's ability to
detect network traffic anomalies. Experiments conducted on the UNSW-NB15 and
CAIDA datasets demonstrate that MamNet outperforms several recent mainstream
models in terms of accuracy, recall, and F1-Score. Specifically, it achieves an
improvement of approximately 2% to 4% in detection performance for complex
traffic patterns and long-term trend detection. The results indicate that
MamNet effectively captures anomalies in network traffic across different time
scales and is suitable for anomaly detection tasks in network security and
traffic management. Future work could further optimize the model structure by
incorporating external network event information, thereby improving the model's
adaptability and stability in complex network environments.

</details>


### [109] [Open-ended Scientific Discovery via Bayesian Surprise](https://arxiv.org/abs/2507.00310)
*Dhruv Agarwal,Bodhisattwa Prasad Majumder,Reece Adamson,Megha Chakravorty,Satvika Reddy Gavireddy,Aditya Parashar,Harshit Surana,Bhavana Dalvi Mishra,Andrew McCallum,Ashish Sabharwal,Peter Clark*

Main category: cs.LG

TL;DR: 本文提出AutoDS方法用于开放式自主科学发现（ASD），以贝叶斯惊喜驱动探索，在21个真实数据集上评估，表现优于竞争对手，向构建开放式ASD系统迈进重要一步。


<details>
  <summary>Details</summary>
Motivation: 现有开放式ASD方法存在不足，如基于多样性启发式的方法难以有效探索假设空间，基于主观代理的方法定义不精确，需更好方法推动科学探索。

Method: 提出AutoDS方法，用贝叶斯惊喜驱动科学探索，通过量化LLM先验和后验信念的认知转变，采用蒙特卡罗树搜索（MCTS）策略，以惊喜值为奖励函数。

Result: 在固定预算下，AutoDS比竞争对手多产生5 - 29%被LLM认为是惊喜的发现，三分之二的发现令领域专家感到惊喜。

Conclusion: AutoDS是构建开放式ASD系统的重要进步。

Abstract: The promise of autonomous scientific discovery (ASD) hinges not only on
answering questions, but also on knowing which questions to ask. Most recent
works in ASD explore the use of large language models (LLMs) in goal-driven
settings, relying on human-specified research questions to guide hypothesis
generation. However, scientific discovery may be accelerated further by
allowing the AI system to drive exploration by its own criteria. The few
existing approaches in open-ended ASD select hypotheses based on diversity
heuristics or subjective proxies for human interestingness, but the former
struggles to meaningfully navigate the typically vast hypothesis space, and the
latter suffers from imprecise definitions. This paper presents AutoDS -- a
method for open-ended ASD that instead drives scientific exploration using
Bayesian surprise. Here, we quantify the epistemic shift from the LLM's prior
beliefs about a hypothesis to its posterior beliefs after gathering
experimental results. To efficiently explore the space of nested hypotheses,
our method employs a Monte Carlo tree search (MCTS) strategy with progressive
widening using surprisal as the reward function. We evaluate AutoDS in the
setting of data-driven discovery across 21 real-world datasets spanning domains
such as biology, economics, finance, and behavioral science. Our results
demonstrate that under a fixed budget, AutoDS substantially outperforms
competitors by producing 5--29\% more discoveries deemed surprising by the LLM.
Our human evaluation further finds that two-thirds of AutoDS discoveries are
surprising to the domain experts, suggesting this is an important step forward
towards building open-ended ASD systems.

</details>


### [110] [$μ^2$Tokenizer: Differentiable Multi-Scale Multi-Modal Tokenizer for Radiology Report Generation](https://arxiv.org/abs/2507.00316)
*Siyou Li,Pengyao Qin,Huanan Wu,Dong Nie,Arun J. Thirunavukarasu,Juntao Yu,Le Zhang*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Automated radiology report generation (RRG) aims to produce detailed textual
reports from clinical imaging, such as computed tomography (CT) scans, to
improve the accuracy and efficiency of diagnosis and provision of management
advice. RRG is complicated by two key challenges: (1) inherent complexity in
extracting relevant information from imaging data under resource constraints,
and (2) difficulty in objectively evaluating discrepancies between
model-generated and expert-written reports. To address these challenges, we
propose $\mu^2$LLM, a $\underline{\textbf{mu}}$ltiscale
$\underline{\textbf{mu}}$ltimodal large language models for RRG tasks. The
novel ${\mu}^2$Tokenizer, as an intermediate layer, integrates multi-modal
features from the multiscale visual tokenizer and the text tokenizer, then
enhances report generation quality through direct preference optimization
(DPO), guided by GREEN-RedLlama. Experimental results on four large CT
image-report medical datasetdemonstrate that our method outperforms existing
approaches, highlighting the potential of our fine-tuned $\mu^2$LLMs on limited
data for RRG tasks.

</details>


### [111] [Exploring Theory-Laden Observations in the Brain Basis of Emotional Experience](https://arxiv.org/abs/2507.00320)
*Christiana Westlin,Ashutosh Singh,Deniz Erdogmus,Georgios Stratis,Lisa Feldman Barrett*

Main category: cs.LG

TL;DR: 重新分析一项情感类型学研究的数据，发现起始假设会影响科学结论，假设需多方法验证。


<details>
  <summary>Details</summary>
Motivation: 挑战传统观点，即民间情感类别构成生物和心理类型学，研究常按此假设设计和分析。

Method: 以情感类别是可变、情境化实例群体的观点，重新分析一项类型学导向研究的数据，对数据方差结构作最少假设。

Result: 未观察到原研究的映射关系，发现个体间存在显著差异。

Conclusion: 起始假设会影响科学结论，假设需用多种分析方法支持才能被认真对待。

Abstract: In the science of emotion, it is widely assumed that folk emotion categories
form a biological and psychological typology, and studies are routinely
designed and analyzed to identify emotion-specific patterns. This approach
shapes the observations that studies report, ultimately reinforcing the
assumption that guided the investigation. Here, we reanalyzed data from one
such typologically-guided study that reported mappings between individual brain
patterns and group-averaged ratings of 34 emotion categories. Our reanalysis
was guided by an alternative view of emotion categories as populations of
variable, situated instances, and which predicts a priori that there will be
significant variation in brain patterns within a category across instances.
Correspondingly, our analysis made minimal assumptions about the structure of
the variance present in the data. As predicted, we did not observe the original
mappings and instead observed significant variation across individuals. These
findings demonstrate how starting assumptions can ultimately impact scientific
conclusions and suggest that a hypothesis must be supported using multiple
analytic methods before it is taken seriously.

</details>


### [112] [Data-Driven Exploration for a Class of Continuous-Time Linear--Quadratic Reinforcement Learning Problems](https://arxiv.org/abs/2507.00358)
*Yilie Huang,Xun Yu Zhou*

Main category: cs.LG

TL;DR: 研究连续时间随机线性 - 二次控制问题的强化学习，提出自适应探索机制，提升学习效率，实现次线性遗憾界，数值实验显示优势。


<details>
  <summary>Details</summary>
Motivation: 已有方法需大量调参且忽略学习进度，为提升学习效率。

Method: 提出无模型、数据驱动的探索机制，通过评判器自适应调整熵正则化，通过行动器调整策略方差。

Result: 实现了与该类线性 - 二次问题最佳已知无模型结果匹配的次线性遗憾界，数值实验表明自适应探索加速收敛并改善遗憾性能。

Conclusion: 自适应探索机制提升了学习效率，优于非自适应无模型和基于模型的方法。

Abstract: We study reinforcement learning (RL) for the same class of continuous-time
stochastic linear--quadratic (LQ) control problems as in
\cite{huang2024sublinear}, where volatilities depend on both states and
controls while states are scalar-valued and running control rewards are absent.
We propose a model-free, data-driven exploration mechanism that adaptively
adjusts entropy regularization by the critic and policy variance by the actor.
Unlike the constant or deterministic exploration schedules employed in
\cite{huang2024sublinear}, which require extensive tuning for implementations
and ignore learning progresses during iterations, our adaptive exploratory
approach boosts learning efficiency with minimal tuning. Despite its
flexibility, our method achieves a sublinear regret bound that matches the
best-known model-free results for this class of LQ problems, which were
previously derived only with fixed exploration schedules. Numerical experiments
demonstrate that adaptive explorations accelerate convergence and improve
regret performance compared to the non-adaptive model-free and model-based
counterparts.

</details>


### [113] [MoNE: Replacing Redundant Experts with Lightweight Novices for Structured Pruning of MoE](https://arxiv.org/abs/2507.00390)
*Geng Zhang,Yuxuan Han,Yuxuan Lou,Wangbo Zhao,Yiqi Zhang,Yang You*

Main category: cs.LG

TL;DR: 本文提出Mixture-of-Novices-and-Experts (MoNE)专家剪枝方法，用轻量级新手替代冗余专家实现模型压缩，实验表明其在多维度表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有基于MoE的模型部署有显著内存开销，现有结构化剪枝方法在模型架构、校准数据源和样本大小三个维度表现不佳。

Method: MoNE基于访问频率和输出方差评估专家冗余，剪枝低使用和输出稳定的专家，用轻量级新手替代。

Result: MoNE在三个维度上始终优于基线方法，准确率下降最小，在25%和50%剪枝率下分别提高平均零样本准确率2.71和3.61。

Conclusion: MoNE是一种有效且鲁棒的模型压缩方法。

Abstract: Mixture-of-Experts (MoE) enables efficient scaling of large language models
by activating only a subset of experts per input token. However, deploying
MoE-based models incurs significant memory overhead due to the need to retain
all experts in memory. While structured pruning is promising to reduce memory
costs, existing methods often show suboptimal performance and unstable
degradation in three dimensions: model architectures, calibration data sources,
and calibration sample sizes. This paper proposes
Mixture-of-Novices-and-Experts (MoNE), a novel expert pruning method that
replaces redundant experts with lightweight novices to achieve effective and
robust model compression. MoNE evaluates expert redundancy based on two
metrics: access frequency and output variance. Experts exhibiting low usage and
stable outputs are pruned and replaced with lightweight novices-unbiased
estimations of their original outputs-minimizing performance degradation.
Extensive experiments demonstrate that MoNE consistently outperforms baseline
methods with minimal accuracy degradation across the three dimensions,
confirming its effectiveness and robustness. Notably, it improves the average
zero shot accuracy across nine downstream tasks by up to 2.71 under 25\%
pruning ratio and 3.61 under 50\% pruning. The code is available at
https://github.com/zxgx/mode-pd.

</details>


### [114] [Diffusion Disambiguation Models for Partial Label Learning](https://arxiv.org/abs/2507.00411)
*Jinfu Fan,Xiaohui Zhong,Kangrui Ren,Jiangnan Li,Linqing Huang*

Main category: cs.LG

TL;DR: 本文利用扩散模型解决部分标签学习（PLL）中的标签消歧问题，提出DDMP模型，实验证明其优势。


<details>
  <summary>Details</summary>
Motivation: 学习模糊标签是实际机器学习应用中的长期问题，部分标签学习需从候选标签中识别真实标签，探索扩散模型在标签消歧中的潜力。

Method: 从生成模型角度重新表述标签消歧问题，提出DDMP模型，利用实例和标签的互补信息构建伪干净标签进行初始扩散训练，引入过渡感知矩阵估计潜在真实标签并动态更新。

Result: 实验显示了DDMP模型的优势以及其对PLL的适用性。

Conclusion: DDMP模型在部分标签学习的标签消歧问题上有良好表现。

Abstract: Learning from ambiguous labels is a long-standing problem in practical
machine learning applications. The purpose of \emph{partial label learning}
(PLL) is to identify the ground-truth label from a set of candidate labels
associated with a given instance. Inspired by the remarkable performance of
diffusion models in various generation tasks, this paper explores their
potential to denoise ambiguous labels through the reverse denoising process.
Therefore, this paper reformulates the label disambiguation problem from the
perspective of generative models, where labels are generated by iteratively
refining initial random guesses. This perspective enables the diffusion model
to learn how label information is generated stochastically. By modeling the
generation uncertainty, we can use the maximum likelihood estimate of the label
for classification inference. However, such ambiguous labels lead to a mismatch
between instance and label, which reduces the quality of generated data. To
address this issue, this paper proposes a \emph{diffusion disambiguation model
for PLL} (DDMP), which first uses the potential complementary information
between instances and labels to construct pseudo-clean labels for initial
diffusion training. Furthermore, a transition-aware matrix is introduced to
estimate the potential ground-truth labels, which are dynamically updated
during the diffusion generation. During training, the ground-truth label is
progressively refined, improving the classifier. Experiments show the advantage
of the DDMP and its suitability for PLL.

</details>


### [115] [Flexible Language Modeling in Continuous Space with Transformer-based Autoregressive Flows](https://arxiv.org/abs/2507.00425)
*Ruixiang Zhang,Shuangfei Zhai,Jiatao Gu,Yizhe Zhang,Huangjie Zheng,Tianrong Chen,Miguel Angel Bautista,Josh Susskind,Navdeep Jaitly*

Main category: cs.LG

TL;DR: 本文探索将语言建模从离散令牌空间转移到连续潜在空间，提出TarFlowLM框架，实验证明其性能强且建模灵活。


<details>
  <summary>Details</summary>
Motivation: 自回归模型基于离散令牌等因素虽成功但限制了建模灵活性，需探索新设计空间。

Method: 提出TarFlowLM框架，采用基于Transformer的自回归归一化流对连续表示建模，提出新的基于混合的耦合变换。

Result: 在语言建模基准测试中展现出强大的似然性能。

Conclusion: 所提出的框架具有灵活的建模能力。

Abstract: Autoregressive models have driven remarkable progress in language modeling.
Their foundational reliance on discrete tokens, unidirectional context, and
single-pass decoding, while central to their success, also inspires the
exploration of a design space that could offer new axes of modeling
flexibility. In this work, we explore an alternative paradigm, shifting
language modeling from a discrete token space to a continuous latent space. We
propose a novel framework TarFlowLM, that employs transformer-based
autoregressive normalizing flows to model these continuous representations.
This approach unlocks substantial flexibility, enabling the construction of
models that can capture global bi-directional context through stacked,
alternating-direction autoregressive transformations, support block-wise
generation with flexible token patch sizes, and facilitate a hierarchical
multi-pass generation process. We further propose new mixture-based coupling
transformations designed to capture complex dependencies within the latent
space shaped by discrete data, and demonstrate theoretical connections to
conventional discrete autoregressive models. Extensive experiments on language
modeling benchmarks demonstrate strong likelihood performance and highlight the
flexible modeling capabilities inherent in our framework.

</details>


### [116] [A Recipe for Causal Graph Regression: Confounding Effects Revisited](https://arxiv.org/abs/2507.00440)
*Yujia Yin,Tianyi Qu,Zihao Wang,Yifan Chen*

Main category: cs.LG

TL;DR: 本文致力于解决因果图回归（CGR）问题，通过重塑对混杂效应的处理和运用对比学习推广因果干预技术，实验验证了方法有效性。


<details>
  <summary>Details</summary>
Motivation: 现有因果图学习（CGL）技术的成功主要体现在分类场景，而图学习中更具挑战性的回归任务被忽视，因此要解决CGR问题。

Method: 重塑现有CGL研究中主要针对分类的混杂效应处理方式，从对比学习的角度将特定于分类的因果干预技术推广到回归。

Result: 在图OOD基准上的大量实验验证了所提方法对CGR的有效性。

Conclusion: 提出的方法能有效解决因果图回归问题，代码可在https://github.com/causal - graph/CGR获取。

Abstract: Through recognizing causal subgraphs, causal graph learning (CGL) has risen
to be a promising approach for improving the generalizability of graph neural
networks under out-of-distribution (OOD) scenarios. However, the empirical
successes of CGL techniques are mostly exemplified in classification settings,
while regression tasks, a more challenging setting in graph learning, are
overlooked. We thus devote this work to tackling causal graph regression (CGR);
to this end we reshape the processing of confounding effects in existing CGL
studies, which mainly deal with classification. Specifically, we reflect on the
predictive power of confounders in graph-level regression, and generalize
classification-specific causal intervention techniques to regression through a
lens of contrastive learning. Extensive experiments on graph OOD benchmarks
validate the efficacy of our proposals for CGR. The model implementation and
the code are provided on https://github.com/causal-graph/CGR.

</details>


### [117] [Iterative Distillation for Reward-Guided Fine-Tuning of Diffusion Models in Biomolecular Design](https://arxiv.org/abs/2507.00445)
*Xingyu Su,Xiner Li,Masatoshi Uehara,Sunwoo Kim,Yulai Zhao,Gabriele Scalia,Ehsan Hajiramezanali,Tommaso Biancalani,Degui Zhi,Shuiwang Ji*

Main category: cs.LG

TL;DR: 提出迭代蒸馏微调框架，解决生物分子设计中扩散模型奖励引导生成微调问题，优于现有RL方法。


<details>
  <summary>Details</summary>
Motivation: 现实应用需对扩散模型基于不可微奖励函数优化，现有RL方法存在不稳定、样本效率低和模式崩溃问题。

Method: 将问题转化为策略蒸馏，在滚动阶段收集离策略数据，滚动输出时模拟基于奖励的软最优策略，通过最小化模拟软最优策略与当前模型策略的KL散度更新模型。

Result: 实证结果表明该方法在蛋白质、小分子和调控DNA设计的不同任务中有效且奖励优化更优。

Conclusion: 所提迭代蒸馏微调框架能使扩散模型针对任意奖励函数优化，相比现有RL方法提升训练稳定性和样本效率。

Abstract: We address the problem of fine-tuning diffusion models for reward-guided
generation in biomolecular design. While diffusion models have proven highly
effective in modeling complex, high-dimensional data distributions, real-world
applications often demand more than high-fidelity generation, requiring
optimization with respect to potentially non-differentiable reward functions
such as physics-based simulation or rewards based on scientific knowledge.
Although RL methods have been explored to fine-tune diffusion models for such
objectives, they often suffer from instability, low sample efficiency, and mode
collapse due to their on-policy nature. In this work, we propose an iterative
distillation-based fine-tuning framework that enables diffusion models to
optimize for arbitrary reward functions. Our method casts the problem as policy
distillation: it collects off-policy data during the roll-in phase, simulates
reward-based soft-optimal policies during roll-out, and updates the model by
minimizing the KL divergence between the simulated soft-optimal policy and the
current model policy. Our off-policy formulation, combined with KL divergence
minimization, enhances training stability and sample efficiency compared to
existing RL-based methods. Empirical results demonstrate the effectiveness and
superior reward optimization of our approach across diverse tasks in protein,
small molecule, and regulatory DNA design.

</details>


### [118] [Overcoming Long-Context Limitations of State-Space Models via Context-Dependent Sparse Attention](https://arxiv.org/abs/2507.00449)
*Zhihao Zhan,Jianan Zhao,Zhaocheng Zhu,Jian Tang*

Main category: cs.LG

TL;DR: 本文聚焦提升SSM长上下文建模能力，提出联合召回任务，证明SSM在解决多查询联合召回的局限性，提出结合CDSA的方案及HAX，实验显示HAX表现更佳。


<details>
  <summary>Details</summary>
Motivation: 主流Transformer架构处理长序列时间复杂度高，SSM难以有效捕捉长距离依赖，需提升其长上下文建模能力。

Method: 提出联合召回任务，证明SSM局限性，提出结合CDSA的方案和HAX。

Result: 在合成和真实世界长上下文基准测试中，HAX始终优于SSM基线和结合CISA的SSM。

Conclusion: HAX能有效提升SSM的长上下文建模能力，在长上下文建模上表现更好。

Abstract: Efficient long-context modeling remains a critical challenge for natural
language processing (NLP), as the time complexity of the predominant
Transformer architecture scales quadratically with the sequence length. While
state-space models (SSMs) offer alternative sub-quadratic solutions, they
struggle to capture long-range dependencies effectively. In this work, we focus
on analyzing and improving the long-context modeling capabilities of SSMs. We
show that the widely used synthetic task, associative recall, which requires a
model to recall a value associated with a single key without context,
insufficiently represents the complexities of real-world long-context modeling.
To address this limitation, we extend the associative recall to a novel
synthetic task, \emph{joint recall}, which requires a model to recall the value
associated with a key given in a specified context. Theoretically, we prove
that SSMs do not have the expressiveness to solve multi-query joint recall in
sub-quadratic time complexity. To resolve this issue, we propose a solution
based on integrating SSMs with Context-Dependent Sparse Attention (CDSA), which
has the expressiveness to solve multi-query joint recall with sub-quadratic
computation. To bridge the gap between theoretical analysis and real-world
applications, we propose locality-sensitive Hashing Attention with sparse Key
Selection (HAX), which instantiates the theoretical solution and is further
tailored to natural language domains. Extensive experiments on both synthetic
and real-world long-context benchmarks show that HAX consistently outperforms
SSM baselines and SSMs integrated with context-independent sparse attention
(CISA).

</details>


### [119] [Recurrent Memory-Augmented Transformers with Chunked Attention for Long-Context Language Modeling](https://arxiv.org/abs/2507.00453)
*Ankit Kashyap*

Main category: cs.LG

TL;DR: 提出结合全局注意力、分块局部注意力和门控FIFO内存机制的Transformer架构，可高效处理长短依赖，适用于多种任务。


<details>
  <summary>Details</summary>
Motivation: 解决长上下文语言建模中高效处理长短依赖且不增加注意力成本的问题。

Method: 将全局注意力与分块局部注意力、门控FIFO内存机制结合，用门控更新机制存储过去的token表示，对每个注意力头应用旋转位置编码，用PyTorch从头实现架构。

Result: 实现了一种轻量级、可扩展的架构。

Conclusion: 该架构适用于对话建模、代码补全和文档理解等任务。

Abstract: We present a Transformer architecture for long-context language modeling that
combines global attention with two biologically inspired components: chunked
local attention and a gated FIFO memory mechanism. This unified attention block
allows the model to efficiently handle both short-range and long-range
dependencies without increasing attention cost quadratically. The memory module
persistently stores past token representations using a gated update mechanism
inspired by recurrent networks. Rotary positional encoding is applied per
attention head to enable directionally disentangled, scale-invariant positional
signals. The architecture is implemented entirely from scratch in PyTorch, with
no reliance on high-level libraries, enabling transparent and modular
experimentation. Our model offers a lightweight and extensible design for tasks
such as dialogue modeling, code completion, and document understanding.

</details>


### [120] [Diversity Conscious Refined Random Forest](https://arxiv.org/abs/2507.00467)
*Sijan Bhattarai,Saurav Bhandari,Girija Bhusal,Saroj Shakya,Tapendra Pandey*

Main category: cs.LG

TL;DR: 本文提出精炼随机森林分类器，通过去除特征和冗余树提高分类准确率。


<details>
  <summary>Details</summary>
Motivation: 传统随机森林依赖大量树和所有特征，导致推理成本高和模型冗余，需改进。

Method: 提出精炼随机森林分类器，先去除非信息特征，确定新树数量，再基于相关性聚类去除冗余树。

Result: 在8个基准数据集上实验表明，该模型比标准随机森林准确率更高。

Conclusion: 精炼随机森林分类器能有效提高分类准确率。

Abstract: Random Forest (RF) is a widely used ensemble learning technique known for its
robust classification performance across diverse domains. However, it often
relies on hundreds of trees and all input features, leading to high inference
cost and model redundancy. In this work, our goal is to grow trees dynamically
only on informative features and then enforce maximal diversity by clustering
and retaining uncorrelated trees. Therefore, we propose a Refined Random Forest
Classifier that iteratively refines itself by first removing the least
informative features and then analytically determines how many new trees should
be grown, followed by correlation-based clustering to remove redundant trees.
The classification accuracy of our model was compared against the standard RF
on the same number of trees. Experiments on 8 multiple benchmark datasets,
including binary and multiclass datasets, demonstrate that the proposed model
achieves improved accuracy compared to standard RF.

</details>


### [121] [PNAct: Crafting Backdoor Attacks in Safe Reinforcement Learning](https://arxiv.org/abs/2507.00485)
*Weiran Guo,Guanjun Liu,Ziyuan Zhou,Ling Wang*

Main category: cs.LG

TL;DR: 文章指出安全强化学习（Safe RL）易受后门攻击，提出首个涉及正负动作样本（PNAct）的攻击框架，进行实验评估，强调Safe RL潜在风险和攻击可行性。


<details>
  <summary>Details</summary>
Motivation: 发现Safe RL易受后门攻击，可能使智能体执行不安全动作，为揭示其潜在风险开展研究。

Method: 引入Safe RL后门攻击相关概念和评估指标，提出含PNAct的攻击框架，理论分析PNAct性质并设计攻击算法，用既定指标实验评估。

Result: 通过实验验证了所提后门攻击框架的有效性。

Conclusion: Safe RL存在潜在风险，此类攻击具有可行性。

Abstract: Reinforcement Learning (RL) is widely used in tasks where agents interact
with an environment to maximize rewards. Building on this foundation, Safe
Reinforcement Learning (Safe RL) incorporates a cost metric alongside the
reward metric, ensuring that agents adhere to safety constraints during
decision-making. In this paper, we identify that Safe RL is vulnerable to
backdoor attacks, which can manipulate agents into performing unsafe actions.
First, we introduce the relevant concepts and evaluation metrics for backdoor
attacks in Safe RL. It is the first attack framework in the Safe RL field that
involves both Positive and Negative Action sample (PNAct) is to implant
backdoors, where positive action samples provide reference actions and negative
action samples indicate actions to be avoided. We theoretically point out the
properties of PNAct and design an attack algorithm. Finally, we conduct
experiments to evaluate the effectiveness of our proposed backdoor attack
framework, evaluating it with the established metrics. This paper highlights
the potential risks associated with Safe RL and underscores the feasibility of
such attacks. Our code and supplementary material are available at
https://github.com/azure-123/PNAct.

</details>


### [122] [Foundation Models for Clinical Records at Health System Scale](https://arxiv.org/abs/2507.00574)
*Haresh Rengaraj Rajamohan,Xiang Gao,Weicheng Zhu,Shih-Lun Huang,Long Chen,Kyunghyun Cho,Cem M. Deniz,Narges Razavian*

Main category: cs.LG

TL;DR: 提出用于顺序电子健康记录 (EHR) 数据的生成式预训练策略，评估其在预测疾病发病率上表现良好。


<details>
  <summary>Details</summary>
Motivation: 大规模预训练在医疗保健领域使用结构化电子健康记录 (EHR) 的潜力尚未充分挖掘。

Method: 使用下一次就诊事件预测对顺序 EHR 数据进行生成式预训练，对预测重复事件进行正则化。

Result: 模型在预测痴呆症和膝骨关节炎发病率的零样本预测中表现与完全微调的掩码预训练 Transformer 基线相当。

Conclusion: 该方法无需昂贵的特定任务微调即可捕捉复杂的临床依赖关系。

Abstract: Large-scale pretraining has transformed modeling of language and other data
types, but its potential remains underexplored in healthcare with structured
electronic health records (EHRs). We present a novel generative pretraining
strategy for sequential EHR data using next-visit event prediction. Our model
learns to autoregressively generate various tokenized clinical events for the
next visit based on patient history and inherently handles the joint prediction
of heterogeneous data types. Additionally, we introduce regularization on
predicting repeated events and highlight a key pitfall in EHR-based foundation
model evaluations: repeated event tokens can inflate performance metrics when
new onsets are not distinguished from subsequent occurrences. Our model is
evaluated via zero-shot prediction for forecasting dementia and knee
osteoarthritis incidence within 2 and 5 years, and the model performance rivals
a fully fine-tuned masked pretrained Transformer baseline, demonstrating that
our approach captures complex clinical dependencies without requiring costly
task-specific fine-tuning.

</details>


### [123] [Quantum Circuit Structure Optimization for Quantum Reinforcement Learning](https://arxiv.org/abs/2507.00589)
*Seok Bin Son,Joongheon Kim*

Main category: cs.LG

TL;DR: 本文提出QRL - NAS算法优化量子强化学习中参数化量子电路（PQC）结构，实验证明其比固定电路的QRL更有效。


<details>
  <summary>Details</summary>
Motivation: 传统量子强化学习（QRL）使用基于经验直觉的固定PQC结构，未验证其最优性，为解决此问题提出QRL - NAS算法。

Method: 将量子神经架构搜索（QNAS）集成到QRL中，以优化PQC结构。

Result: 实验表明QRL - NAS比使用固定电路的QRL获得更高奖励。

Conclusion: QRL - NAS算法有效且具有实用价值。

Abstract: Reinforcement learning (RL) enables agents to learn optimal policies through
environmental interaction. However, RL suffers from reduced learning efficiency
due to the curse of dimensionality in high-dimensional spaces. Quantum
reinforcement learning (QRL) addresses this issue by leveraging superposition
and entanglement in quantum computing, allowing efficient handling of
high-dimensional problems with fewer resources. QRL combines quantum neural
networks (QNNs) with RL, where the parameterized quantum circuit (PQC) acts as
the core computational module. The PQC performs linear and nonlinear
transformations through gate operations, similar to hidden layers in classical
neural networks. Previous QRL studies, however, have used fixed PQC structures
based on empirical intuition without verifying their optimality. This paper
proposes a QRL-NAS algorithm that integrates quantum neural architecture search
(QNAS) to optimize PQC structures within QRL. Experiments demonstrate that
QRL-NAS achieves higher rewards than QRL with fixed circuits, validating its
effectiveness and practical utility.

</details>


### [124] [Residual Reward Models for Preference-based Reinforcement Learning](https://arxiv.org/abs/2507.00611)
*Chenyang Cao,Miguel Rogel-García,Mohamed Nabail,Xueqian Wang,Nicholas Rhinehart*

Main category: cs.LG

TL;DR: 本文提出残差奖励模型（RRM）以有效利用先验知识，在Meta - World环境套件和真实机器人上实验，结果显示该方法显著提升常见基于偏好强化学习（PbRL）方法性能并加速策略学习。


<details>
  <summary>Details</summary>
Motivation: PbRL收敛速度慢，且在预训练和微调使用不同损失函数时，对神经网络模型的可靠优化带来挑战，需有效利用先验知识的方法。

Method: 提出残差奖励模型（RRM），将环境真实奖励拆分为先验奖励和学习奖励，先验奖励在训练前可得，学习奖励通过偏好训练，引入基于状态和图像的RRM版本。

Result: 在Meta - World环境套件实验中，方法大幅提升常见PbRL方法性能；对多种先验奖励类型均有性能提升；在真实机器人实验中，显著加速不同任务的策略学习，比基线在更少步骤取得成功。

Conclusion: 所提出的RRM方法能有效利用先验知识，提升PbRL性能并加速策略学习。

Abstract: Preference-based Reinforcement Learning (PbRL) provides a way to learn
high-performance policies in environments where the reward signal is hard to
specify, avoiding heuristic and time-consuming reward design. However, PbRL can
suffer from slow convergence speed since it requires training in a reward
model. Prior work has proposed learning a reward model from demonstrations and
fine-tuning it using preferences. However, when the model is a neural network,
using different loss functions for pre-training and fine-tuning can pose
challenges to reliable optimization. In this paper, we propose a method to
effectively leverage prior knowledge with a Residual Reward Model (RRM). An RRM
assumes that the true reward of the environment can be split into a sum of two
parts: a prior reward and a learned reward. The prior reward is a term
available before training, for example, a user's ``best guess'' reward
function, or a reward function learned from inverse reinforcement learning
(IRL), and the learned reward is trained with preferences. We introduce
state-based and image-based versions of RRM and evaluate them on several tasks
in the Meta-World environment suite. Experimental results show that our method
substantially improves the performance of a common PbRL method. Our method
achieves performance improvements for a variety of different types of prior
rewards, including proxy rewards, a reward obtained from IRL, and even a
negated version of the proxy reward. We also conduct experiments with a Franka
Panda to show that our method leads to superior performance on a real robot. It
significantly accelerates policy learning for different tasks, achieving
success in fewer steps than the baseline. The videos are presented at
https://sunlighted.github.io/RRM-web/.

</details>


### [125] [Cooperative Sheaf Neural Networks](https://arxiv.org/abs/2507.00647)
*André Ribeiro,Ana Luiza Tenório,Juan Belieni,Amauri H. Souza,Diego Mesquita*

Main category: cs.LG

TL;DR: 本文探讨层扩散能否实现协作行为，给出否定答案，提出合作层神经网络（CSNNs），实验显示其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 探究层扩散是否能展现协作行为，解决现有层扩散方法缺乏消息方向性问题。

Method: 引入有向图上的胞腔层概念，刻画其入度和出度拉普拉斯矩阵，提出CSNNs。

Result: 理论上刻画了CSNN的感受野，实验表明CSNN在层扩散和合作图神经网络方面总体性能更好。

Conclusion: 现有层扩散方法因缺乏消息方向性无法实现协作行为，CSNN能解决该问题并取得更好性能。

Abstract: Sheaf diffusion has recently emerged as a promising design pattern for graph
representation learning due to its inherent ability to handle heterophilic data
and avoid oversmoothing. Meanwhile, cooperative message passing has also been
proposed as a way to enhance the flexibility of information diffusion by
allowing nodes to independently choose whether to propagate/gather information
from/to neighbors. A natural question ensues: is sheaf diffusion capable of
exhibiting this cooperative behavior? Here, we provide a negative answer to
this question. In particular, we show that existing sheaf diffusion methods
fail to achieve cooperative behavior due to the lack of message directionality.
To circumvent this limitation, we introduce the notion of cellular sheaves over
directed graphs and characterize their in- and out-degree Laplacians. We
leverage our construction to propose Cooperative Sheaf Neural Networks (CSNNs).
Theoretically, we characterize the receptive field of CSNN and show it allows
nodes to selectively attend (listen) to arbitrarily far nodes while ignoring
all others in their path, potentially mitigating oversquashing. Our experiments
show that CSNN presents overall better performance compared to prior art on
sheaf diffusion as well as cooperative graph neural networks.

</details>


### [126] [Cognitive Load-Aware Inference: A Neuro-Symbolic Framework for Optimizing the Token Economy of Large Language Models](https://arxiv.org/abs/2507.00653)
*Yilun Zhang*

Main category: cs.LG

TL;DR: 本文提出认知负载感知推理（CLAI）框架优化大语言模型推理，减少令牌消耗且不牺牲准确性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型推理优化策略缺乏认知理论指导，本文旨在填补此空白。

Method: 引入CLAI框架，将认知负载概念量化为指标，提出CLAI - Prompt和CLAI - Tune两种实现路径。

Result: 在多个基准测试中，方法最多减少45%的令牌消耗，CLAI - Tune能自主分解难题。

Conclusion: 模仿大脑资源管理策略可构建更高效、强大的人工智能系统。

Abstract: The escalating computational costs of Large Language Model (LLM) inference
have become a critical barrier to their widespread and sustainable deployment.
While existing optimization strategies are effective, they are predominantly
based on statistical heuristics or architectural modifications, lacking a
guiding cognitive theory to manage the inference process itself. This paper
aims to bridge this gap by introducing a novel paradigm: the Cognitive
Load-Aware Inference (CLAI) framework, which operationalizes principles from
Cognitive Load Theory (CLT) and neuroscience for LLM inference. We formalize
the concepts of Intrinsic Cognitive Load, Extraneous Cognitive Load, and
Germane Cognitive Load into quantifiable LLM metrics ($ICL_{LLM}$, $ECL_{LLM}$,
and $GCL_{LLM}$), thereby reframing the inference process as a cognitive
economics optimization problem: based on the intrinsic complexity of a problem
($ICL_{LLM}$), minimize wasteful computation ($ECL_{LLM}$), and strategically
allocate the token budget to productive reasoning ($GCL_{LLM}$). We propose two
implementation paths: CLAI-Prompt, a zero-shot method that guides a base LLM
through cognitive control steps via a structured meta-prompt, and CLAI-Tune, a
fine-tuned model that internalizes these principles for spontaneous cognitive
economy. Across a range of benchmarks in complex reasoning, long-context
question answering, and code generation, our methods achieve significant
reductions in token consumption (up to 45\%) without sacrificing accuracy.
Furthermore, CLAI-Tune exhibits an emergent ability to autonomously decompose
difficult problems, a key characteristic of human expert cognition. This work
demonstrates that by emulating the brain's resource management strategies, we
can build more efficient, robust, and capable artificial intelligence systems.

</details>


### [127] [Neural Augmented Kalman Filters for Road Network assisted GNSS positioning](https://arxiv.org/abs/2507.00654)
*Hans van Gorp,Davide Belli,Amir Jalalirad,Bence Major*

Main category: cs.LG

TL;DR: 提出用TGNN集成道路网络信息到KF以提高GNSS定位精度，实测定位误差降29%。


<details>
  <summary>Details</summary>
Motivation: GNSS在密集城市环境定位精度受多径和非视距误差影响，以往用道路网络数据的方法有局限。

Method: 训练Temporal Graph Neural Network (TGNN) 集成道路网络信息到Kalman Filter (KF)，用TGNN预测正确路段及不确定性用于KF测量更新步骤。

Result: 用真实GNSS数据和开源道路网络验证，有挑战场景下定位误差较仅用GNSS的KF降低29%。

Conclusion: 此为首个基于深度学习结合道路网络数据和GNSS测量确定用户位置的方法。

Abstract: The Global Navigation Satellite System (GNSS) provides critical positioning
information globally, but its accuracy in dense urban environments is often
compromised by multipath and non-line-of-sight errors. Road network data can be
used to reduce the impact of these errors and enhance the accuracy of a
positioning system. Previous works employing road network data are either
limited to offline applications, or rely on Kalman Filter (KF) heuristics with
little flexibility and robustness. We instead propose training a Temporal Graph
Neural Network (TGNN) to integrate road network information into a KF. The TGNN
is designed to predict the correct road segment and its associated uncertainty
to be used in the measurement update step of the KF. We validate our approach
with real-world GNSS data and open-source road networks, observing a 29%
decrease in positioning error for challenging scenarios compared to a GNSS-only
KF. To the best of our knowledge, ours is the first deep learning-based
approach jointly employing road network data and GNSS measurements to determine
the user position on Earth.

</details>


### [128] [Audio-3DVG: Unified Audio - Point Cloud Fusion for 3D Visual Grounding](https://arxiv.org/abs/2507.00669)
*Duc Cao-Dinh,Khai Le-Duc,Anh Dao,Bach Phan Tat,Chris Ngo,Duy M. H. Nguyen,Nguyen X. Khanh,Thanh Nguyen-Tang*

Main category: cs.LG

TL;DR: 提出Audio - 3DVG框架用于基于音频的3D视觉定位，实验效果良好。


<details>
  <summary>Details</summary>
Motivation: 现有基于口语的3D视觉定位研究不足，受自动语音识别和语音表示学习进展启发。

Method: 将任务分解为对象提及检测和音频引导注意力模块，合成标准3DVG数据集的音频描述用于基准测试。

Result: Audio - 3DVG在基于音频的定位中达到新的最优性能，能与基于文本的方法竞争。

Conclusion: 将口语集成到3D视觉任务有前景。

Abstract: 3D Visual Grounding (3DVG) involves localizing target objects in 3D point
clouds based on natural language. While prior work has made strides using
textual descriptions, leveraging spoken language-known as Audio-based 3D Visual
Grounding-remains underexplored and challenging. Motivated by advances in
automatic speech recognition (ASR) and speech representation learning, we
propose Audio-3DVG, a simple yet effective framework that integrates audio and
spatial information for enhanced grounding. Rather than treating speech as a
monolithic input, we decompose the task into two complementary components.
First, we introduce Object Mention Detection, a multi-label classification task
that explicitly identifies which objects are referred to in the audio, enabling
more structured audio-scene reasoning. Second, we propose an Audio-Guided
Attention module that captures interactions between candidate objects and
relational speech cues, improving target discrimination in cluttered scenes. To
support benchmarking, we synthesize audio descriptions for standard 3DVG
datasets, including ScanRefer, Sr3D, and Nr3D. Experimental results demonstrate
that Audio-3DVG not only achieves new state-of-the-art performance in
audio-based grounding, but also competes with text-based methods-highlighting
the promise of integrating spoken language into 3D vision tasks.

</details>


### [129] [Diffusion Classifier Guidance for Non-robust Classifiers](https://arxiv.org/abs/2507.00687)
*Philipp Vaeth,Dibyanshu Kumar,Benjamin Paassen,Magda Gregorová*

Main category: cs.LG

TL;DR: 本文将分类器引导扩展到非鲁棒分类器，分析其对扩散过程噪声的敏感性，提出利用单步去噪图像预测和稳定技术的方法，实验表明该方法提高了引导稳定性，推动了生成模型的条件采样技术。


<details>
  <summary>Details</summary>
Motivation: 大多数分类器引导方法局限于鲁棒分类器，本文旨在将其扩展到未经过噪声训练的通用非鲁棒分类器。

Method: 分析非鲁棒和鲁棒分类器对扩散过程噪声的敏感性，提出利用单步去噪图像预测和受随机优化方法启发的稳定技术。

Result: 实验结果表明该方法在保持样本多样性和视觉质量的同时，提高了分类器引导的稳定性。

Conclusion: 本工作推进了生成模型的条件采样技术，使更多分类器可用作引导分类器。

Abstract: Classifier guidance is intended to steer a diffusion process such that a
given classifier reliably recognizes the generated data point as a certain
class. However, most classifier guidance approaches are restricted to robust
classifiers, which were specifically trained on the noise of the diffusion
forward process. We extend classifier guidance to work with general,
non-robust, classifiers that were trained without noise. We analyze the
sensitivity of both non-robust and robust classifiers to noise of the diffusion
process on the standard CelebA data set, the specialized SportBalls data set
and the high-dimensional real-world CelebA-HQ data set. Our findings reveal
that non-robust classifiers exhibit significant accuracy degradation under
noisy conditions, leading to unstable guidance gradients. To mitigate these
issues, we propose a method that utilizes one-step denoised image predictions
and implements stabilization techniques inspired by stochastic optimization
methods, such as exponential moving averages. Experimental results demonstrate
that our approach improves the stability of classifier guidance while
maintaining sample diversity and visual quality. This work contributes to
advancing conditional sampling techniques in generative models, enabling a
broader range of classifiers to be used as guidance classifiers.

</details>


### [130] [A Test-Function Approach to Incremental Stability](https://arxiv.org/abs/2507.00695)
*Daniel Pfrommer,Max Simchowitz,Ali Jadbabaie*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This paper presents a novel framework for analyzing
Incremental-Input-to-State Stability ($\delta$ISS) based on the idea of using
rewards as "test functions." Whereas control theory traditionally deals with
Lyapunov functions that satisfy a time-decrease condition, reinforcement
learning (RL) value functions are constructed by exponentially decaying a
Lipschitz reward function that may be non-smooth and unbounded on both sides.
Thus, these RL-style value functions cannot be directly understood as Lyapunov
certificates. We develop a new equivalence between a variant of incremental
input-to-state stability of a closed-loop system under given a policy, and the
regularity of RL-style value functions under adversarial selection of a
H\"older-continuous reward function. This result highlights that the regularity
of value functions, and their connection to incremental stability, can be
understood in a way that is distinct from the traditional Lyapunov-based
approach to certifying stability in control theory.

</details>


### [131] [SCAWaveNet: A Spatial-Channel Attention-based Network for Global Significant Wave Height Retrieval](https://arxiv.org/abs/2507.00701)
*Chong Zhang,Xichao Liu,Yibing Zhan,Dapeng Tao,Jun Ni*

Main category: cs.LG

TL;DR: 提出用于显著波高（SWH）反演的SCAWaveNet网络，评估其性能并与现有模型对比，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型在训练时未充分利用跨通道信息交互的优势。

Method: 提出SCAWaveNet网络，将DDMs各通道特征建模为独立注意力头，为辅助参数设计轻量级注意力机制，最后融合空间和通道级特征。

Result: 使用ERA5作参考，SCAWaveNet平均RMSE为0.438m；使用NDBC浮标数据，平均RMSE为0.432m，相比现有模型降低了误差。

Conclusion: SCAWaveNet在SWH反演中表现良好，能有效降低误差。

Abstract: Recent advancements in spaceborne GNSS missions have produced extensive
global datasets, providing a robust basis for deep learning-based significant
wave height (SWH) retrieval. While existing deep learning models predominantly
utilize CYGNSS data with four-channel information, they often adopt
single-channel inputs or simple channel concatenation without leveraging the
benefits of cross-channel information interaction during training. To address
this limitation, a novel spatial-channel attention-based network, namely
SCAWaveNet, is proposed for SWH retrieval. Specifically, features from each
channel of the DDMs are modeled as independent attention heads, enabling the
fusion of spatial and channel-wise information. For auxiliary parameters, a
lightweight attention mechanism is designed to assign weights along the spatial
and channel dimensions. The final feature integrates both spatial and
channel-level characteristics. Model performance is evaluated using
four-channel CYGNSS data. When ERA5 is used as a reference, SCAWaveNet achieves
an average RMSE of 0.438 m. When using buoy data from NDBC, the average RMSE
reaches 0.432 m. Compared to state-of-the-art models, SCAWaveNet reduces the
average RMSE by at least 3.52% on the ERA5 dataset and by 5.47% on the NDBC
buoy observations. The code is available at
https://github.com/Clifx9908/SCAWaveNet.

</details>


### [132] [Large Reasoning Models are not thinking straight: on the unreliability of thinking trajectories](https://arxiv.org/abs/2507.00711)
*Jhouben Cuesta-Ramirez,Samuel Beaussant,Mehdi Mounsif*

Main category: cs.LG

TL;DR: 研究发现通过强化学习训练的大语言模型在推理基准测试中虽有成果，但存在过度思考问题，实验揭示其整合纠正信息能力的局限。


<details>
  <summary>Details</summary>
Motivation: 质疑大语言模型在推理基准测试中的成果是否反映真实推理能力提升，探究其过度思考问题。

Method: 使用AIME2024数学基准对三个最先进的模型进行实验。

Result: 发现模型存在过度思考现象，即无视正确解决方案，继续生成不必要推理步骤并常得出错误结论，且在整合纠正信息方面存在关键局限。

Conclusion: 大语言模型在实现稳健和可解释推理方面面临新挑战。

Abstract: Large Language Models (LLMs) trained via Reinforcement Learning (RL) have
recently achieved impressive results on reasoning benchmarks. Yet, growing
evidence shows that these models often generate longer but ineffective chains
of thought (CoTs), calling into question whether benchmark gains reflect real
reasoning improvements. We present new evidence of overthinking, where models
disregard correct solutions even when explicitly provided, instead continuing
to generate unnecessary reasoning steps that often lead to incorrect
conclusions. Experiments on three state-of-the-art models using the AIME2024
math benchmark reveal critical limitations in these models ability to integrate
corrective information, posing new challenges for achieving robust and
interpretable reasoning.

</details>


### [133] [Aleatoric and Epistemic Uncertainty Measures for Ordinal Classification through Binary Reduction](https://arxiv.org/abs/2507.00733)
*Stefan Haas,Eyke Hüllermeier*

Main category: cs.LG

TL;DR: 本文提出序数分类中新型的偶然和认知不确定性度量方法，在多个基准数据集上验证其有效性，优于标准度量，突出考虑序数性质的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注名义分类和回归，序数分类中准确的不确定性量化对高风险领域可靠决策至关重要，而该领域研究不足。

Method: 将问题适当简化为基于熵和方差的二元度量，使用梯度提升树和多层感知器集成进行近似贝叶斯推理。

Result: 该方法在错误检测上显著优于标准和按标签的熵与方差度量，在分布外检测中也有有竞争力的表现。

Conclusion: 评估不确定性时考虑分类问题的序数性质很重要。

Abstract: Ordinal classification problems, where labels exhibit a natural order, are
prevalent in high-stakes fields such as medicine and finance. Accurate
uncertainty quantification, including the decomposition into aleatoric
(inherent variability) and epistemic (lack of knowledge) components, is crucial
for reliable decision-making. However, existing research has primarily focused
on nominal classification and regression. In this paper, we introduce a novel
class of measures of aleatoric and epistemic uncertainty in ordinal
classification, which is based on a suitable reduction to (entropy- and
variance-based) measures for the binary case. These measures effectively
capture the trade-off in ordinal classification between exact hit-rate and
minimial error distances. We demonstrate the effectiveness of our approach on
various tabular ordinal benchmark datasets using ensembles of gradient-boosted
trees and multi-layer perceptrons for approximate Bayesian inference. Our
method significantly outperforms standard and label-wise entropy and
variance-based measures in error detection, as indicated by misclassification
rates and mean absolute error. Additionally, the ordinal measures show
competitive performance in out-of-distribution (OOD) detection. Our findings
highlight the importance of considering the ordinal nature of classification
problems when assessing uncertainty.

</details>


### [134] [Evaluating LLMs and Prompting Strategies for Automated Hardware Diagnosis from Textual User-Reports](https://arxiv.org/abs/2507.00742)
*Carlos Caminha,Maria de Lourdes M. Silva,Iago C. Chaves,Felipe T. Brito,Victor A. E. Farias,Javam C. Machado*

Main category: cs.LG

TL;DR: 研究评估多种大语言模型和提示策略识别设备故障组件，达f1分数0.76，发现三个平衡大小与性能的模型。


<details>
  <summary>Details</summary>
Motivation: 从用户模糊文本报告中识别故障组件对自动化测试和提升用户体验至关重要，但任务有挑战，大语言模型有解决问题潜力。

Method: 评估27个开源模型和2个专有大语言模型，采用零样本、少样本、思维链和思维链+少样本四种提示策略，进行98948次推理。

Result: f1分数最高达0.76，发现mistral - small - 24b - instruct和另外两个小模型平衡大小与性能，低显存使用下能在终端设备高效推理。

Conclusion: 三个模型在大小和性能间取得较好平衡，可在终端设备高效推理

Abstract: Computer manufacturers offer platforms for users to describe device faults
using textual reports such as "My screen is flickering". Identifying the faulty
component from the report is essential for automating tests and improving user
experience. However, such reports are often ambiguous and lack detail, making
this task challenging. Large Language Models (LLMs) have shown promise in
addressing such issues. This study evaluates 27 open-source models (1B-72B
parameters) and 2 proprietary LLMs using four prompting strategies: Zero-Shot,
Few-Shot, Chain-of-Thought (CoT), and CoT+Few-Shot (CoT+FS). We conducted
98,948 inferences, processing over 51 million input tokens and generating 13
million output tokens. We achieve f1-score up to 0.76. Results show that three
models offer the best balance between size and performance:
mistral-small-24b-instruct and two smaller models, llama-3.2-1b-instruct and
gemma-2-2b-it, that offer competitive performance with lower VRAM usage,
enabling efficient inference on end-user devices as modern laptops or
smartphones with NPUs.

</details>


### [135] [A Probabilistic Approach to Wildfire Spread Prediction Using a Denoising Diffusion Surrogate Model](https://arxiv.org/abs/2507.00761)
*Wenbo Yu,Anirbit Ghosh,Tobias Sebastian Finn,Rossella Arcucci,Marc Bocquet,Sibo Cheng*

Main category: cs.LG

TL;DR: 利用生成式AI能力，提出首个去噪扩散模型预测野火蔓延，考虑了不确定性，可辅助决策。


<details>
  <summary>Details</summary>
Motivation: 利用生成式AI预测野火蔓延，解决传统模型难以应对野火不确定性和多样环境条件的问题。

Method: 提出首个去噪扩散模型，学习模拟野火的多种可能场景。

Result: 模型产生反映物理意义分布的预测集合，而非单一预测。

Conclusion: 该技术有助于开发更智能、快速、可靠的工具，辅助火灾风险评估和响应规划。

Abstract: Thanks to recent advances in generative AI, computers can now simulate
realistic and complex natural processes. We apply this capability to predict
how wildfires spread, a task made difficult by the unpredictable nature of fire
and the variety of environmental conditions it depends on. In this study, We
present the first denoising diffusion model for predicting wildfire spread, a
new kind of AI framework that learns to simulate fires not just as one fixed
outcome, but as a range of possible scenarios. By doing so, it accounts for the
inherent uncertainty of wildfire dynamics, a feature that traditional models
typically fail to represent. Unlike deterministic approaches that generate a
single prediction, our model produces ensembles of forecasts that reflect
physically meaningful distributions of where fire might go next. This
technology could help us develop smarter, faster, and more reliable tools for
anticipating wildfire behavior, aiding decision-makers in fire risk assessment
and response planning.

</details>


### [136] [Leveraging Genetic Algorithms for Efficient Demonstration Generation in Real-World Reinforcement Learning Environments](https://arxiv.org/abs/2507.00762)
*Tom Maus,Asma Atamna,Tobias Glasmachers*

Main category: cs.LG

TL;DR: 研究用遗传算法（GA）改善强化学习（RL）在工业排序环境中的性能，提出用GA生成专家演示增强策略学习，实验表明该方法显著提升RL性能，框架公开。


<details>
  <summary>Details</summary>
Motivation: RL在现实工业应用中受样本低效和学习动态不稳定等挑战限制，需提高其性能。

Method: 提出用GA生成专家演示，将其纳入深度Q网络（DQN）回放缓冲区进行经验学习，并作为近端策略优化（PPO）代理的热启动轨迹加速训练收敛。

Result: 实验对比显示GA衍生演示显著提高RL性能，用GA生成数据初始化的PPO代理获得更高累积奖励。

Conclusion: 混合学习范式有潜力，启发式搜索方法可补充数据驱动的RL，公开框架利于进一步研究现实应用的自适应RL策略。

Abstract: Reinforcement Learning (RL) has demonstrated significant potential in certain
real-world industrial applications, yet its broader deployment remains limited
by inherent challenges such as sample inefficiency and unstable learning
dynamics. This study investigates the utilization of Genetic Algorithms (GAs)
as a mechanism for improving RL performance in an industrially inspired sorting
environment. We propose a novel approach in which GA-generated expert
demonstrations are used to enhance policy learning. These demonstrations are
incorporated into a Deep Q-Network (DQN) replay buffer for experience-based
learning and utilized as warm-start trajectories for Proximal Policy
Optimization (PPO) agents to accelerate training convergence. Our experiments
compare standard RL training with rule-based heuristics, brute-force
optimization, and demonstration data, revealing that GA-derived demonstrations
significantly improve RL performance. Notably, PPO agents initialized with
GA-generated data achieved superior cumulative rewards, highlighting the
potential of hybrid learning paradigms, where heuristic search methods
complement data-driven RL. The utilized framework is publicly available and
enables further research into adaptive RL strategies for real-world
applications.

</details>


### [137] [BoltzNCE: Learning Likelihoods for Boltzmann Generation with Stochastic Interpolants and Noise Contrastive Estimation](https://arxiv.org/abs/2507.00846)
*Rishal Aggrwal,Jacky Chen,Nicholas M. Boffi,David Ryan Koes*

Main category: cs.LG

TL;DR: 提出通过基于能量的模型学习生成分布的似然性，结合目标函数高效学习密度函数，在丙氨酸二肽系统上效果好且加速估计自由能差。


<details>
  <summary>Details</summary>
Motivation: 解决Boltzmann Generators在大尺度分子系统中获取似然性需计算昂贵雅可比行列式的问题。

Method: 通过噪声对比估计和得分匹配训练基于能量的模型学习生成分布的似然性，用随机插值在先验和生成分布间退火，结合目标函数学习密度函数。

Result: 在丙氨酸二肽系统上得到的自由能分布和能量分布与精确似然法相当，能以数量级速度加速准确估计亚稳状态间的自由能差。

Conclusion: 提出的方法有效，可用于大尺度分子系统的Boltzmann分布高效采样。

Abstract: Efficient sampling from the Boltzmann distribution defined by an energy
function is a key challenge in modeling physical systems such as molecules.
Boltzmann Generators tackle this by leveraging Continuous Normalizing Flows
that transform a simple prior into a distribution that can be reweighted to
match the Boltzmann distribution using sample likelihoods. However, obtaining
likelihoods requires computing costly Jacobians during integration, making it
impractical for large molecular systems. To overcome this, we propose learning
the likelihood of the generated distribution via an energy-based model trained
with noise contrastive estimation and score matching. By using stochastic
interpolants to anneal between the prior and generated distributions, we
combine both the objective functions to efficiently learn the density function.
On the alanine dipeptide system, we demonstrate that our method yields free
energy profiles and energy distributions comparable to those obtained with
exact likelihoods. Additionally, we show that free energy differences between
metastable states can be estimated accurately with orders-of-magnitude speedup.

</details>


### [138] [Quantum Approximate Optimization Algorithm for Spatiotemporal Forecasting of HIV Clusters](https://arxiv.org/abs/2507.00848)
*Don Roosan,Saif Nirzhor,Rubayat Khan,Fahmida Hai,Mohammad Rifat Haidar*

Main category: cs.LG

TL;DR: 使用量子加速机器学习分析HIV流行率，在聚类检测、流行率预测等方面有优势，还找出关键因果路径，可指导干预措施。


<details>
  <summary>Details</summary>
Motivation: HIV流行病学数据复杂，需要先进计算进行准确聚类检测和预测。

Method: 采用量子加速机器学习，对比经典聚类算法与量子近似优化算法（QAOA），开发混合量子 - 经典神经网络预测HIV流行率，用量子贝叶斯网络探索社会决定因素与HIV发病率因果关系。

Result: QAOA聚类检测准确率92%，用时1.6秒；混合量子 - 经典神经网络预测HIV流行率准确率94%；量子贝叶斯分析确定住房不稳定是HIV集群出现和扩张的关键驱动因素。

Conclusion: 量子增强方法在HIV监测中更精确高效，能找出关键因果路径，可指导针对性干预、优化资源分配和解决结构不平等问题。

Abstract: HIV epidemiological data is increasingly complex, requiring advanced
computation for accurate cluster detection and forecasting. We employed
quantum-accelerated machine learning to analyze HIV prevalence at the ZIP-code
level using AIDSVu and synthetic SDoH data for 2022. Our approach compared
classical clustering (DBSCAN, HDBSCAN) with a quantum approximate optimization
algorithm (QAOA), developed a hybrid quantum-classical neural network for HIV
prevalence forecasting, and used quantum Bayesian networks to explore causal
links between SDoH factors and HIV incidence. The QAOA-based method achieved
92% accuracy in cluster detection within 1.6 seconds, outperforming classical
algorithms. Meanwhile, the hybrid quantum-classical neural network predicted
HIV prevalence with 94% accuracy, surpassing a purely classical counterpart.
Quantum Bayesian analysis identified housing instability as a key driver of HIV
cluster emergence and expansion, with stigma exerting a geographically variable
influence. These quantum-enhanced methods deliver greater precision and
efficiency in HIV surveillance while illuminating critical causal pathways.
This work can guide targeted interventions, optimize resource allocation for
PrEP, and address structural inequities fueling HIV transmission.

</details>


### [139] [Aligning Learning and Endogenous Decision-Making](https://arxiv.org/abs/2507.00851)
*Rares Cristian,Pavithra Harsha,Georgia Perakis,Brian Quanz*

Main category: cs.LG

TL;DR: 提出端到端方法处理内生不确定性下决策问题，引入鲁棒优化变体和新的两阶段随机优化问题，实验显示性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决决策时因缺乏反事实信息而难以学习的问题。

Method: 引入端到端方法让ML模型考虑下游影响，引入鲁棒优化变体，构建不确定性集，提出新的两阶段随机优化问题。

Result: 证明鲁棒方法能以高概率捕获接近最优的决策，实验表明该方法在定价、库存分类/推荐问题上性能优于现有方法。

Conclusion: 所提出的方法在处理决策问题上具有优势，能有效解决相关挑战。

Abstract: Many of the observations we make are biased by our decisions. For instance,
the demand of items is impacted by the prices set, and online checkout choices
are influenced by the assortments presented. The challenge in decision-making
under this setting is the lack of counterfactual information, and the need to
learn it instead. We introduce an end-to-end method under endogenous
uncertainty to train ML models to be aware of their downstream, enabling their
effective use in the decision-making stage. We further introduce a robust
optimization variant that accounts for uncertainty in ML models -- specifically
by constructing uncertainty sets over the space of ML models and optimizing
actions to protect against worst-case predictions. We prove guarantees that
this robust approach can capture near-optimal decisions with high probability
as a function of data. Besides this, we also introduce a new class of two-stage
stochastic optimization problems to the end-to-end learning framework that can
now be addressed through our framework. Here, the first stage is an
information-gathering problem to decide which random variable to poll and gain
information about before making a second-stage decision based off of it. We
present several computational experiments for pricing and inventory
assortment/recommendation problems. We compare against existing methods in
online learning/bandits/offline reinforcement learning and show our approach
has consistent improved performance over these. Just as in the endogenous
setting, the model's prediction also depends on the first-stage decision made.
While this decision does not affect the random variable in this setting, it
does affect the correct point forecast that should be made.

</details>


### [140] [Machine Learning-based Early Detection of Potato Sprouting Using Electrophysiological Signals](https://arxiv.org/abs/2507.00862)
*Davide Andreoletti,Aris Marcolongo,Natasa Sarafijanovic Djukic,Julien Roulet,Stefano Billeter,Andrzej Kurenda,Margot Visse-Mansiaux,Brice Dupuis,Carrol Annette Plummer,Beatrice Paoli,Omran Ayoub*

Main category: cs.LG

TL;DR: 本文提出基于机器学习的方法，利用电生理信号实现马铃薯发芽早期预测，实验结果有前景但需进一步改进。


<details>
  <summary>Details</summary>
Motivation: 准确预测马铃薯发芽对存储管理至关重要，现有方法依赖视觉识别，无法提前预测，且CIPC被禁用使需求更迫切。

Method: 提出基于机器学习的方法，利用专用传感器记录块茎电生理信号，对信号预处理、从小波域提取特征，训练监督学习模型，并结合不确定性量化技术。

Result: 实验在马铃薯发芽早期检测中表现良好，能准确预测部分马铃薯发芽日期，总体平均误差可接受。

Conclusion: 结果有前景，但需进一步改进以减少预测误差，特别是最大偏差。

Abstract: Accurately predicting potato sprouting before the emergence of any visual
signs is critical for effective storage management, as sprouting degrades both
the commercial and nutritional value of tubers. Effective forecasting allows
for the precise application of anti-sprouting chemicals (ASCs), minimizing
waste and reducing costs. This need has become even more pressing following the
ban on Isopropyl N-(3-chlorophenyl) carbamate (CIPC) or Chlorpropham due to
health and environmental concerns, which has led to the adoption of
significantly more expensive alternative ASCs. Existing approaches primarily
rely on visual identification, which only detects sprouting after morphological
changes have occurred, limiting their effectiveness for proactive management. A
reliable early prediction method is therefore essential to enable timely
intervention and improve the efficiency of post-harvest storage strategies,
where early refers to detecting sprouting before any visible signs appear. In
this work, we address the problem of early prediction of potato sprouting. To
this end, we propose a novel machine learning (ML)-based approach that enables
early prediction of potato sprouting using electrophysiological signals
recorded from tubers using proprietary sensors. Our approach preprocesses the
recorded signals, extracts relevant features from the wavelet domain, and
trains supervised ML models for early sprouting detection. Additionally, we
incorporate uncertainty quantification techniques to enhance predictions.
Experimental results demonstrate promising performance in the early detection
of potato sprouting by accurately predicting the exact day of sprouting for a
subset of potatoes and while showing acceptable average error across all
potatoes. Despite promising results, further refinements are necessary to
minimize prediction errors, particularly in reducing the maximum observed
deviations.

</details>


### [141] [NN-Former: Rethinking Graph Structure in Neural Architecture Representation](https://arxiv.org/abs/2507.00880)
*Ruihan Xu,Haokui Zhang,Yaowei Wang,Wei Zeng,Shiliang Zhang*

Main category: cs.LG

TL;DR: 本文提出结合GNN和transformer优点的新型神经网络架构属性预测器，在准确率和延迟预测上表现良好。


<details>
  <summary>Details</summary>
Motivation: 深度学习发展需要高效网络设计与部署，现有GNN和transformer在表示神经架构时有各自缺点。

Method: 重新思考神经架构拓扑，提出利用GNN和transformer优点学习增强拓扑的预测器，引入考虑兄弟节点的token mixer和双向图同构前馈网络作为channel mixer。

Result: 该方法在准确率和延迟预测上取得不错表现。

Conclusion: 研究为学习有向无环图拓扑提供了有价值的见解。

Abstract: The growing use of deep learning necessitates efficient network design and
deployment, making neural predictors vital for estimating attributes such as
accuracy and latency. Recently, Graph Neural Networks (GNNs) and transformers
have shown promising performance in representing neural architectures. However,
each of both methods has its disadvantages. GNNs lack the capabilities to
represent complicated features, while transformers face poor generalization
when the depth of architecture grows. To mitigate the above issues, we rethink
neural architecture topology and show that sibling nodes are pivotal while
overlooked in previous research. We thus propose a novel predictor leveraging
the strengths of GNNs and transformers to learn the enhanced topology. We
introduce a novel token mixer that considers siblings, and a new channel mixer
named bidirectional graph isomorphism feed-forward network. Our approach
consistently achieves promising performance in both accuracy and latency
prediction, providing valuable insights for learning Directed Acyclic Graph
(DAG) topology. The code is available at https://github.com/XuRuihan/NNFormer.

</details>


### [142] [TABASCO: A Fast, Simplified Model for Molecular Generation with Improved Physical Quality](https://arxiv.org/abs/2507.00899)
*Carlos Vonessen,Charles Harris,Miruna Cretu,Pietro Liò*

Main category: cs.LG

TL;DR: 提出TABASCO模型用于3D分子生成，简化架构，提高速度和有效性，为特定任务生成模型提供蓝图。


<details>
  <summary>Details</summary>
Motivation: 现有3D分子生成模型生成的分子物理合理性不足。

Method: 采用标准非等变变压器架构，将分子中的原子视为序列，生成后确定性重建键。

Result: 在GEOM - Drugs基准上达到最先进的PoseBusters有效性，推理速度比最强基线快约10倍，且展现出涌现的旋转等变性。

Conclusion: 为训练适用于特定任务的简约、高通量生成模型提供蓝图。

Abstract: State-of-the-art models for 3D molecular generation are based on significant
inductive biases, SE(3), permutation equivariance to respect symmetry and graph
message-passing networks to capture local chemistry, yet the generated
molecules still struggle with physical plausibility. We introduce TABASCO which
relaxes these assumptions: The model has a standard non-equivariant transformer
architecture, treats atoms in a molecule as sequences and reconstructs bonds
deterministically after generation. The absence of equivariant layers and
message passing allows us to significantly simplify the model architecture and
scale data throughput. On the GEOM-Drugs benchmark TABASCO achieves
state-of-the-art PoseBusters validity and delivers inference roughly 10x faster
than the strongest baseline, while exhibiting emergent rotational equivariance
despite symmetry not being hard-coded. Our work offers a blueprint for training
minimalist, high-throughput generative models suited to specialised tasks such
as structure- and pharmacophore-based drug design. We provide a link to our
implementation at github.com/carlosinator/tabasco.

</details>


### [143] [Privacy-Preserving Quantized Federated Learning with Diverse Precision](https://arxiv.org/abs/2507.00920)
*Dang Qua Nguyen,Morteza Hashemi,Erik Perrins,Sergiy A. Vorobyov,David J. Love,Taejoon Kim*

Main category: cs.LG

TL;DR: 提出新的随机量化器和集群大小优化技术，提升隐私保护联邦学习的学习效用。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习存在隐私风险和量化异质性导致学习效用降低问题，以往工作多只解决其一。

Method: 引入新的随机量化器同时实现差分隐私和最小量化误差，采用集群大小优化技术结合线性融合方法解决量化异质性。

Result: 数值模拟表明，与传统LaplaceSQ - FL算法相比，该方法在隐私保护和学习效用方面有优势。

Conclusion: 提出的方法能有效提升隐私保护联邦学习中不同量化分辨率设备集群参与时的学习效用。

Abstract: Federated learning (FL) has emerged as a promising paradigm for distributed
machine learning, enabling collaborative training of a global model across
multiple local devices without requiring them to share raw data. Despite its
advancements, FL is limited by factors such as: (i) privacy risks arising from
the unprotected transmission of local model updates to the fusion center (FC)
and (ii) decreased learning utility caused by heterogeneity in model
quantization resolution across participating devices. Prior work typically
addresses only one of these challenges because maintaining learning utility
under both privacy risks and quantization heterogeneity is a non-trivial task.
In this paper, our aim is therefore to improve the learning utility of a
privacy-preserving FL that allows clusters of devices with different
quantization resolutions to participate in each FL round. Specifically, we
introduce a novel stochastic quantizer (SQ) that is designed to simultaneously
achieve differential privacy (DP) and minimum quantization error. Notably, the
proposed SQ guarantees bounded distortion, unlike other DP approaches. To
address quantization heterogeneity, we introduce a cluster size optimization
technique combined with a linear fusion approach to enhance model aggregation
accuracy. Numerical simulations validate the benefits of our approach in terms
of privacy protection and learning utility compared to the conventional
LaplaceSQ-FL algorithm.

</details>


### [144] [Understanding Generalization in Node and Link Prediction](https://arxiv.org/abs/2507.00927)
*Antonis Vasileiou,Timo Stoll,Christopher Morris*

Main category: cs.LG

TL;DR: 本文提出统一框架分析MPNN在节点和链接预测中的泛化特性，该框架可用于其他分类任务，实证研究支持理论见解。


<details>
  <summary>Details</summary>
Motivation: 现有研究对MPNN在节点和链接预测中的泛化能力理解不足，且存在依赖不现实假设、忽略图结构影响等问题。

Method: 引入统一框架，结合多种架构参数和损失函数，量化图结构的影响。

Result: 实证研究支持了理论见解。

Conclusion: 加深了对MPNN在节点和链接预测任务中泛化能力的理解，且框架可用于其他分类任务。

Abstract: Using message-passing graph neural networks (MPNNs) for node and link
prediction is crucial in various scientific and industrial domains, which has
led to the development of diverse MPNN architectures. Besides working well in
practical settings, their ability to generalize beyond the training set remains
poorly understood. While some studies have explored MPNNs' generalization in
graph-level prediction tasks, much less attention has been given to node- and
link-level predictions. Existing works often rely on unrealistic i.i.d.\@
assumptions, overlooking possible correlations between nodes or links, and
assuming fixed aggregation and impractical loss functions while neglecting the
influence of graph structure. In this work, we introduce a unified framework to
analyze the generalization properties of MPNNs in inductive and transductive
node and link prediction settings, incorporating diverse architectural
parameters and loss functions and quantifying the influence of graph structure.
Additionally, our proposed generalization framework can be applied beyond
graphs to any classification task under the inductive or transductive setting.
Our empirical study supports our theoretical insights, deepening our
understanding of MPNNs' generalization capabilities in these tasks.

</details>


### [145] [Time Series Foundation Models are Flow Predictors](https://arxiv.org/abs/2507.00945)
*Massimiliano Luca,Ciro Beneduce,Bruno Lepri*

Main category: cs.LG

TL;DR: 研究时间序列基础模型（TSFMs）在人群流动预测中的有效性，评估Moirai和TimesFM在三个真实数据集上零样本设置下的表现，模型优于基线。


<details>
  <summary>Details</summary>
Motivation: 探究TSFMs在人群流动预测中的有效性。

Method: 在三个真实世界流动性数据集上，以严格零样本设置部署Moirai和TimesFM模型，仅使用各OD流的时间演变，不使用明确空间信息。

Result: Moirai和TimesFM优于统计和深度学习基线，RMSE降低33%，MAE降低39%，CPC提高49%。

Conclusion: TSFMs对准确、可扩展的流量预测具有实用价值，即使在标注数据有限或缺少空间上下文的场景中。

Abstract: We investigate the effectiveness of time series foundation models (TSFMs) for
crowd flow prediction, focusing on Moirai and TimesFM. Evaluated on three
real-world mobility datasets-Bike NYC, Taxi Beijing, and Spanish national OD
flows-these models are deployed in a strict zero-shot setting, using only the
temporal evolution of each OD flow and no explicit spatial information. Moirai
and TimesFM outperform both statistical and deep learning baselines, achieving
up to 33% lower RMSE, 39% lower MAE and up to 49% higher CPC compared to
state-of-the-art competitors. Our results highlight the practical value of
TSFMs for accurate, scalable flow prediction, even in scenarios with limited
annotated data or missing spatial context.

</details>


### [146] [Benchmarking the Discovery Engine](https://arxiv.org/abs/2507.00964)
*Jack Foxabbott,Arush Tagade,Andrew Cusick,Robbie McCorkell,Leo McKee-Reid,Jugal Patel,Jamie Rumbelow,Jessica Rumbelow,Zohreh Shams*

Main category: cs.LG

TL;DR: 本文介绍Discovery Engine系统，通过与五篇论文对比测试，表明其有潜力成为自动、可解释科学建模新标准。


<details>
  <summary>Details</summary>
Motivation: 验证Discovery Engine系统在不同领域数据集上的性能和价值。

Method: 将Discovery Engine与五篇跨医学、材料科学等领域应用机器学习的论文进行对比测试。

Result: Discovery Engine在预测性能上持平或超越先前研究，还能通过可解释性工件产生更深刻、可行的见解。

Conclusion: Discovery Engine有潜力成为自动、可解释科学建模的新标准，实现从数据中发现复杂知识。

Abstract: The Discovery Engine is a general purpose automated system for scientific
discovery, which combines machine learning with state-of-the-art ML
interpretability to enable rapid and robust scientific insight across diverse
datasets. In this paper, we benchmark the Discovery Engine against five recent
peer-reviewed scientific publications applying machine learning across
medicine, materials science, social science, and environmental science. In each
case, the Discovery Engine matches or exceeds prior predictive performance
while also generating deeper, more actionable insights through rich
interpretability artefacts. These results demonstrate its potential as a new
standard for automated, interpretable scientific modelling that enables complex
knowledge discovery from data.

</details>


### [147] [Scalable Feature Learning on Huge Knowledge Graphs for Downstream Machine Learning](https://arxiv.org/abs/2507.00965)
*Félix Lefebvre,Gaël Varoquaux*

Main category: cs.LG

TL;DR: 提出SEPAL算法解决现有知识图谱嵌入模型的局限，在多图多任务上效果好且可扩展。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱嵌入模型主要为链接预测优化，且因GPU内存限制难以扩展到大型图，需新方法解决这些问题。

Method: 引入SEPAL算法，通过在小核心实体上优化嵌入并通过消息传递传播到图的其余部分来实现全局嵌入对齐。

Result: 在7个大规模知识图谱和46个下游机器学习任务上评估，SEPAL显著优于先前方法，且能扩展其基础嵌入模型。

Conclusion: SEPAL能为下游任务大规模生成高质量嵌入，可在普通硬件上处理大型知识图谱。

Abstract: Many machine learning tasks can benefit from external knowledge. Large
knowledge graphs store such knowledge, and embedding methods can be used to
distill it into ready-to-use vector representations for downstream
applications. For this purpose, current models have however two limitations:
they are primarily optimized for link prediction, via local contrastive
learning, and they struggle to scale to the largest graphs due to GPU memory
limits. To address these, we introduce SEPAL: a Scalable Embedding Propagation
ALgorithm for large knowledge graphs designed to produce high-quality
embeddings for downstream tasks at scale. The key idea of SEPAL is to enforce
global embedding alignment by optimizing embeddings only on a small core of
entities, and then propagating them to the rest of the graph via message
passing. We evaluate SEPAL on 7 large-scale knowledge graphs and 46 downstream
machine learning tasks. Our results show that SEPAL significantly outperforms
previous methods on downstream tasks. In addition, SEPAL scales up its base
embedding model, enabling fitting huge knowledge graphs on commodity hardware.

</details>


### [148] [Reasoning as an Adaptive Defense for Safety](https://arxiv.org/abs/2507.00971)
*Taeyoun Kim,Fahim Tajwar,Aditi Raghunathan,Aviral Kumar*

Main category: cs.LG

TL;DR: 提出TARS方法训练大模型应对安全漏洞，模型表现出自适应行为，能更好平衡安全与拒绝，对攻击更鲁棒。


<details>
  <summary>Details</summary>
Motivation: 研究如何利用自适应分配测试时计算的推理方法，训练对安全漏洞有一定鲁棒性的模型。

Method: 构建TARS方法，这是一种强化学习方法，使用思维链轨迹和平衡安全与任务完成的奖励信号，确定三个关键设计选择。

Result: 用TARS训练的模型在模糊查询上花费更多计算，有更好的安全 - 拒绝权衡，能更好区分安全和不安全提示，对攻击更鲁棒。

Conclusion: 提供了一个有效的、开放的通过逐提示推理训练大模型抵御越狱和有害请求的方法。

Abstract: Reasoning methods that adaptively allocate test-time compute have advanced
LLM performance on easy to verify domains such as math and code. In this work,
we study how to utilize this approach to train models that exhibit a degree of
robustness to safety vulnerabilities, and show that doing so can provide
benefits. We build a recipe called $\textit{TARS}$ (Training Adaptive Reasoners
for Safety), a reinforcement learning (RL) approach that trains models to
reason about safety using chain-of-thought traces and a reward signal that
balances safety with task completion. To build TARS, we identify three critical
design choices: (1) a "lightweight" warmstart SFT stage, (2) a mix of harmful,
harmless, and ambiguous prompts to prevent shortcut behaviors such as too many
refusals, and (3) a reward function to prevent degeneration of reasoning
capabilities during training. Models trained with TARS exhibit adaptive
behaviors by spending more compute on ambiguous queries, leading to better
safety-refusal trade-offs. They also internally learn to better distinguish
between safe and unsafe prompts and attain greater robustness to both white-box
(e.g., GCG) and black-box attacks (e.g., PAIR). Overall, our work provides an
effective, open recipe for training LLMs against jailbreaks and harmful
requests by reasoning per prompt.

</details>


### [149] [Description of the Training Process of Neural Networks via Ergodic Theorem : Ghost nodes](https://arxiv.org/abs/2507.01003)
*Eun-Ji Park,Sangwon Yun*

Main category: cs.LG

TL;DR: 本文提出统一框架理解和加速DNN训练，引入诊断指标区分收敛类型，提出幽灵类别扩展方法加速早期训练并保持渐近行为。


<details>
  <summary>Details</summary>
Motivation: 基于遍历视角研究DNN训练过程，旨在理解和加速DNN通过随机梯度下降的训练。

Method: 分析目标函数几何景观，引入最大Lyapunov指数运行估计作为诊断指标；提出标准分类器的幽灵类别扩展，添加辅助幽灵输出节点。

Result: 幽灵类别扩展严格减少近似误差，收敛后幽灵维度坍塌，扩展模型与原模型不变律一致，存在使总损失不增且原损失任意下降的路径。

Conclusion: 这些结果提供了一种架构层面的干预，能加速早期训练同时保持渐近行为。

Abstract: Recent studies have proposed interpreting the training process from an
ergodic perspective. Building on this foundation we present a unified framework
for understanding and accelerating the training of deep neural networks via
stochastic gradient descent. By analyzing the geometric landscape of the
objective function we introduce a practical diagnostic, the running estimate of
the largest Lyapunov exponent, which provably distinguishes genuine convergence
toward stable minimizers from mere statistical stabilization near saddle
points. We then propose a ghost category extension for standard classifiers
that adds auxiliary ghost output nodes so the model gains extra descent
directions that open a lateral corridor around narrow loss barriers and enable
the optimizer to bypass poor basins during the early training phase. We show
that this extension strictly reduces approximation error and that after
sufficient convergence the ghost dimensions collapse and the extended model's
invariant law coincides with that of the original and there exists a path in
the enlarged parameter space along which the total loss does not increase while
the original loss decreases by an arbitrary margin. Taken together these
results provide a principled architecture level intervention that accelerates
early stage trainability while preserving asymptotic behavior.

</details>


### [150] [ZeCO: Zero Communication Overhead Sequence Parallelism for Linear Attention](https://arxiv.org/abs/2507.01004)
*Yuhong Chou,Zehao Liu,Ruijie Zhu,Xinyi Wan,Tianjian Li,Congying Chu,Qian Liu,Jibin Wu,Zejun Ma*

Main category: cs.LG

TL;DR: 本文提出ZeCO序列并行方法，利用All - Scan原语减少通信开销，理论证明其最优，实验显示比SOTA方法有显著加速。


<details>
  <summary>Details</summary>
Motivation: 现有序列并行方法在处理线性注意力模型时通信开销大，成为主要瓶颈，需要新方法解决。

Method: 提出ZeCO序列并行方法，核心是All - Scan集体通信原语，为各SP等级提供所需初始算子状态并减少通信开销。

Result: 理论证明ZeCO最优，开销可忽略；实验表明All - Scan在SP场景通信最快，在256个GPU和8M序列长度下比SOTA方法加速60%。

Conclusion: ZeCO为训练下一代大语言模型处理长序列提供了有效途径。

Abstract: Linear attention mechanisms deliver significant advantages for Large Language
Models (LLMs) by providing linear computational complexity, enabling efficient
processing of ultra-long sequences (e.g., 1M context). However, existing
Sequence Parallelism (SP) methods, essential for distributing these workloads
across devices, become the primary bottleneck due to substantial communication
overhead. In this paper, we introduce ZeCO (Zero Communication Overhead)
sequence parallelism for linear attention models, a new SP method designed to
overcome these limitations and achieve end-to-end near-linear scalability for
long sequence training. For example, training a model with a 1M sequence length
across 64 devices using ZeCO takes roughly the same time as training with an
16k sequence on a single device. At the heart of ZeCO lies All-Scan, a new
collective communication primitive. All-Scan provides each SP rank with
precisely the initial operator state it requires while maintaining a minimal
communication footprint, effectively eliminating communication overhead.
Theoretically, we prove the optimaity of ZeCO, showing that it introduces only
negligible time and space overhead. Empirically, we compare the communication
costs of different sequence parallelism strategies and demonstrate that
All-Scan achieves the fastest communication in SP scenarios. Specifically, on
256 GPUs with an 8M sequence length, ZeCO achieves a 60\% speedup compared to
the current state-of-the-art (SOTA) SP method. We believe ZeCO establishes a
clear path toward efficiently training next-generation LLMs on previously
intractable sequence lengths.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [151] [SwarmFusion: Revolutionizing Disaster Response with Swarm Intelligence and Deep Learning](https://arxiv.org/abs/2507.00005)
*Vasavi Lankipalle*

Main category: cs.NE

TL;DR: SwarmFusion结合粒子群优化和卷积神经网络，处理多源数据以优化资源分配和路径规划，模拟显示在灾难响应中效果好，可用于多场景危机管理。


<details>
  <summary>Details</summary>
Motivation: 灾难响应需要在混乱环境中快速、自适应决策，提升实时资源分配和路径规划能力。

Method: 提出SwarmFusion混合框架，结合粒子群优化与卷积神经网络，处理卫星、无人机和传感器的实时数据。

Result: 使用DisasterSim2025数据集模拟显示，响应时间最多快40%，幸存者覆盖率达90%。

Conclusion: SwarmFusion是一种可扩展、数据驱动的方法，为时间关键的灾难管理提供变革性解决方案，适用于多种危机场景。

Abstract: Disaster response requires rapid, adaptive decision-making in chaotic
environments. SwarmFusion, a novel hybrid framework, integrates particle swarm
optimization with convolutional neural networks to optimize real-time resource
allocation and path planning. By processing live satellite, drone, and sensor
data, SwarmFusion enhances situational awareness and operational efficiency in
flood and wildfire scenarios. Simulations using the DisasterSim2025 dataset
demonstrate up to 40 percentage faster response times and 90 percentage
survivor coverage compared to baseline methods. This scalable, data-driven
approach offers a transformative solution for time-critical disaster
management, with potential applications across diverse crisis scenarios.

</details>


### [152] [A Review on Zeroing Neural Networks](https://arxiv.org/abs/2507.00387)
*Chengze Jiang,Jie Gui,Long Jin,Shuai Li*

Main category: cs.NE

TL;DR: 本文对零化神经网络（ZNNs）在实现方法、分析理论和实际应用方面的进展进行了综述。


<details>
  <summary>Details</summary>
Motivation: 目前很少有研究致力于阐明不同ZNNs之间的关系及其推导，因此需要对该领域进行系统综述以增进理解。

Method: 开展对ZNNs相关进展的调查研究。

Result: 呈现了ZNNs在实现方法、分析理论和实际应用方面的进展情况。

Conclusion: 通过综述为该领域的系统理解提供了依据。

Abstract: Zeroing neural networks (ZNNs) have demonstrated outstanding performance on
time-varying optimization and control problems. Nonetheless, few studies are
committed to illustrating the relationship among different ZNNs and the
derivation of them. Therefore, reviewing the advances for a systematical
understanding of this field is desirable. This paper provides a survey of ZNNs'
progress regarding implementing methods, analysis theory, and practical
applications.

</details>


### [153] [Novel Complex-Valued Hopfield Neural Networks with Phase and Magnitude Quantization](https://arxiv.org/abs/2507.00461)
*Garimella Ramamurthy,Marcos Eduardo Valle,Tata Jagannadha Swamy*

Main category: cs.NE

TL;DR: 本文提出两种含相位和幅值量化的新型复值Hopfield神经网络，增加状态数并拓展应用范围。


<details>
  <summary>Details</summary>
Motivation: 拓展复值Hopfield神经网络（CvHNNs）的应用范围。

Method: 提出两种新型CvHNNs，分别使用基于直角坐标和极坐标表示的上限型激活函数并进行相位和幅值量化。

Result: 与现有模型相比，提出的CvHNNs显著增加了状态数。

Conclusion: 所提CvHNNs能拓展CvHNNs的潜在应用范围。

Abstract: This research paper introduces two novel complex-valued Hopfield neural
networks (CvHNNs) that incorporate phase and magnitude quantization. The first
CvHNN employs a ceiling-type activation function that operates on the
rectangular coordinate representation of the complex net contribution. The
second CvHNN similarly incorporates phase and magnitude quantization but
utilizes a ceiling-type activation function based on the polar coordinate
representation of the complex net contribution. The proposed CvHNNs, with their
phase and magnitude quantization, significantly increase the number of states
compared to existing models in the literature, thereby expanding the range of
potential applications for CvHNNs.

</details>


### [154] [High-resolution spatial memory requires grid-cell-like neural codes](https://arxiv.org/abs/2507.00598)
*Madison Cotteret,Christopher J. Kymn,Hugh Greatorex,Martin Ziegler,Elisabetta Chicca,Friedrich T. Sommer*

Main category: cs.NE

TL;DR: 论文指出连续吸引子网络（CANs）存在稳定性 - 分辨率困境，提出基于随机特征嵌入的稀疏二进制分布式编码，理论和模拟表明该编码使CANs兼具高稳定性和高分辨率，还拓展了模型应用。


<details>
  <summary>Details</summary>
Motivation: CANs记忆机制对生物系统常见的噪声或异质性等小缺陷敏感，离散化虽增加稳定性但降低分辨率，存在稳定性 - 分辨率困境，需解决此问题。

Method: 研究基于随机特征嵌入的稀疏二进制分布式编码，神经元具有空间周期性感受野。

Result: 理论和模拟表明此类类似网格细胞的编码能使CANs同时实现高稳定性和高分辨率，模型可拓展到嵌入任意非线性流形及推广线性路径积分。

Conclusion: 该工作提供了大脑如何高分辨率、稳健地表示连续变量并在与任务相关的流形上进行灵活计算的理论。

Abstract: Continuous attractor networks (CANs) are widely used to model how the brain
temporarily retains continuous behavioural variables via persistent recurrent
activity, such as an animal's position in an environment. However, this memory
mechanism is very sensitive to even small imperfections, such as noise or
heterogeneity, which are both common in biological systems. Previous work has
shown that discretising the continuum into a finite set of discrete attractor
states provides robustness to these imperfections, but necessarily reduces the
resolution of the represented variable, creating a dilemma between stability
and resolution. We show that this stability-resolution dilemma is most severe
for CANs using unimodal bump-like codes, as in traditional models. To overcome
this, we investigate sparse binary distributed codes based on random feature
embeddings, in which neurons have spatially-periodic receptive fields. We
demonstrate theoretically and with simulations that such grid-cell-like codes
enable CANs to achieve both high stability and high resolution simultaneously.
The model extends to embedding arbitrary nonlinear manifolds into a CAN, such
as spheres or tori, and generalises linear path integration to integration
along freely-programmable on-manifold vector fields. Together, this work
provides a theory of how the brain could robustly represent continuous
variables with high resolution and perform flexible computations over
task-relevant manifolds.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [155] [VTS-Guided AI Interaction Workflow for Business Insights](https://arxiv.org/abs/2507.00347)
*Sun Ding,Ude Enebeli,Atilhan,Manay,Ryan Pua,Kamal Kotak*

Main category: cs.SE

TL;DR: VTS - AI integrates Visual Thinking Strategies into AI agents to extract business insights from unstructured reports. It has a three - tier system, shows good test results, and has planned upgrades for rapid business analysis.


<details>
  <summary>Details</summary>
Motivation: Modern firms struggle to turn dense, unstructured reports into usable insights quickly, and VTS - AI aims to address this gap.

Method: Integrate Visual Thinking Strategies into AI agents. The system works in three tiers, tags issues, links to source pages, and stores action levers in a searchable YAML file.

Result: In tests on an 18 - page business report, it matched ChatGPT's speed and produced richer findings. It can spot key metric directions and flag where more number - crunching is needed.

Conclusion: Planned upgrades like mapping narrative tags to financial ratios aim to make VTS - AI a production - ready, audit - friendly tool for rapid business analysis.

Abstract: Modern firms face a flood of dense, unstructured reports. Turning these
documents into usable insights takes heavy effort and is far from agile when
quick answers are needed. VTS-AI tackles this gap. It integrates Visual
Thinking Strategies, which emphasize evidence-based observation, linking, and
thinking, into AI agents, so the agents can extract business insights from
unstructured text, tables, and images at scale. The system works in three tiers
(micro, meso, macro). It tags issues, links them to source pages, and rolls
them into clear action levers stored in a searchable YAML file. In tests on an
18-page business report, VTS-AI matched the speed of a one-shot ChatGPT prompt
yet produced richer findings: page locations, verbatim excerpts, severity
scores, and causal links. Analysts can accept or adjust these outputs in the
same IDE, keeping human judgment in the loop. Early results show VTS-AI spots
the direction of key metrics and flags where deeper number-crunching is needed.
Next steps include mapping narrative tags to financial ratios, adding
finance-tuned language models through a Model-Context Protocol, and building a
Risk & Safety Layer to stress-test models and secure data. These upgrades aim
to make VTS-AI a production-ready, audit-friendly tool for rapid business
analysis.

</details>


### [156] [An AST-guided LLM Approach for SVRF Code Synthesis](https://arxiv.org/abs/2507.00352)
*Abanoub E. Abdelmalak,Mohamed A. Elsayed,David Abercrombie,Ilhami Torunoglu*

Main category: cs.SE

TL;DR: 本文提出结合AST嵌入和RAG的新方法用于SVRF代码合成，在DRC规则实现基准测试中准确率提升，优化开发并提高生产力。


<details>
  <summary>Details</summary>
Motivation: 随着半导体节点推进，传统SVRF开发方法在复杂设计规则下低效，且存在专业知识差距。

Method: 引入结合AST嵌入和RAG的方法进行SVRF代码合成，评估不同T5模型并提出SVRF特定评分框架。

Result: 在740个DRC规则实现的基准测试中，代码生成准确率相比基本文本微调过程最多提高40%。

Conclusion: 该方法融合行业专业知识与先进编码策略，优化SVRF开发，创建高效编码环境，提高整体生产力。

Abstract: Standard Verification Rule Format (SVRF) is essential for semiconductor
applications like Design Rule Check (DRC), Layout Versus Schematic (LVS), and
Optical Proximity Correction (OPC) and it faces challenges as advancing nodes
create complex design rules that renders traditional SVRF development
ineffective and highlight an expertise gap. This paper introduces a novel
methodology integrating Abstract Syntax Tree (AST) embedding and
Retrieval-Augmented Generation (RAG) for enhanced SVRF code synthesis, ensuring
semantic accuracy and error minimization through structural validation with
domain-specific insights for precise code generation.
  We evaluate different T5-based models and propose an innovative SVRF-specific
scoring framework that complements standard metrics like BLEU and ROUGE-L. In
our approach, AST provides rigorous structural validation, while RAG infuses
relevant domain knowledge, effectively enhancing the code generation workflow.
  Testing on a comprehensive benchmark of 740 DRC rule implementations, our
methodology demonstrates up to a 40\% improvement in code generation accuracy
compared to basic text-based fine-tuning process. This fusion of industry
expertise with advanced coding strategies not only optimizes SVRF development
under limited dataset constraints but also creates a more intuitive and
efficient coding environment. Consequently, users can rapidly iterate through
design cycles, reduce manual error correction, and significantly improve
overall productivity.

</details>


### [157] [iPanda: An Intelligent Protocol Testing and Debugging Agent for Conformance Testing](https://arxiv.org/abs/2507.00378)
*Xikai Sun,Fan Dang,Kebin Liu,Xin Miao,Zihao Yang,Haimo Lu,Yawen Zheng,Yunhao Liu*

Main category: cs.SE

TL;DR: 提出利用大语言模型的端到端协议一致性测试框架iPanda，实验显示其性能优于纯大语言模型方法。


<details>
  <summary>Details</summary>
Motivation: 传统协议一致性测试手动创建用例和脚本的方式劳动密集且低效，大语言模型的能力为自动化提供了机会。

Method: iPanda先使用基于关键词的方法生成测试用例，再用基于代码的检索增强生成方法生成可执行测试代码，还加入迭代自我修正机制提升代码质量，最后执行并分析测试。

Result: 在多种协议上的实验表明，iPanda显著优于纯大语言模型方法，测试代码生成成功率（Pass@1）提高4.675到10.751倍。

Conclusion: iPanda是一个有效的自动化协议一致性测试框架。

Abstract: Conformance testing is essential for ensuring that protocol implementations
comply with their specifications. However, traditional testing approaches
involve manually creating numerous test cases and scripts, making the process
labor-intensive and inefficient. Recently, Large Language Models (LLMs) have
demonstrated impressive text comprehension and code generation abilities,
providing promising opportunities for automation. In this paper, we propose
iPanda, the first end-to-end framework that leverages LLMs to automate protocol
conformance testing. Given a protocol specification document and its
implementation, iPanda first employs a keyword-based method to automatically
generate comprehensive test cases. Then, it utilizes a code-based
retrieval-augmented generation approach to effectively interpret the
implementation and produce executable test code. To further enhance code
quality, iPanda incorporates an iterative self-correction mechanism to refine
generated test scripts interactively. Finally, by executing and analyzing the
generated tests, iPanda systematically verifies compliance between
implementations and protocol specifications. Comprehensive experiments on
various protocols show that iPanda significantly outperforms pure LLM-based
approaches, improving the success rate (Pass@1) of test-code generation by
factors ranging from 4.675 times to 10.751 times.

</details>


### [158] [Recommending Variable Names for Extract Local Variable Refactorings](https://arxiv.org/abs/2507.00413)
*Taiming Wang,Hui Liu,Yuxia Zhang,Yanjie Jiang*

Main category: cs.SE

TL;DR: 本文介绍了VarNamer方法，为提取局部变量重构推荐变量名，经评估优于现有IDE，在C++项目有类似效果，用户研究显示可提升重构效率。


<details>
  <summary>Details</summary>
Motivation: 现有IDE为提取局部变量重构推荐的变量名约70%与开发者手动构造的不同，增加重命名负担，提供的帮助有限。

Method: 通过大规模实证研究确定用于组合变量名的关键上下文，利用程序静态分析技术开发启发式规则，采用数据挖掘技术推荐变量名。

Result: VarNamer比Eclipse精确匹配率提高52.6%，比IntelliJ IDEA提高40.7%；在C++项目有类似表现；用户研究显示可提速27.8%，减少49.3%编辑。

Conclusion: VarNamer优于现有IDE，在不同编程语言有一定通用性，能提升重构效率。

Abstract: Extract local variable is one of the most popular refactorings, and most IDEs
and refactoring tools provide automated support for this refactoring. However,
we find approximately 70% of the names recommended by these IDEs are different
from what developers manually constructed, adding additional renaming burdens
to developers and providing limited assistance. In this paper, we introduce
VarNamer, an automated approach designed to recommend variable names for
extract local variable refactorings. Through a large-scale empirical study, we
identify key contexts that are useful for composing variable names. Leveraging
these insights, we developed a set of heuristic rules through program static
analysis techniques and employ data mining techniques to recommend variable
names effectively. Notably, some of our heuristic rules have been successfully
integrated into Eclipse, where they are now distributed with the latest
releases of the IDE. Evaluation demonstrates its superiority over
state-of-the-art IDEs. Specifically, VarNamer significantly increases the
chance of exact match by 52.6% compared to Eclipse and 40.7% compared to
IntelliJ IDEA. We also evaluated the proposed approach with real-world extract
local variable refactorings conducted in C++ projects, and the results suggest
that the approach can achieve comparable performance on programming languages
besides Java. It may suggest the generalizability of VarNamer. Finally, we
designed and conducted a user study and the results of the user study suggest
that our approach can speed up the refactoring by 27.8% and reduce 49.3% edits
on the recommended variable names.

</details>


### [159] [Embedded DevOps: A Survey on the Application of DevOps Practices in Embedded Software and Firmware Development](https://arxiv.org/abs/2507.00421)
*Parthiv Katapara,Anand Sharma*

Main category: cs.SE

TL;DR: 本文通过文献综述探讨DevOps实践在嵌入式系统和固件开发中的应用，分析其适配情况、指出当前局限并提出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 应对现代软硬件协同设计产品日益增长的复杂性，解决嵌入式系统开发面临的硬件依赖、实时约束和安全关键要求等挑战。

Method: 对20篇学术和行业来源的文献进行综合分析，对工具、测试策略、管道自动化和安全实践等方面的工作进行分类。

Result: 发现当前部署工作流程和可观测性存在局限性。

Conclusion: 为研究人员和从业者提供嵌入式DevOps的综合理解，以结构化视角整合零散文献。

Abstract: The adoption of DevOps practices in embedded systems and firmware development
is emerging as a response to the growing complexity of modern
hardware--software co-designed products. Unlike cloud-native applications,
embedded systems introduce challenges such as hardware dependency, real-time
constraints, and safety-critical requirements. This literature review
synthesizes findings from 20 academic and industrial sources to examine how
DevOps principles--particularly continuous integration, continuous delivery,
and automated testing--are adapted to embedded contexts. We categorize efforts
across tooling, testing strategies, pipeline automation, and security
practices. The review highlights current limitations in deployment workflows
and observability, proposing a roadmap for future research. This work offers
researchers and practitioners a consolidated understanding of Embedded DevOps,
bridging fragmented literature with a structured perspective.

</details>


### [160] [The Influence of HEXACO Personality Traits on the Teamwork Quality in Software Teams -- A Preliminary Research Approach](https://arxiv.org/abs/2507.00481)
*Philipp M. Zähl,Sabine Theis,Martin R. Wolf*

Main category: cs.SE

TL;DR: 本文旨在研究HEXACO人格特质对软件团队协作质量的影响，初步数据收集分析表明人格特质及其构成、其他变量影响团队协作质量，验证了研究设计的有效性并指出改进方向和后续研究途径。


<details>
  <summary>Details</summary>
Motivation: 软件工程研究虽关注流程和技术优化，但人的因素尤其是团队协作也影响优化，且开发者人格对团队协作有重要影响，故研究人格特质对软件团队协作质量的影响。

Method: 进行初步数据收集（n = 54）。

Result: 多个性格特质及其构成对团队协作质量有显著影响，女性比例和年龄分布等其他变量也影响团队协作质量，研究设计有效。

Conclusion: 研究设计有用且有效，为IT组织改善团队协作提供机会，指明进一步研究方向。

Abstract: Although software engineering research has focused on optimizing processes
and technology, there is a growing recognition that human factors, particularly
teamwork, also significantly impact optimization. Recent research suggests that
developer personality has a strong influence on teamwork. In fact, personality
considerations may have a greater impact on software development than processes
and tools. This paper aims to design a study that measures the impact of HEXACO
personality traits on the Teamwork Quality (TWQ) of software teams. A
preliminary data collection (n=54) was conducted for this purpose. The analysis
showed that several personality traits, as well as their composition, had a
significant impact on TWQ. Additionally, other variables, such as the
proportion of women and age distribution, also affected TWQ. The study's
initial results demonstrate the usefulness and validity of the study design.
The results also suggest several opportunities to improve teamwork in IT
organizations and avenues for further research.

</details>


### [161] [Coverage-Guided Testing for Deep Learning Models: A Comprehensive Survey](https://arxiv.org/abs/2507.00496)
*Hongjing Guo,Chuanqi Tao,Zhiqiu Huang,Weiqin Zou*

Main category: cs.SE

TL;DR: 本文对深度学习模型的覆盖引导测试（CGT）方法进行全面综述，提供分类、研究评估实践，指出开放挑战和未来方向，为模型质量保证研究与实践提供路线图。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型应用于安全关键领域，确保其质量是挑战，现有CGT研究方法零散，需全面了解进展和趋势。

Method: 对最先进的CGT方法进行全面综述，根据方法特征和应用场景提供详细分类，研究现有研究的评估实践。

Result: 完成对CGT方法的综述、分类，明确现有研究的评估实践。

Conclusion: 指出结构覆盖与测试目标的相关性、方法跨任务和模型的泛化性等开放挑战及未来方向，为模型质量保证提供路线图。

Abstract: As Deep Learning (DL) models are increasingly applied in safety-critical
domains, ensuring their quality has emerged as a pressing challenge in modern
software engineering. Among emerging validation paradigms, coverage-guided
testing (CGT) has gained prominence as a systematic framework for identifying
erroneous or unexpected model behaviors. Despite growing research attention,
existing CGT studies remain methodologically fragmented, limiting the
understanding of current advances and emerging trends. This work addresses that
gap through a comprehensive review of state-of-the-art CGT methods for DL
models, including test coverage analysis, coverage-guided test input
generation, and coverage-guided test input optimization. This work provides
detailed taxonomies to organize these methods based on methodological
characteristics and application scenarios. We also investigate evaluation
practices adopted in existing studies, including the use of benchmark datasets,
model architectures, and evaluation aspects. Finally, open challenges and
future directions are highlighted in terms of the correlation between
structural coverage and testing objectives, method generalizability across
tasks and models, practical deployment concerns, and the need for standardized
evaluation and tool support. This work aims to provide a roadmap for future
academic research and engineering practice in DL model quality assurance.

</details>


### [162] [A Domain-specific Language and Architecture for Detecting Process Activities from Sensor Streams in IoT](https://arxiv.org/abs/2507.00686)
*Ronny Seiger,Daniel Locher,Marco Kaufmann,Aaron F. Kurz*

Main category: cs.SE

TL;DR: 本文提出用领域特定语言Radiant将物联网传感器数据抽象为业务流程事件，给出软件架构并在智能制造和医疗场景评估。


<details>
  <summary>Details</summary>
Motivation: 物联网系统传感器数据细粒度高，难用于业务流程分析，需将低级别传感器数据提升到业务流程级别。

Method: 开发领域特定语言Radiant指定传感器数据中表示高级流程活动执行的模式，转化为复杂事件处理应用，提出在线事件抽象软件架构。

Result: 在智能制造和智能医疗场景评估监测活动执行的应用，评估结果可告知领域专家活动检测质量和改进潜力。

Conclusion: 借助Radiant和相应软件架构，能实现物联网传感器数据到业务流程的事件抽象，评估结果可指导领域专家改进。

Abstract: Modern Internet of Things (IoT) systems are equipped with a plethora of
sensors providing real-time data about the current operations of their
components, which is crucial for the systems' internal control systems and
processes. However, these data are often too fine-grained to derive useful
insights into the execution of the larger processes an IoT system might be part
of. Process mining has developed advanced approaches for the analysis of
business processes that may also be used in the context of IoT. Bringing
process mining to IoT requires an event abstraction step to lift the low-level
sensor data to the business process level. In this work, we aim to empower
domain experts to perform this step using a newly developed domain-specific
language (DSL) called Radiant. Radiant supports the specification of patterns
within the sensor data that indicate the execution of higher level process
activities. These patterns are translated to complex event processing (CEP)
applications to be used for detecting activity executions at runtime. We
propose a corresponding software architecture for online event abstraction from
IoT sensor streams using the CEP applications. We evaluate these applications
to monitor activity executions using IoT sensors in smart manufacturing and
smart healthcare. The evaluation method and results inform the domain expert
about the quality of activity detections and potential for improvement.

</details>


### [163] [A Hierarchical and Evolvable Benchmark for Fine-Grained Code Instruction Following with Multi-Turn Feedback](https://arxiv.org/abs/2507.00699)
*Guoliang Duan,Mingwei Liu,Yanlin Wang,Chong Wang,Xin Peng,Zibin Zheng*

Main category: cs.SE

TL;DR: 提出MultiCodeIF基准评估代码生成指令遵循能力，评估六个大模型，发现性能差异，反馈可提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有基准常忽视现实开发中复杂编程指令的细微要求，需评估大模型在多维度遵循指令的能力。

Method: 引入MultiCodeIF基准，基于9类27种约束类型的分类法，用自动化管道ConstraGen合成2021个代码任务，进行多轮评估。

Result: 六个大模型性能有差距，对显式约束表现好，多层约束降低成功率，结构化反馈可提升性能。

Conclusion: MultiCodeIF为现实代码生成场景提供框架，缩小合成评估与现实指令复杂性的差距，相关资源开源。

Abstract: Large language models (LLMs) have advanced significantly in code generation,
yet their ability to follow complex programming instructions with layered and
diverse constraints remains underexplored. Existing benchmarks often prioritize
functional correctness, overlooking the nuanced requirements found in
real-world development. We introduce MultiCodeIF, a comprehensive benchmark
designed to evaluate instruction-following in code generation across multiple
dimensions: constraint type, hierarchical levels, and iterative refinement.
Built upon a structured taxonomy of 9 categories and 27 constraint types,
MultiCodeIF enables granular assessment of both functional and non-functional
instruction adherence. Using an automated pipeline, ConstraGen, we synthesize
and evolve 2,021 code tasks sourced from 14 programming languages, supporting
multi-turn evaluation through feedback-driven task variants. Empirical
evaluation of six state-of-the-art LLMs uncovers substantial performance
disparities. The top-performing model, Claude-3-7-Sonnet, achieves 63.0%
average constraint satisfaction, while smaller models like Qwen3-1.7B fall to
44.8%. Models perform well on explicit constraints, but struggle with implicit
or abstract constraints. Tasks with multiple hierarchical constraints
significantly reduce model success rates, from 54.5% in single-level to just
18.8% in multi-level scenarios. However, structured feedback enables
progressive improvement: average constraint satisfaction rises from 63.0% to
83.4% over four iterative refinement rounds. MultiCodeIF provides a scalable,
constraint-aware, and feedback-sensitive framework to benchmark LLMs under
realistic code generation scenarios, bridging the gap between synthetic
evaluations and real-world instruction complexity. The full benchmark dataset,
evaluation pipeline, and source code are available at
https://github.com/SYSUSELab/MultiCodeIF.

</details>


### [164] [Snaps: Bloated and Outdated?](https://arxiv.org/abs/2507.00786)
*Jukka Ruohonen,Qusai Ramadan*

Main category: cs.SE

TL;DR: 分析Snap软件包系统，发现其分发的包存在大小臃肿、更新频率低问题。


<details>
  <summary>Details</summary>
Motivation: 鉴于不同Linux发行版及其版本的异构性，Snap可实现软件直接交付用户，但存在相关批评，需研究其问题。

Method: 对当前分发的snap包进行观察分析。

Result: 当前分发的snap包平均大小臃肿，更新频率过时。

Conclusion: 该研究对软件打包、软件包和包管理器的研究领域有贡献。

Abstract: Snap is an alternative software packaging system developed by Canonical and
provided by default in the Ubuntu Linux distribution. Given the heterogeneity
of various Linux distributions and their various releases, Snap allows an
interoperable delivery of software directly to users. However, concerns and
criticism have also been frequently expressed. Regarding this criticism, the
paper shows that currently distributed snap packages are indeed on average
bloated in terms of their sizes and outdated in terms updating frequencies.
With these empirical observations, this short paper contributes to the research
domain of software packaging, software packages, and package managers.

</details>


### [165] [Echoes of AI: Investigating the Downstream Effects of AI Assistants on Software Maintainability](https://arxiv.org/abs/2507.00788)
*Markus Borg,Dave Hewett,Nadim Hagatulah,Noric Couderc,Emma Söderberg,Donald Graham,Uttam Kini,Dave Farley*

Main category: cs.SE

TL;DR: 研究探讨AI辅助开发对软件可维护性的影响，通过实验发现AI辅助能加速开发，未发现代码可维护性下降迹象。


<details>
  <summary>Details</summary>
Motivation: 已有研究表明AI助手提升了软件开发生产力，但对软件可维护性的影响有待研究。

Method: 进行两阶段对照实验，151名参与者参与，在第一阶段部分人使用AI辅助添加新功能，第二阶段新参与者在无AI辅助下改进代码。

Result: 第一阶段AI辅助开发使后续演进有适度加速，平均CodeHealth略高；习惯使用AI的用户完成第一阶段时，CodeHealth提升显著；使用AI助手使任务完成时间中位数减少30.7%，习惯使用AI的用户平均加速55.9%。

Conclusion: AI助手能有效加速开发，未发现代码可维护性下降迹象，建议未来研究关注代码膨胀和认知债务等风险。

Abstract: [Context] AI assistants, like GitHub Copilot and Cursor, are transforming
software engineering. While several studies highlight productivity
improvements, their impact on maintainability requires further investigation.
[Objective] This study investigates whether co-development with AI assistants
affects software maintainability, specifically how easily other developers can
evolve the resulting source code. [Method] We conducted a two-phase controlled
experiment involving 151 participants, 95% of whom were professional
developers. In Phase 1, participants added a new feature to a Java web
application, with or without AI assistance. In Phase 2, a randomized controlled
trial, new participants evolved these solutions without AI assistance.
[Results] AI-assisted development in Phase 1 led to a modest speedup in
subsequent evolution and slightly higher average CodeHealth. Although neither
difference was significant overall, the increase in CodeHealth was
statistically significant when habitual AI users completed Phase 1. For Phase
1, we also observed a significant effect that corroborates previous
productivity findings: using an AI assistant yielded a 30.7% median decrease in
task completion time. Moreover, for habitual AI users, the mean speedup was
55.9%. [Conclusions] Our study adds to the growing evidence that AI assistants
can effectively accelerate development. Moreover, we did not observe warning
signs of degraded code-level maintainability. We recommend that future research
focus on risks such as code bloat from excessive code generation and the
build-up of cognitive debt as developers invest less mental effort during
implementation.

</details>


### [166] [Out of the Day Job: Perspectives of Industry Practitioners in Co-Design and Delivery of Software Engineering Courses](https://arxiv.org/abs/2507.00803)
*Gillian Daniel,Chris Hall,Per Hammer,Alec-Angus Macdonald,Hollie Marwick-Best,Emma McKenzie,George Popa,Derek Somerville,Tim Storer*

Main category: cs.SE

TL;DR: 本文回顾了格拉斯哥大学与行业合作设计软件工程课程情况，关注行业从业者参与课程设计和交付的视角，给出讨论主题和未来合作建议。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对参与课程设计和交付的行业从业者视角的探讨，了解他们的动机、期望和经验对未来合作的形成和可持续性很重要。

Method: 由学术合著者作为促进者，对行业从业者合著者进行回顾性研究。

Result: 得出了讨论中出现的主题。

Conclusion: 给出了未来合作的建议。

Abstract: Over more than two decades, The University of Glasgow has co-designed and
delivered numerous software engineering focused courses with industry partners,
covering both technical and discipline specific professional skills. Such
collaborations are not unique and many of the benefits are well recognised in
the literature. These include enhancing the real-world relevance of curricula,
developing student professional networks ahead of graduation and easing
recruitment opportunities for employers.
  However, there is relatively little scholarship on the perspectives of
industry practitioners who participate in course design and delivery. This gap
is significant, since the effort invested by practitioners is often substantial
and may require ongoing support from both the industry partner and academic
institution. Understanding the motivations, expectations and experiences of
practitioners who engage in course delivery can guide the formation of future
partnerships and ensure their long-term sustainability.
  We begin to address this gap by reporting on the outcomes of a retrospective
conducted amongst the practitioner coauthors of this paper, with the academic
coauthors acting as facilitators. All coauthors have participated in the recent
co-design and delivery of software engineering courses, but we choose to focus
explicitly on the perspectives of the practitioners. We report on the themes
that emerged from the discussions and our resulting recommendations for future
collaborations.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [167] [Scale-Dependent Multifractality in Bitcoin Realised Volatility: Implications for Rough Volatility Modelling](https://arxiv.org/abs/2507.00575)
*Milan Pontiggia*

Main category: q-fin.ST

TL;DR: 运用Cont和Das（2024）的归一化p - 变差框架评估粗糙波动率模型对比特币已实现波动率的适用性，发现该模型不适用，比特币波动率有多重分形结构。


<details>
  <summary>Details</summary>
Motivation: 评估粗糙波动率模型对比特币已实现波动率的适用性。

Method: 使用归一化p - 变差框架，对2017 - 2024年高频比特币数据进行分析，进行平稳性测试和稳健性检查，采用多重分形去趋势波动分析、对数 - 对数矩标度和小波领导者三种诊断方法。

Result: 归一化统计量始终为负，无法估计有效粗糙度指数；无显著非平稳性或结构突变证据；比特币波动率有多重分形结构，违反粗糙波动率估计的同质性假设。

Conclusion: 粗糙波动率模型在传统市场表现良好，但与比特币波动率的经验特征在结构上不匹配。

Abstract: We assess the applicability of rough volatility models to Bitcoin realised
volatility using the normalised p-variation framework of Cont and Das (2024).
Applying this model free estimator to high-frequency Bitcoin data from 2017 to
2024 across multiple sampling resolutions, we find that the normalised
statistic remains strictly negative throughout, precluding the estimation of a
valid roughness index. Stationarity tests and robustness checks reveal no
significant evidence of non-stationarity or structural breaks as explanatory
factors. Instead, convergent evidence from three complementary diagnostics,
namely multifractal detrended fluctuation analysis, log-log moment scaling, and
wavelet leaders, reveals a multifractal structure in Bitcoin volatility. This
scale-dependent behaviour violates the homogeneity assumptions underlying rough
volatility estimation and accounts for the estimator's systematic failure.
These findings suggest that while rough volatility models perform well in
traditional markets, they are structurally misaligned with the empirical
features of Bitcoin volatility.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [168] [Disentangled Feature Importance](https://arxiv.org/abs/2507.00260)
*Jin-Hong Du,Kathryn Roeder,Larry Wasserman*

Main category: stat.ML

TL;DR: 现有特征重要性量化方法在特征相关时存在低估问题，提出Disentangled Feature Importance (DFI)解决该问题，建立了半参数理论并证明其效率。


<details>
  <summary>Details</summary>
Motivation: 解决标准特征重要性量化方法在特征相关时低估贡献的问题。

Method: 引入DFI，通过最优传输将相关特征转换为独立潜变量，消除相关性失真，计算重要性并归因。

Result: 建立了DFI的半参数理论，证明估计量的一致性、渐近正态性和二阶估计误差，避免了重复子模型拟合和条件协变量分布估计的计算负担。

Conclusion: DFI是解决特征相关时特征重要性量化问题的有效方法，具有计算效率。

Abstract: Feature importance quantification faces a fundamental challenge: when
predictors are correlated, standard methods systematically underestimate their
contributions. We prove that major existing approaches target identical
population functionals under squared-error loss, revealing why they share this
correlation-induced bias.
  To address this limitation, we introduce \emph{Disentangled Feature
Importance (DFI)}, a nonparametric generalization of the classical $R^2$
decomposition via optimal transport. DFI transforms correlated features into
independent latent variables using a transport map, eliminating correlation
distortion. Importance is computed in this disentangled space and attributed
back through the transport map's sensitivity. DFI provides a principled
decomposition of importance scores that sum to the total predictive variability
for latent additive models and to interaction-weighted functional ANOVA
variances more generally, under arbitrary feature dependencies.
  We develop a comprehensive semiparametric theory for DFI. For general
transport maps, we establish root-$n$ consistency and asymptotic normality of
importance estimators in the latent space, which extends to the original
feature space for the Bures-Wasserstein map. Notably, our estimators achieve
second-order estimation error, which vanishes if both regression function and
transport map estimation errors are $o_{\mathbb{P}}(n^{-1/4})$. By design, DFI
avoids the computational burden of repeated submodel refitting and the
challenges of conditional covariate distribution estimation, thereby achieving
computational efficiency.

</details>


### [169] [Enhancing Interpretability in Generative Modeling: Statistically Disentangled Latent Spaces Guided by Generative Factors in Scientific Datasets](https://arxiv.org/abs/2507.00298)
*Arkaprabha Ganguli,Nesar Ramachandra,Julie Bessac,Emil Constantinescu*

Main category: stat.ML

TL;DR: 研究在无监督或半监督下从复杂高维数据中提取生成因素，提出Aux - VAE架构，经多数据集验证有效。


<details>
  <summary>Details</summary>
Motivation: 解决无监督或半监督下从复杂高维数据集中统计提取生成因素的挑战。

Method: 研究基于编解码器的生成模型进行非线性降维，提出Aux - VAE架构，利用辅助变量结合先验统计知识，最小化修改标准VAE损失函数实现解纠缠。

Result: 通过在多个数据集（包括天文模拟）上的比较评估验证了Aux - VAE的有效性。

Conclusion: Aux - VAE架构能有效从复杂高维数据中提取生成因素。

Abstract: This study addresses the challenge of statistically extracting generative
factors from complex, high-dimensional datasets in unsupervised or
semi-supervised settings. We investigate encoder-decoder-based generative
models for nonlinear dimensionality reduction, focusing on disentangling
low-dimensional latent variables corresponding to independent physical factors.
Introducing Aux-VAE, a novel architecture within the classical Variational
Autoencoder framework, we achieve disentanglement with minimal modifications to
the standard VAE loss function by leveraging prior statistical knowledge
through auxiliary variables. These variables guide the shaping of the latent
space by aligning latent factors with learned auxiliary variables. We validate
the efficacy of Aux-VAE through comparative assessments on multiple datasets,
including astronomical simulations.

</details>


### [170] [GRAND: Graph Release with Assured Node Differential Privacy](https://arxiv.org/abs/2507.00402)
*Suqing Liu,Xuan Bi,Tianxi Li*

Main category: stat.ML

TL;DR: 提出GRAND方法，用于在保证节点级差分隐私和保留结构属性的同时发布整个网络，并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有节点级隐私方法存在局限性，如局限于基于查询的方法、无法保留网络关键结构属性，且差分隐私在网络数据（尤其是节点级）的应用研究不足。

Method: 提出GRAND（Graph Release with Assured Node Differential privacy）网络发布机制。

Result: 在广泛的潜在空间模型下，发布的网络渐近地与原始网络遵循相同的分布。

Conclusion: 通过对合成和真实世界数据集的大量实验，证明了该方法的有效性。

Abstract: Differential privacy is a well-established framework for safeguarding
sensitive information in data. While extensively applied across various
domains, its application to network data -- particularly at the node level --
remains underexplored. Existing methods for node-level privacy either focus
exclusively on query-based approaches, which restrict output to pre-specified
network statistics, or fail to preserve key structural properties of the
network. In this work, we propose GRAND (Graph Release with Assured Node
Differential privacy), which is, to the best of our knowledge, the first
network release mechanism that releases entire networks while ensuring
node-level differential privacy and preserving structural properties. Under a
broad class of latent space models, we show that the released network
asymptotically follows the same distribution as the original network. The
effectiveness of the approach is evaluated through extensive experiments on
both synthetic and real-world datasets.

</details>


### [171] [Forward Reverse Kernel Regression for the Schrödinger bridge problem](https://arxiv.org/abs/2507.00640)
*Denis Belomestny,John. Schoenmakers*

Main category: stat.ML

TL;DR: 本文研究薛定谔桥问题，提出非参数近似薛定谔势的迭代蒙特卡罗程序，给出收敛算法、收敛率及最优性证明，还提出非嵌套蒙特卡罗程序。


<details>
  <summary>Details</summary>
Motivation: 研究熵最优传输核心的薛定谔桥问题，针对一般参考过程和端点分布进行求解。

Method: 提出正反迭代蒙特卡罗程序，在皮卡迭代中使用基于核的蒙特卡罗回归，保留迭代中正定性和收缩性。

Result: 得到可证明收敛的算法，给出势估计的收敛率并证明其最优性。

Conclusion: 提出了非嵌套蒙特卡罗程序用于薛定谔桥过程的最终维分布。

Abstract: In this paper, we study the Schr\"odinger Bridge Problem (SBP), which is
central to entropic optimal transport. For general reference processes and
begin--endpoint distributions, we propose a forward-reverse iterative Monte
Carlo procedure to approximate the Schr\"odinger potentials in a nonparametric
way. In particular, we use kernel based Monte Carlo regression in the context
of Picard iteration of a corresponding fixed point problem. By preserving in
the iteration positivity and contractivity in a Hilbert metric sense, we
develop a provably convergent algorithm. Furthermore, we provide convergence
rates for the potential estimates and prove their optimality. Finally, as an
application, we propose a non-nested Monte Carlo procedure for the final
dimensional distributions of the Schr\"odinger Bridge process, based on the
constructed potentials and the forward-reverse simulation method for
conditional diffusions.

</details>


### [172] [An in depth look at the Procrustes-Wasserstein distance: properties and barycenters](https://arxiv.org/abs/2507.00894)
*Davide Adamo,Marco Corneli,Manon Vuillien,Emmanuelle Vila*

Main category: stat.ML

TL;DR: 本文围绕Procrustes - Wasserstein (PW) 展开，讨论初始化策略、引入PW重心概念，提出计算点云代表性形状新方法，经测试性能优越并展示其在考古中的应用。


<details>
  <summary>Details</summary>
Motivation: PW作为一种替代Wasserstein的最优传输距离，更适合点云对齐和比较等任务，需拓展其框架并应用于点云分析。

Method: 构建离散概率测度空间，讨论和测试多种初始化策略，引入PW重心概念并给出估计算法，与现有OT方法进行基准测试。

Result: 提出计算点云代表性形状的新方法，在精确对齐和形状保留场景中性能优于现有OT方法，展示了PW重心在考古中的实用性。

Conclusion: PW在机器学习和计算几何的2D和3D点云分析中有很大潜力。

Abstract: Due to its invariance to rigid transformations such as rotations and
reflections, Procrustes-Wasserstein (PW) was introduced in the literature as an
optimal transport (OT) distance, alternative to Wasserstein and more suited to
tasks such as the alignment and comparison of point clouds. Having that
application in mind, we carefully build a space of discrete probability
measures and show that over that space PW actually is a distance. Algorithms to
solve the PW problems already exist, however we extend the PW framework by
discussing and testing several initialization strategies. We then introduce the
notion of PW barycenter and detail an algorithm to estimate it from the data.
The result is a new method to compute representative shapes from a collection
of point clouds. We benchmark our method against existing OT approaches,
demonstrating superior performance in scenarios requiring precise alignment and
shape preservation. We finally show the usefulness of the PW barycenters in an
archaeological context. Our results highlight the potential of PW in boosting
2D and 3D point cloud analysis for machine learning and computational geometry
applications.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [173] [Harnessing the Power of Reinforcement Learning for Adaptive MCMC](https://arxiv.org/abs/2507.00671)
*Congye Wang,Matthew A. Fisher,Heishiro Kanagawa,Wilson Chen,Chris. J. Oates*

Main category: stat.CO

TL;DR: 文章指出采样算法调优负担增大，探讨RLMH训练奖励问题，提出基于对比散度的新奖励，开发自适应梯度采样器，模拟研究支持其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有采样算法调优负担增加，此前Wang等人虽将Metropolis - Hastings 公式化为马尔可夫决策过程，但其实践益处待挖掘。

Method: 提出基于对比散度的新型奖励用于训练RLMH，开发自适应梯度采样器。

Result: 自然选择的奖励如接受率等对训练RLMH信号不足，基于对比散度的奖励表现更优，模拟研究证明RLMH实践有效。

Conclusion: 提出的新型奖励及自适应梯度采样器提升了RLMH的实践效果。

Abstract: Sampling algorithms drive probabilistic machine learning, and recent years
have seen an explosion in the diversity of tools for this task. However, the
increasing sophistication of sampling algorithms is correlated with an increase
in the tuning burden. There is now a greater need than ever to treat the tuning
of samplers as a learning task in its own right. In a conceptual breakthrough,
Wang et al (2025) formulated Metropolis-Hastings as a Markov decision process,
opening up the possibility for adaptive tuning using Reinforcement Learning
(RL). Their emphasis was on theoretical foundations; realising the practical
benefit of Reinforcement Learning Metropolis-Hastings (RLMH) was left for
subsequent work. The purpose of this paper is twofold: First, we observe the
surprising result that natural choices of reward, such as the acceptance rate,
or the expected squared jump distance, provide insufficient signal for training
RLMH. Instead, we propose a novel reward based on the contrastive divergence,
whose superior performance in the context of RLMH is demonstrated. Second, we
explore the potential of RLMH and present adaptive gradient-based samplers that
balance flexibility of the Markov transition kernel with learnability of the
associated RL task. A comprehensive simulation study using the posteriordb
benchmark supports the practical effectiveness of RLMH.

</details>


### [174] [ForLion: An R Package for Finding Optimal Experimental Designs with Mixed Factors](https://arxiv.org/abs/2507.00923)
*Siting Lin,Yifei Huang,Jie Yang*

Main category: stat.CO

TL;DR: 介绍了实现ForLion和EW ForLion算法的ForLion包，可构建最优设计，支持多种模型和因子类型，提供近似和精确设计及使用教程。


<details>
  <summary>Details</summary>
Motivation: 最优设计对实验者收集信息和准确估计参数很重要，需实现算法构建最优设计的工具。

Method: 实现ForLion算法构建局部D - 最优设计，实现EW ForLion算法生成稳健EW D - 最优设计。

Result: 开发了ForLion包，支持线性模型、广义线性模型和多项逻辑模型，可处理不同类型因子，提供近似和精确设计。

Conclusion: ForLion包能帮助实验者在不同场景下构建最优设计，包含使用教程方便使用。

Abstract: Optimal design is crucial for experimenters to maximize the information
collected from experiments and estimate the model parameters most accurately.
ForLion algorithms have been proposed to find D-optimal designs for experiments
with mixed types of factors. In this paper, we introduce the ForLion package
which implements the ForLion algorithm to construct locally D-optimal designs
and the EW ForLion algorithm to generate robust EW D-optimal designs. The
package supports experiments under linear models (LM), generalized linear
models (GLM), and multinomial logistic models (MLM) with continuous, discrete,
or mixed-type factors. It provides both optimal approximate designs and an
efficient function converting approximate designs into exact designs with
integer-valued allocations of experimental units. Tutorials are included to
show the package's usage across different scenarios.

</details>


### [175] [Toward a Data Processing Pipeline for Mobile-Phone Tracking Data](https://arxiv.org/abs/2507.00952)
*Marcin Jurek,Catherine A. Calder,Corwin Zigler,Bethany Boettner,Christopher R. Browning*

Main category: stat.CO

TL;DR: 本文针对青少年健康与发展研究中的手机追踪数据分析流程，受其“分箱算法”启发提出统计框架，能改善轨迹估计，可作未来研究默认数据处理工具。


<details>
  <summary>Details</summary>
Motivation: 手机定位数据用于研究个体移动模式，但分析流程复杂，需将原始位置数据转化为分析可用对象，当前研究有改进需求。

Method: 受AHDC研究的分箱算法启发，提出统计框架，含形式化概率模型和推理计算方法，用粒子吉布斯算法进行平滑处理。

Result: 所提框架相比原分箱算法，能改善轨迹估计。

Conclusion: 该框架可作为未来手机追踪研究的默认数据处理工具。

Abstract: As mobile phones become ubiquitous, high-frequency smartphone positioning
data are increasingly being used by researchers studying the mobility patterns
of individuals as they go about their daily routines and the consequences of
these patterns for health, behavioral, and other outcomes. A complex data
pipeline underlies empirical research leveraging mobile phone tracking data. A
key component of this pipeline is transforming raw, time-stamped positions into
analysis-ready data objects, typically space-time "trajectories." In this
paper, we break down a key portion of the data analysis pipeline underlying the
Adolescent Health and Development in Context (AHDC) Study, a large-scale,
longitudinal study of youth residing in the Columbus, OH metropolitan area.
Recognizing that the bespoke "binning algorithm" used by AHDC researchers
resembles a time-series filtering algorithm, we propose a statistical framework
- a formal probability model and computational approach to inference - inspired
by the binning algorithm for transforming noisy, time-stamped geographic
positioning observations into mobility trajectories that capture periods of
travel and stability. Our framework, unlike the binning algorithm, allows for
formal smoothing via a particle Gibbs algorithm, improving estimation of
trajectories as compared to the original binning algorithm. We argue that our
framework can be used as a default data processing tool for future mobile-phone
tracking studies.

</details>


### [176] [clustra: A multi-platform k-means clustering algorithm for analysis of longitudinal trajectories in large electronic health records data](https://arxiv.org/abs/2507.00962)
*Nimish Adhikari,Hanna Gerlovin,George Ostrouchov,Rachel Ehrbar,Alyssa B. Dufour,Brian R. Ferolito,Serkalem Demissie,Lauren Costa,Yuk-Lam Ho,Laura Tarko,Edmon Begoli,Kelly Cho,David R. Gagnon*

Main category: stat.CO

TL;DR: 提出跨平台的纵向轨迹聚类方法，对模拟血压数据应用效果好，两平台结果可比。


<details>
  <summary>Details</summary>
Motivation: 纵向数据难用单时间点总结，现有聚类轨迹估计存在处理时间间隔不一致和跨语言算法可用性问题。

Method: 提出使用带薄板回归样条的k - means算法进行纵向轨迹聚类，实现R包clustra和SAS宏，具备灵活聚类、可视化及诊断评估功能。

Result: 应用于模拟血压测量数据时，R包和SAS宏取得可比结果。

Conclusion: R包clustra和SAS宏集成K - means聚类算法处理大电子健康记录数据，两平台结果可比，满足不同平台研究者需求。

Abstract: Background and Objective: Variables collected over time, or longitudinally,
such as biologic measurements in electronic health records data, are not simple
to summarize with a single time-point, and thus can be more holistically
conceptualized as trajectories over time. Cluster analysis with longitudinal
data further allows for clinical representation of groups of subjects with
similar trajectories and identification of unique characteristics, or
phenotypes, that can be investigated as risk factors or disease outcomes. Some
of the challenges in estimating these clustered trajectories lie in the
handling of observations at inconsistent time intervals and the usability of
algorithms across programming languages.
  Methods: We propose longitudinal trajectory clustering using a k-means
algorithm with thin-plate regression splines, implemented across multiple
platforms, the R package clustra and corresponding \SAS macros. The \SAS macros
accommodate flexible clustering approaches, and also include visualization of
the clusters, and silhouette plots for diagnostic evaluation of the appropriate
cluster number. The R package, designed in parallel, has similar functionality,
with additional multi-core processing and Rand-index-based diagnostics.
  Results: The package and macros achieve comparable results when applied to an
example of simulated blood pressure measurements based on real data from
Veterans Affairs Healthcare recipients who were initiated on anti-hypertensive
medication.
  Conclusion: The R package clustra and the SAS macros integrate a K-means
clustering algorithm for longitudinal trajectories that operates with large
electronic health record data. The implementations provide comparable results
in both platforms, satisfying the needs of investigators familiar with, or
constrained by access to, one or the other platform.

</details>


<div id='cond-mat.dis-nn'></div>

# cond-mat.dis-nn [[Back]](#toc)

### [177] [Generalization performance of narrow one-hidden layer networks in the teacher-student setting](https://arxiv.org/abs/2507.00629)
*Jean Barbier,Federica Gerace,Alessandro Ingrosso,Clarissa Lauditi,Enrico M. Malatesta,Gibbs Nwemadji,Rodrigo Pérez Ortiz*

Main category: cond-mat.dis-nn

TL;DR: 本文为窄神经网络开发通用理论，用统计物理方法给出性能表达式，揭示神经元专业化转变，准确预测泛化误差。


<details>
  <summary>Details</summary>
Motivation: 理解神经网络泛化能力对实际数据集学习性能很重要，但缺乏全连接单隐藏层网络在通用激活函数下性能的完整理论。

Method: 使用统计物理方法，依据少量权重统计量给出有限温度（贝叶斯）和经验风险最小化估计器的典型性能的闭式表达式。

Result: 发现样本数量足够大且与网络参数数量成比例时，隐藏神经元会出现专业化转变。理论能准确预测用含噪全批量梯度下降或全批量梯度下降训练的神经网络的泛化误差。

Conclusion: 为窄神经网络开发的通用理论能有效分析其性能和泛化误差。

Abstract: Understanding the generalization abilities of neural networks for simple
input-output distributions is crucial to account for their learning performance
on real datasets. The classical teacher-student setting, where a network is
trained from data obtained thanks to a label-generating teacher model, serves
as a perfect theoretical test bed. In this context, a complete theoretical
account of the performance of fully connected one-hidden layer networks in the
presence of generic activation functions is lacking. In this work, we develop
such a general theory for narrow networks, i.e. networks with a large number of
hidden units, yet much smaller than the input dimension. Using methods from
statistical physics, we provide closed-form expressions for the typical
performance of both finite temperature (Bayesian) and empirical risk
minimization estimators, in terms of a small number of weight statistics. In
doing so, we highlight the presence of a transition where hidden neurons
specialize when the number of samples is sufficiently large and proportional to
the number of parameters of the network. Our theory accurately predicts the
generalization error of neural networks trained on regression or classification
tasks with either noisy full-batch gradient descent (Langevin dynamics) or
full-batch gradient descent.

</details>


<div id='math.DS'></div>

# math.DS [[Back]](#toc)

### [178] [SINDy on slow manifolds](https://arxiv.org/abs/2507.00747)
*Diemen Delgado-Cano,Erick Kracht,Urban Fasel,Benjamin Herrmann*

Main category: math.DS

TL;DR: 提出SINDy变体方法，分两步识别快慢动力学，减少条件数和库大小，实现慢流形动力学准确识别。


<details>
  <summary>Details</summary>
Motivation: 传统SINDy方法用于高维快慢动力系统时存在计算难和病态问题，直接建模慢流形会使库大小爆炸式增长。

Method: 分两步：先识别慢流形，即快变量关于慢变量的代数方程；再学习慢变量在流形上的动力学模型，利用前者构建定制函数库。

Result: 通过数值例子表明该方法显著降低SINDy库的条件数和大小。

Conclusion: 该方法能实现慢流形动力学的准确识别。

Abstract: The sparse identification of nonlinear dynamics (SINDy) has been established
as an effective method to learn interpretable models of dynamical systems from
data. However, for high-dimensional slow-fast dynamical systems, the regression
problem becomes simultaneously computationally intractable and ill-conditioned.
Although, in principle, modeling only the dynamics evolving on the underlying
slow manifold addresses both of these challenges, the truncated fast variables
have to be compensated by including higher-order nonlinearities as candidate
terms for the model, leading to an explosive growth in the size of the SINDy
library. In this work, we develop a SINDy variant that is able to robustly and
efficiently identify slow-fast dynamics in two steps: (i) identify the slow
manifold, that is, an algebraic equation for the fast variables as functions of
the slow ones, and (ii) learn a model for the dynamics of the slow variables
restricted to the manifold. Critically, the equation learned in (i) is
leveraged to build a manifold-informed function library for (ii) that contains
only essential higher-order nonlinearites as candidate terms. Rather than
containing all monomials of up to a certain degree, the resulting custom
library is a sparse subset of the latter that is tailored to the specific
problem at hand. The approach is demonstrated on numerical examples of a
snap-through buckling beam and the flow over a NACA 0012 airfoil. We find that
our method significantly reduces both the condition number and the size of the
SINDy library, thus enabling accurate identification of the dynamics on slow
manifolds.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [179] [MVGBench: Comprehensive Benchmark for Multi-view Generation Models](https://arxiv.org/abs/2507.00006)
*Xianghui Xie,Chuhang Zou,Meher Gitika Karumuri,Jan Eric Lenssen,Gerard Pons-Moll*

Main category: cs.GR

TL;DR: 提出MVGBench基准评估多视图图像生成模型，分析现有方法局限，提出ViFiGen方法表现更优并将公开代码等。


<details>
  <summary>Details</summary>
Motivation: 现有多视图图像生成模型评估指标不适用于生成任务，对不同因素鲁棒性和真实数据泛化性评估不充分，缺乏严格评估协议。

Method: 引入3D自一致性指标，在4种不同数据集上系统比较12种现有模型，分析重要设计选择。

Result: 发现现有方法在鲁棒性和泛化性方面存在重要局限，确定关键设计选择。

Conclusion: 提出的ViFiGen方法在3D一致性上优于所有评估模型。

Abstract: We propose MVGBench, a comprehensive benchmark for multi-view image
generation models (MVGs) that evaluates 3D consistency in geometry and texture,
image quality, and semantics (using vision language models). Recently, MVGs
have been the main driving force in 3D object creation. However, existing
metrics compare generated images against ground truth target views, which is
not suitable for generative tasks where multiple solutions exist while
differing from ground truth. Furthermore, different MVGs are trained on
different view angles, synthetic data and specific lightings -- robustness to
these factors and generalization to real data are rarely evaluated thoroughly.
Without a rigorous evaluation protocol, it is also unclear what design choices
contribute to the progress of MVGs. MVGBench evaluates three different aspects:
best setup performance, generalization to real data and robustness. Instead of
comparing against ground truth, we introduce a novel 3D self-consistency metric
which compares 3D reconstructions from disjoint generated multi-views. We
systematically compare 12 existing MVGs on 4 different curated real and
synthetic datasets. With our analysis, we identify important limitations of
existing methods specially in terms of robustness and generalization, and we
find the most critical design choices. Using the discovered best practices, we
propose ViFiGen, a method that outperforms all evaluated MVGs on 3D
consistency. Our code, model, and benchmark suite will be publicly released.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [180] [Rust vs. C for Python Libraries: Evaluating Rust-Compatible Bindings Toolchains](https://arxiv.org/abs/2507.00264)
*Isabella Basso do Amaral,Renato Cordeiro Ferreira,Alfredo Goldman*

Main category: cs.PL

TL;DR: 比较PyO3、ctypes和cffi在Python中的性能与易用性，用Rust工具为Python开发可实现高性能且无需担心API兼容性。


<details>
  <summary>Details</summary>
Motivation: Python解释器慢，优化关键部分需特殊知识且手动接口繁琐，开发者常依赖复杂第三方库，因此进行性能和易用性比较。

Method: 开展关于PyO3 Python绑定工具链与ctypes和cffi的比较研究。

Result: 使用为Python开发的Rust工具可实现最先进的性能。

Conclusion: 采用Rust工具为Python开发是一个较好的选择，能兼顾性能和避免API兼容性问题。

Abstract: The Python programming language is best known for its syntax and scientific
libraries, but it is also notorious for its slow interpreter. Optimizing
critical sections in Python entails special knowledge of the binary
interactions between programming languages, and can be cumbersome to interface
manually, with implementers often resorting to convoluted third-party
libraries. This comparative study evaluates the performance and ease of use of
the PyO3 Python bindings toolchain for Rust against ctypes and cffi. By using
Rust tooling developed for Python, we can achieve state-of-the-art performance
with no concern for API compatibility.

</details>


### [181] [Estimating Correctness Without Oracles in LLM-Based Code Generation](https://arxiv.org/abs/2507.00057)
*Thomas Valentin,Ardi Madadi,Gaetano Sapia,Marcel Böhme*

Main category: cs.PL

TL;DR: 提出无需正确实现的程序错误性度量方法，实验显示其有效性高，可替代基于oracle的评估。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在代码生成时会产生幻觉，在无正确实现情况下，需量化生成程序的正确性。

Method: 提出一种错误性度量方法——不一致性，在无oracle时能高效估计，并为错误率提供下限。

Result: 在平均代码生成任务中，基于不一致性的方法能自动识别约三分之二的错误程序且无误报，基于不一致性的评估可可靠替代基于oracle的评估。

Conclusion: 基于不一致性的评估与基于oracle的评估在对大语言模型的排序上有很强的一致性。

Abstract: Generating code from natural language specifications is one of the most
successful applications of Large Language Models (LLMs). Yet, they hallucinate:
LLMs produce outputs that may be grammatically correct but are factually
incorrect. Without an existing, correct implementation (i.e., an oracle), can
we quantify how likely the generated program is correct?
  In this paper, we propose a measure of incorrectness, called incoherence,
that can be estimated efficiently in the absence of an oracle and provides a
lower bound on the error, i.e., the probability that the LLM-generated program
for that specification is incorrect. Our experiments demonstrate an
extraordinary effectiveness. For the average code generation task, our
incoherence-based methodology can automatically identify about two-thirds of
incorrect programs without reports of false positives. In fact, an oracle-based
evaluation of LLMs can be reliably replaced by an incoherence-based evaluation.
In particular, we find a very strong agreement between the ranking of LLMs by
the number of programs deemed correct via an oracle (pass@1) and the ranking of
LLMs by the number of programs deemed correct via our incoherence.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [182] [Investigating Stochastic Methods for Prosody Modeling in Speech Synthesis](https://arxiv.org/abs/2507.00227)
*Paul Mayer,Florian Lux,Alejandro Pérez-González-de-Martos,Angelina Elizarova,Lindsey Vanderlyn,Dirk Väth,Ngoc Thang Vu*

Main category: eess.AS

TL;DR: 研究随机方法在TTS合成中生成韵律的有效性，评估表明其能生成媲美人类的自然韵律且增加可控性。


<details>
  <summary>Details</summary>
Motivation: 生成富有表现力的韵律在TTS合成中仍是挑战，特别是显式建模韵律的系统，因此研究随机方法的有效性。

Method: 研究Normalizing Flows、Conditional Flow Matching和Rectified Flows等随机方法，与传统确定性基线和真实人类表现对比。

Result: 广泛的主客观评估表明，随机方法能捕捉人类语音的内在变异性，生成与人类说话者相当的自然韵律。

Conclusion: 随机方法可生成自然韵律，且通过调整采样温度增加了额外的可控性选项。

Abstract: While generative methods have progressed rapidly in recent years, generating
expressive prosody for an utterance remains a challenging task in
text-to-speech synthesis. This is particularly true for systems that model
prosody explicitly through parameters such as pitch, energy, and duration,
which is commonly done for the sake of interpretability and controllability. In
this work, we investigate the effectiveness of stochastic methods for this
task, including Normalizing Flows, Conditional Flow Matching, and Rectified
Flows. We compare these methods to a traditional deterministic baseline, as
well as to real human realizations. Our extensive subjective and objective
evaluations demonstrate that stochastic methods produce natural prosody on par
with human speakers by capturing the variability inherent in human speech.
Further, they open up additional controllability options by allowing the
sampling temperature to be tuned.

</details>


### [183] [LearnAFE: Circuit-Algorithm Co-design Framework for Learnable Audio Analog Front-End](https://arxiv.org/abs/2507.00755)
*Jinhai Hu,Zhongyi Zhang,Cong Sheng Leow,Wang Ling Goh,Yuan Gao*

Main category: eess.AS

TL;DR: 提出音频信号分类中可学习模拟前端（AFE）的电路 - 算法协同设计框架，优化设计在10关键字分类任务中表现良好，且功耗和电容面积有降低。


<details>
  <summary>Details</summary>
Motivation: 指出分别设计AFE和后端分类器不理想，需联合优化实现系统级最优。

Method: 对后端分类器与AFE的传递函数进行联合优化，在分类器的信噪比感知训练循环中调整模拟带通滤波器组的传递函数参数，使用协同设计损失函数LBPF。

Result: 在开源SKY130 130nm CMOS工艺中实现，在输入信号信噪比5 dB至20 dB范围内，10关键字分类任务准确率达90.5% - 94.2%，仅22k分类器参数；相比传统方法，功耗降低8.7%，电容面积降低12.9%。

Conclusion: 所提出的音频AFE的协同设计框架有效，能提升分类性能并减少功耗和电容面积。

Abstract: This paper presents a circuit-algorithm co-design framework for learnable
analog front-end (AFE) in audio signal classification. Designing AFE and
backend classifiers separately is a common practice but non-ideal, as shown in
this paper. Instead, this paper proposes a joint optimization of the backend
classifier with the AFE's transfer function to achieve system-level optimum.
More specifically, the transfer function parameters of an analog bandpass
filter (BPF) bank are tuned in a signal-to-noise ratio (SNR)-aware training
loop for the classifier. Using a co-design loss function LBPF, this work shows
superior optimization of both the filter bank and the classifier. Implemented
in open-source SKY130 130nm CMOS process, the optimized design achieved
90.5%-94.2% accuracy for 10-keyword classification task across a wide range of
input signal SNR from 5 dB to 20 dB, with only 22k classifier parameters.
Compared to conventional approach, the proposed audio AFE achieves 8.7% and
12.9% reduction in power and capacitor area respectively.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [184] [A High-Fidelity Speech Super Resolution Network using a Complex Global Attention Module with Spectro-Temporal Loss](https://arxiv.org/abs/2507.00229)
*Tarikul Islam Tamiti,Biraj Joshi,Rida Hasan,Rashedul Hasan,Taieba Athay,Nursad Mamun,Anomadarshi Barua*

Main category: cs.SD

TL;DR: 本文提出CTFT - Net用于语音超分辨率任务，在复域重建幅度和相位，实验显示其性能优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 多数语音超分辨率方法聚焦幅度重建，而近期研究强调相位重建对提升感知质量的重要性，故需新方法。

Method: 引入CTFT - Net，包含复全局注意力块和复Conformer，采用时域和多分辨率频域损失函数。

Result: CTFT - Net在VCTK数据集上优于NU - Wave、WSRGlow等模型，极端上采样效果好，有效重建高频且无噪声伪影。

Conclusion: CTFT - Net在语音超分辨率任务中表现优秀，能有效重建幅度和相位，提高频率重建和抗噪能力。

Abstract: Speech super-resolution (SSR) enhances low-resolution speech by increasing
the sampling rate. While most SSR methods focus on magnitude reconstruction,
recent research highlights the importance of phase reconstruction for improved
perceptual quality. Therefore, we introduce CTFT-Net, a Complex Time-Frequency
Transformation Network that reconstructs both magnitude and phase in complex
domains for improved SSR tasks. It incorporates a complex global attention
block to model inter-phoneme and inter-frequency dependencies and a complex
conformer to capture long-range and local features, improving frequency
reconstruction and noise robustness. CTFT-Net employs time-domain and
multi-resolution frequency-domain loss functions for better generalization.
Experiments show CTFT-Net outperforms state-of-the-art models (NU-Wave,
WSRGlow, NVSR, AERO) on the VCTK dataset, particularly for extreme upsampling
(2 kHz to 48 kHz), reconstructing high frequencies effectively without noisy
artifacts.

</details>


### [185] [MuteSwap: Silent Face-based Voice Conversion](https://arxiv.org/abs/2507.00498)
*Yifan Liu,Yu Fang,Zhouhan Lin*

Main category: cs.SD

TL;DR: 本文聚焦无声面部语音转换（SFVC）任务，提出MuteSwap框架，实验显示其在语音合成和身份转换上表现出色，尤其在嘈杂环境。


<details>
  <summary>Details</summary>
Motivation: 传统语音转换依赖双方音频输入，在无干净音频（如无声视频、嘈杂环境）时不可行，因此研究仅基于视觉输入的SFVC任务。

Method: 引入MuteSwap框架，采用对比学习对齐跨模态身份，最小化互信息分离共享视觉特征。

Result: MuteSwap在语音合成和身份转换上取得令人印象深刻的表现，在嘈杂环境下依赖音频输入的方法无法产生可理解结果时仍有效。

Conclusion: 证明了训练方法的有效性和SFVC任务的可行性。

Abstract: Conventional voice conversion modifies voice characteristics from a source
speaker to a target speaker, relying on audio input from both sides. However,
this process becomes infeasible when clean audio is unavailable, such as in
silent videos or noisy environments. In this work, we focus on the task of
Silent Face-based Voice Conversion (SFVC), which does voice conversion entirely
from visual inputs. i.e., given images of a target speaker and a silent video
of a source speaker containing lip motion, SFVC generates speech aligning the
identity of the target speaker while preserving the speech content in the
source silent video. As this task requires generating intelligible speech and
converting identity using only visual cues, it is particularly challenging. To
address this, we introduce MuteSwap, a novel framework that employs contrastive
learning to align cross-modality identities and minimize mutual information to
separate shared visual features. Experimental results show that MuteSwap
achieves impressive performance in both speech synthesis and identity
conversion, especially under noisy conditions where methods dependent on audio
input fail to produce intelligible results, demonstrating both the
effectiveness of our training approach and the feasibility of SFVC.

</details>


### [186] [MambAttention: Mamba with Multi-Head Attention for Generalizable Single-Channel Speech Enhancement](https://arxiv.org/abs/2507.00966)
*Nikolai Lund Kühne,Jesper Jensen,Jan Østergaard,Zheng-Hua Tan*

Main category: cs.SD

TL;DR: 本文提出混合架构MambAttention用于单通道语音增强，引入新数据集VB - DemandEx训练，模型在多个数据集表现优异，消融实验分析模块作用，探索与LSTM和xLSTM结合仍以MambAttention最优。


<details>
  <summary>Details</summary>
Motivation: 现有序列模型如LSTM和Mamba在语音增强中存在过拟合问题，且混合Mamba和时频注意力模型及其泛化性能未被探索。

Method: 提出MambAttention混合架构，结合Mamba和共享时频多头注意力模块；引入新数据集VB - DemandEx进行训练。

Result: MambAttention在两个域外数据集DNS 2020和EARS - WHAM_v2上显著优于复杂度相似的现有系统，在域内数据集VB - DemandEx上表现相当；消融实验凸显时频多头注意力模块权重共享对泛化性能的作用；与LSTM和xLSTM结合有性能提升但MambAttention仍最优。

Conclusion: MambAttention架构能有效实现可泛化的单通道语音增强，时频多头注意力模块权重共享有助于提升泛化性能。

Abstract: With the advent of new sequence models like Mamba and xLSTM, several studies
have shown that these models match or outperform state-of-the-art models in
single-channel speech enhancement, automatic speech recognition, and
self-supervised audio representation learning. However, prior research has
demonstrated that sequence models like LSTM and Mamba tend to overfit to the
training set. To address this issue, previous works have shown that adding
self-attention to LSTMs substantially improves generalization performance for
single-channel speech enhancement. Nevertheless, neither the concept of hybrid
Mamba and time-frequency attention models nor their generalization performance
have been explored for speech enhancement. In this paper, we propose a novel
hybrid architecture, MambAttention, which combines Mamba and shared time- and
frequency-multi-head attention modules for generalizable single-channel speech
enhancement. To train our model, we introduce VoiceBank+Demand Extended
(VB-DemandEx), a dataset inspired by VoiceBank+Demand but with more challenging
noise types and lower signal-to-noise ratios. Trained on VB-DemandEx, our
proposed MambAttention model significantly outperforms existing
state-of-the-art LSTM-, xLSTM-, Mamba-, and Conformer-based systems of similar
complexity across all reported metrics on two out-of-domain datasets: DNS 2020
and EARS-WHAM_v2, while matching their performance on the in-domain dataset
VB-DemandEx. Ablation studies highlight the role of weight sharing between the
time- and frequency-multi-head attention modules for generalization
performance. Finally, we explore integrating the shared time- and
frequency-multi-head attention modules with LSTM and xLSTM, which yields a
notable performance improvement on the out-of-domain datasets. However, our
MambAttention model remains superior on both out-of-domain datasets across all
reported evaluation metrics.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [187] [Evolutionary computing-based image segmentation method to detect defects and features in Additive Friction Stir Deposition Process](https://arxiv.org/abs/2507.00046)
*Akshansh Mishra,Eyob Mesele Sefene,Shivraman Thapliyal*

Main category: cs.CV

TL;DR: 提出基于进化计算的图像分割方法分析AFSD过程完整性，用PSO确定阈值，结合多种可视化技术识别缺陷和不均匀性。


<details>
  <summary>Details</summary>
Motivation: 分析AFSD过程的完整性，对增材制造部件进行过程优化和质量评估。

Method: 采用粒子群优化（PSO）确定多层AFSD构建中检测缺陷和特征的最佳分割阈值，将梯度幅度分析与距离变换相结合创建注意力加权可视化，使用多种可视化技术分析样本。

Result: PSO算法为每个样本自动确定156 - 173的最佳阈值，多通道可视化技术有效量化界面质量，基于注意力的分析成功识别AFSD接头中的不完全结合和不均匀区域。

Conclusion: 基于注意力的分析可提供定量指标用于增材制造部件的过程优化和质量评估。

Abstract: This work proposes an evolutionary computing-based image segmentation
approach for analyzing soundness in Additive Friction Stir Deposition (AFSD)
processes. Particle Swarm Optimization (PSO) was employed to determine optimal
segmentation thresholds for detecting defects and features in multilayer AFSD
builds. The methodology integrates gradient magnitude analysis with distance
transforms to create novel attention-weighted visualizations that highlight
critical interface regions. Five AFSD samples processed under different
conditions were analyzed using multiple visualization techniques i.e.
self-attention maps, and multi-channel visualization. These complementary
approaches reveal subtle material transition zones and potential defect regions
which were not readily observable through conventional imaging. The PSO
algorithm automatically identified optimal threshold values (ranging from
156-173) for each sample, enabling precise segmentation of material interfaces.
The multi-channel visualization technique effectively combines boundary
information (red channel), spatial relationships (green channel), and material
density data (blue channel) into cohesive representations that quantify
interface quality. The results demonstrate that attention-based analysis
successfully identifies regions of incomplete bonding and inhomogeneities in
AFSD joints, providing quantitative metrics for process optimization and
quality assessment of additively manufactured components.

</details>


### [188] [Room Scene Discovery and Grouping in Unstructured Vacation Rental Image Collections](https://arxiv.org/abs/2507.00263)
*Vignesh Ram Nithin Kappagantula,Shayan Hassantabar*

Main category: cs.CV

TL;DR: 为解决度假租赁平台房产图片无结构分类问题，提出机器学习管道，性能超现有方法。


<details>
  <summary>Details</summary>
Motivation: 度假租赁平台房产图片缺乏结构化分类，给旅客了解房产空间布局带来挑战。

Method: 提出低延迟、样本高效学习的机器学习管道，集成监督式房间类型检测模型、重叠检测模型和聚类算法，用多模态大语言模型映射床型。

Result: 各模型及整体管道表现良好，显著优于对比学习和预训练嵌入聚类等现有方法。

Conclusion: 所提方法能有效解决房间场景发现、分组及床型识别问题，适合实时和数据稀缺环境。

Abstract: The rapid growth of vacation rental (VR) platforms has led to an increasing
volume of property images, often uploaded without structured categorization.
This lack of organization poses significant challenges for travelers attempting
to understand the spatial layout of a property, particularly when multiple
rooms of the same type are present. To address this issue, we introduce an
effective approach for solving the room scene discovery and grouping problem,
as well as identifying bed types within each bedroom group. This grouping is
valuable for travelers to comprehend the spatial organization, layout, and the
sleeping configuration of the property. We propose a computationally efficient
machine learning pipeline characterized by low latency and the ability to
perform effectively with sample-efficient learning, making it well-suited for
real-time and data-scarce environments. The pipeline integrates a supervised
room-type detection model, a supervised overlap detection model to identify the
overlap similarity between two images, and a clustering algorithm to group the
images of the same space together using the similarity scores. Additionally,
the pipeline maps each bedroom group to the corresponding bed types specified
in the property's metadata, based on the visual content present in the group's
images using a Multi-modal Large Language Model (MLLM) model. We evaluate the
aforementioned models individually and also assess the pipeline in its
entirety, observing strong performance that significantly outperforms
established approaches such as contrastive learning and clustering with
pretrained embeddings.

</details>


### [189] [Moment Sampling in Video LLMs for Long-Form Video QA](https://arxiv.org/abs/2507.00033)
*Mustafa Chasmai,Gauri Jagatap,Gouthaman KV,Grant Van Horn,Subhransu Maji,Andrea Fanelli*

Main category: cs.CV

TL;DR: 现有视频问答方法处理长视频时存在不足，本文提出“时刻采样”方法，通过文本到视频时刻检索模型引导帧采样，在四个数据集上实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 现有视频问答方法在长视频长距离推理方面表现不佳，常用的帧子采样方法存在丢失关键帧和包含冗余信息的问题。

Method: 使用通用文本到视频时刻检索模型引导帧采样过程，提出“时刻采样”方法，采用轻量级时刻检索模型对帧选择进行优先级排序。

Result: 在四个长视频问答数据集上，使用四个最先进的视频大语言模型进行了广泛实验，证明了该方法的有效性。

Conclusion: 所提出的方法能让模型根据问题上下文选择最相关的帧，提高视频大语言模型在长视频问答中的性能。

Abstract: Recent advancements in video large language models (Video LLMs) have
significantly advanced the field of video question answering (VideoQA). While
existing methods perform well on short videos, they often struggle with
long-range reasoning in longer videos. To scale Video LLMs for longer video
content, frame sub-sampling (selecting frames at regular intervals) is commonly
used. However, this approach is suboptimal, often leading to the loss of
crucial frames or the inclusion of redundant information from multiple similar
frames. Missing key frames impairs the model's ability to answer questions
accurately, while redundant frames lead the model to focus on irrelevant video
segments and increase computational resource consumption. In this paper, we
investigate the use of a general-purpose text-to-video moment retrieval model
to guide the frame sampling process. We propose "moment sampling", a novel,
model-agnostic approach that enables the model to select the most relevant
frames according to the context of the question. Specifically, we employ a
lightweight moment retrieval model to prioritize frame selection. By focusing
on the frames most pertinent to the given question, our method enhances
long-form VideoQA performance in Video LLMs. Through extensive experiments on
four long-form VideoQA datasets, using four state-of-the-art Video LLMs, we
demonstrate the effectiveness of the proposed approach.

</details>


### [190] [Catastrophic Forgetting Mitigation via Discrepancy-Weighted Experience Replay](https://arxiv.org/abs/2507.00042)
*Xinrun Xu,Jianwen Yang,Qiuhong Zhang,Zhanbiao Lian,Zhiming Ding,Shan Jiang*

Main category: cs.CV

TL;DR: 本文提出 ER - EMU 算法解决云边协同目标检测中边缘模型更新的灾难性遗忘问题，实验证明该算法能提升检测框架性能。


<details>
  <summary>Details</summary>
Motivation: 云边协同交通监控目标检测中边缘模型更新存在灾难性遗忘问题，现有方法难以有效利用历史数据。

Method: 提出基于自适应经验回放的 ER - EMU 算法，采用 FIFO 原则管理经验缓冲区，用 DDM - ES 算法选样，并用随机采样更新缓冲区。

Result: 在 Bellevue 交通视频数据集实验中，ER - EMU 持续提升多个先进云边协同目标检测框架的性能。

Conclusion: ER - EMU 算法有效解决云边协同目标检测中边缘模型更新的问题，可提升检测性能。

Abstract: Continually adapting edge models in cloud-edge collaborative object detection
for traffic monitoring suffers from catastrophic forgetting, where models lose
previously learned knowledge when adapting to new data distributions. This is
especially problematic in dynamic traffic environments characterised by
periodic variations (e.g., day/night, peak hours), where past knowledge remains
valuable. Existing approaches like experience replay and visual prompts offer
some mitigation, but struggle to effectively prioritize and leverage historical
data for optimal knowledge retention and adaptation. Specifically, simply
storing and replaying all historical data can be inefficient, while treating
all historical experiences as equally important overlooks their varying
relevance to the current domain. This paper proposes ER-EMU, an edge model
update algorithm based on adaptive experience replay, to address these
limitations. ER-EMU utilizes a limited-size experience buffer managed using a
First-In-First-Out (FIFO) principle, and a novel Domain Distance Metric-based
Experience Selection (DDM-ES) algorithm. DDM-ES employs the multi-kernel
maximum mean discrepancy (MK-MMD) to quantify the dissimilarity between target
domains, prioritizing the selection of historical data that is most dissimilar
to the current target domain. This ensures training diversity and facilitates
the retention of knowledge from a wider range of past experiences, while also
preventing overfitting to the new domain. The experience buffer is also updated
using a simple random sampling strategy to maintain a balanced representation
of previous domains. Experiments on the Bellevue traffic video dataset,
involving repeated day/night cycles, demonstrate that ER-EMU consistently
improves the performance of several state-of-the-art cloud-edge collaborative
object detection frameworks.

</details>


### [191] [MR-CLIP: Efficient Metadata-Guided Learning of MRI Contrast Representations](https://arxiv.org/abs/2507.00043)
*Mehmet Yigit Avci,Pedro Borges,Paul Wright,Mehmet Yigitsoy,Sebastien Ourselin,Jorge Cardoso*

Main category: cs.CV

TL;DR: 文章提出MR - CLIP多模态对比学习框架，用于解决磁共振成像（MRI）扫描图像对比度解读难题，该框架可学习到对比度感知表征，在跨模态检索和对比度分类中有效，代码公开。


<details>
  <summary>Details</summary>
Motivation: 临床系统中MRI扫描图像准确解读依赖图像对比度理解，但现有标签不精确且很多数据集缺少标签，可用元数据不完整、有噪声或不一致，需可靠标准化元数据及对比度感知表征以支持临床应用。

Method: 提出MR - CLIP多模态对比学习框架，将MR图像与其DICOM元数据对齐，在包含多种扫描仪和协议的临床数据集上训练。

Result: MR - CLIP能捕捉采集过程和扫描内部的对比度变化，实现解剖学不变表征，在跨模态检索和对比度分类中展现出有效性。

Conclusion: MR - CLIP具有可扩展性，有进一步临床应用潜力，代码和权重已公开。

Abstract: Accurate interpretation of Magnetic Resonance Imaging scans in clinical
systems is based on a precise understanding of image contrast. This contrast is
primarily governed by acquisition parameters, such as echo time and repetition
time, which are stored in the DICOM metadata. To simplify contrast
identification, broad labels such as T1-weighted or T2-weighted are commonly
used, but these offer only a coarse approximation of the underlying acquisition
settings. In many real-world datasets, such labels are entirely missing,
leaving raw acquisition parameters as the only indicators of contrast. Adding
to this challenge, the available metadata is often incomplete, noisy, or
inconsistent. The lack of reliable and standardized metadata complicates tasks
such as image interpretation, retrieval, and integration into clinical
workflows. Furthermore, robust contrast-aware representations are essential to
enable more advanced clinical applications, such as achieving
modality-invariant representations and data harmonization. To address these
challenges, we propose MR-CLIP, a multimodal contrastive learning framework
that aligns MR images with their DICOM metadata to learn contrast-aware
representations, without relying on manual labels. Trained on a diverse
clinical dataset that spans various scanners and protocols, MR-CLIP captures
contrast variations across acquisitions and within scans, enabling
anatomy-invariant representations. We demonstrate its effectiveness in
cross-modal retrieval and contrast classification, highlighting its scalability
and potential for further clinical applications. The code and weights are
publicly available at https://github.com/myigitavci/MR-CLIP.

</details>


### [192] [HistoART: Histopathology Artifact Detection and Reporting Tool](https://arxiv.org/abs/2507.00044)
*Seyed Kahaki,Alexander R. Webber,Ghada Zamzmi,Adarsh Subbaswamy,Rucha Deshpande,Aldo Badano*

Main category: cs.CV

TL;DR: 文章提出三种全切片图像（WSI）伪影检测方法，评估后发现基于基础模型的方法表现最佳，并开发质量报告计分卡。


<details>
  <summary>Details</summary>
Motivation: WSI在癌症诊断中易受切片制备和扫描过程产生的伪影影响，影响下游图像分析，需解决伪影检测问题。

Method: 提出三种伪影检测方法，包括基于基础模型的方法（FMA）、基于ResNet50骨干网络的深度学习方法（DLA）和基于知识的方法（KBA），目标检测六种常见伪影类型。

Result: 在50000多个图像块上评估，FMA的逐块AUROC最高达0.995，优于DLA和KBA。

Conclusion: 开发的质量报告计分卡可将检测结果转化为可操作的见解，量化高质量图像块并可视化伪影分布。

Abstract: In modern cancer diagnostics, Whole Slide Imaging (WSI) is widely used to
digitize tissue specimens for detailed, high-resolution examination; however,
other diagnostic approaches, such as liquid biopsy and molecular testing, are
also utilized based on the cancer type and clinical context. While WSI has
revolutionized digital histopathology by enabling automated, precise analysis,
it remains vulnerable to artifacts introduced during slide preparation and
scanning. These artifacts can compromise downstream image analysis. To address
this challenge, we propose and compare three robust artifact detection
approaches for WSIs: (1) a foundation model-based approach (FMA) using a
fine-tuned Unified Neural Image (UNI) architecture, (2) a deep learning
approach (DLA) built on a ResNet50 backbone, and (3) a knowledge-based approach
(KBA) leveraging handcrafted features from texture, color, and frequency-based
metrics. The methods target six common artifact types: tissue folds,
out-of-focus regions, air bubbles, tissue damage, marker traces, and blood
contamination. Evaluations were conducted on 50,000+ image patches from diverse
scanners (Hamamatsu, Philips, Leica Aperio AT2) across multiple sites. The FMA
achieved the highest patch-wise AUROC of 0.995 (95% CI [0.994, 0.995]),
outperforming the ResNet50-based method (AUROC: 0.977, 95% CI [0.977, 0.978])
and the KBA (AUROC: 0.940, 95% CI [0.933, 0.946]). To translate detection into
actionable insights, we developed a quality report scorecard that quantifies
high-quality patches and visualizes artifact distributions.

</details>


### [193] [CaughtCheating: Is Your MLLM a Good Cheating Detective? Exploring the Boundary of Visual Perception and Reasoning](https://arxiv.org/abs/2507.00045)
*Ming Li,Chenguang Wang,Yijun Liang,Xiyao Wang,Yuhang Zhou,Xiyang Wu,Yuqing Zhang,Ruiyi Zhang,Tianyi Zhou*

Main category: cs.CV

TL;DR: 现有多模态大语言模型在部分基准测试接近满分，研究其能否媲美优秀人类侦探，发现CaughtCheating场景下模型表现近乎为零。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型在各类基准测试表现优异，需更具挑战性测试任务，探究其能否媲美优秀人类侦探。

Method: 研究GPT - o3能处理的困难场景，发现CaughtCheating场景并进行大量实验分析。

Result: 发现CaughtCheating场景下o3性能近乎为零，现有MLLMs缺乏解决此类任务能力。

Conclusion: CaughtCheating提供了有价值和实用的视觉感知与推理任务，成功完成这些任务有助于MLLMs获得人类水平侦探感知和推理能力。

Abstract: Recent agentic Multi-Modal Large Language Models (MLLMs) such as GPT-o3 have
achieved near-ceiling scores on various existing benchmarks, motivating a
demand for more challenging test tasks. These MLLMs have been reported to excel
in a few expert-level tasks for humans, e.g., GeoGuesser, reflecting their
potential as a detective who can notice minuscule cues in an image and weave
them into coherent, situational explanations, leading to a reliable answer. But
can they match the performance of excellent human detectives? To answer this
question, we investigate some hard scenarios where GPT-o3 can still handle, and
find a common scenario where o3's performance drops to nearly zero, which we
name CaughtCheating. It is inspired by the social media requests that ask
others to detect suspicious clues from photos shared by the poster's partner.
We conduct extensive experiments and analysis to understand why existing MLLMs
lack sufficient capability to solve this kind of task. CaughtCheating provides
a class of challenging visual perception and reasoning tasks with great value
and practical usage. Success in these tasks paves the way for MLLMs to acquire
human-level detective perception and reasoning capabilities.

</details>


### [194] [VSF-Med:A Vulnerability Scoring Framework for Medical Vision-Language Models](https://arxiv.org/abs/2507.00052)
*Binesh Sadanandan,Vahid Behzadan*

Main category: cs.CV

TL;DR: 提出用于医学视觉语言模型的VSF - Med框架，可合成大量对抗样本并进行可复现基准测试，分析了多种模型在不同攻击向量下的z分数变化。


<details>
  <summary>Details</summary>
Motivation: 临床环境中对医学视觉语言模型的系统安全评估稀缺，需要相关评估框架。

Method: 引入VSF - Med框架，包含文本提示攻击模板库、基于SSIM阈值的视觉扰动、八维评分标准，通过z分数归一化得出综合风险指标。

Result: 分析得出不同攻击向量下各模型的平均z分数变化，如持久性攻击效果、提示注入有效性、安全绕过成功率等。

Conclusion: VSF - Med框架可有效评估医学视觉语言模型的安全性，不同模型在不同攻击向量下有不同程度的脆弱性增加。

Abstract: Vision Language Models (VLMs) hold great promise for streamlining
labour-intensive medical imaging workflows, yet systematic security evaluations
in clinical settings remain scarce. We introduce VSF--Med, an end-to-end
vulnerability-scoring framework for medical VLMs that unites three novel
components: (i) a rich library of sophisticated text-prompt attack templates
targeting emerging threat vectors; (ii) imperceptible visual perturbations
calibrated by structural similarity (SSIM) thresholds to preserve clinical
realism; and (iii) an eight-dimensional rubric evaluated by two independent
judge LLMs, whose raw scores are consolidated via z-score normalization to
yield a 0--32 composite risk metric. Built entirely on publicly available
datasets and accompanied by open-source code, VSF--Med synthesizes over 30,000
adversarial variants from 5,000 radiology images and enables reproducible
benchmarking of any medical VLM with a single command. Our consolidated
analysis reports mean z-score shifts of $0.90\sigma$ for
persistence-of-attack-effects, $0.74\sigma$ for prompt-injection effectiveness,
and $0.63\sigma$ for safety-bypass success across state-of-the-art VLMs.
Notably, Llama-3.2-11B-Vision-Instruct exhibits a peak vulnerability increase
of $1.29\sigma$ for persistence-of-attack-effects, while GPT-4o shows increases
of $0.69\sigma$ for that same vector and $0.28\sigma$ for prompt-injection
attacks.

</details>


### [195] [MANTA: Cross-Modal Semantic Alignment and Information-Theoretic Optimization for Long-form Multimodal Understanding](https://arxiv.org/abs/2507.00068)
*Ziqi Zhong,Daniel Tang*

Main category: cs.CV

TL;DR: 提出MANTA框架统一视觉和听觉输入到结构化文本空间，解决多模态学习问题，实验效果显著。


<details>
  <summary>Details</summary>
Motivation: 当前多模态学习方法常分开处理模态，导致表征和推理不一致。

Method: 引入MANTA框架，通过信息论优化实现跨模态语义对齐、自适应时间同步、分层内容表征和上下文感知检索，在严格数学框架内形式化该方法。

Result: 在长视频问答任务中整体准确率最高提升22.6%，长视频、时间推理和跨模态理解任务也有显著提升，还引入新的密度估计技术。

Conclusion: MANTA框架为通过结构化文本统一多模态表征奠定了新基础。

Abstract: While multi-modal learning has advanced significantly, current approaches
often treat modalities separately, creating inconsistencies in representation
and reasoning. We introduce MANTA (Multi-modal Abstraction and Normalization
via Textual Alignment), a theoretically-grounded framework that unifies visual
and auditory inputs into a structured textual space for seamless processing
with large language models. MANTA addresses four key challenges: (1) semantic
alignment across modalities with information-theoretic optimization, (2)
adaptive temporal synchronization for varying information densities, (3)
hierarchical content representation for multi-scale understanding, and (4)
context-aware retrieval of sparse information from long sequences. We formalize
our approach within a rigorous mathematical framework, proving its optimality
for context selection under token constraints. Extensive experiments on the
challenging task of Long Video Question Answering show that MANTA improves
state-of-the-art models by up to 22.6% in overall accuracy, with particularly
significant gains (27.3%) on videos exceeding 30 minutes. Additionally, we
demonstrate MANTA's superiority on temporal reasoning tasks (23.8% improvement)
and cross-modal understanding (25.1% improvement). Our framework introduces
novel density estimation techniques for redundancy minimization while
preserving rare signals, establishing new foundations for unifying multimodal
representations through structured text.

</details>


### [196] [An efficient plant disease detection using transfer learning approach](https://arxiv.org/abs/2507.00070)
*Bosubabu Sambana,Hillary Sunday Nnadi,Mohd Anas Wajid,Nwosu Ogochukwu Fidelia,Claudia Camacho-Zuñiga,Henry Dozie Ajuzie,Edeh Michael Onyema*

Main category: cs.CV

TL;DR: 研究提出用迁移学习方法，利用YOLOv7和YOLOv8模型识别和监测植物病害，评估显示YOLOv8效果好，为植物病害早期检测提供自动解决方案。


<details>
  <summary>Details</summary>
Motivation: 植物病害对农业挑战大，早期检测能减轻影响，技术进步提供自动化监测和检测机会。

Method: 采用迁移学习方法，利用YOLOv7和YOLOv8模型，在植物叶片图像数据集上微调模型。

Result: 用多种指标评估模型性能，YOLOv8的mAP、F1 - score、Precision和Recall分别为91.05、89.40、91.22和87.66 ，表明其比其他目标检测方法更有效。

Conclusion: 该方法为植物病害早期检测提供可扩展自动解决方案，有助于提高作物产量、减少人工监测依赖、支持可持续农业。

Abstract: Plant diseases pose significant challenges to farmers and the agricultural
sector at large. However, early detection of plant diseases is crucial to
mitigating their effects and preventing widespread damage, as outbreaks can
severely impact the productivity and quality of crops. With advancements in
technology, there are increasing opportunities for automating the monitoring
and detection of disease outbreaks in plants. This study proposed a system
designed to identify and monitor plant diseases using a transfer learning
approach. Specifically, the study utilizes YOLOv7 and YOLOv8, two
state-ofthe-art models in the field of object detection. By fine-tuning these
models on a dataset of plant leaf images, the system is able to accurately
detect the presence of Bacteria, Fungi and Viral diseases such as Powdery
Mildew, Angular Leaf Spot, Early blight and Tomato mosaic virus. The model's
performance was evaluated using several metrics, including mean Average
Precision (mAP), F1-score, Precision, and Recall, yielding values of 91.05,
89.40, 91.22, and 87.66, respectively. The result demonstrates the superior
effectiveness and efficiency of YOLOv8 compared to other object detection
methods, highlighting its potential for use in modern agricultural practices.
The approach provides a scalable, automated solution for early any plant
disease detection, contributing to enhanced crop yield, reduced reliance on
manual monitoring, and supporting sustainable agricultural practices.

</details>


### [197] [Developing Lightweight DNN Models With Limited Data For Real-Time Sign Language Recognition](https://arxiv.org/abs/2507.00248)
*Nikita Nikitin,Eugene Fomin*

Main category: cs.CV

TL;DR: 提出用轻量级DNN在有限数据上训练的实时手语识别框架，解决数据稀缺等问题，实现高精度低延迟识别，模型集成到应用展示稳定推理。


<details>
  <summary>Details</summary>
Motivation: 解决手语识别中数据稀缺、计算成本高、训练和推理环境帧率差异等关键挑战。

Method: 将手语特定参数编码为矢量化输入，利用MediaPipe进行地标提取，优化DNN架构以实现小于10MB部署，使用'slait data'平台进行数据标注和向量提取。

Result: 在边缘设备上以小于10ms的延迟准确分类343个手势，孤立手势识别准确率达92%，并集成到'slait ai'网络应用中展示稳定推理。

Conclusion: 所提出的框架有效解决手语识别挑战，实现高精度低延迟识别，具备实际应用价值。

Abstract: We present a novel framework for real-time sign language recognition using
lightweight DNNs trained on limited data. Our system addresses key challenges
in sign language recognition, including data scarcity, high computational
costs, and discrepancies in frame rates between training and inference
environments. By encoding sign language specific parameters, such as handshape,
palm orientation, movement, and location into vectorized inputs, and leveraging
MediaPipe for landmark extraction, we achieve highly separable input data
representations. Our DNN architecture, optimized for sub 10MB deployment,
enables accurate classification of 343 signs with less than 10ms latency on
edge devices. The data annotation platform 'slait data' facilitates structured
labeling and vector extraction. Our model achieved 92% accuracy in isolated
sign recognition and has been integrated into the 'slait ai' web application,
where it demonstrates stable inference.

</details>


### [198] [Self-Supervised Multiview Xray Matching](https://arxiv.org/abs/2507.00287)
*Mohamad Dabboussi,Malo Huard,Yann Gousseau,Pietro Gori*

Main category: cs.CV

TL;DR: 提出新的自监督管道，自动生成合成X光视图对应矩阵，提升多视图骨折分类性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于AI的单图像分析方法难以在不同X光视图间建立稳健对应关系，而准确对应对临床评估至关重要。

Method: 使用数字重建X光片（DRR）从无注释CT体积自动生成合成X光视图的多对多对应矩阵，采用基于Transformer的训练阶段预测视图对应关系，将合成视图对应学习作为预训练策略用于真实数据。

Result: 在合成和真实X光数据集上的广泛评估表明，结合对应关系能提高多视图骨折分类性能。

Conclusion: 学习合成X光视图间的对应关系可作为预训练策略，提升真实数据上的多视图骨折自动检测能力。

Abstract: Accurate interpretation of multi-view radiographs is crucial for diagnosing
fractures, muscular injuries, and other anomalies. While significant advances
have been made in AI-based analysis of single images, current methods often
struggle to establish robust correspondences between different X-ray views, an
essential capability for precise clinical evaluations. In this work, we present
a novel self-supervised pipeline that eliminates the need for manual annotation
by automatically generating a many-to-many correspondence matrix between
synthetic X-ray views. This is achieved using digitally reconstructed
radiographs (DRR), which are automatically derived from unannotated CT volumes.
Our approach incorporates a transformer-based training phase to accurately
predict correspondences across two or more X-ray views. Furthermore, we
demonstrate that learning correspondences among synthetic X-ray views can be
leveraged as a pretraining strategy to enhance automatic multi-view fracture
detection on real data. Extensive evaluations on both synthetic and real X-ray
datasets show that incorporating correspondences improves performance in
multi-view fracture classification.

</details>


### [199] [Reducing Variability of Multiple Instance Learning Methods for Digital Pathology](https://arxiv.org/abs/2507.00292)
*Ali Mammadov,Loïc Le Folgoc,Guillaume Hocquet,Pietro Gori*

Main category: cs.CV

TL;DR: 数字病理将组织样本数字化为全切片图像（WSIs），但WSIs给深度学习模型带来挑战，多实例学习（MIL）适用于WSI分类却存在性能不稳定问题，本文提出多保真度模型融合策略解决该问题并验证。


<details>
  <summary>Details</summary>
Motivation: MIL方法在不同运行中性能差异大，难以可靠比较不同MIL方法，主要受权重初始化、批次排序和学习率三个因素影响。

Method: 先训练多个模型几个周期，根据验证分数平均最稳定和有前景的模型，该方法可应用于任何现有MIL模型。

Result: 在WSI分类任务上用2种数据集、3种初始化策略和5种MIL方法进行超2000次实验。

Conclusion: 该方法能减少性能变异性，简化超参数调整，提高可重复性并保持计算效率。

Abstract: Digital pathology has revolutionized the field by enabling the digitization
of tissue samples into whole slide images (WSIs). However, the high resolution
and large size of WSIs present significant challenges when it comes to applying
Deep Learning models. As a solution, WSIs are often divided into smaller
patches with a global label (\textit{i.e., diagnostic}) per slide, instead of a
(too) costly pixel-wise annotation. By treating each slide as a bag of patches,
Multiple Instance Learning (MIL) methods have emerged as a suitable solution
for WSI classification. A major drawback of MIL methods is their high
variability in performance across different runs, which can reach up to 10-15
AUC points on the test set, making it difficult to compare different MIL
methods reliably. This variability mainly comes from three factors: i) weight
initialization, ii) batch (shuffling) ordering, iii) and learning rate. To
address that, we introduce a Multi-Fidelity, Model Fusion strategy for MIL
methods. We first train multiple models for a few epochs and average the most
stable and promising ones based on validation scores. This approach can be
applied to any existing MIL model to reduce performance variability. It also
simplifies hyperparameter tuning and improves reproducibility while maintaining
computational efficiency. We extensively validate our approach on WSI
classification tasks using 2 different datasets, 3 initialization strategies
and 5 MIL methods, for a total of more than 2000 experiments.

</details>


### [200] [Training for X-Ray Vision: Amodal Segmentation, Amodal Content Completion, and View-Invariant Object Representation from Multi-Camera Video](https://arxiv.org/abs/2507.00339)
*Alexander Moore,Amar Saini,Kylie Cancilla,Doug Poland,Carmen Carrano*

Main category: cs.CV

TL;DR: 本文介绍最大的无模态分割和首个无模态内容数据集MOVi - MC - AC，有新贡献并提供大量对象实例标签。


<details>
  <summary>Details</summary>
Motivation: 现有数据缺乏多相机视角场景的对象上下文维度，需新数据集助力计算机视觉深度学习。

Method: 模拟多相机视频中通用家居对象的杂乱场景，为检测和分割提供一致对象ID，提供无模态内容标签。

Result: 发布MOVi - MC - AC数据集，有~580万个对象实例标签，是首个提供真实无模态内容的数据集。

Conclusion: MOVi - MC - AC数据集为对象检测、跟踪和分割等领域做出贡献，推动计算机视觉深度学习发展。

Abstract: Amodal segmentation and amodal content completion require using object priors
to estimate occluded masks and features of objects in complex scenes. Until
now, no data has provided an additional dimension for object context: the
possibility of multiple cameras sharing a view of a scene. We introduce
MOVi-MC-AC: Multiple Object Video with Multi-Cameras and Amodal Content, the
largest amodal segmentation and first amodal content dataset to date. Cluttered
scenes of generic household objects are simulated in multi-camera video.
MOVi-MC-AC contributes to the growing literature of object detection, tracking,
and segmentation by including two new contributions to the deep learning for
computer vision world. Multiple Camera (MC) settings where objects can be
identified and tracked between various unique camera perspectives are rare in
both synthetic and real-world video. We introduce a new complexity to synthetic
video by providing consistent object ids for detections and segmentations
between both frames and multiple cameras each with unique features and motion
patterns on a single scene. Amodal Content (AC) is a reconstructive task in
which models predict the appearance of target objects through occlusions. In
the amodal segmentation literature, some datasets have been released with
amodal detection, tracking, and segmentation labels. While other methods rely
on slow cut-and-paste schemes to generate amodal content pseudo-labels, they do
not account for natural occlusions present in the modal masks. MOVi-MC-AC
provides labels for ~5.8 million object instances, setting a new maximum in the
amodal dataset literature, along with being the first to provide ground-truth
amodal content. The full dataset is available at
https://huggingface.co/datasets/Amar-S/MOVi-MC-AC ,

</details>


### [201] [CGEarthEye:A High-Resolution Remote Sensing Vision Foundation Model Based on the Jilin-1 Satellite Constellation](https://arxiv.org/abs/2507.00356)
*Zhiwei Yi,Xin Cheng,Jingyu Ma,Ruifei Zhu,Junwei Tian,Yuanxiu Zhou,Xinge Zhao,Hongzhe Li*

Main category: cs.CV

TL;DR: 提出适用于吉林一号卫星的RSVFM框架CGEarthEye，构建JLSSD数据集，预训练后在多任务评估中达SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 超高分光学遥感影像获取渠道有限，制约高分辨率遥感视觉基础模型发展，利用吉林一号卫星丰富影像资源。

Method: 提出含五个不同参数规模主干、共21亿参数的CGEarthEye框架；构建1500万规模的JLSSD数据集；采用多种对比策略预训练。

Result: 在10个基准数据集、4类典型遥感任务评估中CGEarthEye达SOTA性能，在多方面表现出色。

Conclusion: CGEarthEye卓越表征能力将促进吉林一号数据在传统地球观测应用中更广泛高效应用。

Abstract: Deep learning methods have significantly advanced the development of
intelligent rinterpretation in remote sensing (RS), with foundational model
research based on large-scale pre-training paradigms rapidly reshaping various
domains of Earth Observation (EO). However, compared to the open accessibility
and high spatiotemporal coverage of medium-resolution data, the limited
acquisition channels for ultra-high-resolution optical RS imagery have
constrained the progress of high-resolution remote sensing vision foundation
models (RSVFM). As the world's largest sub-meter-level commercial RS satellite
constellation, the Jilin-1 constellation possesses abundant sub-meter-level
image resources. This study proposes CGEarthEye, a RSVFM framework specifically
designed for Jilin-1 satellite characteristics, comprising five backbones with
different parameter scales with totaling 2.1 billion parameters. To enhance the
representational capacity of the foundation model, we developed JLSSD, the
first 15-million-scale multi-temporal self-supervised learning (SSL) dataset
featuring global coverage with quarterly temporal sampling within a single
year, constructed through multi-level representation clustering and sampling
strategies. The framework integrates seasonal contrast, augmentation-based
contrast, and masked patch token contrastive strategies for pre-training.
Comprehensive evaluations across 10 benchmark datasets covering four typical RS
tasks demonstrate that the CGEarthEye consistently achieves state-of-the-art
(SOTA) performance. Further analysis reveals CGEarthEye's superior
characteristics in feature visualization, model convergence, parameter
efficiency, and practical mapping applications. This study anticipates that the
exceptional representation capabilities of CGEarthEye will facilitate broader
and more efficient applications of Jilin-1 data in traditional EO application.

</details>


### [202] [ATSTrack: Enhancing Visual-Language Tracking by Aligning Temporal and Spatial Scales](https://arxiv.org/abs/2507.00454)
*Yihao Zhen,Qiang Wang,Yu Qiao,Liangqiong Qu,Huijie Fan*

Main category: cs.CV

TL;DR: 提出名为ATSTrack的视觉语言跟踪器，通过对齐不同输入组件的时空尺度增强特征修改效果，实验性能与现有方法相当。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言跟踪器受视觉和语言输入在时空尺度上固有差异的阻碍，影响其性能。

Method: 将语言描述分解为与视觉输入有不同时空对应关系的短语，细粒度修改特征；引入包含前一帧修改后语言信息的视觉语言标记，引导模型提取与语言描述更相关的视觉特征。

Result: 提出的ATSTrack取得了与现有方法相当的性能。

Conclusion: 提出的ATSTrack通过新方法能有效处理视觉和语言输入在时空尺度上的差异问题，后续将发布代码。

Abstract: A main challenge of Visual-Language Tracking (VLT) is the misalignment
between visual inputs and language descriptions caused by target movement.
Previous trackers have explored many effective feature modification methods to
preserve more aligned features. However, an important yet unexplored factor
ultimately hinders their capability, which is the inherent differences in the
temporal and spatial scale of information between visual and language inputs.
To address this issue, we propose a novel visual-language tracker that enhances
the effect of feature modification by \textbf{A}ligning \textbf{T}emporal and
\textbf{S}patial scale of different input components, named as
\textbf{ATSTrack}. Specifically, we decompose each language description into
phrases with different attributes based on their temporal and spatial
correspondence with visual inputs, and modify their features in a fine-grained
manner. Moreover, we introduce a Visual-Language token that comprises modified
linguistic information from the previous frame to guide the model to extract
visual features that are more relevant to language description, thereby
reducing the impact caused by the differences in spatial scale. Experimental
results show that our proposed ATSTrack achieves performance comparable to
existing methods. Our code will be released.

</details>


### [203] [AdaDeDup: Adaptive Hybrid Data Pruning for Efficient Large-Scale Object Detection Training](https://arxiv.org/abs/2507.00049)
*Feiyang Kang,Nadine Chang,Maying Shen,Marc T. Law,Rafid Mahmood,Ruoxi Jia,Jose M. Alvarez*

Main category: cs.CV

TL;DR: 现有数据修剪方法存在问题，本文提出AdaDeDup框架，结合密度修剪和模型反馈，在多个数据集上实验证明其可提升数据效率。


<details>
  <summary>Details</summary>
Motivation: 大规模数据集的计算负担和冗余性挑战机器学习模型训练，现有数据修剪方法存在不足。

Method: 提出AdaDeDup混合框架，先进行数据分区和初始密度修剪，再用代理模型评估影响，自适应调整修剪阈值。

Result: 在多个大规模目标检测基准测试中，显著优于基线，大幅减少性能下降，修剪20%数据时接近原模型性能。

Conclusion: AdaDeDup框架在提升大规模模型训练的数据效率方面有效，代码已开源。

Abstract: The computational burden and inherent redundancy of large-scale datasets
challenge the training of contemporary machine learning models. Data pruning
offers a solution by selecting smaller, informative subsets, yet existing
methods struggle: density-based approaches can be task-agnostic, while
model-based techniques may introduce redundancy or prove computationally
prohibitive. We introduce Adaptive De-Duplication (AdaDeDup), a novel hybrid
framework that synergistically integrates density-based pruning with
model-informed feedback in a cluster-adaptive manner. AdaDeDup first partitions
data and applies an initial density-based pruning. It then employs a proxy
model to evaluate the impact of this initial pruning within each cluster by
comparing losses on kept versus pruned samples. This task-aware signal
adaptively adjusts cluster-specific pruning thresholds, enabling more
aggressive pruning in redundant clusters while preserving critical data in
informative ones. Extensive experiments on large-scale object detection
benchmarks (Waymo, COCO, nuScenes) using standard models (BEVFormer, Faster
R-CNN) demonstrate AdaDeDup's advantages. It significantly outperforms
prominent baselines, substantially reduces performance degradation (e.g., over
54% versus random sampling on Waymo), and achieves near-original model
performance while pruning 20% of data, highlighting its efficacy in enhancing
data efficiency for large-scale model training. Code is open-sourced.

</details>


### [204] [Visual Anagrams Reveal Hidden Differences in Holistic Shape Processing Across Vision Models](https://arxiv.org/abs/2507.00493)
*Fenil R. Doshi,Thomas Fel,Talia Konkle,George Alvarez*

Main category: cs.CV

TL;DR: 本文提出配置形状分数（CSS）来评估模型配置能力，发现不同模型的配置敏感性差异，高CSS网络依赖长距离交互，且CSS能预测其他形状相关评估，建议构建能融合局部纹理和全局形状的视觉系统。


<details>
  <summary>Details</summary>
Motivation: 当代视觉模型主要利用局部纹理线索，现有形状 - 纹理偏差研究忽略了模型可同时依赖两种线索的可能，且模糊了两种表征的绝对质量。

Method: 提出配置形状分数（CSS），通过测量对象 - 字谜对图像识别能力来评估模型配置能力，对86个不同类型模型进行测试，使用机械探针分析高CSS网络机制。

Result: 发现不同模型有不同的配置敏感性，全自监督和语言对齐的变压器模型处于CSS谱的高端；高CSS网络依赖长距离交互；BagNet控制实验排除“边界破解”策略；配置形状分数能预测其他形状相关评估。

Conclusion: 构建真正鲁棒、通用和类人的视觉系统不应在形状和纹理间做人为选择，而应在架构和学习框架中无缝集成局部纹理和全局配置形状。

Abstract: Humans are able to recognize objects based on both local texture cues and the
configuration of object parts, yet contemporary vision models primarily harvest
local texture cues, yielding brittle, non-compositional features. Work on
shape-vs-texture bias has pitted shape and texture representations in
opposition, measuring shape relative to texture, ignoring the possibility that
models (and humans) can simultaneously rely on both types of cues, and
obscuring the absolute quality of both types of representation. We therefore
recast shape evaluation as a matter of absolute configural competence,
operationalized by the Configural Shape Score (CSS), which (i) measures the
ability to recognize both images in Object-Anagram pairs that preserve local
texture while permuting global part arrangement to depict different object
categories. Across 86 convolutional, transformer, and hybrid models, CSS (ii)
uncovers a broad spectrum of configural sensitivity with fully self-supervised
and language-aligned transformers -- exemplified by DINOv2, SigLIP2 and
EVA-CLIP -- occupying the top end of the CSS spectrum. Mechanistic probes
reveal that (iii) high-CSS networks depend on long-range interactions:
radius-controlled attention masks abolish performance showing a distinctive
U-shaped integration profile, and representational-similarity analyses expose a
mid-depth transition from local to global coding. A BagNet control remains at
chance (iv), ruling out "border-hacking" strategies. Finally, (v) we show that
configural shape score also predicts other shape-dependent evals. Overall, we
propose that the path toward truly robust, generalizable, and human-like vision
systems may not lie in forcing an artificial choice between shape and texture,
but rather in architectural and learning frameworks that seamlessly integrate
both local-texture and global configural shape.

</details>


### [205] [Box-QAymo: Box-Referring VQA Dataset for Autonomous Driving](https://arxiv.org/abs/2507.00525)
*Djamahl Etchegaray,Yuxia Fu,Zi Huang,Yadan Luo*

Main category: cs.CV

TL;DR: 介绍Box - QAymo数据集和基准，用于评估和微调视觉语言模型在用户指定对象上的时空推理能力，揭示当前模型局限，为开发更强大的自动驾驶系统打基础。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在自动驾驶中难以捕捉用户意图，驾驶导向的VQA数据集有局限，无法评估模型对局部用户驱动查询的响应能力。

Method: 提出分层评估协议，包括不同类型问题；众包细粒度对象类和视觉属性，提取对象轨迹构建问答对；进行严格质量控制。

Result: 综合评估显示当前视觉语言模型在感知问题查询上存在显著局限。

Conclusion: 该工作为开发能在现实条件下与用户有效沟通的更强大、可解释的自动驾驶系统提供基础。

Abstract: Interpretable communication is essential for safe and trustworthy autonomous
driving, yet current vision-language models (VLMs) often operate under
idealized assumptions and struggle to capture user intent in real-world
scenarios. Existing driving-oriented VQA datasets are limited to full-scene
descriptions or waypoint prediction, preventing the assessment of whether VLMs
can respond to localized user-driven queries. We introduce Box-QAymo, a
box-referring dataset and benchmark designed to both evaluate and finetune VLMs
on spatial and temporal reasoning over user-specified objects. Users express
intent by drawing bounding boxes, offering a fast and intuitive interface for
focused queries in complex scenes. Specifically, we propose a hierarchical
evaluation protocol that begins with binary sanity-check questions to assess
basic model capacities, and progresses to (1) attribute prediction for
box-referred objects, (2) motion understanding of target instances, and (3)
spatiotemporal motion reasoning over inter-object dynamics across frames. To
support this, we crowd-sourced fine-grained object classes and visual
attributes that reflect the complexity drivers encounter, and extract object
trajectories to construct temporally grounded QA pairs. Rigorous quality
control through negative sampling, temporal consistency checks, and
difficulty-aware balancing guarantee dataset robustness and diversity. Our
comprehensive evaluation reveals significant limitations in current VLMs when
queried about perception questions, highlighting the gap in achieving
real-world performance. This work provides a foundation for developing more
robust and interpretable autonomous driving systems that can communicate
effectively with users under real-world conditions. Project page and dataset
are available at https://djamahl99.github.io/qaymo-pages/.

</details>


### [206] [Not All Attention Heads Are What You Need: Refining CLIP's Image Representation with Attention Ablation](https://arxiv.org/abs/2507.00537)
*Feng Lin,Marco Chen,Haokui Zhang,Xiaotian Yu,Guangming Lu,Rong Xiao*

Main category: cs.CV

TL;DR: 研究CLIP图像编码器中注意力头的作用，提出AAT方法去除有害注意力头，实验表明该方法能提升下游任务性能且不增加推理成本。


<details>
  <summary>Details</summary>
Motivation: 假设CLIP中某些注意力头会对最终表示产生负面影响，去除它们可提升下游任务性能。

Method: 提出Attention Ablation Technique (AAT) 方法，通过操纵注意力权重抑制特定头的贡献，结合两种策略识别并去除有害注意力头。

Result: AAT在多个领域持续提升下游任务性能，在CLIP系列模型的跨模态检索中召回率最高提升11.1%。

Conclusion: AAT有潜力有效优化大规模视觉 - 语言模型，且几乎不增加推理成本。

Abstract: This paper studies the role of attention heads in CLIP's image encoder. While
CLIP has exhibited robust performance across diverse applications, we
hypothesize that certain attention heads negatively affect final
representations and that ablating them can improve performance in downstream
tasks. To capitalize on this insight, we propose a simple yet effective method,
called Attention Ablation Technique (AAT), to suppress the contribution of
specific heads by manipulating attention weights. By integrating two
alternative strategies tailored for different application scenarios, AAT
systematically identifies and ablates detrimental attention heads to enhance
representation quality. Experiments demonstrate that AAT consistently improves
downstream task performance across various domains, boosting recall rate by up
to 11.1% on CLIP-family models for cross-modal retrieval. The results highlight
the potential of AAT to effectively refine large-scale vision-language models
with virtually no increase in inference cost.

</details>


### [207] [Few-shot Classification as Multi-instance Verification: Effective Backbone-agnostic Transfer across Domains](https://arxiv.org/abs/2507.00401)
*Xin Xu,Eibe Frank,Geoffrey Holmes*

Main category: cs.CV

TL;DR: 研究在骨干网络无法微调情况下的跨域少样本学习，提出MIV - head方法，实验显示其准确性高、适应成本低。


<details>
  <summary>Details</summary>
Motivation: 解决在实际应用中骨干网络无法或难以微调时，处理低质量静态嵌入进行少样本分类的问题。

Method: 将少样本分类问题表示为多个实例验证任务，提出MIV - head方法，在目标域少样本数据上训练。

Result: 在多种设置和Meta - dataset基准扩展实验中，MIV - head与先进的“适配器”方法相比，准确性有竞争力且适应成本低，“分类头”方法准确性远不及。

Conclusion: MIV - head在不微调骨干网络情况下，能有效解决跨域少样本学习问题，消融实验验证了其核心组件的合理性。

Abstract: We investigate cross-domain few-shot learning under the constraint that
fine-tuning of backbones (i.e., feature extractors) is impossible or infeasible
-- a scenario that is increasingly common in practical use cases. Handling the
low-quality and static embeddings produced by frozen, "black-box" backbones
leads to a problem representation of few-shot classification as a series of
multiple instance verification (MIV) tasks. Inspired by this representation, we
introduce a novel approach to few-shot domain adaptation, named the "MIV-head",
akin to a classification head that is agnostic to any pretrained backbone and
computationally efficient. The core components designed for the MIV-head, when
trained on few-shot data from a target domain, collectively yield strong
performance on test data from that domain. Importantly, it does so without
fine-tuning the backbone, and within the "meta-testing" phase. Experimenting
under various settings and on an extension of the Meta-dataset benchmark for
cross-domain few-shot image classification, using representative off-the-shelf
convolutional neural network and vision transformer backbones pretrained on
ImageNet1K, we show that the MIV-head achieves highly competitive accuracy when
compared to state-of-the-art "adapter" (or partially fine-tuning) methods
applied to the same backbones, while incurring substantially lower adaptation
cost. We also find well-known "classification head" approaches lag far behind
in terms of accuracy. Ablation study empirically justifies the core components
of our approach. We share our code at https://github.com/xxweka/MIV-head.

</details>


### [208] [AI-Generated Video Detection via Perceptual Straightening](https://arxiv.org/abs/2507.00583)
*Christian Internò,Robert Geirhos,Markus Olhofer,Sunny Liu,Barbara Hammer,David Klindt*

Main category: cs.CV

TL;DR: 本文提出ReStraV方法检测AI生成视频，利用神经表征几何，性能超现有方法且计算高效。


<details>
  <summary>Details</summary>
Motivation: 生成式AI使合成视频逼真，现有检测方法泛化能力弱、难捕捉细微时间不一致性。

Method: 受“感知拉直”假设启发，用预训练自监督视觉变换器DINOv2量化表征域的时间曲率和逐步距离，聚合统计量训练分类器。

Result: 轻量级分类器在VidProM基准上取得97.17%准确率和98.63% AUROC，远超现有基于图像和视频的方法。

Conclusion: ReStraV计算高效，为AI生成视频检测提供新见解。

Abstract: The rapid advancement of generative AI enables highly realistic synthetic
videos, posing significant challenges for content authentication and raising
urgent concerns about misuse. Existing detection methods often struggle with
generalization and capturing subtle temporal inconsistencies. We propose
ReStraV(Representation Straightening Video), a novel approach to distinguish
natural from AI-generated videos. Inspired by the "perceptual straightening"
hypothesis -- which suggests real-world video trajectories become more straight
in neural representation domain -- we analyze deviations from this expected
geometric property. Using a pre-trained self-supervised vision transformer
(DINOv2), we quantify the temporal curvature and stepwise distance in the
model's representation domain. We aggregate statistics of these measures for
each video and train a classifier. Our analysis shows that AI-generated videos
exhibit significantly different curvature and distance patterns compared to
real videos. A lightweight classifier achieves state-of-the-art detection
performance (e.g., 97.17% accuracy and 98.63% AUROC on the VidProM benchmark),
substantially outperforming existing image- and video-based methods. ReStraV is
computationally efficient, it is offering a low-cost and effective detection
solution. This work provides new insights into using neural representation
geometry for AI-generated video detection.

</details>


### [209] [Bisecle: Binding and Separation in Continual Learning for Video Language Understanding](https://arxiv.org/abs/2507.00469)
*Yue Tan,Xiaoqian Hu,Hao Xue,Celso De Melo,Flora D. Salim*

Main category: cs.CV

TL;DR: 受人类大脑海马体机制启发，提出用于视频语言持续学习的Bisecle方法，在VideoQA基准测试上效果良好。


<details>
  <summary>Details</summary>
Motivation: 现实视频多为连续数据流，微调模型计算成本高，现有持续学习框架在大模型下存在灾难性遗忘和更新冲突问题。

Method: 受海马体机制启发，提出Bisecle，用多向监督模块捕捉跨模态关系，设计对比提示学习方案隔离特定任务知识。

Result: 在多个VideoQA基准测试中，能减轻遗忘并增强跨任务泛化能力。

Conclusion: Bisecle能让视觉语言模型在视频理解任务中进行稳健高效的持续学习。

Abstract: Frontier vision-language models (VLMs) have made remarkable improvements in
video understanding tasks. However, real-world videos typically exist as
continuously evolving data streams (e.g., dynamic scenes captured by wearable
glasses), necessitating models to continually adapt to shifting data
distributions and novel scenarios. Considering the prohibitive computational
costs of fine-tuning models on new tasks, usually, a small subset of parameters
is updated while the bulk of the model remains frozen. This poses new
challenges to existing continual learning frameworks in the context of large
multimodal foundation models, i.e., catastrophic forgetting and update
conflict. While the foundation models struggle with parameter-efficient
continual learning, the hippocampus in the human brain has evolved highly
efficient mechanisms for memory formation and consolidation. Inspired by the
rapid Binding and pattern separation mechanisms in the hippocampus, in this
work, we propose Bisecle for video-language continual learning, where a
multi-directional supervision module is used to capture more cross-modal
relationships and a contrastive prompt learning scheme is designed to isolate
task-specific knowledge to facilitate efficient memory storage. Binding and
separation processes further strengthen the ability of VLMs to retain complex
experiences, enabling robust and efficient continual learning in video
understanding tasks. We perform a thorough evaluation of the proposed Bisecle,
demonstrating its ability to mitigate forgetting and enhance cross-task
generalization on several VideoQA benchmarks.

</details>


### [210] [TopoStreamer: Temporal Lane Segment Topology Reasoning in Autonomous Driving](https://arxiv.org/abs/2507.00709)
*Yiming Yang,Yueru Luo,Bingkun He,Hongbin Lin,Suzhong Fu,Chao Yan,Kun Tang,Xinrui Yan,Chao Zheng,Shuguang Cui,Zhen Li*

Main category: cs.CV

TL;DR: 提出TopoStreamer模型用于车道段拓扑推理，在OpenLane - V2数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在一致位置嵌入和时间多属性学习方面存在局限，阻碍准确道路网络重建，为解决此问题开展研究。

Method: 提出TopoStreamer模型，有三项关键改进：流式属性约束、动态车道边界位置编码和车道段去噪；用车道边界分类指标评估现有模型准确性。

Result: 在OpenLane - V2数据集上，TopoStreamer在车道段感知中mAP提升3.4%，在中心线感知任务中OLS提升2.1%。

Conclusion: TopoStreamer模型在车道段拓扑推理方面表现出色，优于现有方法。

Abstract: Lane segment topology reasoning constructs a comprehensive road network by
capturing the topological relationships between lane segments and their
semantic types. This enables end-to-end autonomous driving systems to perform
road-dependent maneuvers such as turning and lane changing. However, the
limitations in consistent positional embedding and temporal multiple attribute
learning in existing methods hinder accurate roadnet reconstruction. To address
these issues, we propose TopoStreamer, an end-to-end temporal perception model
for lane segment topology reasoning. Specifically, TopoStreamer introduces
three key improvements: streaming attribute constraints, dynamic lane boundary
positional encoding, and lane segment denoising. The streaming attribute
constraints enforce temporal consistency in both centerline and boundary
coordinates, along with their classifications. Meanwhile, dynamic lane boundary
positional encoding enhances the learning of up-to-date positional information
within queries, while lane segment denoising helps capture diverse lane segment
patterns, ultimately improving model performance. Additionally, we assess the
accuracy of existing models using a lane boundary classification metric, which
serves as a crucial measure for lane-changing scenarios in autonomous driving.
On the OpenLane-V2 dataset, TopoStreamer demonstrates significant improvements
over state-of-the-art methods, achieving substantial performance gains of +3.4%
mAP in lane segment perception and +2.1% OLS in centerline perception tasks.

</details>


### [211] [Holmes: Towards Effective and Harmless Model Ownership Verification to Personalized Large Vision Models via Decoupling Common Features](https://arxiv.org/abs/2507.00724)
*Linghui Zhu,Yiming Li,Haiqin Weng,Yan Liu,Tianwei Zhang,Shu-Tao Xia,Zhi Wang*

Main category: cs.CV

TL;DR: 论文指出传统DNN防御方法对微调模型无效，提出解耦相似公共特征的个性化模型所有权验证方法，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有针对传统DNN的防御方法对微调的个性化模型存在引入额外安全风险、易误判或无效等问题，需新的防御方法。

Method: 分三个阶段：创建保留公共特征但破坏特定数据集特征的影子模型；训练元分类器识别可疑模型是否包含受害者模型特定数据集特征；通过假设检验进行模型所有权验证。

Result: 在基准数据集上的大量实验验证了该方法能同时检测不同类型的模型窃取。

Conclusion: 所提出的方法有效且能同时检测不同类型的模型窃取攻击。

Abstract: Large vision models achieve remarkable performance in various downstream
tasks, primarily by personalizing pre-trained models through fine-tuning with
private and valuable local data, which makes the personalized model a valuable
intellectual property for its owner. Similar to the era of traditional DNNs,
model stealing attacks also pose significant risks to these personalized
models. However, in this paper, we reveal that most existing defense methods
(developed for traditional DNNs), typically designed for models trained from
scratch, either introduce additional security risks, are prone to misjudgment,
or are even ineffective for fine-tuned models. To alleviate these problems,
this paper proposes a harmless model ownership verification method for
personalized models by decoupling similar common features. In general, our
method consists of three main stages. In the first stage, we create shadow
models that retain common features of the victim model while disrupting
dataset-specific features. We represent the dataset-specific features of the
victim model by the output differences between the shadow and victim models.
After that, a meta-classifier is trained to identify stolen models by
determining whether suspicious models contain the dataset-specific features of
the victim. In the third stage, we conduct model ownership verification by
hypothesis test to mitigate randomness and enhance robustness. Extensive
experiments on benchmark datasets verify the effectiveness of the proposed
method in detecting different types of model stealing simultaneously.

</details>


### [212] [LD-RPS: Zero-Shot Unified Image Restoration via Latent Diffusion Recurrent Posterior Sampling](https://arxiv.org/abs/2507.00790)
*Huaqiu Li,Yong Wang,Tongwen Huang,Hailang Huang,Haoqian Wang,Xiangxiang Chu*

Main category: cs.CV

TL;DR: 提出利用预训练潜扩散模型进行循环后验采样的无数据集统一图像恢复方法，实验效果佳。


<details>
  <summary>Details</summary>
Motivation: 现有图像恢复方法存在泛化性差和封闭集约束问题。

Method: 通过预训练潜扩散模型进行循环后验采样，结合多模态理解模型提供语义先验，用轻量级模块对齐输入与生成偏好，进行循环细化。

Result: 方法性能优于现有方法，验证了有效性和鲁棒性。

Conclusion: 所提方法有效解决现有图像恢复方法问题，代码和数据将公开。

Abstract: Unified image restoration is a significantly challenging task in low-level
vision. Existing methods either make tailored designs for specific tasks,
limiting their generalizability across various types of degradation, or rely on
training with paired datasets, thereby suffering from closed-set constraints.
To address these issues, we propose a novel, dataset-free, and unified approach
through recurrent posterior sampling utilizing a pretrained latent diffusion
model. Our method incorporates the multimodal understanding model to provide
sematic priors for the generative model under a task-blind condition.
Furthermore, it utilizes a lightweight module to align the degraded input with
the generated preference of the diffusion model, and employs recurrent
refinement for posterior sampling. Extensive experiments demonstrate that our
method outperforms state-of-the-art methods, validating its effectiveness and
robustness. Our code and data will be available at
https://github.com/AMAP-ML/LD-RPS.

</details>


### [213] [CAVALRY-V: A Large-Scale Generator Framework for Adversarial Attacks on Video MLLMs](https://arxiv.org/abs/2507.00817)
*Jiaming Zhang,Rui Hu,Qing Guo,Wei Yang Bryan Lim*

Main category: cs.CV

TL;DR: 本文提出CAVALRY - V框架攻击视频多模态大语言模型，在视频和图像理解基准测试中表现出色，有潜力成为多模态系统对抗研究基础方法。


<details>
  <summary>Details</summary>
Motivation: 视频多模态大语言模型易受对抗攻击，但因复杂推理机制、时间依赖和计算限制，相关研究不足。

Method: 提出CAVALRY - V框架，包含双目标语义 - 视觉损失函数和高效两阶段生成器框架。

Result: 在视频理解基准测试中显著优于现有攻击方法，平均提升22.8%；在图像理解上平均提升34.4%。

Conclusion: CAVALRY - V有潜力成为多模态系统对抗研究的基础方法。

Abstract: Video Multimodal Large Language Models (V-MLLMs) have shown impressive
capabilities in temporal reasoning and cross-modal understanding, yet their
vulnerability to adversarial attacks remains underexplored due to unique
challenges: complex cross-modal reasoning mechanisms, temporal dependencies,
and computational constraints. We present CAVALRY-V (Cross-modal
Language-Vision Adversarial Yielding for Videos), a novel framework that
directly targets the critical interface between visual perception and language
generation in V-MLLMs. Our approach introduces two key innovations: (1) a
dual-objective semantic-visual loss function that simultaneously disrupts the
model's text generation logits and visual representations to undermine
cross-modal integration, and (2) a computationally efficient two-stage
generator framework that combines large-scale pre-training for cross-model
transferability with specialized fine-tuning for spatiotemporal coherence.
Empirical evaluation on comprehensive video understanding benchmarks
demonstrates that CAVALRY-V significantly outperforms existing attack methods,
achieving 22.8% average improvement over the best baseline attacks on both
commercial systems (GPT-4.1, Gemini 2.0) and open-source models (QwenVL-2.5,
InternVL-2.5, Llava-Video, Aria, MiniCPM-o-2.6). Our framework achieves
flexibility through implicit temporal coherence modeling rather than explicit
regularization, enabling significant performance improvements even on image
understanding (34.4% average gain). This capability demonstrates CAVALRY-V's
potential as a foundational approach for adversarial research across multimodal
systems.

</details>


### [214] [Do Echo Top Heights Improve Deep Learning Nowcasts?](https://arxiv.org/abs/2507.00845)
*Peter Pavlík,Marc Schleiss,Anna Bou Ezzeddine,Viera Rozinajová*

Main category: cs.CV

TL;DR: 本文探讨将Echo Top Height (ETH)作为辅助输入变量用于深度学习降水临近预报，模型在低降雨率阈值下有改进，但高强度下结果不一致，研究为评估额外变量对临近预报性能的贡献奠定基础。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习临近预报方法大多仅依赖2D雷达反射率场，丢弃了3D雷达体积中的垂直信息，因此探索使用ETH作为辅助输入变量。

Method: 研究ETH与雷达反射率的关系，实现单通3D U - Net，将雷达反射率和ETH作为单独输入通道处理。

Result: 模型在低降雨率阈值下能利用ETH提高预报技巧，但在高降雨强度下结果不一致，含ETH的模型系统性低估降水强度，案例研究表明ETH有时有帮助，有时会增加误差方差。

Conclusion: 该研究为批判性评估额外变量对临近预报性能的潜在贡献提供了基础。

Abstract: Precipitation nowcasting -- the short-term prediction of rainfall using
recent radar observations -- is critical for weather-sensitive sectors such as
transportation, agriculture, and disaster mitigation. While recent deep
learning models have shown promise in improving nowcasting skill, most
approaches rely solely on 2D radar reflectivity fields, discarding valuable
vertical information available in the full 3D radar volume. In this work, we
explore the use of Echo Top Height (ETH), a 2D projection indicating the
maximum altitude of radar reflectivity above a given threshold, as an auxiliary
input variable for deep learning-based nowcasting. We examine the relationship
between ETH and radar reflectivity, confirming its relevance for predicting
rainfall intensity. We implement a single-pass 3D U-Net that processes both the
radar reflectivity and ETH as separate input channels. While our models are
able to leverage ETH to improve skill at low rain-rate thresholds, results are
inconsistent at higher intensities and the models with ETH systematically
underestimate precipitation intensity. Three case studies are used to
illustrate how ETH can help in some cases, but also confuse the models and
increase the error variance. Nonetheless, the study serves as a foundation for
critically assessing the potential contribution of additional variables to
nowcasting performance.

</details>


### [215] [MVP: Winning Solution to SMP Challenge 2025 Video Track](https://arxiv.org/abs/2507.00950)
*Liliang Ye,Yunyao Zhang,Yafeng Wu,Yi-Ping Phoebe Chen,Junqing Yu,Wei Yang,Zikai Song*

Main category: cs.CV

TL;DR: 提出多模态视频预测器MVP用于社交媒体视频流行度预测，在SMP挑战2025视频赛道获第一，代码开源。


<details>
  <summary>Details</summary>
Motivation: 社交媒体是内容传播等中心，准确预测视频流行度有诸多有价值应用。

Method: 构建MVP，整合预训练模型提取的视频特征、用户元数据和上下文信息，进行系统预处理，训练梯度提升回归模型。

Result: 在SMP挑战2025视频赛道官方评估中排名第一。

Conclusion: MVP对社交媒体多模态视频流行度预测有效且可靠。

Abstract: Social media platforms serve as central hubs for content dissemination,
opinion expression, and public engagement across diverse modalities. Accurately
predicting the popularity of social media videos enables valuable applications
in content recommendation, trend detection, and audience engagement. In this
paper, we present Multimodal Video Predictor (MVP), our winning solution to the
Video Track of the SMP Challenge 2025. MVP constructs expressive post
representations by integrating deep video features extracted from pretrained
models with user metadata and contextual information. The framework applies
systematic preprocessing techniques, including log-transformations and outlier
removal, to improve model robustness. A gradient-boosted regression model is
trained to capture complex patterns across modalities. Our approach ranked
first in the official evaluation of the Video Track, demonstrating its
effectiveness and reliability for multimodal video popularity prediction on
social platforms. The source code is available at
https://anonymous.4open.science/r/SMPDVideo.

</details>


### [216] [GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with Scalable Reinforcement Learning](https://arxiv.org/abs/2507.01006)
*Wenyi Hong,Wenmeng Yu,Xiaotao Gu,Guo Wang,Guobing Gan,Haomiao Tang,Jiale Cheng,Ji Qi,Junhui Ji,Lihang Pan,Shuaiqi Duan,Weihan Wang,Yan Wang,Yean Cheng,Zehai He,Zhe Su,Zhen Yang,Ziyang Pan,Aohan Zeng,Baoxu Wang,Boyan Shi,Changyu Pang,Chenhui Zhang,Da Yin,Fan Yang,Guoqing Chen,Jiazheng Xu,Jiali Chen,Jing Chen,Jinhao Chen,Jinghao Lin,Jinjiang Wang,Junjie Chen,Leqi Lei,Leyi Pan,Mingzhi Zhang,Qinkai Zheng,Sheng Yang,Shi Zhong,Shiyu Huang,Shuyuan Zhao,Siyan Xue,Shangqin Tu,Shengbiao Meng,Tianshu Zhang,Tianwei Luo,Tianxiang Hao,Tianle Gong,Wenkai Li,Wei Jia,Xin Lyu,Xuancheng Huang,Yanling Wang,Yadong Xue,Yanfeng Wang,Yifan An,Yifan Du,Yiming Shi,Yiheng Huang,Yilin Niu,Yuan Wang,Yuanchang Yue,Yuchen Li,Yutao Zhang,Yuxuan Zhang,Zhanxiao Du,Zhenyu Hou,Zhao Xue,Zhengxiao Du,Zihan Wang,Peng Zhang,Debing Liu,Bin Xu,Juanzi Li,Minlie Huang,Yuxiao Dong,Jie Tang*

Main category: cs.CV

TL;DR: 介绍视觉语言模型GLM - 4.1V - Thinking，详述开发推理训练框架成果，开源GLM - 4.1V - 9B - Thinking，其在多基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 推进通用多模态推理，开发推理中心训练框架。

Method: 先大规模预训练开发视觉基础模型，再用带课程采样的强化学习（RLCS）释放模型潜力。

Result: GLM - 4.1V - 9B - Thinking在28个公开基准测试中表现出色，超越Qwen2.5 - VL - 7B，部分超越Qwen2.5 - VL - 72B，在长文档理解和STEM推理等任务上与GPT - 4o竞争。

Conclusion: GLM - 4.1V - Thinking具有强大能力，开源代码利于领域研究。

Abstract: We present GLM-4.1V-Thinking, a vision-language model (VLM) designed to
advance general-purpose multimodal reasoning. In this report, we share our key
findings in the development of the reasoning-centric training framework. We
first develop a capable vision foundation model with significant potential
through large-scale pre-training, which arguably sets the upper bound for the
final performance. Reinforcement Learning with Curriculum Sampling (RLCS) then
unlocks the full potential of the model, leading to comprehensive capability
enhancement across a diverse range of tasks, including STEM problem solving,
video understanding, content recognition, coding, grounding, GUI-based agents,
and long document understanding, among others. To facilitate research in this
field, we open-source GLM-4.1V-9B-Thinking, which achieves state-of-the-art
performance among models of comparable size. In a comprehensive evaluation
across 28 public benchmarks, our model outperforms Qwen2.5-VL-7B on nearly all
tasks and achieves comparable or even superior performance on 18 benchmarks
relative to the significantly larger Qwen2.5-VL-72B. Notably,
GLM-4.1V-9B-Thinking also demonstrates competitive or superior performance
compared to closed-source models such as GPT-4o on challenging tasks including
long document understanding and STEM reasoning, further underscoring its strong
capabilities. Code, models and more information are released at
https://github.com/THUDM/GLM-4.1V-Thinking.

</details>


### [217] [Surgical Neural Radiance Fields from One Image](https://arxiv.org/abs/2507.00969)
*Alberto Neri,Maximilan Fehrentz,Veronica Penza,Leonardo S. Mattos,Nazim Haouchine*

Main category: cs.CV

TL;DR: 本文利用单张术中图像和术前数据解决NeRF在手术场景数据有限的问题，方法经临床案例验证有效。


<details>
  <summary>Details</summary>
Motivation: NeRF依赖大量多视图数据，在手术中因数据有限和时间限制难以应用，需解决该问题。

Method: 利用术前MRI数据确定相机视角和图像，通过神经风格迁移将术中图像外观转移到预构建训练集，创建数据集用于单图像NeRF训练。

Result: 经四个神经外科临床案例评估，与真实手术显微镜图像训练的NeRF模型相比有强合成一致性，与真值对比有高结构相似性。

Conclusion: 该方法证明了单图像NeRF训练在手术场景的可行性，克服了传统多视图方法的局限。

Abstract: Purpose: Neural Radiance Fields (NeRF) offer exceptional capabilities for 3D
reconstruction and view synthesis, yet their reliance on extensive multi-view
data limits their application in surgical intraoperative settings where only
limited data is available. In particular, collecting such extensive data
intraoperatively is impractical due to time constraints. This work addresses
this challenge by leveraging a single intraoperative image and preoperative
data to train NeRF efficiently for surgical scenarios.
  Methods: We leverage preoperative MRI data to define the set of camera
viewpoints and images needed for robust and unobstructed training.
Intraoperatively, the appearance of the surgical image is transferred to the
pre-constructed training set through neural style transfer, specifically
combining WTC2 and STROTSS to prevent over-stylization. This process enables
the creation of a dataset for instant and fast single-image NeRF training.
  Results: The method is evaluated with four clinical neurosurgical cases.
Quantitative comparisons to NeRF models trained on real surgical microscope
images demonstrate strong synthesis agreement, with similarity metrics
indicating high reconstruction fidelity and stylistic alignment. When compared
with ground truth, our method demonstrates high structural similarity,
confirming good reconstruction quality and texture preservation.
  Conclusion: Our approach demonstrates the feasibility of single-image NeRF
training in surgical settings, overcoming the limitations of traditional
multi-view methods.

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [218] [Verification of Hamiltonian Path Conjecture (BHR Conjecture) for Integers up to p=31](https://arxiv.org/abs/2507.00059)
*Ranjan N Naik*

Main category: cs.DM

TL;DR: 本文基于频率划分、局部/全局调整操作和回溯法探索BHR猜想，用Python程序寻找p < 37的有效哈密顿路径，结果优于Mariusz Meszka。


<details>
  <summary>Details</summary>
Motivation: 解决BHR猜想，该猜想提出对于给定的p和模p的(p - 1)个正整数的多重集L，完全图Kp中存在连续边长由L元素给出的哈密顿路径。

Method: 采用频率划分、局部/全局调整操作和回溯法，通过Python程序进行探索。

Result: 在探索p < 37的有效哈密顿路径方面取得结果，优于Mariusz Meszka对所有小于等于23的素数的研究结果。

Conclusion: 所采用的方法在解决BHR猜想上有一定成效，能在一定范围内得到更好的结果。

Abstract: The BHR (Buratti-Horak-Rosa) Conjecture (2006) proposes that for every p and
a multiset L of (p-1) positive integers modulo p, there exists a Hamiltonian
path in the Complete Graph Kp with consecutive edge lengths given by the
elements of L. In this article, we outline an approach to the conjecture based
on frequency partitions and local/global adjustment operations and
backtracking. We describe the mathematical strategy, experimental evidence, and
implementation in a Python Program to explore valid Hamiltonian paths p < 37.
This is a result an improvement over by Mariusz Meszka for all primes up to 23
(included) with the aid of a computer.

</details>


### [219] [$σ$-Maximal Ancestral Graphs](https://arxiv.org/abs/2507.00093)
*Binghua Yao,Joris M. Mooij*

Main category: cs.DM

TL;DR: 论文引入σ - Maximal Ancestral Graphs (σ - MAGs) 解决MAGs无法表示循环因果关系的局限，研究其性质并刻画马尔可夫等价类。


<details>
  <summary>Details</summary>
Motivation: Maximal Ancestral Graphs (MAGs) 存在无法表示循环因果关系的固有局限，需要解决该问题。

Method: 引入并研究一类新的图形对象σ - MAGs。

Result: 展示了σ - MAGs如何对可能有循环的有向图进行抽象表示，研究了其性质并刻画了马尔可夫等价类。

Conclusion: σ - MAGs可解决MAGs的局限性，为有循环的有向图提供类似MAGs对DAGs的抽象表示。

Abstract: Maximal Ancestral Graphs (MAGs) provide an abstract representation of
Directed Acyclic Graphs (DAGs) with latent (selection) variables. These
graphical objects encode information about ancestral relations and
d-separations of the DAGs they represent. This abstract representation has been
used amongst others to prove the soundness and completeness of the FCI
algorithm for causal discovery, and to derive a do-calculus for its output. One
significant inherent limitation of MAGs is that they rule out the possibility
of cyclic causal relationships. In this work, we address that limitation. We
introduce and study a class of graphical objects that we coin
''$\sigma$-Maximal Ancestral Graphs'' (''$\sigma$-MAGs''). We show how these
graphs provide an abstract representation of (possibly cyclic) Directed Graphs
(DGs) with latent (selection) variables, analogously to how MAGs represent
DAGs. We study the properties of these objects and provide a characterization
of their Markov equivalence classes.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [220] [A Practical Guide to Interpretable Role-Based Clustering in Multi-Layer Financial Networks](https://arxiv.org/abs/2507.00600)
*Christian Franssen,Iman van Lelyveld,Bernd Heidergott*

Main category: cs.SI

TL;DR: 提出基于角色的多层金融网络聚类方法，用交易数据展示其能揭示机构角色，凸显该方法灵活性和实用价值。


<details>
  <summary>Details</summary>
Motivation: 理解金融机构在互联市场中的功能角色对监管、风险评估和解决计划至关重要。

Method: 采用基于邻近度量、聚类评估标准和算法选择的聚类框架，基于自我中心网络特征构建可解释的节点嵌入。

Result: 利用欧洲央行货币市场统计报告的交易数据，揭示了市场中介、跨部门连接器、外围借贷者等不同机构角色。

Conclusion: 基于角色的聚类方法在分析金融网络和理解复杂市场结构中机构行为方面具有灵活性和实用价值。

Abstract: Understanding the functional roles of financial institutions within
interconnected markets is critical for effective supervision, systemic risk
assessment, and resolution planning. We propose an interpretable role-based
clustering approach for multi-layer financial networks, designed to identify
the functional positions of institutions across different market segments. Our
method follows a general clustering framework defined by proximity measures,
cluster evaluation criteria, and algorithm selection. We construct explainable
node embeddings based on egonet features that capture both direct and indirect
trading relationships within and across market layers. Using transaction-level
data from the ECB's Money Market Statistical Reporting (MMSR), we demonstrate
how the approach uncovers heterogeneous institutional roles such as market
intermediaries, cross-segment connectors, and peripheral lenders or borrowers.
The results highlight the flexibility and practical value of role-based
clustering in analyzing financial networks and understanding institutional
behavior in complex market structures.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [221] [Quantum Speedups for Polynomial-Time Dynamic Programming Algorithms](https://arxiv.org/abs/2507.00823)
*Susanna Caroppo,Giordano Da Lozzo,Giuseppe Di Battista,Michael T. Goodrich,Martin Nöllenburg*

Main category: quant-ph

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We introduce a quantum dynamic programming framework that allows us to
directly extend to the quantum realm a large body of classical dynamic
programming algorithms. The corresponding quantum dynamic programming
algorithms retain the same space complexity as their classical counterpart,
while achieving a computational speedup. For a combinatorial (search or
optimization) problem $\mathcal P$ and an instance $I$ of $\mathcal P$, such a
speedup can be expressed in terms of the average degree $\delta$ of the
dependency digraph $G_{\mathcal{P}}(I)$ of $I$, determined by a recursive
formulation of $\mathcal P$. The nodes of this graph are the subproblems of
$\mathcal P$ induced by $I$ and its arcs are directed from each subproblem to
those on whose solution it relies. In particular, our framework allows us to
solve the considered problems in $\tilde{O}(|V(G_{\mathcal{P}}(I))|
\sqrt{\delta})$ time. As an example, we obtain a quantum version of the
Bellman-Ford algorithm for computing shortest paths from a single source vertex
to all the other vertices in a weighted $n$-vertex digraph with $m$ edges that
runs in $\tilde{O}(n\sqrt{nm})$ time, which improves the best known classical
upper bound when $m \in \Omega(n^{1.4})$.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [222] [Multimodal, Multi-Disease Medical Imaging Foundation Model (MerMED-FM)](https://arxiv.org/abs/2507.00185)
*Yang Zhou,Chrystie Wan Ning Quek,Jun Zhou,Yan Wang,Yang Bai,Yuhe Ke,Jie Yao,Laura Gutierrez,Zhen Ling Teo,Darren Shu Jeng Ting,Brian T. Soetikno,Christopher S. Nielsen,Tobias Elze,Zengxiang Li,Linh Le Dinh,Lionel Tim-Ee Cheng,Tran Nguyen Tuan Anh,Chee Leong Cheng,Tien Yin Wong,Nan Liu,Iain Beehuat Tan,Tony Kiat Hon Lim,Rick Siow Mong Goh,Yong Liu,Daniel Shu Wei Ting*

Main category: eess.IV

TL;DR: 开发多模态多专科基础模型MerMED - FM，经评估表现良好，有潜力用于多学科医学影像解读。


<details>
  <summary>Details</summary>
Motivation: 现有医学影像人工智能模型多为单模态单疾病，多模态多疾病模型临床准确性不一致且训练需大量标注数据。

Method: 采用自监督学习和记忆模块，在来自超十个专科、七种模态的330万张医学图像上训练MerMED - FM。

Result: MerMED - FM在所有模态上表现出色，各模态有对应AUROC值。

Conclusion: MerMED - FM有潜力成为高度适配、多功能、跨专科的基础模型，用于不同医学学科的医学影像解读。

Abstract: Current artificial intelligence models for medical imaging are predominantly
single modality and single disease. Attempts to create multimodal and
multi-disease models have resulted in inconsistent clinical accuracy.
Furthermore, training these models typically requires large, labour-intensive,
well-labelled datasets. We developed MerMED-FM, a state-of-the-art multimodal,
multi-specialty foundation model trained using self-supervised learning and a
memory module. MerMED-FM was trained on 3.3 million medical images from over
ten specialties and seven modalities, including computed tomography (CT), chest
X-rays (CXR), ultrasound (US), pathology patches, color fundus photography
(CFP), optical coherence tomography (OCT) and dermatology images. MerMED-FM was
evaluated across multiple diseases and compared against existing foundational
models. Strong performance was achieved across all modalities, with AUROCs of
0.988 (OCT); 0.982 (pathology); 0.951 (US); 0.943 (CT); 0.931 (skin); 0.894
(CFP); 0.858 (CXR). MerMED-FM has the potential to be a highly adaptable,
versatile, cross-specialty foundation model that enables robust medical imaging
interpretation across diverse medical disciplines.

</details>


### [223] [SurgiSR4K: A High-Resolution Endoscopic Video Dataset for Robotic-Assisted Minimally Invasive Procedures](https://arxiv.org/abs/2507.00209)
*Fengyi Jiang,Xiaorui Zhang,Lingbo Jin,Ruixing Liang,Yuxin Chen,Adi Chola Venkatesh,Jason Culman,Tiantian Wu,Lirong Shao,Wenqing Sun,Cong Gao,Hallie McNamara,Jingpei Lu,Omid Mohareri*

Main category: eess.IV

TL;DR: 介绍首个公开的原生4K手术影像数据集SurgiSR4K，可用于多种计算机视觉任务，推动高分辨率手术成像研究。


<details>
  <summary>Details</summary>
Motivation: 高分辨率成像对微创手术重要，但缺乏适用于机器人辅助微创手术的公开原生4K数据集。

Method: 创建包含多样视觉场景、反映手术常见挑战的SurgiSR4K数据集。

Result: 推出SurgiSR4K数据集，可用于多种计算机视觉任务。

Conclusion: SurgiSR4K为高分辨率手术成像研究提供基础，促进智能成像技术发展以提升手术性能、安全性和可用性。

Abstract: High-resolution imaging is crucial for enhancing visual clarity and enabling
precise computer-assisted guidance in minimally invasive surgery (MIS). Despite
the increasing adoption of 4K endoscopic systems, there remains a significant
gap in publicly available native 4K datasets tailored specifically for
robotic-assisted MIS. We introduce SurgiSR4K, the first publicly accessible
surgical imaging and video dataset captured at a native 4K resolution,
representing realistic conditions of robotic-assisted procedures. SurgiSR4K
comprises diverse visual scenarios including specular reflections, tool
occlusions, bleeding, and soft tissue deformations, meticulously designed to
reflect common challenges faced during laparoscopic and robotic surgeries. This
dataset opens up possibilities for a broad range of computer vision tasks that
might benefit from high resolution data, such as super resolution (SR), smoke
removal, surgical instrument detection, 3D tissue reconstruction, monocular
depth estimation, instance segmentation, novel view synthesis, and
vision-language model (VLM) development. SurgiSR4K provides a robust foundation
for advancing research in high-resolution surgical imaging and fosters the
development of intelligent imaging technologies aimed at enhancing performance,
safety, and usability in image-guided robotic surgeries.

</details>


### [224] [Physics-Informed Neural ODEs for Temporal Dynamics Modeling in Cardiac T1 Mapping](https://arxiv.org/abs/2507.00613)
*Nuno Capitão,Yi Zhang,Yidong Zhao,Qian Tao*

Main category: eess.IV

TL;DR: 本文提出基于物理信息神经常微分方程的加速端到端T1映射框架，从稀疏图像子集实现高精度T1估计，实验显示性能优越。


<details>
  <summary>Details</summary>
Motivation: 传统T1映射扫描时间长、存在运动伪影，现有深度学习方法忽略物理约束，需改进。

Method: 开发连续时间LSTM - ODE模型，利用物理信息神经常微分方程建模，实现任意时间滞后的选择性Look - Locker数据采集。

Result: 在原生和对比后序列的T1估计中表现优越，基于物理的公式优于直接数据驱动的T1先验。

Conclusion: 所提框架解决了现有T1映射方法的问题，提高了T1估计的准确性和效率。

Abstract: Spin-lattice relaxation time ($T_1$) is an important biomarker in cardiac
parametric mapping for characterizing myocardial tissue and diagnosing
cardiomyopathies. Conventional Modified Look-Locker Inversion Recovery (MOLLI)
acquires 11 breath-hold baseline images with interleaved rest periods to ensure
mapping accuracy. However, prolonged scanning can be challenging for patients
with poor breathholds, often leading to motion artifacts that degrade image
quality. In addition, $T_1$ mapping requires voxel-wise nonlinear fitting to a
signal recovery model involving an iterative estimation process. Recent studies
have proposed deep-learning approaches for rapid $T_1$ mapping using shortened
sequences to reduce acquisition time for patient comfort. Nevertheless,
existing methods overlook important physics constraints, limiting
interpretability and generalization. In this work, we present an accelerated,
end-to-end $T_1$ mapping framework leveraging Physics-Informed Neural Ordinary
Differential Equations (ODEs) to model temporal dynamics and address these
challenges. Our method achieves high-accuracy $T_1$ estimation from a sparse
subset of baseline images and ensures efficient null index estimation at test
time. Specifically, we develop a continuous-time LSTM-ODE model to enable
selective Look-Locker (LL) data acquisition with arbitrary time lags.
Experimental results show superior performance in $T_1$ estimation for both
native and post-contrast sequences and demonstrate the strong benefit of our
physics-based formulation over direct data-driven $T_1$ priors.

</details>


### [225] [Medical Image Segmentation Using Advanced Unet: VMSE-Unet and VM-Unet CBAM+](https://arxiv.org/abs/2507.00511)
*Sayandeep Kanrar,Raja Piyush,Qaiser Razi,Debanshi Chakraborty,Vikas Hassija,GSS Chalapathi*

Main category: eess.IV

TL;DR: 提出VMSE U - Net和VM - Unet CBAM+模型提升医学图像分割，VMSE - Unet性能优，具临床应用潜力。


<details>
  <summary>Details</summary>
Motivation: 提升医学图像分割的准确性、特征定位能力和计算效率。

Method: 将Squeeze - and - Excitation (SE)和Convolutional Block Attention Module (CBAM)技术集成到传统VM U - Net框架。

Result: 两个模型在多数据集上优于基线VM - Unet，VMSE - Unet准确率、IoU、精度和召回率最高，损失低，计算效率高。

Conclusion: 增强架构VMSE - Unet是医学图像分析有价值工具，有临床应用潜力，需进一步研究优化。

Abstract: In this paper, we present the VMSE U-Net and VM-Unet CBAM+ model, two
cutting-edge deep learning architectures designed to enhance medical image
segmentation. Our approach integrates Squeeze-and-Excitation (SE) and
Convolutional Block Attention Module (CBAM) techniques into the traditional VM
U-Net framework, significantly improving segmentation accuracy, feature
localization, and computational efficiency. Both models show superior
performance compared to the baseline VM-Unet across multiple datasets. Notably,
VMSEUnet achieves the highest accuracy, IoU, precision, and recall while
maintaining low loss values. It also exhibits exceptional computational
efficiency with faster inference times and lower memory usage on both GPU and
CPU. Overall, the study suggests that the enhanced architecture VMSE-Unet is a
valuable tool for medical image analysis. These findings highlight its
potential for real-world clinical applications, emphasizing the importance of
further research to optimize accuracy, robustness, and computational
efficiency.

</details>


### [226] [MTCNet: Motion and Topology Consistency Guided Learning for Mitral Valve Segmentationin 4D Ultrasound](https://arxiv.org/abs/2507.00660)
*Rusi Chen,Yuanting Yang,Jiezhi Yao,Hongning Song,Ji Zhang,Yongsong Zhou,Yuhao Huang,Ronghao Yang,Dan Jia,Yuhan Zhang,Xing Tao,Haoran Dou,Qing Zhou,Xin Yang,Dong Ni*

Main category: eess.IV

TL;DR: 提出MTCNet用于半监督学习下的4D二尖瓣超声分割，在数据集上表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有4D二尖瓣分析因相位注释有限、运动伪影和成像质量差，且缺乏相间依赖而面临挑战。

Method: 设计跨相位运动引导的一致性学习策略，利用双向注意力记忆库传播时空特征；设计拓扑引导的相关性正则化方法探索物理先验知识。

Result: 在含160名患者1408个相位的数据集上，MTCNet跨相位一致性优于其他先进方法（Dice: 87.30%, HD: 1.75mm）。

Conclusion: MTCNet能有效利用标记和未标记相位间的结构对应关系，可准确进行4D二尖瓣超声分割。

Abstract: Mitral regurgitation is one of the most prevalent cardiac disorders.
Four-dimensional (4D) ultrasound has emerged as the primary imaging modality
for assessing dynamic valvular morphology. However, 4D mitral valve (MV)
analysis remains challenging due to limited phase annotations, severe motion
artifacts, and poor imaging quality. Yet, the absence of inter-phase dependency
in existing methods hinders 4D MV analysis. To bridge this gap, we propose a
Motion-Topology guided consistency network (MTCNet) for accurate 4D MV
ultrasound segmentation in semi-supervised learning (SSL). MTCNet requires only
sparse end-diastolic and end-systolic annotations. First, we design a
cross-phase motion-guided consistency learning strategy, utilizing a
bi-directional attention memory bank to propagate spatio-temporal features.
This enables MTCNet to achieve excellent performance both per- and inter-phase.
Second, we devise a novel topology-guided correlation regularization that
explores physical prior knowledge to maintain anatomically plausible.
Therefore, MTCNet can effectively leverage structural correspondence between
labeled and unlabeled phases. Extensive evaluations on the first largest 4D MV
dataset, with 1408 phases from 160 patients, show that MTCNet performs superior
cross-phase consistency compared to other advanced methods (Dice: 87.30%, HD:
1.75mm). Both the code and the dataset are available at
https://github.com/crs524/MTCNet.

</details>


### [227] [Automated anatomy-based post-processing reduces false positives and improved interpretability of deep learning intracranial aneurysm detection](https://arxiv.org/abs/2507.00832)
*Jisoo Kim,Chu-Hsuan Lin,Alberto Ceballos-Arroyo,Ping Liu,Huaizu Jiang,Shrikanth Yadav,Qi Wan,Lei Qin,Geoffrey S Young*

Main category: eess.IV

TL;DR: 本文提出基于解剖学的启发式学习混合动静脉分割后处理方法，以降低深度学习颅内动脉瘤检测模型的假阳性率，方法5效果最佳。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型检测颅内动脉瘤假阳性率高，阻碍临床应用，需进一步降低假阳性率。

Method: 训练CPM - Net和3D - CNN - TR两个深度学习模型，用多种分割掩码去除模型输出中可能的假阳性。

Result: 方法5效果最佳，CPM - Net假阳性率降低70.6%，3D - CNN - TR降低51.6%，且不减少真阳性。

Conclusion: 基于解剖学的可解释后处理可提升动脉瘤检测模型性能，自动化、领域相关的混合启发式学习处理有前景。

Abstract: Introduction: Deep learning (DL) models can help detect intracranial
aneurysms on CTA, but high false positive (FP) rates remain a barrier to
clinical translation, despite improvement in model architectures and strategies
like detection threshold tuning. We employed an automated, anatomy-based,
heuristic-learning hybrid artery-vein segmentation post-processing method to
further reduce FPs. Methods: Two DL models, CPM-Net and a deformable 3D
convolutional neural network-transformer hybrid (3D-CNN-TR), were trained with
1,186 open-source CTAs (1,373 annotated aneurysms), and evaluated with 143
held-out private CTAs (218 annotated aneurysms). Brain, artery, vein, and
cavernous venous sinus (CVS) segmentation masks were applied to remove possible
FPs in the DL outputs that overlapped with: (1) brain mask; (2) vein mask; (3)
vein more than artery masks; (4) brain plus vein mask; (5) brain plus vein more
than artery masks. Results: CPM-Net yielded 139 true-positives (TP); 79
false-negative (FN); 126 FP. 3D-CNN-TR yielded 179 TP; 39 FN; 182 FP. FPs were
commonly extracranial (CPM-Net 27.3%; 3D-CNN-TR 42.3%), venous (CPM-Net 56.3%;
3D-CNN-TR 29.1%), arterial (CPM-Net 11.9%; 3D-CNN-TR 53.3%), and non-vascular
(CPM-Net 25.4%; 3D-CNN-TR 9.3%) structures. Method 5 performed best, reducing
CPM-Net FP by 70.6% (89/126) and 3D-CNN-TR FP by 51.6% (94/182), without
reducing TP, lowering the FP/case rate from 0.88 to 0.26 for CPM-NET, and from
1.27 to 0.62 for the 3D-CNN-TR. Conclusion: Anatomy-based, interpretable
post-processing can improve DL-based aneurysm detection model performance. More
broadly, automated, domain-informed, hybrid heuristic-learning processing holds
promise for improving the performance and clinical acceptance of aneurysm
detection models.

</details>


### [228] [Deep learning-based segmentation of T1 and T2 cardiac MRI maps for automated disease detection](https://arxiv.org/abs/2507.00903)
*Andreea Bianca Popescu,Andreas Seitz,Heiko Mahrholdt,Jens Wetzl,Athira Jacob,Lucian Mihai Itu,Constantin Suciu,Teodora Chitiboi*

Main category: eess.IV

TL;DR: 研究评估深度学习用于T1/T2图分割及机器学习结合多特征用于疾病检测的效果，发现深度学习分割表现超观察者一致性，多特征结合机器学习提升疾病检测效果。


<details>
  <summary>Details</summary>
Motivation: 参数化组织映射受手动划分时观察者间差异限制，传统方法可能简化心肌复杂性，研究旨在评估深度学习分割准确性、统计特征效用及多特征机器学习对疾病检测的提升。

Method: 手动分割T1和T2图，评估观察者间差异；训练深度学习模型分割左心室血池和心肌；计算心肌像素统计特征用于分类；用多种指标评估分割和分类性能。

Result: 分割模型DICE达85.4%，超观察者一致性；随机森林应用于所有特征使F1分数提高到92.7%。

Conclusion: 深度学习有助于T1/T2图分割，多特征结合机器学习可改善疾病检测。

Abstract: Objectives Parametric tissue mapping enables quantitative cardiac tissue
characterization but is limited by inter-observer variability during manual
delineation. Traditional approaches relying on average relaxation values and
single cutoffs may oversimplify myocardial complexity. This study evaluates
whether deep learning (DL) can achieve segmentation accuracy comparable to
inter-observer variability, explores the utility of statistical features beyond
mean T1/T2 values, and assesses whether machine learning (ML) combining
multiple features enhances disease detection. Materials & Methods T1 and T2
maps were manually segmented. The test subset was independently annotated by
two observers, and inter-observer variability was assessed. A DL model was
trained to segment left ventricle blood pool and myocardium. Average (A), lower
quartile (LQ), median (M), and upper quartile (UQ) were computed for the
myocardial pixels and employed in classification by applying cutoffs or in ML.
Dice similarity coefficient (DICE) and mean absolute percentage error evaluated
segmentation performance. Bland-Altman plots assessed inter-user and
model-observer agreement. Receiver operating characteristic analysis determined
optimal cutoffs. Pearson correlation compared features from model and manual
segmentations. F1-score, precision, and recall evaluated classification
performance. Wilcoxon test assessed differences between classification methods,
with p < 0.05 considered statistically significant. Results 144 subjects were
split into training (100), validation (15) and evaluation (29) subsets.
Segmentation model achieved a DICE of 85.4%, surpassing inter-observer
agreement. Random forest applied to all features increased F1-score (92.7%, p <
0.001). Conclusion DL facilitates segmentation of T1/ T2 maps. Combining
multiple features with ML improves disease detection.

</details>


<div id='physics.app-ph'></div>

# physics.app-ph [[Back]](#toc)

### [229] [Inverse Design in Nanophotonics via Representation Learning](https://arxiv.org/abs/2507.00546)
*Reza Marzban,Ali Adibi,Raphael Pestourie*

Main category: physics.app-ph

TL;DR: 本文综述机器学习增强的纳米光子学逆向设计方法，分为输出端和输入端方法，还提及混合框架，最后指出挑战与机遇。


<details>
  <summary>Details</summary>
Motivation: 传统纳米光子学逆向设计方法在高维非凸设计空间和电磁模拟计算需求方面存在瓶颈，需新方法解决。

Method: 从表示学习角度对机器学习增强的逆向设计方法分类，分为输出端和输入端方法，还介绍混合框架。

Result: 明确两种方法各自在数据需求、泛化能力和新设计发现潜力方面的权衡，混合框架可避免局部最优等。

Conclusion: 强调复杂度管理、几何无关表示、集成制造约束和多物理协同设计等方面的挑战与机遇。

Abstract: Inverse design in nanophotonics, the computational discovery of structures
achieving targeted electromagnetic (EM) responses, has become a key tool for
recent optical advances. Traditional intuition-driven or iterative optimization
methods struggle with the inherently high-dimensional, non-convex design spaces
and the substantial computational demands of EM simulations. Recently, machine
learning (ML) has emerged to address these bottlenecks effectively. This review
frames ML-enhanced inverse design methodologies through the lens of
representation learning, classifying them into two categories: output-side and
input-side approaches. Output-side methods use ML to learn a representation in
the solution space to create a differentiable solver that accelerates
optimization. Conversely, input-side techniques employ ML to learn compact,
latent-space representations of feasible device geometries, enabling efficient
global exploration through generative models. Each strategy presents unique
trade-offs in data requirements, generalization capacity, and novel design
discovery potentials. Hybrid frameworks that combine physics-based optimization
with data-driven representations help escape poor local optima, improve
scalability, and facilitate knowledge transfer. We conclude by highlighting
open challenges and opportunities, emphasizing complexity management,
geometry-independent representations, integration of fabrication constraints,
and advancements in multiphysics co-designs.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [230] [The gradual transformation of inland countries -- human plowing, horse plowing and equity incentives](https://arxiv.org/abs/2507.00067)
*Hongfa Zi,Zhen Liu*

Main category: physics.soc-ph

TL;DR: 现代国家未吸取历史教训，难以迭代古文明，应从历史中学习提升文明综合实力，分析得出文明转型、统治者选择、经济发展等相关结论。


<details>
  <summary>Details</summary>
Motivation: 现代国家仅拥有现代技术，难以迭代古文明，且不知如何从历史中学习促进文明升级，需通过学习历史经验提升文明综合实力和生存能力。

Method: 先探寻各国在冲突中保持长期稳定的原因，再用数学方法论证现阶段如何达到最优解。

Result: 文明从人力耕作转变为马匹耕作易压制民众反抗并赋予其抵抗能力；统治者选择应考虑多种制度；经济发展呈对数正态分布，可用其最大价值划分股权调整财富差距。

Conclusion: 通过研究历史和运用数学方法，得出文明转型、统治者选择和经济发展等方面的可行结论，以促进文明升级和减少内部冲突。

Abstract: Many modern countries have not learned their lessons and often hope for the
wisdom of later generations, resulting in them only possessing modern
technology and difficult to iterate ancient civilizations. At present, there is
no way to tell how we should learn from history and promote the gradual
upgrading of civilization. Therefore, we must tell the history of
civilization's progress and the means of governance, learn from experience to
improve the comprehensive strength and survival ability of civilization, and
achieve an optimal solution for the tempering brought by conflicts and the
reduction of internal conflicts. Firstly, we must follow the footsteps of
history and explore the reasons for the long-term stability of each country in
conflict, including providing economic benefits to the people and means of
suppressing them; then, use mathematical methods to demonstrate how we can
achieve the optimal solution at the current stage. After analysis, we can
conclude that the civilization transformed from human plowing to horse plowing
can easily suppress the resistance of the people and provide them with the
ability to resist; The selection of rulers should consider multiple
institutional aspects, such as exams, elections, and drawing lots; Economic
development follows a lognormal distribution and can be adjusted by expected
value and variance. Using a lognormal distribution with the maximum value to
divide equity can adjust the wealth gap.

</details>


### [231] [How large language models judge and influence human cooperation](https://arxiv.org/abs/2507.00088)
*Alexandre S. Pires,Laurens Samson,Sennay Ghebreab,Fernando P. Santos*

Main category: physics.soc-ph

TL;DR: 研究评估21种大语言模型对合作行为的判断，用演化博弈论模型评估其对人类合作的长期影响，发现模型差异会影响合作，还测试提示以引导模型规范。


<details>
  <summary>Details</summary>
Motivation: 人类依赖大语言模型做社交决策，但基于大语言模型的社交决策的长期影响未知，需研究其对人类合作的影响。

Method: 给21种大语言模型提供不同社交场景中个体合作或拒绝合作的例子，让其判断；用演化博弈论模型评估以模型判断为主导时群体的合作动态；测试提示以引导模型规范。

Result: 评估与好对手合作时模型判断有显著一致性；判断与坏名声个体合作时模型有差异；模型差异会显著影响合作普遍性；目标导向提示能塑造模型判断。

Conclusion: 研究连接了大语言模型建议和长期社会动态，强调需仔细校准大语言模型规范以维护人类合作。

Abstract: Humans increasingly rely on large language models (LLMs) to support decisions
in social settings. Previous work suggests that such tools shape people's moral
and political judgements. However, the long-term implications of LLM-based
social decision-making remain unknown. How will human cooperation be affected
when the assessment of social interactions relies on language models? This is a
pressing question, as human cooperation is often driven by indirect
reciprocity, reputations, and the capacity to judge interactions of others.
Here, we assess how state-of-the-art LLMs judge cooperative actions. We provide
21 different LLMs with an extensive set of examples where individuals cooperate
-- or refuse cooperating -- in a range of social contexts, and ask how these
interactions should be judged. Furthermore, through an evolutionary
game-theoretical model, we evaluate cooperation dynamics in populations where
the extracted LLM-driven judgements prevail, assessing the long-term impact of
LLMs on human prosociality. We observe a remarkable agreement in evaluating
cooperation against good opponents. On the other hand, we notice within- and
between-model variance when judging cooperation with ill-reputed individuals.
We show that the differences revealed between models can significantly impact
the prevalence of cooperation. Finally, we test prompts to steer LLM norms,
showing that such interventions can shape LLM judgements, particularly through
goal-oriented prompts. Our research connects LLM-based advices and long-term
social dynamics, and highlights the need to carefully align LLM norms in order
to preserve human cooperation.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [232] [Twill: Scheduling Compound AI Systems on Heterogeneous Mobile Edge Platforms](https://arxiv.org/abs/2507.00491)
*Zain Taufique,Aman Vyas,Antonio Miele,Pasi Liljeberg,Anil Kanduri*

Main category: cs.MA

TL;DR: 本文提出Twill框架解决复合AI系统在异构移动边缘平台的调度挑战，减少推理延迟。


<details>
  <summary>Details</summary>
Motivation: 现有移动边缘AI推理策略无法处理复合AI系统中DNN和transformer的并发推理，需解决复合AI系统在异构移动边缘平台的调度挑战。

Method: 提出Twill运行时框架，通过任务亲和性感知的集群映射和迁移、优先级感知的任务冻结/解冻和DVFS来处理复合AI工作负载的并发推理请求。

Result: 在Nvidia Jetson Orin NX平台上实现并部署Twill框架，与现有边缘AI推理技术相比，平均减少54%的推理延迟，且符合功率预算。

Conclusion: Twill框架能有效解决复合AI系统在异构移动边缘平台的调度问题，减少推理延迟并满足功率要求。

Abstract: Compound AI (cAI) systems chain multiple AI models to solve complex problems.
cAI systems are typically composed of deep neural networks (DNNs),
transformers, and large language models (LLMs), exhibiting a high degree of
computational diversity and dynamic workload variation. Deploying cAI services
on mobile edge platforms poses a significant challenge in scheduling concurrent
DNN-transformer inference tasks, which arrive dynamically in an unknown
sequence. Existing mobile edge AI inference strategies manage multi-DNN or
transformer-only workloads, relying on design-time profiling, and cannot handle
concurrent inference of DNNs and transformers required by cAI systems. In this
work, we address the challenge of scheduling cAI systems on heterogeneous
mobile edge platforms. We present Twill, a run-time framework to handle
concurrent inference requests of cAI workloads through task affinity-aware
cluster mapping and migration, priority-aware task freezing/unfreezing, and
DVFS, while minimizing inference latency within power budgets. We implement and
deploy our Twill framework on the Nvidia Jetson Orin NX platform. We evaluate
Twill against state-of-the-art edge AI inference techniques over contemporary
DNNs and LLMs, reducing inference latency by 54% on average, while honoring
power budgets.

</details>


### [233] [State and Memory is All You Need for Robust and Reliable AI Agents](https://arxiv.org/abs/2507.00081)
*Matthew Muhoberac,Atharva Parikh,Nirvi Vakharia,Saniya Virani,Aco Radujevic,Savannah Wood,Meghav Verma,Dimitri Metaxotos,Jeyaraman Soundararajan,Thierry Masquelin,Alexander G. Godfrey,Sean Gardner,Dobrila Rudnicki,Sam Michael,Gaurav Chopra*

Main category: cs.MA

TL;DR: 介绍SciBORG框架让基于大语言模型的智能体执行科学任务，验证其在多场景的可靠性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在复杂现实科学工作流应用受内存、规划和工具集成挑战限制。

Method: 引入SciBORG模块化框架，动态构建智能体，用有限状态自动机增强内存。

Result: SciBORG智能体实现可靠执行、自适应规划和可解释状态转换。

Conclusion: 内存和状态感知对智能体规划和可靠性至关重要，为复杂环境部署AI智能体提供基础。

Abstract: Large language models (LLMs) have enabled powerful advances in natural
language understanding and generation. Yet their application to complex,
real-world scientific workflows remain limited by challenges in memory,
planning, and tool integration. Here, we introduce SciBORG (Scientific Bespoke
Artificial Intelligence Agents Optimized for Research Goals), a modular agentic
framework that allows LLM-based agents to autonomously plan, reason, and
achieve robust and reliable domain-specific task execution. Agents are
constructed dynamically from source code documentation and augmented with
finite-state automata (FSA) memory, enabling persistent state tracking and
context-aware decision-making. This approach eliminates the need for manual
prompt engineering and allows for robust, scalable deployment across diverse
applications via maintaining context across extended workflows and to recover
from tool or execution failures. We validate SciBORG through integration with
both physical and virtual hardware, such as microwave synthesizers for
executing user-specified reactions, with context-aware decision making and
demonstrate its use in autonomous multi-step bioassay retrieval from the
PubChem database utilizing multi-step planning, reasoning, agent-to-agent
communication and coordination for execution of exploratory tasks. Systematic
benchmarking shows that SciBORG agents achieve reliable execution, adaptive
planning, and interpretable state transitions. Our results show that memory and
state awareness are critical enablers of agentic planning and reliability,
offering a generalizable foundation for deploying AI agents in complex
environments.

</details>


### [234] [Large Language Model Powered Intelligent Urban Agents: Concepts, Capabilities, and Applications](https://arxiv.org/abs/2507.00914)
*Jindong Han,Yansong Ning,Zirui Yuan,Hang Ni,Fan Liu,Tengfei Lyu,Hao Liu*

Main category: cs.MA

TL;DR: 文章聚焦城市大语言模型代理，介绍概念、研究现状、应用领域，探讨部署问题和未来研究方向，为该领域奠定基础并提供路线图。


<details>
  <summary>Details</summary>
Motivation: 借助大语言模型强大能力实现智慧城市愿景，开展城市大语言模型代理研究。

Method: 先介绍城市大语言模型代理概念，再从工作流程视角调研研究现状，对应用领域分类并展示代表工作，最后讨论部署问题和未来研究方向。

Result: 梳理城市大语言模型代理研究，明确应用领域，提出部署关键问题和未来研究方向。

Conclusion: 为城市大语言模型代理新兴领域奠定基础，提供推进大语言模型与城市智能交叉领域研究的路线图。

Abstract: The long-standing vision of intelligent cities is to create efficient,
livable, and sustainable urban environments using big data and artificial
intelligence technologies. Recently, the advent of Large Language Models (LLMs)
has opened new ways toward realizing this vision. With powerful semantic
understanding and reasoning capabilities, LLMs can be deployed as intelligent
agents capable of autonomously solving complex problems across domains. In this
article, we focus on Urban LLM Agents, which are LLM-powered agents that are
semi-embodied within the hybrid cyber-physical-social space of cities and used
for system-level urban decision-making. First, we introduce the concept of
urban LLM agents, discussing their unique capabilities and features. Second, we
survey the current research landscape from the perspective of agent workflows,
encompassing urban sensing, memory management, reasoning, execution, and
learning. Third, we categorize the application domains of urban LLM agents into
five groups: urban planning, transportation, environment, public safety, and
urban society, presenting representative works in each group. Finally, we
discuss trustworthiness and evaluation issues that are critical for real-world
deployment, and identify several open problems for future research. This survey
aims to establish a foundation for the emerging field of urban LLM agents and
to provide a roadmap for advancing the intersection of LLMs and urban
intelligence. A curated list of relevant papers and open-source resources is
maintained and continuously updated at
https://github.com/usail-hkust/Awesome-Urban-LLM-Agents.

</details>


<div id='math.DG'></div>

# math.DG [[Back]](#toc)

### [235] [Geometric Gaussian Approximations of Probability Distributions](https://arxiv.org/abs/2507.00616)
*Nathaël Da Costa,Bálint Mucsányi,Philipp Hennig*

Main category: math.DG

TL;DR: 研究几何高斯近似的表达能力，证明其通用性并探讨统一近似问题


<details>
  <summary>Details</summary>
Motivation: 在许多应用中，近似复杂概率分布是核心问题，因此研究几何高斯近似的表达能力

Method: 先回顾两种几何高斯近似，探究它们的关系，再给出构造性证明，最后讨论统一近似问题

Result: 证明了几何高斯近似是通用的，可捕获任何概率分布

Conclusion: 探讨了对于给定概率分布族，能否找到共同微分同胚以获得统一高质量几何高斯近似的问题

Abstract: Approximating complex probability distributions, such as Bayesian posterior
distributions, is of central interest in many applications. We study the
expressivity of geometric Gaussian approximations. These consist of
approximations by Gaussian pushforwards through diffeomorphisms or Riemannian
exponential maps. We first review these two different kinds of geometric
Gaussian approximations. Then we explore their relationship to one another. We
further provide a constructive proof that such geometric Gaussian
approximations are universal, in that they can capture any probability
distribution. Finally, we discuss whether, given a family of probability
distributions, a common diffeomorphism can be found to obtain uniformly
high-quality geometric Gaussian approximations for that family.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [236] [Modeling Data Diversity for Joint Instance and Verbalizer Selection in Cold-Start Scenarios](https://arxiv.org/abs/2507.00330)
*Mohna Chakraborty,Adithya Kulkarni,Qi Li*

Main category: cs.CL

TL;DR: 提出 COLDSELECT 方法用于冷启动场景下的语言模型实例和 verbalizer 选择，实验证明其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有基于提示的方法在冷启动无标注数据场景下对模板、verbalizer 和实例选择敏感，且现有研究忽略实例和 verbalizer 之间的依赖。

Method: 提出 COLDSELECT 方法，将 PLM 词汇和 $h_{[MASK]}$ 嵌入映射到共享空间，进行降维和聚类以确保高效多样选择，优化目标为最小化不确定性和最大化多样性。

Result: 在八个基准测试中，COLDSELECT 在减少不确定性和增强泛化能力方面表现优越，在冷启动场景的 verbalizer 和少样本实例选择上优于基线方法。

Conclusion: COLDSELECT 能有效捕捉数据关系，适用于冷启动场景下的实例和 verbalizer 选择。

Abstract: Prompt-based methods leverage the knowledge of pre-trained language models
(PLMs) trained with a masked language modeling (MLM) objective; however, these
methods are sensitive to template, verbalizer, and few-shot instance selection,
particularly in cold-start settings with no labeled data. Existing studies
overlook the dependency between instances and verbalizers, where instance-label
probabilities depend on verbalizer token proximity in the embedding space. To
address this, we propose COLDSELECT, a joint verbalizer and instance selection
approach that models data diversity. COLDSELECT maps PLM vocabulary and
$h_{[MASK]}$ embeddings into a shared space, applying dimensionality reduction
and clustering to ensure efficient and diverse selection. By optimizing for
minimal uncertainty and maximal diversity, COLDSELECT captures data
relationships effectively. Experiments on eight benchmarks demonstrate
COLDSELECT's superiority in reducing uncertainty and enhancing generalization,
outperforming baselines in verbalizer and few-shot instance selection for
cold-start scenarios.

</details>


### [237] [Failure by Interference: Language Models Make Balanced Parentheses Errors When Faulty Mechanisms Overshadow Sound Ones](https://arxiv.org/abs/2507.00322)
*Daking Rai,Samuel Miller,Kevin Moran,Ziyu Yao*

Main category: cs.CL

TL;DR: 研究语言模型在生成平衡括号等简单句法任务上的错误机制，提出RASteer方法改善性能。


<details>
  <summary>Details</summary>
Motivation: 尽管语言模型编码能力进步显著，但在简单句法任务上仍有错误，需研究机制并减轻错误。

Method: 研究不同大小语言模型，发现可靠和不可靠组件，提出RASteer方法识别并增加可靠组件贡献。

Result: RASteer大幅提升平衡括号任务性能，部分模型准确率从0%提升到近100%，不损害通用编码能力，在算术推理任务也有高达约20%的性能提升。

Conclusion: RASteer方法能有效改善语言模型在特定任务上的性能，具有更广泛的适用性。

Abstract: Despite remarkable advances in coding capabilities, language models (LMs)
still struggle with simple syntactic tasks such as generating balanced
parentheses. In this study, we investigate the underlying mechanisms behind the
persistence of these errors across LMs of varying sizes (124M-7B) to both
understand and mitigate the errors. Our study reveals that LMs rely on a number
of components (attention heads and FF neurons) that independently make their
own predictions. While some components reliably promote correct answers across
a generalized range of inputs (i.e., implementing "sound mechanisms''), others
are less reliable and introduce noise by promoting incorrect tokens (i.e.,
implementing "faulty mechanisms''). Errors occur when the faulty mechanisms
overshadow the sound ones and dominantly affect the predictions. Motivated by
this insight, we introduce RASteer, a steering method to systematically
identify and increase the contribution of reliable components for improving
model performance. RASteer substantially improves performance on balanced
parentheses tasks, boosting accuracy of some models from $0$% to around $100$%
without impairing the models' general coding ability. We further demonstrate
its broader applicability in arithmetic reasoning tasks, achieving performance
gains of up to around $20$%.

</details>


### [238] [Two-Stage Reasoning-Infused Learning: Improving Classification with LLM-Generated Reasoning](https://arxiv.org/abs/2507.00214)
*Mads Henrichsen,Rasmus Krebs*

Main category: cs.CL

TL;DR: 本文提出利用大语言模型生成推理来增强文本分类的两阶段方法，在情感分类任务中取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 标准分类模型缺乏显式推理，限制性能、鲁棒性和可解释性，因此引入新方法改进。

Method: 第一阶段在通用推理数据集上微调Llama - 3.2 - 1B - Instruct模型生成推理；第二阶段用微调后的模型创建增强训练数据集，训练下游生成模型输出推理和预测情感。

Result: 输出推理和情感的生成模型在情感预测准确率上比仅输出情感的基线模型提高8.7个百分点。

Conclusion: 大语言模型生成的推理可创建更丰富训练数据集，提升下游NLP任务性能并提供显式解释。

Abstract: Standard classification models often map inputs directly to labels without
explicit reasoning, potentially limiting their performance, robustness, and
interpretability. This paper introduces a novel two-stage approach to enhance
text classification by leveraging Large Language Model (LLM)-generated
reasonings. In the first stage, we fine-tune a Llama-3.2-1B-Instruct model
(henceforth Llama-R-Gen) on a general-purpose reasoning dataset
(syvai/reasoning-gen) to generate textual reasoning (R) given a question and
its answer. In the second stage, this generally trained Llama-R-Gen is used
offline to create an augmented training dataset for a downstream generative
model. This downstream model, based on Llama-3.2-1B-Instruct, takes only the
input text (Q) and is trained to output the generated reasoning (R) immediately
followed by the predicted emotion (A). We demonstrate this methodology on the
dair-ai/emotion dataset for emotion classification. Our experiments show that
the generative model trained to output reasoning and the emotion (Classifier
Q->RA) achieves a significant improvement of 8.7 percentage points in accuracy
(for emotion prediction) compared to a baseline generative model trained solely
to output the emotion (Classifier Q->A), highlighting the strong generalization
capabilities of the reasoning generation and the benefit of explicit reasoning
training. This work underscores the potential of LLM-generated reasonings for
creating richer training datasets, thereby improving the performance of diverse
downstream NLP tasks and providing explicit explanations.

</details>


### [239] [Linearly Decoding Refused Knowledge in Aligned Language Models](https://arxiv.org/abs/2507.00239)
*Aryan Shrivastava,Ari Holtzman*

Main category: cs.CL

TL;DR: 研究使用线性探针从语言模型隐藏状态解码越狱提示信息的程度，发现大量初始拒绝信息可线性解码，且基础模型探针有时能迁移到指令调优版本，表明指令调优未完全消除有害信息。


<details>
  <summary>Details</summary>
Motivation: 探究通过越狱提示获取的信息能否使用基于语言模型隐藏状态训练的线性探针进行解码，以及指令调优是否真正消除了有害信息。

Method: 使用线性探针在语言模型隐藏状态上进行训练，研究其对越狱提示信息的解码能力。

Result: 大量初始拒绝信息可线性解码，基础模型的探针有时能迁移到指令调优版本，探针预测值与语言模型生成的成对比较相关。

Conclusion: 指令调优并未完全消除或重新定位表示空间中的有害信息，只是抑制了其直接表达，有害信息仍可线性访问并在下游行为中间接产生影响。

Abstract: Most commonly used language models (LMs) are instruction-tuned and aligned
using a combination of fine-tuning and reinforcement learning, causing them to
refuse users requests deemed harmful by the model. However, jailbreak prompts
can often bypass these refusal mechanisms and elicit harmful responses. In this
work, we study the extent to which information accessed via jailbreak prompts
is decodable using linear probes trained on LM hidden states. We show that a
great deal of initially refused information is linearly decodable. For example,
across models, the response of a jailbroken LM for the average IQ of a country
can be predicted by a linear probe with Pearson correlations exceeding $0.8$.
Surprisingly, we find that probes trained on base models (which do not refuse)
sometimes transfer to their instruction-tuned versions and are capable of
revealing information that jailbreaks decode generatively, suggesting that the
internal representations of many refused properties persist from base LMs
through instruction-tuning. Importantly, we show that this information is not
merely "leftover" in instruction-tuned models, but is actively used by them: we
find that probe-predicted values correlate with LM generated pairwise
comparisons, indicating that the information decoded by our probes align with
suppressed generative behavior that may be expressed more subtly in other
downstream tasks. Overall, our results suggest that instruction-tuning does not
wholly eliminate or even relocate harmful information in representation
space-they merely suppress its direct expression, leaving it both linearly
accessible and indirectly influential in downstream behavior.

</details>


### [240] [Impact of Fine-Tuning Methods on Memorization in Large Language Models](https://arxiv.org/abs/2507.00258)
*Jie Hou,Chuxiong Wu,Lannan Luo,Qiang Zeng*

Main category: cs.CL

TL;DR: 研究不同大语言模型微调方法的隐私风险，发现基于提示的微调更具隐私保护优势。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型微调范式中，微调时记忆带来的隐私风险受关注较少，需进行研究。

Method: 对流行的微调方法分类，并通过成员推理攻击评估它们对记忆的影响。

Result: 与基于参数的微调相比，基于提示的微调性能相当，且对成员推理攻击的脆弱性更低，无论模型规模如何，基于提示的方法记忆程度都低。

Conclusion: 基于参数的微调更易泄露隐私信息，基于提示的微调是更具隐私保护的选择。

Abstract: As the capabilities of pre-trained large language models (LLMs) continue to
advance, the "pre-train and fine-tune" paradigm has become increasingly
mainstream, leading to the development of various fine-tuning methods. However,
the privacy risks arising from memorization during fine-tuning have received
relatively little attention. To address this gap, we categorize popular
fine-tuning approaches and assess their impact on memorization through the lens
of membership inference attacks (MIAs). Our results show that, compared to
parameter-based fine-tuning, prompt-based fine-tuning achieves competitive
performance while exhibiting lower vulnerability to MIAs. Furthermore,
prompt-based methods maintain low memorization regardless of model scale. These
findings suggest that parameter-based fine-tuning is more prone to leaking
private information, whereas prompt-based fine-tuning serves as a more
privacy-preserving option.

</details>


### [241] [Natural language processing for African languages](https://arxiv.org/abs/2507.00297)
*David Ifeoluwa Adelani*

Main category: cs.CL

TL;DR: 本文聚焦撒哈拉以南非洲低资源语言，分析公开语料噪声、构建高质量语料，研究词嵌入局限性与多语言预训练模型机会，适配模型到非洲语言，开发标注数据集并进行评估。


<details>
  <summary>Details</summary>
Motivation: 当前多语言模型在低资源语言上存在问题，非洲本土语言多为低资源语言，在NLP研究中代表性不足。

Method: 分析公开语料噪声，构建高质量语料，研究词嵌入和多语言预训练模型，用少量单语文本适配模型到非洲语言，开发21种非洲语言的标注数据集，采用多种学习设置进行评估。

Result: 证明词嵌入语义表示质量取决于预训练数据质量，展示多语言预训练模型的机会，开发标注数据集。

Conclusion: 通过一系列研究和实践，为解决非洲语言在NLP研究中的代表性不足问题提供了有效方法。

Abstract: Recent advances in word embeddings and language models use large-scale,
unlabelled data and self-supervised learning to boost NLP performance.
Multilingual models, often trained on web-sourced data like Wikipedia, face
challenges: few low-resource languages are included, their data is often noisy,
and lack of labeled datasets makes it hard to evaluate performance outside
high-resource languages like English. In this dissertation, we focus on
languages spoken in Sub-Saharan Africa where all the indigenous languages in
this region can be regarded as low-resourced in terms of the availability of
labelled data for NLP tasks and unlabelled data found on the web. We analyse
the noise in the publicly available corpora, and curate a high-quality corpus,
demonstrating that the quality of semantic representations learned in word
embeddings does not only depend on the amount of data but on the quality of
pre-training data. We demonstrate empirically the limitations of word
embeddings, and the opportunities the multilingual pre-trained language model
(PLM) offers especially for languages unseen during pre-training and
low-resource scenarios. We further study how to adapt and specialize
multilingual PLMs to unseen African languages using a small amount of
monolingual texts. To address the under-representation of the African languages
in NLP research, we developed large scale human-annotated labelled datasets for
21 African languages in two impactful NLP tasks: named entity recognition and
machine translation. We conduct an extensive empirical evaluation using
state-of-the-art methods across supervised, weakly-supervised, and transfer
learning settings.

</details>


### [242] [TeamCMU at Touché: Adversarial Co-Evolution for Advertisement Integration and Detection in Conversational Search](https://arxiv.org/abs/2507.00509)
*To Eun Kim,João Coelho,Gbemileke Onilude,Jai Singh*

Main category: cs.CL

TL;DR: 本文针对基于RAG的对话系统，提出广告管理模块化管道，包括广告重写器和分类器，通过实验证明该方法能提升广告检测和融入效果，贡献了对抗性共同进化框架。


<details>
  <summary>Details</summary>
Motivation: 随着基于大语言模型和RAG的对话搜索引擎兴起，生成式系统在广告融入方面存在透明度和信任问题，需解决广告管理难题。

Method: 提出模块化管道，含广告重写器和分类器；用合成数据训练分类器；采用监督微调广告重写器和最佳N采样两种广告融入策略。

Result: 基于合成数据和课程学习训练的广告分类器有稳健检测性能；分类器引导的优化显著提升广告融入的隐蔽性。

Conclusion: 研究成果为开发更复杂的广告感知生成式搜索系统和稳健广告分类器提供了对抗性共同进化框架。

Abstract: As conversational search engines increasingly adopt generation-based
paradigms powered by Large Language Models (LLMs) and Retrieval-Augmented
Generation (RAG), the integration of advertisements into generated responses
presents both commercial opportunities and challenges for user experience.
Unlike traditional search, where advertisements are clearly delineated,
generative systems blur the boundary between informational content and
promotional material, raising concerns around transparency and trust. In this
work, we propose a modular pipeline for advertisement management in RAG-based
conversational systems, consisting of an ad-rewriter for seamless ad
integration and a robust ad-classifier for detection. We leverage synthetic
data to train high-performing classifiers, which are then used to guide two
complementary ad-integration strategies: supervised fine-tuning of the
ad-rewriter and a best-of-N sampling approach that selects the least detectable
ad-integrated response among multiple candidates. Our evaluation focuses on two
core questions: the effectiveness of ad classifiers in detecting diverse ad
integration strategies, and the training methods that best support coherent,
minimally intrusive ad insertion. Experimental results show that our
ad-classifier, trained on synthetic advertisement data inspired by marketing
strategies and enhanced through curriculum learning, achieves robust detection
performance. Additionally, we demonstrate that classifier-guided optimization,
through both fine-tuning and best-of-N sampling, significantly improves ad
stealth, enabling more seamless integration. These findings contribute an
adversarial co-evolution framework for developing more sophisticated ad-aware
generative search systems and robust ad classifiers.

</details>


### [243] [TUM-MiKaNi at SemEval-2025 Task 3: Towards Multilingual and Knowledge-Aware Non-factual Hallucination Identification](https://arxiv.org/abs/2507.00579)
*Miriam Anschütz,Ekaterina Gikalo,Niklas Herbster,Georg Groh*

Main category: cs.CL

TL;DR: 本文介绍SemEval - 2025 Task - 3的提交方案，提出两部分管道系统，在多语言上取得有竞争力结果，能助力提升大语言模型输出。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型幻觉问题研究多聚焦英文数据，忽视多语言特性，需解决多语言幻觉问题。

Method: 提出两部分管道，结合基于维基百科的检索式事实验证和基于BERT微调识别常见幻觉模式的系统。

Result: 系统在所有语言上取得有竞争力结果，在八种语言中进入前十，支持共享任务涵盖的十四种语言之外的多种语言。

Conclusion: 该多语言幻觉识别器有助于未来提升大语言模型输出及其实用性。

Abstract: Hallucinations are one of the major problems of LLMs, hindering their
trustworthiness and deployment to wider use cases. However, most of the
research on hallucinations focuses on English data, neglecting the multilingual
nature of LLMs. This paper describes our submission to the SemEval-2025 Task-3
- Mu-SHROOM, the Multilingual Shared-task on Hallucinations and Related
Observable Overgeneration Mistakes. We propose a two-part pipeline that
combines retrieval-based fact verification against Wikipedia with a BERT-based
system fine-tuned to identify common hallucination patterns. Our system
achieves competitive results across all languages, reaching top-10 results in
eight languages, including English. Moreover, it supports multiple languages
beyond the fourteen covered by the shared task. This multilingual hallucination
identifier can help to improve LLM outputs and their usefulness in the future.

</details>


### [244] [Mixture of Reasonings: Teach Large Language Models to Reason with Adaptive Strategies](https://arxiv.org/abs/2507.00606)
*Tao Xiong,Xavier Hu,Wenyan Fan,Shengyu Zhang*

Main category: cs.CL

TL;DR: 传统大语言模型使用手动提示方法有局限，本文提出MoR训练框架，实验显示其提升性能，无需特定提示。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型依赖手动特定提示，导致适应性和效率受限的问题。

Method: 提出MoR训练框架，分思想生成和SFT数据集构建两个阶段。

Result: MoR显著提升性能，MoR150在CoT提示下提升2.2%，相比基线提升13.5%。

Conclusion: MoR无需特定提示，可提供跨任务的通用推理解决方案。

Abstract: Large language models (LLMs) excel in complex tasks through advanced
prompting techniques like Chain-of-Thought (CoT) and Tree-of-Thought (ToT), but
their reliance on manually crafted, task-specific prompts limits adaptability
and efficiency. We introduce Mixture of Reasoning (MoR), a training framework
that embeds diverse reasoning strategies into LLMs for autonomous,
task-adaptive reasoning without external prompt engineering. MoR has two
phases: Thought Generation, creating reasoning chain templates with models like
GPT-4o, and SFT Dataset Construction, pairing templates with benchmark datasets
for supervised fine-tuning.Our experiments show that MoR significantly enhances
performance, with MoR150 achieving 0.730 (2.2% improvement) using CoT prompting
and 0.734 (13.5% improvement) compared to baselines. MoR eliminates the need
for task-specific prompts, offering a generalizable solution for robust
reasoning across diverse tasks.

</details>


### [245] [SAFER: Probing Safety in Reward Models with Sparse Autoencoder](https://arxiv.org/abs/2507.00665)
*Sihang Li,Wei Shi,Ziyuan Xie,Tao Liang,Guojun Ma,Xiang Wang*

Main category: cs.CL

TL;DR: 提出SAFER框架，利用稀疏自编码器解释和改进奖励模型，实验表明能精准调控安全对齐且不影响通用聊天性能。


<details>
  <summary>Details</summary>
Motivation: 强化学习中奖励模型不透明，需方法来解释和改进。

Method: 提出SAFER框架，利用稀疏自编码器挖掘奖励模型激活中的可解释特征，量化特征显著性，设计数据中毒和去噪策略。

Result: SAFER可在少量数据修改下精确降低或增强安全对齐，不牺牲通用聊天性能。

Conclusion: 该方法有助于高风险大语言模型对齐任务中奖励模型的解释、审计和改进。

Abstract: Reinforcement learning from human feedback (RLHF) is a key paradigm for
aligning large language models (LLMs) with human values, yet the reward models
at its core remain largely opaque. In this work, we present sparse Autoencoder
For Enhanced Reward model (\textbf{SAFER}), a novel framework for interpreting
and improving reward models through mechanistic analysis. Leveraging Sparse
Autoencoders (SAEs), we uncover human-interpretable features in reward model
activations, enabling insight into safety-relevant decision-making. We apply
SAFER to safety-oriented preference datasets and quantify the salience of
individual features by activation differences between chosen and rejected
responses. Using these feature-level signals, we design targeted data poisoning
and denoising strategies. Experiments show that SAFER can precisely degrade or
enhance safety alignment with minimal data modification, without sacrificing
general chat performance. Our approach contributes to interpreting, auditing
and refining reward models in high-stakes LLM alignment tasks. Our codes are
available at https://github.com/xzy-101/SAFER-code. \textit{This paper
discusses topics related to large language model safety and may include
discussions or examples that highlight potential risks or unsafe outcomes.}

</details>


### [246] [LitBench: A Benchmark and Dataset for Reliable Evaluation of Creative Writing](https://arxiv.org/abs/2507.00769)
*Daniel Fein,Sebastian Russo,Violet Xiang,Kabir Jolly,Rafael Rafailov,Nick Haber*

Main category: cs.CL

TL;DR: 本文引入创意写作验证基准LitBench，对零样本大模型评判者进行基准测试，训练奖励模型并通过人类研究验证，发现Claude-3.7-Sonnet是最强OTS评判者，训练的奖励模型表现更优，最后发布LitBench和奖励模型。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型创意写作评估缺乏有效自动化方法，OTS语言模型可靠性不明，需要进行可靠评估。

Method: 引入LitBench基准和数据集，对零样本LLM评判者进行基准测试，训练奖励模型并开展在线人类研究。

Result: Claude-3.7-Sonnet是最强OTS评判者，训练的奖励模型准确率达78%，优于OTS评判者，在线人类研究证实训练的奖励模型与人类偏好一致。

Conclusion: 发布LitBench和奖励模型，为创意写作系统的自动化评估和优化提供可靠资源。

Abstract: Evaluating creative writing generated by large language models (LLMs) remains
challenging because open-ended narratives lack ground truths. Without
performant automated evaluation methods, off-the-shelf (OTS) language models
are employed as zero-shot judges, yet their reliability is unclear in this
context. In pursuit of robust evaluation for creative writing, we introduce
LitBench, the first standardized benchmark and paired dataset for creative
writing verification, comprising a held-out test set of 2,480 debiased,
human-labeled story comparisons drawn from Reddit and a 43,827-pair training
corpus of human preference labels. Using LitBench, we (i) benchmark zero-shot
LLM judges, (ii) train Bradley Terry and generative reward models, and (iii)
conduct an online human study to validate reward model rankings on newly
LLM-generated stories. Our benchmark identifies Claude-3.7-Sonnet as the
strongest off-the-shelf judge, reaching 73% agreement with human preferences;
among trained reward models, Bradley-Terry and Generative reward models both
attain an accuracy of 78%, outperforming all off-the-shelf judges. An online
human study further confirms that our trained reward models consistently align
with human preferences in novel LLM-generated stories. We release LitBench and
reward models at
https://huggingface.co/collections/SAA-Lab/litbench-68267b5da3aafe58f9e43461,
providing a vetted resource for reliable, automated evaluation and optimization
of creative writing systems.

</details>


### [247] [Many LLMs Are More Utilitarian Than One](https://arxiv.org/abs/2507.00814)
*Anita Keshmirian,Razan Baltaji,Babak Hemmatian,Hadi Asghari,Lav R. Varshney*

Main category: cs.CL

TL;DR: 研究多智能体大语言模型在道德判断中的群体效应，发现其与人类有表面相似但驱动机制不同。


<details>
  <summary>Details</summary>
Motivation: 随着多智能体系统发展，需了解大语言模型在协作时的道德判断机制，对比个体与群体表现，并探究是否有类似人类群体的功利主义提升。

Method: 对六个模型在两种条件下（单独推理和群体讨论）进行道德困境测试。

Result: 在个人道德困境中，所有模型在群体中比个体时更接受道德违规，部分模型倾向最大化整体福祉，但人类和模型的功利主义提升机制不同。

Conclusion: 大语言模型集体的表面行为模仿人类群体推理，但潜在驱动因素不同，需探讨对AI对齐等方面的影响。

Abstract: Moral judgment is integral to large language model (LLM) alignment and social
reasoning. As multi-agent systems gain prominence, it becomes crucial to
understand how LLMs function collectively during collaboration, compared to
individual agents. In human moral judgment, group deliberation leads to a
utilitarian boost: a tendency to endorse norm violations that maximize benefits
for the greatest number of people despite harms. We study whether a similar
dynamic emerges in multi-agent LLM systems. We tested six models on
well-established sets of moral dilemmas across two conditions: (1) Solo, where
models reasoned independently, and (2) Group, where they engaged in multi-turn
discussions in pairs or triads. In personal moral dilemmas, where agents must
decide to directly harm one individual to maximize the utility for others, all
models found moral violations to be more acceptable when part of a group than
individually, similar to human experiments. Some models endorsed actions that
maximized overall well-being, even if they benefited strangers over familiar
individuals. Others became more willing to violate moral norms in groups.
However, while human groups show a similar action bias, the mechanism for their
utilitarian boost differs from LLMs. Whereas the human shift comes from
heightened sensitivity to decision outcomes, LLM groups show either reduced
norm sensitivity or enhanced impartiality. This suggests that while the surface
behavior of LLM collectives mimics human group reasoning, the underlying
drivers differ. We discuss the implications for AI alignment, multi-agent
design, and artificial moral reasoning.

</details>


### [248] [Stylometry recognizes human and LLM-generated texts in short samples](https://arxiv.org/abs/2507.00838)
*Karol Przystalski,Jan K. Argasiński,Iwona Grabska-Gradzińska,Jeremi K. Ochab*

Main category: cs.CL

TL;DR: 本文探索利用文体ometry区分大语言模型和人类生成的文本，创建基准数据集并使用树模型分类，取得较好结果，证明可区分特定文本类型的机器和人类文本。


<details>
  <summary>Details</summary>
Motivation: 解决模型归属、知识产权和伦理AI使用问题，识别大语言模型文本的写作模式。

Method: 创建基于维基百科的基准数据集，用树模型（决策树和LightGBM）结合人类设计和n-gram的文体特征对文本分类。

Result: 多分类场景马修斯相关系数达0.87，二分类准确率在0.79 - 1之间，维基百科和GPT - 4在平衡数据集上准确率达0.98。

Conclusion: 在大语言模型日益复杂的背景下，至少能区分特定文本类型的机器和人类生成的文本。

Abstract: The paper explores stylometry as a method to distinguish between texts
created by Large Language Models (LLMs) and humans, addressing issues of model
attribution, intellectual property, and ethical AI use. Stylometry has been
used extensively to characterise the style and attribute authorship of texts.
By applying it to LLM-generated texts, we identify their emergent writing
patterns. The paper involves creating a benchmark dataset based on Wikipedia,
with (a) human-written term summaries, (b) texts generated purely by LLMs
(GPT-3.5/4, LLaMa 2/3, Orca, and Falcon), (c) processed through multiple text
summarisation methods (T5, BART, Gensim, and Sumy), and (d) rephrasing methods
(Dipper, T5). The 10-sentence long texts were classified by tree-based models
(decision trees and LightGBM) using human-designed (StyloMetrix) and
n-gram-based (our own pipeline) stylometric features that encode lexical,
grammatical, syntactic, and punctuation patterns. The cross-validated results
reached a performance of up to .87 Matthews correlation coefficient in the
multiclass scenario with 7 classes, and accuracy between .79 and 1. in binary
classification, with the particular example of Wikipedia and GPT-4 reaching up
to .98 accuracy on a balanced dataset. Shapley Additive Explanations pinpointed
features characteristic of the encyclopaedic text type, individual overused
words, as well as a greater grammatical standardisation of LLMs with respect to
human-written texts. These results show -- crucially, in the context of the
increasingly sophisticated LLMs -- that it is possible to distinguish machine-
from human-generated texts at least for a well-defined text type.

</details>


### [249] [Scaling Laws Are Unreliable for Downstream Tasks: A Reality Check](https://arxiv.org/abs/2507.00885)
*Nicholas Lourie,Michael Y. Hu,Kyunghyun Cho*

Main category: cs.CL

TL;DR: 对下游缩放定律现有数据进行元分析，发现接近线性缩放定律情况仅占39%，实验设置变化会改变缩放趋势，需理解缩放定律成功条件。


<details>
  <summary>Details</summary>
Motivation: 明确下游缩放定律从较小规模预训练损失预测大规模任务性能的可能性，解决相关争议。

Method: 对下游缩放定律现有数据进行元分析。

Result: 仅39%的情况接近线性缩放定律，实验设置的细微变化会完全改变缩放趋势。

Conclusion: 需要理解缩放定律成功的条件，建模预训练损失和下游任务性能关系时要考虑偏离线性趋势的情况。

Abstract: Downstream scaling laws aim to predict task performance at larger scales from
pretraining losses at smaller scales. Whether this prediction should be
possible is unclear: some works demonstrate that task performance follows clear
linear scaling trends under transformation, whereas others point out
fundamental challenges to downstream scaling laws, such as emergence and
inverse scaling. In this work, we conduct a meta-analysis of existing data on
downstream scaling laws, finding that close fit to linear scaling laws only
occurs in a minority of cases: 39% of the time. Furthermore, seemingly benign
changes to the experimental setting can completely change the scaling trend.
Our analysis underscores the need to understand the conditions under which
scaling laws succeed. To fully model the relationship between pretraining loss
and downstream task performance, we must embrace the cases in which scaling
behavior deviates from linear trends.

</details>


### [250] [MemeCMD: An Automatically Generated Chinese Multi-turn Dialogue Dataset with Contextually Retrieved Memes](https://arxiv.org/abs/2507.00891)
*Yuheng Wang,Xianhe Tang,Pufeng Huang*

Main category: cs.CL

TL;DR: 介绍自动生成的含上下文检索表情包的中文多轮对话数据集MemeCMD，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有对话数据集多为手动标注或纯文本对话，缺乏多模态交互的表现力和上下文细微差别。

Method: 结合大规模、大语言模型标注的表情包库与双智能体自动生成的多场景对话，引入检索框架和自适应阈值。

Result: 实验证明该方法能生成上下文合适且多样的含表情包对话。

Conclusion: MemeCMD是推进多模态对话AI的可扩展且保护隐私的资源。

Abstract: Memes are widely used in online social interactions, providing vivid,
intuitive, and often humorous means to express intentions and emotions.
Existing dialogue datasets are predominantly limited to either manually
annotated or pure-text conversations, lacking the expressiveness and contextual
nuance that multimodal interactions provide.To address these challenges, we
introduce MemeCMD, an automatically generated Chinese Multi-turn Dialogue
dataset with contextually retrieved memes. Our dataset combines a large-scale,
MLLM-annotated meme library with dialogues auto-generated by dual agents across
diverse scenarios. We introduce a retrieval framework and adaptive threshold to
ensure contextually relevant, naturally spaced meme usage. Experiments
demonstrate the effectiveness of our approach in generating contextually
appropriate and diverse meme-incorporated dialogues, offering a scalable and
privacy-preserving resource for advancing multimodal conversational AI.

</details>


### [251] [SciArena: An Open Evaluation Platform for Foundation Models in Scientific Literature Tasks](https://arxiv.org/abs/2507.01001)
*Yilun Zhao,Kaiyan Zhang,Tiansheng Hu,Sihong Wu,Ronan Le Bras,Taira Anderson,Jonathan Bragg,Joseph Chee Chang,Jesse Dodge,Matt Latzke,Yixin Liu,Charles McGrady,Xiangru Tang,Zihang Wang,Chen Zhao,Hannaneh Hajishirzi,Doug Downey,Arman Cohan*

Main category: cs.CL

TL;DR: 介绍开放协作平台SciArena用于评估科学文献任务上的基础模型，收集超13000票，分析数据，发布元评估基准SciArena - Eval，强调自动化评估方法待改进。


<details>
  <summary>Details</summary>
Motivation: 为科学文献任务的基础模型提供社区驱动的评估方式，推动基于模型的自动化评估系统研究。

Method: 采用社区投票比较模型的方式，借鉴Chatbot Arena评估方法，利用集体智慧评估模型在开放式科学任务上的表现。

Result: 平台支持23个模型，收集超13000票，提交问题多样且符合现实需求，评估者有较强一致性；实验凸显基准挑战。

Conclusion: 需要更可靠的自动化评估方法来进行文献任务评估。

Abstract: We present SciArena, an open and collaborative platform for evaluating
foundation models on scientific literature tasks. Unlike traditional benchmarks
for scientific literature understanding and synthesis, SciArena engages the
research community directly, following the Chatbot Arena evaluation approach of
community voting on model comparisons. By leveraging collective intelligence,
SciArena offers a community-driven evaluation of model performance on
open-ended scientific tasks that demand literature-grounded, long-form
responses. The platform currently supports 23 open-source and proprietary
foundation models and has collected over 13,000 votes from trusted researchers
across diverse scientific domains. We analyze the data collected so far and
confirm that the submitted questions are diverse, aligned with real-world
literature needs, and that participating researchers demonstrate strong
self-consistency and inter-annotator agreement in their evaluations. We discuss
the results and insights based on the model ranking leaderboard. To further
promote research in building model-based automated evaluation systems for
literature tasks, we release SciArena-Eval, a meta-evaluation benchmark based
on our collected preference data. The benchmark measures the accuracy of models
in judging answer quality by comparing their pairwise assessments with human
votes. Our experiments highlight the benchmark's challenges and emphasize the
need for more reliable automated evaluation methods.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [252] [Template-Fitting Meets Deep Learning: Redshift Estimation Using Physics-Guided Neural Networks](https://arxiv.org/abs/2507.00866)
*Jonas Chris Ferrao,Dickson Dias,Pranav Naik,Glory D'Cruz,Anish Naik,Siya Khandeparkar,Manisha Gokuldas Fal Dessai*

Main category: astro-ph.IM

TL;DR: 本文提出结合模板拟合与深度学习的混合方法进行光度红移估计，在PREML数据集上取得良好结果，满足LSST部分要求。


<details>
  <summary>Details</summary>
Motivation: 精确的光度红移估计对观测宇宙学至关重要，传统方法各有优缺点，需要更好的方法。

Method: 提出混合方法，将光谱能量分布模板嵌入网络架构，采用多模态设计，结合交叉注意力机制和贝叶斯层。

Result: 在PREML数据集上，RMS误差为0.0507，3 - sigma灾难性异常率为0.13%，偏差为0.0028，满足LSST部分要求。

Conclusion: 结合物理模板和数据驱动模型在即将到来的宇宙学调查中进行稳健红移估计具有潜力。

Abstract: Accurate photometric redshift estimation is critical for observational
cosmology, especially in large-scale surveys where spectroscopic measurements
are impractical. Traditional approaches include template fitting and machine
learning, each with distinct strengths and limitations. We present a hybrid
method that integrates template fitting with deep learning using physics-guided
neural networks. By embedding spectral energy distribution templates into the
network architecture, our model encodes physical priors into the training
process. The system employs a multimodal design, incorporating cross-attention
mechanisms to fuse photometric and image data, along with Bayesian layers for
uncertainty estimation. We evaluate our model on the publicly available PREML
dataset, which includes approximately 400,000 galaxies from the Hyper
Suprime-Cam PDR3 release, with 5-band photometry, multi-band imaging, and
spectroscopic redshifts. Our approach achieves an RMS error of 0.0507, a
3-sigma catastrophic outlier rate of 0.13%, and a bias of 0.0028. The model
satisfies two of the three LSST photometric redshift requirements for redshifts
below 3. These results highlight the potential of combining physically
motivated templates with data-driven models for robust redshift estimation in
upcoming cosmological surveys.

</details>


<div id='astro-ph.CO'></div>

# astro-ph.CO [[Back]](#toc)

### [253] [Simulation-Efficient Cosmological Inference with Multi-Fidelity SBI](https://arxiv.org/abs/2507.00514)
*Leander Thiele,Adrian E. Bayer,Naoya Takeishi*

Main category: astro-ph.CO

TL;DR: 结合不同保真度的模拟集可降低宇宙学模拟推理的成本，提出基于特征匹配和知识蒸馏的多保真度推理方法，提升后验质量。


<details>
  <summary>Details</summary>
Motivation: 降低宇宙学模拟推理的模拟成本。

Method: 提出基于特征匹配和知识蒸馏的多保真度推理方法。

Result: 改进了后验质量，尤其在小模拟预算和困难推理问题上效果明显。

Conclusion: 结合不同保真度模拟集和所提方法能在宇宙学模拟推理中取得较好效果。

Abstract: The simulation cost for cosmological simulation-based inference can be
decreased by combining simulation sets of varying fidelity. We propose an
approach to such multi-fidelity inference based on feature matching and
knowledge distillation. Our method results in improved posterior quality,
particularly for small simulation budgets and difficult inference problems.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [254] [Toward Edge General Intelligence with Multiple-Large Language Model (Multi-LLM): Architecture, Trust, and Orchestration](https://arxiv.org/abs/2507.00672)
*Haoxiang Luo,Yinqiu Liu,Ruichen Zhang,Jiacheng Wang,Gang Sun,Dusit Niyato,Hongfang Yu,Zehui Xiong,Xianbin Wang,Xuemin Shen*

Main category: cs.NI

TL;DR: 本文探讨在边缘计算中集成多LLM以提升任务性能和适应性，回顾发展历程，讨论关键技术、可信系统和多模态架构，指出未来方向。


<details>
  <summary>Details</summary>
Motivation: 传统AI模型在边缘计算处理复杂动态任务时不足，需要集成多LLM解决问题。

Method: 对从传统边缘AI模型到单LLM部署再到多LLM系统的转变进行回顾，讨论多LLM实现的关键技术。

Result: 介绍了多LLM在边缘计算中的关键技术、可信系统和多模态架构。

Conclusion: 本综述为研究人员和从业者在边缘计算应用中利用多LLM系统提供有价值参考，未来需提升资源效率、进行可信治理并解决隐私等问题。

Abstract: Edge computing enables real-time data processing closer to its source, thus
improving the latency and performance of edge-enabled AI applications. However,
traditional AI models often fall short when dealing with complex, dynamic tasks
that require advanced reasoning and multimodal data processing. This survey
explores the integration of multi-LLMs (Large Language Models) to address this
in edge computing, where multiple specialized LLMs collaborate to enhance task
performance and adaptability in resource-constrained environments. We review
the transition from conventional edge AI models to single LLM deployment and,
ultimately, to multi-LLM systems. The survey discusses enabling technologies
such as dynamic orchestration, resource scheduling, and cross-domain knowledge
transfer that are key for multi-LLM implementation. A central focus is on
trusted multi-LLM systems, ensuring robust decision-making in environments
where reliability and privacy are crucial. We also present multimodal multi-LLM
architectures, where multiple LLMs specialize in handling different data
modalities, such as text, images, and audio, by integrating their outputs for
comprehensive analysis. Finally, we highlight future directions, including
improving resource efficiency, trustworthy governance multi-LLM systems, while
addressing privacy, trust, and robustness concerns. This survey provides a
valuable reference for researchers and practitioners aiming to leverage
multi-LLM systems in edge computing applications.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [255] [Ken Utilization Layer: Hebbian Replay Within a Student's Ken for Adaptive Knowledge Tracing](https://arxiv.org/abs/2507.00032)
*Grey Kuling,Marinka Zitnik*

Main category: cs.CY

TL;DR: 介绍了生物启发的知识追踪架构KUL - KT，结合Hebbian记忆编码和基于梯度的巩固，在多个基准测试中表现出色，训练快且内存使用少。


<details>
  <summary>Details</summary>
Motivation: 构建一个可扩展、输入无关的知识追踪架构，实现少样本个性化和自然遗忘，用于大规模个性化学习。

Method: 结合Hebbian记忆编码与基于梯度的巩固，引入时间衰减的Hebbian记忆更新和Loss - aligned Internal Target (LIT)方法，架构包含快速Hebbian记忆和慢速线性网络。

Result: 在十个公开知识追踪基准测试中优于强基线；课堂部署中提高学习者感知的帮助和降低难度；比基于图的模型训练快1.75倍，使用内存少99.01%。

Conclusion: KUL - KT是一个基于生物学、内存高效且输入灵活的大规模个性化学习框架。

Abstract: We introduce KUL-KT, a biologically inspired architecture for knowledge
tracing (KT), combining Hebbian memory encoding with gradient-based
consolidation in a scalable, input-agnostic framework. KUL-KT adapts the
principle of memory consolidation in neural systems, to student modeling by
introducing two key innovations: (i) a time-decaying Hebbian memory update that
enables graceful forgetting, and (ii) a novel Loss-aligned Internal Target
(LIT) method to compute an ideal internal state, allowing continual learning
without backpropagation through time. The architecture consists of a fast
Hebbian memory that captures each learner interaction via a single associative
update, and a slower linear network that consolidates recalled samples through
gradient descent. This design enables few-shot personalization and natural
forgetting without storing raw data or relying on large cohort training.
Operating entirely in embedding space, KUL-KT supports both structured
(tabular) and unstructured (short-answer) inputs. Empirically, KUL-KT
outperforms strong baselines on ten public KT benchmarks in rank-sensitive
metrics such as nDCG and Recall@10. In a classroom deployment, KUL-KT
personalized quizzes from short-answer data, leading to improved
learner-perceived helpfulness and reduced difficulty (p < 0.05). Ablation
studies confirm that Hebbian decay and LIT are critical for continual
adaptation. Compared to a strong graph-based KT model, KUL-KT trains 1.75x
faster and uses 99.01\% less memory. These results position KUL-KT as a
biologically grounded, memory-efficient, and input-flexible framework for
personalized learning at scale.

</details>


### [256] [Integrating Universal Generative AI Platforms in Educational Labs to Foster Critical Thinking and Digital Literacy](https://arxiv.org/abs/2507.00007)
*Vasiliy Znamenskiy,Rafael Niyazov,Joel Hernandez*

Main category: cs.CY

TL;DR: 提出将GenAI平台融入本科实验活动的教育框架，试点效果良好，提出可复制模型。


<details>
  <summary>Details</summary>
Motivation: 认识到不加批判依赖大语言模型的局限和风险，旨在培养本科生批判性思维和数字素养。

Method: 将GenAI作为研究对象和认知工具，让学生制定特定学科提示并评估GenAI生成的文本、图像和视频回应。

Result: 在非科学专业普通天文学课程的试点实施中，学生参与度和批判性反思程度高，很多学生课后继续活动并在研讨会上展示成果。

Conclusion: 强调教育中结构化AI交互的重要性，表明GenAI结合反思评估方法可改善学习成果，提出适用于跨学科AI集成实验工作的可复制模型。

Abstract: This paper presents a new educational framework for integrating generative
artificial intelligence (GenAI) platforms such as ChatGPT, Claude, and Gemini
into laboratory activities aimed at developing critical thinking and digital
literacy among undergraduate students. Recognizing the limitations and risks of
uncritical reliance on large language models (LLMs), the proposed pedagogical
model reframes GenAI as a research subject and cognitive tool. Students
formulate discipline-specific prompts and evaluate GenAI-generated responses in
text, image, and video modalities. A pilot implementation in a general
astronomy course for non-science majors demonstrated high levels of engagement
and critical reflection, with many students continuing the activity after class
and presenting results at a research symposium. The results highlight the
importance of structured AI interactions in education and suggest that GenAI
can improve learning outcomes when combined with reflective assessment methods.
The study proposes a replicable model for interdisciplinary AI-integrated lab
work, adaptable to scientific disciplines. See the guide to learning activities
based on Generative-Ai platforms: https://doi.org/10.5281/zenodo.15555802

</details>


### [257] [Teaching Programming in the Age of Generative AI: Insights from Literature, Pedagogical Proposals, and Student Perspectives](https://arxiv.org/abs/2507.00108)
*Clemente Rubio-Manzano,Jazna Meza,Rodolfo Fernandez-Santibanez,Christian Vidal-Castro*

Main category: cs.CY

TL;DR: 大语言模型推动编程变革，文章回顾相关研究，提议以代码理解和执行为重点丰富教学方法，用可视化工具促进学生理解，还展示学生意见支持可视化模拟。


<details>
  <summary>Details</summary>
Motivation: 大语言模型驱动编程变革，引发大学编程课程教学、学习和评估方式的深入讨论，需探讨应对之策。

Method: 回顾相关研究，提出以代码理解和执行为重点，利用代码可视化表示和执行模拟作为教学、学习和评估工具，并收集学生意见。

Result: 展示了学生对在编程培训中纳入可视化模拟的初步支持意见。

Conclusion: 以代码理解和执行为重点，结合可视化工具的教学方法有助于在生成式AI背景下提高学生对编程的理解。

Abstract: Computer programming is undergoing a true transformation driven by powerful
new tools for automatic source code generation based on large language models.
This transformation is also manifesting in introductory programming courses at
universities around the world, generating an in-depth debate about how
programming content should be taught, learned, and assessed in the context of
generative artificial intelligence.
  This article aims, on the one hand, to review the most relevant studies on
this issue, highlighting the advantages and disadvantages identified in the
specialized literature. On the other hand, it proposes enriching teaching and
learning methodologies by focusing on code comprehension and execution rather
than on mere coding or program functionality. In particular, it advocates for
the use of visual representations of code and visual simulations of its
execution as effective tools for teaching, learning, and assessing programming,
thus fostering a deeper understanding among students.
  Finally, the opinions of students who took the object-oriented programming
course are presented to provide preliminary context supporting the
incorporation of visual simulations in Java (or other languages) as part of the
training process.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [258] [Process-aware and high-fidelity microstructure generation using stable diffusion](https://arxiv.org/abs/2507.00459)
*Hoang Cuong Phan,Minh Tien Tran,Chihun Lee,Hoheok Kim,Sehyok Oh,Dong-Kyu Kim,Ho Won Lee*

Main category: cond-mat.mtrl-sci

TL;DR: 提出基于SD3.5 - Large的工艺感知生成建模方法用于合成微观结构图像，解决数据有限和变量连续问题，验证了生成图像的真实性，为数据驱动材料设计提供可扩展方法。


<details>
  <summary>Details</summary>
Motivation: 合成基于加工参数的真实微观结构图像对理解材料设计中的工艺 - 结构关系至关重要，但因训练图像有限和加工变量连续，此任务具有挑战性。

Method: 基于SD3.5 - Large提出工艺感知生成建模方法，引入数值感知嵌入编码连续变量，通过DreamBooth和LoRA微调部分模型权重，用微调的U - Net语义分割模型验证真实性。

Result: 语义分割模型准确率达97.1%，平均IoU为85.7%，优于以往方法；物理描述符和空间统计显示合成与真实微观结构高度一致，两点相关和线性路径误差分别低于2.1%和0.6%。

Conclusion: 该方法是SD3.5 - Large首次用于工艺感知微观结构生成，为数据驱动的材料设计提供了可扩展方法。

Abstract: Synthesizing realistic microstructure images conditioned on processing
parameters is crucial for understanding process-structure relationships in
materials design. However, this task remains challenging due to limited
training micrographs and the continuous nature of processing variables. To
overcome these challenges, we present a novel process-aware generative modeling
approach based on Stable Diffusion 3.5 Large (SD3.5-Large), a state-of-the-art
text-to-image diffusion model adapted for microstructure generation. Our method
introduces numeric-aware embeddings that encode continuous variables (annealing
temperature, time, and magnification) directly into the model's conditioning,
enabling controlled image generation under specified process conditions and
capturing process-driven microstructural variations. To address data scarcity
and computational constraints, we fine-tune only a small fraction of the
model's weights via DreamBooth and Low-Rank Adaptation (LoRA), efficiently
transferring the pre-trained model to the materials domain. We validate realism
using a semantic segmentation model based on a fine-tuned U-Net with a VGG16
encoder on 24 labeled micrographs. It achieves 97.1% accuracy and 85.7% mean
IoU, outperforming previous methods. Quantitative analyses using physical
descriptors and spatial statistics show strong agreement between synthetic and
real microstructures. Specifically, two-point correlation and lineal-path
errors remain below 2.1% and 0.6%, respectively. Our method represents the
first adaptation of SD3.5-Large for process-aware microstructure generation,
offering a scalable approach for data-driven materials design.

</details>


### [259] [Testing the spin-bath view of self-attention: A Hamiltonian analysis of GPT-2 Transformer](https://arxiv.org/abs/2507.00683)
*Satadeep Bhattacharjee,Seung-Cheol Lee*

Main category: cond-mat.mtrl-sci

TL;DR: 本文基于Huo和Johnson提出的框架，从GPT - 2模型提取权重矩阵推导哈密顿量，得出相边界对数差距标准，评估显示理论与模型经验有强负相关，消融实验证实因果关系，为自旋浴类比提供实证。


<details>
  <summary>Details</summary>
Motivation: 验证Huo和Johnson提出的将大语言模型注意力机制建模为双体自旋系统的框架，为大语言模型提供基于物理原理的解释，实现可解释性。

Method: 从生产级GPT - 2模型提取查询 - 键权重矩阵，推导每个注意力头的有效哈密顿量，得出相边界对数差距标准，进行系统评估和有针对性的消融实验。

Result: 在144个头和20个事实回忆提示上评估，理论对数差距与模型经验标记排名有强负相关（r≈ - 0.70，p < 10⁻³）；消融实验证实因果联系。

Conclusion: 为生产级模型中的自旋浴类比提供了首个强有力的实证证据，为可解释性提供物理视角，为新型生成模型奠定基础，弥合凝聚态物理理论与AI的差距。

Abstract: The recently proposed physics-based framework by Huo and
Johnson~\cite{huo2024capturing} models the attention mechanism of Large
Language Models (LLMs) as an interacting two-body spin system, offering a
first-principles explanation for phenomena like repetition and bias. Building
on this hypothesis, we extract the complete Query-Key weight matrices from a
production-grade GPT-2 model and derive the corresponding effective Hamiltonian
for every attention head. From these Hamiltonians we obtain analytic
\textit{phase boundaries} logit gap criteria that predict which token should
dominate the next-token distribution for a given context. A systematic
evaluation on 144 heads across 20 factual-recall prompts reveals a strong
negative correlation between the theoretical logit gaps and the model's
empirical token rankings ($r\approx-0.70$, $p<10^{-3}$).Targeted ablations
further show that suppressing the heads most aligned with the spin-bath
predictions induces the anticipated shifts in output probabilities, confirming
a causal link rather than a coincidental association. Taken together, our
findings provide the first strong empirical evidence for the spin-bath analogy
in a production-grade model. This validation not only furnishes a tractable,
physics-inspired lens for interpretability but also provides the groundwork for
novel generative models, bridging the gap between theoretical condensed matter
physics and AI.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [260] [Find a Scapegoat: Poisoning Membership Inference Attack and Defense to Federated Learning](https://arxiv.org/abs/2507.00423)
*Wenjin Mo,Zhiyuan Li,Minghong Fang,Mingwei Fang*

Main category: cs.CR

TL;DR: 本文介绍针对联邦学习的新型中毒成员推理攻击FedPoisonMIA，并提出防御机制，实验验证攻击有效性及防御效果。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习中毒攻击对隐私关注有限，研究新型攻击及防御机制。

Method: 引入FedPoisonMIA攻击，让恶意客户端构建本地模型更新推断成员信息；提出防御机制。

Result: 实验表明FedPoisonMIA攻击有效，防御方法能减轻攻击影响。

Conclusion: FedPoisonMIA可对联邦学习造成隐私威胁，所提防御机制有一定效果。

Abstract: Federated learning (FL) allows multiple clients to collaboratively train a
global machine learning model with coordination from a central server, without
needing to share their raw data. This approach is particularly appealing in the
era of privacy regulations like the GDPR, leading many prominent companies to
adopt it. However, FL's distributed nature makes it susceptible to poisoning
attacks, where malicious clients, controlled by an attacker, send harmful data
to compromise the model. Most existing poisoning attacks in FL aim to degrade
the model's integrity, such as reducing its accuracy, with limited attention to
privacy concerns from these attacks. In this study, we introduce FedPoisonMIA,
a novel poisoning membership inference attack targeting FL. FedPoisonMIA
involves malicious clients crafting local model updates to infer membership
information. Additionally, we propose a robust defense mechanism to mitigate
the impact of FedPoisonMIA attacks. Extensive experiments across various
datasets demonstrate the attack's effectiveness, while our defense approach
reduces its impact to a degree.

</details>


### [261] [Safe Low Bandwidth SPV: A Formal Treatment of Simplified Payment Verification Protocols and Security Bounds](https://arxiv.org/abs/2507.00740)
*Craig S Wright*

Main category: cs.CR

TL;DR: 本文给出比特币白皮书里简化支付验证（SPV）的完整形式化规范、协议描述和数学证明结构，表明其安全性和最优性，重构协议并分析其特性，还引入低带宽优化。


<details>
  <summary>Details</summary>
Motivation: 纠正流行实现对SPV的错误表述，证明SPV在有界对抗假设下的安全性和在特定数字现金系统中的最优性。

Method: 从第一性原理重构SPV协议，基于符号自动机、默克尔成员关系和证明链优势谓词构建验证模型，通过概率和博弈论分析。

Result: 得出协议安全运行的经济界限，验证其在部分连接、敌对中继网络和对抗性传播延迟下的活性和安全性，引入低带宽优化且保证正确性。

Conclusion: 为安全的SPV实现提供蓝图，反驳关于非验证客户端的常见误解。

Abstract: This paper presents a complete formal specification, protocol description,
and mathematical proof structure for Simplified Payment Verification (SPV) as
originally defined in the Bitcoin whitepaper \cite{nakamoto2008}. In stark
contrast to the misrepresentations proliferated by popular implementations, we
show that SPV is not only secure under bounded adversarial assumptions but
strictly optimal for digital cash systems requiring scalable and verifiable
transaction inclusion. We reconstruct the SPV protocol from first principles,
grounding its verification model in symbolic automata, Merkle membership
relations, and chain-of-proof dominance predicates. Through rigorous
probabilistic and game-theoretic analysis, we derive the economic bounds within
which the protocol operates securely and verify its liveness and safety
properties under partial connectivity, hostile relay networks, and adversarial
propagation delay. Our specification further introduces low-bandwidth
optimisations such as adaptive polling and compressed header synchronisation
while preserving correctness. This document serves both as a blueprint for
secure SPV implementation and a rebuttal of common misconceptions surrounding
non-validating clients.

</details>


### [262] [The Secrets Must Not Flow: Scaling Security Verification to Large Codebases (extended version)](https://arxiv.org/abs/2507.00595)
*Linard Arquint,Samarth Kishor,Jason R. Koenig,Joey Dodds,Daniel Kroening,Peter Müller*

Main category: cs.CR

TL;DR: 提出Diodon方法，拆分代码库进行安全协议验证，在两个案例中验证有效。


<details>
  <summary>Details</summary>
Motivation: 现有程序验证器难以扩展到大型代码库，需手动工作。

Method: 将代码库拆分为Core和Application，对Core用半自动化验证技术，对Application用全自动静态分析确保I/O独立。

Result: 证明Diodon方法合理，在两个案例中成功验证，如对1%代码的Core验证获得安全保证。

Conclusion: Diodon方法能有效扩展安全协议验证到大型代码库。

Abstract: Existing program verifiers can prove advanced properties about security
protocol implementations, but are difficult to scale to large codebases because
of the manual effort required. We develop a novel methodology called *Diodon*
that addresses this challenge by splitting the codebase into the protocol
implementation (the *Core*) and the remainder (the *Application*). This split
allows us to apply powerful semi-automated verification techniques to the
security-critical Core, while fully-automatic static analyses scale the
verification to the entire codebase by ensuring that the Application cannot
invalidate the security properties proved for the Core. The static analyses
achieve that by proving *I/O independence*, i.e., that the I/O operations
within the Application are independent of the Core's security-relevant data
(such as keys), and that the Application meets the Core's requirements. We have
proved Diodon sound by first showing that we can safely allow the Application
to perform I/O independent of the security protocol, and second that manual
verification and static analyses soundly compose. We evaluate Diodon on two
case studies: an implementation of the signed Diffie-Hellman key exchange and a
large (100k+ LoC) production Go codebase implementing a key exchange protocol
for which we obtained secrecy and injective agreement guarantees by verifying a
Core of about 1% of the code with the auto-active program verifier Gobra in
less than three person months.

</details>


### [263] [AI-Governed Agent Architecture for Web-Trustworthy Tokenization of Alternative Assets](https://arxiv.org/abs/2507.00096)
*Ailiya Borjigin,Wei Zhou,Cong He*

Main category: cs.CR

TL;DR: 论文提出AI治理的代理架构实现另类资产可信代币化，增强了透明度、安全性和合规性，提供了网络资产可信代币化框架。


<details>
  <summary>Details</summary>
Motivation: 网络代币化生态中确保另类资产可信存在重大挑战，如验证链下资产数据和监管合规等。

Method: 提出AI治理的代理架构，让自主代理协调代币化流程，AI驱动的治理层监控代理行为并通过自适应策略和加密经济激励确保信任。

Result: 该方法增强了资产代币化的透明度、安全性和合规性，通过房地产资产代币化案例展示了其能减轻风险。

Conclusion: 将AI治理与多智能体系统和区块链结合可显著增强代币化资产生态系统的信任，为从业者提供了安全合规代币化平台的部署框架。

Abstract: Alternative Assets tokenization is transforming non-traditional financial
instruments are represented and traded on the web. However, ensuring
trustworthiness in web-based tokenized ecosystems poses significant challenges,
from verifying off-chain asset data to enforcing regulatory compliance. This
paper proposes an AI-governed agent architecture that integrates intelligent
agents with blockchain to achieve web-trustworthy tokenization of alternative
assets. In the proposed architecture, autonomous agents orchestrate the
tokenization process (asset verification, valuation, compliance checking, and
lifecycle management), while an AI-driven governance layer monitors agent
behavior and enforces trust through adaptive policies and cryptoeconomic
incentives. We demonstrate that this approach enhances transparency, security,
and compliance in asset tokenization, addressing key concerns around data
authenticity and fraud. A case study on tokenizing real estate assets
illustrates how the architecture mitigates risks (e.g., fraudulent listings and
money laundering) through real-time AI anomaly detection and on-chain
enforcement. Our evaluation and analysis suggest that combining AI governance
with multi-agent systems and blockchain can significantly bolster trust in
tokenized asset ecosystems. This work offers a novel framework for trustworthy
asset tokenization on the web and provides insights for practitioners aiming to
deploy secure, compliant tokenization platforms.

</details>


### [264] [AI-Hybrid TRNG: Kernel-Based Deep Learning for Near-Uniform Entropy Harvesting from Physical Noise](https://arxiv.org/abs/2507.00145)
*Hasan Yiğit*

Main category: cs.CR

TL;DR: AI-Hybrid TRNG是一个深度学习框架，利用低成本前端和CPU定时抖动生成高熵随机数，满足密码学标准，适用于资源受限平台，拓宽了高完整性随机数生成器的应用范围。


<details>
  <summary>Details</summary>
Motivation: 消除对大型量子设备或昂贵实验室级射频接收器的需求，提供适用于多种场景的高完整性随机数生成器。

Method: 使用低成本拇指大小的射频前端和CPU定时抖动进行训练，采用动态内外网络耦合自适应自然源和重新播种。

Result: 生成的随机数通过NIST SP 800 - 22测试和19项定制统计测试，满足密码学标准，无前向和后向预测偏差，模型占用空间小于0.5MB。

Conclusion: AI-Hybrid TRNG可拓宽高完整性随机数生成器在安全系统等多领域的应用。

Abstract: AI-Hybrid TRNG is a deep-learning framework that extracts near-uniform
entropy directly from physical noise, eliminating the need for bulky quantum
devices or expensive laboratory-grade RF receivers. Instead, it relies on a
low-cost, thumb-sized RF front end, plus CPU-timing jitter, for training, and
then emits 32-bit high-entropy streams without any quantization step.
  Unlike deterministic or trained artificial intelligence random number
generators (RNGs), our dynamic inner-outer network couples adaptive natural
sources and reseeding, yielding truly unpredictable and autonomous sequences.
Generated numbers pass the NIST SP 800-22 battery better than a CPU-based
method. It also passes nineteen bespoke statistical tests for both bit- and
integer-level analysis. All results satisfy cryptographic standards, while
forward and backward prediction experiments reveal no exploitable biases. The
model's footprint is below 0.5 MB, making it deployable on MCUs and FPGA soft
cores, as well as suitable for other resource-constrained platforms.
  By detaching randomness quality from dedicated hardware, AI-Hybrid TRNG
broadens the reach of high-integrity random number generators across secure
systems, cryptographic protocols, embedded and edge devices, stochastic
simulations, and server applications that need randomness.

</details>


### [265] [BadViM: Backdoor Attack against Vision Mamba](https://arxiv.org/abs/2507.00577)
*Yinghao Wu,Liyan Zhang*

Main category: cs.CR

TL;DR: 本文研究ViM对后门攻击的易感性，提出BadViM框架，实验表明其攻击成功率高且对常见防御措施有韧性。


<details>
  <summary>Details</summary>
Motivation: 新型视觉状态空间模型（如ViM）安全影响尤其是后门攻击脆弱性研究不足，故研究其对后门攻击的易感性。

Method: 提出BadViM框架，利用共振频率触发器（RFT）创建隐蔽分布式触发器，提出隐藏状态对齐损失来操纵模型内部表示。

Result: BadViM攻击成功率高，保持干净数据准确率，对常见防御措施有显著韧性。

Conclusion: BadViM是一种有效的针对Vision Mamba的后门攻击框架。

Abstract: Vision State Space Models (SSMs), particularly architectures like Vision
Mamba (ViM), have emerged as promising alternatives to Vision Transformers
(ViTs). However, the security implications of this novel architecture,
especially their vulnerability to backdoor attacks, remain critically
underexplored. Backdoor attacks aim to embed hidden triggers into victim
models, causing the model to misclassify inputs containing these triggers while
maintaining normal behavior on clean inputs. This paper investigates the
susceptibility of ViM to backdoor attacks by introducing BadViM, a novel
backdoor attack framework specifically designed for Vision Mamba. The proposed
BadViM leverages a Resonant Frequency Trigger (RFT) that exploits the frequency
sensitivity patterns of the victim model to create stealthy, distributed
triggers. To maximize attack efficacy, we propose a Hidden State Alignment loss
that strategically manipulates the internal representations of model by
aligning the hidden states of backdoor images with those of target classes.
Extensive experimental results demonstrate that BadViM achieves superior attack
success rates while maintaining clean data accuracy. Meanwhile, BadViM exhibits
remarkable resilience against common defensive measures, including PatchDrop,
PatchShuffle and JPEG compression, which typically neutralize normal backdoor
attacks.

</details>


### [266] [The Age of Sensorial Zero Trust: Why We Can No Longer Trust Our Senses](https://arxiv.org/abs/2507.00907)
*Fabio Correa Xavier*

Main category: cs.CR

TL;DR: 文章提出感官零信任安全理念，将零信任原则扩展到人类感官信息，呼吁领导者培养怀疑文化保护组织。


<details>
  <summary>Details</summary>
Motivation: 应对深度伪造和克隆语音等生成式人工智能欺诈风险，组织需新的安全思维。

Method: 将带外验证、视觉语言模型、加密溯源和人员培训等概念集成到框架，将零信任原则扩展到人类感官信息。

Result: 提出基于实证和学术研究的感官零信任框架。

Conclusion: 在AI时代需对感官信息进行验证，领导者应培养方法怀疑文化保护组织完整性。

Abstract: In a world where deepfakes and cloned voices are emerging as sophisticated
attack vectors, organizations require a new security mindset: Sensorial Zero
Trust [9]. This article presents a scientific analysis of the need to
systematically doubt information perceived through the senses, establishing
rigorous verification protocols to mitigate the risks of fraud based on
generative artificial intelligence. Key concepts, such as Out-of-Band
verification, Vision-Language Models (VLMs) as forensic collaborators,
cryptographic provenance, and human training, are integrated into a framework
that extends Zero Trust principles to human sensory information. The approach
is grounded in empirical findings and academic research, emphasizing that in an
era of AI-generated realities, even our eyes and ears can no longer be
implicitly trusted without verification. Leaders are called to foster a culture
of methodological skepticism to protect organizational integrity in this new
threat landscape.

</details>


<div id='hep-ph'></div>

# hep-ph [[Back]](#toc)

### [267] [Discovering the underlying analytic structure within Standard Model constants using artificial intelligence](https://arxiv.org/abs/2507.00225)
*S. V. Chekanov,H. Kjellerstrand*

Main category: hep-ph

TL;DR: 本文使用符号回归和遗传编程在标准模型基本参数中寻找潜在分析结构，找到简单关系并得到有价值结果。


<details>
  <summary>Details</summary>
Motivation: 在标准模型基本参数中寻找潜在分析结构。

Method: 使用符号回归和遗传编程。

Result: 识别出连接常数对的最简单分析关系，约一千个表达式相对精度优于1%。

Conclusion: 结果可为模型构建者和人工智能方法提供有价值输入，也可能成为连接标准模型所有参数的基础法则的构建块。

Abstract: This paper presents a search for underlying analytic structures among the
fundamental parameters of the Standard Model (SM) using symbolic regression and
genetic programming. We identify the simplest analytic relationships connecting
pairs of these constants and report several notable observations based on about
a thousand expressions with relative precision better than 1%. These results
may serve as valuable inputs for model builders and artificial intelligence
methods aimed at uncovering hidden patterns among the SM constants, or
potentially used as building blocks for a deeper underlying law that connects
all parameters of the SM through a small set of fundamental constants.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [268] [Feature Integration Spaces: Joint Training Reveals Dual Encoding in Neural Network Representations](https://arxiv.org/abs/2507.00269)
*Omar Claflin*

Main category: q-bio.NC

TL;DR: 指出当前SAE方法不足，提出双编码假设，开发训练架构验证，取得重建和误差降低效果，证明双编码等特性并带来架构范式转变。


<details>
  <summary>Details</summary>
Motivation: 解决当前SAE方法无法消除多语义性和存在行为错误的问题。

Method: 开发顺序和联合训练架构来同时捕捉特征身份和集成模式。

Result: 联合训练实现41.3%重建改进和51.6%KL散度误差降低，架构自发形成双峰特征组织，小非线性组件有16.5%独立改进，干预实验显示集成特征有选择性敏感度和系统行为效应。

Conclusion: 为神经表征双编码、有意义的非线性特征交互提供证据，带来架构范式转变，为下一代SAE奠定基础。

Abstract: Current sparse autoencoder (SAE) approaches to neural network
interpretability assume that activations can be decomposed through linear
superposition into sparse, interpretable features. Despite high reconstruction
fidelity, SAEs consistently fail to eliminate polysemanticity and exhibit
pathological behavioral errors. We propose that neural networks encode
information in two complementary spaces compressed into the same substrate:
feature identity and feature integration. To test this dual encoding
hypothesis, we develop sequential and joint-training architectures to capture
identity and integration patterns simultaneously. Joint training achieves 41.3%
reconstruction improvement and 51.6% reduction in KL divergence errors. This
architecture spontaneously develops bimodal feature organization: low squared
norm features contributing to integration pathways and the rest contributing
directly to the residual. Small nonlinear components (3% of parameters) achieve
16.5% standalone improvements, demonstrating parameter-efficient capture of
computational relationships crucial for behavior. Additionally, intervention
experiments using 2x2 factorial stimulus designs demonstrated that integration
features exhibit selective sensitivity to experimental manipulations and
produce systematic behavioral effects on model outputs, including significant
interaction effects across semantic dimensions. This work provides systematic
evidence for (1) dual encoding in neural representations, (2) meaningful
nonlinearly encoded feature interactions, and (3) introduces an architectural
paradigm shift from post-hoc feature analysis to integrated computational
design, establishing foundations for next-generation SAEs.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [269] [Digital Collections Explorer: An Open-Source, Multimodal Viewer for Searching Digital Collections](https://arxiv.org/abs/2507.00961)
*Ying-Hsiang Huang,Benjamin Charles Germain Lee*

Main category: cs.DL

TL;DR: 介绍基于网络的开源探索性搜索平台Digital Collections Explorer，可利用CLIP增强数字藏品视觉发现，描述其架构、应用等并展示案例和可扩展性，还提供公开演示。


<details>
  <summary>Details</summary>
Motivation: 为数字藏品尤其是元数据匮乏的数字档案提供更便捷的访问途径，实现数字档案访问的民主化。

Method: 基于多模态搜索技术，利用CLIP，实现自然语言查询和反向图像搜索。

Result: Digital Collections Explorer可在本地安装并配置运行，能处理多种文化遗产藏品，在有M4芯片的MacBook Pro上可处理数十万张图像。

Conclusion: Digital Collections Explorer具有灵活性和易用性，有潜力实现数字档案访问的民主化。

Abstract: We present Digital Collections Explorer, a web-based, open-source exploratory
search platform that leverages CLIP (Contrastive Language-Image Pre-training)
for enhanced visual discovery of digital collections. Our Digital Collections
Explorer can be installed locally and configured to run on a visual collection
of interest on disk in just a few steps. Building upon recent advances in
multimodal search techniques, our interface enables natural language queries
and reverse image searches over digital collections with visual features. This
paper describes the system's architecture, implementation, and application to
various cultural heritage collections, demonstrating its potential for
democratizing access to digital archives, especially those with impoverished
metadata. We present case studies with maps, photographs, and PDFs extracted
from web archives in order to demonstrate the flexibility of the Digital
Collections Explorer, as well as its ease of use. We demonstrate that the
Digital Collections Explorer scales to hundreds of thousands of images on a
MacBook Pro with an M4 chip. Lastly, we host a public demo of Digital
Collections Explorer.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [270] [From Sentences to Sequences: Rethinking Languages in Biological System](https://arxiv.org/abs/2507.00953)
*Ke Liu,Shuanke Shen,Hao Chen*

Main category: q-bio.BM

TL;DR: 本文探讨大语言模型范式从自然语言处理迁移到生物序列建模，强调结构评估重要性并展示自回归范式适用性。


<details>
  <summary>Details</summary>
Motivation: 自然语言与生物语言内在结构相关性不同，需更好地将NLP成功经验有效迁移到生物领域。

Method: 将生物分子3D结构视为句子语义内容，考虑残基或碱基间强相关性。

Result: 突出了结构评估的重要性，展示了自回归范式在生物语言建模中的适用性。

Conclusion: 通过特定处理方式，可促进NLP在生物领域的有效应用。

Abstract: The paradigm of large language models in natural language processing (NLP)
has also shown promise in modeling biological languages, including proteins,
RNA, and DNA. Both the auto-regressive generation paradigm and evaluation
metrics have been transferred from NLP to biological sequence modeling.
However, the intrinsic structural correlations in natural and biological
languages differ fundamentally. Therefore, we revisit the notion of language in
biological systems to better understand how NLP successes can be effectively
translated to biological domains. By treating the 3D structure of biomolecules
as the semantic content of a sentence and accounting for the strong
correlations between residues or bases, we highlight the importance of
structural evaluation and demonstrate the applicability of the auto-regressive
paradigm in biological language modeling. Code can be found at
\href{https://github.com/zjuKeLiu/RiFold}{github.com/zjuKeLiu/RiFold}

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [271] [Unraveling Global Threads: Pandemic, Geopolitical Conflict, and Resilience in Fashion and Textile Supply Chain](https://arxiv.org/abs/2507.00207)
*Md. Al-Amin,Muneeb Tahir,Amit Talukder,Abdullah Al Mamun,Md Tanjim Hossain,Nigar Sultana*

Main category: econ.GN

TL;DR: 研究新冠疫情和俄乌、巴以、印巴冲突对全球纺织时尚供应链的影响，分析供应链中断情况并探讨恢复和加强供应链的策略。


<details>
  <summary>Details</summary>
Motivation: 新冠疫情和地区冲突对全球纺织时尚供应链产生不稳定影响，需研究其具体影响。

Method: 采用内容分析法，从谷歌学术、北卡罗来纳州立大学Summon数据库和NexisUni等来源获取相关文章和新闻。

Result: 新冠疫情导致订单取消、工厂和店铺关闭、裁员等，同时促进电商发展和供应链数字化；地区冲突的二阶效应比直接军事行动对供应链影响更大。

Conclusion: 应采取策略恢复和加强时尚供应链以应对危机。

Abstract: Several noteworthy scenarios emerged in the global textile and fashion supply
chains during and after the COVID-19 pandemic. The destabilizing influences of
a global pandemic and a geographically localized conflict are being acutely
noticed in the worldwide fashion and textile supply chains. This work examines
the impact of the COVID-19 pandemic, the Russo-Ukraine conflict,
Israel-Palestine conflict, and Indo-Pak conflict on supply chains within the
textile and fashion industry. This research employed a content analysis method
to identify relevant articles and news from sources such as Google Scholar, the
Summon database of North Carolina State University, and the scholarly news
portal NexisUni. The selected papers, news articles, and reports provide a
comprehensive overview of the fashion, textile, and apparel supply chain
disruptions caused by the pandemic and the war in Ukraine, accompanied by
discussions from common supply chain perspectives. Disruptions due to COVID-19
include international brands and retailers canceling orders, closures of stores
and factories in developing countries, layoffs, and furloughs of workers in
both retail stores and supplier factories, the increased prominence of online
and e-commerce businesses, the growing importance of automation and
digitalization in the fashion supply chain, considerations of sustainability,
and the need for a resilient supply chain system to facilitate post-pandemic
recovery. In the case of the Russo-Ukraine war, Israel-Palestine war, and
Indo-Pak war, the second-order effects of the conflict have had a more
significant impact on the textile supply chain than the direct military
operations themselves. In addition to these topics, the study delves into the
potential strategies for restoring and strengthening the fashion supply chain

</details>


### [272] [Satellite and Mobile Phone Data Reveal How Violence Affects Seasonal Migration in Afghanistan](https://arxiv.org/abs/2507.00279)
*Xiao Hui Tai,Suraj R. Nair,Shikhar Mehra,Joshua E. Blumenstock*

Main category: econ.GN

TL;DR: 研究聚焦阿富汗2021年塔利班接管前8年，用卫星图像推断鸦片收获时间，结合手机记录研究暴力冲突对季节性移民的影响，发现劳动力流动对个别暴力事件有韧性，但受长期冲突模式影响。


<details>
  <summary>Details</summary>
Motivation: 以往因缺乏冲突地区移民可靠数据，难以验证暴力和内战会扰乱季节性劳动力流动的假设，故开展研究。

Method: 用卫星图像推断鸦片收获时间，利用全国手机记录数据刻画移民对收获的反应，考察暴力和内战对移民的影响。

Result: 罂粟种植水平高的地区比无种植地区接收更多季节性移民，劳动力流动对个别暴力事件有韧性，但受长期冲突模式如塔利班控制程度影响。

Conclusion: 季节性移民受长期冲突模式影响，而非个别暴力事件。

Abstract: Seasonal migration plays a critical role in stabilizing rural economies and
sustaining the livelihoods of agricultural households. Violence and civil
conflict have long been thought to disrupt these labor flows, but this
hypothesis has historically been hard to test given the lack of reliable data
on migration in conflict zones. Focusing on Afghanistan in the 8-year period
prior to the Taliban's takeover in 2021, we first demonstrate how satellite
imagery can be used to infer the timing of the opium harvest, which employs a
large number of seasonal workers in relatively well-paid jobs. We then use a
dataset of nationwide mobile phone records to characterize the migration
response to this harvest, and examine whether and how violence and civil
conflict disrupt this migration. We find that, on average, districts with high
levels of poppy cultivation receive significantly more seasonal migrants than
districts with no poppy cultivation. These labor flows are surprisingly
resilient to idiosyncratic violent events at the source or destination,
including extreme violence resulting in large numbers of fatalities. However,
seasonal migration is affected by longer-term patterns of conflict, such as the
extent of Taliban control in origin and destination locations.

</details>


### [273] [Factors Influencing Change Orders in Horizontal Construction Projects: A Comparative Analysis of Unit Price and Lump Sum Contracts](https://arxiv.org/abs/2507.00281)
*Mohamed Khalafalla,Tejal Mulay,Shonda L Bernadin*

Main category: econ.GN

TL;DR: 研究通过定量分析FDOT历史投标数据，评估影响DBB项目变更单频率的因素，离散选择模型有助于选合同类型，可减少变更单。


<details>
  <summary>Details</summary>
Motivation: 变更单在建设项目中常见，会增加成本和工期，DBB项目变更单频率更高，需找出改进方法以降低频率。

Method: 利用FDOT历史投标数据，对水平建设项目的五个因素进行评估，用离散选择模型评估两种DBB合同技术。

Result: 分析581个UP和189个LS项目，发现项目规模、工期和工作类型对变更单频率有显著影响，离散选择模型比传统方法更能确定合适合同类型。

Conclusion: 评估水平建设项目的合同技术可提升DBB的使用，减少交通部门的变更单。

Abstract: Change orders (COs) are a common occurrence in construction projects, leading
to increased costs and extended durations. Design-Bid-Build (DBB) projects,
favored by state transportation agencies (STAs), often experience a higher
frequency of COs compared to other project delivery methods. This study aims to
identify areas of improvement to reduce CO frequency in DBB projects through a
quantitative analysis. Historical bidding data from the Florida Department of
Transportation (FDOT) was utilized to evaluate five factors, contracting
technique, project location, type of work, project size, and duration, on
specific horizontal construction projects. Two DBB contracting techniques, Unit
Price (UP) and Lump Sum (LS), were evaluated using a discrete choice model. The
analysis of 581 UP and 189 LS projects revealed that project size, duration,
and type of work had a statistically significant influence on the frequency of
change orders at a 95% confidence level. The discrete choice model showed
significant improvement in identifying the appropriate contract type for a
specific project compared to traditional methods used by STAs. By evaluating
the contracting technique instead of project delivery methods for horizontal
construction projects, the use of DBB can be enhanced, leading to reduced
change orders for STAs.

</details>


<div id='cs.CC'></div>

# cs.CC [[Back]](#toc)

### [274] [Hamiltonicity Parameterized by Mim-Width is (Indeed) Para-NP-Hard](https://arxiv.org/abs/2507.00612)
*Benjamin Bergougnoux,Lars Jaffke*

Main category: cs.CC

TL;DR: 证明哈密顿路径和哈密顿循环在线性mim - width为26的图上是NP难的，填补了相关证明空白。


<details>
  <summary>Details</summary>
Motivation: 填补由mim - width参数化的哈密顿性问题的para - NP难证明中的空白。

Method: 对线性mim - width为26的图进行证明

Result: 证明了哈密顿路径和哈密顿循环在给定线性mim - width为26的图及其线性顺序时是NP难的。

Conclusion: 解决了之前证明中的问题，明确了相关问题在特定图上的复杂度。

Abstract: We prove that Hamiltonian Path and Hamiltonian Cycle are NP-hard on graphs of
linear mim-width 26, even when a linear order of the input graph with mim-width
26 is provided together with input. This fills a gap left by a broken proof of
the para-NP-hardness of Hamiltonicity problems parameterized by mim-width.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [275] [Augmenting Molecular Graphs with Geometries via Machine Learning Interatomic Potentials](https://arxiv.org/abs/2507.00407)
*Cong Fu,Yuchao Lin,Zachary Krueger,Haiyang Yu,Maho Nakata,Jianwen Xie,Emine Kucukbenli,Xiaofeng Qian,Shuiwang Ji*

Main category: physics.chem-ph

TL;DR: 本文尝试仅依靠机器学习原子间势（MLIP）模型获取分子几何结构以用于分子性质预测，结果表明基于弛豫数据训练的MLIP基础模型能提供有价值的分子几何结构。


<details>
  <summary>Details</summary>
Motivation: 准确的分子性质预测需要3D几何结构，而传统获取方法昂贵，因此尝试用机器学习方法获取。

Method: 首先整理包含350万个分子和3亿个快照的大规模分子弛豫数据集，用监督学习训练MLIP基础模型以预测给定3D分子结构的能量和力，之后用训练好的模型通过几何优化获取低能3D几何结构，并引入几何微调，也可在有真实3D几何结构时直接微调模型进行性质预测。

Result: 基于弛豫数据训练的MLIP基础模型能提供有价值的分子几何结构，有利于性质预测。

Conclusion: MLIP基础模型在获取分子几何结构用于性质预测方面有应用价值。

Abstract: Accurate molecular property predictions require 3D geometries, which are
typically obtained using expensive methods such as density functional theory
(DFT). Here, we attempt to obtain molecular geometries by relying solely on
machine learning interatomic potential (MLIP) models. To this end, we first
curate a large-scale molecular relaxation dataset comprising 3.5 million
molecules and 300 million snapshots. Then MLIP foundation models are trained
with supervised learning to predict energy and forces given 3D molecular
structures. Once trained, we show that the foundation models can be used in
different ways to obtain geometries either explicitly or implicitly. First, it
can be used to obtain low-energy 3D geometries via geometry optimization,
providing relaxed 3D geometries for downstream molecular property predictions.
To mitigate potential biases and enhance downstream predictions, we introduce
geometry fine-tuning based on the relaxed 3D geometries. Second, the foundation
models can be directly fine-tuned for property prediction when ground truth 3D
geometries are available. Our results demonstrate that MLIP foundation models
trained on relaxation data can provide valuable molecular geometries that
benefit property predictions.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [276] [Physics-Aware Style Transfer for Adaptive Holographic Reconstruction](https://arxiv.org/abs/2507.00482)
*Chanseok Lee,Fakhriyya Mammadova,Jiseong Barg,Mooseok Jang*

Main category: physics.optics

TL;DR: 提出物理感知风格迁移方法，仅用强度测量数据集自适应学习逆映射操作，展示生物医学应用潜力。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法需高质量复振幅图的真实数据集，研究旨在解决无真实数据时的全息成像逆问题。

Method: 将物到传感器的距离视为衍射图案中的隐式风格，以风格域为中间域构建循环图像翻译。

Result: 仅用强度测量数据集实现自适应逆映射操作，重建动态流动红细胞形态。

Conclusion: 该方法利用测量中的物理线索，为难以获取真实数据的成像应用提供实用学习策略。

Abstract: Inline holographic imaging presents an ill-posed inverse problem of
reconstructing objects' complex amplitude from recorded diffraction patterns.
Although recent deep learning approaches have shown promise over classical
phase retrieval algorithms, they often require high-quality ground truth
datasets of complex amplitude maps to achieve a statistical inverse mapping
operation between the two domains. Here, we present a physics-aware style
transfer approach that interprets the object-to-sensor distance as an implicit
style within diffraction patterns. Using the style domain as the intermediate
domain to construct cyclic image translation, we show that the inverse mapping
operation can be learned in an adaptive manner only with datasets composed of
intensity measurements. We further demonstrate its biomedical applicability by
reconstructing the morphology of dynamically flowing red blood cells,
highlighting its potential for real-time, label-free imaging. As a framework
that leverages physical cues inherently embedded in measurements, the presented
method offers a practical learning strategy for imaging applications where
ground truth is difficult or impossible to obtain.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [277] [InSight-R: A Framework for Risk-informed Human Failure Event Identification and Interface-Induced Risk Assessment Driven by AutoGraph](https://arxiv.org/abs/2507.00066)
*Xingyu Xiao,Jiejuan Tong,Peng Chen,Jun Sun,Zhe Sui,Jingang Liang,Hongru Zhao,Jun Zhao,Haitao Wang*

Main category: cs.HC

TL;DR: 文章指出传统人因可靠性分析方法存在局限，提出InSight - R框架，该框架提升了人因失效事件识别的客观性和可解释性，为界面设计优化提供见解。


<details>
  <summary>Details</summary>
Motivation: 传统人因可靠性分析方法依赖专家判断，存在可重复性、主观性问题，且缺乏对人机界面设计影响的评估，需新方法解决这些局限。

Method: 提出InSight - R框架，将经验行为数据与自动化图执行框架构建的界面嵌入式知识图谱关联，实现基于易错和时间偏差操作路径的自动人因失效事件识别，并探讨设计 - 用户冲突与人因错误的关系。

Result: InSight - R增强了人因失效事件识别的客观性和可解释性，为数字化控制环境中的动态实时人因可靠性评估提供可扩展途径。

Conclusion: 该框架为界面设计优化提供可行见解，推动了机理驱动的人因可靠性分析方法的发展。

Abstract: Human reliability remains a critical concern in safety-critical domains such
as nuclear power, where operational failures are often linked to human error.
While conventional human reliability analysis (HRA) methods have been widely
adopted, they rely heavily on expert judgment for identifying human failure
events (HFEs) and assigning performance influencing factors (PIFs). This
reliance introduces challenges related to reproducibility, subjectivity, and
limited integration of interface-level data. In particular, current approaches
lack the capacity to rigorously assess how human-machine interface design
contributes to operator performance variability and error susceptibility. To
address these limitations, this study proposes a framework for risk-informed
human failure event identification and interface-induced risk assessment driven
by AutoGraph (InSight-R). By linking empirical behavioral data to the
interface-embedded knowledge graph (IE-KG) constructed by the automated
graph-based execution framework (AutoGraph), the InSight-R framework enables
automated HFE identification based on both error-prone and time-deviated
operational paths. Furthermore, we discuss the relationship between
designer-user conflicts and human error. The results demonstrate that InSight-R
not only enhances the objectivity and interpretability of HFE identification
but also provides a scalable pathway toward dynamic, real-time human
reliability assessment in digitalized control environments. This framework
offers actionable insights for interface design optimization and contributes to
the advancement of mechanism-driven HRA methodologies.

</details>


### [278] [Designing an Adaptive Storytelling Platform to Promote Civic Education in Politically Polarized Learning Environments](https://arxiv.org/abs/2507.00161)
*Christopher M. Wegemer,Edward Halim,Jeff Burke*

Main category: cs.HC

TL;DR: 政治极化破坏民主公民教育，本文研究利用新兴AI技术开发AI - DCS平台，以减少极化，促进政治开放性，为相关策略奠定基础并给出启示。


<details>
  <summary>Details</summary>
Motivation: 政治极化加剧基于身份的对不同观点的抵制，破坏民主公民教育，新兴AI技术为减少极化和促进政治开放性干预提供机会。

Method: 运用基于设计的研究（DBR）方法，结合政治心理学和叙事学理论，开发集成面部情绪识别和注意力跟踪的AI - DCS平台，通过GPT - 4实现逐句语言自适应。

Result: 开发出AI - DCS平台原型，可实时评估用户情感和注意力状态，个性化语言风格以维持学生对不同政治观点故事的情感投入。

Conclusion: 为AI支持、情感敏感的策略奠定基础，对公民教育干预、算法素养以及AI对话管理和情感自适应学习环境的人机交互挑战有启示。

Abstract: Political polarization undermines democratic civic education by exacerbating
identity-based resistance to opposing viewpoints. Emerging AI technologies
offer new opportunities to advance interventions that reduce polarization and
promote political open-mindedness. We examined novel design strategies that
leverage adaptive and emotionally-responsive civic narratives that may sustain
students' emotional engagement in stories, and in turn, promote
perspective-taking toward members of political out-groups. Drawing on theories
from political psychology and narratology, we investigate how affective
computing techniques can support three storytelling mechanisms: transportation
into a story world, identification with characters, and interaction with the
storyteller. Using a design-based research (DBR) approach, we iteratively
developed and refined an AI-mediated Digital Civic Storytelling (AI-DCS)
platform. Our prototype integrates facial emotion recognition and attention
tracking to assess users' affective and attentional states in real time.
Narrative content is organized around pre-structured story outlines, with
beat-by-beat language adaptation implemented via GPT-4, personalizing
linguistic tone to sustain students' emotional engagement in stories that
center political perspectives different from their own. Our work offers a
foundation for AI-supported, emotionally-sensitive strategies that address
affective polarization while preserving learner autonomy. We conclude with
implications for civic education interventions, algorithmic literacy, and HCI
challenges associated with AI dialogue management and affect-adaptive learning
environments.

</details>


### [279] [Visual Privacy Management with Generative AI for Blind and Low-Vision People](https://arxiv.org/abs/2507.00286)
*Tanusree Sharma,Yu-Yun Tseng,Lotus Zhang,Ayae Ide,Kelly Avery Mack,Leah Findlater,Danna Gurari,Yang Wang*

Main category: cs.HC

TL;DR: 通过对21名视障人士的访谈，研究其使用生成式AI处理视觉内容时的实践和设计偏好，给出支持以用户为中心的视觉隐私的设计建议。


<details>
  <summary>Details</summary>
Motivation: 生成式AI工具在帮助视障人士处理视觉内容时带来视觉隐私挑战，需了解用户实践和设计偏好。

Method: 对21名视障人士进行访谈研究。

Result: 发现平衡隐私、效率和情感能动性的当前实践，用户会考虑六种关键场景的隐私风险，揭示了如设备端处理等设计偏好。

Conclusion: 给出支持以用户为中心的视觉隐私的可行动设计建议，拓展隐私和他人数据负责任处理的概念。

Abstract: Blind and low vision (BLV) individuals use Generative AI (GenAI) tools to
interpret and manage visual content in their daily lives. While such tools can
enhance the accessibility of visual content and so enable greater user
independence, they also introduce complex challenges around visual privacy. In
this paper, we investigate the current practices and future design preferences
of blind and low vision individuals through an interview study with 21
participants. Our findings reveal a range of current practices with GenAI that
balance privacy, efficiency, and emotional agency, with users accounting for
privacy risks across six key scenarios, such as self-presentation,
indoor/outdoor spatial privacy, social sharing, and handling professional
content. Our findings reveal design preferences, including on-device
processing, zero-retention guarantees, sensitive content redaction,
privacy-aware appearance indicators, and multimodal tactile mirrored
interaction methods. We conclude with actionable design recommendations to
support user-centered visual privacy through GenAI, expanding the notion of
privacy and responsible handling of others data.

</details>


### [280] [Customer Service Representative's Perception of the AI Assistant in an Organization's Call Center](https://arxiv.org/abs/2507.00513)
*Kai Qin,Kexin Du,Yimeng Chen,Yueyan Liu,Jie Cai,Zhiqiang Nie,Nan Gao,Guohui Wei,Shengzhu Wang,Chun Yu*

Main category: cs.HC

TL;DR: 研究电网客服中心客服代表对AI辅助的感知，发现AI有减负和增负两方面影响。


<details>
  <summary>Details</summary>
Motivation: 探究电网客服中心客服代表在与客户互动中对AI辅助的感知。

Method: 实地走访和对13名客服代表进行半结构化访谈。

Result: AI能减轻一些传统负担，但也带来新负担。

Conclusion: 有助于更细致理解组织环境中AI集成，强调客服代表适应新系统的努力和负担。

Abstract: The integration of various AI tools creates a complex socio-technical
environment where employee-customer interactions form the core of work
practices. This study investigates how customer service representatives (CSRs)
at the power grid service customer service call center perceive AI assistance
in their interactions with customers. Through a field visit and semi-structured
interviews with 13 CSRs, we found that AI can alleviate some traditional
burdens during the call (e.g., typing and memorizing) but also introduces new
burdens (e.g., earning, compliance, psychological burdens). This research
contributes to a more nuanced understanding of AI integration in organizational
settings and highlights the efforts and burdens undertaken by CSRs to adapt to
the updated system.

</details>


### [281] [Generative Exaggeration in LLM Social Agents: Consistency, Bias, and Toxicity](https://arxiv.org/abs/2507.00657)
*Jacopo Nudo,Mario Edoardo Pandolfo,Edoardo Loru,Mattia Samory,Matteo Cinelli,Walter Quattrociocchi*

Main category: cs.HC

TL;DR: 研究大语言模型模拟社交媒体政治话语表现，发现丰富上下文虽提升一致性但放大极化等问题，模型输出反映内部优化而非用户行为，挑战其在多领域应用。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型在模拟社交媒体政治话语时的表现。

Method: 利用2024年美国大选期间X平台2100万条互动数据，构建基于1186个真实用户的大语言模型智能体，在控制条件下回复政治相关推文，设置零样本和少样本初始化方式，对比人类回复，评估三个模型家族。

Result: 丰富上下文提升内部一致性，但放大极化、风格化信号和有害语言，出现“生成夸张”现象，模型输出反映内部优化动态而非用户行为。

Conclusion: 大语言模型不能有效模拟用户，存在结构偏差，挑战其在内容审核、审议模拟和政策建模中的应用。

Abstract: We investigate how Large Language Models (LLMs) behave when simulating
political discourse on social media. Leveraging 21 million interactions on X
during the 2024 U.S. presidential election, we construct LLM agents based on
1,186 real users, prompting them to reply to politically salient tweets under
controlled conditions. Agents are initialized either with minimal ideological
cues (Zero Shot) or recent tweet history (Few Shot), allowing one-to-one
comparisons with human replies. We evaluate three model families (Gemini,
Mistral, and DeepSeek) across linguistic style, ideological consistency, and
toxicity. We find that richer contextualization improves internal consistency
but also amplifies polarization, stylized signals, and harmful language. We
observe an emergent distortion that we call "generation exaggeration": a
systematic amplification of salient traits beyond empirical baselines. Our
analysis shows that LLMs do not emulate users, they reconstruct them. Their
outputs, indeed, reflect internal optimization dynamics more than observed
behavior, introducing structural biases that compromise their reliability as
social proxies. This challenges their use in content moderation, deliberative
simulations, and policy modeling.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [282] [Geological Everything Model 3D: A Promptable Foundation Model for Unified and Zero-Shot Subsurface Understanding](https://arxiv.org/abs/2507.00419)
*Yimin Dou,Xinming Wu,Nathan L Bangs,Harpreet Singh Sethi,Jintao Li,Hang Gao,Zhixiang Guo*

Main category: physics.geo-ph

TL;DR: 提出地质万物模型3D (GEM)，统一多种地下分析任务，实现跨任务零样本泛化，具有广泛适用性。


<details>
  <summary>Details</summary>
Motivation: 当前地下分析碎片化，不同任务需单独模型，引入GEM实现统一分析。

Method: 将任务重构为基于地下成像潜结构框架的提示条件推理，通过两阶段训练，结合自监督学习和对抗性微调。

Result: GEM在多种调查和任务中展现广泛适用性，实现跨任务零样本泛化。

Conclusion: GEM以结构感知方式融合专家知识和生成推理，为可扩展的人机协同地球物理AI奠定基础。

Abstract: Understanding Earth's subsurface is critical for energy transition, natural
hazard mitigation, and planetary science. Yet subsurface analysis remains
fragmented, with separate models required for structural interpretation,
stratigraphic analysis, geobody segmentation, and property modeling-each
tightly coupled to specific data distributions and task formulations. We
introduce the Geological Everything Model 3D (GEM), a unified generative
architecture that reformulates all these tasks as prompt-conditioned inference
along latent structural frameworks derived from subsurface imaging. This
formulation moves beyond task-specific models by enabling a shared inference
mechanism, where GEM propagates human-provided prompts-such as well logs,
masks, or structural sketches-along inferred structural frameworks to produce
geologically coherent outputs. Through this mechanism, GEM achieves zero-shot
generalization across tasks with heterogeneous prompt types, without retraining
for new tasks or data sources. This capability emerges from a two-stage
training process that combines self-supervised representation learning on
large-scale field seismic data with adversarial fine-tuning using mixed prompts
and labels across diverse subsurface tasks. GEM demonstrates broad
applicability across surveys and tasks, including Martian radar stratigraphy
analysis, structural interpretation in subduction zones, full seismic
stratigraphic interpretation, geobody delineation, and property modeling. By
bridging expert knowledge with generative reasoning in a structurally aware
manner, GEM lays the foundation for scalable, human-in-the-loop geophysical
AI-transitioning from fragmented pipelines to a vertically integrated,
promptable reasoning system. Project page: https://douyimin.github.io/GEM

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [283] [Guided Unconditional and Conditional Generative Models for Super-Resolution and Inference of Quasi-Geostrophic Turbulence](https://arxiv.org/abs/2507.00719)
*Anantha Narayanan Suresh Babu,Akhil Sadam,Pierre F. J. Lermusiaux*

Main category: physics.flu-dyn

TL;DR: 本文应用四种生成式扩散建模方法对二维准地转湍流进行超分辨率和推理，对比不同方法表现并分析权衡，为地球物理反问题提供指导。


<details>
  <summary>Details</summary>
Motivation: 海洋、天气和气候数值模拟粗糙，观测稀疏有缺口，需方法从这类观测进行超分辨率和推理。

Method: 应用四种生成式扩散建模方法，包括两种引导方法（SDEdit、DPS）和两种条件方法（普通变体、无分类器引导），考虑多种测试用例，采用综合技能指标评估，研究调参敏感性。

Result: SDEdit生成非物理场，DPS低成本生成合理重构但平滑了精细特征，条件方法需重新训练，能重构精细特征、与观测循环一致且统计正确，模型误差与集合标准差高度相关。

Conclusion: 强调扩散模型在实现难易、保真度和循环一致性间的权衡，为地球物理反问题部署提供实用指导。

Abstract: Typically, numerical simulations of the ocean, weather, and climate are
coarse, and observations are sparse and gappy. In this work, we apply four
generative diffusion modeling approaches to super-resolution and inference of
forced two-dimensional quasi-geostrophic turbulence on the beta-plane from
coarse, sparse, and gappy observations. Two guided approaches minimally adapt a
pre-trained unconditional model: SDEdit modifies the initial condition, and
Diffusion Posterior Sampling (DPS) modifies the reverse diffusion process
score. The other two conditional approaches, a vanilla variant and
classifier-free guidance, require training with paired high-resolution and
observation data. We consider eight test cases spanning: two regimes, eddy and
anisotropic-jet turbulence; two Reynolds numbers, 10^3 and 10^4; and two
observation types, 4x coarse-resolution fields and coarse, sparse and gappy
observations. Our comprehensive skill metrics include norms of the
reconstructed vorticity fields, turbulence statistical quantities, and
quantification of the super-resolved probabilistic ensembles and their errors.
We also study the sensitivity to tuning parameters such as guidance strength.
Results show that SDEdit generates unphysical fields, while DPS generates
reasonable reconstructions at low computational cost but with smoothed
fine-scale features. Both conditional approaches require re-training, but they
reconstruct missing fine-scale features, are cycle-consistent with
observations, and possess the correct statistics such as energy spectra.
Further, their mean model errors are highly correlated with and predictable
from their ensemble standard deviations. Results highlight the trade-offs
between ease of implementation, fidelity (sharpness), and cycle-consistency of
the diffusion models, and offer practical guidance for deployment in
geophysical inverse problems.

</details>


<div id='astro-ph.SR'></div>

# astro-ph.SR [[Back]](#toc)

### [284] [Atmospheric model-trained machine learning selection and classification of ultracool TY dwarfs](https://arxiv.org/abs/2507.00957)
*Ankit Biswas*

Main category: astro-ph.SR

TL;DR: 提出新机器学习框架检测和分类晚期T和Y矮星，模型在合成和经验数据集上表现好，发现新候选体。


<details>
  <summary>Details</summary>
Motivation: T和Y光谱类褐矮星统计有限、普查不完整，现有检测框架多限于M、L和早期T矮星。

Method: 基于大气模型的合成测光训练模型，用多项式颜色关系分配光谱类型，训练分类器。

Result: 模型在合成和经验数据集验证表现佳，对象分类指标>99%，平均光谱类型精度在0.35 +/- 0.37子类型内，发现一个未编目的T8.2候选体。

Conclusion: 该模型训练方法能从测光目录中发现暗弱的晚期超冷矮星。

Abstract: The T and Y spectral classes represent the coolest and lowest-mass population
of brown dwarfs, yet their census remains incomplete due to limited statistics.
Existing detection frameworks are often constrained to identifying M, L, and
early T dwarfs, owing to the sparse observational sample of ultracool dwarfs
(UCDs) at later types. This paper presents a novel machine learning framework
capable of detecting and classifying late-T and Y dwarfs, trained entirely on
synthetic photometry from atmospheric models. Utilizing grids from the ATMO
2020 and Sonora Bobcat models, I produce a training dataset over two orders of
magnitude larger than any empirical set of >T6 UCDs. Polynomial color relations
fitted to the model photometry are used to assign spectral types to these
synthetic models, which in turn train an ensemble of classifiers to identify
and classify the spectral type of late UCDs. The model is highly performant
when validating on both synthetic and empirical datasets, verifying catalogs of
known UCDs with object classification metrics >99% and an average spectral type
precision within 0.35 +/- 0.37 subtypes. Application of the model to a 1.5
degree region around Pisces and the UKIDSS UDS field results in the discovery
of one previously uncatalogued T8.2 candidate, demonstrating the ability of
this model-trained approach in discovering faint, late-type UCDs from
photometric catalogs.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [285] [Edge Computing and its Application in Robotics: A Survey](https://arxiv.org/abs/2507.00523)
*Nazish Tahir,Ramviyas Parasuraman*

Main category: cs.RO

TL;DR: 本文聚焦边缘机器人领域，综合评估其发展，分析关键动机、挑战与未来方向，还探讨边缘计算在现实场景的重要性并列出研究挑战。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏对边缘计算融入机器人所带来优势的全面调查，本文旨在填补这一空白。

Method: 强调边缘机器人领域的重要工作，研究最新进展，深入分析当前和新兴解决方案背后的挑战与动机。

Result: 对边缘机器人近期发展进行全面评估，分析关键动机、挑战与未来方向，探讨边缘计算在现实场景重要性。

Conclusion: 列出边缘机器人领域各种开放的研究挑战。

Abstract: The Edge computing paradigm has gained prominence in both academic and
industry circles in recent years. By implementing edge computing facilities and
services in robotics, it becomes a key enabler in the deployment of artificial
intelligence applications to robots. Time-sensitive robotics applications
benefit from the reduced latency, mobility, and location awareness provided by
the edge computing paradigm, which enables real-time data processing and
intelligence at the network's edge. While the advantages of integrating edge
computing into robotics are numerous, there has been no recent survey that
comprehensively examines these benefits. This paper aims to bridge that gap by
highlighting important work in the domain of edge robotics, examining recent
advancements, and offering deeper insight into the challenges and motivations
behind both current and emerging solutions. In particular, this article
provides a comprehensive evaluation of recent developments in edge robotics,
with an emphasis on fundamental applications, providing in-depth analysis of
the key motivations, challenges, and future directions in this rapidly evolving
domain. It also explores the importance of edge computing in real-world
robotics scenarios where rapid response times are critical. Finally, the paper
outlines various open research challenges in the field of edge robotics.

</details>


### [286] [Control-Optimized Deep Reinforcement Learning for Artificially Intelligent Autonomous Systems](https://arxiv.org/abs/2507.00268)
*Oren Fivel,Matan Rudman,Kobi Cohen*

Main category: cs.RO

TL;DR: 本文提出一种控制优化的深度强化学习框架，考虑动作执行不匹配问题，在机械仿真环境评估，展现鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统深度强化学习方法常假设动作完美执行，忽略实际应用中动作执行的不确定性和偏差，影响性能。

Method: 建立两阶段过程，先确定期望动作，再选择合适控制信号；训练时考虑动作不匹配和控制器校正，优化期望动作。

Result: 在五个重构的开源机械仿真环境中评估，展示了对不确定性的鲁棒性。

Conclusion: 该框架缩小了理想学习与实际应用差距，使智能体在工程环境中能应对执行误差和系统干扰，为控制应用提供实用高效方案。

Abstract: Deep reinforcement learning (DRL) has become a powerful tool for complex
decision-making in machine learning and AI. However, traditional methods often
assume perfect action execution, overlooking the uncertainties and deviations
between an agent's selected actions and the actual system response. In
real-world applications, such as robotics, mechatronics, and communication
networks, execution mismatches arising from system dynamics, hardware
constraints, and latency can significantly degrade performance. This work
advances AI by developing a novel control-optimized DRL framework that
explicitly models and compensates for action execution mismatches, a challenge
largely overlooked in existing methods. Our approach establishes a structured
two-stage process: determining the desired action and selecting the appropriate
control signal to ensure proper execution. It trains the agent while accounting
for action mismatches and controller corrections. By incorporating these
factors into the training process, the AI agent optimizes the desired action
with respect to both the actual control signal and the intended outcome,
explicitly considering execution errors. This approach enhances robustness,
ensuring that decision-making remains effective under real-world uncertainties.
Our approach offers a substantial advancement for engineering practice by
bridging the gap between idealized learning and real-world implementation. It
equips intelligent agents operating in engineering environments with the
ability to anticipate and adjust for actuation errors and system disturbances
during training. We evaluate the framework in five widely used open-source
mechanical simulation environments we restructured and developed to reflect
real-world operating conditions, showcasing its robustness against
uncertainties and offering a highly practical and efficient solution for
control-oriented applications.

</details>


### [287] [RoboEval: Where Robotic Manipulation Meets Structured and Scalable Evaluation](https://arxiv.org/abs/2507.00435)
*Yi Ru Wang,Carter Ung,Grant Tannert,Jiafei Duan,Josephine Li,Amy Le,Rishabh Oswal,Markus Grotz,Wilbert Pumacay,Yuquan Deng,Ranjay Krishna,Dieter Fox,Siddhartha Srinivasa*

Main category: cs.RO

TL;DR: 提出RoboEval基准和框架，揭示双手机器人操作策略局限，实验表明其可深入理解策略行为。


<details>
  <summary>Details</summary>
Motivation: 现有双手机器人操作策略评估指标单一，仅报告任务成功与否，会掩盖策略行为的关键弱点，需新评估工具。

Method: 引入分层、语义相关任务，分解为特定技能阶段，有多种挑战能力的变体，配备细粒度诊断指标和大量人类演示。

Result: 成功率相似的策略在执行任务方式上有差异，行为指标与超半数任务 - 指标对的成功相关，二元成功饱和时仍有信息价值。

Conclusion: RoboEval能深入、可操作地理解机器人操作，强调需超越单纯成功的评估工具。

Abstract: We present RoboEval, a simulation benchmark and structured evaluation
framework designed to reveal the limitations of current bimanual manipulation
policies. While prior benchmarks report only binary task success, we show that
such metrics often conceal critical weaknesses in policy behavior -- such as
poor coordination, slipping during grasping, or asymmetric arm usage. RoboEval
introduces a suite of tiered, semantically grounded tasks decomposed into
skill-specific stages, with variations that systematically challenge spatial,
physical, and coordination capabilities. Tasks are paired with fine-grained
diagnostic metrics and 3000+ human demonstrations to support imitation
learning. Our experiments reveal that policies with similar success rates
diverge in how tasks are executed -- some struggle with alignment, others with
temporally consistent bimanual control. We find that behavioral metrics
correlate with success in over half of task-metric pairs, and remain
informative even when binary success saturates. By pinpointing when and how
policies fail, RoboEval enables a deeper, more actionable understanding of
robotic manipulation -- and highlights the need for evaluation tools that go
beyond success alone.

</details>


### [288] [Novel Pigeon-inspired 3D Obstacle Detection and Avoidance Maneuver for Multi-UAV Systems](https://arxiv.org/abs/2507.00443)
*Reza Ahmadvand,Sarah Safura Sharif,Yaser Mike Banad*

Main category: cs.RO

TL;DR: 受罗非鱼和鸽子集体行为启发，提出多无人机系统自然启发的无碰撞编队控制框架，应用于2D和3D场景，结果验证其在动态环境有效性。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统操纵发展使城市地区对多无人机系统需求上升，且存在静态和动态障碍物，需无碰撞编队控制。

Method: 采用半分布式控制方法，基于概率劳埃德算法的集中式引导算法实现无人机最优定位，分布式控制方法实现车辆间碰撞和避障；将框架扩展到3D空间并定义3D机动。

Result: 框架应用于2D和3D场景的多无人机系统，结果表明该方法在含静止和移动障碍物的动态环境中有效。

Conclusion: 所提出的自然启发的无碰撞编队控制框架可行，能在复杂动态环境中实现多无人机系统的避障和编队控制。

Abstract: Recent advances in multi-agent systems manipulation have demonstrated a
rising demand for the implementation of multi-UAV systems in urban areas, which
are always subjected to the presence of static and dynamic obstacles. Inspired
by the collective behavior of tilapia fish and pigeons, the focus of the
presented research is on the introduction of a nature-inspired collision-free
formation control for a multi-UAV system, considering the obstacle avoidance
maneuvers. The developed framework in this study utilizes a semi-distributed
control approach, in which, based on a probabilistic Lloyd's algorithm, a
centralized guidance algorithm works for optimal positioning of the UAVs, while
a distributed control approach has been used for the intervehicle collision and
obstacle avoidance. Further, the presented framework has been extended to the
3D space with a novel definition of 3D maneuvers. Finally, the presented
framework has been applied to multi-UAV systems in 2D and 3D scenarios, and the
obtained results demonstrated the validity of the presented method in dynamic
environments with stationary and moving obstacles.

</details>


### [289] [PI-WAN: A Physics-Informed Wind-Adaptive Network for Quadrotor Dynamics Prediction in Unknown Environments](https://arxiv.org/abs/2507.00816)
*Mengyun Wang,Bo Wang,Yifeng Niu,Chang Wang*

Main category: cs.RO

TL;DR: 提出物理信息风自适应网络（PI - WAN）结合知识驱动和数据驱动方法进行四旋翼动力学建模，通过模拟和实验验证其性能优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 传统物理知识驱动建模方法在未知环境有局限，数据驱动建模方法处理分布外数据泛化能力差，需新方法进行准确四旋翼动力学建模。

Method: 引入PI - WAN，将物理约束嵌入训练过程，采用TCN架构捕捉历史飞行数据时间依赖，用物理信息损失函数提升泛化和鲁棒性，将实时预测结果融入MPC框架。

Result: 综合模拟和真实飞行实验表明，该方法在预测准确性、跟踪精度和对未知环境的鲁棒性方面优于基线方法。

Conclusion: PI - WAN结合知识和数据驱动方法可有效提升四旋翼动力学建模性能，实现更好的闭环跟踪控制。

Abstract: Accurate dynamics modeling is essential for quadrotors to achieve precise
trajectory tracking in various applications. Traditional physical
knowledge-driven modeling methods face substantial limitations in unknown
environments characterized by variable payloads, wind disturbances, and
external perturbations. On the other hand, data-driven modeling methods suffer
from poor generalization when handling out-of-distribution (OoD) data,
restricting their effectiveness in unknown scenarios. To address these
challenges, we introduce the Physics-Informed Wind-Adaptive Network (PI-WAN),
which combines knowledge-driven and data-driven modeling methods by embedding
physical constraints directly into the training process for robust quadrotor
dynamics learning. Specifically, PI-WAN employs a Temporal Convolutional
Network (TCN) architecture that efficiently captures temporal dependencies from
historical flight data, while a physics-informed loss function applies physical
principles to improve model generalization and robustness across previously
unseen conditions. By incorporating real-time prediction results into a model
predictive control (MPC) framework, we achieve improvements in closed-loop
tracking performance. Comprehensive simulations and real-world flight
experiments demonstrate that our approach outperforms baseline methods in terms
of prediction accuracy, tracking precision, and robustness to unknown
environments.

</details>


### [290] [HumanoidGen: Data Generation for Bimanual Dexterous Manipulation via LLM Reasoning](https://arxiv.org/abs/2507.00833)
*Zhi Jing,Siyuan Yang,Jicong Ao,Ting Xiao,Yugang Jiang,Chenjia Bai*

Main category: cs.RO

TL;DR: 本文提出HumanoidGen框架，利用原子操作和大语言模型推理生成约束，创建新基准评估，结果表明扩散策略性能随生成数据集提升。


<details>
  <summary>Details</summary>
Motivation: 现有机器人数据集和模拟基准多针对机械臂平台，类人机器人双手灵巧操作的模拟任务和高质量演示缺乏，且双手灵巧操作复杂，自主数据收集困难。

Method: 提出HumanoidGen框架，基于原子操作对资产和灵巧手进行空间标注，用大语言模型规划器生成手臂运动的空间约束，用蒙特卡罗树搜索变体增强大语言模型推理。

Result: 创建新基准评估数据质量，2D和3D扩散策略的性能可随生成数据集提升。

Conclusion: HumanoidGen框架能有效生成数据，提升扩散策略性能，可用于类人机器人双手灵巧操作。

Abstract: For robotic manipulation, existing robotics datasets and simulation
benchmarks predominantly cater to robot-arm platforms. However, for humanoid
robots equipped with dual arms and dexterous hands, simulation tasks and
high-quality demonstrations are notably lacking. Bimanual dexterous
manipulation is inherently more complex, as it requires coordinated arm
movements and hand operations, making autonomous data collection challenging.
This paper presents HumanoidGen, an automated task creation and demonstration
collection framework that leverages atomic dexterous operations and LLM
reasoning to generate relational constraints. Specifically, we provide spatial
annotations for both assets and dexterous hands based on the atomic operations,
and perform an LLM planner to generate a chain of actionable spatial
constraints for arm movements based on object affordances and scenes. To
further improve planning ability, we employ a variant of Monte Carlo tree
search to enhance LLM reasoning for long-horizon tasks and insufficient
annotation. In experiments, we create a novel benchmark with augmented
scenarios to evaluate the quality of the collected data. The results show that
the performance of the 2D and 3D diffusion policies can scale with the
generated dataset. Project page is https://openhumanoidgen.github.io.

</details>


### [291] [RaGNNarok: A Light-Weight Graph Neural Network for Enhancing Radar Point Clouds on Unmanned Ground Vehicles](https://arxiv.org/abs/2507.00937)
*David Hunt,Shaocheng Luo,Spencer Hallyburton,Shafii Nillongo,Yi Li,Tingjun Chen,Miroslav Pajic*

Main category: cs.RO

TL;DR: 本文提出用于低成本室内移动机器人的实时轻量级框架RaGNNarok，能增强雷达点云，在资源受限设备上高效运行，实验证明其可靠且通用。


<details>
  <summary>Details</summary>
Motivation: 现有基于激光雷达和相机的室内移动机器人方案存在性能差、计算开销大、成本高等问题，基于雷达的定位也有稀疏点云、噪声和误检问题，因此需要更好的解决方案。

Method: 引入基于图神经网络（GNN）的框架RaGNNarok来增强雷达点云。

Result: RaGNNarok在低成本Raspberry Pi 5上推理时间仅7.3ms，在定位、SLAM和自主导航等关键任务的三个不同环境中表现出强可靠性和通用性。

Conclusion: RaGNNarok是低成本室内移动机器人的可靠解决方案。

Abstract: Low-cost indoor mobile robots have gained popularity with the increasing
adoption of automation in homes and commercial spaces. However, existing lidar
and camera-based solutions have limitations such as poor performance in
visually obscured environments, high computational overhead for data
processing, and high costs for lidars. In contrast, mmWave radar sensors offer
a cost-effective and lightweight alternative, providing accurate ranging
regardless of visibility. However, existing radar-based localization suffers
from sparse point cloud generation, noise, and false detections. Thus, in this
work, we introduce RaGNNarok, a real-time, lightweight, and generalizable graph
neural network (GNN)-based framework to enhance radar point clouds, even in
complex and dynamic environments. With an inference time of just 7.3 ms on the
low-cost Raspberry Pi 5, RaGNNarok runs efficiently even on such
resource-constrained devices, requiring no additional computational resources.
We evaluate its performance across key tasks, including localization, SLAM, and
autonomous navigation, in three different environments. Our results demonstrate
strong reliability and generalizability, making RaGNNarok a robust solution for
low-cost indoor mobile robots.

</details>


### [292] [Box Pose and Shape Estimation and Domain Adaptation for Large-Scale Warehouse Automation](https://arxiv.org/abs/2507.00984)
*Xihang Yu,Rajat Talak,Jingnan Shi,Ulrich Viereck,Igor Gilitschenski,Luca Carlone*

Main category: cs.RO

TL;DR: 本文开发自监督领域适应管道，用于估计盒子姿态和形状，经多场景评估，自监督模型表现优于仅在模拟中训练的模型和零样本基线。


<details>
  <summary>Details</summary>
Motivation: 现代仓库自动化系统中智能机器人产生大量未标注数据，需利用这些数据改进感知模型且无需手动标注。

Method: 开发自监督领域适应管道，提出用于自监督盒子姿态和形状估计的校正和认证管道。

Result: 自监督模型显著优于仅在模拟中训练的模型，且相较于零样本3D边界框估计基线有大幅改进。

Conclusion: 所提出的自监督方法能有效利用未标注数据提升盒子姿态和形状估计的感知模型性能。

Abstract: Modern warehouse automation systems rely on fleets of intelligent robots that
generate vast amounts of data -- most of which remains unannotated. This paper
develops a self-supervised domain adaptation pipeline that leverages
real-world, unlabeled data to improve perception models without requiring
manual annotations. Our work focuses specifically on estimating the pose and
shape of boxes and presents a correct-and-certify pipeline for self-supervised
box pose and shape estimation. We extensively evaluate our approach across a
range of simulated and real industrial settings, including adaptation to a
large-scale real-world dataset of 50,000 images. The self-supervised model
significantly outperforms models trained solely in simulation and shows
substantial improvements over a zero-shot 3D bounding box estimation baseline.

</details>


### [293] [Robotic Manipulation by Imitating Generated Videos Without Physical Demonstrations](https://arxiv.org/abs/2507.00990)
*Shivansh Patel,Shraddhaa Mohan,Hanlin Mai,Unnat Jain,Svetlana Lazebnik,Yunzhu Li*

Main category: cs.RO

TL;DR: 介绍RIGVid系统，让机器人通过模仿AI生成视频执行复杂操作任务，评估显示生成视频效果好。


<details>
  <summary>Details</summary>
Motivation: 让机器人在无物理演示和特定训练下执行复杂操作任务。

Method: 用视频扩散模型生成演示视频，VLM过滤结果，6D姿态跟踪器提取轨迹并以无关具身方式重定向到机器人。

Result: 过滤后的生成视频与真实演示效果相当，性能随生成质量提升；依赖生成视频优于VLM关键点预测，强6D姿态跟踪优于其他轨迹提取方法。

Conclusion: 最先进的现成模型生成的视频可为机器人操作提供有效监督。

Abstract: This work introduces Robots Imitating Generated Videos (RIGVid), a system
that enables robots to perform complex manipulation tasks--such as pouring,
wiping, and mixing--purely by imitating AI-generated videos, without requiring
any physical demonstrations or robot-specific training. Given a language
command and an initial scene image, a video diffusion model generates potential
demonstration videos, and a vision-language model (VLM) automatically filters
out results that do not follow the command. A 6D pose tracker then extracts
object trajectories from the video, and the trajectories are retargeted to the
robot in an embodiment-agnostic fashion. Through extensive real-world
evaluations, we show that filtered generated videos are as effective as real
demonstrations, and that performance improves with generation quality. We also
show that relying on generated videos outperforms more compact alternatives
such as keypoint prediction using VLMs, and that strong 6D pose tracking
outperforms other ways to extract trajectories, such as dense feature point
tracking. These findings suggest that videos produced by a state-of-the-art
off-the-shelf model can offer an effective source of supervision for robotic
manipulation.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [294] [HyperFusion: Hierarchical Multimodal Ensemble Learning for Social Media Popularity Prediction](https://arxiv.org/abs/2507.00926)
*Liliang Ye,Yunyao Zhang,Yafeng Wu,Yi-Ping Phoebe Chen,Junqing Yu,Wei Yang,Zikai Song*

Main category: cs.MM

TL;DR: 本文提出HyperFusion框架用于社交媒体热度预测，采用三层融合架构和分层集成策略，通过两阶段训练方法解决数据标注问题，实验取得良好效果。


<details>
  <summary>Details</summary>
Motivation: 社交媒体热度预测对内容优化等很重要，但因多种复杂因素影响，预测具有挑战性。

Method: 提出HyperFusion框架，采用三层融合架构整合不同特征，实现分层集成策略，提出两阶段训练方法，引入新的跨模态相似度度量和分层聚类特征。

Result: HyperFusion在SMP挑战数据集上有竞争力，团队获SMP Challenge 2025（图像赛道）第三名。

Conclusion: HyperFusion框架能有效进行社交媒体热度预测，具有一定优势。

Abstract: Social media popularity prediction plays a crucial role in content
optimization, marketing strategies, and user engagement enhancement across
digital platforms. However, predicting post popularity remains challenging due
to the complex interplay between visual, textual, temporal, and user behavioral
factors. This paper presents HyperFusion, a hierarchical multimodal ensemble
learning framework for social media popularity prediction. Our approach employs
a three-tier fusion architecture that progressively integrates features across
abstraction levels: visual representations from CLIP encoders, textual
embeddings from transformer models, and temporal-spatial metadata with user
characteristics. The framework implements a hierarchical ensemble strategy
combining CatBoost, TabNet, and custom multi-layer perceptrons. To address
limited labeled data, we propose a two-stage training methodology with
pseudo-labeling and iterative refinement. We introduce novel cross-modal
similarity measures and hierarchical clustering features that capture
inter-modal dependencies. Experimental results demonstrate that HyperFusion
achieves competitive performance on the SMP challenge dataset. Our team
achieved third place in the SMP Challenge 2025 (Image Track). The source code
is available at https://anonymous.4open.science/r/SMPDImage.

</details>


<div id='q-bio.PE'></div>

# q-bio.PE [[Back]](#toc)

### [295] [The Evolution of Altruistic Rationality Provides a Solution to Social Dilemmas via Rational Reciprocity](https://arxiv.org/abs/2507.00858)
*Mohammad Salahshour,Iain D. Couzin*

Main category: q-bio.PE

TL;DR: 本文用进化模型研究理性主体群体中利他主义如何进化并促进合作，发现理性主体群体易被有扭曲收益感知的突变个体入侵，进而产生关注他人的理性主体。


<details>
  <summary>Details</summary>
Motivation: 以往研究多关注有限理性个体间合作的进化，本文旨在研究理性主体群体中利他主义如何进化并促进合作。

Method: 使用进化模型进行研究。

Result: 在混合和结构化种群中，客观理性的主体群体易被有扭曲收益感知的突变个体入侵，促进行为多样性，产生能自然解决两人两策略博弈战略问题的关注他人的理性主体。

Conclusion: 在理性主体群体中，通过突变个体的入侵可促进利他主义进化和合作。

Abstract: Decades of scientific inquiry have sought to understand how evolution fosters
cooperation, a concept seemingly at odds with the belief that evolution should
produce rational, self-interested individuals. Most previous work has focused
on the evolution of cooperation among boundedly rational individuals whose
decisions are governed by behavioral rules that do not need to be rational.
Here, using an evolutionary model, we study how altruism can evolve in a
community of rational agents and promote cooperation. We show that in both
well-mixed and structured populations, a population of objectively rational
agents is readily invaded by mutant individuals who make rational decisions but
evolve a distorted (i.e., subjective) perception of their payoffs. This
promotes behavioral diversity and gives rise to the evolution of rational,
other-regarding agents who naturally solve all the known strategic problems of
two-person, two-strategy games by perceiving their games as pure coordination
games.

</details>


<div id='nlin.AO'></div>

# nlin.AO [[Back]](#toc)

### [296] [Hebbian Physics Networks: A Self-Organizing Computational Architecture Based on Local Physical Laws](https://arxiv.org/abs/2507.00641)
*Gunjan Auti,Hirofumi Daiguji,Gouhei Tanaka*

Main category: nlin.AO

TL;DR: 提出Hebbian Physics Network (HPN)框架，以局部Hebbian更新学习，在无监督下可形成物理一致结构，为建模复杂动力系统提供新选择。


<details>
  <summary>Details</summary>
Motivation: 传统物理机器学习方法依赖全局优化，限制可解释性且需外部施加物理约束。

Method: 引入HPN框架，将物理定律编码到局部动力学中，用残差作为热力学信号驱动权重调整。

Result: 在不可压缩流体流动和连续扩散问题上，从随机初始条件无监督地出现物理一致结构。

Conclusion: HPN将计算重构为残差驱动的热力学过程，是建模复杂动力系统可解释、可扩展且基于物理的替代方案。

Abstract: Traditional machine learning approaches in physics rely on global
optimization, limiting interpretability and enforcing physical constraints
externally. We introduce the Hebbian Physics Network (HPN), a self-organizing
computational framework in which learning emerges from local Hebbian updates
driven by violations of conservation laws. Grounded in non-equilibrium
thermodynamics and inspired by Prigogine/'s theory of dissipative structures,
HPNs eliminate the need for global loss functions by encoding physical laws
directly into the system/'s local dynamics. Residuals - quantified imbalances
in continuity, momentum, or energy - serve as thermodynamic signals that drive
weight adaptation through generalized Hebbian plasticity. We demonstrate this
approach on incompressible fluid flow and continuum diffusion, where physically
consistent structures emerge from random initial conditions without
supervision. HPNs reframe computation as a residual-driven thermodynamic
process, offering an interpretable, scalable, and physically grounded
alternative for modeling complex dynamical systems.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [297] [Augmented Physics-Based Li-ion Battery Model via Adaptive Ensemble Sparse Learning and Conformal Prediction](https://arxiv.org/abs/2507.00353)
*Samuel Filgueira da Silva,Mehmet Fatih Ozkan,Faissal El Idrissi,Marcello Canova*

Main category: eess.SY

TL;DR: 本文提出AESI框架增强锂离子电池降阶模型准确性，构建混合模型，评估显示提升了电压预测准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 降阶模型难以捕捉复杂非线性行为，需提升锂离子电池模型准确性。

Method: 提出AESI框架，集成扩展单粒子模型与进化集成稀疏学习策略构建混合模型，结合共形预测方法。

Result: 混合模型提高电压预测准确性，均方误差最多降低46%；共形预测支持预测可靠性，覆盖比率分别达96.85%和97.41%。

Conclusion: AESI框架能有效提升锂离子电池降阶模型的准确性和预测可靠性。

Abstract: Accurate electrochemical models are essential for the safe and efficient
operation of lithium-ion batteries in real-world applications such as
electrified vehicles and grid storage. Reduced-order models (ROM) offer a
balance between fidelity and computational efficiency but often struggle to
capture complex and nonlinear behaviors, such as the dynamics in the cell
voltage response under high C-rate conditions. To address these limitations,
this study proposes an Adaptive Ensemble Sparse Identification (AESI) framework
that enhances the accuracy of reduced-order li-ion battery models by
compensating for unpredictable dynamics. The approach integrates an Extended
Single Particle Model (ESPM) with an evolutionary ensemble sparse learning
strategy to construct a robust hybrid model. In addition, the AESI framework
incorporates a conformal prediction method to provide theoretically guaranteed
uncertainty quantification for voltage error dynamics, thereby improving the
reliability of the model's predictions. Evaluation across diverse operating
conditions shows that the hybrid model (ESPM + AESI) improves the voltage
prediction accuracy, achieving mean squared error reductions of up to 46% on
unseen data. Prediction reliability is further supported by conformal
prediction, yielding statistically valid prediction intervals with coverage
ratios of 96.85% and 97.41% for the ensemble models based on bagging and
stability selection, respectively.

</details>


### [298] [Constellation as a Service: Tailored Connectivity Management in Direct-Satellite-to-Device Networks](https://arxiv.org/abs/2507.00902)
*Feng Wang,Shengyu Zhang,Een-Kee Hong,Tony Q. S. Quek*

Main category: eess.SY

TL;DR: 本文提出CaaS框架应对多星座DS2D连接管理挑战，模拟显示该框架可提升服务率并降低切换开销。


<details>
  <summary>Details</summary>
Motivation: 多星座DS2D连接管理存在高干扰、频繁切换问题，现有方法局限于单星座，导致服务性能不佳。

Method: 提出CaaS框架，将多星座基础设施视为共享资源池，动态形成最优子星座；采用生成式人工智能进行预测性卫星波束赋形和预配置切换路径策略。

Result: 模拟结果表明CaaS显著提高卫星服务率，降低切换开销。

Conclusion: CaaS是管理多星座环境中DS2D连接的高效且可持续的解决方案。

Abstract: Direct-satellite-to-device (DS2D) communication is emerging as a promising
solution for global mobile service extension, leveraging the deployment of
satellite constellations. However, the challenge of managing DS2D connectivity
for multi-constellations becomes outstanding, including high interference and
frequent handovers caused by multi-coverage overlap and rapid satellite
movement. Moreover, existing approaches primarily operate within
single-constellation shell, which inherently limits the ability to exploit the
vast potential of multi-constellation connectivity provision, resulting in
suboptimal DS2D service performances. To address these challenges, this article
proposes a Constellation as a Service (CaaS) framework, which treats the entire
multi-constellation infrastructure as a shared resource pool and dynamically
forms optimal sub-constellations (SCs) for each DS2D service region. The
formation of each SC integrates satellites from various orbits to provide
tailored connectivity based on user demands, guided by two innovative
strategies: predictive satellite beamforming using generative artificial
intelligence (GenAI) and pre-configured handover path for efficient satellite
access and mobility management. Simulation results demonstrate that CaaS
significantly improves satellite service rates while reducing handover
overhead, making it an efficient and continuable solution for managing DS2D
connectivity in multi-constellation environments.

</details>


<div id='econ.TH'></div>

# econ.TH [[Back]](#toc)

### [299] [Reconfiguring Digital Accountability: AI-Powered Innovations and Transnational Governance in a Postnational Accounting Context](https://arxiv.org/abs/2507.00288)
*Claire Li,David Freeborn*

Main category: econ.TH

TL;DR: 研究AI数字创新如何重塑跨国治理中的组织问责制，结合多理论提出策略。


<details>
  <summary>Details</summary>
Motivation: AI系统介入决策使传统问责机制不稳定，需研究组织如何应对跨国压力采用AI技术。

Method: 整合技术接受模型、行动者网络理论和制度理论进行研究，扩展TAM，借鉴ANT重新定义问责制。

Result: 认为问责制在全球社会技术网络中共同构建，提出内部治理重构和外部行动者网络参与两种策略。

Conclusion: 所提策略可促进会计领域负责任、合法且全球认可的AI应用。

Abstract: This study explores how AI-powered digital innovations are reshaping
organisational accountability in a transnational governance context. As AI
systems increasingly mediate decision-making in domains such as auditing and
financial reporting, traditional mechanisms of accountability, based on
control, transparency, and auditability, are being destabilised. We integrate
the Technology Acceptance Model (TAM), Actor-Network Theory (ANT), and
institutional theory to examine how organisations adopt AI technologies in
response to regulatory, ethical, and cultural pressures that transcend national
boundaries. We argue that accountability is co-constructed within global
socio-technical networks, shaped not only by user perceptions but also by
governance logics and normative expectations. Extending TAM, we incorporate
compliance and legitimacy as key factors in perceived usefulness and usability.
Drawing on ANT, we reconceptualise accountability as a relational and emergent
property of networked assemblages. We propose two organisational strategies
including internal governance reconfiguration and external actor-network
engagement to foster responsible, legitimate, and globally accepted AI adoption
in the accounting domain.

</details>
