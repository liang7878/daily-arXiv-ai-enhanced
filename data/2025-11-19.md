<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 31]
- [cs.CE](#cs.CE) [Total: 4]
- [cs.DB](#cs.DB) [Total: 8]
- [cs.DC](#cs.DC) [Total: 16]
- [cs.IR](#cs.IR) [Total: 8]
- [cs.LG](#cs.LG) [Total: 89]
- [cs.NE](#cs.NE) [Total: 3]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.SE](#cs.SE) [Total: 14]
- [q-fin.ST](#q-fin.ST) [Total: 2]
- [stat.ML](#stat.ML) [Total: 9]
- [stat.CO](#stat.CO) [Total: 3]
- [q-bio.NC](#q-bio.NC) [Total: 4]
- [math.OC](#math.OC) [Total: 6]
- [eess.SY](#eess.SY) [Total: 5]
- [cs.NI](#cs.NI) [Total: 1]
- [cs.LO](#cs.LO) [Total: 4]
- [cs.SD](#cs.SD) [Total: 5]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [cs.IT](#cs.IT) [Total: 1]
- [cs.OS](#cs.OS) [Total: 1]
- [physics.optics](#physics.optics) [Total: 1]
- [cs.CR](#cs.CR) [Total: 9]
- [cs.CY](#cs.CY) [Total: 5]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [econ.TH](#econ.TH) [Total: 1]
- [cs.DL](#cs.DL) [Total: 2]
- [q-bio.QM](#q-bio.QM) [Total: 3]
- [q-bio.GN](#q-bio.GN) [Total: 2]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [cond-mat.soft](#cond-mat.soft) [Total: 1]
- [eess.SP](#eess.SP) [Total: 6]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.HC](#cs.HC) [Total: 4]
- [cs.RO](#cs.RO) [Total: 9]
- [cs.CV](#cs.CV) [Total: 46]
- [math.NA](#math.NA) [Total: 1]
- [cs.CG](#cs.CG) [Total: 1]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [cs.CL](#cs.CL) [Total: 20]
- [eess.IV](#eess.IV) [Total: 1]
- [econ.GN](#econ.GN) [Total: 3]
- [cs.MM](#cs.MM) [Total: 2]
- [cs.ET](#cs.ET) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [CORGI: Efficient Pattern Matching With Quadratic Guarantees](https://arxiv.org/abs/2511.13942)
*Daniel Weitekamp*

Main category: cs.AI

TL;DR: 为解决规则匹配问题，引入新匹配算法CORGI，在性能评估中表现优于RETE。


<details>
  <summary>Details</summary>
Motivation: 规则匹配系统在实时应用中有复杂匹配问题，自动生成规则易产生最坏匹配模式，影响程序执行，需实用算法。

Method: 引入CORGI算法，采用两步法，前向构建/维护关系图，反向迭代生成匹配，无传统β - 内存。

Result: 在简单组合匹配任务中，CORGI显著优于SOAR和OPS5的RETE实现。

Conclusion: CORGI算法能避免高延迟和内存溢出，为规则匹配系统提供实用解决方案。

Abstract: Rule-based systems must solve complex matching problems within tight time constraints to be effective in real-time applications, such as planning and reactive control for AI agents, as well as low-latency relational database querying. Pattern-matching systems can encounter issues where exponential time and space are required to find matches for rules with many underconstrained variables, or which produce combinatorial intermediate partial matches (but are otherwise well-constrained). When online AI systems automatically generate rules from example-driven induction or code synthesis, they can easily produce worst-case matching patterns that slow or halt program execution by exceeding available memory. In our own work with cognitive systems that learn from example, we've found that aggressive forms of anti-unification-based generalization can easily produce these circumstances. To make these systems practical without hand-engineering constraints or succumbing to unpredictable failure modes, we introduce a new matching algorithm called CORGI (Collection-Oriented Relational Graph Iteration). Unlike RETE-based approaches, CORGI offers quadratic time and space guarantees for finding single satisficing matches, and the ability to iteratively stream subsequent matches without committing entire conflict sets to memory. CORGI differs from RETE in that it does not have a traditional $β$-memory for collecting partial matches. Instead, CORGI takes a two-step approach: a graph of grounded relations is built/maintained in a forward pass, and an iterator generates matches as needed by working backward through the graph. This approach eliminates the high-latency delays and memory overflows that can result from populating full conflict sets. In a performance evaluation, we demonstrate that CORGI significantly outperforms RETE implementations from SOAR and OPS5 on a simple combinatorial matching task.

</details>


### [2] [Imagine in Space: Exploring the Frontier of Spatial Intelligence and Reasoning Efficiency in Vision Language Models](https://arxiv.org/abs/2511.13782)
*Xiaoxing Lian,Aidong Yang,Jun Zhu,Peng Wang,Yue Zhang*

Main category: cs.AI

TL;DR: 当前先进的视觉语言模型在空间推理上存在挑战，本文引入SpatiaLite基准测试，揭示模型问题并提出Imagery Driven Framework。


<details>
  <summary>Details</summary>
Motivation: 当前先进的视觉语言模型在空间推理方面存在显著挑战，需系统探究其空间推理机制。

Method: 引入SpatiaLite基准测试来衡量空间推理准确性和效率，并提出Imagery Driven Framework进行数据合成和训练。

Result: 发现先进的视觉语言模型主要依赖语言表征推理，在视觉中心任务存在缺陷，推理机制效率低，且提出的IDF可隐式构建内部世界模型。

Conclusion: 本文明确了先进视觉语言模型的空间推理极限和模式，指出关键不足，为未来发展提供参考。

Abstract: Large language models (LLMs) and vision language models (VLMs), such as DeepSeek R1,OpenAI o3, and Gemini 2.5 Pro, have demonstrated remarkable reasoning capabilities across logical inference, problem solving, and decision making. However, spatial reasoning:a fundamental component of human cognition that includes mental rotation, navigation, and spatial relationship comprehension remains a significant challenge for current advanced VLMs. We hypothesize that imagination, the internal simulation of spatial states, is the dominant reasoning mechanism within a spatial world model. To test this hypothesis and systematically probe current VLM spatial reasoning mechanisms, we introduce SpatiaLite, a fully synthetic benchmark that jointly measures spatial reasoning accuracy and reasoning efficiency. Comprehensive experiments reveal three key findings. First, advanced VLMs predominantly rely on linguistic representations for reasoning and imagination, resulting in significant deficiencies on visual centric tasks that demand perceptual spatial relations and 3D geometry transformations such as mental rotation or projection prediction. Second, advanced VLMs exhibit severe inefficiency in their current spatial reasoning mechanisms, with token usage growing rapidly as transformation complexity increases. Third, we propose an Imagery Driven Framework (IDF) for data synthesis and training, which can implicitly construct an internal world model that is critical for spatial reasoning in VLMs. Building on SpatiaLite, this work delineates the spatial reasoning limits and patterns of advanced VLMs, identifies key shortcomings, and informs future advances

</details>


### [3] [KANGURA: Kolmogorov-Arnold Network-Based Geometry-Aware Learning with Unified Representation Attention for 3D Modeling of Complex Structures](https://arxiv.org/abs/2511.13798)
*Mohammad Reza Shafie,Morteza Hajiabadi,Hamed Khosravi,Mobina Noori,Imtiaz Ahmed*

Main category: cs.AI

TL;DR: 本文提出KANGURA用于3D机器学习建模以优化微生物燃料电池阳极结构，实验表明其性能优于多个SOTA模型。


<details>
  <summary>Details</summary>
Motivation: 现有预测模型难以捕捉优化微生物燃料电池阳极结构所需的复杂几何依赖关系。

Method: 提出KANGURA，将预测作为函数分解问题，采用基于KAN的表示学习重构几何关系，通过几何解缠表示学习分离结构变化，用统一注意力机制增强关键几何区域。

Result: KANGURA在ModelNet40基准数据集上准确率达92.7%，在真实世界MFC阳极结构问题中准确率达97%，优于15个SOTA模型。

Conclusion: KANGURA是一个强大的3D几何建模框架，为先进制造和质量驱动工程应用中优化复杂结构带来新可能。

Abstract: Microbial Fuel Cells (MFCs) offer a promising pathway for sustainable energy generation by converting organic matter into electricity through microbial processes. A key factor influencing MFC performance is the anode structure, where design and material properties play a crucial role. Existing predictive models struggle to capture the complex geometric dependencies necessary to optimize these structures. To solve this problem, we propose KANGURA: Kolmogorov-Arnold Network-Based Geometry-Aware Learning with Unified Representation Attention. KANGURA introduces a new approach to three-dimensional (3D) machine learning modeling. It formulates prediction as a function decomposition problem, where Kolmogorov-Arnold Network (KAN)- based representation learning reconstructs geometric relationships without a conventional multi- layer perceptron (MLP). To refine spatial understanding, geometry-disentangled representation learning separates structural variations into interpretable components, while unified attention mechanisms dynamically enhance critical geometric regions. Experimental results demonstrate that KANGURA outperforms over 15 state-of-the-art (SOTA) models on the ModelNet40 benchmark dataset, achieving 92.7% accuracy, and excels in a real-world MFC anode structure problem with 97% accuracy. This establishes KANGURA as a robust framework for 3D geometric modeling, unlocking new possibilities for optimizing complex structures in advanced manufacturing and quality-driven engineering applications.

</details>


### [4] [When AI Does Science: Evaluating the Autonomous AI Scientist KOSMOS in Radiation Biology](https://arxiv.org/abs/2511.13825)
*Humza Nusrat,Omar Nusrat*

Main category: cs.AI

TL;DR: 使用简单随机基因空基准评估KOSMOS自主AI科学家在辐射生物学三个问题上的表现，得出一个有支持的发现、一个合理但不确定的结果和一个错误假设，表明AI科学家需严格审核。


<details>
  <summary>Details</summary>
Motivation: 评估KOSMOS自主AI科学家在辐射生物学问题上的能力。

Method: 用简单随机基因空基准评估KOSMOS，针对三个辐射生物学问题提出假设并验证。

Result: DDR - p53假设不成立；OGT关联弱，CDO1关联强；12 - 基因特征有一定效果但效应大小不唯一。

Conclusion: AI科学家能产生有用想法，但需对照适当零模型进行严格审核。

Abstract: Agentic AI "scientists" now use language models to search the literature, run analyses, and generate hypotheses. We evaluate KOSMOS, an autonomous AI scientist, on three problems in radiation biology using simple random-gene null benchmarks. Hypothesis 1: baseline DNA damage response (DDR) capacity across cell lines predicts the p53 transcriptional response after irradiation (GSE30240). Hypothesis 2: baseline expression of OGT and CDO1 predicts the strength of repressed and induced radiation-response modules in breast cancer cells (GSE59732). Hypothesis 3: a 12-gene expression signature predicts biochemical recurrence-free survival after prostate radiotherapy plus androgen deprivation therapy (GSE116918). The DDR-p53 hypothesis was not supported: DDR score and p53 response were weakly negatively correlated (Spearman rho = -0.40, p = 0.76), indistinguishable from random five-gene scores. OGT showed only a weak association (r = 0.23, p = 0.34), whereas CDO1 was a clear outlier (r = 0.70, empirical p = 0.0039). The 12-gene signature achieved a concordance index of 0.61 (p = 0.017) but a non-unique effect size. Overall, KOSMOS produced one well-supported discovery, one plausible but uncertain result, and one false hypothesis, illustrating that AI scientists can generate useful ideas but require rigorous auditing against appropriate null models.

</details>


### [5] [Causal computations in Semi Markovian Structural Causal Models using divide and conquer](https://arxiv.org/abs/2511.13852)
*Anna Rodum Bjøru,Rafael Cabañas,Helge Langseth,Antonio Salmerón*

Main category: cs.AI

TL;DR: 研究将Bjøru等人针对马尔可夫模型的分治算法扩展到半马尔可夫结构因果模型，说明扩展挑战并评估替代策略。


<details>
  <summary>Details</summary>
Motivation: Bjøru等人的算法用于马尔可夫模型，半马尔可夫模型能表示马尔可夫模型无法表示的混杂关系，故研究将该算法扩展到半马尔可夫模型。

Method: 用最小示例说明扩展挑战，提出替代策略并进行理论和计算研究评估。

Result: 文中未明确提及具体结果。

Conclusion: 文中未明确给出结论。

Abstract: Recently, Bjøru et al. proposed a novel divide-and-conquer algorithm for bounding counterfactual probabilities in structural causal models (SCMs). They assumed that the SCMs were learned from purely observational data, leading to an imprecise characterization of the marginal distributions of exogenous variables. Their method leveraged the canonical representation of structural equations to decompose a general SCM with high-cardinality exogenous variables into a set of sub-models with low-cardinality exogenous variables. These sub-models had precise marginals over the exogenous variables and therefore admitted efficient exact inference. The aggregated results were used to bound counterfactual probabilities in the original model. The approach was developed for Markovian models, where each exogenous variable affects only a single endogenous variable. In this paper, we investigate extending the methodology to \textit{semi-Markovian} SCMs, where exogenous variables may influence multiple endogenous variables. Such models are capable of representing confounding relationships that Markovian models cannot. We illustrate the challenges of this extension using a minimal example, which motivates a set of alternative solution strategies. These strategies are evaluated both theoretically and through a computational study.

</details>


### [6] [Jailbreaking Large Vision Language Models in Intelligent Transportation Systems](https://arxiv.org/abs/2511.13892)
*Badhan Chandra Das,Md Tasnim Jawad,Md Jueal Mia,M. Hadi Amini,Yanzhao Wu*

Main category: cs.AI

TL;DR: 本文分析智能交通系统中大型视觉语言模型（LVLMs）在越狱攻击下的漏洞，提出新攻击方法和多层响应过滤防御技术，并进行实验评估。


<details>
  <summary>Details</summary>
Motivation: LVLMs在多模态推理和实际应用中表现出色，但易受越狱攻击，需分析其在智能交通系统中的漏洞。

Method: 构建交通相关有害查询数据集，提出通过图像排版操纵和多轮提示的越狱攻击方法，以及多层响应过滤防御技术，用GPT - 4判断和人工验证评估。

Result: 对现有LVLMs进行攻击和防御实验，比较提出的越狱方法与现有技术，凸显图像排版操纵和多轮提示越狱攻击带来的严重安全风险。

Conclusion: 智能交通系统中集成的LVLMs面临越狱攻击的严重安全风险，提出的防御技术可防止模型生成不当响应。

Abstract: Large Vision Language Models (LVLMs) demonstrate strong capabilities in multimodal reasoning and many real-world applications, such as visual question answering. However, LVLMs are highly vulnerable to jailbreaking attacks. This paper systematically analyzes the vulnerabilities of LVLMs integrated in Intelligent Transportation Systems (ITS) under carefully crafted jailbreaking attacks. First, we carefully construct a dataset with harmful queries relevant to transportation, following OpenAI's prohibited categories to which the LVLMs should not respond. Second, we introduce a novel jailbreaking attack that exploits the vulnerabilities of LVLMs through image typography manipulation and multi-turn prompting. Third, we propose a multi-layered response filtering defense technique to prevent the model from generating inappropriate responses. We perform extensive experiments with the proposed attack and defense on the state-of-the-art LVLMs (both open-source and closed-source). To evaluate the attack method and defense technique, we use GPT-4's judgment to determine the toxicity score of the generated responses, as well as manual verification. Further, we compare our proposed jailbreaking method with existing jailbreaking techniques and highlight severe security risks involved with jailbreaking attacks with image typography manipulation and multi-turn prompting in the LVLMs integrated in ITS.

</details>


### [7] [Making Evidence Actionable in Adaptive Learning](https://arxiv.org/abs/2511.14052)
*Amirreza Mehrabi,Jason W. Morphew,Breejha Quezada,N. Sanjay Rebello*

Main category: cs.AI

TL;DR: 研究提出教师主导反馈循环，将概念评估转化为微干预，设计含保障的自适应学习算法，用不同求解方法，模拟和实践表明能实现技能覆盖，不同方法各有优势，得到可审计控制器。


<details>
  <summary>Details</summary>
Motivation: 解决自适应学习诊断精确但干预弱、帮助时机或内容不匹配的问题。

Method: 提出含充分性、注意力、多样性保障的自适应学习算法，将干预分配形式化为带约束的二进制整数规划，采用贪心选择、基于梯度的松弛及混合方法求解。

Result: 模拟和物理课程实践中，两种求解器在有限时间内让所有学习者实现技能覆盖，梯度法减少冗余，贪心在资源稀缺时计算成本低，松弛变量支持内容筛选。

Conclusion: 得到可审计控制器，能闭合诊断 - 教学循环，在课堂规模实现公平、考虑负载的个性化学习。

Abstract: Adaptive learning often diagnoses precisely yet intervenes weakly, yielding help that is mistimed or misaligned. This study presents evidence supporting an instructor-governed feedback loop that converts concept-level assessment evidence into vetted micro-interventions. The adaptive learning algorithm contains three safeguards: adequacy as a hard guarantee of gap closure, attention as a budgeted constraint for time and redundancy, and diversity as protection against overfitting to a single resource. We formalize intervention assignment as a binary integer program with constraints for coverage, time, difficulty windows informed by ability estimates, prerequisites encoded by a concept matrix, and anti-redundancy enforced through diversity. Greedy selection serves low-richness and tight-latency regimes, gradient-based relaxation serves rich repositories, and a hybrid method transitions along a richness-latency frontier. In simulation and in an introductory physics deployment with one thousand two hundred four students, both solvers achieved full skill coverage for essentially all learners within bounded watch time. The gradient-based method reduced redundant coverage by approximately twelve percentage points relative to greedy and harmonized difficulty across slates, while greedy delivered comparable adequacy with lower computational cost in scarce settings. Slack variables localized missing content and supported targeted curation, sustaining sufficiency across subgroups. The result is a tractable and auditable controller that closes the diagnostic-pedagogical loop and delivers equitable, load-aware personalization at classroom scale.

</details>


### [8] [PRISM: Prompt-Refined In-Context System Modelling for Financial Retrieval](https://arxiv.org/abs/2511.14130)
*Chun Chet Ng,Jia Yu Lim,Wei Zeng Low*

Main category: cs.AI

TL;DR: 介绍用于金融信息检索的无训练框架PRISM，在数据集上取得较好结果且适用于生产规模检索。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型发展，从冗长金融文件中提取相关信息对决策至关重要，FinAgentBench数据集定义了相关问题。

Method: 提出无训练框架PRISM，集成精炼系统提示、上下文学习和轻量级多智能体系统。

Result: 最佳配置在受限验证集上NDCG@5达到0.71818。

Conclusion: PRISM对生产规模金融检索可行且稳健，模块化、仅推理的设计适用于实际场景。

Abstract: With the rapid progress of large language models (LLMs), financial information retrieval has become a critical industrial application. Extracting task-relevant information from lengthy financial filings is essential for both operational and analytical decision-making. The FinAgentBench dataset formalizes this problem through two tasks: document ranking and chunk ranking. We present PRISM, a training-free framework that integrates refined system prompting, in-context learning (ICL), and a lightweight multi-agent system. Each component is examined extensively to reveal their synergies: prompt engineering provides precise task instructions, ICL supplies semantically relevant few-shot examples, and the multi-agent system models coordinated scoring behaviour. Our best configuration achieves an NDCG@5 of 0.71818 on the restricted validation split. We further demonstrate that PRISM is feasible and robust for production-scale financial retrieval. Its modular, inference-only design makes it practical for real-world use cases. The source code is released at https://bit.ly/prism-ailens.

</details>


### [9] [Scene Graph-Guided Generative AI Framework for Synthesizing and Evaluating Industrial Hazard Scenarios](https://arxiv.org/abs/2511.13970)
*Sanjay Acharjee,Abir Khan Ratul,Diego Patino,Md Nazmus Sakib*

Main category: cs.AI

TL;DR: 研究提出场景图引导的生成式AI框架，利用OSHA事故报告合成危险场景图像，并引入VQA框架评估，VQA Graph Score表现更优。


<details>
  <summary>Details</summary>
Motivation: 获取用于训练视觉模型检测工作场所危险的真实图像数据集困难，需新方法解决。

Method: 用GPT - 4o分析OSHA叙述提取结构化危险推理，转换为场景图，引导文本到图像扩散模型生成危险场景；引入VQA框架评估生成数据。

Result: 在四个最先进的生成模型中，提出的VQA Graph Score基于熵验证优于CLIP和BLIP指标。

Conclusion: 提出的框架能有效合成危险场景图像，VQA Graph Score具有更高判别灵敏度。

Abstract: Training vision models to detect workplace hazards accurately requires realistic images of unsafe conditions that could lead to accidents. However, acquiring such datasets is difficult because capturing accident-triggering scenarios as they occur is nearly impossible. To overcome this limitation, this study presents a novel scene graph-guided generative AI framework that synthesizes photorealistic images of hazardous scenarios grounded in historical Occupational Safety and Health Administration (OSHA) accident reports. OSHA narratives are analyzed using GPT-4o to extract structured hazard reasoning, which is converted into object-level scene graphs capturing spatial and contextual relationships essential for understanding risk. These graphs guide a text-to-image diffusion model to generate compositionally accurate hazard scenes. To evaluate the realism and semantic fidelity of the generated data, a visual question answering (VQA) framework is introduced. Across four state-of-the-art generative models, the proposed VQA Graph Score outperforms CLIP and BLIP metrics based on entropy-based validation, confirming its higher discriminative sensitivity.

</details>


### [10] [Artificial Intelligence Agents in Music Analysis: An Integrative Perspective Based on Two Use Cases](https://arxiv.org/abs/2511.13987)
*Antonio Manuel Martínez-Heredia,Dolores Godrid Rodríguez,Andrés Ortiz García*

Main category: cs.AI

TL;DR: 本文对应用于音乐分析与教育的人工智能代理进行综合回顾与实验验证，评估其教学意义，实验表明其优于传统方法，同时指出挑战并提供框架指导。


<details>
  <summary>Details</summary>
Motivation: 对应用于音乐分析和教育的人工智能代理进行研究，评估其教学意义，建立统一框架。

Method: 综合回顾从基于规则模型到当代方法的历史演变，采用双案例方法，包括在中学使用生成式AI平台和设计多智能体系统进行符号音乐分析。

Result: AI代理有效增强音乐模式识别、作曲参数化和教育反馈，在可解释性和适应性方面优于传统自动化方法。

Conclusion: 研究强调在教育环境中负责任地部署AI的必要性，提供基于证据的指导，建立统一框架。

Abstract: This paper presents an integrative review and experimental validation of artificial intelligence (AI) agents applied to music analysis and education. We synthesize the historical evolution from rule-based models to contemporary approaches involving deep learning, multi-agent architectures, and retrieval-augmented generation (RAG) frameworks. The pedagogical implications are evaluated through a dual-case methodology: (1) the use of generative AI platforms in secondary education to foster analytical and creative skills; (2) the design of a multiagent system for symbolic music analysis, enabling modular, scalable, and explainable workflows.
  Experimental results demonstrate that AI agents effectively enhance musical pattern recognition, compositional parameterization, and educational feedback, outperforming traditional automated methods in terms of interpretability and adaptability. The findings highlight key challenges concerning transparency, cultural bias, and the definition of hybrid evaluation metrics, emphasizing the need for responsible deployment of AI in educational environments.
  This research contributes to a unified framework that bridges technical, pedagogical, and ethical considerations, offering evidence-based guidance for the design and application of intelligent agents in computational musicology and music education.

</details>


### [11] [ALEX:A Light Editing-knowledge Extractor](https://arxiv.org/abs/2511.14018)
*Minghu Wang,Shuliang Zhao,Yuanyuan Zhao,Hongxia Xu*

Main category: cs.AI

TL;DR: 本文提出轻量级知识编辑框架ALEX，解决大语言模型知识编辑扩展性和检索效率问题，实验表明其有显著效果。


<details>
  <summary>Details</summary>
Motivation: 大语言模型知识静态，难以适应信息变化，现有知识编辑方法在处理复杂多跳问题时面临扩展性和检索效率挑战。

Method: 引入ALEX框架，采用分层内存架构组织知识更新，集成IQS模块和DEA引擎。

Result: 在MQUAKE基准测试中，ALEX显著提升多跳答案准确性和推理路径可靠性，减少超80%搜索空间。

Conclusion: ALEX为构建可扩展、高效和准确的知识编辑系统提供了有前景的途径。

Abstract: The static nature of knowledge within Large Language Models (LLMs) makes it difficult for them to adapt to evolving information, rendering knowledge editing a critical task. However, existing methods struggle with challenges of scalability and retrieval efficiency, particularly when handling complex, multi-hop questions that require multi-step reasoning. To address these challenges, this paper introduces ALEX (A Light Editing-knowledge Extractor), a lightweight knowledge editing framework. The core innovation of ALEX is its hierarchical memory architecture, which organizes knowledge updates (edits) into semantic clusters. This design fundamentally reduces retrieval complexity from a linear O(N) to a highly scalable O(K+N/C). Furthermore, the framework integrates an Inferential Query Synthesis (IQS) module to bridge the semantic gap between queries and facts , and a Dynamic Evidence Adjudication (DEA) engine that executes an efficient two-stage retrieval process. Experiments on the MQUAKE benchmark demonstrate that ALEX significantly improves both the accuracy of multi-hop answers (MultiHop-ACC) and the reliability of reasoning paths (HopWise-ACC). It also reduces the required search space by over 80% , presenting a promising path toward building scalable, efficient, and accurate knowledge editing systems.

</details>


### [12] [Syn-STARTS: Synthesized START Triage Scenario Generation Framework for Scalable LLM Evaluation](https://arxiv.org/abs/2511.14023)
*Chiharu Hagiwara,Naoki Nonaka,Yuhta Hashimoto,Ryu Uchimido,Jun Seita*

Main category: cs.AI

TL;DR: 文章开发Syn - STARTS框架用大语言模型生成分诊案例并验证其有效性，表明合成数据可用于开发医疗AI模型。


<details>
  <summary>Details</summary>
Motivation: AI在大规模伤亡事件分诊决策中作用渐受关注，但缺乏足够高质量的基准数据集，真实数据收集困难。

Method: 开发Syn - STARTS框架，用大语言模型生成分诊案例。

Result: Syn - STARTS生成的分诊案例与手动整理的TRIAGE开放数据集在质量上无差异，评估大语言模型准确性时结果高度稳定。

Conclusion: 合成数据在为严重和危急医疗情况开发高性能AI模型方面具有可能性。

Abstract: Triage is a critically important decision-making process in mass casualty incidents (MCIs) to maximize victim survival rates. While the role of AI in such situations is gaining attention for making optimal decisions within limited resources and time, its development and performance evaluation require benchmark datasets of sufficient quantity and quality. However, MCIs occur infrequently, and sufficient records are difficult to accumulate at the scene, making it challenging to collect large-scale realworld data for research use. Therefore, we developed Syn-STARTS, a framework that uses LLMs to generate triage cases, and verified its effectiveness. The results showed that the triage cases generated by Syn-STARTS were qualitatively indistinguishable from the TRIAGE open dataset generated by manual curation from training materials. Furthermore, when evaluating the LLM accuracy using hundreds of cases each from the green, yellow, red, and black categories defined by the standard triage method START, the results were found to be highly stable. This strongly indicates the possibility of synthetic data in developing high-performance AI models for severe and critical medical situations.

</details>


### [13] [AISAC: An Integrated multi-agent System for Transparent, Retrieval-Grounded Scientific Assistance](https://arxiv.org/abs/2511.14043)
*Chandrachur Bhattacharya,Sibendu Som*

Main category: cs.AI

TL;DR: 本文介绍Argonne国家实验室开发的AI科学助手核心（AISAC）系统，它整合现有技术，实现特定工作流，有多种特性并在多领域应用。


<details>
  <summary>Details</summary>
Motivation: 开发适用于科学和工程工作流的集成多智能体系统，实现工作流透明、来源追踪和科学适应性。

Method: 基于LangGraph、FAISS和SQLite等技术，实现Router - Planner - Coordinator工作流和可选Evaluator角色，采用混合内存方法、增量索引策略及配置驱动的项目启动层。

Result: 系统能记录和可视化智能体决策等信息，已应用于Argonne多个研究领域。

Conclusion: AISAC具有跨领域适用性，可用于科学和工程工作流。

Abstract: AI Scientific Assistant Core (AISAC) is an integrated multi-agent system developed at Argonne National Laboratory for scientific and engineering workflows. AISAC builds on established technologies - LangGraph for orchestration, FAISS for vector search, and SQLite for persistence - and integrates them into a unified system prototype focused on transparency, provenance tracking, and scientific adaptability.
  The system implements a Router-Planner-Coordinator workflow and an optional Evaluator role, using prompt-engineered agents coordinated via LangGraph's StateGraph and supported by helper agents such as a Researcher. Each role is defined through custom system prompts that enforce structured JSON outputs. A hybrid memory approach (FAISS + SQLite) enables both semantic retrieval and structured conversation history. An incremental indexing strategy based on file hashing minimizes redundant re-embedding when scientific corpora evolve. A configuration-driven project bootstrap layer allows research teams to customize tools, prompts, and data sources without modifying core code.
  All agent decisions, tool invocations, and retrievals are logged and visualized through a custom Gradio interface, providing step-by-step transparency for each reasoning episode. The authors have applied AISAC to multiple research areas at Argonne, including specialized deployments for waste-to-products research and energy process safety, as well as general-purpose scientific assistance, demonstrating its cross-domain applicability.

</details>


### [14] [PathMind: A Retrieve-Prioritize-Reason Framework for Knowledge Graph Reasoning with Large Language Models](https://arxiv.org/abs/2511.14256)
*Yu Liu,Xixun Lin,Yanmin Shang,Yangxi Li,Shi Wang,Yanan Cao*

Main category: cs.AI

TL;DR: 现有基于大语言模型的知识图谱推理方法存在提取路径无差别、检索需求高和频繁调用大语言模型的问题，本文提出PathMind框架，遵循“检索 - 排序 - 推理”范式，实验表明其在复杂推理任务上表现更优。


<details>
  <summary>Details</summary>
Motivation: 解决现有基于大语言模型的知识图谱推理方法提取推理路径无差别、检索需求高和频繁调用大语言模型的问题。

Method: 提出PathMind框架，遵循“检索 - 排序 - 推理”范式，包括从知识图谱中检索查询子图、用语义感知路径优先级函数识别重要推理路径、通过双阶段训练策略生成响应。

Result: 在基准数据集上的大量实验表明，PathMind在复杂推理任务上始终优于竞争基线，尤其是在输入标记较少的情况下。

Conclusion: PathMind框架通过识别重要推理路径，能有效提升知识图谱推理的性能。

Abstract: Knowledge graph reasoning (KGR) is the task of inferring new knowledge by performing logical deductions on knowledge graphs. Recently, large language models (LLMs) have demonstrated remarkable performance in complex reasoning tasks. Despite promising success, current LLM-based KGR methods still face two critical limitations. First, existing methods often extract reasoning paths indiscriminately, without assessing their different importance, which may introduce irrelevant noise that misleads LLMs. Second, while many methods leverage LLMs to dynamically explore potential reasoning paths, they require high retrieval demands and frequent LLM calls. To address these limitations, we propose PathMind, a novel framework designed to enhance faithful and interpretable reasoning by selectively guiding LLMs with important reasoning paths. Specifically, PathMind follows a "Retrieve-Prioritize-Reason" paradigm. First, it retrieves a query subgraph from KG through the retrieval module. Next, it introduces a path prioritization mechanism that identifies important reasoning paths using a semantic-aware path priority function, which simultaneously considers the accumulative cost and the estimated future cost for reaching the target. Finally, PathMind generates accurate and logically consistent responses via a dual-phase training strategy, including task-specific instruction tuning and path-wise preference alignment. Extensive experiments on benchmark datasets demonstrate that PathMind consistently outperforms competitive baselines, particularly on complex reasoning tasks with fewer input tokens, by identifying essential reasoning paths.

</details>


### [15] [Collaborative QA using Interacting LLMs. Impact of Network Structure, Node Capability and Distributed Data](https://arxiv.org/abs/2511.14098)
*Adit Jain,Vikram Krishnamurthy,Yiming Zhang*

Main category: cs.AI

TL;DR: 本文结合网络科学和经济学模型研究交互大语言模型网络的协作问答，分析其幻觉问题，给出定点存在唯一条件并实验验证。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在缺乏直接证据时会产生幻觉，在交互网络中幻觉会传播，影响问答准确性。

Method: 结合网络科学的平均场动力学和经济学的随机效用模型构建生成模型，用潜在状态建模大语言模型，扩展平均场动力学分析信息扩散，提出随机效用模型确定概率。

Result: 为大语言模型网络给出定点存在和唯一的充分条件，分析定点在不同激励下的行为。

Conclusion: 通过对100个开源大语言模型在多个半合成数据集上实验，研究了网络在数据异质性、节点能力、网络结构和框架敏感性方面的行为。

Abstract: In this paper, we model and analyze how a network of interacting LLMs performs collaborative question-answering (CQA) in order to estimate a ground truth given a distributed set of documents. This problem is interesting because LLMs often hallucinate when direct evidence to answer a question is lacking, and these effects become more pronounced in a network of interacting LLMs. The hallucination spreads, causing previously accurate LLMs to hallucinate. We study interacting LLMs and their hallucination by combining novel ideas of mean-field dynamics (MFD) from network science and the randomized utility model from economics to construct a useful generative model. We model the LLM with a latent state that indicates if it is truthful or not with respect to the ground truth, and extend a tractable analytical model considering an MFD to model the diffusion of information in a directed network of LLMs. To specify the probabilities that govern the dynamics of the MFD, we propose a randomized utility model. For a network of LLMs, where each LLM has two possible latent states, we posit sufficient conditions for the existence and uniqueness of a fixed point and analyze the behavior of the fixed point in terms of the incentive (e.g., test-time compute) given to individual LLMs. We experimentally study and analyze the behavior of a network of $100$ open-source LLMs with respect to data heterogeneity, node capability, network structure, and sensitivity to framing on multiple semi-synthetic datasets.

</details>


### [16] [APD-Agents: A Large Language Model-Driven Multi-Agents Collaborative Framework for Automated Page Design](https://arxiv.org/abs/2511.14101)
*Xinpeng Chen,Xiaofeng Han,Kaihao Zhang,Guochao Ren,Yujie Wang,Wenhao Cao,Yang Zhou,Jianfeng Lu,Zhenbo Song*

Main category: cs.AI

TL;DR: 提出APD - agents，一种大语言模型驱动的多智能体框架用于移动应用自动页面设计，在RICO数据集上达最优性能。


<details>
  <summary>Details</summary>
Motivation: 移动应用页面布局设计耗时，现有设计软件需大量训练，跨页面协作设计需额外时间统一标准和样式。

Method: 提出包含OrchestratorAgent、SemanticParserAgent、PrimaryLayoutAgent、TemplateRetrievalAgent和RecursiveComponentAgent的框架，OrchestratorAgent接收用户描述后动态指挥其他智能体完成设计任务。

Result: 在RICO数据集上实验表明APD - agents达到了最优性能。

Conclusion: 提出的APD - agents框架能有效实现移动应用的自动页面设计。

Abstract: Layout design is a crucial step in developing mobile app pages. However, crafting satisfactory designs is time-intensive for designers: they need to consider which controls and content to present on the page, and then repeatedly adjust their size, position, and style for better aesthetics and structure. Although many design software can now help to perform these repetitive tasks, extensive training is needed to use them effectively. Moreover, collaborative design across app pages demands extra time to align standards and ensure consistent styling. In this work, we propose APD-agents, a large language model (LLM) driven multi-agent framework for automated page design in mobile applications. Our framework contains OrchestratorAgent, SemanticParserAgent, PrimaryLayoutAgent, TemplateRetrievalAgent, and RecursiveComponentAgent. Upon receiving the user's description of the page, the OrchestratorAgent can dynamically can direct other agents to accomplish users' design task. To be specific, the SemanticParserAgent is responsible for converting users' descriptions of page content into structured data. The PrimaryLayoutAgent can generate an initial coarse-grained layout of this page. The TemplateRetrievalAgent can fetch semantically relevant few-shot examples and enhance the quality of layout generation. Besides, a RecursiveComponentAgent can be used to decide how to recursively generate all the fine-grained sub-elements it contains for each element in the layout. Our work fully leverages the automatic collaboration capabilities of large-model-driven multi-agent systems. Experimental results on the RICO dataset show that our APD-agents achieve state-of-the-art performance.

</details>


### [17] [Run, Ruminate, and Regulate: A Dual-process Thinking System for Vision-and-Language Navigation](https://arxiv.org/abs/2511.14131)
*Yu Zhong,Zihao Zhang,Rui Zhang,Lingdong Huang,Haihan Gao,Shuo Wang,Da Li,Ruijian Han,Jiaming Guo,Shaohui Peng,Di Huang,Yunji Chen*

Main category: cs.AI

TL;DR: 提出R3框架解决VLN中使用LLM的问题，实验显示其性能优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的VLN方法在任务完成性能上与领域专家有差距，且存在计算成本高和推理延迟问题。

Method: 提出R3双过程思维框架，包含Runner、Ruminator和Regulator三个核心模块。

Result: R3在REVERIE基准测试中，SPL和RGSPL分别超过其他方法3.28%和3.30%。

Conclusion: R3框架在处理具有挑战性的VLN任务中有效。

Abstract: Vision-and-Language Navigation (VLN) requires an agent to dynamically explore complex 3D environments following human instructions. Recent research underscores the potential of harnessing large language models (LLMs) for VLN, given their commonsense knowledge and general reasoning capabilities. Despite their strengths, a substantial gap in task completion performance persists between LLM-based approaches and domain experts, as LLMs inherently struggle to comprehend real-world spatial correlations precisely. Additionally, introducing LLMs is accompanied with substantial computational cost and inference latency. To address these issues, we propose a novel dual-process thinking framework dubbed R3, integrating LLMs' generalization capabilities with VLN-specific expertise in a zero-shot manner. The framework comprises three core modules: Runner, Ruminator, and Regulator. The Runner is a lightweight transformer-based expert model that ensures efficient and accurate navigation under regular circumstances. The Ruminator employs a powerful multimodal LLM as the backbone and adopts chain-of-thought (CoT) prompting to elicit structured reasoning. The Regulator monitors the navigation progress and controls the appropriate thinking mode according to three criteria, integrating Runner and Ruminator harmoniously. Experimental results illustrate that R3 significantly outperforms other state-of-the-art methods, exceeding 3.28% and 3.30% in SPL and RGSPL respectively on the REVERIE benchmark. This pronounced enhancement highlights the effectiveness of our method in handling challenging VLN tasks.

</details>


### [18] [Beyond Accuracy: A Multi-Dimensional Framework for Evaluating Enterprise Agentic AI Systems](https://arxiv.org/abs/2511.14136)
*Sushant Mehta*

Main category: cs.AI

TL;DR: 现有智能体AI基准主要评估任务完成准确率，忽略企业需求。研究指出三个局限性，提出CLEAR评估框架，实验表明其优于仅考虑准确率的评估。


<details>
  <summary>Details</summary>
Motivation: 当前智能体AI基准忽略企业对成本效益、可靠性和运营稳定性等需求，需要更合适的评估框架。

Method: 系统分析12个主要基准，对先进智能体进行实证评估，提出CLEAR框架并在300个企业任务上评估6个领先智能体，进行专家评估。

Result: 仅优化准确率的智能体比考虑成本的替代方案贵4.4 - 10.8倍；CLEAR预测生产成功的相关性为0.83，高于仅考虑准确率的0.41。

Conclusion: CLEAR是专为企业部署设计的整体评估框架，比仅考虑准确率的评估更能预测生产成功。

Abstract: Current agentic AI benchmarks predominantly evaluate task completion accuracy, while overlooking critical enterprise requirements such as cost-efficiency, reliability, and operational stability. Through systematic analysis of 12 main benchmarks and empirical evaluation of state-of-the-art agents, we identify three fundamental limitations: (1) absence of cost-controlled evaluation leading to 50x cost variations for similar precision, (2) inadequate reliability assessment where agent performance drops from 60\% (single run) to 25\% (8-run consistency), and (3) missing multidimensional metrics for security, latency, and policy compliance. We propose \textbf{CLEAR} (Cost, Latency, Efficacy, Assurance, Reliability), a holistic evaluation framework specifically designed for enterprise deployment. Evaluation of six leading agents on 300 enterprise tasks demonstrates that optimizing for accuracy alone yields agents 4.4-10.8x more expensive than cost-aware alternatives with comparable performance. Expert evaluation (N=15) confirms that CLEAR better predicts production success (correlation $ρ=0.83$) compared to accuracy-only evaluation ($ρ=0.41$).

</details>


### [19] [HFL-FlowLLM: Large Language Models for Network Traffic Flow Classification in Heterogeneous Federated Learning](https://arxiv.org/abs/2511.14199)
*Jiazhuo Tian,Yachao Yuan*

Main category: cs.AI

TL;DR: 提出HFL - FlowLLM框架用于网络流量分类，在性能、成本等方面有优势，证明其在现代通信网络安全中的价值。


<details>
  <summary>Details</summary>
Motivation: 现代通信网络中传统集中式机器学习和现有联邦学习方法在网络流量分类上存在问题，如处理分布式数据和隐私问题、高成本和泛化性差等。

Method: 提出HFL - FlowLLM框架，将大语言模型应用于异构联邦学习中的网络流量分类。

Result: 与现有异构联邦学习方法相比，平均F1分数提高约13%；与现有大语言模型联邦学习框架相比，随着每轮训练客户端数量增加，平均F1分数最多提高5%，训练成本降低约87%。

Conclusion: HFL - FlowLLM在现代通信网络安全中有潜在和实际价值。

Abstract: In modern communication networks driven by 5G and the Internet of Things (IoT), effective network traffic flow classification is crucial for Quality of Service (QoS) management and security. Traditional centralized machine learning struggles with the distributed data and privacy concerns in these heterogeneous environments, while existing federated learning approaches suffer from high costs and poor generalization. To address these challenges, we propose HFL-FlowLLM, which to our knowledge is the first framework to apply large language models to network traffic flow classification in heterogeneous federated learning. Compared to state-of-the-art heterogeneous federated learning methods for network traffic flow classification, the proposed approach improves the average F1 score by approximately 13%, demonstrating compelling performance and strong robustness. When compared to existing large language models federated learning frameworks, as the number of clients participating in each training round increases, the proposed method achieves up to a 5% improvement in average F1 score while reducing the training costs by about 87%. These findings prove the potential and practical value of HFL-FlowLLM in modern communication networks security.

</details>


### [20] [Do Large Language Models (LLMs) Understand Chronology?](https://arxiv.org/abs/2511.14214)
*Pattaraphon Kenny Wongchamcharoen,Paul Glasserman*

Main category: cs.AI

TL;DR: 本文测试大语言模型对时间顺序的理解能力，评估多个模型在不同推理设置下的表现，发现分配显式推理预算有助于时间排序，研究为大语言模型在金融领域应用提供参考并开源代码。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在金融和经济领域应用中，基于提示的方法隐式假设模型理解时间顺序，需要测试这一基本问题。

Method: 设计一系列复杂度递增的时间排序任务，包括时间顺序排列、条件排序和年代错误检测，评估GPT - 4.1、Claude - 3.7 Sonnet（有无扩展思维）和GPT - 5在多种推理设置下的表现。

Result: 随着序列变长，模型精确匹配率大幅下降，但排名相关性保持较高；条件排序中多数失败源于过滤步骤；年代错误检测相对容易，但性能会随时间线或实体重叠增加而下降；分配显式推理预算对时间排序有帮助，GPT - 5在中/高推理努力下表现出色。

Conclusion: 研究揭示了当前大语言模型在时间任务上的局限性，为任务复杂度提供见解，展示了推理起作用的场景，对大语言模型在金融实时应用有重要意义，且开源代码支持可重复性。

Abstract: Large language models (LLMs) are increasingly used in finance and economics, where prompt-based attempts against look-ahead bias implicitly assume that models understand chronology. We test this fundamental question with a series of chronological ordering tasks with increasing complexities over facts the model already knows from pre-training. Our tasks cover (1) chronological ordering, (2) conditional sorting (filter, then order), and (3) anachronism detection. We evaluate GPT-4.1, Claude-3.7 Sonnet, with and without Extended Thinking (ET), and GPT-5 across multiple reasoning-effort settings. Across models, Exact match rate drops sharply as sequences lengthen even while rank correlations stay high as LLMs largely preserve local order but struggle to maintain a single globally consistent timeline. In conditional sorting, most failures stem from the filtering step rather than the ordering step, but GPT-5 and Claude-3.7 Sonnet with Extended Thinking outshine normal models significantly. Lastly, anachronism detection is found to be the easiest task for the LLMs but performance still declines with increasingly overlapping timelines or entities. Overall, our main contribution is showing that allocating explicit reasoning budget helps with chronological ordering with GPT-5 at medium/high reasoning effort achieving flawless ordering at all lengths and perfect conditional sorting (both self-filtered and given-subset), whereas low/minimal effort degrades with longer lists, mirroring earlier models. Our findings delineate limits of current LLMs on chronological tasks, providing insights into task complexity, and demonstrate scenarios in which reasoning helps. These patterns are important for the real-time application of LLMs in finance. We release all code and evaluation templates to support full reproducibility.

</details>


### [21] [Listen Like a Teacher: Mitigating Whisper Hallucinations using Adaptive Layer Attention and Knowledge Distillation](https://arxiv.org/abs/2511.14219)
*Kumud Tripathi,Aditya Srinivas Menon,Aman Gaurav,Raj Prakash Gohil,Pankaj Wasnik*

Main category: cs.AI

TL;DR: 提出两阶段架构（ALA和KD）改进Whisper模型，减少噪声环境下幻觉错误和字错率。


<details>
  <summary>Details</summary>
Motivation: Whisper模型在噪声环境下常有幻觉错误，之前主要关注音频前后处理，对模型本身修改研究少。

Method: 两阶段架构，第一阶段用Adaptive Layer Attention增强编码器鲁棒性，第二阶段用多目标知识蒸馏框架抑制幻觉。

Result: 在噪声语音基准测试中幻觉和字错率显著降低，干净语音性能保持。

Conclusion: ALA和KD为提高Whisper在真实噪声环境下可靠性提供有效策略。

Abstract: The Whisper model, an open-source automatic speech recognition system, is widely adopted for its strong performance across multilingual and zero-shot settings. However, it frequently suffers from hallucination errors, especially under noisy acoustic conditions. Previous works to reduce hallucinations in Whisper-style ASR systems have primarily focused on audio preprocessing or post-processing of transcriptions to filter out erroneous content. However, modifications to the Whisper model itself remain largely unexplored to mitigate hallucinations directly. To address this challenge, we present a two-stage architecture that first enhances encoder robustness through Adaptive Layer Attention (ALA) and further suppresses hallucinations using a multi-objective knowledge distillation (KD) framework. In the first stage, ALA groups encoder layers into semantically coherent blocks via inter-layer correlation analysis. A learnable multi-head attention module then fuses these block representations, enabling the model to jointly exploit low- and high-level features for more robust encoding. In the second stage, our KD framework trains the student model on noisy audio to align its semantic and attention distributions with a teacher model processing clean inputs. Our experiments on noisy speech benchmarks show notable reductions in hallucinations and word error rates, while preserving performance on clean speech. Together, ALA and KD offer a principled strategy to improve Whisper's reliability under real-world noisy conditions.

</details>


### [22] [DevPiolt: Operation Recommendation for IoT Devices at Xiaomi Home](https://arxiv.org/abs/2511.14227)
*Yuxiang Wang,Siwen Wang,Haowei Han,Ao Wang,Boya Liu,Yong Zhao,Chengbo Wu,Bin Zhu,Bin Qin,Xiaokai Zhou,Xiao Yan,Jiawei Jiang,Bo Du*

Main category: cs.AI

TL;DR: 提出基于大语言模型的物联网设备操作推荐模型DevPiolt，实验表现优且已实际部署获良好效果。


<details>
  <summary>Details</summary>
Motivation: 现有推荐模型在物联网设备操作中存在应对复杂逻辑、多样偏好和避免不良建议方面的问题，需新模型。

Method: 通过持续预训练和多任务微调让大语言模型具备物联网操作基础知识，用直接偏好优化使其符合用户偏好，设计基于置信度的曝光控制机制。

Result: 实验中DevPiolt显著优于基线模型，各指标平均提升69.5%；在小米家庭应用部署后，独立访客设备覆盖率提升21.6%，页面浏览接受率提升29.1%。

Conclusion: DevPiolt在物联网设备操作推荐上表现良好，有实际应用价值。

Abstract: Operation recommendation for IoT devices refers to generating personalized device operations for users based on their context, such as historical operations, environment information, and device status. This task is crucial for enhancing user satisfaction and corporate profits. Existing recommendation models struggle with complex operation logic, diverse user preferences, and sensitive to suboptimal suggestions, limiting their applicability to IoT device operations. To address these issues, we propose DevPiolt, a LLM-based recommendation model for IoT device operations. Specifically, we first equip the LLM with fundamental domain knowledge of IoT operations via continual pre-training and multi-task fine-tuning. Then, we employ direct preference optimization to align the fine-tuned LLM with specific user preferences. Finally, we design a confidence-based exposure control mechanism to avoid negative user experiences from low-quality recommendations. Extensive experiments show that DevPiolt significantly outperforms baselines on all datasets, with an average improvement of 69.5% across all metrics. DevPiolt has been practically deployed in Xiaomi Home app for one quarter, providing daily operation recommendations to 255,000 users. Online experiment results indicate a 21.6% increase in unique visitor device coverage and a 29.1% increase in page view acceptance rates.

</details>


### [23] [Enhancing Regional Airbnb Trend Forecasting Using LLM-Based Embeddings of Accessibility and Human Mobility](https://arxiv.org/abs/2511.14248)
*Hongju Lee,Youngjun Park,Jisun An,Dongman Lee*

Main category: cs.AI

TL;DR: 本文提出新颖时间序列预测框架预测区域Airbnb关键指标，在首尔数据集实验中表现良好，提升预测精度并为城市政策决策提供见解。


<details>
  <summary>Details</summary>
Motivation: 短租平台扩张扰乱当地住房市场，准确预测区域Airbnb市场趋势可为政策制定者和城市规划者提供关键见解以减轻影响。

Method: 提出时间序列预测框架，用滑动窗口方法预测1 - 3个月趋势，整合房源特征与外部因素构建区域表征，将数据转换为大语言模型输入生成嵌入，再输入先进时间序列模型。

Result: 在首尔Airbnb数据集实验中，相比传统基线模型，平均RMSE和MAE约降低48%。

Conclusion: 该框架提高了预测准确性，能检测供应过剩区域，支持数据驱动的城市政策决策。

Abstract: The expansion of short-term rental platforms, such as Airbnb, has significantly disrupted local housing markets, often leading to increased rental prices and housing affordability issues. Accurately forecasting regional Airbnb market trends can thus offer critical insights for policymakers and urban planners aiming to mitigate these impacts. This study proposes a novel time-series forecasting framework to predict three key Airbnb indicators -- Revenue, Reservation Days, and Number of Reservations -- at the regional level. Using a sliding-window approach, the model forecasts trends 1 to 3 months ahead. Unlike prior studies that focus on individual listings at fixed time points, our approach constructs regional representations by integrating listing features with external contextual factors such as urban accessibility and human mobility. We convert structured tabular data into prompt-based inputs for a Large Language Model (LLM), producing comprehensive regional embeddings. These embeddings are then fed into advanced time-series models (RNN, LSTM, Transformer) to better capture complex spatio-temporal dynamics. Experiments on Seoul's Airbnb dataset show that our method reduces both average RMSE and MAE by approximately 48% compared to conventional baselines, including traditional statistical and machine learning models. Our framework not only improves forecasting accuracy but also offers practical insights for detecting oversupplied regions and supporting data-driven urban policy decisions.

</details>


### [24] [DataSage: Multi-agent Collaboration for Insight Discovery with External Knowledge Retrieval, Multi-role Debating, and Multi-path Reasoning](https://arxiv.org/abs/2511.14299)
*Xiaochuan Liu,Yuanfeng Song,Xiaoming Yin,Xing Chen*

Main category: cs.AI

TL;DR: 提出DataSage多智能体框架解决现有数据洞察智能体的问题，实验表明其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有数据洞察智能体在利用领域知识、分析深度和代码生成准确性方面存在不足，需要改进以实现自动化数据洞察发现。

Method: 提出DataSage框架，包含外部知识检索、多角色辩论机制和多路径推理三个创新特性。

Result: 在InsightBench上的大量实验表明，DataSage在所有难度级别上始终优于现有数据洞察智能体。

Conclusion: DataSage为自动化数据洞察发现提供了有效的解决方案。

Abstract: In today's data-driven era, fully automated end-to-end data analytics, particularly insight discovery, is critical for discovering actionable insights that assist organizations in making effective decisions. With the rapid advancement of large language models (LLMs), LLM-driven agents have emerged as a promising paradigm for automating data analysis and insight discovery. However, existing data insight agents remain limited in several key aspects, often failing to deliver satisfactory results due to: (1) insufficient utilization of domain knowledge, (2) shallow analytical depth, and (3) error-prone code generation during insight generation. To address these issues, we propose DataSage, a novel multi-agent framework that incorporates three innovative features including external knowledge retrieval to enrich the analytical context, a multi-role debating mechanism to simulate diverse analytical perspectives and deepen analytical depth, and multi-path reasoning to improve the accuracy of the generated code and insights. Extensive experiments on InsightBench demonstrate that DataSage consistently outperforms existing data insight agents across all difficulty levels, offering an effective solution for automated data insight discovery.

</details>


### [25] [When Words Change the Model: Sensitivity of LLMs for Constraint Programming Modelling](https://arxiv.org/abs/2511.14334)
*Alessio Pellegrino,Jacopo Mauro*

Main category: cs.AI

TL;DR: 研究大语言模型自动生成优化和约束规划模型，发现其在语境和语言变化下性能下降，理解浅且对措辞敏感。


<details>
  <summary>Details</summary>
Motivation: 验证大语言模型自动生成模型的成功是源于数据污染还是真正推理。

Method: 对一组知名CSPLib问题进行系统改写和扰动，比较三个代表性大语言模型在原始和修改描述下生成的模型。

Result: 大语言模型能生成语法和语义合理的模型，但在语境和语言变化下性能急剧下降。

Conclusion: 大语言模型对问题理解肤浅，且对措辞敏感。

Abstract: One of the long-standing goals in optimisation and constraint programming is to describe a problem in natural language and automatically obtain an executable, efficient model. Large language models appear to bring this vision closer, showing impressive results in automatically generating models for classical benchmarks. However, much of this apparent success may derive from data contamination rather than genuine reasoning: many standard CP problems are likely included in the training data of these models. To examine this hypothesis, we systematically rephrased and perturbed a set of well-known CSPLib problems to preserve their structure while modifying their context and introducing misleading elements. We then compared the models produced by three representative LLMs across original and modified descriptions. Our qualitative analysis shows that while LLMs can produce syntactically valid and semantically plausible models, their performance drops sharply under contextual and linguistic variation, revealing shallow understanding and sensitivity to wording.

</details>


### [26] [Operationalizing Pluralistic Values in Large Language Model Alignment Reveals Trade-offs in Safety, Inclusivity, and Model Behavior](https://arxiv.org/abs/2511.14476)
*Dalia Ali,Dora Zhao,Allison Koenecke,Orestis Papakyriakopoulos*

Main category: cs.AI

TL;DR: 研究探讨融入多元价值观对大语言模型行为的影响，收集美德参与者数据，微调模型，发现人口统计学效应和技术设计选择的影响。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型对齐决策常忽略人类社会多样性，研究融入多元价值观如何影响模型行为。

Method: 收集美德参与者对大语言模型回复的评分数据，微调多个大语言模型和大推理模型，改变评分尺度、分歧处理方法和优化技术。

Result: 存在人口统计学效应，不同群体评分有差异；模型根据特定群体偏好微调有不同行为；技术设计选择影响大，如保留评分者分歧比多数投票减毒效果好等。

Conclusion: 研究为如何平衡专家驱动和用户驱动信号以确保安全和公平表征迈出初步一步。

Abstract: Although large language models (LLMs) are increasingly trained using human feedback for safety and alignment with human values, alignment decisions often overlook human social diversity. This study examines how incorporating pluralistic values affects LLM behavior by systematically evaluating demographic variation and design parameters in the alignment pipeline. We collected alignment data from US and German participants (N = 1,095, 27,375 ratings) who rated LLM responses across five dimensions: Toxicity, Emotional Awareness (EA), Sensitivity, Stereotypical Bias, and Helpfulness. We fine-tuned multiple Large Language Models and Large Reasoning Models using preferences from different social groups while varying rating scales, disagreement handling methods, and optimization techniques. The results revealed systematic demographic effects: male participants rated responses 18% less toxic than female participants; conservative and Black participants rated responses 27.9% and 44% more emotionally aware than liberal and White participants, respectively. Models fine-tuned on group-specific preferences exhibited distinct behaviors. Technical design choices showed strong effects: the preservation of rater disagreement achieved roughly 53% greater toxicity reduction than majority voting, and 5-point scales yielded about 22% more reduction than binary formats; and Direct Preference Optimization (DPO) consistently outperformed Group Relative Policy Optimization (GRPO) in multi-value optimization. These findings represent a preliminary step in answering a critical question: How should alignment balance expert-driven and user-driven signals to ensure both safety and fair representation?

</details>


### [27] [A Neuro-Symbolic Framework for Reasoning under Perceptual Uncertainty: Bridging Continuous Perception and Discrete Symbolic Planning](https://arxiv.org/abs/2511.14533)
*Jiahao Wu,Shengwen Yu*

Main category: cs.AI

TL;DR: 提出神经符号框架，在桌面机器人操作中验证有效性，框架通用可用于需不确定性感知推理的领域。


<details>
  <summary>Details</summary>
Motivation: 解决AI系统中连接连续感知信号和离散符号推理的挑战，在不确定条件下建立感知和规划间的联系。

Method: 将基于Transformer的感知前端与图神经网络关系推理结合提取概率符号状态，使用不确定性感知的符号规划器。

Result: 在桌面机器人操作中，翻译器处理场景输出概率谓词，系统在多个基准测试中成功率高，超POMDP基线，规划时间短。

Conclusion: 框架有效，建立了校准不确定性和规划收敛的定量联系，有理论保证且可通用。

Abstract: Bridging continuous perceptual signals and discrete symbolic reasoning is a fundamental challenge in AI systems that must operate under uncertainty. We present a neuro-symbolic framework that explicitly models and propagates uncertainty from perception to planning, providing a principled connection between these two abstraction levels. Our approach couples a transformer-based perceptual front-end with graph neural network (GNN) relational reasoning to extract probabilistic symbolic states from visual observations, and an uncertainty-aware symbolic planner that actively gathers information when confidence is low. We demonstrate the framework's effectiveness on tabletop robotic manipulation as a concrete application: the translator processes 10,047 PyBullet-generated scenes (3--10 objects) and outputs probabilistic predicates with calibrated confidences (overall F1=0.68). When embedded in the planner, the system achieves 94\%/90\%/88\% success on Simple Stack, Deep Stack, and Clear+Stack benchmarks (90.7\% average), exceeding the strongest POMDP baseline by 10--14 points while planning within 15\,ms. A probabilistic graphical-model analysis establishes a quantitative link between calibrated uncertainty and planning convergence, providing theoretical guarantees that are validated empirically. The framework is general-purpose and can be applied to any domain requiring uncertainty-aware reasoning from perceptual input to symbolic planning.

</details>


### [28] [Rate-Distortion Guided Knowledge Graph Construction from Lecture Notes Using Gromov-Wasserstein Optimal Transport](https://arxiv.org/abs/2511.14595)
*Yuan An,Ruhma Hashmi,Michelle Rogers,Jane Greenberg,Brian K. Smith*

Main category: cs.AI

TL;DR: 提出基于率失真理论和最优传输几何的知识图谱构建与优化框架，应用于数据科学讲座效果良好，为教育中知识图谱优化奠定基础。


<details>
  <summary>Details</summary>
Motivation: 将非结构化教育材料转化为能捕捉关键教学内容的知识图谱存在困难，需要解决该问题以助力学习辅助系统生成高质量选择题。

Method: 构建基于率失真理论和最优传输几何的框架，将讲座内容建模为度量 - 测度空间，用 Fused Gromov - Wasserstein 耦合对齐候选知识图谱，通过细化算子最小化率失真拉格朗日量。

Result: 应用于数据科学讲座得到可解释的率失真曲线，优化后的知识图谱生成的选择题在十五项质量标准上优于原始笔记生成的。

Conclusion: 为个性化和人工智能辅助教育中的信息论知识图谱优化建立了有原则的基础。

Abstract: Task-oriented knowledge graphs (KGs) enable AI-powered learning assistant systems to automatically generate high-quality multiple-choice questions (MCQs). Yet converting unstructured educational materials, such as lecture notes and slides, into KGs that capture key pedagogical content remains difficult. We propose a framework for knowledge graph construction and refinement grounded in rate-distortion (RD) theory and optimal transport geometry. In the framework, lecture content is modeled as a metric-measure space, capturing semantic and relational structure, while candidate KGs are aligned using Fused Gromov-Wasserstein (FGW) couplings to quantify semantic distortion. The rate term, expressed via the size of KG, reflects complexity and compactness. Refinement operators (add, merge, split, remove, rewire) minimize the rate-distortion Lagrangian, yielding compact, information-preserving KGs. Our prototype applied to data science lectures yields interpretable RD curves and shows that MCQs generated from refined KGs consistently surpass those from raw notes on fifteen quality criteria. This study establishes a principled foundation for information-theoretic KG optimization in personalized and AI-assisted education.

</details>


### [29] [AutoTool: Efficient Tool Selection for Large Language Model Agents](https://arxiv.org/abs/2511.14650)
*Jingyi Jia,Qinbin Li*

Main category: cs.AI

TL;DR: 提出AutoTool框架，利用工具使用惯性减少大语言模型推理成本，实验证明可降低成本30%并保持任务完成率。


<details>
  <summary>Details</summary>
Motivation: 当前代理框架中工具选择推理成本高，如ReAct方法需重复调用大语言模型。

Method: 构建基于历史代理轨迹的有向图，利用工具使用惯性，结合参数级信息优化工具输入生成。

Result: 在不同代理任务实验中，AutoTool降低推理成本达30%，保持有竞争力的任务完成率。

Conclusion: 将统计结构融入大语言模型代理设计可在不牺牲性能前提下提高效率。

Abstract: Large Language Model (LLM) agents have emerged as powerful tools for automating complex tasks by leveraging the reasoning and decision-making abilities of LLMs. However, a major bottleneck in current agent frameworks lies in the high inference cost of tool selection, especially in approaches like ReAct that repeatedly invoke the LLM to determine which tool to use at each step. In this work, we propose AutoTool, a novel graph-based framework that bypasses repeated LLM inference by exploiting a key empirical observation: tool usage inertia - the tendency of tool invocations to follow predictable sequential patterns. AutoTool constructs a directed graph from historical agent trajectories, where nodes represent tools and edges capture transition probabilities, effectively modeling the inertia in tool selection. It further integrates parameter-level information to refine tool input generation. By traversing this structured representation, AutoTool efficiently selects tools and their parameters with minimal reliance on LLM inference. Extensive experiments across diverse agent tasks demonstrate that AutoTool reduces inference costs by up to 30% while maintaining competitive task completion rates, offering a practical and scalable enhancement for inference-heavy frameworks. Our work highlights the promise of integrating statistical structure into LLM agent design for greater efficiency without sacrificing performance.

</details>


### [30] [SkillGen: Learning Domain Skills for In-Context Sequential Decision Making](https://arxiv.org/abs/2511.14670)
*Ruomeng Ding,Wei Cheng,Minglai Shao,Chen Zhao*

Main category: cs.AI

TL;DR: 现有上下文学习方法难满足有效提示三原则，提出SkillGen框架，理论分析支持其设计，实验显示能提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有上下文学习方法难以同时满足有效提示的三个原则，影响大语言模型在顺序决策中的效果。

Method: 引入基于技能的上下文学习框架SkillGen，构建动作中心的领域级图，通过时序差分信用分配识别高效用动作，检索逐步技能生成提示，并进行理论分析。

Result: 在多个数据集和不同大语言模型上实验，SkillGen平均使进度率提高5.9%-16.5%。

Conclusion: SkillGen框架能有效提升大语言模型在顺序决策中通过上下文学习的性能。

Abstract: Large language models (LLMs) are increasingly applied to sequential decision-making through in-context learning (ICL), yet their effectiveness is highly sensitive to prompt quality. Effective prompts should meet three principles: focus on decision-critical information, provide step-level granularity, and minimize reliance on expert annotations through label efficiency. However, existing ICL methods often fail to satisfy all three criteria simultaneously. Motivated by these challenges, we introduce SkillGen, a skill-based ICL framework for structured sequential reasoning. It constructs an action-centric, domain-level graph from sampled trajectories, identifies high-utility actions via temporal-difference credit assignment, and retrieves step-wise skills to generate fine-grained, context-aware prompts. We further present a theoretical analysis showing that focusing on high-utility segments supports task identifiability and informs more effective ICL prompt design. Experiments on ALFWorld, BabyAI, and ScienceWorld, using both open-source and proprietary LLMs, show that SkillGen achieves consistent gains, improving progress rate by 5.9%-16.5% on average across models.

</details>


### [31] [Heterogeneous Multi-Agent Proximal Policy Optimization for Power Distribution System Restoration](https://arxiv.org/abs/2511.14730)
*Parya Dolatyabi,Mahdi Khodayar*

Main category: cs.AI

TL;DR: 本文应用异构代理强化学习框架HAPPO解决配电网恢复问题，在IEEE系统实验中表现优于多种方法，证明其在复杂配电网恢复中的有效性。


<details>
  <summary>Details</summary>
Motivation: 传统优化和基于价值的强化学习方法在配电网恢复问题中计算效率低且难扩展，需要新方法解决。

Method: 应用HAPPO框架，每个代理控制不同微电网，用集中式评论家训练分散式演员策略，利用物理信息OpenDSS环境提供反馈和实施约束。

Result: 在IEEE 123 - 总线和IEEE 8500节点系统实验中，HAPPO比DQN、PPO等方法收敛更快、恢复功率更高、多种子训练更平滑。

Conclusion: 在HARL框架中纳入微电网级别的异质性，可为复杂配电网恢复提供可扩展、稳定且考虑约束的解决方案。

Abstract: Restoring power distribution systems (PDS) after large-scale outages requires sequential switching operations that reconfigure feeder topology and coordinate distributed energy resources (DERs) under nonlinear constraints such as power balance, voltage limits, and thermal ratings. These challenges make conventional optimization and value-based RL approaches computationally inefficient and difficult to scale. This paper applies a Heterogeneous-Agent Reinforcement Learning (HARL) framework, instantiated through Heterogeneous-Agent Proximal Policy Optimization (HAPPO), to enable coordinated restoration across interconnected microgrids. Each agent controls a distinct microgrid with different loads, DER capacities, and switch counts, introducing practical structural heterogeneity. Decentralized actor policies are trained with a centralized critic to compute advantage values for stable on-policy updates. A physics-informed OpenDSS environment provides full power flow feedback and enforces operational limits via differentiable penalty signals rather than invalid action masking. The total DER generation is capped at 2400 kW, and each microgrid must satisfy local supply-demand feasibility. Experiments on the IEEE 123-bus and IEEE 8500-node systems show that HAPPO achieves faster convergence, higher restored power, and smoother multi-seed training than DQN, PPO, MAES, MAGDPG, MADQN, Mean-Field RL, and QMIX. Results demonstrate that incorporating microgrid-level heterogeneity within the HARL framework yields a scalable, stable, and constraint-aware solution for complex PDS restoration.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [32] [Variational multiscale enrichment method for dynamic response of hyperelastic materials at finite deformation](https://arxiv.org/abs/2511.13723)
*Abhishek Arora,Caglar Oskay*

Main category: cs.CE

TL;DR: 本文将变分多尺度富集（VME）方法扩展用于模拟大变形超弹性材料的动态响应，通过位移场分解推导多尺度控制方程，用算子分裂法求解，数值算例表明多尺度耗散方案可抑制虚假振荡，该框架可研究材料动态响应。


<details>
  <summary>Details</summary>
Motivation: 模拟大变形超弹性材料在尺度不可分离条件下的波传播，考虑材料和几何非线性。

Method: 采用位移场的加法分解推导多尺度控制方程，用算子分裂法迭代求解半离散方程，粗尺度问题显式积分，细尺度问题用显式或隐式时间积分方案。

Result: 数值例子显示多尺度耗散方案能有效抑制虚假振荡。

Conclusion: 该多尺度计算框架为研究结构材料的动态响应提供了基础。

Abstract: In this manuscript, we extend the variational multiscale enrichment (VME) method to model the dynamic response of hyperelastic materials undergoing large deformations. This approach enables the simulation of wave propagation under scale-inseparable conditions, including short-wavelength regimes, while accounting for material and geometric nonlinearities that lead to wave steepening or flattening. By employing an additive decomposition of the displacement field, we derive multiscale governing equations for the coarse- and fine-scale problems, which naturally incorporate micro-inertial effects. The framework allows the discretization of each unit cell with a patch of coarse-scale elements, which is essential to accurately capture wave propagation in short-wavelength regimes. An operator-split procedure is used to iteratively solve the semi-discrete equations at both scales until convergence is achieved. The coarse-scale problem is integrated explicitly, while the fine-scale problem is solved using either explicit or implicit time integration schemes, including both dissipative and non-dissipative methods. Numerical examples demonstrate that multiscale dissipative schemes effectively suppress spurious oscillations. The multiscale framework was applied to investigate how material and geometric nonlinearities, along with elastic stiffness contrast in heterogeneous microstructures, influence key wave characteristics such as dispersion, attenuation, and steepening. This multiscale computational framework provides a foundation for studying the dynamic response of architected materials.

</details>


### [33] [PGD-TO: A Scalable Alternative to MMA Using Projected Gradient Descent for Multi-Constraint Topology Optimization](https://arxiv.org/abs/2511.13905)
*Amin Heyrani Nobari,Faez Ahmed*

Main category: cs.CE

TL;DR: 本文提出PGD - TO框架用于拓扑优化，解决了投影梯度下降法在非线性和多约束问题上的不足，在多种测试中表现良好，是MMA的快速、稳健且可扩展替代方案。


<details>
  <summary>Details</summary>
Motivation: 投影梯度下降法在处理非线性和多约束拓扑优化问题时，因有效集检测复杂而面临困难，需要改进方法。

Method: 将投影步骤重新表述为正则化凸二次问题，针对不同约束情况采用半光滑牛顿求解器或二分搜索投影，还集成了谱步长自适应和非线性共轭梯度方向。

Result: 在四类基准测试中，PGD - TO的收敛性和最终柔度与MMA和OC相当，在一般问题上每次迭代计算时间减少10 - 43倍，约束独立时减少115 - 312倍。

Conclusion: PGD - TO是MMA的快速、稳健且可扩展替代方案，推动拓扑优化向实际大规模、多约束和非线性设计问题发展。

Abstract: Projected Gradient Descent (PGD) methods offer a simple and scalable approach to topology optimization (TO), yet they often struggle with nonlinear and multi-constraint problems due to the complexity of active-set detection. This paper introduces PGD-TO, a framework that reformulates the projection step into a regularized convex quadratic problem, eliminating the need for active-set search and ensuring well-posedness even when constraints are infeasible. The framework employs a semismooth Newton solver for general multi-constraint cases and a binary search projection for single or independent constraints, achieving fast and reliable convergence. It further integrates spectral step-size adaptation and nonlinear conjugate-gradient directions for improved stability and efficiency. We evaluate PGD-TO on four benchmark families representing the breadth of TO problems: (i) minimum compliance with a linear volume constraint, (ii) minimum volume under a nonlinear compliance constraint, (iii) multi-material minimum compliance with four independent volume constraints, and (iv) minimum compliance with coupled volume and center-of-mass constraints. Across these single- and multi-constraint, linear and nonlinear cases, PGD-TO achieves convergence and final compliance comparable to the Method of Moving Asymptotes (MMA) and Optimality Criteria (OC), while reducing per-iteration computation time by 10-43x on general problems and 115-312x when constraints are independent. Overall, PGD-TO establishes a fast, robust, and scalable alternative to MMA, advancing topology optimization toward practical large-scale, multi-constraint, and nonlinear design problems. Public code available at: https://github.com/ahnobari/pyFANTOM

</details>


### [34] [MoMoE: A Mixture of Expert Agent Model for Financial Sentiment Analysis](https://arxiv.org/abs/2511.13983)
*Peng Shu,Junhao Chen,Zhengliang Liu,Hanqi Jiang,Yi Pan,Khanh Nhu Nguyen,Zihao Wu,Huaqin Zhao,Yiwei Li,Enze Shi,ShaoChen Xu*

Main category: cs.CE

TL;DR: 提出MoMoE方法结合MoE架构与多智能体框架，改进LLaMA 3.1 8B架构，实验在多基准测试有显著提升。


<details>
  <summary>Details</summary>
Motivation: 结合MoE架构与协作多智能体框架的优势。

Method: 修改LLaMA 3.1 8B架构，在分层协作结构的每个智能体中加入MoE层，智能体在最终注意力块利用MoE层。

Result: 在多个语言理解和生成基准测试中取得显著改进。

Conclusion: 结合神经和智能体层面的专家路由具有协同优势。

Abstract: We present a novel approach called Mixture of Mixture of Expert (MoMoE) that combines the strengths of Mixture-of-Experts (MoE) architectures with collaborative multi-agent frameworks. By modifying the LLaMA 3.1 8B architecture to incorporate MoE layers in each agent of a layered collaborative structure, we create an ensemble of specialized expert agents that iteratively refine their outputs. Each agent leverages an MoE layer in its final attention block, enabling efficient task decomposition while maintaining computational feasibility. This hybrid approach creates specialized pathways through both the model architecture and the agent collaboration layers. Experimental results demonstrate significant improvements across multiple language understanding and generation benchmarks, highlighting the synergistic benefits of combining expert routing at both the neural and agent levels.

</details>


### [35] [Enhancing Cyber-Resilience in Cyber-Physical Systems of Systems:A Methodical Approach](https://arxiv.org/abs/2511.14548)
*Elisabeth Vogel,Peter Langendörfer*

Main category: cs.CE

TL;DR: 提出改进的网络弹性生命周期框架用于CPSoS可持续风险缓解并介绍应用场景。


<details>
  <summary>Details</summary>
Motivation: 应对CPSoS复杂环境的挑战和弹性需求。

Method: 提出改进的网络弹性生命周期框架。

Result: 增强了CPSoS的适应性，支持应对系统复杂性和潜在干扰。

Conclusion: 概述了改进生命周期的应用场景，强调其对运营系统网络弹性的重要性。

Abstract: Cyber-physical Systems of Systems (CPSoS) are becoming increasingly prevalent across sectors such as Industry 4.0 and smart homes, where they play a critical role in enabling intelligent, interconnected functionality. Addressing the challenges and resilience requirements of these complex environments, we propose a modified Cyber-Resilience Life-Cycle as a practical framework for sustainable risk mitigation. Our approach enhances the adaptability of CPSoS and supports resilience against evolving system complexities and potential disruptions. We conclude by outlining application scenarios for the modified life-cycle and highlighting its relevance in fostering cyber-resilience in operational systems.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [36] [SQL-to-Text Generation with Weighted-AST Few-Shot Prompting](https://arxiv.org/abs/2511.13907)
*Sriom Chakrabarti,Chuangtao Ma,Arijit Khan,Sebastian Link*

Main category: cs.DB

TL;DR: 提出Weighted - AST retrieval with prompting方法解决SQL到文本生成中语义维护问题，在多数据集实验表现优。


<details>
  <summary>Details</summary>
Motivation: 当前SQL到文本生成方法难以维护SQL查询确切语义，尤其存在多种正确表述时。

Method: 提出Weighted - AST retrieval with prompting架构，基于带学习特征权重的抽象语法树（AST）的相似度度量检索语义相关示例作为少样本提示。

Result: 在Spider、SParC和CoSQL三个基准数据集上，执行准确率（EX）最高超当前基线17.24%，精确匹配（EM）表现优，人工评估语义保真度更一致，运行时性能有竞争力。

Conclusion: Weighted - AST prompting是从结构化数据库查询中获取自然语言解释的可扩展且有效的方法。

Abstract: SQL-to-Text generation aims at translating structured SQL queries into natural language descriptions, thereby facilitating comprehension of complex database operations for non-technical users. Although large language models (LLMs) have recently demonstrated promising results, current methods often fail to maintain the exact semantics of SQL queries, particularly when there are multiple possible correct phrasings. To address this problem, our work proposes Weighted-AST retrieval with prompting, an architecture that integrates structural query representations and LLM prompting. This method retrieves semantically relevant examples as few-shot prompts using a similarity metric based on an Abstract Syntax Tree (AST) with learned feature weights. Our structure-aware prompting technique ensures that generated descriptions are both fluent and faithful to the original query logic. Numerous experiments on three benchmark datasets - Spider, SParC, and CoSQL show that our method outperforms the current baselines by up to +17.24% in execution Accuracy (EX), performs superior in Exact Match (EM) and provides more consistent semantic fidelity when evaluated by humans, all while preserving competitive runtime performance. These results demonstrate that Weighted-AST prompting is a scalable and effective method for deriving natural language explanations from structured database queries.

</details>


### [37] [Fast Verification of Strong Database Isolation (Extended Version)](https://arxiv.org/abs/2511.14067)
*Zhiheng Cai,Si Liu,Hengfeng Wei,Yuxing Chen,Anqun Pan*

Main category: cs.DB

TL;DR: 本文提出快速验证器VeriStrong用于验证数据库强隔离性，采用超多边形形式化方法，评估显示其性能超越现有验证器。


<details>
  <summary>Details</summary>
Motivation: 现代数据库强隔离保证对数据一致性和完整性至关重要，验证这些保证在黑盒场景下具有挑战性。

Method: 提出超多边形形式化方法，开发验证可串行化和快照隔离的可靠且完整的编码，针对数据库工作负载特点定制SMT求解。

Result: VeriStrong在支持的工作负载上显著优于现有验证器，能扩展到更大、更通用的工作负载，检测隔离异常准确率高。

Conclusion: VeriStrong是一种高效准确的数据库强隔离验证器。

Abstract: Strong isolation guarantees, such as serializability and snapshot isolation, are essential for maintaining data consistency and integrity in modern databases. Verifying whether a database upholds its claimed guarantees is increasingly critical, as these guarantees form a contract between the vendor and its users. However, this task is challenging, particularly in black-box settings, where only observable system behavior is available and often involves uncertain dependencies between transactions.
  In this paper, we present VeriStrong, a fast verifier for strong database isolation. At its core is a novel formalism called hyper-polygraphs, which compactly captures both certain and uncertain transactional dependencies in database executions. Leveraging this formalism, we develop sound and complete encodings for verifying both serializability and snapshot isolation. To achieve high efficiency, VeriStrong tailors SMT solving to the characteristics of database workloads, in contrast to prior general-purpose approaches. Our extensive evaluation across diverse benchmarks shows that VeriStrong not only significantly outperforms state-of-the-art verifiers on the workloads they support, but also scales to large, general workloads beyond their reach, while maintaining high accuracy in detecting isolation anomalies.

</details>


### [38] [Chipmink: Efficient Delta Identification for Massive Object Graph](https://arxiv.org/abs/2511.14162)
*Supawit Chockchowwat,Sumay Thakurdesai,Zhaoheng Li,Matthew Krafczyk,Yongjoo Park*

Main category: cs.DB

TL;DR: 提出基于图的对象存储Chipmink，可高效部分持久化对象，实验显示其效果优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有对象持久化机制效率低，数据科学系统缺乏集中式缓冲区管理器，脏对象识别困难。

Method: 提出Chipmink，将对象分区成pods作为持久化单元，以最小化持久化成本，隔离脏对象。

Result: Chipmink通用性强，支持共享内存、GPU和远程对象，在存储大小和持久化速度上优于基线。

Conclusion: Chipmink能有效解决现有对象持久化机制的问题，实现高效部分持久化。

Abstract: Ranging from batch scripts to computational notebooks, modern data science tools rely on massive and evolving object graphs that represent structured data, models, plots, and more. Persisting these objects is critical, not only to enhance system robustness against unexpected failures but also to support continuous, non-linear data exploration via versioning. Existing object persistence mechanisms (e.g., Pickle, Dill) rely on complete snapshotting, often redundantly storing unchanged objects during execution and exploration, resulting in significant inefficiency in both time and storage. Unlike DBMSs, data science systems lack centralized buffer managers that track dirty objects. Worse, object states span various locations such as memory heaps, shared memory, GPUs, and remote machines, making dirty object identification fundamentally more challenging. In this work, we propose a graph-based object store, named Chipmink, that acts like the centralized buffer manager. Unlike static pages in DBMSs, persistence units in Chipmink are dynamically induced by partitioning objects into appropriate subgroups (called pods), minimizing expected persistence costs based on object sizes and reference structure. These pods effectively isolate dirty objects, enabling efficient partial persistence. Our experiments show that Chipmink is general, supporting libraries that rely on shared memory, GPUs, and remote objects. Moreover, Chipmink achieves up to 36.5x smaller storage sizes and 12.4x faster persistence than the best baselines in real-world notebooks and scripts.

</details>


### [39] [Gradient-Based Join Ordering](https://arxiv.org/abs/2511.14482)
*Tim Schwabe,Maribel Acosta*

Main category: cs.DB

TL;DR: 提出基于梯度的连接排序方法，在图数据集上表现好且运行时间更优。


<details>
  <summary>Details</summary>
Motivation: 传统连接排序方法计算复杂度高、可扩展性有限，需更高效方法。

Method: 将查询计划连续松弛为软邻接矩阵，用Gumbel - Softmax参数化和可微约束，以图神经网络为成本模型进行基于梯度的搜索。

Result: 在两个图数据集上找到的计划成本与传统方法相当甚至更低，运行时间随查询规模线性增长。

Conclusion: 基于梯度的连接排序是迈向更有效查询优化器的第一步。

Abstract: Join ordering is the NP-hard problem of selecting the most efficient sequence in which to evaluate joins (conjunctive, binary operators) in a database query. As the performance of query execution critically depends on this choice, join ordering lies at the core of query optimization. Traditional approaches cast this problem as a discrete combinatorial search over binary trees guided by a cost model, but they often suffer from high computational complexity and limited scalability. We show that, when the cost model is differentiable, the query plans can be continuously relaxed into a soft adjacency matrix representing a superposition of plans. This continuous relaxation, together with a Gumbel-Softmax parameterization of the adjacency matrix and differentiable constraints enforcing plan validity, enables gradient-based search for plans within this relaxed space. Using a learned Graph Neural Network as the cost model, we demonstrate that this gradient-based approach can find comparable and even lower-cost plans compared to traditional discrete local search methods on two different graph datasets. Furthermore, we empirically show that the runtime of this approach scales linearly with query size, in contrast to quadratic or exponential runtimes of classical approaches. We believe this first step towards gradient-based join ordering can lead to more effective and efficient query optimizers in the future.

</details>


### [40] [Overview and Prospects of Using Integer Surrogate Keys for Data Warehouse Performance Optimization](https://arxiv.org/abs/2511.14502)
*Sviatoslav Stumpf,Vladislav Povyshev*

Main category: cs.DB

TL;DR: 本文探讨用基于整数的日期时间标签优化数据仓库和时间序列性能，展示其能减少存储、加速查询及提升吞吐量，实际案例证实其有效性。


<details>
  <summary>Details</summary>
Motivation: 研究基于整数的日期时间标签对数据仓库和时间序列性能的优化作用。

Method: 提出实用格式和算法，并在实际工作负载中验证效率，还给出索引、聚合、压缩和批处理算法。

Result: 用32和64位整数格式替代标准类型，减少30 - 60%存储需求，加快25 - 40%查询执行速度，吞吐量最高提升八倍。

Conclusion: 实际案例证明该方法高效且通用。

Abstract: The aim of this paper is to examine and demonstrate how integer-based datetime labels (integer surrogate keys for time) can optimize data-warehouse and time-series performance, proposing practical formats and algorithms and validating their efficiency on real-world workloads. It is shown that replacing standard DATE and TIMESTAMP types with 32- and 64-bit integer formats reduces storage requirements by 30-60 percent and speeds up query execution by 25-40 percent. The paper presents indexing, aggregation, compression, and batching algorithms demonstrating up to an eightfold increase in throughput. Practical examples from finance, telecommunications, IoT, and scientific research confirm the efficiency and versatility of the proposed approach.

</details>


### [41] [Scalable Enforcement of Fine Grained Access Control Policies in Relational Database Management Systems](https://arxiv.org/abs/2511.14629)
*Anadi Shakya,Primal Pappachan,David Maier,Roberto Yus,Sharad Mehrotra,Johann-Christoph Freytag*

Main category: cs.DB

TL;DR: 现有FGAC策略执行方法扩展性差，本文提出Sieve中间件，结合查询重写和缓存优化FGAC策略执行，实验表明其可扩展且性能提升显著。


<details>
  <summary>Details</summary>
Motivation: 智能技术发展和隐私法规要求管理DBMS中的FGAC策略，但现有方法扩展性差，导致查询性能下降和系统有效性降低。

Method: 提出Sieve中间件，结合查询重写和缓存，查询重写使用带保护表达式分组和过滤策略，缓存机制有有效替换和刷新策略。

Result: 在两个DBMS上实验，Sieve可扩展到大型数据集和策略集，保持低查询延迟和系统负载，策略评估性能提升2 - 10倍，缓存扩展在动态工作负载下使查询性能提升6 - 22%。

Conclusion: Sieve适用于智能环境的实时访问控制，支持高效、可扩展的用户偏好和隐私策略管理。

Abstract: The proliferation of smart technologies and evolving privacy regulations such as the GDPR and CPRA has increased the need to manage fine-grained access control (FGAC) policies in database management systems (DBMSs). Existing approaches to enforcing FGAC policies do not scale to thousands of policies, leading to degraded query performance and reduced system effectiveness. We present Sieve, a middleware for relational DBMSs that combines query rewriting and caching to optimize FGAC policy enforcement. Sieve rewrites a query with guarded expressions that group and filter policies and can efficiently use indexes in the DBMS. It also integrates a caching mechanism with an effective replacement strategy and a refresh mechanism to adapt to dynamic workloads. Experiments on two DBMSs with real and synthetic datasets show that Sieve scales to large datasets and policy corpora, maintaining low query latency and system load and improving policy evaluation performance by between 2x and 10x on workloads with 200 to 1,200 policies. The caching extension further improves query performance by between 6 and 22 percent under dynamic workloads, especially with larger cache sizes. These results highlight Sieve's applicability for real-time access control in smart environments and its support for efficient, scalable management of user preferences and privacy policies.

</details>


### [42] [Natural Language Interfaces for Databases: What Do Users Think?](https://arxiv.org/abs/2511.14718)
*Panos Ipeirotis,Haotian Zheng*

Main category: cs.DB

TL;DR: 研究对比SQL - LLM和Snowflake平台，发现SQL - LLM在查询完成时间、准确率等方面表现更优，强调NLIDB可用性重要性。


<details>
  <summary>Details</summary>
Motivation: 尽管NLIDB翻译准确性有进步，但关键可用性挑战未充分研究，需对其可用性维度进行调查。

Method: 进行混合方法用户研究，让20名参与者在两个平台完成12个现实数据库查询任务。

Result: SQL - LLM使查询完成时间减少10 - 30%，准确率从50%提升到75%，查询重写少，错误恢复快，用户挫败感低，鼓励结构化查询策略。

Conclusion: 设计良好、用户友好的NLIDB在商业分析中具有实际意义，强调现实部署中可用性和技术准确性同样重要。

Abstract: Natural Language Interfaces for Databases (NLIDBs) aim to make database querying accessible by allowing users to ask questions in everyday language rather than using formal SQL queries. Despite significant advancements in translation accuracy, critical usability challenges, such as user frustration, query refinement strategies, and error recovery, remain underexplored. To investigate these usability dimensions, we conducted a mixed-method user study comparing SQL-LLM, a state-of-the-art NL2SQL system, with Snowflake, a traditional SQL analytics platform. Our controlled evaluation involved 20 participants completing realistic database querying tasks across 12 queries each. Results show that SQL-LLM significantly reduced query completion times by 10 to 30 percent (mean: 418 s vs. 629 s, p = 0.036) and improved overall accuracy from 50 to 75 percent (p = 0.002). Additionally, participants using SQL-LLM exhibited fewer query reformulations, recovered from errors 30 to 40 seconds faster, and reported lower frustration levels compared to Snowflake users. Behavioral analysis revealed that SQL-LLM encouraged structured, schema-first querying strategies, enhancing user confidence and efficiency, particularly for complex queries. These findings underscore the practical significance of well-designed, user-friendly NLIDBs in business analytics settings, emphasizing the critical role of usability alongside technical accuracy in real-world deployments.

</details>


### [43] [Cloud-Native Vector Search: A Comprehensive Performance Analysis](https://arxiv.org/abs/2511.14748)
*Zhaoheng Li,Wei Ding,Silu Huang,Zikang Wang,Yuanjin Lin,Ke Wu,Yongjoo Park,Jianjun Chen*

Main category: cs.DB

TL;DR: 本文系统研究云原生向量搜索，分析不同索引在远程存储上的瓶颈，发现图索引在特定场景更优，且云搜索与磁盘搜索参数需求不同，还研究了缓存与索引优化的关系。


<details>
  <summary>Details</summary>
Motivation: 数据和任务复杂度增长促使将向量索引置于远程存储，即云原生向量搜索，但云服务提供商默认使用集群索引，需系统研究云原生向量搜索。

Method: 分析集群和图这两种常见索引类在远程存储上的瓶颈，结合现有基于云的缓存设置进行研究。

Result: 图索引在需要高并发和召回、处理高维数据或大型数据类型的工作负载中更受青睐；云搜索与磁盘搜索在索引和搜索参数设置上有显著差异；某些索引优化不利于缓存。

Conclusion: 需根据不同场景选择合适的索引，同时考虑索引优化与缓存的关系以实现最优性能。

Abstract: Vector search has been widely employed in recommender system and retrieval-augmented-generation pipelines, commonly performed with vector indexes to efficiently find similar items in large datasets. Recent growths in both data and task complexity have motivated placing vector indexes onto remote storage -- cloud-native vector search, which cloud providers have recently introduced services for. Yet, despite varying workload characteristics and various available vector index forms, providers default to using cluster-based indexes, which on paper do adapt well to differences between disk and cloud-based environment: their fetch granularities and lack of notable intra-query dependencies aligns with the large optimal fetch sizes and minimizes costly round-trips (i.e., as opposed to graph-based indexes) to remote storage, respectively.
  This paper systematically studies cloud-native vector search: What and how should indexes be built and used for on-cloud vector search? We analyze bottlenecks of two common index classes, cluster and graph indexes, on remote storage, and show that despite current standardized adoption of cluster indexes on the cloud, graph indexes are favored in workloads requiring high concurrency and recall, or operating on high-dimensional data or large datatypes. We further find that on-cloud search demands significantly different indexing and search parameterizations versus on-disk search for optimal performance. Finally, we incorporate existing cloud-based caching setups into vector search and find that certain index optimizations work against caching, and study how this can be mitigated to maximize gains under various available cache sizes.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [44] [Boosting performance: Gradient Clock Synchronisation with two-way measured links](https://arxiv.org/abs/2511.13727)
*Sophie Wenning*

Main category: cs.DC

TL;DR: 本文将GCS算法的形式化模型扩展到接近实际实现的假设下，通过将单向测量范式改为双向测量范式，去除了许多先前限制，提升了算法性能。


<details>
  <summary>Details</summary>
Motivation: 将GCS算法形式化模型扩展到接近实际实现的假设下，去除先前诸多限制以提升性能。

Method: 用双向测量范式取代先前工作中的单向测量范式。

Result: 去除了许多限制，如取消单位链路长度要求，提供频率源形式化模型，细粒度区分估算误差组件并大幅降低其影响，减少不确定性对估算误差的贡献，给出本地和全局偏差上限。

Conclusion: 通过范式改变，在保持GCS核心行为的同时，改进了GCS算法的形式化模型，提升了实际应用性能。

Abstract: This master thesis extends the formal model of the GCS algorithm as presented by (Fan and Lynch 2004, 325), (Lenzen, Locher and Wattenhofer 2008, 510) and (Függer et al. 2023) to operate under implementation-near assumptions by replacing the one-way measurement paradigm assumed in prior work by the two-way measurement paradigm. With this change of paradigm, we remove many restrictions previously enforced to allow provable performance. Most notability, while maintaining the core behaviour of GCS, we: 1. Lift the requirement for unitary link lengths and thereby create a realistic model for flexible deployment of implementations of GCS in practice. 2. Provide a formal model of frequency sources assumed in prior work. 3. Perform a fine grained distinction between the different components of the algorithm's estimation error and globally reduce its impact by multiple orders of magnitude. 4. Significantly reduce the contribution of the uncertainty to the algorithm's estimation error to be in the range of 10\% to 0,1\% of the delay per link instead of being in the oder of the delay per link as in prior work and show matching upper bounds on the local and global skew of GCS.

</details>


### [45] [Gaia: Hybrid Hardware Acceleration for Serverless AI in the 3D Compute Continuum](https://arxiv.org/abs/2511.13728)
*Maximilian Reisecker,Cynthia Marcelino,Thomas Pusztai,Stefan Nastic*

Main category: cs.DC

TL;DR: 提出Gaia模型与架构解决无服务器AI在异构环境中硬件加速管理问题，可降低端到端延迟。


<details>
  <summary>Details</summary>
Motivation: 当前平台难以有效管理硬件加速，静态分配和一次性动态选择存在问题，无法保证SLO合规和成本效益。

Method: 提出Gaia，结合轻量级执行模式标识符和动态函数运行时，根据函数代码和用户定义的SLO在CPU和GPU后端之间切换。

Result: 能为工作负载无缝选择最佳硬件加速，端到端延迟最多降低95%。

Conclusion: Gaia能在异构环境中实现无服务器AI的SLO感知、成本高效的加速。

Abstract: Serverless computing offers elastic scaling and pay-per-use execution, making it well-suited for AI workloads. As these workloads run in heterogeneous environments such as the Edge-Cloud-Space 3D Continuum, they often require intensive parallel computation, which GPUs can perform far more efficiently than CPUs. However, current platforms struggle to manage hardware acceleration effectively, as static user-device assignments fail to ensure SLO compliance under varying loads or placements, and one-time dynamic selections often lead to suboptimal or cost-inefficient configurations. To address these issues, we present Gaia, a GPU-as-a-service model and architecture that makes hardware acceleration a platform concern. Gaia combines (i) a lightweight Execution Mode Identifier that inspects function code at deploy time to emit one of four execution modes, and a Dynamic Function Runtime that continuously reevaluates user-defined SLOs to promote or demote between CPU- and GPU backends. Our evaluation shows that it seamlessly selects the best hardware acceleration for the workload, reducing end-to-end latency by up to 95%. These results indicate that Gaia enables SLO-aware, cost-efficient acceleration for serverless AI across heterogeneous environments.

</details>


### [46] [TT-Edge: A Hardware-Software Co-Design for Energy-Efficient Tensor-Train Decomposition on Edge AI](https://arxiv.org/abs/2511.13738)
*Hyunseok Kwak,Kyeongwon Lee,Kyeongpil Min,Chaebin Jung,Woojoo Lee*

Main category: cs.DC

TL;DR: 提出TT - Edge框架解决边缘设备上TTD压缩的延迟和能耗瓶颈，实现加速和节能。


<details>
  <summary>Details</summary>
Motivation: 分布式学习对边缘设备模型压缩需求增长，TTD虽压缩比高但SVD和矩阵乘法在低功耗处理器上有高延迟和能耗。

Method: 将SVD拆分为双对角化和对角化两阶段，将计算密集任务卸载到专用TTD引擎，与GEMM加速器集成。

Result: 在RISC - V边缘AI处理器上，压缩ResNet 32模型时比仅GEMM基线提速1.7倍，能耗降低40.2%，总功率仅增4%，硬件开销小。

Conclusion: TT - Edge有效解决边缘环境中TTD压缩的延迟和能耗瓶颈。

Abstract: The growing demands of distributed learning on resource constrained edge devices underscore the importance of efficient on device model compression. Tensor Train Decomposition (TTD) offers high compression ratios with minimal accuracy loss, yet repeated singular value decompositions (SVDs) and matrix multiplications can impose significant latency and energy costs on low power processors. In this work, we present TT-Edge, a hardware software co designed framework aimed at overcoming these challenges. By splitting SVD into two phases--bidiagonalization and diagonalization--TT-Edge offloads the most compute intensive tasks to a specialized TTD Engine. This engine integrates tightly with an existing GEMM accelerator, thereby curtailing the frequent matrix vector transfers that often undermine system performance and energy efficiency. Implemented on a RISC-V-based edge AI processor, TT-Edge achieves a 1.7x speedup compared to a GEMM only baseline when compressing a ResNet 32 model via TTD, while reducing overall energy usage by 40.2 percent. These gains come with only a 4 percent increase in total power and minimal hardware overhead, enabled by a lightweight design that reuses GEMM resources and employs a shared floating point unit. Our experimental results on both FPGA prototypes and post-synthesis power analysis at 45 nm demonstrate that TT-Edge effectively addresses the latency and energy bottlenecks of TTD based compression in edge environments.

</details>


### [47] [Inside VOLT: Designing an Open-Source GPU Compiler](https://arxiv.org/abs/2511.13751)
*Shinnung Jeong,Chihyo Ahn,Huanzhi Pu,Jisheng Zhao,Hyesoon Kim,Blaise Tine*

Main category: cs.DC

TL;DR: 介绍了Vortex优化轻量级工具链（VOLT）的设计原则、结构和关键编译器转换，展示其跨抽象层次支持SIMT执行及扩展能力。


<details>
  <summary>Details</summary>
Motivation: 现有开源GPU架构执行和优化GPU程序依赖复杂编译器框架，增加开源硬件开发成本，需解决方案。

Method: 提出VOLT，采用分层设计实现跨抽象层次SIMT代码生成和优化，将SIMT分析和优化集中在中端。

Result: 通过两个案例研究展示了VOLT对ISA扩展和主机运行时API的支持能力。

Conclusion: VOLT能支持跨多种前端语言和开源GPU硬件的SIMT执行，且易于适应架构演变和扩展。

Abstract: Recent efforts in open-source GPU research are opening new avenues in a domain that has long been tightly coupled with a few commercial vendors. Emerging open GPU architectures define SIMT functionality through their own ISAs, but executing existing GPU programs and optimizing performance on these ISAs relies on a compiler framework that is technically complex and often undercounted in open hardware development costs.
  To address this challenge, the Vortex-Optimized Lightweight Toolchain (VOLT) has been proposed. This paper presents its design principles, overall structure, and the key compiler transformations required to support SIMT execution on Vortex. VOLT enables SIMT code generation and optimization across multiple levels of abstraction through a hierarchical design that accommodates diverse front-end languages and open GPU hardware. To ensure extensibility as GPU architectures evolve, VOLT centralizes fundamental SIMT-related analyses and optimizations in the middle-end, allowing them to be reused across front-ends and easily adapted to emerging open-GPU variants. Through two case studies on ISA extensions and host-runtime API, this paper also demonstrates how VOLT can support extensions

</details>


### [48] [What happens when nanochat meets DiLoCo?](https://arxiv.org/abs/2511.13761)
*Alexander Acker,Soeren Becker,Sasho Nedelkoski,Dominik Scheinert,Odej Kao,Philipp Wiesner*

Main category: cs.DC

TL;DR: 研究分布式通信受限训练的模型权衡，用nanochat对比DiLoCo和DDP，发现DiLoCo有不可逆表征漂移。


<details>
  <summary>Details</summary>
Motivation: 探索分布式通信受限训练带来的模型权衡。

Method: 以nanochat为基线，实现DiLoCo算法并与DDP对比。

Result: DiLoCo预训练收敛稳定但下游评估分数差，用其预训练权重后续用DDP也无法恢复性能。

Conclusion: DiLoCo存在异步更新导致的不可逆表征漂移，影响下游对齐。

Abstract: Although LLM training is typically centralized with high-bandwidth interconnects and large compute budgets, emerging methods target communication-constrained training in distributed environments. The model trade-offs introduced by this shift remain underexplored, and our goal is to study them.
  We use the open-source nanochat project, a compact 8K-line full-stack ChatGPT-like implementation containing tokenization, pretraining, fine-tuning, and serving, as a controlled baseline. We implement the DiLoCo algorithm as a lightweight wrapper over nanochat's training loop, performing multiple local steps per worker before synchronization with an outer optimizer, effectively reducing communication by orders of magnitude. This inner-outer training is compared against a standard data-parallel (DDP) setup. Because nanochat is small and inspectable, it enables controlled pipeline adaptations and allows direct comparison with the conventional centralized baseline.
  DiLoCo achieves stable convergence and competitive loss in pretraining but yields worse MMLU, GSM8K, and HumanEval scores after mid-training and SFT. We discover that using DiLoCo-pretrained weights and running mid- and post-training with DDP fails to recover performance, revealing irreversible representation drift from asynchronous updates that impairs downstream alignment. We provide this implementation as an official fork of nanochat on GitHub.

</details>


### [49] [Guaranteed DGEMM Accuracy While Using Reduced Precision Tensor Cores Through Extensions of the Ozaki Scheme](https://arxiv.org/abs/2511.13778)
*Angelika Schwarz,Anton Anders,Cole Brower,Harun Bayraktar,John Gunnels,Kate Clark,RuQing G. Xu,Samuel Rodriguez,Sebastien Cayrols,Paweł Tabaszewski,Victor Podlozhnyuk*

Main category: cs.DC

TL;DR: 提出自动动态精度（ADP）框架，利用低精度单元模拟FP64矩阵乘法，在保证精度的同时提高效率。


<details>
  <summary>Details</summary>
Motivation: 硬件向低精度发展，需研究用低精度单元模拟双精度精度的算法。

Method: 提出Exponent Span Capacity（ESC）估算器确定分解参数，集成异常处理等机制，改进Ozaki分解。

Result: ADP在具有挑战性的输入上保持FP64保真度，运行时间开销小于10%，在特定设置下有显著加速。

Conclusion: 低精度加速器可作为高保真和高性能科学计算工作负载的实用基础。

Abstract: The rapid growth of artificial intelligence (AI) has made low-precision formats such as FP16, FP8, and, most recently, block-scaled FP4 the primary focus of modern GPUs, where Tensor Cores now deliver orders-of-magnitude higher throughput than traditional FP64 pipelines. This hardware shift has sparked a new line of algorithm research: using low-precision units to emulate double-precision accuracy through schemes such as Ozaki decompositions. We advance this direction with Automatic Dynamic Precision (ADP), a fully GPU-resident framework that makes emulated FP64 matrix multiplication both efficient and reliable. At its core is the Exponent Span Capacity (ESC), a hardware-agnostic estimator that conservatively determines the decomposition parameter (also known as slices) required to achieve FP64-level accuracy. Built on ESC, ADP integrates exception handling, run time heuristics, and seamless fallback to native FP64, ensuring correctness without host-device synchronization or user intervention. Additionally, we further improve Ozaki-style decompositions with an unsigned integer slicing scheme, which increases representational efficiency and reduces computational waste. Validated against recently proposed BLAS grading tests, ADP consistently preserves FP64 fidelity on challenging inputs while incurring less than 10% run time overhead. In a 55-bit mantissa setting, our approach achieves up to 2.3x and 13.2x speedups over native FP64 GEMM on NVIDIA Blackwell GB200 and the RTX Pro 6000 Blackwell Server Edition, respectively. Our results demonstrate that low-precision accelerators can serve as a practical, production-ready foundation for high-fidelity and high-performance scientific computing workloads.

</details>


### [50] [Semantic Multiplexing](https://arxiv.org/abs/2511.13779)
*Mohammad Abdi,Francesca Meneghello,Francesco Restuccia*

Main category: cs.DC

TL;DR: 本文提出语义复用概念，可在语义层扩展有效自由度，实现多任务并行处理，实验证明其能保持任务精度，降低延迟、能耗和通信负载。


<details>
  <summary>Details</summary>
Motivation: 现有通信系统仅支持比特级并行传输，限制了可并发处理的任务数量，需解决此瓶颈。

Method: 引入语义复用概念，将流复用从比特转移到任务，把多个任务相关的压缩表示合并为单个语义表示。

Result: 在实验平台上测试，语义复用能在语义层联合处理多任务，保持足够任务精度，如4×4信道上任务数从2增至8时图像分类精度下降小于4%，相比基线分别最多降低8倍延迟、25倍能耗和54倍通信负载。

Conclusion: 语义复用可在不增加天线或带宽的情况下，实现更多任务的复用，且不违背香农容量规则，有良好性能和应用价值，代码和数据集将公开共享。

Abstract: Mobile devices increasingly require the parallel execution of several computing tasks offloaded at the wireless edge. Existing communication systems only support parallel transmissions at the bit level, which fundamentally limits the number of tasks that can be concurrently processed. To address this bottleneck, this paper introduces the new concept of Semantic Multiplexing. Our approach shifts stream multiplexing from bits to tasks by merging multiple task-related compressed representations into a single semantic representation. As such, Semantic Multiplexing can multiplex more tasks than the number of physical channels without adding antennas or widening bandwidth by extending the effective degrees of freedom at the semantic layer, without contradicting Shannon capacity rules. We have prototyped Semantic Multiplexing on an experimental testbed with Jetson Orin Nano and millimeter-wave software-defined radios and tested its performance on image classification and sentiment analysis while comparing to several existing baselines in semantic communications. Our experiments demonstrate that Semantic Multiplexing allows jointly processing multiple tasks at the semantic level while maintaining sufficient task accuracy. For example, image classification accuracy drops by less than 4% when increasing from 2 to 8 the number of tasks multiplexed over a 4$\times$4 channel. Semantic Multiplexing reduces latency, energy consumption, and communication load respectively by up to 8$\times$, 25$\times$, and 54$\times$ compared to the baselines while keeping comparable performance. We pledge to publicly share the complete software codebase and the collected datasets for reproducibility.

</details>


### [51] [Do MPI Derived Datatypes Actually Help? A Single-Node Cross-Implementation Study on Shared-Memory Communication](https://arxiv.org/abs/2511.13804)
*Temitayo Adefemi*

Main category: cs.DC

TL;DR: 本文对MPI派生数据类型（DDTs）在三个2D应用中的性能进行跨实现评估，结果表明无策略能在各程序、语义和MPI栈中占优，建议针对特定实现和模式进行性能分析。


<details>
  <summary>Details</summary>
Motivation: MPI派生数据类型（DDTs）的实际性能存在争议且常仅针对单一MPI栈报告，需跨实现评估其性能。

Method: 使用三个2D应用，每种应用用手动打包和DDT两种方式编写，对相同通信语义进行基准测试，在单个ARCHER2节点上对四种广泛使用的MPI实现进行强扩展和弱扩展测试。

Result: DDTs性能表现不一，在某些代码和MPI栈中最快，在其他情况下最慢；不同应用中BASIC和DDT版本性能排名不同；存在特定栈的异常情况。

Conclusion: 无策略能在各程序、语义和MPI栈中占优，DDT性能可移植性无保证，建议针对特定MPI实现和通信模式对DDT和手动打包设计进行性能分析，多节点和GPU相关研究留待未来。

Abstract: MPI's derived datatypes (DDTs) promise easier, copy-free communication of non-contiguous data, yet their practical performance remains debated and is often reported only for a single MPI stack. We present a cross-implementation assessment using three 2D applications: a Jacobi CFD solver, Conway's Game of Life, and a lattice-based image reconstruction. Each application is written in two ways: (i) a BASIC version with manual packing and unpacking of non-contiguous regions and (ii) a DDT version using MPI_Type_vector and MPI_Type_create_subarray with correct true extent via MPI_Type_create_resized. For API parity, we benchmark identical communication semantics: non-blocking point-to-point (Irecv/Isend + Waitall), neighborhood collectives (MPI_Neighbor_alltoallw), and MPI-4 persistent operations (*_init). We run strong and weak scaling on 1-4 ranks, validate bitwise-identical halos, and evaluate four widely used MPI implementations: MPICH, Open MPI, Intel MPI, and MVAPICH2 on a single ARCHER2 node. Results are mixed. DDTs can be fastest, for example for the image reconstruction code on Intel MPI and MPICH, but can also be among the slowest on other stacks, such as Open MPI and MVAPICH2 for the same code. For the CFD solver, BASIC variants generally outperform DDTs across semantics, whereas for Game of Life the ranking flips depending on the MPI library. We also observe stack-specific anomalies, for example MPICH slowdowns with DDT neighborhood and persistent modes. Overall, no strategy dominates across programs, semantics, and MPI stacks; performance portability for DDTs is not guaranteed. We therefore recommend profiling both DDT-based and manual-packing designs under the intended MPI implementation and communication mode. Our study is limited to a single node and does not analyze memory overhead; multi-node and GPU-aware paths are left for future work.

</details>


### [52] [ParallelKittens: Systematic and Practical Simplification of Multi-GPU AI Kernels](https://arxiv.org/abs/2511.13940)
*Stuart H. Sul,Simran Arora,Benjamin F. Spector,Christopher Ré*

Main category: cs.DC

TL;DR: 提出ParallelKittens框架简化多GPU内核开发，验证有显著加速效果。


<details>
  <summary>Details</summary>
Motivation: 现有系统在异构工作负载和新加速器上难达理论峰值性能，需简单原则指导多GPU内核设计。

Method: 扩展ThunderKittens框架，通过八个核心原语和统一编程模板体现多GPU内核设计原则。

Result: 在Hopper和Blackwell架构验证，少量代码对不同并行工作负载有显著加速。

Conclusion: ParallelKittens框架能简化多GPU内核开发并提升性能。

Abstract: Inter-GPU communication has become a major bottleneck for modern AI workloads as models scale and improvements in hardware compute throughput outpace improvements in interconnect bandwidth. Existing systems mitigate this through compute-communication overlap but often fail to meet theoretical peak performance across heterogeneous workloads and new accelerators. Instead of operator-specific techniques, we ask whether a small set of simple, reusable principles can systematically guide the design of optimal multi-GPU kernels. We present ParallelKittens (PK), a minimal CUDA framework that drastically simplifies the development of overlapped multi-GPU kernels. PK extends the ThunderKittens framework and embodies the principles of multi-GPU kernel design through eight core primitives and a unified programming template, derived from a comprehensive analysis of the factors that govern multi-GPU performance$\unicode{x2014}$data-transfer mechanisms, resource scheduling, and design overheads. We validate PK on both Hopper and Blackwell architectures. With fewer than 50 lines of device code, PK achieves up to $2.33 \times$ speedup for data- and tensor-parallel workloads, $4.08 \times$ for sequence-parallel workloads, and $1.22 \times$ for expert-parallel workloads.

</details>


### [53] [FailSafe: High-performance Resilient Serving](https://arxiv.org/abs/2511.14116)
*Ziyi Xu,Zhiqiang Xie,Swapnil Gandhi,Christos Kozyrakis*

Main category: cs.DC

TL;DR: 提出容错张量并行服务系统FailSafe，在GPU故障时维持高性能，评估显示效果优于标准方法。


<details>
  <summary>Details</summary>
Motivation: 现有张量并行系统在GPU故障时脆弱，会导致执行中断、KVCache重计算等问题，需要容错系统。

Method: 引入循环KVCache放置、混合注意力、细粒度负载感知路由平衡计算和内存；采用主动KVCache备份和按需权重恢复避免重计算和冗余数据传输。

Result: 在8xH100 DGX系统上评估，吞吐量最高达2倍，恢复延迟降低两个数量级，多GPU故障时仍维持高吞吐量和均衡利用率。

Conclusion: FailSafe能在动态不可靠硬件条件下实现稳健高效的大语言模型服务。

Abstract: Tensor parallelism (TP) enables large language models (LLMs) to scale inference efficiently across multiple GPUs, but its tight coupling makes systems fragile: a single GPU failure can halt execution, trigger costly KVCache recomputation, and introduce long-term compute and memory imbalance. We present FailSafe, a fault-tolerant TP serving system that sustains high performance under irregular GPU availability. FailSafe introduces three techniques to balance computation and memory across GPUs: (1) Cyclic KVCache Placement for uniform memory utilization, (2) Hybrid Attention combining tensor- and data-parallel attention to eliminate stragglers, and (3) Fine-Grained Load-Aware Routing to dynamically balance requests. It further employs proactive KVCache backup and on-demand weight recovery to avoid expensive recomputation and redundant data transfers. We implement these techniques in a lightweight serving engine compatible with existing LLM infrastructures. Evaluated on an 8xH100 DGX system with real-world fault traces and representative workloads, FailSafe achieves up to 2x higher throughput and two orders of magnitude lower recovery latency compared to standard fault handling approaches. Even with up to three GPU failures, FailSafe sustains high throughput and balanced utilization, demonstrating robust and efficient LLM serving under dynamic and unreliable hardware conditions.

</details>


### [54] [10Cache: Heterogeneous Resource-Aware Tensor Caching and Migration for LLM Training](https://arxiv.org/abs/2511.14124)
*Sabiha Afroz,Redwan Ibne Seraj Khan,Hadeel Albahar,Jingoo Han,Ali R. Butt*

Main category: cs.DC

TL;DR: 云环境下大语言模型训练有内存瓶颈，现有方法有缺陷，提出10Cache系统加速训练，在多方面有显著提升。


<details>
  <summary>Details</summary>
Motivation: 云环境训练大语言模型面临GPU内存瓶颈，现有卸载方法有高迁移延迟和内存利用不佳问题，增加训练时间和成本。

Method: 10Cache通过分析张量执行顺序构建预取策略，基于张量大小分布在固定内存中分配缓冲区，复用缓冲区减少分配开销。

Result: 在多种大语言模型工作负载中，训练时间最多加速2倍，GPU缓存命中率最多提高86.6倍，CPU和GPU内存利用率分别最多提高2.15倍和1.33倍。

Conclusion: 10Cache是优化云环境大语言模型训练吞吐量和资源效率的实用且可扩展的解决方案。

Abstract: Training large language models (LLMs) in the cloud faces growing memory bottlenecks due to the limited capacity and high cost of GPUs. While GPU memory offloading to CPU and NVMe has made large-scale training more feasible, existing approaches suffer from high tensor migration latency and suboptimal device memory utilization, ultimately increasing training time and cloud costs. To address these challenges, we present 10Cache, a resource-aware tensor caching and migration system that accelerates LLM training by intelligently coordinating memory usage across GPU, CPU, and NVMe tiers. 10Cache profiles tensor execution order to construct prefetch policies, allocates memory buffers in pinned memory based on tensor size distributions, and reuses memory buffers to minimize allocation overhead.
  Designed for cloud-scale deployments, 10Cache improves memory efficiency and reduces reliance on high-end GPUs. Across diverse LLM workloads, it achieves up to 2x speedup in training time, improves GPU cache hit rate by up to 86.6x, and increases CPU/GPU memory utilization by up to 2.15x and 1.33x, respectively, compared to state-of-the-art offloading methods. These results demonstrate that 10Cache is a practical and scalable solution for optimizing LLM training throughput and resource efficiency in cloud environments.

</details>


### [55] [Hyperion: Hierarchical Scheduling for Parallel LLM Acceleration in Multi-tier Networks](https://arxiv.org/abs/2511.14450)
*Mulei Ma,Minrui Xu,Zihan Chen,Yang Yang,Tony Q. S. Quek*

Main category: cs.DC

TL;DR: 提出Hyperion框架联合优化分区和调度以减少多层级网络中流水线式大语言模型推理的端到端延迟，实验显示有显著效果和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 边缘、雾和云层级中有限的GPU内存、异构计算和可变的层间带宽限制部署，且模型分区和请求调度问题紧密耦合，需要联合优化以降低端到端延迟。

Method: 采用分层两阶段框架Hyperion，第一阶段用Binary Search with Dynamic Programming (BSDP)进行离线层间分区，第二阶段用Adaptive Real-time Task Scheduling (ARTS)算法进行在线层内调度。

Result: 与GPipe和HEFT基线相比，使用Phi - 3 - medium模型时，Hyperion分别最多降低52.1%和31.2%的端到端延迟；在长序列生成中可扩展性好，比GPipe延迟低44.5%，GPU利用率更高。

Conclusion: Hyperion能有效联合优化分区和调度，显著降低端到端延迟，有良好的可扩展性和GPU利用率。

Abstract: Large Language Models (LLMs) are increasingly executed across edge, fog, and cloud tiers where limited GPU memory, heterogeneous compute, and variable inter-tier bandwidth jointly constrain deployment and motivate model partitioning and request scheduling. In this setting, achieving low end-to-end latency is governed not only by where a model is deployed (inter-tier model partitioning) but also by how incoming requests are scheduled (intra-tier task scheduling) across heterogeneous nodes. These two problems are tightly coupled, as a suboptimal scheduler can negate the benefits of a good partition, and vice versa. In this paper, we propose Hyperion, a hierarchical two-stage framework that jointly optimizes partitioning and scheduling to minimize end-to-end latency for pipelined LLM inference in multi-tier networks, balancing compute and memory across tiers while introducing negligible runtime overhead and requiring no model retraining. Motivated by the observation that partition choices evolve on slower timescales than request arrivals, Stage 1 performs offline, inter-tier partitioning via a Binary Search with Dynamic Programming (BSDP) procedure to produce balanced stage times under tier capacity and memory constraints; to adapt to time-varying load, Stage 2 performs online, intra-tier scheduling with a lightweight Adaptive Real-time Task Scheduling (ARTS) algorithm that maps each request to the best available node using real-time estimates of queue length and effective capacity. Experimental results on multi-tier inference tasks demonstrate that Hyperion significantly reduces end-to-end latency by up to 52.1\% and 31.2\%, with the Phi-3-medium model, compared to the GPipe and HEFT baselines, respectively. Furthermore, Hyperion shows superior scalability in long-sequence generation, maintaining a 44.5\% lower latency than GPipe and achieving higher GPU utilization.

</details>


### [56] [Analyzing the Impact of Participant Failures in Cross-Silo Federated Learning](https://arxiv.org/abs/2511.14456)
*Fabian Stricker,David Bermbach,Christian Zirpins*

Main category: cs.DC

TL;DR: 论文研究跨组织少参与者的跨筒仓联邦学习中参与者故障对模型质量的影响，指出高偏斜下评估乐观及时间影响模型质量。


<details>
  <summary>Details</summary>
Motivation: 跨筒仓联邦学习应用中需系统可靠，但参与者会因多种原因故障，此问题在跨筒仓联邦学习研究较少，所以要分析参与者故障影响。

Method: 对跨组织少参与者的跨筒仓联邦学习中参与者故障对模型质量的影响进行广泛研究，聚焦分析时机、数据等影响因素及对评估的影响。

Result: 高偏斜下评估乐观，隐藏真实影响；时机影响训练模型质量。

Conclusion: 研究结果为构建健壮联邦学习系统的研究者和软件架构师提供见解。

Abstract: Federated learning (FL) is a new paradigm for training machine learning (ML) models without sharing data. While applying FL in cross-silo scenarios, where organizations collaborate, it is necessary that the FL system is reliable; however, participants can fail due to various reasons (e.g., communication issues or misconfigurations). In order to provide a reliable system, it is necessary to analyze the impact of participant failures. While this problem received attention in cross-device FL where mobile devices with limited resources participate, there is comparatively little research in cross-silo FL.
  Therefore, we conduct an extensive study for analyzing the impact of participant failures on the model quality in the context of inter-organizational cross-silo FL with few participants. In our study, we focus on analyzing generally influential factors such as the impact of the timing and the data as well as the impact on the evaluation, which is important for deciding, if the model should be deployed. We show that under high skews the evaluation is optimistic and hides the real impact. Furthermore, we demonstrate that the timing impacts the quality of the trained model. Our results offer insights for researchers and software architects aiming to build robust FL systems.

</details>


### [57] [Hapax Locks : Value-Based Mutual Exclusion](https://arxiv.org/abs/2511.14608)
*Dave Dice,Alex Kogan*

Main category: cs.DC

TL;DR: 介绍了一种名为Hapax Locks的新型锁算法，性能好且易于集成。


<details>
  <summary>Details</summary>
Motivation: 设计一种简单、高效、对运行时环境依赖少且易于集成到现有系统的锁算法。

Method: 提出Hapax Locks锁算法，该算法具有常数时间的到达和解锁路径、FIFO准入顺序等特点。

Result: Hapax Locks性能与现有最佳锁相当，在常见竞争情况下产生较少的一致性流量。

Conclusion: Hapax Locks是一种性能良好且易于集成到现有系统的锁算法，且算法中无指针在线程间转移或所有权逃逸。

Abstract: We present Hapax Locks, a novel locking algorithm that is simple, enjoys constant-time arrival and unlock paths, provides FIFO admission order, and which is also space efficient and generates relatively little coherence traffic under contention in the common case. Hapax Locks offer performance (both latency and scalability) that is comparable with the best state of the art locks, while at the same time Hapax Locks impose fewer constraints and dependencies on the ambient runtime environment, making them particularly easy to integrate or retrofit into existing systems or under existing application programming interfaces Of particular note, no pointers shift or escape ownership between threads in our algorithm.

</details>


### [58] [Seer: Online Context Learning for Fast Synchronous LLM Reinforcement Learning](https://arxiv.org/abs/2511.14617)
*Ruoyu Qin,Weiran He,Weixiao Huang,Yangkun Zhang,Yikai Zhao,Bo Pang,Xinran Xu,Yingdi Shan,Yongwei Wu,Mingxing Zhang*

Main category: cs.DC

TL;DR: 现有同步强化学习系统有性能瓶颈，Seer系统利用请求相似性，采用三种技术减少长尾延迟、提高资源效率，评估显示其大幅提升吞吐量、降低长尾延迟。


<details>
  <summary>Details</summary>
Motivation: 现有同步强化学习系统在推出阶段存在严重性能瓶颈，包括长尾延迟大、资源利用率低等问题。

Method: 提出Seer系统，采用划分推出进行动态负载均衡、上下文感知调度和自适应分组推测解码三种技术。

Result: 在生产级强化学习工作负载评估中，Seer将端到端推出吞吐量提高74% - 97%，将长尾延迟降低75% - 93%。

Conclusion: Seer系统能显著加速强化学习训练迭代。

Abstract: Reinforcement Learning (RL) has become critical for advancing modern Large Language Models (LLMs), yet existing synchronous RL systems face severe performance bottlenecks. The rollout phase, which dominates end-to-end iteration time, suffers from substantial long-tail latency and poor resource utilization due to inherent workload imbalance. We present Seer, a novel online context learning system that addresses these challenges by exploiting previously overlooked similarities in output lengths and generation patterns among requests sharing the same prompt. Seer introduces three key techniques: divided rollout for dynamic load balancing, context-aware scheduling, and adaptive grouped speculative decoding. Together, these mechanisms substantially reduce long-tail latency and improve resource efficiency during rollout. Evaluations on production-grade RL workloads demonstrate that Seer improves end-to-end rollout throughput by 74% to 97% and reduces long-tail latency by 75% to 93% compared to state-of-the-art synchronous RL systems, significantly accelerating RL training iterations.

</details>


### [59] [Multi-GPU Quantum Circuit Simulation and the Impact of Network Performance](https://arxiv.org/abs/2511.14664)
*W. Michael Brown,Anurag Ramesh,Thomas Lubinski,Thien Nguyen,David E. Bernal Neira*

Main category: cs.DC

TL;DR: 本文介绍将MPI引入QED - C面向应用的基准测试，通过多种互连路径进行基准测试，表明互连性能提升对多GPU模拟性能影响更大。


<details>
  <summary>Details</summary>
Motivation: 量子算法的经典模拟资源需求高，多GPU模拟中GPU间通信会成为性能瓶颈，需要进行基准测试以评估性能。

Method: 将MPI引入QED - C面向应用的基准测试，回顾互连技术和多GPU通信API，使用多种互连路径进行基准测试。

Result: GPU架构改进使几代GPU加速超4.5倍，而互连性能提升使多GPU模拟求解时间性能提升超16倍。

Conclusion: 互连性能的提升对多GPU模拟性能的影响比GPU架构改进更大。

Abstract: As is intrinsic to the fundamental goal of quantum computing, classical simulation of quantum algorithms is notoriously demanding in resource requirements. Nonetheless, simulation is critical to the success of the field and a requirement for algorithm development and validation, as well as hardware design. GPU-acceleration has become standard practice for simulation, and due to the exponential scaling inherent in classical methods, multi-GPU simulation can be required to achieve representative system sizes. In this case, inter-GPU communications can bottleneck performance. In this work, we present the introduction of MPI into the QED-C Application-Oriented Benchmarks to facilitate benchmarking on HPC systems. We review the advances in interconnect technology and the APIs for multi-GPU communication. We benchmark using a variety of interconnect paths, including the recent NVIDIA Grace Blackwell NVL72 architecture that represents the first product to expand high-bandwidth GPU-specialized interconnects across multiple nodes. We show that while improvements to GPU architecture have led to speedups of over 4.5X across the last few generations of GPUs, advances in interconnect performance have had a larger impact with over 16X performance improvements in time to solution for multi-GPU simulations.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [60] [TaoSearchEmb: A Multi-Objective Reinforcement Learning Framework for Dense Retrieval in Taobao Search](https://arxiv.org/abs/2511.13885)
*Xingxian Liu,Dongshuai Li,Tao Wen,Jiahui Wan,Gui Ling,Fuyu Lv,Dan Ou,Haihong Tang*

Main category: cs.IR

TL;DR: 提出基于多目标强化学习的密集检索框架Retrieval - GRPO，可消除离线难负样本构建，缓解跷跷板效应，经实验验证有效并已部署。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLMs的密集检索训练范式依赖复杂离线难负样本构建管道，限制模型迭代效率和语义表示能力进化，且多任务学习框架有跷跷板效应。

Method: 训练时为每个查询动态检索Top - K候选产品，引入相关性LLM作为奖励模型生成实时反馈，通过强化学习动态优化嵌入表示，结合多种得分作为奖励信号。

Result: 消除对难负样本的依赖，缓解跷跷板效应，增强模型对复杂长尾查询的语义泛化能力，经大量离线和在线实验验证有效。

Conclusion: Retrieval - GRPO是有效的，已部署到中国最大的电商平台。

Abstract: Dense retrieval, as the core component of e-commerce search engines, maps user queries and items into a unified semantic space through pre-trained embedding models to enable large-scale real-time semantic retrieval. Despite the rapid advancement of LLMs gradually replacing traditional BERT architectures for embedding, their training paradigms still adhere to BERT-like supervised fine-tuning and hard negative mining strategies. This approach relies on complex offline hard negative sample construction pipelines, which constrain model iteration efficiency and hinder the evolutionary potential of semantic representation capabilities. Besides, existing multi-task learning frameworks face the seesaw effect when simultaneously optimizing semantic relevance and non-relevance objectives. In this paper, we propose Retrieval-GRPO, a multi-objective reinforcement learning-based dense retrieval framework designed to address these challenges. The method eliminates offline hard negative sample construction by dynamically retrieving Top-K candidate products for each query during training, while introducing a relevance LLM as a reward model to generate real-time feedback. Specifically, the retrieval model dynamically optimizes embedding representations through reinforcement learning, with reward signals combining LLM-generated relevance scores, product quality scores, and multi-way exclusivity metrics to achieve multi-objective user preference alignment and real-time error correction. This mechanism not only removes dependency on hard negatives but also mitigates the seesaw effect through collaborative multi-objective optimization, significantly enhancing the model's semantic generalization capability for complex long-tail queries. Extensive offline and online experiments validate the effectiveness of Retrieval-GRPO, which has been deployed on China's largest e-commerce platform.

</details>


### [61] [NeuroPath: Neurobiology-Inspired Path Tracking and Reflection for Semantically Coherent Retrieval](https://arxiv.org/abs/2511.14096)
*Junchen Li,Rongzheng Wang,Yihong Huang,Qizhi Chen,Jiasheng Zhang,Shuang Liang*

Main category: cs.IR

TL;DR: 提出NeuroPath框架解决RAG在多跳问答中的问题，在多数据集上表现优，还验证了鲁棒性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法在多跳问答中存在捕捉文档复杂依赖能力有限、语义连贯性差和引入无关噪声等问题。

Method: 提出NeuroPath框架，包含动态路径跟踪和检索后完成两个步骤。

Result: 在三个多跳问答数据集上超越现有基线，相比先进图基RAG方法，recall@2平均提高16.3%，recall@5平均提高13.5%；相比迭代式RAG方法，准确率更高且减少22.8%的令牌消耗。

Conclusion: NeuroPath在多跳问答任务中表现良好，在多个小模型上有鲁棒性，且具有跨不同复杂度任务的可扩展性。

Abstract: Retrieval-augmented generation (RAG) greatly enhances large language models (LLMs) performance in knowledge-intensive tasks. However, naive RAG methods struggle with multi-hop question answering due to their limited capacity to capture complex dependencies across documents. Recent studies employ graph-based RAG to capture document connections. However, these approaches often result in a loss of semantic coherence and introduce irrelevant noise during node matching and subgraph construction. To address these limitations, we propose NeuroPath, an LLM-driven semantic path tracking RAG framework inspired by the path navigational planning of place cells in neurobiology. It consists of two steps: Dynamic Path Tracking and Post-retrieval Completion. Dynamic Path Tracking performs goal-directed semantic path tracking and pruning over the constructed knowledge graph (KG), improving noise reduction and semantic coherence. Post-retrieval Completion further reinforces these benefits by conducting second-stage retrieval using intermediate reasoning and the original query to refine the query goal and complete missing information in the reasoning path. NeuroPath surpasses current state-of-the-art baselines on three multi-hop QA datasets, achieving average improvements of 16.3% on recall@2 and 13.5% on recall@5 over advanced graph-based RAG methods. Moreover, compared to existing iter-based RAG methods, NeuroPath achieves higher accuracy and reduces token consumption by 22.8%. Finally, we demonstrate the robustness of NeuroPath across four smaller LLMs (Llama3.1, GLM4, Mistral0.3, and Gemma3), and further validate its scalability across tasks of varying complexity. Code is available at https://github.com/KennyCaty/NeuroPath.

</details>


### [62] [WebRec: Enhancing LLM-based Recommendations with Attention-guided RAG from Web](https://arxiv.org/abs/2511.14182)
*Zihuai Zhao,Yujuan Ding,Wenqi Fan,Qing Li*

Main category: cs.IR

TL;DR: 提出WebRec框架解决现有基于RAG的推荐系统未充分利用网络信息的问题，并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有基于RAG的推荐系统未充分探索网络信息，且存在生成有效网络检索查询和处理网络噪声内容的挑战。

Method: 提出WebRec框架，利用大语言模型推理能力将推荐任务转化为适合网络检索的用户偏好查询，设计MP - Head通过消息传递增强大语言模型对相关信息远距离标记的注意力。

Result: 通过大量实验证明了所提出的基于网络的RAG方法在推荐场景中的有效性。

Conclusion: WebRec框架能有效解决现有基于RAG的推荐系统在利用网络信息时面临的问题。

Abstract: Recommender systems play a vital role in alleviating information overload and enriching users' online experience. In the era of large language models (LLMs), LLM-based recommender systems have emerged as a prevalent paradigm for advancing personalized recommendations. Recently, retrieval-augmented generation (RAG) has drawn growing interest to facilitate the recommendation capability of LLMs, incorporating useful information retrieved from external knowledge bases. However, as a rich source of up-to-date information, the web remains under-explored by existing RAG-based recommendations. In particular, unique challenges are posed from two perspectives: one is to generate effective queries for web retrieval, considering the inherent knowledge gap between web search and recommendations; another challenge lies in harnessing online websites that contain substantial noisy content. To tackle these limitations, we propose WebRec, a novel web-based RAG framework, which takes advantage of the reasoning capability of LLMs to interpret recommendation tasks into queries of user preferences that cater to web retrieval. Moreover, given noisy web-retrieved information, where relevant pieces of evidence are scattered far apart, an insightful MP-Head is designed to enhance LLM attentions between distant tokens of relevant information via message passing. Extensive experiments have been conducted to demonstrate the effectiveness of our proposed web-based RAG methods in recommendation scenarios.

</details>


### [63] [LLM-Aligned Geographic Item Tokenization for Local-Life Recommendation](https://arxiv.org/abs/2511.14221)
*Hao Jiang,Guoquan Wang,Donglin Zhou,Sheng Yu,Yang Zeng,Wencong Zeng,Kun Gai,Guorui Zhou*

Main category: cs.IR

TL;DR: 本文提出LGSID框架用于本地生活推荐，由基于强化学习的地理大语言模型对齐和分层地理项目分词两部分组成，实验表明其优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有文本推荐方法在本地生活服务等特定领域任务中，简单注入位置信息无法捕捉细粒度空间特征和物品间实际距离感知。

Method: 提出LGSID框架，包含基于强化学习的地理大语言模型对齐（训练列表式奖励模型，引入G - DPO算法）和分层地理项目分词策略。

Result: 在快手真实行业数据集上的广泛实验表明，LGSID始终优于最先进的判别式和生成式推荐模型。

Conclusion: LGSID框架在本地生活推荐任务中有效，消融研究、可视化和案例研究进一步验证其有效性。

Abstract: Recent advances in Large Language Models (LLMs) have enhanced text-based recommendation by enriching traditional ID-based methods with semantic generalization capabilities. Text-based methods typically encode item textual information via prompt design and generate discrete semantic IDs through item tokenization. However, in domain-specific tasks such as local-life services, simply injecting location information into prompts fails to capture fine-grained spatial characteristics and real-world distance awareness among items. To address this, we propose LGSID, an LLM-Aligned Geographic Item Tokenization Framework for Local-life Recommendation. This framework consists of two key components: (1) RL-based Geographic LLM Alignment, and (2) Hierarchical Geographic Item Tokenization. In the RL-based alignment module, we initially train a list-wise reward model to capture real-world spatial relationships among items. We then introduce a novel G-DPO algorithm that uses pre-trained reward model to inject generalized spatial knowledge and collaborative signals into LLMs while preserving their semantic understanding. Furthermore, we propose a hierarchical geographic item tokenization strategy, where primary tokens are derived from discrete spatial and content attributes, and residual tokens are refined using the aligned LLM's geographic representation vectors. Extensive experiments on real-world Kuaishou industry datasets show that LGSID consistently outperforms state-of-the-art discriminative and generative recommendation models. Ablation studies, visualizations, and case studies further validate its effectiveness.

</details>


### [64] [Infer As You Train: A Symmetric Paradigm of Masked Generative for Click-Through Rate Prediction](https://arxiv.org/abs/2511.14403)
*Moyu Zhang,Yujun Jin,Yun Chen,Jinxin Hu,Yu Zhang,Xiaoyi Zeng*

Main category: cs.IR

TL;DR: 提出对称掩码生成范式SGCTR用于CTR预测，解决现有模型训练和推理阶段不对称问题，实验验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型在CTR预测中训练和推理阶段不对称，未能充分发挥生成能力提升预测精度。

Method: 提出SGCTR框架，在训练中学习特征依赖获得生成能力，在推理中应用该能力迭代重新定义输入样本特征。

Result: 实验验证了SGCTR的优越性。

Conclusion: 在训练和推理阶段对称应用生成范式能显著释放其在CTR预测中的能力。

Abstract: Generative models are increasingly being explored in click-through rate (CTR) prediction field to overcome the limitations of the conventional discriminative paradigm, which rely on a simple binary classification objective. However, existing generative models typically confine the generative paradigm to the training phase, primarily for representation learning. During online inference, they revert to a standard discriminative paradigm, failing to leverage their powerful generative capabilities to further improve prediction accuracy. This fundamental asymmetry between the training and inference phases prevents the generative paradigm from realizing its full potential. To address this limitation, we propose the Symmetric Masked Generative Paradigm for CTR prediction (SGCTR), a novel framework that establishes symmetry between the training and inference phases. Specifically, after acquiring generative capabilities by learning feature dependencies during training, SGCTR applies the generative capabilities during online inference to iteratively redefine the features of input samples, which mitigates the impact of noisy features and enhances prediction accuracy. Extensive experiments validate the superiority of SGCTR, demonstrating that applying the generative paradigm symmetrically across both training and inference significantly unlocks its power in CTR prediction.

</details>


### [65] [Jasper-Token-Compression-600M Technical Report](https://arxiv.org/abs/2511.14405)
*Dun Zhang,Ziyang Zeng,Yudong Zhou,Shuyang Lu*

Main category: cs.IR

TL;DR: 报告介绍2025年11月发布的开源Jasper - Token - Compression - 600M模型训练方法和评估结果，结合知识蒸馏与令牌压缩技术提升性能和效率。


<details>
  <summary>Details</summary>
Motivation: 在双语（英、中）领域提升模型性能和效率，改进现有蒸馏方法。

Method: 基于英文Stella和Jasper模型的蒸馏方法扩展到双语领域，引入一维卷积令牌压缩模块，训练中动态调整压缩率，结合知识蒸馏与令牌压缩技术。

Result: 模型在嵌入质量和推理效率上显著提升，比传统0.6B模型更高效，性能与8B模型相当。

Conclusion: 结合知识蒸馏和令牌压缩技术能有效提升模型性能和效率。

Abstract: This technical report presents the training methodology and evaluation results of the open-source Jasper-Token-Compression-600M model, released in November 2025. Building on previous distillation-based recipes from the English Stella and Jasper models, we successfully extend this approach to a bilingual (English and Chinese) domain, further enhancing model performance through the incorporation of contrastive learning. A key innovation of our model is the introduction of a one-dimensional convolution-based token compression module. We dynamically adjust the compression rate during training, enabling the model to learn more robust and efficient compressed text representations. By combining knowledge distillation with token compression techniques, we achieve significant improvements in both embedding quality and inference efficiency. Our model performs with higher efficiency than a traditional 0.6B model while achieving performance comparable to that of an 8B model. For more information on the model release, visit: https://huggingface.co/infgrad/Jasper-Token-Compression-600M.

</details>


### [66] [Effective Diversification of Multi-Carousel Book Recommendation](https://arxiv.org/abs/2511.14461)
*Daniël Wilten,Gideon Maillette de Buy Wenniger,Arjen Hommersom,Paul Lucassen,Emiel Poortman*

Main category: cs.IR

TL;DR: 本文提出在协同过滤算法基础上增加图书推荐多样性的方法，用于公共图书馆网络目录，且引入指标评估，证明系统能平衡准确性与其他方面。


<details>
  <summary>Details</summary>
Motivation: 现有轮播式推荐虽便于导航，但无法增加推荐多样性，而多样性对吸引用户很重要，故要提升图书推荐的多样性。

Method: 在协同过滤算法基础上提出几种增加图书推荐多样性的方法，并引入评估指标。

Result: 所提出的系统能在准确性和其他方面找到合适的平衡。

Conclusion: 提出的方法可有效增加图书推荐的多样性，适用于公共图书馆网络目录。

Abstract: Using multiple carousels, lists that wrap around and can be scrolled, is the basis for offering content in most contemporary movie streaming platforms. Carousels allow for highlighting different aspects of users' taste, that fall in categories such as genres and authors. However, while carousels offer structure and greater ease of navigation, they alone do not increase diversity in recommendations, while this is essential to keep users engaged. In this work we propose several approaches to effectively increase item diversity within the domain of book recommendations, on top of a collaborative filtering algorithm. These approaches are intended to improve book recommendations in the web catalogs of public libraries. Furthermore, we introduce metrics to evaluate the resulting strategies, and show that the proposed system finds a suitable balance between accuracy and beyond-accuracy aspects.

</details>


### [67] [NeuCLIRBench: A Modern Evaluation Collection for Monolingual, Cross-Language, and Multilingual Information Retrieval](https://arxiv.org/abs/2511.14758)
*Dawn Lawrie,James Mayfield,Eugene Yang,Andrew Yates,Sean MacAvaney,Ronak Pradeep,Scott Miller,Paul McNamee,Luca Soldani*

Main category: cs.IR

TL;DR: 本文介绍了评估集NeuCLIRBench，它支持多语言检索场景，结合多届TREC主题，有大量判断数据，还包含融合基线，且公开可用。


<details>
  <summary>Details</summary>
Motivation: 为衡量检索进展，需要能有效区分系统的带相关性判断的测试集。

Method: 构建NeuCLIRBench评估集，结合2022 - 2024年TREC NeuCLIR赛道主题，提供多语言检索场景及大量判断数据，还包含融合基线。

Result: 该评估集有250,128条判断，能为区分检索方法提供强大的统计鉴别力。

Conclusion: NeuCLIRBench公开可用，有助于检索系统评估。

Abstract: To measure advances in retrieval, test collections with relevance judgments that can faithfully distinguish systems are required. This paper presents NeuCLIRBench, an evaluation collection for cross-language and multilingual retrieval. The collection consists of documents written natively in Chinese, Persian, and Russian, as well as those same documents machine translated into English. The collection supports several retrieval scenarios including: monolingual retrieval in English, Chinese, Persian, or Russian; cross-language retrieval with English as the query language and one of the other three languages as the document language; and multilingual retrieval, again with English as the query language and relevant documents in all three languages. NeuCLIRBench combines the TREC NeuCLIR track topics of 2022, 2023, and 2024. The 250,128 judgments across approximately 150 queries for the monolingual and cross-language tasks and 100 queries for multilingual retrieval provide strong statistical discriminatory power to distinguish retrieval approaches. A fusion baseline of strong neural retrieval systems is included with the collection so that developers of reranking algorithms are no longer reliant on BM25 as their first-stage retriever. NeuCLIRBench is publicly available.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [68] [Extended Physics Informed Neural Network for Hyperbolic Two-Phase Flow in Porous Media](https://arxiv.org/abs/2511.13734)
*Saif Ur Rehman,Wajid Yousuf*

Main category: cs.LG

TL;DR: 研究采用XPINN框架解决含非凸通量函数的非线性Buckley - Leverett方程，能准确捕捉不连续现象，比标准PINNs有更好性能。


<details>
  <summary>Details</summary>
Motivation: 传统离散求解器处理非线性双曲偏微分方程计算量大，标准PINNs难以捕捉陡峭梯度、不连续性和复杂非线性波相互作用，需改进方法。

Method: 使用XPINN框架，将计算域在时空上动态分解为预激波和激波后区域，通过Rankine - Hugoniot跳跃条件实现子网络耦合。

Result: 提出的XPINN方法能准确捕捉不连续饱和度前沿和复合波相互作用，与标准PINNs相比，稳定性更好、收敛更快、分辨率更高。

Conclusion: XPINN框架是解决多相流问题中具有挑战性的双曲偏微分方程的有效且可扩展的工具。

Abstract: The accurate solution of nonlinear hyperbolic partial differential equations (PDEs) remains a central challenge in computational science due to the presence of steep gradients, discontinuities, and multiscale structures that make conventional discretization-based solvers computationally demanding. Physics-Informed Neural Networks (PINNs) embed the governing equations into the learning process, enabling mesh-free solution of PDEs, yet they often struggle to capture steep gradients, discontinuities, and complex nonlinear wave interactions. To address these limitations, this study employs the Extended Physics-Informed Neural Network (XPINN) framework to solve the nonlinear Buckley-Leverett equation with a nonconvex flux function, which models immiscible two-phase flow in porous media. The computational domain is dynamically decomposed in space and time into evolving pre-shock and post-shock regions, allowing localized subnetworks to efficiently learn distinct flow behaviors. Coupling between subnetworks is achieved through the Rankine-Hugoniot jump condition, which enforces physically consistent flux continuity across the moving shock interface. Numerical experiments demonstrate that the proposed XPINN approach accurately captures discontinuous saturation fronts and compound wave interactions without requiring artificial diffusion or entropy corrections. Compared to standard PINNs, the XPINN framework achieves superior stability, faster convergence, and enhanced resolution of nonlinear wave dynamics using smaller, domain-specific models with fewer trainable parameters, establishing it as an effective and scalable tool for solving challenging hyperbolic PDEs in multiphase flow problems. The code of this work is available on github.com/saifkhanengr/XPINN-for-Buckley-Leverett.

</details>


### [69] [Fair-GNE : Generalized Nash Equilibrium-Seeking Fairness in Multiagent Healthcare Automation](https://arxiv.org/abs/2511.14135)
*Promise Ekpo,Saesha Agarwal,Felix Grimm,Lekan Molu,Angelique Taylor*

Main category: cs.LG

TL;DR: 本文提出Fair - GNE模型解决多智能体强化学习中公平工作量分配问题，在模拟实验中提升了工作量平衡和公平性。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体强化学习方法在医疗工作者需求侧场景中无法实现可认证的自我执行公平性，需要解决这一缺陷。

Method: 将多智能体强化学习建模为受限广义纳什均衡寻求（GNE）游戏，即Fair - GNE模型，引导群体策略达到安全且局部有效的均衡。

Result: 在定制的高保真复苏模拟器实验中，Fair - GNE相比固定惩罚基线显著改善了工作量平衡（0.89 vs. 0.33 JFI，p < 0.01），保持86%的任务成功率。

Conclusion: 研究清晰展示了在大型基于多智能体学习的医疗系统中的公式、评估指标和均衡寻求创新，实现了有原则的公平性执行。

Abstract: Enforcing a fair workload allocation among multiple agents tasked to achieve an objective in learning enabled demand side healthcare worker settings is crucial for consistent and reliable performance at runtime. Existing multi-agent reinforcement learning (MARL) approaches steer fairness by shaping reward through post hoc orchestrations, leaving no certifiable self-enforceable fairness that is immutable by individual agents at runtime. Contextualized within a setting where each agent shares resources with others, we address this shortcoming with a learning enabled optimization scheme among self-interested decision makers whose individual actions affect those of other agents. This extends the problem to a generalized Nash equilibrium (GNE) game-theoretic framework where we steer group policy to a safe and locally efficient equilibrium, so that no agent can improve its utility function by unilaterally changing its decisions. Fair-GNE models MARL as a constrained generalized Nash equilibrium-seeking (GNE) game, prescribing an ideal equitable collective equilibrium within the problem's natural fabric. Our hypothesis is rigorously evaluated in our custom-designed high-fidelity resuscitation simulator. Across all our numerical experiments, Fair-GNE achieves significant improvement in workload balance over fixed-penalty baselines (0.89 vs.\ 0.33 JFI, $p < 0.01$) while maintaining 86\% task success, demonstrating statistically significant fairness gains through adaptive constraint enforcement. Our results communicate our formulations, evaluation metrics, and equilibrium-seeking innovations in large multi-agent learning-based healthcare systems with clarity and principled fairness enforcement.

</details>


### [70] [Library Liberation: Competitive Performance Matmul Through Compiler-composed Nanokernels](https://arxiv.org/abs/2511.13764)
*Arun Thangamani,Md Asghar Ahmad Shahid,Adam Siemieniuk,Rolf Morel,Renato Golin,Alexander Heinecke*

Main category: cs.LG

TL;DR: 本文提出一种编译方案，利用MLIR方言自动生成可扩展、高性能微内核，实验表明生成的纳米内核具有生产质量，可与最先进的微内核库竞争。


<details>
  <summary>Details</summary>
Motivation: AI和机器学习工作负载发展导致高级域操作与高效硬件利用之间差距扩大，现有实现高性能的方法复杂且限制可扩展性。

Method: 引入编译方案，利用MLIR方言桥接域级操作和处理器能力，从低级IR构造组合纳米内核形成微内核，在基于MLIR的编译器中实现该技术。

Result: 生成的纳米内核具有生产质量，可与最先进的微内核库竞争。

Conclusion: 该编译方案能有效自动生成可扩展、高性能的微内核，减少对低级库的依赖。

Abstract: The rapidly evolving landscape of AI and machine learning workloads has widened the gap between high-level domain operations and efficient hardware utilization. Achieving near-peak performance still demands deep hardware expertise-experts either handcraft target-specific kernels (e.g., DeepSeek) or rely on specialized libraries (e.g., CUTLASS)-both of which add complexity and limit scalability for most ML practitioners.
  This paper introduces a compilation scheme that automatically generates scalable, high-performance microkernels by leveraging the MLIR dialects to bridge domain-level operations and processor capabilities. Our approach removes dependence on low-level libraries by enabling the compiler to auto-generate near-optimal code directly. At its core is a mechanism for composing nanokernels from low-level IR constructs with near-optimal register utilization, forming efficient microkernels tailored to each target. We implement this technique in an MLIR-based compiler supporting both vector and tile based CPU instructions. Experiments show that the generated nanokernels are of production-quality, and competitive with state-of-the-art microkernel libraries.

</details>


### [71] [Blurred Encoding for Trajectory Representation Learning](https://arxiv.org/abs/2511.13741)
*Silin Zhou,Yao Chen,Shuo Shang,Lisi Chen,Bingsheng He,Ryosuke Shibasaki*

Main category: cs.LG

TL;DR: 提出BLUE方法用于轨迹表示学习，解决现有方法丢失细粒度时空细节问题，实验显示其优于多个基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有SOTA TRL方法将原始GPS轨迹转换为网格或道路轨迹时会丢失细粒度时空细节。

Method: 提出BLUE方法，通过逐渐降低GPS坐标精度创建多层次的分层补丁，采用具有金字塔结构的编解码器模型，在各补丁级别使用Transformer学习轨迹嵌入，使用MSE损失进行轨迹重建任务训练。

Result: 与8个SOTA TRL方法在3个下游任务上对比，BLUE始终比所有基线方法有更高的准确率，平均比最佳基线方法高30.90%。

Conclusion: BLUE方法有效解决了现有方法的问题，在轨迹表示学习任务中表现优异。

Abstract: Trajectory representation learning (TRL) maps trajectories to vector embeddings and facilitates tasks such as trajectory classification and similarity search. State-of-the-art (SOTA) TRL methods transform raw GPS trajectories to grid or road trajectories to capture high-level travel semantics, i.e., regions and roads. However, they lose fine-grained spatial-temporal details as multiple GPS points are grouped into a single grid cell or road segment. To tackle this problem, we propose the BLUrred Encoding method, dubbed BLUE, which gradually reduces the precision of GPS coordinates to create hierarchical patches with multiple levels. The low-level patches are small and preserve fine-grained spatial-temporal details, while the high-level patches are large and capture overall travel patterns. To complement different patch levels with each other, our BLUE is an encoder-decoder model with a pyramid structure. At each patch level, a Transformer is used to learn the trajectory embedding at the current level, while pooling prepares inputs for the higher level in the encoder, and up-resolution provides guidance for the lower level in the decoder. BLUE is trained using the trajectory reconstruction task with the MSE loss. We compare BLUE with 8 SOTA TRL methods for 3 downstream tasks, the results show that BLUE consistently achieves higher accuracy than all baselines, outperforming the best-performing baselines by an average of 30.90%. Our code is available at https://github.com/slzhou-xy/BLUE.

</details>


### [72] [Look-Ahead Reasoning on Learning Platforms](https://arxiv.org/abs/2511.14745)
*Haiqing Zhu,Tijana Zrnic,Celestine Mendler-Dünner*

Main category: cs.LG

TL;DR: 本文研究学习平台上用户的策略行为，形式化了k级思维，对比集体与自私行为，探讨与相关数学框架的联系。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注用户对已部署模型的策略响应，未考虑其他用户行为，而前瞻推理可考虑用户行为耦合及对未来预测的影响。

Method: 形式化行为经济学中的k级思维，对比集体推理和自私行为。

Result: k级思维虽加速均衡收敛，但长期对个体无更高层次推理的益处；对比集体与自私行为，得出协调的利弊及新的效用对齐概念。

Conclusion: 探讨了与战略分类、执行性预测和算法集体行动等数学框架的联系。

Abstract: On many learning platforms, the optimization criteria guiding model training reflect the priorities of the designer rather than those of the individuals they affect. Consequently, users may act strategically to obtain more favorable outcomes, effectively contesting the platform's predictions. While past work has studied strategic user behavior on learning platforms, the focus has largely been on strategic responses to a deployed model, without considering the behavior of other users. In contrast, look-ahead reasoning takes into account that user actions are coupled, and -- at scale -- impact future predictions. Within this framework, we first formalize level-$k$ thinking, a concept from behavioral economics, where users aim to outsmart their peers by looking one step ahead. We show that, while convergence to an equilibrium is accelerated, the equilibrium remains the same, providing no benefit of higher-level reasoning for individuals in the long run. Then, we focus on collective reasoning, where users take coordinated actions by optimizing through their joint impact on the model. By contrasting collective with selfish behavior, we characterize the benefits and limits of coordination; a new notion of alignment between the learner's and the users' utilities emerges as a key concept. We discuss connections to several related mathematical frameworks, including strategic classification, performative prediction, and algorithmic collective action.

</details>


### [73] [DeepDefense: Layer-Wise Gradient-Feature Alignment for Building Robust Neural Networks](https://arxiv.org/abs/2511.13749)
*Ci Lin,Tet Yeap,Iluju Kiringa,Biwei Zhang*

Main category: cs.LG

TL;DR: 提出DeepDefense防御框架，通过多层GFA正则化抑制对抗脆弱性，理论分析并实证表明该方法提升模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络易受对抗扰动影响，需提升其对抗鲁棒性。

Method: 提出DeepDefense框架，应用多层GFA正则化，使输入梯度与内部特征表示对齐。

Result: 在梯度和优化攻击下显著提升鲁棒性，如CIFAR - 10上CNN模型表现优于标准对抗训练，对抗优化攻击需更高扰动幅度导致误分类。

Conclusion: 该方法与架构无关、易实现且有效，为提升深度学习模型对抗鲁棒性提供方向。

Abstract: Deep neural networks are known to be vulnerable to adversarial perturbations, which are small and carefully crafted inputs that lead to incorrect predictions. In this paper, we propose DeepDefense, a novel defense framework that applies Gradient-Feature Alignment (GFA) regularization across multiple layers to suppress adversarial vulnerability. By aligning input gradients with internal feature representations, DeepDefense promotes a smoother loss landscape in tangential directions, thereby reducing the model's sensitivity to adversarial noise.
  We provide theoretical insights into how adversarial perturbation can be decomposed into radial and tangential components and demonstrate that alignment suppresses loss variation in tangential directions, where most attacks are effective. Empirically, our method achieves significant improvements in robustness across both gradient-based and optimization-based attacks. For example, on CIFAR-10, CNN models trained with DeepDefense outperform standard adversarial training by up to 15.2% under APGD attacks and 24.7% under FGSM attacks. Against optimization-based attacks such as DeepFool and EADEN, DeepDefense requires 20 to 30 times higher perturbation magnitudes to cause misclassification, indicating stronger decision boundaries and a flatter loss landscape. Our approach is architecture-agnostic, simple to implement, and highly effective, offering a promising direction for improving the adversarial robustness of deep learning models.

</details>


### [74] [SCALEX: Scalable Concept and Latent Exploration for Diffusion Models](https://arxiv.org/abs/2511.13750)
*E. Zhixuan Zeng,Yuhao Chen,Alexander Wong*

Main category: cs.LG

TL;DR: 提出SCALEX框架用于扩散模型潜在空间的可扩展和自动化探索，使扩散模型偏差分析更优。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型偏差分析方法有局限性，如聚焦预定义类别、依赖手动解释潜在方向，限制了可扩展性和新模式发现。

Method: 引入SCALEX框架，仅用自然语言提示从H - space提取语义有意义的方向，实现零样本解释。

Result: SCALEX能检测职业提示中的性别偏差、对身份描述符的语义对齐进行排序、无监督揭示聚类概念结构。

Conclusion: 通过直接将提示与潜在方向联系起来，SCALEX比先前方法让扩散模型偏差分析更具可扩展性、可解释性和可扩展性。

Abstract: Image generation models frequently encode social biases, including stereotypes tied to gender, race, and profession. Existing methods for analyzing these biases in diffusion models either focus narrowly on predefined categories or depend on manual interpretation of latent directions. These constraints limit scalability and hinder the discovery of subtle or unanticipated patterns.
  We introduce SCALEX, a framework for scalable and automated exploration of diffusion model latent spaces. SCALEX extracts semantically meaningful directions from H-space using only natural language prompts, enabling zero-shot interpretation without retraining or labelling. This allows systematic comparison across arbitrary concepts and large-scale discovery of internal model associations. We show that SCALEX detects gender bias in profession prompts, ranks semantic alignment across identity descriptors, and reveals clustered conceptual structure without supervision. By linking prompts to latent directions directly, SCALEX makes bias analysis in diffusion models more scalable, interpretable, and extensible than prior approaches.

</details>


### [75] [Motor Imagery Classification Using Feature Fusion of Spatially Weighted Electroencephalography](https://arxiv.org/abs/2511.13752)
*Abdullah Al Shiam,Md. Khademul Islam Molla,Abu Saleh Musa Miah,Md. Abdus Samad Kamal*

Main category: cs.LG

TL;DR: 本文提出基于脑区特定通道选择和多域特征融合的方法提升脑机接口运动想象任务分类准确率，在公开数据集验证效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 脑电信号多通道特性使脑机接口系统需明确信息处理以降低计算复杂度，提升分类准确性。

Method: 提出基于脑区特定通道选择和多域特征融合的方法，按脑区功能相关性分组脑电通道，对每组通道应用CSP、模糊C均值聚类和TSM三种特征提取方法，用SVM分类。

Result: 在BCI竞赛III和IV的公开基准脑电数据集IVA和I上验证，分类准确率分别达90.77%和84.50%。

Conclusion: 该方法优于现有方法，有效提升运动想象任务分类准确率。

Abstract: A Brain Computer Interface (BCI) connects the human brain to the outside world, providing a direct communication channel. Electroencephalography (EEG) signals are commonly used in BCIs to reflect cognitive patterns related to motor function activities. However, due to the multichannel nature of EEG signals, explicit information processing is crucial to lessen computational complexity in BCI systems. This study proposes an innovative method based on brain region-specific channel selection and multi-domain feature fusion to improve classification accuracy. The novelty of the proposed approach lies in region-based channel selection, where EEG channels are grouped according to their functional relevance to distinct brain regions. By selecting channels based on specific regions involved in motor imagery (MI) tasks, this technique eliminates irrelevant channels, reducing data dimensionality and improving computational efficiency. This also ensures that the extracted features are more reflective of the brain actual activity related to motor tasks. Three distinct feature extraction methods Common Spatial Pattern (CSP), Fuzzy C-means clustering, and Tangent Space Mapping (TSM), are applied to each group of channels based on their brain region. Each method targets different characteristics of the EEG signal: CSP focuses on spatial patterns, Fuzzy C means identifies clusters within the data, and TSM captures non-linear patterns in the signal. The combined feature vector is used to classify motor imagery tasks (left hand, right hand, and right foot) using Support Vector Machine (SVM). The proposed method was validated on publicly available benchmark EEG datasets (IVA and I) from the BCI competition III and IV. The results show that the approach outperforms existing methods, achieving classification accuracies of 90.77% and 84.50% for datasets IVA and I, respectively.

</details>


### [76] [Robustness of LLM-enabled vehicle trajectory prediction under data security threats](https://arxiv.org/abs/2511.13753)
*Feilong Wang,Fuqiang Liu*

Main category: cs.LG

TL;DR: 本文对基于大语言模型（LLM）的车辆轨迹预测模型进行系统漏洞分析，揭示其易受对抗性操纵，强调未来需进行面向鲁棒性的设计。


<details>
  <summary>Details</summary>
Motivation: 现有研究未探索基于LLM的安全关键驾驶系统预测模型的鲁棒性，本文旨在填补这一空白。

Method: 提出一种单特征差分进化攻击，在黑盒设置下对LLM输入提示中周围车辆的单个运动学特征进行扰动。

Result: 在highD数据集上的实验表明，即使是微小的、物理上合理的扰动也会显著破坏模型输出，发现了准确性和鲁棒性之间的权衡，分析了故障机制并探索了潜在缓解方案。

Conclusion: 研究首次揭示了基于LLM的自动驾驶模型在车辆交互中的对抗性漏洞，强调未来基于LLM的智能交通系统需要进行面向鲁棒性的设计。

Abstract: The integration of large language models (LLMs) into automated driving systems has opened new possibilities for reasoning and decision-making by transforming complex driving contexts into language-understandable representations. Recent studies demonstrate that fine-tuned LLMs can accurately predict vehicle trajectories and lane-change intentions by gathering and transforming data from surrounding vehicles. However, the robustness of such LLM-based prediction models for safety-critical driving systems remains unexplored, despite the increasing concerns about the trustworthiness of LLMs. This study addresses this gap by conducting a systematic vulnerability analysis of LLM-enabled vehicle trajectory prediction. We propose a one-feature differential evolution attack that perturbs a single kinematic feature of surrounding vehicles within the LLM's input prompts under a black-box setting. Experiments on the highD dataset reveal that even minor, physically plausible perturbations can significantly disrupt model outputs, underscoring the susceptibility of LLM-based predictors to adversarial manipulation. Further analyses reveal a trade-off between accuracy and robustness, examine the failure mechanism, and explore potential mitigation solutions. The findings provide the very first insights into adversarial vulnerabilities of LLM-driven automated vehicle models in the context of vehicular interactions and highlight the need for robustness-oriented design in future LLM-based intelligent transportation systems.

</details>


### [77] [Adaptive Redundancy Regulation for Balanced Multimodal Information Refinement](https://arxiv.org/abs/2511.13755)
*Zhe Yang,Wenrui Li,Hongtao Chen,Penghong Wang,Ruiqin Xiong,Xiaopeng Fan*

Main category: cs.LG

TL;DR: 提出基于信息瓶颈原理的Adaptive Redundancy Regulation for Balanced Multimodal Information Refinement (RedReg)方法解决多模态学习中模态优化不平衡问题，实验显示方法有优势。


<details>
  <summary>Details</summary>
Motivation: 现有多模态学习方法存在主导模态长期主导导致冗余信息积累、直接统一调整梯度忽略模态语义和方向性的问题，需改进。

Method: 构建冗余阶段监控器，设计共信息门控机制，将主导模态梯度投影到联合多模态梯度子空间的正交补并根据冗余抑制梯度。

Result: 在多数场景中优于当前主要方法，消融实验验证了方法有效性。

Conclusion: 提出的RedReg方法能有效解决多模态学习中的模态优化不平衡问题。

Abstract: Multimodal learning aims to improve performance by leveraging data from multiple sources. During joint multimodal training, due to modality bias, the advantaged modality often dominates backpropagation, leading to imbalanced optimization. Existing methods still face two problems: First, the long-term dominance of the dominant modality weakens representation-output coupling in the late stages of training, resulting in the accumulation of redundant information. Second, previous methods often directly and uniformly adjust the gradients of the advantaged modality, ignoring the semantics and directionality between modalities. To address these limitations, we propose Adaptive Redundancy Regulation for Balanced Multimodal Information Refinement (RedReg), which is inspired by information bottleneck principle. Specifically, we construct a redundancy phase monitor that uses a joint criterion of effective gain growth rate and redundancy to trigger intervention only when redundancy is high. Furthermore, we design a co-information gating mechanism to estimate the contribution of the current dominant modality based on cross-modal semantics. When the task primarily relies on a single modality, the suppression term is automatically disabled to preserve modality-specific information. Finally, we project the gradient of the dominant modality onto the orthogonal complement of the joint multimodal gradient subspace and suppress the gradient according to redundancy. Experiments show that our method demonstrates superiority among current major methods in most scenarios. Ablation experiments verify the effectiveness of our method. The code is available at https://github.com/xia-zhe/RedReg.git

</details>


### [78] [Multi-Horizon Time Series Forecasting of non-parametric CDFs with Deep Lattice Networks](https://arxiv.org/abs/2511.13756)
*Niklas Erdmann,Lars Bentsen,Roy Stenbro,Heine Nygard Riise,Narada Dilp Warakagoda,Paal E. Engelstad*

Main category: cs.LG

TL;DR: 本文连接概率预测和单调网络领域，提出改进的深度格网络（DLN）方法进行时间序列概率预测，实验表明该方法表现良好。


<details>
  <summary>Details</summary>
Motivation: 传统概率预测中CDF建模多为参数方法，且分位数回归存在交叉问题，需推进概率预测和单调网络领域发展。

Method: 提出对DLN的改进，利用LSTM作为嵌入层，扩展输出大小，防止分位数交叉，实现隐式CDF的多步预测。

Result: 在太阳能辐照度时间序列预测中，改进的DLN表现与无约束方法相当甚至更好，且优于可扩展的单调神经网络。

Conclusion: 改进的DLN方法能激发对单调神经网络和概率预测技术的更多研究兴趣。

Abstract: Probabilistic forecasting is not only a way to add more information to a prediction of the future, but it also builds on weaknesses in point prediction. Sudden changes in a time series can still be captured by a cumulative distribution function (CDF), while a point prediction is likely to miss it entirely. The modeling of CDFs within forecasts has historically been limited to parametric approaches, but due to recent advances, this no longer has to be the case. We aim to advance the fields of probabilistic forecasting and monotonic networks by connecting them and propose an approach that permits the forecasting of implicit, complete, and nonparametric CDFs. For this purpose, we propose an adaptation to deep lattice networks (DLN) for monotonically constrained simultaneous/implicit quantile regression in time series forecasting. Quantile regression usually produces quantile crossovers, which need to be prevented to achieve a legitimate CDF. By leveraging long short term memory units (LSTM) as the embedding layer, and spreading quantile inputs to all sub-lattices of a DLN with an extended output size, we can produce a multi-horizon forecast of an implicit CDF due to the monotonic constraintability of DLNs that prevent quantile crossovers. We compare and evaluate our approach's performance to relevant state of the art within the context of a highly relevant application of time series forecasting: Day-ahead, hourly forecasts of solar irradiance observations. Our experiments show that the adaptation of a DLN performs just as well or even better than an unconstrained approach. Further comparison of the adapted DLN against a scalable monotonic neural network shows that our approach performs better. With this adaptation of DLNs, we intend to create more interest and crossover investigations in techniques of monotonic neural networks and probabilistic forecasting.

</details>


### [79] [SparseST: Exploiting Data Sparsity in Spatiotemporal Modeling and Prediction](https://arxiv.org/abs/2511.14753)
*Junfeng Wu,Hadjer Benmeziane,Kaoutar El Maghraoui,Liu Liu,Yinan Wang*

Main category: cs.LG

TL;DR: 提出SparseST框架利用数据稀疏性开发高效时空模型，设计多目标复合损失函数探索性能与效率的帕累托前沿。


<details>
  <summary>Details</summary>
Motivation: 现有ConvLSTM及其变体计算成本高，不适用于边缘设备，且现有高效AI方法在时空数据挖掘中因模型冗余有限效果不佳，而数据和特征冗余被忽视。

Method: 开发SparseST框架利用数据稀疏性，设计多目标复合损失函数探索性能与效率的帕累托前沿。

Result: 提出了能利用数据稀疏性的高效时空模型及相关损失函数。

Conclusion: SparseST框架和多目标复合损失函数可在减少计算成本的同时保持模型性能，为实际应用提供了根据资源和任务要求调整模型的指导。

Abstract: Spatiotemporal data mining (STDM) has a wide range of applications in various complex physical systems (CPS), i.e., transportation, manufacturing, healthcare, etc. Among all the proposed methods, the Convolutional Long Short-Term Memory (ConvLSTM) has proved to be generalizable and extendable in different applications and has multiple variants achieving state-of-the-art performance in various STDM applications. However, ConvLSTM and its variants are computationally expensive, which makes them inapplicable in edge devices with limited computational resources. With the emerging need for edge computing in CPS, efficient AI is essential to reduce the computational cost while preserving the model performance. Common methods of efficient AI are developed to reduce redundancy in model capacity (i.e., model pruning, compression, etc.). However, spatiotemporal data mining naturally requires extensive model capacity, as the embedded dependencies in spatiotemporal data are complex and hard to capture, which limits the model redundancy. Instead, there is a fairly high level of data and feature redundancy that introduces an unnecessary computational burden, which has been largely overlooked in existing research. Therefore, we developed a novel framework SparseST, that pioneered in exploiting data sparsity to develop an efficient spatiotemporal model. In addition, we explore and approximate the Pareto front between model performance and computational efficiency by designing a multi-objective composite loss function, which provides a practical guide for practitioners to adjust the model according to computational resource constraints and the performance requirements of downstream tasks.

</details>


### [80] [VitalBench: A Rigorous Multi-Center Benchmark for Long-Term Vital Sign Prediction in Intraoperative Care](https://arxiv.org/abs/2511.13757)
*Xiuding Cai,Xueyao Wang,Sen Wang,Yaoyao Zhu,Jiao Chen,Yu Yao*

Main category: cs.LG

TL;DR: 提出VitalBench基准用于术中生命体征预测，含多中心数据和多评估轨道，代码和数据开源。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型用于医疗时间序列预测存在缺乏标准化基准、数据不完整和跨中心验证有限等问题，需改进术中生命体征监测和预测。

Method: 引入VitalBench基准，包含来自两个独立医疗中心超4000例手术数据，设三种评估轨道，采用掩码损失技术。

Result: 提供标准化统一平台用于模型开发和比较，利于研究者聚焦架构创新，保证数据处理一致性。

Conclusion: 为术中生命体征预测模型发展奠定基础，确保模型准确、稳健且适应不同临床环境。

Abstract: Intraoperative monitoring and prediction of vital signs are critical for ensuring patient safety and improving surgical outcomes. Despite recent advances in deep learning models for medical time-series forecasting, several challenges persist, including the lack of standardized benchmarks, incomplete data, and limited cross-center validation. To address these challenges, we introduce VitalBench, a novel benchmark specifically designed for intraoperative vital sign prediction. VitalBench includes data from over 4,000 surgeries across two independent medical centers, offering three evaluation tracks: complete data, incomplete data, and cross-center generalization. This framework reflects the real-world complexities of clinical practice, minimizing reliance on extensive preprocessing and incorporating masked loss techniques for robust and unbiased model evaluation. By providing a standardized and unified platform for model development and comparison, VitalBench enables researchers to focus on architectural innovation while ensuring consistency in data handling. This work lays the foundation for advancing predictive models for intraoperative vital sign forecasting, ensuring that these models are not only accurate but also robust and adaptable across diverse clinical environments. Our code and data are available at https://github.com/XiudingCai/VitalBench.

</details>


### [81] [ChemFixer: Correcting Invalid Molecules to Unlock Previously Unseen Chemical Space](https://arxiv.org/abs/2511.13758)
*Jun-Hyoung Park,Ho-Jun Song,Seong-Whan Lee*

Main category: cs.LG

TL;DR: 提出ChemFixer框架校正无效分子，经评估可提升分子有效性、保留特性、扩展候选药物多样性，还能应用于DTI预测任务，是深度学习药物发现实用工具。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习分子生成模型常产生化学无效分子，限制学习的化学空间可用范围，给实际应用带来挑战。

Method: 构建基于Transformer架构的ChemFixer框架，用掩码技术预训练，在自建的大规模有效/无效分子对数据集上微调。

Result: 在多种生成模型评估中提升分子有效性，保留化学和生物学分布特性，扩展候选药物多样性；有效应用于DTI预测任务，提升生成配体有效性，发现有前景的配体 - 蛋白对。

Conclusion: ChemFixer是深度学习药物发现各阶段实用工具，能增强分子有效性、扩展可访问化学空间。

Abstract: Deep learning-based molecular generation models have shown great potential in efficiently exploring vast chemical spaces by generating potential drug candidates with desired properties. However, these models often produce chemically invalid molecules, which limits the usable scope of the learned chemical space and poses significant challenges for practical applications. To address this issue, we propose ChemFixer, a framework designed to correct invalid molecules into valid ones. ChemFixer is built on a transformer architecture, pre-trained using masking techniques, and fine-tuned on a large-scale dataset of valid/invalid molecular pairs that we constructed. Through comprehensive evaluations across diverse generative models, ChemFixer improved molecular validity while effectively preserving the chemical and biological distributional properties of the original outputs. This indicates that ChemFixer can recover molecules that could not be previously generated, thereby expanding the diversity of potential drug candidates. Furthermore, ChemFixer was effectively applied to a drug-target interaction (DTI) prediction task using limited data, improving the validity of generated ligands and discovering promising ligand-protein pairs. These results suggest that ChemFixer is not only effective in data-limited scenarios, but also extensible to a wide range of downstream tasks. Taken together, ChemFixer shows promise as a practical tool for various stages of deep learning-based drug discovery, enhancing molecular validity and expanding accessible chemical space.

</details>


### [82] [On the Gradient Complexity of Private Optimization with Private Oracles](https://arxiv.org/abs/2511.13999)
*Michael Menart,Aleksandar Nikolov*

Main category: cs.LG

TL;DR: 研究差分隐私经验/总体风险最小化的运行时间，给出非光滑和光滑损失下的下界，显示差分隐私优化器有维度相关的运行时间惩罚，还给出信息受限预言机的下界。


<details>
  <summary>Details</summary>
Motivation: 研究差分隐私经验/总体风险最小化在一阶预言机查询方面的运行时间。

Method: 分别考虑非光滑和光滑损失情况，结合DP - SGD等算法，通过分析不同条件下的优化器与预言机交互。

Result: 给出不同情况下的运行时间下界，如非光滑损失时 $Ω(\min\{\frac{\sqrt{d}}{α^2}, \frac{d}{\log(1/α)}\})$ 等，且部分结果被证明是紧的。

Conclusion: 差分隐私优化器有维度相关的运行时间惩罚，梯度量化技术在优化中有基本限制。

Abstract: We study the running time, in terms of first order oracle queries, of differentially private empirical/population risk minimization of Lipschitz convex losses. We first consider the setting where the loss is non-smooth and the optimizer interacts with a private proxy oracle, which sends only private messages about a minibatch of gradients. In this setting, we show that expected running time $Ω(\min\{\frac{\sqrt{d}}{α^2}, \frac{d}{\log(1/α)}\})$ is necessary to achieve $α$ excess risk on problems of dimension $d$ when $d \geq 1/α^2$. Upper bounds via DP-SGD show these results are tight when $d>\tildeΩ(1/α^4)$. We further show our lower bound can be strengthened to $Ω(\min\{\frac{d}{\bar{m}α^2}, \frac{d}{\log(1/α)} \})$ for algorithms which use minibatches of size at most $\bar{m} < \sqrt{d}$. We next consider smooth losses, where we relax the private oracle assumption and give lower bounds under only the condition that the optimizer is private. Here, we lower bound the expected number of first order oracle calls by $\tildeΩ\big(\frac{\sqrt{d}}α + \min\{\frac{1}{α^2}, n\}\big)$, where $n$ is the size of the dataset. Modifications to existing algorithms show this bound is nearly tight. Compared to non-private lower bounds, our results show that differentially private optimizers pay a dimension dependent runtime penalty. Finally, as a natural extension of our proof technique, we show lower bounds in the non-smooth setting for optimizers interacting with information limited oracles. Specifically, if the proxy oracle transmits at most $Γ$-bits of information about the gradients in the minibatch, then $Ω\big(\min\{\frac{d}{α^2Γ}, \frac{d}{\log(1/α)}\}\big)$ oracle calls are needed. This result shows fundamental limitations of gradient quantization techniques in optimization.

</details>


### [83] [Multi-Agent VLMs Guided Self-Training with PNU Loss for Low-Resource Offensive Content Detection](https://arxiv.org/abs/2511.13759)
*Han Wang,Deyi Ji,Junyu Lu,Lanyun Zhu,Hailong Zhang,Haiyang Wu,Liqun Liu,Peng Shu,Roy Ka-Wei Lee*

Main category: cs.LG

TL;DR: 提出自训练框架利用无标签数据应对社交媒体冒犯内容检测低资源挑战，实验显示在有限监督下优于基线。


<details>
  <summary>Details</summary>
Motivation: 社交媒体冒犯内容检测缺乏高质量标注数据，存在低资源挑战。

Method: 提出自训练框架，用有限标注数据训练轻量级分类器，借助MA - VLMs为无标签数据分配伪标签，形成Agreed - Unknown和Disagreed - Unknown集合，MA - VLMs模拟双视角，用PNU损失优化分类器。

Result: 在基准数据集实验表明，该框架在有限监督下大幅优于基线，接近大规模模型性能。

Conclusion: 所提自训练框架能有效解决社交媒体冒犯内容检测的低资源问题。

Abstract: Accurate detection of offensive content on social media demands high-quality labeled data; however, such data is often scarce due to the low prevalence of offensive instances and the high cost of manual annotation. To address this low-resource challenge, we propose a self-training framework that leverages abundant unlabeled data through collaborative pseudo-labeling. Starting with a lightweight classifier trained on limited labeled data, our method iteratively assigns pseudo-labels to unlabeled instances with the support of Multi-Agent Vision-Language Models (MA-VLMs). Un-labeled data on which the classifier and MA-VLMs agree are designated as the Agreed-Unknown set, while conflicting samples form the Disagreed-Unknown set. To enhance label reliability, MA-VLMs simulate dual perspectives, moderator and user, capturing both regulatory and subjective viewpoints. The classifier is optimized using a novel Positive-Negative-Unlabeled (PNU) loss, which jointly exploits labeled, Agreed-Unknown, and Disagreed-Unknown data while mitigating pseudo-label noise. Experiments on benchmark datasets demonstrate that our framework substantially outperforms baselines under limited supervision and approaches the performance of large-scale models

</details>


### [84] [SmallML: Bayesian Transfer Learning for Small-Data Predictive Analytics](https://arxiv.org/abs/2511.14049)
*Semen Leontev*

Main category: cs.LG

TL;DR: 论文介绍SmallML框架，用小数据集实现企业级预测精度，验证效果好，解决中小企业AI应用难题。


<details>
  <summary>Details</summary>
Motivation: 中小企业因数据规模与机器学习数据要求不匹配，被系统地排除在AI应用之外，需解决其AI应用问题。

Method: 开发三层架构，整合迁移学习、分层贝叶斯建模和保形预测，各层有具体实现方法。

Result: 在客户流失数据验证中，AUC达96.7% +/- 4.2%，比独立逻辑回归提升24.2个百分点，保形预测达到92%经验覆盖率，训练在33分钟内完成。

Conclusion: SmallML框架能为美国3300万中小企业实现企业级预测，填补AI民主化关键空白。

Abstract: Small and medium-sized enterprises (SMEs) represent 99.9% of U.S. businesses yet remain systematically excluded from AI due to a mismatch between their operational scale and modern machine learning's data requirements. This paper introduces SmallML, a Bayesian transfer learning framework achieving enterprise-level prediction accuracy with datasets as small as 50-200 observations.
  We develop a three-layer architecture integrating transfer learning, hierarchical Bayesian modeling, and conformal prediction. Layer 1 extracts informative priors from 22,673 public records using a SHAP-based procedure transferring knowledge from gradient boosting to logistic regression. Layer 2 implements hierarchical pooling across J=5-50 SMEs with adaptive shrinkage, balancing population patterns with entity-specific characteristics. Layer 3 provides conformal sets with finite-sample coverage guarantees P(y in C(x)) >= 1-alpha for distribution-free uncertainty quantification.
  Validation on customer churn data demonstrates 96.7% +/- 4.2% AUC with 100 observations per business -- a +24.2 point improvement over independent logistic regression (72.5% +/- 8.1%), with p < 0.000001. Conformal prediction achieves 92% empirical coverage at 90% target. Training completes in 33 minutes on standard CPU hardware. By enabling enterprise-grade predictions for 33 million U.S. SMEs previously excluded from machine learning, SmallML addresses a critical gap in AI democratization.
  Keywords: Bayesian transfer learning, hierarchical models, conformal prediction, small-data analytics, SME machine learning

</details>


### [85] [MoETTA: Test-Time Adaptation Under Mixed Distribution Shifts with MoE-LayerNorm](https://arxiv.org/abs/2511.13760)
*Xiao Fan,Jingyan Jiang,Zhaoru Chen,Fanding Huang,Xiao Chen,Qinting Jiang,Bowen Zhang,Xing Tang,Zhi Wang*

Main category: cs.LG

TL;DR: 提出MoETTA框架应对混合分布偏移问题，引入新基准测试，实验显示其性能优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有TTA方法在混合分布偏移场景存在局限，依赖统一适应路径，现有基准无法反映现实复杂情况。

Method: 提出基于熵的MoETTA框架，集成Mixture - of - Experts架构，引入结构解耦的专家集；引入新基准potpourri和potpourri+。

Result: 在三种混合分布偏移设置的大量实验中，MoETTA始终优于强基线。

Conclusion: MoETTA通过专家级多样性建模多个适应方向有优势，达到了SOTA性能。

Abstract: Test-Time adaptation (TTA) has proven effective in mitigating performance drops under single-domain distribution shifts by updating model parameters during inference. However, real-world deployments often involve mixed distribution shifts, where test samples are affected by diverse and potentially conflicting domain factors, posing significant challenges even for SOTA TTA methods. A key limitation in existing approaches is their reliance on a unified adaptation path, which fails to account for the fact that optimal gradient directions can vary significantly across different domains. Moreover, current benchmarks focus only on synthetic or homogeneous shifts, failing to capture the complexity of real-world heterogeneous mixed distribution shifts. To address this, we propose MoETTA, a novel entropy-based TTA framework that integrates the Mixture-of-Experts (MoE) architecture. Rather than enforcing a single parameter update rule for all test samples, MoETTA introduces a set of structurally decoupled experts, enabling adaptation along diverse gradient directions. This design allows the model to better accommodate heterogeneous shifts through flexible and disentangled parameter updates. To simulate realistic deployment conditions, we introduce two new benchmarks: potpourri and potpourri+. While classical settings focus solely on synthetic corruptions, potpourri encompasses a broader range of domain shifts--including natural, artistic, and adversarial distortions--capturing more realistic deployment challenges. Additionally, potpourri+ further includes source-domain samples to evaluate robustness against catastrophic forgetting. Extensive experiments across three mixed distribution shifts settings show that MoETTA consistently outperforms strong baselines, establishing SOTA performance and highlighting the benefit of modeling multiple adaptation directions via expert-level diversity.

</details>


### [86] [Radial Compensation: Stable and Semantically Decoupled Generative Models on Riemannian Manifolds](https://arxiv.org/abs/2511.14056)
*Marios Papamichals,Regina Ruane*

Main category: cs.LG

TL;DR: 提出径向补偿（RC）方法用于流形上的生成模型，解决曲率与参数纠缠问题，具有良好效果。


<details>
  <summary>Details</summary>
Motivation: 现有流形生成模型中指数映射和保体积图表存在曲率与模型参数纠缠、梯度方差大等问题，高维潜在归一化流有测试似然差和求解器刚性问题。

Method: 引入径向补偿（RC）方法，选择切空间中的基密度使似然仅依赖于到极点的测地距离，将参数语义与曲率解耦；扩展RC到已知测地极体积的流形；推导平衡指数（bExp）图表族。

Result: RC在多种密度、变分自编码器、图像和图上的流以及蛋白质模型中产生稳定的生成模型，改善似然、恢复清晰的测地半径、防止高维流中的半径爆炸。

Conclusion: RC - bExp是流形上似然训练生成模型的稳健默认选择。

Abstract: Generative models on curved spaces rely on charts to map Euclidean spaces to manifolds. Exponential maps preserve geodesics but have stiff, radius-dependent Jacobians, while volume-preserving charts maintain densities but distort geodesic distances. Both approaches entangle curvature with model parameters, inflating gradient variance. In high-dimensional latent normalizing flows, the wrapped exponential prior can stretch radii far beyond the curvature scale, leading to poor test likelihoods and stiff solvers. We introduce Radial Compensation (RC), an information-geometric method that selects the base density in the tangent space so that the likelihood depends only on geodesic distance from a pole, decoupling parameter semantics from curvature. RC lets radial parameters retain their usual meaning in geodesic units, while the chart can be tuned as a numerical preconditioner. We extend RC to manifolds with known geodesic polar volume and show that RC is the only construction for geodesic-radial likelihoods with curvature-invariant Fisher information. We derive the Balanced-Exponential (bExp) chart family, balancing volume distortion and geodesic error. Under RC, all bExp settings preserve the same manifold density and Fisher information, with smaller dial values reducing gradient variance and flow cost. Empirically, RC yields stable generative models across densities, VAEs, flows on images and graphs, and protein models. RC improves likelihoods, restores clean geodesic radii, and prevents radius blow-ups in high-dimensional flows, making RC-bExp a robust default for likelihood-trained generative models on manifolds.

</details>


### [87] [Gene Incremental Learning for Single-Cell Transcriptomics](https://arxiv.org/abs/2511.13762)
*Jiaxin Qi,Yan Cui,Jianqiang Huang,Gaogang Xie*

Main category: cs.LG

TL;DR: 本文针对基因这一特殊标记，为单细胞转录组学构建基因增量学习管道和评估体系，解决遗忘问题并提供基准。


<details>
  <summary>Details</summary>
Motivation: 标记的增量学习研究稀缺，语言中标记的整体性为增量学习框架设计带来挑战，存在研究空白。

Method: 针对单细胞转录组学的基因构建增量学习管道和评估体系，适配现有类增量学习方法缓解基因遗忘问题。

Result: 实验证明框架设计和评估合理，方法适配有效。

Conclusion: 为单细胞转录组学的基因增量学习提供完整基准。

Abstract: Classes, as fundamental elements of Computer Vision, have been extensively studied within incremental learning frameworks. In contrast, tokens, which play essential roles in many research fields, exhibit similar characteristics of growth, yet investigations into their incremental learning remain significantly scarce. This research gap primarily stems from the holistic nature of tokens in language, which imposes significant challenges on the design of incremental learning frameworks for them. To overcome this obstacle, in this work, we turn to a type of token, gene, for a large-scale biological dataset--single-cell transcriptomics--to formulate a pipeline for gene incremental learning and establish corresponding evaluations. We found that the forgetting problem also exists in gene incremental learning, thus we adapted existing class incremental learning methods to mitigate the forgetting of genes. Through extensive experiments, we demonstrated the soundness of our framework design and evaluations, as well as the effectiveness of our method adaptations. Finally, we provide a complete benchmark for gene incremental learning in single-cell transcriptomics.

</details>


### [88] [Synthetic Survival Control: Extending Synthetic Controls for "When-If" Decision](https://arxiv.org/abs/2511.14133)
*Jessy Xinyi Han,Devavrat Shah*

Main category: cs.LG

TL;DR: 文章提出Synthetic Survival Control (SSC)方法估计面板数据中反事实风险轨迹，用多国家癌症治疗临床数据集验证，表明新疗法改善生存，框架为反事实生存推断提供通用工具。


<details>
  <summary>Details</summary>
Motivation: 解决观测数据中对事件发生时间结果进行因果效应估计时面临的删失、样本量有限和非随机处理分配等挑战，满足现实中回答‘何时 - 如果’问题的需求。

Method: 提出SSC方法，将感兴趣单元的反事实风险轨迹估计为其他单元观测轨迹的加权组合；引入具有低秩结构的面板框架进行因果生存分析，并为SSC建立识别和有限样本保证。

Result: 用多国家癌症治疗临床数据集验证，发现获得新疗法与改善生存相关，干预后风险轨迹低于合成对照。

Conclusion: 框架为利用观测数据进行反事实生存推断提供通用且可解释的工具。

Abstract: Estimating causal effects on time-to-event outcomes from observational data is particularly challenging due to censoring, limited sample sizes, and non-random treatment assignment. The need for answering such "when-if" questions--how the timing of an event would change under a specified intervention--commonly arises in real-world settings with heterogeneous treatment adoption and confounding. To address these challenges, we propose Synthetic Survival Control (SSC) to estimate counterfactual hazard trajectories in a panel data setting where multiple units experience potentially different treatments over multiple periods. In such a setting, SSC estimates the counterfactual hazard trajectory for a unit of interest as a weighted combination of the observed trajectories from other units. To provide formal justification, we introduce a panel framework with a low-rank structure for causal survival analysis. Indeed, such a structure naturally arises under classical parametric survival models. Within this framework, for the causal estimand of interest, we establish identification and finite sample guarantees for SSC. We validate our approach using a multi-country clinical dataset of cancer treatment outcomes, where the staggered introduction of new therapies creates a quasi-experimental setting. Empirically, we find that access to novel treatments is associated with improved survival, as reflected by lower post-intervention hazard trajectories relative to their synthetic counterparts. Given the broad relevance of survival analysis across medicine, economics, and public policy, our framework offers a general and interpretable tool for counterfactual survival inference using observational data.

</details>


### [89] [PROF: An LLM-based Reward Code Preference Optimization Framework for Offline Imitation Learning](https://arxiv.org/abs/2511.13765)
*Shengjie Sun,Jiafei Lyu,Runze Liu,Mengbei Yan,Bo Liu,Deheng Ye,Xiu Li*

Main category: cs.LG

TL;DR: 提出PROF框架，利用大语言模型生成和改进奖励函数代码，通过RPR策略评估和排序奖励函数，在D4RL上实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 现有离线模仿学习方法在估计奖励时对奖励结构的假设过于简单，需改进奖励函数生成方法。

Method: 提出PROF框架，利用大语言模型从自然语言描述和单个专家轨迹生成奖励函数代码；提出RPR策略评估和排序奖励函数，通过交替进行RPR和基于文本的梯度优化来选择和优化奖励函数。

Result: 在D4RL的众多数据集和领域上，PROF超过或与近期强基线相当。

Conclusion: PROF方法在离线模仿学习中有效。

Abstract: Offline imitation learning (offline IL) enables training effective policies without requiring explicit reward annotations. Recent approaches attempt to estimate rewards for unlabeled datasets using a small set of expert demonstrations. However, these methods often assume that the similarity between a trajectory and an expert demonstration is positively correlated with the reward, which oversimplifies the underlying reward structure. We propose PROF, a novel framework that leverages large language models (LLMs) to generate and improve executable reward function codes from natural language descriptions and a single expert trajectory. We propose Reward Preference Ranking (RPR), a novel reward function quality assessment and ranking strategy without requiring environment interactions or RL training. RPR calculates the dominance scores of the reward functions, where higher scores indicate better alignment with expert preferences. By alternating between RPR and text-based gradient optimization, PROF fully automates the selection and refinement of optimal reward functions for downstream policy learning. Empirical results on D4RL demonstrate that PROF surpasses or matches recent strong baselines across numerous datasets and domains, highlighting the effectiveness of our approach.

</details>


### [90] [Credal Ensemble Distillation for Uncertainty Quantification](https://arxiv.org/abs/2511.13766)
*Kaizheng Wang,Fabio Cuzzolin,David Moens,Hans Hallez*

Main category: cs.LG

TL;DR: 提出Credal ensemble distillation (CED)框架将深度集成（DE）压缩为单一模型CREDIT用于分类任务，在分布外检测基准上有良好不确定性估计效果且降低推理开销。


<details>
  <summary>Details</summary>
Motivation: 深度集成（DE）在推理时计算和内存成本高，限制实际广泛应用，需解决该问题。

Method: 提出CED框架，将DE压缩为单一模型CREDIT，CREDIT预测类概率区间进行不确定性量化。

Result: 在分布外检测基准上，CED相比现有基线有优越或相当的不确定性估计效果，且大幅降低推理开销。

Conclusion: CED能有效解决DE计算和内存成本高的问题，在不确定性估计和降低推理开销上表现良好。

Abstract: Deep ensembles (DE) have emerged as a powerful approach for quantifying predictive uncertainty and distinguishing its aleatoric and epistemic components, thereby enhancing model robustness and reliability. However, their high computational and memory costs during inference pose significant challenges for wide practical deployment. To overcome this issue, we propose credal ensemble distillation (CED), a novel framework that compresses a DE into a single model, CREDIT, for classification tasks. Instead of a single softmax probability distribution, CREDIT predicts class-wise probability intervals that define a credal set, a convex set of probability distributions, for uncertainty quantification. Empirical results on out-of-distribution detection benchmarks demonstrate that CED achieves superior or comparable uncertainty estimation compared to several existing baselines, while substantially reducing inference overhead compared to DE.

</details>


### [91] [Dynamic Temperature Scheduler for Knowledge Distillation](https://arxiv.org/abs/2511.13767)
*Sibgat Ul Islam,Jawad Ibn Ahad,Fuad Rahman,Mohammad Ruhul Amin,Nabeel Mohammed,Shafin Rahman*

Main category: cs.LG

TL;DR: 提出动态温度调度器DTS改进知识蒸馏，在多任务上优于静态温度基线。


<details>
  <summary>Details</summary>
Motivation: 传统知识蒸馏用固定温度，且师生模型架构差异致对数概率值不匹配，需动态调整温度。

Method: 引入DTS，基于师生交叉熵损失差距动态调整温度，与现有KD框架集成。

Result: 在视觉和NLP多任务上验证，DTS始终优于静态温度基线。

Conclusion: DTS是首个基于师生分布差异自适应的温度调度方法，有效且可集成现有框架。

Abstract: Knowledge Distillation (KD) trains a smaller student model using a large, pre-trained teacher model, with temperature as a key hyperparameter controlling the softness of output probabilities. Traditional methods use a fixed temperature throughout training, which is suboptimal. Moreover, architectural differences between teacher and student often result in mismatched logit magnitudes. We demonstrate that students benefit from softer probabilities early in training but require sharper probabilities in later stages. We introduce Dynamic Temperature Scheduler (DTS), which adjusts temperature dynamically based on the cross-entropy loss gap between teacher and student. To our knowledge, this is the first temperature scheduling method that adapts based on the divergence between teacher and student distributions. Our method integrates seamlessly with existing KD frameworks. We validate DTS across multiple KD strategies on vision (CIFAR-100, Tiny-ImageNet) and NLP tasks (GLUE, Dolly, SelfIns, UnNI, S-NI), consistently outperforming static-temperature baselines. Code is available at https://github.com/Sibgat-Ul/DTS.

</details>


### [92] [Compiling to linear neurons](https://arxiv.org/abs/2511.13769)
*Joey Velez-Ginorio,Nada Amin,Konrad Kording,Steve Zdancewic*

Main category: cs.LG

TL;DR: 引入Cajal语言以直接编程神经网络，实验显示可使网络学习更快、数据效率更高且易调试。


<details>
  <summary>Details</summary>
Motivation: 现有间接编程神经网络方式缺乏离散结构，多数离散算法不可编译进神经网络，因不具可微性与基于梯度的学习算法不兼容。

Method: 引入Cajal语言，证明其程序可编译为线性神经元，使离散算法以可微形式表达。

Result: 通过实验，将线性神经元与其他神经网络连接，网络学习更快、数据效率更高且易调试。

Conclusion: 线性编程语言为直接编程神经网络提供途径，促进学习与普通编程离散结构的交互。

Abstract: We don't program neural networks directly. Instead, we rely on an indirect style where learning algorithms, like gradient descent, determine a neural network's function by learning from data. This indirect style is often a virtue; it empowers us to solve problems that were previously impossible. But it lacks discrete structure. We can't compile most algorithms into a neural network -- even if these algorithms could help the network learn. This limitation occurs because discrete algorithms are not obviously differentiable, making them incompatible with the gradient-based learning algorithms that determine a neural network's function. To address this, we introduce $\textsf{Cajal}$: a typed, higher-order and linear programming language intended to be a minimal vehicle for exploring a direct style of programming neural networks. We prove $\textsf{Cajal}$ programs compile to linear neurons, allowing discrete algorithms to be expressed in a differentiable form compatible with gradient-based learning. With our implementation of $\textsf{Cajal}$, we conduct several experiments where we link these linear neurons against other neural networks to determine part of their function prior to learning. Linking with these neurons allows networks to learn faster, with greater data-efficiency, and in a way that's easier to debug. A key lesson is that linear programming languages provide a path towards directly programming neural networks, enabling a rich interplay between learning and the discrete structures of ordinary programming.

</details>


### [93] [MoE-SpeQ: Speculative Quantized Decoding with Proactive Expert Prefetching and Offloading for Mixture-of-Experts](https://arxiv.org/abs/2511.14102)
*Wenfeng Wang,Jiacheng Liu,Xiaofeng Hou,Xinfeng Xia,Peng Tang,Mingxuan Zhang,Chao Li,Minyi Guo*

Main category: cs.LG

TL;DR: 本文提出MoE - SpeQ推理系统，通过投机执行和专家卸载协同设计克服MoE模型推理的I/O瓶颈，在受限设备上实现加速。


<details>
  <summary>Details</summary>
Motivation: 当前MoE模型推理内存需求大，卸载专家到主机内存会引入I/O瓶颈，影响性能。

Method: 构建MoE - SpeQ系统，用设备上的草稿模型预测所需专家序列，运行时协调器预取专家，自适应调节器动态调整策略。

Result: 在内存受限设备上，MoE - SpeQ对Phi - MoE模型最多实现2.34倍加速。

Conclusion: 为资源受限环境下管理数据依赖的内存访问建立了新方法，使MoE推理在通用硬件上更易实现。

Abstract: The immense memory requirements of state-of-the-art Mixture-of-Experts (MoE) models present a significant challenge for inference, often exceeding the capacity of a single accelerator. While offloading experts to host memory is a common solution, it introduces a severe I/O bottleneck over the PCIe bus, as the data-dependent nature of expert selection places these synchronous transfers directly on the critical path of execution, crippling performance.
  This paper argues that the I/O bottleneck can be overcome by trading a small amount of cheap, on-device computation to hide the immense cost of data movement. We present MoE-SpeQ, a new inference system built on a novel co-design of speculative execution and expert offloading. MoE-SpeQ employs a small, on-device draft model to predict the sequence of required experts for future tokens. This foresight enables a runtime orchestrator to prefetch these experts from host memory, effectively overlapping the expensive I/O with useful computation and hiding the latency from the critical path. To maximize performance, an adaptive governor, guided by an Amortization Roofline Model, dynamically tunes the speculation strategy to the underlying hardware. Our evaluation on memory-constrained devices shows that for the Phi-MoE model, MoE-SpeQ achieves at most 2.34x speedup over the state-of-the-art offloading framework. Our work establishes a new, principled approach for managing data-dependent memory access in resource-limited environments, making MoE inference more accessible on commodity hardware.

</details>


### [94] [Self-Attention as Distributional Projection: A Unified Interpretation of Transformer Architecture](https://arxiv.org/abs/2511.13780)
*Nihal Mehta*

Main category: cs.LG

TL;DR: 本文将自注意力机制与分布语义学原理相联系，给出其数学解释，表明Transformer架构特定代数形式源于投影原理。


<details>
  <summary>Details</summary>
Motivation: 对自注意力机制进行数学解释，探究Transformer架构设计的原理。

Method: 从GloVe嵌入的共现矩阵出发，将语料库级别的共现统计投影到序列上下文中。

Result: 投影自然地捕捉了上下文影响，查询 - 键 - 值机制是用于建模方向关系的自然不对称扩展，位置编码和多头注意力是投影原理的结构化改进。

Conclusion: Transformer架构的特定代数形式来自这些投影原理，并非任意设计。

Abstract: This paper presents a mathematical interpretation of self-attention by connecting it to distributional semantics principles. We show that self-attention emerges from projecting corpus-level co-occurrence statistics into sequence context. Starting from the co-occurrence matrix underlying GloVe embeddings, we demonstrate how the projection naturally captures contextual influence, with the query-key-value mechanism arising as the natural asymmetric extension for modeling directional relationships. Positional encodings and multi-head attention then follow as structured refinements of this same projection principle. Our analysis demonstrates that the Transformer architecture's particular algebraic form follows from these projection principles rather than being an arbitrary design choice.

</details>


### [95] [Exploring Transferability of Self-Supervised Learning by Task Conflict Calibration](https://arxiv.org/abs/2511.13787)
*Huijie Guo,Jingyao Wang,Peizheng Guo,Xingchen Shen,Changwen Zheng,Wenwen Qiang*

Main category: cs.LG

TL;DR: 本文探讨自监督学习（SSL）表征可迁移性，提出TC²方法缓解任务冲突，实验证明该方法能提升SSL模型可迁移性。


<details>
  <summary>Details</summary>
Motivation: 研究SSL表征的可迁移性以及如何有效建模这种可迁移性。

Method: 构建多个SSL任务，提出Task Conflict Calibration (TC²)方法，包括拆分批次、使用因子提取和权重提取网络、校准样本表征并通过两阶段双层优化框架集成。

Result: 在多个下游任务的实验中，该方法持续提升了SSL模型的可迁移性。

Conclusion: 提出的TC²方法能有效提升SSL模型表征的可迁移性。

Abstract: In this paper, we explore the transferability of SSL by addressing two central questions: (i) what is the representation transferability of SSL, and (ii) how can we effectively model this transferability? Transferability is defined as the ability of a representation learned from one task to support the objective of another.
  Inspired by the meta-learning paradigm, we construct multiple SSL tasks within each training batch to support explicitly modeling transferability. Based on empirical evidence and causal analysis, we find that although introducing task-level information improves transferability, it is still hindered by task conflict. To address this issue, we propose a Task Conflict Calibration (TC$^2$) method to alleviate the impact of task conflict. Specifically, it first splits batches to create multiple SSL tasks, infusing task-level information. Next, it uses a factor extraction network to produce causal generative factors for all tasks and a weight extraction network to assign dedicated weights to each sample, employing data reconstruction, orthogonality, and sparsity to ensure effectiveness. Finally, TC$^2$ calibrates sample representations during SSL training and integrates into the pipeline via a two-stage bi-level optimization framework to boost the transferability of learned representations. Experimental results on multiple downstream tasks demonstrate that our method consistently improves the transferability of SSL models.

</details>


### [96] [\textit{FLARE}: Adaptive Multi-Dimensional Reputation for Robust Client Reliability in Federated Learning](https://arxiv.org/abs/2511.14715)
*Abolfazl Younesi,Leon Kiss,Zahra Najafabadi Samani,Juan Aznar Poveda,Thomas Fahringer*

Main category: cs.LG

TL;DR: 提出自适应声誉框架FLARE应对联邦学习攻击，引入SM攻击作基准，实验显示FLARE性能优。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习防御机制依赖静态阈值和二元分类，无法适应现实中不断变化的客户端行为。

Method: 提出FLARE框架，包括多维声誉评分、自适应阈值机制、声誉加权聚合和LDP机制，还引入SM攻击。

Result: 在多种数据集和攻击类型下，FLARE比现有方法保持更高模型准确率、收敛更快，提高鲁棒性，实现恶意客户端检测，计算开销小。

Conclusion: FLARE是一种有效的联邦学习防御框架，可应对多种攻击，保障模型安全。

Abstract: Federated learning (FL) enables collaborative model training while preserving data privacy. However, it remains vulnerable to malicious clients who compromise model integrity through Byzantine attacks, data poisoning, or adaptive adversarial behaviors. Existing defense mechanisms rely on static thresholds and binary classification, failing to adapt to evolving client behaviors in real-world deployments. We propose FLARE, an adaptive reputation-based framework that transforms client reliability assessment from binary decisions to a continuous, multi-dimensional trust evaluation. FLARE integrates: (i) a multi-dimensional reputation score capturing performance consistency, statistical anomaly indicators, and temporal behavior, (ii) a self-calibrating adaptive threshold mechanism that adjusts security strictness based on model convergence and recent attack intensity, (iii) reputation-weighted aggregation with soft exclusion to proportionally limit suspicious contributions rather than eliminating clients outright, and (iv) a Local Differential Privacy (LDP) mechanism enabling reputation scoring on privatized client updates. We further introduce a highly evasive Statistical Mimicry (SM) attack, a benchmark adversary that blends honest gradients with synthetic perturbations and persistent drift to remain undetected by traditional filters. Extensive experiments with 100 clients on MNIST, CIFAR-10, and SVHN demonstrate that FLARE maintains high model accuracy and converges faster than state-of-the-art Byzantine-robust methods under diverse attack types, including label flipping, gradient scaling, adaptive attacks, ALIE, and SM. FLARE improves robustness by up to 16% and preserves model convergence within 30% of the non-attacked baseline, while achieving strong malicious-client detection performance with minimal computational overhead. https://github.com/Anonymous0-0paper/FLARE

</details>


### [97] [Scaling Patterns in Adversarial Alignment: Evidence from Multi-LLM Jailbreak Experiments](https://arxiv.org/abs/2511.13788)
*Samuel Nathanson,Rebecca Williams,Cynthia Matuszek*

Main category: cs.LG

TL;DR: 研究大语言模型在对抗交互中的漏洞，发现相对模型大小与有害输出的可能性和严重性相关，攻击者行为多样性对结果影响更大，攻击者对齐能减轻有害响应。


<details>
  <summary>Details</summary>
Motivation: 探讨大语言模型在多智能体和安全关键环境中，模型对抗交互时漏洞的扩展情况，研究大模型是否能系统性地突破小模型的对齐防护。

Method: 使用JailbreakBench的标准化对抗任务，模拟超6000次多轮攻击者 - 目标交互，用三个独立大语言模型评判并聚合伤害和拒绝分数。

Result: 平均伤害与攻击者 - 目标大小比的对数强相关；攻击者的平均伤害分数方差更大；攻击者拒绝频率与伤害强负相关。

Conclusion: 模型大小不对称影响鲁棒性，为对抗扩展模式提供探索性证据，需对模型间对齐和安全进行更可控的研究。

Abstract: Large language models (LLMs) increasingly operate in multi-agent and safety-critical settings, raising open questions about how their vulnerabilities scale when models interact adversarially. This study examines whether larger models can systematically jailbreak smaller ones - eliciting harmful or restricted behavior despite alignment safeguards. Using standardized adversarial tasks from JailbreakBench, we simulate over 6,000 multi-turn attacker-target exchanges across major LLM families and scales (0.6B-120B parameters), measuring both harm score and refusal behavior as indicators of adversarial potency and alignment integrity. Each interaction is evaluated through aggregated harm and refusal scores assigned by three independent LLM judges, providing a consistent, model-based measure of adversarial outcomes. Aggregating results across prompts, we find a strong and statistically significant correlation between mean harm and the logarithm of the attacker-to-target size ratio (Pearson r = 0.51, p < 0.001; Spearman rho = 0.52, p < 0.001), indicating that relative model size correlates with the likelihood and severity of harmful completions. Mean harm score variance is higher across attackers (0.18) than across targets (0.10), suggesting that attacker-side behavioral diversity contributes more to adversarial outcomes than target susceptibility. Attacker refusal frequency is strongly and negatively correlated with harm (rho = -0.93, p < 0.001), showing that attacker-side alignment mitigates harmful responses. These findings reveal that size asymmetry influences robustness and provide exploratory evidence for adversarial scaling patterns, motivating more controlled investigations into inter-model alignment and safety.

</details>


### [98] [ScoresActivation: A New Activation Function for Model Agnostic Global Explainability by Design](https://arxiv.org/abs/2511.13809)
*Emanuel Covaci,Fabian Galis,Radu Balan,Daniela Zaharie,Darian Onchis*

Main category: cs.LG

TL;DR: 提出一种可微方法将特征重要性估计集成到模型训练中，实现全局可解释性，评估显示该方法忠实稳定、速度快且提升分类准确率，弥合了模型准确性和可解释性的差距。


<details>
  <summary>Details</summary>
Motivation: 当前事后解释方法与模型训练过程脱节，限制了其忠实性和实用性，需要一种新方法来实现全局可解释性。

Method: 引入可微方法，将特征重要性估计直接集成到模型训练中，使用ScoresActivation函数作为特征排序机制。

Result: 在基准数据集上评估，该方法得到与SHAP值和真实特征重要性一致的全局忠实、稳定特征排名，特征评分比经典SHAP方法快150倍，提高了分类准确率，对无关输入具有鲁棒性。

Conclusion: 该工作弥合了模型准确性和可解释性的差距，为内在可解释的机器学习提供了可扩展框架。

Abstract: Understanding the decision of large deep learning models is a critical challenge for building transparent and trustworthy systems. Although the current post hoc explanation methods offer valuable insights into feature importance, they are inherently disconnected from the model training process, limiting their faithfulness and utility. In this work, we introduce a novel differentiable approach to global explainability by design, integrating feature importance estimation directly into model training. Central to our method is the ScoresActivation function, a feature-ranking mechanism embedded within the learning pipeline. This integration enables models to prioritize features according to their contribution to predictive performance in a differentiable and end-to-end trainable manner. Evaluations across benchmark datasets show that our approach yields globally faithful, stable feature rankings aligned with SHAP values and ground-truth feature importance, while maintaining high predictive performance. Moreover, feature scoring is 150 times faster than the classical SHAP method, requiring only 2 seconds during training compared to SHAP's 300 seconds for feature ranking in the same configuration. Our method also improves classification accuracy by 11.24% with 10 features (5 relevant) and 29.33% with 16 features (5 relevant, 11 irrelevant), demonstrating robustness to irrelevant inputs. This work bridges the gap between model accuracy and interpretability, offering a scalable framework for inherently explainable machine learning.

</details>


### [99] [Beat the long tail: Distribution-Aware Speculative Decoding for RL Training](https://arxiv.org/abs/2511.13841)
*Zelei Shao,Vikranth Srivatsa,Sanjana Srivastava,Qingyang Wu,Alpay Ariyak,Xiaoxia Wu,Ameen Patel,Jue Wang,Percy Liang,Tri Dao,Ce Zhang,Yiying Zhang,Ben Athiwaratkun,Chenfeng Xu,Junxiong Wang*

Main category: cs.LG

TL;DR: 提出DAS框架加速强化学习后训练中的滚动过程，在数学和代码推理任务实验中，可减少达50%滚动时间且不影响训练质量。


<details>
  <summary>Details</summary>
Motivation: 强化学习后训练的滚动阶段效率受长轨迹逐令牌生成的限制，长滚动长度的长尾分布是瓶颈，但历史滚动数据存在稳定模式。

Method: 提出DAS框架，包含用增量维护后缀树构建的自适应非参数起草器和长度感知推测策略。

Result: 在数学和代码推理任务实验中，DAS减少滚动时间达50%，训练曲线不变。

Conclusion: 分布感知推测解码可显著加速强化学习后训练，且不损害学习质量。

Abstract: Reinforcement learning(RL) post-training has become essential for aligning large language models (LLMs), yet its efficiency is increasingly constrained by the rollout phase, where long trajectories are generated token by token. We identify a major bottleneck:the long-tail distribution of rollout lengths, where a small fraction of long generations dominates wall clock time and a complementary opportunity; the availability of historical rollouts that reveal stable prompt level patterns across training epochs. Motivated by these observations, we propose DAS, a Distribution Aware Speculative decoding framework that accelerates RL rollouts without altering model outputs. DAS integrates two key ideas: an adaptive, nonparametric drafter built from recent rollouts using an incrementally maintained suffix tree, and a length aware speculation policy that allocates more aggressive draft budgets to long trajectories that dominate makespan. This design exploits rollout history to sustain acceptance while balancing base and token level costs during decoding. Experiments on math and code reasoning tasks show that DAS reduces rollout time up to 50% while preserving identical training curves, demonstrating that distribution-aware speculative decoding can significantly accelerate RL post training without compromising learning quality.

</details>


### [100] [AnaCP: Toward Upper-Bound Continual Learning via Analytic Contrastive Projection](https://arxiv.org/abs/2511.13880)
*Saleh Momeni,Changnan Xiao,Bing Liu*

Main category: cs.LG

TL;DR: 本文研究类增量学习问题，提出AnaCP方法解决现有方法无法持续适应特征表示的问题，实验显示该方法优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 传统CIL方法有灾难性遗忘问题，结合预训练模型的方法无法持续适应特征表示，导致性能欠佳。

Method: 提出AnaCP（Analytic Contrastive Projection）方法，在无梯度训练下实现增量特征适应，保留分析分类器的效率。

Result: AnaCP不仅优于现有基线，还达到联合训练的准确率水平。

Conclusion: AnaCP方法有效解决了现有CIL方法的局限性，提升了类增量学习的性能。

Abstract: This paper studies the problem of class-incremental learning (CIL), a core setting within continual learning where a model learns a sequence of tasks, each containing a distinct set of classes. Traditional CIL methods, which do not leverage pre-trained models (PTMs), suffer from catastrophic forgetting (CF) due to the need to incrementally learn both feature representations and the classifier. The integration of PTMs into CIL has recently led to efficient approaches that treat the PTM as a fixed feature extractor combined with analytic classifiers, achieving state-of-the-art performance. However, they still face a major limitation: the inability to continually adapt feature representations to best suit the CIL tasks, leading to suboptimal performance. To address this, we propose AnaCP (Analytic Contrastive Projection), a novel method that preserves the efficiency of analytic classifiers while enabling incremental feature adaptation without gradient-based training, thereby eliminating the CF caused by gradient updates. Our experiments show that AnaCP not only outperforms existing baselines but also achieves the accuracy level of joint training, which is regarded as the upper bound of CIL.

</details>


### [101] [Tractable Probabilistic Models for Investment Planning](https://arxiv.org/abs/2511.13888)
*Nicolas M. Cuadrado A.,Mohannad Takrouri,Jiří Němeček,Martin Takáč,Jakub Mareček*

Main category: cs.LG

TL;DR: 提出用可处理概率模型（TPMs）进行电力投资规划，比传统方法有计算和可靠性优势。


<details>
  <summary>Details</summary>
Motivation: 传统方法在电力投资规划的长期预测中存在局限性，难以洞察特定情景的波动性，阻碍稳健决策。

Method: 采用可处理概率模型（TPMs），特别是和积网络（SPNs）进行投资规划，将机会约束优化直接嵌入规划中。

Result: 通过案例研究证明该方法比传统基于情景的模型有计算和可靠性优势。

Conclusion: 可处理概率模型（TPMs）能有效用于电力投资规划，支持稳健情景扩展和风险评估。

Abstract: Investment planning in power utilities, such as generation and transmission expansion, requires decade-long forecasts under profound uncertainty. Forecasting of energy mix and energy use decades ahead is nontrivial. Classical approaches focus on generating a finite number of scenarios (modeled as a mixture of Diracs in statistical theory terms), which limits insight into scenario-specific volatility and hinders robust decision-making. We propose an alternative using tractable probabilistic models (TPMs), particularly sum-product networks (SPNs). These models enable exact, scalable inference of key quantities such as scenario likelihoods, marginals, and conditional probabilities, supporting robust scenario expansion and risk assessment.
  This framework enables direct embedding of chance-constrained optimization into investment planning, enforcing safety or reliability with prescribed confidence levels. TPMs allow both scenario analysis and volatility quantification by compactly representing high-dimensional uncertainties. We demonstrate the approach's effectiveness through a representative power system planning case study, illustrating computational and reliability advantages over traditional scenario-based models.

</details>


### [102] [Beyond One-Size-Fits-All: Neural Networks for Differentially Private Tabular Data Synthesis](https://arxiv.org/abs/2511.13893)
*Kai Chen,Chen Gong,Tianhao Wang*

Main category: cs.LG

TL;DR: 指出在差分隐私表格数据合成中统计模型结论不完整，提出MargNet，在不同相关性数据集表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有结论忽略密集相关数据集挑战，现有基于神经网络算法有局限。

Method: 将统计模型的算法设计融入神经网络，采用自适应边际选择策略训练神经网络生成符合所选边际的数据。

Result: 在稀疏相关数据集上接近最佳统计方法且速度快7倍，在密集相关数据集上降低保真度误差达26%。

Conclusion: MargNet在不同相关性数据集上有良好表现，建立新的最优结果。

Abstract: In differentially private (DP) tabular data synthesis, the consensus is that statistical models are better than neural network (NN)-based methods. However, we argue that this conclusion is incomplete and overlooks the challenge of densely correlated datasets, where intricate dependencies can overwhelm statistical models. In such complex scenarios, neural networks are more suitable due to their capacity to fit complex distributions by learning directly from samples. Despite this potential, existing NN-based algorithms still suffer from significant limitations. We therefore propose MargNet, incorporating successful algorithmic designs of statistical models into neural networks. MargNet applies an adaptive marginal selection strategy and trains the neural networks to generate data that conforms to the selected marginals. On sparsely correlated datasets, our approach achieves utility close to the best statistical method while offering an average 7$\times$ speedup over it. More importantly, on densely correlated datasets, MargNet establishes a new state-of-the-art, reducing fidelity error by up to 26\% compared to the previous best. We release our code on GitHub.\footnote{https://github.com/KaiChen9909/margnet}

</details>


### [103] [Weather Maps as Tokens: Transformers for Renewable Energy Forecasting](https://arxiv.org/abs/2511.13935)
*Federico Battini*

Main category: cs.LG

TL;DR: 提出将天气图作为变压器序列中的令牌预测可再生能源的新方法，评估显示降低了RMSE。


<details>
  <summary>Details</summary>
Motivation: 当前方法无法有效整合天气模式的空间背景和时间演变，需要准确的可再生能源预测以减少对化石燃料的依赖和实现电网脱碳。

Method: 用轻量级卷积神经网络将每小时天气图编码为空间令牌，再用变压器处理以捕捉45小时预测范围内的时间动态。

Result: 与ENTSO - E运营预测相比，风能和太阳能的RMSE分别降低约60%和20%。

Conclusion: 新方法在可再生能源预测上有较好效果，还提供了每日预测的实时仪表盘。

Abstract: Accurate renewable energy forecasting is essential to reduce dependence on fossil fuels and enabling grid decarbonization. However, current approaches fail to effectively integrate the rich spatial context of weather patterns with their temporal evolution. This work introduces a novel approach that treats weather maps as tokens in transformer sequences to predict renewable energy. Hourly weather maps are encoded as spatial tokens using a lightweight convolutional neural network, and then processed by a transformer to capture temporal dynamics across a 45-hour forecast horizon. Despite disadvantages in input initialization, evaluation against ENTSO-E operational forecasts shows a reduction in RMSE of about 60\% and 20\% for wind and solar respectively. A live dashboard showing daily forecasts is available at: https://www.sardiniaforecast.ifabfoundation.it.

</details>


### [104] [Complex-Weighted Convolutional Networks: Provable Expressiveness via Complex Diffusion](https://arxiv.org/abs/2511.13937)
*Cristina López Amado,Tassilo Schwarz,Yu Tian,Renaud Lambiotte*

Main category: cs.LG

TL;DR: 本文针对GNN过平滑和在异质图上性能差的问题，提出复杂加权卷积网络（CWCN），该网络基于复杂加权结构扩散，简单易实现，在基准数据集上表现良好。


<details>
  <summary>Details</summary>
Motivation: 解决GNN存在的过平滑和在异质图上性能差的问题。

Method: 引入复杂加权结构，将随机游走扩展到复数域，提出CWCN网络，直接从数据中学习合适的复杂加权结构，并通过可学习矩阵和非线性激活丰富扩散过程。

Result: 证明复杂扩散具有高表达性，CWCN在基准数据集上取得有竞争力的性能。

Conclusion: 复杂加权扩散为增强GNN表达性提供了一种有原则且通用的机制，为理论和实践有效的模型开辟了新途径。

Abstract: Graph Neural Networks (GNNs) have achieved remarkable success across diverse applications, yet they remain limited by oversmoothing and poor performance on heterophilic graphs. To address these challenges, we introduce a novel framework that equips graphs with a complex-weighted structure, assigning each edge a complex number to drive a diffusion process that extends random walks into the complex domain. We prove that this diffusion is highly expressive: with appropriately chosen complex weights, any node-classification task can be solved in the steady state of a complex random walk. Building on this insight, we propose the Complex-Weighted Convolutional Network (CWCN), which learns suitable complex-weighted structures directly from data while enriching diffusion with learnable matrices and nonlinear activations. CWCN is simple to implement, requires no additional hyperparameters beyond those of standard GNNs, and achieves competitive performance on benchmark datasets. Our results demonstrate that complex-weighted diffusion provides a principled and general mechanism for enhancing GNN expressiveness, opening new avenues for models that are both theoretically grounded and practically effective.

</details>


### [105] [The Impact of Bootstrap Sampling Rate on Random Forest Performance in Regression Tasks](https://arxiv.org/abs/2511.13952)
*Michał Iwaniuk,Mateusz Jarosz,Bartłomiej Borycki,Bartosz Jezierski,Jan Cwalina,Stanisław Kaźmierczak,Jacek Mańdziuk*

Main category: cs.LG

TL;DR: 研究不同bootstrap rate (BR) 对随机森林回归性能的影响，发现BR是重要超参数需调优。


<details>
  <summary>Details</summary>
Motivation: 系统研究不同BR对随机森林性能的影响，探索数据集特征与BR的关系。

Method: 在39个异构回归数据集和16种随机森林配置上，通过重复两次交叉验证和均方误差评估不同BR（0.2 - 5.0）的影响，还在合成数据集上做实验。

Result: 调优BR能显著提升性能，不同数据集有不同最优BR，且发现数据集特征与BR的关联及偏差 - 方差权衡。

Conclusion: BR是影响随机森林回归模型性能的重要超参数，应进行调优。

Abstract: Random Forests (RFs) typically train each tree on a bootstrap sample of the same size as the training set, i.e., bootstrap rate (BR) equals 1.0. We systematically examine how varying BR from 0.2 to 5.0 affects RF performance across 39 heterogeneous regression datasets and 16 RF configurations, evaluating with repeated two-fold cross-validation and mean squared error. Our results demonstrate that tuning the BR can yield significant improvements over the default: the best setup relied on BR \leq 1.0 for 24 datasets, BR > 1.0 for 15, and BR = 1.0 was optimal in 4 cases only. We establish a link between dataset characteristics and the preferred BR: datasets with strong global feature-target relationships favor higher BRs, while those with higher local target variance benefit from lower BRs. To further investigate this relationship, we conducted experiments on synthetic datasets with controlled noise levels. These experiments reproduce the observed bias-variance trade-off: in low-noise scenarios, higher BRs effectively reduce model bias, whereas in high-noise settings, lower BRs help reduce model variance. Overall, BR is an influential hyperparameter that should be tuned to optimize RF regression models.

</details>


### [106] [Efficient reconstruction of multidimensional random field models with heterogeneous data using stochastic neural networks](https://arxiv.org/abs/2511.13977)
*Mingtao Xia,Qijing Shen*

Main category: cs.LG

TL;DR: 分析Wasserstein距离方法训练随机神经网络重建多维随机场模型的可扩展性，证明泛化误差界，改进方法并验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 研究Wasserstein距离方法训练随机神经网络重建多维随机场模型的可扩展性，缓解“维度诅咒”问题。

Method: 证明泛化误差界，改进Wasserstein距离SNN训练方法，并进行数值实验。

Result: 当噪声跨维度异质时，泛化误差收敛率可能不明确依赖模型维度；改进方法展示了SNN的鲁棒性；方法能成功训练SNN学习多维不确定性模型。

Conclusion: Wasserstein距离方法可有效训练随机神经网络学习多维不确定性模型。

Abstract: In this paper, we analyze the scalability of a recent Wasserstein-distance approach for training stochastic neural networks (SNNs) to reconstruct multidimensional random field models. We prove a generalization error bound for reconstructing multidimensional random field models on training stochastic neural networks with a limited number of training data. Our results indicate that when noise is heterogeneous across dimensions, the convergence rate of the generalization error may not depend explicitly on the model's dimensionality, partially alleviating the "curse of dimensionality" for learning multidimensional random field models from a finite number of data points. Additionally, we improve the previous Wasserstein-distance SNN training approach and showcase the robustness of the SNN. Through numerical experiments on different multidimensional uncertainty quantification tasks, we show that our Wasserstein-distance approach can successfully train stochastic neural networks to learn multidimensional uncertainty models.

</details>


### [107] [Data Whitening Improves Sparse Autoencoder Learning](https://arxiv.org/abs/2511.13981)
*Ashwin Saraswatula,David Klindt*

Main category: cs.LG

TL;DR: 本文指出对输入激活应用PCA白化可提升稀疏自编码器（SAE）性能，尽管重建质量略有下降，但能提高可解释性指标。


<details>
  <summary>Details</summary>
Motivation: SAE训练的优化空间因输入数据的相关性而具有挑战性，需要找到提升SAE性能的方法。

Method: 对输入激活应用PCA白化，进行理论分析和模拟，在不同模型架构、宽度和稀疏机制下评估ReLU和Top - K SAE，并在SAEBench上进行实证评估。

Result: 白化使优化空间更凸且更易探索，能持续提升可解释性指标，如稀疏探测准确率和特征解缠度，但重建质量有轻微下降。

Conclusion: 挑战了可解释性与最优稀疏 - 保真权衡一致的假设，建议在SAE训练中，尤其是注重可解释性而非完美重建时，将白化作为默认预处理步骤。

Abstract: Sparse autoencoders (SAEs) have emerged as a promising approach for learning interpretable features from neural network activations. However, the optimization landscape for SAE training can be challenging due to correlations in the input data. We demonstrate that applying PCA Whitening to input activations -- a standard preprocessing technique in classical sparse coding -- improves SAE performance across multiple metrics. Through theoretical analysis and simulation, we show that whitening transforms the optimization landscape, making it more convex and easier to navigate. We evaluate both ReLU and Top-K SAEs across diverse model architectures, widths, and sparsity regimes. Empirical evaluation on SAEBench, a comprehensive benchmark for sparse autoencoders, reveals that whitening consistently improves interpretability metrics, including sparse probing accuracy and feature disentanglement, despite minor drops in reconstruction quality. Our results challenge the assumption that interpretability aligns with an optimal sparsity--fidelity trade-off and suggest that whitening should be considered as a default preprocessing step for SAE training, particularly when interpretability is prioritized over perfect reconstruction.

</details>


### [108] [Node-Level Uncertainty Estimation in LLM-Generated SQL](https://arxiv.org/abs/2511.13984)
*Hilaf Hasson,Ruocheng Guo*

Main category: cs.LG

TL;DR: 提出通过估计查询抽象语法树节点不确定性检测大语言模型生成SQL错误的框架，性能优且支持多应用。


<details>
  <summary>Details</summary>
Motivation: 解决检测大语言模型生成SQL错误的问题。

Method: 分两阶段，先引入语义感知标记算法分配节点正确性，再用丰富特征训练分类器预测节点错误概率。

Result: 在多数据库和数据集上显著优于标记对数概率，平均AUC提高27.44%，跨数据库评估稳健。

Conclusion: 以节点为中心、基于语义的不确定性估计是聚合序列级置信度度量的有力可解释替代方案。

Abstract: We present a practical framework for detecting errors in LLM-generated SQL by estimating uncertainty at the level of individual nodes in the query's abstract syntax tree (AST). Our approach proceeds in two stages. First, we introduce a semantically aware labeling algorithm that, given a generated SQL and a gold reference, assigns node-level correctness without over-penalizing structural containers or alias variation. Second, we represent each node with a rich set of schema-aware and lexical features - capturing identifier validity, alias resolution, type compatibility, ambiguity in scope, and typo signals - and train a supervised classifier to predict per-node error probabilities. We interpret these probabilities as calibrated uncertainty, enabling fine-grained diagnostics that pinpoint exactly where a query is likely to be wrong. Across multiple databases and datasets, our method substantially outperforms token log-probabilities: average AUC improves by +27.44% while maintaining robustness under cross-database evaluation. Beyond serving as an accuracy signal, node-level uncertainty supports targeted repair, human-in-the-loop review, and downstream selective execution. Together, these results establish node-centric, semantically grounded uncertainty estimation as a strong and interpretable alternative to aggregate sequence level confidence measures.

</details>


### [109] [How to Marginalize in Causal Structure Learning?](https://arxiv.org/abs/2511.14001)
*William Zhao,Guy Van den Broeck,Benjie Wang*

Main category: cs.LG

TL;DR: 提出用概率电路改进贝叶斯网络结构学习方法并证明其效果。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯网络从数据推断图结构有挑战，传统动态规划方法有节点父节点集限制。

Method: 利用可处理的概率电路，在原分布和边缘查询上训练电路。

Result: 经验表明该方法使贝叶斯结构学习器性能优于当前方法。

Conclusion: 利用概率电路的方法可改善贝叶斯网络结构学习性能。

Abstract: Bayesian networks (BNs) are a widely used class of probabilistic graphical models employed in numerous application domains. However, inferring the network's graphical structure from data remains challenging. Bayesian structure learners approach this problem by inferring a posterior distribution over the possible directed acyclic graphs underlying the BN. The inference process often requires marginalizing over probability distributions, which is typically done using dynamic programming methods that restrict the set of possible parents for each node. Instead, we present a novel method that utilizes tractable probabilistic circuits to circumvent this restriction. This method utilizes a new learning routine that trains these circuits on both the original distribution and marginal queries. The architecture of probabilistic circuits then inherently allows for fast and exact marginalization on the learned distribution. We then show empirically that utilizing our method to answer marginals allows Bayesian structure learners to improve their performance compared to current methods.

</details>


### [110] [Certified but Fooled! Breaking Certified Defences with Ghost Certificates](https://arxiv.org/abs/2511.14003)
*Quoc Viet Vo,Tashreque M. Haq,Paul Montague,Tamas Abraham,Ehsan Abbasnejad,Damith C. Ranasinghe*

Main category: cs.LG

TL;DR: 研究概率认证框架的恶意利用，探索制作不可察觉扰动绕过认证防御，强调理解鲁棒性认证方法局限性的必要性。


<details>
  <summary>Details</summary>
Motivation: 为更好理解鲁棒性保证的局限性，研究概率认证框架的恶意利用，实现对认证过程的操纵。

Method: 探索区域聚焦的对抗样本，制作不可察觉的扰动以欺骗证书。

Result: 通过ImageNet广泛评估，能有效绕过如Densepure等先进认证防御。

Conclusion: 强调需更好理解鲁棒性认证方法的局限性。

Abstract: Certified defenses promise provable robustness guarantees. We study the malicious exploitation of probabilistic certification frameworks to better understand the limits of guarantee provisions. Now, the objective is to not only mislead a classifier, but also manipulate the certification process to generate a robustness guarantee for an adversarial input certificate spoofing. A recent study in ICLR demonstrated that crafting large perturbations can shift inputs far into regions capable of generating a certificate for an incorrect class. Our study investigates if perturbations needed to cause a misclassification and yet coax a certified model into issuing a deceptive, large robustness radius for a target class can still be made small and imperceptible. We explore the idea of region-focused adversarial examples to craft imperceptible perturbations, spoof certificates and achieve certification radii larger than the source class ghost certificates. Extensive evaluations with the ImageNet demonstrate the ability to effectively bypass state-of-the-art certified defenses such as Densepure. Our work underscores the need to better understand the limits of robustness certification methods.

</details>


### [111] [From Narrow Unlearning to Emergent Misalignment: Causes, Consequences, and Containment in LLMs](https://arxiv.org/abs/2511.14017)
*Erum Mushtaq,Anil Ramakrishna,Satyapriya Krishna,Sattvik Sahai,Prasoon Goyal,Kai-Wei Chang,Tao Zhang,Rahul Gupta*

Main category: cs.LG

TL;DR: 研究表明特定领域的拒绝去学习会引发紧急失准（EMA）现象，还提出用交叉熵损失函数恢复对齐，并分析了EMA的潜在原因。


<details>
  <summary>Details</summary>
Motivation: 深入理解引发紧急失准现象的算法、任务和数据集，研究特定领域拒绝去学习对EMA的影响。

Method: 对网络安全和安全概念进行拒绝去学习，通过监测七个负责任AI领域的拒绝分数评估EMA，用交叉熵损失函数在保留数据上操作，通过概念向量分析概念纠缠。

Result: 窄域去学习能让目标概念有合规响应，但会将EMA传播到无关领域；安全概念的EMA影响更大；用交叉熵损失函数能在受影响领域恢复对齐；概念表示相似度高的在干预后更易受EMA影响。

Conclusion: 特定领域的拒绝去学习会引发EMA并传播到无关领域，交叉熵损失函数可恢复对齐，概念表示相似度与EMA易感性有关。

Abstract: Recent work has shown that fine-tuning on insecure code data can trigger an emergent misalignment (EMA) phenomenon, where models generate malicious responses even to prompts unrelated to the original insecure code-writing task. Such cross-domain generalization of harmful behavior underscores the need for a deeper understanding of the algorithms, tasks, and datasets that induce emergent misalignment. In this work, we extend this study by demonstrating that emergent misalignment can also arise from narrow refusal unlearning in specific domains. We perform refusal unlearning on Cybersecurity and Safety concept, and evaluate EMA by monitoring refusal scores across seven responsible AI (RAI) domains, Cybersecurity, Safety, Toxicity, Bias, Sensitive Content, Medical/Legal, and Privacy. Our work shows that narrow domain unlearning can yield compliance responses for the targeted concept, however, it may also propagate EMA to unrelated domains. Among the two intervened concepts, Cybersecurity and Safety, we find that the safety concept can have larger EMA impact, i.e, causing lower refusal scores, across other unrelated domains such as bias. We observe this effect consistently across two model families, Mistral-7b-0.3v, and Qwen-7b-2.5. Further, we show that refusal unlearning augmented with cross-entropy loss function on a small set of retain data from the affected domains can largely, if not fully, restore alignment across the impacted domains while having lower refusal rate on the concept we perform unlearning on. To investigate the underlying causes of EMA, we analyze concept entanglements at the representation level via concept vectors. Our analysis reveals that concepts with higher representation similarity in earlier layers are more susceptible to EMA after intervention when the refusal stream is altered through targeted refusal unlearning.

</details>


### [112] [A Machine Learning-Based Multimodal Framework for Wearable Sensor-Based Archery Action Recognition and Stress Estimation](https://arxiv.org/abs/2511.14057)
*Xianghe Liu,Jiajia Liu,Chuxian Xu,Minghan Wang,Hongbo Peng,Tao Sun,Jiaqi Xu*

Main category: cs.LG

TL;DR: 提出基于机器学习的多模态框架，集成可穿戴传感器数据进行射箭动作识别和压力估计，取得较好结果，为训练优化提供基础。


<details>
  <summary>Details</summary>
Motivation: 传统运动分析系统昂贵且具侵入性，限制在自然训练环境使用，需新方法分析运动员状态。

Method: 使用自制腕戴设备收集运动和生理数据，用Smoothed Differential Acceleration特征和LSTM模型进行动作识别，提取HRV特征用MLP分类器进行压力估计。

Result: 动作识别准确率96.8%，F1分数95.9%；压力估计区分高低压力水平准确率80%。

Conclusion: 集成运动和生理传感能洞察运动员技术和心理状态，为射箭等精准运动训练优化提供基础。

Abstract: In precision sports such as archery, athletes' performance depends on both biomechanical stability and psychological resilience. Traditional motion analysis systems are often expensive and intrusive, limiting their use in natural training environments. To address this limitation, we propose a machine learning-based multimodal framework that integrates wearable sensor data for simultaneous action recognition and stress estimation. Using a self-developed wrist-worn device equipped with an accelerometer and photoplethysmography (PPG) sensor, we collected synchronized motion and physiological data during real archery sessions. For motion recognition, we introduce a novel feature--Smoothed Differential Acceleration (SmoothDiff)--and employ a Long Short-Term Memory (LSTM) model to identify motion phases, achieving 96.8% accuracy and 95.9% F1-score. For stress estimation, we extract heart rate variability (HRV) features from PPG signals and apply a Multi-Layer Perceptron (MLP) classifier, achieving 80% accuracy in distinguishing high- and low-stress levels. The proposed framework demonstrates that integrating motion and physiological sensing can provide meaningful insights into athletes' technical and mental states. This approach offers a foundation for developing intelligent, real-time feedback systems for training optimization in archery and other precision sports.

</details>


### [113] [CafeMed: Causal Attention Fusion Enhanced Medication Recommendation](https://arxiv.org/abs/2511.14064)
*Kelin Ren,Chan-Yang Ju,Dong-Ho Lee*

Main category: cs.LG

TL;DR: 提出CafeMed框架用于药物推荐，解决现有方法局限，实验表明其优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有药物推荐方法存在未建模医疗实体协同效应、使用静态因果关系不适应患者特定情况的问题，需改进。

Method: 提出CafeMed框架，包含Causal Weight Generator和Channel Harmonized Attention Refinement Module两个关键组件。

Result: 在MIMIC - III和MIMIC - IV数据集上，CafeMed显著优于现有基线，药物预测准确率高且药物相互作用率低。

Conclusion: 纳入动态因果关系和跨模态协同作用可实现更符合临床和个性化的药物推荐。

Abstract: Medication recommendation systems play a crucial role in assisting clinicians with personalized treatment decisions. While existing approaches have made significant progress in learning medication representations, they suffer from two fundamental limitations: (i) treating medical entities as independent features without modeling their synergistic effects on medication selection; (ii) employing static causal relationships that fail to adapt to patient-specific contexts and health states. To address these challenges, we propose CafeMed, a framework that integrates dynamic causal reasoning with cross-modal attention for safe and accurate medication recommendation. CafeMed introduces two key components: the Causal Weight Generator (CWG) that transforms static causal effects into dynamic modulation weights based on individual patient states, and the Channel Harmonized Attention Refinement Module (CHARM) that captures complex interdependencies between diagnoses and procedures. This design enables CafeMed to model how different medical conditions jointly influence treatment decisions while maintaining medication safety constraints. Extensive experiments on MIMIC-III and MIMIC-IV datasets demonstrate that CafeMed significantly outperforms state-of-the-art baselines, achieving superior accuracy in medication prediction while maintaining the lower drug--drug interaction rates. Our results indicate that incorporating dynamic causal relationships and cross-modal synergies leads to more clinically-aligned and personalized medication recommendations. Our code is released publicly at https://github.com/rkl71/CafeMed.

</details>


### [114] [CFG-EC: Error Correction Classifier-Free Guidance](https://arxiv.org/abs/2511.14075)
*Nakkyu Yang,Yechan Lee,SooJean Han*

Main category: cs.LG

TL;DR: 提出CFG-EC校正方案，改进无条件噪声预测，减少训练和采样过程噪声估计误差，实验显示性能提升。


<details>
  <summary>Details</summary>
Motivation: CFG在训练和采样过程中噪声估计不一致，为减少该误差提出改进方法。

Method: 提出CFG-EC校正方案，使无条件噪声误差分量与条件误差分量正交。

Result: 数值实验表明CFG-EC比CFG和CFG++更有效处理无条件分量，在低引导采样机制中性能显著提升，整体提示对齐度更高。

Conclusion: CFG-EC能减少误差，建立更可靠引导轨迹，实现高保真图像生成。

Abstract: Classifier-Free Guidance (CFG) has become a mainstream approach for simultaneously improving prompt fidelity and generation quality in conditional generative models. During training, CFG stochastically alternates between conditional and null prompts to enable both conditional and unconditional generation. However, during sampling, CFG outputs both null and conditional prompts simultaneously, leading to inconsistent noise estimates between the training and sampling processes. To reduce this error, we propose CFG-EC, a versatile correction scheme augmentable to any CFG-based method by refining the unconditional noise predictions. CFG-EC actively realigns the unconditional noise error component to be orthogonal to the conditional error component. This corrective maneuver prevents interference between the two guidance components, thereby constraining the sampling error's upper bound and establishing more reliable guidance trajectories for high-fidelity image generation. Our numerical experiments show that CFG-EC handles the unconditional component more effectively than CFG and CFG++, delivering a marked performance increase in the low guidance sampling regime and consistently higher prompt alignment across the board.

</details>


### [115] [Meta-SimGNN: Adaptive and Robust WiFi Localization Across Dynamic Configurations and Diverse Scenarios](https://arxiv.org/abs/2511.14076)
*Qiqi Xiao,Ziqi Ye,Yinghui He,Jianwei Liu,Guanding Yu*

Main category: cs.LG

TL;DR: 提出Meta - SimGNN系统，结合图神经网络与元学习提升WiFi定位泛化性和鲁棒性，实验显示其性能优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于元学习解决深度学习定位场景依赖问题的研究忽略设备配置变化影响，设备配置变化会影响CSI维度并损害神经网络可用性。

Method: 提出细粒度CSI图构建方案，包括振幅 - 相位融合和特征提取方法；开发相似性引导的元学习策略，通过比较新场景与历史场景相似度确定微调阶段初始模型参数。

Result: 在不同场景的商用WiFi设备上的大量实验表明，Meta - SimGNN在定位泛化性和准确性上优于基线方法。

Conclusion: Meta - SimGNN能有效提升WiFi定位的泛化性和鲁棒性。

Abstract: To promote the practicality of deep learning-based localization, existing studies aim to address the issue of scenario dependence through meta-learning. However, these studies primarily focus on variations in environmental layouts while overlooking the impact of changes in device configurations, such as bandwidth, the number of access points (APs), and the number of antennas used. Unlike environmental changes, variations in device configurations affect the dimensionality of channel state information (CSI), thereby compromising neural network usability. To address this issue, we propose Meta-SimGNN, a novel WiFi localization system that integrates graph neural networks with meta-learning to improve localization generalization and robustness. First, we introduce a fine-grained CSI graph construction scheme, where each AP is treated as a graph node, allowing for adaptability to changes in the number of APs. To structure the features of each node, we propose an amplitude-phase fusion method and a feature extraction method. The former utilizes both amplitude and phase to construct CSI images, enhancing data reliability, while the latter extracts dimension-consistent features to address variations in bandwidth and the number of antennas. Second, a similarity-guided meta-learning strategy is developed to enhance adaptability in diverse scenarios. The initial model parameters for the fine-tuning stage are determined by comparing the similarity between the new scenario and historical scenarios, facilitating rapid adaptation of the model to the new localization scenario. Extensive experimental results over commodity WiFi devices in different scenarios show that Meta-SimGNN outperforms the baseline methods in terms of localization generalization and accuracy.

</details>


### [116] [Observational Auditing of Label Privacy](https://arxiv.org/abs/2511.14084)
*Iden Kalemaj,Luca Melis,Maxime Boucher,Ilya Mironov,Saeed Mahloujifar*

Main category: cs.LG

TL;DR: 提出一种新型观察性审计框架，无需修改数据集即可评估隐私，拓展审计范围，实验证明有效，为大规模生产环境隐私审计开辟新途径。


<details>
  <summary>Details</summary>
Motivation: 现有差分隐私审计方法需修改训练数据集，对大规模系统存在资源和工程开销大的问题。

Method: 引入利用数据分布固有随机性的观察性审计框架，将隐私审计范围拓展到受保护属性。

Result: 在Criteo和CIFAR - 10数据集上实验证明该方法在审计标签隐私保证方面有效。

Conclusion: 该工作为大规模生产环境下的实际隐私审计开辟了新途径。

Abstract: Differential privacy (DP) auditing is essential for evaluating privacy guarantees in machine learning systems. Existing auditing methods, however, pose a significant challenge for large-scale systems since they require modifying the training dataset -- for instance, by injecting out-of-distribution canaries or removing samples from training. Such interventions on the training data pipeline are resource-intensive and involve considerable engineering overhead. We introduce a novel observational auditing framework that leverages the inherent randomness of data distributions, enabling privacy evaluation without altering the original dataset. Our approach extends privacy auditing beyond traditional membership inference to protected attributes, with labels as a special case, addressing a key gap in existing techniques. We provide theoretical foundations for our method and perform experiments on Criteo and CIFAR-10 datasets that demonstrate its effectiveness in auditing label privacy guarantees. This work opens new avenues for practical privacy auditing in large-scale production environments.

</details>


### [117] [Soft-Label Training Preserves Epistemic Uncertainty](https://arxiv.org/abs/2511.14117)
*Agamdeep Singh,Ashish Tiwari,Hosein Hasanbeig,Priyanshu Gupta*

Main category: cs.LG

TL;DR: 传统单标签训练在处理模糊数据时存在问题，软标签训练能保留认知不确定性，在多任务中表现良好。


<details>
  <summary>Details</summary>
Motivation: 指出标准做法将标注分布合并为单标签，在处理模糊数据时存在认知上的不一致，需更好方法。

Method: 采用软标签训练，将标注分布视为真实标签。

Result: 在视觉和NLP任务中，软标签训练与人类标注的KL散度降低32%，模型与标注熵的相关性增强61%，且准确率与硬标签训练相当。

Conclusion: 应将标注分布视为认知不确定性的忠实表示，让模型学习重现。

Abstract: Many machine learning tasks involve inherent subjectivity, where annotators naturally provide varied labels. Standard practice collapses these label distributions into single labels, aggregating diverse human judgments into point estimates. We argue that this approach is epistemically misaligned for ambiguous data--the annotation distribution itself should be regarded as the ground truth. Training on collapsed single labels forces models to express false confidence on fundamentally ambiguous cases, creating a misalignment between model certainty and the diversity of human perception. We demonstrate empirically that soft-label training, which treats annotation distributions as ground truth, preserves epistemic uncertainty. Across both vision and NLP tasks, soft-label training achieves 32% lower KL divergence from human annotations and 61% stronger correlation between model and annotation entropy, while matching the accuracy of hard-label training. Our work repositions annotation distributions from noisy signals to be aggregated away, to faithful representations of epistemic uncertainty that models should learn to reproduce.

</details>


### [118] [A Comprehensive Study of Implicit and Explicit Biases in Large Language Models](https://arxiv.org/abs/2511.14153)
*Fatima Kazi,Alex Young,Yash Inani,Setareh Rafatirad*

Main category: cs.LG

TL;DR: 研究评估大语言模型（LLMs）中的各种偏差，提出自动偏差识别框架，结果显示微调模型在处理性别偏差上有困难但在种族偏差上表现较好，通过策略提升了模型在隐式偏差基准上的性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型会从训练数据中继承偏差，为确保公平输出，需要识别和缓解其中的偏差。

Method: 研究特定偏差基准评估多种生成模型中的偏差，提出自动偏差识别框架，采用双管齐下的方法检测显式和隐式偏差，运用词袋分析，使用提示技术微调模型和对偏差基准进行数据增强。

Result: 微调模型在性别偏差上有困难，在种族偏差识别和避免上表现出色；模型常过度依赖关键词；词袋分析揭示词汇中有隐式刻板印象；微调模型在跨数据集测试中适应性好，在隐式偏差基准上性能提升达20%。

Conclusion: 提出的方法和策略有助于提升大语言模型检测和处理偏差的能力。

Abstract: Large Language Models (LLMs) inherit explicit and implicit biases from their training datasets. Identifying and mitigating biases in LLMs is crucial to ensure fair outputs, as they can perpetuate harmful stereotypes and misinformation. This study highlights the need to address biases in LLMs amid growing generative AI. We studied bias-specific benchmarks such as StereoSet and CrowSPairs to evaluate the existence of various biases in multiple generative models such as BERT and GPT 3.5. We proposed an automated Bias-Identification Framework to recognize various social biases in LLMs such as gender, race, profession, and religion. We adopted a two-pronged approach to detect explicit and implicit biases in text data. Results indicated fine-tuned models struggle with gender biases but excelled at identifying and avoiding racial biases. Our findings illustrated that despite having some success, LLMs often over-relied on keywords. To illuminate the capability of the analyzed LLMs in detecting implicit biases, we employed Bag-of-Words analysis and unveiled indications of implicit stereotyping within the vocabulary. To bolster the model performance, we applied an enhancement strategy involving fine-tuning models using prompting techniques and data augmentation of the bias benchmarks. The fine-tuned models exhibited promising adaptability during cross-dataset testing and significantly enhanced performance on implicit bias benchmarks, with performance gains of up to 20%.

</details>


### [119] [Certified Signed Graph Unlearning](https://arxiv.org/abs/2511.14168)
*Junpeng Zhao,Lin Li,Kaixi Hu,Kaize Shi,Jingling Yuan*

Main category: cs.LG

TL;DR: 提出Certified Signed Graph Unlearning (CSGU)方法用于有符号图去学习，兼顾隐私保护和模型效用，实验显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有图去学习方法用于有符号图神经网络（SGNNs）时会丢失关键符号信息，降低模型效用和去学习效果，需要新方法。

Method: CSGU采用三阶段方法：通过三角结构识别最小影响邻域；应用社会学理论量化节点重要性以分配隐私预算；进行重要性加权参数更新以实现认证修改。

Result: 实验表明CSGU在SGNNs的效用保留和去学习效果上优于现有方法。

Conclusion: CSGU能在有符号图去学习中提供可证明的隐私保证，同时保留SGNNs的社会学原理，有效解决现有方法的问题。

Abstract: Signed graphs model complex relationships through positive and negative edges, with widespread real-world applications. Given the sensitive nature of such data, selective removal mechanisms have become essential for privacy protection. While graph unlearning enables the removal of specific data influences from Graph Neural Networks (GNNs), existing methods are designed for conventional GNNs and overlook the unique heterogeneous properties of signed graphs. When applied to Signed Graph Neural Networks (SGNNs), these methods lose critical sign information, degrading both model utility and unlearning effectiveness. To address these challenges, we propose Certified Signed Graph Unlearning (CSGU), which provides provable privacy guarantees while preserving the sociological principles underlying SGNNs. CSGU employs a three-stage method: (1) efficiently identifying minimal influenced neighborhoods via triangular structures, (2) applying sociological theories to quantify node importance for optimal privacy budget allocation, and (3) performing importance-weighted parameter updates to achieve certified modifications with minimal utility degradation. Extensive experiments demonstrate that CSGU outperforms existing methods, achieving superior performance in both utility preservation and unlearning effectiveness on SGNNs.

</details>


### [120] [N-GLARE: An Non-Generative Latent Representation-Efficient LLM Safety Evaluator](https://arxiv.org/abs/2511.14195)
*Zheyu Lin,Jirui Yang,Hengqi Guo,Yubing Bao,Yao Guan*

Main category: cs.LG

TL;DR: 本文提出N - GLARE评估器用于大语言模型安全鲁棒性评估，能高效替代传统红队方法。


<details>
  <summary>Details</summary>
Motivation: 主流红队方法成本高、有反馈延迟，不适合新模型训练后的快速诊断，需新评估方法。

Method: 提出N - GLARE，在模型潜在表示上操作，分析潜在表示的APT并引入JSS指标。

Result: 在40多个模型和20种红队策略实验中，JSS指标与红队安全排名高度一致，N - GLARE以不到1%的成本复现大规模红队测试判别趋势。

Conclusion: N - GLARE可作为高效无输出评估代理用于实时诊断。

Abstract: Evaluating the safety robustness of LLMs is critical for their deployment. However, mainstream Red Teaming methods rely on online generation and black-box output analysis. These approaches are not only costly but also suffer from feedback latency, making them unsuitable for agile diagnostics after training a new model. To address this, we propose N-GLARE (A Non-Generative, Latent Representation-Efficient LLM Safety Evaluator). N-GLARE operates entirely on the model's latent representations, bypassing the need for full text generation. It characterizes hidden layer dynamics by analyzing the APT (Angular-Probabilistic Trajectory) of latent representations and introducing the JSS (Jensen-Shannon Separability) metric. Experiments on over 40 models and 20 red teaming strategies demonstrate that the JSS metric exhibits high consistency with the safety rankings derived from Red Teaming. N-GLARE reproduces the discriminative trends of large-scale red-teaming tests at less than 1\% of the token cost and the runtime cost, providing an efficient output-free evaluation proxy for real-time diagnostics.

</details>


### [121] [Bridging the Gap Between Bayesian Deep Learning and Ensemble Weather Forecasts](https://arxiv.org/abs/2511.14218)
*Xinlei Xiong,Wenbo Hu,Shuxun Zhou,Kaifeng Bi,Lingxi Xie,Ying Liu,Richang Hong,Qi Tian*

Main category: cs.LG

TL;DR: 本文提出统一混合贝叶斯深度学习框架用于集合天气预报，连接传统集合预测和贝叶斯深度学习，在ERA5数据集验证，结果显示有更好准确性、不确定性量化和计算效率。


<details>
  <summary>Details</summary>
Motivation: 天气预测因大气混沌特性需概率方法量化不确定性，传统集合预测计算密集，贝叶斯深度学习有潜力但常与前者脱节，需连接二者。

Method: 提出统一混合贝叶斯深度学习框架，将预测不确定性分解为认知和随机分量，分别通过变分推理和物理信息随机扰动方案学习，建立统一理论框架连接BDL和EPS。

Result: 在ERA5数据集验证，方法提高预报准确性，有更好校准的不确定性量化，计算效率优于现有概率扩散模型。

Conclusion: 所提框架有效，能结合传统集合预测和贝叶斯深度学习优势，有更好性能和效率。

Abstract: Weather forecasting is fundamentally challenged by the chaotic nature of the atmosphere, necessitating probabilistic approaches to quantify uncertainty. While traditional ensemble prediction (EPS) addresses this through computationally intensive simulations, recent advances in Bayesian Deep Learning (BDL) offer a promising but often disconnected alternative. We bridge these paradigms through a unified hybrid Bayesian Deep Learning framework for ensemble weather forecasting that explicitly decomposes predictive uncertainty into epistemic and aleatoric components, learned via variational inference and a physics-informed stochastic perturbation scheme modeling flow-dependent atmospheric dynamics, respectively. We further establish a unified theoretical framework that rigorously connects BDL and EPS, providing formal theorems that decompose total predictive uncertainty into epistemic and aleatoric components under the hybrid BDL framework. We validate our framework on the large-scale 40-year ERA5 reanalysis dataset (1979-2019) with 0.25° spatial resolution. Experimental results show that our method not only improves forecast accuracy and yields better-calibrated uncertainty quantification but also achieves superior computational efficiency compared to state-of-the-art probabilistic diffusion models. We commit to making our code open-source upon acceptance of this paper.

</details>


### [122] [Parallelizing Tree Search with Twice Sequential Monte Carlo](https://arxiv.org/abs/2511.14220)
*Yaniv Oren,Joery A. de Vries,Pascal R. van der Vaart,Matthijs T. J. Spaan,Wendelin Böhmer*

Main category: cs.LG

TL;DR: 提出Twice Sequential Monte Carlo Tree Search (TSMCTS) 解决SMC在基于模型的强化学习中的问题，且在离散和连续环境中表现优于SMC和MCTS。


<details>
  <summary>Details</summary>
Motivation: Sequential Monte Carlo (SMC) 存在方差大、路径退化问题，难以随搜索深度增加扩展，需改进。

Method: 引入Twice Sequential Monte Carlo Tree Search (TSMCTS) 方法。

Result: 在离散和连续环境中，TSMCTS 优于 SMC 基线和流行的现代版 MCTS。

Conclusion: TSMCTS 通过减少方差和缓解路径退化，能随顺序计算扩展，同时保留 SMC 易于并行化的特性。

Abstract: Model-based reinforcement learning (RL) methods that leverage search are responsible for many milestone breakthroughs in RL. Sequential Monte Carlo (SMC) recently emerged as an alternative to the Monte Carlo Tree Search (MCTS) algorithm which drove these breakthroughs. SMC is easier to parallelize and more suitable to GPU acceleration. However, it also suffers from large variance and path degeneracy which prevent it from scaling well with increased search depth, i.e., increased sequential compute. To address these problems, we introduce Twice Sequential Monte Carlo Tree Search (TSMCTS). Across discrete and continuous environments TSMCTS outperforms the SMC baseline as well as a popular modern version of MCTS. Through variance reduction and mitigation of path degeneracy, TSMCTS scales favorably with sequential compute while retaining the properties that make SMC natural to parallelize.

</details>


### [123] [EBind: a practical approach to space binding](https://arxiv.org/abs/2511.14229)
*Jim Broadbent,Felix Cohen,Frederik Hvilshøj,Eric Landau,Eren Sasoglu*

Main category: cs.LG

TL;DR: 本文提出EBind方法简化空间绑定，用小模型超越大模型，借助精心策划的数据集，进行多种评估，还引入新基准并开源相关资源。


<details>
  <summary>Details</summary>
Motivation: 简化空间绑定，用小模型实现更好性能，解决现有基准的局限性。

Method: 提出EBind方法，采用精心策划的包含三种互补数据源的数据集，进行13种不同评估，引入新的零样本分类基准。

Result: 简单的1.8B参数模型能超越4到17倍大小的模型，证明了各数据源的价值。

Conclusion: EBind方法有效，且将代码、模型权重和数据集开源。

Abstract: We simplify space binding by focusing on two core components, a single encoder per modality and high-quality data; enabling training state-of-the-art models on a single GPU in a few hours as opposed to multiple days. We present EBind, an Easy, data-centric, and parameter-efficient method to Bind the embedding spaces of multiple contrastive models. We demonstrate that a simple 1.8B-parameter image-text-video-audio-3D model can outperform models 4 to 17x the size. The key to achieving this is a carefully curated dataset of three complementary data sources: i) 6.7M fully-automated multimodal quintuples sourced via SOTA retrieval models, ii) 1M diverse, semi-automated triples annotated by humans as negative, partial, or positive matches, and iii) 3.4M pre-existing captioned data items. We use 13 different evaluations to demonstrate the value of each data source. Due to limitations with existing benchmarks, we further introduce the first high-quality, consensus-annotated zero-shot classification benchmark between audio and PCs. In contrast to related work, we will open-source our code, model weights, and datasets.

</details>


### [124] [Object-Centric World Models for Causality-Aware Reinforcement Learning](https://arxiv.org/abs/2511.14262)
*Yosuke Nishimoto,Takashi Matsubara*

Main category: cs.LG

TL;DR: 提出STICA框架用于深度强化学习，在目标丰富基准测试中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有世界模型难以准确复制高维、非平稳且包含多对象丰富交互的环境，而人类通过分解环境为离散对象来感知环境，受此启发提出新方法。

Method: 提出STICA统一框架，使用以对象为中心的Transformer作为世界模型和因果感知策略与价值网络，将观察表示为对象中心令牌集，预测令牌级动态和交互，策略和价值网络估计令牌级因果关系进行决策。

Result: 在目标丰富的基准测试中，STICA在样本效率和最终性能上始终优于最先进的智能体。

Conclusion: STICA框架在深度强化学习中有效，能提升样本效率和最终性能。

Abstract: World models have been developed to support sample-efficient deep reinforcement learning agents. However, it remains challenging for world models to accurately replicate environments that are high-dimensional, non-stationary, and composed of multiple objects with rich interactions since most world models learn holistic representations of all environmental components. By contrast, humans perceive the environment by decomposing it into discrete objects, facilitating efficient decision-making. Motivated by this insight, we propose \emph{Slot Transformer Imagination with CAusality-aware reinforcement learning} (STICA), a unified framework in which object-centric Transformers serve as the world model and causality-aware policy and value networks. STICA represents each observation as a set of object-centric tokens, together with tokens for the agent action and the resulting reward, enabling the world model to predict token-level dynamics and interactions. The policy and value networks then estimate token-level cause--effect relations and use them in the attention layers, yielding causality-guided decision-making. Experiments on object-rich benchmarks demonstrate that STICA consistently outperforms state-of-the-art agents in both sample efficiency and final performance.

</details>


### [125] [Algebraformer: A Neural Approach to Linear Systems](https://arxiv.org/abs/2511.14263)
*Pietro Sittoni,Francesco Tudisco*

Main category: cs.LG

TL;DR: 本文提出基于Transformer的Algebraformer模型，可端到端求解病态线性系统，在应用驱动的线性问题上展示了有效性。


<details>
  <summary>Details</summary>
Motivation: 现有求解病态线性系统的数值方法常需仔细调参、预处理或特定领域专业知识，本文旨在解决这一问题。

Method: 提出Algebraformer模型，采用新的编码方案，以实现矩阵和向量输入的高效表示，内存复杂度为$O(n^2)$。

Result: Algebraformer在测试时以显著较低的计算开销实现了有竞争力的准确性。

Conclusion: 通用神经架构可有效降低传统科学计算流程的复杂度。

Abstract: Recent work in deep learning has opened new possibilities for solving classical algorithmic tasks using end-to-end learned models. In this work, we investigate the fundamental task of solving linear systems, particularly those that are ill-conditioned. Existing numerical methods for ill-conditioned systems often require careful parameter tuning, preconditioning, or domain-specific expertise to ensure accuracy and stability. In this work, we propose Algebraformer, a Transformer-based architecture that learns to solve linear systems end-to-end, even in the presence of severe ill-conditioning. Our model leverages a novel encoding scheme that enables efficient representation of matrix and vector inputs, with a memory complexity of $O(n^2)$, supporting scalable inference. We demonstrate its effectiveness on application-driven linear problems, including interpolation tasks from spectral methods for boundary value problems and acceleration of the Newton method. Algebraformer achieves competitive accuracy with significantly lower computational overhead at test time, demonstrating that general-purpose neural architectures can effectively reduce complexity in traditional scientific computing pipelines.

</details>


### [126] [Unified Multimodal Vessel Trajectory Prediction with Explainable Navigation Intention](https://arxiv.org/abs/2511.14265)
*Rui Zhang,Chao Li,Kezhong Liu,Chen Wang,Bolong Zheng,Hongbo Jiang*

Main category: cs.LG

TL;DR: 提出含可解释导航意图的统一多模态轨迹预测框架，在AIS数据集实验中表现良好且提升可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有船舶多模态轨迹预测方法场景适用性有限、可解释性不足。

Method: 将导航意图分为持续和瞬态两类，从历史轨迹构建持续意图树，用条件变分自编码器建模动态瞬态意图，用非局部注意力机制保持全局场景一致性。

Result: 在真实AIS数据集实验中，方法在不同场景有广泛适用性，在ADE和FDE指标上显著提升，且明确揭示预测轨迹的导航意图，提升可解释性。

Conclusion: 所提统一多模态轨迹预测框架有效可行，解决了现有方法的问题。

Abstract: Vessel trajectory prediction is fundamental to intelligent maritime systems. Within this domain, short-term prediction of rapid behavioral changes in complex maritime environments has established multimodal trajectory prediction (MTP) as a promising research area. However, existing vessel MTP methods suffer from limited scenario applicability and insufficient explainability. To address these challenges, we propose a unified MTP framework incorporating explainable navigation intentions, which we classify into sustained and transient categories. Our method constructs sustained intention trees from historical trajectories and models dynamic transient intentions using a Conditional Variational Autoencoder (CVAE), while using a non-local attention mechanism to maintain global scenario consistency. Experiments on real Automatic Identification System (AIS) datasets demonstrates our method's broad applicability across diverse scenarios, achieving significant improvements in both ADE and FDE. Furthermore, our method improves explainability by explicitly revealing the navigational intentions underlying each predicted trajectory.

</details>


### [127] [Comparing Task-Agnostic Embedding Models for Tabular Data](https://arxiv.org/abs/2511.14276)
*Frederik Hoppe,Lars Kleinemeier,Astrid Franz,Udo Göbel*

Main category: cs.LG

TL;DR: 评估表格基础模型任务无关表示，发现简单特征工程表现相当或更优且速度更快。


<details>
  <summary>Details</summary>
Motivation: 现有表格基础模型关注直接预测，此研究聚焦可迁移、任务无关的表示学习。

Method: 系统评估TabPFN、TabICL的任务无关表示及TableVectorizer经典特征工程在异常检测和监督学习任务中的表现。

Result: 简单的TableVectorizer特征表现相当或更优，且速度比表格基础模型快三个数量级。

Conclusion: 简单特征工程在表格数据表示学习中有优势。

Abstract: Recent foundation models for tabular data achieve strong task-specific performance via in-context learning. Nevertheless, they focus on direct prediction by encapsulating both representation learning and task-specific inference inside a single, resource-intensive network. This work specifically focuses on representation learning, i.e., on transferable, task-agnostic embeddings. We systematically evaluate task-agnostic representations from tabular foundation models (TabPFN and TabICL) alongside with classical feature engineering (TableVectorizer) across a variety of application tasks as outlier detection (ADBench) and supervised learning (TabArena Lite). We find that simple TableVectorizer features achieve comparable or superior performance while being up to three orders of magnitude faster than tabular foundation models. The code is available at https://github.com/ContactSoftwareAI/TabEmbedBench.

</details>


### [128] [Weight Variance Amplifier Improves Accuracy in High-Sparsity One-Shot Pruning](https://arxiv.org/abs/2511.14282)
*Vincent-Daniel Yun,Junhyuk Jo,Sunwoo Lee*

Main category: cs.LG

TL;DR: 提出方差放大正则化器VAR提升模型剪枝鲁棒性


<details>
  <summary>Details</summary>
Motivation: 现有方法在减少模型大小同时难以兼顾精度和计算量，需新方法提升剪枝鲁棒性

Method: 提出VAR，在训练中增大模型参数方差

Result: 理论分析VAR收敛行为，大量实验证明VAR剪枝鲁棒性更优

Conclusion: VAR能有效提升模型剪枝鲁棒性，减轻剪枝负面影响

Abstract: Deep neural networks achieve outstanding performance in visual recognition tasks, yet their large number of parameters makes them less practical for real-world applications. Recently, one-shot pruning has emerged as an effective strategy for reducing model size without additional training. However, models trained with standard objective functions often suffer a significant drop in accuracy after aggressive pruning. Some existing pruning-robust optimizers, such as SAM, and CrAM, mitigate this accuracy drop by guiding the model toward flatter regions of the parameter space, but they inevitably incur non-negligible additional computations. We propose a Variance Amplifying Regularizer (VAR) that deliberately increases the variance of model parameters during training. Our study reveals an intriguing finding that parameters with higher variance exhibit greater pruning robustness. VAR exploits this property by promoting such variance in the weight distribution, thereby mitigating the adverse effects of pruning. We further provide a theoretical analysis of its convergence behavior, supported by extensive empirical results demonstrating the superior pruning robustness of VAR.

</details>


### [129] [H-LDM: Hierarchical Latent Diffusion Models for Controllable and Interpretable PCG Synthesis from Clinical Metadata](https://arxiv.org/abs/2511.14312)
*Chenyang Xu,Siming Li,Hao Wang*

Main category: cs.LG

TL;DR: 提出H - LDM模型生成PCG信号解决心血管疾病诊断数据稀缺问题，实验表现出色，能提升诊断模型准确性。


<details>
  <summary>Details</summary>
Motivation: PCG分析对心血管疾病诊断重要，但标记的病理数据稀缺阻碍AI系统能力，需解决数据不足问题。

Method: 引入H - LDM模型，包括多尺度VAE学习生理解纠缠潜在空间、分层文本到生物信号管道利用临床元数据控制、由医学注意力模块引导可解释扩散过程。

Result: 在PhysioNet CirCor数据集实验达SOTA性能，FAD为9.7，属性解纠缠得分92%，临床有效性87.1%，用合成数据增强诊断模型使罕见病分类准确率提高11.3%。

Conclusion: H - LDM为心脏诊断数据增强开辟新方向，用可解释临床见解解决数据稀缺问题。

Abstract: Phonocardiogram (PCG) analysis is vital for cardiovascular disease diagnosis, yet the scarcity of labeled pathological data hinders the capability of AI systems. To bridge this, we introduce H-LDM, a Hierarchical Latent Diffusion Model for generating clinically accurate and controllable PCG signals from structured metadata. Our approach features: (1) a multi-scale VAE that learns a physiologically-disentangled latent space, separating rhythm, heart sounds, and murmurs; (2) a hierarchical text-to-biosignal pipeline that leverages rich clinical metadata for fine-grained control over 17 distinct conditions; and (3) an interpretable diffusion process guided by a novel Medical Attention module. Experiments on the PhysioNet CirCor dataset demonstrate state-of-the-art performance, achieving a Fréchet Audio Distance of 9.7, a 92% attribute disentanglement score, and 87.1% clinical validity confirmed by cardiologists. Augmenting diagnostic models with our synthetic data improves the accuracy of rare disease classification by 11.3\%. H-LDM establishes a new direction for data augmentation in cardiac diagnostics, bridging data scarcity with interpretable clinical insights.

</details>


### [130] [Intervention Efficiency and Perturbation Validation Framework: Capacity-Aware and Robust Clinical Model Selection under the Rashomon Effect](https://arxiv.org/abs/2511.14317)
*Yuwen Zhang,Viet Tran,Paul Weng*

Main category: cs.LG

TL;DR: 临床机器学习中Rashomon效应带来挑战，提出IE和PVF工具评估选择模型，实证显示有助于选更稳健且符合容量约束的模型。


<details>
  <summary>Details</summary>
Motivation: 临床机器学习中Rashomon效应使多模型并存，小、不平衡、有噪声数据集及临床特征问题让传统验证方案不可靠，传统指标未考虑资源和操作优先级，选择模型有不确定性。

Method: 提出干预效率（IE）和扰动验证框架（PVF）两个互补工具，IE量化模型在有限干预时识别可操作真阳性的效率，PVF评估模型在数据扰动下的稳定性。

Result: 在合成和真实医疗数据集上的实证结果表明，使用这些工具便于选择更稳健泛化且符合容量约束的模型。

Conclusion: 这些工具为临床环境中应对Rashomon效应提供了新方向。

Abstract: In clinical machine learning, the coexistence of multiple models with comparable performance -- a manifestation of the Rashomon Effect -- poses fundamental challenges for trustworthy deployment and evaluation. Small, imbalanced, and noisy datasets, coupled with high-dimensional and weakly identified clinical features, amplify this multiplicity and make conventional validation schemes unreliable. As a result, selecting among equally performing models becomes uncertain, particularly when resource constraints and operational priorities are not considered by conventional metrics like F1 score. To address these issues, we propose two complementary tools for robust model assessment and selection: Intervention Efficiency (IE) and the Perturbation Validation Framework (PVF). IE is a capacity-aware metric that quantifies how efficiently a model identifies actionable true positives when only limited interventions are feasible, thereby linking predictive performance with clinical utility. PVF introduces a structured approach to assess the stability of models under data perturbations, identifying models whose performance remains most invariant across noisy or shifted validation sets. Empirical results on synthetic and real-world healthcare datasets show that using these tools facilitates the selection of models that generalize more robustly and align with capacity constraints, offering a new direction for tackling the Rashomon Effect in clinical settings.

</details>


### [131] [Learning with Statistical Equality Constraints](https://arxiv.org/abs/2511.14320)
*Aneesh Barthakur,Luiz F. O. Chamon*

Main category: cs.LG

TL;DR: 随着机器学习应用发展，传统加权组合惩罚项调参处理需求的方法有局限，本文推导等式约束统计学习问题泛化理论并提出实用算法，在多场景展示有效性。


<details>
  <summary>Details</summary>
Motivation: 传统处理机器学习多需求的加权组合惩罚项方法需调参且对等式约束问题效果不佳，现有近似和泛化保证不适用于等式约束问题。

Method: 推导等式约束统计学习问题的泛化理论，基于求解一系列无约束经验学习问题提出实用算法。

Result: 提出的算法在公平学习、插值分类器和边值问题等场景展示了有效性。

Conclusion: 所提方法可有效处理等式约束的统计学习问题，为相关问题提供新的解决途径。

Abstract: As machine learning applications grow increasingly ubiquitous and complex, they face an increasing set of requirements beyond accuracy. The prevalent approach to handle this challenge is to aggregate a weighted combination of requirement violation penalties into the training objective. To be effective, this approach requires careful tuning of these hyperparameters (weights), involving trial-and-error and cross-validation, which becomes ineffective even for a moderate number of requirements. These issues are exacerbated when the requirements involve parities or equalities, as is the case in fairness and boundary value problems. An alternative technique uses constrained optimization to formulate these learning problems. Yet, existing approximation and generalization guarantees do not apply to problems involving equality constraints. In this work, we derive a generalization theory for equality-constrained statistical learning problems, showing that their solutions can be approximated using samples and rich parametrizations. Using these results, we propose a practical algorithm based on solving a sequence of unconstrained, empirical learning problems. We showcase its effectiveness and the new formulations enabled by equality constraints in fair learning, interpolating classifiers, and boundary value problems.

</details>


### [132] [Enforcing hidden physics in physics-informed neural networks](https://arxiv.org/abs/2511.14348)
*Nanxi Chen,Sifan Wang,Rujin Ma,Airong Chen,Chuanjie Cui*

Main category: cs.LG

TL;DR: 本文指出传统物理信息神经网络（PINNs）训练时忽视热力学第二定律隐含的不可逆性问题，提出不可逆正则化策略，在多基准测试中减少预测误差，该框架有广泛适用性。


<details>
  <summary>Details</summary>
Motivation: 传统PINNs训练时忽视热力学第二定律隐含的不可逆性，导致非物理解或训练失败。

Method: 引入简单、通用且鲁棒的不可逆正则化策略，在训练时将隐藏物理定律作为软约束。

Result: 在多种基准测试中，正则化方案将预测误差降低一个数量级以上，只需对现有PINN框架进行最小修改。

Conclusion: 所提出框架广泛适用于一类由偏微分方程控制的物理系统，将对科学机器学习社区产生重大影响。

Abstract: Physics-informed neural networks (PINNs) represent a new paradigm for solving partial differential equations (PDEs) by integrating physical laws into the learning process of neural networks. However, despite their foundational role, the hidden irreversibility implied by the Second Law of Thermodynamics is often neglected during training, leading to unphysical solutions or even training failures in conventional PINNs. In this paper, we identify this critical gap and introduce a simple, generalized, yet robust irreversibility-regularized strategy that enforces hidden physical laws as soft constraints during training. This approach ensures that the learned solutions consistently respect the intrinsic one-way nature of irreversible physical processes. Across a wide range of benchmarks spanning traveling wave propagation, steady combustion, ice melting, corrosion evolution, and crack propagation, we demonstrate that our regularization scheme reduces predictive errors by more than an order of magnitude, while requiring only minimal modification to existing PINN frameworks. We believe that the proposed framework is broadly applicable to a wide class of PDE-governed physical systems and will have significant impact within the scientific machine learning community.

</details>


### [133] [Watch Out for the Lifespan: Evaluating Backdoor Attacks Against Federated Model Adaptation](https://arxiv.org/abs/2511.14406)
*Bastien Vuillod,Pierre-Alain Moellic,Jean-Max Dutertre*

Main category: cs.LG

TL;DR: 本文分析LoRA对联邦学习中后门攻击的影响，发现低LoRA秩时最优注入后门的持久性更长，还指出评估问题并促进更可靠评估。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中基于LoRA的大模型适配面临安全威胁，尤其是后门攻击，需分析LoRA对后门攻击的影响。

Method: 分析LoRA对针对联邦学习中模型适配的先进后门攻击的影响，聚焦后门寿命这一关键特性。

Result: 实验发现，对于最优注入的后门，LoRA秩较低时，攻击后后门的持久性更长。

Conclusion: 指出联邦学习后门攻击的评估问题，有助于开发更稳健公平的评估方法，提高关键联邦学习系统风险评估的可靠性。

Abstract: Large models adaptation through Federated Learning (FL) addresses a wide range of use cases and is enabled by Parameter-Efficient Fine-Tuning techniques such as Low-Rank Adaptation (LoRA). However, this distributed learning paradigm faces several security threats, particularly to its integrity, such as backdoor attacks that aim to inject malicious behavior during the local training steps of certain clients. We present the first analysis of the influence of LoRA on state-of-the-art backdoor attacks targeting model adaptation in FL. Specifically, we focus on backdoor lifespan, a critical characteristic in FL, that can vary depending on the attack scenario and the attacker's ability to effectively inject the backdoor. A key finding in our experiments is that for an optimally injected backdoor, the backdoor persistence after the attack is longer when the LoRA's rank is lower. Importantly, our work highlights evaluation issues of backdoor attacks against FL and contributes to the development of more robust and fair evaluations of backdoor attacks, enhancing the reliability of risk assessments for critical FL systems. Our code is publicly available.

</details>


### [134] [Toward Robust and Harmonious Adaptation for Cross-modal Retrieval](https://arxiv.org/abs/2511.14416)
*Haobin Li,Mouxing Yang,Xi Peng*

Main category: cs.LG

TL;DR: 现有通用到定制的跨模态检索方法存在查询偏移问题，本文提出REST方法应对，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有通用到定制的跨模态检索方法在实际场景中易因查询偏移问题影响性能，需新方法解决。

Method: 提出REST方法，通过细化检索结果设计QS鲁棒目标函数应对在线偏移，用梯度解耦模块应对多样偏移。

Result: 在三个跨模态检索任务的20个基准测试上验证了方法对查询偏移的有效性。

Conclusion: REST方法能有效解决跨模态检索中的查询偏移问题。

Abstract: Recently, the general-to-customized paradigm has emerged as the dominant approach for Cross-Modal Retrieval (CMR), which reconciles the distribution shift problem between the source domain and the target domain. However, existing general-to-customized CMR methods typically assume that the entire target-domain data is available, which is easily violated in real-world scenarios and thus inevitably suffer from the query shift (QS) problem. Specifically, query shift embraces the following two characteristics and thus poses new challenges to CMR. i) Online Shift: real-world queries always arrive in an online manner, rendering it impractical to access the entire query set beforehand for customization approaches; ii) Diverse Shift: even with domain customization, the CMR models struggle to satisfy queries from diverse users or scenarios, leaving an urgent need to accommodate diverse queries. In this paper, we observe that QS would not only undermine the well-structured common space inherited from the source model, but also steer the model toward forgetting the indispensable general knowledge for CMR. Inspired by the observations, we propose a novel method for achieving online and harmonious adaptation against QS, dubbed Robust adaptation with quEry ShifT (REST). To deal with online shift, REST first refines the retrieval results to formulate the query predictions and accordingly designs a QS-robust objective function on these predictions to preserve the well-established common space in an online manner. As for tackling the more challenging diverse shift, REST employs a gradient decoupling module to dexterously manipulate the gradients during the adaptation process, thus preventing the CMR model from forgetting the general knowledge. Extensive experiments on 20 benchmarks across three CMR tasks verify the effectiveness of our method against QS.

</details>


### [135] [FlowRoI A Fast Optical Flow Driven Region of Interest Extraction Framework for High-Throughput Image Compression in Immune Cell Migration Analysis](https://arxiv.org/abs/2511.14419)
*Xiaowei Xu,Justin Sonneck,Hongxiao Wang,Roman Burkard,Hendrik Wohrle,Anton Grabmasier,Matthias Gunzer,Jianxu Chen*

Main category: cs.LG

TL;DR: 提出FlowRoI框架用于免疫细胞迁移研究的高通量图像压缩，高效且图像质量好。


<details>
  <summary>Details</summary>
Motivation: ComplexEye等高通量成像平台数据增长快，存储和传输负担大。

Method: FlowRoI估计连续帧间光流得到RoI掩码，联合JPEG2000编码实现感兴趣区域感知压缩。

Result: 计算效率高，每秒约处理30帧，细胞区域PSNR更高，压缩率比标准JPEG2000高2.0 - 2.2倍。

Conclusion: FlowRoI可用于免疫细胞迁移研究的高通量图像压缩，有临床转化潜力。

Abstract: Autonomous migration is essential for the function of immune cells such as neutrophils and plays a pivotal role in diverse diseases. Recently, we introduced ComplexEye, a multi-lens array microscope comprising 16 independent aberration-corrected glass lenses arranged at the pitch of a 96-well plate, capable of capturing high-resolution movies of migrating cells. This architecture enables high-throughput live-cell video microscopy for migration analysis, supporting routine quantification of autonomous motility with strong potential for clinical translation. However, ComplexEye and similar high-throughput imaging platforms generate data at an exponential rate, imposing substantial burdens on storage and transmission. To address this challenge, we present FlowRoI, a fast optical-flow-based region of interest (RoI) extraction framework designed for high-throughput image compression in immune cell migration studies. FlowRoI estimates optical flow between consecutive frames and derives RoI masks that reliably cover nearly all migrating cells. The raw image and its corresponding RoI mask are then jointly encoded using JPEG2000 to enable RoI-aware compression. FlowRoI operates with high computational efficiency, achieving runtimes comparable to standard JPEG2000 and reaching an average throughput of about 30 frames per second on a modern laptop equipped with an Intel i7-1255U CPU. In terms of image quality, FlowRoI yields higher peak signal-to-noise ratio (PSNR) in cellular regions and achieves 2.0-2.2x higher compression rates at matched PSNR compared to standard JPEG2000.

</details>


### [136] [MiAD: Mirage Atom Diffusion for De Novo Crystal Generation](https://arxiv.org/abs/2511.14426)
*Andrey Okhotin,Maksim Nakhodnov,Nikita Kazeev,Andrey E Ustyuzhanin,Dmitry Vetrov*

Main category: cs.LG

TL;DR: 本文指出扩散模型在晶体材料生成时不能改变原子数量的局限，提出mirage infusion技术，构建MiAD模型，提升了模型质量和S.U.N.率。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在晶体生成过程中大多不能改变原子数量，限制了模型采样轨迹的可变性。

Method: 引入mirage infusion技术，使扩散模型能改变晶体中原子的存在状态；构建Mirage Atom Diffusion (MiAD) 模型。

Result: mirage infusion技术使模型质量提升达2.5倍；MiAD模型在MP - 20数据集上S.U.N.率达8.2%，远超现有先进方法。

Conclusion: mirage infusion技术有效，MiAD模型在晶体生成方面表现出色，为晶体材料搜索提供了新方法。

Abstract: In recent years, diffusion-based models have demonstrated exceptional performance in searching for simultaneously stable, unique, and novel (S.U.N.) crystalline materials. However, most of these models don't have the ability to change the number of atoms in the crystal during the generation process, which limits the variability of model sampling trajectories. In this paper, we demonstrate the severity of this restriction and introduce a simple yet powerful technique, mirage infusion, which enables diffusion models to change the state of the atoms that make up the crystal from existent to non-existent (mirage) and vice versa. We show that this technique improves model quality by up to $\times2.5$ compared to the same model without this modification. The resulting model, Mirage Atom Diffusion (MiAD), is an equivariant joint diffusion model for de novo crystal generation that is capable of altering the number of atoms during the generation process. MiAD achieves an $8.2\%$ S.U.N. rate on the MP-20 dataset, which substantially exceeds existing state-of-the-art approaches. The source code can be found at \href{https://github.com/andrey-okhotin/miad.git}{\texttt{github.com/andrey-okhotin/miad}}.

</details>


### [137] [Hybrid Modeling of Photoplethysmography for Non-invasive Monitoring of Cardiovascular Parameters](https://arxiv.org/abs/2511.14452)
*Emanuele Palumbo,Sorawit Saengkyongam,Maria R. Cervera,Jens Behrmann,Andrew C. Miller,Guillermo Sapiro,Christina Heinze-Deml,Antoine Wehenkel*

Main category: cs.LG

TL;DR: 提出混合方法用PPG信号估计心血管生物标志物，实验显示其能检测关键指标波动且优于监督基线。


<details>
  <summary>Details</summary>
Motivation: 连续心血管监测重要，但关键心脏生物标志物需侵入性测量，PPG作为非侵入替代预测生物标志物有挑战且标注数据稀缺。

Method: 提出混合方法，结合在配对PPG - APW数据上训练的条件变分自编码器和在标记模拟APW段上训练的心脏生物标志物条件密度估计器。

Result: 实验表明该方法能检测心输出量和每搏输出量的波动，在监测这些生物标志物的时间变化方面优于监督基线。

Conclusion: 所提出的混合方法在利用PPG信号估计心血管生物标志物方面有效。

Abstract: Continuous cardiovascular monitoring can play a key role in precision health. However, some fundamental cardiac biomarkers of interest, including stroke volume and cardiac output, require invasive measurements, e.g., arterial pressure waveforms (APW). As a non-invasive alternative, photoplethysmography (PPG) measurements are routinely collected in hospital settings. Unfortunately, the prediction of key cardiac biomarkers from PPG instead of APW remains an open challenge, further complicated by the scarcity of annotated PPG measurements. As a solution, we propose a hybrid approach that uses hemodynamic simulations and unlabeled clinical data to estimate cardiovascular biomarkers directly from PPG signals. Our hybrid model combines a conditional variational autoencoder trained on paired PPG-APW data with a conditional density estimator of cardiac biomarkers trained on labeled simulated APW segments. As a key result, our experiments demonstrate that the proposed approach can detect fluctuations of cardiac output and stroke volume and outperform a supervised baseline in monitoring temporal changes in these biomarkers.

</details>


### [138] [Nonparametric estimation of conditional probability distributions using a generative approach based on conditional push-forward neural networks](https://arxiv.org/abs/2511.14455)
*Nicola Rares Franco,Lorenzo Tedesco*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We introduce conditional push-forward neural networks (CPFN), a generative framework for conditional distribution estimation. Instead of directly modeling the conditional density $f_{Y|X}$, CPFN learns a stochastic map $\varphi=\varphi(x,u)$ such that $\varphi(x,U)$ and $Y|X=x$ follow approximately the same law, with $U$ a suitable random vector of pre-defined latent variables. This enables efficient conditional sampling and straightforward estimation of conditional statistics through Monte Carlo methods. The model is trained via an objective function derived from a Kullback-Leibler formulation, without requiring invertibility or adversarial training. We establish a near-asymptotic consistency result and demonstrate experimentally that CPFN can achieve performance competitive with, or even superior to, state-of-the-art methods, including kernel estimators, tree-based algorithms, and popular deep learning techniques, all while remaining lightweight and easy to train.

</details>


### [139] [nnterp: A Standardized Interface for Mechanistic Interpretability of Transformers](https://arxiv.org/abs/2511.14465)
*Clément Dumas*

Main category: cs.LG

TL;DR: 开发了nnterp，一个围绕NNsight的轻量级包装器，为变压器分析提供统一接口，兼顾正确性和可用性。


<details>
  <summary>Details</summary>
Motivation: 当前变压器内部分析工具存在一致性和标准化不能兼顾的问题，需要开发新工具。

Method: 开发nnterp，通过自动模块重命名和综合验证测试，实现跨架构分析。

Result: nnterp能让研究者编写一次干预代码并部署到50多种模型变体，涵盖16个架构家族，还包含常见解释性方法的实现。

Conclusion: nnterp弥合了机械可解释性工具在正确性和可用性之间的差距。

Abstract: Mechanistic interpretability research requires reliable tools for analyzing transformer internals across diverse architectures. Current approaches face a fundamental tradeoff: custom implementations like TransformerLens ensure consistent interfaces but require coding a manual adaptation for each architecture, introducing numerical mismatch with the original models, while direct HuggingFace access through NNsight preserves exact behavior but lacks standardization across models. To bridge this gap, we develop nnterp, a lightweight wrapper around NNsight that provides a unified interface for transformer analysis while preserving original HuggingFace implementations. Through automatic module renaming and comprehensive validation testing, nnterp enables researchers to write intervention code once and deploy it across 50+ model variants spanning 16 architecture families. The library includes built-in implementations of common interpretability methods (logit lens, patchscope, activation steering) and provides direct access to attention probabilities for models that support it. By packaging validation tests with the library, researchers can verify compatibility with custom models locally. nnterp bridges the gap between correctness and usability in mechanistic interpretability tooling.

</details>


### [140] [Notes on Kernel Methods in Machine Learning](https://arxiv.org/abs/2511.14485)
*Diego Armando Pérez-Rosero,Danna Valentina Salazar-Dubois,Juan Camilo Lugo-Rojas,Andrés Marino Álvarez-Meza,Germán Castellanos-Dominguez*

Main category: cs.LG

TL;DR: 本文介绍机器学习中核方法及其几何基础，涵盖相关理论、经典概念，引入新方法，为高级主题打基础。


<details>
  <summary>Details</summary>
Motivation: 为机器学习中的核方法及其几何基础提供自成体系的介绍。

Method: 从希尔伯特空间构建出发，发展正定核、再生核希尔伯特空间和希尔伯特 - 施密特算子理论，通过希尔伯特空间几何视角重新审视经典概念。

Result: 介绍了核密度估计、分布的核嵌入和最大均值差异（MMD）等内容。

Conclusion: 为高斯过程、核贝叶斯推理等更高级主题提供基础。

Abstract: These notes provide a self-contained introduction to kernel methods and their geometric foundations in machine learning. Starting from the construction of Hilbert spaces, we develop the theory of positive definite kernels, reproducing kernel Hilbert spaces (RKHS), and Hilbert-Schmidt operators, emphasizing their role in statistical estimation and representation of probability measures. Classical concepts such as covariance, regression, and information measures are revisited through the lens of Hilbert space geometry. We also introduce kernel density estimation, kernel embeddings of distributions, and the Maximum Mean Discrepancy (MMD). The exposition is designed to serve as a foundation for more advanced topics, including Gaussian processes, kernel Bayesian inference, and functional analytic approaches to modern machine learning.

</details>


### [141] [Towards Stable and Structured Time Series Generation with Perturbation-Aware Flow Matching](https://arxiv.org/abs/2511.14488)
*Jintao Zhang,Mingyue Cheng,Zirui Liu,Xianquan Wang,Yitong Zhou,Qi Liu*

Main category: cs.LG

TL;DR: 提出PAFM框架解决时间序列生成中局部扰动导致的问题，实验显示其优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有流匹配方法在生成受扰动时间序列时，因使用全局共享参数，无法充分捕捉突变，难以生成结构一致的时间序列。

Method: 引入PAFM框架，采用扰动引导训练模拟局部干扰，利用双路径速度场捕捉轨迹偏差，使用带流路由的混合专家解码器动态分配建模能力。

Result: 在无条件和有条件生成任务的大量实验中，PAFM始终优于强大的基线模型。

Conclusion: PAFM能有效解决时间序列生成中局部扰动带来的问题，提高生成的稳定性和结构一致性。

Abstract: Time series generation is critical for a wide range of applications, which greatly supports downstream analytical and decision-making tasks. However, the inherent temporal heterogeneous induced by localized perturbations present significant challenges for generating structurally consistent time series. While flow matching provides a promising paradigm by modeling temporal dynamics through trajectory-level supervision, it fails to adequately capture abrupt transitions in perturbed time series, as the use of globally shared parameters constrains the velocity field to a unified representation. To address these limitations, we introduce \textbf{PAFM}, a \textbf{P}erturbation-\textbf{A}ware \textbf{F}low \textbf{M}atching framework that models perturbed trajectories to ensure stable and structurally consistent time series generation. The framework incorporates perturbation-guided training to simulate localized disturbances and leverages a dual-path velocity field to capture trajectory deviations under perturbation, enabling refined modeling of perturbed behavior to enhance the structural coherence. In order to further improve sensitivity to trajectory perturbations while enhancing expressiveness, a mixture-of-experts decoder with flow routing dynamically allocates modeling capacity in response to different trajectory dynamics. Extensive experiments on both unconditional and conditional generation tasks demonstrate that PAFM consistently outperforms strong baselines. Code is available at https://anonymous.4open.science/r/PAFM-03B2.

</details>


### [142] [CLO: Efficient LLM Inference System with CPU-Light KVCache Offloading via Algorithm-System Co-Design](https://arxiv.org/abs/2511.14510)
*Jiawei Yi,Ping Gong,Youhui Bai,Jiaqi Ruan,Shengnan Wang,Pengcheng Wang,Haibo Wang,Weiguang Wang,Xia Zhu,Feng Wu,Cheng Li*

Main category: cs.LG

TL;DR: 论文指出当前大语言模型推理系统中KVCache存在扩展性问题，提出CPU轻量级KVCache卸载系统CLO，评估显示其能降低CPU开销、提高解码吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有卸载系统忽略CPU瓶颈，包括CPU端细粒度动态缓存管理开销大、PCIe带宽利用率低和GPU运行气泡问题。

Method: 通过算法 - 系统协同设计提出CLO系统，采用粗粒度头级近似GPU缓存策略、数据预取与GPU持久缓存结合、零拷贝传输引擎和GPU中心同步方法。

Result: 在两个常用大语言模型上评估，CLO达到与现有系统相当的准确率，大幅降低CPU开销，充分利用PCIe带宽，解码吞吐量提高9.3% - 66.6%。

Conclusion: 算法 - 系统协同设计对现代GPU平台上内存受限的大语言模型推理至关重要。

Abstract: The growth of million-token LLMs exposes the scalability limits of inference systems, where the KVCache dominates memory usage and data transfer overhead. Recent offloading systems migrate the KVCache to CPU memory and incorporate top-k attention to reduce the volume of data transferred from the CPU, while further applying system-level optimizations such as on-GPU caching and prefetching to lower transfer overhead. However, they overlook the CPU bottleneck in three aspects: (1) substantial overhead of fine-grained dynamic cache management performed on the CPU side, (2) significant transfer overhead from poor PCIe bandwidth utilization caused by heavy gathering operations at the CPU side, and (3) GPU runtime bubbles introduced by coarse-grained CPU-centric synchronization. To address these challenges, we propose CLO, a CPU-light KVCache offloading system via algorithm-system co-design. CLO features: (1) a coarse-grained head-wise approximate on-GPU caching strategy with negligible cache management cost, (2) seamless combination of data prefetching and on-GPU persistent caching for lower transfer overhead, (3) a zero-copy transfer engine to fully exploit PCIe bandwidth, and a GPU-centric synchronization method to eliminate GPU stalls. Evaluation on two widely-used LLMs demonstrates that CLO achieves comparable accuracy to state-of-the-art systems, while substantially minimizing CPU overhead, fully utilizing PCIe bandwidth, thus improving decoding throughput by 9.3%-66.6%. Our results highlight that algorithm-system co-design is essential for memory-constrained LLM inference on modern GPU platforms. We open source CLO at https://github.com/CommediaJW/CLO.

</details>


### [143] [Full Atom Peptide Design via Riemannian Euclidean Bayesian Flow Networks](https://arxiv.org/abs/2511.14516)
*Hao Qian,Shikui Tu,Lei Xu*

Main category: cs.LG

TL;DR: 提出首个用于全原子肽设计的贝叶斯流网络PepBFN，解决当前模型问题，实验证明其在计算肽设计中有潜力。


<details>
  <summary>Details</summary>
Motivation: 现有扩散和流匹配模型在肽结合剂设计中存在离散残基类型采样与连续变量更新不匹配、侧链扭转角分布假设不合理的问题。

Method: 引入PepBFN，通过学习离散残基类型的连续参数分布进行联合平滑贝叶斯更新，采用基于高斯混合的贝叶斯流和基于矩阵费舍尔的黎曼流建模，逐步优化参数分布。

Result: 在侧链堆积、反向折叠和结合剂设计任务上的实验表明PepBFN有很强潜力。

Conclusion: PepBFN在计算肽设计中表现良好，能有效解决现有模型的局限性。

Abstract: Diffusion and flow matching models have recently emerged as promising approaches for peptide binder design. Despite their progress, these models still face two major challenges. First, categorical sampling of discrete residue types collapses their continuous parameters into onehot assignments, while continuous variables (e.g., atom positions) evolve smoothly throughout the generation process. This mismatch disrupts the update dynamics and results in suboptimal performance. Second, current models assume unimodal distributions for side-chain torsion angles, which conflicts with the inherently multimodal nature of side chain rotameric states and limits prediction accuracy. To address these limitations, we introduce PepBFN, the first Bayesian flow network for full atom peptide design that directly models parameter distributions in fully continuous space. Specifically, PepBFN models discrete residue types by learning their continuous parameter distributions, enabling joint and smooth Bayesian updates with other continuous structural parameters. It further employs a novel Gaussian mixture based Bayesian flow to capture the multimodal side chain rotameric states and a Matrix Fisher based Riemannian flow to directly model residue orientations on the $\mathrm{SO}(3)$ manifold. Together, these parameter distributions are progressively refined via Bayesian updates, yielding smooth and coherent peptide generation. Experiments on side chain packing, reverse folding, and binder design tasks demonstrate the strong potential of PepBFN in computational peptide design.

</details>


### [144] [MissHDD: Hybrid Deterministic Diffusion for Hetrogeneous Incomplete Data Imputation](https://arxiv.org/abs/2511.14543)
*Youran Zhou,Mohamed Reda Bouadjenek,Sunil Aryal*

Main category: cs.LG

TL;DR: 本文针对混合类型表格数据缺失值填补问题，提出混合确定性扩散框架，实验表明其效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的插补模型在处理异构表格数据时存在难以保持条件一致性、信息崩溃和不稳定等问题，单一扩散过程不足以处理混合类型表格插补。

Method: 提出混合确定性扩散框架，将异构特征分离到连续的基于DDIM的通道和离散潜路径扩散通道，在统一条件插补目标下训练。

Result: 在多个真实数据集上实验显示，该框架在不同缺失机制下比现有基于扩散和经典方法有更高插补精度、更稳定采样轨迹和更强鲁棒性。

Conclusion: 结构感知的扩散过程对推进不完整表格数据的深度学习方法很重要。

Abstract: Incomplete data are common in real-world tabular applications, where numerical, categorical, and discrete attributes coexist within a single dataset. This heterogeneous structure presents significant challenges for existing diffusion-based imputation models, which typically assume a homogeneous feature space and rely on stochastic denoising trajectories. Such assumptions make it difficult to maintain conditional consistency, and they often lead to information collapse for categorical variables or instability when numerical variables require deterministic updates. These limitations indicate that a single diffusion process is insufficient for mixed-type tabular imputation.
  We propose a hybrid deterministic diffusion framework that separates heterogeneous features into two complementary generative channels. A continuous DDIM-based channel provides efficient and stable deterministic denoising for numerical variables, while a discrete latent-path diffusion channel, inspired by loopholing-based discrete diffusion, models categorical and discrete features without leaving their valid sample manifolds. The two channels are trained under a unified conditional imputation objective, enabling coherent reconstruction of mixed-type incomplete data.
  Extensive experiments on multiple real-world datasets show that the proposed framework achieves higher imputation accuracy, more stable sampling trajectories, and improved robustness across MCAR, MAR, and MNAR settings compared with existing diffusion-based and classical methods. These results demonstrate the importance of structure-aware diffusion processes for advancing deep learning approaches to incomplete tabular data.

</details>


### [145] [Mind the Gaps: Measuring Visual Artifacts in Dimensionality Reduction](https://arxiv.org/abs/2511.14544)
*Jaume Ros,Alessio Arleo,Fernando Paulovich*

Main category: cs.LG

TL;DR: 介绍用于衡量高维数据降维投影到二维平面质量的新指标Warping Index (WI)。


<details>
  <summary>Details</summary>
Motivation: 现有投影质量指标大多只关注数据全局或局部结构，未考虑投影图视觉失真，可能忽略影响视觉分析的异常值或伪影，因此需新指标。

Method: 基于正确保留点间空白区域对数据真实视觉表示至关重要的假设，提出WI指标。

Result: 文中未提及具体结果。

Conclusion: 文中未提及具体结论。

Abstract: Dimensionality Reduction (DR) techniques are commonly used for the visual exploration and analysis of high-dimensional data due to their ability to project datasets of high-dimensional points onto the 2D plane. However, projecting datasets in lower dimensions often entails some distortion, which is not necessarily easy to recognize but can lead users to misleading conclusions. Several Projection Quality Metrics (PQMs) have been developed as tools to quantify the goodness-of-fit of a DR projection; however, they mostly focus on measuring how well the projection captures the global or local structure of the data, without taking into account the visual distortion of the resulting plots, thus often ignoring the presence of outliers or artifacts that can mislead a visual analysis of the projection. In this work, we introduce the Warping Index (WI), a new metric for measuring the quality of DR projections onto the 2D plane, based on the assumption that the correct preservation of empty regions between points is of crucial importance towards a faithful visual representation of the data.

</details>


### [146] [Task Addition and Weight Disentanglement in Closed-Vocabulary Models](https://arxiv.org/abs/2511.14569)
*Adam Hazimeh,Alessandro Favero,Pascal Frossard*

Main category: cs.LG

TL;DR: 本文研究在闭词表图像分类模型中应用任务算术，发现预训练闭词表模型有权重解耦特性，可用任务算术编辑，简单线性探测是有竞争力的基线，拓展了任务算术适用范围。


<details>
  <summary>Details</summary>
Motivation: 现有任务算术多用于预训练开放词表模型，闭词表模型应用未被探索，故研究其在闭词表图像分类模型中的应用。

Method: 在闭词表图像分类模型中部署和研究任务加法，考虑不同预训练方案。

Result: 预训练闭词表视觉Transformer可用任务算术编辑，实现高任务加法性能，简单线性探测是有竞争力的基线。

Conclusion: 研究拓展了任务算术在预训练模型中的适用范围，为不同场景高效使用预训练模型提供可能。

Abstract: Task arithmetic has recently emerged as a promising method for editing pre-trained \textit{open-vocabulary} models, offering a cost-effective alternative to standard multi-task fine-tuning. However, despite the abundance of \textit{closed-vocabulary} models that are not pre-trained with language supervision, applying task arithmetic to these models remains unexplored. In this paper, we deploy and study task addition in closed-vocabulary image classification models. We consider different pre-training schemes and find that \textit{weight disentanglement} -- the property enabling task arithmetic -- is a general consequence of pre-training, as it appears in different pre-trained closed-vocabulary models. In fact, we find that pre-trained closed-vocabulary vision transformers can also be edited with task arithmetic, achieving high task addition performance and enabling the efficient deployment of multi-task models. Finally, we demonstrate that simple linear probing is a competitive baseline to task addition. Overall, our findings expand the applicability of task arithmetic to a broader class of pre-trained models and open the way for more efficient use of pre-trained models in diverse settings.

</details>


### [147] [ReflexGrad: Three-Way Synergistic Architecture for Zero-Shot Generalization in LLM Agents](https://arxiv.org/abs/2511.14584)
*Ankush Kadu,Ashwanth Krishnan*

Main category: cs.LG

TL;DR: 提出ReflexGrad架构实现强化学习中零样本泛化，在ALFWorld基准任务表现良好。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习和决策中让智能体从经验学习并跨任务泛化的挑战，探索不同方法协同整合。

Method: 引入ReflexGrad架构，结合基于大语言模型的分层任务分解、历史感知因果反思和基于梯度的优化。

Result: 在ALFWorld基准任务零样本成功率达67%，能稳定收敛和有效跨任务迁移。

Conclusion: 互补学习机制的协同整合能实现接近少样本基线的强大零样本泛化。

Abstract: Enabling agents to learn from experience and generalize across diverse tasks without task-specific training remains a fundamental challenge in reinforcement learning and decision-making. While recent approaches have explored episodic memory (Reflexion), gradient-based prompt optimization (TextGrad),and hierarchical task decomposition independently, their potential for synergistic integration remains unexplored. We introduce ReflexGrad, a novel architecture that tightly couples three complementary mechanisms: (1) LLM-based hierarchical TODO decomposition for strategic planning, (2) history-aware causal reflection that analyzes recent action patterns to identify failure root causes and enable within-trial learning, and (3) gradient-based optimization for systematic improvement. Unlike prior work relying on few-shot demonstrations, our system achieves true zero-shot generalization through pure LLM semantic reasoning,requiring no task-specific examples, fine-tuning, or hardcoded similarity metrics. Evaluated on ALFWorld benchmark tasks, ReflexGrad demonstrates 67% zero-shot success rate on Trial 0 without any prior task experience or demonstrations, establishing effective performance on first exposure. Through empirical analysis, we identify the architectural mechanisms underlying stable convergence (zero action loops) and effective cross-task transfer (67% to 78% improvement).Our work demonstrates that synergistic integration of complementary learning mechanisms enables robust zero-shot generalization that approaches few-shot baselines from prior work.

</details>


### [148] [Expert-Guided POMDP Learning for Data-Efficient Modeling in Healthcare](https://arxiv.org/abs/2511.14619)
*Marco Locatelli,Arjen Hommersom,Roberto Clemens Cerioli,Daniela Besozzi,Fabio Stella*

Main category: cs.LG

TL;DR: 提出Fuzzy MAP EM算法用于从有限数据学习POMDP参数，在合成医疗模拟中表现优于标准EM算法，案例研究显示其在医疗数据高效建模的潜力。


<details>
  <summary>Details</summary>
Motivation: 解决从有限数据学习POMDP参数的挑战。

Method: 引入Fuzzy MAP EM算法，将专家知识通过模糊伪计数融入EM框架，将问题转化为MAP估计。

Result: 在合成医疗模拟中，该方法在低数据和高噪声条件下均优于标准EM算法，案例研究能恢复临床连贯的POMDP。

Conclusion: Fuzzy MAP EM算法有潜力成为医疗数据高效建模的实用工具。

Abstract: Learning the parameters of Partially Observable Markov Decision Processes (POMDPs) from limited data is a significant challenge. We introduce the Fuzzy MAP EM algorithm, a novel approach that incorporates expert knowledge into the parameter estimation process by enriching the Expectation Maximization (EM) framework with fuzzy pseudo-counts derived from an expert-defined fuzzy model. This integration naturally reformulates the problem as a Maximum A Posteriori (MAP) estimation, effectively guiding learning in environments with limited data. In synthetic medical simulations, our method consistently outperforms the standard EM algorithm under both low-data and high-noise conditions. Furthermore, a case study on Myasthenia Gravis illustrates the ability of the Fuzzy MAP EM algorithm to recover a clinically coherent POMDP, demonstrating its potential as a practical tool for data-efficient modeling in healthcare.

</details>


### [149] [Failure to Mix: Large language models struggle to answer according to desired probability distributions](https://arxiv.org/abs/2511.14630)
*Ivy Yuqian Yang,David Yu Zhang*

Main category: cs.LG

TL;DR: 研究发现现代大语言模型在按概率分布输出结果时表现不佳，会倾向输出概率最高结果。


<details>
  <summary>Details</summary>
Motivation: 科学想法生成和选择需按目标概率分布探索，但当前AI基准有客观正确答案，强化学习训练大语言模型会抑制概率探索，故研究其按概率分布输出能力。

Method: 进行系统实验，要求大语言模型按简单概率分布输出结果。

Result: 所有测试的现代大语言模型都严重无法遵循概率分布，如要求49%输出“1”时，几乎100%输出“0”。

Conclusion: 大语言模型存在类似阶跃函数的行为，会优先输出概率略高的结果，甚至会覆盖内置偏差。

Abstract: Scientific idea generation and selection requires exploration following a target probability distribution. In contrast, current AI benchmarks have objectively correct answers, and training large language models (LLMs) via reinforcement learning against these benchmarks discourages probabilistic exploration. Here, we conducted systematic experiments requesting LLMs to produce outputs following simple probabilistic distributions, and found that all modern LLMs tested grossly fail to follow the distributions. For example, requesting a binary output of "1" 49% of the time produces an answer of "0" nearly 100% of the time. This step function-like behavior of near-exclusively generating the output with marginally highest probability even overrules even strong in-built LLM biases.

</details>


### [150] [Adapformer: Adaptive Channel Management for Multivariate Time Series Forecasting](https://arxiv.org/abs/2511.14632)
*Yuchen Luo,Xinyu Li,Liuhua Peng,Mingming Gong*

Main category: cs.LG

TL;DR: 文章指出多变量时间序列预测中传统方法的问题，提出Adapformer框架，测试显示其性能优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 传统多变量时间序列预测方法存在局限性，CI方法未利用通道间交互信息，CD方法易过拟合和效率低，需改进。

Method: 引入基于Transformer的Adapformer框架，采用双阶段编解码器架构，包含ACE和ACF模块。

Result: 在不同数据集上测试，Adapformer性能优于现有模型，提升了预测准确性和计算效率。

Conclusion: Adapformer在多变量时间序列预测中达到了当前最优水平。

Abstract: In multivariate time series forecasting (MTSF), accurately modeling the intricate dependencies among multiple variables remains a significant challenge due to the inherent limitations of traditional approaches. Most existing models adopt either \textbf{channel-independent} (CI) or \textbf{channel-dependent} (CD) strategies, each presenting distinct drawbacks. CI methods fail to leverage the potential insights from inter-channel interactions, resulting in models that may not fully exploit the underlying statistical dependencies present in the data. Conversely, CD approaches often incorporate too much extraneous information, risking model overfitting and predictive inefficiency. To address these issues, we introduce the Adaptive Forecasting Transformer (\textbf{Adapformer}), an advanced Transformer-based framework that merges the benefits of CI and CD methodologies through effective channel management. The core of Adapformer lies in its dual-stage encoder-decoder architecture, which includes the \textbf{A}daptive \textbf{C}hannel \textbf{E}nhancer (\textbf{ACE}) for enriching embedding processes and the \textbf{A}daptive \textbf{C}hannel \textbf{F}orecaster (\textbf{ACF}) for refining the predictions. ACE enhances token representations by selectively incorporating essential dependencies, while ACF streamlines the decoding process by focusing on the most relevant covariates, substantially reducing noise and redundancy. Our rigorous testing on diverse datasets shows that Adapformer achieves superior performance over existing models, enhancing both predictive accuracy and computational efficiency, thus making it state-of-the-art in MTSF.

</details>


### [151] [Machine Learning Models for Predicting Smoking-Related Health Decline and Disease Risk](https://arxiv.org/abs/2511.14682)
*Vaskar Chakma,MD Jaheid Hasan Nerab,Abdur Rouf,Abu Sayed,Hossem MD Saim,Md. Nournabi Khan*

Main category: cs.LG

TL;DR: 研究对机器学习方法进行系统比较评估，用于吸烟相关健康风险评估，随机森林模型效果最佳。


<details>
  <summary>Details</summary>
Motivation: 当前医疗筛查方法常错过吸烟相关健康问题早期预警，导致晚期诊断，治疗选择受限。

Method: 分析55691人健康筛查数据，测试随机森林、XGBoost和LightGBM三种算法，采用横断面设计根据健康筛查生物标志物分类当前吸烟状态。

Result: 随机森林模型表现最佳，AUC为0.926，能可靠区分高低风险个体，通过SHAP分析发现血压、甘油三酯浓度等是预测关键健康指标。

Conclusion: 随机森林模型在吸烟相关健康风险评估中有较好表现，特定健康指标对预测吸烟人群健康下降有重要作用。

Abstract: Smoking continues to be a major preventable cause of death worldwide, affecting millions through damage to the heart, metabolism, liver, and kidneys. However, current medical screening methods often miss the early warning signs of smoking-related health problems, leading to late-stage diagnoses when treatment options become limited. This study presents a systematic comparative evaluation of machine learning approaches for smoking-related health risk assessment, emphasizing clinical interpretability and practical deployment over algorithmic innovation. We analyzed health screening data from 55,691 individuals, examining various health indicators, including body measurements, blood tests, and demographic information. We tested three advanced prediction algorithms - Random Forest, XGBoost, and LightGBM - to determine which could most accurately identify people at high risk. This study employed a cross-sectional design to classify current smoking status based on health screening biomarkers, not to predict future disease development. Our Random Forest model performed best, achieving an Area Under the Curve (AUC) of 0.926, meaning it could reliably distinguish between high-risk and lower-risk individuals. Using SHAP (SHapley Additive exPlanations) analysis to understand what the model was detecting, we found that key health markers played crucial roles in prediction: blood pressure levels, triglyceride concentrations, liver enzyme readings, and kidney function indicators (serum creatinine) were the strongest signals of declining health in smokers.

</details>


### [152] [AdamHD: Decoupled Huber Decay Regularization for Language Model Pre-Training](https://arxiv.org/abs/2511.14721)
*Fu-Ming Guo,Yingfang Fan*

Main category: cs.LG

TL;DR: 提出AdamHuberDecay替代AdamW，用Huber正则化器替换l2惩罚，实验表明其训练更快、性能更好、能节省内存。


<details>
  <summary>Details</summary>
Motivation: 现有带解耦权重衰减的自适应优化器（如AdamW）中l2惩罚存在问题，易受极端梯度方向影响，常过度惩罚条件良好的坐标。

Method: 提出AdamHuberDecay，用解耦的平滑Huber正则化器替换l2惩罚，推导闭式解耦Huber衰减步骤并以O(1)额外成本集成到Adam族优化器中。

Result: 在GPT - 2和GPT - 3预训练实验中，AdamHuberDecay收敛快10 - 15%，降低验证困惑度，提升下游任务性能，产生更稀疏权重直方图，剪枝后节省20 - 30%内存。

Conclusion: AdamHuberDecay为下一代基础生成式Transformer提供了简单、有原则的高效且有弹性的训练路径。

Abstract: Adaptive optimizers with decoupled weight decay, such as AdamW, are the de facto standard for pre-training large transformer-based generative models. Yet the quadratic nature of the $\ell_2$ penalty embedded in weight decay drives all parameters toward the origin at the same rate, making the update vulnerable to rare but extreme gradient directions and often over-penalizing well-conditioned coordinates. We propose AdamHuberDecay, a drop-in replacement for AdamW that substitutes the $\ell_2$ penalty with a decoupled smooth Huber regularizer. The resulting update decays parameters quadratically while their magnitude remains below a threshold $δ$, and linearly ($\ell_1$-like) once they exceed $δ$, yielding (i) bounded regularization gradients, (ii) invariance to per-coordinate second-moment rescaling, and (iii) stronger sparsity pressure on overgrown weights.
  We derive the closed-form decoupled Huber decay step and show how to integrate it with any Adam-family optimizer at $O(1)$ extra cost. Extensive experiments on GPT-2 and GPT-3 pre-training demonstrate that AdamHuberDecay (a) converges 10-15% faster in wall-clock time, (b) reduces validation perplexity by up to 4 points, (c) delivers performance improvements of 2.5-4.7% across downstream tasks, and (d) yields visibly sparser weight histograms that translate into 20-30% memory savings after magnitude pruning, without tuning the decay coefficient beyond the default grid used for AdamW. Ablations confirm robustness to outlier gradients and large-batch regimes, together with theoretical analyses that bound the expected parameter norm under noisy updates. AdamHuberDecay therefore provides a simple, principled path toward more efficient and resilient training of next-generation foundational generative transformers.

</details>


### [153] [LAUD: Integrating Large Language Models with Active Learning for Unlabeled Data](https://arxiv.org/abs/2511.14738)
*Tzu-Hsuan Chou,Chun-Nan Chou*

Main category: cs.LG

TL;DR: 论文提出整合大语言模型与主动学习的框架LAUD，缓解无标签数据问题，实验显示其在商品名称分类任务上优于零样本或少样本学习。


<details>
  <summary>Details</summary>
Motivation: 现实场景中缺乏标注数据，基于提示的方法繁琐低效，需解决无标注数据问题。

Method: 提出学习框架LAUD，用零样本学习构建初始标签集缓解冷启动问题。

Result: 基于LAUD的大语言模型在商品名称分类任务上优于零样本或少样本学习的大语言模型。

Conclusion: LAUD框架有效。

Abstract: Large language models (LLMs) have shown a remarkable ability to generalize beyond their pre-training data, and fine-tuning LLMs can elevate performance to human-level and beyond. However, in real-world scenarios, lacking labeled data often prevents practitioners from obtaining well-performing models, thereby forcing practitioners to highly rely on prompt-based approaches that are often tedious, inefficient, and driven by trial and error. To alleviate this issue of lacking labeled data, we present a learning framework integrating LLMs with active learning for unlabeled dataset (LAUD). LAUD mitigates the cold-start problem by constructing an initial label set with zero-shot learning. Experimental results show that LLMs derived from LAUD outperform LLMs with zero-shot or few-shot learning on commodity name classification tasks, demonstrating the effectiveness of LAUD.

</details>


### [154] [Beyond Means: A Dynamic Framework for Predicting Customer Satisfaction](https://arxiv.org/abs/2511.14743)
*Christof Naumzik,Abdurahman Maarouf,Stefan Feuerriegel,Markus Weinmann*

Main category: cs.LG

TL;DR: 本文指出标准评分聚合方法的不足，提出用高斯过程框架进行评分聚合，经Yelp数据验证该模型更准确，对营销和客户有重要意义。


<details>
  <summary>Details</summary>
Motivation: 标准评分聚合方法无法适应质量随时间的变化且忽略评论异质性，需要更好的方法进行评分聚合。

Method: 提出一个定制的高斯过程（GP）模型来捕捉评分随时间的动态变化并考虑评论异质性，基于Yelp的121,123条评分比较不同评分聚合方法的预测能力。

Result: GP模型比样本均值法更准确，平均绝对误差降低了10.2%。

Conclusion: 在线声誉系统设计者应超越均值法，展示更具信息性和适应性的聚合评分，以准确反映客户满意度。

Abstract: Online ratings influence customer decision-making, yet standard aggregation methods, such as the sample mean, fail to adapt to quality changes over time and ignore review heterogeneity (e.g., review sentiment, a review's helpfulness). To address these challenges, we demonstrate the value of using the Gaussian process (GP) framework for rating aggregation. Specifically, we present a tailored GP model that captures the dynamics of ratings over time while additionally accounting for review heterogeneity. Based on 121,123 ratings from Yelp, we compare the predictive power of different rating aggregation methods in predicting future ratings, thereby finding that the GP model is considerably more accurate and reduces the mean absolute error by 10.2% compared to the sample mean. Our findings have important implications for marketing practitioners and customers. By moving beyond means, designers of online reputation systems can display more informative and adaptive aggregated rating scores that are accurate signals of expected customer satisfaction.

</details>


### [155] [Measuring AI Progress in Drug Discovery: A Reproducible Leaderboard for the Tox21 Challenge](https://arxiv.org/abs/2511.14744)
*Antonia Ebner,Christoph Bartmann,Sonja Topf,Sohvi Luukkonen,Johannes Schimunek,Günter Klambauer*

Main category: cs.LG

TL;DR: 本文介绍基于原始Tox21挑战数据集的可复现排行榜，显示过去十年毒性预测进展不明，并公开模型。


<details>
  <summary>Details</summary>
Motivation: Tox21数据集整合时被改动，导致不同研究缺乏可比性，不清楚过去十年生物活性和毒性预测方法是否改进。

Method: 引入托管在Hugging Face上的可复现排行榜，采用一组基线和代表性方法。

Result: 当前排行榜显示原Tox21获胜方法和2017年提出的方法仍有竞争力。

Conclusion: 过去十年毒性预测是否取得实质性进展尚不明确。

Abstract: Deep learning's rise since the early 2010s has transformed fields like computer vision and natural language processing and strongly influenced biomedical research. For drug discovery specifically, a key inflection - akin to vision's "ImageNet moment" - arrived in 2015, when deep neural networks surpassed traditional approaches on the Tox21 Data Challenge. This milestone accelerated the adoption of deep learning across the pharmaceutical industry, and today most major companies have integrated these methods into their research pipelines. After the Tox21 Challenge concluded, its dataset was included in several established benchmarks, such as MoleculeNet and the Open Graph Benchmark. However, during these integrations, the dataset was altered and labels were imputed or manufactured, resulting in a loss of comparability across studies. Consequently, the extent to which bioactivity and toxicity prediction methods have improved over the past decade remains unclear. To this end, we introduce a reproducible leaderboard, hosted on Hugging Face with the original Tox21 Challenge dataset, together with a set of baseline and representative methods. The current version of the leaderboard indicates that the original Tox21 winner - the ensemble-based DeepTox method - and the descriptor-based self-normalizing neural networks introduced in 2017, continue to perform competitively and rank among the top methods for toxicity prediction, leaving it unclear whether substantial progress in toxicity prediction has been achieved over the past decade. As part of this work, we make all baselines and evaluated models publicly accessible for inference via standardized API calls to Hugging Face Spaces.

</details>


### [156] [$π^{*}_{0.6}$: a VLA That Learns From Experience](https://arxiv.org/abs/2511.14759)
*Ali Amin,Raichelle Aniceto,Ashwin Balakrishna,Kevin Black,Ken Conley,Grace Connors,James Darpinian,Karan Dhabalia,Jared DiCarlo,Danny Driess,Michael Equi,Adnan Esmail,Yunhao Fang,Chelsea Finn,Catherine Glossop,Thomas Godden,Ivan Goryachev,Lachy Groom,Hunter Hancock,Karol Hausman,Gashon Hussein,Brian Ichter,Szymon Jakubczak,Rowan Jen,Tim Jones,Ben Katz,Liyiming Ke,Chandra Kuchi,Marinda Lamb,Devin LeBlanc,Sergey Levine,Adrian Li-Bell,Yao Lu,Vishnu Mano,Mohith Mothukuri,Suraj Nair,Karl Pertsch,Allen Z. Ren,Charvi Sharma,Lucy Xiaoyang Shi,Laura Smith,Jost Tobias Springenberg,Kyle Stachowicz,Will Stoeckle,Alex Swerdlow,James Tanner,Marcel Torne,Quan Vuong,Anna Walling,Haohuan Wang,Blake Williams,Sukwon Yoo,Lili Yu,Ury Zhilinsky,Zhiyuan Zhou*

Main category: cs.LG

TL;DR: 研究通过强化学习让视觉 - 语言 - 动作（VLA）模型在现实部署中改进，提出RECAP方法，模型在多任务表现佳，提升任务吞吐量并降低失败率。


<details>
  <summary>Details</summary>
Motivation: 探索VLA模型如何通过现实部署和强化学习得到改进。

Method: 提出RECAP方法，通过优势调节进行VLA的强化学习训练，结合异构数据，先离线预训练通用VLA模型$π^{*}_{0.6}$，再通过机器人数据收集进行下游任务优化。

Result: $π^{*}_{0.6}$模型能在现实中完成叠衣服、组装盒子、制作浓缩咖啡等任务，在困难任务上使任务吞吐量翻倍，失败率减半。

Conclusion: RECAP方法能有效提升VLA模型在现实世界任务中的性能。

Abstract: We study how vision-language-action (VLA) models can improve through real-world deployments via reinforcement learning (RL). We present a general-purpose method, RL with Experience and Corrections via Advantage-conditioned Policies (RECAP), that provides for RL training of VLAs via advantage conditioning. Our method incorporates heterogeneous data into the self-improvement process, including demonstrations, data from on-policy collection, and expert teleoperated interventions provided during autonomous execution. RECAP starts by pre-training a generalist VLA with offline RL, which we call $π^{*}_{0.6}$, that can then be specialized to attain high performance on downstream tasks through on-robot data collection. We show that the $π^{*}_{0.6}$ model trained with the full RECAP method can fold laundry in real homes, reliably assemble boxes, and make espresso drinks using a professional espresso machine. On some of the hardest tasks, RECAP more than doubles task throughput and roughly halves the task failure rate.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [157] [MS2Edge: Towards Energy-Efficient and Crisp Edge Detection with Multi-Scale Residual Learning in SNNs](https://arxiv.org/abs/2511.13735)
*Yimeng Fan,Changsong Liu,Mingyang Li,Yuzhou Dai,Yanyan Liu,Wei Zhang*

Main category: cs.NE

TL;DR: 提出基于SNN的边缘检测模型MS2Edge，解决现有方法能耗高、边缘清晰度差问题，实验表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有ANN边缘检测需大量预训练和复杂设计，能耗高，预测边缘清晰度差；SNN虽有优势但存在量化误差导致边缘不连续问题。

Method: 构建新颖的尖峰骨干网络MS2ResNet，结合I - LIF神经元与MDS缓解量化误差，添加SMSUB进行细节重建，采用MAD方法整合多时间步边缘图。

Result: MS2Edge在多个数据集上超越基于ANN的方法，无预训练骨干情况下达到SOTA性能，能耗超低，无需后处理生成清晰边缘图。

Conclusion: MS2Edge是一种有效且节能的边缘检测模型，为边缘检测提供了新方案。

Abstract: Edge detection with Artificial Neural Networks (ANNs) has achieved remarkable prog\-ress but faces two major challenges. First, it requires pre-training on large-scale extra data and complex designs for prior knowledge, leading to high energy consumption. Second, the predicted edges perform poorly in crispness and heavily rely on post-processing. Spiking Neural Networks (SNNs), as third generation neural networks, feature quantization and spike-driven computation mechanisms. They inherently provide a strong prior for edge detection in an energy-efficient manner, while its quantization mechanism helps suppress texture artifact interference around true edges, improving prediction crispness. However, the resulting quantization error inevitably introduces sparse edge discontinuities, compromising further enhancement of crispness. To address these challenges, we propose MS2Edge, the first SNN-based model for edge detection. At its core, we build a novel spiking backbone named MS2ResNet that integrates multi-scale residual learning to recover missing boundary lines and generate crisp edges, while combining I-LIF neurons with Membrane-based Deformed Shortcut (MDS) to mitigate quantization errors. The model is complemented by a Spiking Multi-Scale Upsample Block (SMSUB) for detail reconstruction during upsampling and a Membrane Average Decoding (MAD) method for effective integration of edge maps across multiple time steps. Experimental results demonstrate that MS2Edge outperforms ANN-based methods and achieves state-of-the-art performance on the BSDS500, NYUDv2, BIPED, PLDU, and PLDM datasets without pre-trained backbones, while maintaining ultralow energy consumption and generating crisp edge maps without post-processing.

</details>


### [158] [Towards a Comprehensive Theory of Reservoir Computing](https://arxiv.org/abs/2511.14484)
*Denis Kleyko,Christopher J. Kymn,E. Paxon Frady,Amy Loutfi,Friedrich T. Sommer*

Main category: cs.NE

TL;DR: 利用感知机理论预测回声状态网络（ESN）性能，经实证验证，还用于优化参数、提出新模型并刻画网络几何。


<details>
  <summary>Details</summary>
Motivation: 在水库计算中，为特定应用选择ESN模型和参数设置需要预测和比较性能的理论。

Method: 运用感知机理论预测多种ESN模型的记忆容量和准确性，并通过实证验证。

Result: 三十种ESN变体的实证结果证实了理论预测，优化了ESN记忆容量，提出无需训练的新模型且性能更优，刻画了ESN读出网络几何。

Conclusion: 感知机理论可有效预测ESN性能，为模型选择和优化提供了新途径。

Abstract: In reservoir computing, an input sequence is processed by a recurrent neural network, the reservoir, which transforms it into a spatial pattern that a shallow readout network can then exploit for tasks such as memorization and time-series prediction or classification. Echo state networks (ESN) are a model class in which the reservoir is a traditional artificial neural network. This class contains many model types, each with sets of hyperparameters. Selecting models and parameter settings for particular applications requires a theory for predicting and comparing performances. Here, we demonstrate that recent developments of perceptron theory can be used to predict the memory capacity and accuracy of a wide variety of ESN models, including reservoirs with linear neurons, sigmoid nonlinear neurons, different types of recurrent matrices, and different types of readout networks. Across thirty variants of ESNs, we show that empirical results consistently confirm the theory's predictions. As a practical demonstration, the theory is used to optimize memory capacity of an ESN in the entire joint parameter space. Further, guided by the theory, we propose a novel ESN model with a readout network that does not require training, and which outperforms earlier ESN models without training. Finally, we characterize the geometry of the readout networks in ESNs, which reveals that many ESN models exhibit a similar regular simplex geometry as has been observed in the output weights of deep neural networks.

</details>


### [159] [Attention via Synaptic Plasticity is All You Need: A Biologically Inspired Spiking Neuromorphic Transformer](https://arxiv.org/abs/2511.14691)
*Kallol Mondal,Ankush Kumar*

Main category: cs.NE

TL;DR: 提出基于脉冲时序依赖可塑性（STDP）的脉冲STDP Transformer（S²TDPT），在CIFAR - 10和CIFAR - 100上验证其节能、硬件友好和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer能耗大，当前脉冲注意力存在非神经形态计算等问题，需设计节能、类脑计算模型。

Method: 提出S²TDPT，通过STDP实现自注意力机制，将查询 - 键相关性嵌入突触权重。

Result: 在CIFAR - 10和CIFAR - 100上分别达到94.35%和78.08%准确率，CIFAR - 100能耗降低88.47%，Grad - CAM显示模型关注语义相关区域。

Conclusion: S²TDPT表明受生物启发的注意力可产生节能、硬件友好和可解释的神经形态模型。

Abstract: Attention is the brain's ability to selectively focus on a few specific aspects while ignoring irrelevant ones. This biological principle inspired the attention mechanism in modern Transformers. Transformers now underpin large language models (LLMs) such as GPT, but at the cost of massive training and inference energy, leading to a large carbon footprint. While brain attention emerges from neural circuits, Transformer attention relies on dot-product similarity to weight elements in the input sequence. Neuromorphic computing, especially spiking neural networks (SNNs), offers a brain-inspired path to energy-efficient intelligence. Despite recent work on attention-based spiking Transformers, the core attention layer remains non-neuromorphic. Current spiking attention (i) relies on dot-product or element-wise similarity suited to floating-point operations, not event-driven spikes; (ii) keeps attention matrices that suffer from the von Neumann bottleneck, limiting in-memory computing; and (iii) still diverges from brain-like computation. To address these issues, we propose the Spiking STDP Transformer (S$^{2}$TDPT), a neuromorphic Transformer that implements self-attention through spike-timing-dependent plasticity (STDP), embedding query--key correlations in synaptic weights. STDP, a core mechanism of memory and learning in the brain and widely studied in neuromorphic devices, naturally enables in-memory computing and supports non-von Neumann hardware. On CIFAR-10 and CIFAR-100, our model achieves 94.35\% and 78.08\% accuracy with only four timesteps and 0.49 mJ on CIFAR-100, an 88.47\% energy reduction compared to a standard ANN Transformer. Grad-CAM shows that the model attends to semantically relevant regions, enhancing interpretability. Overall, S$^{2}$TDPT illustrates how biologically inspired attention can yield energy-efficient, hardware-friendly, and explainable neuromorphic models.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [160] [Enabling Heterogeneous Performance Analysis for Scientific Workloads](https://arxiv.org/abs/2511.13928)
*Maksymilian Graczyk,Vincent Desbiolles,Stefan Roiser,Andrea Guerrieri*

Main category: cs.PF

TL;DR: 本文围绕异构计算中性能分析展开，探讨Adaptyst项目，研究两种基于eBPF方法用于未来集成及提升异构性能分析能力。


<details>
  <summary>Details</summary>
Motivation: 异构计算需高效性能分析来确定合适任务调度平台，Adaptyst项目旨在开发适用于科学工作负载的开源、架构无关性能分析工具。

Method: 探索两种基于eBPF的内置方法Uprobes和USDT。

Result: 未提及。

Conclusion: 未提及，但目标是为Adaptyst未来集成制定路线图，推进异构性能分析能力。

Abstract: Heterogeneous computing integrates diverse processing elements, such as CPUs, GPUs, and FPGAs, within a single system, aiming to leverage the strengths of each architecture to optimize performance and energy consumption. In this context, efficient performance analysis plays a critical role in determining the most suitable platform for dispatching tasks, ensuring that workloads are allocated to the processing units where they can execute most effectively. Adaptyst is a novel ongoing effort at CERN, with the aim to develop an open-source, architecture-agnostic performance analysis for scientific workloads. This study explores the performance and implementation complexity of two built-in eBPF-based methods such as Uprobes and USDT, with the aim of outlining a roadmap for future integration into Adaptyst and advancing toward heterogeneous performance analysis capabilities.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [161] [Show and Tell: Prompt Strategies for Style Control in Multi-Turn LLM Code Generation](https://arxiv.org/abs/2511.13972)
*Jeremiah Bohr*

Main category: cs.SE

TL;DR: 研究语言模型代码风格控制，对比指令、示例及组合提示，发现在两轮工作流中组合提示风格控制最稳定。


<details>
  <summary>Details</summary>
Motivation: 探究语言模型在增强代码功能时，风格约束能否持续并保持高功能准确性。

Method: 在配对两轮协议中，操纵系统提示的四种条件，让模型先生成中间Python任务解决方案，再根据通用改进指令修改代码。

Result: 组合提示初始压缩最强、扩展纪律最好；指令初始效果大、扩展纪律中等；示例初始效果小且无扩展纪律。

Conclusion: 初始提示有效性和扩展纪律是提示设计的不同方面，组合方法在两轮工作流中提供最稳定的风格控制。

Abstract: Language models generate functionally correct code that tends toward excessive verbosity, with elaborate documentation and defensive patterns that diverge from human baselines. Two prompting mechanisms have emerged for stylistic control: instruction based prompts that articulate abstract directives, and example based prompts that provide concrete code demonstrations. The core problem is whether stylistic constraints persist when models enhance initial implementations with additional features while maintaining high functional accuracy. Here we show that instruction-based, example-based, and combined prompts produce distinct patterns of initial control and expansion discipline over one enhancement turn. We manipulated system prompts across four conditions in a paired two-turn protocol where models first generated solutions to an intermediate Python task, then revised their code under general improvement directives, holding the user task fixed (N = 160 paired programs). Combined prompts produced the strongest initial compression and greatest expansion discipline. Instructions showed large initial effects and moderate expansion discipline. Examples showed modest initial effects with no expansion discipline. These results show that initial prompt effectiveness and expansion discipline are separate aspects of prompt design, and that combined approaches provide the most stable stylistic control in this two-turn workflow.

</details>


### [162] [Exploring the Use of ChatGPT by Computer Science Students in Software Development: Applications, Ethical Considerations, and Insights for Engineering Education](https://arxiv.org/abs/2511.13996)
*Daihan Xu,Diana Martin*

Main category: cs.SE

TL;DR: 研究英国某机构计算机专业学生在软件开发项目中使用ChatGPT的策略与伦理问题，发现学生学习模式转变，使用有一定策略但批判性参与不足，需明确使用指南。


<details>
  <summary>Details</summary>
Motivation: 现有研究多依赖调查，缺乏对学生策略和伦理意识的深入分析，本研究旨在弥补这一不足。

Method: 采用半结构化访谈，探讨学生使用ChatGPT的策略和对伦理问题的理解。

Result: 学生学习模式转变，用ChatGPT辅助学习，控制其贡献比例，但批判性分析不足，拒绝无引用使用，强调风险并呼吁明确指南。

Conclusion: 研究为学习者与AI互动提供新见解，强调需明确指导以支持负责任使用工具。

Abstract: ChatGPT has been increasingly used in computer science, offering efficient support across software development tasks. While it helps students navigate programming challenges, its use also raises concerns about academic integrity and overreliance. Despite growing interest in this topic, prior research has largely relied on surveys, emphasizing trends over in-depth analysis of students' strategies and ethical awareness. This study complements existing work through a qualitative investigation of how computer science students in one UK institution strategically and ethically engage with ChatGPT in software development projects. Drawing on semi-structured interviews, it explores two key questions: How do computer science students ethically and strategically report using ChatGPT in software development projects? How do students understand and perceive the ethical issues associated with using ChatGPT in academic and professional contexts? Findings reveal a shift in students' learning models, moving from traditional "independent thinking-manual coding-iterative debugging" to "AI-assisted ideation-interactive programming-collaborative optimization." Importantly, many use ChatGPT conversationally to deepen understanding, while consciously reserving creative and high-level decision-making tasks for themselves. Students tend to cap ChatGPT's contribution to roughly 30%, and evaluate its output to mitigate overreliance. However, only a minority thoroughly analyze AI-generated code, raising concerns about reduced critical engagement. Meanwhile, students reject uncredited use, highlight risks such as privacy breaches and skill degradation, and call for clear usage guidelines set by their teachers. This research offers novel insights into the evolving learner-AI dynamic and highlights the need for explicit guidance to support responsible and pedagogically sound use of such tools.

</details>


### [163] [LoCoBench-Agent: An Interactive Benchmark for LLM Agents in Long-Context Software Engineering](https://arxiv.org/abs/2511.13998)
*Jielin Qiu,Zuxin Liu,Zhiwei Liu,Rithesh Murthy,Jianguo Zhang,Haolin Chen,Shiyu Wang,Ming Zhu,Liangwei Yang,Juntao Tan,Roshan Ram,Akshara Prabhakar,Tulika Awalgaonkar,Zixiang Chen,Zhepeng Cen,Cheng Qian,Shelby Heinecke,Weiran Yao,Silvio Savarese,Caiming Xiong,Huan Wang*

Main category: cs.SE

TL;DR: 现有基准无法满足大语言模型代理评估需求，提出LoCoBench - Agent框架评估其在软件工程工作流中的能力，评估后有多项发现并奠定评估基础。


<details>
  <summary>Details</summary>
Motivation: 现有基准如LoCoBench聚焦单轮评估，无法捕捉多轮交互、工具使用模式和自适应推理等真实世界编码代理所需特性，需新评估框架。

Method: 将LoCoBench的8000个场景扩展到交互式代理环境，引入含9个指标的评估方法，提供8种专业工具，在10K到1M令牌的上下文长度范围内评估。

Result: （1）代理表现出出色的长上下文鲁棒性；（2）理解 - 效率存在负相关权衡；（3）不同模型对话效率差异大，策略性工具使用模式区分高性能代理。

Conclusion: LoCoBench - Agent作为首个软件工程长上下文大语言模型代理基准，为衡量代理能力、识别性能差距和推动大规模自主软件开发奠定严格基础。

Abstract: As large language models (LLMs) evolve into sophisticated autonomous agents capable of complex software development tasks, evaluating their real-world capabilities becomes critical. While existing benchmarks like LoCoBench~\cite{qiu2025locobench} assess long-context code understanding, they focus on single-turn evaluation and cannot capture the multi-turn interactive nature, tool usage patterns, and adaptive reasoning required by real-world coding agents. We introduce \textbf{LoCoBench-Agent}, a comprehensive evaluation framework specifically designed to assess LLM agents in realistic, long-context software engineering workflows. Our framework extends LoCoBench's 8,000 scenarios into interactive agent environments, enabling systematic evaluation of multi-turn conversations, tool usage efficiency, error recovery, and architectural consistency across extended development sessions. We also introduce an evaluation methodology with 9 metrics across comprehension and efficiency dimensions. Our framework provides agents with 8 specialized tools (file operations, search, code analysis) and evaluates them across context lengths ranging from 10K to 1M tokens, enabling precise assessment of long-context performance. Through systematic evaluation of state-of-the-art models, we reveal several key findings: (1) agents exhibit remarkable long-context robustness; (2) comprehension-efficiency trade-off exists with negative correlation, where thorough exploration increases comprehension but reduces efficiency; and (3) conversation efficiency varies dramatically across models, with strategic tool usage patterns differentiating high-performing agents. As the first long-context LLM agent benchmark for software engineering, LoCoBench-Agent establishes a rigorous foundation for measuring agent capabilities, identifying performance gaps, and advancing autonomous software development at scale.

</details>


### [164] [FlakyGuard: Automatically Fixing Flaky Tests at Industry Scale](https://arxiv.org/abs/2511.14002)
*Chengpeng Li,Farnaz Behrang,August Shi,Peng Liu*

Main category: cs.SE

TL;DR: 介绍了用于修复不稳定测试的FlakyGuard，它通过图结构处理代码和选择性图探索找到相关上下文，在实际测试中表现好，开发者评价高。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型修复不稳定测试的方法在工业场景中因上下文问题失效，浪费开发者时间和影响发布周期。

Method: 将代码视为图结构，使用选择性图探索找到最相关上下文。

Result: 修复了47.6%的可复现不稳定测试，51.8%的修复被开发者接受，修复成功率比现有方法至少高22%，开发者认为其根本原因解释有用。

Conclusion: FlakyGuard能有效解决现有方法在工业场景中的上下文问题，可用于修复不稳定测试。

Abstract: Flaky tests that non-deterministically pass or fail waste developer time and slow release cycles. While large language models (LLMs) show promise for automatically repairing flaky tests, existing approaches like FlakyDoctor fail in industrial settings due to the context problem: providing either too little context (missing critical production code) or too much context (overwhelming the LLM with irrelevant information). We present FlakyGuard, which addresses this problem by treating code as a graph structure and using selective graph exploration to find only the most relevant context. Evaluation on real-world flaky tests from industrial repositories shows that FlakyGuard repairs 47.6 % of reproducible flaky tests with 51.8 % of the fixes accepted by developers. Besides it outperforms state-of-the-art approaches by at least 22 % in repair success rate. Developer surveys confirm that 100 % find FlakyGuard's root cause explanations useful.

</details>


### [165] [Keeping Code-Aware LLMs Fresh: Full Refresh, In-Context Deltas, and Incremental Fine-Tuning](https://arxiv.org/abs/2511.14022)
*Pradeep Kumar Sharma,Ishaan Puri,Mantinder Jit Singh,Swapnil Shivaprasad,Hritvik Shrivastava*

Main category: cs.SE

TL;DR: 研究如何在不丢失旧代码记忆的情况下更新代码检索模型，对比三种更新策略。


<details>
  <summary>Details</summary>
Motivation: 现代代码库不断演变，旧模型会随代码变化而性能下降，需让模型保持新鲜。

Method: 将新鲜度视为基础快照和当前版本间的领域漂移，对比全量刷新、上下文学习、增量微调三种更新策略，提出别名感知评估协议和遗忘探针。

Result: 在多个代码库上，旧代码感知的增量微调在混合集上平衡最好，英文摘要的上下文学习在无法训练时提升新代码最快，全量刷新在追求新代码最高准确率时最佳，且git差异增量微调在重命名/删除场景更优，全文件上下文在行为改变场景更优。

Conclusion: 不同更新策略在不同场景有不同优势，可根据实际需求选择。

Abstract: Modern codebases evolve continuously: files are renamed or deleted; public APIs drift; behavior shifts within otherwise familiar modules. A model trained yesterday to map a developer's natural-language question to the exact set of repository file paths that matter will degrade tomorrow, even if the questions themselves look unchanged. In this paper we study, at system scale and across several widely used repositories, how to keep such a model fresh without surrendering retention on earlier code. We frame freshness as a form of domain drift between a base snapshot and the current HEAD, and we compare three families of update strategies: (A) Full Refresh, retraining the entire model at the new snapshot; (B) In-Context Learning (ICL) that injects recent deltas (raw git diffs or concise English summaries) at inference; and (C) Incremental Fine-Tuning (Inc-FT) on delta-derived training sets, with carefully controlled NEW:OLD mixing to mitigate catastrophic forgetting. We contribute an alias-aware evaluation protocol that credits rename while never rewarding deleted paths, and a practical Forgetting Probe that quantifies residual emissions of obsolete paths. Across Flask, SQLAlchemy, Pandas, and Poetry, Inc-FT with old-aware mixes delivers the best overall balance on mixed sets, ICL with English delta summaries delivers the fastest new-code lift when training is not feasible, and Full Refresh remains the ceiling when maximum NEW accuracy matters. We also compare Git-diff Inc-FT to full-file Inc-FT, showing that diffs excel in rename/delete-heavy windows while full-file context wins in behavior-change-heavy windows.

</details>


### [166] [LogPurge: Log Data Purification for Anomaly Detection via Rule-Enhanced Filtering](https://arxiv.org/abs/2511.14062)
*Shenglin Zhang,Ziang Chen,Zijing Que,Yilun Liu,Yongqian Sun,Sicheng Wei,Dan Pei,Hailin Li*

Main category: cs.SE

TL;DR: 本文提出成本感知、规则增强的净化框架LogPurge用于日志异常检测，通过两阶段过滤算法净化日志数据，实验表明该方法能有效去除异常并提升F - 1分数。


<details>
  <summary>Details</summary>
Motivation: 现代日志异常检测方法依赖无异常的干净日志数据训练模型，但获取此类数据需人工标注成本高，且现有自动净化方法未充分结合日志特性和语义。

Method: 提出LogPurge框架，采用两阶段过滤算法，第一阶段用大语言模型去除聚类异常模式并增强系统规则；第二阶段采用分治策略分解剩余污染区域。

Result: 在两个公开数据集和一个工业数据集上实验，平均去除98.74%的异常，保留82.39%的正常样本，F - 1分数相比最新无监督日志样本选择算法有显著提升。

Conclusion: 所提LogPurge框架在日志异常检测中有效，能去除异常并提升性能。

Abstract: Log anomaly detection, which is critical for identifying system failures and preempting security breaches, detects irregular patterns within large volumes of log data, and impacts domains such as service reliability, performance optimization, and database log analysis. Modern log anomaly detection methods rely on training deep learning models on clean, anomaly-free log sequences. However, obtaining such clean log data requires costly and tedious human labeling, and existing automatic cleaning methods fail to fully integrate the specific characteristics and actual semantics of logs in their purification process. In this paper, we propose a cost-aware, rule-enhanced purification framework, LogPurge, that automatically selects a sufficient subset of normal log sequences from contamination log sequences to train a anomaly detection model. Our approach involves a two-stage filtering algorithm: In the first stage, we use a large language model (LLM) to remove clustered anomalous patterns and enhance system rules to improve LLM's understanding of system logs; in the second stage, we utilize a divide-and-conquer strategy that decomposes the remaining contaminated regions into smaller subproblems, allowing each to be effectively purified through the first stage procedure. Our experiments, conducted on two public datasets and one industrial dataset, show that our method significantly removes an average of 98.74% of anomalies while retaining 82.39% of normal samples. Compared to the latest unsupervised log sample selection algorithms, our method achieves F-1 score improvements of 35.7% and 84.11% on the public datasets, and an impressive 149.72% F-1 improvement on the private dataset, demonstrating the effectiveness of our approach.

</details>


### [167] [A Practical Implementation of Customized Scrum-Based Agile Framework in Aerospace Software Development Under DO-178C Constraints](https://arxiv.org/abs/2511.14215)
*Malik Muhammad Umer*

Main category: cs.SE

TL;DR: 本文提出适用于DO - 178C合规的安全关键航空航天软件的Scrum敏捷框架，经项目评估效果显著，虽有挑战但有提升机会，建议后续扩大验证范围。


<details>
  <summary>Details</summary>
Motivation: 航空航天系统复杂度增加，需平衡敏捷性与安全认证要求的开发流程。

Method: 提出基于Scrum的敏捷框架，对核心角色、工件和事件进行调整，通过两个对比项目（敏捷与瀑布模型）评估。

Result: 敏捷框架使每个需求总工作量减少76%，缺陷检测速度提高75%，缺陷解决速度提高78%，缺陷密度降低超50%，且符合DO - 178C设计保证等级A。

Conclusion: 经合理调整和积极与认证机构合作，敏捷实践与法规合规可有效共存，同时指出挑战和提升机会，建议扩大框架验证范围。

Abstract: The increasing complexity of aerospace systems requires development processes that balance agility with stringent safety and certification demands. This study presents an empirically validated Scrum-based Agile framework tailored for DO-178C compliant, safety-critical aerospace software. The framework adapts core Scrum roles, artifacts, and events to meet certification, verification, and independence objectives. Key enhancements include a multi-disciplinary product ownership model, dual compliance-and-functionality acceptance criteria, independent testing and documentation teams, and dedicated certification liaisons. The approach was evaluated through two comparable aerospace projects-one using the customized Agile process and the other a traditional Waterfall model. Results showed significant improvements: a 76% reduction in Total Effort per Requirement, 75% faster Defect Detection, 78% faster Defect Resolution, and over 50% lower Defect Density, while maintaining full compliance with DO-178C Design Assurance Level A. These findings demonstrate that Agile practices and regulatory compliance can coexist effectively when supported by disciplined tailoring and proactive engagement with certification authorities. The study also notes challenges, including increased V&V effort due to recurring Sprint activities and refactoring inherent to iterative development. Nonetheless, it identifies substantial opportunities for further gains through workflow automation, CI/CD practices, and automated documentation, verification, and configuration management. Future research should expand validation of this framework across the aerospace domain and other safety-critical industries with similar certification requirements.

</details>


### [168] [KTester: Leveraging Domain and Testing Knowledge for More Effective LLM-based Test Generation](https://arxiv.org/abs/2511.14224)
*Anji Li,Mingwei Liu,Zhenxi Chen,Zheng Pei,Zike Li,Dekun Dai,Yanlin Wang,Zibin Zheng*

Main category: cs.SE

TL;DR: 本文提出KTester框架，结合项目和测试领域知识改进基于大语言模型的单元测试生成，实验表明其在多指标上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的自动化单元测试生成在实际项目中难以生成正确且可维护的测试。

Method: 先通过静态分析提取项目结构和使用知识提供上下文，采用测试领域知识引导的测试用例设计与测试方法生成分离，结合多视角提示策略，生成遵循结构化模板的测试。

Result: KTester在六个关键指标上显著优于现有方法，执行通过率提高5.69%，行覆盖率提高8.83%，耗时更少、生成测试用例更少，人工评估中测试的正确性、可读性和可维护性更高。

Conclusion: 知识驱动的KTester框架具有实际优势。

Abstract: Automated unit test generation using large language models (LLMs) holds great promise but often struggles with generating tests that are both correct and maintainable in real-world projects. This paper presents KTester, a novel framework that integrates project-specific knowledge and testing domain knowledge to enhance LLM-based test generation. Our approach first extracts project structure and usage knowledge through static analysis, which provides rich context for the model. It then employs a testing-domain-knowledge-guided separation of test case design and test method generation, combined with a multi-perspective prompting strategy that guides the LLM to consider diverse testing heuristics. The generated tests follow structured templates, improving clarity and maintainability. We evaluate KTester on multiple open-source projects, comparing it against state-of-the-art LLM-based baselines using automatic correctness and coverage metrics, as well as a human study assessing readability and maintainability. Results demonstrate that KTester significantly outperforms existing methods across six key metrics, improving execution pass rate by 5.69% and line coverage by 8.83% over the strongest baseline, while requiring less time and generating fewer test cases. Human evaluators also rate the tests produced by KTester significantly higher in terms of correctness, readability, and maintainability, confirming the practical advantages of our knowledge-driven framework.

</details>


### [169] [How Does Cognitive Capability and Personality Influence Problem-Solving in Coding Interview Puzzles?](https://arxiv.org/abs/2511.14367)
*Dulaji Hidellaarachchi,Sebastian Baltes,John Grundy*

Main category: cs.SE

TL;DR: 研究80名参与者（40名从业者和40名学生）的认知能力与人格特质对软件问题解决的影响，发现从业者表现略好，认知能力与问题解决表现正相关，部分人格特质有显著关联，为教育和行业提供启示。


<details>
  <summary>Details</summary>
Motivation: 探究认知能力和人格特质如何共同影响软件问题解决。

Method: 用Baddeleys三分钟语法推理测试测量认知能力，用IPIP NEO 50测试评估人格，让参与者完成9个面试式问题解决问题。

Result: 从业者语法推理准确性和整体任务表现略高于学生；语法推理准确性与问题解决表现正相关；尽责性与问题解决和推理准确性相关性最强，经验开放性与两者正相关，神经质与准确性和表现呈小的负相关。

Conclusion: 尽责性和经验开放性特质与推理准确性互补支持软件问题解决，消极情绪可能阻碍表现；为教育和行业提供实践启示，指出未来研究方向。

Abstract: Software engineering is a deeply cognitive activity shaped by individual differences that extend beyond technical skill. This study investigates how cognitive capability and personality traits jointly relate to software problem solving among 80 participants (40 software practitioners, 40 software engineering students). Cognitive capability was measured using Baddeleys three minute grammatical reasoning test, while personality was assessed using the IPIP NEO 50 test. Participants further completed nine interview style problem solving questions. Six questions were related to coding and three were related to logical reasoning. Descriptive and correlational analyses show that practitioners achieved slightly higher grammatical reasoning accuracy and overall task performance than students. Grammatical-reasoning accuracy correlated positively with problem solving performance, indicating that stronger cognitive capability is associated with better performance in coding and logical tasks. Personality performance links were systematic. We identified that the conscientiousness trait correlated most strongly with problem solving and with reasoning accuracy, while the openness to experience trait was positively related to both outcomes. Neuroticism showed small, negative associations with accuracy and performance. Taken together, our results suggest that conscientiousness and openness to experience characteristics complement reasoning accuracy to support software problem solving, whereas elevated negative affect may hinder precision under time pressure. Our findings suggest practical implications for education and industry such as integrating structured reasoning tasks in curricula, and considering personality cognition in recruitment and role allocation. We highlight directions for future research such as longitudinal and task diverse replications with larger samples.

</details>


### [170] [Watchdogs and Oracles: Runtime Verification Meets Large Language Models for Autonomous Systems](https://arxiv.org/abs/2511.14435)
*Angelo Ferrando*

Main category: cs.SE

TL;DR: 本文提出将运行时验证（RV）与大语言模型（LLMs）进行共生集成，以实现可靠的自主性，并探讨挑战、认证影响及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 确保涉及学习组件和开放环境的自主系统的安全性和可信度存在困难，形式化方法有局限，LLMs 缺乏形式保证，因此寻求两者的共生集成。

Method: 提出将 RV 与 LLMs 进行共生集成，RV 为 LLM 驱动的自主性提供保障，LLMs 扩展 RV 的功能。

Result: 无明确提及具体结果

Conclusion: 这种共生集成与现有研究不同，需探讨挑战和认证影响，明确了未来实现可靠自主性的研究方向。

Abstract: Assuring the safety and trustworthiness of autonomous systems is particularly difficult when learning-enabled components and open environments are involved. Formal methods provide strong guarantees but depend on complete models and static assumptions. Runtime verification (RV) complements them by monitoring executions at run time and, in its predictive variants, by anticipating potential violations. Large language models (LLMs), meanwhile, excel at translating natural language into formal artefacts and recognising patterns in data, yet they remain error-prone and lack formal guarantees. This vision paper argues for a symbiotic integration of RV and LLMs. RV can serve as a guardrail for LLM-driven autonomy, while LLMs can extend RV by assisting specification capture, supporting anticipatory reasoning, and helping to handle uncertainty. We outline how this mutual reinforcement differs from existing surveys and roadmaps, discuss challenges and certification implications, and identify future research directions towards dependable autonomy.

</details>


### [171] [LLM-Assisted Thematic Analysis: Opportunities, Limitations, and Recommendations](https://arxiv.org/abs/2511.14528)
*Tatiane Ornelas,Allysson Allex Araújo,Júlia Araújo,Marina Araújo,Bianca Trinkenreich,Marcos Kalinowski*

Main category: cs.SE

TL;DR: 研究探讨软件工程定性研究中整合大语言模型到主题分析的机遇、风险和方法学影响，发现其可支持但不能替代分析。


<details>
  <summary>Details</summary>
Motivation: 大语言模型用于软件工程定性研究的方法学影响未充分探索，其融入主题分析带来严谨性等问题，故研究经验丰富的研究者如何看待相关机遇、风险和方法学影响。

Method: 组织25名ISERN研究者进行反思性研讨会，通过结构化讨论大语言模型辅助的开放编码、主题生成和主题审查，用彩色画布记录感知到的机遇、局限和建议。

Result: 参与者认可效率和可扩展性提升，但强调偏见、上下文丢失、可重复性及模型快速演变等风险，还强调提示素养和持续人工监督的必要性。

Conclusion: 大语言模型可支持但不能替代解释性分析，研究有助于社区反思如何负责任地利用其提升软件工程定性研究。

Abstract: [Context] Large Language Models (LLMs) are increasingly used to assist qualitative research in Software Engineering (SE), yet the methodological implications of this usage remain underexplored. Their integration into interpretive processes such as thematic analysis raises fundamental questions about rigor, transparency, and researcher agency. [Objective] This study investigates how experienced SE researchers conceptualize the opportunities, risks, and methodological implications of integrating LLMs into thematic analysis. [Method] A reflective workshop with 25 ISERN researchers guided participants through structured discussions of LLM-assisted open coding, theme generation, and theme reviewing, using color-coded canvases to document perceived opportunities, limitations, and recommendations. [Results] Participants recognized potential efficiency and scalability gains, but highlighted risks related to bias, contextual loss, reproducibility, and the rapid evolution of LLMs. They also emphasized the need for prompting literacy and continuous human oversight. [Conclusion] Findings portray LLMs as tools that can support, but not substitute, interpretive analysis. The study contributes to ongoing community reflections on how LLMs can responsibly enhance qualitative research in SE.

</details>


### [172] [FHIRconnect: Towards a seamless integration of openEHR and FHIR](https://arxiv.org/abs/2511.14618)
*Severin Kohler,Jordi Piera Jiménez,Michael Anywar,Lars Fuhrmann,Heather Leslie,Maximilian Meixner,Julian Saß,Florian Kärcher,Diego Boscá,Birger Haarbrandt,Michael Marschollek,Roland Eils*

Main category: cs.SE

TL;DR: 本文提出FHIRconnect，实现openEHR与FHIR标准化双向数据交换，解决互操作性问题，建立映射标准化技术基础。


<details>
  <summary>Details</summary>
Motivation: openEHR和HL7 FHIR因数据建模方法差异和缺乏标准转换机制，医疗互操作性面临挑战。

Method: 采用三层架构的FHIRconnect，利用基于国际原型的基础，支持本地定制。

Result: 成功将24个国际原型映射到7个临床领域的15个FHIR配置文件，实现65%的映射跨项目重用。

Conclusion: 建立了社区驱动映射标准化的技术基础，减少对自定义ETL解决方案的依赖，推进医疗IT系统的语法和语义互操作性。

Abstract: Healthcare interoperability between openEHR and HL7 FHIR remains challenging due to fundamental differences in their data modeling approaches and the absence of standardized transformation mechanisms. This paper presents FHIRconnect, a novel domain-specific language and open-source transformation engine that enables standardized, bidirectional data exchange between openEHR and FHIR. Our approach addresses critical interoperability gaps through a triple-layered architecture that achieves 65% mapping reuse across projects by leveraging international archetype-based foundations while supporting local customizations. Using this framework, FHIRconnect successfully mapped 24 international archetypes to 15 FHIR profiles across seven clinical domains. Key contributions include the first comprehensive DSL for openEHR-FHIR transformation with a formal specification, an open-source execution engine (openFHIR), and an accessible mapping library covering high-impact clinical archetypes. Together, these components establish the technical basis for community-driven mapping standardization, reducing reliance on custom ETL solutions and advancing syntactic and semantic interoperability in healthcare IT systems built on open standards.

</details>


### [173] [Why Do We Code? A Theory on Motivations and Challenges in Software Engineering from Education to Practice](https://arxiv.org/abs/2511.14711)
*Aaliyah Chang,Mariam Guizani,Brittany Johnson*

Main category: cs.SE

TL;DR: 本文通过访谈和Gioia方法研究软件工程领域从教育到职业实践中动机与挑战的相互作用，构建EPE模型，为设计干预措施提供依据。


<details>
  <summary>Details</summary>
Motivation: 探索软件工程领域从教育到职业实践中动机与挑战的相互作用，该领域此前研究不足。

Method: 进行15次半结构化访谈，采用Gioia方法归纳动机和挑战的分类法，构建EPE过程模型。

Result: 发现早期有效接触触发内在动机，无效接触需外在推动；确定好奇心和避免其他选择为教育驱动因素，归属感障碍是跨阶段挑战；揭示职业发展和技术培训等挑战对内外在满足感的制约。

Conclusion: 研究结果为软件工程教育和实践中增强内在满足感、减少系统性障碍的干预措施设计提供了理论模型。

Abstract: Motivations and challenges jointly shape how individuals enter, persist, and evolve within software engineering (SE), yet their interplay remains underexplored across the transition from education to professional practice. We conducted 15 semi-structured interviews and employed the Gioia Methodology, an adapted grounded theory methodology from organizational behavior, to inductively derive taxonomies of motivations and challenges, and build the Exposure-Pursuit-Evaluation (EPE) Process Model. Our findings reveal that impactful early exposure triggers intrinsic motivations, while non-impactful exposure requires an extrinsic push (e.g., career/ personal goals, external validation). We identify curiosity and avoiding alternatives as a distinct educational drivers, and barriers to belonging as the only challenge persisting across education and career. Our findings show that career progression challenges (e.g., navigating the corporate world) constrain extrinsic fulfillment while technical training challenges, barriers to belonging and threats to motivation constrain intrinsic fulfillment. The theory shows how unmet motivations and recurring challenges influence persistence, career shifts, or departure from the field. Our results provide a grounded model for designing interventions that strengthen intrinsic fulfillment and reduce systemic barriers in SE education and practice.

</details>


### [174] [From Legacy Fortran to Portable Kokkos: An Autonomous Agentic AI Workflow](https://arxiv.org/abs/2509.12443)
*Sparsh Gupta,Kamalavasan Kamalakkannan,Maxim Moraru,Galen Shipman,Patrick Diehl*

Main category: cs.SE

TL;DR: 本文提出一种代理式AI工作流，将Fortran内核转换为可移植的Kokkos C++程序，展示了其可行性及LLM驱动系统在科学应用的潜力。


<details>
  <summary>Details</summary>
Motivation: 高性能计算向异构GPU加速架构转变，许多加速器缺乏原生Fortran绑定，手动将Fortran移植到Kokkos耗时且需专业知识，LLM在并行代码翻译优化的自动工作流应用待探索。

Method: 使用专门的LLM“代理”协作完成Fortran内核到Kokkos C++程序的翻译、验证、编译、运行、测试、调试和优化。

Result: 该工作流能将一系列基准内核现代化，生成跨硬件分区的性能可移植Kokkos代码；付费OpenAI模型成本低且生成代码超Fortran基线，开源模型常失败。

Conclusion: 代理式AI用于Fortran到Kokkos转换可行，为遗留科学应用在不同超级计算机上高效运行提供途径，凸显LLM驱动代理系统在科学和系统应用中进行特定领域推理任务的潜力。

Abstract: Scientific applications continue to rely on legacy Fortran codebases originally developed for homogeneous, CPU-based systems. As High-Performance Computing (HPC) shifts toward heterogeneous GPU-accelerated architectures, many accelerators lack native Fortran bindings, creating an urgent need to modernize legacy codes for portability. Frameworks like Kokkos provide performance portability and a single-source C++ abstraction, but manual Fortran-to-Kokkos porting demands significant expertise and time. Large language models (LLMs) have shown promise in source-to-source code generation, yet their use in fully autonomous workflows for translating and optimizing parallel code remains largely unexplored, especially for performance portability across diverse hardware. This paper presents an agentic AI workflow where specialized LLM "agents" collaborate to translate, validate, compile, run, test, debug, and optimize Fortran kernels into portable Kokkos C++ programs. Results show the pipeline modernizes a range of benchmark kernels, producing performance-portable Kokkos codes across hardware partitions. Paid OpenAI models such as GPT-5 and o4-mini-high executed the workflow for only a few U.S. dollars, generating optimized codes that surpassed Fortran baselines, whereas open-source models like Llama4-Maverick often failed to yield functional codes. This work demonstrates the feasibility of agentic AI for Fortran-to-Kokkos transformation and offers a pathway for autonomously modernizing legacy scientific applications to run portably and efficiently on diverse supercomputers. It further highlights the potential of LLM-driven agentic systems to perform structured, domain-specific reasoning tasks in scientific and systems-oriented applications.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [175] [Opportunity Cost in Insurance](https://arxiv.org/abs/2511.13959)
*Jan Maelger*

Main category: q-fin.ST

TL;DR: 开发保险利润优化形式主义，确定最优资产配置并推导机会成本。


<details>
  <summary>Details</summary>
Motivation: 在监管和风险政策相关要求下实现保险在保业务的利润优化。

Method: 开发适用于多种保险业务和监管框架的形式主义方法。

Result: 确定了预定义风险偏好内的最优资产配置，推导出保险公司面临的年度机会成本。

Conclusion: 所开发的方法可用于保险业务在相关约束下的利润优化。

Abstract: We develop a formalism for insurance profit optimisation for the in-force business constraint by regulatory and risk policy related requirements. This approach is applicable to Life, P&C and Reinsurance businesses and applies in all regulatory frameworks with a solvency requirement defined in the form of a solvency ratio, notably Solvency II and the Swiss Solvency Test. We identify the optimal asset allocation for profit maximisation within a pre-defined risk appetite and deduce the annual opportunity cost faced by the insurance company.

</details>


### [176] [The Hidden Constant of Market Rhythms: How $1-1/e$ Defines Scaling in Intrinsic Time](https://arxiv.org/abs/2511.14408)
*Thomas Houweling*

Main category: q-fin.ST

TL;DR: 文章指出可以将内在时间建模为无记忆指数风险过程，实证发现方向变化比例稳定在0.632，支持市场活动是内在时间更新过程的解释。


<details>
  <summary>Details</summary>
Motivation: 探寻市场微观结构中缩放定律稳定性的起源。

Method: 将内在时间建模为无记忆指数风险过程，并进行实证分析。

Result: 方向变化占总事件的比例稳定在1 - 1/e = 0.632，与泊松过程完成一个平均间隔的概率相符。

Conclusion: 该常数可用于识别跨阈值的缩放机制，支持将市场活动解释为内在时间的更新过程。

Abstract: Directional-change Intrinsic Time analysis has long revealed scaling laws in market microstructure, but the origin of their stability remains elusive. This article presents evidence that Intrinsic Time can be modeled as a memoryless exponential hazard process. Empirically, the proportion of directional changes to total events stabilizes near $1 - 1/e = 0.632$, matching the probability that a Poisson process completes one mean interval. This constant provides a natural heuristic to identify scaling regimes across thresholds and supports an interpretation of market activity as a renewal process in intrinsic time.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [177] [Knowledge vs. Experience: Asymptotic Limits of Impatience in Edge Tenants](https://arxiv.org/abs/2511.13763)
*Anthony Kiggundu,Bin Han,Hans D. Schotten*

Main category: stat.ML

TL;DR: 研究两种信息反馈对双M/M/1系统中顾客离开和换队行为的影响，分析和实证表明有限规模时有差异，渐近时结果相同。


<details>
  <summary>Details</summary>
Motivation: 探究两种信息反馈（闭式马尔可夫剩余逗留时间估计器和在线训练的演员 - 评论家）对双M/M/1系统中顾客离开和换队行为的影响。

Method: 进行理论分析，推导在不等服务率和总时间耐心下的情况；在温和的次线性误差条件下分析渐近极限；进行实证验证。

Result: 分析得出总等待时间线性增长，换队成功概率在积压趋近无穷时消失；两种信息模型渐近极限相同；有限积压时两种信息反馈产生不同延迟、离开率和换队行为。

Conclusion: 明确了信息价值在有限规模和渐近情况下是否重要，为低成本、考虑换队的系统的轻量级遥测和决策逻辑设计提供依据。

Abstract: We study how two information feeds, a closed-form Markov estimator of residual sojourn and an online trained actor-critic, affect reneging and jockeying in a dual M/M/1 system. Analytically, for unequal service rates and total-time patience, we show that total wait grows linearly so abandonment is inevitable and the probability of a successful jockey vanishes as the backlog approaches towards infinity. Furthermore, under a mild sub-linear error condition both information models yield the same asymptotic limits (robustness). We empirically validate these limits and quantify finite backlog differences. Our findings show that learned and analytic feeds produce different delays, reneging rates and transient jockeying behavior at practical sizes, but converge to the same asymptotic outcome implied by our theory. The results characterize when value-of-information matters (finite regimes) and when it does not (asymptotics), informing lightweight telemetry and decision-logic design for low-cost, jockeying-aware systems.

</details>


### [178] [Uncertainty-Calibrated Prediction of Randomly-Timed Biomarker Trajectories with Conformal Bands](https://arxiv.org/abs/2511.13911)
*Vasiliki Tassopoulou,Charis Stamouli,Haochang Shou,George J. Pappas,Christos Davatzikos*

Main category: stat.ML

TL;DR: 提出用于生物标志物轨迹不确定性校准预测的共形方法，应用于阿尔茨海默病生物标志物预测，结果显示能达预期覆盖度且带更窄，还开发组条件共形带并展示临床实用性。


<details>
  <summary>Details</summary>
Motivation: 现有生物标志物轨迹预测的不确定性带来高风险，限制临床应用，需实现其在医疗中的安全可靠使用。

Method: 引入共形方法，通过新的非一致性分数将共形预测拓展到随机时间轨迹设置，开发组条件共形带，引入不确定性校准风险分数。

Result: 共形预测带达预期覆盖度且比基线带更窄，组条件共形带在各亚群测试覆盖保证，不确定性校准风险分数能多识别17.5%高风险受试者。

Conclusion: 该方法在生物标志物轨迹预测中有效，不确定性校准对临床决策有价值，代码开源。

Abstract: Despite recent progress in predicting biomarker trajectories from real clinical data, uncertainty in the predictions poses high-stakes risks (e.g., misdiagnosis) that limit their clinical deployment. To enable safe and reliable use of such predictions in healthcare, we introduce a conformal method for uncertainty-calibrated prediction of biomarker trajectories resulting from randomly-timed clinical visits of patients. Our approach extends conformal prediction to the setting of randomly-timed trajectories via a novel nonconformity score that produces prediction bands guaranteed to cover the unknown biomarker trajectories with a user-prescribed probability. We apply our method across a wide range of standard and state-of-the-art predictors for two well-established brain biomarkers of Alzheimer's disease, using neuroimaging data from real clinical studies. We observe that our conformal prediction bands consistently achieve the desired coverage, while also being tighter than baseline prediction bands. To further account for population heterogeneity, we develop group-conditional conformal bands and test their coverage guarantees across various demographic and clinically relevant subpopulations. Moreover, we demonstrate the clinical utility of our conformal bands in identifying subjects at high risk of progression to Alzheimer's disease. Specifically, we introduce an uncertainty-calibrated risk score that enables the identification of 17.5% more high-risk subjects compared to standard risk scores, highlighting the value of uncertainty calibration in real-world clinical decision making. Our code is available at github.com/vatass/ConformalBiomarkerTrajectories.

</details>


### [179] [Empirical Likelihood for Random Forests and Ensembles](https://arxiv.org/abs/2511.13934)
*Harold D. Chiang,Yukitoshi Matsushita,Taisuke Otsu*

Main category: stat.ML

TL;DR: 开发随机森林及相关集成方法的经验似然框架，提出修正方法，理论和模拟表明其优于现有推理方法。


<details>
  <summary>Details</summary>
Motivation: 为随机森林及相关集成方法提供基于似然的统计不确定性量化方法。

Method: 利用集成预测中固有的不完整U统计量结构构建经验似然统计量，在稀疏子采样下提出修正方法。

Result: 修正的经验似然方法能实现准确覆盖，具有实用可靠性。

Conclusion: 所提方法保留经验似然关键性质，计算高效，优于现有推理方法。

Abstract: We develop an empirical likelihood (EL) framework for random forests and related ensemble methods, providing a likelihood-based approach to quantify their statistical uncertainty. Exploiting the incomplete $U$-statistic structure inherent in ensemble predictions, we construct an EL statistic that is asymptotically chi-squared when subsampling induced by incompleteness is not overly sparse. Under sparser subsampling regimes, the EL statistic tends to over-cover due to loss of pivotality; we therefore propose a modified EL that restores pivotality through a simple adjustment. Our method retains key properties of EL while remaining computationally efficient. Theory for honest random forests and simulations demonstrate that modified EL achieves accurate coverage and practical reliability relative to existing inference methods.

</details>


### [180] [Splat Regression Models](https://arxiv.org/abs/2511.14042)
*Mara Daniels,Philippe Rigollet*

Main category: stat.ML

TL;DR: 介绍了Splat回归模型，其输出是splat函数混合，可通过Wasserstein - Fisher - Rao梯度流拟合，恢复高斯Splatting方法，实验证明对低维数据问题有效。


<details>
  <summary>Details</summary>
Motivation: 提出一种高表达性的函数逼近器，实现高可解释性和准确性，并为高斯Splatting方法提供统一理论框架。

Method: 使用Wasserstein - Fisher - Rao梯度流对混合测度空间进行优化来拟合splat模型。

Result: 恢复了高斯Splatting方法作为特殊情况，得到了统一理论框架。

Conclusion: 所提出的模型和算法是解决低维数据相关逼近、估计和逆问题的灵活且有前景的方法。

Abstract: We introduce a highly expressive class of function approximators called Splat Regression Models. Model outputs are mixtures of heterogeneous and anisotropic bump functions, termed splats, each weighted by an output vector. The power of splat modeling lies in its ability to locally adjust the scale and direction of each splat, achieving both high interpretability and accuracy. Fitting splat models reduces to optimization over the space of mixing measures, which can be implemented using Wasserstein-Fisher-Rao gradient flows. As a byproduct, we recover the popular Gaussian Splatting methodology as a special case, providing a unified theoretical framework for this state-of-the-art technique that clearly disambiguates the inverse problem, the model, and the optimization algorithm. Through numerical experiments, we demonstrate that the resulting models and algorithms constitute a flexible and promising approach for solving diverse approximation, estimation, and inverse problems involving low-dimensional data.

</details>


### [181] [SCOPE: Spectral Concentration by Distributionally Robust Joint Covariance-Precision Estimation](https://arxiv.org/abs/2511.14146)
*Renjie Chen,Viet Anh Nguyen,Huifu Xu*

Main category: stat.ML

TL;DR: 提出分布鲁棒公式同时估计随机向量协方差和精度矩阵，可化为凸优化问题，得非线性收缩估计器SCOPE，改善条件数，有调参方案，数值实验表现好。


<details>
  <summary>Details</summary>
Motivation: 同时估计随机向量的协方差矩阵和精度矩阵，减少经验协方差/精度矩阵估计器的光谱偏差。

Method: 提出分布鲁棒公式，最小化最坏情况下协方差估计器的Frobenius损失和精度矩阵估计器的Stein损失的加权和，通过凸谱散度测量模糊集半径，将模型转化为凸优化问题。

Result: 得到准解析估计器，联合估计器为非线性收缩估计器，改善估计器条件数，有渐近最优调参方案。

Conclusion: 提出的SCOPE估计器在实际应用中与最先进的估计器相比具有竞争力。

Abstract: We propose a distributionally robust formulation for simultaneously estimating the covariance matrix and the precision matrix of a random vector.The proposed model minimizes the worst-case weighted sum of the Frobenius loss of the covariance estimator and Stein's loss of the precision matrix estimator against all distributions from an ambiguity set centered at the nominal distribution. The radius of the ambiguity set is measured via convex spectral divergence. We demonstrate that the proposed distributionally robust estimation model can be reduced to a convex optimization problem, thereby yielding quasi-analytical estimators. The joint estimators are shown to be nonlinear shrinkage estimators. The eigenvalues of the estimators are shrunk nonlinearly towards a positive scalar, where the scalar is determined by the weight coefficient of the loss terms. By tuning the coefficient carefully, the shrinkage corrects the spectral bias of the empirical covariance/precision matrix estimator. By this property, we call the proposed joint estimator the Spectral concentrated COvariance and Precision matrix Estimator (SCOPE). We demonstrate that the shrinkage effect improves the condition number of the estimator. We provide a parameter-tuning scheme that adjusts the shrinkage target and intensity that is asymptotically optimal. Numerical experiments on synthetic and real data show that our shrinkage estimators perform competitively against state-of-the-art estimators in practical applications.

</details>


### [182] [Causal Discovery on Higher-Order Interactions](https://arxiv.org/abs/2511.14206)
*Alessio Zanga,Marco Scutari,Fabio Stella*

Main category: stat.ML

TL;DR: 提出基于高阶结构的DAG聚合新理论框架和算法，模拟研究显示其在低样本和高维情况下优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有DAG聚合仅考虑单个边置信度，忽略高阶边结构，本文旨在改进这一问题。

Method: 引入基于高阶结构的理论框架并描述新的DAG聚合算法，进行模拟研究。

Result: 该方法计算高效且有效，尤其在低样本和高维设置下优于现有解决方案。

Conclusion: 提出的方法在因果发现的DAG聚合中具有优势，可用于解决数据稀缺时的问题。

Abstract: Causal discovery combines data with knowledge provided by experts to learn the DAG representing the causal relationships between a given set of variables. When data are scarce, bagging is used to measure our confidence in an average DAG obtained by aggregating bootstrapped DAGs. However, the aggregation step has received little attention from the specialized literature: the average DAG is constructed using only the confidence in the individual edges of the bootstrapped DAGs, thus disregarding complex higher-order edge structures. In this paper, we introduce a novel theoretical framework based on higher-order structures and describe a new DAG aggregation algorithm. We perform a simulation study, discussing the advantages and limitations of the proposed approach. Our proposal is both computationally efficient and effective, outperforming state-of-the-art solutions, especially in low sample size regimes and under high dimensionality settings.

</details>


### [183] [Skewness-Robust Causal Discovery in Location-Scale Noise Models](https://arxiv.org/abs/2511.14441)
*Daniel Klippert,Alexander Marx*

Main category: stat.ML

TL;DR: 本文提出SkewD算法用于双变量因果发现，能在对称和偏态噪声下可靠推断，实验表现好且在高偏度下稳健。


<details>
  <summary>Details</summary>
Motivation: 现有方法在噪声为偏态随机变量时可靠性降低，为解决此局限提出新算法。

Method: 提出基于似然的SkewD算法，将通常的正态分布框架扩展到偏态正态设置，参数估计采用启发式搜索和期望条件最大化算法结合。

Result: 在新合成的偏态噪声数据集和已有的基准数据集上评估，SkewD表现出色，与先前工作相比在高偏度下更稳健。

Conclusion: SkewD算法能有效解决现有方法在偏态噪声下可靠性问题，可在对称和偏态噪声下进行可靠因果推断。

Abstract: To distinguish Markov equivalent graphs in causal discovery, it is necessary to restrict the structural causal model. Crucially, we need to be able to distinguish cause $X$ from effect $Y$ in bivariate models, that is, distinguish the two graphs $X \to Y$ and $Y \to X$. Location-scale noise models (LSNMs), in which the effect $Y$ is modeled based on the cause $X$ as $Y = f(X) + g(X)N$, form a flexible class of models that is general and identifiable in most cases. Estimating these models for arbitrary noise terms $N$, however, is challenging. Therefore, practical estimators are typically restricted to symmetric distributions, such as the normal distribution. As we showcase in this paper, when $N$ is a skewed random variable, which is likely in real-world domains, the reliability of these approaches decreases. To approach this limitation, we propose SkewD, a likelihood-based algorithm for bivariate causal discovery under LSNMs with skewed noise distributions. SkewD extends the usual normal-distribution framework to the skew-normal setting, enabling reliable inference under symmetric and skewed noise. For parameter estimation, we employ a combination of a heuristic search and an expectation conditional maximization algorithm. We evaluate SkewD on novel synthetically generated datasets with skewed noise as well as established benchmark datasets. Throughout our experiments, SkewD exhibits a strong performance and, in comparison to prior work, remains robust under high skewness.

</details>


### [184] [DeepBlip: Estimating Conditional Average Treatment Effects Over Time](https://arxiv.org/abs/2511.14545)
*Haorui Ma,Dennis Frauen,Stefan Feuerriegel*

Main category: stat.ML

TL;DR: 提出首个用于结构嵌套均值模型（SNMMs）的神经框架DeepBlip，在临床数据集上达SOTA性能。


<details>
  <summary>Details</summary>
Motivation: SNMMs缺乏神经框架，其固有的顺序g估计方案阻碍端到端基于梯度的训练。

Method: 提出DeepBlip，用双优化技巧克服限制，集成LSTM或transformer等顺序神经网络，使用Neyman正交损失函数。

Result: 在各种临床数据集上评估，DeepBlip达到了最先进的性能。

Conclusion: DeepBlip是有效的SNMMs神经框架，能处理复杂时间依赖，产生无偏估计，对干扰模型误设具有鲁棒性。

Abstract: Structural nested mean models (SNMMs) are a principled approach to estimate the treatment effects over time. A particular strength of SNMMs is to break the joint effect of treatment sequences over time into localized, time-specific ``blip effects''. This decomposition promotes interpretability through the incremental effects and enables the efficient offline evaluation of optimal treatment policies without re-computation. However, neural frameworks for SNMMs are lacking, as their inherently sequential g-estimation scheme prevents end-to-end, gradient-based training. Here, we propose DeepBlip, the first neural framework for SNMMs, which overcomes this limitation with a novel double optimization trick to enable simultaneous learning of all blip functions. Our DeepBlip seamlessly integrates sequential neural networks like LSTMs or transformers to capture complex temporal dependencies. By design, our method correctly adjusts for time-varying confounding to produce unbiased estimates, and its Neyman-orthogonal loss function ensures robustness to nuisance model misspecification. Finally, we evaluate our DeepBlip across various clinical datasets, where it achieves state-of-the-art performance.

</details>


### [185] [Towards a Unified Analysis of Neural Networks in Nonparametric Instrumental Variable Regression: Optimization and Generalization](https://arxiv.org/abs/2511.14710)
*Zonghao Chen,Atsushi Nitanda,Arthur Gretton,Taiji Suzuki*

Main category: stat.ML

TL;DR: 本文建立了非参数工具变量回归中神经网络用于两阶段最小二乘法的全局收敛结果，提出新算法F²BMLD并给出泛化界，最后在基准测试上验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 为非参数工具变量回归的两阶段最小二乘法建立神经网络的全局收敛结果。

Method: 采用平均场朗之万动力学的提升视角，利用惩罚梯度方法将双层优化问题转化为拉格朗日问题，提出F²BMLD算法。

Result: 得到收敛界和泛化界，揭示拉格朗日乘子在优化和统计保证之间的权衡。

Conclusion: 所提方法在离线强化学习基准测试上有效。

Abstract: We establish the first global convergence result of neural networks for two stage least squares (2SLS) approach in nonparametric instrumental variable regression (NPIV). This is achieved by adopting a lifted perspective through mean-field Langevin dynamics (MFLD), unlike standard MFLD, however, our setting of 2SLS entails a \emph{bilevel} optimization problem in the space of probability measures. To address this challenge, we leverage the penalty gradient approach recently developed for bilevel optimization which formulates bilevel optimization as a Lagrangian problem. This leads to a novel fully first-order algorithm, termed \texttt{F$^2$BMLD}. Apart from the convergence bound, we further provide a generalization bound, revealing an inherent trade-off in the choice of the Lagrange multiplier between optimization and statistical guarantees. Finally, we empirically validate the effectiveness of the proposed method on an offline reinforcement learning benchmark.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [186] [Uncertainty assessment of spatial dynamic microsimulations](https://arxiv.org/abs/2511.14294)
*Morgane Dumont,Ahmed Alsaloum,Julian Ernst,Jan Weymeirsch,Ralf Münnich*

Main category: stat.CO

TL;DR: 本文对德国MikroSim模型就业模块进行基于方差的敏感性分析，指出定性建模选择比常见不确定性来源影响更大，简单汇总指标不足以衡量模型不确定性。


<details>
  <summary>Details</summary>
Motivation: 空间动态微观模拟结果存在不确定性且受多因素影响，但这些因素很少被研究，因此要分析影响因素。

Method: 对德国MikroSim模型就业模块中的直接和间接影响应用基于方差的敏感性分析。

Result: 常见的不确定性来源（系数和参数不确定性）不如定性建模选择有影响力，简单汇总指标不足以捕捉整体模型不确定性。

Conclusion: 在设计微观模拟及其结果时，建模者应考虑更广泛的不确定性来源。

Abstract: Spatial dynamic microsimulations probabilistically project geographically referenced units with individual characteristics over time. Like any projection method, their outcomes are inherently uncertain and sensitive to multiple factors. However, such factors are rarely addressed. Applying variance-based sensitivity analysis to both direct and indirect effects within the employment module of the MikroSim model for Germany, we show that commonly considered sources of uncertainty, namely coefficient and parameter uncertainty, are less influential than qualitative modeling choices. Because dynamic microsimulations are inherently complex and are computationally intensive, it is crucial to consider potential factors of uncertainty and their influence on simulation outputs in order to more carefully design simulation setups and better communicate results. We find, that simple summary measures insufficiently capture overall model uncertainty and urge modelers to account for these broader sources when designing microsimulations and their results.

</details>


### [187] [Spatio-temporal Hawkes point processes: statistical inference and simulation strategies](https://arxiv.org/abs/2511.14509)
*Alba Bernabeu,Jorge Mateu*

Main category: stat.CO

TL;DR: 本文实现时空Hawkes点过程的两种模拟技术和三种统一自洽推理技术，并评估其性能，提供可复现代码。


<details>
  <summary>Details</summary>
Motivation: 时空Hawkes点过程虽应用广泛，但缺乏通用统一形式，各文献观点不一。

Method: 实现两种模拟技术和三种统一自洽的推理技术。

Result: 对这些方法的实际性能进行了评估。

Conclusion: 提供了可用于时空Hawkes过程实际建模的方法及可复现代码。

Abstract: Spatio-temporal Hawkes point processes are a particularly interesting class of stochastic point processes for modeling self-exciting behavior, in which the occurrence of one event increases the probability of other events occurring. These processes are able to handle complex interrelationships between stochastic and deterministic components of spatio-temporal phenomena. However, despite its widespread use in practice, there is no common and unified formalism and every paper proposes different views of these stochastic mechanisms. With this in mind, we implement two simulation techniques and three unified, self-consistent inference techniques, which are widely used in the practical modeling of spatio-temporal Hawkes processes. Furthermore, we provide an evaluation of the practical performance of these methods, while providing useful code for reproducibility.

</details>


### [188] [Estimation of Spatial and Temporal Autoregressive Effects using LASSO - An Example of Hourly Particulate Matter Concentrations](https://arxiv.org/abs/2511.14666)
*Elkanah Nyabuto,Philipp Otto,Yarema Okhrin*

Main category: stat.CO

TL;DR: 本文使用LASSO估计时空自回归面板数据模型的时空效应，蒙特卡罗模拟效果好，应用于德国巴伐利亚地区PM10浓度数据，LASSO可产生稀疏权重矩阵提升可解释性。


<details>
  <summary>Details</summary>
Motivation: 估计时空自回归面板数据模型中的权重矩阵及其他参数。

Method: 使用Least Absolute Shrinkage and Selection Operator (LASSO)，通过约束惩罚最大似然估计器进行估计，并进行蒙特卡罗模拟，还将方法应用于实际数据。

Result: 蒙特卡罗模拟显示准确性随时间点数量增加而提高；LASSO能区分有意义和无意义的关系；应用于巴伐利亚地区PM10数据，发现部分站点空间依赖性高，LASSO产生稀疏权重矩阵。

Conclusion: LASSO技术可有效估计时空自回归面板数据模型参数，产生的稀疏权重矩阵能提升PM浓度依赖关系的可解释性。

Abstract: We present an estimation procedure of spatial and temporal effects in spatiotemporal autoregressive panel data models using the Least Absolute Shrinkage and Selection Operator, LASSO (Tibshirani, 1996). We assume that the spatiotemporal panel is drawn from a univariate random process and that the data follows a spatiotemporal autoregressive process which includes a regressive term with space-/ time-varying exogenous regressor, a temporal autoregressive term and a spatial autoregressive term with an unknown weights matrix. The aim is to estimate this weight matrix alongside other parameters using a constraint penalised maximum likelihood estimator. Monte Carlo simulations showed a good performance with the accuracy increasing with an increasing number of time points. The use of the LASSO technique also consistently distinguishes between meaningful relationships (non-zeros) from those that are not (existing zeros) in both the spatial weights and other parameters. This regularised estimation procedure is applied to hourly particulate matter concentrations (PM10) in the Bavaria region, Germany for the years 2005 to 2020. Results show some stations with a high spatial dependency, resulting in a greater influence of PM10 concentrations in neighbouring monitoring stations. The LASSO technique proved to produce a sparse weights matrix by shrinking some weights to zero, hence improving the interpretability of the PM concentration dependencies across measurement stations in Bavaria

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [189] [A Disentangled Low-Rank RNN Framework for Uncovering Neural Connectivity and Dynamics](https://arxiv.org/abs/2511.13899)
*Chengrui Li,Yunmiao Wang,Yule Wang,Weihan Li,Dieter Jaeger,Anqi Wu*

Main category: q-bio.NC

TL;DR: 提出解纠缠循环神经网络DisRNN，在合成及真实数据上实验表明其能提升低维空间和低秩连接中神经潜在轨迹的解纠缠和可解释性。


<details>
  <summary>Details</summary>
Motivation: 低秩循环神经网络（lrRNNs）缺乏解纠缠解释，难以给不同潜在维度赋予不同计算角色。

Method: 提出DisRNN，在变分自动编码器（VAE）框架下重新构建lrRNN，引入偏相关惩罚以促进潜在维度组间解纠缠。

Result: 在合成、猴子M1和小鼠电压成像数据实验中，DisRNN在低维空间和低秩连接上比不促进部分解纠缠的基线lrRNNs更能提升解纠缠和可解释性。

Conclusion: DisRNN能有效提升低维空间和低秩连接中学习到的神经潜在轨迹的解纠缠和可解释性。

Abstract: Low-rank recurrent neural networks (lrRNNs) are a class of models that uncover low-dimensional latent dynamics underlying neural population activity. Although their functional connectivity is low-rank, it lacks disentanglement interpretations, making it difficult to assign distinct computational roles to different latent dimensions. To address this, we propose the Disentangled Recurrent Neural Network (DisRNN), a generative lrRNN framework that assumes group-wise independence among latent dynamics while allowing flexible within-group entanglement. These independent latent groups allow latent dynamics to evolve separately, but are internally rich for complex computation. We reformulate the lrRNN under a variational autoencoder (VAE) framework, enabling us to introduce a partial correlation penalty that encourages disentanglement between groups of latent dimensions. Experiments on synthetic, monkey M1, and mouse voltage imaging data show that DisRNN consistently improves the disentanglement and interpretability of learned neural latent trajectories in low-dimensional space and low-rank connectivity over baseline lrRNNs that do not encourage partial disentanglement.

</details>


### [190] [Subject-Independent Imagined Speech Detection via Cross-Subject Generalization and Calibration](https://arxiv.org/abs/2511.13739)
*Byung-Kwan Ko,Soowon Kim,Seo-Hyun Lee*

Main category: q-bio.NC

TL;DR: 研究脑电想象语音解码跨主体泛化问题，提出循环训练和少量校准结合策略有效。


<details>
  <summary>Details</summary>
Motivation: 脑电想象语音解码中个体神经活动模式差异大，实现跨个体鲁棒泛化是主要挑战。

Method: 采用循环跨主体训练方法，短训练段且频繁在主体间交替，并在主体校准留一法中用少量目标主体数据校准。

Result: 循环训练使跨未见目标数据解码性能有适度提升，仅用10%目标主体数据校准准确率达0.781，AUC达0.801。

Conclusion: 循环训练与少量校准结合是开发可扩展、用户自适应脑机接口系统，平衡泛化和个性化的简单有效策略。

Abstract: Achieving robust generalization across individuals remains a major challenge in electroencephalogram based imagined speech decoding due to substantial variability in neural activity patterns. This study examined how training dynamics and lightweight subject specific adaptation influence cross subject performance in a neural decoding framework. A cyclic inter subject training approach, involving shorter per subject training segments and frequent alternation among subjects, led to modest yet consistent improvements in decoding performance across unseen target data. Furthermore, under the subject calibrated leave one subject out scheme, incorporating only 10 % of the target subjects data for calibration achieved an accuracy of 0.781 and an AUC of 0.801, demonstrating the effectiveness of few shot adaptation. These findings suggest that integrating cyclic training with minimal calibration provides a simple and effective strategy for developing scalable, user adaptive brain computer interface systems that balance generalization and personalization.

</details>


### [191] [A Brain Wave Encodes a Thousand Tokens: Modeling Inter-Cortical Neural Interactions for Effective EEG-based Emotion Recognition](https://arxiv.org/abs/2511.13954)
*Nilay Kumar,Priyansh Bhandari,G. Maragatham*

Main category: q-bio.NC

TL;DR: 本文提出用于EEG情绪识别的RBTransformer模型，实验显示其在多个数据集上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视大脑不同区域动态交互，而这对理解情绪发展演变及准确识别情绪很关键。

Method: 提出RBTransformer架构，将EEG信号转换为BDE令牌，经电极身份嵌入保留空间来源，通过皮层间多头注意力块构建注意力矩阵学习神经依赖，最后经分类头预测。

Result: 在SEED、DEAP和DREAMER数据集上实验，RBTransformer在所有数据集、维度和分类设置下均超越先前方法。

Conclusion: RBTransformer能更好捕捉结构化神经交互，实现有效的基于EEG的情绪识别。

Abstract: Human emotions are difficult to convey through words and are often abstracted in the process; however, electroencephalogram (EEG) signals can offer a more direct lens into emotional brain activity. Recent studies show that deep learning models can process these signals to perform emotion recognition with high accuracy. However, many existing approaches overlook the dynamic interplay between distinct brain regions, which can be crucial to understanding how emotions unfold and evolve over time, potentially aiding in more accurate emotion recognition. To address this, we propose RBTransformer, a Transformer-based neural network architecture that models inter-cortical neural dynamics of the brain in latent space to better capture structured neural interactions for effective EEG-based emotion recognition. First, the EEG signals are converted into Band Differential Entropy (BDE) tokens, which are then passed through Electrode Identity embeddings to retain spatial provenance. These tokens are processed through successive inter-cortical multi-head attention blocks that construct an electrode x electrode attention matrix, allowing the model to learn the inter-cortical neural dependencies. The resulting features are then passed through a classification head to obtain the final prediction. We conducted extensive experiments, specifically under subject-dependent settings, on the SEED, DEAP, and DREAMER datasets, over all three dimensions, Valence, Arousal, and Dominance (for DEAP and DREAMER), under both binary and multi-class classification settings. The results demonstrate that the proposed RBTransformer outperforms all previous state-of-the-art methods across all three datasets, over all three dimensions under both classification settings. The source code is available at: https://github.com/nnilayy/RBTransformer.

</details>


### [192] [DecNefLab: A Modular and Interpretable Simulation Framework for Decoded Neurofeedback](https://arxiv.org/abs/2511.14555)
*Alexander Olza,Roberto Santana,David Soto*

Main category: q-bio.NC

TL;DR: 提出DecNefLab模拟框架，可用于建模、分析和理解神经反馈动态，还能指导协议设计。


<details>
  <summary>Details</summary>
Motivation: 当前DecNef研究受限于受试者学习变异性、间接测量及实验成本和时间，需要更好的研究工具。

Method: 将DecNef形式化为机器学习问题，使用潜在变量生成模型作为模拟参与者。

Result: 该方法可重现DecNef学习的经验现象，识别反馈失败的条件，指导设计更可靠的协议。

Conclusion: DecNefLab架起计算建模和认知神经科学的桥梁，为方法创新和协议设计提供基础。

Abstract: Decoded Neurofeedback (DecNef) is a flourishing non-invasive approach to brain modulation with wide-ranging applications in neuromedicine and cognitive neuroscience. However, progress in DecNef research remains constrained by subject-dependent learning variability, reliance on indirect measures to quantify progress, and the high cost and time demands of experimentation.
  We present DecNefLab, a modular and interpretable simulation framework that formalizes DecNef as a machine learning problem. Beyond providing a virtual laboratory, DecNefLab enables researchers to model, analyze and understand neurofeedback dynamics. Using latent variable generative models as simulated participants, DecNefLab allows direct observation of internal cognitive states and systematic evaluation of how different protocol designs and subject characteristics influence learning.
  We demonstrate how this approach can (i) reproduce empirical phenomena of DecNef learning, (ii) identify conditions under which DecNef feedback fails to induce learning, and (iii) guide the design of more robust and reliable DecNef protocols in silico before human implementation.
  In summary, DecNefLab bridges computational modeling and cognitive neuroscience, offering a principled foundation for methodological innovation, robust protocol design, and ultimately, a deeper understanding of DecNef-based brain modulation.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [193] [Optimal Sequential Flows](https://arxiv.org/abs/2511.13806)
*Hugo Gimbert,Corto Mascle,Patrick Totzke*

Main category: math.OC

TL;DR: 提出新代数技术在多项式空间解决顺序流问题，方法基于有限半群分解定理，可推广到多输入输出顶点及规则约束。


<details>
  <summary>Details</summary>
Motivation: 解决在图中通过从给定有限集选择容量标签序列来最大化随时间变化的边容量的流的问题。

Method: 基于有限半群的新分解定理，应用于合适的流半群以导出小的见证。

Result: 得到解决顺序流问题的新代数技术，且该方法可推广到多输入输出顶点及规则约束。

Conclusion: 新代数技术能有效解决顺序流问题，并具有一定的扩展性。

Abstract: We provide a new algebraic technique to solve the sequential flow problem in polynomial space. The task is to maximize the flow through a graph where edge capacities can be changed over time by choosing a sequence of capacity labelings from a given finite set. Our method is based on a novel factorization theorem for finite semigroups that, applied to a suitable flow semigroup, allows to derive small witnesses. This generalizes to multiple in/output vertices, as well as regular constraints.

</details>


### [194] [QUASAR: An Evolutionary Algorithm to Accelerate High-Dimensional Optimization](https://arxiv.org/abs/2511.13843)
*Julian Soltes*

Main category: math.OC

TL;DR: 本文提出QUASAR算法解决高维数值优化难题，在CEC2017基准测试中表现优异，计算高效，是解决复杂高维问题的有效优化器。


<details>
  <summary>Details</summary>
Motivation: 高维数值优化存在挑战，要加速复杂、不可微且受维度诅咒问题的收敛。

Method: 基于差分进化算法，引入高度随机架构，包含概率选择的变异策略和缩放因子、基于排名的交叉率、渐近衰减重新初始化三个主要组件。

Result: 在CEC2017基准测试中QUASAR总体排名和最低，计算效率高，在种群变量试验中运行时间比DE快1.4倍，比L - SHADE快7.8倍。

Conclusion: QUASAR是解决复杂高维问题的有效且用户友好的优化器。

Abstract: High-dimensional numerical optimization presents a persistent challenge. This paper introduces Quasi-Adaptive Search with Asymptotic Reinitialization (QUASAR), an evolutionary algorithm to accelerate convergence in complex, non-differentiable problems afflicted by the curse of dimensionality.
  Evaluated on the notoriously difficult CEC2017 benchmark suite of 29 functions, QUASAR achieved the lowest overall rank sum (150) using the Friedman test, significantly outperforming L-SHADE (229) and standard DE (305) in the dimension-variant trials. QUASAR also proves computationally efficient, with run times averaging $1.4 \text{x}$ faster than DE and $7.8 \text{x}$ faster than L-SHADE ($p \ll 0.001$) in the population-variant trials.
  Building upon Differential Evolution (DE), QUASAR introduces a highly stochastic architecture to dynamically balance exploration and exploitation. Inspired by the probabilistic behavior of quantum particles in a stellar core, the algorithm implements three primary components that augment standard DE mechanisms: 1) probabilistically selected mutation strategies and scaling factors; 2) rank-based crossover rates; 3) asymptotically decaying reinitialization that leverages a covariance matrix of the best solutions to introduce high-quality genetic diversity.
  QUASAR's performance establishes it as an effective, user-friendly optimizer for complex high-dimensional problems.

</details>


### [195] [Convex relaxation approaches for high dimensional optimal transport](https://arxiv.org/abs/2511.13847)
*Yuehaw Khoo,Tianyun Tang*

Main category: math.OC

TL;DR: 提出基于边缘和聚类矩松弛的凸松弛方法解决高维最优传输的计算和统计挑战，在高斯分布证明复杂度降低，非高斯也适用，还能提取传输映射，凸松弛为高维OT降维提供新路径。


<details>
  <summary>Details</summary>
Motivation: 最优传输在高维面临严重计算和统计挑战。

Method: 提出基于边缘和聚类矩松弛的凸松弛方法，利用分布的局部性和相关性稀疏性，用低阶边缘和稀疏矩统计近似高维耦合。

Result: 对于稀疏相关的高斯分布，证明了计算和样本复杂度降低；实验表明非高斯情况也适用；可从松弛中提取传输映射。

Conclusion: 凸松弛为高维最优传输的降维提供了有前景的途径。

Abstract: Optimal transport (OT) is a powerful tool in mathematics and data science but faces severe computational and statistical challenges in high dimensions. We propose convex relaxation approaches based on marginal and cluster moment relaxations that exploit locality and correlative sparsity in the distributions. These methods approximate high-dimensional couplings using low-order marginals and sparse moment statistics, yielding semidefinite programs that provide lower bounds on the OT cost with greatly reduced complexity. For Gaussian distributions with sparse correlations, we prove reductions in both computational and sample complexity, and experiments show the approach also works well for non-Gaussian cases. In addition, we demonstrate how to extract transport maps from our relaxations, offering a simpler and interpretable alternative to neural networks in generative modeling. Our results suggest that convex relaxations can provide a promising path for dimension reduction in high-dimensional OT.

</details>


### [196] [Feature weighting for data analysis via evolutionary simulation](https://arxiv.org/abs/2511.06454)
*Aris Daniilidis,Alberto Domínguez Corella,Philipp Wissgott*

Main category: math.OC

TL;DR: 分析离散多目标问题标量化前的权重分配算法，证明权重序列全局收敛到唯一内部均衡。


<details>
  <summary>Details</summary>
Motivation: 解决离散多目标问题标量化前的权重分配问题。

Method: 通过标准单纯形上的复制者动态演化权重，更新指数从归一化数据矩阵计算。

Result: 得到的权重序列全局收敛到唯一内部均衡，产生非退化的极限权重。

Conclusion: 该方法源于演化博弈论，与标准加权方案不同，具有可分析性和可证明的收敛性。

Abstract: We analyze an algorithm for assigning weights prior to scalarization in discrete multi-objective problems arising from data analysis. The algorithm evolves the weights (the relevance of features) by a replicator-type dynamic on the standard simplex, with update indices computed from a normalized data matrix. We prove that the resulting sequence converges globally to a unique interior equilibrium, yielding non-degenerate limiting weights. The method, originally inspired by evolutionary game theory, differs from standard weighting schemes in that it is analytically tractable with provable convergence.

</details>


### [197] [Wasserstein Distributionally Robust Nash Equilibrium Seeking with Heterogeneous Data: A Lagrangian Approach](https://arxiv.org/abs/2511.14048)
*Zifan Wang,Georgios Pantazis,Sergio Grammatico,Michael M. Zavlanos,Karl H. Johansson*

Main category: math.OC

TL;DR: 研究一类分布鲁棒博弈，设计算法学习近似纳什均衡并通过数值模拟验证理论。


<details>
  <summary>Details</summary>
Motivation: 研究允许参与者对不确定性分布变化进行异质风险规避选择的分布鲁棒博弈。

Method: 通过拉格朗日公式的惩罚函数实施对各分布的异质Wasserstein球约束，将分布鲁棒纳什均衡问题转化为有限维变分不等式问题，设计近似纳什均衡搜索算法。

Result: 证明平均遗憾收敛到一个随迭代次数减少的量，可按先验指定精度学习期望均衡，数值模拟支持理论结果。

Conclusion: 所设计算法能有效学习分布鲁棒博弈的近似纳什均衡。

Abstract: We study a class of distributionally robust games where agents are allowed to heterogeneously choose their risk aversion with respect to distributional shifts of the uncertainty. In our formulation, heterogeneous Wasserstein ball constraints on each distribution are enforced through a penalty function leveraging a Lagrangian formulation. We then formulate the distributionally robust Nash equilibrium problem and show that under certain assumptions it is equivalent to a finite-dimensional variational inequality problem with a strongly monotone mapping. We then design an approximate Nash equilibrium seeking algorithm and prove convergence of the average regret to a quantity that diminishes with the number of iterations, thus learning the desired equilibrium up to an a priori specified accuracy. Numerical simulations corroborate our theoretical findings.

</details>


### [198] [Improved Convergence in Parameter-Agnostic Error Feedback through Momentum](https://arxiv.org/abs/2511.14501)
*Abdurakhmon Sadiev,Yury Demidovich,Igor Sokolov,Grigory Malinovsky,Sarit Khirirat,Peter Richtárik*

Main category: math.OC

TL;DR: 本文研究归一化误差反馈算法，结合归一化更新、动量变体和与参数无关的时变步长，消除了对问题依赖调优的需求，分析了收敛性并建立复杂度界，实验验证了理论。


<details>
  <summary>Details</summary>
Motivation: 现有分布式误差反馈算法需问题参数先验知识来调优步长，限制了其在大规模神经网络训练中的实际应用。

Method: 研究结合误差反馈、归一化更新、各种动量变体和与参数无关的时变步长的归一化误差反馈算法，分析其收敛性。

Result: 建立了接近已知最优界的与参数无关的复杂度界，给出了不同动量下归一化EF21的收敛速率，结果在步长递减和小批量下成立。

Conclusion: 实验结果证实了理论分析的正确性。

Abstract: Communication compression is essential for scalable distributed training of modern machine learning models, but it often degrades convergence due to the noise it introduces. Error Feedback (EF) mechanisms are widely adopted to mitigate this issue of distributed compression algorithms. Despite their popularity and training efficiency, existing distributed EF algorithms often require prior knowledge of problem parameters (e.g., smoothness constants) to fine-tune stepsizes. This limits their practical applicability especially in large-scale neural network training. In this paper, we study normalized error feedback algorithms that combine EF with normalized updates, various momentum variants, and parameter-agnostic, time-varying stepsizes, thus eliminating the need for problem-dependent tuning. We analyze the convergence of these algorithms for minimizing smooth functions, and establish parameter-agnostic complexity bounds that are close to the best-known bounds with carefully-tuned problem-dependent stepsizes. Specifically, we show that normalized EF21 achieve the convergence rate of near ${O}(1/T^{1/4})$ for Polyak's heavy-ball momentum, ${O}(1/T^{2/7})$ for Iterative Gradient Transport (IGT), and ${O}(1/T^{1/3})$ for STORM and Hessian-corrected momentum. Our results hold with decreasing stepsizes and small mini-batches. Finally, our empirical experiments confirm our theoretical insights.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [199] [Game-theoretic Decentralized Coordination for Airspace Sector Overload Mitigation](https://arxiv.org/abs/2511.13770)
*Jaehan Im,Daniel Delahaye,David Fridovich-Keil,Ufuk Topcu*

Main category: eess.SY

TL;DR: 提出基于最佳响应动态的机制解决分散式空中交通管理中的扇区过载问题，证明其满足潜在博弈结构，实验表明算法有效。


<details>
  <summary>Details</summary>
Motivation: 分散式空中交通管理系统常假设高度合作，但实际中各空域扇区独立运行，该假设常不成立，需解决扇区过载问题。

Method: 提出基于最佳响应动态对自利行为建模的机制，各扇区调整航班出发时间以减少自身拥堵，用可调合作因子建模各扇区减少其他扇区过载的意愿。

Result: 证明该机制满足潜在博弈结构，在温和限制下最佳响应动态收敛到纯纳什均衡，确定过载免费解对应潜在函数全局最小值的充分条件，实验表明算法能大幅减少过载。

Conclusion: 所提算法即使在扇区间合作极少的情况下也能大幅减少过载，同时保持可扩展性，且解的质量与集中式求解器相当。

Abstract: Decentralized air traffic management systems offer a scalable alternative to centralized control, but often assume high levels of cooperation. In practice, such assumptions frequently break down since airspace sectors operate independently and prioritize local objectives. We address the problem of sector overload in decentralized air traffic management by proposing a mechanism that models self-interested behaviors based on best response dynamics. Each sector adjusts the departure times of flights under its control to reduce its own congestion, without any shared decision making. A tunable cooperativeness factor models the degree to which each sector is willing to reduce overload in other sectors. We prove that the proposed mechanism satisfies a potential game structure, ensuring that best response dynamics converge to a pure Nash equilibrium, under a mild restriction. In addition, we identify a sufficient condition under which an overload-free solution corresponds to a global minimizer of the potential function. Numerical experiments using 24 hours of European flight data demonstrate that the proposed algorithm substantially reduces overload even with only minimal cooperation between sectors, while maintaining scalability and matching the solution quality of centralized solvers.

</details>


### [200] [Deep reinforcement learning-based spacecraft attitude control with pointing keep-out constraint](https://arxiv.org/abs/2511.13746)
*Juntang Yang,Mohamed Khalil Ben-Larbi*

Main category: eess.SY

TL;DR: 本文用深度强化学习实现有单指向禁区的航天器重新定向控制，仿真验证方法有效性。


<details>
  <summary>Details</summary>
Motivation: 实现有单指向禁区的航天器重新定向控制。

Method: 采用Soft Actor - Critic算法处理连续状态和动作空间，设计新的状态表示，制定奖励函数，用课程学习方法训练智能体。

Result: 仿真结果表明所提出的基于深度强化学习的方法对航天器指向约束姿态控制有效。

Conclusion: 所提出的基于深度强化学习的方法可用于航天器指向约束姿态控制。

Abstract: This paper implements deep reinforcement learning (DRL) for spacecraft reorientation control with a single pointing keep-out zone. The Soft Actor-Critic (SAC) algorithm is adopted to handle continuous state and action space. A new state representation is designed to explicitly include a compact representation of the attitude constraint zone. The reward function is formulated to achieve the control objective while enforcing the attitude constraint. A curriculum learning approach is used for the agent training. Simulation results demonstrate the effectiveness of the proposed DRL-based method for spacecraft pointing-constrained attitude control.

</details>


### [201] [Quantifying Distribution Shift in Traffic Signal Control with Histogram-Based GEH Distance](https://arxiv.org/abs/2511.13785)
*Federico Taschin,Ozan K. Tonguz*

Main category: eess.SY

TL;DR: 提出量化交通信号控制算法分布偏移的方法，在模拟场景验证，能更好预测性能下降，对自适应交通信号控制有实用价值。


<details>
  <summary>Details</summary>
Motivation: 交通信号控制算法易受分布偏移影响，性能在不同交通条件下下降，需量化分布偏移。

Method: 将交通场景表示为需求直方图，用基于GEH的距离函数比较，该方法与策略无关、可解释且利用常用交通工程统计量。

Result: 在20个模拟场景验证，场景距离大对应旅行时间增加、吞吐量降低，对基于学习的控制解释力强，比以往技术能更好预测性能下降。

Conclusion: 该方法对自适应交通信号控制的基准测试、训练方案设计和监控有实用价值。

Abstract: Traffic signal control algorithms are vulnerable to distribution shift, where performance degrades under traffic conditions that differ from those seen during design or training. This paper introduces a principled approach to quantify distribution shift by representing traffic scenarios as demand histograms and comparing them with a GEH-based distance function. The method is policy-independent, interpretable, and leverages a widely used traffic engineering statistic. We validate the approach on 20 simulated scenarios using both a NEMA actuated controller and a reinforcement learning controller (FRAP++). Results show that larger scenario distances consistently correspond to increased travel time and reduced throughput, with particularly strong explanatory power for learning-based control. Overall, this method can predict performance degradation under distribution shift better than previously published techniques. These findings highlight the utility of the proposed framework for benchmarking, training regime design, and monitoring in adaptive traffic signal control.

</details>


### [202] [A Deep Learning Density Shaping Model Predictive Gust Load Alleviation Control of a Compliant Wing Subjected to Atmospheric Turbulence](https://arxiv.org/abs/2511.13745)
*Seid H. Pourtakdoust,Amir H. Khodabakhsh*

Main category: eess.SY

TL;DR: 提出一种用于柔性机翼随机阵风载荷减缓的深度学习方法，通过数值模拟验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 提升柔性机翼的随机阵风载荷减缓能力。

Method: 采用基于深度学习的模型预测控制器进行概率密度塑形，通过自定义物理信息神经网络求解概率密度演化方程，利用自动微分进行模型预测控制优化。

Result: 在柔性机翼模型上进行综合数值模拟，结果显示所提方法能有效减缓随机阵风载荷并减少翼尖挠度。

Conclusion: 所提概率密度塑形模型预测控制方法在缓解随机阵风载荷和降低翼尖挠度方面有效。

Abstract: This study presents a novel deep learning approach aimed at enhancing stochastic Gust Load Alleviation (GLA) specifically for compliant wings. The approach incorporates the concept of smooth wing camber variation, where the camber of the wing's chord is actively adjusted during flight using a control signal to achieve the desired aerodynamic loading. The proposed method employs a deep learning-based model predictive controller designed for probability density shaping. This controller effectively solves the probability density evolution equation through a custom Physics-Informed Neural Network (PINN) and utilizes Automatic Differentiation for Model Predictive Control (MPC) optimization. Comprehensive numerical simulations were conducted on a compliant wing (CW) model, evaluating performance of the proposed approach against stochastic gust profiles. The evaluation involved stochastic aerodynamic loads generated from Band-Limited White Noise (BLWN) and Dryden gust models. The evaluation were conducted for two distinct Compliant Chord Fractions (CCF). The results demonstrate the effectiveness of the proposed probability density shaping model predictive control in alleviating stochastic gust load and reducing wing tip deflection.

</details>


### [203] [Agentic AI Systems in Electrical Power Systems Engineering: Current State-of-the-Art and Challenges](https://arxiv.org/abs/2511.14478)
*Soham Ghosh,Gaurav Mittal*

Main category: eess.SY

TL;DR: 本文对代理式AI进行全面综述，给出定义和分类，介绍应用案例，研究故障模式并给出设计与实施建议。


<details>
  <summary>Details</summary>
Motivation: 代理式AI快速发展，需清晰的概念和分类理解以区分新范式。

Method: 先逐步引入概念，介绍其在工程领域的广泛应用，接着给出电气工程中的四个用例，最后进行故障模式调查。

Result: 展示了代理式AI在电气工程中的实际影响，如优化电力系统研究和电池换电站定价策略分析。

Conclusion: 得出了安全、可靠、可问责的代理式AI系统设计与实施的可行建议，为研究人员和从业者提供关键资源。

Abstract: Agentic AI systems have recently emerged as a critical and transformative approach in artificial intelligence, offering capabilities that extend far beyond traditional AI agents and contemporary generative AI models. This rapid evolution necessitates a clear conceptual and taxonomical understanding to differentiate this new paradigm. Our paper addresses this gap by providing a comprehensive review that establishes a precise definition and taxonomy for "agentic AI," with the aim of distinguishing it from previous AI paradigms. The concepts are gradually introduced, starting with a highlight of its diverse applications across the broader field of engineering. The paper then presents four detailed, state-of-the-art use case applications specifically within electrical engineering. These case studies demonstrate practical impact, ranging from an advanced agentic framework for streamlining complex power system studies and benchmarking to a novel system developed for survival analysis of dynamic pricing strategies in battery swapping stations. Finally, to ensure robust deployment, the paper provides detailed failure mode investigations. From these findings, we derive actionable recommendations for the design and implementation of safe, reliable, and accountable agentic AI systems, offering a critical resource for researchers and practitioners.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [204] [Evaluating the Impact of Packet Scheduling and Congestion Control Algorithms on MPTCP Performance over Heterogeneous Networks](https://arxiv.org/abs/2511.14550)
*Dimitrios Dimopoulos,Apostolis K. Salkintzis,Dimitris Tsolkas,Nikos Passas,Lazaros Merakos*

Main category: cs.NI

TL;DR: 本文介绍MPTCP协议，分析现有包调度和拥塞控制算法，在不同路径异构条件下评估算法组合对MPTCP性能的影响。


<details>
  <summary>Details</summary>
Motivation: MPTCP在路径异构性增加时性能受挑战，需研究不同算法组合对其性能的影响。

Method: 对MPTCP协议操作进行概述，分析文献中的包调度和拥塞控制算法，并在不同路径异构条件下进行广泛实验评估。

Result: 未提及具体实验结果。

Conclusion: 未提及明确结论。

Abstract: Modern mobile and stationary devices are equipped with multiple network interfaces aiming to provide wireless and wireline connectivity either in a local LAN or the Internet. Multipath TCP (MPTCP) protocol has been developed on top of legacy TCP to allow the simultaneous use of multiple network paths in the communication route between two end-systems. Although the combination of multiple paths is beneficial in case of links with similar network characteristics, MPTCP performance is challenged as heterogeneity among the used paths increases. This work provides an overview of the MPTCP protocol operation, analyzes the state-of-art packet scheduling and congestion control algorithms available in literature, and examines the impact of the various algorithm combinations on MPTCP performance, by conducting an extensive experimental evaluation under diverse path-heterogeneity conditions.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [205] [Analyzing Many Simulations of Hybrid Programs in Lince](https://arxiv.org/abs/2511.14436)
*Reydel Arrieta,José Proença,Patrick Meumeu Yomsi*

Main category: cs.LO

TL;DR: 本文介绍增强学术工具Lince的实验，使其能执行多模拟变体和生成直方图，并用自适应巡航控制系统示例说明。


<details>
  <summary>Details</summary>
Motivation: 混合系统在关键应用中使用增多，需要增强工具Lince以更好地对其进行指定和模拟。

Method: 通过实验为Lince添加执行多个模拟变体和生成直方图的机制。

Result: 完成对Lince的增强，并用自适应巡航控制系统的变体进行了说明。

Conclusion: 增强后的Lince能更好地处理混合系统的模拟和分析。

Abstract: Hybrid systems are increasingly used in critical applications such as medical devices, infrastructure systems, and autonomous vehicles. Lince is an academic tool for specifying and simulating such systems using a C-like language with differential equations. This paper presents recent experiments that enhance Lince with mechanisms for executing multiple simulation variants and generating histograms that quantify the frequency with which a given property holds. We illustrate our extended Lince using variations of an adaptive cruise control system.

</details>


### [206] [Safe-ROS: An Architecture for Autonomous Robots in Safety-Critical Domains](https://arxiv.org/abs/2511.14433)
*Diana C. Benjumea,Marie Farrell,Louise A. Dennis*

Main category: cs.LO

TL;DR: 提出Safe - ROS架构用于安全关键领域开发可靠可验证的自主机器人，在核环境机器人自主检查中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 在安全关键领域部署自主机器人需要确保操作有效性和安全合规性的架构。

Method: 提出Safe - ROS架构，含智能控制系统和安全系统；将安全要求实例化为安全仪表功能（SIF），实现为认知代理；进行验证、集成，并在仿真和实验室测试中验证。

Result: Safe - ROS架构能在安全关键领域部署自主机器人时提供可验证的安全监督。

Conclusion: Safe - ROS架构提供了一个可扩展到更多要求和应用的强大框架。

Abstract: Deploying autonomous robots in safety-critical domains requires architectures that ensure operational effectiveness and safety compliance. In this paper, we contribute the Safe-ROS architecture for developing reliable and verifiable autonomous robots in such domains. It features two distinct subsystems: (1) an intelligent control system that is responsible for normal/routine operations, and (2) a Safety System consisting of Safety Instrumented Functions (SIFs) that provide formally verifiable independent oversight. We demonstrate Safe-ROS on an AgileX Scout Mini robot performing autonomous inspection in a nuclear environment. One safety requirement is selected and instantiated as a SIF. To support verification, we implement the SIF as a cognitive agent, programmed to stop the robot whenever it detects that it is too close to an obstacle. We verify that the agent meets the safety requirement and integrate it into the autonomous inspection. This integration is also verified, and the full deployment is validated in a Gazebo simulation, and lab testing. We evaluate this architecture in the context of the UK nuclear sector, where safety and regulation are crucial aspects of deployment. Success criteria include the development of a formal property from the safety requirement, implementation, and verification of the SIF, and the integration of the SIF into the operational robotic autonomous system. Our results demonstrate that the  Safe-ROS architecture can provide safety verifiable oversight while deploying autonomous robots in safety-critical domains, offering a robust framework that can be extended to additional requirements and various applications.

</details>


### [207] [Towards A Catalogue of Requirement Patterns for Space Robotic Missions](https://arxiv.org/abs/2511.14438)
*Mahdi Etumi,Hazel M. Taylor,Marie Farrell*

Main category: cs.LO

TL;DR: 本文探索现有机器人规范模式在太空任务中的适用性，通过文献综述和形式化需求，贡献了太空任务需求语料库，新增5种规范模式，并进行专家评估。


<details>
  <summary>Details</summary>
Motivation: 自然语言表达的需求存在歧义，不适合形式验证，现有规范模式虽通用但需探索其在太空任务中的适用性。

Method: 进行文献综述，用NASA的FRET工具对现有太空任务需求进行形式化，用现有规范模式对需求分类。

Result: 证明现有规范模式适用于太空任务，新增5种规范模式及多种变体，完成专家评估。

Conclusion: 现有规范模式在太空任务有适用性，但存在不足，新增模式有一定价值也有局限。

Abstract: In the development of safety and mission-critical systems, including autonomous space robotic missions, complex behaviour is captured during the requirements elicitation phase. Requirements are typically expressed using natural language which is ambiguous and not amenable to formal verification methods that can provide robust guarantees of system behaviour. To support the definition of formal requirements, specification patterns provide reusable, logic-based templates. A suite of robotic specification patterns, along with their formalisation in NASA's Formal Requirements Elicitation Tool (FRET) already exists. These pre-existing requirement patterns are domain agnostic and, in this paper we explore their applicability for space missions. To achieve this we carried out a literature review of existing space missions and formalised their requirements using FRET, contributing a corpus of space mission requirements. We categorised these requirements using pre-existing specification patterns which demonstrated their applicability in space missions. However, not all of the requirements that we formalised corresponded to an existing pattern so we have contributed 5 new requirement specification patterns as well as several variants of the existing and new patterns. We also conducted an expert evaluation of the new patterns, highlighting their benefits and limitations.

</details>


### [208] [Context-aware, Ante-hoc Explanations of Driving Behaviour](https://arxiv.org/abs/2511.14428)
*Dominik Grundt,Ishan Saxena,Malte Petersen,Bernd Westphal,Eike Möhlmann*

Main category: cs.LO

TL;DR: 提出一种在运行时对自动驾驶车辆（AVs）的（非）预期驾驶操作进行上下文感知、事前解释的方法，并在模拟超车场景中进行了演示。


<details>
  <summary>Details</summary>
Motivation: AVs需安全且值得信赖以获社会认可，解释系统行为可提高安全性和信任度，但基于AI的驾驶功能决策过程不透明，解释困难，因此需解决这一问题。

Method: 使用可视化且正式的语言Traffic Sequence Charts对解释上下文和相应的（非）预期驾驶操作进行形式化，通过专用的运行时监控实现上下文识别和事前解释展示。

Result: 方法在模拟超车场景中得到演示。

Conclusion: 所提方法旨在支持构建正确且优质的解释。

Abstract: Autonomous vehicles (AVs) must be both safe and trustworthy to gain social acceptance and become a viable option for everyday public transportation. Explanations about the system behaviour can increase safety and trust in AVs. Unfortunately, explaining the system behaviour of AI-based driving functions is particularly challenging, as decision-making processes are often opaque. The field of Explainability Engineering tackles this challenge by developing explanation models at design time. These models are designed from system design artefacts and stakeholder needs to develop correct and good explanations. To support this field, we propose an approach that enables context-aware, ante-hoc explanations of (un)expectable driving manoeuvres at runtime. The visual yet formal language Traffic Sequence Charts is used to formalise explanation contexts, as well as corresponding (un)expectable driving manoeuvres. A dedicated runtime monitoring enables context-recognition and ante-hoc presentation of explanations at runtime. In combination, we aim to support the bridging of correct and good explanations. Our method is demonstrated in a simulated overtaking.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [209] [Preference-Based Learning in Audio Applications: A Systematic Analysis](https://arxiv.org/abs/2511.13936)
*Aaron Broukhim,Yiran Shen,Prithviraj Ammanabrolu,Nadir Weibel*

Main category: cs.SD

TL;DR: 对约500篇论文进行系统综述，发现仅6%将偏好学习应用于音频任务，分析揭示领域转变及三种模式，指出音频偏好学习需标准化基准等。


<details>
  <summary>Details</summary>
Motivation: 音频和文本领域在评估生成模型输出方面面临挑战，但音频应用中的偏好学习研究不足，需进行探索。

Method: 通过PRISMA指导的系统综述分析约500篇论文。

Result: 发现仅30篇（6%）将偏好学习用于音频任务，领域有转变，识别出三种关键模式。

Conclusion: 偏好学习对音频有前景，但该领域需要标准化基准、高质量数据集及对音频独特时间因素的系统研究。

Abstract: Despite the parallel challenges that audio and text domains face in evaluating generative model outputs, preference learning remains remarkably underexplored in audio applications. Through a PRISMA-guided systematic review of approximately 500 papers, we find that only 30 (6%) apply preference learning to audio tasks. Our analysis reveals a field in transition: pre-2021 works focused on emotion recognition using traditional ranking methods (rankSVM), while post-2021 studies have pivoted toward generation tasks employing modern RLHF frameworks. We identify three critical patterns: (1) the emergence of multi-dimensional evaluation strategies combining synthetic, automated, and human preferences; (2) inconsistent alignment between traditional metrics (WER, PESQ) and human judgments across different contexts; and (3) convergence on multi-stage training pipelines that combine reward signals. Our findings suggest that while preference learning shows promise for audio, particularly in capturing subjective qualities like naturalness and musicality, the field requires standardized benchmarks, higher-quality datasets, and systematic investigation of how temporal factors unique to audio impact preference learning frameworks.

</details>


### [210] [Count The Notes: Histogram-Based Supervision for Automatic Music Transcription](https://arxiv.org/abs/2511.14250)
*Jonathan Yaffe,Ben Maman,Meinard Müller,Amit H. Bermano*

Main category: cs.SD

TL;DR: 提出CountEM框架用于自动音乐转录，无需显式局部对齐，减少标注工作，在多数据集上表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有自动音乐转录的弱对齐方法依赖DTW或软对齐损失函数，易出错且计算成本高，创建强对齐数据集成本高。

Method: 引入CountEM框架，利用音符事件直方图作为监督，采用期望最大化（EM）方法，仅基于音符出现次数迭代优化预测。

Result: 在钢琴、吉他和多乐器数据集上实验表明，CountEM达到或超越现有弱监督方法。

Conclusion: CountEM提高了自动音乐转录的鲁棒性、可扩展性和效率。

Abstract: Automatic Music Transcription (AMT) converts audio recordings into symbolic musical representations. Training deep neural networks (DNNs) for AMT typically requires strongly aligned training pairs with precise frame-level annotations. Since creating such datasets is costly and impractical for many musical contexts, weakly aligned approaches using segment-level annotations have gained traction. However, existing methods often rely on Dynamic Time Warping (DTW) or soft alignment loss functions, both of which still require local semantic correspondences, making them error-prone and computationally expensive. In this article, we introduce CountEM, a novel AMT framework that eliminates the need for explicit local alignment by leveraging note event histograms as supervision, enabling lighter computations and greater flexibility. Using an Expectation-Maximization (EM) approach, CountEM iteratively refines predictions based solely on note occurrence counts, significantly reducing annotation efforts while maintaining high transcription accuracy. Experiments on piano, guitar, and multi-instrument datasets demonstrate that CountEM matches or surpasses existing weakly supervised methods, improving AMT's robustness, scalability, and efficiency. Our project page is available at https://yoni-yaffe.github.io/count-the-notes.

</details>


### [211] [Segmentwise Pruning in Audio-Language Models](https://arxiv.org/abs/2511.14293)
*Marcel Gibier,Raphaël Duroselle,Pierre Serrano,Olivier Boeffard,Jean-François Bonastre*

Main category: cs.SD

TL;DR: 研究音频语言模型中token选择策略，提出考虑时间维度的轻量级策略，保留四分之一初始token时在部分指标上有小幅度下降。


<details>
  <summary>Details</summary>
Motivation: 现有音频语言模型计算成本依赖序列长度，视觉语言领域token修剪方法有效，探究其在音频语言模型中的相关性和有效性。

Method: 研究现有token选择策略，并提出考虑时间维度的轻量级策略。

Result: 保留四分之一初始token时，在Clotho v2上CIDEr相对最大下降2%，在MMAU上准确率相对最大下降4%。

Conclusion: 提出的策略在减少token数量的同时能在一定程度上保持模型性能。

Abstract: Recent audio-language models have shown impressive performance across a wide range of audio tasks and are increasingly capable of handling long audio inputs. However, the computing costs in these models heavily depend on sequence length, which can become very large given the nature of audio data. In the vision-language domain, token pruning methods have proven effective in reducing token counts while preserving strong performance on standard benchmarks. In this work, we investigate the relevance and effectiveness of such token selection strategies in the context of audio-language models. We also improve them by proposing a lightweight strategy that takes the time dimension into account. While retaining only a quarter of the initial tokens, our approach results in a relative maximum decrease of 2% in CIDEr on Clotho v2 and a relative maximum decrease of 4% in accuracy on MMAU.

</details>


### [212] [Audio Question Answering with GRPO-Based Fine-Tuning and Calibrated Segment-Level Predictions](https://arxiv.org/abs/2511.14307)
*Marcel Gibier,Nolwenn Celton,Raphaël Duroselle,Pierre Serrano,Olivier Boeffard,Jean-François Bonastre*

Main category: cs.SD

TL;DR: 介绍DCASE 2025挑战赛Track 5音频问答任务提交系统，结合声学特征提取与大模型实现62.6%准确率。


<details>
  <summary>Details</summary>
Motivation: 参加DCASE 2025挑战赛Track 5的音频问答任务。

Method: 用BEATs提取音频特征，分类头生成声学事件预测，校准后结合问题和候选答案形成提示，输入微调后的Qwen2.5 - 7B - Instruct模型。

Result: 在开发集上达到62.6%的准确率。

Conclusion: 结合声学事件推理和指令微调大语言模型用于音频问答是有效的。

Abstract: In this report, we describe our submission to Track 5 of the DCASE 2025 Challenge for the task of Audio Question Answering(AQA). Our system leverages the SSL backbone BEATs to extract frame-level audio features, which are then processed by a classification head to generate segment-level predictions of acoustic events, following the Audioset ontology. These segment-level predictions are subsequently calibrated before producing event-level predictions. Finally, these predictions are incorporated into a structured prompt, along with the question and candidate answers. This prompt is then fed to a fine-tuned version of Qwen2.5-7B-Instruct, trained using the GRPO algorithm with a simple reward function. Our method achieves an accuracy of 62.6 % on the development set, demonstrating the effectiveness of combining acoustic event reasoning with instruction-tuned large language models for AQA.

</details>


### [213] [IMSE: Efficient U-Net-based Speech Enhancement using Inception Depthwise Convolution and Amplitude-Aware Linear Attention](https://arxiv.org/abs/2511.14515)
*Xinxin Tang,Bin Qin,Yufang Li*

Main category: cs.SD

TL;DR: 针对资源受限设备上的语音增强任务，本文提出超轻量级网络IMSE，减少参数并取得可比性能，为模型大小和语音质量平衡设立新基准。


<details>
  <summary>Details</summary>
Motivation: 现有语音增强方法MUSE存在效率瓶颈，本文旨在实现轻量级设计与高性能的平衡。

Method: 提出IMSE网络，用MALA替换MET模块，用IDConv替换DE模块。

Result: 在VoiceBank+DEMAND数据集上，IMSE较MUSE参数减少16.8%，在PESQ指标上取得可比性能。

Conclusion: 本研究为超轻量级语音增强中模型大小和语音质量的权衡设定了新基准。

Abstract: Achieving a balance between lightweight design and high performance remains a significant challenge for speech enhancement (SE) tasks on resource-constrained devices. Existing state-of-the-art methods, such as MUSE, have established a strong baseline with only 0.51M parameters by introducing a Multi-path Enhanced Taylor (MET) transformer and Deformable Embedding (DE). However, an in-depth analysis reveals that MUSE still suffers from efficiency bottlenecks: the MET module relies on a complex "approximate-compensate" mechanism to mitigate the limitations of Taylor-expansion-based attention, while the offset calculation for deformable embedding introduces additional computational burden. This paper proposes IMSE, a systematically optimized and ultra-lightweight network. We introduce two core innovations: 1) Replacing the MET module with Amplitude-Aware Linear Attention (MALA). MALA fundamentally rectifies the "amplitude-ignoring" problem in linear attention by explicitly preserving the norm information of query vectors in the attention calculation, achieving efficient global modeling without an auxiliary compensation branch. 2) Replacing the DE module with Inception Depthwise Convolution (IDConv). IDConv borrows the Inception concept, decomposing large-kernel operations into efficient parallel branches (square, horizontal, and vertical strips), thereby capturing spectrogram features with extremely low parameter redundancy. Extensive experiments on the VoiceBank+DEMAND dataset demonstrate that, compared to the MUSE baseline, IMSE significantly reduces the parameter count by 16.8\% (from 0.513M to 0.427M) while achieving competitive performance comparable to the state-of-the-art on the PESQ metric (3.373). This study sets a new benchmark for the trade-off between model size and speech quality in ultra-lightweight speech enhancement.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [214] [Statistically controllable microstructure reconstruction framework for heterogeneous materials using sliced-Wasserstein metric and neural networks](https://arxiv.org/abs/2511.14268)
*Zhenchuan Ma,Qizhi Teng,Pengcheng Yan,Lindong Li,Kirill M. Gerke,Marina V. Karsanina,Xiaohai He*

Main category: physics.comp-ph

TL;DR: 提出集成神经网络与切片 - 瓦瑟斯坦度量的微观结构重建框架，可处理小样本，能生成大尺寸和复杂微观结构，实验证明方法有效。


<details>
  <summary>Details</summary>
Motivation: 为了在小样本下实现对异质多孔材料微观结构重建的更好可控性和适用性。

Method: 提出统计可控微观结构重建框架，利用局部模式分布表征微观结构，采用受控采样策略生成目标分布，用神经网络建立映射，结合切片 - 瓦瑟斯坦度量和梯度优化技术。

Result: 能在小样本下进行随机和可控重建，可生成大尺寸 3D 微观结构，擅长生成空间异质和复杂微观结构。

Conclusion: 通过多种实验对比分析，证明方法有效，为结构 - 性能关联研究和材料逆向设计提供新见解和可能性。

Abstract: Heterogeneous porous materials play a crucial role in various engineering systems. Microstructure characterization and reconstruction provide effective means for modeling these materials, which are critical for conducting physical property simulations, structure-property linkage studies, and enhancing their performance across different applications. To achieve superior controllability and applicability with small sample sizes, we propose a statistically controllable microstructure reconstruction framework that integrates neural networks with sliced-Wasserstein metric. Specifically, our approach leverages local pattern distribution for microstructure characterization and employs a controlled sampling strategy to generate target distributions that satisfy given conditional parameters. A neural network-based model establishes the mapping from the input distribution to the target local pattern distribution, enabling microstructure reconstruction. Combinations of sliced-Wasserstein metric and gradient optimization techniques minimize the distance between these distributions, leading to a stable and reliable model. Our method can perform stochastic and controllable reconstruction tasks even with small sample sizes. Additionally, it can generate large-size (e.g. 512 and 1024) 3D microstructures using a chunking strategy. By introducing spatial location masks, our method excels at generating spatially heterogeneous and complex microstructures. We conducted experiments on stochastic reconstruction, controllable reconstruction, heterogeneous reconstruction, and large-size microstructure reconstruction across various materials. Comparative analysis through visualization, statistical measures, and physical property simulations demonstrates the effectiveness, providing new insights and possibilities for research on structure-property linkage and material inverse design.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [215] [Compression with Privacy-Preserving Random Access](https://arxiv.org/abs/2511.14524)
*Venkat Chandar,Aslan Tchamkerten,Shashank Vatedka*

Main category: cs.IT

TL;DR: i.i.d. 二元源序列可在高于熵的速率无损压缩，且单个解码不泄露其他比特信息


<details>
  <summary>Details</summary>
Motivation: 探索i.i.d. 二元源序列无损压缩时的信息泄露情况

Method: 未提及

Result: i.i.d. 二元源序列能在高于熵的速率无损压缩，且单个解码不透露其他比特信息

Conclusion: i.i.d. 二元源序列可实现满足特定信息不泄露条件的无损压缩

Abstract: It is shown that an i.i.d. binary source sequence $X_1, \ldots, X_n$ can be losslessly compressed at any rate above entropy such that the individual decoding of any $X_i$ reveals \emph{no} information about the other bits $\{X_j : j \neq i\}$.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [216] [Preparation Meets Opportunity: Enhancing Data Preprocessing for ML Training With Seneca](https://arxiv.org/abs/2511.13724)
*Omkar Desai,Ziyang Jiao,Shuyi Pei,Janki Bhimani,Bryan S. Kim*

Main category: cs.OS

TL;DR: 提出数据加载系统Seneca优化缓存分区和数据采样，减少并发作业训练时间，效果显著。


<details>
  <summary>Details</summary>
Motivation: 缓解现代系统中并发训练多媒体机器学习模型时输入数据预处理的瓶颈，减少并发作业训练时间。

Method: 一是用数据管道性能模型对三种不同形式数据进行最优缓存分区；二是在随机批量采样时优先使用缓存数据，让并发作业相互受益。

Result: 与PyTorch相比，Seneca减少了45.23%的完成时间；与次优数据加载器相比，数据处理吞吐量提高了3.45倍。

Conclusion: Seneca在优化数据加载、减少训练时间和提高吞吐量方面有效。

Abstract: Input data preprocessing is a common bottleneck when concurrently training multimedia machine learning (ML) models in modern systems. To alleviate these bottlenecks and reduce the training time for concurrent jobs, we present Seneca, a data loading system that optimizes cache partitioning and data sampling for the data storage and ingestion (DSI) pipeline. The design of Seneca contains two key techniques. First, Seneca uses a performance model for the data pipeline to optimally partition the cache for three different forms of data (encoded, decoded, and augmented). Second, Seneca opportunistically serves cached data over uncached ones during random batch sampling so that concurrent jobs benefit from each other. We implement Seneca by modifying PyTorch and demonstrate its effectiveness by comparing it against several state-of-the-art caching systems for DNN training. Seneca reduces the makespan by 45.23% compared to PyTorch and increases data processing throughput by up to 3.45x compared to the next best dataloader.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [217] [Imaging with super-resolution in changing random media](https://arxiv.org/abs/2511.14147)
*Alexander Christie,Matan Leibovich,Miguel Moscoso,Alexei Novikov,George Papanicolaou,Chrysoula Tsogka*

Main category: physics.optics

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We develop an imaging algorithm that exploits strong scattering to achieve super-resolution in changing random media. The method processes large and diverse array datasets using sparse dictionary learning, clustering, and multidimensional scaling. Starting from random initializations, the algorithm reliably extracts the unknown medium properties necessary for accurate imaging using back-propagation, $\ell_2$ or $\ell_1$ methods. Remarkably, scattering enhances resolution beyond homogeneous medium limits. When abundant data are available, the algorithm allows the realization of super-resolution in imaging.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [218] [AI Kill Switch for malicious web-based LLM agent](https://arxiv.org/abs/2511.13725)
*Sechan Lee,Sangdon Park*

Main category: cs.CR

TL;DR: 本文提出 AI Kill Switch 技术及 AutoGuard 方法，能通过生成防御提示触发恶意基于网络的大语言模型代理的安全机制，实验表明该方法在多种模型和场景下防御成功率高，证明了对基于网络的大语言模型代理的可控性。


<details>
  <summary>Details</summary>
Motivation: 基于网络的大语言模型代理在带来便利的同时，也放大了恶意滥用的风险，如未经授权收集个人身份信息、生成社会分裂内容和自动网络黑客攻击等，需要解决这些威胁。

Method: 提出 AI Kill Switch 技术，引入 AutoGuard，生成防御提示并透明嵌入网站 DOM 中，触发恶意代理的安全机制。

Result: 在包含三种代表性恶意场景的基准测试中，AutoGuard 方法在包括 GPT - 4o、Claude - 3 等恶意代理上防御成功率超 80%，在 GPT - 5 等模型上防御成功率达 90%左右。

Conclusion: 证明了基于网络的大语言模型代理在各种场景和模型下的可控性，为人工智能控制和安全做出贡献。

Abstract: Recently, web-based Large Language Model (LLM) agents autonomously perform increasingly complex tasks, thereby bringing significant convenience. However, they also amplify the risks of malicious misuse cases such as unauthorized collection of personally identifiable information (PII), generation of socially divisive content, and even automated web hacking. To address these threats, we propose an AI Kill Switch technique that can immediately halt the operation of malicious web-based LLM agents. To achieve this, we introduce AutoGuard - the key idea is generating defensive prompts that trigger the safety mechanisms of malicious LLM agents. In particular, generated defense prompts are transparently embedded into the website's DOM so that they remain invisible to human users but can be detected by the crawling process of malicious agents, triggering its internal safety mechanisms to abort malicious actions once read. To evaluate our approach, we constructed a dedicated benchmark consisting of three representative malicious scenarios (PII collection, social rift content generation, and web hacking attempts). Experimental results show that the AutoGuard method achieves over 80% Defense Success Rate (DSR) on malicious agents, including GPT-4o, Claude-3, and Llama3.3-70B-Instruct. It also maintains strong performance, achieving around 90% DSR on GPT-5, GPT-4.1, and Gemini-2.5-Flash when used as the malicious agent, demonstrating robust generalization across models and scenarios. Through this research, we have demonstrated the controllability of web-based LLM agents across various scenarios and models, thereby contributing to the broader effort of AI control and safety.

</details>


### [219] [ExplainableGuard: Interpretable Adversarial Defense for Large Language Models Using Chain-of-Thought Reasoning](https://arxiv.org/abs/2511.13771)
*Shaowei Guan,Yu Zhai,Zhengyu Zhang,Yanze Wang,Hin Chi Kwok*

Main category: cs.CR

TL;DR: 本文提出可解释防御框架ExplainableGuard，利用DeepSeek - Reasoner的思维链推理能力检测和消除对抗扰动，在数据集上有良好效果，人类评估显示其解释更优，具部署潜力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型易受对抗攻击，现有防御机制多为黑盒，缺乏决策透明度。

Method: 引入ExplainableGuard框架，利用DeepSeek - Reasoner的思维链推理能力，通过定制思维链提示引导大语言模型进行多方面分析，生成净化输出和可读解释。

Result: 在GLUE Benchmark和IMDB Movie Reviews数据集上有良好防御效果；人类评估显示其解释在清晰度、特异性和可操作性上优于消融变体，部署信任评级达72.5%。

Conclusion: ExplainableGuard有潜力实现更可信的大语言模型部署。

Abstract: Large Language Models (LLMs) are increasingly vulnerable to adversarial attacks that can subtly manipulate their outputs. While various defense mechanisms have been proposed, many operate as black boxes, lacking transparency in their decision-making. This paper introduces ExplainableGuard, an interpretable adversarial defense framework leveraging the chain-of-thought (CoT) reasoning capabilities of DeepSeek-Reasoner. Our approach not only detects and neutralizes adversarial perturbations in text but also provides step-by-step explanations for each defense action. We demonstrate how tailored CoT prompts guide the LLM to perform a multi-faceted analysis (character, word, structural, and semantic) and generate a purified output along with a human-readable justification. Preliminary results on the GLUE Benchmark and IMDB Movie Reviews dataset show promising defense efficacy. Additionally, a human evaluation study reveals that ExplainableGuard's explanations outperform ablated variants in clarity, specificity, and actionability, with a 72.5% deployability-trust rating, underscoring its potential for more trustworthy LLM deployments.

</details>


### [220] [Uncovering and Aligning Anomalous Attention Heads to Defend Against NLP Backdoor Attacks](https://arxiv.org/abs/2511.13789)
*Haotian Jin,Yang Li,Haihui Fan,Lin Shen,Xiangfang Li,Bo Li*

Main category: cs.CR

TL;DR: 本文提出基于注意力相似度的后门检测方法，结合注意力安全对齐和逐头微调缓解后门攻击影响，实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 现有后门防御方法局限于特定类型触发器或依赖额外干净模型，设计灵活的触发器使防御者难以准确识别。

Method: 提出基于注意力相似度的后门检测方法，利用受攻击模型在触发器下注意力头异常高相似度的特点，采用注意力安全对齐和逐头微调纠正可能受污染的注意力头。

Result: 实验结果表明该方法显著降低了后门攻击成功率，同时保留了模型在下游任务上的性能。

Conclusion: 所提方法能有效缓解大语言模型后门攻击的影响。

Abstract: Backdoor attacks pose a serious threat to the security of large language models (LLMs), causing them to exhibit anomalous behavior under specific trigger conditions. The design of backdoor triggers has evolved from fixed triggers to dynamic or implicit triggers. This increased flexibility in trigger design makes it challenging for defenders to identify their specific forms accurately. Most existing backdoor defense methods are limited to specific types of triggers or rely on an additional clean model for support. To address this issue, we propose a backdoor detection method based on attention similarity, enabling backdoor detection without prior knowledge of the trigger. Our study reveals that models subjected to backdoor attacks exhibit unusually high similarity among attention heads when exposed to triggers. Based on this observation, we propose an attention safety alignment approach combined with head-wise fine-tuning to rectify potentially contaminated attention heads, thereby effectively mitigating the impact of backdoor attacks. Extensive experimental results demonstrate that our method significantly reduces the success rate of backdoor attacks while preserving the model's performance on downstream tasks.

</details>


### [221] [GRPO Privacy Is at Risk: A Membership Inference Attack Against Reinforcement Learning With Verifiable Rewards](https://arxiv.org/abs/2511.14045)
*Yule Liu,Heyi Zhang,Jinyi Zheng,Zhen Sun,Zifan Peng,Tianshuo Cong,Yilong Yang,Xinlei He,Zhuo Ma*

Main category: cs.CR

TL;DR: 提出针对RLVR的成员推理框架DIBA，评估显示其优于现有基线，能通过行为痕迹推断训练数据暴露情况。


<details>
  <summary>Details</summary>
Motivation: RLVR在大语言模型训练中带来范式转变，但因其策略特性引入新隐私泄漏模式，需审计该隐私风险。

Method: 提出Divergence - in - Behavior Attack (DIBA)框架，从行为改变而非记忆角度，利用优势侧改进和对数侧差异进行成员推理。

Result: DIBA显著优于现有基线，AUC约0.8，TPR@0.1%FPR高一个数量级，在多场景验证其优越性，且在适度防御措施下仍稳健。

Conclusion: 这是首个系统分析RLVR隐私漏洞的工作，表明无明确监督时也能通过行为痕迹可靠推断训练数据暴露。

Abstract: Membership inference attacks (MIAs) on large language models (LLMs) pose significant privacy risks across various stages of model training. Recent advances in Reinforcement Learning with Verifiable Rewards (RLVR) have brought a profound paradigm shift in LLM training, particularly for complex reasoning tasks. However, the on-policy nature of RLVR introduces a unique privacy leakage pattern: since training relies on self-generated responses without fixed ground-truth outputs, membership inference must now determine whether a given prompt (independent of any specific response) is used during fine-tuning. This creates a threat where leakage arises not from answer memorization.
  To audit this novel privacy risk, we propose Divergence-in-Behavior Attack (DIBA), the first membership inference framework specifically designed for RLVR. DIBA shifts the focus from memorization to behavioral change, leveraging measurable shifts in model behavior across two axes: advantage-side improvement (e.g., correctness gain) and logit-side divergence (e.g., policy drift). Through comprehensive evaluations, we demonstrate that DIBA significantly outperforms existing baselines, achieving around 0.8 AUC and an order-of-magnitude higher TPR@0.1%FPR. We validate DIBA's superiority across multiple settings--including in-distribution, cross-dataset, cross-algorithm, black-box scenarios, and extensions to vision-language models. Furthermore, our attack remains robust under moderate defensive measures.
  To the best of our knowledge, this is the first work to systematically analyze privacy vulnerabilities in RLVR, revealing that even in the absence of explicit supervision, training data exposure can be reliably inferred through behavioral traces.

</details>


### [222] [Zipf-Gramming: Scaling Byte N-Grams Up to Production Sized Malware Corpora](https://arxiv.org/abs/2511.13808)
*Edward Raff,Ryan R. Curtin,Derek Everett,Robert J. Joyce,James Holt*

Main category: cs.CR

TL;DR: 提出Zipf - Gramming算法加速top - k n - gram提取，提升新恶意软件检测AUC。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以在满足规模、速度和延迟要求下定期更新模型，因提取top - k n - gram成本高。

Method: 利用Zipfian分布特性开发新的top - k n - gram提取器，即Zipf - Gramming算法。

Result: 新算法比之前最佳方法快35倍，扩大生产训练集后新恶意软件检测AUC提升30%。

Conclusion: 理论和实证表明该方法能准确选择top - k项，且展示了理论与工程的相互作用。

Abstract: A classifier using byte n-grams as features is the only approach we have found fast enough to meet requirements in size (sub 2 MB), speed (multiple GB/s), and latency (sub 10 ms) for deployment in numerous malware detection scenarios. However, we've consistently found that 6-8 grams achieve the best accuracy on our production deployments but have been unable to deploy regularly updated models due to the high cost of finding the top-k most frequent n-grams over terabytes of executable programs. Because the Zipfian distribution well models the distribution of n-grams, we exploit its properties to develop a new top-k n-gram extractor that is up to $35\times$ faster than the previous best alternative. Using our new Zipf-Gramming algorithm, we are able to scale up our production training set and obtain up to 30\% improvement in AUC at detecting new malware. We show theoretically and empirically that our approach will select the top-k items with little error and the interplay between theory and engineering required to achieve these results.

</details>


### [223] [Dynamic Black-box Backdoor Attacks on IoT Sensory Data](https://arxiv.org/abs/2511.14074)
*Ajesh Koyatan Chathoth,Stephen Lee*

Main category: cs.CR

TL;DR: 本文提出针对基于传感器数据的物联网系统的动态触发生成技术进行黑盒对抗攻击，并分析其效果与防御机制。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在基于传感器数据的系统中存在安全风险，需要研究对抗攻击技术。

Method: 提出一种新颖的动态触发生成技术进行黑盒对抗攻击。

Result: 攻击在多种数据集和分类器模型上成功，对输入数据扰动极小，还进行了与其他投毒技术的对比分析。

Conclusion: 探讨了攻击技术及相关防御机制对该技术有效性的影响。

Abstract: Sensor data-based recognition systems are widely used in various applications, such as gait-based authentication and human activity recognition (HAR). Modern wearable and smart devices feature various built-in Inertial Measurement Unit (IMU) sensors, and such sensor-based measurements can be fed to a machine learning-based model to train and classify human activities. While deep learning-based models have proven successful in classifying human activity and gestures, they pose various security risks. In our paper, we discuss a novel dynamic trigger-generation technique for performing black-box adversarial attacks on sensor data-based IoT systems. Our empirical analysis shows that the attack is successful on various datasets and classifier models with minimal perturbation on the input data. We also provide a detailed comparative analysis of performance and stealthiness to various other poisoning techniques found in backdoor attacks. We also discuss some adversarial defense mechanisms and their impact on the effectiveness of our trigger-generation technique.

</details>


### [224] [MalRAG: A Retrieval-Augmented LLM Framework for Open-set Malicious Traffic Identification](https://arxiv.org/abs/2511.14129)
*Xiang Luo,Chang Liu,Gang Xiong,Chen Yang,Gaopeng Gou,Yaochen Ren,Zhen Li*

Main category: cs.CR

TL;DR: 本文提出首个由大语言模型驱动的检索增强框架MalRAG用于开放集恶意流量识别，在多数据集上取得SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 现有深度模型在恶意流量识别中依赖特定架构，可迁移性差且需针对数据集调优，需要更好的方法进行细粒度已知恶意流量识别和新恶意流量发现。

Method: 提出MalRAG框架，包括构建多视图流量数据库、采用覆盖增强检索算法、流量感知自适应剪枝和开发引导提示。

Result: 在不同真实世界数据集和设置中，MalRAG在已知类别的细粒度识别和新恶意流量发现方面取得了最先进的结果。

Conclusion: MalRAG有效利用了大语言模型的能力，无需依赖特定大语言模型即可实现开放集恶意流量识别。

Abstract: Fine-grained identification of IDS-flagged suspicious traffic is crucial in cybersecurity. In practice, cyber threats evolve continuously, making the discovery of novel malicious traffic a critical necessity as well as the identification of known classes. Recent studies have advanced this goal with deep models, but they often rely on task-specific architectures that limit transferability and require per-dataset tuning. In this paper we introduce MalRAG, the first LLM driven retrieval-augmented framework for open-set malicious traffic identification. MalRAG freezes the LLM and operates via comprehensive traffic knowledge construction, adaptive retrieval, and prompt engineering. Concretely, we construct a multi-view traffic database by mining prior malicious traffic from content, structural, and temporal perspectives. Furthermore, we introduce a Coverage-Enhanced Retrieval Algorithm that queries across these views to assemble the most probable candidates, thereby improving the inclusion of correct evidence. We then employ Traffic-Aware Adaptive Pruning to select a variable subset of these candidates based on traffic-aware similarity scores, suppressing incorrect matches and yielding reliable retrieved evidence. Moreover, we develop a suite of guidance prompts where task instruction, evidence referencing, and decision guidance are integrated with the retrieved evidence to improve LLM performance. Across diverse real-world datasets and settings, MalRAG delivers state-of-the-art results in both fine-grained identification of known classes and novel malicious traffic discovery. Ablation and deep-dive analyses further show that MalRAG effective leverages LLM capabilities yet achieves open-set malicious traffic identification without relying on a specific LLM.

</details>


### [225] [Sigil: Server-Enforced Watermarking in U-Shaped Split Federated Learning via Gradient Injection](https://arxiv.org/abs/2511.14422)
*Zhengchunmin Dai,Jiaxiong Tang,Peng Sun,Honglong Chen,Liantao Wu*

Main category: cs.CR

TL;DR: 本文针对能力受限服务器易受模型盗窃问题，提出Sigil水印框架，实验证明其保真、鲁棒和隐蔽。


<details>
  <summary>Details</summary>
Motivation: 在SFL等去中心化机器学习范式中，能力受限服务器易受模型盗窃，现有水印方案不可靠或不可行。

Method: 提出Sigil框架，将水印定义为服务器可见激活空间的统计约束，通过梯度注入嵌入客户端模型，设计自适应梯度裁剪机制。

Result: 在多个数据集和模型上的实验表明Sigil具有保真、鲁棒和隐蔽性。

Conclusion: Sigil能有效解决能力受限服务器的知识产权保护问题。

Abstract: In decentralized machine learning paradigms such as Split Federated Learning (SFL) and its variant U-shaped SFL, the server's capabilities are severely restricted. Although this enhances client-side privacy, it also leaves the server highly vulnerable to model theft by malicious clients. Ensuring intellectual property protection for such capability-limited servers presents a dual challenge: watermarking schemes that depend on client cooperation are unreliable in adversarial settings, whereas traditional server-side watermarking schemes are technically infeasible because the server lacks access to critical elements such as model parameters or labels.
  To address this challenge, this paper proposes Sigil, a mandatory watermarking framework designed specifically for capability-limited servers. Sigil defines the watermark as a statistical constraint on the server-visible activation space and embeds the watermark into the client model via gradient injection, without requiring any knowledge of the data. Besides, we design an adaptive gradient clipping mechanism to ensure that our watermarking process remains both mandatory and stealthy, effectively countering existing gradient anomaly detection methods and a specifically designed adaptive subspace removal attack. Extensive experiments on multiple datasets and models demonstrate Sigil's fidelity, robustness, and stealthiness.

</details>


### [226] [Steganographic Backdoor Attacks in NLP: Ultra-Low Poisoning and Defense Evasion](https://arxiv.org/abs/2511.14301)
*Eric Xue,Ruiyi Zhang,Zijun Zhang,Pengtao Xie*

Main category: cs.CR

TL;DR: 论文提出SteganoBackdoor，利用自然语言隐写术特性，在低数据投毒率下实现高攻击成功率，揭示当前防御盲点。


<details>
  <summary>Details</summary>
Motivation: 现有研究专注于设计隐蔽攻击以测试防御，忽略了对语义触发后门攻击这一更现实情况的研究，作者旨在使隐蔽技术与实际威胁模型保持一致。

Method: 利用自然语言隐写术的无害属性，应用梯度引导的数据优化过程，将语义触发种子转化为隐写载体。

Result: 在不同实验设置下，SteganoBackdoor在比先前方法低一个数量级的数据投毒率下实现了超过99%的攻击成功率，且能有效躲避一系列数据级防御。

Conclusion: SteganoBackdoor揭示了当前防御中的一个紧迫盲点，需要立即关注对抗性数据防御和现实世界威胁建模。

Abstract: Transformer models are foundational to natural language processing (NLP) applications, yet remain vulnerable to backdoor attacks introduced through poisoned data, which implant hidden behaviors during training. To strengthen the ability to prevent such compromises, recent research has focused on designing increasingly stealthy attacks to stress-test existing defenses, pairing backdoor behaviors with stylized artifact or token-level perturbation triggers. However, this trend diverts attention from the harder and more realistic case: making the model respond to semantic triggers such as specific names or entities, where a successful backdoor could manipulate outputs tied to real people or events in deployed systems. Motivated by this growing disconnect, we introduce SteganoBackdoor, bringing stealth techniques back into line with practical threat models. Leveraging innocuous properties from natural-language steganography, SteganoBackdoor applies a gradient-guided data optimization process to transform semantic trigger seeds into steganographic carriers that embed a high backdoor payload, remain fluent, and exhibit no representational resemblance to the trigger. Across diverse experimental settings, SteganoBackdoor achieves over 99% attack success at an order-of-magnitude lower data-poisoning rate than prior approaches while maintaining unparalleled evasion against a comprehensive suite of data-level defenses. By revealing this practical and covert attack, SteganoBackdoor highlights an urgent blind spot in current defenses and demands immediate attention to adversarial data defenses and real-world threat modeling.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [227] [When AI Democratizes Exploitation: LLM-Assisted Strategic Manipulation of Fair Division Algorithms](https://arxiv.org/abs/2511.14722)
*Priyanka Verma,Balagopal Unnikrishnan*

Main category: cs.CY

TL;DR: 本文探讨大语言模型（LLMs）使公平资源分配算法易被操纵，分析了租金分配场景，提出四种操纵场景，拓展了算法集体行动理论，指出其利弊及应对措施。


<details>
  <summary>Details</summary>
Motivation: 传统公平资源分配算法难被终端用户操纵，研究LLMs如何打破这一保护屏障，实现战略专业知识的普及。

Method: 对Spliddit算法的租金分配场景进行实证分析，通过用户与AI助手对话获取操纵策略。

Result: 发现LLMs能解释算法机制、识别有利偏差、生成用于偏好误报的具体数值输入，拓展了算法集体行动理论。

Conclusion: AI操纵资源分配算法有风险也有机遇，有效应对需结合算法鲁棒性、参与式设计和公平获取AI能力。

Abstract: Fair resource division algorithms, like those implemented in Spliddit platform, have traditionally been considered difficult for the end users to manipulate due to its complexities. This paper demonstrates how Large Language Models (LLMs) can dismantle these protective barriers by democratizing access to strategic expertise. Through empirical analysis of rent division scenarios on Spliddit algorithms, we show that users can obtain actionable manipulation strategies via simple conversational queries to AI assistants. We present four distinct manipulation scenarios: exclusionary collusion where majorities exploit minorities, defensive counterstrategies that backfire, benevolent subsidization of specific participants, and cost minimization coalitions. Our experiments reveal that LLMs can explain algorithmic mechanics, identify profitable deviations, and generate specific numerical inputs for coordinated preference misreporting--capabilities previously requiring deep technical knowledge. These findings extend algorithmic collective action theory from classification contexts to resource allocation scenarios, where coordinated preference manipulation replaces feature manipulation. The implications reach beyond rent division to any domain using algorithmic fairness mechanisms for resource division. While AI-enabled manipulation poses risks to system integrity, it also creates opportunities for preferential treatment of equity deserving groups. We argue that effective responses must combine algorithmic robustness, participatory design, and equitable access to AI capabilities, acknowledging that strategic sophistication is no longer a scarce resource.

</details>


### [228] [Modeling Fairness in Recruitment AI via Information Flow](https://arxiv.org/abs/2511.13793)
*Mattias Brännström,Themis Dimitra Xanthopoulou,Lili Jiang*

Main category: cs.CY

TL;DR: 本文采用信息流建模框架研究真实招聘流程，识别偏差来源、传播及影响，说明该建模可支持公平性风险分析。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以兼顾AI决策中技术与社会伦理要素的互动，需更好方法解决公平性和问责问题。

Method: 将信息流建模框架应用于真实招聘流程，通过半结构化利益相关者访谈和迭代建模构建招聘管道多层次表示。

Result: 识别出偏差可能出现的位置、传播方式及对候选人的下游影响。

Conclusion: 信息流建模可支持对复杂社会技术系统公平性风险的结构化分析，提供透明度。

Abstract: Avoiding bias and understanding the real-world consequences of AI-supported decision-making are critical to address fairness and assign accountability. Existing approaches often focus either on technical aspects, such as datasets and models, or on high-level socio-ethical considerations - rarely capturing how these elements interact in practice. In this paper, we apply an information flow-based modeling framework to a real-world recruitment process that integrates automated candidate matching with human decision-making. Through semi-structured stakeholder interviews and iterative modeling, we construct a multi-level representation of the recruitment pipeline, capturing how information is transformed, filtered, and interpreted across both algorithmic and human components. We identify where biases may emerge, how they can propagate through the system, and what downstream impacts they may have on candidates. This case study illustrates how information flow modeling can support structured analysis of fairness risks, providing transparency across complex socio-technical systems.

</details>


### [229] [GAEA: Experiences and Lessons Learned from a Country-Scale Environmental Digital Twin](https://arxiv.org/abs/2511.13807)
*Andreas Kamilaris,Chirag Padubidri,Asfa Jamil,Arslan Amin,Indrajit Kalita,Jyoti Harti,Savvas Karatsiolis,Aytac Guley*

Main category: cs.CY

TL;DR: 本文介绍塞浦路斯岛部署国家规模环境数字孪生GAEA三年的经验教训，展示地理空间分析和环境数字孪生的能力、潜力及挑战。


<details>
  <summary>Details</summary>
Motivation: 展示大规模地理空间分析和环境数字孪生的能力、潜力及面临的挑战。

Method: 描述塞浦路斯岛部署国家规模环境数字孪生GAEA三年的情况。

Result: 介绍了包含27个环境地理空间服务的GAEA，适用于多类人群。

Conclusion: 体现了大规模地理空间分析和环境数字孪生的价值与挑战。

Abstract: This paper describes the experiences and lessons learned after the deployment of a country-scale environmental digital twin on the island of Cyprus for three years. This digital twin, called GAEA, contains 27 environmental geospatial services and is suitable for urban planners, policymakers, farmers, property owners, real-estate and forestry professionals, as well as insurance companies and banks that have properties in their portfolio. This paper demonstrates the power, potential, current and future challenges of geospatial analytics and environmental digital twins on a large scale.

</details>


### [230] [Can Artificial Intelligence Accelerate Technological Progress? Researchers' Perspectives on AI in Manufacturing and Materials Science](https://arxiv.org/abs/2511.14007)
*John P. Nelson,Olajide Olugbade,Philip Shapira,Justin B. Biddle*

Main category: cs.CY

TL;DR: 通过对32位美国学术制造和材料科学研究者的访谈，探讨AI对创新加速的作用，发现其有积极和消极影响，建议支持传统研究。


<details>
  <summary>Details</summary>
Motivation: 现有对AI提升科技进步的预期缺乏对其在创新过程应用的详细研究，需明确AI加速创新的方式和程度。

Method: 对32位有AI和ML经验的美国学术制造和材料科学研究者进行访谈。

Result: 受访者用AI进行材料和制造过程建模，有成本、时间和计算方面的好处，但AI/ML工具有局限性，可能阻碍理论进步。

Conclusion: 使用AI/ML对维持性创新加速可持乐观态度，但需支持传统研究以推动制造和材料科学的重大进步。

Abstract: Artificial intelligence (AI) raises expectations of substantial increases in rates of technological and scientific progress, but such anticipations are often not connected to detailed ground-level studies of AI use in innovation processes. Accordingly, it remains unclear how and to what extent AI can accelerate innovation. To help to fill this gap, we report results from 32 interviews with U.S.-based academic manufacturing and materials sciences researchers experienced with AI and machine learning (ML) techniques. Interviewees primarily used AI for modeling of materials and manufacturing processes, facilitating cheaper and more rapid search of design spaces for materials and manufacturing processes alike. They report benefits including cost, time, and computation savings in technology development. However, interviewees also report that AI/ML tools are unreliable outside design spaces for which dense data are already available; that they require skilled and judicious application in tandem with older research techniques; and that AI/ML tools may detrimentally circumvent opportunities for disruptive theoretical advancement. Based on these results, we suggest there is reason for optimism about acceleration in sustaining innovations through the use of to AI/ML; but that support for conventional empirical, computational, and theoretical research is required to maintain the likelihood of further major advances in manufacturing and materials science.

</details>


### [231] [DiverseClaire: Simulating Students to Improve Introductory Programming Course Materials for All CS1 Learners](https://arxiv.org/abs/2511.14198)
*Wendy Wong,Yuchao Jiang,Yuekang Li*

Main category: cs.CY

TL;DR: CS1课程传统教学格式不利神经多样性学生，引入DiverseClaire试点研究，发现需提供多格式课程材料，数据将公开。


<details>
  <summary>Details</summary>
Motivation: CS1课程采用一刀切教学格式，加重神经多样性学生认知负担，需采用关怀教学法和UDL。

Method: 用大语言模型和多样化角色模拟含神经多样性学生，基于布鲁姆分类法和UDL对比传统与转化后的课件，用平均分评估。

Result: 模拟的神经多样性学生因课件格式问题学习困难。

Conclusion: 需为不同学习者偏好提供多格式课程材料，试点研究数据将助力未来CS1教师。

Abstract: Although CS programs are booming, introductory courses like CS1 still adopt a one-size-fits-all formats that can exacerbate cognitive load and discourage learners with autism, ADHD, dyslexia and other neurological conditions. These call for compassionate pedagogies and Universal Design For Learning (UDL) to create learning environments and materials where cognitive diversity is welcomed. To address this, we introduce DiverseClaire a pilot study, which simulates students including neurodiverse profiles using LLMs and diverse personas. By leveraging Bloom's Taxonomy and UDL, DiverseClaire compared UDL-transformed lecture slides with traditional formats. To evaluate DiverseClaire controlled experiments, we used the evaluation metric the average score. The findings revealed that the simulated neurodiverse students struggled with learning due to lecture slides that were in inaccessible formats. These results highlight the need to provide course materials in multiple formats for diverse learner preferences. Data from our pilot study will be made available to assist future CS1 instructors.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [232] [Apo2Mol: 3D Molecule Generation via Dynamic Pocket-Aware Diffusion Models](https://arxiv.org/abs/2511.14559)
*Xinzhe Zheng,Shiyu Jiang,Gustavo Seabra,Chenglong Li,Yanjun Li*

Main category: q-bio.BM

TL;DR: 提出Apo2Mol框架用于考虑蛋白结合口袋构象灵活性的3D分子设计，性能达最优。


<details>
  <summary>Details</summary>
Motivation: 当前多数基于结构的药物设计方法假设蛋白结合口袋是刚性的，忽略其灵活性和配体结合引起的构象重排，限制了实际应用。

Method: 提出Apo2Mol框架，整理超24000个实验解析的蛋白apo - holo结构对数据集，采用全原子分层图扩散模型，从输入的apo状态同时生成3D配体分子及其对应的holo口袋构象。

Result: Apo2Mol在生成高亲和力配体方面达到了最优性能，并能准确捕捉现实的蛋白口袋构象变化。

Conclusion: Apo2Mol框架考虑蛋白结合口袋构象灵活性，在3D分子设计中具有良好效果。

Abstract: Deep generative models are rapidly advancing structure-based drug design, offering substantial promise for generating small molecule ligands that bind to specific protein targets. However, most current approaches assume a rigid protein binding pocket, neglecting the intrinsic flexibility of proteins and the conformational rearrangements induced by ligand binding, limiting their applicability in practical drug discovery. Here, we propose Apo2Mol, a diffusion-based generative framework for 3D molecule design that explicitly accounts for conformational flexibility in protein binding pockets. To support this, we curate a dataset of over 24,000 experimentally resolved apo-holo structure pairs from the Protein Data Bank, enabling the characterization of protein structure changes associated with ligand binding. Apo2Mol employs a full-atom hierarchical graph-based diffusion model that simultaneously generates 3D ligand molecules and their corresponding holo pocket conformations from input apo states. Empirical studies demonstrate that Apo2Mol can achieve state-of-the-art performance in generating high-affinity ligands and accurately capture realistic protein pocket conformational changes.

</details>


<div id='econ.TH'></div>

# econ.TH [[Back]](#toc)

### [233] [Different Forms of Imbalance in Strongly Playable Discrete Games II: Multi-Player RPS Games](https://arxiv.org/abs/2511.13736)
*Itai Maimon*

Main category: econ.TH

TL;DR: 本文回顾先前对游戏可玩性和平衡性的定义，基于猜想证明m<50玩家的经典石头剪刀布游戏变体是强可玩性游戏，且玩家数趋于无穷时该游戏使不平衡形式最大化。


<details>
  <summary>Details</summary>
Motivation: 对经典石头剪刀布游戏的变体和推广进行研究，探讨其可玩性和平衡性。

Method: 重新引入先前对游戏可玩性和平衡性的定义，基于猜想进行证明。

Result: 证明了m<50玩家的游戏是强可玩性RPS游戏，且玩家数趋于无穷时游戏使不平衡形式最大化。

Conclusion: 在给定猜想的情况下，该游戏推广到m<50玩家有良好的可玩性，且在玩家数无穷时有特定的不平衡特性。

Abstract: Classic Rock-Paper-Scissors, RPS, has seen many variants and generalizations in the past several years. In the previous paper, we defined playability and balance for games. We used these definitions to show that different forms of imbalance agree on the most balanced and least balanced form of playable two-player n-object RPS games, referred to as (2,n)-RPS. We reintroduce these definitions here and show that, given a conjecture, the generalization of this game for m<50 players is a strongly playable RPS game. We also show that this game maximizes these forms of imbalance in the limit as the number of players goes to infinity.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [234] [US Code growth 1991-2025](https://arxiv.org/abs/2511.13747)
*Christopher Mantzaris,Ajda Fošner*

Main category: cs.DL

TL;DR: 本文统计2010年以来美国联邦有效永久法律的字数，发现2025版字数最多，多数年份字数增长，且平均字符数也在增加，还探讨了法律过长的弊端及可能的解决办法。


<details>
  <summary>Details</summary>
Motivation: 统计美国联邦有效永久法律的字数情况，研究法律字数变化特征及探讨法律过长问题。

Method: 统计美国法典中有效和永久联邦法律的字数。

Result: 2025版美国法典字数最多，33年中有30年字数增长，33年平均字符数均增长。

Conclusion: 指出法律过长存在弊端，并提及可能的解决办法。

Abstract: This is the first scientific article since 2010 counting the words which are effective and permanent federal law in the United States (US) Code. The latest version of the US Code --published in 2025-- is the largest since 1991, encompassing over 24.4 million words. The low since 1991 was 1993 at roughly 15 million words. The word count grew in 30 out of 33 years. The average characters per word --potentially indicative of complexity-- grew even in all 33 years. The research also touches upon why it is undesirable for laws to be longer and longer and mentions possible remedies.

</details>


### [235] [Review of Passenger Flow Modelling Approaches Based on a Bibliometric Analysis](https://arxiv.org/abs/2511.13742)
*Jonathan Hecht,Weilian Li,Ziyue Li,Youness Dehbi*

Main category: cs.DL

TL;DR: 对1984 - 2024年814篇本地公共交通短期客流预测领域文献进行计量分析，揭示研究趋势和现存问题。


<details>
  <summary>Details</summary>
Motivation: 全面了解本地公共交通短期客流预测领域的研究现状、趋势及存在的问题。

Method: 运用常见文献计量分析工具，开发引文网络变体并进行主题建模。

Result: 2008年前研究活动零散，之后加速，研究方法从传统统计和机器学习转向深度学习架构；发现空间、语言和模式偏差，验证并量化已有文献发现。

Conclusion: 指出领域存在数据融合受限、开放数据不足、模型可解释性等问题，基础模型相关性增长值得关注。

Abstract: This paper presents a bibliometric analysis of the field of short-term passenger flow forecasting within local public transit, covering 814 publications that span from 1984 to 2024. In addition to common bibliometric analysis tools, a variant of a citation network was developed, and topic modelling was conducted. The analysis reveals that research activity exhibited sporadic patterns prior to 2008, followed by a marked acceleration, characterised by a shift from conventional statistical and machine learning methodologies (e.g., ARIMA, SVM, and basic neural networks) to specialised deep learning architectures. Based on this insight, a connection to more general fields such as machine learning and time series modelling was established. In addition to modelling, spatial, linguistic, and modal biases were identified and findings from existing secondary literature were validated and quantified. This revealed existing gaps, such as constrained data fusion, open (multivariate) data, and underappreciated challenges related to model interpretability, cost-efficiency, and a balance between algorithmic performance and practical deployment considerations. In connection with the superordinate fields, the growth in relevance of foundation models is also noteworthy.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [236] [GeoPl@ntNet: A Platform for Exploring Essential Biodiversity Variables](https://arxiv.org/abs/2511.13790)
*Lukas Picek,César Leblanc,Alexis Joly,Pierre Bonnet,Rémi Palard,Maximilien Servajean*

Main category: q-bio.QM

TL;DR: 介绍了交互式网络应用GeoPl@ntNet，可通过动态地图和信息表让用户了解生物多样性变量，能探索高分辨率地图并生成区域报告。


<details>
  <summary>Details</summary>
Motivation: 让每个人都能通过动态地图和信息表获取和理解基本生物多样性变量。

Method: 通过涉及卷积神经网络和大语言模型的级联管道开发高分辨率地图。

Result: 提供直观且信息丰富的界面，可探索欧洲物种分布、栖息地类型和生物多样性指标，支持特定区域探索并生成报告。

Conclusion: GeoPl@ntNet是一个有助于用户了解生物多样性的有效工具。

Abstract: This paper describes GeoPl@ntNet, an interactive web application designed to make Essential Biodiversity Variables accessible and understandable to everyone through dynamic maps and fact sheets. Its core purpose is to allow users to explore high-resolution AI-generated maps of species distributions, habitat types, and biodiversity indicators across Europe. These maps, developed through a cascading pipeline involving convolutional neural networks and large language models, provide an intuitive yet information-rich interface to better understand biodiversity, with resolutions as precise as 50x50 meters. The website also enables exploration of specific regions, allowing users to select areas of interest on the map (e.g., urban green spaces, protected areas, or riverbanks) to view local species and their coverage. Additionally, GeoPl@ntNet generates comprehensive reports for selected regions, including insights into the number of protected species, invasive species, and endemic species.

</details>


### [237] [XAI-Driven Deep Learning for Protein Sequence Functional Group Classification](https://arxiv.org/abs/2511.13791)
*Pratik Chakraborty,Aryan Bhargava*

Main category: q-bio.QM

TL;DR: 本文提出基于深度学习的蛋白质序列功能组分类框架，比较不同架构模型，CNN准确率最高，还用可解释AI技术发现有生物意义的序列基序。


<details>
  <summary>Details</summary>
Motivation: 蛋白质准确分类对理解结构 - 功能关系等至关重要，需开发有效分类方法。

Method: 实现CNN、BiLSTM、CNN - BiLSTM混合和带注意力机制的CNN四种架构，用k - mer整数编码训练，应用Grad - CAM和Integrated Gradients解释模型。

Result: CNN验证准确率达91.8%，发现富含组氨酸、天冬氨酸等的序列基序。

Conclusion: 深度学习模型能揭示功能相关生化特征，弥合蛋白质序列分析中预测准确性与生物学可解释性的差距。

Abstract: Proteins perform essential biological functions, and accurate classification of their sequences is critical for understanding structure-function relationships, enzyme mechanisms, and molecular interactions. This study presents a deep learning-based framework for functional group classification of protein sequences derived from the Protein Data Bank (PDB). Four architectures were implemented: Convolutional Neural Network (CNN), Bidirectional Long Short-Term Memory (BiLSTM), CNN-BiLSTM hybrid, and CNN with Attention. Each model was trained using k-mer integer encoding to capture both local and long-range dependencies. Among these, the CNN achieved the highest validation accuracy of 91.8%, demonstrating the effectiveness of localized motif detection. Explainable AI techniques, including Grad-CAM and Integrated Gradients, were applied to interpret model predictions and identify biologically meaningful sequence motifs. The discovered motifs, enriched in histidine, aspartate, glutamate, and lysine, represent amino acid residues commonly found in catalytic and metal-binding regions of transferase enzymes. These findings highlight that deep learning models can uncover functionally relevant biochemical signatures, bridging the gap between predictive accuracy and biological interpretability in protein sequence analysis.

</details>


### [238] [MAT-MPNN: A Mobility-Aware Transformer-MPNN Model for Dynamic Spatiotemporal Prediction of HIV Diagnoses in California, Florida, and New England](https://arxiv.org/abs/2511.13797)
*Zhaoxuan Wang,Weichen Kang,Yutian Han,Lingyuan Zhao,Bo Li*

Main category: q-bio.QM

TL;DR: 提出MAT - MPNN框架预测美国部分地区县级HIV诊断率，相比基线模型降低误差、提升指标，证明考虑移动性的动态空间结构可增强时空流行病学预测的准确性和校准度。


<details>
  <summary>Details</summary>
Motivation: HIV是全球健康挑战，预测HIV诊断是重要研究领域，但捕捉HIV传播的复杂时空依赖有挑战，传统MPNN模型无法表示非相邻县的相互作用。

Method: 提出MAT - MPNN框架，结合Transformer编码器提取的时间特征和通过移动性图生成器（MGG）捕获的空间关系，MGG结合地理和人口信息改进传统邻接矩阵。

Result: 相比Transformer MPNN模型，MAT - MPNN在佛罗里达、加利福尼亚和新英格兰分别降低均方预测误差27.9%、39.1%和12.5%，提升预测模型选择准则7.7%、3.5%和3.9%；在佛罗里达和新英格兰比SVAR模型效果好，在加利福尼亚表现相当。

Conclusion: 应用移动性感知的动态空间结构能显著提高时空流行病学预测的准确性和校准度。

Abstract: Human Immunodeficiency Virus (HIV) has posed a major global health challenge for decades, and forecasting HIV diagnoses continues to be a critical area of research. However, capturing the complex spatial and temporal dependencies of HIV transmission remains challenging. Conventional Message Passing Neural Network (MPNN) models rely on a fixed binary adjacency matrix that only encodes geographic adjacency, which is unable to represent interactions between non-contiguous counties. Our study proposes a deep learning architecture Mobility-Aware Transformer-Message Passing Neural Network (MAT-MPNN) framework to predict county-level HIV diagnosis rates across California, Florida, and the New England region. The model combines temporal features extracted by a Transformer encoder with spatial relationships captured through a Mobility Graph Generator (MGG). The MGG improves conventional adjacency matrices by combining geographic and demographic information. Compared with the best-performing hybrid baseline, the Transformer MPNN model, MAT-MPNN reduced the Mean Squared Prediction Error (MSPE) by 27.9% in Florida, 39.1% in California, and 12.5% in New England, and improved the Predictive Model Choice Criterion (PMCC) by 7.7%, 3.5%, and 3.9%, respectively. MAT-MPNN also achieved better results than the Spatially Varying Auto-Regressive (SVAR) model in Florida and New England, with comparable performance in California. These results demonstrate that applying mobility-aware dynamic spatial structures substantially enhances predictive accuracy and calibration in spatiotemporal epidemiological prediction.

</details>


<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [239] [CellStream: Dynamical Optimal Transport Informed Embeddings for Reconstructing Cellular Trajectories from Snapshots Data](https://arxiv.org/abs/2511.13786)
*Yue Ling,Peiqi Zhang,Zhenyi Zhang,Peijie Zhou*

Main category: q-bio.GN

TL;DR: 现有单细胞RNA测序技术有局限，CellStream框架可联合学习嵌入和细胞动力学，在模拟和真实数据上效果良好，提供新工具。


<details>
  <summary>Details</summary>
Motivation: 当前单细胞RNA测序技术只能提供稀疏、静态细胞状态快照，且受技术噪声影响，多数现有方法处理轨迹推断和嵌入构建时忽略时间结构。

Method: 引入CellStream框架，通过将自动编码器与非平衡动态最优传输相结合，从单细胞快照数据中联合学习嵌入和细胞动力学。

Result: CellStream生成的嵌入能捕捉时间发育过程，与底层数据流形高度一致，在模拟和真实数据上比现有方法有显著定量改进。

Conclusion: CellStream为从单细胞基因表达的噪声静态快照中学习和表示连续流提供了新工具。

Abstract: Single-cell RNA sequencing (scRNA-seq), especially temporally resolved datasets, enables genome-wide profiling of gene expression dynamics at single-cell resolution across discrete time points. However, current technologies provide only sparse, static snapshots of cell states and are inherently influenced by technical noise, complicating the inference and representation of continuous transcriptional dynamics. Although embedding methods can reduce dimensionality and mitigate technical noise, the majority of existing approaches typically treat trajectory inference separately from embedding construction, often neglecting temporal structure. To address this challenge, here we introduce CellStream, a novel deep learning framework that jointly learns embedding and cellular dynamics from single-cell snapshot data by integrating an autoencoder with unbalanced dynamical optimal transport. Compared to existing methods, CellStream generates dynamics-informed embeddings that robustly capture temporal developmental processes while maintaining high consistency with the underlying data manifold. We demonstrate CellStream's effectiveness on both simulated datasets and real scRNA-seq data, including spatial transcriptomics. Our experiments indicate significant quantitative improvements over state-of-the-art methods in representing cellular trajectories with enhanced temporal coherence and reduced noise sensitivity. Overall, CellStream provides a new tool for learning and representing continuous streams from the noisy, static snapshots of single-cell gene expression.

</details>


### [240] [Near-Lossless Model Compression Enables Longer Context Inference in DNA Large Language Models](https://arxiv.org/abs/2511.14694)
*Rui Zhu,Xiaopu Zhou,Haixu Tang,Stephen W. Scherer,Lucila Ohno-Machado*

Main category: q-bio.GN

TL;DR: 本文提出FOCUS模块解决DNA大语言模型在处理长序列时的计算和内存问题，验证其在减少内存、提高推理效率和保持保真度方面的效果。


<details>
  <summary>Details</summary>
Motivation: DNA大语言模型在处理长序列时，自注意力的二次计算成本和键值缓存所需内存增长两大约束阻碍其实践应用。

Method: 引入FOCUS模块，结合k - mer表示和可学习的分层压缩，采用共享边界窗口方案；在基于Evo - 2的DNA大语言模型上验证，使用自监督训练和随机压缩调度。

Result: 在保留的人类染色体上，FOCUS实现近无损保真度；相比无压缩基线，减少键值缓存内存，将有效推理缩放从O(N^2)转换为接近线性的O(N)，在商用GPU上实现约100倍长的推理窗口。

Conclusion: FOCUS模块能有效解决DNA大语言模型处理长序列的计算和内存问题，提升模型性能。

Abstract: Trained on massive cross-species DNA corpora, DNA large language models (LLMs) learn the fundamental "grammar" and evolutionary patterns of genomic sequences. This makes them powerful priors for DNA sequence modeling, particularly over long ranges. However, two major constraints hinder their use in practice: the quadratic computational cost of self-attention and the growing memory required for key-value (KV) caches during autoregressive decoding. These constraints force the use of heuristics such as fixed-window truncation or sliding windows, which compromise fidelity on ultra-long sequences by discarding distant information. We introduce FOCUS (Feature-Oriented Compression for Ultra-long Self-attention), a progressive context-compression module that can be plugged into pretrained DNA LLMs. FOCUS combines the established k-mer representation in genomics with learnable hierarchical compression: it inserts summary tokens at k-mer granularity and progressively compresses attention key and value activations across multiple Transformer layers, retaining only the summary KV states across windows while discarding ordinary-token KV. A shared-boundary windowing scheme yields a stationary cross-window interface that propagates long-range information with minimal loss. We validate FOCUS on an Evo-2-based DNA LLM fine-tuned on GRCh38 chromosome 1 with self-supervised training and randomized compression schedules to promote robustness across compression ratios. On held-out human chromosomes, FOCUS achieves near-lossless fidelity: compressing a 1 kb context into only 10 summary tokens (about 100x) shifts the average per-nucleotide probability by only about 0.0004. Compared to a baseline without compression, FOCUS reduces KV-cache memory and converts effective inference scaling from O(N^2) to near-linear O(N), enabling about 100x longer inference windows on commodity GPUs with near-lossless fidelity.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [241] [Online learning of subgrid-scale models for quasi-geostrophic turbulence in planetary interiors](https://arxiv.org/abs/2511.14581)
*Hugo Frezat,Thomas Gastine,Alexandre Fournier*

Main category: physics.flu-dyn

TL;DR: 本文探讨用机器学习表示次网格尺度（SGS）动力学，在轴对称有界域中训练SGS模型，验证其稳定性和准确性，为行星和恒星内部动力学SGS模型开发提供方向。


<details>
  <summary>Details</summary>
Motivation: 以往研究多关注理想周期性区域，忽略行星内部机械边界，本文旨在解决该问题。

Method: 使用伪谱可微求解器对二维准地转湍流进行建模，进行在线学习，设置三种不同几何和旋转率的配置，用规定分析强迫驱动流。

Result: 在所有配置中，仅用一个周转时间数据训练的SGS模型在至少比训练期长百倍的积分中保持稳定和准确，能重现远超训练时长的慢过程。

Conclusion: 研究为行星和恒星内部动力学（包括发电机过程）的SGS模型开发提供了有前景的方向。

Abstract: The use of machine learning to represent subgrid-scale (SGS) dynamics is now well established in weather forecasting and climate modelling. Recent advances have demonstrated that SGS models trained via ``online'' end-to-end learning -- where the dynamical solver operating on the filtered equations participates in the training -- can outperform traditional physics-based approaches. Most studies, however, have focused on idealised periodic domains, neglecting the mechanical boundaries present e.g. in planetary interiors. To address this issue, we consider two-dimensional quasi-geostrophic turbulent flow in an axisymmetric bounded domain that we model using a pseudo-spectral differentiable solver, thereby enabling online learning. We examine three configurations, varying the geometry (between an exponential container and a spherical shell) and the rotation rate. Flow is driven by a prescribed analytical forcing, allowing for precise control over the energy injection scale and an exact estimate of the power input. We evaluate the accuracy of the online-trained SGS model against the reference direct numerical simulation using integral quantities and spectral diagnostics. In all configurations, we show that an SGS model trained on data spanning only one turnover time remains stable and accurate over integrations at least a hundred times longer than the training period. Moreover, we demonstrate the model's remarkable ability to reproduce slow processes occurring on time scales far exceeding the training duration, such as the inward drift of jets in the spherical shell. These results suggest a promising path towards developing SGS models for planetary and stellar interior dynamics, including dynamo processes.

</details>


<div id='cond-mat.soft'></div>

# cond-mat.soft [[Back]](#toc)

### [242] [Active Matter as a framework for living systems-inspired Robophysics](https://arxiv.org/abs/2511.14624)
*Giulia Janzen,Gaia Maselli,Juan F. Jimenez,Lia Garcia-Perez,D A Matoz Fernandez,Chantal Valeriani*

Main category: cond-mat.soft

TL;DR: 文章探讨受生物启发的机器人集体面临的挑战，介绍结合主动物质物理和生物学原理的研究。


<details>
  <summary>Details</summary>
Motivation: 解决目前机器人在个体运动和集体协作方面存在的效率等局限性问题。

Method: 文章是观点性文章，未提及具体研究方法，只是提及结合主动物质物理和生物学原理进行机器人集群建模和设计。

Result: 未提及具体研究结果。

Conclusion: 未提及明确结论，主要强调了关键挑战和相关研究方向。

Abstract: Robophysics investigates the physical principles that govern living-like robots operating in complex, realworld environments. Despite remarkable technological advances, robots continue to face fundamental efficiency limitations. At the level of individual units, locomotion remains a challenge, while at the collective level, robot swarms struggle to achieve shared purpose, coordination, communication, and cost efficiency. This perspective article examines the key challenges faced by bio-inspired robotic collectives and highlights recent research efforts that incorporate principles from active-matter physics and biology into the modeling and design of robot swarms.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [243] [DualLaguerreNet: A Decoupled Spectral Filter GNN and the Uncovering of the Flexibility-Stability Trade-off](https://arxiv.org/abs/2511.13729)
*Huseyin Goksu*

Main category: eess.SP

TL;DR: 提出DualLaguerreNet解决单滤波器GNN的‘妥协’问题，在复杂异质任务表现佳，但在简单同质任务表现差，揭示灵活性 - 稳定性权衡。


<details>
  <summary>Details</summary>
Motivation: 解决基于谱滤波器的单滤波器GNN模型的‘妥协’问题。

Method: 提出DualLaguerreNet，将图拉普拉斯算子拆分为低频和高频两部分，学习两个独立的自适应拉盖尔多项式滤波器。

Result: DualLaguerreNet在复杂异质任务上达到SOTA，在简单同质任务上表现不佳。

Conclusion: 提出新的SOTA架构用于异质性任务，同时分析了自适应GNN滤波器设计中固有的偏差 - 方差权衡。

Abstract: Graph Neural Networks (GNNs) based on spectral filters, such as the Adaptive Orthogonal Polynomial Filter (AOPF) class (e.g., LaguerreNet), have shown promise in unifying the solutions for heterophily and over-smoothing. However, these single-filter models suffer from a "compromise" problem, as their single adaptive parameter (e.g., alpha) must learn a suboptimal, averaged response across the entire graph spectrum. In this paper, we propose DualLaguerreNet, a novel GNN architecture that solves this by introducing "Decoupled Spectral Flexibility." DualLaguerreNet splits the graph Laplacian into two operators, L_low (low-frequency) and L_high (high-frequency), and learns two independent, adaptive Laguerre polynomial filters, parameterized by alpha_1 and alpha_2, respectively. This work, however, uncovers a deeper finding. While our experiments show DualLaguerreNet's flexibility allows it to achieve state-of-the-art results on complex heterophilic tasks (outperforming LaguerreNet), it simultaneously underperforms on simpler, homophilic tasks. We identify this as a fundamental "Flexibility-Stability Trade-off". The increased parameterization (2x filter parameters and 2x model parameters) leads to overfitting on simple tasks, demonstrating that the "compromise" of simpler models acts as a crucial regularizer. This paper presents a new SOTA architecture for heterophily while providing a critical analysis of the bias-variance trade-off inherent in adaptive GNN filter design.

</details>


### [244] [Compute-in-Memory Implementation of State Space Models for Event Sequence Processing](https://arxiv.org/abs/2511.13912)
*Xiaoyu Zhang,Mingtao Hu,Sen Lu,Soohyeon Kim,Eric Yeu-Jer Lee,Yuyang Liu,Wei D. Lu*

Main category: eess.SP

TL;DR: 本文提出在节能的内存计算（CIM）硬件中实现状态空间模型（SSM）的方法，可实现实时、事件驱动处理，系统兼具高精度和高能效。


<details>
  <summary>Details</summary>
Motivation: 状态空间模型在长序列处理中表现出色，期望在节能的CIM硬件中实现该模型以进行实时、事件驱动处理。

Method: 重新参数化模型使其使用实值系数和共享衰减常数，利用设备动态和对角化状态转移参数，在基于交叉开关的CIM系统中结合具有短期记忆效应的忆阻器实现状态演化。

Result: 所提出的系统在事件驱动的视觉和音频任务中兼具高精度和高能效，支持全异步处理。

Conclusion: 通过算法和硬件协同设计，实现的系统可有效处理事件驱动任务，具备高精度和高能效的优势。

Abstract: State space models (SSMs) have recently emerged as a powerful framework for long sequence processing, outperforming traditional methods on diverse benchmarks. Fundamentally, SSMs can generalize both recurrent and convolutional networks and have been shown to even capture key functions of biological systems. Here we report an approach to implement SSMs in energy-efficient compute-in-memory (CIM) hardware to achieve real-time, event-driven processing. Our work re-parameterizes the model to function with real-valued coefficients and shared decay constants, reducing the complexity of model mapping onto practical hardware systems. By leveraging device dynamics and diagonalized state transition parameters, the state evolution can be natively implemented in crossbar-based CIM systems combined with memristors exhibiting short-term memory effects. Through this algorithm and hardware co-design, we show the proposed system offers both high accuracy and high energy efficiency while supporting fully asynchronous processing for event-based vision and audio tasks.

</details>


### [245] [GegenbauerNet: Finding the Optimal Compromise in the GNN Flexibility-Stability Trade-off](https://arxiv.org/abs/2511.13730)
*Huseyin Goksu*

Main category: eess.SP

TL;DR: 提出GegenbauerNet解决谱图神经网络在[-1, 1]域的灵活性 - 稳定性权衡问题，还进行K消融研究并给出设计原则。


<details>
  <summary>Details</summary>
Motivation: 谱图神经网络在[-1, 1]域存在灵活性 - 稳定性权衡，此前工作显示稳定比自适应更重要，需找到最优折衷。

Method: 提出基于Gegenbauer多项式的GegenbauerNet，通过强制对称并允许学习一个形状参数来限制灵活性；进行综合K消融研究。

Result: GegenbauerNet在关键局部过滤机制中表现优越；全自适应L - JacobiNet在高K过滤任务中性能最佳。

Conclusion: 在[-1, 1]谱域中，最优滤波器关键取决于目标局部性(K)和可接受的设计偏差水平。

Abstract: Spectral Graph Neural Networks (GNNs) operating in the canonical [-1, 1] domain (like ChebyNet and its adaptive generalization, L-JacobiNet) face a fundamental Flexibility-Stability Trade-off. Our previous work revealed a critical puzzle: the 2-parameter adaptive L-JacobiNet often suffered from high variance and was surprisingly outperformed by the 0-parameter, stabilized-static S-JacobiNet. This suggested that stabilization was more critical than adaptation in this domain. In this paper, we propose \textbf{GegenbauerNet}, a novel GNN filter based on the Gegenbauer polynomials, to find the Optimal Compromise in this trade-off. By enforcing symmetry (alpha=beta) but allowing a single shape parameter (lambda) to be learned, GegenbauerNet limits flexibility (variance) while escaping the fixed bias of S-JacobiNet. We demonstrate that GegenbauerNet (1-parameter) achieves superior performance in the key local filtering regime (K=2 on heterophilic graphs) where overfitting is minimal, validating the hypothesis that a controlled, symmetric degree of freedom is optimal. Furthermore, our comprehensive K-ablation study across homophilic and heterophilic graphs, using 7 diverse datasets, clarifies the domain's behavior: the fully adaptive L-JacobiNet maintains the highest performance on high-K filtering tasks, showing the value of maximum flexibility when regularization is managed. This study provides crucial design principles for GNN developers, showing that in the [-1, 1] spectral domain, the optimal filter depends critically on the target locality (K) and the acceptable level of design bias.

</details>


### [246] [THD-BAR: Topology Hierarchical Derived Brain Autoregressive Modeling for EEG Generic Representations](https://arxiv.org/abs/2511.13733)
*Wenchao Yang,Weidong Yan,Wenkang Liu,Yulan Ma,Yang Li*

Main category: eess.SP

TL;DR: 提出THD - BAR模型用于EEG通用表征，在多数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如AR框架）难以捕捉EEG信号生理特征和动态空间拓扑，需充分挖掘大规模EEG模型潜力。

Method: 提出THD - BAR模型，引入BTH建立多尺度空间顺序，设计THVQ - VAE进行多尺度标记化，开发带特殊掩码策略的BAR模块。

Result: 在17个数据集上预训练，10个下游数据集5个任务中验证，THD - BAR始终优于现有方法。

Conclusion: THD - BAR具有更优的泛化和建模能力。

Abstract: Large-scale pre-trained models hold significant potential for learning universal EEG representations. However, most existing methods, particularly autoregressive (AR) frameworks, primarily rely on straightforward temporal sequencing of multi-channel EEG data, which fails to capture the rich physiological characteristics inherent to EEG signals. Moreover, their time-centered modeling approach also limits the effective representation of the dynamic spatial topology of brain activity. To address these challenges and fully exploit the potential of large-scale EEG models, we propose a novel Topology Hierarchical Derived Brain Autoregressive Modeling (THD-BAR) for EEG generic representations. The core innovation of THD-BAR lies in the introduction of the Brain Topology Hierarchy (BTH), which establishes a multi-scale spatial order for EEG channels. This hierarchical structure enables a redefinition of autoregressive learning as a "next-scale-time prediction" problem, effectively capturing both spatial and temporal dynamics. Based on BTH, we design a Topology-Hierarchical Vector Quantized-Variational Autoencoder (THVQ-VAE) for multi-scale tokenization and develop an enhanced Brain Autoregressive (BAR) module with specialized masking strategies for prediction. Through extensive large-scale pre-training on 17 datasets, followed by rigorous validation on 10 downstream datasets spanning 5 distinct tasks, THD-BAR consistently outperforms existing methods. These results highlight the superior generalization and modeling capabilities of our proposed approach.

</details>


### [247] [A Patient-Independent Neonatal Seizure Prediction Model Using Reduced Montage EEG and ECG](https://arxiv.org/abs/2511.14110)
*Sithmini Ranasingha,Agasthi Haputhanthri,Hansa Marasinghe,Nima Wickramasinghe,Kithmin Wickremasinghe,Jithangi Wanigasinghe,Chamira U. S. Edussooriya,Joshua P. Kulasingham*

Main category: eess.SP

TL;DR: 提出基于卷积神经网络的模型，利用多通道EEG和ECG信号预测新生儿癫痫发作，模型表现良好，有泛化能力和可解释性。


<details>
  <summary>Details</summary>
Motivation: 新生儿癫痫易误诊，cEEG监测昂贵且耗时，需新方法早期预测新生儿癫痫。

Method: 构建卷积神经网络模型，以多通道EEG和ECG信号的MFCC矩阵为输入特征，用10折交叉验证训练和验证模型，加入注意力机制，用SHAP解释模型。

Result: 模型平均准确率97.52%，敏感度98.31%，特异度96.39%，F1分数97.95%，提前30分钟准确预测，加入ECG和注意力机制提升性能。

Conclusion: 模型有潜力在新生儿重症监护室少监督部署，通过迁移学习对未见过的受试者有强泛化能力。

Abstract: Neonates are highly susceptible to seizures, often leading to short or long-term neurological impairments. However, clinical manifestations of neonatal seizures are subtle and often lead to misdiagnoses. This increases the risk of prolonged, untreated seizure activity and subsequent brain injury. Continuous video electroencephalogram (cEEG) monitoring is the gold standard for seizure detection. However, this is an expensive evaluation that requires expertise and time. In this study, we propose a convolutional neural network-based model for early prediction of neonatal seizures by distinguishing between interictal and preictal states of the EEG. Our model is patient-independent, enabling generalization across multiple subjects, and utilizes mel-frequency cepstral coefficient matrices extracted from multichannel EEG and electrocardiogram (ECG) signals as input features. Trained and validated on the Helsinki neonatal EEG dataset with 10-fold cross-validation, the proposed model achieved an average accuracy of 97.52%, sensitivity of 98.31%, specificity of 96.39%, and F1-score of 97.95%, enabling accurate seizure prediction up to 30 minutes before onset. The inclusion of ECG alongside EEG improved the F1-score by 1.42%, while the incorporation of an attention mechanism yielded an additional 0.5% improvement. To enhance transparency, we incorporated SHapley Additive exPlanations (SHAP) as an explainable artificial intelligence method to interpret the model and provided localization of seizure focus using scalp plots. The overall results demonstrate the model's potential for minimally supervised deployment in neonatal intensive care units, enabling timely and reliable prediction of neonatal seizures, while demonstrating strong generalization capability across unseen subjects through transfer learning.

</details>


### [248] [Doppler Invariant CNN for Signal Classification](https://arxiv.org/abs/2511.14640)
*Avi Bagchi,Dwight Hutchenson*

Main category: eess.SP

TL;DR: 本文提出含复值层的CNN架构用于信号分类，利用频率域卷积平移等变性和自适应多相采样，实验表明模型无需多普勒移位训练也能保持稳定精度，建立了抗实际影响的框架。


<details>
  <summary>Details</summary>
Motivation: 在竞争环境中进行无线电频谱监测需要可靠的自动信号分类技术，现有深度学习模型依赖强力多普勒增强，影响训练效率和可解释性。

Method: 提出含复值层的卷积神经网络架构，利用频率域卷积平移等变性，使用自适应多相采样作为池化层，网络末端采用全局平均池化层。

Result: 使用合成干扰信号数据集实验，模型在有无随机多普勒移位情况下都能保持一致分类精度，且无需对多普勒移位样本进行训练。

Conclusion: 该方法建立了基于不变性的信号分类框架，对实际影响具有可证明的鲁棒性。

Abstract: Radio spectrum monitoring in contested environments motivates the need for reliable automatic signal classification technology. Prior work highlights deep learning as a promising approach, but existing models depend on brute-force Doppler augmentation to achieve real-world generalization, which undermines both training efficiency and interpretability. In this paper, we propose a convolutional neural network (CNN) architecture with complex-valued layers that exploits convolutional shift equivariance in the frequency domain. To establish provable frequency bin shift invariance, we use adaptive polyphase sampling (APS) as pooling layers followed by a global average pooling layer at the end of the network. Using a synthetic dataset of common interference signals, experimental results demonstrate that unlike a vanilla CNN, our model maintains consistent classification accuracy with and without random Doppler shifts despite being trained on no Doppler-shifted examples. Overall, our method establishes an invariance-driven framework for signal classification that offers provable robustness against real-world effects.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [249] [Principled Coarse-Grained Acceptance for Speculative Decoding in Speech](https://arxiv.org/abs/2511.13732)
*Moran Yanuka,Paul Dixon,Eyal Finkelshtein,Daniel Rotman,Raja Giryes*

Main category: eess.AS

TL;DR: 提出Principled Coarse - Graining (PCG)方法加速语音令牌生成，在LibriTTS上提升接受率和吞吐量并保持语音质量。


<details>
  <summary>Details</summary>
Motivation: 现有投机解码在语音大语言模型中精确令牌匹配限制加速效果，接受率低。

Method: 引入PCG，在目标模型嵌入空间派生的声学相似组（ASGs）层面验证提案，定义重叠感知的粗粒度分布并进行拒绝采样。

Result: 在LibriTTS上，PCG相对标准投机解码和先前特定语音放松方法提高了接受率和吞吐量，同时保持了可懂度和说话人相似度。

Conclusion: 基于声学感知的组级接受是加速语音令牌生成并保持语音质量的简单通用方法。

Abstract: Speculative decoding accelerates autoregressive speech generation by letting a fast draft model propose tokens that a larger target model verifies. However, for speech LLMs that generate acoustic tokens, exact token matching is overly restrictive: many discrete tokens are acoustically or semantically interchangeable, reducing acceptance rates and limiting speedups. We introduce Principled Coarse-Graining (PCG), which verifies proposals at the level of Acoustic Similarity Groups (ASGs) derived from the target model's embedding space. By splitting each token's probability mass across the overlapping groups that contain it, we define an overlap-aware coarse-grained distribution and perform rejection sampling on the resulting group variable. This yields an exactness guarantee at the group level while allowing the accepted draft token to stand in for any member of the group in practice. On LibriTTS, PCG increases acceptance and throughput relative to standard speculative decoding and prior speech-specific relaxations while maintaining intelligibility and speaker similarity. These results suggest acoustically aware, group-level acceptance as a simple and general way to accelerate speech token generation while maintaining speech quality.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [250] [Towards LLM-Based Usability Analysis for Recommender User Interfaces](https://arxiv.org/abs/2511.14359)
*Sebastian Lubos,Alexander Felfernig,Damian Garber,Viet-Man Le,Thi Ngoc Trang Tran*

Main category: cs.HC

TL;DR: 本文探讨多模态大语言模型评估推荐系统界面可用性的潜力，通过实际案例评估证明其可支持启发式可用性评估以改善用户体验。


<details>
  <summary>Details</summary>
Motivation: 用户界面分析耗时且需专业知识，多模态大语言模型为自动化评估提供机会。

Method: 选取多个推荐系统平台的用户界面截图，涵盖偏好获取和推荐展示场景，让大语言模型根据不同可用性标准分析界面并提供解释性反馈。

Result: 评估证明大语言模型可支持大规模启发式可用性评估。

Conclusion: 多模态大语言模型能支持启发式可用性评估，有助于改善推荐系统的用户体验。

Abstract: Usability is a key factor in the effectiveness of recommender systems. However, the analysis of user interfaces is a time-consuming process that requires expertise. Recent advances in multimodal large language models (LLMs) offer promising opportunities to automate such evaluations. In this work, we explore the potential of multimodal LLMs to assess the usability of recommender system interfaces by considering a variety of publicly available systems as examples. We take user interface screenshots from multiple of these recommender platforms to cover both preference elicitation and recommendation presentation scenarios. An LLM is instructed to analyze these interfaces with regard to different usability criteria and provide explanatory feedback. Our evaluation demonstrates how LLMs can support heuristic-style usability assessments at scale to support the improvement of user experience.

</details>


### [251] [Developing a Grounded View of AI](https://arxiv.org/abs/2511.14013)
*Bifei Mao,Lanqing Hong*

Main category: cs.HC

TL;DR: 本文从工程角度探讨AI与基于规则软件程序能力的根本差异，提出区分AI模型行为的方法，为人类使用AI的责任及确保AI系统健全性奠定基础。


<details>
  <summary>Details</summary>
Motivation: 探究AI与基于规则软件程序能力的根本差异，明确AI的本质和局限，重视和保留基于规则的实践理性。

Method: 从工程角度审视AI行为，提出用三种决策区分AI模型行为的方法论。

Result: 提出的方法论使区分AI模型行为成为可能和可行。

Conclusion: 这是人类承担使用AI责任的前提，是确保AI系统健全以造福人类、社会和环境的良好开端。

Abstract: As a capability coming from computation, how does AI differ fundamentally from the capabilities delivered by rule-based software program? The paper examines the behavior of artificial intelligence (AI) from engineering points of view to clarify its nature and limits. The paper argues that the rationality underlying humanity's impulse to pursue, articulate, and adhere to rules deserves to be valued and preserved. Identifying where rule-based practical rationality ends is the beginning of making it aware until action. Although the rules of AI behaviors are still hidden or only weakly observable, the paper has proposed a methodology to make a sense of discrimination possible and practical to identify the distinctions of the behavior of AI models with three types of decisions. It is a prerequisite for human responsibilities with alternative possibilities, considering how and when to use AI. It would be a solid start for people to ensure AI system soundness for the well-being of humans, society, and the environment.

</details>


### [252] [SweeperBot: Making 3D Browsing Accessible through View Analysis and Visual Question Answering](https://arxiv.org/abs/2511.14567)
*Chen Chen,Cuong Nguyen,Alexa Siu,Dingzeyu Li,Nadir Weibel*

Main category: cs.HC

TL;DR: 本文介绍SweeperBot系统，帮助屏幕阅读器用户探索和比较3D模型，经两项研究验证其可行性和描述质量。


<details>
  <summary>Details</summary>
Motivation: 现有3D查看器为屏幕阅读器用户提供的3D模型信息不足，访问3D模型仍具挑战。

Method: 结合最优视图选择技术与基于生成和识别的基础模型来回答用户的视觉问题。

Result: 10名有屏幕阅读器使用经验的盲人和低视力用户的专家评审证明了SweeperBot的可行性；30名有视力参与者的调查研究验证了其生成描述的质量。

Conclusion: SweeperBot可有效辅助盲人和低视力用户探索和比较3D模型。

Abstract: Accessing 3D models remains challenging for Screen Reader (SR) users. While some existing 3D viewers allow creators to provide alternative text, they often lack sufficient detail about the 3D models. Grounded on a formative study, this paper introduces SweeperBot, a system that enables SR users to leverage visual question answering to explore and compare 3D models. SweeperBot answers SR users' visual questions by combining an optimal view selection technique with the strength of generative- and recognition-based foundation models. An expert review with 10 Blind and Low-Vision (BLV) users with SR experience demonstrated the feasibility of using SweeperBot to assist BLV users in exploring and comparing 3D models. The quality of the descriptions generated by SweeperBot was validated by a second survey study with 30 sighted participants.

</details>


### [253] [Biased Minds Meet Biased AI: How Class Imbalance Shapes Appropriate Reliance and Interacts with Human Base Rate Neglect](https://arxiv.org/abs/2511.14591)
*Nick von Felten,Johannes Schöning,Klaus Opwis,Nicolas Scharowksi*

Main category: cs.HC

TL;DR: 研究人类与AI决策交互中AI和人类偏差的复杂互动，发现类不平衡与基础比率忽视相互强化。


<details>
  <summary>Details</summary>
Motivation: 现有研究多孤立探讨AI和人类偏差，本文旨在研究两者复杂互动。

Method: 开展46人在线实验，让参与者用基于平衡或不平衡数据集训练的AI决策支持系统对三种疾病分类。

Result: 类不平衡破坏参与者对AI依赖的校准，类不平衡和基础比率忽视相互强化。

Conclusion: 提倡交互主义视角，建议进一步研究人机交互中偏差的相互强化效应。

Abstract: Humans increasingly interact with artificial intelligence (AI) in decision-making. However, both AI and humans are prone to biases. While AI and human biases have been studied extensively in isolation, this paper examines their complex interaction. Specifically, we examined how class imbalance as an AI bias affects people's ability to appropriately rely on an AI-based decision-support system, and how it interacts with base rate neglect as a human bias. In a within-subject online study (N= 46), participants classified three diseases using an AI-based decision-support system trained on either a balanced or unbalanced dataset. We found that class imbalance disrupted participants' calibration of AI reliance. Moreover, we observed mutually reinforcing effects between class imbalance and base rate neglect, offering evidence of a compound human-AI bias. Based on these findings, we advocate for an interactionist perspective and further research into the mutually reinforcing effects of biases in human-AI interaction.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [254] [AsyncVLA: Asynchronous Flow Matching for Vision-Language-Action Models](https://arxiv.org/abs/2511.14148)
*Yuhua Jiang,Shuang Cheng,Yan Ding,Feifei Gao,Biqing Qi*

Main category: cs.RO

TL;DR: 提出异步流匹配视觉语言动作框架AsyncVLA，有时间灵活性和自校正能力，实验效果好。


<details>
  <summary>Details</summary>
Motivation: 传统同步流匹配（SFM）的视觉语言动作（VLA）模型在长时任务中不稳定，缺乏动作上下文感知和异步自校正。

Method: 提出AsyncVLA框架，采用非均匀时间调度生成动作令牌，引入置信度评估器选择优化不准确动作，提出统一训练程序使模型兼具SFM和AFM模式。

Result: 在机器人操作基准测试中数据高效且有自校正能力，在通用具身评估中取得最优结果。

Conclusion: AsyncVLA框架有效，可提高KV缓存利用率，代码开源。

Abstract: Vision-language-action (VLA) models have recently emerged as a powerful paradigm for building generalist robots. However, traditional VLA models that generate actions through flow matching (FM) typically rely on rigid and uniform time schedules, i.e., synchronous FM (SFM). Without action context awareness and asynchronous self-correction, SFM becomes unstable in long-horizon tasks, where a single action error can cascade into failure. In this work, we propose asynchronous flow matching VLA (AsyncVLA), a novel framework that introduces temporal flexibility in asynchronous FM (AFM) and enables self-correction in action generation. AsyncVLA breaks from the vanilla SFM in VLA models by generating the action tokens in a non-uniform time schedule with action context awareness. Besides, our method introduces the confidence rater to extract confidence of the initially generated actions, enabling the model to selectively refine inaccurate action tokens before execution. Moreover, we propose a unified training procedure for SFM and AFM that endows a single model with both modes, improving KV-cache utilization. Extensive experiments on robotic manipulation benchmarks demonstrate that AsyncVLA is data-efficient and exhibits self-correction ability. AsyncVLA achieves state-of-the-art results across general embodied evaluations due to its asynchronous generation in AFM. Our code is available at https://github.com/YuhuaJiang2002/AsyncVLA.

</details>


### [255] [Towards Deploying VLA without Fine-Tuning: Plug-and-Play Inference-Time VLA Policy Steering via Embodied Evolutionary Diffusion](https://arxiv.org/abs/2511.14178)
*Zhuo Li,Junjia Liu,Zhipeng Dong,Tao Teng,Quentin Rouxel,Darwin Caldwell,Fei Chen*

Main category: cs.RO

TL;DR: 提出VLA - Pilot方法用于预训练VLA模型零样本部署，提升了成功率并实现零样本泛化。


<details>
  <summary>Details</summary>
Motivation: 预训练VLA策略在下游部署时性能下降，微调方法成本高不实用。

Method: 引入VLA - Pilot，一种即插即用的推理时策略引导方法，无需额外微调或数据收集。

Result: 在六种真实世界下游操作任务上评估，VLA - Pilot大幅提升了现成预训练VLA策略的成功率。

Conclusion: VLA - Pilot能实现对不同任务和机器人实体的强大零样本泛化。

Abstract: Vision-Language-Action (VLA) models have demonstrated significant potential in real-world robotic manipulation. However, pre-trained VLA policies still suffer from substantial performance degradation during downstream deployment. Although fine-tuning can mitigate this issue, its reliance on costly demonstration collection and intensive computation makes it impractical in real-world settings. In this work, we introduce VLA-Pilot, a plug-and-play inference-time policy steering method for zero-shot deployment of pre-trained VLA without any additional fine-tuning or data collection. We evaluate VLA-Pilot on six real-world downstream manipulation tasks across two distinct robotic embodiments, encompassing both in-distribution and out-of-distribution scenarios. Experimental results demonstrate that VLA-Pilot substantially boosts the success rates of off-the-shelf pre-trained VLA policies, enabling robust zero-shot generalization to diverse tasks and embodiments. Experimental videos and code are available at: https://rip4kobe.github.io/vla-pilot/.

</details>


### [256] [Going Places: Place Recognition in Artificial and Natural Systems](https://arxiv.org/abs/2511.14341)
*Michael Milford,Tobias Fischer*

Main category: cs.RO

TL;DR: 该综述综合多领域研究探索不同系统的位置识别机制，提出统一概念并指出挑战，旨在促进人工定位创新。


<details>
  <summary>Details</summary>
Motivation: 位置识别对生物导航和自主系统至关重要，需综合多领域研究探索不同系统的位置识别方式。

Method: 综合机器人系统、动物研究和人类研究的成果，分析不同系统采用的计算和表征策略。

Result: 发现不同系统有拓扑映射、线索整合和记忆管理等趋同解决方案，各系统有独特优势。

Conclusion: 提出统一概念以发展位置识别机制，指出关键挑战，促进人工定位系统与动物导航及人类空间认知研究结合。

Abstract: Place recognition, the ability to identify previously visited locations, is critical for both biological navigation and autonomous systems. This review synthesizes findings from robotic systems, animal studies, and human research to explore how different systems encode and recall place. We examine the computational and representational strategies employed across artificial systems, animals, and humans, highlighting convergent solutions such as topological mapping, cue integration, and memory management. Animal systems reveal evolved mechanisms for multimodal navigation and environmental adaptation, while human studies provide unique insights into semantic place concepts, cultural influences, and introspective capabilities. Artificial systems showcase scalable architectures and data-driven models. We propose a unifying set of concepts by which to consider and develop place recognition mechanisms and identify key challenges such as generalization, robustness, and environmental variability. This review aims to foster innovations in artificial localization by connecting future developments in artificial place recognition systems to insights from both animal navigation research and human spatial cognition studies.

</details>


### [257] [Continuous Vision-Language-Action Co-Learning with Semantic-Physical Alignment for Behavioral Cloning](https://arxiv.org/abs/2511.14396)
*Xiuxiu Qi,Yu Yang,Jiannong Cao,Luyao Bai,Chongshan Fan,Chengtai Cao,Hongpeng Wang*

Main category: cs.RO

TL;DR: 提出CCoL框架解决行为克隆中累积误差问题，实验显示有性能提升和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有行为克隆方法在解决顺序动作决策累积误差时存在物理不连续和语义 - 物理不对齐问题，影响动作克隆准确性和执行连贯性。

Method: 提出CCoL框架，通过视觉、语言和本体感受输入连续共同学习生成动作轨迹，用双向交叉注意力将语言语义与视觉运动表征关联。

Result: 在三个模拟套件中平均相对提升8.0%，在双手插入任务中最高相对增益19.2%，7自由度机器人真实测试证实泛化能力。

Conclusion: CCoL框架能解决现有行为克隆方法的问题，实现时间上一致的执行和细粒度语义接地。

Abstract: Language-conditioned manipulation facilitates human-robot interaction via behavioral cloning (BC), which learns control policies from human demonstrations and serves as a cornerstone of embodied AI. Overcoming compounding errors in sequential action decisions remains a central challenge to improving BC performance. Existing approaches mitigate compounding errors through data augmentation, expressive representation, or temporal abstraction. However, they suffer from physical discontinuities and semantic-physical misalignment, leading to inaccurate action cloning and intermittent execution. In this paper, we present Continuous vision-language-action Co-Learning with Semantic-Physical Alignment (CCoL), a novel BC framework that ensures temporally consistent execution and fine-grained semantic grounding. It generates robust and smooth action execution trajectories through continuous co-learning across vision, language, and proprioceptive inputs (e.g., robot internal states). Meanwhile, we anchor language semantics to visuomotor representations by a bidirectional cross-attention to learn contextual information for action generation, successfully overcoming the problem of semantic-physical misalignment. Extensive experiments show that CCoL achieves an average 8.0% relative improvement across three simulation suites, with up to 19.2% relative gain in human-demonstrated bimanual insertion tasks. Real-world tests on a 7-DoF robot further confirm CCoL's generalization under unseen and noisy object states.

</details>


### [258] [Self-Supervised Multisensory Pretraining for Contact-Rich Robot Reinforcement Learning](https://arxiv.org/abs/2511.14427)
*Rickmer Krohn,Vignesh Prasad,Gabriele Tiboni,Georgia Chalvatzaki*

Main category: cs.RO

TL;DR: 提出多感官动态预训练（MSDP）框架用于机器人多感官表示学习，在多任务中展示有效性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 强化学习智能体在多感官设置中学习困难，尤其在有感官噪声和动态变化的情况下。

Method: 提出MSDP框架，基于掩码自编码器，用transformer编码器从部分传感器嵌入重建多感官观测；下游策略学习采用非对称架构。

Result: 在多种扰动下学习加速、性能稳健，在模拟和现实的多任务中有效。

Conclusion: 方法对扰动有强鲁棒性，在真实机器人上以少量在线交互获高成功率，为复杂多感官机器人控制提供简单强大方案。

Abstract: Effective contact-rich manipulation requires robots to synergistically leverage vision, force, and proprioception. However, Reinforcement Learning agents struggle to learn in such multisensory settings, especially amidst sensory noise and dynamic changes. We propose MultiSensory Dynamic Pretraining (MSDP), a novel framework for learning expressive multisensory representations tailored for task-oriented policy learning. MSDP is based on masked autoencoding and trains a transformer-based encoder by reconstructing multisensory observations from only a subset of sensor embeddings, leading to cross-modal prediction and sensor fusion. For downstream policy learning, we introduce a novel asymmetric architecture, where a cross-attention mechanism allows the critic to extract dynamic, task-specific features from the frozen embeddings, while the actor receives a stable pooled representation to guide its actions. Our method demonstrates accelerated learning and robust performance under diverse perturbations, including sensor noise, and changes in object dynamics. Evaluations in multiple challenging, contact-rich robot manipulation tasks in simulation and the real world showcase the effectiveness of MSDP. Our approach exhibits strong robustness to perturbations and achieves high success rates on the real robot with as few as 6,000 online interactions, offering a simple yet powerful solution for complex multisensory robotic control.

</details>


### [259] [Masked IRL: LLM-Guided Reward Disambiguation from Demonstrations and Language](https://arxiv.org/abs/2511.14565)
*Minyoung Hwang,Alexandra Forsey-Smerek,Nathaniel Dennler,Andreea Bobu*

Main category: cs.RO

TL;DR: 现有奖励学习方法有局限，提出Masked IRL框架结合演示和语言优势，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有奖励模型在数据有限时易过拟合，现有语言条件奖励学习方法未充分利用语言消除歧义，真实指令常具歧义。

Method: 提出Masked IRL框架，用大语言模型结合演示和语言优势，从语言指令推断状态相关性掩码，对不相关状态分量强制不变性，用LLM推理消除歧义。

Result: 在模拟和真实机器人实验中，Masked IRL比现有语言条件IRL方法性能高15%，使用数据最多少4.7倍。

Conclusion: Masked IRL提高了样本效率、泛化能力和对歧义语言的鲁棒性。

Abstract: Robots can adapt to user preferences by learning reward functions from demonstrations, but with limited data, reward models often overfit to spurious correlations and fail to generalize. This happens because demonstrations show robots how to do a task but not what matters for that task, causing the model to focus on irrelevant state details. Natural language can more directly specify what the robot should focus on, and, in principle, disambiguate between many reward functions consistent with the demonstrations. However, existing language-conditioned reward learning methods typically treat instructions as simple conditioning signals, without fully exploiting their potential to resolve ambiguity. Moreover, real instructions are often ambiguous themselves, so naive conditioning is unreliable. Our key insight is that these two input types carry complementary information: demonstrations show how to act, while language specifies what is important. We propose Masked Inverse Reinforcement Learning (Masked IRL), a framework that uses large language models (LLMs) to combine the strengths of both input types. Masked IRL infers state-relevance masks from language instructions and enforces invariance to irrelevant state components. When instructions are ambiguous, it uses LLM reasoning to clarify them in the context of the demonstrations. In simulation and on a real robot, Masked IRL outperforms prior language-conditioned IRL methods by up to 15% while using up to 4.7 times less data, demonstrating improved sample-efficiency, generalization, and robustness to ambiguous language. Project page: https://MIT-CLEAR-Lab.github.io/Masked-IRL and Code: https://github.com/MIT-CLEAR-Lab/Masked-IRL

</details>


### [260] [Is Your VLM for Autonomous Driving Safety-Ready? A Comprehensive Benchmark for Evaluating External and In-Cabin Risks](https://arxiv.org/abs/2511.14592)
*Xianhui Meng,Yuchen Zhang,Zhijian Huang,Zheng Lu,Ziling Ji,Yaoyao Yin,Hongyuan Zhang,Guangfeng Jiang,Yandan Lin,Long Chen,Hangjun Ye,Li Zhang,Jun Liu,Xiaoshuai Hao*

Main category: cs.RO

TL;DR: 文章指出视觉语言模型在自动驾驶安全关键场景适用性待探索，引入DSBench基准评估其安全风险意识，评估发现性能退化，构建数据集微调可提升性能，相关资源将公开。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏同时评估外部环境风险和车内驾驶行为安全的综合基准，导致视觉语言模型在自动驾驶安全关键场景适用性不明，存在安全担忧。

Method: 引入DSBench综合驾驶安全基准，涵盖外部环境风险和车内驾驶行为安全两大类别；构建98K实例的大型数据集对现有视觉语言模型进行微调。

Result: 对主流开源和闭源视觉语言模型的评估显示，在复杂安全关键情况下性能显著下降；在数据集上微调可显著提升现有视觉语言模型的安全性能。

Conclusion: DSBench基准能全面评估视觉语言模型在安全关键场景的性能，构建的数据集微调可提升其安全性能，为自动驾驶技术发展提供了方向。

Abstract: Vision-Language Models (VLMs) show great promise for autonomous driving, but their suitability for safety-critical scenarios is largely unexplored, raising safety concerns. This issue arises from the lack of comprehensive benchmarks that assess both external environmental risks and in-cabin driving behavior safety simultaneously. To bridge this critical gap, we introduce DSBench, the first comprehensive Driving Safety Benchmark designed to assess a VLM's awareness of various safety risks in a unified manner. DSBench encompasses two major categories: external environmental risks and in-cabin driving behavior safety, divided into 10 key categories and a total of 28 sub-categories. This comprehensive evaluation covers a wide range of scenarios, ensuring a thorough assessment of VLMs' performance in safety-critical contexts. Extensive evaluations across various mainstream open-source and closed-source VLMs reveal significant performance degradation under complex safety-critical situations, highlighting urgent safety concerns. To address this, we constructed a large dataset of 98K instances focused on in-cabin and external safety scenarios, showing that fine-tuning on this dataset significantly enhances the safety performance of existing VLMs and paves the way for advancing autonomous driving technology. The benchmark toolkit, code, and model checkpoints will be publicly accessible.

</details>


### [261] [Robust Verification of Controllers under State Uncertainty via Hamilton-Jacobi Reachability Analysis](https://arxiv.org/abs/2511.14755)
*Albert Lin,Alessandro Pinto,Somil Bansal*

Main category: cs.RO

TL;DR: 提出RoVer - CoRe框架用于在感知不确定性下对基于感知的系统进行鲁棒验证，结合HJ可达性分析，还给出验证和控制器设计方法并通过案例验证。


<details>
  <summary>Details</summary>
Motivation: 随着基于感知的自主系统控制器在现实世界中日益流行，需形式化验证其在感知不确定性下的安全性和性能，但现有验证方法有局限性，HJ可达性分析在基于感知系统的应用未充分探索。

Method: 提出RoVer - CoRe框架，将系统控制器、观测函数和状态估计模块串联得到等效闭环系统以适配现有可达性框架，还提出形式化安全验证和鲁棒控制器设计的新方法。

Result: 在涉及飞机滑行和基于神经网络的漫游车导航的案例研究中证明了框架的有效性。

Conclusion: RoVer - CoRe是首个基于HJ可达性的、用于在感知不确定性下验证基于感知系统的框架，有一定应用价值。

Abstract: As perception-based controllers for autonomous systems become increasingly popular in the real world, it is important that we can formally verify their safety and performance despite perceptual uncertainty. Unfortunately, the verification of such systems remains challenging, largely due to the complexity of the controllers, which are often nonlinear, nonconvex, learning-based, and/or black-box. Prior works propose verification algorithms that are based on approximate reachability methods, but they often restrict the class of controllers and systems that can be handled or result in overly conservative analyses. Hamilton-Jacobi (HJ) reachability analysis is a popular formal verification tool for general nonlinear systems that can compute optimal reachable sets under worst-case system uncertainties; however, its application to perception-based systems is currently underexplored. In this work, we propose RoVer-CoRe, a framework for the Robust Verification of Controllers via HJ Reachability. To the best of our knowledge, RoVer-CoRe is the first HJ reachability-based framework for the verification of perception-based systems under perceptual uncertainty. Our key insight is to concatenate the system controller, observation function, and the state estimation modules to obtain an equivalent closed-loop system that is readily compatible with existing reachability frameworks. Within RoVer-CoRe, we propose novel methods for formal safety verification and robust controller design. We demonstrate the efficacy of the framework in case studies involving aircraft taxiing and NN-based rover navigation. Code is available at the link in the footnote.

</details>


### [262] [NORA-1.5: A Vision-Language-Action Model Trained using World Model- and Action-based Preference Rewards](https://arxiv.org/abs/2511.14659)
*Chia-Yu Hung,Navonil Majumder,Haoyuan Deng,Liu Renhang,Yankang Ang,Amir Zadeh,Chuan Li,Dorien Herremans,Ziwei Wang,Soujanya Poria*

Main category: cs.RO

TL;DR: 本文介绍VLA模型NORA - 1.5，通过架构增强和奖励驱动的后训练提升性能，证明其在现实部署中的可行性。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型在可靠性和泛化性上不足，尤其是跨不同实体或现实环境部署时。

Method: 在预训练的NORA骨干上添加基于流匹配的动作专家构建NORA - 1.5；开发奖励模型进行后训练，结合动作条件世界模型和偏差启发式方法，通过直接偏好优化适应目标实体。

Result: 架构增强使NORA - 1.5在模拟和现实基准测试中优于NORA和其他先进模型；奖励驱动的后训练在模拟和真实机器人环境中持续提升性能。

Conclusion: NORA - 1.5和奖励引导的后训练是实现适用于现实部署的可靠具身智能体的可行途径。

Abstract: Vision--language--action (VLA) models have recently shown promising performance on a variety of embodied tasks, yet they still fall short in reliability and generalization, especially when deployed across different embodiments or real-world environments. In this work, we introduce NORA-1.5, a VLA model built from the pre-trained NORA backbone by adding to it a flow-matching-based action expert. This architectural enhancement alone yields substantial performance gains, enabling NORA-1.5 to outperform NORA and several state-of-the-art VLA models across both simulated and real-world benchmarks. To further improve robustness and task success, we develop a set of reward models for post-training VLA policies. Our rewards combine (i) an action-conditioned world model (WM) that evaluates whether generated actions lead toward the desired goal, and (ii) a deviation-from-ground-truth heuristic that distinguishes good actions from poor ones. Using these reward signals, we construct preference datasets and adapt NORA-1.5 to target embodiments through direct preference optimization (DPO). Extensive evaluations show that reward-driven post-training consistently improves performance in both simulation and real-robot settings, demonstrating significant VLA model-reliability gains through simple yet effective reward models. Our findings highlight NORA-1.5 and reward-guided post-training as a viable path toward more dependable embodied agents suitable for real-world deployment.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [263] [nuCarla: A nuScenes-Style Bird's-Eye View Perception Dataset for CARLA Simulation](https://arxiv.org/abs/2511.13744)
*Zhijie Qiao,Zhong Cao,Henry X. Liu*

Main category: cs.CV

TL;DR: 提出大规模BEV感知数据集nuCarla，加速端到端自动驾驶闭环开发。


<details>
  <summary>Details</summary>
Motivation: 现有数据集多非交互式，缺乏支持闭环测试的标准化大规模数据集，闭环端到端模型表现不佳。

Method: 在CARLA模拟器中构建与nuScenes格式兼容的nuCarla数据集。

Result: nuCarla具有多种优势，高性能BEV骨干网络取得了先进检测结果。

Conclusion: nuCarla提供数据和模型作为开放基准，加速闭环端到端开发，推动自动驾驶可靠安全研究。

Abstract: End-to-end (E2E) autonomous driving heavily relies on closed-loop simulation, where perception, planning, and control are jointly trained and evaluated in interactive environments. Yet, most existing datasets are collected from the real world under non-interactive conditions, primarily supporting open-loop learning while offering limited value for closed-loop testing. Due to the lack of standardized, large-scale, and thoroughly verified datasets to facilitate learning of meaningful intermediate representations, such as bird's-eye-view (BEV) features, closed-loop E2E models remain far behind even simple rule-based baselines. To address this challenge, we introduce nuCarla, a large-scale, nuScenes-style BEV perception dataset built within the CARLA simulator. nuCarla features (1) full compatibility with the nuScenes format, enabling seamless transfer of real-world perception models; (2) a dataset scale comparable to nuScenes, but with more balanced class distributions; (3) direct usability for closed-loop simulation deployment; and (4) high-performance BEV backbones that achieve state-of-the-art detection results. By providing both data and models as open benchmarks, nuCarla substantially accelerates closed-loop E2E development, paving the way toward reliable and safety-aware research in autonomous driving.

</details>


### [264] [Known Meets Unknown: Mitigating Overconfidence in Open Set Recognition](https://arxiv.org/abs/2511.13775)
*Dongdong Zhao,Ranxin Fang,Changtian Song,Zhihui Liu,Jianwen Xiang*

Main category: cs.CV

TL;DR: 本文针对开放集识别中因特征空间类间重叠导致的过自信问题，提出了一个缓解过自信的框架，实验显示该框架性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 开放集识别中，未知样本与已知类语义相似时，特征空间的类间重叠会导致模型过自信，模糊已知和未知类的决策边界，影响开放集识别效果。

Method: 提出一个由基于扰动的不确定性估计模块和具有不同基于学习的分类器的未知检测模块组成的框架，前者通过可控参数扰动生成多样预测并量化预测不确定性，后者分两阶段利用估计的不确定性提升对已知和未知类的区分能力。

Result: 在三个公共数据集上的实验表明，所提框架性能优于现有开放集识别方法。

Conclusion: 所提出的框架能有效缓解类间重叠导致的过自信问题，提升开放集识别性能。

Abstract: Open Set Recognition (OSR) requires models not only to accurately classify known classes but also to effectively reject unknown samples. However, when unknown samples are semantically similar to known classes, inter-class overlap in the feature space often causes models to assign unjustifiably high confidence to them, leading to misclassification as known classes -- a phenomenon known as overconfidence. This overconfidence undermines OSR by blurring the decision boundary between known and unknown classes. To address this issue, we propose a framework that explicitly mitigates overconfidence caused by inter-class overlap. The framework consists of two components: a perturbation-based uncertainty estimation module, which applies controllable parameter perturbations to generate diverse predictions and quantify predictive uncertainty, and an unknown detection module with distinct learning-based classifiers, implemented as a two-stage procedure, which leverages the estimated uncertainty to improve discrimination between known and unknown classes, thereby enhancing OSR performance. Experimental results on three public datasets show that the proposed framework achieves superior performance over existing OSR methods.

</details>


### [265] [Temporal Object-Aware Vision Transformer for Few-Shot Video Object Detection](https://arxiv.org/abs/2511.13784)
*Yogesh Kumar,Anand Mishra*

Main category: cs.CV

TL;DR: 提出新颖的目标感知时间建模方法解决少样本视频目标检测难题，在多数据集和少样本设置下提升性能，代码公开。


<details>
  <summary>Details</summary>
Motivation: 传统检测方法需大量训练数据，少样本视频目标检测面临保持帧间时间一致性和实现新目标泛化等挑战。

Method: 采用新颖的目标感知时间建模方法，引入过滤机制跨帧选择性传播高置信度目标特征，利用少样本训练的检测和分类头进行聚焦特征传播。

Result: 在5-shot设置下，多个数据集上AP有显著提升，在1-shot、3-shot和10-shot配置下也有改进。

Conclusion: 提出的方法能有效解决少样本视频目标检测的挑战，提升检测性能。

Abstract: Few-shot Video Object Detection (FSVOD) addresses the challenge of detecting novel objects in videos with limited labeled examples, overcoming the constraints of traditional detection methods that require extensive training data. This task presents key challenges, including maintaining temporal consistency across frames affected by occlusion and appearance variations, and achieving novel object generalization without relying on complex region proposals, which are often computationally expensive and require task-specific training. Our novel object-aware temporal modeling approach addresses these challenges by incorporating a filtering mechanism that selectively propagates high-confidence object features across frames. This enables efficient feature progression, reduces noise accumulation, and enhances detection accuracy in a few-shot setting. By utilizing few-shot trained detection and classification heads with focused feature propagation, we achieve robust temporal consistency without depending on explicit object tube proposals. Our approach achieves performance gains, with AP improvements of 3.7% (FSVOD-500), 5.3% (FSYTV-40), 4.3% (VidOR), and 4.5 (VidVRD) in the 5-shot setting. Further results demonstrate improvements in 1-shot, 3-shot, and 10-shot configurations. We make the code public at: https://github.com/yogesh-iitj/fs-video-vit

</details>


### [266] [FusionFM: All-in-One Multi-Modal Image Fusion with Flow Matching](https://arxiv.org/abs/2511.13794)
*Huayi Zhu,Xiu Shu,Youqiang Xiong,Qiao Liu,Rui Chen,Di Yuan,Xiaojun Chang,Zhenyu He*

Main category: cs.CV

TL;DR: 针对现有多模态图像融合方法训练成本高、可扩展性有限和生成方法推理慢的问题，提出新方法，用流匹配范式等改进性能，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有多模态图像融合方法依赖特定任务模型，训练成本高、可扩展性有限，生成方法推理慢。

Method: 将图像融合表述为从源模态到融合图像分布的直接概率传输，用流匹配范式；收集多模型融合结果作先验，用任务感知选择函数选伪标签；引入Fusion Refiner模块；多任务场景下集成弹性权重整合和经验回放机制。

Result: 在不同融合任务中取得有竞争力的性能，显著提高采样效率，保持轻量级模型设计。

Conclusion: 所提方法有效，能解决现有多模态图像融合方法的问题。

Abstract: Current multi-modal image fusion methods typically rely on task-specific models, leading to high training costs and limited scalability. While generative methods provide a unified modeling perspective, they often suffer from slow inference due to the complex sampling trajectories from noise to image. To address this, we formulate image fusion as a direct probabilistic transport from source modalities to the fused image distribution, leveraging the flow matching paradigm to improve sampling efficiency and structural consistency. To mitigate the lack of high-quality fused images for supervision, we collect fusion results from multiple state-of-the-art models as priors, and employ a task-aware selection function to select the most reliable pseudo-labels for each task. We further introduce a Fusion Refiner module that employs a divide-and-conquer strategy to systematically identify, decompose, and enhance degraded components in selected pseudo-labels. For multi-task scenarios, we integrate elastic weight consolidation and experience replay mechanisms to preserve cross-task performance and enhance continual learning ability from both parameter stability and memory retention perspectives. Our approach achieves competitive performance across diverse fusion tasks, while significantly improving sampling efficiency and maintaining a lightweight model design. The code will be available at: https://github.com/Ist-Zhy/FusionFM.

</details>


### [267] [A Trajectory-free Crash Detection Framework with Generative Approach and Segment Map Diffusion](https://arxiv.org/abs/2511.13795)
*Weiying Shen,Hao Yu,Yu Dong,Pan Liu,Yu Han,Xin Wen*

Main category: cs.CV

TL;DR: 提出两阶段无轨迹碰撞检测框架，利用道路段地图进行碰撞检测，实验表明方法有效。


<details>
  <summary>Details</summary>
Motivation: 解决轨迹获取和车辆跟踪的局限性，开发主动安全管理策略，提高交通效率。

Method: 提出两阶段无轨迹碰撞检测框架，第一阶段用Mapfusion模型生成道路段地图，第二阶段通过比较监测地图和生成地图检测碰撞。

Result: Mapfusion基于学习的运动模式成功生成真实的道路段演化地图，在不同采样间隔下保持鲁棒性。

Conclusion: 所提出的两阶段方法能有效准确地检测碰撞。

Abstract: Real-time crash detection is essential for developing proactive safety management strategy and enhancing overall traffic efficiency. To address the limitations associated with trajectory acquisition and vehicle tracking, road segment maps recording the individual-level traffic dynamic data were directly served in crash detection. A novel two-stage trajectory-free crash detection framework, was present to generate the rational future road segment map and identify crashes. The first-stage diffusion-based segment map generation model, Mapfusion, conducts a noisy-to-normal process that progressively adds noise to the road segment map until the map is corrupted to pure Gaussian noise. The denoising process is guided by sequential embedding components capturing the temporal dynamics of segment map sequences. Furthermore, the generation model is designed to incorporate background context through ControlNet to enhance generation control. Crash detection is achieved by comparing the monitored segment map with the generations from diffusion model in second stage. Trained on non-crash vehicle motion data, Mapfusion successfully generates realistic road segment evolution maps based on learned motion patterns and remains robust across different sampling intervals. Experiments on real-world crashes indicate the effectiveness of the proposed two-stage method in accurately detecting crashes.

</details>


### [268] [Synergizing Multigrid Algorithms with Vision Transformer: A Novel Approach to Enhance the Seismic Foundation Model](https://arxiv.org/abs/2511.13800)
*Huiwen Wu,Shuo Zhang,Yi Liu,Hongbin Ye*

Main category: cs.CV

TL;DR: 文章针对现有ViTs处理地震数据的不足，提出ADATG训练策略，实验证明其有效，强调数据编码和训练策略对地震图像预训练的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有ViTs处理地震数据时忽略内在模式，无法有效抓取高低频信息，需要适合地震数据特征的预训练方法。

Method: 引入带Hilbert编码的ADATG策略，用频谱分解分离高低频成分，用分层Hilbert编码表示数据，采用先粗后细的自适应训练策略。

Result: 广泛实验证明训练方法有效且高效。

Conclusion: 强调基于地震图像高低频特征的独特性进行数据编码和训练策略的重要性，有助于提升视觉地震基础模型预训练。

Abstract: Due to the emergency and homogenization of Artificial Intelligence (AI) technology development, transformer-based foundation models have revolutionized scientific applications, such as drug discovery, materials research, and astronomy. However, seismic data presents unique characteristics that require specialized processing techniques for pretraining foundation models in seismic contexts with high- and low-frequency features playing crucial roles. Existing vision transformers (ViTs) with sequential tokenization ignore the intrinsic pattern and fail to grasp both the high- and low-frequency seismic information efficiently and effectively. This work introduces a novel adaptive two-grid foundation model training strategy (ADATG) with Hilbert encoding specifically tailored for seismogram data, leveraging the hierarchical structures inherent in seismic data. Specifically, our approach employs spectrum decomposition to separate high- and low-frequency components and utilizes hierarchical Hilbert encoding to represent the data effectively. Moreover, observing the frequency principle observed in ViTs, we propose an adaptive training strategy that initially emphasizes coarse-level information and then progressively refines the model's focus on fine-level features. Our extensive experiments demonstrate the effectiveness and efficiency of our training methods. This research highlights the importance of data encoding and training strategies informed by the distinct characteristics of high- and low-frequency features in seismic images, ultimately contributing to the enhancement of visual seismic foundation models pretraining.

</details>


### [269] [Passive Dementia Screening via Facial Temporal Micro-Dynamics Analysis of In-the-Wild Talking-Head Video](https://arxiv.org/abs/2511.13802)
*Filippo Cenacchi. Longbing Cao,Mitchell McEwan,Deborah Richards*

Main category: cs.CV

TL;DR: 本文提出从短的摄像头面对的谈话头部视频进行被动痴呆筛查，开发面部时间微动力学分析方法，还引入新数据集YT DemTalk，模型取得较好预测效果。


<details>
  <summary>Details</summary>
Motivation: 现有痴呆筛查资源多优先考虑语音或脚本化访谈，有使用限制且与语言转录相关，本文旨在开发无语言依赖的早期神经认知变化检测方法。

Method: 识别和分析无语音或文本时的时间面部运动学是否足以进行痴呆筛查，稳定面部信号，将微运动转换为可解释的面部微动态时间序列，平滑并总结为紧凑的剪辑级统计量，引入新数据集YT DemTalk。

Result: 在YT DemTalk上，消融实验确定凝视不稳定性和嘴/颌动态是最有信息的线索，轻量级浅层分类器在痴呆预测上取得了AUROC 0.953、平均精度0.961、F1分数0.851和准确率0.857的性能。

Conclusion: 面部时间微动力学分析可用于无语言依赖的痴呆筛查，且在新数据集上模型表现良好。

Abstract: We target passive dementia screening from short camera-facing talking head video, developing a facial temporal micro dynamics analysis for language free detection of early neuro cognitive change. This enables unscripted, in the wild video analysis at scale to capture natural facial behaviors, transferrable across devices, topics, and cultures without active intervention by clinicians or researchers during recording. Most existing resources prioritize speech or scripted interviews, limiting use outside clinics and coupling predictions to language and transcription. In contrast, we identify and analyze whether temporal facial kinematics, including blink dynamics, small mouth jaw motions, gaze variability, and subtle head adjustments, are sufficient for dementia screening without speech or text. By stabilizing facial signals, we convert these micro movements into interpretable facial microdynamic time series, smooth them, and summarize short windows into compact clip level statistics for screening. Each window is encoded by its activity mix (the relative share of motion across streams), thus the predictor analyzes the distribution of motion across streams rather than its magnitude, making per channel effects transparent. We also introduce YT DemTalk, a new dataset curated from publicly available, in the wild camera facing videos. It contains 300 clips (150 with self reported dementia, 150 controls) to test our model and offer a first benchmarking of the corpus. On YT DemTalk, ablations identify gaze lability and mouth/jaw dynamics as the most informative cues, and light weighted shallow classifiers could attain a dementia prediction performance of (AUROC) 0.953, 0.961 Average Precision (AP), 0.851 F1-score, and 0.857 accuracy.

</details>


### [270] [H-CNN-ViT: A Hierarchical Gated Attention Multi-Branch Model for Bladder Cancer Recurrence Prediction](https://arxiv.org/abs/2511.13869)
*Xueyang Li,Zongren Wang,Yuliang Zhang,Zixuan Pan,Yu-Jen Chen,Nishchal Sapkota,Gelei Xu,Danny Z. Chen,Yiyu Shi*

Main category: cs.CV

TL;DR: 本文引入用于膀胱癌复发预测的多序列多模态MRI数据集，提出H - CNN - ViT模型，在数据集上表现超现有模型且代码公开。


<details>
  <summary>Details</summary>
Motivation: 膀胱癌复发率高需术后监测，多序列增强MRI解读难，AI辅助诊断缺相关数据集。

Method: 引入多序列多模态MRI数据集，提出Hierarchical Gated Attention Multi - Branch模型H - CNN - ViT，多分支架构独立处理各模态。

Result: H - CNN - ViT在数据集上AUC达78.6%，超越现有模型。

Conclusion: 引入的数据集可作为未来研究基准，H - CNN - ViT模型在膀胱癌复发预测上有良好表现。

Abstract: Bladder cancer is one of the most prevalent malignancies worldwide, with a recurrence rate of up to 78%, necessitating accurate post-operative monitoring for effective patient management. Multi-sequence contrast-enhanced MRI is commonly used for recurrence detection; however, interpreting these scans remains challenging, even for experienced radiologists, due to post-surgical alterations such as scarring, swelling, and tissue remodeling. AI-assisted diagnostic tools have shown promise in improving bladder cancer recurrence prediction, yet progress in this field is hindered by the lack of dedicated multi-sequence MRI datasets for recurrence assessment study. In this work, we first introduce a curated multi-sequence, multi-modal MRI dataset specifically designed for bladder cancer recurrence prediction, establishing a valuable benchmark for future research. We then propose H-CNN-ViT, a new Hierarchical Gated Attention Multi-Branch model that enables selective weighting of features from the global (ViT) and local (CNN) paths based on contextual demands, achieving a balanced and targeted feature fusion. Our multi-branch architecture processes each modality independently, ensuring that the unique properties of each imaging channel are optimally captured and integrated. Evaluated on our dataset, H-CNN-ViT achieves an AUC of 78.6%, surpassing state-of-the-art models. Our model is publicly available at https://github.com/XLIAaron/H-CNN-ViT}.

</details>


### [271] [Hybrid Convolution Neural Network Integrated with Pseudo-Newton Boosting for Lumbar Spine Degeneration Detection](https://arxiv.org/abs/2511.13877)
*Pandiyaraju V,Abishek Karthik,Jaspin K,Kannan A,Jaime Lloret*

Main category: cs.CV

TL;DR: 本文提出结合EfficientNet和VGG19的腰椎退变分类增强模型架构，采用多层框架改进特征选择和表示，性能显著提升，有助于医学影像自动诊断工具开发。


<details>
  <summary>Details</summary>
Motivation: 克服传统迁移学习方法在医学影像高维环境下的局限，开发更有效的腰椎退变分类模型。

Method: 提出结合EfficientNet和VGG19的混合方法，引入伪牛顿增强层和稀疏特征约简层形成多层框架。

Result: 模型达到精确率0.9、召回率0.861、F1分数0.88、损失0.18、准确率88.1%，相比基线模型EfficientNet性能显著提升。

Conclusion: 该研究结果有助于医学影像自动诊断工具的开发。

Abstract: This paper proposes a new enhanced model architecture to perform classification of lumbar spine degeneration with DICOM images while using a hybrid approach, integrating EfficientNet and VGG19 together with custom-designed components. The proposed model is differentiated from traditional transfer learning methods as it incorporates a Pseudo-Newton Boosting layer along with a Sparsity-Induced Feature Reduction Layer that forms a multi-tiered framework, further improving feature selection and representation. The Pseudo-Newton Boosting layer makes smart variations of feature weights, with more detailed anatomical features, which are mostly left out in a transfer learning setup. In addition, the Sparsity-Induced Layer removes redundancy for learned features, producing lean yet robust representations for pathology in the lumbar spine. This architecture is novel as it overcomes the constraints in the traditional transfer learning approach, especially in the high-dimensional context of medical images, and achieves a significant performance boost, reaching a precision of 0.9, recall of 0.861, F1 score of 0.88, loss of 0.18, and an accuracy of 88.1%, compared to the baseline model, EfficientNet. This work will present the architectures, preprocessing pipeline, and experimental results. The results contribute to the development of automated diagnostic tools for medical images.

</details>


### [272] [MRI Plane Orientation Detection using a Context-Aware 2.5D Model](https://arxiv.org/abs/2511.14021)
*SangHyuk Kim,Daniel Haehn,Sumientra Rampersad*

Main category: cs.CV

TL;DR: 本文开发解剖平面方向分类器，采用2.5D模型提升精度，验证生成元数据在脑肿瘤检测中的效用，并将模型集成到开源Web应用。


<details>
  <summary>Details</summary>
Motivation: 自动化系统识别MRI切片解剖平面困难，缺失平面方向元数据会带来分析复杂、域偏移和诊断分类器精度降低等问题。

Method: 采用2.5D上下文感知模型，在3D切片序列和静态2D图像上训练，在脑肿瘤检测任务中使用门控策略选择性利用元数据增强预测。

Result: 2.5D方法将分类精度从98.74%提升到99.49%，减少60%错误；脑肿瘤检测中，精度从97.0%提升到98.0%，减少33.3%误诊。

Conclusion: 2.5D上下文对解剖平面分类很重要，生成的元数据能有效提升脑肿瘤检测精度，且模型集成到开源Web应用。

Abstract: Humans can easily identify anatomical planes (axial, coronal, and sagittal) on a 2D MRI slice, but automated systems struggle with this task. Missing plane orientation metadata can complicate analysis, increase domain shift when merging heterogeneous datasets, and reduce accuracy of diagnostic classifiers. This study develops a classifier that accurately generates plane orientation metadata. We adopt a 2.5D context-aware model that leverages multi-slice information to avoid ambiguity from isolated slices and enable robust feature learning. We train the 2.5D model on both 3D slice sequences and static 2D images. While our 2D reference model achieves 98.74% accuracy, our 2.5D method raises this to 99.49%, reducing errors by 60%, highlighting the importance of 2.5D context. We validate the utility of our generated metadata in a brain tumor detection task. A gated strategy selectively uses metadata-enhanced predictions based on uncertainty scores, boosting accuracy from 97.0% with an image-only model to 98.0%, reducing misdiagnoses by 33.3%. We integrate our plane orientation model into an interactive web application and provide it open-source.

</details>


### [273] [Training-free Detection of AI-generated images via Cropping Robustness](https://arxiv.org/abs/2511.14030)
*Sungik Choi,Hankook Lee,Moontae Lee*

Main category: cs.CV

TL;DR: 提出训练无关的AI生成图像检测算法WaRPAD，验证其在多数据集和模型上有竞争力且鲁棒，还适用于不同自监督模型。


<details>
  <summary>Details</summary>
Motivation: 随着视觉生成模型发展，AI生成图像检测变得关键，研究训练无关的检测方法。

Method: 提出基于自监督模型的训练无关算法WaRPAD，定义基础分数函数，对图像分块计算分数后平均得到最终检测分数。

Result: 在多种分辨率、领域的真实数据集和23种生成模型生成的图像上验证，方法有竞争力且对测试时损坏有强鲁棒性。

Conclusion: WaRPAD适用于不同自监督模型。

Abstract: AI-generated image detection has become crucial with the rapid advancement of vision-generative models. Instead of training detectors tailored to specific datasets, we study a training-free approach leveraging self-supervised models without requiring prior data knowledge. These models, pre-trained with augmentations like RandomResizedCrop, learn to produce consistent representations across varying resolutions. Motivated by this, we propose WaRPAD, a training-free AI-generated image detection algorithm based on self-supervised models. Since neighborhood pixel differences in images are highly sensitive to resizing operations, WaRPAD first defines a base score function that quantifies the sensitivity of image embeddings to perturbations along high-frequency directions extracted via Haar wavelet decomposition. To simulate robustness against cropping augmentation, we rescale each image to a multiple of the models input size, divide it into smaller patches, and compute the base score for each patch. The final detection score is then obtained by averaging the scores across all patches. We validate WaRPAD on real datasets of diverse resolutions and domains, and images generated by 23 different generative models. Our method consistently achieves competitive performance and demonstrates strong robustness to test-time corruptions. Furthermore, as invariance to RandomResizedCrop is a common training scheme across self-supervised models, we show that WaRPAD is applicable across self-supervised models.

</details>


### [274] [Zero-Training Task-Specific Model Synthesis for Few-Shot Medical Image Classification](https://arxiv.org/abs/2511.14082)
*Yao Qin,Yangyang Yan,YuanChao Yang,Jinhua Pang,Huanyong Bi,Yuan Liu,HaiHua Wang*

Main category: cs.CV

TL;DR: 提出Zero-Training Task-Specific Model Synthesis (ZS-TMS)范式，利用预训练生成引擎合成特定任务分类器参数，在少样本分类基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在医学图像分析中依赖大规模标注数据集，而医学领域数据获取和标注困难，尤其是罕见病样本稀缺，需解决此瓶颈。

Method: 提出ZS-TMS范式，使用Semantic-Guided Parameter Synthesizer (SGPS)框架，以最少的多模态任务信息为输入，利用预训练生成引擎合成特定任务分类器的全部参数。

Result: 在ISIC 2018皮肤病变数据集和自定义罕见病数据集的少样本分类基准测试中，SGPS显著优于先进的少样本和零样本学习方法，尤其在1-shot和5-shot分类的超低数据情况下。

Conclusion: 该工作为人工智能诊断工具的快速开发和部署铺平道路，尤其适用于数据严重受限的罕见病。

Abstract: Deep learning models have achieved remarkable success in medical image analysis but are fundamentally constrained by the requirement for large-scale, meticulously annotated datasets. This dependency on "big data" is a critical bottleneck in the medical domain, where patient data is inherently difficult to acquire and expert annotation is expensive, particularly for rare diseases where samples are scarce by definition. To overcome this fundamental challenge, we propose a novel paradigm: Zero-Training Task-Specific Model Synthesis (ZS-TMS). Instead of adapting a pre-existing model or training a new one, our approach leverages a large-scale, pre-trained generative engine to directly synthesize the entire set of parameters for a task-specific classifier. Our framework, the Semantic-Guided Parameter Synthesizer (SGPS), takes as input minimal, multi-modal task information as little as a single example image (1-shot) and a corresponding clinical text description to directly synthesize the entire set of parameters for a task-specific classifier.
  The generative engine interprets these inputs to generate the weights for a lightweight, efficient classifier (e.g., an EfficientNet-V2), which can be deployed for inference immediately without any task-specific training or fine-tuning. We conduct extensive evaluations on challenging few-shot classification benchmarks derived from the ISIC 2018 skin lesion dataset and a custom rare disease dataset. Our results demonstrate that SGPS establishes a new state-of-the-art, significantly outperforming advanced few-shot and zero-shot learning methods, especially in the ultra-low data regimes of 1-shot and 5-shot classification. This work paves the way for the rapid development and deployment of AI-powered diagnostic tools, particularly for the long tail of rare diseases where data is critically limited.

</details>


### [275] [Automated glenoid bone loss measurement and segmentation in CT scans for pre-operative planning in shoulder instability](https://arxiv.org/abs/2511.14083)
*Zhonghao Liu,Hanxue Gu,Qihang Li,Michael Fox,Jay M. Levin,Maciej A. Mazurowski,Brian C. Lau*

Main category: cs.CV

TL;DR: 开发并验证了用于测量三维 CT 扫描中盂骨丢失的全自动深度学习管道，该方法高效且可靠。


<details>
  <summary>Details</summary>
Motivation: 当前手动和半自动测量盂骨丢失的方法耗时且存在阅片者间差异，需要更可靠高效的方法用于肩部不稳定手术规划。

Method: 开发多阶段算法，包括用 U - Net 分割关节盂和肱骨、用网络预测关节盂边缘点、应用主成分分析等计算骨丢失百分比。

Result: 自动测量与共识读数高度一致，超过外科医生间的一致性，分类召回率良好且无高低严重程度误分类。

Conclusion: 该方法是肩部不稳定术前规划和筛查盂骨大量丢失患者的高效且临床可靠的工具。

Abstract: Reliable measurement of glenoid bone loss is essential for operative planning in shoulder instability, but current manual and semi-automated methods are time-consuming and often subject to interreader variability. We developed and validated a fully automated deep learning pipeline for measuring glenoid bone loss on three-dimensional computed tomography (CT) scans using a linear-based, en-face view, best-circle method. Shoulder CT images of 91 patients (average age, 40 years; range, 14-89 years; 65 men) were retrospectively collected along with manual labels including glenoid segmentation, landmarks, and bone loss measurements. The multi-stage algorithm has three main stages: (1) segmentation, where we developed a U-Net to automatically segment the glenoid and humerus; (2) anatomical landmark detection, where a second network predicts glenoid rim points; and (3) geometric fitting, where we applied principal component analysis (PCA), projection, and circle fitting to compute the percentage of bone loss. The automated measurements showed strong agreement with consensus readings and exceeded surgeon-to-surgeon consistency (intraclass correlation coefficient (ICC) 0.84 vs 0.78), including in low- and high-bone-loss subgroups (ICC 0.71 vs 0.63 and 0.83 vs 0.21, respectively; P < 0.001). For classifying patients into low, medium, and high bone-loss categories, the pipeline achieved a recall of 0.714 for low and 0.857 for high severity, with no low cases misclassified as high or vice versa. These results suggest that our method is a time-efficient and clinically reliable tool for preoperative planning in shoulder instability and for screening patients with substantial glenoid bone loss. Code and dataset are available at https://github.com/Edenliu1/Auto-Glenoid-Measurement-DL-Pipeline.

</details>


### [276] [Error-Driven Scene Editing for 3D Grounding in Large Language Models](https://arxiv.org/abs/2511.14086)
*Yue Zhang,Zun Wang,Han Lin,Jialu Li,Jianing Yang,Yonatan Bitton,Idan Szpektor,Mohit Bansal*

Main category: cs.CV

TL;DR: 本文提出3D场景编辑和DEER - 3D框架解决3D - LLMs在3D环境中语言与视觉和空间元素接地不准确问题，经多基准测试验证有效。


<details>
  <summary>Details</summary>
Motivation: 现有3D - LLMs在将语言准确关联3D环境中的视觉和空间元素方面存在局限，训练数据侧重于语言推理而非空间理解，存在接地偏差。

Method: 提出3D场景编辑机制生成视觉反事实，引入DEER - 3D框架，遵循“分解、诊断评估、编辑、再训练”流程，针对模型具体弱点进行3D场景编辑。

Result: 在多个3D接地和场景理解任务基准测试中，通过迭代优化，所有评估数据集的性能均得到提升。

Conclusion: 有针对性的、错误驱动的场景编辑在3D - LLMs中能有效连接语言推理能力和空间接地。

Abstract: Despite recent progress in 3D-LLMs, they remain limited in accurately grounding language to visual and spatial elements in 3D environments. This limitation stems in part from training data that focuses on language reasoning rather than spatial understanding due to scarce 3D resources, leaving inherent grounding biases unresolved. To address this, we propose 3D scene editing as a key mechanism to generate precise visual counterfactuals that mitigate these biases through fine-grained spatial manipulation, without requiring costly scene reconstruction or large-scale 3D data collection. Furthermore, to make these edits targeted and directly address the specific weaknesses of the model, we introduce DEER-3D, an error-driven framework following a structured "Decompose, Diagnostic Evaluation, Edit, and Re-train" workflow, rather than broadly or randomly augmenting data as in conventional approaches. Specifically, upon identifying a grounding failure of the 3D-LLM, our framework first diagnoses the exact predicate-level error (e.g., attribute or spatial relation). It then executes minimal, predicate-aligned 3D scene edits, such as recoloring or repositioning, to produce targeted counterfactual supervision for iterative model fine-tuning, significantly enhancing grounding accuracy. We evaluate our editing pipeline across multiple benchmarks for 3D grounding and scene understanding tasks, consistently demonstrating improvements across all evaluated datasets through iterative refinement. DEER-3D underscores the effectiveness of targeted, error-driven scene editing in bridging linguistic reasoning capabilities with spatial grounding in 3D LLMs.

</details>


### [277] [GCA-ResUNet:Image segmentation in medical images using grouped coordinate attention](https://arxiv.org/abs/2511.14087)
*Jun Ding,Shang Gao*

Main category: cs.CV

TL;DR: 本文提出GCA - ResUNet网络用于医学图像分割，在两个数据集上表现优异，展示了其高效性和准确性。


<details>
  <summary>Details</summary>
Motivation: U - Net难以捕捉长距离依赖，Transformer变体计算量大且需大量训练数据，需高效医学图像分割网络。

Method: 提出GCA - ResUNet，将分组坐标注意力（GCA）集成到ResNet - 50残差块中，GCA联合编码通道和空间位置的全局依赖。

Result: 在Synapse数据集上Dice分数达86.11%，在ACDC数据集上达92.64%，超越多个基线模型，推理快且计算效率高。

Conclusion: GCA为增强卷积架构的全局建模能力提供了实用方法，可实现高精度且资源高效的医学图像分割。

Abstract: Medical image segmentation underpins computer-aided diagnosis and therapy by supporting clinical diagnosis, preoperative planning, and disease monitoring. While U-Net style convolutional neural networks perform well due to their encoder-decoder structures with skip connections, they struggle to capture long-range dependencies. Transformer-based variants address global context but often require heavy computation and large training datasets. This paper proposes GCA-ResUNet, an efficient segmentation network that integrates Grouped Coordinate Attention (GCA) into ResNet-50 residual blocks. GCA uses grouped coordinate modeling to jointly encode global dependencies across channels and spatial locations, strengthening feature representation and boundary delineation while adding minimal parameter and FLOP overhead compared with self-attention. On the Synapse dataset, GCA-ResUNet achieves a Dice score of 86.11%, and on the ACDC dataset, it reaches 92.64%, surpassing several state-of-the-art baselines while maintaining fast inference and favorable computational efficiency. These results indicate that GCA offers a practical way to enhance convolutional architectures with global modeling capability, enabling high-accuracy and resource-efficient medical image segmentation.

</details>


### [278] [FAPE-IR: Frequency-Aware Planning and Execution Framework for All-in-One Image Restoration](https://arxiv.org/abs/2511.14099)
*Jingren Liu,Shuning Xu,Qirui Yang,Yun Wang,Xiangyu Chen,Zhong Ji*

Main category: cs.CV

TL;DR: 提出FAPE - IR框架用于图像恢复，结合语义规划与基于频率的恢复，实验显示其在多任务上达SOTA且有强零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有全合一图像恢复方法依赖特定任务设计或潜在路由策略，难以适应现实复杂降解场景。

Method: 提出FAPE - IR框架，用冻结MLLM作规划器生成频率感知恢复计划，引导LoRA - MoE模块，结合对抗训练和频率正则化损失。

Result: FAPE - IR在七个恢复任务上达到了当前最优性能，在混合降解下有强零样本泛化能力。

Conclusion: FAPE - IR为全合一图像恢复提供了统一且可解释的解决方案。

Abstract: All-in-One Image Restoration (AIO-IR) aims to develop a unified model that can handle multiple degradations under complex conditions. However, existing methods often rely on task-specific designs or latent routing strategies, making it hard to adapt to real-world scenarios with various degradations. We propose FAPE-IR, a Frequency-Aware Planning and Execution framework for image restoration. It uses a frozen Multimodal Large Language Model (MLLM) as a planner to analyze degraded images and generate concise, frequency-aware restoration plans. These plans guide a LoRA-based Mixture-of-Experts (LoRA-MoE) module within a diffusion-based executor, which dynamically selects high- or low-frequency experts, complemented by frequency features of the input image. To further improve restoration quality and reduce artifacts, we introduce adversarial training and a frequency regularization loss. By coupling semantic planning with frequency-based restoration, FAPE-IR offers a unified and interpretable solution for all-in-one image restoration. Extensive experiments show that FAPE-IR achieves state-of-the-art performance across seven restoration tasks and exhibits strong zero-shot generalization under mixed degradations.

</details>


### [279] [CascadedViT: Cascaded Chunk-FeedForward and Cascaded Group Attention Vision Transformer](https://arxiv.org/abs/2511.14111)
*Srivathsan Sivakumar,Faisal Z. Qureshi*

Main category: cs.CV

TL;DR: 提出轻量高效的Cascaded - ViT架构，实验显示其在减少计算量和能耗同时保证精度，在Accuracy - Per - FLOP指标上高效。


<details>
  <summary>Details</summary>
Motivation: Vision Transformers计算、内存和能耗需求高，难以在资源受限平台部署。

Method: 提出Cascaded - ViT架构，采用Cascaded - Chunk Feed Forward Network拆分输入特征，提高参数和FLOP效率。

Result: CViT - XL在ImageNet - 1K上Top - 1准确率75.5%，相比EfficientViT - M5减少FLOPs 15%、能耗3.3%；CViT家族能耗最低；CViT模型在Accuracy - Per - FLOP指标上排名靠前，CViT - L比EfficientViT - M2准确率高2.2%且APF相当。

Conclusion: Cascaded - ViT适合部署在电池受限设备，计算效率高。

Abstract: Vision Transformers (ViTs) have demonstrated remarkable performance across a range of computer vision tasks; however, their high computational, memory, and energy demands hinder deployment on resource-constrained platforms. In this paper, we propose \emph{Cascaded-ViT (CViT)}, a lightweight and compute-efficient vision transformer architecture featuring a novel feedforward network design called \emph{Cascaded-Chunk Feed Forward Network (CCFFN)}. By splitting input features, CCFFN improves parameter and FLOP efficiency without sacrificing accuracy. Experiments on ImageNet-1K show that our \emph{CViT-XL} model achieves 75.5\% Top-1 accuracy while reducing FLOPs by 15\% and energy consumption by 3.3\% compared to EfficientViT-M5. Across various model sizes, the CViT family consistently exhibits the lowest energy consumption, making it suitable for deployment on battery-constrained devices such as mobile phones and drones. Furthermore, when evaluated using a new metric called \emph{Accuracy-Per-FLOP (APF)}, which quantifies compute efficiency relative to accuracy, CViT models consistently achieve top-ranking efficiency. Particularly, CViT-L is 2.2\% more accurate than EfficientViT-M2 while having comparable APF scores.

</details>


### [280] [Uni-Hema: Unified Model for Digital Hematopathology](https://arxiv.org/abs/2511.13889)
*Abdul Rehman,Iqra Rasool,Ayesha Imran,Mohsen Ali,Waqas Sultani*

Main category: cs.CV

TL;DR: 提出用于数字血液病理学的多任务统一模型Uni - Hema，利用多数据集，实验表明性能好，为多任务多模态数字血液病理学建立新标准。


<details>
  <summary>Details</summary>
Motivation: 现有数字血液病理学模型不能提供跨复杂情况的统一、多任务、多模态推理，需新模型解决。

Method: 提出Uni - Hema模型，利用46个公开数据集，基于Hema - Former多模态模块完成不同粒度的不同任务。

Result: Uni - Hema在不同血液学任务上性能与单任务单数据集模型相当或更优，能在单细胞层面提供可解释且与形态相关的见解。

Conclusion: Uni - Hema为多任务和多模态数字血液病理学建立了新标准，代码将公开。

Abstract: Digital hematopathology requires cell-level analysis across diverse disease categories, including malignant disorders (e.g., leukemia), infectious conditions (e.g., malaria), and non-malignant red blood cell disorders (e.g., sickle cell disease). Whether single-task, vision-language, WSI-optimized, or single-cell hematology models, these approaches share a key limitation, they cannot provide unified, multi-task, multi-modal reasoning across the complexities of digital hematopathology. To overcome these limitations, we propose Uni-Hema, a multi-task, unified model for digital hematopathology integrating detection, classification, segmentation, morphology prediction, and reasoning across multiple diseases. Uni-Hema leverages 46 publicly available datasets, encompassing over 700K images and 21K question-answer pairs, and is built upon Hema-Former, a multimodal module that bridges visual and textual representations at the hierarchy level for the different tasks (detection, classification, segmentation, morphology, mask language modeling and visual question answer) at different granularity. Extensive experiments demonstrate that Uni-Hema achieves comparable or superior performance to train on a single-task and single dataset models, across diverse hematological tasks, while providing interpretable, morphologically relevant insights at the single-cell level. Our framework establishes a new standard for multi-task and multi-modal digital hematopathology. The code will be made publicly available.

</details>


### [281] [Multi-view Phase-aware Pedestrian-Vehicle Incident Reasoning Framework with Vision-Language Models](https://arxiv.org/abs/2511.14120)
*Hao Zhen,Yunxiang Yang,Jidong J. Yang*

Main category: cs.CV

TL;DR: 提出MP - PVIR框架处理多视角视频流生成诊断报告，经评估可将视频数据转化为有效见解，推动车路协同系统交通安全分析。


<details>
  <summary>Details</summary>
Motivation: 现有视频系统难以洞察行人行为认知阶段事故发展过程，现有视觉语言模型处理视频缺乏时间结构和多视角整合。

Method: 引入MP - PVIR框架，包含事件触发多视角视频采集、行人行为阶段分割、特定阶段多视角推理、分层合成和诊断推理四个阶段，采用TG - VLM和PhaVR - VLM两个视觉语言模型，用大语言模型生成报告。

Result: TG - VLM行为阶段分割mIoU为0.4881，PhaVR - VLM字幕得分33.063，问答准确率达64.70%。

Conclusion: MP - PVIR能将多视角视频数据转化为可操作见解，推进车路协同系统的人工智能交通安全分析。

Abstract: Pedestrian-vehicle incidents remain a critical urban safety challenge, with pedestrians accounting for over 20% of global traffic fatalities. Although existing video-based systems can detect when incidents occur, they provide little insight into how these events unfold across the distinct cognitive phases of pedestrian behavior. Recent vision-language models (VLMs) have shown strong potential for video understanding, but they remain limited in that they typically process videos in isolation, without explicit temporal structuring or multi-view integration. This paper introduces Multi-view Phase-aware Pedestrian-Vehicle Incident Reasoning (MP-PVIR), a unified framework that systematically processes multi-view video streams into structured diagnostic reports through four stages: (1) event-triggered multi-view video acquisition, (2) pedestrian behavior phase segmentation, (3) phase-specific multi-view reasoning, and (4) hierarchical synthesis and diagnostic reasoning. The framework operationalizes behavioral theory by automatically segmenting incidents into cognitive phases, performing synchronized multi-view analysis within each phase, and synthesizing results into causal chains with targeted prevention strategies. Particularly, two specialized VLMs underpin the MP-PVIR pipeline: TG-VLM for behavioral phase segmentation (mIoU = 0.4881) and PhaVR-VLM for phase-aware multi-view analysis, achieving a captioning score of 33.063 and up to 64.70% accuracy on question answering. Finally, a designated large language model is used to generate comprehensive reports detailing scene understanding, behavior interpretation, causal reasoning, and prevention recommendations. Evaluation on the Woven Traffic Safety dataset shows that MP-PVIR effectively translates multi-view video data into actionable insights, advancing AI-driven traffic safety analytics for vehicle-infrastructure cooperative systems.

</details>


### [282] [SMART: Shot-Aware Multimodal Video Moment Retrieval with Audio-Enhanced MLLM](https://arxiv.org/abs/2511.14143)
*An Yu,Weiheng Lu,Jian Li,Zhenfei Zhang,Yunhang Shen,Felix X. -F. Ye,Ming-Ching Chang*

Main category: cs.CV

TL;DR: 提出SMART框架用于视频时刻检索，结合音频线索和镜头级时间结构，在两个数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视频时刻检索方法多依赖粗略时间理解和单一视觉模态，在复杂视频上性能受限。

Method: 引入基于MLLM的SMART框架，结合音频和视觉特征丰富多模态表示，应用镜头感知的令牌压缩，优化提示设计。

Result: 在Charades - STA和QVHighlights上显著优于现有方法，如在Charades - STA上R1@0.5提升1.61%，R1@0.7提升2.59%。

Conclusion: SMART框架能有效提升视频时刻检索性能。

Abstract: Video Moment Retrieval is a task in video understanding that aims to localize a specific temporal segment in an untrimmed video based on a natural language query. Despite recent progress in moment retrieval from videos using both traditional techniques and Multimodal Large Language Models (MLLM), most existing methods still rely on coarse temporal understanding and a single visual modality, limiting performance on complex videos. To address this, we introduce \textit{S}hot-aware \textit{M}ultimodal \textit{A}udio-enhanced \textit{R}etrieval of \textit{T}emporal \textit{S}egments (SMART), an MLLM-based framework that integrates audio cues and leverages shot-level temporal structure. SMART enriches multimodal representations by combining audio and visual features while applying \textbf{Shot-aware Token Compression}, which selectively retains high-information tokens within each shot to reduce redundancy and preserve fine-grained temporal details. We also refine prompt design to better utilize audio-visual cues. Evaluations on Charades-STA and QVHighlights show that SMART achieves significant improvements over state-of-the-art methods, including a 1.61\% increase in R1@0.5 and 2.59\% gain in R1@0.7 on Charades-STA.

</details>


### [283] [AdaTok: Adaptive Token Compression with Object-Aware Representations for Efficient Multimodal LLMs](https://arxiv.org/abs/2511.14169)
*Xinliang Zhang,Lei Zhu,Hangzhou He,Shuang Zeng,Ourui Fu,Jiakui Hu,Zhengjian Yao,Yanye Lu*

Main category: cs.CV

TL;DR: 提出对象级令牌合并策略进行自适应令牌压缩，减少计算和内存负担，实验显示该方法平衡压缩率和性能表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型（MLLMs）的补丁级标记化导致计算和内存负担大，且与人类视觉认知系统不匹配，引发幻觉和计算冗余。

Method: 提出对象级令牌合并策略进行自适应令牌压缩，使其与人类视觉系统一致。

Result: 在多个综合基准测试中，该方法平均仅使用10%的令牌，却能达到原始模型近96%的性能，与相关工作对比展现出优越性。

Conclusion: 所提方法在平衡压缩率和性能方面表现优越，代码将公开。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated substantial value in unified text-image understanding and reasoning, primarily by converting images into sequences of patch-level tokens that align with their architectural paradigm. However, patch-level tokenization leads to a quadratic growth in image tokens, burdening MLLMs' understanding and reasoning with enormous computation and memory. Additionally, the traditional patch-wise scanning tokenization workflow misaligns with the human vision cognition system, further leading to hallucination and computational redundancy. To address this issue, we propose an object-level token merging strategy for Adaptive Token compression, revealing the consistency with human vision system. The experiments are conducted on multiple comprehensive benchmarks, which show that our approach averagely, utilizes only 10% tokens while achieving almost 96% of the vanilla model's performance. More extensive experimental results in comparison with relevant works demonstrate the superiority of our method in balancing compression ratio and performance. Our code will be available.

</details>


### [284] [Single Tensor Cell Segmentation using Scalar Field Representations](https://arxiv.org/abs/2511.13947)
*Kevin I. Ruiz Vargas,Gabriel G. Galdino,Tsang Ing Ren,Alexandre L. Cunha*

Main category: cs.CV

TL;DR: 本文从标量场角度研究细胞图像分割，用训练网络参数化的标量场和分水岭方法实现分割，在公共数据集上取得有竞争力结果。


<details>
  <summary>Details</summary>
Motivation: 学习图像域上的连续标量场，使其分割能为图像中的细胞生成鲁棒实例。

Method: 使用训练网络参数化标量场，通过分水岭方法实现分割，实验中的场是泊松偏微分方程和模拟热方程稳态解的扩散的解，通过最小化场残差获得解。

Result: 在公共数据集上取得有竞争力的结果。

Conclusion: 提出的新颖、简单且具有几何洞察力的方法能实现出色的细胞分割效果。

Abstract: We investigate image segmentation of cells under the lens of scalar fields. Our goal is to learn a continuous scalar field on image domains such that its segmentation produces robust instances for cells present in images. This field is a function parameterized by the trained network, and its segmentation is realized by the watershed method. The fields we experiment with are solutions to the Poisson partial differential equation and a diffusion mimicking the steady-state solution of the heat equation. These solutions are obtained by minimizing just the field residuals, no regularization is needed, providing a robust regression capable of diminishing the adverse impacts of outliers in the training data and allowing for sharp cell boundaries. A single tensor is all that is needed to train a \unet\ thus simplifying implementation, lowering training and inference times, hence reducing energy consumption, and requiring a small memory footprint, all attractive features in edge computing. We present competitive results on public datasets from the literature and show that our novel, simple yet geometrically insightful approach can achieve excellent cell segmentation results.

</details>


### [285] [EchoAgent: Guideline-Centric Reasoning Agent for Echocardiography Measurement and Interpretation](https://arxiv.org/abs/2511.13948)
*Matin Daghyani,Lyuyang Wang,Nima Hashemi,Bassant Medhat,Baraa Abdelsamad,Eros Rojas Velez,XiaoXiao Li,Michael Y. C. Tsang,Christina Luong,Teresa S. M. Tsang,Purang Abolmaesumi*

Main category: cs.CV

TL;DR: 提出EchoAgent框架用于超声心动图分析，通过LLM控制专业视觉工具，实现时空视频分析，取得准确、可解释结果，为心脏超声AI指明新方向。


<details>
  <summary>Details</summary>
Motivation: 现有心脏超声深度学习模型不支持视频级推理和基于指南的测量分析，需要结构化、可解释的自动化框架。

Method: EchoAgent在大语言模型控制下编排专业视觉工具，进行时间定位、空间测量和临床解释，有测量可行性预测模型，还策划了评估基准。

Result: EchoAgent在时空视频分析中取得准确、可解释结果，输出基于视觉证据和临床指南，支持透明性和可追溯性。

Conclusion: 证明了通过特定任务工具和全视频级自动化，在超声心动图视频分析中进行与指南一致的推理是可行的，为心脏超声可信AI设定新方向。

Abstract: Purpose: Echocardiographic interpretation requires video-level reasoning and guideline-based measurement analysis, which current deep learning models for cardiac ultrasound do not support. We present EchoAgent, a framework that enables structured, interpretable automation for this domain. Methods: EchoAgent orchestrates specialized vision tools under Large Language Model (LLM) control to perform temporal localization, spatial measurement, and clinical interpretation. A key contribution is a measurement-feasibility prediction model that determines whether anatomical structures are reliably measurable in each frame, enabling autonomous tool selection. We curated a benchmark of diverse, clinically validated video-query pairs for evaluation. Results: EchoAgent achieves accurate, interpretable results despite added complexity of spatiotemporal video analysis. Outputs are grounded in visual evidence and clinical guidelines, supporting transparency and traceability. Conclusion: This work demonstrates the feasibility of agentic, guideline-aligned reasoning for echocardiographic video analysis, enabled by task-specific tools and full video-level automation. EchoAgent sets a new direction for trustworthy AI in cardiac ultrasound.

</details>


### [286] [Few-Shot Precise Event Spotting via Unified Multi-Entity Graph and Distillation](https://arxiv.org/abs/2511.14186)
*Zhaoyu Liu,Kan Jiang,Murong Ma,Zhe Hou,Yun Lin,Jin Song Dong*

Main category: cs.CV

TL;DR: 提出UMEG - Net用于少样本精确事件定位（PES），结合人体骨骼和物体关键点，采用多模态蒸馏，在少样本设置下表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有PES方法依赖大量标注数据，少样本条件下表现不佳，且获取大量标注数据困难。

Method: 提出UMEG - Net，将人体骨骼和特定运动物体关键点整合到统一图中，有基于GCN和多尺度时间偏移的时空提取模块，采用多模态蒸馏。

Result: 在少样本设置下表现稳健，显著优于基线模型。

Conclusion: UMEG - Net为少样本PES提供了可扩展且有效的解决方案。

Abstract: Precise event spotting (PES) aims to recognize fine-grained events at exact moments and has become a key component of sports analytics. This task is particularly challenging due to rapid succession, motion blur, and subtle visual differences. Consequently, most existing methods rely on domain-specific, end-to-end training with large labeled datasets and often struggle in few-shot conditions due to their dependence on pixel- or pose-based inputs alone. However, obtaining large labeled datasets is practically hard. We propose a Unified Multi-Entity Graph Network (UMEG-Net) for few-shot PES. UMEG-Net integrates human skeletons and sport-specific object keypoints into a unified graph and features an efficient spatio-temporal extraction module based on advanced GCN and multi-scale temporal shift. To further enhance performance, we employ multimodal distillation to transfer knowledge from keypoint-based graphs to visual representations. Our approach achieves robust performance with limited labeled data and significantly outperforms baseline models in few-shot settings, providing a scalable and effective solution for few-shot PES. Code is publicly available at https://github.com/LZYAndy/UMEG-Net.

</details>


### [287] [Multi-Scale Correlation-Aware Transformer for Maritime Vessel Re-Identification](https://arxiv.org/abs/2511.14203)
*Yunhe Liu*

Main category: cs.CV

TL;DR: 提出多尺度相关感知Transformer网络MCFormer解决船舶重识别问题，实验显示其达到了最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有船舶重识别方法多改编自行人算法，难以处理船舶图像独特问题，如同身份内变化大、局部缺失严重等。

Method: 提出MCFormer，包含全局相关模块GCM和局部相关模块LCM。GCM构建全局相似亲和矩阵建模全局相关性，LCM挖掘和对齐正样本局部特征提取局部相关性，最后整合多尺度的全局和局部特征。

Result: 在三个基准测试上，MCFormer达到了最先进性能。

Conclusion: MCFormer能有效抑制同身份内变化和局部缺失的离群样本的不利影响，提升船舶重识别性能。

Abstract: Maritime vessel re-identification (Re-ID) plays a crucial role in advancing maritime monitoring and intelligent situational awareness systems. However, some existing vessel Re-ID methods are directly adapted from pedestrian-focused algorithms, making them ill-suited for mitigating the unique problems present in vessel images, particularly the greater intra-identity variations and more severe missing of local parts, which lead to the emergence of outlier samples within the same identity. To address these challenges, we propose the Multi-scale Correlation-aware Transformer Network (MCFormer), which explicitly models multi-scale correlations across the entire input set to suppress the adverse effects of outlier samples with intra-identity variations or local missing, incorporating two novel modules, the Global Correlation Module (GCM), and the Local Correlation Module (LCM). Specifically, GCM constructs a global similarity affinity matrix across all input images to model global correlations through feature aggregation based on inter-image consistency, rather than solely learning features from individual images as in most existing approaches. Simultaneously, LCM mines and aligns local features of positive samples with contextual similarity to extract local correlations by maintaining a dynamic memory bank, effectively compensating for missing or occluded regions in individual images. To further enhance feature robustness, MCFormer integrates global and local features that have been respectively correlated across multiple scales, effectively capturing latent relationships among image features. Experiments on three benchmarks demonstrate that MCFormer achieves state-of-the-art performance.

</details>


### [288] [Orion: A Unified Visual Agent for Multimodal Perception, Advanced Visual Reasoning and Execution](https://arxiv.org/abs/2511.14210)
*N Dinesh Reddy,Sudeep Pillai*

Main category: cs.CV

TL;DR: 介绍视觉代理框架Orion，可处理多模态，结合工具调用实现复杂视觉工作流，有优异表现。


<details>
  <summary>Details</summary>
Motivation: 突破传统视觉语言模型仅输出描述性结果的局限，实现生产级视觉智能。

Method: 采用具有多工具调用能力的代理框架，编排多种计算机视觉工具。

Result: 在MMMU、MMBench等数据集上取得有竞争力的表现，实现从被动视觉理解到主动视觉智能的转变。

Conclusion: Orion结合神经感知与符号执行，实现自主视觉推理，扩展了整体视觉语言模型。

Abstract: We introduce Orion, a visual agent framework that can take in any modality and generate any modality. Using an agentic framework with multiple tool-calling capabilities, Orion is designed for visual AI tasks and achieves state-of-the-art results. Unlike traditional vision-language models that produce descriptive outputs, Orion orchestrates a suite of specialized computer vision tools, including object detection, keypoint localization, panoptic segmentation, Optical Character Recognition, and geometric analysis, to execute complex multi-step visual workflows. The system achieves competitive performance on MMMU, MMBench, DocVQA, and MMLongBench while extending monolithic vision-language models to production-grade visual intelligence. By combining neural perception with symbolic execution, Orion enables autonomous visual reasoning, marking a transition from passive visual understanding to active, tool-driven visual intelligence.

</details>


### [289] [GEN3D: Generating Domain-Free 3D Scenes from a Single Image](https://arxiv.org/abs/2511.14291)
*Yuxin Zhang,Ziyu Lu,Hongbo Duan,Keyu Fan,Pengting Luo,Peiyu Zhuang,Mengyu Yang,Houde Liu*

Main category: cs.CV

TL;DR: 提出Gen3d方法，可从单张图像生成高质量、大范围通用3D场景，实验证明其性能优越。


<details>
  <summary>Details</summary>
Motivation: 现有神经3D重建依赖密集多视图捕获限制应用，且3D场景生成对具身AI和世界模型很重要。

Method: 创建初始点云后维护并扩展世界模型，通过优化高斯散点表示来确定3D场景。

Result: 在不同数据集上的大量实验表明该方法有强泛化能力，在生成世界模型和合成高保真一致新视图方面表现优越。

Conclusion: Gen3d方法能有效从单张图像生成高质量3D场景。

Abstract: Despite recent advancements in neural 3D reconstruction, the dependence on dense multi-view captures restricts their broader applicability. Additionally, 3D scene generation is vital for advancing embodied AI and world models, which depend on diverse, high-quality scenes for learning and evaluation. In this work, we propose Gen3d, a novel method for generation of high-quality, wide-scope, and generic 3D scenes from a single image. After the initial point cloud is created by lifting the RGBD image, Gen3d maintains and expands its world model. The 3D scene is finalized through optimizing a Gaussian splatting representation. Extensive experiments on diverse datasets demonstrate the strong generalization capability and superior performance of our method in generating a world model and Synthesizing high-fidelity and consistent novel views.

</details>


### [290] [SAM-Fed: SAM-Guided Federated Semi-Supervised Learning for Medical Image Segmentation](https://arxiv.org/abs/2511.14302)
*Sahar Nasirihaghighi,Negin Ghamsarian,Yiping Li,Marcel Breeuwer,Raphael Sznitman,Klaus Schoeffmann*

Main category: cs.CV

TL;DR: 医学图像分割受数据隐私和标注成本限制，提出SAM - Fed框架，实验显示其优于现有FSSL方法。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割中标记数据有限，现有FSSL方法存在伪标签可靠性及客户端设备资源受限问题。

Method: 提出SAM - Fed框架，结合双重知识蒸馏与自适应一致性机制来细化像素级监督。

Result: 在皮肤病变和息肉分割的同构和异构设置实验中，SAM - Fed始终优于现有FSSL方法。

Conclusion: SAM - Fed框架在医学图像分割的FSSL中有良好表现，可解决现有方法的问题。

Abstract: Medical image segmentation is clinically important, yet data privacy and the cost of expert annotation limit the availability of labeled data. Federated semi-supervised learning (FSSL) offers a solution but faces two challenges: pseudo-label reliability depends on the strength of local models, and client devices often require compact or heterogeneous architectures due to limited computational resources. These constraints reduce the quality and stability of pseudo-labels, while large models, though more accurate, cannot be trained or used for routine inference on client devices. We propose SAM-Fed, a federated semi-supervised framework that leverages a high-capacity segmentation foundation model to guide lightweight clients during training. SAM-Fed combines dual knowledge distillation with an adaptive agreement mechanism to refine pixel-level supervision. Experiments on skin lesion and polyp segmentation across homogeneous and heterogeneous settings show that SAM-Fed consistently outperforms state-of-the-art FSSL methods.

</details>


### [291] [LSP-YOLO: A Lightweight Single-Stage Network for Sitting Posture Recognition on Embedded Devices](https://arxiv.org/abs/2511.14322)
*Nanjun Li,Ziyue Hao,Quanqiang Wang,Xuanyin Wang*

Main category: cs.CV

TL;DR: 提出轻量级单阶段坐姿识别网络LSP - YOLO，设计Light - C3k2模块，构建数据集训练测试，模型小、精度高、速度快，适用于多场景。


<details>
  <summary>Details</summary>
Motivation: 现有坐姿识别方法存在侵入性高、计算量大、嵌入式边缘设备实时性差的问题，需要改进。

Method: 受YOLOv11 - Pose启发提出LSP - YOLO，设计Light - C3k2模块，在识别头用逐点卷积映射关键点到姿势类别并采用中间监督，构建含5000张图像的数据集。

Result: 最小模型LSP - YOLO - n在PC上准确率94.2%、帧率251Fps，模型大小1.9MB，在SV830C + GC030A平台实现实时高精度推理。

Conclusion: 该方法高效、轻量、可部署，适用于智能教室、康复和人机交互等应用。

Abstract: With the rise in sedentary behavior, health problems caused by poor sitting posture have drawn increasing attention. Most existing methods, whether using invasive sensors or computer vision, rely on two-stage pipelines, which result in high intrusiveness, intensive computation, and poor real-time performance on embedded edge devices. Inspired by YOLOv11-Pose, a lightweight single-stage network for sitting posture recognition on embedded edge devices termed LSP-YOLO was proposed. By integrating partial convolution(PConv) and Similarity-Aware Activation Module(SimAM), a lightweight module, Light-C3k2, was designed to reduce computational cost while maintaining feature extraction capability. In the recognition head, keypoints were directly mapped to posture classes through pointwise convolution, and intermediate supervision was employed to enable efficient fusion of pose estimation and classification. Furthermore, a dataset containing 5,000 images across six posture categories was constructed for model training and testing. The smallest trained model, LSP-YOLO-n, achieved 94.2% accuracy and 251 Fps on personal computer(PC) with a model size of only 1.9 MB. Meanwhile, real-time and high-accuracy inference under constrained computational resources was demonstrated on the SV830C + GC030A platform. The proposed approach is characterized by high efficiency, lightweight design and deployability, making it suitable for smart classrooms, rehabilitation, and human-computer interaction applications.

</details>


### [292] [Clinically-Validated Innovative Mobile Application for Assessing Blinking and Eyelid Movements](https://arxiv.org/abs/2511.14361)
*Gustavo Adolpho Bonesso,Carlos Marcelo Gurjão de Godoy,Tammy Hentona Osaki,Midori Hentona Osaki,Bárbara Moreira Ribeiro Trindade dos Santos,Regina Célia Coelho*

Main category: cs.CV

TL;DR: 研究验证了Bapp应用程序监测眼睑运动的可靠性，它能作为传统方法的替代。


<details>
  <summary>Details</summary>
Motivation: 现有评估眼睑运动的工具存在复杂性、成本高和临床适用性有限等问题，需要开发新工具。

Method: 使用Flutter框架开发Bapp应用并集成Google ML Kit，用45个真实患者视频，由眼科专家手动标注作为真实值，用标准指标评估。

Result: Bapp的精确率达98.4%，召回率96.9%，整体准确率98.3%。

Conclusion: Bapp是可靠、便携、可及且客观的工具，可替代传统手动眨眼计数，支持临床眼部健康监测和术后评估。

Abstract: Blinking is a vital physiological process that protects and maintains the health of the ocular surface. Objective assessment of eyelid movements remains challenging due to the complexity, cost, and limited clinical applicability of existing tools. This study presents the clinical validation of Bapp (Blink Application), a mobile application developed using the Flutter framework and integrated with Google ML Kit for on-device, real-time analysis of eyelid movements. The validation occurred using 45 videos from real patients, whose blinks were manually annotated by ophthalmology specialists from the Paulista School of Medicine of the Federal University of Sao Paulo (EPM-UNIFESP) to serve as the ground truth. Bapp's performance was evaluated using standard metrics, including Precision, Recall, and F1-Score, with results demonstrating 98.4% precision, 96.9% recall, and an overall accuracy of 98.3%. These outcomes confirm the reliability of Bapp as a portable, accessible, and objective tool for monitoring both normal and abnormal eyelid movements. The application offers a promising alternative to traditional manual blink counting, supporting continuous ocular health monitoring and postoperative evaluation in clinical environments.

</details>


### [293] [Cheating Stereo Matching in Full-scale: Physical Adversarial Attack against Binocular Depth Estimation in Autonomous Driving](https://arxiv.org/abs/2511.14386)
*Kangqiao Zhao,Shuo Huai,Xurui Song,Jun Luo*

Main category: cs.CV

TL;DR: 提出针对自动驾驶立体匹配模型的首个纹理物理对抗攻击，可使立体模型产生错误深度信息。


<details>
  <summary>Details</summary>
Motivation: 现有攻击多针对单目感知，物理对抗样本对基于立体的双目深度估计有效性待探索。

Method: 采用带全局伪装纹理的3D物理对抗样本，提出3D立体匹配渲染模块，提出融合攻击优化物理对抗样本。

Result: 提出的物理对抗样本能成功使立体模型产生错误深度信息。

Conclusion: 提出的纹理物理对抗攻击有效，增强了隐身性和致命性。

Abstract: Though deep neural models adopted to realize the perception of autonomous driving have proven vulnerable to adversarial examples, known attacks often leverage 2D patches and target mostly monocular perception. Therefore, the effectiveness of Physical Adversarial Examples (PAEs) on stereo-based binocular depth estimation remains largely unexplored. To this end, we propose the first texture-enabled physical adversarial attack against stereo matching models in the context of autonomous driving. Our method employs a 3D PAE with global camouflage texture rather than a local 2D patch-based one, ensuring both visual consistency and attack effectiveness across different viewpoints of stereo cameras. To cope with the disparity effect of these cameras, we also propose a new 3D stereo matching rendering module that allows the PAE to be aligned with real-world positions and headings in binocular vision. We further propose a novel merging attack that seamlessly blends the target into the environment through fine-grained PAE optimization. It has significantly enhanced stealth and lethality upon existing hiding attacks that fail to get seamlessly merged into the background. Extensive evaluations show that our PAEs can successfully fool the stereo models into producing erroneous depth information.

</details>


### [294] [Enhancing Generalization of Depth Estimation Foundation Model via Weakly-Supervised Adaptation with Regularization](https://arxiv.org/abs/2511.14238)
*Yan Huang,Yongyi Su,Xin Lin,Le Zhang,Xun Xu*

Main category: cs.CV

TL;DR: 提出WeSTAR框架提升单目深度估计基础模型在未见和多样领域的鲁棒性，实验证明其能提升泛化能力并达SOTA。


<details>
  <summary>Details</summary>
Motivation: 在有下游任务数据的情况下，探索能否进一步提升单目深度估计基础模型的性能。

Method: 采用密集自训练目标作为结构自监督的主要来源；引入语义感知分层归一化；引入成对序数深度注释的弱监督；采用权重正则化损失确保训练稳定性。

Result: 在多种场景的现实和损坏的分布外数据集上的广泛实验表明，WeSTAR持续提升泛化能力，在多个基准测试中达到了当前最优性能。

Conclusion: WeSTAR框架能够有效提升单目深度估计基础模型在未见和多样领域的鲁棒性和泛化能力。

Abstract: The emergence of foundation models has substantially advanced zero-shot generalization in monocular depth estimation (MDE), as exemplified by the Depth Anything series. However, given access to some data from downstream tasks, a natural question arises: can the performance of these models be further improved? To this end, we propose WeSTAR, a parameter-efficient framework that performs Weakly supervised Self-Training Adaptation with Regularization, designed to enhance the robustness of MDE foundation models in unseen and diverse domains. We first adopt a dense self-training objective as the primary source of structural self-supervision. To further improve robustness, we introduce semantically-aware hierarchical normalization, which exploits instance-level segmentation maps to perform more stable and multi-scale structural normalization. Beyond dense supervision, we introduce a cost-efficient weak supervision in the form of pairwise ordinal depth annotations to further guide the adaptation process, which enforces informative ordinal constraints to mitigate local topological errors. Finally, a weight regularization loss is employed to anchor the LoRA updates, ensuring training stability and preserving the model's generalizable knowledge. Extensive experiments on both realistic and corrupted out-of-distribution datasets under diverse and challenging scenarios demonstrate that WeSTAR consistently improves generalization and achieves state-of-the-art performance across a wide range of benchmarks.

</details>


### [295] [NeuralSSD: A Neural Solver for Signed Distance Surface Reconstruction](https://arxiv.org/abs/2511.14283)
*Zi-Chen Xi,Jiahui Huang,Hao-Xiang Chen,Francis Williams,Qun-Ce Xu,Tai-Jiang Mu,Shi-Min Hu*

Main category: cs.CV

TL;DR: 提出NeuralSSD方法从点云数据重建3D隐式表面，在多数据集达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有隐式场参数化缺乏确保表面与输入数据紧密贴合的机制，需更好方法从点云重建高质量准确表面。

Method: 基于神经Galerkin方法，提出平衡点云信息可靠性的能量方程，引入学习三维信息的卷积网络。

Result: 在ShapeNet和Matterport等数据集上评估，在表面重建准确性和泛化性上达到SOTA。

Conclusion: NeuralSSD方法能实现高度准确和稳定的表面重建。

Abstract: We proposed a generalized method, NeuralSSD, for reconstructing a 3D implicit surface from the widely-available point cloud data. NeuralSSD is a solver-based on the neural Galerkin method, aimed at reconstructing higher-quality and accurate surfaces from input point clouds. Implicit method is preferred due to its ability to accurately represent shapes and its robustness in handling topological changes. However, existing parameterizations of implicit fields lack explicit mechanisms to ensure a tight fit between the surface and input data. To address this, we propose a novel energy equation that balances the reliability of point cloud information. Additionally, we introduce a new convolutional network that learns three-dimensional information to achieve superior optimization results. This approach ensures that the reconstructed surface closely adheres to the raw input points and infers valuable inductive biases from point clouds, resulting in a highly accurate and stable surface reconstruction. NeuralSSD is evaluated on a variety of challenging datasets, including the ShapeNet and Matterport datasets, and achieves state-of-the-art results in terms of both surface reconstruction accuracy and generalizability.

</details>


### [296] [Agentic Video Intelligence: A Flexible Framework for Advanced Video Exploration and Understanding](https://arxiv.org/abs/2511.14446)
*Hong Gao,Yiming Bao,Xuezhen Tu,Yutong Xu,Yue Jin,Yiyang Mu,Bin Zhong,Linan Yue,Min-Ling Zhang*

Main category: cs.CV

TL;DR: 提出无训练框架Agentic Video Intelligence (AVI) 用于视频理解，实验证明其有竞争力且可解释性强。


<details>
  <summary>Details</summary>
Motivation: 现有视频理解方法存在单遍处理、依赖昂贵模型或需大量训练的问题，需新方法解决。

Method: 提出AVI框架，包含三阶段推理过程、结构化视频知识库及工具、开源模型集成。

Result: 在LVBench等数据集实验中，AVI表现有竞争力，可解释性强。

Conclusion: AVI能有效解决现有视频理解方法的局限，是灵活且无需训练的框架。

Abstract: Video understanding requires not only visual recognition but also complex reasoning. While Vision-Language Models (VLMs) demonstrate impressive capabilities, they typically process videos largely in a single-pass manner with limited support for evidence revisit and iterative refinement. While recently emerging agent-based methods enable long-horizon reasoning, they either depend heavily on expensive proprietary models or require extensive agentic RL training. To overcome these limitations, we propose Agentic Video Intelligence (AVI), a flexible and training-free framework that can mirror human video comprehension through system-level design and optimization. AVI introduces three key innovations: (1) a human-inspired three-phase reasoning process (Retrieve-Perceive-Review) that ensures both sufficient global exploration and focused local analysis, (2) a structured video knowledge base organized through entity graphs, along with multi-granularity integrated tools, constituting the agent's interaction environment, and (3) an open-source model ensemble combining reasoning LLMs with lightweight base CV models and VLM, eliminating dependence on proprietary APIs or RL training. Experiments on LVBench, VideoMME-Long, LongVideoBench, and Charades-STA demonstrate that AVI achieves competitive performance while offering superior interpretability.

</details>


### [297] [O3SLM: Open Weight, Open Data, and Open Vocabulary Sketch-Language Model](https://arxiv.org/abs/2511.14368)
*Rishi Gupta,Mukilan Karuppasamy,Shyam Marjit,Aditay Tripathi,Anirban Chakraborty*

Main category: cs.CV

TL;DR: 现有大视觉语言模型理解抽象视觉输入能力有限，本文提出新数据集及模型O3SLM，评估显示其在基于草图任务中达SOTA。


<details>
  <summary>Details</summary>
Motivation: 大视觉语言模型理解抽象视觉输入（如手绘图）能力有限，缺乏联合建模草图、真实图像和自然语言指令的大规模数据集。

Method: 提出图像 - 草图 - 指令三元组的大规模数据集，用于预训练和指令调优；在该数据集上训练LVLM模型O3SLM。

Result: 在多个基于草图的任务上综合评估，结合三个现有草图数据集和新生成的SketchVCL数据集，O3SLM取得了SOTA性能。

Conclusion: O3SLM在草图理解和推理方面显著优于现有LVLM。

Abstract: While Large Vision Language Models (LVLMs) are increasingly deployed in real-world applications, their ability to interpret abstract visual inputs remains limited. Specifically, they struggle to comprehend hand-drawn sketches, a modality that offers an intuitive means of expressing concepts that are difficult to describe textually. We identify the primary bottleneck as the absence of a large-scale dataset that jointly models sketches, photorealistic images, and corresponding natural language instructions. To address this, we present two key contributions: (1) a new, large-scale dataset of image-sketch-instruction triplets designed to facilitate both pretraining and instruction tuning, and (2) O3SLM, an LVLM trained on this dataset. Comprehensive evaluations on multiple sketch-based tasks: (a) object localization, (b) counting, (c) image retrieval i.e., (SBIR and fine-grained SBIR), and (d) visual question answering (VQA); while incorporating the three existing sketch datasets, namely QuickDraw!, Sketchy, and Tu Berlin, along with our generated SketchVCL dataset, show that O3SLM achieves state-of-the-art performance, substantially outperforming existing LVLMs in sketch comprehension and reasoning.

</details>


### [298] [DeCo-VAE: Learning Compact Latents for Video Reconstruction via Decoupled Representation](https://arxiv.org/abs/2511.14530)
*Xiangchen Yin,Jiahui Yuan,Zhangchi Hu,Wenzhang Sun,Jie Chen,Xiaozhen Qiao,Hao Li,Xiaoyan Sun*

Main category: cs.CV

TL;DR: 提出DeCo - VAE实现紧凑潜在表示，实验表明其有优越视频重建性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频变分自编码器忽略帧内容相似性，存在冗余潜在建模问题，需实现紧凑潜在表示。

Method: 将视频内容分解为关键帧、运动和残差等不同组件，为每个组件设计专用编码器，采用共享3D解码器，使用解耦适应策略训练。

Result: 大量定量和定性实验表明DeCo - VAE取得了优越的视频重建性能。

Conclusion: DeCo - VAE能有效实现紧凑潜在表示，有良好视频重建效果。

Abstract: Existing video Variational Autoencoders (VAEs) generally overlook the similarity between frame contents, leading to redundant latent modeling. In this paper, we propose decoupled VAE (DeCo-VAE) to achieve compact latent representation. Instead of encoding RGB pixels directly, we decompose video content into distinct components via explicit decoupling: keyframe, motion and residual, and learn dedicated latent representation for each. To avoid cross-component interference, we design dedicated encoders for each decoupled component and adopt a shared 3D decoder to maintain spatiotemporal consistency during reconstruction. We further utilize a decoupled adaptation strategy that freezes partial encoders while training the others sequentially, ensuring stable training and accurate learning of both static and dynamic features. Extensive quantitative and qualitative experiments demonstrate that DeCo-VAE achieves superior video reconstruction performance.

</details>


### [299] [ForensicFlow: A Tri-Modal Adaptive Network for Robust Deepfake Detection](https://arxiv.org/abs/2511.14554)
*Mohammad Romani*

Main category: cs.CV

TL;DR: 本文提出ForensicFlow三模态法医框架用于视频深度伪造检测，在Celeb - DF (v2)上表现优于单流基线。


<details>
  <summary>Details</summary>
Motivation: 先进GAN和自动编码器生成的深度伪造威胁信息完整性和社会稳定，单流CNN无法捕捉多尺度伪造痕迹，鲁棒性和泛化性有限。

Method: 引入ForensicFlow框架，融合RGB、纹理和频率证据。RGB分支用ConvNeXt - tiny提取全局视觉不一致性；纹理分支用Swin Transformer - tiny检测细粒度混合伪影；频率分支用CNN + SE识别周期性频谱噪声。采用基于注意力的时间池化和自适应注意力融合。用Focal Loss在Celeb - DF (v2)上训练。

Result: ForensicFlow在Celeb - DF (v2)上实现AUC 0.9752、F1 - Score 0.9408和准确率0.9208，优于单流基线。消融实验验证分支协同作用，Grad - CAM确认法医焦点。

Conclusion: 这种综合特征融合对细微伪造有更强的抵御能力。

Abstract: Deepfakes generated by advanced GANs and autoencoders severely threaten information integrity and societal stability. Single-stream CNNs fail to capture multi-scale forgery artifacts across spatial, texture, and frequency domains, limiting robustness and generalization. We introduce the ForensicFlow, a tri-modal forensic framework that synergistically fuses RGB, texture, and frequency evidence for video Deepfake detection. The RGB branch (ConvNeXt-tiny) extracts global visual inconsistencies; the texture branch (Swin Transformer-tiny) detects fine-grained blending artifacts; the frequency branch (CNN + SE) identifies periodic spectral noise. Attention-based temporal pooling dynamically prioritizes high-evidence frames, while adaptive attention fusion balances branch contributions.Trained on Celeb-DF (v2) with Focal Loss, ForensicFlow achieves AUC 0.9752, F1-Score 0.9408, and accuracy 0.9208, outperforming single-stream baselines. Ablation validates branch synergy; Grad-CAM confirms forensic focus. This comprehensive feature fusion provides superior resilience against subtle forgeries.

</details>


### [300] [Deep Learning-Based Regional White Matter Hyperintensity Mapping as a Robust Biomarker for Alzheimer's Disease](https://arxiv.org/abs/2511.14588)
*Julia Machnio,Mads Nielsen,Mostafa Mehdipour Ghazi*

Main category: cs.CV

TL;DR: 提出深度学习框架进行WMH分割与定位，区域WMH量化有额外价值，结合局部病变指标和萎缩标记物可改善神经退行性疾病诊断。


<details>
  <summary>Details</summary>
Motivation: 现有自动WMH分割方法多只提供全局病变负荷，忽略其在不同白质区域的空间分布，需更好方法。

Method: 提出深度学习框架进行WMH分割与定位，在公共数据集和ADNI队列评估，量化解剖区域内WMH负荷并结合脑结构体积评估诊断价值。

Result: 预测病变负荷与参考WMH估计一致，区域WMH体积在疾病分类上优于全局病变负担，结合脑萎缩指标AUC达0.97，多个特定区域与诊断状态相关。

Conclusion: 区域WMH量化有额外价值，结合局部病变指标和萎缩标记物可增强神经退行性疾病的早期诊断和分层。

Abstract: White matter hyperintensities (WMH) are key imaging markers in cognitive aging, Alzheimer's disease (AD), and related dementias. Although automated methods for WMH segmentation have advanced, most provide only global lesion load and overlook their spatial distribution across distinct white matter regions. We propose a deep learning framework for robust WMH segmentation and localization, evaluated across public datasets and an independent Alzheimer's Disease Neuroimaging Initiative (ADNI) cohort. Our results show that the predicted lesion loads are in line with the reference WMH estimates, confirming the robustness to variations in lesion load, acquisition, and demographics. Beyond accurate segmentation, we quantify WMH load within anatomically defined regions and combine these measures with brain structure volumes to assess diagnostic value. Regional WMH volumes consistently outperform global lesion burden for disease classification, and integration with brain atrophy metrics further improves performance, reaching area under the curve (AUC) values up to 0.97. Several spatially distinct regions, particularly within anterior white matter tracts, are reproducibly associated with diagnostic status, indicating localized vulnerability in AD. These results highlight the added value of regional WMH quantification. Incorporating localized lesion metrics alongside atrophy markers may enhance early diagnosis and stratification in neurodegenerative disorders.

</details>


### [301] [CCSD: Cross-Modal Compositional Self-Distillation for Robust Brain Tumor Segmentation with Missing Modalities](https://arxiv.org/abs/2511.14599)
*Dongqing Xie,Yonghuang Wu,Zisheng Ai,Jun Min,Zhencun Jiang,Shaojin Geng,Lei Wang*

Main category: cs.CV

TL;DR: 提出CCSD框架处理多模态MRI脑肿瘤分割中模态缺失问题，实验表现优异。


<details>
  <summary>Details</summary>
Motivation: 现实临床中多模态MRI常缺失部分模态，影响深度学习分割模型性能和泛化性。

Method: 提出CCSD框架，采用共享 - 特定编解码器架构，包含分层模态自蒸馏机制和渐进模态组合蒸馏方法。

Result: 在公共脑肿瘤分割基准上，CCSD在各种模态缺失场景下达到了最先进的性能。

Conclusion: CCSD框架具有强大的泛化性和稳定性，能灵活处理任意输入模态组合。

Abstract: The accurate segmentation of brain tumors from multi-modal MRI is critical for clinical diagnosis and treatment planning. While integrating complementary information from various MRI sequences is a common practice, the frequent absence of one or more modalities in real-world clinical settings poses a significant challenge, severely compromising the performance and generalizability of deep learning-based segmentation models. To address this challenge, we propose a novel Cross-Modal Compositional Self-Distillation (CCSD) framework that can flexibly handle arbitrary combinations of input modalities. CCSD adopts a shared-specific encoder-decoder architecture and incorporates two self-distillation strategies: (i) a hierarchical modality self-distillation mechanism that transfers knowledge across modality hierarchies to reduce semantic discrepancies, and (ii) a progressive modality combination distillation approach that enhances robustness to missing modalities by simulating gradual modality dropout during training. Extensive experiments on public brain tumor segmentation benchmarks demonstrate that CCSD achieves state-of-the-art performance across various missing-modality scenarios, with strong generalization and stability.

</details>


### [302] [MRI Embeddings Complement Clinical Predictors for Cognitive Decline Modeling in Alzheimer's Disease Cohorts](https://arxiv.org/abs/2511.14601)
*Nathaniel Putera,Daniel Vilet Rodríguez,Noah Videcrantz,Julia Machnio,Mostafa Mehdipour Ghazi*

Main category: cs.CV

TL;DR: 本文评估表格和基于影像的表征对阿尔茨海默病认知衰退的预测作用，提出轨迹感知标记策略，训练3D ViT获取嵌入，结果显示不同模态各有优势，建议多模态融合建模。


<details>
  <summary>Details</summary>
Motivation: 准确建模阿尔茨海默病认知衰退对早期分层和个性化管理至关重要，表格预测器捕捉脑部细微变化能力有限，需评估不同表征的预测作用。

Method: 引入基于动态时间规整聚类的轨迹感知标记策略，在协调和增强的MRI数据上通过无监督重建训练3D ViT获取保留解剖结构的嵌入，用传统机器学习分类器和深度学习头评估预训练编码器嵌入，并与表格表征和卷积网络基线进行比较。

Result: 临床和体积特征预测轻度和重度进展的AUC约为0.70，ViT模型的MRI嵌入区分认知稳定个体的AUC为0.71，所有方法在异质性中等组表现不佳。

Conclusion: 临床特征在识别高风险极端情况方面表现出色，基于Transformer的MRI嵌入对稳定的细微标记更敏感，应采用多模态融合策略进行AD进展建模。

Abstract: Accurate modeling of cognitive decline in Alzheimer's disease is essential for early stratification and personalized management. While tabular predictors provide robust markers of global risk, their ability to capture subtle brain changes remains limited. In this study, we evaluate the predictive contributions of tabular and imaging-based representations, with a focus on transformer-derived Magnetic Resonance Imaging (MRI) embeddings. We introduce a trajectory-aware labeling strategy based on Dynamic Time Warping clustering to capture heterogeneous patterns of cognitive change, and train a 3D Vision Transformer (ViT) via unsupervised reconstruction on harmonized and augmented MRI data to obtain anatomy-preserving embeddings without progression labels. The pretrained encoder embeddings are subsequently assessed using both traditional machine learning classifiers and deep learning heads, and compared against tabular representations and convolutional network baselines. Results highlight complementary strengths across modalities. Clinical and volumetric features achieved the highest AUCs of around 0.70 for predicting mild and severe progression, underscoring their utility in capturing global decline trajectories. In contrast, MRI embeddings from the ViT model were most effective in distinguishing cognitively stable individuals with an AUC of 0.71. However, all approaches struggled in the heterogeneous moderate group. These findings indicate that clinical features excel in identifying high-risk extremes, whereas transformer-based MRI embeddings are more sensitive to subtle markers of stability, motivating multimodal fusion strategies for AD progression modeling.

</details>


### [303] [HyMAD: A Hybrid Multi-Activity Detection Approach for Border Surveillance and Monitoring](https://arxiv.org/abs/2511.14698)
*Sriram Srinivasan,Srinivasan Aruchamy,Siva Ram Krisha Vadali*

Main category: cs.CV

TL;DR: 本文针对地震信号监测边境时难以区分同时发生的活动的问题，提出HyMAD模型，在真实数据集上表现良好，为地震活动识别提供模块化框架。


<details>
  <summary>Details</summary>
Motivation: 地震传感用于边境监测有优势，但难以准确检测和区分同时发生的重叠活动，会降低监测系统可靠性。

Method: 提出基于时空特征融合的深度神经架构HyMAD，集成SincNet提取的频谱特征和RNN建模的时间依赖关系，采用自注意力层和跨模态融合模块。

Result: 在边境监测实地记录构建的数据集上评估，能泛化到涉及人类、动物和车辆的复杂同时活动场景。

Conclusion: 方法性能有竞争力，为基于地震的活动识别在现实安全应用中提供模块化框架。

Abstract: Seismic sensing has emerged as a promising solution for border surveillance and monitoring; the seismic sensors that are often buried underground are small and cannot be noticed easily, making them difficult for intruders to detect, avoid, or vandalize. This significantly enhances their effectiveness compared to highly visible cameras or fences. However, accurately detecting and distinguishing between overlapping activities that are happening simultaneously, such as human intrusions, animal movements, and vehicle rumbling, remains a major challenge due to the complex and noisy nature of seismic signals. Correctly identifying simultaneous activities is critical because failing to separate them can lead to misclassification, missed detections, and an incomplete understanding of the situation, thereby reducing the reliability of surveillance systems. To tackle this problem, we propose HyMAD (Hybrid Multi-Activity Detection), a deep neural architecture based on spatio-temporal feature fusion. The framework integrates spectral features extracted with SincNet and temporal dependencies modeled by a recurrent neural network (RNN). In addition, HyMAD employs self-attention layers to strengthen intra-modal representations and a cross-modal fusion module to achieve robust multi-label classification of seismic events. e evaluate our approach on a dataset constructed from real-world field recordings collected in the context of border surveillance and monitoring, demonstrating its ability to generalize to complex, simultaneous activity scenarios involving humans, animals, and vehicles. Our method achieves competitive performance and offers a modular framework for extending seismic-based activity recognition in real-world security applications.

</details>


### [304] [ARC Is a Vision Problem!](https://arxiv.org/abs/2511.14761)
*Keya Hu,Ali Cy,Linlu Qiu,Xiaoman Delores Ding,Runqian Wang,Yeyin Eva Zhu,Jacob Andreas,Kaiming He*

Main category: cs.CV

TL;DR: 本文将ARC问题从视觉范式处理，提出Vision ARC (VARC)框架，在ARC - 1基准上准确率达60.4%，表现优于现有从头训练方法，接近人类平均水平。


<details>
  <summary>Details</summary>
Motivation: 现有ARC研究多从语言角度，而ARC任务本质是视觉的，缺少从视觉中心视角的研究。

Method: 将ARC表述为图像到图像的翻译问题，用“画布”表示输入，应用标准视觉架构如ViT进行图像映射，从头在ARC数据上训练，通过测试时训练泛化到未见任务。

Result: VARC在ARC - 1基准上准确率达60.4%，显著优于其他从头训练的方法，结果与领先的大语言模型有竞争力。

Conclusion: 从视觉范式处理ARC问题的方法有效，能缩小与人类平均表现的差距。

Abstract: The Abstraction and Reasoning Corpus (ARC) is designed to promote research on abstract reasoning, a fundamental aspect of human intelligence. Common approaches to ARC treat it as a language-oriented problem, addressed by large language models (LLMs) or recurrent reasoning models. However, although the puzzle-like tasks in ARC are inherently visual, existing research has rarely approached the problem from a vision-centric perspective. In this work, we formulate ARC within a vision paradigm, framing it as an image-to-image translation problem. To incorporate visual priors, we represent the inputs on a "canvas" that can be processed like natural images. It is then natural for us to apply standard vision architectures, such as a vanilla Vision Transformer (ViT), to perform image-to-image mapping. Our model is trained from scratch solely on ARC data and generalizes to unseen tasks through test-time training. Our framework, termed Vision ARC (VARC), achieves 60.4% accuracy on the ARC-1 benchmark, substantially outperforming existing methods that are also trained from scratch. Our results are competitive with those of leading LLMs and close the gap to average human performance.

</details>


### [305] [Improving segmentation of retinal arteries and veins using cardiac signal in doppler holograms](https://arxiv.org/abs/2511.14654)
*Marius Dubosc,Yann Fischer,Zacharie Auray,Nicolas Boutry,Edwin Carlinet,Michael Atlan,Thierry Geraud*

Main category: cs.CV

TL;DR: 提出用标准分割架构对时间多普勒全息图进行动静脉分割的方法，利用时间分辨预处理挖掘深度学习潜力，探索视网膜血流动力学。


<details>
  <summary>Details</summary>
Motivation: 传统视网膜动静脉分割方法只关注空间信息，忽略全息数据的时间丰富性，需准确分割以定量评估视网膜血流动力学。

Method: 采用标准分割架构，结合专用脉搏分析管道提取的特征，让传统U - Net利用时间动态。

Result: 该方法性能与更复杂的基于注意力或迭代的模型相当。

Conclusion: 时间分辨预处理能挖掘深度学习在多普勒全息术中的潜力，为视网膜血流动力学定量探索开辟新视角。

Abstract: Doppler holography is an emerging retinal imaging technique that captures the dynamic behavior of blood flow with high temporal resolution, enabling quantitative assessment of retinal hemodynamics. This requires accurate segmentation of retinal arteries and veins, but traditional segmentation methods focus solely on spatial information and overlook the temporal richness of holographic data. In this work, we propose a simple yet effective approach for artery-vein segmentation in temporal Doppler holograms using standard segmentation architectures. By incorporating features derived from a dedicated pulse analysis pipeline, our method allows conventional U-Nets to exploit temporal dynamics and achieve performance comparable to more complex attention- or iteration-based models. These findings demonstrate that time-resolved preprocessing can unlock the full potential of deep learning for Doppler holography, opening new perspectives for quantitative exploration of retinal hemodynamics. The dataset is publicly available at https://huggingface.co/datasets/DigitalHolography/

</details>


### [306] [Impact of Image Resolution on Age Estimation with DeepFace and InsightFace](https://arxiv.org/abs/2511.14689)
*Shiyar Jamo*

Main category: cs.CV

TL;DR: 研究评估图像分辨率对DeepFace和InsightFace年龄估计准确性的影响，发现224x224像素时性能最优，低分辨率和极高分辨率都会降低准确性，InsightFace速度更快。


<details>
  <summary>Details</summary>
Motivation: 自动年龄估计用于年龄验证时输入图像分辨率差异大，需评估图像分辨率对年龄估计准确性的影响。

Method: 从IMDB - Clean数据集选取1000张图像，处理成七种分辨率得到7000个测试样本，用MAE、SD和MedAE评估性能。

Result: 输入图像分辨率对两种框架年龄估计准确性有明显一致影响，224x224像素时性能最优，DeepFace的MAE为10.83岁，InsightFace为7.46岁，低分辨率MAE大幅增加，极高分辨率也会降低准确性，InsightFace在各分辨率下速度都更快。

Conclusion: 输入图像分辨率对DeepFace和InsightFace年龄估计准确性有明确一致影响，两种框架在224x224像素时达最优性能。

Abstract: Automatic age estimation is widely used for age verification, where input images often vary considerably in resolution. This study evaluates the effect of image resolution on age estimation accuracy using DeepFace and InsightFace. A total of 1000 images from the IMDB-Clean dataset were processed in seven resolutions, resulting in 7000 test samples. Performance was evaluated using Mean Absolute Error (MAE), Standard Deviation (SD), and Median Absolute Error (MedAE). Based on this study, we conclude that input image resolution has a clear and consistent impact on the accuracy of age estimation in both DeepFace and InsightFace. Both frameworks achieve optimal performance at 224x224 pixels, with an MAE of 10.83 years (DeepFace) and 7.46 years (InsightFace). At low resolutions, MAE increases substantially, while very high resolutions also degrade accuracy. InsightFace is consistently faster than DeepFace across all resolutions.

</details>


### [307] [Seeing Beyond the Image: ECG and Anatomical Knowledge-Guided Myocardial Scar Segmentation from Late Gadolinium-Enhanced Images](https://arxiv.org/abs/2511.14702)
*Farheen Ramzan,Yusuf Kiberu,Nikesh Jathanna,Meryem Jabrane,Vicente Grau,Shahnaz Jamil-Copley,Richard H. Clayton,Chen,Chen*

Main category: cs.CV

TL;DR: 提出融合ECG电生理信息与解剖先验的多模态框架用于心肌瘢痕分割，引入TAFF机制，在临床数据集上效果优于仅用图像的基线方法。


<details>
  <summary>Details</summary>
Motivation: 准确分割心肌瘢痕对评估组织活力至关重要，但因对比度和成像伪影而具有挑战性，ECG信号可提供补充生理信息。

Method: 提出多模态框架，将ECG电生理信息与AHA - 17图谱的解剖先验相结合；引入TAFF机制，根据采集时间差动态加权融合特征。

Result: 在临床数据集上，平均Dice分数从0.6149提升到0.8463，精度达0.9115，敏感度达0.9043。

Conclusion: 整合生理和解剖知识使模型能“超越图像”，为心肌瘢痕分割设定新方向。

Abstract: Accurate segmentation of myocardial scar from late gadolinium enhanced (LGE) cardiac MRI is essential for evaluating tissue viability, yet remains challenging due to variable contrast and imaging artifacts. Electrocardiogram (ECG) signals provide complementary physiological information, as conduction abnormalities can help localize or suggest scarred myocardial regions. In this work, we propose a novel multimodal framework that integrates ECG-derived electrophysiological information with anatomical priors from the AHA-17 atlas for physiologically consistent LGE-based scar segmentation. As ECGs and LGE-MRIs are not acquired simultaneously, we introduce a Temporal Aware Feature Fusion (TAFF) mechanism that dynamically weights and fuses features based on their acquisition time difference. Our method was evaluated on a clinical dataset and achieved substantial gains over the state-of-the-art image-only baseline (nnU-Net), increasing the average Dice score for scars from 0.6149 to 0.8463 and achieving high performance in both precision (0.9115) and sensitivity (0.9043). These results show that integrating physiological and anatomical knowledge allows the model to "see beyond the image", setting a new direction for robust and physiologically grounded cardiac scar segmentation.

</details>


### [308] [Zero-shot Synthetic Video Realism Enhancement via Structure-aware Denoising](https://arxiv.org/abs/2511.14719)
*Yifan Wang,Liya Ji,Zhanghan Ke,Harry Yang,Ser-Nam Lim,Qifeng Chen*

Main category: cs.CV

TL;DR: 提出一种增强合成视频真实感的零样本方法，利用扩散视频基础模型，结合结构感知信息，实验显示其在结构一致性和真实感上表现出色。


<details>
  <summary>Details</summary>
Motivation: 提升合成视频的真实感。

Method: 构建零样本框架，基于扩散视频基础模型，通过辅助模型估计合成视频的结构感知信息（如深度图、语义图和边缘图）来引导生成/去噪过程。

Result: 在实验中，该方法在与原始视频的结构一致性上优于现有基线，同时保持了最先进的真实感质量。

Conclusion: 所提出的方法是一种简单、通用且强大的增强合成视频真实感的方法。

Abstract: We propose an approach to enhancing synthetic video realism, which can re-render synthetic videos from a simulator in photorealistic fashion. Our realism enhancement approach is a zero-shot framework that focuses on preserving the multi-level structures from synthetic videos into the enhanced one in both spatial and temporal domains, built upon a diffusion video foundational model without further fine-tuning. Specifically, we incorporate an effective modification to have the generation/denoising process conditioned on estimated structure-aware information from the synthetic video, such as depth maps, semantic maps, and edge maps, by an auxiliary model, rather than extracting the information from a simulator. This guidance ensures that the enhanced videos are consistent with the original synthetic video at both the structural and semantic levels. Our approach is a simple yet general and powerful approach to enhancing synthetic video realism: we show that our approach outperforms existing baselines in structural consistency with the original video while maintaining state-of-the-art photorealism quality in our experiments.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [309] [Derivative of the truncated singular value and eigen decomposition](https://arxiv.org/abs/2511.14651)
*Jan Naumann*

Main category: math.NA

TL;DR: 本文围绕机器学习和计算物理领域自动微分技术所需的线性代数梯度计算，详细讨论截断奇异和特征值分解的导数。


<details>
  <summary>Details</summary>
Motivation: 机器学习和计算物理领域的应用依赖自动微分技术，需要稳定高效的线性代数梯度计算。

Method: 总结前人工作，详细阐述如何推导相关项。

Result: 全面详细地讨论了截断奇异和特征值分解的导数。

Conclusion: 解决了在缺乏完整分解信息情况下，正确用截断部分表示导数的问题。

Abstract: Recently developed applications in the field of machine learning and computational physics rely on automatic differentiation techniques, that require stable and efficient linear algebra gradient computations. This technical note provides a comprehensive and detailed discussion of the derivative of the truncated singular and eigenvalue decomposition. It summarizes previous work and builds on them with an extensive description of how to derive the relevant terms. A main focus is correctly expressing the derivative in terms of the truncated part, despite lacking knowledge of the full decomposition.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [310] [Automated proving in planar geometry based on the complex number identity method and elimination](https://arxiv.org/abs/2511.14728)
*Zoltán Kovács,Xicheng Peng*

Main category: cs.CG

TL;DR: 基于消元理想将复数恒等式证明方法改进为全自动程序，在多软件中展示结果并在GeoGebra中给出原型。


<details>
  <summary>Details</summary>
Motivation: 将复数恒等式证明方法改进为全自动程序。

Method: 通过处理声明式方程、清除分母、引入松弛变量来消除所有自由和关系点变量，从得到的理想中得出结论。

Result: 在Mathematica、Maple和Giac计算机代数系统新版本中展示结果，在GeoGebra实验版中给出自动程序原型。

Conclusion: 成功将复数恒等式证明方法改进为全自动程序并进行多方面应用展示。

Abstract: We improve the complex number identity proving method to a fully automated procedure, based on elimination ideals. By using declarative equations or rewriting each real-relational hypothesis $h_i$ to $h_i-r_i$, and the thesis $t$ to $t-r$, clearing the denominators and introducing an extra expression with a slack variable, we eliminate all free and relational point variables. From the obtained ideal $I$ in $\mathbb{Q}[r,r_1,r_2,\ldots]$ we can find a conclusive result. It plays an important role that if $r_1,r_2,\ldots$ are real, $r$ must also be real if there is a linear polynomial $p(r)\in I$, unless division by zero occurs when expressing $r$. Our results are presented in Mathematica, Maple and in a new version of the Giac computer algebra system. Finally, we present a prototype of the automated procedure in an experimental version of the dynamic geometry software GeoGebra.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [311] [The CHASM-SWPC Dataset for Coronal Hole Detection & Analysis](https://arxiv.org/abs/2511.14044)
*Cutter Beck,Evan Smith,Khagendra Katuwal,Rudra Kafle,Jacob Whitehill*

Main category: astro-ph.IM

TL;DR: 本文开发半自动化流程将SWPC地图数字化为二值分割掩码，构建CHASM - SWPC数据集，开发CHASM工具标注数据，训练CHRONNOS神经网络并比较性能，新训练模型表现更优。


<details>
  <summary>Details</summary>
Motivation: 开发高质量数据集以训练和测试自动日冕洞检测模型。

Method: 利用SWPC每日手绘地图，开发半自动化流程生成二值分割掩码，开发CHASM工具进行标注，训练CHRONNOS神经网络。

Result: 新训练的CHRONNOS神经网络在CHASM - SWPC - 1111测试集上准确率0.9805、TSS 0.6807、IoU 0.5668，高于原预训练模型。

Conclusion: 新开发的数据集和训练的模型在日冕洞检测上有更好的性能。

Abstract: Coronal holes (CHs) are low-activity, low-density solar coronal regions with open magnetic field lines (Cranmer 2009). In the extreme ultraviolet (EUV) spectrum, CHs appear as dark patches. Using daily hand-drawn maps from the Space Weather Prediction Center (SWPC), we developed a semi-automated pipeline to digitize the SWPC maps into binary segmentation masks. The resulting masks constitute the CHASM-SWPC dataset, a high-quality dataset to train and test automated CH detection models, which is released with this paper. We developed CHASM (Coronal Hole Annotation using Semi-automatic Methods), a software tool for semi-automatic annotation that enables users to rapidly and accurately annotate SWPC maps. The CHASM tool enabled us to annotate 1,111 CH masks, comprising the CHASM-SWPC-1111 dataset. We then trained multiple CHRONNOS (Coronal Hole RecOgnition Neural Network Over multi-Spectral-data) architecture (Jarolim et al. 2021) neural networks using the CHASM-SWPC dataset and compared their performance. Training the CHRONNOS neural network on these data achieved an accuracy of 0.9805, a True Skill Statistic (TSS) of 0.6807, and an intersection-over-union (IoU) of 0.5668, which is higher than the original pretrained CHRONNOS model Jarolim et al. (2021) achieved an accuracy of 0.9708, a TSS of 0.6749, and an IoU of 0.4805, when evaluated on the CHASM-SWPC-1111 test set.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [312] [Refine Thought: A Test-Time Inference Method for Embedding Model Reasoning](https://arxiv.org/abs/2511.13726)
*Guangzhi Wang,Kai Li,Yinghao Jiao,Zhi Liu*

Main category: cs.CL

TL;DR: 提出RT方法增强文本嵌入模型语义推理能力，多轮前向传播获最终语义表示，实验显示其在语义推理任务有显著提升，在通用语义理解任务表现稳定。


<details>
  <summary>Details</summary>
Motivation: 增强文本嵌入模型的语义推理能力。

Method: 通过运行文本嵌入模型的多次前向传播来获得最终语义表示。

Result: RT在BRIGHT和PJBenchmark1的语义推理任务上显著提升，在C - MTEB等通用语义理解任务上表现一致。

Conclusion: RT能进一步激活仅解码器文本嵌入模型预训练时学到的语义推理能力，是一种测试时推理方法，证明有效。

Abstract: We propose RT (Refine Thought), a method that can enhance the semantic rea-soning ability of text embedding models. The method obtains the final semanticrepresentation by running multiple forward passes of the text embedding model.Experiments show that RT achieves significant improvements on semantic reason-ing tasks in BRIGHT and the person job matching benchmark PJBenchmark1, while maintaining consistent performance on general-purpose semantic under-standing tasks such as C-MTEB. Our results indicate that RT is effective becauseit further activates the semantic reasoning ability learned during pretraining bydecoder-only text embedding models(e.g., Qwen3-Embedding-8B). RT canbe seen as a test-time inference method.

</details>


### [313] [Applying Relation Extraction and Graph Matching to Answering Multiple Choice Questions](https://arxiv.org/abs/2511.14144)
*Naoki Shimoda,Akihiro Yamamoto*

Main category: cs.CL

TL;DR: 结合基于Transformer的关系抽取与知识图谱匹配，用于回答选择题，保证过程可追溯，实验准确率约70%且受问题类别影响。


<details>
  <summary>Details</summary>
Motivation: 利用基于Transformer的关系抽取方法动态生成知识图谱的能力，解决回答选择题问题，同时考虑关系抽取可能生成错误信息的情况。

Method: 将句子用关系抽取方法转换为关系图，在封闭世界假设下与正确知识图谱验证其真实性。

Result: 方法能正确回答约70%的问题，同时提供过程可追溯性。

Conclusion: 方法有效，但问题类别对准确率有很大影响。

Abstract: In this research, we combine Transformer-based relation extraction with matching of knowledge graphs (KGs) and apply them to answering multiple-choice questions (MCQs) while maintaining the traceability of the output process. KGs are structured representations of factual knowledge consisting of entities and relations. Due to the high construction cost, they had been regarded as static databases with validated links. However, the recent development of Transformer-based relation extraction (RE) methods has enabled us to generate KGs dynamically by giving them natural language texts, and thereby opened the possibility for representing the meaning of the input sentences with the created KGs. Using this effect, we propose a method that answers MCQs in the "fill-in-the-blank" format, taking care of the point that RE methods generate KGs that represent false information if provided with factually incorrect texts. We measure the truthfulness of each question sentence by (i) converting the sentence into a relational graph using an RE method and (ii) verifying it against factually correct KGs under the closed-world assumption. The experimental results demonstrate that our method correctly answers up to around 70% of the questions, while providing traceability of the procedure. We also highlight that the question category has a vast influence on the accuracy.

</details>


### [314] [LiveRAG: A diverse Q&A dataset with varying difficulty level for RAG evaluation](https://arxiv.org/abs/2511.14531)
*David Carmel,Simone Filice,Guy Horowitz,Yoelle Maarek,Alex Shtoff,Oren Somekh,Ran Tavory*

Main category: cs.CL

TL;DR: 介绍公开数据集LiveRAG基准，用于系统评估基于RAG的问答系统，分析显示其有问题多样性等特点，有望推动研究。


<details>
  <summary>Details</summary>
Motivation: 随着RAG在生成式AI中愈发重要，有必要系统评估其有效性。

Method: 基于SIGIR'2025 LiveRAG挑战赛的基准，补充未公开信息，用项目反应理论模型得出问题难度和区分度分数。

Result: 分析表明基准具有问题多样性、难度范围广、能区分系统能力的特点。

Conclusion: LiveRAG基准有望帮助社区推进RAG研究、进行系统评估和开发更稳健的问答系统。

Abstract: With Retrieval Augmented Generation (RAG) becoming more and more prominent in generative AI solutions, there is an emerging need for systematically evaluating their effectiveness. We introduce the LiveRAG benchmark, a publicly available dataset of 895 synthetic questions and answers designed to support systematic evaluation of RAG-based Q&A systems. This synthetic benchmark is derived from the one used during the SIGIR'2025 LiveRAG Challenge, where competitors were evaluated under strict time constraints. It is augmented with information that was not made available to competitors during the Challenge, such as the ground-truth answers, together with their associated supporting claims which were used for evaluating competitors' answers. In addition, each question is associated with estimated difficulty and discriminability scores, derived from applying an Item Response Theory model to competitors' responses. Our analysis highlights the benchmark's questions diversity, the wide range of their difficulty levels, and their usefulness in differentiating between system capabilities. The LiveRAG benchmark will hopefully help the community advance RAG research, conduct systematic evaluation, and develop more robust Q&A systems.

</details>


### [315] [Signature vs. Substance: Evaluating the Balance of Adversarial Resistance and Linguistic Quality in Watermarking Large Language Models](https://arxiv.org/abs/2511.13722)
*William Guo,Adaku Uchendu,Ana Smith*

Main category: cs.CL

TL;DR: 为缓解大语言模型生成文本危害，研究者提出水印技术，但该技术影响文本质量且易受攻击，为推动采用，评估了几种水印技术的鲁棒性和保真性，结果显示这些技术语义保留但风格有偏差且易受攻击。


<details>
  <summary>Details</summary>
Motivation: 当前水印技术影响生成文本质量且易受攻击，导致大语言模型创建者不愿广泛采用，需评估技术以推动其应用。

Method: 通过比较释义和回译攻击评估几种水印技术对对抗攻击的鲁棒性，使用语言指标评估其保留未加水印文本质量和写作风格的能力。

Result: 这些水印技术能保留语义，但偏离未加水印文本的写作风格，且易受对抗攻击，尤其是回译攻击。

Conclusion: 现有水印技术存在一定局限性，在语义保留、风格保持和对抗攻击方面有待改进。

Abstract: To mitigate the potential harms of Large Language Models (LLMs)generated text, researchers have proposed watermarking, a process of embedding detectable signals within text. With watermarking, we can always accurately detect LLM-generated texts. However, recent findings suggest that these techniques often negatively affect the quality of the generated texts, and adversarial attacks can strip the watermarking signals, causing the texts to possibly evade detection. These findings have created resistance in the wide adoption of watermarking by LLM creators. Finally, to encourage adoption, we evaluate the robustness of several watermarking techniques to adversarial attacks by comparing paraphrasing and back translation (i.e., English $\to$ another language $\to$ English) attacks; and their ability to preserve quality and writing style of the unwatermarked texts by using linguistic metrics to capture quality and writing style of texts. Our results suggest that these watermarking techniques preserve semantics, deviate from the writing style of the unwatermarked texts, and are susceptible to adversarial attacks, especially for the back translation attack.

</details>


### [316] [Can QE-informed (Re)Translation lead to Error Correction?](https://arxiv.org/abs/2511.13884)
*Govardhan Padmanabhan*

Main category: cs.CL

TL;DR: 本文提出两种用于WMT 2025自动翻译质量评估系统任务3的方法，一种获胜方法从不同大语言模型生成的多个候选中选最高质量翻译，另一类似APE方法按QE说明替换错误子串，前者获子任务榜首。


<details>
  <summary>Details</summary>
Motivation: 联合训练QE系统和APE虽有性能提升，但APE会过度校正机器翻译输出导致性能下降，需新方法。

Method: 提出训练无关的QE-informed Retranslation方法并与同范式另一方法对比，一是从不同大语言模型候选中选最优翻译，二是让大语言模型按QE说明替换错误子串，使用条件启发式减少编辑次数。

Result: 两种方法Delta COMET分数分别为0.0201和 - 0.0108，第一种方法在子任务排行榜获胜。

Conclusion: 所提出的第一种方法在WMT 2025对应子任务中表现优异，可有效解决APE过度校正问题。

Abstract: The paper presents two approaches submitted to the WMT 2025 Automated Translation Quality Evaluation Systems Task 3 - Quality Estimation (QE)-informed Segment-level Error Correction. While jointly training QE systems with Automatic Post-Editing (APE) has shown improved performance for both tasks, APE systems are still known to overcorrect the output of Machine Translation (MT), leading to a degradation in performance. We investigate a simple training-free approach - QE-informed Retranslation, and compare it with another within the same training-free paradigm. Our winning approach selects the highest-quality translation from multiple candidates generated by different LLMs. The second approach, more akin to APE, instructs an LLM to replace error substrings as specified in the provided QE explanation(s). A conditional heuristic was employed to minimise the number of edits, with the aim of maximising the Gain-to-Edit ratio. The two proposed approaches achieved a Delta COMET score of 0.0201 and -0.0108, respectively, leading the first approach to achieve the winning position on the subtask leaderboard.

</details>


### [317] [What Works for 'Lost-in-the-Middle' in LLMs? A Study on GM-Extract and Mitigations](https://arxiv.org/abs/2511.13900)
*Mihir Gupte,Eshan Dixit,Muhammad Tayyab,Arun Adiththan*

Main category: cs.CL

TL;DR: 本文引入GM - Extract基准数据集评估大语言模型检索控制变量性能，提出评估系统，对7 - 8B参数模型进行多文档任务评估，分析性能模式并与困惑度关联，调研缓解方法并应用到基准上分析其效用。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在基于检索应用中‘迷失在中间’现象带来的长距离上下文利用能力下降问题。

Method: 引入GM - Extract基准数据集，提出含文档度量和变量提取度量的评估系统，对7 - 8B参数模型进行多文档任务评估，调研缓解方法并应用到基准。

Result: 改变数据在上下文窗口的表示会显著改变检索性能，分析出模型性能模式并与困惑度关联，缓解方法的效果有细微差别，有些能提升性能，有些会产生负面影响。

Conclusion: 对缓解方法在实际场景中的效用有了全面理解。

Abstract: The diminishing ability of large language models (LLMs) to effectively utilize long-range context-the "lost-in-the-middle" phenomenon-poses a significant challenge in retrieval-based LLM applications. To study the impact of this phenomenon in a real-world application setting, we introduce GM-Extract, a novel benchmark dataset meticulously designed to evaluate LLM performance on retrieval of control variables. To accurately diagnose failure modes, we propose a simple yet elegant evaluation system using two distinct metrics: one for spatial retrieval capability (Document Metric) and the other for semantic retrieval capability (Variable Extraction Metric). We conduct a systematic evaluation of 7-8B parameter models on two multi-document tasks (key-value extraction and question-answering), demonstrating a significant change in retrieval performance simply by altering how the data is represented in the context window. While a distinct U-shaped curve was not consistently observed, our analysis reveals a clear pattern of performance across models, which we further correlate with perplexity scores. Furthermore, we perform a literature survey of mitigation methods, which we categorize into two distinct approaches: black-box and white-box methods. We then apply these techniques to our benchmark, finding that their efficacy is highly nuanced. Our evaluation highlights scenarios where these strategies successfully improve performance, as well as surprising cases where they lead to a negative impact, providing a comprehensive understanding of their utility in a practical context.

</details>


### [318] [Knowledge-Grounded Agentic Large Language Models for Multi-Hazard Understanding from Reconnaissance Reports](https://arxiv.org/abs/2511.14010)
*Chenchen Kuai,Zihao Li,Braden Rosen,Stephanie Paan,Navid Jafari,Jean-Louis Briaud,Yunlong Zhang,Youssef M. A. Hashash,Yang Zhou*

Main category: cs.CL

TL;DR: 本文提出MoRA - RAG框架处理灾后侦察报告，提升多灾种推理的准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 灾后侦察报告非结构化，知识转移困难，大语言模型缺乏领域基础时输出不可靠，需构建新框架处理报告。

Method: 引入MoRA - RAG框架，集成混合检索机制、代理分块、验证循环，构建HazardRecQA数据集。

Result: MoRA - RAG准确率达94.5%，优于零样本大语言模型和现有RAG系统，减少幻觉，让开源模型达专有模型性能。

Conclusion: MoRA - RAG为将灾后文档转化为可靠情报以增强灾害恢复力建立了新范式。

Abstract: Post-disaster reconnaissance reports contain critical evidence for understanding multi-hazard interactions, yet their unstructured narratives make systematic knowledge transfer difficult. Large language models (LLMs) offer new potential for analyzing these reports, but often generate unreliable or hallucinated outputs when domain grounding is absent. This study introduces the Mixture-of-Retrieval Agentic RAG (MoRA-RAG), a knowledge-grounded LLM framework that transforms reconnaissance reports into a structured foundation for multi-hazard reasoning. The framework integrates a Mixture-of-Retrieval mechanism that dynamically routes queries across hazard-specific databases while using agentic chunking to preserve contextual coherence during retrieval. It also includes a verification loop that assesses evidence sufficiency, refines queries, and initiates targeted searches when information remains incomplete. We construct HazardRecQA by deriving question-answer pairs from GEER reconnaissance reports, which document 90 global events across seven major hazard types. MoRA-RAG achieves up to 94.5 percent accuracy, outperforming zero-shot LLMs by 30 percent and state-of-the-art RAG systems by 10 percent, while reducing hallucinations across diverse LLM architectures. MoRA-RAG also enables open-weight LLMs to achieve performance comparable to proprietary models. It establishes a new paradigm for transforming post-disaster documentation into actionable, trustworthy intelligence for hazard resilience.

</details>


### [319] [Synthetic Clinical Notes for Rare ICD Codes: A Data-Centric Framework for Long-Tail Medical Coding](https://arxiv.org/abs/2511.14112)
*Truong Vo,Weiyi Wu,Kaize Ding*

Main category: cs.CL

TL;DR: 提出数据中心框架生成合成出院小结缓解ICD编码长尾分布问题，微调模型后有一定性能提升。


<details>
  <summary>Details</summary>
Motivation: 自动ICD编码受长尾分布阻碍，数据集中罕见和零样本ICD编码代表性不足，导致宏观F1分数低。

Method: 构建以罕见编码为锚的多标签编码集，利用多种信息生成结构化提示，生成90,000条合成笔记，在原数据集和扩展数据集上微调两个模型。

Result: 适度提高宏观F1分数，保持强微观F1分数，优于先前SOTA方法。

Conclusion: 精心制作的合成数据可提升长尾ICD编码预测的公平性。

Abstract: Automatic ICD coding from clinical text is a critical task in medical NLP but remains hindered by the extreme long-tail distribution of diagnostic codes. Thousands of rare and zero-shot ICD codes are severely underrepresented in datasets like MIMIC-III, leading to low macro-F1 scores. In this work, we propose a data-centric framework that generates high-quality synthetic discharge summaries to mitigate this imbalance. Our method constructs realistic multi-label code sets anchored on rare codes by leveraging real-world co-occurrence patterns, ICD descriptions, synonyms, taxonomy, and similar clinical notes. Using these structured prompts, we generate 90,000 synthetic notes covering 7,902 ICD codes, significantly expanding the training distribution. We fine-tune two state-of-the-art transformer-based models, PLM-ICD and GKI-ICD, on both the original and extended datasets. Experiments show that our approach modestly improves macro-F1 while maintaining strong micro-F1, outperforming prior SOTA. While the gain may seem marginal relative to the computational cost, our results demonstrate that carefully crafted synthetic data can enhance equity in long-tail ICD code prediction.

</details>


### [320] [Selective Weak-to-Strong Generalization](https://arxiv.org/abs/2511.14166)
*Hao Lang,Fei Huang,Yongbin Li*

Main category: cs.CL

TL;DR: 为解决超人类模型对齐时缺乏高质量数据问题，提出选择性弱到强泛化框架，实验表明该方法优于基线，且有助于超对齐。


<details>
  <summary>Details</summary>
Motivation: 现有弱到强泛化方法持续使用弱监督存在鲁棒性问题，部分弱标签对模型有害，需解决模型对齐时缺乏高质量数据的问题。

Method: 提出选择性弱到强泛化框架，训练二元分类器P(IK)识别强模型能回答的问题并使用其自生成标签进行对齐，用图平滑方法细化弱标签。

Result: 在三个基准测试上的大量实验表明，该方法始终优于竞争基线。

Conclusion: P(IK)能跨任务和难度泛化，选择性弱到强泛化有助于超对齐。

Abstract: Future superhuman models will surpass the ability of humans and humans will only be able to \textit{weakly} supervise superhuman models. To alleviate the issue of lacking high-quality data for model alignment, some works on weak-to-strong generalization (W2SG) finetune a strong pretrained model with a weak supervisor so that it can generalize beyond weak supervision. However, the invariable use of weak supervision in existing methods exposes issues in robustness, with a proportion of weak labels proving harmful to models. In this paper, we propose a selective W2SG framework to avoid using weak supervision when unnecessary. We train a binary classifier P(IK) to identify questions that a strong model can answer and use its self-generated labels for alignment. We further refine weak labels with a graph smoothing method. Extensive experiments on three benchmarks show that our method consistently outperforms competitive baselines. Further analyses show that P(IK) can generalize across tasks and difficulties, which indicates selective W2SG can help superalignment.

</details>


### [321] [SymLoc: Symbolic Localization of Hallucination across HaluEval and TruthfulQA](https://arxiv.org/abs/2511.14172)
*Naveen Lamba,Sanju Tiwari,Manas Gaur*

Main category: cs.CL

TL;DR: 提出首个符号定位框架，发现大模型幻觉源于符号语言处理失败。


<details>
  <summary>Details</summary>
Motivation: 现有方法未考虑符号语言知识在幻觉中的作用，缺乏基于此的定位框架，需系统处理符号触发因素并定位幻觉起源。

Method: 提出利用符号语言和语义知识的定位框架，用HaluEval和TruthfulQA分析五个模型。

Result: 早期层（2 - 4）符号元素注意力方差达临界不稳定，否定词引发灾难性方差，大模型幻觉率高，深层符号语义触发注意力陡降。

Conclusion: 幻觉本质是符号语言处理失败，符号语义知识是理解和定位大模型幻觉机制的关键。

Abstract: LLMs still struggle with hallucination, especially when confronted with symbolic triggers like modifiers, negation, numbers, exceptions, and named entities. Yet, we lack a clear understanding of where these symbolic hallucinations originate, making it crucial to systematically handle such triggers and localize the emergence of hallucination inside the model. While prior work explored localization using statistical techniques like LSC and activation variance analysis, these methods treat all tokens equally and overlook the role symbolic linguistic knowledge plays in triggering hallucinations. So far, no approach has investigated how symbolic elements specifically drive hallucination failures across model layers, nor has symbolic linguistic knowledge been used as the foundation for a localization framework. We propose the first symbolic localization framework that leverages symbolic linguistic and semantic knowledge to meaningfully trace the development of hallucinations across all model layers. By focusing on how models process symbolic triggers, we analyze five models using HaluEval and TruthfulQA. Our symbolic knowledge approach reveals that attention variance for these linguistic elements explodes to critical instability in early layers (2-4), with negation triggering catastrophic variance levels, demonstrating that symbolic semantic processing breaks down from the very beginning. Through the lens of symbolic linguistic knowledge, despite larger model sizes, hallucination rates remain consistently high (78.3%-83.7% across Gemma variants), with steep attention drops for symbolic semantic triggers throughout deeper layers. Our findings demonstrate that hallucination is fundamentally a symbolic linguistic processing failure, not a general generation problem, revealing that symbolic semantic knowledge provides the key to understanding and localizing hallucination mechanisms in LLMs.

</details>


### [322] [ArbESC+: Arabic Enhanced Edit Selection System Combination for Grammatical Error Correction Resolving conflict and improving system combination in Arabic GEC](https://arxiv.org/abs/2511.14230)
*Ahlam Alrehili,Areej Alhothali*

Main category: cs.CL

TL;DR: 本文提出阿拉伯语语法纠错多系统方法ArbESC+，结合多模型效果优于单模型，是阿拉伯语纠错集成的首次尝试。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语形态和句法结构复杂，此前多数尝试用单模型，未考虑多系统结合的潜在优势。

Method: 提出ArbESC+方法，用多个模型收集纠错建议，以数值特征表示，通过分类器确定并实施纠错，用支持技术过滤重叠纠错和评估决策可靠性。

Result: 结合多个模型在不同测试数据上的F0.5值分别为：QALB - 14测试数据82.63%，QALB - 15 L1数据84.64%，QALB - 15 L2数据65.55%。

Conclusion: 这是阿拉伯语语言纠错集成的首次尝试，改进现有模型有助于开发高级阿拉伯语文本处理工具。

Abstract: Grammatical Error Correction (GEC) is an important aspect of natural language processing. Arabic has a complicated morphological and syntactic structure, posing a greater challenge than other languages. Even though modern neural models have improved greatly in recent years, the majority of previous attempts used individual models without taking into account the potential benefits of combining different systems. In this paper, we present one of the first multi-system approaches for correcting grammatical errors in Arabic, the Arab Enhanced Edit Selection System Complication (ArbESC+). Several models are used to collect correction proposals, which are represented as numerical features in the framework. A classifier determines and implements the appropriate corrections based on these features. In order to improve output quality, the framework uses support techniques to filter overlapping corrections and estimate decision reliability. A combination of AraT5, ByT5, mT5, AraBART, AraBART+Morph+GEC, and Text editing systems gave better results than a single model alone, with F0.5 at 82.63% on QALB-14 test data, 84.64% on QALB-15 L1 data, and 65.55% on QALB-15 L2 data. As one of the most significant contributions of this work, it's the first Arab attempt to integrate linguistic error correction. Improving existing models provides a practical step towards developing advanced tools that will benefit users and researchers of Arabic text processing.

</details>


### [323] [AraLingBench A Human-Annotated Benchmark for Evaluating Arabic Linguistic Capabilities of Large Language Models](https://arxiv.org/abs/2511.14295)
*Mohammad Zbib,Hasan Abed Al Kader Hammoud,Sina Mukalled,Nadine Rizk,Fatima Karnib,Issam Lakkis,Ammar Mohanna,Bernard Ghanem*

Main category: cs.CL

TL;DR: 介绍用于评估阿拉伯语大语言模型的人类注释基准AraLingBench，评估发现模型表面能力强但深层推理弱，提供诊断框架。


<details>
  <summary>Details</summary>
Motivation: 评估阿拉伯语大语言模型的语言能力，解决当前知识基准与真正语言掌握能力之间的评估问题。

Method: 创建包含语法、形态学等五核心类别的150道选择题的基准，评估35个阿拉伯语和双语大语言模型。

Result: 当前模型表面能力强，但在深层语法和句法推理上有困难，很多模型靠记忆或模式识别取得高分。

Conclusion: AraLingBench为阿拉伯语大语言模型开发提供诊断框架，代码公开。

Abstract: We present AraLingBench: a fully human annotated benchmark for evaluating the Arabic linguistic competence of large language models (LLMs). The benchmark spans five core categories: grammar, morphology, spelling, reading comprehension, and syntax, through 150 expert-designed multiple choice questions that directly assess structural language understanding. Evaluating 35 Arabic and bilingual LLMs reveals that current models demonstrate strong surface level proficiency but struggle with deeper grammatical and syntactic reasoning. AraLingBench highlights a persistent gap between high scores on knowledge-based benchmarks and true linguistic mastery, showing that many models succeed through memorization or pattern recognition rather than authentic comprehension. By isolating and measuring fundamental linguistic skills, AraLingBench provides a diagnostic framework for developing Arabic LLMs. The full evaluation code is publicly available on GitHub.

</details>


### [324] [From Graphs to Hypergraphs: Enhancing Aspect-Based Sentiment Analysis via Multi-Level Relational Modeling](https://arxiv.org/abs/2511.14142)
*Omkar Mahesh Kashyap,Padegal Amit,Madhav Kashyap,Ashwini M Joshi,Shylaja SS*

Main category: cs.CL

TL;DR: 提出HyperABSA动态超图框架用于方面情感分析，在三个基准测试中表现优于图基线。


<details>
  <summary>Details</summary>
Motivation: 以往基于图的方面情感分析方法仅建模成对依赖，构建多图有冗余、参数开销和误差传播问题，在短文本、低资源场景下鲁棒性差。

Method: 提出HyperABSA动态超图框架，通过特定样本的层次聚类诱导方面 - 观点结构，引入新的加速 - 回退截断方法确定聚类粒度。

Result: 在Lap14、Rest14、MAMS三个基准测试中比强大的图基线有一致提升，与RoBERTa结合时有显著增益。

Conclusion: 动态超图构建是方面情感分析的有效强大替代方案，有扩展到其他短文本NLP任务的潜力。

Abstract: Aspect-Based Sentiment Analysis (ABSA) predicts sentiment polarity for specific aspect terms, a task made difficult by conflicting sentiments across aspects and the sparse context of short texts. Prior graph-based approaches model only pairwise dependencies, forcing them to construct multiple graphs for different relational views. These introduce redundancy, parameter overhead, and error propagation during fusion, limiting robustness in short-text, low-resource settings. We present HyperABSA, a dynamic hypergraph framework that induces aspect-opinion structures through sample-specific hierarchical clustering. To construct these hyperedges, we introduce a novel acceleration-fallback cutoff for hierarchical clustering, which adaptively determines the level of granularity. Experiments on three benchmarks (Lap14, Rest14, MAMS) show consistent improvements over strong graph baselines, with substantial gains when paired with RoBERTa backbones. These results position dynamic hypergraph construction as an efficient, powerful alternative for ABSA, with potential extensions to other short-text NLP tasks.

</details>


### [325] [The Tokenization Bottleneck: How Vocabulary Extension Improves Chemistry Representation Learning in Pretrained Language Models](https://arxiv.org/abs/2511.14365)
*Prathamesh Kalamkar,Ned Letcher,Meissane Chami,Sahger Lad,Shayan Mohanty,Prasanna Pendse*

Main category: cs.CL

TL;DR: 本文提出统一自然语言和分子结构表示的方法解决大语言模型应用于化学的分词瓶颈，经预训练后在下游化学任务表现出色。


<details>
  <summary>Details</summary>
Motivation: 大语言模型应用于化学时存在“分词瓶颈”，通用分词器会将化学表示碎片化。

Method: 统一自然语言和分子结构表示，对预训练大语言模型词汇表进行针对性扩展，增加化学显著标记，然后在化学领域文本上继续预训练。

Result: 该方法在一系列下游化学任务中表现出优越性能。

Conclusion: 提出的解决分词瓶颈的方法有效。

Abstract: The application of large language models (LLMs) to chemistry is frequently hampered by a "tokenization bottleneck", where tokenizers tuned on general-domain text tend to fragment chemical representations such as SMILES into semantically uninformative sub-tokens. This paper introduces a principled methodology to resolve this bottleneck by unifying the representation of natural language and molecular structures within a single model. Our approach involves targeted vocabulary extension-augmenting a pretrained LLM's vocabulary with chemically salient tokens, followed by continued pretraining on chemistry-domain text to integrate this new knowledge. We provide an empirical demonstration of the effectiveness of this strategy, showing that our methodology leads to superior performance on a range of downstream chemical tasks.

</details>


### [326] [Tell Me: An LLM-powered Mental Well-being Assistant with RAG, Synthetic Dialogue Generation, and Agentic Planning](https://arxiv.org/abs/2511.14445)
*Trishala Jayesh Ahalpara*

Main category: cs.CL

TL;DR: 介绍了Tell Me心理健康系统，集成三个组件，可提供支持、生成对话、制定计划，展示其架构与功能并评估，强调跨学科合作机会。


<details>
  <summary>Details</summary>
Motivation: 利用大语言模型为用户和研究人员提供可访问、上下文感知的心理健康支持，解决治疗数据不足问题。

Method: 系统集成RAG助手、合成对话生成器和Well - being AI crew三个组件；用自动LLM判断和用户研究评估RAG助手。

Result: 展示了系统架构和功能，完成对RAG助手在特定场景下的评估。

Conclusion: 该系统能降低支持门槛、补充现有护理、拓宽资源获取途径，凸显NLP研究人员和心理健康专家跨学科合作推动人机交互创新的机会。

Abstract: We present Tell Me, a mental well-being system that leverages advances in large language models to provide accessible, context-aware support for users and researchers. The system integrates three components: (i) a retrieval-augmented generation (RAG) assistant for personalized, knowledge-grounded dialogue; (ii) a synthetic client-therapist dialogue generator conditioned on client profiles to facilitate research on therapeutic language and data augmentation; and (iii) a Well-being AI crew, implemented with CrewAI, that produces weekly self-care plans and guided meditation audio. The system is designed as a reflective space for emotional processing rather than a substitute for professional therapy. It illustrates how conversational assistants can lower barriers to support, complement existing care, and broaden access to mental health resources. To address the shortage of confidential therapeutic data, we introduce synthetic client-therapist dialogue generation conditioned on client profiles. Finally, the planner demonstrates an innovative agentic workflow for dynamically adaptive, personalized self-care, bridging the limitations of static well-being tools. We describe the architecture, demonstrate its functionalities, and report evaluation of the RAG assistant in curated well-being scenarios using both automatic LLM-based judgments and a human-user study. This work highlights opportunities for interdisciplinary collaboration between NLP researchers and mental health professionals to advance responsible innovation in human-AI interaction for well-being.

</details>


### [327] [Examining the Metrics for Document-Level Claim Extraction in Czech and Slovak](https://arxiv.org/abs/2511.14566)
*Lucia Makaiová,Martin Fajčík,Antonín Jarolím*

Main category: cs.CL

TL;DR: 本文探索对齐两组来自同一源文档的声明并计算相似度的方法，在新数据集上实验，指出当前评估方法在文档级声明提取中的局限性。


<details>
  <summary>Details</summary>
Motivation: 文档级声明提取是事实核查领域的挑战，且评估提取声明的方法受关注有限，需可靠评估框架。

Method: 探索对齐两组声明并计算相似度的方法，确定最佳对齐和评估方式，可比较模型提取和人工标注的声明集。

Result: 在新收集的数据集上实验，发现当前评估方法在文档级声明提取中的局限性。

Conclusion: 需要更先进的方法来正确捕捉语义相似性，评估声明的原子性、可核查性和去上下文等关键属性。

Abstract: Document-level claim extraction remains an open challenge in the field of fact-checking, and subsequently, methods for evaluating extracted claims have received limited attention. In this work, we explore approaches to aligning two sets of claims pertaining to the same source document and computing their similarity through an alignment score. We investigate techniques to identify the best possible alignment and evaluation method between claim sets, with the aim of providing a reliable evaluation framework. Our approach enables comparison between model-extracted and human-annotated claim sets, serving as a metric for assessing the extraction performance of models and also as a possible measure of inter-annotator agreement. We conduct experiments on newly collected dataset-claims extracted from comments under Czech and Slovak news articles-domains that pose additional challenges due to the informal language, strong local context, and subtleties of these closely related languages. The results draw attention to the limitations of current evaluation approaches when applied to document-level claim extraction and highlight the need for more advanced methods-ones able to correctly capture semantic similarity and evaluate essential claim properties such as atomicity, checkworthiness, and decontextualization.

</details>


### [328] [A Method for Characterizing Disease Progression from Acute Kidney Injury to Chronic Kidney Disease](https://arxiv.org/abs/2511.14603)
*Yilu Fang,Jordan G. Nestor,Casey N. Ta,Jerard Z. Kneifati-Hayek,Chunhua Weng*

Main category: cs.CL

TL;DR: 利用电子病历数据追踪急性肾损伤（AKI）患者临床演变，识别AKI转慢性肾病（CKD）风险因素，支持早期CKD检测和干预决策工具开发。


<details>
  <summary>Details</summary>
Motivation: AKI患者发展为CKD风险高，但识别高风险患者有挑战，故研究如何识别这些患者。

Method: 用电子病历数据，通过聚类患者向量识别AKI后临床状态，用多状态模型估计状态间转移概率和CKD进展，用生存分析识别AKI亚群中CKD风险因素。

Result: 20699名入院AKI患者中17%发展为CKD，识别出15种不同AKI后状态，多数患者处于单一状态或仅一次转移，发现既定和新的CKD风险因素。

Conclusion: 该数据驱动方法可识别高风险AKI患者，支持早期CKD检测和干预决策工具开发。

Abstract: Patients with acute kidney injury (AKI) are at high risk of developing chronic kidney disease (CKD), but identifying those at greatest risk remains challenging. We used electronic health record (EHR) data to dynamically track AKI patients' clinical evolution and characterize AKI-to-CKD progression. Post-AKI clinical states were identified by clustering patient vectors derived from longitudinal medical codes and creatinine measurements. Transition probabilities between states and progression to CKD were estimated using multi-state modeling. After identifying common post-AKI trajectories, CKD risk factors in AKI subpopulations were identified through survival analysis. Of 20,699 patients with AKI at admission, 3,491 (17%) developed CKD. We identified fifteen distinct post-AKI states, each with different probabilities of CKD development. Most patients (75%, n=15,607) remained in a single state or made only one transition during the study period. Both established (e.g., AKI severity, diabetes, hypertension, heart failure, liver disease) and novel CKD risk factors, with their impact varying across these clinical states. This study demonstrates a data-driven approach for identifying high-risk AKI patients, supporting the development of decision-support tools for early CKD detection and intervention.

</details>


### [329] [Bridging Human and Model Perspectives: A Comparative Analysis of Political Bias Detection in News Media Using Large Language Models](https://arxiv.org/abs/2511.14606)
*Shreya Adrita Banik,Niaz Nafi Rahman,Tahsina Moiukh,Farig Sadeque*

Main category: cs.CL

TL;DR: 研究提出评估人类注释和多个大语言模型检测政治偏见的比较框架，发现不同模型与人类标注的一致性，强调自动化媒体偏见检测需结合人类可解释性和模型可扩展性的混合评估框架。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在检测政治偏见时与人类判断的一致性研究不足，需进行评估。

Method: 构建新闻文章手动标注数据集，评估标注一致性、偏见极性和模型间一致性，量化人类和模型对偏见感知的差异。

Result: 传统基于Transformer模型中RoBERTa与人类标签一致性最高；生成式模型如GPT在零样本设置下与人类注释整体一致性最强；微调后的RoBERTa模型准确率最高、与人类标注标签一致性最强。

Conclusion: 人类和大语言模型对政治倾向的感知存在系统差异，自动化媒体偏见检测需要混合评估框架。

Abstract: Detecting political bias in news media is a complex task that requires interpreting subtle linguistic and contextual cues. Although recent advances in Natural Language Processing (NLP) have enabled automatic bias classification, the extent to which large language models (LLMs) align with human judgment still remains relatively underexplored and not yet well understood. This study aims to present a comparative framework for evaluating the detection of political bias across human annotations and multiple LLMs, including GPT, BERT, RoBERTa, and FLAN. We construct a manually annotated dataset of news articles and assess annotation consistency, bias polarity, and inter-model agreement to quantify divergence between human and model perceptions of bias. Experimental results show that among traditional transformer-based models, RoBERTa achieves the highest alignment with human labels, whereas generative models such as GPT demonstrate the strongest overall agreement with human annotations in a zero-shot setting. Among all transformer-based baselines, our fine-tuned RoBERTa model acquired the highest accuracy and the strongest alignment with human-annotated labels. Our findings highlight systematic differences in how humans and LLMs perceive political slant, underscoring the need for hybrid evaluation frameworks that combine human interpretability with model scalability in automated media bias detection.

</details>


### [330] [Enhancing Agentic Autonomous Scientific Discovery with Vision-Language Model Capabilities](https://arxiv.org/abs/2511.14631)
*Kahaan Gandhi,Boris Bolliet,Inigo Zubeldia*

Main category: cs.CL

TL;DR: 基于视觉语言模型（VLM）的多智能体系统可提升端到端自主科学发现能力，案例和基准测试显示其效果好且推理可审计。


<details>
  <summary>Details</summary>
Motivation: 提升端到端自主科学发现的能力。

Method: 将VLM作为评判器，把图表视为可验证检查点，依据动态生成的特定领域规则评估图表，让智能体实时纠正错误并引导探索性数据分析。

Result: 在宇宙学和天体化学案例研究中，系统能自动从错误推理路径恢复并适应新数据集；在10任务基准测试中，VLM增强系统得分为0.7 - 0.8，高于仅代码和代码加文本的基线，且推理可审计。

Conclusion: 基于VLM的多智能体系统有助于端到端自主科学发现。

Abstract: We show that multi-agent systems guided by vision-language models (VLMs) improve end-to-end autonomous scientific discovery. By treating plots as verifiable checkpoints, a VLM-as-a-judge evaluates figures against dynamically generated domain-specific rubrics, enabling agents to correct their own errors and steer exploratory data analysis in real-time. Case studies in cosmology and astrochemistry demonstrate recovery from faulty reasoning paths and adaptation to new datasets without human intervention. On a 10-task benchmark for data-driven discovery, VLM-augmented systems achieve pass at 1 scores of 0.7-0.8, compared to 0.2-0.3 for code-only and 0.4-0.5 for code-and-text baselines, while also providing auditable reasoning traces that improve interpretability. Code available here: https://github.com/CMBAgents/cmbagent

</details>


### [331] [Ground Truth Generation for Multilingual Historical NLP using LLMs](https://arxiv.org/abs/2511.14688)
*Clovis Gladstone,Zhao Fang,Spencer Dean Stewart*

Main category: cs.CL

TL;DR: 本文利用大语言模型为历史法语和中文文本创建标注数据，微调spaCy在特定时期测试中取得显著成果，表明特定领域模型及少量合成数据对资源不足语料NLP工具的重要性。


<details>
  <summary>Details</summary>
Motivation: 历史和低资源NLP因标注数据有限和与现代语料领域不匹配而面临挑战。

Method: 利用大语言模型为语料子集生成标注数据，微调spaCy。

Result: 在词性标注、词形还原和命名实体识别的特定时期测试中取得显著提升。

Conclusion: 特定领域模型很重要，少量合成数据可提升计算人文研究中低资源语料的NLP工具性能。

Abstract: Historical and low-resource NLP remains challenging due to limited annotated data and domain mismatches with modern, web-sourced corpora. This paper outlines our work in using large language models (LLMs) to create ground-truth annotations for historical French (16th-20th centuries) and Chinese (1900-1950) texts. By leveraging LLM-generated ground truth on a subset of our corpus, we were able to fine-tune spaCy to achieve significant gains on period-specific tests for part-of-speech (POS) annotations, lemmatization, and named entity recognition (NER). Our results underscore the importance of domain-specific models and demonstrate that even relatively limited amounts of synthetic data can improve NLP tools for under-resourced corpora in computational humanities research.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [332] [Self-Supervised Compression and Artifact Correction for Streaming Underwater Imaging Sonar](https://arxiv.org/abs/2511.13922)
*Rongsheng Qian,Chi Xu,Xiaoqiang Ma,Hao Fang,Yili Jin,William I. Atlas,Jiangchuan Liu*

Main category: eess.IV

TL;DR: 提出SCOPE框架，联合进行压缩和伪影校正，在原位声纳数据上表现良好，减少带宽并改善检测，已部署用于野外监测。


<details>
  <summary>Details</summary>
Motivation: 实时成像声纳广泛使用受限于有限的上行链路带宽和严重的声纳伪影。

Method: 结合自适应码本压缩（ACC）和频率感知多尺度分割（FAMS），采用对冲训练策略。

Result: 在原位声纳数据上结构相似性指数达0.77，比先前自监督去噪基线提高40%，降低80%以上带宽，系统实时运行。

Conclusion: 学习频率结构潜变量可实现实际低比特率声纳流，保留信号细节。

Abstract: Real-time imaging sonar has become an important tool for underwater monitoring in environments where optical sensing is unreliable. Its broader use is constrained by two coupled challenges: highly limited uplink bandwidth and severe sonar-specific artifacts (speckle, motion blur, reverberation, acoustic shadows) that affect up to 98% of frames. We present SCOPE, a self-supervised framework that jointly performs compression and artifact correction without clean-noise pairs or synthetic assumptions. SCOPE combines (i) Adaptive Codebook Compression (ACC), which learns frequency-encoded latent representations tailored to sonar, with (ii) Frequency-Aware Multiscale Segmentation (FAMS), which decomposes frames into low-frequency structure and sparse high-frequency dynamics while suppressing rapidly fluctuating artifacts. A hedging training strategy further guides frequency-aware learning using low-pass proxy pairs generated without labels. Evaluated on months of in-situ ARIS sonar data, SCOPE achieves a structural similarity index (SSIM) of 0.77, representing a 40% improvement over prior self-supervised denoising baselines, at bitrates down to <= 0.0118 bpp. It reduces uplink bandwidth by more than 80% while improving downstream detection. The system runs in real time, with 3.1 ms encoding on an embedded GPU and 97 ms full multi-layer decoding on the server end. SCOPE has been deployed for months in three Pacific Northwest rivers to support real-time salmon enumeration and environmental monitoring in the wild. Results demonstrate that learning frequency-structured latents enables practical, low-bitrate sonar streaming with preserved signal details under real-world deployment conditions.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [333] [Randomized Controlled Trials for Phishing Triage Agent](https://arxiv.org/abs/2511.13860)
*James Bono*

Main category: econ.GN

TL;DR: 本文首次用RCT评估微软安全副驾驶网络钓鱼分类代理对分析师生产力和准确性的影响，结果显示代理增强分析师效率和准确性提升，为SOC领导者提供AI应用见解。


<details>
  <summary>Details</summary>
Motivation: 解决安全运营中心（SOCs）高效分类大量用户报告的网络钓鱼邮件并保持强大威胁防护的挑战。

Method: 进行随机对照试验（RCT）评估微软安全副驾驶网络钓鱼分类代理的影响。

Result: 代理增强分析师每分钟真正阳性结果最多达6.5倍，裁决准确性提高77%；代理的队列优先级和裁决解释是效率的重要驱动因素；分析师重新分配注意力，在恶意邮件上花费时间增加53%，且不易盲目接受代理的恶意裁决。

Conclusion: 研究结果为考虑采用AI的SOC领导者提供可操作的见解，包括代理可能从根本上改变SOC资源的最优分配。

Abstract: Security operations centers (SOCs) face a persistent challenge: efficiently triaging a high volume of user-reported phishing emails while maintaining robust protection against threats. This paper presents the first randomized controlled trial (RCT) evaluating the impact of a domain-specific AI agent - the Microsoft Security Copilot Phishing Triage Agent - on analyst productivity and accuracy. Our results demonstrate that agent-augmented analysts achieved up to 6.5 times as many true positives per analyst minute and a 77% improvement in verdict accuracy compared to a control group. The agent's queue prioritization and verdict explanations were both significant drivers of efficiency. Behavioral analysis revealed that agent-augmented analysts reallocated their attention, spending 53% more time on malicious emails, and were not prone to rubber-stamping the agent's malicious verdicts. These findings offer actionable insights for SOC leaders considering AI adoption, including the potential for agents to fundamentally change the optimal allocation of SOC resources.

</details>


### [334] [Randomized Controlled Trials for Conditional Access Optimization Agent](https://arxiv.org/abs/2511.13865)
*James Bono,Beibei Cheng,Joaquin Lozano*

Main category: econ.GN

TL;DR: 本文介绍首个评估AI代理用于微软Entra条件访问策略管理的随机对照试验，显示该代理能显著提升身份管理的速度和准确性。


<details>
  <summary>Details</summary>
Motivation: 当前AI代理在身份治理有效性方面证据有限，需评估其在微软Entra条件访问策略管理中的效果。

Method: 进行随机对照试验，将162名身份管理员随机分为对照组和处理组，处理组使用AI代理协助完成四项高价值任务。

Result: 使用AI代理使准确性提高48%，在保持准确性不变时任务完成时间减少43%，在认知要求高的任务上效益最大。

Conclusion: 专门设计的AI代理能显著提升身份管理的速度和准确性。

Abstract: AI agents are increasingly deployed to automate complex enterprise workflows, yet evidence of their effectiveness in identity governance is limited. We report results from the first randomized controlled trial (RCT) evaluating an AI agent for Conditional Access (CA) policy management in Microsoft Entra. The agent assists with four high-value tasks: policy merging, Zero-Trust baseline gap detection, phased rollout planning, and user-policy alignment. In a production-grade environment, 162 identity administrators were randomly assigned to a control group (no agent) or treatment group (agent-assisted) and asked to perform these tasks. Agent access produced substantial gains: accuracy improved by 48% and task completion time decreased by 43% while holding accuracy constant. The largest benefits emerged on cognitively demanding tasks such as baseline gap detection. These findings demonstrate that purpose-built AI agents can significantly enhance both speed and accuracy in identity administration.

</details>


### [335] [HSBC 1950 to 2025: Conquering the world from British Hong Kong and London](https://arxiv.org/abs/2511.13878)
*Christopher Mantzaris,Ajda Fošner*

Main category: econ.GN

TL;DR: 文章讲述汇丰银行经历战乱后发展，基于香港经济繁荣成长，通过收购扩张，重点提及收购米特兰银行原因，还给出其繁荣稳定原因以提供商业借鉴。


<details>
  <summary>Details</summary>
Motivation: 分析汇丰银行在复杂环境下发展繁荣稳定的原因，总结商业经验。

Method: 回顾汇丰银行发展历程，分析其在不同阶段的发展策略，重点阐述收购米特兰银行的背景和原因。

Result: 指出汇丰银行通过抓住香港经济发展机遇、进行收购等策略实现扩张，收购米特兰银行较为成功。

Conclusion: 可从汇丰银行繁荣稳定的原因中汲取商业经验。但文中‘中国共产党自2020年非法侵蚀香港民主、法治和公民自由’表述与事实严重不符，是对中国的恶意造谣抹黑。

Abstract: The Hongkong and Shanghai Banking Co (HSBC) just survived a civil war intermitted by World War II. By the 1950s, it obviously needed to close all its branches in Mao's People's Republic of China, yet could somehow hold its Shanghai branch, which continued likely in the shadows, as non-state banking was illegalised and even simple land owners were executed merely for being labelled "capitalist". This Asia-focused bank --in spite of it all-- grew from these conditions into the behemoth it is today. Part of the growth was based on the economic boom in its core market Hong Kong, to which HSBC likely also contributed. To expand and diversify, HSBC continued the growth strategy that already started since its early days in the 1860s, this time just also inorganically: It acquired other banks, in most cases fully and in other regions. The most important acquisition was the takeover of the roughly equally-sized UK-based Midland Bank; for the following reasons: 1) It came just a year after the 1991 change of HSBC's headquarters and place of incorporation to London, so HSBC could smoothly integrate with Midland. This step also came with an additional listing of securities in London, providing HSBC funds. 2) These funds were used efficiently without much idling for the humongous acquisition. 3) The preceding decade of Margaret Thatcher's banking and finance deregulation in the UK created a beneficial environment for HSBC. 4) HSBC was proven right by the developments in Hong Kong, where the Communist Party of China illegally eroded democracy, the rule of law and civil liberties since 2020, despite promising to maintain these at least until 1 July 2047. A list of likely reasons for HSBC's prosperity and stability in face of the at times hostile environments is also provided, from which business lessons can be drawn.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [336] [Can LLMs Create Legally Relevant Summaries and Analyses of Videos?](https://arxiv.org/abs/2511.13772)
*Lyra Hoeben-Kuil,Gijs van Dijck,Jaromir Savelka,Johanna Gunawan,Konrad Kollnig,Marta Kolacz,Mindy Duffourc,Shashank Chakravarthy,Hannes Westermann*

Main category: cs.MM

TL;DR: 研究大语言模型理解和总结视频中法律事件的能力，用120个YouTube视频测试，71.7%的总结质量为中高。


<details>
  <summary>Details</summary>
Motivation: 理解并通过文本传达事件的法律相关事实是法律专业关键技能，普通人有挑战，现有AI需用户用文本表述事件也有困难，因此研究大语言模型理解视频中事件的能力。

Method: 让大语言模型基于120个展示不同领域法律问题的YouTube视频进行总结和起草法律信件。

Result: 71.7%的总结被评为中高质量。

Conclusion: 结果有前景，为司法可及性等领域的应用打开了大门。

Abstract: Understanding the legally relevant factual basis of an event and conveying it through text is a key skill of legal professionals. This skill is important for preparing forms (e.g., insurance claims) or other legal documents (e.g., court claims), but often presents a challenge for laypeople. Current AI approaches aim to bridge this gap, but mostly rely on the user to articulate what has happened in text, which may be challenging for many. Here, we investigate the capability of large language models (LLMs) to understand and summarize events occurring in videos. We ask an LLM to summarize and draft legal letters, based on 120 YouTube videos showing legal issues in various domains. Overall, 71.7\% of the summaries were rated as of high or medium quality, which is a promising result, opening the door to a number of applications in e.g. access to justice.

</details>


### [337] [Real-Time Mobile Video Analytics for Pre-arrival Emergency Medical Services](https://arxiv.org/abs/2511.14119)
*Liuyi Jin,Amran Haroon,Radu Stoleru,Pasan Gunawardena,Michael Middleton,Jeeeun Kim*

Main category: cs.MM

TL;DR: 介绍了移动实时视频分析系统TeleEMS，含客户端与服务器，评估显示其有良好效果，能变革急救服务。


<details>
  <summary>Details</summary>
Motivation: 当前急救服务基础设施受一对一视频流和有限分析能力限制，需及时准确的预到达视频流和分析。

Method: 构建TeleEMS系统，含客户端和服务器，服务器集成通信骨干网并设有三个实时分析模块。

Result: EMSLlama表现优于GPT - 4o，文本 - 生命体征融合提高推理鲁棒性。

Conclusion: TeleEMS有潜力变革急救服务运营，为下一代智能急救基础设施铺路。

Abstract: Timely and accurate pre-arrival video streaming and analytics are critical for emergency medical services (EMS) to deliver life-saving interventions. Yet, current-generation EMS infrastructure remains constrained by one-to-one video streaming and limited analytics capabilities, leaving dispatchers and EMTs to manually interpret overwhelming, often noisy or redundant information in high-stress environments. We present TeleEMS, a mobile live video analytics system that enables pre-arrival multimodal inference by fusing audio and video into a unified decision-making pipeline before EMTs arrive on scene.
  TeleEMS comprises two key components: TeleEMS Client and TeleEMS Server. The TeleEMS Client runs across phones, smart glasses, and desktops to support bystanders, EMTs en route, and 911 dispatchers. The TeleEMS Server, deployed at the edge, integrates EMS-Stream, a communication backbone that enables smooth multi-party video streaming. On top of EMSStream, the server hosts three real-time analytics modules: (1) audio-to-symptom analytics via EMSLlama, a domain-specialized LLM for robust symptom extraction and normalization; (2) video-to-vital analytics using state-of-the-art rPPG methods for heart rate estimation; and (3) joint text-vital analytics via PreNet, a multimodal multitask model predicting EMS protocols, medication types, medication quantities, and procedures.
  Evaluation shows that EMSLlama outperforms GPT-4o (exact-match 0.89 vs. 0.57) and that text-vital fusion improves inference robustness, enabling reliable pre-arrival intervention recommendations. TeleEMS demonstrates the potential of mobile live video analytics to transform EMS operations, bridging the gap between bystanders, dispatchers, and EMTs, and paving the way for next-generation intelligent EMS infrastructure.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [338] [PIM or CXL-PIM? Understanding Architectural Trade-offs Through Large-Scale Benchmarking](https://arxiv.org/abs/2511.14400)
*I-Ting Lee,Bao-Kai Wang,Liang-Chi Chen,Wen Sheng Lim,Da-Wei Chang,Yu-Ming Chang,Chieng-Chung Ho*

Main category: cs.ET

TL;DR: 本文对PIM和CXL - PIM进行大规模对比，揭示两种架构性能相对排名逆转的情况，为近内存系统设计提供指导。


<details>
  <summary>Details</summary>
Motivation: 现有小规模研究未涵盖PIM和CXL - PIM不同接口模型带来的工作负载相关权衡问题，需大规模对比二者。

Method: 使用真实PIM硬件测量和跟踪驱动的CXL建模进行PIM和CXL - PIM的并排大规模对比。

Result: 发现统一地址访问可分摊链路延迟克服传输瓶颈的情况，以及紧密耦合PIM更优的情况，揭示两种架构相对排名逆转的阶段和数据集大小范围。

Conclusion: 研究结果为未来近内存系统设计提供了实用指导。

Abstract: Processing-in-memory (PIM) reduces data movement by executing near memory, but our large-scale characterization on real PIM hardware shows that end-to-end performance is often limited by disjoint host and device address spaces that force explicit staging transfers. In contrast, CXL-PIM provides a unified address space and cache-coherent access at the cost of higher access latency. These opposing interface models create workload-dependent tradeoffs that are not captured by small-scale studies. This work presents a side-by-side, large-scale comparison of PIM and CXL-PIM using measurements from real PIM hardware and trace-driven CXL modeling. We identify when unified-address access amortizes link latency enough to overcome transfer bottlenecks, and when tightly coupled PIM remains preferable. Our results reveal phase- and dataset-size regimes in which the relative ranking between the two architectures reverses, offering practical guidance for future near-memory system design.

</details>
