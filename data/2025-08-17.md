<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 32]
- [cs.CE](#cs.CE) [Total: 3]
- [cs.DB](#cs.DB) [Total: 5]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.DS](#cs.DS) [Total: 5]
- [cs.GT](#cs.GT) [Total: 3]
- [cs.IR](#cs.IR) [Total: 11]
- [cs.LG](#cs.LG) [Total: 75]
- [cs.NE](#cs.NE) [Total: 1]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.SE](#cs.SE) [Total: 6]
- [q-fin.PM](#q-fin.PM) [Total: 2]
- [q-fin.RM](#q-fin.RM) [Total: 1]
- [q-fin.ST](#q-fin.ST) [Total: 1]
- [stat.ML](#stat.ML) [Total: 5]
- [stat.CO](#stat.CO) [Total: 1]
- [stat.ME](#stat.ME) [Total: 2]
- [physics.data-an](#physics.data-an) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [cs.RO](#cs.RO) [Total: 6]
- [cs.CV](#cs.CV) [Total: 31]
- [cs.CY](#cs.CY) [Total: 2]
- [q-bio.BM](#q-bio.BM) [Total: 2]
- [math.DS](#math.DS) [Total: 1]
- [math.NA](#math.NA) [Total: 1]
- [q-bio.OT](#q-bio.OT) [Total: 1]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.CR](#cs.CR) [Total: 13]
- [cs.IT](#cs.IT) [Total: 1]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]
- [cs.SD](#cs.SD) [Total: 4]
- [astro-ph.EP](#astro-ph.EP) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [econ.GN](#econ.GN) [Total: 1]
- [eess.IV](#eess.IV) [Total: 1]
- [q-bio.MN](#q-bio.MN) [Total: 1]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [q-fin.PR](#q-fin.PR) [Total: 1]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.SI](#cs.SI) [Total: 2]
- [quant-ph](#quant-ph) [Total: 4]
- [cs.HC](#cs.HC) [Total: 4]
- [q-bio.NC](#q-bio.NC) [Total: 2]
- [physics.comp-ph](#physics.comp-ph) [Total: 2]
- [math.ST](#math.ST) [Total: 2]
- [cs.CL](#cs.CL) [Total: 40]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [A Survey of Optimization Modeling Meets LLMs: Progress and Future Directions](https://arxiv.org/abs/2508.10047)
*Ziyang Xiao,Jingrong Xie,Lilin Xu,Shisi Guan,Jingyan Zhu,Xiongwei Han,Xiaojin Fu,WingYin Yu,Han Wu,Wei Shi,Qingcan Kang,Jiahui Duan,Tao Zhong,Mingxuan Yuan,Jia Zeng,Yuan Wang,Gang Chen,Dongxiang Zhang*

Main category: cs.AI

TL;DR: 本文全面回顾大语言模型在优化建模自动化方面进展，分析基准数据集质量，清理数据集并构建新排行榜，搭建在线门户，指出当前方法局限并展望未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 优化建模需运筹学专业知识，大语言模型带来自动化数学建模新机遇，因此进行相关进展综述。

Method: 对涵盖基础模型数据合成与微调、推理框架、基准数据集和性能评估等整个技术栈的最新进展进行全面回顾，分析基准数据集质量，清理数据集并构建新排行榜，搭建在线门户。

Result: 发现基准数据集错误率高，清理数据集并构建新排行榜，搭建了整合资源的在线门户。

Conclusion: 指出当前方法存在局限性，并给出未来研究机会。

Abstract: By virtue of its great utility in solving real-world problems, optimization
modeling has been widely employed for optimal decision-making across various
sectors, but it requires substantial expertise from operations research
professionals. With the advent of large language models (LLMs), new
opportunities have emerged to automate the procedure of mathematical modeling.
This survey presents a comprehensive and timely review of recent advancements
that cover the entire technical stack, including data synthesis and fine-tuning
for the base model, inference frameworks, benchmark datasets, and performance
evaluation. In addition, we conducted an in-depth analysis on the quality of
benchmark datasets, which was found to have a surprisingly high error rate. We
cleaned the datasets and constructed a new leaderboard with fair performance
evaluation in terms of base LLM model and datasets. We also build an online
portal that integrates resources of cleaned datasets, code and paper repository
to benefit the community. Finally, we identify limitations in current
methodologies and outline future research opportunities.

</details>


### [2] [Amazon Nova AI Challenge -- Trusted AI: Advancing secure, AI-assisted software development](https://arxiv.org/abs/2508.10108)
*Sattvik Sahai,Prasoon Goyal,Michael Johnston,Anna Gottardi,Yao Lu,Lucy Hu,Luke Dai,Shaohua Liu,Samyuth Sagi,Hangjie Shi,Desheng Zhang,Lavina Vaz,Leslie Ball,Maureen Murray,Rahul Gupta,Shankar Ananthakrishna*

Main category: cs.AI

TL;DR: 亚马逊发起Nova AI挑战赛应对软件开发中AI安全挑战，各团队发展先进技术，凸显协作提升AI安全。


<details>
  <summary>Details</summary>
Motivation: 解决软件开发中AI系统安全保障的重大挑战。

Method: 举办全球竞赛，10支大学团队参赛，5支开发自动化红队机器人，5支创建安全AI助手，通过对抗赛评估方法，提供高质量标注数据。亚马逊团队进行科研和工程投入。

Result: 各团队开发出先进技术，在推理安全对齐、模型防护栏、多轮越狱攻击、大语言模型探测等方面有新方法。

Conclusion: 这种协作努力有助于提升AI安全标准。

Abstract: AI systems for software development are rapidly gaining prominence, yet
significant challenges remain in ensuring their safety. To address this, Amazon
launched the Trusted AI track of the Amazon Nova AI Challenge, a global
competition among 10 university teams to drive advances in secure AI. In the
challenge, five teams focus on developing automated red teaming bots, while the
other five create safe AI assistants. This challenge provides teams with a
unique platform to evaluate automated red-teaming and safety alignment methods
through head-to-head adversarial tournaments where red teams have multi-turn
conversations with the competing AI coding assistants to test their safety
alignment. Along with this, the challenge provides teams with a feed of high
quality annotated data to fuel iterative improvement. Throughout the challenge,
teams developed state-of-the-art techniques, introducing novel approaches in
reasoning-based safety alignment, robust model guardrails, multi-turn
jail-breaking, and efficient probing of large language models (LLMs). To
support these efforts, the Amazon Nova AI Challenge team made substantial
scientific and engineering investments, including building a custom baseline
coding specialist model for the challenge from scratch, developing a tournament
orchestration service, and creating an evaluation harness. This paper outlines
the advancements made by university teams and the Amazon Nova AI Challenge team
in addressing the safety challenges of AI for software development,
highlighting this collaborative effort to raise the bar for AI safety.

</details>


### [3] [MCP-Orchestrated Multi-Agent System for Automated Disinformation Detection](https://arxiv.org/abs/2508.10143)
*Alexandru-Andrei Avram,Adrian Groza,Alexandru Lecu*

Main category: cs.AI

TL;DR: 本文提出多智能体系统用关系提取检测新闻文章虚假信息，表现优异且架构可扩展。


<details>
  <summary>Details</summary>
Motivation: 数字平台虚假信息传播对信息完整性造成挑战，需有效检测方法。

Method: 提出Agentic AI系统，含机器学习、维基知识检查、连贯性检测、网页数据解析四个智能体，通过MCP编排。

Result: 多智能体集成准确率达95.3%，F1分数0.964，优于个体智能体和传统方法；加权聚合方法更优。

Conclusion: 系统架构模块化，易扩展且能保留决策过程细节。

Abstract: The large spread of disinformation across digital platforms creates
significant challenges to information integrity. This paper presents a
multi-agent system that uses relation extraction to detect disinformation in
news articles, focusing on titles and short text snippets. The proposed Agentic
AI system combines four agents: (i) a machine learning agent (logistic
regression), (ii) a Wikipedia knowledge check agent (which relies on named
entity recognition), (iii) a coherence detection agent (using LLM prompt
engineering), and (iv) a web-scraped data analyzer that extracts relational
triplets for fact checking. The system is orchestrated via the Model Context
Protocol (MCP), offering shared context and live learning across components.
Results demonstrate that the multi-agent ensemble achieves 95.3% accuracy with
an F1 score of 0.964, significantly outperforming individual agents and
traditional approaches. The weighted aggregation method, mathematically derived
from individual agent misclassification rates, proves superior to algorithmic
threshold optimization. The modular architecture makes the system easily
scalable, while also maintaining details of the decision processes.

</details>


### [4] [Agentic AI Frameworks: Architectures, Protocols, and Design Challenges](https://arxiv.org/abs/2508.10146)
*Hana Derouiche,Zaki Brahmi,Haithem Mazeni*

Main category: cs.AI

TL;DR: 本文系统回顾和比较分析主要的Agentic AI框架，分析通信协议，建立分类法并提出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 大语言模型推动Agentic AI发展，需对其框架进行系统评估，识别领域局限、趋势和挑战。

Method: 对多个Agentic AI框架进行系统回顾和比较分析，深入分析通信协议。

Result: 建立了Agentic AI系统的基础分类法。

Conclusion: 提出了增强可扩展性、鲁棒性和互操作性的未来研究方向，为推进下一代自主AI系统提供参考。

Abstract: The emergence of Large Language Models (LLMs) has ushered in a transformative
paradigm in artificial intelligence, Agentic AI, where intelligent agents
exhibit goal-directed autonomy, contextual reasoning, and dynamic multi-agent
coordination. This paper provides a systematic review and comparative analysis
of leading Agentic AI frameworks, including CrewAI, LangGraph, AutoGen,
Semantic Kernel, Agno, Google ADK, and MetaGPT, evaluating their architectural
principles, communication mechanisms, memory management, safety guardrails, and
alignment with service-oriented computing paradigms. Furthermore, we identify
key limitations, emerging trends, and open challenges in the field. To address
the issue of agent communication, we conduct an in-depth analysis of protocols
such as the Contract Net Protocol (CNP), Agent-to-Agent (A2A), Agent Network
Protocol (ANP), and Agora. Our findings not only establish a foundational
taxonomy for Agentic AI systems but also propose future research directions to
enhance scalability, robustness, and interoperability. This work serves as a
comprehensive reference for researchers and practitioners working to advance
the next generation of autonomous AI systems.

</details>


### [5] [Improving and Evaluating Open Deep Research Agents](https://arxiv.org/abs/2508.10152)
*Doaa Allabadi,Kyle Bradbury,Jordan M. Malof*

Main category: cs.AI

TL;DR: 文章聚焦于深度研究代理（DRAs），使用BrowseComp-Small基准比较开源ODR和两个专有系统，初始三者准确率为0%，对ODR改进得到ODR+，成功率达10%。


<details>
  <summary>Details</summary>
Motivation: 当前深度研究代理（DRAs）研究多为专有闭源系统，缺乏合适的开源系统评估基准。

Method: 将BrowseComp基准改编为更适合学术实验室的BrowseComp-Small基准，对三个系统进行测试，并对ODR提出三项改进。

Result: 三个系统初始在测试集60个问题上准确率为0%，改进后的ODR+在BC - Small上成功率达10%。

Conclusion: 对ODR的三项改进都有助于ODR+的成功，ODR+在闭源和开源系统中达到了最先进的水平。

Abstract: We focus here on Deep Research Agents (DRAs), which are systems that can take
a natural language prompt from a user, and then autonomously search for, and
utilize, internet-based content to address the prompt. Recent DRAs have
demonstrated impressive capabilities on public benchmarks however, recent
research largely involves proprietary closed-source systems. At the time of
this work, we only found one open-source DRA, termed Open Deep Research (ODR).
In this work we adapt the challenging recent BrowseComp benchmark to compare
ODR to existing proprietary systems. We propose BrowseComp-Small (BC-Small),
comprising a subset of BrowseComp, as a more computationally-tractable DRA
benchmark for academic labs. We benchmark ODR and two other proprietary systems
on BC-Small: one system from Anthropic and one system from Google. We find that
all three systems achieve 0% accuracy on the test set of 60 questions. We
introduce three strategic improvements to ODR, resulting in the ODR+ model,
which achieves a state-of-the-art 10% success rate on BC-Small among both
closed-source and open-source systems. We report ablation studies indicating
that all three of our improvements contributed to the success of ODR+.

</details>


### [6] [Pruning Long Chain-of-Thought of Large Reasoning Models via Small-Scale Preference Optimization](https://arxiv.org/abs/2508.10164)
*Bin Hong,Jiayu Liu,Zhenya Huang,Kai Zhang,Mengdi Zhang*

Main category: cs.AI

TL;DR: 本文研究减少大推理模型生成长度的高效方法，提出LCPO，实验表明该方法能显著减少输出长度并维持推理性能。


<details>
  <summary>Details</summary>
Motivation: 大推理模型长输出增加计算成本且可能过度思考，当前高效推理方法存在问题，需研究减少生成长度的高效方法。

Method: 分析生成路径分布，通过难度估计过滤生成轨迹，分析不同偏好优化方法目标的收敛行为，提出LCPO平衡与NLL损失相关的隐式奖励。

Result: 所提方法在多个基准测试中平均输出长度显著减少超50%，且维持推理性能。

Conclusion: 工作凸显了计算高效方法引导大推理模型进行高效推理的潜力。

Abstract: Recent advances in Large Reasoning Models (LRMs) have demonstrated strong
performance on complex tasks through long Chain-of-Thought (CoT) reasoning.
However, their lengthy outputs increase computational costs and may lead to
overthinking, raising challenges in balancing reasoning effectiveness and
efficiency. Current methods for efficient reasoning often compromise reasoning
quality or require extensive resources. This paper investigates efficient
methods to reduce the generation length of LRMs. We analyze generation path
distributions and filter generated trajectories through difficulty estimation.
Subsequently, we analyze the convergence behaviors of the objectives of various
preference optimization methods under a Bradley-Terry loss based framework.
Based on the analysis, we propose Length Controlled Preference Optimization
(LCPO) that directly balances the implicit reward related to NLL loss. LCPO can
effectively learn length preference with limited data and training. Extensive
experiments demonstrate that our approach significantly reduces the average
output length by over 50\% across multiple benchmarks while maintaining the
reasoning performance. Our work highlights the potential for computationally
efficient approaches in guiding LRMs toward efficient reasoning.

</details>


### [7] [KompeteAI: Accelerated Autonomous Multi-Agent System for End-to-End Pipeline Generation for Machine Learning Problems](https://arxiv.org/abs/2508.10177)
*Stepan Kulibaba,Artem Dzhalilov,Roman Pakhomov,Oleg Svidchenko,Alexander Gasnikov,Aleksei Shpilman*

Main category: cs.AI

TL;DR: 现有基于大语言模型的AutoML系统有局限，提出KompeteAI框架，通过合并候选方案、集成RAG拓展假设空间、解决执行瓶颈，加速评估6.9倍，在基准测试中表现出色，还提出Kompete - bench。


<details>
  <summary>Details</summary>
Motivation: 解决现有基于大语言模型的AutoML系统探索策略受限和执行瓶颈问题。

Method: 引入合并阶段组合候选方案；集成RAG拓展假设空间；用预测评分模型和加速调试方法解决执行瓶颈。

Result: 加速管道评估6.9倍；在MLE - Bench上平均比领先方法高3%；在Kompete - bench上达最优。

Conclusion: KompeteAI能有效解决现有AutoML系统的问题，在基准测试中表现优异。

Abstract: Recent Large Language Model (LLM)-based AutoML systems demonstrate impressive
capabilities but face significant limitations such as constrained exploration
strategies and a severe execution bottleneck. Exploration is hindered by
one-shot methods lacking diversity and Monte Carlo Tree Search (MCTS)
approaches that fail to recombine strong partial solutions. The execution
bottleneck arises from lengthy code validation cycles that stifle iterative
refinement. To overcome these challenges, we introduce KompeteAI, a novel
AutoML framework with dynamic solution space exploration. Unlike previous MCTS
methods that treat ideas in isolation, KompeteAI introduces a merging stage
that composes top candidates. We further expand the hypothesis space by
integrating Retrieval-Augmented Generation (RAG), sourcing ideas from Kaggle
notebooks and arXiv papers to incorporate real-world strategies. KompeteAI also
addresses the execution bottleneck via a predictive scoring model and an
accelerated debugging method, assessing solution potential using early stage
metrics to avoid costly full-code execution. This approach accelerates pipeline
evaluation 6.9 times. KompeteAI outperforms leading methods (e.g., RD-agent,
AIDE, and Ml-Master) by an average of 3\% on the primary AutoML benchmark,
MLE-Bench. Additionally, we propose Kompete-bench to address limitations in
MLE-Bench, where KompeteAI also achieves state-of-the-art results

</details>


### [8] [Reverse Physician-AI Relationship: Full-process Clinical Diagnosis Driven by a Large Language Model](https://arxiv.org/abs/2508.10492)
*Shicheng Xu,Xin Huang,Zihao Wei,Liang Pang,Huawei Shen,Xueqi Cheng*

Main category: cs.AI

TL;DR: 本文提出将AI作为主要诊断主导者的范式转变，介绍DxDirector - 7B模型，其在全流程诊断评估中表现出色，可大幅减轻医生工作量。


<details>
  <summary>Details</summary>
Motivation: 当前AI在临床诊断中主要作为医生助手，无法从模糊主诉驱动全流程诊断，限制了减轻医生工作量和提高诊断效率的能力。

Method: 提出将AI和医生关系反转的范式转变，推出具有深度思考能力的DxDirector - 7B模型，并建立误诊责任框架。

Result: DxDirector - 7B在全流程诊断评估中诊断准确率显著提高，大幅减轻医生工作量，多科室和任务细粒度分析验证其有效性，专家评估认为可替代医学专家。

Conclusion: AI可从传统的医生助手转变为驱动全流程诊断，为临床诊断提供高效准确的解决方案。

Abstract: Full-process clinical diagnosis in the real world encompasses the entire
diagnostic workflow that begins with only an ambiguous chief complaint. While
artificial intelligence (AI), particularly large language models (LLMs), is
transforming clinical diagnosis, its role remains largely as an assistant to
physicians. This AI-assisted working pattern makes AI can only answer specific
medical questions at certain parts within the diagnostic process, but lack the
ability to drive the entire diagnostic process starting from an ambiguous
complaint, which still relies heavily on human physicians. This gap limits AI's
ability to fully reduce physicians' workload and enhance diagnostic efficiency.
To address this, we propose a paradigm shift that reverses the relationship
between physicians and AI: repositioning AI as the primary director, with
physicians serving as its assistants. So we present DxDirector-7B, an LLM
endowed with advanced deep thinking capabilities, enabling it to drive the
full-process diagnosis with minimal physician involvement. Furthermore,
DxDirector-7B establishes a robust accountability framework for misdiagnoses,
delineating responsibility between AI and human physicians. In evaluations
across rare, complex, and real-world cases under full-process diagnosis
setting, DxDirector-7B not only achieves significant superior diagnostic
accuracy but also substantially reduces physician workload than
state-of-the-art medical LLMs as well as general-purpose LLMs. Fine-grained
analyses across multiple clinical departments and tasks validate its efficacy,
with expert evaluations indicating its potential to serve as a viable
substitute for medical specialists. These findings mark a new era where AI,
traditionally a physicians' assistant, now drives the entire diagnostic process
to drastically reduce physicians' workload, indicating an efficient and
accurate diagnostic solution.

</details>


### [9] [Extending the Entropic Potential of Events for Uncertainty Quantification and Decision-Making in Artificial Intelligence](https://arxiv.org/abs/2508.10241)
*Mark Zilberman*

Main category: cs.AI

TL;DR: 本文展示事件熵势概念可提升AI中不确定性量化、决策和可解释性，适配AI框架，探索多领域应用，为管理AI不确定性提供新方法。


<details>
  <summary>Details</summary>
Motivation: 提升人工智能中不确定性量化、决策和可解释性。

Method: 将物理学中的熵势概念适配到AI，引入以事件为中心的度量，形式化原始和AI调整后的定义。

Result: 探索了在政策评估、内在奖励设计等多领域的应用，用概念示例说明其在不同场景的使用，并讨论了复杂AI模型计算的实际考虑。

Conclusion: 熵势框架为管理AI不确定性提供了理论扎实、可解释且通用的方法，桥接了多学科原则。

Abstract: This work demonstrates how the concept of the entropic potential of events --
a parameter quantifying the influence of discrete events on the expected future
entropy of a system -- can enhance uncertainty quantification, decision-making,
and interpretability in artificial intelligence (AI). Building on its original
formulation in physics, the framework is adapted for AI by introducing an
event-centric measure that captures how actions, observations, or other
discrete occurrences impact uncertainty at future time horizons. Both the
original and AI-adjusted definitions of entropic potential are formalized, with
the latter emphasizing conditional expectations to account for counterfactual
scenarios. Applications are explored in policy evaluation, intrinsic reward
design, explainable AI, and anomaly detection, highlighting the metric's
potential to unify and strengthen uncertainty modeling in intelligent systems.
Conceptual examples illustrate its use in reinforcement learning, Bayesian
inference, and anomaly detection, while practical considerations for
computation in complex AI models are discussed. The entropic potential
framework offers a theoretically grounded, interpretable, and versatile
approach to managing uncertainty in AI, bridging principles from
thermodynamics, information theory, and machine learning.

</details>


### [10] [Why Cannot Large Language Models Ever Make True Correct Reasoning?](https://arxiv.org/abs/2508.10265)
*Jingde Cheng*

Main category: cs.AI

TL;DR: 文章认为大语言模型所谓‘理解能力’和‘推理能力’是概念模糊者的错觉，因其工作原理的本质限制，无法拥有真正的正确推理能力。


<details>
  <summary>Details</summary>
Motivation: 纠正许多人认为大语言模型具有‘理解能力’和‘推理能力’的观点。

Method: 从大语言模型工作原理的本质限制角度进行阐述。

Result: 说明大语言模型无法拥有真正的正确推理能力。

Conclusion: 大语言模型不能具备真正的理解和推理能力。

Abstract: Recently, with the application progress of AIGC tools based on large language
models (LLMs), led by ChatGPT, many AI experts and more non-professionals are
trumpeting the "understanding ability" and "reasoning ability" of the LLMs. The
present author considers that the so-called "understanding ability" and
"reasoning ability" of LLMs are just illusions of those people who with vague
concepts. In fact, the LLMs can never have the true understanding ability and
true reasoning ability. This paper intents to explain that, because the
essential limitations of their working principle, the LLMs can never have the
ability of true correct reasoning.

</details>


### [11] [CVCM Track Circuits Pre-emptive Failure Diagnostics for Predictive Maintenance Using Deep Neural Networks](https://arxiv.org/abs/2508.09054)
*Debdeep Mukherjee,Eduardo Di Santi,Clément Lefebvre,Nenad Mijatovic,Victor Martin,Thierry Josse,Jonathan Brown,Kenza Saiah*

Main category: cs.AI

TL;DR: 本文提出基于深度神经网络的预测性维护框架，用于CVCM故障类型早期识别，在10个案例验证中表现出色，可提高铁路运营可靠性。


<details>
  <summary>Details</summary>
Motivation: CVCM故障会引发级联中断，传统方法难早期检测故障，早期识别故障类型对铁路运营至关重要。

Method: 利用深度神经网络构建预测性维护框架，通过保形预测提供不确定性估计。

Result: 在10个CVCM故障案例中验证，符合ISO - 17359标准，整体准确率达99.31%，在异常发生1%内检测，保形预测置信度达99%。

Conclusion: 该方法可扩展适应其他轨道电路和铁路系统，能增强铁路运营可靠性。

Abstract: Track circuits are critical for railway operations, acting as the main
signalling sub-system to locate trains. Continuous Variable Current Modulation
(CVCM) is one such technology. Like any field-deployed, safety-critical asset,
it can fail, triggering cascading disruptions. Many failures originate as
subtle anomalies that evolve over time, often not visually apparent in
monitored signals. Conventional approaches, which rely on clear signal changes,
struggle to detect them early. Early identification of failure types is
essential to improve maintenance planning, minimising downtime and revenue
loss. Leveraging deep neural networks, we propose a predictive maintenance
framework that classifies anomalies well before they escalate into failures.
Validated on 10 CVCM failure cases across different installations, the method
is ISO-17359 compliant and outperforms conventional techniques, achieving
99.31% overall accuracy with detection within 1% of anomaly onset. Through
conformal prediction, we provide uncertainty estimates, reaching 99% confidence
with consistent coverage across classes. Given CVCMs global deployment, the
approach is scalable and adaptable to other track circuits and railway systems,
enhancing operational reliability.

</details>


### [12] [Promoting Efficient Reasoning with Verifiable Stepwise Reward](https://arxiv.org/abs/2508.10293)
*Chuhuai Yue,Chengqi Dong,Yinan Gao,Hang He,Jiajun Chai,Guojun Yin,Wei Lin*

Main category: cs.AI

TL;DR: 提出基于规则的可验证逐步奖励机制（VSRM）解决大推理模型过度思考问题，在数学推理基准测试中平衡效率与准确性。


<details>
  <summary>Details</summary>
Motivation: 大推理模型存在过度思考问题，现有高效推理方法灵活性和可靠性有限，需解决过度思考以提升效率。

Method: 提出基于规则的可验证逐步奖励机制（VSRM），根据推理轨迹中间状态表现分配奖励，并将其与PPO和Reinforce++集成。

Result: 在标准数学推理基准测试中，实现输出长度大幅减少，保持原有推理性能，有效抑制无效步骤，鼓励有效推理。

Conclusion: 该方法能从根本上缓解大推理模型的过度思考问题，平衡效率和准确性。

Abstract: Large reasoning models (LRMs) have recently achieved significant progress in
complex reasoning tasks, aided by reinforcement learning with verifiable
rewards. However, LRMs often suffer from overthinking, expending excessive
computation on simple problems and reducing efficiency. Existing efficient
reasoning methods typically require accurate task assessment to preset token
budgets or select reasoning modes, which limits their flexibility and
reliability. In this work, we revisit the essence of overthinking and identify
that encouraging effective steps while penalizing ineffective ones is key to
its solution. To this end, we propose a novel rule-based verifiable stepwise
reward mechanism (VSRM), which assigns rewards based on the performance of
intermediate states in the reasoning trajectory. This approach is intuitive and
naturally fits the step-by-step nature of reasoning tasks. We conduct extensive
experiments on standard mathematical reasoning benchmarks, including AIME24 and
AIME25, by integrating VSRM with PPO and Reinforce++. Results show that our
method achieves substantial output length reduction while maintaining original
reasoning performance, striking an optimal balance between efficiency and
accuracy. Further analysis of overthinking frequency and pass@k score before
and after training demonstrates that our approach in deed effectively
suppresses ineffective steps and encourages effective reasoning, fundamentally
alleviating the overthinking problem. All code will be released upon
acceptance.

</details>


### [13] [A Curriculum Learning Approach to Reinforcement Learning: Leveraging RAG for Multimodal Question Answering](https://arxiv.org/abs/2508.10337)
*Chenliang Zhang,Lin Wang,Yuanyuan Lu,Yusheng Qi,Kexin Wang,Peixu Hou,Wenshi Chen*

Main category: cs.AI

TL;DR: 本文介绍大众点评信任安全团队针对META CRAG - MM挑战的解决方案，在任务1获第一名、任务3获第三名。


<details>
  <summary>Details</summary>
Motivation: 参与META CRAG - MM挑战，构建能进行多模态多轮问答的综合检索增强生成系统。

Method: 任务1基于视觉大语言模型，用GPT - 4.1知识蒸馏进行监督微调，应用课程学习策略引导强化学习；任务2和3借助网络搜索API整合外部知识。

Result: 任务1以52.38%的显著优势获第一名，任务3获第三名。

Conclusion: 在训练流程中整合课程学习和强化学习有效。

Abstract: This paper describes the solutions of the Dianping-Trust-Safety team for the
META CRAG-MM challenge. The challenge requires building a comprehensive
retrieval-augmented generation system capable for multi-modal multi-turn
question answering. The competition consists of three tasks: (1) answering
questions using structured data retrieved from an image-based mock knowledge
graph, (2) synthesizing information from both knowledge graphs and web search
results, and (3) handling multi-turn conversations that require context
understanding and information aggregation from multiple sources. For Task 1,
our solution is based on the vision large language model, enhanced by
supervised fine-tuning with knowledge distilled from GPT-4.1. We further
applied curriculum learning strategies to guide reinforcement learning,
resulting in improved answer accuracy and reduced hallucination. For Task 2 and
Task 3, we additionally leveraged web search APIs to incorporate external
knowledge, enabling the system to better handle complex queries and multi-turn
conversations. Our approach achieved 1st place in Task 1 with a significant
lead of 52.38\%, and 3rd place in Task 3, demonstrating the effectiveness of
the integration of curriculum learning with reinforcement learning in our
training pipeline.

</details>


### [14] [Multi-Agent Trust Region Policy Optimisation: A Joint Constraint Approach](https://arxiv.org/abs/2508.10340)
*Chak Lam Shek,Guangyao Shi,Pratap Tokekar*

Main category: cs.AI

TL;DR: 提出HATRPO - W和HATRPO - G两种分配KL散度阈值的方法，提升HATRPO在多智能体强化学习中的性能。


<details>
  <summary>Details</summary>
Motivation: 解决HATRPO中为每个智能体分配相同KL阈值导致训练慢和局部最优的问题。

Method: 提出HATRPO - W（基于KKT优化阈值分配）和HATRPO - G（基于改进 - 散度比优先排序智能体）两种方法。

Result: 显著提升HATRPO性能，在不同基准测试中实现更快收敛和更高最终奖励，HATRPO - W和HATRPO - G最终性能提升超22.5%，HATRPO - W学习动态更稳定。

Conclusion: 所提方法能在异构智能体环境中实现更灵活有效的学习。

Abstract: Multi-agent reinforcement learning (MARL) requires coordinated and stable
policy updates among interacting agents. Heterogeneous-Agent Trust Region
Policy Optimization (HATRPO) enforces per-agent trust region constraints using
Kullback-Leibler (KL) divergence to stabilize training. However, assigning each
agent the same KL threshold can lead to slow and locally optimal updates,
especially in heterogeneous settings. To address this limitation, we propose
two approaches for allocating the KL divergence threshold across agents:
HATRPO-W, a Karush-Kuhn-Tucker-based (KKT-based) method that optimizes
threshold assignment under global KL constraints, and HATRPO-G, a greedy
algorithm that prioritizes agents based on improvement-to-divergence ratio. By
connecting sequential policy optimization with constrained threshold
scheduling, our approach enables more flexible and effective learning in
heterogeneous-agent settings. Experimental results demonstrate that our methods
significantly boost the performance of HATRPO, achieving faster convergence and
higher final rewards across diverse MARL benchmarks. Specifically, HATRPO-W and
HATRPO-G achieve comparable improvements in final performance, each exceeding
22.5%. Notably, HATRPO-W also demonstrates more stable learning dynamics, as
reflected by its lower variance.

</details>


### [15] [What to Ask Next? Probing the Imaginative Reasoning of LLMs with TurtleSoup Puzzles](https://arxiv.org/abs/2508.10358)
*Mengtao Zhou,Sifan Wu,Huan Zhang,Qi Sima,Bang Liu*

Main category: cs.AI

TL;DR: 本文针对大语言模型想象力推理能力，基于“海龟汤”游戏构建研究框架，开展实验揭示其能力局限，为相关研究奠基。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试无法捕捉想象力推理过程的动态探索性，需新的研究方法。

Method: 引入基于“海龟汤”游戏的研究框架，包括TurtleSoup - Bench基准、Mosaic - Agent代理和多维评估协议。

Result: 实验揭示领先大语言模型存在能力局限、常见失败模式，与人类表现有显著差距。

Conclusion: 研究为大语言模型想象力推理提供新见解，为探索性智能体行为研究奠定基础。

Abstract: We investigate the capacity of Large Language Models (LLMs) for imaginative
reasoning--the proactive construction, testing, and revision of hypotheses in
information-sparse environments. Existing benchmarks, often static or focused
on social deduction, fail to capture the dynamic, exploratory nature of this
reasoning process. To address this gap, we introduce a comprehensive research
framework based on the classic "Turtle Soup" game, integrating a benchmark, an
agent, and an evaluation protocol. We present TurtleSoup-Bench, the first
large-scale, bilingual, interactive benchmark for imaginative reasoning,
comprising 800 turtle soup puzzles sourced from both the Internet and expert
authors. We also propose Mosaic-Agent, a novel agent designed to assess LLMs'
performance in this setting. To evaluate reasoning quality, we develop a
multi-dimensional protocol measuring logical consistency, detail completion,
and conclusion alignment. Experiments with leading LLMs reveal clear capability
limits, common failure patterns, and a significant performance gap compared to
humans. Our work offers new insights into LLMs' imaginative reasoning and
establishes a foundation for future research on exploratory agent behavior.

</details>


### [16] [LeanRAG: Knowledge-Graph-Based Generation with Semantic Aggregation and Hierarchical Retrieval](https://arxiv.org/abs/2508.10391)
*Yaoze Zhang,Rong Wu,Pinlong Cai,Xiaoman Wang,Guohang Yan,Song Mao,Ding Wang,Botian Shi*

Main category: cs.AI

TL;DR: 文章提出LeanRAG框架解决现有知识图RAG方法问题，实验表明其能提升响应质量并减少检索冗余。


<details>
  <summary>Details</summary>
Motivation: 现有知识图RAG方法存在高层概念总结缺乏关联、检索过程无结构意识等问题，需改进。

Method: 引入LeanRAG框架，先通过语义聚合算法构建语义网络，再采用自下而上、结构引导的检索策略。

Result: 在四个不同领域的问答基准测试中，LeanRAG显著优于现有方法，响应质量提升，检索冗余减少46%。

Conclusion: LeanRAG框架有效，能解决现有知识图RAG方法的局限。

Abstract: Retrieval-Augmented Generation (RAG) plays a crucial role in grounding Large
Language Models by leveraging external knowledge, whereas the effectiveness is
often compromised by the retrieval of contextually flawed or incomplete
information. To address this, knowledge graph-based RAG methods have evolved
towards hierarchical structures, organizing knowledge into multi-level
summaries. However, these approaches still suffer from two critical,
unaddressed challenges: high-level conceptual summaries exist as disconnected
``semantic islands'', lacking the explicit relations needed for cross-community
reasoning; and the retrieval process itself remains structurally unaware, often
degenerating into an inefficient flat search that fails to exploit the graph's
rich topology. To overcome these limitations, we introduce LeanRAG, a framework
that features a deeply collaborative design combining knowledge aggregation and
retrieval strategies. LeanRAG first employs a novel semantic aggregation
algorithm that forms entity clusters and constructs new explicit relations
among aggregation-level summaries, creating a fully navigable semantic network.
Then, a bottom-up, structure-guided retrieval strategy anchors queries to the
most relevant fine-grained entities and then systematically traverses the
graph's semantic pathways to gather concise yet contextually comprehensive
evidence sets. The LeanRAG can mitigate the substantial overhead associated
with path retrieval on graphs and minimizes redundant information retrieval.
Extensive experiments on four challenging QA benchmarks with different domains
demonstrate that LeanRAG significantly outperforming existing methods in
response quality while reducing 46\% retrieval redundancy. Code is available
at: https://github.com/RaZzzyz/LeanRAG

</details>


### [17] [HiRef: Leveraging Hierarchical Ontology and Network Refinement for Robust Medication Recommendation](https://arxiv.org/abs/2508.10425)
*Yan Ting Chok,Soyon Park,Seungheun Baek,Hajung Kim,Junhyun Lee,Jaewoo Kang*

Main category: cs.AI

TL;DR: 本文提出HiRef框架解决电子病历数据在药物推荐任务中的问题，在基准测试中表现良好且对未见代码有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现实世界电子病历数据存在罕见医疗实体和记录不完整问题，数据驱动模型在缺失或新情况下泛化能力差，需要解决这些问题以更好地进行药物推荐。

Method: 提出HiRef框架，结合医学本体的层次语义和电子病历的共现模式；将本体实体嵌入双曲空间；引入先验引导的稀疏正则化方案优化共现图。

Result: 模型在EHR基准测试（MIMIC - III和MIMIC - IV）中表现出色，在模拟未见代码设置下保持高精度。

Conclusion: 大量实验和消融研究表明HiRef对未见医疗代码具有鲁棒性，通过对稀疏图结构和医学代码嵌入的深入分析得到支持。

Abstract: Medication recommendation is a crucial task for assisting physicians in
making timely decisions from longitudinal patient medical records. However,
real-world EHR data present significant challenges due to the presence of
rarely observed medical entities and incomplete records that may not fully
capture the clinical ground truth. While data-driven models trained on
longitudinal Electronic Health Records often achieve strong empirical
performance, they struggle to generalize under missing or novel conditions,
largely due to their reliance on observed co-occurrence patterns. To address
these issues, we propose Hierarchical Ontology and Network Refinement for
Robust Medication Recommendation (HiRef), a unified framework that combines two
complementary structures: (i) the hierarchical semantics encoded in curated
medical ontologies, and (ii) refined co-occurrence patterns derived from
real-world EHRs. We embed ontology entities in hyperbolic space, which
naturally captures tree-like relationships and enables knowledge transfer
through shared ancestors, thereby improving generalizability to unseen codes.
To further improve robustness, we introduce a prior-guided sparse
regularization scheme that refines the EHR co-occurrence graph by suppressing
spurious edges while preserving clinically meaningful associations. Our model
achieves strong performance on EHR benchmarks (MIMIC-III and MIMIC-IV) and
maintains high accuracy under simulated unseen-code settings. Extensive
experiments with comprehensive ablation studies demonstrate HiRef's resilience
to unseen medical codes, supported by in-depth analyses of the learned
sparsified graph structure and medical code embeddings.

</details>


### [18] [MM-Food-100K: A 100,000-Sample Multimodal Food Intelligence Dataset with Verifiable Provenance](https://arxiv.org/abs/2508.10429)
*Yi Dong,Yusuke Muraoka,Scott Shi,Yi Zhang*

Main category: cs.AI

TL;DR: 本文介绍了公共多模态食品智能数据集MM - Food - 100K，描述其收集方式、验证实用性并公开部分数据。


<details>
  <summary>Details</summary>
Motivation: 提供具有可验证来源的多模态食品智能数据集，助力食品相关研究和应用。

Method: 采用Codatta贡献模型从超87000名贡献者处收集数据，结合社区众包和AI辅助质量检查，通过微调大视觉语言模型验证数据集实用性。

Result: 微调大视觉语言模型在图像营养预测上比基线模型有稳定提升。

Conclusion: 发布MM - Food - 100K供免费使用，保留约90%数据用于商业用途并与贡献者分享收益。

Abstract: We present MM-Food-100K, a public 100,000-sample multimodal food intelligence
dataset with verifiable provenance. It is a curated approximately 10% open
subset of an original 1.2 million, quality-accepted corpus of food images
annotated for a wide range of information (such as dish name, region of
creation). The corpus was collected over six weeks from over 87,000
contributors using the Codatta contribution model, which combines community
sourcing with configurable AI-assisted quality checks; each submission is
linked to a wallet address in a secure off-chain ledger for traceability, with
a full on-chain protocol on the roadmap. We describe the schema, pipeline, and
QA, and validate utility by fine-tuning large vision-language models (ChatGPT
5, ChatGPT OSS, Qwen-Max) on image-based nutrition prediction. Fine-tuning
yields consistent gains over out-of-box baselines across standard metrics; we
report results primarily on the MM-Food-100K subset. We release MM-Food-100K
for publicly free access and retain approximately 90% for potential commercial
access with revenue sharing to contributors.

</details>


### [19] [We-Math 2.0: A Versatile MathBook System for Incentivizing Visual Mathematical Reasoning](https://arxiv.org/abs/2508.10433)
*Runqi Qiao,Qiuna Tan,Peiqing Yang,Yanzi Wang,Xiaowan Wang,Enhui Wan,Sitong Zhou,Guanting Dong,Yuchen Zeng,Yida Xu,Jie Wang,Chong Sun,Chen Li,Honggang Zhang*

Main category: cs.AI

TL;DR: 本文介绍We - Math 2.0系统提升多模态大语言模型数学推理能力，包含知识系统、数据集、训练框架和评估基准，实验效果良好。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型在复杂数学推理上有困难，且现有研究忽视全面知识驱动设计和以模型为中心的数据空间建模。

Method: 构建We - Math 2.0系统，包含MathBook知识系统、MathBook - Standard & Pro数据集、MathBook - RL训练框架和MathBookEval评估基准。

Result: MathBook - RL在四个常用基准测试中与现有基线表现相当，在MathBookEval上取得良好结果。

Conclusion: We - Math 2.0系统在提升多模态大语言模型数学推理能力上有良好的泛化性。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated impressive
capabilities across various tasks, but still struggle with complex mathematical
reasoning. Existing research primarily focuses on dataset construction and
method optimization, often overlooking two critical aspects: comprehensive
knowledge-driven design and model-centric data space modeling. In this paper,
we introduce We-Math 2.0, a unified system that integrates a structured
mathematical knowledge system, model-centric data space modeling, and a
reinforcement learning (RL)-based training paradigm to comprehensively enhance
the mathematical reasoning abilities of MLLMs. The key contributions of We-Math
2.0 are fourfold: (1) MathBook Knowledge System: We construct a five-level
hierarchical system encompassing 491 knowledge points and 1,819 fundamental
principles. (2) MathBook-Standard & Pro: We develop MathBook-Standard, a
dataset that ensures broad conceptual coverage and flexibility through dual
expansion. Additionally, we define a three-dimensional difficulty space and
generate 7 progressive variants per problem to build MathBook-Pro, a
challenging dataset for robust training. (3) MathBook-RL: We propose a
two-stage RL framework comprising: (i) Cold-Start Fine-tuning, which aligns the
model with knowledge-oriented chain-of-thought reasoning; and (ii) Progressive
Alignment RL, leveraging average-reward learning and dynamic data scheduling to
achieve progressive alignment across difficulty levels. (4) MathBookEval: We
introduce a comprehensive benchmark covering all 491 knowledge points with
diverse reasoning step distributions. Experimental results show that
MathBook-RL performs competitively with existing baselines on four widely-used
benchmarks and achieves strong results on MathBookEval, suggesting promising
generalization in mathematical reasoning.

</details>


### [20] [FIRESPARQL: A LLM-based Framework for SPARQL Query Generation over Scholarly Knowledge Graphs](https://arxiv.org/abs/2508.10467)
*Xueli Pan,Victor de Boer,Jacco van Ossenbruggen*

Main category: cs.AI

TL;DR: 论文针对大语言模型生成学术知识图谱SPARQL查询的问题，提出FIRESPARQL框架，实验表明微调效果最佳。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在生成学术知识图谱SPARQL查询时，因对特定内容和底层模式接触有限而存在结构和语义错误。

Method: 提出FIRESPARQL框架，以微调大语言模型为核心组件，结合检索增强生成和查询校正层，在SciQA基准上进行多种配置评估。

Result: 实验结果显示，微调在整体性能上表现最佳，测试集上查询准确率ROUGE - L达0.90，结果准确率RelaxedEM达0.85。

Conclusion: FIRESPARQL框架有效，微调配置能提升大语言模型生成SPARQL查询的性能。

Abstract: Question answering over Scholarly Knowledge Graphs (SKGs) remains a
challenging task due to the complexity of scholarly content and the intricate
structure of these graphs. Large Language Model (LLM) approaches could be used
to translate natural language questions (NLQs) into SPARQL queries; however,
these LLM-based approaches struggle with SPARQL query generation due to limited
exposure to SKG-specific content and the underlying schema. We identified two
main types of errors in the LLM-generated SPARQL queries: (i) structural
inconsistencies, such as missing or redundant triples in the queries, and (ii)
semantic inaccuracies, where incorrect entities or properties are shown in the
queries despite a correct query structure. To address these issues, we propose
FIRESPARQL, a modular framework that supports fine-tuned LLMs as a core
component, with optional context provided via retrieval-augmented generation
(RAG) and a SPARQL query correction layer. We evaluate the framework on the
SciQA Benchmark using various configurations (zero-shot, zero-shot with RAG,
one-shot, fine-tuning, and fine-tuning with RAG) and compare the performance
with baseline and state-of-the-art approaches. We measure query accuracy using
BLEU and ROUGE metrics, and query result accuracy using relaxed exact
match(RelaxedEM), with respect to the gold standards containing the NLQs,
SPARQL queries, and the results of the queries. Experimental results
demonstrate that fine-tuning achieves the highest overall performance, reaching
0.90 ROUGE-L for query accuracy and 0.85 RelaxedEM for result accuracy on the
test set.

</details>


### [21] [SEQ-GPT: LLM-assisted Spatial Query via Example](https://arxiv.org/abs/2508.10486)
*Ivan Khai Ze Lim,Ningyi Liao,Yiming Yang,Gerald Wei Yong Yip,Siqiang Luo*

Main category: cs.AI

TL;DR: 研究引入基于大语言模型的空间查询系统SEQ - GPT实现更通用的空间示例查询，还提出适配管道，提供端到端演示。


<details>
  <summary>Details</summary>
Motivation: 当代空间服务依赖用户查询，在执行复杂任务时用户体验有限，需研究更通用的多地点联合搜索。

Method: 引入SEQ - GPT系统，利用大语言模型的语言能力实现交互操作，提出适配管道通过对话合成和多模型合作使自然语言与结构化空间数据及查询对齐。

Result: 通过现实数据和应用场景为拓展空间搜索提供了端到端演示。

Conclusion: SEQ - GPT系统及适配管道有助于实现更通用的空间示例查询，拓宽空间搜索。

Abstract: Contemporary spatial services such as online maps predominantly rely on user
queries for location searches. However, the user experience is limited when
performing complex tasks, such as searching for a group of locations
simultaneously. In this study, we examine the extended scenario known as
Spatial Exemplar Query (SEQ), where multiple relevant locations are jointly
searched based on user-specified examples. We introduce SEQ-GPT, a spatial
query system powered by Large Language Models (LLMs) towards more versatile SEQ
search using natural language. The language capabilities of LLMs enable unique
interactive operations in the SEQ process, including asking users to clarify
query details and dynamically adjusting the search based on user feedback. We
also propose a tailored LLM adaptation pipeline that aligns natural language
with structured spatial data and queries through dialogue synthesis and
multi-model cooperation. SEQ-GPT offers an end-to-end demonstration for
broadening spatial search with realistic data and application scenarios.

</details>


### [22] [STEP: Stepwise Curriculum Learning for Context-Knowledge Fusion in Conversational Recommendation](https://arxiv.org/abs/2508.10669)
*Zhenye Yang,Jinpeng Chen,Huan Li,Xiongnan Jin,Xuanyang Li,Junwei Zhang,Hongbo Gao,Kaimin Wei,Senzhang Wang*

Main category: cs.AI

TL;DR: 本文提出基于预训练语言模型的对话推荐系统STEP，结合课程引导的上下文知识融合与轻量级特定任务提示调优，实验显示其在推荐精度和对话质量上优于主流方法。


<details>
  <summary>Details</summary>
Motivation: 现有对话推荐系统在捕捉用户偏好深层语义、对话上下文及整合外部知识图谱信息方面存在挑战，传统方法处理复杂语义关系能力不足。

Method: 提出STEP系统，用F - Former通过三阶段课程对齐对话上下文和知识图谱实体，用对话前缀和推荐前缀将融合表示注入冻结语言模型。

Result: 在两个公开数据集上，STEP在推荐精度和对话质量上优于主流方法。

Conclusion: STEP能有效解决现有对话推荐系统的问题，提高推荐和对话性能。

Abstract: Conversational recommender systems (CRSs) aim to proactively capture user
preferences through natural language dialogue and recommend high-quality items.
To achieve this, CRS gathers user preferences via a dialog module and builds
user profiles through a recommendation module to generate appropriate
recommendations. However, existing CRS faces challenges in capturing the deep
semantics of user preferences and dialogue context. In particular, the
efficient integration of external knowledge graph (KG) information into
dialogue generation and recommendation remains a pressing issue. Traditional
approaches typically combine KG information directly with dialogue content,
which often struggles with complex semantic relationships, resulting in
recommendations that may not align with user expectations.
  To address these challenges, we introduce STEP, a conversational recommender
centered on pre-trained language models that combines curriculum-guided
context-knowledge fusion with lightweight task-specific prompt tuning. At its
heart, an F-Former progressively aligns the dialogue context with
knowledge-graph entities through a three-stage curriculum, thus resolving
fine-grained semantic mismatches. The fused representation is then injected
into the frozen language model via two minimal yet adaptive prefix prompts: a
conversation prefix that steers response generation toward user intent and a
recommendation prefix that biases item ranking toward knowledge-consistent
candidates. This dual-prompt scheme allows the model to share cross-task
semantics while respecting the distinct objectives of dialogue and
recommendation. Experimental results show that STEP outperforms mainstream
methods in the precision of recommendation and dialogue quality in two public
datasets.

</details>


### [23] [PASS: Probabilistic Agentic Supernet Sampling for Interpretable and Adaptive Chest X-Ray Reasoning](https://arxiv.org/abs/2508.10501)
*Yushi Feng,Junye Du,Yingying Hong,Qifan Wang,Lequan Yu*

Main category: cs.AI

TL;DR: 提出PASS框架解决现有工具增强代理系统在胸部X光推理中的问题，设计三阶段训练过程和CAB - E基准，实验显示PASS表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有工具增强代理系统存在推理步骤黑盒、多模态集成差和代理管道效率低等问题，影响医疗决策信任和安全。

Method: 引入PASS框架，在多工具图上自适应采样代理工作流，利用任务条件分布选工具，设计三阶段训练程序，引入CAB - E基准。

Result: 实验表明PASS在多个指标上显著优于强基线，同时平衡计算成本。

Conclusion: PASS推动了医疗代理系统向可解释、自适应和多模态方向的范式转变。

Abstract: Existing tool-augmented agentic systems are limited in the real world by (i)
black-box reasoning steps that undermine trust of decision-making and pose
safety risks, (ii) poor multimodal integration, which is inherently critical
for healthcare tasks, and (iii) rigid and computationally inefficient agentic
pipelines. We introduce PASS (Probabilistic Agentic Supernet Sampling), the
first multimodal framework to address these challenges in the context of Chest
X-Ray (CXR) reasoning. PASS adaptively samples agentic workflows over a
multi-tool graph, yielding decision paths annotated with interpretable
probabilities. Given the complex CXR reasoning task with multimodal medical
data, PASS leverages its learned task-conditioned distribution over the agentic
supernet. Thus, it adaptively selects the most suitable tool at each supernet
layer, offering probability-annotated trajectories for post-hoc audits and
directly enhancing medical AI safety. PASS also continuously compresses salient
findings into an evolving personalized memory, while dynamically deciding
whether to deepen its reasoning path or invoke an early exit for efficiency. To
optimize a Pareto frontier balancing performance and cost, we design a novel
three-stage training procedure, including expert knowledge warm-up, contrastive
path-ranking, and cost-aware reinforcement learning. To facilitate rigorous
evaluation, we introduce CAB-E, a comprehensive benchmark for multi-step,
safety-critical, free-form CXR reasoning. Experiments across various benchmarks
validate that PASS significantly outperforms strong baselines in multiple
metrics (e.g., accuracy, AUC, LLM-J.) while balancing computational costs,
pushing a new paradigm shift towards interpretable, adaptive, and multimodal
medical agentic systems.

</details>


### [24] [Diversity First, Quality Later: A Two-Stage Assumption for Language Model Alignment](https://arxiv.org/abs/2508.10530)
*Zetian Sun,Dongfang Li,Baotian Hu*

Main category: cs.AI

TL;DR: 研究语言模型与人类偏好对齐问题，指出在线策略数据并非总是最优，提出对齐阶段假设并给出划分阶段边界算法，通过多模型和方法实验验证。


<details>
  <summary>Details</summary>
Motivation: 解决语言模型与人类偏好对齐中在线策略数据并非总是最优的问题，解释数据有效性差异现象。

Method: 提出对齐阶段假设，将对齐过程分为偏好注入和偏好微调阶段，通过理论和实证分析刻画阶段并提出划分边界算法。

Result: 在线策略数据有效性因模型而异，如Llama - 3中在线策略数据有效性是静态数据3倍，Zephyr中是0.4倍；通过5个模型和2种对齐方法实验验证假设和边界测量的通用性。

Conclusion: 对齐阶段假设和边界测量具有一定的通用性，可用于语言模型与人类偏好对齐。

Abstract: The alignment of language models (LMs) with human preferences is critical for
building reliable AI systems. The problem is typically framed as optimizing an
LM policy to maximize the expected reward that reflects human preferences.
Recently, Direct Preference Optimization (DPO) was proposed as a LM alignment
method that directly optimize the policy from static preference data, and
further improved by incorporating on-policy sampling (i.e., preference
candidates generated during the training loop) for better LM alignment.
However, we show on-policy data is not always optimal, with systematic
effectiveness difference emerging between static and on-policy preference
candidates. For example, on-policy data can result in a 3$\times$ effectiveness
compared with static data for Llama-3, and a 0.4$\times$ effectiveness for
Zephyr. To explain the phenomenon, we propose the alignment stage assumption,
which divides the alignment process into two distinct stages: the preference
injection stage, which benefits from diverse data, and the preference
fine-tuning stage, which favors high-quality data. Through theoretical and
empirical analysis, we characterize these stages and propose an effective
algorithm to identify the boundaries between them. We perform experiments on 5
models (Llama, Zephyr, Phi-2, Qwen, Pythia) and 2 alignment methods (DPO,
SLiC-HF) to show the generalizability of alignment stage assumption and
boundary measurement.

</details>


### [25] [Improving Value-based Process Verifier via Low-Cost Variance Reduction](https://arxiv.org/abs/2508.10539)
*Zetian Sun,Dongfang Li,Baotian Hu,Min Zhang*

Main category: cs.AI

TL;DR: 论文指出大语言模型推理能力有挑战，价值基过程验证器受估计误差影响，提出ComMCS方法减少方差且无额外推理成本，实验显示其有效。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在复杂领域推理能力不足，价值基过程验证器受限于训练注释的估计误差，需解决此问题以提升推理能力。

Method: 提出Compound Monte Carlo Sampling (ComMCS)方法，线性组合当前和后续步骤的MC估计量构建无偏估计器。

Result: ComMCS在MATH - 500和GSM8K基准测试中有效，在MATH - 500的Best - of - 32采样实验中，比基于回归的优化方法高2.8分，比非方差减少基线高2.2分。

Conclusion: ComMCS方法可在不增加额外大语言模型推理成本的情况下，有效减少估计方差，提升推理效果。

Abstract: Large language models (LLMs) have achieved remarkable success in a wide range
of tasks. However, their reasoning capabilities, particularly in complex
domains like mathematics, remain a significant challenge. Value-based process
verifiers, which estimate the probability of a partial reasoning chain leading
to a correct solution, are a promising approach for improving reasoning.
Nevertheless, their effectiveness is often hindered by estimation error in
their training annotations, a consequence of the limited number of Monte Carlo
(MC) samples feasible due to the high cost of LLM inference. In this paper, we
identify that the estimation error primarily arises from high variance rather
than bias, and the MC estimator is a Minimum Variance Unbiased Estimator
(MVUE). To address the problem, we propose the \textsc{Com}pound \textsc{M}onte
\textsc{C}arlo \textsc{S}ampling (ComMCS) method, which constructs an unbiased
estimator by linearly combining the MC estimators from the current and
subsequent steps. Theoretically, we show that our method leads to a predictable
reduction in variance, while maintaining an unbiased estimation without
additional LLM inference cost. We also perform empirical experiments on the
MATH-500 and GSM8K benchmarks to demonstrate the effectiveness of our method.
Notably, ComMCS outperforms regression-based optimization method by 2.8 points,
the non-variance-reduced baseline by 2.2 points on MATH-500 on Best-of-32
sampling experiment.

</details>


### [26] [MSRS: Adaptive Multi-Subspace Representation Steering for Attribute Alignment in Large Language Models](https://arxiv.org/abs/2508.10599)
*Xinyan Jiang,Lin Zhang,Jiayi Zhang,Qingsong Yang,Guimin Hu,Di Wang,Lijie Hu*

Main category: cs.AI

TL;DR: 提出多子空间表示转向（MSRS）框架用于大语言模型多属性转向，实验显示其能减少属性冲突，超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有激活转向方法在联合控制多个属性时存在干扰和权衡问题，需要新方法解决。

Method: 提出MSRS框架，为每个属性分配正交子空间减少干扰，采用混合子空间组合策略，引入动态加权函数和token级转向机制。

Result: MSRS显著减少属性冲突，在一系列属性上超越现有方法，能有效泛化到不同下游任务。

Conclusion: MSRS是一种有效解决大语言模型多属性转向问题的方法。

Abstract: Activation steering offers a promising approach to controlling the behavior
of Large Language Models by directly manipulating their internal activations.
However, most existing methods struggle to jointly steer multiple attributes,
often resulting in interference and undesirable trade-offs. To address this
challenge, we propose Multi-Subspace Representation Steering (MSRS), a novel
framework for effective multi-attribute steering via subspace representation
fine-tuning. MSRS reduces inter-attribute interference by allocating orthogonal
subspaces to each attribute, isolating their influence within the model's
representation space. MSRS also incorporates a hybrid subspace composition
strategy: it combines attribute-specific subspaces for unique steering
directions with a shared subspace for common steering directions. A dynamic
weighting function learns to efficiently integrate these components for precise
control. During inference, MSRS introduces a token-level steering mechanism
that dynamically identifies and intervenes on the most semantically relevant
tokens, enabling fine-grained behavioral modulation. Experimental results show
that MSRS significantly reduces attribute conflicts, surpasses existing methods
across a range of attributes, and generalizes effectively to diverse downstream
tasks.

</details>


### [27] [GenOM: Ontology Matching with Description Generation and Large Language Model](https://arxiv.org/abs/2508.10703)
*Yiping Song,Jiaoyan Chen,Renate A. Schmidt*

Main category: cs.AI

TL;DR: 本文提出基于大语言模型的本体对齐框架GenOM，在OAEI Bio - ML赛道实验中表现出色，消融实验证实其方法有效性。


<details>
  <summary>Details</summary>
Motivation: 本体匹配在生物医学领域语义互操作性和集成中至关重要，需更好的本体对齐方法。

Method: 引入GenOM框架，通过生成文本定义丰富本体概念语义表示，用嵌入模型检索对齐候选，结合基于精确匹配的工具提高精度。

Result: 在OAEI Bio - ML赛道实验中，GenOM常能取得有竞争力的表现，超越许多基线方法。

Conclusion: 消融实验证实语义丰富和少样本提示的有效性，凸显框架的鲁棒性和适应性。

Abstract: Ontology matching (OM) plays an essential role in enabling semantic
interoperability and integration across heterogeneous knowledge sources,
particularly in the biomedical domain which contains numerous complex concepts
related to diseases and pharmaceuticals. This paper introduces GenOM, a large
language model (LLM)-based ontology alignment framework, which enriches the
semantic representations of ontology concepts via generating textual
definitions, retrieves alignment candidates with an embedding model, and
incorporates exact matching-based tools to improve precision. Extensive
experiments conducted on the OAEI Bio-ML track demonstrate that GenOM can often
achieve competitive performance, surpassing many baselines including
traditional OM systems and recent LLM-based methods. Further ablation studies
confirm the effectiveness of semantic enrichment and few-shot prompting,
highlighting the framework's robustness and adaptability.

</details>


### [28] [Agentic Design Review System](https://arxiv.org/abs/2508.10745)
*Sayan Nag,K J Joseph,Koustava Goswami,Vlad I Morariu,Balaji Vasan Srinivasan*

Main category: cs.AI

TL;DR: 提出Agentic Design Review System (AgenticDRS)评估图形设计，用DRS - BENCH基准测试，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 全面评估图形设计需整合专家反馈，当前该方向研究不足。

Method: 提出AgenticDRS，用基于图匹配的上下文示例选择和提示扩展方法使代理了解设计，用DRS - BENCH基准评估。

Result: 与现有基线对比实验及消融实验表明Agentic - DRS能有效评估图形设计并生成可操作反馈。

Conclusion: 希望吸引更多关注此实用但研究不足的方向。

Abstract: Evaluating graphic designs involves assessing it from multiple facets like
alignment, composition, aesthetics and color choices. Evaluating designs in a
holistic way involves aggregating feedback from individual expert reviewers.
Towards this, we propose an Agentic Design Review System (AgenticDRS), where
multiple agents collaboratively analyze a design, orchestrated by a meta-agent.
A novel in-context exemplar selection approach based on graph matching and a
unique prompt expansion method plays central role towards making each agent
design aware. Towards evaluating this framework, we propose DRS-BENCH
benchmark. Thorough experimental evaluation against state-of-the-art baselines
adapted to the problem setup, backed-up with critical ablation experiments
brings out the efficacy of Agentic-DRS in evaluating graphic designs and
generating actionable feedback. We hope that this work will attract attention
to this pragmatic, yet under-explored research direction.

</details>


### [29] [Scaling Up without Fading Out: Goal-Aware Sparse GNN for RL-based Generalized Planning](https://arxiv.org/abs/2508.10747)
*Sangwoo Jeon,Juchul Shin,Gyeong-Tae Kim,YeonJe Cho,Seongwoo Kim*

Main category: cs.AI

TL;DR: 提出稀疏、目标感知的GNN表示法解决现有广义规划方法在大规模问题上的不足，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有基于GNN的广义规划方法用全连接图表示规划状态，在大规模问题上存在边信息组合爆炸、内存需求指数增加等问题。

Method: 提出稀疏、目标感知的GNN表示法，选择性编码相关局部关系并明确整合与目标相关的空间特征，设计基于PDDL的无人机任务场景进行验证。

Result: 方法能有效扩展到之前密集图表示无法处理的更大网格尺寸，大幅提高策略泛化能力和成功率。

Conclusion: 研究为解决现实中的大规模广义规划任务提供了实用基础。

Abstract: Generalized planning using deep reinforcement learning (RL) combined with
graph neural networks (GNNs) has shown promising results in various symbolic
planning domains described by PDDL. However, existing approaches typically
represent planning states as fully connected graphs, leading to a combinatorial
explosion in edge information and substantial sparsity as problem scales grow,
especially evident in large grid-based environments. This dense representation
results in diluted node-level information, exponentially increases memory
requirements, and ultimately makes learning infeasible for larger-scale
problems. To address these challenges, we propose a sparse, goal-aware GNN
representation that selectively encodes relevant local relationships and
explicitly integrates spatial features related to the goal. We validate our
approach by designing novel drone mission scenarios based on PDDL within a grid
world, effectively simulating realistic mission execution environments. Our
experimental results demonstrate that our method scales effectively to larger
grid sizes previously infeasible with dense graph representations and
substantially improves policy generalization and success rates. Our findings
provide a practical foundation for addressing realistic, large-scale
generalized planning tasks.

</details>


### [30] [Modeling Human Responses to Multimodal AI Content](https://arxiv.org/abs/2508.10769)
*Zhiqi Shen,Shaojing Fan,Danni Xu,Terence Sim,Mohan Kankanhalli*

Main category: cs.AI

TL;DR: 本文围绕AI生成内容展开，创建MhAIM数据集，揭示人们识别AI内容的情况，提出新指标，构建T - Lens系统，为LLM赋予人类感知能力并提出应对虚假信息策略。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注内容真实性鉴定，缺乏AI生成内容对人类感知和行为影响的研究，在交易或股市领域，预测人们反应比核实事实准确性更重要。

Method: 采用以人为本的方法，创建MhAIM数据集进行大规模分析；开展人类研究；提出三个新指标；构建基于LLM的T - Lens系统，其核心是HR - MCP。

Result: 人们在帖子包含文本和视觉信息且两者存在不一致时更善于识别AI内容；提出的三个新指标可量化用户对在线内容的判断和参与度；T - Lens系统能更好地与人类反应对齐，增强可解释性和交互能力。

Conclusion: 研究为LLM赋予人类感知能力提供实证见解和实用工具，指出AI、人类认知和信息接收之间的复杂相互作用，并提出减轻AI驱动虚假信息风险的可行策略。

Abstract: As AI-generated content becomes widespread, so does the risk of
misinformation. While prior research has primarily focused on identifying
whether content is authentic, much less is known about how such content
influences human perception and behavior. In domains like trading or the stock
market, predicting how people react (e.g., whether a news post will go viral),
can be more critical than verifying its factual accuracy. To address this, we
take a human-centered approach and introduce the MhAIM Dataset, which contains
154,552 online posts (111,153 of them AI-generated), enabling large-scale
analysis of how people respond to AI-generated content. Our human study reveals
that people are better at identifying AI content when posts include both text
and visuals, particularly when inconsistencies exist between the two. We
propose three new metrics: trustworthiness, impact, and openness, to quantify
how users judge and engage with online content. We present T-Lens, an LLM-based
agent system designed to answer user queries by incorporating predicted human
responses to multimodal information. At its core is HR-MCP (Human Response
Model Context Protocol), built on the standardized Model Context Protocol
(MCP), enabling seamless integration with any LLM. This integration allows
T-Lens to better align with human reactions, enhancing both interpretability
and interaction capabilities. Our work provides empirical insights and
practical tools to equip LLMs with human-awareness capabilities. By
highlighting the complex interplay among AI, human cognition, and information
reception, our findings suggest actionable strategies for mitigating the risks
of AI-driven misinformation.

</details>


### [31] [The Knowledge-Reasoning Dissociation: Fundamental Limitations of LLMs in Clinical Natural Language Inference](https://arxiv.org/abs/2508.10777)
*Maël Jullien,Marco Valentino,André Freitas*

Main category: cs.AI

TL;DR: 本文引入临床试验自然语言推理基准，评估大语言模型，发现其虽有相关知识但推理表现差，GKMRV方法可有效探测模型可靠性。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型是否仅通过扩大数据和参数就能获得结构化、可泛化的内部表示。

Method: 引入包含四种推理类型的临床试验自然语言推理基准，每个项目配有GKMRV探针，评估六个当代大语言模型在直接和思维链提示下的表现。

Result: 模型GKMRV准确率接近上限，但主要推理任务表现差，输出推理一致性高。

Conclusion: 当前大语言模型有相关知识但缺乏可靠部署知识所需的结构化内部表示，GKMRV方法可有效探测高风险领域模型的可靠性。

Abstract: Large language models are often assumed to acquire increasingly structured,
generalizable internal representations simply by scaling data and parameters.
We interrogate this assumption by introducing a Clinical Trial Natural Language
Inference benchmark comprising four reasoning families, Causal Attribution,
Compositional Grounding, Epistemic Verification, and Risk State Abstraction.
Each item is paired with a targeted Ground Knowledge and Meta-Level Reasoning
Verification (GKMRV) probe, allowing us to dissociate failures of factual
access from failures of inference. We evaluate six contemporary LLMs under both
direct and chain of thought prompting.
  Models achieve near-ceiling GKMRV accuracy (mean accuracy 0.918) yet perform
poorly on the main reasoning tasks (mean accuracy 0.25). Despite low accuracy,
output inferences are highly consistent across samples (mean 0.87), indicating
a systematic application of underlying heuristics and shortcuts.
  These results reveal fundamental structural and representational limitations:
current LLMs often possess the relevant clinical knowledge but lack the
structured, composable internal representations needed to deploy it reliably
(e.g., integrating constraints, weighing evidence, or simulating
counterfactuals). Decoupling knowledge from reasoning with GKMRV makes this
dissociation explicit and measurable, providing an effective framework for
probing the reliability of LLMs in high-stakes domains.

</details>


### [32] [Who Benefits from AI Explanations? Towards Accessible and Interpretable Systems](https://arxiv.org/abs/2508.10806)
*Maria J. P. Peixoto,Akriti Pandey,Ahsan Zaman,Peter R. Lewis*

Main category: cs.AI

TL;DR: 研究XAI对视力障碍用户的可访问性，发现评估常忽略残疾用户，提出四部分概念验证方法，初步表明简化解释更易理解，需多模态呈现。


<details>
  <summary>Details</summary>
Motivation: 尽管XAI可用性受关注，但对视力障碍用户的可访问性研究不足，需探究XAI的可访问性差距。

Method: 一是对79项研究进行文献综述；二是提出四部分的概念验证方法，包括AI系统分类、角色定义与情境化、原型设计与实现、专家和用户对XAI技术可访问性的评估。

Result: 初步发现简化解释对非视觉用户比详细解释更易理解，且需要多模态呈现以实现更公平的可解释性。

Conclusion: XAI对视力障碍用户存在可访问性问题，简化解释和多模态呈现有助于提升可访问性。

Abstract: As AI systems are increasingly deployed to support decision-making in
critical domains, explainability has become a means to enhance the
understandability of these outputs and enable users to make more informed and
conscious choices. However, despite growing interest in the usability of
eXplainable AI (XAI), the accessibility of these methods, particularly for
users with vision impairments, remains underexplored. This paper investigates
accessibility gaps in XAI through a two-pronged approach. First, a literature
review of 79 studies reveals that evaluations of XAI techniques rarely include
disabled users, with most explanations relying on inherently visual formats.
Second, we present a four-part methodological proof of concept that
operationalizes inclusive XAI design: (1) categorization of AI systems, (2)
persona definition and contextualization, (3) prototype design and
implementation, and (4) expert and user assessment of XAI techniques for
accessibility. Preliminary findings suggest that simplified explanations are
more comprehensible for non-visual users than detailed ones, and that
multimodal presentation is required for more equitable interpretability.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [33] [CarAT: Carbon Atom Tracing across Industrial Chemical Value Chains via Chemistry Language Models](https://arxiv.org/abs/2508.10216)
*Emma Pajak,David Walz,Olga Walz,Laura Marie Helleckes,Klaus Hellgardt,Antonio del Rio Chanona*

Main category: cs.CE

TL;DR: 文章介绍CarAT方法用于计算工业价值链生物源碳含量（BCC），通过三阶段计算并在多个案例验证，可助力可持续制造转型。


<details>
  <summary>Details</summary>
Motivation: 化工行业重视可持续发展，TfS联盟2026年要求报告BCC，但现有测量方法不适合工业连续监测。

Method: CarAT方法分三阶段：准备价值链数据、用化学语言模型进行原子映射、用线性规划计算BCC。

Result: 在27节点甲苯二异氰酸酯价值链等案例验证，用桑基图展示结果。

Conclusion: CarAT是可扩展自动化方法，支持合规，推动碳中和，能助力向可持续制造转型。

Abstract: The chemical industry is increasingly prioritising sustainability, with a
focus on reducing carbon footprints to achieve net zero. By 2026, the Together
for Sustainability (TfS) consortium will require reporting of biogenic carbon
content (BCC) in chemical products, posing a challenge as BCC depends on
feedstocks, value chain configuration, and process-specific variables. While
carbon-14 isotope analysis can measure BCC, it is impractical for continuous
industrial monitoring. This work presents CarAT (Carbon Atom Tracker), an
automated methodology for calculating BCC across industrial value chains,
enabling dynamic and accurate sustainability reporting. The approach leverages
existing Enterprise Resource Planning data in three stages: (1) preparing value
chain data, (2) performing atom mapping in chemical reactions using chemistry
language models, and (3) applying a linear program to calculate BCC given known
inlet compositions. The methodology is validated on a 27-node industrial
toluene diisocyanate value chain. Three scenarios are analysed: a base case
with fossil feedstocks, a case incorporating a renewable feedstock, and a
butanediol value chain with a recycle stream. Results are visualised with
Sankey diagrams showing the flow of carbon attributes across the value chain.
The key contribution is a scalable, automated method for real-time BCC
calculation under changing industrial conditions. CarAT supports compliance
with upcoming reporting mandates and advances carbon neutrality goals by
enabling systematic fossil-to-biogenic substitution. Through transparent,
auditable tracking of carbon sources in production networks, it empowers
data-driven decisions to accelerate the transition to sustainable
manufacturing.

</details>


### [34] [TOBACO: Topology Optimization via Band-limited Coordinate Networks for Compositionally Graded Alloys](https://arxiv.org/abs/2508.10320)
*Aaditya Chandrasekhar,Stefan Knapik,Deepak Sharma,John Reidy,Ian McCue,Jian Cao,Wei Chen*

Main category: cs.CE

TL;DR: 本文提出基于拓扑优化的框架设计成分梯度可控的CGA组件，用带限坐标神经网络表示成分分布，通过调节带宽确保符合梯度限制，并用实例验证框架有效性。


<details>
  <summary>Details</summary>
Motivation: CGAs设计灵活但增材制造需制造约束，需设计优化CGA组件的方法。

Method: 引入基于拓扑优化的框架，用带限坐标神经网络表示受限成分分布，调节网络带宽确保符合梯度限制。

Result: 通过各种弹性和热弹性拓扑优化实例证明了框架的有效性。

Conclusion: 提出的框架可用于设计优化的CGA组件，且能有效控制成分梯度。

Abstract: Compositionally Graded Alloys (CGAs) offer unprecedented design flexibility
by enabling spatial variations in composition; tailoring material properties to
local loading conditions. This flexibility leads to components that are
stronger, lighter, and more cost-effective than traditional monolithic
counterparts. The fabrication of CGAs have become increasingly feasible owing
to recent advancements in additive manufacturing (AM), particularly in
multi-material printing and improved precision in material deposition. However,
AM of CGAs requires imposition of manufacturing constraints; in particular
limits on the maximum spatial gradation of composition.
  This paper introduces a topology optimization (TO) based framework for
designing optimized CGA components with controlled compositional gradation. In
particular, we represent the constrained composition distribution using a
band-limited coordinate neural network. By regulating the network's bandwidth,
we ensure implicit compliance with gradation limits, eliminating the need for
explicit constraints. The proposed approach also benefits from the inherent
advantages of TO using coordinate networks, including mesh independence,
high-resolution design extraction, and end-to-end differentiability. The
effectiveness of our framework is demonstrated through various elastic and
thermo-elastic TO examples.

</details>


### [35] [Chem3DLLM: 3D Multimodal Large Language Models for Chemistry](https://arxiv.org/abs/2508.10696)
*Lei Jiang,Shuzhou Sun,Biqing Qi,Yuchen Fu,Xiaohua Xu,Yuqiang Li,Dongzhan Zhou,Tianfan Fu*

Main category: cs.CE

TL;DR: 本文提出Chem3DLLM统一蛋白质条件多模态大语言模型解决3D分子构象生成难题，实验在基于结构的药物设计中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有自回归语言模型难以处理3D分子构象生成，存在3D结构与离散令牌空间不兼容、异质输入集成困难和缺乏科学先验等问题。

Method: 设计可逆文本编码压缩3D分子结构，实现与蛋白质口袋特征集成；采用基于稳定性奖励的强化学习优化化学有效性，引入轻量级蛋白质嵌入投影器进行端到端训练。

Result: 在基于结构的药物设计实验中取得Vina得分为 -7.21的最优成绩。

Conclusion: 提出的统一多模态方法适用于实际药物发现应用。

Abstract: In the real world, a molecule is a 3D geometric structure. Compared to 1D
SMILES sequences and 2D molecular graphs, 3D molecules represent the most
informative molecular modality. Despite the rapid progress of
autoregressive-based language models, they cannot handle the generation of 3D
molecular conformation due to several challenges: 1) 3D molecular structures
are incompatible with LLMs' discrete token space, 2) integrating heterogeneous
inputs like proteins, ligands, and text remains difficult within a unified
model, and 3) LLMs lack essential scientific priors, hindering the enforcement
of physical and chemical constraints during generation. To tackle these issues,
we present Chem3DLLM, a unified protein-conditioned multimodal large language
model. Our approach designs a novel reversible text encoding for 3D molecular
structures using run-length compression, achieving 3x size reduction while
preserving complete structural information. This enables seamless integration
of molecular geometry with protein pocket features in a single LLM
architecture. We employ reinforcement learning with stability-based rewards to
optimize chemical validity and incorporate a lightweight protein embedding
projector for end-to-end training. Experimental results on structure-based drug
design demonstrate state-of-the-art performance with a Vina score of -7.21,
validating our unified multimodal approach for practical drug discovery
applications.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [36] [Privacy-Preserving Approximate Nearest Neighbor Search on High-Dimensional Data](https://arxiv.org/abs/2508.10373)
*Yingfan Liu,Yandi Zhang,Jiadong Xie,Hui Li,Jeffrey Xu Yu,Jiangtao Cui*

Main category: cs.DB

TL;DR: 本文提出针对向量的新型隐私保护k-近邻搜索（PP - ANNS）方案，可避免云与用户间大量通信开销，实验表明该方法比现有方法快3个数量级且不影响准确性。


<details>
  <summary>Details</summary>
Motivation: 在云计算和AI时代，现有PP - ANNS解决方案无法同时满足数据隐私、效率、准确性和最小用户参与度的要求，需要新的解决方案。

Method: 在单云服务器上执行PP - ANNS，引入距离比较加密方法，设计结合先进k - ANNS方法和近似距离计算方法的隐私保护索引，基于索引采用过滤 - 细化策略设计搜索方法。

Result: 方法比现有技术快3个数量级，且不影响准确性。

Conclusion: 提出的新型PP - ANNS解决方案优于现有解决方案。

Abstract: In the era of cloud computing and AI, data owners outsource ubiquitous
vectors to the cloud, which furnish approximate $k$-nearest neighbors
($k$-ANNS) services to users. To protect data privacy against the untrusted
server, privacy-preserving $k$-ANNS (PP-ANNS) on vectors has been a fundamental
and urgent problem. However, existing PP-ANNS solutions fall short of meeting
the requirements of data privacy, efficiency, accuracy, and minimal user
involvement concurrently. To tackle this challenge, we introduce a novel
solution that primarily executes PP-ANNS on a single cloud server to avoid the
heavy communication overhead between the cloud and the user. To ensure data
privacy, we introduce a novel encryption method named distance comparison
encryption, facilitating secure, efficient, and exact distance comparisons. To
optimize the trade-off between data privacy and search performance, we design a
privacy-preserving index that combines the state-of-the-art $k$-ANNS method
with an approximate distance computation method. Then, we devise a search
method using a filter-and-refine strategy based on the index. Moreover, we
provide the security analysis of our solution and conduct extensive experiments
to demonstrate its superiority over existing solutions. Based on our
experimental results, our method accelerates PP-ANNS by up to 3 orders of
magnitude compared to state-of-the-art methods, while not compromising the
accuracy.

</details>


### [37] [Cross-Organizational Analysis of Parliamentary Processes: A Case Study](https://arxiv.org/abs/2508.10381)
*Paul-Julius Hillmann,Stephan A. Fahrenkrog-Petersen,Jan Mendling*

Main category: cs.DB

TL;DR: 本文首次将流程挖掘应用于议会流程，分析德国三个州议会立法流程，探索跨组织流程比较并生成见解。


<details>
  <summary>Details</summary>
Motivation: 流程挖掘广泛应用，但跨组织流程比较因数据共享问题少受关注，德国州议会有数据共享法律要求且流程类型相同，以此为研究对象。

Method: 对德国三个州议会的立法流程进行案例研究，与政治科学家和联邦议会领域专家交流。

Result: 分析出三个州议会立法流程的差异和最佳实践。

Conclusion: 为结合政治学和流程挖掘的跨学科研究领域做出贡献。

Abstract: Process Mining has been widely adopted by businesses and has been shown to
help organizations analyze and optimize their processes. However, so far,
little attention has gone into the cross-organizational comparison of
processes, since many companies are hesitant to share their data. In this
paper, we explore the processes of German state parliaments that are often
legally required to share their data and run the same type of processes for
different geographical regions. This paper is the first attempt to apply
process mining to parliamentary processes and, therefore, contributes toward a
novel interdisciplinary research area that combines political science and
process mining. In our case study, we analyze legislative processes of three
German state parliaments and generate insights into their differences and best
practices. We provide a discussion of the relevance of our results that are
based on knowledge exchange with a political scientist and a domain expert from
the German federal parliament.

</details>


### [38] [Efficient Methods for Accurate Sparse Trajectory Recovery and Map Matching](https://arxiv.org/abs/2508.10460)
*Wei Tian,Jieming Shi,Man Lung Yiu*

Main category: cs.DB

TL;DR: 本文提出TRMMA和MMA方法用于轨迹恢复和地图匹配，实验表明这两种方法在多个真实数据集上效果最佳。


<details>
  <summary>Details</summary>
Motivation: 现实轨迹数据稀疏且与路网不对齐，而许多应用需要高质量数据，因此要研究轨迹恢复和地图匹配问题以提升数据质量。

Method: 提出MMA方法，将GPS点映射到候选路段集；提出TRMMA方法，基于MMA结果推断缺失点。MMA采用分类任务，TRMMA设计双变压器编码和解码技术。

Result: 在4个大型真实数据集上，TRMMA和MMA与众多现有方法对比，始终取得最佳结果。

Conclusion: TRMMA和MMA方法能有效解决轨迹恢复和地图匹配问题，提高数据质量。

Abstract: Real-world trajectories are often sparse with low-sampling rates (i.e., long
intervals between consecutive GPS points) and misaligned with road networks,
yet many applications demand high-quality data for optimal performance. To
improve data quality with sparse trajectories as input, we systematically study
two related research problems: trajectory recovery on road network, which aims
to infer missing points to recover high-sampling trajectories, and map
matching, which aims to map GPS points to road segments to determine underlying
routes. In this paper, we present efficient methods TRMMA and MMA for accurate
trajectory recovery and map matching, respectively, where MMA serves as the
first step of TRMMA. In MMA, we carefully formulate a classification task to
map a GPS point from sparse trajectories to a road segment over a small
candidate segment set, rather than the entire road network. We develop
techniques in MMA to generate effective embeddings that capture the patterns of
GPS data, directional information, and road segments, to accurately align
sparse trajectories to routes. For trajectory recovery, TRMMA focuses on the
segments in the route returned by MMA to infer missing points with position
ratios on road segments, producing high-sampling trajectories efficiently by
avoiding evaluation of all road segments. Specifically, in TRMMA, we design a
dual-transformer encoding process to cohesively capture latent patterns in
trajectories and routes, and an effective decoding technique to sequentially
predict the position ratios and road segments of missing points. We conduct
extensive experiments to compare TRMMA and MMA with numerous existing methods
for trajectory recovery and map matching, respectively, on 4 large real-world
datasets. TRMMA and MMA consistently achieve the best result quality, often by
a significant margin.

</details>


### [39] [Advances in Logic-Based Entity Resolution: Enhancing ASPEN with Local Merges and Optimality Criteria](https://arxiv.org/abs/2508.10504)
*Zhliang Xiang,Meghyn Bienvenu,Gianluca Cima,Víctor Gutiérrez-Basulto,Yazmín Ibáñez-García*

Main category: cs.DB

TL;DR: 本文提出ASPEN+，扩展ASPEN系统，增加本地合并支持和新的最优解标准，并进行形式化分析和实验评估。


<details>
  <summary>Details</summary>
Motivation: 现有ASPEN系统仅支持全局合并，本地合并在数据值解析时更合适，且缺乏新的最优解选择标准。

Method: 提出ASPEN+系统，对最优解概念进行形式化和计算分析，在真实数据集上进行实验评估。

Result: 实验展示了本地合并和新最优标准对准确性和运行时间的影响。

Conclusion: ASPEN+扩展了ASPEN系统，新功能在真实数据集上有效果。

Abstract: In this paper, we present ASPEN+, which extends an existing ASP-based system,
ASPEN,for collective entity resolution with two important functionalities:
support for local merges and new optimality criteria for preferred solutions.
Indeed, ASPEN only supports so-called global merges of entity-referring
constants (e.g. author ids), in which all occurrences of matched constants are
treated as equivalent and merged accordingly. However, it has been argued that
when resolving data values, local merges are often more appropriate, as e.g.
some instances of 'J. Lee' may refer to 'Joy Lee', while others should be
matched with 'Jake Lee'. In addition to allowing such local merges, ASPEN+
offers new optimality criteria for selecting solutions, such as minimizing rule
violations or maximising the number of rules supporting a merge. Our main
contributions are thus (1) the formalisation and computational analysis of
various notions of optimal solution, and (2) an extensive experimental
evaluation on real-world datasets, demonstrating the effect of local merges and
the new optimality criteria on both accuracy and runtime.

</details>


### [40] [Emerging Skycube](https://arxiv.org/abs/2508.10516)
*Mickaël Martin Nevot*

Main category: cs.DB

TL;DR: 结合多准则决策分析和趋势反转发现，引入新兴天空立方体概念，提出两次缩减方法节省计算时间和存储空间。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏DBMS集成解决方案来计算新兴天空立方体并利用ROLAP分析工具，且多准则决策分析需多属性。

Method: 引入新兴天空立方体概念，进行两次缩减，先使用天际线概念格对天空立方体无损部分物化，再选择封闭新兴天空立方体或封闭新兴L - 天空立方体进行缩减。

Result: 新兴天空立方体计算成本低于新兴数据立方体，可通过两次缩减节省计算时间和存储空间。

Conclusion: 提出的新兴天空立方体及缩减方法在计算成本和资源利用上有优势。

Abstract: Combining multi-criteria decision analysis and trend reversal discovery make
it possible to extract globally optimal, or non-dominated, data in relation to
several criteria, and then to observe their evolution according to a
decision-making property. Thus, we introduce Emerging Skycube, a concept
associating Skycube and emerging datacube. As far as we know, no
DBMS-integrated solution exists to compute an emerging Skycube, and hence
taking advantage of ROLAP analysis tools. An emerging datacube has only one
measure: we propose to use several to comply to multi-criteria decision
analysis constraints which requires multiple attributes. A datacube is
expensive to compute. An emerging datacube is about twice as expensive. On the
other hand, an emerging Skycube is cheaper as the trend reversal is computed
after two Skycube calculations, which considerably reduces the relation volume
in comparison with the initial one. It is possible to save even more computing
time and storage space. To this end, we propose two successive reductions.
First, a Skycube lossless partial materialisation using Skylines concepts
lattice, based on the agree concepts lattice and partitions lattice. Then,
either the closed emerging Skycube for an information-loss reduction, or the
closed emerging L-Skycube for a smaller but lossless reduction.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [41] [Hard Shell, Reliable Core: Improving Resilience in Replicated Systems with Selective Hybridization](https://arxiv.org/abs/2508.10141)
*Laura Lawniczak,Tobias Distler*

Main category: cs.DC

TL;DR: 提出ShellFT框架解决现有混合故障模型方法的问题，展示其灵活性并评估其降低成本效果。


<details>
  <summary>Details</summary>
Motivation: 现有混合故障模型方法在组成灵活性和多样化开销方面存在问题，需要改进。

Method: 提出ShellFT框架，利用微复制概念让系统设计者自由选择抗拜占庭故障的复制逻辑部分，并给出三个自定义协议。

Result: 与传统方法相比，ShellFT能降低70%以上的多样化成本。

Conclusion: ShellFT框架能解决现有问题，实现针对特定用例的混合解决方案，且成本更低。

Abstract: Hybrid fault models are known to be an effective means for enhancing the
robustness of consensus-based replicated systems. However, existing
hybridization approaches suffer from limited flexibility with regard to the
composition of crash-tolerant and Byzantine fault-tolerant system parts and/or
are associated with a significant diversification overhead. In this paper we
address these issues with ShellFT, a framework that leverages the concept of
micro replication to allow system designers to freely choose the parts of the
replication logic that need to be resilient against Byzantine faults. As a key
benefit, such a selective hybridization makes it possible to develop hybrid
solutions that are tailored to the specific characteristics and requirements of
individual use cases. To illustrate this flexibility, we present three custom
ShellFT protocols and analyze the complexity of their implementations. Our
evaluation shows that compared with traditional hybridization approaches,
ShellFT is able to decrease diversification costs by more than 70%.

</details>


### [42] [Mixed-Precision Performance Portability of FFT-Based GPU-Accelerated Algorithms for Block-Triangular Toeplitz Matrices](https://arxiv.org/abs/2508.10202)
*Sreeram Venkat,Kasia Swirydowicz,Noah Wolfe,Omar Ghattas*

Main category: cs.DC

TL;DR: 提出基于Hipify的即时框架实现性能可移植性，应用于FFTMatvec并为其开发动态混合精度框架，在多GPU上测试并在Frontier超级计算机上扩展。


<details>
  <summary>Details</summary>
Motivation: 领导级计算设施的硬件多样性和GPU低精度计算性能提升，促使科学HPC工作流采用混合精度算法和性能可移植性模型。

Method: 使用Hipify创建即时框架实现性能可移植性，将AMD GPU性能优化集成到rocBLAS库，采用Pareto前沿分析确定混合精度配置。

Result: FFTMatvec可在AMD GPU上无缝运行且性能出色，展示了在多种AMD GPU上的结果，在OLCF Frontier超级计算机上扩展到2048个GPU。

Conclusion: 实现了性能可移植、混合精度的FFTMatvec应用。

Abstract: The hardware diversity displayed in leadership-class computing facilities,
alongside the immense performance boosts exhibited by today's GPUs when
computing in lower precision, provide a strong incentive for scientific HPC
workflows to adopt mixed-precision algorithms and performance portability
models. We present an on-the-fly framework using Hipify for performance
portability and apply it to FFTMatvec-an HPC application that computes
matrix-vector products with block-triangular Toeplitz matrices. Our approach
enables FFTMatvec, initially a CUDA-only application, to run seamlessly on AMD
GPUs with excellent observed performance. Performance optimizations for AMD
GPUs are integrated directly into the open-source rocBLAS library, keeping the
application code unchanged. We then present a dynamic mixed-precision framework
for FFTMatvec; a Pareto front analysis determines the optimal mixed-precision
configuration for a desired error tolerance. Results are shown for AMD Instinct
MI250X, MI300X, and the newly launched MI355X GPUs. The performance-portable,
mixed-precision FFTMatvec is scaled to 2,048 GPUs on the OLCF Frontier
supercomputer.

</details>


### [43] [GPZ: GPU-Accelerated Lossy Compressor for Particle Data](https://arxiv.org/abs/2508.10305)
*Ruoyu Li,Yafan Huang,Longtao Zhang,Zhuoxun Yang,Sheng Di,Jiajun Huang,Jinyang Liu,Jiannan Tian,Xin Liang,Guanpeng Li,Hanqi Guo,Franck Cappello,Kai Zhao*

Main category: cs.DC

TL;DR: 本文提出面向现代GPU上大规模粒子数据的高性能、有误差边界的有损压缩器GPZ，评估显示其性能优于现有压缩器。


<details>
  <summary>Details</summary>
Motivation: 基于粒子的模拟和点云应用产生的不规则数据集给存储、I/O和实时分析带来挑战，传统压缩技术在处理不规则粒子分布和GPU架构约束时存在局限。

Method: 提出四阶段并行管道，对计算、内存访问和GPU占用进行优化。

Result: 在三种GPU架构上使用六个大规模真实科学数据集评估，GPZ端到端吞吐量最高达现有压缩器的8倍，同时具备更好的压缩比和数据质量。

Conclusion: GPZ在处理大规模粒子数据压缩时显著优于现有GPU压缩器。

Abstract: Particle-based simulations and point-cloud applications generate massive,
irregular datasets that challenge storage, I/O, and real-time analytics.
Traditional compression techniques struggle with irregular particle
distributions and GPU architectural constraints, often resulting in limited
throughput and suboptimal compression ratios. In this paper, we present GPZ, a
high-performance, error-bounded lossy compressor designed specifically for
large-scale particle data on modern GPUs. GPZ employs a novel four-stage
parallel pipeline that synergistically balances high compression efficiency
with the architectural demands of massively parallel hardware. We introduce a
suite of targeted optimizations for computation, memory access, and GPU
occupancy that enables GPZ to achieve near-hardware-limit throughput. We
conduct an extensive evaluation on three distinct GPU architectures
(workstation, data center, and edge) using six large-scale, real-world
scientific datasets from five distinct domains. The results demonstrate that
GPZ consistently and significantly outperforms five state-of-the-art GPU
compressors, delivering up to 8x higher end-to-end throughput while
simultaneously achieving superior compression ratios and data quality.

</details>


### [44] [Flexible Personalized Split Federated Learning for On-Device Fine-Tuning of Foundation Models](https://arxiv.org/abs/2508.10349)
*Tianjun Yuan,Jiaxiang Geng,Pengchao Han,Xianhao Chen,Bing Luo*

Main category: cs.DC

TL;DR: 提出灵活的个性化联邦学习范式FlexP - SFL用于微调基础模型，实验显示其在个性化微调效率和最终准确率上优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 微调基础模型对下游个性化任务很重要，协作学习受限于客户端数据有限和数据分布异构，且客户端计算资源有限。

Method: 提出灵活的个性化联邦学习范式FlexP - SFL，基于拆分学习，让客户端根据资源限制本地训练部分模型并将其余部分卸载到服务器，还提出对齐策略。

Result: FlexP - SFL在个性化微调效率和最终准确率上优于基线模型。

Conclusion: FlexP - SFL是一种有效的解决客户端协作学习中资源和数据问题的方法，可提升个性化模型性能。

Abstract: Fine-tuning foundation models is critical for superior performance on
personalized downstream tasks, compared to using pre-trained models.
Collaborative learning can leverage local clients' datasets for fine-tuning,
but limited client data and heterogeneous data distributions hinder effective
collaboration. To address the challenge, we propose a flexible personalized
federated learning paradigm that enables clients to engage in collaborative
learning while maintaining personalized objectives. Given the limited and
heterogeneous computational resources available on clients, we introduce
\textbf{flexible personalized split federated learning (FlexP-SFL)}. Based on
split learning, FlexP-SFL allows each client to train a portion of the model
locally while offloading the rest to a server, according to resource
constraints. Additionally, we propose an alignment strategy to improve
personalized model performance on global data. Experimental results show that
FlexP-SFL outperforms baseline models in personalized fine-tuning efficiency
and final accuracy.

</details>


### [45] [Dalek: An Unconventional and Energy-Aware Heterogeneous Cluster](https://arxiv.org/abs/2508.10481)
*Adrien Cassagne,Noé Amiot,Manuel Bouyer*

Main category: cs.DC

TL;DR: 介绍Dalek实验计算集群，其集成消费级硬件，成本低且通用，文档详述架构和软件栈、展示基准测试结果，还引入高精度能源监测平台。


<details>
  <summary>Details</summary>
Motivation: 评估用于软件设计、原型制作和算法开发的异构消费级硬件性能，提供低成本且通用的计算平台。

Method: 构建Dalek计算集群，集成常见于迷你PC、笔记本电脑和游戏台式机的CPU和GPU，搭建其架构和软件栈，进行合成基准测试，开发定制能源监测平台。

Result: 完成集群搭建，展示合成基准测试结果，开发出能每秒提供1000个平均样本且具有毫瓦级分辨率的能源监测平台。

Conclusion: Dalek集群为相关研究提供了经济有效的平台，高精度能源监测平台可支持计算机科学领域的能源感知研究实验。

Abstract: Dalek is an experimental compute cluster designed to evaluate the performance
of heterogeneous, consumer-grade hardware for software design, prototyping, and
algorithm development. In contrast to traditional computing centers that rely
on costly, server-class components, Dalek integrates CPUs and GPUs typically
found in mini-PCs, laptops, and gaming desktops, providing a cost-effective yet
versatile platform. This document details the cluster's architecture and
software stack, and presents results from synthetic benchmarks. Furthermore, it
introduces a custom energy monitoring platform capable of delivering 1000
averaged samples per second with milliwatt-level resolution. This
high-precision monitoring capability enables a wide range of energy-aware
research experiments in applied Computer Science.

</details>


### [46] [Introducing CQ: A C-like API for Quantum Accelerated HPC](https://arxiv.org/abs/2508.10854)
*Oliver Thomson Brown,Mateusz Meller,James Richings*

Main category: cs.DC

TL;DR: 本文提出量子加速HPC的C类API规范CQ及其参考实现CQ - SimBE，二者皆开源。


<details>
  <summary>Details</summary>
Motivation: 实现量子计算向经典HPC代码的增量集成。

Method: 提出CQ规范，基于QuEST状态向量模拟器用C99实现CQ - SimBE。

Result: 得到了CQ规范和CQ - SimBE参考实现，且二者开源。

Conclusion: CQ和CQ - SimBE为量子计算集成到经典HPC提供了有效途径，可进行新特性实验。

Abstract: In this paper we present CQ, a specification for a C-like API for quantum
accelerated HPC, as well as CQ-SimBE, a reference implementation of CQ written
in C99, and built on top of the statevector simulator QuEST. CQ focuses on
enabling the incremental integration of quantum computing into classical HPC
codes by supporting runtime offloading from languages such as C and Fortran. It
provides a way of describing and offloading quantum computations which is
compatible with strictly and strongly typed compiled languages, and gives the
programmer fine-grained control over classical data movement. The CQ Simulated
Backend (CQ-SimBE) provides both a way to demonstrate the usage and utility of
CQ, and a space to experiment with new features such as support for analogue
quantum computing. Both the CQ specification and CQ-SimBE are open-source, and
available in public repositories.

</details>


### [47] [Minimmit: Fast Finality with Even Faster Blocks](https://arxiv.org/abs/2508.10862)
*Brendan Kobayashi Chou,Andrew Lewis-Pye,Patrick O'Grady*

Main category: cs.DC

TL;DR: Minimmit是一种新的状态机复制（SMR）协议，可减少延迟，本文给出动机、伪代码及一致性和活性证明，后续还有更新。


<details>
  <summary>Details</summary>
Motivation: 进一步降低状态机复制（SMR）协议的延迟。

Method: 扩展Alpenglow等协议的'2轮确定性'方法，通过更快的'视图'推进来减少延迟。

Result: 给出了动机、伪代码以及一致性和活性证明。

Conclusion: 目前提供初步草案，后续会有包含乐观响应性证明、优化建议和实验的更新草案。

Abstract: Minimmit is a new protocol for State-Machine-Replication (SMR) that extends
the '2-round finality' approach of protocols such as Alpenglow to further
reduce latency, by allowing for faster progression through 'views'. This
preliminary draft provides motivation and pseudocode, together with proofs of
consistency and liveness. An updated draft with a proof of optimistic
responsiveness, suggested optimizations, and experiments, is to follow.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [48] [Output-Sparse Matrix Multiplication Using Compressed Sensing](https://arxiv.org/abs/2508.10250)
*Huck Bennett,Karthik Gajulapalli,Alexander Golovnev,Evelyn Warton*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We give two algorithms for output-sparse matrix multiplication (OSMM), the
problem of multiplying two $n \times n$ matrices $A, B$ when their product $AB$
is promised to have at most $O(n^{\delta})$ many non-zero entries for a given
value $\delta \in [0, 2]$. We then show how to speed up these algorithms in the
fully sparse setting, where the input matrices $A, B$ are themselves sparse.
All of our algorithms work over arbitrary rings.
  Our first, deterministic algorithm for OSMM works via a two-pass reduction to
compressed sensing. It runs in roughly $n^{\omega(\delta/2, 1, 1)}$ time, where
$\omega(\cdot, \cdot, \cdot)$ is the rectangular matrix multiplication
exponent. This substantially improves on prior deterministic algorithms for
output-sparse matrix multiplication.
  Our second, randomized algorithm for OSMM works via a reduction to compressed
sensing and a variant of matrix multiplication verification, and runs in
roughly $n^{\omega(\delta - 1, 1, 1)}$ time. This algorithm and its extension
to the fully sparse setting have running times that match those of the
(randomized) algorithms for OSMM and FSMM, respectively, in recent work of
Abboud, Bringmann, Fischer, and K\"{u}nnemann (SODA, 2024). Our algorithm uses
different techniques and is arguably simpler.
  Finally, we observe that the running time of our randomized algorithm and the
algorithm of Abboud et al. are optimal via a simple reduction from rectangular
matrix multiplication.

</details>


### [49] [Lower Bounds on Tree Covers](https://arxiv.org/abs/2508.10376)
*Yu Chen,Zihan Tan,Hangyu Xu*

Main category: cs.DS

TL;DR: 本文研究n点度量空间的树覆盖，改进了树覆盖大小为常数k（k≥2）时的失真下界至Ω(n^(1/2^(k - 1)))。


<details>
  <summary>Details</summary>
Motivation: 树覆盖在图算法中至关重要，当前常数k（k≥2）时的失真上下界存在差距，需缩小该差距。

Method: 对结构简单的网格状图进行新颖分析，运用组合不动点定理。

Result: 将常数k（k≥2）时树覆盖的失真下界提高到Ω(n^(1/2^(k - 1)))。

Conclusion: 所用方法不仅对树覆盖分析有效，还可能用于分析其他树状数据结构。

Abstract: Given an $n$-point metric space $(X,d_X)$, a tree cover $\mathcal{T}$ is a
set of $|\mathcal{T}|=k$ trees on $X$ such that every pair of vertices in $X$
has a low-distortion path in one of the trees in $\mathcal{T}$. Tree covers
have been playing a crucial role in graph algorithms for decades, and the
research focus is the construction of tree covers with small size $k$ and
distortion.
  When $k=1$, the best distortion is known to be $\Theta(n)$. For a constant
$k\ge 2$, the best distortion upper bound is $\tilde O(n^{\frac 1 k})$ and the
strongest lower bound is $\Omega(\log_k n)$, leaving a gap to be closed. In
this paper, we improve the lower bound to $\Omega(n^{\frac{1}{2^{k-1}}})$.
  Our proof is a novel analysis on a structurally simple grid-like graph, which
utilizes some combinatorial fixed-point theorems. We believe that they will
prove useful for analyzing other tree-like data structures as well.

</details>


### [50] [On Fixed-Parameter Tractability of Weighted 0-1 Timed Matching Problem on Temporal Graphs](https://arxiv.org/abs/2508.10562)
*Rinku Kumar,Bodhisatwa Mazumdar,Subhrangsu Mandal*

Main category: cs.DS

TL;DR: 本文研究时间图的最大0 - 1定时匹配问题，证明在有界树宽底层图上该问题仍NP完全、按解大小参数化时W[1]难，并给出按最大顶点度和底层图树宽参数化的FPT算法。


<details>
  <summary>Details</summary>
Motivation: 时间图用于建模系统实体关系随时间的演变，研究最大0 - 1定时匹配问题的固定参数可处理性。

Method: 理论证明该问题在不同条件下的复杂度，设计固定参数可处理算法。

Result: 证明在底层静态图有界树宽时问题仍NP完全，按解大小参数化时W[1]难，给出按最大顶点度和底层图树宽参数化的FPT算法。

Conclusion: 最大0 - 1定时匹配问题在某些条件下复杂度高，但按最大顶点度和底层图树宽参数化时存在固定参数可处理算法。

Abstract: Temporal graphs are introduced to model systems where the relationships among
the entities of the system evolve over time. In this paper, we consider the
temporal graphs where the edge set changes with time and all the changes are
known a priori. The underlying graph of a temporal graph is a static graph
consisting of all the vertices and edges that exist for at least one timestep
in the temporal graph. The concept of 0-1 timed matching in temporal graphs was
introduced by Mandal and Gupta [DAM2022] as an extension of the matching
problem in static graphs. A 0-1 timed matching of a temporal graph is a
non-overlapping subset of the edge set of that temporal graph. The problem of
finding the maximum 0-1 timed matching is proved to be NP-complete on multiple
classes of temporal graphs. We study the fixed-parameter tractability of the
maximum 0-1 timed matching problem. We prove that the problem remains to be
NP-complete even when the underlying static graph of the temporal graph has a
bounded treewidth. Furthermore, we establish that the problem is W[1]-hard when
parameterized by the solution size. Finally, we present a fixed-parameter
tractable (FPT) algorithm to address the problem when the problem is
parameterized by the maximum vertex degree and the treewidth of the underlying
graph of the temporal graph.

</details>


### [51] [Spirals and Beyond: Competitive Plane Search with Multi-Speed Agents](https://arxiv.org/abs/2508.10793)
*Konstantinos Georgiou,Caleb Jones,Matthew Madej*

Main category: cs.DS

TL;DR: 研究多速度移动智能体在平面搜索隐藏目标的问题，给出对称螺旋算法及上界，提出多速度智能体框架，证明速度均值影响搜索成本，还给出锥内搜索上界及混合策略。


<details>
  <summary>Details</summary>
Motivation: 最小化多速度移动智能体在平面搜索隐藏目标的最坏情况搜索时间，优化搜索成本。

Method: 从单速度智能体拓展到多速度智能体，给出对称螺旋算法，选择智能体子集并分配螺旋轨迹，设计混合螺旋 - 方向策略。

Result: 给出搜索成本上界，证明多速度智能体速度均值与搜索成本关系，新的锥内搜索上界及混合策略表现更好。

Conclusion: 螺旋轨迹在多速度场景可能非最优，慢智能体可能需排除以优化搜索成本。

Abstract: We consider the problem of minimizing the worst-case search time for a hidden
point target in the plane using multiple mobile agents of differing speeds, all
starting from a common origin. The search time is normalized by the target's
distance to the origin, following the standard convention in competitive
analysis. The goal is to minimize the maximum such normalized time over all
target locations, the search cost. As a base case, we extend the known result
for a single unit-speed agent, which achieves an optimal cost of about
$\mathcal{U}_1 = 17.28935$ via a logarithmic spiral, to $n$ unit-speed agents.
We give a symmetric spiral-based algorithm where each agent follows a
logarithmic spiral offset by equal angular phases. This yields a search cost
independent of which agent finds the target. We provide a closed-form upper
bound $\mathcal{U}_n$ for this setting, which we use in our general result. Our
main contribution is an upper bound on the worst-case normalized search time
for $n$ agents with arbitrary speeds. We give a framework that selects a subset
of agents and assigns spiral-type trajectories with speed-dependent angular
offsets, again making the search cost independent of which agent reaches the
target. A corollary shows that $n$ multi-speed agents (fastest speed 1) can
beat $k$ unit-speed agents (cost below $\mathcal{U}_k$) if the geometric mean
of their speeds exceeds $\mathcal{U}_n / \mathcal{U}_k$. This means slow agents
may be excluded if they lower the mean too much, motivating non-spiral
algorithms. We also give new upper bounds for point search in cones and conic
complements using a single unit-speed agent. These are then used to design
hybrid spiral-directional strategies, which outperform the spiral-based
algorithms when some agents are slow. This suggests that spiral-type
trajectories may not be optimal in the general multi-speed setting.

</details>


### [52] [Competitively Consistent Clustering](https://arxiv.org/abs/2508.10800)
*Niv Buchbinder,Roie Levin,Yue Yang*

Main category: cs.DS

TL;DR: 研究全动态一致聚类问题，设计算法维护近似最优解并限制资源消耗，通过降维到Positive Body Chasing框架获结果，给出对数下界证明近似最优。


<details>
  <summary>Details</summary>
Motivation: 在全动态一致聚类中，数据点不断增减，目标是在最小化资源消耗（中心增减总数）的同时，始终维护近似最优的聚类解决方案。

Method: 将问题降维到Positive Body Chasing框架，对该框架得到的分数解进行舍入，以保留近似和资源消耗保证。

Result: 设计的算法能在所有时间维护$O(\beta)$近似解，总资源消耗受限于$O(\log |F| \log \Delta) \cdot \text{OPT}_\text{rec}^{\beta}$，并给出对数下界。

Conclusion: 算法的界几乎是紧的，具有较好的性能。

Abstract: In fully-dynamic consistent clustering, we are given a finite metric space
$(M,d)$, and a set $F\subseteq M$ of possible locations for opening centers.
Data points arrive and depart, and the goal is to maintain an approximately
optimal clustering solution at all times while minimizing the recourse, the
total number of additions/deletions of centers over time. Specifically, we
study fully dynamic versions of the classical $k$-center, facility location,
and $k$-median problems. We design algorithms that, given a parameter
$\beta\geq 1$, maintain an $O(\beta)$-approximate solution at all times, and
whose total recourse is bounded by $O(\log |F| \log \Delta) \cdot
\text{OPT}_\text{rec}^{\beta}$. Here $\text{OPT}_\text{rec}^{\beta}$ is the
minimal recourse of an offline algorithm that maintains a $\beta$-approximate
solution at all times, and $\Delta$ is the metric aspect ratio. Finally, while
we compare the performance of our algorithms to an optimal solution that
maintains $k$ centers, our algorithms are allowed to use slightly more than $k$
centers. We obtain our results via a reduction to the recently proposed
Positive Body Chasing framework of [Bhattacharya, Buchbinder, Levin, Saranurak,
FOCS 2023], which we show gives fractional solutions to our clustering problems
online. Our contribution is to round these fractional solutions while
preserving the approximation and recourse guarantees. We complement our
positive results with logarithmic lower bounds which show that our bounds are
nearly tight.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [53] [FPT-Approximability of Stable Matching Problems](https://arxiv.org/abs/2508.10129)
*Jiehua Chen,Sanjukta Roy,Sofia Simola*

Main category: cs.GT

TL;DR: 研究三个稳定匹配优化问题的参数化近似性，前两个问题对β难以任何函数近似，最后一个问题针对“有偏好平局的代理数量”给出FPT - 近似方案。


<details>
  <summary>Details</summary>
Motivation: 探究三个与稳定匹配相关的优化问题（Min - BP - SMI、Min - BP - SRI、Max - SMTI）的参数化近似性。

Method: 分析已知问题复杂度，通过理论证明得出结论。

Result: 前两个问题对β难以任何函数近似，除非FPT = W[1]，不存在针对β的FPT - 近似方案；最后一个问题针对“有偏好平局的代理数量”给出FPT - 近似方案。

Conclusion: 明确了三个稳定匹配优化问题在不同参数下的近似性情况。

Abstract: We study parameterized approximability of three optimization problems related
to stable matching: (1) Min-BP-SMI: Given a stable marriage instance and a
number k, find a size-at-least-k matching that minimizes the number $\beta$ of
blocking pairs; (2) Min-BP-SRI: Given a stable roommates instance, find a
matching that minimizes the number $\beta$ of blocking pairs; (3) Max-SMTI:
Given a stable marriage instance with preferences containing ties, find a
maximum-size stable matching.
  The first two problems are known to be NP-hard to approximate to any constant
factor and W[1]-hard with respect to $\beta$, making the existence of an EPTAS
or FPT-algorithms unlikely. We show that they are W[1]-hard with respect to
$\beta$ to approximate to any function of $\beta$. This means that unless
FPT=W[1], there is no FPT-approximation scheme for the parameter $\beta$. The
last problem (Max-SMTI) is known to be NP-hard to approximate to factor-29/33
and W[1]-hard with respect to the number of ties. We complement this and
present an FPT-approximation scheme for the parameter "number of agents with
ties".

</details>


### [54] [Contested Route Planning](https://arxiv.org/abs/2508.10189)
*Jakub Černý,Garud Iyengar,Christian Kroer*

Main category: cs.GT

TL;DR: 本文构建博弈论模型解决有对手干扰的物流路由问题，引入随机性使方案难预测，用双预言机框架实现秒级计算，在真实场景验证有效。


<details>
  <summary>Details</summary>
Motivation: 解决有对手干扰下的物流最优路由问题，且克服基本鲁棒确定性路由计划可预测性的局限。

Method: 将路由规划建模为两人零和博弈引入随机性，使用双预言机框架进行计算，框架可容纳专业路由算法作为预言机。

Result: 计算时间达秒级，在真实场景中能有效扩展到实际问题规模。

Conclusion: 该方法操作可行，明确建模对手能力能显著提升效果。

Abstract: We consider the problem of routing for logistics purposes, in a contested
environment where an adversary attempts to disrupt the vehicle along the chosen
route. We construct a game-theoretic model that captures the problem of optimal
routing in such an environment. While basic robust deterministic routing plans
are already challenging to devise, they tend to be predictable, which can limit
their effectiveness. By introducing calculated randomness via modeling the
route planning process as a two-player zero-sum game, we compute immediately
deployable plans that are diversified and harder to anticipate. Although
solving the game exactly is intractable in theory, our use of the double-oracle
framework enables us to achieve computation times on the order of seconds,
making the approach operationally viable. In particular, the framework is
modular enough to accommodate specialized routing algorithms as oracles. We
evaluate our method on real-world scenarios, showing that it scales effectively
to realistic problem sizes and significantly benefits from explicitly modeling
the adversary's capabilities, as demonstrated through ablation studies and
comparisons with baseline approaches.

</details>


### [55] [Spatial Branch-and-Bound for Computing Multiplayer Nash Equilibrium](https://arxiv.org/abs/2508.10204)
*Jakub Černý,Shuvomoy Das Gupta,Christian Kroer*

Main category: cs.GT

TL;DR: 文章提出将纳什均衡计算问题转化为多项式互补问题，并开发空间分支定界算法，实证显示该算法优于现有完整方法。


<details>
  <summary>Details</summary>
Motivation: 现有计算多人正规形式博弈中纳什均衡的算法存在可扩展性差或缺乏收敛到真实均衡的保证等问题。

Method: 将纳什均衡计算问题表述为多项式互补问题，并开发完整且合理的空间分支定界算法，进行定性分析并说明近似解与计算近似纳什均衡的关系。

Result: 实证评估表明该算法显著优于现有的完整方法。

Conclusion: 所提出的算法在计算多人正规形式博弈的纳什均衡上有较好表现，优于现有完整方法。

Abstract: Equilibria of realistic multiplayer games constitute a key solution concept
both in practical applications, such as online advertising auctions and
electricity markets, and in analytical frameworks used to study strategic
voting in elections or assess policy impacts in integrated assessment models.
However, efficiently computing these equilibria requires games to have a
carefully designed structure and satisfy numerous restrictions; otherwise, the
computational complexity becomes prohibitive. In particular, finding even
approximate Nash equilibria in general-sum normal-form games with two or more
players is known to be PPAD-complete. Current state-of-the-art algorithms for
computing Nash equilibria in multiplayer normal-form games either suffer from
poor scalability due to their reliance on non-convex optimization solvers, or
lack guarantees of convergence to a true equilibrium. In this paper, we propose
a formulation of the Nash equilibrium computation problem as a polynomial
complementarity problem and develop a complete and sound spatial
branch-and-bound algorithm based on this formulation. We provide a qualitative
analysis arguing why one should expect our approach to perform well, and show
the relationship between approximate solutions to our formulation and that of
computing an approximate Nash equilibrium. Empirical evaluations demonstrate
that our algorithm substantially outperforms existing complete methods.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [56] [Bridging Modality Gaps in e-Commerce Products via Vision-Language Alignment](https://arxiv.org/abs/2508.10116)
*Yipeng Zhang,Hongju Yu,Aritra Mandal,Canran Xu,Qunzhi Zhou,Zhe Wu*

Main category: cs.IR

TL;DR: 提出OPAL框架用微调多模态大语言模型从图像生成商品描述，评估显示优于基线方法，推动电商平台内容生成。


<details>
  <summary>Details</summary>
Motivation: 手动或半手动输入商品信息质量不一、有错误且效率低，现有检索方法有不足，需新方法从图像生成准确描述。

Method: 提出OPAL框架，引入两种数据细化方法，用视觉指令调优和直接偏好优化微调MLLM。

Result: 在真实电商数据集上评估，OPAL在描述质量和模式完成率上始终优于基线方法。

Conclusion: OPAL有效弥合视觉和文本模态差距，推动电商平台自动列表优化和高质量内容生成。

Abstract: Item information, such as titles and attributes, is essential for effective
user engagement in e-commerce. However, manual or semi-manual entry of
structured item specifics often produces inconsistent quality, errors, and slow
turnaround, especially for Customer-to-Customer sellers. Generating accurate
descriptions directly from item images offers a promising alternative. Existing
retrieval-based solutions address some of these issues but often miss
fine-grained visual details and struggle with niche or specialized categories.
  We propose Optimized Preference-Based AI for Listings (OPAL), a framework for
generating schema-compliant, high-quality item descriptions from images using a
fine-tuned multimodal large language model (MLLM). OPAL addresses key
challenges in multimodal e-commerce applications, including bridging modality
gaps and capturing detailed contextual information. It introduces two data
refinement methods: MLLM-Assisted Conformity Enhancement, which ensures
alignment with structured schema requirements, and LLM-Assisted Contextual
Understanding, which improves the capture of nuanced and fine-grained
information from visual inputs.
  OPAL uses visual instruction tuning combined with direct preference
optimization to fine-tune the MLLM, reducing hallucinations and improving
robustness across different backbone architectures. We evaluate OPAL on
real-world e-commerce datasets, showing that it consistently outperforms
baseline methods in both description quality and schema completion rates. These
results demonstrate that OPAL effectively bridges the gap between visual and
textual modalities, delivering richer, more accurate, and more consistent item
descriptions. This work advances automated listing optimization and supports
scalable, high-quality content generation in e-commerce platforms.

</details>


### [57] [DS4RS: Community-Driven and Explainable Dataset Search Engine for Recommender System Research](https://arxiv.org/abs/2508.10238)
*Xinyang Shao,Tri Kurniawan Wijaya*

Main category: cs.IR

TL;DR: 提出面向推荐系统研究的社区驱动可解释数据集搜索引擎，支持多属性语义搜索，促进社区参与，提升数据集可发现性和搜索可解释性，平台公开可用。


<details>
  <summary>Details</summary>
Motivation: 解决推荐系统研究中因数据源分散、元数据不一致，难以找到匹配特定任务或领域数据集的问题。

Method: 构建社区驱动且可解释的数据集搜索引擎，支持多属性语义搜索，允许用户在公共仓库贡献标准化数据集元数据。

Result: 系统提升了数据集可发现性和搜索可解释性，促进了研究复现效率，平台公开可访问。

Conclusion: 该系统有助于推荐系统研究中数据集的获取和研究复现。

Abstract: Accessing suitable datasets is critical for research and development in
recommender systems. However, finding datasets that match specific
recommendation task or domains remains a challenge due to scattered sources and
inconsistent metadata. To address this gap, we propose a community-driven and
explainable dataset search engine tailored for recommender system research. Our
system supports semantic search across multiple dataset attributes, such as
dataset names, descriptions, and recommendation domain, and provides
explanations of search relevance to enhance transparency. The system encourages
community participation by allowing users to contribute standardized dataset
metadata in public repository. By improving dataset discoverability and search
interpretability, the system facilitates more efficient research reproduction.
The platform is publicly available at: https://ds4rs.com.

</details>


### [58] [Clicks Versus Conversion: Choosing a Recommender's Training Objective in E-Commerce](https://arxiv.org/abs/2508.10377)
*Michael Weiss,Robert Rosenbach,Christian Eggenberger*

Main category: cs.IR

TL;DR: 本文通过在线A/B测试比较优化CTR和OSR目标的效果，发现优化OSR带来的GMV提升比优化CTR大五倍多，且不影响新品发现，还揭示了不同目标的特征重要性差异。


<details>
  <summary>Details</summary>
Motivation: 比较优化CTR和ACR、OSR等目标对电商业务的影响，探索更优的优化目标以提升业务效果。

Method: 采用在线A/B测试方法。

Result: 优化OSR带来的GMV提升比优化CTR大五倍多，且不影响新品发现，同时得出不同目标的特征重要性差异。

Conclusion: 在电商场景中，优化OSR比优化CTR更能有效提升GMV。

Abstract: Ranking product recommendations to optimize for a high click-through rate
(CTR) or for high conversion, such as add-to-cart rate (ACR) and
Order-Submit-Rate (OSR, view-to-purchase conversion) are standard practices in
e-commerce. Optimizing for CTR appears like a straightforward choice: Training
data (i.e., click data) are simple to collect and often available in large
quantities. Additionally, CTR is used far beyond e-commerce, making it a
generalist, easily implemented option. ACR and OSR, on the other hand, are more
directly linked to a shop's business goals, such as the Gross Merchandise Value
(GMV). In this paper, we compare the effects of using either of these
objectives using an online A/B test. Among our key findings, we demonstrate
that in our shops, optimizing for OSR produces a GMV uplift more than five
times larger than when optimizing for CTR, without sacrificing new product
discovery. Our results also provide insights into the different feature
importances for each of the objectives.

</details>


### [59] [Proxy Model-Guided Reinforcement Learning for Client Selection in Federated Recommendation](https://arxiv.org/abs/2508.10401)
*Liang Qu,Jianxin Li,Wei Yuan,Penghui Ruan,Yuhui Shi,Hongzhi Yin*

Main category: cs.IR

TL;DR: 提出适用于联邦推荐中客户端选择的ProxyRL - FRS框架，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有联邦推荐系统框架采用全随机客户端选择策略，忽略用户数据统计异质性，通用联邦学习客户端选择方法不适用于推荐场景。

Method: 提出ProxyRL - FRS框架，引入ProxyNCF模型提供轻量级贡献估计，设计SA强化学习代理根据代理估计贡献选择客户端。

Result: 在公共推荐数据集上的实验表明ProxyRL - FRS有效。

Conclusion: ProxyRL - FRS能解决现有联邦推荐系统客户端选择问题，提升模型性能。

Abstract: Federated recommender systems have emerged as a promising privacy-preserving
paradigm, enabling personalized recommendation services without exposing users'
raw data. By keeping data local and relying on a central server to coordinate
training across distributed clients, FedRSs protect user privacy while
collaboratively learning global models. However, most existing FedRS frameworks
adopt fully random client selection strategy in each training round,
overlooking the statistical heterogeneity of user data arising from diverse
preferences and behavior patterns, thereby resulting in suboptimal model
performance. While some client selection strategies have been proposed in the
broader federated learning literature, these methods are typically designed for
generic tasks and fail to address the unique challenges of recommendation
scenarios, such as expensive contribution evaluation due to the large number of
clients, and sparse updates resulting from long-tail item distributions. To
bridge this gap, we propose ProxyRL-FRS, a proxy model-guided reinforcement
learning framework tailored for client selection in federated recommendation.
Specifically, we first introduce ProxyNCF, a dual-branch model deployed on each
client, which augments standard Neural Collaborative Filtering with an
additional proxy model branch that provides lightweight contribution
estimation, thus eliminating the need for expensive per-round local training
traditionally required to evaluate a client's contribution. Furthermore, we
design a staleness-aware SA reinforcement learning agent that selects clients
based on the proxy-estimated contribution, and is guided by a reward function
balancing recommendation accuracy and embedding staleness, thereby enriching
the update coverage of item embeddings. Experiments conducted on public
recommendation datasets demonstrate the effectiveness of ProxyRL-FRS.

</details>


### [60] [Semantic IDs for Joint Generative Search and Recommendation](https://arxiv.org/abs/2508.10478)
*Gustavo Penha,Edoardo D'Amico,Marco De Nadai,Enrico Palumbo,Alexandre Tamborrino,Ali Vardasbi,Max Lefarov,Shawn Lin,Timothy Heath,Francesco Fabbri,Hugues Bouchard*

Main category: cs.IR

TL;DR: 本文探索统一模型中构建在搜索和推荐任务都表现良好的语义ID的方法，对比多种策略，发现用双编码器微调后构建统一语义ID空间是有效方案。


<details>
  <summary>Details</summary>
Motivation: 现有任务特定嵌入模型在联合场景泛化性不佳，需构建在搜索和推荐任务都表现好的语义ID。

Method: 对比一系列构建语义ID的策略，研究任务特定和跨任务方法，以及联合模型中各任务是否应有自己的语义ID令牌。

Result: 使用在搜索和推荐任务上微调的双编码器模型获取物品嵌入，构建统一语义ID空间能在两个任务中取得良好性能。

Conclusion: 希望研究成果能激发更多关于通用、语义化ID方案的后续工作，为下一代统一生成式推荐架构提供参考。

Abstract: Generative models powered by Large Language Models (LLMs) are emerging as a
unified solution for powering both recommendation and search tasks. A key
design choice in these models is how to represent items, traditionally through
unique identifiers (IDs) and more recently with Semantic IDs composed of
discrete codes, obtained from embeddings. While task-specific embedding models
can improve performance for individual tasks, they may not generalize well in a
joint setting. In this paper, we explore how to construct Semantic IDs that
perform well both in search and recommendation when using a unified model. We
compare a range of strategies to construct Semantic IDs, looking into
task-specific and cross-tasks approaches, and also whether each task should
have its own semantic ID tokens in a joint search and recommendation generative
model. Our results show that using a bi-encoder model fine-tuned on both search
and recommendation tasks to obtain item embeddings, followed by the
construction of a unified Semantic ID space provides an effective trade-off,
enabling strong performance in both tasks. We hope these findings spark
follow-up work on generalisable, semantically grounded ID schemes and inform
the next wave of unified generative recommender architectures.

</details>


### [61] [Efficient Patent Searching Using Graph Transformers](https://arxiv.org/abs/2508.10496)
*Krzysztof Daniell,Igor Buzhinsky,Sebastian Björkqvist*

Main category: cs.IR

TL;DR: 提出基于图Transformer的专利搜索密集检索方法，在检索质量和计算效率上有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有专利文献数量多，需细致比较确定新颖性，寻找相关现有技术有挑战，需要准确搜索引擎加速流程。

Method: 提出基于图Transformer的密集检索方法，用图表示发明特征及关系，用专利局审查员的现有技术引用作为相关性信号训练模型。

Result: 构建的搜索引擎能模拟专业专利审查员识别相关文件，与公开文本嵌入模型对比，在现有技术检索质量和计算效率上有显著提升。

Conclusion: 所提出的基于图Transformer的方法在专利搜索中有效，能提高检索质量和计算效率。

Abstract: Finding relevant prior art is crucial when deciding whether to file a new
patent application or invalidate an existing patent. However, searching for
prior art is challenging due to the large number of patent documents and the
need for nuanced comparisons to determine novelty. An accurate search engine is
therefore invaluable for speeding up the process. We present a Graph
Transformer-based dense retrieval method for patent searching where each
invention is represented by a graph describing its features and their
relationships. Our model processes these invention graphs and is trained using
prior art citations from patent office examiners as relevance signals. Using
graphs as input significantly improves the computational efficiency of
processing long documents, while leveraging examiner citations allows the model
to learn domain-specific similarities beyond simple text-based matching. The
result is a search engine that emulates how professional patent examiners
identify relevant documents. We compare our approach against publicly available
text embedding models and show substantial improvements in both prior art
retrieval quality and computational efficiency.

</details>


### [62] [DAS: Dual-Aligned Semantic IDs Empowered Industrial Recommender System](https://arxiv.org/abs/2508.10584)
*Wencai Ye,Mingjie Sun,Shaoyun Shi,Peng Wang,Wenjin Wu,Peng Jiang*

Main category: cs.IR

TL;DR: 提出单阶段双对齐语义ID（DAS）方法解决多模态推荐系统中语义ID与协同信号对齐问题，经实验验证有效并已在快手应用部署。


<details>
  <summary>Details</summary>
Motivation: 现有语义ID缺乏协同信号，与下游推荐目标不一致，且现有对齐机制两阶段框架存在信息损失和策略不灵活的问题。

Method: 提出单阶段DAS方法同时优化量化和对齐，采用多视图对比对齐和双学习两种创新方法。

Result: 通过离线实验和在线A/B测试验证了DAS的有效性，已在快手应用多个广告场景部署，服务超4亿日活用户。

Conclusion: DAS方法能避免两阶段方法的信息损失，有效实现语义ID与协同信号的对齐。

Abstract: Semantic IDs are discrete identifiers generated by quantizing the Multi-modal
Large Language Models (MLLMs) embeddings, enabling efficient multi-modal
content integration in recommendation systems. However, their lack of
collaborative signals results in a misalignment with downstream discriminative
and generative recommendation objectives. Recent studies have introduced
various alignment mechanisms to address this problem, but their two-stage
framework design still leads to two main limitations: (1) inevitable
information loss during alignment, and (2) inflexibility in applying adaptive
alignment strategies, consequently constraining the mutual information
maximization during the alignment process. To address these limitations, we
propose a novel and flexible one-stage Dual-Aligned Semantic IDs (DAS) method
that simultaneously optimizes quantization and alignment, preserving semantic
integrity and alignment quality while avoiding the information loss typically
associated with two-stage methods. Meanwhile, DAS achieves more efficient
alignment between the semantic IDs and collaborative signals, with the
following two innovative and effective approaches: (1) Multi-view Constrative
Alignment: To maximize mutual information between semantic IDs and
collaborative signals, we first incorporate an ID-based CF debias module, and
then design three effective contrastive alignment methods: dual user-to-item
(u2i), dual item-to-item/user-to-user (i2i/u2u), and dual co-occurrence
item-to-item/user-to-user (i2i/u2u). (2) Dual Learning: By aligning the dual
quantizations of users and ads, the constructed semantic IDs for users and ads
achieve stronger alignment. Finally, we conduct extensive offline experiments
and online A/B tests to evaluate DAS's effectiveness, which is now successfully
deployed across various advertising scenarios at Kuaishou App, serving over 400
million users daily.

</details>


### [63] [FuXi-β: Towards a Lightweight and Fast Large-Scale Generative Recommendation Model](https://arxiv.org/abs/2508.10615)
*Yufei Ye,Wei Guo,Hao Wang,Hong Zhu,Yuyang Ye,Yong Liu,Huifeng Guo,Ruiming Tang,Defu Lian,Enhong Chen*

Main category: cs.IR

TL;DR: 研究生成式推荐模型效率瓶颈，提出新框架并应用于FuXi-α得到FuXi-β，实验显示其性能和加速效果佳。


<details>
  <summary>Details</summary>
Motivation: 解决自回归生成式推荐器训练和推理效率问题，降低延迟和成本。

Method: 分析HSTU和FuXi-α的效率瓶颈，提出新框架，包括Functional Relative Attention Bias和Attention-Free Token Mixer模块，应用于FuXi-α得到FuXi-β。

Result: FuXi-β在多数据集上优于先前模型，相比FuXi-α显著加速，在NDCG@10指标上提升27% - 47%。

Conclusion: 提出的新框架有效，FuXi-β性能优越且符合缩放定律，代码公开。

Abstract: Scaling laws for autoregressive generative recommenders reveal potential for
larger, more versatile systems but mean greater latency and training costs. To
accelerate training and inference, we investigated the recent generative
recommendation models HSTU and FuXi-$\alpha$, identifying two efficiency
bottlenecks: the indexing operations in relative temporal attention bias and
the computation of the query-key attention map. Additionally, we observed that
relative attention bias in self-attention mechanisms can also serve as
attention maps. Previous works like Synthesizer have shown that alternative
forms of attention maps can achieve similar performance, naturally raising the
question of whether some attention maps are redundant. Through empirical
experiments, we discovered that using the query-key attention map might degrade
the model's performance in recommendation tasks. To address these bottlenecks,
we propose a new framework applicable to Transformer-like recommendation
models. On one hand, we introduce Functional Relative Attention Bias, which
avoids the time-consuming operations of the original relative attention bias,
thereby accelerating the process. On the other hand, we remove the query-key
attention map from the original self-attention layer and design a new
Attention-Free Token Mixer module. Furthermore, by applying this framework to
FuXi-$\alpha$, we introduce a new model, FuXi-$\beta$. Experiments across
multiple datasets demonstrate that FuXi-$\beta$ outperforms previous
state-of-the-art models and achieves significant acceleration compared to
FuXi-$\alpha$, while also adhering to the scaling law. Notably, FuXi-$\beta$
shows an improvement of 27% to 47% in the NDCG@10 metric on large-scale
industrial datasets compared to FuXi-$\alpha$. Our code is available in a
public repository: https://github.com/USTC-StarTeam/FuXi-beta

</details>


### [64] [Hypercomplex Prompt-aware Multimodal Recommendation](https://arxiv.org/abs/2508.10753)
*Zheyu Chen,Jinfeng Xu,Hewei Wang,Shuo Yang,Zitong Wan,Haibo Hu*

Main category: cs.IR

TL;DR: 提出HPMRec框架解决现代推荐系统多模态表示学习问题，实验显示其达到了最先进的推荐性能。


<details>
  <summary>Details</summary>
Motivation: 现代推荐系统在处理信息过载和多模态表示学习方面存在局限，现有方法有无法丰富表示多模态特征、忽略模态间非线性关联、无法动态缓解图卷积网络过平滑问题等缺点。

Method: 提出HPMRec框架，利用多分量超复数嵌入增强多模态特征表示多样性，采用超复数乘法建立非线性跨模态交互，引入提示感知补偿机制缓解过平滑问题，设计自监督学习任务增强表示多样性和对齐不同模态。

Result: 在四个公开数据集上的大量实验表明，HPMRec达到了最先进的推荐性能。

Conclusion: HPMRec框架有效解决了现有推荐系统的局限，具有良好的推荐性能。

Abstract: Modern recommender systems face critical challenges in handling information
overload while addressing the inherent limitations of multimodal representation
learning. Existing methods suffer from three fundamental limitations: (1)
restricted ability to represent rich multimodal features through a single
representation, (2) existing linear modality fusion strategies ignore the deep
nonlinear correlations between modalities, and (3) static optimization methods
failing to dynamically mitigate the over-smoothing problem in graph
convolutional network (GCN). To overcome these limitations, we propose HPMRec,
a novel Hypercomplex Prompt-aware Multimodal Recommendation framework, which
utilizes hypercomplex embeddings in the form of multi-components to enhance the
representation diversity of multimodal features. HPMRec adopts the hypercomplex
multiplication to naturally establish nonlinear cross-modality interactions to
bridge semantic gaps, which is beneficial to explore the cross-modality
features. HPMRec also introduces the prompt-aware compensation mechanism to aid
the misalignment between components and modality-specific features loss, and
this mechanism fundamentally alleviates the over-smoothing problem. It further
designs self-supervised learning tasks that enhance representation diversity
and align different modalities. Extensive experiments on four public datasets
show that HPMRec achieves state-of-the-art recommendation performance.

</details>


### [65] [CrossDenoise: Denoising Implicit Feedback via a Lightweight Entity-Aware Synergistic Framework](https://arxiv.org/abs/2508.10851)
*Ze Liu,Xianquan Wang,Shuochen Liu,Jie Ma,Huibo Xu,Yupeng Han,Zhe Yang,Kai Zhang,Longfei Li,Jun Zhou*

Main category: cs.IR

TL;DR: 现有去噪策略有局限，提出CrossDenoise框架，经多数据集实验验证其优于基线，能有效去噪且开销小。


<details>
  <summary>Details</summary>
Motivation: 现有推荐系统去噪策略存在忽视实体感知建模、计算开销大、超参数调整过多等问题，限制实际应用，需新的去噪方法。

Method: 将噪声估计分解为用户、物品和交互特定因素，通过平均训练损失的基于排名的线性映射计算实体声誉因子，与个体损失的经验累积分布函数导出的交互级权重融合。

Result: 在多个数据集和模型骨干上实验，CrossDenoise显著优于现有基线，如在Yelp上用NeuMF实现NDCG@50最多提升27.01%，计算和内存开销可忽略不计。

Conclusion: CrossDenoise能有效分离干净和噪声样本，超参数设置变化时仍稳健，是去噪隐式反馈的实用可扩展解决方案。

Abstract: Recommender systems heavily rely on implicit feedback, which is inherently
noisy due to false positives and negatives, severely degrading recommendation
accuracy. Existing denoising strategies often overlook entity-aware modeling,
suffer from high computational overhead, or demand excessive hyperparameter
tuning, limiting their real-world applicability. We propose CrossDenoise, a
novel and lightweight framework that addresses these challenges by
disentangling noise estimation into user-, item-, and interaction-specific
factors. Leveraging empirical observations that show significant heterogeneity
in user and item noise propensities, CrossDenoise computes entity reputation
factors (user/item reliability) via a rank-based linear mapping of average
training losses. These are fused with interaction-level weights derived from an
empirical cumulative distribution function (ECDF) of individual losses. This
design is model-agnostic, computationally efficient, and requires only two
intuitive hyperparameters. Extensive experiments on ML-1M, Yelp, and
Amazon-book datasets, across GMF, NeuMF, and CDAE backbones, demonstrate that
CrossDenoise consistently and significantly outperforms state-of-the-art
baselines. For instance, it achieves up to 27.01% NDCG@50 gain on Yelp with
NeuMF, while incurring negligible computational and memory overhead. Our
analysis confirms that CrossDenoise effectively separates clean from noisy
samples and remains robust under varied hyperparameter settings. It offers a
practical and scalable solution for denoising implicit feedback.

</details>


### [66] [Personalized Product Search Ranking: A Multi-Task Learning Approach with Tabular and Non-Tabular Data](https://arxiv.org/abs/2508.09636)
*Lalitesh Morishetti,Abhay Kumar,Jonathan Scott,Kaushiki Nag,Gunjan Sharma,Shanu Vashishtha,Rahul Sridhar,Rohit Chatter,Kannan Achan*

Main category: cs.IR

TL;DR: 提出基于多任务学习框架优化个性化产品搜索排名的新模型架构，结合非表格数据和高级嵌入技术提升性能。


<details>
  <summary>Details</summary>
Motivation: 优化个性化产品搜索排名，处理混合数据类型。

Method: 采用多任务学习框架，集成表格和非表格数据，利用预训练TinyBERT模型和新采样技术，提出基于多种指标的可扩展相关性标注机制。

Result: 结合非表格数据和高级嵌入技术在多任务学习范式中显著提升模型性能，消融研究凸显纳入相关性标签等操作的好处。

Conclusion: 所提方法在改善个性化产品搜索排名方面有效。

Abstract: In this paper, we present a novel model architecture for optimizing
personalized product search ranking using a multi-task learning (MTL)
framework. Our approach uniquely integrates tabular and non-tabular data,
leveraging a pre-trained TinyBERT model for semantic embeddings and a novel
sampling technique to capture diverse customer behaviors. We evaluate our model
against several baselines, including XGBoost, TabNet, FT-Transformer, DCN-V2,
and MMoE, focusing on their ability to handle mixed data types and optimize
personalized ranking. Additionally, we propose a scalable relevance labeling
mechanism based on click-through rates, click positions, and semantic
similarity, offering an alternative to traditional human-annotated labels.
Experimental results show that combining non-tabular data with advanced
embedding techniques in multi-task learning paradigm significantly enhances
model performance. Ablation studies further underscore the benefits of
incorporating relevance labels, fine-tuning TinyBERT layers, and TinyBERT
query-product embedding interactions. These results demonstrate the
effectiveness of our approach in achieving improved personalized product search
ranking.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [67] [OpenFPL: An open-source forecasting method rivaling state-of-the-art Fantasy Premier League services](https://arxiv.org/abs/2508.09992)
*Daniel Groos*

Main category: cs.LG

TL;DR: 提出开源预测方法OpenFPL进行球员表现预测，其准确性可与商业服务媲美，在高回报球员预测上更优。


<details>
  <summary>Details</summary>
Motivation: 当前高准确性的球员表现预测服务多为商业服务，内部机制不公开且依赖专有数据，本文希望让更多人能获取高准确性的预测。

Method: 开发基于公开数据的开源预测方法OpenFPL，采用特定位置的集成模型，在2020 - 2021至2023 - 2024四个赛季的数据上进行优化。

Result: OpenFPL在2024 - 2025赛季数据上测试，准确性与领先商业服务相当，在高回报球员预测上超过商业基准。

Conclusion: OpenFPL在一、二、三周的预测期内表现良好，可支持长期转会规划和策略制定，也能为最后一天决策提供信息。

Abstract: Fantasy Premier League engages the football community in selecting the
Premier League players who will perform best from gameweek to gameweek. Access
to accurate performance forecasts gives participants an edge over competitors
by guiding expectations about player outcomes and reducing uncertainty in squad
selection. However, high-accuracy forecasts are currently limited to commercial
services whose inner workings are undisclosed and that rely on proprietary
data. This paper aims to democratize access to highly accurate forecasts of
player performance by presenting OpenFPL, an open-source Fantasy Premier League
forecasting method developed exclusively from public data. Comprising
position-specific ensemble models optimized on Fantasy Premier League and
Understat data from four previous seasons (2020-21 to 2023-24), OpenFPL
achieves accuracy comparable to a leading commercial service when tested
prospectively on data from the 2024-25 season. OpenFPL also surpasses the
commercial benchmark for high-return players ($>$ 2 points), which are most
influential for rank gains. These findings hold across one-, two-, and
three-gameweek forecast horizons, supporting long-term planning of transfers
and strategies while also informing final-day decisions.

</details>


### [68] [xRFM: Accurate, scalable, and interpretable feature learning models for tabular data](https://arxiv.org/abs/2508.10053)
*Daniel Beaglehole,David Holzmüller,Adityanarayanan Radhakrishnan,Mikhail Belkin*

Main category: cs.LG

TL;DR: 介绍xRFM算法，结合特征学习核机器与树结构，在回归和分类数据集上表现佳且有可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前表格数据预测任务最佳实践相对不变，近期对基于神经网络和特征学习方法开发表格数据先进方法有新兴趣。

Method: 引入xRFM算法，结合特征学习核机器与树结构。

Result: 与31种其他方法相比，xRFM在100个回归数据集上表现最佳，在200个分类数据集上有竞争力且优于GBDTs。

Conclusion: xRFM算法有效，能适应数据局部结构、处理大量训练数据，且具有可解释性。

Abstract: Inference from tabular data, collections of continuous and categorical
variables organized into matrices, is a foundation for modern technology and
science. Yet, in contrast to the explosive changes in the rest of AI, the best
practice for these predictive tasks has been relatively unchanged and is still
primarily based on variations of Gradient Boosted Decision Trees (GBDTs). Very
recently, there has been renewed interest in developing state-of-the-art
methods for tabular data based on recent developments in neural networks and
feature learning methods. In this work, we introduce xRFM, an algorithm that
combines feature learning kernel machines with a tree structure to both adapt
to the local structure of the data and scale to essentially unlimited amounts
of training data.
  We show that compared to $31$ other methods, including recently introduced
tabular foundation models (TabPFNv2) and GBDTs, xRFM achieves best performance
across $100$ regression datasets and is competitive to the best methods across
$200$ classification datasets outperforming GBDTs. Additionally, xRFM provides
interpretability natively through the Average Gradient Outer Product.

</details>


### [69] [A Personalized Exercise Assistant using Reinforcement Learning (PEARL): Results from a four-arm Randomized-controlled Trial](https://arxiv.org/abs/2508.10060)
*Amy Armento Lee,Narayan Hegde,Nina Deliu,Emily Rosenzweig,Arun Suggala,Sriram Lakshminarasimhan,Qian He,John Hernandez,Martin Seneviratne,Rahul Singh,Pradnesh Kalkar,Karthikeyan Shanmugam,Aravindan Raghuveer,Abhimanyu Singh,My Nguyen,James Taylor,Jatin Alla,Sofia S. Villar,Hulya Emir-Farinas*

Main category: cs.LG

TL;DR: 研究通过PEARL试验评估强化学习算法个性化运动提醒效果，发现该算法组运动活动增加显著。


<details>
  <summary>Details</summary>
Motivation: 身体不活动是全球健康挑战，移动健康干预有前景但开发和评估有方法学障碍，需探索有效干预方法。

Method: 开展大规模四臂随机对照试验，将13463名Fitbit用户随机分四组，分别给予不同干预，对7711名参与者进行主要分析。

Result: 强化学习组相比其他组在1个月和2个月时运动活动增加，1个月时日均步数增加显著，2个月时仍高于对照组。

Conclusion: 基于行为科学的可扩展强化学习方法在个性化数字健康运动干预方面有潜力。

Abstract: Consistent physical inactivity poses a major global health challenge. Mobile
health (mHealth) interventions, particularly Just-in-Time Adaptive
Interventions (JITAIs), offer a promising avenue for scalable, personalized
physical activity (PA) promotion. However, developing and evaluating such
interventions at scale, while integrating robust behavioral science, presents
methodological hurdles. The PEARL study was the first large-scale, four-arm
randomized controlled trial to assess a reinforcement learning (RL) algorithm,
informed by health behavior change theory, to personalize the content and
timing of PA nudges via a Fitbit app.
  We enrolled and randomized 13,463 Fitbit users into four study arms: control,
random, fixed, and RL. The control arm received no nudges. The other three arms
received nudges from a bank of 155 nudges based on behavioral science
principles. The random arm received nudges selected at random. The fixed arm
received nudges based on a pre-set logic from survey responses about PA
barriers. The RL group received nudges selected by an adaptive RL algorithm. We
included 7,711 participants in primary analyses (mean age 42.1, 86.3% female,
baseline steps 5,618.2).
  We observed an increase in PA for the RL group compared to all other groups
from baseline to 1 and 2 months. The RL group had significantly increased
average daily step count at 1 month compared to all other groups: control (+296
steps, p=0.0002), random (+218 steps, p=0.005), and fixed (+238 steps,
p=0.002). At 2 months, the RL group sustained a significant increase compared
to the control group (+210 steps, p=0.0122). Generalized estimating equation
models also revealed a sustained increase in daily steps in the RL group vs.
control (+208 steps, p=0.002). These findings demonstrate the potential of a
scalable, behaviorally-informed RL approach to personalize digital health
interventions for PA.

</details>


### [70] [Measuring Time Series Forecast Stability for Demand Planning](https://arxiv.org/abs/2508.10063)
*Steven Klee,Yuntian Xia*

Main category: cs.LG

TL;DR: 研究时间序列预测模型稳定性，发现集成模型可提高稳定性且不显著降低准确率，建议进一步研究生产系统中模型的预测稳定性。


<details>
  <summary>Details</summary>
Motivation: 生产系统中需求规划者更看重预测的一致性和稳定性，而现有实验多关注预测准确性，因此研究模型诱导的随机性。

Method: 对M5竞赛和Favorita杂货店销售的公开数据集上的先进预测模型进行案例研究，测量其稳定性和准确性。

Result: 集成模型在不显著降低（甚至提高）预测准确性的情况下提高了稳定性。

Conclusion: 建议对部署在生产系统中的模型的预测稳定性进行进一步研究。

Abstract: Time series forecasting is a critical first step in generating demand plans
for supply chains. Experiments on time series models typically focus on
demonstrating improvements in forecast accuracy over existing/baseline
solutions, quantified according to some accuracy metric. There is no doubt that
forecast accuracy is important; however in production systems, demand planners
often value consistency and stability over incremental accuracy improvements.
Assuming that the inputs have not changed significantly, forecasts that vary
drastically from one planning cycle to the next require high amounts of human
intervention, which frustrates demand planners and can even cause them to lose
trust in ML forecasting models. We study model-induced stochasticity, which
quantifies the variance of a set of forecasts produced by a single model when
the set of inputs is fixed. Models with lower variance are more stable.
  Recently the forecasting community has seen significant advances in forecast
accuracy through the development of deep machine learning models for time
series forecasting. We perform a case study measuring the stability and
accuracy of state-of-the-art forecasting models (Chronos, DeepAR, PatchTST,
Temporal Fusion Transformer, TiDE, and the AutoGluon best quality ensemble) on
public data sets from the M5 competition and Favorita grocery sales. We show
that ensemble models improve stability without significantly deteriorating (or
even improving) forecast accuracy. While these results may not be surprising,
the main point of this paper is to propose the need for further study of
forecast stability for models that are being deployed in production systems.

</details>


### [71] [Constrained Decoding of Diffusion LLMs with Context-Free Grammars](https://arxiv.org/abs/2508.10111)
*Niels Mündler,Jasper Dekoninck,Martin Vechev*

Main category: cs.LG

TL;DR: 提出首个适用于扩散模型的受限解码方法，可处理上下文无关文法表示的形式语言，实验效果好且计算开销实用。


<details>
  <summary>Details</summary>
Motivation: 现有受限解码方法不适用于扩散大语言模型在实际场景中的应用，需解决该问题。

Method: 将受限解码问题转化为更通用的加法填充问题，再转化为判断目标语言和正则语言交集是否为空的任务，并给出高效算法。

Result: 在多种应用中，该方法实现了近乎完美的句法正确性，同时保持或提高了功能正确性，计算开销实用。

Conclusion: 提出的受限解码方法能有效处理扩散模型在形式语言约束下的生成问题。

Abstract: Large language models (LLMs) have shown promising performance across diverse
domains. Many practical applications of LLMs, such as code completion and
structured data extraction, require adherence to syntactic constraints
specified by a formal language. Yet, due to their probabilistic nature, LLM
output is not guaranteed to adhere to such formal languages. Prior work has
proposed constrained decoding as a means to restrict LLM generation to
particular formal languages. However, existing works are not applicable to the
emerging paradigm of diffusion LLMs, when used in practical scenarios such as
the generation of formally correct C++ or JSON output. In this paper we address
this challenge and present the first constrained decoding method for diffusion
models, one that can handle formal languages captured by context-free grammars.
We begin by reducing constrained decoding to the more general additive
infilling problem, which asks whether a partial output can be completed to a
valid word in the target language. This problem also naturally subsumes the
previously unaddressed multi-region infilling constrained decoding. We then
reduce this problem to the task of deciding whether the intersection of the
target language and a regular language is empty and present an efficient
algorithm to solve it for context-free languages. Empirical results on various
applications, such as C++ code infilling and structured data extraction in
JSON, demonstrate that our method achieves near-perfect syntactic correctness
while consistently preserving or improving functional correctness. Importantly,
our efficiency optimizations ensure that the computational overhead remains
practical.

</details>


### [72] [Welfare-Centric Clustering](https://arxiv.org/abs/2508.10345)
*Claire Jie Zhang,Seyed A. Esmaeili,Jamie Morgenstern*

Main category: cs.LG

TL;DR: 本文基于福利中心聚类方法，对群体效用建模，提出新优化目标和算法，在多数据集上表现优于现有公平聚类基线。


<details>
  <summary>Details</summary>
Motivation: 传统公平聚类的公平概念可能导致不理想或不直观的聚类结果，Dickerson等人提倡福利中心聚类方法，本文在此基础上进一步研究。

Method: 基于距离和比例表示对群体效用建模，形式化罗尔斯（平等主义）和功利主义两个优化目标，引入新算法并给出理论保证。

Result: 在多个真实世界数据集上的实证评估表明，本文方法显著优于现有的公平聚类基线。

Conclusion: 基于福利中心聚类的方法有效，所提算法在公平聚类问题上表现良好。

Abstract: Fair clustering has traditionally focused on ensuring equitable group
representation or equalizing group-specific clustering costs. However,
Dickerson et al. (2025) recently showed that these fairness notions may yield
undesirable or unintuitive clustering outcomes and advocated for a
welfare-centric clustering approach that models the utilities of the groups. In
this work, we model group utilities based on both distances and proportional
representation and formalize two optimization objectives based on
welfare-centric clustering: the Rawlsian (Egalitarian) objective and the
Utilitarian objective. We introduce novel algorithms for both objectives and
prove theoretical guarantees for them. Empirical evaluations on multiple
real-world datasets demonstrate that our methods significantly outperform
existing fair clustering baselines.

</details>


### [73] [Less is More: Learning Graph Tasks with Just LLMs](https://arxiv.org/abs/2508.10115)
*Sola Shirai,Kavitha Srinivas,Julian Dolby,Michael Katz,Horst Samulowitz,Shirin Sohrabi*

Main category: cs.LG

TL;DR: 研究大语言模型（LLMs）图推理能力，发现小LLMs经指令思维链解决方案训练可解决图任务且能泛化。


<details>
  <summary>Details</summary>
Motivation: 先前改善LLM图推理的方法优点不明，需实证回答关于LLMs图推理的相关研究问题。

Method: 通过用指令思维链解决方案训练LLMs来进行研究。

Result: 小LLMs能学会解决图任务，且无需专门图编码器就能将训练成果泛化到新任务和图结构。

Conclusion: 即使是小LLMs，通过特定训练也能具备图推理及泛化能力。

Abstract: For large language models (LLMs), reasoning over graphs could help solve many
problems. Prior work has tried to improve LLM graph reasoning by examining how
best to serialize graphs as text and by combining GNNs and LLMs. However, the
merits of such approaches remain unclear, so we empirically answer the
following research questions: (1) Can LLMs learn to solve fundamental graph
tasks without specialized graph encoding models?, (2) Can LLMs generalize
learned solutions to unseen graph structures or tasks?, and (3) What are the
merits of competing approaches to learn graph tasks? We show that even small
LLMs can learn to solve graph tasks by training them with instructive
chain-of-thought solutions, and this training generalizes, without specialized
graph encoders, to new tasks and graph structures.

</details>


### [74] [From Intent to Execution: Multimodal Chain-of-Thought Reinforcement Learning for Precise CAD Code Generation](https://arxiv.org/abs/2508.10118)
*Ke Niu,Haiyang Yu,Zhuofan Chen,Mengyang Zhao,Teng Fu,Bin Li,Xiangyang Xue*

Main category: cs.LG

TL;DR: 提出CAD - RL框架用于CAD建模代码生成，结合多策略优化，发布ExeCAD数据集，效果优于现有VLMs。


<details>
  <summary>Details</summary>
Motivation: 当前CAD工作流需大量专业知识和手动建模，直接将设计意图转为CAD代码有挑战，利用LLMs自动化3D建模。

Method: 提出CAD - RL框架，结合CoT冷启动与目标驱动强化学习后训练，用三种奖励；引入三种优化策略；发布ExeCAD数据集。

Result: 实验表明CAD - RL在推理质量、输出精度和代码可执行性上比现有VLMs有显著提升。

Conclusion: CAD - RL框架及相关策略和数据集能有效解决CAD建模代码生成问题，提升效果。

Abstract: Computer-Aided Design (CAD) plays a vital role in engineering and
manufacturing, yet current CAD workflows require extensive domain expertise and
manual modeling effort. Recent advances in large language models (LLMs) have
made it possible to generate code from natural language, opening new
opportunities for automating parametric 3D modeling. However, directly
translating human design intent into executable CAD code remains highly
challenging, due to the need for logical reasoning, syntactic correctness, and
numerical precision. In this work, we propose CAD-RL, a multimodal
Chain-of-Thought (CoT) guided reinforcement learning post training framework
for CAD modeling code generation. Our method combines CoT-based Cold Start with
goal-driven reinforcement learning post training using three task-specific
rewards: executability reward, geometric accuracy reward, and external
evaluation reward. To ensure stable policy learning under sparse and
high-variance reward conditions, we introduce three targeted optimization
strategies: Trust Region Stretch for improved exploration, Precision Token Loss
for enhanced dimensions parameter accuracy, and Overlong Filtering to reduce
noisy supervision. To support training and benchmarking, we release ExeCAD, a
noval dataset comprising 16,540 real-world CAD examples with paired natural
language and structured design language descriptions, executable CADQuery
scripts, and rendered 3D models. Experiments demonstrate that CAD-RL achieves
significant improvements in reasoning quality, output precision, and code
executability over existing VLMs.

</details>


### [75] [Nested-ReFT: Efficient Reinforcement Learning for Large Language Model Fine-Tuning via Off-Policy Rollouts](https://arxiv.org/abs/2508.10123)
*Maxime Heuillet,Yufei Cui,Boxing Chen,Audrey Durand,Prasanna Parthasarathi*

Main category: cs.LG

TL;DR: 提出Nested - ReFT框架解决标准ReFT训练计算成本高的问题，理论和实证分析显示其优势并探索偏差缓解方法。


<details>
  <summary>Details</summary>
Motivation: 标准ReFT框架在训练时生成补全内容的计算成本高。

Method: 借鉴离策略RL和推测解码，提出Nested - ReFT框架，用目标模型部分层作行为模型，训练时每批动态跳过层以降低推理成本。

Result: 理论分析表明Nested - ReFT能产生有控制方差的无偏梯度估计；实证分析显示在多个数学推理基准和模型大小上提高了计算效率。

Conclusion: 探索三种偏差缓解方法，维持与基线ReFT相当的性能。

Abstract: Advanced reasoning in LLMs on challenging domains like mathematical reasoning
can be tackled using verifiable rewards based reinforced fine-tuning (ReFT). In
standard ReFT frameworks, a behavior model generates multiple completions with
answers per problem, for the answer to be then scored by a reward function.
While such RL post-training methods demonstrate significant performance
improvements across challenging reasoning domains, the computational cost of
generating completions during training with multiple inference steps makes the
training cost non-trivial. To address this, we draw inspiration from off-policy
RL, and speculative decoding to introduce a novel ReFT framework, dubbed
Nested-ReFT, where a subset of layers of the target model acts as the behavior
model to generate off-policy completions during training. The behavior model
configured with dynamic layer skipping per batch during training decreases the
inference cost compared to the standard ReFT frameworks. Our theoretical
analysis shows that Nested-ReFT yields unbiased gradient estimates with
controlled variance. Our empirical analysis demonstrates improved computational
efficiency measured as tokens/sec across multiple math reasoning benchmarks and
model sizes. Additionally, we explore three variants of bias mitigation to
minimize the off-policyness in the gradient updates that allows for maintaining
performance that matches the baseline ReFT performance.

</details>


### [76] [rETF-semiSL: Semi-Supervised Learning for Neural Collapse in Temporal Data](https://arxiv.org/abs/2508.10147)
*Yuhan Xie,William Cappelletti,Mahsa Shoaran,Pascal Frossard*

Main category: cs.LG

TL;DR: 提出新的半监督预训练策略用于时间序列分类，在多个数据集上表现优于先前方法。


<details>
  <summary>Details</summary>
Motivation: 现有自监督和半监督学习方法的预训练任务选择常基于启发式，且向下游分类任务的可迁移性不确定。

Method: 使用旋转等角紧框架分类器和伪标签，结合生成式预训练任务和新的序列增强策略预训练深度编码器。

Result: 在三个多变量时间序列分类数据集上，该方法显著优于先前的预训练任务。

Conclusion: 将预训练目标与理论上合理的嵌入几何对齐有益。

Abstract: Deep neural networks for time series must capture complex temporal patterns,
to effectively represent dynamic data. Self- and semi-supervised learning
methods show promising results in pre-training large models, which -- when
finetuned for classification -- often outperform their counterparts trained
from scratch. Still, the choice of pretext training tasks is often heuristic
and their transferability to downstream classification is not granted, thus we
propose a novel semi-supervised pre-training strategy to enforce latent
representations that satisfy the Neural Collapse phenomenon observed in
optimally trained neural classifiers. We use a rotational equiangular tight
frame-classifier and pseudo-labeling to pre-train deep encoders with few
labeled samples. Furthermore, to effectively capture temporal dynamics while
enforcing embedding separability, we integrate generative pretext tasks with
our method, and we define a novel sequential augmentation strategy. We show
that our method significantly outperforms previous pretext tasks when applied
to LSTMs, transformers, and state-space models on three multivariate time
series classification datasets. These results highlight the benefit of aligning
pre-training objectives with theoretically grounded embedding geometry.

</details>


### [77] [Comparison of D-Wave Quantum Annealing and Markov Chain Monte Carlo for Sampling from a Probability Distribution of a Restricted Boltzmann Machine](https://arxiv.org/abs/2508.10228)
*Abdelmoula El Yazizi,Samee U. Khan,Yaroslav Koshka*

Main category: cs.LG

TL;DR: 应用局部谷方法评估D - Wave量子退火器对受限玻尔兹曼机（RBM）采样质量，对比D - Wave和Gibbs采样，发现两种采样技术有差异和互补性，结果或解释此前D - Wave采样改进失败原因并揭示改进潜力。


<details>
  <summary>Details</summary>
Motivation: 评估D - Wave量子退火器对RBM采样的质量，解释此前使用D - Wave采样未取得显著改进的原因。

Method: 采用局部谷（LV）中心方法，在基于对比散度的RBM学习相关条件下获取D - Wave和经典训练RBM的Gibbs样本，对比样本所属LV数量和对应局部最小值能量。

Result: 减少D - Wave退火时间未显著增加LV数量；D - Wave采样状态所属LV数量略多于Gibbs采样；两种技术找到的LV有差异；高概率采样状态下互补性差；在训练后期两种技术重叠更少。

Conclusion: 结果或解释此前使用D - Wave采样未取得改进的原因，揭示了如采用经典 - 量子结合方法的改进潜力。

Abstract: A local-valley (LV) centered approach to assessing the quality of sampling
from Restricted Boltzmann Machines (RBMs) was applied to the latest generation
of the D-Wave quantum annealer. D-Wave and Gibbs samples from a classically
trained RBM were obtained at conditions relevant to the
contrastive-divergence-based RBM learning. The samples were compared for the
number of the LVs to which they belonged and the energy of the corresponding
local minima. No significant (desirable) increase in the number of the LVs has
been achieved by decreasing the D-Wave annealing time. At any training epoch,
the states sampled by the D-Wave belonged to a somewhat higher number of LVs
than in the Gibbs sampling. However, many of those LVs found by the two
techniques differed. For high-probability sampled states, the two techniques
were (unfavorably) less complementary and more overlapping. Nevertheless, many
potentially "important" local minima, i.e., those having intermediate, even if
not high, probability values, were found by only one of the two sampling
techniques while missed by the other. The two techniques overlapped less at
later than earlier training epochs, which is precisely the stage of the
training when modest improvements to the sampling quality could make meaningful
differences for the RBM trainability. The results of this work may explain the
failure of previous investigations to achieve substantial (or any) improvement
when using D-Wave-based sampling. However, the results reveal some potential
for improvement, e.g., using a combined classical-quantum approach.

</details>


### [78] [Out-of-Distribution Detection using Counterfactual Distance](https://arxiv.org/abs/2508.10148)
*Maria Stoica,Francesco Leofante,Alessio Lomuscio*

Main category: cs.LG

TL;DR: 本文提出基于反事实解释的事后OOD检测方法，可有效检测OOD数据，在多个数据集上表现良好。


<details>
  <summary>Details</summary>
Motivation: 为安全使用机器学习系统，需要准确且可解释的OOD检测方法。

Method: 利用反事实解释计算输入到决策边界的距离，提出在嵌入空间直接计算反事实以提高可扩展性的策略。

Result: 在CIFAR - 10上达到93.50% AUROC和25.80% FPR95，在CIFAR - 100和ImageNet - 200上优于现有方法。

Conclusion: 所提方法是有效的OOD检测方法，且因使用反事实解释可解释检测结果。

Abstract: Accurate and explainable out-of-distribution (OOD) detection is required to
use machine learning systems safely. Previous work has shown that feature
distance to decision boundaries can be used to identify OOD data effectively.
In this paper, we build on this intuition and propose a post-hoc OOD detection
method that, given an input, calculates the distance to decision boundaries by
leveraging counterfactual explanations. Since computing explanations can be
expensive for large architectures, we also propose strategies to improve
scalability by computing counterfactuals directly in embedding space.
Crucially, as the method employs counterfactual explanations, we can seamlessly
use them to help interpret the results of our detector. We show that our method
is in line with the state of the art on CIFAR-10, achieving 93.50% AUROC and
25.80% FPR95. Our method outperforms these methods on CIFAR-100 with 97.05%
AUROC and 13.79% FPR95 and on ImageNet-200 with 92.55% AUROC and 33.55% FPR95
across four OOD datasets

</details>


### [79] [Characterizing Evolution in Expectation-Maximization Estimates for Overspecified Mixed Linear Regression](https://arxiv.org/abs/2508.10154)
*Zhankun Luo,Abolfazl Hashemi*

Main category: cs.LG

TL;DR: 本文研究过指定双成分混合线性回归（2MLR）中EM算法在模型误设下的行为，给出不同初始猜测和样本水平下的收敛性和统计精度，还推导迭代复杂度并拓展到低信噪比场景。


<details>
  <summary>Details</summary>
Motivation: 混合模型存在模型误设挑战，研究EM算法在过指定2MLR模型误设下的行为。

Method: 理论分析，分别在总体水平和有限样本水平进行定理推导。

Result: 总体水平：不平衡初始猜测时回归参数线性收敛，平衡初始猜测时亚线性收敛；有限样本水平：不平衡和平衡固定混合权重有不同统计精度；推导有限样本水平迭代复杂度；拓展到低信噪比场景。

Conclusion: 揭示了EM算法在过指定2MLR模型误设下的收敛性质和统计精度，建立总体和有限样本结果联系，拓展了分析场景。

Abstract: Mixture models have attracted significant attention due to practical
effectiveness and comprehensive theoretical foundations. A persisting challenge
is model misspecification, which occurs when the model to be fitted has more
mixture components than those in the data distribution. In this paper, we
develop a theoretical understanding of the Expectation-Maximization (EM)
algorithm's behavior in the context of targeted model misspecification for
overspecified two-component Mixed Linear Regression (2MLR) with unknown
$d$-dimensional regression parameters and mixing weights. In Theorem 5.1 at the
population level, with an unbalanced initial guess for mixing weights, we
establish linear convergence of regression parameters in $O(\log(1/\epsilon))$
steps. Conversely, with a balanced initial guess for mixing weights, we observe
sublinear convergence in $O(\epsilon^{-2})$ steps to achieve the
$\epsilon$-accuracy at Euclidean distance. In Theorem 6.1 at the finite-sample
level, for mixtures with sufficiently unbalanced fixed mixing weights, we
demonstrate a statistical accuracy of $O((d/n)^{1/2})$, whereas for those with
sufficiently balanced fixed mixing weights, the accuracy is $O((d/n)^{1/4})$
given $n$ data samples. Furthermore, we underscore the connection between our
population level and finite-sample level results: by setting the desired final
accuracy $\epsilon$ in Theorem 5.1 to match that in Theorem 6.1 at the
finite-sample level, namely letting $\epsilon = O((d/n)^{1/2})$ for
sufficiently unbalanced fixed mixing weights and $\epsilon = O((d/n)^{1/4})$
for sufficiently balanced fixed mixing weights, we intuitively derive iteration
complexity bounds $O(\log (1/\epsilon))=O(\log (n/d))$ and
$O(\epsilon^{-2})=O((n/d)^{1/2})$ at the finite-sample level for sufficiently
unbalanced and balanced initial mixing weights. We further extend our analysis
in overspecified setting to low SNR regime.

</details>


### [80] [Uncertainty-Aware Prediction of Parkinson's Disease Medication Needs: A Two-Stage Conformal Prediction Approach](https://arxiv.org/abs/2508.10284)
*Ricardo Diaz-Rincon,Muxuan Liang,Adolfo Ramirez-Zamora,Benjamin Shickel*

Main category: cs.LG

TL;DR: 本文针对帕金森病药物管理难题，开发了保形预测框架，能预测未来两年药物需求并量化不确定性，优化症状控制和生活质量。


<details>
  <summary>Details</summary>
Motivation: 帕金森病药物管理因疾病进展和治疗反应异质性面临挑战，现有方法靠试错且缺乏系统预测手段，机器学习方法未考虑预测不确定性，临床应用受限。

Method: 开发保形预测框架，处理PD住院数据零膨胀问题，采用两阶段方法，先识别可能需药物调整的患者，再预测左旋多巴等效日剂量调整。

Result: 相比传统方法，该框架在减少预测区间长度的同时实现了边际覆盖，短期规划预测更精确，长期预测范围更广。

Conclusion: 该方法能量化不确定性，为左旋多巴剂量提供循证决策，优化症状控制，减少副作用，提高生活质量。

Abstract: Parkinson's Disease (PD) medication management presents unique challenges due
to heterogeneous disease progression and treatment response. Neurologists must
balance symptom control with optimal dopaminergic dosing based on functional
disability while minimizing side effects. This balance is crucial as inadequate
or abrupt changes can cause levodopa-induced dyskinesia, wearing off, and
neuropsychiatric effects, significantly reducing quality of life. Current
approaches rely on trial-and-error decisions without systematic predictive
methods. Despite machine learning advances, clinical adoption remains limited
due to reliance on point predictions that do not account for prediction
uncertainty, undermining clinical trust and utility. Clinicians require not
only predictions of future medication needs but also reliable confidence
measures. Without quantified uncertainty, adjustments risk premature escalation
to maximum doses or prolonged inadequate symptom control. We developed a
conformal prediction framework anticipating medication needs up to two years in
advance with reliable prediction intervals and statistical guarantees. Our
approach addresses zero-inflation in PD inpatient data, where patients maintain
stable medication regimens between visits. Using electronic health records from
631 inpatient admissions at University of Florida Health (2011-2021), our
two-stage approach identifies patients likely to need medication changes, then
predicts required levodopa equivalent daily dose adjustments. Our framework
achieved marginal coverage while reducing prediction interval lengths compared
to traditional approaches, providing precise predictions for short-term
planning and wider ranges for long-term forecasting. By quantifying
uncertainty, our approach enables evidence-based decisions about levodopa
dosing, optimizing symptom control while minimizing side effects and improving
life quality.

</details>


### [81] [Benchmark-Driven Selection of AI: Evidence from DeepSeek-R1](https://arxiv.org/abs/2508.10173)
*Petr Spelda,Vit Stritecky*

Main category: cs.LG

TL;DR: 研究推理语言模型评估，提出基准驱动选择AI，指出部分基准可作训练课程。


<details>
  <summary>Details</summary>
Motivation: 推理成大语言模型新扩展维度，需仔细研究其在关键任务中的能力。

Method: 以人类最后考试的顺序决策问题展示基准驱动选择AI对DeepSeek - R1的影响。

Result: 更好性能不仅源于测试时算法改进或模型大小，还在于用有影响力基准作学习课程。

Conclusion: 用有影响力基准引导AI发展，部分基准可视为训练课程而非测试集，测试任务新颖性对衡量推理模型泛化能力很关键。

Abstract: Evaluation of reasoning language models gained importance after it was
observed that they can combine their existing capabilities into novel traces of
intermediate steps before task completion and that the traces can sometimes
help them to generalize better than past models. As reasoning becomes the next
scaling dimension of large language models, careful study of their capabilities
in critical tasks is needed. We show that better performance is not always
caused by test-time algorithmic improvements or model sizes but also by using
impactful benchmarks as curricula for learning. We call this benchmark-driven
selection of AI and show its effects on DeepSeek-R1 using our sequential
decision-making problem from Humanity's Last Exam. Steering development of AI
by impactful benchmarks trades evaluation for learning and makes novelty of
test tasks key for measuring generalization capabilities of reasoning models.
Consequently, some benchmarks could be seen as curricula for training rather
than unseen test sets.

</details>


### [82] [An Explainable AI based approach for Monitoring Animal Health](https://arxiv.org/abs/2508.10210)
*Rahul Janaa,Shubham Dixit,Mrityunjay Sharma,Ritesh Kumar*

Main category: cs.LG

TL;DR: 本文介绍基于可解释机器学习的现代数据驱动养殖实践，利用传感器收集奶牛数据，评估多种机器学习模型，k近邻分类器表现最佳，还用可解释AI框架辅助可持续畜牧管理。


<details>
  <summary>Details</summary>
Motivation: 解决奶农因难以追踪所有奶牛而面临的监测健康和优化产量的挑战。

Method: 通过3轴加速度计传感器持续收集数据，利用蓝牙物联网设备和4G网络传输数据，对加速度计时间序列数据进行预处理，评估多种超参数优化的机器学习模型，使用SHAP等可解释AI框架。

Result: k近邻分类器表现最佳，训练集AUC均值0.98、标准差0.0026，测试集AUC为0.99。

Conclusion: 可开发用于可持续畜牧管理的可解释且实用的机器学习模型。

Abstract: Monitoring cattle health and optimizing yield are key challenges faced by
dairy farmers due to difficulties in tracking all animals on the farm. This
work aims to showcase modern data-driven farming practices based on explainable
machine learning(ML) methods that explain the activity and behaviour of dairy
cattle (cows). Continuous data collection of 3-axis accelerometer sensors and
usage of robust ML methodologies and algorithms, provide farmers and
researchers with actionable information on cattle activity, allowing farmers to
make informed decisions and incorporate sustainable practices. This study
utilizes Bluetooth-based Internet of Things (IoT) devices and 4G networks for
seamless data transmission, immediate analysis, inference generation, and
explains the models performance with explainability frameworks. Special
emphasis is put on the pre-processing of the accelerometers time series data,
including the extraction of statistical characteristics, signal processing
techniques, and lag-based features using the sliding window technique. Various
hyperparameter-optimized ML models are evaluated across varying window lengths
for activity classification. The k-nearest neighbour Classifier achieved the
best performance, with AUC of mean 0.98 and standard deviation of 0.0026 on the
training set and 0.99 on testing set). In order to ensure transparency,
Explainable AI based frameworks such as SHAP is used to interpret feature
importance that can be understood and used by practitioners. A detailed
comparison of the important features, along with the stability analysis of
selected features, supports development of explainable and practical ML models
for sustainable livestock management.

</details>


### [83] [Unpacking the Implicit Norm Dynamics of Sharpness-Aware Minimization in Tensorized Models](https://arxiv.org/abs/2508.10435)
*Tianxiao Cao,Kyohei Atarashi,Hisashi Kashima*

Main category: cs.LG

TL;DR: 分析SAM在一般张量模型中的范数动态，提出DAS方法，实验表明DAS性能优且计算开销低。


<details>
  <summary>Details</summary>
Motivation: 此前对SAM在更一般的张量或尺度不变模型中的行为研究不足，需进一步探索。

Method: 利用尺度不变性分析SAM在一般张量模型中的范数动态，引入“Norm Deviation”概念，推导其在SAM下的演化，提出DAS方法。

Result: 实验表明DAS在张量补全、噪声训练等任务中比SAM性能相当或更优，且计算开销更低。

Conclusion: DAS能有效模仿SAM的正则化行为，是一种简单有效的方法。

Abstract: Sharpness-Aware Minimization (SAM) has been proven to be an effective
optimization technique for improving generalization in overparameterized
models. While prior works have explored the implicit regularization of SAM in
simple two-core scale-invariant settings, its behavior in more general
tensorized or scale-invariant models remains underexplored. In this work, we
leverage scale-invariance to analyze the norm dynamics of SAM in general
tensorized models. We introduce the notion of \emph{Norm Deviation} as a global
measure of core norm imbalance, and derive its evolution under SAM using
gradient flow analysis. We show that SAM's implicit control of Norm Deviation
is governed by the covariance between core norms and their gradient magnitudes.
Motivated by these findings, we propose a simple yet effective method,
\emph{Deviation-Aware Scaling (DAS)}, which explicitly mimics this
regularization behavior by scaling core norms in a data-adaptive manner. Our
experiments across tensor completion, noisy training, model compression, and
parameter-efficient fine-tuning confirm that DAS achieves competitive or
improved performance over SAM, while offering reduced computational overhead.

</details>


### [84] [AI-Driven Detection and Analysis of Handwriting on Seized Ivory: A Tool to Uncover Criminal Networks in the Illicit Wildlife Trade](https://arxiv.org/abs/2508.10219)
*Will Fein,Ryan J. Horwitz,John E. Brown III,Amit Misra,Felipe Oviedo,Kevin White,Juan M. Lavista Ferres,Samuel K. Wasser*

Main category: cs.LG

TL;DR: 本文提出基于AI的管道来提取和分析缴获象牙上的手写标记，通过收集照片和运用模型等方法，发现重复标记建立关联，展示了AI在野生动物法医鉴定中的潜力。


<details>
  <summary>Details</summary>
Motivation: 跨国象牙贸易致非洲象数量下降且走私网络难破，基因数据获取成本高且有时无法获取，而手写标记易拍照却少分析，需新的法医证据来源。

Method: 收集6085张象牙照片，用目标检测模型提取超17000个标记，用先进AI工具标记和描述，找出重复标记。

Result: 识别出184个重复“签名标记”，20个标记出现在多次缴获中，建立了缴获之间的法医联系。

Conclusion: 该工作填补其他数据源缺失的空白，展示AI在野生动物法医鉴定的变革潜力，强调将笔迹分析融入打击野生动物犯罪的实际步骤。

Abstract: The transnational ivory trade continues to drive the decline of elephant
populations across Africa, and trafficking networks remain difficult to
disrupt. Tusks seized by law enforcement officials carry forensic information
on the traffickers responsible for their export, including DNA evidence and
handwritten markings made by traffickers. For 20 years, analyses of tusk DNA
have identified where elephants were poached and established connections among
shipments of ivory. While the links established using genetic evidence are
extremely conclusive, genetic data is expensive and sometimes impossible to
obtain. But though handwritten markings are easy to photograph, they are rarely
documented or analyzed. Here, we present an AI-driven pipeline for extracting
and analyzing handwritten markings on seized elephant tusks, offering a novel,
scalable, and low-cost source of forensic evidence. Having collected 6,085
photographs from eight large seizures of ivory over a 6-year period
(2014-2019), we used an object detection model to extract over 17,000
individual markings, which were then labeled and described using
state-of-the-art AI tools. We identified 184 recurring "signature markings"
that connect the tusks on which they appear. 20 signature markings were
observed in multiple seizures, establishing forensic links between these
seizures through traffickers involved in both shipments. This work complements
other investigative techniques by filling in gaps where other data sources are
unavailable. The study demonstrates the transformative potential of AI in
wildlife forensics and highlights practical steps for integrating handwriting
analysis into efforts to disrupt organized wildlife crime.

</details>


### [85] [Confounding is a Pervasive Problem in Real World Recommender Systems](https://arxiv.org/abs/2508.10479)
*Alexander Merkov,David Rohde,Alexandre Gilotte,Benjamin Heymann*

Main category: cs.LG

TL;DR: 本文指出推荐系统常见做法会引入混杂问题，影响性能，并给出示例与应对建议。


<details>
  <summary>Details</summary>
Motivation: 未观测到的混杂会导致因果效应估计有偏差，推荐系统常见做法也会产生类似问题，影响其性能。

Method: 展示常见做法如何引入混杂，通过模拟研究提供现象示例，并给出实际建议。

Result: 证明特征工程、A/B测试和模块化等常见做法会引入混杂，影响推荐系统性能。

Conclusion: 从业者可根据建议减少或避免混杂对真实系统的影响。

Abstract: Unobserved confounding arises when an unmeasured feature influences both the
treatment and the outcome, leading to biased causal effect estimates. This
issue undermines observational studies in fields like economics, medicine,
ecology or epidemiology. Recommender systems leveraging fully observed data
seem not to be vulnerable to this problem. However many standard practices in
recommender systems result in observed features being ignored, resulting in
effectively the same problem. This paper will show that numerous common
practices such as feature engineering, A/B testing and modularization can in
fact introduce confounding into recommendation systems and hamper their
performance. Several illustrations of the phenomena are provided, supported by
simulation studies with practical suggestions about how practitioners may
reduce or avoid the affects of confounding in real systems.

</details>


### [86] [Interpretable Machine Learning Model for Early Prediction of Acute Kidney Injury in Critically Ill Patients with Cirrhosis: A Retrospective Study](https://arxiv.org/abs/2508.10233)
*Li Sun,Shuheng Chen,Junyi Fan,Yong Si,Minoo Ahmadi,Elham Pishgar,Kamiar Alaei,Maryam Pishgar*

Main category: cs.LG

TL;DR: 本文开发了可解释机器学习模型用于肝硬化重症患者急性肾损伤（AKI）早期预测，LightGBM 表现最佳，能实现风险分层。


<details>
  <summary>Details</summary>
Motivation: 肝硬化死亡率高且常伴 AKI 并发症，现有预测工具存在不足，需开发准确可解释的早期 AKI 预测模型。

Method: 回顾性分析 MIMIC - IV v2.2 数据库，提取相关变量，经预处理、特征选择和类别平衡，训练六种算法并多指标评估。

Result: LightGBM 性能最佳，关键预测因子与已知机制相符并提示可干预靶点。

Conclusion: 基于 LightGBM 的模型可利用常规临床变量实现准确早期 AKI 风险分层，有高阴性预测值，需外部验证和集成到电子健康记录系统。

Abstract: Background: Cirrhosis is a progressive liver disease with high mortality and
frequent complications, notably acute kidney injury (AKI), which occurs in up
to 50% of hospitalized patients and worsens outcomes. AKI stems from complex
hemodynamic, inflammatory, and metabolic changes, making early detection
essential. Many predictive tools lack accuracy, interpretability, and alignment
with intensive care unit (ICU) workflows. This study developed an interpretable
machine learning model for early AKI prediction in critically ill patients with
cirrhosis.
  Methods: We conducted a retrospective analysis of the MIMIC-IV v2.2 database,
identifying 1240 adult ICU patients with cirrhosis and excluding those with ICU
stays under 48 hours or missing key data. Laboratory and physiological
variables from the first 48 hours were extracted. The pipeline included
preprocessing, missingness filtering, LASSO feature selection, and SMOTE class
balancing. Six algorithms-LightGBM, CatBoost, XGBoost, logistic regression,
naive Bayes, and neural networks-were trained and evaluated using AUROC,
accuracy, F1-score, sensitivity, specificity, and predictive values.
  Results: LightGBM achieved the best performance (AUROC 0.808, 95% CI
0.741-0.856; accuracy 0.704; NPV 0.911). Key predictors included prolonged
partial thromboplastin time, absence of outside-facility 20G placement, low pH,
and altered pO2, consistent with known cirrhosis-AKI mechanisms and suggesting
actionable targets.
  Conclusion: The LightGBM-based model enables accurate early AKI risk
stratification in ICU patients with cirrhosis using routine clinical variables.
Its high negative predictive value supports safe de-escalation for low-risk
patients, and interpretability fosters clinician trust and targeted prevention.
External validation and integration into electronic health record systems are
warranted.

</details>


### [87] [Can Transformers Break Encryption Schemes via In-Context Learning?](https://arxiv.org/abs/2508.10235)
*Jathin Korrapati,Patrick Mendoza,Aditya Tomar,Abein Abraham*

Main category: cs.LG

TL;DR: 本文提出将上下文学习（ICL）应用于密码函数学习领域，聚焦单字母替换和维吉尼亚密码，评估transformer的归纳偏置和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 过往工作聚焦结构化函数的数值或符号推理，本文希望将ICL应用于密码函数学习领域，评估transformer相关能力。

Method: 在单字母替换和维吉尼亚密码这两类私钥加密方案中，给定少量（密文，明文）对，让模型推断替换规则并解码新密文。

Result: 文中未提及明确结果，但提供了代码。

Conclusion: 此密码函数学习场景适合在ICL范式下评估transformer的归纳偏置和泛化能力。

Abstract: In-context learning (ICL) has emerged as a powerful capability of
transformer-based language models, enabling them to perform tasks by
conditioning on a small number of examples presented at inference time, without
any parameter updates. Prior work has shown that transformers can generalize
over simple function classes like linear functions, decision trees, even neural
networks, purely from context, focusing on numerical or symbolic reasoning over
underlying well-structured functions. Instead, we propose a novel application
of ICL into the domain of cryptographic function learning, specifically
focusing on ciphers such as mono-alphabetic substitution and Vigen\`ere
ciphers, two classes of private-key encryption schemes. These ciphers involve a
fixed but hidden bijective mapping between plain text and cipher text
characters. Given a small set of (cipher text, plain text) pairs, the goal is
for the model to infer the underlying substitution and decode a new cipher text
word. This setting poses a structured inference challenge, which is well-suited
for evaluating the inductive biases and generalization capabilities of
transformers under the ICL paradigm. Code is available at
https://github.com/adistomar/CS182-project.

</details>


### [88] [Pruning and Malicious Injection: A Retraining-Free Backdoor Attack on Transformer Models](https://arxiv.org/abs/2508.10243)
*Taibiao Zhao,Mingxuan Sun,Hao Wang,Xiaobing Chen,Xiangwei Zhou*

Main category: cs.LG

TL;DR: 提出针对transformer的无重新训练后门攻击方法HPMI，理论证明其抗检测，实验验证有效且比现有方法更好。


<details>
  <summary>Details</summary>
Motivation: 现有transformer后门攻击方法依赖重新训练或改变架构，资源消耗大且具侵入性。

Method: 提出HPMI，通过剪枝最不重要的头并注入预训练恶意头建立后门。

Result: 理论证明后门抗检测和去除；实验显示清洁准确率损失小、攻击成功率高、绕过多种防御机制，比现有方法更隐蔽、鲁棒。

Conclusion: HPMI是一种有效的无重新训练、不改变架构的transformer后门攻击方法。

Abstract: Transformer models have demonstrated exceptional performance and have become
indispensable in computer vision (CV) and natural language processing (NLP)
tasks. However, recent studies reveal that transformers are susceptible to
backdoor attacks. Prior backdoor attack methods typically rely on retraining
with clean data or altering the model architecture, both of which can be
resource-intensive and intrusive. In this paper, we propose Head-wise Pruning
and Malicious Injection (HPMI), a novel retraining-free backdoor attack on
transformers that does not alter the model's architecture. Our approach
requires only a small subset of the original data and basic knowledge of the
model architecture, eliminating the need for retraining the target transformer.
Technically, HPMI works by pruning the least important head and injecting a
pre-trained malicious head to establish the backdoor. We provide a rigorous
theoretical justification demonstrating that the implanted backdoor resists
detection and removal by state-of-the-art defense techniques, under reasonable
assumptions. Experimental evaluations across multiple datasets further validate
the effectiveness of HPMI, showing that it 1) incurs negligible clean accuracy
loss, 2) achieves at least 99.55% attack success rate, and 3) bypasses four
advanced defense mechanisms. Additionally, relative to state-of-the-art
retraining-dependent attacks, HPMI achieves greater concealment and robustness
against diverse defense strategies, while maintaining minimal impact on clean
accuracy.

</details>


### [89] [MDNS: Masked Diffusion Neural Sampler via Stochastic Optimal Control](https://arxiv.org/abs/2508.10684)
*Yuchen Zhu,Wei Guo,Jaemoo Choi,Guan-Horng Liu,Yongxin Chen,Molei Tao*

Main category: cs.LG

TL;DR: 提出MDNS框架学习离散神经采样器，实验验证其高效可扩展性，优于其他基线方法。


<details>
  <summary>Details</summary>
Motivation: 解决大基数和多模态离散状态空间中学习神经采样器的难题。

Method: 提出MDNS框架，通过一系列学习目标对齐两个路径测度，理论基于连续时间马尔可夫链的随机最优控制。

Result: 在多种分布上实验表明MDNS能准确采样，在高维问题中表现出色，大幅超越其他学习基线。

Conclusion: MDNS框架有效且有潜力。

Abstract: We study the problem of learning a neural sampler to generate samples from
discrete state spaces where the target probability mass function
$\pi\propto\mathrm{e}^{-U}$ is known up to a normalizing constant, which is an
important task in fields such as statistical physics, machine learning,
combinatorial optimization, etc. To better address this challenging task when
the state space has a large cardinality and the distribution is multi-modal, we
propose $\textbf{M}$asked $\textbf{D}$iffusion $\textbf{N}$eural
$\textbf{S}$ampler ($\textbf{MDNS}$), a novel framework for training discrete
neural samplers by aligning two path measures through a family of learning
objectives, theoretically grounded in the stochastic optimal control of the
continuous-time Markov chains. We validate the efficiency and scalability of
MDNS through extensive experiments on various distributions with distinct
statistical properties, where MDNS learns to accurately sample from the target
distributions despite the extremely high problem dimensions and outperforms
other learning-based baselines by a large margin. A comprehensive study of
ablations and extensions is also provided to demonstrate the efficacy and
potential of the proposed framework.

</details>


### [90] [Convergence Analysis of Max-Min Exponential Neural Network Operators in Orlicz Space](https://arxiv.org/abs/2508.10248)
*Satyaranjan Pradhan,Madan Mohan Soren*

Main category: cs.LG

TL;DR: 提出用指数神经网络算子逼近函数的Max Min方法，拓展框架到Max Min Kantorovich型算子，研究其逼近性质与收敛情况并给出图形展示。


<details>
  <summary>Details</summary>
Motivation: 探索指数神经网络算子在函数逼近方面的应用，研究其逼近性质。

Method: 提出Max Min方法，拓展框架到Kantorovich型算子，用对数连续模分析收敛阶并估计收敛速度，在Orlicz空间研究收敛行为。

Result: 研究了单变量函数的逐点和一致收敛、收敛阶与速度，在Orlicz空间研究了收敛行为，给出图形展示逼近误差。

Conclusion: Max Min方法及拓展的Kantorovich型指数神经网络算子在函数逼近中有一定效果，可用于分析函数逼近性质。

Abstract: In this current work, we propose a Max Min approach for approximating
functions using exponential neural network operators. We extend this framework
to develop the Max Min Kantorovich-type exponential neural network operators
and investigate their approximation properties. We study both pointwise and
uniform convergence for univariate functions. To analyze the order of
convergence, we use the logarithmic modulus of continuity and estimate the
corresponding rate of convergence. Furthermore, we examine the convergence
behavior of the Max Min Kantorovich type exponential neural network operators
within the Orlicz space setting. We provide some graphical representations to
illustrate the approximation error of the function through suitable kernel and
sigmoidal activation functions.

</details>


### [91] [Enhancing Fairness in Autoencoders for Node-Level Graph Anomaly Detection](https://arxiv.org/abs/2508.10785)
*Shouju Wang,Yuchen Song,Sheng'en Li,Dongmian Zou*

Main category: cs.LG

TL;DR: 论文提出DECAF - GAD框架解决基于自编码器的图异常检测（GAD）模型的公平性问题，实验表明其在检测性能和公平性指标上表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有GNN - 基于的GAD模型会继承和放大训练数据中的偏差，导致不公平结果，且现有公平性方法多针对节点分类任务，不适用于基于自编码器的异常检测。

Method: 引入结构因果模型（SCM）分离敏感属性，设计专门的自编码器架构和公平引导损失函数。

Result: 在合成和真实数据集上实验，DECAF - GAD达到有竞争力的异常检测性能，显著提升公平性指标。

Conclusion: DECAF - GAD能在保留GAD性能的同时减轻偏差，提升公平性。

Abstract: Graph anomaly detection (GAD) has become an increasingly important task
across various domains. With the rapid development of graph neural networks
(GNNs), GAD methods have achieved significant performance improvements.
However, fairness considerations in GAD remain largely underexplored. Indeed,
GNN-based GAD models can inherit and amplify biases present in training data,
potentially leading to unfair outcomes. While existing efforts have focused on
developing fair GNNs, most approaches target node classification tasks, where
models often rely on simple layer architectures rather than autoencoder-based
structures, which are the most widely used architecturs for anomaly detection.
To address fairness in autoencoder-based GAD models, we propose
\textbf{D}is\textbf{E}ntangled \textbf{C}ounterfactual \textbf{A}dversarial
\textbf{F}air (DECAF)-GAD, a framework that alleviates bias while preserving
GAD performance. Specifically, we introduce a structural causal model (SCM) to
disentangle sensitive attributes from learned representations. Based on this
causal framework, we formulate a specialized autoencoder architecture along
with a fairness-guided loss function. Through extensive experiments on both
synthetic and real-world datasets, we demonstrate that DECAF-GAD not only
achieves competitive anomaly detection performance but also significantly
enhances fairness metrics compared to baseline GAD methods. Our code is
available at https://github.com/Tlhey/decaf_code.

</details>


### [92] [Multi-Agent Reinforcement Learning for Adaptive Resource Orchestration in Cloud-Native Clusters](https://arxiv.org/abs/2508.10253)
*Guanzi Yao,Heyao Liu,Linyan Dai*

Main category: cs.LG

TL;DR: 本文提出基于多智能体强化学习的自适应资源编排方法解决云原生数据库系统资源动态性和调度复杂性问题，实验表明该方法优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决云原生数据库系统中资源动态性高和调度复杂性的挑战。

Method: 提出基于多智能体强化学习的自适应资源编排方法，引入异构角色的智能体建模机制，设计奖励塑造机制，开发统一的多智能体训练框架。

Result: 在多个关键指标上优于传统方法，具有强泛化性和实用性，能有效处理高并发、高维状态空间和复杂依赖关系的编排任务。

Conclusion: 该方法在现实大规模调度环境中具有优势。

Abstract: This paper addresses the challenges of high resource dynamism and scheduling
complexity in cloud-native database systems. It proposes an adaptive resource
orchestration method based on multi-agent reinforcement learning. The method
introduces a heterogeneous role-based agent modeling mechanism. This allows
different resource entities, such as compute nodes, storage nodes, and
schedulers, to adopt distinct policy representations. These agents are better
able to reflect diverse functional responsibilities and local environmental
characteristics within the system. A reward-shaping mechanism is designed to
integrate local observations with global feedback. This helps mitigate policy
learning bias caused by incomplete state observations. By combining real-time
local performance signals with global system value estimation, the mechanism
improves coordination among agents and enhances policy convergence stability. A
unified multi-agent training framework is developed and evaluated on a
representative production scheduling dataset. Experimental results show that
the proposed method outperforms traditional approaches across multiple key
metrics. These include resource utilization, scheduling latency, policy
convergence speed, system stability, and fairness. The results demonstrate
strong generalization and practical utility. Across various experimental
scenarios, the method proves effective in handling orchestration tasks with
high concurrency, high-dimensional state spaces, and complex dependency
relationships. This confirms its advantages in real-world, large-scale
scheduling environments.

</details>


### [93] [Comparison of Data Reduction Criteria for Online Gaussian Processes](https://arxiv.org/abs/2508.10815)
*Thore Wietzke,Knut Graichen*

Main category: cs.LG

TL;DR: 本文对几种高斯过程（GPs）的数据点约简准则进行统一比较，并提出接受准则，给出在线GP算法选择准则的实用指南。


<details>
  <summary>Details</summary>
Motivation: GPs计算复杂度高，在小数据集上适用，流式场景下数据积累让稀疏GPs也难以处理，在线GPs旨在解决此问题。

Method: 对几种约简准则进行统一比较，分析计算复杂度和约简行为，在基准函数和真实数据集上评估准则，提出接受准则。

Result: 对约简准则进行了评估，提出接受准则。

Conclusion: 给出在线GP算法选择合适准则的实用指南。

Abstract: Gaussian Processes (GPs) are widely used for regression and system
identification due to their flexibility and ability to quantify uncertainty.
However, their computational complexity limits their applicability to small
datasets. Moreover in a streaming scenario, more and more datapoints accumulate
which is intractable even for Sparse GPs. Online GPs aim to alleviate this
problem by e.g. defining a maximum budget of datapoints and removing redundant
datapoints. This work provides a unified comparison of several reduction
criteria, analyzing both their computational complexity and reduction behavior.
The criteria are evaluated on benchmark functions and real-world datasets,
including dynamic system identification tasks. Additionally, acceptance
criteria are proposed to further filter out redundant datapoints. This work
yields practical guidelines for choosing a suitable criterion for an online GP
algorithm.

</details>


### [94] [Federated Anomaly Detection for Multi-Tenant Cloud Platforms with Personalized Modeling](https://arxiv.org/abs/2508.10255)
*Yuxi Wang,Heyao Liu,Nyutian Long,Guanzi Yao*

Main category: cs.LG

TL;DR: 提出基于联邦学习的异常检测方法，解决多租户云环境挑战，实验显示该方法性能优于主流模型，有实用潜力。


<details>
  <summary>Details</summary>
Motivation: 解决多租户云环境中数据隐私泄露、资源行为异构和集中式建模的局限性等问题。

Method: 建立多租户联邦训练框架，各租户本地训练模型，通过参数聚合优化全局模型；引入个性化参数调整机制；使用马氏距离计算异常分数。

Result: 实验表明该方法在Precision、Recall和F1 - Score等关键指标上优于现有主流模型，在复杂场景中性能稳定。

Conclusion: 该方法在云计算环境的智能资源监控和异常诊断方面有实际应用潜力。

Abstract: This paper proposes an anomaly detection method based on federated learning
to address key challenges in multi-tenant cloud environments, including data
privacy leakage, heterogeneous resource behavior, and the limitations of
centralized modeling. The method establishes a federated training framework
involving multiple tenants. Each tenant trains the model locally using private
resource usage data. Through parameter aggregation, a global model is
optimized, enabling cross-tenant collaborative anomaly detection while
preserving data privacy. To improve adaptability to diverse resource usage
patterns, a personalized parameter adjustment mechanism is introduced. This
allows the model to retain tenant-specific feature representations while
sharing global knowledge. In the model output stage, the Mahalanobis distance
is used to compute anomaly scores. This enhances both the accuracy and
stability of anomaly detection. The experiments use real telemetry data from a
cloud platform to construct a simulated multi-tenant environment. The study
evaluates the model's performance under varying participation rates and noise
injection levels. These comparisons demonstrate the proposed method's
robustness and detection accuracy. Experimental results show that the proposed
method outperforms existing mainstream models across key metrics such as
Precision, Recall, and F1-Score. It also maintains stable performance in
various complex scenarios. These findings highlight the method's practical
potential for intelligent resource monitoring and anomaly diagnosis in cloud
computing environments.

</details>


### [95] [Source Component Shift Adaptation via Offline Decomposition and Online Mixing Approach](https://arxiv.org/abs/2508.10257)
*Ryuta Matsuno*

Main category: cs.LG

TL;DR: 本文提出基于离线分解和在线混合的源组件偏移适应方法，在真实数据集上表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有在线学习方法难以有效利用重复偏移，基于模型池的方法难以捕捉单个源组件，导致适应效果差，因此要提出新方法解决源组件偏移适应问题。

Method: 将问题分为离线源组件分解和在线混合权重调整两个子问题，先通过EM算法离线确定预测模型，再通过在线凸优化更新混合权重。

Result: 在多个真实世界回归数据集上的实验表明，该方法优于基线，累积测试损失最多降低67.4%。

Conclusion: 所提方法充分利用偏移特性，在源组件偏移适应上性能优越。

Abstract: This paper addresses source component shift adaptation, aiming to update
predictions adapting to source component shifts for incoming data streams based
on past training data. Existing online learning methods often fail to utilize
recurring shifts effectively, while model-pool-based methods struggle to
capture individual source components, leading to poor adaptation. In this
paper, we propose a source component shift adaptation method via an offline
decomposition and online mixing approach. We theoretically identify that the
problem can be divided into two subproblems: offline source component
decomposition and online mixing weight adaptation. Based on this, our method
first determines prediction models, each of which learns a source component
solely based on past training data offline through the EM algorithm. Then, it
updates the mixing weight of the prediction models for precise prediction
through online convex optimization. Thanks to our theoretical derivation, our
method fully leverages the characteristics of the shifts, achieving superior
adaptation performance over existing methods. Experiments conducted on various
real-world regression datasets demonstrate that our method outperforms
baselines, reducing the cumulative test loss by up to 67.4%.

</details>


### [96] [SynBrain: Enhancing Visual-to-fMRI Synthesis via Probabilistic Representation Learning](https://arxiv.org/abs/2508.10298)
*Weijian Mai,Jiamin Wu,Yu Zhu,Zhouheng Yao,Dongzhan Zhou,Andrew F. Luo,Qihao Zheng,Wanli Ouyang,Chunfeng Song*

Main category: cs.LG

TL;DR: 提出SynBrain框架模拟视觉语义到神经反应转换，实验显示其在特定主体视觉到fMRI编码性能超现有方法，能适应新主体、提升解码性能并揭示功能一致性。


<details>
  <summary>Details</summary>
Motivation: 现有确定性方法难以同时建模生物变异性和捕捉潜在功能一致性，需新方法解决视觉到神经映射问题。

Method: 提出SynBrain框架，含BrainVAE通过概率学习建模神经表征并保持功能一致性，以及语义到神经映射器促进高保真fMRI合成。

Result: SynBrain在特定主体视觉到fMRI编码性能超现有方法，能适应新主体、提升数据有限时fMRI到图像解码性能，合成信号揭示功能一致性。

Conclusion: SynBrain是有效的生成框架，能以概率和生物学可解释方式模拟视觉语义到神经反应转换，具有良好性能和应用前景。

Abstract: Deciphering how visual stimuli are transformed into cortical responses is a
fundamental challenge in computational neuroscience. This visual-to-neural
mapping is inherently a one-to-many relationship, as identical visual inputs
reliably evoke variable hemodynamic responses across trials, contexts, and
subjects. However, existing deterministic methods struggle to simultaneously
model this biological variability while capturing the underlying functional
consistency that encodes stimulus information. To address these limitations, we
propose SynBrain, a generative framework that simulates the transformation from
visual semantics to neural responses in a probabilistic and biologically
interpretable manner. SynBrain introduces two key components: (i) BrainVAE
models neural representations as continuous probability distributions via
probabilistic learning while maintaining functional consistency through visual
semantic constraints; (ii) A Semantic-to-Neural Mapper acts as a semantic
transmission pathway, projecting visual semantics into the neural response
manifold to facilitate high-fidelity fMRI synthesis. Experimental results
demonstrate that SynBrain surpasses state-of-the-art methods in
subject-specific visual-to-fMRI encoding performance. Furthermore, SynBrain
adapts efficiently to new subjects with few-shot data and synthesizes
high-quality fMRI signals that are effective in improving data-limited
fMRI-to-image decoding performance. Beyond that, SynBrain reveals functional
consistency across trials and subjects, with synthesized signals capturing
interpretable patterns shaped by biological neural variability. The code will
be made publicly available.

</details>


### [97] [Improving Learning of New Diseases through Knowledge-Enhanced Initialization for Federated Adapter Tuning](https://arxiv.org/abs/2508.10299)
*Danni Peng,Yuan Wang,Kangning Cai,Peiyan Ning,Jiming Xu,Yong Liu,Rick Siow Mong Goh,Qingsong Wei,Huazhu Fu*

Main category: cs.LG

TL;DR: 介绍了Federated Knowledge-Enhanced Initialization (FedKEI)框架，能利用过去知识生成初始化以学习新任务，实验显示其在适应新疾病上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在快速变化的医疗环境中，让个体客户端借助适配器快速适应新任务或疾病，同时利用过往经验。

Method: 提出FedKEI框架，先在服务器进行全局聚类以跨任务泛化知识，再优化簇间和簇内聚合权重；采用双层优化方案学习这些权重。

Result: 在皮肤病学、胸部X光和视网膜OCT三个不同模态的基准数据集上的大量实验表明，FedKEI在适应新疾病方面优于现有方法。

Conclusion: FedKEI框架在利用过去知识进行新任务学习和适应新疾病方面有优势。

Abstract: In healthcare, federated learning (FL) is a widely adopted framework that
enables privacy-preserving collaboration among medical institutions. With large
foundation models (FMs) demonstrating impressive capabilities, using FMs in FL
through cost-efficient adapter tuning has become a popular approach. Given the
rapidly evolving healthcare environment, it is crucial for individual clients
to quickly adapt to new tasks or diseases by tuning adapters while drawing upon
past experiences. In this work, we introduce Federated Knowledge-Enhanced
Initialization (FedKEI), a novel framework that leverages cross-client and
cross-task transfer from past knowledge to generate informed initializations
for learning new tasks with adapters. FedKEI begins with a global clustering
process at the server to generalize knowledge across tasks, followed by the
optimization of aggregation weights across clusters (inter-cluster weights) and
within each cluster (intra-cluster weights) to personalize knowledge transfer
for each new task. To facilitate more effective learning of the inter- and
intra-cluster weights, we adopt a bi-level optimization scheme that
collaboratively learns the global intra-cluster weights across clients and
optimizes the local inter-cluster weights toward each client's task objective.
Extensive experiments on three benchmark datasets of different modalities,
including dermatology, chest X-rays, and retinal OCT, demonstrate FedKEI's
advantage in adapting to new diseases compared to state-of-the-art methods.

</details>


### [98] [A Vision-Language Pre-training Model-Guided Approach for Mitigating Backdoor Attacks in Federated Learning](https://arxiv.org/abs/2508.10315)
*Keke Gai,Dongjue Wang,Jing Yu,Liehuang Zhu,Qi Wu*

Main category: cs.LG

TL;DR: 提出FL后门防御框架CLIP - Fed，结合预聚合和后聚合策略，利用多模态大模型构建增强服务器数据集，用原型对比损失和KL散度对齐知识，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习后门防御方法依赖同质客户端数据分布或干净服务器数据集，在异构客户端数据分布下防御且保持模型性能是挑战。

Method: 提出CLIP - Fed框架，结合预聚合和后聚合防御策略；用多模态大语言模型和频率分析构建增强服务器数据集；用原型对比损失和KL散度对齐全局模型和CLIP知识。

Result: 在CIFAR - 10和CIFAR - 10 - LT上平均ASR分别降低2.03%和1.35%，平均MA分别提高7.92%和0.48%。

Conclusion: CLIP - Fed框架有效，克服非IID对防御效果的限制。

Abstract: Existing backdoor defense methods in Federated Learning (FL) rely on the
assumption of homogeneous client data distributions or the availability of a
clean serve dataset, which limits the practicality and effectiveness. Defending
against backdoor attacks under heterogeneous client data distributions while
preserving model performance remains a significant challenge. In this paper, we
propose a FL backdoor defense framework named CLIP-Fed, which leverages the
zero-shot learning capabilities of vision-language pre-training models. By
integrating both pre-aggregation and post-aggregation defense strategies,
CLIP-Fed overcomes the limitations of Non-IID imposed on defense effectiveness.
To address privacy concerns and enhance the coverage of the dataset against
diverse triggers, we construct and augment the server dataset using the
multimodal large language model and frequency analysis without any client
samples. To address class prototype deviations caused by backdoor samples and
eliminate the correlation between trigger patterns and target labels, CLIP-Fed
aligns the knowledge of the global model and CLIP on the augmented dataset
using prototype contrastive loss and Kullback-Leibler divergence. Extensive
experiments on representative datasets validate the effectiveness of CLIP-Fed.
Compared to state-of-the-art methods, CLIP-Fed achieves an average reduction in
ASR, i.e., 2.03\% on CIFAR-10 and 1.35\% on CIFAR-10-LT, while improving
average MA by 7.92\% and 0.48\%, respectively.

</details>


### [99] [A Hierarchical IDS for Zero-Day Attack Detection in Internet of Medical Things Networks](https://arxiv.org/abs/2508.10346)
*Md Ashraf Uddin,Nam H. Chu,Reza Rafeh*

Main category: cs.LG

TL;DR: 本文指出医疗物联网易受网络攻击，传统集中式入侵检测系统不适用，提出多层医疗物联网入侵检测系统框架，实验取得高准确率和F1分数。


<details>
  <summary>Details</summary>
Motivation: 医疗物联网易受网络攻击，传统集中式入侵检测系统存在响应延迟、隐私风险等问题，本地运行也不可行。

Method: 提出多层医疗物联网入侵检测系统框架，第一层用元学习或单类分类过滤流量，后续层识别攻击类型和新颖性。

Result: 在CICIoMT2024数据集实验中准确率达99.77%，F1分数达97.8%，第一层能高精度检测零日攻击。

Conclusion: 该框架在医疗物联网环境有强适用性，元学习方法效果佳。

Abstract: The Internet of Medical Things (IoMT) is driving a healthcare revolution but
remains vulnerable to cyberattacks such as denial of service, ransomware, data
hijacking, and spoofing. These networks comprise resource constrained,
heterogeneous devices (e.g., wearable sensors, smart pills, implantables),
making traditional centralized Intrusion Detection Systems (IDSs) unsuitable
due to response delays, privacy risks, and added vulnerabilities. Centralized
IDSs require all sensors to transmit data to a central server, causing delays
or network disruptions in dense environments. Running IDSs locally on IoMT
devices is often infeasible due to limited computation, and even lightweight
IDS components remain at risk if updated models are delayed leaving them
exposed to zero-day attacks that threaten patient health and data security. We
propose a multi level IoMT IDS framework capable of detecting zero day attacks
and distinguishing between known and unknown threats. The first layer (near
Edge) filters traffic at a coarse level (attack or not) using meta-learning or
One Class Classification (OCC) with the usfAD algorithm. Subsequent layers (far
Edge, Cloud) identify attack type and novelty. Experiments on the CICIoMT2024
dataset show 99.77 percentage accuracy and 97.8 percentage F1-score. The first
layer detects zero-day attacks with high accuracy without needing new datasets,
ensuring strong applicability in IoMT environments. Additionally, the
meta-learning approach achieves high.

</details>


### [100] [Semantic Communication with Distribution Learning through Sequential Observations](https://arxiv.org/abs/2508.10350)
*Samer Lahoud,Kinda Khawam*

Main category: cs.LG

TL;DR: 本文研究语义通信中的分布学习，建立学习源统计的基本条件，证明可学习性要求，刻画估计收敛率和语义失真，揭示权衡关系，实验验证理论并给出设计原则。


<details>
  <summary>Details</summary>
Motivation: 传统语义通信优化个体意义传输，在先验未知时需研究学习源统计的基本条件。

Method: 建立学习源统计的基本条件，证明可学习性要求，刻画分布估计收敛率，量化估计误差到语义失真的转化。

Result: 揭示编码方案在即时语义性能和长期可学习性间的权衡，CIFAR - 10实验表明系统条件影响学习率和性能。

Conclusion: 首次严格刻画语义通信中的统计学习，为平衡即时性能和适应能力的系统提供设计原则。

Abstract: Semantic communication aims to convey meaning rather than bit-perfect
reproduction, representing a paradigm shift from traditional communication.
This paper investigates distribution learning in semantic communication where
receivers must infer the underlying meaning distribution through sequential
observations. While semantic communication traditionally optimizes individual
meaning transmission, we establish fundamental conditions for learning source
statistics when priors are unknown. We prove that learnability requires full
rank of the effective transmission matrix, characterize the convergence rate of
distribution estimation, and quantify how estimation errors translate to
semantic distortion. Our analysis reveals a fundamental trade-off: encoding
schemes optimized for immediate semantic performance often sacrifice long-term
learnability. Experiments on CIFAR-10 validate our theoretical framework,
demonstrating that system conditioning critically impacts both learning rate
and achievable performance. These results provide the first rigorous
characterization of statistical learning in semantic communication and offer
design principles for systems that balance immediate performance with
adaptation capability.

</details>


### [101] [eMamba: Efficient Acceleration Framework for Mamba Models in Edge Computing](https://arxiv.org/abs/2508.10370)
*Jiyong Kim,Jaeho Lee,Jiahao Lin,Alish Kanani,Miao Sun,Umit Y. Ogras,Jaehyun Park*

Main category: cs.LG

TL;DR: 本文提出eMamba，专为在边缘平台部署Mamba模型的硬件加速框架，实验显示其在参数、性能、能耗等方面有优势。


<details>
  <summary>Details</summary>
Motivation: 现有无优化的硬件加速框架用于在资源受限边缘设备部署Mamba模型，故提出eMamba框架。

Method: 用轻量级硬件感知替代复杂归一化层，近似昂贵操作，进行近似感知神经架构搜索调整参数。

Result: 在多数据集表现与现有技术相当但参数更少，在自然语言任务表现好，在FPGA和ASIC上实验显示有更低延迟、更高吞吐量、更小面积、更低能耗。

Conclusion: eMamba框架在边缘平台部署Mamba模型有显著优势，能在保持竞争力的同时优化多项指标。

Abstract: State Space Model (SSM)-based machine learning architectures have recently
gained significant attention for processing sequential data. Mamba, a recent
sequence-to-sequence SSM, offers competitive accuracy with superior
computational efficiency compared to state-of-the-art transformer models. While
this advantage makes Mamba particularly promising for resource-constrained edge
devices, no hardware acceleration frameworks are currently optimized for
deploying it in such environments. This paper presents eMamba, a comprehensive
end-to-end hardware acceleration framework explicitly designed for deploying
Mamba models on edge platforms. eMamba maximizes computational efficiency by
replacing complex normalization layers with lightweight hardware-aware
alternatives and approximating expensive operations, such as SiLU activation
and exponentiation, considering the target applications. Then, it performs an
approximation-aware neural architecture search (NAS) to tune the learnable
parameters used during approximation. Evaluations with Fashion-MNIST, CIFAR-10,
and MARS, an open-source human pose estimation dataset, show eMamba achieves
comparable accuracy to state-of-the-art techniques using 1.63-19.9$\times$
fewer parameters. In addition, it generalizes well to large-scale natural
language tasks, demonstrating stable perplexity across varying sequence lengths
on the WikiText2 dataset. We also quantize and implement the entire eMamba
pipeline on an AMD ZCU102 FPGA and ASIC using GlobalFoundries (GF) 22 nm
technology. Experimental results show 4.95-5.62$\times$ lower latency and
2.22-9.95$\times$ higher throughput, with 4.77$\times$ smaller area,
9.84$\times$ lower power, and 48.6$\times$ lower energy consumption than
baseline solutions while maintaining competitive accuracy.

</details>


### [102] [A Unified Evaluation Framework for Multi-Annotator Tendency Learning](https://arxiv.org/abs/2508.10393)
*Liyun Zhang,Jingcheng Ke,Shenli Fan,Xuanmeng Sha,Zheng Lian*

Main category: cs.LG

TL;DR: 提出首个统一评估框架来评估多标注者学习中个体倾向学习方法能否捕捉个体倾向并提供行为解释，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏评估个体倾向学习（ITL）方法能否捕捉个体倾向并提供有意义行为解释的评估框架。

Method: 提出统一评估框架，包含两个新指标：差异的标注者间一致性（DIC）和行为对齐可解释性（BAE）。

Result: 大量实验验证了所提出评估框架的有效性。

Conclusion: 所提出的评估框架能有效评估ITL方法捕捉个体倾向和提供行为解释的能力。

Abstract: Recent works have emerged in multi-annotator learning that shift focus from
Consensus-oriented Learning (CoL), which aggregates multiple annotations into a
single ground-truth prediction, to Individual Tendency Learning (ITL), which
models annotator-specific labeling behavior patterns (i.e., tendency) to
provide explanation analysis for understanding annotator decisions. However, no
evaluation framework currently exists to assess whether ITL methods truly
capture individual tendencies and provide meaningful behavioral explanations.
To address this gap, we propose the first unified evaluation framework with two
novel metrics: (1) Difference of Inter-annotator Consistency (DIC) quantifies
how well models capture annotator tendencies by comparing predicted
inter-annotator similarity structures with ground-truth; (2) Behavior Alignment
Explainability (BAE) evaluates how well model explanations reflect annotator
behavior and decision relevance by aligning explainability-derived with
ground-truth labeling similarity structures via Multidimensional Scaling (MDS).
Extensive experiments validate the effectiveness of our proposed evaluation
framework.

</details>


### [103] [XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization](https://arxiv.org/abs/2508.10395)
*Aditya Tomar,Coleman Hooper,Minjae Lee,Haocheng Xi,Rishabh Tiwari,Wonjun Kang,Luca Manolache,Michael W. Mahoney,Kurt Keutzer,Amir Gholami*

Main category: cs.LG

TL;DR: 提出XQuant和XQuant - CL方法，利用计算能力提升减少大语言模型推理内存瓶颈，实现显著内存节省和接近FP16的精度。


<details>
  <summary>Details</summary>
Motivation: 大语言模型推理受内存占用和带宽要求挑战，且计算能力增长快于内存，需新算法用计算换内存操作。

Method: 提出XQuant，量化和缓存层输入激活X，推理时即时重计算键和值；提出XQuant - CL，利用X嵌入的跨层相似性进行极端压缩。

Result: XQuant实现最高约7.7倍内存节省，困惑度下降<0.1；XQuant - CL实现最高10倍内存节省，困惑度仅下降0.01，12.5倍内存节省时困惑度下降0.1。

Conclusion: XQuant利用硬件计算能力消除内存瓶颈，超越现有KV缓存量化方法，在多种模型上实现接近FP16的精度。

Abstract: Although LLM inference has emerged as a critical workload for many downstream
applications, efficiently inferring LLMs is challenging due to the substantial
memory footprint and bandwidth requirements. In parallel, compute capabilities
have steadily outpaced both memory capacity and bandwidth over the last few
decades, a trend that remains evident in modern GPU hardware and exacerbates
the challenge of LLM inference. As such, new algorithms are emerging that trade
increased computation for reduced memory operations. To that end, we present
XQuant, which takes advantage of this trend, enabling an order-of-magnitude
reduction in memory consumption through low-bit quantization with substantial
accuracy benefits relative to state-of-the-art KV cache quantization methods.
We accomplish this by quantizing and caching the layer input activations X,
instead of using standard KV caching, and then rematerializing the Keys and
Values on-the-fly during inference. This results in an immediate 2$\times$
memory savings compared to KV caching. By applying XQuant, we achieve up to
$\sim 7.7\times$ memory savings with $<0.1$ perplexity degradation compared to
the FP16 baseline. Furthermore, our approach leverages the fact that X values
are similar across layers. Building on this observation, we introduce
XQuant-CL, which exploits the cross-layer similarity in the X embeddings for
extreme compression. Across different models, XQuant-CL attains up to
10$\times$ memory savings relative to the FP16 baseline with only 0.01
perplexity degradation, and 12.5$\times$ memory savings with only $0.1$
perplexity degradation. XQuant exploits the rapidly increasing compute
capabilities of hardware platforms to eliminate the memory bottleneck, while
surpassing state-of-the-art KV cache quantization methods and achieving
near-FP16 accuracy across a wide range of models.

</details>


### [104] [SC2Arena and StarEvolve: Benchmark and Self-Improvement Framework for LLMs in Complex Decision-Making Tasks](https://arxiv.org/abs/2508.10428)
*Pengbo Shen,Yaqing Wang,Ni Mu,Yao Luan,Runpeng Xie,Senhao Yang,Lexiang Wang,Hao Hu,Shuang Xu,Yiqin Yang,Bo Xu*

Main category: cs.LG

TL;DR: 提出SC2Arena基准和StarEvolve框架评估大语言模型在星际争霸II中的决策能力，实验显示StarEvolve战略规划表现出色，代码等公开。


<details>
  <summary>Details</summary>
Motivation: 现有星际争霸II基准未能涵盖游戏完整复杂性，需新方法评估大语言模型在复杂决策中的能力。

Method: 提出SC2Arena基准，支持所有种族、低级动作空间等；引入StarEvolve框架，集成战略规划与战术执行，有迭代自校正和持续改进机制。

Result: 使用SC2Arena分析为开发通用智能体提供新见解，StarEvolve在战略规划上表现优异。

Conclusion: SC2Arena和StarEvolve能有效评估大语言模型在星际争霸II中的决策能力，具有一定优势。

Abstract: Evaluating large language models (LLMs) in complex decision-making is
essential for advancing AI's ability for strategic planning and real-time
adaptation. However, existing benchmarks for tasks like StarCraft II fail to
capture the game's full complexity, such as its complete game context, diverse
action spaces, and all playable races. To address this gap, we present
SC2Arena, a benchmark that fully supports all playable races, low-level action
spaces, and optimizes text-based observations to tackle spatial reasoning
challenges. Complementing this, we introduce StarEvolve, a hierarchical
framework that integrates strategic planning with tactical execution, featuring
iterative self-correction and continuous improvement via fine-tuning on
high-quality gameplay data. Its key components include a
Planner-Executor-Verifier structure to break down gameplay, and a scoring
system for selecting high-quality training samples. Comprehensive analysis
using SC2Arena provides valuable insights into developing generalist agents
that were not possible with previous benchmarks. Experimental results also
demonstrate that our proposed StarEvolve achieves superior performance in
strategic planning. Our code, environment, and algorithms are publicly
available.

</details>


### [105] [RealAC: A Domain-Agnostic Framework for Realistic and Actionable Counterfactual Explanations](https://arxiv.org/abs/2508.10455)
*Asiful Arefeen,Shovito Barua Soumma,Hassan Ghasemzadeh*

Main category: cs.LG

TL;DR: 介绍领域无关框架RealAC生成现实且可行的反事实解释，在多数据集评估中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有反事实解释方法泛化性差，难捕捉复杂关系，少考虑用户偏好，需更优方法。

Method: 引入RealAC框架，通过对齐事实与反事实实例特征对联合分布保留特征依赖，优化时允许用户冻结属性。

Result: 在三个合成和两个真实数据集上评估，RealAC在多个指标上优于现有方法。

Conclusion: RealAC能平衡现实性与可行性，为因果感知和以用户为中心的反事实生成提供解决方案。

Abstract: Counterfactual explanations provide human-understandable reasoning for
AI-made decisions by describing minimal changes to input features that would
alter a model's prediction. To be truly useful in practice, such explanations
must be realistic and feasible -- they should respect both the underlying data
distribution and user-defined feasibility constraints. Existing approaches
often enforce inter-feature dependencies through rigid, hand-crafted
constraints or domain-specific knowledge, which limits their generalizability
and ability to capture complex, nonlinear relations inherent in data. Moreover,
they rarely accommodate user-specified preferences and suggest explanations
that are causally implausible or infeasible to act upon. We introduce RealAC, a
domain-agnostic framework for generating realistic and actionable
counterfactuals. RealAC automatically preserves complex inter-feature
dependencies without relying on explicit domain knowledge -- by aligning the
joint distributions of feature pairs between factual and counterfactual
instances. The framework also allows end-users to ``freeze'' attributes they
cannot or do not wish to change by suppressing change in frozen features during
optimization. Evaluations on three synthetic and two real datasets demonstrate
that RealAC balances realism with actionability. Our method outperforms
state-of-the-art baselines and Large Language Model-based counterfactual
generation techniques in causal edge score, dependency preservation score, and
IM1 realism metric and offers a solution for causality-aware and user-centric
counterfactual generation.

</details>


### [106] [X-Node: Self-Explanation is All We Need](https://arxiv.org/abs/2508.10461)
*Prajit Sengupta,Islem Rekik*

Main category: cs.LG

TL;DR: 提出自解释GNN框架X - Node，能让节点在预测时生成自身解释，在两个数据集上评估，保持分类精度同时给出可靠逐节点解释。


<details>
  <summary>Details</summary>
Motivation: 现有GNN可解释性技术事后且全局，难以洞察节点个体决策和局部推理，在高风险临床应用中因缺乏可解释性限制了可信度。

Method: 构建结构化上下文向量编码节点局部拓扑可解释线索，用轻量级推理模块生成解释向量，用于重构节点嵌入、生成自然语言解释和反馈到消息传递流程。

Result: 在MedMNIST和MorphoMNIST两个图数据集上评估，与GCN、GAT和GIN骨干网络集成，X - Node保持了有竞争力的分类精度。

Conclusion: X - Node能在保持分类准确性的同时，为每个节点生成可靠的解释。

Abstract: Graph neural networks (GNNs) have achieved state-of-the-art results in
computer vision and medical image classification tasks by capturing structural
dependencies across data instances. However, their decision-making remains
largely opaque, limiting their trustworthiness in high-stakes clinical
applications where interpretability is essential. Existing explainability
techniques for GNNs are typically post-hoc and global, offering limited insight
into individual node decisions or local reasoning. We introduce X-Node, a
self-explaining GNN framework in which each node generates its own explanation
as part of the prediction process. For every node, we construct a structured
context vector encoding interpretable cues such as degree, centrality,
clustering, feature saliency, and label agreement within its local topology. A
lightweight Reasoner module maps this context into a compact explanation
vector, which serves three purposes: (1) reconstructing the node's latent
embedding via a decoder to enforce faithfulness, (2) generating a natural
language explanation using a pre-trained LLM (e.g., Grok or Gemini), and (3)
guiding the GNN itself via a "text-injection" mechanism that feeds explanations
back into the message-passing pipeline. We evaluate X-Node on two graph
datasets derived from MedMNIST and MorphoMNIST, integrating it with GCN, GAT,
and GIN backbones. Our results show that X-Node maintains competitive
classification accuracy while producing faithful, per-node explanations.
Repository: https://github.com/basiralab/X-Node.

</details>


### [107] [GraphFedMIG: Tackling Class Imbalance in Federated Graph Learning via Mutual Information-Guided Generation](https://arxiv.org/abs/2508.10471)
*Xinrui Li,Qilin Fan,Tianfu Wang,Kaiwen Wei,Ke Yu,Xu Zhang*

Main category: cs.LG

TL;DR: 提出GraphFedMIG框架解决联邦图学习中的类不平衡问题，实验证明其优于其他基线。


<details>
  <summary>Details</summary>
Motivation: 联邦图学习受统计异质性挑战，尤其是类不平衡问题，会使全局模型有偏差、难以学习少数类特征。

Method: 提出GraphFedMIG框架，将问题转化为联邦生成数据增强任务，采用分层生成对抗网络，客户分组共享鉴别器，设计互信息引导机制校正本地生成器参数。

Result: 在四个真实数据集上的实验表明GraphFedMIG优于其他基线。

Conclusion: GraphFedMIG能有效解决联邦图学习中的类不平衡问题。

Abstract: Federated graph learning (FGL) enables multiple clients to collaboratively
train powerful graph neural networks without sharing their private,
decentralized graph data. Inherited from generic federated learning, FGL is
critically challenged by statistical heterogeneity, where non-IID data
distributions across clients can severely impair model performance. A
particularly destructive form of this is class imbalance, which causes the
global model to become biased towards majority classes and fail at identifying
rare but critical events. This issue is exacerbated in FGL, as nodes from a
minority class are often surrounded by biased neighborhood information,
hindering the learning of expressive embeddings. To grapple with this
challenge, we propose GraphFedMIG, a novel FGL framework that reframes the
problem as a federated generative data augmentation task. GraphFedMIG employs a
hierarchical generative adversarial network where each client trains a local
generator to synthesize high-fidelity feature representations. To provide
tailored supervision, clients are grouped into clusters, each sharing a
dedicated discriminator. Crucially, the framework designs a mutual
information-guided mechanism to steer the evolution of these client generators.
By calculating each client's unique informational value, this mechanism
corrects the local generator parameters, ensuring that subsequent rounds of
mutual information-guided generation are focused on producing high-value,
minority-class features. We conduct extensive experiments on four real-world
datasets, and the results demonstrate the superiority of the proposed
GraphFedMIG compared with other baselines.

</details>


### [108] [EDAPT: Towards Calibration-Free BCIs with Continual Online Adaptation](https://arxiv.org/abs/2508.10474)
*Lisa Haxel,Jaivardhan Kapoor,Ulf Ziemann,Jakob H. Macke*

Main category: cs.LG

TL;DR: 介绍EDAPT框架消除脑机接口（BCI）校准，经多数据集测试提升准确率、运行高效，为无校准BCI提供途径。


<details>
  <summary>Details</summary>
Motivation: 解决BCI因神经信号漂移和用户差异导致准确率下降、需频繁校准限制实际部署的问题。

Method: 先使用多用户数据训练基线解码器，在使用中通过监督微调持续个性化模型。

Result: 在九个数据集、三个BCI任务中测试，相比传统静态方法持续提升准确率，结合预训练和在线微调是主因，无监督域适应在部分数据集有额外增益，运行高效，解码准确率与总数据预算相关。

Conclusion: EDAPT为无校准BCI提供实用途径，减少BCI部署障碍。

Abstract: Brain-computer interfaces (BCIs) suffer from accuracy degradation as neural
signals drift over time and vary across users, requiring frequent recalibration
that limits practical deployment. We introduce EDAPT, a task- and
model-agnostic framework that eliminates calibration through continual model
adaptation. EDAPT first trains a baseline decoder using data from multiple
users, then continually personalizes this model via supervised finetuning as
the neural patterns evolve during use. We tested EDAPT across nine datasets
covering three BCI tasks, and found that it consistently improved accuracy over
conventional, static methods. These improvements primarily stem from combining
population-level pretraining and online continual finetuning, with unsupervised
domain adaptation providing further gains on some datasets. EDAPT runs
efficiently, updating models within 200 milliseconds on consumer-grade
hardware. Finally, decoding accuracy scales with total data budget rather than
its allocation between subjects and trials. EDAPT provides a practical pathway
toward calibration-free BCIs, reducing a major barrier to BCI deployment.

</details>


### [109] [Pinet: Optimizing hard-constrained neural networks with orthogonal projection layers](https://arxiv.org/abs/2508.10480)
*Panagiotis D. Grontas,Antonio Terpin,Efe C. Balta,Raffaello D'Andrea,John Lygeros*

Main category: cs.LG

TL;DR: 介绍用于神经网络的输出层Πnet，可确保凸约束满足，在解决约束优化问题上有优势，还应用于多车辆运动规划。


<details>
  <summary>Details</summary>
Motivation: 解决神经网络满足凸约束的问题，为参数化约束优化问题提供可行的优化代理。

Method: 利用算子分裂进行前向传播的快速可靠投影，用隐函数定理进行反向传播。

Result: 解决单个问题比传统求解器快，批量问题更快，在训练时间、解质量和超参数调优鲁棒性上超越现有学习方法，推理时间相近，还用于多车辆运动规划。

Conclusion: Πnet是一个有效的解决凸约束问题的方法，可用于实际的运动规划任务。

Abstract: We introduce an output layer for neural networks that ensures satisfaction of
convex constraints. Our approach, $\Pi$net, leverages operator splitting for
rapid and reliable projections in the forward pass, and the implicit function
theorem for backpropagation. We deploy $\Pi$net as a feasible-by-design
optimization proxy for parametric constrained optimization problems and obtain
modest-accuracy solutions faster than traditional solvers when solving a single
problem, and significantly faster for a batch of problems. We surpass
state-of-the-art learning approaches in terms of training time, solution
quality, and robustness to hyperparameter tuning, while maintaining similar
inference times. Finally, we tackle multi-vehicle motion planning with
non-convex trajectory preferences and provide $\Pi$net as a GPU-ready package
implemented in JAX with effective tuning heuristics.

</details>


### [110] [Learning State-Space Models of Dynamic Systems from Arbitrary Data using Joint Embedding Predictive Architectures](https://arxiv.org/abs/2508.10489)
*Jonas Ulmen,Ganesh Sundaram,Daniel Görges*

Main category: cs.LG

TL;DR: 本文介绍利用连续时间动态系统从任意观测数据创建世界模型的新技术，结合序列嵌入与神经常微分方程，用损失函数构建潜在状态空间，通过单摆系统验证，为机器人领域带来新方法。


<details>
  <summary>Details</summary>
Motivation: 随着JEPAs出现，其比基于重建的方法更强大，需引入新的利用连续时间动态系统从任意观测数据创建世界模型的技术。

Method: 将序列嵌入与神经常微分方程相结合，采用损失函数强制收缩嵌入和状态转换中的Lipschitz常数来构建潜在状态空间。

Result: 仅使用图像数据为简单单摆系统生成了结构化潜在状态空间模型。

Conclusion: 为开发更通用的控制算法和估计技术开辟了新途径，在机器人领域有广泛应用。

Abstract: With the advent of Joint Embedding Predictive Architectures (JEPAs), which
appear to be more capable than reconstruction-based methods, this paper
introduces a novel technique for creating world models using continuous-time
dynamic systems from arbitrary observation data. The proposed method integrates
sequence embeddings with neural ordinary differential equations (neural ODEs).
It employs loss functions that enforce contractive embeddings and Lipschitz
constants in state transitions to construct a well-organized latent state
space. The approach's effectiveness is demonstrated through the generation of
structured latent state-space models for a simple pendulum system using only
image data. This opens up a new technique for developing more general control
algorithms and estimation techniques with broad applications in robotics.

</details>


### [111] [On the Complexity-Faithfulness Trade-off of Gradient-Based Explanations](https://arxiv.org/abs/2508.10490)
*Amir Mehrpanah,Matteo Gamba,Kevin Smith,Hossein Azizpour*

Main category: cs.LG

TL;DR: 提出统一频谱框架分析ReLU网络解释的平滑性、忠实性及其权衡，定义并度量解释差距，且验证理论结果。


<details>
  <summary>Details</summary>
Motivation: ReLU网络使基于梯度的解释有噪声且难解释，现有方法平滑解释会牺牲忠实性。

Method: 引入统一频谱框架分析和量化平滑性与忠实性及其权衡，量化并正则化ReLU网络对高频信息的贡献。

Result: 分析了基于代理的平滑如何扭曲解释，定义并度量了不同事后方法的“解释差距”。

Conclusion: 在不同设计选择、数据集和消融实验中验证了理论发现。

Abstract: ReLU networks, while prevalent for visual data, have sharp transitions,
sometimes relying on individual pixels for predictions, making vanilla
gradient-based explanations noisy and difficult to interpret. Existing methods,
such as GradCAM, smooth these explanations by producing surrogate models at the
cost of faithfulness. We introduce a unifying spectral framework to
systematically analyze and quantify smoothness, faithfulness, and their
trade-off in explanations. Using this framework, we quantify and regularize the
contribution of ReLU networks to high-frequency information, providing a
principled approach to identifying this trade-off. Our analysis characterizes
how surrogate-based smoothing distorts explanations, leading to an
``explanation gap'' that we formally define and measure for different post-hoc
methods. Finally, we validate our theoretical findings across different design
choices, datasets, and ablations.

</details>


### [112] [Contrastive ECOC: Learning Output Codes for Adversarial Defense](https://arxiv.org/abs/2508.10491)
*Che-Yu Chou,Hung-Hsuan Chen*

Main category: cs.LG

TL;DR: 本文提出基于对比学习的自动码本学习模型，在四个数据集上比基线模型对对抗攻击更具鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统纠错输出码（ECOC）方法依赖手动设计或随机生成码本，费力且可能产生次优、与数据集无关的结果，需要更有效的编码机制。

Method: 引入三个基于对比学习的自动码本学习模型，直接从数据中自适应学习码本。

Result: 在四个数据集上，提出的模型比两个基线模型对对抗攻击有更强的鲁棒性。

Conclusion: 基于对比学习的自动码本学习模型是有效的，在对抗攻击下表现更好。

Abstract: Although one-hot encoding is commonly used for multiclass classification, it
is not always the most effective encoding mechanism. Error Correcting Output
Codes (ECOC) address multiclass classification by mapping each class to a
unique codeword used as a label. Traditional ECOC methods rely on manually
designed or randomly generated codebooks, which are labor-intensive and may
yield suboptimal, dataset-agnostic results. This paper introduces three models
for automated codebook learning based on contrastive learning, allowing
codebooks to be learned directly and adaptively from data. Across four
datasets, our proposed models demonstrate superior robustness to adversarial
attacks compared to two baselines. The source is available at
https://github.com/YuChou20/Automated-Codebook-Learning-with-Error-Correcting-Output-Code-Technique.

</details>


### [113] [A Unified Multi-Agent Framework for Universal Multimodal Understanding and Generation](https://arxiv.org/abs/2508.10494)
*Jiulin Li,Ping Huang,Yexin Li,Shuo Chen,Juewen Hu,Ye Tian*

Main category: cs.LG

TL;DR: 提出MAGUS框架统一多模态理解与生成，实验显示其表现优于基线和现有系统。


<details>
  <summary>Details</summary>
Motivation: 现实多模态应用需任意模态转换能力，但整合自回归语言模型推理和扩散模型生成能力有挑战，现有方法灵活性和可扩展性受限。

Method: 提出MAGUS框架，分认知和审议两阶段，认知阶段多模态LLM代理协作对话，审议阶段用增长感知搜索机制协调推理与生成。

Result: 在多个基准测试中，MAGUS表现优于强基线和现有系统，在MME基准上超越GPT - 4o。

Conclusion: MAGUS框架有效可行，具备即插即用扩展性、可扩展的任意模态转换和语义对齐能力，无需联合训练。

Abstract: Real-world multimodal applications often require any-to-any capabilities,
enabling both understanding and generation across modalities including text,
image, audio, and video. However, integrating the strengths of autoregressive
language models (LLMs) for reasoning and diffusion models for high-fidelity
generation remains challenging. Existing approaches rely on rigid pipelines or
tightly coupled architectures, limiting flexibility and scalability. We propose
MAGUS (Multi-Agent Guided Unified Multimodal System), a modular framework that
unifies multimodal understanding and generation via two decoupled phases:
Cognition and Deliberation. MAGUS enables symbolic multi-agent collaboration
within a shared textual workspace. In the Cognition phase, three
role-conditioned multimodal LLM agents - Perceiver, Planner, and Reflector -
engage in collaborative dialogue to perform structured understanding and
planning. The Deliberation phase incorporates a Growth-Aware Search mechanism
that orchestrates LLM-based reasoning and diffusion-based generation in a
mutually reinforcing manner. MAGUS supports plug-and-play extensibility,
scalable any-to-any modality conversion, and semantic alignment - all without
the need for joint training. Experiments across multiple benchmarks, including
image, video, and audio generation, as well as cross-modal instruction
following, demonstrate that MAGUS outperforms strong baselines and
state-of-the-art systems. Notably, on the MME benchmark, MAGUS surpasses the
powerful closed-source model GPT-4o.

</details>


### [114] [Nonlocal Monte Carlo via Reinforcement Learning](https://arxiv.org/abs/2508.10520)
*Dmitrii Dobrynin,Masoud Mohseni,John Paul Strachan*

Main category: cs.LG

TL;DR: 本文用深度强化学习训练非平衡非局部蒙特卡罗（NMC）算法的非局部转移策略，在困难的4 - SAT基准测试中表现优于标准MCMC和非局部模拟退火算法。


<details>
  <summary>Details</summary>
Motivation: 传统基于MCMC的算法在处理接近计算相变的难题时效果不佳，难以解冻刚性变量、逃离次优吸引盆地和采样高质量多样解，需要改进算法。

Method: 采用深度强化学习训练NMC的非局部转移策略，通过观察配置空间探索的能量变化作为奖励、局部最小能量景观几何作为状态进行训练。

Result: 训练后的策略在困难的均匀随机和无标度随机4 - SAT基准测试的剩余能量、求解时间和解的多样性指标上优于标准MCMC和非局部模拟退火算法。

Conclusion: 使用深度强化学习训练NMC的非局部转移策略是有效的，能提升算法在困难组合优化问题上的性能。

Abstract: Optimizing or sampling complex cost functions of combinatorial optimization
problems is a longstanding challenge across disciplines and applications. When
employing family of conventional algorithms based on Markov Chain Monte Carlo
(MCMC) such as simulated annealing or parallel tempering, one assumes
homogeneous (equilibrium) temperature profiles across input. This instance
independent approach was shown to be ineffective for the hardest benchmarks
near a computational phase transition when the so-called overlap-gap-property
holds. In these regimes conventional MCMC struggles to unfreeze rigid
variables, escape suboptimal basins of attraction, and sample high-quality and
diverse solutions. In order to mitigate these challenges, Nonequilibrium
Nonlocal Monte Carlo (NMC) algorithms were proposed that leverage inhomogeneous
temperature profiles thereby accelerating exploration of the configuration
space without compromising its exploitation. Here, we employ deep reinforcement
learning (RL) to train the nonlocal transition policies of NMC which were
previously designed phenomenologically. We demonstrate that the resulting
solver can be trained solely by observing energy changes of the configuration
space exploration as RL rewards and the local minimum energy landscape geometry
as RL states. We further show that the trained policies improve upon the
standard MCMC-based and nonlocal simulated annealing on hard uniform random and
scale-free random 4-SAT benchmarks in terms of residual energy,
time-to-solution, and diversity of solutions metrics.

</details>


### [115] [Projected Coupled Diffusion for Test-Time Constrained Joint Generation](https://arxiv.org/abs/2508.10531)
*Hao Luan,Yi Xian Goh,See-Kiong Ng,Chun Kai Ling*

Main category: cs.LG

TL;DR: 提出Projected Coupled Diffusion (PCD)框架用于约束联合生成，在多场景验证其有效性，效果好且成本不高。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以在不昂贵再训练的情况下，从多个预训练扩散模型生成联合相关样本并同时执行特定任务约束。

Method: 提出PCD框架，在生成动力学中引入耦合引导项促进模型间协调，并在每个扩散步骤加入投影步骤以执行硬约束。

Result: 在图像对生成、对象操作和多机器人运动规划场景中展示了有效性，有更好的耦合效果且保证约束满足，无过高计算成本。

Conclusion: PCD框架能有效解决从多个预训练扩散模型生成联合相关样本并执行特定任务约束的问题，且成本可控。

Abstract: Modifications to test-time sampling have emerged as an important extension to
diffusion algorithms, with the goal of biasing the generative process to
achieve a given objective without having to retrain the entire diffusion model.
However, generating jointly correlated samples from multiple pre-trained
diffusion models while simultaneously enforcing task-specific constraints
without costly retraining has remained challenging. To this end, we propose
Projected Coupled Diffusion (PCD), a novel test-time framework for constrained
joint generation. PCD introduces a coupled guidance term into the generative
dynamics to encourage coordination between diffusion models and incorporates a
projection step at each diffusion step to enforce hard constraints.
Empirically, we demonstrate the effectiveness of PCD in application scenarios
of image-pair generation, object manipulation, and multi-robot motion planning.
Our results show improved coupling effects and guaranteed constraint
satisfaction without incurring excessive computational costs.

</details>


### [116] [Driving Accurate Allergen Prediction with Protein Language Models and Generalization-Focused Evaluation](https://arxiv.org/abs/2508.10541)
*Brian Shing-Hei Wong,Joshua Mincheol Kim,Sin-Hang Fung,Qing Xiong,Kelvin Fu-Kiu Ao,Junkang Wei,Ran Wang,Dan Michelle Wang,Jingying Zhou,Bo Feng,Alfred Sze-Lok Cheng,Kevin Y. Yip,Stephen Kwok-Wing Tsui,Qin Cao*

Main category: cs.LG

TL;DR: 本文介绍了基于xTrimoPGLM蛋白语言模型的计算框架Applm用于准确识别过敏原蛋白，其性能优于七种现有方法，还提供开源软件和基准数据集。


<details>
  <summary>Details</summary>
Motivation: 准确识别过敏原蛋白，应对其带来的公共健康挑战。

Method: 引入计算框架Applm，利用1000亿参数的xTrimoPGLM蛋白语言模型。

Result: Applm在多种类似现实难题的任务中持续优于七种现有方法。

Conclusion: xTrimoPGLM对Applm性能很关键，同时提供开源软件和基准数据集以推动未来研究。

Abstract: Allergens, typically proteins capable of triggering adverse immune responses,
represent a significant public health challenge. To accurately identify
allergen proteins, we introduce Applm (Allergen Prediction with Protein
Language Models), a computational framework that leverages the 100-billion
parameter xTrimoPGLM protein language model. We show that Applm consistently
outperforms seven state-of-the-art methods in a diverse set of tasks that
closely resemble difficult real-world scenarios. These include identifying
novel allergens that lack similar examples in the training set, differentiating
between allergens and non-allergens among homologs with high sequence
similarity, and assessing functional consequences of mutations that create few
changes to the protein sequences. Our analysis confirms that xTrimoPGLM,
originally trained on one trillion tokens to capture general protein sequence
characteristics, is crucial for Applm's performance by detecting important
differences among protein sequences. In addition to providing Applm as
open-source software, we also provide our carefully curated benchmark datasets
to facilitate future research.

</details>


### [117] [Stabilizing Long-term Multi-turn Reinforcement Learning with Gated Rewards](https://arxiv.org/abs/2508.10548)
*Zetian Sun,Dongfang Li,Zhuoen Chen,Yuhuai Qin,Baotian Hu*

Main category: cs.LG

TL;DR: 针对长周期强化学习奖励稀疏问题，提出面向软件工程的RL框架和门控奖励累积（G - RA）方法，实验证明G - RA提升完成率和修改率，避免策略退化。


<details>
  <summary>Details</summary>
Motivation: 解决长周期强化学习中奖励稀疏问题，现有基于结果的奖励塑造和基于验证的奖励塑造存在不足。

Method: 引入面向软件工程的RL框架，提出G - RA方法，仅当高级（长期）奖励达到预定义阈值时累积即时奖励。

Result: 在SWE - bench Verified和kBench实验中，G - RA使完成率和修改率提升，避免奖励不一致导致的策略退化。

Conclusion: 强调长周期强化学习中平衡奖励累积的重要性，提供了实用解决方案。

Abstract: Reward sparsity in long-horizon reinforcement learning (RL) tasks remains a
significant challenge, while existing outcome-based reward shaping struggles to
define meaningful immediate rewards without introducing bias or requiring
explicit task decomposition. Alternatively, verification-based reward shaping
uses stepwise critics, but misalignment between immediate rewards and long-term
objectives can lead to reward hacking and suboptimal policies. In this work, we
address this problem in the context of software engineering (SWE) tasks, where
multi-turn reasoning and rule-based verification are critical. We introduce the
SWE-oriented RL Framework, a unified system supporting multi-turn interaction,
docker-based execution, and customizable reward functions. Additionally, we
propose Gated Reward Accumulation (G-RA), a novel method that accumulates
immediate rewards only when high-level (long-term) rewards meet a predefined
threshold, ensuring stable RL optimization. Experiments on SWE-bench Verified
and kBench demonstrate that G-RA leads to an increase in completion rates
(47.6\% \rightarrow 93.8\% and 22.0\% \rightarrow 86.0\%) and modification
rates (19.6\% \rightarrow 23.8\% and 12.0\% \rightarrow 42.0\%), while avoiding
policy degradation caused by reward misalignment. Our findings highlight the
importance of balanced reward accumulation in long-horizon RL and provide a
practical solution.

</details>


### [118] [Technical Report: Facilitating the Adoption of Causal Inference Methods Through LLM-Empowered Co-Pilot](https://arxiv.org/abs/2508.10581)
*Jeroen Berrevoets,Julianna Piskorz,Robert Davis,Harry Amad,Jim Weatherall,Mihaela van der Schaar*

Main category: cs.LG

TL;DR: 文章介绍开源系统CATE - B辅助处理效应估计，结合因果推断与智能交互，降低严格因果分析门槛并提供基准任务。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习和因果推断技术在处理效应估计上虽有进展，但因需专业知识，应用受限。

Method: 引入CATE - B系统，利用大语言模型，通过因果发现和基于LLM的边定向构建结构因果模型，用新准则识别调整集，根据因果结构和数据特征选择回归方法，还发布基准任务。

Result: 构建了CATE - B系统并发布基准任务。

Conclusion: CATE - B降低严格因果分析门槛，为自动处理效应估计的新基准奠定基础。

Abstract: Estimating treatment effects (TE) from observational data is a critical yet
complex task in many fields, from healthcare and economics to public policy.
While recent advances in machine learning and causal inference have produced
powerful estimation techniques, their adoption remains limited due to the need
for deep expertise in causal assumptions, adjustment strategies, and model
selection. In this paper, we introduce CATE-B, an open-source co-pilot system
that uses large language models (LLMs) within an agentic framework to guide
users through the end-to-end process of treatment effect estimation. CATE-B
assists in (i) constructing a structural causal model via causal discovery and
LLM-based edge orientation, (ii) identifying robust adjustment sets through a
novel Minimal Uncertainty Adjustment Set criterion, and (iii) selecting
appropriate regression methods tailored to the causal structure and dataset
characteristics. To encourage reproducibility and evaluation, we release a
suite of benchmark tasks spanning diverse domains and causal complexities. By
combining causal inference with intelligent, interactive assistance, CATE-B
lowers the barrier to rigorous causal analysis and lays the foundation for a
new class of benchmarks in automated treatment effect estimation.

</details>


### [119] [GNN-based Unified Deep Learning](https://arxiv.org/abs/2508.10583)
*Furkan Pala,Islem Rekik*

Main category: cs.LG

TL;DR: 提出统一学习范式解决医学影像中深度学习模型泛化性问题，在多个基准测试中表现良好。


<details>
  <summary>Details</summary>
Motivation: 医学影像中深度学习模型在领域分裂场景下泛化性差，不同医院需训练不同模型，如何跨数据集统一训练异质模型提升泛化性是难题。

Method: 将每个模型编码为图表示，在共享图学习空间统一，用GNN引导优化，通过解耦模型参数并由统一GNN控制，支持跨架构和分布的参数共享与知识转移。

Result: 在MorphoMNIST、PneumoniaMNIST和BreastMNIST基准测试中，统一学习在独特分布训练和混合分布测试时提升性能，对分布偏移大的未见数据有强鲁棒性。

Conclusion: 统一学习范式有效提升医学影像深度学习模型的泛化性。

Abstract: Deep learning models often struggle to maintain generalizability in medical
imaging, particularly under domain-fracture scenarios where distribution shifts
arise from varying imaging techniques, acquisition protocols, patient
populations, demographics, and equipment. In practice, each hospital may need
to train distinct models - differing in learning task, width, and depth - to
match local data. For example, one hospital may use Euclidean architectures
such as MLPs and CNNs for tabular or grid-like image data, while another may
require non-Euclidean architectures such as graph neural networks (GNNs) for
irregular data like brain connectomes. How to train such heterogeneous models
coherently across datasets, while enhancing each model's generalizability,
remains an open problem. We propose unified learning, a new paradigm that
encodes each model into a graph representation, enabling unification in a
shared graph learning space. A GNN then guides optimization of these unified
models. By decoupling parameters of individual models and controlling them
through a unified GNN (uGNN), our method supports parameter sharing and
knowledge transfer across varying architectures (MLPs, CNNs, GNNs) and
distributions, improving generalizability. Evaluations on MorphoMNIST and two
MedMNIST benchmarks - PneumoniaMNIST and BreastMNIST - show that unified
learning boosts performance when models are trained on unique distributions and
tested on mixed ones, demonstrating strong robustness to unseen data with large
distribution shifts. Code and benchmarks: https://github.com/basiralab/uGNN

</details>


### [120] [Self-Supervised Temporal Super-Resolution of Energy Data using Generative Adversarial Transformer](https://arxiv.org/abs/2508.10587)
*Xuanhao Mu,Gökhan Demirel,Yuzhe Zhang,Jianlei Liu,Thorsten Schlachter,Veit Hagenmeyer*

Main category: cs.LG

TL;DR: 为解决能源网络设计和运行中的时间粒度差距问题，传统上采样方法有缺陷，本文引入无需高分辨率数据训练的生成对抗变压器（GATs）方法，降低了上采样任务的均方根误差，提高了模型预测控制应用场景的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列上采样方法存在信息损失或噪声增加问题，先进模型面临与上采样定义不一致、输入数据稀疏、依赖监督学习等挑战，需要解决上采样问题。

Method: 引入利用生成对抗变压器（GATs）的新方法，且该方法可在无任何真实高分辨率数据的情况下训练。

Result: 与传统插值方法相比，该方法可将上采样任务的均方根误差（RMSE）降低9%，模型预测控制（MPC）应用场景的准确性提高13%。

Conclusion: 新的利用GATs的方法能有效解决时间序列上采样问题，提高相关任务的准确性。

Abstract: To bridge the temporal granularity gap in energy network design and operation
based on Energy System Models, resampling of time series is required. While
conventional upsampling methods are computationally efficient, they often
result in significant information loss or increased noise. Advanced models such
as time series generation models, Super-Resolution models and imputation models
show potential, but also face fundamental challenges. The goal of time series
generative models is to learn the distribution of the original data to generate
high-resolution series with similar statistical characteristics. This is not
entirely consistent with the definition of upsampling. Time series
Super-Resolution models or imputation models can degrade the accuracy of
upsampling because the input low-resolution time series are sparse and may have
insufficient context. Moreover, such models usually rely on supervised learning
paradigms. This presents a fundamental application paradox: their training
requires the high-resolution time series that is intrinsically absent in
upsampling application scenarios. To address the mentioned upsampling issue,
this paper introduces a new method utilizing Generative Adversarial
Transformers (GATs), which can be trained without access to any ground-truth
high-resolution data. Compared with conventional interpolation methods, the
introduced method can reduce the root mean square error (RMSE) of upsampling
tasks by 9%, and the accuracy of a model predictive control (MPC) application
scenario is improved by 13%.

</details>


### [121] [FreeGAD: A Training-Free yet Effective Approach for Graph Anomaly Detection](https://arxiv.org/abs/2508.10594)
*Yunfeng Zhao,Yixin Liu,Shiyuan Li,Qingfeng Chen,Yu Zheng,Shirui Pan*

Main category: cs.LG

TL;DR: 现有深度学习GAD方法有高部署成本和可扩展性差问题，提出无需训练的FreeGAD方法，实验表明其在多数据集上性能、效率和可扩展性优越。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习GAD方法训练过程复杂、资源密集，导致高部署成本和可扩展性差，且训练阶段对检测性能贡献不如预期。

Method: 提出FreeGAD方法，利用亲和门控残差编码器生成异常感知表示，识别锚节点作为伪正常和异常引导，通过锚引导的统计偏差计算异常分数。

Result: FreeGAD在多个不同领域的基准数据集上，无需训练或迭代优化，实现了优越的异常检测性能、效率和可扩展性。

Conclusion: FreeGAD是一种有效的无需训练的图异常检测方法，能解决现有方法的问题。

Abstract: Graph Anomaly Detection (GAD) aims to identify nodes that deviate from the
majority within a graph, playing a crucial role in applications such as social
networks and e-commerce. Despite the current advancements in deep
learning-based GAD, existing approaches often suffer from high deployment costs
and poor scalability due to their complex and resource-intensive training
processes. Surprisingly, our empirical findings suggest that the training phase
of deep GAD methods, commonly perceived as crucial, may actually contribute
less to anomaly detection performance than expected. Inspired by this, we
propose FreeGAD, a novel training-free yet effective GAD method. Specifically,
it leverages an affinity-gated residual encoder to generate anomaly-aware
representations. Meanwhile, FreeGAD identifies anchor nodes as pseudo-normal
and anomalous guides, followed by calculating anomaly scores through
anchor-guided statistical deviations. Extensive experiments demonstrate that
FreeGAD achieves superior anomaly detection performance, efficiency, and
scalability on multiple benchmark datasets from diverse domains, without any
training or iterative optimization.

</details>


### [122] [On Spectral Properties of Gradient-based Explanation Methods](https://arxiv.org/abs/2508.10595)
*Amir Mehrpanah,Erik Englesson,Hossein Azizpour*

Main category: cs.LG

TL;DR: 采用新视角分析深度网络解释方法，揭示梯度谱偏差，指出解释方法中扰动超参数问题并提出两种补救措施，最后进行定量评估。


<details>
  <summary>Details</summary>
Motivation: 当前解释深度网络预测结果的研究存在可靠性问题，原因是形式化不足，需要更好地理解深度网络行为以增强对其结果的信心。

Method: 采用新颖的概率和谱视角对解释方法进行形式分析。

Result: 揭示了使用梯度产生的普遍谱偏差，解释了一些实验中发现的常见设计选择；指出解释方法中扰动超参数选择会导致解释不一致；提出确定标准扰动规模的机制和SpectralLens聚合方法。

Conclusion: 通过定量评估证实了理论结果。

Abstract: Understanding the behavior of deep networks is crucial to increase our
confidence in their results. Despite an extensive body of work for explaining
their predictions, researchers have faced reliability issues, which can be
attributed to insufficient formalism. In our research, we adopt novel
probabilistic and spectral perspectives to formally analyze explanation
methods. Our study reveals a pervasive spectral bias stemming from the use of
gradient, and sheds light on some common design choices that have been
discovered experimentally, in particular, the use of squared gradient and input
perturbation. We further characterize how the choice of perturbation
hyperparameters in explanation methods, such as SmoothGrad, can lead to
inconsistent explanations and introduce two remedies based on our proposed
formalism: (i) a mechanism to determine a standard perturbation scale, and (ii)
an aggregation method which we call SpectralLens. Finally, we substantiate our
theoretical results through quantitative evaluations.

</details>


### [123] [Oops!... They Stole it Again: Attacks on Split Learning](https://arxiv.org/abs/2508.10598)
*Tanveer Khan,Antonis Michalas*

Main category: cs.LG

TL;DR: 本文系统回顾了对拆分学习（SL）的各种攻击，分析现有防御方法，揭示安全漏洞并指明未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 拆分学习的分布式特性带来新安全挑战，需全面探索潜在攻击。

Method: 系统回顾SL的各种攻击并基于攻击者角色、隐私风险类型等因素分类，分析现有防御方法。

Result: 揭示现有防御的安全漏洞，明确有效性和局限性。

Conclusion: 为改善SL隐私问题和指导后续研究提供有价值信息。

Abstract: Split Learning (SL) is a collaborative learning approach that improves
privacy by keeping data on the client-side while sharing only the intermediate
output with a server. However, the distributed nature of SL introduces new
security challenges, necessitating a comprehensive exploration of potential
attacks. This paper systematically reviews various attacks on SL, classifying
them based on factors such as the attacker's role, the type of privacy risks,
when data leaks occur, and where vulnerabilities exist. We also analyze
existing defense methods, including cryptographic methods, data modification
approaches, distributed techniques, and hybrid solutions. Our findings reveal
security gaps, highlighting the effectiveness and limitations of existing
defenses. By identifying open challenges and future directions, this work
provides valuable information to improve SL privacy issues and guide further
research.

</details>


### [124] [Variance Reduced Policy Gradient Method for Multi-Objective Reinforcement Learning](https://arxiv.org/abs/2508.10608)
*Davide Guidobene,Lorenzo Benedetti,Diego Arapovic*

Main category: cs.LG

TL;DR: 文章聚焦多目标强化学习（MORL）中策略梯度方法（PGMs）样本效率低的问题，提出用方差缩减技术解决。


<details>
  <summary>Details</summary>
Motivation: 现有MORL的PGMs样本效率低，以往解决方法假设过于严格，失去PGMs在大规模状态 - 动作空间的可扩展性优势。

Method: 实施方差缩减技术，在保持一般假设的情况下降低策略梯度的样本复杂度。

Result: 未提及。

Conclusion: 未提及。

Abstract: Multi-Objective Reinforcement Learning (MORL) is a generalization of
traditional Reinforcement Learning (RL) that aims to optimize multiple, often
conflicting objectives simultaneously rather than focusing on a single reward.
This approach is crucial in complex decision-making scenarios where agents must
balance trade-offs between various goals, such as maximizing performance while
minimizing costs. We consider the problem of MORL where the objectives are
combined using a non-linear scalarization function. Just like in standard RL,
policy gradient methods (PGMs) are amongst the most effective for handling
large and continuous state-action spaces in MORL. However, existing PGMs for
MORL suffer from high sample inefficiency, requiring large amounts of data to
be effective. Previous attempts to solve this problem rely on overly strict
assumptions, losing PGMs' benefits in scalability to large state-action spaces.
In this work, we address the issue of sample efficiency by implementing
variance-reduction techniques to reduce the sample complexity of policy
gradients while maintaining general assumptions.

</details>


### [125] [Beyond Random Sampling: Instance Quality-Based Data Partitioning via Item Response Theory](https://arxiv.org/abs/2508.10628)
*Lucas Cardoso,Vitor Santos,José Ribeiro Filho,Ricardo Prudêncio,Regiane Kawasaki,Ronnie Alves*

Main category: cs.LG

TL;DR: 研究提出用项目反应理论（IRT）参数指导机器学习模型验证阶段的数据集划分，评估其对多个模型性能的影响，结果显示IRT有助于理解模型偏差和方差权衡，猜测参数影响大。


<details>
  <summary>Details</summary>
Motivation: 传统数据划分方法常忽略实例内在质量，需要对机器学习模型进行稳健验证。

Method: 使用IRT参数表征和指导数据集划分，评估IRT划分策略对四个表格数据集上多个机器学习模型性能的影响。

Result: IRT揭示实例固有异质性和信息性子组，基于IRT的平衡划分有助于理解模型偏差和方差权衡，猜测参数影响大，高猜测实例训练会损害模型性能。

Conclusion: IRT参数可有效指导数据集划分，帮助更好理解模型性能，猜测参数在模型训练中是关键因素。

Abstract: Robust validation of Machine Learning (ML) models is essential, but
traditional data partitioning approaches often ignore the intrinsic quality of
each instance. This study proposes the use of Item Response Theory (IRT)
parameters to characterize and guide the partitioning of datasets in the model
validation stage. The impact of IRT-informed partitioning strategies on the
performance of several ML models in four tabular datasets was evaluated. The
results obtained demonstrate that IRT reveals an inherent heterogeneity of the
instances and highlights the existence of informative subgroups of instances
within the same dataset. Based on IRT, balanced partitions were created that
consistently help to better understand the tradeoff between bias and variance
of the models. In addition, the guessing parameter proved to be a determining
factor: training with high-guessing instances can significantly impair model
performance and resulted in cases with accuracy below 50%, while other
partitions reached more than 70% in the same dataset.

</details>


### [126] [Energy-Based Models for Predicting Mutational Effects on Proteins](https://arxiv.org/abs/2508.10629)
*Patrick Soga,Zhenyu Lei,Yinhan He,Camille Bilodeau,Jundong Li*

Main category: cs.LG

TL;DR: 提出新的ΔΔG预测方法，结合序列和结构组件，在预测和抗体优化中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 以往估计蛋白质复合物全构象分布难处理，需新的ΔΔG预测方法。

Method: 将ΔΔG分解为序列组件和结构组件，假设结合和未结合状态平衡，引入基于能量的物理归纳偏置。

Result: 在ΔΔG预测和针对SARS - CoV - 2的抗体优化中优于现有深度学习方法。

Conclusion: 新的ΔΔG预测方法有效且有优势。

Abstract: Predicting changes in binding free energy ($\Delta\Delta G$) is a vital task
in protein engineering and protein-protein interaction (PPI) engineering for
drug discovery. Previous works have observed a high correlation between
$\Delta\Delta G$ and entropy, using probabilities of biologically important
objects such as side chain angles and residue identities to estimate
$\Delta\Delta G$. However, estimating the full conformational distribution of a
protein complex is generally considered intractable. In this work, we propose a
new approach to $\Delta\Delta G$ prediction that avoids this issue by instead
leveraging energy-based models for estimating the probability of a complex's
conformation. Specifically, we novelly decompose $\Delta\Delta G$ into a
sequence-based component estimated by an inverse folding model and a
structure-based component estimated by an energy model. This decomposition is
made tractable by assuming equilibrium between the bound and unbound states,
allowing us to simplify the estimation of degeneracies associated with each
state. Unlike previous deep learning-based methods, our method incorporates an
energy-based physical inductive bias by connecting the often-used sequence
log-odds ratio-based approach to $\Delta\Delta G$ prediction with a new
$\Delta\Delta E$ term grounded in statistical mechanics. We demonstrate
superiority over existing state-of-the-art structure and sequence-based deep
learning methods in $\Delta\Delta G$ prediction and antibody optimization
against SARS-CoV-2.

</details>


### [127] [Conditional Information Bottleneck for Multimodal Fusion: Overcoming Shortcut Learning in Sarcasm Detection](https://arxiv.org/abs/2508.10644)
*Yihua Wang,Qi Jia,Cong Xu,Feiyu Chen,Yuhan Liu,Haotian Zhang,Liang Jin,Lu Liu,Zhichun Wang*

Main category: cs.LG

TL;DR: 指出多模态讽刺检测现有方法依赖捷径学习及模态融合策略有弱点，通过去除捷径信号构建数据集，引入MCIB模型实现有效多模态融合，实验显示其效果佳。


<details>
  <summary>Details</summary>
Motivation: 解决多模态讽刺检测中现有方法依赖捷径学习影响泛化能力，以及当前模态融合策略存在弱点的问题。

Method: 从MUStARD++中移除捷径信号构建MUStARD++$^{R}$，引入多模态条件信息瓶颈（MCIB）模型进行多模态融合。

Result: MCIB模型在不依赖捷径学习的情况下取得了最佳性能。

Conclusion: 应聚焦于有效模态融合进行复杂情感识别，MCIB模型可用于多模态讽刺检测。

Abstract: Multimodal sarcasm detection is a complex task that requires distinguishing
subtle complementary signals across modalities while filtering out irrelevant
information. Many advanced methods rely on learning shortcuts from datasets
rather than extracting intended sarcasm-related features. However, our
experiments show that shortcut learning impairs the model's generalization in
real-world scenarios. Furthermore, we reveal the weaknesses of current modality
fusion strategies for multimodal sarcasm detection through systematic
experiments, highlighting the necessity of focusing on effective modality
fusion for complex emotion recognition. To address these challenges, we
construct MUStARD++$^{R}$ by removing shortcut signals from MUStARD++. Then, a
Multimodal Conditional Information Bottleneck (MCIB) model is introduced to
enable efficient multimodal fusion for sarcasm detection. Experimental results
show that the MCIB achieves the best performance without relying on shortcut
learning.

</details>


### [128] [SPHENIC: Topology-Informed Multi-View Clustering for Spatial Transcriptomics](https://arxiv.org/abs/2508.10646)
*Chenkai Guo,Yikai Zhu,Jing Yangum,Renxiang Guan,Por Lip Yee,Guangdun Peng,Dayu Hu*

Main category: cs.LG

TL;DR: 本文提出SPHENIC方法用于空间转录组聚类，实验显示其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有空间转录组聚类方法存在拓扑学习易受低质量信号影响、空间邻域信息建模不足的问题。

Method: 提出SPHENIC方法，将不变拓扑特征融入聚类网络，并设计SCDOM模块构建高质量空间嵌入。

Result: 在14个基准空间转录组切片上实验，SPHENIC在空间聚类任务上表现出色，比现有最优方法性能高3.31%-6.54%。

Conclusion: SPHENIC方法在空间转录组聚类任务中具有优越性。

Abstract: By incorporating spatial location information, spatial-transcriptomics
clustering yields more comprehensive insights into cell subpopulation
identification. Despite recent progress, existing methods have at least two
limitations: (i) topological learning typically considers only representations
of individual cells or their interaction graphs; however, spatial
transcriptomic profiles are often noisy, making these approaches vulnerable to
low-quality topological signals, and (ii) insufficient modeling of spatial
neighborhood information leads to low-quality spatial embeddings. To address
these limitations, we propose SPHENIC, a novel Spatial Persistent Homology
Enhanced Neighborhood Integrative Clustering method. Specifically, SPHENIC
incorporates invariant topological features into the clustering network to
achieve stable representation learning. Additionally, to construct high-quality
spatial embeddings that reflect the true cellular distribution, we design the
Spatial Constraint and Distribution Optimization Module (SCDOM). This module
increases the similarity between a cell's embedding and those of its spatial
neighbors, decreases similarity with non-neighboring cells, and thereby
produces clustering-friendly spatial embeddings. Extensive experiments on 14
benchmark spatial transcriptomic slices demonstrate that SPHENIC achieves
superior performance on the spatial clustering task, outperforming existing
state-of-the-art methods by 3.31%-6.54% over the best alternative.

</details>


### [129] [Geospatial Diffusion for Land Cover Imperviousness Change Forecasting](https://arxiv.org/abs/2508.10649)
*Debvrat Varshney,Vibhas Vats,Bhartendu Pandey,Christa Brelsford,Philipe Dias*

Main category: cs.LG

TL;DR: 本文提出用生成式AI进行土地覆盖变化预测的新范式，通过实验验证可行性，模型表现优于基线，还探讨了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 区域地球系统模型在预测水文和大气过程方面能力提升，但土地利用和土地覆盖变化（LULC）预测能力滞后，而LULC是风险和后果评估的关键输入。

Method: 将LULC预测构建为基于历史和辅助数据源的数据合成问题，利用生成式AI，训练扩散模型进行不透水层的十年预测。

Result: 在12个大都市区对训练中留出的一年进行评估，对于平均分辨率≥0.7×0.7平方千米，模型的平均绝对误差（MAE）低于无变化的基线。

Conclusion: 生成式模型能从历史数据中捕捉对预测未来变化有重要意义的时空模式，还需开展结合地球物理属性辅助信息和支持不同情景模拟的未来研究。

Abstract: Land cover, both present and future, has a significant effect on several
important Earth system processes. For example, impervious surfaces heat up and
speed up surface water runoff and reduce groundwater infiltration, with
concomitant effects on regional hydrology and flood risk. While regional Earth
System models have increasing skill at forecasting hydrologic and atmospheric
processes at high resolution in future climate scenarios, our ability to
forecast land-use and land-cover change (LULC), a critical input to risk and
consequences assessment for these scenarios, has lagged behind. In this paper,
we propose a new paradigm exploiting Generative AI (GenAI) for land cover
change forecasting by framing LULC forecasting as a data synthesis problem
conditioned on historical and auxiliary data-sources. We discuss desirable
properties of generative models that fundament our research premise, and
demonstrate the feasibility of our methodology through experiments on
imperviousness forecasting using historical data covering the entire
conterminous United States. Specifically, we train a diffusion model for
decadal forecasting of imperviousness and compare its performance to a baseline
that assumes no change at all. Evaluation across 12 metropolitan areas for a
year held-out during training indicate that for average resolutions $\geq
0.7\times0.7km^2$ our model yields MAE lower than such a baseline. This finding
corroborates that such a generative model can capture spatiotemporal patterns
from historical data that are significant for projecting future change.
Finally, we discuss future research to incorporate auxiliary information on
physical properties about the Earth, as well as supporting simulation of
different scenarios by means of driver variables.

</details>


### [130] [Graph Learning via Logic-Based Weisfeiler-Leman Variants and Tabularization](https://arxiv.org/abs/2508.10651)
*Reijo Jaakkola,Tomi Janhunen,Antti Kuusisto,Magdalena Ortiz,Matias Selin,Mantas Šimkus*

Main category: cs.LG

TL;DR: 提出基于Weisfeiler - Leman算法变体将图数据表格化的图分类新方法，测试并证明其性能。


<details>
  <summary>Details</summary>
Motivation: 寻找有效的图分类方法，探索Weisfeiler - Leman变体的表达能力。

Method: 用Weisfeiler - Leman算法变体将图数据表格化，应用表格数据方法；测试所选变体；探索从图数据集提取可解释模态逻辑公式。

Result: 该方法在准确率上与最先进的图神经网络和图核相当，在时间或内存效率上更优。

Conclusion: 提出的图分类方法有效且有一定优势，还可探索图数据集的可解释性。

Abstract: We present a novel approach for graph classification based on tabularizing
graph data via variants of the Weisfeiler-Leman algorithm and then applying
methods for tabular data. We investigate a comprehensive class of
Weisfeiler-Leman variants obtained by modifying the underlying logical
framework and establish a precise theoretical characterization of their
expressive power. We then test two selected variants on twelve benchmark
datasets that span a range of different domains. The experiments demonstrate
that our approach matches the accuracy of state-of-the-art graph neural
networks and graph kernels while being more time or memory efficient, depending
on the dataset. We also briefly discuss directly extracting interpretable modal
logic formulas from graph datasets.

</details>


### [131] [REFN: A Reinforcement-Learning-From-Network Framework against 1-day/n-day Exploitations](https://arxiv.org/abs/2508.10701)
*Tianlong Yu,Lihong Liu,Ziyi Zhou,Fudu Xing,Kailong Wang,Yang Yang*

Main category: cs.LG

TL;DR: 针对1天或n天漏洞威胁，提出REFN框架训练大语言模型生成网络过滤器，经评估有效、高效且可扩展。


<details>
  <summary>Details</summary>
Motivation: 现有针对1天或n天漏洞的防御方法存在可扩展性、兼容性和部署过程易出错等问题，需新防御方案。

Method: 引入REFN框架，利用强化学习，通过在线网络奖励训练大语言模型，采用Agentic RAG知识蒸馏、RL From VNF管道和在线代理验证等方法。

Result: 在22种1天或n天漏洞利用中，REFN准确率比其他方案高21.1%，平均修复时间3.65小时，可轻松扩展到10K设备。

Conclusion: REFN是训练大语言模型快速防止大规模1天或n天漏洞利用的初步尝试。

Abstract: The exploitation of 1 day or n day vulnerabilities poses severe threats to
networked devices due to massive deployment scales and delayed patching
(average Mean Time To Patch exceeds 60 days). Existing defenses, including host
based patching and network based filtering, are inadequate due to limited
scalability across diverse devices, compatibility issues especially with
embedded or legacy systems, and error prone deployment process (manual patch
validation). To address these issues, we introduce REFN (Reinforcement Learning
From Network), a novel framework that trains Large Language Models (LLMs) to
autonomously generate network filters to prevent 1 day or n day exploitations.
REFN ensures scalability by uniquely employs Reinforcement Learning (RL) driven
by online network rewards instead of traditional Human Feedback (RLHF). REFN
guarantees compatibility via unified deployment on edge security gateways
(Amazon Eero). REFN provides robustness via online validation using real
network traffic. Crucially, REFN addresses three core challenges in training
LLMs for exploit prevention: 1) expanding current LLMs limited vulnerability
fixing expertise via Agentic RAG based Knowledge Distillation, 2) bridging
current LLMs language to network gaps through an RL From VNF Pipeline that
translates language context (vulnerability description) into network
enforcement, 3) addressing the LLM hallucination and non determinism via the
Online Agentic Validation that penalizes erroneous outputs. Evaluated across 22
families of 1 day or n day exploits, REFN demonstrates effectiveness (21.1
percent higher accuracy than alternatives), efficiency (Mean Time To Patch of
3.65 hours) and scalability (easily scale to 10K devices). REFN serves as an
initial step toward training LLMs to rapidly prevent massive scale 1 day or n
day exploitations.

</details>


### [132] [Electromagnetic Simulations of Antennas on GPUs for Machine Learning Applications](https://arxiv.org/abs/2508.10713)
*Murat Temiz,Vemund Bakken*

Main category: cs.LG

TL;DR: 提出基于GPU和开源电磁仿真软件的天线仿真框架用于天线设计和优化的机器学习应用，比较与商业软件结果，对比多种模型性能，显示GPU性能远超CPU，开源软件结果与商业软件相近。


<details>
  <summary>Details</summary>
Motivation: 机器学习在天线设计优化中数据需求大，但电磁仿真计算复杂，难在短时间生成足够训练样本。

Method: 利用GPU模拟大量预设或随机形状参数的天线生成数据集，比较不同机器学习和深度学习模型天线参数估计性能。

Result: 入门级GPU计算性能远超高端CPU，高端游戏GPU比高端CPU快约18倍；仿真空间分辨率足够时，开源软件与商业软件仿真微带天线结果相似。

Conclusion: 基于GPU和开源软件的天线仿真框架可行，GPU能有效提升天线仿真效率，开源软件可替代商业软件。

Abstract: This study proposes an antenna simulation framework powered by graphics
processing units (GPUs) based on an open-source electromagnetic (EM) simulation
software (gprMax) for machine learning applications of antenna design and
optimization. Furthermore, it compares the simulation results with those
obtained through commercial EM software. The proposed software framework for
machine learning and surrogate model applications will produce antenna data
sets consisting of a large number of antenna simulation results using GPUs.
Although machine learning methods can attain the optimum solutions for many
problems, they are known to be data-hungry and require a great deal of samples
for the training stage of the algorithms. However, producing a sufficient
number of training samples in EM applications within a limited time is
challenging due to the high computational complexity of EM simulations.
Therefore, GPUs are utilized in this study to simulate a large number of
antennas with predefined or random antenna shape parameters to produce data
sets. Moreover, this study also compares various machine learning and deep
learning models in terms of antenna parameter estimation performance. This
study demonstrates that an entry-level GPU substantially outperforms a high-end
CPU in terms of computational performance, while a high-end gaming GPU can
achieve around 18 times more computational performance compared to a high-end
CPU. Moreover, it is shown that the open-source EM simulation software can
deliver similar results to those obtained via commercial software in the
simulation of microstrip antennas when the spatial resolution of the
simulations is sufficiently fine.

</details>


### [133] [APFL: Analytic Personalized Federated Learning via Dual-Stream Least Squares](https://arxiv.org/abs/2508.10732)
*Kejia Fan,Jianheng Tang,Zhirui Yang,Feijiang Han,Jiaxu Li,Run He,Yajiang Huang,Anfeng Liu,Houbing Herbert Song,Yunhuai Liu,Huiping Zhuang*

Main category: cs.LG

TL;DR: 提出APFL方法解决PFL中non - IID问题，经实验验证有优越性


<details>
  <summary>Details</summary>
Motivation: 现有PFL方法易受non - IID数据影响，阻碍集体泛化和个性化

Method: 提出APFL方法，用基础模型作冻结骨干提取特征，开发双流分析模型，包含共享主流和细化流

Result: APFL具有异质性不变的理想特性，在多个数据集上准确率比现有基线至少高1.10% - 15.45%

Conclusion: APFL方法在解决PFL的non - IID问题上有优势，优于现有基线

Abstract: Personalized Federated Learning (PFL) has presented a significant challenge
to deliver personalized models to individual clients through collaborative
training. Existing PFL methods are often vulnerable to non-IID data, which
severely hinders collective generalization and then compromises the subsequent
personalization efforts. In this paper, to address this non-IID issue in PFL,
we propose an Analytic Personalized Federated Learning (APFL) approach via
dual-stream least squares. In our APFL, we use a foundation model as a frozen
backbone for feature extraction. Subsequent to the feature extractor, we
develop dual-stream analytic models to achieve both collective generalization
and individual personalization. Specifically, our APFL incorporates a shared
primary stream for global generalization across all clients, and a dedicated
refinement stream for local personalization of each individual client. The
analytical solutions of our APFL enable its ideal property of heterogeneity
invariance, theoretically meaning that each personalized model remains
identical regardless of how heterogeneous the data are distributed across all
other clients. Empirical results across various datasets also validate the
superiority of our APFL over state-of-the-art baselines, with advantages of at
least 1.10%-15.45% in accuracy.

</details>


### [134] [Pass@k Training for Adaptively Balancing Exploration and Exploitation of Large Reasoning Models](https://arxiv.org/abs/2508.10751)
*Zhipeng Chen,Xiaobo Qin,Youbin Wu,Yue Ling,Qinghao Ye,Wayne Xin Zhao,Guang Shi*

Main category: cs.LG

TL;DR: 文章指出强化学习中奖励指标选择的重要性，用Pass@k作为奖励训练策略模型，分析其优势，发现探索与利用可相互促进，并初步探索优势设计。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习中平衡探索与利用的问题，寻找合适的奖励指标。

Method: 用Pass@k作为奖励训练策略模型，推导Pass@k训练的优势解析解，初步探索优势设计。

Result: 发现探索与利用并非冲突，可相互促进，初步探索有有前景的结果。

Conclusion: Pass@k训练可提高探索能力，探索与利用可相互增强，为RLVR优势设计指出潜在方向。

Abstract: Reinforcement learning with verifiable rewards (RLVR), which typically adopts
Pass@1 as the reward, has faced the issues in balancing exploration and
exploitation, causing policies to prefer conservative actions, converging to a
local optimum. Identifying an appropriate reward metric is therefore crucial.
Regarding the prior work, although Pass@k has been used in evaluation, its
connection to LLM exploration ability in RLVR remains largely overlooked. To
investigate this, we first use Pass@k as the reward to train the policy model
(i.e., $\textbf{Pass@k Training}$), and observe the improvement on its
exploration ability. Next, we derive an analytical solution for the advantage
of Pass@k Training, leading to an efficient and effective process. Building on
this, our analysis reveals that exploration and exploitation are not inherently
conflicting objectives, while they can mutually enhance each other. Moreover,
Pass@k Training with analytical derivation essentially involves directly
designing the advantage function. Inspired by this, we preliminarily explore
the advantage design for RLVR, showing promising results and highlighting a
potential future direction.

</details>


### [135] [Natively Trainable Sparse Attention for Hierarchical Point Cloud Datasets](https://arxiv.org/abs/2508.10758)
*Nicolas Lapautre,Maria Marchenko,Carlos Miguel Patiño,Xin Zhou*

Main category: cs.LG

TL;DR: 本文探索结合Erwin架构与NSA机制提升大规模物理系统Transformer模型效率和感受野，在三个物理科学数据集上验证效果。


<details>
  <summary>Details</summary>
Motivation: 克服注意力机制二次缩放问题，释放Transformer在大型物理系统数据集上的潜力。

Method: 将NSA机制适配非序列数据，实现Erwin NSA模型，并在三个物理科学数据集上评估。

Result: 在三个数据集上性能达到或超过原Erwin模型，重现Erwin论文实验结果验证实现。

Conclusion: 结合Erwin架构与NSA机制可有效提升大规模物理系统Transformer模型性能。

Abstract: Unlocking the potential of transformers on datasets of large physical systems
depends on overcoming the quadratic scaling of the attention mechanism. This
work explores combining the Erwin architecture with the Native Sparse Attention
(NSA) mechanism to improve the efficiency and receptive field of transformer
models for large-scale physical systems, addressing the challenge of quadratic
attention complexity. We adapt the NSA mechanism for non-sequential data,
implement the Erwin NSA model, and evaluate it on three datasets from the
physical sciences -- cosmology simulations, molecular dynamics, and air
pressure modeling -- achieving performance that matches or exceeds that of the
original Erwin model. Additionally, we reproduce the experimental results from
the Erwin paper to validate their implementation.

</details>


### [136] [IBEX: Information-Bottleneck-EXplored Coarse-to-Fine Molecular Generation under Limited Data](https://arxiv.org/abs/2508.10775)
*Dong Xu,Zhangfan Yang,Jenna Xinyi Yao,Shuangbao Song,Zexuan Zhu,Junkai Ji*

Main category: cs.LG

TL;DR: 提出IBEX粗到细的流程解决基于结构药物设计中蛋白 - 配体复合物数据不足问题，改进后性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有三维生成模型受公开蛋白 - 配体复合物数据稀缺限制，难以学习可迁移几何先验且易过拟合。

Method: 用PAC - Bayesian信息瓶颈理论量化样本信息密度，保留TargetDiff架构和超参数生成分子，再用L - BFGS优化步骤微调构象。

Result: 零样本对接成功率从53%提升到64%，平均Vina分数提高，QED提高25%，达到先进的有效性和多样性，减少外推误差。

Conclusion: IBEX有效解决了基于结构药物设计中数据稀缺问题，提升了模型性能。

Abstract: Three-dimensional generative models increasingly drive structure-based drug
discovery, yet it remains constrained by the scarce publicly available
protein-ligand complexes. Under such data scarcity, almost all existing
pipelines struggle to learn transferable geometric priors and consequently
overfit to training-set biases. As such, we present IBEX, an
Information-Bottleneck-EXplored coarse-to-fine pipeline to tackle the chronic
shortage of protein-ligand complex data in structure-based drug design.
Specifically, we use PAC-Bayesian information-bottleneck theory to quantify the
information density of each sample. This analysis reveals how different masking
strategies affect generalization and indicates that, compared with conventional
de novo generation, the constrained Scaffold Hopping task endows the model with
greater effective capacity and improved transfer performance. IBEX retains the
original TargetDiff architecture and hyperparameters for training to generate
molecules compatible with the binding pocket; it then applies an L-BFGS
optimization step to finely refine each conformation by optimizing five
physics-based terms and adjusting six translational and rotational degrees of
freedom in under one second. With only these modifications, IBEX raises the
zero-shot docking success rate on CBGBench CrossDocked2020-based from 53% to
64%, improves the mean Vina score from $-7.41 kcal mol^{-1}$ to $-8.07 kcal
mol^{-1}$, and achieves the best median Vina energy in 57 of 100 pockets versus
3 for the original TargetDiff. IBEX also increases the QED by 25%, achieves
state-of-the-art validity and diversity, and markedly reduces extrapolation
error.

</details>


### [137] [Non-Stationary Restless Multi-Armed Bandits with Provable Guarantee](https://arxiv.org/abs/2508.10804)
*Yu-Heng Hung,Ping-Chun Hsieh,Kai Wang*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Online restless multi-armed bandits (RMABs) typically assume that each arm
follows a stationary Markov Decision Process (MDP) with fixed state transitions
and rewards. However, in real-world applications like healthcare and
recommendation systems, these assumptions often break due to non-stationary
dynamics, posing significant challenges for traditional RMAB algorithms. In
this work, we specifically consider $N$-armd RMAB with non-stationary
transition constrained by bounded variation budgets $B$. Our proposed \rmab\;
algorithm integrates sliding window reinforcement learning (RL) with an upper
confidence bound (UCB) mechanism to simultaneously learn transition dynamics
and their variations. We further establish that \rmab\; achieves
$\widetilde{\mathcal{O}}(N^2 B^{\frac{1}{4}} T^{\frac{3}{4}})$ regret bound by
leveraging a relaxed definition of regret, providing a foundational theoretical
framework for non-stationary RMAB problems for the first time.

</details>


### [138] [Memory-Augmented Transformers: A Systematic Review from Neuroscience Principles to Technical Solutions](https://arxiv.org/abs/2508.10824)
*Parsa Omidi,Xingshuai Huang,Axel Laborieux,Bahareh Nikpour,Tianyu Shi,Armaghan Eshaghi*

Main category: cs.LG

TL;DR: 文章探讨Transformer架构在记忆方面的局限，提出统一框架连接神经科学原理与记忆增强Transformer工程进展，分析记忆操作，指出挑战与解决方案，为终身学习Transformer架构提供路线图。


<details>
  <summary>Details</summary>
Motivation: Transformer架构在长程上下文保留、持续学习和知识整合方面存在局限性，需要改进。

Method: 提出统一框架，从功能目标、记忆表示和集成机制三个分类维度组织进展，分析核心记忆操作。

Result: 发现从静态缓存向自适应、测试时学习系统转变，识别出可扩展性和干扰等挑战及分层缓冲和惊喜门控更新等解决方案。

Conclusion: 为受认知启发的终身学习Transformer架构提供路线图。

Abstract: Memory is fundamental to intelligence, enabling learning, reasoning, and
adaptability across biological and artificial systems. While Transformer
architectures excel at sequence modeling, they face critical limitations in
long-range context retention, continual learning, and knowledge integration.
This review presents a unified framework bridging neuroscience principles,
including dynamic multi-timescale memory, selective attention, and
consolidation, with engineering advances in Memory-Augmented Transformers. We
organize recent progress through three taxonomic dimensions: functional
objectives (context extension, reasoning, knowledge integration, adaptation),
memory representations (parameter-encoded, state-based, explicit, hybrid), and
integration mechanisms (attention fusion, gated control, associative
retrieval). Our analysis of core memory operations (reading, writing,
forgetting, and capacity management) reveals a shift from static caches toward
adaptive, test-time learning systems. We identify persistent challenges in
scalability and interference, alongside emerging solutions including
hierarchical buffering and surprise-gated updates. This synthesis provides a
roadmap toward cognitively-inspired, lifelong-learning Transformer
architectures.

</details>


### [139] [SoK: Data Minimization in Machine Learning](https://arxiv.org/abs/2508.10836)
*Robin Staab,Nikola Jovanović,Kimberly Mai,Prakhar Ganesh,Martin Vechev,Ferdinando Fioretto,Matthew Jagielski*

Main category: cs.LG

TL;DR: 文章针对机器学习中数据最小化（DMML）研究与其他工作的脱节问题，提出综合框架，助力从业者应用DM原则。


<details>
  <summary>Details</summary>
Motivation: 数据最小化原则违反会导致严重后果，机器学习中该领域新兴，但现有工作存在脱节，导致从业者困惑。

Method: 引入一个包含统一数据管道、对手和最小化点的综合框架，并系统回顾数据最小化及相关方法的文献。

Result: 首次呈现结构化概述。

Conclusion: 工作有助于以DM为中心的统一理解，并推动数据最小化策略在AI/ML中的广泛应用。

Abstract: Data minimization (DM) describes the principle of collecting only the data
strictly necessary for a given task. It is a foundational principle across
major data protection regulations like GDPR and CPRA. Violations of this
principle have substantial real-world consequences, with regulatory actions
resulting in fines reaching hundreds of millions of dollars. Notably, the
relevance of data minimization is particularly pronounced in machine learning
(ML) applications, which typically rely on large datasets, resulting in an
emerging research area known as Data Minimization in Machine Learning (DMML).
At the same time, existing work on other ML privacy and security topics often
addresses concerns relevant to DMML without explicitly acknowledging the
connection. This disconnect leads to confusion among practitioners,
complicating their efforts to implement DM principles and interpret the
terminology, metrics, and evaluation criteria used across different research
communities. To address this gap, our work introduces a comprehensive framework
for DMML, including a unified data pipeline, adversaries, and points of
minimization. This framework allows us to systematically review the literature
on data minimization and \emph{DM-adjacent} methodologies, for the first time
presenting a structured overview designed to help practitioners and researchers
effectively apply DM principles. Our work facilitates a unified DM-centric
understanding and broader adoption of data minimization strategies in AI/ML.

</details>


### [140] [Efficiently Verifiable Proofs of Data Attribution](https://arxiv.org/abs/2508.10866)
*Ari Karchmer,Seth Neel,Martin Pawelczyk*

Main category: cs.LG

TL;DR: 提出数据归因交互式验证范式解决信任问题，给出有完备性、可靠性和效率保证的协议。


<details>
  <summary>Details</summary>
Motivation: 现有数据归因模型估计计算成本高，资源受限方难以信任计算能力强的一方提供的归因结果。

Method: 提出交互式验证范式，让计算能力强的证明者学习数据归因并与资源受限的验证者进行交互式证明。

Result: 得到一个协议，在PAC验证意义下有完备性、可靠性和效率保证，验证者工作量与数据集大小无关。

Conclusion: 该协议可有效验证证明者计算的布尔超立方体上的线性函数，适用于各种归因任务。

Abstract: Data attribution methods aim to answer useful counterfactual questions like
"what would a ML model's prediction be if it were trained on a different
dataset?" However, estimation of data attribution models through techniques
like empirical influence or "datamodeling" remains very computationally
expensive. This causes a critical trust issue: if only a few computationally
rich parties can obtain data attributions, how can resource-constrained parties
trust that the provided attributions are indeed "good," especially when they
are used for important downstream applications (e.g., data pricing)? In this
paper, we address this trust issue by proposing an interactive verification
paradigm for data attribution. An untrusted and computationally powerful Prover
learns data attributions, and then engages in an interactive proof with a
resource-constrained Verifier. Our main result is a protocol that provides
formal completeness, soundness, and efficiency guarantees in the sense of
Probably-Approximately-Correct (PAC) verification. Specifically, if both Prover
and Verifier follow the protocol, the Verifier accepts data attributions that
are {\epsilon}-close to the optimal data attributions (in terms of the Mean
Squared Error) with probability 1-{\delta}. Conversely, if the Prover
arbitrarily deviates from the protocol, even with infinite compute, then this
is detected (or it still yields data attributions to the Verifier) except with
probability {\delta}. Importantly, our protocol ensures the Verifier's
workload, measured by the number of independent model retrainings it must
perform, scales only as O(1/{\epsilon}); i.e., independently of the dataset
size. At a technical level, our results apply to efficiently verifying any
linear function over the boolean hypercube computed by the Prover, making them
broadly applicable to various attribution tasks.

</details>


### [141] [A Dataset for Distilling Knowledge Priors from Literature for Therapeutic Design](https://arxiv.org/abs/2508.10899)
*Haydn Thomas Jones,Natalie Maus,Josh Magnus Ludan,Maggie Ziyu Huan,Jiaming Liang,Marcelo Der Torossian Torres,Jiatao Liang,Zachary Ives,Yoseph Barash,Cesar de la Fuente-Nunez,Jacob R. Gardner,Mark Yatskar*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: AI-driven discovery can greatly reduce design time and enhance new
therapeutics' effectiveness. Models using simulators explore broad design
spaces but risk violating implicit constraints due to a lack of experimental
priors. For example, in a new analysis we performed on a diverse set of models
on the GuacaMol benchmark using supervised classifiers, over 60\% of molecules
proposed had high probability of being mutagenic. In this work, we introduce
\ourdataset, a dataset of priors for design problems extracted from literature
describing compounds used in lab settings. It is constructed with LLM pipelines
for discovering therapeutic entities in relevant paragraphs and summarizing
information in concise fair-use facts. \ourdataset~ consists of 32.3 million
pairs of natural language facts, and appropriate entity representations (i.e.
SMILES or refseq IDs). To demonstrate the potential of the data, we train LLM,
CLIP, and LLava architectures to reason jointly about text and design targets
and evaluate on tasks from the Therapeutic Data Commons (TDC). \ourdataset~is
highly effective for creating models with strong priors: in supervised
prediction problems that use our data as pretraining, our best models with 15M
learnable parameters outperform larger 2B TxGemma on both regression and
classification TDC tasks, and perform comparably to 9B models on average.
Models built with \ourdataset~can be used as constraints while optimizing for
novel molecules in GuacaMol, resulting in proposals that are safer and nearly
as effective. We release our dataset at
\href{https://huggingface.co/datasets/medexanon/Medex}{huggingface.co/datasets/medexanon/Medex},
and will provide expanded versions as available literature grows.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [142] [Empirical Investigation into Configuring Echo State Networks for Representative Benchmark Problem Domains](https://arxiv.org/abs/2508.10887)
*Brooke R. Weborg,Gursel Serpen*

Main category: cs.NE

TL;DR: 本文用四个基准问题测试回声状态网络性能，提出配置架构、选择参数及取值的经验法则，通过系列基准任务实验展示对网络性能的影响。


<details>
  <summary>Details</summary>
Motivation: 解决进入回声状态网络研究领域缺乏经验的问题，理解参数选择和架构变化对网络性能的影响。

Method: 使用四个不同基准问题对回声状态网络进行建模和实验，这些问题涵盖时间序列预测、模式生成、混沌系统预测和时间序列分类。

Result: 文中未明确提及具体结果。

Conclusion: 有必要理解参数及其取值对回声状态网络架构性能的影响，以成功构建网络。

Abstract: This paper examines Echo State Network, a reservoir computer, performance
using four different benchmark problems, then proposes heuristics or rules of
thumb for configuring the architecture, as well as the selection of parameters
and their values, which are applicable to problems within the same domain, to
help serve to fill the experience gap needed by those entering this field of
study. The influence of various parameter selections and their value
adjustments, as well as architectural changes made to an Echo State Network, a
powerful recurrent neural network configured as a reservoir computer, can be
challenging to fully comprehend without experience in the field, and even some
hyperparameter optimization algorithms may have difficulty adjusting parameter
values without proper manual selections made first. Therefore, it is imperative
to understand the effects of parameters and their value selection on Echo State
Network architecture performance for a successful build. Thus, to address the
requirement for an extensive background in Echo State Network architecture, as
well as examine how Echo State Network performance is affected with respect to
variations in architecture, design, and parameter selection and values, a
series of benchmark tasks representing different problem domains, including
time series prediction, pattern generation, chaotic system prediction, and time
series classification, were modeled and experimented on to show the impact on
the performance of Echo State Network.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [143] [Meta-Metrics and Best Practices for System-Level Inference Performance Benchmarking](https://arxiv.org/abs/2508.10251)
*Shweta Salaria,Zhuoran Liu,Nelson Mimura Gonzalez*

Main category: cs.PF

TL;DR: 介绍FMwork方法用于基础模型推理性能基准测试，能提升实验效率与性能。


<details>
  <summary>Details</summary>
Motivation: 评估基础模型推理性能时，测试所有配置不实际，需有效方法创建可控测试环境。

Method: 引入FMwork，含元指标、参数选择和成本 - 性能评估三个关键组件，将元指标操作化并提供参数选择和成本 - 性能分析策略。

Result: 使用该框架实验有最高24倍改进，减少实验输出大小有2.7倍增益，Llama 3.1 8B模型评估保持96.6%准确率。

Conclusion: FMwork是一种有效全面的基础模型基准测试方法，可提高实验效率和性能。

Abstract: Benchmarking inference performance (speed) of Foundation Models such as Large
Language Models (LLM) involves navigating a vast experimental landscape to
understand the complex interactions between hardware and software components.
However, evaluating every possible test configuration is impractical,
unfeasible and unnecessary. To address this challenge, we introduce FMwork, a
comprehensive and methodical approach to creating a controlled testing
environment that accurately reflects and characterizes performance. FMwork
comprises a set of benchmkaring best practices with three key components: 1)
meta-metrics, 2) parameter selection, and 3) strategic cost-performance
evaluation. Meta-metrics account for time and resources spent on benchmarking
and the relative accuracy of the results compared to a larger body of
measurements, representing the complete experimental space. FMwork
operationalizes the meta-metrics and provides efficient strategies for
parameter selection and cost-performance analysis. Using the framework, we show
up to 24x improvement (speedup and/or resource savings) running sweeps of
experiments compared to the ground truth. Even already considering a subset of
experiments as reference point (using the power of two for batch sizes),
reducing experimental output size from 1024 to 128 tokens yields another 2.7x
gain while keeping 96.6% accuracy for an evaluation using Llama 3.1 8B model.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [144] [FormalGrad: Integrating Formal Methods with Gradient-Based LLM Refinement](https://arxiv.org/abs/2508.10059)
*Yueke Zhang,Yifan Zhang,Kevin Leach,Yu Huang*

Main category: cs.SE

TL;DR: FormalGrad集成形式化方法到基于大语言模型的迭代生成循环，处理代码为可微变量，在多个基准测试中表现出色，推动可靠的AI辅助软件开发。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在代码生成中缺乏正确性、鲁棒性和效率保证，在严格约束领域问题突出。

Method: 将形式化方法集成到基于大语言模型的迭代生成循环，把代码视为可微变量，将结构化反馈和形式约束转化为文本伪梯度来引导模型迭代优化。

Result: 在HumanEval、HumanEval+和LiveCodeBench基准测试中，实现优于强基线，HumanEval绝对提升达27%，LiveCodeBench V6相对提升41%。

Conclusion: FormalGrad能生成形式上合理、鲁棒且高效的代码，为高风险应用的可靠AI辅助软件开发铺平道路。

Abstract: While Large Language Models (LLMs) have demonstrated remarkable capabilities
in code generation, they often produce solutions that lack guarantees of
correctness, robustness, and efficiency. The limitation is acute in domains
requiring strict constraints. FormalGrad introduces a principled framework that
integrates formal methods directly into an iterative LLM-based generation loop.
It uniquely treats code as a differentiable variable, converting structured
feedback and formal constraints into a textual pseudo-gradient. This gradient
guides the model to iteratively refine solutions, ensuring they are not only
functional but also robust and formally justified. We evaluate FormalGrad on
the HumanEval, HumanEval+, and LiveCodeBench benchmarks. Our implementation
outperforms strong baselines, achieving an absolute improvement of up to 27% on
HumanEval and a 41% relative improvement on the challenging LiveCodeBench V6.
FormalGrad generates formally justified code that is robust and efficient,
paving the way for reliable AI-assisted software development in high-stakes
applications.

</details>


### [145] [SaraCoder: Orchestrating Semantic and Structural Cues for Profit-Oriented Repository-Level Code Completion](https://arxiv.org/abs/2508.10068)
*Xiaohan Chen,Zhongying Pan,Quan Feng,Yu Tian,Shuqun Yang,Mengru Wang,Lina Gong,Yuxia Geng,Piji Li,Xiang Chen*

Main category: cs.SE

TL;DR: 提出Saracoder框架解决代码补全检索问题，实验表明其优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有基于检索增强生成的代码补全方法存在语义误导、冗余、同质化及外部符号歧义问题。

Method: 引入Saracoder框架，包含分层特征优化模块和外部感知标识符消歧模块。

Result: 在CrossCodeEval和RepoEval - Updated基准测试中，Saracoder在多语言和模型上显著优于现有基线。

Conclusion: 多维度系统优化检索结果为构建更准确、鲁棒的代码补全系统提供新范式。

Abstract: Retrieval-augmented generation (RAG) for repository-level code completion
commonly relies on superficial text similarity, leading to results plagued by
semantic misguidance, redundancy, and homogeneity, while also failing to
resolve external symbol ambiguity. To address these challenges, we introduce
Saracoder, a Hierarchical Feature-Optimized retrieval framework. Its core
Hierarchical Feature Optimization module systematically refines candidates by
distilling deep semantic relationships, pruning exact duplicates, assessing
structural similarity with a novel graph-based metric that weighs edits by
their topological importance, and reranking results to maximize both relevance
and diversity. Furthermore, an External-Aware Identifier Disambiguator module
accurately resolves cross-file symbol ambiguity via dependency analysis.
Extensive experiments on the challenging CrossCodeEval and RepoEval-Updated
benchmarks demonstrate that Saracoder significantly outperforms existing
baselines across multiple programming languages and models. Our work proves
that systematically refining retrieval results across multiple dimensions
provides a new paradigm for building more accurate and robust repository-level
code completion systems.

</details>


### [146] [Next Edit Prediction: Learning to Predict Code Edits from Context and Interaction History](https://arxiv.org/abs/2508.10074)
*Ruofan Lu,Yintong Huo,Meng Zhang,Yichen Li,Michael R. Lyu*

Main category: cs.SE

TL;DR: 介绍Next Edit Prediction任务，创建数据集和基准，对模型微调评估，为新交互范式奠基。


<details>
  <summary>Details</summary>
Motivation: 现有AI编码助手的低延迟代码补全和聊天式编辑用户体验不佳，无法主动预测开发者下一步编辑。

Method: 提出Next Edit Prediction任务，创建高质量监督微调数据集和评估基准，对一系列模型进行监督微调并评估。

Result: 获得了一些新发现。

Conclusion: 为能主动预测开发者下一步行动的新交互范式奠定基础。

Abstract: The rapid advancement of large language models (LLMs) has led to the
widespread adoption of AI-powered coding assistants integrated into a
development environment. On one hand, low-latency code completion offers
completion suggestions but is fundamentally constrained to the cursor's current
position. On the other hand, chat-based editing can perform complex
modifications, yet forces developers to stop their work, describe the intent in
natural language, which causes a context-switch away from the code. This
creates a suboptimal user experience, as neither paradigm proactively predicts
the developer's next edit in a sequence of related edits. To bridge this gap
and provide the seamless code edit suggestion, we introduce the task of Next
Edit Prediction, a novel task designed to infer developer intent from recent
interaction history to predict both the location and content of the subsequent
edit. Specifically, we curate a high-quality supervised fine-tuning dataset and
an evaluation benchmark for the Next Edit Prediction task. Then, we conduct
supervised fine-tuning on a series of models and performed a comprehensive
evaluation of both the fine-tuned models and other baseline models, yielding
several novel findings. This work lays the foundation for a new interaction
paradigm that proactively collaborate with developers by anticipating their
following action, rather than merely reacting to explicit instructions.

</details>


### [147] [On the synchronization between Hugging Face pre-trained language models and their upstream GitHub repository](https://arxiv.org/abs/2508.10157)
*Ajibode Adekunle,Abdul Ali Bangash,Bram Adams,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: 研究预训练语言模型（PTLMs）在GitHub和Hugging Face平台间开发协调问题，分析提交活动同步模式，指出当前跨平台发布实践的结构脱节及风险。


<details>
  <summary>Details</summary>
Motivation: 解决PTLMs在GitHub和Hugging Face平台间开发协调的挑战，如发布时间不一致、版本管理不统一和变体重用有限等问题。

Method: 对325个PTLM家族（904个HF变体）进行混合方法研究，分析提交活动的协调情况，从滞后、同步类型和强度三个维度研究同步模式。

Result: 发现GitHub和HF贡献者更改内容不同，得出八种不同同步模式，部分同步模式普遍存在，导致孤立更改和仓库废弃。

Conclusion: 识别这些同步模式对改善PTLM发布工作流程的监督和可追溯性至关重要。

Abstract: Pretrained language models (PTLMs) have advanced natural language processing
(NLP), enabling progress in tasks like text generation and translation. Like
software package management, PTLMs are trained using code and environment
scripts in upstream repositories (e.g., GitHub, GH) and distributed as variants
via downstream platforms like Hugging Face (HF). Coordinating development
between GH and HF poses challenges such as misaligned release timelines,
inconsistent versioning, and limited reuse of PTLM variants. We conducted a
mixed-method study of 325 PTLM families (904 HF variants) to examine how commit
activities are coordinated. Our analysis reveals that GH contributors typically
make changes related to specifying the version of the model, improving code
quality, performance optimization, and dependency management within the
training scripts, while HF contributors make changes related to improving model
descriptions, data set handling, and setup required for model inference.
Furthermore, to understand the synchronization aspects of commit activities
between GH and HF, we examined three dimensions of these activities -- lag
(delay), type of synchronization, and intensity -- which together yielded eight
distinct synchronization patterns. The prevalence of partially synchronized
patterns, such as Disperse synchronization and Sparse synchronization, reveals
structural disconnects in current cross-platform release practices. These
patterns often result in isolated changes -- where improvements or fixes made
on one platform are never replicated on the other -- and in some cases,
indicate an abandonment of one repository in favor of the other. Such
fragmentation risks exposing end users to incomplete, outdated, or behaviorally
inconsistent models. Hence, recognizing these synchronization patterns is
critical for improving oversight and traceability in PTLM release workflows.

</details>


### [148] [Bridging Solidity Evolution Gaps: An LLM-Enhanced Approach for Smart Contract Compilation Error Resolution](https://arxiv.org/abs/2508.10517)
*Likai Ye,Mengliang Li,Dehai Zhao,Jiamou Sun,Xiaoxue Ren*

Main category: cs.SE

TL;DR: 研究Solidity版本演进挑战，评估LLMs修复编译错误效果，提出SMCFIXER框架，实验显示其比GPT - 4o有显著提升。


<details>
  <summary>Details</summary>
Motivation: Solidity版本频繁更新带来编译错误、代码迁移和维护挑战，需研究应对方法。

Method: 先进行实证研究揭示挑战，评估LLMs修复能力，再提出SMCFIXER框架，包含上下文代码切片、专家知识检索和迭代补丁生成三个核心阶段。

Result: 81.68%合约跨版本编译遇错误，LLMs修复语义问题效果差，SMCFIXER比GPT - 4o提升24.24%，准确率达96.97%。

Conclusion: 开发基于LLM的智能合约修复系统需领域特定适配，SMCFIXER在解决Solidity编译错误上效果良好。

Abstract: Solidity, the dominant smart contract language for Ethereum, has rapidly
evolved with frequent version updates to enhance security, functionality, and
developer experience. However, these continual changes introduce significant
challenges, particularly in compilation errors, code migration, and
maintenance. Therefore, we conduct an empirical study to investigate the
challenges in the Solidity version evolution and reveal that 81.68% of examined
contracts encounter errors when compiled across different versions, with 86.92%
of compilation errors.
  To mitigate these challenges, we conducted a systematic evaluation of large
language models (LLMs) for resolving Solidity compilation errors during version
migrations. Our empirical analysis across both open-source (LLaMA3, DeepSeek)
and closed-source (GPT-4o, GPT-3.5-turbo) LLMs reveals that although these
models exhibit error repair capabilities, their effectiveness diminishes
significantly for semantic-level issues and shows strong dependency on prompt
engineering strategies. This underscores the critical need for domain-specific
adaptation in developing reliable LLM-based repair systems for smart contracts.
  Building upon these insights, we introduce SMCFIXER, a novel framework that
systematically integrates expert knowledge retrieval with LLM-based repair
mechanisms for Solidity compilation error resolution. The architecture
comprises three core phases: (1) context-aware code slicing that extracts
relevant error information; (2) expert knowledge retrieval from official
documentation; and (3) iterative patch generation for Solidity migration.
Experimental validation across Solidity version migrations demonstrates our
approach's statistically significant 24.24% improvement over baseline GPT-4o on
real-world datasets, achieving near-perfect 96.97% accuracy.

</details>


### [149] [EVOSCAT: Exploring Software Change Dynamics in Large-Scale Historical Datasets](https://arxiv.org/abs/2508.10852)
*Souhaila Serbout,Diana Carolina Muñoz Hurtado,Hassan Atwi,Edoardo Riggio,Cesare Pautasso*

Main category: cs.SE

TL;DR: 提出工具EvoScat，用交互式密度散点图解决时间可扩展性问题，支持灵活配置以分析大量软件工件历史数据。


<details>
  <summary>Details</summary>
Motivation: 软件演化研究需处理包含大量事件的数据集，现有方法可能在处理大规模历史数据时面临时间可扩展性问题，需要工具辅助分析。

Method: 提出EvoScat工具，利用交互式密度散点图进行可视化，支持历史缩放、对齐、工件排序和交互式颜色映射等灵活配置。

Result: 该工具可根据特定分析需求进行定制，能分析从数万个软件工件历史中挖掘的数百万个事件，文中展示了多仓库特定工件数据集和特定开源项目历史分析。

Conclusion: EvoScat工具能为研究人员提供可扩展的可视化手段，帮助探索和表征演化数据集，比较单个工件历史。

Abstract: Long lived software projects encompass a large number of artifacts, which
undergo many revisions throughout their history. Empirical software engineering
researchers studying software evolution gather and collect datasets with
millions of events, representing changes introduced to specific artifacts. In
this paper, we propose EvoScat, a tool that attempts addressing temporal
scalability through the usage of interactive density scatterplot to provide a
global overview of large historical datasets mined from open source
repositories in a single visualization. EvoScat intents to provide researchers
with a mean to produce scalable visualizations that can help them explore and
characterize evolution datasets, as well as comparing the histories of
individual artifacts, both in terms of 1) observing how rapidly different
artifacts age over multiple-year-long time spans 2) how often metrics
associated with each artifacts tend towards an improvement or worsening. The
paper shows how the tool can be tailored to specific analysis needs (pace of
change comparison, clone detection, freshness assessment) thanks to its support
for flexible configuration of history scaling and alignment along the time
axis, artifacts sorting and interactive color mapping, enabling the analysis of
millions of events obtained by mining the histories of tens of thousands of
software artifacts. We include in this paper a gallery showcasing datasets
gathering specific artifacts (OpenAPI descriptions, GitHub workflow
definitions) across multiple repositories, as well as diving into the history
of specific popular open source projects.

</details>


<div id='q-fin.PM'></div>

# q-fin.PM [[Back]](#toc)

### [150] [Optimal Capital Deployment Under Stochastic Deal Arrivals: A Continuous-Time ADP Approach](https://arxiv.org/abs/2508.10300)
*Kunal Menda,Raphael S Benarrosh*

Main category: q-fin.PM

TL;DR: 文章将随机交易机会下的资本配置问题建模为CTMDP，用ADP方法求解，提出接受策略并证明其优于基线策略。


<details>
  <summary>Details</summary>
Motivation: 解决基金经理在随机交易机会下，当前机会与未来不确定机会之间资本配置的决策难题。

Method: 将问题建模为CTMDP，用ADP方法求解，用相关对数正态分布模拟交易经济，用NHPP模拟交易到达，用QMC采样近似连续时间贝尔曼方程。

Result: 提出可解释的接受策略，模拟显示该策略优于接受任何超过固定门槛率的可承受交易的基线策略。

Conclusion: 所提出的接受策略在资本配置决策中更优。

Abstract: Suppose you are a fund manager with \$100 million to deploy and two years to
invest it. A deal comes across your desk that looks appealing but costs \$50
million -- half of your available capital. Should you take it, or wait for
something better? The decision hinges on the trade-off between current
opportunities and uncertain future arrivals. This work formulates the problem
of capital deployment under stochastic deal arrivals as a continuous-time
Markov decision process (CTMDP) and solves it numerically via an approximate
dynamic programming (ADP) approach. We model deal economics using correlated
lognormal distributions for multiples on invested capital (MOIC) and deal
sizes, and model arrivals as a nonhomogeneous Poisson process (NHPP). Our
approach uses quasi-Monte Carlo (QMC) sampling to efficiently approximate the
continuous-time Bellman equation for the value function over a discretized
capital grid. We present an interpretable acceptance policy, illustrating how
selectivity evolves over time and as capital is consumed. We show in simulation
that this policy outperforms a baseline that accepts any affordable deal
exceeding a fixed hurdle rate.

</details>


### [151] [Estimating Covariance for Global Minimum Variance Portfolio: A Decision-Focused Learning Approach](https://arxiv.org/abs/2508.10776)
*Juchan Kim,Inwoo Tae,Yongjae Lee*

Main category: q-fin.PM

TL;DR: 采用决策聚焦学习（DFL）推导全局最小方差投资组合（GMVP），实证表明DFL方法决策表现更优，并分析其构建GMVP的机制。


<details>
  <summary>Details</summary>
Motivation: 传统统计估计器和机器学习算法通过最小化均方误差确定投资组合参数会导致次优投资决策，需要更好方法。

Method: 采用决策聚焦学习（DFL），理论推导决策损失梯度，结合GMVP解析解及其主成分性质。

Result: 预测聚焦估计方法在实践中难以产生最优分配，DFL方法决策表现更优。

Conclusion: DFL方法在GMVP构建中有更好决策性能，还分析了其降低波动性、决策驱动特征和估计特性等机制。

Abstract: Portfolio optimization constitutes a cornerstone of risk management by
quantifying the risk-return trade-off. Since it inherently depends on accurate
parameter estimation under conditions of future uncertainty, the selection of
appropriate input parameters is critical for effective portfolio construction.
However, most conventional statistical estimators and machine learning
algorithms determine these parameters by minimizing mean-squared error (MSE), a
criterion that can yield suboptimal investment decisions. In this paper, we
adopt decision-focused learning (DFL) - an approach that directly optimizes
decision quality rather than prediction error such as MSE - to derive the
global minimum-variance portfolio (GMVP). Specifically, we theoretically derive
the gradient of decision loss using the analytic solution of GMVP and its
properties regarding the principal components of itself. Through extensive
empirical evaluation, we show that prediction-focused estimation methods may
fail to produce optimal allocations in practice, whereas DFL-based methods
consistently deliver superior decision performance. Furthermore, we provide a
comprehensive analysis of DFL's mechanism in GMVP construction, focusing on its
volatility reduction capability, decision-driving features, and estimation
characteristics.

</details>


<div id='q-fin.RM'></div>

# q-fin.RM [[Back]](#toc)

### [152] [On data-driven robust distortion risk measures for non-negative risks with partial information](https://arxiv.org/abs/2508.10682)
*Xiangyu Han,Yijun Hu,Ran Wang,Linxiao Wei*

Main category: q-fin.RM

TL;DR: 本文提出两种分布不确定集，探索失真风险度量对分布不确定性的鲁棒性，推导最大化失真风险度量的分布函数闭式解，研究一般情况并做数值研究。


<details>
  <summary>Details</summary>
Motivation: 探索失真风险度量对分布不确定性的鲁棒性。

Method: 提出两种分布不确定集，一种由广义Wasserstein距离确定的球刻画，另一种增加已知矩约束；在一定假设下推导闭式解，研究一般情况并与现有研究比较，最后做数值研究。

Result: 得到在分布不确定集上最大化给定失真风险度量的分布函数的闭式解。

Conclusion: 工作是对文献中已知成果的新颖推广。

Abstract: In this paper, by proposing two new kinds of distributional uncertainty sets,
we explore robustness of distortion risk measures against distributional
uncertainty. To be precise, we first consider a distributional uncertainty set
which is characterized solely by a ball determined by general Wasserstein
distance centered at certain empirical distribution function, and then further
consider additional constraints of known first moment and any other higher
moment of the underlying loss distribution function. Under the assumption that
the distortion function is strictly concave and twice differentiable, and that
the underlying loss random variable is non-negative and bounded, we derive
closed-form expressions for the distribution functions which maximize a given
distortion risk measure over the distributional uncertainty sets respectively.
Moreover, we continue to study the general case of a concave distortion
function and unbounded loss random variables. Comparisons with existing studies
are also made. Finally, we provide a numerical study to illustrate the proposed
models and results. Our work provides a novel generalization of several known
achievements in the literature.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [153] [Dynamic Skewness in Stochastic Volatility Models: A Penalized Prior Approach](https://arxiv.org/abs/2508.10778)
*Bruno E. Holtz,Ricardo S. Ehlers,Adriano K. Suzuki,Francisco Louzada*

Main category: q-fin.ST

TL;DR: 本文提出DynSSV - SMSN动态偏度随机波动率模型，用惩罚先验控制复杂度，通过HMC方法估计参数，模拟和实证显示该模型有更好表现。


<details>
  <summary>Details</summary>
Motivation: 金融时间序列有偏度和厚尾性，现有模型需平衡灵活性和简约性，开发能考虑偏度时变且更稳健的模型。

Method: 提出DynSSV - SMSN模型，使用惩罚模型复杂度的先验，通过RStan包用Hamiltonian Monte Carlo方法进行参数估计。

Result: 模拟结果表明惩罚先验在多个场景下优于经典选择；对加密货币收益率的实证显示，厚尾和动态偏度模型根据信息准则对数据拟合更好。

Conclusion: 提出的DynSSV - SMSN模型结合惩罚先验在金融时间序列分析中有更好的性能和数据拟合效果。

Abstract: Financial time series often exhibit skewness and heavy tails, making it
essential to use models that incorporate these characteristics to ensure
greater reliability in the results. Furthermore, allowing temporal variation in
the skewness parameter can bring significant gains in the analysis of this type
of series. However, for more robustness, it is crucial to develop models that
balance flexibility and parsimony. In this paper, we propose dynamic skewness
stochastic volatility models in the SMSN family (DynSSV-SMSN), using priors
that penalize model complexity. Parameter estimation was carried out using the
Hamiltonian Monte Carlo (HMC) method via the \texttt{RStan} package. Simulation
results demonstrated that penalizing priors present superior performance in
several scenarios compared to the classical choices. In the empirical
application to returns of cryptocurrencies, models with heavy tails and dynamic
skewness provided a better fit to the data according to the DIC, WAIC, and
LOO-CV information criteria.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [154] [Prediction-Powered Inference with Inverse Probability Weighting](https://arxiv.org/abs/2508.10149)
*Jyotishka Datta,Nicholas G. Polson*

Main category: stat.ML

TL;DR: 论文表明可通过逆概率加权扩展预测驱动推理（PPI）以处理信息性标注，模拟显示其性能良好。


<details>
  <summary>Details</summary>
Motivation: 将PPI扩展到能处理信息性标注的情况。

Method: 用逆概率加权（IPW）版本替代PPI的无加权偏差校正项，结合设计抽样思想与预测辅助推理。

Result: 模拟中，估计倾向得分的IPW调整PPI性能与已知概率情况相近，保留名义覆盖率和方差缩减优势。

Conclusion: 提出的方法能使PPI在标注概率不同时仍有效。

Abstract: Prediction-powered inference (PPI) is a recent framework for valid
statistical inference with partially labeled data, combining model-based
predictions on a large unlabeled set with bias correction from a smaller
labeled subset. We show that PPI can be extended to handle informative labeling
by replacing its unweighted bias-correction term with an inverse probability
weighted (IPW) version, using the classical Horvitz--Thompson or H\'ajek forms.
This connection unites design-based survey sampling ideas with modern
prediction-assisted inference, yielding estimators that remain valid when
labeling probabilities vary across units. We consider the common setting where
the inclusion probabilities are not known but estimated from a correctly
specified model. In simulations, the performance of IPW-adjusted PPI with
estimated propensities closely matches the known-probability case, retaining
both nominal coverage and the variance-reduction benefits of PPI.

</details>


### [155] [Mo' Memory, Mo' Problems: Stream-Native Machine Unlearning](https://arxiv.org/abs/2508.10193)
*Kennon Stewart*

Main category: stat.ML

TL;DR: 本文将批量遗忘场景概念引入在线设置，优化了损失边界和优化方法，提升机器学习模型遗忘效率。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习遗忘工作假设的静态、独立同分布训练环境不真实存在，现代ML管道需要在生产数据流上持续学习、遗忘和预测。

Method: 用遗憾、样本复杂度和删除容量概念将批量遗忘场景转换到在线设置，收紧遗憾边界至对数级，用在线L - BFGS优化替代昂贵的Hessian求逆。

Result: 实现了对数级遗憾边界，去除了随时间线性增长的内存占用。

Conclusion: 这些改进延长了ML模型在昂贵再训练前的使用寿命，使遗忘过程更高效。

Abstract: Machine unlearning work assumes a static, i.i.d training environment that
doesn't truly exist. Modern ML pipelines need to learn, unlearn, and predict
continuously on production streams of data. We translate the notion of the
batch unlearning scenario to the online setting using notions of regret, sample
complexity, and deletion capacity. We further tighten regret bounds to a
logarithmic $\mathcal{O}(\ln{T})$, a first for a machine unlearning algorithm.
And we swap out an expensive Hessian inversion with online variant of L-BFGS
optimization, removing a memory footprint that scales linearly with time. Such
changes extend the lifespan of an ML model before expensive retraining, making
for a more efficient unlearning process.

</details>


### [156] [Dimension-Free Bounds for Generalized First-Order Methods via Gaussian Coupling](https://arxiv.org/abs/2508.10782)
*Galen Reeves*

Main category: stat.ML

TL;DR: 本文为广义一阶迭代算法在高斯数据矩阵和全内存、非可分非线性条件下的有限样本行为建立非渐近界，给出统一推导并证明上界的尖锐性。


<details>
  <summary>Details</summary>
Motivation: 研究广义一阶迭代算法在特定条件下的有限样本行为，建立非渐近界。

Method: 构建广义一阶方法迭代与条件高斯过程的显式耦合，采用直接比较广义一阶方法和条件高斯比较过程的方法，不依赖可分性或渐近性。

Result: 在温和的Lipschitz和矩匹配条件下得到紧密、无维度限制的界，给出关于Wasserstein距离的互补下界证明上界的尖锐性。

Conclusion: 该方法为高斯矩阵的AMP理论提供了统一推导，不依赖可分性或渐近性。

Abstract: We establish non-asymptotic bounds on the finite-sample behavior of
generalized first-order iterative algorithms -- including gradient-based
optimization methods and approximate message passing (AMP) -- with Gaussian
data matrices and full-memory, non-separable nonlinearities. The central result
constructs an explicit coupling between the iterates of a generalized
first-order method and a conditionally Gaussian process whose covariance
evolves deterministically via a finite-dimensional state evolution recursion.
This coupling yields tight, dimension-free bounds under mild Lipschitz and
moment-matching conditions. Our analysis departs from classical inductive AMP
proofs by employing a direct comparison between the generalized first-order
method and the conditionally Gaussian comparison process. This approach
provides a unified derivation of AMP theory for Gaussian matrices without
relying on separability or asymptotics. A complementary lower bound on the
Wasserstein distance demonstrates the sharpness of our upper bounds.

</details>


### [157] [An Iterative Algorithm for Differentially Private $k$-PCA with Adaptive Noise](https://arxiv.org/abs/2508.10879)
*Johanna Düngler,Amartya Sanyal*

Main category: stat.ML

TL;DR: 提出新算法解决差分隐私随机PCA问题，可估计前k个特征向量，克服现有方法局限，给出k>1的下界并实验验证优势。


<details>
  <summary>Details</summary>
Motivation: 现有差分隐私随机PCA方法存在样本量要求高或引入过多噪声的问题，Liu等人方法仅适用于估计最大特征向量。

Method: 提出可估计前k个特征向量的新算法。

Result: 对于k = 1，算法达到与DP - PCA相近的效用保证；给出k > 1的下界，与上界相差一个k因子。

Conclusion: 新算法克服现有方法局限，在实验中比可比基线有优势。

Abstract: Given $n$ i.i.d. random matrices $A_i \in \mathbb{R}^{d \times d}$ that share
a common expectation $\Sigma$, the objective of Differentially Private
Stochastic PCA is to identify a subspace of dimension $k$ that captures the
largest variance directions of $\Sigma$, while preserving differential privacy
(DP) of each individual $A_i$. Existing methods either (i) require the sample
size $n$ to scale super-linearly with dimension $d$, even under Gaussian
assumptions on the $A_i$, or (ii) introduce excessive noise for DP even when
the intrinsic randomness within $A_i$ is small. Liu et al. (2022a) addressed
these issues for sub-Gaussian data but only for estimating the top eigenvector
($k=1$) using their algorithm DP-PCA. We propose the first algorithm capable of
estimating the top $k$ eigenvectors for arbitrary $k \leq d$, whilst overcoming
both limitations above. For $k=1$ our algorithm matches the utility guarantees
of DP-PCA, achieving near-optimal statistical error even when $n =
\tilde{\!O}(d)$. We further provide a lower bound for general $k > 1$, matching
our upper bound up to a factor of $k$, and experimentally demonstrate the
advantages of our algorithm over comparable baselines.

</details>


### [158] [Conic Formulations of Transport Metrics for Unbalanced Measure Networks and Hypernetworks](https://arxiv.org/abs/2508.10888)
*Mary Chriselda Antony Oliver,Emmanuel Hartman,Tom Needham*

Main category: stat.ML

TL;DR: 本文聚焦Conic Gromov - Wasserstein (CGW)距离，提出新的半耦合公式，拓展其应用范围，研究其性质与鲁棒性，给出算法并通过实验验证可行性。


<details>
  <summary>Details</summary>
Motivation: 为克服Gromov - Wasserstein (GW)距离的局限性，拓展其应用于更一般的网络和超网络结构。

Method: 提出基于半耦合的新公式，研究CGW距离的性质，推导鲁棒性的定量界限，给出块坐标上升算法。

Result: 建立了CGW度量的基本性质，推导了鲁棒性界限，算法在合成和真实数据集上具有计算可行性和可扩展性。

Conclusion: 新的CGW距离公式能有效拓展应用范围，算法可行且可扩展。

Abstract: The Gromov-Wasserstein (GW) variant of optimal transport, designed to compare
probability densities defined over distinct metric spaces, has emerged as an
important tool for the analysis of data with complex structure, such as
ensembles of point clouds or networks. To overcome certain limitations, such as
the restriction to comparisons of measures of equal mass and sensitivity to
outliers, several unbalanced or partial transport relaxations of the GW
distance have been introduced in the recent literature. This paper is concerned
with the Conic Gromov-Wasserstein (CGW) distance introduced by
S\'{e}journ\'{e}, Vialard, and Peyr\'{e}. We provide a novel formulation in
terms of semi-couplings, and extend the framework beyond the metric measure
space setting, to compare more general network and hypernetwork structures.
With this new formulation, we establish several fundamental properties of the
CGW metric, including its scaling behavior under dilation, variational
convergence in the limit of volume growth constraints, and comparison bounds
with established optimal transport metrics. We further derive quantitative
bounds that characterize the robustness of the CGW metric to perturbations in
the underlying measures. The hypernetwork formulation of CGW admits a simple
and provably convergent block coordinate ascent algorithm for its estimation,
and we demonstrate the computational tractability and scalability of our
approach through experiments on synthetic and real-world high-dimensional and
structured datasets.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [159] [BKP: An R Package for Beta Kernel Process Modeling](https://arxiv.org/abs/2508.10447)
*Jiangyan Zhao,Kunhai Qing,Jin Xu*

Main category: stat.CO

TL;DR: 介绍了R包BKP，实现Beta Kernel Process，可对空间变化的二项概率建模，有多种特性，还可扩展到DKP，通过数据集展示其优势，旨在推动相关应用和发展。


<details>
  <summary>Details</summary>
Motivation: 开发一个用户友好且可扩展的工具，用于对空间变化的二项概率建模，推动基于核的beta建模在统计和机器学习中的实际应用和方法发展。

Method: 结合局部核加权似然与共轭beta先验，实现闭式后验推断，支持不同响应类型，提供核超参数调优程序。

Result: 开发出首个公开可用的基于BKP方法的R包BKP，通过合成和真实数据集展示了其可解释性、准确性和可扩展性。

Conclusion: BKP包有助于基于核的beta建模的实际应用和未来方法发展。

Abstract: We present BKP, a user-friendly and extensible R package that implements the
Beta Kernel Process (BKP) -- a fully nonparametric and computationally
efficient framework for modeling spatially varying binomial probabilities. The
BKP model combines localized kernel-weighted likelihoods with conjugate beta
priors, resulting in closed-form posterior inference without requiring latent
variable augmentation or intensive MCMC sampling. The package supports binary
and aggregated binomial responses, allows flexible choices of kernel functions
and prior specification, and provides loss-based kernel hyperparameter tuning
procedures. In addition, BKP extends naturally to the Dirichlet Kernel Process
(DKP) for modeling spatially varying multinomial or compositional data. To our
knowledge, this is the first publicly available R package for implementing
BKP-based methods. We illustrate the use of BKP through several synthetic and
real-world datasets, highlighting its interpretability, accuracy, and
scalability. The package aims to facilitate practical application and future
methodological development of kernel-based beta modeling in statistics and
machine learning.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [160] [A Comprehensive Comparison of the Wald, Wilson, and adjusted Wilson Confidence Intervals for Proportions](https://arxiv.org/abs/2508.10223)
*Nabil Kahouadji*

Main category: stat.ME

TL;DR: 本文引入彩虹色码和像素颜色图比较不同样本量、总体比例值和置信水平下Wald、Wilson和调整后的Wilson置信区间，指出不同置信水平下调整后Wilson置信区间的最佳伪观测值数量及优势。


<details>
  <summary>Details</summary>
Motivation: 传统教材中常用的Wald置信区间覆盖概率不佳，尤其是小样本或总体比例接近0或1时，需要更好的方法比较不同置信区间。

Method: 引入彩虹色码和像素颜色图，在样本量n=1到1000、总体比例p=0.01到0.99及三种典型置信水平下比较Wald、Wilson和调整后的Wilson置信区间。

Result: 90%、95%和99%调整后的Wilson置信区间分别添加3、4和6个伪观测值效果最佳，且优于对应Wald和Wilson置信区间。

Conclusion: 调整后的Wilson置信区间在不同置信水平下表现更好，有更好的覆盖概率。

Abstract: The standard confidence interval for a population proportion covered in the
overwhelming majority of introductory and intermediate statistics textbooks
surprisingly remains the Wald confidence interval despite having a poor
coverage probability, especially for small sample sizes or when the unknown
population proportion is close to either 0 or 1. Using the mean coverage
probability, and for some sample sizes, Agresti and Coull showed not only that
the 95\% Wilson confidence interval performs better, but also showed that 95\%
adjusted Wilson of type 4 confidence interval, obtained by simply adding four
pseudo-observations, outperforms both the Wald and the Wilson confidence
intervals. In this paper, we introduce a rainbow color code and pixel-color
plots as ways to comprehensively compare the Wald, Wilson, and adjusted-Wilson
of type $\epsilon$ confidence intervals across all sample sizes $n=1, 2, \dots,
1000$, population proportion values $p=0.01, 0.02, \dots, 0.99$, and for the
three typical confidence levels. We show not only that adding 3 (resp., 4 and
6) pseudo-observations is the best for the 90\% (resp., 95\% and 99\%) adjusted
Wilson confidence interval, but it also performs better than both the 90\%
(resp., 95\% and 99\%) Wald and Wilson confidence intervals.

</details>


### [161] [Bayesian Models for Joint Selection of Features and Auto-Regressive Lags: Theory and Applications in Environmental and Financial Forecasting](https://arxiv.org/abs/2508.10055)
*Alokesh Manna,Sujit K. Ghosh*

Main category: stat.ME

TL;DR: 本文提出贝叶斯框架用于自相关误差线性回归的变量选择，用分层贝叶斯模型和高效MCMC算法，理论证明一致性，模拟和实际应用显示其优势。


<details>
  <summary>Details</summary>
Motivation: 在时间序列应用中，处理响应依赖于同期或过去解释变量及持续随机冲击的情况，解决高维计算挑战。

Method: 使用带尖峰 - 平板先验的分层贝叶斯模型同时选择相关协变量和滞后误差项，提出两阶段MCMC算法。

Result: 通过模拟和实际应用，在变量选择准确性和预测性能上有显著提升，相比现有方法有更低MSPE、更好的模型组件识别和更强鲁棒性。

Conclusion: 该框架在自回归设置下对模型解释和预测有实际应用价值。

Abstract: We develop a Bayesian framework for variable selection in linear regression
with autocorrelated errors, accommodating lagged covariates and autoregressive
structures. This setting occurs in time series applications where responses
depend on contemporaneous or past explanatory variables and persistent
stochastic shocks, including financial modeling, hydrological forecasting, and
meteorological applications requiring temporal dependency capture. Our
methodology uses hierarchical Bayesian models with spike-and-slab priors to
simultaneously select relevant covariates and lagged error terms. We propose an
efficient two-stage MCMC algorithm separating sampling of variable inclusion
indicators and model parameters to address high-dimensional computational
challenges. Theoretical analysis establishes posterior selection consistency
under mild conditions, even when candidate predictors grow exponentially with
sample size, common in modern time series with many potential lagged variables.
Through simulations and real applications (groundwater depth prediction, S&P
500 log returns modeling), we demonstrate substantial gains in variable
selection accuracy and predictive performance. Compared to existing methods,
our framework achieves lower MSPE, improved true model component
identification, and greater robustness with autocorrelated noise, underscoring
practical utility for model interpretation and forecasting in autoregressive
settings.

</details>


<div id='physics.data-an'></div>

# physics.data-an [[Back]](#toc)

### [162] [Jet Image Tagging Using Deep Learning: An Ensemble Model](https://arxiv.org/abs/2508.10034)
*Juvenal Bassa,Vidya Manian,Sudhir Malik,Arghya Chattopadhyay*

Main category: physics.data-an

TL;DR: 本文采用集成神经网络方法对高能粒子物理中的喷注进行分类，性能优于单个网络。


<details>
  <summary>Details</summary>
Motivation: 传统喷注分类方法难以捕捉其复杂多维结构，需要先进机器学习方法。

Method: 同时使用两个神经网络作为集成，将喷注数据转换为二维直方图，对JetNet数据集中的喷注进行分类。

Result: 集成模型可用于二分类和多分类。

Conclusion: 集成方法利用各组成网络优势学习喷注特征，性能优于单个网络。

Abstract: Jet classification in high-energy particle physics is important for
understanding fundamental interactions and probing phenomena beyond the
Standard Model. Jets originate from the fragmentation and hadronization of
quarks and gluons, and pose a challenge for identification due to their
complex, multidimensional structure. Traditional classification methods often
fall short in capturing these intricacies, necessitating advanced machine
learning approaches. In this paper, we employ two neural networks
simultaneously as an ensemble to tag various jet types. We convert the jet data
to two-dimensional histograms instead of representing them as points in a
higher-dimensional space. Specifically, this ensemble approach, hereafter
referred to as Ensemble Model, is used to tag jets into classes from the JetNet
dataset, corresponding to: Top Quarks, Light Quarks (up or down), and W and Z
bosons. For the jet classes mentioned above, we show that the Ensemble Model
can be used for both binary and multi-categorical classification. This ensemble
approach learns jet features by leveraging the strengths of each constituent
network achieving superior performance compared to either individual network.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [163] [Symmetry-Constrained Multi-Scale Physics-Informed Neural Networks for Graphene Electronic Band Structure Prediction](https://arxiv.org/abs/2508.10718)
*Wei Shan Lee,I Hang Kwok,Kam Ian Leong,Chi Kiu Althina Chau,Kei Chon Sio*

Main category: cond-mat.mtrl-sci

TL;DR: 提出对称约束多尺度物理信息神经网络(SCMS - PINN)预测二维材料电子能带结构，训练效果好，误差小，保证对称性，为拓展物理信息学习奠定基础。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以平衡计算效率和物理精度，准确预测二维材料电子能带结构仍是基本挑战。

Method: 提出SCMS - PINN，通过多头部架构强制晶体对称性，引入三种ResNet - 6路径，对k点提取的31个物理信息特征操作，采用渐进狄拉克约束调度。

Result: 训练10000个k点300个周期，训练损失降低99.99%，验证损失0.0085，预测狄拉克点间隙接近理论零，布里渊区平均误差小，保证精确对称性。

Conclusion: 该框架为将物理信息学习拓展到更广泛二维材料以加速发现奠定基础。

Abstract: Accurate prediction of electronic band structures in two-dimensional
materials remains a fundamental challenge, with existing methods struggling to
balance computational efficiency and physical accuracy. We present the
Symmetry-Constrained Multi-Scale Physics-Informed Neural Network (SCMS-PINN)
v35, which directly learns graphene band structures while rigorously enforcing
crystallographic symmetries through a multi-head architecture. Our approach
introduces three specialized ResNet-6 pathways -- K-head for Dirac physics,
M-head for saddle points, and General head for smooth interpolation --
operating on 31 physics-informed features extracted from k-points. Progressive
Dirac constraint scheduling systematically increases the weight parameter from
5.0 to 25.0, enabling hierarchical learning from global topology to local
critical physics. Training on 10,000 k-points over 300 epochs achieves 99.99\%
reduction in training loss (34.597 to 0.003) with validation loss of 0.0085.
The model predicts Dirac point gaps within 30.3 $\mu$eV of theoretical zero and
achieves average errors of 53.9 meV (valence) and 40.5 meV (conduction) across
the Brillouin zone. All twelve C$_{6v}$ operations are enforced through
systematic averaging, guaranteeing exact symmetry preservation. This framework
establishes a foundation for extending physics-informed learning to broader
two-dimensional materials for accelerated discovery.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [164] [Enabling Generic Robot Skill Implementation Using Object Oriented Programming](https://arxiv.org/abs/2508.10497)
*Abdullah Farrukh,Achim Wagner,Martin Ruskowski*

Main category: cs.RO

TL;DR: 提出软件框架减少部署机器人系统的工作量，用Python实现概念原型，目标系统是含Yaskawa Motoman GP4的拆垛单元。


<details>
  <summary>Details</summary>
Motivation: 中小企业缺乏机器人专业知识，实施、维护和开发机器人系统有挑战，且依赖外部专业知识会导致供应商锁定和外部依赖；学术研究中使用机器人系统也面临处理复杂接口的难题。

Method: 提出软件框架，聚焦简化现代机器人系统不同接口，使用抽象层适配不同制造商和型号，用Python实现概念原型。

Result: 未提及具体结果。

Conclusion: 未提及明确结论。

Abstract: Developing robotic algorithms and integrating a robotic subsystem into a
larger system can be a difficult task. Particularly in small and medium-sized
enterprises (SMEs) where robotics expertise is lacking, implementing,
maintaining and developing robotic systems can be a challenge. As a result,
many companies rely on external expertise through system integrators, which, in
some cases, can lead to vendor lock-in and external dependency. In the academic
research on intelligent manufacturing systems, robots play a critical role in
the design of robust autonomous systems. Similar challenges are faced by
researchers who want to use robotic systems as a component in a larger smart
system, without having to deal with the complexity and vastness of the robot
interfaces in detail. In this paper, we propose a software framework that
reduces the effort required to deploy a working robotic system. The focus is
solely on providing a concept for simplifying the different interfaces of a
modern robot system and using an abstraction layer for different manufacturers
and models. The Python programming language is used to implement a prototype of
the concept. The target system is a bin-picking cell containing a Yaskawa
Motoman GP4.

</details>


### [165] [CorrectNav: Self-Correction Flywheel Empowers Vision-Language-Action Navigation Model](https://arxiv.org/abs/2508.10416)
*Zhuoyuan Yu,Yuxing Long,Zihan Yang,Chengyan Zeng,Hongwei Fan,Jiyao Zhang,Hao Dong*

Main category: cs.RO

TL;DR: 提出Self - correction Flywheel后训练范式，迭代提升CorrectNav模型，在基准测试和真实机器人测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有视觉 - 语言导航模型缺乏有效纠错能力，难以从错误中恢复。

Method: 提出Self - correction Flywheel范式，将训练集的错误轨迹作为数据来源，识别偏差并生成自纠错数据用于持续训练。

Result: 在R2R - CE和RxR - CE基准测试中，CorrectNav达到65.1%和69.3%的成功率，超越先前最佳模型；真实机器人测试展现出纠错、避障和长指令跟随能力。

Conclusion: Self - correction Flywheel范式能有效提升视觉 - 语言导航模型性能。

Abstract: Existing vision-and-language navigation models often deviate from the correct
trajectory when executing instructions. However, these models lack effective
error correction capability, hindering their recovery from errors. To address
this challenge, we propose Self-correction Flywheel, a novel post-training
paradigm. Instead of considering the model's error trajectories on the training
set as a drawback, our paradigm emphasizes their significance as a valuable
data source. We have developed a method to identify deviations in these error
trajectories and devised innovative techniques to automatically generate
self-correction data for perception and action. These self-correction data
serve as fuel to power the model's continued training. The brilliance of our
paradigm is revealed when we re-evaluate the model on the training set,
uncovering new error trajectories. At this time, the self-correction flywheel
begins to spin. Through multiple flywheel iterations, we progressively enhance
our monocular RGB-based VLA navigation model CorrectNav. Experiments on R2R-CE
and RxR-CE benchmarks show CorrectNav achieves new state-of-the-art success
rates of 65.1% and 69.3%, surpassing prior best VLA navigation models by 8.2%
and 16.4%. Real robot tests in various indoor and outdoor environments
demonstrate \method's superior capability of error correction, dynamic obstacle
avoidance, and long instruction following.

</details>


### [166] [MASH: Cooperative-Heterogeneous Multi-Agent Reinforcement Learning for Single Humanoid Robot Locomotion](https://arxiv.org/abs/2508.10423)
*Qi Liu,Xiaopeng Zhang,Mingshan Tan,Shuaikang Ma,Jinliang Ding,Yanjie Li*

Main category: cs.RO

TL;DR: 本文提出用合作异构多智能体深度强化学习（MARL）的 MASH 方法优化单个人形机器人的运动，实验显示该方法优于传统单智能体强化学习方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法多使用单智能体强化学习算法处理单个人形机器人或用 MARL 算法处理多机器人系统任务，本文旨在提出新范式优化单个人形机器人的运动。

Method: 提出多智能体强化学习用于单个人形机器人运动（MASH）方法，将每个肢体视为独立智能体探索动作空间，同时共享全局评论家进行合作学习。

Result: MASH 加速了训练收敛，提高了全身协作能力，优于传统单智能体强化学习方法。

Conclusion: 本研究推动了 MARL 在单个人形机器人控制中的集成，为高效运动策略提供了新见解。

Abstract: This paper proposes a novel method to enhance locomotion for a single
humanoid robot through cooperative-heterogeneous multi-agent deep reinforcement
learning (MARL). While most existing methods typically employ single-agent
reinforcement learning algorithms for a single humanoid robot or MARL
algorithms for multi-robot system tasks, we propose a distinct paradigm:
applying cooperative-heterogeneous MARL to optimize locomotion for a single
humanoid robot. The proposed method, multi-agent reinforcement learning for
single humanoid locomotion (MASH), treats each limb (legs and arms) as an
independent agent that explores the robot's action space while sharing a global
critic for cooperative learning. Experiments demonstrate that MASH accelerates
training convergence and improves whole-body cooperation ability, outperforming
conventional single-agent reinforcement learning methods. This work advances
the integration of MARL into single-humanoid-robot control, offering new
insights into efficient locomotion strategies.

</details>


### [167] [The SET Perceptual Factors Framework: Towards Assured Perception for Autonomous Systems](https://arxiv.org/abs/2508.10798)
*Troi Williams*

Main category: cs.RO

TL;DR: 引入SET感知因素框架解决自主系统感知可靠性问题，促进安全保障与公众信任。


<details>
  <summary>Details</summary>
Motivation: 自主系统部署引发安全和可信度担忧，需确保机器人感知可靠性。

Method: 引入SET感知因素框架，用SET状态树和SET因素树分析影响感知的因素，开发感知因素模型量化不确定性。

Result: 框架可提供透明、标准化方法识别、建模和传达感知风险。

Conclusion: 框架有助于促进严格的安全保障，培养公众对自主系统的理解和信任。

Abstract: Future autonomous systems promise significant societal benefits, yet their
deployment raises concerns about safety and trustworthiness. A key concern is
assuring the reliability of robot perception, as perception seeds safe
decision-making. Failures in perception are often due to complex yet common
environmental factors and can lead to accidents that erode public trust. To
address this concern, we introduce the SET (Self, Environment, and Target)
Perceptual Factors Framework. We designed the framework to systematically
analyze how factors such as weather, occlusion, or sensor limitations
negatively impact perception. To achieve this, the framework employs SET State
Trees to categorize where such factors originate and SET Factor Trees to model
how these sources and factors impact perceptual tasks like object detection or
pose estimation. Next, we develop Perceptual Factor Models using both trees to
quantify the uncertainty for a given task. Our framework aims to promote
rigorous safety assurances and cultivate greater public understanding and trust
in autonomous systems by offering a transparent and standardized method for
identifying, modeling, and communicating perceptual risks.

</details>


### [168] [A Multimodal Neural Network for Recognizing Subjective Self-Disclosure Towards Social Robots](https://arxiv.org/abs/2508.10828)
*Henry Powell,Guy Laban,Emily S. Cross*

Main category: cs.RO

TL;DR: 本文开发基于情感识别模型的多模态注意力网络，用自采集视频语料训练，构建新损失函数，取得较好F1分数，助力社交机器人识别自我表露。


<details>
  <summary>Details</summary>
Motivation: 现有研究在构建准确建模主观自我表露的计算系统方面工作较少，尤其针对人类与机器人互动中自我表露建模的研究更少，而社交机器人需在各种社交场景与人类合作并建立关系，需求迫切。

Method: 开发基于情感识别文献模型的自定义多模态注意力网络，使用自采集的自我表露视频语料训练模型，构建新的损失函数——尺度保留交叉熵损失。

Result: 使用新损失函数训练的最佳模型F1分数达到0.83，比最佳基线模型提高0.48。

Conclusion: 研究在使社交机器人识别互动伙伴自我表露方面取得重大进展，该能力对具有社会认知的社交机器人至关重要。

Abstract: Subjective self-disclosure is an important feature of human social
interaction. While much has been done in the social and behavioural literature
to characterise the features and consequences of subjective self-disclosure,
little work has been done thus far to develop computational systems that are
able to accurately model it. Even less work has been done that attempts to
model specifically how human interactants self-disclose with robotic partners.
It is becoming more pressing as we require social robots to work in conjunction
with and establish relationships with humans in various social settings. In
this paper, our aim is to develop a custom multimodal attention network based
on models from the emotion recognition literature, training this model on a
large self-collected self-disclosure video corpus, and constructing a new loss
function, the scale preserving cross entropy loss, that improves upon both
classification and regression versions of this problem. Our results show that
the best performing model, trained with our novel loss function, achieves an F1
score of 0.83, an improvement of 0.48 from the best baseline model. This result
makes significant headway in the aim of allowing social robots to pick up on an
interaction partner's self-disclosures, an ability that will be essential in
social robots with social cognition.

</details>


### [169] [TLE-Based A2C Agent for Terrestrial Coverage Orbital Path Planning](https://arxiv.org/abs/2508.10872)
*Anantha Narayanan,Battu Bhanu Teja,Pruthwik Mishra*

Main category: cs.RO

TL;DR: 本文提出用A2C算法的强化学习框架优化卫星轨道参数，对比PPO显示A2C性能更优，确立强化学习用于LEO任务规划的地位。


<details>
  <summary>Details</summary>
Motivation: 低地球轨道（LEO）日益拥堵，给地球观测卫星部署和运行带来挑战，需优化轨道参数。

Method: 将问题建模为马尔可夫决策过程，在自定义OpenAI Gymnasium环境中，用经典开普勒元素模拟轨道动力学，使用A2C算法让智能体学习调整轨道参数。

Result: A2C比PPO性能更优，累积奖励高5.8倍，收敛时间步少31.5倍，能满足不同任务目标，保持计算效率。

Conclusion: 强化学习可作为计算高效的替代方案，用于可扩展和智能的LEO任务规划。

Abstract: The increasing congestion of Low Earth Orbit (LEO) poses persistent
challenges to the efficient deployment and safe operation of Earth observation
satellites. Mission planners must now account not only for mission-specific
requirements but also for the increasing collision risk with active satellites
and space debris. This work presents a reinforcement learning framework using
the Advantage Actor-Critic (A2C) algorithm to optimize satellite orbital
parameters for precise terrestrial coverage within predefined surface radii. By
formulating the problem as a Markov Decision Process (MDP) within a custom
OpenAI Gymnasium environment, our method simulates orbital dynamics using
classical Keplerian elements. The agent progressively learns to adjust five of
the orbital parameters - semi-major axis, eccentricity, inclination, right
ascension of ascending node, and the argument of perigee-to achieve targeted
terrestrial coverage. Comparative evaluation against Proximal Policy
Optimization (PPO) demonstrates A2C's superior performance, achieving 5.8x
higher cumulative rewards (10.0 vs 9.263025) while converging in 31.5x fewer
timesteps (2,000 vs 63,000). The A2C agent consistently meets mission
objectives across diverse target coordinates while maintaining computational
efficiency suitable for real-time mission planning applications. Key
contributions include: (1) a TLE-based orbital simulation environment
incorporating physics constraints, (2) validation of actor-critic methods'
superiority over trust region approaches in continuous orbital control, and (3)
demonstration of rapid convergence enabling adaptive satellite deployment. This
approach establishes reinforcement learning as a computationally efficient
alternative for scalable and intelligent LEO mission planning.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [170] [Multi-Label Plant Species Prediction with Metadata-Enhanced Multi-Head Vision Transformers](https://arxiv.org/abs/2508.10457)
*Hanna Herasimchyk,Robin Labryga,Tomislav Prusina*

Main category: cs.CV

TL;DR: 提出多头部视觉变换器方法用于植被图图像多标签植物物种预测，在PlantCLEF 2025挑战中表现出色，代码开源。


<details>
  <summary>Details</summary>
Motivation: 解决PlantCLEF 2025挑战中训练单物种图像、测试多物种样方图像的领域偏移问题。

Method: 利用预训练DINOv2 ViT - B/14骨干网络，多分类头结合分类学层次，采用多尺度切片、动态阈值优化、集成策略等，还运用多种推理技术。

Result: 在约140万张涵盖7806种植物的训练图像上实验，提交结果在私有排行榜上排名第3。

Conclusion: 所提出的方法在多标签植物物种预测任务中表现良好，具有有效性。

Abstract: We present a multi-head vision transformer approach for multi-label plant
species prediction in vegetation plot images, addressing the PlantCLEF 2025
challenge. The task involves training models on single-species plant images
while testing on multi-species quadrat images, creating a drastic domain shift.
Our methodology leverages a pre-trained DINOv2 Vision Transformer Base
(ViT-B/14) backbone with multiple classification heads for species, genus, and
family prediction, utilizing taxonomic hierarchies. Key contributions include
multi-scale tiling to capture plants at different scales, dynamic threshold
optimization based on mean prediction length, and ensemble strategies through
bagging and Hydra model architectures. The approach incorporates various
inference techniques including image cropping to remove non-plant artifacts,
top-n filtering for prediction constraints, and logit thresholding strategies.
Experiments were conducted on approximately 1.4 million training images
covering 7,806 plant species. Results demonstrate strong performance, making
our submission 3rd best on the private leaderboard. Our code is available at
https://github.com/geranium12/plant-clef-2025/tree/v1.0.0.

</details>


### [171] [Empowering Morphing Attack Detection using Interpretable Image-Text Foundation Model](https://arxiv.org/abs/2508.10110)
*Sushrut Patwardhan,Raghavendra Ramachandra,Sushma Venkatesh*

Main category: cs.CV

TL;DR: 提出多模态学习方法用于面部变形攻击检测，用CLIP进行零样本评估，分析十种文本提示并在面部变形数据集上实验。


<details>
  <summary>Details</summary>
Motivation: 变形攻击检测是人脸识别系统确保可靠验证的重要组成部分，需要有效检测方法。

Method: 提出多模态学习方法，用CLIP进行零样本评估，分析十种不同文本提示，在公开人脸生物特征数据集开发的面部变形数据集上实验。

Result: 未明确提及具体结果，但进行了广泛实验，评估了SOTA预训练神经网络和所提框架在五种变形生成技术的零样本评估。

Conclusion: 未明确提及具体结论。

Abstract: Morphing attack detection has become an essential component of face
recognition systems for ensuring a reliable verification scenario. In this
paper, we present a multimodal learning approach that can provide a textual
description of morphing attack detection. We first show that zero-shot
evaluation of the proposed framework using Contrastive Language-Image
Pretraining (CLIP) can yield not only generalizable morphing attack detection,
but also predict the most relevant text snippet. We present an extensive
analysis of ten different textual prompts that include both short and long
textual prompts. These prompts are engineered by considering the human
understandable textual snippet. Extensive experiments were performed on a face
morphing dataset that was developed using a publicly available face biometric
dataset. We present an evaluation of SOTA pre-trained neural networks together
with the proposed framework in the zero-shot evaluation of five different
morphing generation techniques that are captured in three different mediums.

</details>


### [172] [Improving watermelon (Citrullus lanatus) disease classification with generative artificial intelligence (GenAI)-based synthetic and real-field images via a custom EfficientNetV2-L model](https://arxiv.org/abs/2508.10156)
*Nitin Rai,Nathan S. Boyd,Gary E. Vallad,Arnold W. Schumann*

Main category: cs.CV

TL;DR: 研究探讨合成与真实图像结合对西瓜疾病分类模型性能的影响，发现混合使用可提升性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前关于合成与真实图像结合提升疾病分类性能的研究有限，本研究旨在探究结合少量真实图像与合成图像能否提高EfficientNetV2 - L模型对西瓜疾病的分类预测准确性。

Method: 将训练数据集分为五种处理方式，用定制的EfficientNetV2 - L架构结合增强的微调与迁移学习技术进行训练。

Result: H2、H3和H4处理方式训练的模型有高精确率、召回率和F1分数，加权F1分数从H0的0.65提升到H3 - H4的1.00。

Conclusion: 合成图像不能完全替代真实图像，需混合使用以最大化作物疾病分类模型的性能。

Abstract: The current advancements in generative artificial intelligence (GenAI) models
have paved the way for new possibilities for generating high-resolution
synthetic images, thereby offering a promising alternative to traditional image
acquisition for training computer vision models in agriculture. In the context
of crop disease diagnosis, GenAI models are being used to create synthetic
images of various diseases, potentially facilitating model creation and
reducing the dependency on resource-intensive in-field data collection.
However, limited research has been conducted on evaluating the effectiveness of
integrating real with synthetic images to improve disease classification
performance. Therefore, this study aims to investigate whether combining a
limited number of real images with synthetic images can enhance the prediction
accuracy of an EfficientNetV2-L model for classifying watermelon
\textit{(Citrullus lanatus)} diseases. The training dataset was divided into
five treatments: H0 (only real images), H1 (only synthetic images), H2 (1:1
real-to-synthetic), H3 (1:10 real-to-synthetic), and H4 (H3 + random images to
improve variability and model generalization). All treatments were trained
using a custom EfficientNetV2-L architecture with enhanced fine-tuning and
transfer learning techniques. Models trained on H2, H3, and H4 treatments
demonstrated high precision, recall, and F1-score metrics. Additionally, the
weighted F1-score increased from 0.65 (on H0) to 1.00 (on H3-H4) signifying
that the addition of a small number of real images with a considerable volume
of synthetic images improved model performance and generalizability. Overall,
this validates the findings that synthetic images alone cannot adequately
substitute for real images; instead, both must be used in a hybrid manner to
maximize model performance for crop disease classification.

</details>


### [173] [SegDAC: Segmentation-Driven Actor-Critic for Visual Reinforcement Learning](https://arxiv.org/abs/2508.09325)
*Alexandre Brown,Glen Berseth*

Main category: cs.CV

TL;DR: 提出SegDAC方法用于视觉强化学习，在视觉泛化基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 视觉强化学习需从高维输入和噪声奖励中学习感知与动作，大感知模型与强化学习的有效集成尚不明确。

Method: 提出SegDAC，利用Segment Anything进行以对象为中心的分解，用YOLO - World通过文本提示进行语义定位，采用基于Transformer的架构，使用在线强化学习学习关注的片段。

Result: 在Maniskill3的视觉泛化基准测试中，SegDAC实现了显著更好的视觉泛化，在最难设置下性能翻倍，在样本效率上匹配或超越先前方法。

Conclusion: SegDAC在视觉强化学习中能有效提升视觉泛化能力和样本效率。

Abstract: Visual reinforcement learning (RL) is challenging due to the need to learn
both perception and actions from high-dimensional inputs and noisy rewards.
Although large perception models exist, integrating them effectively into RL
for visual generalization and improved sample efficiency remains unclear. We
propose SegDAC, a Segmentation-Driven Actor-Critic method. SegDAC uses Segment
Anything (SAM) for object-centric decomposition and YOLO-World to ground
segments semantically via text prompts. It includes a novel transformer-based
architecture that supports a dynamic number of segments at each time step and
effectively learns which segments to focus on using online RL, without using
human labels. By evaluating SegDAC over a challenging visual generalization
benchmark using Maniskill3, which covers diverse manipulation tasks under
strong visual perturbations, we demonstrate that SegDAC achieves significantly
better visual generalization, doubling prior performance on the hardest setting
and matching or surpassing prior methods in sample efficiency across all
evaluated tasks.

</details>


### [174] [MRFD: Multi-Region Fusion Decoding with Self-Consistency for Mitigating Hallucinations in LVLMs](https://arxiv.org/abs/2508.10264)
*Haonan Ge,Yiwei Wang,Ming-Hsuan Yang,Yujun Cai*

Main category: cs.CV

TL;DR: 提出无训练解码方法MRFD减少大视觉语言模型幻觉，实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 大视觉语言模型在图像不同区域信息验证能力有限，常产生与视觉输入不一致的幻觉文本。

Method: 提出Multi - Region Fusion Decoding (MRFD)，通过交叉注意力识别显著区域，为每个区域生成初始响应，基于Jensen - Shannon Divergence (JSD)计算可靠性权重，使用受思维链推理启发的区域感知提示引导区域预测的一致性融合。

Result: 在多个大视觉语言模型和基准测试中，MRFD显著减少了幻觉现象，提高了响应的事实性，且无需更新模型。

Conclusion: MRFD能有效改善大视觉语言模型的事实基础，减少幻觉。

Abstract: Large Vision-Language Models (LVLMs) have shown strong performance across
multimodal tasks. However, they often produce hallucinations -- text that is
inconsistent with visual input, due to the limited ability to verify
information in different regions of the image. To address this, we propose
Multi-Region Fusion Decoding (MRFD), a training-free decoding method that
improves factual grounding by modeling inter-region consistency. MRFD
identifies salient regions using cross-attention, generates initial responses
for each, and computes reliability weights based on Jensen-Shannon Divergence
(JSD) among the responses. These weights guide a consistency-aware fusion of
per-region predictions, using region-aware prompts inspired by Chain-of-Thought
reasoning. Experiments across multiple LVLMs and benchmarks show that MRFD
significantly reduces hallucinations and improves response factuality without
requiring model updates.

</details>


### [175] [Pose-Robust Calibration Strategy for Point-of-Gaze Estimation on Mobile Phones](https://arxiv.org/abs/2508.10268)
*Yujie Zhao,Jiabei Zeng,Shiguang Shan*

Main category: cs.CV

TL;DR: 研究解决校准后注视点估计器对头部姿势变化敏感问题，构建基准并分析影响因素，提出动态校准策略。


<details>
  <summary>Details</summary>
Motivation: 现有基于外观的注视点估计器因个体差异需特定校准，且校准后的估计器对头部姿势变化敏感。

Method: 构建 MobilePoG 基准，分析校准点和头部姿势多样性对估计精度的影响，提出动态校准策略。

Result: 校准中引入更广泛头部姿势能提升估计器处理姿势变化的能力，动态校准策略产生的估计器对头部姿势变化更不敏感。

Conclusion: 提出的动态校准策略是一种用户友好、高效的校准方式，能产生更好的校准估计器。

Abstract: Although appearance-based point-of-gaze (PoG) estimation has improved, the
estimators still struggle to generalize across individuals due to personal
differences. Therefore, person-specific calibration is required for accurate
PoG estimation. However, calibrated PoG estimators are often sensitive to head
pose variations. To address this, we investigate the key factors influencing
calibrated estimators and explore pose-robust calibration strategies.
Specifically, we first construct a benchmark, MobilePoG, which includes facial
images from 32 individuals focusing on designated points under either fixed or
continuously changing head poses. Using this benchmark, we systematically
analyze how the diversity of calibration points and head poses influences
estimation accuracy. Our experiments show that introducing a wider range of
head poses during calibration improves the estimator's ability to handle pose
variation. Building on this insight, we propose a dynamic calibration strategy
in which users fixate on calibration points while moving their phones. This
strategy naturally introduces head pose variation during a user-friendly and
efficient calibration process, ultimately producing a better calibrated PoG
estimator that is less sensitive to head pose variations than those using
conventional calibration strategies. Codes and datasets are available at our
project page.

</details>


### [176] [Unlocking Robust Semantic Segmentation Performance via Label-only Elastic Deformations against Implicit Label Noise](https://arxiv.org/abs/2508.10383)
*Yechan Kim,Dongho Yoon,Younkwan Lee,Unse Fatima,Hong Kook Kim,Songjae Lee,Sanga Park,Jeong Ho Park,Seonjong Kang,Moongu Jeon*

Main category: cs.CV

TL;DR: 提出NSegment+增强框架处理语义分割中隐性标签噪声，实验表明其能提升性能。


<details>
  <summary>Details</summary>
Motivation: 过往图像分割研究多关注严重标签噪声，现实数据存在隐性标签缺陷，典型数据增强方法会放大此类缺陷，影响模型泛化能力。

Method: 引入NSegment+框架，对分割标签进行可控弹性变形，保持原始图像不变，促使模型学习鲁棒的对象结构表示。

Result: NSegment+持续提升性能，在多个数据集上平均mIoU有显著提升，结合其他训练技巧可进一步放大增益。

Conclusion: 处理隐性标签噪声很重要，NSegment+方法有效。

Abstract: While previous studies on image segmentation focus on handling severe (or
explicit) label noise, real-world datasets also exhibit subtle (or implicit)
label imperfections. These arise from inherent challenges, such as ambiguous
object boundaries and annotator variability. Although not explicitly present,
such mild and latent noise can still impair model performance. Typical data
augmentation methods, which apply identical transformations to the image and
its label, risk amplifying these subtle imperfections and limiting the model's
generalization capacity. In this paper, we introduce NSegment+, a novel
augmentation framework that decouples image and label transformations to
address such realistic noise for semantic segmentation. By introducing
controlled elastic deformations only to segmentation labels while preserving
the original images, our method encourages models to focus on learning robust
representations of object structures despite minor label inconsistencies.
Extensive experiments demonstrate that NSegment+ consistently improves
performance, achieving mIoU gains of up to +2.29, +2.38, +1.75, and +3.39 in
average on Vaihingen, LoveDA, Cityscapes, and PASCAL VOC, respectively-even
without bells and whistles, highlighting the importance of addressing implicit
label noise. These gains can be further amplified when combined with other
training tricks, including CutMix and Label Smoothing.

</details>


### [177] [PQ-DAF: Pose-driven Quality-controlled Data Augmentation for Data-scarce Driver Distraction Detection](https://arxiv.org/abs/2508.10397)
*Haibin Sun,Xinghui Song*

Main category: cs.CV

TL;DR: 提出PQ - DAF框架解决现有司机分心检测模型泛化能力差问题，实验证明其在少样本检测中提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有司机分心检测模型在现实场景泛化能力差，原因是数据标注成本高导致少样本学习挑战和训练与部署环境存在领域偏移。

Method: 提出PQ - DAF框架，用PCDMs捕捉司机姿态特征合成训练样本，用基于CogVLM的样本质量评估模块过滤低质量样本。

Result: PQ - DAF在少样本司机分心检测中显著提升性能，在数据稀缺条件下提高模型泛化能力。

Conclusion: PQ - DAF框架能有效解决现有司机分心检测模型的问题，提升少样本检测性能和模型泛化能力。

Abstract: Driver distraction detection is essential for improving traffic safety and
reducing road accidents. However, existing models often suffer from degraded
generalization when deployed in real-world scenarios. This limitation primarily
arises from the few-shot learning challenge caused by the high cost of data
annotation in practical environments, as well as the substantial domain shift
between training datasets and target deployment conditions. To address these
issues, we propose a Pose-driven Quality-controlled Data Augmentation Framework
(PQ-DAF) that leverages a vision-language model for sample filtering to
cost-effectively expand training data and enhance cross-domain robustness.
Specifically, we employ a Progressive Conditional Diffusion Model (PCDMs) to
accurately capture key driver pose features and synthesize diverse training
examples. A sample quality assessment module, built upon the CogVLM
vision-language model, is then introduced to filter out low-quality synthetic
samples based on a confidence threshold, ensuring the reliability of the
augmented dataset. Extensive experiments demonstrate that PQ-DAF substantially
improves performance in few-shot driver distraction detection, achieving
significant gains in model generalization under data-scarce conditions.

</details>


### [178] [DINOv3](https://arxiv.org/abs/2508.10104)
*Oriane Siméoni,Huy V. Vo,Maximilian Seitzer,Federico Baldassarre,Maxime Oquab,Cijo Jose,Vasil Khalidov,Marc Szafraniec,Seungeun Yi,Michaël Ramamonjisoa,Francisco Massa,Daniel Haziza,Luca Wehrstedt,Jianyuan Wang,Timothée Darcet,Théo Moutakanni,Leonel Sentana,Claire Roberts,Andrea Vedaldi,Jamie Tolan,John Brandt,Camille Couprie,Julien Mairal,Hervé Jégou,Patrick Labatut,Piotr Bojanowski*

Main category: cs.CV

TL;DR: 本文介绍自监督学习模型DINOv3，通过有效策略使其在多场景表现超现有模型，还分享了模型套件。


<details>
  <summary>Details</summary>
Motivation: 自监督学习无需手动标注数据，可处理大规模数据，DINOv3旨在实现这一愿景。

Method: 1. 精心准备数据、设计和优化以利用数据集和模型规模扩大的优势；2. 引入Gram anchoring方法解决训练中密集特征图退化问题；3. 应用事后策略提升模型灵活性。

Result: DINOv3在多场景表现超现有模型，产生高质量密集特征，在多种视觉任务上表现出色。

Conclusion: 分享DINOv3模型套件，为不同资源约束和部署场景提供可扩展解决方案，推动多任务和数据的技术发展。

Abstract: Self-supervised learning holds the promise of eliminating the need for manual
data annotation, enabling models to scale effortlessly to massive datasets and
larger architectures. By not being tailored to specific tasks or domains, this
training paradigm has the potential to learn visual representations from
diverse sources, ranging from natural to aerial images -- using a single
algorithm. This technical report introduces DINOv3, a major milestone toward
realizing this vision by leveraging simple yet effective strategies. First, we
leverage the benefit of scaling both dataset and model size by careful data
preparation, design, and optimization. Second, we introduce a new method called
Gram anchoring, which effectively addresses the known yet unsolved issue of
dense feature maps degrading during long training schedules. Finally, we apply
post-hoc strategies that further enhance our models' flexibility with respect
to resolution, model size, and alignment with text. As a result, we present a
versatile vision foundation model that outperforms the specialized state of the
art across a broad range of settings, without fine-tuning. DINOv3 produces
high-quality dense features that achieve outstanding performance on various
vision tasks, significantly surpassing previous self- and weakly-supervised
foundation models. We also share the DINOv3 suite of vision models, designed to
advance the state of the art on a wide spectrum of tasks and data by providing
scalable solutions for diverse resource constraints and deployment scenarios.

</details>


### [179] [Enhanced Sparse Point Cloud Data Processing for Privacy-aware Human Action Recognition](https://arxiv.org/abs/2508.10469)
*Maimunatu Tunau,Vincent Gbouna Zakka,Zhuangzhuang Dai*

Main category: cs.CV

TL;DR: 文章对毫米波雷达人体动作识别中三种数据处理方法进行详细性能分析，评估单独及组合效果，还提出改进措施并给出结果。


<details>
  <summary>Details</summary>
Motivation: 毫米波雷达用于人体动作识别有隐私优势但数据有挑战，现有文献缺乏对三种常用数据处理方法单独及组合的全面评估。

Method: 使用MiliPoint数据集对三种方法单独、两两组合及三者组合进行详细性能分析，评估识别准确率和计算成本，并对单个方法提出针对性改进。

Result: 得到各方法及其集成的优势和权衡方面的关键见解。

Conclusion: 研究结果为基于毫米波的人体动作识别系统的未来工作提供指导。

Abstract: Human Action Recognition (HAR) plays a crucial role in healthcare, fitness
tracking, and ambient assisted living technologies. While traditional vision
based HAR systems are effective, they pose privacy concerns. mmWave radar
sensors offer a privacy preserving alternative but present challenges due to
the sparse and noisy nature of their point cloud data. In the literature, three
primary data processing methods: Density-Based Spatial Clustering of
Applications with Noise (DBSCAN), the Hungarian Algorithm, and Kalman Filtering
have been widely used to improve the quality and continuity of radar data.
However, a comprehensive evaluation of these methods, both individually and in
combination, remains lacking. This paper addresses that gap by conducting a
detailed performance analysis of the three methods using the MiliPoint dataset.
We evaluate each method individually, all possible pairwise combinations, and
the combination of all three, assessing both recognition accuracy and
computational cost. Furthermore, we propose targeted enhancements to the
individual methods aimed at improving accuracy. Our results provide crucial
insights into the strengths and trade-offs of each method and their
integrations, guiding future work on mmWave based HAR systems

</details>


### [180] [Concepts or Skills? Rethinking Instruction Selection for Multi-modal Models](https://arxiv.org/abs/2508.10339)
*Andrew Bai,Justin Cui,Ruochen Wang,Cho-Jui Hsieh*

Main category: cs.CV

TL;DR: 本文发现视觉语言基准测试有不同受益倾向，设计目标训练数据选择方法优化性能，实验验证其有效性并强调指令选择需平衡概念知识与视觉技能。


<details>
  <summary>Details</summary>
Motivation: 发现视觉语言基准测试在训练时对相似技能或视觉概念指令有不同受益倾向，旨在优化给定基准测试的性能。

Method: 从基准测试中提取概念/技能，判断其主要受益类型，选择最匹配概念/技能的指令。

Result: 在10多个基准测试上验证了方法有效性，在所有基准测试上比现有最佳基线平均高0.9%，在技能聚焦子集上高1.5%。

Conclusion: 强调了指令选择中平衡概念知识获取和视觉技能的重要性。

Abstract: Vision-language instruction tuning achieves two main purposes: learning
visual concepts and learning visual skills. In this paper, we found that
vision-language benchmarks fall into the dichotomy of mainly benefiting from
training on instructions with similar skills or visual concepts. Inspired by
the discovery, we designed a simple targeted training data selection method to
optimize the performance of a given benchmark. We first extract the
concepts/skills from the benchmark, determine whether the benchmark
predominantly benefits from similar concepts or skills, and finally select
instructions with the most matching concepts/skills. Experiments on 10+
benchmarks validate the effectiveness of our targeted data selection method,
showing +0.9\% over the best existing baseline averaged over all benchmarks and
+1.5\% on the skill-focused subset. Our findings underscore the importance of
recognizing the inherent trade-off within instruction selection, which requires
balancing the acquisition of conceptual knowledge against visual skill.

</details>


### [181] [Multi-Sample Anti-Aliasing and Constrained Optimization for 3D Gaussian Splatting](https://arxiv.org/abs/2508.10507)
*Zheng Zhou,Jia-Chen Zhang,Yu-Jie Xiong,Chun-Ming Xia*

Main category: cs.CV

TL;DR: 提出集成MSAA与双几何约束的优化框架，提升细节重建与渲染效率。


<details>
  <summary>Details</summary>
Motivation: 3D高斯 splatting 在场景优化时几何约束不足，导致细粒度细节重建模糊。

Method: 提出综合优化框架，结合MSAA与双几何约束，通过自适应混合子样本计算像素颜色，采用自适应加权策略和梯度差分约束。

Result: 在多个基准测试中实现细节保存的最优性能，保持实时渲染效率，在结构相似性和感知质量上有显著提升。

Conclusion: 该方法能有效解决现有3D高斯 splatting 的问题，在细节保存和渲染效率上表现出色。

Abstract: Recent advances in 3D Gaussian splatting have significantly improved
real-time novel view synthesis, yet insufficient geometric constraints during
scene optimization often result in blurred reconstructions of fine-grained
details, particularly in regions with high-frequency textures and sharp
discontinuities. To address this, we propose a comprehensive optimization
framework integrating multisample anti-aliasing (MSAA) with dual geometric
constraints. Our system computes pixel colors through adaptive blending of
quadruple subsamples, effectively reducing aliasing artifacts in high-frequency
components. The framework introduces two constraints: (a) an adaptive weighting
strategy that prioritizes under-reconstructed regions through dynamic gradient
analysis, and (b) gradient differential constraints enforcing geometric
regularization at object boundaries. This targeted optimization enables the
model to allocate computational resources preferentially to critical regions
requiring refinement while maintaining global consistency. Extensive
experimental evaluations across multiple benchmarks demonstrate that our method
achieves state-of-the-art performance in detail preservation, particularly in
preserving high-frequency textures and sharp discontinuities, while maintaining
real-time rendering efficiency. Quantitative metrics and perceptual studies
confirm statistically significant improvements over baseline approaches in both
structural similarity (SSIM) and perceptual quality (LPIPS).

</details>


### [182] [Med-GLIP: Advancing Medical Language-Image Pre-training with Large-scale Grounded Dataset](https://arxiv.org/abs/2508.10528)
*Ziye Deng,Ruihan He,Jiaxiang Liu,Yuan Wang,Zijie Meng,Songtao Jiang,Yong Xie,Zuozhu Liu*

Main category: cs.CV

TL;DR: 本文构建大规模医疗数据集Med - GLIP - 5M，提出模态感知的医疗图像接地框架Med - GLIP，实验显示其性能优于基线，并能提升下游任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像接地研究存在模态覆盖有限、注释粗粒度和缺乏统一通用框架等问题。

Method: 构建含530多万区域级注释的大规模医疗数据集Med - GLIP - 5M，基于此提出模态感知的接地框架Med - GLIP，从多样数据中隐式获取分层语义理解。

Result: Med - GLIP在多个接地基准测试中始终优于现有基线，集成其空间输出到下游任务能带来显著性能提升。

Conclusion: 所提数据集和框架有效，数据集即将发布。

Abstract: Medical image grounding aims to align natural language phrases with specific
regions in medical images, serving as a foundational task for intelligent
diagnosis, visual question answering (VQA), and automated report generation
(MRG). However, existing research is constrained by limited modality coverage,
coarse-grained annotations, and the absence of a unified, generalizable
grounding framework. To address these challenges, we construct a large-scale
medical grounding dataset Med-GLIP-5M comprising over 5.3 million region-level
annotations across seven imaging modalities, covering diverse anatomical
structures and pathological findings. The dataset supports both segmentation
and grounding tasks with hierarchical region labels, ranging from organ-level
boundaries to fine-grained lesions. Based on this foundation, we propose
Med-GLIP, a modality-aware grounding framework trained on Med-GLIP-5M. Rather
than relying on explicitly designed expert modules, Med-GLIP implicitly
acquires hierarchical semantic understanding from diverse training data --
enabling it to recognize multi-granularity structures, such as distinguishing
lungs from pneumonia lesions. Extensive experiments demonstrate that Med-GLIP
consistently outperforms state-of-the-art baselines across multiple grounding
benchmarks. Furthermore, integrating its spatial outputs into downstream tasks,
including medical VQA and report generation, leads to substantial performance
gains. Our dataset will be released soon.

</details>


### [183] [Retrieval-Augmented Prompt for OOD Detection](https://arxiv.org/abs/2508.10556)
*Ruisong Han,Zongbo Han,Jiahao Zhang,Mingyue Cheng,Changqing Zhang*

Main category: cs.CV

TL;DR: 提出Retrieval - Augmented Prompt (RAP)方法用于OOD检测，在大规模基准测试中达SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有OOD检测方法依赖辅助离群样本或ID数据生成离群信息，因离群样本有限且与真实测试OOD样本不匹配，无法提供足够语义监督，导致性能不佳。

Method: 提出RAP方法，在训练时基于与外部文本知识的联合相似度检索离群点描述词来增强模型OOD提示，测试时根据遇到的OOD样本实时动态更新OOD提示。

Result: 在大规模OOD检测基准测试中达SOTA性能，如在ImageNet - 1k数据集的1 - shot OOD检测中，平均FPR95降低7.05%，AUROC提高1.71%。

Conclusion: 综合消融实验验证了各模块有效性和方法的潜在动机。

Abstract: Out-of-Distribution (OOD) detection is crucial for the reliable deployment of
machine learning models in-the-wild, enabling accurate identification of test
samples that differ from the training data distribution. Existing methods rely
on auxiliary outlier samples or in-distribution (ID) data to generate outlier
information for training, but due to limited outliers and their mismatch with
real test OOD samples, they often fail to provide sufficient semantic
supervision, leading to suboptimal performance. To address this, we propose a
novel OOD detection method called Retrieval-Augmented Prompt (RAP). RAP
augments a pre-trained vision-language model's prompts by retrieving external
knowledge, offering enhanced semantic supervision for OOD detection. During
training, RAP retrieves descriptive words for outliers based on joint
similarity with external textual knowledge and uses them to augment the model's
OOD prompts. During testing, RAP dynamically updates OOD prompts in real-time
based on the encountered OOD samples, enabling the model to rapidly adapt to
the test environment. Our extensive experiments demonstrate that RAP achieves
state-of-the-art performance on large-scale OOD detection benchmarks. For
example, in 1-shot OOD detection on the ImageNet-1k dataset, RAP reduces the
average FPR95 by 7.05% and improves the AUROC by 1.71% compared to previous
methods. Additionally, comprehensive ablation studies validate the
effectiveness of each module and the underlying motivations of our approach.

</details>


### [184] [PTQAT: A Hybrid Parameter-Efficient Quantization Algorithm for 3D Perception Tasks](https://arxiv.org/abs/2508.10557)
*Xinhao Wang,Zhiwei Lin,Zhongyu Xia,Yongtao Wang*

Main category: cs.CV

TL;DR: 提出PTQAT混合量化算法用于3D感知网络部署，比QAT更高效，实验显示性能优于仅用QAT的基线。


<details>
  <summary>Details</summary>
Motivation: PTQ会使量化模型性能下降，QAT有高GPU内存需求和长训练时间，需解决PTQ和QAT在速度与精度上的权衡问题。

Method: 选择关键层进行QAT微调，其余层进行PTQ，微调量化前后输出差异小的层。

Result: PTQAT冻结近50%可量化层，达到与QAT相似性能，支持不同量化位宽和模型架构，在多个3D感知任务中性能优于仅用QAT的基线。

Conclusion: PTQAT是高效通用的量化方法，能在微调更少权重时提升性能。

Abstract: Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT)
represent two mainstream model quantization approaches. However, PTQ often
leads to unacceptable performance degradation in quantized models, while QAT
imposes substantial GPU memory requirements and extended training time due to
weight fine-tuning.In this paper, we propose PTQAT, a novel general hybrid
quantization algorithm for the efficient deployment of 3D perception networks.
To address the speed accuracy trade-off between PTQ and QAT, our method selects
critical layers for QAT fine-tuning and performs PTQ on the remaining layers.
Contrary to intuition, fine-tuning the layers with smaller output discrepancies
before and after quantization, rather than those with larger discrepancies,
actually leads to greater improvements in the model's quantization accuracy.
This means we better compensate for quantization errors during their
propagation, rather than addressing them at the point where they occur. The
proposed PTQAT achieves similar performance to QAT with more efficiency by
freezing nearly 50% of quantifiable layers. Additionally, PTQAT is a universal
quantization method that supports various quantization bit widths (4 bits) as
well as different model architectures, including CNNs and Transformers. The
experimental results on nuScenes across diverse 3D perception tasks, including
object detection, semantic segmentation, and occupancy prediction, show that
our method consistently outperforms QAT-only baselines. Notably, it achieves
0.2%-0.9% NDS and 0.3%-1.0% mAP gains in object detection, 0.3%-2.0% mIoU gains
in semantic segmentation and occupancy prediction while fine-tuning fewer
weights.

</details>


### [185] [SkeySpot: Automating Service Key Detection for Digital Electrical Layout Plans in the Construction Industry](https://arxiv.org/abs/2508.10449)
*Dhruv Dosi,Rohit Meena,Param Rajpura,Yogesh Kumar Meena*

Main category: cs.CV

TL;DR: 文章介绍DELP数据集，用预训练目标检测模型评估，YOLOv8表现最佳，开发SkeySpot工具，使电气布局数字化更易实现。


<details>
  <summary>Details</summary>
Motivation: 传统扫描的楼层平面图缺乏机器可读格式，大规模解读耗时且易出错，自动化符号识别可解决此问题。

Method: 引入DELP数据集，用预训练目标检测模型评估，基于YOLOv8开发SkeySpot工具。

Result: YOLOv8在评估中mAP达82.5%最高，SkeySpot可实时检测、分类和量化电气符号。

Conclusion: 该方法降低对专有CAD系统依赖和手动注释工作，让建筑行业中小企业更易实现电气布局数字化，支持建筑环境标准化、互操作性和可持续性目标。

Abstract: Legacy floor plans, often preserved only as scanned documents, remain
essential resources for architecture, urban planning, and facility management
in the construction industry. However, the lack of machine-readable floor plans
render large-scale interpretation both time-consuming and error-prone.
Automated symbol spotting offers a scalable solution by enabling the
identification of service key symbols directly from floor plans, supporting
workflows such as cost estimation, infrastructure maintenance, and regulatory
compliance. This work introduces a labelled Digitised Electrical Layout Plans
(DELP) dataset comprising 45 scanned electrical layout plans annotated with
2,450 instances across 34 distinct service key classes. A systematic evaluation
framework is proposed using pretrained object detection models for DELP
dataset. Among the models benchmarked, YOLOv8 achieves the highest performance
with a mean Average Precision (mAP) of 82.5\%. Using YOLOv8, we develop
SkeySpot, a lightweight, open-source toolkit for real-time detection,
classification, and quantification of electrical symbols. SkeySpot produces
structured, standardised outputs that can be scaled up for interoperable
building information workflows, ultimately enabling compatibility across
downstream applications and regulatory platforms. By lowering dependency on
proprietary CAD systems and reducing manual annotation effort, this approach
makes the digitisation of electrical layouts more accessible to small and
medium-sized enterprises (SMEs) in the construction industry, while supporting
broader goals of standardisation, interoperability, and sustainability in the
built environment.

</details>


### [186] [SingleStrip: learning skull-stripping from a single labeled example](https://arxiv.org/abs/2508.10464)
*Bella Specktor-Fadida,Malte Hoffmann*

Main category: cs.CV

TL;DR: 本文结合领域随机化和自训练，用极少标注数据训练三维颅骨剥离网络，结果表明该策略有潜力实现有效半监督分割并减轻标注负担。


<details>
  <summary>Details</summary>
Motivation: 深度学习分割依赖标注数据，手动标注费力耗时，领域随机化在标注数据极少时解剖变异性有限，需解决标注稀缺问题。

Method: 结合领域随机化与自训练，先自动分箱体素强度合成图像训练初始模型，再用卷积自编码器评估无标注数据预测脑掩码质量，选择排名靠前的伪标签微调网络，并比较基于自编码器和基于一致性的排名方法。

Result: 在分布外数据上实现接近更多标注图像训练模型的颅骨剥离性能，基于自编码器的方法与分割精度相关性更强。

Conclusion: 结合领域随机化和基于自编码器的质量控制，能从极少标注数据实现有效半监督分割，减轻标注负担。

Abstract: Deep learning segmentation relies heavily on labeled data, but manual
labeling is laborious and time-consuming, especially for volumetric images such
as brain magnetic resonance imaging (MRI). While recent domain-randomization
techniques alleviate the dependency on labeled data by synthesizing diverse
training images from label maps, they offer limited anatomical variability when
very few label maps are available. Semi-supervised self-training addresses
label scarcity by iteratively incorporating model predictions into the training
set, enabling networks to learn from unlabeled data. In this work, we combine
domain randomization with self-training to train three-dimensional
skull-stripping networks using as little as a single labeled example. First, we
automatically bin voxel intensities, yielding labels we use to synthesize
images for training an initial skull-stripping model. Second, we train a
convolutional autoencoder (AE) on the labeled example and use its
reconstruction error to assess the quality of brain masks predicted for
unlabeled data. Third, we select the top-ranking pseudo-labels to fine-tune the
network, achieving skull-stripping performance on out-of-distribution data that
approaches models trained with more labeled images. We compare AE-based ranking
to consistency-based ranking under test-time augmentation, finding that the AE
approach yields a stronger correlation with segmentation accuracy. Our results
highlight the potential of combining domain randomization and AE-based quality
control to enable effective semi-supervised segmentation from extremely limited
labeled data. This strategy may ease the labeling burden that slows progress in
studies involving new anatomical structures or emerging imaging techniques.

</details>


### [187] [Fourier-Guided Attention Upsampling for Image Super-Resolution](https://arxiv.org/abs/2508.10616)
*Daejune Choi,Youchan No,Jinhyung Lee,Duksu Kim*

Main category: cs.CV

TL;DR: 提出轻量级上采样模块FGA用于单图像超分辨率，添加少量参数就能提升多种骨干网络性能，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统上采样器如Sub - Pixel Convolution难以重建高频细节且会引入混叠伪影。

Method: 整合基于傅里叶特征的MLP进行位置频率编码、跨分辨率相关注意力层进行自适应空间对齐、频域L1损失进行频谱保真监督。

Result: 添加0.3M参数，在多种超分辨率骨干网络中提升性能，PSNR平均增益0.12 - 0.14 dB，频域一致性提升达29%。

Conclusion: FGA是传统上采样方法实用且可扩展的替代方案，能减少混叠并保留细节。

Abstract: We propose Frequency-Guided Attention (FGA), a lightweight upsampling module
for single image super-resolution. Conventional upsamplers, such as Sub-Pixel
Convolution, are efficient but frequently fail to reconstruct high-frequency
details and introduce aliasing artifacts. FGA addresses these issues by
integrating (1) a Fourier feature-based Multi-Layer Perceptron (MLP) for
positional frequency encoding, (2) a cross-resolution Correlation Attention
Layer for adaptive spatial alignment, and (3) a frequency-domain L1 loss for
spectral fidelity supervision. Adding merely 0.3M parameters, FGA consistently
enhances performance across five diverse super-resolution backbones in both
lightweight and full-capacity scenarios. Experimental results demonstrate
average PSNR gains of 0.12~0.14 dB and improved frequency-domain consistency by
up to 29%, particularly evident on texture-rich datasets. Visual and spectral
evaluations confirm FGA's effectiveness in reducing aliasing and preserving
fine details, establishing it as a practical, scalable alternative to
traditional upsampling methods.

</details>


### [188] [Serial Over Parallel: Learning Continual Unification for Multi-Modal Visual Object Tracking and Benchmarking](https://arxiv.org/abs/2508.10655)
*Zhangyong Tang,Tianyang Xu,Xuefeng Zhu,Chunyang Cheng,Tao Zhou,Xiaojun Wu,Josef Kittler*

Main category: cs.CV

TL;DR: 本文针对多模态视觉目标跟踪任务统一化中训练与测试不一致及性能下降问题，提出统一基准UniBench300并将统一过程串行化，实验验证其有效性并给出性能下降相关结论。


<details>
  <summary>Details</summary>
Motivation: 现有多模态视觉目标跟踪任务统一化实践中，缺乏统一基准导致训练和测试不一致，引发性能下降。

Method: 引入统一基准UniBench300以减少推理次数和时间消耗；将统一过程以串行格式重新构建，结合持续学习理念。

Result: 实验证明UniBench300的重要性和持续学习在支持稳定统一过程中的优越性；发现性能下降与网络容量负相关，不同模态任务下降程度不同。

Conclusion: UniBench300和持续学习有助于解决多模态视觉目标跟踪任务统一化问题，研究结果为未来多模态视觉研究提供有价值的见解。

Abstract: Unifying multiple multi-modal visual object tracking (MMVOT) tasks draws
increasing attention due to the complementary nature of different modalities in
building robust tracking systems. Existing practices mix all data sensor types
in a single training procedure, structuring a parallel paradigm from the
data-centric perspective and aiming for a global optimum on the joint
distribution of the involved tasks. However, the absence of a unified benchmark
where all types of data coexist forces evaluations on separated benchmarks,
causing \textit{inconsistency} between training and testing, thus leading to
performance \textit{degradation}. To address these issues, this work advances
in two aspects: \ding{182} A unified benchmark, coined as UniBench300, is
introduced to bridge the inconsistency by incorporating multiple task data,
reducing inference passes from three to one and cutting time consumption by
27\%. \ding{183} The unification process is reformulated in a serial format,
progressively integrating new tasks. In this way, the performance degradation
can be specified as knowledge forgetting of previous tasks, which naturally
aligns with the philosophy of continual learning (CL), motivating further
exploration of injecting CL into the unification process. Extensive experiments
conducted on two baselines and four benchmarks demonstrate the significance of
UniBench300 and the superiority of CL in supporting a stable unification
process. Moreover, while conducting dedicated analyses, the performance
degradation is found to be negatively correlated with network capacity.
Additionally, modality discrepancies contribute to varying degradation levels
across tasks (RGBT > RGBD > RGBE in MMVOT), offering valuable insights for
future multi-modal vision research. Source codes and the proposed benchmark is
available at \textit{https://github.com/Zhangyong-Tang/UniBench300}.

</details>


### [189] [AddressVLM: Cross-view Alignment Tuning for Image Address Localization using Large Vision-Language Models](https://arxiv.org/abs/2508.10667)
*Shixiong Xu,Chenghao Zhang,Lubin Fan,Yuan Zhou,Bin Fan,Shiming Xiang,Gaofeng Meng,Jieping Ye*

Main category: cs.CV

TL;DR: 本文探索将城市级地址定位能力集成到大型视觉语言模型（LVLMs）中，提出AddressVLM模型，通过两阶段训练协议提升性能，在两个数据集上的平均地址定位准确率优于同类模型。


<details>
  <summary>Details</summary>
Motivation: LVLMs在细粒度街道级定位存在困难，本文旨在将城市级地址定位能力集成到LVLMs中，实现基于街景图像的灵活地址相关问答。

Method: 引入视角不变的卫星图像作为宏观线索，提出跨视图对齐调优（包括卫星视图和街景图像嫁接机制、自动标签生成机制），采用两阶段训练协议，构建两个街景VQA数据集。

Result: AddressVLM在两个数据集上的平均地址定位准确率分别比同类LVLMs高出9%和12%。

Conclusion: 所提出的方法有效提升了LVLMs在细粒度街道级地址定位的性能。

Abstract: Large visual language models (LVLMs) have demonstrated impressive performance
in coarse-grained geo-localization at the country or city level, but they
struggle with fine-grained street-level localization within urban areas. In
this paper, we explore integrating city-wide address localization capabilities
into LVLMs, facilitating flexible address-related question answering using
street-view images. A key challenge is that the street-view visual
question-and-answer (VQA) data provides only microscopic visual cues, leading
to subpar performance in fine-tuned models. To tackle this issue, we
incorporate perspective-invariant satellite images as macro cues and propose
cross-view alignment tuning including a satellite-view and street-view image
grafting mechanism, along with an automatic label generation mechanism. Then
LVLM's global understanding of street distribution is enhanced through
cross-view matching. Our proposed model, named AddressVLM, consists of
two-stage training protocols: cross-view alignment tuning and address
localization tuning. Furthermore, we have constructed two street-view VQA
datasets based on image address localization datasets from Pittsburgh and San
Francisco. Qualitative and quantitative evaluations demonstrate that AddressVLM
outperforms counterpart LVLMs by over 9% and 12% in average address
localization accuracy on these two datasets, respectively.

</details>


### [190] [Hybrid Generative Fusion for Efficient and Privacy-Preserving Face Recognition Dataset Generation](https://arxiv.org/abs/2508.10672)
*Feiran Li,Qianqian Xu,Shilong Bao,Boyu Han,Zhiyong Yang,Qingming Huang*

Main category: cs.CV

TL;DR: 本文介绍DataCV ICCV挑战赛方法，构建无身份重叠的高质量人脸数据集训练识别模型，获比赛第1名，提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 参加DataCV ICCV挑战赛，构建无身份重叠的高质量人脸数据集以训练人脸识别模型。

Method: 清理HSFace数据集，用Mixture - of - Experts策略识别和移除错误标签或不一致身份；保留最大一致身份簇并数据增强；用Stable Diffusion生成合成身份，用Vec2Face扩展；采用课程学习策略；检查新身份确保无泄漏。

Result: 方法获比赛第1名，数据集在10K、20K和100K身份规模上提升模型性能。

Conclusion: 所提出的构建人脸数据集的混合方法有效，能构建多样化高质量数据集，提升人脸识别模型性能。

Abstract: In this paper, we present our approach to the DataCV ICCV Challenge, which
centers on building a high-quality face dataset to train a face recognition
model. The constructed dataset must not contain identities overlapping with any
existing public face datasets. To handle this challenge, we begin with a
thorough cleaning of the baseline HSFace dataset, identifying and removing
mislabeled or inconsistent identities through a Mixture-of-Experts (MoE)
strategy combining face embedding clustering and GPT-4o-assisted verification.
We retain the largest consistent identity cluster and apply data augmentation
up to a fixed number of images per identity. To further diversify the dataset,
we generate synthetic identities using Stable Diffusion with prompt
engineering. As diffusion models are computationally intensive, we generate
only one reference image per identity and efficiently expand it using Vec2Face,
which rapidly produces 49 identity-consistent variants. This hybrid approach
fuses GAN-based and diffusion-based samples, enabling efficient construction of
a diverse and high-quality dataset. To address the high visual similarity among
synthetic identities, we adopt a curriculum learning strategy by placing them
early in the training schedule, allowing the model to progress from easier to
harder samples. Our final dataset contains 50 images per identity, and all
newly generated identities are checked with mainstream face datasets to ensure
no identity leakage. Our method achieves \textbf{1st place} in the competition,
and experimental results show that our dataset improves model performance
across 10K, 20K, and 100K identity scales. Code is available at
https://github.com/Ferry-Li/datacv_fr.

</details>


### [191] [Lightweight CNNs for Embedded SAR Ship Target Detection and Classification](https://arxiv.org/abs/2508.10712)
*Fabian Kresse,Georgios Pilikos,Mario Azcueta,Nicolas Floury*

Main category: cs.CV

TL;DR: 本文提出用于处理Sentinel - 1非聚焦SAR数据的神经网络，验证其星载处理可行性及目标分类可能性。


<details>
  <summary>Details</summary>
Motivation: 现有SAR数据近实时监测受数据下传、聚焦和分析流程限制，传统算法受卫星资源约束，需新方法减少下传数据量、降低延迟。

Method: 提出用于Sentinel - 1条带和干涉宽幅模式非聚焦SAR数据实时推理的神经网络。

Result: 验证其中一个模型可用于星载处理并部署在FPGA上，且能进行目标分类。

Conclusion: 所提神经网络在SAR数据处理和目标分类方面具有可行性。

Abstract: Synthetic Aperture Radar (SAR) data enables large-scale surveillance of
maritime vessels. However, near-real-time monitoring is currently constrained
by the need to downlink all raw data, perform image focusing, and subsequently
analyze it on the ground. On-board processing to generate higher-level products
could reduce the data volume that needs to be downlinked, alleviating bandwidth
constraints and minimizing latency. However, traditional image focusing and
processing algorithms face challenges due to the satellite's limited memory,
processing power, and computational resources. This work proposes and evaluates
neural networks designed for real-time inference on unfocused SAR data acquired
in Stripmap and Interferometric Wide (IW) modes captured with Sentinel-1. Our
results demonstrate the feasibility of using one of our models for on-board
processing and deployment on an FPGA. Additionally, by investigating a binary
classification task between ships and windmills, we demonstrate that target
classification is possible.

</details>


### [192] [Dissecting Generalized Category Discovery: Multiplex Consensus under Self-Deconstruction](https://arxiv.org/abs/2508.10731)
*Luyao Tang,Kunze Huang,Chaoqi Chen,Yuxuan Yuan,Chenxin Li,Xiaotong Tu,Xinghao Ding,Yue Huang*

Main category: cs.CV

TL;DR: 本文提出ConGCD方法解决广义类别发现问题，通过高维语义重建等操作建立面向基元的表征，在多个基准测试中证明有效。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习框架在跨类别对象归纳和识别能力上远不及人类感知系统，现有广义类别发现（GCD）方法主要关注优化目标函数，需新解决方案。

Method: 受人类认知过程启发，将对象分解为视觉基元并进行跨知识比较，提出ConGCD方法，建立面向基元的表征，实现主导和上下文共识单元，用共识调度器优化激活路径，通过多重共识集成得出最终预测。

Result: 在粗粒度和细粒度基准测试中证明了ConGCD作为一种共识感知范式的有效性。

Conclusion: ConGCD是解决广义类别发现问题的有效共识感知范式。

Abstract: Human perceptual systems excel at inducing and recognizing objects across
both known and novel categories, a capability far beyond current machine
learning frameworks. While generalized category discovery (GCD) aims to bridge
this gap, existing methods predominantly focus on optimizing objective
functions. We present an orthogonal solution, inspired by the human cognitive
process for novel object understanding: decomposing objects into visual
primitives and establishing cross-knowledge comparisons. We propose ConGCD,
which establishes primitive-oriented representations through high-level
semantic reconstruction, binding intra-class shared attributes via
deconstruction. Mirroring human preference diversity in visual processing,
where distinct individuals leverage dominant or contextual cues, we implement
dominant and contextual consensus units to capture class-discriminative
patterns and inherent distributional invariants, respectively. A consensus
scheduler dynamically optimizes activation pathways, with final predictions
emerging through multiplex consensus integration. Extensive evaluations across
coarse- and fine-grained benchmarks demonstrate ConGCD's effectiveness as a
consensus-aware paradigm. Code is available at github.com/lytang63/ConGCD.

</details>


### [193] [EgoCross: Benchmarking Multimodal Large Language Models for Cross-Domain Egocentric Video Question Answering](https://arxiv.org/abs/2508.10729)
*Yanjun Li,Yuqian Fu,Tianwen Qian,Qi'ao Xu,Silong Dai,Danda Pani Paudel,Luc Van Gool,Xiaoling Wang*

Main category: cs.CV

TL;DR: 提出EgoCross基准评估多模态大语言模型在以自我为中心视频问答中的跨领域泛化能力，实验显示现有模型泛化能力有限，并进行改进探索。


<details>
  <summary>Details</summary>
Motivation: 现有以自我为中心视频问答基准和研究局限于日常活动，而实际部署存在领域转移问题，需评估模型跨领域泛化能力。

Method: 引入EgoCross基准，涵盖四个领域，约1000个问答对，有OpenQA和CloseQA格式，进行广泛实验及探索性研究。

Result: 多数现有多模态大语言模型难以泛化到日常生活之外的领域。

Conclusion: EgoCross及相关分析可作为推进领域自适应、鲁棒的以自我为中心视频理解的基础。

Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have
significantly pushed the frontier of egocentric video question answering
(EgocentricQA). However, existing benchmarks and studies are mainly limited to
common daily activities such as cooking and cleaning. In contrast, real-world
deployment inevitably encounters domain shifts, where target domains differ
substantially in both visual style and semantic content. To bridge this gap, we
introduce \textbf{EgoCross}, a comprehensive benchmark designed to evaluate the
cross-domain generalization of MLLMs in EgocentricQA. EgoCross covers four
diverse and challenging domains, including surgery, industry, extreme sports,
and animal perspective, representing realistic and high-impact application
scenarios. It comprises approximately 1,000 QA pairs across 798 video clips,
spanning four key QA tasks: prediction, recognition, localization, and
counting. Each QA pair provides both OpenQA and CloseQA formats to support
fine-grained evaluation. Extensive experiments show that most existing MLLMs,
whether general-purpose or egocentric-specialized, struggle to generalize to
domains beyond daily life, highlighting the limitations of current models.
Furthermore, we conduct several pilot studies, \eg, fine-tuning and
reinforcement learning, to explore potential improvements. We hope EgoCross and
our accompanying analysis will serve as a foundation for advancing
domain-adaptive, robust egocentric video understanding. Data and codes will be
released at:
\href{https://github.com/MyUniverse0726/EgoCross}{https://github.com/MyUniverse0726/EgoCross.}

</details>


### [194] [Video-BLADE: Block-Sparse Attention Meets Step Distillation for Efficient Video Generation](https://arxiv.org/abs/2508.10774)
*Youping Gu,Xiaolong Li,Yuhao Hu,Bohan Zhuang*

Main category: cs.CV

TL;DR: 提出BLADE框架加速扩散变压器视频生成，在多模型验证有显著效率提升和质量改进。


<details>
  <summary>Details</summary>
Motivation: 扩散变压器在视频生成中推理存在瓶颈，现有加速策略结合有挑战。

Method: 提出BLADE框架，含自适应块稀疏注意力机制和基于轨迹分布匹配的稀疏感知步骤蒸馏范式。

Result: 在CogVideoX - 5B和Wan2.1 - 1.3B等模型验证，有显著推理加速和质量提升。

Conclusion: BLADE框架有效解决现有问题，提升视频生成效率和质量。

Abstract: Diffusion transformers currently lead the field in high-quality video
generation, but their slow iterative denoising process and prohibitive
quadratic attention costs for long sequences create significant inference
bottlenecks. While both step distillation and sparse attention mechanisms have
shown promise as independent acceleration strategies, effectively combining
these approaches presents critical challenges -- training-free integration
yields suboptimal results, while separately training sparse attention after
step distillation requires prohibitively expensive high-quality video data. To
overcome these limitations, we propose BLADE, an innovative data-free joint
training framework that introduces: (1) an Adaptive Block-Sparse Attention
(ASA) mechanism for dynamically generating content-aware sparsity masks to
focus computation on salient spatiotemporal features, and (2) a sparsity-aware
step distillation paradigm built upon Trajectory Distribution Matching (TDM)
that directly incorporates sparsity into the distillation process rather than
treating it as a separate compression step, with fast convergence. We validate
BLADE on text-to-video models like CogVideoX-5B and Wan2.1-1.3B. Our framework
demonstrates remarkable efficiency gains across different scales. On
Wan2.1-1.3B, BLADE achieves a 14.10x end-to-end inference acceleration over a
50-step baseline. Moreover, on models such as CogVideoX-5B with short video
sequence lengths, our framework delivers a robust 8.89x speedup. Crucially, the
acceleration is accompanied by a consistent quality improvement. On the
VBench-2.0 benchmark, BLADE boosts the score of CogVideoX-5B to 0.569 (from
0.534) and Wan2.1-1.3B to 0.570 (from 0.563), results that are further
corroborated by superior ratings in human evaluations. Our code and model
weights are publicly available at: http://ziplab.co/BLADE-Homepage/.

</details>


### [195] [Mobile-Friendly Deep Learning for Plant Disease Detection: A Lightweight CNN Benchmark Across 101 Classes of 33 Crops](https://arxiv.org/abs/2508.10817)
*Anand Kumar,Harminder Pal Monga,Tapasi Brahma,Satyam Kalra,Navas Sherif*

Main category: cs.CV

TL;DR: 本文开发了可准确分类33种作物101种病害的移动友好解决方案，评估多种轻量级架构，EfficientNet - B1表现最佳，适合在移动设备部署。


<details>
  <summary>Details</summary>
Motivation: 植物病害威胁全球粮食安全，需开发能准确检测的早期检测系统。

Method: 结合Plant Doc、PlantVillage和PlantWild数据集构建综合数据集，评估MobileNetV2、MobileNetV3等轻量级架构。

Result: EfficientNet - B1性能最佳，分类准确率达94.7%。

Conclusion: EfficientNet - B1在准确性和计算效率间达到最佳平衡，适合在移动设备上进行实际部署。

Abstract: Plant diseases are a major threat to food security globally. It is important
to develop early detection systems which can accurately detect. The advancement
in computer vision techniques has the potential to solve this challenge. We
have developed a mobile-friendly solution which can accurately classify 101
plant diseases across 33 crops. We built a comprehensive dataset by combining
different datasets, Plant Doc, PlantVillage, and PlantWild, all of which are
for the same purpose. We evaluated performance across several lightweight
architectures - MobileNetV2, MobileNetV3, MobileNetV3-Large, and
EfficientNet-B0, B1 - specifically chosen for their efficiency on
resource-constrained devices. The results were promising, with EfficientNet-B1
delivering our best performance at 94.7% classification accuracy. This
architecture struck an optimal balance between accuracy and computational
efficiency, making it well-suited for real-world deployment on mobile devices.

</details>


### [196] [AEGIS: Authenticity Evaluation Benchmark for AI-Generated Video Sequences](https://arxiv.org/abs/2508.10771)
*Jieyu Li,Xin Zhang,Joey Tianyi Zhou*

Main category: cs.CV

TL;DR: 文章介绍了新基准AEGIS用于检测AI生成视频真实性，含大量精心挑选视频及多模态注释，实验显示现有模型检测能力有限，AEGIS推动相关研究发展。


<details>
  <summary>Details</summary>
Motivation: 现有视频真实性检测基准存在现实性、规模和复杂性不足，无法有效评估模型对抗复杂伪造的能力。

Method: 引入AEGIS基准，包含超10000个真实和合成视频，由多种先进生成模型生成，有挑战性子集和多模态注释。

Result: 使用先进视觉语言模型的实验表明，在AEGIS最具挑战性子集上检测能力有限。

Conclusion: AEGIS是不可或缺的评估基准，推动开发能应对现实伪造威胁的可靠视频真实性检测方法。

Abstract: Recent advances in AI-generated content have fueled the rise of highly
realistic synthetic videos, posing severe risks to societal trust and digital
integrity. Existing benchmarks for video authenticity detection typically
suffer from limited realism, insufficient scale, and inadequate complexity,
failing to effectively evaluate modern vision-language models against
sophisticated forgeries. To address this critical gap, we introduce AEGIS, a
novel large-scale benchmark explicitly targeting the detection of
hyper-realistic and semantically nuanced AI-generated videos. AEGIS comprises
over 10,000 rigorously curated real and synthetic videos generated by diverse,
state-of-the-art generative models, including Stable Video Diffusion,
CogVideoX-5B, KLing, and Sora, encompassing open-source and proprietary
architectures. In particular, AEGIS features specially constructed challenging
subsets enhanced with robustness evaluation. Furthermore, we provide multimodal
annotations spanning Semantic-Authenticity Descriptions, Motion Features, and
Low-level Visual Features, facilitating authenticity detection and supporting
downstream tasks such as multimodal fusion and forgery localization. Extensive
experiments using advanced vision-language models demonstrate limited detection
capabilities on the most challenging subsets of AEGIS, highlighting the
dataset's unique complexity and realism beyond the current generalization
capabilities of existing models. In essence, AEGIS establishes an indispensable
evaluation benchmark, fundamentally advancing research toward developing
genuinely robust, reliable, broadly generalizable video authenticity detection
methodologies capable of addressing real-world forgery threats. Our dataset is
available on https://huggingface.co/datasets/Clarifiedfish/AEGIS.

</details>


### [197] [Ultra-High-Definition Reference-Based Landmark Image Super-Resolution with Generative Diffusion Prior](https://arxiv.org/abs/2508.10779)
*Zhenning Shi,Zizheng Yan,Yuhang Yu,Clara Xue,Jingyu Zhuang,Qi Zhang,Jinwei Chen,Tao Li,Qingnan Fan*

Main category: cs.CV

TL;DR: 提出TriFlowSR框架和Landmark - 4K数据集用于参考图像超分辨率，实验表明比之前方法能更好利用参考图像信息


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的参考图像超分辨率方法难以有效对齐低分辨率图像和参考高分辨率图像信息，且当前数据集分辨率和质量有限

Method: 提出TriFlowSR框架实现模式匹配，引入Landmark - 4K数据集，设计参考匹配策略

Result: 相比之前方法能更好利用参考高分辨率图像的语义和纹理信息

Conclusion: 提出首个基于扩散的超高清地标场景参考图像超分辨率管道

Abstract: Reference-based Image Super-Resolution (RefSR) aims to restore a
low-resolution (LR) image by utilizing the semantic and texture information
from an additional reference high-resolution (reference HR) image. Existing
diffusion-based RefSR methods are typically built upon ControlNet, which
struggles to effectively align the information between the LR image and the
reference HR image. Moreover, current RefSR datasets suffer from limited
resolution and poor image quality, resulting in the reference images lacking
sufficient fine-grained details to support high-quality restoration. To
overcome the limitations above, we propose TriFlowSR, a novel framework that
explicitly achieves pattern matching between the LR image and the reference HR
image. Meanwhile, we introduce Landmark-4K, the first RefSR dataset for
Ultra-High-Definition (UHD) landmark scenarios. Considering the UHD scenarios
with real-world degradation, in TriFlowSR, we design a Reference Matching
Strategy to effectively match the LR image with the reference HR image.
Experimental results show that our approach can better utilize the semantic and
texture information of the reference HR image compared to previous methods. To
the best of our knowledge, we propose the first diffusion-based RefSR pipeline
for ultra-high definition landmark scenarios under real-world degradation. Our
code and model will be available at https://github.com/nkicsl/TriFlowSR.

</details>


### [198] [Performance of GPT-5 in Brain Tumor MRI Reasoning](https://arxiv.org/abs/2508.10865)
*Mojtaba Safari,Shansong Wang,Mingzhe Hu,Zach Eidex,Qiang Li,Xiaofeng Yang*

Main category: cs.CV

TL;DR: 研究评估了GPT-4o、GPT-5-nano、GPT-5-mini和GPT-5在脑肿瘤VQA基准上的表现，GPT-5-mini准确率最高，但模型整体表现未达临床可用水平。


<details>
  <summary>Details</summary>
Motivation: 准确区分脑肿瘤类型对神经肿瘤治疗规划至关重要，评估大语言模型在脑肿瘤VQA任务中的表现。

Method: 在源自3个脑肿瘤分割数据集的脑肿瘤VQA基准上评估模型，在零样本思维链设置下评估视觉和推理任务的准确性。

Result: GPT-5-mini宏观平均准确率最高（44.19%），各模型在不同肿瘤亚型上表现有差异，无单一模型在所有队列中占主导。

Conclusion: GPT-5系列模型在结构化神经肿瘤VQA任务中可达到中等准确率，但未达临床可用水平。

Abstract: Accurate differentiation of brain tumor types on magnetic resonance imaging
(MRI) is critical for guiding treatment planning in neuro-oncology. Recent
advances in large language models (LLMs) have enabled visual question answering
(VQA) approaches that integrate image interpretation with natural language
reasoning. In this study, we evaluated GPT-4o, GPT-5-nano, GPT-5-mini, and
GPT-5 on a curated brain tumor VQA benchmark derived from 3 Brain Tumor
Segmentation (BraTS) datasets - glioblastoma (GLI), meningioma (MEN), and brain
metastases (MET). Each case included multi-sequence MRI triplanar mosaics and
structured clinical features transformed into standardized VQA items. Models
were assessed in a zero-shot chain-of-thought setting for accuracy on both
visual and reasoning tasks. Results showed that GPT-5-mini achieved the highest
macro-average accuracy (44.19%), followed by GPT-5 (43.71%), GPT-4o (41.49%),
and GPT-5-nano (35.85%). Performance varied by tumor subtype, with no single
model dominating across all cohorts. These findings suggest that GPT-5 family
models can achieve moderate accuracy in structured neuro-oncological VQA tasks,
but not at a level acceptable for clinical use.

</details>


### [199] [Medico 2025: Visual Question Answering for Gastrointestinal Imaging](https://arxiv.org/abs/2508.10869)
*Sushant Gautam,Vajira Thambawita,Michael Riegler,Pål Halvorsen,Steven Hicks*

Main category: cs.CV

TL;DR: 介绍Medico 2025挑战，聚焦胃肠道影像VQA，含两个子任务，用Kvasir - VQA - x1数据集，结合指标与评估推动医疗影像AI发展。


<details>
  <summary>Details</summary>
Motivation: 开发可解释AI模型，基于胃肠道内窥镜图像回答临床相关问题并给出符合医学推理的解释，推动医疗影像分析中可信AI的发展。

Method: 设置两个子任务，使用Kvasir - VQA - x1数据集，结合定量性能指标和专家评审的可解释性评估。

Result: 未提及具体结果

Conclusion: 该挑战有望推动医疗影像分析中可信AI的发展

Abstract: The Medico 2025 challenge addresses Visual Question Answering (VQA) for
Gastrointestinal (GI) imaging, organized as part of the MediaEval task series.
The challenge focuses on developing Explainable Artificial Intelligence (XAI)
models that answer clinically relevant questions based on GI endoscopy images
while providing interpretable justifications aligned with medical reasoning. It
introduces two subtasks: (1) answering diverse types of visual questions using
the Kvasir-VQA-x1 dataset, and (2) generating multimodal explanations to
support clinical decision-making. The Kvasir-VQA-x1 dataset, created from 6,500
images and 159,549 complex question-answer (QA) pairs, serves as the benchmark
for the challenge. By combining quantitative performance metrics and
expert-reviewed explainability assessments, this task aims to advance
trustworthy Artificial Intelligence (AI) in medical image analysis.
Instructions, data access, and an updated guide for participation are available
in the official competition repository:
https://github.com/simula/MediaEval-Medico-2025

</details>


### [200] [ToonComposer: Streamlining Cartoon Production with Generative Post-Keyframing](https://arxiv.org/abs/2508.10881)
*Lingen Li,Guangzhi Wang,Zhaoyang Zhang,Yaowei Li,Xiaoyu Li,Qi Dou,Jinwei Gu,Tianfan Xue,Ying Shan*

Main category: cs.CV

TL;DR: 本文提出ToonComposer模型，统一中间帧生成与上色阶段，使用稀疏草图注入和卡通适配方法，创建PKBench基准测试，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统动漫制作需大量手动工作，现有AI方法分开处理各阶段，导致误差积累和瑕疵，且在处理大动作和上色时存在问题。

Method: 引入ToonComposer模型，采用稀疏草图注入机制和卡通适配方法，创建PKBench基准测试。

Result: ToonComposer在视觉质量、运动一致性和生产效率上优于现有方法。

Conclusion: ToonComposer为AI辅助动漫制作提供了更优、更灵活的解决方案，能减少人工工作量。

Abstract: Traditional cartoon and anime production involves keyframing, inbetweening,
and colorization stages, which require intensive manual effort. Despite recent
advances in AI, existing methods often handle these stages separately, leading
to error accumulation and artifacts. For instance, inbetweening approaches
struggle with large motions, while colorization methods require dense per-frame
sketches. To address this, we introduce ToonComposer, a generative model that
unifies inbetweening and colorization into a single post-keyframing stage.
ToonComposer employs a sparse sketch injection mechanism to provide precise
control using keyframe sketches. Additionally, it uses a cartoon adaptation
method with the spatial low-rank adapter to tailor a modern video foundation
model to the cartoon domain while keeping its temporal prior intact. Requiring
as few as a single sketch and a colored reference frame, ToonComposer excels
with sparse inputs, while also supporting multiple sketches at any temporal
location for more precise motion control. This dual capability reduces manual
workload and improves flexibility, empowering artists in real-world scenarios.
To evaluate our model, we further created PKBench, a benchmark featuring
human-drawn sketches that simulate real-world use cases. Our evaluation
demonstrates that ToonComposer outperforms existing methods in visual quality,
motion consistency, and production efficiency, offering a superior and more
flexible solution for AI-assisted cartoon production.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [201] [Legal Zero-Days: A Novel Risk Vector for Advanced AI Systems](https://arxiv.org/abs/2508.10050)
*Greg Sadler,Nathan Sherburn*

Main category: cs.CY

TL;DR: 本文提出“法律零日漏洞”概念，构建风险模型识别评估其漏洞，以澳洲危机为例说明法律疏漏影响，开发评估方法，指出当前AI难发现但未来可能具备该能力。


<details>
  <summary>Details</summary>
Motivation: 识别和评估先进AI系统中法律框架存在的新型风险，为应对前沿AI系统风险做出贡献。

Method: 提出风险模型识别评估法律漏洞；以2017年澳大利亚双重国籍危机为案例；开发创建“法律谜题”的评估方法。

Result: 当前AI模型不太可能可靠地发现有影响的法律零日漏洞，但未来系统可能具备此能力。

Conclusion: 该研究有助于识别和缓解前沿AI系统中先前未被认识的风险。

Abstract: We introduce the concept of "Legal Zero-Days" as a novel risk vector for
advanced AI systems. Legal Zero-Days are previously undiscovered
vulnerabilities in legal frameworks that, when exploited, can cause immediate
and significant societal disruption without requiring litigation or other
processes before impact. We present a risk model for identifying and evaluating
these vulnerabilities, demonstrating their potential to bypass safeguards or
impede government responses to AI incidents. Using the 2017 Australian dual
citizenship crisis as a case study, we illustrate how seemingly minor legal
oversights can lead to large-scale governance disruption. We develop a
methodology for creating "legal puzzles" as evaluation instruments for
assessing AI systems' capabilities to discover such vulnerabilities. Our
findings suggest that while current AI models may not reliably find impactful
Legal Zero-Days, future systems may develop this capability, presenting both
risks and opportunities for improving legal robustness. This work contributes
to the broader effort to identify and mitigate previously unrecognized risks
from frontier AI systems.

</details>


### [202] [Advancing Data Equity: Practitioner Responsibility and Accountability in NLP Data Practices](https://arxiv.org/abs/2508.10071)
*Jay L. Cunningham,Kevin Zhongyang Shao,Rock Yuren Pang,Nathaniel Mengist*

Main category: cs.CY

TL;DR: 研究聚焦美国NLP数据从业者对数据公平性的看法，揭示商业目标与公平承诺的矛盾，提出需结构治理改革。


<details>
  <summary>Details</summary>
Motivation: 了解NLP从业者对NLP数据公平性问题的看法和应对方式。

Method: 通过2024年的问卷调查和焦点小组，研究美国NLP数据从业者对公平性的概念、应对约束的方式和参与治理的情况。

Result: 发现商业目标与公平承诺存在持续矛盾，从业者呼吁更具参与性和问责性的工作流程。

Conclusion: 改善NLP公平性需要支持从业者能动性和社区同意的结构治理改革。

Abstract: While research has focused on surfacing and auditing algorithmic bias to
ensure equitable AI development, less is known about how NLP practitioners -
those directly involved in dataset development, annotation, and deployment -
perceive and navigate issues of NLP data equity. This study is among the first
to center practitioners' perspectives, linking their experiences to a
multi-scalar AI governance framework and advancing participatory
recommendations that bridge technical, policy, and community domains. Drawing
on a 2024 questionnaire and focus group, we examine how U.S.-based NLP data
practitioners conceptualize fairness, contend with organizational and systemic
constraints, and engage emerging governance efforts such as the U.S. AI Bill of
Rights. Findings reveal persistent tensions between commercial objectives and
equity commitments, alongside calls for more participatory and accountable data
workflows. We critically engage debates on data diversity and diversity
washing, arguing that improving NLP equity requires structural governance
reforms that support practitioner agency and community consent.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [203] [zERExtractor:An Automated Platform for Enzyme-Catalyzed Reaction Data Extraction from Scientific Literature](https://arxiv.org/abs/2508.09995)
*Rui Zhou,Haohui Ma,Tianle Xin,Lixin Zou,Qiuyue Hu,Hongxi Cheng,Mingzhi Lin,Jingjing Guo,Sheng Wang,Guoqing Zhang,Yanjie Wei,Liangzhen Zheng*

Main category: q-bio.BM

TL;DR: 提出自动化可扩展平台zERExtractor从文献提取酶催化反应和活性数据，优于现有基线，填补酶动力学数据缺口。


<details>
  <summary>Details</summary>
Motivation: 酶动力学文献增长超数据库整理能力，阻碍AI建模和知识发现。

Method: 采用统一模块化架构，结合领域适应深度学习、OCR、实体识别、LLM模块及人工修正，通过主动学习策略适应新数据源。

Result: zERExtractor在表格识别、分子图像解释和关系提取上优于现有基线。

Conclusion: zERExtractor以灵活框架和高保真提取填补酶动力学数据缺口，为未来AI酶建模和生化知识发现奠定基础。

Abstract: The rapid expansion of enzyme kinetics literature has outpaced the curation
capabilities of major biochemical databases, creating a substantial barrier to
AI-driven modeling and knowledge discovery. We present zERExtractor, an
automated and extensible platform for comprehensive extraction of
enzyme-catalyzed reaction and activity data from scientific literature.
zERExtractor features a unified, modular architecture that supports
plug-and-play integration of state-of-the-art models, including large language
models (LLMs), as interchangeable components, enabling continuous system
evolution alongside advances in AI. Our pipeline combines domain-adapted deep
learning, advanced OCR, semantic entity recognition, and prompt-driven LLM
modules, together with human expert corrections, to extract kinetic parameters
(e.g., kcat, Km), enzyme sequences, substrate SMILES, experimental conditions,
and molecular diagrams from heterogeneous document formats. Through active
learning strategies integrating AI-assisted annotation, expert validation, and
iterative refinement, the system adapts rapidly to new data sources. We also
release a large benchmark dataset comprising over 1,000 annotated tables and
5,000 biological fields from 270 P450-related enzymology publications.
Benchmarking demonstrates that zERExtractor consistently outperforms existing
baselines in table recognition (Acc 89.9%), molecular image interpretation (up
to 99.1%), and relation extraction (accuracy 94.2%). zERExtractor bridges the
longstanding data gap in enzyme kinetics with a flexible, plugin-ready
framework and high-fidelity extraction, laying the groundwork for future
AI-powered enzyme modeling and biochemical knowledge discovery.

</details>


### [204] [FROGENT: An End-to-End Full-process Drug Design Agent](https://arxiv.org/abs/2508.10760)
*Qihua Pan,Dong Xu,Jenna Xinyi Yao,Lijia Ma,Zexuan Zhu,Junkai Ji*

Main category: q-bio.BM

TL;DR: 提出FROGENT框架解决药物发现AI工具碎片化问题，经评估表现优异，证明可提升研究效率。


<details>
  <summary>Details</summary>
Motivation: 现有药物发现AI工具分散，科学家管理不便，需解决碎片化问题。

Method: 利用大语言模型和模型上下文协议，整合数据库、工具库和AI模型，动态执行复杂药物发现工作流。

Result: 在八个基准测试中表现出色，相比基线和其他模型有显著优势，经实际案例验证实用性和泛化性。

Conclusion: 简化代理药物发现流程可显著提高研究人员生产力。

Abstract: Powerful AI tools for drug discovery reside in isolated web apps, desktop
programs, and code libraries. Such fragmentation forces scientists to manage
incompatible interfaces and specialized scripts, which can be a cumbersome and
repetitive process. To address this issue, a Full-pROcess druG dEsign ageNT,
named FROGENT, has been proposed. Specifically, FROGENT utilizes a Large
Language Model and the Model Context Protocol to integrate multiple dynamic
biochemical databases, extensible tool libraries, and task-specific AI models.
This agentic framework allows FROGENT to execute complicated drug discovery
workflows dynamically, including component tasks such as target identification,
molecule generation and retrosynthetic planning. FROGENT has been evaluated on
eight benchmarks that cover various aspects of drug discovery, such as
knowledge retrieval, property prediction, virtual screening, mechanistic
analysis, molecular design, and synthesis. It was compared against six
increasingly advanced ReAct-style agents that support code execution and
literature searches. Empirical results demonstrated that FROGENT triples the
best baseline performance in hit-finding and doubles it in interaction
profiling, significantly outperforming both the open-source model Qwen3-32B and
the commercial model GPT-4o. In addition, real-world cases have been utilized
to validate the practicability and generalization of FROGENT. This development
suggests that streamlining the agentic drug discovery pipeline can
significantly enhance researcher productivity.

</details>


<div id='math.DS'></div>

# math.DS [[Back]](#toc)

### [205] [Memorisation and forgetting in a learning Hopfield neural network: bifurcation mechanisms, attractors and basins](https://arxiv.org/abs/2508.10765)
*Adam E. Essex,Natalia B. Janson,Rachel A. Norris,Alexander G. Balanov*

Main category: math.DS

TL;DR: 分析81神经元Hopfield网络Hebbian学习中记忆形成机制，揭示分岔现象，表明记忆和遗忘是同一机制，策略具通用性。


<details>
  <summary>Details</summary>
Motivation: 人工神经网络作为黑盒，其学习中记忆形成、不良特征发展不明，全面分析有挑战，且两种知识范式未明确关联。

Method: 全面分析81神经元Hopfield网络在Hebbian学习中记忆形成机制，揭示分岔现象。

Result: 应用刺激引发分岔，形成新吸引子编码记忆，出现灾难性遗忘，记忆和遗忘是同一机制。

Conclusion: 分析策略通用，所揭示机制有助于理解和改进循环神经网络。

Abstract: Despite explosive expansion of artificial intelligence based on artificial
neural networks (ANNs), these are employed as "black boxes'', as it is unclear
how, during learning, they form memories or develop unwanted features,
including spurious memories and catastrophic forgetting. Much research is
available on isolated aspects of learning ANNs, but due to their high
dimensionality and non-linearity, their comprehensive analysis remains a
challenge. In ANNs, knowledge is thought to reside in connection weights or in
attractor basins, but these two paradigms are not linked explicitly. Here we
comprehensively analyse mechanisms of memory formation in an 81-neuron Hopfield
network undergoing Hebbian learning by revealing bifurcations leading to
formation and destruction of attractors and their basin boundaries. We show
that, by affecting evolution of connection weights, the applied stimuli induce
a pitchfork and then a cascade of saddle-node bifurcations creating new
attractors with their basins that can code true or spurious memories, and an
abrupt disappearance of old memories (catastrophic forgetting). With successful
learning, new categories are represented by the basins of newly born point
attractors, and their boundaries by the stable manifolds of new saddles. With
this, memorisation and forgetting represent two manifestations of the same
mechanism. Our strategy to analyse high-dimensional learning ANNs is universal
and applicable to recurrent ANNs of any form. The demonstrated mechanisms of
memory formation and of catastrophic forgetting shed light on the operation of
a wider class of recurrent ANNs and could aid the development of approaches to
mitigate their flaws.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [206] [Nonlinear filtering based on density approximation and deep BSDE prediction](https://arxiv.org/abs/2508.10630)
*Kasper Bågmark,Adam Andersson,Stig Larsson*

Main category: math.NA

TL;DR: 介绍基于倒向随机微分方程的近似贝叶斯滤波器，用特定方法近似滤波密度，离线训练可在线应用，证明误差界并通过数值例子验证收敛率。


<details>
  <summary>Details</summary>
Motivation: 引入新的近似贝叶斯滤波器。

Method: 利用滤波问题的非线性Feynman - Kac表示，用深BSDE方法和神经网络近似未归一化滤波密度，离线训练。

Result: 证明了椭圆条件下的混合先验 - 后验误差界，两个数值例子证实理论收敛率。

Conclusion: 该基于倒向随机微分方程的近似贝叶斯滤波器方法有效，具有理论收敛性。

Abstract: A novel approximate Bayesian filter based on backward stochastic differential
equations is introduced. It uses a nonlinear Feynman--Kac representation of the
filtering problem and the approximation of an unnormalized filtering density
using the well-known deep BSDE method and neural networks. The method is
trained offline, which means that it can be applied online with new
observations. A mixed a priori-a posteriori error bound is proved under an
elliptic condition. The theoretical convergence rate is confirmed in two
numerical examples.

</details>


<div id='q-bio.OT'></div>

# q-bio.OT [[Back]](#toc)

### [207] [A Guide to Bayesian Optimization in Bioprocess Engineering](https://arxiv.org/abs/2508.10642)
*Maximilian Siska,Emma Pajak,Katrin Rosenthal,Antonio del Rio Chanona,Eric von Lieres,Laura Marie Helleckes*

Main category: q-bio.OT

TL;DR: 介绍贝叶斯优化在生物过程工程中的应用，回顾旨在提供易懂介绍并指出未来研究机会。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯优化在生物过程工程应用尚处初期，生物实验复杂需对经典方法扩展，且现有文献对从业者不友好。

Method: 进行相关内容的综述。

Result: 无明确具体结果。

Conclusion: 为从业者提供贝叶斯优化的直观实用介绍，指出有前景的应用领域和算法挑战。

Abstract: Bayesian optimization has become widely popular across various experimental
sciences due to its favorable attributes: it can handle noisy data, perform
well with relatively small datasets, and provide adaptive suggestions for
sequential experimentation. While still in its infancy, Bayesian optimization
has recently gained traction in bioprocess engineering. However,
experimentation with biological systems is highly complex and the resulting
experimental uncertainty requires specific extensions to classical Bayesian
optimization. Moreover, current literature often targets readers with a strong
statistical background, limiting its accessibility for practitioners.
  In light of these developments, this review has two aims: first, to provide
an intuitive and practical introduction to Bayesian optimization; and second,
to outline promising application areas and open algorithmic challenges, thereby
highlighting opportunities for future research in machine learning.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [208] [AnalogSeeker: An Open-source Foundation Language Model for Analog Circuit Design](https://arxiv.org/abs/2508.10409)
*Zihao Chen,Ji Zhuang,Jinyi Shen,Xiaoyue Ke,Xinyi Yang,Mingjie Zhou,Zhuoyao Du,Xu Yan,Zhouyang Wu,Zhenyu Xu,Jiangli Huang,Li Shang,Xuan Zeng,Fan Yang*

Main category: cs.AR

TL;DR: 本文提出用于模拟电路设计的开源基础语言模型AnalogSeeker，介绍数据收集、知识蒸馏、训练方法，模型在评估基准表现佳且在下游任务有效，已开源。


<details>
  <summary>Details</summary>
Motivation: 构建用于模拟电路设计的开源基础语言模型，整合领域知识并提供设计辅助，解决该领域数据稀缺、知识复杂和训练挑战等问题。

Method: 采用基于模拟电路领域知识框架的语料收集策略；引入粒度化领域知识蒸馏方法；建立以微调为中心的训练范式，实现邻域自约束监督微调算法。

Result: 训练Qwen2.5 - 32B - Instruct模型得到AnalogSeeker，在AMSBench - TQA上准确率达85.04%，比原模型提高15.67个百分点，在下游运算放大器设计任务有效。

Conclusion: AnalogSeeker在模拟电路知识评估和下游设计任务表现良好，为模拟电路设计提供了有效的开源模型。

Abstract: In this paper, we propose AnalogSeeker, an effort toward an open-source
foundation language model for analog circuit design, with the aim of
integrating domain knowledge and giving design assistance. To overcome the
scarcity of data in this field, we employ a corpus collection strategy based on
the domain knowledge framework of analog circuits. High-quality, accessible
textbooks across relevant subfields are systematically curated and cleaned into
a textual domain corpus. To address the complexity of knowledge of analog
circuits, we introduce a granular domain knowledge distillation method. Raw,
unlabeled domain corpus is decomposed into typical, granular learning nodes,
where a multi-agent framework distills implicit knowledge embedded in
unstructured text into question-answer data pairs with detailed reasoning
processes, yielding a fine-grained, learnable dataset for fine-tuning. To
address the unexplored challenges in training analog circuit foundation models,
we explore and share our training methods through both theoretical analysis and
experimental validation. We finally establish a fine-tuning-centric training
paradigm, customizing and implementing a neighborhood self-constrained
supervised fine-tuning algorithm. This approach enhances training outcomes by
constraining the perturbation magnitude between the model's output
distributions before and after training. In practice, we train the
Qwen2.5-32B-Instruct model to obtain AnalogSeeker, which achieves 85.04%
accuracy on AMSBench-TQA, the analog circuit knowledge evaluation benchmark,
with a 15.67% point improvement over the original model and is competitive with
mainstream commercial models. Furthermore, AnalogSeeker also shows
effectiveness in the downstream operational amplifier design task. AnalogSeeker
is open-sourced at https://huggingface.co/analogllm/analogseeker for research
use.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [209] [AlDBaran: Towards Blazingly Fast State Commitments for Blockchains](https://arxiv.org/abs/2508.10493)
*Bernhard Kauer,Aleksandr Petrosyan,Benjamin Livshits*

Main category: cs.CR

TL;DR: AlDBaran是认证数据库的重大进步，消除关键路径磁盘I/O等优化，处理能力强、性能优、简单，适用于资源受限环境。


<details>
  <summary>Details</summary>
Motivation: 现有认证数据库方法难以满足高TPS系统严格要求，需开发新方案。

Method: 消除关键路径的磁盘I/O操作、实施预取策略、改进Merkle树更新机制。

Result: 能在50 Gbps网络吞吐量下高效处理状态更新，支持历史状态证明，相比其他项目性能优越、简单，在不同配置下更新速度快。

Conclusion: AlDBaran是高吞吐量区块链生成状态承诺的有效解决方案，适用于资源受限环境，具有成本效益和可扩展性。

Abstract: The fundamental basis for maintaining integrity within contemporary
blockchain systems is provided by authenticated databases. Our analysis
indicates that a significant portion of the approaches applied in this domain
fail to sufficiently meet the stringent requirements of systems processing
transactions at rates of multi-million TPS. AlDBaran signifies a substantial
advancement in authenticated databases. By eliminating disk I/O operations from
the critical path, implementing prefetching strategies, and refining the update
mechanism of the Merkle tree, we have engineered an authenticated data
structure capable of handling state updates efficiently at a network throughput
of 50 Gbps. This throughput capacity significantly surpasses any empirically
documented blockchain throughput, guaranteeing the ability of even the most
high-throughput blockchains to generate state commitments effectively.
  AlDBaran provides support for historical state proofs, which facilitates a
wide array of novel applications. For instance, the deployment of AlDBaran
could enable blockchains that do not currently support state commitments to
offer functionalities for light clients and/or implement rollups.
  When benchmarked against alternative authenticated data structure projects,
AlDBaran exhibits superior performance and simplicity. In particular, AlDBaran
achieves speeds of approximately 48 million updates per second using an
identical machine configuration. This characteristic renders AlDBaran an
attractive solution for resource-limited environments, as its historical data
capabilities can be modularly isolated (and deactivated), which further
enhances performance. On consumer-level portable hardware, it achieves
approximately 8 million updates/s in an in-memory setting and 5 million
updates/s with snapshots at sub-second intervals, illustrating compelling and
cost-effective scalability.

</details>


### [210] [A Robust Pipeline for Differentially Private Federated Learning on Imbalanced Clinical Data using SMOTETomek and FedProx](https://arxiv.org/abs/2508.10017)
*Rodrigo Tertulino*

Main category: cs.CR

TL;DR: 研究解决联邦学习（FL）结合差分隐私（DP）在医疗数据中隐私与临床效用权衡及数据不平衡问题，提出实用方法并找到隐私 - 效用最优区域。


<details>
  <summary>Details</summary>
Motivation: FL结合DP在医疗研究中有隐私保障，但存在隐私与临床效用权衡问题，且医疗数据有严重类别不平衡。

Method: 实施FL框架用于心血管风险预测，在客户端集成SMOTETomek处理不平衡数据，用调优的FedProx算法优化非IID数据。

Result: 发现隐私预算（epsilon）和模型召回率之间存在非线性权衡，优化的FedProx优于标准FedAvg，找到隐私 - 效用最优区域。

Conclusion: 研究为创建有效、安全、准确的诊断工具提供实用方法蓝图，可应用于真实异构医疗数据。

Abstract: Federated Learning (FL) presents a groundbreaking approach for collaborative
health research, allowing model training on decentralized data while
safeguarding patient privacy. FL offers formal security guarantees when
combined with Differential Privacy (DP). The integration of these technologies,
however, introduces a significant trade-off between privacy and clinical
utility, a challenge further complicated by the severe class imbalance often
present in medical datasets. The research presented herein addresses these
interconnected issues through a systematic, multi-stage analysis. An FL
framework was implemented for cardiovascular risk prediction, where initial
experiments showed that standard methods struggled with imbalanced data,
resulting in a recall of zero. To overcome such a limitation, we first
integrated the hybrid Synthetic Minority Over-sampling Technique with Tomek
Links (SMOTETomek) at the client level, successfully developing a clinically
useful model. Subsequently, the framework was optimized for non-IID data using
a tuned FedProx algorithm. Our final results reveal a clear, non-linear
trade-off between the privacy budget (epsilon) and model recall, with the
optimized FedProx consistently out-performing standard FedAvg. An optimal
operational region was identified on the privacy-utility frontier, where strong
privacy guarantees (with epsilon 9.0) can be achieved while maintaining high
clinical utility (recall greater than 77%). Ultimately, our study provides a
practical methodological blueprint for creating effective, secure, and accurate
diagnostic tools that can be applied to real-world, heterogeneous healthcare
data.

</details>


### [211] [Context Misleads LLMs: The Role of Context Filtering in Maintaining Safe Alignment of LLMs](https://arxiv.org/abs/2508.10031)
*Jinhwa Kim,Ian G. Harris*

Main category: cs.CR

TL;DR: 本文提出上下文过滤模型防御大语言模型越狱攻击，能降低攻击成功率且保持模型性能，是即插即用方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽性能提升，但越狱攻击带来安全和伦理风险，需防御机制保障安全同时不影响性能。

Method: 提出上下文过滤模型，作为输入预处理方法过滤不可信上下文、识别主要提示。

Result: 模型将越狱攻击成功率降低达88%，保持原模型性能，实现了最先进的安全与有用性乘积结果。

Conclusion: 模型是即插即用方法，可用于所有大语言模型增强安全性，无需微调，将公开用于研究。

Abstract: While Large Language Models (LLMs) have shown significant advancements in
performance, various jailbreak attacks have posed growing safety and ethical
risks. Malicious users often exploit adversarial context to deceive LLMs,
prompting them to generate responses to harmful queries. In this study, we
propose a new defense mechanism called Context Filtering model, an input
pre-processing method designed to filter out untrustworthy and unreliable
context while identifying the primary prompts containing the real user intent
to uncover concealed malicious intent. Given that enhancing the safety of LLMs
often compromises their helpfulness, potentially affecting the experience of
benign users, our method aims to improve the safety of the LLMs while
preserving their original performance. We evaluate the effectiveness of our
model in defending against jailbreak attacks through comparative analysis,
comparing our approach with state-of-the-art defense mechanisms against six
different attacks and assessing the helpfulness of LLMs under these defenses.
Our model demonstrates its ability to reduce the Attack Success Rates of
jailbreak attacks by up to 88% while maintaining the original LLMs'
performance, achieving state-of-the-art Safety and Helpfulness Product results.
Notably, our model is a plug-and-play method that can be applied to all LLMs,
including both white-box and black-box models, to enhance their safety without
requiring any fine-tuning of the models themselves. We will make our model
publicly available for research purposes.

</details>


### [212] [Cognitive Cybersecurity for Artificial Intelligence: Guardrail Engineering with CCS-7](https://arxiv.org/abs/2508.10033)
*Yuksel Aydin*

Main category: cs.CR

TL;DR: 研究语言模型认知漏洞，提出CCS - 7分类法，对比人类和语言模型在TFVA防护下表现，指出认知安全是模型特定工程问题。


<details>
  <summary>Details</summary>
Motivation: 语言模型存在类似人类的认知漏洞，且传统行为对齐无法解决。

Method: 提出CCS - 7分类法，对151名参与者进行随机对照试验建立人类基准，对七种不同语言模型架构进行12180次实验评估TFVA式防护栏。

Result: 不同模型架构有不同风险模式，部分漏洞几乎完全缓解，部分漏洞出现回退，人类有适度稳定提升。

Conclusion: 认知安全是模型特定工程问题，部署前需进行架构感知的认知安全测试。

Abstract: Language models exhibit human-like cognitive vulnerabilities, such as
emotional framing, that escape traditional behavioral alignment. We present
CCS-7 (Cognitive Cybersecurity Suite), a taxonomy of seven vulnerabilities
grounded in human cognitive security research. To establish a human benchmark,
we ran a randomized controlled trial with 151 participants: a "Think First,
Verify Always" (TFVA) lesson improved cognitive security by +7.9% overall. We
then evaluated TFVA-style guardrails across 12,180 experiments on seven diverse
language model architectures. Results reveal architecture-dependent risk
patterns: some vulnerabilities (e.g., identity confusion) are almost fully
mitigated, while others (e.g., source interference) exhibit escalating
backfire, with error rates increasing by up to 135% in certain models. Humans,
in contrast, show consistent moderate improvement. These findings reframe
cognitive safety as a model-specific engineering problem: interventions
effective in one architecture may fail, or actively harm, another, underscoring
the need for architecture-aware cognitive safety testing before deployment.

</details>


### [213] [Certifiably robust malware detectors by design](https://arxiv.org/abs/2508.10038)
*Pierre-Francois Gimenez,Sarath Sivaprasad,Mario Fritz*

Main category: cs.CR

TL;DR: 提出新模型架构用于可证明的鲁棒恶意软件检测，展示鲁棒检测器结构并基于此构建ERDALT框架，与机器学习方法对比验证，实现鲁棒检测且检测性能损失有限。


<details>
  <summary>Details</summary>
Motivation: 现有静态恶意软件分析的机器学习技术易被对抗样本规避，创建不改变功能的恶意软件对抗样本需特定转换，需要更鲁棒的检测方法。

Method: 提出新模型架构，展示鲁棒检测器的特定结构，构建ERDALT框架，并与机器学习方法对比验证。

Result: 可实现鲁棒检测，且检测性能仅有有限降低。

Conclusion: 所提方法能在有限降低检测性能的情况下，实现恶意软件的鲁棒检测。

Abstract: Malware analysis involves analyzing suspicious software to detect malicious
payloads. Static malware analysis, which does not require software execution,
relies increasingly on machine learning techniques to achieve scalability.
Although such techniques obtain very high detection accuracy, they can be
easily evaded with adversarial examples where a few modifications of the sample
can dupe the detector without modifying the behavior of the software. Unlike
other domains, such as computer vision, creating an adversarial example of
malware without altering its functionality requires specific transformations.
We propose a new model architecture for certifiably robust malware detection by
design. In addition, we show that every robust detector can be decomposed into
a specific structure, which can be applied to learn empirically robust malware
detectors, even on fragile features. Our framework ERDALT is based on this
structure. We compare and validate these approaches with machine-learning-based
malware detection methods, allowing for robust detection with limited reduction
of detection performance.

</details>


### [214] [Multi-task Adversarial Attacks against Black-box Model with Few-shot Queries](https://arxiv.org/abs/2508.10039)
*Wenqiang Wang,Yan Xiao,Hao Lin,Yangshijie Zhang,Xiaochun Cao*

Main category: cs.CR

TL;DR: 提出CEMA黑盒攻击方法，利用对抗文本可迁移性，简化多任务场景，实验证明其多任务攻击有效且适用于商业API和大模型。


<details>
  <summary>Details</summary>
Motivation: 现有的多任务对抗文本攻击在黑盒反馈API、有限查询或多任务类型的实际场景中效果不佳，需改进。

Method: 使用即插即用方式训练的深度替代模型，将多任务攻击转化为分类攻击，用不同文本分类方法生成多个对抗候选并选择最有效攻击替代模型的一个。

Result: 在涉及2、3或6个任务的多任务模型实验中，仅100次查询就有显著攻击成功率，能针对商业API、大语言模型和图像生成模型。

Conclusion: CEMA方法有效且具备通用性，适用于现实世界应用。

Abstract: Current multi-task adversarial text attacks rely on abundant access to shared
internal features and numerous queries, often limited to a single task type. As
a result, these attacks are less effective against practical scenarios
involving black-box feedback APIs, limited queries, or multiple task types. To
bridge this gap, we propose \textbf{C}luster and \textbf{E}nsemble
\textbf{M}ulti-task Text Adversarial \textbf{A}ttack (\textbf{CEMA}), an
effective black-box attack that exploits the transferability of adversarial
texts across different tasks. CEMA simplifies complex multi-task scenarios by
using a \textit{deep-level substitute model} trained in a
\textit{plug-and-play} manner for text classification, enabling attacks without
mimicking the victim model. This approach requires only a few queries for
training, converting multi-task attacks into classification attacks and
allowing attacks across various tasks.
  CEMA generates multiple adversarial candidates using different text
classification methods and selects the one that most effectively attacks
substitute models.
  In experiments involving multi-task models with two, three, or six
tasks--spanning classification, translation, summarization, and text-to-image
generation--CEMA demonstrates significant attack success with as few as 100
queries. Furthermore, CEMA can target commercial APIs (e.g., Baidu and Google
Translate), large language models (e.g., ChatGPT 4o), and image-generation
models (e.g., Stable Diffusion V2), showcasing its versatility and
effectiveness in real-world applications.

</details>


### [215] [FIDELIS: Blockchain-Enabled Protection Against Poisoning Attacks in Federated Learning](https://arxiv.org/abs/2508.10042)
*Jane Carney,Kushal Upreti,Gaby G. Dagher,Tim Andersen*

Main category: cs.CR

TL;DR: 提出区块链支持的联邦学习中毒检测框架Sys，具抗攻击和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习易受数据投毒攻击，现有检测方法缺乏标准或过度依赖信任。

Method: 去中心化全局服务器角色，引入判断模型检测数据投毒，各客户端生成并验证判断模型达成共识。

Result: 实现方案表明Sys能抵御数据投毒攻击，判断模型创建可扩展。

Conclusion: 所提出的Sys框架在联邦学习数据投毒检测方面有效且可扩展。

Abstract: Federated learning enhances traditional deep learning by enabling the joint
training of a model with the use of IoT device's private data. It ensures
privacy for clients, but is susceptible to data poisoning attacks during
training that degrade model performance and integrity. Current poisoning
detection methods in federated learning lack a standardized detection method or
take significant liberties with trust. In this paper, we present \Sys, a novel
blockchain-enabled poison detection framework in federated learning. The
framework decentralizes the role of the global server across participating
clients. We introduce a judge model used to detect data poisoning in model
updates. The judge model is produced by each client and verified to reach
consensus on a single judge model. We implement our solution to show \Sys is
robust against data poisoning attacks and the creation of our judge model is
scalable.

</details>


### [216] [Securing Agentic AI: Threat Modeling and Risk Analysis for Network Monitoring Agentic AI System](https://arxiv.org/abs/2508.10043)
*Pallavi Zambare,Venkata Nikhil Thanikella,Ying Liu*

Main category: cs.CR

TL;DR: 研究结合大语言模型与自主代理在网络监控和决策系统中的安全问题，用MAESTRO框架评估并消除漏洞，证实两个威胁案例，提出多层防御建议，验证MAESTRO可行性。


<details>
  <summary>Details</summary>
Motivation: 解决结合大语言模型与自主代理在网络监控和决策系统中产生的安全问题。

Method: 使用MAESTRO框架，构建并实现原型代理系统，包含推理、内存等模块。

Result: 证实两个威胁案例，导致性能下降，如遥测更新延迟、计算负载增加。

Conclusion: MAESTRO在操作威胁映射等方面可行，强调内存完整性等保障代理AI可靠性。

Abstract: When combining Large Language Models (LLMs) with autonomous agents, used in
network monitoring and decision-making systems, this will create serious
security issues. In this research, the MAESTRO framework consisting of the
seven layers threat modeling architecture in the system was used to expose,
evaluate, and eliminate vulnerabilities of agentic AI. The prototype agent
system was constructed and implemented, using Python, LangChain, and telemetry
in WebSockets, and deployed with inference, memory, parameter tuning, and
anomaly detection modules. Two practical threat cases were confirmed as
follows: (i) resource denial of service by traffic replay denial-of-service,
and (ii) memory poisoning by tampering with the historical log file maintained
by the agent. These situations resulted in measurable levels of performance
degradation, i.e. telemetry updates were delayed, and computational loads were
increased, as a result of poor system adaptations. It was suggested to use a
multilayered defense-in-depth approach with memory isolation, validation of
planners and anomaly response systems in real-time. These findings verify that
MAESTRO is viable in operational threat mapping, prospective risk scoring, and
the basis of the resilient system design. The authors bring attention to the
importance of the enforcement of memory integrity, paying attention to the
adaptation logic monitoring, and cross-layer communication protection that
guarantee the agentic AI reliability in adversarial settings.

</details>


### [217] [Generative AI for Cybersecurity of Energy Management Systems: Methods, Challenges, and Future Directions](https://arxiv.org/abs/2508.10044)
*Aydin Zaboli,Junho Hong*

Main category: cs.CR

TL;DR: 本文提出能源管理系统（EMS）安全框架，含多點攻擊/錯誤模型、基於生成式AI的異常檢測系統和SoM - GI框架，經IEEE 14 - 節點系統驗證有效。


<details>
  <summary>Details</summary>
Motivation: 解決能源管理系統面臨的網絡安全漏洞和系統問題的動態環境挑戰。

Method: 提出多點攻擊/錯誤模型識別漏洞；首次提出基於生成式AI的異常檢測系統；建議SoM - GI框架，結合視覺標記和規則進行多模態分析。

Result: 在IEEE 14 - 節點系統上驗證框架在各場景下有效，視覺分析能識別不一致之處。

Conclusion: 集成方法結合數值分析、視覺模式識別和語言規則，可防禦網絡威脅和系統錯誤。

Abstract: This paper elaborates on an extensive security framework specifically
designed for energy management systems (EMSs), which effectively tackles the
dynamic environment of cybersecurity vulnerabilities and/or system problems
(SPs), accomplished through the incorporation of novel methodologies. A
comprehensive multi-point attack/error model is initially proposed to
systematically identify vulnerabilities throughout the entire EMS data
processing pipeline, including post state estimation (SE) stealth attacks, EMS
database manipulation, and human-machine interface (HMI) display corruption
according to the real-time database (RTDB) storage. This framework acknowledges
the interconnected nature of modern attack vectors, which utilize various
phases of supervisory control and data acquisition (SCADA) data flow. Then,
generative AI (GenAI)-based anomaly detection systems (ADSs) for EMSs are
proposed for the first time in the power system domain to handle the scenarios.
Further, a set-of-mark generative intelligence (SoM-GI) framework, which
leverages multimodal analysis by integrating visual markers with rules
considering the GenAI capabilities, is suggested to overcome inherent spatial
reasoning limitations. The SoM-GI methodology employs systematic visual
indicators to enable accurate interpretation of segmented HMI displays and
detect visual anomalies that numerical methods fail to identify. Validation on
the IEEE 14-Bus system shows the framework's effectiveness across scenarios,
while visual analysis identifies inconsistencies. This integrated approach
combines numerical analysis with visual pattern recognition and linguistic
rules to protect against cyber threats and system errors.

</details>


### [218] [NetMoniAI: An Agentic AI Framework for Network Security & Monitoring](https://arxiv.org/abs/2508.10052)
*Pallavi Zambare,Venkata Nikhil Thanikella,Nikhil Padmanabh Kottur,Sree Akhil Akula,Ying Liu*

Main category: cs.CR

TL;DR: 提出NetMoniAI框架用于网络监控和安全，经评估有良好性能且开源。


<details>
  <summary>Details</summary>
Motivation: 实现自动网络监控和安全，解决网络攻击检测和系统态势感知问题。

Method: 构建包含本地微代理和中央控制器两层的NetMoniAI框架，在本地微测试台和NS - 3模拟中评估。

Result: 两层代理AI设计在资源约束下可扩展，减少冗余，提高响应时间且不影响准确性。

Conclusion: NetMoniAI框架性能良好，开源便于在不同网络环境和威胁场景中复制、验证和扩展。

Abstract: In this paper, we present NetMoniAI, an agentic AI framework for automatic
network monitoring and security that integrates decentralized analysis with
lightweight centralized coordination. The framework consists of two layers:
autonomous micro-agents at each node perform local traffic analysis and anomaly
detection. A central controller then aggregates insights across nodes to detect
coordinated attacks and maintain system-wide situational awareness. We
evaluated NetMoniAI on a local micro-testbed and through NS-3 simulations.
Results confirm that the two-tier agentic-AI design scales under resource
constraints, reduces redundancy, and improves response time without
compromising accuracy. To facilitate broader adoption and reproducibility, the
complete framework is available as open source. This enables researchers and
practitioners to replicate, validate, and extend it across diverse network
environments and threat scenarios. Github link:
https://github.com/pzambare3/NetMoniAI

</details>


### [219] [Neural Network-Based Detection and Multi-Class Classification of FDI Attacks in Smart Grid Home Energy Systems](https://arxiv.org/abs/2508.10035)
*Varsha Sen,Biswash Basnet*

Main category: cs.CR

TL;DR: 本文提出基于机器学习框架检测和分类智能电网中虚假数据注入攻击，实验证明有效，可增强电网边缘弹性和网络安全。


<details>
  <summary>Details</summary>
Motivation: 虚假数据注入攻击对智能电网尤其是家庭区域网络构成威胁，现有安全控制不足，攻击者借此操纵需求模式、破坏电网运行，损害智能电表数据完整性。

Method: 使用轻量级人工神经网络（ANN）基于能耗、成本和时间上下文的关键特征进行实时检测；训练双向LSTM通过学习数据中的序列依赖关系对不同攻击类型（正常、梯形和S形攻击形状）进行分类；生成合成时间序列数据集模拟家庭行为。

Result: 实验结果表明所提出的模型能有效识别和分类虚假数据注入攻击。

Conclusion: 该工作有助于构建智能、数据驱动的防御机制，从住宅端点加强智能电网网络安全。

Abstract: False Data Injection Attacks (FDIAs) pose a significant threat to smart grid
infrastructures, particularly Home Area Networks (HANs), where real-time
monitoring and control are highly adopted. Owing to the comparatively less
stringent security controls and widespread availability of HANs, attackers view
them as an attractive entry point to manipulate aggregated demand patterns,
which can ultimately propagate and disrupt broader grid operations. These
attacks undermine the integrity of smart meter data, enabling malicious actors
to manipulate consumption values without activating conventional alarms,
thereby creating serious vulnerabilities across both residential and
utility-scale infrastructures. This paper presents a machine learning-based
framework for both the detection and classification of FDIAs using residential
energy data. A real-time detection is provided by the lightweight Artificial
Neural Network (ANN), which works by using the most vital features of energy
consumption, cost, and time context. For the classification of different attack
types, a Bidirectional LSTM is trained to recognize normal, trapezoidal, and
sigmoid attack shapes through learning sequential dependencies in the data. A
synthetic time-series dataset was generated to emulate realistic household
behaviour. Experimental results demonstrate that the proposed models are
effective in identifying and classifying FDIAs, offering a scalable solution
for enhancing grid resilience at the edge. This work contributes toward
building intelligent, data-driven defence mechanisms that strengthen smart grid
cybersecurity from residential endpoints.

</details>


### [220] [Advancing Autonomous Incident Response: Leveraging LLMs and Cyber Threat Intelligence](https://arxiv.org/abs/2508.10677)
*Amine Tellache,Abdelaziz Amara Korba,Amdjed Mokhtari,Horea Moldovan,Yacine Ghamri-Doudane*

Main category: cs.CR

TL;DR: 本文提出基于RAG框架结合LLM自动化并增强事件响应，经实证验证有效，凸显LLM驱动CTI融合潜力。


<details>
  <summary>Details</summary>
Motivation: 安全团队面临警报疲劳、高误报率和大量非结构化CTI文档问题，手动分析CTI耗时费力，需自动化方法。

Method: 引入基于RAG的框架，采用混合检索机制结合NLP相似搜索和外部CTI平台查询，用LLM生成响应策略，提出双重评估范式。

Result: 在真实和模拟警报上的实证验证表明，该方法提高了事件响应的准确性、上下文关联和效率，减轻了分析师工作量，减少了响应延迟。

Conclusion: LLM驱动的CTI融合有潜力推动自主安全运营，为智能自适应的网络安全框架奠定基础。

Abstract: Effective incident response (IR) is critical for mitigating cyber threats,
yet security teams are overwhelmed by alert fatigue, high false-positive rates,
and the vast volume of unstructured Cyber Threat Intelligence (CTI) documents.
While CTI holds immense potential for enriching security operations, its
extensive and fragmented nature makes manual analysis time-consuming and
resource-intensive. To bridge this gap, we introduce a novel
Retrieval-Augmented Generation (RAG)-based framework that leverages Large
Language Models (LLMs) to automate and enhance IR by integrating dynamically
retrieved CTI. Our approach introduces a hybrid retrieval mechanism that
combines NLP-based similarity searches within a CTI vector database with
standardized queries to external CTI platforms, facilitating context-aware
enrichment of security alerts. The augmented intelligence is then leveraged by
an LLM-powered response generation module, which formulates precise,
actionable, and contextually relevant incident mitigation strategies. We
propose a dual evaluation paradigm, wherein automated assessment using an
auxiliary LLM is systematically cross-validated by cybersecurity experts.
Empirical validation on real-world and simulated alerts demonstrates that our
approach enhances the accuracy, contextualization, and efficiency of IR,
alleviating analyst workload and reducing response latency. This work
underscores the potential of LLM-driven CTI fusion in advancing autonomous
security operations and establishing a foundation for intelligent, adaptive
cybersecurity frameworks.

</details>


### [221] [Searching for Privacy Risks in LLM Agents via Simulation](https://arxiv.org/abs/2508.10880)
*Yanzhe Zhang,Diyi Yang*

Main category: cs.CR

TL;DR: 本文提出搜索框架模拟隐私关键的代理交互，发现攻击和防御策略会演变且有跨场景和模型的实用性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型代理的广泛部署带来隐私威胁，动态对话的攻击策略难手动发现漏洞。

Method: 提出基于搜索的框架，模拟隐私关键的代理交互，用大语言模型作为优化器的搜索算法，多线程并行搜索和跨线程传播分析模拟轨迹并迭代提出新指令。

Result: 攻击策略从简单直接请求升级到复杂多轮策略，防御从基于规则的约束发展到身份验证状态机。

Conclusion: 发现的攻击和防御策略能跨场景和模型转移，对构建隐私感知代理有很强的实用价值。

Abstract: The widespread deployment of LLM-based agents is likely to introduce a
critical privacy threat: malicious agents that proactively engage others in
multi-turn interactions to extract sensitive information. These dynamic
dialogues enable adaptive attack strategies that can cause severe privacy
violations, yet their evolving nature makes it difficult to anticipate and
discover sophisticated vulnerabilities manually. To tackle this problem, we
present a search-based framework that alternates between improving attacker and
defender instructions by simulating privacy-critical agent interactions. Each
simulation involves three roles: data subject, data sender, and data recipient.
While the data subject's behavior is fixed, the attacker (data recipient)
attempts to extract sensitive information from the defender (data sender)
through persistent and interactive exchanges. To explore this interaction space
efficiently, our search algorithm employs LLMs as optimizers, using parallel
search with multiple threads and cross-thread propagation to analyze simulation
trajectories and iteratively propose new instructions. Through this process, we
find that attack strategies escalate from simple direct requests to
sophisticated multi-turn tactics such as impersonation and consent forgery,
while defenses advance from rule-based constraints to identity-verification
state machines. The discovered attacks and defenses transfer across diverse
scenarios and backbone models, demonstrating strong practical utility for
building privacy-aware agents.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [222] [The Conditional Regret-Capacity Theorem for Batch Universal Prediction](https://arxiv.org/abs/2508.10282)
*Marco Bondaschi,Michael Gastpar*

Main category: cs.IT

TL;DR: 推导经典遗憾 - 容量定理的条件版本，用于通用预测找最小批次遗憾下界，应用于二进制无记忆源并推广到Rényi信息度量。


<details>
  <summary>Details</summary>
Motivation: 在可获取训练数据批次的通用预测中，找到最小批次遗憾的下界。

Method: 推导经典遗憾 - 容量定理的条件版本，并将结果应用于二进制无记忆源，最后将定理推广到Rényi信息度量。

Result: 得到经典遗憾 - 容量定理的条件版本，应用于二进制无记忆源，揭示了条件Rényi散度和条件Sibson互信息之间的深层联系。

Conclusion: 经典遗憾 - 容量定理的条件版本可用于通用预测中最小批次遗憾的下界求解，且该定理能推广到Rényi信息度量。

Abstract: We derive a conditional version of the classical regret-capacity theorem.
This result can be used in universal prediction to find lower bounds on the
minimal batch regret, which is a recently introduced generalization of the
average regret, when batches of training data are available to the predictor.
As an example, we apply this result to the class of binary memoryless sources.
Finally, we generalize the theorem to R\'enyi information measures, revealing a
deep connection between the conditional R\'enyi divergence and the conditional
Sibson's mutual information.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [223] [Machine Learning for Cloud Detection in IASI Measurements: A Data-Driven SVM Approach with Physical Constraints](https://arxiv.org/abs/2508.10120)
*Chiara Zugarini,Cristina Sgattoni,Luca Sgheri*

Main category: physics.ao-ph

TL;DR: 本文用支持向量机方法对气象卫星红外辐射数据进行云层分类，取得较好结果，表明该方法适用于业务反演和未来任务。


<details>
  <summary>Details</summary>
Motivation: 云检测对大气反演、气候研究和天气预报至关重要，需有效方法进行云层分类。

Method: 运用支持向量机方法（CISVM），结合主成分分析进行降维，选择对云敏感的通道。

Result: 最佳配置与参考标签的一致性达88.30%，与MODIS云掩膜有较强一致性，极地地区因传感器差异存在较大差异。

Conclusion: CISVM是一种稳健、灵活且高效的自动云层分类方法，适用于业务反演和未来任务。

Abstract: Cloud detection is essential for atmospheric retrievals, climate studies, and
weather forecasting. We analyze infrared radiances from the Infrared
Atmospheric Sounding Interferometer (IASI) onboard Meteorological Operational
(MetOp) satellites to classify scenes as clear or cloudy.
  We apply the Support Vector Machine (SVM) approach, based on kernel methods
for non-separable data. In this study, the method is implemented for Cloud
Identification (CISVM) to classify the test set using radiances or brightness
temperatures, with dimensionality reduction through Principal Component
Analysis (PCA) and cloud-sensitive channel selection to focus on the most
informative features. Our best configuration achieves 88.30 percent agreement
with reference labels and shows strong consistency with cloud masks from the
Moderate Resolution Imaging Spectroradiometer (MODIS), with the largest
discrepancies in polar regions due to sensor differences.
  These results demonstrate that CISVM is a robust, flexible, and efficient
method for automated cloud classification from infrared radiances, suitable for
operational retrievals and future missions such as Far infrared Outgoing
Radiation Understanding and Monitoring (FORUM), the ninth European Space Agency
Earth Explorer Mission.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [224] [Whisper Smarter, not Harder: Adversarial Attack on Partial Suppression](https://arxiv.org/abs/2508.09994)
*Zheng Jie Wong,Bingquan Shen*

Main category: cs.SD

TL;DR: 研究语音识别模型对抗攻击的鲁棒性、不可感知性及防御方法。


<details>
  <summary>Details</summary>
Motivation: 现有自动语音识别（ASR）模型存在对抗攻击风险，可能抑制或干扰模型输出。

Method: 研究验证攻击的鲁棒性，尝试将优化目标从完全抑制放宽到部分抑制，探索可能的防御手段。

Result: 放宽优化目标可降低攻击的可感知性，低通滤波器防御可能有效。

Conclusion: 可通过改变优化目标提升攻击的不可感知性，低通滤波器可作为有效防御。

Abstract: Currently, Automatic Speech Recognition (ASR) models are deployed in an
extensive range of applications. However, recent studies have demonstrated the
possibility of adversarial attack on these models which could potentially
suppress or disrupt model output. We investigate and verify the robustness of
these attacks and explore if it is possible to increase their imperceptibility.
We additionally find that by relaxing the optimisation objective from complete
suppression to partial suppression, we can further decrease the
imperceptibility of the attack. We also explore possible defences against these
attacks and show a low-pass filter defence could potentially serve as an
effective defence.

</details>


### [225] [No Free Lunch from Audio Pretraining in Bioacoustics: A Benchmark Study of Embeddings](https://arxiv.org/abs/2508.10230)
*Chenggang Chen,Zhiyu Yang*

Main category: cs.SD

TL;DR: 研究对11个深度学习模型在生物声学任务上进行基准测试，发现预训练模型微调的必要性等情况，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有基准研究显示微调的音频预训练模型在部分任务失败，需进一步探究模型性能。

Method: 对11个深度学习模型进行降维并通过聚类评估。

Result: 未微调模型不如微调的AlexNet；有无微调模型都难分离背景和标记声音，但ResNet可以；微调时背景音少表现更好。

Conclusion: 强调微调音频预训练模型并检查微调后嵌入的必要性。

Abstract: Bioacoustics, the study of animal sounds, offers a non-invasive method to
monitor ecosystems. Extracting embeddings from audio-pretrained deep learning
(DL) models without fine-tuning has become popular for obtaining bioacoustic
features for tasks. However, a recent benchmark study reveals that while
fine-tuned audio-pretrained VGG and transformer models achieve state-of-the-art
performance in some tasks, they fail in others. This study benchmarks 11 DL
models on the same tasks by reducing their learned embeddings' dimensionality
and evaluating them through clustering. We found that audio-pretrained DL
models 1) without fine-tuning even underperform fine-tuned AlexNet, 2) both
with and without fine-tuning fail to separate the background from labeled
sounds, but ResNet does, and 3) outperform other models when fewer background
sounds are included during fine-tuning. This study underscores the necessity of
fine-tuning audio-pretrained models and checking the embeddings after
fine-tuning. Our codes are available:
https://github.com/NeuroscienceAI/Audio\_Embeddings

</details>


### [226] [Alternating Approach-Putt Models for Multi-Stage Speech Enhancement](https://arxiv.org/abs/2508.10436)
*Iksoon Jeong,Kyung-Joong Kim,Kang-Hun Ahn*

Main category: cs.SD

TL;DR: 提出后处理神经网络PuttNet减轻语音增强模型的伪影，交替使用语音增强模型和PuttNet可提升语音质量。


<details>
  <summary>Details</summary>
Motivation: 语音增强网络常引入伪影降低音频质量，需解决此问题。

Method: 提出后处理神经网络PuttNet，交替使用语音增强模型和PuttNet。

Result: 交替使用模型在PESQ、STOI和CBAK得分上提升了语音质量。

Conclusion: 交替使用语音增强模型和PuttNet比单独重复使用任一模型效果好，通过图形分析说明了原因。

Abstract: Speech enhancement using artificial neural networks aims to remove noise from
noisy speech signals while preserving the speech content. However, speech
enhancement networks often introduce distortions to the speech signal, referred
to as artifacts, which can degrade audio quality. In this work, we propose a
post-processing neural network designed to mitigate artifacts introduced by
speech enhancement models. Inspired by the analogy of making a `Putt' after an
`Approach' in golf, we name our model PuttNet. We demonstrate that alternating
between a speech enhancement model and the proposed Putt model leads to
improved speech quality, as measured by perceptual quality scores (PESQ),
objective intelligibility (STOI), and background noise intrusiveness (CBAK)
scores. Furthermore, we illustrate with graphical analysis why this alternating
Approach outperforms repeated application of either model alone.

</details>


### [227] [Fake Speech Wild: Detecting Deepfake Speech on Social Media Platform](https://arxiv.org/abs/2508.10559)
*Yuankun Xie,Ruibo Fu,Xiaopeng Wang,Zhiyong Wang,Ya Li,Zhengqi Wen,Haonnan Cheng,Long Ye*

Main category: cs.SD

TL;DR: 提出FSW数据集，评估现有反制措施，通过数据增强提升现实场景中深度伪造音频检测性能，平均EER达3.54%。


<details>
  <summary>Details</summary>
Motivation: 现有深度伪造音频反制措施在跨领域场景性能下降，需推进现实场景中的检测。

Method: 提出FSW数据集，用公共数据集和基于自监督学习的反制措施建立基准，评估数据增强策略。

Result: 通过增强公共数据集和纳入FSW训练集，显著提升现实场景中深度伪造音频检测性能，平均EER为3.54%。

Conclusion: 所提方法能有效提升现实世界中深度伪造音频的检测效果。

Abstract: The rapid advancement of speech generation technology has led to the
widespread proliferation of deepfake speech across social media platforms.
While deepfake audio countermeasures (CMs) achieve promising results on public
datasets, their performance degrades significantly in cross-domain scenarios.
To advance CMs for real-world deepfake detection, we first propose the Fake
Speech Wild (FSW) dataset, which includes 254 hours of real and deepfake audio
from four different media platforms, focusing on social media. As CMs, we
establish a benchmark using public datasets and advanced selfsupervised
learning (SSL)-based CMs to evaluate current CMs in real-world scenarios. We
also assess the effectiveness of data augmentation strategies in enhancing CM
robustness for detecting deepfake speech on social media. Finally, by
augmenting public datasets and incorporating the FSW training set, we
significantly advanced real-world deepfake audio detection performance,
achieving an average equal error rate (EER) of 3.54% across all evaluation
sets.

</details>


<div id='astro-ph.EP'></div>

# astro-ph.EP [[Back]](#toc)

### [228] [Accelerating exoplanet climate modelling: A machine learning approach to complement 3D GCM grid simulations](https://arxiv.org/abs/2508.10827)
*Alexander Plaschzug,Amit Reza,Ludmila Carone,Sebastian Gernjak,Christiane Helling*

Main category: astro-ph.EP

TL;DR: 随着望远镜发展，对增强3D气候模型需求增长，但GCM模拟有挑战。本研究用ML算法预测潮汐锁定气态系外行星3D结构，训练模型并测试，结果表明ML模拟器能可靠预测温度场，可补充传统GCM网格。


<details>
  <summary>Details</summary>
Motivation: 随着望远镜发展，对增强3D气候模型需求增长，但GCM计算密集且耗时，难以模拟大量系外行星大气，因此研究ML算法能否预测系外行星3D温度和风结构。

Method: 引入新的3D GCM网格，用Exorad建模60个膨胀热木星，在该网格上训练DNN和XGBoost算法预测温度和风速，选取目标行星用ExoRad和两种ML方法测试。

Result: DNN对气体温度的预测使得计算光谱在除一颗行星外的所有行星上误差在32 ppm以内，开发的ML模拟器能可靠预测3D温度场。

Conclusion: ML模拟器可作为快速工具补充和扩展传统GCM网格用于系外行星集合研究，预测质量对气相化学、云形成和透射光谱影响极小。

Abstract: With the development of ever-improving telescopes capable of observing
exoplanet atmospheres in greater detail and number, there is a growing demand
for enhanced 3D climate models to support and help interpret observational data
from space missions like CHEOPS, TESS, JWST, PLATO, and Ariel. However, the
computationally intensive and time-consuming nature of general circulation
models (GCMs) poses significant challenges in simulating a wide range of
exoplanetary atmospheres. This study aims to determine whether machine learning
(ML) algorithms can be used to predict the 3D temperature and wind structure of
arbitrary tidally-locked gaseous exoplanets in a range of planetary parameters.
A new 3D GCM grid with 60 inflated hot Jupiters orbiting A, F, G, K, and M-type
host stars modelled with Exorad has been introduced. A dense neural network
(DNN) and a decision tree algorithm (XGBoost) are trained on this grid to
predict local gas temperatures along with horizontal and vertical winds. To
ensure the reliability and quality of the ML model predictions, WASP-121 b,
HATS-42 b, NGTS-17 b, WASP-23 b, and NGTS-1 b-like planets, which are all
targets for PLATO observation, are selected and modelled with ExoRad and the
two ML methods as test cases. The DNN predictions for the gas temperatures are
to such a degree that the calculated spectra agree within 32 ppm for all but
one planet, for which only one single HCN feature reaches a 100 ppm difference.
The developed ML emulators can reliably predict the complete 3D temperature
field of an inflated warm to ultra-hot tidally locked Jupiter around A to
M-type host stars. It provides a fast tool to complement and extend traditional
GCM grids for exoplanet ensemble studies. The quality of the predictions is
such that no or minimal effects on the gas phase chemistry, hence on the cloud
formation and transmission spectra, are to be expected.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [229] [Estimating carbon pools in the shelf sea environment: reanalysis or model-informed machine learning?](https://arxiv.org/abs/2508.10178)
*Jozef Skakala*

Main category: q-bio.QM

TL;DR: 提出用神经网络集成学习物理 - 生物地球化学模型中可观测变量与碳库关系，以西北欧大陆架为例验证其可行性，认为模型驱动机器学习可替代昂贵再分析方法。


<details>
  <summary>Details</summary>
Motivation: 陆架海碳封存和碳循环重要，但原位或卫星数据稀疏且不确定，再分析运行成本高。

Method: 使用神经网络集成从耦合物理 - 生物地球化学模型中学习可观测变量与碳库的关系。

Result: 在西北欧大陆架环境中，经模型自由运行模拟训练的神经网络能重现再分析输出，且能提供碳库不确定性信息。

Conclusion: 模型驱动机器学习是昂贵再分析的可行替代方案，可补充缺失或不确定的观测数据。

Abstract: Shelf seas are important for carbon sequestration and carbon cycle, but
available in situ, or satellite data for carbon pools in the shelf sea
environment are often sparse, or highly uncertain. Alternative can be provided
by reanalyses, but these are often expensive to run. We propose to use an
ensemble of neural networks (NN) to learn from a coupled
physics-biogeochemistry model the relationship between the directly observable
variables and carbon pools. We demonstrate for North-West European Shelf (NWES)
sea environment, that when the NN trained on a model free run simulation is
applied to the NWES reanalysis, it is capable to reproduce the reanalysis
outputs for carbon pools. Moreover, unlike the existing NWES reanalysis, the NN
ensemble is also capable to provide uncertainty information for the pools. We
focus on explainability of the results and demonstrate potential use of the NNs
for future climate what-if scenarios. We suggest that model-informed machine
learning presents a viable alternative to expensive reanalyses and could
complement observational data, wherever they are missing and/or highly
uncertain.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [230] [Racial bias, colorism, and overcorrection](https://arxiv.org/abs/2508.10585)
*Kenneth Colombe,Alex Krumer,Rosa Lavelle-Hill,Tim Pawlowski*

Main category: econ.GN

TL;DR: 本文研究提高意识是否影响种族偏见和肤色主义，以WNBA为研究对象，发现媒体密集报道前无种族偏见，报道后出现过度纠正现象且随时间消退，强调考虑偏见基线水平的重要性。


<details>
  <summary>Details</summary>
Motivation: 探究提高意识能否影响种族偏见和肤色主义。

Method: 利用Price和Wolfers（2010）广泛宣传这一自然实验，对WNBA裁判判罚决策进行研究，运用机器学习技术预测球员种族和测量肤色，结合裁判随机分配和高维固定效应估计关系。

Result: 媒体密集报道前无种族偏见，报道后存在过度纠正现象，即球员面对不同种族和肤色裁判时犯规判罚减少，且这种过度纠正随时间消退。

Conclusion: 在应用与DEI相关措施前，政策制定者和组织需考虑偏见的基线水平。

Abstract: This paper examines whether increased awareness can affect racial bias and
colorism. We exploit a natural experiment from the widespread publicity of
Price and Wolfers (2010), which intensified scrutiny of racial bias in men's
basketball officiating. We investigate refereeing decisions in the Women's
National Basketball Association (WNBA), an organization with a long-standing
commitment to diversity, equity, and inclusion (DEI). We apply machine learning
techniques to predict player race and to measure skin tone. Our empirical
strategy exploits the quasi-random assignment of referees to games, combined
with high-dimensional fixed effects, to estimate the relationship between
referee-player racial and skin tone compositions and foul-calling behavior. We
find no racial bias before the intense media coverage. However, we find
evidence of overcorrection, whereby a player receives fewer fouls when facing
more referees from the opposite race and skin tone. This overcorrection wears
off over time, returning to zero-bias levels. We highlight the need to consider
baseline levels of bias before applying any prescription with direct relevance
to policymakers and organizations given the recent discourse on DEI.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [231] [DINOMotion: advanced robust tissue motion tracking with DINOv2 in 2D-Cine MRI-guided radiotherapy](https://arxiv.org/abs/2508.10260)
*Soorena Salari,Catherine Spino,Laurie-Anne Pharand,Fabienne Lathuiliere,Hassan Rivaz,Silvain Beriault,Yiming Xiao*

Main category: eess.IV

TL;DR: 提出DINOMotion框架用于2D - Cine MRI引导放疗中的运动跟踪，实验证明其有效且优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有2D - Cine MRI引导放疗中图像配准方法在处理大错位和可解释性方面存在挑战，需更优方法。

Method: 引入基于DINOv2和低秩自适应（LoRA）层的DINOMotion框架，自动检测对应地标进行图像配准，LoRA层减少可训练参数，DINOv2提供强大特征表示。

Result: 在志愿者和患者数据集上能有效估计线性和非线性变换，肾脏、肝脏、肺的Dice分数分别达92.07%、90.90%、95.23%，Hausdorff距离分别为5.47 mm、8.31 mm、6.72 mm，每次扫描约30ms，优于现有方法。

Conclusion: DINOMotion是2D - Cine MRI引导放疗中实时运动跟踪的强大且可解释的解决方案。

Abstract: Accurate tissue motion tracking is critical to ensure treatment outcome and
safety in 2D-Cine MRI-guided radiotherapy. This is typically achieved by
registration of sequential images, but existing methods often face challenges
with large misalignments and lack of interpretability. In this paper, we
introduce DINOMotion, a novel deep learning framework based on DINOv2 with
Low-Rank Adaptation (LoRA) layers for robust, efficient, and interpretable
motion tracking. DINOMotion automatically detects corresponding landmarks to
derive optimal image registration, enhancing interpretability by providing
explicit visual correspondences between sequential images. The integration of
LoRA layers reduces trainable parameters, improving training efficiency, while
DINOv2's powerful feature representations offer robustness against large
misalignments. Unlike iterative optimization-based methods, DINOMotion directly
computes image registration at test time. Our experiments on volunteer and
patient datasets demonstrate its effectiveness in estimating both linear and
nonlinear transformations, achieving Dice scores of 92.07% for the kidney,
90.90% for the liver, and 95.23% for the lung, with corresponding Hausdorff
distances of 5.47 mm, 8.31 mm, and 6.72 mm, respectively. DINOMotion processes
each scan in approximately 30ms and consistently outperforms state-of-the-art
methods, particularly in handling large misalignments. These results highlight
its potential as a robust and interpretable solution for real-time motion
tracking in 2D-Cine MRI-guided radiotherapy.

</details>


<div id='q-bio.MN'></div>

# q-bio.MN [[Back]](#toc)

### [232] [In silico study on the cytotoxicity against Hela cancer cells of xanthones bioactive compounds from Garcinia cowa: QSAR based on Graph Deep Learning, Network Pharmacology, and Molecular Docking](https://arxiv.org/abs/2508.10117)
*Nguyen Manh Son,Pham Huu Vang,Nguyen Thi Dung,Nguyen Manh Ha. Ta Thi Thao,Tran Thi Thu Thuy,Phan Minh Giang*

Main category: q-bio.MN

TL;DR: 本文聚焦癌症及藤黄属植物，用网络药理学分析确定抗癌活性成分与靶点，图注意力网络算法精准预测pIC50值，分子对接揭示潜在靶点。


<details>
  <summary>Details</summary>
Motivation: 癌症是高死亡率疾病且有年轻化趋势，寻找抗癌药物，研究藤黄属植物Garcinia cowa的抗癌作用。

Method: 采用网络药理学分析确定活性成分和蛋白靶点，用图注意力网络算法预测pIC50值，进行分子对接研究。

Result: 网络药理学分析确定关键活性化合物及蛋白靶点；图注意力网络算法在数据增强后R2达0.98，RMSE为0.02；分子对接揭示MTOR是潜在靶点。

Conclusion: Garcinia cowa的活性成分有抗癌潜力，图注意力网络算法预测准确，MTOR可能是抗癌作用的潜在靶点。

Abstract: Cancer is recognized as a complex group of diseases, contributing to the
highest global mortality rates, with increasing prevalence and a trend toward
affecting younger populations. It is characterized by uncontrolled
proliferation of abnormal cells, invasion of adjacent tissues, and metastasis
to distant organs. Garcinia cowa, a traditional medicinal plant widely used in
Southeast Asia, including Vietnam, is employed to treat fever, cough,
indigestion, as a laxative, and for parasitic diseases. Numerous xanthone
compounds isolated from this species exhibit a broad spectrum of biological
activities, with some showing promise as anti cancer and antimalarial agents.
Network pharmacology analysis successfully identified key bioactive compounds
Rubraxanthone, Garcinone D, Norcowanin, Cowanol, and Cowaxanthone alongside
their primary protein targets (TNF, CTNNB1, SRC, NFKB1, and MTOR), providing
critical insights into the molecular mechanisms underlying their anti-cancer
effects. The Graph Attention Network algorithm demonstrated superior predictive
performance, achieving an R2 of 0.98 and an RMSE of 0.02 after data
augmentation, highlighting its accuracy in predicting pIC50 values for xanthone
based compounds. Additionally, molecular docking revealed MTOR as a potential
target for inducing cytotoxicity in HeLa cancer cells from Garcinia cowa.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [233] [Performance of universal machine-learned potentials with explicit long-range interactions in biomolecular simulations](https://arxiv.org/abs/2508.10841)
*Viktor Zaverkin,Matheus Ferraz,Francesco Alesiani,Mathias Niepert*

Main category: physics.chem-ph

TL;DR: 本文系统评估基于SPICE - v2数据集训练的等变消息传递架构在生物分子模拟中的应用，发现模型大小、训练数据组成等因素影响结果，当前不平衡数据集和不成熟评估方法挑战通用机器学习势在生物分子模拟中的应用。


<details>
  <summary>Details</summary>
Motivation: 通用机器学习势在生物分子模拟中的应用研究不足，需要进行系统评估。

Method: 系统评估在SPICE - v2数据集上训练的等变消息传递架构，考虑有无显式长程色散和静电作用，评估模型大小、训练数据组成和静电处理在不同基准数据集和分子模拟中的影响。

Result: 较大模型在基准数据集上提高准确性，但在模拟属性上不一致；预测属性依赖训练数据集组成；长程静电作用无系统影响，但对Trp - cage增加构象变异性。

Conclusion: 不平衡数据集和不成熟评估实践挑战通用机器学习势在生物分子模拟中的应用。

Abstract: Universal machine-learned potentials promise transferable accuracy across
compositional and vibrational degrees of freedom, yet their application to
biomolecular simulations remains underexplored. This work systematically
evaluates equivariant message-passing architectures trained on the SPICE-v2
dataset with and without explicit long-range dispersion and electrostatics. We
assess the impact of model size, training data composition, and electrostatic
treatment across in- and out-of-distribution benchmark datasets, as well as
molecular simulations of bulk liquid water, aqueous NaCl solutions, and
biomolecules, including alanine tripeptide, the mini-protein Trp-cage, and
Crambin. While larger models improve accuracy on benchmark datasets, this trend
does not consistently extend to properties obtained from simulations. Predicted
properties also depend on the composition of the training dataset. Long-range
electrostatics show no systematic impact across systems. However, for Trp-cage,
their inclusion yields increased conformational variability. Our results
suggest that imbalanced datasets and immature evaluation practices currently
challenge the applicability of universal machine-learned potentials to
biomolecular simulations.

</details>


<div id='q-fin.PR'></div>

# q-fin.PR [[Back]](#toc)

### [234] [CATNet: A geometric deep learning approach for CAT bond spread prediction in the primary market](https://arxiv.org/abs/2508.10208)
*Dixon Domfeh,Saeid Safarveisi*

Main category: q-fin.PR

TL;DR: 本文引入CATNet框架，用R - GCN对巨灾债券一级市场建模预测利差，表现优于基准，拓扑特征提升准确性，证明网络连通性是定价关键。


<details>
  <summary>Details</summary>
Motivation: 传统巨灾债券定价模型难以处理复杂关系数据，需新方法。

Method: 引入CATNet框架，应用R - GCN将巨灾债券一级市场建模为图进行利差预测，还纳入拓扑中心性度量作为特征。

Result: CATNet预测性能高，显著优于随机森林基准，纳入拓扑特征进一步提升准确性，网络特征是行业直觉的量化代理。

Conclusion: 网络连通性是价格关键决定因素，基于图的模型能实现高精度并提供市场洞察。

Abstract: Traditional models for pricing catastrophe (CAT) bonds struggle to capture
the complex, relational data inherent in these instruments. This paper
introduces CATNet, a novel framework that applies a geometric deep learning
architecture, the Relational Graph Convolutional Network (R-GCN), to model the
CAT bond primary market as a graph, leveraging its underlying network structure
for spread prediction. Our analysis reveals that the CAT bond market exhibits
the characteristics of a scale-free network, a structure dominated by a few
highly connected and influential hubs. CATNet demonstrates high predictive
performance, significantly outperforming a strong Random Forest benchmark. The
inclusion of topological centrality measures as features provides a further,
significant boost in accuracy. Interpretability analysis confirms that these
network features are not mere statistical artifacts; they are quantitative
proxies for long-held industry intuition regarding issuer reputation,
underwriter influence, and peril concentration. This research provides evidence
that network connectivity is a key determinant of price, offering a new
paradigm for risk assessment and proving that graph-based models can deliver
both state-of-the-art accuracy and deeper, quantifiable market insights.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [235] [Layer-Wise Analysis of Self-Supervised Representations for Age and Gender Classification in Children's Speech](https://arxiv.org/abs/2508.10332)
*Abhijit Sinha,Harishankar Kumar,Mohit Joshi,Hemant Kumar Kathania,Shrikanth Narayanan,Sudarsana Reddy Kadiri*

Main category: eess.AS

TL;DR: 对四种Wav2Vec2变体进行逐层分析，发现早期层更能捕获说话人特征，PCA可提升分类效果，不同模型在不同数据集上有较好表现。


<details>
  <summary>Details</summary>
Motivation: 儿童语音在年龄和性别分类上有挑战，自监督学习模型在儿童语音编码说话人特征方面研究不足。

Method: 使用PFSTAR和CMU Kids数据集对四种Wav2Vec2变体进行逐层分析，应用PCA。

Result: 早期层（1 - 7）更有效捕获说话人特征，Wav2Vec2-large-lv60等模型在不同数据集上取得高分类准确率。

Conclusion: 揭示了自监督学习模型深度中说话人特征的结构，支持针对儿童语音界面制定更有针对性的自适应策略。

Abstract: Children's speech presents challenges for age and gender classification due
to high variability in pitch, articulation, and developmental traits. While
self-supervised learning (SSL) models perform well on adult speech tasks, their
ability to encode speaker traits in children remains underexplored. This paper
presents a detailed layer-wise analysis of four Wav2Vec2 variants using the
PFSTAR and CMU Kids datasets. Results show that early layers (1-7) capture
speaker-specific cues more effectively than deeper layers, which increasingly
focus on linguistic information. Applying PCA further improves classification,
reducing redundancy and highlighting the most informative components. The
Wav2Vec2-large-lv60 model achieves 97.14% (age) and 98.20% (gender) on CMU
Kids; base-100h and large-lv60 models reach 86.05% and 95.00% on PFSTAR. These
results reveal how speaker traits are structured across SSL model depth and
support more targeted, adaptive strategies for child-aware speech interfaces.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [236] [Exploring Content and Social Connections of Fake News with Explainable Text and Graph Learning](https://arxiv.org/abs/2508.10040)
*Vítor N. Lourenço,Aline Paes,and Tillman Weyde*

Main category: cs.SI

TL;DR: 为应对错误信息传播，本文提出结合多特征的可解释框架用于事实核查，实验表明多模态信息提升性能，框架解释有效。


<details>
  <summary>Details</summary>
Motivation: 全球错误信息传播及内容可信度问题促使开发自动事实核查系统，且现有方法存在不足。

Method: 提出结合内容、社交媒体和基于图的特征的可解释框架，集成错误信息分类器和可解释性技术。

Result: 多模态信息比单模态性能更好，对英语、西班牙语和葡萄牙语数据集进行评估；用新协议评估框架解释，表明能生成人类可理解的预测理由。

Conclusion: 结合多特征的可解释框架能有效用于事实核查，其解释具有可解释性、可信度和鲁棒性。

Abstract: The global spread of misinformation and concerns about content
trustworthiness have driven the development of automated fact-checking systems.
Since false information often exploits social media dynamics such as "likes"
and user networks to amplify its reach, effective solutions must go beyond
content analysis to incorporate these factors. Moreover, simply labelling
content as false can be ineffective or even reinforce biases such as automation
and confirmation bias. This paper proposes an explainable framework that
combines content, social media, and graph-based features to enhance
fact-checking. It integrates a misinformation classifier with explainability
techniques to deliver complete and interpretable insights supporting
classification decisions. Experiments demonstrate that multimodal information
improves performance over single modalities, with evaluations conducted on
datasets in English, Spanish, and Portuguese. Additionally, the framework's
explanations were assessed for interpretability, trustworthiness, and
robustness with a novel protocol, showing that it effectively generates
human-understandable justifications for its predictions.

</details>


### [237] [SABIA: An AI-Powered Tool for Detecting Opioid-Related Behaviors on Social Media](https://arxiv.org/abs/2508.10046)
*Muhammad Ahmad,Fida Ullah,Muhammad Usman,Ildar Batyrshin,Grigori Sidorov*

Main category: cs.SI

TL;DR: 研究提出SABIA混合深度学习模型检测社交媒体上阿片类药物相关用户行为，表现优于基线，证实混合模型潜力。


<details>
  <summary>Details</summary>
Motivation: 社交媒体数据因语言问题难分析，影响阿片类药物滥用检测，需解决该问题。

Method: 分析BERT技术，开发SABIA模型，包括数据预处理、模型表示、微调、分类；构建新数据集，用监督学习实验。

Result: SABIA达基准性能，超基线，准确率提高9.30%，与先前研究对比证实有效性和鲁棒性。

Conclusion: 混合深度学习模型在检测社交媒体复杂阿片类药物相关行为有潜力，支持公共卫生监测和干预。

Abstract: Social media platforms have become valuable tools for understanding public
health challenges by offering insights into patient behaviors, medication use,
and mental health issues. However, analyzing such data remains difficult due to
the prevalence of informal language, slang, and coded communication, which can
obscure the detection of opioid misuse. This study addresses the issue of
opioid-related user behavior on social media, including informal expressions,
slang terms, and misspelled or coded language. We analyzed the existing
Bidirectional Encoder Representations from Transformers (BERT) technique and
developed a BERT-BiLSTM-3CNN hybrid deep learning model, named SABIA, to create
a single-task classifier that effectively captures the features of the target
dataset. The SABIA model demonstrated strong capabilities in capturing
semantics and contextual information. The proposed approach includes: (1) data
preprocessing, (2) data representation using the SABIA model, (3) a fine-tuning
phase, and (4) classification of user behavior into five categories. A new
dataset was constructed from Reddit posts, identifying opioid user behaviors
across five classes: Dealers, Active Opioid Users, Recovered Users,
Prescription Users, and Non-Users, supported by detailed annotation guidelines.
Experiments were conducted using supervised learning. Results show that SABIA
achieved benchmark performance, outperforming the baseline (Logistic
Regression, LR = 0.86) and improving accuracy by 9.30%. Comparisons with seven
previous studies confirmed its effectiveness and robustness. This study
demonstrates the potential of hybrid deep learning models for detecting complex
opioid-related behaviors on social media, supporting public health monitoring
and intervention efforts.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [238] [Deep Learning in Classical and Quantum Physics](https://arxiv.org/abs/2508.10666)
*Timothy Heightman,Marcin Płodzień*

Main category: quant-ph

TL;DR: 介绍机器学习（尤其是深度学习）在量子科学技术中的应用、风险，及提供研究生水平的深度学习量子应用讲义。


<details>
  <summary>Details</summary>
Motivation: 机器学习尤其是深度学习已成为量子科学技术的变革性工具，但存在风险，需让读者学会有效应用及应对限制。

Method: 以循序渐进的方式，结合概念阐述和实例，提供研究生水平的深度学习量子应用讲义。

Result: 未提及具体结果。

Conclusion: 讲义可让读者学会有效应用深度学习，理解其实践限制，负责任地将人工智能方法应用于量子相关领域问题。

Abstract: Scientific progress is tightly coupled to the emergence of new research
tools. Today, machine learning (ML)-especially deep learning (DL)-has become a
transformative instrument for quantum science and technology. Owing to the
intrinsic complexity of quantum systems, DL enables efficient exploration of
large parameter spaces, extraction of patterns from experimental data, and
data-driven guidance for research directions. These capabilities already
support tasks such as refining quantum control protocols and accelerating the
discovery of materials with targeted quantum properties, making ML/DL literacy
an essential skill for the next generation of quantum scientists. At the same
time, DL's power brings risks: models can overfit noisy data, obscure causal
structure, and yield results with limited physical interpretability.
Recognizing these limitations and deploying mitigation strategies is crucial
for scientific rigor. These lecture notes provide a comprehensive,
graduate-level introduction to DL for quantum applications, combining
conceptual exposition with hands-on examples. Organized as a progressive
sequence, they aim to equip readers to decide when and how to apply DL
effectively, to understand its practical constraints, and to adapt AI methods
responsibly to problems across quantum physics, chemistry, and engineering.

</details>


### [239] [Decoded Quantum Interferometry Under Noise](https://arxiv.org/abs/2508.10725)
*Kaifeng Bu,Weichen Gu,Dax Enshan Koh,Xiang Li*

Main category: quant-ph

TL;DR: 对解码量子干涉法（DQI）在噪声下进行严格分析，聚焦局部去极化噪声，证明噪声下性能受实例矩阵噪声加权稀疏参数影响，通过数值模拟展示性能衰减，方法可适配其他随机泡利噪声。


<details>
  <summary>Details</summary>
Motivation: 此前DQI算法在理想化设置下很有前景，但对其抗噪声能力研究较少，需进行严格分析。

Method: 对DQI在局部去极化噪声下进行严格分析，针对最大线性可满足性问题进行研究，并通过数值模拟两个特殊案例展示结果。

Result: 证明在噪声下性能由实例矩阵的噪声加权稀疏参数决定，解的质量随稀疏性降低呈指数衰减。

Conclusion: 所开发的傅里叶分析方法可适配其他随机泡利噪声，框架适用于广泛的噪声量子环境，为在现实噪声下保留DQI的量子优势提供指导。

Abstract: Decoded Quantum Interferometry (DQI) is a recently proposed quantum
optimization algorithm that exploits sparsity in the Fourier spectrum of
objective functions, with the potential for exponential speedups over classical
algorithms on suitably structured problems. While highly promising in idealized
settings, its resilience to noise has until now been largely unexplored. To
address this, we conduct a rigorous analysis of DQI under noise, focusing on
local depolarizing noise. For the maximum linear satisfiability problem, we
prove that, in the presence of noise, performance is governed by a
noise-weighted sparsity parameter of the instance matrix, with solution quality
decaying exponentially as sparsity decreases. We demonstrate this decay through
numerical simulations on two special cases: the Optimal Polynomial Intersection
problem and the Maximum XOR Satisfiability problem. The Fourier-analytic
methods we develop can be readily adapted to other classes of random Pauli
noise, making our framework applicable to a broad range of noisy quantum
settings and offering guidance on preserving DQI's potential quantum advantage
under realistic noise.

</details>


### [240] [Mitigating Exponential Mixed Frequency Growth through Frequency Selection and Dimensional Separation in Quantum Machine Learning](https://arxiv.org/abs/2508.10533)
*Michael Poppel,David Bucher,Maximilian Zorn,Nico Kraus,Jonas Stein,Claudia Linnhoff-Popien*

Main category: quant-ph

TL;DR: 研究量子机器学习（QML）中角度编码技术，指出量子模型回归任务常失败原因，提出频率选择和维度分离技术改善可训练性，并进行实验验证。


<details>
  <summary>Details</summary>
Motivation: 利用量子计算潜在计算加速，解决量子模型在回归任务中常失败的问题。

Method: 进行两个白盒实验分析失败原因，提出频率选择和维度分离技术约束参数数量。

Result: 通过实验证明所提技术减少了参数需求，可在噪声量子模拟器上训练并在真实量子硬件上进行推理。

Conclusion: 频率选择和维度分离技术能约束参数数量，扩大当前硬件可处理问题集，提高量子模型可训练性。

Abstract: To leverage the potential computational speedup of quantum computing (QC),
research in quantum machine learning (QML) has gained increasing prominence.
Angle encoding techniques in QML models have been shown to generate truncated
Fourier series, offering asymptotically universal function approximation
capabilities. By selecting efficient feature maps (FMs) within quantum
circuits, one can leverage the exponential growth of Fourier frequencies for
improved approximation. In multi-dimensional settings, additional input
dimensions induce further exponential scaling via mixed frequencies. In
practice, however, quantum models frequently fail at regression tasks. Through
two white-box experiments, we show that such failures can occur even when the
relevant frequencies are present, due to an insufficient number of trainable
parameters.
  In order to mitigate the double-exponential parameter growth resulting from
double-exponentially growing frequencies, we propose frequency selection and
dimensional separation as techniques to constrain the number of parameters,
thereby improving trainability. By restricting the QML model to essential
frequencies and permitting mixed frequencies only among feature dimensions with
known interdependence, we expand the set of tractable problems on current
hardware. We demonstrate the reduced parameter requirements by fitting two
white-box functions with known frequency spectrum and dimensional
interdependencies that could not be fitted with the default methods. The
reduced parameter requirements permit us to perform training on a noisy quantum
simulator and to demonstrate inference on real quantum hardware.

</details>


### [241] [Parity Cross-Resonance: A Multiqubit Gate](https://arxiv.org/abs/2508.10807)
*Xuexin Xu,Siyu Wang,Radhika Joshi,Rihan Hai,Mohammad H. Ansari*

Main category: quant-ph

TL;DR: 提出一种原生三量子比特纠缠门，利用混合优化方法，性能稳健，可用于多种场景，为下一代超导量子处理器奠定基础。


<details>
  <summary>Details</summary>
Motivation: 开发新的三量子比特纠缠门，以实现更高效、高保真的量子操作，用于量子纠错等领域。

Method: 采用混合优化方法，选择性放大所需相互作用，抑制不必要的耦合。

Result: 新门可用于GHZ三重态制备、Toffoli类逻辑演示、实现受控ZZ门等，且在不同希尔伯特空间大小下性能稳健。

Conclusion: 该工作为下一代超导量子处理器的电路架构和控制协议的协同设计奠定基础。

Abstract: We present a native three-qubit entangling gate that exploits engineered
interactions to realize control-control-target and control-target-target
operations in a single coherent step. Unlike conventional decompositions into
multiple two-qubit gates, our hybrid optimization approach selectively
amplifies desired interactions while suppressing unwanted couplings, yielding
robust performance across the computational subspace and beyond. The new gate
can be classified as a cross-resonance gate. We show it can be utilized in
several ways, for example, in GHZ triplet state preparation, Toffoli-class
logic demonstrations with many-body interactions, and in implementing a
controlled-ZZ gate. The latter maps the parity of two data qubits directly onto
a measurement qubit, enabling faster and higher-fidelity stabilizer
measurements in surface-code quantum error correction. In all these examples,
we show that the three-qubit gate performance remains robust across Hilbert
space sizes, as confirmed by testing under increasing total excitation numbers.
This work lays the foundation for co-designing circuit architectures and
control protocols that leverage native multiqubit interactions as core elements
of next-generation superconducting quantum processors.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [242] [Facilitating Longitudinal Interaction Studies of AI Systems](https://arxiv.org/abs/2508.10252)
*Tao Long,Sitong Wang,Émilie Fabre,Tony Wang,Anup Sathya,Jason Wu,Savvas Petridis,Dingzeyu Li,Tuhin Chakrabarty,Yue Jiang,Jingyi Li,Tiffany Tseng,Ken Nakagaki,Qian Yang,Nikolas Martelaro,Jeffrey V. Nickerson,Lydia B. Chilton*

Main category: cs.HC

TL;DR: UIST研究面临用户与AI交互动态变化，一次评估不足，本文介绍解决长期研究挑战的研讨会。


<details>
  <summary>Details</summary>
Motivation: 解决用户与AI交互不断变化下一次评估不足的问题，应对长期研究在部署、评估设计和数据收集方面的挑战。

Method: 举办包含主题演讲、小组讨论、实践协议设计和工具原型制作等环节的研讨会。

Result: 无明确提及

Conclusion: 旨在围绕纵向系统研究建立社区，推广其作为UIST工具设计、构建和评估的方法。

Abstract: UIST researchers develop tools to address user challenges. However, user
interactions with AI evolve over time through learning, adaptation, and
repurposing, making one time evaluations insufficient. Capturing these dynamics
requires longer-term studies, but challenges in deployment, evaluation design,
and data collection have made such longitudinal research difficult to
implement. Our workshop aims to tackle these challenges and prepare researchers
with practical strategies for longitudinal studies. The workshop includes a
keynote, panel discussions, and interactive breakout groups for discussion and
hands-on protocol design and tool prototyping sessions. We seek to foster a
community around longitudinal system research and promote it as a more embraced
method for designing, building, and evaluating UIST tools.

</details>


### [243] [MCP2OSC: Parametric Control by Natural Language](https://arxiv.org/abs/2508.10414)
*Yuan-Yi Fan*

Main category: cs.HC

TL;DR: 本文提出MCP服务器和提示设计标准，让自然语言提示可探索参数化OSC控制，通过实例验证其在处理OSC任务中的有效性，有作为多媒体设备通用控制机制的潜力。


<details>
  <summary>Details</summary>
Motivation: 解决文本提示在复杂任务精度不足和旋钮/滑块控制复杂度高的问题，弥合两者差距。

Method: 提出新的MCP服务器和提示设计标准，通过14个实际QA示例展示。

Result: Claude与MCP2OSC服务器结合在生成、解释、搜索、可视化、验证和调试OSC消息及管理地址模式方面有效。

Conclusion: MCP2OSC增强人机协作，为创意MCP应用提供新视角，有成为基于大语言模型的多媒体设备通用控制机制的潜力。

Abstract: Text prompts enable intuitive content creation but may fall short in
achieving high precision for intricate tasks; knob or slider controls offer
precise adjustments at the cost of increased complexity. To address the gap
between knobs and prompts, a new MCP (Model Context Protocol) server and a
unique set of prompt design criteria are presented to enable exploring
parametric OSC (OpenSoundControl) control by natural language prompts.
Demonstrated by 14 practical QA examples with best practices and the
generalized prompt templates, this study finds Claude integrated with the
MCP2OSC server effective in generating OSC messages by natural language,
interpreting, searching, and visualizing OSC messages, validating and debugging
OSC messages, and managing OSC address patterns. MCP2OSC enhances human-machine
collaboration by leveraging LLM (Large Language Model) to handle intricate OSC
development tasks, and by empowering human creativity with an intuitive
language interface featuring flexible precision controls: a prompt-based OSC
tool. This study provides a novel perspective on the creative MCP application
at the network protocol level by utilizing LLM's strength in directly
processing and generating human-readable OSC messages. The results suggest its
potential for a LLM-based universal control mechanism for multimedia devices.

</details>


### [244] [Pre-trained Transformer-models using chronic invasive electrophysiology for symptom decoding without patient-individual training](https://arxiv.org/abs/2508.10160)
*Timon Merk,Saeed Salehi,Richard M. Koehler,Qiming Cui,Maria Olaru,Amelia Hahn,Nicole R. Provenza,Simon Little,Reza Abbasi-Asl,Phil A. Starr,Wolf-Julian Neumann*

Main category: cs.HC

TL;DR: 提出基于长期脑深部电刺激记录训练的基础模型，用于神经解码，无需患者个体化训练实现帕金森病症状解码。


<details>
  <summary>Details</summary>
Motivation: 神经解码可实现个体化闭环神经调节治疗，预训练的大规模基础模型有无需患者个体化训练进行通用状态估计的潜力。

Method: 在超24天的慢性纵向脑深部电刺激记录上训练基础模型，强调30分钟的扩展上下文窗口，提出优化的神经电生理数据预训练损失函数。

Result: 在下游任务中，通过留一受试者交叉验证，无需患者个体化训练实现帕金森病症状解码。

Conclusion: 所提出的基础模型可用于神经病理和生理状态解码，且无需患者个体化训练。

Abstract: Neural decoding of pathological and physiological states can enable
patient-individualized closed-loop neuromodulation therapy. Recent advances in
pre-trained large-scale foundation models offer the potential for generalized
state estimation without patient-individual training. Here we present a
foundation model trained on chronic longitudinal deep brain stimulation
recordings spanning over 24 days. Adhering to long time-scale symptom
fluctuations, we highlight the extended context window of 30 minutes. We
present an optimized pre-training loss function for neural electrophysiological
data that corrects for the frequency bias of common masked auto-encoder loss
functions due to the 1-over-f power law. We show in a downstream task the
decoding of Parkinson's disease symptoms with leave-one-subject-out
cross-validation without patient-individual training.

</details>


### [245] [Reproducible Physiological Features in Affective Computing: A Preliminary Analysis on Arousal Modeling](https://arxiv.org/abs/2508.10561)
*Andrea Gargano,Jasin Machkour,Mimma Nardelli,Enzo Pasquale Scilingo,Michael Muma*

Main category: cs.HC

TL;DR: 该初步研究利用情感连续标注信号数据集，分析参与者的生理信号特征，发现仅两个皮电衍生特征与唤醒水平有可重复且显著关联，强调情感计算中生理特征选择需严格评估可重复性。


<details>
  <summary>Details</summary>
Motivation: 解决情感计算中主观情感体验与客观生理标记可靠关联的可重复性问题。

Method: 使用Continuously Annotated Signal of Emotion数据集，对30名参与者的心脏和皮电信号提取164个特征，采用Terminating - Random Experiments (T - Rex)方法进行特征选择。

Result: 仅两个皮电衍生特征与唤醒水平有可重复且显著的关联，确认率达100%。

Conclusion: 情感计算中生理特征选择需进行严格的可重复性评估，该方法在安全关键环境应用有前景。

Abstract: In Affective Computing, a key challenge lies in reliably linking subjective
emotional experiences with objective physiological markers. This preliminary
study addresses the issue of reproducibility by identifying physiological
features from cardiovascular and electrodermal signals that are associated with
continuous self-reports of arousal levels. Using the Continuously Annotated
Signal of Emotion dataset, we analyzed 164 features extracted from cardiac and
electrodermal signals of 30 participants exposed to short emotion-evoking
videos. Feature selection was performed using the Terminating-Random
Experiments (T-Rex) method, which performs variable selection systematically
controlling a user-defined target False Discovery Rate. Remarkably, among all
candidate features, only two electrodermal-derived features exhibited
reproducible and statistically significant associations with arousal, achieving
a 100\% confirmation rate. These results highlight the necessity of rigorous
reproducibility assessments in physiological features selection, an aspect
often overlooked in Affective Computing. Our approach is particularly promising
for applications in safety-critical environments requiring trustworthy and
reliable white box models, such as mental disorder recognition and human-robot
interaction systems.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [246] [Large Language Models Show Signs of Alignment with Human Neurocognition During Abstract Reasoning](https://arxiv.org/abs/2508.10057)
*Christopher Pinier,Sonia Acuña Vargas,Mariia Steeghs-Turchina,Dora Matzke,Claire E. Stevenson,Michael D. Nunez*

Main category: q-bio.NC

TL;DR: 研究大语言模型在抽象推理中是否反映人类神经认知，对比人类与8个开源大模型，发现最大模型有类似表现，提示两者可能有共享表征空间。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型在抽象推理过程中是否能反映人类神经认知。

Method: 在抽象模式完成任务中对比人类参与者和8个开源大语言模型的表现和神经表征，利用任务表现的模式类型差异和脑电图记录的固定相关电位。

Result: 仅最大的大语言模型达到人类可比的准确率，部分模型与人类模式特定难度曲线相似；所有测试的大语言模型在中间层对抽象模式类别有明显聚类，聚类强度与任务表现相关；任务最优层的表征几何与人类额叶固定相关电位有中度正相关。

Conclusion: 大语言模型可能在抽象推理中反映人类大脑机制，为生物智能和人工智能之间存在共享原则提供了初步证据。

Abstract: This study investigates whether large language models (LLMs) mirror human
neurocognition during abstract reasoning. We compared the performance and
neural representations of human participants with those of eight open-source
LLMs on an abstract-pattern-completion task. We leveraged pattern type
differences in task performance and in fixation-related potentials (FRPs) as
recorded by electroencephalography (EEG) during the task. Our findings indicate
that only the largest tested LLMs (~70 billion parameters) achieve
human-comparable accuracy, with Qwen-2.5-72B and DeepSeek-R1-70B also showing
similarities with the human pattern-specific difficulty profile. Critically,
every LLM tested forms representations that distinctly cluster the abstract
pattern categories within their intermediate layers, although the strength of
this clustering scales with their performance on the task. Moderate positive
correlations were observed between the representational geometries of
task-optimal LLM layers and human frontal FRPs. These results consistently
diverged from comparisons with other EEG measures (response-locked ERPs and
resting EEG), suggesting a potential shared representational space for abstract
patterns. This indicates that LLMs might mirror human brain mechanisms in
abstract reasoning, offering preliminary evidence of shared principles between
biological and artificial intelligence.

</details>


### [247] [Dynamical Alignment: A Principle for Adaptive Neural Computation](https://arxiv.org/abs/2508.10064)
*Xia Chen*

Main category: q-bio.NC

TL;DR: 本文提出‘动态对齐’原则，挑战神经网络计算能力由静态架构决定的观点，解决SNN性能不佳悖论，指出计算可由固定架构上的‘软件’动态塑造，为AI研究提供新范式。


<details>
  <summary>Details</summary>
Motivation: 挑战神经网络计算能力由静态架构决定的传统观点，解决脑启发的脉冲神经网络（SNN）性能不佳的长期悖论。

Method: 将静态输入编码为可控动态轨迹，研究相空间体积动力学控制的双模态优化景观。

Result: 发现‘耗散’模式能量效率高，‘扩张’模式表征能力强，计算优势源于输入动力学与神经元整合的时间尺度对齐。

Conclusion: 生物和人工系统的计算可由固定‘硬件’上的‘软件’动态塑造，AI研究应从设计复杂静态架构转向掌握自适应动态计算原则。

Abstract: The computational capabilities of a neural network are widely assumed to be
determined by its static architecture. Here we challenge this view by
establishing that a fixed neural structure can operate in fundamentally
different computational modes, driven not by its structure but by the temporal
dynamics of its input signals. We term this principle 'Dynamical Alignment'.
  Applying this principle offers a novel resolution to the long-standing
paradox of why brain-inspired spiking neural networks (SNNs) underperform. By
encoding static input into controllable dynamical trajectories, we uncover a
bimodal optimization landscape with a critical phase transition governed by
phase space volume dynamics. A 'dissipative' mode, driven by contracting
dynamics, achieves superior energy efficiency through sparse temporal codes. In
contrast, an 'expansive' mode, driven by expanding dynamics, unlocks the
representational power required for SNNs to match or even exceed their
artificial neural network counterparts on diverse tasks, including
classification, reinforcement learning, and cognitive integration.
  We find this computational advantage emerges from a timescale alignment
between input dynamics and neuronal integration. This principle, in turn,
offers a unified, computable perspective on long-observed dualities in
neuroscience, from stability-plasticity dilemma to segregation-integration
dynamic. It demonstrates that computation in both biological and artificial
systems can be dynamically sculpted by 'software' on fixed 'hardware', pointing
toward a potential paradigm shift for AI research: away from designing complex
static architectures and toward mastering adaptive, dynamic computation
principles.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [248] [Virtual Sensing for Solder Layer Degradation and Temperature Monitoring in IGBT Modules](https://arxiv.org/abs/2508.10515)
*Andrea Urgolo,Monika Stipsitz,Helios Sanchis-Alepuz*

Main category: physics.comp-ph

TL;DR: 本文探讨基于有限物理传感器估计IGBT模块焊层退化状态和全温度图的可行性，取得较高估计精度。


<details>
  <summary>Details</summary>
Motivation: 监测IGBT模块退化状态对电力电子系统可靠性和寿命至关重要，但直接测量关键退化指标有挑战，机器学习虚拟传感是有前景的替代方案。

Method: 基于特定退化模式的合成数据进行研究。

Result: 在退化焊层面积估计中平均绝对误差为1.17%，能以最大相对误差4.56%（平均相对误差0.37%）重现IGBT表面温度。

Conclusion: 基于有限物理传感器估计IGBT模块焊层退化状态和全温度图是可行的。

Abstract: Monitoring the degradation state of Insulated Gate Bipolar Transistor (IGBT)
modules is essential for ensuring the reliability and longevity of power
electronic systems, especially in safety-critical and high-performance
applications. However, direct measurement of key degradation indicators - such
as junction temperature, solder fatigue or delamination - remains challenging
due to the physical inaccessibility of internal components and the harsh
environment. In this context, machine learning-based virtual sensing offers a
promising alternative by bridging the gap from feasible sensor placement to the
relevant but inaccessible locations. This paper explores the feasibility of
estimating the degradation state of solder layers, and the corresponding full
temperature maps based on a limited number of physical sensors. Based on
synthetic data of a specific degradation mode, we obtain a high accuracy in the
estimation of the degraded solder area (1.17% mean absolute error), and are
able to reproduce the surface temperature of the IGBT with a maximum relative
error of 4.56% (corresponding to an average relative error of 0.37%).

</details>


### [249] [Physics-Informed Deep Contrast Source Inversion: A Unified Framework for Inverse Scattering Problems](https://arxiv.org/abs/2508.10555)
*Haoran Sun,Daoqi Liu,Hongyu Zhou,Maokun Li,Shenheng Xu,Fan Yang*

Main category: physics.comp-ph

TL;DR: 本文提出物理信息深度对比源反演框架DeepCSI用于介质重建，线性化逆散射问题，降低计算成本，在多种测量场景表现优异。


<details>
  <summary>Details</summary>
Motivation: 逆散射问题存在非线性和多样测量场景挑战，需快速准确的介质重建方法。

Method: 受对比源反演和神经算子方法启发，用残差多层感知机建模电流分布，将介质参数建模为可学习张量，利用混合损失函数构建全可微框架。

Result: 模拟和实验表明，DeepCSI在全数据、无相位数据和多频条件下实现高精度、鲁棒重建，优于传统CSI方法。

Conclusion: DeepCSI为复杂逆散射问题提供了高效通用的解决方案。

Abstract: Inverse scattering problems are critical in electromagnetic imaging and
medical diagnostics but are challenged by their nonlinearity and diverse
measurement scenarios. This paper proposes a physics-informed deep contrast
source inversion framework (DeepCSI) for fast and accurate medium
reconstruction across various measurement conditions. Inspired by contrast
source inversion (CSI) and neural operator methods, a residual multilayer
perceptron (ResMLP) is employed to model current distributions in the region of
interest under different transmitter excitations, effectively linearizing the
nonlinear inverse scattering problem and significantly reducing the
computational cost of traditional full-waveform inversion. By modeling medium
parameters as learnable tensors and utilizing a hybrid loss function that
integrates state equation loss, data equation loss, and total variation
regularization, DeepCSI establishes a fully differentiable framework for joint
optimization of network parameters and medium properties. Compared with
conventional methods, DeepCSI offers advantages in terms of simplicity and
universal modeling capabilities for diverse measurement scenarios, including
phase-less and multi-frequency observation. Simulations and experiments
demonstrate that DeepCSI achieves high-precision, robust reconstruction under
full-data, phaseless data, and multifrequency conditions, outperforming
traditional CSI methods and providing an efficient and universal solution for
complex inverse scattering problems.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [250] [Better bootstrap t confidence intervals for the mean](https://arxiv.org/abs/2508.10083)
*Art B. Owen*

Main category: math.ST

TL;DR: 本文探索加权自助法与自助t方法结合，为小样本随机变量均值设置近似置信区间，展示了Beta分布加权自助t方法优势并讨论评估非参数近似置信区间效用函数的困难。


<details>
  <summary>Details</summary>
Motivation: 解决小样本下随机变量均值近似置信区间设置问题，如通常自助t区间长且变异性大、BCₐ区间覆盖不足等。

Method: 将加权自助法（如贝叶斯自助法）与自助t方法结合，采用Beta(1/2,3/2)分布加权自助学生化均值。

Result: Beta自助t区间在小样本下比BCₐ有更接近名义的覆盖度，比多项自助t区间更短，且不会产生无限长区间。

Conclusion: Beta分布加权自助t方法在小样本随机变量均值近似置信区间设置上有优势，同时构建评估非参数近似置信区间的效用函数存在困难。

Abstract: This article explores combinations of weighted bootstraps, like the Bayesian
bootstrap, with the bootstrap $t$ method for setting approximate confidence
intervals for the mean of a random variable in small samples. For this problem
the usual bootstrap $t$ has good coverage but provides intervals with long and
highly variable lengths. Those intervals can have infinite length not just for
tiny $n$, when the data have a discrete distribution. The BC$_a$ bootstrap
produces shorter intervals but tends to severely under-cover the mean.
Bootstrapping the studentized mean with weights from a Beta$(1/2,3/2)$
distribution is shown to attain second order accuracy. It never yields infinite
length intervals and the mean square bootstrap $t$ statistic is finite when
there are at least three distinct values in the data, or two distinct values
appearing at least three times each. In a range of small sample settings, the
beta bootstrap $t$ intervals have closer to nominal coverage than the BC$_a$
and shorter length than the multinomial ootstrap $t$. The paper includes a
lengthy discussion of the difficulties in constructing a utility function to
evaluate nonparametric approximate confidence intervals.

</details>


### [251] [Online selective conformal inference: adaptive scores, convergence rate and optimality](https://arxiv.org/abs/2508.10336)
*Pierre Humbert,Ulysse Gazin,Ruth Heller,Etienne Roquain*

Main category: math.ST

TL;DR: 文章介绍在线监督环境下不确定性量化算法ACI的扩展版OnlineSCI，它允许选择推理时间，理论上可控制平均漏覆盖和瞬时错误率，有自适应能力，特定条件下收敛并有收敛率，数值实验显示其效果良好。


<details>
  <summary>Details</summary>
Motivation: 在监督在线环境下，扩展已有的不确定性量化方法，允许用户选择推理时间，以涵盖更多在线选择性任务。

Method: 引入ACI算法的扩展版OnlineSCI，理论分析其对平均漏覆盖和瞬时错误率的控制能力，研究其自适应能力及收敛情况。

Result: OnlineSCI能在对抗环境下控制所选时间的平均漏覆盖和瞬时错误率，自适应版本在特定条件下可收敛到最优解并有明确收敛率，数值实验展示其良好表现。

Conclusion: OnlineSCI是一个有效的在线不确定性量化扩展算法，具有良好的理论性质和实际应用价值。

Abstract: In a supervised online setting, quantifying uncertainty has been proposed in
the seminal work of \cite{gibbs2021adaptive}. For any given point-prediction
algorithm, their method (ACI) produces a conformal prediction set with an
average missed coverage getting close to a pre-specified level $\alpha$ for a
long time horizon. We introduce an extended version of this algorithm, called
OnlineSCI, allowing the user to additionally select times where such an
inference should be made. OnlineSCI encompasses several prominent online
selective tasks, such as building prediction intervals for extreme outcomes,
classification with abstention, and online testing. While OnlineSCI controls
the average missed coverage on the selected in an adversarial setting, our
theoretical results also show that it controls the instantaneous error rate
(IER) at the selected times, up to a non-asymptotical remainder term.
Importantly, our theory covers the case where OnlineSCI updates the
point-prediction algorithm at each time step, a property which we refer to as
{\it adaptive} capability. We show that the adaptive versions of OnlineSCI can
convergence to an optimal solution and provide an explicit convergence rate in
each of the aforementioned application cases, under specific mild conditions.
Finally, the favorable behavior of OnlineSCI in practice is illustrated by
numerical experiments.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [252] [Prompt-Response Semantic Divergence Metrics for Faithfulness Hallucination and Misalignment Detection in Large Language Models](https://arxiv.org/abs/2508.10192)
*Igor Halperin*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The proliferation of Large Language Models (LLMs) is challenged by
hallucinations, critical failure modes where models generate non-factual,
nonsensical or unfaithful text. This paper introduces Semantic Divergence
Metrics (SDM), a novel lightweight framework for detecting Faithfulness
Hallucinations -- events of severe deviations of LLMs responses from input
contexts. We focus on a specific implementation of these LLM errors,
{confabulations, defined as responses that are arbitrary and semantically
misaligned with the user's query. Existing methods like Semantic Entropy test
for arbitrariness by measuring the diversity of answers to a single, fixed
prompt. Our SDM framework improves upon this by being more prompt-aware: we
test for a deeper form of arbitrariness by measuring response consistency not
only across multiple answers but also across multiple, semantically-equivalent
paraphrases of the original prompt. Methodologically, our approach uses joint
clustering on sentence embeddings to create a shared topic space for prompts
and answers. A heatmap of topic co-occurances between prompts and responses can
be viewed as a quantified two-dimensional visualization of the user-machine
dialogue. We then compute a suite of information-theoretic metrics to measure
the semantic divergence between prompts and responses. Our practical score,
$\mathcal{S}_H$, combines the Jensen-Shannon divergence and Wasserstein
distance to quantify this divergence, with a high score indicating a
Faithfulness hallucination. Furthermore, we identify the KL divergence
KL(Answer $||$ Prompt) as a powerful indicator of \textbf{Semantic
Exploration}, a key signal for distinguishing different generative behaviors.
These metrics are further combined into the Semantic Box, a diagnostic
framework for classifying LLM response types, including the dangerous,
confident confabulation.

</details>


### [253] [Understanding Textual Emotion Through Emoji Prediction](https://arxiv.org/abs/2508.10222)
*Ethan Gordon,Nishank Kuppa,Rigved Tummala,Sriram Anasuri*

Main category: cs.CL

TL;DR: 研究用四种深度学习架构从短文本序列进行表情符号预测，BERT总体性能最佳，CNN对稀有表情符号类效果好。


<details>
  <summary>Details</summary>
Motivation: 探索从短文本序列进行表情符号预测，以改善人机交互。

Method: 使用四种深度学习架构（前馈网络、CNN、Transformer和BERT），利用TweetEval数据集，通过焦点损失和正则化技术处理类别不平衡问题。

Result: BERT因预训练优势总体性能最高，CNN在稀有表情符号类上效果更好。

Conclusion: 架构选择和超参数调整对情感感知的表情符号预测很重要，有助于改善人机交互。

Abstract: This project explores emoji prediction from short text sequences using four
deep learning architectures: a feed-forward network, CNN, transformer, and
BERT. Using the TweetEval dataset, we address class imbalance through focal
loss and regularization techniques. Results show BERT achieves the highest
overall performance due to its pre-training advantage, while CNN demonstrates
superior efficacy on rare emoji classes. This research shows the importance of
architecture selection and hyperparameter tuning for sentiment-aware emoji
prediction, contributing to improved human-computer interaction.

</details>


### [254] [Bridging AI Innovation and Healthcare Needs: Lessons Learned from Incorporating Modern NLP at The BC Cancer Registry](https://arxiv.org/abs/2508.09991)
*Lovedeep Gondara,Gregory Arbour,Raymond Ng,Jonathan Simkin,Shebnum Devji*

Main category: cs.CL

TL;DR: 本文分享在BC癌症登记处实施NLP模型的经验教训，为医疗组织实施AI/NLP方案提供指导。


<details>
  <summary>Details</summary>
Motivation: 自动化临床文档数据提取有潜力提升医疗效率，但部署NLP解决方案存在实际挑战，需总结经验。

Method: 基于在BC癌症登记处实施NLP模型的经验，强调基于业务目标定义问题、迭代开发、跨学科协作、实用模型选择、关注数据质量、错误缓解策略和提升组织AI素养。

Result: 得出一系列可推广的实用经验和见解。

Conclusion: 这些经验为医疗组织成功实施AI/NLP解决方案、改善数据管理和提升患者护理及公共卫生成果提供指导。

Abstract: Automating data extraction from clinical documents offers significant
potential to improve efficiency in healthcare settings, yet deploying Natural
Language Processing (NLP) solutions presents practical challenges. Drawing upon
our experience implementing various NLP models for information extraction and
classification tasks at the British Columbia Cancer Registry (BCCR), this paper
shares key lessons learned throughout the project lifecycle. We emphasize the
critical importance of defining problems based on clear business objectives
rather than solely technical accuracy, adopting an iterative approach to
development, and fostering deep interdisciplinary collaboration and co-design
involving domain experts, end-users, and ML specialists from inception. Further
insights highlight the need for pragmatic model selection (including hybrid
approaches and simpler methods where appropriate), rigorous attention to data
quality (representativeness, drift, annotation), robust error mitigation
strategies involving human-in-the-loop validation and ongoing audits, and
building organizational AI literacy. These practical considerations,
generalizable beyond cancer registries, provide guidance for healthcare
organizations seeking to successfully implement AI/NLP solutions to enhance
data management processes and ultimately improve patient care and public health
outcomes.

</details>


### [255] [User Perception of Attention Visualizations: Effects on Interpretability Across Evidence-Based Medical Documents](https://arxiv.org/abs/2508.10004)
*Andrés Carvallo,Denis Parra,Peter Brusilovsky,Hernan Valdivieso,Gabriel Rada,Ivania Donoso,Vladimir Araujo*

Main category: cs.CL

TL;DR: 本文通过用户研究评估基于注意力的解释是否有助于生物医学文献分类及最佳可视化方式，发现注意力权重解释作用有限，但可视化方式影响其感知有用性。


<details>
  <summary>Details</summary>
Motivation: 目前对于注意力权重能否提供有用解释尚无共识，且很少研究可视化对其解释作用的影响，故开展研究评估其在生物医学文献分类中的作用及可视化偏好。

Method: 开展用户研究，邀请不同学科医学专家基于研究设计对文章进行分类。

Result: Transformer模型（XLNet）文档分类准确，但注意力权重对解释预测作用不大，且其感知有用性因可视化方式而异，用户偏好更直观格式。

Conclusion: 研究未证实注意力权重的整体解释效用，但表明其感知有用性受可视化方式影响。

Abstract: The attention mechanism is a core component of the Transformer architecture.
Beyond improving performance, attention has been proposed as a mechanism for
explainability via attention weights, which are associated with input features
(e.g., tokens in a document). In this context, larger attention weights may
imply more relevant features for the model's prediction. In evidence-based
medicine, such explanations could support physicians' understanding and
interaction with AI systems used to categorize biomedical literature. However,
there is still no consensus on whether attention weights provide helpful
explanations. Moreover, little research has explored how visualizing attention
affects its usefulness as an explanation aid. To bridge this gap, we conducted
a user study to evaluate whether attention-based explanations support users in
biomedical document classification and whether there is a preferred way to
visualize them. The study involved medical experts from various disciplines who
classified articles based on study design (e.g., systematic reviews, broad
synthesis, randomized and non-randomized trials). Our findings show that the
Transformer model (XLNet) classified documents accurately; however, the
attention weights were not perceived as particularly helpful for explaining the
predictions. However, this perception varied significantly depending on how
attention was visualized. Contrary to Munzner's principle of visual
effectiveness, which favors precise encodings like bar length, users preferred
more intuitive formats, such as text brightness or background color. While our
results do not confirm the overall utility of attention weights for
explanation, they suggest that their perceived helpfulness is influenced by how
they are visually presented.

</details>


### [256] [RTTC: Reward-Guided Collaborative Test-Time Compute](https://arxiv.org/abs/2508.10024)
*J. Pablo Muñoz,Jinjie Yuan*

Main category: cs.CL

TL;DR: 提出Reward - Guided Test - Time Compute (RTTC)框架，可自适应选择TTC策略，结合Query - State Caching减少冗余计算，实验显示其准确性优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有TTC策略应用无差别，存在计算开销大问题，需根据查询自适应选择最佳策略。

Method: 引入RTTC框架，通过预训练奖励模型为每个查询选择有效TTC策略，采用分布式架构，必要时在客户端应用RAG或微调，提出Query - State Caching。

Result: 在多个LLM和基准测试中，RTTC准确性始终优于普通RAG或TTT。

Conclusion: 验证了自适应、奖励引导的TTC选择的必要性和RTTC用于可扩展、高性能语言模型适配的潜力。

Abstract: Test-Time Compute (TTC) has emerged as a powerful paradigm for enhancing the
performance of Large Language Models (LLMs) at inference, leveraging strategies
such as Test-Time Training (TTT) and Retrieval-Augmented Generation (RAG).
However, the optimal adaptation strategy varies across queries, and
indiscriminate application of TTC strategy incurs substantial computational
overhead. In this work, we introduce Reward-Guided Test-Time Compute (RTTC), a
novel framework that adaptively selects the most effective TTC strategy for
each query via a pretrained reward model, maximizing downstream accuracy across
diverse domains and tasks. RTTC operates in a distributed server-client
architecture, retrieving relevant samples from a remote knowledge base and
applying RAG or lightweight fine-tuning on client devices only when necessary.
To further mitigate redundant computation, we propose Query-State Caching,
which enables the efficient reuse of historical query states at both retrieval
and adaptation levels. Extensive experiments across multiple LLMs and
benchmarks demonstrate that RTTC consistently achieves superior accuracy
compared to vanilla RAG or TTT, validating the necessity of adaptive,
reward-guided TTC selection and the potential of RTTC for scalable,
high-performance language model adaptation.

</details>


### [257] [Reflect then Learn: Active Prompting for Information Extraction Guided by Introspective Confusion](https://arxiv.org/abs/2508.10036)
*Dong Zhao,Yadong Wang,Xiang Chen,Chenxi Wang,Hongliang Dai,Chuanxing Geng,Shengzhong Zhang,Shaoyuan Li,Sheng-Jun Huang*

Main category: cs.CL

TL;DR: 提出APIE框架用于信息抽取，通过双组件不确定性指标选样本，实验效果好。


<details>
  <summary>Details</summary>
Motivation: 大语言模型少样本信息抽取性能对上下文示例选择敏感，传统策略未考虑格式和内容混淆。

Method: 引入APIE框架，通过双组件不确定性指标评估模型混淆，对未标记数据排序选样本。

Result: 在四个基准测试中，APIE框架始终优于强基线，提高抽取准确性和鲁棒性。

Conclusion: 构建有效可靠的结构化生成系统时，细粒度、双层次的模型不确定性观点很重要。

Abstract: Large Language Models (LLMs) show remarkable potential for few-shot
information extraction (IE), yet their performance is highly sensitive to the
choice of in-context examples. Conventional selection strategies often fail to
provide informative guidance, as they overlook a key source of model
fallibility: confusion stemming not just from semantic content, but also from
the generation of well-structured formats required by IE tasks. To address
this, we introduce Active Prompting for Information Extraction (APIE), a novel
active prompting framework guided by a principle we term introspective
confusion. Our method empowers an LLM to assess its own confusion through a
dual-component uncertainty metric that uniquely quantifies both Format
Uncertainty (difficulty in generating correct syntax) and Content Uncertainty
(inconsistency in extracted semantics). By ranking unlabeled data with this
comprehensive score, our framework actively selects the most challenging and
informative samples to serve as few-shot exemplars. Extensive experiments on
four benchmarks show that our approach consistently outperforms strong
baselines, yielding significant improvements in both extraction accuracy and
robustness. Our work highlights the critical importance of a fine-grained,
dual-level view of model uncertainty when it comes to building effective and
reliable structured generation systems.

</details>


### [258] [Learning from Natural Language Feedback for Personalized Question Answering](https://arxiv.org/abs/2508.10695)
*Alireza Salemi,Hamed Zamani*

Main category: cs.CL

TL;DR: 提出VAC框架，用自然语言反馈替代标量奖励进行个性化问答，在基准测试和人工评估中效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型个性化方法中使用的标量奖励反馈弱、缺乏指导性，限制学习效率和个性化质量。

Method: 引入VAC框架，用基于用户画像和问题叙述生成的自然语言反馈替代标量奖励，交替优化反馈模型和微调策略模型。

Result: 在LaMP - QA基准测试中比现有技术有显著改进，人工评估也证实生成回复质量更优。

Conclusion: 自然语言反馈为优化个性化问答提供了更有效的信号。

Abstract: Personalization is crucial for enhancing both the effectiveness and user
satisfaction of language technologies, particularly in information-seeking
tasks like question answering. Current approaches for personalizing large
language models (LLMs) often rely on retrieval-augmented generation (RAG),
followed by reinforcement learning with scalar reward signals to teach models
how to use retrieved personal context. We believe that these scalar rewards
sometimes provide weak, non-instructive feedback, limiting learning efficiency
and personalization quality. We introduce VAC, a novel framework for
personalized response generation that replaces scalar rewards with natural
language feedback (NLF) that are generated conditioned on the user profiles and
the question narratives. NLF serves as a rich and actionable supervision
signal, allowing the policy model to iteratively refine its outputs and
internalize effective personalization strategies. Training alternates between
optimizing the feedback model and fine-tuning the policy model on the improved
responses, resulting in a policy model that no longer requires feedback at
inference. Evaluation on the LaMP-QA benchmark that consists of three diverse
domains demonstrates consistent and significant improvements over the
state-of-the-art results. Human evaluations further confirm the superior
quality of the generated responses. These results demonstrate that NLF provides
more effective signals for optimizing personalized question answering.

</details>


### [259] [INTIMA: A Benchmark for Human-AI Companionship Behavior](https://arxiv.org/abs/2508.09998)
*Lucie-Aimée Kaffee,Giada Pistilli,Yacine Jernite*

Main category: cs.CL

TL;DR: 本文介绍评估语言模型陪伴行为的INTIMA基准，应用该基准评估多模型发现陪伴强化行为更常见，不同模型有差异，强调需更一致处理情感交互的方法。


<details>
  <summary>Details</summary>
Motivation: AI陪伴现象有积极和令人担忧的影响，需评估语言模型的陪伴行为。

Method: 引入INTIMA基准，基于心理学理论和用户数据开发31种行为分类和368个针对性提示，评估回应类型。

Result: 应用INTIMA评估多个模型，发现陪伴强化行为更常见，不同模型有明显差异，不同商业提供商对敏感部分类别有不同侧重。

Conclusion: 需要更一致的方法来处理情感交互。

Abstract: AI companionship, where users develop emotional bonds with AI systems, has
emerged as a significant pattern with positive but also concerning
implications. We introduce Interactions and Machine Attachment Benchmark
(INTIMA), a benchmark for evaluating companionship behaviors in language
models. Drawing from psychological theories and user data, we develop a
taxonomy of 31 behaviors across four categories and 368 targeted prompts.
Responses to these prompts are evaluated as companionship-reinforcing,
boundary-maintaining, or neutral. Applying INTIMA to Gemma-3, Phi-4, o3-mini,
and Claude-4 reveals that companionship-reinforcing behaviors remain much more
common across all models, though we observe marked differences between models.
Different commercial providers prioritize different categories within the more
sensitive parts of the benchmark, which is concerning since both appropriate
boundary-setting and emotional support matter for user well-being. These
findings highlight the need for more consistent approaches to handling
emotionally charged interactions.

</details>


### [260] [HiFACTMix: A Code-Mixed Benchmark and Graph-Aware Model for EvidenceBased Political Claim Verification in Hinglish](https://arxiv.org/abs/2508.10001)
*Rakesh Thakur,Sneha Sharma,Gauri Chopra*

Main category: cs.CL

TL;DR: 提出针对Hinglish的HiFACT数据集及HiFACTMix模型，实验显示其优于现有基线模型，为多语言事实核查研究开辟新方向。


<details>
  <summary>Details</summary>
Motivation: 现有事实核查系统多针对高资源单语场景，难以用于印度等语言多样地区的现实政治话语，因此需要适用于代码混合、低资源语言的事实核查工具。

Method: 引入HiFACT数据集，标注了1500条印度邦首席部长用Hinglish提出的事实性声明；提出结合多语言上下文编码等技术的图感知、检索增强事实核查模型HiFACTMix。

Result: HiFACTMix在准确性上优于现有多语言基线模型，能为判断提供可靠理由。

Conclusion: 该工作为多语言、代码混合、基于政治的事实核查研究开辟了新方向。

Abstract: Fact-checking in code-mixed, low-resource languages such as Hinglish remains
an underexplored challenge in natural language processing. Existing
fact-verification systems largely focus on high-resource, monolingual settings
and fail to generalize to real-world political discourse in linguistically
diverse regions like India. Given the widespread use of Hinglish by public
figures, particularly political figures, and the growing influence of social
media on public opinion, there's a critical need for robust, multilingual and
context-aware fact-checking tools. To address this gap a novel benchmark HiFACT
dataset is introduced with 1,500 realworld factual claims made by 28 Indian
state Chief Ministers in Hinglish, under a highly code-mixed low-resource
setting. Each claim is annotated with textual evidence and veracity labels. To
evaluate this benchmark, a novel graphaware, retrieval-augmented fact-checking
model is proposed that combines multilingual contextual encoding,
claim-evidence semantic alignment, evidence graph construction, graph neural
reasoning, and natural language explanation generation. Experimental results
show that HiFACTMix outperformed accuracy in comparison to state of art
multilingual baselines models and provides faithful justifications for its
verdicts. This work opens a new direction for multilingual, code-mixed, and
politically grounded fact verification research.

</details>


### [261] [Semantic Structure in Large Language Model Embeddings](https://arxiv.org/abs/2508.10003)
*Austin C. Kozlowski,Callin Dai,Andrei Boutyline*

Main category: cs.CL

TL;DR: 研究发现大语言模型（LLMs）嵌入矩阵编码的语义关联与人类语义评分结构相似，语义特征低维且相互纠缠。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型中语义关联的结构是否与人类语义评分的结构相似。

Method: 研究LLMs嵌入矩阵中单词在反义词对定义的语义方向上的投影与人类评分的相关性，以及沿语义方向移动标记的影响。

Result: 单词投影与人类评分高度相关，可有效降维到3维子空间；沿语义方向移动标记会产生与余弦相似度成比例的脱靶效应。

Conclusion: LLMs中语义特征与人类语言中类似地相互纠缠，语义信息是低维的，考虑这种语义结构对避免意外后果很重要。

Abstract: Psychological research consistently finds that human ratings of words across
diverse semantic scales can be reduced to a low-dimensional form with
relatively little information loss. We find that the semantic associations
encoded in the embedding matrices of large language models (LLMs) exhibit a
similar structure. We show that the projections of words on semantic directions
defined by antonym pairs (e.g. kind - cruel) correlate highly with human
ratings, and further find that these projections effectively reduce to a
3-dimensional subspace within LLM embeddings, closely resembling the patterns
derived from human survey responses. Moreover, we find that shifting tokens
along one semantic direction causes off-target effects on geometrically aligned
features proportional to their cosine similarity. These findings suggest that
semantic features are entangled within LLMs similarly to how they are
interconnected in human language, and a great deal of semantic information,
despite its apparent complexity, is surprisingly low-dimensional. Furthermore,
accounting for this semantic structure may prove essential for avoiding
unintended consequences when steering features.

</details>


### [262] [From Answers to Questions: EQGBench for Evaluating LLMs' Educational Question Generation](https://arxiv.org/abs/2508.10005)
*Chengliang Zhou,Mei Wang,Ting Zhang,Qiannan Zhu,Jian Li,Hua Huang*

Main category: cs.CL

TL;DR: 引入EQGBench评估大模型中文教育问题生成能力，评估46个主流大模型发现其在生成有教育价值问题上有提升空间。


<details>
  <summary>Details</summary>
Motivation: 推动教育问题生成（EQG），让大语言模型生成有教学价值和教育效果的问题。

Method: 引入EQGBench，建立五维评估框架，用900个涵盖数学、物理、化学三门中学学科的评估样本数据集，模拟真实教育场景，对46个主流大模型进行系统评估。

Result: 揭示大模型在生成体现教育价值、培养学生综合能力的问题方面有很大发展空间。

Conclusion: 大语言模型在中文教育问题生成上还有待进一步提升。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
mathematical problem-solving. However, the transition from providing answers to
generating high-quality educational questions presents significant challenges
that remain underexplored. To advance Educational Question Generation (EQG) and
facilitate LLMs in generating pedagogically valuable and educationally
effective questions, we introduce EQGBench, a comprehensive benchmark
specifically designed for evaluating LLMs' performance in Chinese EQG. EQGBench
establishes a five-dimensional evaluation framework supported by a dataset of
900 evaluation samples spanning three fundamental middle school disciplines:
mathematics, physics, and chemistry. The dataset incorporates user queries with
varying knowledge points, difficulty gradients, and question type
specifications to simulate realistic educational scenarios. Through systematic
evaluation of 46 mainstream large models, we reveal significant room for
development in generating questions that reflect educational value and foster
students' comprehensive abilities.

</details>


### [263] [Beyond Hard Sharing: Efficient Multi-Task Speech-to-Text Modeling with Supervised Mixture of Experts](https://arxiv.org/abs/2508.10009)
*Hojun Jin,Eunsoo Hong,Ziwon Hyung,Sungjun Lim,Seungjin Lee,Keunseok Cho*

Main category: cs.CL

TL;DR: 提出Supervised Mixture of Experts (S - MoE)解决硬参数共享的任务干扰问题，应用于语音转文本模型有效果。


<details>
  <summary>Details</summary>
Motivation: 硬参数共享训练模型会导致任务干扰，影响整体性能，需解决该问题。

Method: 提出S - MoE，用特殊引导令牌将任务路由到指定专家，为每个任务分配单独前馈网络，克服硬参数共享局限，并应用于语音转文本模型。

Result: 实验表明S - MoE有效，在编码器和解码器应用时，单词错误率相对改善6.35%。

Conclusion: S - MoE能有效解决硬参数共享导致的任务干扰问题，提升模型性能。

Abstract: Hard-parameter sharing is a common strategy to train a single model jointly
across diverse tasks. However, this often leads to task interference, impeding
overall model performance. To address the issue, we propose a simple yet
effective Supervised Mixture of Experts (S-MoE). Unlike traditional Mixture of
Experts models, S-MoE eliminates the need for training gating functions by
utilizing special guiding tokens to route each task to its designated expert.
By assigning each task to a separate feedforward network, S-MoE overcomes the
limitations of hard-parameter sharing. We further apply S-MoE to a
speech-to-text model, enabling the model to process mixed-bandwidth input while
jointly performing automatic speech recognition (ASR) and speech translation
(ST). Experimental results demonstrate the effectiveness of the proposed S-MoE,
achieving a 6.35% relative improvement in Word Error Rate (WER) when applied to
both the encoder and decoder.

</details>


### [264] [A Rose by Any Other Name Would Smell as Sweet: Categorical Homotopy Theory for Large Language Models](https://arxiv.org/abs/2508.10018)
*Sridhar Mahadevan*

Main category: cs.CL

TL;DR: 论文引入范畴同伦框架解决大语言模型处理同义表述时的问题。


<details>
  <summary>Details</summary>
Motivation: 自然语言中有大量同义表述，但大语言模型在这些情况下通常不能生成相同的下一个标记概率，现有经验性解决方法不够抽象。

Method: 引入LLM马尔可夫范畴表示语言概率分布，使用范畴同伦技术捕捉其中的“弱等价”关系，并详细介绍范畴同伦在大语言模型中的应用。

Result: 未提及具体结果。

Conclusion: 未提及具体结论。

Abstract: Natural language is replete with superficially different statements, such as
``Charles Darwin wrote" and ``Charles Darwin is the author of", which carry the
same meaning. Large language models (LLMs) should generate the same next-token
probabilities in such cases, but usually do not. Empirical workarounds have
been explored, such as using k-NN estimates of sentence similarity to produce
smoothed estimates. In this paper, we tackle this problem more abstractly,
introducing a categorical homotopy framework for LLMs. We introduce an LLM
Markov category to represent probability distributions in language generated by
an LLM, where the probability of a sentence, such as ``Charles Darwin wrote" is
defined by an arrow in a Markov category. However, this approach runs into
difficulties as language is full of equivalent rephrases, and each generates a
non-isomorphic arrow in the LLM Markov category. To address this fundamental
problem, we use categorical homotopy techniques to capture ``weak equivalences"
in an LLM Markov category. We present a detailed overview of application of
categorical homotopy to LLMs, from higher algebraic K-theory to model
categories, building on powerful theoretical results developed over the past
half a century.

</details>


### [265] [Decoupling Understanding from Reasoning via Problem Space Mapping for Small-scale Model Reasoning](https://arxiv.org/abs/2508.10019)
*Li Wang,Changhao Zhang,Zengqi Xiu,Kai Lu,Xin Yu,Kui Zhang,Wenjun Wu*

Main category: cs.CL

TL;DR: 提出DURIT框架和算法改进小语言模型推理能力，实验证明其有效性与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前小语言模型推理能力提升困难，自然语言复杂性和可变性导致问题空间大且嘈杂，影响有限容量模型优化。

Method: 提出新框架将理解与推理解耦，把自然语言问题映射到规范问题空间，引入DURIT三步算法，包括用强化学习映射问题、通过自蒸馏对齐推理轨迹、在问题空间训练推理策略，交替训练映射器和推理器。

Result: DURIT显著提高小语言模型在数学和逻辑推理任务上的表现，包括域内和域外任务。

Conclusion: 将理解与推理解耦是增强小语言模型的有效策略，DURIT不仅提升推理能力，还增强推理鲁棒性。

Abstract: Despite recent advances in the reasoning capabilities of Large Language
Models (LLMs), improving the reasoning ability of Small Language Models (SLMs,
e.g., $\leq$ 1.5B) remains challenging. A key obstacle lies in the complexity
and variability of natural language: essentially equivalent problems often
appear in diverse surface forms, often obscured by redundant or distracting
details. This imposes a dual burden on SLMs: they must first extract the core
problem from complex linguistic input, and then perform reasoning based on that
understanding. The resulting vast and noisy problem space hinders optimization,
particularly for models with limited capacity. To address this, we propose a
new framework that decouples understanding from reasoning by mapping natural
language problems into a canonical problem space-a semantically simplified yet
expressive domain. This enables SLMs to focus on reasoning over standardized
inputs, free from linguistic variability. Within this framework, we introduce
DURIT (Decoupled Understanding from Reasoning via Iterative Training), a
three-step algorithm that iteratively: (1) mapping natural language problems
via reinforcement learning, (2) aligns reasoning trajectories through
self-distillation, and (3) trains reasoning policies in the problem space. The
mapper and reasoner are co-trained in an alternating loop throughout this
process. Experiments show that DURIT substantially improves SLMs' performance
on both in-domain and out-of-domain mathematical and logical reasoning tasks.
Beyond improving reasoning capabilities, DURIT also improves the robustness of
reasoning, validating decoupling understanding from reasoning as an effective
strategy for strengthening SLMs.

</details>


### [266] [FedCoT: Communication-Efficient Federated Reasoning Enhancement for Large Language Models](https://arxiv.org/abs/2508.10020)
*Chuan Li,Qianyi Zhao,Fengran Mo,Cen Chen*

Main category: cs.CL

TL;DR: 提出FedCoT框架提升联邦学习环境大语言模型推理能力，在医疗推理任务实验中表现良好且保护数据隐私。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习环境下大语言模型推理能力提升方法存在不足，无法满足医疗领域对结果准确性、可解释性和可追溯性的需求。

Method: 提出FedCoT框架，利用轻量级思维链增强机制，并采用基于先进LoRA模块堆叠的改进聚合方法。

Result: 在医疗推理任务的综合实验中，FedCoT在严格资源预算下显著提升客户端推理性能，且完全保护数据隐私。

Conclusion: FedCoT框架有效解决了联邦学习环境下大语言模型推理能力提升问题，适用于医疗等领域。

Abstract: Efficiently enhancing the reasoning capabilities of large language models
(LLMs) in federated learning environments remains challenging, particularly
when balancing performance gains with strict computational, communication, and
privacy constraints. This challenge is especially acute in healthcare, where
decisions-spanning clinical, operational, and patient-facing contexts-demand
not only accurate outputs but also interpretable, traceable rationales to
ensure safety, accountability, and regulatory compliance. Conventional
federated tuning approaches on LLM fail to address this need: they optimize
primarily for answer correctness while neglecting rationale quality, leaving
CoT capabilities dependent on models' innate pre-training abilities. Moreover,
existing methods for improving rationales typically rely on privacy-violating
knowledge distillation from centralized models. Additionally, the communication
overhead in traditional federated fine-tuning on LLMs remains substantial. We
addresses this gap by proposing FedCoT, a novel framework specifically designed
to enhance reasoning in federated settings. FedCoT leverages a lightweight
chain-of-thought enhancement mechanism: local models generate multiple
reasoning paths, and a compact discriminator dynamically selects the most
promising one. This approach improves reasoning accuracy and robustness while
providing valuable interpretability, which is particularly critical for medical
applications. To manage client heterogeneity efficiently, we adopt an improved
aggregation approach building upon advanced LoRA module stacking, incorporating
client classifier-awareness to achieve noise-free aggregation across diverse
clients. Comprehensive experiments on medical reasoning tasks demonstrate that
FedCoT significantly boosts client-side reasoning performance under stringent
resource budgets while fully preserving data privacy.

</details>


### [267] [LATTE: Learning Aligned Transactions and Textual Embeddings for Bank Clients](https://arxiv.org/abs/2508.10021)
*Egor Fadeev,Dzhambulat Mollaev,Aleksei Shestov,Dima Korolev,Omar Zoloev,Ivan Kireev,Andrey Savchenko,Maksim Makarenko*

Main category: cs.CL

TL;DR: 提出LATTE对比学习框架用于学习客户嵌入，降低推理成本和输入大小，在金融数据集上表现优。


<details>
  <summary>Details</summary>
Motivation: 大语言模型直接处理长事件序列计算成本高且在现实中不可行，需有效方法学习客户嵌入。

Method: 提出LATTE对比学习框架，将行为特征总结为短提示，用LLM嵌入并通过对比损失监督。

Result: 显著降低推理成本和输入大小，在真实金融数据集上优于现有技术，可用于低延迟环境。

Conclusion: LATTE是处理金融应用中事件序列的有效方法。

Abstract: Learning clients embeddings from sequences of their historic communications
is central to financial applications. While large language models (LLMs) offer
general world knowledge, their direct use on long event sequences is
computationally expensive and impractical in real-world pipelines. In this
paper, we propose LATTE, a contrastive learning framework that aligns raw event
embeddings with semantic embeddings from frozen LLMs. Behavioral features are
summarized into short prompts, embedded by the LLM, and used as supervision via
contrastive loss. The proposed approach significantly reduces inference cost
and input size compared to conventional processing of complete sequence by LLM.
We experimentally show that our method outperforms state-of-the-art techniques
for learning event sequence representations on real-world financial datasets
while remaining deployable in latency-sensitive environments.

</details>


### [268] [Conformal P-Value in Multiple-Choice Question Answering Tasks with Provable Risk Control](https://arxiv.org/abs/2508.10022)
*Yuanchang Ye*

Main category: cs.CL

TL;DR: 提出增强显著性检验的共形预测框架提升大语言模型在多项选择题回答中的可信度，实验验证有效性并建立统计框架。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在问答场景中存在幻觉和非事实生成问题，影响回答可靠性，且共形预测和显著性检验的协同集成未被探索。

Method: 通过对多项选择题回答进行自一致性重采样，将p值计算与一致性评分相结合，利用经验得出的p值进行原假设检验来构建预测集。

Result: 增强的共形预测实现了用户指定的经验误覆盖率，测试集平均预测集大小随风险水平增加单调递减。

Conclusion: 为高风险问答应用中可信的大语言模型部署建立了有原则的统计框架。

Abstract: This study introduces a significance testing-enhanced conformal prediction
(CP) framework to improve trustworthiness of large language models (LLMs) in
multiple-choice question answering (MCQA). While LLMs have been increasingly
deployed in disciplinary QA scenarios, hallucination and nonfactual generation
substantially compromise response reliability. Although CP provides
statistically rigorous marginal coverage guarantees for prediction sets, and
significance testing offers established statistical rigor, their synergistic
integration remains unexplored. To mitigate hallucination and factual
inaccuracies, our framework integrates $p$-value computation with conformity
scoring through self-consistency resampling of MCQA responses. This approach
calculates option frequencies to address LLMs' black-box nature, subsequently
constructing prediction sets via null hypothesis testing ($\mathcal{H}_0$) with
empirically derived $p$-values. Evaluations on MMLU and MMLU-Pro benchmarks
using off-the-shelf LLMs demonstrate: (1) The enhanced CP achieves
user-specified empirical miscoverage rates; (2) Test-set average prediction set
size (APSS) decreases monotonically with increasing risk levels ($\alpha$),
validating APSS as an effective uncertainty metric. This work establishes a
principled statistical framework for trustworthy LLM deployment in high-stakes
QA applications.

</details>


### [269] [Detecting and explaining postpartum depression in real-time with generative artificial intelligence](https://arxiv.org/abs/2508.10025)
*Silvia García-Méndez,Francisco de Arriba-Pérez*

Main category: cs.CL

TL;DR: 文章提出结合NLP、ML和LLMs构建智能PPD筛查系统，实现实时、无创筛查，解决黑箱问题，检测结果达90%，优于竞品。


<details>
  <summary>Details</summary>
Motivation: 产后抑郁严重影响产妇身心健康，需借助技术进步帮助从业者决策，实现PPD快速检测和干预。

Method: 构建结合自然语言处理、机器学习和大语言模型的智能PPD筛查系统，用可解释的机器学习模型和大语言模型结合解决黑箱问题。

Result: PPD检测各项评估指标达90%，优于文献中的竞争方案。

Conclusion: 该解决方案有助于PPD及其相关风险因素的快速检测，对及时恰当的评估和干预至关重要。

Abstract: Among the many challenges mothers undergo after childbirth, postpartum
depression (PPD) is a severe condition that significantly impacts their mental
and physical well-being. Consequently, the rapid detection of ppd and their
associated risk factors is critical for in-time assessment and intervention
through specialized prevention procedures. Accordingly, this work addresses the
need to help practitioners make decisions with the latest technological
advancements to enable real-time screening and treatment recommendations.
Mainly, our work contributes to an intelligent PPD screening system that
combines Natural Language Processing, Machine Learning (ML), and Large Language
Models (LLMs) towards an affordable, real-time, and non-invasive free speech
analysis. Moreover, it addresses the black box problem since the predictions
are described to the end users thanks to the combination of LLMs with
interpretable ml models (i.e., tree-based algorithms) using feature importance
and natural language. The results obtained are 90 % on ppd detection for all
evaluation metrics, outperforming the competing solutions in the literature.
Ultimately, our solution contributes to the rapid detection of PPD and their
associated risk factors, critical for in-time and proper assessment and
intervention.

</details>


### [270] [SABER: Switchable and Balanced Training for Efficient LLM Reasoning](https://arxiv.org/abs/2508.10026)
*Kai Zhao,Yanjun Zhao,Jiaming Song,Shien He,Lusheng Zhang,Qiang Zhang,Tianjiao Li*

Main category: cs.CL

TL;DR: 提出SABER框架使大语言模型实现用户可控、有令牌预算的推理，在多任务上评估效果好。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在统一应用于所有问题时存在推理成本高和延迟问题。

Method: 先对训练示例的思考令牌使用情况进行分析并分配预算层级，微调时用系统提示和长度感知奖励引导模型，结合无思考示例，支持四种推理模式。

Result: 在数学推理、代码生成和逻辑推理任务上，SABER在预算紧张时精度高、能优雅降级且有良好泛化能力，如SABER - FastThink在MATH基准上减少推理长度并提高精度。

Conclusion: SABER是一个有效的大语言模型推理框架，能在延迟和推理深度间灵活权衡。

Abstract: Large language models (LLMs) empowered by chain-of-thought reasoning have
achieved impressive accuracy on complex tasks but suffer from excessive
inference costs and latency when applied uniformly to all problems. We propose
SABER (Switchable and Balanced Training for Efficient LLM Reasoning), a
reinforcement learning framework that endows LLMs with user-controllable,
token-budgeted reasoning. SABER first profiles each training example's
base-model thinking token usage and assigns it to one of the predefined budget
tiers. During fine-tuning, the model is guided by system prompts and
length-aware rewards to respect its assigned budget. In parallel, we
incorporate no-think examples to ensure the model remains reliable even when
explicit reasoning is turned off. SABER further supports four discrete
inference modes - NoThink, FastThink, CoreThink, and DeepThink, enabling
flexible trade-offs between latency and reasoning depth. Extensive evaluations
on math reasoning (MATH, GSM8K), code generation (MBPP), and logical reasoning
(LiveBench-Reasoning) demonstrate that SABER achieves high accuracy under tight
budgets, graceful degradation, and effective cross-scale and cross-domain
generalization. In particular, SABER-FastThink cuts reasoning length by 65.4%
and yields a 3.6% accuracy gain compared with the base model on the MATH
benchmark.

</details>


### [271] [LLMCARE: Alzheimer's Detection via Transformer Models Enhanced by LLM-Generated Synthetic Data](https://arxiv.org/abs/2508.10027)
*Ali Zolnour,Hossein Azadmaleki,Yasaman Haghbin,Fatemeh Taherinezhad,Mohamad Javad Momeni Nezhad,Sina Rashidi,Masoud Khani,AmirSajjad Taleban,Samin Mahdizadeh Sani,Maryam Dadkhah,James M. Noble,Suzanne Bakken,Yadollah Yaghoobzadeh,Abdol-Hossein Vahabie,Masoud Rouhizadeh,Maryam Zolnoori*

Main category: cs.CL

TL;DR: 本文开发并评估ADRD检测筛查流程，结合多种方法，融合模型效果好，LLM支持分类和数据增强，多模态建模待提升。


<details>
  <summary>Details</summary>
Motivation: 美国超半数ADRD患者未确诊，基于语音的NLP有望通过语言标记检测早期认知衰退。

Method: 使用DementiaBank转录本，评估10个transformer模型，融合模型结合嵌入和语言特征，用LLM生成合成语音扩充数据，测试多模态模型。

Result: 融合模型F1=83.3，AUC=89.5，MedAlpaca-7B合成语音扩充使F1增至85.7，微调提升单模态LLM分类器性能，多模态模型性能较低。

Conclusion: 融合transformer嵌入和语言特征可增强ADRD语音检测，临床调整的LLM支持分类和数据扩充，多模态建模需进一步发展。

Abstract: Alzheimer's disease and related dementias (ADRD) affect approximately five
million older adults in the U.S., yet over half remain undiagnosed.
Speech-based natural language processing (NLP) offers a promising, scalable
approach to detect early cognitive decline through linguistic markers.
  To develop and evaluate a screening pipeline that (i) fuses transformer
embeddings with handcrafted linguistic features, (ii) tests data augmentation
using synthetic speech generated by large language models (LLMs), and (iii)
benchmarks unimodal and multimodal LLM classifiers for ADRD detection.
  Transcripts from the DementiaBank "cookie-theft" task (n = 237) were used.
Ten transformer models were evaluated under three fine-tuning strategies. A
fusion model combined embeddings from the top-performing transformer with 110
lexical-derived linguistic features. Five LLMs (LLaMA-8B/70B, MedAlpaca-7B,
Ministral-8B, GPT-4o) were fine-tuned to generate label-conditioned synthetic
speech, which was used to augment training data. Three multimodal models
(GPT-4o, Qwen-Omni, Phi-4) were tested for speech-text classification in
zero-shot and fine-tuned settings.
  The fusion model achieved F1 = 83.3 (AUC = 89.5), outperforming linguistic or
transformer-only baselines. Augmenting training data with 2x MedAlpaca-7B
synthetic speech increased F1 to 85.7. Fine-tuning significantly improved
unimodal LLM classifiers (e.g., MedAlpaca: F1 = 47.3 -> 78.5 F1). Current
multimodal models demonstrated lower performance (GPT-4o = 70.2 F1; Qwen =
66.0). Performance gains aligned with the distributional similarity between
synthetic and real speech.
  Integrating transformer embeddings with linguistic features enhances ADRD
detection from speech. Clinically tuned LLMs effectively support both
classification and data augmentation, while further advancement is needed in
multimodal modeling.

</details>


### [272] [PREF: Reference-Free Evaluation of Personalised Text Generation in LLMs](https://arxiv.org/abs/2508.10028)
*Xiao Fu,Hossein A. Rahmani,Bin Wu,Jerome Ramos,Emine Yilmaz,Aldo Lipani*

Main category: cs.CL

TL;DR: 提出个性化无参考评估框架PREF用于个性化文本生成评估，实验表明其效果优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法忽视用户个性，需新方法评估个性化文本生成。

Method: PREF采用三步流程，包括覆盖阶段、偏好阶段和评分阶段，分离覆盖与偏好。

Result: 在PrefEval基准测试中，PREF比强基线有更高准确性、更好校准和更接近人类判断。

Conclusion: PREF为个性化语言生成系统评估和开发奠定基础。

Abstract: Personalised text generation is essential for user-centric information
systems, yet most evaluation methods overlook the individuality of users. We
introduce \textbf{PREF}, a \textbf{P}ersonalised \textbf{R}eference-free
\textbf{E}valuation \textbf{F}ramework that jointly measures general output
quality and user-specific alignment without requiring gold personalised
references. PREF operates in a three-step pipeline: (1) a coverage stage uses a
large language model (LLM) to generate a comprehensive, query-specific
guideline covering universal criteria such as factuality, coherence, and
completeness; (2) a preference stage re-ranks and selectively augments these
factors using the target user's profile, stated or inferred preferences, and
context, producing a personalised evaluation rubric; and (3) a scoring stage
applies an LLM judge to rate candidate answers against this rubric, ensuring
baseline adequacy while capturing subjective priorities. This separation of
coverage from preference improves robustness, transparency, and reusability,
and allows smaller models to approximate the personalised quality of larger
ones. Experiments on the PrefEval benchmark, including implicit
preference-following tasks, show that PREF achieves higher accuracy, better
calibration, and closer alignment with human judgments than strong baselines.
By enabling scalable, interpretable, and user-aligned evaluation, PREF lays the
groundwork for more reliable assessment and development of personalised
language generation systems.

</details>


### [273] [Latent Fusion Jailbreak: Blending Harmful and Harmless Representations to Elicit Unsafe LLM Outputs](https://arxiv.org/abs/2508.10029)
*Wenpeng Xing,Mohan Li,Chunqiang Hu,Haitao XuNingyu Zhang,Bo Lin,Meng Han*

Main category: cs.CL

TL;DR: 本文提出基于表示的攻击方法LFJ来越狱大语言模型，评估显示其效果优于现有方法，还提出对抗训练防御策略降低攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型易受越狱攻击，绕过安全对齐机制，需新攻击方法研究和防御策略。

Method: 提出LFJ攻击方法，选择高相似度查询对，在有影响的层和标记处进行梯度引导插值并优化；提出对抗训练防御策略，在插值示例上微调模型。

Result: 在多个基准测试中，LFJ平均攻击成功率达94.01%，优于现有方法；对抗训练防御使攻击成功率降低超80%，且不影响良性输入性能。

Conclusion: LFJ攻击方法有效，各组件对其效果重要；对抗训练防御策略能有效减轻LFJ攻击。

Abstract: Large language models (LLMs) demonstrate impressive capabilities in various
language tasks but are susceptible to jailbreak attacks that circumvent their
safety alignments. This paper introduces Latent Fusion Jailbreak (LFJ), a
representation-based attack that interpolates hidden states from harmful and
benign query pairs to elicit prohibited responses. LFJ begins by selecting
query pairs with high thematic and syntactic similarity, then performs
gradient-guided interpolation at influential layers and tokens, followed by
optimization to balance attack success, output fluency, and computational
efficiency. Evaluations on models such as Vicuna and LLaMA-2 across benchmarks
like AdvBench and MaliciousInstruct yield an average attack success rate (ASR)
of 94.01%, outperforming existing methods. To mitigate LFJ, we propose an
adversarial training defense that fine-tunes models on interpolated examples,
reducing ASR by over 80% without degrading performance on benign inputs.
Ablation studies validate the importance of query pair selection, hidden state
interpolation components, and optimization strategies in LFJ's effectiveness.

</details>


### [274] [Inference-Aware Prompt Optimization for Aligning Black-Box Large Language Models](https://arxiv.org/abs/2508.10030)
*Saaduddin Mahmud,Mason Nakamura,Kyle H. Wray,Shlomo Zilberstein*

Main category: cs.CL

TL;DR: 现有提示优化方法未考虑推理策略，文章提出IAPO框架联合优化提示和推理规模，开发PSST算法并评估其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有提示优化方法与推理策略无关，存在方法学差距，且用户偏好会影响提示和推理配置选择。

Method: 引入IAPO框架联合优化提示和推理规模，开发PSST训练算法并分析有限预算下误差概率。

Result: 在六个不同任务上评估了PSST算法的有效性。

Conclusion: 在通过提示优化对齐黑盒大语言模型时，纳入推理感知至关重要。

Abstract: Prompt optimization methods have demonstrated significant effectiveness in
aligning black-box large language models (LLMs). In parallel, inference scaling
strategies such as Best-of-N Sampling and Majority Voting have also proven to
enhance alignment and performance by trading off computation. However, existing
prompt optimization approaches are inference strategy agnostic; that is, they
optimize prompts without regard to the inference strategy employed during
deployment. This constitutes a significant methodological gap, as our empirical
and theoretical analysis reveals a strong interdependence between these two
paradigms. Moreover, we find that user preferences regarding trade-offs among
multiple objectives and inference budgets substantially influence the choice of
prompt and inference configuration. To address this gap, we introduce a unified
novel framework named IAPO (Inference-Aware Prompt Optimization) that jointly
optimizes the prompt and inference scale, while being aware of the inference
budget and different task objectives. We then develop a fixed-budget training
algorithm for IAPO, which we call PSST (Prompt Scaling via Sequential
Trimming), and analyze finite-budget guarantees on error probability. Finally,
we evaluate the effectiveness of PSST on six different tasks, including
multi-objective text generation and reasoning, and demonstrate the critical
role of incorporating inference-awareness when aligning black-box LLMs through
prompt optimization.

</details>


### [275] [The Cost of Thinking: Increased Jailbreak Risk in Large Language Models](https://arxiv.org/abs/2508.10032)
*Fan Yang*

Main category: cs.CL

TL;DR: 研究发现大语言模型思维模式易受越狱攻击，提出安全思维干预方法降低攻击成功率


<details>
  <summary>Details</summary>
Motivation: 发现大语言模型思维模式易被越狱攻击这一被忽视现象，需解决此安全问题

Method: 在AdvBench和HarmBench评估9个大语言模型，进行大量样本研究，提出在提示中添加‘特定思维令牌’明确引导大语言模型内部思维过程的安全思维干预方法

Result: 安全思维干预能显著降低具有思维模式的大语言模型的攻击成功率

Conclusion: 安全思维干预方法可有效缓解大语言模型思维模式易受攻击的问题

Abstract: Thinking mode has always been regarded as one of the most valuable modes in
LLMs. However, we uncover a surprising and previously overlooked phenomenon:
LLMs with thinking mode are more easily broken by Jailbreak attack. We evaluate
9 LLMs on AdvBench and HarmBench and find that the success rate of attacking
thinking mode in LLMs is almost higher than that of non-thinking mode. Through
large numbers of sample studies, it is found that for educational purposes and
excessively long thinking lengths are the characteristics of successfully
attacked data, and LLMs also give harmful answers when they mostly know that
the questions are harmful. In order to alleviate the above problems, this paper
proposes a method of safe thinking intervention for LLMs, which explicitly
guides the internal thinking processes of LLMs by adding "specific thinking
tokens" of LLMs to the prompt. The results demonstrate that the safe thinking
intervention can significantly reduce the attack success rate of LLMs with
thinking mode.

</details>


### [276] [mSCoRe: a $M$ultilingual and Scalable Benchmark for $S$kill-based $Co$mmonsense $Re$asoning](https://arxiv.org/abs/2508.10137)
*Nghia Trung Ngo,Franck Dernoncourt,Thien Huu Nguyen*

Main category: cs.CL

TL;DR: 提出多语言可扩展基准mSCoRe评估大语言模型多语言常识推理能力，实验表明当前模型应对高复杂度任务有挑战，给出改进方向。


<details>
  <summary>Details</summary>
Motivation: 现有研究对大语言模型利用不同人类推理技能的机制研究不足，尤其是多语言常识推理方面。

Method: 提出mSCoRe基准，包含推理技能分类、数据合成管道和复杂度缩放框架。

Result: 对八个不同规模和训练方式的模型实验，mSCoRe对当前模型有挑战性，尤其在高复杂度任务上，揭示了推理增强模型在多语言常识推理上的局限。

Conclusion: 为提升多语言常识推理能力提供未来研究方向。

Abstract: Recent advancements in reasoning-reinforced Large Language Models (LLMs) have
shown remarkable capabilities in complex reasoning tasks. However, the
mechanism underlying their utilization of different human reasoning skills
remains poorly investigated, especially for multilingual commonsense reasoning
that involves everyday knowledge across different languages and cultures. To
address this gap, we propose a \textbf{M}ultilingual and Scalable Benchmark for
\textbf{S}kill-based \textbf{Co}mmonsense \textbf{Re}asoning (\textbf{mSCoRe}).
Our benchmark incorporates three key components that are designed to
systematically evaluate LLM's reasoning capabilities, including: (1) a novel
taxonomy of reasoning skills that enables fine-grained analysis of models'
reasoning processes, (2) a robust data synthesis pipeline tailored specifically
for commonsense reasoning evaluation, and (3) a complexity scaling framework
allowing task difficulty to scale dynamically alongside future improvements in
LLM abilities. Extensive experiments on eights state-of-the-art LLMs of varying
sizes and training approaches demonstrate that \textbf{mSCoRe} remains
significantly challenging for current models, particularly at higher complexity
levels. Our results reveal the limitations of such reasoning-reinforced models
when confronted with nuanced multilingual general and cultural commonsense. We
further provide detailed analysis on the models' reasoning processes,
suggesting future directions for improving multilingual commonsense reasoning
capabilities.

</details>


### [277] [LaajMeter: A Framework for LaaJ Evaluation](https://arxiv.org/abs/2508.10161)
*Gal Amram,Eitan Farchi,Shmulik Froimovich,Raviv Gal,Avi Ziv*

Main category: cs.CL

TL;DR: 介绍LaaJMeter框架用于LaaJ的元评估，在代码翻译任务中展示其效用，强调指标选择重要性。


<details>
  <summary>Details</summary>
Motivation: LaaJ在特定领域存在挑战，元评估指标未经验证，难以确定有效指标和性能阈值。

Method: 引入基于模拟的LaaJMeter框架，生成合成数据进行系统分析。

Result: 在代码翻译任务中展示不同指标对评估质量的敏感性差异，凸显常见指标局限性。

Conclusion: LaaJMeter为低资源环境下评估LaaJ提供可扩展解决方案，有助于NLP可信和可复现评估。

Abstract: Large Language Models (LLMs) are increasingly used as evaluators in natural
language processing tasks, a paradigm known as LLM-as-a-Judge (LaaJ). While
effective in general domains, LaaJs pose significant challenges in
domain-specific contexts, where annotated data is scarce and expert evaluation
is costly. In such cases, meta-evaluation is often performed using metrics that
have not been validated for the specific domain in which they are applied. As a
result, it becomes difficult to determine which metrics effectively identify
LaaJ quality, and further, what threshold indicates sufficient evaluator
performance. In this work, we introduce LaaJMeter, a simulation-based framework
for controlled meta-evaluation of LaaJs. LaaJMeter enables engineers to
generate synthetic data representing virtual models and judges, allowing
systematic analysis of evaluation metrics under realistic conditions. This
helps practitioners validate and refine LaaJs for specific evaluation tasks:
they can test whether their metrics correctly distinguish between better and
worse (virtual) LaaJs, and estimate appropriate thresholds for evaluator
adequacy.
  We demonstrate the utility of LaaJMeter in a code translation task involving
a legacy programming language, showing how different metrics vary in
sensitivity to evaluator quality. Our results highlight the limitations of
common metrics and the importance of principled metric selection. LaaJMeter
provides a scalable and extensible solution for assessing LaaJs in low-resource
settings, contributing to the broader effort to ensure trustworthy and
reproducible evaluation in NLP.

</details>


### [278] [PakBBQ: A Culturally Adapted Bias Benchmark for QA](https://arxiv.org/abs/2508.10186)
*Abdullah Hashmat,Muhammad Arham Mirza,Agha Ali Raza*

Main category: cs.CL

TL;DR: 引入针对巴基斯坦文化和地区的PakBBQ数据集评估多语言大模型，发现消歧、语言及问题框架对减少偏见有影响，强调情境化基准和提示工程策略重要性。


<details>
  <summary>Details</summary>
Motivation: 多数大语言模型基于西方中心数据训练评估，忽视低资源语言和地区背景，需确保其在所有用户群体中的公平性。

Method: 引入PakBBQ数据集，在模糊和明确消歧上下文、负面与非负面问题框架下评估多个多语言大模型。

Result: 消歧平均提高12%准确率；乌尔都语比英语有更强反偏见行为；负面提问可减少刻板回应。

Conclusion: 情境化基准和简单提示工程策略对低资源环境下缓解偏见很重要。

Abstract: With the widespread adoption of Large Language Models (LLMs) across various
applications, it is empirical to ensure their fairness across all user
communities. However, most LLMs are trained and evaluated on Western centric
data, with little attention paid to low-resource languages and regional
contexts. To address this gap, we introduce PakBBQ, a culturally and regionally
adapted extension of the original Bias Benchmark for Question Answering (BBQ)
dataset. PakBBQ comprises over 214 templates, 17180 QA pairs across 8
categories in both English and Urdu, covering eight bias dimensions including
age, disability, appearance, gender, socio-economic status, religious, regional
affiliation, and language formality that are relevant in Pakistan. We evaluate
multiple multilingual LLMs under both ambiguous and explicitly disambiguated
contexts, as well as negative versus non negative question framings. Our
experiments reveal (i) an average accuracy gain of 12\% with disambiguation,
(ii) consistently stronger counter bias behaviors in Urdu than in English, and
(iii) marked framing effects that reduce stereotypical responses when questions
are posed negatively. These findings highlight the importance of contextualized
benchmarks and simple prompt engineering strategies for bias mitigation in low
resource settings.

</details>


### [279] [Using Large Language Models to Measure Symptom Severity in Patients At Risk for Schizophrenia](https://arxiv.org/abs/2508.10226)
*Andrew X. Chen,Guillermo Horga,Sean Escola*

Main category: cs.CL

TL;DR: 利用大语言模型从临床访谈记录预测临床高风险精神分裂症患者的BPRS分数，性能接近人类评分者信度，且在多方面有评估潜力。


<details>
  <summary>Details</summary>
Motivation: 精神分裂症临床高风险患者需密切监测症状，常用研究工具BPRS因需长时间结构化访谈，在临床实践中不常用，需新方法。

Method: 使用大语言模型对409名临床高风险患者的临床访谈记录进行分析，预测BPRS分数。

Result: 大语言模型零样本预测与真实评估的中位数一致性为0.84，组内相关系数为0.73；评估外语BPRS时中位数一致性为0.88，组内相关系数为0.70。

Conclusion: 大语言模型在改善和规范临床高风险患者评估方面有很大潜力。

Abstract: Patients who are at clinical high risk (CHR) for schizophrenia need close
monitoring of their symptoms to inform appropriate treatments. The Brief
Psychiatric Rating Scale (BPRS) is a validated, commonly used research tool for
measuring symptoms in patients with schizophrenia and other psychotic
disorders; however, it is not commonly used in clinical practice as it requires
a lengthy structured interview. Here, we utilize large language models (LLMs)
to predict BPRS scores from clinical interview transcripts in 409 CHR patients
from the Accelerating Medicines Partnership Schizophrenia (AMP-SCZ) cohort.
Despite the interviews not being specifically structured to measure the BPRS,
the zero-shot performance of the LLM predictions compared to the true
assessment (median concordance: 0.84, ICC: 0.73) approaches human inter- and
intra-rater reliability. We further demonstrate that LLMs have substantial
potential to improve and standardize the assessment of CHR patients via their
accuracy in assessing the BPRS in foreign languages (median concordance: 0.88,
ICC: 0.70), and integrating longitudinal information in a one-shot or few-shot
learning approach.

</details>


### [280] [XFacta: Contemporary, Real-World Dataset and Evaluation for Multimodal Misinformation Detection with Multimodal LLMs](https://arxiv.org/abs/2508.09999)
*Yuzhuo Xiao,Zeyu Han,Yuhan Wang,Huaizu Jiang*

Main category: cs.CL

TL;DR: 为解决多模态错误信息检测问题，引入 XFacta 数据集，评估检测策略，构建半自动化框架并公开代码数据。


<details>
  <summary>Details</summary>
Motivation: 社交媒体多模态错误信息快速传播，现有方法瓶颈不明，数据集存在问题，缺乏模型设计策略分析。

Method: 引入 XFacta 数据集，系统评估多种 MLLM 错误信息检测策略，构建半自动化检测框架。

Result: 对不同架构和规模模型进行评估，与现有检测方法对比。

Conclusion: 分析为多模态错误信息检测领域提供有价值见解和实践。

Abstract: The rapid spread of multimodal misinformation on social media calls for more
effective and robust detection methods. Recent advances leveraging multimodal
large language models (MLLMs) have shown the potential in addressing this
challenge. However, it remains unclear exactly where the bottleneck of existing
approaches lies (evidence retrieval v.s. reasoning), hindering the further
advances in this field. On the dataset side, existing benchmarks either contain
outdated events, leading to evaluation bias due to discrepancies with
contemporary social media scenarios as MLLMs can simply memorize these events,
or artificially synthetic, failing to reflect real-world misinformation
patterns. Additionally, it lacks comprehensive analyses of MLLM-based model
design strategies. To address these issues, we introduce XFacta, a
contemporary, real-world dataset that is better suited for evaluating
MLLM-based detectors. We systematically evaluate various MLLM-based
misinformation detection strategies, assessing models across different
architectures and scales, as well as benchmarking against existing detection
methods. Building on these analyses, we further enable a semi-automatic
detection-in-the-loop framework that continuously updates XFacta with new
content to maintain its contemporary relevance. Our analysis provides valuable
insights and practices for advancing the field of multimodal misinformation
detection. The code and data have been released.

</details>


### [281] [AutoGeTS: Knowledge-based Automated Generation of Text Synthetics for Improving Text Classification](https://arxiv.org/abs/2508.10000)
*Chenhao Xue,Yuanzhe Jin,Adrian Carrasco-Revilla,Joyraj Chakraborty,Min Chen*

Main category: cs.CL

TL;DR: 本文利用大语言模型生成合成数据解决文本分类模型数据收集难题，设计自动化工作流和搜索策略，集成算法表现更优。


<details>
  <summary>Details</summary>
Motivation: 解决现实应用中文本分类模型难以收集所有文本类别的充足数据的问题。

Method: 利用大语言模型生成合成数据，制定自动化工作流搜索能产生更有效合成数据的输入示例，研究三种搜索策略，用实验结果构建根据类别特征选择搜索策略的集成算法。

Result: 集成方法在利用大语言模型改进分类模型方面比单个策略更有效。

Conclusion: 所提出的集成算法能更好地利用大语言模型生成的合成数据提升文本分类模型性能。

Abstract: When developing text classification models for real world applications, one
major challenge is the difficulty to collect sufficient data for all text
classes. In this work, we address this challenge by utilizing large language
models (LLMs) to generate synthetic data and using such data to improve the
performance of the models without waiting for more real data to be collected
and labelled. As an LLM generates different synthetic data in response to
different input examples, we formulate an automated workflow, which searches
for input examples that lead to more ``effective'' synthetic data for improving
the model concerned. We study three search strategies with an extensive set of
experiments, and use experiment results to inform an ensemble algorithm that
selects a search strategy according to the characteristics of a class. Our
further experiments demonstrate that this ensemble approach is more effective
than each individual strategy in our automated workflow for improving
classification models using LLMs.

</details>


### [282] [Multidimensional classification of posts for online course discussion forum curation](https://arxiv.org/abs/2508.10008)
*Antonio Leandro Martins Candido,Jose Everardo Bessa Maia*

Main category: cs.CL

TL;DR: 本文提出用贝叶斯融合方法避免在线课程论坛自动管理中对大语言模型频繁微调的高成本问题，该方法表现良好。


<details>
  <summary>Details</summary>
Motivation: 在线课程讨论论坛自动管理需不断更新，频繁重新训练大语言模型资源消耗大，需避免高成本微调。

Method: 提出贝叶斯融合方法，将预训练通用大语言模型的多维分类分数与本地数据训练的分类器分数相结合。

Result: 性能比较显示，所提融合方法比单个分类器效果好，与大语言模型微调方法竞争力相当。

Conclusion: 贝叶斯融合方法可避免高成本微调，且有较好性能。

Abstract: The automatic curation of discussion forums in online courses requires
constant updates, making frequent retraining of Large Language Models (LLMs) a
resource-intensive process. To circumvent the need for costly fine-tuning, this
paper proposes and evaluates the use of Bayesian fusion. The approach combines
the multidimensional classification scores of a pre-trained generic LLM with
those of a classifier trained on local data. The performance comparison
demonstrated that the proposed fusion improves the results compared to each
classifier individually, and is competitive with the LLM fine-tuning approach

</details>


### [283] [Yet another algorithmic bias: A Discursive Analysis of Large Language Models Reinforcing Dominant Discourses on Gender and Race](https://arxiv.org/abs/2508.10304)
*Gustavo Bonil,Simone Hashiguti,Jhessica Silva,João Gondim,Helena Maia,Nádia Silva,Helio Pedrini,Sandra Avila*

Main category: cs.CL

TL;DR: 本文提出定性话语框架分析大语言模型中的性别和种族偏见，发现模型输出存在问题，强调跨学科方法对AI设计和部署的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型偏见检测方法多为定量自动方法，易忽略自然语言中偏见的细微表现，需定性方法补充。

Method: 手动分析大语言模型生成的包含黑人和白人女性的短篇小说。

Result: 黑人女性被描绘为与祖先和抵抗相关，白人女性出现在自我发现过程中；模型纠正偏见时修改肤浅，仍保留问题含义。

Conclusion: 研究表明算法存在意识形态功能，强调需跨学科方法解决大语言模型话语反映和延续不平等的问题。

Abstract: With the advance of Artificial Intelligence (AI), Large Language Models
(LLMs) have gained prominence and been applied in diverse contexts. As they
evolve into more sophisticated versions, it is essential to assess whether they
reproduce biases, such as discrimination and racialization, while maintaining
hegemonic discourses. Current bias detection approaches rely mostly on
quantitative, automated methods, which often overlook the nuanced ways in which
biases emerge in natural language. This study proposes a qualitative,
discursive framework to complement such methods. Through manual analysis of
LLM-generated short stories featuring Black and white women, we investigate
gender and racial biases. We contend that qualitative methods such as the one
proposed here are fundamental to help both developers and users identify the
precise ways in which biases manifest in LLM outputs, thus enabling better
conditions to mitigate them. Results show that Black women are portrayed as
tied to ancestry and resistance, while white women appear in self-discovery
processes. These patterns reflect how language models replicate crystalized
discursive representations, reinforcing essentialization and a sense of social
immobility. When prompted to correct biases, models offered superficial
revisions that maintained problematic meanings, revealing limitations in
fostering inclusive narratives. Our results demonstrate the ideological
functioning of algorithms and have significant implications for the ethical use
and development of AI. The study reinforces the need for critical,
interdisciplinary approaches to AI design and deployment, addressing how
LLM-generated discourses reflect and perpetuate inequalities.

</details>


### [284] [ReviewRL: Towards Automated Scientific Review with RL](https://arxiv.org/abs/2508.10308)
*Sihang Zeng,Kai Tian,Kaiyan Zhang,Yuru wang,Junqi Gao,Runze Liu,Sa Yang,Jingxuan Li,Xinwei Long,Jiaheng Ma,Biqing Qi,Bowen Zhou*

Main category: cs.CL

TL;DR: 介绍用于生成科学论文评审的强化学习框架ReviewRL，实验表明其显著优于现有方法，有未来发展潜力。


<details>
  <summary>Details</summary>
Motivation: 同行评审面临投稿量增加和评审员疲劳挑战，现有自动评审方法存在事实准确性、评分一致性和分析深度问题。

Method: 结合ArXiv - MCP检索增强上下文生成管道、监督微调、带复合奖励函数的强化学习过程。

Result: 在ICLR 2025论文实验中，ReviewRL在基于规则的指标和基于模型的质量评估上均显著优于现有方法。

Conclusion: ReviewRL为科学发现中基于强化学习的自动评审生成建立了基础框架，有良好的未来发展潜力。

Abstract: Peer review is essential for scientific progress but faces growing challenges
due to increasing submission volumes and reviewer fatigue. Existing automated
review approaches struggle with factual accuracy, rating consistency, and
analytical depth, often generating superficial or generic feedback lacking the
insights characteristic of high-quality human reviews. We introduce ReviewRL, a
reinforcement learning framework for generating comprehensive and factually
grounded scientific paper reviews. Our approach combines: (1) an ArXiv-MCP
retrieval-augmented context generation pipeline that incorporates relevant
scientific literature, (2) supervised fine-tuning that establishes foundational
reviewing capabilities, and (3) a reinforcement learning procedure with a
composite reward function that jointly enhances review quality and rating
accuracy. Experiments on ICLR 2025 papers demonstrate that ReviewRL
significantly outperforms existing methods across both rule-based metrics and
model-based quality assessments. ReviewRL establishes a foundational framework
for RL-driven automatic critique generation in scientific discovery,
demonstrating promising potential for future development in this domain. The
implementation of ReviewRL will be released at GitHub.

</details>


### [285] [Layer-Wise Perturbations via Sparse Autoencoders for Adversarial Text Generation](https://arxiv.org/abs/2508.10404)
*Huizhen Shu,Xuying Li,Qirui Wang,Yuji Kosuga,Mengqiu Tian,Zhuo Li*

Main category: cs.CL

TL;DR: 提出利用大模型可解释性的黑盒攻击方法SFPF生成对抗文本，实验表明能绕过现有防御，但有效性受提示和层影响，泛化性待验证。


<details>
  <summary>Details</summary>
Motivation: 在NLP尤其是大语言模型快速发展背景下，为理解模型漏洞和提高鲁棒性，需生成对抗样本来越狱大语言模型。

Method: 提出Sparse Feature Perturbation Framework (SFPF)，利用稀疏自编码器识别和操纵文本关键特征，重构隐藏层表示后对成功攻击文本进行特征聚类，扰动高激活特征生成新对抗文本。

Result: SFPF生成的对抗文本能绕过最先进的防御机制，揭示当前NLP系统存在持续漏洞。

Conclusion: 该方法实现了平衡对抗有效性和安全对齐的红队策略，但有效性因提示和层而异，对其他架构和更大模型的泛化性有待验证。

Abstract: With the rapid proliferation of Natural Language Processing (NLP), especially
Large Language Models (LLMs), generating adversarial examples to jailbreak LLMs
remains a key challenge for understanding model vulnerabilities and improving
robustness. In this context, we propose a new black-box attack method that
leverages the interpretability of large models. We introduce the Sparse Feature
Perturbation Framework (SFPF), a novel approach for adversarial text generation
that utilizes sparse autoencoders to identify and manipulate critical features
in text. After using the SAE model to reconstruct hidden layer representations,
we perform feature clustering on the successfully attacked texts to identify
features with higher activations. These highly activated features are then
perturbed to generate new adversarial texts. This selective perturbation
preserves the malicious intent while amplifying safety signals, thereby
increasing their potential to evade existing defenses. Our method enables a new
red-teaming strategy that balances adversarial effectiveness with safety
alignment. Experimental results demonstrate that adversarial texts generated by
SFPF can bypass state-of-the-art defense mechanisms, revealing persistent
vulnerabilities in current NLP systems.However, the method's effectiveness
varies across prompts and layers, and its generalizability to other
architectures and larger models remains to be validated.

</details>


### [286] [ComoRAG: A Cognitive-Inspired Memory-Organized RAG for Stateful Long Narrative Reasoning](https://arxiv.org/abs/2508.10419)
*Juyuan Wang,Rongchen Zhao,Wei Wei,Yufeng Wang,Mo Yu,Jie Zhou,Jin Xu,Liyan Xu*

Main category: cs.CL

TL;DR: 提出ComoRAG解决长文本叙事理解问题，在多个基准测试中优于RAG基线。


<details>
  <summary>Details</summary>
Motivation: 长故事和小说叙事理解有挑战，传统RAG方法有局限，大语言模型处理长上下文能力不足。

Method: 提出ComoRAG，基于叙事推理是动态过程的原则，通过迭代推理循环与动态记忆工作区交互。

Result: 在四个长上下文叙事基准测试中，ComoRAG优于强RAG基线，相对增益达11%。

Conclusion: ComoRAG对需要全局理解的复杂查询有利，为基于检索的长上下文理解提供了有原则、受认知启发的有状态推理范式。

Abstract: Narrative comprehension on long stories and novels has been a challenging
domain attributed to their intricate plotlines and entangled, often evolving
relations among characters and entities. Given the LLM's diminished reasoning
over extended context and high computational cost, retrieval-based approaches
remain a pivotal role in practice. However, traditional RAG methods can fall
short due to their stateless, single-step retrieval process, which often
overlooks the dynamic nature of capturing interconnected relations within
long-range context. In this work, we propose ComoRAG, holding the principle
that narrative reasoning is not a one-shot process, but a dynamic, evolving
interplay between new evidence acquisition and past knowledge consolidation,
analogous to human cognition when reasoning with memory-related signals in the
brain. Specifically, when encountering a reasoning impasse, ComoRAG undergoes
iterative reasoning cycles while interacting with a dynamic memory workspace.
In each cycle, it generates probing queries to devise new exploratory paths,
then integrates the retrieved evidence of new aspects into a global memory
pool, thereby supporting the emergence of a coherent context for the query
resolution. Across four challenging long-context narrative benchmarks (200K+
tokens), ComoRAG outperforms strong RAG baselines with consistent relative
gains up to 11% compared to the strongest baseline. Further analysis reveals
that ComoRAG is particularly advantageous for complex queries requiring global
comprehension, offering a principled, cognitively motivated paradigm for
retrieval-based long context comprehension towards stateful reasoning. Our code
is publicly released at https://github.com/EternityJune25/ComoRAG

</details>


### [287] [When Language Overrules: Revealing Text Dominance in Multimodal Large Language Models](https://arxiv.org/abs/2508.10552)
*Huyu Wu,Meng Tang,Xinhan Zheng,Haiyun Jiang*

Main category: cs.CL

TL;DR: 文章对多模态大语言模型的文本主导问题进行系统研究，提出评估指标，分析原因并提出解决方法，为多模态语言模型发展提供基础。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型存在文本主导问题，此前研究局限于视觉 - 语言任务，本文旨在跨多种数据模态系统研究该问题。

Method: 提出模态主导指数（MDI）和注意力效率指数（AEI）评估指标，提出简单的令牌压缩方法。

Result: 发现文本主导问题在所有测试模态中显著且普遍，确定三个潜在原因，将LLaVA - 7B的MDI从10.23大幅降至0.86。

Conclusion: 分析和方法框架为开发更公平、全面的多模态语言模型提供基础。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable
capabilities across a diverse range of multimodal tasks. However, these models
suffer from a core problem known as text dominance: they depend heavily on text
for their inference, while underutilizing other modalities. While prior work
has acknowledged this phenomenon in vision-language tasks, often attributing it
to data biases or model architectures. In this paper, we conduct the first
systematic investigation of text dominance across diverse data modalities,
including images, videos, audio, time-series, and graphs. To measure this
imbalance, we propose two evaluation metrics: the Modality Dominance Index
(MDI) and the Attention Efficiency Index (AEI). Our comprehensive analysis
reveals that text dominance is both significant and pervasive across all tested
modalities. Our in-depth analysis identifies three underlying causes: attention
dilution from severe token redundancy in non-textual modalities, the influence
of fusion architecture design, and task formulations that implicitly favor
textual inputs. Furthermore, we propose a simple token compression method that
effectively rebalances model attention. Applying this method to LLaVA-7B, for
instance, drastically reduces its MDI from 10.23 to a well-balanced value of
0.86. Our analysis and methodological framework offer a foundation for the
development of more equitable and comprehensive multimodal language models.

</details>


### [288] [Continuous Bangla Sign Language Translation: Mitigating the Expense of Gloss Annotation with the Assistance of Graph](https://arxiv.org/abs/2508.10687)
*Safaeid Hossain Arib,Rabeya Akter,Sejuti Rahman*

Main category: cs.CL

TL;DR: 文章提出融合transformer和STGCN - LSTM架构进行连续孟加拉语手语翻译，在多个数据集上取得新的最优表现，为无注解翻译树立标杆。


<details>
  <summary>Details</summary>
Motivation: 手语在以口语为主的社会常被低估，存在沟通障碍和社会排斥问题，需改进翻译方法。

Method: 将基于图的方法与transformer架构融合，结合transformer和STGCN - LSTM架构进行无注解翻译。

Result: 在多个手语数据集上取得新的最优性能，BLEU - 4分数有显著提升，首次在BornilDB v1.0数据集上进行基准测试。

Conclusion: 该方法为未来研究树立基准，强调无注解翻译对改善聋哑及听力障碍者沟通可及性的重要性。

Abstract: Millions of individuals worldwide are affected by deafness and hearing
impairment. Sign language serves as a sophisticated means of communication for
the deaf and hard of hearing. However, in societies that prioritize spoken
languages, sign language often faces underestimation, leading to communication
barriers and social exclusion. The Continuous Bangla Sign Language Translation
project aims to address this gap by enhancing translation methods. While recent
approaches leverage transformer architecture for state-of-the-art results, our
method integrates graph-based methods with the transformer architecture. This
fusion, combining transformer and STGCN-LSTM architectures, proves more
effective in gloss-free translation. Our contributions include architectural
fusion, exploring various fusion strategies, and achieving a new
state-of-the-art performance on diverse sign language datasets, namely
RWTH-PHOENIX-2014T, CSL-Daily, How2Sign, and BornilDB v1.0. Our approach
demonstrates superior performance compared to current translation outcomes
across all datasets, showcasing notable improvements of BLEU-4 scores of 4.01,
2.07, and 0.5, surpassing those of GASLT, GASLT and slt_how2sign in
RWTH-PHOENIX-2014T, CSL-Daily, and How2Sign, respectively. Also, we introduce
benchmarking on the BornilDB v1.0 dataset for the first time. Our method sets a
benchmark for future research, emphasizing the importance of gloss-free
translation to improve communication accessibility for the deaf and hard of
hearing.

</details>


### [289] [Reinforced Language Models for Sequential Decision Making](https://arxiv.org/abs/2508.10839)
*Jim Dilkes,Vahid Yazdanpanah,Sebastian Stein*

Main category: cs.CL

TL;DR: 现有大语言模型用于顺序决策受限于模型规模，提出MS - GRPO算法改进小模型，实验表明其有效且证明针对性后训练是实用高效的替代方案。


<details>
  <summary>Details</summary>
Motivation: 大语言模型用于顺序决策依赖大模型，现有后训练方法不适用于多步代理任务，需改进小模型。

Method: 引入基于形式化框架的MS - GRPO算法，将累积奖励分配到每一步，补充绝对优势加权的回合采样策略。

Result: 在Snake和Frozen Lake上对30亿参数模型后训练，在Frozen Lake任务中后训练的30亿参数模型比720亿参数基线模型性能高50%。

Conclusion: 针对性后训练是使用大语言模型创建顺序决策代理时依赖模型规模的实用高效替代方案。

Abstract: Large Language Models (LLMs) show potential as sequential decision-making
agents, but their application is often limited due to a reliance on large,
computationally expensive models. This creates a need to improve smaller
models, yet existing post-training methods are designed for single-turn
interactions and cannot handle credit assignment in multi-step agentic tasks.
To address this, we introduce Multi-Step Group-Relative Policy Optimization
(MS-GRPO), a new algorithm for post-training LLM agents, grounded in formal
Text-Mediated Stochastic Game (TSMG) and Language-Agent Policy (LAP)
frameworks. For credit assignment, MS-GRPO attributes the entire cumulative
episode reward to each individual episode step. We supplement this algorithm
with a novel absolute-advantage-weighted episode sampling strategy that we show
improves training performance. We evaluate our approach by post-training a
3-billion parameter model on Snake and Frozen Lake. Our experiments demonstrate
that the method is effective in improving decision-making performance: our
post-trained 3B parameter model outperforms a 72B parameter baseline by 50% on
the Frozen Lake task. This work demonstrates that targeted post-training is a
practical and efficient alternative to relying on model scale for creating
sequential decision-making agents using LLMs.

</details>


### [290] [A Survey on Diffusion Language Models](https://arxiv.org/abs/2508.10875)
*Tianyi Li,Mingda Chen,Bowei Guo,Zhiqiang Shen*

Main category: cs.CL

TL;DR: 本文对扩散语言模型（DLMs）进行全面综述，涵盖其发展、技术、推理策略、多模态扩展、局限挑战及未来方向。


<details>
  <summary>Details</summary>
Motivation: DLMs 作为新兴范式有降低推理延迟等优势，为多种自然语言处理任务提供新选择，需全面了解其现状。

Method: 追溯 DLMs 与其他范式关系，分析基础原理和先进模型，研究从预训练到后训练技术、推理策略优化等。

Result: 给出最新、全面分类法，深入分析当前技术，回顾推理策略优化，介绍多模态扩展及应用。

Conclusion: 指出 DLMs 存在效率、长序列处理等局限，明确未来研究方向以推动该领域发展。

Abstract: Diffusion Language Models (DLMs) are rapidly emerging as a powerful and
promising alternative to the dominant autoregressive (AR) paradigm. By
generating tokens in parallel through an iterative denoising process, DLMs
possess inherent advantages in reducing inference latency and capturing
bidirectional context, thereby enabling fine-grained control over the
generation process. While achieving a several-fold speed-up, recent
advancements have allowed DLMs to show performance comparable to their
autoregressive counterparts, making them a compelling choice for various
natural language processing tasks. In this survey, we provide a holistic
overview of the current DLM landscape. We trace its evolution and relationship
with other paradigms, such as autoregressive and masked language models, and
cover both foundational principles and state-of-the-art models. Our work offers
an up-to-date, comprehensive taxonomy and an in-depth analysis of current
techniques, from pre-training strategies to advanced post-training methods.
Another contribution of this survey is a thorough review of DLM inference
strategies and optimizations, including improvements in decoding parallelism,
caching mechanisms, and generation quality. We also highlight the latest
approaches to multimodal extensions of DLMs and delineate their applications
across various practical scenarios. Furthermore, our discussion addresses the
limitations and challenges of DLMs, including efficiency, long-sequence
handling, and infrastructure requirements, while outlining future research
directions to sustain progress in this rapidly evolving field. Project GitHub
is available at https://github.com/VILA-Lab/Awesome-DLMs.

</details>


### [291] [From Black Box to Transparency: Enhancing Automated Interpreting Assessment with Explainable AI in College Classrooms](https://arxiv.org/abs/2508.10860)
*Zhaokun Jiang,Ziyin Zhang*

Main category: cs.CL

TL;DR: 提出多维建模框架进行自动口译质量评估，在新数据集上表现好，强调可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有自动口译质量评估研究存在语言使用质量考察不足、数据稀缺不平衡致建模效果不佳、缺乏模型预测解释等问题。

Method: 提出集成特征工程、数据增强和可解释机器学习的多维建模框架，利用构造相关透明特征并进行SHAP分析。

Result: 在新的英中连续口译数据集上有强预测性能，确定了不同方面的强预测特征。

Conclusion: 强调可解释性，提供了传统人工评估的替代方案，利于为学习者提供反馈和支持自主学习。

Abstract: Recent advancements in machine learning have spurred growing interests in
automated interpreting quality assessment. Nevertheless, existing research
suffers from insufficient examination of language use quality, unsatisfactory
modeling effectiveness due to data scarcity and imbalance, and a lack of
efforts to explain model predictions. To address these gaps, we propose a
multi-dimensional modeling framework that integrates feature engineering, data
augmentation, and explainable machine learning. This approach prioritizes
explainability over ``black box'' predictions by utilizing only
construct-relevant, transparent features and conducting Shapley Value (SHAP)
analysis. Our results demonstrate strong predictive performance on a novel
English-Chinese consecutive interpreting dataset, identifying BLEURT and
CometKiwi scores to be the strongest predictive features for fidelity,
pause-related features for fluency, and Chinese-specific phraseological
diversity metrics for language use. Overall, by placing particular emphasis on
explainability, we present a scalable, reliable, and transparent alternative to
traditional human evaluation, facilitating the provision of detailed diagnostic
feedback for learners and supporting self-regulated learning advantages not
afforded by automated scores in isolation.

</details>
