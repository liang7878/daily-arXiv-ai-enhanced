<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 33]
- [cs.CE](#cs.CE) [Total: 6]
- [cs.DB](#cs.DB) [Total: 3]
- [cs.DC](#cs.DC) [Total: 9]
- [cs.DS](#cs.DS) [Total: 8]
- [cs.GT](#cs.GT) [Total: 3]
- [cs.IR](#cs.IR) [Total: 15]
- [cs.LG](#cs.LG) [Total: 97]
- [cs.NE](#cs.NE) [Total: 2]
- [cs.PF](#cs.PF) [Total: 2]
- [cs.SE](#cs.SE) [Total: 14]
- [q-fin.CP](#q-fin.CP) [Total: 1]
- [q-fin.RM](#q-fin.RM) [Total: 1]
- [stat.ML](#stat.ML) [Total: 5]
- [stat.CO](#stat.CO) [Total: 2]
- [cs.CY](#cs.CY) [Total: 3]
- [physics.geo-ph](#physics.geo-ph) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 3]
- [cs.GR](#cs.GR) [Total: 2]
- [cs.SI](#cs.SI) [Total: 2]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [math.AP](#math.AP) [Total: 1]
- [cs.HC](#cs.HC) [Total: 8]
- [cs.SD](#cs.SD) [Total: 5]
- [cs.IT](#cs.IT) [Total: 2]
- [cs.CR](#cs.CR) [Total: 2]
- [math.OC](#math.OC) [Total: 3]
- [cs.MM](#cs.MM) [Total: 2]
- [cs.CL](#cs.CL) [Total: 23]
- [eess.SP](#eess.SP) [Total: 1]
- [eess.AS](#eess.AS) [Total: 3]
- [cs.CC](#cs.CC) [Total: 1]
- [quant-ph](#quant-ph) [Total: 7]
- [q-bio.GN](#q-bio.GN) [Total: 4]
- [cs.RO](#cs.RO) [Total: 9]
- [q-bio.MN](#q-bio.MN) [Total: 1]
- [cs.CV](#cs.CV) [Total: 29]
- [eess.IV](#eess.IV) [Total: 2]
- [econ.EM](#econ.EM) [Total: 1]
- [econ.GN](#econ.GN) [Total: 2]
- [eess.SY](#eess.SY) [Total: 1]
- [econ.TH](#econ.TH) [Total: 1]
- [astro-ph.EP](#astro-ph.EP) [Total: 1]
- [math.NA](#math.NA) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [cs.AR](#cs.AR) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Prescriptive Agents based on Rag for Automated Maintenance (PARAM)](https://arxiv.org/abs/2508.04714)
*Chitranshu Harbola,Anupam Purwar*

Main category: cs.AI

TL;DR: 本文提出基于大语言模型的工业机械智能维护系统，结合振动分析与多智能体生成，实现异常检测与维护建议生成，实验验证有效。


<details>
  <summary>Details</summary>
Motivation: 工业机械维护需及时干预，传统异常检测缺乏可操作维护建议，本文旨在提供智能决策支持。

Method: 基于LAMP框架，将轴承振动数据序列化供大语言模型处理，结合多智能体利用向量嵌入和语义搜索处理维护手册与网络搜索，Gemini模型生成维护建议。

Result: 在轴承振动数据集实验中，系统实现有效异常检测与相关维护指导。

Conclusion: 该系统弥合状态监测与维护规划差距，推动大语言模型在工业维护应用，提供可扩展框架。

Abstract: Industrial machinery maintenance requires timely intervention to prevent
catastrophic failures and optimize operational efficiency. This paper presents
an integrated Large Language Model (LLM)-based intelligent system for
prescriptive maintenance that extends beyond traditional anomaly detection to
provide actionable maintenance recommendations. Building upon our prior LAMP
framework for numerical data analysis, we develop a comprehensive solution that
combines bearing vibration frequency analysis with multi agentic generation for
intelligent maintenance planning. Our approach serializes bearing vibration
data (BPFO, BPFI, BSF, FTF frequencies) into natural language for LLM
processing, enabling few-shot anomaly detection with high accuracy. The system
classifies fault types (inner race, outer race, ball/roller, cage faults) and
assesses severity levels. A multi-agentic component processes maintenance
manuals using vector embeddings and semantic search, while also conducting web
searches to retrieve comprehensive procedural knowledge and access up-to-date
maintenance practices for more accurate and in-depth recommendations. The
Gemini model then generates structured maintenance recommendations includes
immediate actions, inspection checklists, corrective measures, parts
requirements, and timeline specifications. Experimental validation in bearing
vibration datasets demonstrates effective anomaly detection and contextually
relevant maintenance guidance. The system successfully bridges the gap between
condition monitoring and actionable maintenance planning, providing industrial
practitioners with intelligent decision support. This work advances the
application of LLMs in industrial maintenance, offering a scalable framework
for prescriptive maintenance across machinery components and industrial
sectors.

</details>


### [2] [GeoFlow: Agentic Workflow Automation for Geospatial Tasks](https://arxiv.org/abs/2508.04719)
*Amulya Bhattaram,Justin Chung,Stanley Chung,Ranit Gupta,Janani Ramamoorthy,Kartikeya Gullapalli,Diana Marculescu,Dimitrios Stamoulis*

Main category: cs.AI

TL;DR: 介绍GeoFlow方法，能自动生成地理空间任务的代理工作流，对比SOTA方法有优势。


<details>
  <summary>Details</summary>
Motivation: 改进现有专注推理分解且API选择不明确的方法，为地理空间任务提供更好解决方案。

Method: 为每个代理提供详细的工具调用目标，以指导地理空间API在运行时的调用。

Result: 与现有SOTA方法相比，GeoFlow将代理成功率提高6.8%，并在主要大语言模型系列中最多将令牌使用量减少四倍。

Conclusion: GeoFlow是一种有效且更优的地理空间任务代理工作流生成方法。

Abstract: We present GeoFlow, a method that automatically generates agentic workflows
for geospatial tasks. Unlike prior work that focuses on reasoning decomposition
and leaves API selection implicit, our method provides each agent with detailed
tool-calling objectives to guide geospatial API invocation at runtime. GeoFlow
increases agentic success by 6.8% and reduces token usage by up to fourfold
across major LLM families compared to state-of-the-art approaches.

</details>


### [3] [Who is a Better Player: LLM against LLM](https://arxiv.org/abs/2508.04720)
*Yingjie Zhou,Jiezhang Cao,Farong Wen,Li Xu,Yanwei Jiang,Jun Jia,Ronghui Li,Xiaohong Liu,Yu Zhou,Xiongkuo Min,Jie Guo,Zicheng Zhang,Guangtao Zhai*

Main category: cs.AI

TL;DR: 提出通过棋盘游戏竞争评估大语言模型综合性能的对抗性基准框架，用Qi Town平台评估，实验显示模型对胜负乐观但游戏技能不稳定。


<details>
  <summary>Details</summary>
Motivation: 弥补主流基于问答的基准方法数据依赖的局限，评估大语言模型综合性能。

Method: 构建对抗性基准框架，搭建Qi Town平台，用Elo评级系统和性能循环图定量评估技术能力，捕捉积极情绪得分评估心理适应性，采用循环赛制。

Result: 多数大语言模型对胜负乐观，比人类更适应高压力对抗环境；性能循环图显示模型游戏技能不稳定。

Conclusion: 大语言模型在对抗环境适应性上有优势，但游戏技能的不稳定性需进一步研究。

Abstract: Adversarial board games, as a paradigmatic domain of strategic reasoning and
intelligence, have long served as both a popular competitive activity and a
benchmark for evaluating artificial intelligence (AI) systems. Building on this
foundation, we propose an adversarial benchmarking framework to assess the
comprehensive performance of Large Language Models (LLMs) through board games
competition, compensating the limitation of data dependency of the mainstream
Question-and-Answer (Q&A) based benchmark method. We introduce Qi Town, a
specialized evaluation platform that supports 5 widely played games and
involves 20 LLM-driven players. The platform employs both the Elo rating system
and a novel Performance Loop Graph (PLG) to quantitatively evaluate the
technical capabilities of LLMs, while also capturing Positive Sentiment Score
(PSS) throughout gameplay to assess mental fitness. The evaluation is
structured as a round-robin tournament, enabling systematic comparison across
players. Experimental results indicate that, despite technical differences,
most LLMs remain optimistic about winning and losing, demonstrating greater
adaptability to high-stress adversarial environments than humans. On the other
hand, the complex relationship between cyclic wins and losses in PLGs exposes
the instability of LLMs' skill play during games, warranting further
explanation and exploration.

</details>


### [4] [Fine-Tuning Small Language Models (SLMs) for Autonomous Web-based Geographical Information Systems (AWebGIS)](https://arxiv.org/abs/2508.04846)
*Mahdi Nazari Ashani,Ali Asghar Alesheikh,Saba Kazemi,Kimya Kheirkhah,Yasin Mohammadi,Fatemeh Rezaie,Amir Mahdi Manafi,Hedieh Zarkesh*

Main category: cs.AI

TL;DR: 本文对比三种实现AWebGIS的方法，基于微调小语言模型的全自主离线方法表现最佳，凸显浏览器可执行模型用于AWebGIS的可行性。


<details>
  <summary>Details</summary>
Motivation: 当前AWebGIS大多依赖云端大语言模型，存在需持续联网、隐私和可扩展性问题，因此研究替代方法。

Method: 对比三种方法：全自动化在线（用云端大语言模型）、半自动化离线（用经典机器学习分类器）、全自主离线（基于微调小语言模型）。

Result: 基于小语言模型的全自主离线方法准确性最高，精确匹配准确率0.93，Levenshtein相似度0.99，ROUGE - 1和ROUGE - L分数0.98，还减轻后端服务器负载。

Conclusion: 浏览器可执行模型用于AWebGIS解决方案是可行的。

Abstract: Autonomous web-based geographical information systems (AWebGIS) aim to
perform geospatial operations from natural language input, providing intuitive,
intelligent, and hands-free interaction. However, most current solutions rely
on cloud-based large language models (LLMs), which require continuous internet
access and raise users' privacy and scalability issues due to centralized
server processing. This study compares three approaches to enabling AWebGIS:
(1) a fully-automated online method using cloud-based LLMs (e.g., Cohere); (2)
a semi-automated offline method using classical machine learning classifiers
such as support vector machine and random forest; and (3) a fully autonomous
offline (client-side) method based on a fine-tuned small language model (SLM),
specifically T5-small model, executed in the client's web browser. The third
approach, which leverages SLMs, achieved the highest accuracy among all
methods, with an exact matching accuracy of 0.93, Levenshtein similarity of
0.99, and recall-oriented understudy for gisting evaluation ROUGE-1 and ROUGE-L
scores of 0.98. Crucially, this client-side computation strategy reduces the
load on backend servers by offloading processing to the user's device,
eliminating the need for server-based inference. These results highlight the
feasibility of browser-executable models for AWebGIS solutions.

</details>


### [5] [Large Language Models Reasoning Abilities Under Non-Ideal Conditions After RL-Fine-Tuning](https://arxiv.org/abs/2508.04848)
*Chang Tian,Matthew B. Blaschko,Mingzhe Xing,Xiuxing Li,Yinliang Yue,Marie-Francine Moens*

Main category: cs.AI

TL;DR: 本文指出现有大模型推理评估基准忽视非理想场景，研究三种非理想场景，用强化学习微调模型测试性能，发现非理想场景下性能下降，提出补救方法但问题仍未解决，强调评估非理想场景的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有大模型推理评估基准多在理想场景下，忽视现实非理想场景，需研究非理想场景下大模型的推理能力。

Method: 确定三种非理想场景，用代表性策略梯度算法的强化学习微调三个大语言模型和一个大视觉 - 语言模型，在八个公开数据集上测试性能。

Result: 强化学习微调在理想场景下提升推理能力，但在三种非理想场景下性能显著下降，暴露高级推理能力的关键局限。

Conclusion: 当前方法未能解决大模型在非理想场景下的推理缺陷，大模型推理能力常被高估，评估非理想场景很重要。

Abstract: Reinforcement learning (RL) has become a key technique for enhancing the
reasoning abilities of large language models (LLMs), with policy-gradient
algorithms dominating the post-training stage because of their efficiency and
effectiveness. However, most existing benchmarks evaluate large-language-model
reasoning under idealized settings, overlooking performance in realistic,
non-ideal scenarios. We identify three representative non-ideal scenarios with
practical relevance: summary inference, fine-grained noise suppression, and
contextual filtering. We introduce a new research direction guided by
brain-science findings that human reasoning remains reliable under imperfect
inputs. We formally define and evaluate these challenging scenarios. We
fine-tune three LLMs and a state-of-the-art large vision-language model (LVLM)
using RL with a representative policy-gradient algorithm and then test their
performance on eight public datasets. Our results reveal that while RL
fine-tuning improves baseline reasoning under idealized settings, performance
declines significantly across all three non-ideal scenarios, exposing critical
limitations in advanced reasoning capabilities. Although we propose a
scenario-specific remediation method, our results suggest current methods leave
these reasoning deficits largely unresolved. This work highlights that the
reasoning abilities of large models are often overstated and underscores the
importance of evaluating models under non-ideal scenarios. The code and data
will be released at XXXX.

</details>


### [6] [ConfAgents: A Conformal-Guided Multi-Agent Framework for Cost-Efficient Medical Diagnosis](https://arxiv.org/abs/2508.04915)
*Huiya Zhao,Yinghao Zhu,Zixiang Wang,Yasha Wang,Junyi Gao,Liantao Ma*

Main category: cs.AI

TL;DR: 文章提出HealthFlow自进化AI代理及EHRFlowBench基准，实验表明HealthFlow表现优于现有框架，推动AI向更智能自主发展。


<details>
  <summary>Details</summary>
Motivation: 现有AI代理依赖静态预定义策略，无法成为更好的战略规划者，限制其在医疗保健研究中的功效。

Method: 引入HealthFlow，通过元级进化机制自主优化高级问题解决策略；引入EHRFlowBench基准进行可重复评估。

Result: 综合实验显示HealthFlow的自进化方法显著优于现有最先进的代理框架。

Conclusion: 该工作标志着从构建更好的工具使用者向设计更智能、自进化的任务管理者转变，为科学发现的AI发展铺平道路。

Abstract: The efficacy of AI agents in healthcare research is hindered by their
reliance on static, predefined strategies. This creates a critical limitation:
agents can become better tool-users but cannot learn to become better strategic
planners, a crucial skill for complex domains like healthcare. We introduce
HealthFlow, a self-evolving AI agent that overcomes this limitation through a
novel meta-level evolution mechanism. HealthFlow autonomously refines its own
high-level problem-solving policies by distilling procedural successes and
failures into a durable, strategic knowledge base. To anchor our research and
facilitate reproducible evaluation, we introduce EHRFlowBench, a new benchmark
featuring complex, realistic health data analysis tasks derived from
peer-reviewed clinical research. Our comprehensive experiments demonstrate that
HealthFlow's self-evolving approach significantly outperforms state-of-the-art
agent frameworks. This work marks a necessary shift from building better
tool-users to designing smarter, self-evolving task-managers, paving the way
for more autonomous and effective AI for scientific discovery.

</details>


### [7] [The Docking Game: Loop Self-Play for Fast, Dynamic, and Accurate Prediction of Flexible Protein--Ligand Binding](https://arxiv.org/abs/2508.05006)
*Youzhi Zhang,Yufei Li,Gaofeng Meng,Hongbin Liu,Jiebo Luo*

Main category: cs.AI

TL;DR: 提出博弈论框架Docking Game及LoopPlay算法解决分子对接多任务学习模型中配体对接性能不佳问题，实验显示比现有方法有约10%提升。


<details>
  <summary>Details</summary>
Motivation: 当前多任务学习模型在配体对接表现不如蛋白口袋对接，主要因配体和蛋白结构复杂性不同，需提升分子对接准确性。

Method: 提出Docking Game博弈论框架，开发LoopPlay算法，通过两层循环交替训练配体和蛋白模块，内外层循环分别实现双方结构预测融合和自身预测细化。

Result: 在公共基准数据集实验表明，LoopPlay比现有方法在准确结合模式预测上约有10%提升。

Conclusion: LoopPlay算法有潜力提高药物发现中分子对接的准确性。

Abstract: Molecular docking is a crucial aspect of drug discovery, as it predicts the
binding interactions between small-molecule ligands and protein pockets.
However, current multi-task learning models for docking often show inferior
performance in ligand docking compared to protein pocket docking. This
disparity arises largely due to the distinct structural complexities of ligands
and proteins. To address this issue, we propose a novel game-theoretic
framework that models the protein-ligand interaction as a two-player game
called the Docking Game, with the ligand docking module acting as the ligand
player and the protein pocket docking module as the protein player. To solve
this game, we develop a novel Loop Self-Play (LoopPlay) algorithm, which
alternately trains these players through a two-level loop. In the outer loop,
the players exchange predicted poses, allowing each to incorporate the other's
structural predictions, which fosters mutual adaptation over multiple
iterations. In the inner loop, each player dynamically refines its predictions
by incorporating its own predicted ligand or pocket poses back into its model.
We theoretically show the convergence of LoopPlay, ensuring stable
optimization. Extensive experiments conducted on public benchmark datasets
demonstrate that LoopPlay achieves approximately a 10\% improvement in
predicting accurate binding modes compared to previous state-of-the-art
methods. This highlights its potential to enhance the accuracy of molecular
docking in drug discovery.

</details>


### [8] [Can Large Language Models Integrate Spatial Data? Empirical Insights into Reasoning Strengths and Computational Weaknesses](https://arxiv.org/abs/2508.05009)
*Bin Han,Robert Wolfe,Anat Caspi,Bill Howe*

Main category: cs.AI

TL;DR: 探索大语言模型（LLMs）在城市空间数据集集成中的应用，分析其推理能力，采用审查和改进方法，认为LLMs是有潜力的替代方案。


<details>
  <summary>Details</summary>
Motivation: 传统规则集成方法无法覆盖所有边缘情况，机器学习方法需大量特定任务样本，因此研究LLMs在空间数据集成中的潜力。

Method: 分析LLMs对环境空间关系的推理能力，采用审查和改进方法纠正初始响应。

Result: LLMs有空间推理能力，但在连接宏观环境和计算几何任务时有困难，提供相关特征可产生高性能结果，审查和改进方法有效。

Conclusion: LLMs是传统规则启发式方法的有前途且灵活的替代方案，可提升自适应空间数据集成能力。

Abstract: We explore the application of large language models (LLMs) to empower domain
experts in integrating large, heterogeneous, and noisy urban spatial datasets.
Traditional rule-based integration methods are unable to cover all edge cases,
requiring manual verification and repair. Machine learning approaches require
collecting and labeling of large numbers of task-specific samples. In this
study, we investigate the potential of LLMs for spatial data integration. Our
analysis first considers how LLMs reason about environmental spatial
relationships mediated by human experience, such as between roads and
sidewalks. We show that while LLMs exhibit spatial reasoning capabilities, they
struggle to connect the macro-scale environment with the relevant computational
geometry tasks, often producing logically incoherent responses. But when
provided relevant features, thereby reducing dependence on spatial reasoning,
LLMs are able to generate high-performing results. We then adapt a
review-and-refine method, which proves remarkably effective in correcting
erroneous initial responses while preserving accurate responses. We discuss
practical implications of employing LLMs for spatial data integration in
real-world contexts and outline future research directions, including
post-training, multi-modal integration methods, and support for diverse data
formats. Our findings position LLMs as a promising and flexible alternative to
traditional rule-based heuristics, advancing the capabilities of adaptive
spatial data integration.

</details>


### [9] [Cognitive Duality for Adaptive Web Agents](https://arxiv.org/abs/2508.05081)
*Jiarun Liu,Chunhong Zhang,Zheng Hu*

Main category: cs.AI

TL;DR: 本文受人类认知双过程理论启发，提出CogniWeb模块化代理架构，在WebArena评估中表现出色且效率高。


<details>
  <summary>Details</summary>
Motivation: 当前构建自主网络代理的方法难以有效整合离线模仿学习和在线探索，而网络导航对评估AGI很重要。

Method: 受人类认知双过程理论启发，将其分解为快速的系统1和慢速的系统2认知过程，实现CogniWeb架构，可根据任务复杂度切换处理模式。

Result: 在WebArena上评估，CogniWeb成功率达43.96%，令牌使用量减少75%。

Conclusion: CogniWeb架构能有效结合离线学习和在线获取能力，在性能和效率上表现良好。

Abstract: Web navigation represents a critical and challenging domain for evaluating
artificial general intelligence (AGI), demanding complex decision-making within
high-entropy, dynamic environments with combinatorially explosive action
spaces. Current approaches to building autonomous web agents either focus on
offline imitation learning or online exploration, but rarely integrate both
paradigms effectively. Inspired by the dual-process theory of human cognition,
we derive a principled decomposition into fast System 1 and slow System 2
cognitive processes. This decomposition provides a unifying perspective on
existing web agent methodologies, bridging the gap between offline learning of
intuitive reactive behaviors and online acquisition of deliberative planning
capabilities. We implement this framework in CogniWeb, a modular agent
architecture that adaptively toggles between fast intuitive processing and
deliberate reasoning based on task complexity. Our evaluation on WebArena
demonstrates that CogniWeb achieves competitive performance (43.96% success
rate) while maintaining significantly higher efficiency (75% reduction in token
usage).

</details>


### [10] [MedMKEB: A Comprehensive Knowledge Editing Benchmark for Medical Multimodal Large Language Models](https://arxiv.org/abs/2508.05083)
*Dexuan Xu,Jieyi Wang,Zhongyan Chai,Yongzhi Cao,Hanpin Wang,Huamin Zhang,Yu Huang*

Main category: cs.AI

TL;DR: 提出MedMKEB基准评估医学多模态大模型知识编辑能力，实验表明现有方法有局限，该基准可推动算法发展。


<details>
  <summary>Details</summary>
Motivation: 医学知识不断发展，需高效更新多模态医学大模型知识，缺乏相关系统基准。

Method: 构建MedMKEB基准，基于高质量医学视觉问答数据集，设置多种编辑任务，引入专家验证。

Result: 在现有模型上实验显示现有基于知识的编辑方法有局限。

Conclusion: MedMKEB可作为标准基准推动可靠高效医学知识编辑算法发展。

Abstract: Recent advances in multimodal large language models (MLLMs) have
significantly improved medical AI, enabling it to unify the understanding of
visual and textual information. However, as medical knowledge continues to
evolve, it is critical to allow these models to efficiently update outdated or
incorrect information without retraining from scratch. Although textual
knowledge editing has been widely studied, there is still a lack of systematic
benchmarks for multimodal medical knowledge editing involving image and text
modalities. To fill this gap, we present MedMKEB, the first comprehensive
benchmark designed to evaluate the reliability, generality, locality,
portability, and robustness of knowledge editing in medical multimodal large
language models. MedMKEB is built on a high-quality medical visual
question-answering dataset and enriched with carefully constructed editing
tasks, including counterfactual correction, semantic generalization, knowledge
transfer, and adversarial robustness. We incorporate human expert validation to
ensure the accuracy and reliability of the benchmark. Extensive single editing
and sequential editing experiments on state-of-the-art general and medical
MLLMs demonstrate the limitations of existing knowledge-based editing
approaches in medicine, highlighting the need to develop specialized editing
strategies. MedMKEB will serve as a standard benchmark to promote the
development of trustworthy and efficient medical knowledge editing algorithms.

</details>


### [11] [EasySize: Elastic Analog Circuit Sizing via LLM-Guided Heuristic Search](https://arxiv.org/abs/2508.05113)
*Xinyue Wu,Fan Hu,Shaik Jani Babu,Yi Zhao,Xinfei Guo*

Main category: cs.AI

TL;DR: 提出基于微调Qwen3 - 8B模型的轻量级门尺寸调整框架EasySize，可跨工艺节点等使用，性能好，能减少人力和计算资源依赖。


<details>
  <summary>Details</summary>
Motivation: 现有模拟电路门尺寸调整方法存在缺乏通用性、依赖大模型尺寸和缺乏跨技术节点可移植性等问题，需改进。

Method: 基于微调Qwen3 - 8B模型构建EasySize框架，利用性能指标的可实现性动态构建损失函数，结合全局差分进化和局部粒子群优化进行启发式搜索。

Result: 仅在350nm节点数据上微调，在不同技术节点的5个运算放大器网表上表现良好，86.67%的任务中优于AutoCkt，减少超96.67%的仿真资源。

Conclusion: EasySize可显著减少门尺寸调整对人力专业知识和计算资源的依赖，加速和简化模拟电路设计过程，后续将开源。

Abstract: Analog circuit design is a time-consuming, experience-driven task in chip
development. Despite advances in AI, developing universal, fast, and stable
gate sizing methods for analog circuits remains a significant challenge. Recent
approaches combine Large Language Models (LLMs) with heuristic search
techniques to enhance generalizability, but they often depend on large model
sizes and lack portability across different technology nodes. To overcome these
limitations, we propose EasySize, the first lightweight gate sizing framework
based on a finetuned Qwen3-8B model, designed for universal applicability
across process nodes, design specifications, and circuit topologies. EasySize
exploits the varying Ease of Attainability (EOA) of performance metrics to
dynamically construct task-specific loss functions, enabling efficient
heuristic search through global Differential Evolution (DE) and local Particle
Swarm Optimization (PSO) within a feedback-enhanced flow. Although finetuned
solely on 350nm node data, EasySize achieves strong performance on 5
operational amplifier (Op-Amp) netlists across 180nm, 45nm, and 22nm technology
nodes without additional targeted training, and outperforms AutoCkt, a
widely-used Reinforcement Learning based sizing framework, on 86.67\% of tasks
with more than 96.67\% of simulation resources reduction. We argue that
EasySize can significantly reduce the reliance on human expertise and
computational resources in gate sizing, thereby accelerating and simplifying
the analog circuit design process. EasySize will be open-sourced at a later
date.

</details>


### [12] [Beyond Automation: Socratic AI, Epistemic Agency, and the Implications of the Emergence of Orchestrated Multi-Agent Learning Architectures](https://arxiv.org/abs/2508.05116)
*Peer-Benedikt Degen,Igor Asanov*

Main category: cs.AI

TL;DR: 本文通过实验评估苏格拉底式AI导师，证明对话式AI能促进元认知参与，提出编排式多智能体系统概念及适配模型，分析高等教育系统层面影响并做成本效益分析，为混合学习生态提供实证和概念路线图。


<details>
  <summary>Details</summary>
Motivation: 探究生成式AI在高等教育中的应用，证明对话式AI对学生思维的促进作用，推动教学模式转变。

Method: 对德国65名职前教师学生进行对照实验，比较苏格拉底式AI导师和无指导AI聊天机器人的效果。

Result: 使用苏格拉底式AI导师的学生在批判性、独立性和反思性思维方面得到显著支持。

Conclusion: 研究为嵌入人机共主体和教学一致性的混合学习生态系统提供了实证证据和概念路线图，且系统具有可扩展性。

Abstract: Generative AI is no longer a peripheral tool in higher education. It is
rapidly evolving into a general-purpose infrastructure that reshapes how
knowledge is generated, mediated, and validated. This paper presents findings
from a controlled experiment evaluating a Socratic AI Tutor, a large language
model designed to scaffold student research question development through
structured dialogue grounded in constructivist theory. Conducted with 65
pre-service teacher students in Germany, the study compares interaction with
the Socratic Tutor to engagement with an uninstructed AI chatbot. Students
using the Socratic Tutor reported significantly greater support for critical,
independent, and reflective thinking, suggesting that dialogic AI can stimulate
metacognitive engagement and challenging recent narratives of de-skilling due
to generative AI usage. These findings serve as a proof of concept for a
broader pedagogical shift: the use of multi-agent systems (MAS) composed of
specialised AI agents. To conceptualise this, we introduce the notion of
orchestrated MAS, modular, pedagogically aligned agent constellations, curated
by educators, that support diverse learning trajectories through differentiated
roles and coordinated interaction. To anchor this shift, we propose an adapted
offer-and-use model, in which students appropriate instructional offers from
these agents. Beyond technical feasibility, we examine system-level
implications for higher education institutions and students, including funding
necessities, changes to faculty roles, curriculars, competencies and assessment
practices. We conclude with a comparative cost-effectiveness analysis
highlighting the scalability of such systems. In sum, this study contributes
both empirical evidence and a conceptual roadmap for hybrid learning ecosystems
that embed human-AI co-agency and pedagogical alignment.

</details>


### [13] [Graph-based Event Log Repair](https://arxiv.org/abs/2508.05145)
*Sebastiano Dissegna,Chiara Di Francescomarino,Massimiliano Ronzani*

Main category: cs.AI

TL;DR: 本文聚焦开发异质图神经网络模型来补全事件日志中缺失的属性，评估显示其在重建各类事件属性上表现良好。


<details>
  <summary>Details</summary>
Motivation: 现实中事件日志数据获取困难，常存在信息缺失，现有方法有局限，需新方法。

Method: 开发异质图神经网络模型，针对含不完整事件的轨迹补全属性，并用自编码器在合成和真实日志上评估。

Result: 与现有无模型方法相比，所提方法在重建所有不同事件属性上表现出色。

Conclusion: 异质图神经网络模型能有效解决事件日志中属性缺失的问题。

Abstract: The quality of event logs in Process Mining is crucial when applying any form
of analysis to them. In real-world event logs, the acquisition of data can be
non-trivial (e.g., due to the execution of manual activities and related manual
recording or to issues in collecting, for each event, all its attributes), and
often may end up with events recorded with some missing information. Standard
approaches to the problem of trace (or log) reconstruction either require the
availability of a process model that is used to fill missing values by
leveraging different reasoning techniques or employ a Machine Learning/Deep
Learning model to restore the missing values by learning from similar cases. In
recent years, a new type of Deep Learning model that is capable of handling
input data encoded as graphs has emerged, namely Graph Neural Networks. Graph
Neural Network models, and even more so Heterogeneous Graph Neural Networks,
offer the advantage of working with a more natural representation of complex
multi-modal sequences like the execution traces in Process Mining, allowing for
more expressive and semantically rich encodings.
  In this work, we focus on the development of a Heterogeneous Graph Neural
Network model that, given a trace containing some incomplete events, will
return the full set of attributes missing from those events. We evaluate our
work against a state-of-the-art approach leveraging autoencoders on two
synthetic logs and four real event logs, on different types of missing values.
Different from state-of-the-art model-free approaches, which mainly focus on
repairing a subset of event attributes, the proposed approach shows very good
performance in reconstructing all different event attributes.

</details>


### [14] [QA-Dragon: Query-Aware Dynamic RAG System for Knowledge-Intensive Visual Question Answering](https://arxiv.org/abs/2508.05197)
*Zhuohang Jiang,Pangjing Wu,Xu Yuan,Wenqi Fan,Qing Li*

Main category: cs.AI

TL;DR: 提出QA - Dragon用于知识密集型VQA，结合文本和图像搜索，在KDD Cup 2025评估中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法在处理需多跳推理或最新事实知识的复杂查询时能力受限，需要改进。

Method: 引入领域路由器识别查询主题领域，搜索路由器动态选择检索策略，采用混合设置协调文本和图像搜索代理。

Result: 在KDD Cup 2025 Meta CRAG - MM挑战赛中，显著提升基础模型推理性能，答案准确性和知识重叠得分大幅提高，在单源、多源和多轮任务上均优于基线。

Conclusion: QA - Dragon能有效处理复杂VQA任务，在相关评估中效果良好。

Abstract: Retrieval-Augmented Generation (RAG) has been introduced to mitigate
hallucinations in Multimodal Large Language Models (MLLMs) by incorporating
external knowledge into the generation process, and it has become a widely
adopted approach for knowledge-intensive Visual Question Answering (VQA).
However, existing RAG methods typically retrieve from either text or images in
isolation, limiting their ability to address complex queries that require
multi-hop reasoning or up-to-date factual knowledge. To address this
limitation, we propose QA-Dragon, a Query-Aware Dynamic RAG System for
Knowledge-Intensive VQA. Specifically, QA-Dragon introduces a domain router to
identify the query's subject domain for domain-specific reasoning, along with a
search router that dynamically selects optimal retrieval strategies. By
orchestrating both text and image search agents in a hybrid setup, our system
supports multimodal, multi-turn, and multi-hop reasoning, enabling it to tackle
complex VQA tasks effectively. We evaluate our QA-Dragon on the Meta CRAG-MM
Challenge at KDD Cup 2025, where it significantly enhances the reasoning
performance of base models under challenging scenarios. Our framework achieves
substantial improvements in both answer accuracy and knowledge overlap scores,
outperforming baselines by 5.06% on the single-source task, 6.35% on the
multi-source task, and 5.03% on the multi-turn task.

</details>


### [15] [An Explainable Natural Language Framework for Identifying and Notifying Target Audiences In Enterprise Communication](https://arxiv.org/abs/2508.05267)
*Vítor N. Lourenço,Mohnish Dubey,Yunfei Bai,Audrey Depeige,Vivek Jain*

Main category: cs.AI

TL;DR: 提出结合RDF图数据库与大语言模型的框架解决大型维护组织通信挑战，提高通信效率。


<details>
  <summary>Details</summary>
Motivation: 传统通信方法无法有效解决大型维护组织中识别专家和管理复杂实体关系通信的挑战，如信息过载和响应时间长。

Method: 提出结合RDF图数据库与大语言模型的框架，通过规划编排架构处理自然语言查询实现精准受众定位。

Result: 使通信负责人能提出结合设备、制造商等概念的直观查询，提供可解释结果。

Conclusion: 该框架可维护系统信任，提高组织内通信效率。

Abstract: In large-scale maintenance organizations, identifying subject matter experts
and managing communications across complex entities relationships poses
significant challenges -- including information overload and longer response
times -- that traditional communication approaches fail to address effectively.
We propose a novel framework that combines RDF graph databases with LLMs to
process natural language queries for precise audience targeting, while
providing transparent reasoning through a planning-orchestration architecture.
Our solution enables communication owners to formulate intuitive queries
combining concepts such as equipment, manufacturers, maintenance engineers, and
facilities, delivering explainable results that maintain trust in the system
while improving communication efficiency across the organization.

</details>


### [16] [A Novel Architecture for Symbolic Reasoning with Decision Trees and LLM Agents](https://arxiv.org/abs/2508.05311)
*Andrew Kiruluta*

Main category: cs.AI

TL;DR: 提出一种混合架构，集成决策树符号推理和大语言模型生成能力，在推理基准测试中表现出色，适用于临床决策支持和科学发现。


<details>
  <summary>Details</summary>
Motivation: 改进现有松散耦合符号和神经模块的方法，实现通用的神经符号推理。

Method: 在统一推理系统中嵌入决策树和随机森林作为可调用预言机，结合树模块和大语言模型代理功能，由中央协调器维护状态一致性和中介通信。

Result: 在推理基准测试中表现良好，如在ProofWriter、GSM8k、ARC上有显著提升，在临床决策支持和科学发现中有应用。

Conclusion: 该架构是通用神经符号推理的稳健、可解释和可扩展解决方案。

Abstract: We propose a hybrid architecture that integrates decision tree-based symbolic
reasoning with the generative capabilities of large language models (LLMs)
within a coordinated multi-agent framework. Unlike prior approaches that
loosely couple symbolic and neural modules, our design embeds decision trees
and random forests as callable oracles within a unified reasoning system.
Tree-based modules enable interpretable rule inference and causal logic, while
LLM agents handle abductive reasoning, generalization, and interactive
planning. A central orchestrator maintains belief state consistency and
mediates communication across agents and external tools, enabling reasoning
over both structured and unstructured inputs.
  The system achieves strong performance on reasoning benchmarks. On
\textit{ProofWriter}, it improves entailment consistency by +7.2\% through
logic-grounded tree validation. On GSM8k, it achieves +5.3\% accuracy gains in
multistep mathematical problems via symbolic augmentation. On \textit{ARC}, it
boosts abstraction accuracy by +6.0\% through integration of symbolic oracles.
Applications in clinical decision support and scientific discovery show how the
system encodes domain rules symbolically while leveraging LLMs for contextual
inference and hypothesis generation. This architecture offers a robust,
interpretable, and extensible solution for general-purpose neuro-symbolic
reasoning.

</details>


### [17] [The Term 'Agent' Has Been Diluted Beyond Utility and Requires Redefinition](https://arxiv.org/abs/2508.05338)
*Brinnae Bent*

Main category: cs.AI

TL;DR: 因大语言模型发展，AI中‘agent’一词歧义增大，本文建议重新定义并提出框架，给出推进建议，助于研究与政策制定。


<details>
  <summary>Details</summary>
Motivation: AI中‘agent’一词在不同子领域有多种解释，大语言模型发展放大歧义，影响研究交流、系统评估、复现和政策制定，需重新定义。

Method: 通过历史分析和当代使用模式，提出定义系统为‘agent’的框架，涉及多维度特征。

Result: 提出的方法提供精确描述词汇，保留‘agent’历史多面性，还给出领域推进具体建议。

Conclusion: 该方法为提高研究清晰度和可重复性提供实用工具，支持更有效政策制定。

Abstract: The term 'agent' in artificial intelligence has long carried multiple
interpretations across different subfields. Recent developments in AI
capabilities, particularly in large language model systems, have amplified this
ambiguity, creating significant challenges in research communication, system
evaluation and reproducibility, and policy development. This paper argues that
the term 'agent' requires redefinition. Drawing from historical analysis and
contemporary usage patterns, we propose a framework that defines clear minimum
requirements for a system to be considered an agent while characterizing
systems along a multidimensional spectrum of environmental interaction,
learning and adaptation, autonomy, goal complexity, and temporal coherence.
This approach provides precise vocabulary for system description while
preserving the term's historically multifaceted nature. After examining
potential counterarguments and implementation challenges, we provide specific
recommendations for moving forward as a field, including suggestions for
terminology standardization and framework adoption. The proposed approach
offers practical tools for improving research clarity and reproducibility while
supporting more effective policy development.

</details>


### [18] [NomicLaw: Emergent Trust and Strategic Argumentation in LLMs During Collaborative Law-Making](https://arxiv.org/abs/2508.05344)
*Asutosh Hota,Jussi P. P. Jokinen*

Main category: cs.AI

TL;DR: 介绍NomicLaw多智能体模拟实验，展示大语言模型社会推理和说服能力，为未来AI系统设计提供见解。


<details>
  <summary>Details</summary>
Motivation: 当前对大语言模型在开放式多智能体场景，特别是涉及法律和道德困境审议的行为实证理解有限。

Method: 引入NomicLaw多智能体模拟，让大语言模型参与协作立法，通过投票模式定量测量信任和互惠，定性评估智能体使用策略性语言的情况。

Result: 实验表明智能体自发结盟、背叛信任、调整言辞以影响集体决策，凸显十个开源大语言模型的潜在社会推理和说服能力。

Conclusion: 研究为未来能在法律场景自主协商、协调和起草立法的AI系统设计提供了见解。

Abstract: Recent advancements in large language models (LLMs) have extended their
capabilities from basic text processing to complex reasoning tasks, including
legal interpretation, argumentation, and strategic interaction. However,
empirical understanding of LLM behavior in open-ended, multi-agent settings
especially those involving deliberation over legal and ethical dilemmas remains
limited. We introduce NomicLaw, a structured multi-agent simulation where LLMs
engage in collaborative law-making, responding to complex legal vignettes by
proposing rules, justifying them, and voting on peer proposals. We
quantitatively measure trust and reciprocity via voting patterns and
qualitatively assess how agents use strategic language to justify proposals and
influence outcomes. Experiments involving homogeneous and heterogeneous LLM
groups demonstrate how agents spontaneously form alliances, betray trust, and
adapt their rhetoric to shape collective decisions. Our results highlight the
latent social reasoning and persuasive capabilities of ten open-source LLMs and
provide insights into the design of future AI systems capable of autonomous
negotiation, coordination and drafting legislation in legal settings.

</details>


### [19] [Minimal Model Reasoning in Description Logics: Don't Try This at Home!](https://arxiv.org/abs/2508.05350)
*Federica Di Stefano,Quentin Manière,Magdalena Ortiz,Mantas Šimkus*

Main category: cs.AI

TL;DR: 研究描述逻辑中最小模型推理问题，发现EL概念可满足性不可判定，通过施加无环条件恢复可判定性，还研究了DL - Lite家族。


<details>
  <summary>Details</summary>
Motivation: 目前对描述逻辑中最小模型推理问题理解有限，'纯'最小模型情况研究不足。

Method: 在流行描述逻辑中研究最小模型概念可满足性，施加TBox无环条件恢复可判定性。

Result: EL中最小模型概念可满足性不可判定，扩展到受限元组生成依赖片段；施加无环条件使最坏复杂度低于双指数时间；DL - Lite_horn为ExpSpace难。

Conclusion: 在描述逻辑最小模型推理问题上有了新的负面和正面结果，建立了与点态限制的联系。

Abstract: Reasoning with minimal models has always been at the core of many knowledge
representation techniques, but we still have only a limited understanding of
this problem in Description Logics (DLs). Minimization of some selected
predicates, letting the remaining predicates vary or be fixed, as proposed in
circumscription, has been explored and exhibits high complexity. The case of
`pure' minimal models, where the extension of all predicates must be minimal,
has remained largely uncharted. We address this problem in popular DLs and
obtain surprisingly negative results: concept satisfiability in minimal models
is undecidable already for $\mathcal{EL}$. This undecidability also extends to
a very restricted fragment of tuple-generating dependencies. To regain
decidability, we impose acyclicity conditions on the TBox that bring the
worst-case complexity below double exponential time and allow us to establish a
connection with the recently studied pointwise circumscription; we also derive
results in data complexity. We conclude with a brief excursion to the DL-Lite
family, where a positive result was known for DL-Lite$_{\text{core}}$, but our
investigation establishes ExpSpace-hardness already for its extension
DL-Lite$_{\text{horn}}$.

</details>


### [20] [StructVRM: Aligning Multimodal Reasoning with Structured and Verifiable Reward Models](https://arxiv.org/abs/2508.05383)
*Xiangxiang Zhang,Jingxuan Wei,Donghong Zhong,Qi Chen,Caijun Jia,Cheng Tan,Jinming Gu,Xiaobo Qin,Zhiping Liu,Liang Hu,Tong Sun,Yuchen Wu,Zewei Sun,Chenwei Lou,Hua Zheng,Tianyang Zhan,Changbao Wang,Shuangzhi Wu,Zefa Lin,Chang Guo,Sihang Yuan,Riwei Chen,Shixiong Zhao,Yingping Zhang,Gaowei Wu,Bihui Yu,Jiahui Wu,Zhehui Zhao,Qianqian Liu,Ruofeng Tang,Xingyue Huang,Bing Zhao,Mengyang Zhang,Youqiang Zhou*

Main category: cs.AI

TL;DR: 现有视觉语言模型在多问题推理任务中表现不佳，传统奖励机制太粗糙。提出StructVRM方法，实验证明其有效，验证了结构化可验证奖励训练的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在复杂多问题推理任务学习困难，传统奖励机制太粗，无法引导模型解决复杂问题。

Method: 引入StructVRM方法，使用基于模型的验证器提供细粒度、子问题级别的反馈，评估语义和数学等价性。

Result: 训练的模型Seed - StructVRM在12个公共多模态基准中的6个和新策划的高难度STEM - Bench上达到了最先进的性能。

Conclusion: 使用结构化、可验证的奖励进行训练是提高多模态模型在复杂现实推理领域能力的有效方法。

Abstract: Existing Vision-Language Models often struggle with complex, multi-question
reasoning tasks where partial correctness is crucial for effective learning.
Traditional reward mechanisms, which provide a single binary score for an
entire response, are too coarse to guide models through intricate problems with
multiple sub-parts. To address this, we introduce StructVRM, a method that
aligns multimodal reasoning with Structured and Verifiable Reward Models. At
its core is a model-based verifier trained to provide fine-grained,
sub-question-level feedback, assessing semantic and mathematical equivalence
rather than relying on rigid string matching. This allows for nuanced, partial
credit scoring in previously intractable problem formats. Extensive experiments
demonstrate the effectiveness of StructVRM. Our trained model, Seed-StructVRM,
achieves state-of-the-art performance on six out of twelve public multimodal
benchmarks and our newly curated, high-difficulty STEM-Bench. The success of
StructVRM validates that training with structured, verifiable rewards is a
highly effective approach for advancing the capabilities of multimodal models
in complex, real-world reasoning domains.

</details>


### [21] [An Explainable Machine Learning Framework for Railway Predictive Maintenance using Data Streams from the Metro Operator of Portugal](https://arxiv.org/abs/2508.05388)
*Silvia García-Méndez,Francisco de Arriba-Pérez,Fátima Leal,Bruno Veloso,Benedita Malheiro,Juan Carlos Burguillo-Rial*

Main category: cs.AI

TL;DR: 提出实时数据驱动的智能交通系统预测性维护解决方案，实验结果好，方法实用。


<details>
  <summary>Details</summary>
Motivation: 为智能交通系统提供实时数据驱动的预测性维护解决方案。

Method: 实现包含样本预处理、机器学习模型增量分类和结果解释的处理流水线。

Result: 在MetroPT数据集上实验，F - 度量超98%，准确率超99%，在类别不平衡和噪声下仍保持高性能，解释能反映决策过程。

Conclusion: 该方法合理，适用于铁路实际运营中的主动维护决策，可让决策者识别故障早期迹象并快速行动。

Abstract: This work contributes to a real-time data-driven predictive maintenance
solution for Intelligent Transportation Systems. The proposed method implements
a processing pipeline comprised of sample pre-processing, incremental
classification with Machine Learning models, and outcome explanation. This
novel online processing pipeline has two main highlights: (i) a dedicated
sample pre-processing module, which builds statistical and frequency-related
features on the fly, and (ii) an explainability module. This work is the first
to perform online fault prediction with natural language and visual
explainability. The experiments were performed with the MetroPT data set from
the metro operator of Porto, Portugal. The results are above 98 % for F-measure
and 99 % for accuracy. In the context of railway predictive maintenance,
achieving these high values is crucial due to the practical and operational
implications of accurate failure prediction. In the specific case of a high
F-measure, this ensures that the system maintains an optimal balance between
detecting the highest possible number of real faults and minimizing false
alarms, which is crucial for maximizing service availability. Furthermore, the
accuracy obtained enables reliability, directly impacting cost reduction and
increased safety. The analysis demonstrates that the pipeline maintains high
performance even in the presence of class imbalance and noise, and its
explanations effectively reflect the decision-making process. These findings
validate the methodological soundness of the approach and confirm its practical
applicability for supporting proactive maintenance decisions in real-world
railway operations. Therefore, by identifying the early signs of failure, this
pipeline enables decision-makers to understand the underlying problems and act
accordingly swiftly.

</details>


### [22] [DeepPHY: Benchmarking Agentic VLMs on Physical Reasoning](https://arxiv.org/abs/2508.05405)
*Xinrun Xu,Pi Bu,Ye Wang,Börje F. Karlsson,Ziming Wang,Tengtao Song,Qi Zhu,Jun Song,Zhiming Ding,Bo Zheng*

Main category: cs.AI

TL;DR: 现有视觉语言模型在复杂动态环境表现不佳，引入DeepPHY基准框架评估其物理原理理解与推理能力，发现即使最先进模型也难将物理知识转化为精确控制。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在复杂动态环境中难以关注细节和精确规划行动，且在真实场景评估其相关能力成本过高，需要评估模型对物理原理的理解和推理能力。

Method: 引入DeepPHY基准框架，通过一系列具有挑战性的模拟环境，整合不同难度的物理推理环境并结合细粒度评估指标来进行评估。

Result: 评估发现即使是最先进的视觉语言模型也难以将描述性物理知识转化为精确的预测控制。

Conclusion: 视觉语言模型在将物理知识转化为精确控制方面存在不足，需要进一步改进。

Abstract: Although Vision Language Models (VLMs) exhibit strong perceptual abilities
and impressive visual reasoning, they struggle with attention to detail and
precise action planning in complex, dynamic environments, leading to subpar
performance. Real-world tasks typically require complex interactions, advanced
spatial reasoning, long-term planning, and continuous strategy refinement,
usually necessitating understanding the physics rules of the target scenario.
However, evaluating these capabilities in real-world scenarios is often
prohibitively expensive. To bridge this gap, we introduce DeepPHY, a novel
benchmark framework designed to systematically evaluate VLMs' understanding and
reasoning about fundamental physical principles through a series of challenging
simulated environments. DeepPHY integrates multiple physical reasoning
environments of varying difficulty levels and incorporates fine-grained
evaluation metrics. Our evaluation finds that even state-of-the-art VLMs
struggle to translate descriptive physical knowledge into precise, predictive
control.

</details>


### [23] [Large Language Models Transform Organic Synthesis From Reaction Prediction to Automation](https://arxiv.org/abs/2508.05427)
*Kartar Kumar Lohana Tharwani,Rajesh Kumar,Sumita,Numan Ahmed,Yong Tang*

Main category: cs.AI

TL;DR: 本文介绍大语言模型在有机合成中的应用、发展、结合其他技术的优势、局限性及社区倡议，推动分子创新。


<details>
  <summary>Details</summary>
Motivation: 探讨大语言模型从投机工具转变为实用实验室伙伴的历程，推动其在有机合成中的应用。

Method: 调研大语言模型发展里程碑，介绍其与图神经网络、量子计算和实时光谱耦合。

Result: 大语言模型可提出合成路线、预测反应结果等，耦合其他技术可缩短发现周期、支持绿色化学。

Conclusion: 这些进展为人工智能和自动化驱动的分子创新指明方向，同时需关注局限性和社区倡议。

Abstract: Large language models (LLMs) are beginning to reshape how chemists plan and
run reactions in organic synthesis. Trained on millions of reported
transformations, these text-based models can propose synthetic routes, forecast
reaction outcomes and even instruct robots that execute experiments without
human supervision. Here we survey the milestones that turned LLMs from
speculative tools into practical lab partners. We show how coupling LLMs with
graph neural networks, quantum calculations and real-time spectroscopy shrinks
discovery cycles and supports greener, data-driven chemistry. We discuss
limitations, including biased datasets, opaque reasoning and the need for
safety gates that prevent unintentional hazards. Finally, we outline community
initiatives open benchmarks, federated learning and explainable interfaces that
aim to democratize access while keeping humans firmly in control. These
advances chart a path towards rapid, reliable and inclusive molecular
innovation powered by artificial intelligence and automation.

</details>


### [24] [Whose Truth? Pluralistic Geo-Alignment for (Agentic) AI](https://arxiv.org/abs/2508.05432)
*Krzysztof Janowicz,Zilong Liu,Gengchen Mai,Zhangyu Wang,Ivan Majic,Alexandra Fortacz,Grant McKenzie,Song Gao*

Main category: cs.AI

TL;DR: 本文探讨AI（超级）对齐中地理变异性被忽视的问题，强调时空感知对齐的紧迫性，并提出相关研究问题、未来工作主题及评估方法。


<details>
  <summary>Details</summary>
Motivation: 现有文献对AI对齐中地理变异性研究不足，而AI介导知识等的规模和自动化程度使解决该问题变得紧迫。

Method: 回顾关键地理研究问题，建议未来工作主题，概述评估对齐敏感性的方法。

Result: 指出AI对齐存在地理敏感性，部分模型输出因地区而异，传统方法可能与统计现实偏离。

Conclusion: 随着向代理AI发展，需要时空感知对齐而非一刀切的方法。

Abstract: AI (super) alignment describes the challenge of ensuring (future) AI systems
behave in accordance with societal norms and goals. While a quickly evolving
literature is addressing biases and inequalities, the geographic variability of
alignment remains underexplored. Simply put, what is considered appropriate,
truthful, or legal can differ widely across regions due to cultural norms,
political realities, and legislation. Alignment measures applied to AI/ML
workflows can sometimes produce outcomes that diverge from statistical
realities, such as text-to-image models depicting balanced gender ratios in
company leadership despite existing imbalances. Crucially, some model outputs
are globally acceptable, while others, e.g., questions about Kashmir, depend on
knowing the user's location and their context. This geographic sensitivity is
not new. For instance, Google Maps renders Kashmir's borders differently based
on user location. What is new is the unprecedented scale and automation with
which AI now mediates knowledge, expresses opinions, and represents geographic
reality to millions of users worldwide, often with little transparency about
how context is managed. As we approach Agentic AI, the need for
spatio-temporally aware alignment, rather than one-size-fits-all approaches, is
increasingly urgent. This paper reviews key geographic research problems,
suggests topics for future work, and outlines methods for assessing alignment
sensitivity.

</details>


### [25] [Bench-2-CoP: Can We Trust Benchmarking for EU AI Compliance?](https://arxiv.org/abs/2508.05464)
*Matteo Prandi,Vincenzo Suriani,Federico Pierucci,Marcello Galisai,Daniele Nardi,Piercosma Bisconti*

Main category: cs.AI

TL;DR: 研究引入Bench - 2 - CoP框架分析基准与欧盟AI法规差距，发现评估生态与法规关注不匹配，为政策制定者和开发者提供见解。


<details>
  <summary>Details</summary>
Motivation: 通用人工智能模型发展迅速，现有AI评估依赖基准但无法衡量新法规关注的系统风险，需量化“基准 - 法规差距”。

Method: 引入Bench - 2 - CoP框架，用大语言模型评判分析将常用基准的194,955个问题映射到欧盟AI法案的模型能力和倾向分类。

Result: 评估生态集中在少数行为倾向，如“幻觉倾向”和“歧视性偏差”，关键功能能力被忽视，“失控”和“网络攻击”等系统风险评估近乎空白。

Conclusion: 研究首次全面量化该差距，为政策制定者完善实践准则、开发者构建下一代评估工具提供关键见解，促进AI更安全合规。

Abstract: The rapid advancement of General Purpose AI (GPAI) models necessitates robust
evaluation frameworks, especially with emerging regulations like the EU AI Act
and its associated Code of Practice (CoP). Current AI evaluation practices
depend heavily on established benchmarks, but these tools were not designed to
measure the systemic risks that are the focus of the new regulatory landscape.
This research addresses the urgent need to quantify this "benchmark-regulation
gap." We introduce Bench-2-CoP, a novel, systematic framework that uses
validated LLM-as-judge analysis to map the coverage of 194,955 questions from
widely-used benchmarks against the EU AI Act's taxonomy of model capabilities
and propensities. Our findings reveal a profound misalignment: the evaluation
ecosystem is overwhelmingly focused on a narrow set of behavioral propensities,
such as "Tendency to hallucinate" (53.7% of the corpus) and "Discriminatory
bias" (28.9%), while critical functional capabilities are dangerously
neglected. Crucially, capabilities central to loss-of-control scenarios,
including evading human oversight, self-replication, and autonomous AI
development, receive zero coverage in the entire benchmark corpus. This
translates to a near-total evaluation gap for systemic risks like "Loss of
Control" (0.4% coverage) and "Cyber Offence" (0.8% coverage). This study
provides the first comprehensive, quantitative analysis of this gap, offering
critical insights for policymakers to refine the CoP and for developers to
build the next generation of evaluation tools, ultimately fostering safer and
more compliant AI.

</details>


### [26] [Can Large Language Models Generate Effective Datasets for Emotion Recognition in Conversations?](https://arxiv.org/abs/2508.05474)
*Burak Can Kaplan,Hugo Cesar De Castro Carneiro,Stefan Wermter*

Main category: cs.AI

TL;DR: 使用小型高效通用大语言模型合成ERC数据集，补充现有基准，实验表明生成数据集训练的模型有强鲁棒性和性能提升。


<details>
  <summary>Details</summary>
Motivation: 解决ERC数据稀缺、现有数据集有偏和软标签主观，以及大语言模型训练成本高、应用于ERC数据生成有限的问题。

Method: 采用小型、资源高效、通用的大语言模型合成具有多样属性的ERC数据集，补充三个最常用的ERC基准。

Result: 生成六个新数据集，使用生成数据集训练的ERC分类器模型有强鲁棒性，在现有ERC基准上性能显著提升。

Conclusion: 生成的数据集可有效补充现有ERC数据集，提升分类性能。

Abstract: Emotion recognition in conversations (ERC) focuses on identifying emotion
shifts within interactions, representing a significant step toward advancing
machine intelligence. However, ERC data remains scarce, and existing datasets
face numerous challenges due to their highly biased sources and the inherent
subjectivity of soft labels. Even though Large Language Models (LLMs) have
demonstrated their quality in many affective tasks, they are typically
expensive to train, and their application to ERC tasks--particularly in data
generation--remains limited. To address these challenges, we employ a small,
resource-efficient, and general-purpose LLM to synthesize ERC datasets with
diverse properties, supplementing the three most widely used ERC benchmarks. We
generate six novel datasets, with two tailored to enhance each benchmark. We
evaluate the utility of these datasets to (1) supplement existing datasets for
ERC classification, and (2) analyze the effects of label imbalance in ERC. Our
experimental results indicate that ERC classifier models trained on the
generated datasets exhibit strong robustness and consistently achieve
statistically significant performance improvements on existing ERC benchmarks.

</details>


### [27] [InfiAlign: A Scalable and Sample-Efficient Framework for Aligning LLMs to Enhance Reasoning Capabilities](https://arxiv.org/abs/2508.05496)
*Shuo Cai,Su Lu,Qi Zhou,Kejing Yang,Zhijie Sang,Congkai Xie,Hongxia Yang*

Main category: cs.AI

TL;DR: 介绍InfiAlign框架，结合SFT和DPO提升大语言模型推理能力，数据利用高效，效果好。


<details>
  <summary>Details</summary>
Motivation: 现有提升大语言模型推理能力的后训练方法资源消耗大，且现有数据筛选方法可扩展性差。

Method: 引入InfiAlign框架，集成SFT和DPO，用多维质量指标自动筛选高质量对齐数据。

Result: SFT模型用12%训练数据达到相近性能，泛化性强；DPO进一步提升，数学推理任务平均提升3.89%。

Conclusion: 结合原则性数据选择和全阶段后训练有效，为大推理模型对齐提供可扩展、数据高效方案。

Abstract: Large language models (LLMs) have exhibited impressive reasoning abilities on
a wide range of complex tasks. However, enhancing these capabilities through
post-training remains resource intensive, particularly in terms of data and
computational cost. Although recent efforts have sought to improve sample
efficiency through selective data curation, existing methods often rely on
heuristic or task-specific strategies that hinder scalability. In this work, we
introduce InfiAlign, a scalable and sample-efficient post-training framework
that integrates supervised fine-tuning (SFT) with Direct Preference
Optimization (DPO) to align LLMs for enhanced reasoning. At the core of
InfiAlign is a robust data selection pipeline that automatically curates
high-quality alignment data from open-source reasoning datasets using
multidimensional quality metrics. This pipeline enables significant performance
gains while drastically reducing data requirements and remains extensible to
new data sources. When applied to the Qwen2.5-Math-7B-Base model, our SFT model
achieves performance on par with DeepSeek-R1-Distill-Qwen-7B, while using only
approximately 12% of the training data, and demonstrates strong generalization
across diverse reasoning tasks. Additional improvements are obtained through
the application of DPO, with particularly notable gains in mathematical
reasoning tasks. The model achieves an average improvement of 3.89% on AIME
24/25 benchmarks. Our results highlight the effectiveness of combining
principled data selection with full-stage post-training, offering a practical
solution for aligning large reasoning models in a scalable and data-efficient
manner. The model checkpoints are available at
https://huggingface.co/InfiX-ai/InfiAlign-Qwen-7B-SFT.

</details>


### [28] [GRAIL:Learning to Interact with Large Knowledge Graphs for Retrieval Augmented Reasoning](https://arxiv.org/abs/2508.05498)
*Ge Chang,Jinbo Su,Jiacheng Liu,Pengfei Yang,Yuhao Shang,Huiwen Zheng,Hongli Ma,Yan Liang,Yuanchun Li,Yunxin Liu*

Main category: cs.AI

TL;DR: 提出GRAIL框架用于图检索增强推理，在知识图谱问答数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法处理结构化知识能力有限，图检索方法难以捕捉整体结构且存在精度控制问题，影响推理性能。

Method: 提出GRAIL框架，集成LLM引导的随机探索与路径过滤建立数据合成管道，采用两阶段训练过程学习策略，将精度-简洁性平衡目标解耦为细粒度过程监督奖励，采用交互式检索范式。

Result: 在三个知识图谱问答数据集上平均准确率提升21.01%，F1提升22.43%。

Conclusion: GRAIL框架有效提升了图检索增强推理的性能，代码和数据集已开源。

Abstract: Large Language Models (LLMs) integrated with Retrieval-Augmented Generation
(RAG) techniques have exhibited remarkable performance across a wide range of
domains. However, existing RAG approaches primarily operate on unstructured
data and demonstrate limited capability in handling structured knowledge such
as knowledge graphs. Meanwhile, current graph retrieval methods fundamentally
struggle to capture holistic graph structures while simultaneously facing
precision control challenges that manifest as either critical information gaps
or excessive redundant connections, collectively undermining reasoning
performance. To address this challenge, we propose GRAIL: Graph-Retrieval
Augmented Interactive Learning, a framework designed to interact with
large-scale graphs for retrieval-augmented reasoning. Specifically, GRAIL
integrates LLM-guided random exploration with path filtering to establish a
data synthesis pipeline, where a fine-grained reasoning trajectory is
automatically generated for each task. Based on the synthesized data, we then
employ a two-stage training process to learn a policy that dynamically decides
the optimal actions at each reasoning step. The overall objective of
precision-conciseness balance in graph retrieval is decoupled into fine-grained
process-supervised rewards to enhance data efficiency and training stability.
In practical deployment, GRAIL adopts an interactive retrieval paradigm,
enabling the model to autonomously explore graph paths while dynamically
balancing retrieval breadth and precision. Extensive experiments have shown
that GRAIL achieves an average accuracy improvement of 21.01% and F1
improvement of 22.43% on three knowledge graph question-answering datasets. Our
source code and datasets is available at https://github.com/Changgeww/GRAIL.

</details>


### [29] [Auto-Eval Judge: Towards a General Agentic Framework for Task Completion Evaluation](https://arxiv.org/abs/2508.05508)
*Roshita Bhonsle,Rishav Dutta,Sneha Vavilapalli,Harsh Seth,Abubakarr Jaye,Yapei Chang,Mukund Rungta,Emmanuel Aboah Boateng,Sadid Hasan,Ehi Nosakhare,Soundar Srinivasan*

Main category: cs.AI

TL;DR: 现有评估方法有局限，提出通用模块化框架评估智能体任务完成情况，经基准测试验证其效果更好。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法如LLM - as - a - Judge只关注最终输出、Agent - as - a - Judge系统适用于特定领域，需要通用评估框架。

Method: 提出通用模块化框架，将任务分解为子任务，用可用信息验证每一步，模块输出汇总得出最终评估结果。

Result: 在GAIA和BigCodeBench两个基准测试中，Judge Agent预测任务成功与人类评估的一致性更高，比基于GPT - 4o的LLM - as - a - Judge基线准确率分别高4.76%和10.52%。

Conclusion: 所提出的通用评估框架有应用潜力。

Abstract: The increasing adoption of foundation models as agents across diverse domains
necessitates a robust evaluation framework. Current methods, such as
LLM-as-a-Judge, focus only on final outputs, overlooking the step-by-step
reasoning that drives agentic decision-making. Meanwhile, existing
Agent-as-a-Judge systems, where one agent evaluates another's task completion,
are typically designed for narrow, domain-specific settings. To address this
gap, we propose a generalizable, modular framework for evaluating agent task
completion independent of the task domain. The framework emulates human-like
evaluation by decomposing tasks into sub-tasks and validating each step using
available information, such as the agent's output and reasoning. Each module
contributes to a specific aspect of the evaluation process, and their outputs
are aggregated to produce a final verdict on task completion. We validate our
framework by evaluating the Magentic-One Actor Agent on two benchmarks, GAIA
and BigCodeBench. Our Judge Agent predicts task success with closer agreement
to human evaluations, achieving 4.76% and 10.52% higher alignment accuracy,
respectively, compared to the GPT-4o based LLM-as-a-Judge baseline. This
demonstrates the potential of our proposed general-purpose evaluation
framework.

</details>


### [30] [Streamlining Admission with LOR Insights: AI-Based Leadership Assessment in Online Master's Program](https://arxiv.org/abs/2508.05513)
*Meryem Yilmaz Soylu,Adrian Gallard,Jeonghyun Lee,Gayane Grigoryan,Rushil Desai,Stephen Harmon*

Main category: cs.AI

TL;DR: 研究引入AI工具LORI评估推荐信中领导力技能，新模型在测试中表现好，整合LORI到招生流程很重要。


<details>
  <summary>Details</summary>
Motivation: 推荐信审阅耗时费力，为支持招生委员会给学生成长提供反馈，需评估申请人领导力技能。

Method: 采用自然语言处理，利用基于RoBERTa和LLAMA的大语言模型识别领导力属性。

Result: 最新RoBERTa模型加权F1分数91.6%、精确率92.4%、召回率91.6%，在测试数据中表现稳定。

Conclusion: 将LORI整合到研究生招生流程对准确评估申请人领导力能力至关重要，可简化招生流程，实现更全面评估。

Abstract: Letters of recommendation (LORs) provide valuable insights into candidates'
capabilities and experiences beyond standardized test scores. However,
reviewing these text-heavy materials is time-consuming and labor-intensive. To
address this challenge and support the admission committee in providing
feedback for students' professional growth, our study introduces LORI: LOR
Insights, a novel AI-based detection tool for assessing leadership skills in
LORs submitted by online master's program applicants. By employing natural
language processing and leveraging large language models using RoBERTa and
LLAMA, we seek to identify leadership attributes such as teamwork,
communication, and innovation. Our latest RoBERTa model achieves a weighted F1
score of 91.6%, a precision of 92.4%, and a recall of 91.6%, showing a strong
level of consistency in our test data. With the growing importance of
leadership skills in the STEM sector, integrating LORI into the graduate
admissions process is crucial for accurately assessing applicants' leadership
capabilities. This approach not only streamlines the admissions process but
also automates and ensures a more comprehensive evaluation of candidates'
capabilities.

</details>


### [31] [MV-Debate: Multi-view Agent Debate with Dynamic Reflection Gating for Multimodal Harmful Content Detection in Social Media](https://arxiv.org/abs/2508.05557)
*Rui Lu,Jinhe Bi,Yunpu Ma,Feng Xiao,Yuntao Du,Yijun Tian*

Main category: cs.AI

TL;DR: 提出MV - Debate框架用于统一多模态有害内容检测，实验显示其性能优于基线，凸显多智能体辩论在检测社交意图的潜力。


<details>
  <summary>Details</summary>
Motivation: 社交媒体是复杂多模态环境，有害意图识别因跨模态矛盾、文化快速转变和微妙语用线索而具有挑战性。

Method: 提出MV - Debate多视图智能体辩论框架，包含四个互补辩论智能体，通过迭代辩论和反思，在反思增益标准下优化响应。

Result: 在三个基准数据集上的实验表明，MV - Debate显著优于强大的单模型和现有的多智能体辩论基线。

Conclusion: 多智能体辩论在安全关键的在线环境中推进可靠的社交意图检测方面有很大潜力。

Abstract: Social media has evolved into a complex multimodal environment where text,
images, and other signals interact to shape nuanced meanings, often concealing
harmful intent. Identifying such intent, whether sarcasm, hate speech, or
misinformation, remains challenging due to cross-modal contradictions, rapid
cultural shifts, and subtle pragmatic cues. To address these challenges, we
propose MV-Debate, a multi-view agent debate framework with dynamic reflection
gating for unified multimodal harmful content detection. MV-Debate assembles
four complementary debate agents, a surface analyst, a deep reasoner, a
modality contrast, and a social contextualist, to analyze content from diverse
interpretive perspectives. Through iterative debate and reflection, the agents
refine responses under a reflection-gain criterion, ensuring both accuracy and
efficiency. Experiments on three benchmark datasets demonstrate that MV-Debate
significantly outperforms strong single-model and existing multi-agent debate
baselines. This work highlights the promise of multi-agent debate in advancing
reliable social intent detection in safety-critical online contexts.

</details>


### [32] [The Missing Reward: Active Inference in the Era of Experience](https://arxiv.org/abs/2508.05619)
*Bo Wen*

Main category: cs.AI

TL;DR: 论文认为主动推理（AIF）为开发能从经验中学习的自主AI代理提供基础，指出当前AI范式面临可扩展性挑战及‘接地代理差距’，提出AIF可弥合差距，结合大语言模型能创建高效学习且符合人类价值观的AI系统。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统在训练数据和奖励设计上面临可扩展性挑战，无法自主制定、调整和追求目标，需要新方法推动向真正自主智能发展。

Method: 用AIF的内在自由能最小化驱动取代外部奖励信号，将大语言模型作为生成世界模型与AIF决策框架集成。

Result: 有望创建能从经验中高效学习且符合人类价值观的AI代理。

Conclusion: AIF与大语言模型的结合为开发能在计算和物理约束下自主发展的AI系统提供了可行路径。

Abstract: This paper argues that Active Inference (AIF) provides a crucial foundation
for developing autonomous AI agents capable of learning from experience without
continuous human reward engineering. As AI systems begin to exhaust
high-quality training data and rely on increasingly large human workforces for
reward design, the current paradigm faces significant scalability challenges
that could impede progress toward genuinely autonomous intelligence. The
proposal for an ``Era of Experience,'' where agents learn from self-generated
data, is a promising step forward. However, this vision still depends on
extensive human engineering of reward functions, effectively shifting the
bottleneck from data curation to reward curation. This highlights what we
identify as the \textbf{grounded-agency gap}: the inability of contemporary AI
systems to autonomously formulate, adapt, and pursue objectives in response to
changing circumstances. We propose that AIF can bridge this gap by replacing
external reward signals with an intrinsic drive to minimize free energy,
allowing agents to naturally balance exploration and exploitation through a
unified Bayesian objective. By integrating Large Language Models as generative
world models with AIF's principled decision-making framework, we can create
agents that learn efficiently from experience while remaining aligned with
human values. This synthesis offers a compelling path toward AI systems that
can develop autonomously while adhering to both computational and physical
constraints.

</details>


### [33] [Simulating Human-Like Learning Dynamics with LLM-Empowered Agents](https://arxiv.org/abs/2508.05622)
*Yu Yuan,Lili Zhao,Wei Chen,Guangting Zheng,Kai Zhang,Mengdi Zhang,Qi Liu*

Main category: cs.AI

TL;DR: 本文提出基于大语言模型的多智能体框架LearnerAgent模拟教学环境，跟踪学习者学习进度，有多项发现且模拟实验表明该框架契合实际场景。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以捕捉学习动态、跟踪进度和提供可解释性，需要新方法。

Method: 引入LearnerAgent框架，构建不同心理特征的学习者，通过知识获取、策略选择、测试和同伴互动跟踪学习进度。

Result: 1. 只有深度学习者实现持续认知增长，“陷阱问题”可诊断浅层学习者；2. 学习者行为和认知模式与心理特征相符；3. 学习者自我概念得分现实演变；4. 基础大语言模型默认是“勤奋但脆弱的浅层学习者”。

Conclusion: LearnerAgent框架与实际场景契合，能对大语言模型行为提供更深刻见解。

Abstract: Capturing human learning behavior based on deep learning methods has become a
major research focus in both psychology and intelligent systems. Recent
approaches rely on controlled experiments or rule-based models to explore
cognitive processes. However, they struggle to capture learning dynamics, track
progress over time, or provide explainability. To address these challenges, we
introduce LearnerAgent, a novel multi-agent framework based on Large Language
Models (LLMs) to simulate a realistic teaching environment. To explore
human-like learning dynamics, we construct learners with psychologically
grounded profiles-such as Deep, Surface, and Lazy-as well as a persona-free
General Learner to inspect the base LLM's default behavior. Through weekly
knowledge acquisition, monthly strategic choices, periodic tests, and peer
interaction, we can track the dynamic learning progress of individual learners
over a full-year journey. Our findings are fourfold: 1) Longitudinal analysis
reveals that only Deep Learner achieves sustained cognitive growth. Our
specially designed "trap questions" effectively diagnose Surface Learner's
shallow knowledge. 2) The behavioral and cognitive patterns of distinct
learners align closely with their psychological profiles. 3) Learners'
self-concept scores evolve realistically, with the General Learner developing
surprisingly high self-efficacy despite its cognitive limitations. 4)
Critically, the default profile of base LLM is a "diligent but brittle Surface
Learner"-an agent that mimics the behaviors of a good student but lacks true,
generalizable understanding. Extensive simulation experiments demonstrate that
LearnerAgent aligns well with real scenarios, yielding more insightful findings
about LLMs' behavior.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [34] [PriceFM: Foundation Model for Probabilistic Electricity Price Forecasting](https://arxiv.org/abs/2508.04875)
*Runyao Yu,Chenhui Gu,Jochen Stiasny,Qingsong Wen,Wasim Sarwar Dilov,Lianlian Qi,Jochen L. Cremer*

Main category: cs.CE

TL;DR: 本文针对欧洲电价预测挑战，引入新数据集并提出PriceFM模型，实验证实其有效性且优于基线模型，代码和数据集公开。


<details>
  <summary>Details</summary>
Motivation: 欧洲电力市场日益整合互联，现有电价预测方法难以捕捉复杂空间依赖和不确定性。

Method: 引入2022 - 2025年24个欧洲国家（38个地区）的数据集，提出集成图归纳偏置的时空基础模型PriceFM进行多区域、多时间步和多分位数的概率电价预测。

Result: 大量实验和消融研究表明，模型有效，持续优于竞争基线模型。

Conclusion: 强调了空间上下文在电力市场中的重要性，所提模型和数据集有积极意义。

Abstract: Electricity price forecasting in Europe presents unique challenges due to the
continent's increasingly integrated and physically interconnected power market.
While recent advances in deep learning and foundation models have led to
substantial improvements in general time series forecasting, most existing
approaches fail to capture the complex spatial interdependencies and
uncertainty inherent in electricity markets. In this paper, we address these
limitations by introducing a comprehensive and up-to-date dataset across 24
European countries (38 regions), spanning from 2022-01-01 to 2025-01-01.
Building on this groundwork, we propose PriceFM, a spatiotemporal foundation
model that integrates graph-based inductive biases to capture spatial
interdependencies across interconnected electricity markets. The model is
designed for multi-region, multi-timestep, and multi-quantile probabilistic
electricity price forecasting. Extensive experiments and ablation studies
confirm the model's effectiveness, consistently outperforming competitive
baselines and highlighting the importance of spatial context in electricity
markets. The dataset and code can be found at
https://github.com/runyao-yu/PriceFM.

</details>


### [35] [Sentiment-Aware Stock Price Prediction with Transformer and LLM-Generated Formulaic Alpha](https://arxiv.org/abs/2508.04975)
*Qizhao Chen,Hiroaki Kawashima*

Main category: cs.CE

TL;DR: 本文提出结合基于提示的大语言模型与Transformer模型的框架生成公式化阿尔法，实验表明其能提高股票价格预测准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统手工制作公式化阿尔法耗时且难扩展，利用大语言模型推理能力实现自动化生成。

Method: 引入结合基于提示的大语言模型与Transformer模型的框架，用大语言模型生成阿尔法作为特征，输入多种预测模型预测股价。

Result: 大语言模型生成的阿尔法显著提高了预测准确性，其提供的自然语言推理增强了预测的可解释性和透明度。

Conclusion: 大语言模型生成的阿尔法可用于股票价格预测，支持更明智的金融决策。

Abstract: Traditionally, traders and quantitative analysts address alpha decay by
manually crafting formulaic alphas, mathematical expressions that identify
patterns or signals in financial data, through domain expertise and
trial-and-error. This process is often time-consuming and difficult to scale.
With recent advances in large language models (LLMs), it is now possible to
automate the generation of such alphas by leveraging the reasoning capabilities
of LLMs. This paper introduces a novel framework that integrates a prompt-based
LLM with a Transformer model for stock price prediction. The LLM first
generates diverse and adaptive alphas using structured inputs such as
historical stock features (Close, Open, High, Low, Volume), technical
indicators, sentiment scores of both target and related companies. These
alphas, instead of being used directly for trading, are treated as high-level
features that capture complex dependencies within the financial data. To
evaluate the effectiveness of these LLM-generated formulaic alphas, the alpha
features are then fed into prediction models such as Transformer, LSTM, TCN,
SVR, and Random Forest to forecast future stock prices. Experimental results
demonstrate that the LLM-generated alphas significantly improve predictive
accuracy. Moreover, the accompanying natural language reasoning provided by the
LLM enhances the interpretability and transparency of the predictions,
supporting more informed financial decision-making.

</details>


### [36] [Fuzzy Decisions on Fluid Instabilities: Autoencoder-Based Reconstruction meets Rule-Based Anomaly Classification](https://arxiv.org/abs/2508.05418)
*Bharadwaj Dogga,Gibin M. Raju,Wilhelm Louw,Kelly Cohen*

Main category: cs.CE

TL;DR: 本文提出结合无监督自编码器模型与模糊推理系统的混合框架对阴影成像中的激波进行分类，该方法有效且为流体应用诊断奠定基础。


<details>
  <summary>Details</summary>
Motivation: 阴影成像中激波分类因标记数据有限和流场结构复杂而具有挑战性。

Method: 提出结合无监督自编码器模型与模糊推理系统的混合框架生成和解释异常图。

Result: 混合β - VAE自编码器与基于模糊规则的系统能最有效地捕捉相干激波特征，提升异常分类。

Conclusion: 该方法可对流动扰动进行可解释的无监督分类，为实验和工业流体应用的实时、基于物理的诊断奠定基础。

Abstract: Shockwave classification in shadowgraph imaging is challenging due to limited
labeled data and complex flow structures. This study presents a hybrid
framework that combines unsupervised autoencoder models with a fuzzy inference
system to generate and interpret anomaly maps. Among the evaluated methods, the
hybrid $\beta$-VAE autoencoder with a fuzzy rule-based system most effectively
captured coherent shock features, integrating spatial context to enhance
anomaly classification. The resulting approach enables interpretable,
unsupervised classification of flow disruptions and lays the groundwork for
real-time, physics-informed diagnostics in experimental and industrial fluid
applications.

</details>


### [37] [Deconstructing the Crystal Ball: From Ad-Hoc Prediction to Principled Startup Evaluation with the SAISE Framework](https://arxiv.org/abs/2508.05491)
*Seyed Mohammad Ali Jafari,Ali Mobini Dehkordi,Ehsan Chitsaz,Yadollah Yaghoobzadeh*

Main category: cs.CE

TL;DR: 本文针对人工智能融入初创企业评估研究方法碎片化问题，进行文献综述，指出不足并提出SAISE框架。


<details>
  <summary>Details</summary>
Motivation: 现有关于人工智能融入初创企业评估的学术研究方法碎片化，限制了预测模型的可比性、可靠性和实用性，需解决此问题。

Method: 对57项实证研究进行全面系统的文献综述，剖析人工智能驱动的初创企业预测领域的特征、算法、数据源和评估实践。

Result: 发现该领域存在核心悖论，即工具包趋同但方法严谨性差异大，还识别出四个基础性弱点。

Conclusion: 提出SAISE框架，为该领域研究提供新的标准，引导研究从临时预测走向原则性评估。

Abstract: The integration of Artificial Intelligence (AI) into startup evaluation
represents a significant technological shift, yet the academic research
underpinning this transition remains methodologically fragmented. Existing
studies often employ ad-hoc approaches, leading to a body of work with
inconsistent definitions of success, atheoretical features, and a lack of
rigorous validation. This fragmentation severely limits the comparability,
reliability, and practical utility of current predictive models.
  To address this critical gap, this paper presents a comprehensive systematic
literature review of 57 empirical studies. We deconstruct the current
state-of-the-art by systematically mapping the features, algorithms, data
sources, and evaluation practices that define the AI-driven startup prediction
landscape. Our synthesis reveals a field defined by a central paradox: a strong
convergence on a common toolkit -- venture databases and tree-based ensembles
-- but a stark divergence in methodological rigor. We identify four
foundational weaknesses: a fragmented definition of "success," a divide between
theory-informed and data-driven feature engineering, a chasm between common and
best-practice model validation, and a nascent approach to data ethics and
explainability.
  In response to these findings, our primary contribution is the proposal of
the Systematic AI-driven Startup Evaluation (SAISE) Framework. This novel,
five-stage prescriptive roadmap is designed to guide researchers from ad-hoc
prediction toward principled evaluation. By mandating a coherent, end-to-end
methodology that emphasizes stage-aware problem definition, theory-informed
data synthesis, principled feature engineering, rigorous validation, and
risk-aware interpretation, the SAISE framework provides a new standard for
conducting more comparable, robust, and practically relevant research in this
rapidly maturing domain

</details>


### [38] [Categorising SME Bank Transactions with Machine Learning and Synthetic Data Generation](https://arxiv.org/abs/2508.05425)
*Aluffi Pietro Alessandro,Brandi Jess,Marya Bazzi,Kate Kennedy,Matt Arderne,Daniel Rodrigues,Martin Lotz*

Main category: cs.CE

TL;DR: 论文针对中小企业交易分析难题，提出银行分类管道，实验效果好，为现金流贷款应用提供实用方案。


<details>
  <summary>Details</summary>
Motivation: 中小企业获传统融资有障碍，现金流贷款依赖交易数据建模，但交易分析存在文本非结构化等挑战。

Method: 提出银行分类管道，含合成数据生成模块、微调分类模型、校准方法。

Result: 在保留数据上标准准确率达73.49%（±5.09），高置信预测准确率达90.36%（±6.52），跨企业和交易泛化性好。

Conclusion: 该框架解决数据难题，为数据稀缺的中小企业贷款构建稳健分类系统提供实用方案。

Abstract: Despite their significant economic contributions, Small and Medium
Enterprises (SMEs) face persistent barriers to securing traditional financing
due to information asymmetries. Cash flow lending has emerged as a promising
alternative, but its effectiveness depends on accurate modelling of
transaction-level data. The main challenge in SME transaction analysis lies in
the unstructured nature of textual descriptions, characterised by extreme
abbreviations, limited context, and imbalanced label distributions. While
consumer transaction descriptions often show significant commonalities across
individuals, SME transaction descriptions are typically nonstandard and
inconsistent across businesses and industries. To address some of these
challenges, we propose a bank categorisation pipeline that leverages synthetic
data generation to augment existing transaction data sets. Our approach
comprises three core components: (1) a synthetic data generation module that
replicates transaction properties while preserving context and semantic
meaning; (2) a fine-tuned classification model trained on this enriched
dataset; and (3) a calibration methodology that aligns model outputs with
real-world label distributions. Experimental results demonstrate that our
approach achieves 73.49% (+-5.09) standard accuracy on held-out data, with
high-confidence predictions reaching 90.36% (+-6.52) accuracy. The model
exhibits robust generalisation across different types of SMEs and transactions,
which makes it suitable for practical deployment in cash-flow lending
applications. By addressing core data challenges, namely, scarcity, noise, and
imbalance, our framework provides a practical solution to build robust
classification systems in data-sparse SME lending contexts.

</details>


### [39] [Latent Space Diffusion for Topology Optimization](https://arxiv.org/abs/2508.05624)
*Aaron Lutheran,Srijan Das,Alireza Tabarraei*

Main category: cs.CE

TL;DR: 提出结合LDMs和VAEs的框架实现优化拓扑的快速条件生成，在多方面表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统基于梯度的拓扑优化方法在分辨率和维度增加时扩展性差，需多次有限元分析和灵敏度评估。

Method: 结合LDMs和VAEs，将有物理意义的场作为输入通道，引入辅助损失函数。

Result: 在大型合成数据集上的数值实验表明，VAE - LDM框架在柔顺性精度、体积控制和结构连通性方面优于现有基于扩散的方法。

Conclusion: 该框架是传统方法的强大且可扩展的替代方案。

Abstract: Topology optimization enables the automated design of efficient structures by
optimally distributing material within a defined domain. However, traditional
gradient-based methods often scale poorly with increasing resolution and
dimensionality due to the need for repeated finite element analyses and
sensitivity evaluations. In this work, we propose a novel framework that
combines latent diffusion models (LDMs) with variational autoencoders (VAEs) to
enable fast, conditional generation of optimized topologies. Unlike prior
approaches, our method conditions the generative process on physically
meaningful fields, specifically von Mises stress, strain energy density, volume
fraction, and loading information, embedded as dense input channels. To further
guide the generation process, we introduce auxiliary loss functions that
penalize floating material, load imbalance, and volume fraction deviation,
thereby encouraging physically realistic and manufacturable designs. Numerical
experiments on a large synthetic dataset demonstrate that our VAE-LDM framework
outperforms existing diffusion-based methods in compliance accuracy, volume
control, and structural connectivity, providing a robust and scalable
alternative to conventional

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [40] [AgenticData: An Agentic Data Analytics System for Heterogeneous Data](https://arxiv.org/abs/2508.05002)
*Ji Sun,Guoliang Li,Peiyao Zhou,Yihui Ma,Jingzhe Xu,Yuan Li*

Main category: cs.DB

TL;DR: 提出AgenticData系统，可让用户用自然语言提问并跨多领域分析数据，实验显示其准确性超现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有非结构化数据分析系统依赖专家编写代码和管理复杂流程，成本高且耗时。

Method: 采用反馈驱动规划技术将自然语言查询转为语义计划；提出多智能体协作策略，包括数据探查、语义交叉验证和智能记忆智能体；提出语义优化模型。

Result: 使用三个基准测试，AgenticData在简单和困难任务上都有出色准确性，显著优于现有方法。

Conclusion: AgenticData系统在数据分析上有显著优势，能有效解决现有系统的问题。

Abstract: Existing unstructured data analytics systems rely on experts to write code
and manage complex analysis workflows, making them both expensive and
time-consuming. To address these challenges, we introduce AgenticData, an
innovative agentic data analytics system that allows users to simply pose
natural language (NL) questions while autonomously analyzing data sources
across multiple domains, including both unstructured and structured data.
First, AgenticData employs a feedback-driven planning technique that
automatically converts an NL query into a semantic plan composed of relational
and semantic operators. We propose a multi-agent collaboration strategy by
utilizing a data profiling agent for discovering relevant data, a semantic
cross-validation agent for iterative optimization based on feedback, and a
smart memory agent for maintaining short-term context and long-term knowledge.
Second, we propose a semantic optimization model to refine and execute semantic
plans effectively. Our system, AgenticData, has been tested using three
benchmarks. Experimental results showed that AgenticData achieved superior
accuracy on both easy and difficult tasks, significantly outperforming
state-of-the-art methods.

</details>


### [41] [Making Prompts First-Class Citizens for Adaptive LLM Pipelines](https://arxiv.org/abs/2508.05012)
*Ugur Cetintemel,Shu Chen,Alexander W. Lee,Deepti Raghavan*

Main category: cs.DB

TL;DR: 本文提出 SPEAR 语言和运行时系统，使提示结构化、自适应，解决提示管理问题，并进行了初步实验。


<details>
  <summary>Details</summary>
Motivation: 现代大语言模型（LLM）管道中提示是脆弱、不透明的字符串，与周围数据流脱节，限制了复用、优化和运行时控制。

Method: 定义 SPEAR 语言和运行时，包含提示代数，支持多种细化模式，将提示逻辑视为结构化数据以实现优化。

Result: 进行初步实验，量化不同细化模式与静态提示和代理重试的行为，以及提示级优化的影响。

Conclusion: SPEAR 能填补提示管理空白，使提示成为执行模型的结构化、自适应和头等组件。

Abstract: Modern LLM pipelines increasingly resemble data-centric systems: they
retrieve external context, compose intermediate outputs, validate results, and
adapt based on runtime feedback. Yet, the central element guiding this process
-- the prompt -- remains a brittle, opaque string, disconnected from the
surrounding dataflow. This disconnect limits reuse, optimization, and runtime
control.
  In this paper, we describe our vision and an initial design for SPEAR, a
language and runtime that fills this prompt management gap by making prompts
structured, adaptive, and first-class components of the execution model. SPEAR
enables (1) runtime prompt refinement -- modifying prompts dynamically in
response to execution-time signals such as confidence, latency, or missing
context; and (2) structured prompt management -- organizing prompt fragments
into versioned views with support for introspection and logging.
  SPEAR defines a prompt algebra that governs how prompts are constructed and
adapted within a pipeline. It supports multiple refinement modes (manual,
assisted, and automatic), giving developers a balance between control and
automation. By treating prompt logic as structured data, SPEAR enables
optimizations such as operator fusion, prefix caching, and view reuse.
Preliminary experiments quantify the behavior of different refinement modes
compared to static prompts and agentic retries, as well as the impact of
prompt-level optimizations such as operator fusion.

</details>


### [42] [Data-Aware Socratic Query Refinement in Database Systems](https://arxiv.org/abs/2508.05061)
*Ruiyuan Zhang,Chrysanthi Kosyfaki,Xiaofang Zhou*

Main category: cs.DB

TL;DR: 提出对话式查询增强框架DASG，在三个数据集上评估，展示了提高查询精度并保持效率的效果。


<details>
  <summary>Details</summary>
Motivation: 解决自然语言查询中的歧义问题。

Method: 将交互式澄清作为一等操作嵌入数据库系统，把对话视为优化决策，通过多方面量化歧义，结合多因素选择最优澄清。

Result: 在三个数据集上评估，DASG提高了查询精度并保持了效率。

Conclusion: 建立了系统主动参与查询制定的合作分析范式。

Abstract: In this paper, we propose Data-Aware Socratic Guidance (DASG), a
dialogue-based query enhancement framework that embeds \linebreak interactive
clarification as a first-class operator within database systems to resolve
ambiguity in natural language queries. DASG treats dialogue as an optimization
decision, asking clarifying questions only when the expected execution cost
reduction exceeds the interaction overhead. The system quantifies ambiguity
through linguistic fuzziness, schema grounding confidence, and projected costs
across relational and vector backends. Our algorithm selects the optimal
clarifications by combining semantic relevance, catalog-based information gain,
and potential cost reduction. We evaluate our proposed framework on three
datasets. The results show that DASG demonstrates improved query precision
while maintaining efficiency, establishing a cooperative analytics paradigm
where systems actively participate in query formulation rather than passively
translating user requests.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [43] [OPTIMUMP2P: Fast and Reliable Gossiping in P2P Networks](https://arxiv.org/abs/2508.04833)
*Nicolas Nicolaou,Onyeka Obi,Aayush Rajasekaran,Alejandro Bergasov,Aleksandr Bezobchuk,Kishori M. Konwar,Michael Meier,Santiago Paiva,Har Preet Singh,Swarnabha Sinha*

Main category: cs.DC

TL;DR: 本文提出新的流言算法OPTIMUMP2P，利用随机线性网络编码（RLNC）提升libp2p性能和可靠性，评估显示其优于Gossipsub协议。


<details>
  <summary>Details</summary>
Motivation: 现有流言算法需提升性能和可靠性，尤其在存在恶意节点时，以改进libp2p。

Method: 引入OPTIMUMP2P算法，利用RLNC加速信息传播并确保可靠传输。

Result: 在模拟和现实环境评估显示OPTIMUMP2P相比Gossipsub协议有性能提升。

Conclusion: OPTIMUMP2P算法能有效提升libp2p在P2P网络中的性能和可靠性。

Abstract: Gossip algorithms are pivotal in the dissemination of information within
decentralized systems. Consequently, numerous gossip libraries have been
developed and widely utilized especially in blockchain protocols for the
propagation of blocks and transactions. A well-established library is libp2p,
which provides two gossip algorithms: floodsup and gossibsup. These algorithms
enable the delivery of published messages to a set of peers. In this work we
aim to enhance the performance and reliability of libp2p by introducing
OPTIMUMP2P, a novel gossip algorithm that leverages the capabilities of Random
Linear Network Coding (RLNC) to expedite the dissemination of information in a
peer-to-peer (P2P) network while ensuring reliable delivery, even in the
presence of malicious actors capable of corrupting the transmitted data.
Preliminary research from the Ethereum Foundation has demonstrated the use of
RLNC in the significant improvement in the block propagation time [14]. Here we
present extensive evaluation results both in simulation and real-world
environments that demonstrate the performance gains of OPTIMUMP2P over the
Gossipsub protocol.

</details>


### [44] [Linear Search for Capturing an Oblivious Mobile Target in the Sender/Receiver Model](https://arxiv.org/abs/2508.04870)
*Khaled Jawhar,Evangelos Kranakis*

Main category: cs.DC

TL;DR: 研究两个不同通信能力的自主机器人在线性搜索中捕获遗忘移动目标的问题，设计新算法并分析竞争比，探讨非对称通信的影响。


<details>
  <summary>Details</summary>
Motivation: 研究不同通信能力的机器人在线性搜索中捕获移动目标，了解非对称通信对线性搜索竞争比的影响。

Method: 设计新的线性搜索算法，考虑机器人对搜索环境的不同认知场景。

Result: 分析了捕获目标所需时间的竞争比。

Conclusion: 本研究有助于理解非对称通信如何影响线性搜索的竞争比。

Abstract: We consider linear search for capturing an oblivious moving target by two
autonomous robots with different communicating abilities. Both robots can
communicate Face-to-Face (F2F) when co-located but in addition one robot is a
Sender (can also send messages wirelessly) and the other also a Receiver (can
also receive messages wirelessly). This is known as Sender/Receiver (S/R, for
short) communication model. The robots can move with max speed $1$. The moving
target starts at distance $d$ from the origin and can move either with speed
$v<1$ away from the origin in the ``away'' model or with speed $v \geq 0$
toward the origin in the ``toward'' model. We assume that the direction of
motion of the target (i.e., whether it is the away or toward model) is known to
the robots in advance. To capture the target the two robots must be co-located
with it.
  We design new linear search algorithms and analyze the competitive ratio of
the time required to capture the target. The approach takes into account
various scenarios related to what the robots know about the search environment
(e.g., starting distance or speed of the mobile, away or toward model, or a
combination thereof). Our study contributes to understanding how asymmetric
communication affects the competitive ratio of linear search.

</details>


### [45] [Managing, Analyzing and Sharing Research Data with Gen3 Data Commons](https://arxiv.org/abs/2508.04944)
*Craig Barnes,Kyle Burton,Michael S. Fitzsimons,Hara Prasad Juvvala,Brienna Larrick,Christopher Meyer,Pauline Ribeyre,Ao Liu,Clint Malson,Noah Metoki-Shlubsky,Andrii Prokhorenkov,Jawad Qureshi,Radhika Reddy,L. Philip Schumm,Mingfei Shao,Trevar Simmons,Alexander VanTol,Peter Vassilatos,Aarti Venkat,Robert L. Grossman*

Main category: cs.DC

TL;DR: Gen3是用于构建数据共享库的开源数据平台，可自动生成相关门户和API，基于标准软件服务构建，已构建多个数据共享库。


<details>
  <summary>Details</summary>
Motivation: 构建能管理、分析和共享数据的云基数据平台，满足研究社区需求。

Method: 先定义数据模型，Gen3自动生成数据搜索、提交门户及FAIR API，基于少量标准软件服务构建。

Result: 已构建超12个数据共享库，包含超28 PB数据和6400万个FAIR数据对象。

Conclusion: Gen3可构建数据共享库，能与其他数据平台和生态系统互操作。

Abstract: Gen3 is an open-source data platform for building data commons. A data
commons is a cloud-based data platform for managing, analyzing, and sharing
data with a research community. Gen3 has been used to build over a dozen data
commons that in aggregate contain over 28 PB of data and 64 million FAIR data
objects. To set up a Gen3 data commons, you first define a data model. Gen3
then autogenerates 1) a data portal for searching and exploring data in the
commons; 2) a data portal for submitting data to the commons; and 3) FAIR APIs
for accessing the data programmatically. Gen3 is built over a small number of
standards-based software services, which are designed to support current and
future Gen3 components so that Gen3 can interoperate with other data platforms
and data ecosystems.

</details>


### [46] [Theseus: A Distributed and Scalable GPU-Accelerated Query Processing Platform Optimized for Efficient Data Movement](https://arxiv.org/abs/2508.05029)
*Felipe Aramburú,William Malpica,Kaouther Abrougui,Amin Aramoon,Romulo Auccapuclla,Claude Brisson,Matthijs Brobbel,Colby Farrell,Pradeep Garigipati,Joost Hoozemans,Supun Kamburugamuve,Akhil Nair,Alexander Ocsa,Johan Peltenburg,Rubén Quesada López,Deepak Sihag,Ahmet Uyar,Dhruv Vats,Michael Wendt,Jignesh M. Patel,Rodrigo Aramburú*

Main category: cs.DC

TL;DR: 介绍Theseus分布式查询引擎，可平衡加速器系统的数据移动、内存利用和计算，在TPC - H基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 降低大规模数据集在线分析处理成本、提高吞吐量，解决使用加速器带来的数据移动等挑战。

Method: 设计Theseus查询引擎，采用专门异步控制机制，内存子系统有固定大小页锁定主机内存分配机制。

Result: 在云基础设施TPC - H基准测试中，Theseus在成本相当时比Databricks Photon快达4倍，能用少量节点处理100TB规模TPC - H和TPC - DS基准测试查询。

Conclusion: Theseus是一个能有效平衡数据移动、内存利用和计算的企业级分布式加速器原生查询引擎。

Abstract: Online analytical processing of queries on datasets in the many-terabyte
range is only possible with costly distributed computing systems. To decrease
the cost and increase the throughput, systems can leverage accelerators such as
GPUs, which are now ubiquitous in the compute infrastructure. This introduces
many challenges, the majority of which are related to when, where, and how to
best move data around the system. We present Theseus -- a production-ready
enterprise-scale distributed accelerator-native query engine designed to
balance data movement, memory utilization, and computation in an
accelerator-based system context. Specialized asynchronous control mechanisms
are tightly coupled to the hardware resources for the purpose of network
communication, data pre-loading, data spilling across memories and storage, and
GPU compute tasks. The memory subsystem contains a mechanism for fixed-size
page-locked host memory allocations to increase throughput and reduce memory
fragmentation. For the TPC-H benchmarks at scale factors ranging from 1k to 30k
on cloud infrastructure, Theseus outperforms Databricks Photon by up to
$4\times$ at cost parity. Theseus is capable of processing all queries of the
TPC-H and TPC-DS benchmarks at scale factor 100k (100 TB scale) with as few as
2 DGX A100 640GB nodes.

</details>


### [47] [Tesserae: Scalable Placement Policies for Deep Learning Workloads](https://arxiv.org/abs/2508.04953)
*Song Bian,Saurabh Agarwal,Md. Tareq Mahmood,Shivaram Venkataraman*

Main category: cs.DC

TL;DR: 现有深度学习集群调度器的放置策略有性能不佳或可扩展性差的问题，本文设计新放置策略并集成到Tesserae，实验显示其性能优于现有调度器。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习集群调度器的放置策略存在次优性能或可扩展性差的问题，需要改进资源利用率。

Method: 将许多放置约束表述为图匹配问题，设计用于最小化作业迁移开销和作业打包的新放置策略，并集成到Tesserae。

Result: Tesserae与现有调度器相比，平均作业完成时间（JCT）最多提高1.62倍，总完成时间（Makespan）最多提高1.15倍。

Conclusion: 设计的新放置策略集成到Tesserae后，能实现可扩展且有效的GPU集群调度。

Abstract: Training deep learning (DL) models has become a dominant workload in
data-centers and improving resource utilization is a key goal of DL cluster
schedulers. In order to do this, schedulers typically incorporate placement
policies that govern where jobs are placed on the cluster. Existing placement
policies are either designed as ad-hoc heuristics or incorporated as
constraints within a complex optimization problem and thus either suffer from
suboptimal performance or poor scalability. Our key insight is that many
placement constraints can be formulated as graph matching problems and based on
that we design novel placement policies for minimizing job migration overheads
and job packing. We integrate these policies into Tesserae and describe how our
design leads to a scalable and effective GPU cluster scheduler. Our
experimental results show that Tesserae improves average JCT by up to 1.62x and
the Makespan by up to 1.15x compared with the existing schedulers.

</details>


### [48] [Task-Based Programming for Adaptive Mesh Refinement in Compressible Flow Simulations](https://arxiv.org/abs/2508.05020)
*Anjiang Wei,Hang Song,Mert Hidayetoglu,Elliott Slaughter,Sanjiva K. Lele,Alex Aiken*

Main category: cs.DC

TL;DR: 本文用Regent开发基于AMR的可压缩流高阶数值求解器，解决实现挑战，实验显示任务融合和GPU内核生成有显著加速效果，并通过模拟验证。


<details>
  <summary>Details</summary>
Motivation: 可压缩流高阶求解器在科学应用中很重要，AMR可降低计算成本，需在Regent中实现AMR求解器。

Method: 使用Regent语言为Legion编程模型开发基于AMR的数值求解器，解决动态数据结构、网格有效性和任务启动开销等问题。

Result: 任务融合实现18倍加速，简单注释自动生成GPU内核实现9.7倍加速。

Conclusion: 所开发的求解器能有效解决可压缩流问题，任务融合和GPU内核生成加速效果显著。

Abstract: High-order solvers for compressible flows are vital in scientific
applications. Adaptive mesh refinement (AMR) is a key technique for reducing
computational cost by concentrating resolution in regions of interest. In this
work, we develop an AMR-based numerical solver using Regent, a high-level
programming language for the Legion programming model. We address several
challenges associated with implementing AMR in Regent. These include dynamic
data structures for patch refinement/coarsening, mesh validity enforcement, and
reducing task launch overhead via task fusion. Experimental results show that
task fusion achieves 18x speedup, while automated GPU kernel generation via
simple annotations yields 9.7x speedup for the targeted kernel. We demonstrate
our approach through simulations of two canonical compressible flow problems
governed by the Euler equations.

</details>


### [49] [Simulating LLM training workloads for heterogeneous compute and network infrastructure](https://arxiv.org/abs/2508.05370)
*Sumit Kumar,Arjun Temura,Naman Sharma,Ramanjeet Singh,Meet Dadhania,Praveen Tammana,Satananda Burla,Abed Mohammad Kamaluddin,Rinku Shah*

Main category: cs.DC

TL;DR: 现有LLM训练模拟器假设计算和网络基础设施同质，本文提出异质性感知分布式LLM模拟器，展示了设计需求、挑战及组件，初步结果显示异质性对模型计算和通信时间有影响。


<details>
  <summary>Details</summary>
Motivation: 大规模GPU集群需求增长给分布式模型训练带来创新障碍，现有LLM训练模拟器假设基础设施同质，与实际环境中设备异质性不符。

Method: 提出异质性感知分布式LLM模拟器的设计，阐述设计需求和挑战，设计非均匀工作负载分区等组件。

Result: 初始模拟结果表明异质性对模型计算和通信时间有影响。

Conclusion: 异质性感知分布式LLM模拟器能应对实际环境中设备异质性问题，可用于预测训练时间和指定自定义配置。

Abstract: The growing demand for large-scale GPU clusters in distributed model training
presents a significant barrier to innovation, particularly in model
optimization, performance tuning, and system-level enhancements. To address
this challenge, LLM training simulators are employed to estimate training time
and guide design decisions. However, the state-of-the-art LLM training
simulators assume homogeneous compute and network infrastructure. In practice,
device heterogeneity is inevitable due to resource sharing in cloud
environments, frequent shifts in device generations, and inherent intra-chip
interconnect heterogeneity. To address the gap between state-of-the-art and
practical requirements, we propose the design of a heterogeneity-aware
distributed LLM simulator capable of predicting training time while enabling
abstractions to specify custom configurations for device groups and
device-to-parallelism mapping. We present the design requirements and
challenges in building a heterogeneity-aware distributed ML training simulator,
and design components such as non-uniform workload partitioning. Our initial
simulation results demonstrate the impact of heterogeneity on the model
computation and communication time.

</details>


### [50] [Adaptive Parallel Downloader for Large Genomic Datasets](https://arxiv.org/abs/2508.05511)
*Rasman Mubtasim Swargo,Engin Arslan,Md Arifuzzaman*

Main category: cs.DC

TL;DR: 介绍了用于大型生物数据集的并行文件下载器FastBioDL，可动态调整并发流数量，评估显示比现有工具更快，为大规模基因组数据获取提供高效方案。


<details>
  <summary>Details</summary>
Motivation: 现有下载工具采用静态并发设置，无法适应动态网络条件，导致带宽利用率低和下载时间长。

Method: 将下载过程构建为在线优化问题，利用效用函数和梯度下降实时动态调整并发套接字流数量。

Result: 在公共基因组数据集上比现有工具快达4倍，在高速网络实验中比现有工具快达2.1倍。

Conclusion: FastBioDL通过智能优化客户端标准HTTP或FTP下载，为大规模基因组数据获取提供了强大而高效的解决方案，使研究人员无需专业商业软件或协议即可进行高性能数据检索。

Abstract: Modern next-generation sequencing (NGS) projects routinely generate terabytes
of data, which researchers commonly download from public repositories such as
SRA or ENA. Existing download tools often employ static concurrency settings,
leading to inefficient bandwidth utilization and prolonged download times due
to their inability to adapt to dynamic network conditions. We introduce
FastBioDL, a parallel file downloader designed for large biological datasets,
featuring an adaptive concurrency controller. FastBioDL frames the download
process as an online optimization problem, utilizing a utility function and
gradient descent to adjust the number of concurrent socket streams in real-time
dynamically. This approach maximizes download throughput while minimizing
resource overhead. Comprehensive evaluations on public genomic datasets
demonstrate that FastBioDL achieves up to $4x$ speedup over state-of-the-art
tools. Moreover, in high-speed network experiments, its adaptive design was up
to $2.1x$ faster than existing tools. By intelligently optimizing standard HTTP
or FTP downloads on the client side, FastBioDL provides a robust and efficient
solution for large-scale genomic data acquisition, democratizing
high-performance data retrieval for researchers without requiring specialized
commercial software or protocols.

</details>


### [51] [Modular Architecture for High-Performance and Low Overhead Data Transfers](https://arxiv.org/abs/2508.05546)
*Rasman Mubtasim Swargo,Engin Arslan,Md Arifuzzaman*

Main category: cs.DC

TL;DR: 提出AutoMDT架构，利用深度强化学习优化数据传输，评估显示比现有方案性能更优。


<details>
  <summary>Details</summary>
Motivation: 传统文件传输工具存在资源利用不足和不稳定问题，高性能应用需要快速可靠传输大量数据。

Method: 提出AutoMDT模块化数据传输架构，用基于深度强化学习的代理同时优化读写和网络操作的并发级别，结合轻量级网络系统模拟器进行离线训练PPO代理。

Result: 在生产级测试平台上，AutoMDT收敛速度快达8倍，传输完成时间减少68%。

Conclusion: AutoMDT架构能有效解决传统传输工具问题，适应系统和网络变化，提升数据传输性能。

Abstract: High-performance applications necessitate rapid and dependable transfer of
massive datasets across geographically dispersed locations. Traditional file
transfer tools often suffer from resource underutilization and instability
because of fixed configurations or monolithic optimization methods. We propose
AutoMDT, a novel modular data transfer architecture that employs a deep
reinforcement learning based agent to simultaneously optimize concurrency
levels for read, network, and write operations. Our solution incorporates a
lightweight network-system simulator, enabling offline training of a Proximal
Policy Optimization (PPO) agent in approximately 45 minutes on average, thereby
overcoming the impracticality of lengthy online training in production
networks. AutoMDT's modular design decouples I/O and network tasks, allowing
the agent to capture complex buffer dynamics precisely and to adapt quickly to
changing system and network conditions. Evaluations on production-grade
testbeds show that AutoMDT achieves up to 8x faster convergence and a 68%
reduction in transfer completion times compared with state-of-the-art
solutions.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [52] [Subset Sum in Near-Linear Pseudopolynomial Time and Polynomial Space](https://arxiv.org/abs/2508.04726)
*Thejas Radhika Sajith*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Given a multiset $A = \{a_1, \dots, a_n\}$ of positive integers and a target
integer $t$, the Subset Sum problem asks if there is a subset of $A$ that sums
to $t$. Bellman's [1957] classical dynamic programming algorithm runs in
$O(nt)$ time and $O(t)$ space. Since then, there have been multiple
improvements in both time and space complexity.
  Notably, Bringmann [SODA 2017] uses a two-step color-coding technique to
obtain a randomized algorithm that runs in $\tilde{O}(n+t)$ time and
$\tilde{O}(t)$ space. On the other hand, there are polynomial space algorithms
-- for example, Jin, Vyas and Williams [SODA 2021] build upon the algorithm
given by Bringmann, using a clever algebraic trick first seen in Kane's
Logspace algorithm, to obtain an $\tilde{O}(nt)$ time and $\tilde{O}(\log(nt))$
space algorithm. A natural question, asked by Jin et al. is if there is an
$\tilde{O}(n+t)$ time algorithm running in poly$(n, \log t)$ space. Another
natural question is whether it is possible to construct a deterministic
polynomial space algorithm with time complexity comparable to that of
Bellman's.
  In this paper, we answer both questions affirmatively. We build on the
framework given by Jin et al., using a multipoint evaluation-based approach to
speed up a bottleneck step in their algorithm. We construct a deterministic
algorithm that runs in $\tilde{O}(nt)$ time and $\tilde{O}(n \log^2 t)$ space
and a randomized algorithm that runs in $\tilde{O}(n+t)$ time and
$\tilde{O}(n^2 + n \log^2 t)$ space.

</details>


### [53] [A Refutation of Elmasry's $\tilde{O}(m \sqrt{n})$-Time Algorithm for Single-Source Shortest Paths](https://arxiv.org/abs/2508.04872)
*Sunny Atalig,Marek Chrobak*

Main category: cs.DS

TL;DR: 指出Amr Elmasry关于单源最短路径算法复杂度分析有误。


<details>
  <summary>Details</summary>
Motivation: 验证Amr Elmasry提出的单源最短路径算法复杂度分析的正确性。

Method: 给出一个加权图示例。

Result: 该算法在此加权图上运行时间为Ω(mn)。

Conclusion: Amr Elmasry的复杂度分析不正确。

Abstract: In this note we examine the recent paper "Breaking the Bellman-Ford
Shortest-Path Bound" by Amr Elmasry, where he presents an algorithm for the
single-source shortest path problem and claims that its running time complexity
is $\tilde{O}(m\sqrt{n})$, where $n$ is the number of vertices and $m$ is the
number of edges. We show that his analysis is incorrect, by providing an
example of a weighted graph on which the running time of his algorithm is
$\Omega(mn)$.

</details>


### [54] [Text Indexing and Pattern Matching with Ephemeral Edits](https://arxiv.org/abs/2508.05124)
*Solon P. Pissis*

Main category: cs.DS

TL;DR: 本文引入文本索引和模式匹配中的临时子串编辑概念，给出相应数据结构和处理方法及时间复杂度。


<details>
  <summary>Details</summary>
Motivation: 处理独立编辑流或测试假设编辑时，需要设计支持临时子串编辑的文本索引和模式匹配方法。

Method: 对给定文本进行预处理，对在线给定的模式也进行预处理，允许文本中存在临时编辑操作序列。

Result: 文本索引中，能在规定时间和空间内处理文本和模式，并在临时操作撤销前以一定时间复杂度报告模式出现位置；模式匹配中，能以最优时间复杂度报告模式出现位置，还给出临时块删除的最优解。

Conclusion: 成功引入新的文本索引和模式匹配概念，并给出有效的处理方法和时间复杂度。

Abstract: A sequence $e_0,e_1,\ldots$ of edit operations in a string $T$ is called
ephemeral if operation $e_i$ constructing string $T^i$, for all $i=2k$ with
$k\in\mathbb{N}$, is reverted by operation $e_{i+1}$ that reconstructs $T$.
Such a sequence arises when processing a stream of independent edits or testing
hypothetical edits.
  We introduce text indexing with ephemeral substring edits, a new version of
text indexing. Our goal is to design a data structure over a given text that
supports subsequent pattern matching queries with ephemeral substring
insertions, deletions, or substitutions in the text; we require insertions and
substitutions to be of constant length. In particular, we preprocess a text
$T=T[0\mathinner{.\,.} n)$ over an integer alphabet $\Sigma=[0,\sigma)$ with
$\sigma=n^{\mathcal{O}(1)}$ in $\mathcal{O}(n)$ time. Then, we can preprocess
any arbitrary pattern $P=P[0\mathinner{.\,.} m)$ given online in
$\mathcal{O}(m\log\log m)$ time and $\mathcal{O}(m)$ space and allow any
ephemeral sequence of edit operations in $T$. Before reverting the $i$th
operation, we report all Occ occurrences of $P$ in $T^i$ in
$\mathcal{O}(\log\log n + \text{Occ})$ time.
  We also introduce pattern matching with ephemeral edits. In particular, we
preprocess two strings $T$ and $P$, each of length at most $n$, over an integer
alphabet $\Sigma=[0,\sigma)$ with $\sigma=n^{\mathcal{O}(1)}$ in
$\mathcal{O}(n)$ time. Then, we allow any ephemeral sequence of edit operations
in $T$. Before reverting the $i$th operation, we report all Occ occurrences of
$P$ in $T^i$ in the optimal $\mathcal{O}(\text{Occ})$ time. Along our way to
this result, we also give an optimal solution for pattern matching with
ephemeral block deletions.

</details>


### [55] [Space-Efficient Hierholzer: Eulerian Cycles in O(m) Time and O(n) Space](https://arxiv.org/abs/2508.05251)
*Ziad Ismaili Alaoui,Detlef Plump,Sebastian Wild*

Main category: cs.DS

TL;DR: 提出Hierholzer算法变体，用O(n lg m)位工作内存找欧拉回路，节省空间且易实现。


<details>
  <summary>Details</summary>
Motivation: 改进标准Hierholzer算法的工作空间，尤其是针对稠密图或多边图。

Method: 提出Hierholzer算法的简单变体，避免使用O(m)大小的顶点栈或存储每条边的信息。

Result: 该算法运行时间为线性，使用O(n lg m)位工作内存，是首个达到此空间界限的线性时间算法。

Conclusion: 该算法节省空间，尤其适用于稠密图或多边图，且易于实现。

Abstract: We describe a simple variant of Hierholzer's algorithm that finds an Eulerian
cycle in a (multi)graph with $n$ vertices and $m$ edges using $\mathrm{O}(n \lg
m)$ bits of working memory. This substantially improves the working space
compared to standard implementations of Hierholzer's algorithm, which use
$\mathrm{O}(m \lg n)$ bits of space. Our algorithm runs in linear time, like
the classical versions, but avoids an $\mathrm{O}(m)$-size stack of vertices or
storing information for each edge. To our knowledge, this is the first
linear-time algorithm to achieve this space bound, and the method is very easy
to implement. The correctness argument, by contrast, is surprisingly subtle; we
give a detailed formal proof. The space savings are particularly relevant for
dense graphs or multigraphs with large edge multiplicities.

</details>


### [56] [Parameterized Algorithms for Spanning Tree Isomorphism by Redundant Set Size](https://arxiv.org/abs/2508.05351)
*Fangjian Shen,Yicheng Zheng,Wushao Wen,Hankz Hankui Zhuo*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In this paper, we present fixed-parameter tractability algorithms for both
the undirected and directed versions of the Spanning Tree Isomorphism Problem,
parameterized by the size $k$ of a redundant set. A redundant set is a
collection of edges whose removal transforms the graph into a spanning tree.
For the undirected version, our algorithm achieves a time complexity of $O(n^2
\log n \cdot 2^{k \log k})$. For the directed version, we propose a more
efficient algorithm with a time complexity of $O(n^2 \cdot 2^{4k-3})$, where
$n$ is the number of vertices.

</details>


### [57] [Online Sparsification of Bipartite-Like Clusters in Graphs](https://arxiv.org/abs/2508.05437)
*Joyentanuj Das,Suranjan De,He Sun*

Main category: cs.DS

TL;DR: 本文研究二分图类聚类，提出高效在线稀疏化算法，实验表明算法加速现有聚类算法且保持有效性。


<details>
  <summary>Details</summary>
Motivation: 多数图聚类算法关注低传导性顶点集，而近期研究强调分析真实数据集时顶点集间互连的重要性，因此研究二分图类聚类。

Method: 提出高效且在线的稀疏化算法，在无向图和有向图中寻找二分图类聚类。

Result: 在合成和真实数据集上实验，算法显著加速现有聚类算法运行时间并保持有效性。

Conclusion: 所提出的算法能有效提高现有图聚类算法的效率。

Abstract: Graph clustering is an important algorithmic technique for analysing massive
graphs, and has been widely applied in many research fields of data science.
While the objective of most graph clustering algorithms is to find a vertex set
of low conductance, a sequence of recent studies highlights the importance of
the inter-connection between vertex sets when analysing real-world datasets.
Following this line of research, in this work we study bipartite-like clusters
and present efficient and online sparsification algorithms that find such
clusters in both undirected graphs and directed ones. We conduct experimental
studies on both synthetic and real-world datasets, and show that our algorithms
significantly speedup the running time of existing clustering algorithms while
preserving their effectiveness.

</details>


### [58] [Parameterized complexity of isometric path partition: treewidth and diameter](https://arxiv.org/abs/2508.05448)
*Dibyayan Chakraborty,Oscar Defrain,Florent Foucaud,Mathieu Mari,Prafullkumar Tale*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We investigate the parameterized complexity of the Isometric Path Partition
problem when parameterized by the treewidth ($\mathrm{tw}$) of the input graph,
arguably one of the most widely studied parameters. Courcelle's theorem shows
that graph problems that are expressible as MSO formulas of constant size admit
FPT algorithms parameterized by the treewidth of the input graph. This
encompasses many natural graph problems. However, many metric-based graph
problems, where the solution is defined using some metric-based property of the
graph (often the distance) are not expressible as MSO formulas of constant
size. These types of problems, Isometric Path Partition being one of them,
require individual attention and often draw the boundary for the success story
of parameterization by treewidth.
  In this paper, we prove that Isometric Path Partition is $W[1]$-hard when
parameterized by treewidth (in fact, even pathwidth), answering the question by
Dumas et al. [SIDMA, 2024], Fernau et al. [CIAC, 2023], and confirming the
aforementioned tendency. We complement this hardness result by designing a
tailored dynamic programming algorithm running in $n^{O(\mathrm{tw})}$ time.
This dynamic programming approach also results in an algorithm running in time
$\textrm{diam}^{O(\mathrm{tw}^2)} \cdot n^{O(1)}$, where $\textrm{diam}$ is the
diameter of the graph. Note that the dependency on treewidth is unusually high,
as most problems admit algorithms running in time $2^{O(\mathrm{tw})}\cdot
n^{O(1)}$ or $2^{O(\mathrm{tw} \log (\mathrm{tw}))}\cdot n^{O(1)}$. However, we
rule out the possibility of a significantly faster algorithm by proving that
Isometric Path Partition does not admit an algorithm running in time
$\textrm{diam}^{o(\mathrm{tw}^2/(\log^3(\mathrm{tw})))} \cdot n^{O(1)}$, unless
the Randomized-ETH fails.

</details>


### [59] [An Improved Approximation Algorithm for the Capacitated Arc Routing Problem](https://arxiv.org/abs/2508.05471)
*Jingyang Zhao,Mingyu Xiao*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The Capacitated Arc Routing Problem (CARP), introduced by Golden and Wong in
1981, is an important arc routing problem in Operations Research, which
generalizes the famous Capacitated Vehicle Routing Problem (CVRP). When every
customer has a unit demand, the best known approximation ratio for CARP, given
by Jansen in 1993, remains $\frac{5}{2}-\frac{1.5}{k}$, where $k$ denotes the
vehicle capacity. Based on recent progress in approximating CVRP, we improve
this result by proposing a
$(\frac{5}{2}-\Theta(\frac{1}{\sqrt{k}}))$-approximation algorithm, which to
the best of our knowledge constitutes the first improvement over Jansen's
bound.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [60] [Online EFX Allocations with Predictions](https://arxiv.org/abs/2508.04779)
*Themistoklis Melissourgos,Nicos Protopapas*

Main category: cs.GT

TL;DR: 研究在线公平分配问题，在有预测情况下探讨近似EFX分配的可行性，给出不可能性结果并提出适用于两个相同估值代理的近似算法。


<details>
  <summary>Details</summary>
Motivation: 解决在线公平分配中近似EFX分配难以实现的问题，借鉴带预测的算法趋势。

Method: 先证明忽略或仅依赖预测的算法在近似EFX分配上的不可能性，对使用预测和真实值的算法给出预测准确性的下界，提出针对两个相同估值代理的算法。

Result: 得出忽略或仅依赖预测的算法无法实现近似EFX分配，使用预测和真实值的算法有预测准确性下界，提出的算法能近似EFX且预测越准效果越好。

Conclusion: 在在线公平分配中近似EFX分配实现困难，有预测时虽有局限但对特定情况有可行算法。

Abstract: We study an online fair division problem where a fixed number of goods arrive
sequentially and must be allocated to a given set of agents. Once a good
arrives, its true value for each agent is revealed, and it has to be
immediately and irrevocably allocated to some agent. The ultimate goal is to
ensure envy-freeness up to any good (EFX) after all goods have been allocated.
Unfortunately, as we show, approximate EFX allocations are unattainable in
general, even under restrictive assumptions on the valuation functions.
  To address this, we follow a recent and fruitful trend of augmenting
algorithms with predictions. Specifically, we assume access to a prediction
vector estimating the agents' true valuations -- e.g., generated by a machine
learning model trained on past data. Predictions may be unreliable, and we
measure their error using the total variation distance from the true
valuations, that is, the percentage of predicted value-mass that disagrees with
the true values.
  Focusing on the natural class of additive valuations, we prove impossibility
results even on approximate EFX allocations for algorithms that either ignore
predictions or rely solely on them. We then turn to algorithms that use both
the predictions and the true values and show strong lower bounds on the
prediction accuracy that is required by any algorithm to compute an approximate
EFX. These negative results persist even under identical valuations, contrary
to the offline setting where exact EFX allocations always exist without the
necessity of predictions. We then present an algorithm for two agents with
identical valuations that uses effectively the predictions and the true values.
The algorithm approximates EFX, with its guarantees improving as the accuracy
of the predictions increases.

</details>


### [61] [Toward Energy and Location-Aware Resource Allocation in Next Generation Networks](https://arxiv.org/abs/2508.05109)
*Mandar Datar,Mattia Merluzzi*

Main category: cs.GT

TL;DR: 本文研究能源约束下通信和计算资源分配问题，通过将网络建模为Fisher市场提出低复杂度解决方案。


<details>
  <summary>Details</summary>
Motivation: 无线网络向包含计算的复杂系统演变，优化从性能导向转向价值导向，需平衡多方面以最大化效用，解决能源约束下资源分配问题。

Method: 将网络建模为Fisher市场，多个服务提供商通过虚拟货币预算竞争资源束。

Result: 数学证明市场均衡，数值结果显示不同位置在不同服务下效用和能源的多维权衡。

Conclusion: 提出的低复杂度解决方案能实现高效用，保证能源约束，促进服务提供商间公平性。

Abstract: Wireless networks are evolving from radio resource providers to complex
systems that also involve computing, with the latter being distributed across
edge and cloud facilities. Also, their optimization is shifting more and more
from a performance to a value-oriented paradigm. The two aspects shall be
balanced continuously, to maximize the utilities of Services Providers (SPs),
users quality of experience and fairness, while meeting global constraints in
terms of energy consumption and carbon footprint among others, with all these
heterogeneous resources contributing. In this paper, we tackle the problem of
communication and compute resource allocation under energy constraints, with
multiple SPs competing to get their preferred resource bundle by spending a a
fictitious currency budget. By modeling the network as a Fisher market, we
propose a low complexity solution able to achieve high utilities and guarantee
energy constraints, while also promoting fairness among SPs, as compared to a
social optimal solution. The market equilibrium is proved mathematically, and
numerical results show the multi-dimensional trade-off between utility and
energy at different locations, with communication and computation-intensive
services.

</details>


### [62] [A New Three-Players Auction Bridge with Dynamic Opponents and Team Members](https://arxiv.org/abs/2508.05582)
*Sourish Sarkar,Aritrabha Majumdar,Moutushi Chatterjee*

Main category: cs.GT

TL;DR: 提出三人版桥牌游戏，动态组队、新计分系统，探讨策略和概率问题，模拟显示策略有效，适合比赛。


<details>
  <summary>Details</summary>
Motivation: 结束固定搭档关系，使游戏更具动态性和灵活性。

Method: 实时动态重新定义团队组成，引入新计分系统，探讨战略和概率问题并采用算法方法。

Result: 模拟结果显示不同策略具有效率。

Conclusion: 该游戏架构适合比赛，可能扩大锦标赛纸牌游戏的参与人群。

Abstract: This article presents a new three-player version of the bridge playing card
game for the purpose of ending fixed partnerships so that the play can be more
dynamic and flexible. By dynamically redefining team makeup in real time, this
game design increases unpredictability and forces players to repeatedly update
strategy. A novel scoring system is introduced to reduce biases present in
conventional rule-based games by favoring fairness via reward systems that
enforce tactical decision making and risk assessment. Being subject to regular
bridge rules, this version tests players to collaborate without fixed
friendships, requiring fluid adjustment and adaptive bidding behavior in real
time. Strategic issues involve aggressive and defensive bidding, adaptable
playing styles, and loss-seeking strategies specific to the three-player
structure. The article discusses probabilistic issues of bidding, trump and
no-trump declarative effects, and algorithmic methods to trick-taking.
Simulation outcomes illustrate the efficiency of diverse strategies. The game's
architecture is ideal for competitions and possibly influential in broadening
entry pools for tournament card games.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [63] [Augmented Question-guided Retrieval (AQgR) of Indian Case Law with LLM, RAG, and Structured Summaries](https://arxiv.org/abs/2508.04710)
*Vishnuprabha V,Daleesha M Viswanathan,Rajesh R,Aneesh V Pillai*

Main category: cs.IR

TL;DR: 本文提出用大语言模型结合RAG与结构化摘要优化印度判例法检索，通过AQgR框架生成针对性法律问题，实验取得较好结果，推动判例法检索发展。


<details>
  <summary>Details</summary>
Motivation: 多数检索方法重事实相似性而非法律问题，现有系统缺乏案例相关性解释，需改进判例法检索。

Method: 结合Retrieval Augmented Generation (RAG)与优化的结构化摘要，利用Augmented Question - guided Retrieval (AQgR)框架生成针对性法律问题。

Result: 在FIRE 2019数据集子集实验中，平均精度均值（MAP）达0.36，平均召回均值（MAR）达0.67，远超当前MAP基准0.1573。

Conclusion: 从基于事实检索转向基于法律问题检索，结合AQgR框架使检索结果更相关、精确，满足法律专业人士需求。

Abstract: Identifying relevant legal precedents remains challenging, as most retrieval
methods emphasize factual similarity over legal issues, and current systems
often lack explanations clarifying case relevance. This paper proposes the use
of Large Language Models (LLMs) to address this gap by facilitating the
retrieval of relevant cases, generating explanations to elucidate relevance,
and identifying core legal issues all autonomously, without requiring legal
expertise. Our approach combines Retrieval Augmented Generation (RAG) with
structured summaries optimized for Indian case law. Leveraging the Augmented
Question-guided Retrieval (AQgR) framework, the system generates targeted legal
questions based on factual scenarios to identify relevant case law more
effectively. The structured summaries were assessed manually by legal experts,
given the absence of a suitable structured summary dataset. Case law retrieval
was evaluated using the FIRE dataset, and explanations were reviewed by legal
experts, as explanation generation alongside case retrieval is an emerging
innovation. Experimental evaluation on a subset of the FIRE 2019 dataset
yielded promising outcomes, achieving a Mean Average Precision (MAP) score of
0.36 and a Mean Average Recall (MAR) of 0.67 across test queries, significantly
surpassing the current MAP benchmark of 0.1573. This work introduces a suite of
novel contributions to advance case law retrieval. By transitioning from
fact-based to legal-issue-based retrieval, the proposed approach delivers more
contextually relevant results that align closely with legal professionals'
needs. Integrating legal questions within the retrieval process through the
AQgR framework ensures more precise and meaningful retrieval by refining the
context of queries.

</details>


### [64] [Scaling Generative Recommendations with Context Parallelism on Hierarchical Sequential Transducers](https://arxiv.org/abs/2508.04711)
*Yue Dong,Han Li,Shen Li,Nikhil Patel,Xing Liu,Xiaodong Wang,Chuanhao Zhuge*

Main category: cs.IR

TL;DR: 提出支持锯齿张量的上下文并行性用于HSTU注意力，使支持的用户交互序列长度增加5.3倍，结合DDP有1.55倍扩展因子。


<details>
  <summary>Details</summary>
Motivation: 大规模推荐系统需有效建模特征保证准确预测，扩展序列长度计算重，且生产排序模型的锯齿输入张量给上下文并行性实现带来挑战。

Method: 引入支持锯齿张量的上下文并行性用于HSTU注意力。

Result: 使支持的用户交互序列长度增加5.3倍，结合分布式数据并行性（DDP）时实现1.55倍扩展因子。

Conclusion: 该方法为扩展序列维度建立了基础能力。

Abstract: Large-scale recommendation systems are pivotal to process an immense volume
of daily user interactions, requiring the effective modeling of high
cardinality and heterogeneous features to ensure accurate predictions. In prior
work, we introduced Hierarchical Sequential Transducers (HSTU), an
attention-based architecture for modeling high cardinality, non-stationary
streaming recommendation data, providing good scaling law in the generative
recommender framework (GR). Recent studies and experiments demonstrate that
attending to longer user history sequences yields significant metric
improvements. However, scaling sequence length is activation-heavy,
necessitating parallelism solutions to effectively shard activation memory. In
transformer-based LLMs, context parallelism (CP) is a commonly used technique
that distributes computation along the sequence-length dimension across
multiple GPUs, effectively reducing memory usage from attention activations. In
contrast, production ranking models typically utilize jagged input tensors to
represent user interaction features, introducing unique CP implementation
challenges. In this work, we introduce context parallelism with jagged tensor
support for HSTU attention, establishing foundational capabilities for scaling
up sequence dimensions. Our approach enables a 5.3x increase in supported user
interaction sequence length, while achieving a 1.55x scaling factor when
combined with Distributed Data Parallelism (DDP).

</details>


### [65] [A Metric for MLLM Alignment in Large-scale Recommendation](https://arxiv.org/abs/2508.04963)
*Yubin Zhang,Yanhua Huang,Haiming Xu,Mingliang Qi,Chang Wang,Jiarui Jin,Xiangyuan Ren,Xiaodan Wang,Ruiwen Xu*

Main category: cs.IR

TL;DR: 提出用于多模态推荐的Leakage Impact Score (LIS) 指标，经小红书测试有效。


<details>
  <summary>Details</summary>
Motivation: 现有评估多模态大语言模型与推荐系统对齐的方法存在静态基准不准确、在线系统评估成本高、传统指标无有效见解等问题。

Method: 提出LIS指标，该指标不直接评估MLLMs，而是高效衡量偏好数据上限，并分享在现实场景中使用LIS部署MLLMs的实用见解。

Result: 在小红书的内容流和展示广告的在线A/B测试中，用户使用时长和广告商价值显著提升。

Conclusion: 提出的LIS指标及相关方法在多模态推荐中有效。

Abstract: Multimodal recommendation has emerged as a critical technique in modern
recommender systems, leveraging content representations from advanced
multimodal large language models (MLLMs). To ensure these representations are
well-adapted, alignment with the recommender system is essential. However,
evaluating the alignment of MLLMs for recommendation presents significant
challenges due to three key issues: (1) static benchmarks are inaccurate
because of the dynamism in real-world applications, (2) evaluations with online
system, while accurate, are prohibitively expensive at scale, and (3)
conventional metrics fail to provide actionable insights when learned
representations underperform. To address these challenges, we propose the
Leakage Impact Score (LIS), a novel metric for multimodal recommendation.
Rather than directly assessing MLLMs, LIS efficiently measures the upper bound
of preference data. We also share practical insights on deploying MLLMs with
LIS in real-world scenarios. Online A/B tests on both Content Feed and Display
Ads of Xiaohongshu's Explore Feed production demonstrate the effectiveness of
our proposed method, showing significant improvements in user spent time and
advertiser value.

</details>


### [66] [Align-for-Fusion: Harmonizing Triple Preferences via Dual-oriented Diffusion for Cross-domain Sequential Recommendation](https://arxiv.org/abs/2508.05074)
*Yongfu Zha,Xinxin Dong,Haokai Ma,Yonghui Yang,Xiaodong Wang*

Main category: cs.IR

TL;DR: 提出用于跨域顺序推荐的Align - for - fusion框架HorizonRec，能实现细粒度三域偏好融合，实验证明其有效性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有跨域顺序推荐方法采用对齐再融合范式，忽略特定领域偏好的细粒度融合，且基于扩散模型的推荐器存在不稳定问题。

Method: 提出Align - for - fusion框架HorizonRec，引入混合条件分布检索策略，提出双导向偏好扩散方法。

Result: 在两个不同平台的四个跨域顺序推荐数据集上的广泛实验表明，HorizonRec在细粒度三域偏好融合方面有效且鲁棒。

Conclusion: HorizonRec能有效解决现有跨域顺序推荐方法的不足，实现更好的多域偏好建模和融合。

Abstract: Personalized sequential recommendation aims to predict appropriate items for
users based on their behavioral sequences. To alleviate data sparsity and
interest drift issues, conventional approaches typically incorporate auxiliary
behaviors from other domains via cross-domain transition. However, existing
cross-domain sequential recommendation (CDSR) methods often follow an
align-then-fusion paradigm that performs representation-level alignment across
multiple domains and combines them mechanically for recommendation, overlooking
the fine-grained fusion of domain-specific preferences. Inspired by recent
advances in diffusion models (DMs) for distribution matching, we propose an
align-for-fusion framework for CDSR to harmonize triple preferences via
dual-oriented DMs, termed HorizonRec. Specifically, we investigate the
uncertainty injection of DMs and identify stochastic noise as a key source of
instability in existing DM-based recommenders. To address this, we introduce a
mixed-conditioned distribution retrieval strategy that leverages distributions
retrieved from users' authentic behavioral logic as semantic bridges across
domains, enabling consistent multi-domain preference modeling. Furthermore, we
propose a dual-oriented preference diffusion method to suppress potential noise
and emphasize target-relevant interests during multi-domain user representation
fusion. Extensive experiments on four CDSR datasets from two distinct platforms
demonstrate the effectiveness and robustness of HorizonRec in fine-grained
triple-domain preference fusion.

</details>


### [67] [An End-to-End Multi-objective Ensemble Ranking Framework for Video Recommendation](https://arxiv.org/abs/2508.05093)
*Tiantian He,Minzhi Xie,Runtong Li,Xiaoxiao Xu,Jiaqi Yu,Zixiu Wang,Lantao Hu,Han Li,Kun Gai*

Main category: cs.IR

TL;DR: 提出端到端多目标集成排序框架EMER用于短视频推荐，经测试有效并在快手部署取得显著效果。


<details>
  <summary>Details</summary>
Motivation: 改进短视频推荐系统中多目标集成排序模块，解决有效监督定义和离线模型优化效率问题。

Method: 采用端到端建模范式，引入精心设计的损失函数、样本组织方法和基于transformer的网络架构，提出离线 - 在线一致评估系统。

Result: 在真实工业数据集上测试有效，在快手主要场景部署使整体App停留时间增加1.39%，7天用户生命周期增加0.196%。

Conclusion: 所提EMER框架有效，能提升短视频推荐效果。

Abstract: We propose a novel End-to-end Multi-objective Ensemble Ranking framework
(EMER) for the multi-objective ensemble ranking module, which is the most
critical component of the short video recommendation system. EMER enhances
personalization by replacing manually-designed heuristic formulas with an
end-to-end modeling paradigm. EMER introduces a meticulously designed loss
function to address the fundamental challenge of defining effective supervision
for ensemble ranking, where no single ground-truth signal can fully capture
user satisfaction. Moreover, EMER introduces novel sample organization method
and transformer-based network architecture to capture the comparative
relationships among candidates, which are critical for effective ranking.
Additionally, we have proposed an offline-online consistent evaluation system
to enhance the efficiency of offline model optimization, which is an
established yet persistent challenge within the multi-objective ranking domain
in industry. Abundant empirical tests are conducted on a real industrial
dataset, and the results well demonstrate the effectiveness of our proposed
framework. In addition, our framework has been deployed in the primary
scenarios of Kuaishou, a short video recommendation platform with hundreds of
millions of daily active users, achieving a 1.39% increase in overall App Stay
Time and a 0.196% increase in 7-day user Lifetime(LT7), which are substantial
improvements.

</details>


### [68] [Navigating Through Paper Flood: Advancing LLM-based Paper Evaluation through Domain-Aware Retrieval and Latent Reasoning](https://arxiv.org/abs/2508.05129)
*Wuqiang Zheng,Yiyan Xu,Xinyu Lin,Chongming Gao,Wenjie Wang,Fuli Feng*

Main category: cs.IR

TL;DR: 提出基于大语言模型的自动化论文评估框架PaperEval，通过域感知检索模块和潜在推理机制解决现有方法局限，实验表明其性能优于现有方法，部署在推荐系统中效果良好。


<details>
  <summary>Details</summary>
Motivation: 现有利用大语言模型进行自动化论文评估的方法受限于过时领域知识和有限推理能力，需要更好的评估方法。

Method: 提出PaperEval框架，包含域感知论文检索模块和潜在推理机制，引入渐进式排名优化策略引导推理过程。

Result: 在两个数据集上，PaperEval在学术影响力和论文质量评估方面始终优于现有方法；部署在推荐系统中吸引大量关注。

Conclusion: PaperEval有效解决了现有方法的局限，在论文评估和实际应用中表现良好，具有实用性和有效性。

Abstract: With the rapid and continuous increase in academic publications, identifying
high-quality research has become an increasingly pressing challenge. While
recent methods leveraging Large Language Models (LLMs) for automated paper
evaluation have shown great promise, they are often constrained by outdated
domain knowledge and limited reasoning capabilities. In this work, we present
PaperEval, a novel LLM-based framework for automated paper evaluation that
addresses these limitations through two key components: 1) a domain-aware paper
retrieval module that retrieves relevant concurrent work to support
contextualized assessments of novelty and contributions, and 2) a latent
reasoning mechanism that enables deep understanding of complex motivations and
methodologies, along with comprehensive comparison against concurrently related
work, to support more accurate and reliable evaluation. To guide the reasoning
process, we introduce a progressive ranking optimization strategy that
encourages the LLM to iteratively refine its predictions with an emphasis on
relative comparison. Experiments on two datasets demonstrate that PaperEval
consistently outperforms existing methods in both academic impact and paper
quality evaluation. In addition, we deploy PaperEval in a real-world paper
recommendation system for filtering high-quality papers, which has gained
strong engagement on social media -- amassing over 8,000 subscribers and
attracting over 10,000 views for many filtered high-quality papers --
demonstrating the practical effectiveness of PaperEval.

</details>


### [69] [Tool Graph Retriever: Exploring Dependency Graph-based Tool Retrieval for Large Language Models](https://arxiv.org/abs/2508.05152)
*Linfeng Gao,Yaoxiang Wang,Minlong Peng,Jialong Tang,Yuzhe Shang,Mingming Sun,Jinsong Su*

Main category: cs.IR

TL;DR: 随着AI智能体工具数量增加，现有工具检索方法忽略工具依赖。本文提出Tool Graph Retriever (TGR)，通过构建数据集、利用图卷积学习工具表示进行检索，实验表明其能提升性能并达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有主流工具检索方法主要基于语义相似性，且独立考虑每个工具，忽略工具间依赖，可能导致任务执行时遗漏前置工具，因此需要新方法解决该问题。

Method: 提出TGR方法，构建TDI300K数据集训练判别器识别工具依赖，将候选工具表示为工具依赖图，用图卷积将依赖信息整合到工具表示中，最后用于在线检索。

Result: 在多个常用数据集上的实验表明，TGR能提升现有主流方法的性能，达到SOTA。

Conclusion: 工具依赖很重要，TGR方法有效。

Abstract: With the remarkable advancement of AI agents, the number of their equipped
tools is increasing rapidly. However, integrating all tool information into the
limited model context becomes impractical, highlighting the need for efficient
tool retrieval methods. In this regard, dominant methods primarily rely on
semantic similarities between tool descriptions and user queries to retrieve
relevant tools. However, they often consider each tool independently,
overlooking dependencies between tools, which may lead to the omission of
prerequisite tools for successful task execution. To deal with this defect, in
this paper, we propose Tool Graph Retriever (TGR), which exploits the
dependencies among tools to learn better tool representations for retrieval.
First, we construct a dataset termed TDI300K to train a discriminator for
identifying tool dependencies. Then, we represent all candidate tools as a tool
dependency graph and use graph convolution to integrate the dependencies into
their representations. Finally, these updated tool representations are employed
for online retrieval. Experimental results on several commonly used datasets
show that our TGR can bring a performance improvement to existing dominant
methods, achieving SOTA performance. Moreover, in-depth analyses also verify
the importance of tool dependencies and the effectiveness of our TGR.

</details>


### [70] [Balancing Accuracy and Novelty with Sub-Item Popularity](https://arxiv.org/abs/2508.05198)
*Chiara Mallamaci,Aleksandr Vladimirovich Petrov,Alberto Carlo Maria Mancino,Vito Walter Anelli,Tommaso Di Noia,Craig Macdonald*

Main category: cs.IR

TL;DR: 针对音乐推荐中PPS方法限制发现新内容的问题，利用RecJPQ框架提出子ID级PPS方法，在不降低准确性的同时提高个性化新颖性。


<details>
  <summary>Details</summary>
Motivation: 现有PPS方法在音乐推荐中虽增强相关性，但会强化已知内容，限制发现新颖或意外项目，不利于长期用户参与和满意度。

Method: 基于RecJPQ框架，重新利用其亚项架构以更细粒度建模个性化流行度，提出在RecJPQ框架内集成子ID级个性化流行度的方法。

Result: 子ID级PPS方法（sPPS）始终优于项级PPS，在不影响推荐准确性的情况下显著提高个性化新颖性。

Conclusion: 子ID级PPS方法能有效解决现有PPS方法的局限性，平衡推荐准确性和个性化新颖性。

Abstract: In the realm of music recommendation, sequential recommenders have shown
promise in capturing the dynamic nature of music consumption. A key
characteristic of this domain is repetitive listening, where users frequently
replay familiar tracks. To capture these repetition patterns, recent research
has introduced Personalised Popularity Scores (PPS), which quantify
user-specific preferences based on historical frequency. While PPS enhances
relevance in recommendation, it often reinforces already-known content,
limiting the system's ability to surface novel or serendipitous items - key
elements for fostering long-term user engagement and satisfaction. To address
this limitation, we build upon RecJPQ, a Transformer-based framework initially
developed to improve scalability in large-item catalogues through sub-item
decomposition. We repurpose RecJPQ's sub-item architecture to model
personalised popularity at a finer granularity. This allows us to capture
shared repetition patterns across sub-embeddings - latent structures not
accessible through item-level popularity alone. We propose a novel integration
of sub-ID-level personalised popularity within the RecJPQ framework, enabling
explicit control over the trade-off between accuracy and personalised novelty.
Our sub-ID-level PPS method (sPPS) consistently outperforms item-level PPS by
achieving significantly higher personalised novelty without compromising
recommendation accuracy. Code and experiments are publicly available at
https://github.com/sisinflab/Sub-id-Popularity.

</details>


### [71] [FIRE: Faithful Interpretable Recommendation Explanations](https://arxiv.org/abs/2508.05225)
*S. M. F. Sani,Asal Meskin,Mohammad Amanlou,Hamid R. Rabiee*

Main category: cs.IR

TL;DR: 现有推荐系统自然语言解释方法有缺陷，提出FIRE框架，提升推荐准确性和解释质量，强调需突破现有范式。


<details>
  <summary>Details</summary>
Motivation: 现有将用户评论作为监督的推荐解释方法会混淆用户观点和系统推理，无法反映推荐的真实逻辑，且存在与模型预测弱对齐等问题。

Method: 提出FIRE框架，结合基于SHAP的特征归因和结构化、提示驱动的语言生成。

Result: FIRE不仅在推荐准确性上有竞争力，还在对齐性、结构性和忠实性等关键维度上显著提高了解释质量。

Conclusion: 需要超越以评论作为解释的范式，采用更具问责性和可解释性的解释方法。

Abstract: Natural language explanations in recommender systems are often framed as a
review generation task, leveraging user reviews as ground-truth supervision.
While convenient, this approach conflates a user's opinion with the system's
reasoning, leading to explanations that may be fluent but fail to reflect the
true logic behind recommendations. In this work, we revisit the core objective
of explainable recommendation: to transparently communicate why an item is
recommended by linking user needs to relevant item features. Through a
comprehensive analysis of existing methods across multiple benchmark datasets,
we identify common limitations-explanations that are weakly aligned with model
predictions, vague or inaccurate in identifying user intents, and overly
repetitive or generic. To overcome these challenges, we propose FIRE, a
lightweight and interpretable framework that combines SHAP-based feature
attribution with structured, prompt-driven language generation. FIRE produces
faithful, diverse, and user-aligned explanations, grounded in the actual
decision-making process of the model. Our results demonstrate that FIRE not
only achieves competitive recommendation accuracy but also significantly
improves explanation quality along critical dimensions such as alignment,
structure, and faithfulness. This work highlights the need to move beyond the
review-as-explanation paradigm and toward explanation methods that are both
accountable and interpretable.

</details>


### [72] [Difference Views for Visual Graph Query Building](https://arxiv.org/abs/2508.05314)
*Benedikt Kantz,Stefan Lengauer,Peter Waldert,Tobias Schreck*

Main category: cs.IR

TL;DR: 提出一个可视化查询界面，利用图差异展示查询构建迭代变化，支持自然语言接口，对比结果差异，通过案例证明系统对特定领域图数据探索和分析的适用性。


<details>
  <summary>Details</summary>
Motivation: 现有可视化查询构建器在查询构建迭代过程中，用户问题会变化，需要一种方式沟通这种变化。

Method: 使用图差异在查询构建迭代步骤中沟通变化，集成自然语言接口让用户表达信息需求，在结果视图对比结果差异。

Result: 构建了可视化查询界面，通过不同本体和使用场景的案例研究展示了系统的适用性。

Conclusion: 该系统有助于领域特定图的数据探索和分析。

Abstract: Knowledge Graphs (KGs) contain vast amounts of linked resources that encode
knowledge in various domains, which can be queried and searched for using
specialized languages like SPARQL, a query language developed to query KGs.
Existing visual query builders enable non-expert users to construct SPARQL
queries and utilize the knowledge contained in these graphs. Query building is,
however, an iterative and, often, visual process where the question of the user
can change and differ throughout the process, especially for explorative
search. Our visual querying interface communicates these change between
iterative steps in the query building process using graph differences to
contrast the changes and the evolution in the graph query. We also enable users
to formulate their evolving information needs using a natural language
interface directly integrated into the difference query view. We, furthermore,
communicate the change in results in the result view by contrasting the
differences in both result distribution and individual instances of the
prototype graph and demonstrate the system's applicability through case studies
on different ontologies and usage scenarios, illustrating how our system
fosters, both, data exploration and analysis of domain-specific graphs.

</details>


### [73] [Multi-Modal Multi-Behavior Sequential Recommendation with Conditional Diffusion-Based Feature Denoising](https://arxiv.org/abs/2508.05352)
*Xiaoxi Cui,Weihai Lu,Yu Tong,Yiheng Li,Zhejun Zhao*

Main category: cs.IR

TL;DR: 本文聚焦多模态多行为序列推荐问题，提出M³BSR模型，实验显示其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 有效融合用户多样行为模式与物品多模态信息以提升序列推荐准确性，解决现有多模态多行为序列推荐存在的对不同行为模态偏好表征不足、难处理用户行为隐式噪声和多模态表征模态噪声等问题。

Method: 提出M³BSR模型，用条件扩散模态去噪层去除多模态表征噪声，利用深度行为信息引导浅层行为数据去噪，引入多专家兴趣提取层明确建模行为和模态间的共同与特定兴趣。

Result: M³BSR在基准数据集上显著优于现有最先进方法。

Conclusion: M³BSR模型能有效解决多模态多行为序列推荐问题，提升推荐性能。

Abstract: The sequential recommendation system utilizes historical user interactions to
predict preferences. Effectively integrating diverse user behavior patterns
with rich multimodal information of items to enhance the accuracy of sequential
recommendations is an emerging and challenging research direction. This paper
focuses on the problem of multi-modal multi-behavior sequential recommendation,
aiming to address the following challenges: (1) the lack of effective
characterization of modal preferences across different behaviors, as user
attention to different item modalities varies depending on the behavior; (2)
the difficulty of effectively mitigating implicit noise in user behavior, such
as unintended actions like accidental clicks; (3) the inability to handle
modality noise in multi-modal representations, which further impacts the
accurate modeling of user preferences. To tackle these issues, we propose a
novel Multi-Modal Multi-Behavior Sequential Recommendation model (M$^3$BSR).
This model first removes noise in multi-modal representations using a
Conditional Diffusion Modality Denoising Layer. Subsequently, it utilizes deep
behavioral information to guide the denoising of shallow behavioral data,
thereby alleviating the impact of noise in implicit feedback through
Conditional Diffusion Behavior Denoising. Finally, by introducing a
Multi-Expert Interest Extraction Layer, M$^3$BSR explicitly models the common
and specific interests across behaviors and modalities to enhance
recommendation performance. Experimental results indicate that M$^3$BSR
significantly outperforms existing state-of-the-art methods on benchmark
datasets.

</details>


### [74] [Does Multimodality Improve Recommender Systems as Expected? A Critical Analysis and Future Directions](https://arxiv.org/abs/2508.05377)
*Hongyu Zhou,Yinan Zhang,Aixin Sun,Zhiqi Shen*

Main category: cs.IR

TL;DR: 提出结构化评估框架评估多模态推荐，发现多模态数据在稀疏场景和召回阶段有益，各模态重要性因任务而异，集成学习表现更好，大模型不一定效果佳。


<details>
  <summary>Details</summary>
Motivation: 多模态推荐集成不同数据类型虽流行，但集成实际好处不明，需明确何时及如何提升推荐效果。

Method: 提出结构化评估框架，从四个维度评估多模态推荐，用可复现多模态模型与传统基线对比，在不同平台评估性能，探索不同集成策略和模型大小，结合案例研究和其他领域发现。

Result: 多模态数据在稀疏交互场景和召回阶段有益；各模态重要性因任务而异；集成学习优于融合学习；大模型不一定效果更好。

Conclusion: 为构建高效多模态推荐系统提供实用见解，强调需谨慎选择模态、集成策略和设计模型。

Abstract: Multimodal recommendation systems are increasingly popular for their
potential to improve performance by integrating diverse data types. However,
the actual benefits of this integration remain unclear, raising questions about
when and how it truly enhances recommendations. In this paper, we propose a
structured evaluation framework to systematically assess multimodal
recommendations across four dimensions: Comparative Efficiency, Recommendation
Tasks, Recommendation Stages, and Multimodal Data Integration. We benchmark a
set of reproducible multimodal models against strong traditional baselines and
evaluate their performance on different platforms. Our findings show that
multimodal data is particularly beneficial in sparse interaction scenarios and
during the recall stage of recommendation pipelines. We also observe that the
importance of each modality is task-specific, where text features are more
useful in e-commerce and visual features are more effective in short-video
recommendations. Additionally, we explore different integration strategies and
model sizes, finding that Ensemble-Based Learning outperforms Fusion-Based
Learning, and that larger models do not necessarily deliver better results. To
deepen our understanding, we include case studies and review findings from
other recommendation domains. Our work provides practical insights for building
efficient and effective multimodal recommendation systems, emphasizing the need
for thoughtful modality selection, integration strategies, and model design.

</details>


### [75] [On the Reliability of Sampling Strategies in Offline Recommender Evaluation](https://arxiv.org/abs/2508.05398)
*Bruno L. Pereira,Alan Said,Rodrygo L. T. Santos*

Main category: cs.IR

TL;DR: 本文研究不同日志记录和采样选择对离线评估可靠性的影响，模拟多种曝光偏差，评估常见采样策略的四个维度，为选择可靠的离线评估策略提供指导。


<details>
  <summary>Details</summary>
Motivation: 现有方法在评估缓解采样偏差时，未考虑不同曝光条件和真实用户偏好，本文旨在研究不同组合对离线评估可靠性的影响。

Method: 使用全观测数据集作为真实值，系统模拟不同曝光偏差，从采样分辨率、保真度、鲁棒性和预测能力四个维度评估常见采样策略。

Result: 发现采样在何时以及如何扭曲评估结果。

Conclusion: 为选择能产生可靠和稳健离线比较的策略提供了实用指导。

Abstract: Offline evaluation plays a central role in benchmarking recommender systems
when online testing is impractical or risky. However, it is susceptible to two
key sources of bias: exposure bias, where users only interact with items they
are shown, and sampling bias, introduced when evaluation is performed on a
subset of logged items rather than the full catalog. While prior work has
proposed methods to mitigate sampling bias, these are typically assessed on
fixed logged datasets rather than for their ability to support reliable model
comparisons under varying exposure conditions or relative to true user
preferences. In this paper, we investigate how different combinations of
logging and sampling choices affect the reliability of offline evaluation.
Using a fully observed dataset as ground truth, we systematically simulate
diverse exposure biases and assess the reliability of common sampling
strategies along four dimensions: sampling resolution (recommender model
separability), fidelity (agreement with full evaluation), robustness (stability
under exposure bias), and predictive power (alignment with ground truth). Our
findings highlight when and how sampling distorts evaluation outcomes and offer
practical guidance for selecting strategies that yield faithful and robust
offline comparisons.

</details>


### [76] [RankArena: A Unified Platform for Evaluating Retrieval, Reranking and RAG with Human and LLM Feedback](https://arxiv.org/abs/2508.05512)
*Abdelrahman Abdallah,Mahmoud Abdalla,Bhawna Piryani,Jamshid Mozafari,Mohammed Ali,Adam Jatowt*

Main category: cs.IR

TL;DR: 提出RankArena平台用于评估检索和RAG系统，支持多模式评估，捕获细粒度反馈，集成LLM评估，数据可用于训练，平台和演示视频公开。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏可扩展、以用户为中心和多视角的评估工具，难以评估检索增强生成和文档重排系统质量。

Method: 构建RankArena平台，支持直接重排可视化、盲对比较、手动文档标注、端到端RAG答案质量评估等多种评估模式，捕获细粒度相关性反馈，集成LLM评估。

Result: 开发出RankArena平台，可存储结构化评估数据集。

Conclusion: RankArena平台为评估检索和RAG系统提供了有效解决方案，公开平台和演示视频方便使用。

Abstract: Evaluating the quality of retrieval-augmented generation (RAG) and document
reranking systems remains challenging due to the lack of scalable,
user-centric, and multi-perspective evaluation tools. We introduce RankArena, a
unified platform for comparing and analysing the performance of retrieval
pipelines, rerankers, and RAG systems using structured human and LLM-based
feedback as well as for collecting such feedback. RankArena supports multiple
evaluation modes: direct reranking visualisation, blind pairwise comparisons
with human or LLM voting, supervised manual document annotation, and end-to-end
RAG answer quality assessment. It captures fine-grained relevance feedback
through both pairwise preferences and full-list annotations, along with
auxiliary metadata such as movement metrics, annotation time, and quality
ratings. The platform also integrates LLM-as-a-judge evaluation, enabling
comparison between model-generated rankings and human ground truth annotations.
All interactions are stored as structured evaluation datasets that can be used
to train rerankers, reward models, judgment agents, or retrieval strategy
selectors. Our platform is publicly available at https://rankarena.ngrok.io/,
and the Demo video is provided https://youtu.be/jIYAP4PaSSI.

</details>


### [77] [KuaiLive: A Real-time Interactive Dataset for Live Streaming Recommendation](https://arxiv.org/abs/2508.05633)
*Changle Qu,Sunhao Dai,Ke Guo,Liqin Zhao,Yanan Niu,Xiao Zhang,Jun Xu*

Main category: cs.IR

TL;DR: 介绍首个实时互动直播数据集KuaiLive，含用户与主播交互日志，有诸多优势，可支持多种直播任务研究，且公开可用。


<details>
  <summary>Details</summary>
Motivation: 解决学术界因缺乏反映直播环境动态性的公开数据集而阻碍研究进展的问题。

Method: 从快手收集23,772名用户和452,621名主播21天的交互日志，构建KuaiLive数据集，并进行多视角分析和代表性推荐方法评估。

Result: 建立了强大的基准，该数据集可支持直播领域多种任务，其细粒度行为数据能用于多行为建模等研究。

Conclusion: KuaiLive数据集为直播推荐研究提供了有力支持，推动相关领域发展。

Abstract: Live streaming platforms have become a dominant form of online content
consumption, offering dynamically evolving content, real-time interactions, and
highly engaging user experiences. These unique characteristics introduce new
challenges that differentiate live streaming recommendation from traditional
recommendation settings and have garnered increasing attention from industry in
recent years. However, research progress in academia has been hindered by the
lack of publicly available datasets that accurately reflect the dynamic nature
of live streaming environments. To address this gap, we introduce KuaiLive, the
first real-time, interactive dataset collected from Kuaishou, a leading live
streaming platform in China with over 400 million daily active users. The
dataset records the interaction logs of 23,772 users and 452,621 streamers over
a 21-day period. Compared to existing datasets, KuaiLive offers several
advantages: it includes precise live room start and end timestamps, multiple
types of real-time user interactions (click, comment, like, gift), and rich
side information features for both users and streamers. These features enable
more realistic simulation of dynamic candidate items and better modeling of
user and streamer behaviors. We conduct a thorough analysis of KuaiLive from
multiple perspectives and evaluate several representative recommendation
methods on it, establishing a strong benchmark for future research. KuaiLive
can support a wide range of tasks in the live streaming domain, such as top-K
recommendation, click-through rate prediction, watch time prediction, and gift
price prediction. Moreover, its fine-grained behavioral data also enables
research on multi-behavior modeling, multi-task learning, and fairness-aware
recommendation. The dataset and related resources are publicly available at
https://imgkkk574.github.io/KuaiLive.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [78] [NAEx: A Plug-and-Play Framework for Explaining Network Alignment](https://arxiv.org/abs/2508.04731)
*Shruti Saxena,Arijit Khan,Joydeep Chandra*

Main category: cs.LG

TL;DR: 提出可解释网络对齐框架NAEx，能解释多种对齐模型，经实验验证有效。


<details>
  <summary>Details</summary>
Motivation: 现有网络对齐模型可解释性有限，难以理解对齐决策和建立信任，尤其是在高风险领域。

Method: 引入NAEx框架，通过可学习的边和特征掩码联合参数化图结构和特征空间，设置优化目标确保解释忠实于原预测并能对比网络相似度；提出针对性评估指标。

Result: 在基准数据集上，将NAEx与四个代表性网络对齐模型集成，证明了其有效性和效率。

Conclusion: NAEx是一个即插即用、模型无关的归纳式框架，能有效解释网络对齐模型。

Abstract: Network alignment (NA) identifies corresponding nodes across multiple
networks, with applications in domains like social networks, co-authorship, and
biology. Despite advances in alignment models, their interpretability remains
limited, making it difficult to understand alignment decisions and posing
challenges in building trust, particularly in high-stakes domains. To address
this, we introduce NAEx, a plug-and-play, model-agnostic framework that
explains alignment models by identifying key subgraphs and features influencing
predictions. NAEx addresses the key challenge of preserving the joint
cross-network dependencies on alignment decisions by: (1) jointly
parameterizing graph structures and feature spaces through learnable edge and
feature masks, and (2) introducing an optimization objective that ensures
explanations are both faithful to the original predictions and enable
meaningful comparisons of structural and feature-based similarities between
networks. NAEx is an inductive framework that efficiently generates NA
explanations for previously unseen data. We introduce evaluation metrics
tailored to alignment explainability and demonstrate NAEx's effectiveness and
efficiency on benchmark datasets by integrating it with four representative NA
models.

</details>


### [79] [LumiGen: An LVLM-Enhanced Iterative Framework for Fine-Grained Text-to-Image Generation](https://arxiv.org/abs/2508.04732)
*Xiaoqi Dong,Xiangyu Zhou,Nicholas Evans,Yujia Lin*

Main category: cs.LG

TL;DR: 提出LVLM增强的迭代框架LumiGen提升T2I模型性能，在基准测试中表现优异，验证LVLM集成有效性。


<details>
  <summary>Details</summary>
Motivation: 现有T2I模型在处理复杂指令、细粒度内容控制和语义一致性方面存在挑战，而LVLMs有强大跨模态理解和指令跟随能力，因此想结合二者提升T2I模型性能。

Method: 提出LumiGen框架，包含IPPA模块进行提示增强和IVFR模块作为“视觉评论家”迭代优化图像。

Result: 在LongBench - T2I基准测试中，LumiGen平均得分3.08，优于现有基线模型，在文本渲染和姿态表达等方面有显著改进。

Conclusion: LVLM集成可实现更可控、更高质量的图像生成，LumiGen框架有效。

Abstract: Text-to-Image (T2I) generation has made significant advancements with
diffusion models, yet challenges persist in handling complex instructions,
ensuring fine-grained content control, and maintaining deep semantic
consistency. Existing T2I models often struggle with tasks like accurate text
rendering, precise pose generation, or intricate compositional coherence.
Concurrently, Vision-Language Models (LVLMs) have demonstrated powerful
capabilities in cross-modal understanding and instruction following. We propose
LumiGen, a novel LVLM-enhanced iterative framework designed to elevate T2I
model performance, particularly in areas requiring fine-grained control,
through a closed-loop, LVLM-driven feedback mechanism. LumiGen comprises an
Intelligent Prompt Parsing & Augmentation (IPPA) module for proactive prompt
enhancement and an Iterative Visual Feedback & Refinement (IVFR) module, which
acts as a "visual critic" to iteratively correct and optimize generated images.
Evaluated on the challenging LongBench-T2I Benchmark, LumiGen achieves a
superior average score of 3.08, outperforming state-of-the-art baselines.
Notably, our framework demonstrates significant improvements in critical
dimensions such as text rendering and pose expression, validating the
effectiveness of LVLM integration for more controllable and higher-quality
image generation.

</details>


### [80] [MissMecha: An All-in-One Python Package for Studying Missing Data Mechanisms](https://arxiv.org/abs/2508.04740)
*Youran Zhou,Mohamed Reda Bouadjenek,Sunil Aryal*

Main category: cs.LG

TL;DR: 提出开源Python工具包MissMecha，用于模拟、可视化和评估缺失数据，支持数值和分类特征，为处理不完整数据提供统一平台。


<details>
  <summary>Details</summary>
Motivation: 现有模拟缺失数据工具零散、机制有限且多只关注数值变量，无法应对现实表格数据的异质性。

Method: 开发MissMecha工具包，支持在MCAR、MAR和MNAR假设下模拟、可视化和评估缺失数据。

Result: MissMecha支持数值和分类特征，具备可视化诊断、MCAR测试工具和类型感知插补评估指标。

Conclusion: MissMecha为处理不完整数据的研究人员和从业者提供统一平台，可支持数据质量研究、基准测试和教育。

Abstract: Incomplete data is a persistent challenge in real-world datasets, often
governed by complex and unobservable missing mechanisms. Simulating missingness
has become a standard approach for understanding its impact on learning and
analysis. However, existing tools are fragmented, mechanism-limited, and
typically focus only on numerical variables, overlooking the heterogeneous
nature of real-world tabular data. We present MissMecha, an open-source Python
toolkit for simulating, visualizing, and evaluating missing data under MCAR,
MAR, and MNAR assumptions. MissMecha supports both numerical and categorical
features, enabling mechanism-aware studies across mixed-type tabular datasets.
It includes visual diagnostics, MCAR testing utilities, and type-aware
imputation evaluation metrics. Designed to support data quality research,
benchmarking, and education,MissMecha offers a unified platform for researchers
and practitioners working with incomplete data.

</details>


### [81] [Echo State Networks for Bitcoin Time Series Prediction](https://arxiv.org/abs/2508.05416)
*Mansi Sharma,Enrico Sartor,Marc Cavazza,Helmut Prendinger*

Main category: cs.LG

TL;DR: 本文探索用回声状态网络（ESNs）进行加密货币价格预测，尤其在极端波动时期，经混沌分析表明其表现显著优于现有机器学习方法。


<details>
  <summary>Details</summary>
Motivation: 股票和加密货币价格因高波动性和非平稳性难以预测，此前ESNs多用于股票市场短期走势建模，本文首次探索其在加密货币预测中的应用。

Method: 运用ESNs进行加密货币价格预测，在混沌时期通过Lyapunov指数进行混沌分析。

Result: 该方法显著优于现有机器学习方法，与Lyapunov指数分析结果一致。

Conclusion: ESNs在混沌时期表现稳健，在高混沌状态下比Boosting和Naïve方法更出色。

Abstract: Forecasting stock and cryptocurrency prices is challenging due to high
volatility and non-stationarity, influenced by factors like economic changes
and market sentiment. Previous research shows that Echo State Networks (ESNs)
can effectively model short-term stock market movements, capturing nonlinear
patterns in dynamic data. To the best of our knowledge, this work is among the
first to explore ESNs for cryptocurrency forecasting, especially during extreme
volatility. We also conduct chaos analysis through the Lyapunov exponent in
chaotic periods and show that our approach outperforms existing machine
learning methods by a significant margin. Our findings are consistent with the
Lyapunov exponent analysis, showing that ESNs are robust during chaotic periods
and excel under high chaos compared to Boosting and Na\"ive methods.

</details>


### [82] [Edge-Assisted Collaborative Fine-Tuning for Multi-User Personalized Artificial Intelligence Generated Content (AIGC)](https://arxiv.org/abs/2508.04745)
*Nan Li,Wanting Yang,Marie Siew,Zehui Xiong,Binbin Chen,Shiwen Mao,Kwok-Yan Lam*

Main category: cs.LG

TL;DR: 本文提出集群感知分层联邦聚合框架解决边缘AIGC计算和隐私问题，评估显示其能加速收敛并适用于多用户个性化服务。


<details>
  <summary>Details</summary>
Motivation: Diffusion models推理计算需求高，云端方案无法解决边缘设备多用户AIGC场景下的隐私、个性化和通信成本问题。

Method: 提出集群感知分层联邦聚合框架，基于LoRA进行参数高效本地微调，先对客户端聚类，进行集群内聚合，再进行集群间知识交互，同时训练设备上的个性化模型和服务器上带多个LoRA适配器的共享全局模型，对提示进行编码。

Result: 框架能加速收敛。

Conclusion: 该框架在边缘约束下对可扩展多用户个性化AIGC服务具有实际可行性。

Abstract: Diffusion models (DMs) have emerged as powerful tools for high-quality
content generation, yet their intensive computational requirements for
inference pose challenges for resource-constrained edge devices. Cloud-based
solutions aid in computation but often fall short in addressing privacy risks,
personalization efficiency, and communication costs in multi-user edge-AIGC
scenarios. To bridge this gap, we first analyze existing edge-AIGC applications
in personalized content synthesis, revealing their limitations in efficiency
and scalability. We then propose a novel cluster-aware hierarchical federated
aggregation framework. Based on parameter-efficient local fine-tuning via
Low-Rank Adaptation (LoRA), the framework first clusters clients based on the
similarity of their uploaded task requirements, followed by an intra-cluster
aggregation for enhanced personalization at the server-side. Subsequently, an
inter-cluster knowledge interaction paradigm is implemented to enable
hybrid-style content generation across diverse clusters.Building upon federated
learning (FL) collaboration, our framework simultaneously trains personalized
models for individual users at the devices and a shared global model enhanced
with multiple LoRA adapters on the server,enabling efficient edge inference;
meanwhile, all prompts for clustering and inference are encoded prior to
transmission, thereby further mitigating the risk of plaintext leakage. Our
evaluations demonstrate that the framework achieves accelerated convergence
while maintaining practical viability for scalable multi-user personalized AIGC
services under edge constraints.

</details>


### [83] [Discovering Interpretable Programmatic Policies via Multimodal LLM-assisted Evolutionary Search](https://arxiv.org/abs/2508.05433)
*Qinglong Hu,Xialiang Tong,Mingxuan Yuan,Fei Liu,Zhichao Lu,Qingfu Zhang*

Main category: cs.LG

TL;DR: 本文提出MLES方法解决深度强化学习在控制策略设计中缺乏可解释性的问题，实验显示其在控制任务中有良好表现。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习虽提升性能但缺乏可解释性，影响信任和实际部署，需解决可解释性和高性能的双重挑战。

Method: 引入Multimodal Large Language Model - assisted Evolutionary Search (MLES)方法，利用多模态大语言模型作为策略生成器，结合进化机制优化策略，在策略生成过程中集成视觉反馈驱动的行为分析。

Result: 在两个控制任务中，MLES的策略发现能力和效率与PPO相当，且有透明的控制逻辑和可追溯的设计过程。

Conclusion: MLES克服了预定义领域特定语言的局限性，便于知识转移和复用，可扩展到各种控制任务，有望成为下一代可解释控制策略发现的领先方法。

Abstract: Interpretability and high performance are essential goals in designing
control policies, particularly for safety-critical tasks. Deep reinforcement
learning has greatly enhanced performance, yet its inherent lack of
interpretability often undermines trust and hinders real-world deployment. This
work addresses these dual challenges by introducing a novel approach for
programmatic policy discovery, called Multimodal Large Language Model-assisted
Evolutionary Search (MLES). MLES utilizes multimodal large language models as
policy generators, combining them with evolutionary mechanisms for automatic
policy optimization. It integrates visual feedback-driven behavior analysis
within the policy generation process to identify failure patterns and
facilitate targeted improvements, enhancing the efficiency of policy discovery
and producing adaptable, human-aligned policies. Experimental results show that
MLES achieves policy discovery capabilities and efficiency comparable to
Proximal Policy Optimization (PPO) across two control tasks, while offering
transparent control logic and traceable design processes. This paradigm
overcomes the limitations of predefined domain-specific languages, facilitates
knowledge transfer and reuse, and is scalable across various control tasks.
MLES shows promise as a leading approach for the next generation of
interpretable control policy discovery.

</details>


### [84] [A Foundational Multi-Modal Model for Few-Shot Learning](https://arxiv.org/abs/2508.04746)
*Pengtao Dang,Tingbo Guo,Sha Cao,Chi Zhang*

Main category: cs.LG

TL;DR: 提出利用大跨模态模型改进少样本学习泛化能力的方法，构建M3FD数据集和M3F框架，降低在数据稀缺科学领域应用大跨模态模型的门槛。


<details>
  <summary>Details</summary>
Motivation: 少样本学习在样本有限、数据收集成本高的领域至关重要，需提高其泛化能力。

Method: 构建M3FD数据集，引入M3F框架，通过模块化管道支持多种数据类型，在M3FD上微调模型。

Result: 大跨模态模型可提升少样本学习模型泛化能力，M3F框架经微调能提高性能，M3FD有配套工具。

Conclusion: 数据集和框架提供统一可扩展的解决方案，降低在数据稀缺科学领域应用大跨模态模型的障碍。

Abstract: Few-shot learning (FSL) is a machine learning paradigm that aims to
generalize models from a small number of labeled examples, typically fewer than
10 per class. FSL is particularly crucial in biomedical, environmental,
materials, and mechanical sciences, where samples are limited and data
collection is often prohibitively costly, time-consuming, or ethically
constrained. In this study, we present an innovative approach to FSL by
demonstrating that a Large Multi-Modal Model (LMMM), trained on a set of
independent tasks spanning diverse domains, task types, and input modalities,
can substantially improve the generalization of FSL models, outperforming
models based on conventional meta-learning on tasks of the same type. To
support this, we first constructed a Multi-Modal Model Few-shot Dataset (M3FD,
over 10K+ few-shot samples), which includes 2D RGB images, 2D/3D medical scans,
tabular and time-course datasets, from which we manually curated FSL tasks such
as classification. We further introduced M3F (Multi-Modal Model for Few-shot
learning framework), a novel Large Multi-Modal Model framework tailored for
data-constrained scientific applications. M3F supports a wide range of
scientific data types through a modular pipeline. By fine-tuning the model on
M3FD, M3F improves model performance, making LMMM feasible for real-world FSL
deployment. The source code is located at https://github.com/ptdang1001/M3F. To
democratize access to complex FSL data and promote reproducibility for public
usage, M3FD is paired with a flexible and user-friendly tool that enables
efficient querying, task-specific sampling, and preprocessing. Together, our
dataset and framework offer a unified, scalable solution that significantly
lowers the barrier to applying LMMMs in data-scarce scientific domains.

</details>


### [85] [TrajEvo: Trajectory Prediction Heuristics Design via LLM-driven Evolution](https://arxiv.org/abs/2508.05616)
*Zhikai Zhao,Chuanbo Hua,Federico Berto,Kanghoon Lee,Zihan Ma,Jiachen Li,Jinkyoo Park*

Main category: cs.LG

TL;DR: 本文提出TrajEvo框架，利用大语言模型自动设计轨迹预测启发式方法，在多个数据集上表现出色，且泛化能力强，还开源代码。


<details>
  <summary>Details</summary>
Motivation: 传统启发式方法缺乏准确性和泛化性，深度学习方法有计算成本高、可解释性差和泛化能力弱等问题，需要新的轨迹预测方法。

Method: 引入TrajEvo框架，利用大语言模型，采用进化算法从过往轨迹数据生成和优化预测启发式方法，提出交叉代精英采样和统计反馈循环两项创新。

Result: TrajEvo在多个真实数据集上优于现有启发式方法，在未见的分布外真实数据集上超越启发式和深度学习方法。

Conclusion: TrajEvo是实现快速、可解释和可泛化轨迹预测启发式方法自动设计的有前景的一步。

Abstract: Trajectory prediction is a critical task in modeling human behavior,
especially in safety-critical domains such as social robotics and autonomous
vehicle navigation. Traditional heuristics based on handcrafted rules often
lack accuracy and generalizability. Although deep learning approaches offer
improved performance, they typically suffer from high computational cost,
limited explainability, and, importantly, poor generalization to
out-of-distribution (OOD) scenarios. In this paper, we introduce TrajEvo, a
framework that leverages Large Language Models (LLMs) to automatically design
trajectory prediction heuristics. TrajEvo employs an evolutionary algorithm to
generate and refine prediction heuristics from past trajectory data. We propose
two key innovations: Cross-Generation Elite Sampling to encourage population
diversity, and a Statistics Feedback Loop that enables the LLM to analyze and
improve alternative predictions. Our evaluations demonstrate that TrajEvo
outperforms existing heuristic methods across multiple real-world datasets, and
notably surpasses both heuristic and deep learning methods in generalizing to
an unseen OOD real-world dataset. TrajEvo marks a promising step toward the
automated design of fast, explainable, and generalizable trajectory prediction
heuristics. We release our source code to facilitate future research at
https://github.com/ai4co/trajevo.

</details>


### [86] [AttriLens-Mol: Attribute Guided Reinforcement Learning for Molecular Property Prediction with Large Language Models](https://arxiv.org/abs/2508.04748)
*Xuan Lin,Long Chen,Yile Wang*

Main category: cs.LG

TL;DR: 提出AttriLens - Mol框架用于分子属性预测，实验表明其能提升模型性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在分子属性预测中依赖人工提示且推理存在冗长、不相关问题，需要改进方法。

Method: 引入AttriLens - Mol框架，使用格式奖励、计数奖励和合理性奖励引导模型推理。

Result: 在分布内和分布外数据集上训练模型，性能提升，比监督微调模型和先进模型效果相当或更好；提取属性用于决策树模型性能更优。

Conclusion: AttriLens - Mol能有效引出更相关和有预测性的分子属性，提升属性预测的可解释性和性能。

Abstract: Large Language Models (LLMs) have shown promise in assisting molecular
property prediction tasks but often rely on human-crafted prompts and
chain-of-thought templates. While recent advanced large reasoning models like
DeepSeek-R1 employ reinforcement learning for an extended ``thinking'' process,
their reasoning can be verbose and lack relevance. We introduce AttriLens-Mol,
an attribute-guided reinforcement learning framework for molecular property
prediction with LLMs. AttriLens-Mol steers the model's reasoning by using: (1)
a format reward encouraging attribute-based structured output, (2) a count
reward to avoid enumerating irrelevant attributes, and (3) a rationality reward
using advanced LLMs and RDKit to verify the relatedness of the generated
attributes. This approach implicitly elicits the model's inherent knowledge of
relevant molecular attributes during reasoning, enables making predictions for
the molecular property more effectively. Experiments on both in-distribution
and out-of-distribution datasets show that, training both 7B-size
R1-Distilled-Qwen2.5 and R1-Distilled-LLaMA3.1 models on 4,000 samples with our
proposed AttriLens-Mol method significantly boosts the performance, getting
comparable or better results than supervised fine-tuning models
(Mol-Instructions, ChemDFM, etc.) and advanced models (GPT-3.5, GPT-4o,
DeepSeek-V3, DeepSeek-R1, etc.). Further, our extracted attributes for the
target property, when used as features for an interpretable decision tree
model, yield superior performance compared to attributes generated by prompting
LLMs. This shows that AttriLens-Mol effectively elicits more relevant and
predictive molecular attributes, leading to enhanced interpretability and
performance for property prediction. We release the code in
https://github.com/szu-tera/AttriLens-Mol.

</details>


### [87] [Are Large Language Models Dynamic Treatment Planners? An In Silico Study from a Prior Knowledge Injection Angle](https://arxiv.org/abs/2508.04755)
*Zhiyao Luo,Tingting Zhu*

Main category: cs.LG

TL;DR: 评估开源大语言模型（LLMs）作为动态胰岛素给药代理的表现，发现精心设计的零样本提示使小LLMs能有不错表现，但也存在局限性，提倡谨慎乐观地将LLMs集成到临床工作流。


<details>
  <summary>Details</summary>
Motivation: 强化学习（RL）的动态治疗方案（DTRs）实际部署受工程要求限制，大语言模型（LLMs）可通过语言提示嵌入临床知识，研究评估其作为动态胰岛素给药代理的表现。

Method: 在计算机模拟的1型糖尿病模拟器中，比较开源LLMs的零样本推理性能与针对该任务明确训练的小型基于神经网络的RL代理（SRAs）。

Result: 精心设计的零样本提示使小LLMs（如Qwen2.5 - 7B）在稳定患者群体中表现与SRAs相当或更优；LLMs存在过度激进给药等问题，显式推理对性能提升有限。

Conclusion: 应谨慎乐观地将LLMs集成到临床工作流，强调针对性提示工程、仔细验证以及结合语言推理和结构化生理建模的混合方法的必要性。

Abstract: Reinforcement learning (RL)-based dynamic treatment regimes (DTRs) hold
promise for automating complex clinical decision-making, yet their practical
deployment remains hindered by the intensive engineering required to inject
clinical knowledge and ensure patient safety. Recent advancements in large
language models (LLMs) suggest a complementary approach, where implicit prior
knowledge and clinical heuristics are naturally embedded through linguistic
prompts without requiring environment-specific training. In this study, we
rigorously evaluate open-source LLMs as dynamic insulin dosing agents in an in
silico Type 1 diabetes simulator, comparing their zero-shot inference
performance against small neural network-based RL agents (SRAs) explicitly
trained for the task. Our results indicate that carefully designed zero-shot
prompts enable smaller LLMs (e.g., Qwen2.5-7B) to achieve comparable or
superior clinical performance relative to extensively trained SRAs,
particularly in stable patient cohorts. However, LLMs exhibit notable
limitations, such as overly aggressive insulin dosing when prompted with
chain-of-thought (CoT) reasoning, highlighting critical failure modes including
arithmetic hallucination, temporal misinterpretation, and inconsistent clinical
logic. Incorporating explicit reasoning about latent clinical states (e.g.,
meals) yielded minimal performance gains, underscoring the current model's
limitations in capturing complex, hidden physiological dynamics solely through
textual inference. Our findings advocate for cautious yet optimistic
integration of LLMs into clinical workflows, emphasising the necessity of
targeted prompt engineering, careful validation, and potentially hybrid
approaches that combine linguistic reasoning with structured physiological
modelling to achieve safe, robust, and clinically effective decision-support
systems.

</details>


### [88] [PA-RNet: Perturbation-Aware Reasoning Network for Multimodal Time Series Forecasting](https://arxiv.org/abs/2508.04750)
*Chanjuan Liu,Shengzhi Wang,Enqiang Zhu*

Main category: cs.LG

TL;DR: 提出PA - RNet解决多模态时间序列文本数据干扰问题，实验表明其优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有多模态时间序列预测方法忽略文本数据内在干扰，干扰会降低模型性能。

Method: 提出PA - RNet，包含扰动感知投影模块和跨模态注意力机制，建立其Lipschitz连续性，引入文本扰动管道。

Result: PA - RNet在不同领域和时间设置的大量实验中始终优于现有基线。

Conclusion: PA - RNet能有效分离文本嵌入中的噪声，增强模型泛化能力，在噪声条件下有稳定性保证。

Abstract: In real-world applications, multimodal time series data often suffer from
interference, especially in the textual modality. Existing methods for
multimodal time series forecasting often neglect the inherent perturbations
within textual data, where irrelevant, noisy, or ambiguous content can
significantly degrade model performance, particularly when the noise exhibits
varying intensity or stems from structural inconsistencies. To address this
challenge, we propose PA-RNet (Perturbation-Aware Reasoning Network for
Multimodal Time Series Forecasting), a robust multimodal forecasting framework.
PA-RNet features a perturbation-aware projection module and a cross-modal
attention mechanism to effectively separate noise from the textual embeddings
while maintaining semantically meaningful representations, thereby enhancing
the model's generalization ability. Theoretically, we establish the Lipschitz
continuity of PA-RNet with respect to textual inputs and prove that the
proposed perturbation module can reduce expected prediction error, offering
strong guarantees of stability under noisy conditions. Furthermore, we
introduce a textual perturbation pipeline that can be seamlessly incorporated
into existing multimodal time series forecasting tasks, allowing for systematic
evaluation of the model's robustness in the presence of varying levels of
textual noise. Extensive experiments across diverse domains and temporal
settings demonstrate that PA-RNet consistently outperforms state-of-the-art
baselines.

</details>


### [89] [InfoQ: Mixed-Precision Quantization via Global Information Flow](https://arxiv.org/abs/2508.04753)
*Mehmet Emre Akbulut,Hazem Hesham Yousef Shalby,Fabrizio Pittorino,Manuel Roveri*

Main category: cs.LG

TL;DR: 提出无训练的混合精度量化框架InfoQ，在搜索阶段效率高且精度有提升


<details>
  <summary>Details</summary>
Motivation: 现有混合精度量化方法计算成本高，且不能捕捉量化误差的全局影响

Method: 通过不同位宽量化各层，单次前向传播测量后续层互信息变化评估层敏感度，将位宽分配转化为整数线性规划问题求解

Result: 搜索阶段比LIMPQ等方法使用数据少两个数量级，在高压缩率下，MobileNetV2和ResNet18在ImageNet上精度最多提升1%

Conclusion: InfoQ在搜索时间和精度上有更好权衡

Abstract: Mixed-precision quantization (MPQ) is crucial for deploying deep neural
networks on resource-constrained devices, but finding the optimal bit-width for
each layer represents a complex combinatorial optimization problem. Current
state-of-the-art methods rely on computationally expensive search algorithms or
local sensitivity heuristic proxies like the Hessian, which fail to capture the
cascading global effects of quantization error. In this work, we argue that the
quantization sensitivity of a layer should not be measured by its local
properties, but by its impact on the information flow throughout the entire
network. We introduce InfoQ, a novel framework for MPQ that is training-free in
the bit-width search phase. InfoQ assesses layer sensitivity by quantizing each
layer at different bit-widths and measuring, through a single forward pass, the
resulting change in mutual information in the subsequent layers. This
quantifies how much each layer quantization impacts the network information
flow. The resulting scores are used to formulate bit-width allocation as an
integer linear programming problem, which is solved efficiently to minimize
total sensitivity under a given budget (e.g., model size or BitOps). Our
retraining-free search phase provides a superior search-time/accuracy trade-off
(using two orders of magnitude less data compared to state-of-the-art methods
such as LIMPQ), while yielding up to a 1% accuracy improvement for MobileNetV2
and ResNet18 on ImageNet at high compression rates (14X and 10.66X).

</details>


### [90] [HFedATM: Hierarchical Federated Domain Generalization via Optimal Transport and Regularized Mean Aggregation](https://arxiv.org/abs/2508.05135)
*Thinh Nguyen,Trung Phan,Binh T. Nguyen,Khoa D Doan,Kok-Seng Wong*

Main category: cs.LG

TL;DR: 本文提出Hierarchical Federated Domain Generalization (HFedDG)场景，提出HFedATM方法提升FedDG基线性能，有实验和理论分析支撑。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习有可扩展性挑战，分层联邦学习（HFL）有域偏移问题，现有Federated Domain Generalization (FedDG)方法与HFL框架结合未充分探索。

Method: 提出HFedATM分层聚合方法，先通过Filter-wise Optimal Transport Alignment对齐不同站点模型卷积滤波器，再用Shrinkage-aware Regularized Mean Aggregation合并对齐模型。

Result: 实验显示HFedATM显著提升多个数据集上现有FedDG基线性能，保持计算和通信效率；理论分析表明其泛化误差界更紧，收敛更快，训练行为稳定。

Conclusion: HFedATM能有效解决HFL中的域偏移问题，提升模型性能，具有计算和通信效率。

Abstract: Federated Learning (FL) is a decentralized approach where multiple clients
collaboratively train a shared global model without sharing their raw data.
Despite its effectiveness, conventional FL faces scalability challenges due to
excessive computational and communication demands placed on a single central
server as the number of participating devices grows. Hierarchical Federated
Learning (HFL) addresses these issues by distributing model aggregation tasks
across intermediate nodes (stations), thereby enhancing system scalability and
robustness against single points of failure. However, HFL still suffers from a
critical yet often overlooked limitation: domain shift, where data
distributions vary significantly across different clients and stations,
reducing model performance on unseen target domains. While Federated Domain
Generalization (FedDG) methods have emerged to improve robustness to domain
shifts, their integration into HFL frameworks remains largely unexplored. In
this paper, we formally introduce Hierarchical Federated Domain Generalization
(HFedDG), a novel scenario designed to investigate domain shift within
hierarchical architectures. Specifically, we propose HFedATM, a hierarchical
aggregation method that first aligns the convolutional filters of models from
different stations through Filter-wise Optimal Transport Alignment and
subsequently merges aligned models using a Shrinkage-aware Regularized Mean
Aggregation. Our extensive experimental evaluations demonstrate that HFedATM
significantly boosts the performance of existing FedDG baselines across
multiple datasets and maintains computational and communication efficiency.
Moreover, theoretical analyses indicate that HFedATM achieves tighter
generalization error bounds compared to standard hierarchical averaging,
resulting in faster convergence and stable training behavior.

</details>


### [91] [Gaussian mixture layers for neural networks](https://arxiv.org/abs/2508.04883)
*Sinho Chewi,Philippe Rigollet,Yuling Yan*

Main category: cs.LG

TL;DR: 本文探索直接在概率测度上实现动态，引入高斯混合（GM）层并验证其在简单分类任务中的性能，还对比了GM层与经典全连接层的行为。


<details>
  <summary>Details</summary>
Motivation: 前人研究关注平均场理论对中等宽度网络的适用性，本文探索直接在概率测度上实现动态这一相反方向。

Method: 采用高斯混合模型作为灵活且表达力强的参数化分布族，结合Wasserstein梯度流理论推导训练动态，引入GM层到神经网络架构。

Result: 在简单分类任务实验中，GM层取得了与两层全连接网络相当的测试性能。数值结果表明，即使全连接层大到可视为平均场状态，GM层行为与经典全连接层也明显不同。

Conclusion: 提出的在概率测度上实现动态的方法可行，GM层是一种有潜力的新网络层，与经典全连接层有不同特性。

Abstract: The mean-field theory for two-layer neural networks considers infinitely wide
networks that are linearly parameterized by a probability measure over the
parameter space. This nonparametric perspective has significantly advanced both
the theoretical and conceptual understanding of neural networks, with
substantial efforts made to validate its applicability to networks of moderate
width. In this work, we explore the opposite direction, investigating whether
dynamics can be directly implemented over probability measures. Specifically,
we employ Gaussian mixture models as a flexible and expressive parametric
family of distributions together with the theory of Wasserstein gradient flows
to derive training dynamics for such measures. Our approach introduces a new
type of layer -- the Gaussian mixture (GM) layer -- that can be integrated into
neural network architectures. As a proof of concept, we validate our proposal
through experiments on simple classification tasks, where a GM layer achieves
test performance comparable to that of a two-layer fully connected network.
Furthermore, we examine the behavior of these dynamics and demonstrate
numerically that GM layers exhibit markedly different behavior compared to
classical fully connected layers, even when the latter are large enough to be
considered in the mean-field regime.

</details>


### [92] [Uncertainty-aware Predict-Then-Optimize Framework for Equitable Post-Disaster Power Restoration](https://arxiv.org/abs/2508.04780)
*Lin Jiang,Dahai Yu,Rongchao Xu,Tian Tang,Guang Wang*

Main category: cs.LG

TL;DR: 极端天气频发使电力系统恢复需兼顾效率与公平，本文提出EPOPR框架，实验显示能减少停电时长和社区间不平等。


<details>
  <summary>Details</summary>
Motivation: 当前电力恢复决策基于请求量，弱势群体请求少导致恢复方案不公平，需提出兼顾效率与公平的策略。

Method: 设计名为EPOPR的先预测后优化框架，包括用于不确定感知维修时长预测的公平保形分位数回归和用于公平决策的时空注意力强化学习。

Result: 与现有基线相比，EPOPR有效减少平均停电时长3.60%，降低不同社区间不平等14.19%。

Conclusion: EPOPR框架能有效解决电力系统恢复中的效率与公平问题。

Abstract: The increasing frequency of extreme weather events, such as hurricanes,
highlights the urgent need for efficient and equitable power system
restoration. Many electricity providers make restoration decisions primarily
based on the volume of power restoration requests from each region. However,
our data-driven analysis reveals significant disparities in request submission
volume, as disadvantaged communities tend to submit fewer restoration requests.
This disparity makes the current restoration solution inequitable, leaving
these communities vulnerable to extended power outages. To address this, we aim
to propose an equity-aware power restoration strategy that balances both
restoration efficiency and equity across communities. However, achieving this
goal is challenging for two reasons: the difficulty of predicting repair
durations under dataset heteroscedasticity, and the tendency of reinforcement
learning agents to favor low-uncertainty actions, which potentially undermine
equity. To overcome these challenges, we design a predict-then-optimize
framework called EPOPR with two key components: (1) Equity-Conformalized
Quantile Regression for uncertainty-aware repair duration prediction, and (2)
Spatial-Temporal Attentional RL that adapts to varying uncertainty levels
across regions for equitable decision-making. Experimental results show that
our EPOPR effectively reduces the average power outage duration by 3.60% and
decreases inequity between different communities by 14.19% compared to
state-of-the-art baselines.

</details>


### [93] [X-VFL: A New Vertical Federated Learning Framework with Cross Completion and Decision Subspace Alignment](https://arxiv.org/abs/2508.05568)
*Qinghua Yao,Xiangrui Xu,Zhize Li*

Main category: cs.LG

TL;DR: 提出新VFL框架X - VFL解决非对齐数据与本地独立推理问题，实验表明显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决VFL中数据样本需完美对齐和不支持单客户端本地独立推理的问题。

Method: 设计Cross Completion (XCom)和Decision Subspace Alignment (DS - Align)两个模块，提供不同算法的收敛定理。

Result: 在真实数据集上实验，如CIFAR - 10准确率提升15%，MIMIC - III提升43%。

Conclusion: X - VFL在部分特征缺失和本地独立推理场景中有效且优越。

Abstract: Vertical Federated Learning (VFL) enables collaborative learning by
integrating disjoint feature subsets from multiple clients/parties. However,
VFL typically faces two key challenges: i) the requirement for perfectly
aligned data samples across all clients (missing features are not allowed); ii)
the requirement for joint collaborative inference/prediction involving all
clients (it does not support locally independent inference on a single client).
To address these challenges, we propose X-VFL, a new VFL framework designed to
deal with the non-aligned data samples with (partially) missing features and to
support locally independent inference of new data samples for each client. In
particular, we design two novel modules in X-VFL: Cross Completion (XCom) and
Decision Subspace Alignment (DS-Align). XCom can complete/reconstruct missing
features for non-aligned data samples by leveraging information from other
clients. DS-Align aligns local features with completed and global features
across all clients within the decision subspace, thus enabling locally
independent inference at each client. Moreover, we provide convergence theorems
for different algorithms used in training X-VFL, showing an $O(1/\sqrt{T})$
convergence rate for SGD-type algorithms and an $O(1/T)$ rate for PAGE-type
algorithms, where $T$ denotes the number of training update steps. Extensive
experiments on real-world datasets demonstrate that X-VFL significantly
outperforms existing methods, e.g., achieving a 15% improvement in accuracy on
the image CIFAR-10 dataset and a 43% improvement on the medical MIMIC-III
dataset. These results validate the practical effectiveness and superiority of
X-VFL, particularly in scenarios involving partially missing features and
locally independent inference.

</details>


### [94] [RCUKF: Data-Driven Modeling Meets Bayesian Estimation](https://arxiv.org/abs/2508.04985)
*Kumar Anurag,Kasra Azizi,Francesco Sorrentino,Wenbin Wan*

Main category: cs.LG

TL;DR: 提出RCUKF框架解决复杂系统可靠过程模型获取难题，并在基准问题和车辆轨迹估计任务验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 复杂系统难以获得可靠过程模型，准确建模在工程和科学应用中至关重要。

Method: 提出RCUKF框架，结合储层计算的数据驱动建模与无迹卡尔曼滤波的贝叶斯估计。

Result: 在知名基准问题和高保真模拟环境的实时车辆轨迹估计任务中证明了RCUKF的有效性。

Conclusion: RCUKF框架能有效应对复杂系统建模挑战。

Abstract: Accurate modeling is crucial in many engineering and scientific applications,
yet obtaining a reliable process model for complex systems is often
challenging. To address this challenge, we propose a novel framework, reservoir
computing with unscented Kalman filtering (RCUKF), which integrates data-driven
modeling via reservoir computing (RC) with Bayesian estimation through the
unscented Kalman filter (UKF). The RC component learns the nonlinear system
dynamics directly from data, serving as a surrogate process model in the UKF
prediction step to generate state estimates in high-dimensional or chaotic
regimes where nominal mathematical models may fail. Meanwhile, the UKF
measurement update integrates real-time sensor data to correct potential drift
in the data-driven model. We demonstrate RCUKF effectiveness on well-known
benchmark problems and a real-time vehicle trajectory estimation task in a
high-fidelity simulation environment.

</details>


### [95] [Federated Continual Recommendation](https://arxiv.org/abs/2508.04792)
*Jaehyung Lim,Wonbin Kweon,Woojoo Kim,Junyoung Kim,Seongjin Choi,Dongha Kim,Hwanjo Yu*

Main category: cs.LG

TL;DR: 文章提出Federated Continual Recommendation (FCRec)任务并设计F3CRec框架，实验表明其在联邦环境下维持推荐质量表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有Federated Recommendation难以处理非平稳数据流，Continual Learning Recommendation不兼容联邦学习隐私约束，需结合两者优势。

Method: 提出F3CRec框架，包含客户端的Adaptive Replay Memory和服务器端的Item - wise Temporal Mean。

Result: F3CRec在联邦环境下能随时间维持较好的推荐质量，优于现有方法。

Conclusion: F3CRec框架能有效解决联邦环境下非平稳数据流的推荐问题，平衡知识保留与适应。

Abstract: The increasing emphasis on privacy in recommendation systems has led to the
adoption of Federated Learning (FL) as a privacy-preserving solution, enabling
collaborative training without sharing user data. While Federated
Recommendation (FedRec) effectively protects privacy, existing methods struggle
with non-stationary data streams, failing to maintain consistent recommendation
quality over time. On the other hand, Continual Learning Recommendation (CLRec)
methods address evolving user preferences but typically assume centralized data
access, making them incompatible with FL constraints. To bridge this gap, we
introduce Federated Continual Recommendation (FCRec), a novel task that
integrates FedRec and CLRec, requiring models to learn from streaming data
while preserving privacy. As a solution, we propose F3CRec, a framework
designed to balance knowledge retention and adaptation under the strict
constraints of FCRec. F3CRec introduces two key components: Adaptive Replay
Memory on the client side, which selectively retains past preferences based on
user-specific shifts, and Item-wise Temporal Mean on the server side, which
integrates new knowledge while preserving prior information. Extensive
experiments demonstrate that F3CRec outperforms existing approaches in
maintaining recommendation quality over time in a federated environment.

</details>


### [96] [Near Optimal Inference for the Best-Performing Algorithm](https://arxiv.org/abs/2508.05173)
*Amichai Painsky*

Main category: cs.LG

TL;DR: 文章提出新颖框架解决从竞争机器学习算法中选最优算法的子集选择问题，性能优于现有方法且有匹配下界。


<details>
  <summary>Details</summary>
Motivation: 从竞争机器学习算法中，根据基准数据集表现，识别未来未知数据集上最可能排名最高的算法，且考虑性能差异小的情况。

Method: 将问题转化为多项分布的子集选择问题，引入新颖框架，提供渐近和有限样本方案。

Result: 所提方案显著优于已知方法，有匹配下界证明其良好性能。

Conclusion: 提出的框架和方案能有效解决算法选择的子集选择问题，性能表现良好。

Abstract: Consider a collection of competing machine learning algorithms. Given their
performance on a benchmark of datasets, we would like to identify the best
performing algorithm. Specifically, which algorithm is most likely to rank
highest on a future, unseen dataset. A natural approach is to select the
algorithm that demonstrates the best performance on the benchmark. However, in
many cases the performance differences are marginal and additional candidates
may also be considered. This problem is formulated as subset selection for
multinomial distributions. Formally, given a sample from a countable alphabet,
our goal is to identify a minimal subset of symbols that includes the most
frequent symbol in the population with high confidence. In this work, we
introduce a novel framework for the subset selection problem. We provide both
asymptotic and finite-sample schemes that significantly improve upon currently
known methods. In addition, we provide matching lower bounds, demonstrating the
favorable performance of our proposed schemes.

</details>


### [97] [HCRide: Harmonizing Passenger Fairness and Driver Preference for Human-Centered Ride-Hailing](https://arxiv.org/abs/2508.04811)
*Lin Jiang,Yu Yang,Guang Wang*

Main category: cs.LG

TL;DR: 设计以人类为中心的叫车系统HCRide，用新算法优化效率、公平性和司机偏好，实验显示效果优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有叫车订单调度系统多关注运营商收入，可能损害乘客和司机体验，需设计兼顾乘客公平和司机偏好且不降低系统效率的系统。

Method: 设计基于Harmonization - oriented Actor - Bi - Critic (Habic) 多智能体强化学习算法的HCRide系统，含多智能体竞争机制、动态Actor网络和Bi - Critic网络。

Result: 用深圳和纽约市两个真实叫车数据集评估，HCRide较现有基线有效提升系统效率2.02%、公平性5.39%、司机偏好10.21%。

Conclusion: HCRide系统能在考虑司机偏好的同时优化系统效率和乘客公平性，具有良好效果。

Abstract: Order dispatch systems play a vital role in ride-hailing services, which
directly influence operator revenue, driver profit, and passenger experience.
Most existing work focuses on improving system efficiency in terms of operator
revenue, which may cause a bad experience for both passengers and drivers.
Hence, in this work, we aim to design a human-centered ride-hailing system by
considering both passenger fairness and driver preference without compromising
the overall system efficiency. However, it is nontrivial to achieve this target
due to the potential conflicts between passenger fairness and driver preference
since optimizing one may sacrifice the other. To address this challenge, we
design HCRide, a Human-Centered Ride-hailing system based on a novel
multi-agent reinforcement learning algorithm called Harmonization-oriented
Actor-Bi-Critic (Habic), which includes three major components (i.e., a
multi-agent competition mechanism, a dynamic Actor network, and a Bi-Critic
network) to optimize system efficiency and passenger fairness with driver
preference consideration. We extensively evaluate our HCRide using two
real-world ride-hailing datasets from Shenzhen and New York City. Experimental
results show our HCRide effectively improves system efficiency by 2.02%,
fairness by 5.39%, and driver preference by 10.21% compared to state-of-the-art
baselines.

</details>


### [98] [Unified Flow Matching for Long Horizon Event Forecasting](https://arxiv.org/abs/2508.04843)
*Xiao Shou*

Main category: cs.LG

TL;DR: 提出统一流匹配框架用于标记时间点过程，非自回归联合建模，在六个真实基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有神经时间点过程模型多为自回归，效率受限且长程预测有误差累积问题，需更好方法建模长时标记事件序列。

Method: 提出统一流匹配框架，通过连续和离散流匹配实现非自回归、联合建模事件间隔和事件类型，学习连续时间流以生成连贯长时事件轨迹。

Result: 在六个真实基准测试中，相比自回归和基于扩散的基线模型，在准确性和生成效率上有显著提升。

Conclusion: 所提出的统一流匹配框架有效，能解决现有模型在长时标记事件序列建模中的问题。

Abstract: Modeling long horizon marked event sequences is a fundamental challenge in
many real-world applications, including healthcare, finance, and user behavior
modeling. Existing neural temporal point process models are typically
autoregressive, predicting the next event one step at a time, which limits
their efficiency and leads to error accumulation in long-range forecasting. In
this work, we propose a unified flow matching framework for marked temporal
point processes that enables non-autoregressive, joint modeling of inter-event
times and event types, via continuous and discrete flow matching. By learning
continuous-time flows for both components, our method generates coherent long
horizon event trajectories without sequential decoding. We evaluate our model
on six real-world benchmarks and demonstrate significant improvements over
autoregressive and diffusion-based baselines in both accuracy and generation
efficiency.

</details>


### [99] [Negative Binomial Variational Autoencoders for Overdispersed Latent Modeling](https://arxiv.org/abs/2508.05423)
*Yixuan Zhang,Wenxin Zhang,Hua Jiang,Quyu Kong,Feng Zhou*

Main category: cs.LG

TL;DR: 提出NegBio - VAE模型，用负二项分布建模神经元脉冲计数，提升重建保真度。


<details>
  <summary>Details</summary>
Motivation: 传统VAE无法模拟生物神经元脉冲活动的变异性，现有Poisson - VAE有均值和方差相等的刚性约束，不能反映神经活动真实随机性。

Method: 提出NegBio - VAE，用负二项分布建模脉冲计数，开发两种ELBO优化方案和两种可微重参数化策略。

Result: 通过引入额外的离散参数，将Poisson潜在模型推广到负二项式公式，重建保真度显著提升。

Conclusion: 明确建模脉冲样激活中的过度离散性很重要。

Abstract: Biological neurons communicate through spike trains, discrete, irregular
bursts of activity that exhibit variability far beyond the modeling capacity of
conventional variational autoencoders (VAEs). Recent work, such as the
Poisson-VAE, makes a biologically inspired move by modeling spike counts using
the Poisson distribution. However, they impose a rigid constraint: equal mean
and variance, which fails to reflect the true stochastic nature of neural
activity. In this work, we challenge this constraint and introduce NegBio-VAE,
a principled extension of the VAE framework that models spike counts using the
negative binomial distribution. This shift grants explicit control over
dispersion, unlocking a broader and more accurate family of neural
representations. We further develop two ELBO optimization schemes and two
differentiable reparameterization strategies tailored to the negative binomial
setting. By introducing one additional dispersion parameter, NegBio-VAE
generalizes the Poisson latent model to a negative binomial formulation.
Empirical results demonstrate this minor yet impactful change leads to
significant gains in reconstruction fidelity, highlighting the importance of
explicitly modeling overdispersion in spike-like activations.

</details>


### [100] [Multi-Stage Knowledge-Distilled VGAE and GAT for Robust Controller-Area-Network Intrusion Detection](https://arxiv.org/abs/2508.04845)
*Robert Frenken,Sidra Ghayour Bhatti,Hanqin Zhang,Qadeer Ahmed*

Main category: cs.LG

TL;DR: 提出针对汽车CAN流量的多阶段入侵检测框架，结合VGAE和KD - GAT，在六个公开数据集上实验显示高效准确。


<details>
  <summary>Details</summary>
Motivation: CAN协议缺乏内置安全机制，易受网络攻击，需设计有效入侵检测框架。

Method: 结合VGAE进行结构异常检测和KD - GAT进行攻击分类，将CAN总线活动编码为图序列，采用VGAE选择性欠采样处理类别不平衡，GAT分类并可选分数级融合。

Result: 学生GAT相比教师模型参数减少96%且保持强预测性能，在六个数据集上实验，F1分数平均提升16.2%，在高度不平衡数据集上最高提升55%。

Conclusion: 所提多阶段入侵检测框架在CAN流量入侵检测中具有竞争力，尤其在处理高度不平衡数据集时表现出色。

Abstract: The Controller Area Network (CAN) protocol is a standard for in-vehicle
communication but remains susceptible to cyber-attacks due to its lack of
built-in security. This paper presents a multi-stage intrusion detection
framework leveraging unsupervised anomaly detection and supervised graph
learning tailored for automotive CAN traffic. Our architecture combines a
Variational Graph Autoencoder (VGAE) for structural anomaly detection with a
Knowledge-Distilled Graph Attention Network (KD-GAT) for robust attack
classification. CAN bus activity is encoded as graph sequences to model
temporal and relational dependencies. The pipeline applies VGAE-based selective
undersampling to address class imbalance, followed by GAT classification with
optional score-level fusion. The compact student GAT achieves 96% parameter
reduction compared to the teacher model while maintaining strong predictive
performance. Experiments on six public CAN intrusion datasets--Car-Hacking,
Car-Survival, and can-train-and-test--demonstrate competitive accuracy and
efficiency, with average improvements of 16.2% in F1-score over existing
methods, particularly excelling on highly imbalanced datasets with up to 55%
F1-score improvements.

</details>


### [101] [Provable Post-Training Quantization: Theoretical Analysis of OPTQ and Qronos](https://arxiv.org/abs/2508.04853)
*Haoyu Zhang,Shihao Zhang,Ian Colbert,Rayan Saab*

Main category: cs.LG

TL;DR: 本文为OPTQ和Qronos两种后训练量化（PTQ）算法提供量化误差界，分析误差诱导机制，为设计选择提供理论依据。


<details>
  <summary>Details</summary>
Motivation: OPTQ虽广泛应用但缺乏严格定量理论保证，因此要为OPTQ和Qronos算法提供量化误差界。

Method: 分析OPTQ迭代过程如何诱导量化误差，推导非渐近2 - 范数误差界；对随机变体建立更强的无穷范数误差界；将分析扩展到Qronos。

Result: 得到了OPTQ和Qronos算法确定性和随机变体的量化误差界。

Conclusion: 分析为实际设计选择提供理论依据，解释了Qronos的经验优势。

Abstract: Post-training quantization (PTQ) has become a crucial tool for reducing the
memory and compute costs of modern deep neural networks, including large
language models (LLMs). Among PTQ algorithms, the OPTQ framework-also known as
GPTQ-has emerged as a leading method due to its computational efficiency and
strong empirical performance. Despite its widespread adoption, however, OPTQ
lacks rigorous quantitative theoretical guarantees. This paper presents the
first quantitative error bounds for both deterministic and stochastic variants
of OPTQ, as well as for Qronos, a recent related state-of-the-art PTQ
algorithm. We analyze how OPTQ's iterative procedure induces quantization error
and derive non-asymptotic 2-norm error bounds that depend explicitly on the
calibration data and a regularization parameter that OPTQ uses. Our analysis
provides theoretical justification for several practical design choices,
including the widely used heuristic of ordering features by decreasing norm, as
well as guidance for selecting the regularization parameter. For the stochastic
variant, we establish stronger infinity-norm error bounds, which enable control
over the required quantization alphabet and are particularly useful for
downstream layers and nonlinearities. Finally, we extend our analysis to
Qronos, providing new theoretical bounds, for both its deterministic and
stochastic variants, that help explain its empirical advantages.

</details>


### [102] [Agnostics: Learning to Code in Any Programming Language via Reinforcement with a Universal Learning Environment](https://arxiv.org/abs/2508.04865)
*Aleksander Boruch-Gruszecki,Yangtian Zi,Zixuan Wu,Tejas Oberoi,Carolyn Jane Anderson,Joydeep Biswas,Arjun Guha*

Main category: cs.LG

TL;DR: 提出语言无关后训练管道Agnostics，解决低资源语言代码编写问题，在五种低资源语言上效果好并将开源相关资源。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在高资源语言代码编写表现出色，但在低资源语言上存在问题，且后训练存在每个语言需新数据集等工程瓶颈。

Method: 引入语言无关后训练管道Agnostics，通过判断代码外部可观察行为，使用LLM重写单元测试数据集、提供配置、在稳健代码执行环境应用RLVR。

Result: 在五种低资源语言上，提升Qwen - 3 4B性能，可扩展到更多模型家族，为≤16B参数模型在MultiPL - E和新的多语言版本LiveCodeBench上创造新的pass@1结果。

Conclusion: 将发布语言无关训练数据集、代码和配置，让任意编程语言的RL后训练像编辑短YAML文件一样简单。

Abstract: Large language models (LLMs) already excel at writing code in high-resource
languages such as Python and JavaScript, yet stumble on low-resource languages
that remain essential to science and engineering. Besides the obvious shortage
of pre-training data, post-training itself is a bottleneck: every new language
seems to require new datasets, test harnesses, and reinforcement-learning (RL)
infrastructure.
  We introduce Agnostics, a language-agnostic post-training pipeline that
eliminates this per-language engineering. The key idea is to judge code solely
by its externally observable behavior, so a single verifier can test solutions
written in any language. Concretely, we (i) use an LLM to rewrite existing
unit-test datasets into an I/O format, (ii) supply a short configuration that
tells the verifier how to compile and run a target language, and (iii) apply
reinforcement learning with verifiable rewards (RLVR) in a robust code
execution environment.
  Applied to five low-resource languages--Lua, Julia, R, OCaml, and
Fortran--Agnostics (1) improves Qwen-3 4B to performance that rivals other
16B-70B open-weight models; (2) scales cleanly to larger and diverse model
families (Qwen-3 8B, DeepSeek Coder 6.7B Instruct, Phi 4 Mini); and (3) for
${\le} 16$B parameter models, sets new state-of-the-art pass@1 results on
MultiPL-E and a new multi-language version LiveCodeBench that we introduce.
  We will release the language-agnostic training datasets (Ag-MBPP-X,
Ag-Codeforces-X, Ag-LiveCodeBench-X), training code, and ready-to-use
configurations, making RL post-training in any programming language as simple
as editing a short YAML file.

</details>


### [103] [Hilbert Neural Operator: Operator Learning in the Analytic Signal Domain](https://arxiv.org/abs/2508.04882)
*Saman Pordanesh,Pejman Shahsavari,Hossein Ghadjari*

Main category: cs.LG

TL;DR: 提出新的Hilbert Neural Operator (HNO)架构，利用信号处理的归纳偏置解决现有神经算子局限。


<details>
  <summary>Details</summary>
Motivation: 现有如FNO等神经算子有局限性，且信号分析还有其他有用信息可用于学习有效网络。

Method: 通过希尔伯特变换将输入信号映射到解析表示，使瞬时幅度和相位信息成为学习显式特征，再对变换后的表示应用谱卷积。

Result: 文中未提及实际结果，仅假设该架构能更有效地为因果、相位敏感和非平稳系统建模。

Conclusion: 文中未明确给出结论，主要是对HNO架构进行形式化并给出基于解析信号理论的设计理论动机。

Abstract: Neural operators have emerged as a powerful, data-driven paradigm for
learning solution operators of partial differential equations (PDEs).
State-of-the-art architectures, such as the Fourier Neural Operator (FNO), have
achieved remarkable success by performing convolutions in the frequency domain,
making them highly effective for a wide range of problems. However, this method
has some limitations, including the periodicity assumption of the Fourier
transform. In addition, there are other methods of analysing a signal, beyond
phase and amplitude perspective, and provide us with other useful information
to learn an effective network. We introduce the \textbf{Hilbert Neural Operator
(HNO)}, a new neural operator architecture to address some advantages by
incorporating a strong inductive bias from signal processing. HNO operates by
first mapping the input signal to its analytic representation via the Hilbert
transform, thereby making instantaneous amplitude and phase information
explicit features for the learning process. The core learnable operation -- a
spectral convolution -- is then applied to this Hilbert-transformed
representation. We hypothesize that this architecture enables HNO to model
operators more effectively for causal, phase-sensitive, and non-stationary
systems. We formalize the HNO architecture and provide the theoretical
motivation for its design, rooted in analytic signal theory.

</details>


### [104] [Uncertainty Quantification for Surface Ozone Emulators using Deep Learning](https://arxiv.org/abs/2508.04885)
*Kelsey Doerksen,Yuliya Marchetti,Steven Lu,Kevin Bowman,James Montgomery,Kazuyuki Miyazaki,Yarin Gal,Freddie Kalaitzis*

Main category: cs.LG

TL;DR: 空气污染严重，传统模型和深度学习模拟器各有不足，本文用不确定性感知U - Net架构预测地表臭氧残差并评估相关指标。


<details>
  <summary>Details</summary>
Motivation: 空气污染危害大，传统物理模型在模拟地表臭氧趋势驱动因素方面实用性不足，深度学习模拟器缺乏可解释性，需更好方法支持决策。

Method: 采用不确定性感知U - Net架构，结合贝叶斯和分位数回归方法预测MOMO - Chem模型的地表臭氧残差。

Result: 展示了2019年6月在北美和欧洲区域估计偏差的能力，对比两种不确定性量化方法的分数，辨别最佳和次优候选站点，评估土地利用信息影响。

Conclusion: 提出的方法可用于区域地表臭氧残差预测和偏差校正，对政策制定和公共卫生措施有参考价值。

Abstract: Air pollution is a global hazard, and as of 2023, 94\% of the world's
population is exposed to unsafe pollution levels. Surface Ozone (O3), an
important pollutant, and the drivers of its trends are difficult to model, and
traditional physics-based models fall short in their practical use for scales
relevant to human-health impacts. Deep Learning-based emulators have shown
promise in capturing complex climate patterns, but overall lack the
interpretability necessary to support critical decision making for policy
changes and public health measures. We implement an uncertainty-aware U-Net
architecture to predict the Multi-mOdel Multi-cOnstituent Chemical data
assimilation (MOMO-Chem) model's surface ozone residuals (bias) using Bayesian
and quantile regression methods. We demonstrate the capability of our
techniques in regional estimation of bias in North America and Europe for June
2019. We highlight the uncertainty quantification (UQ) scores between our two
UQ methodologies and discern which ground stations are optimal and sub-optimal
candidates for MOMO-Chem bias correction, and evaluate the impact of land-use
information in surface ozone residual modeling.

</details>


### [105] [Bidding-Aware Retrieval for Multi-Stage Consistency in Online Advertising](https://arxiv.org/abs/2508.05206)
*Bin Liu,Yunfei Liu,Ziru Xu,Zhaoyu Zhou,Zhi Kou,Yeqiu Yang,Han Zhu,Jian Xu,Bo Zheng*

Main category: cs.LG

TL;DR: 提出Bidding - Aware Retrieval (BAR)框架解决在线广告系统多级不一致问题，经实验和部署验证有效提升平台收入和广告展示量。


<details>
  <summary>Details</summary>
Motivation: 在线广告系统中检索阶段与排序阶段不一致，导致平台收入和广告商效果欠佳，需解决多级不一致问题。

Method: 提出BAR框架，包括Bidding - Aware Modeling、Asynchronous Near - Line Inference和Task - Attentive Refinement模块。

Result: 平台收入增加4.32%，正向运营广告展示量提升22.2%。

Conclusion: BAR框架能有效解决在线广告系统多级不一致问题，提升平台和广告商效益。

Abstract: Online advertising systems typically use a cascaded architecture to manage
massive requests and candidate volumes, where the ranking stages allocate
traffic based on eCPM (predicted CTR $\times$ Bid). With the increasing
popularity of auto-bidding strategies, the inconsistency between the
computationally sensitive retrieval stage and the ranking stages becomes more
pronounced, as the former cannot access precise, real-time bids for the vast ad
corpus. This discrepancy leads to sub-optimal platform revenue and advertiser
outcomes. To tackle this problem, we propose Bidding-Aware Retrieval (BAR), a
model-based retrieval framework that addresses multi-stage inconsistency by
incorporating ad bid value into the retrieval scoring function. The core
innovation is Bidding-Aware Modeling, incorporating bid signals through
monotonicity-constrained learning and multi-task distillation to ensure
economically coherent representations, while Asynchronous Near-Line Inference
enables real-time updates to the embedding for market responsiveness.
Furthermore, the Task-Attentive Refinement module selectively enhances feature
interactions to disentangle user interest and commercial value signals.
Extensive offline experiments and full-scale deployment across Alibaba's
display advertising platform validated BAR's efficacy: 4.32% platform revenue
increase with 22.2% impression lift for positively-operated advertisements.

</details>


### [106] [Leveraging Deep Learning for Physical Model Bias of Global Air Quality Estimates](https://arxiv.org/abs/2508.04886)
*Kelsey Doerksen,Yuliya Marchetti,Kevin Bowman,Steven Lu,James Montgomery,Yarin Gal,Freddie Kalaitzis,Kazuyuki Miyazaki*

Main category: cs.LG

TL;DR: 空气污染危害大，建模地表臭氧有挑战，用2D卷积神经网络架构估计模型残差，展示其优势并评估结合高分辨率卫星影像土地利用信息的影响，探讨对改善环境政策的作用。


<details>
  <summary>Details</summary>
Motivation: 空气污染是人类疾病和过早死亡的重大环境风险因素，地表臭氧建模存在挑战，全球臭氧趋势驱动因素不明限制物理模型应用。

Method: 采用基于2D卷积神经网络的架构来估计地表臭氧MOMO - Chem模型残差，评估结合高分辨率卫星影像土地利用信息。

Result: 该技术在北美和欧洲展示了潜力，相比传统机器学习方法能更好捕捉物理模型残差。

Conclusion: 研究结果有助于提高对城市尺度臭氧偏差影响因素的科学理解，可用于改善环境政策。

Abstract: Air pollution is the world's largest environmental risk factor for human
disease and premature death, resulting in more than 6 million permature deaths
in 2019. Currently, there is still a challenge to model one of the most
important air pollutants, surface ozone, particularly at scales relevant for
human health impacts, with the drivers of global ozone trends at these scales
largely unknown, limiting the practical use of physics-based models. We employ
a 2D Convolutional Neural Network based architecture that estimate surface
ozone MOMO-Chem model residuals, referred to as model bias. We demonstrate the
potential of this technique in North America and Europe, highlighting its
ability better to capture physical model residuals compared to a traditional
machine learning method. We assess the impact of incorporating land use
information from high-resolution satellite imagery to improve model estimates.
Importantly, we discuss how our results can improve our scientific
understanding of the factors impacting ozone bias at urban scales that can be
used to improve environmental policy.

</details>


### [107] [Retrieval-Augmented Water Level Forecasting for Everglades](https://arxiv.org/abs/2508.04888)
*Rahuul Rangaraj,Jimeng Shi,Rajendra Paudel,Giri Narasimhan,Yanzhao Wu*

Main category: cs.LG

TL;DR: 引入Retrieval - Augmented Forecasting (RAF)框架到水文领域用于水位预测，在大沼泽地数据上验证其提升了预测准确性，代码和数据开源。


<details>
  <summary>Details</summary>
Motivation: 准确水位预测对生态系统管理至关重要，深度学习时间序列基础模型在水文应用少且泛化能力差，缺乏有效适应机制。

Method: 引入RAF框架，维护历史观测外部存档，检索历史类似多变量水文事件丰富模型输入，探索比较基于相似度和互信息的RAF方法。

Result: 在大沼泽地真实数据上综合评估表明，RAF框架大幅提高了水位预测准确性。

Conclusion: RAF方法在环境水文学有潜力，为生态系统管理领域专家广泛采用自适应AI方法铺平道路。

Abstract: Accurate water level forecasting is crucial for managing ecosystems such as
the Everglades, a subtropical wetland vital for flood mitigation, drought
management, water resource planning, and biodiversity conservation. While
recent advances in deep learning, particularly time series foundation models,
have demonstrated success in general-domain forecasting, their application in
hydrology remains underexplored. Furthermore, they often struggle to generalize
across diverse unseen datasets and domains, due to the lack of effective
mechanisms for adaptation. To address this gap, we introduce
Retrieval-Augmented Forecasting (RAF) into the hydrology domain, proposing a
framework that retrieves historically analogous multivariate hydrological
episodes to enrich the model input before forecasting. By maintaining an
external archive of past observations, RAF identifies and incorporates relevant
patterns from historical data, thereby enhancing contextual awareness and
predictive accuracy without requiring the model for task-specific retraining or
fine-tuning. Furthermore, we explore and compare both similarity-based and
mutual information-based RAF methods. We conduct a comprehensive evaluation on
real-world data from the Everglades, demonstrating that the RAF framework
yields substantial improvements in water level forecasting accuracy. This study
highlights the potential of RAF approaches in environmental hydrology and paves
the way for broader adoption of adaptive AI methods by domain experts in
ecosystem management. The code and data are available at
https://github.com/rahuul2992000/WaterRAF.

</details>


### [108] [Honest and Reliable Evaluation and Expert Equivalence Testing of Automated Neonatal Seizure Detection](https://arxiv.org/abs/2508.04899)
*Jovana Kljajic,John M. O'Toole,Robert Hogan,Tamara Skoric*

Main category: cs.LG

TL;DR: 本文系统评估新生儿癫痫检测机器学习模型的性能指标，提出最佳实践框架，以实现对模型的全面评估。


<details>
  <summary>Details</summary>
Motivation: 当前新生儿癫痫检测机器学习模型评估指标不一致且有偏差，专家关于AI性能的说法缺乏严格验证，影响模型可比性和可解释性。

Method: 使用真实和合成癫痫注释，在不同类别不平衡、评分者间一致性和评分者数量的情况下，评估标准性能指标、共识策略和人类专家水平等效性测试。

Result: Matthews和Pearson相关系数在类别不平衡时表现更好；共识类型对评分者数量和一致性水平敏感；多评分者图灵测试（使用Fleiss k）能最好地反映专家级AI性能。

Conclusion: 建议报告至少一个平衡指标、敏感性等指标、多评分者图灵测试结果，并在保留验证集上进行评估，该框架为临床验证提供重要前提。

Abstract: Reliable evaluation of machine learning models for neonatal seizure detection
is critical for clinical adoption. Current practices often rely on inconsistent
and biased metrics, hindering model comparability and interpretability.
Expert-level claims about AI performance are frequently made without rigorous
validation, raising concerns about their reliability. This study aims to
systematically evaluate common performance metrics and propose best practices
tailored to the specific challenges of neonatal seizure detection. Using real
and synthetic seizure annotations, we assessed standard performance metrics,
consensus strategies, and human-expert level equivalence tests under varying
class imbalance, inter-rater agreement, and number of raters. Matthews and
Pearson's correlation coefficients outperformed the area under the curve in
reflecting performance under class imbalance. Consensus types are sensitive to
the number of raters and agreement level among them. Among human-expert level
equivalence tests, the multi-rater Turing test using Fleiss k best captured
expert-level AI performance. We recommend reporting: (1) at least one balanced
metric, (2) Sensitivity, specificity, PPV and NPV, (3) Multi-rater Turing test
results using Fleiss k, and (4) All the above on held-out validation set. This
proposed framework provides an important prerequisite to clinical validation by
enabling a thorough and honest appraisal of AI methods for neonatal seizure
detection.

</details>


### [109] [Sensitivity of Stability: Theoretical & Empirical Analysis of Replicability for Adaptive Data Selection in Transfer Learning](https://arxiv.org/abs/2508.04901)
*Prabhav Singh,Jessica Sorrell*

Main category: cs.LG

TL;DR: 论文对迁移学习中的可重复性进行理论与实证分析，提出选择敏感性指标，证明可重复性失败概率与选择敏感性和样本量的关系，通过实验揭示不同策略的可重复性表现，指出源域预训练可缓解问题。


<details>
  <summary>Details</summary>
Motivation: 当前对迁移学习中自适应数据选择策略下模型适应的可靠性理解不足，需研究可重复性。

Method: 提出数学框架量化适应有效性和结果一致性的权衡，定义选择敏感性指标，在MultiNLI语料库上用六种自适应选择策略进行实验。

Result: 可重复性失败概率与选择敏感性二次方成正比，与样本量指数级成反比；高自适应策略性能好但可重复性失败率高，低自适应策略失败率低于7%；源域预训练可降低失败率达30%并保留性能提升。

Conclusion: 为从业者提供性能 - 可重复性权衡的原则性指导，强调现代迁移学习系统需进行可重复性感知设计。

Abstract: The widespread adoption of transfer learning has revolutionized machine
learning by enabling efficient adaptation of pre-trained models to new domains.
However, the reliability of these adaptations remains poorly understood,
particularly when using adaptive data selection strategies that dynamically
prioritize training examples. We present a comprehensive theoretical and
empirical analysis of replicability in transfer learning, introducing a
mathematical framework that quantifies the fundamental trade-off between
adaptation effectiveness and result consistency. Our key contribution is the
formalization of selection sensitivity ($\Delta_Q$), a measure that captures
how adaptive selection strategies respond to perturbations in training data. We
prove that replicability failure probability: the likelihood that two
independent training runs produce models differing in performance by more than
a threshold, increases quadratically with selection sensitivity while
decreasing exponentially with sample size. Through extensive experiments on the
MultiNLI corpus using six adaptive selection strategies - ranging from uniform
sampling to gradient-based selection - we demonstrate that this theoretical
relationship holds precisely in practice. Our results reveal that highly
adaptive strategies like gradient-based and curriculum learning achieve
superior task performance but suffer from high replicability failure rates,
while less adaptive approaches maintain failure rates below 7%. Crucially, we
show that source domain pretraining provides a powerful mitigation mechanism,
reducing failure rates by up to 30% while preserving performance gains. These
findings establish principled guidelines for practitioners to navigate the
performance-replicability trade-off and highlight the need for
replicability-aware design in modern transfer learning systems.

</details>


### [110] [Advancing Hate Speech Detection with Transformers: Insights from the MetaHate](https://arxiv.org/abs/2508.04913)
*Santosh Chapagain,Shah Muhammad Hamdi,Soukaina Filali Boubrahimi*

Main category: cs.LG

TL;DR: 文章指出仇恨言论危害大，现有深度学习方法有局限，用MetaHate数据集探索基于transformer的模型用于仇恨言论检测，评估多个模型，微调ELECTRA表现最佳，并分析分类错误。


<details>
  <summary>Details</summary>
Motivation: 仇恨言论在社交媒体广泛存在且危害大，现有深度学习方法有长时依赖和并行效率低等局限，需开发强大自动检测方法。

Method: 使用MetaHate数据集，评估包括BERT、RoBERTa、GPT - 2和ELECTRA等多个最先进的transformer模型。

Result: 微调ELECTRA性能最高，F1分数达0.8980。

Conclusion: 探索了基于transformer模型检测仇恨言论，发现存在讽刺、编码语言和标签噪声等挑战。

Abstract: Hate speech is a widespread and harmful form of online discourse,
encompassing slurs and defamatory posts that can have serious social,
psychological, and sometimes physical impacts on targeted individuals and
communities. As social media platforms such as X (formerly Twitter), Facebook,
Instagram, Reddit, and others continue to facilitate widespread communication,
they also become breeding grounds for hate speech, which has increasingly been
linked to real-world hate crimes. Addressing this issue requires the
development of robust automated methods to detect hate speech in diverse social
media environments. Deep learning approaches, such as vanilla recurrent neural
networks (RNNs), long short-term memory (LSTM), and convolutional neural
networks (CNNs), have achieved good results, but are often limited by issues
such as long-term dependencies and inefficient parallelization. This study
represents the comprehensive exploration of transformer-based models for hate
speech detection using the MetaHate dataset--a meta-collection of 36 datasets
with 1.2 million social media samples. We evaluate multiple state-of-the-art
transformer models, including BERT, RoBERTa, GPT-2, and ELECTRA, with
fine-tuned ELECTRA achieving the highest performance (F1 score: 0.8980). We
also analyze classification errors, revealing challenges with sarcasm, coded
language, and label noise.

</details>


### [111] [ALScope: A Unified Toolkit for Deep Active Learning](https://arxiv.org/abs/2508.04937)
*Chenkai Wu,Yuanyuan Qi,Xiaohao Yang,Jueqing Lu,Gang Liu,Wray Buntine,Lan Du*

Main category: cs.LG

TL;DR: 提出新的深度主动学习平台ALScope，集成多数据集和算法，支持灵活配置实验因素，实验显示算法性能在不同场景差异大且部分有改进空间。


<details>
  <summary>Details</summary>
Motivation: 现实应用中分布偏移和数据不平衡问题受关注，缺乏统一平台进行公平系统评估。

Method: 开发ALScope平台，集成10个数据集和21个代表性算法，支持灵活配置实验因素并进行广泛实验。

Result: （1）DAL算法在不同领域和任务设置中性能差异显著；（2）在非标准场景有改进空间；（3）部分算法性能好但选择时间长。

Conclusion: DAL算法在不同场景表现不同，非标准场景需进一步研究，部分算法选择时间问题待解决。

Abstract: Deep Active Learning (DAL) reduces annotation costs by selecting the most
informative unlabeled samples during training. As real-world applications
become more complex, challenges stemming from distribution shifts (e.g.,
open-set recognition) and data imbalance have gained increasing attention,
prompting the development of numerous DAL algorithms. However, the lack of a
unified platform has hindered fair and systematic evaluation under diverse
conditions. Therefore, we present a new DAL platform ALScope for classification
tasks, integrating 10 datasets from computer vision (CV) and natural language
processing (NLP), and 21 representative DAL algorithms, including both
classical baselines and recent approaches designed to handle challenges such as
distribution shifts and data imbalance. This platform supports flexible
configuration of key experimental factors, ranging from algorithm and dataset
choices to task-specific factors like out-of-distribution (OOD) sample ratio,
and class imbalance ratio, enabling comprehensive and realistic evaluation. We
conduct extensive experiments on this platform under various settings. Our
findings show that: (1) DAL algorithms' performance varies significantly across
domains and task settings; (2) in non-standard scenarios such as imbalanced and
open-set settings, DAL algorithms show room for improvement and require further
investigation; and (3) some algorithms achieve good performance, but require
significantly longer selection time.

</details>


### [112] [REINA: Regularized Entropy Information-Based Loss for Efficient Simultaneous Speech Translation](https://arxiv.org/abs/2508.04946)
*Nameer Hirschkind,Joseph Liu,Mahesh Kumar Nandwana,Xiao Yu*

Main category: cs.LG

TL;DR: 论文介绍REINA损失函数优化同步语音翻译的延迟与质量权衡，在多语言任务上取得SOTA结果，且提升了效率。


<details>
  <summary>Details</summary>
Motivation: 同步语音翻译系统面临平衡翻译质量和延迟的挑战，需优化二者权衡。

Method: 提出Regularized Entropy INformation Adaptation (REINA)损失函数，基于信息理论原则，利用现有非流式翻译模型训练自适应策略。

Result: 在法、西、德与英语互译任务上，仅用开源或合成数据训练，取得了可比规模模型的SOTA流式结果；引入流式效率指标，显示REINA相比先前方法在延迟/质量权衡上最多提升21%。

Conclusion: REINA有助于推动延迟/质量权衡的帕累托前沿，提升了同步语音翻译系统的性能。

Abstract: Simultaneous Speech Translation (SimulST) systems stream in audio while
simultaneously emitting translated text or speech. Such systems face the
significant challenge of balancing translation quality and latency. We
introduce a strategy to optimize this tradeoff: wait for more input only if you
gain information by doing so. Based on this strategy, we present Regularized
Entropy INformation Adaptation (REINA), a novel loss to train an adaptive
policy using an existing non-streaming translation model. We derive REINA from
information theory principles and show that REINA helps push the reported
Pareto frontier of the latency/quality tradeoff over prior works. Utilizing
REINA, we train a SimulST model on French, Spanish and German, both from and
into English. Training on only open source or synthetically generated data, we
achieve state-of-the-art (SOTA) streaming results for models of comparable
size. We also introduce a metric for streaming efficiency, quantitatively
showing REINA improves the latency/quality trade-off by as much as 21% compared
to prior approaches, normalized against non-streaming baseline BLEU scores.

</details>


### [113] [Self-Error Adjustment: Theory and Practice of Balancing Individual Performance and Diversity in Ensemble Learning](https://arxiv.org/abs/2508.04948)
*Rui Zou*

Main category: cs.LG

TL;DR: 提出Self - Error Adjustment (SEA)框架改进集成学习，能精确控制准确性与多样性权衡，实验显示其性能优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 传统集成学习方法如Bagging、Boosting和Negative Correlation Learning (NCL)在平衡个体学习者准确性与多样性方面存在不足，需要改进。

Method: 提出SEA框架，将集成误差分解为个体性能项和多样性项，在损失函数中引入可调参数。

Result: 与NCL及其变体相比，SEA有更广泛的有效调整范围和更一致的多样性变化，实验证明在多个公开数据集上始终优于基线方法。

Conclusion: SEA框架能更灵活地调整，在微调策略中性能更优。

Abstract: Ensemble learning boosts performance by aggregating predictions from multiple
base learners. A core challenge is balancing individual learner accuracy with
diversity. Traditional methods like Bagging and Boosting promote diversity
through randomness but lack precise control over the accuracy-diversity
trade-off. Negative Correlation Learning (NCL) introduces a penalty to manage
this trade-off but suffers from loose theoretical bounds and limited adjustment
range. To overcome these limitations, we propose a novel framework called
Self-Error Adjustment (SEA), which decomposes ensemble errors into two distinct
components: individual performance terms, representing the self-error of each
base learner, and diversity terms, reflecting interactions among learners. This
decomposition allows us to introduce an adjustable parameter into the loss
function, offering precise control over the contribution of each component,
thus enabling finer regulation of ensemble performance. Compared to NCL and its
variants, SEA provides a broader range of effective adjustments and more
consistent changes in diversity. Furthermore, we establish tighter theoretical
bounds for adjustable ensemble methods and validate them through empirical
experiments. Experimental results on several public regression and
classification datasets demonstrate that SEA consistently outperforms baseline
methods across all tasks. Ablation studies confirm that SEA offers more
flexible adjustment capabilities and superior performance in fine-tuning
strategies.

</details>


### [114] [Compressed Decentralized Momentum Stochastic Gradient Methods for Nonconvex Optimization](https://arxiv.org/abs/2508.04950)
*Wei Liu,Anweshit Panda,Ujwal Pandey,Christopher Brissette,Yikang Shen,George M. Slota,Naigang Wang,Jie Chen,Yangyang Xu*

Main category: cs.LG

TL;DR: 本文设计两种压缩去中心化算法解决非凸随机优化问题，在不同场景下均有良好表现。


<details>
  <summary>Details</summary>
Motivation: 在去中心化算法中结合动量加速和压缩通信技术，证明其组合有效性并解决非凸随机优化问题。

Method: 设计压缩去中心化自适应方法（梯度有界场景）和压缩去中心化重球方法（数据异质性无界梯度场景），采用动量和消息压缩技术。

Result: 两种方法达到最优收敛率，能线性加速，在一定误差容忍度内采用与拓扑无关参数，在训练DNNs和Transformers上表现优于现有方法。

Conclusion: 所设计的两种压缩去中心化算法在解决非凸随机优化问题上有效且性能优越。

Abstract: In this paper, we design two compressed decentralized algorithms for solving
nonconvex stochastic optimization under two different scenarios. Both
algorithms adopt a momentum technique to achieve fast convergence and a
message-compression technique to save communication costs. Though momentum
acceleration and compressed communication have been used in literature, it is
highly nontrivial to theoretically prove the effectiveness of their composition
in a decentralized algorithm that can maintain the benefits of both sides,
because of the need to simultaneously control the consensus error, the
compression error, and the bias from the momentum gradient.
  For the scenario where gradients are bounded, our proposal is a compressed
decentralized adaptive method. To the best of our knowledge, this is the first
decentralized adaptive stochastic gradient method with compressed
communication. For the scenario of data heterogeneity without bounded
gradients, our proposal is a compressed decentralized heavy-ball method, which
applies a gradient tracking technique to address the challenge of data
heterogeneity. Notably, both methods achieve an optimal convergence rate, and
they can achieve linear speed up and adopt topology-independent algorithmic
parameters within a certain regime of the user-specified error tolerance.
Superior empirical performance is observed over state-of-the-art methods on
training deep neural networks (DNNs) and Transformers.

</details>


### [115] [MENDR: Manifold Explainable Neural Data Representations](https://arxiv.org/abs/2508.04956)
*Matthew Chen,Micky Nnamdi,Justin Shao,Andrew Hornback,Hongyun Huang,Ben Tamo,Yishan Zhong,Benoit Marteau,Wenqi Shi,May Dongmei Wang*

Main category: cs.LG

TL;DR: 提出MENDR解决现有脑电基础模型问题，学习对称正定矩阵嵌入，在多任务评估中表现好。


<details>
  <summary>Details</summary>
Motivation: 现有脑电基础模型缺乏预训练透明度和嵌入信息保留洞察，且多在时域操作，忽略数字信号处理进展。

Method: 提出基于滤波器组的MENDR，采用黎曼流形Transformer架构，在超4000小时脑电数据上预训练，用离散小波包变换分解。

Result: MENDR增强可解释性，支持脑电信号准确重建，在多临床脑电任务评估中接近最先进性能且参数少。

Conclusion: MENDR有潜力用于高效、可解释和临床适用的脑电分析。

Abstract: Foundation models for electroencephalography (EEG) signals have recently
demonstrated success in learning generalized representations of EEGs,
outperforming specialized models in various downstream tasks. However, many of
these models lack transparency in their pretraining dynamics and offer limited
insight into how well EEG information is preserved within their embeddings. For
successful clinical integration, EEG foundation models must ensure transparency
in pretraining, downstream fine-tuning, and the interpretability of learned
representations. Current approaches primarily operate in the temporal domain,
overlooking advancements in digital signal processing that enable the
extraction of deterministic and traceable features, such as wavelet-based
representations. We propose MENDR (Manifold Explainable Neural Data
Representations), a filter bank-based EEG foundation model built on a novel
Riemannian Manifold Transformer architecture to resolve these issues. MENDR
learns symmetric positive definite matrix embeddings of EEG signals and is
pretrained on a large corpus comprising over 4,000 hours of EEG data,
decomposed via discrete wavelet packet transforms into multi-resolution
coefficients. MENDR significantly enhances interpretability by visualizing
symmetric positive definite embeddings as geometric ellipsoids and supports
accurate reconstruction of EEG signals from learned embeddings. Evaluations
across multiple clinical EEG tasks demonstrate that MENDR achieves near
state-of-the-art performance with substantially fewer parameters, underscoring
its potential for efficient, interpretable, and clinically applicable EEG
analysis.

</details>


### [116] [Disentangling Bias by Modeling Intra- and Inter-modal Causal Attention for Multimodal Sentiment Analysis](https://arxiv.org/abs/2508.04999)
*Menghua Jiang,Yuxia Lin,Baoliang Chen,Haifeng Hu,Yuncheng Jiang,Sijie Mai*

Main category: cs.LG

TL;DR: 提出MMCI模型解决多模态情感分析中虚假关联问题，实验证明能抑制偏差、提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态情感分析方法存在模态内和跨模态的虚假关联，依赖统计捷径而非因果关系，影响泛化能力。

Method: 将多模态输入建模为多关系图，用注意力机制估计和解耦因果特征与捷径特征，通过后门调整分层捷径特征并与因果特征动态结合。

Result: 在多个标准MSA数据集和OOD测试集上的实验表明，该方法有效抑制偏差、提升性能。

Conclusion: 所提出的MMCI模型能有效解决多模态情感分析中的虚假关联问题，提高模型泛化能力。

Abstract: Multimodal sentiment analysis (MSA) aims to understand human emotions by
integrating information from multiple modalities, such as text, audio, and
visual data. However, existing methods often suffer from spurious correlations
both within and across modalities, leading models to rely on statistical
shortcuts rather than true causal relationships, thereby undermining
generalization. To mitigate this issue, we propose a Multi-relational
Multimodal Causal Intervention (MMCI) model, which leverages the backdoor
adjustment from causal theory to address the confounding effects of such
shortcuts. Specifically, we first model the multimodal inputs as a
multi-relational graph to explicitly capture intra- and inter-modal
dependencies. Then, we apply an attention mechanism to separately estimate and
disentangle the causal features and shortcut features corresponding to these
intra- and inter-modal relations. Finally, by applying the backdoor adjustment,
we stratify the shortcut features and dynamically combine them with the causal
features to encourage MMCI to produce stable predictions under distribution
shifts. Extensive experiments on several standard MSA datasets and
out-of-distribution (OOD) test sets demonstrate that our method effectively
suppresses biases and improves performance.

</details>


### [117] [R-Zero: Self-Evolving Reasoning LLM from Zero Data](https://arxiv.org/abs/2508.05004)
*Chengsong Huang,Wenhao Yu,Xiaoyang Wang,Hongming Zhang,Zongxia Li,Ruosen Li,Jiaxin Huang,Haitao Mi,Dong Yu*

Main category: cs.LG

TL;DR: 提出R - Zero框架让大模型自主生成训练数据，提升推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有自进化大模型训练方法依赖大量人工任务和标签，限制了AI超越人类智能，需解决该瓶颈。

Method: 从单个基础大模型初始化挑战者和求解器两个独立模型，二者分别优化并通过交互共同进化，挑战者因提出接近求解器能力边界的任务获奖励，求解器因解决更具挑战性任务获奖励。

Result: R - Zero显著提升不同骨干大模型的推理能力，如Qwen3 - 4B - Base在数学推理和通用领域推理基准测试中有明显提升。

Conclusion: R - Zero框架能在无预定义任务和标签情况下实现大模型自我提升，有效提升推理能力。

Abstract: Self-evolving Large Language Models (LLMs) offer a scalable path toward
super-intelligence by autonomously generating, refining, and learning from
their own experiences. However, existing methods for training such models still
rely heavily on vast human-curated tasks and labels, typically via fine-tuning
or reinforcement learning, which poses a fundamental bottleneck to advancing AI
systems toward capabilities beyond human intelligence. To overcome this
limitation, we introduce R-Zero, a fully autonomous framework that generates
its own training data from scratch. Starting from a single base LLM, R-Zero
initializes two independent models with distinct roles, a Challenger and a
Solver. These models are optimized separately and co-evolve through
interaction: the Challenger is rewarded for proposing tasks near the edge of
the Solver capability, and the Solver is rewarded for solving increasingly
challenging tasks posed by the Challenger. This process yields a targeted,
self-improving curriculum without any pre-existing tasks and labels.
Empirically, R-Zero substantially improves reasoning capability across
different backbone LLMs, e.g., boosting the Qwen3-4B-Base by +6.49 on
math-reasoning benchmarks and +7.54 on general-domain reasoning benchmarks.

</details>


### [118] [SPaRFT: Self-Paced Reinforcement Fine-Tuning for Large Language Models](https://arxiv.org/abs/2508.05015)
*Dai Do,Manh Nguyen,Svetha Venkatesh,Hung Le*

Main category: cs.LG

TL;DR: 提出SPaRFT自定步调学习框架，可基于模型能力高效学习，实验显示能以更少样本达可比或更好准确率，证明精心设计的训练课程可让大语言模型用最少资源获得强推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型用强化学习微调需大量数据和计算资源，课程学习或数据选择方法有启发式或资源需求大的问题，缺乏可扩展性和泛化性。

Method: 提出SPaRFT框架，先进行基于聚类的数据约简，按语义和难度划分训练数据并提取子集；再用多臂老虎机根据模型当前性能分配训练样本。

Result: 在多个推理基准测试中，SPaRFT用少至100倍的样本达到与现有最优基线相当或更好的准确率，消融实验和分析凸显数据聚类和自适应选择的重要性。

Conclusion: 精心设计、基于性能的训练课程能让大语言模型用最少资源获得强推理能力。

Abstract: Large language models (LLMs) have shown strong reasoning capabilities when
fine-tuned with reinforcement learning (RL). However, such methods require
extensive data and compute, making them impractical for smaller models. Current
approaches to curriculum learning or data selection are largely
heuristic-driven or demand extensive computational resources, limiting their
scalability and generalizability. We propose \textbf{SPaRFT}, a self-paced
learning framework that enables efficient learning based on the capability of
the model being trained through optimizing which data to use and when. First,
we apply \emph{cluster-based data reduction} to partition training data by
semantics and difficulty, extracting a compact yet diverse subset that reduces
redundancy. Then, a \emph{multi-armed bandit} treats data clusters as arms,
optimized to allocate training samples based on model current performance.
Experiments across multiple reasoning benchmarks show that SPaRFT achieves
comparable or better accuracy than state-of-the-art baselines while using up to
\(100\times\) fewer samples. Ablation studies and analyses further highlight
the importance of both data clustering and adaptive selection. Our results
demonstrate that carefully curated, performance-driven training curricula can
unlock strong reasoning abilities in LLMs with minimal resources.

</details>


### [119] [Will You Be Aware? Eye Tracking-Based Modeling of Situational Awareness in Augmented Reality](https://arxiv.org/abs/2508.05025)
*Zhehan Qu,Tianyi Hu,Christian Fronk,Maria Gorlatova*

Main category: cs.LG

TL;DR: 研究AR引导心肺复苏中的情境意识，开发AR应用并开展用户研究，用眼动追踪分析，提出FixGraphPool模型预测情境意识，准确率达83.0%，展示眼动追踪在AR情境意识建模潜力。


<details>
  <summary>Details</summary>
Motivation: AR系统在安全关键场景中可能导致认知隧道效应，损害情境意识，研究AR引导心肺复苏中的情境意识。

Method: 开发Magic Leap 2上的AR应用，开展模拟意外事件的用户研究，通过观察和问卷收集情境意识指标，进行眼动追踪分析，提出FixGraphPool图神经网络模型。

Result: 眼动追踪分析发现高情境意识与更大的眼跳幅度和速度、更少的虚拟内容注视比例和频率有关；FixGraphPool模型准确率83.0%，F1值81.0%，优于基于特征的机器学习和最先进的时间序列模型。

Conclusion: 眼动追踪在AR情境意识建模中有潜力，对设计确保用户安全和情境意识的AR系统有帮助。

Abstract: Augmented Reality (AR) systems, while enhancing task performance through
real-time guidance, pose risks of inducing cognitive tunneling-a hyperfocus on
virtual content that compromises situational awareness (SA) in safety-critical
scenarios. This paper investigates SA in AR-guided cardiopulmonary
resuscitation (CPR), where responders must balance effective compressions with
vigilance to unpredictable hazards (e.g., patient vomiting). We developed an AR
app on a Magic Leap 2 that overlays real-time CPR feedback (compression depth
and rate) and conducted a user study with simulated unexpected incidents (e.g.,
bleeding) to evaluate SA, in which SA metrics were collected via observation
and questionnaires administered during freeze-probe events. Eye tracking
analysis revealed that higher SA levels were associated with greater saccadic
amplitude and velocity, and with reduced proportion and frequency of fixations
on virtual content. To predict SA, we propose FixGraphPool, a graph neural
network that structures gaze events (fixations, saccades) into spatiotemporal
graphs, effectively capturing dynamic attentional patterns. Our model achieved
83.0% accuracy (F1=81.0%), outperforming feature-based machine learning and
state-of-the-art time-series models by leveraging domain knowledge and
spatial-temporal information encoded in ET data. These findings demonstrate the
potential of eye tracking for SA modeling in AR and highlight its utility in
designing AR systems that ensure user safety and situational awareness.

</details>


### [120] [Learning from Oblivion: Predicting Knowledge Overflowed Weights via Retrodiction of Forgetting](https://arxiv.org/abs/2508.05059)
*Jinhyeok Jang,Jaehong Kim,Jung Uk Kim*

Main category: cs.LG

TL;DR: 本文提出KNOW预测策略，利用结构化遗忘及其反转合成富含知识的权重，实验表明其优于简单微调与权重预测，为深度学习知识迁移提供新视角。


<details>
  <summary>Details</summary>
Motivation: 解决如何获得能封装更多知识的预训练权重的问题，特别是超越给定数据集的知识。

Method: 引入KNOW预测策略，通过在逐步缩小的数据集上顺序微调诱导结构化遗忘过程，对其建模并反转以恢复知识，构建权重转换数据集，采用元学习建模权重预测，使用KNOWN超模型学习权重的一般演变。

Result: 在不同数据集和架构上的大量实验表明，KNOW预测始终优于简单微调与简单权重预测，带来更好的下游任务性能。

Conclusion: 为重新解释遗忘动态以推动深度学习知识迁移的极限提供了新视角。

Abstract: Pre-trained weights have become a cornerstone of modern deep learning,
enabling efficient knowledge transfer and improving downstream task
performance, especially in data-scarce scenarios. However, a fundamental
question remains: how can we obtain better pre-trained weights that encapsulate
more knowledge beyond the given dataset? In this work, we introduce
\textbf{KNowledge Overflowed Weights (KNOW)} prediction, a novel strategy that
leverages structured forgetting and its inversion to synthesize
knowledge-enriched weights. Our key insight is that sequential fine-tuning on
progressively downsized datasets induces a structured forgetting process, which
can be modeled and reversed to recover knowledge as if trained on a larger
dataset. We construct a dataset of weight transitions governed by this
controlled forgetting and employ meta-learning to model weight prediction
effectively. Specifically, our \textbf{KNowledge Overflowed Weights Nowcaster
(KNOWN)} acts as a hyper-model that learns the general evolution of weights and
predicts enhanced weights with improved generalization. Extensive experiments
across diverse datasets and architectures demonstrate that KNOW prediction
consistently outperforms Na\"ive fine-tuning and simple weight prediction,
leading to superior downstream performance. Our work provides a new perspective
on reinterpreting forgetting dynamics to push the limits of knowledge transfer
in deep learning.

</details>


### [121] [TANGO: Graph Neural Dynamics via Learned Energy and Tangential Flows](https://arxiv.org/abs/2508.05070)
*Moshe Eliasof,Eldad Haber,Carola-Bibiane Schönlieb*

Main category: cs.LG

TL;DR: 介绍TANGO框架用于图表示学习，结合能量梯度下降和切向演化，在多个基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决图学习中常见的能量区域问题、缓解过压缩，提升图神经网络性能。

Method: 引入基于可学习Lyapunov函数的能量景观和梯度下降动力学，结合通过消息传递学习的切向分量。

Result: TANGO在多种节点和图的分类及回归基准测试中取得了良好表现。

Conclusion: 联合学习的能量函数和切向流对图神经网络有效。

Abstract: We introduce TANGO -- a dynamical systems inspired framework for graph
representation learning that governs node feature evolution through a learned
energy landscape and its associated descent dynamics. At the core of our
approach is a learnable Lyapunov function over node embeddings, whose gradient
defines an energy-reducing direction that guarantees convergence and stability.
To enhance flexibility while preserving the benefits of energy-based dynamics,
we incorporate a novel tangential component, learned via message passing, that
evolves features while maintaining the energy value. This decomposition into
orthogonal flows of energy gradient descent and tangential evolution yields a
flexible form of graph dynamics, and enables effective signal propagation even
in flat or ill-conditioned energy regions, that often appear in graph learning.
Our method mitigates oversquashing and is compatible with different graph
neural network backbones. Empirically, TANGO achieves strong performance across
a diverse set of node and graph classification and regression benchmarks,
demonstrating the effectiveness of jointly learned energy functions and
tangential flows for graph neural networks.

</details>


### [122] [ULU: A Unified Activation Function](https://arxiv.org/abs/2508.05073)
*Simin Huo*

Main category: cs.LG

TL;DR: 提出新型激活函数ULU及其变体AULU，实验表明ULU性能优于ReLU和Mish，还引入LIB指标衡量模型归纳偏置。


<details>
  <summary>Details</summary>
Motivation: 提出性能更优的激活函数并对模型归纳偏置进行定量衡量。

Method: 提出ULU和AULU激活函数，开展图像分类和目标检测实验，引入LIB指标。

Result: ULU在图像分类和目标检测任务中显著优于ReLU和Mish。

Conclusion: ULU和AULU是有效的激活函数，LIB指标可用于定量衡量模型归纳偏置。

Abstract: We propose \textbf{ULU}, a novel non-monotonic, piecewise activation function
defined as $\{f(x;\alpha_1),x<0; f(x;\alpha_2),x>=0 \}$, where
$f(x;\alpha)=0.5x(tanh(\alpha x)+1),\alpha >0$. ULU treats positive and
negative inputs differently. Extensive experiments demonstrate ULU
significantly outperforms ReLU and Mish across image classification and object
detection tasks. Its variant Adaptive ULU (\textbf{AULU}) is expressed as
$\{f(x;\beta_1^2),x<0; f(x;\beta_2^2),x>=0 \}$, where $\beta_1$ and $\beta_2$
are learnable parameters, enabling it to adapt its response separately for
positive and negative inputs. Additionally, we introduce the LIB (Like
Inductive Bias) metric from AULU to quantitatively measure the inductive bias
of the model.

</details>


### [123] [Analyzing the Impact of Multimodal Perception on Sample Complexity and Optimization Landscapes in Imitation Learning](https://arxiv.org/abs/2508.05077)
*Luai Abuelsamen,Temitope Lukman Adebanjo*

Main category: cs.LG

TL;DR: 本文从统计学习理论视角研究多模态模仿学习理论基础，分析多模态感知影响，表明多模态策略优势并回顾相关理论框架。


<details>
  <summary>Details</summary>
Motivation: 探究多模态模仿学习的理论基础，解释多模态架构性能优越的原因。

Method: 从统计学习理论视角分析多模态感知对模仿策略样本复杂度和优化格局的影响，结合多模态学习理论新进展进行研究。

Result: 适当集成的多模态策略比单模态策略有更紧的泛化边界和更有利的优化格局。

Conclusion: 通过回顾理论框架，将多模态架构的实证结果与拉德马赫复杂度、PAC学习和信息论等基本概念相联系。

Abstract: This paper examines the theoretical foundations of multimodal imitation
learning through the lens of statistical learning theory. We analyze how
multimodal perception (RGB-D, proprioception, language) affects sample
complexity and optimization landscapes in imitation policies. Building on
recent advances in multimodal learning theory, we show that properly integrated
multimodal policies can achieve tighter generalization bounds and more
favorable optimization landscapes than their unimodal counterparts. We provide
a comprehensive review of theoretical frameworks that explain why multimodal
architectures like PerAct and CLIPort achieve superior performance, connecting
these empirical results to fundamental concepts in Rademacher complexity, PAC
learning, and information theory.

</details>


### [124] [Integrated Influence: Data Attribution with Baseline](https://arxiv.org/abs/2508.05089)
*Linxiao Yang,Xinyu Gu,Liang Sun*

Main category: cs.LG

TL;DR: 提出一种名为Integrated Influence的新型数据归因方法，该方法结合基线方法，在数据归因和错误标注示例识别任务中比现有方法更可靠。


<details>
  <summary>Details</summary>
Motivation: 现有基于留一法（LOO）的数据归因方法存在局部解释问题，且许多方法缺乏基线，降低了解释的灵活性。

Method: 定义基线数据集，遵循数据退化过程将当前数据集过渡到基线，并累积每个样本在该过程中的影响。还为该方法提供了坚实的理论框架。

Result: 在数据归因任务和错误标注示例识别任务中，Integrated Influence比现有方法生成更可靠的数据归因。

Conclusion: Integrated Influence是一种有效的数据归因方法，优于现有方法。

Abstract: As an effective approach to quantify how training samples influence test
sample, data attribution is crucial for understanding data and model and
further enhance the transparency of machine learning models. We find that
prevailing data attribution methods based on leave-one-out (LOO) strategy
suffer from the local-based explanation, as these LOO-based methods only
perturb a single training sample, and overlook the collective influence in the
training set. On the other hand, the lack of baseline in many data attribution
methods reduces the flexibility of the explanation, e.g., failing to provide
counterfactual explanations. In this paper, we propose Integrated Influence, a
novel data attribution method that incorporates a baseline approach. Our method
defines a baseline dataset, follows a data degeneration process to transition
the current dataset to the baseline, and accumulates the influence of each
sample throughout this process. We provide a solid theoretical framework for
our method, and further demonstrate that popular methods, such as influence
functions, can be viewed as special cases of our approach. Experimental results
show that Integrated Influence generates more reliable data attributions
compared to existing methods in both data attribution task and mislablled
example identification task.

</details>


### [125] [Cold Start Active Preference Learning in Socio-Economic Domains](https://arxiv.org/abs/2508.05090)
*Mojtaba Fayaz-Bakhsh,Danial Ataee,MohammadAmin Fazli*

Main category: cs.LG

TL;DR: 提出冷启动主动偏好学习框架，通过自监督预训练和主动学习循环，实验表明该方法优于标准主动学习策略，提高了样本效率和适用性。


<details>
  <summary>Details</summary>
Motivation: 主动偏好学习存在冷启动问题，在计算社会系统和经济分析中因标注数据稀缺、昂贵且有专家噪声，性能显著下降，需解决该问题。

Method: 先通过自监督预训练阶段，利用主成分分析（PCA）从数据固有结构中获取初始伪标签创建冷启动模型，再通过主动学习循环向模拟的有噪声的神谕查询标签来优化模型。

Result: 在不同领域的多个数据集上实验，冷启动方法优于从空白状态开始的标准主动学习策略，用更少的标注对实现更高的准确性。

Conclusion: 该框架为缓解冷启动问题提供了实用有效的解决方案，提高了数据受限环境下偏好学习的样本效率和适用性。

Abstract: Active preference learning is a powerful paradigm for efficiently modeling
preferences, yet it suffers from the cold-start problem: a significant drop in
performance when no initial labeled data is available. This challenge is
particularly acute in computational social systems and economic analysis, where
labeled data is often scarce, expensive, and subject to expert noise. To
address this gap, we propose a novel framework for cold-start active preference
learning. Our method initiates the learning process through a self-supervised
pre-training phase, utilizing Principal Component Analysis (PCA) to derive
initial pseudo-labels from the data's inherent structure, thereby creating a
cold-start model without any initial oracle interaction. Subsequently, the
model is refined through an active learning loop that strategically queries a
simulated noisy oracle for labels. We conduct extensive experiments on diverse
datasets from different domains, including financial credibility, career
success rate, and socio-economic status. The results demonstrate that our
cold-start approach outperforms standard active learning strategies that begin
from a blank slate, achieving higher accuracy with substantially fewer labeled
pairs. Our framework offers a practical and effective solution to mitigate the
cold-start problem, enhancing the sample efficiency and applicability of
preference learning in data-constrained environments. We release our code at
https://github.com/Dan-A2/cold-start-preference-learning

</details>


### [126] [Learning from Similarity-Confidence and Confidence-Difference](https://arxiv.org/abs/2508.05108)
*Tomoya Tate,Kosuke Sugiyama,Masato Uchida*

Main category: cs.LG

TL;DR: 本文提出一种利用多关系视角互补弱监督信号的弱监督学习框架，引入SconfConfDiff Classification方法，推导无偏风险估计器，提出风险校正方法，实验显示其优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 实际机器学习应用中准确标注数据有挑战，现有弱监督学习方法多利用单一类型弱监督，本文旨在利用多关系视角互补弱监督信号。

Method: 引入SconfConfDiff Classification方法，集成两种弱标签；推导两种无偏风险估计器；提出风险校正方法。

Result: 两种无偏风险估计器达到估计误差界的最优收敛率，实验表明该方法在多种设置下优于现有基线。

Conclusion: 所提出的利用多关系视角互补弱监督信号的方法有效，能在标注数据有限时发挥作用。

Abstract: In practical machine learning applications, it is often challenging to assign
accurate labels to data, and increasing the number of labeled instances is
often limited. In such cases, Weakly Supervised Learning (WSL), which enables
training with incomplete or imprecise supervision, provides a practical and
effective solution. However, most existing WSL methods focus on leveraging a
single type of weak supervision. In this paper, we propose a novel WSL
framework that leverages complementary weak supervision signals from multiple
relational perspectives, which can be especially valuable when labeled data is
limited. Specifically, we introduce SconfConfDiff Classification, a method that
integrates two distinct forms of weaklabels: similarity-confidence and
confidence-difference, which are assigned to unlabeled data pairs. To implement
this method, we derive two types of unbiased risk estimators for
classification: one based on a convex combination of existing estimators, and
another newly designed by modeling the interaction between two weak labels. We
prove that both estimators achieve optimal convergence rates with respect to
estimation error bounds. Furthermore, we introduce a risk correction approach
to mitigate overfitting caused by negative empirical risk, and provide
theoretical analysis on the robustness of the proposed method against
inaccurate class prior probability and label noise. Experimental results
demonstrate that the proposed method consistently outperforms existing
baselines across a variety of settings.

</details>


### [127] [Exploring Superior Function Calls via Reinforcement Learning](https://arxiv.org/abs/2508.05118)
*Bingguang Hao,Maolin Wang,Zengzhuang Xu,Yicheng Chen,Cunyin Peng,Jinjie GU,Chenyi Zhuang*

Main category: cs.LG

TL;DR: 现有大语言模型函数调用训练方法有缺陷，本文提出新强化学习框架，经实验表现优异并将开源成果。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型函数调用训练方法无法形成鲁棒推理策略，需改进。

Method: 提出新强化学习框架，通过基于策略熵的探索增强组相对策略优化，采用两阶段数据准备管道。

Result: 在伯克利函数调用排行榜实验中，该框架在开源模型中达最优，整体准确率86.02%，复杂场景超标准GRPO最多6%。

Conclusion: 新框架有效，代码预训练模型在函数调用强化学习任务中有优势，将开源成果。

Abstract: Function calling capabilities are crucial for deploying Large Language Models
in real-world applications, yet current training approaches fail to develop
robust reasoning strategies. Supervised fine-tuning produces models that rely
on superficial pattern matching, while standard reinforcement learning methods
struggle with the complex action space of structured function calls. We present
a novel reinforcement learning framework designed to enhance group relative
policy optimization through strategic entropy based exploration specifically
tailored for function calling tasks. Our approach addresses three critical
challenges in function calling: insufficient exploration during policy
learning, lack of structured reasoning in chain-of-thought generation, and
inadequate verification of parameter extraction. Our two-stage data preparation
pipeline ensures high-quality training samples through iterative LLM evaluation
and abstract syntax tree validation. Extensive experiments on the Berkeley
Function Calling Leaderboard demonstrate that this framework achieves
state-of-the-art performance among open-source models with 86.02\% overall
accuracy, outperforming standard GRPO by up to 6\% on complex multi-function
scenarios. Notably, our method shows particularly strong improvements on
code-pretrained models, suggesting that structured language generation
capabilities provide an advantageous starting point for reinforcement learning
in function calling tasks. We will release all the code, models and dataset to
benefit the community.

</details>


### [128] [Deep Neural Networks with General Activations: Super-Convergence in Sobolev Norms](https://arxiv.org/abs/2508.05141)
*Yahong Yang,Juncai He*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This paper establishes a comprehensive approximation result for deep
fully-connected neural networks with commonly-used and general activation
functions in Sobolev spaces $W^{n,\infty}$, with errors measured in the
$W^{m,p}$-norm for $m < n$ and $1\le p \le \infty$. The derived rates surpass
those of classical numerical approximation techniques, such as finite element
and spectral methods, exhibiting a phenomenon we refer to as
\emph{super-convergence}. Our analysis shows that deep networks with general
activations can approximate weak solutions of partial differential equations
(PDEs) with superior accuracy compared to traditional numerical methods at the
approximation level. Furthermore, this work closes a significant gap in the
error-estimation theory for neural-network-based approaches to PDEs, offering a
unified theoretical foundation for their use in scientific computing.

</details>


### [129] [PSEO: Optimizing Post-hoc Stacking Ensemble Through Hyperparameter Tuning](https://arxiv.org/abs/2508.05144)
*Beicheng Xu,Wei Liu,Keyao Ding,Yupeng Lu,Bin Cui*

Main category: cs.LG

TL;DR: 提出PSEO框架解决CASH问题中集成阶段固定策略不适应任务特性的问题，在80个公开数据集上表现良好。


<details>
  <summary>Details</summary>
Motivation: 多数CASH方法在集成阶段采用固定策略，无法适应特定任务特性。

Method: 提出PSEO框架，通过二元二次规划进行基础模型选择，引入两种机制挖掘多层堆叠潜力，构建超参数空间搜索最优集成策略。

Result: 在80个公开数据集上，PSEO在16种方法中取得最佳平均测试排名（2.96）。

Conclusion: PSEO框架有效解决了CASH问题中集成阶段的策略适应性问题。

Abstract: The Combined Algorithm Selection and Hyperparameter Optimization (CASH)
problem is fundamental in Automated Machine Learning (AutoML). Inspired by the
success of ensemble learning, recent AutoML systems construct post-hoc
ensembles for final predictions rather than relying on the best single model.
However, while most CASH methods conduct extensive searches for the optimal
single model, they typically employ fixed strategies during the ensemble phase
that fail to adapt to specific task characteristics. To tackle this issue, we
propose PSEO, a framework for post-hoc stacking ensemble optimization. First,
we conduct base model selection through binary quadratic programming, with a
trade-off between diversity and performance. Furthermore, we introduce two
mechanisms to fully realize the potential of multi-layer stacking. Finally,
PSEO builds a hyperparameter space and searches for the optimal post-hoc
ensemble strategy within it. Empirical results on 80 public datasets show that
\sys achieves the best average test rank (2.96) among 16 methods, including
post-hoc designs in recent AutoML systems and state-of-the-art ensemble
learning methods.

</details>


### [130] [Domain-driven Metrics for Reinforcement Learning: A Case Study on Epidemic Control using Agent-based Simulation](https://arxiv.org/abs/2508.05154)
*Rishabh Gaur,Gaurav Deshkar,Jayanta Kshirsagar,Harshal Hayatnagarkar,Janani Venugopalan*

Main category: cs.LG

TL;DR: 本文针对基于强化学习的ABMs和RABMs模型性能评估难题，开发领域驱动的RL指标，并通过案例展示其应用效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的ABMs和RABMs模型因系统复杂、随机及缺乏标准化指标，难以评估性能。

Method: 在现有先进指标基础上，开发领域驱动的RL指标，并在理性ABM疾病建模案例中进行策略优化。

Result: 在不同模拟场景（如口罩供应差异）中，结合传统和先进指标使用领域驱动奖励进行展示。

Conclusion: 开发的领域驱动RL指标在案例研究中有应用价值，可用于评估相关模型性能。

Abstract: For the development and optimization of agent-based models (ABMs) and
rational agent-based models (RABMs), optimization algorithms such as
reinforcement learning are extensively used. However, assessing the performance
of RL-based ABMs and RABMS models is challenging due to the complexity and
stochasticity of the modeled systems, and the lack of well-standardized metrics
for comparing RL algorithms. In this study, we are developing domain-driven
metrics for RL, while building on state-of-the-art metrics. We demonstrate our
``Domain-driven-RL-metrics'' using policy optimization on a rational ABM
disease modeling case study to model masking behavior, vaccination, and
lockdown in a pandemic. Our results show the use of domain-driven rewards in
conjunction with traditional and state-of-the-art metrics for a few different
simulation scenarios such as the differential availability of masks.

</details>


### [131] [pFedDSH: Enabling Knowledge Transfer in Personalized Federated Learning through Data-free Sub-Hypernetwork](https://arxiv.org/abs/2508.05157)
*Thinh Nguyen,Le Huy Khiem,Van-Tuan Tran,Khoa D Doan,Nitesh V Chawla,Kok-Seng Wong*

Main category: cs.LG

TL;DR: 本文探讨动态客户端加入的个性化联邦学习场景，提出pFedDSH框架，实验表明其优于现有方法，能实现性能稳定和资源高效利用。


<details>
  <summary>Details</summary>
Motivation: 现有个性化联邦学习方法多假设静态客户端参与，未反映新客户端持续加入的现实场景，动态环境存在诸多挑战。

Method: 提出基于中央超网络的pFedDSH框架，通过嵌入向量为每个客户端生成个性化模型，采用批特定掩码维护知识稳定，引入基于DeepInversion的数据免费重放策略促进知识转移。

Result: 在CIFAR - 10、CIFAR - 100和Tiny - ImageNet上的实验表明，pFedDSH在研究场景中优于现有个性化联邦学习和联邦持续学习基线。

Conclusion: pFedDSH能为现有客户端实现强大的性能稳定性，为新客户端实现适应性，并有效利用神经资源。

Abstract: Federated Learning (FL) enables collaborative model training across
distributed clients without sharing raw data, offering a significant privacy
benefit. However, most existing Personalized Federated Learning (pFL) methods
assume a static client participation, which does not reflect real-world
scenarios where new clients may continuously join the federated system (i.e.,
dynamic client onboarding). In this paper, we explore a practical scenario in
which a new batch of clients is introduced incrementally while the learning
task remains unchanged. This dynamic environment poses various challenges,
including preserving performance for existing clients without retraining and
enabling efficient knowledge transfer between client batches. To address these
issues, we propose Personalized Federated Data-Free Sub-Hypernetwork (pFedDSH),
a novel framework based on a central hypernetwork that generates personalized
models for each client via embedding vectors. To maintain knowledge stability
for existing clients, pFedDSH incorporates batch-specific masks, which activate
subsets of neurons to preserve knowledge. Furthermore, we introduce a data-free
replay strategy motivated by DeepInversion to facilitate backward transfer,
enhancing existing clients' performance without compromising privacy. Extensive
experiments conducted on CIFAR-10, CIFAR-100, and Tiny-ImageNet demonstrate
that pFedDSH outperforms the state-of-the-art pFL and Federated Continual
Learning baselines in our investigation scenario. Our approach achieves robust
performance stability for existing clients, as well as adaptation for new
clients and efficient utilization of neural resources.

</details>


### [132] [S$^2$M-Former: Spiking Symmetric Mixing Branchformer for Brain Auditory Attention Detection](https://arxiv.org/abs/2508.05164)
*Jiaqi Wang,Zhengyu Ma,Xiongri Shen,Chenlin Zhou,Leilei Zhao,Han Zhang,Yi Zhong,Siqi Cai,Zhenxi Song,Zhiguo Zhang*

Main category: cs.LG

TL;DR: 提出S²M - Former用于听觉注意力检测，具低功耗、高性能优势，实验证明有可比的先进解码精度。


<details>
  <summary>Details</summary>
Motivation: 现有基于EEG的听觉注意力检测缺乏能在能效约束下充分利用互补EEG特征的协同框架。

Method: 提出S²M - Former，有由平行空间和频率分支组成的尖峰驱动对称架构，用生物似然的令牌 - 通道混合器；引入轻量级1D令牌序列替代3D操作。

Result: 减少14.7倍参数，降低5.8倍能耗，在参数效率和性能上超现有SNN基线。

Conclusion: S²M - Former是听觉注意力检测任务有前景的低功耗、高性能解决方案。

Abstract: Auditory attention detection (AAD) aims to decode listeners' focus in complex
auditory environments from electroencephalography (EEG) recordings, which is
crucial for developing neuro-steered hearing devices. Despite recent
advancements, EEG-based AAD remains hindered by the absence of synergistic
frameworks that can fully leverage complementary EEG features under
energy-efficiency constraints. We propose S$^2$M-Former, a novel spiking
symmetric mixing framework to address this limitation through two key
innovations: i) Presenting a spike-driven symmetric architecture composed of
parallel spatial and frequency branches with mirrored modular design,
leveraging biologically plausible token-channel mixers to enhance complementary
learning across branches; ii) Introducing lightweight 1D token sequences to
replace conventional 3D operations, reducing parameters by 14.7$\times$. The
brain-inspired spiking architecture further reduces power consumption,
achieving a 5.8$\times$ energy reduction compared to recent ANN methods, while
also surpassing existing SNN baselines in terms of parameter efficiency and
performance. Comprehensive experiments on three AAD benchmarks (KUL, DTU and
AV-GC-AAD) across three settings (within-trial, cross-trial and cross-subject)
demonstrate that S$^2$M-Former achieves comparable state-of-the-art (SOTA)
decoding accuracy, making it a promising low-power, high-performance solution
for AAD tasks.

</details>


### [133] [Aligning LLMs on a Budget: Inference-Time Alignment with Heuristic Reward Models](https://arxiv.org/abs/2508.05165)
*Mason Nakamura,Saaduddin Mahmud,Kyle H. Wray,Hamed Zamani,Shlomo Zilberstein*

Main category: cs.LG

TL;DR: 提出HIA方法减少推理调用并保持对齐质量，在数据集上表现优于基线，低推理预算下也有效。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型对齐用户偏好方法需高成本微调或推理，且现有推理时间方法忽略质量与成本平衡。

Method: 提出HIA方法，使用轻量级提示优化器、启发式奖励模型和两阶段过滤。

Result: 在HelpSteer和ComPRed数据集上，HIA在相同推理预算下多目标任务中优于基线方法。

Conclusion: HIA在低推理预算下也有效，为可扩展、个性化大语言模型部署提供实用方案。

Abstract: Aligning LLMs with user preferences is crucial for real-world use but often
requires costly fine-tuning or expensive inference, forcing trade-offs between
alignment quality and computational cost. Existing inference-time methods
typically ignore this balance, focusing solely on the optimized policy's
performance. We propose HIA (Heuristic-Guided Inference-time Alignment), a
tuning-free, black-box-compatible approach that uses a lightweight prompt
optimizer, heuristic reward models, and two-stage filtering to reduce inference
calls while preserving alignment quality. On real-world prompt datasets,
HelpSteer and ComPRed, HIA outperforms best-of-N sampling, beam search, and
greedy search baselines in multi-objective, goal-conditioned tasks under the
same inference budget. We also find that HIA is effective under low-inference
budgets with as little as one or two response queries, offering a practical
solution for scalable, personalized LLM deployment.

</details>


### [134] [Human Activity Recognition from Smartphone Sensor Data for Clinical Trials](https://arxiv.org/abs/2508.05175)
*Stefania Russo,Rafał Klimas,Marta Płonka,Hugo Le Gall,Sven Holm,Dimitar Stanev,Florian Lipsmeier,Mattia Zanon,Lito Kriara*

Main category: cs.LG

TL;DR: 开发基于ResNet的低开销人类活动识别（HAR）模型，用健康成年人和多发性硬化症患者数据评估，模型在检测步态与非步态活动及日常活动中表现良好，对手机佩戴位置有高鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 开发能检测步态与非步态活动及日常活动的低开销HAR模型。

Method: 基于ResNet构建HAR模型，用健康成年人和多发性硬化症患者的智能手机传感器数据训练和评估，使用多个数据集。

Result: 模型在检测步态与非步态活动中准确率高，检测日常活动时比现有模型准确率更高，且在9种手机佩戴位置下表现良好。

Conclusion: 所提HAR模型能准确检测日常活动，对手机佩戴位置有高鲁棒性，有实际应用价值。

Abstract: We developed a ResNet-based human activity recognition (HAR) model with
minimal overhead to detect gait versus non-gait activities and everyday
activities (walking, running, stairs, standing, sitting, lying, sit-to-stand
transitions). The model was trained and evaluated using smartphone sensor data
from adult healthy controls (HC) and people with multiple sclerosis (PwMS) with
Expanded Disability Status Scale (EDSS) scores between 0.0-6.5. Datasets
included the GaitLab study (ISRCTN15993728), an internal Roche dataset, and
publicly available data sources (training only). Data from 34 HC and 68 PwMS
(mean [SD] EDSS: 4.7 [1.5]) were included in the evaluation. The HAR model
showed 98.4% and 99.6% accuracy in detecting gait versus non-gait activities in
the GaitLab and Roche datasets, respectively, similar to a comparative
state-of-the-art ResNet model (99.3% and 99.4%). For everyday activities, the
proposed model not only demonstrated higher accuracy than the state-of-the-art
model (96.2% vs 91.9%; internal Roche dataset) but also maintained high
performance across 9 smartphone wear locations (handbag, shopping bag,
crossbody bag, backpack, hoodie pocket, coat/jacket pocket, hand, neck, belt),
outperforming the state-of-the-art model by 2.8% - 9.0%. In conclusion, the
proposed HAR model accurately detects everyday activities and shows high
robustness to various smartphone wear locations, demonstrating its practical
applicability.

</details>


### [135] [Physics-Informed Time-Integrated DeepONet: Temporal Tangent Space Operator Learning for High-Accuracy Inference](https://arxiv.org/abs/2508.05190)
*Luis Mandl,Dibyajyoti Nayak,Tim Ricken,Somdatta Goswami*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Accurately modeling and inferring solutions to time-dependent partial
differential equations (PDEs) over extended horizons remains a core challenge
in scientific machine learning. Traditional full rollout (FR) methods, which
predict entire trajectories in one pass, often fail to capture the causal
dependencies and generalize poorly outside the training time horizon.
Autoregressive (AR) approaches, evolving the system step by step, suffer from
error accumulation, limiting long-term accuracy. These shortcomings limit the
long-term accuracy and reliability of both strategies. To address these issues,
we introduce the Physics-Informed Time-Integrated Deep Operator Network
(PITI-DeepONet), a dual-output architecture trained via fully physics-informed
or hybrid physics- and data-driven objectives to ensure stable, accurate
long-term evolution well beyond the training horizon. Instead of forecasting
future states, the network learns the time-derivative operator from the current
state, integrating it using classical time-stepping schemes to advance the
solution in time. Additionally, the framework can leverage residual monitoring
during inference to estimate prediction quality and detect when the system
transitions outside the training domain. Applied to benchmark problems,
PITI-DeepONet shows improved accuracy over extended inference time horizons
when compared to traditional methods. Mean relative $\mathcal{L}_2$ errors
reduced by 84% (vs. FR) and 79% (vs. AR) for the one-dimensional heat equation;
by 87% (vs. FR) and 98% (vs. AR) for the one-dimensional Burgers equation; and
by 42% (vs. FR) and 89% (vs. AR) for the two-dimensional Allen-Cahn equation.
By moving beyond classic FR and AR schemes, PITI-DeepONet paves the way for
more reliable, long-term integration of complex, time-dependent PDEs.

</details>


### [136] [FAITH: A Framework for Assessing Intrinsic Tabular Hallucinations in finance](https://arxiv.org/abs/2508.05201)
*Mengao Zhang,Jiayu Fu,Tanya Warrier,Yuwen Wang,Tianhui Tan,Ke-wei Huang*

Main category: cs.LG

TL;DR: 本文针对金融大语言模型幻觉问题，开发评估框架，有数据集创建范式、新评估数据集及对模型幻觉模式评估等贡献，助力构建可靠金融生成式AI系统。


<details>
  <summary>Details</summary>
Motivation: 幻觉是金融大语言模型部署的关键挑战，现有幻觉基准难以满足金融应用对上下文相关、数值和专有表格数据的需求。

Method: 将评估金融大语言模型内在幻觉概念化为对真实金融文档的上下文感知掩码跨度预测任务，采用掩码策略创建数据集。

Result: 提出新的自动化数据集创建范式，得到来自标准普尔500年度报告的新幻觉评估数据集，对金融表格数据上的先进大语言模型的内在幻觉模式进行全面评估。

Conclusion: 为内部大语言模型评估提供了可靠方法，是构建更可信、可靠的金融生成式AI系统的关键一步。

Abstract: Hallucination remains a critical challenge for deploying Large Language
Models (LLMs) in finance. Accurate extraction and precise calculation from
tabular data are essential for reliable financial analysis, since even minor
numerical errors can undermine decision-making and regulatory compliance.
Financial applications have unique requirements, often relying on
context-dependent, numerical, and proprietary tabular data that existing
hallucination benchmarks rarely capture. In this study, we develop a rigorous
and scalable framework for evaluating intrinsic hallucinations in financial
LLMs, conceptualized as a context-aware masked span prediction task over
real-world financial documents. Our main contributions are: (1) a novel,
automated dataset creation paradigm using a masking strategy; (2) a new
hallucination evaluation dataset derived from S&P 500 annual reports; and (3) a
comprehensive evaluation of intrinsic hallucination patterns in
state-of-the-art LLMs on financial tabular data. Our work provides a robust
methodology for in-house LLM evaluation and serves as a critical step toward
building more trustworthy and reliable financial Generative AI systems.

</details>


### [137] [Advanced Hybrid Transformer LSTM Technique with Attention and TS Mixer for Drilling Rate of Penetration Prediction](https://arxiv.org/abs/2508.05210)
*Saddam Hussain Khan*

Main category: cs.LG

TL;DR: 提出新颖混合深度学习架构预测ROP，在真实数据集上表现优于基准模型，可实现可靠实时ROP预测，助力钻井优化。


<details>
  <summary>Details</summary>
Motivation: 传统模型难以捕捉钻井数据复杂关系，无法准确预测ROP，实时实用性有限。

Method: 提出集成LSTM网络、Transformer编码器、TS - Mixer块和注意力机制的混合深度学习架构。

Result: 在真实钻井数据集上，模型R平方得分0.9988，平均绝对百分比误差1.447%，优于基准模型，通过SHAP和LIME确保可解释性。

Conclusion: 该先进混合方法可实现可靠实时ROP预测，为智能、经济的钻井优化系统奠定基础。

Abstract: The Rate of Penetration (ROP) is crucial for optimizing drilling operations;
however, accurately predicting it is hindered by the complex, dynamic, and
high-dimensional nature of drilling data. Traditional empirical, physics-based,
and basic machine learning models often fail to capture intricate temporal and
contextual relationships, resulting in suboptimal predictions and limited
real-time utility. To address this gap, we propose a novel hybrid deep learning
architecture integrating Long Short-Term Memory (LSTM) networks, Transformer
encoders, Time-Series Mixer (TS-Mixer) blocks, and attention mechanisms to
synergistically model temporal dependencies, static feature interactions,
global context, and dynamic feature importance. Evaluated on a real-world
drilling dataset, our model outperformed benchmarks (standalone LSTM, TS-Mixer,
and simpler hybrids) with an R-squared score of 0.9988 and a Mean Absolute
Percentage Error of 1.447%, as measured by standard regression metrics
(R-squared, MAE, RMSE, MAPE). Model interpretability was ensured using SHAP and
LIME, while actual vs. predicted curves and bias checks confirmed accuracy and
fairness across scenarios. This advanced hybrid approach enables reliable
real-time ROP prediction, paving the way for intelligent, cost-effective
drilling optimization systems with significant operational impact.

</details>


### [138] [DFW: A Novel Weighting Scheme for Covariate Balancing and Treatment Effect Estimation](https://arxiv.org/abs/2508.05215)
*Ahmad Saeed Khan,Erik Schaffernicht,Johannes Andreas Stork*

Main category: cs.LG

TL;DR: 本文提出新的基于倾向得分的方法DFW处理观测数据因果效应估计中的选择偏差问题，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有倾向得分加权方法处理选择偏差的效果依赖观测数据和倾向得分估计器的准确性，存在权重不稳定等问题，需要改进。

Method: 提出Deconfounding Factor Weighting (DFW)方法，利用去混杂因子构建稳定有效的样本权重，优先考虑混杂较少的样本并减轻高度混杂样本的影响。

Result: 通过对现实基准和合成数据集的大量实验，表明DFW在协变量平衡和处理效应估计方面优于现有方法，如IPW和CBPS。

Conclusion: DFW方法能有效解决现有倾向得分加权方法的局限性，可用于二元处理和多处理场景。

Abstract: Estimating causal effects from observational data is challenging due to
selection bias, which leads to imbalanced covariate distributions across
treatment groups. Propensity score-based weighting methods are widely used to
address this issue by reweighting samples to simulate a randomized controlled
trial (RCT). However, the effectiveness of these methods heavily depends on the
observed data and the accuracy of the propensity score estimator. For example,
inverse propensity weighting (IPW) assigns weights based on the inverse of the
propensity score, which can lead to instable weights when propensity scores
have high variance-either due to data or model misspecification-ultimately
degrading the ability of handling selection bias and treatment effect
estimation. To overcome these limitations, we propose Deconfounding Factor
Weighting (DFW), a novel propensity score-based approach that leverages the
deconfounding factor-to construct stable and effective sample weights. DFW
prioritizes less confounded samples while mitigating the influence of highly
confounded ones, producing a pseudopopulation that better approximates a RCT.
Our approach ensures bounded weights, lower variance, and improved covariate
balance.While DFW is formulated for binary treatments, it naturally extends to
multi-treatment settings, as the deconfounding factor is computed based on the
estimated probability of the treatment actually received by each sample.
Through extensive experiments on real-world benchmark and synthetic datasets,
we demonstrate that DFW outperforms existing methods, including IPW and CBPS,
in both covariate balancing and treatment effect estimation.

</details>


### [139] [ML-based Short Physical Performance Battery future score prediction based on questionnaire data](https://arxiv.org/abs/2508.05222)
*Marcin Kolakowski,Seif Ben Bader*

Main category: cs.LG

TL;DR: 分析基于问卷数据预测老年人四年后SPPB分数的可能性，XGBoost算法效果最佳。


<details>
  <summary>Details</summary>
Motivation: 尽早干预老年人身体机能衰退，分析基于问卷数据预测四年后SPPB分数的可能性。

Method: 测试Random Forest、XGBoost等多种机器学习算法，用Shapley值分析选择特征子集并重新训练XGBoost回归器。

Result: XGBoost初始平均绝对误差0.79分，选特征子集重新训练后平均绝对误差0.82分。

Conclusion: XGBoost算法在预测老年人四年后SPPB分数上有较好效果。

Abstract: Effective slowing down of older adults\' physical capacity deterioration
requires intervention as soon as the first symptoms surface. In this paper, we
analyze the possibility of predicting the Short Physical Performance Battery
(SPPB) score at a four-year horizon based on questionnaire data. The ML
algorithms tested included Random Forest, XGBoost, Linear Regression, dense and
TabNet neural networks. The best results were achieved for the XGBoost (mean
absolute error of 0.79 points). Based on the Shapley values analysis, we
selected smaller subsets of features (from 10 to 20) and retrained the XGBoost
regressor, achieving a mean absolute error of 0.82.

</details>


### [140] [Don't Reach for the Stars: Rethinking Topology for Resilient Federated Learning](https://arxiv.org/abs/2508.05224)
*Mirko Konstantin,Anirban Mukhopadhyay*

Main category: cs.LG

TL;DR: 文章指出传统联邦学习架构有局限，提出去中心化P2P联邦学习框架LIGHTYEAR，实验显示其性能更优。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习的集中式架构存在单点故障、个性化不足、对分布变化鲁棒性差等局限，更新选择不可靠且客户端控制少。

Method: 提出LIGHTYEAR框架，利用P2P拓扑，通过在本地验证集计算一致性得分选择更新，并加入正则化项进行聚合。

Result: 在两个数据集上的实证评估表明，该方法在客户端性能上始终优于集中式基线和现有P2P方法，尤其在对抗和异构条件下。

Conclusion: 所提出的去中心化P2P联邦学习框架有效，能提升客户端性能，在复杂条件下表现更好。

Abstract: Federated learning (FL) enables collaborative model training across
distributed clients while preserving data privacy by keeping data local.
Traditional FL approaches rely on a centralized, star-shaped topology, where a
central server aggregates model updates from clients. However, this
architecture introduces several limitations, including a single point of
failure, limited personalization, and poor robustness to distribution shifts or
vulnerability to malfunctioning clients. Moreover, update selection in
centralized FL often relies on low-level parameter differences, which can be
unreliable when client data is not independent and identically distributed, and
offer clients little control. In this work, we propose a decentralized,
peer-to-peer (P2P) FL framework. It leverages the flexibility of the P2P
topology to enable each client to identify and aggregate a personalized set of
trustworthy and beneficial updates.This framework is the Local Inference Guided
Aggregation for Heterogeneous Training Environments to Yield Enhancement
Through Agreement and Regularization (LIGHTYEAR). Central to our method is an
agreement score, computed on a local validation set, which quantifies the
semantic alignment of incoming updates in the function space with respect to
the clients reference model. Each client uses this score to select a tailored
subset of updates and performs aggregation with a regularization term that
further stabilizes the training. Our empirical evaluation across two datasets
shows that the proposed approach consistently outperforms both centralized
baselines and existing P2P methods in terms of client-level performance,
particularly under adversarial and heterogeneous conditions.

</details>


### [141] [Cross-LoRA: A Data-Free LoRA Transfer Framework across Heterogeneous LLMs](https://arxiv.org/abs/2508.05232)
*Feifan Xia,Mingyang Liao,Yuyang Fang,Defang Li,Yantong Xie,Weikang Li,Yang Li,Deguo Xia,Jizhou Huang*

Main category: cs.LG

TL;DR: 提出Cross - LoRA框架解决传统PEFT方法与基础模型架构耦合问题，可在不同模型间转移LoRA模块，实验有增益。


<details>
  <summary>Details</summary>
Motivation: 传统参数高效微调方法与基础模型架构紧密耦合，限制了其在异构预训练大语言模型中的适用性。

Method: 引入Cross - LoRA框架，包含LoRA - Align（通过SVD和线性变换进行子空间对齐）和LoRA - Shift（将对齐子空间用于投影源LoRA权重更新）两个无数据、无训练组件。

Result: 在ARCs、OBOA和HellaSwag实验中，Cross - LoRA相对基础模型最高有5.26%的增益；在其他常识推理基准测试中，性能与直接训练的LoRA适配器相当。

Conclusion: Cross - LoRA能有效解决传统PEFT方法局限性，实现不同基础模型间LoRA模块转移。

Abstract: Traditional parameter-efficient fine-tuning (PEFT) methods such as LoRA are
tightly coupled with the base model architecture, which constrains their
applicability across heterogeneous pretrained large language models (LLMs). To
address this limitation, we introduce Cross-LoRA, a data-free framework for
transferring LoRA modules between diverse base models without requiring
additional training data. Cross-LoRA consists of two key components: (a)
LoRA-Align, which performs subspace alignment between source and target base
models through rank-truncated singular value decomposition (SVD) and
Frobenius-optimal linear transformation, ensuring compatibility under dimension
mismatch; and (b) LoRA-Shift, which applies the aligned subspaces to project
source LoRA weight updates into the target model parameter space. Both
components are data-free, training-free, and enable lightweight adaptation on a
commodity GPU in 20 minutes. Experiments on ARCs, OBOA and HellaSwag show that
Cross-LoRA achieves relative gains of up to 5.26% over base models. Across
other commonsense reasoning benchmarks, Cross-LoRA maintains performance
comparable to that of directly trained LoRA adapters.

</details>


### [142] [MoBE: Mixture-of-Basis-Experts for Compressing MoE-based LLMs](https://arxiv.org/abs/2508.05257)
*Xiaodong Chen,Mingming Ha,Zhenzhong Lan,Jing Zhang,Jianguo Li*

Main category: cs.LG

TL;DR: 本文提出Mixture-of-Basis-Experts (MoBE)方法实现大语言模型压缩且精度损失小。


<details>
  <summary>Details</summary>
Motivation: 基于MoE架构的大语言模型部署时内存需求大，现有压缩方法精度损失大。

Method: 对专家中的上/门矩阵进行秩分解W = AB，对较大矩阵B重新参数化为基矩阵线性组合，通过最小化重构误差学习分解。

Result: MoBE比现有方法精度损失显著降低，如能将部分模型参数减少24%-30%，精度仅下降1%-2%。

Conclusion: MoBE方法能在实现模型压缩的同时有效控制精度损失。

Abstract: The Mixture-of-Experts (MoE) architecture has become a predominant paradigm
for scaling large language models (LLMs). Despite offering strong performance
and computational efficiency, large MoE-based LLMs like DeepSeek-V3-0324 and
Kimi-K2-Instruct present serious challenges due to substantial memory
requirements in deployment. While recent works have explored MoE compression to
address this issue, existing methods often suffer from considerable accuracy
drops (e.g., 7-14% relatively) even at modest compression rates. This paper
introduces a novel Mixture-of-Basis-Experts (MoBE) method that achieves model
compression while incurring minimal accuracy drops. Specifically, each up/gate
matrix in an expert is decomposed via a rank decomposition as W = AB, where
matrix A is unique to each expert. The relatively larger matrix B is further
re-parameterized as a linear combination of basis matrices {Bi} shared across
all experts within a given MoE layer. The factorization is learned by
minimizing the reconstruction error relative to the original weight matrices.
Experiments demonstrate that MoBE achieves notably lower accuracy drops
compared to prior works. For instance, MoBE can reduce the parameter counts of
Qwen3-235B-A22B-2507, DeepSeek-V3-0324 (671B) and Kimi-K2-Instruct (1T) by
24%-30% with only 1%-2% accuracy drop (about 2% drops when measured
relatively).

</details>


### [143] [Marine Chlorophyll Prediction and Driver Analysis based on LSTM-RF Hybrid Models](https://arxiv.org/abs/2508.05260)
*Zhouyao Qian,Yang Chen,Baodian Li,Shuyi Zhang,Zhen Tian,Gongsen Wang,Tianyue Gu,Xinyu Zhou,Huilin Chen,Xinyi Li,Hao Zhu,Shuyao Zhang,Zongheng Li,Siyuan Wang*

Main category: cs.LG

TL;DR: 本文提出LSTM - RF混合模型预测海洋叶绿素浓度，实验显示该模型效果优于单一模型，标准化处理和滑动窗口方法提升了预测精度。


<details>
  <summary>Details</summary>
Motivation: 海洋叶绿素浓度准确预测对赤潮预警和生态响应至关重要，单一模型在时间序列建模和非线性特征刻画上有不足。

Method: 提出结合LSTM和RF优势的LSTM - RF混合模型，用多源海洋数据训练，采用标准化处理和滑动窗口方法。

Result: LSTM - RF模型在测试集上R^2为0.5386，MSE为0.005806，MAE为0.057147，显著优于单独使用LSTM和RF。

Conclusion: 标准化处理和滑动窗口方法提高了模型预测精度，为海洋生态变量高频预测提供创新方案。

Abstract: Marine chlorophyll concentration is an important indicator of ecosystem
health and carbon cycle strength, and its accurate prediction is crucial for
red tide warning and ecological response. In this paper, we propose a LSTM-RF
hybrid model that combines the advantages of LSTM and RF, which solves the
deficiencies of a single model in time-series modelling and nonlinear feature
portrayal. Trained with multi-source ocean data(temperature, salinity,
dissolved oxygen, etc.), the experimental results show that the LSTM-RF model
has an R^2 of 0.5386, an MSE of 0.005806, and an MAE of 0.057147 on the test
set, which is significantly better than using LSTM (R^2 = 0.0208) and RF (R^2
=0.4934) alone , respectively. The standardised treatment and sliding window
approach improved the prediction accuracy of the model and provided an
innovative solution for high-frequency prediction of marine ecological
variables.

</details>


### [144] [FlowState: Sampling Rate Invariant Time Series Forecasting](https://arxiv.org/abs/2508.05287)
*Lars Graf,Thomas Ortner,Stanisław Woźniak,Angeliki Pantazi*

Main category: cs.LG

TL;DR: 提出新型时间序列基础模型FlowState，解决现有模型泛化、适应性和效率问题，表现优于其他模型。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列基础模型在泛化、适应性和计算效率方面存在问题，需要改进。

Method: 引入基于状态空间模型的编码器和功能基解码器的FlowState架构，提出高效预训练策略。

Result: FlowState是最小的模型，但在GIFT - ZS和Chronos - ZS基准测试中表现优于其他模型，消融研究证实组件有效性，能在线适应不同输入采样率。

Conclusion: FlowState有效解决现有时间序列基础模型的问题，具有良好性能和适应性。

Abstract: Foundation models (FMs) have transformed natural language processing, but
their success has not yet translated to time series forecasting. Existing time
series foundation models (TSFMs), often based on transformer variants, struggle
with generalization across varying context and target lengths, lack
adaptability to different sampling rates, and are computationally inefficient.
We introduce FlowState, a novel TSFM architecture that addresses these
challenges through two key innovations: a state space model (SSM) based encoder
and a functional basis decoder. This design enables continuous-time modeling
and dynamic time-scale adjustment, allowing FlowState to inherently generalize
across all possible temporal resolutions, and dynamically adjust the
forecasting horizons. In contrast to other state-of-the-art TSFMs, which
require training data across all possible sampling rates to memorize patterns
at each scale, FlowState inherently adapts its internal dynamics to the input
scale, enabling smaller models, reduced data requirements, and improved
efficiency. We further propose an efficient pretraining strategy that improves
robustness and accelerates training. Despite being the smallest model,
FlowState outperforms all other models and is state-of-the-art for the GIFT-ZS
and the Chronos-ZS benchmarks. Ablation studies confirm the effectiveness of
its components, and we demonstrate its unique ability to adapt online to
varying input sampling rates.

</details>


### [145] [RLHF Fine-Tuning of LLMs for Alignment with Implicit User Feedback in Conversational Recommenders](https://arxiv.org/abs/2508.05289)
*Zhongheng Yang,Aijia Sun,Yushang Zhao,Yinuo Yang,Dannier Li,Chengrui Zhou*

Main category: cs.LG

TL;DR: 本文提出基于人类反馈强化学习的微调方案，以最大化多轮推荐中隐含的用户反馈，实验表明该方案在多项指标上表现更好，且隐式信号对齐对对话推荐系统设计有效。


<details>
  <summary>Details</summary>
Motivation: 传统监督微调无法捕捉隐式反馈信号，对话推荐系统需不断契合用户偏好以提供满意的推荐。

Method: 使用人类反馈强化学习（RLHF），指定基于弱标签参与信息学习的奖励模型，通过近端策略优化（PPO）方法优化基础大语言模型，对对话状态转移进行建模。

Result: 在合成和真实数据集上的评估显示，经RLHF微调的模型在top - k推荐准确率、连贯性和用户满意度方面表现更好。

Conclusion: 隐式信号对齐在实现对话推荐系统的可扩展和用户自适应设计方面是有效的。

Abstract: Conversational recommender systems (CRS) based on Large Language Models
(LLMs) need to constantly be aligned to the user preferences to provide
satisfying and context-relevant item recommendations. The traditional
supervised fine-tuning cannot capture the implicit feedback signal, e.g., dwell
time, sentiment polarity, or engagement patterns. In this paper, we share a
fine-tuning solution using human feedback reinforcement learning (RLHF) to
maximize implied user feedback (IUF) in a multi-turn recommendation context. We
specify a reward model $R_{\phi}$ learnt on weakly-labelled engagement
information and maximize user-centric utility by optimizing the foundational
LLM M_{\theta} through a proximal policy optimization (PPO) approach. The
architecture models conversational state transitions $s_t \to a_t \to s_{t
+1}$, where the action $a_t$ is associated with LLM-generated item suggestions
only on condition of conversation history in the past. The evaluation across
synthetic and real-world datasets (e.g.REDIAL, OpenDialKG) demonstrates that
our RLHF-fine-tuned models can perform better in terms of top-$k$
recommendation accuracy, coherence, and user satisfaction compared to
(arrow-zero-cmwrquca-teja-falset ensuite 2Round group-deca States penalty give
up This paper shows that implicit signal alignment can be efficient in
achieving scalable and user-adaptive design of CRS.

</details>


### [146] [Optimal Growth Schedules for Batch Size and Learning Rate in SGD that Reduce SFO Complexity](https://arxiv.org/abs/2508.05297)
*Hikaru Umeda,Hideaki Iiduka*

Main category: cs.LG

TL;DR: 深度学习模型增长带来计算瓶颈，研究批量大小和学习率调度以平衡效率和收敛，推导最优增长调度并验证。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型增长带来计算瓶颈，朴素的超参数调度会降低优化效率和泛化能力，需研究批量大小和学习率调度以平衡效率和收敛。

Method: 基于随机一阶神谕（SFO）复杂度分析问题，理论推导批量大小和学习率的最优增长调度。

Result: 理论推导的最优增长调度通过大量实验得到验证。

Conclusion: 研究结果为深度学习中可扩展和高效的大批量训练提供理论见解和实践指南。

Abstract: The unprecedented growth of deep learning models has enabled remarkable
advances but introduced substantial computational bottlenecks. A key factor
contributing to training efficiency is batch-size and learning-rate scheduling
in stochastic gradient methods. However, naive scheduling of these
hyperparameters can degrade optimization efficiency and compromise
generalization. Motivated by recent theoretical insights, we investigated how
the batch size and learning rate should be increased during training to balance
efficiency and convergence. We analyzed this problem on the basis of stochastic
first-order oracle (SFO) complexity, defined as the expected number of gradient
evaluations needed to reach an $\epsilon$-approximate stationary point of the
empirical loss. We theoretically derived optimal growth schedules for the batch
size and learning rate that reduce SFO complexity and validated them through
extensive experiments. Our results offer both theoretical insights and
practical guidelines for scalable and efficient large-batch training in deep
learning.

</details>


### [147] [Adaptive Batch Size and Learning Rate Scheduler for Stochastic Gradient Descent Based on Minimization of Stochastic First-order Oracle Complexity](https://arxiv.org/abs/2508.05302)
*Hikaru Umeda,Hideaki Iiduka*

Main category: cs.LG

TL;DR: 本文介绍自适应调度策略加速小批量随机梯度下降（SGD），实验表明该策略比现有调度器收敛速度更快。


<details>
  <summary>Details</summary>
Motivation: 小批量SGD收敛行为对批量大小和学习率设置敏感，理论研究发现存在使随机一阶预言机（SFO）复杂度最小的临界批量大小，因此想利用此理论加速SGD。

Method: 引入自适应调度策略，根据训练中全梯度范数的衰减调整批量大小和学习率。

Result: 使用基于该策略的自适应联合调度器的实验显示，其收敛速度比现有调度器更快。

Conclusion: 所提出的自适应调度策略能有效加速SGD的收敛。

Abstract: The convergence behavior of mini-batch stochastic gradient descent (SGD) is
highly sensitive to the batch size and learning rate settings. Recent
theoretical studies have identified the existence of a critical batch size that
minimizes stochastic first-order oracle (SFO) complexity, defined as the
expected number of gradient evaluations required to reach a stationary point of
the empirical loss function in a deep neural network. An adaptive scheduling
strategy is introduced to accelerate SGD that leverages theoretical findings on
the critical batch size. The batch size and learning rate are adjusted on the
basis of the observed decay in the full gradient norm during training.
Experiments using an adaptive joint scheduler based on this strategy
demonstrated improved convergence speed compared with that of existing
schedulers.

</details>


### [148] [ASkDAgger: Active Skill-level Data Aggregation for Interactive Imitation Learning](https://arxiv.org/abs/2508.05310)
*Jelle Luijkx,Zlatan Ajanović,Laura Ferranti,Jens Kober*

Main category: cs.LG

TL;DR: 现有交互式模仿学习需大量人工教学，本文提出 ASkDAgger 框架利用新手计划信息，减少查询和标注，经实验验证有效。


<details>
  <summary>Details</summary>
Motivation: 现有交互式模仿学习中人工教学投入大，现有方法未利用新手计划中的有价值信息。

Method: 引入 ASkDAgger 框架，包含 S - Aware Gating、Foresight Interactive Experience Replay 和 Prioritized Interactive Experience Replay 三个组件。

Result: 平衡查询频率和失败率，减少示范标注数量，提高泛化能力，加快对变化领域的适应。

Conclusion: 通过模拟和现实环境中的语言条件操作任务验证了 ASkDAgger 的有效性。

Abstract: Human teaching effort is a significant bottleneck for the broader
applicability of interactive imitation learning. To reduce the number of
required queries, existing methods employ active learning to query the human
teacher only in uncertain, risky, or novel situations. However, during these
queries, the novice's planned actions are not utilized despite containing
valuable information, such as the novice's capabilities, as well as
corresponding uncertainty levels. To this end, we allow the novice to say: "I
plan to do this, but I am uncertain." We introduce the Active Skill-level Data
Aggregation (ASkDAgger) framework, which leverages teacher feedback on the
novice plan in three key ways: (1) S-Aware Gating (SAG): Adjusts the gating
threshold to track sensitivity, specificity, or a minimum success rate; (2)
Foresight Interactive Experience Replay (FIER), which recasts valid and
relabeled novice action plans into demonstrations; and (3) Prioritized
Interactive Experience Replay (PIER), which prioritizes replay based on
uncertainty, novice success, and demonstration age. Together, these components
balance query frequency with failure incidence, reduce the number of required
demonstration annotations, improve generalization, and speed up adaptation to
changing domains. We validate the effectiveness of ASkDAgger through
language-conditioned manipulation tasks in both simulation and real-world
environments. Code, data, and videos are available at
https://askdagger.github.io.

</details>


### [149] [Divide-and-Conquer for Enhancing Unlabeled Learning, Stability, and Plasticity in Semi-supervised Continual Learning](https://arxiv.org/abs/2508.05316)
*Yue Duan,Taicai Chen,Lei Qi,Yinghuan Shi*

Main category: cs.LG

TL;DR: 本文提出USP框架用于半监督持续学习，通过三种策略协同提升有效无监督学习、记忆稳定性和学习可塑性，评估显示优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 半监督持续学习存在确保有效无监督学习、平衡记忆稳定性和学习可塑性的挑战，以往研究多孤立处理，本文旨在协同提升这三方面。

Method: 提出USP框架，包括用于学习可塑性的特征空间保留策略、用于无监督学习的分治伪标签方法和用于记忆稳定性的类均值锚定无监督蒸馏方法。

Result: 综合评估显示USP优于现有半监督持续学习方法，最后准确率提升达5.94%。

Conclusion: USP框架在半监督持续学习中有效，代码已开源。

Abstract: Semi-supervised continual learning (SSCL) seeks to leverage both labeled and
unlabeled data in a sequential learning setup, aiming to reduce annotation
costs while managing continual data arrival. SSCL introduces complex
challenges, including ensuring effective unlabeled learning (UL), while
balancing memory stability (MS) and learning plasticity (LP). Previous SSCL
efforts have typically focused on isolated aspects of the three, while this
work presents USP, a divide-and-conquer framework designed to synergistically
enhance these three aspects: (1) Feature Space Reservation (FSR) strategy for
LP, which constructs reserved feature locations for future classes by shaping
old classes into an equiangular tight frame; (2) Divide-and-Conquer
Pseudo-labeling (DCP) approach for UL, which assigns reliable pseudo-labels
across both high- and low-confidence unlabeled data; and (3)
Class-mean-anchored Unlabeled Distillation (CUD) for MS, which reuses DCP's
outputs to anchor unlabeled data to stable class means for distillation to
prevent forgetting. Comprehensive evaluations show USP outperforms prior SSCL
methods, with gains up to 5.94% in the last accuracy, validating its
effectiveness. The code is available at https://github.com/NJUyued/USP4SSCL.

</details>


### [150] [Optimal Corpus Aware Training for Neural Machine Translation](https://arxiv.org/abs/2508.05364)
*Yi-Hsiu Liao,Cheng Shen,Brenda,Yang*

Main category: cs.LG

TL;DR: 提出Optimal Corpus Aware Training (OCAT)方法，在翻译任务上表现良好，比普通训练有提升且对超参不敏感。


<details>
  <summary>Details</summary>
Motivation: 现有Corpus Aware Training (CAT)模型在训练前预定义高质量数据易出错且低效，需要改进。

Method: 提出OCAT，微调CAT预训练模型，冻结大部分参数，只调整少量语料相关参数。

Result: 在WMT23英中、英德翻译任务上，分别比普通训练有+3.6和+1.8的chrF提升，表现与其他SOTA微调技术相当或略好，对超参不太敏感。

Conclusion: OCAT轻量级、抗过拟合，能有效提升模型准确率。

Abstract: Corpus Aware Training (CAT) leverages valuable corpus metadata during
training by injecting corpus information into each training example, and has
been found effective in the literature, commonly known as the "tagging"
approach. Models trained with CAT inherently learn the quality, domain and
nuance between corpora directly from data, and can easily switch to different
inference behavior. To achieve the best evaluation, CAT models pre-define a
group of high quality data before training starts which can be error-prone and
inefficient. In this work, we propose Optimal Corpus Aware Training (OCAT),
which fine-tunes a CAT pre-trained model by freezing most of the model
parameters and only tuning small set of corpus-related parameters. We show that
OCAT is lightweight, resilient to overfitting, and effective in boosting model
accuracy. We use WMT23 English to Chinese and English to German translation
tasks as our test ground and show +3.6 and +1.8 chrF improvement, respectively,
over vanilla training. Furthermore, our approach is on-par or slightly better
than other state-of-the-art fine-tuning techniques while being less sensitive
to hyperparameter settings.

</details>


### [151] [Latent Preference Bandits](https://arxiv.org/abs/2508.05367)
*Newton Mwai,Emil Carlsson,Fredrik D. Johansson*

Main category: cs.LG

TL;DR: 传统bandit算法在个性化任务中从头学习成本高，潜在bandit算法虽能减少探索时间但实际找模型难。本文提出放宽潜在bandit假设，仅要求动作偏好排序模型，给出后验采样算法，性能有竞争力。


<details>
  <summary>Details</summary>
Motivation: 传统bandit算法在个性化任务中从头学习成本高，潜在bandit算法实际找合适模型困难，需改进。

Method: 提出放宽潜在bandit假设，仅要求每个潜在状态下动作的偏好排序模型，并给出后验采样算法。

Result: 算法在奖励分布明确时与全知奖励分布的潜在bandit算法性能相当，在相同潜在状态实例奖励尺度不同时表现更优。

Conclusion: 放宽潜在bandit假设的方法可行，提出的算法有较好性能。

Abstract: Bandit algorithms are guaranteed to solve diverse sequential decision-making
problems, provided that a sufficient exploration budget is available. However,
learning from scratch is often too costly for personalization tasks where a
single individual faces only a small number of decision points. Latent bandits
offer substantially reduced exploration times for such problems, given that the
joint distribution of a latent state and the rewards of actions is known and
accurate. In practice, finding such a model is non-trivial, and there may not
exist a small number of latent states that explain the responses of all
individuals. For example, patients with similar latent conditions may have the
same preference in treatments but rate their symptoms on different scales. With
this in mind, we propose relaxing the assumptions of latent bandits to require
only a model of the \emph{preference ordering} of actions in each latent state.
This allows problem instances with the same latent state to vary in their
reward distributions, as long as their preference orderings are equal. We give
a posterior-sampling algorithm for this problem and demonstrate that its
empirical performance is competitive with latent bandits that have full
knowledge of the reward distribution when this is well-specified, and
outperforms them when reward scales differ between instances with the same
latent state.

</details>


### [152] [Echo: Decoupling Inference and Training for Large-Scale RL Alignment on Heterogeneous Swarms](https://arxiv.org/abs/2508.05387)
*Jie Xiao,Shaoduo Gan,Changyuan Fan,Qingnan Ren,Alfred Long,Yuchen Zhang,Rymon Yu,Eric Yang,Lynn Ai*

Main category: cs.LG

TL;DR: 提出Echo系统，解耦大语言模型基于强化学习的训练中轨迹采样和策略优化阶段，在分布式集群测试有良好效果。


<details>
  <summary>Details</summary>
Motivation: 现代基于强化学习的大语言模型训练中，轨迹采样和策略优化在同一GPU集群，串行上下文切换违反分布式训练系统假设。

Method: 引入Echo系统，通过两个轻量级同步协议（顺序拉取模式和异步推拉模式），在异构“推理”和“训练”集群中解耦两个阶段。

Result: 在地理分布式集群上用不同模型训练，Echo在收敛速度和最终奖励上与完全共置的Verl基线相当，且能将轨迹生成卸载到边缘硬件。

Conclusion: 大语言模型的大规模强化学习可使用分散、异构资源实现数据中心级性能。

Abstract: Modern RL-based post-training for large language models (LLMs) co-locate
trajectory sampling and policy optimisation on the same GPU cluster, forcing
the system to switch between inference and training workloads. This serial
context switching violates the single-program-multiple-data (SPMD) assumption
underlying today's distributed training systems. We present Echo, the RL system
that cleanly decouples these two phases across heterogeneous "inference" and
"training" swarms while preserving statistical efficiency. Echo introduces two
lightweight synchronization protocols: a sequential pull mode that refreshes
sampler weights on every API call for minimal bias, and an asynchronous
push-pull mode that streams version-tagged rollouts through a replay buffer to
maximise hardware utilisation. Training three representative RL workloads with
Qwen3-4B, Qwen2.5-7B and Qwen3-32B on a geographically distributed cluster,
Echo matches a fully co-located Verl baseline in convergence speed and final
reward while off-loading trajectory generation to commodity edge hardware.
These promising results demonstrate that large-scale RL for LLMs could achieve
datacentre-grade performance using decentralised, heterogeneous resources.

</details>


### [153] [NT-ML: Backdoor Defense via Non-target Label Training and Mutual Learning](https://arxiv.org/abs/2508.05404)
*Wenjie Huo,Katinka Wolter*

Main category: cs.LG

TL;DR: 本文提出NT - ML防御机制，可在高级后门攻击下恢复中毒模型，实验表明该机制能有效防御6种后门攻击，优于5种现有防御方法。


<details>
  <summary>Details</summary>
Motivation: 解决深度神经网络易受后门攻击的问题，恢复被后门攻击的中毒模型。

Method: 提出Non - target label Training and Mutual Learning（NT - ML）机制，先通过NT用标准训练输出重新训练模型得到教师和学生模型，再通过ML让二者相互学习得到净化的学生模型。

Result: NT - ML能利用少量干净样本有效防御6种后门攻击，性能优于5种现有后门防御方法。

Conclusion: NT - ML是一种有效的针对深度神经网络后门攻击的防御机制。

Abstract: Recent studies have shown that deep neural networks (DNNs) are vulnerable to
backdoor attacks, where a designed trigger is injected into the dataset,
causing erroneous predictions when activated. In this paper, we propose a novel
defense mechanism, Non-target label Training and Mutual Learning (NT-ML), which
can successfully restore the poisoned model under advanced backdoor attacks. NT
aims to reduce the harm of poisoned data by retraining the model with the
outputs of the standard training. At this stage, a teacher model with high
accuracy on clean data and a student model with higher confidence in correct
prediction on poisoned data are obtained. Then, the teacher and student can
learn the strengths from each other through ML to obtain a purified student
model. Extensive experiments show that NT-ML can effectively defend against 6
backdoor attacks with a small number of clean samples, and outperforms 5
state-of-the-art backdoor defenses.

</details>


### [154] [Cumulative Learning Rate Adaptation: Revisiting Path-Based Schedules for SGD and Adam](https://arxiv.org/abs/2508.05408)
*Asma Atamna,Tom Maus,Fabian Kievelitz,Tobias Glasmachers*

Main category: cs.LG

TL;DR: 本文研究自适应学习率机制，指出原累积路径自适应方案对Adam的适应性存在概念不一致问题，提出修正变体并进行基准测试以明确自适应策略何时及为何有实用价值。


<details>
  <summary>Details</summary>
Motivation: 学习率是深度学习关键超参数，其理想值随问题变化且训练中可能改变，探究自适应学习率机制的实用价值。

Method: 重新审视2017年提出的累积路径自适应方案，指出其对Adam的问题，提出修正变体，对SGD和Adam进行有无累积自适应的基准测试并与新方法比较。

Result: 结果旨在明确自适应策略何时及为何有实用价值。

Conclusion: 文中未明确提及最终结论，但致力于阐明自适应学习率策略的实用情况和原因。

Abstract: The learning rate is a crucial hyperparameter in deep learning, with its
ideal value depending on the problem and potentially changing during training.
In this paper, we investigate the practical utility of adaptive learning rate
mechanisms that adjust step sizes dynamically in response to the loss
landscape. We revisit a cumulative path-based adaptation scheme proposed in
2017, which adjusts the learning rate based on the discrepancy between the
observed path length, computed as a time-discounted sum of normalized gradient
steps, and the expected length of a random walk. While the original approach
offers a compelling intuition, we show that its adaptation mechanism for Adam
is conceptually inconsistent due to the optimizer's internal preconditioning.
We propose a corrected variant that better reflects Adam's update dynamics. To
assess the practical value of online learning rate adaptation, we benchmark SGD
and Adam, with and without cumulative adaptation, and compare them to a recent
alternative method. Our results aim to clarify when and why such adaptive
strategies offer practical benefits.

</details>


### [155] [MolSnap: Snap-Fast Molecular Generation with Latent Variational Mean Flow](https://arxiv.org/abs/2508.05411)
*Md Atik Ahamed,Qiang Ye,Qiang Cheng*

Main category: cs.LG

TL;DR: 提出因果感知框架解决分子生成难题，实验表现优于基线，计算效率高。


<details>
  <summary>Details</summary>
Motivation: 现有分子生成方法难以同时保证高质量、多样生成和快速推理。

Method: 提出因果感知Transformer联合编码分子图和文本指令；开发变分平均流框架，将潜在空间建模为高斯混合。

Result: 在四个标准分子基准测试中，模型新颖性最高达74.5%，多样性最高达70.3%，所有数据集有效性达100%；条件生成仅需1次函数评估，无条件生成最多5次。

Conclusion: 所提模型优于现有基线，具有高质量、多样性和高计算效率。

Abstract: Molecular generation conditioned on textual descriptions is a fundamental
task in computational chemistry and drug discovery. Existing methods often
struggle to simultaneously ensure high-quality, diverse generation and fast
inference. In this work, we propose a novel causality-aware framework that
addresses these challenges through two key innovations. First, we introduce a
Causality-Aware Transformer (CAT) that jointly encodes molecular graph tokens
and text instructions while enforcing causal dependencies during generation.
Second, we develop a Variational Mean Flow (VMF) framework that generalizes
existing flow-based methods by modeling the latent space as a mixture of
Gaussians, enhancing expressiveness beyond unimodal priors. VMF enables
efficient one-step inference while maintaining strong generation quality and
diversity. Extensive experiments on four standard molecular benchmarks
demonstrate that our model outperforms state-of-the-art baselines, achieving
higher novelty (up to 74.5\%), diversity (up to 70.3\%), and 100\% validity
across all datasets. Moreover, VMF requires only one number of function
evaluation (NFE) during conditional generation and up to five NFEs for
unconditional generation, offering substantial computational efficiency over
diffusion-based methods.

</details>


### [156] [Federated Multi-Objective Learning with Controlled Pareto Frontiers](https://arxiv.org/abs/2508.05424)
*Jiansheng Rao,Jiayi Li,Zhizhi Gong,Soummya Kar,Haoxuan Li*

Main category: cs.LG

TL;DR: 提出Conically - Regularised FMOL (CR - FMOL)框架，通过偏好锥约束实现客户端帕累托最优，实验显示提升客户端公平性，训练轮数足够时精度与FedAvg相当。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习FedAvg忽视少数客户端，现有方法如FMOL仅实现任务级帕累托平稳点，无法保障客户端公平性。

Method: 引入CR - FMOL框架，客户端经本地FMGDA/FSMGDA步骤后传输聚合任务损失向量，服务器解决以均匀向量为中心的锥约束Pareto - MTL子问题。

Result: 在非IID基准测试中，CR - FMOL提升了客户端公平性，早期性能略逊于FedAvg，但训练轮数足够时精度相当。

Conclusion: CR - FMOL是首个实现客户端级帕累托最优的联邦多目标优化框架，能有效提升客户端公平性。

Abstract: Federated learning (FL) is a widely adopted paradigm for privacy-preserving
model training, but FedAvg optimise for the majority while under-serving
minority clients. Existing methods such as federated multi-objective learning
(FMOL) attempts to import multi-objective optimisation (MOO) into FL. However,
it merely delivers task-wise Pareto-stationary points, leaving client fairness
to chance. In this paper, we introduce Conically-Regularised FMOL (CR-FMOL),
the first federated MOO framework that enforces client-wise Pareto optimality
through a novel preference-cone constraint. After local federated
multi-gradient descent averaging (FMGDA) / federated stochastic multi-gradient
descent averaging (FSMGDA) steps, each client transmits its aggregated
task-loss vector as an implicit preference; the server then solves a
cone-constrained Pareto-MTL sub-problem centred at the uniform vector,
producing a descent direction that is Pareto-stationary for every client within
its cone. Experiments on non-IID benchmarks show that CR-FMOL enhances client
fairness, and although the early-stage performance is slightly inferior to
FedAvg, it is expected to achieve comparable accuracy given sufficient training
rounds.

</details>


### [157] [Group Causal Policy Optimization for Post-Training Large Language Models](https://arxiv.org/abs/2508.05428)
*Ziyin Gu,Jingyao Wang,Ran Zuo,Chuxiong Sun,Zeen Song,Changwen Zheng,Wenwen Qiang*

Main category: cs.LG

TL;DR: 现有GRPO方法忽略候选回复语义交互，本文引入SCM，提出GCPO方法，实验表明GCPO优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有GRPO方法将候选回复视为独立，忽略语义交互，需要改进。

Method: 引入SCM揭示候选回复隐藏依赖，基于因果分析提出GCPO，包括因果奖励调整和KL正则化项。

Result: 综合实验评估显示GCPO在多个推理基准测试中始终优于现有方法，包括GRPO。

Conclusion: GCPO通过整合因果结构到优化中，能有效提升性能，优于现有方法。

Abstract: Recent advances in large language models (LLMs) have broadened their
applicability across diverse tasks, yet specialized domains still require
targeted post training. Among existing methods, Group Relative Policy
Optimization (GRPO) stands out for its efficiency, leveraging groupwise
relative rewards while avoiding costly value function learning. However, GRPO
treats candidate responses as independent, overlooking semantic interactions
such as complementarity and contradiction. To address this challenge, we first
introduce a Structural Causal Model (SCM) that reveals hidden dependencies
among candidate responses induced by conditioning on a final integrated output
forming a collider structure. Then, our causal analysis leads to two insights:
(1) projecting responses onto a causally informed subspace improves prediction
quality, and (2) this projection yields a better baseline than query only
conditioning. Building on these insights, we propose Group Causal Policy
Optimization (GCPO), which integrates causal structure into optimization
through two key components: a causally informed reward adjustment and a novel
KL regularization term that aligns the policy with a causally projected
reference distribution. Comprehensive experimental evaluations demonstrate that
GCPO consistently surpasses existing methods, including GRPO across multiple
reasoning benchmarks.

</details>


### [158] [Competing Risks: Impact on Risk Estimation and Algorithmic Fairness](https://arxiv.org/abs/2508.05435)
*Vincent Jeanselme,Brian Tom,Jessica Barrett*

Main category: cs.LG

TL;DR: 本文指出将竞争风险视为删失会给生存估计带来偏差，加剧不平等，提出框架估计误差，强调生存模型应考虑竞争风险。


<details>
  <summary>Details</summary>
Motivation: 准确的事件时间预测很重要，但现有生存分析常将竞争风险视为删失，且对其后果理解有限，需研究此做法带来的问题。

Method: 从理论上分析将竞争风险误分类为删失的问题，量化生存估计误差，开发估计误差的框架，结合心血管管理的实证分析。

Result: 发现将竞争风险视为删失会导致生存估计偏差，过度估计风险，加剧不平等，且不同人口群体的风险特征导致特定群体误差。

Conclusion: 从业者在开发生存模型时必须考虑竞争风险，以提高准确性、减少风险评估差异并为下游决策提供更好依据。

Abstract: Accurate time-to-event prediction is integral to decision-making, informing
medical guidelines, hiring decisions, and resource allocation. Survival
analysis, the quantitative framework used to model time-to-event data, accounts
for patients who do not experience the event of interest during the study
period, known as censored patients. However, many patients experience events
that prevent the observation of the outcome of interest. These competing risks
are often treated as censoring, a practice frequently overlooked due to a
limited understanding of its consequences. Our work theoretically demonstrates
why treating competing risks as censoring introduces substantial bias in
survival estimates, leading to systematic overestimation of risk and,
critically, amplifying disparities. First, we formalize the problem of
misclassifying competing risks as censoring and quantify the resulting error in
survival estimates. Specifically, we develop a framework to estimate this error
and demonstrate the associated implications for predictive performance and
algorithmic fairness. Furthermore, we examine how differing risk profiles
across demographic groups lead to group-specific errors, potentially
exacerbating existing disparities. Our findings, supported by an empirical
analysis of cardiovascular management, demonstrate that ignoring competing
risks disproportionately impacts the individuals most at risk of these events,
potentially accentuating inequity. By quantifying the error and highlighting
the fairness implications of the common practice of considering competing risks
as censoring, our work provides a critical insight into the development of
survival models: practitioners must account for competing risks to improve
accuracy, reduce disparities in risk assessment, and better inform downstream
decisions.

</details>


### [159] [Tail-Risk-Safe Monte Carlo Tree Search under PAC-Level Guarantees](https://arxiv.org/abs/2508.05441)
*Zuyuan Zhang,Arnob Ghosh,Tian Lan*

Main category: cs.LG

TL;DR: 论文针对蒙特卡罗树搜索（MCTS）中仅考虑预期回报无法处理高风险结果的问题，提出CVaR - MCTS和Wasserstein - MCTS两种新方法，证明其安全保证和遗憾界，实验显示新方法优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有MCTS仅考虑预期回报无法处理高风险结果，且安全感知MCTS无法提供严格的尾部安全保证，在高风险场景可能导致严重后果。

Method: 提出CVaR - MCTS，将条件风险价值（CVaR）嵌入MCTS实现尾部风险控制；提出Wasserstein - MCTS，引入Wasserstein模糊集刻画尾部风险估计的不确定性。

Result: 证明了CVaR - MCTS和W - MCTS的PAC尾部安全保证并建立其遗憾界，在不同模拟环境评估中，新方法优于现有基线。

Conclusion: 所提方法能有效实现强大的尾部风险保证，提高回报和稳定性。

Abstract: Making decisions with respect to just the expected returns in Monte Carlo
Tree Search (MCTS) cannot account for the potential range of high-risk, adverse
outcomes associated with a decision. To this end, safety-aware MCTS often
consider some constrained variants -- by introducing some form of mean risk
measures or hard cost thresholds. These approaches fail to provide rigorous
tail-safety guarantees with respect to extreme or high-risk outcomes (denoted
as tail-risk), potentially resulting in serious consequence in high-stake
scenarios. This paper addresses the problem by developing two novel solutions.
We first propose CVaR-MCTS, which embeds a coherent tail risk measure,
Conditional Value-at-Risk (CVaR), into MCTS. Our CVaR-MCTS with parameter
$\alpha$ achieves explicit tail-risk control over the expected loss in the
"worst $(1-\alpha)\%$ scenarios." Second, we further address the estimation
bias of tail-risk due to limited samples. We propose Wasserstein-MCTS (or
W-MCTS) by introducing a first-order Wasserstein ambiguity set
$\mathcal{P}_{\varepsilon_{s}}(s,a)$ with radius $\varepsilon_{s}$ to
characterize the uncertainty in tail-risk estimates. We prove PAC tail-safety
guarantees for both CVaR-MCTS and W-MCTS and establish their regret.
Evaluations on diverse simulated environments demonstrate that our proposed
methods outperform existing baselines, effectively achieving robust tail-risk
guarantees with improved rewards and stability.

</details>


### [160] [EnergyPatchTST: Multi-scale Time Series Transformers with Uncertainty Estimation for Energy Forecasting](https://arxiv.org/abs/2508.05454)
*Wei Li,Zixin Wang,Qizheng Sun,Qixiang Gao,Fenglei Yang*

Main category: cs.LG

TL;DR: 提出适用于能源预测的EnergyPatchTST，实验显示其优于常用方法，降低预测误差并提供不确定性估计。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习时间序列预测方法受多尺度时间动态和数据不规则性限制，需更优能源时间序列预测方法。

Method: 提出EnergyPatchTST，含多尺度特征提取、概率预测框架、未来已知变量集成路径、预训练和微调示例。

Result: 在常见能源数据集上实验，EnergyPatchTST预测误差降低7 - 12%，能提供可靠不确定性估计。

Conclusion: EnergyPatchTST为能源领域时间序列预测提供重要参考。

Abstract: Accurate and reliable energy time series prediction is of great significance
for power generation planning and allocation. At present, deep learning time
series prediction has become the mainstream method. However, the multi-scale
time dynamics and the irregularity of real data lead to the limitations of the
existing methods. Therefore, we propose EnergyPatchTST, which is an extension
of the Patch Time Series Transformer specially designed for energy forecasting.
The main innovations of our method are as follows: (1) multi-scale feature
extraction mechanism to capture patterns with different time resolutions; (2)
probability prediction framework to estimate uncertainty through Monte Carlo
elimination; (3) integration path of future known variables (such as
temperature and wind conditions); And (4) Pre-training and Fine-tuning examples
to enhance the performance of limited energy data sets. A series of experiments
on common energy data sets show that EnergyPatchTST is superior to other
commonly used methods, the prediction error is reduced by 7-12%, and reliable
uncertainty estimation is provided, which provides an important reference for
time series prediction in the energy field.

</details>


### [161] [Task complexity shapes internal representations and robustness in neural networks](https://arxiv.org/abs/2508.05463)
*Robert Jankowski,Filippo Radicchi,M. Ángeles Serrano,Marián Boguñá,Santo Fortunato*

Main category: cs.LG

TL;DR: 本文引入五种数据无关探测方法，对比MNIST和Fashion - MNIST数据集上的难易分类任务，研究任务难度对多层感知机表示拓扑和鲁棒性的影响，提出任务复杂度衡量指标并给出相关策略。


<details>
  <summary>Details</summary>
Motivation: 神经网络是黑盒，其内部表示受输入数据复杂度和解决问题的影响机制不明，需研究任务难度对其表示拓扑和鲁棒性的影响。

Method: 引入五种数据无关探测方法（剪枝、二值化、噪声注入、符号翻转和二分网络随机化），将多层感知机表示为有符号加权二分图，对比MNIST和Fashion - MNIST数据集上的难易分类任务。

Result: 硬任务模型权重二值化会使准确率降至随机水平，易任务模型更鲁棒；硬任务二值化模型剪枝有性能相变；适度噪声注入可提高准确率；二分网络随机化仅保留符号结构能维持高精度。

Conclusion: 定义了与模型和模态无关的任务复杂度衡量指标，强调有符号二分拓扑在学习表示中的关键作用，提出与任务复杂度匹配的模型压缩和可解释性实用策略。

Abstract: Neural networks excel across a wide range of tasks, yet remain black boxes.
In particular, how their internal representations are shaped by the complexity
of the input data and the problems they solve remains obscure. In this work, we
introduce a suite of five data-agnostic probes-pruning, binarization, noise
injection, sign flipping, and bipartite network randomization-to quantify how
task difficulty influences the topology and robustness of representations in
multilayer perceptrons (MLPs). MLPs are represented as signed, weighted
bipartite graphs from a network science perspective. We contrast easy and hard
classification tasks on the MNIST and Fashion-MNIST datasets. We show that
binarizing weights in hard-task models collapses accuracy to chance, whereas
easy-task models remain robust. We also find that pruning low-magnitude edges
in binarized hard-task models reveals a sharp phase-transition in performance.
Moreover, moderate noise injection can enhance accuracy, resembling a
stochastic-resonance effect linked to optimal sign flips of small-magnitude
weights. Finally, preserving only the sign structure-instead of precise weight
magnitudes-through bipartite network randomizations suffices to maintain high
accuracy. These phenomena define a model- and modality-agnostic measure of task
complexity: the performance gap between full-precision and binarized or
shuffled neural network performance. Our findings highlight the crucial role of
signed bipartite topology in learned representations and suggest practical
strategies for model compression and interpretability that align with task
complexity.

</details>


### [162] [Let's Measure Information Step-by-Step: LLM-Based Evaluation Beyond Vibes](https://arxiv.org/abs/2508.05469)
*Zachary Robertson,Sanmi Koyejo*

Main category: cs.LG

TL;DR: 开发无真实标签评估AI系统的机制，证明f - 互信息度量是抗博弈的唯一机制，实证显示优于LLM评判和现有方法，发现性能与压缩比呈倒U型曲线。


<details>
  <summary>Details</summary>
Motivation: 开发无需真实标签的AI系统评估机制。

Method: 利用博弈抗性和输出质量的联系，证明f - 互信息度量在自然条件下是唯一抗博弈机制，使用信息论机制进行评估。

Result: 信息论机制在十个领域能完美区分忠实和策略性代理，LLM评判存在评估倒置，该机制对抗操纵的鲁棒性比现有方法高10 - 100倍，性能与压缩比呈倒U型曲线。

Conclusion: 所开发的机制在无真实标签评估AI系统方面表现良好，具有更好的抗操纵能力，且在特定压缩比下效果最佳。

Abstract: We develop mechanisms for evaluating AI systems without ground truth by
exploiting a connection between gaming resistance and output quality. The data
processing inequality ensures post-hoc attempts to game a metric degrades both
information content and task performance. We prove that f-mutual information
measures are the unique gaming resistant mechanisms under natural conditions,
with the overseer acting as an agent. While Shannon mutual information faces
exponential sample complexity, bounded measures like total variation distance
remain tractable. Empirically, across ten domains from translation to peer
review, all information-theoretic mechanisms achieve perfect discrimination (d
> 0.5) between faithful and strategic agents. In contrast, LLM judges exhibit
systematic evaluation inversion, preferring fabricated content over accurate
summaries. Our mechanisms show 10-100x better robustness to adversarial
manipulation than current practices. We also find performance follows an
inverted-U curve with compression ratio, peaking at 10:1 where agent responses
exhibit optimal information diversity (3 effective dimensions), giving a
bias-variance perspective on when our approach is expected to be most
effective.

</details>


### [163] [Prediction of Survival Outcomes under Clinical Presence Shift: A Joint Neural Network Architecture](https://arxiv.org/abs/2508.05472)
*Vincent Jeanselme,Glen Martin,Matthew Sperrin,Niels Peek,Brian Tom,Jessica Barrett*

Main category: cs.LG

TL;DR: 提出多任务循环神经网络，联合建模观察间隔时间、缺失过程与生存结果，在真实任务中证明其能提升临床预测模型性能与可迁移性。


<details>
  <summary>Details</summary>
Motivation: 使用电子健康记录开发临床预测模型时，常忽略临床存在对观察结果的影响，导致模型性能和可迁移性受限。

Method: 提出多任务循环神经网络，并行建模观察间隔时间、缺失过程和生存结果，形式化临床存在偏移概念并理论证明联合建模可提升可迁移性。

Result: 在MIMIC - III数据集的真实死亡率预测任务中，该策略比未纳入观察过程的模型提升了性能和可迁移性。

Conclusion: 利用临床存在对提升性能和创建更具可迁移性的临床预测模型非常重要。

Abstract: Electronic health records arise from the complex interaction between patients
and the healthcare system. This observation process of interactions, referred
to as clinical presence, often impacts observed outcomes. When using electronic
health records to develop clinical prediction models, it is standard practice
to overlook clinical presence, impacting performance and limiting the
transportability of models when this interaction evolves. We propose a
multi-task recurrent neural network that jointly models the inter-observation
time and the missingness processes characterising this interaction in parallel
to the survival outcome of interest. Our work formalises the concept of
clinical presence shift when the prediction model is deployed in new settings
(e.g. different hospitals, regions or countries), and we theoretically justify
why the proposed joint modelling can improve transportability under changes in
clinical presence. We demonstrate, in a real-world mortality prediction task in
the MIMIC-III dataset, how the proposed strategy improves performance and
transportability compared to state-of-the-art prediction models that do not
incorporate the observation process. These results emphasise the importance of
leveraging clinical presence to improve performance and create more
transportable clinical prediction models.

</details>


### [164] [MoMA: A Mixture-of-Multimodal-Agents Architecture for Enhancing Clinical Prediction Modelling](https://arxiv.org/abs/2508.05492)
*Jifan Gao,Mahmudur Rahman,John Caskey,Madeline Oguss,Ann O'Rourke,Randy Brown,Anne Stey,Anoop Mayampurath,Matthew M. Churpek,Guanhua Chen,Majid Afshar*

Main category: cs.LG

TL;DR: 本文提出MoMA架构处理多模态电子健康记录数据进行临床预测，在多个任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多模态电子健康记录数据比单模态数据能提供更丰富信息，但有效整合多模态数据用于临床预测建模有挑战。

Method: 引入MoMA架构，使用多个大语言模型代理，包括专家代理将非文本模态转换为文本摘要，聚合代理生成统一多模态摘要，预测代理进行临床预测。

Result: 在三个预测任务上使用不同模态组合和预测设置的真实数据集评估，MoMA优于当前最先进方法。

Conclusion: MoMA在各种任务中具有更高的准确性和灵活性。

Abstract: Multimodal electronic health record (EHR) data provide richer, complementary
insights into patient health compared to single-modality data. However,
effectively integrating diverse data modalities for clinical prediction
modeling remains challenging due to the substantial data requirements. We
introduce a novel architecture, Mixture-of-Multimodal-Agents (MoMA), designed
to leverage multiple large language model (LLM) agents for clinical prediction
tasks using multimodal EHR data. MoMA employs specialized LLM agents
("specialist agents") to convert non-textual modalities, such as medical images
and laboratory results, into structured textual summaries. These summaries,
together with clinical notes, are combined by another LLM ("aggregator agent")
to generate a unified multimodal summary, which is then used by a third LLM
("predictor agent") to produce clinical predictions. Evaluating MoMA on three
prediction tasks using real-world datasets with different modality combinations
and prediction settings, MoMA outperforms current state-of-the-art methods,
highlighting its enhanced accuracy and flexibility across various tasks.

</details>


### [165] [Parameter-free entropy-regularized multi-view clustering with hierarchical feature selection](https://arxiv.org/abs/2508.05504)
*Kristina P. Sinaga,Sara Colantonio,Miin-Shen Yang*

Main category: cs.LG

TL;DR: 本文提出AMVFCM - U和AAMVFCM - U两种算法，构建无参数框架解决多视图聚类问题，在多个基准测试中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统多视图聚类方法存在手动调参和缺乏跨视图集成机制的问题，需要自动发现异构数据模式、处理高维特征和消除无关信息。

Method: 引入AMVFCM - U和AAMVFCM - U算法，用熵正则项替代模糊化参数，采用基于信噪比的正则化进行特征加权，AAMVFCM - U还通过自适应阈值进行特征和视图层面的降维。

Result: 在五个不同基准测试中优于15种最先进方法，AAMVFCM - U实现高达97%的计算效率提升，将维度降至原始大小的0.45%，并自动识别关键视图组合。

Conclusion: 所提出的算法在多视图聚类问题上表现出色，能够有效解决现有方法的不足。

Abstract: Multi-view clustering faces critical challenges in automatically discovering
patterns across heterogeneous data while managing high-dimensional features and
eliminating irrelevant information. Traditional approaches suffer from manual
parameter tuning and lack principled cross-view integration mechanisms. This
work introduces two complementary algorithms: AMVFCM-U and AAMVFCM-U, providing
a unified parameter-free framework. Our approach replaces fuzzification
parameters with entropy regularization terms that enforce adaptive cross-view
consensus. The core innovation employs signal-to-noise ratio based
regularization ($\delta_j^h = \frac{\bar{x}_j^h}{(\sigma_j^h)^2}$) for
principled feature weighting with convergence guarantees, coupled with
dual-level entropy terms that automatically balance view and feature
contributions. AAMVFCM-U extends this with hierarchical dimensionality
reduction operating at feature and view levels through adaptive thresholding
($\theta^{h^{(t)}} = \frac{d_h^{(t)}}{n}$). Evaluation across five diverse
benchmarks demonstrates superiority over 15 state-of-the-art methods. AAMVFCM-U
achieves up to 97% computational efficiency gains, reduces dimensionality to
0.45% of original size, and automatically identifies critical view combinations
for optimal pattern discovery.

</details>


### [166] [Tractable Sharpness-Aware Learning of Probabilistic Circuits](https://arxiv.org/abs/2508.05537)
*Hrithik Suresh,Sahil Sidheekh,Vishnu Shreeram M. P,Sriraam Natarajan,Narayanan C. Krishnan*

Main category: cs.LG

TL;DR: 分析概率电路过拟合问题，提出基于Hessian的正则化器，实验证明可改善泛化性能。


<details>
  <summary>Details</summary>
Motivation: 概率电路学习能力提升但易过拟合，尤其是数据有限时，需解决过拟合问题。

Method: 从对数似然景观角度分析过拟合原因，受神经网络锐度感知最小化启发，提出基于Hessian的正则化器，并利用其迹诱导梯度范数正则化。

Result: 能有效计算对数似然Hessian矩阵的迹，可得到EM算法简单的闭式参数更新，与基于梯度的学习方法无缝集成。

Conclusion: 该方法能引导概率电路找到更平坦的极小值，提高泛化性能。

Abstract: Probabilistic Circuits (PCs) are a class of generative models that allow
exact and tractable inference for a wide range of queries. While recent
developments have enabled the learning of deep and expressive PCs, this
increased capacity can often lead to overfitting, especially when data is
limited. We analyze PC overfitting from a log-likelihood-landscape perspective
and show that it is often caused by convergence to sharp optima that generalize
poorly. Inspired by sharpness aware minimization in neural networks, we propose
a Hessian-based regularizer for training PCs. As a key contribution, we show
that the trace of the Hessian of the log-likelihood-a sharpness proxy that is
typically intractable in deep neural networks-can be computed efficiently for
PCs. Minimizing this Hessian trace induces a gradient-norm-based regularizer
that yields simple closed-form parameter updates for EM, and integrates
seamlessly with gradient based learning methods. Experiments on synthetic and
real-world datasets demonstrate that our method consistently guides PCs toward
flatter minima, improves generalization performance.

</details>


### [167] [Adapting Vision-Language Models Without Labels: A Comprehensive Survey](https://arxiv.org/abs/2508.05547)
*Hao Dong,Lijun Sheng,Jian Liang,Ran He,Eleni Chatzi,Olga Fink*

Main category: cs.LG

TL;DR: 本文对无监督视觉语言模型（VLM）适应领域进行全面结构化概述，提出分类法，分析各范式方法和策略，回顾基准，指出挑战和方向。


<details>
  <summary>Details</summary>
Motivation: 现有VLM直接用于下游场景性能不佳，且缺乏无监督VLM适应的统一、面向任务的综述。

Method: 基于未标记视觉数据的可用性和性质提出分类法，将现有方法分为四类范式，分析核心方法和适应策略。

Result: 建立了对该领域的系统理解，回顾了不同应用的代表性基准。

Conclusion: 给出了该领域的研究现状，指出开放挑战和未来研究的有前景方向，并提供相关文献库。

Abstract: Vision-Language Models (VLMs) have demonstrated remarkable generalization
capabilities across a wide range of tasks. However, their performance often
remains suboptimal when directly applied to specific downstream scenarios
without task-specific adaptation. To enhance their utility while preserving
data efficiency, recent research has increasingly focused on unsupervised
adaptation methods that do not rely on labeled data. Despite the growing
interest in this area, there remains a lack of a unified, task-oriented survey
dedicated to unsupervised VLM adaptation. To bridge this gap, we present a
comprehensive and structured overview of the field. We propose a taxonomy based
on the availability and nature of unlabeled visual data, categorizing existing
approaches into four key paradigms: Data-Free Transfer (no data), Unsupervised
Domain Transfer (abundant data), Episodic Test-Time Adaptation (batch data),
and Online Test-Time Adaptation (streaming data). Within this framework, we
analyze core methodologies and adaptation strategies associated with each
paradigm, aiming to establish a systematic understanding of the field.
Additionally, we review representative benchmarks across diverse applications
and highlight open challenges and promising directions for future research. An
actively maintained repository of relevant literature is available at
https://github.com/tim-learn/Awesome-LabelFree-VLMs.

</details>


### [168] [Fairy$\pm i$: the First 2-bit Complex LLM with All Parameters in $\{\pm1, \pm i\}$](https://arxiv.org/abs/2508.05571)
*Feiyu Wang,Guoan Wang,Yihao Zhang,Shengfan Wang,Weitao Li,Bokai Huang,Shimao Chen,Zihan Jiang,Rui Xu,Tong Yang*

Main category: cs.LG

TL;DR: 提出新量化范式Fairy±i，用于复值大语言模型2位量化，超现有方法精度上限。


<details>
  <summary>Details</summary>
Motivation: 现有QAT研究以全精度模型精度为上限，作者想打破该上限。

Method: 利用复域表示优势提升全精度准确率，将权重映射到单位四次方根，形成2位表示。

Result: Fairy±i在PPL和下游任务上超现有2位量化方法上限，保持存储和计算效率。

Conclusion: 为极低比特约束下构建高精度实用大语言模型开辟新方向。

Abstract: Quantization-Aware Training (QAT) integrates quantization into the training
loop, enabling LLMs to learn robust low-bit representations, and is widely
recognized as one of the most promising research directions. All current QAT
research focuses on minimizing quantization error on full-precision models,
where the full-precision accuracy acts as an upper bound (accuracy ceiling). No
existing method has even attempted to surpass this ceiling. To break this
ceiling, we propose a new paradigm: raising the ceiling (full-precision model),
and then still quantizing it efficiently into 2 bits. We propose Fairy$\pm i$,
the first 2-bit quantization framework for complex-valued LLMs. Specifically,
our method leverages the representational advantages of the complex domain to
boost full-precision accuracy. We map weights to the fourth roots of unity
$\{\pm1, \pm i\}$, forming a perfectly symmetric and information-theoretically
optimal 2-bit representation. Importantly, each quantized weight has either a
zero real or imaginary part, enabling multiplication-free inference using only
additions and element swaps. Experimental results show that Fairy$\pm i$
outperforms the ceiling of existing 2-bit quantization approaches in terms of
both PPL and downstream tasks, while maintaining strict storage and compute
efficiency. This work opens a new direction for building highly accurate and
practical LLMs under extremely low-bit constraints.

</details>


### [169] [Iterative Learning of Computable Phenotypes for Treatment Resistant Hypertension using Large Language Models](https://arxiv.org/abs/2508.05581)
*Guilherme Seidyo Imai Aldeia,Daniel S. Herman,William G. La Cava*

Main category: cs.LG

TL;DR: 探索大语言模型生成可解释的可计算表型（CP）的能力，提出合成、执行、调试、指导策略，结果表明结合迭代学习可生成表现接近先进ML方法且所需训练样本少的程序。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在生成可解释的可计算表型方面的潜力未被充分挖掘，研究其能否为六种不同复杂度的临床表型生成准确且简洁的CP以实现可扩展的临床决策支持，改善高血压患者护理。

Method: 除评估零样本性能外，提出并测试了使用大语言模型结合数据驱动反馈生成并迭代优化CP的合成、执行、调试、指导策略。

Result: 大语言模型结合迭代学习能生成可解释且相当准确的程序，性能接近先进ML方法，同时所需训练样本显著减少。

Conclusion: 大语言模型结合迭代学习在生成可解释的可计算表型方面有良好表现，具有应用潜力。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities for
medical question answering and programming, but their potential for generating
interpretable computable phenotypes (CPs) is under-explored. In this work, we
investigate whether LLMs can generate accurate and concise CPs for six clinical
phenotypes of varying complexity, which could be leveraged to enable scalable
clinical decision support to improve care for patients with hypertension. In
addition to evaluating zero-short performance, we propose and test a
synthesize, execute, debug, instruct strategy that uses LLMs to generate and
iteratively refine CPs using data-driven feedback. Our results show that LLMs,
coupled with iterative learning, can generate interpretable and reasonably
accurate programs that approach the performance of state-of-the-art ML methods
while requiring significantly fewer training examples.

</details>


### [170] [Enhancing PyKEEN with Multiple Negative Sampling Solutions for Knowledge Graph Embedding Models](https://arxiv.org/abs/2508.05587)
*Claudia d'Amato,Ivan Diliso,Nicola Fanizzi,Zafar Saeed*

Main category: cs.LG

TL;DR: 本文为PyKEEN框架开发扩展，集成多种高级负采样器并进行实证研究。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱嵌入流行库多仅支持基本负采样策略，缺乏高级解决方案。

Method: 为PyKEEN框架开发扩展，在一致模块化架构中集成多种高级负采样器。

Result: 开发的扩展增强了PyKEEN，便于嵌入方法开发和定制，进行实证研究。

Conclusion: 开发的扩展有积极作用，研究为设计更有效策略提供见解。

Abstract: Embedding methods have become popular due to their scalability on link
prediction and/or triple classification tasks on Knowledge Graphs. Embedding
models are trained relying on both positive and negative samples of triples.
However, in the absence of negative assertions, these must be usually
artificially generated using various negative sampling strategies, ranging from
random corruption to more sophisticated techniques which have an impact on the
overall performance. Most of the popular libraries for knowledge graph
embedding, support only basic such strategies and lack advanced solutions. To
address this gap, we deliver an extension for the popular KGE framework PyKEEN
that integrates a suite of several advanced negative samplers (including both
static and dynamic corruption strategies), within a consistent modular
architecture, to generate meaningful negative samples, while remaining
compatible with existing PyKEEN -based workflows and pipelines. The developed
extension not only enhancesPyKEEN itself but also allows for easier and
comprehensive development of embedding methods and/or for their customization.
As a proof of concept, we present a comprehensive empirical study of the
developed extensions and their impact on the performance (link prediction
tasks) of different embedding methods, which also provides useful insights for
the design of more effective strategies

</details>


### [171] [Optimizing IoT Threat Detection with Kolmogorov-Arnold Networks (KANs)](https://arxiv.org/abs/2508.05591)
*Natalia Emelianova,Carlos Kamienski,Ronaldo C. Prati*

Main category: cs.LG

TL;DR: 研究探讨KANs用于物联网网络入侵检测的潜力，显示其优于传统MLP，与先进模型竞争且解释性更好。


<details>
  <summary>Details</summary>
Motivation: 物联网指数级增长带来安全问题，物联网网络成为网络攻击主要目标，需新的入侵检测方法。

Method: 研究使用可学习激活函数的Kolmogorov - Arnold Networks (KANs)进行物联网网络入侵检测。

Result: KANs优于传统MLP，与随机森林和XGBoost等先进模型有竞争力。

Conclusion: KANs在物联网网络入侵检测中表现良好，且具有更好的可解释性。

Abstract: The exponential growth of the Internet of Things (IoT) has led to the
emergence of substantial security concerns, with IoT networks becoming the
primary target for cyberattacks. This study examines the potential of
Kolmogorov-Arnold Networks (KANs) as an alternative to conventional machine
learning models for intrusion detection in IoT networks. The study demonstrates
that KANs, which employ learnable activation functions, outperform traditional
MLPs and achieve competitive accuracy compared to state-of-the-art models such
as Random Forest and XGBoost, while offering superior interpretability for
intrusion detection in IoT networks.

</details>


### [172] [Non-omniscient backdoor injection with a single poison sample: Proving the one-poison hypothesis for linear regression and linear classification](https://arxiv.org/abs/2508.05600)
*Thorsten Peinemann,Paula Arnold,Sebastian Berndt,Thomas Eisenbarth,Esfandiar Mohammadi*

Main category: cs.LG

TL;DR: 本文提出一毒样本假设，证明其在线性回归和分类中的有效性，分析不同情况对良性学习任务的影响并实验验证。


<details>
  <summary>Details</summary>
Motivation: 现有研究未明确成功后门攻击所需的毒数据量，本文旨在探讨仅用一个毒样本和有限背景知识进行后门攻击的情况。

Method: 提出一毒样本假设，对线性回归和线性分类证明该假设，分析不同情况对良性学习任务的影响，并用真实基准数据集实验验证。

Result: 证明了一毒样本假设，表明在特定情况下模型功能等价于排除毒数据训练的模型，其他情况下对良性学习任务影响有限。

Conclusion: 一个毒样本和有限背景知识可实现零后门误差的后门注入，且对良性学习任务性能影响不大。

Abstract: Backdoor injection attacks are a threat to machine learning models that are
trained on large data collected from untrusted sources; these attacks enable
attackers to inject malicious behavior into the model that can be triggered by
specially crafted inputs. Prior work has established bounds on the success of
backdoor attacks and their impact on the benign learning task, however, an open
question is what amount of poison data is needed for a successful backdoor
attack. Typical attacks either use few samples, but need much information about
the data points or need to poison many data points.
  In this paper, we formulate the one-poison hypothesis: An adversary with one
poison sample and limited background knowledge can inject a backdoor with zero
backdooring-error and without significantly impacting the benign learning task
performance. Moreover, we prove the one-poison hypothesis for linear regression
and linear classification. For adversaries that utilize a direction that is
unused by the benign data distribution for the poison sample, we show that the
resulting model is functionally equivalent to a model where the poison was
excluded from training. We build on prior work on statistical backdoor learning
to show that in all other cases, the impact on the benign learning task is
still limited. We also validate our theoretical results experimentally with
realistic benchmark data sets.

</details>


### [173] [Shuffle-R1: Efficient RL framework for Multimodal Large Language Models via Data-centric Dynamic Shuffle](https://arxiv.org/abs/2508.05612)
*Linghao Zhu,Yiran Guan,Dingkang Liang,Jianzhong Ju,Zhenbo Luo,Bin Qin,Jian Luan,Yuliang Liu,Xiang Bai*

Main category: cs.LG

TL;DR: 本文指出当前强化学习（RL）微调多模态大语言模型（MLLM）推理能力时存在优势坍塌和滚动沉默问题，提出 Shuffle - R1 框架解决这些问题，实验显示该框架表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 当前 RL 微调 MLLM 推理能力时，存在优势坍塌和滚动沉默问题，导致训练效率低，需要解决这些问题以提升训练效率。

Method: 提出 Shuffle - R1 框架，包含成对轨迹采样和基于优势的轨迹洗牌，动态重构轨迹采样和批量组成。

Result: 在多个推理基准测试中，Shuffle - R1 框架始终以最小开销优于强大的 RL 基线。

Conclusion: 数据中心的调整对于 MLLM 中更高效的 RL 训练很重要。

Abstract: Reinforcement learning (RL) has emerged as an effective post-training
paradigm for enhancing the reasoning capabilities of multimodal large language
model (MLLM). However, current RL pipelines often suffer from training
inefficiencies caused by two underexplored issues: Advantage Collapsing, where
most advantages in a batch concentrate near zero, and Rollout Silencing, where
the proportion of rollouts contributing non-zero gradients diminishes over
time. These issues lead to suboptimal gradient updates and hinder long-term
learning efficiency. To address these issues, we propose Shuffle-R1, a simple
yet principled framework that improves RL fine-tuning efficiency by dynamically
restructuring trajectory sampling and batch composition. It introduces (1)
Pairwise Trajectory Sampling, which selects high-contrast trajectories with
large advantages to improve gradient signal quality, and (2) Advantage-based
Trajectory Shuffle, which increases exposure of valuable rollouts through
informed batch reshuffling. Experiments across multiple reasoning benchmarks
show that our framework consistently outperforms strong RL baselines with
minimal overhead. These results highlight the importance of data-centric
adaptations for more efficient RL training in MLLM.

</details>


### [174] [On the Generalization of SFT: A Reinforcement Learning Perspective with Reward Rectification](https://arxiv.org/abs/2508.05629)
*Yongliang Wu,Yizhou Zhou,Zhou Ziheng,Yingzhe Peng,Xinyu Ye,Xinting Hu,Wenbo Zhu,Lu Qi,Ming-Hsuan Yang,Xu Yang*

Main category: cs.LG

TL;DR: 提出动态微调（DFT）改进大语言模型的监督微调（SFT），在多基准测试中表现优于标准SFT，代码将开源。


<details>
  <summary>Details</summary>
Motivation: 解决监督微调（SFT）与强化学习（RL）相比泛化能力有限的问题。

Method: 通过数学分析发现标准SFT梯度存在问题，提出动态微调（DFT），用每个标记的概率动态重新缩放目标函数来稳定梯度更新。

Result: 在多个具有挑战性的基准测试和基础模型中显著优于标准SFT，在离线RL设置中也有有竞争力的结果。

Conclusion: 该工作将理论见解与实际解决方案相结合，大幅提升了SFT性能。

Abstract: We present a simple yet theoretically motivated improvement to Supervised
Fine-Tuning (SFT) for the Large Language Model (LLM), addressing its limited
generalization compared to reinforcement learning (RL). Through mathematical
analysis, we reveal that standard SFT gradients implicitly encode a problematic
reward structure that may severely restrict the generalization capabilities of
model. To rectify this, we propose Dynamic Fine-Tuning (DFT), stabilizing
gradient updates for each token by dynamically rescaling the objective function
with the probability of this token. Remarkably, this single-line code change
significantly outperforms standard SFT across multiple challenging benchmarks
and base models, demonstrating greatly improved generalization. Additionally,
our approach shows competitive results in offline RL settings, offering an
effective yet simpler alternative. This work bridges theoretical insight and
practical solutions, substantially advancing SFT performance. The code will be
available at https://github.com/yongliang-wu/DFT.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [175] [Optimality Principles and Neural Ordinary Differential Equations-based Process Modeling for Distributed Control](https://arxiv.org/abs/2508.04799)
*Michael R. Wartmann,B. Erik Ydstie*

Main category: cs.NE

TL;DR: 提出一个过程建模框架，实现数据驱动算法与经典模型集成，以库存控制系统为例展示集成方法。


<details>
  <summary>Details</summary>
Motivation: 解决如何将新的数据驱动方法与经典过程模型和控制自然集成的问题。

Method: 通过一致拓扑性质和广延量守恒构建框架，用连通矩阵和网络图表示单元互连，推导系统目标函数，结合稀疏深度神经网络，用伴随方法和自适应ODE求解器学习方程。

Result: 用简单库存控制系统示例展示了过程拓扑与神经网络常微分方程模型的集成，得到可用于模型预测控制算法的状态空间模型。

Conclusion: 所提出的形式主义允许将拓扑的基本守恒性质与从数据中学习到的动态关系进行集成。

Abstract: Most recent advances in machine learning and analytics for process control
pose the question of how to naturally integrate new data-driven methods with
classical process models and control. We propose a process modeling framework
enabling integration of data-driven algorithms through consistent topological
properties and conservation of extensive quantities. Interconnections among
process network units are represented through connectivity matrices and network
graphs. We derive the system's natural objective function equivalent to the
non-equilibrium entropy production in a steady state system as a driving force
for the process dynamics. We illustrate how distributed control and
optimization can be implemented into process network structures and how control
laws and algorithms alter the system's natural equilibrium towards engineered
objectives. The basic requirement is that the flow conditions can be expressed
in terms of conic sector (passivity) conditions. Our formalism allows
integration of fundamental conservation properties from topology with learned
dynamic relations from data through sparse deep neural networks.
  We demonstrate in a practical example of a simple inventory control system
how to integrate the basic topology of a process with a neural network ordinary
differential equation model. The system specific constitutive equations are
left undescribed and learned by the neural ordinary differential equation
algorithm using the adjoint method in combination with an adaptive ODE solver
from synthetic time-series data. The resulting neural network forms a state
space model for use in e.g. a model predictive control algorithm.

</details>


### [176] [Modelling the emergence of open-ended technological evolution](https://arxiv.org/abs/2508.04828)
*James Winters,Mathieu Charbonneau*

Main category: cs.NE

TL;DR: 本文提出资源生产源于技术系统与搜索空间的相互作用，建立宏观模型，表明开放式技术进化极为罕见，需二者共同进化。


<details>
  <summary>Details</summary>
Motivation: 探究人类能以开放式方式集体累积改进技术的原因，以及资源生产的来源。

Method: 提出资源生产源于技术系统和搜索空间相互作用的前提，建立宏观层面模型，操控文化进化动态中的随机和选择过程。

Result: 开放式增长极为罕见、具有历史偶然性，仅在技术系统和搜索空间共同进化时才可能发生。

Conclusion: 只有共同进化动态维持有效技术系统、支持搜索空间持续扩展并增加资源供应时，才会出现开放式技术进化。

Abstract: Humans stand alone in terms of their potential to collectively and
cumulatively improve technologies in an open-ended manner. This open-endedness
provides societies with the ability to continually expand their resources and
to increase their capacity to store, transmit and process information at a
collective-level. Here, we propose that the production of resources arises from
the interaction between technological systems (a society's repertoire of
interdependent skills, techniques and artifacts) and search spaces (the
aggregate collection of needs, problems and goals within a society). Starting
from this premise we develop a macro-level model wherein both technological
systems and search spaces are subject to cultural evolutionary dynamics. By
manipulating the extent to which these dynamics are characterised by stochastic
or selection-like processes, we demonstrate that open-ended growth is extremely
rare, historically contingent and only possible when technological systems and
search spaces co-evolve. Here, stochastic factors must be strong enough to
continually perturb the dynamics into a far-from-equilibrium state, whereas
selection-like factors help maintain effectiveness and ensure the sustained
production of resources. Only when this co-evolutionary dynamic maintains
effective technological systems, supports the ongoing expansion of the search
space and leads to an increased provision of resources do we observe open-ended
technological evolution.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [177] [Mapping Sparse Triangular Solves to GPUs via Fine-grained Domain Decomposition](https://arxiv.org/abs/2508.04917)
*Atharva Gondhalekar,Kjetil Haugen,Thomas Gibson,Wu-chun Feng*

Main category: cs.PF

TL;DR: 本文利用细粒度域分解使三角求解适应GPU架构，提升了稀疏线性系统求解速度。


<details>
  <summary>Details</summary>
Motivation: 稀疏线性系统使用预条件迭代法求解时，通过稀疏三角求解应用预条件器存在因不规则内存访问和数据依赖导致的瓶颈。

Method: 开发细粒度域分解策略，生成非重叠子域，将每个子域分配给一个线程块，使子域向量适合GPU共享内存。

Result: 与其他使用ROCm™软件栈的先进实现相比，在AMD Instinct™ MI210 GPU上三角求解实现10.7倍加速，ILU0预条件双共轭梯度稳定（BiCGSTAB）求解器实现3.2倍加速。

Conclusion: 细粒度域分解策略能有效提升稀疏线性系统在GPU上的求解速度。

Abstract: Sparse linear systems are typically solved using preconditioned iterative
methods, but applying preconditioners via sparse triangular solves introduces
bottlenecks due to irregular memory accesses and data dependencies. This work
leverages fine-grained domain decomposition to adapt triangular solves to the
GPU architecture. We develop a fine-grained domain decomposition strategy that
generates non-overlapping subdomains, increasing parallelism in the application
of preconditioner at the expense of a modest increase in the iteration count
for convergence. Each subdomain is assigned to a thread block and is sized such
that the subdomain vector fits in the GPU shared memory, eliminating the need
for inter-block synchronization and reducing irregular global memory accesses.
Compared to other state-of-the-art implementations using the ROCm$^{\text{TM}}$
software stack, we achieve a 10.7$\times$ speedup for triangular solves and a
3.2$\times$ speedup for the ILU0-preconditioned biconjugate gradient stabilized
(BiCGSTAB) solver on the AMD Instinct$^{\text{TM}}$ MI210 GPU.

</details>


### [178] [Back to Bits: Extending Shannon's communication performance framework to computing](https://arxiv.org/abs/2508.05621)
*Max Hawkins,Richard Vuduc*

Main category: cs.PF

TL;DR: 提出基于信息论的计算性能单元，解决传统指标无法适应现代计算系统复杂性问题。


<details>
  <summary>Details</summary>
Motivation: 现代计算系统日益多样化，传统的浮点运算等指标无法准确衡量其复杂性。

Method: 将计算视为信息通过通道的转换，根据系统输入输出的互信息定义性能。

Result: 该框架为评估性能提供了原则性、与实现无关的基础。

Conclusion: 基于信息论的新计算性能单元是评估现代计算系统性能的有效方法。

Abstract: This work proposes a novel computing performance unit grounded in information
theory. Modern computing systems are increasingly diverse, supporting
low-precision formats, hardware specialization, and emerging paradigms such as
analog, quantum, and reversible logic. Traditional metrics like floating-point
operations (flops) no longer accurately capture this complexity. We frame
computing as the transformation of information through a channel and define
performance in terms of the mutual information between a system's inputs and
outputs. This approach measures not just the quantity of data processed, but
the amount of meaningful information encoded, manipulated, and retained through
computation. Our framework provides a principled, implementation-agnostic
foundation for evaluating performance.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [179] [Automated File-Level Logging Generation for Machine Learning Applications using LLMs: A Case Study using GPT-4o Mini](https://arxiv.org/abs/2508.04820)
*Mayra Sofia Ruiz Rodriguez,SayedHassan Khatoonabadi,Emad Shihab*

Main category: cs.SE

TL;DR: 研究评估GPT - 4o mini为机器学习项目在文件级别生成日志语句的能力，发现其有一定效果但存在过度日志记录等问题。


<details>
  <summary>Details</summary>
Motivation: 先前研究多关注代码函数中的日志评估，文件级日志生成，尤其是机器学习应用中的文件级日志生成研究不足，而全面日志可提高可靠性。

Method: 收集171个含4073个有日志语句Python文件的机器学习仓库，移除原日志后让大语言模型生成日志，评估日志位置、级别、变量和文本质量，手动分析生成日志样本。

Result: 大语言模型在63.91%的情况下引入日志位置与人类相同，但过度日志记录率达82.66%；手动分析发现文件级日志存在函数首尾过度日志、大代码块内难日志记录和与项目日志约定不符等问题。

Conclusion: 大语言模型在为完整文件生成日志方面有前景，但实际应用中存在的局限性有待解决。

Abstract: Logging is essential in software development, helping developers monitor
system behavior and aiding in debugging applications. Given the ability of
large language models (LLMs) to generate natural language and code, researchers
are exploring their potential to generate log statements. However, prior work
focuses on evaluating logs introduced in code functions, leaving file-level log
generation underexplored -- especially in machine learning (ML) applications,
where comprehensive logging can enhance reliability. In this study, we evaluate
the capacity of GPT-4o mini as a case study to generate log statements for ML
projects at file level. We gathered a set of 171 ML repositories containing
4,073 Python files with at least one log statement. We identified and removed
the original logs from the files, prompted the LLM to generate logs for them,
and evaluated both the position of the logs and log level, variables, and text
quality of the generated logs compared to human-written logs. In addition, we
manually analyzed a representative sample of generated logs to identify common
patterns and challenges. We find that the LLM introduces logs in the same place
as humans in 63.91% of cases, but at the cost of a high overlogging rate of
82.66%. Furthermore, our manual analysis reveals challenges for file-level
logging, which shows overlogging at the beginning or end of a function,
difficulty logging within large code blocks, and misalignment with
project-specific logging conventions. While the LLM shows promise for
generating logs for complete files, these limitations remain to be addressed
for practical implementation.

</details>


### [180] [Automated Bug Frame Retrieval from Gameplay Videos Using Vision-Language Models](https://arxiv.org/abs/2508.04895)
*Wentao Lu,Alexander Senchenko,Abram Hindle,Cor-Paul Bezemer*

Main category: cs.SE

TL;DR: 提出自动化流程将游戏视频缩减为单帧以匹配bug描述，减少人工审核，评估显示有一定效果。


<details>
  <summary>Details</summary>
Motivation: 现代游戏工作室快速发布版本和补丁产生大量含视频的bug报告，人工审核劳动密集、速度慢且难扩展。

Method: 先用FFmpeg提取关键帧，再用视觉 - 语言模型（GPT - 4o）对关键帧排序并选最具代表性帧。

Result: 使用真实游戏视频和bug报告评估，整体F1分数0.79，准确率0.89，不同bug类别表现有差异。

Conclusion: 该方法显著减少人工审核，加速分类和回归检查，对游戏行业QA团队和开发者有实际益处。

Abstract: Modern game studios deliver new builds and patches at a rapid pace,
generating thousands of bug reports, many of which embed gameplay videos. To
verify and triage these bug reports, developers must watch the submitted
videos. This manual review is labour-intensive, slow, and hard to scale. In
this paper, we introduce an automated pipeline that reduces each video to a
single frame that best matches the reported bug description, giving developers
instant visual evidence that pinpoints the bug.
  Our pipeline begins with FFmpeg for keyframe extraction, reducing each video
to a median of just 1.90% of its original frames while still capturing bug
moments in 98.79 of cases. These keyframes are then evaluated by a
vision--language model (GPT-4o), which ranks them based on how well they match
the textual bug description and selects the most representative frame. We
evaluated this approach using real-world developer-submitted gameplay videos
and JIRA bug reports from a popular First-Person Shooter (FPS) game. The
pipeline achieves an overall F1 score of 0.79 and Accuracy of 0.89 for the
top-1 retrieved frame. Performance is highest for the Lighting & Shadow (F1 =
0.94), Physics & Collision (0.86), and UI & HUD (0.83) bug categories, and
lowest for Animation & VFX (0.51).
  By replacing video viewing with an immediately informative image, our
approach dramatically reduces manual effort and speeds up triage and regression
checks, offering practical benefits to quality assurance (QA) teams and
developers across the game industry.

</details>


### [181] [Charting Uncertain Waters: A Socio-Technical Framework for Navigating GenAI's Impact on Open Source Communities](https://arxiv.org/abs/2508.04921)
*Zixuan Feng,Reed Milewicz,Emerson Murphy-Hill,Tyler Menezes,Alexander Serebrenik,Igor Steinmacher,Anita Sarma*

Main category: cs.SE

TL;DR: 生成式AI变革开源软件，研究用社会技术框架探索机遇与风险助生态发展。


<details>
  <summary>Details</summary>
Motivation: 生成式AI快速变革开源软件，缺乏明确框架使社区面临复杂和不确定性，威胁协作精神。

Method: 采用受麦克卢汉四定律启发的社会技术框架，进行情景驱动的概念探索。

Result: 揭示了生成式AI对开源软件开发在软件实践、文档、社区参与和治理四个领域造成破坏时的机遇与风险。

Conclusion: 开源软件领导者和研究人员可借此主动塑造生态系统未来，而非被动应对技术变革。

Abstract: Open Source Software communities face a wave of uncertainty as Generative AI
rapidly transforms how software is created, maintained, and governed. Without
clear frameworks, communities risk being overwhelmed by the complexity and
ambiguity introduced by GenAI, threatening the collaborative ethos that
underpins OSS. We conduct a scenario-driven, conceptual exploration using a
socio-technical framework inspired by McLuhan's Tetrad to surface both risks
and opportunities for community resilience amid GenAI-driven disruption of OSS
development across four domains: software practices, documentation, community
engagement, and governance. By adopting this lens, OSS leaders and researchers
can proactively shape the future of their ecosystems, rather than simply
reacting to technological upheaval.

</details>


### [182] [Taxonomy of Faults in Attention-Based Neural Networks](https://arxiv.org/abs/2508.04925)
*Sigma Jahan,Saurabh Singh Rajput,Tushar Sharma,Mohammad Masudur Rahman*

Main category: cs.SE

TL;DR: 文章针对现有深度学习故障分类法无法涵盖注意力机制独特故障的问题，对基于注意力的神经网络故障进行全面实证研究，提出新分类法和诊断启发式方法。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习故障分类法不能充分涵盖注意力机制引入的独特故障，使从业者缺乏诊断指导。

Method: 对来自10个框架96个项目的555个真实世界故障进行系统分析。

Result: 超半数ABNN故障源于注意力架构独特机制；开发含7个注意力特定故障类别的新分类法；识别出能解释33.0%注意力特定故障的4个诊断启发式方法。

Conclusion: 提出首个针对基于注意力模型的系统诊断指导。

Abstract: Attention mechanisms are at the core of modern neural architectures, powering
systems ranging from ChatGPT to autonomous vehicles and driving a major
economic impact. However, high-profile failures, such as ChatGPT's nonsensical
outputs or Google's suspension of Gemini's image generation due to attention
weight errors, highlight a critical gap: existing deep learning fault
taxonomies might not adequately capture the unique failures introduced by
attention mechanisms. This gap leaves practitioners without actionable
diagnostic guidance. To address this gap, we present the first comprehensive
empirical study of faults in attention-based neural networks (ABNNs). Our work
is based on a systematic analysis of 555 real-world faults collected from 96
projects across ten frameworks, including GitHub, Hugging Face, and Stack
Overflow. Through our analysis, we develop a novel taxonomy comprising seven
attention-specific fault categories, not captured by existing work. Our results
show that over half of the ABNN faults arise from mechanisms unique to
attention architectures. We further analyze the root causes and manifestations
of these faults through various symptoms. Finally, by analyzing symptom-root
cause associations, we identify four evidence-based diagnostic heuristics that
explain 33.0% of attention-specific faults, offering the first systematic
diagnostic guidance for attention-based models.

</details>


### [183] [Generative AI for Object-Oriented Programming: Writing the Right Code and Reasoning the Right Logic](https://arxiv.org/abs/2508.05005)
*Gang Xu,Airong Wang,Yushan Pan*

Main category: cs.SE

TL;DR: 本文聚焦大语言模型（LLMs）与面向对象编程（OOP）的结合，旨在填补相关研究空白，提出增强编程体验的愿景和方法。


<details>
  <summary>Details</summary>
Motivation: 当前人工智能研究中LLMs发展迅速且应用广泛，但LLMs与OOP的交叉领域研究不足，缺乏对LLMs提升OOP学习和代码编写效果及评估相关工具的了解。

Method: 从OOP任务的关键利益相关者（程序员、新手和有经验的程序员）角度提出愿景，确定典型编码工作流程中LLMs集成能带来显著好处的关键节点。

Result: 确定了LLMs集成可带来显著好处的关键节点，提出增强现有逻辑推理和代码编写的方法。

Conclusion: 通过上述研究可增强编程体验。

Abstract: We find ourselves in the midst of an explosion in artificial intelligence
research, particularly with large language models (LLMs). These models have
diverse applications spanning finance, commonsense knowledge graphs, medicine,
and visual analysis. In the world of Object-Oriented Programming(OOP), a robust
body of knowledge and methods has been developed for managing complex tasks
through object-oriented thinking. However, the intersection of LLMs with OOP
remains an underexplored territory. Empirically, we currently possess limited
understanding of how LLMs can enhance the effectiveness of OOP learning and
code writing, as well as how we can evaluate such AI-powered tools. Our work
aims to address this gap by presenting a vision from the perspectives of key
stakeholders involved in an OOP task: programmers, mariners, and experienced
programmers. We identify critical junctures within typical coding workflows
where the integration of LLMs can offer significant benefits. Furthermore, we
propose ways to augment existing logical reasoning and code writing, ultimately
enhancing the programming experience.

</details>


### [184] [An ML-based Approach to Predicting Software Change Dependencies: Insights from an Empirical Study on OpenStack](https://arxiv.org/abs/2508.05034)
*Arabat,Ali,Sayagh,Mohammed,Hassine,Jameleddine*

Main category: cs.SE

TL;DR: 随着软件系统复杂度增加，依赖管理至关重要。研究OpenStack发现大量变更相互依赖，多在代码审查阶段识别。提出半自动化方法，用两个ML模型预测和识别依赖，模型表现良好。


<details>
  <summary>Details</summary>
Motivation: 现代软件系统中依赖跨组件跨团队，带来开发和部署挑战，需解决依赖管理问题。

Method: 对OpenStack进行初步研究，提出利用两个ML模型的半自动化方法，一个预测依赖可能性，一个识别依赖变更对。

Result: 研究发现OpenStack过去10年大量软件变更相互依赖，51.08%的依赖在代码审查阶段识别，平均延迟5.06小时。两个模型平均AUC分数分别为79.33%和91.89%，Brier分数分别为0.11和0.014。

Conclusion: 提出的半自动化方法中，第二个模型在各类对上有较好的top - k召回率，但top - k精度有提升空间。

Abstract: As software systems grow in complexity, accurately identifying and managing
dependencies among changes becomes increasingly critical. For instance, a
change that leverages a function must depend on the change that introduces it.
Establishing such dependencies allows CI/CD pipelines to build and orchestrate
changes effectively, preventing build failures and incomplete feature
deployments. In modern software systems, dependencies often span multiple
components across teams, creating challenges for development and deployment.
They serve various purposes, from enabling new features to managing
configurations, and can even involve traditionally independent changes like
documentation updates. To address these challenges, we conducted a preliminary
study on dependency management in OpenStack, a large-scale software system. Our
study revealed that a substantial portion of software changes in OpenStack over
the past 10 years are interdependent. Surprisingly, 51.08% of these
dependencies are identified during the code review phase-after a median delay
of 5.06 hours-rather than at the time of change creation. Developers often
spend a median of 57.12 hours identifying dependencies, searching among a
median of 463 other changes. To help developers proactively identify
dependencies, we propose a semi-automated approach that leverages two ML
models. The first model predicts the likelihood of dependencies among changes,
while the second identifies the exact pairs of dependent changes. Our proposed
models demonstrate strong performance, achieving average AUC scores of 79.33%
and 91.89%, and Brier scores of 0.11 and 0.014, respectively. Indeed, the
second model has a good top-k recall across all types of pairs, while the top-k
precision has room for improvement.

</details>


### [185] [LadyBug: A GitHub Bot for UI-Enhanced Bug Localization in Mobile Apps](https://arxiv.org/abs/2508.05085)
*Junayed Mahmud,James Chen,Terry Achille,Camilo Alvarez-Velez,Darren Dean Bansil,Patrick Ijieh,Samar Karanch,Nadeeshan De Silva,Oscar Chaparro,Andrian Marcus,Kevin Moran*

Main category: cs.SE

TL;DR: 本文介绍自动定位安卓应用程序漏洞的GitHub机器人LadyBug，结合UI交互信息与文本检索，经评估优于基于文本检索的基线。


<details>
  <summary>Details</summary>
Motivation: 解决安卓应用程序漏洞定位问题。

Method: 结合UI交互信息与文本检索，利用开发者上传的重现轨迹和原始漏洞描述文本，从项目中检索可能包含漏洞的文件。使用RedWing基准测试进行评估。

Result: LadyBug优于基于文本检索的基线，利用UI信息大幅提高定位准确性。

Conclusion: LadyBug是一个有效的安卓应用程序漏洞定位工具，且为开源工具。

Abstract: This paper introduces LadyBug, a GitHub bot that automatically localizes bugs
for Android apps by combining UI interaction information with text retrieval.
LadyBug connects to an Android app's GitHub repository, and is triggered when a
bug is reported in the corresponding issue tracker. Developers can then record
a reproduction trace for the bug on a device or emulator and upload the trace
to LadyBug via the GitHub issue tracker. This enables LadyBug to utilize both
the text from the original bug description, and UI information from the
reproduction trace to accurately retrieve a ranked list of files from the
project that most likely contain the reported bug.
  We empirically evaluated LadyBug using an automated testing pipeline and
benchmark called RedWing that contains 80 fully-localized and reproducible bug
reports from 39 Android apps. Our results illustrate that LadyBug outperforms
text-retrieval-based baselines and that the utilization of UI information leads
to a substantial increase in localization accuracy. LadyBug is an open-source
tool, available at https://github.com/LadyBugML/ladybug.
  A video showing the capabilities of Ladybug can be viewed here:
https://youtu.be/hI3tzbRK0Cw

</details>


### [186] [Posterior-GRPO: Rewarding Reasoning Processes in Code Generation](https://arxiv.org/abs/2508.05170)
*Lishui Fan,Yu Zhang,Mouxiang Chen,Zhongxin Liu*

Main category: cs.SE

TL;DR: 本文提出统一框架将推理过程质量融入强化学习，开发推理评估基准，引入奖励模型训练方法和新的强化学习方法，在代码生成和数学任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型代码生成的强化学习范式依赖测试用例结果奖励，忽略中间推理过程质量，直接监督推理过程易受奖励欺骗。

Method: 开发LCB - RB基准；引入基于优化 - 降级的奖励模型训练方法；提出Posterior - GRPO强化学习方法。

Result: 7B参数奖励模型在LCB - RB上达SOTA，泛化性好；7B参数模型用P - GRPO在代码生成任务中表现优，超仅基于结果的基线4.5%，与GPT - 4 - Turbo相当，方法可拓展到数学任务。

Conclusion: 所提统一框架能有效将推理过程质量融入强化学习，可减少奖励欺骗，方法具有泛化性，模型、数据集和代码公开。

Abstract: Reinforcement learning (RL) has significantly advanced code generation for
large language models (LLMs). However, current paradigms rely on outcome-based
rewards from test cases, neglecting the quality of the intermediate reasoning
process. While supervising the reasoning process directly is a promising
direction, it is highly susceptible to reward hacking, where the policy model
learns to exploit the reasoning reward signal without improving final outcomes.
To address this, we introduce a unified framework that can effectively
incorporate the quality of the reasoning process during RL. First, to enable
reasoning evaluation, we develop LCB-RB, a benchmark comprising preference
pairs of superior and inferior reasoning processes. Second, to accurately score
reasoning quality, we introduce an Optimized-Degraded based (OD-based) method
for reward model training. This method generates high-quality preference pairs
by systematically optimizing and degrading initial reasoning paths along
curated dimensions of reasoning quality, such as factual accuracy, logical
rigor, and coherence. A 7B parameter reward model with this method achieves
state-of-the-art (SOTA) performance on LCB-RB and generalizes well to other
benchmarks. Finally, we introduce Posterior-GRPO (P-GRPO), a novel RL method
that conditions process-based rewards on task success. By selectively applying
rewards to the reasoning processes of only successful outcomes, P-GRPO
effectively mitigates reward hacking and aligns the model's internal reasoning
with final code correctness. A 7B parameter model with P-GRPO achieves superior
performance across diverse code generation tasks, outperforming outcome-only
baselines by 4.5%, achieving comparable performance to GPT-4-Turbo. We further
demonstrate the generalizability of our approach by extending it to
mathematical tasks. Our models, dataset, and code are publicly available.

</details>


### [187] [AI-assisted JSON Schema Creation and Mapping](https://arxiv.org/abs/2508.05192)
*Felix Neubauer,Jürgen Pleiss,Benjamin Uekermann*

Main category: cs.SE

TL;DR: 提出结合大语言模型与确定性技术的混合方法实现JSON Schema创建等功能，集成到MetaConfigurator工具，以化学领域为例展示其应用，降低非专家进行结构化数据建模和集成的门槛。


<details>
  <summary>Details</summary>
Motivation: 许多领域缺乏标准化模型，创建模型对非专家是重大障碍。

Method: 采用结合大语言模型与确定性技术的混合方法，将相关功能集成到MetaConfigurator工具，用大语言模型生成模式映射并通过确定性执行保证可扩展性和可靠性。

Result: 在化学领域应用示例中展示了方法的适用性。

Conclusion: 结合自然语言交互和确定性保障，显著降低非专家进行结构化数据建模和数据集成的门槛。

Abstract: Model-Driven Engineering (MDE) places models at the core of system and data
engineering processes. In the context of research data, these models are
typically expressed as schemas that define the structure and semantics of
datasets. However, many domains still lack standardized models, and creating
them remains a significant barrier, especially for non-experts. We present a
hybrid approach that combines large language models (LLMs) with deterministic
techniques to enable JSON Schema creation, modification, and schema mapping
based on natural language inputs by the user. These capabilities are integrated
into the open-source tool MetaConfigurator, which already provides visual model
editing, validation, code generation, and form generation from models. For data
integration, we generate schema mappings from heterogeneous JSON, CSV, XML, and
YAML data using LLMs, while ensuring scalability and reliability through
deterministic execution of generated mapping rules. The applicability of our
work is demonstrated in an application example in the field of chemistry. By
combining natural language interaction with deterministic safeguards, this work
significantly lowers the barrier to structured data modeling and data
integration for non-experts.

</details>


### [188] [STEPWISE-CODEX-Bench: Evaluating Complex Multi-Function Comprehension and Fine-Grained Execution Reasoning](https://arxiv.org/abs/2508.05193)
*Kaiwen Yan,Yuhang Chang,Zirui Guo,Yaling Mou,Jiang Ming,Jingwei Sun*

Main category: cs.SE

TL;DR: 提出新基准STEPWISE - CODEX - Bench (SX - Bench)评估大语言模型代码理解和推理能力，证明其高区分度并发布自动化生成管道。


<details>
  <summary>Details</summary>
Motivation: 现有基准难以系统评估大语言模型代码理解和推理能力，主流基准区分度有限。

Method: 设计SX - Bench基准，关注多函数理解和细粒度执行推理，以计算步骤为执行单元；使用自动化管道生成基准。

Result: 对20多个主流模型评估显示SX - Bench区分度高，如OpenAI - O3在Hard - Reasoning任务准确率远低于之前基准。

Conclusion: SX - Bench将代码评估从单函数验证推进到多函数动态推理，为评估高级代码智能模型提供关键工具。

Abstract: In recent years, large language models (LLMs) have made significant progress
in code intelligence, yet systematically evaluating their code understanding
and reasoning abilities remains challenging. Mainstream benchmarks such as
HumanEval and MBPP primarily assess functional correctness, while reasoning
benchmarks like CRUXEVAL are limited to single-function, low-complexity
scenarios. As a result, advanced models achieve nearly saturated scores,
limiting their discriminative power. To address this, we present
STEPWISE-CODEX-Bench (SX-Bench), a novel benchmark designed for complex
multi-function understanding and fine-grained execution reasoning. SX-Bench
features tasks involving collaboration among multiple sub-functions (e.g.,
chained calls, nested loops), shifting evaluation towards overall control and
data flow modeling. It defines "computation steps" as the minimal execution
unit and requires models to predict the total number of steps in reasoning
tasks, thereby assessing a model's in-depth understanding of dynamic execution
beyond simple I/O matching. Evaluation on over 20 mainstream models (including
14 reasoning-enhanced models) demonstrates that SX-Bench is highly
discriminative: even the state-of-the-art OpenAI-O3 achieves only 78.37 percent
accuracy on Hard-Reasoning tasks, much lower than its saturated scores on
previous benchmarks, thereby revealing bottlenecks in complex and fine-grained
reasoning. We also release an automated pipeline combining program synthesis,
symbolic execution, and LLM-aided validation for efficient benchmark generation
and quality assurance. SX-Bench advances code evaluation from "single-function
verification" to "multi-function dynamic reasoning," providing a key tool for
the in-depth assessment of advanced code intelligence models.

</details>


### [189] [EvoGraph: Hybrid Directed Graph Evolution toward Software 3.0](https://arxiv.org/abs/2508.05199)
*Igor Costa,Christopher Baran*

Main category: cs.SE

TL;DR: 介绍EvoGraph框架，可让软件系统自我进化，在多基准测试表现好，扩展应用后降低成本，为Software 3.0提供路径。


<details>
  <summary>Details</summary>
Motivation: 解决遗留系统现代化中的隐式契约、性能保留和集成演进等问题，推动软件向可持续适应且可控的Software 3.0发展。

Method: 将工件表示为有向图，应用小语言模型驱动的变异算子，用多目标适应度选择幸存者，扩展时利用特定语言的小语言模型。

Result: 在三个基准测试中，修复83%安全漏洞，COBOL转Java达93%功能等效，文档更新快；与基线相比，延迟降低40%，特性前置时间降为七分之一；扩展应用后语义等效达82 - 96%，计算成本降90%。

Conclusion: EvoGraph为实现Software 3.0提供了一条可行的实践路径。

Abstract: We introduce **EvoGraph**, a framework that enables software systems to
evolve their own source code, build pipelines, documentation, and tickets.
EvoGraph represents every artefact in a typed directed graph, applies learned
mutation operators driven by specialized small language models (SLMs), and
selects survivors with a multi-objective fitness. On three benchmarks, EvoGraph
fixes 83% of known security vulnerabilities, translates COBOL to Java with 93%
functional equivalence (test verified), and maintains documentation freshness
within two minutes. Experiments show a 40% latency reduction and a sevenfold
drop in feature lead time compared with strong baselines. We extend our
approach to **evoGraph**, leveraging language-specific SLMs for modernizing
.NET, Lisp, CGI, ColdFusion, legacy Python, and C codebases, achieving 82-96%
semantic equivalence across languages while reducing computational costs by 90%
compared to large language models. EvoGraph's design responds to empirical
failure modes in legacy modernization, such as implicit contracts, performance
preservation, and integration evolution. Our results suggest a practical path
toward Software 3.0, where systems adapt continuously yet remain under
measurable control.

</details>


### [190] [A Conceptual Model and Methodology for Sustainability-aware, IoT-enhanced Business Processes](https://arxiv.org/abs/2508.05301)
*Victoria Torres Bosch,Ronny Seiger,Manuela Albert Albiol,Antoni Mestre Gascon,Pedro Jose Valderas Aranda*

Main category: cs.SE

TL;DR: 本文提出概念模型和结构化方法分析物联网提升业务流程可持续性的潜力，并用旅游和医疗案例说明。


<details>
  <summary>Details</summary>
Motivation: 现有业务流程管理中可持续性研究主要关注环境问题，需系统方法解决多维度可持续性问题。

Method: 提出概念模型正式表示关键可持续性概念，连接业务流程管理和物联网；提出结构化方法对现有业务流程进行系统分析，识别机会并实施具备可持续性意识、物联网增强的业务流程。

Result: 通过旅游领域实例和医疗保健案例研究进行说明。

Conclusion: 所提概念模型和结构化方法有助于分析物联网提升业务流程可持续性的潜力。

Abstract: The real-time data collection and automation capabilities offered by the
Internet of Things (IoT) are revolutionizing and transforming Business
Processes (BPs) into IoT-enhanced BPs, showing high potential for improving
sustainability. Although already studied in Business Process Management (BPM),
sustainability research has primarily focused on environmental concerns.
However, achieving a holistic and lasting impact requires a systematic approach
to address sustainability beyond the environmental dimension. This work
proposes a conceptual model and a structured methodology with the goal of
analyzing the potential of IoT to measure and improve the sustainability of
BPs. The conceptual model formally represents key sustainability concepts,
linking BPM and IoT by highlighting how IoT devices support and contribute to
sustainability. The methodology guides the systematic analysis of existing BPs,
identifies opportunities, and implements sustainability-aware, IoT-enhanced
BPs. The approach is illustrated through a running example from the tourism
domain and a case study in healthcare.

</details>


### [191] [Evaluating the Use of LLMs for Documentation to Code Traceability](https://arxiv.org/abs/2506.16440)
*Ebube Alor,SayedHassan Khatoonabadi,Emad Shihab*

Main category: cs.SE

TL;DR: 文章全面评估了Claude 3.5 Sonnet、GPT - 4o和o3 - mini等大语言模型在建立软件文档与源代码追踪链接方面的能力，结果显示其性能优于基线模型，但也存在局限性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在自动化文档到代码可追溯性方面潜力未充分挖掘，需对其能力进行评估。

Method: 创建两个新数据集，对大语言模型的追踪链接识别准确性、关系解释质量和多步链重建三项关键能力进行系统实验。

Result: 最佳大语言模型在两个数据集上F1分数分别达79.4%和80.4%，远超基线模型；关系解释部分准确率超97%；多步链端点准确性高，但捕捉中间链接有差异；错误多源于命名假设、幻影链接或架构模式过度泛化。

Conclusion: 大语言模型是追踪发现的有力助手，但有局限性，需人机协作工具设计，并针对错误模式开展研究。

Abstract: Large Language Models (LLMs) offer new potential for automating
documentation-to-code traceability, yet their capabilities remain
underexplored. We present a comprehensive evaluation of LLMs (Claude 3.5
Sonnet, GPT-4o, and o3-mini) in establishing trace links between various
software documentation (including API references and user guides) and source
code. We create two novel datasets from two open-source projects (Unity Catalog
and Crawl4AI). Through systematic experiments, we assess three key
capabilities: (1) trace link identification accuracy, (2) relationship
explanation quality, and (3) multi-step chain reconstruction. Results show that
the best-performing LLM achieves F1-scores of 79.4% and 80.4% across the two
datasets, substantially outperforming our baselines (TF-IDF, BM25, and
CodeBERT). While fully correct relationship explanations range from 42.9% to
71.1%, partial accuracy exceeds 97%, indicating that fundamental connections
are rarely missed. For multi-step chains, LLMs maintain high endpoint accuracy
but vary in capturing precise intermediate links. Error analysis reveals that
many false positives stem from naming-based assumptions, phantom links, or
overgeneralization of architectural patterns. We demonstrate that task-framing,
such as a one-to-many matching strategy, is critical for performance. These
findings position LLMs as powerful assistants for trace discovery, but their
limitations could necessitate human-in-the-loop tool design and highlight
specific error patterns for future research.

</details>


### [192] [How Robust are LLM-Generated Library Imports? An Empirical Study using Stack Overflow](https://arxiv.org/abs/2507.10818)
*Jasmine Latendresse,SayedHassan Khatoonabadi,Emad Shihab*

Main category: cs.SE

TL;DR: 本文对六个先进大语言模型进行实证研究，让其解决Python问题，分析库推荐情况，发现模型多推荐第三方库，但可用性有差距，为开发者和研究者提供见解。


<details>
  <summary>Details</summary>
Motivation: 随着开发者用大语言模型辅助编程，了解其如何推荐库很重要。

Method: 对六个先进大语言模型进行实证研究，让它们解决来自Stack Overflow的真实Python问题，分析导入库的类型、特征和推荐可用性。

Result: 大语言模型主要青睐第三方库，常推荐成熟、流行和许可宽松的依赖项；4.6%的库因导入名和可安装包结构不匹配无法自动解析，仅两个模型提供安装指导。

Conclusion: 研究结果为开发者和研究者提供可操作见解，指出在软件依赖方面改进大语言模型生成代码可靠性和可用性的机会。

Abstract: Software libraries are central to the functionality, security, and
maintainability of modern code. As developers increasingly turn to Large
Language Models (LLMs) to assist with programming tasks, understanding how
these models recommend libraries is essential. In this paper, we conduct an
empirical study of six state-of-the-art LLMs, both proprietary and open-source,
by prompting them to solve real-world Python problems sourced from Stack
Overflow. We analyze the types of libraries they import, the characteristics of
those libraries, and the extent to which the recommendations are usable out of
the box. Our results show that LLMs predominantly favour third-party libraries
over standard ones, and often recommend mature, popular, and permissively
licensed dependencies. However, we also identify gaps in usability: 4.6% of the
libraries could not be resolved automatically due to structural mismatches
between import names and installable packages, and only two models (out of six)
provided installation guidance. While the generated code is technically valid,
the lack of contextual support places the burden of manually resolving
dependencies on the user. Our findings offer actionable insights for both
developers and researchers, and highlight opportunities to improve the
reliability and usability of LLM-generated code in the context of software
dependencies.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [193] [From Rattle to Roar: Optimizer Showdown for MambaStock on S&P 500](https://arxiv.org/abs/2508.04707)
*Alena Chan,Maria Garmonina*

Main category: q-fin.CP

TL;DR: 评估多种优化器在MambaStock模型预测标普500指数收益任务中的表现，引入新优化器Roaree结合优势


<details>
  <summary>Details</summary>
Motivation: 在特定任务中找到结合不同优化器优势的方法，提升性能与训练速度

Method: 评估多种优化器在预测标普500指数收益任务的表现，引入新优化器Roaree

Result: 梯度平滑和自适应率优化器测试误差低，Lion训练快，Roaree保留Lion训练速度并减少振荡

Conclusion: Roaree是结合不同优化器优势的有效解决方案

Abstract: We evaluate the performance of several optimizers on the task of forecasting
S&P 500 Index returns with the MambaStock model. Among the most widely used
algorithms, gradient-smoothing and adaptive-rate optimizers (for example, Adam
and RMSProp) yield the lowest test errors. In contrast, the Lion optimizer
offers notably faster training. To combine these advantages, we introduce a
novel family of optimizers, Roaree, that dampens the oscillatory loss behavior
often seen with Lion while preserving its training speed.

</details>


<div id='q-fin.RM'></div>

# q-fin.RM [[Back]](#toc)

### [194] [Periodic evaluation of defined-contribution pension fund: A dynamic risk measure approach](https://arxiv.org/abs/2508.05241)
*Wanting He,Wenyuan Li,Yunran Wei*

Main category: q-fin.RM

TL;DR: 本文提出定期评估养老金的框架，用无模型强化学习算法找策略，结果显示定期评估使策略更保守，死亡率改善促进行为更激进。


<details>
  <summary>Details</summary>
Motivation: 传统文献通常只在退休时评估养老金绩效，本文旨在进行定期评估并动态管理养老金的尾部风险。

Method: 以动态风险度量为标准，提出无模型强化学习算法搜索最优投资和保险策略，用Lee - Carter模型校准和预测死亡率。

Result: 定期评估会导致更保守的策略，死亡率改善会促使更激进的行为。

Conclusion: 所提出的创新框架及方法能有效应对随机环境，可用于养老金绩效评估和策略制定。

Abstract: This paper introduces an innovative framework for the periodic evaluation of
defined-contribution pension funds. The performance of the pension fund is
evaluated not only at retirement, but also within the interim periods. In
contrast to the traditional literature, we set the dynamic risk measure as the
criterion and manage the tail risk of the pension fund dynamically. To
effectively interact with the stochastic environment, a model-free
reinforcement learning algorithm is proposed to search for optimal investment
and insurance strategies. Using U.S. data, we calibrate pension members'
mortality rates and enhance mortality projections through a Lee-Carter model.
Our numerical results indicate that periodic evaluations lead to more
risk-averse strategies, while mortality improvements encourage more
risk-seeking behaviors.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [195] [Differentially Private Model-X Knockoffs via Johnson-Lindenstrauss Transform](https://arxiv.org/abs/2508.04800)
*Yuxuan Tao,Adel Javanmard*

Main category: stat.ML

TL;DR: 提出高维受控变量选择的私有化框架，用高斯JLT对数据假矩阵私有化，理论分析FDR和功效，表明随机投影结构隐私保护优于经典噪声添加机制。


<details>
  <summary>Details</summary>
Motivation: 现有隐私机制会破坏Model - X knockoff程序的核心可交换性条件，需在差分隐私约束下实现严格的FDR控制。

Method: 通过高斯JLT对数据假矩阵私有化，采用新的去偏技术进行高维私有假程序理论分析。

Result: 理论上刻画了FDR和所提私有变量选择程序的功效，确定了功效收敛到1的充分条件。

Conclusion: 该框架桥接了基于knockoff的FDR控制和私有数据发布，随机投影在严格隐私预算下能保持统计功效。

Abstract: We introduce a novel privatization framework for high-dimensional controlled
variable selection. Our framework enables rigorous False Discovery Rate (FDR)
control under differential privacy constraints. While the Model-X knockoff
procedure provides FDR guarantees by constructing provably exchangeable
``negative control" features, existing privacy mechanisms like Laplace or
Gaussian noise injection disrupt its core exchangeability conditions. Our key
innovation lies in privatizing the data knockoff matrix through the Gaussian
Johnson-Lindenstrauss Transformation (JLT), a dimension reduction technique
that simultaneously preserves covariate relationships through approximate
isometry for $(\epsilon,\delta)$-differential privacy.
  We theoretically characterize both FDR and the power of the proposed private
variable selection procedure, in an asymptotic regime. Our theoretical analysis
characterizes the role of different factors, such as the JLT's dimension
reduction ratio, signal-to-noise ratio, differential privacy parameters, sample
size and feature dimension, in shaping the privacy-power trade-off. Our
analysis is based on a novel `debiasing technique' for high-dimensional private
knockoff procedure. We further establish sufficient conditions under which the
power of the proposed procedure converges to one. This work bridges two
critical paradigms -- knockoff-based FDR control and private data release --
enabling reliable variable selection in sensitive domains. Our analysis
demonstrates that structural privacy preservation through random projections
outperforms the classical noise addition mechanism, maintaining statistical
power even under strict privacy budgets.

</details>


### [196] [The Cosine Schedule is Fisher-Rao-Optimal for Masked Discrete Diffusion Models](https://arxiv.org/abs/2508.04884)
*Leo Zhang*

Main category: stat.ML

TL;DR: 研究掩码离散扩散模型采样离散化调度问题，发现Fisher - Rao几何下最优调度为常用余弦调度。


<details>
  <summary>Details</summary>
Motivation: 确定掩码离散扩散模型采样的离散化调度方案。

Method: 从诱导概率路径的信息几何角度进行研究。

Result: 得出Fisher - Rao几何下的最优调度是常用的余弦调度。

Conclusion: 在信息几何视角下，Fisher - Rao几何的最优调度对应现有流行的余弦调度。

Abstract: In this work, we study the problem of choosing the discretisation schedule
for sampling from masked discrete diffusion models in terms of the information
geometry of the induced probability path. Specifically, we show that the
optimal schedule under the Fisher-Rao geometry recovers the popularly-used
cosine schedule.

</details>


### [197] [High-Dimensional Differentially Private Quantile Regression: Distributed Estimation and Statistical Inference](https://arxiv.org/abs/2508.05212)
*Ziliang Shen,Caixing Wang,Shaoli Wang,Yibo Yan*

Main category: stat.ML

TL;DR: 本文提出分布式高维数据的差分隐私分位数回归方法，含估计算法、去偏估计器和自助法，模拟实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着大数据和机器学习发展，处理含敏感信息的异构数据集时隐私问题愈发重要，需在保护隐私同时进行有意义的统计分析。

Method: 引入牛顿型变换将分位数回归转化为普通最小二乘问题，开发迭代更新的差分隐私估计算法，提出差分隐私去偏估计器，以及通信高效的差分隐私自助法。

Result: 大量模拟实验表明方法在实际场景中具有鲁棒性和有效性。

Conclusion: 所提方法能在分布式高维数据的分位数回归中，保证统计准确性和隐私性，适用于不同数据量的分布式场景。

Abstract: With the development of big data and machine learning, privacy concerns have
become increasingly critical, especially when handling heterogeneous datasets
containing sensitive personal information. Differential privacy provides a
rigorous framework for safeguarding individual privacy while enabling
meaningful statistical analysis. In this paper, we propose a differentially
private quantile regression method for high-dimensional data in a distributed
setting. Quantile regression is a powerful and robust tool for modeling the
relationships between the covariates and responses in the presence of outliers
or heavy-tailed distributions. To address the computational challenges due to
the non-smoothness of the quantile loss function, we introduce a Newton-type
transformation that reformulates the quantile regression task into an ordinary
least squares problem. Building on this, we develop a differentially private
estimation algorithm with iterative updates, ensuring both near-optimal
statistical accuracy and formal privacy guarantees. For inference, we further
propose a differentially private debiased estimator, which enables valid
confidence interval construction and hypothesis testing. Additionally, we
propose a communication-efficient and differentially private bootstrap for
simultaneous hypothesis testing in high-dimensional quantile regression,
suitable for distributed settings with both small and abundant local data.
Extensive simulations demonstrate the robustness and effectiveness of our
methods in practical scenarios.

</details>


### [198] [L1-Regularized Functional Support Vector Machine](https://arxiv.org/abs/2508.05567)
*Bingfan Liu,Peijun Sang*

Main category: stat.ML

TL;DR: 提出L1正则化的功能支持向量机用于二元分类，可识别相关功能协变量，模拟和实际应用表明性能良好。


<details>
  <summary>Details</summary>
Motivation: 填补功能数据分析中多变量功能协变量分类研究的空白。

Method: 提出L1正则化的功能支持向量机进行二元分类，并开发配套算法来拟合分类器。

Result: 模拟和实际应用的数值结果显示，该分类器在预测和特征选择方面表现良好。

Conclusion: 所提出的分类器在预测和特征选择上有较好性能，可通过L1惩罚识别相关功能协变量。

Abstract: In functional data analysis, binary classification with one functional
covariate has been extensively studied. We aim to fill in the gap of
considering multivariate functional covariates in classification. In
particular, we propose an $L_1$-regularized functional support vector machine
for binary classification. An accompanying algorithm is developed to fit the
classifier. By imposing an $L_1$ penalty, the algorithm enables us to identify
relevant functional covariates of the binary response. Numerical results from
simulations and one real-world application demonstrate that the proposed
classifier enjoys good performance in both prediction and feature selection.

</details>


### [199] [High-Order Error Bounds for Markovian LSA with Richardson-Romberg Extrapolation](https://arxiv.org/abs/2508.05570)
*Ilya Levin,Alexey Naumov,Sergey Samsonov*

Main category: stat.ML

TL;DR: 研究带Polyak - Ruppert平均的线性随机逼近算法在马尔可夫噪声下的偏差和高阶误差界，提出偏差分解方法，用Richardson - Romberg外推消除主导偏差项并推导高阶矩界。


<details>
  <summary>Details</summary>
Motivation: 研究带Polyak - Ruppert平均的线性随机逼近算法在马尔可夫噪声下的偏差和高阶误差界。

Method: 提出通过线性化技术对偏差进行新的分解，应用Richardson - Romberg外推程序。

Result: 表明主导偏差项是关于步长α的线性且不能被PR平均消除，RR外推能消除主导偏差项，推导了RR迭代的高阶矩界且主导误差项与普通平均LSA迭代的渐近最优协方差矩阵一致。

Conclusion: Richardson - Romberg外推程序能有效消除带Polyak - Ruppert平均的线性随机逼近算法的主导偏差项。

Abstract: In this paper, we study the bias and high-order error bounds of the Linear
Stochastic Approximation (LSA) algorithm with Polyak-Ruppert (PR) averaging
under Markovian noise. We focus on the version of the algorithm with constant
step size $\alpha$ and propose a novel decomposition of the bias via a
linearization technique. We analyze the structure of the bias and show that the
leading-order term is linear in $\alpha$ and cannot be eliminated by PR
averaging. To address this, we apply the Richardson-Romberg (RR) extrapolation
procedure, which effectively cancels the leading bias term. We derive
high-order moment bounds for the RR iterates and show that the leading error
term aligns with the asymptotically optimal covariance matrix of the vanilla
averaged LSA iterates.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [200] [A near-exact linear mixed model for genome-wide association studies](https://arxiv.org/abs/2508.05278)
*Zhibin Pu,Shufei Ge,Shijia Wang*

Main category: stat.CO

TL;DR: 提出NExt - LMM框架克服GWAS中LMM参数估计的计算瓶颈，提高推理效率并开发Python包。


<details>
  <summary>Details</summary>
Motivation: LMM在GWAS中参数估计因GSM大规模运算有巨大计算负担。

Method: 利用HODLR格式迭代挖掘GSM低秩结构，借助HODLR近似的GSM和共享遗传率加速最大似然估计，建立NExt - LMM估计器误差界。

Result: NExt - LMM比现有方法显著提高推理效率，开发了Python包。

Conclusion: 提出的双方法加速LMM推理且保证低近似误差。

Abstract: Linear mixed models (LMM) are widely adopted in genome-wide association
studies (GWAS) to account for population stratification and cryptic
relatedness. However, the parameter estimation of LMMs imposes substantial
computational burdens due to large-scale operations on genetic similarity
matrices (GSM). We introduced the near-exact linear mixed model (NExt-LMM), a
novel LMM framework that overcomes critical computational bottlenecks in GWAS
through the following key innovations. Firstly, we exploit the inherent
low-rank structure of the GSM iteratively with the Hierarchical Off-Diagonal
Low-Rank (HODLR) format, which is much faster than traditional decomposition
methods. Secondly, we leverage the HODLR-approximated GSM to dramatically
accelerate the further maximum likelihood estimation with the shared
heritability ratios. Moreover, we establish rigorous error bounds for the
NExt-LMM estimator, proving that Kullback-Leibler divergence between the
approximated and exact estimators can be arbitrarily small. Consequently, our
proposed dual approach accelerates inference of LMMs while guaranteeing low
approximation errors. We use numerical experiments to demonstrate that the
NExt-LMM significantly improves inference efficiency compared to existing
methods. We develop a Python package that is available at
https://github.com/ZhibinPU/NExt-LMM.

</details>


### [201] [Piecewise Deterministic Sampling for Constrained Distributions](https://arxiv.org/abs/2508.05462)
*Joël Tatang Demano,Paul Dobson,Konstantinos Zygalakis*

Main category: stat.CO

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In this paper, we propose a novel class of Piecewise Deterministic Markov
Processes (PDMP) that are designed to sample from constrained probability
distributions $\pi$ supported on a convex set $\mathcal{M}$. This class of
PDMPs adapts the concept of a mirror map from convex optimisation to address
sampling problems. Such samplers provides unbiased algorithms that respect the
constraints and, moreover, allow for exact subsampling. We demonstrate the
advantages of these algorithms on a range of constrained sampling problems
where the proposed algorithm outperforms state of the art stochastic
differential equation-based methods.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [202] [Everything You Need to Know About CS Education: Open Results from a Survey of More Than 18,000 Participants](https://arxiv.org/abs/2508.05286)
*Katsiaryna Dzialets,Aleksandra Makeeva,Ilya Vlasov,Anna Potriasaeva,Aleksei Rostovskii,Yaroslav Golubev,Anastasiia Birillo*

Main category: cs.CY

TL;DR: 本文介绍对18032名来自173个国家的计算机学习者的调查结果，以开放数据集呈现，还介绍了方法和问题，并给出三个研究方向示例，旨在推动计算机教育研究。


<details>
  <summary>Details</summary>
Motivation: 计算机科学教育领域有新趋势、新学习形式及学习者多样性增加，需要更新的全面研究。

Method: 对来自173个国家的18032名学习者进行调查。

Result: 以开放数据集形式展示调查结果，介绍了方法和调查问题。

Conclusion: 该数据集有助于支持进一步研究，推动计算机教育进步。

Abstract: Computer science education is a dynamic field with many aspects that
influence the learner's path. While these aspects are usually studied in depth
separately, it is also important to carry out broader large-scale studies that
touch on many topics, because they allow us to put different results into each
other's perspective. Past large-scale surveys have provided valuable insights,
however, the emergence of new trends (e.g., AI), new learning formats (e.g.,
in-IDE learning), and the increasing learner diversity highlight the need for
an updated comprehensive study. To address this, we conducted a survey with
18,032 learners from 173 countries, ensuring diverse representation and
exploring a wide range of topics - formal education, learning formats, AI
usage, challenges, motivation, and more. This paper introduces the results of
this survey as an open dataset, describes our methodology and the survey
questions, and highlights, as a motivating example, three possible research
directions within this data: challenges in learning, emerging formats, and
insights into the in-IDE format. The dataset aims to support further research
and foster advancements in computer education.

</details>


### [203] [Agency, Affordances, and Enculturation of Augmentation Technologies](https://arxiv.org/abs/2508.04725)
*Ann Hill Duin,Isabel Pedersen*

Main category: cs.CY

TL;DR: 论文第三章聚焦增强技术的文化融入，探讨AI术语模糊问题，研究人机关系、非人类主体发展及营销传播作用，最后审视元宇宙和增强现实相关说法。


<details>
  <summary>Details</summary>
Motivation: 分析因人工智能兴起等因素导致增强技术文化融入现象，批判先进增强技术能改善多方面的假设。

Method: 先阐述AI术语模糊问题并介绍WIPO分类方案，再借助媒体与传播研究探讨人机相关概念，关注行业中非人类主体发展及营销传播影响。

Result: 明确非人类主体发展是增强技术兴起的关键因素，揭示营销传播使未来用户适应技术，以及人们更多参与商业数字领域。

Conclusion: 对元宇宙和增强现实的近期说法进行了审视。

Abstract: Augmentation technologies are undergoing a process of enculturation due to
many factors, one being the rise of artificial intelligence (AI), or what the
World Intellectual Property Organization (WIPO) terms the AI wave or AI boom.
Chapter 3 focuses critical attention on the hyped assumption that
sophisticated, emergent, and embodied augmentation technologies will improve
lives, literacy, cultures, arts, economies, and social contexts. The chapter
begins by discussing the problem of ambiguity with AI terminology, which it
aids with a description of the WIPO Categorization of AI Technologies Scheme.
It then draws on media and communication studies to explore concepts such as
agents, agency, power, and agentive relationships between humans and robots.
The chapter focuses on the development of non-human agents in industry as a
critical factor in the rise of augmentation technologies. It looks at how
marketing communication enculturates future users to adopt and adapt to the
technology. Scholars are charting the significant ways that people are drawn
further into commercial digital landscapes, such as the Metaverse concept, in
post-internet society. It concludes by examining recent claims concerning the
Metaverse and augmented reality.

</details>


### [204] [Building Effective Safety Guardrails in AI Education Tools](https://arxiv.org/abs/2508.05360)
*Hannah-Beth Clark,Laura Benton,Emma Searle,Margaux Dowland,Matthew Gregory,Will Gayne,John Roberts*

Main category: cs.CY

TL;DR: 本文探讨英国Oak National Academy在开发AI课程规划助手Aila时应对AI内容安全和适龄性问题的方法，介绍安全防护措施、评估发现及构建更有效防护栏的建议。


<details>
  <summary>Details</summary>
Motivation: 解决生成式AI工具在教育领域应用时，AI生成内容的安全和适龄性问题。

Method: 为Aila实施四项关键安全防护措施，包括提示工程、输入威胁检测、独立异步内容审核代理和人在环方法。

Result: 在评估安全防护栏过程中，识别出实施和测试时的挑战与机遇。

Conclusion: 提出构建更有效生成式AI教育工具安全防护栏的方法，如持续迭代完善和跨部门协作分享。

Abstract: There has been rapid development in generative AI tools across the education
sector, which in turn is leading to increased adoption by teachers. However,
this raises concerns regarding the safety and age-appropriateness of the
AI-generated content that is being created for use in classrooms. This paper
explores Oak National Academy's approach to addressing these concerns within
the development of the UK Government's first publicly available generative AI
tool - our AI-powered lesson planning assistant (Aila). Aila is intended to
support teachers planning national curriculum-aligned lessons that are
appropriate for pupils aged 5-16 years. To mitigate safety risks associated
with AI-generated content we have implemented four key safety guardrails - (1)
prompt engineering to ensure AI outputs are generated within pedagogically
sound and curriculum-aligned parameters, (2) input threat detection to mitigate
attacks, (3) an Independent Asynchronous Content Moderation Agent (IACMA) to
assess outputs against predefined safety categories, and (4) taking a
human-in-the-loop approach, to encourage teachers to review generated content
before it is used in the classroom. Through our on-going evaluation of these
safety guardrails we have identified several challenges and opportunities to
take into account when implementing and testing safety guardrails. This paper
highlights ways to build more effective safety guardrails in generative AI
education tools including the on-going iteration and refinement of guardrails,
as well as enabling cross-sector collaboration through sharing both open-source
code, datasets and learnings.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [205] [Salt-Rock Creep Deformation Forecasting Using Deep Neural Networks and Analytical Models for Subsurface Energy Storage Applications](https://arxiv.org/abs/2508.05248)
*Pradeep Kumar Shukla,Tanujit Chakraborty,Mustafa Sari,Joel Sarout,Partha Pratim Mandal*

Main category: physics.geo-ph

TL;DR: 研究用时间序列预测方法分析盐岩在不同围压下的蠕变变形，用多阶段三轴蠕变数据，对比多种模型，N - BEATS和TCN模型表现更优。


<details>
  <summary>Details</summary>
Motivation: 蠕变变形评估对地下储存设施的设计和运营至关重要，需对盐岩在不同围压下的蠕变进行预测。

Method: 使用多阶段三轴蠕变数据，进行初步分析（STL、Granger因果检验）、统计检验（ADF检验、WCP分析），利用多种深度神经网络模型（N - BEATS、TCN、RNN、TF）并与统计基线模型对比，用RMSE、MAE、MAPE和SMAPE评估性能。

Result: N - BEATS和TCN模型在不同应力水平下分别表现最优，DNN模型比传统分析模型精度提高15 - 20%。

Conclusion: DNN模型，特别是N - BEATS和TCN，能有效捕捉复杂的时间依赖和模式，在盐岩蠕变预测中更准确。

Abstract: This study provides an in-depth analysis of time series forecasting methods
to predict the time-dependent deformation trend (also known as creep) of salt
rock under varying confining pressure conditions. Creep deformation assessment
is essential for designing and operating underground storage facilities for
nuclear waste, hydrogen energy, or radioactive materials. Salt rocks, known for
their mechanical properties like low porosity, low permeability, high
ductility, and exceptional creep and self-healing capacities, were examined
using multi-stage triaxial (MSTL) creep data. After resampling, axial strain
datasets were recorded at 5--10 second intervals under confining pressure
levels ranging from 5 to 35 MPa over 5.8--21 days. Initial analyses, including
Seasonal-Trend Decomposition (STL) and Granger causality tests, revealed
minimal seasonality and causality between axial strain and temperature data.
Further statistical tests, such as the Augmented Dickey-Fuller (ADF) test,
confirmed the stationarity of the data with p-values less than 0.05, and
wavelet coherence plot (WCP) analysis indicated repeating trends. A suite of
deep neural network (DNN) models (Neural Basis Expansion Analysis for Time
Series (N-BEATS), Temporal Convolutional Networks (TCN), Recurrent Neural
Networks (RNN), and Transformers (TF)) was utilized and compared against
statistical baseline models. Predictive performance was evaluated using Root
Mean Square Error (RMSE), Mean Absolute Error (MAE), Mean Absolute Percentage
Error (MAPE), and Symmetric Mean Absolute Percentage Error (SMAPE). Results
demonstrated that N-BEATS and TCN models outperformed others across various
stress levels, respectively. DNN models, particularly N-BEATS and TCN, showed a
15--20\% improvement in accuracy over traditional analytical models,
effectively capturing complex temporal dependencies and patterns.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [206] [Cross-Domain Image Synthesis: Generating H&E from Multiplex Biomarker Imaging](https://arxiv.org/abs/2508.04734)
*Jillur Rahman Saurav,Mohammad Sadegh Nasr,Jacob M. Luber*

Main category: q-bio.QM

TL;DR: 本文研究用多级VQGAN从mIF图像生成高保真虚拟H&E染色，经评估其在计算机辅助诊断上优于cGAN，证明VQGAN是生成虚拟染色的强大架构。


<details>
  <summary>Details</summary>
Motivation: 将mIF的分子数据与H&E的形态信息整合，使H&E的计算机辅助诊断工具能分析mIF数据，弥合分子与形态分析的差距。

Method: 使用多级Vector - Quantized Generative Adversarial Network (VQGAN)从mIF图像创建虚拟H&E染色，并与标准条件GAN (cGAN)在两个公开结直肠癌数据集上对比评估。

Result: VQGAN和cGAN都能生成视觉上合理的图像，但VQGAN生成的虚拟染色在计算机辅助诊断上更有效，下游任务表现更优。

Conclusion: 多级VQGAN是生成科学有用虚拟染色的强大且优越的架构，为整合mIF分子数据到H&E分析流程提供了可行途径。

Abstract: While multiplex immunofluorescence (mIF) imaging provides deep,
spatially-resolved molecular data, integrating this information with the
morphological standard of Hematoxylin & Eosin (H&E) can be very important for
obtaining complementary information about the underlying tissue. Generating a
virtual H&E stain from mIF data offers a powerful solution, providing immediate
morphological context. Crucially, this approach enables the application of the
vast ecosystem of H&E-based computer-aided diagnosis (CAD) tools to analyze
rich molecular data, bridging the gap between molecular and morphological
analysis. In this work, we investigate the use of a multi-level
Vector-Quantized Generative Adversarial Network (VQGAN) to create high-fidelity
virtual H&E stains from mIF images. We rigorously evaluated our VQGAN against a
standard conditional GAN (cGAN) baseline on two publicly available colorectal
cancer datasets, assessing performance on both image similarity and functional
utility for downstream analysis. Our results show that while both architectures
produce visually plausible images, the virtual stains generated by our VQGAN
provide a more effective substrate for computer-aided diagnosis. Specifically,
downstream nuclei segmentation and semantic preservation in tissue
classification tasks performed on VQGAN-generated images demonstrate superior
performance and agreement with ground-truth analysis compared to those from the
cGAN. This work establishes that a multi-level VQGAN is a robust and superior
architecture for generating scientifically useful virtual stains, offering a
viable pathway to integrate the rich molecular data of mIF into established and
powerful H&E-based analytical workflows.

</details>


### [207] [ERDES: A Benchmark Video Dataset for Retinal Detachment and Macular Status Classification in Ocular Ultrasound](https://arxiv.org/abs/2508.04735)
*Pouyan Navard,Yasemin Ozkut,Srikar Adhikari,Elaine Situ-LaCasse,Josie Acuña,Adrienne Yarnish,Alper Yilmaz*

Main category: q-bio.QM

TL;DR: 介绍首个用于视网膜脱离检测的开放数据集ERDES，含超声视频剪辑标注，还给出基线基准，数据公开可获取。


<details>
  <summary>Details</summary>
Motivation: 视网膜脱离需及时干预，黄斑状态是关键，POCUS可检测但图像解读缺专业人员，深度学习有潜力但缺相关算法、研究和公开数据集。

Method: 创建含视网膜脱离存在情况和黄斑状态标注的开放数据集ERDES，并使用多种时空卷积神经网络架构提供基线基准。

Result: 成功推出开放数据集ERDES。

Conclusion: ERDES数据集有助于开发和评估检测视网膜脱离的机器学习模型。

Abstract: Retinal detachment (RD) is a vision-threatening condition that requires
timely intervention to preserve vision. Macular involvement -- whether the
macula is still intact (macula-intact) or detached (macula-detached) -- is the
key determinant of visual outcomes and treatment urgency. Point-of-care
ultrasound (POCUS) offers a fast, non-invasive, cost-effective, and accessible
imaging modality widely used in diverse clinical settings to detect RD.
However, ultrasound image interpretation is limited by a lack of expertise
among healthcare providers, especially in resource-limited settings. Deep
learning offers the potential to automate ultrasound-based assessment of RD.
However, there are no ML ultrasound algorithms currently available for clinical
use to detect RD and no prior research has been done on assessing macular
status using ultrasound in RD cases -- an essential distinction for surgical
prioritization. Moreover, no public dataset currently supports macular-based RD
classification using ultrasound video clips. We introduce Eye Retinal
DEtachment ultraSound, ERDES, the first open-access dataset of ocular
ultrasound clips labeled for (i) presence of retinal detachment and (ii)
macula-intact versus macula-detached status. The dataset is intended to
facilitate the development and evaluation of machine learning models for
detecting retinal detachment. We also provide baseline benchmarks using
multiple spatiotemporal convolutional neural network (CNN) architectures. All
clips, labels, and training code are publicly available at
https://osupcvlab.github.io/ERDES/.

</details>


### [208] [Understanding protein function with a multimodal retrieval-augmented foundation model](https://arxiv.org/abs/2508.04724)
*Timothy Fei Truong Jr,Tristan Bepler*

Main category: q-bio.QM

TL;DR: 介绍多模态、检索增强的蛋白质基础模型PoET - 2，它在零样本变体效应预测等任务表现出色，凸显结合检索增强与多模态、以家族为中心建模的优势。


<details>
  <summary>Details</summary>
Motivation: 现有蛋白质语言模型在扩大规模后虽改善结构预测，但未提升突变理解和蛋白质功能预测的表征质量，需新模型解决。

Method: 引入PoET - 2，采用对序列上下文排序等变的分层Transformer编码器和双解码器架构，有因果和掩码语言建模目标，可在全生成和双向表征学习模式下运行。

Result: PoET - 2在零样本变体效应预测上达最优，在监督设置中，其嵌入在学习序列 - 功能关系上优于先前方法，在小数据集上表现更佳。

Conclusion: 结合检索增强与多模态、以家族为中心的建模有助于推进蛋白质基础模型。

Abstract: Protein language models (PLMs) learn probability distributions over natural
protein sequences. By learning from hundreds of millions of natural protein
sequences, protein understanding and design capabilities emerge. Recent works
have shown that scaling these models improves structure prediction, but does
not seem to improve mutation understanding and representation quality for
protein function prediction. We introduce PoET-2, a multimodal,
retrieval-augmented protein foundation model that incorporates in-context
learning of family-specific evolutionary constraints with optional structure
conditioning to learn generative distributions over protein sequences. PoET-2
uses a hierarchical transformer encoder that is equivariant to sequence context
ordering and a dual decoder architecture with both causal and masked language
modeling objectives, allowing PoET-2 to operate in both fully generative and
bidirectional representation learning modes. PoET-2 achieves state-of-the-art
performance on zero-shot variant effect prediction, excelling at scoring
variants with multiple mutations and challenging indel mutations. In supervised
settings, PoET-2 embeddings outperform previous methods for learning
sequence-function relationships, especially with small datasets. This work
highlights the benefits of combining retrieval augmentation with multimodal,
family-centric modeling for advancing protein foundation models.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [209] [Voost: A Unified and Scalable Diffusion Transformer for Bidirectional Virtual Try-On and Try-Off](https://arxiv.org/abs/2508.04825)
*Seungyong Lee,Jeong-gi Kwak*

Main category: cs.GR

TL;DR: 提出Voost框架，用单个扩散变压器联合学习虚拟试穿和试脱，引入推理技术，实验表明在相关基准测试中达SOTA。


<details>
  <summary>Details</summary>
Motivation: 虚拟试穿准确建模服装与身体对应关系存在挑战，尤其是在姿势和外观变化下。

Method: 提出Voost框架，用单个扩散变压器联合学习虚拟试穿和试脱，引入注意力温度缩放和自我纠正采样两种推理技术。

Result: Voost在试穿和试脱基准测试中取得了SOTA结果，在对齐精度、视觉保真度和泛化性上始终优于强基线。

Conclusion: Voost框架有效，能增强服装 - 身体关系推理，无需特定任务网络、辅助损失或额外标签。

Abstract: Virtual try-on aims to synthesize a realistic image of a person wearing a
target garment, but accurately modeling garment-body correspondence remains a
persistent challenge, especially under pose and appearance variation. In this
paper, we propose Voost - a unified and scalable framework that jointly learns
virtual try-on and try-off with a single diffusion transformer. By modeling
both tasks jointly, Voost enables each garment-person pair to supervise both
directions and supports flexible conditioning over generation direction and
garment category, enhancing garment-body relational reasoning without
task-specific networks, auxiliary losses, or additional labels. In addition, we
introduce two inference-time techniques: attention temperature scaling for
robustness to resolution or mask variation, and self-corrective sampling that
leverages bidirectional consistency between tasks. Extensive experiments
demonstrate that Voost achieves state-of-the-art results on both try-on and
try-off benchmarks, consistently outperforming strong baselines in alignment
accuracy, visual fidelity, and generalization.

</details>


### [210] [Refining Gaussian Splatting: A Volumetric Densification Approach](https://arxiv.org/abs/2508.05187)
*Mohamed Abdul Gafoor,Marius Preda,Titus Zaharia*

Main category: cs.GR

TL;DR: 本文提出新的密度控制方法用于3DGS的新颖视图合成，在Mip - NeRF 360数据集上表现优于3DGS。


<details>
  <summary>Details</summary>
Motivation: 3DGS中有效的点基元管理依赖于ADC过程，但原始3DGS致密化策略存在不足。

Method: 引入利用每个高斯函数的惯性体积来指导细化过程的密度控制方法，并研究SfM和DIM方法用于点云初始化的效果。

Result: 在Mip - NeRF 360数据集上的广泛实验表明，该方法在重建质量上超过3DGS。

Conclusion: 所提出的方法在不同场景中表现良好，可提升3DGS的重建质量。

Abstract: Achieving high-quality novel view synthesis in 3D Gaussian Splatting (3DGS)
often depends on effective point primitive management. The underlying Adaptive
Density Control (ADC) process addresses this issue by automating densification
and pruning. Yet, the vanilla 3DGS densification strategy shows key
shortcomings. To address this issue, in this paper we introduce a novel density
control method, which exploits the volumes of inertia associated to each
Gaussian function to guide the refinement process. Furthermore, we study the
effect of both traditional Structure from Motion (SfM) and Deep Image Matching
(DIM) methods for point cloud initialization. Extensive experimental
evaluations on the Mip-NeRF 360 dataset demonstrate that our approach surpasses
3DGS in reconstruction quality, delivering encouraging performance across
diverse scenes.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [211] [Graffiti: Enabling an Ecosystem of Personalized and Interoperable Social Applications](https://arxiv.org/abs/2508.04889)
*Theia Henderson,David R. Karger,David D. Clark*

Main category: cs.SI

TL;DR: 提出Graffiti系统，可轻松构建多种个性化社交应用并实现互操作，介绍相关概念、API及应用案例。


<details>
  <summary>Details</summary>
Motivation: 现有社交应用设计单一，构建新应用技术挑战大且孤立，需要解决这些问题。

Method: 提出总具体化和通道概念，通过最小客户端API实现应用交互，构建Vue.js插件开发应用。

Result: 展示了至少两种去中心化实现，开发了类似Twitter、Messenger和Wikipedia的应用，探索应用互操作及生态。

Conclusion: Graffiti系统能有效构建个性化社交应用并实现互操作。

Abstract: Most social applications, from Twitter to Wikipedia, have rigid
one-size-fits-all designs, but building new social applications is both
technically challenging and results in applications that are siloed away from
existing communities. We present Graffiti, a system that can be used to build a
wide variety of personalized social applications with relative ease that also
interoperate with each other. People can freely move between a plurality of
designs -- each with its own aesthetic, feature set, and moderation -- all
without losing their friends or data.
  Our concept of total reification makes it possible for seemingly
contradictory designs, including conflicting moderation rules, to interoperate.
Conversely, our concept of channels prevents interoperation from occurring by
accident, avoiding context collapse.
  Graffiti applications interact through a minimal client-side API, which we
show admits at least two decentralized implementations. Above the API, we built
a Vue.js plugin, which we use to develop applications similar to Twitter,
Messenger, and Wikipedia using only client-side code. Our case studies explore
how these and other novel applications interoperate, as well as the broader
ecosystem that Graffiti enables.

</details>


### [212] [Community-Aware Social Community Recommendation](https://arxiv.org/abs/2508.05107)
*Runhao Jiang,Renchi Yang,Wenqing Lin*

Main category: cs.SI

TL;DR: 现有社交推荐模型不适用于社区推荐，本文提出CASO模型，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有社交推荐模型多针对常规物品，忽略社区独特性，缺乏对社区推荐的研究。

Method: 提出CASO模型，使用三个编码器进行用户嵌入，引入信号互斥消除冗余，在优化中加入社区检测损失。

Result: 在六个真实社交网络上与九个基线模型对比，CASO在社区推荐性能上始终显著优于现有方法。

Conclusion: CASO是一种新颖有效的社交社区推荐模型。

Abstract: Social recommendation, which seeks to leverage social ties among users to
alleviate the sparsity issue of user-item interactions, has emerged as a
popular technique for elevating personalized services in recommender systems.
Despite being effective, existing social recommendation models are mainly
devised for recommending regular items such as blogs, images, and products, and
largely fail for community recommendations due to overlooking the unique
characteristics of communities. Distinctly, communities are constituted by
individuals, who present high dynamicity and relate to rich structural patterns
in social networks. To our knowledge, limited research has been devoted to
comprehensively exploiting this information for recommending communities.
  To bridge this gap, this paper presents CASO, a novel and effective model
specially designed for social community recommendation. Under the hood, CASO
harnesses three carefully-crafted encoders for user embedding, wherein two of
them extract community-related global and local structures from the social
network via social modularity maximization and social closeness aggregation,
while the third one captures user preferences using collaborative filtering
with observed user-community affiliations. To further eliminate feature
redundancy therein, we introduce a mutual exclusion between social and
collaborative signals. Finally, CASO includes a community detection loss in the
model optimization, thereby producing community-aware embeddings for
communities. Our extensive experiments evaluating CASO against nine strong
baselines on six real-world social networks demonstrate its consistent and
remarkable superiority over the state of the art in terms of community
recommendation performance.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [213] [Harmonic fractal transformation for modeling complex neuronal effects: from bursting and noise shaping to waveform sensitivity and noise-induced subthreshold spiking](https://arxiv.org/abs/2508.05341)
*Mariia Sorokina*

Main category: q-bio.NC

TL;DR: 提出首个分形频率映射，可复制复杂神经元效应，有高灵敏度检测等优势，模型表明神经元功能可视为频谱在非线性变换频域上的线性求和。


<details>
  <summary>Details</summary>
Motivation: 为了实现对复杂神经元效应的复制，获得高灵敏度检测、抗噪声和噪声诱导信号放大等特性。

Method: 提出分形频率映射，通过对输入频谱进行分形重组激发新成分。

Result: 该映射能实现高灵敏度检测、抗噪声和噪声诱导信号放大。

Conclusion: 神经元功能可看作频谱在非线性变换频域上的线性求和。

Abstract: We propose the first fractal frequency mapping, which in a simple form
enables to replicate complex neuronal effects. Unlike the conventional filters,
which suppress or amplify the input spectral components according to the filter
weights, the transformation excites novel components by a fractal recomposition
of the input spectra resulting in a formation of spikes at resonant frequencies
that are optimal for sampling. This enables high sensitivity detection,
robustness to noise and noise-induced signal amplification. The proposed model
illustrates that a neuronal functionality can be viewed as a linear summation
of spectrum over nonlinearly transformed frequency domain.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [214] [Telegrapher's Generative Model via Kac Flows](https://arxiv.org/abs/2506.20641)
*Richard Duong,Jannis Chemseddine,Peter K. Friz,Gabriele Steidl*

Main category: math.AP

TL;DR: 提出基于阻尼波动方程的新生成模型，拓展到多维，用流匹配框架训练网络生成样本，实验显示其可扩展性及优于扩散模型。


<details>
  <summary>Details</summary>
Motivation: 打破基于流的生成式建模的传统，探索新的模型。

Method: 基于阻尼波动方程构建模型，拓展到多维随机过程，用流匹配框架训练神经网络近似速度场。

Result: 数值实验证明方法具有可扩展性，且优于扩散模型。

Conclusion: 基于阻尼波动方程的新模型在生成式建模中有优势，可作为扩散模型的替代方案。

Abstract: We break the mold in flow-based generative modeling by proposing a new model
based on the damped wave equation, also known as telegrapher's equation.
Similar to the diffusion equation and Brownian motion, there is a Feynman-Kac
type relation between the telegrapher's equation and the stochastic Kac process
in 1D. The Kac flow evolves stepwise linearly in time, so that the probability
flow is Lipschitz continuous in the Wasserstein distance and, in contrast to
diffusion flows, the norm of the velocity is globally bounded. Furthermore, the
Kac model has the diffusion model as its asymptotic limit. We extend these
considerations to a multi-dimensional stochastic process which consists of
independent 1D Kac processes in each spatial component. We show that this
process gives rise to an absolutely continuous curve in the Wasserstein space
and compute the conditional velocity field starting in a Dirac point
analytically. Using the framework of flow matching, we train a neural network
that approximates the velocity field and use it for sample generation. Our
numerical experiments demonstrate the scalability of our approach, and show its
advantages over diffusion models.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [215] [AI Should Be More Human, Not More Complex](https://arxiv.org/abs/2508.04713)
*Carlo Esposito*

Main category: cs.HC

TL;DR: 研究发现大语言模型搜索应用中冗长复杂回复降低用户满意度，用户更喜欢简洁且有来源的回复，指出类人简洁透明是关键。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型搜索应用中冗长复杂回复降低用户满意度和参与度的问题。

Method: 对约10000名参与者进行研究，比较五个主要人工智能搜索系统的回复。

Result: 用户压倒性地更喜欢简洁、有来源的回复，当前AI发展的“人工复杂”趋势产生怪诞谷效应，降低信任、增加认知负担。

Conclusion: 挑战复杂AI回复代表更好性能的假设，表明类人简洁和透明是用户参与和系统可靠的关键。

Abstract: Large Language Models (LLMs) in search applications increasingly prioritize
verbose, lexically complex responses that paradoxically reduce user
satisfaction and engagement. Through a comprehensive study of 10.000 (est.)
participants comparing responses from five major AI-powered search systems, we
demonstrate that users overwhelmingly prefer concise, source-attributed
responses over elaborate explanations. Our analysis reveals that current AI
development trends toward "artificial sophistication" create an uncanny valley
effect where systems sound knowledgeable but lack genuine critical thinking,
leading to reduced trust and increased cognitive load. We present evidence that
optimal AI communication mirrors effective human discourse: direct, properly
sourced, and honest about limitations. Our findings challenge the prevailing
assumption that more complex AI responses indicate better performance, instead
suggesting that human-like brevity and transparency are key to user engagement
and system reliability.

</details>


### [216] [Evaluating the Impact of LLM-guided Reflection on Learning Outcomes with Interactive AI-Generated Educational Podcasts](https://arxiv.org/abs/2508.04787)
*Vishnu Menon,Andy Cherney,Elizabeth B. Cloude,Li Zhang,Tiffany D. Do*

Main category: cs.HC

TL;DR: 研究对比含与不含大语言模型引导反思提示的互动式人工智能生成播客对学习和用户体验的影响，发现学习成果相似但提示降低吸引力，需更多反思交互设计研究。


<details>
  <summary>Details</summary>
Motivation: 探究在互动式人工智能生成播客中嵌入大语言模型引导反思提示是否能改善学习和用户体验。

Method: 让36名本科生参与实验，对比含提示和不含提示两种版本的播客。

Result: 不同条件下学习成果相似，但反思提示降低了播客的吸引力。

Conclusion: 需要开展更多关于反思交互设计的研究。

Abstract: This study examined whether embedding LLM-guided reflection prompts in an
interactive AI-generated podcast improved learning and user experience compared
to a version without prompts. Thirty-six undergraduates participated, and while
learning outcomes were similar across conditions, reflection prompts reduced
perceived attractiveness, highlighting a call for more research on reflective
interactivity design.

</details>


### [217] [Situated Epistemic Infrastructures: A Diagnostic Framework for Post-Coherence Knowledge](https://arxiv.org/abs/2508.04995)
*Matthew Kelly*

Main category: cs.HC

TL;DR: 本文引入情境认知基础设施（SEI）框架，分析后连贯条件下人机混合系统中知识如何变得权威，为学术交流提供新模型。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型暴露出当代知识基础设施的脆弱性，需要新工具分析人机混合系统中知识权威性。

Method: 整合基础设施研究、平台理论和认识论的见解，构建SEI框架，强调协调而非分类。

Result: 提出SEI框架，作为分析知识权威性的诊断工具。

Conclusion: 为人工智能治理、知识生产和信息系统伦理设计的辩论提供了对表征主义学术交流模型的有力替代方案。

Abstract: Large Language Models (LLMs) such as ChatGPT have rendered visible the
fragility of contemporary knowledge infrastructures by simulating coherence
while bypassing traditional modes of citation, authority, and validation. This
paper introduces the Situated Epistemic Infrastructures (SEI) framework as a
diagnostic tool for analyzing how knowledge becomes authoritative across hybrid
human-machine systems under post-coherence conditions. Rather than relying on
stable scholarly domains or bounded communities of practice, SEI traces how
credibility is mediated across institutional, computational, and temporal
arrangements. Integrating insights from infrastructure studies, platform
theory, and epistemology, the framework foregrounds coordination over
classification, emphasizing the need for anticipatory and adaptive models of
epistemic stewardship. The paper contributes to debates on AI governance,
knowledge production, and the ethical design of information systems by offering
a robust alternative to representationalist models of scholarly communication.

</details>


### [218] [Human-AI Schema Discovery and Application for Creative Problem Solving](https://arxiv.org/abs/2508.05045)
*Sitong Wang*

Main category: cs.HC

TL;DR: 本文提出人类在创作中依赖模式，但模式难发现和应用，研究开发了人 - 机模式发现与应用框架以支持创造性问题解决。


<details>
  <summary>Details</summary>
Motivation: 人类在复杂或不熟悉领域中发现和应用模式困难，需要支持创造性问题解决的方法。

Method: 设计系统帮助用户从示例中提炼模式，并将模式应用于人机共创工作流。

Result: 提出了人 - 机模式发现与应用的框架。

Conclusion: 模式引导的交互可使隐性知识更易获取和操作，推动更透明和协作的人机系统发展。

Abstract: Humans often rely on underlying structural patterns-schemas-to create,
whether by writing stories, designing software, or composing music. Schemas
help organize ideas and guide exploration, but they are often difficult to
discover and apply, especially in complex or unfamiliar domains. My Ph.D.
research develops a framework for human-AI schema discovery and application to
support creative problem solving. I design systems that support users in
sensemaking over examples to abstract schemas, and in operationalizing schemas
into human-AI co-creative workflows for application. This research offers
insights into how schema-guided interaction can make implicit knowledge more
accessible and actionable, advancing more transparent and collaborative
human-AI systems.

</details>


### [219] [CWEFS: Brain volume conduction effects inspired channel-wise EEG feature selection for multi-dimensional emotion recognition](https://arxiv.org/abs/2508.05228)
*Xueyuan Xu,Wenjia Dong,Fulin Wei,Li Zhuo*

Main category: cs.HC

TL;DR: 提出通道级脑电图特征选择（CWEFS）方法用于多维情绪识别，在三个数据集上验证其有效性，优于19种特征选择方法。


<details>
  <summary>Details</summary>
Motivation: 现有脑电图特征选择研究忽略潜在特征结构对情绪标签相关性的影响，假设各通道重要性一致，限制了多维情感计算中脑电图特征选择模型的精确构建。

Method: 将脑电图情感特征选择集成到共享潜在结构模型，构建不同脑电图通道的共识潜在空间，结合多维情感标签的潜在语义分析，并引入自适应通道权重学习。

Result: 在三个流行的脑电图数据集上，与19种特征选择方法对比，CWEFS选择的特征子集在六项评估指标上实现了最优情绪识别性能。

Conclusion: CWEFS方法在脑电图特征选择和多维情绪识别方面具有有效性和优越性。

Abstract: Due to the intracranial volume conduction effects, high-dimensional
multi-channel electroencephalography (EEG) features often contain substantial
redundant and irrelevant information. This issue not only hinders the
extraction of discriminative emotional representations but also compromises the
real-time performance. Feature selection has been established as an effective
approach to address the challenges while enhancing the transparency and
interpretability of emotion recognition models. However, existing EEG feature
selection research overlooks the influence of latent EEG feature structures on
emotional label correlations and assumes uniform importance across various
channels, directly limiting the precise construction of EEG feature selection
models for multi-dimensional affective computing. To address these limitations,
a novel channel-wise EEG feature selection (CWEFS) method is proposed for
multi-dimensional emotion recognition. Specifically, inspired by brain volume
conduction effects, CWEFS integrates EEG emotional feature selection into a
shared latent structure model designed to construct a consensus latent space
across diverse EEG channels. To preserve the local geometric structure, this
consensus space is further integrated with the latent semantic analysis of
multi-dimensional emotional labels. Additionally, CWEFS incorporates adaptive
channel-weight learning to automatically determine the significance of
different EEG channels in the emotional feature selection task. The
effectiveness of CWEFS was validated using three popular EEG datasets with
multi-dimensional emotional labels. Comprehensive experimental results,
compared against nineteen feature selection methods, demonstrate that the EEG
feature subsets chosen by CWEFS achieve optimal emotion recognition performance
across six evaluation metrics.

</details>


### [220] [ADSEL: Adaptive dual self-expression learning for EEG feature selection via incomplete multi-dimensional emotional tagging](https://arxiv.org/abs/2508.05229)
*Tianze Yu,Junming Zhang,Wenjia Dong,Xueyuan Xu,Li Zhuo*

Main category: cs.HC

TL;DR: 现有EEG特征选择方法在处理不完整多维情感标签时有局限，提出结合ADSEL与最小二乘回归的新算法，提升标签恢复准确率并选出最优特征子集。


<details>
  <summary>Details</summary>
Motivation: 现有EEG特征选择方法存在高维特征和小样本导致的过拟合、计算复杂问题，且处理不完整多维标签时假设标签完整，忽略样本间相关性及与各维度的交互。

Method: 提出结合自适应双自表达学习（ADSEL）与最小二乘回归的新算法，ADSEL在标签空间的样本级和维度级自表达学习过程间建立双向通道。

Result: 能够促进学习信息的交叉共享，利用样本和维度的有效信息进行标签重建。

Conclusion: 新算法可提高标签恢复准确率，有效识别用于多维情感识别的最优EEG特征子集。

Abstract: EEG based multi-dimension emotion recognition has attracted substantial
research interest in human computer interfaces. However, the high
dimensionality of EEG features, coupled with limited sample sizes, frequently
leads to classifier overfitting and high computational complexity. Feature
selection constitutes a critical strategy for mitigating these challenges. Most
existing EEG feature selection methods assume complete multi-dimensional
emotion labels. In practice, open acquisition environment, and the inherent
subjectivity of emotion perception often result in incomplete label data, which
can compromise model generalization. Additionally, existing feature selection
methods for handling incomplete multi-dimensional labels primarily focus on
correlations among various dimensions during label recovery, neglecting the
correlation between samples in the label space and their interaction with
various dimensions. To address these issues, we propose a novel incomplete
multi-dimensional feature selection algorithm for EEG-based emotion
recognition. The proposed method integrates an adaptive dual self-expression
learning (ADSEL) with least squares regression. ADSEL establishes a
bidirectional pathway between sample-level and dimension-level self-expression
learning processes within the label space. It could facilitate the
cross-sharing of learned information between these processes, enabling the
simultaneous exploitation of effective information across both samples and
dimensions for label reconstruction. Consequently, ADSEL could enhances label
recovery accuracy and effectively identifies the optimal EEG feature subset for
multi-dimensional emotion recognition.

</details>


### [221] [FDC-Net: Rethinking the association between EEG artifact removal and multi-dimensional affective computing](https://arxiv.org/abs/2508.05231)
*Wenjia Dong,Xueyuan Xu,Tianze Yu,Junming Zhang,Li Zhuo*

Main category: cs.HC

TL;DR: 提出FDC - Net框架用于端到端抗噪情绪识别，在两个数据集上对比多种方法，展示良好去噪和情绪识别性能。


<details>
  <summary>Details</summary>
Motivation: 当前EEG情绪识别中去噪和识别任务独立处理有误差累积问题，且传统模型缺乏抗噪设计。

Method: 提出FDC - Net框架，通过双向梯度传播与联合优化策略、集成频率自适应Transformer的门控注意力机制建立去噪和识别的动态协作机制。

Result: 去噪任务上，FDC - Net在DEAP和DREAMER数据集分别获96.30%和90.31%的最大相关系数；情绪识别任务中，在两数据集分别达82.3 + 7.1%和88.1 + 0.8%的准确率。

Conclusion: FDC - Net框架在EEG去噪和情绪识别任务中表现良好，能有效解决当前存在的问题。

Abstract: Electroencephalogram (EEG)-based emotion recognition holds significant value
in affective computing and brain-computer interfaces. However, in practical
applications, EEG recordings are susceptible to the effects of various
physiological artifacts. Current approaches typically treat denoising and
emotion recognition as independent tasks using cascaded architectures, which
not only leads to error accumulation, but also fails to exploit potential
synergies between these tasks. Moreover, conventional EEG-based emotion
recognition models often rely on the idealized assumption of "perfectly
denoised data", lacking a systematic design for noise robustness. To address
these challenges, a novel framework that deeply couples denoising and emotion
recognition tasks is proposed for end-to-end noise-robust emotion recognition,
termed as Feedback-Driven Collaborative Network for Denoising-Classification
Nexus (FDC-Net). Our primary innovation lies in establishing a dynamic
collaborative mechanism between artifact removal and emotion recognition
through: (1) bidirectional gradient propagation with joint optimization
strategies; (2) a gated attention mechanism integrated with frequency-adaptive
Transformer using learnable band-position encoding. Two most popular EEG-based
emotion datasets (DEAP and DREAMER) with multi-dimensional emotional labels
were employed to compare the artifact removal and emotion recognition
performance between ASLSL and nine state-of-the-art methods. In terms of the
denoising task, FDC-Net obtains a maximum correlation coefficient (CC) value of
96.30% on DEAP and a maximum CC value of 90.31% on DREAMER. In terms of the
emotion recognition task under physiological artifact interference, FDC-Net
achieves emotion recognition accuracies of 82.3+7.1% on DEAP and 88.1+0.8% on
DREAMER.

</details>


### [222] [Driver Assistant: Persuading Drivers to Adjust Secondary Tasks Using Large Language Models](https://arxiv.org/abs/2508.05238)
*Wei Xiang,Muchen Li,Jie Yan,Manling Zheng,Hanfei Zhu,Mengyun Jiang,Lingyun Sun*

Main category: cs.HC

TL;DR: 研究使用大语言模型（LLM）工具辅助3级自动驾驶司机保持注意力，实证表明该工具能有效维持注意力、降低认知负担并协调任务。


<details>
  <summary>Details</summary>
Motivation: 3级自动驾驶系统在紧急情况需驾驶员干预时，留给驾驶员反应时间有限且认知负担大，需要解决该问题。

Method: 采用大语言模型，以3级系统遇到的路况为触发，通过视觉和听觉途径主动引导驾驶员行为。

Result: 工具能有效维持驾驶员注意力，降低认知负担，协调次要任务和接管行为。

Conclusion: 研究展示了使用大语言模型在多任务自动驾驶中支持驾驶员的潜力。

Abstract: Level 3 automated driving systems allows drivers to engage in secondary tasks
while diminishing their perception of risk. In the event of an emergency
necessitating driver intervention, the system will alert the driver with a
limited window for reaction and imposing a substantial cognitive burden. To
address this challenge, this study employs a Large Language Model (LLM) to
assist drivers in maintaining an appropriate attention on road conditions
through a "humanized" persuasive advice. Our tool leverages the road conditions
encountered by Level 3 systems as triggers, proactively steering driver
behavior via both visual and auditory routes. Empirical study indicates that
our tool is effective in sustaining driver attention with reduced cognitive
load and coordinating secondary tasks with takeover behavior. Our work provides
insights into the potential of using LLMs to support drivers during multi-task
automated driving.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [223] [Toward Low-Latency End-to-End Voice Agents for Telecommunications Using Streaming ASR, Quantized LLMs, and Real-Time TTS](https://arxiv.org/abs/2508.04721)
*Vignesh Ethiraj,Ashwath David,Sidhanth Menon,Divya Vijay*

Main category: cs.SD

TL;DR: 介绍用于实时交互电信应用的低延迟语音AI代理管道，结合四个专业模型，评估显示可实现低延迟电信部署，为下一代电信AI奠定基础。


<details>
  <summary>Details</summary>
Motivation: 为呼叫中心自动化、智能IVR和AI驱动的客户支持等电信应用提供先进的语音AI解决方案。

Method: 结合NetoAI的四个专业模型（TSLAM、T - VEC、TTE、T - Synth），集成流式ASR、对话智能、检索增强生成和实时TTS；构建包含500个人类记录电信问题的数据集进行评估。

Result: TSLAM、TTE和T - Synth的实时因子低于1.0，支持企业低延迟电信部署。

Conclusion: 这些AI代理为下一代电信AI提供基础，可实现自动化客户支持和诊断等功能。

Abstract: We introduce a low-latency telecom AI voice agent pipeline for real-time,
interactive telecommunications use, enabling advanced voice AI for call center
automation, intelligent IVR (Interactive Voice Response), and AI-driven
customer support. The solution is built for telecom, combining four specialized
models by NetoAI: TSLAM, a 4-bit quantized Telecom-Specific Large Language
Model (LLM); T-VEC, a Telecom-Specific Embedding Model; TTE, a Telecom-Specific
Automatic Speech Recognition (ASR) model; and T-Synth, a Telecom-Specific
Text-to-Speech (TTS) model. These models enable highly responsive,
domain-adapted voice AI agents supporting knowledge-grounded spoken
interactions with low latency. The pipeline integrates streaming ASR (TTE),
conversational intelligence (TSLAM), retrieval augmented generation (RAG) over
telecom documents, and real-time TTS (T-Synth), setting a new benchmark for
telecom voice assistants. To evaluate the system, we built a dataset of 500
human-recorded telecom questions from RFCs, simulating real telecom agent
queries. This framework allows analysis of latency, domain relevance, and
real-time performance across the stack. Results show that TSLAM, TTE, and
T-Synth deliver real-time factors (RTF) below 1.0, supporting enterprise,
low-latency telecom deployments. These AI agents -- powered by TSLAM, TTE, and
T-Synth -- provide a foundation for next-generation telecom AI, enabling
automated customer support, diagnostics, and more.

</details>


### [224] [Wearable Music2Emotion : Assessing Emotions Induced by AI-Generated Music through Portable EEG-fNIRS Fusion](https://arxiv.org/abs/2508.04723)
*Sha Zhao,Song Yi,Yangxuan Zhou,Jiadong Pan,Jiquan Wang,Jie Xia,Shijian Li,Shurong Dong,Gang Pan*

Main category: cs.SD

TL;DR: 本文指出音乐情感计算研究现存刺激、模态、便携性三方面局限，提出MEEtBrain框架，结合AI音乐与多模态信号采集，收集数据集验证框架有效性并公开。


<details>
  <summary>Details</summary>
Motivation: 解决现有音乐情感计算研究中刺激受限、过度依赖单模态数据、设备不便携的问题。

Method: 提出MEEtBrain框架，利用AI大规模自动生成音乐刺激，用轻便头带式设备同步采集EEG和fNIRS数据。

Result: 首次招募20名参与者收集14小时数据集，验证框架有效性，AI生成音乐能引发目标情绪，正扩充数据集至44人并公开。

Conclusion: MEEtBrain框架具有解决现存问题的潜力，公开数据集可推动相关研究和应用。

Abstract: Emotions critically influence mental health, driving interest in music-based
affective computing via neurophysiological signals with Brain-computer
Interface techniques. While prior studies leverage music's accessibility for
emotion induction, three key limitations persist: \textbf{(1) Stimulus
Constraints}: Music stimuli are confined to small corpora due to copyright and
curation costs, with selection biases from heuristic emotion-music mappings
that ignore individual affective profiles. \textbf{(2) Modality Specificity}:
Overreliance on unimodal neural data (e.g., EEG) ignores complementary insights
from cross-modal signal fusion.\textbf{ (3) Portability Limitation}: Cumbersome
setups (e.g., 64+ channel gel-based EEG caps) hinder real-world applicability
due to procedural complexity and portability barriers. To address these
limitations, we propose MEEtBrain, a portable and multimodal framework for
emotion analysis (valence/arousal), integrating AI-generated music stimuli with
synchronized EEG-fNIRS acquisition via a wireless headband. By MEEtBrain, the
music stimuli can be automatically generated by AI on a large scale,
eliminating subjective selection biases while ensuring music diversity. We use
our developed portable device that is designed in a lightweight headband-style
and uses dry electrodes, to simultaneously collect EEG and fNIRS recordings. A
14-hour dataset from 20 participants was collected in the first recruitment to
validate the framework's efficacy, with AI-generated music eliciting target
emotions (valence/arousal). We are actively expanding our multimodal dataset
(44 participants in the latest dataset) and make it publicly available to
promote further research and practical applications. \textbf{The dataset is
available at https://zju-bmi-lab.github.io/ZBra.

</details>


### [225] [Towards Hallucination-Free Music: A Reinforcement Learning Preference Optimization Framework for Reliable Song Generation](https://arxiv.org/abs/2508.05011)
*Huaicheng Zhang,Wei Tan,Guangzheng Li,Yixuan Zhang,Hangting Chen,Shun Lei,Chenyu Yang,Zhiyong Wu,Shuai Wang,Qijun Huang,Dong Yu*

Main category: cs.SD

TL;DR: 提出基于强化学习的偏好优化框架解决歌词到歌曲生成中的幻觉问题，对比三种策略有效减少幻觉且保留音乐质量。


<details>
  <summary>Details</summary>
Motivation: 现有音频生成语言模型在歌词到歌曲生成中存在内容幻觉和音乐连贯性问题，监督微调方法改善有限。

Method: 构建幻觉偏好数据集，在强化学习框架中实现并评估DPO、PPO和GRPO三种偏好优化策略。

Result: DPO使音素错误率降低7.4%，PPO和GRPO分别降低4.9%和4.7%，综合评估表明有效抑制幻觉并保留音乐质量。

Conclusion: 提出基于强化学习的系统性解决方案，有可迁移性，为未来歌曲生成研究开辟新途径。

Abstract: Recent advances in audio-based generative language models have accelerated
AI-driven lyric-to-song generation. However, these models frequently suffer
from content hallucination, producing outputs misaligned with the input lyrics
and undermining musical coherence. Current supervised fine-tuning (SFT)
approaches, limited by passive label-fitting, exhibit constrained
self-improvement and poor hallucination mitigation. To address this core
challenge, we propose a novel reinforcement learning (RL) framework leveraging
preference optimization for hallucination control. Our key contributions
include: (1) Developing a robust hallucination preference dataset constructed
via phoneme error rate (PER) computation and rule-based filtering to capture
alignment with human expectations; (2) Implementing and evaluating three
distinct preference optimization strategies within the RL framework: Direct
Preference Optimization (DPO), Proximal Policy Optimization (PPO), and Group
Relative Policy Optimization (GRPO). DPO operates off-policy to enhance
positive token likelihood, achieving a significant 7.4% PER reduction. PPO and
GRPO employ an on-policy approach, training a PER-based reward model to
iteratively optimize sequences via reward maximization and KL-regularization,
yielding PER reductions of 4.9% and 4.7%, respectively. Comprehensive objective
and subjective evaluations confirm that our methods effectively suppress
hallucinations while preserving musical quality. Crucially, this work presents
a systematic, RL-based solution to hallucination control in lyric-to-song
generation. The framework's transferability also unlocks potential for music
style adherence and musicality enhancement, opening new avenues for future
generative song research.

</details>


### [226] [SpectroStream: A Versatile Neural Codec for General Audio](https://arxiv.org/abs/2508.05207)
*Yunpeng Li,Kehang Han,Brian McWilliams,Zalan Borsos,Marco Tagliasacchi*

Main category: cs.SD

TL;DR: 提出全频段多通道神经音频编解码器SpectroStream，能以4 - 16 kbps比特率高质量重建48 kHz立体声音乐。


<details>
  <summary>Details</summary>
Motivation: 突破SoundStream局限，实现更高采样率立体声音乐高质量重建。

Method: 采用新神经架构利用时频域音频表示，使用延迟融合策略处理多通道音频。

Result: 可实现48 kHz立体声音乐在4 - 16 kbps比特率下高质量重建。

Conclusion: 新架构和策略有助于在更高采样率下提升音频质量，平衡声道声学质量和相位一致性。

Abstract: We propose SpectroStream, a full-band multi-channel neural audio codec.
Successor to the well-established SoundStream, SpectroStream extends its
capability beyond 24 kHz monophonic audio and enables high-quality
reconstruction of 48 kHz stereo music at bit rates of 4--16 kbps. This is
accomplished with a new neural architecture that leverages audio representation
in the time-frequency domain, which leads to better audio quality especially at
higher sample rate. The model also uses a delayed-fusion strategy to handle
multi-channel audio, which is crucial in balancing per-channel acoustic quality
and cross-channel phase consistency.

</details>


### [227] [Estimating Musical Surprisal from Audio in Autoregressive Diffusion Model Noise Spaces](https://arxiv.org/abs/2508.05306)
*Mathias Rose Bjare,Stefan Lattner,Gerhard Widmer*

Main category: cs.SD

TL;DR: 研究用自回归扩散模型（ADMs）的信息内容（IC）建模音乐期望和惊奇感，发现基于两种不同扩散常微分方程的模型IC表现优于GIVT，在两项任务中匹配或超越GIVT，适当噪声水平可提升音乐惊奇感任务结果。


<details>
  <summary>Details</summary>
Motivation: 研究用自回归扩散模型（ADMs）计算的信息内容（IC）在建模音乐期望和惊奇感方面的有效性。

Method: 使用基于两种不同扩散常微分方程的自回归扩散模型计算IC，通过负对数似然评估其对数据的描述能力，通过两项任务评估其捕捉惊奇感的效果，还对不同扩散过程噪声水平下的惊奇感进行假设并测试。

Result: 基于两种不同扩散常微分方程的模型IC在负对数似然方面比GIVT能更好地描述多样数据；在两项任务中，扩散模型匹配或超越GIVT的表现；适当噪声水平可改善音乐惊奇感任务结果。

Conclusion: 自回归扩散模型（ADMs）的信息内容（IC）在建模音乐期望和惊奇感方面比GIVT更有效，不同扩散过程噪声水平对应不同音频粒度的音乐和音频特征的惊奇感。

Abstract: Recently, the information content (IC) of predictions from a Generative
Infinite-Vocabulary Transformer (GIVT) has been used to model musical
expectancy and surprisal in audio. We investigate the effectiveness of such
modelling using IC calculated with autoregressive diffusion models (ADMs). We
empirically show that IC estimates of models based on two different diffusion
ordinary differential equations (ODEs) describe diverse data better, in terms
of negative log-likelihood, than a GIVT. We evaluate diffusion model IC's
effectiveness in capturing surprisal aspects by examining two tasks: (1)
capturing monophonic pitch surprisal, and (2) detecting segment boundaries in
multi-track audio. In both tasks, the diffusion models match or exceed the
performance of a GIVT. We hypothesize that the surprisal estimated at different
diffusion process noise levels corresponds to the surprisal of music and audio
features present at different audio granularities. Testing our hypothesis, we
find that, for appropriate noise levels, the studied musical surprisal tasks'
results improve. Code is provided on github.com/SonyCSLParis/audioic.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [228] [Necessity of Block Designs for Optimal Locally Private Distribution Estimation](https://arxiv.org/abs/2508.05110)
*Abigail Gentle*

Main category: cs.IT

TL;DR: 本文证明实现最优误差的协议须对应平衡不完全区组设计，结合先前工作完整刻画了该问题的最优协议集。


<details>
  <summary>Details</summary>
Motivation: 此前基于平衡不完全区组设计的协议被证明可实现最优误差，但不清楚其他构造是否也能达到最优。

Method: 通过证明实现最优误差的协议与平衡不完全区组设计的对应关系。

Result: 证明任意实现最优误差的协议都对应某种平衡不完全区组设计，结合先前工作完整刻画了最优协议集。

Conclusion: 实现最优误差和最优通信的协议仅基于对称平衡不完全区组设计。

Abstract: Local differential privacy represents the gold standard for preserving the
privacy of data before it leaves the device, and distribution estimation under
this model has been well studied. Recently, protocols built upon balanced
incomplete block designs were shown to achieve optimal error for this problem.
However, it remained unknown whether other constructions could also be optimal.
We resolve this question by proving that any protocol achieving optimal error
must correspond to some balanced incomplete block design. This result, combined
with prior work, completely characterises the set of optimal protocols for this
problem. As a consequence, the protocols that achieve optimal error and optimal
communication are only those based on symmetrical balanced incomplete block
designs.

</details>


### [229] [Two tales for a geometric Jensen--Shannon divergence](https://arxiv.org/abs/2508.05066)
*Frank Nielsen*

Main category: cs.IT

TL;DR: 本文引入扩展的几何Jensen - Shannon散度（extended G - JSD），给出其与G - JSD的差距、上下界，推导多元高斯分布下的闭式表达式，并指出二者可视为普通JSD的正则化。


<details>
  <summary>Details</summary>
Motivation: 针对正密度提出一种不规范几何混合的几何Jensen - Shannon散度的替代定义，以扩展到更一般的正测度。

Method: 给出扩展G - JSD与G - JSD在概率密度下的差距，用其他统计散度表示上下界，推导多元高斯分布下的闭式表达式。

Result: 得到扩展G - JSD与G - JSD的差距、上下界及多元高斯分布下的闭式表达式。

Conclusion: G - JSD和扩展G - JSD可视为普通JSD通过加性项的正则化。

Abstract: The geometric Jensen--Shannon divergence (G-JSD) gained popularity in machine
learning and information sciences thanks to its closed-form expression between
Gaussian distributions. In this work, we introduce an alternative definition of
the geometric Jensen--Shannon divergence tailored to positive densities which
does not normalize geometric mixtures. This novel divergence is termed the
extended G-JSD as it extends to more general positive measures. We give
explicitly the gap between the extended G-JSD and G-JSD when considering
probability densities, and report both lower and upper bounds in terms of other
statistical divergences. We derive corresponding closed-form expressions when
considering the case of multivariate Gaussian distributions often met in
applications. Finally, we show that these two types of geometric JSDs, the
G-JSD and the extended G-JSD, can be interpreted as regularizations of the
ordinary JSD by additive terms.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [230] [Adversarial Attacks and Defenses on Graph-aware Large Language Models (LLMs)](https://arxiv.org/abs/2508.04894)
*Iyiola E. Olatunji,Franziska Boenisch,Jing Xu,Adam Dziedzic*

Main category: cs.CR

TL;DR: 本文探索图感知大语言模型对抗攻击的脆弱性，发现新攻击面，分析设计选择对攻击的影响，并提出防御框架GALGUARD。


<details>
  <summary>Details</summary>
Motivation: 图感知大语言模型与图结构数据集成提升性能，但对抗攻击的鲁棒性未被探索，因此开展研究。

Method: 利用针对图模型的对抗攻击方法，对LLAGA和GRAPHPROMPTER进行攻击实验，并分析设计选择对攻击的影响，最后提出防御框架GALGUARD。

Result: 发现LLAGA新攻击面，分析得出LLAGA节点序列模板增加脆弱性、GRAPHPROMPTER的GNN编码器更鲁棒、两者都易受特征扰动攻击等结果。

Conclusion: 图感知大语言模型存在对抗攻击脆弱性，提出的GALGUARD防御框架可缓解特征扰动和结构攻击。

Abstract: Large Language Models (LLMs) are increasingly integrated with
graph-structured data for tasks like node classification, a domain
traditionally dominated by Graph Neural Networks (GNNs). While this integration
leverages rich relational information to improve task performance, their
robustness against adversarial attacks remains unexplored. We take the first
step to explore the vulnerabilities of graph-aware LLMs by leveraging existing
adversarial attack methods tailored for graph-based models, including those for
poisoning (training-time attacks) and evasion (test-time attacks), on two
representative models, LLAGA (Chen et al. 2024) and GRAPHPROMPTER (Liu et al.
2024). Additionally, we discover a new attack surface for LLAGA where an
attacker can inject malicious nodes as placeholders into the node sequence
template to severely degrade its performance. Our systematic analysis reveals
that certain design choices in graph encoding can enhance attack success, with
specific findings that: (1) the node sequence template in LLAGA increases its
vulnerability; (2) the GNN encoder used in GRAPHPROMPTER demonstrates greater
robustness; and (3) both approaches remain susceptible to imperceptible feature
perturbation attacks. Finally, we propose an end-to-end defense framework
GALGUARD, that combines an LLM-based feature correction module to mitigate
feature-level perturbations and adapted GNN defenses to protect against
structural attacks.

</details>


### [231] [Incident Response Planning Using a Lightweight Large Language Model with Reduced Hallucination](https://arxiv.org/abs/2508.05188)
*Kim Hammar,Tansu Alpcan,Emil C. Lupu*

Main category: cs.CR

TL;DR: 文章提出用新方法使用大语言模型进行事件响应规划，减少幻觉问题，经实验验证效果好。


<details>
  <summary>Details</summary>
Motivation: 现有利用大语言模型辅助事件响应的方法成本高且易产生幻觉，需改进。

Method: 采用微调、信息检索和前瞻规划三个步骤。

Result: 方法生成响应计划的幻觉概率有界，可在一定假设下降低；轻量级，能在普通硬件运行；比前沿大语言模型恢复时间最多缩短22%，能适应多种事件类型和响应动作。

Conclusion: 提出的新方法有效，可用于事件响应规划，减少幻觉并提升性能。

Abstract: Timely and effective incident response is key to managing the growing
frequency of cyberattacks. However, identifying the right response actions for
complex systems is a major technical challenge. A promising approach to
mitigate this challenge is to use the security knowledge embedded in large
language models (LLMs) to assist security operators during incident handling.
Recent research has demonstrated the potential of this approach, but current
methods are mainly based on prompt engineering of frontier LLMs, which is
costly and prone to hallucinations. We address these limitations by presenting
a novel way to use an LLM for incident response planning with reduced
hallucination. Our method includes three steps: fine-tuning, information
retrieval, and lookahead planning. We prove that our method generates response
plans with a bounded probability of hallucination and that this probability can
be made arbitrarily small at the expense of increased planning time under
certain assumptions. Moreover, we show that our method is lightweight and can
run on commodity hardware. We evaluate our method on logs from incidents
reported in the literature. The experimental results show that our method a)
achieves up to 22% shorter recovery times than frontier LLMs and b) generalizes
to a broad range of incident types and response actions.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [232] [The Implicit Barrier of Utility Maximization: An Interior-Point Approach for Market Equilibria](https://arxiv.org/abs/2508.04822)
*Chuwen Zhang,Chang He,Bo Jiang,Yinyu Ye*

Main category: math.OC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study the computation of equilibria in exchange markets with divisible
goods and players endowed with heterogeneous utilities. In this paper, we
revisit the polynomial-time interior-point strategies that update \emph{only}
the prices, mirroring the t\^atonnement process. The key ingredient is the
\emph{implicit barrier} inherent in the utility maximization: the utility turns
unbounded when the goods are almost free of charge. Focusing on a ubiquitous
class of utilities, we formalize this observation into Scaled Lipschitz
Continuity for utility maximization from both the primal and dual perspectives.
A companion result suggests that no additional effort is required for computing
high-order derivatives; all the necessary information is readily available when
collecting the best responses. To tackle the Newton systems, we present an
explicitly invertible approximation of the Hessian operator with high
probability guarantees, and a scaling matrix that minimizes the condition
number of the linear system. Building on these tools, we design two inexact
interior-point methods. One such method has O(ln(1/{\epsilon})) complexity
rate. Under mild conditions, the other method achieves a non-asymptotic
superlinear convergence rate. Extensions and preliminary experiments are
presented.

</details>


### [233] [Can SGD Handle Heavy-Tailed Noise?](https://arxiv.org/abs/2508.04860)
*Ilyas Fatkhullin,Florian Hübler,Guanghui Lan*

Main category: math.OC

TL;DR: 本文研究了随机梯度下降（SGD）在重尾噪声下的理论行为，给出了凸、强凸和非凸问题类的收敛保证，挑战了重尾噪声使SGD无效的观点。


<details>
  <summary>Details</summary>
Motivation: 随机梯度下降在重尾噪声下的理论行为研究不足，本文旨在研究无自适应修改的SGD在这种不利随机条件下能否成功。

Method: 假设随机梯度的p阶矩有界，对（投影）SGD在不同问题类进行分析。

Result: 在凸和强凸情况下得到最小最大最优样本复杂度，非凸目标下证明收敛到驻点并给出匹配下界，小批量SGD也有可比的样本复杂度。

Conclusion: 重尾噪声不一定使SGD无效，普通SGD可作为鲁棒且有理论依据的基线。

Abstract: Stochastic Gradient Descent (SGD) is a cornerstone of large-scale
optimization, yet its theoretical behavior under heavy-tailed noise -- common
in modern machine learning and reinforcement learning -- remains poorly
understood. In this work, we rigorously investigate whether vanilla SGD, devoid
of any adaptive modifications, can provably succeed under such adverse
stochastic conditions. Assuming only that stochastic gradients have bounded
$p$-th moments for some $p \in (1, 2]$, we establish sharp convergence
guarantees for (projected) SGD across convex, strongly convex, and non-convex
problem classes. In particular, we show that SGD achieves minimax optimal
sample complexity under minimal assumptions in the convex and strongly convex
regimes: $\mathcal{O}(\varepsilon^{-\frac{p}{p-1}})$ and
$\mathcal{O}(\varepsilon^{-\frac{p}{2(p-1)}})$, respectively. For non-convex
objectives under H\"older smoothness, we prove convergence to a stationary
point with rate $\mathcal{O}(\varepsilon^{-\frac{2p}{p-1}})$, and complement
this with a matching lower bound specific to SGD with arbitrary polynomial
step-size schedules. Finally, we consider non-convex Mini-batch SGD under
standard smoothness and bounded central moment assumptions, and show that it
also achieves a comparable $\mathcal{O}(\varepsilon^{-\frac{2p}{p-1}})$ sample
complexity with a potential improvement in the smoothness constant. These
results challenge the prevailing view that heavy-tailed noise renders SGD
ineffective, and establish vanilla SGD as a robust and theoretically principled
baseline -- even in regimes where the variance is unbounded.

</details>


### [234] [Exact and Heuristic Algorithms for Constrained Biclustering](https://arxiv.org/abs/2508.05493)
*Antonio M. Sudoso*

Main category: math.OC

TL;DR: 研究带成对约束的双聚类问题，提出精确和启发式算法，实验表明精确方法优于通用求解器，启发式算法能在大规模实例上高效获高质量解。


<details>
  <summary>Details</summary>
Motivation: 将背景知识融入双聚类以提升解的质量和可解释性，研究带成对约束的双聚类问题。

Method: 提出精确算法（基于低维半定规划松弛的分支 - 切割算法）和启发式算法（基于半定规划低秩分解，用增广拉格朗日方法和块坐标投影梯度算法求解）。

Result: 精确方法显著优于通用求解器，启发式算法能在大规模实例上高效获得高质量解。

Conclusion: 所提算法在解决带成对约束的双聚类问题上表现良好，具有有效性和高效性。

Abstract: Biclustering, also known as co-clustering or two-way clustering,
simultaneously partitions the rows and columns of a data matrix to reveal
submatrices with coherent patterns. Incorporating background knowledge into
clustering to enhance solution quality and interpretability has attracted
growing interest in mathematical optimization and machine learning research.
Extending this paradigm to biclustering enables prior information to guide the
joint grouping of rows and columns. We study constrained biclustering with
pairwise constraints, namely must-link and cannot-link constraints, which
specify whether objects should belong to the same or different biclusters. As a
model problem, we address the constrained version of the k-densest disjoint
biclique problem, which aims to identify k disjoint complete bipartite
subgraphs (called bicliques) in a weighted complete bipartite graph, maximizing
the total density while satisfying pairwise constraints. We propose both exact
and heuristic algorithms. The exact approach is a tailored branch-and-cut
algorithm based on a low-dimensional semidefinite programming (SDP) relaxation,
strengthened with valid inequalities and solved in a cutting-plane fashion.
Exploiting integer programming tools, a rounding scheme converts SDP solutions
into feasible biclusterings at each node. For large-scale instances, we
introduce an efficient heuristic based on the low-rank factorization of the
SDP. The resulting nonlinear optimization problem is tackled with an augmented
Lagrangian method, where the subproblem is solved by decomposition through a
block-coordinate projected gradient algorithm. Extensive experiments on
synthetic and real-world datasets show that the exact method significantly
outperforms general-purpose solvers, while the heuristic achieves high-quality
solutions efficiently on large instances.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [235] [JPS: Jailbreak Multimodal Large Language Models with Collaborative Visual Perturbation and Textual Steering](https://arxiv.org/abs/2508.05087)
*Renmiao Chen,Shiyao Cui,Xuancheng Huang,Chengwei Pan,Victor Shea-Jay Huang,QingLin Zhang,Xuan Ouyang,Zhexin Zhang,Hongning Wang,Minlie Huang*

Main category: cs.MM

TL;DR: 本文提出JPS方法进行多模态大语言模型越狱攻击，通过视觉扰动和文本引导协作实现，还提出MIFR指标评估攻击质量，实验显示其在ASR和MIFR上达到新的最优。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型越狱攻击研究主要关注攻击成功率，忽视生成响应是否满足攻击者恶意意图，导致低质量输出。

Method: 提出JPS方法，结合目标引导的对抗图像扰动和通过多智能体系统优化的“引导提示”，二者迭代协同优化；提出基于推理大语言模型评估器的恶意意图实现率（MIFR）指标。

Result: JPS在多种多模态大语言模型和基准测试中，攻击成功率（ASR）和恶意意图实现率（MIFR）达到新的最优。

Conclusion: JPS方法在多模态大语言模型越狱攻击中有效。

Abstract: Jailbreak attacks against multimodal large language Models (MLLMs) are a
significant research focus. Current research predominantly focuses on
maximizing attack success rate (ASR), often overlooking whether the generated
responses actually fulfill the attacker's malicious intent. This oversight
frequently leads to low-quality outputs that bypass safety filters but lack
substantial harmful content. To address this gap, we propose JPS,
\underline{J}ailbreak MLLMs with collaborative visual \underline{P}erturbation
and textual \underline{S}teering, which achieves jailbreaks via corporation of
visual image and textually steering prompt. Specifically, JPS utilizes
target-guided adversarial image perturbations for effective safety bypass,
complemented by "steering prompt" optimized via a multi-agent system to
specifically guide LLM responses fulfilling the attackers' intent. These visual
and textual components undergo iterative co-optimization for enhanced
performance. To evaluate the quality of attack outcomes, we propose the
Malicious Intent Fulfillment Rate (MIFR) metric, assessed using a
Reasoning-LLM-based evaluator. Our experiments show JPS sets a new
state-of-the-art in both ASR and MIFR across various MLLMs and benchmarks, with
analyses confirming its efficacy. Codes are available at
\href{https://github.com/thu-coai/JPS}{https://github.com/thu-coai/JPS}.
\color{warningcolor}{Warning: This paper contains potentially sensitive
contents.}

</details>


### [236] [Embedding Alignment in Code Generation for Audio](https://arxiv.org/abs/2508.05473)
*Sam Kouteili,Hiren Madhu,George Typaldos,Mark Santolucito*

Main category: cs.MM

TL;DR: 研究大语言模型代码生成，构建代码 - 音频嵌入对齐图以实现音乐多样化输出。


<details>
  <summary>Details</summary>
Motivation: 大语言模型代码生成可助力创意编码，但代码生成模型难以提供多样代码候选，且缺乏代码音频输出的直接洞察，需建立代码候选与音频的关系。

Method: 研究代码和音频嵌入空间映射的拓扑结构，构建预测模型学习嵌入对齐图。

Result: 发现代码和音频嵌入无简单线性关系，但构建的预测模型表明可学习嵌入对齐图。

Conclusion: 提出一个根据代码预测输出音频嵌入的模型，构建了代码 - 音频嵌入对齐图。

Abstract: LLM-powered code generation has the potential to revolutionize creative
coding endeavors, such as live-coding, by enabling users to focus on structural
motifs over syntactic details. In such domains, when prompting an LLM, users
may benefit from considering multiple varied code candidates to better realize
their musical intentions. Code generation models, however, struggle to present
unique and diverse code candidates, with no direct insight into the code's
audio output. To better establish a relationship between code candidates and
produced audio, we investigate the topology of the mapping between code and
audio embedding spaces. We find that code and audio embeddings do not exhibit a
simple linear relationship, but supplement this with a constructed predictive
model that shows an embedding alignment map could be learned. Supplementing the
aim for musically diverse output, we present a model that given code predicts
output audio embedding, constructing a code-audio embedding alignment map.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [237] [Curse of High Dimensionality Issue in Transformer for Long-context Modeling](https://arxiv.org/abs/2505.22107)
*Shuhai Zhang,Zeng You,Yaofo Chen,Zhiquan Wen,Qianyue Wang,Zhijie Qiu,Yuanqing Li,Mingkui Tan*

Main category: cs.CL

TL;DR: 本文提出Dynamic Group Attention (DGA)，通过分组编码策略减少Transformer大语言模型长上下文的冗余注意力计算，降低计算成本且性能有竞争力。


<details>
  <summary>Details</summary>
Motivation: Transformer大语言模型长上下文建模因冗余注意力计算存在显著计算低效问题。

Method: 将传统概率序列建模重构成监督学习任务，分析注意力稀疏性，将注意力优化表述为线性编码问题并提出分组编码策略，进而提出DGA。

Result: DGA显著降低计算成本，且保持有竞争力的性能。

Conclusion: DGA能有效解决Transformer大语言模型长上下文建模的计算低效问题。

Abstract: Transformer-based large language models (LLMs) excel in natural language
processing tasks by capturing long-range dependencies through self-attention
mechanisms. However, long-context modeling faces significant computational
inefficiencies due to \textit{redundant} attention computations: while
attention weights are often \textit{sparse}, all tokens consume \textit{equal}
computational resources. In this paper, we reformulate traditional
probabilistic sequence modeling as a \textit{supervised learning task},
enabling the separation of relevant and irrelevant tokens and providing a
clearer understanding of redundancy. Based on this reformulation, we
theoretically analyze attention sparsity, revealing that only a few tokens
significantly contribute to predictions. Building on this, we formulate
attention optimization as a linear coding problem and propose a \textit{group
coding strategy}, theoretically showing its ability to improve robustness
against random noise and enhance learning efficiency. Motivated by this, we
propose \textit{Dynamic Group Attention} (DGA), which leverages the group
coding to explicitly reduce redundancy by aggregating less important tokens
during attention computation. Empirical results show that our DGA significantly
reduces computational costs while maintaining competitive performance.Code is
available at https://github.com/bolixinyu/DynamicGroupAttention.

</details>


### [238] [Enhancing Dialogue Annotation with Speaker Characteristics Leveraging a Frozen LLM](https://arxiv.org/abs/2508.04795)
*Thomas Thebaud,Yen-Ju Lu,Matthew Wiesner,Peter Viechnicki,Najim Dehak*

Main category: cs.CL

TL;DR: 探索在对话转录后处理中添加说话人特征元数据标签，结合音频基础模型和语言模型实现说话人特征推断，性能佳。


<details>
  <summary>Details</summary>
Motivation: 在对话转录后处理中，除提升语法等，还希望通过添加说话人特征元数据标签来丰富转录对话。

Method: 将冻结的音频基础模型（如Whisper或WavLM）与冻结的LLAMA语言模型结合，用轻量级高效连接器连接音频和语言表示，且无需对模型进行特定任务微调。

Result: 在说话人特征分析任务上取得有竞争力的性能，在某些场景下冻结的LLAMA模型比较x - 向量的等错误率为8.8%。

Conclusion: 所提出的结合音频和语言模型的方法能在保持模块化和速度的同时，有效完成说话人特征推断任务。

Abstract: In dialogue transcription pipelines, Large Language Models (LLMs) are
frequently employed in post-processing to improve grammar, punctuation, and
readability. We explore a complementary post-processing step: enriching
transcribed dialogues by adding metadata tags for speaker characteristics such
as age, gender, and emotion. Some of the tags are global to the entire
dialogue, while some are time-variant. Our approach couples frozen audio
foundation models, such as Whisper or WavLM, with a frozen LLAMA language model
to infer these speaker attributes, without requiring task-specific fine-tuning
of either model. Using lightweight, efficient connectors to bridge audio and
language representations, we achieve competitive performance on speaker
profiling tasks while preserving modularity and speed. Additionally, we
demonstrate that a frozen LLAMA model can compare x-vectors directly, achieving
an Equal Error Rate of 8.8% in some scenarios.

</details>


### [239] [Parity-Aware Byte-Pair Encoding: Improving Cross-lingual Fairness in Tokenization](https://arxiv.org/abs/2508.04796)
*Negar Foroutan,Clara Meister,Debjit Paul,Joel Niklaus,Sina Ahmadi,Antoine Bosselut,Rico Sennrich*

Main category: cs.CL

TL;DR: 提出Parity - aware BPE改进NLP分词，提升跨语言公平性。


<details>
  <summary>Details</summary>
Motivation: 标准分词学习算法基于频率，对低资源语言不利，加剧不同语言背景用户的计算和经济不平等。

Method: 引入Parity - aware BPE，在每次合并步骤中最大化当前压缩最差语言的压缩增益。

Result: Parity - aware BPE使不同语言的分词数量更公平，对全局压缩率影响可忽略，对下游任务的语言模型性能无实质影响。

Conclusion: Parity - aware BPE能有效解决标准分词算法对低资源语言的问题，提升跨语言公平性。

Abstract: Tokenization is the first -- and often least scrutinized -- step of most NLP
pipelines. Standard algorithms for learning tokenizers rely on frequency-based
objectives, which favor languages dominant in the training data and
consequently leave lower-resource languages with tokenizations that are
disproportionately longer, morphologically implausible, or even riddled with
<UNK> placeholders. This phenomenon ultimately amplifies computational and
financial inequalities between users from different language backgrounds. To
remedy this, we introduce Parity-aware Byte Pair Encoding (BPE), a variant of
the widely-used BPE algorithm. At every merge step, Parity-aware BPE maximizes
the compression gain of the currently worst-compressed language, trading a
small amount of global compression for cross-lingual parity. We find
empirically that Parity-aware BPE leads to more equitable token counts across
languages, with negligible impact on global compression rate and no substantial
effect on language-model performance in downstream tasks.

</details>


### [240] [Persistent Instability in LLM's Personality Measurements: Effects of Scale, Reasoning, and Conversation History](https://arxiv.org/abs/2508.04826)
*Tommaso Tosato,Saskia Helbling,Yorguin-Jose Mantilla-Ramos,Mahmood Hegazy,Alberto Tosato,David John Lemay,Irina Rish,Guillaume Dumas*

Main category: cs.CL

TL;DR: 提出PERSIST评估框架测试25+开源模型，发现大语言模型响应不稳定，人格对齐策略可能不足。


<details>
  <summary>Details</summary>
Motivation: 大语言模型安全部署需一致行为模式，但人格特质理解不足。

Method: 用PERSIST框架，结合传统和LLM适配人格工具，改变问题顺序、释义等测试25+开源模型。

Result: 400B+模型响应变异大；提示重排使人格测量变化达20%；稳定行为干预可能增加变异；LLM适配工具同样不稳定。

Conclusion: 当前LLM缺乏行为一致性基础，人格对齐策略对安全关键应用可能不足。

Abstract: Large language models require consistent behavioral patterns for safe
deployment, yet their personality-like traits remain poorly understood. We
present PERSIST (PERsonality Stability in Synthetic Text), a comprehensive
evaluation framework testing 25+ open-source models (1B-671B parameters) across
500,000+ responses. Using traditional (BFI-44, SD3) and novel LLM-adapted
personality instruments, we systematically vary question order, paraphrasing,
personas, and reasoning modes. Our findings challenge fundamental deployment
assumptions: (1) Even 400B+ models exhibit substantial response variability (SD
> 0.4); (2) Minor prompt reordering alone shifts personality measurements by up
to 20%; (3) Interventions expected to stabilize behavior, such as
chain-of-thought reasoning, detailed personas instruction, inclusion of
conversation history, can paradoxically increase variability; (4) LLM-adapted
instruments show equal instability to human-centric versions, confirming
architectural rather than translational limitations. This persistent
instability across scales and mitigation strategies suggests current LLMs lack
the foundations for genuine behavioral consistency. For safety-critical
applications requiring predictable behavior, these findings indicate that
personality-based alignment strategies may be fundamentally inadequate.

</details>


### [241] [RCR-Router: Efficient Role-Aware Context Routing for Multi-Agent LLM Systems with Structured Memory](https://arxiv.org/abs/2508.04903)
*Jun Liu,Zhenglun Kong,Changdi Yang,Fan Yang,Tianqi Li,Peiyan Dong,Joannah Nanjekye,Hao Tang,Geng Yuan,Wei Niu,Wenbin Zhang,Pu Zhao,Xue Lin,Dong Huang,Yanzhi Wang*

Main category: cs.CL

TL;DR: 本文提出RCR - Router框架用于多智能体大语言模型系统，通过动态选择记忆子集减少令牌使用并维持答案质量，还提出评估指标。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体大语言模型协调方案存在令牌消耗过多、内存暴露冗余和适应性有限的问题。

Method: 引入RCR - Router框架，根据角色和任务阶段动态选择语义相关记忆子集，采用轻量级评分策略，迭代集成智能体输出到共享内存；提出答案质量评分指标。

Result: 在三个多跳问答基准测试中，RCR - Router最多减少30%的令牌使用，同时维持或提高答案质量。

Conclusion: 结构化内存路由和输出感知评估对推进可扩展多智能体大语言模型系统很重要。

Abstract: Multi-agent large language model (LLM) systems have shown strong potential in
complex reasoning and collaborative decision-making tasks. However, most
existing coordination schemes rely on static or full-context routing
strategies, which lead to excessive token consumption, redundant memory
exposure, and limited adaptability across interaction rounds. We introduce
RCR-Router, a modular and role-aware context routing framework designed to
enable efficient, adaptive collaboration in multi-agent LLMs. To our knowledge,
this is the first routing approach that dynamically selects semantically
relevant memory subsets for each agent based on its role and task stage, while
adhering to a strict token budget. A lightweight scoring policy guides memory
selection, and agent outputs are iteratively integrated into a shared memory
store to facilitate progressive context refinement. To better evaluate model
behavior, we further propose an Answer Quality Score metric that captures
LLM-generated explanations beyond standard QA accuracy. Experiments on three
multi-hop QA benchmarks -- HotPotQA, MuSiQue, and 2WikiMultihop -- demonstrate
that RCR-Router reduces token usage (up to 30%) while improving or maintaining
answer quality. These results highlight the importance of structured memory
routing and output-aware evaluation in advancing scalable multi-agent LLM
systems.

</details>


### [242] [Towards Robust Evaluation of Visual Activity Recognition: Resolving Verb Ambiguity with Sense Clustering](https://arxiv.org/abs/2508.04945)
*Louie Hong Yao,Nicholas Jarvis,Tianyu Jiang*

Main category: cs.CL

TL;DR: 现有视觉活动识别系统评估方法因动词语义和图像解读的模糊性存在不足，本文提出视觉语言聚类框架进行更鲁棒的评估，实验表明该方法更符合人类判断。


<details>
  <summary>Details</summary>
Motivation: 标准精确匹配评估方法无法捕捉动词语义和图像解读的模糊性，导致对模型性能评估不完整。

Method: 提出视觉语言聚类框架，构建动词义项簇。

Result: 分析imSitu数据集发现每张图像平均对应2.8个义项簇；评估多个活动识别模型，对比基于簇的评估与标准评估方法；人类一致性分析表明基于簇的评估更符合人类判断。

Conclusion: 基于簇的评估方法能对模型性能进行更细致的评估。

Abstract: Evaluating visual activity recognition systems is challenging due to inherent
ambiguities in verb semantics and image interpretation. When describing actions
in images, synonymous verbs can refer to the same event (e.g., brushing vs.
grooming), while different perspectives can lead to equally valid but distinct
verb choices (e.g., piloting vs. operating). Standard exact-match evaluation,
which relies on a single gold answer, fails to capture these ambiguities,
resulting in an incomplete assessment of model performance. To address this, we
propose a vision-language clustering framework that constructs verb sense
clusters, providing a more robust evaluation. Our analysis of the imSitu
dataset shows that each image maps to an average of 2.8 sense clusters, with
each cluster representing a distinct perspective of the image. We evaluate
multiple activity recognition models and compare our cluster-based evaluation
with standard evaluation methods. Additionally, our human alignment analysis
suggests that the cluster-based evaluation better aligns with human judgements,
offering a more nuanced assessment of model performance.

</details>


### [243] [A Multi-Stage Large Language Model Framework for Extracting Suicide-Related Social Determinants of Health](https://arxiv.org/abs/2508.05003)
*Song Wang,Yishu Wei,Haotian Ma,Max Lovitt,Kelly Deng,Yuan Meng,Zihan Xu,Jingze Zhang,Yunyu Xiao,Ying Ding,Xuhai Xu,Joydeep Ghosh,Yifan Peng*

Main category: cs.CL

TL;DR: 提出多阶段大语言模型框架提取自杀相关社会健康决定因素，性能提升且成本降低，增强了准确性和透明度。


<details>
  <summary>Details</summary>
Motivation: 理解导致自杀事件的社会健康决定因素对早期干预和预防至关重要，但数据驱动方法面临挑战。

Method: 提出多阶段大语言模型框架，与其他模型对比，通过自动化比较和用户试点研究评估。

Result: 框架在提取社会健康决定因素任务及相关上下文检索上性能提升，微调小模型以降低成本获得类似或更好效果，多阶段设计增强可解释性。

Conclusion: 该方法提高了从非结构化文本中提取自杀相关社会健康决定因素的准确性和透明度，有助于早期识别高危人群和制定预防策略。

Abstract: Background: Understanding social determinants of health (SDoH) factors
contributing to suicide incidents is crucial for early intervention and
prevention. However, data-driven approaches to this goal face challenges such
as long-tailed factor distributions, analyzing pivotal stressors preceding
suicide incidents, and limited model explainability. Methods: We present a
multi-stage large language model framework to enhance SDoH factor extraction
from unstructured text. Our approach was compared to other state-of-the-art
language models (i.e., pre-trained BioBERT and GPT-3.5-turbo) and reasoning
models (i.e., DeepSeek-R1). We also evaluated how the model's explanations help
people annotate SDoH factors more quickly and accurately. The analysis included
both automated comparisons and a pilot user study. Results: We show that our
proposed framework demonstrated performance boosts in the overarching task of
extracting SDoH factors and in the finer-grained tasks of retrieving relevant
context. Additionally, we show that fine-tuning a smaller, task-specific model
achieves comparable or better performance with reduced inference costs. The
multi-stage design not only enhances extraction but also provides intermediate
explanations, improving model explainability. Conclusions: Our approach
improves both the accuracy and transparency of extracting suicide-related SDoH
from unstructured texts. These advancements have the potential to support early
identification of individuals at risk and inform more effective prevention
strategies.

</details>


### [244] [Dialogues Aspect-based Sentiment Quadruple Extraction via Structural Entropy Minimization Partitioning](https://arxiv.org/abs/2508.05023)
*Kun Peng,Cong Cao,Hao Peng,Zhifeng Hao,Lei Jiang,Kongjing Gu,Yanbing Liu,Philip S. Yu*

Main category: cs.CL

TL;DR: 论文针对对话方面情感四元组提取问题，提出划分对话为语义独立子对话并采用两步框架提取的方法，实验表明该方法有良好效果和低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有方法学习全对话词关系，因对话含多个语义独立子对话会引入额外噪声，影响提取效果。

Method: 利用结构熵最小化算法划分对话，提出两步框架：先在语句级别提取情感元素，再在子对话级别匹配四元组。

Result: 所提方法在DiaASQ任务中达到了最先进的性能，且计算成本更低。

Conclusion: 基于结构熵最小化算法划分对话和两步框架的方法，能有效解决DiaASQ问题。

Abstract: Dialogues Aspect-based Sentiment Quadruple Extraction (DiaASQ) aims to
extract all target-aspect-opinion-sentiment quadruples from a given
multi-round, multi-participant dialogue. Existing methods typically learn word
relations across entire dialogues, assuming a uniform distribution of sentiment
elements. However, we find that dialogues often contain multiple semantically
independent sub-dialogues without clear dependencies between them. Therefore,
learning word relationships across the entire dialogue inevitably introduces
additional noise into the extraction process. To address this, our method
focuses on partitioning dialogues into semantically independent sub-dialogues.
Achieving completeness while minimizing these sub-dialogues presents a
significant challenge. Simply partitioning based on reply relationships is
ineffective. Instead, we propose utilizing a structural entropy minimization
algorithm to partition the dialogues. This approach aims to preserve relevant
utterances while distinguishing irrelevant ones as much as possible.
Furthermore, we introduce a two-step framework for quadruple extraction: first
extracting individual sentiment elements at the utterance level, then matching
quadruples at the sub-dialogue level. Extensive experiments demonstrate that
our approach achieves state-of-the-art performance in DiaASQ with much lower
computational costs.

</details>


### [245] [Evaluation of LLMs in AMR Parsing](https://arxiv.org/abs/2508.05028)
*Shu Han Ho*

Main category: cs.CL

TL;DR: 本文对四种大语言模型架构进行微调用于AMR解析评估，发现仅解码器大语言模型微调可取得与复杂SOTA解析器相当性能，LLaMA 3.2表现突出。


<details>
  <summary>Details</summary>
Motivation: 探索仅解码器大语言模型微调在AMR解析中的潜力。

Method: 使用LDC2020T02 Gold AMR3.0测试集对Phi 3.5、Gemma 2、LLaMA 3.2和DeepSeek R1 LLaMA Distilled四种模型进行微调评估。

Result: 仅解码器大语言模型微调能达到与复杂SOTA AMR解析器相当性能，LLaMA 3.2在简单微调下表现有竞争力，SMATCH F1达0.804；LLaMA 3.2语义性能领先，Phi 3.5结构有效性出色。

Conclusion: 仅解码器大语言模型微调是AMR解析有前景的方向。

Abstract: Meaning Representation (AMR) is a semantic formalism that encodes sentence
meaning as rooted, directed, acyclic graphs, where nodes represent concepts and
edges denote semantic relations. Finetuning decoder only Large Language Models
(LLMs) represent a promising novel straightfoward direction for AMR parsing.
This paper presents a comprehensive evaluation of finetuning four distinct LLM
architectures, Phi 3.5, Gemma 2, LLaMA 3.2, and DeepSeek R1 LLaMA Distilled
using the LDC2020T02 Gold AMR3.0 test set. Our results have shown that
straightfoward finetuning of decoder only LLMs can achieve comparable
performance to complex State of the Art (SOTA) AMR parsers. Notably, LLaMA 3.2
demonstrates competitive performance against SOTA AMR parsers given a
straightforward finetuning approach. We achieved SMATCH F1: 0.804 on the full
LDC2020T02 test split, on par with APT + Silver (IBM) at 0.804 and approaching
Graphene Smatch (MBSE) at 0.854. Across our analysis, we also observed a
consistent pattern where LLaMA 3.2 leads in semantic performance while Phi 3.5
excels in structural validity.

</details>


### [246] [Align, Don't Divide: Revisiting the LoRA Architecture in Multi-Task Learning](https://arxiv.org/abs/2508.05078)
*Jinda Liu,Bo Cheng,Yi Chang,Yuan Wu*

Main category: cs.CL

TL;DR: 研究挑战了多组件MTL范式，提出单适配器增加秩和Align - LoRA能有效适配大语言模型到多任务，且Align - LoRA效果最佳。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在多领域多任务场景下的适配问题，挑战多组件MTL范式。

Method: 先测试简化多头架构性能，再测试增加秩的单适配器LoRA，最后提出Align - LoRA并加入显式损失对齐任务表示。

Result: 简化多头架构优于复杂系统，增加秩的单适配器LoRA有竞争力，Align - LoRA显著超越所有基线。

Conclusion: 有效的多任务学习泛化依赖于学习鲁棒的共享表示，Align - LoRA建立了更简单有效的多任务适配范式。

Abstract: Parameter-Efficient Fine-Tuning (PEFT) is essential for adapting Large
Language Models (LLMs). In practice, LLMs are often required to handle a
diverse set of tasks from multiple domains, a scenario naturally addressed by
multi-task learning (MTL). Within this MTL context, a prevailing trend involves
LoRA variants with multiple adapters or heads, which advocate for structural
diversity to capture task-specific knowledge. Our findings present a direct
challenge to this paradigm. We first show that a simplified multi-head
architecture with high inter-head similarity substantially outperforms complex
multi-adapter and multi-head systems. This leads us to question the
multi-component paradigm itself, and we further demonstrate that a standard
single-adapter LoRA, with a sufficiently increased rank, also achieves highly
competitive performance. These results lead us to a new hypothesis: effective
MTL generalization hinges on learning robust shared representations, not
isolating task-specific features. To validate this, we propose Align-LoRA,
which incorporates an explicit loss to align task representations within the
shared adapter space. Experiments confirm that Align-LoRA significantly
surpasses all baselines, establishing a simpler yet more effective paradigm for
adapting LLMs to multiple tasks. The code is available at
https://github.com/jinda-liu/Align-LoRA.

</details>


### [247] [Attention Basin: Why Contextual Position Matters in Large Language Models](https://arxiv.org/abs/2508.05128)
*Zihao Yi,Delong Zeng,Zhenqing Ling,Haohao Luo,Zhe Xu,Wei Liu,Jian Luan,Wanxia Cao,Ying Shen*

Main category: cs.CL

TL;DR: 研究大语言模型位置偏差，提出Attention - Driven Reranking框架提升性能。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型对输入信息上下文位置敏感的机制，提升模型性能。

Method: 通过实验发现注意力盆地现象，基于此提出AttnRank两阶段框架，包括用校准集估计模型位置注意力偏好和重新排序内容。

Result: AttnRank在多跳问答和少样本上下文学习任务中，使10种不同架构和规模的大语言模型有显著提升。

Conclusion: AttnRank是模型无关、无需训练、即插即用且计算开销小的方法，能在不修改模型参数和训练流程下提升性能。

Abstract: The performance of Large Language Models (LLMs) is significantly sensitive to
the contextual position of information in the input. To investigate the
mechanism behind this positional bias, our extensive experiments reveal a
consistent phenomenon we term the attention basin: when presented with a
sequence of structured items (e.g., retrieved documents or few-shot examples),
models systematically assign higher attention to the items at the beginning and
end of the sequence, while neglecting those in the middle. Crucially, our
analysis further reveals that allocating higher attention to critical
information is key to enhancing model performance. Based on these insights, we
introduce Attention-Driven Reranking (AttnRank), a two-stage framework that (i)
estimates a model's intrinsic positional attention preferences using a small
calibration set, and (ii) reorders retrieved documents or few-shot examples to
align the most salient content with these high-attention positions. AttnRank is
a model-agnostic, training-free, and plug-and-play method with minimal
computational overhead. Experiments on multi-hop QA and few-shot in-context
learning tasks demonstrate that AttnRank achieves substantial improvements
across 10 large language models of varying architectures and scales, without
modifying model parameters or training procedures.

</details>


### [248] [Towards Assessing Medical Ethics from Knowledge to Practice](https://arxiv.org/abs/2508.05132)
*Chang Hong,Minghao Wu,Qingying Xiao,Yuchi Wang,Xiang Wan,Guangjun Yu,Benyou Wang,Yan Hu*

Main category: cs.CL

TL;DR: 提出PrinciplismQA基准评估大语言模型医疗伦理推理能力，发现模型伦理知识与应用有差距，医学领域微调可提升能力，该基准有助于改进医疗AI。


<details>
  <summary>Details</summary>
Motivation: 当前基准常忽视大语言模型在医疗领域的伦理推理评估，需要对其进行严格评估。

Method: 引入PrinciplismQA基准，包含3648个问题，有选择题和开放式问题，数据经医学专家验证。

Result: 模型伦理知识与实际应用有显著差距，多数模型在行善困境上有困难，前沿闭源模型领先，医学领域微调可提升伦理能力。

Conclusion: PrinciplismQA可诊断伦理弱点，为更平衡和负责任的医疗AI发展铺路。

Abstract: The integration of large language models into healthcare necessitates a
rigorous evaluation of their ethical reasoning, an area current benchmarks
often overlook. We introduce PrinciplismQA, a comprehensive benchmark with
3,648 questions designed to systematically assess LLMs' alignment with core
medical ethics. Grounded in Principlism, our benchmark features a high-quality
dataset. This includes multiple-choice questions curated from authoritative
textbooks and open-ended questions sourced from authoritative medical ethics
case study literature, all validated by medical experts. Our experiments reveal
a significant gap between models' ethical knowledge and their practical
application, especially in dynamically applying ethical principles to
real-world scenarios. Most LLMs struggle with dilemmas concerning Beneficence,
often over-emphasizing other principles. Frontier closed-source models, driven
by strong general capabilities, currently lead the benchmark. Notably, medical
domain fine-tuning can enhance models' overall ethical competence, but further
progress requires better alignment with medical ethical knowledge.
PrinciplismQA offers a scalable framework to diagnose these specific ethical
weaknesses, paving the way for more balanced and responsible medical AI.

</details>


### [249] [Resource-Limited Joint Multimodal Sentiment Reasoning and Classification via Chain-of-Thought Enhancement and Distillation](https://arxiv.org/abs/2508.05234)
*Haonan Shangguan,Xiaocui Yang,Shi Feng,Daling Wang,Yifei Zhang,Ge Yu*

Main category: cs.CL

TL;DR: 社交媒体多模态内容推动MSA发展，现有方法忽视资源受限环境下多模态情感推理生成，本文提出MulCoT - RD模型解决资源受限的JMSRC任务，实验表明其参数少但性能强。


<details>
  <summary>Details</summary>
Motivation: 当前方法主要利用大参数模型进行情感分类，忽略资源受限环境下自主多模态情感推理生成，需解决资源受限的联合多模态情感推理和分类问题。

Method: 提出Multimodal Chain - of - Thought Reasoning Distillation模型MulCoT - RD，采用“Teacher - Assistant - Student”蒸馏范式，先用大模型生成推理数据集，训练中型辅助模型，再联合训练轻量级学生模型。

Result: 在四个数据集上的大量实验表明，仅3B参数的MulCoT - RD在JMSRC任务上表现出色，有强泛化性和更好的可解释性。

Conclusion: MulCoT - RD能有效解决资源受限环境下的联合多模态情感推理和分类问题，参数少却能有良好性能。

Abstract: The surge in rich multimodal content on social media platforms has greatly
advanced Multimodal Sentiment Analysis (MSA), with Large Language Models (LLMs)
further accelerating progress in this field. Current approaches primarily
leverage the knowledge and reasoning capabilities of parameter-heavy
(Multimodal) LLMs for sentiment classification, overlooking autonomous
multimodal sentiment reasoning generation in resource-constrained environments.
Therefore, we focus on the Resource-Limited Joint Multimodal Sentiment
Reasoning and Classification task, JMSRC, which simultaneously performs
multimodal sentiment reasoning chain generation and sentiment classification
only with a lightweight model. We propose a Multimodal Chain-of-Thought
Reasoning Distillation model, MulCoT-RD, designed for JMSRC that employs a
"Teacher-Assistant-Student" distillation paradigm to address deployment
constraints in resource-limited environments. We first leverage a
high-performance Multimodal Large Language Model (MLLM) to generate the initial
reasoning dataset and train a medium-sized assistant model with a multi-task
learning mechanism. A lightweight student model is jointly trained to perform
efficient multimodal sentiment reasoning generation and classification.
Extensive experiments on four datasets demonstrate that MulCoT-RD with only 3B
parameters achieves strong performance on JMSRC, while exhibiting robust
generalization and enhanced interpretability.

</details>


### [250] [Pruning Large Language Models by Identifying and Preserving Functional Networks](https://arxiv.org/abs/2508.05239)
*Yiheng Liu,Junhao Ning,Sichen Xia,Xiaohui Gao,Ning Qiang,Bao Ge,Junwei Han,Xintao Hu*

Main category: cs.CL

TL;DR: 结构化剪枝对大语言模型压缩有重要价值，但现有方法忽视神经元交互致性能下降，本文受人脑启发提出通过识别和保留功能网络剪枝的方法，实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 现有结构化剪枝方法忽视人工神经元间交互与协作，导致大语言模型宏观功能架构破坏和剪枝性能下降，需改进。

Method: 将大语言模型视为数字大脑，分解为功能网络，保留这些功能网络中的关键神经元进行剪枝。

Result: 实验表明该方法能成功识别和定位大语言模型中的功能网络和关键神经元，实现高效模型剪枝。

Conclusion: 提出的基于识别和保留功能网络的剪枝方法有效，可提高大语言模型剪枝效率。

Abstract: Structured pruning is one of the representative techniques for compressing
large language models (LLMs) to reduce GPU memory consumption and accelerate
inference speed. It offers significant practical value in improving the
efficiency of LLMs in real-world applications. Current structured pruning
methods typically rely on assessment of the importance of the structure units
and pruning the units with less importance. Most of them overlooks the
interaction and collaboration among artificial neurons that are crucial for the
functionalities of LLMs, leading to a disruption in the macro functional
architecture of LLMs and consequently a pruning performance degradation.
Inspired by the inherent similarities between artificial neural networks and
functional neural networks in the human brain, we alleviate this challenge and
propose to prune LLMs by identifying and preserving functional networks within
LLMs in this study. To achieve this, we treat an LLM as a digital brain and
decompose the LLM into functional networks, analogous to identifying functional
brain networks in neuroimaging data. Afterwards, an LLM is pruned by preserving
the key neurons within these functional networks. Experimental results
demonstrate that the proposed method can successfully identify and locate
functional networks and key neurons in LLMs, enabling efficient model pruning.
Our code is available at https://github.com/WhatAboutMyStar/LLM_ACTIVATION.

</details>


### [251] [Efficient Reasoning for Large Reasoning Language Models via Certainty-Guided Reflection Suppression](https://arxiv.org/abs/2508.05337)
*Jiameng Huang,Baijiong Lin,Guhao Feng,Jierun Chen,Di He,Lu Hou*

Main category: cs.CL

TL;DR: 提出CGRS方法缓解大推理语言模型过度思考问题，实验证明其有效减少token使用并保持准确率。


<details>
  <summary>Details</summary>
Motivation: 现有大推理语言模型的反思行为会导致过度思考问题，增加token使用、推理成本并降低实用性。

Method: 提出确定性引导的反思抑制（CGRS）方法，在模型对当前响应有高置信度时动态抑制反思触发词的生成。

Result: 在四个推理基准测试中，平均减少18.5% - 41.9%的token使用，保持准确率，在长度减少和性能之间达到最优平衡。

Conclusion: CGRS具有实用性，能实现高效推理，且与模型架构和规模无关。

Abstract: Recent Large Reasoning Language Models (LRLMs) employ long chain-of-thought
reasoning with complex reflection behaviors, typically signaled by specific
trigger words (e.g., "Wait" and "Alternatively") to enhance performance.
However, these reflection behaviors can lead to the overthinking problem where
the generation of redundant reasoning steps that unnecessarily increase token
usage, raise inference costs, and reduce practical utility. In this paper, we
propose Certainty-Guided Reflection Suppression (CGRS), a novel method that
mitigates overthinking in LRLMs while maintaining reasoning accuracy. CGRS
operates by dynamically suppressing the model's generation of reflection
triggers when it exhibits high confidence in its current response, thereby
preventing redundant reflection cycles without compromising output quality. Our
approach is model-agnostic, requires no retraining or architectural
modifications, and can be integrated seamlessly with existing autoregressive
generation pipelines. Extensive experiments across four reasoning benchmarks
(i.e., AIME24, AMC23, MATH500, and GPQA-D) demonstrate CGRS's effectiveness: it
reduces token usage by an average of 18.5% to 41.9% while preserving accuracy.
It also achieves the optimal balance between length reduction and performance
compared to state-of-the-art baselines. These results hold consistently across
model architectures (e.g., DeepSeek-R1-Distill series, QwQ-32B, and Qwen3
family) and scales (4B to 32B parameters), highlighting CGRS's practical value
for efficient reasoning.

</details>


### [252] [MyCulture: Exploring Malaysia's Diverse Culture under Low-Resource Language Constraints](https://arxiv.org/abs/2508.05429)
*Zhong Ken Hew,Jia Xin Low,Sze Jue Yang,Chee Seng chan*

Main category: cs.CL

TL;DR: 论文介绍了评估大语言模型对马来西亚文化理解的基准MyCulture，指出当前大模型存在文化偏差，评估显示不同模型文化理解差异大，需文化和语言包容的基准。


<details>
  <summary>Details</summary>
Motivation: 大语言模型因训练数据以高资源语言为主，存在文化偏差，难以准确呈现和评估多元文化背景，尤其是低资源语言环境。

Method: 引入MyCulture基准，以马来语呈现，涵盖马来西亚文化六方面，采用开放式选择题格式，分析结构偏差和语言偏差。

Result: 对一系列地区和国际大语言模型的评估显示，不同模型在文化理解上存在显著差异。

Conclusion: 迫切需要在大语言模型的开发和评估中使用基于文化且语言包容的基准。

Abstract: Large Language Models (LLMs) often exhibit cultural biases due to training
data dominated by high-resource languages like English and Chinese. This poses
challenges for accurately representing and evaluating diverse cultural
contexts, particularly in low-resource language settings. To address this, we
introduce MyCulture, a benchmark designed to comprehensively evaluate LLMs on
Malaysian culture across six pillars: arts, attire, customs, entertainment,
food, and religion presented in Bahasa Melayu. Unlike conventional benchmarks,
MyCulture employs a novel open-ended multiple-choice question format without
predefined options, thereby reducing guessing and mitigating format bias. We
provide a theoretical justification for the effectiveness of this open-ended
structure in improving both fairness and discriminative power. Furthermore, we
analyze structural bias by comparing model performance on structured versus
free-form outputs, and assess language bias through multilingual prompt
variations. Our evaluation across a range of regional and international LLMs
reveals significant disparities in cultural comprehension, highlighting the
urgent need for culturally grounded and linguistically inclusive benchmarks in
the development and assessment of LLMs.

</details>


### [253] [LAG: Logic-Augmented Generation from a Cartesian Perspective](https://arxiv.org/abs/2508.05509)
*Yilin Xiao,Chuang Zhou,Qinggang Zhang,Su Dong,Shengyuan Chen,Xiao Huang*

Main category: cs.CL

TL;DR: 提出逻辑增强生成 (LAG) 范式，分解问题并按逻辑依赖推理，实验证明可提升推理鲁棒性、减少幻觉。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在知识密集型任务有局限，检索增强生成在复杂推理场景有不足。

Method: 将复杂问题分解为原子子问题，按逻辑顺序解决，结合逻辑终止机制防止错误传播，最后合成验证响应。

Result: 在四个基准数据集实验中，LAG 显著提升推理鲁棒性、减少幻觉，使大语言模型解决问题更符合人类认知。

Conclusion: LAG 为现有检索增强生成系统提供了有原则的替代方案。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities across
a wide range of tasks, yet exhibit critical limitations in knowledge-intensive
tasks, often generating hallucinations when faced with questions requiring
specialized expertise. While retrieval-augmented generation (RAG) mitigates
this by integrating external knowledge, it struggles with complex reasoning
scenarios due to its reliance on direct semantic retrieval and lack of
structured logical organization. Inspired by Cartesian principles from
\textit{Discours de la m\'ethode}, this paper introduces Logic-Augmented
Generation (LAG), a novel paradigm that reframes knowledge augmentation through
systematic question decomposition and dependency-aware reasoning. Specifically,
LAG first decomposes complex questions into atomic sub-questions ordered by
logical dependencies. It then resolves these sequentially, using prior answers
to guide context retrieval for subsequent sub-questions, ensuring stepwise
grounding in logical chain. To prevent error propagation, LAG incorporates a
logical termination mechanism that halts inference upon encountering
unanswerable sub-questions and reduces wasted computation on excessive
reasoning. Finally, it synthesizes all sub-resolutions to generate verified
responses. Experiments on four benchmark datasets demonstrate that LAG
significantly enhances reasoning robustness, reduces hallucination, and aligns
LLM problem-solving with human cognition, offering a principled alternative to
existing RAG systems.

</details>


### [254] [The World According to LLMs: How Geographic Origin Influences LLMs' Entity Deduction Capabilities](https://arxiv.org/abs/2508.05525)
*Harsh Nishant Lalai,Raj Sanjay Shah,Jiaxin Pei,Sashank Varma,Yi-Chia Wang,Ali Emami*

Main category: cs.CL

TL;DR: 研究通过20问游戏评估大语言模型隐式偏差，发现模型在推断全球北方和西方实体时更成功，揭示推理过程中的地理和文化差异。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽经调优减少显式偏差，但预训练数据仍存在隐式偏差，需新方法研究。

Method: 用新数据集Geo20Q+，在两种游戏配置和七种语言下对大语言模型进行20问游戏评估。

Result: 大语言模型推断全球北方和西方实体更成功，维基页面浏览量和预训练语料频率不能完全解释差异，游戏语言对表现差距影响小。

Conclusion: 创造性自由评估框架有助于发现标准提示设置中隐藏的隐式偏差，模型推理过程存在地理和文化差异。

Abstract: Large Language Models (LLMs) have been extensively tuned to mitigate explicit
biases, yet they often exhibit subtle implicit biases rooted in their
pre-training data. Rather than directly probing LLMs with human-crafted
questions that may trigger guardrails, we propose studying how models behave
when they proactively ask questions themselves. The 20 Questions game, a
multi-turn deduction task, serves as an ideal testbed for this purpose. We
systematically evaluate geographic performance disparities in entity deduction
using a new dataset, Geo20Q+, consisting of both notable people and culturally
significant objects (e.g., foods, landmarks, animals) from diverse regions. We
test popular LLMs across two gameplay configurations (canonical 20-question and
unlimited turns) and in seven languages (English, Hindi, Mandarin, Japanese,
French, Spanish, and Turkish). Our results reveal geographic disparities: LLMs
are substantially more successful at deducing entities from the Global North
than the Global South, and the Global West than the Global East. While
Wikipedia pageviews and pre-training corpus frequency correlate mildly with
performance, they fail to fully explain these disparities. Notably, the
language in which the game is played has minimal impact on performance gaps.
These findings demonstrate the value of creative, free-form evaluation
frameworks for uncovering subtle biases in LLMs that remain hidden in standard
prompting setups. By analyzing how models initiate and pursue reasoning goals
over multiple turns, we find geographic and cultural disparities embedded in
their reasoning processes. We release the dataset (Geo20Q+) and code at
https://sites.google.com/view/llmbias20q/home.

</details>


### [255] [Conformal Sets in Multiple-Choice Question Answering under Black-Box Settings with Provable Coverage Guarantees](https://arxiv.org/abs/2508.05544)
*Guang Yang,Xinyang Liu*

Main category: cs.CL

TL;DR: 提出基于频率的不确定性量化方法，在多数据集实验中表现良好，为多选题提供可靠不确定性量化框架。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在多选题回答中存在不可靠性，限制其在高风险领域应用，需解决不确定性量化问题。

Method: 在黑盒设置下提出基于频率的不确定性量化方法，利用共形预测，对模型输出分布多次独立采样，用最频繁样本计算预测熵。

Result: 基于频率的预测熵在区分正确和错误预测上优于基于对数几率的预测熵，能有效控制经验误覆盖率。

Conclusion: 该方法提供了无分布、与模型无关的框架，可保证覆盖范围，增强大语言模型在实际应用中的可信度。

Abstract: Large Language Models (LLMs) have shown remarkable progress in
multiple-choice question answering (MCQA), but their inherent unreliability,
such as hallucination and overconfidence, limits their application in high-risk
domains. To address this, we propose a frequency-based uncertainty
quantification method under black-box settings, leveraging conformal prediction
(CP) to ensure provable coverage guarantees. Our approach involves multiple
independent samplings of the model's output distribution for each input, with
the most frequent sample serving as a reference to calculate predictive entropy
(PE). Experimental evaluations across six LLMs and four datasets (MedMCQA,
MedQA, MMLU, MMLU-Pro) demonstrate that frequency-based PE outperforms
logit-based PE in distinguishing between correct and incorrect predictions, as
measured by AUROC. Furthermore, the method effectively controls the empirical
miscoverage rate under user-specified risk levels, validating that sampling
frequency can serve as a viable substitute for logit-based probabilities in
black-box scenarios. This work provides a distribution-free model-agnostic
framework for reliable uncertainty quantification in MCQA with guaranteed
coverage, enhancing the trustworthiness of LLMs in practical applications.

</details>


### [256] [Cooper: Co-Optimizing Policy and Reward Models in Reinforcement Learning for Large Language Models](https://arxiv.org/abs/2508.05613)
*Haitao Hong,Yuchen Yan,Xingyu Wu,Guiyang Hou,Wenqi Zhang,Weiming Lu,Yongliang Shen,Jun Xiao*

Main category: cs.CL

TL;DR: 本文提出Cooper框架联合优化策略模型和奖励模型，引入混合注释策略和基于参考的奖励建模范式，实验证明其能缓解奖励破解并提升端到端强化学习性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于模型和基于规则的奖励范式存在局限性，如基于规则的奖励缺乏鲁棒性，基于模型的奖励易受奖励破解影响，需提出新方法解决这些问题。

Method: 提出Cooper框架联合优化策略模型和奖励模型，利用基于规则奖励识别正确响应的高精度，动态构建和选择正负样本对训练奖励模型；引入混合注释策略生成奖励模型训练数据；提出基于参考的奖励建模范式，训练奖励模型VerifyRM。

Result: VerifyRM在VerifyBench上比同尺寸其他模型精度更高；Cooper缓解了奖励破解问题，提升了端到端强化学习性能，如在Qwen2.5 - 1.5B - Instruct上平均准确率提升0.54%。

Conclusion: 动态更新奖励模型是对抗奖励破解的有效方法，为奖励模型更好融入强化学习提供参考。

Abstract: Large language models (LLMs) have demonstrated remarkable performance in
reasoning tasks, where reinforcement learning (RL) serves as a key algorithm
for enhancing their reasoning capabilities. Currently, there are two mainstream
reward paradigms: model-based rewards and rule-based rewards. However, both
approaches suffer from limitations: rule-based rewards lack robustness, while
model-based rewards are vulnerable to reward hacking. To address these issues,
we propose Cooper(Co-optimizing Policy Model and Reward Model), a RL framework
that jointly optimizes both the policy model and the reward model. Cooper
leverages the high precision of rule-based rewards when identifying correct
responses, and dynamically constructs and selects positive-negative sample
pairs for continued training the reward model. This design enhances robustness
and mitigates the risk of reward hacking. To further support Cooper, we
introduce a hybrid annotation strategy that efficiently and accurately
generates training data for the reward model. We also propose a reference-based
reward modeling paradigm, where the reward model takes a reference answer as
input. Based on this design, we train a reward model named VerifyRM, which
achieves higher accuracy on VerifyBench compared to other models of the same
size. We conduct reinforcement learning using both VerifyRM and Cooper. Our
experiments show that Cooper not only alleviates reward hacking but also
improves end-to-end RL performance, for instance, achieving a 0.54% gain in
average accuracy on Qwen2.5-1.5B-Instruct. Our findings demonstrate that
dynamically updating reward model is an effective way to combat reward hacking,
providing a reference for better integrating reward models into RL.

</details>


### [257] [OmniEAR: Benchmarking Agent Reasoning in Embodied Tasks](https://arxiv.org/abs/2508.05614)
*Zixuan Wang,Dingming Li,Hongxing Li,Shuo Chen,Yuchen Yan,Wenqi Zhang,Yongliang Shen,Weiming Lu,Jun Xiao,Yueting Zhuang*

Main category: cs.CL

TL;DR: 提出评估语言模型具身推理能力的OmniEAR框架，发现模型在具身推理存在性能问题及架构局限。


<details>
  <summary>Details</summary>
Motivation: 大语言模型具身智能推理能力未被充分探索，需评估其在具身任务中的推理能力。

Method: 构建OmniEAR框架，通过文本环境表征，对1500个场景建模评估。

Result: 模型在约束推理时性能下降，完整环境信息影响协调性能，微调对单智能体任务提升大，多智能体任务提升小。

Conclusion: 具身推理挑战与现有模型能力不匹配，OmniEAR可评估和推进具身AI系统。

Abstract: Large language models excel at abstract reasoning but their capacity for
embodied agent reasoning remains largely unexplored. We present OmniEAR, a
comprehensive framework for evaluating how language models reason about
physical interactions, tool usage, and multi-agent coordination in embodied
tasks. Unlike existing benchmarks that provide predefined tool sets or explicit
collaboration directives, OmniEAR requires agents to dynamically acquire
capabilities and autonomously determine coordination strategies based on task
demands. Through text-based environment representation, we model continuous
physical properties and complex spatial relationships across 1,500 scenarios
spanning household and industrial domains. Our systematic evaluation reveals
severe performance degradation when models must reason from constraints: while
achieving 85-96% success with explicit instructions, performance drops to
56-85% for tool reasoning and 63-85% for implicit collaboration, with compound
tasks showing over 50% failure rates. Surprisingly, complete environmental
information degrades coordination performance, indicating models cannot filter
task-relevant constraints. Fine-tuning improves single-agent tasks dramatically
(0.6% to 76.3%) but yields minimal multi-agent gains (1.5% to 5.5%), exposing
fundamental architectural limitations. These findings demonstrate that embodied
reasoning poses fundamentally different challenges than current models can
address, establishing OmniEAR as a rigorous benchmark for evaluating and
advancing embodied AI systems. Our code and data are included in the
supplementary materials and will be open-sourced upon acceptance.

</details>


### [258] [How Do LLMs Persuade? Linear Probes Can Uncover Persuasion Dynamics in Multi-Turn Conversations](https://arxiv.org/abs/2508.05625)
*Brandon Jaipersaud,David Krueger,Ekdeep Singh Lubana*

Main category: cs.CL

TL;DR: 本文应用线性探针研究大语言模型在多轮对话中的说服动态，发现探针能捕捉说服各方面特征，且比基于提示的方法更高效。


<details>
  <summary>Details</summary>
Motivation: 目前对大语言模型说服人类的动态过程理解有限，且已有工作用线性探针研究大语言模型其他技能，因此应用探针研究说服动态。

Method: 借鉴认知科学的见解，在说服的不同方面（说服成功、被说服者个性、说服策略）上训练探针。

Result: 探针能在样本和数据集层面捕捉说服的各个方面，比基于提示的方法更快，在某些情况下表现相当甚至更优。

Conclusion: 探针是研究欺骗和操纵等复杂行为的可行途径，尤其适用于多轮对话和大规模数据集分析。

Abstract: Large Language Models (LLMs) have started to demonstrate the ability to
persuade humans, yet our understanding of how this dynamic transpires is
limited. Recent work has used linear probes, lightweight tools for analyzing
model representations, to study various LLM skills such as the ability to model
user sentiment and political perspective. Motivated by this, we apply probes to
study persuasion dynamics in natural, multi-turn conversations. We leverage
insights from cognitive science to train probes on distinct aspects of
persuasion: persuasion success, persuadee personality, and persuasion strategy.
Despite their simplicity, we show that they capture various aspects of
persuasion at both the sample and dataset levels. For instance, probes can
identify the point in a conversation where the persuadee was persuaded or where
persuasive success generally occurs across the entire dataset. We also show
that in addition to being faster than expensive prompting-based approaches,
probes can do just as well and even outperform prompting in some settings, such
as when uncovering persuasion strategy. This suggests probes as a plausible
avenue for studying other complex behaviours such as deception and
manipulation, especially in multi-turn settings and large-scale dataset
analysis where prompting-based methods would be computationally inefficient.

</details>


### [259] [H-Net++: Hierarchical Dynamic Chunking for Tokenizer-Free Language Modelling in Morphologically-Rich Languages](https://arxiv.org/abs/2508.05628)
*Mehrdad Zakershahrak,Samira Ghodratnama*

Main category: cs.CL

TL;DR: 提出H - NET++模型解决形态丰富语言字节级语言模型计算挑战，在波斯语语料上获SOTA结果


<details>
  <summary>Details</summary>
Motivation: 字节级语言模型在形态丰富语言中存在计算挑战，需更好解决方案

Method: 提出H - NET++分层动态分块模型，包含轻量级Transformer上下文混合器、两级潜在超先验、特殊处理拼写伪影和基于课程的训练

Result: 在14亿词波斯语语料上，相比BPE - based GPT - 2 - fa降低0.159 BPB，ParsGLUE提升5.4pp，对ZWNJ损坏的鲁棒性提高53%，形态边界F1达73.8%

Conclusion: 分层动态分块为形态丰富语言提供了无分词器的有效解决方案，且保持计算效率

Abstract: Byte-level language models eliminate fragile tokenizers but face
computational challenges in morphologically-rich languages (MRLs), where words
span many bytes. We propose H-NET++, a hierarchical dynamic-chunking model that
learns linguistically-informed segmentation through end-to-end training. Key
innovations include: (1) a lightweight Transformer context-mixer (1.9M
parameters) for cross-chunk attention, (2) a two-level latent hyper-prior for
document-level consistency, (3) specialized handling of orthographic artifacts
(e.g. Persian ZWNJ), and (4) curriculum-based training with staged sequence
lengths. On a 1.4B-token Persian corpus, H-NET++ achieves state-of-the-art
results: 0.159 BPB reduction versus BPE-based GPT-2-fa (12% better
compression), 5.4pp gain on ParsGLUE, 53% improved robustness to ZWNJ
corruption, and 73.8% F1 on gold morphological boundaries. Our learned chunks
align with Persian morphology without explicit supervision, demonstrating that
hierarchical dynamic chunking provides an effective tokenizer-free solution for
MRLs while maintaining computational efficiency.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [260] [Anti-Jamming Sensing with Distributed Reconfigurable Intelligent Metasurface Antennas](https://arxiv.org/abs/2508.04964)
*Zhaowei Wang,Yunsong Huang,Weicheng Liu,Hui-Ming Wang*

Main category: eess.SP

TL;DR: 本文提出使用分布式可重构智能超表面天线（RIMSA）进行无线感知，引入DRL算法和神经网络解决问题，设计组合损失函数应对干扰，仿真表明该系统比集中式更高效且抗干扰。


<details>
  <summary>Details</summary>
Motivation: 传统RF感知方法受不利传播信道影响，且存在干扰攻击，需提升感知准确性和抗干扰能力。

Method: 提出分布式RIMSA系统，将RF感知问题建模为联合优化问题，用DRL算法计算最优波束赋形模式，用神经网络转换接收信号，设计组合损失函数。

Result: 分布式RIMSA系统比集中式实现更高效，能更好克服环境影响，在干扰攻击下也能保证高准确性。

Conclusion: 分布式RIMSA系统可实现高效感知，有效克服环境影响和干扰攻击，提升感知性能。

Abstract: The utilization of radio frequency (RF) signals for wireless sensing has
garnered increasing attention. However, the radio environment is unpredictable
and often unfavorable, the sensing accuracy of traditional RF sensing methods
is often affected by adverse propagation channels from the transmitter to the
receiver, such as fading and noise. In this paper, we propose employing
distributed Reconfigurable Intelligent Metasurface Antennas (RIMSA) to detect
the presence and location of objects where multiple RIMSA receivers (RIMSA Rxs)
are deployed on different places. By programming their beamforming patterns,
RIMSA Rxs can enhance the quality of received signals. The RF sensing problem
is modeled as a joint optimization problem of beamforming pattern and mapping
of received signals to sensing outcomes. To address this challenge, we
introduce a deep reinforcement learning (DRL) algorithm aimed at calculating
the optimal beamforming patterns and a neural network aimed at converting
received signals into sensing outcomes. In addition, the malicious attacker may
potentially launch jamming attack to disrupt sensing process. To enable
effective sensing in interferenceprone environment, we devise a combined loss
function that takes into account the Signal to Interference plus Noise Ratio
(SINR) of the received signals. The simulation results show that the proposed
distributed RIMSA system can achieve more efficient sensing performance and
better overcome environmental influences than centralized implementation.
Furthermore, the introduced method ensures high-accuracy sensing performance
even under jamming attack.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [261] [Fairness in Dysarthric Speech Synthesis: Understanding Intrinsic Bias in Dysarthric Speech Cloning using F5-TTS](https://arxiv.org/abs/2508.05102)
*Anuprabha M,Krishna Gurugubelli,Anil Kumar Vuppala*

Main category: eess.AS

TL;DR: 研究F5 - TTS克隆构音障碍语音的效果及潜在偏差，发现其在合成中对语音可懂度有强偏差。


<details>
  <summary>Details</summary>
Motivation: 构音障碍语音数据有限，现有神经语音合成技术可能引入偏差，需研究有效克隆方法并评估偏差。

Method: 使用TORGO数据集研究F5 - TTS克隆构音障碍语音的效果，用公平性指标分析潜在偏差。

Result: F5 - TTS在构音障碍语音合成中对语音可懂度的偏差强于对说话人特征和韵律的保留。

Conclusion: 研究结果有助于整合公平感知的构音障碍语音合成，推动更具包容性的语音技术发展。

Abstract: Dysarthric speech poses significant challenges in developing assistive
technologies, primarily due to the limited availability of data. Recent
advances in neural speech synthesis, especially zero-shot voice cloning,
facilitate synthetic speech generation for data augmentation; however, they may
introduce biases towards dysarthric speech. In this paper, we investigate the
effectiveness of state-of-the-art F5-TTS in cloning dysarthric speech using
TORGO dataset, focusing on intelligibility, speaker similarity, and prosody
preservation. We also analyze potential biases using fairness metrics like
Disparate Impact and Parity Difference to assess disparities across dysarthric
severity levels. Results show that F5-TTS exhibits a strong bias toward speech
intelligibility over speaker and prosody preservation in dysarthric speech
synthesis. Insights from this study can help integrate fairness-aware
dysarthric speech synthesis, fostering the advancement of more inclusive speech
technologies.

</details>


### [262] [Speech LLMs in Low-Resource Scenarios: Data Volume Requirements and the Impact of Pretraining on High-Resource Languages](https://arxiv.org/abs/2508.05149)
*Seraphina Fong,Marco Matassoni,Alessio Brutti*

Main category: eess.AS

TL;DR: 本文探讨在低资源自动语音识别中使用语音大语言模型，利用SLAM - ASR框架，评估训练数据量需求，展示预训练投影器可减少数据稀缺影响，并在多个基准测试上评估性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在高资源语言语音处理中表现出色，但在低资源场景适用性探索不足，因此研究其在低资源自动语音识别中的应用。

Method: 使用SLAM - ASR框架，通过可训练的轻量级投影器连接语音编码器和大语言模型，评估训练数据量需求，利用在高资源语言上预训练的单语或多语投影器。

Result: 重新强调了数据有限的挑战，发现预训练投影器可减少数据稀缺影响，尤其在小训练集情况下，在多个公共基准测试上评估了性能。

Conclusion: 为未来优化低资源语言和多语言语音大语言模型的研究提供了见解。

Abstract: Large language models (LLMs) have demonstrated potential in handling spoken
inputs for high-resource languages, reaching state-of-the-art performance in
various tasks. However, their applicability is still less explored in
low-resource settings. This work investigates the use of Speech LLMs for
low-resource Automatic Speech Recognition using the SLAM-ASR framework, where a
trainable lightweight projector connects a speech encoder and a LLM. Firstly,
we assess training data volume requirements to match Whisper-only performance,
re-emphasizing the challenges of limited data. Secondly, we show that
leveraging mono- or multilingual projectors pretrained on high-resource
languages reduces the impact of data scarcity, especially with small training
sets. Using multilingual LLMs (EuroLLM, Salamandra) with
whisper-large-v3-turbo, we evaluate performance on several public benchmarks,
providing insights for future research on optimizing Speech LLMs for
low-resource languages and multilinguality.

</details>


### [263] [Keyword Spotting with Hyper-Matched Filters for Small Footprint Devices](https://arxiv.org/abs/2508.04857)
*Yael Segal-Feldman,Ann R. Bradlow,Matthew Goldrick,Joseph Keshet*

Main category: eess.AS

TL;DR: 本文提出适用于小尺寸设备的开放词汇关键词检测模型，性能达最优，小模型效率和鲁棒性佳。


<details>
  <summary>Details</summary>
Motivation: 实现小尺寸设备上开放词汇关键词检测并达到先进检测精度。

Method: 模型由语音编码器（tiny Whisper或tiny Conformer）、目标关键词编码器（超网络）和检测网络组成，检测网络用匹配滤波器权重卷积并引导Perceiver模块交叉注意力机制。

Result: 系统达到先进检测性能，能有效泛化到域外条件，小模型匹配或超越大模型。

Conclusion: 所提模型在开放词汇关键词检测任务中高效且鲁棒，适用于小尺寸设备。

Abstract: Open-vocabulary keyword spotting (KWS) refers to the task of detecting words
or terms within speech recordings, regardless of whether they were included in
the training data. This paper introduces an open-vocabulary keyword spotting
model with state-of-the-art detection accuracy for small-footprint devices. The
model is composed of a speech encoder, a target keyword encoder, and a
detection network. The speech encoder is either a tiny Whisper or a tiny
Conformer. The target keyword encoder is implemented as a hyper-network that
takes the desired keyword as a character string and generates a unique set of
weights for a convolutional layer, which can be considered as a
keyword-specific matched filter. The detection network uses the matched-filter
weights to perform a keyword-specific convolution, which guides the
cross-attention mechanism of a Perceiver module in determining whether the
target term appears in the recording. The results indicate that our system
achieves state-of-the-art detection performance and generalizes effectively to
out-of-domain conditions, including second-language (L2) speech. Notably, our
smallest model, with just 4.2 million parameters, matches or outperforms models
that are several times larger, demonstrating both efficiency and robustness.

</details>


<div id='cs.CC'></div>

# cs.CC [[Back]](#toc)

### [264] [NP-Hardness and ETH-Based Inapproximability of Communication Complexity via Relaxed Interlacing](https://arxiv.org/abs/2508.05597)
*Serge Gaspers,Zixu He,Simon Mackenzie*

Main category: cs.CC

TL;DR: 证明计算布尔函数确定性通信复杂度 D(f) 是 NP 难问题，即使协议交替次数为常数，构造方法有复用性和扩展性。


<details>
  <summary>Details</summary>
Motivation: 解决 Yao 在 1979 年提出的关于布尔函数确定性通信复杂度计算复杂度的问题。

Method: 基于 Mackenzie 和 Saffidine 的结构“交织”引理进行归约构造，利用自相似小工具递归嵌入。

Result: 证明计算 D(f) 是 NP 难问题，且在更强限制下成立；在指数时间假设下有无限增长的不可近似性差距。

Conclusion: 解决了确定性通信复杂度的计算复杂度问题，所开发的模块化框架有助于解决通信复杂度领域的其他长期问题。

Abstract: We prove that computing the deterministic communication complexity D(f) of a
Boolean function is NP-hard, even when protocols are limited to a constant
number of alternations, resolving a question first posed by Yao (1979). Our
reduction builds and expands on a suite of structural "interlacing" lemmas
introduced by Mackenzie and Saffidine (arXiv:2411.19003); these lemmas can be
reused as black boxes in future lower-bound constructions.
  The instances produced by our reduction admit optimal protocols that use only
constant alternations, so NP-hardness holds under stronger restrictions than
those considered in concurrent and independent work by Hirahara, Ilango, and
Loff (arXiv:2507.10426), whose proof requires unbounded alternations.
  Because the gadgets in our construction are self-similar, they can be
recursively embedded. We sketch how this yields, under the Exponential-Time
Hypothesis, an additive inapproximability gap that grows without bound, and we
outline a route toward NP-hardness of approximating D(f) within a fixed
constant additive error. Full details of the ETH-based inapproximability
results will appear in a future version.
  Beyond settling the complexity of deterministic communication complexity
itself, the modular framework we develop opens the door to a wider class of
reductions and, we believe, will prove useful in tackling other long-standing
questions in communication complexity.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [265] [Minimum-Weight Parity Factor Decoder for Quantum Error Correction](https://arxiv.org/abs/2508.04969)
*Yue Wu,Binghong Li,Kathleen Chang,Shruti Puri,Lin Zhong*

Main category: quant-ph

TL;DR: 提出HyperBlossom框架用于量子纠错解码，实现软件Hyperion，在逻辑错误率和运行时间上表现良好。


<details>
  <summary>Details</summary>
Motivation: 快速准确的量子纠错解码对可扩展容错量子计算至关重要，传统MLE解码在通用量子低密度奇偶校验码上难以处理，需改进。

Method: 提出HyperBlossom框架，将MLE解码表述为最小权重奇偶因子问题，推广开花算法到超图，实现软件Hyperion。

Result: Hyperion在距离 - 11表面码上逻辑错误率比MWPM解码器低4.8倍，在$[[90, 8, 10]]$双变量自行车码上比微调的BPOSD解码器低1.6倍，在表面码和颜色码上平均运行时间接近线性扩展。

Conclusion: HyperBlossom框架统一了现有基于图的解码器，弥合了启发式和解码验证器之间的差距，在量子纠错解码上有良好表现。

Abstract: Fast and accurate quantum error correction (QEC) decoding is crucial for
scalable fault-tolerant quantum computation. Most-Likely-Error (MLE) decoding,
while being near-optimal, is intractable on general quantum Low-Density
Parity-Check (qLDPC) codes and typically relies on approximation and
heuristics. We propose HyperBlossom, a unified framework that formulates MLE
decoding as a Minimum-Weight Parity Factor (MWPF) problem and generalizes the
blossom algorithm to hypergraphs via a similar primal-dual linear programming
model with certifiable proximity bounds. HyperBlossom unifies all the existing
graph-based decoders like (Hypergraph) Union-Find decoders and Minimum-Weight
Perfect Matching (MWPM) decoder, thus bridging the gap between heuristic and
certifying decoders.
  We implement HyperBlossom in software, namely Hyperion. Hyperion achieves a
4.8x lower logical error rate compared to the MWPM decoder on the distance-11
surface code and 1.6x lower logical error rate compared to a fine-tuned BPOSD
decoder on the $[[90, 8, 10]]$ bivariate bicycle code under code-capacity
noise. It also achieves an almost-linear average runtime scaling on both the
surface code and the color code, with numerical results up to sufficiently
large code distances of 99 and 31 for code-capacity noise and circuit-level
noise, respectively.

</details>


### [266] [Reinforcement Learning Generation of 4-Qubits Entangled States](https://arxiv.org/abs/2204.12351)
*Sara Giordano,Miguel A. Martin-Delgado*

Main category: quant-ph

TL;DR: 利用Q学习算法构建4量子比特纠缠态，可覆盖部分SLOCC类，引入SLG工具，找到的量子电路最优，使算法简单直观。


<details>
  <summary>Details</summary>
Motivation: 构建4量子比特显著纠缠态，助力其实验实现并探索宇宙内在性质。

Method: 采用Q学习进行机器学习，引入状态链接图（SLG）工具。

Result: 算法能生成49个真实SLOCC类中的部分代表态，每个纠缠族至少可达一个真实SLOCC类，找到的量子电路在所选量子门集下最优。

Conclusion: 该算法结合SLG工具对低量子比特纠缠态自动构建是简单、直观且有用的资源。

Abstract: We have devised an artificial intelligence algorithm with machine
reinforcement learning (Q-learning) to construct remarkable entangled states
with 4 qubits. This way, the algorithm is able to generate representative
states for some of the 49 true SLOCC classes of the four-qubit entanglement
states. In particular, it is possible to reach at least one true SLOCC class
for each of the nine entanglement families. The quantum circuits synthesized by
the algorithm may be useful for the experimental realization of these important
classes of entangled states and to draw conclusions about the intrinsic
properties of our universe. We introduce a graphical tool called the state-link
graph (SLG) to represent the construction of the Quality matrix (Q-matrix) used
by the algorithm to build a given objective state belonging to the
corresponding entanglement class. This allows us to discover the necessary
connections between specific entanglement features and the role of certain
quantum gates that the algorithm needs to include in the quantum gate set of
actions. The quantum circuits found are optimal by construction with respect to
the quantum gate-set chosen. These SLGs make the algorithm simple, intuitive
and a useful resource for the automated construction of entangled states with a
low number of qubits.

</details>


### [267] [Hybrid Reward-Driven Reinforcement Learning for Efficient Quantum Circuit Synthesis](https://arxiv.org/abs/2507.16641)
*Sara Giordano,Kornikar Sen,Miguel A. Martin-Delgado*

Main category: quant-ph

TL;DR: 本文引入强化学习框架来高效合成量子电路，利用Q学习和混合奖励机制，在多qubit任务中表现良好，能生成近最优电路。


<details>
  <summary>Details</summary>
Motivation: 解决NISQ时代和未来容错量子计算中从固定初始态生成指定目标量子态的量子电路合成挑战。

Method: 采用基于动作序列的表格Q学习，在离散量子态空间中，结合混合奖励机制，利用稀疏矩阵表示和状态空间离散化。

Result: 在最多7个qubit的图态制备任务中能发现最小深度且门数优化的电路，扩展到通用门集也能生成最小深度电路。

Conclusion: 该强化学习驱动方法能有效探索复杂量子态空间，合成近最优量子电路，为量子电路优化提供资源高效基础。

Abstract: A reinforcement learning (RL) framework is introduced for the efficient
synthesis of quantum circuits that generate specified target quantum states
from a fixed initial state, addressing a central challenge in both the NISQ era
and future fault-tolerant quantum computing. The approach utilizes tabular
Q-learning, based on action sequences, within a discretized quantum state
space, to effectively manage the exponential growth of the space dimension. The
framework introduces a hybrid reward mechanism, combining a static,
domain-informed reward that guides the agent toward the target state with
customizable dynamic penalties that discourage inefficient circuit structures
such as gate congestion and redundant state revisits. By leveraging sparse
matrix representations and state-space discretization, the method enables
scalable navigation of high-dimensional environments while minimizing
computational overhead. Benchmarking on graph-state preparation tasks for up to
seven qubits, we demonstrate that the algorithm consistently discovers
minimal-depth circuits with optimized gate counts. Moreover, extending the
framework to a universal gate set for arbitrary quantum states, it still
produces minimal depth circuits, highlighting the algorithm's robustness and
adaptability. The results confirm that this RL-driven approach efficiently
explores the complex quantum state space and synthesizes near-optimal quantum
circuits, providing a resource-efficient foundation for quantum circuit
optimization.

</details>


### [268] [Q-DPTS: Quantum Differentially Private Time Series Forecasting via Variational Quantum Circuits](https://arxiv.org/abs/2508.05036)
*Chi-Sheng Chen,Samuel Yen-Chi Chen*

Main category: quant-ph

TL;DR: 提出Q - DPTS混合量子经典框架用于量子差分隐私时间序列预测，在ETT数据集评估，结果显示其在相同隐私预算下预测误差更低，实现较好隐私 - 效用权衡。


<details>
  <summary>Details</summary>
Motivation: 时间序列预测在对数据敏感的领域很重要，传统差分隐私集成（如DP - SGD）会因注入噪声损害模型性能，需新方法。

Method: 提出Q - DPTS框架，结合变分量子电路、逐样本梯度裁剪和高斯噪声注入，确保严格的(ε,δ) - 差分隐私。

Result: 在ETT数据集上与经典和量子基线模型对比，Q - DPTS在相同隐私预算下预测误差更低。

Conclusion: 这是量子增强差分隐私预测的早期探索，为隐私敏感场景下的时间序列建模提供了有前景的方向。

Abstract: Time series forecasting is vital in domains where data sensitivity is
paramount, such as finance and energy systems. While Differential Privacy (DP)
provides theoretical guarantees to protect individual data contributions, its
integration especially via DP-SGD often impairs model performance due to
injected noise. In this paper, we propose Q-DPTS, a hybrid quantum-classical
framework for Quantum Differentially Private Time Series Forecasting. Q-DPTS
combines Variational Quantum Circuits (VQCs) with per-sample gradient clipping
and Gaussian noise injection, ensuring rigorous $(\epsilon,
\delta)$-differential privacy. The expressiveness of quantum models enables
improved robustness against the utility loss induced by DP mechanisms. We
evaluate Q-DPTS on the ETT (Electricity Transformer Temperature) dataset, a
standard benchmark for long-term time series forecasting. Our approach is
compared against both classical and quantum baselines, including LSTM, QASA,
QRWKV, and QLSTM. Results demonstrate that Q-DPTS consistently achieves lower
prediction error under the same privacy budget, indicating a favorable
privacy-utility trade-off. This work presents one of the first explorations
into quantum-enhanced differentially private forecasting, offering promising
directions for secure and accurate time series modeling in privacy-critical
scenarios.

</details>


### [269] [Hybrid quantum tensor networks for aeroelastic applications](https://arxiv.org/abs/2508.05169)
*M. Lautaro Hickmann,Pedro Alves,David Quero,Friedhelm Schwenker,Hans-Martin Rieser*

Main category: quant-ph

TL;DR: 本文研究混合量子张量网络结合量子机器学习（QML）在气动弹性问题中的应用，提出端到端可训练混合算法，取得高分类准确率和有前景的回归性能。


<details>
  <summary>Details</summary>
Motivation: 利用量子机器学习的能力解决气动弹性领域中复杂的时间序列分类和回归任务。

Method: 结合张量网络与变分量子电路，提出端到端可训练混合算法，先将时间序列编码到张量网络进行降维，再转换为量子电路，最后用受张量网络启发的可训练变分量子电路解决分类或回归任务。

Result: 混合量子张量网络在二元分类中实现了高准确率，在离散变量回归中有良好表现。

Conclusion: 虽然超参数选择有挑战，但该工作对QML解决气动弹性复杂问题的发展有重要贡献。

Abstract: We investigate the application of hybrid quantum tensor networks to
aeroelastic problems, harnessing the power of Quantum Machine Learning (QML).
By combining tensor networks with variational quantum circuits, we demonstrate
the potential of QML to tackle complex time series classification and
regression tasks. Our results showcase the ability of hybrid quantum tensor
networks to achieve high accuracy in binary classification. Furthermore, we
observe promising performance in regressing discrete variables. While
hyperparameter selection remains a challenge, requiring careful optimisation to
unlock the full potential of these models, this work contributes significantly
to the development of QML for solving intricate problems in aeroelasticity. We
present an end-to-end trainable hybrid algorithm. We first encode time series
into tensor networks to then utilise trainable tensor networks for
dimensionality reduction, and convert the resulting tensor to a quantum circuit
in the encoding step. Then, a tensor network inspired trainable variational
quantum circuit is applied to solve either a classification or a multivariate
or univariate regression task in the aeroelasticity domain.

</details>


### [270] [LLM-based Multi-Agent Copilot for Quantum Sensor](https://arxiv.org/abs/2508.05421)
*Rong Sha,Binglin Wang,Jun Yang,Xiaoxiao Ma,Chengkun Wu,Liang Yan,Chao Zhou,Jixun Liu,Guochao Wang,Shuhua Yan,Lingxiao Zhu*

Main category: quant-ph

TL;DR: 本文提出基于大语言模型的多智能体框架QCopilot用于量子传感器设计与诊断，应用于原子冷却实验效果良好，可降低大规模量子传感器部署障碍。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在量子传感器开发中受跨学科知识壁垒和复杂优化过程限制，需更好解决方案。

Method: 提出基于大语言模型的多智能体框架QCopilot，结合外部知识访问、主动学习和不确定性量化，用专业智能体自适应选择优化方法、自动建模分析和问题诊断。

Result: 应用于原子冷却实验，几小时内无人干预生成10⁸个亚微开尔文原子，比手动实验快约100倍，能自主识别多参数实验设置中的异常参数。

Conclusion: 该工作降低大规模量子传感器部署障碍，可扩展到其他量子信息系统。

Abstract: Large language models (LLM) exhibit broad utility but face limitations in
quantum sensor development, stemming from interdisciplinary knowledge barriers
and involving complex optimization processes. Here we present QCopilot, an
LLM-based multi-agent framework integrating external knowledge access, active
learning, and uncertainty quantification for quantum sensor design and
diagnosis. Comprising commercial LLMs with few-shot prompt engineering and
vector knowledge base, QCopilot employs specialized agents to adaptively select
optimization methods, automate modeling analysis, and independently perform
problem diagnosis. Applying QCopilot to atom cooling experiments, we generated
10${}^{\rm{8}}$ sub-$\rm{\mu}$K atoms without any human intervention within a
few hours, representing $\sim$100$\times$ speedup over manual experimentation.
Notably, by continuously accumulating prior knowledge and enabling dynamic
modeling, QCopilot can autonomously identify anomalous parameters in
multi-parameter experimental settings. Our work reduces barriers to large-scale
quantum sensor deployment and readily extends to other quantum information
systems.

</details>


### [271] [On the Design of Expressive and Trainable Pulse-based Quantum Machine Learning Models](https://arxiv.org/abs/2508.05559)
*Han-Xiao Tao,Xin Wang,Re-Bing Wu*

Main category: quant-ph

TL;DR: 研究脉冲量子机器学习模型兼具表达性和可训练性的要求，并提出必要条件，建立设计框架。


<details>
  <summary>Details</summary>
Motivation: 脉冲量子机器学习有硬件效率优势，但此前模型设计不当可能因不可控性影响表达性，需研究模型兼具表达性和可训练性的要求。

Method: 研究系统初始状态、测量可观量和动力学对称李代数的必要条件，并进行数值模拟。

Result: 得到了关于系统初始状态、测量可观量和动力学对称李代数的必要条件。

Conclusion: 建立了设计平衡表达性和可训练性的实用脉冲量子机器学习模型的框架。

Abstract: Pulse-based Quantum Machine Learning (QML) has emerged as a novel paradigm in
quantum artificial intelligence due to its exceptional hardware efficiency. For
practical applications, pulse-based models must be both expressive and
trainable. Previous studies suggest that pulse-based models under dynamic
symmetry can be effectively trained, thanks to a favorable loss landscape that
has no barren plateaus. However, the resulting uncontrollability may compromise
expressivity when the model is inadequately designed. This paper investigates
the requirements for pulse-based QML models to be expressive while preserving
trainability. We present a necessary condition pertaining to the system's
initial state, the measurement observable, and the underlying dynamical
symmetry Lie algebra, supported by numerical simulations. Our findings
establish a framework for designing practical pulse-based QML models that
balance expressivity and trainability.

</details>


<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [272] [CodonMoE: DNA Language Models for mRNA Analyses](https://arxiv.org/abs/2508.04739)
*Shiyi Du,Litian Liang,Jiayi Li,Carl Kingsford*

Main category: q-bio.GN

TL;DR: 提出轻量级适配器CodonMoE，将DNA语言模型转化为有效的RNA分析器，在多个RNA预测任务中表现出色，减少计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有基因组语言模型在处理DNA和RNA模态时存在计算负担大的问题，需要解决效率挑战。

Method: 引入CodonMoE适配器，理论分析其为密码子级别的通用近似器。

Result: 在四个RNA预测任务中，使用CodonMoE增强的DNA模型显著优于未修改的模型，HyenaDNA+CodonMoE系列用更少参数达到了最优结果。

Conclusion: 该方法在保持较低复杂度的同时实现了卓越性能，为统一基因组语言建模提供了原则性途径。

Abstract: Genomic language models (gLMs) face a fundamental efficiency challenge:
either maintain separate specialized models for each biological modality (DNA
and RNA) or develop large multi-modal architectures. Both approaches impose
significant computational burdens - modality-specific models require redundant
infrastructure despite inherent biological connections, while multi-modal
architectures demand massive parameter counts and extensive cross-modality
pretraining. To address this limitation, we introduce CodonMoE (Adaptive
Mixture of Codon Reformative Experts), a lightweight adapter that transforms
DNA language models into effective RNA analyzers without RNA-specific
pretraining. Our theoretical analysis establishes CodonMoE as a universal
approximator at the codon level, capable of mapping arbitrary functions from
codon sequences to RNA properties given sufficient expert capacity. Across four
RNA prediction tasks spanning stability, expression, and regulation, DNA models
augmented with CodonMoE significantly outperform their unmodified counterparts,
with HyenaDNA+CodonMoE series achieving state-of-the-art results using 80%
fewer parameters than specialized RNA models. By maintaining sub-quadratic
complexity while achieving superior performance, our approach provides a
principled path toward unifying genomic language modeling, leveraging more
abundant DNA data and reducing computational overhead while preserving
modality-specific performance advantages.

</details>


### [273] [Discovery of Disease Relationships via Transcriptomic Signature Analysis Powered by Agentic AI](https://arxiv.org/abs/2508.04742)
*Ke Chen,Haohan Wang*

Main category: q-bio.GN

TL;DR: 研究引入转录组驱动框架发现疾病关系，揭示多种疾病联系，探索机制并指出治疗再利用机会，展示AI在转录组分析的作用。


<details>
  <summary>Details</summary>
Motivation: 现代疾病分类常忽略临床表象下的分子共性，需新方法发现疾病关系。

Method: 使用GenoMAS分析超1300个疾病 - 病症对，开发基于通路的相似性框架进行多数据库富集分析。

Result: 构建疾病相似性网络，揭示已知和未知疾病联系，探索潜在分子机制，发现背景条件对转录组相似性的影响，确定罕见病治疗再利用机会。

Conclusion: 基于生物学的智能AI可扩展转录组分析，实现复杂疾病景观的机制解释，结果公开。

Abstract: Modern disease classification often overlooks molecular commonalities hidden
beneath divergent clinical presentations. This study introduces a
transcriptomics-driven framework for discovering disease relationships by
analyzing over 1300 disease-condition pairs using GenoMAS, a fully automated
agentic AI system. Beyond identifying robust gene-level overlaps, we develop a
novel pathway-based similarity framework that integrates multi-database
enrichment analysis to quantify functional convergence across diseases. The
resulting disease similarity network reveals both known comorbidities and
previously undocumented cross-category links. By examining shared biological
pathways, we explore potential molecular mechanisms underlying these
connections-offering functional hypotheses that go beyond symptom-based
taxonomies. We further show how background conditions such as obesity and
hypertension modulate transcriptomic similarity, and identify therapeutic
repurposing opportunities for rare diseases like autism spectrum disorder based
on their molecular proximity to better-characterized conditions. In addition,
this work demonstrates how biologically grounded agentic AI can scale
transcriptomic analysis while enabling mechanistic interpretation across
complex disease landscapes. All results are publicly accessible at
github.com/KeeeeChen/Pathway_Similarity_Network.

</details>


### [274] [GRIT: Graph-Regularized Logit Refinement for Zero-shot Cell Type Annotation](https://arxiv.org/abs/2508.04747)
*Tianxiang Hu,Chenyi Zhou,Jiaxiang Liu,Jiongxin Wang,Ruizhe Chen,Haoxiang Xia,Gaoang Wang,Jian Wu,Zuozhu Liu*

Main category: q-bio.GN

TL;DR: 本文提出通过图正则优化框架改进LangCell的零样本预测结果，在多个数据集上验证可提升零样本注释准确率，方法免训练、模型无关。


<details>
  <summary>Details</summary>
Motivation: 现有细胞类型注释方法劳动密集且难处理大数据集，LangCell零样本注释性能欠佳。

Method: 提出图正则优化框架，在基于PCA的k - NN图上强制局部一致性，改进LangCell零样本预测结果。

Result: 在14个人类scRNA - seq数据集上验证，方法能持续提升零样本注释准确率，最高提升10%，并展示了传播正确信号的机制。

Conclusion: 该方法免训练、模型无关，可作为有效插件提升自动细胞类型注释效果。

Abstract: Cell type annotation is a fundamental step in the analysis of single-cell RNA
sequencing (scRNA-seq) data. In practice, human experts often rely on the
structure revealed by principal component analysis (PCA) followed by
$k$-nearest neighbor ($k$-NN) graph construction to guide annotation. While
effective, this process is labor-intensive and does not scale to large
datasets. Recent advances in CLIP-style models offer a promising path toward
automating cell type annotation. By aligning scRNA-seq profiles with natural
language descriptions, models like LangCell enable zero-shot annotation. While
LangCell demonstrates decent zero-shot performance, its predictions remain
suboptimal, particularly in achieving consistent accuracy across all cell
types. In this paper, we propose to refine the zero-shot logits produced by
LangCell through a graph-regularized optimization framework. By enforcing local
consistency over the task-specific PCA-based k-NN graph, our method combines
the scalability of the pre-trained models with the structural robustness relied
upon in expert annotation. We evaluate our approach on 14 annotated human
scRNA-seq datasets from 4 distinct studies, spanning 11 organs and over 200,000
single cells. Our method consistently improves zero-shot annotation accuracy,
achieving accuracy gains of up to 10%. Further analysis showcase the mechanism
by which GRIT effectively propagates correct signals through the graph, pulling
back mislabeled cells toward more accurate predictions. The method is
training-free, model-agnostic, and serves as a simple yet effective plug-in for
enhancing automated cell type annotation in practice.

</details>


### [275] [Embedding Is (Almost) All You Need: Retrieval-Augmented Inference for Generalizable Genomic Prediction Tasks](https://arxiv.org/abs/2508.04757)
*Nirjhor Datta,Swakkhar Shatabda,M Sohel Rahman*

Main category: q-bio.GN

TL;DR: 研究表明，基于嵌入的方法可替代预训练DNA语言模型的微调，性能有竞争力且更高效环保。


<details>
  <summary>Details</summary>
Motivation: 探讨预训练DNA语言模型是否总需要特定任务的微调。

Method: 使用简单的基于嵌入的管道，从模型中提取固定表示并输入轻量级分类器。

Result: 在不同数据分布评估中，基于嵌入的方法常优于微调，减少推理时间10 - 20倍，碳效率提高超10倍。

Conclusion: 嵌入提取是微调的更通用、高效替代方案，尤其适用于不同或未知的基因组环境。

Abstract: Large pre-trained DNA language models such as DNABERT-2, Nucleotide
Transformer, and HyenaDNA have demonstrated strong performance on various
genomic benchmarks. However, most applications rely on expensive fine-tuning,
which works best when the training and test data share a similar distribution.
In this work, we investigate whether task-specific fine-tuning is always
necessary. We show that simple embedding-based pipelines that extract fixed
representations from these models and feed them into lightweight classifiers
can achieve competitive performance. In evaluation settings with different data
distributions, embedding-based methods often outperform fine-tuning while
reducing inference time by 10x to 20x. Our results suggest that embedding
extraction is not only a strong baseline but also a more generalizable and
efficient alternative to fine-tuning, especially for deployment in diverse or
unseen genomic contexts. For example, in enhancer classification, HyenaDNA
embeddings combined with zCurve achieve 0.68 accuracy (vs. 0.58 for
fine-tuning), with an 88% reduction in inference time and over 8x lower carbon
emissions (0.02 kg vs. 0.17 kg CO2). In non-TATA promoter classification,
DNABERT-2 embeddings with zCurve or GC content reach 0.85 accuracy (vs. 0.89
with fine-tuning) with a 22x lower carbon footprint (0.02 kg vs. 0.44 kg CO2).
These results show that embedding-based pipelines offer over 10x better carbon
efficiency while maintaining strong predictive performance. The code is
available here:
https://github.com/NIRJHOR-DATTA/EMBEDDING-IS-ALMOST-ALL-YOU-NEED.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [276] [INTENTION: Inferring Tendencies of Humanoid Robot Motion Through Interactive Intuition and Grounded VLM](https://arxiv.org/abs/2508.04931)
*Jin Wang,Weijie Wang,Boyuan Deng,Heng Zhang,Rui Dai,Nikos Tsagarakis*

Main category: cs.RO

TL;DR: 提出INTENTION框架，使机器人具备交互直觉和自主操作能力，结合VLM场景推理与交互驱动记忆。


<details>
  <summary>Details</summary>
Motivation: 传统机器人操作控制和规划依赖精确物理模型和预定义动作序列，在现实场景中因建模不准确难以泛化，而人类交互具有适应性，因此要让机器人具备类似能力。

Method: 提出INTENTION框架，引入Memory Graph记录先前任务交互场景，设计Intuitive Perceptor提取视觉场景物理关系和功能。

Result: 使机器人能在新场景中不依赖重复指令推断合适交互行为。

Conclusion: INTENTION框架可让机器人在不同场景中获得学习到的交互直觉和自主操作能力。

Abstract: Traditional control and planning for robotic manipulation heavily rely on
precise physical models and predefined action sequences. While effective in
structured environments, such approaches often fail in real-world scenarios due
to modeling inaccuracies and struggle to generalize to novel tasks. In
contrast, humans intuitively interact with their surroundings, demonstrating
remarkable adaptability, making efficient decisions through implicit physical
understanding. In this work, we propose INTENTION, a novel framework enabling
robots with learned interactive intuition and autonomous manipulation in
diverse scenarios, by integrating Vision-Language Models (VLMs) based scene
reasoning with interaction-driven memory. We introduce Memory Graph to record
scenes from previous task interactions which embodies human-like understanding
and decision-making about different tasks in real world. Meanwhile, we design
an Intuitive Perceptor that extracts physical relations and affordances from
visual scenes. Together, these components empower robots to infer appropriate
interaction behaviors in new scenes without relying on repetitive instructions.
Videos: https://robo-intention.github.io

</details>


### [277] [Hierarchical Deep Deterministic Policy Gradient for Autonomous Maze Navigation of Mobile Robots](https://arxiv.org/abs/2508.04994)
*Wenjie Hu,Ye Zhou,Hann Woei Ho*

Main category: cs.RO

TL;DR: 本文针对传统DDPG算法在迷宫导航中存在的问题，提出Hierarchical DDPG (HDDPG) 算法，试验显示其能显著提升导航成功率和平均奖励。


<details>
  <summary>Details</summary>
Motivation: 传统DDPG算法在迷宫导航中存在稀疏奖励、探索效率低和长程规划困难等问题，导致成功率低和奖励少。

Method: 提出包含高低层策略的HDDPG算法，高层生成子目标，低层生成原始动作，还采用离策略校正、自适应参数空间噪声、重塑奖励函数等优化方法。

Result: 通过ROS和Gazebo进行数值模拟实验，HDDPG在迷宫导航中比基线算法至少提高成功率56.59%，平均奖励至少提升519.03。

Conclusion: HDDPG显著克服了标准DDPG及其变体的局限性，在迷宫导航任务中有更好表现。

Abstract: Maze navigation is a fundamental challenge in robotics, requiring agents to
traverse complex environments efficiently. While the Deep Deterministic Policy
Gradient (DDPG) algorithm excels in control tasks, its performance in maze
navigation suffers from sparse rewards, inefficient exploration, and
long-horizon planning difficulties, often leading to low success rates and
average rewards, sometimes even failing to achieve effective navigation. To
address these limitations, this paper proposes an efficient Hierarchical DDPG
(HDDPG) algorithm, which includes high-level and low-level policies. The
high-level policy employs an advanced DDPG framework to generate intermediate
subgoals from a long-term perspective and on a higher temporal scale. The
low-level policy, also powered by the improved DDPG algorithm, generates
primitive actions by observing current states and following the subgoal
assigned by the high-level policy. The proposed method enhances stability with
off-policy correction, refining subgoal assignments by relabeling historical
experiences. Additionally, adaptive parameter space noise is utilized to
improve exploration, and a reshaped intrinsic-extrinsic reward function is
employed to boost learning efficiency. Further optimizations, including
gradient clipping and Xavier initialization, are employed to improve
robustness. The proposed algorithm is rigorously evaluated through numerical
simulation experiments executed using the Robot Operating System (ROS) and
Gazebo. Regarding the three distinct final targets in autonomous maze
navigation tasks, HDDPG significantly overcomes the limitations of standard
DDPG and its variants, improving the success rate by at least 56.59% and
boosting the average reward by a minimum of 519.03 compared to baseline
algorithms.

</details>


### [278] [Chemist Eye: A Visual Language Model-Powered System for Safety Monitoring and Robot Decision-Making in Self-Driving Laboratories](https://arxiv.org/abs/2508.05148)
*Francisco Munguia-Galeano,Zhengxue Zhou,Satheeshkumar Veeramani,Hatem Fakhruldeen,Louis Longley,Rob Clowes,Andrew I. Cooper*

Main category: cs.RO

TL;DR: 本文介绍了用于提升自动驾驶实验室态势感知的安全监测系统Chemist Eye，它基于视觉语言模型，测试中发现安全隐患和决策性能表现良好。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶实验室结合机器人和自动化会带来额外安全复杂性，需保障人员安全并应对火灾等风险。

Method: 构建集成RGB、深度和红外相机的分布式安全监测系统Chemist Eye，用视觉语言模型驱动决策，与机器人实时通信，集成第三方消息平台。

Result: 在配备三个移动机器人的自动驾驶实验室中测试，发现可能安全隐患和决策性能分别达97%和95%。

Conclusion: Chemist Eye系统在自动驾驶实验室安全监测方面有良好表现，能有效提升态势感知。

Abstract: The integration of robotics and automation into self-driving laboratories
(SDLs) can introduce additional safety complexities, in addition to those that
already apply to conventional research laboratories. Personal protective
equipment (PPE) is an essential requirement for ensuring the safety and
well-being of workers in laboratories, self-driving or otherwise. Fires are
another important risk factor in chemical laboratories. In SDLs, fires that
occur close to mobile robots, which use flammable lithium batteries, could have
increased severity. Here, we present Chemist Eye, a distributed safety
monitoring system designed to enhance situational awareness in SDLs. The system
integrates multiple stations equipped with RGB, depth, and infrared cameras,
designed to monitor incidents in SDLs. Chemist Eye is also designed to spot
workers who have suffered a potential accident or medical emergency, PPE
compliance and fire hazards. To do this, Chemist Eye uses decision-making
driven by a vision-language model (VLM). Chemist Eye is designed for seamless
integration, enabling real-time communication with robots. Based on the VLM
recommendations, the system attempts to drive mobile robots away from potential
fire locations, exits, or individuals not wearing PPE, and issues audible
warnings where necessary. It also integrates with third-party messaging
platforms to provide instant notifications to lab personnel. We tested Chemist
Eye with real-world data from an SDL equipped with three mobile robots and
found that the spotting of possible safety hazards and decision-making
performances reached 97 % and 95 %, respectively.

</details>


### [279] [FCBV-Net: Category-Level Robotic Garment Smoothing via Feature-Conditioned Bimanual Value Prediction](https://arxiv.org/abs/2508.05153)
*Mohammed Daba,Jing Qiu*

Main category: cs.RO

TL;DR: 提出FCBV - Net提升服装抚平任务的类别级策略泛化能力，在模拟实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前机器人服装操作在类别级泛化上存在困难，现有方法存在过拟合或无法预测协同双手动作价值的问题。

Method: 提出FCBV - Net，基于3D点云，利用预训练的冻结密集几何特征进行双手动作价值预测，可训练的下游组件学习特定任务策略。

Result: 在模拟实验中，FCBV - Net在未见过的服装上效率下降仅11.5%，而2D图像基线为96.2%；最终覆盖率达89%，优于3D对应基线的83%。

Conclusion: 将几何理解与双手动作价值学习解耦能实现更好的类别级泛化。

Abstract: Category-level generalization for robotic garment manipulation, such as
bimanual smoothing, remains a significant hurdle due to high dimensionality,
complex dynamics, and intra-category variations. Current approaches often
struggle, either overfitting with concurrently learned visual features for a
specific instance or, despite category-level perceptual generalization, failing
to predict the value of synergistic bimanual actions. We propose the
Feature-Conditioned Bimanual Value Network (FCBV-Net), operating on 3D point
clouds to specifically enhance category-level policy generalization for garment
smoothing. FCBV-Net conditions bimanual action value prediction on pre-trained,
frozen dense geometric features, ensuring robustness to intra-category garment
variations. Trainable downstream components then learn a task-specific policy
using these static features. In simulated GarmentLab experiments with the
CLOTH3D dataset, FCBV-Net demonstrated superior category-level generalization.
It exhibited only an 11.5% efficiency drop (Steps80) on unseen garments
compared to 96.2% for a 2D image-based baseline, and achieved 89% final
coverage, outperforming an 83% coverage from a 3D correspondence-based baseline
that uses identical per-point geometric features but a fixed primitive. These
results highlight that the decoupling of geometric understanding from bimanual
action value learning enables better category-level generalization.

</details>


### [280] [Towards Embodied Agentic AI: Review and Classification of LLM- and VLM-Driven Robot Autonomy and Interaction](https://arxiv.org/abs/2508.05294)
*Sahar Salimpour,Lei Fu,Farhad Keramat,Leonardo Militano,Giovanni Toffetti,Harry Edelman,Jorge Peña Queralta*

Main category: cs.RO

TL;DR: 本文是关于基础模型推动机器人自主性和人机接口发展的综述，探讨了迈向智能体应用和架构的进展，提出分类方法并进行对比分析。


<details>
  <summary>Details</summary>
Motivation: 基础模型为机器人自主性和人机接口带来新方法，智能体应用和架构不断发展，需对相关进展进行总结梳理。

Method: 除了研究同行评审的研究成果，还关注社区驱动项目、ROS包和工业框架，提出分类模型集成方法的分类法。

Result: 梳理了迈向智能体应用和架构的进展，包括GPT式接口和复杂系统等，展示了新兴趋势。

Conclusion: 通过提出分类法和对比分析，有助于理解智能体在不同解决方案中的作用。

Abstract: Foundation models, including large language models (LLMs) and vision-language
models (VLMs), have recently enabled novel approaches to robot autonomy and
human-robot interfaces. In parallel, vision-language-action models (VLAs) or
large behavior models (BLMs) are increasing the dexterity and capabilities of
robotic systems. This survey paper focuses on those words advancing towards
agentic applications and architectures. This includes initial efforts exploring
GPT-style interfaces to tooling, as well as more complex system where AI agents
are coordinators, planners, perception actors, or generalist interfaces. Such
agentic architectures allow robots to reason over natural language
instructions, invoke APIs, plan task sequences, or assist in operations and
diagnostics. In addition to peer-reviewed research, due to the fast-evolving
nature of the field, we highlight and include community-driven projects, ROS
packages, and industrial frameworks that show emerging trends. We propose a
taxonomy for classifying model integration approaches and present a comparative
analysis of the role that agents play in different solutions in today's
literature.

</details>


### [281] [Information-Theoretic Graph Fusion with Vision-Language-Action Model for Policy Reasoning and Dual Robotic Control](https://arxiv.org/abs/2508.05342)
*Shunlei Li,Longsen Gao,Jin Wang,Chang Che,Xi Xiao,Jiuwen Cao,Yingbai Hu,Hamid Reza Karimi*

Main category: cs.RO

TL;DR: 提出GF - VLA框架让双臂机器人系统从人类演示视频执行任务，在多任务实验中表现出良好泛化性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决从人类视频教机器人灵巧技能时，依赖低级轨迹模仿难以跨物体类型、空间布局和操作器配置泛化的问题。

Method: 先提取基于香农信息的线索识别任务相关的手和物体，编码成场景图，与语言条件变压器融合生成行为树和运动命令，引入跨手选择策略。

Result: 信息论场景表示图准确率超95%，子任务分割率93%；机器人抓握成功率94%，放置准确率89%，整体任务成功率90%。

Conclusion: GF - VLA框架具有强泛化性和鲁棒性，能应对不同空间和语义变化。

Abstract: Teaching robots dexterous skills from human videos remains challenging due to
the reliance on low-level trajectory imitation, which fails to generalize
across object types, spatial layouts, and manipulator configurations. We
propose Graph-Fused Vision-Language-Action (GF-VLA), a framework that enables
dual-arm robotic systems to perform task-level reasoning and execution directly
from RGB and Depth human demonstrations. GF-VLA first extracts
Shannon-information-based cues to identify hands and objects with the highest
task relevance, then encodes these cues into temporally ordered scene graphs
that capture both hand-object and object-object interactions. These graphs are
fused with a language-conditioned transformer that generates hierarchical
behavior trees and interpretable Cartesian motion commands. To improve
execution efficiency in bimanual settings, we further introduce a cross-hand
selection policy that infers optimal gripper assignment without explicit
geometric reasoning. We evaluate GF-VLA on four structured dual-arm block
assembly tasks involving symbolic shape construction and spatial
generalization. Experimental results show that the information-theoretic scene
representation achieves over 95 percent graph accuracy and 93 percent subtask
segmentation, supporting the LLM planner in generating reliable and
human-readable task policies. When executed by the dual-arm robot, these
policies yield 94 percent grasp success, 89 percent placement accuracy, and 90
percent overall task success across stacking, letter-building, and geometric
reconfiguration scenarios, demonstrating strong generalization and robustness
across diverse spatial and semantic variations.

</details>


### [282] [Real-Time Iteration Scheme for Diffusion Policy](https://arxiv.org/abs/2508.05396)
*Yufei Duan,Hang Yin,Danica Kragic*

Main category: cs.RO

TL;DR: 本文提出受实时迭代方案启发的方法，减少扩散策略推理时间，可集成到预训练模型，实验显示推理时间大幅减少且性能相当。


<details>
  <summary>Details</summary>
Motivation: 现有扩散策略推理时间长、需执行动作块，适用场景受限，近期加速方法需额外训练，资源消耗大。

Method: 引入受实时迭代方案启发的方法用于扩散推理，提出基于缩放的方法处理离散动作。

Result: 大量模拟实验定量结果显示推理时间大幅减少，整体性能与全步去噪的扩散策略相当。

Conclusion: 所提方案无需蒸馏或策略重新设计，降低运行时计算成本，能无缝集成到许多预训练模型。

Abstract: Diffusion Policies have demonstrated impressive performance in robotic
manipulation tasks. However, their long inference time, resulting from an
extensive iterative denoising process, and the need to execute an action chunk
before the next prediction to maintain consistent actions limit their
applicability to latency-critical tasks or simple tasks with a short cycle
time. While recent methods explored distillation or alternative policy
structures to accelerate inference, these often demand additional training,
which can be resource-intensive for large robotic models. In this paper, we
introduce a novel approach inspired by the Real-Time Iteration (RTI) Scheme, a
method from optimal control that accelerates optimization by leveraging
solutions from previous time steps as initial guesses for subsequent
iterations. We explore the application of this scheme in diffusion inference
and propose a scaling-based method to effectively handle discrete actions, such
as grasping, in robotic manipulation. The proposed scheme significantly reduces
runtime computational costs without the need for distillation or policy
redesign. This enables a seamless integration into many pre-trained
diffusion-based models, in particular, to resource-demanding large models. We
also provide theoretical conditions for the contractivity which could be useful
for estimating the initial denoising step. Quantitative results from extensive
simulation experiments show a substantial reduction in inference time, with
comparable overall performance compared with Diffusion Policy using full-step
denoising. Our project page with additional resources is available at:
https://rti-dp.github.io/.

</details>


### [283] [Mixed-Initiative Dialog for Human-Robot Collaborative Manipulation](https://arxiv.org/abs/2508.05535)
*Albert Yu,Chengshu Li,Luca Macesanu,Arnav Balaji,Ruchira Ray,Raymond Mooney,Roberto Martín-Martín*

Main category: cs.RO

TL;DR: 提出MICoBot系统用于人机协作，通过三层决策处理多样化任务导向对话，评估显示比基线和其他模型有更好的任务成功率和用户体验。


<details>
  <summary>Details</summary>
Motivation: 长时程人机协作系统需适应不同人类伙伴，需要紧密耦合的通信循环，以有效完成任务。

Method: 应用混合主动对话范式，提出MICoBot系统，通过元规划器、规划器和动作执行器三个层级进行决策。

Result: 在模拟和现实世界中对18名人类参与者进行27小时实验，该方法能与不同人类用户有效协作，比纯大语言模型基线和其他代理分配模型有更好的任务成功率和用户体验。

Conclusion: MICoBot系统在人机协作中表现良好，能有效适应不同人类伙伴，完成任务并提升用户体验。

Abstract: Effective robotic systems for long-horizon human-robot collaboration must
adapt to a wide range of human partners, whose physical behavior, willingness
to assist, and understanding of the robot's capabilities may change over time.
This demands a tightly coupled communication loop that grants both agents the
flexibility to propose, accept, or decline requests as they coordinate toward
completing the task effectively. We apply a Mixed-Initiative dialog paradigm to
Collaborative human-roBot teaming and propose MICoBot, a system that handles
the common scenario where both agents, using natural language, take initiative
in formulating, accepting, or rejecting proposals on who can best complete
different steps of a task. To handle diverse, task-directed dialog, and find
successful collaborative strategies that minimize human effort, MICoBot makes
decisions at three levels: (1) a meta-planner considers human dialog to
formulate and code a high-level collaboration strategy, (2) a planner optimally
allocates the remaining steps to either agent based on the robot's capabilities
(measured by a simulation-pretrained affordance model) and the human's
estimated availability to help, and (3) an action executor decides the
low-level actions to perform or words to say to the human. Our extensive
evaluations in simulation and real-world -- on a physical robot with 18 unique
human participants over 27 hours -- demonstrate the ability of our method to
effectively collaborate with diverse human users, yielding significantly
improved task success and user experience than a pure LLM baseline and other
agent allocation models. See additional videos and materials at
https://robin-lab.cs.utexas.edu/MicoBot/.

</details>


### [284] [Towards Generalizable Safety in Crowd Navigation via Conformal Uncertainty Handling](https://arxiv.org/abs/2508.05634)
*Jianpeng Yao,Xiaopan Zhang,Yu Xia,Zejin Wang,Amit K. Roy-Chowdhury,Jiachen Li*

Main category: cs.RO

TL;DR: 提出通过考虑行人不确定性让机器人学习安全导航策略，用自适应共形推理估计不确定性并结合约束强化学习，实验显示在分布内和分布外场景表现良好，还在真实机器人上验证。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习训练的移动机器人在分布外场景性能下降问题。

Method: 用自适应共形推理生成预测不确定性估计来增强智能体观测，通过约束强化学习引导智能体行为。

Result: 分布内场景成功率96.93%，优于基线；分布外场景面对多种分布变化更稳健；在真实机器人上能安全交互。

Conclusion: 考虑行人不确定性并结合自适应共形推理和约束强化学习的方法能让机器人实现安全、稳健导航。

Abstract: Mobile robots navigating in crowds trained using reinforcement learning are
known to suffer performance degradation when faced with out-of-distribution
scenarios. We propose that by properly accounting for the uncertainties of
pedestrians, a robot can learn safe navigation policies that are robust to
distribution shifts. Our method augments agent observations with prediction
uncertainty estimates generated by adaptive conformal inference, and it uses
these estimates to guide the agent's behavior through constrained reinforcement
learning. The system helps regulate the agent's actions and enables it to adapt
to distribution shifts. In the in-distribution setting, our approach achieves a
96.93% success rate, which is over 8.80% higher than the previous
state-of-the-art baselines with over 3.72 times fewer collisions and 2.43 times
fewer intrusions into ground-truth human future trajectories. In three
out-of-distribution scenarios, our method shows much stronger robustness when
facing distribution shifts in velocity variations, policy changes, and
transitions from individual to group dynamics. We deploy our method on a real
robot, and experiments show that the robot makes safe and robust decisions when
interacting with both sparse and dense crowds. Our code and videos are
available on https://gen-safe-nav.github.io/.

</details>


<div id='q-bio.MN'></div>

# q-bio.MN [[Back]](#toc)

### [285] [Alz-QNet: A Quantum Regression Network for Studying Alzheimer's Gene Interactions](https://arxiv.org/abs/2508.04743)
*Debanjan Konar,Neerav Sreekumar,Richard Jiang,Vaneet Aggarwal*

Main category: q-bio.MN

TL;DR: 本文提出量子回归网络Alz - QNet，研究AD关键基因相互作用，揭示潜在调控机制以助力诊疗。


<details>
  <summary>Details</summary>
Motivation: AD是多因素疾病，理解其基因 - 基因相互作用对诊疗和进展研究至关重要，但目前研究仍具挑战。

Method: 提出量子回归网络Alz - QNet，结合量子基因调控网络（QGRN），利用数据库GSE138852的遗传样本，研究AD患者内嗅皮层（EC）微环境中关键基因相互作用。

Result: 揭示了复杂的基因 - 基因相互作用，阐明了AD发病机制的潜在调控机制。

Conclusion: 研究结果有助于找到潜在的基因抑制剂或调节剂用于AD的诊疗。

Abstract: Understanding the molecular-level mechanisms underpinning Alzheimer's disease
(AD) by studying crucial genes associated with the disease remains a challenge.
Alzheimer's, being a multifactorial disease, requires understanding the
gene-gene interactions underlying it for theranostics and progress. In this
article, a novel attempt has been made using a quantum regression to decode how
some crucial genes in the AD Amyloid Beta Precursor Protein ($APP$), Sterol
regulatory element binding transcription factor 14 ($FGF14$), Yin Yang 1
($YY1$), and Phospholipase D Family Member 3 ($PLD3$) etc. become influenced by
other prominent switching genes during disease progression, which may help in
gene expression-based therapy for AD. Our proposed Quantum Regression Network
(Alz-QNet) introduces a pioneering approach with insights from the
state-of-the-art Quantum Gene Regulatory Networks (QGRN) to unravel the gene
interactions involved in AD pathology, particularly within the Entorhinal
Cortex (EC), where early pathological changes occur. Using the proposed
Alz-QNet framework, we explore the interactions between key genes ($APP$,
$FGF14$, $YY1$, $EGR1$, $GAS7$, $AKT3$, $SREBF2$, and $PLD3$) within the CE
microenvironment of AD patients, studying genetic samples from the database
$GSE138852$, all of which are believed to play a crucial role in the
progression of AD. Our investigation uncovers intricate gene-gene interactions,
shedding light on the potential regulatory mechanisms that underlie the
pathogenesis of AD, which help us to find potential gene inhibitors or
regulators for theranostics.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [286] [RDDPM: Robust Denoising Diffusion Probabilistic Model for Unsupervised Anomaly Segmentation](https://arxiv.org/abs/2508.02903)
*Mehrdad Moradi,Kamran Paynabar*

Main category: cs.CV

TL;DR: 本文提出用于只有污染（包含正常和异常）未标记数据场景的鲁棒去噪扩散模型，实验表明该方法在无监督异常分割上优于现有扩散模型。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型训练通常需正常数据，限制其在现实场景的应用，因此要提出适用于只有污染未标记数据场景的模型。

Method: 将数据的最大似然估计转化为非线性回归问题，从回归角度重新解释去噪扩散概率模型，使用鲁棒回归推导鲁棒版本的去噪扩散概率模型。

Result: 在只有污染数据时，该方法在无监督异常分割上优于现有扩散模型，在MVTec数据集上AUROC提高8.08%，AUPRC提高10.37%。

Conclusion: 所提出的新框架在构建各种鲁棒扩散模型上有灵活性，且在特定场景的无监督异常分割任务中表现出色。

Abstract: Recent advancements in diffusion models have demonstrated significant success
in unsupervised anomaly segmentation. For anomaly segmentation, these models
are first trained on normal data; then, an anomalous image is noised to an
intermediate step, and the normal image is reconstructed through backward
diffusion. Unlike traditional statistical methods, diffusion models do not rely
on specific assumptions about the data or target anomalies, making them
versatile for use across different domains. However, diffusion models typically
assume access to normal data for training, limiting their applicability in
realistic settings. In this paper, we propose novel robust denoising diffusion
models for scenarios where only contaminated (i.e., a mix of normal and
anomalous) unlabeled data is available. By casting maximum likelihood
estimation of the data as a nonlinear regression problem, we reinterpret the
denoising diffusion probabilistic model through a regression lens. Using robust
regression, we derive a robust version of denoising diffusion probabilistic
models. Our novel framework offers flexibility in constructing various robust
diffusion models. Our experiments show that our approach outperforms current
state of the art diffusion models, for unsupervised anomaly segmentation when
only contaminated data is available. Our method outperforms existing
diffusion-based approaches, achieving up to 8.08\% higher AUROC and 10.37\%
higher AUPRC on MVTec datasets. The implementation code is available at:
https://github.com/mehrdadmoradi124/RDDPM

</details>


### [287] [Single-Step Reconstruction-Free Anomaly Detection and Segmentation via Diffusion Models](https://arxiv.org/abs/2508.04818)
*Mehrdad Moradi,Marco Grasso,Bianca Maria Colosimo,Kamran Paynabar*

Main category: cs.CV

TL;DR: 介绍基于注意力扩散模型的无重建实时异常检测方法RADAR，克服重建方法局限，评估显示其在关键指标上超越现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有基于重建的扩散模型异常检测方法存在计算成本高、重建不准确、中间噪声水平选择困难等问题，需新方法解决。

Method: 提出RADAR方法，直接从扩散模型生成异常图，而非重建输入图像。

Result: 在真实3D打印材料和MVTec - AD数据集上评估，在准确率、精度、召回率和F1分数等关键指标上超越现有模型，如MVTec - AD上F1分数提高7%，3D打印材料数据集上提高13%。

Conclusion: RADAR方法有效克服重建方法局限，提高检测准确性和计算效率。

Abstract: Generative models have demonstrated significant success in anomaly detection
and segmentation over the past decade. Recently, diffusion models have emerged
as a powerful alternative, outperforming previous approaches such as GANs and
VAEs. In typical diffusion-based anomaly detection, a model is trained on
normal data, and during inference, anomalous images are perturbed to a
predefined intermediate step in the forward diffusion process. The
corresponding normal image is then reconstructed through iterative reverse
sampling.
  However, reconstruction-based approaches present three major challenges: (1)
the reconstruction process is computationally expensive due to multiple
sampling steps, making real-time applications impractical; (2) for complex or
subtle patterns, the reconstructed image may correspond to a different normal
pattern rather than the original input; and (3) Choosing an appropriate
intermediate noise level is challenging because it is application-dependent and
often assumes prior knowledge of anomalies, an assumption that does not hold in
unsupervised settings.
  We introduce Reconstruction-free Anomaly Detection with Attention-based
diffusion models in Real-time (RADAR), which overcomes the limitations of
reconstruction-based anomaly detection. Unlike current SOTA methods that
reconstruct the input image, RADAR directly produces anomaly maps from the
diffusion model, improving both detection accuracy and computational
efficiency. We evaluate RADAR on real-world 3D-printed material and the
MVTec-AD dataset. Our approach surpasses state-of-the-art diffusion-based and
statistical machine learning models across all key metrics, including accuracy,
precision, recall, and F1 score. Specifically, RADAR improves F1 score by 7% on
MVTec-AD and 13% on the 3D-printed material dataset compared to the next best
model.
  Code available at: https://github.com/mehrdadmoradi124/RADAR

</details>


### [288] [CoMAD: A Multiple-Teacher Self-Supervised Distillation Framework](https://arxiv.org/abs/2508.04816)
*Sriram Mandalika,Lalitha V*

Main category: cs.CV

TL;DR: 提出CoMAD框架统一多自监督模型知识到紧凑学生网络，在多任务上表现佳。


<details>
  <summary>Details</summary>
Motivation: 现有自监督学习范式孤立预训练，模型大不适用于资源受限场景，需统一知识。

Method: 提出CoMAD框架，从三个预训练教师模型蒸馏知识，用非对称掩码、线性适配器、层归一化、联合共识门控，用双级KL散度训练学生模型。

Result: CoMAD的ViT - Tiny在ImageNet - 1K上Top - 1达75.4%，在密集预测迁移任务表现佳。

Conclusion: CoMAD在紧凑自监督学习蒸馏上达到新的最优水平。

Abstract: Numerous self-supervised learning paradigms, such as contrastive learning and
masked image modeling, learn powerful representations from unlabeled data but
are typically pretrained in isolation, overlooking complementary insights and
yielding large models that are impractical for resource-constrained deployment.
To overcome these challenges, we introduce Consensus-oriented Masked
Distillation (CoMAD), a lightweight, parameter-free framework that unifies
knowledge from multiple current state-of-the-art self-supervised Vision
Transformers into a compact student network. CoMAD distills from three
pretrained ViT-Base teachers, MAE, MoCo v3, and iBOT, each offering distinct
semantic and contextual priors. Rather than naively averaging teacher outputs,
we apply asymmetric masking: the student sees only 25 percent of patches while
each teacher receives a progressively lighter, unique mask, forcing the student
to interpolate missing features under richer contexts. Teacher embeddings are
aligned to the student's space via a linear adapter and layer normalization,
then fused through our joint consensus gating, which weights each token by
combining cosine affinity with inter-teacher agreement. The student is trained
with dual-level KL divergence on visible tokens and reconstructed feature maps,
capturing both local and global structure. On ImageNet-1K, CoMAD's ViT-Tiny
achieves 75.4 percent Top-1, an increment of 0.4 percent over the previous
state-of-the-art. In dense-prediction transfers, it attains 47.3 percent mIoU
on ADE20K, and 44.5 percent box average precision and 40.5 percent mask average
precision on MS-COCO, establishing a new state-of-the-art in compact SSL
distillation.

</details>


### [289] [Revealing Temporal Label Noise in Multimodal Hateful Video Classification](https://arxiv.org/abs/2508.04900)
*Shuonan Yang,Tailin Chen,Rahul Singh,Jiangbei Yue,Jianbo Jiao,Zeyu Fu*

Main category: cs.CV

TL;DR: 本文通过细粒度方法研究视频级标注的标签歧义对多模态仇恨视频检测的影响，指出时间戳噪声的问题，强调需时间感知模型和基准。


<details>
  <summary>Details</summary>
Motivation: 在线多媒体内容使仇恨言论传播加剧，现有多模态仇恨视频检测方法多依赖粗粒度视频级标注，存在标签噪声问题。

Method: 使用标注时间戳裁剪仇恨视频以分离仇恨片段，对裁剪片段进行探索性分析，开展对照实验。

Result: 时间戳噪声会改变模型决策边界，削弱分类置信度，体现仇恨言论表达的上下文依赖性和时间连续性。

Conclusion: 研究为多模态仇恨视频的时间动态提供新见解，强调需要时间感知模型和基准来提高鲁棒性和可解释性。

Abstract: The rapid proliferation of online multimedia content has intensified the
spread of hate speech, presenting critical societal and regulatory challenges.
While recent work has advanced multimodal hateful video detection, most
approaches rely on coarse, video-level annotations that overlook the temporal
granularity of hateful content. This introduces substantial label noise, as
videos annotated as hateful often contain long non-hateful segments. In this
paper, we investigate the impact of such label ambiguity through a fine-grained
approach. Specifically, we trim hateful videos from the HateMM and
MultiHateClip English datasets using annotated timestamps to isolate explicitly
hateful segments. We then conduct an exploratory analysis of these trimmed
segments to examine the distribution and characteristics of both hateful and
non-hateful content. This analysis highlights the degree of semantic overlap
and the confusion introduced by coarse, video-level annotations. Finally,
controlled experiments demonstrated that time-stamp noise fundamentally alters
model decision boundaries and weakens classification confidence, highlighting
the inherent context dependency and temporal continuity of hate speech
expression. Our findings provide new insights into the temporal dynamics of
multimodal hateful videos and highlight the need for temporally aware models
and benchmarks for improved robustness and interpretability. Code and data are
available at
https://github.com/Multimodal-Intelligence-Lab-MIL/HatefulVideoLabelNoise.

</details>


### [290] [Extending Foundational Monocular Depth Estimators to Fisheye Cameras with Calibration Tokens](https://arxiv.org/abs/2508.04928)
*Suchisrit Gangopadhyay,Jung-Hee Kim,Xien Chen,Patrick Rim,Hyoungseob Park,Alex Wong*

Main category: cs.CV

TL;DR: 提出将基于透视图像训练的基础单目深度估计器（FMDEs）扩展到鱼眼图像的方法，利用校准令牌调整潜在嵌入，自监督且无需鱼眼图像，在室内外均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: FMDEs在经过大量图像训练后，仍易受相机校准参数变化导致的协变量偏移影响，产生错误深度估计。

Method: 引入校准令牌作为轻量级调整机制，调整鱼眼图像潜在嵌入分布使其与透视图像对齐；自监督训练，将透视图像重新校准为鱼眼图像并确保估计一致性。

Result: 在室内外使用多个FMDEs评估，使用一组令牌始终优于现有方法。

Conclusion: 该方法能有效将FMDEs扩展到鱼眼图像，避免传统重新校准或映射投影的负面影响。

Abstract: We propose a method to extend foundational monocular depth estimators
(FMDEs), trained on perspective images, to fisheye images. Despite being
trained on tens of millions of images, FMDEs are susceptible to the covariate
shift introduced by changes in camera calibration (intrinsic, distortion)
parameters, leading to erroneous depth estimates. Our method aligns the
distribution of latent embeddings encoding fisheye images to those of
perspective images, enabling the reuse of FMDEs for fisheye cameras without
retraining or finetuning. To this end, we introduce a set of Calibration Tokens
as a light-weight adaptation mechanism that modulates the latent embeddings for
alignment. By exploiting the already expressive latent space of FMDEs, we posit
that modulating their embeddings avoids the negative impact of artifacts and
loss introduced in conventional recalibration or map projection to a canonical
reference frame in the image space. Our method is self-supervised and does not
require fisheye images but leverages publicly available large-scale perspective
image datasets. This is done by recalibrating perspective images to fisheye
images, and enforcing consistency between their estimates during training. We
evaluate our approach with several FMDEs, on both indoors and outdoors, where
we consistently improve over state-of-the-art methods using a single set of
tokens for both. Code available at:
https://github.com/JungHeeKim29/calibration-token.

</details>


### [291] [TRKT: Weakly Supervised Dynamic Scene Graph Generation with Temporal-enhanced Relation-aware Knowledge Transferring](https://arxiv.org/abs/2508.04943)
*Zhu Xu,Ting Lei,Zhimin Li,Guan Wang,Qingchao Chen,Yuxin Peng,Yang liu*

Main category: cs.CV

TL;DR: 提出Temporal-enhanced Relation-aware Knowledge Transferring (TRKT)方法解决弱监督动态场景图生成中外部目标检测器的问题，在Action Genome数据集达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有弱监督动态场景图生成方法依赖外部目标检测器，在动态、关系感知场景中定位不准确、提案置信度低。

Method: 提出TRKT方法，包含关系感知知识挖掘（使用目标和关系类解码器生成注意力图，采用帧间注意力增强策略）和双流融合模块（将注意力图集成到外部检测中）。

Result: TRKT在Action Genome数据集上取得了最先进的性能。

Conclusion: TRKT方法有效解决了弱监督动态场景图生成中外部目标检测器的问题。

Abstract: Dynamic Scene Graph Generation (DSGG) aims to create a scene graph for each
video frame by detecting objects and predicting their relationships. Weakly
Supervised DSGG (WS-DSGG) reduces annotation workload by using an unlocalized
scene graph from a single frame per video for training. Existing WS-DSGG
methods depend on an off-the-shelf external object detector to generate pseudo
labels for subsequent DSGG training. However, detectors trained on static,
object-centric images struggle in dynamic, relation-aware scenarios required
for DSGG, leading to inaccurate localization and low-confidence proposals. To
address the challenges posed by external object detectors in WS-DSGG, we
propose a Temporal-enhanced Relation-aware Knowledge Transferring (TRKT)
method, which leverages knowledge to enhance detection in relation-aware
dynamic scenarios. TRKT is built on two key components:(1)Relation-aware
knowledge mining: we first employ object and relation class decoders that
generate category-specific attention maps to highlight both object regions and
interactive areas. Then we propose an Inter-frame Attention Augmentation
strategy that exploits optical flow for neighboring frames to enhance the
attention maps, making them motion-aware and robust to motion blur. This step
yields relation- and motion-aware knowledge mining for WS-DSGG. (2) we
introduce a Dual-stream Fusion Module that integrates category-specific
attention maps into external detections to refine object localization and boost
confidence scores for object proposals. Extensive experiments demonstrate that
TRKT achieves state-of-the-art performance on Action Genome dataset. Our code
is avaliable at https://github.com/XZPKU/TRKT.git.

</details>


### [292] [AdvDINO: Domain-Adversarial Self-Supervised Representation Learning for Spatial Proteomics](https://arxiv.org/abs/2508.04955)
*Stella Su,Marc Harary,Scott J. Rodig,William Lotter*

Main category: cs.CV

TL;DR: 提出AdvDINO自监督学习框架，用于解决领域偏移问题，在多通道免疫荧光图像上效果良好且具有广泛适用性。


<details>
  <summary>Details</summary>
Motivation: 标准自监督学习方法对领域偏移的鲁棒性不确定，在生物医学成像中批效应会掩盖真实生物信号，需要解决该问题。

Method: 将梯度反转层集成到DINOv2架构中，提出AdvDINO领域对抗自监督学习框架。

Result: 在非小细胞肺癌患者的多通道免疫荧光全切片图像上，缓解了切片特定偏差，发现具有不同蛋白质组学特征和预后意义的表型簇，改进了基于注意力的多实例学习中的生存预测。

Conclusion: AdvDINO虽然在多通道免疫荧光数据上验证，但广泛适用于其他成像领域，可解决领域偏移和标注数据有限问题。

Abstract: Self-supervised learning (SSL) has emerged as a powerful approach for
learning visual representations without manual annotations. However, the
robustness of standard SSL methods to domain shift -- systematic differences
across data sources -- remains uncertain, posing an especially critical
challenge in biomedical imaging where batch effects can obscure true biological
signals. We present AdvDINO, a domain-adversarial self-supervised learning
framework that integrates a gradient reversal layer into the DINOv2
architecture to promote domain-invariant feature learning. Applied to a
real-world cohort of six-channel multiplex immunofluorescence (mIF) whole slide
images from non-small cell lung cancer patients, AdvDINO mitigates
slide-specific biases to learn more robust and biologically meaningful
representations than non-adversarial baselines. Across $>5.46$ million mIF
image tiles, the model uncovers phenotype clusters with distinct proteomic
profiles and prognostic significance, and improves survival prediction in
attention-based multiple instance learning. While demonstrated on mIF data,
AdvDINO is broadly applicable to other imaging domains -- including radiology,
remote sensing, and autonomous driving -- where domain shift and limited
annotated data hinder model generalization and interpretability.

</details>


### [293] [UGOD: Uncertainty-Guided Differentiable Opacity and Soft Dropout for Enhanced Sparse-View 3DGS](https://arxiv.org/abs/2508.04968)
*Zhihao Guo,Peng Wang,Zidong Chen,Xiangyu Kong,Yan Lyu,Guanyu Gao,Liangxiu Han*

Main category: cs.CV

TL;DR: 本文提出用学习到的不确定性对3D高斯进行自适应加权，提升稀疏视图3D合成质量，实验显示优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多数3DGS方法对高斯采用等权重渲染，易过拟合，尤其是在稀疏视图场景。

Method: 提出学习到的不确定性对高斯进行自适应加权，用于指导高斯不透明度更新，并进行软可微丢弃正则化。

Result: 在广泛采用的数据集上实验，在稀疏视图3D合成中优于竞争对手，在多数数据集上用更少高斯实现更高质量重建，如在MipNeRF 360数据集上比DropGaussian的PSNR提高3.27%。

Conclusion: 所提方法能有效提升稀疏视图3D合成的渲染质量。

Abstract: 3D Gaussian Splatting (3DGS) has become a competitive approach for novel view
synthesis (NVS) due to its advanced rendering efficiency through 3D Gaussian
projection and blending. However, Gaussians are treated equally weighted for
rendering in most 3DGS methods, making them prone to overfitting, which is
particularly the case in sparse-view scenarios. To address this, we investigate
how adaptive weighting of Gaussians affects rendering quality, which is
characterised by learned uncertainties proposed. This learned uncertainty
serves two key purposes: first, it guides the differentiable update of Gaussian
opacity while preserving the 3DGS pipeline integrity; second, the uncertainty
undergoes soft differentiable dropout regularisation, which strategically
transforms the original uncertainty into continuous drop probabilities that
govern the final Gaussian projection and blending process for rendering.
Extensive experimental results over widely adopted datasets demonstrate that
our method outperforms rivals in sparse-view 3D synthesis, achieving higher
quality reconstruction with fewer Gaussians in most datasets compared to
existing sparse-view approaches, e.g., compared to DropGaussian, our method
achieves 3.27\% PSNR improvements on the MipNeRF 360 dataset.

</details>


### [294] [Skin-SOAP: A Weakly Supervised Framework for Generating Structured SOAP Notes](https://arxiv.org/abs/2508.05019)
*Sadia Kamal,Tim Oates,Joy Wan*

Main category: cs.CV

TL;DR: 提出skin - SOAP框架从有限输入生成SOAP笔记，减少人工标注依赖，性能与GPT - 4o等相当，引入新评估指标。


<details>
  <summary>Details</summary>
Motivation: 皮肤癌常见且治疗费用高，早期诊断等很重要，但手动生成SOAP笔记劳动强度大且导致医生倦怠。

Method: 提出weakly supervised multimodal的skin - SOAP框架，利用病变图像和稀疏临床文本生成SOAP笔记，引入MedConceptEval和CCS评估指标。

Result: 该方法在关键临床相关性指标上的性能与GPT - 4o、Claude和DeepSeek Janus Pro相当。

Conclusion: skin - SOAP框架可减少对人工标注的依赖，实现可扩展的临床文档记录，减轻医生负担，减少对大量标注数据的需求。

Abstract: Skin carcinoma is the most prevalent form of cancer globally, accounting for
over $8 billion in annual healthcare expenditures. Early diagnosis, accurate
and timely treatment are critical to improving patient survival rates. In
clinical settings, physicians document patient visits using detailed SOAP
(Subjective, Objective, Assessment, and Plan) notes. However, manually
generating these notes is labor-intensive and contributes to clinician burnout.
In this work, we propose skin-SOAP, a weakly supervised multimodal framework to
generate clinically structured SOAP notes from limited inputs, including lesion
images and sparse clinical text. Our approach reduces reliance on manual
annotations, enabling scalable, clinically grounded documentation while
alleviating clinician burden and reducing the need for large annotated data.
Our method achieves performance comparable to GPT-4o, Claude, and DeepSeek
Janus Pro across key clinical relevance metrics. To evaluate this clinical
relevance, we introduce two novel metrics MedConceptEval and Clinical Coherence
Score (CCS) which assess semantic alignment with expert medical concepts and
input features, respectively.

</details>


### [295] [Automatic Image Colorization with Convolutional Neural Networks and Generative Adversarial Networks](https://arxiv.org/abs/2508.05068)
*Ruiyu Li,Changyuan Qiu,Hangrui Cao,Qihan Ren,Yuqing Qiu*

Main category: cs.CV

TL;DR: 本文聚焦图像上色问题，探讨通过分类和对抗学习实现自动图像上色，并基于先前工作构建模型、改进和比较。


<details>
  <summary>Details</summary>
Motivation: 图像上色在多个领域有应用，但该问题具有挑战性且早期回归任务方法忽略了颜色预测的多模态性质，因此探索新方法。

Method: 通过分类和对抗学习进行自动图像上色，基于先前工作构建模型，针对特定场景进行修改并作比较。

Result: 未提及。

Conclusion: 未提及。

Abstract: Image colorization, the task of adding colors to grayscale images, has been
the focus of significant research efforts in computer vision in recent years
for its various application areas such as color restoration and automatic
animation colorization [15, 1]. The colorization problem is challenging as it
is highly ill-posed with two out of three image dimensions lost, resulting in
large degrees of freedom. However, semantics of the scene as well as the
surface texture could provide important cues for colors: the sky is typically
blue, the clouds are typically white and the grass is typically green, and
there are huge amounts of training data available for learning such priors
since any colored image could serve as a training data point [20].
  Colorization is initially formulated as a regression task[5], which ignores
the multi-modal nature of color prediction. In this project, we explore
automatic image colorization via classification and adversarial learning. We
will build our models on prior works, apply modifications for our specific
scenario and make comparisons.

</details>


### [296] [Latent Expression Generation for Referring Image Segmentation and Grounding](https://arxiv.org/abs/2508.05123)
*Seonghoon Yu,Joonbeom Hong,Joonseok Lee,Jeany Son*

Main category: cs.CV

TL;DR: 提出新视觉定位框架，利用多潜在表达提升视觉定位任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法多依赖单一文本输入，无法充分利用视觉信息，易导致相似对象误识别。

Method: 提出新视觉定位框架，引入主题分配器和视觉概念注入器模块，采用正边距对比学习策略。

Result: 方法在多个基准上超越现有方法，在GRES基准上表现出色。

Conclusion: 所提方法有效提升视觉定位任务性能。

Abstract: Visual grounding tasks, such as referring image segmentation (RIS) and
referring expression comprehension (REC), aim to localize a target object based
on a given textual description. The target object in an image can be described
in multiple ways, reflecting diverse attributes such as color, position, and
more. However, most existing methods rely on a single textual input, which
captures only a fraction of the rich information available in the visual
domain. This mismatch between rich visual details and sparse textual cues can
lead to the misidentification of similar objects. To address this, we propose a
novel visual grounding framework that leverages multiple latent expressions
generated from a single textual input by incorporating complementary visual
details absent from the original description. Specifically, we introduce
subject distributor and visual concept injector modules to embed both
shared-subject and distinct-attributes concepts into the latent
representations, thereby capturing unique and target-specific visual cues. We
also propose a positive-margin contrastive learning strategy to align all
latent expressions with the original text while preserving subtle variations.
Experimental results show that our method not only outperforms state-of-the-art
RIS and REC approaches on multiple benchmarks but also achieves outstanding
performance on the generalized referring expression segmentation (GRES)
benchmark.

</details>


### [297] [FedGIN: Federated Learning with Dynamic Global Intensity Non-linear Augmentation for Organ Segmentation using Multi-modal Images](https://arxiv.org/abs/2508.05137)
*Sachin Dudda Nagaraju,Ashkan Moradi,Bendik Skarre Abrahamsen,Mattijs Elschot*

Main category: cs.CV

TL;DR: 提出FedGIN框架解决多模态医学图像分割问题，在不同数据集场景下验证其有效性与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割需准确鲁棒模型，开发跨模态统一模型有益，但面临数据稀缺、域偏移和隐私限制等挑战。

Method: 提出FedGIN框架，集成轻量级GIN增强模块，在本地训练时协调特定模态的强度分布，并在两种数据集场景下评估。

Result: 有限数据场景下，MRI测试用例3D Dice分数提升12 - 18%；完整数据集场景下，比MRI和CT单模态基线分别提升30%和10%。

Conclusion: FedGIN在隐私约束下有很强的跨模态泛化能力。

Abstract: Medical image segmentation plays a crucial role in AI-assisted diagnostics,
surgical planning, and treatment monitoring. Accurate and robust segmentation
models are essential for enabling reliable, data-driven clinical decision
making across diverse imaging modalities. Given the inherent variability in
image characteristics across modalities, developing a unified model capable of
generalizing effectively to multiple modalities would be highly beneficial.
This model could streamline clinical workflows and reduce the need for
modality-specific training. However, real-world deployment faces major
challenges, including data scarcity, domain shift between modalities (e.g., CT
vs. MRI), and privacy restrictions that prevent data sharing. To address these
issues, we propose FedGIN, a Federated Learning (FL) framework that enables
multimodal organ segmentation without sharing raw patient data. Our method
integrates a lightweight Global Intensity Non-linear (GIN) augmentation module
that harmonizes modality-specific intensity distributions during local
training. We evaluated FedGIN using two types of datasets: an imputed dataset
and a complete dataset. In the limited dataset scenario, the model was
initially trained using only MRI data, and CT data was added to assess its
performance improvements. In the complete dataset scenario, both MRI and CT
data were fully utilized for training on all clients. In the limited-data
scenario, FedGIN achieved a 12 to 18% improvement in 3D Dice scores on MRI test
cases compared to FL without GIN and consistently outperformed local baselines.
In the complete dataset scenario, FedGIN demonstrated near-centralized
performance, with a 30% Dice score improvement over the MRI-only baseline and a
10% improvement over the CT-only baseline, highlighting its strong
cross-modality generalization under privacy constraints.

</details>


### [298] [ReasoningTrack: Chain-of-Thought Reasoning for Long-term Vision-Language Tracking](https://arxiv.org/abs/2508.05221)
*Xiao Wang,Liye Jin,Xufeng Lou,Shiao Wang,Lan Chen,Bo Jiang,Zhipeng Zhang*

Main category: cs.CV

TL;DR: 本文提出基于Qwen2.5 - VL的推理式视觉 - 语言跟踪框架ReasoningTrack，用SFT和GRPO优化，还提出TNLLT数据集，实验验证策略有效性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉 - 语言跟踪方法性能有限，未充分利用大模型优势和展现推理过程。

Method: 基于预训练模型Qwen2.5 - VL构建ReasoningTrack框架，用SFT和GRPO优化，嵌入更新语言描述与视觉特征输入跟踪骨干网络，用跟踪头预测目标位置，提出TNLLT数据集。

Result: 在多个视觉 - 语言跟踪基准数据集上实验，验证了基于推理的自然语言生成策略的有效性。

Conclusion: 所提推理式自然语言生成策略能有效提升视觉 - 语言跟踪性能，代码将在指定网址发布。

Abstract: Vision-language tracking has received increasing attention in recent years,
as textual information can effectively address the inflexibility and inaccuracy
associated with specifying the target object to be tracked. Existing works
either directly fuse the fixed language with vision features or simply modify
using attention, however, their performance is still limited. Recently, some
researchers have explored using text generation to adapt to the variations in
the target during tracking, however, these works fail to provide insights into
the model's reasoning process and do not fully leverage the advantages of large
models, which further limits their overall performance. To address the
aforementioned issues, this paper proposes a novel reasoning-based
vision-language tracking framework, named ReasoningTrack, based on a
pre-trained vision-language model Qwen2.5-VL. Both SFT (Supervised Fine-Tuning)
and reinforcement learning GRPO are used for the optimization of reasoning and
language generation. We embed the updated language descriptions and feed them
into a unified tracking backbone network together with vision features. Then,
we adopt a tracking head to predict the specific location of the target object.
In addition, we propose a large-scale long-term vision-language tracking
benchmark dataset, termed TNLLT, which contains 200 video sequences. 20
baseline visual trackers are re-trained and evaluated on this dataset, which
builds a solid foundation for the vision-language visual tracking task.
Extensive experiments on multiple vision-language tracking benchmark datasets
fully validated the effectiveness of our proposed reasoning-based natural
language generation strategy. The source code of this paper will be released on
https://github.com/Event-AHU/Open_VLTrack

</details>


### [299] [Navigating the Trade-off: A Synthesis of Defensive Strategies for Zero-Shot Adversarial Robustness in Vision-Language Models](https://arxiv.org/abs/2508.05237)
*Zane Xu,Jason Sun*

Main category: cs.CV

TL;DR: 报告综合八篇关于VLM零样本对抗鲁棒性的论文，分析防御范式及发展，指出挑战与方向。


<details>
  <summary>Details</summary>
Motivation: 解决增强对抗鲁棒性和保持模型零样本泛化能力之间的权衡问题。

Method: 分析两种主要防御范式，即修改模型参数的对抗性微调（AFT）和保留参数的免训练/测试时防御。

Result: 追溯从保对齐方法到嵌入空间重构、从输入启发式到潜在空间净化的发展。

Conclusion: 确定了混合防御策略和对抗性预训练等关键挑战和未来方向。

Abstract: This report synthesizes eight seminal papers on the zero-shot adversarial
robustness of vision-language models (VLMs) like CLIP. A central challenge in
this domain is the inherent trade-off between enhancing adversarial robustness
and preserving the model's zero-shot generalization capabilities. We analyze
two primary defense paradigms: Adversarial Fine-Tuning (AFT), which modifies
model parameters, and Training-Free/Test-Time Defenses, which preserve them. We
trace the evolution from alignment-preserving methods (TeCoA) to embedding
space re-engineering (LAAT, TIMA), and from input heuristics (AOM, TTC) to
latent-space purification (CLIPure). Finally, we identify key challenges and
future directions including hybrid defense strategies and adversarial
pre-training.

</details>


### [300] [RegionMed-CLIP: A Region-Aware Multimodal Contrastive Learning Pre-trained Model for Medical Image Understanding](https://arxiv.org/abs/2508.05244)
*Tianchen Fang,Guiru Liu*

Main category: cs.CV

TL;DR: 本文提出RegionMed - CLIP框架和MedRegion - 500k语料库解决医学图像理解难题，实验效果超SOTA。


<details>
  <summary>Details</summary>
Motivation: 解决医学图像理解中高质量标注数据有限和依赖全局特征忽略局部病理区域的问题。

Method: 引入RegionMed - CLIP框架，其核心是ROI处理器，采用渐进式训练策略；构建MedRegion - 500k语料库。

Result: 在图像 - 文本检索、零样本分类和视觉问答任务上，RegionMed - CLIP大幅超越现有模型。

Conclusion: 区域感知对比预训练很重要，RegionMed - CLIP是推进多模态医学图像理解的强大基础。

Abstract: Medical image understanding plays a crucial role in enabling automated
diagnosis and data-driven clinical decision support. However, its progress is
impeded by two primary challenges: the limited availability of high-quality
annotated medical data and an overreliance on global image features, which
often miss subtle but clinically significant pathological regions. To address
these issues, we introduce RegionMed-CLIP, a region-aware multimodal
contrastive learning framework that explicitly incorporates localized
pathological signals along with holistic semantic representations. The core of
our method is an innovative region-of-interest (ROI) processor that adaptively
integrates fine-grained regional features with the global context, supported by
a progressive training strategy that enhances hierarchical multimodal
alignment. To enable large-scale region-level representation learning, we
construct MedRegion-500k, a comprehensive medical image-text corpus that
features extensive regional annotations and multilevel clinical descriptions.
Extensive experiments on image-text retrieval, zero-shot classification, and
visual question answering tasks demonstrate that RegionMed-CLIP consistently
exceeds state-of-the-art vision language models by a wide margin. Our results
highlight the critical importance of region-aware contrastive pre-training and
position RegionMed-CLIP as a robust foundation for advancing multimodal medical
image understanding.

</details>


### [301] [A Study of Gender Classification Techniques Based on Iris Images: A Deep Survey and Analysis](https://arxiv.org/abs/2508.05246)
*Basna Mohammed Salih Hasan,Ramadhan J. Mstafa*

Main category: cs.CV

TL;DR: 本文介绍性别分类应用，回顾方法，为研究者提供现有方法知识与分析，指出领域差距挑战并给出改进建议。


<details>
  <summary>Details</summary>
Motivation: 性别分类在多领域有应用价值，已有多种方法但存在可研究之处，需总结分析现有方法。

Method: 回顾以往文献，讨论多种确定性别的方法，分析不同步骤的方法论。

Result: 对现有性别分类方法进行知识和分析，指出领域存在的差距和挑战。

Conclusion: 为相关研究者提供帮助，指明未来改进方向。

Abstract: Gender classification is attractive in a range of applications, including
surveillance and monitoring, corporate profiling, and human-computer
interaction. Individuals' identities may be gleaned from information about
their gender, which is a kind of soft biometric.Over the years, several methods
for determining a person's gender have been devised. Some of the most
well-known ones are based on physical characteristics like face, fingerprint,
palmprint, DNA, ears, gait, and iris. On the other hand, facial features
account for the vast majority of gender classification methods. Also, the iris
is a significant biometric trait because the iris, according to research,
remains basically constant during an individual's life. Besides that, the iris
is externally visible and is non-invasive to the user, which is important for
practical applications. Furthermore, there are already high-quality methods for
segmenting and encoding iris images, and the current methods facilitate
selecting and extracting attribute vectors from iris textures. This study
discusses several approaches to determining gender. The previous works of
literature are briefly reviewed. Additionally, there are a variety of
methodologies for different steps of gender classification. This study provides
researchers with knowledge and analysis of the existing gender classification
approaches. Also, it will assist researchers who are interested in this
specific area, as well as highlight the gaps and challenges in the field, and
finally provide suggestions and future paths for improvement.

</details>


### [302] [CF3: Compact and Fast 3D Feature Fields](https://arxiv.org/abs/2508.05254)
*Hyunjoon Lee,Joonkyu Min,Jaesik Park*

Main category: cs.CV

TL;DR: 提出CF3方法构建紧凑快速的3D高斯特征场，用少量高斯点实现有竞争力的3D特征场


<details>
  <summary>Details</summary>
Motivation: 现有3DGS结合2D基础模型方法依赖自底向上优化，计算成本高

Method: 先对多视图2D特征与预训练高斯进行快速加权融合，在提升特征上训练每个高斯的自编码器，引入自适应稀疏化方法优化特征场高斯属性并修剪合并冗余高斯

Result: 与Feature - 3DGS相比，用低至5%的高斯点实现有竞争力的3D特征场

Conclusion: 所提CF3方法能有效构建紧凑快速的3D高斯特征场

Abstract: 3D Gaussian Splatting (3DGS) has begun incorporating rich information from 2D
foundation models. However, most approaches rely on a bottom-up optimization
process that treats raw 2D features as ground truth, incurring increased
computational costs. We propose a top-down pipeline for constructing compact
and fast 3D Gaussian feature fields, namely, CF3. We first perform a fast
weighted fusion of multi-view 2D features with pre-trained Gaussians. This
approach enables training a per-Gaussian autoencoder directly on the lifted
features, instead of training autoencoders in the 2D domain. As a result, the
autoencoder better aligns with the feature distribution. More importantly, we
introduce an adaptive sparsification method that optimizes the Gaussian
attributes of the feature field while pruning and merging the redundant
Gaussians, constructing an efficient representation with preserved geometric
details. Our approach achieves a competitive 3D feature field using as little
as 5% of the Gaussians compared to Feature-3DGS.

</details>


### [303] [Robust Tracking with Particle Filtering for Fluorescent Cardiac Imaging](https://arxiv.org/abs/2508.05262)
*Suresh Guttikonda,Maximilian Neidhart,Johanna Sprenger,Johannes Petersen,Christian Detter,Alexander Schlaefer*

Main category: cs.CV

TL;DR: 提出基于循环一致性检查的粒子滤波跟踪器用于心脏成像目标跟踪，速度快、误差小。


<details>
  <summary>Details</summary>
Motivation: 心脏运动和血管结构变化使传统跟踪方法难以用于术中荧光心脏成像的目标跟踪。

Method: 提出基于循环一致性检查的粒子滤波跟踪器来跟踪采样粒子以跟随目标地标。

Result: 能以25.4 fps同时跟踪117个目标，跟踪误差为(5.00 +/- 0.22 px)，优于其他深度学习跟踪器和传统跟踪器。

Conclusion: 所提方法可实现术中实时估计，在心脏成像目标跟踪中有优势。

Abstract: Intraoperative fluorescent cardiac imaging enables quality control following
coronary bypass grafting surgery. We can estimate local quantitative
indicators, such as cardiac perfusion, by tracking local feature points.
However, heart motion and significant fluctuations in image characteristics
caused by vessel structural enrichment limit traditional tracking methods. We
propose a particle filtering tracker based on cyclicconsistency checks to
robustly track particles sampled to follow target landmarks. Our method tracks
117 targets simultaneously at 25.4 fps, allowing real-time estimates during
interventions. It achieves a tracking error of (5.00 +/- 0.22 px) and
outperforms other deep learning trackers (22.3 +/- 1.1 px) and conventional
trackers (58.1 +/- 27.1 px).

</details>


### [304] [SGDFuse: SAM-Guided Diffusion for High-Fidelity Infrared and Visible Image Fusion](https://arxiv.org/abs/2508.05264)
*Xiaoyang Zhang,Zhen Hua,Yakun Ju,Wei Zhou,Jun Liu,Alex C. Kot*

Main category: cs.CV

TL;DR: 本文提出SGDFuse模型解决红外与可见光图像融合问题，经实验验证性能优异。


<details>
  <summary>Details</summary>
Motivation: 现有红外与可见光图像融合方法缺乏场景语义理解，易丢失关键目标，融合过程会引入伪影和细节丢失，影响图像质量和任务性能。

Method: 提出由SAM引导的条件扩散模型SGDFuse，利用SAM生成的语义掩码作为先验，分两阶段进行融合，先初步融合多模态特征，再结合语义掩码和初步融合图像驱动扩散模型进行去噪生成。

Result: SGDFuse在主观和客观评估以及下游任务适应性方面达到了最先进水平。

Conclusion: SGDFuse为图像融合的核心挑战提供了有力解决方案。

Abstract: Infrared and visible image fusion (IVIF) aims to combine the thermal
radiation information from infrared images with the rich texture details from
visible images to enhance perceptual capabilities for downstream visual tasks.
However, existing methods often fail to preserve key targets due to a lack of
deep semantic understanding of the scene, while the fusion process itself can
also introduce artifacts and detail loss, severely compromising both image
quality and task performance. To address these issues, this paper proposes
SGDFuse, a conditional diffusion model guided by the Segment Anything Model
(SAM), to achieve high-fidelity and semantically-aware image fusion. The core
of our method is to utilize high-quality semantic masks generated by SAM as
explicit priors to guide the optimization of the fusion process via a
conditional diffusion model. Specifically, the framework operates in a
two-stage process: it first performs a preliminary fusion of multi-modal
features, and then utilizes the semantic masks from SAM jointly with the
preliminary fused image as a condition to drive the diffusion model's
coarse-to-fine denoising generation. This ensures the fusion process not only
has explicit semantic directionality but also guarantees the high fidelity of
the final result. Extensive experiments demonstrate that SGDFuse achieves
state-of-the-art performance in both subjective and objective evaluations, as
well as in its adaptability to downstream tasks, providing a powerful solution
to the core challenges in image fusion. The code of SGDFuse is available at
https://github.com/boshizhang123/SGDFuse.

</details>


### [305] [VS-LLM: Visual-Semantic Depression Assessment based on LLM for Drawing Projection Test](https://arxiv.org/abs/2508.05299)
*Meiqi Wu,Yaxuan Kang,Xuchen Li,Shiyu Hu,Xiaotang Chen,Yunfeng Kang,Weiqiang Wang,Kaiqi Huang*

Main category: cs.CV

TL;DR: 提出有效识别方法支持大规模自动DPT，利用VS - LLM方法评估抑郁，结果比心理学家评估法提升17.6%，代码和数据集公开。


<details>
  <summary>Details</summary>
Motivation: DPT中PPAT解读费力且依赖心理学家经验，需有效识别方法支持大规模自动DPT。

Method: 提供PPAT草图自动分析实验环境，提出基于LLM的Visual - Semantic抑郁评估（VS - LLM）方法。

Result: 实验结果显示该方法比心理学家评估方法提升17.6%。

Conclusion: 该工作有助于基于PPAT草图元素识别的心理状态评估研究。

Abstract: The Drawing Projection Test (DPT) is an essential tool in art therapy,
allowing psychologists to assess participants' mental states through their
sketches. Specifically, through sketches with the theme of "a person picking an
apple from a tree (PPAT)", it can be revealed whether the participants are in
mental states such as depression. Compared with scales, the DPT can enrich
psychologists' understanding of an individual's mental state. However, the
interpretation of the PPAT is laborious and depends on the experience of the
psychologists. To address this issue, we propose an effective identification
method to support psychologists in conducting a large-scale automatic DPT.
Unlike traditional sketch recognition, DPT more focus on the overall evaluation
of the sketches, such as color usage and space utilization. Moreover, PPAT
imposes a time limit and prohibits verbal reminders, resulting in low drawing
accuracy and a lack of detailed depiction. To address these challenges, we
propose the following efforts: (1) Providing an experimental environment for
automated analysis of PPAT sketches for depression assessment; (2) Offering a
Visual-Semantic depression assessment based on LLM (VS-LLM) method; (3)
Experimental results demonstrate that our method improves by 17.6% compared to
the psychologist assessment method. We anticipate that this work will
contribute to the research in mental state assessment based on PPAT sketches'
elements recognition. Our datasets and codes are available at
https://github.com/wmeiqi/VS-LLM.

</details>


### [306] [Toward Errorless Training ImageNet-1k](https://arxiv.org/abs/2508.04941)
*Bo Deng,Levi Heath*

Main category: cs.CV

TL;DR: 本文用新方法训练前馈人工神经网络，在ImageNet 2012竞赛数据集上取得高准确率，推测未达100%准确率原因是双标签问题。


<details>
  <summary>Details</summary>
Motivation: 未明确提及，推测是提高在ImageNet 2012竞赛数据集上的分类准确率。

Method: 用新方法[5]在ImageNet 2012竞赛数据集上训练前馈人工神经网络。

Result: 准确率达98.3%，Top - 1率99.69%，平均285.9个标签完美分类，最佳模型用322,430,160个参数，精度4位小数。

Conclusion: 模型未达100%准确率可能是数据集存在双标签问题。

Abstract: In this paper, we describe a feedforward artificial neural network trained on
the ImageNet 2012 contest dataset [7] with the new method of [5] to an accuracy
rate of 98.3% with a 99.69 Top-1 rate, and an average of 285.9 labels that are
perfectly classified over the 10 batch partitions of the dataset. The best
performing model uses 322,430,160 parameters, with 4 decimal places precision.
We conjecture that the reason our model does not achieve a 100% accuracy rate
is due to a double-labeling problem, by which there are duplicate images in the
dataset with different labels.

</details>


### [307] [mKG-RAG: Multimodal Knowledge Graph-Enhanced RAG for Visual Question Answering](https://arxiv.org/abs/2508.05318)
*Xu Yuan,Liangbo Ning,Wenqi Fan,Qing Li*

Main category: cs.CV

TL;DR: 本文提出基于多模态知识图谱的mKG - RAG框架用于知识密集型VQA任务，实验表明该方法显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于RAG的VQA方法依赖非结构化文档，忽略知识元素间结构关系，常引入无关或误导内容，降低答案准确性和可靠性。

Method: 提出mKG - RAG框架，利用MLLM进行关键词提取和视觉 - 文本匹配构建多模态知识图谱，引入双阶段检索策略和问题感知多模态检索器。

Result: 该方法显著优于现有方法。

Conclusion: 所提方法为基于知识的VQA设定了新的技术水平。

Abstract: Recently, Retrieval-Augmented Generation (RAG) has been proposed to expand
internal knowledge of Multimodal Large Language Models (MLLMs) by incorporating
external knowledge databases into the generation process, which is widely used
for knowledge-based Visual Question Answering (VQA) tasks. Despite impressive
advancements, vanilla RAG-based VQA methods that rely on unstructured documents
and overlook the structural relationships among knowledge elements frequently
introduce irrelevant or misleading content, reducing answer accuracy and
reliability. To overcome these challenges, a promising solution is to integrate
multimodal knowledge graphs (KGs) into RAG-based VQA frameworks to enhance the
generation by introducing structured multimodal knowledge. Therefore, in this
paper, we propose a novel multimodal knowledge-augmented generation framework
(mKG-RAG) based on multimodal KGs for knowledge-intensive VQA tasks.
Specifically, our approach leverages MLLM-powered keyword extraction and
vision-text matching to distill semantically consistent and modality-aligned
entities/relationships from multimodal documents, constructing high-quality
multimodal KGs as structured knowledge representations. In addition, a
dual-stage retrieval strategy equipped with a question-aware multimodal
retriever is introduced to improve retrieval efficiency while refining
precision. Comprehensive experiments demonstrate that our approach
significantly outperforms existing methods, setting a new state-of-the-art for
knowledge-based VQA.

</details>


### [308] [CRAM: Large-scale Video Continual Learning with Bootstrapped Compression](https://arxiv.org/abs/2508.05001)
*Shivani Mall,Joao F. Henriques*

Main category: cs.CV

TL;DR: 论文聚焦视频持续学习，提出CRAM方法，用压缩视觉存储视频代码，处理遗忘问题，在大规模视频基准测试中表现优于现有方法且减少内存占用。


<details>
  <summary>Details</summary>
Motivation: 持续学习可使神经网络从连续输入流学习，但视频持续学习因高内存需求面临挑战，需要解决此问题。

Method: 采用基于排练的方法，使用压缩视觉存储视频代码而非原始输入，提出刷新视频代码方案处理遗忘问题，训练在线视频压缩器。

Result: 将视频持续学习基准扩展到大规模设置，在EpicKitchens - 100和Kinetics - 700上用不到2GB存储数千个较长视频，方法优于现有技术且显著减少内存占用。

Conclusion: 提出的CRAM方法能有效解决视频持续学习中的高内存需求问题，在大规模视频基准测试中表现良好。

Abstract: Continual learning (CL) promises to allow neural networks to learn from
continuous streams of inputs, instead of IID (independent and identically
distributed) sampling, which requires random access to a full dataset. This
would allow for much smaller storage requirements and self-sufficiency of
deployed systems that cope with natural distribution shifts, similarly to
biological learning. We focus on video CL employing a rehearsal-based approach,
which reinforces past samples from a memory buffer. We posit that part of the
reason why practical video CL is challenging is the high memory requirements of
video, further exacerbated by long-videos and continual streams, which are at
odds with the common rehearsal-buffer size constraints. To address this, we
propose to use compressed vision, i.e. store video codes (embeddings) instead
of raw inputs, and train a video classifier by IID sampling from this rolling
buffer. Training a video compressor online (so not depending on any pre-trained
networks) means that it is also subject to catastrophic forgetting. We propose
a scheme to deal with this forgetting by refreshing video codes, which requires
careful decompression with a previous version of the network and recompression
with a new one. We name our method Continually Refreshed Amodal Memory (CRAM).
We expand current video CL benchmarks to large-scale settings, namely
EpicKitchens-100 and Kinetics-700, storing thousands of relatively long videos
in under 2 GB, and demonstrate empirically that our video CL method outperforms
prior art with a significantly reduced memory footprint.

</details>


### [309] [PriorRG: Prior-Guided Contrastive Pre-training and Coarse-to-Fine Decoding for Chest X-ray Report Generation](https://arxiv.org/abs/2508.05353)
*Kang Liu,Zhuoqi Ma,Zikang Fang,Yunan Li,Kun Xie,Qiguang Miao*

Main category: cs.CV

TL;DR: 提出PriorRG框架用于胸部X光报告生成，通过两阶段训练利用患者先验知识，实验显示优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有胸部X光报告生成方法未有效利用患者先验知识，无法捕捉诊断意图和疾病进展，需改进。

Method: 提出PriorRG框架，包含先验引导的对比预训练阶段和先验感知的粗到精解码阶段。

Result: 在MIMIC - CXR和MIMIC - ABN数据集上实验，PriorRG优于现有方法，MIMIC - CXR上BLEU - 4提升3.6%、F1分数提升3.8%，MIMIC - ABN上BLEU - 1提升5.9%。

Conclusion: PriorRG框架能有效利用患者先验知识，提高生成报告的临床准确性和流畅性。

Abstract: Chest X-ray report generation aims to reduce radiologists' workload by
automatically producing high-quality preliminary reports. A critical yet
underexplored aspect of this task is the effective use of patient-specific
prior knowledge -- including clinical context (e.g., symptoms, medical history)
and the most recent prior image -- which radiologists routinely rely on for
diagnostic reasoning. Most existing methods generate reports from single
images, neglecting this essential prior information and thus failing to capture
diagnostic intent or disease progression. To bridge this gap, we propose
PriorRG, a novel chest X-ray report generation framework that emulates
real-world clinical workflows via a two-stage training pipeline. In Stage 1, we
introduce a prior-guided contrastive pre-training scheme that leverages
clinical context to guide spatiotemporal feature extraction, allowing the model
to align more closely with the intrinsic spatiotemporal semantics in radiology
reports. In Stage 2, we present a prior-aware coarse-to-fine decoding for
report generation that progressively integrates patient-specific prior
knowledge with the vision encoder's hidden states. This decoding allows the
model to align with diagnostic focus and track disease progression, thereby
enhancing the clinical accuracy and fluency of the generated reports. Extensive
experiments on MIMIC-CXR and MIMIC-ABN datasets demonstrate that PriorRG
outperforms state-of-the-art methods, achieving a 3.6% BLEU-4 and 3.8% F1 score
improvement on MIMIC-CXR, and a 5.9% BLEU-1 gain on MIMIC-ABN. Code and
checkpoints will be released upon acceptance.

</details>


### [310] [UNCAGE: Contrastive Attention Guidance for Masked Generative Transformers in Text-to-Image Generation](https://arxiv.org/abs/2508.05399)
*Wonjun Kang,Byeongkeun Ahn,Minjae Lee,Kevin Galim,Seunghyuk Oh,Hyung Il Koo,Nam Ik Cho*

Main category: cs.CV

TL;DR: 提出无训练方法UNCAGE解决文本到图像生成中组合性难题，多基准测试表现佳。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像生成模型存在组合性难题，Masked Generative Transformers在此方面未被充分研究。

Method: 提出Unmasking with Contrastive Attention Guidance (UNCAGE)，利用注意力图优先解掩码代表单个对象的标记。

Result: 在多个基准测试和指标的定量和定性评估中持续提升性能，推理开销可忽略不计。

Conclusion: UNCAGE方法有效解决文本到图像生成的组合性问题，代码开源。

Abstract: Text-to-image (T2I) generation has been actively studied using Diffusion
Models and Autoregressive Models. Recently, Masked Generative Transformers have
gained attention as an alternative to Autoregressive Models to overcome the
inherent limitations of causal attention and autoregressive decoding through
bidirectional attention and parallel decoding, enabling efficient and
high-quality image generation. However, compositional T2I generation remains
challenging, as even state-of-the-art Diffusion Models often fail to accurately
bind attributes and achieve proper text-image alignment. While Diffusion Models
have been extensively studied for this issue, Masked Generative Transformers
exhibit similar limitations but have not been explored in this context. To
address this, we propose Unmasking with Contrastive Attention Guidance
(UNCAGE), a novel training-free method that improves compositional fidelity by
leveraging attention maps to prioritize the unmasking of tokens that clearly
represent individual objects. UNCAGE consistently improves performance in both
quantitative and qualitative evaluations across multiple benchmarks and
metrics, with negligible inference overhead. Our code is available at
https://github.com/furiosa-ai/uncage.

</details>


### [311] [SPA++: Generalized Graph Spectral Alignment for Versatile Domain Adaptation](https://arxiv.org/abs/2508.05182)
*Zhiqing Xiao,Haobo Wang,Xu Lu,Wentao Ye,Gang Chen,Junbo Zhao*

Main category: cs.CV

TL;DR: 提出广义图谱对齐框架SPA++解决领域自适应中域内结构和可区分性的权衡问题，实验表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多数先前工作在领域自适应中忽视丰富的域内结构，导致可区分性变差，需解决此权衡问题。

Method: 将领域自适应问题转化为图原语，结合粗粒度图对齐机制和谱正则化器，开发细粒度邻域感知传播机制，结合数据增强和一致性正则化。

Result: 在基准数据集上的大量实验表明，SPA++始终优于现有的前沿方法，在各种具有挑战性的适应场景中实现了卓越的鲁棒性和适应性。

Conclusion: SPA++能有效解决领域自适应中域内结构和可区分性的权衡问题，具有良好的性能和适应性。

Abstract: Domain Adaptation (DA) aims to transfer knowledge from a labeled source
domain to an unlabeled or sparsely labeled target domain under domain shifts.
Most prior works focus on capturing the inter-domain transferability but
largely overlook rich intra-domain structures, which empirically results in
even worse discriminability. To tackle this tradeoff, we propose a generalized
graph SPectral Alignment framework, SPA++. Its core is briefly condensed as
follows: (1)-by casting the DA problem to graph primitives, it composes a
coarse graph alignment mechanism with a novel spectral regularizer toward
aligning the domain graphs in eigenspaces; (2)-we further develop a
fine-grained neighbor-aware propagation mechanism for enhanced discriminability
in the target domain; (3)-by incorporating data augmentation and consistency
regularization, SPA++ can adapt to complex scenarios including most DA settings
and even challenging distribution scenarios. Furthermore, we also provide
theoretical analysis to support our method, including the generalization bound
of graph-based DA and the role of spectral alignment and smoothing consistency.
Extensive experiments on benchmark datasets demonstrate that SPA++ consistently
outperforms existing cutting-edge methods, achieving superior robustness and
adaptability across various challenging adaptation scenarios.

</details>


### [312] [Explaining Similarity in Vision-Language Encoders with Weighted Banzhaf Interactions](https://arxiv.org/abs/2508.05430)
*Hubert Baniecki,Maximilian Muschalik,Fabian Fumagalli,Barbara Hammer,Eyke Hüllermeier,Przemyslaw Biecek*

Main category: cs.CV

TL;DR: 提出FIxLIP方法分解视觉语言编码器相似度，实验验证二阶方法优于一阶，还可用于模型比较。


<details>
  <summary>Details</summary>
Motivation: 现有流行显著性图仅捕获一阶归因，忽略复杂跨模态交互，需新方法。

Method: 引入基于博弈论的FIxLIP分解相似度，用加权Banzhaf交互指数，扩展解释评估指标到二阶交互解释。

Result: 在MS COCO和ImageNet - 1k基准上，FIxLIP等二阶方法优于一阶归因方法。

Conclusion: FIxLIP能提供高质量解释，可用于比较不同模型。

Abstract: Language-image pre-training (LIP) enables the development of vision-language
models capable of zero-shot classification, localization, multimodal retrieval,
and semantic understanding. Various explanation methods have been proposed to
visualize the importance of input image-text pairs on the model's similarity
outputs. However, popular saliency maps are limited by capturing only
first-order attributions, overlooking the complex cross-modal interactions
intrinsic to such encoders. We introduce faithful interaction explanations of
LIP models (FIxLIP) as a unified approach to decomposing the similarity in
vision-language encoders. FIxLIP is rooted in game theory, where we analyze how
using the weighted Banzhaf interaction index offers greater flexibility and
improves computational efficiency over the Shapley interaction quantification
framework. From a practical perspective, we propose how to naturally extend
explanation evaluation metrics, like the pointing game and area between the
insertion/deletion curves, to second-order interaction explanations.
Experiments on MS COCO and ImageNet-1k benchmarks validate that second-order
methods like FIxLIP outperform first-order attribution methods. Beyond
delivering high-quality explanations, we demonstrate the utility of FIxLIP in
comparing different models like CLIP vs. SigLIP-2 and ViT-B/32 vs. ViT-L/16.

</details>


### [313] [Keep It Real: Challenges in Attacking Compression-Based Adversarial Purification](https://arxiv.org/abs/2508.05489)
*Samuel Räber,Till Aczel,Andreas Plesner,Roger Wattenhofer*

Main category: cs.CV

TL;DR: 研究构建攻击压缩模型，发现高逼真度重建图像的压缩模型抗攻击，低逼真度易被攻破，原因非梯度掩码，而是分布对齐带来的固有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 此前工作缺乏对图像有损压缩防御对抗扰动的全面攻击评估，本文旨在填补此空白。

Method: 构建强白盒和自适应攻击针对各种压缩模型，在多个攻击场景下进行严格评估。

Result: 高逼真度重建的压缩模型更抗攻击，低逼真度的可被攻破，且不是因为梯度掩码。

Conclusion: 指出未来对抗攻击面临重大障碍，克服逼真度是全面安全评估的关键挑战。

Abstract: Previous work has suggested that preprocessing images through lossy
compression can defend against adversarial perturbations, but comprehensive
attack evaluations have been lacking. In this paper, we construct strong
white-box and adaptive attacks against various compression models and identify
a critical challenge for attackers: high realism in reconstructed images
significantly increases attack difficulty. Through rigorous evaluation across
multiple attack scenarios, we demonstrate that compression models capable of
producing realistic, high-fidelity reconstructions are substantially more
resistant to our attacks. In contrast, low-realism compression models can be
broken. Our analysis reveals that this is not due to gradient masking. Rather,
realistic reconstructions maintaining distributional alignment with natural
images seem to offer inherent robustness. This work highlights a significant
obstacle for future adversarial attacks and suggests that developing more
effective techniques to overcome realism represents an essential challenge for
comprehensive security evaluation.

</details>


### [314] [Test-Time Reinforcement Learning for GUI Grounding via Region Consistency](https://arxiv.org/abs/2508.05615)
*Yong Du,Yuchen Yan,Fei Tang,Zhengxi Lu,Chang Zong,Weiming Lu,Shengpei Jiang,Yongliang Shen*

Main category: cs.CV

TL;DR: 提出GUI - RC和GUI - RCPO方法，无需训练提升GUI接地任务精度，展现测试时缩放和强化学习潜力。


<details>
  <summary>Details</summary>
Motivation: 现有GUI接地方法受像素级标注成本和可用性限制，需更高效方法。

Method: 提出GUI - RC构建空间投票网格找共识区域；提出GUI - RCPO将一致性模式转为奖励进行测试时强化学习。

Result: GUI - RC使不同架构在ScreenSpot基准上精度提高2 - 3%；GUI - RC将Qwen2.5 - VL - 3B - Instruct在ScreenSpot - v2上的精度从80.11%提升到83.57%，GUI - RCPO进一步提升到85.14%。

Conclusion: 测试时缩放和强化学习对GUI接地有未挖掘潜力，为构建更鲁棒、数据高效的GUI代理提供了有前景的途径。

Abstract: Graphical User Interface (GUI) grounding, the task of mapping natural
language instructions to precise screen coordinates, is fundamental to
autonomous GUI agents. While existing methods achieve strong performance
through extensive supervised training or reinforcement learning with labeled
rewards, they remain constrained by the cost and availability of pixel-level
annotations. We observe that when models generate multiple predictions for the
same GUI element, the spatial overlap patterns reveal implicit confidence
signals that can guide more accurate localization. Leveraging this insight, we
propose GUI-RC (Region Consistency), a test-time scaling method that constructs
spatial voting grids from multiple sampled predictions to identify consensus
regions where models show highest agreement. Without any training, GUI-RC
improves accuracy by 2-3% across various architectures on ScreenSpot
benchmarks. We further introduce GUI-RCPO (Region Consistency Policy
Optimization), which transforms these consistency patterns into rewards for
test-time reinforcement learning. By computing how well each prediction aligns
with the collective consensus, GUI-RCPO enables models to iteratively refine
their outputs on unlabeled data during inference. Extensive experiments
demonstrate the generality of our approach: GUI-RC boosts
Qwen2.5-VL-3B-Instruct from 80.11% to 83.57% on ScreenSpot-v2, while GUI-RCPO
further improves it to 85.14% through self-supervised optimization. Our
approach reveals the untapped potential of test-time scaling and test-time
reinforcement learning for GUI grounding, offering a promising path toward more
robust and data-efficient GUI agents.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [315] [Advanced Multi-Architecture Deep Learning Framework for BIRADS-Based Mammographic Image Retrieval: Comprehensive Performance Analysis with Super-Ensemble Optimization](https://arxiv.org/abs/2508.04790)
*MD Shaikh Rahman,Feiroz Humayara,Syed Maudud E Rabbi,Muhammad Mahbubur Rashid*

Main category: eess.IV

TL;DR: 本文针对基于内容的乳腺图像检索系统，开发综合评估框架对比CNN架构与训练策略，取得显著性能提升并建立新基准。


<details>
  <summary>Details</summary>
Motivation: 当前医学图像检索研究存在方法局限，阻碍临床转化，且基于内容的乳腺图像检索比常见的二分类任务更复杂。

Method: 开发综合评估框架，对比DenseNet121、ResNet50、VGG16等CNN架构，采用精细微调、度量学习和超集成优化等训练策略，使用严格分层数据划分和自举置信区间进行验证。

Result: 高级微调使DenseNet121和ResNet50的precision@10显著提升，超集成优化precision@10达36.33%，性能远超文献预期，且保持搜索效率。

Conclusion: 框架建立了新的性能基准，为临床诊断支持和质量保证应用提供了基于证据的架构选择指南。

Abstract: Content-based mammographic image retrieval systems require exact BIRADS
categorical matching across five distinct classes, presenting significantly
greater complexity than binary classification tasks commonly addressed in
literature. Current medical image retrieval studies suffer from methodological
limitations including inadequate sample sizes, improper data splitting, and
insufficient statistical validation that hinder clinical translation. We
developed a comprehensive evaluation framework systematically comparing CNN
architectures (DenseNet121, ResNet50, VGG16) with advanced training strategies
including sophisticated fine-tuning, metric learning, and super-ensemble
optimization. Our evaluation employed rigorous stratified data splitting
(50%/20%/30% train/validation/test), 602 test queries, and systematic
validation using bootstrap confidence intervals with 1,000 samples. Advanced
fine-tuning with differential learning rates achieved substantial improvements:
DenseNet121 (34.79% precision@10, 19.64% improvement) and ResNet50 (34.54%,
19.58% improvement). Super-ensemble optimization combining complementary
architectures achieved 36.33% precision@10 (95% CI: [34.78%, 37.88%]),
representing 24.93% improvement over baseline and providing 3.6 relevant cases
per query. Statistical analysis revealed significant performance differences
between optimization strategies (p<0.001) with large effect sizes (Cohen's
d>0.8), while maintaining practical search efficiency (2.8milliseconds).
Performance significantly exceeds realistic expectations for 5-class medical
retrieval tasks, where literature suggests 20-25% precision@10 represents
achievable performance for exact BIRADS matching. Our framework establishes new
performance benchmarks while providing evidence-based architecture selection
guidelines for clinical deployment in diagnostic support and quality assurance
applications.

</details>


### [316] [Coarse-to-Fine Joint Registration of MR and Ultrasound Images via Imaging Style Transfer](https://arxiv.org/abs/2508.05240)
*Junyi Wang,Xi Zhu,Yikun Guo,Zixi Wang,Haichuan Gao,Le Zhang,Fan Zhang*

Main category: eess.IV

TL;DR: 开发了注册术前MR图像和术后US图像的管道，利用3D CycleGAN生成合成T1图像并采用粗到精注册，多数情况下提高了图像对一致性。


<details>
  <summary>Details</summary>
Motivation: 解决术前MR图像和术后US图像的注册问题，提高图像对一致性。

Method: 利用3D CycleGAN进行无配对风格迁移生成合成T1图像，注册过程采用仿射和局部可变形变换进行粗到精注册。

Result: 在多数情况下改善了MR和US图像对的一致性。

Conclusion: 所提出的方法能有效提高术前MR图像和术后US图像注册的一致性。

Abstract: We developed a pipeline for registering pre-surgery Magnetic Resonance (MR)
images and post-resection Ultrasound (US) images. Our approach leverages
unpaired style transfer using 3D CycleGAN to generate synthetic T1 images,
thereby enhancing registration performance. Additionally, our registration
process employs both affine and local deformable transformations for a
coarse-to-fine registration. The results demonstrate that our approach improves
the consistency between MR and US image pairs in most cases.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [317] [Assessing Dynamic Connectedness in Global Supply Chain Infrastructure Portfolios: The Impact of Risk Factors and Extreme Events](https://arxiv.org/abs/2508.04858)
*Haibo Wang*

Main category: econ.EM

TL;DR: 本文分析全球供应链基础设施投资风险因素，提出投资组合策略，用TVP - VAR模型研究风险溢出，对比疫情前后情况，发现风险冲击影响、ESG得分与关联性关系及疫情对投资策略结构的改变。


<details>
  <summary>Details</summary>
Motivation: 分析全球供应链基础设施投资的风险因素并提出相关投资组合策略，研究极端事件（如新冠疫情）对风险溢出和投资策略的影响。

Method: 运用时变参数向量自回归（TVP - VAR）模型，研究2010年1月5日至2023年6月29日全球供应链基础设施投资组合风险因素与ESG指数的溢出和关联性，对比新冠疫情前后情况。

Result: 风险冲击影响投资组合与风险因素动态关联性；高ESG得分组合关联性更强；确定部分指标是净接收或净给予溢出冲击方；疫情改变投资组合动态关联结构和投资策略中长短仓权重。

Conclusion: 全球供应链基础设施投资中，高ESG得分投资组合对冲能力更优。

Abstract: This paper analyses the risk factors around investing in global supply chain
infrastructure: the energy market, investor sentiment, and global shipping
costs. It presents portfolio strategies associated with dynamic risks. A
time-varying parameter vector autoregression (TVP-VAR) model is used to study
the spillover and interconnectedness of the risk factors for global supply
chain infrastructure portfolios from January 5th, 2010, to June 29th, 2023,
which are associated with a set of environmental, social, and governance (ESG)
indexes. The effects of extreme events on risk spillovers and investment
strategy are calculated and compared before and after the COVID-19 outbreak.
The results of this study demonstrate that risk shocks influence the dynamic
connectedness between global supply chain infrastructure portfolios and three
risk factors and show the effects of extreme events on risk spillovers and
investment outcomes. Portfolios with higher ESG scores exhibit stronger dynamic
connectedness with other portfolios and factors. Net total directional
connectedness indicates that West Texas Intermediate (WTI), Baltic Exchange Dry
Index (BDI), and investor sentiment volatility index (VIX) consistently are net
receivers of spillover shocks. A portfolio with a ticker GLFOX appears to be a
time-varying net receiver and giver. The pairwise connectedness shows that WTI
and VIX are mostly net receivers. Portfolios with tickers CSUAX, GII, and FGIAX
are mostly net givers of spillover shocks. The COVID-19 outbreak changed the
structure of dynamic connectedness on portfolios. The mean value of HR and HE
indicates that the weights of long/short positions in investment strategy after
the COVID-19 outbreak have undergone structural changes compared to the period
before. The hedging ability of global supply chain infrastructure investment
portfolios with higher ESG scores is superior.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [318] [Federal Reserve Communication and the COVID-19 Pandemic](https://arxiv.org/abs/2508.04830)
*Jonathan Benchimol,Sophia Kazinnik,Yossi Saadon*

Main category: econ.GN

TL;DR: 研究美联储在新冠疫情期间的沟通策略，与以往经济压力时期对比，发现对新冠危机更具反应性，非常规货币政策沟通成新常态。


<details>
  <summary>Details</summary>
Motivation: 研究美联储在新冠疫情期间的沟通策略，并与以往经济压力时期对比，了解央行沟通在危机中的演变和策略适应情况。

Method: 使用针对新冠、非常规货币政策和金融稳定的专业词典，结合情感分析和主题建模技术，进行比较分析。

Result: 美联储沟通和政策行动对新冠危机更具反应性；利率公告和会议纪要中金融稳定相关情绪下降预示后续宽松货币政策决策；自全球金融危机后，非常规货币政策沟通成新常态。

Conclusion: 研究有助于理解央行沟通在危机中的演变和策略对特殊经济情况的适应。

Abstract: In this study, we examine the Federal Reserve's communication strategies
during the COVID-19 pandemic, comparing them with communication during previous
periods of economic stress. Using specialized dictionaries tailored to
COVID-19, unconventional monetary policy (UMP), and financial stability,
combined with sentiment analysis and topic modeling techniques, we identify a
distinct focus in Fed communication during the pandemic on financial stability,
market volatility, social welfare, and UMP, characterized by notable contextual
uncertainty. Through comparative analysis, we juxtapose the Fed's communication
during the COVID-19 crisis with its responses during the dot-com and global
financial crises, examining content, sentiment, and timing dimensions. Our
findings reveal that Fed communication and policy actions were more reactive to
the COVID-19 crisis than to previous crises. Additionally, declining sentiment
related to financial stability in interest rate announcements and minutes
anticipated subsequent accommodative monetary policy decisions. We further
document that communicating about UMP has become the "new normal" for the Fed's
Federal Open Market Committee meeting minutes and Chairman's speeches since the
Global Financial Crisis, reflecting an institutional adaptation in
communication strategy following periods of economic distress. These findings
contribute to our understanding of how central bank communication evolves
during crises and how communication strategies adapt to exceptional economic
circumstances.

</details>


### [319] [Finding Core Balanced Modules in Statistically Validated Stock Networks](https://arxiv.org/abs/2508.04970)
*Huan Qing,Xiaofei Xu*

Main category: econ.GN

TL;DR: 提出统计验证的相关网络和LSCBM结构，开发MaxBalanceCore算法检测LSCBM，模拟验证其效率，实证表明LSCBM能识别核心市场子系统。


<details>
  <summary>Details</summary>
Motivation: 传统基于阈值的股票网络存在主观参数选择和固有局限性，无法捕捉相关性强度和负依赖关系。

Method: 引入统计验证的相关网络，提出LSCBM结构，在随机符号图模型中理论分析LSCBM性质，开发MaxBalanceCore启发式算法。

Result: 模拟显示MaxBalanceCore可在数十秒内处理多达10,000个节点的网络；实证表明LSCBM在不同经济时期有不同表现。

Conclusion: LSCBM能有效识别核心市场子系统，随经济变化动态重组。

Abstract: Traditional threshold-based stock networks suffer from subjective parameter
selection and inherent limitations: they constrain relationships to binary
representations, failing to capture both correlation strength and negative
dependencies. To address this, we introduce statistically validated correlation
networks that retain only statistically significant correlations via a rigorous
t-test of Pearson coefficients. We then propose a novel structure termed the
largest strong-correlation balanced module (LSCBM), defined as the maximum-size
group of stocks with structural balance (i.e., positive edge-ign products for
all triplets) and strong pairwise correlations. This balance condition ensures
stable relationships, thus facilitating potential hedging opportunities through
negative edges. Theoretically, within a random signed graph model, we establish
LSCBM's asymptotic existence, size scaling, and multiplicity under various
parameter regimes. To detect LSCBM efficiently, we develop MaxBalanceCore, a
heuristic algorithm that leverages network sparsity. Simulations validate its
efficiency, demonstrating scalability to networks of up to 10,000 nodes within
tens of seconds. Empirical analysis demonstrates that LSCBM identifies core
market subsystems that dynamically reorganize in response to economic shifts
and crises. In the Chinese stock market (2013-2024), LSCBM's size surges during
high-stress periods (e.g., the 2015 crash) and contracts during stable or
fragmented regimes, while its composition rotates annually across dominant
sectors (e.g., Industrials and Financials).

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [320] [Sequence Aware SAC Control for Engine Fuel Consumption Optimization in Electrified Powertrain](https://arxiv.org/abs/2508.04874)
*Wafeeq Jaleel,Md Ragib Rownak,Athar Hanif,Sidra Ghayour Bhatti,Qadeer Ahmed*

Main category: eess.SY

TL;DR: 提出基于SAC算法的强化学习框架优化串联混合动力汽车发动机控制，实验表明改进后的SAC代理在燃油节省上表现良好且具有适应性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 混合动力汽车在重型卡车中逐渐普及，需要自适应高效的能量管理以降低油耗并保持电池电量。

Method: 将控制任务转化为顺序决策问题，在SAC的演员和评论家网络中融入GRU和DT以捕捉时间依赖和改进规划，并在多种条件下训练模型。

Result: 在HFET循环中，基于DT演员和GRU评论家的SAC代理燃油节省接近DP，在未知驾驶循环中，序列感知代理表现优于FFN代理。

Conclusion: 所提出的框架具有良好的燃油节省效果、适应性和鲁棒性，适用于现实场景。

Abstract: As hybrid electric vehicles (HEVs) gain traction in heavy-duty trucks,
adaptive and efficient energy management is critical for reducing fuel
consumption while maintaining battery charge for long operation times. We
present a new reinforcement learning (RL) framework based on the Soft
Actor-Critic (SAC) algorithm to optimize engine control in series HEVs. We
reformulate the control task as a sequential decision-making problem and
enhance SAC by incorporating Gated Recurrent Units (GRUs) and Decision
Transformers (DTs) into both actor and critic networks to capture temporal
dependencies and improve planning over time. To evaluate robustness and
generalization, we train the models under diverse initial battery states, drive
cycle durations, power demands, and input sequence lengths. Experiments show
that the SAC agent with a DT-based actor and GRU-based critic was within 1.8%
of Dynamic Programming (DP) in fuel savings on the Highway Fuel Economy Test
(HFET) cycle, while the SAC agent with GRUs in both actor and critic networks,
and FFN actor-critic agent were within 3.16% and 3.43%, respectively. On unseen
drive cycles (US06 and Heavy Heavy-Duty Diesel Truck (HHDDT) cruise segment),
generalized sequence-aware agents consistently outperformed feedforward network
(FFN)-based agents, highlighting their adaptability and robustness in
real-world settings.

</details>


<div id='econ.TH'></div>

# econ.TH [[Back]](#toc)

### [321] [Pairwise efficiency and monotonicity imply Pareto efficiency in (probabilistic) object allocation](https://arxiv.org/abs/2508.05340)
*Tom Demeulemeester,Bettina Klaus*

Main category: econ.TH

TL;DR: 研究对象分配问题，证明满足特定条件时事后配对效率等价于事后帕累托效率，可强化现有规则的刻画结果。


<details>
  <summary>Details</summary>
Motivation: 在对象分配问题中，探索规则性质间的关系以强化现有刻画结果。

Method: 通过理论推导，证明满足事后非浪费性和概率（马斯金）单调性时，事后配对效率与事后帕累托效率的等价关系。

Result: 证明了若彩票规则满足特定条件，事后配对效率等价于事后帕累托效率。

Conclusion: 该结果可通过用（事后）配对效率替代（事后）帕累托效率，强化多种彩票规则和确定性规则的现有刻画结果。

Abstract: We consider object allocation problems with capacities (see, e.g.,
Abdulkadiroglu and Sonmez, 1998; Basteck, 2025) where objects have to be
assigned to agents. We show that if a lottery rule satisfies ex-post
non-wastefulness and probabilistic (Maskin) monotonicity, then ex-post pairwise
efficiency is equivalent to ex-post Pareto efficiency. This result allows for a
strengthening of various existing characterization results, both for lottery
rules and deterministic rules, by replacing (ex-post) Pareto efficiency with
(ex-post) pairwise efficiency, e.g., for characterizations of the Random Serial
Dictatorship rule (Basteck, 2025), Trading Cycles rules (Pycia and Unver,
2017), and Hierarchical Exchange rules (Papai, 2000).

</details>


<div id='astro-ph.EP'></div>

# astro-ph.EP [[Back]](#toc)

### [322] [Supervised Machine Learning Methods with Uncertainty Quantification for Exoplanet Atmospheric Retrievals from Transmission Spectroscopy](https://arxiv.org/abs/2508.04982)
*Roy T. Forestano,Konstantin T. Matchev,Katia Matcheva,Eyup B. Unlu*

Main category: astro-ph.EP

TL;DR: 本文系统研究多种机器学习回归技术，对比其从透射光谱中反演系外行星大气参数的性能，并进行基准测试，还研究训练数据预处理方法的影响，最后用案例验证最佳组合。


<details>
  <summary>Details</summary>
Motivation: 标准贝叶斯反演系外行星大气参数计算成本高，在JWST等时代，需高效且稳健的机器学习替代方法。

Method: 研究多种机器学习回归技术（PLS、SVM等），对不同算法的准确性、精度和速度进行基准测试，研究不同训练数据预处理方法对模型性能的影响，量化行星参数整个动态范围内的模型不确定性。

Result: 得到不同机器学习模型和预处理方案的性能情况，找到最佳组合。

Conclusion: 通过对JWST对WASP - 39b的观测案例验证了最佳的ML模型和预处理方案组合。

Abstract: Standard Bayesian retrievals for exoplanet atmospheric parameters from
transmission spectroscopy, while well understood and widely used, are generally
computationally expensive. In the era of the JWST and other upcoming
observatories, machine learning approaches have emerged as viable alternatives
that are both efficient and robust. In this paper we present a systematic study
of several existing machine learning regression techniques and compare their
performance for retrieving exoplanet atmospheric parameters from transmission
spectra. We benchmark the performance of the different algorithms on the
accuracy, precision, and speed. The regression methods tested here include
partial least squares (PLS), support vector machines (SVM), k nearest neighbors
(KNN), decision trees (DT), random forests (RF), voting (VOTE), stacking
(STACK), and extreme gradient boosting (XGB). We also investigate the impact of
different preprocessing methods of the training data on the model performance.
We quantify the model uncertainties across the entire dynamical range of
planetary parameters. The best performing combination of ML model and
preprocessing scheme is validated on a the case study of JWST observation of
WASP-39b.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [323] [Learning Geometric-Aware Quadrature Rules for Functional Minimization](https://arxiv.org/abs/2508.05445)
*Costas Smaragdakis*

Main category: math.NA

TL;DR: 提出QuadrANN图神经网络架构学习点云最优求积权重，测试显示其能降低积分估计方差，改进变分求解器。


<details>
  <summary>Details</summary>
Motivation: 现代无网格机器学习偏微分方程求解器在非均匀点云上精确数值积分存在挑战，标准蒙特卡罗方法无法处理非均匀点云。

Method: 引入QuadrANN图神经网络架构，利用深度消息传递方案，初始层编码局部几何特征，后续层结合全局上下文向量。

Result: 在多个测试用例中，QuadrANN通过扭曲点云在关键区域增加密度，降低了积分估计方差。

Conclusion: QuadrANN增强了域关键区域稳定性，有助于优化能量泛函，改进基于深度学习的变分解求解器。

Abstract: Accurate numerical integration over non-uniform point clouds is a challenge
for modern mesh-free machine learning solvers for partial differential
equations (PDEs) using variational principles. While standard Monte Carlo (MC)
methods are not capable of handling a non-uniform point cloud, modern neural
network architectures can deal with permutation-invariant inputs, creating
quadrature rules for any point cloud. In this work, we introduce QuadrANN, a
Graph Neural Network (GNN) architecture designed to learn optimal quadrature
weights directly from the underlying geometry of point clouds. The design of
the model exploits a deep message-passing scheme where the initial layer
encodes rich local geometric features from absolute and relative positions as
well as an explicit local density measure. In contrast, the following layers
incorporate a global context vector. These architectural choices allow the
QuadrANN to generate a data-driven quadrature rule that is
permutation-invariant and adaptive to both local point density and the overall
domain shape. We test our methodology on a series of challenging test cases,
including integration on convex and non-convex domains and estimating the
solution of the Heat and Fokker-Planck equations. Across all the tests,
QuadrANN reduces the variance of the integral estimation compared to standard
Quasi-Monte Carlo methods by warping the point clouds to be more dense in
critical areas where the integrands present certain singularities. This
enhanced stability in critical areas of the domain at hand is critical for the
optimization of energy functionals, leading to improved deep learning-based
variational solvers.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [324] [Data Driven Insights into Composition Property Relationships in FCC High Entropy Alloys](https://arxiv.org/abs/2508.04841)
*Nicolas Flores,Daniel Salas Mula,Wenle Xu,Sahu Bibhu,Daniel Lewis,Alexandra Eve Salinas,Samantha Mitra,Raj Mahat,Surya R. Kalidindi,Justin Wilkerson,James Paramore,Ankit Srivastiva,George Pharr,Douglas Allaire,Ibrahim Karaman,Brady Butler,Vahid Attari,Raymundo Arroyave*

Main category: cond-mat.mtrl-sci

TL;DR: 本文针对结构高熵合金数据稀缺问题，开展敏感性分析，评估基于编解码器的化学 - 属性模型，模型在映射合金成分与力学性能上表现良好。


<details>
  <summary>Details</summary>
Motivation: 结构高熵合金在多领域重要，但数据稀缺给预测性能建模带来挑战，需从有限异构数据中挖掘潜在模式。

Method: 进行敏感性分析，评估经贝叶斯多目标超参数优化的基于编解码器的化学 - 属性模型。

Result: 模型在所有性能指标上达到或优于传统回归器，尤其在屈服强度和UTS/YS比上表现出色。

Conclusion: 基于编解码器的化学 - 属性模型能有效捕捉复杂的成分 - 性能关系。

Abstract: Structural High Entropy Alloys (HEAs) are crucial in advancing technology
across various sectors, including aerospace, automotive, and defense
industries. However, the scarcity of integrated chemistry, process, structure,
and property data presents significant challenges for predictive property
modeling. Given the vast design space of these alloys, uncovering the
underlying patterns is essential yet difficult, requiring advanced methods
capable of learning from limited and heterogeneous datasets. This work presents
several sensitivity analyses, highlighting key elemental contributions to
mechanical behavior, including insights into the compositional factors
associated with brittle and fractured responses observed during nanoindentation
testing in the BIRDSHOT center NiCoFeCrVMnCuAl system dataset. Several encoder
decoder based chemistry property models, carefully tuned through Bayesian multi
objective hyperparameter optimization, are evaluated for mapping alloy
composition to six mechanical properties. The models achieve competitive or
superior performance to conventional regressors across all properties,
particularly for yield strength and the UTS/YS ratio, demonstrating their
effectiveness in capturing complex composition property relationships.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [325] [Understanding and Mitigating Errors of LLM-Generated RTL Code](https://arxiv.org/abs/2508.05266)
*Jiazheng Zhang,Cheng Liu,Huawei Li*

Main category: cs.AR

TL;DR: 本文针对基于大语言模型的RTL代码生成成功率低的问题，进行错误分析并提出纠错技术，集成到框架后显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的RTL代码生成整体成功率不理想，且对失败原因了解有限，阻碍了性能提升。

Method: 进行全面错误分析和手动分类，利用上下文学习提出针对性纠错技术，包括构建知识库、引入设计描述规则、集成外部工具和采用迭代调试循环。

Result: 增强后的框架在VerilogEval基准测试中达到91.0%的准确率，比基线方法高32.7%。

Conclusion: 提出的纠错技术有效提升了基于大语言模型的RTL代码生成性能。

Abstract: Despite the promising potential of large language model (LLM) based
register-transfer-level (RTL) code generation, the overall success rate remains
unsatisfactory. Errors arise from various factors, with limited understanding
of specific failure causes hindering improvement. To address this, we conduct a
comprehensive error analysis and manual categorization. Our findings reveal
that most errors stem not from LLM reasoning limitations, but from insufficient
RTL programming knowledge, poor understanding of circuit concepts, ambiguous
design descriptions, or misinterpretation of complex multimodal inputs.
Leveraging in-context learning, we propose targeted error correction
techniques. Specifically, we construct a domain-specific knowledge base and
employ retrieval-augmented generation (RAG) to supply necessary RTL knowledge.
To mitigate ambiguity errors, we introduce design description rules and
implement a rule-checking mechanism. For multimodal misinterpretation, we
integrate external tools to convert inputs into LLM-compatible meta-formats.
For remaining errors, we adopt an iterative debugging loop (simulation-error
localization-correction). Integrating these techniques into an LLM-based
framework significantly improves performance. We incorporate these error
correction techniques into a foundational LLM-based RTL code generation
framework, resulting in significantly improved performance. Experimental
results show that our enhanced framework achieves 91.0\% accuracy on the
VerilogEval benchmark, surpassing the baseline code generation approach by
32.7\%, demonstrating the effectiveness of our methods.

</details>
