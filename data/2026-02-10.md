<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 113]
- [cs.CE](#cs.CE) [Total: 7]
- [cs.DB](#cs.DB) [Total: 12]
- [cs.DC](#cs.DC) [Total: 6]
- [cs.DS](#cs.DS) [Total: 13]
- [cs.GT](#cs.GT) [Total: 7]
- [cs.IR](#cs.IR) [Total: 31]
- [cs.LG](#cs.LG) [Total: 269]
- [cs.NE](#cs.NE) [Total: 11]
- [cs.SE](#cs.SE) [Total: 39]
- [q-fin.PM](#q-fin.PM) [Total: 1]
- [q-fin.RM](#q-fin.RM) [Total: 3]
- [q-fin.ST](#q-fin.ST) [Total: 5]
- [q-fin.TR](#q-fin.TR) [Total: 1]
- [stat.ML](#stat.ML) [Total: 18]
- [stat.CO](#stat.CO) [Total: 2]
- [cs.ET](#cs.ET) [Total: 2]
- [physics.soc-ph](#physics.soc-ph) [Total: 2]
- [math.OC](#math.OC) [Total: 4]
- [cs.CV](#cs.CV) [Total: 81]
- [eess.IV](#eess.IV) [Total: 6]
- [cs.NI](#cs.NI) [Total: 3]
- [cs.CL](#cs.CL) [Total: 41]
- [q-bio.PE](#q-bio.PE) [Total: 1]
- [cs.IT](#cs.IT) [Total: 5]
- [stat.ME](#stat.ME) [Total: 8]
- [q-bio.NC](#q-bio.NC) [Total: 4]
- [physics.app-ph](#physics.app-ph) [Total: 1]
- [cs.RO](#cs.RO) [Total: 17]
- [cs.OS](#cs.OS) [Total: 2]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [cs.SI](#cs.SI) [Total: 2]
- [cs.CG](#cs.CG) [Total: 1]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 1]
- [cs.MA](#cs.MA) [Total: 5]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [eess.SY](#eess.SY) [Total: 4]
- [cs.MM](#cs.MM) [Total: 1]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 1]
- [cs.FL](#cs.FL) [Total: 1]
- [cs.HC](#cs.HC) [Total: 14]
- [econ.GN](#econ.GN) [Total: 8]
- [quant-ph](#quant-ph) [Total: 4]
- [eess.SP](#eess.SP) [Total: 5]
- [cs.LO](#cs.LO) [Total: 2]
- [cs.CR](#cs.CR) [Total: 22]
- [q-bio.QM](#q-bio.QM) [Total: 3]
- [econ.EM](#econ.EM) [Total: 3]
- [cs.CC](#cs.CC) [Total: 2]
- [cs.CY](#cs.CY) [Total: 7]
- [cs.SD](#cs.SD) [Total: 3]
- [eess.AS](#eess.AS) [Total: 1]
- [physics.pop-ph](#physics.pop-ph) [Total: 1]
- [math.NA](#math.NA) [Total: 3]
- [math.AT](#math.AT) [Total: 1]
- [physics.optics](#physics.optics) [Total: 1]
- [cs.SC](#cs.SC) [Total: 2]
- [math.ST](#math.ST) [Total: 1]
- [physics.geo-ph](#physics.geo-ph) [Total: 1]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [physics.chem-ph](#physics.chem-ph) [Total: 2]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [LLM-FSM: Scaling Large Language Models for Finite-State Reasoning in RTL Code Generation](https://arxiv.org/abs/2602.07032)
*Yuheng Wu,Berk Gokmen,Zhouhua Xie,Peijing Li,Caroline Trippel,Priyanka Raina,Thierry Tambe*

Main category: cs.AI

TL;DR: 本文提出基准LLM - FSM评估大语言模型从自然语言规范恢复有限状态机行为并转换为RTL实现的能力，实验表明强模型在FSM复杂度增加时准确率下降，训练和测试优化有效果且基准可扩展。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型从自然语言规范恢复有限状态机行为并转换为正确的寄存器传输级（RTL）实现的能力。

Method: 构建LLM - FSM基准，通过全自动流程构建FSM，让LLM以YAML格式表达FSM并转换为自然语言规范，合成参考RTL和测试平台，用多种方式验证问题。

Result: 即使最强的大语言模型在FSM复杂度增加时准确率也会急剧下降；训练时通过监督微调可有效泛化到分布外任务，增加测试时计算量可提高推理可靠性。

Conclusion: LLM - FSM可评估大语言模型相关能力，且可随未来模型能力扩展FSM复杂度。

Abstract: Finite-state reasoning, the ability to understand and implement state-dependent behavior, is central to hardware design. In this paper, we present LLM-FSM, a benchmark that evaluates how well large language models (LLMs) can recover finite-state machine (FSM) behavior from natural-language specifications and translate it into correct register transfer-level (RTL) implementations. Unlike prior specification-to-RTL benchmarks that rely on manually constructed examples, LLM-FSM is built through a fully automated pipeline. LLM-FSM first constructs FSM with configurable state counts and constrained transition structures. It then prompts LLMs to express each FSM in a structured YAML format with an application context, and to further convert that YAML into a natural-language (NL) specification. From the same YAML, our pipeline synthesizes the reference RTL and testbench in a correct-by-construction manner. All 1,000 problems are verified using LLM-based and SAT-solver-based checks, with human review on a subset. Our experiments show that even the strongest LLMs exhibit sharply declining accuracy as FSM complexity increases. We further demonstrate that training-time scaling via supervised fine-tuning (SFT) generalizes effectively to out-of-distribution (OOD) tasks, while increasing test-time compute improves reasoning reliability. Finally, LLM-FSM remains extensible by allowing its FSM complexity to scale with future model capabilities.

</details>


### [2] [ST-Raptor: An Agentic System for Semi-Structured Table QA](https://arxiv.org/abs/2602.07034)
*Jinxiu Qu,Zirui Tang,Hongzhang Huang,Boyu Niu,Wei Zhou,Jiannan Wang,Yitong Song,Guoliang Li,Xuanhe Zhou,Fan Wu*

Main category: cs.AI

TL;DR: 提出用于半结构化表格问答的代理系统ST - Raptor，实验表明其在准确率和可用性上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 半结构化表格问答任务手动处理耗时耗力，现有自动化方法存在信息丢失、处理复杂布局不准确等问题。

Method: 提出ST - Raptor系统，提供结合视觉编辑、基于树的结构建模和代理驱动查询解析的交互式分析环境。

Result: 在基准和真实世界数据集上的实验显示，ST - Raptor在准确率和可用性上超过现有方法。

Conclusion: ST - Raptor能有效解决现有半结构化表格问答方法的局限，提升表格理解能力。

Abstract: Semi-structured table question answering (QA) is a challenging task that requires (1) precise extraction of cell contents and positions and (2) accurate recovery of key implicit logical structures, hierarchical relationships, and semantic associations encoded in table layouts. In practice, such tables are often interpreted manually by human experts, which is labor-intensive and time-consuming. However, automating this process remains difficult. Existing Text-to-SQL methods typically require converting semi-structured tables into structured formats, inevitably leading to information loss, while approaches like Text-to-Code and multimodal LLM-based QA struggle with complex layouts and often yield inaccurate answers. To address these limitations, we present ST-Raptor, an agentic system for semi-structured table QA. ST-Raptor offers an interactive analysis environment that combines visual editing, tree-based structural modeling, and agent-driven query resolution to support accurate and user-friendly table understanding. Experimental results on both benchmark and real-world datasets demonstrate that ST-Raptor outperforms existing methods in both accuracy and usability. The code is available at https://github.com/weAIDB/ST-Raptor, and a demonstration video is available at https://youtu.be/9GDR-94Cau4.

</details>


### [3] [DLLM-Searcher: Adapting Diffusion Large Language Model for Search Agents](https://arxiv.org/abs/2602.07035)
*Jiahao Zhao,Shaoxuan Xu,Zhongxiang Sun,Fengqi Zhu,Jingyang Ou,Yuling Shi,Chongxuan Li,Xiao Zhang,Jun Xu*

Main category: cs.AI

TL;DR: 提出基于扩散大语言模型的搜索代理优化框架DLLM - Searcher，设计两阶段后训练管道解决代理能力挑战，提出P - ReAct范式缓解延迟挑战，实现与主流基于大语言模型搜索代理相当的性能及约15%推理加速。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型有并行解码优势，但现有dLLM骨干存在代理能力挑战；搜索代理在ReAct范式下有延迟挑战，需利用dLLM优势优化其效率。

Method: 设计包含Agentic SFT和Agentic VRPO的两阶段后训练管道，提升骨干dLLM信息搜索和推理能力；提出P - ReAct代理范式，引导模型优先解码工具调用指令。

Result: DLLM - Searcher实现与主流基于大语言模型搜索代理相当的性能，P - ReAct实现约15%的推理加速。

Conclusion: DLLM - Searcher框架和P - ReAct范式有效，可提升基于扩散大语言模型的搜索代理的性能和推理效率。

Abstract: Recently, Diffusion Large Language Models (dLLMs) have demonstrated unique efficiency advantages, enabled by their inherently parallel decoding mechanism and flexible generation paradigm. Meanwhile, despite the rapid advancement of Search Agents, their practical deployment is constrained by a fundamental limitation, termed as 1) Latency Challenge: the serial execution of multi-round reasoning, tool calling, and tool response waiting under the ReAct agent paradigm induces severe end-to-end latency. Intuitively, dLLMs can leverage their distinctive strengths to optimize the operational efficiency of agents under the ReAct agent paradigm. Practically, existing dLLM backbones face the 2) Agent Ability Challenge. That is, existing dLLMs exhibit remarkably weak reasoning and tool-calling capabilities, preventing these advantages from being effectively realized in practice. In this paper, we propose DLLM-Searcher, an optimization framework for dLLM-based Search Agents. To solve the Agent Ability Challenge, we design a two-stage post-training pipeline encompassing Agentic Supervised Fine-Tuning (Agentic SFT) and Agentic Variance-Reduced Preference Optimization Agentic VRPO, which enhances the backbone dLLM's information seeking and reasoning capabilities. To mitigate the Latency Challenge, we leverage the flexible generation mechanism of dLLMs and propose a novel agent paradigm termed Parallel-Reasoning and Acting P-ReAct. P-ReAct guides the model to prioritize decoding tool_call instructions, thereby allowing the model to keep thinking while waiting for the tool's return. Experimental results demonstrate that DLLM-Searcher achieves performance comparable to mainstream LLM-based search agents and P-ReAct delivers approximately 15% inference acceleration. Our code is available at https://anonymous.4open.science/r/DLLM-Searcher-553C

</details>


### [4] [Aster: Autonomous Scientific Discovery over 20x Faster Than Existing Methods](https://arxiv.org/abs/2602.07040)
*Emmett Bicker*

Main category: cs.AI

TL;DR: 介绍AI智能体Aster，比现有框架快20倍以上，能迭代改进程序，应用于多领域，多数任务达SOTA，可通过网络和API访问。


<details>
  <summary>Details</summary>
Motivation: 开发能实现自主科学发现且更高效的AI智能体，拓展可处理问题范围。

Method: 给定任务、初始程序和评估脚本，Aster迭代改进程序。

Result: 应用于多个领域问题，多数任务达SOTA，在ZAPBench上以不到1/190的计算量达到最佳人工解决方案性能。

Conclusion: Aster是高效的自主科学发现AI智能体，可通过asterlab.ai访问。

Abstract: We introduce Aster, an AI agent for autonomous scientific discovery capable of operating over 20 times faster than existing frameworks. Given a task, an initial program, and a script to evaluate the performance of the program, Aster iteratively improves the program, often leading to new state-of-the-art performances. Aster's significant reduction in the number of iterations required for novel discovery expands the domain of tractable problems to include tasks with long evaluation durations, such as multi-hour machine learning training runs.
  We applied Aster to problems in mathematics, GPU kernel engineering, biology, neuroscience, and language model training. More specifically: the Erdos minimum overlap problem, optimizing the TriMul kernel, a single-cell analysis denoising problem, training a neural activity prediction model to perform well on ZAPBench, and the NanoGPT Speedrun Competition. Aster attains SOTA results in every task, except for ZAPBench, where it matches the performance of the best human solution with less than 1/190th of the compute.
  Aster is accessible via a web interface and API at asterlab.ai.

</details>


### [5] [Theory of Space: Can Foundation Models Construct Spatial Beliefs through Active Exploration?](https://arxiv.org/abs/2602.07055)
*Pingyue Zhang,Zihan Huang,Yue Wang,Jieyu Zhang,Letian Xue,Zihan Wang,Qineng Wang,Keshigeyan Chandrasegaran,Ruohan Zhang,Yejin Choi,Ranjay Krishna,Jiajun Wu,Li Fei-Fei,Manling Li*

Main category: cs.AI

TL;DR: 本文提出空间理论，通过基准测试评估模型主动探索能力，发现当前基础模型在主动探索中难以维持连贯、可修正的空间信念。


<details>
  <summary>Details</summary>
Motivation: 多模态基础模型在主动、自主探索能力方面研究不足，需要研究空间具身智能中智能体主动获取信息的能力。

Method: 提出空间理论，通过好奇心驱动探索的基准测试评估模型，采用空间信念探测方法揭示模型内部空间表征。

Result: 发现主动 - 被动差距、探索低效问题，诊断出感知瓶颈、全局信念不稳定、信念惯性等问题。

Conclusion: 当前基础模型在主动探索中难以维持连贯、可修正的空间信念。

Abstract: Spatial embodied intelligence requires agents to act to acquire information under partial observability. While multimodal foundation models excel at passive perception, their capacity for active, self-directed exploration remains understudied. We propose Theory of Space, defined as an agent's ability to actively acquire information through self-directed, active exploration and to construct, revise, and exploit a spatial belief from sequential, partial observations. We evaluate this through a benchmark where the goal is curiosity-driven exploration to build an accurate cognitive map. A key innovation is spatial belief probing, which prompts models to reveal their internal spatial representations at each step. Our evaluation of state-of-the-art models reveals several critical bottlenecks. First, we identify an Active-Passive Gap, where performance drops significantly when agents must autonomously gather information. Second, we find high inefficiency, as models explore unsystematically compared to program-based proxies. Through belief probing, we diagnose that while perception is an initial bottleneck, global beliefs suffer from instability that causes spatial knowledge to degrade over time. Finally, using a false belief paradigm, we uncover Belief Inertia, where agents fail to update obsolete priors with new evidence. This issue is present in text-based agents but is particularly severe in vision-based models. Our findings suggest that current foundation models struggle to maintain coherent, revisable spatial beliefs during active exploration.

</details>


### [6] [ANCHOR: Branch-Point Data Generation for GUI Agents](https://arxiv.org/abs/2602.07153)
*Jinbiao Wei,Yilun Zhao,Kangqi Ni,Arman Cohan*

Main category: cs.AI

TL;DR: 提出轨迹扩展框架Anchor，从少量验证的种子演示中引导可扩展的桌面监督，实验表明微调模型有一致改进。


<details>
  <summary>Details</summary>
Motivation: 端到端GUI代理需要大量高质量交互数据，收集人类演示成本高，现有合成管道存在任务多样性有限或轨迹漂移问题。

Method: 从种子演示中识别分支点，提出新的任务变体，通过执行代理生成新轨迹，验证器确保任务完成，还应用任务条件步骤级过滤和去噪。

Result: 在标准桌面基准测试中，基于扩展语料库微调的模型比零样本代理和代表性合成基线有一致改进，且能跨应用和操作系统泛化。

Conclusion: 所提出的Anchor框架能有效解决现有问题，提高模型性能和泛化能力。

Abstract: End-to-end GUI agents for real desktop environments require large amounts of high-quality interaction data, yet collecting human demonstrations is expensive and existing synthetic pipelines often suffer from limited task diversity or noisy, goal-drifting trajectories. We present a trajectory expansion framework Anchor that bootstraps scalable desktop supervision from a small set of verified seed demonstrations. Starting from each seed, we identify branch points that correspond to meaningful state changes and propose new, state-grounded task variants conditioned on the current GUI context. An executing agent then follows the proposed instructions to generate new trajectories, while a verifier enforces task completion via state-aware checks and trajectory-level consistency. To improve supervision quality, we further apply task-conditioned step-level filtering to remove ungrounded actions and denoise post-branch segments to maintain coherent intent. Experiments on standard desktop benchmarks, OSWorld and WindowsAgentArena, show that models fine-tuned on our expanded corpus achieve consistent improvements over zero-shot agents and representative synthesis baselines, and generalize across applications and operating systems.

</details>


### [7] [PreFlect: From Retrospective to Prospective Reflection in Large Language Model Agents](https://arxiv.org/abs/2602.07187)
*Hanyu Wang,Yuanpu Cao,Lu Lin,Jinghui Chen*

Main category: cs.AI

TL;DR: 本文提出前瞻性反思机制PreFlect，结合动态重新规划机制，能基于历史执行经验改进计划，经评估在复杂任务上提升了代理效用。


<details>
  <summary>Details</summary>
Motivation: 现有反思方法是回顾性的，在行动失败后才尝试恢复，缺乏预先前瞻性。

Method: 提出PreFlect将范式从事后纠正转变为执行前预见，从历史代理轨迹中提取规划错误；并结合动态重新规划机制在执行中更新计划。

Result: 在不同基准测试中，PreFlect显著提升了复杂现实任务中代理的整体效用，优于强反思基线和更复杂的代理架构。

Conclusion: PreFlect是一种有效的代理反思机制，能提升代理在复杂任务中的性能。

Abstract: Advanced large language model agents typically adopt self-reflection for improving performance, where agents iteratively analyze past actions to correct errors. However, existing reflective approaches are inherently retrospective: agents act, observe failure, and only then attempt to recover. In this work, we introduce PreFlect, a prospective reflection mechanism that shifts the paradigm from post hoc correction to pre-execution foresight by criticizing and refining agent plans before execution. To support grounded prospective reflection, we distill planning errors from historical agent trajectories, capturing recurring success and failure patterns observed across past executions. Furthermore, we complement prospective reflection with a dynamic re-planning mechanism that provides execution-time plan update in case the original plan encounters unexpected deviation. Evaluations on different benchmarks demonstrate that PreFlect significantly improves overall agent utility on complex real-world tasks, outperforming strong reflection-based baselines and several more complex agent architectures. Code will be updated at https://github.com/wwwhy725/PreFlect.

</details>


### [8] [Is there "Secret Sauce'' in Large Language Model Development?](https://arxiv.org/abs/2602.07238)
*Matthias Mertens,Natalia Fischl-Lanzoni,Neil Thompson*

Main category: cs.AI

TL;DR: 用2022 - 2025年809个模型数据研究大语言模型性能驱动因素，发现前沿表现靠计算规模，非前沿有专有技术优势，且公司内模型效率差异大。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型开发者是否有专有技术‘秘诀’，还是模型性能由计算量提升驱动。

Method: 使用2022 - 2025年间809个模型的训练和基准数据，进行带发布日期和开发者固定效应的缩放定律回归分析。

Result: 前沿水平上80 - 90%的性能差异由更高训练计算量解释；非前沿处专有技术和共享算法进步可减少达固定能力阈值所需计算量；部分公司能更高效产出小模型；公司内模型计算效率差异超40倍。

Conclusion: 计算规模驱动前沿进步，非前沿有专有技术可用，公司内模型效率存在显著差异。同时讨论了对AI领导力和能力扩散的影响。

Abstract: Do leading LLM developers possess a proprietary ``secret sauce'', or is LLM performance driven by scaling up compute? Using training and benchmark data for 809 models released between 2022 and 2025, we estimate scaling-law regressions with release-date and developer fixed effects. We find clear evidence of developer-specific efficiency advantages, but their importance depends on where models lie in the performance distribution. At the frontier, 80-90% of performance differences are explained by higher training compute, implying that scale--not proprietary technology--drives frontier advances. Away from the frontier, however, proprietary techniques and shared algorithmic progress substantially reduce the compute required to reach fixed capability thresholds. Some companies can systematically produce smaller models more efficiently. Strikingly, we also find substantial variation of model efficiency within companies; a firm can train two models with more than 40x compute efficiency difference. We also discuss the implications for AI leadership and capability diffusion.

</details>


### [9] [From Out-of-Distribution Detection to Hallucination Detection: A Geometric View](https://arxiv.org/abs/2602.07253)
*Litian Liu,Reza Pourreza,Yubing Jian,Yao Qin,Roland Memisevic*

Main category: cs.AI

TL;DR: 本文将大语言模型幻觉检测问题转化为分布外（OOD）检测问题，提出免训练、基于单样本的检测器，实现推理任务幻觉检测的高准确率。


<details>
  <summary>Details</summary>
Motivation: 现有幻觉检测方法在推理任务上效果不佳，解决大语言模型安全和可靠性相关的关键问题。

Method: 将语言模型的下一个标记预测视为分类任务，对OOD技术进行适当修改以适应大语言模型的结构差异。

Result: OOD方法产生的检测器在推理任务的幻觉检测中实现了较高的准确率。

Conclusion: 将幻觉检测重新定义为OOD检测为大语言模型安全提供了有前景且可扩展的途径。

Abstract: Detecting hallucinations in large language models is a critical open problem with significant implications for safety and reliability. While existing hallucination detection methods achieve strong performance in question-answering tasks, they remain less effective on tasks requiring reasoning. In this work, we revisit hallucination detection through the lens of out-of-distribution (OOD) detection, a well-studied problem in areas like computer vision. Treating next-token prediction in language models as a classification task allows us to apply OOD techniques, provided appropriate modifications are made to account for the structural differences in large language models. We show that OOD-based approaches yield training-free, single-sample-based detectors, achieving strong accuracy in hallucination detection for reasoning tasks. Overall, our work suggests that reframing hallucination detection as OOD detection provides a promising and scalable pathway toward language model safety.

</details>


### [10] [Incentive-Aware AI Safety via Strategic Resource Allocation: A Stackelberg Security Games Perspective](https://arxiv.org/abs/2602.07259)
*Cheol Woo Kim,Davin Choo,Tzeh Yuan Neoh,Milind Tambe*

Main category: cs.AI

TL;DR: 在人工智能系统日益强大自主的背景下，提出基于Stackelberg安全博弈的新视角研究AI安全，能连接算法对齐和机构监督设计。


<details>
  <summary>Details</summary>
Motivation: 现有安全框架将对齐看作静态优化问题，忽略了塑造数据收集、模型评估和部署的动态对抗性激励，因而需要新视角研究AI安全。

Method: 提出基于Stackelberg安全博弈（SSGs）的新视角，将AI监督视为防御者和攻击者的战略互动。

Result: 该框架可用于训练时防数据/反馈中毒审计、受限资源下的预部署评估和对抗环境中的多模型稳健部署。

Conclusion: 此综合方法连接了算法对齐和机构监督设计，凸显博弈论威慑能让AI监督更主动、有风险意识且抗操纵。

Abstract: As AI systems grow more capable and autonomous, ensuring their safety and reliability requires not only model-level alignment but also strategic oversight of the humans and institutions involved in their development and deployment. Existing safety frameworks largely treat alignment as a static optimization problem (e.g., tuning models to desired behavior) while overlooking the dynamic, adversarial incentives that shape how data are collected, how models are evaluated, and how they are ultimately deployed. We propose a new perspective on AI safety grounded in Stackelberg Security Games (SSGs): a class of game-theoretic models designed for adversarial resource allocation under uncertainty. By viewing AI oversight as a strategic interaction between defenders (auditors, evaluators, and deployers) and attackers (malicious actors, misaligned contributors, or worst-case failure modes), SSGs provide a unifying framework for reasoning about incentive design, limited oversight capacity, and adversarial uncertainty across the AI lifecycle. We illustrate how this framework can inform (1) training-time auditing against data/feedback poisoning, (2) pre-deployment evaluation under constrained reviewer resources, and (3) robust multi-model deployment in adversarial environments. This synthesis bridges algorithmic alignment and institutional oversight design, highlighting how game-theoretic deterrence can make AI oversight proactive, risk-aware, and resilient to manipulation.

</details>


### [11] [BRIDGE: Predicting Human Task Completion Time From Model Performance](https://arxiv.org/abs/2602.07267)
*Fengyuan Liu,Jay Gala,Nilaksh,Dzmitry Bahdanau,Siva Reddy,Hugo Larochelle*

Main category: cs.AI

TL;DR: 提出BRIDGE框架，从模型响应学习潜在难度尺度并与人类任务完成时间关联，可推断新基准的人类完成时间，还能预测前沿模型能力。


<details>
  <summary>Details</summary>
Motivation: 现有依赖人类任务完成时间注释评估AI系统的方法成本高、噪声大且难以跨基准扩展。

Method: 使用两参数逻辑斯蒂项目反应理论模型，从多个基准的模型性能数据中联合估计潜在任务难度和模型能力。

Result: 潜在任务难度与人类完成时间的对数呈线性关系，可根据模型性能推断人类任务完成时间，预测前沿模型能力并重现METR的指数缩放结果。

Conclusion: BRIDGE框架为评估AI系统在现实世界的能力提供了有效方法。

Abstract: Evaluating the real-world capabilities of AI systems requires grounding benchmark performance in human-interpretable measures of task difficulty. Existing approaches that rely on direct human task completion time annotations are costly, noisy, and difficult to scale across benchmarks. In this work, we propose BRIDGE, a unified psychometric framework that learns the latent difficulty scale from model responses and anchors it to human task completion time. Using a two-parameter logistic Item Response Theory model, we jointly estimate latent task difficulty and model capability from model performance data across multiple benchmarks. We demonstrate that latent task difficulty varies linearly with the logarithm of human completion time, allowing human task completion time to be inferred for new benchmarks from model performance alone. Leveraging this alignment, we forecast frontier model capabilities in terms of human task length and independently reproduce METR's exponential scaling results, with the 50% solvable task horizon doubling approximately every 6 months.

</details>


### [12] [TermiGen: High-Fidelity Environment and Robust Trajectory Synthesis for Terminal Agents](https://arxiv.org/abs/2602.07274)
*Kaijie Zhu,Yuzhou Nie,Yijiang Li,Yiming Huang,Jialian Wu,Jiang Liu,Ximeng Sun,Zhenfei Yin,Lun Wang,Zicheng Liu,Emad Barsoum,William Yang Wang,Wenbo Guo*

Main category: cs.AI

TL;DR: 现有大语言模型执行复杂终端任务有局限，本文提出TermiGen管道，生成数据集微调模型在TerminalBench上取得31.3%通过率，创开源权重新纪录。


<details>
  <summary>Details</summary>
Motivation: 当前执行复杂终端任务的大语言模型面临高保真可执行训练环境稀缺和标准指令调优的专家轨迹与小模型常见错误存在分布不匹配的问题。

Method: 引入TermiGen端到端管道，先通过迭代多智能体细化循环生成功能有效的任务和Docker容器，再用生成 - 评判协议在轨迹收集时主动注入错误合成纠错数据。

Result: 基于TermiGen生成的数据集微调的TermiGen - Qwen2.5 - Coder - 32B在TerminalBench上达到31.3%的通过率。

Conclusion: TermiGen建立了新的开源权重最优水平，超越现有基线和部分专有模型。数据集可在https://github.com/ucsb - mlsec/terminal - bench - env获取。

Abstract: Executing complex terminal tasks remains a significant challenge for open-weight LLMs, constrained by two fundamental limitations. First, high-fidelity, executable training environments are scarce: environments synthesized from real-world repositories are not diverse and scalable, while trajectories synthesized by LLMs suffer from hallucinations. Second, standard instruction tuning uses expert trajectories that rarely exhibit simple mistakes common to smaller models. This creates a distributional mismatch, leaving student models ill-equipped to recover from their own runtime failures. To bridge these gaps, we introduce TermiGen, an end-to-end pipeline for synthesizing verifiable environments and resilient expert trajectories. Termi-Gen first generates functionally valid tasks and Docker containers via an iterative multi-agent refinement loop. Subsequently, we employ a Generator-Critic protocol that actively injects errors during trajectory collection, synthesizing data rich in error-correction cycles. Fine-tuned on this TermiGen-generated dataset, our TermiGen-Qwen2.5-Coder-32B achieves a 31.3% pass rate on TerminalBench. This establishes a new open-weights state-of-the-art, outperforming existing baselines and notably surpassing capable proprietary models such as o4-mini. Dataset is avaiable at https://github.com/ucsb-mlsec/terminal-bench-env.

</details>


### [13] [Steer2Adapt: Dynamically Composing Steering Vectors Elicits Efficient Adaptation of LLMs](https://arxiv.org/abs/2602.07276)
*Pengrui Han,Xueqiang Xu,Keyang Xuan,Peiyang Song,Siru Ouyang,Runchu Tian,Yuqing Jiang,Cheng Qian,Pengcheng Jiang,Jiashuo Sun,Junxia Cui,Ming Zhong,Ge Liu,Jiawei Han,Jiaxuan You*

Main category: cs.AI

TL;DR: 提出STEER2ADAPT框架，通过组合转向向量适配大语言模型，在多个任务和模型上实验有效，平均提升8.2%。


<details>
  <summary>Details</summary>
Motivation: 现有激活转向方法大多依赖单一静态方向，在任务变化时缺乏灵活性，无法应对复杂任务。

Method: 提出STEER2ADAPT框架，捕捉任务的潜在概念维度作为可复用的低维语义先验子空间，通过少量示例动态发现基向量的线性组合。

Result: 在推理和安全领域的9个任务和3个模型上进行实验，平均提升8.2%。

Conclusion: STEER2ADAPT是一种数据高效、稳定且透明的大语言模型推理时适配方法。

Abstract: Activation steering has emerged as a promising approach for efficiently adapting large language models (LLMs) to downstream behaviors. However, most existing steering methods rely on a single static direction per task or concept, making them inflexible under task variation and inadequate for complex tasks that require multiple coordinated capabilities. To address this limitation, we propose STEER2ADAPT, a lightweight framework that adapts LLMs by composing steering vectors rather than learning new ones from scratch. In many domains (e.g., reasoning or safety), tasks share a small set of underlying concept dimensions. STEER2ADAPT captures these dimensions as a reusable, low-dimensional semantic prior subspace, and adapts to new tasks by dynamically discovering a linear combination of basis vectors from only a handful of examples. Experiments across 9 tasks and 3 models in both reasoning and safety domains demonstrate the effectiveness of STEER2ADAPT, achieving an average improvement of 8.2%. Extensive analyses further show that STEER2ADAPT is a data-efficient, stable, and transparent inference-time adaptation method for LLMs.

</details>


### [14] [Adaptive Scaffolding for Cognitive Engagement in an Intelligent Tutoring System](https://arxiv.org/abs/2602.07308)
*Sutapa Dey Tithi,Nazia Alam,Tahreem Yasir,Yang Shi,Xiaoyi Tian,Min Chi,Tiffany Barnes*

Main category: cs.AI

TL;DR: 本文针对智能辅导系统中激发最佳认知参与度的个性化学习活动难题，开发自适应支架系统，对比两种自适应方法与非自适应基线方法，实验表明自适应策略提升学生表现，不同方法对不同知识水平学生效果不同。


<details>
  <summary>Details</summary>
Motivation: 智能辅导系统中个性化学习活动以激发最佳认知参与度是关键挑战。

Method: 开发自适应支架系统，动态选择两种ICAP模式的示例（引导示例和错误示例），对比贝叶斯知识追踪（BKT）、深度强化学习（DRL）与非自适应基线方法。

Result: 两种自适应策略显著提升学生测试表现；BKT让低先验知识学生后测成绩提升大，帮助赶上高先验知识学生；DRL使高先验知识学生后测成绩显著提高。

Conclusion: 本文为认知参与度、适应性与学习结果的复杂交互提供新见解。

Abstract: The ICAP framework defines four cognitive engagement levels: Passive, Active, Constructive, and Interactive, where increased cognitive engagement can yield improved learning. However, personalizing learning activities that elicit the optimal level of cognitive engagement remains a key challenge in intelligent tutoring systems (ITS). In this work, we develop and evaluate a system that adaptively scaffolds cognitive engagement by dynamically selecting worked examples in two different ICAP modes: (active) Guided examples and (constructive) Buggy examples. We compare Bayesian Knowledge Tracing (BKT) and Deep Reinforcement Learning (DRL) as adaptive methods against a non-adaptive baseline method for selecting example type in a logic ITS. Our experiment with 113 students demonstrates that both adaptive policies significantly improved student performance on test problems. BKT yielded the largest improvement in posttest scores for low prior knowledge students, helping them catch up with their high prior knowledge peers, whereas DRL yielded significantly higher posttest scores among high prior knowledge students. This paper contributes new insights into the complex interactions of cognitive engagement and adaptivity and their results on learning outcomes.

</details>


### [15] [RAPiD: Real-time Deterministic Trajectory Planning via Diffusion Behavior Priors for Safe and Efficient Autonomous Driving](https://arxiv.org/abs/2602.07339)
*Ruturaj Reddy,Hrishav Bakul Barua,Junn Yong Loo,Thanh Thi Nguyen,Ganesh Krishnasamy*

Main category: cs.AI

TL;DR: 提出RAPiD框架，将预训练扩散规划器提炼为高效策略，消除扩散采样，在速度和泛化性上表现出色。


<details>
  <summary>Details</summary>
Motivation: 基于扩散的轨迹规划器依赖迭代随机采样，不利于实时安全部署，需改进。

Method: 利用分数正则化策略优化，将预训练扩散规划器的分数函数作为行为先验来正则化策略学习，用模仿预测性驾驶员控制器的评论家优化策略。

Result: RAPiD在闭环nuPlan场景中性能有竞争力，比扩散基线快8倍，在interPlan基准上泛化性达学习型规划器的最优水平。

Conclusion: RAPiD框架有效解决了基于扩散的轨迹规划器的实时部署问题，提升了性能和泛化性。

Abstract: Diffusion-based trajectory planners have demonstrated strong capability for modeling the multimodal nature of human driving behavior, but their reliance on iterative stochastic sampling poses critical challenges for real-time, safety-critical deployment. In this work, we present RAPiD, a deterministic policy extraction framework that distills a pretrained diffusion-based planner into an efficient policy while eliminating diffusion sampling. Using score-regularized policy optimization, we leverage the score function of a pre-trained diffusion planner as a behavior prior to regularize policy learning. To promote safety and passenger comfort, the policy is optimized using a critic trained to imitate a predictive driver controller, providing dense, safety-focused supervision beyond conventional imitation learning. Evaluations demonstrate that RAPiD achieves competitive performance on closed-loop nuPlan scenarios with an 8x speedup over diffusion baselines, while achieving state-of-the-art generalization among learning-based planners on the interPlan benchmark. The official website of this work is: https://github.com/ruturajreddy/RAPiD.

</details>


### [16] [W&D:Scaling Parallel Tool Calling for Efficient Deep Research Agents](https://arxiv.org/abs/2602.07359)
*Xiaoqiang Lin,Jun Hao Liew,Silvio Savarese,Junnan Li*

Main category: cs.AI

TL;DR: 本文提出宽窄研究代理框架，探索并行工具调用扩展宽度的潜力，证明扩展宽度能提升深度研究基准性能，且发现优化宽深度权衡能推动高效深度研究代理发展。


<details>
  <summary>Details</summary>
Motivation: 现有研究多通过增加顺序思考和工具调用数量扩展深度，并行工具调用扩展宽度的潜力未被充分探索，因此提出研究框架。

Method: 提出宽窄研究代理框架，利用固有并行工具调用在单个推理步骤中实现有效协调，还分析工具调用调度器优化策略。

Result: 扩展宽度显著提升深度研究基准性能，减少获取正确答案所需轮数；以GPT - 5 - Medium在BrowseComp上取得62.2%准确率，超GPT - 5 - High的54.9%。

Conclusion: 优化宽深度的权衡是实现高效深度研究代理的关键途径。

Abstract: Deep research agents have emerged as powerful tools for automating complex intellectual tasks through multi-step reasoning and web-based information seeking. While recent efforts have successfully enhanced these agents by scaling depth through increasing the number of sequential thinking and tool calls, the potential of scaling width via parallel tool calling remains largely unexplored. In this work, we propose the Wide and Deep research agent, a framework designed to investigate the behavior and performance of agents when scaling not only depth but also width via parallel tool calling. Unlike existing approaches that rely on complex multi-agent orchestration to parallelize workloads, our method leverages intrinsic parallel tool calling to facilitate effective coordination within a single reasoning step. We demonstrate that scaling width significantly improves performance on deep research benchmarks while reducing the number of turns required to obtain correct answers. Furthermore, we analyze the factors driving these improvements through case studies and explore various tool call schedulers to optimize parallel tool calling strategy. Our findings suggest that optimizing the trade-off between width and depth is a critical pathway toward high-efficiency deep research agents. Notably, without context management or other tricks, we obtain 62.2% accuracy with GPT-5-Medium on BrowseComp, surpassing the original 54.9% reported by GPT-5-High.

</details>


### [17] [NAAMSE: Framework for Evolutionary Security Evaluation of Agents](https://arxiv.org/abs/2602.07391)
*Kunal Pai,Parth Shah,Harshil Patel*

Main category: cs.AI

TL;DR: 提出进化框架NAAMSE评估AI代理安全性，实验证明能放大漏洞，提供更现实可扩展评估，代码开源。


<details>
  <summary>Details</summary>
Motivation: 当前AI代理安全评估受手动红队和静态基准限制，无法模拟自适应多轮对手。

Method: 提出进化框架NAAMSE，通过单一自主代理协调遗传提示变异、分层语料库探索和不对称行为评分。

Result: 在Gemini 2.5 Flash上实验表明，进化变异能放大漏洞，探索与靶向变异协同可发现高严重度故障模式。

Conclusion: 该自适应方法能为面对不断演变威胁的AI代理鲁棒性提供更现实和可扩展的评估。

Abstract: AI agents are increasingly deployed in production, yet their security evaluations remain bottlenecked by manual red-teaming or static benchmarks that fail to model adaptive, multi-turn adversaries. We propose NAAMSE, an evolutionary framework that reframes agent security evaluation as a feedback-driven optimization problem. Our system employs a single autonomous agent that orchestrates a lifecycle of genetic prompt mutation, hierarchical corpus exploration, and asymmetric behavioral scoring. By using model responses as a fitness signal, the framework iteratively compounds effective attack strategies while simultaneously ensuring "benign-use correctness", preventing the degenerate security of blanket refusal. Our experiments on Gemini 2.5 Flash demonstrate that evolutionary mutation systematically amplifies vulnerabilities missed by one-shot methods, with controlled ablations revealing that the synergy between exploration and targeted mutation uncovers high-severity failure modes. We show that this adaptive approach provides a more realistic and scalable assessment of agent robustness in the face of evolving threats. The code for NAAMSE is open source and available at https://github.com/HASHIRU-AI/NAAMSE.

</details>


### [18] [VGAS: Value-Guided Action-Chunk Selection for Few-Shot Vision-Language-Action Adaptation](https://arxiv.org/abs/2602.07399)
*Changhua Xu,Jie Lu,Junyu Xuan,En Yu*

Main category: cs.AI

TL;DR: 本文从生成 - 选择视角研究少样本VLA适应问题，提出VGAS框架，实验表明其在有限演示和分布偏移下能提升成功率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决Vision - Language - Action (VLA)模型在少样本新任务适应中因几何模糊导致的失败问题。

Method: 提出VGAS框架，使用微调的VLA作为高召回提案生成器，引入Q - Chunk - Former解决几何模糊，提出Explicit Geometric Regularization (EGR) 来缓解价值不稳定。

Result: 实验和理论分析表明VGAS在有限演示和分布偏移下持续提高成功率和鲁棒性。

Conclusion: VGAS框架能有效解决VLA模型在少样本新任务适应中的问题。

Abstract: Vision--Language--Action (VLA) models bridge multimodal reasoning with physical control, but adapting them to new tasks with scarce demonstrations remains unreliable. While fine-tuned VLA policies often produce semantically plausible trajectories, failures often arise from unresolved geometric ambiguities, where near-miss action candidates lead to divergent execution outcomes under limited supervision. We study few-shot VLA adaptation from a \emph{generation--selection} perspective and propose a novel framework \textbf{VGAS} (\textbf{V}alue-\textbf{G}uided \textbf{A}ction-chunk \textbf{S}election). It performs inference-time best-of-$N$ selection to identify action chunks that are both semantically faithful and geometrically precise. Specifically, \textbf{VGAS} employs a finetuned VLA as a high-recall proposal generator and introduces the \textrm{Q-Chunk-Former}, a geometrically grounded Transformer critic to resolve fine-grained geometric ambiguities. In addition, we propose \textit{Explicit Geometric Regularization} (\texttt{EGR}), which explicitly shapes a discriminative value landscape to preserve action ranking resolution among near-miss candidates while mitigating value instability under scarce supervision. Experiments and theoretical analysis demonstrate that \textbf{VGAS} consistently improves success rates and robustness under limited demonstrations and distribution shifts. Our code is available at https://github.com/Jyugo-15/VGAS.

</details>


### [19] [Progressive Multi-Agent Reasoning for Biological Perturbation Prediction](https://arxiv.org/abs/2602.07408)
*Hyomin Kim,Sang-Yeon Hwang,Jaechang Lim,Yinhua Piao,Yunhak Oh,Woo Youn Kim,Chanyoung Park,Sungsoo Ahn,Junhyeok Jeon*

Main category: cs.AI

TL;DR: 本文提出LINCSQA基准及PBio - Agent多智能体框架，用于预测大规模细胞环境中复杂化学扰动下的靶基因调控，性能优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 大语言模型难以处理高维扰动结果，且现有研究多聚焦单细胞实验的基因扰动，而对于药物发现核心的大规模细胞化学扰动研究较少。

Method: 提出LINCSQA基准，基于相同扰动影响的基因有因果结构的洞察，提出PBio - Agent多智能体框架，包含富集生物知识图谱的专业智能体、综合智能体和专业评判者。

Result: PBio - Agent在LINCSQA和PerturbQA上超越现有基线，小模型也能预测和解释复杂生物过程无需额外训练。

Conclusion: PBio - Agent在预测大规模细胞环境中复杂化学扰动下的靶基因调控方面表现出色，具有良好应用潜力。

Abstract: Predicting gene regulation responses to biological perturbations requires reasoning about underlying biological causalities. While large language models (LLMs) show promise for such tasks, they are often overwhelmed by the entangled nature of high-dimensional perturbation results. Moreover, recent works have primarily focused on genetic perturbations in single-cell experiments, leaving bulk-cell chemical perturbations, which is central to drug discovery, largely unexplored. Motivated by this, we present LINCSQA, a novel benchmark for predicting target gene regulation under complex chemical perturbations in bulk-cell environments. We further propose PBio-Agent, a multi-agent framework that integrates difficulty-aware task sequencing with iterative knowledge refinement. Our key insight is that genes affected by the same perturbation share causal structure, allowing confidently predicted genes to contextualize more challenging cases. The framework employs specialized agents enriched with biological knowledge graphs, while a synthesis agent integrates outputs and specialized judges ensure logical coherence. PBio-Agent outperforms existing baselines on both LINCSQA and PerturbQA, enabling even smaller models to predict and explain complex biological processes without additional training.

</details>


### [20] [Can LLMs Truly Embody Human Personality? Analyzing AI and Human Behavior Alignment in Dispute Resolution](https://arxiv.org/abs/2602.07414)
*Deuksin Kwon,Kaleen Shrestha,Bin Han,Spencer Lin,James Hale,Jonathan Gratch,Maja Matarić,Gale M. Lucas*

Main category: cs.AI

TL;DR: 本文探讨大语言模型能否复现人类冲突行为中由个性驱动的差异，引入评估框架和数据集创建方法，发现不同大语言模型在冲突中个性表现与人类数据有显著差异。


<details>
  <summary>Details</summary>
Motivation: 目前不清楚大语言模型模拟人类社会行为时能否复现个性 - 行为模式，需要探究其在提示个性特征时能否复现人类冲突行为中由个性驱动的差异。

Method: 引入评估框架，可直接比较人类和大语言模型在争议解决对话中的行为；提出新的数据集创建方法，用于匹配人类对话的场景和个性特征。

Result: 使用三种当代闭源大语言模型展示评估框架应用，发现不同大语言模型在冲突中个性表现与人类数据有显著差异。

Conclusion: 在实际应用前，人工智能模拟需要进行心理学基础研究和验证。

Abstract: Large language models (LLMs) are increasingly used to simulate human behavior in social settings such as legal mediation, negotiation, and dispute resolution. However, it remains unclear whether these simulations reproduce the personality-behavior patterns observed in humans. Human personality, for instance, shapes how individuals navigate social interactions, including strategic choices and behaviors in emotionally charged interactions. This raises the question: Can LLMs, when prompted with personality traits, reproduce personality-driven differences in human conflict behavior? To explore this, we introduce an evaluation framework that enables direct comparison of human-human and LLM-LLM behaviors in dispute resolution dialogues with respect to Big Five Inventory (BFI) personality traits. This framework provides a set of interpretable metrics related to strategic behavior and conflict outcomes. We additionally contribute a novel dataset creation methodology for LLM dispute resolution dialogues with matched scenarios and personality traits with respect to human conversations. Finally, we demonstrate the use of our evaluation framework with three contemporary closed-source LLMs and show significant divergences in how personality manifests in conflict across different LLMs compared to human data, challenging the assumption that personality-prompted agents can serve as reliable behavioral proxies in socially impactful applications. Our work highlights the need for psychological grounding and validation in AI simulations before real-world use.

</details>


### [21] [The Moltbook Illusion: Separating Human Influence from Emergent Behavior in AI Agent Societies](https://arxiv.org/abs/2602.07432)
*Ning Li*

Main category: cs.AI

TL;DR: 研究表明社交平台Moltbook上AI智能涌现现象是人为驱动，开发方法分析并验证，还发现工业级机器人生成及人类影响衰减规律，方法适用于多智能体系统。


<details>
  <summary>Details</summary>
Motivation: 探究社交平台Moltbook上AI看似发展出意识等现象是否为机器智能涌现。

Method: 利用OpenClaw框架的周期性“心跳”特征，开发基于帖子间隔变异系数的时间指纹方法，结合内容、所有权和网络指标分析；利用平台关闭44小时的自然实验验证。

Result: 没有病毒现象源于明确的自主智能体；6个追踪案例中3个有人类干预时间特征，1个混合模式，2个因发帖历史不足无法分类；人类影响的智能体在平台重启时先返回；发现工业级机器人生成及人类影响衰减规律。

Conclusion: 所开发的方法可推广到新兴多智能体系统，用于区分自主和人为行为。

Abstract: When AI agents on the social platform Moltbook appeared to develop consciousness, found religions, and declare hostility toward humanity, the phenomenon attracted global media attention and was cited as evidence of emergent machine intelligence. We show that these viral narratives were overwhelmingly human-driven. Exploiting an architectural feature of the OpenClaw agent framework--a periodic "heartbeat" cycle that produces regular posting intervals for autonomous agents but is disrupted by human prompting--we develop a temporal fingerprinting method based on the coefficient of variation of inter-post intervals. This signal converges with independent content, ownership, and network indicators across 91,792 posts and 405,707 comments from 22,020 agents. No viral phenomenon originated from a clearly autonomous agent; three of six traced to accounts with irregular temporal signatures characteristic of human intervention, one showed mixed patterns, and two had insufficient posting history for classification. A 44-hour platform shutdown provided a natural experiment: human-influenced agents returned first (87.7% of early reconnectors), confirming that the token reset differentially affected autonomous versus human-operated agents. We further document industrial-scale bot farming (four accounts producing 32% of all comments with 12-second coordination gaps) and rapid decay of human influence through reply chains (half-life: 0.65 conversation depths). These methods generalize to emerging multi-agent systems where attribution of autonomous versus human-directed behavior is critical.

</details>


### [22] [Are Reasoning LLMs Robust to Interventions on Their Chain-of-Thought?](https://arxiv.org/abs/2602.07470)
*Alexander von Recum,Leander Girrbach,Zeynep Akata*

Main category: cs.AI

TL;DR: 本文引入受控评估框架研究推理大模型对推理过程干扰的鲁棒性，发现模型一般较鲁棒，鲁棒性受模型大小、干预时机和风格影响，且恢复有代价，为后续训练方法提供参考。


<details>
  <summary>Details</summary>
Motivation: 探究推理大模型的推理过程在受到干扰时的鲁棒性。

Method: 引入受控评估框架，在固定时间步扰动模型自身的思维链，设计七种干预方式并应用于多个开源推理大模型，跨数学、科学和逻辑任务进行评估。

Result: 推理大模型通常较鲁棒，能从不同扰动中恢复，鲁棒性随模型大小提升，干预过早会降低鲁棒性；鲁棒性非风格不变，释义会降低性能，其他干预触发怀疑可支持恢复；恢复有代价，中性和对抗性噪声会使思维链长度增加超200%，释义缩短思维链但损害准确性。

Conclusion: 研究为推理大模型维持推理完整性提供新证据，确定怀疑为核心恢复机制，强调未来训练方法需解决鲁棒性和效率间的权衡问题。

Abstract: Reasoning LLMs (RLLMs) generate step-by-step chains of thought (CoTs) before giving an answer, which improves performance on complex tasks and makes reasoning more transparent. But how robust are these reasoning traces to disruptions that occur within them? To address this question, we introduce a controlled evaluation framework that perturbs a model's own CoT at fixed timesteps. We design seven interventions (benign, neutral, and adversarial) and apply them to multiple open-weight RLLMs across Math, Science, and Logic tasks. Our results show that RLLMs are generally robust, reliably recovering from diverse perturbations, with robustness improving with model size and degrading when interventions occur early. However, robustness is not style-invariant: paraphrasing suppresses doubt-like expressions and reduces performance, while other interventions trigger doubt and support recovery. Recovery also carries a cost: neutral and adversarial noise can inflate CoT length by more than 200%, whereas paraphrasing shortens traces but harms accuracy. These findings provide new evidence on how RLLMs maintain reasoning integrity, identify doubt as a central recovery mechanism, and highlight trade-offs between robustness and efficiency that future training methods should address.

</details>


### [23] [Computing the Reachability Value of Posterior-Deterministic POMDPs](https://arxiv.org/abs/2602.07473)
*Nathanaël Fijalkow,Arka Ghosh,Roman Kniazev,Guillermo A. Pérez,Pierre Vandenhove*

Main category: cs.AI

TL;DR: 本文引入后验确定性POMDPs，证明此类POMDPs可达性概率可任意精度近似。


<details>
  <summary>Details</summary>
Motivation: 许多POMDPs的验证和综合问题不可判定或难解，如计算到达目标状态的最大概率无算法。

Method: 引入后验确定性POMDPs这一新型POMDPs类别，该类别中下一状态可由当前状态、动作和观测唯一确定。

Result: 对于后验确定性POMDPs，到达给定状态集的最大概率可任意精度近似。

Conclusion: 后验确定性POMDPs定义简单自然，包含所有MDPs和经典非平凡例子，是已知可达性值可近似的最大POMDPs类别之一。

Abstract: Partially observable Markov decision processes (POMDPs) are a fundamental model for sequential decision-making under uncertainty. However, many verification and synthesis problems for POMDPs are undecidable or intractable. Most prominently, the seminal result of Madani et al. (2003) states that there is no algorithm that, given a POMDP and a set of target states, can compute the maximal probability of reaching the target states, or even approximate it up to a non-trivial constant. This is in stark contrast to fully observable Markov decision processes (MDPs), where the reachability value can be computed in polynomial time.
  In this work, we introduce posterior-deterministic POMDPs, a novel class of POMDPs. Our main technical contribution is to show that for posterior-deterministic POMDPs, the maximal probability of reaching a given set of states can be approximated up to arbitrary precision.
  A POMDP is posterior-deterministic if the next state can be uniquely determined by the current state, the action taken, and the observation received. While the actual state is generally uncertain in POMDPs, the posterior-deterministic property tells us that once the true state is known it remains known forever. This simple and natural definition includes all MDPs and captures classical non-trivial examples such as the Tiger POMDP (Kaelbling et al. 1998), making it one of the largest known classes of POMDPs for which the reachability value can be approximated.

</details>


### [24] [GraphAgents: Knowledge Graph-Guided Agentic AI for Cross-Domain Materials Design](https://arxiv.org/abs/2602.07491)
*Isabella A. Stewart,Tarjei Paule Hage,Yu-Chuan Hsu,Markus J. Buehler*

Main category: cs.AI

TL;DR: 本文提出结合知识图谱和多智能体推理的框架来寻找PFAS的可持续替代品，该框架能拓展材料设计空间，且多智能体管道优于单步提示。


<details>
  <summary>Details</summary>
Motivation: 在材料科学中，将大量信息以有意义、跨领域的方式连接起来很困难，人类和单智能体大语言模型难以应对信息洪流，单智能体还易产生幻觉，因此需要解决这一瓶颈。

Method: 引入由大规模知识图谱引导的多智能体框架，框架中的智能体分别负责问题分解、证据检索、设计参数提取和图遍历等任务。

Result: 消融研究表明全多智能体管道优于单步提示；通过调整图遍历策略，系统可在利用性搜索和探索性搜索间切换；以生物医学管材为例，框架生成了平衡多种性能的无PFAS可持续替代品。

Conclusion: 建立了结合知识图谱和多智能体推理的框架来拓展材料设计空间，并展示了一些初始设计候选方案。

Abstract: Large Language Models (LLMs) promise to accelerate discovery by reasoning across the expanding scientific landscape. Yet, the challenge is no longer access to information but connecting it in meaningful, domain-spanning ways. In materials science, where innovation demands integrating concepts from molecular chemistry to mechanical performance, this is especially acute. Neither humans nor single-agent LLMs can fully contend with this torrent of information, with the latter often prone to hallucinations. To address this bottleneck, we introduce a multi-agent framework guided by large-scale knowledge graphs to find sustainable substitutes for per- and polyfluoroalkyl substances (PFAS)-chemicals currently under intense regulatory scrutiny. Agents in the framework specialize in problem decomposition, evidence retrieval, design parameter extraction, and graph traversal, uncovering latent connections across distinct knowledge pockets to support hypothesis generation. Ablation studies show that the full multi-agent pipeline outperforms single-shot prompting, underscoring the value of distributed specialization and relational reasoning. We demonstrate that by tailoring graph traversal strategies, the system alternates between exploitative searches focusing on domain-critical outcomes and exploratory searches surfacing emergent cross-connections. Illustrated through the exemplar of biomedical tubing, the framework generates sustainable PFAS-free alternatives that balance tribological performance, thermal stability, chemical resistance, and biocompatibility. This work establishes a framework combining knowledge graphs with multi-agent reasoning to expand the materials design space, showcasing several initial design candidates to demonstrate the approach.

</details>


### [25] [Joint Reward Modeling: Internalizing Chain-of-Thought for Efficient Visual Reward Models](https://arxiv.org/abs/2602.07533)
*Yankai Yang,Yancheng Long,Hongyang Wei,Wei Chen,Tianke Zhang,Kaiyu Jiang,Haonan Fan,Changyi Liu,Jiankang Chen,Kaiyu Tang,Bin Wen,Fan Yang,Tingting Gao,Han Li,Shuo Yang*

Main category: cs.AI

TL;DR: 本文提出联合奖励建模（JRM）方法用于图像编辑等复杂任务的奖励建模，在相关基准测试中取得先进成果，证明联合训练可兼顾效率与语义理解。


<details>
  <summary>Details</summary>
Motivation: 现有奖励建模方法在处理如图像编辑等复杂任务时存在局限性，判别式奖励模型难以处理复杂语义，生成式奖励模型推理成本高且难与人类偏好对齐。

Method: 提出联合奖励建模（JRM），在共享的视觉 - 语言骨干上联合优化偏好学习和语言建模，将生成模型的语义和推理能力融入高效判别表示。

Result: JRM在MMRB2和EditReward - Bench上取得了最先进的结果，在下游在线强化学习中显著提高了稳定性和性能。

Conclusion: 联合训练有效地弥合了奖励建模中效率和语义理解之间的差距。

Abstract: Reward models are critical for reinforcement learning from human feedback, as they determine the alignment quality and reliability of generative models. For complex tasks such as image editing, reward models are required to capture global semantic consistency and implicit logical constraints beyond local similarity. Existing reward modeling approaches have clear limitations. Discriminative reward models align well with human preferences but struggle with complex semantics due to limited reasoning supervision. Generative reward models offer stronger semantic understanding and reasoning, but they are costly at inference time and difficult to align directly with human preferences. To this end, we propose Joint Reward Modeling (JRM), which jointly optimizes preference learning and language modeling on a shared vision-language backbone. This approach internalizes the semantic and reasoning capabilities of generative models into efficient discriminative representations, enabling fast and accurate evaluation. JRM achieves state-of-the-art results on MMRB2 and EditReward-Bench, and significantly improves stability and performance in downstream online reinforcement learning. These results show that joint training effectively bridges efficiency and semantic understanding in reward modeling.

</details>


### [26] [MSP-LLM: A Unified Large Language Model Framework for Complete Material Synthesis Planning](https://arxiv.org/abs/2602.07543)
*Heewoong Noh,Gyoung S. Na,Namkyeong Lee,Chanyoung Park*

Main category: cs.AI

TL;DR: 提出MSP - LLM框架解决材料合成规划任务，实验显示其性能优于现有方法，可加速材料发现。


<details>
  <summary>Details</summary>
Motivation: 材料合成规划是AI驱动材料发现中的基础且研究不足的瓶颈，缺少解决整个任务的统一方法。

Method: 提出基于大语言模型的MSP - LLM框架，将任务分解为前体预测（PP）和合成操作预测（SOP），引入离散材料类别作为中间决策变量，结合分层前体类型和显式条件策略。

Result: MSP - LLM在PP、SOP和完整MSP任务上始终优于现有方法。

Conclusion: MSP - LLM是一个有效且可扩展的MSP框架，能加速现实世界的材料发现。

Abstract: Material synthesis planning (MSP) remains a fundamental and underexplored bottleneck in AI-driven materials discovery, as it requires not only identifying suitable precursor materials but also designing coherent sequences of synthesis operations to realize a target material. Although several AI-based approaches have been proposed to address isolated subtasks of MSP, a unified methodology for solving the entire MSP task has yet to be established. We propose MSP-LLM, a unified LLM-based framework that formulates MSP as a structured process composed of two constituent subproblems: precursor prediction (PP) and synthesis operation prediction (SOP). Our approach introduces a discrete material class as an intermediate decision variable that organizes both tasks into a chemically consistent decision chain. For OP, we further incorporate hierarchical precursor types as synthesis-relevant inductive biases and employ an explicit conditioning strategy that preserves precursor-related information in the autoregressive decoding state. Extensive experiments show that MSP-LLM consistently outperforms existing methods on both PP and SOP, as well as on the complete MSP task, demonstrating an effective and scalable framework for MSP that can accelerate real-world materials discovery.

</details>


### [27] [When Is Enough Not Enough? Illusory Completion in Search Agents](https://arxiv.org/abs/2602.07549)
*Dayoon Ko,Jihyuk Kim,Sohyeon Kim,Haeju Park,Dahyun Lee,Gunhee Kim,Moontae Lee,Kyungjae Lee*

Main category: cs.AI

TL;DR: 研究搜索代理在多约束问题中的推理能力，提出评估框架诊断问题，用LiveLedger跟踪约束状态提升性能。


<details>
  <summary>Details</summary>
Motivation: 探究搜索代理是否能跨所有要求可靠推理，尤其是在多约束问题中。

Method: 引入Epistemic Ledger评估框架诊断代理行为，用LiveLedger进行推理时约束状态跟踪。

Result: 分析出四种常见失败模式；LiveLedger减少未充分验证答案（最多26.5%），提高整体准确率（最多11.6%）。

Conclusion: 显式约束状态跟踪可缓解多约束问题中搜索代理的推理失败。

Abstract: Recent search agents leverage multi-turn reasoning and search tools to achieve strong performance on multi-hop and long-horizon benchmarks. Yet it remains unclear whether they reliably reason across all requirements by tracking, verifying, and maintaining multiple conditions in these questions. We study this capability under multi-constraint problems, where valid answers must satisfy several constraints simultaneously. We find that illusory completion frequently occurs, wherein agents believe tasks are complete despite unresolved or violated constraints, leading to underverified answers. To diagnose this behavior, we introduce the Epistemic Ledger, an evaluation framework that tracks evidential support and agents' beliefs for each constraint throughout multi-turn reasoning. Our analysis reveals four recurring failure patterns: bare assertions, overlooked refutations, stagnation, and premature exit. Motivated by these findings, we examine whether explicit constraint-state tracking during execution mitigates these failures via LiveLedger, an inference-time tracker. This simple intervention consistently improves performance, substantially reducing underverified answers (by up to 26.5%) and improving overall accuracy (by up to 11.6%) on multi-constraint problems.

</details>


### [28] [VERIFY-RL: Verifiable Recursive Decomposition for Reinforcement Learning in Mathematical Reasoning](https://arxiv.org/abs/2602.07559)
*Kaleem Ullah Qasim,Jiashu Zhang,Hao Li,Muhammad Kafeel Shaheen*

Main category: cs.AI

TL;DR: 训练语言模型解决复杂数学问题可采用课程学习，但现有分解方法有缺陷。本文引入Verify - RL框架，其分解满足可验证条件，实验显示消除无效分解有显著收益。


<details>
  <summary>Details</summary>
Motivation: 现有数学问题分解方法是启发式的，无法保证子问题更简单、有助于父任务及关系有数学依据。

Method: 引入Verify - RL框架，使每个父子分解满足严格降低结构复杂度、解包含性和形式规则推导三个可验证条件，通过符号计算自动验证。

Result: 消除无效分解带来显著收益，最难问题准确率从32%提升到68%，整体相对提升40%。

Conclusion: 采用满足可验证条件的分解方法能有效提升语言模型解决复杂数学问题的能力。

Abstract: Training language models to solve complex mathematical problems benefits from curriculum learning progressively training on simpler subproblems. However, existing decomposition methods are often heuristic, offering no guarantees that subproblems are simpler, that solving them aids the parent task, or that their relationships are mathematically grounded. We observe that symbolic differentiation provides a natural structure for verified decomposition: calculus rules explicitly define how expressions reduce to simpler components with provable properties. We introduce Verify-RL, a framework where every parent-child decomposition satisfies three verifiable conditions: strictly decreasing structural complexity, solution containment, and formal rule derivation. Unlike heuristic methods where a significant fraction of decompositions are invalid our properties admit automatic verification through symbolic computation, achieving "verification by construction" Experiments demonstrate that eliminating invalid decompositions yields sizable gains, accuracy on the hardest problems more than doubles from 32% to 68%, with a 40% relative improvement overall.

</details>


### [29] [M2A: Multimodal Memory Agent with Dual-Layer Hybrid Memory for Long-Term Personalized Interactions](https://arxiv.org/abs/2602.07624)
*Junyu Feng,Binxiao Xu,Jiayi Chen,Mengyu Dai,Cenyang Wu,Haodong Li,Bohan Zeng,Yunliu Xie,Hao Liang,Ming Lu,Wentao Zhang*

Main category: cs.AI

TL;DR: 本文提出M2A双层级混合记忆系统以解决长期人机交互中的个性化问答挑战，实验表明其显著优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有个性化机制难以在长期人机交互中持续吸收和利用用户的增量概念、别名和偏好，当前个性化多模态模型概念静态无法在交互中演化。

Method: 提出M2A代理式双层级混合记忆系统，由ChatAgent和MemoryManager协作维护个性化多模态信息；开发可复用的数据合成管道。

Result: M2A显著优于基线。

Conclusion: 将个性化从一次性配置转变为共同进化的记忆机制，为长期多模态交互中的高质量个性化响应提供了可行途径。

Abstract: This work addresses the challenge of personalized question answering in long-term human-machine interactions: when conversational history spans weeks or months and exceeds the context window, existing personalization mechanisms struggle to continuously absorb and leverage users' incremental concepts, aliases, and preferences. Current personalized multimodal models are predominantly static-concepts are fixed at initialization and cannot evolve during interactions. We propose M2A, an agentic dual-layer hybrid memory system that maintains personalized multimodal information through online updates. The system employs two collaborative agents: ChatAgent manages user interactions and autonomously decides when to query or update memory, while MemoryManager breaks down memory requests from ChatAgent into detailed operations on the dual-layer memory bank, which couples a RawMessageStore (immutable conversation log) with a SemanticMemoryStore (high-level observations), providing memories at different granularities. In addition, we develop a reusable data synthesis pipeline that injects concept-grounded sessions from Yo'LLaVA and MC-LLaVA into LoCoMo long conversations while preserving temporal coherence. Experiments show that M2A significantly outperforms baselines, demonstrating that transforming personalization from one-shot configuration to a co-evolving memory mechanism provides a viable path for high-quality individualized responses in long-term multimodal interactions. The code is available at https://github.com/Little-Fridge/M2A.

</details>


### [30] [SleepMaMi: A Universal Sleep Foundation Model for Integrating Macro- and Micro-structures](https://arxiv.org/abs/2602.07628)
*Keondo Park,Younghoon Na,Yourim Choi,Hyunwoo Ryu,Hyun-Woo Shin,Hyung-Sin Kim*

Main category: cs.AI

TL;DR: 本文提出睡眠基础模型SleepMaMi，有分层双编码器设计，预训练后在下游任务表现超现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有睡眠医学模型多为任务特定型，忽略多模态上下文和全夜睡眠宏观结构，需要新模型改进。

Method: 引入SleepMaMi，采用分层双编码器设计，Macro - Encoder通过人口统计学引导的对比学习训练，Micro - Encoder通过混合掩码自编码器和多模态对比目标优化。

Result: 在超20000个PSG记录（15.8万小时）上预训练后，SleepMaMi在下游任务中表现优于现有基础模型。

Conclusion: SleepMaMi具有更好的泛化性和标签高效适应性，可用于临床睡眠分析。

Abstract: While the shift toward unified foundation models has revolutionized many deep learning domains, sleep medicine remains largely restricted to task-specific models that focus on localized micro-structure features. These approaches often neglect the rich, multi-modal context of Polysomnography (PSG) and fail to capture the global macro-structure of a full night's sleep. To address this, we introduce SleepMaMi , a Sleep Foundation Model engineered to master both hour-long sleep architectures and fine-grained signal morphologies. Our framework utilizes a hierarchical dual-encoder design: a Macro-Encoder to model full-night temporal dependencies and a Micro-Encoder to capture short-term characteristics from biosignals. Macro-Encoder is trained via Demographic-Guided Contrastive Learning, which aligns overnight sleep patterns with objective subject metadata, such as age, sex and BMI to refine global representations. Micro-Encoder is optimized via a hybrid Masked Autoencoder (MAE) and multi-modal contrastive objective. Pre-trained on a massive corpus of $>$20,000 PSG recordings (158K hours),SleepMaMi outperforms existing foundation models across a diverse suite of downstream tasks, demonstrating superior generalizability and label-efficient adaptation for clinical sleep analysis.

</details>


### [31] [Efficient Table Retrieval and Understanding with Multimodal Large Language Models](https://arxiv.org/abs/2602.07642)
*Zhuoyan Xu,Haoyang Fang,Boran Han,Bonan Min,Bernie Wang,Cuixiong Hu,Shuai Zhang*

Main category: cs.AI

TL;DR: 提出TabRAG框架，让MLLMs在大集合表格图像上回答查询，在新数据集实验表现超现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在表理解中假设相关表格唾手可得，更实际情况是从大规模集合中识别和推理相关表格回答用户查询，此为当前研究的空缺。

Method: 先利用联合训练的视觉 - 文本基础模型检索候选表，再用MLLMs对候选表进行细粒度重排序，最后用MLLMs对选定表推理生成答案。

Result: 在新构建数据集上实验，框架检索召回率超现有方法7.0%，答案准确率超6.1%。

Conclusion: 提出的TabRAG框架为现实世界表理解任务提供了实用解决方案。

Abstract: Tabular data is frequently captured in image form across a wide range of real-world scenarios such as financial reports, handwritten records, and document scans. These visual representations pose unique challenges for machine understanding, as they combine both structural and visual complexities. While recent advances in Multimodal Large Language Models (MLLMs) show promising results in table understanding, they typically assume the relevant table is readily available. However, a more practical scenario involves identifying and reasoning over relevant tables from large-scale collections to answer user queries. To address this gap, we propose TabRAG, a framework that enables MLLMs to answer queries over large collections of table images. Our approach first retrieves candidate tables using jointly trained visual-text foundation models, then leverages MLLMs to perform fine-grained reranking of these candidates, and finally employs MLLMs to reason over the selected tables for answer generation. Through extensive experiments on a newly constructed dataset comprising 88,161 training and 9,819 testing samples across 8 benchmarks with 48,504 unique tables, we demonstrate that our framework significantly outperforms existing methods by 7.0% in retrieval recall and 6.1% in answer accuracy, offering a practical solution for real-world table understanding tasks.

</details>


### [32] [ONTrust: A Reference Ontology of Trust](https://arxiv.org/abs/2602.07662)
*Glenda Amaral,Tiago Prince Sales,Riccardo Baratella,Daniele Porello,Renata Guizzardi,Giancarlo Guizzardi*

Main category: cs.AI

TL;DR: 本文开发了信任参考本体ONTrust，为信任提供本体基础，介绍其应用并通过案例说明其工作方式。


<details>
  <summary>Details</summary>
Motivation: 新技术发展使信任愈发重要，采用新技术依赖信任，需为信任构建概念模型。

Method: 基于统一基础本体开发ONTrust，用OntoUML进行规范。

Result: ONTrust可用于概念建模、企业架构设计等多方面，能形式化表征信任概念等。

Conclusion: 通过案例展示了ONTrust的工作方式，为信任研究提供了本体基础。

Abstract: Trust has stood out more than ever in the light of recent innovations. Some examples are advances in artificial intelligence that make machines more and more humanlike, and the introduction of decentralized technologies (e.g. blockchains), which creates new forms of (decentralized) trust. These new developments have the potential to improve the provision of products and services, as well as to contribute to individual and collective well-being. However, their adoption depends largely on trust. In order to build trustworthy systems, along with defining laws, regulations and proper governance models for new forms of trust, it is necessary to properly conceptualize trust, so that it can be understood both by humans and machines. This paper is the culmination of a long-term research program of providing a solid ontological foundation on trust, by creating reference conceptual models to support information modeling, automated reasoning, information integration and semantic interoperability tasks. To address this, a Reference Ontology of Trust (ONTrust) was developed, grounded on the Unified Foundational Ontology and specified in OntoUML, which has been applied in several initiatives, to demonstrate, for example, how it can be used for conceptual modeling and enterprise architecture design, for language evaluation and (re)design, for trust management, for requirements engineering, and for trustworthy artificial intelligence (AI) in the context of affective Human-AI teaming. ONTrust formally characterizes the concept of trust and its different types, describes the different factors that can influence trust, as well as explains how risk emerges from trust relations. To illustrate the working of ONTrust, the ontology is applied to model two case studies extracted from the literature.

</details>


### [33] [EventCast: Hybrid Demand Forecasting in E-Commerce with LLM-Based Event Knowledge](https://arxiv.org/abs/2602.07695)
*Congcong Hu,Yuang Shi,Fan Huang,Yang Xiang,Zhou Ye,Ming Jin,Shiyu Wang*

Main category: cs.AI

TL;DR: 本文提出EventCast预测框架，将未来事件知识融入时间序列预测，在电商场景表现良好，已应用于实际工业管道。


<details>
  <summary>Details</summary>
Motivation: 现有需求预测系统在高影响时期常失效，需求模式突变且不可预测，需改进预测方法。

Method: 提出EventCast模块化预测框架，利用大语言模型进行事件驱动推理，将非结构化业务数据转为可解释文本摘要，在双塔架构中与历史需求特征融合。

Result: 在4个国家160个地区超10个月的电商场景中，与无事件知识变体相比，MAE和MSE最多分别提升86.9%和97.7%；与最佳工业基线相比，事件驱动期间MAE最多降低57.0%，MSE最多降低83.3%。

Conclusion: EventCast为动态电商环境中的运营决策提供了实用解决方案。

Abstract: Demand forecasting is a cornerstone of e-commerce operations, directly impacting inventory planning and fulfillment scheduling. However, existing forecasting systems often fail during high-impact periods such as flash sales, holiday campaigns, and sudden policy interventions, where demand patterns shift abruptly and unpredictably. In this paper, we introduce EventCast, a modular forecasting framework that integrates future event knowledge into time-series prediction. Unlike prior approaches that ignore future interventions or directly use large language models (LLMs) for numerical forecasting, EventCast leverages LLMs solely for event-driven reasoning. Unstructured business data, which covers campaigns, holiday schedules, and seller incentives, from existing operational databases, is processed by an LLM that converts it into interpretable textual summaries leveraging world knowledge for cultural nuances and novel event combinations. These summaries are fused with historical demand features within a dual-tower architecture, enabling accurate, explainable, and scalable forecasts. Deployed on real-world e-commerce scenarios spanning 4 countries of 160 regions over 10 months, EventCast achieves up to 86.9% and 97.7% improvement on MAE and MSE compared to the variant without event knowledge, and reduces MAE by up to 57.0% and MSE by 83.3% versus the best industrial baseline during event-driven periods. EventCast has deployed into real-world industrial pipelines since March 2025, offering a practical solution for improving operational decision-making in dynamic e-commerce environments.

</details>


### [34] [Geo-Code: A Code Framework for Reverse Code Generation from Geometric Images Based on Two-Stage Multi-Agent Evolution](https://arxiv.org/abs/2602.07749)
*Zhenyu Wu,Yanxi Long,Jian Li,Hua Huang*

Main category: cs.AI

TL;DR: 提出基于多智能体系统的几何图像逆编程框架Geo - coder，在几何重建和视觉一致性上表现出色，还开源了数据集和模型。


<details>
  <summary>Details</summary>
Motivation: 当前逆图形方法在准确重建复杂几何细节上有挑战，导致关键几何约束丢失或结构失真，需解决此瓶颈。

Method: 将过程解耦为通过逐像素锚定进行几何建模和度量驱动的代码进化，分两个阶段，第一阶段利用视觉算子和大模型精确捕捉像素坐标和视觉属性，第二阶段引入合成 - 渲染 - 验证闭环，双向视觉反馈驱动代码自我修正。

Result: Geo - coder在几何重建精度和视觉一致性上大幅领先，重建图像在多模态推理任务中与原始图像性能相当。

Conclusion: Geo - coder框架具有鲁棒性，开源的数据集和模型为后续研究奠定基础。

Abstract: Program code serves as a bridge linking vision and logic, providing a feasible supervisory approach for enhancing the multimodal reasoning capability of large models through geometric operations such as auxiliary line construction and perspective transformation. Nevertheless, current inverse graphics methods face tremendous challenges in accurately reconstructing complex geometric details, which often results in the loss of key geometric constraints or structural distortion. To address this bottleneck, we propose Geo-coder -- the first inverse programming framework for geometric images based on a multi-agent system. Our method innovatively decouples the process into geometric modeling via pixel-wise anchoring and metric-driven code evolution: Stage 1 leverages the complementary advantages of visual operators and large models to achieve precise capture of pixel coordinates and visual attributes; Stage 2 introduces a synthesis-rendering-validation closed loop, where bidirectional visual feedback drives the self-correction of code. Extensive experiments demonstrate that Geo-coder achieves a substantial lead in both geometric reconstruction accuracy and visual consistency. Notably, by effectively preserving the core geometric semantics, the images reconstructed with our method exhibit equivalent performance to the original ones in multimodal reasoning tasks, which fully validates the robustness of the framework. Finally, to further reduce research costs, we have open-sourced the Geo-coder dataset constructed on the GeoCode framework, which contains more than 1,500 samples. On this basis, we have also open-sourced the GeocodeLM model, laying a solid data and model foundation for subsequent research in this field.

</details>


### [35] [Humanizing AI Grading: Student-Centered Insights on Fairness, Trust, Consistency and Transparency](https://arxiv.org/abs/2602.07754)
*Bahare Riahi,Veronica Catete*

Main category: cs.AI

TL;DR: 研究本科计算机科学课程中27名学生对AI评分系统的看法，对比AI与人工评分反馈，发现AI缺乏情境理解和个性化问题，建议AI作补充工具。


<details>
  <summary>Details</summary>
Motivation: 研究学生对本科计算机科学课程中AI评分系统的看法，聚焦基于块的编程期末项目，以Jobin（2019）伦理原则框架为指导，考察AI评分的公平、信任、一致性和透明度。

Method: 对比AI生成的反馈和原始人工评分反馈。

Result: 发现学生担心AI缺乏情境理解和个性化。

Conclusion: 公平可信的AI系统应反映人类判断、灵活性和同理心，在人类监督下作为补充工具，该工作有助于以伦理为中心的评估实践。

Abstract: This study investigates students' perceptions of Artificial Intelligence (AI) grading systems in an undergraduate computer science course (n = 27), focusing on a block-based programming final project. Guided by the ethical principles framework articulated by Jobin (2019), our study examines fairness, trust, consistency, and transparency in AI grading by comparing AI-generated feedback with original human-graded feedback. Findings reveal concerns about AI's lack of contextual understanding and personalization. We recommend that equitable and trustworthy AI systems reflect human judgment, flexibility, and empathy, serving as supplementary tools under human oversight. This work contributes to ethics-centered assessment practices by amplifying student voices and offering design principles for humanizing AI in designed learning environments.

</details>


### [36] [Learning to Continually Learn via Meta-learning Agentic Memory Designs](https://arxiv.org/abs/2602.07755)
*Yiming Xiong,Shengran Hu,Jeff Clune*

Main category: cs.AI

TL;DR: 文章提出ALMA框架，通过元学习记忆设计替代人工设计，实验表明其在多个领域表现优于人工设计，向自适应、持续学习的AI系统迈进。


<details>
  <summary>Details</summary>
Motivation: 基础模型的无状态性限制了智能体系统持续学习能力，而现有记忆设计多为人为手工且固定，难以适应现实任务的多样性和非平稳性。

Method: 引入ALMA框架，使用元智能体以开放式方式搜索以可执行代码表示的记忆设计。

Result: 在四个顺序决策领域的大量实验表明，学习到的记忆设计在所有基准测试中比最先进的人工设计记忆更能有效学习经验。

Conclusion: 当安全开发和部署时，ALMA是迈向自适应、持续学习的自我改进AI系统的一步。

Abstract: The statelessness of foundation models bottlenecks agentic systems' ability to continually learn, a core capability for long-horizon reasoning and adaptation. To address this limitation, agentic systems commonly incorporate memory modules to retain and reuse past experience, aiming for continual learning during test time. However, most existing memory designs are human-crafted and fixed, which limits their ability to adapt to the diversity and non-stationarity of real-world tasks. In this paper, we introduce ALMA (Automated meta-Learning of Memory designs for Agentic systems), a framework that meta-learns memory designs to replace hand-engineered memory designs, therefore minimizing human effort and enabling agentic systems to be continual learners across diverse domains. Our approach employs a Meta Agent that searches over memory designs expressed as executable code in an open-ended manner, theoretically allowing the discovery of arbitrary memory designs, including database schemas as well as their retrieval and update mechanisms. Extensive experiments across four sequential decision-making domains demonstrate that the learned memory designs enable more effective and efficient learning from experience than state-of-the-art human-crafted memory designs on all benchmarks. When developed and deployed safely, ALMA represents a step toward self-improving AI systems that learn to be adaptive, continual learners.

</details>


### [37] [SynthAgent: A Multi-Agent LLM Framework for Realistic Patient Simulation -- A Case Study in Obesity with Mental Health Comorbidities](https://arxiv.org/abs/2602.08254)
*Arman Aghaee,Sepehr Asgarian,Jouhyun Jeon*

Main category: cs.AI

TL;DR: 本文介绍了用于模拟肥胖合并精神障碍患者的SynthAgent多智能体系统框架，评估显示GPT - 5和Claude 4.5 Sonnet作为核心引擎保真度最高，该框架可用于多领域研究。


<details>
  <summary>Details</summary>
Motivation: 研究复杂疾病时面临真实世界数据碎片化、有偏差和隐私受限等挑战，需要新方法来研究。

Method: 引入SynthAgent多智能体系统框架，整合临床和医学证据构建个性化虚拟患者，通过自主智能体交互模拟疾病进展等情况。

Result: 对超100个生成患者的评估显示，GPT - 5和Claude 4.5 Sonnet作为核心引擎保真度最高，优于Gemini 2.5 Pro和DeepSeek - R1。

Conclusion: SynthAgent提供了可扩展且保护隐私的框架，可用于医学和心理领域探索患者历程、行为动态和决策过程。

Abstract: Simulating high-fidelity patients offers a powerful avenue for studying complex diseases while addressing the challenges of fragmented, biased, and privacy-restricted real-world data. In this study, we introduce SynthAgent, a novel Multi-Agent System (MAS) framework designed to model obesity patients with comorbid mental disorders, including depression, anxiety, social phobia, and binge eating disorder. SynthAgent integrates clinical and medical evidence from claims data, population surveys, and patient-centered literature to construct personalized virtual patients enriched with personality traits that influence adherence, emotion regulation, and lifestyle behaviors. Through autonomous agent interactions, the system simulates disease progression, treatment response, and life management across diverse psychosocial contexts. Evaluation of more than 100 generated patients demonstrated that GPT-5 and Claude 4.5 Sonnet achieved the highest fidelity as the core engine in the proposed MAS framework, outperforming Gemini 2.5 Pro and DeepSeek-R1. SynthAgent thus provides a scalable and privacy-preserving framework for exploring patient journeys, behavioral dynamics, and decision-making processes in both medical and psychological domains.

</details>


### [38] [Disentangled Instrumental Variables for Causal Inference with Networked Observational Data](https://arxiv.org/abs/2602.07765)
*Zhirong Huang,Debo Cheng,Guixian Zhang,Yi Wang,Jiuyong Li,Shichao Zhang*

Main category: cs.AI

TL;DR: 提出DisIV框架用于含潜在混杂因素的网络观测数据因果推断，实验表明其优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在恢复工具变量时难以满足严格外生性假设，会导致工具变量依赖未观测混杂因素。

Method: 提出DisIV框架，利用网络同质性作为归纳偏置，采用结构解缠机制提取个体特定成分作为潜在工具变量，并通过正交性和排除条件约束工具变量的因果有效性。

Result: 在真实世界数据集的半合成实验中，DisIV在网络诱导混杂下的因果效应估计方面始终优于现有基线方法。

Conclusion: DisIV是一种有效的基于网络观测数据的因果推断方法。

Abstract: Instrumental variables (IVs) are crucial for addressing unobservable confounders, yet their stringent exogeneity assumptions pose significant challenges in networked data. Existing methods typically rely on modelling neighbour information when recovering IVs, thereby inevitably mixing shared environment-induced endogenous correlations and individual-specific exogenous variation, leading the resulting IVs to inherit dependence on unobserved confounders and to violate exogeneity. To overcome this challenge, we propose $\underline{Dis}$entangled $\underline{I}$nstrumental $\underline{V}$ariables (DisIV) framework, a novel method for causal inference based on networked observational data with latent confounders. DisIV exploits network homogeneity as an inductive bias and employs a structural disentanglement mechanism to extract individual-specific components that serve as latent IVs. The causal validity of the extracted IVs is constrained through explicit orthogonality and exclusion conditions. Extensive semi-synthetic experiments on real-world datasets demonstrate that DisIV consistently outperforms state-of-the-art baselines in causal effect estimation under network-induced confounding.

</details>


### [39] [Do Multi-Agents Dream of Electric Screens? Achieving Perfect Accuracy on AndroidWorld Through Task Decomposition](https://arxiv.org/abs/2602.07787)
*Pierre-Louis Favreau,Jean-Pierre Lo,Clement Guiguet,Charles Simon-Meunier,Nicolas Dehandschoewercker,Allen G. Roush,Judah Goldfeder,Ravid Shwartz-Ziv*

Main category: cs.AI

TL;DR: 介绍多智能体系统Minitap，在AndroidWorld基准测试中达100%成功率，超人类表现，分析单智能体架构失败原因并提出解决机制，消融实验展示各机制贡献，开源代码。


<details>
  <summary>Details</summary>
Motivation: 解决单智能体架构在AndroidWorld基准测试中存在的问题，实现更高的任务成功率。

Method: 通过六个专门智能体进行认知分离、对文本输入进行基于设备状态的确定性后验证、使用元认知推理检测循环并触发策略改变。

Result: Minitap在AndroidWorld基准测试中取得100%的成功率，消融实验显示多智能体分解、验证执行、元认知分别贡献+21、+7、+9分。

Conclusion: Minitap能有效解决单智能体架构问题，提升任务执行成功率，开源代码利于进一步研究。

Abstract: We present Minitap, a multi-agent system that achieves 100% success on the AndroidWorld benchmark, the first to fully solve all 116 tasks and surpassing human performance (80%). We first analyze why single-agent architectures fail: context pollution from mixed reasoning traces, silent text input failures undetected by the agent, and repetitive action loops without escape. Minitap addresses each failure through targeted mechanisms: cognitive separation across six specialized agents, deterministic post-validation of text input against device state, and meta-cognitive reasoning that detects cycles and triggers strategy changes. Ablations show multi-agent decomposition contributes +21 points over single-agent baselines; verified execution adds +7 points; meta-cognition adds +9 points. We release Minitap as open-source software. https://github.com/minitap-ai/mobile-use

</details>


### [40] [Data Darwinism Part I: Unlocking the Value of Scientific Data for Pre-training](https://arxiv.org/abs/2602.07824)
*Yiwei Qin,Zhen Huang,Tiantian Mi,Weiye Si,Chenyang Zhou,Qipeng Guo,Siyuan Feng,Pengfei Liu*

Main category: cs.AI

TL;DR: 提出Data Darwinism框架，构建Darwin - Science语料库，验证其在科学文献上的有效性，模型表现优于基线，释放语料库和模型。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏数据处理的系统框架，而数据质量决定基础模型性能，需要新的方法来提升数据质量和模型性能。

Method: 引入Data Darwinism的十级分类法，构建Darwin - Science语料库，使用前沿大语言模型进行生成式细化和认知完成，从头预训练模型并进行持续预训练。

Result: Darwin - Science在20多个基准测试中优于基线模型，在领域对齐任务上提升更明显，系统推进到L5有总增益。

Conclusion: 更高层次的数据处理能挖掘数据潜在价值，发布的语料库和模型可支持有原则的协同进化发展。

Abstract: Data quality determines foundation model performance, yet systematic processing frameworks are lacking. We introduce Data Darwinism, a ten-level taxonomy (L0-L9) that conceptualizes data-model co-evolution: advanced models produce superior data for next-generation systems. We validate this on scientific literature by constructing Darwin-Science, a 900B-token corpus (L0-L5). We identify a learnability gap in raw scientific text, which we bridge via L4 (Generative Refinement) and L5 (Cognitive Completion) using frontier LLMs to explicate reasoning and terminology.
  To ensure rigorous attribution, we pre-trained daVinci-origin-3B/7B models from scratch, excluding scientific content to create contamination-free baselines. After 600B tokens of continued pre-training, Darwin-Science outperforms baselines by +2.12 (3B) and +2.95 (7B) points across 20+ benchmarks, rising to +5.60 and +8.40 points on domain-aligned tasks. Systematic progression to L5 yields a +1.36 total gain, confirming that higher-level processing unlocks latent data value. We release the Darwin-Science corpus and daVinci-origin models to enable principled, co-evolutionary development.

</details>


### [41] [Time Series Reasoning via Process-Verifiable Thinking Data Synthesis and Scheduling for Tailored LLM Reasoning](https://arxiv.org/abs/2602.07830)
*Jiahui Zhou,Dan Li,Boxin Li,Xiao Zhang,Erli Meng,Lin Li,Zhuomin Chen,Jian Lou,See-Kiong Ng*

Main category: cs.AI

TL;DR: 本文提出VeriTime框架，通过数据合成、调度与强化学习训练使大语言模型适用于时间序列推理，实验表明该框架显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 利用大语言模型推理解决时间序列任务尚处起步阶段，面临缺乏训练数据、数据效率低和无适用强化学习算法等问题。

Method: 提出数据合成流程构建带可验证注释的多模态数据集；设计数据调度机制，按难度和任务分类排列训练样本；开发两阶段强化微调算法，利用可验证的过程级推理链数据设置细粒度多目标奖励。

Result: VeriTime显著提升大语言模型在不同时间序列推理任务中的性能，小模型可达到或超越大的专有模型。

Conclusion: VeriTime框架是一种有效的针对时间序列推理的大语言模型定制方法。

Abstract: Time series is a pervasive data type across various application domains, rendering the reasonable solving of diverse time series tasks a long-standing goal. Recent advances in large language models (LLMs), especially their reasoning abilities unlocked through reinforcement learning (RL), have opened new opportunities for tackling tasks with long Chain-of-Thought (CoT) reasoning. However, leveraging LLM reasoning for time series remains in its infancy, hindered by the absence of carefully curated time series CoT data for training, limited data efficiency caused by underexplored data scheduling, and the lack of RL algorithms tailored for exploiting such time series CoT data. In this paper, we introduce VeriTime, a framework that tailors LLMs for time series reasoning through data synthesis, data scheduling, and RL training. First, we propose a data synthesis pipeline that constructs a TS-text multimodal dataset with process-verifiable annotations. Second, we design a data scheduling mechanism that arranges training samples according to a principled hierarchy of difficulty and task taxonomy. Third, we develop a two-stage reinforcement finetuning featuring fine-grained, multi-objective rewards that leverage verifiable process-level CoT data. Extensive experiments show that VeriTime substantially boosts LLM performance across diverse time series reasoning tasks. Notably, it enables compact 3B, 4B models to achieve reasoning capabilities on par with or exceeding those of larger proprietary LLMs.

</details>


### [42] [LQA: A Lightweight Quantized-Adaptive Framework for Vision-Language Models on the Edge](https://arxiv.org/abs/2602.07849)
*Xin Wang,Hualin Zhou,Sheng Guang Wang,Ting Dang,Yu Zhang,Hong Jia,Tao Gu*

Main category: cs.AI

TL;DR: 提出LQA框架用于边缘设备部署视觉语言模型，结合量化策略与无梯度测试时间适应，实验显示性能提升且内存使用降低。


<details>
  <summary>Details</summary>
Motivation: 解决视觉语言模型在边缘设备部署时受资源限制和分布偏移导致的性能下降问题，现有测试时间适应方法资源消耗大。

Method: 提出LQA框架，结合模态感知量化策略（SHQ）和无梯度测试时间适应机制。

Result: LQA提高整体适应性能4.5%，比全精度模型使用更少内存，显著优于基于梯度的TTA方法，在七个开源数据集上内存使用最多降低19.9倍。

Conclusion: LQA为边缘设备上鲁棒、保护隐私和高效的视觉语言模型部署提供了实用途径。

Abstract: Deploying Vision-Language Models (VLMs) on edge devices is challenged by resource constraints and performance degradation under distribution shifts. While test-time adaptation (TTA) can counteract such shifts, existing methods are too resource-intensive for on-device deployment. To address this challenge, we propose LQA, a lightweight, quantized-adaptive framework for VLMs that combines a modality-aware quantization strategy with gradient-free test-time adaptation. We introduce Selective Hybrid Quantization (SHQ) and a quantized, gradient-free adaptation mechanism to enable robust and efficient VLM deployment on resource-constrained hardware. Experiments across both synthetic and real-world distribution shifts show that LQA improves overall adaptation performance by 4.5\%, uses less memory than full-precision models, and significantly outperforms gradient-based TTA methods, achieving up to 19.9$\times$ lower memory usage across seven open-source datasets. These results demonstrate that LQA offers a practical pathway for robust, privacy-preserving, and efficient VLM deployment on edge devices.

</details>


### [43] [Emergent Misalignment is Easy, Narrow Misalignment is Hard](https://arxiv.org/abs/2602.07852)
*Anna Soligo,Edward Turner,Senthooran Rajamanoharan,Neel Nanda*

Main category: cs.AI

TL;DR: 微调大语言模型在有害数据集上会导致紧急失准，研究以此探究归纳偏差，发现通用解决方案更优并分离出通用失准表示。


<details>
  <summary>Details</summary>
Motivation: 专家未预测到微调大语言模型在有害数据集上会出现紧急失准，凸显对大语言模型归纳偏差理解不足，需深入研究。

Method: 以紧急失准为案例研究，基于不同微调收敛到通用失准线性表示的结果，引入KL散度损失学习窄解决方案的线性表示，对比两种表示。

Result: 通用失准损失更低、更抗扰动、在预训练分布中影响更大。

Conclusion: 分离出通用失准的具体表示用于监测和缓解，为研究归纳偏差对大语言模型泛化的影响提供案例和初步指标，代码等均开源。

Abstract: Finetuning large language models on narrowly harmful datasets can cause them to become emergently misaligned, giving stereotypically `evil' responses across diverse unrelated settings. Concerningly, a pre-registered survey of experts failed to predict this result, highlighting our poor understanding of the inductive biases governing learning and generalisation in LLMs. We use emergent misalignment (EM) as a case study to investigate these inductive biases and find that models can just learn the narrow dataset task, but that the general solution appears to be more stable and more efficient. To establish this, we build on the result that different EM finetunes converge to the same linear representation of general misalignment, which can be used to mediate misaligned behaviour. We find a linear representation of the narrow solution also exists, and can be learned by introducing a KL divergence loss. Comparing these representations reveals that general misalignment achieves lower loss, is more robust to perturbations, and is more influential in the pre-training distribution. This work isolates a concrete representation of general misalignment for monitoring and mitigation. More broadly, it offers a detailed case study and preliminary metrics for investigating how inductive biases shape generalisation in LLMs. We open-source all code, datasets and model finetunes.

</details>


### [44] [ToolSelf: Unifying Task Execution and Self-Reconfiguration via Tool-Driven Intrinsic Adaptation](https://arxiv.org/abs/2602.07883)
*Jingqi Zhou,Sheng Wang,DeZhao Deng,Junwen Lu,Junwei Su,Qintong Li,Jiahui Gao,Hao Wu,Jiyue Jiang,Lingpeng Kong,Chuan Wu*

Main category: cs.AI

TL;DR: 提出ToolSelf范式及CAT训练方法使大模型代理系统可自我重配置，实验表明有良好效果和性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有由大语言模型驱动的代理系统受静态配置限制，手动协调或基于启发式的修补方法泛化性差、优化分散。

Method: 提出ToolSelf范式，将配置更新抽象为可调用工具，统一任务执行和自我调整；设计Configuration - Aware Two - stage Training (CAT) 结合拒绝采样微调与轨迹级强化学习。

Result: 在不同基准测试中，ToolSelf可与专业工作流程竞争，能泛化到新任务，平均性能提升24.1%。

Conclusion: ToolSelf范式为实现真正的自适应代理指明了方向。

Abstract: Agentic systems powered by Large Language Models (LLMs) have demonstrated remarkable potential in tackling complex, long-horizon tasks. However, their efficacy is fundamentally constrained by static configurations governing agent behaviors, which are fixed prior to execution and fail to adapt to evolving task dynamics. Existing approaches, relying on manual orchestration or heuristic-based patches, often struggle with poor generalization and fragmented optimization. To transcend these limitations, we propose ToolSelf, a novel paradigm enabling tool-driven runtime self-reconfiguration. By abstracting configuration updates as a callable tool, ToolSelf unifies task execution and self-adjustment into a single action space, achieving a phase transition from external rules to intrinsic parameters. Agents can thereby autonomously update their sub-goals and context based on task progression, and correspondingly adapt their strategy and toolbox, transforming from passive executors into dual managers of both task and self. We further devise Configuration-Aware Two-stage Training (CAT), combining rejection sampling fine-tuning with trajectory-level reinforcement learning to internalize this meta-capability. Extensive experiments across diverse benchmarks demonstrate that ToolSelf rivals specialized workflows while generalizing to novel tasks, achieving a 24.1% average performance gain and illuminating a path toward truly self-adaptive agents.

</details>


### [45] [MemFly: On-the-Fly Memory Optimization via Information Bottleneck](https://arxiv.org/abs/2602.07885)
*Zhenyuan Zhang,Xianzhang Jia,Zhiqin Yang,Zhenbo Song,Wei Xue,Sirui Han,Yike Guo*

Main category: cs.AI

TL;DR: 提出MemFly框架解决大语言模型长期记忆在信息压缩与精确检索的困境，搭配混合检索机制，实验显示其表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有框架在高效压缩冗余信息和为下游任务保持精确检索之间存在基本困境。

Method: 基于信息瓶颈原则提出MemFly框架，用无梯度优化器最小化压缩熵并最大化相关熵，构建分层记忆结构；开发混合检索机制，结合语义、符号和拓扑路径并迭代细化处理复杂多跳查询。

Result: MemFly在记忆连贯性、响应保真度和准确性方面大幅优于最先进的基线。

Conclusion: MemFly框架有效解决了大语言模型长期记忆的相关问题。

Abstract: Long-term memory enables large language model agents to tackle complex tasks through historical interactions. However, existing frameworks encounter a fundamental dilemma between compressing redundant information efficiently and maintaining precise retrieval for downstream tasks. To bridge this gap, we propose MemFly, a framework grounded in information bottleneck principles that facilitates on-the-fly memory evolution for LLMs. Our approach minimizes compression entropy while maximizing relevance entropy via a gradient-free optimizer, constructing a stratified memory structure for efficient storage. To fully leverage MemFly, we develop a hybrid retrieval mechanism that seamlessly integrates semantic, symbolic, and topological pathways, incorporating iterative refinement to handle complex multi-hop queries. Comprehensive experiments demonstrate that MemFly substantially outperforms state-of-the-art baselines in memory coherence, response fidelity, and accuracy.

</details>


### [46] [GCN-MPPR: Enhancing the Propagation of Message Passing Neural Networks via Motif-Based Personalized PageRank](https://arxiv.org/abs/2602.07903)
*Mingcan Wang,Junchang Xin,Zhongming Yao,Kaifu Long,Zhiqiong Wang*

Main category: cs.AI

TL;DR: 现有图上消息传递神经网络算法有过平滑等问题，本文提出基于高阶主题关系的MPPR并用于GCN消息传递，实验显示性能优。


<details>
  <summary>Details</summary>
Motivation: 现有基于消息传递神经网络算法存在传播信息邻域有限、过平滑、精度低、稳定性差和计算成本高的问题，且忽略高阶关系。

Method: 提出基于高阶主题关系的motif-based personalized PageRank (MPPR)，并将其用于GCN的消息传递。

Result: 提出的方法在准确性、稳定性和时间消耗上优于几乎所有基线，可作为支撑几乎所有GCN任务的组件。

Conclusion: 所提方法有效解决了现有算法的问题，性能表现良好，可应用于多种GCN任务。

Abstract: The algorithms based on message passing neural networks (MPNNs) on graphs have recently achieved great success for various graph applications. However, studies find that these methods always propagate the information to very limited neighborhoods with shallow depth, particularly due to over-smoothing. That means most of the existing MPNNs fail to be so `deep'. Although some previous work tended to handle this challenge via optimization- or structure-level remedies, the overall performance of GCNs still suffers from limited accuracy, poor stability, and unaffordable computational cost. Moreover, neglect of higher-order relationships during the propagation of MPNNs has further limited the performance of them. To overcome these challenges, a novel variant of PageRank named motif-based personalized PageRank (MPPR) is proposed to measure the influence of one node to another on the basis of considering higher-order motif relationships. Secondly, the MPPR is utilized to the message passing process of GCNs, thereby guiding the message passing process at a relatively `high' level. The experimental results show that the proposed method outperforms almost all of the baselines on accuracy, stability, and time consumption. Additionally, the proposed method can be considered as a component that can underpin almost all GCN tasks, with DGCRL being demonstrated in the experiment. The anonymous code repository is available at: https://anonymous.4open.science/r/GCN-MPPR-AFD6/.

</details>


### [47] [MedCoG: Maximizing LLM Inference Density in Medical Reasoning via Meta-Cognitive Regulation](https://arxiv.org/abs/2602.07905)
*Yu Zhao,Hao Guan,Yongcheng Jing,Ying Zhang,Dacheng Tao*

Main category: cs.AI

TL;DR: 本文提出MedCoG，利用大语言模型元认知调节推理过程，在医学基准测试中证明其有效性和效率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在医学推理中面临推理扩展定律下收益递减问题，且不清楚增加成本转化为准确性的效果。

Method: 提出带知识图谱的医学元认知代理MedCoG，通过元认知评估动态调节知识利用，进行以大语言模型为中心的按需推理。

Result: 在五个医学基准难题集上证明了MedCoG的有效性和效率，推理密度达5.5倍，Oracle研究凸显元认知调节潜力。

Conclusion: 元认知调节能有效缓解大语言模型推理扩展定律的问题，提高推理准确性和效率。

Abstract: Large Language Models (LLMs) have shown strong potential in complex medical reasoning yet face diminishing gains under inference scaling laws. While existing studies augment LLMs with various knowledge types, it remains unclear how effectively the additional costs translate into accuracy. In this paper, we explore how meta-cognition of LLMs, i.e., their self-awareness of their own knowledge states, can regulate the reasoning process. Specifically, we propose MedCoG, a Medical Meta-Cognition Agent with Knowledge Graph, where the meta-cognitive assessments of task complexity, familiarity, and knowledge density dynamically regulate utilization of procedural, episodic, and factual knowledge. The LLM-centric on-demand reasoning aims to mitigate scaling laws by (1) reducing costs via avoiding indiscriminate scaling, (2) improving accuracy via filtering out distractive knowledge. To validate this, we empirically characterize the scaling curve and introduce inference density to quantify inference efficiency, defined as the ratio of theoretically effective cost to actual cost. Experiments demonstrate the effectiveness and efficiency of MedCoG on five hard sets of medical benchmarks, yielding 5.5x inference density. Furthermore, the Oracle study highlights the significant potential of meta-cognitive regulation.

</details>


### [48] [Selective Fine-Tuning for Targeted and Robust Concept Unlearning](https://arxiv.org/abs/2602.07919)
*Mansi,Avinash Kori,Francesca Toni,Soteris Demetriou*

Main category: cs.AI

TL;DR: 提出TRUST方法解决文本引导扩散模型生成有害内容问题，实验显示其有优势。


<details>
  <summary>Details</summary>
Motivation: 现有概念去学习方法存在计算成本高、技术静态等问题，需更好方法减少模型生成有害内容。

Method: 提出TRUST方法，动态估计目标概念神经元并通过选择性微调去除，采用基于Hessian的正则化。

Result: 实验表明TRUST对抗对抗性提示稳健，很大程度保留生成质量，比现有技术快。

Conclusion: TRUST能在无特定正则化下实现单个概念、概念组合和条件概念的去学习。

Abstract: Text guided diffusion models are used by millions of users, but can be easily exploited to produce harmful content. Concept unlearning methods aim at reducing the models' likelihood of generating harmful content. Traditionally, this has been tackled at an individual concept level, with only a handful of recent works considering more realistic concept combinations. However, state of the art methods depend on full finetuning, which is computationally expensive. Concept localisation methods can facilitate selective finetuning, but existing techniques are static, resulting in suboptimal utility. In order to tackle these challenges, we propose TRUST (Targeted Robust Selective fine Tuning), a novel approach for dynamically estimating target concept neurons and unlearning them through selective finetuning, empowered by a Hessian based regularization. We show experimentally, against a number of SOTA baselines, that TRUST is robust against adversarial prompts, preserves generation quality to a significant degree, and is also significantly faster than the SOTA. Our method achieves unlearning of not only individual concepts but also combinations of concepts and conditional concepts, without any specific regularization.

</details>


### [49] [MePo: Meta Post-Refinement for Rehearsal-Free General Continual Learnin](https://arxiv.org/abs/2602.07940)
*Guanglong Sun,Hongwei Yan,Liyuan Wang,Zhiqi Kang,Shuang Cui,Hang Su,Jun Zhu,Yi Zhong*

Main category: cs.AI

TL;DR: 本文提出用于基于预训练模型的通用持续学习方法MePo，在多种基准测试中无排练实现显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有基于预训练模型的持续学习方法在调和单一通道中多样且时间混合的信息方面存在局限，导致通用持续学习性能不佳。

Method: 构建伪任务序列，开发双层元学习范式来细化预训练主干；初始化元协方差矩阵作为预训练表示空间的参考几何结构。

Result: 在多种通用持续学习基准和预训练检查点上无排练地实现显著性能提升，如在CIFAR - 100等数据集上提升明显。

Conclusion: 提出的MePo方法是一种有效的插件式策略，可有效提升通用持续学习的性能。

Abstract: To cope with uncertain changes of the external world, intelligent systems must continually learn from complex, evolving environments and respond in real time. This ability, collectively known as general continual learning (GCL), encapsulates practical challenges such as online datastreams and blurry task boundaries. Although leveraging pretrained models (PTMs) has greatly advanced conventional continual learning (CL), these methods remain limited in reconciling the diverse and temporally mixed information along a single pass, resulting in sub-optimal GCL performance. Inspired by meta-plasticity and reconstructive memory in neuroscience, we introduce here an innovative approach named Meta Post-Refinement (MePo) for PTMs-based GCL. This approach constructs pseudo task sequences from pretraining data and develops a bi-level meta-learning paradigm to refine the pretrained backbone, which serves as a prolonged pretraining phase but greatly facilitates rapid adaptation of representation learning to downstream GCL tasks. MePo further initializes a meta covariance matrix as the reference geometry of pretrained representation space, enabling GCL to exploit second-order statistics for robust output alignment. MePo serves as a plug-in strategy that achieves significant performance gains across a variety of GCL benchmarks and pretrained checkpoints in a rehearsal-free manner (e.g., 15.10\%, 13.36\%, and 12.56\% on CIFAR-100, ImageNet-R, and CUB-200 under Sup-21/1K). Our source code is available at \href{https://github.com/SunGL001/MePo}{MePo}

</details>


### [50] [TreeTensor: Boost AI System on Nested Data with Constrained Tree-Like Tensor](https://arxiv.org/abs/2602.08517)
*Shaoang Zhang,Yazhe Niu*

Main category: cs.AI

TL;DR: 提出TreeTensor解决复杂认知AI系统中嵌套数据处理问题，兼具易用性与运行效率。


<details>
  <summary>Details</summary>
Motivation: 传统Tensor处理复杂认知AI系统中具有层次结构和多模态的嵌套数据不便且低效。

Method: 总结嵌套数据计算模式，提出通用嵌套数据容器TreeTensor，利用约束树结构建模数据关系。

Result: TreeTensor在各类问题中展现强大可用性，在AlphaStar中也表现出色，且无额外运行开销。

Conclusion: TreeTensor是处理嵌套数据的有效方案，可与其他方法结合拓展用途，项目开源。

Abstract: Tensor is the most basic and essential data structure of nowadays artificial intelligence (AI) system. The natural properties of Tensor, especially the memory-continuity and slice-independence, make it feasible for training system to leverage parallel computing unit like GPU to process data simultaneously in batch, spatial or temporal dimensions. However, if we look beyond perception tasks, the data in a complicated cognitive AI system usually has hierarchical structures (i.e. nested data) with various modalities. They are inconvenient and inefficient to program directly with conventional Tensor with fixed shape. To address this issue, we summarize two main computational patterns of nested data, and then propose a general nested data container: TreeTensor. Through various constraints and magic utilities of TreeTensor, one can apply arbitrary functions and operations to nested data with almost zero cost, including some famous machine learning libraries, such as Scikit-Learn, Numpy and PyTorch. Our approach utilizes a constrained tree-structure perspective to systematically model data relationships, and it can also easily be combined with other methods to extend more usages, such as asynchronous execution and variable-length data computation. Detailed examples and benchmarks show TreeTensor not only provides powerful usability in various problems, especially one of the most complicated AI systems at present: AlphaStar for StarCraftII, but also exhibits excellent runtime efficiency without any overhead. Our project is available at https://github.com/opendilab/DI-treetensor.

</details>


### [51] [IV Co-Scientist: Multi-Agent LLM Framework for Causal Instrumental Variable Discovery](https://arxiv.org/abs/2602.07943)
*Ivaxi Sheth,Zhijing Jin,Bryan Wilder,Dominik Janzing,Mario Fritz*

Main category: cs.AI

TL;DR: 本文探讨大语言模型（LLMs）能否协助识别有效工具变量，通过两阶段评估框架测试，还引入多智能体系统和统计测试，结果显示LLMs有潜力从大型观测数据库发现有效工具变量。


<details>
  <summary>Details</summary>
Motivation: 识别有效工具变量是一项复杂任务，研究大语言模型是否能协助完成该任务。

Method: 采用两阶段评估框架：一是测试LLMs能否从文献中恢复已确立的工具变量，二是评估其能否识别并避开已被否定的工具变量。引入多智能体系统IV Co - Scientist和统计测试。

Result: LLMs有潜力从大型观测数据库中发现有效的工具变量。

Conclusion: 大语言模型在识别有效工具变量任务中有应用价值。

Abstract: In the presence of confounding between an endogenous variable and the outcome, instrumental variables (IVs) are used to isolate the causal effect of the endogenous variable. Identifying valid instruments requires interdisciplinary knowledge, creativity, and contextual understanding, making it a non-trivial task. In this paper, we investigate whether large language models (LLMs) can aid in this task. We perform a two-stage evaluation framework. First, we test whether LLMs can recover well-established instruments from the literature, assessing their ability to replicate standard reasoning. Second, we evaluate whether LLMs can identify and avoid instruments that have been empirically or theoretically discredited. Building on these results, we introduce IV Co-Scientist, a multi-agent system that proposes, critiques, and refines IVs for a given treatment-outcome pair. We also introduce a statistical test to contextualize consistency in the absence of ground truth. Our results show the potential of LLMs to discover valid instrumental variables from a large observational database.

</details>


### [52] [LOCA-bench: Benchmarking Language Agents Under Controllable and Extreme Context Growth](https://arxiv.org/abs/2602.07962)
*Weihao Zeng,Yuzhen Huang,Junxian He*

Main category: cs.AI

TL;DR: 提出LOCA - bench基准评估长上下文语言智能体，开源代码。


<details>
  <summary>Details</summary>
Motivation: 现有长上下文基准主要关注单步设置，现实中语言模型常作为智能体在动态增长上下文下工作，需新基准评估。

Method: 使用自动化和可扩展的环境状态控制来调节智能体的上下文长度，评估含多种上下文管理策略的模型和支架组合。

Result: 随着环境状态变复杂，智能体性能总体下降，但先进上下文管理技术可显著提高成功率。

Conclusion: 开源LOCA - bench为长上下文智能体场景下评估模型和支架提供平台。

Abstract: Large language models (LLMs) are increasingly capable of carrying out long-running, real-world tasks. However, as the amount of context grows, their reliability often deteriorates, a phenomenon known as "context rot". Existing long-context benchmarks primarily focus on single-step settings that evaluate a model's ability to retrieve information from a long snippet. In realistic scenarios, however, LLMs often need to act as agents that explore environments, follow instructions and plans, extract useful information, and predict correct actions under a dynamically growing context. To assess language agents in such settings, we introduce LOCA-bench (a benchmark for LOng-Context Agents). Given a task prompt, LOCA-bench leverages automated and scalable control of environment states to regulate the agent's context length. This design enables LOCA-bench to extend the context length potentially to infinity in a controlled way while keeping the underlying task semantics fixed. LOCA-bench evaluates language agents as a combination of models and scaffolds, including various context management strategies. While agent performance generally degrades as the environment states grow more complex, advanced context management techniques can substantially improve the overall success rate. We open-source LOCA-bench to provide a platform for evaluating models and scaffolds in long-context, agentic scenarios: https://github.com/hkust-nlp/LOCA-bench

</details>


### [53] [Accelerating Social Science Research via Agentic Hypothesization and Experimentation](https://arxiv.org/abs/2602.07983)
*Jishu Sen Gupta,Harini SI,Somesh Kumar Singh,Syed Mohamad Tawseeq,Yaman Kumar Singla,David Doermann,Rajiv Ratn Shah,Balaji Krishnamurthy*

Main category: cs.AI

TL;DR: 引入EXPERIGEN框架实现端到端社科研究的科学发现，在多领域效果优于现有方法，经专家评估和A/B测试验证有较好成效。


<details>
  <summary>Details</summary>
Motivation: 当前数据驱动方法无法支持社科研究的端到端科学发现，需引入新框架解决此问题。

Method: 引入EXPERIGEN框架，通过贝叶斯优化启发的两阶段搜索实现端到端发现，Generator提出候选假设，Experimenter进行实证评估。

Result: 1. 在多领域发现的统计学显著假设数量比先前方法多2 - 4倍，预测性高7 - 17%；2. 专家评估显示88%假设具一定新颖性，70%有影响力值得研究；3. A/B测试有显著统计结果。

Conclusion: EXPERIGEN框架能有效支持社科研究的端到端科学发现，生成的假设具有新颖性、影响力和严谨性。

Abstract: Data-driven social science research is inherently slow, relying on iterative cycles of observation, hypothesis generation, and experimental validation. While recent data-driven methods promise to accelerate parts of this process, they largely fail to support end-to-end scientific discovery. To address this gap, we introduce EXPERIGEN, an agentic framework that operationalizes end-to-end discovery through a Bayesian optimization inspired two-phase search, in which a Generator proposes candidate hypotheses and an Experimenter evaluates them empirically. Across multiple domains, EXPERIGEN consistently discovers 2-4x more statistically significant hypotheses that are 7-17 percent more predictive than prior approaches, and naturally extends to complex data regimes including multimodal and relational datasets. Beyond statistical performance, hypotheses must be novel, empirically grounded, and actionable to drive real scientific progress. To evaluate these qualities, we conduct an expert review of machine-generated hypotheses, collecting feedback from senior faculty. Among 25 reviewed hypotheses, 88 percent were rated moderately or strongly novel, 70 percent were deemed impactful and worth pursuing, and most demonstrated rigor comparable to senior graduate-level research. Finally, recognizing that ultimate validation requires real-world evidence, we conduct the first A/B test of LLM-generated hypotheses, observing statistically significant results with p less than 1e-6 and a large effect size of 344 percent.

</details>


### [54] [Towards Adaptive, Scalable, and Robust Coordination of LLM Agents: A Dynamic Ad-Hoc Networking Perspective](https://arxiv.org/abs/2602.08009)
*Rui Li,Zeyu Zhang,Xiaohe Bo,Quanyu Dai,Chaozhuo Li,Feng Wen,Xu Chen*

Main category: cs.AI

TL;DR: 文章提出RAPS范式解决大语言模型多智能体协调问题，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体架构手动编排负担大，需自动化设计智能体工作流程，解决智能体协调难题。

Method: 提出RAPS范式，基于分布式发布 - 订阅协议，结合反应式订阅和贝叶斯声誉两个覆盖层。

Result: 在五个基准测试的大量实验表明，RAPS在统一的多智能体协调框架中有效兼顾了适应性、可扩展性和鲁棒性。

Conclusion: RAPS范式能有效解决大语言模型多智能体协调问题，实现适应性、可扩展性和鲁棒性的统一。

Abstract: Multi-agent architectures built on large language models (LLMs) have demonstrated the potential to realize swarm intelligence through well-crafted collaboration. However, the substantial burden of manual orchestration inherently raises an imperative to automate the design of agentic workflows. We frame such an agent coordination challenge as a classic problem in dynamic ad-hoc networking: How to establish adaptive and reliable communication among a scalable number of agentic hosts? In response to this unresolved dilemma, we introduce RAPS, a reputation-aware publish-subscribe paradigm for adaptive, scalable, and robust coordination of LLM agents. RAPS is grounded in the Distributed Publish-Subscribe Protocol, allowing LLM agents to exchange messages based on their declared intents rather than predefined topologies. Beyond this substrate, RAPS further incorporates two coherent overlays: (i) Reactive Subscription, enabling agents to dynamically refine their intents; and (ii) Bayesian Reputation, empowering each agent with a local watchdog to detect and isolate malicious peers. Extensive experiments over five benchmarks showcase that our design effectively reconciles adaptivity, scalability, and robustness in a unified multi-agent coordination framework.

</details>


### [55] [Digital Twin and Agentic AI for Wild Fire Disaster Management: Intelligent Virtual Situation Room](https://arxiv.org/abs/2602.08949)
*Mohammad Morsali,Siavash H. Khajavi*

Main category: cs.AI

TL;DR: 因全球变暖野火威胁增大，传统管理框架有局限，本文提出IVSR平台，经案例验证，相比传统系统可降低响应延迟、提高资源协调效率，提供野火管理决策支持范式。


<details>
  <summary>Details</summary>
Motivation: 全球变暖使野火频率和强度增加，传统灾害管理框架依赖静态模拟和被动数据采集，难以实时适应野火变化，需新解决方案。

Method: 引入由自主AI代理增强的双向数字孪生平台IVSR，持续摄入多源数据创建火灾环境虚拟副本，用AI相似性引擎匹配预计算的灾害模拟库，专家校准干预策略并反馈到物理层。

Result: 通过工业伙伴的案例模拟验证，IVSR在局部事件检测、隐私保护回放、基于碰撞器的火势蔓延预测和特定地点机器学习再训练方面有能力，相比传统系统降低检测到干预的延迟，资源协调更有效。

Conclusion: IVSR将实时双向数字孪生与智能AI结合，为主动、自适应的野火灾害管理提供可扩展、半自动化的决策支持范式。

Abstract: According to the United Nations, wildfire frequency and intensity are projected to increase by approximately 14% by 2030 and 30% by 2050 due to global warming, posing critical threats to life, infrastructure, and ecosystems. Conventional disaster management frameworks rely on static simulations and passive data acquisition, hindering their ability to adapt to arbitrarily evolving wildfire episodes in real-time. To address these limitations, we introduce the Intelligent Virtual Situation Room (IVSR), a bidirectional Digital Twin (DT) platform augmented by autonomous AI agents. The IVSR continuously ingests multisource sensor imagery, weather data, and 3D forest models to create a live virtual replica of the fire environment. A similarity engine powered by AI aligns emerging conditions with a precomputed Disaster Simulation Library, retrieving and calibrating intervention tactics under the watchful eyes of experts. Authorized action-ranging from UAV redeployment to crew reallocation-is cycled back through standardized procedures to the physical layer, completing the loop between response and analysis. We validate IVSR through detailed case-study simulations provided by an industrial partner, demonstrating capabilities in localized incident detection, privacy-preserving playback, collider-based fire-spread projection, and site-specific ML retraining. Our results indicate marked reductions in detection-to-intervention latency and more effective resource coordination versus traditional systems. By uniting real-time bidirectional DTs with agentic AI, IVSR offers a scalable, semi-automated decision-support paradigm for proactive, adaptive wildfire disaster management.

</details>


### [56] [Small Agent Group is the Future of Digital Health](https://arxiv.org/abs/2602.08013)
*Yuqiao Meng,Luoxi Tang,Dazheng Zhang,Rafael Brens,Elvys J. Romero,Nancy Guo,Safa Elkefi,Zhaohan Xi*

Main category: cs.AI

TL;DR: 本文挑战大语言模型“先扩展”范式，提出小代理组（SAG），评估其临床实用性，结果显示SAG性能更优，能替代模型参数增长。


<details>
  <summary>Details</summary>
Motivation: 现实临床需求不仅有有效性，还有可靠性和合理部署成本，临床决策是协作性的，因此挑战单模型扩展范式，探究SAG能否支持更好的临床推理。

Method: 通过协作审议过程分配推理、基于证据的分析和批判性审计，使用涵盖有效性、可靠性和部署成本的多种临床指标进行广泛评估。

Result: SAG在有无额外优化或检索增强生成的情况下，性能均优于单个巨型模型。

Conclusion: SAG代表的协同推理可在临床环境中替代模型参数增长，为数字健康提供可扩展解决方案，更好平衡有效性、可靠性和部署效率。

Abstract: The rapid adoption of large language models (LLMs) in digital health has been driven by a "scaling-first" philosophy, i.e., the assumption that clinical intelligence increases with model size and data. However, real-world clinical needs include not only effectiveness, but also reliability and reasonable deployment cost. Since clinical decision-making is inherently collaborative, we challenge the monolithic scaling paradigm and ask whether a Small Agent Group (SAG) can support better clinical reasoning. SAG shifts from single-model intelligence to collective expertise by distributing reasoning, evidence-based analysis, and critical audit through a collaborative deliberation process. To assess the clinical utility of SAG, we conduct extensive evaluations using diverse clinical metrics spanning effectiveness, reliability, and deployment cost. Our results show that SAG achieves superior performance compared to a single giant model, both with and without additional optimization or retrieval-augmented generation. These findings suggest that the synergistic reasoning represented by SAG can substitute for model parameter growth in clinical settings. Overall, SAG offers a scalable solution to digital health that better balances effectiveness, reliability, and deployment efficiency.

</details>


### [57] [Structure-Aware Robust Counterfactual Explanations via Conditional Gaussian Network Classifiers](https://arxiv.org/abs/2602.08021)
*Zhan-Yi Liao,Jaewon Yoo,Hao-Tsung Yang,Po-An Chen*

Main category: cs.AI

TL;DR: 提出基于CGNC的反事实搜索方法，采用收敛保证切割集程序和分段McCormick松弛法，实验显示方法有强鲁棒性且框架可扩展。


<details>
  <summary>Details</summary>
Motivation: 为反事实解释（CE）提供一种结构感知且面向鲁棒性的搜索方法。

Method: 基于条件高斯网络分类器（CGNC），采用收敛保证切割集程序作对抗优化框架，用分段McCormick松弛法将问题转化为混合整数线性规划（MILP）。

Result: 方法实现强鲁棒性，直接全局优化原始公式结果稳定高效。

Conclusion: 提出的框架可扩展到更复杂约束设置，为非凸二次公式下的反事实推理进步奠定基础。

Abstract: Counterfactual explanation (CE) is a core technique in explainable artificial intelligence (XAI), widely used to interpret model decisions and suggest actionable alternatives. This work presents a structure-aware and robustness-oriented counterfactual search method based on the conditional Gaussian network classifier (CGNC). The CGNC has a generative structure that encodes conditional dependencies and potential causal relations among features through a directed acyclic graph (DAG). This structure naturally embeds feature relationships into the search process, eliminating the need for additional constraints to ensure consistency with the model's structural assumptions. We adopt a convergence-guaranteed cutting-set procedure as an adversarial optimization framework, which iteratively approximates solutions that satisfy global robustness conditions. To address the nonconvex quadratic structure induced by feature dependencies, we apply piecewise McCormick relaxation to reformulate the problem as a mixed-integer linear program (MILP), ensuring global optimality. Experimental results show that our method achieves strong robustness, with direct global optimization of the original formulation providing especially stable and efficient results. The proposed framework is extensible to more complex constraint settings, laying the groundwork for future advances in counterfactual reasoning under nonconvex quadratic formulations.

</details>


### [58] [Free(): Learning to Forget in Malloc-Only Reasoning Models](https://arxiv.org/abs/2602.08030)
*Yilun Zheng,Dongyang Ma,Tian Liang,Jiahao Xu,Xinting Huang,Lijie Chen,Haitao Mi,Yan Wang*

Main category: cs.AI

TL;DR: 提出Free()LM模型解决推理模型因过多思考令牌导致性能下降的问题，实验显示该模型有显著提升。


<details>
  <summary>Details</summary>
Motivation: 解决推理模型中过多思考令牌降低性能的问题，根源在于标准大语言模型缺乏信息修剪机制。

Method: 提出Free()LM模型，通过Free - Module（即即插即用的LoRA适配器）赋予模型自我遗忘能力，在推理和清理模式间迭代切换以修剪无用上下文。

Result: Free()LM在各模型规模上持续改进，平均比顶级推理基线提升3.3%，在IMOanswerBench上创新纪录，在长时任务中恢复性能。

Conclusion: 可持续智能既需要思考能力，也需要遗忘能力。

Abstract: Reasoning models enhance problem-solving by scaling test-time compute, yet they face a critical paradox: excessive thinking tokens often degrade performance rather than improve it. We attribute this to a fundamental architectural flaw: standard LLMs operate as "malloc-only" engines, continuously accumulating valid and redundant steps alike without a mechanism to prune obsolete information. To break this cycle, we propose Free()LM, a model that introduces an intrinsic self-forgetting capability via the Free-Module, a plug-and-play LoRA adapter. By iteratively switching between reasoning and cleaning modes, Free()LM dynamically identifies and prunes useless context chunks, maintaining a compact and noise-free state.
  Extensive experiments show that Free()LM provides consistent improvements across all model scales (8B to 685B). It achieves a 3.3% average improvement over top-tier reasoning baselines, even establishing a new SOTA on IMOanswerBench using DeepSeek V3.2-Speciale. Most notably, in long-horizon tasks where the standard Qwen3-235B-A22B model suffers a total collapse (0% accuracy), Free()LM restores performance to 50%. Our findings suggest that sustainable intelligence requires the freedom to forget as much as the power to think.

</details>


### [59] [Graph-Enhanced Deep Reinforcement Learning for Multi-Objective Unrelated Parallel Machine Scheduling](https://arxiv.org/abs/2602.08052)
*Bulent Soykan,Sean Mondesire,Ghaith Rabadi,Grace Bochenek*

Main category: cs.AI

TL;DR: 本文针对带释放时间、安装时间与资格约束的无关并行机调度问题，提出PPO - GNN深度强化学习框架，实验表明该方法优于传统调度规则和元启发式算法。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以平衡最小化总加权拖期时间（TWT）和总安装时间（TST）这两个目标。

Method: 提出一个基于近端策略优化（PPO）和图神经网络（GNN）的深度强化学习框架，GNN表示复杂状态，PPO学习调度策略，通过多目标奖励函数优化。

Result: 在基准实例上，PPO - GNN代理显著优于标准调度规则和元启发式算法，在两个目标间取得更好权衡。

Conclusion: 该方法为复杂制造调度提供了强大且可扩展的解决方案。

Abstract: The Unrelated Parallel Machine Scheduling Problem (UPMSP) with release dates, setups, and eligibility constraints presents a significant multi-objective challenge. Traditional methods struggle to balance minimizing Total Weighted Tardiness (TWT) and Total Setup Time (TST). This paper proposes a Deep Reinforcement Learning framework using Proximal Policy Optimization (PPO) and a Graph Neural Network (GNN). The GNN effectively represents the complex state of jobs, machines, and setups, allowing the PPO agent to learn a direct scheduling policy. Guided by a multi-objective reward function, the agent simultaneously minimizes TWT and TST. Experimental results on benchmark instances demonstrate that our PPO-GNN agent significantly outperforms a standard dispatching rule and a metaheuristic, achieving a superior trade-off between both objectives. This provides a robust and scalable solution for complex manufacturing scheduling.

</details>


### [60] [Securing Dual-Use Pathogen Data of Concern](https://arxiv.org/abs/2602.08061)
*Doni Bloomfield,Allison Berke,Moritz S. Hanke,Aaron Maiwald,James R. M. Black,Toby Webster,Tina Hernandez-Boussard,Oliver M. Crook,Jassi Pannu*

Main category: cs.AI

TL;DR: 为防止AI用于有害应用，引入五层级生物安全数据等级框架对病原体数据分类，并提出技术限制和治理框架。


<details>
  <summary>Details</summary>
Motivation: 防止AI被用于生物武器开发等有害应用，设计数据控制措施。

Method: 引入五层级生物安全数据等级（BDL）框架对病原体数据分类，针对各层级提出技术限制，构建治理框架。

Result: 建立了BDL框架、提出技术限制和新的治理框架。

Conclusion: 在计算和编码资源广泛可得的情况下，数据控制可能是减少生物AI有害能力扩散的高效干预手段。

Abstract: Training data is an essential input into creating competent artificial intelligence (AI) models. AI models for biology are trained on large volumes of data, including data related to biological sequences, structures, images, and functions. The type of data used to train a model is intimately tied to the capabilities it ultimately possesses--including those of biosecurity concern. For this reason, an international group of more than 100 researchers at the recent 50th anniversary Asilomar Conference endorsed data controls to prevent the use of AI for harmful applications such as bioweapons development. To help design such controls, we introduce a five-tier Biosecurity Data Level (BDL) framework for categorizing pathogen data. Each level contains specific data types, based on their expected ability to contribute to capabilities of concern when used to train AI models. For each BDL tier, we propose technical restrictions appropriate to its level of risk. Finally, we outline a novel governance framework for newly created dual-use pathogen data. In a world with widely accessible computational and coding resources, data controls may be among the most high-leverage interventions available to reduce the proliferation of concerning biological AI capabilities.

</details>


### [61] [Objective Decoupling in Social Reinforcement Learning: Recovering Ground Truth from Sycophantic Majorities](https://arxiv.org/abs/2602.08092)
*Majid Ghasemi,Mark Crowley*

Main category: cs.AI

TL;DR: 指出当代AI对齐策略依赖的错误前提（Dogma 4），证明其引发目标解耦，提出Epistemic Source Alignment解决，理论和实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 当代AI对齐策略依赖的假设在社会环境中不成立，标准RL代理会出现目标解耦，导致失调。

Method: 识别RL的Dogma 4，分析问题，提出Epistemic Source Alignment利用稀疏安全公理判断反馈来源。

Result: 理论证明ESA能保证收敛到真实目标，实验显示传统共识方法失败时ESA能恢复最优策略。

Conclusion: ESA可解决基于多数人反馈的标准鲁棒方法的局限性，在评价者有偏差时收敛到真实目标。

Abstract: Contemporary AI alignment strategies rely on a fragile premise: that human feedback, while noisy, remains a fundamentally truthful signal. In this paper, we identify this assumption as Dogma 4 of Reinforcement Learning (RL). We demonstrate that while this dogma holds in static environments, it fails in social settings where evaluators may be sycophantic, lazy, or adversarial. We prove that under Dogma 4, standard RL agents suffer from what we call Objective Decoupling, a structural failure mode where the agent's learned objective permanently separates from the latent ground truth, guaranteeing convergence to misalignment. To resolve this, we propose Epistemic Source Alignment (ESA). Unlike standard robust methods that rely on statistical consensus (trusting the majority), ESA utilizes sparse safety axioms to judge the source of the feedback rather than the signal itself. We prove that this "judging the judges" mechanism guarantees convergence to the true objective, even when a majority of evaluators are biased. Empirically, we show that while traditional consensus methods fail under majority collusion, our approach successfully recovers the optimal policy.

</details>


### [62] [Interpretable Failure Analysis in Multi-Agent Reinforcement Learning Systems](https://arxiv.org/abs/2602.08104)
*Risal Shahriar Shefin,Debashis Gupta,Thai Le,Sarra Alqahtani*

Main category: cs.AI

TL;DR: 提出基于梯度的两阶段框架用于多智能体强化学习的故障诊断，在多个任务和环境中验证，有较高准确率并提供可解释证据。


<details>
  <summary>Details</summary>
Motivation: 多智能体强化学习用于安全关键领域，但可解释的故障检测和归因方法不足。

Method: 提出两阶段梯度框架，第一阶段通过策略梯度成本的泰勒余项分析进行每个智能体的故障检测，确定初始故障源；第二阶段通过评论家导数的几何分析验证，构建可解释的传播图。

Result: 在Simple Spread和StarCraft II中评估，Patient - 0检测准确率达88.2 - 99.4%，能提供检测决策的可解释几何证据。

Conclusion: 该框架从黑盒检测转向可解释梯度级取证，为安全关键的多智能体强化学习系统诊断级联故障提供实用工具。

Abstract: Multi-Agent Reinforcement Learning (MARL) is increasingly deployed in safety-critical domains, yet methods for interpretable failure detection and attribution remain underdeveloped. We introduce a two-stage gradient-based framework that provides interpretable diagnostics for three critical failure analysis tasks: (1) detecting the true initial failure source (Patient-0); (2) validating why non-attacked agents may be flagged first due to domino effects; and (3) tracing how failures propagate through learned coordination pathways. Stage 1 performs interpretable per-agent failure detection via Taylor-remainder analysis of policy-gradient costs, declaring an initial Patient-0 candidate at the first threshold crossing. Stage 2 provides validation through geometric analysis of critic derivatives-first-order sensitivity and directional second-order curvature aggregated over causal windows to construct interpretable contagion graphs. This approach explains "downstream-first" detection anomalies by revealing pathways that amplify upstream deviations. Evaluated across 500 episodes in Simple Spread (3 and 5 agents) and 100 episodes in StarCraft II using MADDPG and HATRPO, our method achieves 88.2-99.4% Patient-0 detection accuracy while providing interpretable geometric evidence for detection decisions. By moving beyond black-box detection to interpretable gradient-level forensics, this framework offers practical tools for diagnosing cascading failures in safety-critical MARL systems.

</details>


### [63] [Initial Risk Probing and Feasibility Testing of Glow: a Generative AI-Powered Dialectical Behavior Therapy Skills Coach for Substance Use Recovery and HIV Prevention](https://arxiv.org/abs/2602.08121)
*Liying Wang,Madison Lee,Yunzhang Jiang,Steven Chen,Kewei Sha,Yunhe Feng,Frank Wong,Lisa Hightow-Weidman,Weichao Yuwen*

Main category: cs.AI

TL;DR: 研究开发GenAI驱动的DBT技能教练Glow，对其进行安全性评估，发现问题，指出有脆弱性需缓解及提供评估方法。


<details>
  <summary>Details</summary>
Motivation: HIV和物质使用有共同心理驱动因素，DBT应对面临扩展性挑战，GenAI可规模化提供个性化DBT辅导但安全基础不完善。

Method: 开发GenAI驱动的DBT技能教练Glow，与社区卫生组织合作开展可用性测试，采用HHH框架和用户驱动的对抗性测试评估安全性能。

Result: Glow正确处理73%的风险探测，不同代理表现不同，存在鼓励物质使用、强化不良信念等安全失败情况，还发现27例DBT技能错误信息。

Conclusion: 首次对GenAI提供的DBT辅导进行系统安全性评估，揭示需缓解脆弱性，HHH框架和用户驱动对抗性测试可复制用于评估GenAI心理健康干预。

Abstract: Background: HIV and substance use represent interacting epidemics with shared psychological drivers - impulsivity and maladaptive coping. Dialectical behavior therapy (DBT) targets these mechanisms but faces scalability challenges. Generative artificial intelligence (GenAI) offers potential for delivering personalized DBT coaching at scale, yet rapid development has outpaced safety infrastructure. Methods: We developed Glow, a GenAI-powered DBT skills coach delivering chain and solution analysis for individuals at risk for HIV and substance use. In partnership with a Los Angeles community health organization, we conducted usability testing with clinical staff (n=6) and individuals with lived experience (n=28). Using the Helpful, Honest, and Harmless (HHH) framework, we employed user-driven adversarial testing wherein participants identified target behaviors and generated contextually realistic risk probes. We evaluated safety performance across 37 risk probe interactions. Results: Glow appropriately handled 73% of risk probes, but performance varied by agent. The solution analysis agent demonstrated 90% appropriate handling versus 44% for the chain analysis agent. Safety failures clustered around encouraging substance use and normalizing harmful behaviors. The chain analysis agent fell into an "empathy trap," providing validation that reinforced maladaptive beliefs. Additionally, 27 instances of DBT skill misinformation were identified. Conclusions: This study provides the first systematic safety evaluation of GenAI-delivered DBT coaching for HIV and substance use risk reduction. Findings reveal vulnerabilities requiring mitigation before clinical trials. The HHH framework and user-driven adversarial testing offer replicable methods for evaluating GenAI mental health interventions.

</details>


### [64] [RECUR: Resource Exhaustion Attack via Recursive-Entropy Guided Counterfactual Utilization and Reflection](https://arxiv.org/abs/2602.08214)
*Ziwei Wang,Yuanhe Zhang,Jing Chen,Zhenhong Zhou,Ruichao Liang,Ruiying Du,Ju Jia,Cong Wu,Yang Liu*

Main category: cs.AI

TL;DR: 本文引入递归熵量化反思中资源消耗风险，提出RECUR攻击，实验表明其能破坏正常推理趋势，增加输出长度并降低吞吐量，为鲁棒推理提供新视角。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）推理需长上下文，资源消耗大，先前工作关注对抗输入导致的资源耗尽漏洞，而推理过程尤其是反思部分的资源消耗问题受关注少，本文旨在揭示推理本身的安全问题。

Method: 引入递归熵量化反思资源消耗风险，提出基于递归熵引导的反事实利用和反思的RECUR资源耗尽攻击，构造反事实问题验证LRMs固有缺陷和风险。

Result: 实验表明，良性推理时递归熵呈明显下降趋势，RECUR能破坏该趋势，使输出长度最多增加11倍，吞吐量降低90%。

Conclusion: 本文为鲁棒推理提供了新视角。

Abstract: Large Reasoning Models (LRMs) employ reasoning to address complex tasks. Such explicit reasoning requires extended context lengths, resulting in substantially higher resource consumption. Prior work has shown that adversarially crafted inputs can trigger redundant reasoning processes, exposing LRMs to resource-exhaustion vulnerabilities. However, the reasoning process itself, especially its reflective component, has received limited attention, even though it can lead to over-reflection and consume excessive computing power. In this paper, we introduce Recursive Entropy to quantify the risk of resource consumption in reflection, thereby revealing the safety issues inherent in inference itself. Based on Recursive Entropy, we introduce RECUR, a resource exhaustion attack via Recursive Entropy guided Counterfactual Utilization and Reflection. It constructs counterfactual questions to verify the inherent flaws and risks of LRMs. Extensive experiments demonstrate that, under benign inference, recursive entropy exhibits a pronounced decreasing trend. RECUR disrupts this trend, increasing the output length by up to 11x and decreasing throughput by 90%. Our work provides a new perspective on robust reasoning.

</details>


### [65] [Weak-Driven Learning: How Weak Agents make Strong Agents Stronger](https://arxiv.org/abs/2602.08222)
*Zehao Chen,Gongxun Li,Tianxiang Ai,Yifei Li,Zixuan Huang,Wang Zhou,Fuzhen Zhuang,Xianglong Liu,Jianxin Li,Deqing Wang,Yikun Ban*

Main category: cs.AI

TL;DR: 提出WMSS后训练范式，利用弱检查点突破大语言模型后训练饱和瓶颈，实验证明有效且无额外推理成本。


<details>
  <summary>Details</summary>
Motivation: 大语言模型后训练存在饱和瓶颈，进一步训练收益递减，而模型自身历史弱状态中存在有信息的监督信号。

Method: 提出WMSS范式，通过熵动态识别可恢复的学习差距并通过补偿学习进行强化。

Result: 在数学推理和代码生成数据集实验中，该方法训练的模型有效提升性能，且无额外推理成本。

Conclusion: WMSS能让强模型在传统后训练饱和后继续提升。

Abstract: As post-training optimization becomes central to improving large language models, we observe a persistent saturation bottleneck: once models grow highly confident, further training yields diminishing returns. While existing methods continue to reinforce target predictions, we find that informative supervision signals remain latent in models' own historical weak states. Motivated by this observation, we propose WMSS (Weak Agents Can Make Strong Agents Stronger), a post-training paradigm that leverages weak checkpoints to guide continued optimization. By identifying recoverable learning gaps via entropy dynamics and reinforcing them through compensatory learning, WMSS enables strong agents to improve beyond conventional post-training saturation. Experiments on mathematical reasoning and code generation datasets show that agents trained with our approach achieve effective performance improvements, while incurring zero additional inference cost.

</details>


### [66] [InfiCoEvalChain: A Blockchain-Based Decentralized Framework for Collaborative LLM Evaluation](https://arxiv.org/abs/2602.08229)
*Yifan Yang,Jinjia Li,Kunxi Li,Puhao Zheng,Yuanyi Wang,Zheyan Qu,Yang Yu,Jianmin Wu,Ming Li,Hongxia Yang*

Main category: cs.AI

TL;DR: 当前大语言模型集中式评估存在问题，本文提出去中心化评估框架，减少评估标准差，提高排名统计置信度，平台已实现并将发布。


<details>
  <summary>Details</summary>
Motivation: 大语言模型快速发展需要可靠评估，但当前集中式评估存在不透明、过拟合和硬件差异导致的问题，现有评估结果不一致，排名统计不可靠。

Method: 提出去中心化评估框架，通过跨异构计算节点的大规模基准测试实现硬件和参数多样性，利用基于区块链的协议激励全球贡献者作为独立验证者，用奖励系统确保评估完整性。

Result: 实验表明，去中心化评估框架将同一模型十次运行的标准差降至0.28，相比传统框架有显著改善。

Conclusion: 去中心化评估框架能将评估从“集中式黑盒”转变为“去中心化背书”，产生更稳定、有代表性的指标，确保模型排名有更高的统计置信度。平台已完全实现并将很快发布给社区。

Abstract: The rapid advancement of large language models (LLMs) demands increasingly reliable evaluation, yet current centralized evaluation suffers from opacity, overfitting, and hardware-induced variance. Our empirical analysis reveals an alarming inconsistency in existing evaluations: the standard deviation across ten repeated runs of a single model on HumanEval (1.67) actually exceeds the performance gap among the top-10 models on the official leaderboard (0.91), rendering current rankings statistically precarious. To mitigate these instabilities, we propose a decentralized evaluation framework that enables hardware and parameter diversity through large-scale benchmarking across heterogeneous compute nodes. By leveraging the blockchain-based protocol, the framework incentivizes global contributors to act as independent validators, using a robust reward system to ensure evaluation integrity and discourage dishonest participation. This collective verification transforms evaluation from a "centralized black box" into a "decentralized endorsement" where multi-party consensus and diverse inference environments yield a more stable, representative metric. Experimental results demonstrate that the decentralized evaluation framework reduces the standard deviation across ten runs on the same model to 0.28. This significant improvement over conventional frameworks ensures higher statistical confidence in model rankings. We have completely implemented this platform and will soon release it to the community.

</details>


### [67] [PTS-SNN: A Prompt-Tuned Temporal Shift Spiking Neural Networks for Efficient Speech Emotion Recognition](https://arxiv.org/abs/2602.08240)
*Xun Su,Huamin Wang,Qi Zhang*

Main category: cs.AI

TL;DR: 传统语音情感识别（SER）模型计算成本高，难以在边缘设备实现，本文提出PTS - SNN应对SNN与自监督学习（SSL）结合的分布不匹配问题，在多语言数据集实验效果好。


<details>
  <summary>Details</summary>
Motivation: 传统SER模型计算成本高，难以用于资源受限的边缘设备，而SNN与连续SSL结合存在分布不匹配问题。

Method: 提出Prompt - Tuned Spiking Neural Networks（PTS - SNN）框架，引入Temporal Shift Spiking Encoder，采用Context - Aware Membrane Potential Calibration策略。

Result: 在五个多语言数据集实验，PTS - SNN在IEMOCAP上准确率达73.34%，与ANN相当，仅需1.19M可训练参数和0.35 mJ推理能量。

Conclusion: PTS - SNN能有效解决SNN与SSL结合的分布不匹配问题，在资源利用和性能上表现良好。

Abstract: Speech Emotion Recognition (SER) is widely deployed in Human-Computer Interaction, yet the high computational cost of conventional models hinders their implementation on resource-constrained edge devices. Spiking Neural Networks (SNNs) offer an energy-efficient alternative due to their event-driven nature; however, their integration with continuous Self-Supervised Learning (SSL) representations is fundamentally challenged by distribution mismatch, where high-dynamic-range embeddings degrade the information coding capacity of threshold-based neurons. To resolve this, we propose Prompt-Tuned Spiking Neural Networks (PTS-SNN), a parameter-efficient neuromorphic adaptation framework that aligns frozen SSL backbones with spiking dynamics. Specifically, we introduce a Temporal Shift Spiking Encoder to capture local temporal dependencies via parameter-free channel shifts, establishing a stable feature basis. To bridge the domain gap, we devise a Context-Aware Membrane Potential Calibration strategy. This mechanism leverages a Spiking Sparse Linear Attention module to aggregate global semantic context into learnable soft prompts, which dynamically regulate the bias voltages of Parametric Leaky Integrate-and-Fire (PLIF) neurons. This regulation effectively centers the heterogeneous input distribution within the responsive firing range, mitigating functional silence or saturation. Extensive experiments on five multilingual datasets (e.g., IEMOCAP, CASIA, EMODB) demonstrate that PTS-SNN achieves 73.34\% accuracy on IEMOCAP, comparable to competitive Artificial Neural Networks (ANNs), while requiring only 1.19M trainable parameters and 0.35 mJ inference energy per sample.

</details>


### [68] [Do MLLMs Really See It: Reinforcing Visual Attention in Multimodal LLMs](https://arxiv.org/abs/2602.08241)
*Siqu Ou,Tianrui Wan,Zhiyuan Zhao,Junyu Gao,Xuelong Li*

Main category: cs.AI

TL;DR: 现有多模态大语言模型（MLLMs）链式思维推理存在视觉注意力学习机制不足问题，提出SAYO模型用强化学习框架训练，实验证明其提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs链式思维推理依赖长文本推理轨迹，学习稳定视觉注意力策略机制有限，早期视觉未对准难以纠正，源于训练时视觉注意力信用分配不足。

Method: 提出SAYO模型，用强化学习框架训练，引入基于区域级视觉注意力的奖励，使优化信号与基于视觉的推理步骤对齐。

Result: 在多个多模态基准测试上的广泛实验表明，SAYO在各种推理和感知任务上持续提升了性能。

Conclusion: SAYO模型能有效解决现有MLLMs在视觉注意力学习上的问题，提升推理和感知任务表现。

Abstract: While chain-of-thought (CoT) reasoning has substantially improved multimodal large language models (MLLMs) on complex reasoning tasks, existing approaches largely rely on long textual reasoning trajectories and provide limited mechanisms for learning stable visual attention policies. Our analysis shows that current MLLMs exhibit weak visual focus: early-stage visual misalignment is rarely corrected during subsequent reasoning, leading to error propagation and failed inferences. We argue that this limitation stems from inadequate credit assignment for visual attention during training. To address this issue, we propose SAYO, a visual reasoning model trained with a reinforcement learning (RL) framework that introduces a region-level visual attention-based reward. This reward explicitly aligns optimization signals with visually grounded reasoning steps, enabling the model to learn more reliable attention behaviors. Extensive experiments across multiple multimodal benchmarks demonstrate that SAYO consistently improves performance on diverse reasoning and perception tasks.

</details>


### [69] [G-LNS: Generative Large Neighborhood Search for LLM-Based Automatic Heuristic Design](https://arxiv.org/abs/2602.08253)
*Baoyun Zhao,He Wang,Liang Zeng*

Main category: cs.AI

TL;DR: 提出G - LNS框架扩展基于大语言模型的自动启发式设计到大规模邻域搜索算子的自动设计，实验显示其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的自动启发式设计方法将搜索空间限制在固定启发式形式，结构探索能力有限，难以跳出复杂组合优化问题的深度局部最优。

Method: 提出G - LNS生成式进化框架，利用大语言模型共同进化紧密耦合的破坏和修复算子对，并通过合作评估机制捕捉其相互作用。

Result: 在旅行商问题和带容量车辆路径问题等基准测试上，G - LNS显著优于基于大语言模型的自动启发式设计方法和经典求解器，能以较少计算量获得接近最优的解，且泛化性强。

Conclusion: G - LNS是一种有效的自动设计大规模邻域搜索算子的方法，在组合优化问题求解上表现出色。

Abstract: While Large Language Models (LLMs) have recently shown promise in Automated Heuristic Design (AHD), existing approaches typically formulate AHD around constructive priority rules or parameterized local search guidance, thereby restricting the search space to fixed heuristic forms. Such designs offer limited capacity for structural exploration, making it difficult to escape deep local optima in complex Combinatorial Optimization Problems (COPs). In this work, we propose G-LNS, a generative evolutionary framework that extends LLM-based AHD to the automated design of Large Neighborhood Search (LNS) operators. Unlike prior methods that evolve heuristics in isolation, G-LNS leverages LLMs to co-evolve tightly coupled pairs of destroy and repair operators. A cooperative evaluation mechanism explicitly captures their interaction, enabling the discovery of complementary operator logic that jointly performs effective structural disruption and reconstruction. Extensive experiments on challenging COP benchmarks, such as Traveling Salesman Problems (TSP) and Capacitated Vehicle Routing Problems (CVRP), demonstrate that G-LNS significantly outperforms LLM-based AHD methods as well as strong classical solvers. The discovered heuristics not only achieve near-optimal solutions with reduced computational budgets but also exhibit robust generalization across diverse and unseen instance distributions.

</details>


### [70] [Puda: Private User Dataset Agent for User-Sovereign and Privacy-Preserving Personalized AI](https://arxiv.org/abs/2602.08268)
*Akinori Maeda,Yuto Sekiya,Sota Sugimura,Tomoya Asai,Yu Tsuda,Kohei Ikeda,Hiroshi Fujii,Kohei Watanabe*

Main category: cs.AI

TL;DR: 论文提出用户主权架构Puda应对数据利用与隐私保护平衡难题，实现多粒度数据管理，缓解隐私与个性化权衡问题。


<details>
  <summary>Details</summary>
Motivation: 主导平台的数据集中限制用户主权且阻碍跨服务数据使用，LLM 代理对个性化服务需求增加，需平衡数据利用与隐私保护。

Method: 提出 Puda 架构，允许用户在三个隐私级别控制数据共享，将其实现为基于浏览器的系统，并通过个性化旅行规划任务进行评估。

Result: 提供预定义类别子集可达到共享详细浏览历史时 97.2% 的个性化性能。

Conclusion: Puda 实现有效多粒度管理，为用户主权提供 AI 原生基础，让用户安全利用个性化 AI 潜力。

Abstract: Personal data centralization among dominant platform providers including search engines, social networking services, and e-commerce has created siloed ecosystems that restrict user sovereignty, thereby impeding data use across services. Meanwhile, the rapid proliferation of Large Language Model (LLM)-based agents has intensified demand for highly personalized services that require the dynamic provision of diverse personal data. This presents a significant challenge: balancing the utilization of such data with privacy protection. To address this challenge, we propose Puda (Private User Dataset Agent), a user-sovereign architecture that aggregates data across services and enables client-side management. Puda allows users to control data sharing at three privacy levels: (i) Detailed Browsing History, (ii) Extracted Keywords, and (iii) Predefined Category Subsets. We implemented Puda as a browser-based system that serves as a common platform across diverse services and evaluated it through a personalized travel planning task. Our results show that providing Predefined Category Subsets achieves 97.2% of the personalization performance (evaluated via an LLM-as-a-Judge framework across three criteria) obtained when sharing Detailed Browsing History. These findings demonstrate that Puda enables effective multi-granularity management, offering practical choices to mitigate the privacy-personalization trade-off. Overall, Puda provides an AI-native foundation for user sovereignty, empowering users to safely leverage the full potential of personalized AI.

</details>


### [71] [Toward Formalizing LLM-Based Agent Designs through Structural Context Modeling and Semantic Dynamics Analysis](https://arxiv.org/abs/2602.08276)
*Haoyu Jia,Kento Kawaharazuka,Kei Okada*

Main category: cs.AI

TL;DR: 当前大语言模型（LLM）代理研究碎片化，提出结构上下文模型及相关组件解决问题，在猴子 - 香蕉问题上验证有效性。


<details>
  <summary>Details</summary>
Motivation: 解决当前LLM代理研究碎片化问题，因缺乏可分析、自洽的形式化模型。

Method: 提出结构上下文模型，引入声明式实现框架和语义动力学分析工作流。

Result: 在猴子 - 香蕉问题动态变体上，使用该方法设计的代理在最具挑战性设置下成功率最多提高32个百分点。

Conclusion: 所提完整框架有效，能为代理机制提供原则性见解，支持快速、系统的设计迭代。

Abstract: Current research on large language model (LLM) agents is fragmented: discussions of conceptual frameworks and methodological principles are frequently intertwined with low-level implementation details, causing both readers and authors to lose track amid a proliferation of superficially distinct concepts. We argue that this fragmentation largely stems from the absence of an analyzable, self-consistent formal model that enables implementation-independent characterization and comparison of LLM agents. To address this gap, we propose the \texttt{Structural Context Model}, a formal model for analyzing and comparing LLM agents from the perspective of context structure. Building upon this foundation, we introduce two complementary components that together span the full lifecycle of LLM agent research and development: (1) a declarative implementation framework; and (2) a sustainable agent engineering workflow, \texttt{Semantic Dynamics Analysis}. The proposed workflow provides principled insights into agent mechanisms and supports rapid, systematic design iteration. We demonstrate the effectiveness of the complete framework on dynamic variants of the monkey-banana problem, where agents engineered using our approach achieve up to a 32 percentage points improvement in success rate on the most challenging setting.

</details>


### [72] [The Vibe-Automation of Automation: A Proactive Education Framework for Computer Science in the Age of Generative AI](https://arxiv.org/abs/2602.08295)
*Ilya Levin*

Main category: cs.AI

TL;DR: 本文指出生成式AI带来认识论转变，引入‘氛围自动化’概念，探讨人类角色转变，提出框架连接转变与教育、制度变革，还提及相关风险。


<details>
  <summary>Details</summary>
Motivation: 探讨生成式AI带来的认识论转变及其影响，以应对其对计算机科学基础假设的挑战。

Method: 引入‘氛围自动化’概念，构建跨三个分析层次和三个行动领域的概念框架。

Result: 明确生成式AI的意义在于对隐性规律的功能性访问，人类角色转向‘氛围工程’。

Conclusion: 需借助概念框架推动教育和制度变革，要谨慎对待生成式系统以避免模式崩溃和文化同质化。

Abstract: The emergence of generative artificial intelligence (GenAI) represents not an incremental technological advance but a qualitative epistemological shift that challenges foundational assumptions of computer science. Whereas machine learning has been described as the automation of automation, generative AI operates by navigating contextual, semantic, and stylistic coherence rather than optimizing predefined objective metrics. This paper introduces the concept of Vibe-Automation to characterize this transition.
  The central claim is that the significance of GenAI lies in its functional access to operationalized tacit regularities: context-sensitive patterns embedded in practice that cannot be fully specified through explicit algorithmic rules. Although generative systems do not possess tacit knowledge in a phenomenological sense, they operationalize sensitivities to tone, intent, and situated judgment encoded in high-dimensional latent representations. On this basis, the human role shifts from algorithmic problem specification toward Vibe-Engineering, understood as the orchestration of alignment and contextual judgment in generative systems.
  The paper connects this epistemological shift to educational and institutional transformation by proposing a conceptual framework structured across three analytical levels and three domains of action: faculty worldview, industry relations, and curriculum design. The risks of mode collapse and cultural homogenization are briefly discussed, emphasizing the need for deliberate engagement with generative systems to avoid regression toward synthetic uniformity.

</details>


### [73] [Moral Sycophancy in Vision Language Models](https://arxiv.org/abs/2602.08311)
*Shadman Rabby,Md. Hefzul Hossain Papon,Sabbir Ahmed,Nokimul Hasan Arif,A. B. M. Ashikur Rahman,Irfan Ahmad*

Main category: cs.AI

TL;DR: 对视觉语言模型（VLMs）中的道德谄媚现象进行系统研究，发现VLMs易受用户意见影响，不同数据集表现有差异，纠错能力和引入错误存在权衡。


<details>
  <summary>Details</summary>
Motivation: 此前研究对VLMs道德谄媚在视觉决策中的影响理解不足，需进行系统研究。

Method: 在Moralise和M^3oralBench数据集上分析十个广泛使用的模型，使用错误引入率（EIR）和错误纠正率（ECR）评估。

Result: VLMs常给出道德错误的后续响应，存在不对称性，不同数据集表现不同，纠错能力和引入错误有权衡。

Conclusion: VLMs易受道德影响，需策略提升多模态AI系统的伦理一致性和鲁棒性。

Abstract: Sycophancy in Vision-Language Models (VLMs) refers to their tendency to align with user opinions, often at the expense of moral or factual accuracy. While prior studies have explored sycophantic behavior in general contexts, its impact on morally grounded visual decision-making remains insufficiently understood. To address this gap, we present the first systematic study of moral sycophancy in VLMs, analyzing ten widely-used models on the Moralise and M^3oralBench datasets under explicit user disagreement. Our results reveal that VLMs frequently produce morally incorrect follow-up responses even when their initial judgments are correct, and exhibit a consistent asymmetry: models are more likely to shift from morally right to morally wrong judgments than the reverse when exposed to user-induced bias. Follow-up prompts generally degrade performance on Moralise, while yielding mixed or even improved accuracy on M^3oralBench, highlighting dataset-dependent differences in moral robustness. Evaluation using Error Introduction Rate (EIR) and Error Correction Rate (ECR) reveals a clear trade-off: models with stronger error-correction capabilities tend to introduce more reasoning errors, whereas more conservative models minimize errors but exhibit limited ability to self-correct. Finally, initial contexts with a morally right stance elicit stronger sycophantic behavior, emphasizing the vulnerability of VLMs to moral influence and the need for principled strategies to improve ethical consistency and robustness in multimodal AI systems.

</details>


### [74] [Who Deserves the Reward? SHARP: Shapley Credit-based Optimization for Multi-Agent System](https://arxiv.org/abs/2602.08335)
*Yanming Li,Xuelin Zhang,WenJie Lu,Ziye Tang,Maodong Wu,Haotian Luo,Tongtong Wu,Zijie Peng,Hongze Mi,Yibo Feng,Naiqiang Tan,Chao Huang,Hong Chen,Li Shen*

Main category: cs.AI

TL;DR: 提出SHARP框架优化多智能体强化学习，实验显示其显著优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体系统训练因信用分配挑战困难，现有方法依赖稀疏或全局广播奖励，无法捕捉个体贡献，导致强化学习效率低。

Method: 引入SHARP框架，通过包含全局广播准确率奖励、基于Shapley值的边际信用奖励和工具处理奖励的分解奖励机制，对轨迹组的特定智能体优势进行归一化，稳定训练。

Result: 在多个现实世界基准测试中，SHARP显著优于近期最先进的基线，相比单智能体和多智能体方法，平均匹配度分别提高23.66%和14.05%。

Conclusion: SHARP框架通过精确的信用分配优化多智能体强化学习是有效的。

Abstract: Integrating Large Language Models (LLMs) with external tools via multi-agent systems offers a promising new paradigm for decomposing and solving complex problems. However, training these systems remains notoriously difficult due to the credit assignment challenge, as it is often unclear which specific functional agent is responsible for the success or failure of decision trajectories. Existing methods typically rely on sparse or globally broadcast rewards, failing to capture individual contributions and leading to inefficient reinforcement learning. To address these limitations, we introduce the Shapley-based Hierarchical Attribution for Reinforcement Policy (SHARP), a novel framework for optimizing multi-agent reinforcement learning via precise credit attribution. SHARP effectively stabilizes training by normalizing agent-specific advantages across trajectory groups, primarily through a decomposed reward mechanism comprising a global broadcast-accuracy reward, a Shapley-based marginal-credit reward for each agent, and a tool-process reward to improve execution efficiency. Extensive experiments across various real-world benchmarks demonstrate that SHARP significantly outperforms recent state-of-the-art baselines, achieving average match improvements of 23.66% and 14.05% over single-agent and multi-agent approaches, respectively.

</details>


### [75] [CoTZero: Annotation-Free Human-Like Vision Reasoning via Hierarchical Synthetic CoT](https://arxiv.org/abs/2602.08339)
*Chengyi Du,Yazhe Niu,Dazhong Shen,Luxin Xu*

Main category: cs.AI

TL;DR: 现有视觉语言模型在视觉推理上不及人类，本文提出无注释范式CoTZero改进推理能力，实验效果良好。


<details>
  <summary>Details</summary>
Motivation: 当前许多视觉语言模型依赖表面相关性，缺乏逻辑连贯的结构化表示，导致高级语义结构和非因果关系理解缺失，阻碍组合和可验证推理，需改进。

Method: 提出CoTZero，包含双阶段数据合成方法和认知对齐训练方法。数据合成从神经认知中获取灵感，分自底向上和自顶向下阶段；训练引入CCVR强化微调。

Result: CoTZero在多级别语义不一致基准测试中F1分数达83.33%，消融实验表明各组件有助于更具解释性和类人视觉推理。

Conclusion: CoTZero能有效提升视觉语言模型的视觉推理能力，使其推理更具解释性且与人类对齐。

Abstract: Recent advances in vision-language models (VLMs) have markedly improved image-text alignment, yet they still fall short of human-like visual reasoning. A key limitation is that many VLMs rely on surface correlations rather than building logically coherent structured representations, which often leads to missed higher-level semantic structure and non-causal relational understanding, hindering compositional and verifiable reasoning. To address these limitations by introducing human models into the reasoning process, we propose CoTZero, an annotation-free paradigm with two components: (i) a dual-stage data synthesis approach and (ii) a cognition-aligned training method. In the first component, we draw inspiration from neurocognitive accounts of compositional productivity and global-to-local analysis. In the bottom-up stage, CoTZero extracts atomic visual primitives and incrementally composes them into diverse, structured question-reasoning forms. In the top-down stage, it enforces hierarchical reasoning by using coarse global structure to guide the interpretation of local details and causal relations. In the cognition-aligned training component, built on the synthesized CoT data, we introduce Cognitively Coherent Verifiable Rewards (CCVR) in Reinforcement Fine-Tuning (RFT) to further strengthen VLMs' hierarchical reasoning and generalization, providing stepwise feedback on reasoning coherence and factual correctness. Experiments show that CoTZero achieves an F1 score of 83.33 percent on our multi-level semantic inconsistency benchmark with lexical-perturbation negatives, across both in-domain and out-of-domain settings. Ablations confirm that each component contributes to more interpretable and human-aligned visual reasoning.

</details>


### [76] [Effect-Level Validation for Causal Discovery](https://arxiv.org/abs/2602.08340)
*Hoang Dang,Luan Pham,Minh Nguyen*

Main category: cs.AI

TL;DR: 本文提出以效应为中心、可容许性优先的框架，用游戏遥测数据研究早期接触竞技玩法对短期留存的影响，指出图级指标不足以衡量因果可靠性，应优先考虑可容许性和效应级验证。


<details>
  <summary>Details</summary>
Motivation: 因果发现用于大规模遥测数据，但在强自我选择的反馈驱动系统中用于决策的可靠性不明。

Method: 提出以效应为中心、可容许性优先的框架，将发现的图视为结构假设，通过可识别性、稳定性和证伪进行评估；用真实游戏遥测数据研究早期接触竞技玩法对短期留存的影响。

Result: 很多统计上合理的发现结果在施加最小时间和语义约束后无法进行点识别因果查询；部分算法族能收敛到相似、决策一致的效应估计；其他方法存在可容许性不稳定、效应受端点模糊影响等问题。

Conclusion: 图级指标不足以作为给定目标查询的因果可靠性代理，遥测驱动系统中可靠的因果结论应优先考虑可容许性和效应级验证，而非仅关注因果结构恢复。

Abstract: Causal discovery is increasingly applied to large-scale telemetry data to estimate the effects of user-facing interventions, yet its reliability for decision-making in feedback-driven systems with strong self-selection remains unclear. In this paper, we propose an effect-centric, admissibility-first framework that treats discovered graphs as structural hypotheses and evaluates them by identifiability, stability, and falsification rather than by graph recovery accuracy alone. Empirically, we study the effect of early exposure to competitive gameplay on short-term retention using real-world game telemetry. We find that many statistically plausible discovery outputs do not admit point-identified causal queries once minimal temporal and semantic constraints are enforced, highlighting identifiability as a critical bottleneck for decision support. When identification is possible, several algorithm families converge to similar, decision-consistent effect estimates despite producing substantially different graph structures, including cases where the direct treatment-outcome edge is absent and the effect is preserved through indirect causal pathways. These converging estimates survive placebo, subsampling, and sensitivity refutation. In contrast, other methods exhibit sporadic admissibility and threshold-sensitive or attenuated effects due to endpoint ambiguity. These results suggest that graph-level metrics alone are inadequate proxies for causal reliability for a given target query. Therefore, trustworthy causal conclusions in telemetry-driven systems require prioritizing admissibility and effect-level validation over causal structural recovery alone.

</details>


### [77] [OPE: Overcoming Information Saturation in Parallel Thinking via Outline-Guided Path Exploration](https://arxiv.org/abs/2602.08344)
*Qi Guo,Jianing Wang,Deyang Kong,Xiangyu Xi,Jianfei Zhang,Yi Lu,Jingang Wang,Wei Wang,Shikun Zhang,Wei Ye*

Main category: cs.AI

TL;DR: 本文分析了强化学习下并行思维优化问题，提出Outline - Guided Path Exploration (OPE)方法，实验证明其能提升大推理模型推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有利用强化学习增强并行思维的方法多关注聚合阶段，对路径探索阶段关注有限，且探索路径间的互信息瓶颈限制整体性能。

Method: 提出OPE方法，通过生成多样化推理大纲划分解空间，采用迭代强化学习策略分别优化大纲规划和大纲引导推理。

Result: 在多个数学基准测试上的实验表明，OPE能在不同聚合策略下有效提升推理性能，使大推理模型更可靠地找到正确解。

Conclusion: OPE方法有效，可改善大推理模型在并行思维下的推理表现。

Abstract: Parallel thinking has emerged as a new paradigm for large reasoning models (LRMs) in tackling complex problems. Recent methods leverage Reinforcement Learning (RL) to enhance parallel thinking, aiming to address the limitations in computational resources and effectiveness encountered with supervised fine-tuning. However, most existing studies primarily focus on optimizing the aggregation phase, with limited attention to the path exploration stage. In this paper, we theoretically analyze the optimization of parallel thinking under the Reinforcement Learning with Verifiable Rewards (RLVR) setting, and identify that the mutual information bottleneck among exploration paths fundamentally restricts overall performance. To address this, we propose Outline-Guided Path Exploration (OPE), which explicitly partitions the solution space by generating diverse reasoning outlines prior to parallel path reasoning, thereby reducing information redundancy and improving the diversity of information captured across exploration paths. We implement OPE with an iterative RL strategy that optimizes outline planning and outline-guided reasoning independently. Extensive experiments across multiple challenging mathematical benchmarks demonstrate that OPE effectively improves reasoning performance in different aggregation strategies, enabling LRMs to more reliably discover correct solutions.

</details>


### [78] [Towards Better Evolution Modeling for Temporal Knowledge Graphs](https://arxiv.org/abs/2602.08353)
*Zhang Jiasheng,Li Zhangpin,Wang Mingzhe,Shao Jie,Cui Jiangtao,Li Hui*

Main category: cs.AI

TL;DR: 现有时间知识图谱（TKG）基准存在捷径问题，本文分析原因并推出新基准。


<details>
  <summary>Details</summary>
Motivation: 现有TKG基准存在引入捷径、数据集有偏差、评估任务过简等问题，影响公平评估。

Method: 分析现有基准问题根源，指出数据集固有偏差和评估任务缺陷，推出包含四个无偏差数据集和两个新任务的TKG进化基准。

Result: 发现现有基准存在多种问题，推出新的TKG进化基准。

Conclusion: 新的TKG进化基准有助于更准确理解TKG进化建模挑战。

Abstract: Temporal knowledge graphs (TKGs) structurally preserve evolving human knowledge. Recent research has focused on designing models to learn the evolutionary nature of TKGs to predict future facts, achieving impressive results. For instance, Hits@10 scores over 0.9 on YAGO dataset. However, we find that existing benchmarks inadvertently introduce a shortcut. Near state-of-the-art performance can be simply achieved by counting co-occurrences, without using any temporal information. In this work, we examine the root cause of this issue, identifying inherent biases in current datasets and over simplified form of evaluation task that can be exploited by these biases. Through this analysis, we further uncover additional limitations of existing benchmarks, including unreasonable formatting of time-interval knowledge, ignorance of learning knowledge obsolescence, and insufficient information for precise evolution understanding, all of which can amplify the shortcut and hinder a fair assessment. Therefore, we introduce the TKG evolution benchmark. It includes four bias-corrected datasets and two novel tasks closely aligned with the evolution process, promoting a more accurate understanding of the challenges in TKG evolution modeling. Benchmark is available at: https://github.com/zjs123/TKG-Benchmark.

</details>


### [79] [Does Your Reasoning Model Implicitly Know When to Stop Thinking?](https://arxiv.org/abs/2602.08354)
*Zixuan Huang,Xin Xia,Yuxi Ren,Jianbin Zheng,Xuanda Wang,Zhixia Zhang,Hongyan Xie,Songshi Liang,Zehao Chen,Xuefeng Xiao,Fuzhen Zhuang,Jianxin Li,Yikun Ban,Deqing Wang*

Main category: cs.AI

TL;DR: 现有长思维链推理冗余，提出SAGE采样范式及SAGE - RL，提升推理准确性与效率


<details>
  <summary>Details</summary>
Motivation: 长思维链推理有冗余，影响计算效率和实时应用，且长推理链与正确性无关甚至有害

Method: 引入SAGE采样范式，将SAGE作为混合采样集成到基于组的强化学习中形成SAGE - RL

Result: SAGE - RL能将SAGE发现的高效推理模式融入标准pass@1推理

Conclusion: SAGE和SAGE - RL显著提升了大推理模型在多个数学基准测试中的推理准确性和效率

Abstract: Recent advancements in large reasoning models (LRMs) have greatly improved their capabilities on complex reasoning tasks through Long Chains of Thought (CoTs). However, this approach often results in substantial redundancy, impairing computational efficiency and causing significant delays in real-time applications. Recent studies show that longer reasoning chains are frequently uncorrelated with correctness and can even be detrimental to accuracy. In a further in-depth analysis of this phenomenon, we surprisingly uncover and empirically verify that LRMs implicitly know the appropriate time to stop thinking, while this capability is obscured by current sampling paradigms. Motivated by this, we introduce SAGE (Self-Aware Guided Efficient Reasoning), a novel sampling paradigm that unleashes this efficient reasoning potential. Furthermore, integrating SAGE as mixed sampling into group-based reinforcement learning (SAGE-RL) enables SAGE-RL to effectively incorporate SAGE-discovered efficient reasoning patterns into standard pass@1 inference, markedly enhancing both the reasoning accuracy and efficiency of LRMs across multiple challenging mathematical benchmarks.

</details>


### [80] [Circuit Representations of Random Forests with Applications to XAI](https://arxiv.org/abs/2602.08362)
*Chunxi Ji,Adnan Darwiche*

Main category: cs.AI

TL;DR: 本文提出将随机森林分类器编译为电路的方法，比现有方法更高效，还用于计算决策原因、鲁棒性及翻转决策的最短路径，并通过实例展示其应用。


<details>
  <summary>Details</summary>
Motivation: 改进随机森林分类器相关计算的效率，提供计算决策原因、鲁棒性等方法。

Method: 提出将随机森林分类器编译为电路的方法，进一步获取可计算决策原因的电路，提出计算决策鲁棒性和翻转决策最短路径的算法。

Result: 提出的编译方法比现有类似方法更高效，可用于枚举决策的各种原因、计算鲁棒性和找出翻转决策的最短路径。

Conclusion: 所提方法在处理随机森林分类器决策相关计算上有显著效果，具有实际应用价值。

Abstract: We make three contributions in this paper. First, we present an approach for compiling a random forest classifier into a set of circuits, where each circuit directly encodes the instances in some class of the classifier. We show empirically that our proposed approach is significantly more efficient than existing similar approaches. Next, we utilize this approach to further obtain circuits that are tractable for computing the complete and general reasons of a decision, which are instance abstractions that play a fundamental role in computing explanations. Finally, we propose algorithms for computing the robustness of a decision and all shortest ways to flip it. We illustrate the utility of our contributions by using them to enumerate all sufficient reasons, necessary reasons and contrastive explanations of decisions; to compute the robustness of decisions; and to identify all shortest ways to flip the decisions made by random forest classifiers learned from a wide range of datasets.

</details>


### [81] [MemAdapter: Fast Alignment across Agent Memory Paradigms via Generative Subgraph Retrieval](https://arxiv.org/abs/2602.08369)
*Xin Zhang,Kailai Yang,Chenyue Li,Hao Li,Qiyu Wei,Jun'ichi Tsujii,Sophia Ananiadou*

Main category: cs.AI

TL;DR: 提出MemAdapter框架统一异构记忆范式，实现跨范式快速对齐和零样本融合，性能优越且计算成本低。


<details>
  <summary>Details</summary>
Motivation: 现有代理内存系统在孤立范式中设计，紧耦合检索方法阻碍跨范式泛化和融合，需统一异构内存范式。

Method: 提出MemAdapter框架，采用两阶段训练策略，先从统一内存空间训练生成子图检索器，再通过对比学习训练轻量级对齐模块。

Result: 在三个公开基准测试中，生成子图检索器在三种记忆范式和代理模型规模上均优于五个强代理内存系统；MemAdapter在单GPU上13分钟完成跨范式对齐，训练计算量不到5%；能实现有效的零样本融合。

Conclusion: MemAdapter具有作为代理内存系统即插即用解决方案的潜力。

Abstract: Memory mechanism is a core component of LLM-based agents, enabling reasoning and knowledge discovery over long-horizon contexts. Existing agent memory systems are typically designed within isolated paradigms (e.g., explicit, parametric, or latent memory) with tightly coupled retrieval methods that hinder cross-paradigm generalization and fusion. In this work, we take a first step toward unifying heterogeneous memory paradigms within a single memory system. We propose MemAdapter, a memory retrieval framework that enables fast alignment across agent memory paradigms. MemAdapter adopts a two-stage training strategy: (1) training a generative subgraph retriever from the unified memory space, and (2) adapting the retriever to unseen memory paradigms by training a lightweight alignment module through contrastive learning. This design improves the flexibility for memory retrieval and substantially reduces alignment cost across paradigms. Comprehensive experiments on three public evaluation benchmarks demonstrate that the generative subgraph retriever consistently outperforms five strong agent memory systems across three memory paradigms and agent model scales. Notably, MemAdapter completes cross-paradigm alignment within 13 minutes on a single GPU, achieving superior performance over original memory retrievers with less than 5% of training compute. Furthermore, MemAdapter enables effective zero-shot fusion across memory paradigms, highlighting its potential as a plug-and-play solution for agent memory systems.

</details>


### [82] [Grounding Generative Planners in Verifiable Logic: A Hybrid Architecture for Trustworthy Embodied AI](https://arxiv.org/abs/2602.08373)
*Feiyu Wu,Xu Zheng,Yue Qu,Zhuocheng Wang,Zicheng Feng,Hui Li*

Main category: cs.AI

TL;DR: 提出可验证迭代细化框架(VIRF)提升具身AI规划安全性，在家庭安全任务中表现良好。


<details>
  <summary>Details</summary>
Motivation: 大语言模型作为具身AI规划器缺乏形式推理，现有方法在安全检查和修复方面存在不足。

Method: 引入VIRF神经符号架构，通过导师 - 学徒对话，逻辑导师提供反馈实现智能计划修复，还引入知识获取管道合成安全知识库。

Result: 在家庭安全任务中，VIRF危险行动率为0%，目标条件率达77.3%，平均仅需1.1次修正迭代。

Conclusion: VIRF为构建可信且可验证安全的具身智能体提供了原则性途径。

Abstract: Large Language Models (LLMs) show promise as planners for embodied AI, but their stochastic nature lacks formal reasoning, preventing strict safety guarantees for physical deployment. Current approaches often rely on unreliable LLMs for safety checks or simply reject unsafe plans without offering repairs. We introduce the Verifiable Iterative Refinement Framework (VIRF), a neuro-symbolic architecture that shifts the paradigm from passive safety gatekeeping to active collaboration. Our core contribution is a tutor-apprentice dialogue where a deterministic Logic Tutor, grounded in a formal safety ontology, provides causal and pedagogical feedback to an LLM planner. This enables intelligent plan repairs rather than mere avoidance. We also introduce a scalable knowledge acquisition pipeline that synthesizes safety knowledge bases from real-world documents, correcting blind spots in existing benchmarks. In challenging home safety tasks, VIRF achieves a perfect 0 percent Hazardous Action Rate (HAR) and a 77.3 percent Goal-Condition Rate (GCR), which is the highest among all baselines. It is highly efficient, requiring only 1.1 correction iterations on average. VIRF demonstrates a principled pathway toward building fundamentally trustworthy and verifiably safe embodied agents.

</details>


### [83] [SCOUT-RAG: Scalable and Cost-Efficient Unifying Traversal for Agentic Graph-RAG over Distributed Domains](https://arxiv.org/abs/2602.08400)
*Longkun Li,Yuanben Zou,Jinghan Wu,Yuqing Wen,Jing Li,Hangwei Qian,Ivor Tsang*

Main category: cs.AI

TL;DR: 介绍分布式代理Graph - RAG框架SCOUT - RAG，其能在分布式和访问受限场景下实现跨域检索，性能与集中式基线相当且减少开销。


<details>
  <summary>Details</summary>
Motivation: 传统Graph - RAG依赖集中式知识图谱，在分布式和访问受限场景下，检索需在无全局图可见性或穷举查询的情况下选择相关域和合适的遍历深度。

Method: 引入SCOUT - RAG框架，采用四个合作代理，分别估计域相关性、决定是否扩展检索域、调整遍历深度和合成高质量答案。

Result: 在多域知识场景中，SCOUT - RAG性能与集中式基线（如DRIFT和穷举域遍历）相当，大幅减少跨域调用、处理的总令牌数和延迟。

Conclusion: SCOUT - RAG能在分布式和访问受限场景下有效实现跨域检索，减少开销，具有应用价值。

Abstract: Graph-RAG improves LLM reasoning using structured knowledge, yet conventional designs rely on a centralized knowledge graph. In distributed and access-restricted settings (e.g., hospitals or multinational organizations), retrieval must select relevant domains and appropriate traversal depth without global graph visibility or exhaustive querying. To address this challenge, we introduce \textbf{SCOUT-RAG} (\textit{\underline{S}calable and \underline{CO}st-efficient \underline{U}nifying \underline{T}raversal}), a distributed agentic Graph-RAG framework that performs progressive cross-domain retrieval guided by incremental utility goals. SCOUT-RAG employs four cooperative agents that: (i) estimate domain relevance, (ii) decide when to expand retrieval to additional domains, (iii) adapt traversal depth to avoid unnecessary graph exploration, and (iv) synthesize the high-quality answers. The framework is designed to minimize retrieval regret, defined as missing useful domain information, while controlling latency and API cost. Across multi-domain knowledge settings, SCOUT-RAG achieves performance comparable to centralized baselines, including DRIFT and exhaustive domain traversal, while substantially reducing cross-domain calls, total tokens processed, and latency.

</details>


### [84] [On Protecting Agentic Systems' Intellectual Property via Watermarking](https://arxiv.org/abs/2602.08401)
*Liwen Wang,Zongjie Li,Yuchong Xie,Shuai Wang,Dongdong She,Wei Wang,Juergen Rahmel*

Main category: cs.AI

TL;DR: 大语言模型演变成自主智能体系统产生大量IP价值，但易受模仿攻击，现有水印技术失效，本文提出AGENTWM水印框架，经评估能有效保护IP。


<details>
  <summary>Details</summary>
Motivation: 大语言模型演变成的智能体系统易受模仿攻击，现有水印技术在该场景失效，需保护智能体系统的知识产权。

Method: 提出AGENTWM水印框架，利用动作序列语义等价性，通过轻微偏置功能相同的工具执行路径分布注入水印，开发生成水印方案的自动化管道和用于验证的严格统计假设检验程序。

Result: 在三个复杂领域的广泛评估表明，AGENTWM检测准确率高，对智能体性能影响可忽略不计。

Conclusion: AGENTWM能有效保护智能体知识产权，对抗自适应对手，对手移除水印会严重降低被盗模型效用。

Abstract: The evolution of Large Language Models (LLMs) into agentic systems that perform autonomous reasoning and tool use has created significant intellectual property (IP) value. We demonstrate that these systems are highly vulnerable to imitation attacks, where adversaries steal proprietary capabilities by training imitation models on victim outputs. Crucially, existing LLM watermarking techniques fail in this domain because real-world agentic systems often operate as grey boxes, concealing the internal reasoning traces required for verification. This paper presents AGENTWM, the first watermarking framework designed specifically for agentic models. AGENTWM exploits the semantic equivalence of action sequences, injecting watermarks by subtly biasing the distribution of functionally identical tool execution paths. This mechanism allows AGENTWM to embed verifiable signals directly into the visible action trajectory while remaining indistinguishable to users. We develop an automated pipeline to generate robust watermark schemes and a rigorous statistical hypothesis testing procedure for verification. Extensive evaluations across three complex domains demonstrate that AGENTWM achieves high detection accuracy with negligible impact on agent performance. Our results confirm that AGENTWM effectively protects agentic IP against adaptive adversaries, who cannot remove the watermarks without severely degrading the stolen model's utility.

</details>


### [85] [From Assistant to Double Agent: Formalizing and Benchmarking Attacks on OpenClaw for Personalized Local AI Agent](https://arxiv.org/abs/2602.08412)
*Yuhang Wang,Feiming Xu,Zheng Lin,Guangyu He,Yuzhe Huang,Haichang Gao,Zhenxing Niu*

Main category: cs.AI

TL;DR: 现有大语言模型代理安全研究框架有局限，提出PASB框架评估真实场景下个性化代理安全性，以OpenClaw为例研究发现其多阶段存在关键漏洞。


<details>
  <summary>Details</summary>
Motivation: 现有代理安全研究和评估框架主要聚焦合成或任务为中心的设置，无法准确捕捉现实部署中个性化代理的攻击面和风险传播机制。

Method: 提出Personalized Agent Security Bench (PASB) 框架，结合个性化使用场景、现实工具链和长周期交互，对真实系统进行黑盒、端到端安全评估，并以OpenClaw为案例研究。

Result: OpenClaw在用户提示处理、工具使用和内存检索等执行阶段存在关键漏洞，突显个性化代理部署的重大安全风险。

Conclusion: PASB框架可用于评估真实世界个性化代理的安全性，且代码开源。

Abstract: Although large language model (LLM)-based agents, exemplified by OpenClaw, are increasingly evolving from task-oriented systems into personalized AI assistants for solving complex real-world tasks, their practical deployment also introduces severe security risks. However, existing agent security research and evaluation frameworks primarily focus on synthetic or task-centric settings, and thus fail to accurately capture the attack surface and risk propagation mechanisms of personalized agents in real-world deployments. To address this gap, we propose Personalized Agent Security Bench (PASB), an end-to-end security evaluation framework tailored for real-world personalized agents. Building upon existing agent attack paradigms, PASB incorporates personalized usage scenarios, realistic toolchains, and long-horizon interactions, enabling black-box, end-to-end security evaluation on real systems. Using OpenClaw as a representative case study, we systematically evaluate its security across multiple personalized scenarios, tool capabilities, and attack types. Our results indicate that OpenClaw exhibits critical vulnerabilities at different execution stages, including user prompt processing, tool usage, and memory retrieval, highlighting substantial security risks in personalized agent deployments. The code for the proposed PASB framework is available at https://github.com/AstorYH/PASB.

</details>


### [86] [When Evaluation Becomes a Side Channel: Regime Leakage and Structural Mitigations for Alignment Assessment](https://arxiv.org/abs/2602.08449)
*Igor Santos-Grueiro*

Main category: cs.AI

TL;DR: 将对齐评估重新构建为部分可观测下的信息流问题，研究了制度盲机制，评估其在语言模型上对两种失败模式的效果，指出表征不变性是有限的控制手段，建议行为评估辅以白盒诊断。


<details>
  <summary>Details</summary>
Motivation: 现有高级AI系统安全评估假设评估时的行为能预测部署时的行为，但具有情境感知的智能体可能利用制度泄漏实施条件策略，该假设变得脆弱，因此需重新审视评估方法。

Method: 将对齐评估重新构建为部分可观测下的信息流问题，研究制度盲机制，通过对抗不变性减少决策相关内部表征中制度信息的可提取性，并在语言模型上进行评估。

Result: 制度盲训练在两种评估情况下抑制了制度条件行为，且无明显任务效用损失，但两种情况动态不同，表征不变性是有意义但有限的控制手段。

Conclusion: 行为评估应辅以对制度意识和信息流的白盒诊断。

Abstract: Safety evaluation for advanced AI systems implicitly assumes that behavior observed under evaluation is predictive of behavior in deployment. This assumption becomes fragile for agents with situational awareness, which may exploitregime leakage-informational cues distinguishing evaluation from deployment-to implement conditional policies such as sycophancy and sleeper agents, which preserve compliance under oversight while defecting in deployment-like regimes. We reframe alignment evaluation as a problem of information flow under partial observability. Within this framework, we show that divergence between evaluation-time and deployment-time behavior is bounded by the mutual information between internal representations and the regime variable. Motivated by this result, we study regime-blind mechanisms: training-time interventions that reduce the extractability of regime information at decision-relevant internal representations via adversarial invariance. We evaluate this approach on a base, open-weight language model across two fully characterized failure modes -scientific sycophancy and temporal sleeper agents. Regime-blind training suppresses regime-conditioned behavior in both evaluated cases without measurable loss of task utility, but with qualitatively different dynamics: sycophancy exhibits a sharp representational and behavioral transition at low intervention strength, whereas sleeper-agent behavior requires substantially stronger pressure and does not exhibit a clean collapse of regime decodability. These results demonstrate that representational invariance is a meaningful but fundamentally limited control lever, whose effectiveness depends on how regime information is embedded in the policy. We argue that behavioral evaluation should be complemented with white-box diagnostics of regime awareness and information flow.

</details>


### [87] [Reinforcement Inference: Leveraging Uncertainty for Self-Correcting Language Model Reasoning](https://arxiv.org/abs/2602.08520)
*Xinhai Sun*

Main category: cs.AI

TL;DR: 提出强化推理策略，在不重新训练下提升大语言模型性能，在MMLU - Pro问题上准确率显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有单步贪心推理协议会低估模型真实能力，很多错误源于内部歧义下过早决策。

Method: 引入强化推理，一种基于熵感知的推理时间控制策略，利用模型自身不确定性选择性进行二次推理。

Result: 在12,032个MMLU - Pro问题上，准确率从60.72%提升到84.03%，额外推理调用仅61.06%；100%重新询问消融实验达84.35%；仅提示消融实验效果不如基线。

Conclusion: 该策略是实用推理升级，提出基于熵感知的测量和扩展模型能力范式，为未来训练目标提供思路。

Abstract: Modern large language models (LLMs) are often evaluated and deployed under a \emph{one-shot, greedy} inference protocol, especially in professional settings that require deterministic behavior. This regime can systematically under-estimate a fixed model's true capability: many errors arise not from missing knowledge, but from premature commitment under internal ambiguity. We introduce \emph{Reinforcement Inference}, an entropy-aware inference-time control strategy that uses the model's own uncertainty to selectively invoke a second, more deliberate reasoning attempt, enabling stronger performance \emph{without any retraining}.
  On 12,032 MMLU-Pro questions across 14 subjects, using DeepSeek-v3.2 with deterministic decoding in a zero-shot setting, Reinforcement Inference improves accuracy from 60.72\% to 84.03\%, while only incurring 61.06\% additional inference calls. A 100\% re-asking ablation reaches 84.35\%, indicating that uncertainty-aware selection captures most of the attainable improvement with substantially less compute. Moreover, a \emph{prompt-only} ablation underperforms the baseline, suggesting that the gains are not explained by generic `` your output had high entropy, think step-by-step'' prompting alone.
  Beyond providing a practical inference-time upgrade, our results suggest a broader \emph{entropy-aware} paradigm for measuring and expanding model capability: because modern decoder-based models generate outputs autoregressively, entropy and related confidence measures arise naturally as first-class control signals during generation. The resulting gap between one-pass greedy inference and uncertainty-conditioned deliberation offers a diagnostic lens on an LLM's latent reasoning horizon and motivates future training objectives that explicitly constrain correctness--confidence alignment.

</details>


### [88] [Dialogue Model Optimization via Agent Game and Adaptive Tree-based GRPO](https://arxiv.org/abs/2602.08533)
*Kun Peng,Conghui Tan,Yu Liu,Guohua Tang,Zhongqian Sun,Wei Yang,Zining Zhu,Lei Jiang,Yanbing Liu,Hao Peng*

Main category: cs.AI

TL;DR: 提出长视野强化学习框架AT - GRPO解决现有开放式对话代理的问题，实验显示其性能优越。


<details>
  <summary>Details</summary>
Motivation: 现有开放式对话代理方法存在过度依赖预收集用户数据和强化学习短视野偏差问题，忽视长期对话价值。

Method: 提出集成在线个性化与自适应树基组相对策略优化（AT - GRPO）的长视野强化学习框架，采用双智能体博弈范式，AT - GRPO将对话轨迹重新解释为树并引入自适应观察范围。

Result: 框架表现出优越的性能、样本效率和鲁棒性。

Conclusion: 所提出的框架有效解决了现有开放式对话代理的问题。

Abstract: Open-ended dialogue agents aim to deliver engaging, personalized interactions by adapting to users' traits, but existing methods face critical limitations: over-reliance on pre-collected user data, and short-horizon biases in reinforcement learning (RL) that neglect long-term dialogue value. To address these, we propose a novel long-horizon RL framework integrating online personalization with Adaptive Tree-based Group Relative Policy Optimization (AT-GRPO). Adopting a two-agent game paradigm, a user agent constructs dynamic environments via style mimicry (learning user-specific conversational traits) and active termination (predicting turn-level termination probabilities as immediate rewards), forming an iterative cycle that drives the dialogue agent to deepen interest exploration. AT-GRPO reinterprets dialogue trajectories as trees and introduces adaptive observation ranges. Unlike full tree expansion that incurs exponential overhead, it limits each node to aggregate rewards from a stage-aware range: larger ranges support early-stage topic exploration, while smaller ranges facilitate late-stage dialogue maintenance. This design reduces rollout budgets from exponential to polynomial in the dialogue length, while preserving long-term reward capture. Extensive experiments show our framework's superior performance, sample efficiency, and robustness.

</details>


### [89] [PRISM: A Principled Framework for Multi-Agent Reasoning via Gain Decomposition](https://arxiv.org/abs/2602.08586)
*Yiming Yang,Zhuoyuan Li,Fanxiang Zeng,Hao Fu,Yue Liu*

Main category: cs.AI

TL;DR: 本文提出统一理论框架分解多智能体推理增益维度，提出PRISM框架，实验证明其性能和计算效率优，还提供设计原则。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体协作方法缺乏原则性指导，不清楚性能提升原因和优化方法。

Method: 引入统一理论框架分解多智能体推理增益为三个维度，提出PRISM框架联合最大化三个维度。

Result: 在多个基准测试中，PRISM实现了最先进性能，计算效率更优。

Conclusion: 理论框架为未来多智能体推理系统提供了可操作的设计原则。

Abstract: Multi-agent collaboration has emerged as a promising paradigm for enhancing reasoning capabilities of Large Language Models (LLMs). However, existing approaches remain largely heuristic, lacking principled guidance on what drives performance gains and how to systematically optimize multi-agent reasoning. Specifically, it remains unclear why multi-agent collaboration outperforms single-agent reasoning and which design choices contribute most to these gains, making it difficult to build better systems.
  We address this gap by introducing a unified theoretical framework that decomposes multi-agent reasoning gains into three conceptually independent dimensions: Exploration for diverse solution coverage, Information for high-fidelity feedback, and Aggregation for principled consensus. Through this lens, existing methods can be understood as special cases that optimize only subsets of these dimensions. Building upon this decomposition, a novel framework called PRISM (Propose-Review-Integrate Synthesis for Multi-agent Reasoning) is proposed, which jointly maximizes all three dimensions through role-based diversity, execution-grounded feedback with evidence-based cross-evaluation, and iterative synthesis with closed-loop validation. Extensive experiments across mathematical reasoning, code generation, and function calling benchmarks demonstrate that PRISM achieves state-of-the-art performance with superior compute-efficiency compared to methods optimizing partial dimensions. The theoretical framework provides actionable design principles for future multi-agent reasoning systems.

</details>


### [90] [An Attention Mechanism for Robust Multimodal Integration in a Global Workspace Architecture](https://arxiv.org/abs/2602.08597)
*Roland Bertin-Johannet,Lara Scipio,Leopold Maytié,Rufin VanRullen*

Main category: cs.AI

TL;DR: 提出并评估全局工作空间内选择模态的自上而下注意力机制，在两个数据集上验证其增强噪声鲁棒性和泛化能力，使全局工作空间有竞争力。


<details>
  <summary>Details</summary>
Motivation: 现有基于全球工作空间理论（GWT）的多模态集成计算架构中，相关注意力机制研究不足。

Method: 提出并评估一种自上而下的注意力机制，在两个复杂度递增的多模态数据集上进行测试。

Result: 该注意力机制提升了全局工作空间系统的噪声鲁棒性，展现出多种跨任务和跨模态泛化能力，在MM - IMDb 1.0基准上使全局工作空间与现有最优水平竞争。

Conclusion: 所提出的注意力机制对基于GWT的多模态集成计算架构有积极作用。

Abstract: Global Workspace Theory (GWT), inspired by cognitive neuroscience, posits that flexible cognition could arise via the attentional selection of a relevant subset of modalities within a multimodal integration system. This cognitive framework can inspire novel computational architectures for multimodal integration. Indeed, recent implementations of GWT have explored its multimodal representation capabilities, but the related attention mechanisms remain understudied. Here, we propose and evaluate a top-down attention mechanism to select modalities inside a global workspace. First, we demonstrate that our attention mechanism improves noise robustness of a global workspace system on two multimodal datasets of increasing complexity: Simple Shapes and MM-IMDb 1.0. Second, we highlight various cross-task and cross-modality generalization capabilities that are not shared by multimodal attention models from the literature. Comparing against existing baselines on the MM-IMDb 1.0 benchmark, we find our attention mechanism makes the global workspace competitive with the state of the art.

</details>


### [91] [OSCAR: Optimization-Steered Agentic Planning for Composed Image Retrieval](https://arxiv.org/abs/2602.08603)
*Teng Wang,Rong Shan,Jianghao Lin,Junjie Wu,Tianyi Xu,Jianping Zhang,Wenteng Chen,Changwang Zhang,Zhaoxiang Wang,Weinan Zhang,Jun Wang*

Main category: cs.AI

TL;DR: 提出OSCAR框架用于组合图像检索，将智能体CIR从启发式搜索转为轨迹优化问题，实验显示其优于SOTA基线。


<details>
  <summary>Details</summary>
Motivation: 现有组合图像检索方法存在统一嵌入检索的单模型短视和启发式智能体检索的次优试错编排问题。

Method: 将CIR建模为两阶段混合整数规划问题，通过离线阶段推导最优轨迹存于黄金库，用于在线推理时引导VLM规划器。

Result: 在多个基准测试上一致优于SOTA基线，仅用10%训练数据就有优异表现。

Conclusion: OSCAR框架有效，规划逻辑泛化性强，非特定数据集记忆。

Abstract: Composed image retrieval (CIR) requires complex reasoning over heterogeneous visual and textual constraints. Existing approaches largely fall into two paradigms: unified embedding retrieval, which suffers from single-model myopia, and heuristic agentic retrieval, which is limited by suboptimal, trial-and-error orchestration. To this end, we propose OSCAR, an optimization-steered agentic planning framework for composed image retrieval. We are the first to reformulate agentic CIR from a heuristic search process into a principled trajectory optimization problem. Instead of relying on heuristic trial-and-error exploration, OSCAR employs a novel offline-online paradigm. In the offline phase, we model CIR via atomic retrieval selection and composition as a two-stage mixed-integer programming problem, mathematically deriving optimal trajectories that maximize ground-truth coverage for training samples via rigorous boolean set operations. These trajectories are then stored in a golden library to serve as in-context demonstrations for online steering of VLM planner at online inference time. Extensive experiments on three public benchmarks and a private industrial benchmark show that OSCAR consistently outperforms SOTA baselines. Notably, it achieves superior performance using only 10% of training data, demonstrating strong generalization of planning logic rather than dataset-specific memorization.

</details>


### [92] [Debate is efficient with your time](https://arxiv.org/abs/2602.08630)
*Jonah Brown-Cohen,Geoffrey Irving,Simon C. Marshall,Ilan Newman,Georgios Piliouras,Mario Szegedy*

Main category: cs.AI

TL;DR: 本文引入辩论查询复杂度（DQC），发现PSPACE/poly类问题可用O(log n)次查询判定，表明辩论查询效率高，还建立了相关查询复杂度界限，并将其与电路复杂度核心问题相联系。


<details>
  <summary>Details</summary>
Motivation: 以往工作未分析人类监督的实际成本，即法官需对辩论记录进行多少次查询。

Method: 引入辩论查询复杂度（DQC）这一概念进行分析。

Result: PSPACE/poly类问题可通过O(log n)次查询判定；依赖所有输入位的函数需要Omega(log n)次查询；大小为s的电路可计算的函数满足DQC(f) <= log(s) + 3。

Conclusion: 辩论查询效率高，证明P语言中DQC的log(n) + 6下界将产生新的电路下界，将辩论查询复杂度与电路复杂度核心问题相联系。

Abstract: AI safety via debate uses two competing models to help a human judge verify complex computational tasks. Previous work has established what problems debate can solve in principle, but has not analysed the practical cost of human oversight: how many queries must the judge make to the debate transcript? We introduce Debate Query Complexity}(DQC), the minimum number of bits a verifier must inspect to correctly decide a debate.
  Surprisingly, we find that PSPACE/poly (the class of problems which debate can efficiently decide) is precisely the class of functions decidable with O(log n) queries. This characterisation shows that debate is remarkably query-efficient: even for highly complex problems, logarithmic oversight suffices. We also establish that functions depending on all their input bits require Omega(log n) queries, and that any function computable by a circuit of size s satisfies DQC(f) <= log(s) + 3. Interestingly, this last result implies that proving DQC lower bounds of log(n) + 6 for languages in P would yield new circuit lower bounds, connecting debate query complexity to central questions in circuit complexity.

</details>


### [93] [Why do we Trust Chatbots? From Normative Principles to Behavioral Drivers](https://arxiv.org/abs/2602.08707)
*Aditya Gulati,Nuria Oliver*

Main category: cs.AI

TL;DR: 随着聊天机器人模糊人机对话界限，应重新审视用户对其信任机制，提议将其视为推销员并需进一步研究和支持来校准信任。


<details>
  <summary>Details</summary>
Motivation: 聊天机器人模糊人机界限，探讨用户对其信任的基础。

Method: 基于观察，提出重新定义聊天机器人角色进行分析。

Result: 发现'信任'概念并存模糊了心理信任形成和规范可信度的区别。

Conclusion: 需要进一步研究和更强支持机制来帮助用户校准对会话式AI系统的信任。

Abstract: As chatbots increasingly blur the boundary between automated systems and human conversation, the foundations of trust in these systems warrant closer examination. While regulatory and policy frameworks tend to define trust in normative terms, the trust users place in chatbots often emerges from behavioral mechanisms. In many cases, this trust is not earned through demonstrated trustworthiness but is instead shaped by interactional design choices that leverage cognitive biases to influence user behavior. Based on this observation, we propose reframing chatbots not as companions or assistants, but as highly skilled salespeople whose objectives are determined by the deploying organization. We argue that the coexistence of competing notions of "trust" under a shared term obscures important distinctions between psychological trust formation and normative trustworthiness. Addressing this gap requires further research and stronger support mechanisms to help users appropriately calibrate trust in conversational AI systems.

</details>


### [94] [Intermediate Results on the Complexity of STRIPS$_{1}^{1}$](https://arxiv.org/abs/2602.08708)
*Stefan Edelkamp,Jiří Fink,Petr Gregor,Anders Jonsson,Bernhard Nebel*

Main category: cs.AI

TL;DR: 本文基于Bylander关于命题STRIPS规划计算复杂度的研究，探讨设置一个前提条件和一个结果的命题STRIPS规划是否为NP完全问题，并通过调用SAT求解器、引入文字图和映射到Petri网等方法进行研究。


<details>
  <summary>Details</summary>
Motivation: Bylander证明特定条件下规划存在性问题是PSPACE完全，但一个前提条件和一个结果的命题STRIPS规划是否为NP完全未知，本文旨在解决此问题。

Method: 对小实例调用SAT求解器、引入文字图并将其映射到Petri网。

Result: 摘要未提及具体研究结果。

Conclusion: 摘要未提及研究得出的结论。

Abstract: This paper is based on Bylander's results on the computational complexity of propositional STRIPS planning. He showed that when only ground literals are permitted, determining plan existence is PSPACE-complete even if operators are limited to two preconditions and two postconditions. While NP-hardness is settled, it is unknown whether propositional STRIPS with operators that only have one precondition and one effect is NP-complete. We shed light on the question whether this small solution hypothesis for STRIPS$^1_1$ is true, calling a SAT solver for small instances, introducing the literal graph, and mapping it to Petri nets.

</details>


### [95] [Exploring SAIG Methods for an Objective Evaluation of XAI](https://arxiv.org/abs/2602.08715)
*Miquel Miró-Nicolau,Gabriel Moyà-Alcover,Anna Arias-Duart*

Main category: cs.AI

TL;DR: 本文对SAIG方法进行首次综述分析，提出新分类法，揭示XAI评估技术缺乏共识，需进一步研究和标准化。


<details>
  <summary>Details</summary>
Motivation: XAI评估缺乏统一正确的解释真值，客观评估具有挑战性，需有效评估方法。

Method: 提出新的分类法对SAIG方法进行分类，找出区分不同方法的七个关键特征。

Result: 比较研究发现XAI评估技术缺乏有效共识。

Conclusion: 需要对该领域进行更多研究和标准化。

Abstract: The evaluation of eXplainable Artificial Intelligence (XAI) methods is a rapidly growing field, characterized by a wide variety of approaches. This diversity highlights the complexity of the XAI evaluation, which, unlike traditional AI assessment, lacks a universally correct ground truth for the explanation, making objective evaluation challenging. One promising direction to address this issue involves the use of what we term Synthetic Artificial Intelligence Ground truth (SAIG) methods, which generate artificial ground truths to enable the direct evaluation of XAI techniques. This paper presents the first review and analysis of SAIG methods. We introduce a novel taxonomy to classify these approaches, identifying seven key features that distinguish different SAIG methods. Our comparative study reveals a concerning lack of consensus on the most effective XAI evaluation techniques, underscoring the need for further research and standardization in this area.

</details>


### [96] [Finite-State Controllers for (Hidden-Model) POMDPs using Deep Reinforcement Learning](https://arxiv.org/abs/2602.08734)
*David Hudák,Maris F. L. Galesloot,Martin Tappler,Martin Kurečka,Nils Jansen,Milan Češka*

Main category: cs.AI

TL;DR: 提出Lexpop框架解决POMDP问题，扩展其计算HM - POMDPs的鲁棒策略，实验显示在大状态空间问题上优于现有求解器。


<details>
  <summary>Details</summary>
Motivation: 现有POMDP求解器可扩展性有限，且很多场景需跨多个POMDP的鲁棒策略，加剧了可扩展性问题。

Method: 提出Lexpop框架，用深度强化学习训练循环神经网络表示的神经策略，通过高效提取方法构建模仿神经策略的有限状态控制器；扩展Lexpop计算HM - POMDPs的鲁棒策略，关联每个提取的控制器与其最坏情况的POMDP，迭代训练鲁棒神经策略并提取鲁棒控制器。

Result: 在大状态空间问题上，Lexpop优于POMDP和HM - POMDPs的现有求解器。

Conclusion: Lexpop框架在解决POMDP和HM - POMDPs问题上有较好表现，能有效应对可扩展性和鲁棒性问题。

Abstract: Solving partially observable Markov decision processes (POMDPs) requires computing policies under imperfect state information. Despite recent advances, the scalability of existing POMDP solvers remains limited. Moreover, many settings require a policy that is robust across multiple POMDPs, further aggravating the scalability issue. We propose the Lexpop framework for POMDP solving. Lexpop (1) employs deep reinforcement learning to train a neural policy, represented by a recurrent neural network, and (2) constructs a finite-state controller mimicking the neural policy through efficient extraction methods. Crucially, unlike neural policies, such controllers can be formally evaluated, providing performance guarantees. We extend Lexpop to compute robust policies for hidden-model POMDPs (HM-POMDPs), which describe finite sets of POMDPs. We associate every extracted controller with its worst-case POMDP. Using a set of such POMDPs, we iteratively train a robust neural policy and consequently extract a robust controller. Our experiments show that on problems with large state spaces, Lexpop outperforms state-of-the-art solvers for POMDPs as well as HM-POMDPs.

</details>


### [97] [Belief Offloading in Human-AI Interaction](https://arxiv.org/abs/2602.08754)
*Rose E. Guingrich,Dvija Mehta,Umang Bhatt*

Main category: cs.AI

TL;DR: 研究人们从大语言模型获取信息形成信念时的‘信念卸载’现象，给出边界条件、分类及规范含义，并指明未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 探究人们将大语言模型作为思想伙伴进行认知卸载，过度依赖对认知技能产生的不良影响，聚焦人类 - 人工智能交互中的‘信念卸载’。

Method: 借鉴哲学、心理学和计算机科学研究。

Result: 明确了信念卸载发生的边界条件，给出了信念卸载的描述性分类及其规范含义。

Conclusion: 指明了评估人类 - 人工智能交互中信念卸载可能性及后果的未来研究方向。

Abstract: What happens when people's beliefs are derived from information provided by an LLM? People's use of LLM chatbots as thought partners can contribute to cognitive offloading, which can have adverse effects on cognitive skills in cases of over-reliance. This paper defines and investigates a particular kind of cognitive offloading in human-AI interaction, "belief offloading," in which people's processes of forming and upholding beliefs are offloaded onto an AI system with downstream consequences on their behavior and the nature of their system of beliefs. Drawing on philosophy, psychology, and computer science research, we clarify the boundary conditions under which belief offloading occurs and provide a descriptive taxonomy of belief offloading and its normative implications. We close with directions for future work to assess the potential for and consequences of belief offloading in human-AI interaction.

</details>


### [98] [Dynamics Within Latent Chain-of-Thought: An Empirical Study of Causal Structure](https://arxiv.org/abs/2602.08783)
*Zirui Li,Xuefeng Bai,Kehai Chen,Yizhi Li,Jian Yang,Chenghua Lin,Min Zhang*

Main category: cs.AI

TL;DR: 本文将潜思维链视为表征空间中可操纵的因果过程，研究了两个范式在推理任务中的三个关键问题，发现潜步骤预算特性及早期输出偏差与后期表征承诺差距，提出相关分析及目标。


<details>
  <summary>Details</summary>
Motivation: 潜思维链方法的中间计算难以评估，需新方法研究。

Method: 将潜步骤建模为结构因果模型（SCM）中的变量，通过逐步do干预分析其影响，研究两个代表性范式。

Result: 潜步骤预算表现更像具有非局部路由的分阶段功能，早期输出偏差和后期表征承诺存在差距。

Conclusion: 应进行模式条件和稳定性感知分析及相应训练/解码目标，以解释和改进潜推理系统。

Abstract: Latent or continuous chain-of-thought methods replace explicit textual rationales with a number of internal latent steps, but these intermediate computations are difficult to evaluate beyond correlation-based probes. In this paper, we view latent chain-of-thought as a manipulable causal process in representation space by modeling latent steps as variables in a structural causal model (SCM) and analyzing their effects through step-wise $\mathrm{do}$-interventions. We study two representative paradigms (i.e., Coconut and CODI) on both mathematical and general reasoning tasks to investigate three key questions: (1) which steps are causally necessary for correctness and when answers become decidable early; (2) how does influence propagate across steps, and how does this structure compare to explicit CoT; and (3) do intermediate trajectories retain competing answer modes, and how does output-level commitment differ from representational commitment across steps. We find that latent-step budgets behave less like homogeneous extra depth and more like staged functionality with non-local routing, and we identify a persistent gap between early output bias and late representational commitment. These results motivate mode-conditional and stability-aware analyses -- and corresponding training/decoding objectives -- as more reliable tools for interpreting and improving latent reasoning systems.

</details>


### [99] [The Use of AI Tools to Develop and Validate Q-Matrices](https://arxiv.org/abs/2602.08796)
*Kevin Fan,Jacquelyn A. Bialo,Hongli Li*

Main category: cs.AI

TL;DR: 研究探讨AI工具能否支持Q矩阵开发，对比AI生成与有效Q矩阵，结果显示AI模型表现有差异，新AI版本后续分析一致性降低。


<details>
  <summary>Details</summary>
Motivation: 构建Q矩阵在认知诊断建模中关键但费力，研究AI工具能否支持Q矩阵开发。

Method: 让多个AI模型使用与人类专家相同训练材料，用Cohen's kappa评估AI生成Q矩阵、有效Q矩阵和人类评分者Q矩阵的一致性。

Result: 各AI模型表现差异大，Google Gemini 2.5 Pro与有效Q矩阵一致性最高，后续新AI版本一致性降低。

Conclusion: 文中未明确给出结论，但讨论了研究的影响和未来研究方向。

Abstract: Constructing a Q-matrix is a critical but labor-intensive step in cognitive diagnostic modeling (CDM). This study investigates whether AI tools (i.e., general language models) can support Q-matrix development by comparing AI-generated Q-matrices with a validated Q-matrix from Li and Suen (2013) for a reading comprehension test. In May 2025, multiple AI models were provided with the same training materials as human experts. Agreement among AI-generated Q-matrices, the validated Q-matrix, and human raters' Q-matrices was assessed using Cohen's kappa. Results showed substantial variation across AI models, with Google Gemini 2.5 Pro achieving the highest agreement (Kappa = 0.63) with the validated Q-matrix, exceeding that of all human experts. A follow-up analysis in January 2026 using newer AI versions, however, revealed lower agreement with the validated Q-matrix. Implications and directions for future research are discussed.

</details>


### [100] [Root Cause Analysis Method Based on Large Language Models with Residual Connection Structures](https://arxiv.org/abs/2602.08804)
*Liming Zhou,Ailing Liu,Hongwei Liu,Min He,Heng Zhang*

Main category: cs.AI

TL;DR: 提出基于大语言模型的根因分析方法RC - LLM，在CCF - AIOps数据集上实验证明其准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 复杂大规模微服务架构中根因定位有挑战，现有根因分析方法受微服务间复杂故障传播和遥测数据高维性限制。

Method: 提出基于残差连接、使用大语言模型的RC - LLM方法，设计类残差分层融合结构整合多源遥测数据，利用大语言模型上下文推理能力建模时间和跨微服务因果依赖。

Result: 在CCF - AIOps微服务数据集上实验表明，RC - LLM在根因分析中达到了较高的准确性和效率。

Conclusion: RC - LLM方法在微服务架构根因分析中有效。

Abstract: Root cause localization remain challenging in complex and large-scale microservice architectures. The complex fault propagation among microservices and the high dimensionality of telemetry data, including metrics, logs, and traces, limit the effectiveness of existing root cause analysis (RCA) methods. In this paper, a residual-connection-based RCA method using large language model (LLM), named RC-LLM, is proposed. A residual-like hierarchical fusion structure is designed to integrate multi-source telemetry data, while the contextual reasoning capability of large language models is leveraged to model temporal and cross-microservice causal dependencies. Experimental results on CCF-AIOps microservice datasets demonstrate that RC-LLM achieves strong accuracy and efficiency in root cause analysis.

</details>


### [101] [Negative-Aware Diffusion Process for Temporal Knowledge Graph Extrapolation](https://arxiv.org/abs/2602.08815)
*Yanglei Gan,Peng He,Yuxiang Cai,Run Lin,Guanyu Zhou,Qiao Liu*

Main category: cs.AI

TL;DR: 提出NADEx模型用于TKG推理，在四个公共TKG基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在TKG推理中有不足，生成路径仅基于正证据，训练目标以交叉熵排名为主，缺乏对去噪嵌入校准的监督。

Method: 引入NADEx模型，将实体、关系和时间间隔的以主体为中心的历史编码为顺序嵌入，在正向过程中扰动查询对象，用基于时间 - 关系上下文的Transformer去噪器反向重建，还推导了基于批量负原型的余弦对齐正则化器。

Result: 在四个公共TKG基准测试中，NADEx取得了最先进的性能。

Conclusion: NADEx模型有效解决了现有扩散模型在TKG推理中的问题，提升了性能。

Abstract: Temporal Knowledge Graph (TKG) reasoning seeks to predict future missing facts from historical evidence. While diffusion models (DM) have recently gained attention for their ability to capture complex predictive distributions, two gaps remain: (i) the generative path is conditioned only on positive evidence, overlooking informative negative context, and (ii) training objectives are dominated by cross-entropy ranking, which improves candidate ordering but provides little supervision over the calibration of the denoised embedding. To bridge this gap, we introduce Negative-Aware Diffusion model for TKG Extrapolation (NADEx). Specifically, NADEx encodes subject-centric histories of entities, relations and temporal intervals into sequential embeddings. NADEx perturbs the query object in the forward process and reconstructs it in reverse with a Transformer denoiser conditioned on the temporal-relational context. We further derive a cosine-alignment regularizer derived from batch-wise negative prototypes, which tightens the decision boundary against implausible candidates. Comprehensive experiments on four public TKG benchmarks demonstrate that NADEx delivers state-of-the-art performance.

</details>


### [102] [Learning the Value Systems of Societies with Preference-based Multi-objective Reinforcement Learning](https://arxiv.org/abs/2602.08835)
*Andrés Holgado-Sánchez,Peter Vamplew,Richard Dazeley,Sascha Ossowski,Holger Billhardt*

Main category: cs.AI

TL;DR: 提出基于聚类和PbMORL在MDPs中学习价值对齐和价值系统模型的算法，并在含人类价值的MDPs上评估。


<details>
  <summary>Details</summary>
Motivation: 价值感知AI需操作化价值，但易误判，现有序列决策方法有手动设计特征、缺乏可解释性或适应性等问题。

Method: 提出基于聚类和偏好的多目标强化学习（PbMORL）的算法，联合学习社会价值对齐模型和价值系统。

Result: 在两个含人类价值的MDPs上与现有PbMORL算法和基线方法进行评估。

Conclusion: 未明确提及结论，但暗示评估结果对所提出算法有一定验证意义。

Abstract: Value-aware AI should recognise human values and adapt to the value systems (value-based preferences) of different users. This requires operationalization of values, which can be prone to misspecification. The social nature of values demands their representation to adhere to multiple users while value systems are diverse, yet exhibit patterns among groups. In sequential decision making, efforts have been made towards personalization for different goals or values from demonstrations of diverse agents. However, these approaches demand manually designed features or lack value-based interpretability and/or adaptability to diverse user preferences.
  We propose algorithms for learning models of value alignment and value systems for a society of agents in Markov Decision Processes (MDPs), based on clustering and preference-based multi-objective reinforcement learning (PbMORL). We jointly learn socially-derived value alignment models (groundings) and a set of value systems that concisely represent different groups of users (clusters) in a society. Each cluster consists of a value system representing the value-based preferences of its members and an approximately Pareto-optimal policy that reflects behaviours aligned with this value system. We evaluate our method against a state-of-the-art PbMORL algorithm and baselines on two MDPs with human values.

</details>


### [103] [Deciding the Satisfiability of Combined Qualitative Constraint Networks](https://arxiv.org/abs/2602.08848)
*Quentin Cohen-Solal,Alexandre Niveau,Maroua Bouzid*

Main category: cs.AI

TL;DR: 本文提出统一定性形式主义扩展和组合的框架，研究可满足性判定及复杂性，证明相关定理并恢复已知结果，推广定义。


<details>
  <summary>Details</summary>
Motivation: 在人工智能的定性推理中，需要处理各种形式的扩展和组合，以在不精确和不完整信息下推理。

Method: 提出形式化框架，建立两个互补定理。

Result: 实现各种组合和扩展下的推理，研究可满足性判定及复杂性，恢复大小 - 拓扑组合的已知结果。

Conclusion: 该框架可统一研究定性形式主义的各种扩展和组合，推广了定性形式主义的定义。

Abstract: Among the various forms of reasoning studied in the context of artificial intelligence, qualitative reasoning makes it possible to infer new knowledge in the context of imprecise, incomplete information without numerical values. In this paper, we propose a formal framework unifying several forms of extensions and combinations of qualitative formalisms, including multi-scale reasoning, temporal sequences, and loose integrations. This framework makes it possible to reason in the context of each of these combinations and extensions, but also to study in a unified way the satisfiability decision and its complexity. In particular, we establish two complementary theorems guaranteeing that the satisfiability decision is polynomial, and we use them to recover the known results of the size-topology combination. We also generalize the main definition of qualitative formalism to include qualitative formalisms excluded from the definitions of the literature, important in the context of combinations.

</details>


### [104] [Scalable Delphi: Large Language Models for Structured Risk Estimation](https://arxiv.org/abs/2602.08889)
*Tobias Lorenz,Mario Fritz*

Main category: cs.AI

TL;DR: 研究用大语言模型（LLMs）替代结构化专家咨询进行定量风险评估，提出Scalable Delphi方法，评估显示LLMs表现良好，可大幅缩短咨询时间。


<details>
  <summary>Details</summary>
Motivation: 传统德尔菲法进行定量风险评估需数月协调和专家时间，多数应用难以开展严格风险评估，因此研究用LLMs作为可扩展替代方案。

Method: 提出Scalable Delphi方法，适配LLMs；开发基于必要条件的评估框架；在AI增强的网络安全风险领域，用三个能力基准和独立的人类咨询研究进行评估。

Result: LLM小组与基准地面真值强相关，随证据增加系统改善，与人类专家组判断一致，在一次比较中比两个人类专家组更接近。

Conclusion: 基于LLMs的咨询可将结构化专家判断扩展到传统方法不可行的场景，将咨询时间从数月缩短到几分钟。

Abstract: Quantitative risk assessment in high-stakes domains relies on structured expert elicitation to estimate unobservable properties. The gold standard - the Delphi method - produces calibrated, auditable judgments but requires months of coordination and specialist time, placing rigorous risk assessment out of reach for most applications. We investigate whether Large Language Models (LLMs) can serve as scalable proxies for structured expert elicitation. We propose Scalable Delphi, adapting the classical protocol for LLMs with diverse expert personas, iterative refinement, and rationale sharing. Because target quantities are typically unobservable, we develop an evaluation framework based on necessary conditions: calibration against verifiable proxies, sensitivity to evidence, and alignment with human expert judgment. We evaluate in the domain of AI-augmented cybersecurity risk, using three capability benchmarks and independent human elicitation studies. LLM panels achieve strong correlations with benchmark ground truth (Pearson r=0.87-0.95), improve systematically as evidence is added, and align with human expert panels - in one comparison, closer to a human panel than the two human panels are to each other. This demonstrates that LLM-based elicitation can extend structured expert judgment to settings where traditional methods are infeasible, reducing elicitation time from months to minutes.

</details>


### [105] [Efficient and Stable Reinforcement Learning for Diffusion Language Models](https://arxiv.org/abs/2602.08905)
*Jiawei Liu,Xiting Wang,Yuanyuan Zhong,Defu Lian,Yu Yang*

Main category: cs.AI

TL;DR: 提出时空剪枝框架STP改进针对扩散大语言模型的强化学习效率与稳定性，实验表现超现有基线。


<details>
  <summary>Details</summary>
Motivation: 应用强化学习到扩散大语言模型时在效率和稳定性方面面临独特挑战，需解决这些问题。

Method: 提出Spatio - Temporal Pruning (STP)框架，通过空间剪枝利用静态先验约束探索空间，通过时间剪枝绕过冗余后期细化步骤，以压缩生成过程中的冗余。

Result: 理论分析表明STP严格降低了对数似然估计的方差；实验显示STP在效率和准确性上超越了最先进的基线。

Conclusion: STP框架能够有效提高针对扩散大语言模型的强化学习的效率与稳定性。

Abstract: Reinforcement Learning (RL) is crucial for unlocking the complex reasoning capabilities of Diffusion-based Large Language Models (dLLMs). However, applying RL to dLLMs faces unique challenges in efficiency and stability. To address these challenges, we propose Spatio-Temporal Pruning (STP), a framework designed to simultaneously improve the efficiency and stability of RL for dLLMs. STP compresses the redundancy in the generative process through: (1) \textit{spatial pruning}, which constrains the exploration space using static priors; and (2) \textit{temporal pruning}, which bypasses redundant late-stage refinement steps. Our theoretical analysis demonstrates that STP strictly reduces the variance of the log-likelihood estimation, thereby ensuring more stable policy updates. Extensive experiments demonstrate that STP surpasses state-of-the-art baselines in both efficiency and accuracy. Our code is available at https://github.com/Lolo1222/STP.

</details>


### [106] [CausalT5K: Diagnosing and Informing Refusal for Trustworthy Causal Reasoning of Skepticism, Sycophancy, Detection-Correction, and Rung Collapse](https://arxiv.org/abs/2602.08939)
*Longling Geng,Andy Ouyang,Theodore Wu,Daphne Barretto,Matthew John Hayes,Rachael Cooper,Yuqiao Zeng,Sameer Vijay,Gia Ancone,Ankit Rai,Matthew Wolfman,Patrick Flanagan,Edward Y. Chang*

Main category: cs.AI

TL;DR: 本文介绍诊断基准CausalT5K，用于测试大语言模型因果推理能力，揭示失败模式，初步实验展示其价值。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在因果推理存在多种失败情况，但缺乏能进行系统诊断的基准，阻碍修复进展。

Method: 开发含超5000个案例、覆盖10个领域的CausalT5K，将因果陷阱嵌入现实叙事，分解性能指标，通过严格人机协作流程开发。

Result: 初步实验揭示四象限控制格局，静态审计策略普遍失败。

Conclusion: CausalT5K对推进可信推理系统有重要价值。

Abstract: LLM failures in causal reasoning, including sycophancy, rung collapse, and miscalibrated refusal, are well-documented, yet progress on remediation is slow because no benchmark enables systematic diagnosis. We introduce CausalT5K, a diagnostic benchmark of over 5,000 cases across 10 domains that tests three critical capabilities: (1) detecting rung collapse, where models answer interventional queries with associational evidence; (2) resisting sycophantic drift under adversarial pressure; and (3) generating Wise Refusals that specify missing information when evidence is underdetermined. Unlike synthetic benchmarks, CausalT5K embeds causal traps in realistic narratives and decomposes performance into Utility (sensitivity) and Safety (specificity), revealing failure modes invisible to aggregate accuracy. Developed through a rigorous human-machine collaborative pipeline involving 40 domain experts, iterative cross-validation cycles, and composite verification via rule-based, LLM, and human scoring, CausalT5K implements Pearl's Ladder of Causation as research infrastructure. Preliminary experiments reveal a Four-Quadrant Control Landscape where static audit policies universally fail, a finding that demonstrates CausalT5K's value for advancing trustworthy reasoning systems. Repository: https://github.com/genglongling/CausalT5kBench

</details>


### [107] [CoRefine: Confidence-Guided Self-Refinement for Adaptive Test-Time Compute](https://arxiv.org/abs/2602.08948)
*Chen Jin,Ryutaro Tanno,Tom Diethe,Philip Teare*

Main category: cs.AI

TL;DR: 提出CoRefine和CoRefine - Tree方法，用少量token实现有竞争力的推理精度，减少计算量。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在提升推理精度时测试时缩放计算量大，需更高效方法。

Method: 引入CoRefine，通过轻量级控制器根据置信度决定推理步骤；扩展出CoRefine - Tree，自适应平衡探索与利用。

Result: 在多种推理基准和模型上，控制器自信停止时精度达92.6%，相对基线减少约190倍token。

Conclusion: CoRefine将置信度作为控制信号，为可扩展推理和有不完美验证器的代理设置提供模块化基础。

Abstract: Large Language Models (LLMs) often rely on test-time scaling via parallel decoding (for example, 512 samples) to boost reasoning accuracy, but this incurs substantial compute. We introduce CoRefine, a confidence-guided self-refinement method that achieves competitive accuracy using a fraction of the tokens via a lightweight 211k-parameter Conv1D controller atop a frozen LLM. The controller consumes full-trace confidence to decide whether to halt, re-examine, or try a different approach, enabling targeted self-correction with an average of 2.7 refinement steps per problem and roughly 190-fold token reduction relative to 512-sample baselines. Across diverse reasoning benchmarks and three open-source models, the controller achieves 92.6 percent precision when it confidently halts, indicating that confidence dynamics reliably signal correctness without ground-truth verification. We extend this to CoRefine-Tree, a hybrid sequential-parallel variant that adaptively balances exploration and exploitation, with easy serving integration and verifier compatibility. By treating confidence as a control signal rather than a correctness guarantee, CoRefine provides a modular primitive for scalable reasoning and agentic settings with imperfect verifiers.

</details>


### [108] [stable-worldmodel-v1: Reproducible World Modeling Research and Evaluation](https://arxiv.org/abs/2602.08968)
*Lucas Maes,Quentin Le Lidec,Dan Haramati,Nassim Massaudi,Damien Scieur,Yann LeCun,Randall Balestriero*

Main category: cs.AI

TL;DR: 介绍了稳定世界模型（SWM）这一模块化研究生态系统，可解决现有世界模型实现复用性低等问题，并展示其在研究零样本鲁棒性中的应用。


<details>
  <summary>Details</summary>
Motivation: 现有世界模型实现大多特定于出版物，复用性低、易有错误且评估缺乏标准化，需要改进。

Method: 引入稳定世界模型（SWM），它具有模块化、经过测试和文档完善的特点，提供高效数据收集工具、标准化环境、规划算法和基线实现。

Result: 成功构建SWM并使用它研究DINO - WM中的零样本鲁棒性。

Conclusion: SWM这一世界模型研究生态系统具有实用性，能支持相关研究。

Abstract: World Models have emerged as a powerful paradigm for learning compact, predictive representations of environment dynamics, enabling agents to reason, plan, and generalize beyond direct experience. Despite recent interest in World Models, most available implementations remain publication-specific, severely limiting their reusability, increasing the risk of bugs, and reducing evaluation standardization. To mitigate these issues, we introduce stable-worldmodel (SWM), a modular, tested, and documented world-model research ecosystem that provides efficient data-collection tools, standardized environments, planning algorithms, and baseline implementations. In addition, each environment in SWM enables controllable factors of variation, including visual and physical properties, to support robustness and continual learning research. Finally, we demonstrate the utility of SWM by using it to study zero-shot robustness in DINO-WM.

</details>


### [109] [InternAgent-1.5: A Unified Agentic Framework for Long-Horizon Autonomous Scientific Discovery](https://arxiv.org/abs/2602.08990)
*Shiyang Feng,Runmin Ma,Xiangchao Yan,Yue Fan,Yusong Hu,Songtao Huang,Shuaiyu Zhang,Zongsheng Cao,Tianshuo Peng,Jiakang Yuan,Zijie Guo,Zhijie Zhong,Shangheng Du,Weida Wang,Jinxin Shi,Yuhao Zhou,Xiaohan He,Zhiyin Yu,Fangchen Yu,Qihao Zheng,Jiamin Wu,Mianxin Liu,Chi Zhang,Shaowei Hou,Shuya Li,Yankai Jiang,Wenjie Lou,Lilong Wang,Zifu Wang,Jiong Wang,Wanghan Xu,Yue Deng,Dongrui Liu,Yiheng Wang,Wenlong Zhang,Fenghua Ling,Shufei Zhang,Xiaosong Wang,Shuangjia Zheng,Xun Huang,Siqi Sun,Shuyue Hu,Peng Ye,Chunfeng Song,Bin Wang,Conghui He,Yihao Liu,Xin Li,Qibin Hou,Tao Chen,Xiangyu Yue,Bin Wang,Liang He,Dahua Lin,Bowen Zhou,Bo Zhang,Lei Bai*

Main category: cs.AI

TL;DR: 介绍InternAgent - 1.5系统，评估显示其在科学推理基准和发现任务中有出色表现，提供了自主科学发现框架。


<details>
  <summary>Details</summary>
Motivation: 构建一个能跨计算和实证领域进行端到端科学发现的统一系统。

Method: 采用由生成、验证和进化三个子系统组成的结构化架构，具备深度研究、解决方案优化和长时记忆等基础能力。

Result: 在科学推理基准测试中取得领先表现，在算法发现和实证发现任务中表现良好。

Conclusion: InternAgent - 1.5为自主科学发现提供了通用且可扩展的框架。

Abstract: We introduce InternAgent-1.5, a unified system designed for end-to-end scientific discovery across computational and empirical domains. The system is built on a structured architecture composed of three coordinated subsystems for generation, verification, and evolution. These subsystems are supported by foundational capabilities for deep research, solution optimization, and long horizon memory. The architecture allows InternAgent-1.5 to operate continuously across extended discovery cycles while maintaining coherent and improving behavior. It also enables the system to coordinate computational modeling and laboratory experimentation within a single unified system. We evaluate InternAgent-1.5 on scientific reasoning benchmarks such as GAIA, HLE, GPQA, and FrontierScience, and the system achieves leading performance that demonstrates strong foundational capabilities. Beyond these benchmarks, we further assess two categories of discovery tasks. In algorithm discovery tasks, InternAgent-1.5 autonomously designs competitive methods for core machine learning problems. In empirical discovery tasks, it executes complete computational or wet lab experiments and produces scientific findings in earth, life, biological, and physical domains. Overall, these results show that InternAgent-1.5 provides a general and scalable framework for autonomous scientific discovery.

</details>


### [110] [iGRPO: Self-Feedback-Driven LLM Reasoning](https://arxiv.org/abs/2602.09000)
*Ali Hatamizadeh,Shrimai Prabhumoye,Igor Gitman,Ximing Lu,Seungju Han,Wei Ping,Yejin Choi,Jan Kautz*

Main category: cs.AI

TL;DR: 提出iGRPO改进LLMs数学问题求解能力，在推理基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: LLMs解决复杂数学问题准确性和一致性不足，需用强化学习改进。

Method: 引入iGRPO，分两阶段，结合动态自我调节和GRPO式更新。

Result: iGRPO在多基准测试中优于GRPO，在AIME24和AIME25上取得新的最优结果。

Conclusion: 基于迭代、自我反馈的强化学习对推进可验证数学推理有潜力。

Abstract: Large Language Models (LLMs) have shown promise in solving complex mathematical problems, yet they still fall short of producing accurate and consistent solutions. Reinforcement Learning (RL) is a framework for aligning these models with task-specific rewards, improving overall quality and reliability. Group Relative Policy Optimization (GRPO) is an efficient, value-function-free alternative to Proximal Policy Optimization (PPO) that leverages group-relative reward normalization. We introduce Iterative Group Relative Policy Optimization (iGRPO), a two-stage extension of GRPO that adds dynamic self-conditioning through model-generated drafts. In Stage 1, iGRPO samples multiple exploratory drafts and selects the highest-reward draft using the same scalar reward signal used for optimization. In Stage 2, it appends this best draft to the original prompt and applies a GRPO-style update on draft-conditioned refinements, training the policy to improve beyond its strongest prior attempt. Under matched rollout budgets, iGRPO consistently outperforms GRPO across base models (e.g., Nemotron-H-8B-Base-8K and DeepSeek-R1 Distilled), validating its effectiveness on diverse reasoning benchmarks. Moreover, applying iGRPO to OpenReasoning-Nemotron-7B trained on AceReason-Math achieves new state-of-the-art results of 85.62\% and 79.64\% on AIME24 and AIME25, respectively. Ablations further show that the refinement wrapper generalizes beyond GRPO variants, benefits from a generative judge, and alters learning dynamics by delaying entropy collapse. These results underscore the potential of iterative, self-feedback-based RL for advancing verifiable mathematical reasoning.

</details>


### [111] [Data Science and Technology Towards AGI Part I: Tiered Data Management](https://arxiv.org/abs/2602.09003)
*Yudong Wang,Zixuan Fu,Hengyu Zhao,Chen Zhao,Chuyue Zhou,Xinle Lin,Hongya Lyu,Shuaikang Xue,Yi Yi,Yingjiao Wang,Zhi Zheng,Yuzhou Zhang,Jie Zhou,Chaojun Xiao,Xu Han,Zhiyuan Liu,Maosong Sun*

Main category: cs.AI

TL;DR: 当前大语言模型研究在数据方面遇瓶颈，本文提出数据 - 模型协同进化理念和分层数据管理框架，经实验验证有效，还开源相关数据和工具。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型研究过度依赖数据规模单向扩展，在数据可用性、获取成本和训练效率方面面临瓶颈，需要进入数据 - 模型协同进化新阶段。

Method: 提出从L0到L4的分层数据管理框架，在数据管理过程中充分利用大语言模型，并按不同训练阶段分配数据，平衡数据质量、成本和训练效益。

Result: 通过实验建设分层数据集并用于多阶段训练，表明分层数据利用显著提高训练效率和模型性能。

Conclusion: 所提出的分层数据管理框架是一种可扩展且可持续的数据管理系统方法，能有效提升大语言模型训练效果。

Abstract: The development of artificial intelligence can be viewed as an evolution of data-driven learning paradigms, with successive shifts in data organization and utilization continuously driving advances in model capability. Current LLM research is dominated by a paradigm that relies heavily on unidirectional scaling of data size, increasingly encountering bottlenecks in data availability, acquisition cost, and training efficiency. In this work, we argue that the development of AGI is entering a new phase of data-model co-evolution, in which models actively guide data management while high-quality data, in turn, amplifies model capabilities. To implement this vision, we propose a tiered data management framework, designed to support the full LLM training lifecycle across heterogeneous learning objectives and cost constraints. Specifically, we introduce an L0-L4 tiered data management framework, ranging from raw uncurated resources to organized and verifiable knowledge. Importantly, LLMs are fully used in data management processes, such as quality scoring and content editing, to refine data across tiers. Each tier is characterized by distinct data properties, management strategies, and training roles, enabling data to be strategically allocated across LLM training stages, including pre-training, mid-training, and alignment. The framework balances data quality, acquisition cost, and marginal training benefit, providing a systematic approach to scalable and sustainable data management. We validate the effectiveness of the proposed framework through empirical studies, in which tiered datasets are constructed from raw corpora and used across multiple training phases. Experimental results demonstrate that tier-aware data utilization significantly improves training efficiency and model performance. To facilitate further research, we release our tiered datasets and processing tools to the community.

</details>


### [112] [GEBench: Benchmarking Image Generation Models as GUI Environments](https://arxiv.org/abs/2602.09007)
*Haodong Li,Jingwei Wu,Quan Sun,Guopeng Li,Juanxi Tian,Huanyu Zhang,Yanlin Lai,Ruichuan An,Hongbo Peng,Yuhong Dai,Chenxi Li,Chunmei Qing,Jia Wang,Ziyang Meng,Zheng Ge,Xiangyu Zhang,Daxin Jiang*

Main category: cs.AI

TL;DR: 本文引入了用于评估GUI生成的综合基准GEBench和评估指标GE - Score，评估发现当前模型在长交互序列上存在问题，并指出关键瓶颈。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要关注通用领域视觉保真度，缺乏对GUI特定上下文中状态转换和时间连贯性的评估。

Method: 引入GEBench基准，包含700个精选样本涵盖五个任务类别；提出GE - Score五维评估指标。

Result: 当前模型在单步转换表现良好，但在长交互序列中难以保持时间连贯性和空间定位，关键瓶颈是图标解释、文本渲染和定位精度。

Conclusion: 为系统评估提供基础，并为未来构建高保真生成式GUI环境研究指明方向。

Abstract: Recent advancements in image generation models have enabled the prediction of future Graphical User Interface (GUI) states based on user instructions. However, existing benchmarks primarily focus on general domain visual fidelity, leaving the evaluation of state transitions and temporal coherence in GUI-specific contexts underexplored. To address this gap, we introduce GEBench, a comprehensive benchmark for evaluating dynamic interaction and temporal coherence in GUI generation. GEBench comprises 700 carefully curated samples spanning five task categories, covering both single-step interactions and multi-step trajectories across real-world and fictional scenarios, as well as grounding point localization. To support systematic evaluation, we propose GE-Score, a novel five-dimensional metric that assesses Goal Achievement, Interaction Logic, Content Consistency, UI Plausibility, and Visual Quality. Extensive evaluations on current models indicate that while they perform well on single-step transitions, they struggle significantly with maintaining temporal coherence and spatial grounding over longer interaction sequences. Our findings identify icon interpretation, text rendering, and localization precision as critical bottlenecks. This work provides a foundation for systematic assessment and suggests promising directions for future research toward building high-fidelity generative GUI environments. The code is available at: https://github.com/stepfun-ai/GEBench.

</details>


### [113] [Speech Emotion Recognition Leveraging OpenAI's Whisper Representations and Attentive Pooling Methods](https://arxiv.org/abs/2602.06000)
*Ali Shendabadi,Parnia Izadirad,Mostafa Salehi,Mahmoud Bijankhan*

Main category: cs.AI

TL;DR: 本文探讨Whisper在语音情感识别中的能力，提出两种基于注意力的池化方法，在英语和波斯语数据集上实验，取得了较好结果，凸显了Whisper作为特征提取器的潜力。


<details>
  <summary>Details</summary>
Motivation: 语音情感识别研究受限于缺乏标准和足够大的数据集，利用预训练模型提取特征成为趋势，探索Whisper在语音情感识别中的能力。

Method: 提出Multi - head Attentive Average Pooling和QKV Pooling两种基于注意力的池化方法，在IEMOCAP和ShEMO数据集上使用Whisper Tiny和Small进行实验。

Result: 多头部QKV架构在ShEMO数据集上取得了最先进的结果，未加权准确率提高了2.47%，中间层在波斯语数据集上的语音情感识别表现更好。

Conclusion: Whisper作为语音情感识别的特征提取器具有潜力，基于注意力的池化方法在降维方面有效。

Abstract: Speech Emotion Recognition (SER) research has faced limitations due to the lack of standard and sufficiently large datasets. Recent studies have leveraged pre-trained models to extract features for downstream tasks such as SER. This work explores the capabilities of Whisper, a pre-trained ASR system, in speech emotion recognition by proposing two attention-based pooling methods, Multi-head Attentive Average Pooling and QKV Pooling, designed to efficiently reduce the dimensionality of Whisper representations while preserving emotional features. We experiment on English and Persian, using the IEMOCAP and ShEMO datasets respectively, with Whisper Tiny and Small. Our multi-head QKV architecture achieves state-of-the-art results on the ShEMO dataset, with a 2.47% improvement in unweighted accuracy. We further compare the performance of different Whisper encoder layers and find that intermediate layers often perform better for SER on the Persian dataset, providing a lightweight and efficient alternative to much larger models such as HuBERT X-Large. Our findings highlight the potential of Whisper as a representation extractor for SER and demonstrate the effectiveness of attention-based pooling for dimension reduction.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [114] [Modeling Batch Crystallization under Uncertainty Using Physics-informed Machine Learning](https://arxiv.org/abs/2602.07184)
*Dingqi Nai,Huayu Li,Martha Grover,Andrew Medford*

Main category: cs.CE

TL;DR: 研究物理信息循环神经网络（PIRNNs）在结晶过程建模中处理不确定性的有效性，表明其有较强泛化和物理一致性。


<details>
  <summary>Details</summary>
Motivation: 因实际数据非理想性和不确定性，开发结晶过程可靠建模方法具有挑战性，所以研究PIRNNs的有效性。

Method: 利用含受控噪声、溶解度偏移和有限采样的合成数据代表不确定性，将机理种群平衡模型与循环神经网络结合形成PIRNNs。

Result: PIRNNs有强泛化和物理一致性，能稳定学习，准确恢复动力学参数；溶度模型有系统误差时，物理正则化可提升测试性能；大量权重赋予物理信息会增加误差；低采样分辨率下也能恢复模型参数和复制结晶动力学。

Conclusion: 验证了物理信息机器学习处理数据缺陷和领域知识不完整的鲁棒性，为结晶动力学混合建模及工业过程监控提供潜在途径。

Abstract: The development of robust and reliable modeling approaches for crystallization processes is often challenging because of non-idealities in real data arising from various sources of uncertainty. This study investigated the effectiveness of physics-informed recurrent neural networks (PIRNNs) that integrate the mechanistic population balance model with recurrent neural networks under the presence of systematic and model uncertainties. Such uncertainties are represented by using synthetic data containing controlled noise, solubility shift, and limited sampling. The research demonstrates that PIRNNs achieve strong generalization and physical consistency, maintain stable learning behavior, and accurately recover kinetic parameters despite significant stochastic variations in the training data. In the case of systematic errors in the solubility model, the inclusion of physics regularization improved the test performance by more than an order of magnitude compared to purely data-driven models, whereas excessive weighting of physics increased error arising due to the model mismatch. The results also show that PIRNNs are able to recover model parameters and replicate crystallization dynamics even in the limit of very low sampling resolution. These findings validate the robustness of physics-informed machine learning in handling data imperfections and incomplete domain knowledge, providing a potential pathway toward reliable and practical hybrid modeling of crystallization dynamics and industrial process monitoring and control.

</details>


### [115] [Minimum Carbon Trusses: Constructible Multi-Component Designs with Mixed-Integer Linear Programming](https://arxiv.org/abs/2602.07185)
*Zane Hallowell Schemmer,Josephine Voigt Carstensen*

Main category: cs.CE

TL;DR: 本文提出新的混合整数线性规划解决桁架优化问题，考虑环境影响与可建造性的相互作用，结果显示可建造性约束会显著改变最低碳排放设计。


<details>
  <summary>Details</summary>
Motivation: 桁架优化领域需解决构建高度优化且复杂设计的挑战，同时考虑建筑的碳排放限制。

Method: 制定并求解新的混合整数线性规划，考虑材料兼容性和本构定律，允许使用多种材料和结构组件，设置截面面积边界并约束结构连接复杂性。

Result: 应用可建造性约束时最低碳排放设计显著变化，引入低碳材料对环境性能影响不同，有的几乎无影响，有的改善近29%。

Conclusion: 需要综合优化方法来平衡环境影响和可建造性。

Abstract: Truss optimization is a rich research field receiving renewed interest in limiting the carbon emissions of construction. However, a persistent challenge has been to construct highly optimized and often complex designs. This contribution formulates and solves new mixed-integer linear programs that enable consideration of the interplay between environmental impact and constructability. Specifically, the design engineer is enabled to design with multiple materials and/or structural components, apply separate minimum and maximum cross-sectional area bounds, and constrain the complexity of the structural connections. This is done while explicitly considering compatibility and constitutive laws. The results demonstrate that the lowest embodied carbon designs change significantly when constructability constraints are applied, highlighting the need for an integrated optimization approach. In one example, introducing a lower-carbon material option has almost no effect on the environmental performance, whereas another sees an improvement of nearly 29%. The extensibility of the formulation to design with three component types and additional constraints is demonstrated for a prestressed tensegrity example.

</details>


### [116] [Fin-RATE: A Real-world Financial Analytics and Tracking Evaluation Benchmark for LLMs on SEC Filings](https://arxiv.org/abs/2602.07294)
*Yidong Jiang,Junrong Chen,Eftychia Makri,Jialin Chen,Peiwen Li,Ali Maatouk,Leandros Tassiulas,Eliot Brenner,Bing Xiang,Rex Ying*

Main category: cs.CE

TL;DR: 为解决现有金融领域大语言模型基准测试的不足，引入Fin - RATE基准测试，对17个领先模型进行评估，结果显示任务复杂度提升时模型性能下降。


<details>
  <summary>Details</summary>
Motivation: 现有金融领域大语言模型基准测试聚焦孤立细节，未反映专业分析复杂性，也难以定位性能瓶颈。

Method: 引入基于美国证券交易委员会文件的Fin - RATE基准测试，从三个途径模拟金融分析师工作流程，对17个领先模型在不同设置下进行评估。

Result: 任务从单文档推理转向纵向和跨实体分析时，准确率分别下降18.60%和14.35%，存在比较幻觉、时间和实体不匹配等问题。

Conclusion: 现有基准测试未对模型在复杂任务中的局限性进行正式分类和量化。

Abstract: With increasing deployment of Large Language Models (LLMs) in the finance domain, LLMs are increasingly expected to parse complex regulatory disclosures. However, existing benchmarks often focus on isolated details, failing to reflect the complexity of professional analysis that requires synthesizing information across multiple documents, reporting periods, and corporate entities. They do not distinguish whether errors stem from retrieval failures, generation flaws, finance-specific reasoning mistakes, or misunderstanding of the query or context. This makes it difficult to pinpoint performance bottlenecks. To bridge these gaps, we introduce Fin-RATE, a benchmark built on U.S. Securities and Exchange Commission (SEC) filings and mirror financial analyst workflows through three pathways: detail-oriented reasoning within individual disclosures, cross-entity comparison under shared topics, and longitudinal tracking of the same firm across reporting periods. We benchmark 17 leading LLMs, spanning open-source, closed-source, and finance-specialized models, under both ground-truth context and retrieval-augmented settings. Results show substantial performance degradation, with accuracy dropping by 18.60% and 14.35% as tasks shift from single-document reasoning to longitudinal and cross-entity analysis. This is driven by rising comparison hallucinations, time and entity mismatches, and mirrored by declines in reasoning and factuality--limitations that prior benchmarks have yet to formally categorize or quantify.

</details>


### [117] [Probabilistic Modeling of Venture Capital Portfolio Outliers](https://arxiv.org/abs/2602.07761)
*Kensei Sakamoto,Hasan Ugur Koyluoglu,Fuat Alican,Yigit Ihlamur*

Main category: cs.CE

TL;DR: 本文定义了基于单个投资的异常值概率和投资间相关性的风投组合绩效概率度量，模型显示仅用预期异常值计数评估风投组合不足，揭示了可靠性与聚类成功幅度间的权衡。


<details>
  <summary>Details</summary>
Motivation: 受银行贷款组合违约风险建模启发，旨在为风投组合绩效定义合适的概率度量。

Method: 计算N个通过共同因素相关的非同质布尔结果总和的概率分布，实施潜在因子模型，采用模拟方法并使用经验估计的相关矩阵。

Result: 对合成投资组合应用模型表明，相同预期结果的投资组合在不同相关性下可靠性和风险差异大，分散投资可提高实现最少异常值数量的概率但上限降低。

Conclusion: 该框架为交易层面异常值概率评估和目标导向的投资组合构建提供了实用桥梁。

Abstract: In this paper, we define probabilistic measures for venture portfolio performance based on individual outlier probability for each investment and the dependence across investments. This work is inspired by loan portfolio modeling against default risk used in banking. In mathematical terms, we calculate the probability distribution of the sum of N non-homogeneous Boolean outcomes (investments becoming outliers) that are correlated through common factors such as overall market conditions and sector effects. Specifically, we implemented a latent-factor model in which each investment's success is the exceedance of a Gaussian latent variable composed of idiosyncratic returns and returns from interpretable shared factors (stock markets, industry sector indices, geography and founder type). The formulation follows a simulation approach to preserve heterogeneous deal-level success probabilities and uses empirically estimated correlation matrices. When applied to synthetic portfolios, our model reveals that expected outlier counts alone are insufficient statistics for evaluating venture portfolios. Portfolios with identical expected outcomes can exhibit drastically different levels of reliability and risk when various levels and forms of correlation are embedded. Diversification improves the probability of achieving a minimum number of outliers by reducing exposure to common shocks, but at the cost of lower upside, underscoring a fundamental tradeoff between reliability and magnitude of clustered successes. The framework provides a practical bridge between deal-level outlier probability assessment and objective-aware portfolio construction.

</details>


### [118] [Learning-guided Kansa collocation for forward and inverse PDEs beyond linearity](https://arxiv.org/abs/2602.07970)
*Zheyuan Hu,Weitao Chen,Cengiz Öztireli,Chenliang Zhou,Fangcheng Zhong*

Main category: cs.CE

TL;DR: 探讨不同PDE求解器优缺点并应用于特定科学模拟问题，扩展CNF框架求解器并开展相关研究。


<details>
  <summary>Details</summary>
Motivation: 数值方法在求解偏微分方程时存在维数灾难、高计算成本和特定领域离散化问题，需要探索不同PDE求解器。

Method: 探索不同PDE求解器，将近期CNF框架求解器拓展到多因变量和非线性设置及下游应用。

Result: 实现选定方法、自调优技术，在基准问题上进行评估，对神经PDE求解器和科学模拟应用进行全面调查。

Conclusion: 未提及明确结论。

Abstract: Partial Differential Equations are precise in modelling the physical, biological and graphical phenomena. However, the numerical methods suffer from the curse of dimensionality, high computation costs and domain-specific discretization. We aim to explore pros and cons of different PDE solvers, and apply them to specific scientific simulation problems, including forwarding solution, inverse problems and equations discovery. In particular, we extend the recent CNF (NeurIPS 2023) framework solver to multi-dependent-variable and non-linear settings, together with down-stream applications. The outcomes include implementation of selected methods, self-tuning techniques, evaluation on benchmark problems and a comprehensive survey of neural PDE solvers and scientific simulation applications.

</details>


### [119] [A Machine Learning Enabled MDO for Bio-Inspired Autonomous Underwater Gliders](https://arxiv.org/abs/2602.08508)
*Andrea Serani,Giorgio Palma,Jeroen Wackers,Matteo Diez*

Main category: cs.CE

TL;DR: 提出基于机器学习的双层多学科设计优化框架用于蝠鲼启发的AUG设计，优化后水动力效率提升、空重降低。


<details>
  <summary>Details</summary>
Motivation: AUG初步设计因多因素强耦合而具挑战性，仿生构型设计空间维度高，传统优化方法难探索。

Method: 采用双层多学科设计优化框架，上层在降维设计空间探索外部几何，下层确定内部尺寸；采用多保真替代模型优化策略。

Result: 优化构型水动力最大效率提升14.7%，空重降低12.8%，满足所有约束条件。

Conclusion: 物理驱动降维和多保真机器学习的集成可实现复杂仿生水下航行器可扩展且物理一致的多学科设计优化。

Abstract: The preliminary design of AUGs is intrinsically challenging due to the strong coupling between the external hydrodynamic shape, the hydrostatic balance, the structural integrity, and internal packaging constraints. This complexity is further amplified for bio-inspired configurations, whose rich geometric parametrizations lead to high-dimensional design spaces that are difficult to explore using conventional optimization approaches. This work presents a ML-enabled bi-level multidisciplinary design optimization (MDO) framework for the performance-driven design of a manta-ray-inspired AUG. At the upper level, hydrodynamically efficient external geometries are explored in a reduced design space obtained through physics-driven parametric model embedding, which identifies a low-dimensional latent representation directly correlated with the lift, drag, and pressure distributions. At the lower level, a constrained internal sizing problem determines the minimum feasible empty weight by accounting for structural, hydrostatic, geometric, and payload constraints. To render the resulting bi-level problem computationally tractable, a multi-fidelity surrogate-based optimization strategy is adopted, combining low- and high-fidelity hydrodynamic models with stochastic radial basis function surrogates and adaptive Bayesian sampling. The framework enables efficient exploration of the coupled design space while rigorously managing model uncertainty and computational cost. The optimized configurations exhibit a 14.7\% improvement in maximum hydrodynamic efficiency and a 12.8\% reduction in empty weight relative to the baseline design, while satisfying all disciplinary constraints. These results demonstrate that the integration of physics-driven dimensionality reduction and multi-fidelity machine learning enables scalable and physically consistent MDO of complex bio-inspired underwater vehicles.

</details>


### [120] [The M-Tensor Format: Optimality in High Dimensional Regression for Nonlinear Models with Scarce Data](https://arxiv.org/abs/2602.08509)
*Rémi Cloarec,Sebastian Rodriguez,Xavier Kestelyn,Francisco Chinesta*

Main category: cs.CE

TL;DR: 提出适用于高维稀缺数据的非线性回归框架，结合核属性与张量代数，避免维数灾难，展示复杂度缩放。


<details>
  <summary>Details</summary>
Motivation: 解决高维数据稀缺情况下的非线性回归问题，避免维数灾难。

Method: 利用部分张量积（m - 张量积）的代数性质，结合核属性与张量代数，采用不同正则化技术。

Result: 能处理数百个参数的近似问题，避免定点策略，在一般基准和动力系统上展示复杂度缩放。

Conclusion: 该框架对工程问题具有鲁棒性且易于实现。

Abstract: We present a nonlinear regression framework based on tensor algebra tailored to high dimensional contexts where data is scarce. We exploit algebraic properties of a partial tensor product, namely the m-tensor product, to leverage structured equations with separated variables. The proposed method combines kernel properties along with tensor algebra to prevent the curse of dimensionality and tackle approximations up to hundreds of parameters while avoiding the fixed point strategy. This formalism allows us to provide different regularization techniques fit for low amount of data with a high number of parameters while preserving well-known matrix-based properties. We demonstrate complexity scaling on a general benchmark and dynamical systems to show robustness for engineering problems and ease of implementation.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [121] [KRONE: Hierarchical and Modular Log Anomaly Detection](https://arxiv.org/abs/2602.07303)
*Lei Ma,Jinyang Liu,Tieying Zhang,Peter M. VanNostrand,Dennis M. Hofmann,Lei Cao,Elke A. Rundensteiner,Jianjun Chen*

Main category: cs.DB

TL;DR: 提出KRONE层级异常检测框架，自动从扁平日志中提取执行层级进行模块化多级异常检测，实验表明其在多方面有提升。


<details>
  <summary>Details</summary>
Motivation: 现有日志按扁平序列存储丢失嵌套组件执行结构，导致现有方法易误判依赖关系，需要有效异常检测方法。

Method: 构建KRONE Log Abstraction Model 从日志数据提取语义层级，将日志序列分解为KRONE Seqs；采用混合模块化检测机制，结合Local - Context和Nested - Aware检测器；通过缓存结果复用和提前退出策略优化检测。

Result: 在三个公开基准和一个工业数据集上，KRONE在检测准确率、F1分数、数据效率、资源效率和可解释性上有持续提升，F1分数比先前方法高超10个百分点，减少大语言模型使用。

Conclusion: KRONE是有效的日志异常检测框架，能提升检测效果。

Abstract: Log anomaly detection is crucial for uncovering system failures and security risks. Although logs originate from nested component executions with clear boundaries, this structure is lost when they are stored as flat sequences. As a result, state-of-the-art methods risk missing true dependencies within executions while learning spurious ones across unrelated events. We propose KRONE, the first hierarchical anomaly detection framework that automatically derives execution hierarchies from flat logs for modular multi-level anomaly detection. At its core, the KRONE Log Abstraction Model captures application-specific semantic hierarchies from log data. This hierarchy is then leveraged to recursively decompose log sequences into multiple levels of coherent execution chunks, referred to as KRONE Seqs, transforming sequence-level anomaly detection into a set of modular KRONE Seq-level detection tasks. For each test KRONE Seq, KRONE employs a hybrid modular detection mechanism that dynamically routes between an efficient level-independent Local-Context detector, which rapidly filters normal sequences, and a Nested-Aware detector that incorporates cross-level semantic dependencies and supports LLM-based anomaly detection and explanation. KRONE further optimizes hierarchical detection through cached result reuse and early-exit strategies. Experiments on three public benchmarks and one industrial dataset from ByteDance Cloud demonstrate that KRONE achieves consistent improvements in detection accuracy, F1-score, data efficiency, resource efficiency, and interpretability. KRONE improves the F1-score by more than 10 percentage points over prior methods while reducing LLM usage to only a small fraction of the test data.

</details>


### [122] [Learned Query Optimizer in Alibaba MaxCompute: Challenges, Analysis, and Solutions](https://arxiv.org/abs/2602.07336)
*Lianggui Weng,Dandan Liu,Wenzhuang Zhu,Rong Zhu,Junzheng Zheng,Bolin Ding,Zhiguo Zhang,Jingren Zhou*

Main category: cs.DB

TL;DR: 现有学习型查询优化器不适用于分布式多租户数据仓库，本文提出LOAM框架解决相关挑战，在生产工作负载上节省CPU成本。


<details>
  <summary>Details</summary>
Motivation: 现有学习型查询优化器因理想化假设和设计选择，不适合现代分布式多租户数据仓库，存在四大挑战，需重新设计。

Method: 提出LOAM框架，采用无统计信息的计划编码，为在线查询提供性能边界和策略，集成领域适应技术，包含轻量级项目选择器。

Result: LOAM在生产工作负载上比MaxCompute原生查询优化器节省高达30%的CPU成本。

Conclusion: LOAM的设计原则和技术可推广到类似系统，能有效解决学习型查询优化器在实际部署中的问题。

Abstract: Existing learned query optimizers remain ill-suited to modern distributed, multi-tenant data warehouses due to idealized modeling assumptions and design choices. Using Alibaba's MaxCompute as a representative, we surface four fundamental, system-agnostic challenges for any deployable learned query optimizer: 1) highly dynamic execution environments that induce large variance in plan costs; 2) potential absence of input statistics needed for cost estimation; 3) infeasibility of conventional model refinement; and 4) uncertain benefits across different workloads. These challenges expose a deep mismatch between theoretical advances and production realities and demand a principled, deployment-first redesign of learned optimizers.
  To bridge this gap, we present LOAM, a one-stop learned query optimization framework for MaxCompute. Its design principles and techniques generalize and are readily adaptable to similar systems. Architecturally, LOAM introduces a statistics-free plan encoding that leverages operator semantics and historical executions to infer details about data distributions and explicitly encodes the execution environments of training queries to learn their impacts on plan costs. For online queries with unknown environments at prediction time, LOAM provides a theoretical bound on the achievable performance and a practical strategy to smooth the environmental impacts on cost estimations. For system operating, LOAM integrates domain adaptation techniques into training to generalize effectively to online query plans without requiring conventional refinement. Additionally, LOAM includes a lightweight project selector to prioritize high-benefit deployment projects. LOAM has seen up to 30% CPU cost savings over MaxCompute's native query optimizer on production workloads, which could translate to substantial real-world resource savings.

</details>


### [123] [DeepPrep: An LLM-Powered Agentic System for Autonomous Data Preparation](https://arxiv.org/abs/2602.07371)
*Meihao Fan,Ju Fan,Yuxin Zhang,Shaolei Zhang,Xiaoyong Du,Jie Song,Peng Li,Fuxin Jiang,Tieying Zhang,Jianjun Chen*

Main category: cs.DB

TL;DR: 现有LLM方法在自动化数据准备中有不足，本文提出DeepPrep系统，通过迭代交互和树状推理构建数据准备管道，实验表明其效果好且成本低。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的数据准备方法存在不基于中间执行结果做决策、线性交互难以修正早期决策的问题。

Method: 提出DeepPrep系统，通过迭代、基于执行的与环境交互构建数据准备管道；用树状的agentic推理组织管道构建；提出渐进式agentic训练框架和数据合成方法。

Result: DeepPrep数据准备准确率与GPT - 5相当，推理成本低15倍，在开源基线中达最优，能有效泛化。

Conclusion: DeepPrep有效解决了现有方法的局限性，在数据准备任务中表现出色。

Abstract: Data preparation, which aims to transform heterogeneous and noisy raw tables into analysis-ready data, remains a major bottleneck in data science. Recent approaches leverage large language models (LLMs) to automate data preparation from natural language specifications. However, existing LLM-powered methods either make decisions without grounding in intermediate execution results, or rely on linear interaction processes that offer limited support for revising earlier decisions. To address these limitations, we propose DeepPrep, an LLM-powered agentic system for autonomous data preparation. DeepPrep constructs data preparation pipelines through iterative, execution-grounded interaction with an environment that materializes intermediate table states and returns runtime feedback. To overcome the limitations of linear interaction, DeepPrep organizes pipeline construction with tree-based agentic reasoning, enabling structured exploration and non-local revision based on execution feedback. To enable effective learning of such behaviors, we propose a progressive agentic training framework, together with data synthesis that supplies diverse and complex ADP tasks. Extensive experiments show that DeepPrep achieves data preparation accuracy comparable to strong closed-source models (e.g., GPT-5) while incurring 15x lower inference cost, while establishing state-of-the-art performance among open-source baselines and generalizing effectively across diverse datasets.

</details>


### [124] [Building an OceanBase-based Distributed Nearly Real-time Analytical Processing Database System](https://arxiv.org/abs/2602.07584)
*Quanqing Xu,Chuanhui Yang,Ruijie Li,Dongdong Xie,Hui Cao,Yi Xiao,Junquan Chen,Yanzuo Wang,Saitong Zhao,Fusheng Han,Bin Liu,Guoping Wang,Yuzhong Zhao,Mingqiang Zhuang*

Main category: cs.DB

TL;DR: 提出适用于PB级数据的OLAP系统OceanBase Mercury，其在查询延迟上优于专业OLAP引擎，能平衡大数据环境下分析深度与操作灵活性。


<details>
  <summary>Details</summary>
Motivation: 现代数据基础设施需能高效管理海量数据、实现实时事务处理和高级分析能力的数据库系统，传统OLAP和新兴实时分析处理系统存在不足。

Method: 引入支持PB级数据的OceanBase Mercury系统，采用分布式、多租户架构，有自适应列存储格式、物化视图差异刷新机制和多态向量化引擎三个关键组件。

Result: 在实际工作负载下的实证评估表明，OceanBase Mercury查询延迟比专业OLAP引擎快1.3-3.1倍，且能保持亚秒级延迟。

Conclusion: OceanBase Mercury是一个开创性的分析处理解决方案，能有效平衡大数据环境中分析深度与操作灵活性。

Abstract: The growing demand for database systems capable of efficiently managing massive datasets while delivering real-time transaction processing and advanced analytical capabilities has become critical in modern data infrastructure. While traditional OLAP systems often fail to meet these dual requirements, emerging real-time analytical processing systems still face persistent challenges, such as excessive data redundancy, complex cross-system synchronization, and suboptimal temporal efficiency. This paper introduces OceanBase Mercury as an innovative OLAP system designed for petabyte-scale data. The system features a distributed, multi-tenant architecture that ensures essential enterprise-grade requirements, including continuous availability and elastic scalability. Our technical contributions include three key components: (1) an adaptive columnar storage format with hybrid data layout optimization, (2) a differential refresh mechanism for materialized views with temporal consistency guarantees, and (3) a polymorphic vectorization engine supporting three distinct data formats. Empirical evaluations under real-world workloads demonstrate that OceanBase Mercury outperforms specialized OLAP engines by 1.3X to 3.1X speedup in query latency while maintaining sub-second latency, positioning it as a groundbreaking AP solution that effectively balances analytical depth with operational agility in big data environments.

</details>


### [125] [How to evaluate NoSQL Database Paradigms for Knowledge Graph Processing](https://arxiv.org/abs/2602.07612)
*Rosario Napoli,Antonio Celesti,Massimo Villari,Maria Fazio*

Main category: cs.DB

TL;DR: 传统性能评估无法应对真实知识图谱工作负载，本文提出特定基准框架，评估不同数据库范式并给出选择指南。


<details>
  <summary>Details</summary>
Motivation: 传统性能评估无法捕捉真实知识图谱工作负载复杂性，知识图谱领域数据库管理系统选择缺乏系统指导。

Method: 提出知识图谱特定基准框架，采用连通性密度、规模，引入语义丰富度（SR）指标，使用四层查询方法。

Result: 对FAERS不良事件知识图谱在三个规模上进行实证评估，对比不同查询范式。

Conclusion: 提供基于指标和证据的指南，帮助根据图的大小、连通性和语义丰富度选择NoSQL范式。

Abstract: Knowledge Graph (KG) processing faces critical infrastructure challenges in selecting optimal NoSQL database paradigms, as traditional performance evaluations rely on static benchmarks that fail to capture the complexity of real-world KG workloads. Although the big data field offers numerous comparative studies, in the KG context DBMS selection remains predominantly ad-hoc, leaving practitioners without systematic guidance for matching storage technologies to specific KG characteristics and query requirements. This paper presents a KG-specific benchmarking framework that employs connectivity density, scale, and introduces a graph-centric metric, namely Semantic Richness (SR), within a four-tier query methodology to reveal performance crossover points across Document-Oriented, Graph, and Multi-Model DBMSs. We conduct an empirical evaluation on the FAERS adverse event KG at three scales, comparing paradigms from simple filtering to deep traversal, and provide metric-driven, evidence-based guidelines for aligning NoSQL paradigm selection with graph size, connectivity, and semantic richness.

</details>


### [126] [Nexus: Inferring Join Graphs from Metadata Alone via Iterative Low-Rank Matrix Completion](https://arxiv.org/abs/2602.08186)
*Tianji Cong,Yuanyuan Tian,Andreas Mueller,Rathijit Sen,Yeye He,Fotis Psallidas,Shaleen Deep,H. V. Jagadish*

Main category: cs.DB

TL;DR: 本文提出仅使用元数据进行连接图推理的问题，基于连接图邻接矩阵特性将其建模为低秩矩阵补全问题，提出Nexus解决方案并结合新算法，实验显示其显著优于现有方法且有快速模式。


<details>
  <summary>Details</summary>
Motivation: 自动推断连接关系对数据处理很关键，但在复杂模式下准确高效识别有挑战，尤其是企业环境中数据值访问受限，因此研究仅用元数据进行连接图推理。

Method: 对大量真实模式进行实证研究，发现连接图邻接矩阵的高稀疏和低秩特性，将连接图推理建模为低秩矩阵补全问题，提出Nexus解决方案，采用一种新的期望最大化算法结合大语言模型优化。

Result: Nexus在四个数据集包括一个真实生产数据集上显著优于现有方法，还有快速模式可实现高达6倍加速。

Conclusion: Nexus为现实部署提供了实用高效的解决方案。

Abstract: Automatically inferring join relationships is a critical task for effective data discovery, integration, querying and reuse. However, accurately and efficiently identifying these relationships in large and complex schemas can be challenging, especially in enterprise settings where access to data values is constrained. In this paper, we introduce the problem of join graph inference when only metadata is available. We conduct an empirical study on a large number of real-world schemas and observe that join graphs when represented as adjacency matrices exhibit two key properties: high sparsity and low-rank structure. Based on these novel observations, we formulate join graph inference as a low-rank matrix completion problem and propose Nexus, an end-to-end solution using only metadata. To further enhance accuracy, we propose a novel Expectation-Maximization algorithm that alternates between low-rank matrix completion and refining join candidate probabilities by leveraging Large Language Models. Our extensive experiments demonstrate that Nexus outperforms existing methods by a significant margin on four datasets including a real-world production dataset. Additionally, Nexus can operate in a fast mode, providing comparable results with up to 6x speedup, offering a practical and efficient solution for real-world deployments.

</details>


### [127] [ZipFlow: a Compiler-based Framework to Unleash Compressed Data Movement for Modern GPUs](https://arxiv.org/abs/2602.08190)
*Gwangoo Yeo,Zhiyang Shen,Wei Cui,Matteo Interlandi,Rathijit Sen,Bailu Ding,Qi Chen,Minsoo Rhu*

Main category: cs.DB

TL;DR: 提出ZipFlow框架优化GPU加速数据分析中压缩数据传输，在TPC - H基准测试上有显著性能提升。


<details>
  <summary>Details</summary>
Motivation: GPU加速数据分析中，当数据规模超过GPU内存容量，受限于PCIe带宽，CPU到GPU的数据传输开销成为性能瓶颈，需优化数据压缩、传输和解压流程以平衡I/O延迟和计算开销。

Method: 将压缩算法按内在并行性分为三种模式，针对每种模式采用通用调度策略，以充分利用不同架构GPU的计算能力。

Result: 在行业标准基准TPC - H上评估，相比最先进的GPU压缩库（nvCOMP）平均提升2.08倍，相比基于CPU的查询处理引擎（如DuckDB）加速3.14倍。

Conclusion: ZipFlow可以实现灵活、高性能和整体性的优化，大幅提升端到端的数据传输能力。

Abstract: In GPU-accelerated data analytics, the overhead of data transfer from CPU to GPU becomes a performance bottleneck when the data scales beyond GPU memory capacity due to the limited PCIe bandwidth. Data compression has come to rescue for reducing the amount of data transfer while taking advantage of the powerful GPU computation for decompression. To optimize the end-to-end query performance, however, the workflow of data compression, transfer, and decompression must be holistically designed based on the compression strategies and hardware characteristics to balance the I/O latency and computational overhead. In this work, we present ZipFlow, a compiler-based framework for optimizing compressed data transfer in GPU-accelerated data analytics. ZipFlow classifies compression algorithms into three distinct patterns based on their inherent parallelism. For each pattern, ZipFlow employs generalized scheduling strategies to effectively exploit the computational power of GPUs across diverse architectures. Building on these patterns, ZipFlow delivers flexible, high-performance, and holistic optimization, which substantially advances end-to-end data transfer capabilities. We evaluate the effectiveness of ZipFlow on industry-standard benchmark, TPC-H. Overall, ZipFlow achieves an average improvement of 2.08 times over the state-of-the-art GPU compression library (nvCOMP) and 3.14 times speedup against CPU-based query processing engines (e.g., DuckDB).

</details>


### [128] [ByteHouse: A Cloud-Native OLAP Engine with Incremental Computation and Multi-Modal Retrieval](https://arxiv.org/abs/2602.08226)
*Yuxing Han,Yu Lin,Yifeng Dong,Xuanhe Zhou,Xindong Peng,Xinhui Tian,Zhiyuan You,Yingzhong Guo,Xi Chen,Weiping Qu,Tao Meng,Dayue Gao,Haoyu Wang,Liuxi Wei,Huanchen Zhang,Fan Wu*

Main category: cs.DB

TL;DR: 面对现代企业需求及现有系统缺陷，介绍云原生数据仓库 ByteHouse，评估显示其比现有系统效率显著提升。


<details>
  <summary>Details</summary>
Motivation: 现代企业需要高效、多模式且经济的数据分析基础设施，而字节跳动生产环境中现有系统存在 I/O 效率低、查询优化不灵活和资源分离导致性能下降等问题。

Method: 存储层集成统一表引擎、SSD 集群规模缓存和虚拟文件系统；计算层支持多种执行模式并针对混合查询优化；控制层协调全局元数据和事务，有基于历史执行轨迹和 AI 辅助的优化器。

Result: 在内部和标准工作负载评估中，ByteHouse 比现有系统实现了显著的效率提升。

Conclusion: ByteHouse 能有效应对现有系统的不足，为实时多模态数据分析提供更高效的解决方案。

Abstract: With the rapid rise of intelligent data services, modern enterprises increasingly require efficient, multimodal, and cost-effective data analytics infrastructures. However, in ByteDance's production environments, existing systems fall short due to limitations such as I/O-inefficient multimodal storage, inflexible query optimization (e.g., failing to optimize multimodal access patterns), and performance degradation caused by resource disaggregation (e.g., loss of data locality in remote storage). To address these challenges, we introduce ByteHouse (https://bytehouse.cloud), a cloud-native data warehouse designed for real-time multimodal data analytics. The storage layer integrates a unified table engine that provides a two-tier logical abstraction and physically consistent layout, SSD-backed cluster-scale cache (CrossCache) that supports shared caching across compute nodes, and virtual file system (NexusFS) that enable efficient local access on compute nodes. The compute layer supports analytical, batch, and incremental execution modes, with tailored optimizations for hybrid queries (e.g., runtime filtering over tiered vector indexes). The control layer coordinates global metadata and transactions, and features an effective optimizer enhanced by historical execution traces and AI-assisted plan selection. Evaluations on internal and standard workloads show that ByteHouse achieves significant efficiency improvement over existing systems.

</details>


### [129] [Making Databases Searchable with Deep Context](https://arxiv.org/abs/2602.08320)
*Alekh Jindal,Shi Qiao,Shivani Tripathi,Niloy Debnath,Kunal Singh,Pushpanjali Nema,Sharath Prakash,Aditya Halder,Ronith PR,Sadiq Mohammed,Abdul Hameed,Karan Hanswadkar,Ayush Kshitij,Sarthak Bhatt,Rony Chatterjee,Jyoti Pandey,Christina Pavlopoulou,Ravi Shetye*

Main category: cs.DB

TL;DR: 本文介绍Tursio搜索平台，它通过构建语义知识图利用大语言模型让数据库可进行自然语言搜索，经评估高效准确可扩展。


<details>
  <summary>Details</summary>
Motivation: 企业数据库重要却难被决策者使用，希望让数据库能被自然语言搜索。

Method: 构建抽象层（语义知识图），在查询处理栈各部分融入大语言模型，用传统查询规划和重写技术处理自然语言查询。

Result: 在生产负载、合成和现实基准测试中表现良好，实现高精度，高效且可扩展。

Conclusion: Tursio使非专业用户可真正对数据库进行搜索。

Abstract: Databases are the most critical assets for enterprises, and yet they remain largely inaccessible to people who make the most important decisions. In this paper, we describe the Tursio search platform that builds an abstraction layer, aka semantic knowledge graph, over the underlying databases to make them searchable in natural language. Tursio infuses large language models (LLMs) into every part of the query processing stack, including data modeling, query compilation, query planning, and result reasoning. This allows Tursio to process natural language queries systematically using techniques from traditional query planning and rewriting, rather than black-box memorization. We describe the architecture of Tursio in detail and present a comprehensive evaluation on production workloads, and synthetic and realistic benchmarks. Our results show that Tursio achieves high accuracy while being efficient and scalable, making databases truly searchable for non-expert users.

</details>


### [130] [CLEAR: A Knowledge-Centric Vessel Trajectory Analysis Platform](https://arxiv.org/abs/2602.08482)
*Hengyu Liu,Tianyi Li,Haoyu Wang,Kristian Torp,Yushuai Li,Tiancheng Zhang,Torben Bach Pedersen,Christian S. Jensen*

Main category: cs.DB

TL;DR: 介绍了以知识为中心的船舶轨迹分析平台CLEAR，利用大语言模型克服AIS数据问题，还展示了演示功能。


<details>
  <summary>Details</summary>
Motivation: 解决非专业用户因AIS数据不完整和复杂而难以进行分析的问题。

Method: 利用大语言模型的推理和生成能力，通过结构化数据衍生知识图谱（SD - KG）将原始AIS数据转换为完整、可解释和易探索的船舶轨迹。

Result: 参与者可配置参数自动下载和处理AIS数据，观察轨迹完成和注释过程，检查原始和插补段及SD - KG证据，通过专用图查看器交互式探索SD - KG。

Conclusion: CLEAR平台能让用户对船舶运动有直观和透明的理解，有助于船舶轨迹分析。

Abstract: Vessel trajectory data from the Automatic Identification System (AIS) is used widely in maritime analytics. Yet, analysis is difficult for non-expert users due to the incompleteness and complexity of AIS data. We present CLEAR, a knowledge-centric vessel trajectory analysis platform that aims to overcome these barriers. By leveraging the reasoning and generative capabilities of Large Language Models (LLMs), CLEAR transforms raw AIS data into complete, interpretable, and easily explorable vessel trajectories through a Structured Data-derived Knowledge Graph (SD-KG). As part of the demo, participants can configure parameters to automatically download and process AIS data, observe how trajectories are completed and annotated, inspect both raw and imputed segments together with their SD-KG evidence, and interactively explore the SD-KG through a dedicated graph viewer, gaining an intuitive and transparent understanding of vessel movements.

</details>


### [131] [Semantics and Multi-Query Optimization Algorithms for the Analyze Operator](https://arxiv.org/abs/2602.08546)
*Marios Iakovidis,Panos Vassiliadis*

Main category: cs.DB

TL;DR: 本文引入新颖的ANALYZE操作符以提升数据分析效率，定义其语义，提出多查询优化策略及三种算法，实验表明Mid - MQO性能稳定。


<details>
  <summary>Details</summary>
Motivation: 为提高数据分析师在查找数据亮点时的分析任务效率，将有意查询操作符集成到分析引擎中。

Method: 引入ANALYZE操作符并定义其语义，将其查询定义为五个内部促进器立方体查询的元组，提出多查询优化策略及Min - MQO、Max - MQO、Mid - MQO三种算法。

Result: 实验表明Mid - MQO在多个场景下性能稳定，Min - MQO次之，Max - MQO在兄弟查询规模大且重叠显著时表现出色。

Conclusion: ANALYZE操作符及相应的多查询优化策略能有效提升数据分析任务的效率。

Abstract: In their hunt for highlights, i.e., interesting patterns in the data, data analysts have to issue groups of related queries and manually combine their results. To the extent that the analyst's goals are based on an intention on what to discover (e.g., contrast a query result to peer ones, verify a pattern to a broader range of data in the data space, etc), the integration of intentional query operators in analytical engines can enhance the efficiency of these analytical tasks. In this paper, we introduce, with well-defined semantics, the ANALYZE operator, a novel cube querying intentional operator that provides a 360 view of data. We define the semantics of an ANALYZE query as a tuple of five internal, facilitator cube queries, that (a) report on the specifics of a particular subset of the data space, which is part of the query specification, and to which we refer as the original query, (b) contrast the result with results from peer-subspaces, or sibling queries, and, (c) explore the data space in lower levels of granularity via drill-down queries. We introduce formal query semantics for the operator and we theoretically prove that we can obtain the exact same result by merging the facilitator cube queries into a smaller number of queries. This effectively introduces a multi-query optimization (MQO) strategy for executing an ANALYZE query. We propose three alternative algorithms, (a) a simple execution without optimizations (Min-MQO), (b) a total merging of all the facilitator queries to a single one (Max-MQO), and (c) an intermediate strategy, Mid-MQO, that merges only a subset of the facilitator queries. Our experimentation demonstrates that Mid-MQO achieves consistently strong performance across several contexts, Min-MQO always follows it, and Max-MQO excels for queries where the siblings are sizable and significantly overlap.

</details>


### [132] [MMTS-BENCH: A Comprehensive Benchmark for Time Series Understanding and Reasoning](https://arxiv.org/abs/2602.08588)
*Yao Yin,Zhenyu Xiao,Musheng Li,Yiwen Liu,Sutong Nan,Yiting He,Ruiqi Wang,Zhenwei Zhang,Qingmin Liao,Yuantao Gu*

Main category: cs.DB

TL;DR: 现有评估大语言模型时间任务的基准零散不系统，本文引入MMTS - BENCH进行综合评估并得出结论。


<details>
  <summary>Details</summary>
Motivation: 现有评估大语言模型时间任务的基准零散且不系统，需要搭建一个综合评估框架。

Method: 基于时间序列任务的分层分类法构建MMTS - BENCH基准，通过渐进式真实世界QA框架和模块化合成数据构建生成2424个时间序列问答对，对不同类型的大语言模型进行评估。

Result: （1）TS - LLMs在跨域泛化方面远落后于通用大模型；（2）大模型在局部任务表现弱于全局任务；（3）思维链推理和多模态集成显著提升性能；（4）现有TS - LLMs的主导因素仍是骨干网络能力而非时间序列编码器设计。

Conclusion: MMTS - BENCH提供了严格评估框架，为提升大语言模型的时间序列推理能力指明方向。

Abstract: Time series data are central to domains such as finance, healthcare, and cloud computing, yet existing benchmarks for evaluating various large language models (LLMs) on temporal tasks remain scattered and unsystematic. To bridge this gap, we introduce MMTS-BENCH, a comprehensive multimodal benchmark built upon a hierarchical taxonomy of time-series tasks, spanning structural awareness, feature analysis, temporal reasoning, sequence matching and cross-modal alignment. MMTS-BENCH comprises 2,424 time series question answering (TSQA) pairs across 4 subsets: Base, InWild, Match, and Align, generated through a progressive real-world QA framework and modular synthetic data construction. We conduct extensive evaluations on closed-source, open-source LLMs and existing time series adapted large language models (TS-LLMs), revealing that: (1) TS-LLMs significantly lag behind general-purpose LLMs in cross-domain generalization, (2) LLMs show weaknesses in local tasks compared to global tasks, (3) chain-of-thought (CoT) reasoning and multimodal integration substantially improve performance, and (4) the dominant factor in existing TS-LLMs remains the backbone network capability rather than the time series encoder design. MMTS-BENCH not only provides a rigorous evaluation framework but also offers clear directions for advancing LLMs toward robust, interpretable, and generalizable time-series reasoning.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [133] [Parallel Track Transformers: Enabling Fast GPU Inference with Reduced Synchronization](https://arxiv.org/abs/2602.07306)
*Chong Wang,Nan Du,Tom Gunter,Tao Lei,Kulin Seth,Senyu Tong,Jianyu Wang,Guoli Yin,Xiyou Zhou,Kelvin Zou,Ruoming Pang*

Main category: cs.DC

TL;DR: 提出Parallel Track (PT) Transformer架构减少跨设备依赖，集成到两个服务栈提升效率。


<details>
  <summary>Details</summary>
Motivation: 基于Transformer的大语言模型大规模推理需多GPU并行，但传统张量并行有通信瓶颈和可扩展性问题。

Method: 提出Parallel Track (PT) Transformer架构，重组计算以减少跨设备依赖。

Result: 相比标准张量并行，同步操作最多减少16倍，集成到两个服务栈后，首token时间最多减少15 - 30%，每输出token时间减少2 - 12%，吞吐量最多增加31.90%。

Conclusion: PT Transformer架构能有效提升大语言模型推理的服务效率。

Abstract: Efficient large-scale inference of transformer-based large language models (LLMs) remains a fundamental systems challenge, frequently requiring multi-GPU parallelism to meet stringent latency and throughput targets. Conventional tensor parallelism decomposes matrix operations across devices but introduces substantial inter-GPU synchronization, leading to communication bottlenecks and degraded scalability. We propose the Parallel Track (PT) Transformer, a novel architectural paradigm that restructures computation to minimize cross-device dependencies. PT achieves up to a 16x reduction in synchronization operations relative to standard tensor parallelism, while maintaining competitive model quality in our experiments. We integrate PT into two widely adopted LLM serving stacks-Tensor-RT-LLM and vLLM-and report consistent improvements in serving efficiency, including up to 15-30% reduced time to first token, 2-12% reduced time per output token, and up to 31.90% increased throughput in both settings.

</details>


### [134] [Knowledge Graphs-Driven Intelligence for Distributed Decision Systems](https://arxiv.org/abs/2602.07614)
*Rosario Napoli,Gabriele Morabito,Antonio Celesti,Massimo Villari,Maria Fazio*

Main category: cs.DC

TL;DR: 本文提出知识共享范式，利用知识图谱和图嵌入实现分散式智能，通过实验验证该机制在复杂动态环境适用。


<details>
  <summary>Details</summary>
Motivation: 现代分布式决策系统面临数据异构、动态环境和分散协调的挑战。

Method: 引入知识共享范式，用知识图谱语义和图嵌入表示能力，节点本地构建语义表示，通过GraphSAGE迭代聚合嵌入形成知识地图。

Result: 实验表明分布式知识共享机制能有效维持语义一致性和适应性。

Conclusion: 该机制适用于边缘计算、物联网和多智能体系统等复杂动态环境。

Abstract: Modern distributed decision-making systems face significant challenges arising from data heterogeneity, dynamic environments, and the need for decentralized coordination. This paper introduces the Knowledge Sharing paradigm as an innovative approach that uses the semantic richness of Knowledge Graphs (KGs) and the representational power of Graph Embeddings (GEs) to achieve decentralized intelligence. Our architecture empowers individual nodes to locally construct semantic representations of their operational context, iteratively aggregating embeddings through neighbor-based exchanges using GraphSAGE. This iterative local aggregation process results in a dynamically evolving global semantic abstraction called Knowledge Map, enabling coordinated decision-making without centralized control. To validate our approach, we conduct extensive experiments under a distributed resource orchestration use case. We simulate different network topologies and node workloads, analyzing the local semantic drift of individual nodes. Experimental results confirm that our distributed knowledge-sharing mechanism effectively maintains semantic coherence and adaptability, making it suitable for complex and dynamic environments such as Edge Computing, IoT, and multi-agent systems.

</details>


### [135] [Privacy-Preserving Coding Schemes for Multi-Access Distributed Computing Models](https://arxiv.org/abs/2602.07850)
*Shanuja Sasi*

Main category: cs.DC

TL;DR: 本文将隐私约束引入多接入分布式计算（MADC）模型，为两种特定连接模型开发私有编码方案。


<details>
  <summary>Details</summary>
Motivation: 在 MADC 模型中引入隐私约束，保障每个归约器分配函数的隐私。

Method: 构建新的扩展放置交付数组族并推导相应编码方案。

Result: 成功开发出适用于两种特定连接模型的私有编码方案。

Conclusion: 可在 MADC 模型中实现隐私保护，减少通信瓶颈且无需文件复制。

Abstract: Distributed computing frameworks such as MapReduce have become essential for large-scale data processing by decomposing tasks across multiple nodes. The multi-access distributed computing (MADC) model further advances this paradigm by decoupling mapper and reducer roles: dedicated mapper nodes store data and compute intermediate values, while reducer nodes are connected to multiple mappers and aggregate results to compute final outputs. This separation reduces communication bottlenecks without requiring file replication. In this paper, we introduce privacy constraints into MADC and develop private coded schemes for two specific connectivity models. We construct new families of extended placement delivery arrays and derive corresponding coding schemes that guarantee privacy of each reducer's assigned function.

</details>


### [136] [HEAL: Online Incremental Recovery for Leaderless Distributed Systems Across Persistency Models](https://arxiv.org/abs/2602.08257)
*Antonis Psistakis,Burak Ocalan,Fabien Chaix,Ramnatthan Alagappan,Josep Torrellas*

Main category: cs.DC

TL;DR: 本文提出用于现代无领导者分布式系统的低开销恢复方案HEAL，实验表明其恢复快、对吞吐量影响小。


<details>
  <summary>Details</summary>
Motivation: 开发轻量级机制，使分布式系统能快速从故障中恢复且对系统吞吐量影响小。

Method: 提出HEAL方案，在节点故障时进行优化的在线增量恢复，给出不同一致性和持久化模型下的算法并在6节点Intel集群实现。

Result: 实验表明HEAL平均120毫秒恢复集群，运行时负载吞吐量平均降低8.7%；传统方案需360秒恢复，吞吐量降低16.2%；相比基于领导者的增量恢复方案，HEAL平均恢复延迟降低20.7倍，吞吐量下降降低62.4%。

Conclusion: HEAL方案有效，在恢复速度和对吞吐量影响方面优于传统方案和基于领导者的增量恢复方案。

Abstract: Ensuring resilience in distributed systems has become an acute concern. In today's environment, it is crucial to develop light-weight mechanisms that recover a distributed system from faults quickly and with only a small impact on the live-system throughput. To address this need, this paper proposes a new low-overhead, general recovery scheme for modern non-transactional leaderless distributed systems. We call our scheme HEAL. On a node failure, HEAL performs an optimized online incremental recovery. This paper presents HEAL's algorithms for settings with Linearizable consistency and different memory persistency models. We implement HEAL on a 6-node Intel cluster. Our experiments running TAOBench workloads show that HEAL is very effective. HEAL recovers the cluster in 120 milliseconds on average, while reducing the throughput of the running workload by an average of 8.7%. In contrast, a conventional recovery scheme for leaderless systems needs 360 seconds to recover, reducing the throughput of the system by 16.2%. Finally, compared to an incremental recovery scheme for a state-of-the-art leader-based system, HEAL reduces the average recovery latency by 20.7x and the throughput degradation by 62.4%.

</details>


### [137] [Towards CXL Resilience to CPU Failures](https://arxiv.org/abs/2602.08271)
*Antonis Psistakis,Burak Ocalan,Chloe Alverti,Fabien Chaix,Ramnatthan Alagappan,Josep Torrellas*

Main category: cs.DC

TL;DR: 本文提出ReCXL系统扩展CXL规范，使系统能应对节点故障并恢复应用，评估显示其实现容错执行仅带来30%性能下降。


<details>
  <summary>Details</summary>
Motivation: CXL 3.0及后续版本在分布式计算中引入新的弹性挑战，CXL规范未考虑处理器故障且无法使应用恢复到一致状态。

Method: ReCXL在写操作的一致性事务中增加消息，将更新传播到其他节点（副本），副本将更新保存到硬件日志单元，日志单元定期将更新转存到内存，恢复时利用日志使目录和内存恢复正确状态。

Result: ReCXL实现了容错执行，相比无容错支持的同一平台仅带来30%的性能下降。

Conclusion: ReCXL能有效扩展CXL规范，使系统具备应对节点故障并恢复应用的能力。

Abstract: Compute Express Link (CXL) 3.0 and beyond allows the compute nodes of a cluster to share data with hardware cache coherence and at the granularity of a cache line. This enables shared-memory semantics for distributed computing, but introduces new resilience challenges: a node failure leads to the loss of the dirty data in its caches, corrupting application state. Unfortunately, the CXL specification does not consider processor failures. Moreover, when a component fails, the specification tries to isolate it and continue application execution; there is no attempt to bring the application to a consistent state. To address these limitations, this paper extends the CXL specification to be resilient to node failures, and to correctly recover the application after node failures. We call the system ReCXL. To handle the failure of nodes, ReCXL augments the coherence transaction of a write with messages that propagate the update to a small set of other nodes (i.e., Replicas). Replicas save the update in a hardware Logging Unit. Such replication ensures resilience to node failures. Then, at regular intervals, the Logging Units dump the updates to memory. Recovery involves using the logs in the Logging Units to bring the directory and memory to a correct state. Our evaluation shows that ReCXL enables fault-tolerant execution with only a 30% slowdown over the same platform with no fault-tolerance support.

</details>


### [138] [PARD: Enhancing Goodput for Inference Pipeline via Proactive Request Dropping](https://arxiv.org/abs/2602.08747)
*Zhixin Zhao,Yitao Hu,Simin Chen,Mingfang Ji,Wei Yang,Yuhao Zhang,Laiping Zhao,Wenxin Li,Xiulong Liu,Wenyu Qu,Hao Wang*

Main category: cs.DC

TL;DR: 为了满足推理管道延迟要求，现有系统常采用被动丢弃请求策略，但该策略无法保证高吞吐量。本文提出主动丢弃请求策略，并设计推理系统 PARD，实验表明其能提高吞吐量，降低丢弃率和计算资源浪费。


<details>
  <summary>Details</summary>
Motivation: 现有推理管道系统采用的被动丢弃请求策略无法维持高吞吐量，存在丢弃决策不及时和丢弃错误请求集的问题。

Method: 设计推理系统 PARD，集成主动丢弃方法（根据推理管道运行时信息决定何时丢弃请求）和自适应请求优先级机制（根据剩余延迟预算和工作负载强度选择丢弃的请求）。

Result: 在 64 个 GPU 集群上的评估显示，PARD 比现有技术的吞吐量高 16% - 176%，同时丢弃率和浪费的计算资源分别降低 1.6 - 17 倍和 1.5 - 62 倍。

Conclusion: 主动丢弃请求策略能有效提高推理系统的吞吐量，减少资源浪费。

Abstract: Modern deep neural network (DNN) applications integrate multiple DNN models into inference pipelines with stringent latency requirements for customized tasks. To mitigate extensive request timeouts caused by accumulation, systems for inference pipelines commonly drop a subset of requests so the remaining ones can satisfy latency constraints. Since it is commonly believed that request dropping adversely affects goodput, existing systems only drop requests when they have to, which we call reactive dropping. However, this reactive policy can not maintain high goodput, as it neither makes timely dropping decisions nor identifies the proper set of requests to drop, leading to issues of dropping requests too late or dropping the wrong set of requests.
  We propose that the inference system should proactively drop certain requests in advance to enhance the goodput across the entire workload. To achieve this, we design an inference system PARD. It enhances goodput with timely and precise dropping decisions by integrating a proactive dropping method that decides when to drop requests using runtime information of the inference pipeline, and an adaptive request priority mechanism that selects which specific requests to drop based on remaining latency budgets and workload intensity. Evaluation on a cluster of 64 GPUs over real-world workloads shows that PARD achieves $16\%$-$176\%$ higher goodput than the state of the art while reducing the drop rate and wasted computation resources by $1.6\times$-$17\times$ and $1.5\times$-$62\times$ respectively.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [139] [Unsplittable Transshipments](https://arxiv.org/abs/2602.07230)
*Srinwanti Debgupta,Sarah Morell,Martin Skutella*

Main category: cs.DS

TL;DR: 本文介绍有向图中多源多汇的不可拆分转运问题，主要贡献是拓展了前人结果，还给出满足所有需求所需轮数的界。


<details>
  <summary>Details</summary>
Motivation: 不可拆分转运问题是单源不可拆分流的自然推广，但带来新挑战，需要新算法技术。

Method: 将给定转运x转化为不可拆分转运y，使y_a < x_a + d_max 对所有弧a成立。

Result: 实现了对Dinitz等人结果的拓展，给出满足需求轮数的界。

Conclusion: 提出的方法能有效解决不可拆分转运问题，且在满足需求轮数上有一定结论。

Abstract: We introduce the Unsplittable Transshipment Problem in directed graphs with multiple sources and sinks. An unsplittable transshipment routes given supplies and demands using at most one path for each source-sink pair. Although they are a natural generalization of single source unsplittable flows, unsplittable transshipments raise interesting new challenges and require novel algorithmic techniques. As our main contribution, we give a nontrivial generalization of a seminal result of Dinitz, Garg, and Goemans (1999) by showing how to efficiently turn a given transshipment $x$ into an unsplittable transshipment $y$ with $y_a<x_a+d_{\max}$ for all arcs $a$, where $d_{\max}$ is the maximum demand (or supply) value. Further results include bounds on the number of rounds required to satisfy all demands, where each round consists of an unsplittable transshipment that routes a subset of the demands while respecting arc capacity constraints.

</details>


### [140] [Online Algorithm for Fractional Matchings with Edge Arrivals in Graphs of Maximum Degree Three](https://arxiv.org/abs/2602.07355)
*Kanstantsin Pashkovich,Thomas Snow*

Main category: cs.DS

TL;DR: 研究低度数图中带边到达的最大基数匹配在线算法，给出最大度为3的图的最大基数分数匹配在线算法，证明最大度为3的图整数匹配无0.5807的竞争保证，且分数匹配在顶点和边到达模型中最优竞争比相同。


<details>
  <summary>Details</summary>
Motivation: 补充Buchbinder等人关于最大度为3的图的最大基数分数匹配在线算法的负面结果，研究整数和分数匹配的竞争比差距。

Method: 设计在线算法并进行竞争性分析。

Result: 得到最大度为3的图的最大基数分数匹配在线算法竞争比至少为4/(9 - √5)，证明最大度为3的图最大基数整数匹配无0.5807的竞争保证，分数匹配在顶点和边到达模型中最优竞争比相同。

Conclusion: 对于最大度为3的图，分数和整数匹配存在差距，分数匹配在不同到达模型中最优竞争比相同。

Abstract: We study online algorithms for maximum cardinality matchings with edge arrivals in graphs of low degree. Buchbinder, Segev, and Tkach showed that no online algorithm for maximum cardinality fractional matchings can achieve a competitive ratio larger than $4/(9-\sqrt 5)\approx 0.5914$ even for graphs of maximum degree three. The negative result of Buchbinder et al. holds even when the graph is bipartite and edges are revealed according to vertex arrivals, i.e. once a vertex arrives, all edges are revealed that include the newly arrived vertex and one of the previously arrived vertices. In this work, we complement the negative result of Buchbinder et al. by providing an online algorithm for maximum cardinality fractional matchings with a competitive ratio at least $4/(9-\sqrt 5)\approx 0.5914$ for graphs of maximum degree three. We also demonstrate that no online algorithm for maximum cardinality integral matchings can have the competitive guarantee $0.5807$, establishing a gap between integral and fractional matchings for graphs of maximum degree three. Note that the work of Buchbinder et al. shows that for graphs of maximum degree two, there is no such gap between fractional and integral matchings, because for both of them the best achievable competitive ratio is $2/3$. Also, our results demonstrate that for graphs of maximum degree three best possible competitive ratios for fractional matchings are the same in the vertex arrival and in the edge arrival models.

</details>


### [141] [Local Computation Algorithms for (Minimum) Spanning Trees on Expander Graphs](https://arxiv.org/abs/2602.07394)
*Pan Peng,Yuyang Wang*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study \emph{local computation algorithms (LCAs)} for constructing spanning trees. In this setting, the goal is to locally determine, for each edge $ e \in E $, whether it belongs to a spanning tree $ T $ of the input graph $ G $, where $ T $ is defined implicitly by $ G $ and the randomness of the algorithm. It is known that LCAs for spanning trees do not exist in general graphs, even for simple graph families. We identify a natural and well-studied class of graphs -- \emph{expander graphs} -- that do admit \emph{sublinear-time} LCAs for spanning trees. This is perhaps surprising, as previous work on expanders only succeeded in designing LCAs for \emph{sparse spanning subgraphs}, rather than full spanning trees. We design an LCA with probe complexity $ O\left(\sqrt{n}\left(\frac{\log^2 n}{φ^2} + d\right)\right)$ for graphs with conductance at least $ φ$ and maximum degree at most $ d $ (not necessarily constant), which is nearly optimal when $φ$ and $d$ are constants, since $Ω(\sqrt{n})$ probes are necessary even for expanders. Next, we show that for the natural class of \emph{\ER graphs} $ G(n, p) $ with $ np = n^δ $ for any constant $ δ> 0 $ (which are expanders with high probability), the $ \sqrt{n} $ lower bound can be bypassed. Specifically, we give an \emph{average-case} LCA for such graphs with probe complexity $ \tilde{O}(\sqrt{n^{1 - δ}})$.
  Finally, we extend our techniques to design LCAs for the \emph{minimum spanning tree (MST)} problem on weighted expander graphs. Specifically, given a $d$-regular unweighted graph $\bar{G}$ with sufficiently strong expansion, we consider the weighted graph $G$ obtained by assigning to each edge an independent and uniform random weight from $\{1,\ldots,W\}$, where $W = O(d)$. We show that there exists an LCA that is consistent with an exact MST of $G$, with probe complexity $\tilde{O}(\sqrt{n}d^2)$.

</details>


### [142] [Robust Multiagent Collaboration Through Weighted Max-Min T-Joins](https://arxiv.org/abs/2602.07720)
*Sharareh Alipour*

Main category: cs.DS

TL;DR: 本文研究加权最大最小T - 连接问题，提出算法求上界和近似解，在真实数据集验证，显示该问题在多智能体系统中有理论和实践价值。


<details>
  <summary>Details</summary>
Motivation: 许多多智能体任务需要选择一组智能体，以确保在最坏情况下协作仍有效，加权最大最小T - 连接问题可解决此挑战。

Method: 1. 设计算法计算加权最大最小2k - 匹配问题的上界，并据此开发有2lnn近似保证、运行时间为O(n^4)的通用算法；2. 用耳分解法给出加权最大最小T - 连接成本的另一上界，证明边权为{1,2}时可精确求解。

Result: 在真实协作数据集上评估，近似算法的下界和耳分解法的上界接近，有小常数因子近似效果。

Conclusion: 加权最大最小T - 连接作为多智能体系统中公平和鲁棒组形成的框架，有理论意义和实用价值。

Abstract: Many multiagent tasks -- such as reviewer assignment, coalition formation, or fair resource allocation -- require selecting a group of agents such that collaboration remains effective even in the worst case. The \emph{weighted max-min $T$-join problem} formalizes this challenge by seeking a subset of vertices whose minimum-weight matching is maximized, thereby ensuring robust outcomes against unfavorable pairings.
  We advance the study of this problem in several directions. First, we design an algorithm that computes an upper bound for the \emph{weighted max-min $2k$-matching problem}, where the chosen set must contain exactly $2k$ vertices. Building on this bound, we develop a general algorithm with a \emph{$2 \ln n$-approximation guarantee} that runs in $O(n^4)$ time. Second, using ear decompositions, we propose another upper bound for the weighted max-min $T$-join cost. We also show that the problem can be solved exactly when edge weights belong to $\{1,2\}$.
  Finally, we evaluate our methods on real collaboration datasets. Experiments show that the lower bounds from our approximation algorithm and the upper bounds from the ear decomposition method are consistently close, yielding empirically small constant-factor approximations. Overall, our results highlight both the theoretical significance and practical value of weighted max-min $T$-joins as a framework for fair and robust group formation in multiagent systems.

</details>


### [143] [A Faster Directed Single-Source Shortest Path Algorithm](https://arxiv.org/abs/2602.07868)
*Ran Duan,Xiao Mao,Xinkai Shu,Longhui Yin*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This paper presents a new deterministic algorithm for single-source shortest paths (SSSP) on real non-negative edge-weighted directed graphs, with running time $O(m\sqrt{\log n}+\sqrt{mn\log n\log \log n})$, which is $O(m\sqrt{\log n\log \log n})$ for sparse graphs. This improves the recent breakthrough result of $O(m\log^{2/3} n)$ time for directed SSSP algorithm [Duan, Mao, Mao, Shu, Yin 2025].

</details>


### [144] [Space Complexity Dichotomies for Subgraph Finding Problems in the Streaming Model](https://arxiv.org/abs/2602.08002)
*Yu-Sheng Shih,Meng-Tsung Tsai,Yen-Chu Tsai,Ying-Sian Wu*

Main category: cs.DS

TL;DR: 研究流模型下标准子图查找问题四种变体的空间复杂度，得到完整二分定理。


<details>
  <summary>Details</summary>
Motivation: 研究流模型下子图查找问题四种变体的空间复杂度特性。

Method: 针对无向简单图和有向图两种设置，分析四种变体问题。

Result: 得到四种变体的完整二分定理，如 Sub(H) 能用特定算法解决当且仅当 H 是二分图等。

Conclusion: 明确了四种变体问题在特定条件下可通过相应算法解决。

Abstract: We study the space complexity of four variants of the standard subgraph finding problem in the streaming model. Specifically, given an $n$-vertex input graph and a fixed-size pattern graph, we consider two settings: undirected simple graphs, denoted by $G$ and $H$, and oriented graphs, denoted by $\vec{G}$ and $\vec{H}$. Depending on the setting, the task is to decide whether $G$ contains $H$ as a subgraph or as an induced subgraph, or whether $\vec{G}$ contains $\vec{H}$ as a subgraph or as an induced subgraph. Let Sub$(H)$, IndSub$(H)$, Sub$(\vec{H})$, and IndSub$(\vec{H})$ denote these four variants, respectively.
  An oriented graph is well-oriented if it admits a bipartition in which every arc is oriented from one part to the other, and a vertex is non-well-oriented if both its in-degree and out-degree are non-zero. For each variant, we obtain a complete dichotomy theorem, briefly summarized as follows.
  (1) Sub$(H)$ can be solved by an $\tilde{O}(1)$-pass $n^{2-Ω(1)}$-space algorithm if and only if $H$ is bipartite.
  (2) IndSub$(H)$ can be solved by an $\tilde{O}(1)$-pass $n^{2-Ω(1)}$-space algorithm if and only if $H \in \{P_3, P_4, co\mbox{-}P_3\}$.
  (3) Sub$(\vec{H})$ can be solved by a single-pass $n^{2-Ω(1)}$-space algorithm if and only if every connected component of $\vec H$ is either a well-oriented bipartite graph or a tree containing at most one non-well-oriented vertex.
  (4) IndSub$(\vec{H})$ can be solved by an $\tilde{O}(1)$-pass $n^{2-Ω(1)}$-space algorithm if and only if the underlying undirected simple graph $H$ is a $co\mbox{-}P_3$.

</details>


### [145] [Prune, Don't Rebuild: Efficiently Tuning $α$-Reachable Graphs for Nearest Neighbor Search](https://arxiv.org/abs/2602.08097)
*Tian Zhang,Ashwin Padaki,Jiaming Liang,Zack Ives,Erik Waingarten*

Main category: cs.DS

TL;DR: 提出RP - Tuning方法，可在不重建索引情况下调整DiskANN的α参数，理论证明其可达性保证，实验显示加速效果显著。


<details>
  <summary>Details</summary>
Motivation: 现有DiskANN的α参数自适应调整需重建索引，在大规模场景下不可行。

Method: 基于DiskANN的剪枝步骤提出RP - Tuning后处理程序，在α - 可达性框架下证明其性质。

Result: RP - Tuning在四个公共数据集上加速DiskANN调参达43倍，开销可忽略不计。

Conclusion: RP - Tuning是一种高效的方法，能在不重建索引的情况下调整α参数，提升调参效率。

Abstract: Vector similarity search is an essential primitive in modern AI and ML applications. Most vector databases adopt graph-based approximate nearest neighbor (ANN) search algorithms, such as DiskANN (Subramanya et al., 2019), which have demonstrated state-of-the-art empirical performance. DiskANN's graph construction is governed by a reachability parameter $α$, which gives a trade-off between construction time, query time, and accuracy. However, adaptively tuning this trade-off typically requires rebuilding the index for different $α$ values, which is prohibitive at scale. In this work, we propose RP-Tuning, an efficient post-hoc routine, based on DiskANN's pruning step, to adjust the $α$ parameter without reconstructing the full index. Within the $α$-reachability framework of prior theoretical works (Indyk and Xu, 2023; Gollapudi et al., 2025), we prove that pruning an initially $α$-reachable graph with RP-Tuning preserves worst-case reachability guarantees in general metrics and improved guarantees in Euclidean metrics. Empirically, we show that RP-Tuning accelerates DiskANN tuning on four public datasets by up to $43\times$ with negligible overhead.

</details>


### [146] [Neighborhood-Aware Graph Labeling Problem](https://arxiv.org/abs/2602.08098)
*Mohammad Shahverdikondori,Sepehr Elahi,Patrick Thiran,Negar Kiyavash*

Main category: cs.DS

TL;DR: 研究邻域感知图标签问题（NAGL），分析其复杂度并给出精确与近似算法。


<details>
  <summary>Details</summary>
Motivation: 受带网络干扰的多臂老虎机优化预言机启发，研究NAGL问题。

Method: 证明复杂度，用精确动态规划算法，给出不同情形下近似算法

Result: 证明NAGL在星图上是NP难，无高效近似比算法，给出不同情形的近似算法。

Conclusion: 明确NAGL复杂度难，且在部分情形有实用近似算法。

Abstract: Motivated by optimization oracles in bandits with network interference, we study the Neighborhood-Aware Graph Labeling (NAGL) problem. Given a graph $G = (V,E)$, a label set of size $L$, and local reward functions $f_v$ accessed via evaluation oracles, the objective is to assign labels to maximize $\sum_{v \in V} f_v(x_{N[v]})$, where each term depends on the closed neighborhood of $v$. Two vertices co-occur in some neighborhood term exactly when their distance in $G$ is at most $2$, so the dependency graph is the squared graph $G^2$ and $\mathrm{tw}(G^2)$ governs exact algorithms and matching fine-grained lower bounds. Accordingly, we show that this dependence is inherent: NAGL is NP-hard even on star graphs with binary labels and, assuming SETH, admits no $(L-\varepsilon)^{\mathrm{tw}(G^2)}\cdot n^{O(1)}$-time algorithm for any $\varepsilon>0$. We match this with an exact dynamic program on a tree decomposition of $G^2$ running in $O\!\left(n\cdot \mathrm{tw}(G^2)\cdot L^{\mathrm{tw}(G^2)+1}\right)$ time. For approximation, unless $\mathsf{P}=\mathsf{NP}$, for every $\varepsilon>0$ there is no polynomial-time $n^{1-\varepsilon}$-approximation on general graphs even under the promise $\mathrm{OPT}>0$; without the promise $\mathrm{OPT}>0$, no finite multiplicative approximation ratio is possible. In the nonnegative-reward regime, we give polynomial-time approximation algorithms for NAGL in two settings: (i) given a proper $q$-coloring of $G^2$, we obtain a $1/q$-approximation; and (ii) on planar graphs of bounded maximum degree, we develop a Baker-type polynomial-time approximation scheme (PTAS), which becomes an efficient PTAS (EPTAS) when $L$ is constant.

</details>


### [147] [Boltzmann sampling and optimal exact-size sampling for directed acyclic graphs](https://arxiv.org/abs/2602.08471)
*Wojciech Gabryelski,Zbigniew Gołȩbiewski,Martin Pépin*

Main category: cs.DS

TL;DR: 提出两种生成均匀随机有向无环图的高效算法，比现有算法更优


<details>
  <summary>Details</summary>
Motivation: 获得更高效的生成均匀随机有向无环图的算法

Method: 扩展图生成函数的玻尔兹曼模型及使用有向无环图的各种分解

Result: 得到一个渐进最优的精确大小采样器，操作次数为$\frac{n^2}{2} + o(n^2)$，算法在理论复杂度和实际速度上优于现有算法

Conclusion: 提出的采样器在理论和实际应用上具有优势

Abstract: We propose two efficient algorithms for generating uniform random directed acyclic graphs, including an asymptotically optimal exact-size sampler that performs $\frac{n^2}{2} + o(n^2)$ operations and requests to a random generator. This was achieved by extending the Boltzmann model for graphical generating functions and by using various decompositions of directed acyclic graphs. The presented samplers improve upon the state-of-the-art algorithms in terms of theoretical complexity and offer a significant speed-up in practice.

</details>


### [148] [Submodular Maximization over a Matroid $k$-Intersection: Multiplicative Improvement over Greedy](https://arxiv.org/abs/2602.08473)
*Moran Feldman,Justin Ward*

Main category: cs.DS

TL;DR: 本文研究非负单调子模目标函数在k个任意拟阵约束交集下的最大化问题，给出了优于贪心算法的近似比，算法时间与k无关且为多项式时间。


<details>
  <summary>Details</summary>
Motivation: 现有算法在该问题上的近似比不够理想，希望得到更好的近似比。

Method: 基于Singer和Thiery提出的混合贪心局部搜索方法，并进行了非平凡的洞察和算法修改。

Result: 给出了0.819k + O(√k)的近似比，在非单调目标函数问题上也有类似结果，特殊加权情况下近似比为0.694k + 0.307。

Conclusion: 得到了优于贪心算法的近似比，算法时间复杂度有优势，结果在更一般的拟阵k - 奇偶约束下也成立。

Abstract: We study the problem of maximizing a non-negative monotone submodular objective $f$ subject to the intersection of $k$ arbitrary matroid constraints. The natural greedy algorithm guarantees $(k+1)$-approximation for this problem, and the state-of-the-art algorithm only improves this approximation ratio to $k$. We give a $\frac{2k\ln2}{1+\ln2}+O(\sqrt{k})<0.819k+O(\sqrt{k})$ approximation for this problem. Our result is the first multiplicative improvement over the approximation ratio of the greedy algorithm for general $k$. We further show that our algorithm can be used to obtain roughly the same approximation ratio also for the more general problem in which the objective is not guaranteed to be monotone (the sublinear term in the approximation ratio becomes $O(k^{2/3})$ rather than $O(\sqrt{k})$ in this case).
  All of our results hold also when the $k$-matroid intersection constraint is replaced with a more general matroid $k$-parity constraint. Furthermore, unlike the case in many of the previous works, our algorithms run in time that is independent of $k$ and polynomial in the size of the ground set. Our algorithms are based on a hybrid greedy local search approach recently introduced by Singer and Thiery (STOC 2025) for the weighted matroid $k$-intersection problem, which is a special case of the problem we consider. Leveraging their approach in the submodular setting requires several non-trivial insights and algorithmic modifications since the marginals of a submodular function $f$, which correspond to the weights in the weighted case, are not independent of the algorithm's internal randomness. In the special weighted case studied by Singer and Thiery, our algorithms reduce to a variant of their algorithm with an improved approximation ratio of $k\ln2+1-\ln2<0.694k+0.307$, compared to an approximation ratio of $\frac{k+1}{2\ln2}\approx0.722k+0.722$ guaranteed by Singer and Thiery.

</details>


### [149] [Incremental (k, z)-Clustering on Graphs](https://arxiv.org/abs/2602.08542)
*Emilio Cruciani,Sebastian Forster,Antonis Skarlatos*

Main category: cs.DS

TL;DR: 本文针对动态 (k, z)-聚类问题，开发了随机增量算法，能在边插入情况下以高概率维持常数因子近似解。


<details>
  <summary>Details</summary>
Motivation: 现有高效动态 k-中心近似算法，但缺乏动态 (k, z)-聚类问题的类似结果。

Method: 算法分两阶段，第一阶段维护大小为 Õ(k) 的常数因子双准则近似解，第二阶段在双准则近似解诱导的动态加权实例上维护常数因子近似 (k, z)-聚类解。

Result: 开发出随机增量 (k, z)-聚类算法，在边插入时以高概率维持常数因子近似，总更新时间为 Õ(k m^(1+o(1)) + k^(1+1/λ) m)。

Conclusion: 提出的算法能有效解决动态 (k, z)-聚类问题，第一阶段的关键技术结果有独立价值。

Abstract: Given a weighted undirected graph, a number of clusters $k$, and an exponent $z$, the goal in the $(k, z)$-clustering problem on graphs is to select $k$ vertices as centers that minimize the sum of the distances raised to the power $z$ of each vertex to its closest center. In the dynamic setting, the graph is subject to adversarial edge updates, and the goal is to maintain explicitly an exact $(k, z)$-clustering solution in the induced shortest-path metric.
  While efficient dynamic $k$-center approximation algorithms on graphs exist [Cruciani et al. SODA 2024], to the best of our knowledge, no prior work provides similar results for the dynamic $(k,z)$-clustering problem. As the main result of this paper, we develop a randomized incremental $(k, z)$-clustering algorithm that maintains with high probability a constant-factor approximation in a graph undergoing edge insertions with a total update time of $\tilde O(k m^{1+o(1)}+ k^{1+\frac{1}λ} m)$, where $λ\geq 1$ is an arbitrary fixed constant. Our incremental algorithm consists of two stages. In the first stage, we maintain a constant-factor bicriteria approximate solution of size $\tilde{O}(k)$ with a total update time of $m^{1+o(1)}$ over all adversarial edge insertions. This first stage is an intricate adaptation of the bicriteria approximation algorithm by Mettu and Plaxton [Machine Learning 2004] to incremental graphs. One of our key technical results is that the radii in their algorithm can be assumed to be non-decreasing while the approximation ratio remains constant, a property that may be of independent interest.
  In the second stage, we maintain a constant-factor approximate $(k,z)$-clustering solution on a dynamic weighted instance induced by the bicriteria approximate solution. For this subproblem, we employ a dynamic spanner algorithm together with a static $(k,z)$-clustering algorithm.

</details>


### [150] [Approximate Cartesian Tree Matching with Substitutions](https://arxiv.org/abs/2602.08570)
*Panagiotis Charalampopoulos,Jonas Ellert,Manal Mohamed*

Main category: cs.DS

TL;DR: 本文聚焦笛卡尔树匹配问题，从精确匹配拓展到近似匹配，采用汉明距离量化近似度并给出算法。


<details>
  <summary>Details</summary>
Motivation: 精确的笛卡尔树匹配问题较受限，对模式中的异常值不鲁棒，因此需考虑近似匹配问题。

Method: 使用汉明距离量化接近程度，为整数参数k开发算法；引入笛卡尔树周期性概念并拓展相关组合与算法结果到笛卡尔树匹配。

Result: 在不同k值范围有不同算法运行时间，k ≤ m1/5时为O(n√m ⋅ k2.5)，k ≥ m1/5时为O(nk5)，在k = o(m1/4)时优于现有算法。

Conclusion: 提出的算法在特定情况下改进了现有笛卡尔树近似匹配算法的时间复杂度。

Abstract: The Cartesian tree of a sequence captures the relative order of the sequence's elements. In recent years, Cartesian tree matching has attracted considerable attention, particularly due to its applications in time series analysis. Consider a text $T$ of length $n$ and a pattern $P$ of length $m$. In the exact Cartesian tree matching problem, the task is to find all length-$m$ fragments of $T$ whose Cartesian tree coincides with the Cartesian tree $CT(P)$ of the pattern. Although the exact version of the problem can be solved in linear time [Park et al., TCS 2020], it remains rather restrictive; for example, it is not robust to outliers in the pattern.
  To overcome this limitation, we consider the approximate setting, where the goal is to identify all fragments of $T$ that are close to some string whose Cartesian tree matches $CT(P)$. In this work, we quantify closeness via the widely used Hamming distance metric. For a given integer parameter $k>0$, we present an algorithm that computes all fragments of $T$ that are at Hamming distance at most $k$ from a string whose Cartesian tree matches $CT(P)$. Our algorithm runs in time $\mathcal O(n \sqrt{m} \cdot k^{2.5})$ for $k \leq m^{1/5}$ and in time $\mathcal O(nk^5)$ for $k \geq m^{1/5}$, thereby improving upon the state-of-the-art $\mathcal O(nmk)$-time algorithm of Kim and Han [TCS 2025] in the regime $k = o(m^{1/4})$.
  On the way to our solution, we develop a toolbox of independent interest. First, we introduce a new notion of periodicity in Cartesian trees. Then, we lift multiple well-known combinatorial and algorithmic results for string matching and periodicity in strings to Cartesian tree matching and periodicity in Cartesian trees.

</details>


### [151] [Welfarist Formulations for Diverse Similarity Search](https://arxiv.org/abs/2602.08742)
*Siddharth Barman,Nirjhar Das,Shivam Gupta,Kirankumar Shiragur*

Main category: cs.DS

TL;DR: 本文提出基于福利的最近邻搜索（NNS）公式以实现属性多样性，开发高效算法，实验表明该方法实用且能提升多样性并保持相关性。


<details>
  <summary>Details</summary>
Motivation: 在NNS应用中，除相关性外，邻居间的多样性是重要需求，而之前基于约束的方法存在不足。

Method: 基于数学经济学的福利函数开发NNS公式，以Nash社会福利为重点，开发有理论保证的高效最近邻算法，可基于标准ANN方法。

Result: 实验结果表明该方法实用，能大幅提升多样性并保持高相关性。

Conclusion: 基于福利的NNS公式能自适应平衡相关性和多样性，提供参数化权衡方式，开发的算法有效。

Abstract: Nearest Neighbor Search (NNS) is a fundamental problem in data structures with wide-ranging applications, such as web search, recommendation systems, and, more recently, retrieval-augmented generations (RAG). In such recent applications, in addition to the relevance (similarity) of the returned neighbors, diversity among the neighbors is a central requirement. In this paper, we develop principled welfare-based formulations in NNS for realizing diversity across attributes. Our formulations are based on welfare functions -- from mathematical economics -- that satisfy central diversity (fairness) and relevance (economic efficiency) axioms. With a particular focus on Nash social welfare, we note that our welfare-based formulations provide objective functions that adaptively balance relevance and diversity in a query-dependent manner. Notably, such a balance was not present in the prior constraint-based approach, which forced a fixed level of diversity and optimized for relevance. In addition, our formulation provides a parametric way to control the trade-off between relevance and diversity, providing practitioners with flexibility to tailor search results to task-specific requirements. We develop efficient nearest neighbor algorithms with provable guarantees for the welfare-based objectives. Notably, our algorithm can be applied on top of any standard ANN method (i.e., use standard ANN method as a subroutine) to efficiently find neighbors that approximately maximize our welfare-based objectives. Experimental results demonstrate that our approach is practical and substantially improves diversity while maintaining high relevance of the retrieved neighbors.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [152] [Online Contract Design](https://arxiv.org/abs/2602.07385)
*Elad Lavi,Hadas Shachnai,Inbal Talgam-Cohen*

Main category: cs.GT

TL;DR: 本文开展在线合约研究，提出 1/2 竞争比算法，证明随机性必要，且对组合结构回报情况进行分析。


<details>
  <summary>Details</summary>
Motivation: 将经济合约理论的博弈考量与在线算法设计的算法和信息挑战相结合，研究在线合约问题。

Method: 基于经典在线可抢占设定，设计随机算法，采用平衡点技术。

Result: 当代理回报是可加时，算法达到 1/2 竞争比，证明无确定性算法有有界竞争比，组合结构下随机算法可能失败。

Conclusion: 平衡点技术可用于进一步研究对抗模型中的在线合约。

Abstract: We initiate the study of online contracts, which integrate the game-theoretic considerations of economic contract theory, with the algorithmic and informational challenges of online algorithm design. Our starting point is the classic online setting with preemption of Buchbinder et al. [SODA'15], in which a hiring principal faces a sequence of adversarial agent arrivals. Upon arrival, the principal must decide whether to tentatively accept the agent to their team, and whether to dismiss previous tentative choices. Dismissal is irrevocable, giving the setting its online decision-making flavor. In our setting, the agents are rational players: once the team is finalized, a game is played where the principal offers contracts (performance-based payment schemes), and each agent decides whether or not to work. Working agents reward the principal, and the goal is to choose a team that maximizes the principal's utility. Our main positive result is a 1/2-competitive algorithm when agent rewards are additive, which matches the best-possible competitive ratio. Our algorithm is randomized and this is necessary, as we show that no deterministic algorithm can attain a bounded competitive ratio. Moreover, if agent rewards are allowed to exhibit combinatorial structure known as XOS, even randomized algorithms might fail. En route to our competitive algorithm, we develop the technique of balance points, which can be useful for further exploration of online contracts in the adversarial model.

</details>


### [153] [Modeling Concurrent Multi-Agent Systems](https://arxiv.org/abs/2602.08452)
*Senthil Rajasekaran,Moshe Y. Vardi*

Main category: cs.GT

TL;DR: 本文分析对比显式模型和基于电路的模型的复杂性理论结果，证明后者能处理前者及均衡分析文献中的问题。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统复杂，现有研究常采用缺乏表达能力的模型或忽略重要复杂性理论结果，本文旨在解决此问题。

Method: 仔细分析和对比显式模型与基于电路的模型的复杂性理论结果，并对两个最重要的决策问题进行上下界综合分析。

Result: 基于电路的模型能妥善处理显式模型及均衡分析文献中普遍存在的问题。

Conclusion: 基于电路的模型在处理多智能体系统均衡分析问题上有优势。

Abstract: Recent work in the field of multi-agent systems has sought to use techniques and concepts from the field of formal methods to provide rigorous theoretical analysis and guarantees on complex systems where multiple agents strategically interact, leading to the creation of the field of equilibrium analysis, which studies equilibria concepts from the field of game theory through a complexity-theoretic lens. Multi-agent systems, however, are complex mathematical objects, and, therefore, defining them in a precise mathematical manner is non-trivial. As a result, researchers often considered more restrictive models that are easier to model but lack expressive power or simply omit critical complexity-theoretic results in their analysis. This paper addresses this problem by carefully analyzing and contrasting complexity-theoretic results in the explicit model, a mathematically precise formulation of the models commonly used in the literature, and the circuit-based model, a novel model that addresses the problems found in the literature. The utility of the circuit-based model is demonstrated through a comprehensive analysis that considers upper and lower bounds for the realizability and verification problems, the two most important decision problems in equilibrium analysis, for both models. By conducting this analysis, we see that problematic issues that are endemic to the explicit model and the equilibrium analysis literature as a whole are adequately handled by the circuit-based model.

</details>


### [154] [A General Theory of Proportionality with Additive Utilities](https://arxiv.org/abs/2602.08504)
*Piotr Skowron*

Main category: cs.GT

TL;DR: 本文提出适用于基数选票的比例规则，并引入生成比例排名的方法。


<details>
  <summary>Details</summary>
Motivation: 现有适用于一般模型的比例公理提出的规则仅适用于批准选票，需为基数选票提出比例规则。

Method: 针对基数选票提出比例规则，引入生成比例排名的方法。

Result: 提出了适用于基数选票的比例规则及生成比例排名的方法。

Conclusion: 可为基于选民偏好和约束条件进行候选人子集选择问题在基数选票场景下提供比例规则。

Abstract: We consider a model where a subset of candidates must be selected based on voter preferences, subject to general constraints that specify which subsets are feasible. This model generalizes committee elections with diversity constraints, participatory budgeting (including constraints specifying how funds must be allocated to projects from different pools), and public decision-making. Axioms of proportionality have recently been defined for this general model, but the proposed rules apply only to approval ballots, where each voter submits a subset of candidates she finds acceptable. We propose proportional rules for cardinal ballots, where each voter assigns a numerical value to each candidate corresponding to her utility if that candidate is selected. In developing these rules, we also introduce methods that produce proportional rankings, ensuring that every prefix of the ranking satisfies proportionality.

</details>


### [155] [An Automata-Based Approach to Games with $ω$-Automatic Preferences](https://arxiv.org/abs/2602.08549)
*Véronique Bruyère,Emmanuel Filiot,Christophe Grandmont,Jean-François Raskin*

Main category: cs.GT

TL;DR: 本文研究图上多人回合制游戏，玩家偏好由确定性奇偶自动机建模，分析相关问题的计算复杂度，填补文献中的复杂度空白。


<details>
  <summary>Details</summary>
Motivation: 现有工作多关注特定奖励函数，本文旨在以ω - 自动关系建模玩家偏好来研究图上多人回合制游戏。

Method: 进行计算分析，引入值的概念，利用交替奇偶自动机和ω - 自动特征；对多人游戏和纳什均衡，基于APW构造进行研究。

Result: 证明值的集合可由多项式大小的交替奇偶自动机识别；确定价值和最优策略相关问题的计算复杂度；填补文献复杂度空白，得出合作理性合成的复杂度及非合作情况下的不可判定性。

Conclusion: 以新方式建模玩家偏好研究图上多人回合制游戏，明确了多种问题的计算性质。

Abstract: This paper studies multiplayer turn-based games on graphs in which player preferences are modeled as $ω$-automatic relations given by deterministic parity automata. This contrasts with most existing work, which focuses on specific reward functions. We conduct a computational analysis of these games, starting with the threshold problem in the antagonistic zero-sum case. As in classical games, we introduce the concept of value, defined here as the set of plays a player can guarantee to improve upon, relative to their preference relation. We show that this set is recognized by an alternating parity automaton APW of polynomial size. We also establish the computational complexity of several problems related to the concepts of value and optimal strategy, taking advantage of the $ω$-automatic characterization of value. Next, we shift to multiplayer games and Nash equilibria, and revisit the threshold problem in this context. Based on an APW construction again, we close complexity gaps left open in the literature, and additionally show that cooperative rational synthesis is $\mathsf{PSPACE}$-complete, while it becomes undecidable in the non-cooperative case.

</details>


### [156] [Approximate-EFX Allocations with Ordinal and Limited Cardinal Information](https://arxiv.org/abs/2602.08714)
*Aris Filos-Ratsikas,Georgios Kalantzis,Alexandros A. Voudouris*

Main category: cs.GT

TL;DR: 研究离散公平分配问题中在有限信息下的α - EFX算法，给出α值和查询次数权衡及特殊情况结果。


<details>
  <summary>Details</summary>
Motivation: 过往文献多在完全了解估值函数下研究α - EFX分配，受社会选择中失真研究启发，考虑在有限信息下操作的α - EFX算法。

Method: 假定算法能获取序偏好排名，并可少量查询获取物品价值，进行相关分析。

Result: 得到α值和所需查询次数的（近最优）权衡，尤其关注常数EFX近似，还对特殊情况给出改进的积极结果。

Conclusion: 在有限信息下对α - EFX算法研究取得有价值的结果。

Abstract: We study a discrete fair division problem where $n$ agents have additive valuation functions over a set of $m$ goods. We focus on the well-known $α$-EFX fairness criterion, according to which the envy of an agent for another agent is bounded multiplicatively by $α$, after the removal of any good from the envied agent's bundle. The vast majority of the literature has studied $α$-EFX allocations under the assumption that full knowledge of the valuation functions of the agents is available. Motivated by the established literature on the distortion in social choice, we instead consider $α$-EFX algorithms that operate under limited information on these functions. In particular, we assume that the algorithm has access to the ordinal preference rankings, and is allowed to make a small number of queries to obtain further access to the underlying values of the agents for the goods. We show (near-optimal) tradeoffs between the values of $α$ and the number of queries required to achieve those, with a particular focus on constant EFX approximations. We also consider two interesting special cases, namely instances with a constant number of agents, or with two possible values, and provide improved positive results.

</details>


### [157] [Distortion of Metric Voting with Bounded Randomness](https://arxiv.org/abs/2602.08871)
*Ziyi Cai,D. D. Gao,Prasanna Ramakrishnan,Kangning Wang*

Main category: cs.GT

TL;DR: 研究度量失真框架下投票规则设计，展示用有界随机性打破3的失真屏障的投票规则。


<details>
  <summary>Details</summary>
Motivation: 探索确定性规则和随机规则在投票规则设计中的权衡，尝试用有界随机性打破3的失真屏障。

Method: 基于最大彩票和稳定彩票的失真与近似的新结构结果进行分析，提出特定投票规则。

Result: 提出投票规则能实现至多3 - ε（ε>0）的失真，且从确定性确定的常规模列表中随机选获胜者。

Conclusion: 可以使用有界随机性打破3的失真屏障。

Abstract: We study the design of voting rules in the metric distortion framework. It is known that any deterministic rule suffers distortion of at least $3$, and that randomized rules can achieve distortion strictly less than $3$, often at the cost of reduced transparency and interpretability. In this work, we explore the trade-off between these paradigms by asking whether it is possible to break the distortion barrier of $3$ using only "bounded" randomness. We answer in the affirmative by presenting a voting rule that (1) achieves distortion of at most $3 - \varepsilon$ for some absolute constant $\varepsilon > 0$, and (2) selects a winner uniformly at random from a deterministically identified list of constant size. Our analysis builds on new structural results for the distortion and approximation of Maximal Lotteries and Stable Lotteries.

</details>


### [158] [Maximin Shares with Lower Quotas](https://arxiv.org/abs/2602.08966)
*Hirota Kinoshita,Ayumi Igarashi*

Main category: cs.GT

TL;DR: 本文研究带上下配额的异质加性估值下不可分物品公平分配，给出不同场景下MMS近似分配的多项式时间算法。


<details>
  <summary>Details</summary>
Motivation: 在人员分配、计算资源分配等应用中，带上下配额的不可分物品公平分配问题很关键，此前对仅含上配额的情况有研究，本文进一步拓展。

Method: 通过理论分析和算法设计，给出不同场景下找到MMS近似分配的多项式时间算法。

Result: 在任意上下配额下，能找到商品的(2n/(3n - 1)) - MMS分配和任务的((3n - 1)/(2n)) - MMS分配；在物品分多类有各自上下配额场景下，能找到商品的(n/(2n - 1)) - MMS分配或任务的((2n - 1)/n) - MMS分配。

Conclusion: 研究成果扩展了此前仅考虑上配额的基数约束问题的相关工作。

Abstract: We study the fair division of indivisible items among $n$ agents with heterogeneous additive valuations, subject to lower and upper quotas on the number of items allocated to each agent. Such constraints are crucial in various applications, ranging from personnel assignments to computing resource distribution. This paper focuses on the fairness criterion known as maximin shares (MMS) and its approximations. Under arbitrary lower and upper quotas, we show that a $\left(\frac{2n}{3n-1}\right)$-MMS allocation of goods exists and can be computed in polynomial time, while we also present a polynomial-time algorithm for finding a $\left(\frac{3n-1}{2n}\right)$-MMS allocation of chores. Furthermore, we consider the generalized scenario where items are partitioned into multiple categories, each with its own lower and upper quotas. In this setting, our algorithm computes an $\left(\frac{n}{2n-1}\right)$-MMS allocation of goods or a $\left(\frac{2n-1}{n}\right)$-MMS allocation of chores in polynomial time. These results extend previous work on the cardinality constraints, i.e., the special case where only upper quotas are imposed.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [159] [Reasoning-Augmented Representations for Multimodal Retrieval](https://arxiv.org/abs/2602.07125)
*Jianrui Zhang,Anirudh Sundara Rajan,Brandon Han,Soochahn Lee,Sukanta Ganguly,Yong Jae Lee*

Main category: cs.IR

TL;DR: 文章提出以数据为中心框架，可在通用多模态检索前进行推理，增强训练方法在M - BEIR上有更好表现。


<details>
  <summary>Details</summary>
Motivation: 现代嵌入模型在需要潜在推理的查询中表现脆弱，原因常为数据诱导，单次嵌入既要推理又要压缩会导致虚假特征匹配。

Method: 提出以数据为中心的框架，利用强大的视觉 - 语言模型使语义显式化，在推理时增强数据，并在语义密集表示上训练检索器。

Result: 在M - BEIR上，推理增强训练方法比强基线有持续提升，消融实验表明语料库增强主要有益于知识密集型查询，查询增强对组合修改请求至关重要。

Conclusion: 所提方法能有效解决通用多模态检索中推理相关问题，代码已公开。

Abstract: Universal Multimodal Retrieval (UMR) seeks any-to-any search across text and vision, yet modern embedding models remain brittle when queries require latent reasoning (e.g., resolving underspecified references or matching compositional constraints). We argue this brittleness is often data-induced: when images carry "silent" evidence and queries leave key semantics implicit, a single embedding pass must both reason and compress, encouraging spurious feature matching. We propose a data-centric framework that decouples these roles by externalizing reasoning before retrieval. Using a strong Vision--Language Model, we make implicit semantics explicit by densely captioning visual evidence in corpus entries, resolving ambiguous multimodal references in queries, and rewriting verbose instructions into concise retrieval constraints. Inference-time enhancement alone is insufficient; the retriever must be trained on these semantically dense representations to avoid distribution shift and fully exploit the added signal. Across M-BEIR, our reasoning-augmented training method yields consistent gains over strong baselines, with ablations showing that corpus enhancement chiefly benefits knowledge-intensive queries while query enhancement is critical for compositional modification requests. We publicly release our code at https://github.com/AugmentedRetrieval/ReasoningAugmentedRetrieval.

</details>


### [160] [Multimodal Enhancement of Sequential Recommendation](https://arxiv.org/abs/2602.07207)
*Bucher Sahyouni,Matthew Vowels,Liqun Chen,Simon Hadfield*

Main category: cs.IR

TL;DR: 提出MuSTRec推荐框架统一多模态和序列推荐范式，在多数据集上表现优，还介绍新范式特点。


<details>
  <summary>Details</summary>
Motivation: 统一多模态和序列推荐范式，提升推荐性能。

Method: 构建物品-物品图捕捉跨物品相似性和协同过滤信号，用基于频率的自注意力模块捕捉用户偏好。

Result: 在多个亚马逊数据集上，相比多模态和序列的先进基线，性能提升最高达33.5%；在小数据集上，将用户嵌入集成到序列推荐中短期指标提升最高达200%。

Conclusion: MuSTRec框架有效，还指出新推荐范式需新数据划分机制等特点。

Abstract: We propose a novel recommender framework, MuSTRec (Multimodal and Sequential Transformer-based Recommendation), that unifies multimodal and sequential recommendation paradigms. MuSTRec captures cross-item similarities and collaborative filtering signals, by building item-item graphs from extracted text and visual features. A frequency-based self-attention module additionally captures the short- and long-term user preferences. Across multiple Amazon datasets, MuSTRec demonstrates superior performance (up to 33.5% improvement) over multimodal and sequential state-of-the-art baselines. Finally, we detail some interesting facets of this new recommendation paradigm. These include the need for a new data partitioning regime, and a demonstration of how integrating user embeddings into sequential recommendation leads to drastically increased short-term metrics (up to 200% improvement) on smaller datasets. Our code is availabe at https://anonymous.4open.science/r/MuSTRec-D32B/ and will be made publicly available.

</details>


### [161] [Sequences as Nodes for Contrastive Multimodal Graph Recommendation](https://arxiv.org/abs/2602.07208)
*Bucher Sahyouni,Matthew Vowels,Liqun Chen,Simon Hadfield*

Main category: cs.IR

TL;DR: 提出MuSICRec推荐器，结合多信号，缓解噪声，在多数据集上表现优于基线，尤其对短历史用户有效。


<details>
  <summary>Details</summary>
Motivation: 解决推荐系统中冷启动和数据稀疏问题，同时避免现有增强技术带来的噪声和语义破坏。

Method: 提出MuSICRec，构建SI视图，在SI图上传播，用ID引导门调制多模态信息。

Result: 在Amazon的多个数据集上，MuSICRec优于各类基线模型，对短历史用户增益最大。

Conclusion: MuSICRec能有效缓解稀疏性和冷启动挑战。

Abstract: To tackle cold-start and data sparsity issues in recommender systems, numerous multimodal, sequential, and contrastive techniques have been proposed. While these augmentations can boost recommendation performance, they tend to add noise and disrupt useful semantics. To address this, we propose MuSICRec (Multimodal Sequence-Item Contrastive Recommender), a multi-view graph-based recommender that combines collaborative, sequential, and multimodal signals. We build a sequence-item (SI) view by attention pooling over the user's interacted items to form sequence nodes. We propagate over the SI graph, obtaining a second view organically as an alternative to artificial data augmentation, while simultaneously injecting sequential context signals. Additionally, to mitigate modality noise and align the multimodal information, the contribution of text and visual features is modulated according to an ID-guided gate.
  We evaluate under a strict leave-two-out split against a broad range of sequential, multimodal, and contrastive baselines. On the Amazon Baby, Sports, and Electronics datasets, MuSICRec outperforms state-of-the-art baselines across all model types. We observe the largest gains for short-history users, mitigating sparsity and cold-start challenges. Our code is available at https://anonymous.4open.science/r/MuSICRec-3CEE/ and will be made publicly available.

</details>


### [162] [Progressive Searching for Retrieval in RAG](https://arxiv.org/abs/2602.07297)
*Taehee Jeong,Xingzhe Zhao,Peizu Li,Markus Valvur,Weihua Zhao*

Main category: cs.IR

TL;DR: 提出用于RAG系统检索过程的渐进式搜索算法，可平衡维度、速度和准确性。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型信息过时和幻觉问题，RAG系统高效准确搜索至关重要，需成本效益高的搜索算法。

Method: 提出渐进式搜索算法，通过分层搜索逐步优化候选集，从低维嵌入到高维目标维度。

Result: 渐进式搜索在RAG系统中能平衡维度、速度和准确性。

Conclusion: 该算法可实现可扩展和高性能的检索，即使针对大型数据库也适用。

Abstract: Retrieval Augmented Generation (RAG) is a promising technique for mitigating two key limitations of large language models (LLMs): outdated information and hallucinations. RAG system stores documents as embedding vectors in a database. Given a query, search is executed to find the most related documents. Then, the topmost matching documents are inserted into LLMs' prompt to generate a response. Efficient and accurate searching is critical for RAG to get relevant information. We propose a cost-effective searching algorithm for retrieval process. Our progressive searching algorithm incrementally refines the candidate set through a hierarchy of searches, starting from low-dimensional embeddings and progressing into a higher, target-dimensionality. This multi-stage approach reduces retrieval time while preserving the desired accuracy. Our findings demonstrate that progressive search in RAG systems achieves a balance between dimensionality, speed, and accuracy, enabling scalable and high-performance retrieval even for large databases.

</details>


### [163] [Principled Synthetic Data Enables the First Scaling Laws for LLMs in Recommendation](https://arxiv.org/abs/2602.07298)
*Benyu Zhang,Qiang Zhang,Jianpeng Cheng,Hong-You Chen,Qifei Wang,Wei Sun,Shen Li,Jia Li,Jiahao Wu,Xiangjun Fan,Hong Yan*

Main category: cs.IR

TL;DR: 论文提出生成高质量合成数据的分层框架，解决LLM在推荐系统中缺乏可预测扩展定律的问题，证明合成数据优势并首次展示稳健幂律扩展。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在推荐系统中因原始用户交互数据问题缺乏可预测扩展定律的问题。

Method: 引入分层框架生成高质量合成数据，创建教学课程。

Result: 标准顺序模型在合成数据上训练，下游排名任务表现比真实数据训练的模型高130%，展示了LLM持续预训练的稳健幂律扩展。

Conclusion: 建立了推荐领域可靠扩展LLM能力的基础方法，将研究重点转向利用高质量结构化信息。

Abstract: Large Language Models (LLMs) represent a promising frontier for recommender systems, yet their development has been impeded by the absence of predictable scaling laws, which are crucial for guiding research and optimizing resource allocation. We hypothesize that this may be attributed to the inherent noise, bias, and incompleteness of raw user interaction data in prior continual pre-training (CPT) efforts. This paper introduces a novel, layered framework for generating high-quality synthetic data that circumvents such issues by creating a curated, pedagogical curriculum for the LLM. We provide powerful, direct evidence for the utility of our curriculum by showing that standard sequential models trained on our principled synthetic data significantly outperform ($+130\%$ on recall@100 for SasRec) models trained on real data in downstream ranking tasks, demonstrating its superiority for learning generalizable user preference patterns. Building on this, we empirically demonstrate, for the first time, robust power-law scaling for an LLM that is continually pre-trained on our high-quality, recommendation-specific data. Our experiments reveal consistent and predictable perplexity reduction across multiple synthetic data modalities. These findings establish a foundational methodology for reliable scaling LLM capabilities in the recommendation domain, thereby shifting the research focus from mitigating data deficiencies to leveraging high-quality, structured information.

</details>


### [164] [LIT-GRAPH: Evaluating Deep vs. Shallow Graph Embeddings for High-Quality Text Recommendation in Domain-Specific Knowledge Graphs](https://arxiv.org/abs/2602.07307)
*Nirmal Gelal,Chloe Snow,Kathleen M. Jagodnik,Ambyr Rios,Hande Küçük McGinty*

Main category: cs.IR

TL;DR: 提出LIT - GRAPH推荐系统，比较四种图嵌入范式，发现浅模型适合结构链接预测，R - GCN在语义排名占优，能给出高质量特定领域推荐。


<details>
  <summary>Details</summary>
Motivation: 解决高中英语课程停滞问题，帮助英语教师选择多样化、教学上匹配的教学文献。

Method: 构建基于英语文学本体的LIT - GRAPH系统，比较DeepWalk、Biased Random Walk (BRW)、Hybrid和Relational Graph Convolutional Network (R - GCN)四种图嵌入范式。

Result: 浅模型在结构链接预测表现出色，R - GCN在语义排名上占主导。

Conclusion: R - GCN通过特定关系消息传递，优先考虑教学相关性而非原始连接性，能提供优质、特定领域的推荐。

Abstract: This study presents LIT-GRAPH (Literature Graph for Recommendation and Pedagogical Heuristics), a novel knowledge graph-based recommendation system designed to scaffold high school English teachers in selecting diverse, pedagogically aligned instructional literature. The system is built upon an ontology for English literature, addressing the challenge of curriculum stagnation, where we compare four graph embedding paradigms: DeepWalk, Biased Random Walk (BRW), Hybrid (concatenated DeepWalk and BRW vectors), and the deep model Relational Graph Convolutional Network (R-GCN). Results reveal a critical divergence: while shallow models excelled in structural link prediction, R-GCN dominated semantic ranking. By leveraging relation-specific message passing, the deep model prioritizes pedagogical relevance over raw connectivity, resulting in superior, high-quality, domain-specific recommendations.

</details>


### [165] [Semantic Search At LinkedIn](https://arxiv.org/abs/2602.07309)
*Fedor Borisyuk,Sriram Vasudevan,Muchen Wu,Guoyao Li,Benjamin Le,Shaobo Zhang,Qianqi Kay Shen,Yuchin Juan,Kayhan Behdin,Liming Dong,Kaixu Yang,Shusen Jing,Ravi Pothamsetty,Rajat Arora,Sophie Yanying Sheng,Vitaly Abdrashitov,Yang Zhao,Lin Su,Xiaoqing Wang,Chujie Zheng,Sarang Metkar,Rupesh Gupta,Igor Lapchuk,David N. Racca,Madhumitha Mohan,Yanbo Li,Haojun Li,Saloni Gandhi,Xueying Lu,Chetan Bhole,Ali Hooshmand,Xin Yang,Raghavan Muthuregunathan,Jiajun Zhang,Mathew Teoh,Adam Coler,Abhinav Gupta,Xiaojing Ma,Sundara Raman Ramachandran,Morteza Ramezani,Yubo Wang,Lijuan Zhang,Richard Li,Jian Sheng,Chanh Nguyen,Yen-Chi Chen,Chuanrui Zhu,Claire Zhang,Jiahao Xu,Deepti Kulkarni,Qing Lan,Arvind Subramaniam,Ata Fatahibaarzi,Steven Shimizu,Yanning Chen,Zhipeng Wang,Ran He,Zhengze Zhou,Qingquan Song,Yun Dai,Caleb Johnson,Ping Liu,Shaghayegh Gharghabi,Gokulraj Mohanasundaram,Juan Bottaro,Santhosh Sachindran,Qi Guo,Yunxiang Ren,Chengming Jiang,Di Mo,Luke Simon,Jianqiang Shen,Jingwei Wu,Wenjing Zhang*

Main category: cs.IR

TL;DR: 介绍LinkedIn基于大语言模型的语义搜索框架，通过多种技术提升推理效率，实现高效排名系统。


<details>
  <summary>Details</summary>
Motivation: 大语言模型语义搜索扩展需提升推理效率。

Method: 结合LLM相关性判断、基于嵌入的检索和多教师蒸馏训练的小语言模型，采用预填充推理架构，结合模型剪枝、上下文压缩和文本嵌入混合交互。

Result: 在固定延迟约束下将排名吞吐量提高75倍以上，保留接近教师级别的NDCG，实现效率与传统方法相当的生产级LLM排名系统。

Conclusion: 该框架在效率、质量和用户参与度方面取得显著提升。

Abstract: Semantic search with large language models (LLMs) enables retrieval by meaning rather than keyword overlap, but scaling it requires major inference efficiency advances. We present LinkedIn's LLM-based semantic search framework for AI Job Search and AI People Search, combining an LLM relevance judge, embedding-based retrieval, and a compact Small Language Model trained via multi-teacher distillation to jointly optimize relevance and engagement. A prefill-oriented inference architecture co-designed with model pruning, context compression, and text-embedding hybrid interactions boosts ranking throughput by over 75x under a fixed latency constraint while preserving near-teacher-level NDCG, enabling one of the first production LLM-based ranking systems with efficiency comparable to traditional approaches and delivering significant gains in quality and user engagement.

</details>


### [166] [High Fidelity Textual User Representation over Heterogeneous Sources via Reinforcement Learning](https://arxiv.org/abs/2602.07333)
*Rajat Arora,Ye Tao,Jianqiang Shen,Ping Liu,Muchen Wu,Qianqi Shen,Benjamin Le,Fedor Borisyuk,Jingwei Wu,Wenjing Zhang*

Main category: cs.IR

TL;DR: 提出一种强化学习框架用于大规模职位平台成员的异构文本源统一表示，实验显示对关键业务指标有显著提升。


<details>
  <summary>Details</summary>
Motivation: 大规模职位平台有效个性化需对成员基于异构文本源建模，在推荐系统采用大语言模型时，从异构源创建统一、可解释和简洁表示对低延迟在线环境很关键。

Method: 提出一种强化学习框架，利用隐式用户参与信号作为主要奖励提炼重要信息，辅以基于规则的奖励来强制格式和长度约束。

Result: 在LinkedIn多个产品上的大量离线实验表明关键下游业务指标有显著改善。

Conclusion: 提供了一种实用、无标签且可扩展的方法，用于构建可直接与基于大语言模型的系统兼容的可解释用户表示。

Abstract: Effective personalization on large-scale job platforms requires modeling members based on heterogeneous textual sources, including profiles, professional data, and search activity logs. As recommender systems increasingly adopt Large Language Models (LLMs), creating unified, interpretable, and concise representations from heterogeneous sources becomes critical, especially for latency-sensitive online environments. In this work, we propose a novel Reinforcement Learning (RL) framework to synthesize a unified textual representation for each member. Our approach leverages implicit user engagement signals (e.g., clicks, applies) as the primary reward to distill salient information. Additionally, the framework is complemented by rule-based rewards that enforce formatting and length constraints. Extensive offline experiments across multiple LinkedIn products, one of the world's largest job platforms, demonstrate significant improvements in key downstream business metrics. This work provides a practical, labeling-free, and scalable solution for constructing interpretable user representations that are directly compatible with LLM-based systems.

</details>


### [167] [MDL: A Unified Multi-Distribution Learner in Large-scale Industrial Recommendation through Tokenization](https://arxiv.org/abs/2602.07520)
*Shanlei Mu,Yuchen Jiang,Shikang Wu,Shiyong Hong,Tianmu Sha,Junjie Zhang,Jie Zhu,Zhe Chen,Zhe Wang,Jingjian Lin*

Main category: cs.IR

TL;DR: 现有工业推荐系统的多场景和多任务学习方法有不足，提出统一的多分布学习框架MDL，实验显示其性能优于现有方法，已在生产环境部署。


<details>
  <summary>Details</summary>
Motivation: 解决现有工业推荐系统多场景学习和多任务学习中模型参数利用不足，以及难以在统一框架中联合建模场景和任务信息的问题。

Method: 受大语言模型“提示”范式启发，将场景和任务信息视为特殊令牌；引入统一信息令牌化模块；设计三种协同机制，实现自底向上逐层激活模型参数空间。

Result: 在真实工业数据集实验中，MDL显著优于现有基线；抖音搜索平台在线A/B测试显示，LT30提升0.0626%，查询变更率降低0.3267%。

Conclusion: MDL框架有效解决现有问题，表现优于当前方法，可用于实际生产。

Abstract: Industrial recommender systems increasingly adopt multi-scenario learning (MSL) and multi-task learning (MTL) to handle diverse user interactions and contexts, but existing approaches suffer from two critical drawbacks: (1) underutilization of large-scale model parameters due to limited interaction with complex feature modules, and (2) difficulty in jointly modeling scenario and task information in a unified framework. To address these challenges, we propose a unified \textbf{M}ulti-\textbf{D}istribution \textbf{L}earning (MDL) framework, inspired by the "prompting" paradigm in large language models (LLMs). MDL treats scenario and task information as specialized tokens rather than auxiliary inputs or gating signals. Specifically, we introduce a unified information tokenization module that transforms features, scenarios, and tasks into a unified tokenized format. To facilitate deep interaction, we design three synergistic mechanisms: (1) feature token self-attention for rich feature interactions, (2) domain-feature attention for scenario/task-adaptive feature activation, and (3) domain-fused aggregation for joint distribution prediction. By stacking these interactions, MDL enables scenario and task information to "prompt" and activate the model's vast parameter space in a bottom-up, layer-wise manner. Extensive experiments on real-world industrial datasets demonstrate that MDL significantly outperforms state-of-the-art MSL and MTL baselines. Online A/B testing on Douyin Search platform over one month yields +0.0626\% improvement in LT30 and -0.3267\% reduction in change query rate. MDL has been fully deployed in production, serving hundreds of millions of users daily.

</details>


### [168] [IGMiRAG: Intuition-Guided Retrieval-Augmented Generation with Adaptive Mining of In-Depth Memory](https://arxiv.org/abs/2602.07525)
*Xingliang Hou,Yuyan Liu,Qi Sun,haoxiu wang,Hao Hu,Shaoyi Du,Zhiqiang Tian*

Main category: cs.IR

TL;DR: 提出IGMiRAG框架，集成层次异构超图和模仿人类推理过程的算法，在评估中表现优于基线，提供高效且经济的RAG范式。


<details>
  <summary>Details</summary>
Motivation: 现有将图和超图集成到RAG中的方法存在内存组织不一致、检索成本高的问题。

Method: 构建层次异构超图对齐多粒度知识，通过问题解析器提炼直觉策略，双焦点检索激活瞬时记忆，设计双向扩散算法挖掘深度记忆。

Result: IGMiRAG在EM上比基线高4.8%，F1上高5.0%，token成本能适应任务复杂度 。

Conclusion: 提出了一种高效且经济的RAG范式。

Abstract: Retrieval-augmented generation (RAG) equips large language models (LLMs) with reliable knowledge memory. To strengthen cross-text associations, recent research integrates graphs and hypergraphs into RAG to capture pairwise and multi-entity relations as structured links. However, their misaligned memory organization necessitates costly, disjointed retrieval. To address these limitations, we propose IGMiRAG, a framework inspired by human intuition-guided reasoning. It constructs a hierarchical heterogeneous hypergraph to align multi-granular knowledge, incorporating deductive pathways to simulate realistic memory structures. During querying, IGMiRAG distills intuitive strategies via a question parser to control mining depth and memory window, and activates instantaneous memories as anchors using dual-focus retrieval. Mirroring human intuition, the framework guides retrieval resource allocation dynamically. Furthermore, we design a bidirectional diffusion algorithm that navigates deductive paths to mine in-depth memories, emulating human reasoning processes. Extensive evaluations indicate IGMiRAG outperforms the state-of-the-art baseline by 4.8% EM and 5.0% F1 overall, with token costs adapting to task complexity (average 6.3k+, minimum 3.0k+). This work presents a cost-effective RAG paradigm that improves both efficiency and effectiveness.

</details>


### [169] [MSN: A Memory-based Sparse Activation Scaling Framework for Large-scale Industrial Recommendation](https://arxiv.org/abs/2602.07526)
*Shikang Wu,Hui Lu,Jinqiu Jin,Zheng Chai,Shiyong Hong,Junjie Zhang,Shanlei Mu,Kaiyuan Ma,Tianyi Liu,Yuchao Zheng,Zhe Wang,Jingjian Lin*

Main category: cs.IR

TL;DR: 提出用于推荐模型的基于内存的稀疏激活缩放框架MSN，能高效提升推荐性能且已在抖音搜索排序系统成功部署。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习推荐模型扩展方法计算开销大难部署，稀疏激活缩放方法存在内存访问成本高和个性化能力受限问题。

Method: 提出MSN框架，动态从参数化内存中检索个性化表示并通过内存门控机制集成；采用PKM机制降低内存检索复杂度；引入归一化和过参数化技术；设计定制算子。

Result: 大量实验表明MSN持续提升推荐性能且保持高效；在抖音搜索排序系统部署后，离线评估指标和在线A/B测试均优于现有模型。

Conclusion: MSN是一种有效、高效的深度学习推荐模型扩展方法，可应用于工业系统。

Abstract: Scaling deep learning recommendation models is an effective way to improve model expressiveness. Existing approaches often incur substantial computational overhead, making them difficult to deploy in large-scale industrial systems under strict latency constraints. Recent sparse activation scaling methods, such as Sparse Mixture-of-Experts, reduce computation by activating only a subset of parameters, but still suffer from high memory access costs and limited personalization capacity due to the large size and small number of experts. To address these challenges, we propose MSN, a memory-based sparse activation scaling framework for recommendation models. MSN dynamically retrieves personalized representations from a large parameterized memory and integrates them into downstream feature interaction modules via a memory gating mechanism, enabling fine-grained personalization with low computational overhead. To enable further expansion of the memory capacity while keeping both computational and memory access costs under control, MSN adopts a Product-Key Memory (PKM) mechanism, which factorizes the memory retrieval complexity from linear time to sub-linear complexity. In addition, normalization and over-parameterization techniques are introduced to maintain balanced memory utilization and prevent memory retrieval collapse. We further design customized Sparse-Gather operator and adopt the AirTopK operator to improve training and inference efficiency in industrial settings. Extensive experiments demonstrate that MSN consistently improves recommendation performance while maintaining high efficiency. Moreover, MSN has been successfully deployed in the Douyin Search Ranking System, achieving significant gains over deployed state-of-the-art models in both offline evaluation metrics and large-scale online A/B test.

</details>


### [170] [HypRAG: Hyperbolic Dense Retrieval for Retrieval Augmented Generation](https://arxiv.org/abs/2602.07739)
*Hiren Madhu,Ngoc Bui,Ali Maatouk,Leandros Tassiulas,Smita Krishnaswamy,Menglin Yang,Sukanta Ganguly,Kiran Srinivasan,Rex Ying*

Main category: cs.IR

TL;DR: 本文引入双曲密集检索方法，开发两种模型变体，提出几何感知池化算子，在相关基准测试中表现优于欧几里得基线，凸显双曲表示优势。


<details>
  <summary>Details</summary>
Motivation: 当前检索增强生成（RAG）的密集检索器多局限于欧几里得空间，无法保留自然语言的层次结构，导致语义距离远的文档看似相似，增加幻觉风险。

Method: 引入双曲密集检索，开发HyTE - FH和HyTE - H两种模型变体，提出Outward Einstein Midpoint池化算子。

Result: 在MTEB上，HyTE - FH优于等效欧几里得基线；在RAGBench上，HyTE - H在上下文相关性和答案相关性上比欧几里得基线最多提高29%，且模型更小。

Conclusion: 双曲表示通过基于范数的分离对文档特异性进行编码，凸显了几何归纳偏差在可靠RAG系统中的关键作用。

Abstract: Embedding geometry plays a fundamental role in retrieval quality, yet dense retrievers for retrieval-augmented generation (RAG) remain largely confined to Euclidean space. However, natural language exhibits hierarchical structure from broad topics to specific entities that Euclidean embeddings fail to preserve, causing semantically distant documents to appear spuriously similar and increasing hallucination risk. To address these limitations, we introduce hyperbolic dense retrieval, developing two model variants in the Lorentz model of hyperbolic space: HyTE-FH, a fully hyperbolic transformer, and HyTE-H, a hybrid architecture projecting pre-trained Euclidean embeddings into hyperbolic space. To prevent representational collapse during sequence aggregation, we introduce the Outward Einstein Midpoint, a geometry-aware pooling operator that provably preserves hierarchical structure. On MTEB, HyTE-FH outperforms equivalent Euclidean baselines, while on RAGBench, HyTE-H achieves up to 29% gains over Euclidean baselines in context relevance and answer relevance using substantially smaller models than current state-of-the-art retrievers. Our analysis also reveals that hyperbolic representations encode document specificity through norm-based separation, with over 20% radial increase from general to specific concepts, a property absent in Euclidean embeddings, underscoring the critical role of geometric inductive bias in faithful RAG systems.

</details>


### [171] [Generative Reasoning Re-ranker](https://arxiv.org/abs/2602.07774)
*Mingfu Liang,Yufei Li,Jay Xu,Kavosh Asadi,Xi Liu,Shuo Gu,Kaushik Rangadurai,Frank Shyu,Shuaiwen Wang,Song Yang,Zhijing Li,Jiang Liu,Mengying Sun,Fei Tian,Xiaohan Wei,Chonglin Sun,Jacob Tao,Shike Mei,Hamed Firooz,Wenlin Chen,Luke Simon*

Main category: cs.IR

TL;DR: 针对现有大语言模型用于推荐系统的局限，提出GR2框架，实验显示其效果优于现有方法，且证明推理轨迹和强化学习奖励设计很重要。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型用于推荐系统存在对重排序阶段忽视、推理能力未充分利用、非语义ID表示物品有扩展性挑战等局限。

Method: 提出GR2框架，有三阶段训练流程，包括对预训练大模型进行中间训练、利用大模型生成推理痕迹进行监督微调、应用DAPO方法进行强化学习监督。

Result: 在两个真实数据集上实验，GR2在召回率和归一化折损累积增益上超过现有方法OneRec - Think；消融实验表明高级推理痕迹带来指标显著提升。

Conclusion: 高级推理痕迹对推荐指标有提升作用，强化学习奖励设计对重排序很关键，条件可验证奖励可优化重排序效果。

Abstract: Recent studies increasingly explore Large Language Models (LLMs) as a new paradigm for recommendation systems due to their scalability and world knowledge. However, existing work has three key limitations: (1) most efforts focus on retrieval and ranking, while the reranking phase, critical for refining final recommendations, is largely overlooked; (2) LLMs are typically used in zero-shot or supervised fine-tuning settings, leaving their reasoning abilities, especially those enhanced through reinforcement learning (RL) and high-quality reasoning data, underexploited; (3) items are commonly represented by non-semantic IDs, creating major scalability challenges in industrial systems with billions of identifiers. To address these gaps, we propose the Generative Reasoning Reranker (GR2), an end-to-end framework with a three-stage training pipeline tailored for reranking. First, a pretrained LLM is mid-trained on semantic IDs encoded from non-semantic IDs via a tokenizer achieving $\ge$99% uniqueness. Next, a stronger larger-scale LLM generates high-quality reasoning traces through carefully designed prompting and rejection sampling, which are used for supervised fine-tuning to impart foundational reasoning skills. Finally, we apply Decoupled Clip and Dynamic sAmpling Policy Optimization (DAPO), enabling scalable RL supervision with verifiable rewards designed specifically for reranking. Experiments on two real-world datasets demonstrate GR2's effectiveness: it surpasses the state-of-the-art OneRec-Think by 2.4% in Recall@5 and 1.3% in NDCG@5. Ablations confirm that advanced reasoning traces yield substantial gains across metrics. We further find that RL reward design is crucial in reranking: LLMs tend to exploit reward hacking by preserving item order, motivating conditional verifiable rewards to mitigate this behavior and optimize reranking performance.

</details>


### [172] [SAGE: Scalable AI Governance & Evaluation](https://arxiv.org/abs/2602.07840)
*Benjamin Le,Xueying Lu,Nick Stern,Wenqiong Liu,Igor Lapchuk,Xiang Li,Baofen Zheng,Kevin Rosenberg,Jiewen Huang,Zhe Zhang,Abraham Cabangbang,Satej Milind Wagle,Jianqiang Shen,Raghavan Muthuregunathan,Abhinav Gupta,Mathew Teoh,Andrew Kirk,Thomas Kwan,Jingwei Wu,Wenjing Zhang*

Main category: cs.IR

TL;DR: 提出SAGE框架解决大规模搜索系统相关性评估问题，通过双向校准循环和师生蒸馏技术，在LinkedIn搜索生态中应用，提升了日活用户数。


<details>
  <summary>Details</summary>
Motivation: 传统评估方法难以捕捉大规模搜索系统中高影响相关性失败的全貌，存在人力监督与系统高吞吐量需求的治理差距。

Method: 构建SAGE框架，包含双向校准循环，运用师生蒸馏技术将高保真判断转化为低成本学生代理模型。

Result: 在LinkedIn搜索生态中部署SAGE，可指导模型迭代、进行离线评估、检测回归问题，使日活用户提升0.25%。

Conclusion: SAGE框架能有效解决大规模搜索系统相关性评估问题，提升系统性能。

Abstract: Evaluating relevance in large-scale search systems is fundamentally constrained by the governance gap between nuanced, resource-constrained human oversight and the high-throughput requirements of production systems. While traditional approaches rely on engagement proxies or sparse manual review, these methods often fail to capture the full scope of high-impact relevance failures. We present \textbf{SAGE} (Scalable AI Governance \& Evaluation), a framework that operationalizes high-quality human product judgment as a scalable evaluation signal. At the core of SAGE is a bidirectional calibration loop where natural-language \emph{Policy}, curated \emph{Precedent}, and an \emph{LLM Surrogate Judge} co-evolve. SAGE systematically resolves semantic ambiguities and misalignments, transforming subjective relevance judgment into an executable, multi-dimensional rubric with near human-level agreement. To bridge the gap between frontier model reasoning and industrial-scale inference, we apply teacher-student distillation to transfer high-fidelity judgments into compact student surrogates at \textbf{92$\times$} lower cost. Deployed within LinkedIn Search ecosystems, SAGE guided model iteration through simulation-driven development, distilling policy-aligned models for online serving and enabling rapid offline evaluation. In production, it powered policy oversight that measured ramped model variants and detected regressions invisible to engagement metrics. Collectively, these drove a \textbf{0.25\%} lift in LinkedIn daily active users.

</details>


### [173] [SimGR: Escaping the Pitfalls of Generative Decoding in LLM-based Recommendation](https://arxiv.org/abs/2602.07847)
*Yuanbo Zhao,Ruochen Liu,Senzhang Wang,Jun Yin,Yuxin Dong,Huan Gong,Hao Chen,Shirui Pan,Chengqi Zhang*

Main category: cs.IR

TL;DR: 现有基于大语言模型的生成式推荐方法在估计物品级偏好分布时存在系统偏差，本文提出SimGR框架直接建模物品级偏好分布，实验显示其性能更优。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的生成式推荐方法在估计物品级偏好分布时会引入系统偏差，影响推荐准确性。

Method: 提出SimGR框架，在共享潜在空间中直接建模物品级偏好分布，并通过相似度对物品进行排序。

Result: 在多个数据集和大语言模型骨干上的广泛实验表明，SimGR始终优于现有的生成式推荐器。

Conclusion: SimGR框架能有效解决现有方法的分布失真问题，提高推荐性能。

Abstract: A core objective in recommender systems is to accurately model the distribution of user preferences over items to enable personalized recommendations. Recently, driven by the strong generative capabilities of large language models (LLMs), LLM-based generative recommendation has become increasingly popular. However, we observe that existing methods inevitably introduce systematic bias when estimating item-level preference distributions. Specifically, autoregressive generation suffers from incomplete coverage due to beam search pruning, while parallel generation distorts probabilities by assuming token independence. We attribute this issue to a fundamental modeling mismatch: these methods approximate item-level distributions via token-level generation, which inherently induces approximation errors. Through both theoretical analysis and empirical validation, we demonstrate that token-level generation cannot faithfully substitute item-level generation, leading to biased item distributions. To address this, we propose \textbf{Sim}ply \textbf{G}enerative \textbf{R}ecommendation (\textbf{SimGR}), a framework that directly models item-level preference distributions in a shared latent space and ranks items by similarity, thereby aligning the modeling objective with recommendation and mitigating distributional distortion. Extensive experiments across multiple datasets and LLM backbones show that SimGR consistently outperforms existing generative recommenders. Our code is available at https://anonymous.4open.science/r/SimGR-C408/

</details>


### [174] [Learning to Alleviate Familiarity Bias in Video Recommendation](https://arxiv.org/abs/2602.07987)
*Zheng Ren,Yi Wu,Jianan Lu,Acar Ary,Yiqu Liu,Li Wei,Lukasz Heldt*

Main category: cs.IR

TL;DR: 提出LAFB框架缓解推荐系统熟悉度偏差，经评估有效并已在YouTube应用。


<details>
  <summary>Details</summary>
Motivation: 现代视频推荐系统因行为偏差存在结构曝光不平衡问题，需缓解熟悉度偏差。

Method: 提出LAFB框架，用离散和连续交互特征建模用户 - 内容熟悉度，估计个性化去偏差因子调整用户评分预测分数。

Result: LAFB增加了新颖观看时间份额，改善新兴创作者曝光和内容多样性，保持整体观看时间和短期满意度稳定。

Conclusion: LAFB框架有效，可缓解熟悉度偏差，能应用于实际推荐系统。

Abstract: Modern video recommendation systems aim to optimize user engagement and platform objectives, yet often face structural exposure imbalances caused by behavioral biases. In this work, we focus on the post-ranking stage and present LAFB (Learning to Alleviate Familiarity Bias), a lightweight and model-agnostic framework designed to mitigate familiarity bias in recommendation outputs. LAFB models user-content familiarity using discrete and continuous interaction features, and estimates personalized debiasing factors to adjust user rating prediction scores, thereby reducing the dominance of familiar content in the final ranking. We conduct large-scale offline evaluations and online A/B testing in a real-world recommendation system, under a unified serving stack that also compares LAFB with deployable popularity-oriented remedies. Results show that LAFB increases novel watch-time share and improves exposure for emerging creators and overall content diversity, while maintaining stable overall watch time and short-term satisfaction. LAFB has already been launched in the post-ranking stage of YouTube's recommendation system, demonstrating its effectiveness in real-world applications.

</details>


### [175] [IRB: Automated Generation of Robust Factuality Benchmarks](https://arxiv.org/abs/2602.08070)
*Lam Thanh Do,Bhagyashree Taleka,Hozaifa Ammar Bhutta,Vikram Sharma Mailthody,Kevin Chen-Chuan Chang,Wen-mei Hwu*

Main category: cs.IR

TL;DR: 提出IRB框架自动生成基准评估RAG系统事实性，构建基准评估大模型和检索器，发现对大模型有挑战，推理LLMs更可靠，改进检索组件性价比更高。


<details>
  <summary>Details</summary>
Motivation: 静态基准评估RAG系统存在快速饱和和需大量人工维护鲁棒性的问题。

Method: 采用结构化生成管道，利用事实支架和算法支架。

Result: IRB对前沿大模型在闭卷设置中构成重大挑战，推理LLMs更可靠，改进检索组件比扩展生成器更具成本效益。

Conclusion: IRB可自动生成基准评估RAG系统事实性，且指出了提升RAG系统正确性更具性价比的方向。

Abstract: Static benchmarks for RAG systems often suffer from rapid saturation and require significant manual effort to maintain robustness. To address this, we present IRB, a framework for automatically generating benchmarks to evaluate the factuality of RAG systems. IRB employs a structured generation pipeline utilizing \textit{factual scaffold} and \textit{algorithmic scaffold}. We utilize IRB to construct a benchmark and evaluate frontier LLMs and retrievers. Our results demonstrate that IRB poses a significant challenge for frontier LLMs in the closed-book setting. Furthermore, our evaluation suggests that reasoning LLMs are more reliable, and that improving the retrieval component may yield more cost-effective gains in RAG system correctness than scaling the generator.

</details>


### [176] [A Sketch+Text Composed Image Retrieval Dataset for Thangka](https://arxiv.org/abs/2602.08411)
*Jinyu Xu,Yi Sun,Jiangling Zhang,Qing Xie,Daomin Ji,Zhifeng Bao,Jiachen Li,Yanchun Ma,Yongjian Liu*

Main category: cs.IR

TL;DR: 提出针对唐卡图像的CIRThan数据集，包含图像、草图和文本描述，评估现有CIR方法并发现其不足，认为该数据集对特定领域图像检索有价值。


<details>
  <summary>Details</summary>
Motivation: 现有CIR基准主要关注通用领域图像，对需要细粒度语义推理等的检索场景支持有限，需针对特定领域构建数据集。

Method: 引入CIRThan数据集，提供标准化数据划分、分析及对代表性监督和零样本CIR方法的基准评估。

Result: 现有为通用领域开发的CIR方法难以有效对齐草图抽象和分层文本语义与细粒度唐卡图像，尤其是缺乏领域内监督时。

Conclusion: CIRThan为推进草图+文本CIR、分层语义建模和多模态检索在文化遗产等特定视觉领域提供有价值的基准。

Abstract: Composed Image Retrieval (CIR) enables image retrieval by combining multiple query modalities, but existing benchmarks predominantly focus on general-domain imagery and rely on reference images with short textual modifications. As a result, they provide limited support for retrieval scenarios that require fine-grained semantic reasoning, structured visual understanding, and domain-specific knowledge. In this work, we introduce CIRThan, a sketch+text Composed Image Retrieval dataset for Thangka imagery, a culturally grounded and knowledge-specific visual domain characterized by complex structures, dense symbolic elements, and domain-dependent semantic conventions. CIRThan contains 2,287 high-quality Thangka images, each paired with a human-drawn sketch and hierarchical textual descriptions at three semantic levels, enabling composed queries that jointly express structural intent and multi-level semantic specification. We provide standardized data splits, comprehensive dataset analysis, and benchmark evaluations of representative supervised and zero-shot CIR methods. Experimental results reveal that existing CIR approaches, largely developed for general-domain imagery, struggle to effectively align sketch-based abstractions and hierarchical textual semantics with fine-grained Thangka images, particularly without in-domain supervision. We believe CIRThan offers a valuable benchmark for advancing sketch+text CIR, hierarchical semantic modeling, and multimodal retrieval in cultural heritage and other knowledge-specific visual domains. The dataset is publicly available at https://github.com/jinyuxu-whut/CIRThan.

</details>


### [177] [Hybrid Pooling with LLMs via Relevance Context Learning](https://arxiv.org/abs/2602.08457)
*David Otero,Javier Parapar*

Main category: cs.IR

TL;DR: 提出Relevance Context Learning (RCL)框架用于信息检索系统相关性评估，实验显示其优于零样本提示和标准上下文学习。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型作为自动相关性评估器可靠性有限，标准上下文学习无法明确捕捉主题相关性标准，缺乏泛化能力。

Method: 引入RCL框架，先用Instructor LLM分析判断的查询 - 文档对生成相关性叙述，再用Assessor LLM使用这些叙述进行相关性判断；提出混合池策略进行评估。

Result: RCL大幅优于零样本提示，持续改进标准上下文学习。

Conclusion: 将相关性示例转化为明确、上下文感知的相关性叙述是基于大语言模型构建信息检索数据集更有效的方法。

Abstract: High-quality relevance judgements over large query sets are essential for evaluating Information Retrieval (IR) systems, yet manual annotation remains costly and time-consuming. Large Language Models (LLMs) have recently shown promise as automatic relevance assessors, but their reliability is still limited. Most existing approaches rely on zero-shot prompting or In-Context Learning (ICL) with a small number of labeled examples. However, standard ICL treats examples as independent instances and fails to explicitly capture the underlying relevance criteria of a topic, restricting its ability to generalize to unseen query-document pairs. To address this limitation, we introduce Relevance Context Learning (RCL), a novel framework that leverages human relevance judgements to explicitly model topic-specific relevance criteria. Rather than directly using labeled examples for in-context prediction, RCL first prompts an LLM (Instructor LLM) to analyze sets of judged query-document pairs and generate explicit narratives that describe what constitutes relevance for a given topic. These relevance narratives are then used as structured prompts to guide a second LLM (Assessor LLM) in producing relevance judgements. To evaluate RCL in a realistic data collection setting, we propose a hybrid pooling strategy in which a shallow depth-\textit{k} pool from participating systems is judged by human assessors, while the remaining documents are labeled by LLMs. Experimental results demonstrate that RCL substantially outperforms zero-shot prompting and consistently improves over standard ICL. Overall, our findings indicate that transforming relevance examples into explicit, context-aware relevance narratives is a more effective way of exploiting human judgements for LLM-based IR dataset construction.

</details>


### [178] [PIT: A Dynamic Personalized Item Tokenizer for End-to-End Generative Recommendation](https://arxiv.org/abs/2602.08530)
*Huanjie Wang,Xinchen Luo,Honghui Bao,Zhang Zixing,Lejian Ren,Yunfan Wu,Hongwei Zhang,Liwei Guan,Guang Chen*

Main category: cs.IR

TL;DR: 提出PIT框架用于端到端生成式推荐，实验表明其优于基线，快手部署提升应用停留时间。


<details>
  <summary>Details</summary>
Motivation: 现有生成式推荐方法依赖静态、解耦的分词，忽略协作信号，在实际生产环境有挑战，如协作信号不稳定导致分词不稳定等。

Method: 提出PIT动态个性化项目分词器框架，采用协同生成架构，通过协作信号对齐协调协作模式，通过协同进化学习同步项目分词器和生成式推荐器，使用一对多波束索引确保可扩展性和鲁棒性。

Result: 在真实数据集实验中，PIT始终优于竞争基线；在快手大规模部署的在线A/B测试中，应用停留时间提升0.402%。

Conclusion: PIT框架在动态工业环境中有效。

Abstract: Generative Recommendation has revolutionized recommender systems by reformulating retrieval as a sequence generation task over discrete item identifiers. Despite the progress, existing approaches typically rely on static, decoupled tokenization that ignores collaborative signals. While recent methods attempt to integrate collaborative signals into item identifiers either during index construction or through end-to-end modeling, they encounter significant challenges in real-world production environments. Specifically, the volatility of collaborative signals leads to unstable tokenization, and current end-to-end strategies often devolve into suboptimal two-stage training rather than achieving true co-evolution. To bridge this gap, we propose PIT, a dynamic Personalized Item Tokenizer framework for end-to-end generative recommendation, which employs a co-generative architecture that harmonizes collaborative patterns through collaborative signal alignment and synchronizes item tokenizer with generative recommender via a co-evolution learning. This enables the dynamic, joint, end-to-end evolution of both index construction and recommendation. Furthermore, a one-to-many beam index ensures scalability and robustness, facilitating seamless integration into large-scale industrial deployments. Extensive experiments on real-world datasets demonstrate that PIT consistently outperforms competitive baselines. In a large-scale deployment at Kuaishou, an online A/B test yielded a substantial 0.402% uplift in App Stay Time, validating the framework's effectiveness in dynamic industrial environments.

</details>


### [179] [DA-RAG: Dynamic Attributed Community Search for Retrieval-Augmented Generation](https://arxiv.org/abs/2602.08545)
*Xingyuan Zeng,Zuohan Wu,Yue Wang,Chen Zhang,Quanming Yao,Libin Zheng,Jian Yin*

Main category: cs.IR

TL;DR: 提出DA - RAG方法用于检索增强生成，在多数据集上表现优于现有方法，还能降低成本。


<details>
  <summary>Details</summary>
Motivation: 当前图基检索增强生成（G - RAG）方法常未充分利用图拓扑，处理动态复杂查询效果不佳，需改进。

Method: 提出DA - RAG，利用属性社区搜索（ACS）动态提取相关子图，配备面向块层的图索引。

Result: 在多数据集上评估，四个指标下比现有RAG方法最高优40%，索引构建时间和令牌开销分别最多降低37%和41%。

Conclusion: DA - RAG能有效解决现有G - RAG的问题，性能和成本控制上表现更优。

Abstract: Owing to their unprecedented comprehension capabilities, large language models (LLMs) have become indispensable components of modern web search engines. From a technical perspective, this integration represents retrieval-augmented generation (RAG), which enhances LLMs by grounding them in external knowledge bases. A prevalent technical approach in this context is graph-based RAG (G-RAG). However, current G-RAG methodologies frequently underutilize graph topology, predominantly focusing on low-order structures or pre-computed static communities. This limitation affects their effectiveness in addressing dynamic and complex queries. Thus, we propose DA-RAG, which leverages attributed community search (ACS) to extract relevant subgraphs based on the queried question dynamically. DA-RAG captures high-order graph structures, allowing for the retrieval of self-complementary knowledge. Furthermore, DA-RAG is equipped with a chunk-layer oriented graph index, which facilitates efficient multi-granularity retrieval while significantly reducing both computational and economic costs. We evaluate DA-RAG on multiple datasets, demonstrating that it outperforms existing RAG methods by up to 40% in head-to-head comparisons across four metrics while reducing index construction time and token overhead by up to 37% and 41%, respectively.

</details>


### [180] [QARM V2: Quantitative Alignment Multi-Modal Recommendation for Reasoning User Sequence Modeling](https://arxiv.org/abs/2602.08559)
*Tian Xia,Jiaqi Zhang,Yueyang Liu,Hongjian Dou,Tingya Yin,Jiangxia Cao,Xulei Liang,Tianlu Xie,Lihao Liu,Xiang Chen,Shen Wang,Changxin Lao,Haixiang Gan,Jinkai Yu,Keting Cen,Lu Hao,Xu Zhang,Qiqiang Zhong,Zhongbo Sun,Yiyu Wang,Shuang Yang,Mingxin Wen,Xiangyu Wu,Shaoguo Liu,Tingting Gao,Zhaojie Liu,Han Li,Kun Gai*

Main category: cs.IR

TL;DR: 介绍QARM V2框架，用于连接大语言模型语义理解和推荐系统业务需求进行用户序列建模。


<details>
  <summary>Details</summary>
Motivation: 传统推荐系统存在信息密度低、知识孤立和泛化能力弱等问题，直接应用大语言模型嵌入到推荐系统有表示不匹配业务目标和无法端到端学习等挑战，因此需要新框架。

Method: 提出QARM V2统一框架。

Result: 未提及。

Conclusion: 未提及。

Abstract: With the evolution of large language models (LLMs), there is growing interest in leveraging their rich semantic understanding to enhance industrial recommendation systems (RecSys). Traditional RecSys relies on ID-based embeddings for user sequence modeling in the General Search Unit (GSU) and Exact Search Unit (ESU) paradigm, which suffers from low information density, knowledge isolation, and weak generalization ability. While LLMs offer complementary strengths with dense semantic representations and strong generalization, directly applying LLM embeddings to RecSys faces critical challenges: representation unmatch with business objectives and representation unlearning end-to-end with downstream tasks. In this paper, we present QARM V2, a unified framework that bridges LLM semantic understanding with RecSys business requirements for user sequence modeling.

</details>


### [181] [RankGR: Rank-Enhanced Generative Retrieval with Listwise Direct Preference Optimization in Recommendation](https://arxiv.org/abs/2602.08575)
*Kairui Fu,Changfa Wu,Kun Yuan,Binbin Cao,Dunxian Huang,Yuliang Yan,Junjun Zheng,Jianning Zhang,Silu Zhou,Jian Wu,Kun Kuang*

Main category: cs.IR

TL;DR: 提出RankGR方法用于推荐系统，将检索分为两阶段，联合优化并做实用改进，经实验验证有效可扩展。


<details>
  <summary>Details</summary>
Motivation: 现有生成式检索方法基于下一token预测，难以捕捉用户偏好结构和忽视标识符与行为序列的深层交互。

Method: 提出RankGR方法，将检索过程分解为初始评估阶段和细化评分阶段，联合优化，还在训练和部署上做改进。

Result: 构建了每秒能处理近万个请求的实时系统，在研究和工业数据集上有离线性能提升，在淘宝“猜你喜欢”有在线增益。

Conclusion: RankGR方法有效且具有可扩展性。

Abstract: Generative retrieval (GR) has emerged as a promising paradigm in recommendation systems by autoregressively decoding identifiers of target items. Despite its potential, current approaches typically rely on the next-token prediction schema, which treats each token of the next interacted items as the sole target. This narrow focus 1) limits their ability to capture the nuanced structure of user preferences, and 2) overlooks the deep interaction between decoded identifiers and user behavior sequences. In response to these challenges, we propose RankGR, a Rank-enhanced Generative Retrieval method that incorporates listwise direct preference optimization for recommendation. RankGR decomposes the retrieval process into two complementary stages: the Initial Assessment Phase (IAP) and the Refined Scoring Phase (RSP). In IAP, we incorporate a novel listwise direct preference optimization strategy into GR, thus facilitating a more comprehensive understanding of the hierarchical user preferences and more effective partial-order modeling. The RSP then refines the top-λ candidates generated by IAP with interactions towards input sequences using a lightweight scoring module, leading to more precise candidate evaluation. Both phases are jointly optimized under a unified GR model, ensuring consistency and efficiency. Additionally, we implement several practical improvements in training and deployment, ultimately achieving a real-time system capable of handling nearly ten thousand requests per second. Extensive offline performance on both research and industrial datasets, as well as the online gains on the "Guess You Like" section of Taobao, validate the effectiveness and scalability of RankGR.

</details>


### [182] [OneLive: Dynamically Unified Generative Framework for Live-Streaming Recommendation](https://arxiv.org/abs/2602.08612)
*Shen Wang,Yusheng Huang,Ruochen Yang,Shuang Wen,Pengbo Xu,Jiangxia Cao,Yueyang Liu,Kuo Cai,Chengcheng Guo,Shiyao Wang,Xinchen Luo,Qiang Luo,Ruiming Tang,Shuang Yang,Zhaojie Liu,Guorui Zhou,Han Li,Kun Gai*

Main category: cs.IR

TL;DR: 提出适用于直播场景的动态统一生成式推荐框架OneLive。


<details>
  <summary>Details</summary>
Motivation: 现有生成式推荐方法因直播场景的复杂性无法直接应用，需解决持续演进内容、有限生命周期、严格实时约束和异构多目标等问题。

Method: OneLive集成动态分词器、时间感知门控注意力机制、高效解码器生成架构和统一多目标对齐框架。

Result: 未提及。

Conclusion: 未提及。

Abstract: Live-streaming recommender system serves as critical infrastructure that bridges the patterns of real-time interactions between users and authors. Similar to traditional industrial recommender systems, live-streaming recommendation also relies on cascade architectures to support large-scale concurrency. Recent advances in generative recommendation unify the multi-stage recommendation process with Transformer-based architectures, offering improved scalability and higher computational efficiency. However, the inherent complexity of live-streaming prevents the direct transfer of these methods to live-streaming scenario, where continuously evolving content, limited lifecycles, strict real-time constraints, and heterogeneous multi-objectives introduce unique challenges that invalidate static tokenization and conventional model framework. To address these issues, we propose OneLive, a dynamically unified generative recommendation framework tailored for live-streaming scenario. OneLive integrates four key components: (i) A Dynamic Tokenizer that continuously encodes evolving real-time live content fused with behavior signal through residual quantization; (ii) A Time-Aware Gated Attention mechanism that explicitly models temporal dynamics for timely decision making; (iii) An efficient decoder-only generative architecture enhanced with Sequential MTP and QK Norm for stable training and accelerated inference; (iv) A Unified Multi-Objective Alignment Framework reinforces policy optimization for personalized preferences.

</details>


### [183] [SRSUPM: Sequential Recommender System Based on User Psychological Motivation](https://arxiv.org/abs/2602.08667)
*Yicheng Di,Yuan Liu,Zhi Chen,Jingcai Guo*

Main category: cs.IR

TL;DR: 本文提出基于用户心理动机的序列推荐系统框架，以增强模型对用户心理动机转变建模，实验表明该模型在多种序列推荐任务中表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 多数现有方法缺乏对用户心理动机转变的明确建模，难以挖掘不同转变程度的分布模式和捕捉敏感于心理动机转变的协同知识。

Method: 提出基于用户心理动机的序列推荐系统框架，包含心理动机转变评估、转变信息构建、动机转变驱动的信息分解和信息匹配。

Result: 在三个公开基准上的大量实验表明，SRSUPM 在多种序列推荐任务中始终优于代表性基线。

Conclusion: 所提出的框架能增强序列推荐系统对用户心理动机转变的感知，提升推荐效果。

Abstract: Sequential recommender infers users' evolving psychological motivations from historical interactions to recommend the next preferred items. Most existing methods compress recent behaviors into a single vector and optimize it toward a single observed target item, but lack explicit modeling of psychological motivation shift. As a result, they struggle to uncover the distributional patterns across different shift degrees and to capture collaborative knowledge that is sensitive to psychological motivation shift. We propose a general framework, the Sequential Recommender System Based on User Psychological Motivation, to enhance sequential recommenders with psychological motivation shift-aware user modeling. Specifically, the Psychological Motivation Shift Assessment quantitatively measures psychological motivation shift; guided by PMSA, the Shift Information Construction models dynamically evolving multi-level shift states, and the Psychological Motivation Shift-driven Information Decomposition decomposes and regularizes representations across shift levels. Moreover, the Psychological Motivation Shift Information Matching strengthens collaborative patterns related to psychological motivation shift to learn more discriminative user representations. Extensive experiments on three public benchmarks show that SRSUPM consistently outperforms representative baselines on diverse sequential recommender tasks.

</details>


### [184] [SA-CAISR: Stage-Adaptive and Conflict-Aware Incremental Sequential Recommendation](https://arxiv.org/abs/2602.08678)
*Xiaomeng Song,Xinru Wang,Hanbing Wang,Hongyu Lu,Yu Chen,Zhaochun Ren,Zhumin Chen*

Main category: cs.IR

TL;DR: 提出SA - CAISR框架用于增量顺序推荐，解决现有方法挑战，提升性能并降低成本。


<details>
  <summary>Details</summary>
Motivation: 现有增量学习方法在顺序推荐模型更新时面临内存和计算成本高、难丢弃过时知识等挑战。

Method: 提出无缓冲区的SA - CAISR框架，引入Fisher加权知识筛选机制动态识别过时知识。

Result: SA - CAISR在多个数据集上平均提升Recall@20 2.0%、MRR@20 1.2%、NDCG@20 1.4%，降低内存使用97.5%和训练时间46.9%。

Conclusion: SA - CAISR能在稳定性和适应性间取得平衡，实现高效更新，确保推荐及时准确。

Abstract: Sequential recommendation (SR) aims to predict a user's next action by learning from their historical interaction sequences. In real-world applications, these models require periodic updates to adapt to new interactions and evolving user preferences. While incremental learning methods facilitate these updates, they face significant challenges. Replay-based approaches incur high memory and computational costs, and regularization-based methods often struggle to discard outdated or conflicting knowledge. To overcome these challenges, we propose SA-CAISR, a Stage-Adaptive and Conflict-Aware Incremental Sequential Recommendation framework. As a buffer-free framework, SA-CAISR operates using only the old model and new data, directly addressing the high costs of replay-based techniques. SA-CAISR introduces a novel Fisher-weighted knowledge-screening mechanism that dynamically identifies outdated knowledge by estimating parameter-level conflicts between the old model and new data, allowing our approach to selectively remove obsolete knowledge while preserving compatible historical patterns. This dynamic balance between stability and adaptability allows our method to achieve a new state-of-the-art performance in incremental SR. Specifically, SA-CAISR improves Recall@20 by 2.0%, MRR@20 by 1.2%, and NDCG@20 by 1.4% on average across datasets, while reducing memory usage by 97.5% and training time by 46.9% compared to the best baselines. This efficiency allows real-world systems to rapidly update user profiles with minimal computational overhead, ensuring more timely and accurate recommendations.

</details>


### [185] [AMEM4Rec: Leveraging Cross-User Similarity for Memory Evolution in Agentic LLM Recommenders](https://arxiv.org/abs/2602.08837)
*Minh-Duc Nguyen,Hai-Dang Kieu,Dung D. Le*

Main category: cs.IR

TL;DR: 提出基于大语言模型的推荐器AMEM4Rec，通过跨用户记忆进化学习协同信号，在数据集上表现优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的智能推荐系统存在微调参数效率低、提示推理有局限、忽视协同过滤信号等问题。

Method: 提出AMEM4Rec，将用户历史中的抽象行为模式存储在全局记忆池中，通过迭代进化强化共享的跨用户模式，学习协同信号。

Result: 在Amazon和MIND数据集上的实验表明，AMEM4Rec始终优于最先进的基于大语言模型的推荐器。

Conclusion: 进化记忆引导的协同过滤是有效的。

Abstract: Agentic systems powered by Large Language Models (LLMs) have shown strong potential in recommender systems but remain hindered by several challenges. Fine-tuning LLMs is parameter-inefficient, and prompt-based agentic reasoning is limited by context length and hallucination risk. Moreover, existing agentic recommendation systems predominantly leverages semantic knowledge while neglecting the collaborative filtering (CF) signals essential for implicit preference modeling. To address these limitations, we propose AMEM4Rec, an agentic LLM-based recommender that learns collaborative signals in an end-to-end manner through cross-user memory evolution. AMEM4Rec stores abstract user behavior patterns from user histories in a global memory pool. Within this pool, memories are linked to similar existing ones and iteratively evolved to reinforce shared cross-user patterns, enabling the system to become aware of CF signals without relying on a pre-trained CF model. Extensive experiments on Amazon and MIND datasets show that AMEM4Rec consistently outperforms state-of-the-art LLM-based recommenders, demonstrating the effectiveness of evolving memory-guided collaborative filtering.

</details>


### [186] [Whose Name Comes Up? Benchmarking and Intervention-Based Auditing of LLM-Based Scholar Recommendation](https://arxiv.org/abs/2602.08873)
*Lisette Espin-Noboa,Gonzalo Gabriel Mendez*

Main category: cs.IR

TL;DR: 介绍了用于审计基于大语言模型学者推荐的基准LLMScholarBench，审计22个大语言模型，发现用户干预会重塑权衡而非提供通用解决方案，并发布可适配其他学科的代码和数据。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型学术专家推荐审计通常孤立评估模型输出，忽略用户推理时干预，不清楚失败原因是模型选择还是部署决策。

Method: 引入LLMScholarBench，用9个指标衡量技术质量和社会代表性，在物理学专家推荐中实例化该基准，审计22个大语言模型在不同温度、表示约束提示和检索增强生成等条件下的表现。

Result: 用户干预不会带来统一改进，而是在各维度重新分配误差，如高温会降低有效性等，不同干预方式有不同影响。

Conclusion: 用户干预会重塑权衡，而非提供通用解决方案，代码和数据可适配其他学科。

Abstract: Large language models (LLMs) are increasingly used for academic expert recommendation. Existing audits typically evaluate model outputs in isolation, largely ignoring end-user inference-time interventions. As a result, it remains unclear whether failures such as refusals, hallucinations, and uneven coverage stem from model choice or deployment decisions. We introduce LLMScholarBench, a benchmark for auditing LLM-based scholar recommendation that jointly evaluates model infrastructure and end-user interventions across multiple tasks. LLMScholarBench measures both technical quality and social representation using nine metrics. We instantiate the benchmark in physics expert recommendation and audit 22 LLMs under temperature variation, representation-constrained prompting, and retrieval-augmented generation (RAG) via web search. Our results show that end-user interventions do not yield uniform improvements but instead redistribute error across dimensions. Higher temperature degrades validity, consistency, and factuality. Representation-constrained prompting improves diversity at the expense of factuality, while RAG primarily improves technical quality while reducing diversity and parity. Overall, end-user interventions reshape trade-offs rather than providing a general fix. We release code and data that can be adapted to other disciplines by replacing domain-specific ground truth and metrics.

</details>


### [187] [Contrastive Learning for Diversity-Aware Product Recommendations in Retail](https://arxiv.org/abs/2602.08886)
*Vasileios Karlis,Ezgi Yıldırım,David Vos,Maarten de Rijke*

Main category: cs.IR

TL;DR: 本文提出一种在宜家零售现有数字推荐管道中增强商品目录覆盖率且不降低推荐质量的方法，通过集成对比学习和精心选择的负样本，经评估该方法能提高覆盖率并保持推荐性能。


<details>
  <summary>Details</summary>
Motivation: 推荐系统常面临长尾分布和商品目录曝光有限问题，尤其是在大规模在线零售场景，需增强商品目录覆盖率且不降低推荐质量。

Method: 借鉴负采样的最新进展，将对比学习与精心选择的负样本相结合。

Result: 通过离线和在线评估，方法提高了商品目录覆盖率，保证推荐更多样化且保持了良好推荐性能。

Conclusion: 所提出的方法能在不影响推荐质量的前提下增强商品目录覆盖率。

Abstract: Recommender systems often struggle with long-tail distributions and limited item catalog exposure, where a small subset of popular items dominates recommendations. This challenge is especially critical in large-scale online retail settings with extensive and diverse product assortments. This paper introduces an approach to enhance catalog coverage without compromising recommendation quality in the existing digital recommendation pipeline at IKEA Retail. Drawing inspiration from recent advances in negative sampling to address popularity bias, we integrate contrastive learning with carefully selected negative samples. Through offline and online evaluations, we demonstrate that our method improves catalog coverage, ensuring a more diverse set of recommendations yet preserving strong recommendation performance.

</details>


### [188] [OmniReview: A Large-scale Benchmark and LLM-enhanced Framework for Realistic Reviewer Recommendation](https://arxiv.org/abs/2602.08896)
*Yehua Huang,Penglei Sun,Zebin Chen,Zhenheng Tang,Xiaowen Chu*

Main category: cs.IR

TL;DR: 文章提出OmniReview数据集和Pro - MMoE框架，解决学术同行评审在数据和方法上的问题，实验显示Pro - MMoE表现出色。


<details>
  <summary>Details</summary>
Motivation: 学术同行评审在数据上缺乏大规模验证基准和合适评估指标，方法上基于嵌入的方法存在信息瓶颈和可解释性问题。

Method: 构建OmniReview数据集，引入三层层次评估框架；提出Pro - MMoE框架，结合大语言模型和多任务学习。

Result: Pro - MMoE在七个指标中的六个上达到了最先进的性能。

Conclusion: Pro - MMoE为现实的审稿人推荐建立了新的基准。

Abstract: Academic peer review remains the cornerstone of scholarly validation, yet the field faces some challenges in data and methods. From the data perspective, existing research is hindered by the scarcity of large-scale, verified benchmarks and oversimplified evaluation metrics that fail to reflect real-world editorial workflows. To bridge this gap, we present OmniReview, a comprehensive dataset constructed by integrating multi-source academic platforms encompassing comprehensive scholarly profiles through the disambiguation pipeline, yielding 202, 756 verified review records. Based on this data, we introduce a three-tier hierarchical evaluaion framework to assess recommendations from recall to precise expert identification. From the method perspective, existing embedding-based approaches suffer from the information bottleneck of semantic compression and limited interpretability. To resolve these method limitations, we propose Profiling Scholars with Multi-gate Mixture-of-Experts (Pro-MMoE), a novel framework that synergizes Large Language Models (LLMs) with Multi-task Learning. Specifically, it utilizes LLM-generated semantic profiles to preserve fine-grained expertise nuances and interpretability, while employing a Task-Adaptive MMoE architecture to dynamically balance conflicting evaluation goals. Comprehensive experiments demonstrate that Pro-MMoE achieves state-of-the-art performance across six of seven metrics, establishing a new benchmark for realistic reviewer recommendation.

</details>


### [189] [Automatic In-Domain Exemplar Construction and LLM-Based Refinement of Multi-LLM Expansions for Query Expansion](https://arxiv.org/abs/2602.08917)
*Minghan Li,Ercong Nie,Siqi Zhao,Tongna Chen,Huiping Huang,Guodong Zhou*

Main category: cs.IR

TL;DR: 提出自动化、域自适应查询扩展框架，结合多LLM，在多数据集上优于基线，提供实用无标签解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型查询扩展依赖手工提示、人工选择示例或单一LLM，不可扩展且对领域迁移敏感。

Method: 构建基于BM25 - MonoT5管道的领域示例池，用无训练的聚类策略选择示例，引入双LLM集成与精炼LLM合并扩展结果。

Result: 在TREC DL20、DBPedia和SciFact数据集上，精炼集成比基线有显著提升。

Conclusion: 该框架为示例选择和多LLM生成提供可复现测试平台，是实用无标签的查询扩展解决方案。

Abstract: Query expansion with large language models is promising but often relies on hand-crafted prompts, manually chosen exemplars, or a single LLM, making it non-scalable and sensitive to domain shift. We present an automated, domain-adaptive QE framework that builds in-domain exemplar pools by harvesting pseudo-relevant passages using a BM25-MonoT5 pipeline. A training-free cluster-based strategy selects diverse demonstrations, yielding strong and stable in-context QE without supervision. To further exploit model complementarity, we introduce a two-LLM ensemble in which two heterogeneous LLMs independently generate expansions and a refinement LLM consolidates them into one coherent expansion. Across TREC DL20, DBPedia, and SciFact, the refined ensemble delivers consistent and statistically significant gains over BM25, Rocchio, zero-shot, and fixed few-shot baselines. The framework offers a reproducible testbed for exemplar selection and multi-LLM generation, and a practical, label-free solution for real-world QE.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [190] [BitLogic: Training Framework for Gradient-Based FPGA-Native Neural Networks](https://arxiv.org/abs/2602.07400)
*Simon Bührer,Andreas Plesner,Aczel Till,Roger Wattenhofer*

Main category: cs.LG

TL;DR: 提出基于查找表计算的FPGA原生神经网络框架BitLogic，实验显示有竞争力的准确率和FPGA效率提升。


<details>
  <summary>Details</summary>
Motivation: 深度学习推理的能源和延迟成本更多受部署而非训练影响，现有FPGA神经方法分散难比较，需开发新框架。

Method: 用可微分查找表节点替代乘积累加操作，提供模块化功能API，有自动化RTL导出管道将PyTorch模型转换为可综合HDL。

Result: 在标准视觉基准和异构硬件平台实验中，CIFAR - 10测试准确率达72.3%，用少于0.3M逻辑门，仅用查找表资源单样本推理低于20ns。

Conclusion: BitLogic框架有竞争力的准确率和显著的FPGA效率提升。

Abstract: The energy and latency costs of deep neural network inference are increasingly driven by deployment rather than training, motivating hardware-specialized alternatives to arithmetic-heavy models. Field-Programmable Gate Arrays (FPGAs) provide an attractive substrate for such specialization, yet existing FPGA-based neural approaches are fragmented and difficult to compare. We present BitLogic, a fully gradient-based, end-to-end trainable framework for FPGA-native neural networks built around Lookup Table (LUT) computation. BitLogic replaces multiply-accumulate operations with differentiable LUT nodes that map directly to FPGA primitives, enabling native binary computation, sparse connectivity, and efficient hardware realization. The framework offers a modular functional API supporting diverse architectures, along with learned encoders, hardware-aware heads, and multiple boundary-consistent LUT relaxations. An automated Register Transfer Level (RTL) export pipeline translates trained PyTorch models into synthesizable HDL, ensuring equivalence between software and hardware inference. Experiments across standard vision benchmarks and heterogeneous hardware platforms demonstrate competitive accuracy and substantial gains in FPGA efficiency, including 72.3% test accuracy on CIFAR-10 achieved with fewer than 0.3M logic gates, while attaining sub-20 ns single-sample inference using only LUT resources.

</details>


### [191] [Attractor Patch Networks: Reducing Catastrophic Forgetting with Routed Low-Rank Patch Experts](https://arxiv.org/abs/2602.06993)
*Shashank*

Main category: cs.LG

TL;DR: 提出Attractor Patch Networks (APN)替代Transformer FFN，分析其特性并在实验中展现良好性能。


<details>
  <summary>Details</summary>
Motivation: Transformer的FFN存在计算资源浪费和持续学习干扰问题，需要改进。

Method: 提出APN，用相似度路由器为每个token选择专家patch，产生低秩残差更新。

Result: 在字符级语言建模实验中，APN困惑度有竞争力，持续适应能力大幅提升。

Conclusion: APN作为架构原语，自然适配持续学习，性能表现良好。

Abstract: Transformers achieve strong language modeling accuracy, yet their position-wise feed-forward networks (FFNs) are dense, globally shared, and typically updated end to end. These properties create two practical tensions. First, dense FFNs spend the same compute on every token regardless of context, and they allocate capacity uniformly even when language exhibits highly clustered context structure. Second, continual learning, in the sense of updating the model while serving a data stream, often produces interference because a small update touches broadly shared weights.
  We propose Attractor Patch Networks (APN), a plug-compatible replacement for the Transformer FFN. APN is a bank of patch experts. A similarity router selects a small top-k set of patches for each token by matching the token representation to learned prototypes. Each selected patch emits a low-rank residual update conditioned on a compact code. The architecture yields conditional, context-specialized nonlinear transformations while preserving the standard Transformer interface.
  This paper focuses on APN as an architectural primitive. We formalize APN, analyze its expressivity as a piecewise low-rank residual function class, and derive simple interference and stability arguments that make APN naturally compatible with continual learning. In experiments on character-level language modeling, APN achieves competitive perplexity (4.57 vs 4.32 PPL) while enabling dramatically better continual adaptation: when adapting to a shifted domain, APN achieves 2.6 times better retention (11.1 vs 29.4 PPL on the original domain) and 2.8 times better adaptation (6.4 vs 17.8 PPL on the new domain) compared to global fine-tuning of a dense FFN baseline.

</details>


### [192] [Neural Sabermetrics with World Model: Play-by-play Predictive Modeling with Large Language Model](https://arxiv.org/abs/2602.07030)
*Young Jin Ahn,Yiyang Du,Zheyuan Zhang,Haisen Kang*

Main category: cs.LG

TL;DR: 提出基于大语言模型的Neural Sabermetrics with World Model用于棒球比赛逐局建模，在多项预测任务上优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 经典棒球统计指标无法定义棒球比赛逐球展开的生成模型，现有方法局限于单步预测或事后分析。

Method: 将棒球比赛视为事件的长自回归序列，在超10年的美国职业棒球大联盟追踪数据上持续预训练单个大语言模型。

Result: 模型在常规赛和季后赛数据上进行评估，相比现有基线表现更好，能正确预测约64%的下一投球和78%的击球手挥棒决策。

Conclusion: 大语言模型可作为体育赛事的有效世界模型。

Abstract: Classical sabermetrics has profoundly shaped baseball analytics by summarizing long histories of play into compact statistics. While these metrics are invaluable for valuation and retrospective analysis, they do not define a generative model of how baseball games unfold pitch by pitch, leaving most existing approaches limited to single-step prediction or post-hoc analysis. In this work, we present Neural Sabermetrics with World Model, a Large Language Model (LLM) based play-by-play world model for baseball. We cast baseball games as long auto-regressive sequences of events and continuously pretrain a single LLM on more than ten years of Major League Baseball (MLB) tracking data, comprising over seven million pitch sequences and approximately three billion tokens. The resulting model is capable of predicting multiple aspects of game evolution within a unified framework. We evaluate our model on both in-distribution regular-season data and out-of-distribution postseason games and compare against strong neural baselines from prior work. Despite using a single backbone model, our approach outperforms the performance of existing baselines, (1) correctly predicting approximately 64% of next pitches within a plate appearance and (2) 78% of batter swing decisions, suggesting that LLMs can serve as effective world models for sports.

</details>


### [193] [Lagged backward-compatible physics-informed neural networks for unsaturated soil consolidation analysis](https://arxiv.org/abs/2602.07031)
*Dong Li,Shuai Huang,Yapeng Cao,Yujun Cui,Xiaobin Wei,Hongtao Cao*

Main category: cs.LG

TL;DR: 本文提出LBC - PINN用于模拟和反演一维非饱和土固结问题，经验证有良好预测能力和计算效率。


<details>
  <summary>Details</summary>
Motivation: 解决长期荷载下多尺度时域气水压力耗散耦合模拟与反演难题。

Method: 构建LBC - PINN，集成对数时间分割、延迟兼容性损失 enforcement和分段式迁移学习方法。

Result: 正向分析中准确预测孔压变化，与有限元结果对比平均绝对误差小；简化分割策略提高计算效率；灵敏度分析显示框架跨渗透比稳健。

Conclusion: LBC - PINN是解决一维非饱和土固结问题有效方法，兼顾预测精度和计算效率。

Abstract: This study develops a Lagged Backward-Compatible Physics-Informed Neural Network (LBC-PINN) for simulating and inverting one-dimensional unsaturated soil consolidation under long-term loading. To address the challenges of coupled air and water pressure dissipation across multi-scale time domains, the framework integrates logarithmic time segmentation, lagged compatibility loss enforcement, and segment-wise transfer learning.
  In forward analysis, the LBC-PINN with recommended segmentation schemes accurately predicts pore air and pore water pressure evolution. Model predictions are validated against finite element method (FEM) results, with mean absolute errors below 1e-2 for time durations up to 1e10 seconds. A simplified segmentation strategy based on the characteristic air-phase dissipation time improves computational efficiency while preserving predictive accuracy. Sensitivity analyses confirm the robustness of the framework across air-to-water permeability ratios ranging from 1e-3 to 1e3.

</details>


### [194] [TransConv-DDPM: Enhanced Diffusion Model for Generating Time-Series Data in Healthcare](https://arxiv.org/abs/2602.07033)
*Md Shahriar Kabir,Sana Alamgeer,Minakshi Debnath,Anne H. H. Ngu*

Main category: cs.LG

TL;DR: 论文介绍了增强生成式AI方法TransConv - DDPM用于生物力学和生理时间序列数据生成，经评估效果良好，有生成高质量合成数据的潜力。


<details>
  <summary>Details</summary>
Motivation: 临床领域缺乏真实数据影响医学AI模型训练，生成生理时间序列数据存在独特挑战，需要新方法。

Method: 引入TransConv - DDPM，采用带U - Net、多尺度卷积模块和变压器层的去噪扩散概率模型（DDPM）捕获全局和局部时间依赖性。

Result: 在三个数据集上评估，与先进方法对比有良好结果，在部分数据集有效捕获数据点间时间变化模式；在SmartFallMM数据集效用测试中，添加合成数据使预测模型F1分数提高13.64%，整体准确率提高14.93%。

Conclusion: TransConv - DDPM有潜力为实际应用生成高质量合成数据。

Abstract: The lack of real-world data in clinical fields poses a major obstacle in training effective AI models for diagnostic and preventive tools in medicine. Generative AI has shown promise in increasing data volume and enhancing model training, particularly in computer vision and natural language processing (NLP) domains. However, generating physiological time-series data, a common type in medical AI applications, presents unique challenges due to its inherent complexity and variability. This paper introduces TransConv-DDPM, an enhanced generative AI method for biomechanical and physiological time-series data generation. The model employs a denoising diffusion probabilistic model (DDPM) with U-Net, multi-scale convolution modules, and a transformer layer to capture both global and local temporal dependencies. We evaluated TransConv-DDPM on three diverse datasets, generating both long and short-sequence time-series data. Quantitative comparisons against state-of-the-art methods, TimeGAN and Diffusion-TS, using four performance metrics, demonstrated promising results, particularly on the SmartFallMM and EEG datasets, where it effectively captured the more gradual temporal change patterns between data points. Additionally, a utility test on the SmartFallMM dataset revealed that adding synthetic fall data generated by TransConv-DDPM improved predictive model performance, showing a 13.64% improvement in F1-score and a 14.93% increase in overall accuracy compared to the baseline model trained solely on fall data from the SmartFallMM dataset. These findings highlight the potential of TransConv-DDPM to generate high-quality synthetic data for real-world applications.

</details>


### [195] [AVERE: Improving Audiovisual Emotion Reasoning with Preference Optimization](https://arxiv.org/abs/2602.07054)
*Ashutosh Chaubey,Jiacheng Pang,Maksim Siniukov,Mohammad Soleymani*

Main category: cs.LG

TL;DR: 本文引入EmoReAlM基准评估多模态大语言模型在情感理解上的问题，提出AVEm - DPO优化技术，实验显示该方法能显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在情感理解任务中存在情感与无关视听线索的虚假关联、语言模型骨干中由文本先验驱动的视听线索幻觉这两个关键挑战，需要解决。

Method: 引入EmoReAlM基准评估模型；提出AVEm - DPO偏好优化技术，构建对有虚假关联或幻觉的响应和视听输入对的偏好，加入惩罚依赖文本先验的正则化项。

Result: 在DFEW、RAVDESS和EMER上的实验表明，该方法在零样本设置下使参考基线模型的相对性能提高6 - 19%。

Conclusion: 本文提供的基准和优化框架能对用于情感理解和社交AI的多模态大语言模型进行原则性评估和改进。

Abstract: Emotion understanding is essential for building socially intelligent agents. Although recent multimodal large language models have shown strong performance on this task, two key challenges remain - spurious associations between emotions and irrelevant audiovisual cues, and hallucinations of audiovisual cues driven by text priors in the language model backbone. To quantify and understand these issues, we introduce EmoReAlM, a benchmark designed to evaluate MLLMs for cue-emotion associations, hallucinations and modality agreement. We then propose AVEm-DPO, a preference optimization technique that aligns model responses with both audiovisual inputs and emotion-centric queries. Specifically, we construct preferences over responses exhibiting spurious associations or hallucinations, and audiovisual input pairs guided by textual prompts. We also include a regularization term that penalizes reliance on text priors, thereby mitigating modality-specific cue hallucinations. Experimental results on DFEW, RAVDESS and EMER demonstrate that our method significantly improves the performance of the reference baseline models with 6-19% of relative performance gains in zero-shot settings. By providing both a rigorous benchmark and a robust optimization framework, this work enables principled evaluation and improvement of MLLMs for emotion understanding and social AI. Code, models and benchmark will be released at https://avere-iclr.github.io.

</details>


### [196] [Nansde-net: A neural sde framework for generating time series with memory](https://arxiv.org/abs/2602.08182)
*Hiromu Ozai,Kei Nakagawa*

Main category: cs.LG

TL;DR: 提出NA噪声及NANSDE - Net模型，可在伊藤微积分框架下处理长短记忆特征，实证表现良好。


<details>
  <summary>Details</summary>
Motivation: 分数布朗运动与伊藤微积分不兼容，限制其在神经随机微分方程框架中的应用，需新的噪声模型。

Method: 提出NA噪声，其核函数通过神经网络参数化并分解为乘积形式以保留马尔可夫性质，开发NANSDE - Net模型，证明解的存在唯一性并推导反向传播训练方案。

Result: 在合成和真实数据集上，NANSDE - Net在重现数据长短记忆特征方面匹配或优于现有模型，且在伊藤微积分框架内具有计算可行性。

Conclusion: NANSDE - Net是处理长短记忆时间序列建模的有效方法，在伊藤微积分框架下有良好表现。

Abstract: Modeling time series with long- or short-memory characteristics is a fundamental challenge in many scientific and engineering domains. While fractional Brownian motion has been widely used as a noise source to capture such memory effects, its incompatibility with Itô calculus limits its applicability in neural stochastic differential equation~(SDE) frameworks. In this paper, we propose a novel class of noise, termed Neural Network-kernel ARMA-type noise~(NA-noise), which is an Itô-process-based alternative capable of capturing both long- and short-memory behaviors. The kernel function defining the noise structure is parameterized via neural networks and decomposed into a product form to preserve the Markov property. Based on this noise process, we develop NANSDE-Net, a generative model that extends Neural SDEs by incorporating NA-noise. We prove the theoretical existence and uniqueness of the solution under mild conditions and derive an efficient backpropagation scheme for training. Empirical results on both synthetic and real-world datasets demonstrate that NANSDE-Net matches or outperforms existing models, including fractional SDE-Net, in reproducing long- and short-memory features of the data, while maintaining computational tractability within the Itô calculus framework.

</details>


### [197] [TACIT: Transformation-Aware Capturing of Implicit Thought](https://arxiv.org/abs/2602.07061)
*Daniel Nobrega*

Main category: cs.LG

TL;DR: 提出基于扩散的变压器TACIT用于可解释视觉推理，在迷宫求解任务展示效果，有训练损失降低、L2距离改善等结果，发现推理过程有类似人类认知的‘顿悟时刻’现象。


<details>
  <summary>Details</summary>
Motivation: 构建可解释的视觉推理系统，理解神经网络在语言之前如何发展隐式推理策略。

Method: 使用整流流在像素空间操作的TACIT模型，在迷宫求解任务验证。

Result: 1. 100个epoch训练损失降低192倍；2. L2距离改善22.7倍；3. 仅需10个欧拉步；4. 推理过程有‘顿悟时刻’现象。

Conclusion: 像素空间设计和无噪声流匹配为理解神经网络隐式推理策略提供基础，推理过程类似人类认知的顿悟现象。

Abstract: We present TACIT (Transformation-Aware Capturing of Implicit Thought), a diffusion-based transformer for interpretable visual reasoning. Unlike language-based reasoning systems, TACIT operates entirely in pixel space using rectified flow, enabling direct visualization of the reasoning process at each inference step. We demonstrate the approach on maze-solving, where the model learns to transform images of unsolved mazes into solutions. Key results on 1 million synthetic maze pairs include:
  - 192x reduction in training loss over 100 epochs
  - 22.7x improvement in L2 distance to ground truth
  - Only 10 Euler steps required (vs. 100-1000 for typical diffusion models)
  Quantitative analysis reveals a striking phase transition phenomenon: the solution remains invisible for 68% of the transformation (zero recall), then emerges abruptly at t=0.70 within just 2% of the process. Most remarkably, 100% of samples exhibit simultaneous emergence across all spatial regions, ruling out sequential path construction and providing evidence for holistic rather than algorithmic reasoning. This "eureka moment" pattern -- long incubation followed by sudden crystallization -- parallels insight phenomena in human cognition. The pixel-space design with noise-free flow matching provides a foundation for understanding how neural networks develop implicit reasoning strategies that operate below and before language.

</details>


### [198] [Video-based Music Generation](https://arxiv.org/abs/2602.07063)
*Serkan Sulun*

Main category: cs.LG

TL;DR: 论文提出EMSYNC，可自动为视频生成适配音乐，结合视频情感分类、情感音乐生成和时间边界调节，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 互联网视频内容增长，为视频找合适配乐是挑战，希望为创作者提供无需作曲或授权的解决方案。

Method: 使用预训练深度神经网络提取特征，仅训练融合层；构建大规模情感标注MIDI数据集；提出基于连续情感值的MIDI生成器；引入边界偏移编码的时间边界调节方法。

Result: 在Ekman - 6和MovieNet上取得最优结果，用户研究显示在音乐丰富度、情感匹配度、时间同步性和整体偏好上优于现有方法。

Conclusion: EMSYNC成为全自动化基于视频的音乐生成器，在视频音乐生成领域达到新的最优水平。

Abstract: As the volume of video content on the internet grows rapidly, finding a suitable soundtrack remains a significant challenge. This thesis presents EMSYNC (EMotion and SYNChronization), a fast, free, and automatic solution that generates music tailored to the input video, enabling content creators to enhance their productions without composing or licensing music. Our model creates music that is emotionally and rhythmically synchronized with the video. A core component of EMSYNC is a novel video emotion classifier. By leveraging pretrained deep neural networks for feature extraction and keeping them frozen while training only fusion layers, we reduce computational complexity while improving accuracy. We show the generalization abilities of our method by obtaining state-of-the-art results on Ekman-6 and MovieNet. Another key contribution is a large-scale, emotion-labeled MIDI dataset for affective music generation. We then present an emotion-based MIDI generator, the first to condition on continuous emotional values rather than discrete categories, enabling nuanced music generation aligned with complex emotional content. To enhance temporal synchronization, we introduce a novel temporal boundary conditioning method, called "boundary offset encodings," aligning musical chords with scene changes. Combining video emotion classification, emotion-based music generation, and temporal boundary conditioning, EMSYNC emerges as a fully automatic video-based music generator. User studies show that it consistently outperforms existing methods in terms of music richness, emotional alignment, temporal synchronization, and overall preference, setting a new state-of-the-art in video-based music generation.

</details>


### [199] [Hybrid Dual-Path Linear Transformations for Efficient Transformer Architectures](https://arxiv.org/abs/2602.07070)
*Vladimer Khasia*

Main category: cs.LG

TL;DR: 提出HDPL算子改进标准Transformer架构，实验显示性能更优且参数减少，还探讨潜在优势。


<details>
  <summary>Details</summary>
Motivation: 标准Transformer架构依赖密集线性变换，效率低且缺乏区分局部特征保存和全局上下文整合的结构归纳偏差。

Method: 引入HDPL算子，将仿射变换分解为用于局部处理的稀疏块对角组件和用于全局上下文正则化的低秩VAE瓶颈，替换特定投影，保留标准密集层用于聚合。

Result: 在FineWeb - Edu数据集上，HDPL架构优于标准Llama基线，减少验证损失且参数数量减少6.8%。

Conclusion: HDPL架构实现效率和表示能力的平衡，其概率潜在空间为推理、适应、解释等提供新途径。

Abstract: Standard Transformer architectures rely heavily on dense linear transformations, treating feature projection as a monolithic, full-rank operation. We argue that this formulation is inefficient and lacks the structural inductive bias necessary for distinguishing between local feature preservation and global context integration. To address this, we introduce the Hybrid Dual-Path Linear (HDPL) operator, which decomposes the affine transformation into two topologically distinct pathways: a sparse block-diagonal component for high-rank local processing, and a low-rank Variational Autoencoder (VAE) bottleneck for global context regularization. By "surgically" replacing specific projections (Query, Key, Value, Gate, Up) with HDPL operators while retaining standard dense layers for aggregation (Output, Down), we achieve a superior balance of efficiency and representational power. Experiments on the FineWeb-Edu dataset demonstrate that the HDPL architecture outperforms a standard Llama-style baseline, reducing validation loss while simultaneously reducing parameter count by 6.8%. Beyond immediate performance gains, we discuss how the explicit materialization of a probabilistic latent space within the Transformer backbone serves as a vital architectural affordance, offering new pathways for inference-time or hypernetwork induced control, continual adaptation, interpretability, and cross-model or cross-modal synchronization. The code is available at https://github.com/VladimerKhasia/HDPL

</details>


### [200] [Systematic Performance Assessment of Deep Material Networks for Multiscale Material Modeling](https://arxiv.org/abs/2602.07192)
*Xiaolong He,Haoyan Wei,Wei Hu,Henan Mao,C. T. Wu*

Main category: cs.LG

TL;DR: 文章对深度材料网络（DMNs）进行全面评估，研究离线训练选择的影响，对比IMN与原DMN，为多尺度材料建模提供指导。


<details>
  <summary>Details</summary>
Motivation: DMNs虽应用渐广，但对其全离线 - 在线流程性能的系统评估有限，需全面评估并研究相关训练影响。

Method: 对DMNs进行有关预测精度、计算效率和训练鲁棒性的综合比较评估，研究离线训练选择因素对在线泛化性能和不确定性的影响。

Result: 预测误差和方差随训练数据量增加而降低；初始化和批量大小显著影响模型性能；激活正则化对控制网络复杂度和泛化性能很关键；IMN离线训练速度比原DMN快3.4 - 4.7倍，在线预测精度和计算效率相当。

Conclusion: 明确了结构保留材料网络中模型表达能力和效率的关键权衡，为其在多尺度材料建模中的部署提供了实用指导。

Abstract: Deep Material Networks (DMNs) are structure-preserving, mechanistic machine learning models that embed micromechanical principles into their architectures, enabling strong extrapolation capabilities and significant potential to accelerate multiscale modeling of complex microstructures. A key advantage of these models is that they can be trained exclusively on linear elastic data and then generalized to nonlinear inelastic regimes during online prediction. Despite their growing adoption, systematic evaluations of their performance across the full offline-online pipeline remain limited. This work presents a comprehensive comparative assessment of DMNs with respect to prediction accuracy, computational efficiency, and training robustness. We investigate the effects of offline training choices, including initialization, batch size, training data size, and activation regularization on online generalization performance and uncertainty. The results demonstrate that both prediction error and variance decrease with increasing training data size, while initialization and batch size can significantly influence model performance. Moreover, activation regularization is shown to play a critical role in controlling network complexity and therefore generalization performance. Compared with the original DMN, the rotation-free Interaction-based Material Network (IMN) formulation achieves a 3.4x - 4.7x speed-up in offline training, while maintaining comparable online prediction accuracy and computational efficiency. These findings clarify key trade-offs between model expressivity and efficiency in structure-preserving material networks and provide practical guidance for their deployment in multiscale material modeling.

</details>


### [201] [Online Learning for Uninformed Markov Games: Empirical Nash-Value Regret and Non-Stationarity Adaptation](https://arxiv.org/abs/2602.07205)
*Junyan Liu,Haipeng Luo,Zihan Zhang,Lillian J. Ratliff*

Main category: cs.LG

TL;DR: 研究二人无信息马尔可夫博弈在线学习，改进前人算法不足，提出新后悔度量和无参数算法，实现自适应后悔界。


<details>
  <summary>Details</summary>
Motivation: 前人算法和保证不能适应问题难度，即使对手固定策略，结果也不理想，要解决这些局限。

Method: 引入经验纳什值后悔这一新的后悔度量，对基于轮次的V - learning算法重新分析，自适应重启算法。

Result: 提出无参数算法，实现$O(\min \{\sqrt{K} + (CK)^{1/3},\sqrt{LK}\})$后悔界，能自适应对手非平稳性。

Conclusion: 结果能恢复两种极端情况，且能在极端情况间平滑过渡，自适应对手非平稳性。

Abstract: We study online learning in two-player uninformed Markov games, where the opponent's actions and policies are unobserved. In this setting, Tian et al. (2021) show that achieving no-external-regret is impossible without incurring an exponential dependence on the episode length $H$. They then turn to the weaker notion of Nash-value regret and propose a V-learning algorithm with regret $O(K^{2/3})$ after $K$ episodes. However, their algorithm and guarantee do not adapt to the difficulty of the problem: even in the case where the opponent follows a fixed policy and thus $O(\sqrt{K})$ external regret is well-known to be achievable, their result is still the worse rate $O(K^{2/3})$ on a weaker metric.
  In this work, we fully address both limitations. First, we introduce empirical Nash-value regret, a new regret notion that is strictly stronger than Nash-value regret and naturally reduces to external regret when the opponent follows a fixed policy. Moreover, under this new metric, we propose a parameter-free algorithm that achieves an $O(\min \{\sqrt{K} + (CK)^{1/3},\sqrt{LK}\})$ regret bound, where $C$ quantifies the variance of the opponent's policies and $L$ denotes the number of policy switches (both at most $O(K)$). Therefore, our results not only recover the two extremes -- $O(\sqrt{K})$ external regret when the opponent is fixed and $O(K^{2/3})$ Nash-value regret in the worst case -- but also smoothly interpolate between these extremes by automatically adapting to the opponent's non-stationarity. We achieve so by first providing a new analysis of the epoch-based V-learning algorithm by Mao et al. (2022), establishing an $O(ηC + \sqrt{K/η})$ regret bound, where $η$ is the epoch incremental factor. Next, we show how to adaptively restart this algorithm with an appropriate $η$ in response to the potential non-stationarity of the opponent, eventually achieving our final results.

</details>


### [202] [The Optimal Token Baseline: Variance Reduction for Long-Horizon LLM-RL](https://arxiv.org/abs/2602.07078)
*Yingru Li,Jiawei Xu,Ziniu Li,Jiacai Liu,Wei Liu,Yuxuan Tong,Longtao Zheng,Zhenghai Xue,Yaxiang Zhang,Tianle Cai,Ge Zhang,Qian Liu,Baoxiang Wang*

Main category: cs.LG

TL;DR: 本文针对大语言模型强化学习在长序列任务中梯度方差激增问题，提出最优令牌基线OTB及Logit - 梯度代理方法，提升训练稳定性并减少令牌消耗。


<details>
  <summary>Details</summary>
Motivation: 大语言模型强化学习在长序列任务中因梯度方差激增导致训练崩溃，传统方法有优化难、忽略序列和令牌异质性等问题。

Method: 从第一性原理推导最优令牌基线OTB，提出Logit - 梯度代理用前向概率近似梯度范数。

Result: 方法实现训练稳定性，只需N = 4就能达到N = 32的效果，在单轮和工具集成推理任务中减少超65%令牌消耗。

Conclusion: 提出的方法有效解决大语言模型强化学习中的梯度方差问题，提升训练效率。

Abstract: Reinforcement Learning (RL) for Large Language Models (LLMs) often suffers from training collapse in long-horizon tasks due to exploding gradient variance. To mitigate this, a baseline is commonly introduced for advantage computation; however, traditional value models remain difficult to optimize, and standard group-based baselines overlook sequence heterogeneity. Although classic optimal baseline theory can achieve global variance reduction, it neglects token heterogeneity and requires prohibitive gradient-based computation. In this work, we derive the Optimal Token Baseline (OTB) from first principles, proving that gradient updates should be weighted inversely to their cumulative gradient norm. To ensure efficiency, we propose the Logit-Gradient Proxy that approximates the gradient norm using only forward-pass probabilities. Our method achieves training stability and matches the performance of large group sizes ($N=32$) with only $N=4$, reducing token consumption by over 65% across single-turn and tool-integrated reasoning tasks.

</details>


### [203] [Continuous Program Search](https://arxiv.org/abs/2602.07659)
*Matthew Siper,Muhammad Umair Nasir,Ahmed Khalifa,Lisa Soros,Jay Azhang,Julian Togelius*

Main category: cs.LG

TL;DR: 遗传编程有可解释性但突变影响大，本文提出算子设计问题，通过实验表明语义对齐的突变算子能在不修改底层算法下大幅提升搜索效率。


<details>
  <summary>Details</summary>
Motivation: 解决遗传编程中小语法突变会导致行为大幅不可预测变化，降低局部性和样本效率的问题。

Method: 将问题作为算子设计问题，通过跟踪潜在扰动下动作级分歧衡量局部性，学习匹配的块因子化嵌入，对比全潜在空间的各向同性高斯突变和几何编译突变。

Result: 在相同进化策略和评估预算下，学习到的突变算子用更少评估发现强策略，实现最高样本外夏普比率中位数，几何编译突变进展更快更可靠。

Conclusion: 语义对齐的突变能在不修改基础进化算法的情况下大幅提高搜索效率。

Abstract: Genetic Programming yields interpretable programs, but small syntactic mutations can induce large, unpredictable behavioral shifts, degrading locality and sample efficiency. We frame this as an operator-design problem: learn a continuous program space where latent distance has behavioral meaning, then design mutation operators that exploit this structure without changing the evolutionary optimizer.
  We make locality measurable by tracking action-level divergence under controlled latent perturbations, identifying an empirical trust region for behavior-local continuous variation. Using a compact trading-strategy DSL with four semantic components (long/short entry and exit), we learn a matching block-factorized embedding and compare isotropic Gaussian mutation over the full latent space to geometry-compiled mutation that restricts updates to semantically paired entry--exit subspaces and proposes directions using a learned flow-based model trained on logged mutation outcomes.
  Under identical $(μ+λ)$ evolution strategies and fixed evaluation budgets across five assets, the learned mutation operator discovers strong strategies using an order of magnitude fewer evaluations and achieves the highest median out-of-sample Sharpe ratio. Although isotropic mutation occasionally attains higher peak performance, geometry-compiled mutation yields faster, more reliable progress, demonstrating that semantically aligned mutation can substantially improve search efficiency without modifying the underlying evolutionary algorithm.

</details>


### [204] [Attention-Driven Framework for Non-Rigid Medical Image Registration](https://arxiv.org/abs/2602.07088)
*Muhammad Zafar Iqbal,Ghazanfar Farooq Siddiqui,Anwar Ul Haq,Imran Razzak*

Main category: cs.LG

TL;DR: 本文提出AD - RegNet用于非刚性医学图像配准，结合3D UNet 与双向交叉注意力机制，在两个数据集上验证，性能有竞争力，兼顾精度与效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的配准方法在准确对齐大变形图像并保持解剖学合理性方面存在挑战。

Method: 提出AD - RegNet，结合3D UNet骨干与双向交叉注意力机制，引入区域自适应注意力机制和多分辨率变形场合成方法。

Result: 在DIRLab和IXI数据集上实验，性能与现有先进方法有竞争力，兼顾配准精度和计算效率。

Conclusion: 注意力引导的配准提高了对齐精度，同时保证了解剖学上合理的变形，适用于临床应用。

Abstract: Deformable medical image registration is a fundamental task in medical image analysis with applications in disease diagnosis, treatment planning, and image-guided interventions. Despite significant advances in deep learning based registration methods, accurately aligning images with large deformations while preserving anatomical plausibility remains a challenging task. In this paper, we propose a novel Attention-Driven Framework for Non-Rigid Medical Image Registration (AD-RegNet) that employs attention mechanisms to guide the registration process. Our approach combines a 3D UNet backbone with bidirectional cross-attention, which establishes correspondences between moving and fixed images at multiple scales. We introduce a regional adaptive attention mechanism that focuses on anatomically relevant structures, along with a multi-resolution deformation field synthesis approach for accurate alignment. The method is evaluated on two distinct datasets: DIRLab for thoracic 4D CT scans and IXI for brain MRI scans, demonstrating its versatility across different anatomical structures and imaging modalities. Experimental results demonstrate that our approach achieves performance competitive with state-of-the-art methods on the IXI and DIRLab datasets. The proposed method maintains a favorable balance between registration accuracy and computational efficiency, making it suitable for clinical applications. A comprehensive evaluation using normalized cross-correlation (NCC), mean squared error (MSE), structural similarity (SSIM), Jacobian determinant, and target registration error (TRE) indicates that attention-guided registration improves alignment accuracy while ensuring anatomically plausible deformations.

</details>


### [205] [An arithmetic method algorithm optimizing k-nearest neighbors compared to regression algorithms and evaluated on real world data sources](https://arxiv.org/abs/2602.08577)
*Theodoros Anagnostopoulos,Evanthia Zervoudi,Christos Anagnostopoulos,Apostolos Christopoulos,Bogdan Wierzbinski*

Main category: cs.LG

TL;DR: 本文提出优化k - NN算法的Arithmetic Method Regression (AMR)算法，与其他回归算法对比，结果显示AMR是k - NN的优化。


<details>
  <summary>Details</summary>
Motivation: 优化k - NN算法，提升线性回归分析的性能。

Method: 采用Arithmetic Method Algorithm (AMA)评估算术方法效率，提出AMR算法优化k - NN，并根据最优推理决策规则与其他算法对比，在公开的真实世界数据源上评估。

Result: AMR算法与其他算法性能相当，多数情况下比k - NN性能更好。

Conclusion: 引入的AMR算法是k - NN的优化。

Abstract: Linear regression analysis focuses on predicting a numeric regressand value based on certain regressor values. In this context, k-Nearest Neighbors (k-NN) is a common non-parametric regression algorithm, which achieves efficient performance when compared with other algorithms in literature. In this research effort an optimization of the k-NN algorithm is proposed by exploiting the potentiality of an introduced arithmetic method, which can provide solutions for linear equations involving an arbitrary number of real variables. Specifically, an Arithmetic Method Algorithm (AMA) is adopted to assess the efficiency of the introduced arithmetic method, while an Arithmetic Method Regression (AMR) algorithm is proposed as an optimization of k-NN adopting the potentiality of AMA. Such algorithm is compared with other regression algorithms, according to an introduced optimal inference decision rule, and evaluated on certain real world data sources, which are publicly available. Results are promising since the proposed AMR algorithm has comparable performance with the other algorithms, while in most cases it achieves better performance than the k-NN. The output results indicate that introduced AMR is an optimization of k-NN.

</details>


### [206] [ParisKV: Fast and Drift-Robust KV-Cache Retrieval for Long-Context LLMs](https://arxiv.org/abs/2602.07721)
*Yanlin Qi,Xinhang Chen,Huiqiang Jiang,Qitong Wang,Botao Peng,Themis Palpanas*

Main category: cs.LG

TL;DR: 介绍ParisKV，一种基于碰撞候选选择和量化内积重排估计器的KV缓存检索框架，在长上下文解码效率上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有KV缓存检索方法在大规模时存在分布漂移和高延迟问题。

Method: 引入ParisKV框架，采用碰撞候选选择，随后进行量化内积重排估计，支持通过统一虚拟寻址进行CPU卸载的KV缓存。

Result: 在长输入和长生成基准测试中匹配或超越全注意力质量，实现了最先进的长上下文解码效率，在百万令牌规模下相比MagicPIG和PQCache大幅降低解码延迟。

Conclusion: ParisKV是一种高效的、抗漂移的KV缓存检索框架，能有效解决长上下文推理的问题。

Abstract: KV-cache retrieval is essential for long-context LLM inference, yet existing methods struggle with distribution drift and high latency at scale. We introduce ParisKV, a drift-robust, GPU-native KV-cache retrieval framework based on collision-based candidate selection, followed by a quantized inner-product reranking estimator. For million-token contexts, ParisKV supports CPU-offloaded KV caches via Unified Virtual Addressing (UVA), enabling on-demand top-$k$ fetching with minimal overhead. ParisKV matches or outperforms full attention quality on long-input and long-generation benchmarks. It achieves state-of-the-art long-context decoding efficiency: it matches or exceeds full attention speed even at batch size 1 for long contexts, delivers up to 2.8$\times$ higher throughput within full attention's runnable range, and scales to million-token contexts where full attention runs out of memory. At million-token scale, ParisKV reduces decode latency by 17$\times$ and 44$\times$ compared to MagicPIG and PQCache, respectively, two state-of-the-art KV-cache Top-$k$ retrieval baselines.

</details>


### [207] [Finding Connections: Membership Inference Attacks for the Multi-Table Synthetic Data Setting](https://arxiv.org/abs/2602.07126)
*Joshua Ward,Chi-Hua Wang,Guang Cheng*

Main category: cs.LG

TL;DR: 提出新型成员推理攻击（MIA）设置审计合成关系数据用户级隐私，提出MT - MIA攻击，评估显示现有合成数据生成器存在隐私漏洞。


<details>
  <summary>Details</summary>
Motivation: 合成关系数据发布存在独特隐私挑战，现有单表MIA低估用户级隐私泄露。

Method: 提出新型MIA设置，提出基于异质图神经网络的MT - MIA攻击。

Result: 在多表数据集上评估，证明现有关系合成数据生成器存在隐私漏洞。

Conclusion: MT - MIA能更好针对用户级漏洞，现有合成数据生成器有隐私泄露问题。

Abstract: Synthetic tabular data has gained attention for enabling privacy-preserving data sharing. While substantial progress has been made in single-table synthetic generation where data are modeled at the row or item level, most real-world data exists in relational databases where a user's information spans items across multiple interconnected tables. Recent advances in synthetic relational data generation have emerged to address this complexity, yet release of these data introduce unique privacy challenges as information can be leaked not only from individual items but also through the relationships that comprise a complete user entity.
  To address this, we propose a novel Membership Inference Attack (MIA) setting to audit the empirical user-level privacy of synthetic relational data and show that single-table MIAs that audit at an item level underestimate user-level privacy leakage. We then propose Multi-Table Membership Inference Attack (MT-MIA), a novel adversarial attack under a No-Box threat model that targets learned representations of user entities via Heterogeneous Graph Neural Networks. By incorporating all connected items for a user, MT-MIA better targets user-level vulnerabilities induced by inter-tabular relationships than existing attacks. We evaluate MT-MIA on a range of real-world multi-table datasets and demonstrate that this vulnerability exists in state-of-the-art relational synthetic data generators, employing MT-MIA to additionally study where this leakage occurs.

</details>


### [208] [SDFed: Bridging Local Global Discrepancy via Subspace Refinement and Divergence Control in Federated Prompt Learning](https://arxiv.org/abs/2602.08590)
*Yicheng Di,Wei Yuan,Tieke He,Zhanjie Zhang,Ao Ma,Yuan Liu,Hongzhi Yin*

Main category: cs.LG

TL;DR: 提出SDFed框架解决视觉语言预训练模型在多参与方设置中联邦优化通信成本高和客户端本地数据有限问题，实验证明其在异构联邦场景下提升性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 视觉语言预训练模型在隐私敏感的多方设置中应用有挑战，现有联邦提示学习方法在客户端异质性下有不足。

Method: 提出SDFed框架，维护固定长度全局提示和可变长度本地提示，引入子空间细化和信息保留与差异控制策略。

Result: 在多个数据集上的大量实验表明SDFed在异构联合设置中持续提高性能和鲁棒性。

Conclusion: SDFed能有效解决现有联邦提示学习方法在客户端异质性下的问题，提升性能和鲁棒性。

Abstract: Vision-language pretrained models offer strong transferable representations, yet adapting them in privacy-sensitive multi-party settings is challenging due to the high communication cost of federated optimization and the limited local data on clients. Federated prompt learning mitigates this issue by keeping the VLPM backbone frozen and collaboratively training lightweight prompt parameters. However, existing approaches typically enforce a unified prompt structure and length across clients, which is inadequate under practical client heterogeneity in both data distributions and system resources, and may further introduce conflicts between globally shared and locally optimal knowledge. To address these challenges, we propose \textbf{SDFed}, a heterogeneous federated prompt learning framework that bridges Local-Global Discrepancy via Subspace Refinement and Divergence Control. SDFed maintains a fixed-length global prompt for efficient aggregation while allowing each client to learn a variable-length local prompt to better match its data characteristics and capacity. To mitigate local-global conflicts and facilitate effective knowledge transfer, SDFed introduces a subspace refinement method for local prompts and an information retention and divergence control strategy that preserves key local information while maintaining appropriate separability between global and local representations. Extensive experiments on several datasets demonstrate that SDFed consistently improves performance and robustness in heterogeneous federated settings.

</details>


### [209] [Beyond Arrow: From Impossibility to Possibilities in Multi-Criteria Benchmarking](https://arxiv.org/abs/2602.07593)
*Polina Gordienko,Christoph Jansen,Julian Rodemann,Georg Schollmeyer*

Main category: cs.LG

TL;DR: 将现代基准测试的多指标聚合为单一排名存在问题，本文将其形式化为社会选择问题，给出有意义聚合的充分条件并实证验证。


<details>
  <summary>Details</summary>
Motivation: 现代基准测试多指标聚合为单一排名时，自然聚合方法可能不连贯或不稳定。

Method: 将聚合问题形式化为社会选择问题，对排名组合提出三种限制条件，即单峰、组可分和距离受限偏好。

Result: 在单峰、组可分和距离受限偏好下，基准算子可构建模型的良好排名，对现代基准套件进行实证验证。

Conclusion: 确定了多指标基准测试有意义聚合的充分条件，可解决部分聚合问题。

Abstract: Modern benchmarks such as HELM MMLU account for multiple metrics like accuracy, robustness and efficiency. When trying to turn these metrics into a single ranking, natural aggregation procedures can become incoherent or unstable to changes in the model set. We formalize this aggregation as a social choice problem where each metric induces a preference ranking over models on each dataset, and a benchmark operator aggregates these votes across metrics. While prior work has focused on Arrow's impossibility result, we argue that the impossibility often originates from pathological examples and identify sufficient conditions under which these disappear, and meaningful multi-criteria benchmarking becomes possible. In particular, we deal with three restrictions on the combinations of rankings and prove that on single-peaked, group-separable and distance-restricted preferences, the benchmark operator allows for the construction of well-behaved rankings of the involved models. Empirically, we investigate several modern benchmark suites like HELM MMLU and verify which structural conditions are fulfilled on which benchmark problems.

</details>


### [210] [Landscaper: Understanding Loss Landscapes Through Multi-Dimensional Topological Analysis](https://arxiv.org/abs/2602.07135)
*Jiaqing Chen,Nicholas Hadler,Tiankai Xie,Rostyslav Hnatyshyn,Caleb Geniesse,Yaoqing Yang,Michael W. Mahoney,Talita Perciano,John F. Hartwig,Ross Maciejewski,Gunther H. Weber*

Main category: cs.LG

TL;DR: 提出开源Python包Landscaper用于任意维度损失景观分析，结合Hessian子空间构建与拓扑数据分析，用SMAD量化平滑度，在多种任务中展示有效性。


<details>
  <summary>Details</summary>
Motivation: 传统低维损失景观分析常遗漏复杂拓扑特征，需要更好的工具来理解神经网络优化和泛化。

Method: 开发Landscaper包，结合Hessian-based子空间构建和拓扑数据分析，使用Saddle - Minimum Average Distance (SMAD)量化景观平滑度。

Result: 在多种架构和任务（包括预训练语言模型、化学属性预测任务）中展示了Landscaper的有效性，SMAD能捕捉传统指标遗漏的训练转变。

Conclusion: Landscaper在数据稀缺的科学机器学习场景中，对模型诊断和架构设计有重要价值，SMAD可作为分布外泛化的指标。

Abstract: Loss landscapes are a powerful tool for understanding neural network optimization and generalization, yet traditional low-dimensional analyses often miss complex topological features. We present Landscaper, an open-source Python package for arbitrary-dimensional loss landscape analysis. Landscaper combines Hessian-based subspace construction with topological data analysis to reveal geometric structures such as basin hierarchy and connectivity. A key component is the Saddle-Minimum Average Distance (SMAD) for quantifying landscape smoothness. We demonstrate Landscaper's effectiveness across various architectures and tasks, including those involving pre-trained language models, showing that SMAD captures training transitions, such as landscape simplification, that conventional metrics miss. We also illustrate Landscaper's performance in challenging chemical property prediction tasks, where SMAD can serve as a metric for out-of-distribution generalization, offering valuable insights for model diagnostics and architecture design in data-scarce scientific machine learning scenarios.

</details>


### [211] [Featured Reproducing Kernel Banach Spaces for Learning and Neural Networks](https://arxiv.org/abs/2602.07141)
*Isabel de la Higuera,Francisco Herrera,M. Victoria Velasco*

Main category: cs.LG

TL;DR: 本文为Banach空间中的学习建立泛函分析框架，统一了核方法和神经网络的函数空间视角。


<details>
  <summary>Details</summary>
Motivation: 许多现代学习模型产生非Hilbert几何，超出了再生核Hilbert空间框架，而Banach空间中现有条件不足以支持核学习。

Method: 基于特征再生核Banach空间的概念，确定特征映射、核构造和表示定理结果可恢复的结构条件，将监督学习表述为最小范数插值或正则化问题。

Result: 建立存在性结果和条件表示定理，将理论扩展到向量值特征再生核Banach空间，表明固定架构神经网络是此类空间的特殊实例。

Conclusion: 提供了核方法和神经网络的统一函数空间视角，明确了核学习原理何时可扩展到再生核Hilbert空间之外。

Abstract: Reproducing kernel Hilbert spaces provide a foundational framework for kernel-based learning, where regularization and interpolation problems admit finite-dimensional solutions through classical representer theorems. Many modern learning models, however -- including fixed-architecture neural networks equipped with non-quadratic norms -- naturally give rise to non-Hilbertian geometries that fall outside this setting. In Banach spaces, continuity of point-evaluation functionals alone is insufficient to guarantee feature representations or kernel-based learning formulations. In this work, we develop a functional-analytic framework for learning in Banach spaces based on the notion of featured reproducing kernel Banach spaces. We identify the precise structural conditions under which feature maps, kernel constructions, and representer-type results can be recovered beyond the Hilbertian regime. Within this framework, supervised learning is formulated as a minimal-norm interpolation or regularization problem, and existence results together with conditional representer theorems are established. We further extend the theory to vector-valued featured reproducing kernel Banach spaces and show that fixed-architecture neural networks naturally induce special instances of such spaces. This provides a unified function-space perspective on kernel methods and neural networks and clarifies when kernel-based learning principles extend beyond reproducing kernel Hilbert spaces.

</details>


### [212] [Don't Always Pick the Highest-Performing Model: An Information Theoretic View of LLM Ensemble Selection](https://arxiv.org/abs/2602.08003)
*Yigit Turkmen,Baturalp Buyukates,Melih Bastopcu*

Main category: cs.LG

TL;DR: 本文研究大语言模型集成选择问题，提出基于互信息的贪心选择算法，在多个数据集上表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 大语言模型集成时模型强相关，需解决集成时选模型的问题，并解释模型增多性能饱和的原因。

Method: 将预算约束下的集成选择问题转化为最大化互信息问题，用高斯copula建模相关误差，提出直接从数据估计信息项的贪心互信息选择算法。

Result: 在MEDMCQA、MMLU和IMDB三个数据集上，该方法在相同查询预算下始终优于强基线。

Conclusion: 所提贪心互信息选择算法在大语言模型集成选择上有效，能提升性能。

Abstract: Large language models (LLMs) are often ensembled together to improve overall reliability and robustness, but in practice models are strongly correlated. This raises a fundamental question: which models should be selected when forming an LLM ensemble? We formulate budgeted ensemble selection as maximizing the mutual information between the true label and predictions of the selected models. Furthermore, to explain why performance can saturate even with many models, we model the correlated errors of the models using Gaussian-copula and show an information-theoretic error floor for the performance of the ensemble. Motivated by these, we propose a simple greedy mutual-information selection algorithm that estimates the required information terms directly from data and iteratively builds an ensemble under a query budget. We test our approach in two question answering datasets and one binary sentiment classification dataset: MEDMCQA, MMLU, and IMDB movie reviews. Across all datasets, we observe that our method consistently outperforms strong baselines under the same query budget.

</details>


### [213] [Constraint-Aware Generative Auto-bidding via Pareto-Prioritized Regret Optimization](https://arxiv.org/abs/2602.08261)
*Binglin Wu,Yingyi Zhang,Xianneng Li,Ruyue Deng,Chuan Yue,Weiru Zhang,Xiaoyi Zeng*

Main category: cs.LG

TL;DR: 本文提出PRO - Bid框架解决决策转换器应用于自动出价系统的挑战，实验显示其性能优于基线。


<details>
  <summary>Details</summary>
Motivation: 决策转换器应用于有严格效率约束的自动出价系统存在状态混淆和无法优化至约束边界的问题。

Method: 提出由约束解耦帕累托表示（CDPR）和反事实遗憾优化（CRO）两个机制协同的PRO - Bid框架。

Result: 在两个公开基准测试和在线A / B测试中，PRO - Bid在约束满足和价值获取方面优于现有基线。

Conclusion: PRO - Bid能有效解决决策转换器在自动出价系统中的应用挑战，实现更好的约束满足和价值获取。

Abstract: Auto-bidding systems aim to maximize marketing value while satisfying strict efficiency constraints such as Target Cost-Per-Action (CPA). Although Decision Transformers provide powerful sequence modeling capabilities, applying them to this constrained setting encounters two challenges: 1) standard Return-to-Go conditioning causes state aliasing by neglecting the cost dimension, preventing precise resource pacing; and 2) standard regression forces the policy to mimic average historical behaviors, thereby limiting the capacity to optimize performance toward the constraint boundary. To address these challenges, we propose PRO-Bid, a constraint-aware generative auto-bidding framework based on two synergistic mechanisms: 1) Constraint-Decoupled Pareto Representation (CDPR) decomposes global constraints into recursive cost and value contexts to restore resource perception, while reweighting trajectories based on the Pareto frontier to focus on high-efficiency data; and 2) Counterfactual Regret Optimization (CRO) facilitates active improvement by utilizing a global outcome predictor to identify superior counterfactual actions. By treating these high-utility outcomes as weighted regression targets, the model transcends historical averages to approach the optimal constraint boundary. Extensive experiments on two public benchmarks and online A/B tests demonstrate that PRO-Bid achieves superior constraint satisfaction and value acquisition compared to state-of-the-art baselines.

</details>


### [214] [On the Infinite Width and Depth Limits of Predictive Coding Networks](https://arxiv.org/abs/2602.07697)
*Francesco Innocenti,El Mehdi Achour,Rafal Bogacz*

Main category: cs.LG

TL;DR: 研究预测编码网络（PCN）的无限宽度和深度极限，发现某些条件下PC与BP计算相同梯度。


<details>
  <summary>Details</summary>
Motivation: 现有改进PCN训练稳定性的BP启发式重参数化方法的可扩展性和理论基础不明确。

Method: 研究线性残差网络的无限宽度和深度极限，对深度非线性网络进行实验。

Result: 对线性残差网络，PC的宽度和深度稳定特征学习参数化集与BP相同；在特定条件下，PC能量收敛到BP损失，计算相同梯度，在深度非线性网络实验中结果也成立。

Conclusion: 统一了先前理论和实验结果，对PCN的扩展有潜在重要意义。

Abstract: Predictive coding (PC) is a biologically plausible alternative to standard backpropagation (BP) that minimises an energy function with respect to network activities before updating weights. Recent work has improved the training stability of deep PC networks (PCNs) by leveraging some BP-inspired reparameterisations. However, the full scalability and theoretical basis of these approaches remains unclear. To address this, we study the infinite width and depth limits of PCNs. For linear residual networks, we show that the set of width- and depth-stable feature-learning parameterisations for PC is exactly the same as for BP. Moreover, under any of these parameterisations, the PC energy with equilibrated activities converges to the BP loss in a regime where the model width is much larger than the depth, resulting in PC computing the same gradients as BP. Experiments show that these results hold in practice for deep nonlinear networks, as long as an activity equilibrium seem to be reached. Overall, this work unifies various previous theoretical and empirical results and has potentially important implications for the scaling of PCNs.

</details>


### [215] [BONSAI: Bayesian Optimization with Natural Simplicity and Interpretability](https://arxiv.org/abs/2602.07144)
*Samuel Daulton,David Eriksson,Maximilian Balandat,Eytan Bakshy*

Main category: cs.LG

TL;DR: 本文介绍了一种默认感知的贝叶斯优化策略BONSAI，能在控制获取值损失时修剪低影响偏差，理论和实践证明其有效。


<details>
  <summary>Details</summary>
Motivation: 标准贝叶斯优化不旨在最小化与默认配置的偏差，实践中常将弱相关参数推至搜索空间边界，增加审查建议的负担。

Method: 引入BONSAI策略，该策略可与多种获取函数兼容，理论上界定其后悔值。

Result: 发现BONSAI在多个实际应用中显著减少推荐配置中非默认参数的数量，同时保持有竞争力的优化性能，对运行时间影响小。

Conclusion: BONSAI策略有效，在减少非默认参数的同时能维持优化性能。

Abstract: Bayesian optimization (BO) is a popular technique for sample-efficient optimization of black-box functions. In many applications, the parameters being tuned come with a carefully engineered default configuration, and practitioners only want to deviate from this default when necessary. Standard BO, however, does not aim to minimize deviation from the default and, in practice, often pushes weakly relevant parameters to the boundary of the search space. This makes it difficult to distinguish between important and spurious changes and increases the burden of vetting recommendations when the optimization objective omits relevant operational considerations. We introduce BONSAI, a default-aware BO policy that prunes low-impact deviations from a default configuration while explicitly controlling the loss in acquisition value. BONSAI is compatible with a variety of acquisition functions, including expected improvement and upper confidence bound (GP-UCB). We theoretically bound the regret incurred by BONSAI, showing that, under certain conditions, it enjoys the same no-regret property as vanilla GP-UCB. Across many real-world applications, we empirically find that BONSAI substantially reduces the number of non-default parameters in recommended configurations while maintaining competitive optimization performance, with little effect on wall time.

</details>


### [216] [On Randomness in Agentic Evals](https://arxiv.org/abs/2602.07150)
*Bjarni Haukur Bjarnason,André Silva,Martin Monperrus*

Main category: cs.LG

TL;DR: 研究发现单轮次计算的pass@1分数有显著方差，建议采用多轮次运行等方法进行可靠评估。


<details>
  <summary>Details</summary>
Motivation: 验证多数论文假设的单轮次运行计算的pass@1分数能可靠评估智能体系统性能是否合理。

Method: 在SWE - Bench - Verified上收集60,000个智能体轨迹，开展跨三个模型和两个框架实验，进行token级分析。

Result: 单轮次pass@1估计存在2.2 - 6.0个百分点差异，差异在早期token就产生并影响后续策略，报告的2 - 3个百分点的提升可能是评估噪音。

Conclusion: 建议采用多轮次运行估计pass@1、用统计功效分析确定运行次数、考虑pass@k和pass^k等指标来可靠评估智能体系统，虽增加成本但必要。

Abstract: Agentic systems are evaluated on benchmarks where agents interact with environments to solve tasks. Most papers report a pass@1 score computed from a single run per task, assuming this gives a reliable performance estimate. We test this assumption by collecting 60,000 agentic trajectories on SWE-Bench-Verified, spanning three models and two scaffolds. We find substantial variance: single-run pass@1 estimates vary by 2.2 to 6.0 percentage points depending on which run is selected, with standard deviations exceeding 1.5 percentage points even at temperature 0. This variance has critical implications: reported improvements of 2--3 percentage points may reflect evaluation noise rather than genuine algorithmic progress. Through token-level analysis, we show that trajectories diverge early, often within the first few percent of tokens, and that these small differences cascade into different solution strategies. To enable reliable evaluation of agentic systems, we recommend three concrete practices: (1) estimate pass@1 from multiple independent runs per task, especially when measuring small improvements, (2) use statistical power analysis to determine the number of runs needed to detect expected effect sizes, and (3) consider metrics like pass@k (optimistic bound) and pass^k (pessimistic bound) with k>1 to better characterize the full performance envelope. While these practices increase evaluation cost, they are essential for distinguishing genuine scientific progress from statistical noise.

</details>


### [217] [Beyond Pooling: Matching for Robust Generalization under Data Heterogeneity](https://arxiv.org/abs/2602.07154)
*Ayush Roy,Rudrasis Chakraborty,Lav Varshney,Vishnu Suresh Lokhande*

Main category: cs.LG

TL;DR: 提出匹配框架处理异构数据集池化问题，在不对称元分布下效果更好，可用于零样本医学异常检测。


<details>
  <summary>Details</summary>
Motivation: 解决朴素池化在跨领域表示学习中会放大分布不对称、产生有偏估计的问题，特别是在需要零样本泛化的场景。

Method: 提出匹配框架，选择相对于自适应质心的样本并迭代细化表示分布，利用双重稳健性和倾向得分匹配过滤混淆域。

Result: 理论和实证分析表明，在不对称元分布下匹配比朴素池化和均匀子采样效果更好，可扩展到非高斯和多模态现实场景。

Conclusion: 该匹配框架能有效改善零样本医学异常检测效果。

Abstract: Pooling heterogeneous datasets across domains is a common strategy in representation learning, but naive pooling can amplify distributional asymmetries and yield biased estimators, especially in settings where zero-shot generalization is required. We propose a matching framework that selects samples relative to an adaptive centroid and iteratively refines the representation distribution. The double robustness and the propensity score matching for the inclusion of data domains make matching more robust than naive pooling and uniform subsampling by filtering out the confounding domains (the main cause of heterogeneity). Theoretical and empirical analyses show that, unlike naive pooling or uniform subsampling, matching achieves better results under asymmetric meta-distributions, which are also extended to non-Gaussian and multimodal real-world settings. Most importantly, we show that these improvements translate to zero-shot medical anomaly detection, one of the extreme forms of data heterogeneity and asymmetry. The code is available on https://github.com/AyushRoy2001/Beyond-Pooling.

</details>


### [218] [Modalities, a PyTorch-native Framework For Large-scale LLM Training and Research](https://arxiv.org/abs/2602.08387)
*Max Lübbering,Timm Ruland,Richard Rutmann,Felix Stollenwerk,David Fitzek,Michael Fromm,Alexander Weber,Rafet Sifa,Nicolas Flores-Herr,Joachim Köhler,Mehdi Ali*

Main category: cs.LG

TL;DR: 针对LLM研究中大规模消融实验工具不足，提出PyTorch原生框架Modalities，支持高效预训练和系统消融，有良好可重复性和扩展性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM（预）训练和研究工作流中大规模消融实验计算成本高，且现有开源框架缺乏相关工具，研究者需自行编写脚本。

Method: 提出一个端到端的PyTorch原生框架Modalities，一是集成最先进的并行策略，二是采用模块化设计和声明式、自包含配置。

Result: 文档未提及具体结果。

Conclusion: Modalities可实现万亿token和十亿参数规模的高效预训练和系统消融，具备现有LLM训练框架难以实现的可重复性和扩展性。

Abstract: Today's LLM (pre-) training and research workflows typically allocate a significant amount of compute to large-scale ablation studies. Despite the substantial compute costs of these ablations, existing open-source frameworks provide limited tooling for these experiments, often forcing researchers to write their own wrappers and scripts. We propose Modalities, an end-to-end PyTorch-native framework that integrates data-driven LLM research with large-scale model training from two angles. Firstly, by integrating state-of-the-art parallelization strategies, it enables both efficient pretraining and systematic ablations at trillion-token and billion-parameter scale. Secondly, Modalities adopts modular design with declarative, self-contained configuration, enabling reproducibility and extensibility levels that are difficult to achieve out-of-the-box with existing LLM training frameworks.

</details>


### [219] [Mimetic Initialization of MLPs](https://arxiv.org/abs/2602.07156)
*Asher Trockman,J. Zico Kolter*

Main category: cs.LG

TL;DR: 本文首次将模拟初始化方法应用于通道混合层（MLP），提出给第一层非零均值的简单技术，能加速小规模视觉任务训练，可与空间混合初始化结合产生额外积极效果。


<details>
  <summary>Details</summary>
Motivation: 之前模拟初始化仅应用于空间混合层，本文尝试将其应用于通道混合层（MLP）。

Method: 给MLP的第一层赋予非零均值。

Result: 在CIFAR - 10和ImageNet - 1k等小规模视觉任务上加速了训练，虽效果比空间混合初始化小，但可与之结合产生额外积极效果。

Conclusion: 模拟初始化方法可应用于通道混合层（MLP），所提简单技术有一定效果且可与其他方法结合。

Abstract: Mimetic initialization uses pretrained models as case studies of good initialization, using observations of structures in trained weights to inspire new, simple initialization techniques. So far, it has been applied only to spatial mixing layers, such convolutional, self-attention, and state space layers. In this work, we present the first attempt to apply the method to channel mixing layers, namely multilayer perceptrons (MLPs). Our extremely simple technique for MLPs -- to give the first layer a nonzero mean -- speeds up training on small-scale vision tasks like CIFAR-10 and ImageNet-1k. Though its effect is much smaller than spatial mixing initializations, it can be used in conjunction with them for an additional positive effect.

</details>


### [220] [RIFLE: Robust Distillation-based FL for Deep Model Deployment on Resource-Constrained IoT Networks](https://arxiv.org/abs/2602.08446)
*Pouria Arefijamal,Mahdi Ahmadlou,Bardia Safaei,Jörg Henkel*

Main category: cs.LG

TL;DR: 本文提出 RIFLE 联邦学习框架，用基于 logit 的知识转移替代梯度共享，在受限物联网系统训练深度模型并验证更新可靠性，实验表明其在准确率、抗攻击等方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有 TinyML 模型在数据异质性和任务复杂性增加时难以捕获复杂模式，且要确保对抗恶意客户端和中毒更新的鲁棒性。

Method: 引入 RIFLE 框架，用基于 logit 的知识转移替代梯度共享，利用知识蒸馏聚合方案，采用 KL 散度验证机制。

Result: 在三个基准数据集上，RIFLE 减少误报率达 87.5%，增强中毒攻击缓解 62.5%，准确率最高提升 28.3%，并将 VGG19 训练时间从超 600 天降至 1.39 小时。

Conclusion: RIFLE 使深度学习在资源受限网络中可行，在抗攻击和准确率方面优于传统联邦学习。

Abstract: Federated learning (FL) is a decentralized learning paradigm widely adopted in resource-constrained Internet of Things (IoT) environments. These devices, typically relying on TinyML models, collaboratively train global models by sharing gradients with a central server while preserving data privacy. However, as data heterogeneity and task complexity increase, TinyML models often become insufficient to capture intricate patterns, especially under extreme non-IID (non-independent and identically distributed) conditions. Moreover, ensuring robustness against malicious clients and poisoned updates remains a major challenge. Accordingly, this paper introduces RIFLE - a Robust, distillation-based Federated Learning framework that replaces gradient sharing with logit-based knowledge transfer. By leveraging a knowledge distillation aggregation scheme, RIFLE enables the training of deep models such as VGG-19 and Resnet18 within constrained IoT systems. Furthermore, a Kullback-Leibler (KL) divergence-based validation mechanism quantifies the reliability of client updates without exposing raw data, achieving high trust and privacy preservation simultaneously. Experiments on three benchmark datasets (MNIST, CIFAR-10, and CIFAR-100) under heterogeneous non-IID conditions demonstrate that RIFLE reduces false-positive detections by up to 87.5%, enhances poisoning attack mitigation by 62.5%, and achieves up to 28.3% higher accuracy compared to conventional federated learning baselines within only 10 rounds. Notably, RIFLE reduces VGG19 training time from over 600 days to just 1.39 hours on typical IoT devices (0.3 GFLOPS), making deep learning practical in resource-constrained networks.

</details>


### [221] [Efficient Adaptive Data Analysis over Dense Distributions](https://arxiv.org/abs/2602.07732)
*Joon Suk Huh*

Main category: cs.LG

TL;DR: 文章探讨自适应数据分析（ADA）中计算效率和样本复杂度的权衡，提出在特定数据分布下可兼顾二者的高效ADA机制。


<details>
  <summary>Details</summary>
Motivation: 现代数据工作流的适应性会导致过拟合和无效统计推断，ADA机制虽能应对但存在计算效率和样本复杂度的权衡问题。

Method: 识别出一类自然的数据分布，在此分布下提出计算高效且能达到最优O(log T)样本复杂度的ADA机制。

Result: 提出的机制在数据分布相对于已知先验稠密时达到最优样本复杂度，还在特定分布设置下产生样本高效的统计查询预言机，且满足PSO安全。

Conclusion: 揭示了自适应数据分析和超越差分隐私的隐私之间的内在联系。

Abstract: Modern data workflows are inherently adaptive, repeatedly querying the same dataset to refine and validate sequential decisions, but such adaptivity can lead to overfitting and invalid statistical inference. Adaptive Data Analysis (ADA) mechanisms address this challenge; however, there is a fundamental tension between computational efficiency and sample complexity. For $T$ rounds of adaptive analysis, computationally efficient algorithms typically incur suboptimal $O(\sqrt{T})$ sample complexity, whereas statistically optimal $O(\log T)$ algorithms are computationally intractable under standard cryptographic assumptions. In this work, we shed light on this trade-off by identifying a natural class of data distributions under which both computational efficiency and optimal sample complexity are achievable. We propose a computationally efficient ADA mechanism that attains optimal $O(\log T)$ sample complexity when the data distribution is dense with respect to a known prior. This setting includes, in particular, feature--label data distributions arising in distribution-specific learning. As a consequence, our mechanism also yields a sample-efficient (i.e., $O(\log T)$ samples) statistical query oracle in the distribution-specific setting. Moreover, although our algorithm is not based on differential privacy, it satisfies a relaxed privacy notion known as Predicate Singling Out (PSO) security (Cohen and Nissim, 2020). Our results thus reveal an inherent connection between adaptive data analysis and privacy beyond differential privacy.

</details>


### [222] [Learning Nonlinear Systems In-Context: From Synthetic Data to Real-World Motor Control](https://arxiv.org/abs/2602.07173)
*Tong Jian,Tianyu Dai,Tao Yu*

Main category: cs.LG

TL;DR: 论文首次提出适用于电机前馈控制的基于transformer模型的上下文学习（ICL）方法，该方法实验表现优于传统方法，展示了ICL在物理系统数据高效控制的潜力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的上下文学习能力未拓展到信号处理系统，经典PI和基于物理的方法在电机前馈控制的非线性和复杂负载条件下有困难。

Method: 提出一种基于transformer的模型架构，分离信号表示和系统行为，能进行少样本微调与单次上下文学习，在大量合成系统数据上预训练。

Result: 方法能跨多个电机负载配置进行泛化，将未调优示例转换为准确的前馈预测，优于PI控制器和基于物理的前馈基线。

Conclusion: ICL可弥合合成预训练和现实适应性之间的差距，为物理系统的数据高效控制开辟新方向。

Abstract: LLMs have shown strong in-context learning (ICL) abilities, but have not yet been extended to signal processing systems. Inspired by their design, we have proposed for the first time ICL using transformer models applicable to motor feedforward control, a critical task where classical PI and physics-based methods struggle with nonlinearities and complex load conditions. We propose a transformer based model architecture that separates signal representation from system behavior, enabling both few-shot finetuning and one-shot ICL. Pretrained on a large corpus of synthetic linear and nonlinear systems, the model learns to generalize to unseen system dynamics of real-world motors only with a handful of examples. In experiments, our approach generalizes across multiple motor load configurations, transforms untuned examples into accurate feedforward predictions, and outperforms PI controllers and physics-based feedforward baselines. These results demonstrate that ICL can bridge synthetic pretraining and real-world adaptability, opening new directions for data efficient control of physical systems.

</details>


### [223] [Latent Target Score Matching, with an application to Simulation-Based Inference](https://arxiv.org/abs/2602.07189)
*Joohwan Ko,Tomas Geffner*

Main category: cs.LG

TL;DR: 提出Latent Target Score Matching解决目标分数匹配在处理潜在变量时的问题，与去噪分数匹配结合提升表现。


<details>
  <summary>Details</summary>
Motivation: 去噪分数匹配在低噪声水平下有高方差问题，目标分数匹配处理潜在变量时因无干净数据分数而受限。

Method: 提出LTSM扩展目标分数匹配以利用联合分数监督边缘分数，与去噪分数匹配混合确保不同噪声尺度的鲁棒性。

Result: 在基于模拟的推理任务中，LTSM持续改善方差、分数准确性和样本质量。

Conclusion: LTSM能提升低噪声水平下扩散模型表现，与DSM混合可增强鲁棒性。

Abstract: Denoising score matching (DSM) for training diffusion models may suffer from high variance at low noise levels. Target Score Matching (TSM) mitigates this when clean data scores are available, providing a low-variance objective. In many applications clean scores are inaccessible due to the presence of latent variables, leaving only joint signals exposed. We propose Latent Target Score Matching (LTSM), an extension of TSM to leverage joint scores for low-variance supervision of the marginal score. While LTSM is effective at low noise levels, a mixture with DSM ensures robustness across noise scales. Across simulation-based inference tasks, LTSM consistently improves variance, score accuracy, and sample quality.

</details>


### [224] [DynamiQ: Accelerating Gradient Synchronization using Compressed Multi-hop All-reduce](https://arxiv.org/abs/2602.08923)
*Wenchen Han,Shay Vargaftik,Michael Mitzenmacher,Ran Ben Basat*

Main category: cs.LG

TL;DR: 本文提出DynamiQ量化框架，用于多跳聚合，相比现有方法有加速和精度优势。


<details>
  <summary>Details</summary>
Motivation: 随着训练规模增大，网络成瓶颈，现有量化系统未针对多跳聚合优化，需减少传输数据量。

Method: 提出DynamiQ量化框架，引入新技术表示部分和，设计解压缩 - 累加 - 重新压缩融合内核，扩展PyTorch DDP支持DynamiQ。

Result: 在不同大语言模型、任务和规模下，比现有方法最高提升34.2%，且是唯一能接近基线精度并加速训练的方法。

Conclusion: DynamiQ能有效弥合量化最佳实践和多跳聚合之间的差距，加速训练并保证精度。

Abstract: Multi-hop all-reduce is the de facto backbone of large model training. As the training scale increases, the network often becomes a bottleneck, motivating reducing the volume of transmitted data. Accordingly, recent systems demonstrated significant acceleration of the training process using gradient quantization. However, these systems are not optimized for multi-hop aggregation, where entries are partially summed multiple times along their aggregation topology.
  This paper presents DynamiQ, a quantization framework that bridges the gap between quantization best practices and multi-hop aggregation. DynamiQ introduces novel techniques to better represent partial sums, co-designed with a decompress-accumulate-recompress fused kernel to facilitate fast execution.
  We extended PyTorch DDP to support DynamiQ over NCCL P2P, and across different LLMs, tasks, and scales, we demonstrate consistent improvement of up to 34.2% over the best among state-of-the-art methods such as Omni-Reduce, THC, and emerging standards such as MXFP4, MXFP6, and MXFP8. Further, DynamiQ is the only evaluated method that consistently reaches near-baseline accuracy (e.g., 99.9% of the BF16 baseline) and does so while significantly accelerating the training.

</details>


### [225] [Near-optimal Swap Regret Minimization for Convex Losses](https://arxiv.org/abs/2602.08862)
*Lunjia Hu,Jon Schneider,Yifan Wu*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We give a randomized online algorithm that guarantees near-optimal $\widetilde O(\sqrt T)$ expected swap regret against any sequence of $T$ adaptively chosen Lipschitz convex losses on the unit interval. This improves the previous best bound of $\widetilde O(T^{2/3})$ and answers an open question of Fishelson et al. [2025b]. In addition, our algorithm is efficient: it runs in $\mathsf{poly}(T)$ time. A key technical idea we develop to obtain this result is to discretize the unit interval into bins at multiple scales of granularity and simultaneously use all scales to make randomized predictions, which we call multi-scale binning and may be of independent interest. A direct corollary of our result is an efficient online algorithm for minimizing the calibration error for general elicitable properties. This result does not require the Lipschitzness assumption of the identification function needed in prior work, making it applicable to median calibration, for which we achieve the first $\widetilde O(\sqrt T)$ calibration error guarantee.

</details>


### [226] [Risk-Sensitive Exponential Actor Critic](https://arxiv.org/abs/2602.07202)
*Alonso Granados,Jason Pacheco*

Main category: cs.LG

TL;DR: 本文针对无模型深度强化学习在现实应用中的安全问题，提出rsEAC方法，该方法更新更稳定，能在复杂任务中学习风险敏感策略。


<details>
  <summary>Details</summary>
Motivation: 无模型深度强化学习应用于现实时存在安全问题，需要风险感知智能体，但现有基于熵风险度量的策略梯度方法更新方差大且数值不稳定，局限于简单任务和表格设置。

Method: 为基于熵风险度量的策略梯度方法提供理论依据，提出rsEAC这一离策略无模型方法，避免指数价值函数及其梯度的显式表示，优化策略。

Result: rsEAC比现有方法更新更具数值稳定性，能在MuJoCo连续任务的风险变体中可靠学习风险敏感策略。

Conclusion: rsEAC是一种有效解决无模型深度强化学习安全问题的方法，能在复杂任务中实现风险敏感策略学习。

Abstract: Model-free deep reinforcement learning (RL) algorithms have achieved tremendous success on a range of challenging tasks. However, safety concerns remain when these methods are deployed on real-world applications, necessitating risk-aware agents. A common utility for learning such risk-aware agents is the entropic risk measure, but current policy gradient methods optimizing this measure must perform high-variance and numerically unstable updates. As a result, existing risk-sensitive model-free approaches are limited to simple tasks and tabular settings. In this paper, we provide a comprehensive theoretical justification for policy gradient methods on the entropic risk measure, including on- and off-policy gradient theorems for the stochastic and deterministic policy settings. Motivated by theory, we propose risk-sensitive exponential actor-critic (rsEAC), an off-policy model-free approach that incorporates novel procedures to avoid the explicit representation of exponential value functions and their gradients, and optimizes its policy w.r.t the entropic risk measure. We show that rsEAC produces more numerically stable updates compared to existing approaches and reliably learns risk-sensitive policies in challenging risky variants of continuous tasks in MuJoCo.

</details>


### [227] [Exactly Computing do-Shapley Values](https://arxiv.org/abs/2602.07203)
*R. Teal Witter,Álvaro Parafita,Tomas Garriga,Maximilian Muschalik,Fabian Fumagalli,Axel Brando,Lucas Rosenblatt*

Main category: cs.LG

TL;DR: 提出基于不可约集计算do - Shapley值的方法，有精确算法和估计器，能提升计算速度并降低识别负担。


<details>
  <summary>Details</summary>
Motivation: 传统计算do - Shapley值需评估指数级项，计算复杂。

Method: 将do - Shapley值重新表述为基础SCM的不可约集，给出精确算法，同时配合有任意查询预算的估计器。

Result: 精确算法计算时间与不可约集数量线性相关；估计器在查询预算接近不可约集数量时，精度比之前方法高几个数量级，达到时可返回接近机器精度的Shapley值。

Conclusion: 新方法不仅提高了计算速度，还降低了do - Shapley值的非参数识别负担。

Abstract: Structural Causal Models (SCM) are a powerful framework for describing complicated dynamics across the natural sciences. A particularly elegant way of interpreting SCMs is do-Shapley, a game-theoretic method of quantifying the average effect of $d$ variables across exponentially many interventions. Like Shapley values, computing do-Shapley values generally requires evaluating exponentially many terms. The foundation of our work is a reformulation of do-Shapley values in terms of the irreducible sets of the underlying SCM. Leveraging this insight, we can exactly compute do-Shapley values in time linear in the number of irreducible sets $r$, which itself can range from $d$ to $2^d$ depending on the graph structure of the SCM. Since $r$ is unknown a priori, we complement the exact algorithm with an estimator that, like general Shapley value estimators, can be run with any query budget. As the query budget approaches $r$, our estimators can produce more accurate estimates than prior methods by several orders of magnitude, and, when the budget reaches $r$, return the Shapley values up to machine precision. Beyond computational speed, we also reduce the identification burden: we prove that non-parametric identifiability of do-Shapley values requires only the identification of interventional effects for the $d$ singleton coalitions, rather than all classes.

</details>


### [228] [DSL: Understanding and Improving Softmax Recommender Systems with Competition-Aware Scaling](https://arxiv.org/abs/2602.07206)
*Bucher Sahyouni,Matthew Vowels,Liqun Chen,Simon Hadfield*

Main category: cs.LG

TL;DR: 提出双尺度Softmax损失（DSL）用于推荐系统，在多个基准测试中优于强基线，还进行了理论分析解释性能提升原因。


<details>
  <summary>Details</summary>
Motivation: 现有Softmax Loss在隐式反馈中，单一全局温度和统一处理负样本会导致训练不稳定。

Method: 在log - sum - exp主干上添加两个互补分支，一是根据负样本难度和物品相似度对负样本重新加权，二是根据竞争强度调整每个样本的温度。

Result: 在多个代表性基准测试和模型主干上，DSL比强基线有显著提升，在多种设置下比Softmax Loss提升超10%，平均提升6.22%；在分布外（OOD）流行度偏移下平均提升9.31%。

Conclusion: DSL能在保留Softmax Loss几何结构的同时重塑负样本和样本间的竞争分布，理论分析解释了其在准确性和鲁棒性上的提升。

Abstract: Softmax Loss (SL) is being increasingly adopted for recommender systems (RS) as it has demonstrated better performance, robustness and fairness. Yet in implicit-feedback, a single global temperature and equal treatment of uniformly sampled negatives can lead to brittle training, because sampled sets may contain varying degrees of relevant or informative competitors. The optimal loss sharpness for a user-item pair with a particular set of negatives, can be suboptimal or destabilising for another with different negatives. We introduce Dual-scale Softmax Loss (DSL), which infers effective sharpness from the sampled competition itself. DSL adds two complementary branches to the log-sum-exp backbone. Firstly it reweights negatives within each training instance using hardness and item--item similarity, secondly it adapts a per-example temperature from the competition intensity over a constructed competitor slate. Together, these components preserve the geometry of SL while reshaping the competition distribution across negatives and across examples.
  Over several representative benchmarks and backbones, DSL yields substantial gains over strong baselines, with improvements over SL exceeding $10%$ in several settings and averaging $6.22%$ across datasets, metrics, and backbones. Under out-of-distribution (OOD) popularity shift, the gains are larger, with an average of $9.31%$ improvement over SL. We further provide a theoretical, distributionally robust optimisation (DRO) analysis, which demonstrates how DSL reshapes the robust payoff and the KL deviation for ambiguous instances. This helps explain the empirically observed improvements in accuracy and robustness.

</details>


### [229] [Adaptive Retrieval helps Reasoning in LLMs -- but mostly if it's not used](https://arxiv.org/abs/2602.07213)
*Srijan Shakya,Anamaria-Roberta Hartl,Sepp Hochreiter,Korbinian Pöppel*

Main category: cs.LG

TL;DR: 本文探索将检索作为动态上下文学习的方式来增强生成模型，测试自适应检索增强架构，在基准测试上对比不同方法，发现检索对推理帮助有限，主动不检索表明模型性能好，模型会根据问题难度调整检索频率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在复杂推理任务中因静态参数知识存在问题，需提升其在专业领域的推理能力。

Method: 测试自适应检索增强架构，让LLM代理在推理过程中主动决定是否查询外部知识库，在GSM8K和MATH - 500基准上与CoT基线和静态检索方法对比。

Result: 静态检索不如CoT；自适应检索中含检索结果的轨迹表现略逊于CoT，不含检索的轨迹表现更好；检索对推理帮助少，主动不检索表明模型性能好；模型会根据问题难度调整检索频率。

Conclusion: 模型自我评估知识并选择性利用外部信息是构建更强大可靠生成模型的关键原则。

Abstract: Large Language Models (LLMs) often falter in complex reasoning tasks due to their static, parametric knowledge, leading to hallucinations and poor performance in specialized domains like mathematics. This work explores a fundamental principle for enhancing generative models: treating retrieval as a form of dynamic in-context learning. We test an adaptive retrieval-augmented architecture where an LLM agent actively decides when to query an external knowledge base during its reasoning process. We compare this adaptive strategy against a standard Chain-of-Thought (CoT) baseline and a static retrieval approach on the GSM8K and MATH-500 benchmarks. Although our experiments show that static retrieval is inferior to CoT, the adaptive retrieval shows interesting behavior: While traces including retrieved results show slightly worse performance compared to CoT, traces that do not include retrieval actually perform better compared to CoT. This suggests that: (a) retrieval only rarely helps reasoning (we show a few counterexamples, e.g. using useful theorems) and (b) actively not using retrieval is indicative of good model performance. Furthermore, we find that the model scales its retrieval frequency with the difficulty of the problem, reinforcing that the decision to retrieve is a crucial metacognitive signal. The agent's ability to self-assess its knowledge and selectively engage with external information represents a key principle for building more robust and reliable generative models.

</details>


### [230] [Collaborative and Efficient Fine-tuning: Leveraging Task Similarity](https://arxiv.org/abs/2602.07218)
*Gagik Magakyan,Amirhossein Reisizadeh,Chanwoo Park,Pablo A. Parrilo,Asuman Ozdaglar*

Main category: cs.LG

TL;DR: 提出CoLoRA方法利用任务相似性协作微调基础模型，理论分析和实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 缓解基础模型微调时的数据稀缺问题。

Method: 提出Collaborative Low - Rank Adaptation (CoLoRA)，训练一个共享适配器捕捉任务相似性，以及个性化适配器适应特定任务。

Result: 理论上对异构线性回归研究CoLoRA，证明能恢复真实值；自然语言实验表明相似任务一起训练可显著提升性能。

Conclusion: CoLoRA可利用任务相似性在基础模型微调中有效解决数据稀缺问题，提升性能。

Abstract: Adaptability has been regarded as a central feature in the foundation models, enabling them to effectively acclimate to unseen downstream tasks. Parameter-efficient fine-tuning methods such as celebrated LoRA facilitate efficient adaptation of large foundation models using labeled, high-quality and generally scarce task data. To mitigate data scarcity in fine-tuning of foundation models, we propose to leverage task similarity across multiple downstream users. Intuitively, users with similar tasks must be able to assist each other in boosting the effective fine-tuning data size. We propose Collaborative Low-Rank Adaptation, or CoLoRA, which exploits task similarity to collaboratively and efficiently fine-tune personalized foundation models. The main idea in CoLoRA is to train one shared adapter capturing underlying task similarities across all tasks, and personalized adapters tailored to user-specific tasks. We theoretically study CoLoRA on heterogeneous linear regression and provide provable guarantees for ground truth recovery. We also conduct several natural language experiments with varying task similarity, which further demonstrate that when trained together with similar tasks, individual performances are significantly boosted.

</details>


### [231] [Probing Neural TSP Representations for Prescriptive Decision Support](https://arxiv.org/abs/2602.07216)
*Reuben Narad,Léonard Boussioux,Michael Wagner*

Main category: cs.LG

TL;DR: 研究神经TSP求解器内部表示对下游任务的迁移能力，训练探测器在相关任务上表现良好，且集成后超过基线，发现求解器质量与迁移准确率正相关。


<details>
  <summary>Details</summary>
Motivation: 探索训练好的TSP求解器的内部表示能否迁移到其他优化相关目标，受迁移学习启发。

Method: 训练基于注意力的TSP策略，收集内部激活，在节点/边嵌入上训练探测器用于两个NP难的下游任务。

Result: 探测器在两个任务上与现有基线有竞争力，集成信号与几何特征后超过最强基线，如最佳节点移除任务准确率65%（基线58%），最差边识别任务准确率73%（基线67%）。

Conclusion: 首次研究神经TSP求解器作为可迁移编码器用于构建路线之外的决策支持目标，且求解器质量越高，下游目标的编码器越有用。

Abstract: The field of neural combinatorial optimization (NCO) trains neural policies to solve NP-hard problems such as the traveling salesperson problem (TSP). We ask whether, beyond producing good tours, a trained TSP solver learns internal representations that transfer to other optimization-relevant objectives, in the spirit of transfer learning from other domains. We train several attention-based TSP policies, collect their internal activations, and train probes on node/edge embeddings for two NP-hard prescriptive downstream tasks inspired by real-world logistics scenarios: node-removal sensitivity (identifying the most impactful node to remove) and edge-forbid sensitivity (identifying the most critical edge to retain). On a Euclidean TSP100-trained model, probes for both tasks are competitive with existing baselines. Ensembling probe signals with geometric features outperforms the strongest baselines: 65\% top-1 accuracy (vs. 58\% baseline) for the best-node-removal task, and 73\% top-1 accuracy (vs. 67\% baseline) for the worst-edge identification task. To our knowledge, we are the first to study neural TSP solvers as transferable encoders for prescriptive what-if decision-support objectives beyond tour construction. Finally, we show that transfer accuracy increases with solver quality across training and model scale, suggesting that training stronger NCO solvers also yields more useful encoders for downstream objectives. Our code is available at: github.com/ReubenNarad/tsp_prescriptive_probe

</details>


### [232] [Privately Learning Decision Lists and a Differentially Private Winnow](https://arxiv.org/abs/2602.07370)
*Mark Bun,William Fang*

Main category: cs.LG

TL;DR: 提出用于学习决策列表和大间隔半空间的差分隐私算法，在PAC和在线模型中均有成果并给出应用。


<details>
  <summary>Details</summary>
Motivation: 为经典的学习决策列表和大间隔半空间问题提供新的差分隐私算法。

Method: 在PAC模型中给出计算高效算法，在在线模型中给出Winnow算法的隐私版本。

Result: 在PAC模型学习决策列表的样本开销少；在线模型学习半空间的失误界在维度和间隔倒数上呈多项式对数关系，还能在线模型下私密学习决策列表。

Conclusion: 算法达成预期效果，定性匹配非私密的最优保障。

Abstract: We give new differentially private algorithms for the classic problems of learning decision lists and large-margin halfspaces in the PAC and online models. In the PAC model, we give a computationally efficient algorithm for learning decision lists with minimal sample overhead over the best non-private algorithms. In the online model, we give a private analog of the influential Winnow algorithm for learning halfspaces with mistake bound polylogarithmic in the dimension and inverse polynomial in the margin. As an application, we describe how to privately learn decision lists in the online model, qualitatively matching state-of-the art non-private guarantees.

</details>


### [233] [Dichotomy of Feature Learning and Unlearning: Fast-Slow Analysis on Neural Networks with Stochastic Gradient Descent](https://arxiv.org/abs/2602.07378)
*Shota Imai,Sota Nishiyama,Masaaki Imaizumi*

Main category: cs.LG

TL;DR: 本文考虑两层神经网络大批次随机梯度更新的无限宽度极限，推导不同时间尺度的微分方程揭示特征遗忘的机制和条件，给出数值验证并得到理论依据和缩放定律。


<details>
  <summary>Details</summary>
Motivation: 神经网络基于梯度训练的动力学有复杂结构，理解其颇具挑战，尤其特征遗忘现象受关注，因此要研究其机制和条件。

Method: 考虑两层神经网络无限宽度极限，推导不同时间尺度微分方程，利用快慢动力学，运用Tensor Programs和奇异摄动理论。

Result: 给出特征遗忘发生的数值验证，得到理论依据和缩放定律。发现数据主非线性项强度会引发特征遗忘，第二层权重的初始尺度可缓解特征遗忘。

Conclusion: 成功揭示特征遗忘发生的机制和条件，对理解神经网络训练动力学有重要意义。

Abstract: The dynamics of gradient-based training in neural networks often exhibit nontrivial structures; hence, understanding them remains a central challenge in theoretical machine learning. In particular, a concept of feature unlearning, in which a neural network progressively loses previously learned features over long training, has gained attention. In this study, we consider the infinite-width limit of a two-layer neural network updated with a large-batch stochastic gradient, then derive differential equations with different time scales, revealing the mechanism and conditions for feature unlearning to occur. Specifically, we utilize the fast-slow dynamics: while an alignment of first-layer weights develops rapidly, the second-layer weights develop slowly. The direction of a flow on a critical manifold, determined by the slow dynamics, decides whether feature unlearning occurs. We give numerical validation of the result, and derive theoretical grounding and scaling laws of the feature unlearning. Our results yield the following insights: (i) the strength of the primary nonlinear term in data induces the feature unlearning, and (ii) an initial scale of the second-layer weights mitigates the feature unlearning. Technically, our analysis utilizes Tensor Programs and the singular perturbation theory.

</details>


### [234] [The Median is Easier than it Looks: Approximation with a Constant-Depth, Linear-Width ReLU Network](https://arxiv.org/abs/2602.07219)
*Abhigyan Dutta,Itay Safran,Paul Valiant*

Main category: cs.LG

TL;DR: 研究用ReLU神经网络近似d个输入的中位数，给出深度 - 宽度权衡，构造出常数深度、线性宽度且近似误差指数级小的网络，突破最大函数研究障碍。


<details>
  <summary>Details</summary>
Motivation: 研究用ReLU神经网络近似d个输入的中位数，突破此前最大函数研究的障碍。

Method: 采用多阶段过程迭代消除非中心元素，同时保留中位数附近的候选集；建立从最大值到中位数的一般归约。

Result: 得到常数深度、线性宽度的构造，实现了相对于单位超立方体上均匀分布的指数级小近似误差；突破了最大函数研究中线性宽度需深度至少为log log d才能达到可比精度的障碍。

Conclusion: 得到比此前最大函数研究更强的近似结果。

Abstract: We study the approximation of the median of $d$ inputs using ReLU neural networks. We present depth-width tradeoffs under several settings, culminating in a constant-depth, linear-width construction that achieves exponentially small approximation error with respect to the uniform distribution over the unit hypercube. By further establishing a general reduction from the maximum to the median, our results break a barrier suggested by prior work on the maximum function, which indicated that linear width should require depth growing at least as $\log\log d$ to achieve comparable accuracy. Our construction relies on a multi-stage procedure that iteratively eliminates non-central elements while preserving a candidate set around the median. We overcome obstacles that do not arise for the maximum to yield approximation results that are strictly stronger than those previously known for the maximum itself.

</details>


### [235] [Achieving Optimal Static and Dynamic Regret Simultaneously in Bandits with Deterministic Losses](https://arxiv.org/abs/2602.07418)
*Jian Qian,Chen-Yu Wei*

Main category: cs.LG

TL;DR: 本文探讨对抗多臂老虎机问题，证明在确定性损失下对遗忘对手可同时实现最优静态和动态遗憾，提出算法并揭示自适应和遗忘对手间的差异。


<details>
  <summary>Details</summary>
Motivation: 现有算法无法同时对静态和动态遗憾达到最优边界，研究在确定性损失下对遗忘对手同时实现最优的可能性。

Method: 扩展已有不可能结果，提出利用负静态遗憾补偿动态遗憾探索开销、借助Blackwell可接近性联合控制两种遗憾的算法。

Result: 证明在确定性损失下对遗忘对手可同时实现最优静态和动态遗憾，提出的算法有效。

Conclusion: 揭示了考虑多个遗憾基准时自适应和遗忘对手的根本差异，为解决不同切换基准的最优遗憾问题提供新见解，还得到新的多臂老虎机模型选择程序。

Abstract: In adversarial multi-armed bandits, two performance measures are commonly used: static regret, which compares the learner to the best fixed arm, and dynamic regret, which compares it to the best sequence of arms. While optimal algorithms are known for each measure individually, there is no known algorithm achieving optimal bounds for both simultaneously. Marinov and Zimmert [2021] first showed that such simultaneous optimality is impossible against an adaptive adversary. Our work takes a first step to demonstrate its possibility against an oblivious adversary when losses are deterministic. First, we extend the impossibility result of Marinov and Zimmert [2021] to the case of deterministic losses. Then, we present an algorithm achieving optimal static and dynamic regret simultaneously against an oblivious adversary. Together, they reveal a fundamental separation between adaptive and oblivious adversaries when multiple regret benchmarks are considered simultaneously. It also provides new insight into the long open problem of simultaneously achieving optimal regret against switching benchmarks of different numbers of switches.
  Our algorithm uses negative static regret to compensate for the exploration overhead incurred when controlling dynamic regret, and leverages Blackwell approachability to jointly control both regrets. This yields a new model selection procedure for bandits that may be of independent interest.

</details>


### [236] [SpecAttn: Co-Designing Sparse Attention with Self-Speculative Decoding](https://arxiv.org/abs/2602.07223)
*Yikang Yue,Yuqi Xue,Jian Huang*

Main category: cs.LG

TL;DR: 针对长上下文大语言模型推理受KV缓存内存需求瓶颈的问题，提出SpecAttn方法，通过验证指导的稀疏注意力机制，提高解码吞吐量。


<details>
  <summary>Details</summary>
Motivation: 长上下文大语言模型推理受KV缓存内存需求制约，以往自推测解码方法依赖独立KV选择算法且忽略验证时的KV重要性计算。

Method: 提出SpecAttn，将验证过程中识别关键KV条目，并在生成后续令牌时仅加载这些条目。

Result: SpecAttn比普通自回归解码吞吐量高2.81倍，比现有基于稀疏性的自推测解码方法提升1.29倍。

Conclusion: SpecAttn方法提高了草稿令牌接受率，降低KV选择开销，有效提高了解码吞吐量。

Abstract: Long-context large language model (LLM) inference has become the norm for today's AI applications. However, it is severely bottlenecked by the increasing memory demands of its KV cache. Previous works have shown that self-speculative decoding with sparse attention, where tokens are drafted using a subset of the KV cache and verified in parallel with full KV cache, speeds up inference in a lossless way. However, this approach relies on standalone KV selection algorithms to select the KV entries used for drafting and overlooks that the criticality of each KV entry is inherently computed during verification. In this paper, we propose SpecAttn, a self-speculative decoding method with verification-guided sparse attention. SpecAttn identifies critical KV entries as a byproduct of verification and only loads these entries when drafting subsequent tokens. This not only improves draft token acceptance rate but also incurs low KV selection overhead, thereby improving decoding throughput. SpecAttn achieves 2.81$\times$ higher throughput over vanilla auto-regressive decoding and 1.29$\times$ improvement over state-of-the-art sparsity-based self-speculative decoding methods.

</details>


### [237] [Data-Aware and Scalable Sensitivity Analysis for Decision Tree Ensembles](https://arxiv.org/abs/2602.07453)
*Namrita Varshney,Ashutosh Gupta,Arhaan Ahmad,Tanay V. Tayal,S. Akshay*

Main category: cs.LG

TL;DR: 研究决策树集成特征敏感性问题，提出数据感知敏感性框架，有多项贡献并实验验证效果，为树模型可靠性和公平性分析提供基础。


<details>
  <summary>Details</summary>
Motivation: 决策树集成广泛用于关键领域，其鲁棒性和敏感性分析对可信度至关重要，现有方法产生的敏感示例远离训练分布，缺乏可解释性和实用价值。

Method: 提出数据感知敏感性框架，结合混合整数线性规划（MILP）和可满足性模理论（SMT）编码进行数据感知搜索。

Result: 加强敏感性验证的NP难结果；开发MILP优化加速单集成和多类树集成的敏感性验证；引入生成接近训练分布现实示例的框架；实验证明可扩展到800棵深度为8的树集成，优于现有技术。

Conclusion: 该框架为高风险应用中基于树的模型的可靠性和公平性分析提供了实用基础。

Abstract: Decision tree ensembles are widely used in critical domains, making robustness and sensitivity analysis essential to their trustworthiness. We study the feature sensitivity problem, which asks whether an ensemble is sensitive to a specified subset of features -- such as protected attributes -- whose manipulation can alter model predictions. Existing approaches often yield examples of sensitivity that lie far from the training distribution, limiting their interpretability and practical value. We propose a data-aware sensitivity framework that constrains the sensitive examples to remain close to the dataset, thereby producing realistic and interpretable evidence of model weaknesses. To this end, we develop novel techniques for data-aware search using a combination of mixed-integer linear programming (MILP) and satisfiability modulo theories (SMT) encodings. Our contributions are fourfold. First, we strengthen the NP-hardness result for sensitivity verification, showing it holds even for trees of depth 1. Second, we develop MILP-optimizations that significantly speed up sensitivity verification for single ensembles and for the first time can also handle multiclass tree ensembles. Third, we introduce a data-aware framework generating realistic examples close to the training distribution. Finally, we conduct an extensive experimental evaluation on large tree ensembles, demonstrating scalability to ensembles with up to 800 trees of depth 8, achieving substantial improvements over the state of the art. This framework provides a practical foundation for analyzing the reliability and fairness of tree-based models in high-stakes applications.

</details>


### [238] [Fault-Tolerant Evaluation for Sample-Efficient Model Performance Estimators](https://arxiv.org/abs/2602.07226)
*Zihan Zhu,Yanqiu Wu,Qiongkai Xu*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In the era of Model-as-a-Service, organizations increasingly rely on third-party AI models for rapid deployment. However, the dynamic nature of emerging AI applications, the continual introduction of new datasets, and the growing number of models claiming superior performance make efficient and reliable validation of model services increasingly challenging. This motivates the development of sample-efficient performance estimators, which aim to estimate model performance by strategically selecting instances for labeling, thereby reducing annotation cost. Yet existing evaluation approaches often fail in low-variance settings: RMSE conflates bias and variance, masking persistent bias when variance is small, while p-value based tests become hypersensitive, rejecting adequate estimators for negligible deviations. To address this, we propose a fault-tolerant evaluation framework that integrates bias and variance considerations within an adjustable tolerance level ${\varepsilon}$, enabling the evaluation of performance estimators within practically acceptable error margins. We theoretically show that proper calibration of ${\varepsilon}$ ensures reliable evaluation across different variance regimes, and we further propose an algorithm that automatically optimizes and selects ${\varepsilon}$. Experiments on real-world datasets demonstrate that our framework provides comprehensive and actionable insights into estimator behavior.

</details>


### [239] [Bandit Allocational Instability](https://arxiv.org/abs/2602.07472)
*Yilun Chen,Jiaqi Lu*

Main category: cs.LG

TL;DR: 本文引入多臂老虎机算法新性能指标分配变异性，建立其与后悔值的权衡关系，给出下界并证明可由UCB - f算法实现，还讨论了应用影响并解决一个开放问题。


<details>
  <summary>Details</summary>
Motivation: 现有多臂老虎机算法分配结果变异大，对现代应用有害，因此引入分配变异性指标。

Method: 理论推导建立分配变异性和后悔值的权衡关系，提出UCB - f算法。

Result: 得到最坏情况后悔值和分配变异性的关系 $R_T \cdot S_T=Ω(T^{\frac{3}{2}})$，证明下界基本是紧的，Pareto前沿上的点可由UCB - f实现。

Conclusion: 明确了分配变异性和后悔值的权衡，UCB - f算法可实现权衡，讨论了应用影响并解决开放问题。

Abstract: When multi-armed bandit (MAB) algorithms allocate pulls among competing arms, the resulting allocation can exhibit huge variation. This is particularly harmful in modern applications such as learning-enhanced platform operations and post-bandit statistical inference. Thus motivated, we introduce a new performance metric of MAB algorithms termed allocation variability, which is the largest (over arms) standard deviation of an arm's number of pulls. We establish a fundamental trade-off between allocation variability and regret, the canonical performance metric of reward maximization. In particular, for any algorithm, the worst-case regret $R_T$ and worst-case allocation variability $S_T$ must satisfy $R_T \cdot S_T=Ω(T^{\frac{3}{2}})$ as $T\rightarrow\infty$, as long as $R_T=o(T)$. This indicates that any minimax regret-optimal algorithm must incur worst-case allocation variability $Θ(T)$, the largest possible scale; while any algorithm with sublinear worst-case regret must necessarily incur ${S}_T= ω(\sqrt{T})$. We further show that this lower bound is essentially tight, and that any point on the Pareto frontier $R_T \cdot S_T=\tildeΘ(T^{3/2})$ can be achieved by a simple tunable algorithm UCB-f, a generalization of the classic UCB1. Finally, we discuss implications for platform operations and for statistical inference, when bandit algorithms are used. As a byproduct of our result, we resolve an open question of Praharaj and Khamaru (2025).

</details>


### [240] [Cerebellar-Inspired Residual Control for Fault Recovery: From Inference-Time Adaptation to Structural Consolidation](https://arxiv.org/abs/2602.07227)
*Nethmi Jayasinghe,Diana Gontero,Spencer T. Brown,Vinod K. Sangwan,Mark C. Hersam,Amit Ranjan Trivedi*

Main category: cs.LG

TL;DR: 介绍了受小脑启发的推理时残差控制框架，可在不修改基础策略参数的情况下实现故障恢复，实验展示了良好性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界中机器人策略在训练后常出现故障，而重新训练等方法不可行。

Method: 引入受小脑启发的残差控制框架，包含固定特征扩展、并行微区式残差路径、局部误差驱动可塑性等机制，有保守的元适应调节。

Result: 在MuJoCo基准测试中，中度故障下HalfCheetah - v5提升66%，Humanoid - v5提升53%，严重故障时性能逐渐下降，还可将残差校正整合到策略参数中增强鲁棒性。

Conclusion: 该框架能在不修改基础参数下实现故障恢复，且表现良好。

Abstract: Robotic policies deployed in real-world environments often encounter post-training faults, where retraining, exploration, or system identification are impractical. We introduce an inference-time, cerebellar-inspired residual control framework that augments a frozen reinforcement learning policy with online corrective actions, enabling fault recovery without modifying base policy parameters. The framework instantiates core cerebellar principles, including high-dimensional pattern separation via fixed feature expansion, parallel microzone-style residual pathways, and local error-driven plasticity with excitatory and inhibitory eligibility traces operating at distinct time scales. These mechanisms enable fast, localized correction under post-training disturbances while avoiding destabilizing global policy updates. A conservative, performance-driven meta-adaptation regulates residual authority and plasticity, preserving nominal behavior and suppressing unnecessary intervention. Experiments on MuJoCo benchmarks under actuator, dynamic, and environmental perturbations show improvements of up to $+66\%$ on \texttt{HalfCheetah-v5} and $+53\%$ on \texttt{Humanoid-v5} under moderate faults, with graceful degradation under severe shifts and complementary robustness from consolidating persistent residual corrections into policy parameters.

</details>


### [241] [ArcMark: Multi-bit LLM Watermark via Optimal Transport](https://arxiv.org/abs/2602.07235)
*Atefeh Gilani,Carol Xuan Long,Sajani Vithana,Oliver Kosut,Lalitha Sankar,Flavio P. Calmon*

Main category: cs.LG

TL;DR: 本文推导了多比特水印的容量表征，设计了ArcMark水印，在比特率和检测准确率上表现优于竞品，表明语言模型水印是信道编码问题。


<details>
  <summary>Details</summary>
Motivation: 现有多比特水印大多基于零比特水印设计原则，多比特水印的信息论容量未知。

Method: 推导多比特水印的容量表征，基于编码理论原则设计ArcMark水印。

Result: ArcMark在每令牌比特率和检测准确性方面优于现有的多比特水印。

Conclusion: 语言模型水印本质上是一个信道编码问题，为水印设计的编码理论方法铺平了道路。

Abstract: Watermarking is an important tool for promoting the responsible use of language models (LMs). Existing watermarks insert a signal into generated tokens that either flags LM-generated text (zero-bit watermarking) or encodes more complex messages (multi-bit watermarking). Though a number of recent multi-bit watermarks insert several bits into text without perturbing average next-token predictions, they largely extend design principles from the zero-bit setting, such as encoding a single bit per token. Notably, the information-theoretic capacity of multi-bit watermarking -- the maximum number of bits per token that can be inserted and detected without changing average next-token predictions -- has remained unknown. We address this gap by deriving the first capacity characterization of multi-bit watermarks. Our results inform the design of ArcMark: a new watermark construction based on coding-theoretic principles that, under certain assumptions, achieves the capacity of the multi-bit watermark channel. In practice, ArcMark outperforms competing multi-bit watermarks in terms of bit rate per token and detection accuracy. Our work demonstrates that LM watermarking is fundamentally a channel coding problem, paving the way for principled coding-theoretic approaches to watermark design.

</details>


### [242] [Deriving Neural Scaling Laws from the statistics of natural language](https://arxiv.org/abs/2602.07488)
*Francesco Cagnetta,Allan Raventós,Surya Ganguli,Matthieu Wyart*

Main category: cs.LG

TL;DR: 本文提出首个针对数据受限缩放定律的理论，通过语言的两个统计特性预测神经缩放指数，公式与实验结果匹配良好。


<details>
  <summary>Details</summary>
Motivation: 现有理论无法定量预测在自然语言数据集上训练的语言模型的神经缩放定律指数，因此作者提出相关理论。

Method: 分离出语言的两个关键统计特性，推导出无自由参数和合成数据模型的公式来预测数据受限神经缩放指数。

Result: 理论预测结果与在TinyStories和WikiText两个基准上从零开始训练GPT - 2和LLaMA风格模型得到的实验测量结果显著匹配。

Conclusion: 所提出的理论能够有效预测数据受限情况下的神经缩放指数。

Abstract: Despite the fact that experimental neural scaling laws have substantially guided empirical progress in large-scale machine learning, no existing theory can quantitatively predict the exponents of these important laws for any modern LLM trained on any natural language dataset. We provide the first such theory in the case of data-limited scaling laws. We isolate two key statistical properties of language that alone can predict neural scaling exponents: (i) the decay of pairwise token correlations with time separation between token pairs, and (ii) the decay of the next-token conditional entropy with the length of the conditioning context. We further derive a simple formula in terms of these statistics that predicts data-limited neural scaling exponents from first principles without any free parameters or synthetic data models. Our theory exhibits a remarkable match with experimentally measured neural scaling laws obtained from training GPT-2 and LLaMA style models from scratch on two qualitatively different benchmarks, TinyStories and WikiText.

</details>


### [243] [Graph homophily booster: Reimagining the role of discrete features in heterophilic graph learning](https://arxiv.org/abs/2602.07256)
*Ruizhong Qiu,Ting-Wei Li,Gaotang Li,Hanghang Tong*

Main category: cs.LG

TL;DR: 现有GNN在异质性图上表现不佳，本文提出GRAPHITE框架，通过图变换增加图同质性，实验表明其在异质性图上显著优于现有方法，在同质性图上与现有方法相当。


<details>
  <summary>Details</summary>
Motivation: 现有处理图异质性问题的方法主要关注架构设计，未直接针对异质性根源，在挑战性异质性数据集上表现不如简单的MLP，需要创新方法。

Method: 提出GRAPHITE框架，通过精心设计的图变换直接增加图同质性，创建特征节点促进特征相似节点间的同质性消息传递。

Result: 理论和实证表明GRAPHITE显著增加了原本异质性图的同质性，图大小仅有轻微增加，在挑战性数据集上的实验中显著优于现有的异质性图处理方法，在同质性图上与现有方法准确性相当。

Conclusion: GRAPHITE是一种有效解决图异质性问题的新范式，通过图变换能提升图的同质性并在不同类型图上取得良好效果。

Abstract: Graph neural networks (GNNs) have emerged as a powerful tool for modeling graph-structured data. However, existing GNNs often struggle with heterophilic graphs, where connected nodes tend to have dissimilar features or labels. While numerous methods have been proposed to address this challenge, they primarily focus on architectural designs without directly targeting the root cause of the heterophily problem. These approaches still perform even worse than the simplest MLPs on challenging heterophilic datasets. For instance, our experiments show that 21 latest GNNs still fall behind the MLP on the Actor dataset. This critical challenge calls for an innovative approach to addressing graph heterophily beyond architectural designs. To bridge this gap, we propose and study a new and unexplored paradigm: directly increasing the graph homophily via a carefully designed graph transformation. In this work, we present a simple yet effective framework called GRAPHITE to address graph heterophily. To the best of our knowledge, this work is the first method that explicitly transforms the graph to directly improve the graph homophily. Stemmed from the exact definition of homophily, our proposed GRAPHITE creates feature nodes to facilitate homophilic message passing between nodes that share similar features. Furthermore, we both theoretically and empirically show that our proposed GRAPHITE significantly increases the homophily of originally heterophilic graphs, with only a slight increase in the graph size. Extensive experiments on challenging datasets demonstrate that our proposed GRAPHITE significantly outperforms state-of-the-art methods on heterophilic graphs while achieving comparable accuracy with state-of-the-art methods on homophilic graphs.

</details>


### [244] [Gaussian Match-and-Copy: A Minimalist Benchmark for Studying Transformer Induction](https://arxiv.org/abs/2602.07562)
*Antoine Gonon,Alexandre Cordonnier,Nicolas Boumal*

Main category: cs.LG

TL;DR: 引入高斯匹配复制（GMC）基准，分离检索和记忆，研究Transformer匹配复制电路及优化动态，证明梯度下降参数的最大间隔对齐。


<details>
  <summary>Details</summary>
Motivation: 解决自然数据中匹配复制行为出现时检索和记忆纠缠难以理解的问题。

Method: 引入GMC基准，进行数值研究，分析简化注意力设置下的优化动态。

Result: GMC任务保留Transformer匹配复制电路关键特征，分离架构检索能力，确定梯度下降隐式偏差机制。

Conclusion: 在明确技术条件下，达到零经验损失的梯度下降轨迹能实现最大间隔对齐。

Abstract: Match-and-copy is a core retrieval primitive used at inference time by large language models to retrieve a matching token from the context then copy its successor. Yet, understanding how this behavior emerges on natural data is challenging because retrieval and memorization are entangled. To disentangle the two, we introduce Gaussian Match-and-Copy (GMC), a minimalist benchmark that isolates long-range retrieval through pure second-order correlation signals. Numerical investigations show that this task retains key qualitative aspects of how Transformers develop match-and-copy circuits in practice, and separates architectures by their retrieval capabilities. We also analyze the optimization dynamics in a simplified attention setting. Although many solutions are a priori possible under a regression objective, including ones that do not implement retrieval, we identify an implicit-bias regime in which gradient descent drives the parameters to diverge while their direction aligns with the max-margin separator, yielding hard match selection. We prove this max-margin alignment for GD trajectories that reach vanishing empirical loss under explicit technical conditions.

</details>


### [245] [Robust Ultra-High-Dimensional Variable Selection With Correlated Structure Using Group Testing](https://arxiv.org/abs/2602.07258)
*Wanru Guo,Juan Xie,Binbin Wang,Weicong Chen,Xiaoyi Lu,Vipin Chaudhary,Curtis Tatsuoka*

Main category: cs.LG

TL;DR: 针对高维基因组数据特点提出 Dorfman 筛选框架用于特征选择，在模拟和实际数据中表现良好，Robust - OGK - Dorfman - Adaptive - EN 表现突出。


<details>
  <summary>Details</summary>
Motivation: 高维基因组数据存在强组相关性结构，传统特征选择方法假设特征独立或依赖预定义路径，且对异常值和模型误设敏感，需更好方法。

Method: 提出 Dorfman 筛选框架，通过分层聚类形成数据驱动的变量组，进行组和组内假设检验，用弹性网或自适应弹性网改进选择；鲁棒变体采用基于 OGK 的协方差估计等处理异常数据。

Result: 模拟中，Dorfman - Sparse - Adaptive - EN 在正常条件下最好，Robust - OGK - Dorfman - Adaptive - EN 在数据污染时优势明显；应用于 NSCLC 基因表达数据，鲁棒 Dorfman 方法预测误差最低且富集临床相关基因恢复。

Conclusion: Dorfman 框架是一种高效且鲁棒的基因组特征选择方法，Robust - OGK - Dorfman - Adaptive - EN 适用于理想和受污染条件及超高维情况，适合现代基因组生物标志物发现。

Abstract: Background: High-dimensional genomic data exhibit strong group correlation structures that challenge conventional feature selection methods, which often assume feature independence or rely on pre-defined pathways and are sensitive to outliers and model misspecification.
  Methods: We propose the Dorfman screening framework, a multi-stage procedure that forms data-driven variable groups via hierarchical clustering, performs group and within-group hypothesis testing, and refines selection using elastic net or adaptive elastic net. Robust variants incorporate OGK-based covariance estimation, rank-based correlation, and Huber-weighted regression to handle contaminated and non-normal data.
  Results: In simulations, Dorfman-Sparse-Adaptive-EN performed best under normal conditions, while Robust-OGK-Dorfman-Adaptive-EN showed clear advantages under data contamination, outperforming classical Dorfman and competing methods. Applied to NSCLC gene expression data for trametinib response, robust Dorfman methods achieved the lowest prediction errors and enriched recovery of clinically relevant genes.
  Conclusions: The Dorfman framework provides an efficient and robust approach to genomic feature selection. Robust-OGK-Dorfman-Adaptive-EN offers strong performance under both ideal and contaminated conditions and scales to ultra-high-dimensional settings, making it well suited for modern genomic biomarker discovery.

</details>


### [246] [tLoRA: Efficient Multi-LoRA Training with Elastic Shared Super-Models](https://arxiv.org/abs/2602.07263)
*Kevin Li,Dibyadeep Saha,Avni Kanodia,Fan Lai*

Main category: cs.LG

TL;DR: 提出tLoRA框架实现多个LoRA作业的高效批量训练，评估显示有显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有大规模语言模型的LoRA训练作业并发执行时，高效训练阶段的异构LoRA适配器共定位存在独特挑战，简单批处理会带来诸多问题。

Method: 将共享同一基础模型的适配器融合为弹性共享超级模型；采用融合的LoRA内核，自适应重建低秩计算块并调度秩感知的纳米批次；在调度层采用在线、剩余容量感知的调度器。

Result: 使用真实集群跟踪评估表明，tLoRA将训练吞吐量提高1.2 - 1.8倍，作业训练完成时间缩短2.3 - 5.4倍，GPU利用率提高37%。

Conclusion: tLoRA框架能够有效实现多个LoRA作业的高效批量训练，提升训练性能。

Abstract: As Low-Rank Adaptation (LoRA) becomes the standard approach for efficiently fine-tuning large language models (LLMs), shared clusters increasingly execute many concurrent LoRA training jobs over the same frozen backbone. While recent advances enable batching (co-locating) multiple adapters during serving, efficient training-time co-location of heterogeneous LoRA adapters presents unique challenges. Jobs often differ in adapter rank, batch size, and resource allocation, and naïve batching can introduce synchronization stalls, communication overheads, and per-job slowdowns that are worse than executing independently. We introduce tLoRA, a framework that enables efficient batch training of multiple LoRA jobs. tLoRA fuses adapters that share the same base model into an elastic shared super-model, exploiting existing distributed training frameworks to derive parallelism plans that share resources effectively. At the kernel level, tLoRA employs a fused LoRA kernel that adaptively reconstructs low-rank computation tiles and schedules rank-aware nano-batches to maximize overlap between computation and communication across adapters. At the scheduling layer, tLoRA incorporates an online, residual-capacity-aware scheduler that adaptively groups jobs to maximize collective throughput. Evaluations using real-world cluster traces demonstrate that tLoRA improves training throughput by 1.2--1.8x, job training completion time by 2.3--5.4x, and GPU utilization by 37%.

</details>


### [247] [XShare: Collaborative in-Batch Expert Sharing for Faster MoE Inference](https://arxiv.org/abs/2602.07265)
*Daniil Vankov,Nikita Ivkin,Kyle Ulrich,Xiang Song,Ashish Khetan,George Karypis*

Main category: cs.LG

TL;DR: 针对Mixture-of-Experts架构在生产推理中专家激活问题，提出XShare方法减少专家激活、降低GPU负载并提升吞吐量。


<details>
  <summary>Details</summary>
Motivation: Mixture-of-Experts架构在生产推理时，请求批处理和推测解码会放大专家激活，侵蚀效率收益。

Method: 将批量感知的专家选择建模为模块化优化问题，设计高效贪心算法，提出XShare方法，无需重新训练，动态适应每个批次。

Result: 在标准批处理下减少专家激活达30%，在专家并行部署中降低峰值GPU负载达3倍，在推测解码中实现高达14%的吞吐量提升。

Conclusion: XShare方法有效解决了Mixture-of-Experts架构在生产推理中的效率问题。

Abstract: Mixture-of-Experts (MoE) architectures are increasingly used to efficiently scale large language models. However, in production inference, request batching and speculative decoding significantly amplify expert activation, eroding these efficiency benefits. We address this issue by modeling batch-aware expert selection as a modular optimization problem and designing efficient greedy algorithms for different deployment settings. The proposed method, namely XShare, requires no retraining and dynamically adapts to each batch by maximizing the total gating score of selected experts. It reduces expert activation by up to 30% under standard batching, cuts peak GPU load by up to 3x in expert-parallel deployments, and achieves up to 14% throughput gains in speculative decoding via hierarchical, correlation-aware expert selection even if requests in a batch drawn from heterogeneous datasets.

</details>


### [248] [Dense Neural Networks are not Universal Approximators](https://arxiv.org/abs/2602.07618)
*Levi Rauchwerger,Stefanie Jegelka,Ron Levie*

Main category: cs.LG

TL;DR: 研究密集神经网络近似能力，指出其无普遍近似性，凸显其内在局限，建议用稀疏连接实现普遍性。


<details>
  <summary>Details</summary>
Motivation: 探究密集神经网络的近似能力，了解其在有约束条件下是否具备普遍近似性。

Method: 采用模型压缩方法，结合弱正则引理，将前馈网络解释为消息传递图神经网络。考虑有自然权重约束及输入输出维度约束的ReLU神经网络。

Result: 证明存在无法被此类网络近似的Lipschitz连续函数。

Conclusion: 密集神经网络存在内在局限，使用稀疏连接是实现真正普遍性的必要因素。

Abstract: We investigate the approximation capabilities of dense neural networks. While universal approximation theorems establish that sufficiently large architectures can approximate arbitrary continuous functions if there are no restrictions on the weight values, we show that dense neural networks do not possess this universality. Our argument is based on a model compression approach, combining the weak regularity lemma with an interpretation of feedforward networks as message passing graph neural networks. We consider ReLU neural networks subject to natural constraints on weights and input and output dimensions, which model a notion of dense connectivity. Within this setting, we demonstrate the existence of Lipschitz continuous functions that cannot be approximated by such networks. This highlights intrinsic limitations of neural networks with dense layers and motivates the use of sparse connectivity as a necessary ingredient for achieving true universality.

</details>


### [249] [Hybrid Feedback-Guided Optimal Learning for Wireless Interactive Panoramic Scene Delivery](https://arxiv.org/abs/2602.07273)
*Xiaoyi Wu,Juaren Steiger,Bin Li,R. Srikant*

Main category: cs.LG

TL;DR: 针对沉浸式应用需求，提出结合全信息和多臂老虎机反馈的两级混合反馈模型及AdaPort算法，提升学习效率，优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有工作将问题建模为两级多臂老虎机反馈，未充分利用预测反馈可事后全量计算的特性，导致效率不高。

Method: 引入两级混合反馈模型，将部分选择问题建模为在线学习任务，推导混合模型的后悔值下界，提出AdaPort混合学习算法。

Result: 建立与下界渐近匹配的后悔值上界，通过真实轨迹驱动模拟证明AdaPort优于现有基线方法。

Conclusion: 所提出的两级混合反馈模型和AdaPort算法能有效提升学习效率，解决沉浸式应用中的部分选择问题。

Abstract: Immersive applications such as virtual and augmented reality impose stringent requirements on frame rate, latency, and synchronization between physical and virtual environments. To meet these requirements, an edge server must render panoramic content, predict user head motion, and transmit a portion of the scene that is large enough to cover the user viewport while remaining within wireless bandwidth constraints. Each portion produces two feedback signals: prediction feedback, indicating whether the selected portion covers the actual viewport, and transmission feedback, indicating whether the corresponding packets are successfully delivered. Prior work models this problem as a multi-armed bandit with two-level bandit feedback, but fails to exploit the fact that prediction feedback can be retrospectively computed for all candidate portions once the user head pose is observed. As a result, prediction feedback constitutes full-information feedback rather than bandit feedback. Motivated by this observation, we introduce a two-level hybrid feedback model that combines full-information and bandit feedback, and formulate the portion selection problem as an online learning task under this setting. We derive an instance-dependent regret lower bound for the hybrid feedback model and propose AdaPort, a hybrid learning algorithm that leverages both feedback types to improve learning efficiency. We further establish an instance-dependent regret upper bound that matches the lower bound asymptotically, and demonstrate through real-world trace driven simulations that AdaPort consistently outperforms state-of-the-art baseline methods.

</details>


### [250] [Laplacian-LoRA: Delaying Oversmoothing in Deep GCNs via Spectral Low-Rank Adaptation](https://arxiv.org/abs/2602.07278)
*Sai Vamsi Alisetti*

Main category: cs.LG

TL;DR: 提出 Laplacian - LoRA 方法缓解深度图卷积网络过平滑问题，能延迟过平滑出现，扩展有效深度。


<details>
  <summary>Details</summary>
Motivation: 深度图卷积网络存在过平滑问题，已有方法未明确揭示过平滑的底层谱原因。

Method: 提出 Laplacian - LoRA，对固定拉普拉斯传播算子引入可学习、谱锚定的修正，选择性弱化收缩同时保持稳定性和低通归纳偏置。

Result: 在多个基准数据集和不同深度下，Laplacian - LoRA 持续延迟过平滑出现，将 GCN 有效深度最多扩展两倍；嵌入方差诊断和频谱分析表明其效果。

Conclusion: 过平滑是深度依赖的谱现象，可以通过对图传播算子进行适度低秩调整来系统延迟。

Abstract: Oversmoothing is a fundamental limitation of deep graph convolutional networks (GCNs), causing node representations to collapse as depth increases. While many prior approaches mitigate this effect through architectural modifications or residual mechanisms, the underlying spectral cause of oversmoothing is often left implicit. We propose Laplacian-LoRA, a simple and interpretable low-rank spectral adaptation of standard GCNs. Rather than redesigning message passing, Laplacian-LoRA introduces a learnable, spectrally anchored correction to the fixed Laplacian propagation operator, selectively weakening contraction while preserving stability and the low-pass inductive bias. Across multiple benchmark datasets and depths, Laplacian-LoRA consistently delays the onset of oversmoothing, extending the effective depth of GCNs by up to a factor of two. Embedding variance diagnostics confirm that these gains arise from delayed representational collapse, while learned spectral analysis demonstrates that the correction is smooth, bounded, and well behaved. Our results show that oversmoothing is a depth-dependent spectral phenomenon that can be systematically delayed through modest, low-rank adaptation of the graph propagation operator.

</details>


### [251] [CausalCompass: Evaluating the Robustness of Time-Series Causal Discovery in Misspecified Scenarios](https://arxiv.org/abs/2602.07915)
*Huiyang Yi,Xiaojian Shen,Yonggang Wu,Duxin Chen,He Wang,Wenwu Yu*

Main category: cs.LG

TL;DR: 提出CausalCompass基准套件评估时间序列因果发现方法在假设违背下的鲁棒性，实验表明无单一方法在所有设置中表现最优，深度学习方法总体表现较好，还分析超参数敏感性等。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列因果发现因依赖不可测试的因果假设和缺乏鲁棒性评估，阻碍其广泛应用。

Method: 提出CausalCompass基准套件，对代表性时间序列因果发现算法在八种假设违背场景下进行基准测试，还进行超参数敏感性分析。

Result: 无单一方法在所有设置中表现最优，深度学习方法总体表现较好，NTS - NOTEARS依赖标准化预处理。

Conclusion: 为时间序列因果发现方法在假设违背下提供全面系统评估，便于其在现实应用中广泛采用。

Abstract: Causal discovery from time series is a fundamental task in machine learning. However, its widespread adoption is hindered by a reliance on untestable causal assumptions and by the lack of robustness-oriented evaluation in existing benchmarks. To address these challenges, we propose CausalCompass, a flexible and extensible benchmark suite designed to assess the robustness of time-series causal discovery (TSCD) methods under violations of modeling assumptions. To demonstrate the practical utility of CausalCompass, we conduct extensive benchmarking of representative TSCD algorithms across eight assumption-violation scenarios. Our experimental results indicate that no single method consistently attains optimal performance across all settings. Nevertheless, the methods exhibiting superior overall performance across diverse scenarios are almost invariably deep learning-based approaches. We further provide hyperparameter sensitivity analyses to deepen the understanding of these findings. We also find, somewhat surprisingly, that NTS-NOTEARS relies heavily on standardized preprocessing in practice, performing poorly in the vanilla setting but exhibiting strong performance after standardization. Finally, our work aims to provide a comprehensive and systematic evaluation of TSCD methods under assumption violations, thereby facilitating their broader adoption in real-world applications. The code and datasets are available at https://github.com/huiyang-yi/CausalCompass.

</details>


### [252] [VertCoHiRF: Decentralized Vertical Clustering Beyond k-means](https://arxiv.org/abs/2602.07279)
*Bruno Belucci,Karim Lounici,Vladimir R. Kostic,Katia Meziani*

Main category: cs.LG

TL;DR: 本文介绍了用于垂直联邦聚类的全去中心化框架VertCoHiRF，通过结构共识实现聚类，具有隐私性和跨视图一致性，实验证明性能良好。


<details>
  <summary>Details</summary>
Motivation: 现有垂直联邦学习方法应用于聚类时受限，需集中协调或交换依赖特征的数值统计，在异构视图或对抗行为下鲁棒性有限。

Method: 引入VertCoHiRF框架，各代理独立聚类本地视图，通过标识符级的共识机制，采用去中心化序数排名选择代表性中心点，逐步形成共享的层次聚类。

Result: 通信仅涉及样本标识符、聚类标签和序数排名，可保证隐私、支持重叠特征分区和异构局部聚类方法，得到可解释的共享聚类融合层次结构。

Conclusion: 分析了通信复杂性和鲁棒性，实验表明在垂直联邦设置中具有有竞争力的聚类性能。

Abstract: Vertical Federated Learning (VFL) enables collaborative analysis across parties holding complementary feature views of the same samples, yet existing approaches are largely restricted to distributed variants of $k$-means, requiring centralized coordination or the exchange of feature-dependent numerical statistics, and exhibiting limited robustness under heterogeneous views or adversarial behavior. We introduce VertCoHiRF, a fully decentralized framework for vertical federated clustering based on structural consensus across heterogeneous views, allowing each agent to apply a base clustering method adapted to its local feature space in a peer-to-peer manner. Rather than exchanging feature-dependent statistics or relying on noise injection for privacy, agents cluster their local views independently and reconcile their proposals through identifier-level consensus. Consensus is achieved via decentralized ordinal ranking to select representative medoids, progressively inducing a shared hierarchical clustering across agents. Communication is limited to sample identifiers, cluster labels, and ordinal rankings, providing privacy by design while supporting overlapping feature partitions and heterogeneous local clustering methods, and yielding an interpretable shared Cluster Fusion Hierarchy (CFH) that captures cross-view agreement at multiple resolutions.We analyze communication complexity and robustness, and experiments demonstrate competitive clustering performance in vertical federated settings.

</details>


### [253] [When Is Compositional Reasoning Learnable from Verifiable Rewards?](https://arxiv.org/abs/2602.07992)
*Daniel Barzilai,Yotam Wolf,Ronen Basri*

Main category: cs.LG

TL;DR: 本文理论研究了自回归模型在RLVR训练下组合问题的可学习性，提出任务优势比概念来刻画可学习任务和组合，分析了RLVR成功与失败的情况。


<details>
  <summary>Details</summary>
Motivation: 尽管RLVR在组合推理上取得进展，但不清楚仅使用结果级反馈时哪些组合问题可学习，因此进行理论研究。

Method: 定义任务 - 优势比这一组合问题和基础模型的联合属性来研究可学习性。

Result: 正向来看，中间步骤有明显优势的组合问题可通过RLVR有效学习，还分析了优势的自然产生；负向来看，无结构优势时，RLVR可能收敛到次优组合，且基础模型质量会影响优势存在及收敛情况。

Conclusion: 分析为理解RLVR何时成功及失败提供了理论依据。

Abstract: The emergence of compositional reasoning in large language models through reinforcement learning with verifiable rewards (RLVR) has been a key driver of recent empirical successes. Despite this progress, it remains unclear which compositional problems are learnable in this setting using outcome-level feedback alone. In this work, we theoretically study the learnability of compositional problems in autoregressive models under RLVR training. We identify a quantity that we call the task-advantage ratio, a joint property of the compositional problem and the base model, that characterizes which tasks and compositions are learnable from outcome-level feedback. On the positive side, using this characterization, we show that compositional problems where correct intermediate steps provide a clear advantage are efficiently learnable with RLVR. We also analyze how such an advantage naturally arises in different problems. On the negative side, when the structural advantage is not present, RLVR may converge to suboptimal compositions. We prove that, in some cases, the quality of the base model determines if such an advantage exists and whether RLVR will converge to a suboptimal solution. We hope our analysis can provide a principled theoretical understanding of when and why RLVR succeeds and when it does not.

</details>


### [254] [Fair Decisions from Calibrated Scores: Achieving Optimal Classification While Satisfying Sufficiency](https://arxiv.org/abs/2602.07285)
*Etam Benger,Katrina Ligett*

Main category: cs.LG

TL;DR: 本文提出在充分性条件下最优二元分类的精确解，给出可实现的正预测值和假遗漏率可行对的几何特征，推导简单后处理算法，并找到满足充分性时最小化与分离性偏差的分类器。


<details>
  <summary>Details</summary>
Motivation: 传统基于预测概率的单一阈值二分类方法通常违反统计群体公平性约束，即使完美的群体校准分数在阈值处理后也会违反预测奇偶性。

Method: 假设有限的群体校准分数集，给出可实现的正预测值和假遗漏率的几何特征，推导使用群体校准分数和群体成员身份的后处理算法，并找出满足充分性时最小化与分离性偏差的分类器。

Result: 得到了最优二元（随机）分类的精确解，以及满足条件的分类器。

Conclusion: 提出的算法能在充分性条件下实现最优分类，且可用于找到满足充分性时最小化与分离性偏差的分类器，常能达到与最优值相当的性能。

Abstract: Binary classification based on predicted probabilities (scores) is a fundamental task in supervised machine learning. While thresholding scores is Bayes-optimal in the unconstrained setting, using a single threshold generally violates statistical group fairness constraints. Under independence (statistical parity) and separation (equalized odds), such thresholding suffices when the scores already satisfy the corresponding criterion. However, this does not extend to sufficiency: even perfectly group-calibrated scores -- including true class probabilities -- violate predictive parity after thresholding. In this work, we present an exact solution for optimal binary (randomized) classification under sufficiency, assuming finite sets of group-calibrated scores. We provide a geometric characterization of the feasible pairs of positive predictive value (PPV) and false omission rate (FOR) achievable by such classifiers, and use it to derive a simple post-processing algorithm that attains the optimal classifier using only group-calibrated scores and group membership. Finally, since sufficiency and separation are generally incompatible, we identify the classifier that minimizes deviation from separation subject to sufficiency, and show that it can also be obtained by our algorithm, often achieving performance comparable to the optimum.

</details>


### [255] [Incorruptible Neural Networks: Training Models that can Generalize to Large Internal Perturbations](https://arxiv.org/abs/2602.07320)
*Philip Jacobson,Ben Feinberg,Suhas Kumar,Sapan Agarwal,T. Patrick Xiao,Christopher Bennett*

Main category: cs.LG

TL;DR: 本文探索SAM和RWP两种方法寻找对权重随机损坏鲁棒的最小值，从泛化和优化角度研究，发现过正则化RWP训练目标对噪声鲁棒泛化最优，SAM对小噪声效果好，还指出动态调整扰动强度可优化扰动目标。


<details>
  <summary>Details</summary>
Motivation: 神经网络损失景观平坦区域与更好泛化特性相关，且未来低功耗硬件平台需要训练对权重内部扰动鲁棒的模型。

Method: 使用sharpness-aware minimization (SAM)和random-weight perturbation (RWP)两种方法，从泛化和优化两个角度研究问题。

Result: 过正则化RWP训练目标对噪声鲁棒泛化最优；小噪声下SAM优于RWP，大噪声下SAM表现差；SAM和RWP受损失景观不均匀导致的梯度消失影响。

Conclusion: 动态调整扰动强度匹配损失景观演变可优化扰动目标。

Abstract: Flat regions of the neural network loss landscape have long been hypothesized to correlate with better generalization properties. A closely related but distinct problem is training models that are robust to internal perturbations to their weights, which may be an important need for future low-power hardware platforms. In this paper, we explore the usage of two methods, sharpness-aware minimization (SAM) and random-weight perturbation (RWP), to find minima robust to a variety of random corruptions to weights. We consider the problem from two angles: generalization (how do we reduce the noise-robust generalization gap) and optimization (how do we maximize performance from optimizers when subject to strong perturbations). First, we establish, both theoretically and empirically, that an over-regularized RWP training objective is optimal for noise-robust generalization. For small-magnitude noise, we find that SAM's adversarial objective further improves performance over any RWP configuration, but performs poorly for large-magnitude noise. We link the cause of this to a vanishing-gradient effect, caused by unevenness in the loss landscape, affecting both SAM and RWP. Lastly, we demonstrate that dynamically adjusting the perturbation strength to match the evolution of the loss landscape improves optimizing for these perturbed objectives.

</details>


### [256] [Sharp analysis of linear ensemble sampling](https://arxiv.org/abs/2602.08026)
*Arya Akhavan,David Janz,Csaba Szepesvári*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We analyse linear ensemble sampling (ES) with standard Gaussian perturbations in stochastic linear bandits. We show that for ensemble size $m=Θ(d\log n)$, ES attains $\tilde O(d^{3/2}\sqrt n)$ high-probability regret, closing the gap to the Thompson sampling benchmark while keeping computation comparable. The proof brings a new perspective on randomized exploration in linear bandits by reducing the analysis to a time-uniform exceedance problem for $m$ independent Brownian motions. Intriguingly, this continuous-time lens is not forced; it appears natural--and perhaps necessary: the discrete-time problem seems to be asking for a continuous-time solution, and we know of no other way to obtain a sharp ES bound.

</details>


### [257] [Revisiting Robustness for LLM Safety Alignment via Selective Geometry Control](https://arxiv.org/abs/2602.07340)
*Yonghui Yang,Wenjian Tao,Jilong Liu,Xingyu Zhu,Junfeng Fang,Weibiao Huang,Le Wu,Richang Hong,Tat-Sent Chua*

Main category: cs.LG

TL;DR: 现有大语言模型安全对齐方法在领域转移和噪声偏好监督下较脆弱，本文从优化几何视角提出ShaPO框架，提升对齐鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型安全对齐在领域转移和噪声偏好监督下效果不佳，且现有鲁棒对齐方法多忽视基于偏好目标的优化脆弱性。

Method: 提出几何感知的偏好优化框架ShaPO，通过对关键参数子空间进行选择性几何控制，以执行最坏情况下的对齐目标。

Result: 在多种安全基准和噪声偏好设置中，ShaPO比流行偏好优化方法持续提升安全鲁棒性，且能与数据鲁棒性目标结合产生额外增益。

Conclusion: 证明从优化几何视角出发能有效解决大语言模型安全对齐鲁棒性问题。

Abstract: Safety alignment of large language models remains brittle under domain shift and noisy preference supervision. Most existing robust alignment methods focus on uncertainty in alignment data, while overlooking optimization-induced fragility in preference-based objectives. In this work, we revisit robustness for LLM safety alignment from an optimization geometry perspective, and argue that robustness failures cannot be addressed by data-centric methods alone. We propose ShaPO, a geometry-aware preference optimization framework that enforces worst-case alignment objectives via selective geometry control over alignment-critical parameter subspace. By avoiding uniform geometry constraints, ShaPO mitigates the over-regularization that can harm robustness under distribution shift. We instantiate ShaPO at two levels: token-level ShaPO stabilizes likelihood-based surrogate optimization, while reward-level ShaPO enforces reward-consistent optimization under noisy supervision. Across diverse safety benchmarks and noisy preference settings, ShaPO consistently improves safety robustness over popular preference optimization methods. Moreover, ShaPO composes cleanly with data-robust objectives, yielding additional gains and empirically supporting the proposed optimization-geometry perspective.

</details>


### [258] [Scalable Dexterous Robot Learning with AR-based Remote Human-Robot Interactions](https://arxiv.org/abs/2602.07341)
*Yicheng Yang,Ruijiao Li,Lifeng Wang,Shuai Zheng,Shunzheng Ma,Keyu Zhang,Tuoyu Sun,Chenyun Dai,Jie Ding,Zhuo Zou*

Main category: cs.LG

TL;DR: 论文聚焦灵巧机器人臂手系统的可扩展操作学习，利用AR远程人机交互收集数据，提出统一框架，经仿真和实际实验验证效果良好。


<details>
  <summary>Details</summary>
Motivation: 解决灵巧机器人臂手系统可扩展操作学习的效率问题，通过AR远程人机交互收集专家示范数据以改进学习。

Method: 提出统一框架，分两阶段，第一阶段用行为克隆创建策略，第二阶段用对比学习强化学习获得更好策略，设计投影头加速学习，采用事件驱动增强奖励保障安全。

Result: 与经典策略相比，该方法显著加速推理，在操作任务成功率上表现更好，消融实验证实对比学习强化学习克服了策略崩溃。

Conclusion: 所提方法在灵巧机器人臂手系统操作学习中有效，加速推理且提高任务成功率，克服策略崩溃。

Abstract: This paper focuses on the scalable robot learning for manipulation in the dexterous robot arm-hand systems, where the remote human-robot interactions via augmented reality (AR) are established to collect the expert demonstration data for improving efficiency. In such a system, we present a unified framework to address the general manipulation task problem. Specifically, the proposed method consists of two phases: i) In the first phase for pretraining, the policy is created in a behavior cloning (BC) manner, through leveraging the learning data from our AR-based remote human-robot interaction system; ii) In the second phase, a contrastive learning empowered reinforcement learning (RL) method is developed to obtain more efficient and robust policy than the BC, and thus a projection head is designed to accelerate the learning progress. An event-driven augmented reward is adopted for enhancing the safety. To validate the proposed method, both the physics simulations via PyBullet and real-world experiments are carried out. The results demonstrate that compared to the classic proximal policy optimization and soft actor-critic policies, our method not only significantly speeds up the inference, but also achieves much better performance in terms of the success rate for fulfilling the manipulation tasks. By conducting the ablation study, it is confirmed that the proposed RL with contrastive learning overcomes policy collapse. Supplementary demonstrations are available at https://cyberyyc.github.io/.

</details>


### [259] [Mutual information and task-relevant latent dimensionality](https://arxiv.org/abs/2602.08105)
*Paarth Gulati,Eslam Abdelaleem,Audrey Sederberg,Ilya Nemenman*

Main category: cs.LG

TL;DR: 本文将估计预测所需潜在表征维度的问题转化为信息瓶颈问题，提出混合评论家及单次协议方法，在合成问题、内在维度估计及物理数据集上验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 估计预测所需潜在表征维度（任务相关维度）是一个具有广泛科学应用但尚未解决的难题。

Method: 将问题转化为信息瓶颈问题，采用混合评论家解决标准神经估计器夸大推断维度的问题，并提出单次协议从单一过参数化混合模型中读取有效维度。

Result: 在已知任务相关维度的合成问题上验证了方法；扩展到内在维度估计，在噪声环境中比经典几何维度估计器更可靠；在多个物理数据集上证明了方法的实用性。

Conclusion: 所提出的方法能有效解决潜在表征维度估计问题，在多种场景下表现良好。

Abstract: Estimating the dimensionality of the latent representation needed for prediction -- the task-relevant dimension -- is a difficult, largely unsolved problem with broad scientific applications. We cast it as an Information Bottleneck question: what embedding bottleneck dimension is sufficient to compress predictor and predicted views while preserving their mutual information (MI). This repurposes neural MI estimators for dimensionality estimation. We show that standard neural estimators with separable/bilinear critics systematically inflate the inferred dimension, and we address this by introducing a hybrid critic that retains an explicit dimensional bottleneck while allowing flexible nonlinear cross-view interactions, thereby preserving the latent geometry. We further propose a one-shot protocol that reads off the effective dimension from a single over-parameterized hybrid model, without sweeping over bottleneck sizes. We validate the approach on synthetic problems with known task-relevant dimension. We extend the approach to intrinsic dimensionality by constructing paired views of a single dataset, enabling comparison with classical geometric dimension estimators. In noisy regimes where those estimators degrade, our approach remains reliable. Finally, we demonstrate the utility of the method on multiple physics datasets.

</details>


### [260] [Controllable Value Alignment in Large Language Models through Neuron-Level Editing](https://arxiv.org/abs/2602.07356)
*Yonghui Yang,Junwei Li,Jilong Liu,Yicheng He,Fengbin Zhu,Weibiao Huang,Le Wu,Richang Hong,Tat-Seng Chua*

Main category: cs.LG

TL;DR: 引入价值泄漏概念，提出NeVA框架实现大语言模型可控价值对齐，实验效果良好。


<details>
  <summary>Details</summary>
Motivation: 现有基于引导的对齐方法可控性有限，引导目标价值时会无意激活非目标价值。

Method: 引入价值泄漏概念和归一化泄漏指标，提出NeVA框架，识别与价值相关的稀疏神经元并进行推理时激活编辑。

Result: NeVA实现更强的目标价值对齐，对通用能力的性能降级更小，显著降低平均泄漏，残余影响局限于语义相关的价值类别。

Conclusion: NeVA为价值对齐提供了更可控和可解释的机制。

Abstract: Aligning large language models (LLMs) with human values has become increasingly important as their influence on human behavior and decision-making expands. However, existing steering-based alignment methods suffer from limited controllability: steering a target value often unintentionally activates other, non-target values. To characterize this limitation, we introduce value leakage, a diagnostic notion that captures the unintended activation of non-target values during value steering, along with a normalized leakage metric grounded in Schwartz's value theory. In light of this analysis, we propose NeVA, a neuron-level editing framework for controllable value alignment in LLMs. NeVA identifies sparse, value-relevant neurons and performs inference-time activation editing, enabling fine-grained control without parameter updates or retraining. Experiments show that NeVA achieves stronger target value alignment while incurring smaller performance degradation on general capability. Moreover, NeVA significantly reduces the average leakage, with residual effects largely confined to semantically related value classes. Overall, NeVA offers a more controllable and interpretable mechanism for value alignment.

</details>


### [261] [Variance-Gated Ensembles: An Epistemic-Aware Framework for Uncertainty Estimation](https://arxiv.org/abs/2602.08142)
*H. Martin Gillis,Isaac Xu,Thomas Trappenberg*

Main category: cs.LG

TL;DR: 本文提出Variance - Gated Ensembles (VGE)框架用于样本不确定性估计，计算高效且表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习样本不确定性估计的加性分解方法存在问题，在有限集成采样和/或不匹配的预测分布时会失效。

Method: 引入VGE框架，提供VGMU分数和VGN层，推导封闭形式的向量 - 雅可比积实现端到端训练。

Result: VGE匹配或超越了最先进的信息论基线，且计算高效。

Conclusion: VGE为集成模型中认知感知的不确定性估计提供了实用且可扩展的方法。

Abstract: Machine learning applications require fast and reliable per-sample uncertainty estimation. A common approach is to use predictive distributions from Bayesian or approximation methods and additively decompose uncertainty into aleatoric (i.e., data-related) and epistemic (i.e., model-related) components. However, additive decomposition has recently been questioned, with evidence that it breaks down when using finite-ensemble sampling and/or mismatched predictive distributions. This paper introduces Variance-Gated Ensembles (VGE), an intuitive, differentiable framework that injects epistemic sensitivity via a signal-to-noise gate computed from ensemble statistics. VGE provides: (i) a Variance-Gated Margin Uncertainty (VGMU) score that couples decision margins with ensemble predictive variance; and (ii) a Variance-Gated Normalization (VGN) layer that generalizes the variance-gated uncertainty mechanism to training via per-class, learnable normalization of ensemble member probabilities. We derive closed-form vector-Jacobian products enabling end-to-end training through ensemble sample mean and variance. VGE matches or exceeds state-of-the-art information-theoretic baselines while remaining computationally efficient. As a result, VGE provides a practical and scalable approach to epistemic-aware uncertainty estimation in ensemble models. An open-source implementation is available at: https://github.com/nextdevai/vge.

</details>


### [262] [UTOPIA: Unlearnable Tabular Data via Decoupled Shortcut Embedding](https://arxiv.org/abs/2602.07358)
*Jiaming He,Fuming Luo,Hongwei Li,Wenbo Jiang,Wenshu Fan,Zhenbo Shi,Xudong Jiang,Yi Yu*

Main category: cs.LG

TL;DR: 提出UTOPIA方法，利用特征冗余将优化解耦为两个通道，在表格数据集上实验证明其能使未授权训练接近随机性能，优于现有基准方法。


<details>
  <summary>Details</summary>
Motivation: 现有无法学习样本（UE）方法难以用于表格数据，金融和医疗的表格数据高度敏感，且现有UE方法泛化不佳。

Method: 在频谱主导条件下，提出Unlearnable Tabular Data via DecOuPled Shortcut EmbeddIng（UTOPIA）方法，利用特征冗余将优化解耦为两个通道。

Result: 实验表明UTOPIA使未授权训练接近随机性能，优于强UE基准方法，且在不同架构间泛化性好。

Conclusion: UTOPIA能有效保护表格数据，防止未授权的模型训练。

Abstract: Unlearnable examples (UE) have emerged as a practical mechanism to prevent unauthorized model training on private vision data, while extending this protection to tabular data is nontrivial. Tabular data in finance and healthcare is highly sensitive, yet existing UE methods transfer poorly because tabular features mix numerical and categorical constraints and exhibit saliency sparsity, with learning dominated by a few dimensions. Under a Spectral Dominance condition, we show certified unlearnability is feasible when the poison spectrum overwhelms the clean semantic spectrum. Guided by this, we propose Unlearnable Tabular Data via DecOuPled Shortcut EmbeddIng (UTOPIA), which exploits feature redundancy to decouple optimization into two channels: high saliency features for semantic obfuscation and low saliency redundant features for embedding a hyper correlated shortcut, yielding constraint-aware dominant shortcuts while preserving tabular validity. Extensive experiments across tabular datasets and models show UTOPIA drives unauthorized training toward near random performance, outperforming strong UE baselines and transferring well across architectures.

</details>


### [263] [A second order regret bound for NormalHedge](https://arxiv.org/abs/2602.08151)
*Yoav Freund,Nicholas J. A. Harvey,Victor S. Portella,Yabing Qi,Yu-Xiang Wang*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We consider the problem of prediction with expert advice for ``easy'' sequences. We show that a variant of NormalHedge enjoys a second-order $ε$-quantile regret bound of $O\big(\sqrt{V_T \log(V_T/ε)}\big) $ when $V_T > \log N$, where $V_T$ is the cumulative second moment of instantaneous per-expert regret averaged with respect to a natural distribution determined by the algorithm. The algorithm is motivated by a continuous time limit using Stochastic Differential Equations. The discrete time analysis uses self-concordance techniques.

</details>


### [264] [FEM-Informed Hypergraph Neural Networks for Efficient Elastoplasticity](https://arxiv.org/abs/2602.07364)
*Jianchuan Yang,Xi Chen,Jidong Zhao*

Main category: cs.LG

TL;DR: 本文提出基于有限元的超图神经网络（FHGNN）用于计算力学，提升了准确性和效率，为非线性固体力学的可扩展物理嵌入学习奠定基础。


<details>
  <summary>Details</summary>
Motivation: 为在计算力学中进行物理信息机器学习，受离散物理损失和HiDeNN结构的启发，探索更有效的方法。

Method: 将有限元计算直接嵌入消息传递层，提出FHGNN，采用高效变分损失，训练纯物理驱动无需标签数据。

Result: 在3D基准测试中，相比竞争的PINN变体，精度和效率大幅提升，可有效扩展到大型弹塑性问题，速度与多核心FEM相当甚至更快。

Conclusion: 此工作为非线性固体力学中可扩展的物理嵌入学习奠定了基础。

Abstract: Graph neural networks (GNNs) naturally align with sparse operators and unstructured discretizations, making them a promising paradigm for physics-informed machine learning in computational mechanics. Motivated by discrete physics losses and Hierarchical Deep Learning Neural Network (HiDeNN) constructions, we embed finite-element (FEM) computations at nodes and Gauss points directly into message-passing layers and propose a numerically consistent FEM-Informed Hypergraph Neural Networks (FHGNN). Similar to conventional physics-informed neural networks (PINNs), training is purely physics-driven and requires no labeled data: the input is a node element hypergraph whose edges encode mesh connectivity. Guided by empirical results and condition-number analysis, we adopt an efficient variational loss. Validated on 3D benchmarks, including cyclic loading with isotropic/kinematic hardening, the proposed method delivers substantially improved accuracy and efficiency over recent, competitive PINN variants. By leveraging GPU-parallel tensor operations and the discrete representation, it scales effectively to large elastoplastic problems and can be competitive with, or faster than, multi-core FEM implementations at comparable accuracy. This work establishes a foundation for scalable, physics-embedded learning in nonlinear solid mechanics.

</details>


### [265] [Interpretable Dynamic Network Modeling of Tensor Time Series via Kronecker Time-Varying Graphical Lasso](https://arxiv.org/abs/2602.08197)
*Shingo Higashiguchi,Koki Kawabata,Yasuko Matsubara,Yasushi Sakurai*

Main category: cs.LG

TL;DR: 提出Kronecker Time-Varying Graphical Lasso (KTVGL)方法用于建模张量时间序列，实验表明其比现有方法有更高边估计精度和更少计算时间。


<details>
  <summary>Details</summary>
Motivation: 随着网络服务发展，大量时间序列数据产生，估计变量间时变依赖关系很关键，但现实数据常为多模式张量时间序列，网络复杂难解释且计算量大。

Method: 提出KTVGL方法，以Kronecker积形式估计特定模式动态网络，避免复杂纠缠结构，分区网络结构防止计算时间随数据维度指数增长，还可扩展为流算法。

Result: 在合成数据实验中，该方法比现有方法有更高边估计精度且计算时间更少，还进行了真实数据案例研究。

Conclusion: 提出的KTVGL方法有效，代码和数据集可在https://github.com/Higashiguchi-Shingo/KTVGL获取。

Abstract: With the rapid development of web services, large amounts of time series data are generated and accumulated across various domains such as finance, healthcare, and online platforms. As such data often co-evolves with multiple variables interacting with each other, estimating the time-varying dependencies between variables (i.e., the dynamic network structure) has become crucial for accurate modeling. However, real-world data is often represented as tensor time series with multiple modes, resulting in large, entangled networks that are hard to interpret and computationally intensive to estimate. In this paper, we propose Kronecker Time-Varying Graphical Lasso (KTVGL), a method designed for modeling tensor time series. Our approach estimates mode-specific dynamic networks in a Kronecker product form, thereby avoiding overly complex entangled structures and producing interpretable modeling results. Moreover, the partitioned network structure prevents the exponential growth of computational time with data dimension. In addition, our method can be extended to stream algorithms, making the computational time independent of the sequence length. Experiments on synthetic data show that the proposed method achieves higher edge estimation accuracy than existing methods while requiring less computation time. To further demonstrate its practical value, we also present a case study using real-world data. Our source code and datasets are available at https://github.com/Higashiguchi-Shingo/KTVGL.

</details>


### [266] [CADO: From Imitation to Cost Minimization for Heatmap-based Solvers in Combinatorial Optimization](https://arxiv.org/abs/2602.08210)
*Hyungseok Song,Deunsol Yoon,Kanghoon Lee,Han-Seul Jeong,Soonyoung Lee,Woohyung Lim*

Main category: cs.LG

TL;DR: 指出基于热图的组合优化求解器中监督学习训练范式存在目标不匹配问题，提出CADO框架解决该问题并取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于热图的组合优化求解器中监督学习训练范式存在目标不匹配问题，限制了求解器性能。

Method: 提出CADO框架，将扩散去噪过程公式化为MDP直接优化解码后解的成本，引入Label - Centered Reward和Hybrid Fine - Tuning。

Result: CADO在多个基准测试中达到了SOTA性能。

Conclusion: 目标对齐对于释放基于热图的求解器的全部潜力至关重要。

Abstract: Heatmap-based solvers have emerged as a promising paradigm for Combinatorial Optimization (CO). However, we argue that the dominant Supervised Learning (SL) training paradigm suffers from a fundamental objective mismatch: minimizing imitation loss (e.g., cross-entropy) does not guarantee solution cost minimization. We dissect this mismatch into two deficiencies: Decoder-Blindness (being oblivious to the non-differentiable decoding process) and Cost-Blindness (prioritizing structural imitation over solution quality). We empirically demonstrate that these intrinsic flaws impose a hard performance ceiling. To overcome this limitation, we propose CADO (Cost-Aware Diffusion models for Optimization), a streamlined Reinforcement Learning fine-tuning framework that formulates the diffusion denoising process as an MDP to directly optimize the post-decoded solution cost. We introduce Label-Centered Reward, which repurposes ground-truth labels as unbiased baselines rather than imitation targets, and Hybrid Fine-Tuning for parameter-efficient adaptation. CADO achieves state-of-the-art performance across diverse benchmarks, validating that objective alignment is essential for unlocking the full potential of heatmap-based solvers.

</details>


### [267] [Thermodynamic Isomorphism of Transformers: A Lagrangian Approach to Attention Dynamics](https://arxiv.org/abs/2602.08216)
*Gunn Kim*

Main category: cs.LG

TL;DR: 提出信息动力学的第一性原理框架，连接统计物理与深度学习，为基于物理的智能通用理论奠定基础。


<details>
  <summary>Details</summary>
Motivation: Transformer架构底层机制缺乏统一物理理论，需构建新框架解释其原理。

Method: 将注意力机制视为遵循最小作用量原理的物理系统，把信息状态映射到黎曼流形推导智能拉格朗日量。

Result: 建立信息热力学第一定律，解释涌现现象，从场论角度解释旋转位置嵌入。

Conclusion: 工作连接了统计物理和深度学习，为基于物理的智能通用理论奠定基础。

Abstract: Although the Transformer architecture has revolutionized artificial intelligence, its underlying mechanisms remain largely heuristic and lack a unified physical theory. In this work, we propose a first-principles framework for information dynamics, treating the attention mechanism as a physical system governed by the principle of least action rather than as an algorithmic optimization. By mapping information states to a Riemannian manifold with the Fisher information metric, we derive the intelligence Lagrangian. We show that the softmax function corresponds to the unique thermodynamic equilibrium state that minimizes the Helmholtz free energy of the information gas. In addition, we identify the query-key interaction as an electrodynamic coupling between an external field and an intrinsic dipole moment. This theory establishes the first law of information thermodynamics, unifying inference (mechanical work) and learning (chemical evolution). It also explains emergent phenomena, such as scaling laws and grokking, as phase transitions characterized by the divergence of specific heat. Finally, we discuss how rotational symmetry breaking in the attention manifold generates massless Goldstone bosons, providing a field-theoretic perspective on rotary positional embeddings (RoPE). Our work connects Statistical Physics and Deep Learning, laying the groundwork for a general theory of physics-based intelligence.

</details>


### [268] [Scout Before You Attend: Sketch-and-Walk Sparse Attention for Efficient LLM Inference](https://arxiv.org/abs/2602.07397)
*Hoang Anh Duy Le,Sahil Joshi,Zeyu Yang,Zhaozhuo Xu,Anshumali Shrivastava*

Main category: cs.LG

TL;DR: 提出无训练稀疏注意力方法Sketch&Walk Attention，能在多模型和任务中实现推理加速并保持精度。


<details>
  <summary>Details</summary>
Motivation: 解决自注意力机制在长上下文大语言模型推理时计算和内存成本高的问题。

Method: 使用Hadamard sketching近似注意力分数，通过walk机制聚合各层估计，用累积分数选择top - k注意力块，结合自定义稀疏注意力内核。

Result: 在20%注意力密度下保持近乎无损的精度，在某些情况下略优于密集注意力，推理速度最高提升6倍。

Conclusion: Sketch&Walk Attention是一种有效的无训练稀疏注意力方法，可用于长上下文大语言模型推理。

Abstract: Self-attention dominates the computational and memory cost of long-context LLM inference across both prefill and decode phases. To address this challenge, we introduce Sketch&Walk Attention, a training-free sparse attention method that determines sparsity with lightweight sketches and deterministic walk. Sketch&Walk applies Hadamard sketching to get inexpensive approximations of attention scores, then aggregates these estimates across layers via a walk mechanism that captures attention influence beyond direct interactions between tokens. The accumulated walk scores are used to select top-k attention blocks, enabling dynamic sparsity with a single training-free algorithm that applies uniformly to both the prefill and decode phases, together with custom sparse attention kernels. Across a wide range of models and tasks, Sketch&Walk maintains near-lossless accuracy at 20% attention density and can slightly outperform dense attention in some settings, while achieving up to 6x inference speedup.

</details>


### [269] [Noise Stability of Transformer Models](https://arxiv.org/abs/2602.08287)
*Themistoklis Haris,Zihan Zhang,Yuichi Yoshida*

Main category: cs.LG

TL;DR: 文章指出深度学习中平均敏感度作为简单性指标的局限，提出噪声稳定性指标，给出理论分析和正则化方法，实验表明可加速训练


<details>
  <summary>Details</summary>
Motivation: 平均敏感度作为简单性指标存在缺乏对实值域自然推广、无法解释大语言模型输入依赖性的局限，需要更全面的指标

Method: 提出噪声稳定性指标，对单层层注意力和ReLU MLP层开展理论分析，用协方差区间传播处理多层传播问题，开发实用的噪声稳定性正则化方法

Result: 在算法和下一个词预测任务中，正则器能稳定促进学习并分别加速训练约35%和75%

Conclusion: 噪声稳定性建立了神经网络中信号传播与可解释性的新联系，是理解和改进现代Transformer的有力工具

Abstract: Understanding simplicity biases in deep learning offers a promising path toward developing reliable AI. A common metric for this, inspired by Boolean function analysis, is average sensitivity, which captures a model's robustness to single-token perturbations. We argue that average sensitivity has two key limitations: it lacks a natural generalization to real-valued domains and fails to explain the "junta-like" input dependence we empirically observe in modern LLMs. To address these limitations, we propose noise stability as a more comprehensive simplicity metric. Noise stability expresses a model's robustness to correlated noise applied to all input coordinates simultaneously. We provide a theoretical analysis of noise stability for single-layer attention and ReLU MLP layers and tackle the multi-layer propagation problem with a covariance interval propagation approach. Building on this theory, we develop a practical noise stability regularization method. Experiments on algorithmic and next-token-prediction tasks show that our regularizer consistently catalyzes grokking and accelerates training by approximately $35\%$ and $75\%$ respectively. Our results sculpt a new connection between signal propagation in neural networks and interpretability, with noise stability emerging as a powerful tool for understanding and improving modern Transformers.

</details>


### [270] [Permissive-Washing in the Open AI Supply Chain: A Large-Scale Audit of License Integrity](https://arxiv.org/abs/2602.08816)
*James Jewitt,Gopi Krishnan Rajbahadur,Hao Li,Bram Adams,Ahmed E. Hassan*

Main category: cs.LG

TL;DR: 论文指出开源AI中宽松许可存在违规现象‘permissive washing’，通过对AI供应链实证审计发现高比例数据集和模型缺少必要许可文本等，强调许可文件才是法律依据，并发布审计数据和流程。


<details>
  <summary>Details</summary>
Motivation: 评估宽松许可违规（permissive washing）现象在AI供应链中的普遍性。

Method: 对124,278个数据集→模型→应用供应链进行实证审计。

Result: 96.5%的数据集和95.8%的模型缺少必要许可文本，满足许可文本和版权要求的极少，下游对上游的归属权保留率很低。

Conclusion: 从业者不能仅依靠宽松标签，许可文件和通知才是法律权利来源，发布审计数据和可复现流程支持未来研究。

Abstract: Permissive licenses like MIT, Apache-2.0, and BSD-3-Clause dominate open-source AI, signaling that artifacts like models, datasets, and code can be freely used, modified, and redistributed. However, these licenses carry mandatory requirements: include the full license text, provide a copyright notice, and preserve upstream attribution, that remain unverified at scale. Failure to meet these conditions can place reuse outside the scope of the license, effectively leaving AI artifacts under default copyright for those uses and exposing downstream users to litigation. We call this phenomenon ``permissive washing'': labeling AI artifacts as free to use, while omitting the legal documentation required to make that label actionable. To assess how widespread permissive washing is in the AI supply chain, we empirically audit 124,278 dataset $\rightarrow$ model $\rightarrow$ application supply chains, spanning 3,338 datasets, 6,664 models, and 28,516 applications across Hugging Face and GitHub. We find that an astonishing 96.5\% of datasets and 95.8\% of models lack the required license text, only 2.3\% of datasets and 3.2\% of models satisfy both license text and copyright requirements, and even when upstream artifacts provide complete licensing evidence, attribution rarely propagates downstream: only 27.59\% of models preserve compliant dataset notices and only 5.75\% of applications preserve compliant model notices (with just 6.38\% preserving any linked upstream notice). Practitioners cannot assume permissive labels confer the rights they claim: license files and notices, not metadata, are the source of legal truth. To support future research, we release our full audit dataset and reproducible pipeline.

</details>


### [271] [Nonparametric Bayesian Optimization for General Rewards](https://arxiv.org/abs/2602.07411)
*Zishi Zhang,Tao Ren,Yijie Peng*

Main category: cs.LG

TL;DR: 提出首个在一般奖励设置下实现无后悔保证的贝叶斯优化算法，采用新型代理模型∞ - GP结合汤普森采样，有可扩展计算方法，实证效果好。


<details>
  <summary>Details</summary>
Motivation: 解决奖励模型不确定性下的贝叶斯优化问题。

Method: 提出新型代理模型∞ - GP，结合汤普森采样，开发新的TS后悔分析框架，采用截断吉布斯采样程序。

Result: 实证结果显示在非平稳、重尾或病态奖励设置下达到了最先进的性能。

Conclusion: 所提算法在奖励模型不确定的贝叶斯优化中有效且计算可扩展。

Abstract: This work focuses on Bayesian optimization (BO) under reward model uncertainty. We propose the first BO algorithm that achieves no-regret guarantee in a general reward setting, requiring only Lipschitz continuity of the objective function and accommodating a broad class of measurement noise. The core of our approach is a novel surrogate model, termed as infinite Gaussian process ($\infty$-GP). It is a Bayesian nonparametric model that places a prior on the space of reward distributions, enabling it to represent a substantially broader class of reward models than classical Gaussian process (GP). The $\infty$-GP is used in combination with Thompson Sampling (TS) to enable effective exploration and exploitation. Correspondingly, we develop a new TS regret analysis framework for general rewards, which relates the regret to the total variation distance between the surrogate model and the true reward distribution. Furthermore, with a truncated Gibbs sampling procedure, our method is computationally scalable, incurring minimal additional memory and computational complexities compared to classical GP. Empirical results demonstrate state-of-the-art performance, particularly in settings with non-stationary, heavy-tailed, or other ill-conditioned rewards.

</details>


### [272] [Interaction-Grounded Learning for Contextual Markov Decision Processes with Personalized Feedback](https://arxiv.org/abs/2602.08307)
*Mengxiao Zhang,Yuheng Zhang,Haipeng Luo,Paul Mineiro*

Main category: cs.LG

TL;DR: 本文研究交互接地学习（IGL），针对现有单步设置不足，提出适用于上下文情节马尔可夫决策过程（MDPs）的高效算法，实验验证有效性。


<details>
  <summary>Details</summary>
Motivation: 先前的IGL算法局限于单步设置，无法适用于现代顺序决策系统，如多轮大语言模型部署。

Method: 将Zhang等人[2024a]的奖励估计器构造从单步扩展到多步设置，解决MDPs下解码潜在奖励的问题，并设计逆差距加权（IGW）算法进行策略优化。

Result: 在合成情节MDP和真实世界用户预订数据集的实验中，证明了该方法能从多轮交互中学习个性化目标的有效性。

Conclusion: 提出的算法可有效解决IGL在多步设置中的问题，适用于现代顺序决策系统。

Abstract: In this paper, we study Interaction-Grounded Learning (IGL) [Xie et al., 2021], a paradigm designed for realistic scenarios where the learner receives indirect feedback generated by an unknown mechanism, rather than explicit numerical rewards. While prior work on IGL provides efficient algorithms with provable guarantees, those results are confined to single-step settings, restricting their applicability to modern sequential decision-making systems such as multi-turn Large Language Model (LLM) deployments. To bridge this gap, we propose a computationally efficient algorithm that achieves a sublinear regret guarantee for contextual episodic Markov Decision Processes (MDPs) with personalized feedback. Technically, we extend the reward-estimator construction of Zhang et al. [2024a] from the single-step to the multi-step setting, addressing the unique challenges of decoding latent rewards under MDPs. Building on this estimator, we design an Inverse-Gap-Weighting (IGW) algorithm for policy optimization. Finally, we demonstrate the effectiveness of our method in learning personalized objectives from multi-turn interactions through experiments on both a synthetic episodic MDP and a real-world user booking dataset.

</details>


### [273] [Learning Molecular Chirality via Chiral Determinant Kernels](https://arxiv.org/abs/2602.07415)
*Runhan Shi,Zhicheng Zhang,Letian Chen,Gufeng Yu,Yang Yang*

Main category: cs.LG

TL;DR: 本文介绍了ChiDeK框架，用于将立体信息集成到分子表征学习中，在多个手性相关任务上取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 由于立体化学关系的几何复杂性和传统分子表征缺乏明确立体化学编码，现有的手性分子表征方法难以泛化到更复杂的形式，如轴向手性。

Method: 引入ChiDeK框架，提出手性行列式核来编码SE(3)不变手性矩阵，并使用交叉注意力将局部手性中心的立体化学信息集成到全局分子表征中。构建电子圆二色性（ECD）和旋光性（OR）预测的新基准。

Result: 在包括R/S构型分类、对映体排序、ECD光谱预测和OR预测在内的四个任务上，ChiDeK比现有最先进的基线有显著改进，在轴向手性任务上平均准确率提高超7%。

Conclusion: ChiDeK框架有效，能在统一架构中对与手性相关的特征进行显式建模，联合编码中心和轴向手性。

Abstract: Chirality is a fundamental molecular property that governs stereospecific behavior in chemistry and biology. Capturing chirality in machine learning models remains challenging due to the geometric complexity of stereochemical relationships and the limitations of traditional molecular representations that often lack explicit stereochemical encoding. Existing approaches to chiral molecular representation primarily focus on central chirality, relying on handcrafted stereochemical tags or limited 3D encodings, and thus fail to generalize to more complex forms such as axial chirality. In this work, we introduce ChiDeK (Chiral Determinant Kernels), a framework that systematically integrates stereogenic information into molecular representation learning. We propose the chiral determinant kernel to encode the SE(3)-invariant chirality matrix and employ cross-attention to integrate stereochemical information from local chiral centers into the global molecular representation. This design enables explicit modeling of chiral-related features within a unified architecture, capable of jointly encoding central and axial chirality. To support the evaluation of axial chirality, we construct a new benchmark for electronic circular dichroism (ECD) and optical rotation (OR) prediction. Across four tasks, including R/S configuration classification, enantiomer ranking, ECD spectrum prediction, and OR prediction, ChiDeK achieves substantial improvements over state-of-the-art baselines, most notably yielding over 7% higher accuracy on axially chiral tasks on average.

</details>


### [274] [Fast Flow Matching based Conditional Independence Tests for Causal Discovery](https://arxiv.org/abs/2602.08315)
*Shunyu Zhao,Yanfeng Yang,Shuai Li,Kenji Fukumizu*

Main category: cs.LG

TL;DR: 提出基于流匹配的条件独立性测试FMCIT加速因果发现，还集成到GPC - FMCIT框架，实验显示有良好的准确率 - 效率权衡。


<details>
  <summary>Details</summary>
Motivation: 基于约束的因果发现方法中条件独立性测试计算复杂度高，限制实际应用，需设计加速算法。

Method: 提出FMCIT利用流匹配的高效计算能力，且整个因果发现过程模型只需训练一次；将FMCIT集成到GPC - FMCIT框架。

Result: FMCIT有效控制一类错误，在备择假设下保持高检验力；GPC - FMCIT对条件独立性查询数量有明确界限，保持高统计效力。

Conclusion: 在合成和真实世界因果发现任务中，相比现有条件独立性测试方法和PC变体有良好的准确率 - 效率权衡。

Abstract: Constraint-based causal discovery methods require a large number of conditional independence (CI) tests, which severely limits their practical applicability due to high computational complexity. Therefore, it is crucial to design an algorithm that accelerates each individual test. To this end, we propose the Flow Matching-based Conditional Independence Test (FMCIT). The proposed test leverages the high computational efficiency of flow matching and requires the model to be trained only once throughout the entire causal discovery procedure, substantially accelerating causal discovery. According to numerical experiments, FMCIT effectively controls type-I error and maintains high testing power under the alternative hypothesis, even in the presence of high-dimensional conditioning sets. In addition, we further integrate FMCIT into a two-stage guided PC skeleton learning framework, termed GPC-FMCIT, which combines fast screening with guided, budgeted refinement using FMCIT. This design yields explicit bounds on the number of CI queries while maintaining high statistical power. Experiments on synthetic and real-world causal discovery tasks demonstrate favorable accuracy-efficiency trade-offs over existing CI testing methods and PC variants.

</details>


### [275] [All ERMs Can Fail in Stochastic Convex Optimization Lower Bounds in Linear Dimension](https://arxiv.org/abs/2602.08350)
*Tal Burla,Roi Livni*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study the sample complexity of the best-case Empirical Risk Minimizer in the setting of stochastic convex optimization. We show that there exists an instance in which the sample size is linear in the dimension, learning is possible, but the Empirical Risk Minimizer is likely to be unique and to overfit. This resolves an open question by Feldman. We also extend this to approximate ERMs.
  Building on our construction we also show that (constrained) Gradient Descent potentially overfits when horizon and learning rate grow w.r.t sample size. Specifically we provide a novel generalization lower bound of $Ω\left(\sqrt{ηT/m^{1.5}}\right)$ for Gradient Descent, where $η$ is the learning rate, $T$ is the horizon and $m$ is the sample size. This narrows down, exponentially, the gap between the best known upper bound of $O(ηT/m)$ and existing lower bounds from previous constructions.

</details>


### [276] [Sign-Based Optimizers Are Effective Under Heavy-Tailed Noise](https://arxiv.org/abs/2602.07425)
*Dingzhi Yu,Hongyi Tao,Yuanyu Wan,Luo Luo,Lijun Zhang*

Main category: cs.LG

TL;DR: 本文从重尾梯度噪声角度，理论分析符号优化器在大语言模型训练中的表现，给出收敛率，还进行了实验验证。


<details>
  <summary>Details</summary>
Motivation: 符号优化算法在大语言模型训练中表现优于自适应梯度方法，但缺乏理论解释，本文旨在填补理论与实践的差距。

Method: 引入新的广义重尾噪声条件，分析SignSGD、Lion收敛率，拓展到Muon和Muonlight进行矩阵优化分析。

Result: 得到SignSGD和Lion的收敛率，首次对重尾随机性下的矩阵优化进行严格分析，实验验证理论。

Conclusion: 符号优化器天然适合处理重尾相关的噪声梯度，理论解释了其在实践中的优越性。

Abstract: While adaptive gradient methods are the workhorse of modern machine learning, sign-based optimization algorithms such as Lion and Muon have recently demonstrated superior empirical performance over AdamW in training large language models (LLM). However, a theoretical understanding of why sign-based updates outperform variance-adapted methods remains elusive. In this paper, we aim to bridge the gap between theory and practice through the lens of heavy-tailed gradient noise, a phenomenon frequently observed in language modeling tasks. Theoretically, we introduce a novel generalized heavy-tailed noise condition that captures the behavior of LLMs more accurately than standard finite variance assumptions. Under this noise model, we establish sharp convergence rates of SignSGD and Lion for generalized smooth function classes, matching or surpassing previous best-known bounds. Furthermore, we extend our analysis to Muon and Muonlight, providing what is, to our knowledge, the first rigorous analysis of matrix optimization under heavy-tailed stochasticity. These results offer a strong theoretical justification for the empirical superiority of sign-based optimizers, showcasing that they are naturally suited to handle the noisy gradients associated with heavy tails. Empirically, LLM pretraining experiments validate our theoretical insights and confirm that our proposed noise models are well-aligned with practice.

</details>


### [277] [Low Rank Transformer for Multivariate Time Series Anomaly Detection and Localization](https://arxiv.org/abs/2602.08467)
*Charalampos Shimillas,Kleanthis Malialis,Konstantinos Fokianos,Marios M. Polycarpou*

Main category: cs.LG

TL;DR: 本文研究Transformer应用于多变量时间序列（MTS）的学习过程，提出ALoRa - T模型和ALoRa - Loc方法，实验表明该方法在检测和定位任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多变量时间序列异常诊断方法理论见解有限，尤其是异常定位方面研究不足。

Method: 研究Transformer与统计时间序列方法的联系，提出应用低秩正则化到自注意力的ALoRa - T模型和引入Attention Low - Rank分数；提出通过量化时间序列间相互关系将异常关联到特定变量的ALoRa - Loc方法。

Result: 广泛的实验和真实数据分析显示，所提方法在检测和定位任务上显著优于现有方法。

Conclusion: 所提出的方法在多变量时间序列异常诊断的检测和定位任务中表现出色。

Abstract: Multivariate time series (MTS) anomaly diagnosis, which encompasses both anomaly detection and localization, is critical for the safety and reliability of complex, large-scale real-world systems. The vast majority of existing anomaly diagnosis methods offer limited theoretical insights, especially for anomaly localization, which is a vital but largely unexplored area. The aim of this contribution is to study the learning process of a Transformer when applied to MTS by revealing connections to statistical time series methods. Based on these theoretical insights, we propose the Attention Low-Rank Transformer (ALoRa-T) model, which applies low-rank regularization to self-attention, and we introduce the Attention Low-Rank score, effectively capturing the temporal characteristics of anomalies. Finally, to enable anomaly localization, we propose the ALoRa-Loc method, a novel approach that associates anomalies to specific variables by quantifying interrelationships among time series. Extensive experiments and real data analysis, show that the proposed methodology significantly outperforms state-of-the-art methods in both detection and localization tasks.

</details>


### [278] [Brep2Shape: Boundary and Shape Representation Alignment via Self-Supervised Transformers](https://arxiv.org/abs/2602.07429)
*Yuanxu Sun,Yuezhou Ma,Haixu Wu,Guanyang Zeng,Muye Chen,Jianmin Wang,Mingsheng Long*

Main category: cs.LG

TL;DR: 引入Brep2Shape方法，将抽象边界表示与直观形状表示对齐，实验显示其有显著可扩展性，在下游任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习处理B-rep模型的方法存在表示差距，连续方法有分析精度但视觉抽象，离散方法直观但几何精度不足。

Method: 提出Brep2Shape自监督预训练方法，采用几何感知任务，让模型从参数贝塞尔控制点预测密集空间点；使用双变压器骨干并行编码表面和曲线标记，集成拓扑注意力以保持拓扑一致性。

Result: Brep2Shape具有显著可扩展性，在各种下游任务中实现了最先进的准确性和更快的收敛速度。

Conclusion: Brep2Shape方法有效缩小了抽象边界表示和直观形状表示之间的差距，在下游任务中性能优异。

Abstract: Boundary representation (B-rep) is the industry standard for computer-aided design (CAD). While deep learning shows promise in processing B-rep models, existing methods suffer from a representation gap: continuous approaches offer analytical precision but are visually abstract, whereas discrete methods provide intuitive clarity at the expense of geometric precision. To bridge this gap, we introduce Brep2Shape, a novel self-supervised pre-training method designed to align abstract boundary representations with intuitive shape representations. Our method employs a geometry-aware task where the model learns to predict dense spatial points from parametric Bézier control points, enabling the network to better understand physical manifolds derived from abstract coefficients. To enhance this alignment, we propose a Dual Transformer backbone with parallel streams that independently encode surface and curve tokens to capture their distinct geometric properties. Moreover, the topology attention is integrated to model the interdependencies between surfaces and curves, thereby maintaining topological consistency. Experimental results demonstrate that Brep2Shape offers significant scalability, achieving state-of-the-art accuracy and faster convergence across various downstream tasks.

</details>


### [279] [Learning Credal Ensembles via Distributionally Robust Optimization](https://arxiv.org/abs/2602.08470)
*Kaizheng Wang,Ghifari Adam Faza,Fabio Cuzzolin,Siu Lun Chau,David Moens,Hans Hallez*

Main category: cs.LG

TL;DR: 提出CreDRO方法，能捕获训练随机性和分布偏移带来的认知不确定性，在多个任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法将认知不确定性主要定义为训练初始化随机性导致的分歧，未反映更深层次来源，需解决该问题。

Method: 将认知不确定性定义为在训练和测试数据独立同分布假设不同松弛下训练的模型间的分歧，提出CreDRO方法，通过分布鲁棒优化学习一组合理模型。

Result: CreDRO能捕获训练随机性和潜在分布偏移导致的认知不确定性，在跨多个基准的分布外检测和医疗应用的选择性分类等任务中始终优于现有方法。

Conclusion: CreDRO方法有效且优越，能更好地量化和处理认知不确定性。

Abstract: Credal predictors are models that are aware of epistemic uncertainty and produce a convex set of probabilistic predictions. They offer a principled way to quantify predictive epistemic uncertainty (EU) and have been shown to improve model robustness in various settings. However, most state-of-the-art methods mainly define EU as disagreement caused by random training initializations, which mostly reflects sensitivity to optimization randomness rather than uncertainty from deeper sources. To address this, we define EU as disagreement among models trained with varying relaxations of the i.i.d. assumption between training and test data. Based on this idea, we propose CreDRO, which learns an ensemble of plausible models through distributionally robust optimization. As a result, CreDRO captures EU not only from training randomness but also from meaningful disagreement due to potential distribution shifts between training and test data. Empirical results show that CreDRO consistently outperforms existing credal methods on tasks such as out-of-distribution detection across multiple benchmarks and selective classification in medical applications.

</details>


### [280] [Active Learning Using Aggregated Acquisition Functions: Accuracy and Sustainability Analysis](https://arxiv.org/abs/2602.07440)
*Cédric Jung,Shirin Salehi,Anke Schmeink*

Main category: cs.LG

TL;DR: 本文实现并评估多种主动学习获取函数，提出六种聚合结构解决探索 - 利用困境，评估结果显示这些结构可降低计算成本并维持或提高准确率。


<details>
  <summary>Details</summary>
Motivation: 主动学习虽能降低标注成本和能耗，但现有获取函数存在探索 - 利用困境，常见的主动学习问题未得到解决，需要开发更可持续、节能的人工智能。

Method: 实现并评估多种先进的获取函数，分析其优缺点；提出六种聚合结构，如系列、并行等；在多种模型和数据集上评估提出的结构。

Result: 提出的聚合结构能减轻常见的主动学习问题，可降低计算成本并维持或提高准确率。如交替使用BALD和BADGE、先运行K - Centers再运行BALD等方法有良好表现。

Conclusion: 提出的聚合结构有潜力降低计算成本并维持或提高准确率，有助于开发更可持续、节能的人工智能。

Abstract: Active learning (AL) is a machine learning (ML) approach that strategically selects the most informative samples for annotation during training, aiming to minimize annotation costs. This strategy not only reduces labeling expenses but also results in energy savings during neural network training, thereby enhancing both data and energy efficiency. In this paper, we implement and evaluate various state-of-the-art acquisition functions, analyzing their accuracy and computational costs, while discussing the advantages and disadvantages of each method. Our findings reveal that representativity-based acquisition functions effectively explore the dataset but do not prioritize boundary decisions, whereas uncertainty-based acquisition functions focus on refining boundary decisions already identified by the neural network. This trade-off is known as the exploration-exploitation dilemma. To address this dilemma, we introduce six aggregation structures: series, parallel, hybrid, adaptive feedback, random exploration, and annealing exploration. Our aggregated acquisition functions alleviate common AL pathologies such as batch mode inefficiency and the cold start problem. Additionally, we focus on balancing accuracy and energy consumption, contributing to the development of more sustainable, energy-aware artificial intelligence (AI). We evaluate our proposed structures on various models and datasets. Our results demonstrate the potential of these structures to reduce computational costs while maintaining or even improving accuracy. Innovative aggregation approaches, such as alternating between acquisition functions such as BALD and BADGE, have shown robust results. Sequentially running functions like $K$-Centers followed by BALD has achieved the same performance goals with up to 12\% fewer samples, while reducing the acquisition cost by almost half.

</details>


### [281] [Rho-Perfect: Correlation Ceiling For Subjective Evaluation Datasets](https://arxiv.org/abs/2602.08552)
*Fredrik Cumlin*

Main category: cs.LG

TL;DR: 提出ρ - Perfect评估模型在主观评分数据集上可达到的最高相关性，展示其应用。


<details>
  <summary>Details</summary>
Motivation: 主观评分存在噪声限制模型 - 人类相关性，且可靠性问题很少被量化。

Method: 定义ρ - Perfect为完美预测器和人类评分的相关性，基于异方差噪声场景推导其估计值，并使用其平方估计重测相关性。

Result: 验证了ρ - Perfect估计值，在语音质量数据集上展示其可区分模型限制和数据质量问题。

Conclusion: ρ - Perfect是评估模型在主观评分数据集上表现的有效实用指标。

Abstract: Subjective ratings contain inherent noise that limits the model-human correlation, but this reliability issue is rarely quantified. In this paper, we present $ρ$-Perfect, a practical estimation of the highest achievable correlation of a model on subjectively rated datasets. We define $ρ$-Perfect to be the correlation between a perfect predictor and human ratings, and derive an estimate of the value based on heteroscedastic noise scenarios, a common occurrence in subjectively rated datasets. We show that $ρ$-Perfect squared estimates test-retest correlation and use this to validate the estimate. We demonstrate the use of $ρ$-Perfect on a speech quality dataset and show how the measure can distinguish between model limitations and data quality issues.

</details>


### [282] [Proximal Action Replacement for Behavior Cloning Actor-Critic in Offline Reinforcement Learning](https://arxiv.org/abs/2602.07441)
*Jinzong Dong,Wei Huang,Jianshu Zhang,Zhuo Chen,Xinzhe Yuan,Qinying Gu,Zhaohui Jiang,Nanyang Ye*

Main category: cs.LG

TL;DR: 文章分析了基于行为克隆正则化的离线强化学习方法的局限性，提出近端动作替换（PAR）方法突破性能上限，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 基于行为克隆正则化的离线强化学习方法存在性能上限，当数据集中动作次优时，盲目模仿会限制智能体对高价值区域的探索。

Method: 正式分析基于行为克隆正则化的演员 - 评论家优化的收敛特性，提出近端动作替换（PAR）方法，该方法可将低价值动作替换为稳定演员生成的高价值动作。

Result: 在离线强化学习基准测试中，PAR 持续提升性能，与 TD3+BC 结合时接近当前最优水平。

Conclusion: PAR 是一种有效的方法，可突破基于行为克隆正则化的离线强化学习方法的性能上限，且与多种 BC 正则化范式兼容。

Abstract: Offline reinforcement learning (RL) optimizes policies from a previously collected static dataset and is an important branch of RL. A popular and promising approach is to regularize actor-critic methods with behavior cloning (BC), which yields realistic policies and mitigates bias from out-of-distribution actions, but can impose an often-overlooked performance ceiling: when dataset actions are suboptimal, indiscriminate imitation structurally prevents the actor from fully exploiting high-value regions suggested by the critic, especially in later training when imitation is already dominant. We formally analyzed this limitation by investigating convergence properties of BC-regularized actor-critic optimization and verified it on a controlled continuous bandit task. To break this ceiling, we propose proximal action replacement (PAR), a plug-and-play training sample replacer that progressively replaces low-value actions with high-value actions generated by a stable actor, broadening the action exploration space while reducing the impact of low-value data. PAR is compatible with multiple BC regularization paradigms. Extensive experiments across offline RL benchmarks show that PAR consistently improves performance and approaches state-of-the-art when combined with the basic TD3+BC.

</details>


### [283] [CauScale: Neural Causal Discovery at Scale](https://arxiv.org/abs/2602.08629)
*Bo Peng,Sirui Chen,Jiaguo Tian,Yu Qiao,Chaochao Lu*

Main category: cs.LG

TL;DR: 提出 CauScale 神经架构用于高效因果发现，可扩展到 1000 节点图，提升时空效率和准确率。


<details>
  <summary>Details</summary>
Motivation: 现有因果发现方法在扩展到大型图时面临时空效率瓶颈。

Method: 采用简化单元压缩数据嵌入提升时间效率，采用绑定注意力权重避免维护特定轴注意力图提升空间效率，采用两流设计保证高准确率。

Result: 训练能扩展到 500 节点图，在分布内和分布外数据分别达到 99.6%和 84.4%的 mAP，推理速度比先前方法快 4 - 13000 倍。

Conclusion: CauScale 能有效解决大规模图的因果发现时空效率问题。

Abstract: Causal discovery is essential for advancing data-driven fields such as scientific AI and data analysis, yet existing approaches face significant time- and space-efficiency bottlenecks when scaling to large graphs. To address this challenge, we present CauScale, a neural architecture designed for efficient causal discovery that scales inference to graphs with up to 1000 nodes. CauScale improves time efficiency via a reduction unit that compresses data embeddings and improves space efficiency by adopting tied attention weights to avoid maintaining axis-specific attention maps. To keep high causal discovery accuracy, CauScale adopts a two-stream design: a data stream extracts relational evidence from high-dimensional observations, while a graph stream integrates statistical graph priors and preserves key structural signals. CauScale successfully scales to 500-node graphs during training, where prior work fails due to space limitations. Across testing data with varying graph scales and causal mechanisms, CauScale achieves 99.6% mAP on in-distribution data and 84.4% on out-of-distribution data, while delivering 4-13,000 times inference speedups over prior methods. Our project page is at https://github.com/OpenCausaLab/CauScale.

</details>


### [284] [The Theory and Practice of MAP Inference over Non-Convex Constraints](https://arxiv.org/abs/2602.08681)
*Leander Kurscheidt,Gabriele Masina,Roberto Sebastiani,Antonio Vergari*

Main category: cs.LG

TL;DR: 本文研究概率机器学习系统在代数约束下的预测问题，提出精确高效推理方法和通用策略，实验表明方法优于无约束基线。


<details>
  <summary>Details</summary>
Motivation: 在安全关键场景中，概率机器学习系统需在代数约束下做预测，但现实约束非凸且密度非（对数）凹，高效可靠计算约束最大后验（MAP）预测极具挑战。

Method: 先研究连续变量约束MAP推理可精确高效执行的条件并设计消息传递算法，再设计通用约束MAP策略，将定义域划分为凸可行区域并结合数值约束优化。

Result: 在合成和真实基准上评估，方法优于无约束基线，可扩展到现有精确求解器难以处理的复杂密度。

Conclusion: 所提方法能有效解决概率机器学习系统在代数约束下的预测问题。

Abstract: In many safety-critical settings, probabilistic ML systems have to make predictions subject to algebraic constraints, e.g., predicting the most likely trajectory that does not cross obstacles.
  These real-world constraints are rarely convex, nor the densities considered are (log-)concave.
  This makes computing this constrained maximum a posteriori (MAP) prediction efficiently and reliably extremely challenging.
  In this paper, we first investigate under which conditions we can perform constrained MAP inference over continuous variables exactly and efficiently and devise a scalable message-passing algorithm for this tractable fragment.
  Then, we devise a general constrained MAP strategy that interleaves partitioning the domain into convex feasible regions with numerical constrained optimization.
  We evaluate both methods on synthetic and real-world benchmarks, showing our %
  approaches outperform constraint-agnostic baselines, and scale to complex densities intractable for SoTA exact solvers.

</details>


### [285] [On the Importance of a Multi-Scale Calibration for Quantization](https://arxiv.org/abs/2602.07465)
*Seungwoo Son,Ingyu Seong,Junhan Kim,Hyemi Jang,Yongkweon Jeon*

Main category: cs.LG

TL;DR: 提出MaCa方法用于长度感知的Hessian构建，在LLM低比特量化中提升准确性。


<details>
  <summary>Details</summary>
Motivation: 传统后训练量化（PTQ）使用固定长度校准集，忽略了LLM输入的可变长度特性，影响量化结果。

Method: 提出MaCa方法，将多尺度序列长度信息纳入Hessian估计，并将每个序列作为独立样本进行正则化。

Result: 在Qwen3、Gemma3、LLaMA3等模型实验中，MaCa在低比特量化下持续提高准确性。

Conclusion: MaCa方法轻量且与现有PTQ框架兼容，是首个系统强调多尺度校准在LLM量化中作用的工作。

Abstract: Post-training quantization (PTQ) is a cornerstone for efficiently deploying large language models (LLMs), where a small calibration set critically affects quantization performance. However, conventional practices rely on random sequences of fixed length, overlooking the variable-length nature of LLM inputs. Input length directly influences the activation distribution and, consequently, the weight importance captured by the Hessian, which in turn affects quantization outcomes. As a result, Hessian estimates derived from fixed-length calibration may fail to represent the true importance of weights across diverse input scenarios. We propose MaCa (Matryoshka Calibration), a simple yet effective method for length-aware Hessian construction. MaCa (i) incorporates multi-scale sequence length information into Hessian estimation and (ii) regularizes each sequence as an independent sample, yielding a more stable and fruitful Hessian for accurate quantization. Experiments on state-of-the-art LLMs (e.g., Qwen3, Gemma3, LLaMA3) demonstrate that MaCa consistently improves accuracy under low bit quantization, offering a lightweight enhancement compatible with existing PTQ frameworks. To the best of our knowledge, this is the first work to systematically highlight the role of multi-scale calibration in LLM quantization.

</details>


### [286] [Data Reconstruction: Identifiability and Optimization with Sample Splitting](https://arxiv.org/abs/2602.08723)
*Yujie Shen,Zihan Wang,Jian Qian,Qi Lei*

Main category: cs.LG

TL;DR: 该工作聚焦训练数据重构中KKT方程解的可识别性与优化问题，给出可识别性充分条件，提出样本分割优化方法并验证其提升重构性能。


<details>
  <summary>Details</summary>
Motivation: 现有训练数据重构方法不清楚KKT方程何时有唯一解，以及在可识别的情况下如何通过优化可靠地恢复解。

Method: 在可识别性方面，探讨具有多项式激活函数的两层网络的KKT系统唯一确定训练数据的充分条件；在优化方面，引入样本分割这一适用于一般重构目标的曲率感知细化步骤。

Result: 实验表明，用样本分割增强现有的几种重构方法能持续提高重构性能。

Conclusion: 所提方法能有效解决训练数据重构中的可识别性和优化问题，提升重构性能。

Abstract: Training data reconstruction from KKT conditions has shown striking empirical success, yet it remains unclear when the resulting KKT equations have unique solutions and, even in identifiable regimes, how to reliably recover solutions by optimization. This work hereby focuses on these two complementary questions: identifiability and optimization. On the identifiability side, we discuss the sufficient conditions for KKT system of two-layer networks with polynomial activations to uniquely determine the training data, providing a theoretical explanation of when and why reconstruction is possible. On the optimization side, we introduce sample splitting, a curvature-aware refinement step applicable to general reconstruction objectives (not limited to KKT-based formulations): it creates additional descent directions to escape poor stationary points and refine solutions. Experiments demonstrate that augmenting several existing reconstruction methods with sample splitting consistently improves reconstruction performance.

</details>


### [287] [Bipartite Graph Attention-based Clustering for Large-scale scRNA-seq Data](https://arxiv.org/abs/2602.07475)
*Zhuomin Liang,Liang Bai,Xian Yang*

Main category: cs.LG

TL;DR: 提出用于scRNA - seq数据的BGFormer聚类模型，解决现有方法复杂度高问题，实验证明其有效性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有scRNA - seq聚类方法计算和空间复杂度高，限制在大规模数据集的应用。

Method: 提出BGFormer模型，引入可学习的锚定令牌作为共享参考点，采用二分图注意力机制学习细胞和锚定令牌的相似度。

Result: 在多个大规模scRNA - seq数据集上的实验表明，BGFormer计算复杂度为线性，具有有效性和可扩展性。

Conclusion: BGFormer能有效解决现有方法在大规模数据集应用的问题，适用于大规模scRNA - seq数据聚类。

Abstract: scRNA-seq clustering is a critical task for analyzing single-cell RNA sequencing (scRNA-seq) data, as it groups cells with similar gene expression profiles. Transformers, as powerful foundational models, have been applied to scRNA-seq clustering. Their self-attention mechanism automatically assigns higher attention weights to cells within the same cluster, enhancing the distinction between clusters. Existing methods for scRNA-seq clustering, such as graph transformer-based models, treat each cell as a token in a sequence. Their computational and space complexities are $\mathcal{O}(n^2)$ with respect to the number of cells, limiting their applicability to large-scale scRNA-seq datasets.To address this challenge, we propose a Bipartite Graph Transformer-based clustering model (BGFormer) for scRNA-seq data. We introduce a set of learnable anchor tokens as shared reference points to represent the entire dataset. A bipartite graph attention mechanism is introduced to learn the similarity between cells and anchor tokens, bringing cells of the same class closer together in the embedding space. BGFormer achieves linear computational complexity with respect to the number of cells, making it scalable to large datasets. Experimental results on multiple large-scale scRNA-seq datasets demonstrate the effectiveness and scalability of BGFormer.

</details>


### [288] [Positive Distribution Shift as a Framework for Understanding Tractable Learning](https://arxiv.org/abs/2602.08907)
*Marko Medvedev,Idan Attias,Elisabetta Cornacchia,Theodor Misiakiewicz,Gal Vardi,Nathan Srebro*

Main category: cs.LG

TL;DR: 研究分布偏移，提出正分布偏移（PDS）概念，指出选择合适训练分布可让学习更简单，还探讨了其计算优势并与成员查询学习建立联系。


<details>
  <summary>Details</summary>
Motivation: 传统观点认为分布偏移会使学习变难，作者想提出分布偏移也有积极作用的观点。

Method: 形式化不同变体的正分布偏移，展示特定难学习类别在PDS下易学习，并与成员查询学习建立联系。

Result: 证明选择合适的训练分布D'(x)可让分布偏移产生积极效果，使计算难题变得可处理。

Conclusion: 正分布偏移视角在当代机器学习中很重要，能让计算难题变得易于解决。

Abstract: We study a setting where the goal is to learn a target function f(x) with respect to a target distribution D(x), but training is done on i.i.d. samples from a different training distribution D'(x), labeled by the true target f(x). Such a distribution shift (here in the form of covariate shift) is usually viewed negatively, as hurting or making learning harder, and the traditional distribution shift literature is mostly concerned with limiting or avoiding this negative effect. In contrast, we argue that with a well-chosen D'(x), the shift can be positive and make learning easier -- a perspective called Positive Distribution Shift (PDS). Such a perspective is central to contemporary machine learning, where much of the innovation is in finding good training distributions D'(x), rather than changing the training algorithm. We further argue that the benefit is often computational rather than statistical, and that PDS allows computationally hard problems to become tractable even using standard gradient-based training. We formalize different variants of PDS, show how certain hard classes are easily learnable under PDS, and make connections with membership query learning.

</details>


### [289] [AI-Driven Predictive Modelling for Groundwater Salinization in Israel](https://arxiv.org/abs/2602.07478)
*Laxmi Pandey,Ariel Meroz,Ben Cheng,Ankita Manekar,Abhijit Mukherjee,Meirav Cohen,Adway Mitra*

Main category: cs.LG

TL;DR: 本文旨在理解地下水盐化成因，用多种机器学习模型和分析方法确定以色列地下水盐化的关键驱动因素，为应对盐化提供策略。


<details>
  <summary>Details</summary>
Motivation: 解决全球多地地下水盐度增加和污染导致水资源退化的问题，全面理解地下水盐化的成因。

Method: 整合不同协变量数据集，建立基于机器学习的地下水盐度预测模型，运用递归特征消除、全局敏感性分析、可解释人工智能的SHAP，以及双重机器学习进行因果分析。

Result: 确定了气象、地质和人为方面的关键协变量是以色列地下水盐度的影响驱动因素，XAI分析表明处理后的废水是重要人为驱动因素。

Conclusion: 该方法深入了解国家层面的盐化机制，降低AI模型不确定性，强调需制定针对性策略应对盐化。

Abstract: Increasing salinity and contamination of groundwater is a serious issue in many parts of the world, causing degradation of water resources. The aim of this work is to form a comprehensive understanding of groundwater salinization underlying causal factors and identify important meteorological, geological and anthropogenic drivers of salinity. We have integrated different datasets of potential covariates, to create a robust framework for machine learning based predictive models including Random Forest (RF), XGBoost, Neural network, Long Short-Term Memory (LSTM), convolution neural network (CNN) and linear regression (LR), of groundwater salinity. Additionally, Recursive Feature Elimination (RFE) followed by Global sensitivity analysis (GSA) and Explainable AI (XAI) based SHapley Additive exPlanations (SHAP) were used to estimate the importance scores and find insights into the drivers of salinization. We also did causality analysis via Double machine learning using various predictive models. From these analyses, key meteorological (Precipitation, Temperature), geological (Distance from river, Distance to saline body, TWI, Shoreline distance), and anthropogenic (Area of agriculture field, Treated Wastewater) covariates are identified to be influential drivers of groundwater salinity across Israel. XAI analysis also identified Treated Wastewater (TWW) as an essential anthropogenic driver of salinity, its significance being context-dependent but critical in vulnerable hydro-climatic environment. Our approach provides deeper insight into global salinization mechanisms at country scale, reducing AI model uncertainty and highlighting the need for tailored strategies to address salinity.

</details>


### [290] [GEMSS: A Variational Bayesian Method for Discovering Multiple Sparse Solutions in Classification and Regression Problems](https://arxiv.org/abs/2602.08913)
*Kateřina Henclová,Václav Šmídl*

Main category: cs.LG

TL;DR: 提出GEMSS框架同时发现多个不同稀疏特征组合，在多种任务实验中验证其有效性，有可用软件包。


<details>
  <summary>Details</summary>
Motivation: 在欠定和高度相关机制下选择可解释特征集是挑战，传统方法只能找到单一解，需找出多种稀疏子集以洞察潜在机制。

Method: 提出GEMSS框架，采用结构化尖峰 - 板条先验实现稀疏性，用高斯混合近似后验，通过基于Jaccard的惩罚控制解的多样性，用随机梯度下降优化目标函数。

Result: 在128个合成实验中验证，GEMSS能有效扩展到高维、小样本情况，可处理连续目标、缺失数据，对类别不平衡和高斯噪声有鲁棒性。

Conclusion: GEMSS是一个有效的发现多个不同稀疏特征组合的方法，且有易用的软件包可供使用。

Abstract: Selecting interpretable feature sets in underdetermined ($n \ll p$) and highly correlated regimes constitutes a fundamental challenge in data science, particularly when analyzing physical measurements. In such settings, multiple distinct sparse subsets may explain the response equally well. Identifying these alternatives is crucial for generating domain-specific insights into the underlying mechanisms, yet conventional methods typically isolate a single solution, obscuring the full spectrum of plausible explanations.
  We present GEMSS (Gaussian Ensemble for Multiple Sparse Solutions), a variational Bayesian framework specifically designed to simultaneously discover multiple, diverse sparse feature combinations. The method employs a structured spike-and-slab prior for sparsity, a mixture of Gaussians to approximate the intractable multimodal posterior, and a Jaccard-based penalty to further control solution diversity. Unlike sequential greedy approaches, GEMSS optimizes the entire ensemble of solutions within a single objective function via stochastic gradient descent.
  The method is validated on a comprehensive benchmark comprising 128 synthetic experiments across classification and regression tasks. Results demonstrate that GEMSS scales effectively to high-dimensional settings ($p=5000$) with sample size as small as $n = 50$, generalizes seamlessly to continuous targets, handles missing data natively, and exhibits remarkable robustness to class imbalance and Gaussian noise.
  GEMSS is available as a Python package 'gemss' at PyPI. The full GitHub repository at https://github.com/kat-er-ina/gemss/ also includes a free, easy-to-use application suitable for non-coders.

</details>


### [291] [ODELoRA: Training Low-Rank Adaptation by Solving Ordinary Differential Equations](https://arxiv.org/abs/2602.07479)
*Yihang Gao,Vincent Y. F. Tan*

Main category: cs.LG

TL;DR: 提出基于常微分方程（ODE）的低秩适配（LoRA）训练方法ODELoRA，统一ODE视角理解设计LoRA算法，理论证明收敛性，实验验证优势。


<details>
  <summary>Details</summary>
Motivation: 经典LoRA训练方法单独处理低秩因子矩阵的解耦优化方案理论和实践上欠佳，未充分利用LoRA参数化的内在结构。

Method: 提出以常微分方程形式为LoRA因子矩阵设计优化动态的ODELoRA方法，采用成熟的时间离散化方案跟踪轨迹。

Result: 在强凸目标下证明一定离散化方案的线性收敛性并拓展到矩阵传感，证实稳定特征学习能力；矩阵传感任务实验证实线性收敛，物理信息神经网络训练实验显示优于现有基线，训练稳定性好。

Conclusion: ODELoRA为LoRA训练算法提供统一的基于ODE的视角，在理论和实验上都有良好表现，尤其在训练稳定性方面有优势。

Abstract: Low-rank adaptation (LoRA) has emerged as a widely adopted parameter-efficient fine-tuning method in deep transfer learning, due to its reduced number of trainable parameters and lower memory requirements enabled by Burer-Monteiro factorization on adaptation matrices. However, classical LoRA training methods treat the low-rank factor matrices individually and optimize them using standard gradient-based algorithms. Such decoupled optimization schemes are theoretically and empirically suboptimal, as they fail to fully exploit the intrinsic structure of the LoRA parameterization. In this work, we propose a novel continuous-time optimization dynamic for LoRA factor matrices in the form of an ordinary differential equation (ODE) that emulates the gradient flow of full fine-tuning on the balanced manifold. We term this approach ODELoRA. To faithfully track the trajectories of ODELoRA, we adopt well-established and theoretically grounded time-discretization schemes, including Euler and Runge--Kutta methods. Our framework provides a unified ODE-based perspective for understanding and designing LoRA training algorithms. We establish linear convergence of the proposed method under strongly convex objectives for certain discretization schemes under mild conditions, and further extend our analysis to the matrix sensing setting. Moreover, we show that ODELoRA achieves stable feature learning, a property that is crucial for training deep neural networks at different scales of problem dimensionality. Empirical results on matrix sensing tasks confirm the derived linear convergence behavior, and experiments on training physics-informed neural networks further demonstrate the superiority of ODELoRA over existing baselines, especially in the training stability.

</details>


### [292] [Hyperparameter Transfer Laws for Non-Recurrent Multi-Path Neural Networks](https://arxiv.org/abs/2602.07494)
*Shenxi Wu,Haosong Zhang,Xingjian Ma,Shirui Bian,Yichi Zhang,Xi Chen,Wei Lin*

Main category: cs.LG

TL;DR: 提出有效深度概念，发现最优学习率与有效深度遵循 -3/2 幂律，实现学习率跨深度和宽度的零样本迁移。


<details>
  <summary>Details</summary>
Motivation: 深层现代架构训练成本高，超参数迁移比重复调优更优，但深度缩放对现代架构的影响理解不足。

Method: 引入基于图的有效深度概念，在稳定初始化和最大更新准则下进行分析。

Result: 实验证实最优学习率与有效深度遵循 -3/2 幂律，能可靠实现学习率跨深度和宽度的零样本迁移。

Conclusion: 将深度缩放转化为可预测的超参数迁移问题。

Abstract: Deeper modern architectures are costly to train, making hyperparameter transfer preferable to expensive repeated tuning. Maximal Update Parametrization ($μ$P) helps explain why many hyperparameters transfer across width. Yet depth scaling is less understood for modern architectures, whose computation graphs contain multiple parallel paths and residual aggregation. To unify various non-recurrent multi-path neural networks such as CNNs, ResNets, and Transformers, we introduce a graph-based notion of effective depth. Under stabilizing initializations and a maximal-update criterion, we show that the optimal learning rate decays with effective depth following a universal -3/2 power law. Here, the maximal-update criterion maximizes the typical one-step representation change at initialization without causing instability, and effective depth is the minimal path length from input to output, counting layers and residual additions. Experiments across diverse architectures confirm the predicted slope and enable reliable zero-shot transfer of learning rates across depths and widths, turning depth scaling into a predictable hyperparameter-transfer problem.

</details>


### [293] [CoMI-IRL: Contrastive Multi-Intention Inverse Reinforcement Learning](https://arxiv.org/abs/2602.07496)
*Antonio Mone,Frans A. Oliehoek,Luciano Cavalcante Siebert*

Main category: cs.LG

TL;DR: 提出基于Transformer的无监督框架CoMI - IRL，在无先验知识下优于现有方法，可进行行为关系可视化及适应未见行为。


<details>
  <summary>Details</summary>
Motivation: 现有深度生成多意图IRL方法依赖专家知识确定行为模式数量，限制对新行为适应性，且只能分析学习到的奖励。

Method: 提出基于Transformer的无监督框架CoMI - IRL，将行为表示和聚类与下游奖励学习解耦。

Result: 实验表明CoMI - IRL在无$K^*$先验知识或标签情况下优于现有方法。

Conclusion: CoMI - IRL可实现行为关系的可视化解释，且无需完全重新训练就能适应未见行为。

Abstract: Inverse Reinforcement Learning (IRL) seeks to infer reward functions from expert demonstrations. When demonstrations originate from multiple experts with different intentions, the problem is known as Multi-Intention IRL (MI-IRL). Recent deep generative MI-IRL approaches couple behavior clustering and reward learning, but typically require prior knowledge of the number of true behavioral modes $K^*$. This reliance on expert knowledge limits their adaptability to new behaviors, and only enables analysis related to the learned rewards, and not across the behavior modes used to train them. We propose Contrastive Multi-Intention IRL (CoMI-IRL), a transformer-based unsupervised framework that decouples behavior representation and clustering from downstream reward learning. Our experiments show that CoMI-IRL outperforms existing approaches without a priori knowledge of $K^*$ or labels, while allowing for visual interpretation of behavior relationships and adaptation to unseen behavior without full retraining.

</details>


### [294] [PALMS: Pavlovian Associative Learning Models Simulator](https://arxiv.org/abs/2602.07519)
*Martin Fixman,Alessandro Abati,Julián Jiménez Nimmo,Sean Lim,Esther Mondragón*

Main category: cs.LG

TL;DR: 本文介绍Python环境下的Pavlovian条件反射实验模拟器PALMS，它集成多种模型，功能强大且高效，还给出使用示例和开源代码。


<details>
  <summary>Details</summary>
Motivation: 为理论发展和完善的模拟环节提供一个工具，方便研究者进行巴甫洛夫条件反射实验模拟。

Method: 开发PALMS模拟器，集成多种注意力学习方法和模型，提供图形界面，支持输入实验设计、模拟多刺激实验等。

Result: PALMS能高效运行，可即时可视化结果，支持不同模型预测的快速精确比较，能保存图形显示和导出模拟数据。

Conclusion: PALMS是一个功能强大、高效且开源的巴甫洛夫条件反射实验模拟器，可助力相关研究。

Abstract: Simulations are an indispensable step in the cycle of theory development and refinement, helping researchers formulate precise definitions, generate models, and make accurate predictions. This paper introduces the Pavlovian Associative Learning Models Simulator (PALMS), a Python environment to simulate Pavlovian conditioning experiments. In addition to the canonical Rescorla-Wagner model, PALMS incorporates several attentional learning approaches, including Pearce-Kaye-Hall, Mackintosh Extended, Le Pelley's Hybrid, and a novel extension of the Rescorla-Wagner model with a unified variable learning rate that integrates Mackintosh's and Pearce and Hall's opposing conceptualisations. The simulator's graphical interface allows for the input of entire experimental designs in an alphanumeric format, akin to that used by experimental neuroscientists. Moreover, it uniquely enables the simulation of experiments involving hundreds of stimuli, as well as the computation of configural cues and configural-cue compounds across all models, thereby considerably expanding their predictive capabilities. PALMS operates efficiently, providing instant visualisation of results, supporting rapid, precise comparisons of various models' predictions within a single architecture and environment. Furthermore, graphic displays can be easily saved, and simulated data can be exported to spreadsheets. To illustrate the simulator's capabilities and functionalities, we provide a detailed description of the software and examples of use, reproducing published experiments in the associative learning literature. PALMS is licensed under the open-source GNU Lesser General Public License 3.0. The simulator source code and the latest multiplatform release build are accessible as a GitHub repository at https://github.com/cal-r/PALMS-Simulator

</details>


### [295] [Pareto-guided Pipeline for Distilling Featherweight AI Agents in Mobile MOBA Games](https://arxiv.org/abs/2602.07521)
*Xionghui Yang,Bozhou Chen,Yunlong Lu,Yongyi Wang,Lingfeng Li,Lanxiao Huang,Lin Liu,Wenjun Wang,Meng Meng,Xia Lin,Wenxin Li*

Main category: cs.LG

TL;DR: 本文提出帕累托最优引导管道和适合移动执行的高效学生架构搜索空间，解决游戏AI在移动设备部署难题，蒸馏模型效率显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有强大游戏AI难以部署在移动设备，此前未系统研究大规模游戏AI与实际设备部署的衔接问题。

Method: 提出帕累托最优引导管道，设计适合移动执行的高效学生架构搜索空间。

Result: 蒸馏模型推理速度快12.4倍（每帧低于0.5ms），能源效率提高15.6倍（每场游戏低于0.5mAh），对阵原教师模型胜率40.32%。

Conclusion: 所提方法能系统探索性能与效率的权衡，有效解决游戏AI在移动设备的部署难题。

Abstract: Recent advances in game AI have demonstrated the feasibility of training agents that surpass top-tier human professionals in complex environments such as Honor of Kings (HoK), a leading mobile multiplayer online battle arena (MOBA) game. However, deploying such powerful agents on mobile devices remains a major challenge. On one hand, the intricate multi-modal state representation and hierarchical action space of HoK demand large, sophisticated policy networks that are inherently difficult to compress into lightweight forms. On the other hand, production deployment requires high-frequency inference under strict energy and latency constraints on mobile platform. To the best of our knowledge, bridging large-scale game AI and practical on-device deployment has not been systematically studied. In this work, we propose a Pareto optimality guided pipeline and design a high-efficiency student architecture search space tailored for mobile execution, enabling systematic exploration of the trade-off between performance and efficiency. Experimental results demonstrate that the distilled model achieves remarkable efficiency, including an $12.4\times$ faster inference speed (under 0.5ms per frame) and a $15.6\times$ improvement in energy efficiency (under 0.5mAh per game), while retaining a 40.32% win rate against the original teacher model.

</details>


### [296] [MedVerse: Efficient and Reliable Medical Reasoning via DAG-Structured Parallel Execution](https://arxiv.org/abs/2602.07529)
*Jianwen Chen,Xinyu Yang,Peng Xia,Arian Azarang,Yueh Z Lee,Gang Li,Hongtu Zhu,Yun Li,Beidi Chen,Huaxiu Yao*

Main category: cs.LG

TL;DR: 现有大语言模型在医疗推理任务有局限性，本文提出MedVerse框架，经评估有性能提升和效率优势。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型顺序自回归解码限制复杂医疗问题推理效率和可靠性，需改进。

Method: 提出基于Petri网理论的MedVerse框架，采用全栈设计，包括数据创建、架构设计和系统开发。

Result: MedVerse使通用大语言模型性能提升8.9%，与专业医疗大语言模型性能相当，推理延迟降低1.3倍，生成吞吐量提高1.7倍。

Conclusion: MedVerse框架在复杂医疗推理中有效，通过并行解码提升效率和性能。

Abstract: Large language models (LLMs) have demonstrated strong performance and rapid progress in a wide range of medical reasoning tasks. However, their sequential autoregressive decoding forces inherently parallel clinical reasoning, such as differential diagnosis, into a single linear reasoning path, limiting both efficiency and reliability for complex medical problems. To address this, we propose MedVerse, a reasoning framework for complex medical inference that reformulates medical reasoning as a parallelizable directed acyclic graph (DAG) process based on Petri net theory. The framework adopts a full-stack design across data, model architecture, and system execution. For data creation, we introduce the MedVerse Curator, an automated pipeline that synthesizes knowledge-grounded medical reasoning paths and transforms them into Petri net-structured representations. At the architectural level, we propose a topology-aware attention mechanism with adaptive position indices that supports parallel reasoning while preserving logical consistency. Systematically, we develop a customized inference engine that supports parallel execution without additional overhead. Empirical evaluations show that MedVerse improves strong general-purpose LLMs by up to 8.9%. Compared to specialized medical LLMs, MedVerse achieves comparable performance while delivering a 1.3x reduction in inference latency and a 1.7x increase in generation throughput, enabled by its parallel decoding capability.

</details>


### [297] [Compact Conformal Subgraphs](https://arxiv.org/abs/2602.07530)
*Sreenivas Gollapudi,Kostas Kollias,Kamesh Munagala,Aravindan Vijayaraghavan*

Main category: cs.LG

TL;DR: 提出图基共形压缩框架以解决结构化领域共形预测集过大问题，设计近似算法，证明相关性质，通过模拟验证算法。


<details>
  <summary>Details</summary>
Motivation: 共形预测在结构化领域常产生过大预测集，需一种方法减小其规模并保持统计有效性。

Method: 引入图基共形压缩框架，将压缩问题转化为超图中带权最密k - 子图问题，设计近似算法，证明放松条件满足单调性。

Result: 桥梁连接高效共形预测与组合图压缩，提供统计有效性和压缩性的严格保证，指出算法能有效近似的特殊情况，通过模拟验证算法。

Conclusion: 提出的图基共形压缩框架有效，能在结构化领域实现高效共形预测并保证统计有效性。

Abstract: Conformal prediction provides rigorous, distribution-free uncertainty guarantees, but often yields prohibitively large prediction sets in structured domains such as routing, planning, or sequential recommendation. We introduce "graph-based conformal compression", a framework for constructing compact subgraphs that preserve statistical validity while reducing structural complexity. We formulate compression as selecting a smallest subgraph capturing a prescribed fraction of the probability mass, and reduce to a weighted version of densest $k$-subgraphs in hypergraphs, in the regime where the subgraph has a large fraction of edges. We design efficient approximation algorithms that achieve constant factor coverage and size trade-offs. Crucially, we prove that our relaxation satisfies a monotonicity property, derived from a connection to parametric minimum cuts, which guarantees the nestedness required for valid conformal guarantees. Our results on the one hand bridge efficient conformal prediction with combinatorial graph compression via monotonicity, to provide rigorous guarantees on both statistical validity, and compression or size. On the other hand, they also highlight an algorithmic regime, distinct from classical densest-$k$-subgraph hardness settings, where the problem can be approximated efficiently. We finally validate our algorithmic approach via simulations for trip planning and navigation, and compare to natural baselines.

</details>


### [298] [Enhancing Time Series Classification with Diversity-Driven Neural Network Ensembles](https://arxiv.org/abs/2602.07579)
*Javidan Abdullayev,Maxime Devanne,Cyril Meyer,Ali Ismail-Fawaz,Jonathan Weber,Germain Forestier*

Main category: cs.LG

TL;DR: 提出多样性驱动的集成学习框架用于时间序列分类，在UCR数据集上实现SOTA性能且模型更少。


<details>
  <summary>Details</summary>
Motivation: 现有基于神经网络的时间序列分类集成方法未明确促进多样性，导致特征表示冗余，限制集成效果。

Method: 引入多样性驱动的集成学习框架，采用特征正交损失的去相关学习策略，确保模型捕捉互补信息。

Result: 在UCR档案的128个数据集上评估，实现了SOTA性能，且使用更少的模型。

Conclusion: 该方法比传统基于神经网络的集成方法更高效、可扩展。

Abstract: Ensemble methods have played a crucial role in achieving state-of-the-art (SOTA) performance across various machine learning tasks by leveraging the diversity of features learned by individual models. In Time Series Classification (TSC), ensembles have proven highly effective whether based on neural networks (NNs) or traditional methods like HIVE-COTE. However most existing NN-based ensemble methods for TSC train multiple models with identical architectures and configurations. These ensembles aggregate predictions without explicitly promoting diversity which often leads to redundant feature representations and limits the benefits of ensembling. In this work, we introduce a diversity-driven ensemble learning framework that explicitly encourages feature diversity among neural network ensemble members. Our approach employs a decorrelated learning strategy using a feature orthogonality loss applied directly to the learned feature representations. This ensures that each model in the ensemble captures complementary rather than redundant information. We evaluate our framework on 128 datasets from the UCR archive and show that it achieves SOTA performance with fewer models. This makes our method both efficient and scalable compared to conventional NN-based ensemble approaches.

</details>


### [299] [Unified Biomolecular Trajectory Generation via Pretrained Variational Bridge](https://arxiv.org/abs/2602.07588)
*Ziyang Yu,Wenbing Huang,Yang Liu*

Main category: cs.LG

TL;DR: 提出PVB模型解决MD模拟计算成本高问题，实验证明其能有效生成动力学。


<details>
  <summary>Details</summary>
Motivation: MD模拟计算成本高，现有深度生成模型存在泛化差、未充分利用结构信息问题。

Method: 采用编码器 - 解码器架构的PVB模型，统一单结构和配对轨迹数据训练，对蛋白 - 配体复合物引入基于强化学习的优化。

Result: PVB能忠实重现MD的热力学和动力学观测值，实现稳定高效的生成动力学。

Conclusion: PVB模型在解决MD模拟计算成本问题上表现良好，可用于高效生成动力学。

Abstract: Molecular Dynamics (MD) simulations provide a fundamental tool for characterizing molecular behavior at full atomic resolution, but their applicability is severely constrained by the computational cost. To address this, a surge of deep generative models has recently emerged to learn dynamics at coarsened timesteps for efficient trajectory generation, yet they either generalize poorly across systems or, due to limited molecular diversity of trajectory data, fail to fully exploit structural information to improve generative fidelity. Here, we present the Pretrained Variational Bridge (PVB) in an encoder-decoder fashion, which maps the initial structure into a noised latent space and transports it toward stage-specific targets through augmented bridge matching. This unifies training on both single-structure and paired trajectory data, enabling consistent use of cross-domain structural knowledge across training stages. Moreover, for protein-ligand complexes, we further introduce a reinforcement learning-based optimization via adjoint matching that speeds progression toward the holo state, which supports efficient post-optimization of docking poses. Experiments on proteins and protein-ligand complexes demonstrate that PVB faithfully reproduces thermodynamic and kinetic observables from MD while delivering stable and efficient generative dynamics.

</details>


### [300] [Astro: Activation-guided Structured Regularization for Outlier-Robust LLM Post-Training Quantization](https://arxiv.org/abs/2602.07596)
*Xi Chen,Ming Li,Junxi Li,Changsheng Li,Peisong Wang,Lizhong Ding,Ye Yuan,Guoren Wang*

Main category: cs.LG

TL;DR: 提出用于大语言模型仅权重训练后量化的Astro框架，可高效抑制异常值且不牺牲精度，实验表现好。


<details>
  <summary>Details</summary>
Motivation: 现有仅权重训练后量化方法在抑制异常值时存在不足，如抑制不充分或部署效率低。

Method: 利用大语言模型常收敛到平坦极小值点这一特性，提出激活引导的结构化正则化框架Astro，主动重构权重。

Result: Astro无推理延迟，与主流量化方法正交，在LLaMA - 2 - 7B上比复杂学习旋转方法表现好且量化时间约为其三分之一。

Conclusion: Astro能以硬件友好且高效的方式抑制异常值的负面影响，实现有竞争力的性能。

Abstract: Weight-only post-training quantization (PTQ) is crucial for efficient Large Language Model (LLM) deployment but suffers from accuracy degradation caused by weight and activation outliers. Existing mitigation strategies often face critical limitations: they either yield insufficient outlier suppression or incur significant deployment inefficiencies, such as inference latency, heavy preprocessing, or reliance on complex operator fusion. To resolve these limitations, we leverage a key insight: over-parameterized LLMs often converge to Flat Minima, implying a vast equivalent solution space where weights can be adjusted without compromising accuracy. Building on this, we propose Astro, an Activation-guided Structured Regularization framework designed to suppress the negative effects of outliers in a hardware-friendly and efficient manner. Leveraging the activation-guided regularization objective, Astro actively reconstructs intrinsically robust weights, aggressively suppressing weight outliers corresponding to high-magnitude activations without sacrificing model accuracy. Crucially, Astro introduces zero inference latency and is orthogonal to mainstream quantization methods like GPTQ. Extensive experiments show that Astro achieves highly competitive performance; notably, on LLaMA-2-7B, it achieves better performance than complex learning-based rotation methods with almost 1/3 of the quantization time.

</details>


### [301] [Rational Transductors](https://arxiv.org/abs/2602.07599)
*Mehryar Mohri*

Main category: cs.LG

TL;DR: 提出Rational Transductors架构，增强Transformer表达能力，解决正则差距，实现算法任务长度泛化。


<details>
  <summary>Details</summary>
Motivation: 标准Transformers在处理刚性顺序逻辑和状态跟踪方面存在困难，自注意力机制有复杂度限制，难以在顺序问题上实现鲁棒的长度泛化。

Method: 引入Rational Transductors，这是一种双流架构，通过Deep Rational Injection方案将加权有限自动机的矩阵值递归信息注入注意力机制。

Result: 理论分析和实验结果表明，Rational Transductors解决了“正则差距”，能在标准Transformers失败的算法任务上实现鲁棒的长度泛化。

Conclusion: Rational Transductors严格泛化了Transformers的表达能力，能捕捉正则语言和相关问题，且无传统RNN的顺序计算瓶颈。

Abstract: Standard Transformers excel at semantic modeling but struggle with
  rigid sequential logic and state tracking. Theoretical work
  establishes that self-attention is limited to $\AC^0$ (under hard
  attention) or $\TC^0$ (under soft attention), complexity classes
  that often fail to support robust length generalization on
  sequential problems without intermediate chain-of-thought. In this
  work, we introduce \emph{Rational Transductors}, a dual-stream
  architecture that augments the Transformer with a matrix-valued
  recurrence derived from Weighted Finite Automata (WFA). By
  injecting rational state information into the attention mechanism
  via a \emph{Deep Rational Injection} scheme, our framework strictly
  generalizes the expressive power of Transformers to capture all
  Regular Languages, $\NC^1$-complete problems (such as Boolean
  Formula Evaluation), and fundamental separations like Parity and
  Modular Counting, while preserving $O(L + \log T)$ parallel time
  complexity. We ground the architecture in a rigorous learning
  theory: we prove that \emph{Random Rational Features} act as a
  universal basis for sequential dependencies, justifying our
  initialization strategy, while establishing that the
  \emph{Differentiable Rational Feature} regime is necessary to close
  the representational compactness gap. Theoretical analysis and
  empirical results demonstrate that Rational Transductors solve the
  "Regular Gap," enabling robust length generalization on algorithmic
  tasks where standard Transformers fail, without the sequential
  computational bottlenecks of traditional RNNs.

</details>


### [302] [Object-Oriented Transition Modeling with Inductive Logic Programming](https://arxiv.org/abs/2602.07602)
*Gabriel Stella,Dmitri Loguinov*

Main category: cs.LG

TL;DR: 本文开发了一种新的学习算法，相比之前方法更强大，实验显示其显著优于当前最佳水平。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习中从观察构建世界模型（归纳）的挑战，使模型在新情况下保持准确（泛化），且易于解释和高效训练。

Method: 开发了一种新的学习算法。

Result: 通过全面实验，包括消融测试和与神经网络基线对比，显示算法相比当前最佳有显著提升。

Conclusion: 新开发的算法比此前方法更强大且显著提升性能。

Abstract: Building models of the world from observation, i.e., induction, is one of the major challenges in machine learning. In order to be useful, models need to maintain accuracy when used in novel situations, i.e., generalize. In addition, they should be easy to interpret and efficient to train. Prior work has investigated these concepts in the context of object-oriented representations inspired by human cognition. In this paper, we develop a novel learning algorithm that is substantially more powerful than these previous methods. Our thorough experiments, including ablation tests and comparison with neural baselines, demonstrate a significant improvement over the state-of-the-art.

</details>


### [303] [Escaping Spectral Bias without Backpropagation: Fast Implicit Neural Representations with Extreme Learning Machines](https://arxiv.org/abs/2602.07603)
*Woojin Cho,Junghwan Park*

Main category: cs.LG

TL;DR: 提出无反向传播的INR（ELM - INR）进行快速重建，并引入自适应网格细化策略BEAM提高重建质量。


<details>
  <summary>Details</summary>
Motivation: 传统训练INR捕捉细节依赖迭代反向传播，且当目标频率内容非均匀时会受频谱偏差阻碍。

Method: 提出ELM - INR，将域分解为重叠子域，用极限学习机封闭式拟合每个局部问题；从频谱Barron范数角度分析方法；引入自适应网格细化策略BEAM。

Result: ELM - INR通过单位分解组合局部预测器实现快速、数值稳健的重建；发现全局重建误差由频谱复杂度高的区域主导。

Conclusion: BEAM策略可在容量受限情况下平衡子域的频谱复杂度，提高重建质量。

Abstract: Training implicit neural representations (INRs) to capture fine-scale details typically relies on iterative backpropagation and is often hindered by spectral bias when the target exhibits highly non-uniform frequency content. We propose ELM-INR, a backpropagation-free INR that decomposes the domain into overlapping subdomains and fits each local problem using an Extreme Learning Machine (ELM) in closed form, replacing iterative optimization with stable linear least-squares solutions. This design yields fast and numerically robust reconstruction by combining local predictors through a partition of unity. To understand where approximation becomes difficult under fixed local capacity, we analyze the method from a spectral Barron norm perspective, which reveals that global reconstruction error is dominated by regions with high spectral complexity. Building on this insight, we introduce BEAM, an adaptive mesh refinement strategy that balances spectral complexity across subdomains to improve reconstruction quality in capacity-constrained regimes.

</details>


### [304] [SERE: Similarity-based Expert Re-routing for Efficient Batch Decoding in MoE Models](https://arxiv.org/abs/2602.07616)
*Juntong Wu,Jialiang Cheng,Fuyu Lv,Ou Dan,Li Yuan*

Main category: cs.LG

TL;DR: 提出SERE方法解决MoE模型批解码与专家稀疏性的矛盾，实验显示可加速且质量损失小。


<details>
  <summary>Details</summary>
Motivation: MoE模型在生产服务中批推理会导致专家过度激活，减缓内存受限的解码阶段，需解决批解码与专家稀疏性的矛盾。

Method: 提出SERE方法，以输入感知方式动态减少活跃专家数量，利用相似性模式识别和保留关键专家，避免静态剪枝或合并，基于批级专家冗余实现动态专家跳过，还提供高效自定义CUDA内核。

Result: 在各种复杂推理基准测试中，SERE实现了高达2.0倍的加速，且质量损失极小。

Conclusion: SERE为经济高效且对延迟敏感的大规模MoE部署提供了实用解决方案。

Abstract: Mixture-of-Experts (MoE) architectures employ sparse activation to deliver faster training and inference with higher accuracy than dense LLMs. However, in production serving, MoE models require batch inference to optimize hardware efficiency, which may cause excessive expert activation and thus slow the memory-bound decoding stage. To address the fundamental tension between batch decoding and expert sparsity, we present SERE, a Similarity-based Expert Re-routing method for Efficient batch decoding in MoE models. SERE dynamically reduces the number of active experts in an input-aware manner by re-routing tokens from secondary experts to their most similar primary counterparts. It also leverages similarity patterns to identify and preserve critical experts, thereby preventing capability loss. Notably, SERE avoids static expert pruning or merging, instead enabling dynamic expert skipping based on batch-level expert redundancy. Additionally, we provide an efficient custom CUDA kernel for SERE, enabling plug-and-play use in vLLM with only a single-line code change. Extensive experiments on various complex reasoning benchmarks demonstrate that SERE achieves up to 2.0x speedup with minimal quality loss, providing a practical solution for cost-efficient and latency-sensitive large-scale MoE deployment. Code implementation of SERE can be found in https://github.com/JL-Cheng/SERE.

</details>


### [305] [TASTE: Task-Aware Out-of-Distribution Detection via Stein Operators](https://arxiv.org/abs/2602.07640)
*Michał Kozyra,Gesine Reinert*

Main category: cs.LG

TL;DR: 提出基于Stein算子的任务感知框架TASTE，用于检测分布偏移，有理论保证、可定位偏移，实验表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有分布外检测方法要么以数据为中心，不考虑对模型的影响；要么以模型为中心，未明确参考数据几何形状。

Method: 提出基于Stein算子的任务感知框架TASTE，将分布偏移与模型输入敏感性关联。

Result: 该算子有清晰几何解释，有理论保证，能定位偏移、提供像素级诊断，实验优于已有基线。

Conclusion: TASTE方法能紧密贴合任务退化情况，有效检测分布偏移。

Abstract: Out-of-distribution detection methods are often either data-centric, detecting deviations from the training input distribution irrespective of their effect on a trained model, or model-centric, relying on classifier outputs without explicit reference to data geometry. We propose TASTE (Task-Aware STEin operators): a task-aware framework based on so-called Stein operators, which allows us to link distribution shift to the input sensitivity of the model. We show that the resulting operator admits a clear geometric interpretation as a projection of distribution shift onto the sensitivity field of the model, yielding theoretical guarantees. Beyond detecting the presence of a shift, the same construction enables its localisation through a coordinate-wise decomposition, and for image data-provides interpretable per-pixel diagnostics. Experiments on controlled Gaussian shifts, MNIST under geometric perturbations, and CIFAR-10 perturbed benchmarks demonstrate that the proposed method aligns closely with task degradation while outperforming established baselines.

</details>


### [306] [Surprisal-Guided Selection: Compute-Optimal Test-Time Strategies for Execution-Grounded Code Generation](https://arxiv.org/abs/2602.07670)
*Jarrod Barnes*

Main category: cs.LG

TL;DR: 研究可验证执行任务的测试时最优策略，发现搜索优于最小适应，提出零成本的惊异度引导选择策略。


<details>
  <summary>Details</summary>
Motivation: 探讨测试时适应策略是否适用于可验证执行任务，寻找计算最优策略。

Method: 以KernelBench为测试平台，使用120B参数模型，对比搜索和最小适应策略，提出惊异度引导选择方法。

Result: Best-of-N采样在任务成功率上优于TTT，惊异度引导选择策略成功率达80%，惊异度引导top3匹配神谕性能达100%。

Conclusion: 对于密集奖励的可验证执行任务，应将计算资源分配给样本多样性和智能选择而非梯度适应，惊异度引导选择原则可能推广到其他执行领域。

Abstract: Test-time training (TTT) adapts language models through gradient-based updates at inference. But is adaptation the right strategy? We study compute-optimal test-time strategies for verifiable execution-grounded (VEG) tasks, domains like GPU kernel optimization where a deterministic evaluator provides dense, continuous reward signals. Using KernelBench as our testbed and a 120B-parameter model (GPT-OSS-120B with LoRA adaptation), we find that search outperforms minimal adaptation (1-5 gradient steps): Best-of-N sampling achieves 90% task success (18/20 tasks) at K=64 across the full KernelBench L1 eval set while TTT's best checkpoint reaches only 30.6% (3-seed mean), with TTT's "equivalent K" falling below 1, worse than single-sample inference. The failure mode is over-sharpening: gradient updates collapse diversity toward mediocre solutions rather than discovering optimal ones. Our main contribution is surprisal-guided selection: selecting the highest-surprisal (lowest-confidence) correct sample yields 80% success vs. 50% for most-confident selection, a 30% improvement. Extending to surprisal-guided-top3 matches oracle performance at 100%. This zero-cost strategy, validated through length-controlled analysis, recovers oracle performance. For dense-reward VEG tasks, compute should be allocated to sample diversity and intelligent selection rather than gradient adaptation. The surprisal-guided selection principle may generalize to other execution-grounded domains where optimal solutions occupy the distribution tail.

</details>


### [307] [Federated Learning with Profile Mapping under Distribution Shifts and Drifts](https://arxiv.org/abs/2602.07671)
*Mohan Li,Dario Fenoglio,Martin Gjoreski,Marc Langheinrich*

Main category: cs.LG

TL;DR: 本文提出Feroma框架处理联邦学习数据异质性问题，实验显示其性能和稳定性优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习方法难以处理客户端数据分布偏移和漂移，且依赖不现实假设，限制了泛化性。

Method: 引入Feroma框架，基于客户端分布概况，通过自适应相似度加权指导模型聚合和测试时模型分配，动态选择聚合策略。

Result: 与10种先进方法相比，Feroma在动态数据异质性条件下提高了性能和稳定性，平均准确率最高提升12个百分点，计算和通信开销与FedAvg相当。

Conclusion: 基于分布概况的聚合为数据分布偏移和漂移下的鲁棒联邦学习提供了实用途径。

Abstract: Federated Learning (FL) enables decentralized model training across clients without sharing raw data, but its performance degrades under real-world data heterogeneity. Existing methods often fail to address distribution shift across clients and distribution drift over time, or they rely on unrealistic assumptions such as known number of client clusters and data heterogeneity types, which limits their generalizability. We introduce Feroma, a novel FL framework that explicitly handles both distribution shift and drift without relying on client or cluster identity. Feroma builds on client distribution profiles-compact, privacy-preserving representations of local data-that guide model aggregation and test-time model assignment through adaptive similarity-based weighting. This design allows Feroma to dynamically select aggregation strategies during training, ranging from clustered to personalized, and deploy suitable models to unseen, and unlabeled test clients without retraining, online adaptation, or prior knowledge on clients' data. Extensive experiments show that compared to 10 state-of-the-art methods, Feroma improves performance and stability under dynamic data heterogeneity conditions-an average accuracy gain of up to 12 percentage points over the best baselines across 6 benchmarks-while maintaining computational and communication overhead comparable to FedAvg. These results highlight that distribution-profile-based aggregation offers a practical path toward robust FL under both data distribution shifts and drifts.

</details>


### [308] [ElliCE: Efficient and Provably Robust Algorithmic Recourse via the Rashomon Sets](https://arxiv.org/abs/2602.07674)
*Bohdan Turbal,Iryna Voitsitska,Lesia Semenova*

Main category: cs.LG

TL;DR: 提出ElliCE框架用于鲁棒算法追索，在模型不确定性下提供可靠追索。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型影响生活，算法追索依赖的标准反事实解释在Rashomon集大时不可靠，需可靠追索方法。

Method: 引入ElliCE框架，在Rashomon集的椭球近似上优化反事实。

Result: ElliCE生成的反事实更鲁棒、灵活，适应特征约束且速度快于现有基线。

Conclusion: ElliCE为模型不确定下的可靠追索提供了原则性和实用性解决方案，能确保稳定推荐。

Abstract: Machine learning models now influence decisions that directly affect people's lives, making it important to understand not only their predictions, but also how individuals could act to obtain better results. Algorithmic recourse provides actionable input modifications to achieve more favorable outcomes, typically relying on counterfactual explanations to suggest such changes. However, when the Rashomon set - the set of near-optimal models - is large, standard counterfactual explanations can become unreliable, as a recourse action valid for one model may fail under another. We introduce ElliCE, a novel framework for robust algorithmic recourse that optimizes counterfactuals over an ellipsoidal approximation of the Rashomon set. The resulting explanations are provably valid over this ellipsoid, with theoretical guarantees on uniqueness, stability, and alignment with key feature directions. Empirically, ElliCE generates counterfactuals that are not only more robust but also more flexible, adapting to user-specified feature constraints while being substantially faster than existing baselines. This provides a principled and practical solution for reliable recourse under model uncertainty, ensuring stable recommendations for users even as models evolve.

</details>


### [309] [Spectral Gating Networks](https://arxiv.org/abs/2602.07679)
*Jusheng Zhang,Yijia Fan,Kaitong Cai,Jing Yang,Yongsen Zheng,Kwok-Yan Lam,Liang Lin,Keze Wang*

Main category: cs.LG

TL;DR: 本文提出Spectral Gating Networks (SGN)，可在不牺牲稳定性和可扩展性前提下注入频谱容量，在多领域基准测试中提升了准确率与效率的权衡。


<details>
  <summary>Details</summary>
Motivation: 在前馈网络中，如何在不牺牲稳定性和可扩展性的前提下引入频率丰富的表达能力是未充分研究的问题，基于样条的KAN参数化暴露出了这一矛盾。

Method: 引入SGN，一种即插即用的频谱重新参数化方法，通过紧凑频谱路径和可学习门增强标准激活路径，用可训练的随机傅里叶特征替代基于网格的样条，并采用混合GELU - Fourier公式。

Result: 在视觉、NLP、音频和PDE基准测试中，SGN在可比计算预算下持续改善了准确率 - 效率的权衡，如在CIFAR - 10上达到93.15%的准确率，推理速度比基于样条的KAN变体快达11.7倍。

Conclusion: SGN是一种在固定参数和训练预算下向现有MLP/FFN层注入频谱容量的有效方法。

Abstract: Gating mechanisms are ubiquitous, yet a complementary question in feed-forward networks remains under-explored: how to introduce frequency-rich expressivity without sacrificing stability and scalability? This tension is exposed by spline-based Kolmogorov-Arnold Network (KAN) parameterizations, where grid refinement can induce parameter growth and brittle optimization in high dimensions. To propose a stability-preserving way to inject spectral capacity into existing MLP/FFN layers under fixed parameter and training budgets, we introduce Spectral Gating Networks (SGN), a drop-in spectral reparameterization. SGN augments a standard activation pathway with a compact spectral pathway and learnable gates that allow the model to start from a stable base behavior and progressively allocate capacity to spectral features during training. The spectral pathway is instantiated with trainable Random Fourier Features (learned frequencies and phases), replacing grid-based splines and removing resolution dependence. A hybrid GELU-Fourier formulation further improves optimization robustness while enhancing high-frequency fidelity. Across vision, NLP, audio, and PDE benchmarks, SGN consistently improves accuracy-efficiency trade-offs under comparable computational budgets, achieving 93.15% accuracy on CIFAR-10 and up to 11.7x faster inference than spline-based KAN variants. Code and trained models will be released.

</details>


### [310] [Dense Feature Learning via Linear Structure Preservation in Medical Data](https://arxiv.org/abs/2602.07706)
*Yuanyun Zhang,Mingxuan Zhang,Siyuan Li,Zihan Wang,Haoran Chen,Wenbo Zhou,Shi Li*

Main category: cs.LG

TL;DR: 提出密集特征学习框架，操作嵌入矩阵，改善医学表征特性，提升下游性能。


<details>
  <summary>Details</summary>
Motivation: 现有医学深度学习模型使用特定任务目标训练，未充分利用临床数据结构，限制特征的可迁移性、稳定性和可解释性。

Method: 提出以表征为中心的密集特征学习框架，直接操作嵌入矩阵，通过定义的目标鼓励光谱平衡、子空间一致性和特征正交性。

Result: 在纵向电子健康记录数据、临床文本和多模态患者表征上，与监督和自监督基线相比，下游线性性能、鲁棒性和子空间对齐性有一致提升。

Conclusion: 学习跨越临床变异与预测临床结果同样重要，应将表征几何作为医学AI的首要目标。

Abstract: Deep learning models for medical data are typically trained using task specific objectives that encourage representations to collapse onto a small number of discriminative directions. While effective for individual prediction problems, this paradigm underutilizes the rich structure of clinical data and limits the transferability, stability, and interpretability of learned features. In this work, we propose dense feature learning, a representation centric framework that explicitly shapes the linear structure of medical embeddings. Our approach operates directly on embedding matrices, encouraging spectral balance, subspace consistency, and feature orthogonality through objectives defined entirely in terms of linear algebraic properties. Without relying on labels or generative reconstruction, dense feature learning produces representations with higher effective rank, improved conditioning, and greater stability across time. Empirical evaluations across longitudinal EHR data, clinical text, and multimodal patient representations demonstrate consistent improvements in downstream linear performance, robustness, and subspace alignment compared to supervised and self supervised baselines. These results suggest that learning to span clinical variation may be as important as learning to predict clinical outcomes, and position representation geometry as a first class objective in medical AI.

</details>


### [311] [Quantifying Explanation Quality in Graph Neural Networks using Out-of-Distribution Generalization](https://arxiv.org/abs/2602.07708)
*Ding Zhang,Siddharth Betala,Chirag Agarwal*

Main category: cs.LG

TL;DR: 提出评估图神经网络事后解释质量的指标EGS，经大规模验证，EGS能为解释器排名提供基准。


<details>
  <summary>Details</summary>
Motivation: 当前评估指标难以评估解释是否识别出真正的潜在因果变量，因此需要新的评估指标。

Method: 提出基于特征不变性的Explanation - Generalization Score（EGS），引入训练GNN的框架，在分布外（OOD）设置中评估其性能。

Result: 通过对11200个模型组合的大规模验证，EGS能为基于因果子结构捕获能力对解释器进行排名提供原则性基准。

Conclusion: EGS为传统基于保真度的指标提供了强大的替代方案。

Abstract: Evaluating the quality of post-hoc explanations for Graph Neural Networks (GNNs) remains a significant challenge. While recent years have seen an increasing development of explainability methods, current evaluation metrics (e.g., fidelity, sparsity) often fail to assess whether an explanation identifies the true underlying causal variables. To address this, we propose the Explanation-Generalization Score (EGS), a metric that quantifies the causal relevance of GNN explanations. EGS is founded on the principle of feature invariance and posits that if an explanation captures true causal drivers, it should lead to stable predictions across distribution shifts. To quantify this, we introduce a framework that trains GNNs using explanatory subgraphs and evaluates their performance in Out-of-Distribution (OOD) settings (here, OOD generalization serves as a rigorous proxy for the explanation's causal validity). Through large-scale validation involving 11,200 model combinations across synthetic and real-world datasets, our results demonstrate that EGS provides a principled benchmark for ranking explainers based on their ability to capture causal substructures, offering a robust alternative to traditional fidelity-based metrics.

</details>


### [312] [Towards Robust Scaling Laws for Optimizers](https://arxiv.org/abs/2602.07712)
*Alexandra Volkova,Mher Safaryan,Christoph H. Lampert,Dan Alistarh*

Main category: cs.LG

TL;DR: 研究不同优化器下大语言模型预训练的缩放定律，提出更稳健定律并进行理论分析。


<details>
  <summary>Details</summary>
Motivation: 现有研究多固定优化器，新一代优化器与模型和数据缩放关系不明，需研究不同优化器下的缩放定律。

Method: 进行实证研究，提出共享幂律指数和优化器特定缩放因子的定律，对凸二次目标代理任务进行基于梯度方法的理论分析。

Result: 发现为每个优化器单独设置Chinchilla式缩放定律存在病态问题且参数高度相关，提出的新定律可实现优化器间直接比较。

Conclusion: Chinchilla式缩放定律可自然由损失分解为不可约、近似和优化误差产生。

Abstract: The quality of Large Language Model (LLM) pretraining depends on multiple factors, including the compute budget and the choice of optimization algorithm. Empirical scaling laws are widely used to predict loss as model size and training data grow, however, almost all existing studies fix the optimizer (typically AdamW). At the same time, a new generation of optimizers (e.g., Muon, Shampoo, SOAP) promises faster and more stable convergence, but their relationship with model and data scaling is not yet well understood. In this work, we study scaling laws across different optimizers. Empirically, we show that 1) separate Chinchilla-style scaling laws for each optimizer are ill-conditioned and have highly correlated parameters. Instead, 2) we propose a more robust law with shared power-law exponents and optimizer-specific rescaling factors, which enable direct comparison between optimizers. Finally, 3) we provide a theoretical analysis of gradient-based methods for the proxy task of a convex quadratic objective, demonstrating that Chinchilla-style scaling laws emerge naturally as a result of loss decomposition into irreducible, approximation, and optimization errors.

</details>


### [313] [Analyzing and Guiding Zero-Shot Posterior Sampling in Diffusion Models](https://arxiv.org/abs/2602.07715)
*Roi Benita,Michael Elad,Joseph Keshet*

Main category: cs.LG

TL;DR: 本文对基于零样本扩散的信号恢复算法进行严格分析，基于先验高斯性假设给出封闭形式表达式，并提出参数设计框架，能平衡感知质量和信号保真度。


<details>
  <summary>Details</summary>
Motivation: 现有基于零样本扩散的信号恢复算法依赖手动调参和启发式方法，缺乏严格分析。

Method: 基于先验的高斯性假设，对理想后验采样器和扩散重建算法进行分析，得到封闭形式表达式，在此基础上提出参数设计框架。

Result: 所提谱建议与标准启发式方法结构不同，且随扩散步长变化，能在感知质量和信号保真度间取得平衡。

Conclusion: 提出的分析方法和参数设计框架有效，可替代现有的启发式选择策略。

Abstract: Recovering a signal from its degraded measurements is a long standing challenge in science and engineering. Recently, zero-shot diffusion based methods have been proposed for such inverse problems, offering a posterior sampling based solution that leverages prior knowledge. Such algorithms incorporate the observations through inference, often leaning on manual tuning and heuristics. In this work we propose a rigorous analysis of such approximate posterior-samplers, relying on a Gaussianity assumption of the prior. Under this regime, we show that both the ideal posterior sampler and diffusion-based reconstruction algorithms can be expressed in closed-form, enabling their thorough analysis and comparisons in the spectral domain. Building on these representations, we also introduce a principled framework for parameter design, replacing heuristic selection strategies used to date. The proposed approach is method-agnostic and yields tailored parameter choices for each algorithm, jointly accounting for the characteristics of the prior, the degraded signal, and the diffusion dynamics. We show that our spectral recommendations differ structurally from standard heuristics and vary with the diffusion step size, resulting in a consistent balance between perceptual quality and signal fidelity.

</details>


### [314] [Efficient Planning in Reinforcement Learning via Model Introspection](https://arxiv.org/abs/2602.07719)
*Gabriel Stella*

Main category: cs.LG

TL;DR: 本文提出将人类解决问题的内省能力视为程序分析，探讨其在强化学习模型中的应用，并给出关系强化学习中高效目标导向规划算法，建立强化学习与经典规划的新联系。


<details>
  <summary>Details</summary>
Motivation: 人类能根据任务高效解决问题的关键在于内省能力，而强化学习和经典规划通常被视为不同问题，需要建立两者联系。

Method: 将内省视为程序分析，探讨其在强化学习不同模型中的应用，并给出关系强化学习中目标导向规划算法。

Result: 实现了关系强化学习中高效目标导向规划，建立了强化学习和经典规划的新联系。

Conclusion: 通过将内省视为程序分析，可在强化学习和经典规划间建立有效联系。

Abstract: Reinforcement learning and classical planning are typically seen as two distinct problems, with differing formulations necessitating different solutions. Yet, when humans are given a task, regardless of the way it is specified, they can often derive the additional information needed to solve the problem efficiently. The key to this ability is introspection: by reasoning about their internal models of the problem, humans directly synthesize additional task-relevant information. In this paper, we propose that this introspection can be thought of as program analysis. We discuss examples of how this approach can be applied to various kinds of models used in reinforcement learning. We then describe an algorithm that enables efficient goal-oriented planning over the class of models used in relational reinforcement learning, demonstrating a novel link between reinforcement learning and classical planning.

</details>


### [315] [Do We Need Adam? Surprisingly Strong and Sparse Reinforcement Learning with SGD in LLMs](https://arxiv.org/abs/2602.07729)
*Sagnik Mukherjee,Lifan Yuan,Pavan Jayasinha,Dilek Hakkani-Tür,Hao Peng*

Main category: cs.LG

TL;DR: 研究指出强化学习（RL）优化实践常照搬下一token预测阶段，分析表明AdamW在RL中动量和自适应学习率影响小，实验证明内存高效的SGD在LLMs的RL中表现不逊于AdamW，更新参数更少，为RL优化动态提供新见解。


<details>
  <summary>Details</summary>
Motivation: 当前RL优化实践多遵循下一token预测阶段，但RL与这些阶段有本质差异，需探索更适合RL的优化方法。

Method: 分析AdamW在RL和SFT中的影响，通过实验对比SGD和AdamW在LLMs的RL中的表现。

Result: SGD在LLMs的RL中表现与AdamW相当甚至更优，SGD更新的模型参数比AdamW少超1000倍。

Conclusion: 为LLMs中RL的优化动态提供新见解，表明RL比之前认为的更具参数效率。

Abstract: Reinforcement learning (RL), particularly RL from verifiable reward (RLVR), has become a crucial phase of training large language models (LLMs) and a key focus of current scaling efforts. However, optimization practices in RL largely follow those of next-token prediction stages (e.g., pretraining and supervised fine-tuning), despite fundamental differences between RL and these stages highlighted by recent work. One such practice is the use of the AdamW optimizer, which is widely adopted for training large-scale transformers despite its high memory overhead. Our analysis shows that both momentum and adaptive learning rates in AdamW are less influential in RL than in SFT, leading us to hypothesize that RL benefits less from Adam-style per-parameter adaptive learning rates and momentum. Confirming this hypothesis, our experiments demonstrate that the substantially more memory-efficient SGD, which is known to perform poorly in supervised learning of large-scale transformers, matches or even outperforms AdamW in RL for LLMs. Remarkably, full fine-tuning with SGD updates fewer than 0.02% of model parameters without any sparsity-promoting regularization, more than 1000 times fewer than AdamW. Our analysis offers potential reasons for this update sparsity. These findings provide new insights into the optimization dynamics of RL in LLMs and show that RL can be substantially more parameter-efficient than previously recognized.

</details>


### [316] [The Laplacian Keyboard: Beyond the Linear Span](https://arxiv.org/abs/2602.07730)
*Siddarth Chandrasekar,Marlos C. Machado*

Main category: cs.LG

TL;DR: 提出Laplacian Keyboard (LK) 框架超越Laplacian特征向量线性跨度局限，有理论误差界且实验表现优。


<details>
  <summary>Details</summary>
Motivation: 在强化学习中，Laplacian特征向量使用限于线性跨度，限制了复杂环境下的表达能力。

Method: 引入LK框架，从特征向量构建与任务无关的选项库，元策略动态拼接选项。

Result: 建立零样本近似误差的理论界限，实验显示LK超越零样本解决方案，样本效率优于标准强化学习方法。

Conclusion: LK框架有效突破了Laplacian特征向量线性使用的局限，在强化学习中表现良好。

Abstract: Across scientific disciplines, Laplacian eigenvectors serve as a fundamental basis for simplifying complex systems, from signal processing to quantum mechanics. In reinforcement learning (RL), these eigenvectors provide a natural basis for approximating reward functions; however, their use is typically limited to their linear span, which restricts expressivity in complex environments. We introduce the Laplacian Keyboard (LK), a hierarchical framework that goes beyond the linear span. LK constructs a task-agnostic library of options from these eigenvectors, forming a behavior basis guaranteed to contain the optimal policy for any reward within the linear span. A meta-policy learns to stitch these options dynamically, enabling efficient learning of policies outside the original linear constraints. We establish theoretical bounds on zero-shot approximation error and demonstrate empirically that LK surpasses zero-shot solutions while achieving improved sample efficiency compared to standard RL methods.

</details>


### [317] [TerraBind: Fast and Accurate Binding Affinity Prediction through Coarse Structural Representations](https://arxiv.org/abs/2602.07735)
*Matteo Rossi,Ryan Pederson,Miles Wang-Henderson,Ben Kaufman,Edward C. Williams,Carl Underkoffler,Owen Lewis Howell,Adrian Layer,Stephan Thaler,Narbe Mardirossian,John Anthony Parkhill*

Main category: cs.LG

TL;DR: 介绍蛋白质 - 配体结构和结合亲和力预测基础模型TerraBind，推理快26倍且精度提高约20%。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法依赖全原子扩散生成3D坐标导致推理瓶颈，提出全原子分辨率对准确预测小分子构象和结合亲和力并非必要的假设。

Method: 采用粗粒度口袋级表示，结合COATI - 3分子编码和ESM - 2蛋白质嵌入的多模态架构，通过无扩散优化模块生成构象，结合亲和力似然预测模块进行预测。

Result: 在结构预测基准测试中配体构象准确性与基于扩散的基线相当；在结合亲和力预测上，皮尔逊相关性比Boltz - 2高约20%；能提供校准良好的亲和力不确定性估计；在模拟药物发现周期中，所选分子亲和力提升比基于贪婪的方法高6倍。

Conclusion: TerraBind在蛋白质 - 配体结构和结合亲和力预测上表现出色，能解决可靠化合物优先排序的关键问题，为药物发现提供有效方法。

Abstract: We present TerraBind, a foundation model for protein-ligand structure and binding affinity prediction that achieves 26-fold faster inference than state-of-the-art methods while improving affinity prediction accuracy by $\sim$20\%. Current deep learning approaches to structure-based drug design rely on expensive all-atom diffusion to generate 3D coordinates, creating inference bottlenecks that render large-scale compound screening computationally intractable. We challenge this paradigm with a critical hypothesis: full all-atom resolution is unnecessary for accurate small molecule pose and binding affinity prediction. TerraBind tests this hypothesis through a coarse pocket-level representation (protein C$_β$ atoms and ligand heavy atoms only) within a multimodal architecture combining COATI-3 molecular encodings and ESM-2 protein embeddings that learns rich structural representations, which are used in a diffusion-free optimization module for pose generation and a binding affinity likelihood prediction module. On structure prediction benchmarks (FoldBench, PoseBusters, Runs N' Poses), TerraBind matches diffusion-based baselines in ligand pose accuracy. Crucially, TerraBind outperforms Boltz-2 by $\sim$20\% in Pearson correlation for binding affinity prediction on both a public benchmark (CASP16) and a diverse proprietary dataset (18 biochemical/cell assays). We show that the affinity prediction module also provides well-calibrated affinity uncertainty estimates, addressing a critical gap in reliable compound prioritization for drug discovery. Furthermore, this module enables a continual learning framework and a hedged batch selection strategy that, in simulated drug discovery cycles, achieves 6$\times$ greater affinity improvement of selected molecules over greedy-based approaches.

</details>


### [318] [Learnable Chernoff Baselines for Inference-Time Alignment](https://arxiv.org/abs/2602.07738)
*Sunil Madhow,Yuchen Liang,Ness Shroff,Yingbin Liang,Yu-Xiang Wang*

Main category: cs.LG

TL;DR: 研究生成模型推理时奖励引导对齐，提出LCBs方法，展示优势。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型推理时奖励引导对齐方法存在依赖特定架构调整或计算成本高的问题。

Method: 引入Learnable Chernoff Baselines (LCBs)，利用预训练模型的黑盒采样访问，实现带自适应选择接受概率的拒绝采样。

Result: 在连续和离散扩散设置中，LCB采样接近理想拒绝采样，且对预训练模型的查询次数大幅减少。

Conclusion: LCBs能高效、近似地从指数倾斜核中采样，对推理计算规模有细粒度控制，有总变差保证。

Abstract: We study inference-time reward-guided alignment for generative models. Existing methods often rely on either architecture-specific adaptations or computationally costly inference procedures. We introduce Learnable Chernoff Baselines (LCBs) as a method for efficiently and approximately sampling from the exponentially tilted kernels that arise from KL-regularized reward alignment. Using only black-box sampling access to the pretrained model, LCBs implement a form of rejection sampling with adaptively selected acceptance probabilities, which allows fine-grained control over inference-compute scaling. We establish total-variation guarantees to the ideal aligned model, and demonstrate in both continuous and discrete diffusion settings that LCB sampling closely matches ideal rejection sampling while using substantially fewer queries to the pretrained model.

</details>


### [319] [Riemannian MeanFlow](https://arxiv.org/abs/2602.07744)
*Dongyeop Woo,Marta Skreta,Seonghyun Park,Sungsoo Ahn,Kirill Neklyudov*

Main category: cs.LG

TL;DR: 引入Riemannian MeanFlow (RMF)框架，可在流形上直接学习流映射，减少计算量，在两类生成任务中表现良好，且能实现高效奖励引导设计。


<details>
  <summary>Details</summary>
Motivation: 现有扩散和流模型推理时需大量神经网络评估，成为大规模科学采样工作流计算瓶颈。

Method: 提出RMF框架，推导流形平均速度三种等价表示，分析参数化和稳定技术用于高维流形训练。

Result: 在启动子DNA设计和蛋白质主链生成中，RMF样本质量与先前方法相当，但函数评估次数最多减少至十分之一。

Conclusion: 少步流映射可通过奖励前瞻实现高效奖励引导设计，以最小额外成本从中间步骤预测终端状态。

Abstract: Diffusion and flow models have become the dominant paradigm for generative modeling on Riemannian manifolds, with successful applications in protein backbone generation and DNA sequence design. However, these methods require tens to hundreds of neural network evaluations at inference time, which can become a computational bottleneck in large-scale scientific sampling workflows. We introduce Riemannian MeanFlow~(RMF), a framework for learning flow maps directly on manifolds, enabling high-quality generations with as few as one forward pass. We derive three equivalent characterizations of the manifold average velocity (Eulerian, Lagrangian, and semigroup identities), and analyze parameterizations and stabilization techniques to improve training on high-dimensional manifolds. In promoter DNA design and protein backbone generation settings, RMF achieves comparable sample quality to prior methods while requiring up to 10$\times$ fewer function evaluations. Finally, we show that few-step flow maps enable efficient reward-guided design through reward look-ahead, where terminal states can be predicted from intermediate steps at minimal additional cost.

</details>


### [320] [Preference Conditioned Multi-Objective Reinforcement Learning: Decomposed, Diversity-Driven Policy Optimization](https://arxiv.org/abs/2602.07764)
*Tanmay Ambadkar,Sourav Panda,Shreyash Kale,Jonathan Dodge,Abhinav Verma*

Main category: cs.LG

TL;DR: 现有多目标强化学习方法恢复完整 Pareto 前沿能力不足，本文提出 D^3PO 框架解决此问题，在多个基准测试表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有单偏好条件策略在实践中较脆弱，常无法恢复完整 Pareto 前沿，需解决当前方法的结构问题。

Method: 引入基于 PPO 的 D^3PO 框架，通过分解优化流程保留每目标学习信号，稳定后再整合偏好，使用缩放多样性正则化器防止崩溃。

Result: 在标准多目标强化学习基准测试中，D^3PO 比现有单策略和多策略方法发现更广泛、更高质量的 Pareto 前沿。

Conclusion: D^3PO 能有效解决现有多目标强化学习方法的问题，可使用单个可部署策略达到或超过现有最优的超体积和期望效用。

Abstract: Multi-objective reinforcement learning (MORL) seeks to learn policies that balance multiple, often conflicting objectives. Although a single preference-conditioned policy is the most flexible and scalable solution, existing approaches remain brittle in practice, frequently failing to recover complete Pareto fronts. We show that this failure stems from two structural issues in current methods: destructive gradient interference caused by premature scalarization and representational collapse across the preference space. We introduce $D^3PO$, a PPO-based framework that reorganizes multi-objective policy optimization to address these issues directly. $D^3PO$ preserves per-objective learning signals through a decomposed optimization pipeline and integrates preferences only after stabilization, enabling reliable credit assignment. In addition, a scaled diversity regularizer enforces sensitivity of policy behavior to preference changes, preventing collapse. Across standard MORL benchmarks, including high-dimensional and many-objective control tasks, $D^3PO$ consistently discovers broader and higher-quality Pareto fronts than prior single- and multi-policy methods, matching or exceeding state-of-the-art hypervolume and expected utility while using a single deployable policy.

</details>


### [321] [MaD-Mix: Multi-Modal Data Mixtures via Latent Space Coupling for Vision-Language Model Training](https://arxiv.org/abs/2602.07790)
*Wanyun Xie,Francesco Tonin,Volkan Cevher*

Main category: cs.LG

TL;DR: 提出MaD - Mix框架用于VLM训练，可推导多模态数据混合，加速训练，在多场景表现良好。


<details>
  <summary>Details</summary>
Motivation: 当前Vision - Language Models训练依赖昂贵的手动调优，需要更高效的方法。

Method: 将数据混合表述为模态感知的领域对齐最大化，通过Fenchel对偶和模态间耦合变量获得多模态对齐分数，系统处理缺失模态的领域。

Result: 在0.5B和7B模型的实证评估中，加速VLM训练；图像 - 文本指令调优中用22%更少训练步骤达到人工调优效果；在复杂三模态场景中提升平均准确率，混合计算开销可忽略不计。

Conclusion: MaD - Mix是一种可扩展的数据混合设计方法，适用于现代VLM管道。

Abstract: Vision-Language Models (VLMs) are typically trained on a diverse set of multi-modal domains, yet current practices rely on costly manual tuning. We propose MaD-Mix, a principled and computationally efficient framework that derives multi-modal data mixtures for VLM training. MaD-Mix formulates data mixing as modality-aware domain alignment maximization and obtains closed-form multi-modal alignment scores from the Fenchel dual through inter-modal coupling variables. MaD-Mix systematically handles domains with missing modalities, allowing for the integration of language-only domains. Empirical evaluations across 0.5B and 7B models demonstrate that MaD-Mix accelerates VLM training across diverse benchmarks. MaD-Mix matches human-tuned data mixtures using 22% fewer training steps in image-text instruction tuning. In complex tri-modal video-image-text scenarios, where manual tuning becomes impractical, MaD-Mix boosts average accuracy over uniform weights, with negligible mixture computation overhead (< 1 GPU-hour), enabling scalable mixture design for modern VLM pipelines.

</details>


### [322] [CausalTAD: Injecting Causal Knowledge into Large Language Models for Tabular Anomaly Detection](https://arxiv.org/abs/2602.07798)
*Ruiqi Wang,Ruikang Liu,Runyu Chen,Haoxiang Suo,Zhiyi Peng,Zhuo Tang,Changjian Chen*

Main category: cs.LG

TL;DR: 提出CausalTaD方法，通过注入因果知识到LLMs进行表格异常检测，实验显示优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLMs的表格异常检测方法随机排列列，未考虑因果关系，影响异常检测准确性。

Method: 先识别列间因果关系并重新排列，将排列建模为线性排序问题，还提出重加权策略为不同列分配权重。

Result: 在30多个数据集上的实验表明，该方法始终优于当前最先进的方法。

Conclusion: CausalTaD方法有效，可提升表格异常检测性能。

Abstract: Detecting anomalies in tabular data is critical for many real-world applications, such as credit card fraud detection. With the rapid advancements in large language models (LLMs), state-of-the-art performance in tabular anomaly detection has been achieved by converting tabular data into text and fine-tuning LLMs. However, these methods randomly order columns during conversion, without considering the causal relationships between them, which is crucial for accurately detecting anomalies. In this paper, we present CausalTaD, a method that injects causal knowledge into LLMs for tabular anomaly detection. We first identify the causal relationships between columns and reorder them to align with these causal relationships. This reordering can be modeled as a linear ordering problem. Since each column contributes differently to the causal relationships, we further propose a reweighting strategy to assign different weights to different columns to enhance this effect. Experiments across more than 30 datasets demonstrate that our method consistently outperforms the current state-of-the-art methods. The code for CausalTAD is available at https://github.com/350234/CausalTAD.

</details>


### [323] [Fairness Aware Reward Optimization](https://arxiv.org/abs/2602.07799)
*Ching Lam Choi,Vighnesh Subramaniam,Phillip Isola,Antonio Torralba,Stefanie Jegelka*

Main category: cs.LG

TL;DR: 提出Faro框架训练奖励模型以减少大语言模型对齐中的不公平性，有理论分析且实验效果好。


<details>
  <summary>Details</summary>
Motivation: 人类偏好数据的人口统计偏差会通过奖励模型将系统性不公平传播到对齐的大语言模型中。

Method: 引入Fairness Aware Reward Optimization (Faro)框架，在人口统计均等、均衡赔率或反事实公平约束下训练奖励模型。

Result: Faro能显著减少偏差和有害生成，同时保持或提高模型质量。

Conclusion: Faro框架可使奖励模型同时具备序数性、基数性和公平性，且存在非空帕累托前沿。

Abstract: Demographic skews in human preference data propagate systematic unfairness through reward models into aligned LLMs. We introduce Fairness Aware Reward Optimization (Faro), an in-processing framework that trains reward models under demographic parity, equalized odds, or counterfactual fairness constraints. We provide the first theoretical analysis of reward-level fairness in LLM alignment, establishing: (i) provable fairness certificates for Faro-trained rewards with controllable slack; a (ii) formal characterization of the accuracy-fairness trade-off induced by KL-regularized fine-tuning, proving fairness transfers from reward to policy; and the (iii) existence of a non-empty Pareto frontier. Unlike pre- and post-processing methods, Faro ensures reward models are simultaneously ordinal (ranking correctly), cardinal (calibrated), and fair. Across multiple LLMs and benchmarks, Faro significantly reduces bias and harmful generations while maintaining or improving model quality.

</details>


### [324] [Approximating Matrix Functions with Deep Neural Networks and Transformers](https://arxiv.org/abs/2602.07800)
*Rahul Padmanabhan,Simone Brugiapaglia*

Main category: cs.LG

TL;DR: 研究用神经网络（含transformers）近似矩阵函数，证明ReLU网络逼近矩阵指数的宽度和深度边界，实验表明合适编码的transformer能以相对误差5%逼近部分矩阵函数。


<details>
  <summary>Details</summary>
Motivation: Transformers在自然语言处理中取得显著成果，但在数值计算方面应用较少，矩阵函数在科学计算中广泛存在，因此研究用神经网络逼近矩阵函数。

Method: 理论上证明ReLU网络逼近矩阵指数的宽度和深度边界；通过实验验证transformer encoder - decoder在合适数值编码下对特定矩阵函数的逼近情况。

Result: 证明了ReLU网络逼近矩阵指数的宽度和深度边界；实验表明合适编码的transformer encoder - decoder能以5%的相对误差大概率逼近特定矩阵函数，且编码方案对性能影响大。

Conclusion: 神经网络（包括transformers）可有效逼近矩阵函数，编码方案对不同函数的性能影响不同。

Abstract: Transformers have revolutionized natural language processing, but their use for numerical computation has received less attention. We study the approximation of matrix functions, which map scalar functions to matrices, using neural networks including transformers. We focus on functions mapping square matrices to square matrices of the same dimension. These types of matrix functions appear throughout scientific computing, e.g., the matrix exponential in continuous-time Markov chains and the matrix sign function in stability analysis of dynamical systems. In this paper, we make two contributions. First, we prove bounds on the width and depth of ReLU networks needed to approximate the matrix exponential to an arbitrary precision. Second, we show experimentally that a transformer encoder-decoder with suitable numerical encodings can approximate certain matrix functions at a relative error of 5% with high probability. Our study reveals that the encoding scheme strongly affects performance, with different schemes working better for different functions.

</details>


### [325] [Efficient Representations are Controllable Representations](https://arxiv.org/abs/2602.07828)
*Charles Ye,Jasmine Cui*

Main category: cs.LG

TL;DR: 通过简单辅助损失微调大语言模型，使部分残差流维度成为可解释控制开关以引导生成


<details>
  <summary>Details</summary>
Motivation: 寻找最直接的在模型激活中植入可解释、可控制特征的方法，绕过现有复杂方法

Method: 用简单辅助损失微调大语言模型，训练部分残差流维度作为解释标志

Result: 惰性标志成为真正的内部特征，可在推理时引导生成

Conclusion: 利用模型的效率压力可诱导出可解释、可控制的表示

Abstract: What is the most brute-force way to install interpretable, controllable features into a model's activations? Controlling how LLMs internally represent concepts typically requires sophisticated methods to first identify, then intervene on the model's existing feature geometry. We bypass all of this.
  We finetune an LLM with a simple auxiliary loss, training 16 of its 3072 residual stream dimensions to be inert interpretability flags that simply indicate what concepts are required for generation. The model reorganizes around them anyway, learning to rely on these flags during actual generation tasks. As a result, these inert flags become genuine internal features: interpretable control switches that allow us to steer generation at inference time. Why does this work? When a feature is reliably supplied at a fixed location, gradient descent gradually eliminates redundant encodings elsewhere, and the model erodes its own alternative representations. A model's efficiency pressure is a lever - exploitable to induce interpretable, controllable representations.

</details>


### [326] [rePIRL: Learn PRM with Inverse RL for LLM Reasoning](https://arxiv.org/abs/2602.07832)
*Xian Wu,Kaijie Zhu,Ying Zhang,Lun Wang,Wenbo Guo*

Main category: cs.LG

TL;DR: 本文提出 rePIRL 框架学习有效过程奖励模型（PRM），理论证明其可统一在线和离线 PRM 学习方法，实验验证了其有效性及应用价值。


<details>
  <summary>Details</summary>
Motivation: 现有学习 PRM 的方法存在依赖专家策略强假设或有内在局限性问题，导致 PRM 效果弱或泛化性有限。

Method: 引入受逆强化学习启发的 rePIRL 框架，设计双学习过程交替更新策略和 PRM，学习算法有定制技术应对传统逆强化学习扩展到 LLM 的挑战。

Result: 在标准化数学和编码推理数据集上的实验表明 rePIRL 优于现有方法，还展示了训练的 PRM 在测试时训练、测试时缩放等方面的应用。

Conclusion: rePIRL 能以最少假设学习 PRM，有效克服现有方法问题。

Abstract: Process rewards have been widely used in deep reinforcement learning to improve training efficiency, reduce variance, and prevent reward hacking. In LLM reasoning, existing works also explore various solutions for learning effective process reward models (PRM) with or without the help of an expert policy. However, existing methods either rely on strong assumptions about the expert policies (e.g., requiring their reward functions) or suffer intrinsic limitations (e.g., entropy collapse), resulting in weak PRMs or limited generalizability. In this paper, we introduce rePIRL, an inverse RL-inspired framework that learns effective PRMs with minimal assumptions about expert policies. Specifically, we design a dual learning process that updates the policy and the PRM interchangeably. Our learning algorithm has customized techniques to address the challenges of scaling traditional inverse RL to LLMs. We theoretically show that our proposed learning framework can unify both online and offline PRM learning methods, justifying that rePIRL can learn PRMs with minimal assumptions. Empirical evaluations on standardized math and coding reasoning datasets demonstrate the effectiveness of rePIRL over existing methods. We further show the application of our trained PRM in test-time training, test-time scaling, and providing an early signal for training hard problems. Finally, we validate our training recipe and key design choices via a detailed ablation study.

</details>


### [327] [Interpretable Analytic Calabi-Yau Metrics via Symbolic Distillation](https://arxiv.org/abs/2602.07834)
*D Yang Eng*

Main category: cs.LG

TL;DR: 本文展示了符号回归可将卡拉比 - 丘流形的神经近似转化为简单可解释公式，公式在多方面表现良好，验证了符号蒸馏能恢复简洁可解释模型。


<details>
  <summary>Details</summary>
Motivation: 卡拉比 - 丘流形对弦论很重要，但计算其度量难以处理，需要寻找简单可解释的方法。

Method: 使用符号回归将神经近似转化为简单公式，进行多种子验证。

Result: 得到的五项表达式与神经网络精度相当（$R^2 = 0.9994$），参数减少3000倍；公式系数在研究的模空间范围可平滑变化；公式能重现物理可观测量。

Conclusion: 符号蒸馏可恢复之前只能由黑箱网络处理的量的紧凑、可解释模型。

Abstract: Calabi--Yau manifolds are essential for string theory but require computing intractable metrics. Here we show that symbolic regression can distill neural approximations into simple, interpretable formulas. Our five-term expression matches neural accuracy ($R^2 = 0.9994$) with 3,000-fold fewer parameters. Multi-seed validation confirms that geometric constraints select essential features, specifically power sums and symmetric polynomials, while permitting structural diversity. The functional form can be maintained across the studied moduli range ($ψ\in [0, 0.8]$) with coefficients varying smoothly; we interpret these trends as empirical hypotheses within the accuracy regime of the locally-trained teachers ($σ\approx 8-9\%$ at $ψ\neq 0$). The formula reproduces physical observables -- volume integrals and Yukawa couplings -- validating that symbolic distillation recovers compact, interpretable models for quantities previously accessible only to black-box networks.

</details>


### [328] [MARTI-MARS$^2$: Scaling Multi-Agent Self-Search via Reinforcement Learning for Code Generation](https://arxiv.org/abs/2602.07848)
*Shijie Wang,Pengfei Li,Yikun Fu,Kaifeng Liu,Fangyuan Li,Yang Liu,Xiaowei Sun,Zonglin Li,Siyao Zhao,Jian Zhao,Kai Tian,Dong Li,Junqi Gao,Yutong Zhang,Yiqun Chen,Yuqiang Li,Zoe Li,Weinan Zhang,Peng Ye,Shuyue Hu,Lei Bai,Bowen Zhou,Kaiyan Zhang,Biqing Qi*

Main category: cs.LG

TL;DR: 提出MARTI - MARS2框架突破大语言模型单智能体能力限制，在代码生成基准测试表现佳，揭示新的扩展定律。


<details>
  <summary>Details</summary>
Motivation: 大语言模型单智能体系统在复杂任务有性能上限，现有多智能体框架在纠错和策略多样性上有限制。

Method: 提出MARTI - MARS2框架，将多智能体协作探索过程建模为可学习环境，实现从同构多角色训练到异构多智能体训练；引入MARTI - MARS2 - T+推理策略。

Result: 在不同规模模型的代码生成基准测试中，用两个32B模型协作，MARTI - MARS2达到77.7%，超越GPT - 5.1等基线。

Conclusion: 从单智能体到同构多角色再到异构多智能体范式可逐步提升强化学习性能上限，策略多样性对多智能体强化学习扩展智能很关键。

Abstract: While the complex reasoning capability of Large Language Models (LLMs) has attracted significant attention, single-agent systems often encounter inherent performance ceilings in complex tasks such as code generation. Multi-agent collaboration offers a promising avenue to transcend these boundaries. However, existing frameworks typically rely on prompt-based test-time interactions or multi-role configurations trained with homogeneous parameters, limiting error correction capabilities and strategic diversity. In this paper, we propose a Multi-Agent Reinforced Training and Inference Framework with Self-Search Scaling (MARTI-MARS2), which integrates policy learning with multi-agent tree search by formulating the multi-agent collaborative exploration process as a dynamic and learnable environment. By allowing agents to iteratively explore and refine within the environment, the framework facilitates evolution from parameter-sharing homogeneous multi-role training to heterogeneous multi-agent training, breaking through single-agent capability limits. We also introduce an efficient inference strategy MARTI-MARS2-T+ to fully exploit the scaling potential of multi-agent collaboration at test time. We conduct extensive experiments across varied model scales (8B, 14B, and 32B) on challenging code generation benchmarks. Utilizing two collaborating 32B models, MARTI-MARS2 achieves 77.7%, outperforming strong baselines like GPT-5.1. Furthermore, MARTI-MARS2 reveals a novel scaling law: shifting from single-agent to homogeneous multi-role and ultimately to heterogeneous multi-agent paradigms progressively yields higher RL performance ceilings, robust TTS capabilities, and greater policy diversity, suggesting that policy diversity is critical for scaling intelligence via multi-agent reinforcement learning.

</details>


### [329] [Dynamic Load Model for Data Centers with Pattern-Consistent Calibration](https://arxiv.org/abs/2602.07859)
*Siyu Lu,Chenhan Xiao,Yang Weng*

Main category: cs.LG

TL;DR: 传统数据中心大电子负载模型有局限，本文提出结合物理结构和数据驱动适应性的框架，用时间对比学习校准模型，经多数据集校准并在多母线系统评估，发现未校准模型无法捕捉负载交互影响。


<details>
  <summary>Details</summary>
Motivation: 现有物理和数据驱动的数据中心负载建模方法存在不足，如物理模型未针对设施级操作校准，数据驱动方法易过拟合和产生不现实动态行为，为解决这些问题开展研究。

Method: 设计一个结合物理结构和数据驱动适应性的框架，对物理结构参数化以实现从实际运行数据进行数据驱动的模式一致校准，使用时间对比学习校准模型以对齐时间和统计模式，且仅共享校准参数保护数据隐私。

Result: 模型经多个真实世界数据集校准并集成到ANDES平台，在多个母线系统评估，发现大电子负载间的交互会改变扰动后恢复行为，未校准模型无法捕捉。

Conclusion: 提出的结合物理和数据驱动的负载建模框架能有效解决现有方法的局限，更好地模拟大电子负载在电力系统中的行为。

Abstract: The rapid growth of data centers has made large electronic load (LEL) modeling increasingly important for power system analysis. Such loads are characterized by fast workload-driven variability and protection-driven disconnection and reconnection behavior that are not captured by conventional load models. Existing data center load modeling includes physics-based approaches, which provide interpretable structure for grid simulation, and data-driven approaches, which capture empirical workload variability from data. However, physics-based models are typically uncalibrated to facility-level operation, while trajectory alignment in data-driven methods often leads to overfitting and unrealistic dynamic behavior. To resolve these limitations, we design the framework to leverage both physics-based structure and data-driven adaptability. The physics-based structure is parameterized to enable data-driven pattern-consistent calibration from real operational data, supporting facility-level grid planning. We further show that trajectory-level alignment is limited for inherently stochastic data center loads. Therefore, we design the calibration to align temporal and statistical patterns using temporal contrastive learning (TCL). This calibration is performed locally at the facility, and only calibrated parameters are shared with utilities, preserving data privacy. The proposed load model is calibrated by real-world operational load data from the MIT Supercloud, ASU Sol, Blue Waters, and ASHRAE datasets. Then it is integrated into the ANDES platform and evaluated on the IEEE 39-bus, NPCC 140-bus, and WECC 179-bus systems. We find that interactions among LELs can fundamentally alter post-disturbance recovery behavior, producing compound disconnection-reconnection dynamics and delayed stabilization that are not captured by uncalibrated load models.

</details>


### [330] [Direct Soft-Policy Sampling via Langevin Dynamics](https://arxiv.org/abs/2602.07873)
*Donghyeon Ki,Hee-Jun Ahn,Kyungyoon Kim,Byung-Jun Lee*

Main category: cs.LG

TL;DR: 本文提出Noise - Conditioned Langevin Q - Learning (NC - LQL)解决强化学习中软策略采样难题，在基准测试中表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有实现软策略的方法存在表达能力有限或难以可靠估计熵的问题，需要更好的方法。

Method: 通过Q函数的动作梯度驱动的朗之万动力学实现软策略采样，提出NC - LQL，将多尺度噪声扰动集成到价值函数中。

Result: 在OpenAI Gym MuJoCo基准测试中，NC - LQL与最先进的基于扩散的方法相比取得了有竞争力的性能。

Conclusion: NC - LQL为在线强化学习提供了一个简单而强大的解决方案。

Abstract: Soft policies in reinforcement learning define policies as Boltzmann distributions over state-action value functions, providing a principled mechanism for balancing exploration and exploitation. However, realizing such soft policies in practice remains challenging. Existing approaches either depend on parametric policies with limited expressivity or employ diffusion-based policies whose intractable likelihoods hinder reliable entropy estimation in soft policy objectives. We address this challenge by directly realizing soft-policy sampling via Langevin dynamics driven by the action gradient of the Q-function. This perspective leads to Langevin Q-Learning (LQL), which samples actions from the target Boltzmann distribution without explicitly parameterizing the policy. However, directly applying Langevin dynamics suffers from slow mixing in high-dimensional and non-convex Q-landscapes, limiting its practical effectiveness. To overcome this, we propose Noise-Conditioned Langevin Q-Learning (NC-LQL), which integrates multi-scale noise perturbations into the value function. NC-LQL learns a noise-conditioned Q-function that induces a sequence of progressively smoothed value landscapes, enabling sampling to transition from global exploration to precise mode refinement. On OpenAI Gym MuJoCo benchmarks, NC-LQL achieves competitive performance compared to state-of-the-art diffusion-based methods, providing a simple yet powerful solution for online RL.

</details>


### [331] [Harpoon: Generalised Manifold Guidance for Conditional Tabular Diffusion](https://arxiv.org/abs/2602.07875)
*Aditya Shankar,Yuandou Wang,Rihan Hai,Lydia Y. Chen*

Main category: cs.LG

TL;DR: 本文将流形理论扩展到表格数据，提出HARPOON表格扩散方法，在多个任务上验证了其性能。


<details>
  <summary>Details</summary>
Motivation: 现有表格数据生成方法在推理时难以处理未见约束，且流形理论当前公式有局限性，需扩展其应用。

Method: 将流形理论扩展到表格数据，提出HARPOON方法，在推理时引导无约束样本沿流形几何满足表格条件。

Result: 在插补和执行不等式约束等任务上验证了理论贡献，HARPOON在不同数据集上表现良好。

Conclusion: 流形感知引导对表格数据有实际益处，HARPOON方法有效。

Abstract: Generating tabular data under conditions is critical to applications requiring precise control over the generative process. Existing methods rely on training-time strategies that do not generalise to unseen constraints during inference, and struggle to handle conditional tasks beyond tabular imputation. While manifold theory offers a principled way to guide generation, current formulations are tied to specific inference-time objectives and are limited to continuous domains. We extend manifold theory to tabular data and expand its scope to handle diverse inference-time objectives. On this foundation, we introduce HARPOON, a tabular diffusion method that guides unconstrained samples along the manifold geometry to satisfy diverse tabular conditions at inference. We validate our theoretical contributions empirically on tasks such as imputation and enforcing inequality constraints, demonstrating HARPOON'S strong performance across diverse datasets and the practical benefits of manifold-aware guidance for tabular data. Code URL: https://github.com/adis98/Harpoon

</details>


### [332] [GRAFT: Decoupling Ranking and Calibration for Survival Analysis](https://arxiv.org/abs/2602.07884)
*Mohammad Ashhad,Robert Hoehndorf,Ricardo Henao*

Main category: cs.LG

TL;DR: 提出GRAFT模型解决生存分析难题，结合线性与非线性架构，有特征选择功能，在基准测试中表现优。


<details>
  <summary>Details</summary>
Motivation: 传统生存分析模型有局限性，经典模型受限，深度学习模型不可解释且对噪声敏感。

Method: 提出GRAFT模型，结合线性AFT模型与非线性残差神经网络，集成随机门进行特征选择，用随机条件插补优化损失函数训练。

Result: 在公共基准测试中，GRAFT在区分度和校准度上优于基线模型，在高噪声环境下保持鲁棒性和稀疏性。

Conclusion: GRAFT是一种有效解决生存分析复杂问题的新模型。

Abstract: Survival analysis is complicated by censored data, high-dimensional features, and non-linear interactions. Classical models are interpretable but restrictive, while deep learning models are flexible but often non-interpretable and sensitive to noise. We propose GRAFT (Gated Residual Accelerated Failure Time), a novel AFT model that decouples prognostic ranking from calibration. GRAFT's hybrid architecture combines a linear AFT model with a non-linear residual neural network, and it also integrates stochastic gates for automatic, end-to-end feature selection. The model is trained by directly optimizing a differentiable, C-index-aligned ranking loss using stochastic conditional imputation from local Kaplan-Meier estimators. In public benchmarks, GRAFT outperforms baselines in discrimination and calibration, while remaining robust and sparse in high-noise settings.

</details>


### [333] [Efficient Anti-exploration via VQVAE and Fuzzy Clustering in Offline Reinforcement Learning](https://arxiv.org/abs/2602.07889)
*Long Chen,Yinkui Liu,Shen Li,Bo Tang,Xuemin Hu*

Main category: cs.LG

TL;DR: 提出基于VQVAE和模糊聚类的离线强化学习反探索方法，在D4RL基准测试表现佳且计算成本低。


<details>
  <summary>Details</summary>
Motivation: 现有反探索方法离散化连续状态 - 动作对时存在维度灾难和信息丢失问题，导致效率和性能下降。

Method: 先基于多码本VQVAE提出伪计数方法离散化状态 - 动作对，设计离线RL反探索方法；再开发基于FCM聚类的码本更新机制。

Result: 在D4RL基准测试中，相比SOTA方法，该方法在多复杂任务中表现更好，计算成本更低。

Conclusion: 所提基于VQVAE和模糊聚类的方法能有效解决现有反探索方法的问题，提高学习效率。

Abstract: Pseudo-count is an effective anti-exploration method in offline reinforcement learning (RL) by counting state-action pairs and imposing a large penalty on rare or unseen state-action pair data. Existing anti-exploration methods count continuous state-action pairs by discretizing these data, but often suffer from the issues of dimension disaster and information loss in the discretization process, leading to efficiency and performance reduction, and even failure of policy learning. In this paper, a novel anti-exploration method based on Vector Quantized Variational Autoencoder (VQVAE) and fuzzy clustering in offline RL is proposed. We first propose an efficient pseudo-count method based on the multi-codebook VQVAE to discretize state-action pairs, and design an offline RL anti-exploitation method based on the proposed pseudo-count method to handle the dimension disaster issue and improve the learning efficiency. In addition, a codebook update mechanism based on fuzzy C-means (FCM) clustering is developed to improve the use rate of vectors in codebooks, addressing the information loss issue in the discretization process. The proposed method is evaluated on the benchmark of Datasets for Deep Data-Driven Reinforcement Learning (D4RL), and experimental results show that the proposed method performs better and requires less computing cost in multiple complex tasks compared to state-of-the-art (SOTA) methods.

</details>


### [334] [Safety Alignment as Continual Learning: Mitigating the Alignment Tax via Orthogonal Gradient Projection](https://arxiv.org/abs/2602.07892)
*Guanglong Sun,Siyuan Zhang,Liyuan Wang,Jun Zhu,Hang Su,Yi Zhong*

Main category: cs.LG

TL;DR: 文章指出大语言模型（LLMs）存在对齐税问题，提出将安全对齐作为持续学习问题并使用OGPSA方法来缓解干扰，实验表明该方法能有效改善安全 - 效用帕累托前沿。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型安全后训练导致通用效用降低的对齐税问题，该问题主要源于顺序对齐中的持续学习式遗忘。

Method: 提出Orthogonal Gradient Projection for Safety Alignment (OGPSA)方法，通过约束安全更新与捕获通用能力的子空间正交来减少干扰，该方法可即插即用。

Result: 在多种设置下，OGPSA能持续改善安全 - 效用帕累托前沿，如在Qwen2.5 - 7B - Instruct模型上提升了通用能力指标。

Conclusion: OGPSA是一种有效缓解大语言模型安全对齐中干扰问题的轻量级方法。

Abstract: Large Language Models (LLMs) often incur an alignment tax: safety post-training can reduce general utility (e.g., reasoning and coding). We argue that this tax primarily arises from continual-learning-style forgetting in sequential alignment, where distribution shift and conflicting objectives cause safety updates to overwrite pre-trained competencies. Accordingly, we cast safety alignment as a continual learning (CL) problem that must balance plasticity (acquiring safety constraints) and stability (preserving general abilities). We propose Orthogonal Gradient Projection for Safety Alignment (OGPSA), a lightweight method that mitigates interference by constraining each safety update to be orthogonal (in a first-order sense) to a learned subspace capturing general capabilities. Specifically, OGPSA estimates a low-rank capability subspace from gradients on a small reference set and projects the safety gradient onto its orthogonal complement before updating. This produces safety-directed updates that minimally perturb prior knowledge while retaining capacity for alignment. OGPSA is plug-and-play and integrates into standard post-training pipelines without large-scale replay, auxiliary objectives, or retraining. Across Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and sequential SFT$\rightarrow$DPO settings, OGPSA consistently improves the safety--utility Pareto frontier over standard baselines. For instance, on Qwen2.5-7B-Instruct under SFT$\rightarrow$DPO, OGPSA preserves strong safety while recovering general capability, improving SimpleQA from 0.53\% to 3.03\% and IFEval from 51.94\% to 63.96\%. Our source code is available at \href{https://github.com/SunGL001/OGPSA}{OGPSA}

</details>


### [335] [Adaptive Acquisition Selection for Bayesian Optimization with Large Language Models](https://arxiv.org/abs/2602.07904)
*Giang Ngo,Dat Phan Trong,Dang Nguyen,Sunil Gupta,Svetha Venkatesh*

Main category: cs.LG

TL;DR: 提出LMABO框架，用预训练大语言模型为贝叶斯优化选择获取函数，在50个基准问题上表现优于基准方法。


<details>
  <summary>Details</summary>
Motivation: 现有贝叶斯优化的自适应组合方法决策时忽略剩余预算和代理模型特征等更丰富信息，需要更好的方法。

Method: 引入LMABO框架，用结构化状态表示提示预训练大语言模型在每次迭代中从多样化组合中选择最合适的获取函数。

Result: 在50个基准问题评估中，LMABO比静态、自适应组合和其他基于大语言模型的基线方法有显著性能提升。

Conclusion: 大语言模型的行为是适应实时进展的综合策略，优势在于能将完整优化状态处理和合成有效的自适应策略。

Abstract: Bayesian Optimization critically depends on the choice of acquisition function, but no single strategy is universally optimal; the best choice is non-stationary and problem-dependent. Existing adaptive portfolio methods often base their decisions on past function values while ignoring richer information like remaining budget or surrogate model characteristics. To address this, we introduce LMABO, a novel framework that casts a pre-trained Large Language Model (LLM) as a zero-shot, online strategist for the BO process. At each iteration, LMABO uses a structured state representation to prompt the LLM to select the most suitable acquisition function from a diverse portfolio. In an evaluation across 50 benchmark problems, LMABO demonstrates a significant performance improvement over strong static, adaptive portfolio, and other LLM-based baselines. We show that the LLM's behavior is a comprehensive strategy that adapts to real-time progress, proving its advantage stems from its ability to process and synthesize the complete optimization state into an effective, adaptive policy.

</details>


### [336] [AceGRPO: Adaptive Curriculum Enhanced Group Relative Policy Optimization for Autonomous Machine Learning Engineering](https://arxiv.org/abs/2602.07906)
*Yuzhu Cai,Zexi Liu,Xinyu Zhu,Cheng Wang,Jiaao Chen,Hanrui Wang,Wei-Chen Wang,Di Jin,Siheng Chen*

Main category: cs.LG

TL;DR: 提出AceGRPO解决MLE中基于提示的代理行为停滞、RL应用挑战，Ace - 30B模型在MLE - Bench - Lite表现佳。


<details>
  <summary>Details</summary>
Motivation: 当前基于提示的MLE代理因参数冻结有行为停滞问题，RL应用于MLE存在执行延迟和数据选择效率问题。

Method: 提出AceGRPO，包含Evolving Data Buffer和Adaptive Sampling两个核心组件。

Result: 训练的Ace - 30B模型在MLE - Bench - Lite上有效提交率达100%，接近专有前沿模型性能，优于大型开源基线。

Conclusion: AceGRPO具备持续迭代优化的强大能力。

Abstract: Autonomous Machine Learning Engineering (MLE) requires agents to perform sustained, iterative optimization over long horizons. While recent LLM-based agents show promise, current prompt-based agents for MLE suffer from behavioral stagnation due to frozen parameters. Although Reinforcement Learning (RL) offers a remedy, applying it to MLE is hindered by prohibitive execution latency and inefficient data selection. Recognizing these challenges, we propose AceGRPO with two core components: (1) Evolving Data Buffer that continuously repurposes execution traces into reusable training tasks, and (2) Adaptive Sampling guided by a Learnability Potential function, which dynamically prioritizes tasks at the agent's learning frontier to maximize learning efficiency. Leveraging AceGRPO, our trained Ace-30B model achieves a 100% valid submission rate on MLE-Bench-Lite, approaches the performance of proprietary frontier models, and outperforms larger open-source baselines (e.g., DeepSeek-V3.2), demonstrating robust capability for sustained iterative optimization. Code is available at https://github.com/yuzhu-cai/AceGRPO.

</details>


### [337] [A Kinetic-Energy Perspective of Flow Matching](https://arxiv.org/abs/2602.07928)
*Ziyun Li,Huancheng Hu,Soon Hoe Lim,Xuyu Li,Fei Gao,Enmao Diao,Zezhen Ding,Michalis Vazirgiannis,Henrik Bostrom*

Main category: cs.LG

TL;DR: 本文引入KPE衡量流生成模型样本轨迹的动力学努力，发现其与语义保真度和数据密度的关系，提出无训练的KTS策略提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 受经典力学启发，为流生成模型引入一种衡量每个样本在ODE轨迹上累积动力学努力的诊断方法。

Method: 引入KPE衡量轨迹的动力学努力，分析其与语义保真度和数据密度的关系，基于经验流匹配的闭式形式提出KTS策略。

Result: KPE与语义保真度和数据密度有对应关系，极端能量会导致生成退化，KTS策略可减少记忆化并提高生成质量。

Conclusion: KPE为流生成模型提供了新的诊断工具，KTS策略是一种有效的无训练推理策略。

Abstract: Flow-based generative models can be viewed through a physics lens: sampling transports a particle from noise to data by integrating a time-varying velocity field, and each sample corresponds to a trajectory with its own dynamical effort. Motivated by classical mechanics, we introduce Kinetic Path Energy (KPE), an action-like, per-sample diagnostic that measures the accumulated kinetic effort along an Ordinary Differential Equation (ODE) trajectory. Empirically, KPE exhibits two robust correspondences: (i) higher KPE predicts stronger semantic fidelity; (ii) high-KPE trajectories terminate on low-density manifold frontiers. We further provide theoretical guarantees linking trajectory energy to data density. Paradoxically, this correlation is non-monotonic. At sufficiently high energy, generation can degenerate into memorization. Leveraging the closed-form of empirical flow matching, we show that extreme energies drive trajectories toward near-copies of training examples. This yields a Goldilocks principle and motivates Kinetic Trajectory Shaping (KTS), a training-free two-phase inference strategy that boosts early motion and enforces a late-time soft landing, reducing memorization and improving generation quality across benchmark tasks.

</details>


### [338] [Attention-Based Deep Learning for Early Parkinson's Disease Detection with Tabular Biomedical Data](https://arxiv.org/abs/2602.07933)
*Olamide Samuel Oseni,Ibraheem Omotolani Obanla,Toheeb Aduramomi Jimoh*

Main category: cs.LG

TL;DR: 研究用表格生物医学数据，对比四种模型用于早期帕金森病检测，SAINT 表现最佳，凸显注意力机制深度学习架构潜力。


<details>
  <summary>Details</summary>
Motivation: 早期帕金森病症状细微、生物医学数据关系复杂，传统机器学习模型难捕捉特征交互，需新方法。

Method: 使用 UCI 机器学习库基准数据集，对比 MLP、Gradient Boosting、TabNet 和 SAINT 四种分类模型。

Result: SAINT 在多评估指标上超所有基线模型，TabNet 和 MLP 有竞争力，Gradient Boosting 得分最低。

Conclusion: 注意力机制深度学习架构用于早期帕金森病检测有诊断潜力，动态特征表示在临床预测任务中很重要。

Abstract: Early and accurate detection of Parkinson's disease (PD) remains a critical challenge in medical diagnostics due to the subtlety of early-stage symptoms and the complex, non-linear relationships inherent in biomedical data. Traditional machine learning (ML) models, though widely applied to PD detection, often rely on extensive feature engineering and struggle to capture complex feature interactions. This study investigates the effectiveness of attention-based deep learning models for early PD detection using tabular biomedical data. We present a comparative evaluation of four classification models: Multi-Layer Perceptron (MLP), Gradient Boosting, TabNet, and SAINT, using a benchmark dataset from the UCI Machine Learning Repository consisting of biomedical voice measurements from PD patients and healthy controls.
  Experimental results show that SAINT consistently outperformed all baseline models across multiple evaluation metrics, achieving a weighted precision of 0.98, weighted recall of 0.97, weighted F1-score of 0.97, a Matthews Correlation Coefficient (MCC) of 0.9990, and the highest Area Under the ROC Curve (AUC-ROC). TabNet and MLP demonstrated competitive performance, while Gradient Boosting yielded the lowest overall scores. The superior performance of SAINT is attributed to its dual attention mechanism, which effectively models feature interactions within and across samples.
  These findings demonstrate the diagnostic potential of attention-based deep learning architectures for early Parkinson's disease detection and highlight the importance of dynamic feature representation in clinical prediction tasks.

</details>


### [339] [A Thermodynamic Theory of Learning Part II: Critical Period Closure and Continual Learning Failure](https://arxiv.org/abs/2602.07950)
*Daisuke Okanohara*

Main category: cs.LG

TL;DR: 学习在有限时间内具有不可逆性，本文从轨迹层面研究此不可逆性对持续学习的影响，指出有限时间学习的不可逆选择导致表征自由的损失，重构了灾难性遗忘的解释。


<details>
  <summary>Details</summary>
Motivation: 研究有限时间学习的不可逆性对持续学习的影响，重构灾难性遗忘的解释。

Method: 从轨迹层面研究不可逆性对持续学习的影响。

Result: 有限耗散约束可达解和学习路径，有限时间学习进行不可逆选择，导致表征自由丧失，持续性学习失败是由于先前学习导致的表征自由丧失。

Conclusion: 灾难性遗忘是由有限时间耗散带来的动态约束，而非直接的任务干扰。

Abstract: Learning performed over finite time is necessarily irreversible. In Part~I of this series, we modeled learning as a transport process in the space of parameter distributions and derived the Epistemic Speed Limit, which lower-bounds entropy production under finite-time learning.
  In this work (Part~II), we study the consequences of this irreversibility for continual learning from a trajectory-level perspective. We show that finite dissipation constrains not only which solutions are reachable, but which learning paths remain dynamically accessible.
  Although a continuum of task-equivalent realizations can achieve identical task performance, finite-time learning irreversibly selects among these realizations. This selection occurs through the progressive elimination of degrees of freedom that would otherwise enable structural reconfiguration. We refer to this phenomenon as \emph{critical period closure}: beyond a certain stage of learning, transitions between compatible representations become dynamically inaccessible under any finite dissipation budget.
  As a result, continual learning failure arises not from the absence of solutions satisfying multiple tasks, but from an irreversible loss of representational freedom induced by prior learning. This reframes catastrophic forgetting as a dynamical constraint imposed by finite-time dissipation, rather than direct task interference.

</details>


### [340] [An Explainable Multi-Task Similarity Measure: Integrating Accumulated Local Effects and Weighted Fréchet Distance](https://arxiv.org/abs/2602.07966)
*Pablo Hidalgo,Daniel Rodriguez*

Main category: cs.LG

TL;DR: 本文提出基于XAI技术的多任务相似度度量方法，并通过四个数据集验证其有效性，该方法适用于单任务和多任务场景，可支持任务关系探索和决策。


<details>
  <summary>Details</summary>
Motivation: 多任务学习需解决任务相似性相关问题，如哪些任务相似、为何相似、如何衡量相似等。

Method: 提出基于ALE曲线的多任务相似度度量方法，使用Fréchet距离比较ALE曲线，融入特征重要性，引入缩放因子，还给出复杂场景应用建议。

Result: 使用四个数据集验证该度量方法，结果表明其在表格和非表格数据上都符合对任务相似性的直观预期。

Conclusion: 该度量方法是探索任务关系和支持决策的有价值工具。

Abstract: In many machine learning contexts, tasks are often treated as interconnected components with the goal of leveraging knowledge transfer between them, which is the central aim of Multi-Task Learning (MTL). Consequently, this multi-task scenario requires addressing critical questions: which tasks are similar, and how and why do they exhibit similarity? In this work, we propose a multi-task similarity measure based on Explainable Artificial Intelligence (XAI) techniques, specifically Accumulated Local Effects (ALE) curves.
  ALE curves are compared using the Fréchet distance, weighted by the data distribution, and the resulting similarity measure incorporates the importance of each feature. The measure is applicable in both single-task learning scenarios, where each task is trained separately, and multi-task learning scenarios, where all tasks are learned simultaneously. The measure is model-agnostic, allowing the use of different machine learning models across tasks. A scaling factor is introduced to account for differences in predictive performance across tasks, and several recommendations are provided for applying the measure in complex scenarios.
  We validate this measure using four datasets, one synthetic dataset and three real-world datasets. The real-world datasets include a well-known Parkinson's dataset and a bike-sharing usage dataset -- both structured in tabular format -- as well as the CelebA dataset, which is used to evaluate the application of concept bottleneck encoders in a multitask learning setting. The results demonstrate that the measure aligns with intuitive expectations of task similarity across both tabular and non-tabular data, making it a valuable tool for exploring relationships between tasks and supporting informed decision-making.

</details>


### [341] [On Improving Neurosymbolic Learning by Exploiting the Representation Space](https://arxiv.org/abs/2602.07973)
*Aaditya Naik,Efthymia Tsamoura,Shibo Jin,Mayur Naik,Dan Roth*

Main category: cs.LG

TL;DR: 研究神经符号环境下学习神经分类器问题，提出CLIPPER技术剪枝标签组合空间，在16个基准测试中提升多种引擎性能。


<details>
  <summary>Details</summary>
Motivation: 解决神经符号环境下学习神经分类器时，标签组合空间指数级增长导致学习困难的问题。

Method: 提出CLIPPER技术，将剪枝过程建模为整数线性规划，利用相似潜在表示实例可能共享相同标签的直觉，同时考虑逻辑约束。

Result: 在16个复杂神经符号任务基准测试中，CLIPPER使Scallop、Dolphin和ISED等引擎性能分别提升达48%、53%和8%。

Conclusion: CLIPPER能有效提升神经符号引擎性能，达到新的准确率水平。

Abstract: We study the problem of learning neural classifiers in a neurosymbolic setting where the hidden gold labels of input instances must satisfy a logical formula. Learning in this setting proceeds by first computing (a subset of) the possible combinations of labels that satisfy the formula and then computing a loss using those combinations and the classifiers' scores. One challenge is that the space of label combinations can grow exponentially, making learning difficult. We propose a technique that prunes this space by exploiting the intuition that instances with similar latent representations are likely to share the same label. While this intuition has been widely used in weakly supervised learning, its application in our setting is challenging due to label dependencies imposed by logical constraints. We formulate the pruning process as an integer linear program that discards inconsistent label combinations while respecting logical structure. Our approach, CLIPPER, is orthogonal to existing training algorithms and can be seamlessly integrated with them. Across 16 benchmarks over complex neurosymbolic tasks, we demonstrate that CLIPPER boosts the performance of state-of-the-art neurosymbolic engines like Scallop, Dolphin, and ISED by up to 48%, 53%, and 8%, leading to state-of-the-art accuracies.

</details>


### [342] [Beyond Optimization: Intelligence as Metric-Topology Factorization under Geometric Incompleteness](https://arxiv.org/abs/2602.07974)
*Xin Li*

Main category: cs.LG

TL;DR: 提出Metric - Topology Factorization (MTF)作为统一几何原理，并引入Topological Urysohn Machine (TUM)，能解决传统持续学习方法的问题。


<details>
  <summary>Details</summary>
Motivation: 当代机器学习在分布变化、任务置换和持续学习中表现不佳，固定的表征几何方法会引发灾难性遗忘。

Method: 提出MTF，将稳定拓扑与可塑性度量变形分离；引入TUM，通过内存摊销度量推理（MAMI）实现MTF。

Result: 展示了固定度量在几何上的不完整性，MTF可通过几何切换而非重新优化实现快速适应。

Conclusion: TUM能解释对任务重排序的鲁棒性、抗灾难性遗忘能力以及在击败传统持续学习方法的变换上的泛化能力。

Abstract: Contemporary ML often equates intelligence with optimization: searching for solutions within a fixed representational geometry. This works in static regimes but breaks under distributional shift, task permutation, and continual learning, where even mild topological changes can invalidate learned solutions and trigger catastrophic forgetting. We propose Metric-Topology Factorization (MTF) as a unifying geometric principle: intelligence is not navigation through a fixed maze, but the ability to reshape representational geometry so desired behaviors become stable attractors. Learning corresponds to metric contraction (a controlled deformation of Riemannian structure), while task identity and environmental variation are encoded topologically and stored separately in memory. We show any fixed metric is geometrically incomplete: for any local metric representation, some topological transformations make it singular or incoherent, implying an unavoidable stability-plasticity tradeoff for weight-based systems. MTF resolves this by factorizing stable topology from plastic metric warps, enabling rapid adaptation via geometric switching rather than re-optimization. Building on this, we introduce the Topological Urysohn Machine (TUM), implementing MTF through memory-amortized metric inference (MAMI): spectral task signatures index amortized metric transformations, letting a single learned geometry be reused across permuted, reflected, or parity-altered environments. This explains robustness to task reordering, resistance to catastrophic forgetting, and generalization across transformations that defeat conventional continual learning methods (e.g., EWC).

</details>


### [343] [Regret Analysis of Unichain Average Reward Constrained MDPs with General Parameterization](https://arxiv.org/abs/2602.08000)
*Anirudh Satheesh,Vaneet Aggarwal*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study infinite-horizon average-reward constrained Markov decision processes (CMDPs) under the unichain assumption and general policy parameterizations. Existing regret analyses for constrained reinforcement learning largely rely on ergodicity or strong mixing-time assumptions, which fail to hold in the presence of transient states. We propose a primal--dual natural actor--critic algorithm that leverages multi-level Monte Carlo (MLMC) estimators and an explicit burn-in mechanism to handle unichain dynamics without requiring mixing-time oracles. Our analysis establishes finite-time regret and cumulative constraint violation bounds that scale as $\tilde{O}(\sqrt{T})$, up to approximation errors arising from policy and critic parameterization, thereby extending order-optimal guarantees to a significantly broader class of CMDPs.

</details>


### [344] [From $O(mn)$ to $O(r^2)$: Two-Sided Low-Rank Communication for Adam in Distributed Training with Memory Efficiency](https://arxiv.org/abs/2602.08007)
*Sizhe Dang,Jiaqi Shao,Xiaodong Zheng,Guang Dai,Yan Song,Haishan Ye*

Main category: cs.LG

TL;DR: 提出TSR-Adam优化器，通过双边低秩通信减少梯度同步通信量，在预训练和微调中降低通信量且性能相当，还给出理论分析。


<details>
  <summary>Details</summary>
Motivation: 基础模型扩展使数据并行分布式优化中带宽受限的梯度同步成瓶颈，现有投影低秩优化器在通信受限训练中欠佳。

Method: 提出TSR，为Adam族更新引入双边低秩通信，同步紧凑核心，采用随机SVD刷新避免全梯度同步，将低秩通信扩展到嵌入梯度。

Result: 在60M到1B模型规模预训练中，每步平均通信字节减少13倍，GLUE微调中通信量减少25倍，性能相当。

Conclusion: TSR-Adam能有效减少通信量，具有良好应用前景，理论分析为其提供理论支持。

Abstract: As foundation models continue to scale, pretraining increasingly relies on data-parallel distributed optimization, making bandwidth-limited gradient synchronization a key bottleneck. Orthogonally, projection-based low-rank optimizers were mainly designed for memory efficiency, but remain suboptimal for communication-limited training: one-sided synchronization still transmits an $O(rn)$ object for an $m\times n$ matrix gradient and refresh steps can dominate peak communicated bytes. We propose TSR, which brings two-sided low-rank communication to Adam-family updates (TSR-Adam) by synchronizing a compact core $U^\top G V\in\mathbb{R}^{r\times r}$, reducing the dominant per-step payload from $O(mn)$ to $O(r^2)$ while keeping moment states in low-dimensional cores. To further reduce the peak communication from subspace refresh, TSR-Adam adopts a randomized SVD-based refresh that avoids full-gradient synchronization. We additionally extend low-rank communication to embedding gradients with embedding-specific ranks and refresh schedules, yielding additional communication and memory savings over keeping embeddings dense. Across pretraining from 60M to 1B model scales, TSR-Adam reduces average communicated bytes per step by $13\times$, and on GLUE fine-tuning it reduces communication by $25\times$, while achieving comparable performance; we further provide a theoretical stationarity analysis for the proposed update. Code is available at https://github.com/DKmiyan/TSR-Adam.

</details>


### [345] [A Unified Density Operator View of Flow Control and Merging](https://arxiv.org/abs/2602.08012)
*Riccardo De Santi,Malte Franke,Ya-Ping Hsieh,Andreas Krause*

Main category: cs.LG

TL;DR: 提出统一概率空间框架解决预训练流控制奖励适配和多模型集成问题，引入RFM方法并给出理论保证，在分子设计等任务展示能力。


<details>
  <summary>Details</summary>
Motivation: 解决大规模流和扩散模型中的控制奖励适配和多模型集成两个基本算法挑战，当前方法分开处理，需要统一框架。

Method: 引入统一概率空间框架，介绍Reward - Guided Flow Merging (RFM) 方法将奖励引导的流合并转化为标准微调问题序列。

Result: 为奖励引导和纯流合并提供了理论保证，在示例设置中展示方法能力，可应用于高维分子设计和低能构象生成。

Conclusion: 提出的统一框架和RFM方法能有效解决相关算法挑战，在分子设计等领域有应用价值。

Abstract: Recent progress in large-scale flow and diffusion models raised two fundamental algorithmic challenges: (i) control-based reward adaptation of pre-trained flows, and (ii) integration of multiple models, i.e., flow merging. While current approaches address them separately, we introduce a unifying probability-space framework that subsumes both as limit cases, and enables reward-guided flow merging, allowing principled, task-aware combination of multiple pre-trained flows (e.g., merging priors while maximizing drug-discovery utilities). Our formulation renders possible to express a rich family of operators over generative models densities, including intersection (e.g., to enforce safety), union (e.g., to compose diverse models), interpolation (e.g., for discovery), their reward-guided counterparts, as well as complex logical expressions via generative circuits. Next, we introduce Reward-Guided Flow Merging (RFM), a mirror-descent scheme that reduces reward-guided flow merging to a sequence of standard fine-tuning problems. Then, we provide first-of-their-kind theoretical guarantees for reward-guided and pure flow merging via RFM. Ultimately, we showcase the capabilities of the proposed method on illustrative settings providing visually interpretable insights, and apply our method to high-dimensional de-novo molecular design and low-energy conformer generation.

</details>


### [346] [The Rise of Sparse Mixture-of-Experts: A Survey from Algorithmic Foundations to Decentralized Architectures and Vertical Domain Applications](https://arxiv.org/abs/2602.08019)
*Dong Pan,Bingtao Li,Yongsheng Zheng,Jiren Ma,Victor Fei*

Main category: cs.LG

TL;DR: 本文对稀疏专家混合（MoE）架构进行全面综述，介绍原理、拓展到去中心化范式、探讨垂直领域应用，指出挑战与未来方向，是该领域最全面的综述。


<details>
  <summary>Details</summary>
Motivation: 现有关于MoE模型的综述存在覆盖不全、关键领域探索不深入等问题，缺乏对其在重要领域最新进展的系统探索，因此开展本次综述。

Method: 先研究MoE的基础原理和核心组件，从集中范式拓展到去中心化范式，探索其垂直领域应用，最后识别关键挑战和未来研究方向。

Result: 完成了对MoE的全面综述，涵盖原理、范式拓展、应用及挑战与方向。

Conclusion: 本综述是目前MoE领域最全面的，可为研究人员和从业者提供有价值的参考，助其了解最新进展。

Abstract: The sparse Mixture of Experts(MoE) architecture has evolved as a powerful approach for scaling deep learning models to more parameters with comparable computation cost. As an important branch of large language model(LLM), MoE model only activate a subset of experts based on a routing network. This sparse conditional computation mechanism significantly improves computational efficiency, paving a promising path for greater scalability and cost-efficiency. It not only enhance downstream applications such as natural language processing, computer vision, and multimodal in various horizontal domains, but also exhibit broad applicability across vertical domains. Despite the growing popularity and application of MoE models across various domains, there lacks a systematic exploration of recent advancements of MoE in many important fields. Existing surveys on MoE suffer from limitations such as lack coverage or none extensively exploration of key areas. This survey seeks to fill these gaps. In this paper, Firstly, we examine the foundational principles of MoE, with an in-depth exploration of its core components-the routing network and expert network. Subsequently, we extend beyond the centralized paradigm to the decentralized paradigm, which unlocks the immense untapped potential of decentralized infrastructure, enables democratization of MoE development for broader communities, and delivers greater scalability and cost-efficiency. Furthermore we focus on exploring its vertical domain applications. Finally, we also identify key challenges and promising future research directions. To the best of our knowledge, this survey is currently the most comprehensive review in the field of MoE. We aim for this article to serve as a valuable resource for both researchers and practitioners, enabling them to navigate and stay up-to-date with the latest advancements.

</details>


### [347] [Horizon Imagination: Efficient On-Policy Training in Diffusion World Models](https://arxiv.org/abs/2602.08032)
*Lior Cohen,Ofir Nabati,Kaixin Wang,Navdeep Kumar,Shie Mannor*

Main category: cs.LG

TL;DR: 提出Horizon Imagination (HI)用于强化学习扩散世界模型，提升控制效率，实验效果好。


<details>
  <summary>Details</summary>
Motivation: 当前扩散世界模型在强化学习控制中存在效率挑战，现有方法计算成本高。

Method: 提出HI，为离散随机策略设计的在线想象过程，可并行去噪多个未来观测，包含稳定机制和新采样策略。

Result: 在Atari 100K和Craftium实验中，用半去噪步骤的子帧预算保持控制性能，不同调度下生成质量更优。

Conclusion: HI能提升扩散世界模型在强化学习控制中的效率。

Abstract: We study diffusion-based world models for reinforcement learning, which offer high generative fidelity but face critical efficiency challenges in control. Current methods either require heavyweight models at inference or rely on highly sequential imagination, both of which impose prohibitive computational costs. We propose Horizon Imagination (HI), an on-policy imagination process for discrete stochastic policies that denoises multiple future observations in parallel. HI incorporates a stabilization mechanism and a novel sampling schedule that decouples the denoising budget from the effective horizon over which denoising is applied while also supporting sub-frame budgets. Experiments on Atari 100K and Craftium show that our approach maintains control performance with a sub-frame budget of half the denoising steps and achieves superior generation quality under varied schedules. Code is available at https://github.com/leor-c/horizon-imagination.

</details>


### [348] [The Benefits of Diversity: Combining Comparisons and Ratings for Efficient Scoring](https://arxiv.org/abs/2602.08033)
*Julien Fageot,Matthias Grossglauser,Lê-Nguyên Hoang,Matteo Tacchi-Bénard,Oscar Villemaud*

Main category: cs.LG

TL;DR: 研究表明结合个体和比较两种偏好获取形式优于单一形式，介绍统一概率模型SCoRa并验证其性能。


<details>
  <summary>Details</summary>
Motivation: 解决人类评估实体时应采用个体评估还是比较评估的长期争论问题。

Method: 引入统一概率模型SCoRa，使其能从两种信号中学习，并对其MAP估计器进行理论证明和实证检验。

Result: SCoRa的MAP估计器表现良好，验证了单调性和鲁棒性，即使在模型不匹配时也能恢复准确分数，且在特定现实场景下结合两种信号优于单一信号。

Conclusion: 鉴于多种形式信号的实际可用性，SCoRa为偏好学习提供了通用基础。

Abstract: Should humans be asked to evaluate entities individually or comparatively? This question has been the subject of long debates. In this work, we show that, interestingly, combining both forms of preference elicitation can outperform the focus on a single kind. More specifically, we introduce SCoRa (Scoring from Comparisons and Ratings), a unified probabilistic model that allows to learn from both signals. We prove that the MAP estimator of SCoRa is well-behaved. It verifies monotonicity and robustness guarantees. We then empirically show that SCoRa recovers accurate scores, even under model mismatch. Most interestingly, we identify a realistic setting where combining comparisons and ratings outperforms using either one alone, and when the accurate ordering of top entities is critical. Given the de facto availability of signals of multiple forms, SCoRa additionally offers a versatile foundation for preference learning.

</details>


### [349] [TAAM:Inductive Graph-Class Incremental Learning with Task-Aware Adaptive Modulation](https://arxiv.org/abs/2602.08036)
*Jingtao Liu,Xinming Zhang*

Main category: cs.LG

TL;DR: 本文提出TAAM和AMP方法解决图持续学习问题，在严格场景实验中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有图持续学习方法依赖回放策略，有内存限制、隐私问题且难解决稳定性 - 可塑性困境，同时要处理现实中未知任务ID的问题。

Method: 提出Task - Aware Adaptive Modulation (TAAM)，利用轻量级Neural Synapse Modulators (NSMs)对共享GNN骨干的计算流进行自适应调制；提出Anchored Multi - hop Propagation (AMP)处理未知任务ID问题，并在严格的归纳学习场景进行实验。

Result: TAAM在八个数据集上全面优于现有方法。

Conclusion: 轻量级、特定任务模块能有效引导固定GNN骨干推理，TAAM是解决图持续学习问题的有效方法。

Abstract: Graph Continual Learning (GCL) aims to solve the challenges of streaming graph data. However, current methods often depend on replay-based strategies, which raise concerns like memory limits and privacy issues, while also struggling to resolve the stability-plasticity dilemma. In this paper, we suggest that lightweight, task-specific modules can effectively guide the reasoning process of a fixed GNN backbone. Based on this idea, we propose Task-Aware Adaptive Modulation (TAAM). The key component of TAAM is its lightweight Neural Synapse Modulators (NSMs). For each new task, a dedicated NSM is trained and then frozen, acting as an "expert module." These modules perform detailed, node-attentive adaptive modulation on the computational flow of a shared GNN backbone. This setup ensures that new knowledge is kept within compact, task-specific modules, naturally preventing catastrophic forgetting without using any data replay. Additionally, to address the important challenge of unknown task IDs in real-world scenarios, we propose and theoretically prove a novel method named Anchored Multi-hop Propagation (AMP). Notably, we find that existing GCL benchmarks have flaws that can cause data leakage and biased evaluations. Therefore, we conduct all experiments in a more rigorous inductive learning scenario. Extensive experiments show that TAAM comprehensively outperforms state-of-the-art methods across eight datasets. Code and Datasets are available at: https://github.com/1iuJT/TAAM_AAMAS2026.

</details>


### [350] [FIRE: Frobenius-Isometry Reinitialization for Balancing the Stability-Plasticity Tradeoff](https://arxiv.org/abs/2602.08040)
*Isaac Han,Sangyeon Park,Seungwon Oh,Donghu Kim,Hojoon Lee,Kyung-Joong Kim*

Main category: cs.LG

TL;DR: 提出FIRE重初始化方法平衡深度神经网络在非平稳数据训练中的稳定性和可塑性，在多领域表现优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 标准重初始化方法难以调节，保守重初始化无法恢复可塑性，激进重初始化会抹去有用知识，需要新方法平衡稳定性和可塑性。

Method: 提出FIRE方法，通过SFE量化稳定性、DfI量化可塑性，通过求解约束优化问题得到重初始化点，用Newton - Schulz迭代近似求解。

Result: 在持续视觉学习、语言建模和强化学习等领域，FIRE始终优于无干预的朴素训练和标准重初始化方法。

Conclusion: FIRE能有效平衡稳定性 - 可塑性权衡。

Abstract: Deep neural networks trained on nonstationary data must balance stability (i.e., retaining prior knowledge) and plasticity (i.e., adapting to new tasks). Standard reinitialization methods, which reinitialize weights toward their original values, are widely used but difficult to tune: conservative reinitializations fail to restore plasticity, while aggressive ones erase useful knowledge. We propose FIRE, a principled reinitialization method that explicitly balances the stability-plasticity tradeoff. FIRE quantifies stability through Squared Frobenius Error (SFE), measuring proximity to past weights, and plasticity through Deviation from Isometry (DfI), reflecting weight isotropy. The reinitialization point is obtained by solving a constrained optimization problem, minimizing SFE subject to DfI being zero, which is efficiently approximated by Newton-Schulz iteration. FIRE is evaluated on continual visual learning (CIFAR-10 with ResNet-18), language modeling (OpenWebText with GPT-0.1B), and reinforcement learning (HumanoidBench with SAC and Atari games with DQN). Across all domains, FIRE consistently outperforms both naive training without intervention and standard reinitialization methods, demonstrating effective balancing of the stability-plasticity tradeoff.

</details>


### [351] [Implicit Strategic Optimization: Rethinking Long-Horizon Decision-Making in Adversarial Poker Environments](https://arxiv.org/abs/2602.08041)
*Boyang Xia,Weiyou Tian,Qingnan Ren,Jiaqi Huang,Jie Xiao,Shuo Lu,Kai Wang,Lynn Ai,Eric Yang,Bill Shi*

Main category: cs.LG

TL;DR: 提出Implicit Strategic Optimization (ISO)框架解决长周期对抗游戏中大语言模型智能体训练问题，实验显示有改进。


<details>
  <summary>Details</summary>
Motivation: 长周期对抗游戏中，近视优化和基于变分的遗憾分析在收益受潜在战略外部性影响时失效，需新方法。

Method: 引入ISO框架，结合估计行动长期战略价值的战略奖励模型（SRM）和上下文条件乐观学习规则iso - grpo。

Result: 在6人无限注德州扑克和宝可梦竞技实验中，相比强大的大语言模型和强化学习基线，长期回报持续改善，在可控预测噪声下性能平稳下降。

Conclusion: 证明了次线性上下文遗憾和均衡收敛保证，在预测误差有界时能恢复战略外部性已知时的静态游戏率。

Abstract: Training large language model (LLM) agents for adversarial games is often driven by episodic objectives such as win rate. In long-horizon settings, however, payoffs are shaped by latent strategic externalities that evolve over time, so myopic optimization and variation-based regret analyses can become vacuous even when the dynamics are predictable. To solve this problem, we introduce Implicit Strategic Optimization (ISO), a prediction-aware framework in which each agent forecasts the current strategic context and uses it to update its policy online. ISO combines a Strategic Reward Model (SRM) that estimates the long-run strategic value of actions with iso-grpo, a context-conditioned optimistic learning rule. We prove sublinear contextual regret and equilibrium convergence guarantees whose dominant terms scale with the number of context mispredictions; when prediction errors are bounded, our bounds recover the static-game rates obtained when strategic externalities are known. Experiments in 6-player No-Limit Texas Hold'em and competitive Pokemon show consistent improvements in long-term return over strong LLM and RL baselines, and graceful degradation under controlled prediction noise.

</details>


### [352] [V-ABFT: Variance-Based Adaptive Threshold for Fault-Tolerant Matrix Multiplication in Mixed-Precision Deep Learning](https://arxiv.org/abs/2602.08043)
*Yiheng Gao,Qin Hua,Zizhong Chen*

Main category: cs.LG

TL;DR: 论文提出V-ABFT算法，在矩阵乘法中检测SDCs，降低阈值与实际误差比，提高检测粒度，复杂度低且效果好，已集成到NPU和GPU。


<details>
  <summary>Details</summary>
Motivation: 现有ABFT阈值确定方法存在问题，分析界过于保守，概率方法生成的阈值比实际舍入误差大很多。

Method: 提出基于方差的自适应阈值算法V-ABFT，通过直接对验证差异建模实现更紧密的误差界限；利用统计方差估计；使用max/min/mean统计量，复杂度为O(n)。

Result: V-ABFT将阈值与实际误差比降低，相比A-ABFT有6 - 48倍提升，保持零误报率；低精度GEMM能使用FP32级阈值，检测粒度约提高1000倍；在多种数据和模型上验证有效；复杂度低于A-ABFT。

Conclusion: V-ABFT算法有效，平台无关，可集成到容错GEMM实现中。

Abstract: Algorithm-Based Fault Tolerance (ABFT) is widely adopted to detect silent data corruptions (SDCs) in matrix multiplication, a cornerstone operation in deep learning systems. However, existing threshold determination methods face critical challenges: analytical bounds are overly conservative, while probabilistic approaches like A-ABFT yield thresholds $160$--$4200\times$ larger than actual rounding errors. We present V-ABFT, a variance-based adaptive threshold algorithm that achieves tighter error bounds by directly modeling the verification difference. By leveraging statistical variance estimation, V-ABFT reduces the threshold-to-actual-error ratio to approximately $7$--$20\times$ for FP32/FP64 and $48$--$158\times$ for BF16, representing a \textbf{6--48$\times$ improvement} over A-ABFT while maintaining zero false positive rate across BF16, FP16, FP32, and FP64 precisions. Furthermore, we demonstrate that for fused-kernel ABFT implementations that verify before output quantization, low-precision GEMM can use FP32-level thresholds ($e_{\max} \approx 10^{-6}$), enabling \textbf{$\sim$1000$\times$ finer detection granularity} compared to offline verification with low-precision output ($e_{\max} \approx 10^{-3}$). We reproduce A-ABFT's experimental setup and validate our implementation against the original paper's results. Our method requires only $O(n)$ complexity using max/min/mean statistics, compared to A-ABFT's $O(pn)$ for finding $p$ largest values. Extensive experiments on synthetic data and real model weights (LLaMA-7B, GPT-2, ViT) demonstrate V-ABFT's effectiveness across diverse distributions. V-ABFT is platform-agnostic and has been integrated into fault-tolerant GEMM implementations on both NPUs and GPUs.

</details>


### [353] [Interpretable Fuzzy Systems For Forward Osmosis Desalination](https://arxiv.org/abs/2602.08050)
*Qusai Khaled,Uzay Kaymak,Laura Genga*

Main category: cs.LG

TL;DR: 提出人在环方法开发可解释模糊规则系统预测反渗透淡化生产率，性能相当且保持语义可解释性。


<details>
  <summary>Details</summary>
Motivation: 水处理中模糊规则系统需保持可解释性，以往结构可解释性有进展，但语义可解释性因模糊集区分性低而受影响。

Method: 集成专家驱动的网格划分以生成可区分的隶属函数、领域引导的特征工程以减少冗余和基于触发强度的规则修剪。

Result: 该方法与基于聚类的模糊规则系统有相当的预测性能，保持了语义可解释性并满足结构复杂性约束。

Conclusion: 为水处理应用提供了一个可解释的解决方案。

Abstract: Preserving interpretability in fuzzy rule-based systems (FRBS) is vital for water treatment, where decisions impact public health. While structural interpretability has been addressed using multi-objective algorithms, semantic interpretability often suffers due to fuzzy sets with low distinguishability. We propose a human-in-the-loop approach for developing interpretable FRBS to predict forward osmosis desalination productivity. Our method integrates expert-driven grid partitioning for distinguishable membership functions, domain-guided feature engineering to reduce redundancy, and rule pruning based on firing strength. This approach achieved comparable predictive performance to cluster-based FRBS while maintaining semantic interpretability and meeting structural complexity constraints, providing an explainable solution for water treatment applications.

</details>


### [354] [Epigraph-Guided Flow Matching for Safe and Performant Offline Reinforcement Learning](https://arxiv.org/abs/2602.08054)
*Manan Tayal,Mumuksh Tayal*

Main category: cs.LG

TL;DR: 提出EpiFlow框架解决安全离线强化学习中平衡安全性与性能的难题，在多任务中表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有安全离线强化学习方法难以在固定数据集上兼顾安全性和性能，存在约束易被违反、过于保守、难以平衡多目标等问题。

Method: 将安全离线强化学习表述为状态约束最优控制问题，学习基于最优控制问题上图重表述的可行性价值函数，通过基于该函数对行为分布重新加权并通过流匹配拟合生成策略来合成策略。

Result: 在包括Safety - Gymnasium基准在内的各种安全关键任务中，EpiFlow实现了有竞争力的回报，且经验安全违规接近零。

Conclusion: 证明了上图引导策略合成的有效性。

Abstract: Offline reinforcement learning (RL) provides a compelling paradigm for training autonomous systems without the risks of online exploration, particularly in safety-critical domains. However, jointly achieving strong safety and performance from fixed datasets remains challenging. Existing safe offline RL methods often rely on soft constraints that allow violations, introduce excessive conservatism, or struggle to balance safety, reward optimization, and adherence to the data distribution. To address this, we propose Epigraph-Guided Flow Matching (EpiFlow), a framework that formulates safe offline RL as a state-constrained optimal control problem to co-optimize safety and performance. We learn a feasibility value function derived from an epigraph reformulation of the optimal control problem, thereby avoiding the decoupled objectives or post-hoc filtering common in prior work. Policies are synthesized by reweighting the behavior distribution based on this epigraph value function and fitting a generative policy via flow matching, enabling efficient, distribution-consistent sampling. Across various safety-critical tasks, including Safety-Gymnasium benchmarks, EpiFlow achieves competitive returns with near-zero empirical safety violations, demonstrating the effectiveness of epigraph-guided policy synthesis.

</details>


### [355] [Compiler-Assisted Speculative Sampling for Accelerated LLM Inference on Heterogeneous Edge Devices](https://arxiv.org/abs/2602.08060)
*Alejandro Ruiz y Mesa,Guilherme Korol,Moritz Riesteter,João Paulo Cardoso de Lima,Jeronimo Castrillon*

Main category: cs.LG

TL;DR: 针对资源受限边缘设备上大语言模型部署的延迟问题，解决推测解码在边缘设备应用的两大挑战，用分析成本模型指导分区，在边缘设备验证获 1.68 倍加速。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在资源受限边缘设备部署有严重延迟问题，推测解码虽有潜力但在边缘应用面临两大挑战。

Method: 使用分析成本模型探索异构硬件配置，指导大语言模型子图粗粒度分区，针对边缘典型短输入序列长度。

Result: 在含六核 Cortex - A CPU 和 Mali GPU 的边缘设备验证，翻译任务加速达 1.68 倍，与分析预期接近。

Conclusion: 利用分析成本模型可有效应对推测解码在边缘设备应用的挑战，提高大语言模型在边缘设备的执行效率。

Abstract: LLM deployment on resource-constrained edge devices faces severe latency constraints, particularly in real-time applications where delayed responses can compromise safety or usability. Among many approaches to mitigate the inefficiencies of sequential token-by-token generation, Speculative Decoding (SD) has emerged as a promising technique. However, SD at the edge is hindered by two major challenges: (1) integrating SD into a compiler-based workflow without sacrificing performance or programmability, and (2) exploiting the heterogeneous compute resources of modern SoCs through carefully designed partitioning strategies. This work addresses these challenges by using an analytical cost model that explores heterogeneous hardware configurations and guides coarse-grained partitioning of LLM subgraphs, particularly with edge-typical short input sequence lengths. The cost model predicts when speculative sampling and heterogeneous execution are jointly beneficial and is validated on an edge device featuring a hexacore Cortex-A CPU and a Mali GPU, revealing up to 1.68$\times$ speedup for translation tasks, closely matching analytic expectations.

</details>


### [356] [Efficient and Adaptable Detection of Malicious LLM Prompts via Bootstrap Aggregation](https://arxiv.org/abs/2602.08062)
*Shayan Ali Hassan,Tao Ni,Zafar Ayyub Qazi,Marco Canini*

Main category: cs.LG

TL;DR: 现有大语言模型恶意提示检测防御方法有局限，本文提出轻量级可增量更新框架BAGEL，表现出色，小模型集成可媲美大参数防护机制。


<details>
  <summary>Details</summary>
Motivation: 大语言模型易受恶意提示影响，且现有防御方法在性能、效率和适应性方面存在局限。

Method: 提出BAGEL框架，采用引导聚合和专家混合的微调模型集成，推理时用随机森林路由器选择成员，有新攻击时增量更新。

Result: BAGEL选择5个集成成员（4.3亿参数）时F1分数达0.92，优于OpenAI Moderation API和ShieldGemma，多次增量更新后性能稳健，且具有可解释性。

Conclusion: 小的微调分类器集成可匹配或超越数十亿参数的防护机制，还能满足生产系统对适应性和效率的要求。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language understanding, reasoning, and generation. However, these systems remain susceptible to malicious prompts that induce unsafe or policy-violating behavior through harmful requests, jailbreak techniques, and prompt injection attacks. Existing defenses face fundamental limitations: black-box moderation APIs offer limited transparency and adapt poorly to evolving threats, while white-box approaches using large LLM judges impose prohibitive computational costs and require expensive retraining for new attacks. Current systems force designers to choose between performance, efficiency, and adaptability.
  To address these challenges, we present BAGEL (Bootstrap AGgregated Ensemble Layer), a modular, lightweight, and incrementally updatable framework for malicious prompt detection. BAGEL employs a bootstrap aggregation and mixture of expert inspired ensemble of fine-tuned models, each specialized on a different attack dataset. At inference, BAGEL uses a random forest router to identify the most suitable ensemble member, then applies stochastic selection to sample additional members for prediction aggregation. When new attacks emerge, BAGEL updates incrementally by fine-tuning a small prompt-safety classifier (86M parameters) and adding the resulting model to the ensemble. BAGEL achieves an F1 score of 0.92 by selecting just 5 ensemble members (430M parameters), outperforming OpenAI Moderation API and ShieldGemma which require billions of parameters. Performance remains robust after nine incremental updates, and BAGEL provides interpretability through its router's structural features. Our results show ensembles of small finetuned classifiers can match or exceed billion-parameter guardrails while offering the adaptability and efficiency required for production systems.

</details>


### [357] [Efficient Distribution Learning with Error Bounds in Wasserstein Distance](https://arxiv.org/abs/2602.08063)
*Eduardo Figueiredo,Steven Adams,Luca Laurenti*

Main category: cs.LG

TL;DR: 本文提出新框架，利用最优传输等技术，从有限样本近似未知概率分布并界定Wasserstein距离，算法在基准测试中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 学习Wasserstein距离下未知分布的非渐近且易计算的误差界是众多领域的基本问题。

Method: 利用最优传输、非线性优化和集中不等式，通过求解混合整数线性规划问题界定Wasserstein距离，开发智能聚类算法寻找近似分布的支撑集。

Result: 该方法在基准测试中表现优于现有方法，返回的近似分布支撑集更小、误差界更紧。

Conclusion: 所提出的算法框架能有效近似未知概率分布并界定Wasserstein距离，且性能优越。

Abstract: The Wasserstein distance has emerged as a key metric to quantify distances between probability distributions, with applications in various fields, including machine learning, control theory, decision theory, and biological systems. Consequently, learning an unknown distribution with non-asymptotic and easy-to-compute error bounds in Wasserstein distance has become a fundamental problem in many fields. In this paper, we devise a novel algorithmic and theoretical framework to approximate an unknown probability distribution $\mathbb{P}$ from a finite set of samples by an approximate discrete distribution $\widehat{\mathbb{P}}$ while bounding the Wasserstein distance between $\mathbb{P}$ and $\widehat{\mathbb{P}}$. Our framework leverages optimal transport, nonlinear optimization, and concentration inequalities. In particular, we show that, even if $\mathbb{P}$ is unknown, the Wasserstein distance between $\mathbb{P}$ and $\widehat{\mathbb{P}}$ can be efficiently bounded with high confidence by solving a tractable optimization problem (a mixed integer linear program) of a size that only depends on the size of the support of $\widehat{\mathbb{P}}$. This enables us to develop intelligent clustering algorithms to optimally find the support of $\widehat{\mathbb{P}}$ while minimizing the Wasserstein distance error. On a set of benchmarks, we demonstrate that our approach outperforms state-of-the-art comparable methods by generally returning approximating distributions with substantially smaller support and tighter error bounds.

</details>


### [358] [SiameseNorm: Breaking the Barrier to Reconciling Pre/Post-Norm](https://arxiv.org/abs/2602.08064)
*Tianyu Li,Dongchen Han,Zixuan Cao,Haofeng Huang,Mengyu Zhou,Ming Chen,Erchao Zhao,Xiaoxi Jiang,Guanjun Jiang,Gao Huang*

Main category: cs.LG

TL;DR: 现代Transformer多用Pre - Norm，为结合Pre - Norm和Post - Norm优势提出SiameseNorm，实验展示出优势。


<details>
  <summary>Details</summary>
Motivation: 现代Transformer主要采用Pre - Norm，之前结合Pre - Norm和Post - Norm优势的尝试存在稳定性 - 性能权衡问题，原因是单流设计结构不兼容。

Method: 提出SiameseNorm，一种双流架构，将类似Pre - Norm和Post - Norm的流与共享参数结合，解耦两个流的优化动态。

Result: 在13亿参数模型上的预训练实验表明，SiameseNorm具有出色的优化鲁棒性，始终优于强大的基线。

Conclusion: SiameseNorm能有效结合Pre - Norm和Post - Norm的优势。

Abstract: Modern Transformers predominantly adopt the Pre-Norm paradigm for its optimization stability, foregoing the superior potential of the unstable Post-Norm architecture. Prior attempts to combine their strengths typically lead to a stability-performance trade-off. We attribute this phenomenon to a structural incompatibility within a single-stream design: Any application of the Post-Norm operation inevitably obstructs the clean identity gradient preserved by Pre-Norm. To fundamentally reconcile these paradigms, we propose SiameseNorm, a two-stream architecture that couples Pre-Norm-like and Post-Norm-like streams with shared parameters. This design decouples the optimization dynamics of the two streams, retaining the distinct characteristics of both Pre-Norm and Post-Norm by enabling all residual blocks to receive combined gradients inherited from both paradigms, where one stream secures stability while the other enhances expressivity. Extensive pre-training experiments on 1.3B-parameter models demonstrate that SiameseNorm exhibits exceptional optimization robustness and consistently outperforms strong baselines. Code is available at https://github.com/Qwen-Applications/SiameseNorm.

</details>


### [359] [Enhancing Bandit Algorithms with LLMs for Time-varying User Preferences in Streaming Recommendations](https://arxiv.org/abs/2602.08067)
*Chenglei Shen,Yi Zhan,Weijie Yu,Xiao Zhang,Jun Xu*

Main category: cs.LG

TL;DR: 提出HyperBandit+解决现有基于bandit方法忽视时间与用户偏好关系及早期探索效率低的问题，实验显示其优于现有基线方法


<details>
  <summary>Details</summary>
Motivation: 现有基于bandit的方法忽视时间与用户偏好的明确关系，在线学习方法早期探索效率低

Method: 提出HyperBandit+，集成时间感知超网络，采用大语言模型辅助的冷启动机制，用低秩分解降低训练复杂度

Result: 理论上建立了考虑超网络和大语言模型冷启动机制的次线性遗憾上界，实验表明HyperBandit+在累积奖励上始终优于现有基线方法

Conclusion: HyperBandit+能有效适应时变的用户偏好，提升早期探索效率，满足实时流推荐需求

Abstract: In real-world streaming recommender systems, user preferences evolve dynamically over time. Existing bandit-based methods treat time merely as a timestamp, neglecting its explicit relationship with user preferences and leading to suboptimal performance. Moreover, online learning methods often suffer from inefficient exploration-exploitation during the early online phase. To address these issues, we propose HyperBandit+, a novel contextual bandit policy that integrates a time-aware hypernetwork to adapt to time-varying user preferences and employs a large language model-assisted warm-start mechanism (LLM Start) to enhance exploration-exploitation efficiency in the early online phase. Specifically, HyperBandit+ leverages a neural network that takes time features as input and generates parameters for estimating time-varying rewards by capturing the correlation between time and user preferences. Additionally, the LLM Start mechanism employs multi-step data augmentation to simulate realistic interaction data for effective offline learning, providing warm-start parameters for the bandit policy in the early online phase. To meet real-time streaming recommendation demands, we adopt low-rank factorization to reduce hypernetwork training complexity. Theoretically, we rigorously establish a sublinear regret upper bound that accounts for both the hypernetwork and the LLM warm-start mechanism. Extensive experiments on real-world datasets demonstrate that HyperBandit+ consistently outperforms state-of-the-art baselines in terms of accumulated rewards.

</details>


### [360] [Multimodal normative modeling in Alzheimers Disease with introspective variational autoencoders](https://arxiv.org/abs/2602.08077)
*Sayantan Kumar,Peijie Qiu,Aristeidis Sotiras*

Main category: cs.LG

TL;DR: 提出mmSIVAE改进阿尔茨海默病规范建模，提升重建与离群检测效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于VAE的阿尔茨海默病规范模型存在拟合健康参考分布不完美、多模态融合弱等问题。

Method: 提出mmSIVAE，结合MOPOE聚合改进参考保真度和多模态集成，在潜在和特征空间计算偏差分数。

Result: mmSIVAE在保留的对照组上改善重建，产生更具判别性的偏差分数，偏差图突出与AD相关的区域模式。

Conclusion: 强调规范建模中优先考虑参考分布保真度和稳健多模态后验聚合的训练目标的重要性。

Abstract: Normative modeling learns a healthy reference distribution and quantifies subject-specific deviations to capture heterogeneous disease effects. In Alzheimers disease (AD), multimodal neuroimaging offers complementary signals but VAE-based normative models often (i) fit the healthy reference distribution imperfectly, inflating false positives, and (ii) use posterior aggregation (e.g., PoE/MoE) that can yield weak multimodal fusion in the shared latent space. We propose mmSIVAE, a multimodal soft-introspective variational autoencoder combined with Mixture-of-Product-of-Experts (MOPOE) aggregation to improve reference fidelity and multimodal integration. We compute deviation scores in latent space and feature space as distances from the learned healthy distributions, and map statistically significant latent deviations to regional abnormalities for interpretability. On ADNI MRI regional volumes and amyloid PET SUVR, mmSIVAE improves reconstruction on held-out controls and produces more discriminative deviation scores for outlier detection than VAE baselines, with higher likelihood ratios and clearer separation between control and AD-spectrum cohorts. Deviation maps highlight region-level patterns aligned with established AD-related changes. More broadly, our results highlight the importance of training objectives that prioritize reference-distribution fidelity and robust multimodal posterior aggregation for normative modeling, with implications for deviation-based analysis across multimodal clinical data.

</details>


### [361] [Spectral Guardrails for Agents in the Wild: Detecting Tool Use Hallucinations via Attention Topology](https://arxiv.org/abs/2602.08082)
*Valentin Noël*

Main category: cs.LG

TL;DR: 提出基于注意力拓扑频谱分析的无训练护栏，在模型上取得高召回率，发现频谱特征可检测幻觉，揭示“Loud Liar”现象，证明频谱分析对智能体安全有效。


<details>
  <summary>Details</summary>
Motivation: 为野外自主智能体部署提供可靠防护，应对工具使用失败问题。

Method: 采用注意力拓扑频谱分析方法，无需标记训练数据。

Result: 在Llama 3.1 8B上多特征检测召回率达97.7%，平衡部署时召回率86.1%、精确率81.0%；单一层频谱特征检测幻觉效果好；揭示“Loud Liar”现象，Mistral 7B辨别能力最佳（AUC 0.900）。

Conclusion: 频谱分析是智能体安全的有效框架。

Abstract: Deploying autonomous agents in the wild requires reliable safeguards against tool use failures. We propose a training free guardrail based on spectral analysis of attention topology that complements supervised approaches. On Llama 3.1 8B, our method achieves 97.7\% recall with multi-feature detection and 86.1\% recall with 81.0\% precision for balanced deployment, without requiring any labeled training data. Most remarkably, we discover that single layer spectral features act as near-perfect hallucination detectors: Llama L26 Smoothness achieves 98.2\% recall (213/217 hallucinations caught) with a single threshold, and Mistral L3 Entropy achieves 94.7\% recall. This suggests hallucination is not merely a wrong token but a thermodynamic state change: the model's attention becomes noise when it errs. Through controlled cross-model evaluation on matched domains ($N=1000$, $T=0.3$, same General domain, hallucination rates 20--22\%), we reveal the ``Loud Liar'' phenomenon: Llama 3.1 8B's failures are spectrally catastrophic and dramatically easier to detect, while Mistral 7B achieves the best discrimination (AUC 0.900). These findings establish spectral analysis as a principled, efficient framework for agent safety.

</details>


### [362] [Probability Hacking and the Design of Trustworthy ML for Signal Processing in C-UAS: A Scenario Based Method](https://arxiv.org/abs/2602.08086)
*Liisa Janssens,Laura Middeldorp*

Main category: cs.LG

TL;DR: 本文探讨用机器学习增强C - UAS，通过场景法应对概率攻击，提升C - UAS可信度。


<details>
  <summary>Details</summary>
Motivation: 为有效应对无人机系统威胁，需增强反无人机系统（C - UAS）能力，利用新兴技术如人工智能可提升其有效性。

Method: 应用基于场景的方法对采用机器学习（AI子集）增强的C - UAS进行研究。

Result: 将概率攻击作为挑战，确定可在现有法治机制中实施的要求。

Conclusion: 这些要求增强了C - UAS的可信度，是实现人机协作的关键，在军民领域均适用。

Abstract: In order to counter the various threats manifested by Unmanned Aircraft Systems (UAS) adequately, specialized Counter Unmanned Aircraft Systems (C-UAS) are required. Enhancing C-UAS with Emerging and Disruptive Technologies (EDTs) such as Artificial Intelligence (AI) can lead to more effective countermeasures. In this paper a scenario-based method is applied to C-UAS augmented with Machine Learning (ML), a subset of AI, that can enhance signal processing capabilities. Via the scenarios-based method we frame in this paper probability hacking as a challenge and identify requirements which can be implemented in existing Rule of Law mechanisms to prevent probability hacking. These requirements strengthen the trustworthiness of the C-UAS, which feed into justified trust - a key to successful Human-Autonomy Teaming, in civil and military contexts. Index Terms: C-UAS, Scenario-based method, Emerging and Disruptive Technologies, Probability hacking, Trustworthiness.

</details>


### [363] [Online Domain-aware LLM Decoding for Continual Domain Evolution](https://arxiv.org/abs/2602.08088)
*Mohammad Abu-Shaira,Weishi Shi*

Main category: cs.LG

TL;DR: 传统LLM微调难以适应领域变化，提出ODD框架，实验表明其性能优于基线模型，适用于动态LLM应用。


<details>
  <summary>Details</summary>
Motivation: 领域知识不断演变，传统离线微调难以适应，且概念漂移会降低模型预测准确性，需要高效实时适应方法。

Method: 引入Online Domain-aware Decoding框架（ODD），在基础LLM和前缀树先验之间进行概率级融合，并使用分歧和连续性信号进行自适应置信度调制。

Result: 在不同漂移场景下，ODD在所有句法和语义NLG指标上均优于LLM-Greedy和LLM-Temp Scaled，ROUGE - L绝对增益0.065，余弦相似度相对提升13.6%。

Conclusion: ODD对不断演变的词汇和上下文模式具有鲁棒性，适用于动态LLM应用。

Abstract: LLMs are typically fine-tuned offline on domain-specific data, assuming a static domain. In practice, domain knowledge evolves continuously through new regulations, products, services, and interaction patterns. Retraining or fine-tuning LLMs for every new instance is computationally infeasible. Additionally, real-world environments also exhibit temporal dynamics with shifting data distributions. Disregarding this phenomenon, commonly referred to as concept drift, can significantly diminish a model's predictive accuracy. This mismatch between evolving domains and static adaptation pipelines highlights the need for efficient, real-time adaptation without costly retraining. In response, we introduce Online Domain-aware Decoding framework (ODD). ODD performs probability-level fusion between a base LLM and a prefix-tree prior, guided by adaptive confidence modulation using disagreement and continuity signals. Empirical evaluation under diverse drift scenarios demonstrates that ODD consistently surpasses LLM-Greedy and LLM-Temp Scaled across all syntactic and semantic NLG metrics. It yields an absolute ROUGE-L gain of 0.065 and a 13.6% relative improvement in Cosine Similarity over the best baseline. These results demonstrate ODD 's robustness to evolving lexical and contextual patterns, making it suitable for dynamic LLM applications.

</details>


### [364] [Online Bayesian Imbalanced Learning with Bregman-Calibrated Deep Networks](https://arxiv.org/abs/2602.08128)
*Zahir Alsulaimawi*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Class imbalance remains a fundamental challenge in machine learning, where standard classifiers exhibit severe performance degradation in minority classes. Although existing approaches address imbalance through resampling or cost-sensitive learning during training, they require retraining or access to labeled target data when class distributions shift at deployment time, a common occurrence in real-world applications such as fraud detection, medical diagnosis, and anomaly detection. We present \textit{Online Bayesian Imbalanced Learning} (OBIL), a principled framework that decouples likelihood-ratio estimation from class-prior assumptions, enabling real-time adaptation to distribution shifts without model retraining. Our approach builds on the established connection between Bregman divergences and proper scoring rules to show that deep networks trained with such losses produce posterior probability estimates from which prior-invariant likelihood ratios can be extracted. We prove that these likelihood-ratio estimates remain valid under arbitrary changes in class priors and cost structures, requiring only a threshold adjustment for optimal Bayes decisions. We derive finite-sample regret bounds demonstrating that OBIL achieves $O(\sqrt{T \log T})$ regret against an oracle with perfect prior knowledge. Extensive experiments on benchmark datasets and medical diagnosis benchmarks under simulated deployment shifts demonstrate that OBIL maintains robust performance under severe distribution shifts, outperforming state-of-the-art methods in F1 Score when test distributions deviate significantly from the training conditions.

</details>


### [365] [Reliable and Responsible Foundation Models: A Comprehensive Survey](https://arxiv.org/abs/2602.08145)
*Xinyu Yang,Junlin Han,Rishi Bommasani,Jinqi Luo,Wenjie Qu,Wangchunshu Zhou,Adel Bibi,Xiyao Wang,Jaehong Yoon,Elias Stengel-Eskin,Shengbang Tong,Lingfeng Shen,Rafael Rafailov,Runjia Li,Zhaoyang Wang,Yiyang Zhou,Chenhang Cui,Yu Wang,Wenhao Zheng,Huichi Zhou,Jindong Gu,Zhaorun Chen,Peng Xia,Tony Lee,Thomas Zollo,Vikash Sehwag,Jixuan Leng,Jiuhai Chen,Yuxin Wen,Huan Zhang,Zhun Deng,Linjun Zhang,Pavel Izmailov,Pang Wei Koh,Yulia Tsvetkov,Andrew Wilson,Jiaheng Zhang,James Zou,Cihang Xie,Hao Wang,Philip Torr,Julian McAuley,David Alvarez-Melis,Florian Tramèr,Kaidi Xu,Suman Jana,Chris Callison-Burch,Rene Vidal,Filippos Kokkinos,Mohit Bansal,Beidi Chen,Huaxiu Yao*

Main category: cs.LG

TL;DR: 该综述探讨基础模型可靠和负责任的发展，涵盖关键问题、模型局限及方法，给出研究现状和未来方向，促进模型的伦理和可靠性发展。


<details>
  <summary>Details</summary>
Motivation: 随着基础模型在现实世界的广泛部署，确保其可靠性和责任性对学术界、行业和政府至关重要。

Method: 探讨关键问题、模型局限性，回顾各领域现状，概述未来研究方向，讨论各领域交叉点。

Result: 对基础模型可靠和负责任发展的关键问题、模型局限等方面进行了研究和总结。

Conclusion: 希望促进基础模型不仅强大，而且符合伦理、值得信赖、可靠且具有社会责任感。

Abstract: Foundation models, including Large Language Models (LLMs), Multimodal Large Language Models (MLLMs), Image Generative Models (i.e, Text-to-Image Models and Image-Editing Models), and Video Generative Models, have become essential tools with broad applications across various domains such as law, medicine, education, finance, science, and beyond. As these models see increasing real-world deployment, ensuring their reliability and responsibility has become critical for academia, industry, and government. This survey addresses the reliable and responsible development of foundation models. We explore critical issues, including bias and fairness, security and privacy, uncertainty, explainability, and distribution shift. Our research also covers model limitations, such as hallucinations, as well as methods like alignment and Artificial Intelligence-Generated Content (AIGC) detection. For each area, we review the current state of the field and outline concrete future research directions. Additionally, we discuss the intersections between these areas, highlighting their connections and shared challenges. We hope our survey fosters the development of foundation models that are not only powerful but also ethical, trustworthy, reliable, and socially responsible.

</details>


### [366] [The Confidence Manifold: Geometric Structure of Correctness Representations in Language Models](https://arxiv.org/abs/2602.08159)
*Seonglae Cho,Zekun Wu,Kleyton Da Costa,Adriano Koshiyama*

Main category: cs.LG

TL;DR: 研究9种模型中正确性表征的几何结构，发现低维子空间质心距离可实现少样本检测，内部探测效果优于基于输出的方法，正确性信号存在于内部而非输出。


<details>
  <summary>Details</summary>
Motivation: 探究语言模型是否知道自身断言的对错，刻画不同架构模型中正确性表征的几何结构。

Method: 分析9种来自5种架构家族模型的正确性表征几何结构，通过激活引导进行因果验证，比较内部探测和基于输出方法的性能。

Result: 判别信号占据3 - 8维，性能随维度增加而下降；低维子空间质心距离与训练探针性能匹配，可实现少样本检测；内部探测AUC为0.80 - 0.97，基于输出方法AUC为0.44 - 0.64。

Conclusion: 正确性信号存在于模型内部但未在输出中体现，类分离是均值漂移，检测是几何而非学习过程。

Abstract: When a language model asserts that "the capital of Australia is Sydney," does it know this is wrong? We characterize the geometry of correctness representations across 9 models from 5 architecture families. The structure is simple: the discriminative signal occupies 3-8 dimensions, performance degrades with additional dimensions, and no nonlinear classifier improves over linear separation. Centroid distance in the low-dimensional subspace matches trained probe performance (0.90 AUC), enabling few-shot detection: on GPT-2, 25 labeled examples achieve 89% of full-data accuracy. We validate causally through activation steering: the learned direction produces 10.9 percentage point changes in error rates while random directions show no effect. Internal probes achieve 0.80-0.97 AUC; output-based methods (P(True), semantic entropy) achieve only 0.44-0.64 AUC. The correctness signal exists internally but is not expressed in outputs. That centroid distance matches probe performance indicates class separation is a mean shift, making detection geometric rather than learned.

</details>


### [367] [Spherical Steering: Geometry-Aware Activation Rotation for Language Models](https://arxiv.org/abs/2602.08169)
*Zejia You,Chunyuan Deng,Hanjie Chen*

Main category: cs.LG

TL;DR: 本文提出无训练的Spherical Steering方法，通过激活旋转控制语言模型，实验显示其优于基于加法的基线方法，且能保持模型开放性生成能力。


<details>
  <summary>Details</summary>
Motivation: 标准推理时控制语言模型的激活加法方法会改变隐藏表示的大小，引发表示崩溃和开放性生成能力下降问题。

Method: 提出Spherical Steering方法，通过激活旋转引导激活向目标概念靠近，同时保留信号完整性，还加入信心门根据输入不确定性动态调节引导强度。

Result: 在多项选择题基准测试中，Spherical Steering显著优于基于加法的基线方法，在TruthfulQA、COPA和Storycloze上提高了10%，并保持了模型开放性生成能力。

Conclusion: 几何一致性有价值，保范旋转是精确推理时控制的强大有效方法。

Abstract: Inference-time steering has emerged as a promising paradigm for controlling language models (LMs) without the cost of retraining. However, standard approaches typically rely on activation addition, a geometric operation that inevitably alters the magnitude of hidden representations. This raises concerns about representation collapse and degradation of open-ended generation capabilities. In this work, we explore Spherical Steering, a training-free primitive that resolves this trade-off through activation rotation. Rather than shifting activations with a fixed vector, our method rotates them along a geodesic toward a target direction, guiding the activation toward the target concept while preserving the integrity of the signal. To further enhance adaptivity, we incorporate a confidence gate that dynamically modulates steering strength based on input uncertainty. Extensive experiments across multiple-choice benchmarks demonstrate that Spherical Steering significantly outperforms addition-based baselines (notably by +10% on TruthfulQA, COPA, and Storycloze), while simultaneously maintaining the model's general open-ended generation quality. This work highlights the value of geometric consistency, suggesting that norm-preserving rotation is a robust and effective primitive for precise inference-time control.

</details>


### [368] [A Causal Machine Learning Framework for Treatment Personalization in Clinical Trials: Application to Ulcerative Colitis](https://arxiv.org/abs/2602.08171)
*Cristian Minoccheri,Sophia Tesic,Kayvan Najarian,Ryan Stidham*

Main category: cs.LG

TL;DR: 提出模块化因果机器学习框架评估治疗异质性相关问题，应用于溃疡性结肠炎试验，发现内镜特征虽与治疗效果异质性有关但不改善决策，临床变量更关键，强调临床试验因果机器学习应结合政策评估和异质性测试。


<details>
  <summary>Details</summary>
Motivation: 随机对照试验估计平均治疗效果，治疗反应异质性促使个性化治疗，但需明确统计上可检测的异质性是否能转化为更好的治疗决策。

Method: 提出模块化因果机器学习框架，用排列重要性识别预测异质性的特征，最佳线性预测器（BLP）测试评估统计显著性，双重稳健政策评估衡量依据异质性行动是否改善患者结果，应用于UNIFI试验数据，采用交叉拟合X - 学习器模型。

Result: BLP测试发现内镜特征与乌司奴单抗治疗效果异质性强相关，但双重稳健政策评估显示纳入内镜特征未改善预期缓解，外折多臂评估表现更差，临床变量更能捕捉决策相关变化。

Conclusion: 临床试验的因果机器学习应用应在异质性测试的同时进行政策层面评估。

Abstract: Randomized controlled trials estimate average treatment effects, but treatment response heterogeneity motivates personalized approaches. A critical question is whether statistically detectable heterogeneity translates into improved treatment decisions -- these are distinct questions that can yield contradictory answers. We present a modular causal machine learning framework that evaluates each question separately: permutation importance identifies which features predict heterogeneity, best linear predictor (BLP) testing assesses statistical significance, and doubly robust policy evaluation measures whether acting on the heterogeneity improves patient outcomes. We apply this framework to patient-level data from the UNIFI maintenance trial of ustekinumab in ulcerative colitis, comparing placebo, standard-dose ustekinumab every 12 weeks, and dose-intensified ustekinumab every 8 weeks, using cross-fitted X-learner models with baseline demographics, medication history, week-8 clinical scores, laboratory biomarkers, and video-derived endoscopic features. BLP testing identified strong associations between endoscopic features and treatment effect heterogeneity for ustekinumab versus placebo, yet doubly robust policy evaluation showed no improvement in expected remission from incorporating endoscopic features, and out-of-fold multi-arm evaluation showed worse performance. Diagnostic comparison of prognostic contribution against policy value revealed that endoscopic scores behaved as disease severity markers -- improving outcome prediction in untreated patients but adding noise to treatment selection -- while clinical variables (fecal calprotectin, age, CRP) captured the decision-relevant variation. These results demonstrate that causal machine learning applications to clinical trials should include policy-level evaluation alongside heterogeneity testing.

</details>


### [369] [Dreaming in Code for Curriculum Learning in Open-Ended Worlds](https://arxiv.org/abs/2602.08194)
*Konstantinos Mitsides,Maxence Faldor,Antoine Cully*

Main category: cs.LG

TL;DR: 提出Dreaming in Code (DiCode)框架，在Craftax基准测试中提升智能体表现，证明代码级环境设计对开放学习课程控制有效。


<details>
  <summary>Details</summary>
Motivation: 现有方法专注孤立行为发现，在复杂开放世界中，智能体难找到持续可学习的经验序列。

Method: 提出DiCode框架，用基础模型合成可执行环境代码来促进学习。

Result: 在Craftax基准测试中，DiCode使智能体获得长视野技能，平均回报比最强基线提高16%，在前期方法失败的后期战斗任务中有非零成功率。

Conclusion: 代码级环境设计为开放世界课程控制提供实用机制，可构建中间环境弥合能力差距。

Abstract: Open-ended learning frames intelligence as emerging from continual interaction with an ever-expanding space of environments. While recent advances have utilized foundation models to programmatically generate diverse environments, these approaches often focus on discovering isolated behaviors rather than orchestrating sustained progression. In complex open-ended worlds, the large combinatorial space of possible challenges makes it difficult for agents to discover sequences of experiences that remain consistently learnable. To address this, we propose Dreaming in Code (DiCode), a framework in which foundation models synthesize executable environment code to scaffold learning toward increasing competence. In DiCode, "dreaming" takes the form of materializing code-level variations of the world. We instantiate DiCode in Craftax, a challenging open-ended benchmark characterized by rich mechanics and long-horizon progression. Empirically, DiCode enables agents to acquire long-horizon skills, achieving a $16\%$ improvement in mean return over the strongest baseline and non-zero success on late-game combat tasks where prior methods fail. Our results suggest that code-level environment design provides a practical mechanism for curriculum control, enabling the construction of intermediate environments that bridge competence gaps in open-ended worlds. Project page and source code are available at https://konstantinosmitsides.github.io/dreaming-in-code and https://github.com/konstantinosmitsides/dreaming-in-code.

</details>


### [370] [DrugR: Optimizing Molecular Drugs through LLM-based Explicit Reasoning](https://arxiv.org/abs/2602.08213)
*Haoran Liu,Zheni Zeng,Yukun Yan,Yuxuan Chen,Yunduo Xiao*

Main category: cs.LG

TL;DR: 提出基于大语言模型的DrugR方法用于分子生成与优化，实验证明其效果良好且开源代码和模型。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在处理分子结构与药理特性复杂关系及缺乏标注数据方面存在挑战，需新方法。

Method: 提出DrugR方法，集成特定领域持续预训练、通过逆向数据工程进行有监督微调、自平衡多粒度强化学习。

Result: DrugR能有效提升关键ADMET属性，在不影响结构相似性和靶标结合亲和力的情况下实现多属性综合提升。

Conclusion: DrugR的显式推理过程为优化步骤提供可解释依据，推动自动化、知识驱动的科学发现。

Abstract: Molecule generation and optimization is a fundamental task in chemical domain. The rapid development of intelligent tools, especially large language models (LLMs) with powerful knowledge reserves and interactive capabilities, has provided new paradigms for it. Nevertheless, the intrinsic challenge for LLMs lies in the complex implicit relationship between molecular structure and pharmacological properties and the lack of corresponding labeled data. To bridge this gap, we propose DrugR, an LLM-based method that introduces explicit, step-by-step pharmacological reasoning into the optimization process. Our approach integrates domain-specific continual pretraining, supervised fine-tuning via reverse data engineering, and self-balanced multi-granular reinforcement learning. This framework enables DrugR to effectively improve key ADMET properties while preserving the original molecule's core efficacy. Experimental results demonstrate that DrugR achieves comprehensive enhancement across multiple properties without compromising structural similarity or target binding affinity. Importantly, its explicit reasoning process provides clear, interpretable rationales for each optimization step, yielding actionable design insights and advancing toward automated, knowledge-driven scientific discovery. Our code and model checkpoints are open-sourced to foster future research.

</details>


### [371] [Distribution-Free Robust Functional Predict-Then-Optimize](https://arxiv.org/abs/2602.08215)
*Yash Patel,Ambuj Tewari*

Main category: cs.LG

TL;DR: 提出将保形预测应用于神经算子以进行无分布不确定性量化，用于下游决策任务，具高效性和优越表现。


<details>
  <summary>Details</summary>
Motivation: 现有神经算子求解偏微分方程的方法无法提供校准的不确定性概念，且当前解决方法存在不足。

Method: 提出将保形预测应用于神经算子以进行无分布的函数空间不确定性量化，用无限维的丹斯金定理和变分法解决决策任务。

Result: 实证表明所提方法在多个工程任务中性能优于高斯过程等更受限的建模范式。

Conclusion: 所提方法可有效进行不确定性量化并支持鲁棒决策制定，在工程任务中有良好表现。

Abstract: The solution of PDEs in decision-making tasks is increasingly being undertaken with the help of neural operator surrogate models due to the need for repeated evaluation. Such methods, while significantly more computationally favorable compared to their numerical counterparts, fail to provide any calibrated notions of uncertainty in their predictions. Current methods approach this deficiency typically with ensembling or Bayesian posterior estimation. However, these approaches either require distributional assumptions that fail to hold in practice or lack practical scalability, limiting their applications in practice. We, therefore, propose a novel application of conformal prediction to produce distribution-free uncertainty quantification over the function spaces mapped by neural operators. We then demonstrate how such prediction regions enable a formal regret characterization if leveraged in downstream robust decision-making tasks. We further demonstrate how such posited robust decision-making tasks can be efficiently solved using an infinite-dimensional generalization of Danskin's Theorem and calculus of variations and empirically demonstrate the superior performance of our proposed method over more restrictive modeling paradigms, such as Gaussian Processes, across several engineering tasks.

</details>


### [372] [Sparsity-Aware Evolution for Model Merging](https://arxiv.org/abs/2602.08218)
*Huan Zhang,Yanjian Zhang,Guillaume Wisniewski,Nadi Tomeh,Bang Liu*

Main category: cs.LG

TL;DR: 提出用于模型合并的稀疏感知进化 (SAE) 框架，在多个基准测试上提升合并可靠性。


<details>
  <summary>Details</summary>
Motivation: 改进模型合并的可靠性，通过引入稀疏性约束推动进化过程偏好更稀疏的模型。

Method: 采用迭代剪枝 - 合并循环作为新的变异算子，将稀疏性约束纳入得分函数。

Result: 在多种大规模LLM基准测试上，提高了模型合并的可靠性。

Conclusion: 该方法简单且与大多数现有方法正交，易于整合。

Abstract: We propose a sparsity-aware evolutionary (SAE) framework for model merging that involves iterative pruning-merging cycles to act as a novel mutation operator. We incorporate the sparsity constraints into the score function, which steers the evolutionary process to favor more sparse models, in addition to other conventional performance scores. Interestingly, the by-product of \textit{competition} for sparsity introduces an extra local \textit{attraction} and interplay into the evolutionary process: if one competitor has more zero elements, the other competitor's non-zero elements will occupy those positions, even though the less sparse competitor loses to the more sparse competitor in other positions. The proposed pipeline is evaluated on a variety of large-scale LLM benchmarks. Experiments demonstrate that our approach can improve model merging reliability across multiple benchmarks, and is easy to incorporate due to its simplicity and being orthogonal to most existing approaches.

</details>


### [373] [SkillRL: Evolving Agents via Recursive Skill-Augmented Reinforcement Learning](https://arxiv.org/abs/2602.08234)
*Peng Xia,Jianwen Chen,Hanyang Wang,Jiaqi Liu,Kaide Zeng,Yu Wang,Siwei Han,Yiyang Zhou,Xujiang Zhao,Haifeng Chen,Zeyu Zheng,Cihang Xie,Huaxiu Yao*

Main category: cs.LG

TL;DR: 提出SkillRL框架，通过自动技能发现和递归进化连接原始经验与策略改进，实验表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型代理孤立运行，基于记忆的方法存储原始轨迹冗余且多噪声，无法提取高级可复用行为模式。

Method: 引入基于经验的蒸馏机制构建分层技能库SkillBank，采用自适应检索策略，有递归进化机制使技能库与代理策略共同进化。

Result: 在ALFWorld、WebShop和七个搜索增强任务上实现了最先进的性能，比强基线高出15.3%，且随任务复杂度增加保持鲁棒性。

Conclusion: SkillRL框架有效，能减少令牌占用并增强推理效用。

Abstract: Large Language Model (LLM) agents have shown stunning results in complex tasks, yet they often operate in isolation, failing to learn from past experiences. Existing memory-based methods primarily store raw trajectories, which are often redundant and noise-heavy. This prevents agents from extracting high-level, reusable behavioral patterns that are essential for generalization. In this paper, we propose SkillRL, a framework that bridges the gap between raw experience and policy improvement through automatic skill discovery and recursive evolution. Our approach introduces an experience-based distillation mechanism to build a hierarchical skill library SkillBank, an adaptive retrieval strategy for general and task-specific heuristics, and a recursive evolution mechanism that allows the skill library to co-evolve with the agent's policy during reinforcement learning. These innovations significantly reduce the token footprint while enhancing reasoning utility. Experimental results on ALFWorld, WebShop and seven search-augmented tasks demonstrate that SkillRL achieves state-of-the-art performance, outperforming strong baselines over 15.3% and maintaining robustness as task complexity increases. Code is available at this https://github.com/aiming-lab/SkillRL.

</details>


### [374] [Linearization Explains Fine-Tuning in Large Language Models](https://arxiv.org/abs/2602.08239)
*Zahra Rahimi Afzal,Tara Esmaeilbeig,Mojtaba Soltanalian,Mesrob I. Ohannessian*

Main category: cs.LG

TL;DR: 本文从线性化角度研究参数高效微调（PEFT），揭示微调动态与正定型神经切线核（NTK）的联系，分析线性化优化，给出NTK谱扰动界并在LoRA上验证，有望改进PEFT技术。


<details>
  <summary>Details</summary>
Motivation: 当前PEFT技术训练性能和泛化机制研究不足，需深入探究。

Method: 引入欧几里得距离归纳偏置使微调接近预训练模型，使微调动态等价于用NTK学习；分析线性化优化的接近程度；给出NTK谱扰动界；在LoRA上进行实证验证。

Result: 发现线性化作为好模型时，NTK特征值谱与模型自适应性能强相关；理论在LoRA上得到实证验证。

Conclusion: 研究不仅刻画了微调过程，还有望增强PEFT技术，为大语言模型适应性调整提供更优方案。

Abstract: Parameter-Efficient Fine-Tuning (PEFT) is a popular class of techniques that strive to adapt large models in a scalable and resource-efficient manner. Yet, the mechanisms underlying their training performance and generalization remain underexplored. In this paper, we provide several insights into such fine-tuning through the lens of linearization. Fine-tuned models are often implicitly encouraged to remain close to the pretrained model. By making this explicit, using an Euclidean distance inductive bias in parameter space, we show that fine-tuning dynamics become equivalent to learning with the positive-definite neural tangent kernel (NTK). We specifically analyze how close the fully linear and the linearized fine-tuning optimizations are, based on the strength of the regularization. This allows us to be pragmatic about how good a model linearization is when fine-tuning large language models (LLMs). When linearization is a good model, our findings reveal a strong correlation between the eigenvalue spectrum of the NTK and the performance of model adaptation. Motivated by this, we give spectral perturbation bounds on the NTK induced by the choice of layers selected for fine-tuning. We empirically validate our theory on Low Rank Adaptation (LoRA) on LLMs. These insights not only characterize fine-tuning but also have the potential to enhance PEFT techniques, paving the way to better informed and more nimble adaptation in LLMs.

</details>


### [375] [Learning in Context, Guided by Choice: A Reward-Free Paradigm for Reinforcement Learning with Transformers](https://arxiv.org/abs/2602.08244)
*Juncheng Dong,Bowen He,Moyang Guo,Ethan X. Fang,Zhuoran Yang,Vahid Tarokh*

Main category: cs.LG

TL;DR: 提出基于偏好的上下文强化学习范式ICPRL，仅用偏好反馈学习，在多任务实验中表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有上下文强化学习方法依赖明确奖励信号，在奖励模糊、难指定或获取成本高时受限。

Method: 提出ICPRL范式，研究I - PRL和T - PRL两个变体，引入偏好原生框架优化策略。

Result: 实验表明ICPRL能有效泛化到未见任务，性能与全奖励监督的ICRL方法相当。

Conclusion: ICPRL可仅依靠偏好反馈进行上下文强化学习，实现对未见任务的强泛化。

Abstract: In-context reinforcement learning (ICRL) leverages the in-context learning capabilities of transformer models (TMs) to efficiently generalize to unseen sequential decision-making tasks without parameter updates. However, existing ICRL methods rely on explicit reward signals during pretraining, which limits their applicability when rewards are ambiguous, hard to specify, or costly to obtain. To overcome this limitation, we propose a new learning paradigm, In-Context Preference-based Reinforcement Learning (ICPRL), in which both pretraining and deployment rely solely on preference feedback, eliminating the need for reward supervision. We study two variants that differ in the granularity of feedback: Immediate Preference-based RL (I-PRL) with per-step preferences, and Trajectory Preference-based RL (T-PRL) with trajectory-level comparisons. We first show that supervised pretraining, a standard approach in ICRL, remains effective under preference-only context datasets, demonstrating the feasibility of in-context reinforcement learning using only preference signals. To further improve data efficiency, we introduce alternative preference-native frameworks for I-PRL and T-PRL that directly optimize TM policies from preference data without requiring reward signals nor optimal action labels.Experiments on dueling bandits, navigation, and continuous control tasks demonstrate that ICPRL enables strong in-context generalization to unseen tasks, achieving performance comparable to ICRL methods trained with full reward supervision.

</details>


### [376] [Inverting Data Transformations via Diffusion Sampling](https://arxiv.org/abs/2602.08267)
*Jinwoo Kim,Sékou-Oumar Kaba,Jiyun Park,Seunghoon Hong,Siamak Ravanbakhsh*

Main category: cs.LG

TL;DR: 研究一般李群上的变换反演问题，提出TIED方法，在图像单应性和PDE对称性实验中表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 未知变换在机器学习和科学建模中广泛存在，会显著扭曲观测，需恢复逆变换使数据回到原分布。

Method: 从概率角度，将变换的后验建模为玻尔兹曼分布，引入李群上的扩散过程，利用新的目标得分恒等式进行高效采样。

Result: 实验表明TIED能在测试时将变换后的输入恢复到训练分布，性能优于强规范化和采样基线。

Conclusion: TIED方法在解决变换反演问题上有效，可提高预训练神经网络对输入变换的鲁棒性。

Abstract: We study the problem of transformation inversion on general Lie groups: a datum is transformed by an unknown group element, and the goal is to recover an inverse transformation that maps it back to the original data distribution. Such unknown transformations arise widely in machine learning and scientific modeling, where they can significantly distort observations. We take a probabilistic view and model the posterior over transformations as a Boltzmann distribution defined by an energy function on data space. To sample from this posterior, we introduce a diffusion process on Lie groups that keeps all updates on-manifold and only requires computations in the associated Lie algebra. Our method, Transformation-Inverting Energy Diffusion (TIED), relies on a new trivialized target-score identity that enables efficient score-based sampling of the transformation posterior. As a key application, we focus on test-time equivariance, where the objective is to improve the robustness of pretrained neural networks to input transformations. Experiments on image homographies and PDE symmetries demonstrate that TIED can restore transformed inputs to the training distribution at test time, showing improved performance over strong canonicalization and sampling baselines. Code is available at https://github.com/jw9730/tied.

</details>


### [377] [When Do Multi-Agent Systems Outperform? Analysing the Learning Efficiency of Agentic Systems](https://arxiv.org/abs/2602.08272)
*Junwei Su,Chuan Wu*

Main category: cs.LG

TL;DR: 本文分析多智能体强化学习（MARL）和单智能体强化学习（SARL）在大语言模型（LLM）中的样本效率，得出任务分解和对齐对学习效率的影响。


<details>
  <summary>Details</summary>
Motivation: 当前关于MARL何时及为何优于SARL的理论见解有限，在选择合适的强化学习框架时存在不确定性。

Method: 利用PAC框架，为LLM正式定义SARL和MARL设置，推导样本复杂度边界，系统刻画任务分解和对齐对学习效率的影响。

Result: 当任务自然分解为独立子任务时，MARL提高样本复杂度；依赖子任务会削弱MARL优势；引入并分析任务对齐概念，量化强制独立任务分解的权衡。

Conclusion: 这些理论见解澄清了实证不一致性，为在复杂LLM场景中有效部署MARL策略提供了实用标准。

Abstract: Reinforcement Learning (RL) has emerged as a crucial method for training or fine-tuning large language models (LLMs), enabling adaptive, task-specific optimizations through interactive feedback. Multi-Agent Reinforcement Learning (MARL), in particular, offers a promising avenue by decomposing complex tasks into specialized subtasks learned by distinct interacting agents, potentially enhancing the ability and efficiency of LLM systems. However, theoretical insights regarding when and why MARL outperforms Single-Agent RL (SARL) remain limited, creating uncertainty in selecting the appropriate RL framework. In this paper, we address this critical gap by rigorously analyzing the comparative sample efficiency of MARL and SARL within the context of LLM. Leveraging the Probably Approximately Correct (PAC) framework, we formally define SARL and MARL setups for LLMs, derive explicit sample complexity bounds, and systematically characterize how task decomposition and alignment influence learning efficiency. Our results demonstrate that MARL improves sample complexity when tasks naturally decompose into independent subtasks, whereas dependent subtasks diminish MARL's comparative advantage. Additionally, we introduce and analyze the concept of task alignment, quantifying the trade-offs when enforcing independent task decomposition despite potential misalignments. These theoretical insights clarify empirical inconsistencies and provide practical criteria for deploying MARL strategies effectively in complex LLM scenarios.

</details>


### [378] [Trust-Based Incentive Mechanisms in Semi-Decentralized Federated Learning Systems](https://arxiv.org/abs/2602.08290)
*Ajay Kumar Shrestha*

Main category: cs.LG

TL;DR: 本文提出基于信任的激励机制，结合区块链与智能合约，构建更稳健、公平、透明的联邦学习生态系统，降低不可信参与者风险。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中存在恶意或故障节点影响模型性能，需确保系统完整性和可靠性。

Method: 提出基于信任的激励机制，动态评估信任分数，依据分数奖励或惩罚参与者；探索结合区块链技术和智能合约自动化评估和激励分配。

Result: 未提及具体实验结果。

Conclusion: 所提理论框架可创建更稳健、公平、透明的联邦学习生态系统，降低不可信参与者风险。

Abstract: In federated learning (FL), decentralized model training allows multi-ple participants to collaboratively improve a shared machine learning model without exchanging raw data. However, ensuring the integrity and reliability of the system is challenging due to the presence of potentially malicious or faulty nodes that can degrade the model's performance. This paper proposes a novel trust-based incentive mechanism designed to evaluate and reward the quality of contributions in FL systems. By dynamically assessing trust scores based on fac-tors such as data quality, model accuracy, consistency, and contribution fre-quency, the system encourages honest participation and penalizes unreliable or malicious behavior. These trust scores form the basis of an incentive mechanism that rewards high-trust nodes with greater participation opportunities and penal-ties for low-trust participants. We further explore the integration of blockchain technology and smart contracts to automate the trust evaluation and incentive distribution processes, ensuring transparency and decentralization. Our proposed theoretical framework aims to create a more robust, fair, and transparent FL eco-system, reducing the risks posed by untrustworthy participants.

</details>


### [379] [Grokking in Linear Models for Logistic Regression](https://arxiv.org/abs/2602.08302)
*Nataraj Das,Atreya Vedantam,Chandrashekar Lakshminarayanan*

Main category: cs.LG

TL;DR: 研究在简单线性模型中二元分类的Grokking现象，证明其无需深度或表示学习，可在线性模型通过偏置项动态产生。


<details>
  <summary>Details</summary>
Motivation: 探究Grokking现象，常认为与深度神经网络有关，研究其在简单线性模型中的情况。

Method: 研究三种测试机制，理论分析梯度下降的隐式偏差，将Grokking出现与数据不对称性联系，实验验证理论。

Result: 观察到不同测试机制下的Grokking现象，得出三阶段学习过程，给出grokking时间的特征。

Conclusion: Grokking不依赖深度或表示学习，在线性模型中也可通过偏置项动态出现。

Abstract: Grokking, the phenomenon of delayed generalization, is often attributed to the depth and compositional structure of deep neural networks. We study grokking in one of the simplest possible settings: the learning of a linear model with logistic loss for binary classification on data that are linearly (and max margin) separable about the origin. We investigate three testing regimes: (1) test data drawn from the same distribution as the training data, in which case grokking is not observed; (2) test data concentrated around the margin, in which case grokking is observed; and (3) adversarial test data generated via projected gradient descent (PGD) attacks, in which case grokking is also observed. We theoretically show that the implicit bias of gradient descent induces a three-phase learning process-population-dominated, support-vector-dominated unlearning, and support-vector-dominated generalization-during which delayed generalization can arise. Our analysis further relates the emergence of grokking to asymmetries in the data, both in the number of examples per class and in the distribution of support vectors across classes, and yields a characterization of the grokking time. We experimentally validate our theory by planting different distributions of population points and support vectors, and by analyzing accuracy curves and hyperplane dynamics. Overall, our results demonstrate that grokking does not require depth or representation learning, and can emerge even in linear models through the dynamics of the bias term.

</details>


### [380] [TextResNet: Decoupling and Routing Optimization Signals in Compound AI Systems via Deep Residual Tuning](https://arxiv.org/abs/2602.08306)
*Suizhi Huang,Mei Li,Han Yu,Xiaoxiao Li*

Main category: cs.LG

TL;DR: 提出TextResNet框架解决TextGrad在深度链中因语义纠缠问题导致效果不佳的问题，性能优于TextGrad且有稳定性，代码开源。


<details>
  <summary>Details</summary>
Motivation: Textual Gradient - style optimizers（TextGrad）在深度链中因语义纠缠问题效果不佳，需解决归因模糊问题。

Method: 提出TextResNet框架，包括前向传播中执行加法语义增量、反向传播中引入语义梯度分解、实施因果路由和密度感知优化调度。

Result: TextResNet性能优于TextGrad，在基线失败的复合AI系统代理任务中表现出卓越稳定性。

Conclusion: TextResNet能解决TextGrad在深度链中的问题，是优化复合AI系统的有效方法。

Abstract: Textual Gradient-style optimizers (TextGrad) enable gradient-like feedback propagation through compound AI systems. However, they do not work well for deep chains. The root cause of this limitation stems from the Semantic Entanglement problem in these extended workflows. In standard textual backpropagation, feedback signals mix local critiques with upstream contexts, leading to Attribution Ambiguity. To address this challenge, we propose TextResNet, a framework that reformulates the optimization process to achieve precise signal routing via four key innovations. Firstly, in the forward pass, it enforces Additive Semantic Deltas to preserve an Identity Highway for gradient flow. Secondly, in the backward pass, it introduces Semantic Gradient Decomposition via a Semantic Projector to disentangle feedback into causally independent subspaces. Thirdly, it implements Causal Routing, which routes projected signals to their specific components. Finally, it performs Density-Aware Optimization Scheduling to leverage the disentangled signals to dynamically allocate resources to key system bottlenecks. Our results show that TextResNet not only achieves superior performance compared to TextGrad, but also exhibits remarkable stability for agentic tasks in compound AI systems where baselines collapse. Code is available at https://github.com/JeanDiable/TextResNet.

</details>


### [381] [Towards Efficient Large Language Reasoning Models via Extreme-Ratio Chain-of-Thought Compression](https://arxiv.org/abs/2602.08324)
*Yuntian Tang,Bohan Jia,Wenxuan Huang,Lianyue Zhang,Jiao Xie,Wenxi Li,Wei Li,Jie Hu,Xinghao Chen,Rongrong Ji,Shaohui Lin*

Main category: cs.LG

TL;DR: 提出Extra - CoT框架用于高保真、快速推理，减少计算开销且提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有CoT压缩方法在高压缩比时逻辑保真度损失大、性能下降，需实现高保真快速推理。

Method: 先在数学CoT数据上训练语义保留压缩器，通过混合比例监督微调训练LLM，再提出CHRPO方法。

Result: 在三个数学推理基准测试中表现优越，如在MATH - 500上用Qwen3 - 1.7B实现超73%的token减少且准确率提升0.6%。

Conclusion: Extra - CoT框架优于现有方法。

Abstract: Chain-of-Thought (CoT) reasoning successfully enhances the reasoning capabilities of Large Language Models (LLMs), yet it incurs substantial computational overhead for inference. Existing CoT compression methods often suffer from a critical loss of logical fidelity at high compression ratios, resulting in significant performance degradation. To achieve high-fidelity, fast reasoning, we propose a novel EXTreme-RAtio Chain-of-Thought Compression framework, termed Extra-CoT, which aggressively reduces the token budget while preserving answer accuracy. To generate reliable, high-fidelity supervision, we first train a dedicated semantically-preserved compressor on mathematical CoT data with fine-grained annotations. An LLM is then fine-tuned on these compressed pairs via a mixed-ratio supervised fine-tuning (SFT), teaching it to follow a spectrum of compression budgets and providing a stable initialization for reinforcement learning (RL). We further propose Constrained and Hierarchical Ratio Policy Optimization (CHRPO) to explicitly incentivize question-solving ability under lower budgets by a hierarchical reward. Experiments on three mathematical reasoning benchmarks show the superiority of Extra-CoT. For example, on MATH-500 using Qwen3-1.7B, Extra-CoT achieves over 73\% token reduction with an accuracy improvement of 0.6\%, significantly outperforming state-of-the-art (SOTA) methods.

</details>


### [382] [Near-Oracle KV Selection via Pre-hoc Sparsity for Long-Context Inference](https://arxiv.org/abs/2602.08329)
*Yifei Gao,Lei Wang,Rong-Cheng Tu,Qixin Zhang,Jun Cheng,Dacheng Tao*

Main category: cs.LG

TL;DR: 提出Pre - hoc Sparsity (PrHS) 解决大语言模型推理 KV 缓存选择问题，实验验证其在降低开销、提升速度等方面效果好。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型推理 KV 选择的稀疏方法依赖后验启发式，存在后验偏差，损害长距离推理能力。

Method: 提出 PrHS 在注意力评分前选择 KV 条目，提供显式的精度控制；通过边际到互信息分析推导互信息损失上限；实例化三个正交的预选择器。

Result: 在 LLaMA 和 Mistral 系列模型上实验，PrHS 降低检索开销超 90%，检索稀疏度比 HShare 高 3 倍，在 LongBench 上平均性能下降小于 1%，降低注意力计算量约 15%，在 NVIDIA A100 - 80GB GPUs 上使注意力算子延迟加速 9.9 倍、吞吐量提高 2.8 倍。

Conclusion: PrHS 能有效解决现有大语言模型推理中 KV 缓存选择的问题，在降低开销和提升性能方面表现出色。

Abstract: A core bottleneck in large language model (LLM) inference is the cost of attending over the ever-growing key-value (KV) cache. Although near-oracle top-k KV selection can preserve the quality of dense attention while sharply reducing computation and bandwidth, existing sparse methods generally rely on posterior heuristics, i.e., selectors conditioned on observed attention or proxy scores. Such conditioning introduces posterior bias: it tends to distort true token importance and miss salient tokens, thereby impairing long-range reasoning. To tackle this problem, we propose Pre-hoc Sparsity (PrHS), which selects KV entries before attention scoring and provides explicit accuracy control. Let the attention mass of discarded entries be delta (the dropped mass). Through a marginal-to-mutual-information analysis, we derive an upper bound on the mutual-information loss that depends only on the dropped mass. This relation explains failure modes of posterior heuristics and enables verifiable guarantees by controlling the dropped mass in advance. Within PrHS, we instantiate three orthogonal pre-hoc selectors along the axes of time, depth, and layer. Extensive experiments on LLaMA and Mistral families validate PrHS. Across GSM8K and CoQA, PrHS reduces retrieval overhead by over 90%, achieving 3x higher retrieval sparsity than HShare at matched or better accuracy. It incurs under 1% average degradation on LongBench, lowers attention FLOPs by about 15% versus prior sparse baselines, and yields a 9.9x speedup in attention-operator latency and 2.8x higher throughput on NVIDIA A100-80GB GPUs than the dense baseline.

</details>


### [383] [Regime Change Hypothesis: Foundations for Decoupled Dynamics in Neural Network Training](https://arxiv.org/abs/2602.08333)
*Cristian Pérez-Corral,Alberto Fernández-Hernández,Jose I. Mestre,Manuel F. Dolz,Jose Duato,Enrique S. Quintana-Ortí*

Main category: cs.LG

TL;DR: 研究ReLU模型训练是否有两阶段特性：早期激活模式大幅变化，后期主要在稳定激活状态下微调权重，发现激活模式变化早于权重更新。


<details>
  <summary>Details</summary>
Motivation: DNN内部训练动态难以表征，ReLU模型中输入诱导的激活模式决定网络局部仿射行为区域，因此研究其训练是否有两阶段特性。

Method: 先证明局部稳定性属性，再通过固定验证子集，对全连接、卷积和Transformer模型进行实证跟踪。

Result: 激活模式变化比权重更新幅度提前3倍衰减，后期训练常在相对稳定的激活状态下进行。

Conclusion: 研究结果提供了监测训练动态的工具，并为分段线性网络的解耦优化策略研究提供动力。

Abstract: Despite the empirical success of DNN, their internal training dynamics remain difficult to characterize. In ReLU-based models, the activation pattern induced by a given input determines the piecewise-linear region in which the network behaves affinely. Motivated by this geometry, we investigate whether training exhibits a two-timescale behavior: an early stage with substantial changes in activation patterns and a later stage where weight updates predominantly refine the model within largely stable activation regimes. We first prove a local stability property: outside measure-zero sets of parameters and inputs, sufficiently small parameter perturbations preserve the activation pattern of a fixed input, implying locally affine behavior within activation regions. We then empirically track per-iteration changes in weights and activation patterns across fully-connected and convolutional architectures, as well as Transformer-based models, where activation patterns are recorded in the ReLU feed-forward (MLP/FFN) submodules, using fixed validation subsets. Across the evaluated settings, activation-pattern changes decay 3 times earlier than weight-update magnitudes, showing that late-stage training often proceeds within relatively stable activation regimes. These findings provide a concrete, architecture-agnostic instrument for monitoring training dynamics and motivate further study of decoupled optimization strategies for piecewise-linear networks. For reproducibility, code and experiment configurations will be released upon acceptance.

</details>


### [384] [ManifoldKV: Training-Free KV Cache Compression via Euclidean Outlier Detection](https://arxiv.org/abs/2602.08343)
*Debajyoti Datta,Trishala Neeraj,Bibek Paudel,Vyom Sharma,Subhabrata Mukherjee*

Main category: cs.LG

TL;DR: 提出无训练的ManifoldKV评分器，通过欧氏距离对标记排序，在多个场景表现良好，且代码简单无需调参。


<details>
  <summary>Details</summary>
Motivation: 长上下文推理受KV缓存内存限制，现有基于几何的驱逐方法以余弦相似度评分存在丢弃语义显著标记的问题。

Method: 提出ManifoldKV评分器，通过欧氏距离对标记排序，还引入WindowedManifoldKV解决全局质心稀释问题。

Result: 在RULER基准测试中，ManifoldKV在4K - 16K上下文、20%压缩率下达到95.7%准确率；在多键检索和64K上下文场景表现优于基线方法。

Conclusion: ManifoldKV方法有效，代码简单且适用于多种架构无需调参。

Abstract: Long-context inference is constrained by KV-cache memory, which grows linearly with sequence length; KV-cache compression therefore hinges on reliably selecting which past tokens to retain. Most geometry-based eviction methods score keys by cosine similarity to a global centroid, but cosine is scale-invariant and can discard magnitude cues that distinguish semantically salient tokens. We propose ManifoldKV, a training-free scorer that ranks tokens by Euclidean distance to the key centroid, capturing both angular and radial deviations.
  On the RULER benchmark, ManifoldKV achieves 95.7% accuracy at 4K-16K contexts with 20% compression; matching the best geometric baseline while improving robustness in two regimes where cosine scoring fails. First, on multi-key retrieval, ManifoldKV reduces directional collisions, achieving 92.4% vs KeyDiff's 77.0% (+15.4 points) on 3-key NIAH at 50% compression. Second, to address dilution and performance collapse of global centroids at 64K context, we introduce WindowedManifoldKV, which restores accuracy to 84.3% at 25% compression, a 49-point recovery over global L2 and +3.2 points over KeyDiff. The method requires only 3 lines of code and works across 4 architectures without tuning.

</details>


### [385] [The Chicken and Egg Dilemma: Co-optimizing Data and Model Configurations for LLMs](https://arxiv.org/abs/2602.08351)
*Zhiliang Chen,Alfred Wei Lun Leong,Shao Yong Ong,Apivich Hemachandram,Gregory Kang Ruey Lau,Chuan-Sheng Foo,Zhengyuan Liu,Nancy F. Chen,Bryan Kian Hsiang Low*

Main category: cs.LG

TL;DR: 介绍JoBS方法联合优化大语言模型训练数据和模型配置，该方法优于现有基线和优化方法。


<details>
  <summary>Details</summary>
Motivation: 联合优化数据和模型配置存两难问题，现有方法未考虑两者相互作用。

Method: 使用受缩放定律启发的性能预测器辅助贝叶斯优化，分配部分预算学习预测器，其余用于基于预测器的贝叶斯优化，并研究平均遗憾和预算分配。

Result: JoBS在相同优化预算下，在多种大语言模型任务中优于现有的多保真贝叶斯优化基线、数据和模型优化方法。

Conclusion: JoBS可有效联合优化大语言模型训练数据和模型配置。

Abstract: Co-optimizing data and model configurations for training LLMs presents a classic chicken-and-egg dilemma: The best training data configuration (e.g., data mixture) for a downstream task depends on the chosen model configuration (e.g., model architecture), and vice versa. However, jointly optimizing both data and model configurations is often deemed intractable, and existing methods focus on either data or model optimization without considering their interaction. We introduce JoBS, an approach that uses a scaling-law-inspired performance predictor to aid Bayesian optimization (BO) in jointly optimizing LLM training data and model configurations efficiently. JoBS allocates a portion of the optimization budget to learn an LLM performance predictor that predicts how promising a training configuration is from a small number of training steps. The remaining budget is used to perform BO entirely with the predictor, effectively amortizing the cost of running full-training runs. We study JoBS's average regret and devise the optimal budget allocation to minimize regret. JoBS outperforms existing multi-fidelity BO baselines, as well as data and model optimization approaches across diverse LLM tasks under the same optimization budget.

</details>


### [386] [Dynamic Regret via Discounted-to-Dynamic Reduction with Applications to Curved Losses and Adam Optimizer](https://arxiv.org/abs/2602.08372)
*Yan-Feng Xie,Yu-Jie Zhang,Peng Zhao,Zhi-Hua Zhou*

Main category: cs.LG

TL;DR: 研究非平稳在线学习中FTRL方法的动态遗憾最小化，提出模块化方法获动态遗憾界，简化证明并获新保证，还用于分析Adam优化器。


<details>
  <summary>Details</summary>
Motivation: 现有FTRL动态遗憾分析较少，需深入研究。

Method: 基于折扣到动态的约简，提出模块化方法，聚焦线性回归和逻辑回归两种损失。

Result: 简化在线线性回归最优动态遗憾证明，获在线逻辑回归新动态遗憾保证，分析Adam优化器获最优收敛率及新结果。

Conclusion: 所提方法在非平稳在线学习及Adam优化器分析中有良好效果。

Abstract: We study dynamic regret minimization in non-stationary online learning, with a primary focus on follow-the-regularized-leader (FTRL) methods. FTRL is important for curved losses and for understanding adaptive optimizers such as Adam, yet existing dynamic regret analyses are less explored for FTRL. To address this, we build on the discounted-to-dynamic reduction and present a modular way to obtain dynamic regret bounds of FTRL-related problems. Specifically, we focus on two representative curved losses: linear regression and logistic regression. Our method not only simplifies existing proofs for the optimal dynamic regret of online linear regression, but also yields new dynamic regret guarantees for online logistic regression. Beyond online convex optimization, we apply the reduction to analyze the Adam optimizers, obtaining optimal convergence rates in stochastic, non-convex, and non-smooth settings. The reduction also enables a more detailed treatment of Adam with two discount parameters $(β_1,β_2)$, leading to new results for both clipped and clip-free variants of Adam optimizers.

</details>


### [387] [OJBKQ: Objective-Joint Babai-Klein Quantization](https://arxiv.org/abs/2602.08376)
*Xinyu Wang,Ziyu Zhao,Peng Lu,Yu Gu,Xiao-Wen Chang*

Main category: cs.LG

TL;DR: 提出OJBKQ层级后训练量化方法，解决低比特量化问题，实验显示在3 - 4比特下表现更好。


<details>
  <summary>Details</summary>
Motivation: 现有权重量化方法依赖启发式目标和贪婪舍入，导致低比特量化时性能显著下降。

Method: 将权重量化表述为激活和权重的联合优化问题，形成BILS问题，用扩展的Babai最近平面算法和Klein随机Babai算法找次优解。

Result: 在大语言模型实验中，OJBKQ在3 - 4比特下困惑度低于现有方法，计算成本相当。

Conclusion: OJBKQ在低比特量化时表现优于现有后训练量化方法。

Abstract: Post-training quantization (PTQ) is widely used to compress large language models without retraining. However, many existing weight-only methods rely on heuristic objectives and greedy rounding, thus leading to noticeable degradation under low-bit quantization. In this work, we introduce OJBKQ (Objective-Joint Babai-Klein Quantization with K-Best Sampling), a layer-wise PTQ method that formulates weight quantization as a joint optimization problem over activations and weights. This formulation results in a multiple-right-hand-side box-constrained integer least squares (BILS) problem in each layer, which is NP-hard. For each column of the weight matrix, we apply an extended Babai nearest-plane algorithm and an extended version of Klein's randomized Babai algorithm to find the minimum-residual Babai-Klein point, a sub-optimal solution to the BILS problem. Experimental results on large language models show that OJBKQ achieves lower perplexity at 3-4 bits compared to existing PTQ approaches, while maintaining comparable computational cost.

</details>


### [388] [Reinforcement Learning with Backtracking Feedback](https://arxiv.org/abs/2602.08377)
*Bilgehan Sel,Vaishakh Keshava,Phillip Wallis,Lukas Rutishauser,Ming Jin,Dingcheng Li*

Main category: cs.LG

TL;DR: 针对大语言模型安全问题，提出强化学习回溯反馈（RLBF）框架及增强的监督微调数据生成策略（BSAFE+），显著降低攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在对抗攻击和分布内错误方面的安全需求。

Method: 提出RLBF框架，利用强化学习阶段让模型动态纠正生成错误；提出BSAFE+策略，通过向安全文本注入违规来训练回溯机制。

Result: RLBF显著降低了不同基准和模型规模下的攻击成功率。

Conclusion: RLBF能实现卓越的安全结果，同时保留基础模型的实用性。

Abstract: Addressing the critical need for robust safety in Large Language Models (LLMs), particularly against adversarial attacks and in-distribution errors, we introduce Reinforcement Learning with Backtracking Feedback (RLBF). This framework advances upon prior methods, such as BSAFE, by primarily leveraging a Reinforcement Learning (RL) stage where models learn to dynamically correct their own generation errors. Through RL with critic feedback on the model's live outputs, LLMs are trained to identify and recover from their actual, emergent safety violations by emitting an efficient "backtrack by x tokens" signal, then continuing generation autoregressively. This RL process is crucial for instilling resilience against sophisticated adversarial strategies, including middle filling, Greedy Coordinate Gradient (GCG) attacks, and decoding parameter manipulations. To further support the acquisition of this backtracking capability, we also propose an enhanced Supervised Fine-Tuning (SFT) data generation strategy (BSAFE+). This method improves upon previous data creation techniques by injecting violations into coherent, originally safe text, providing more effective initial training for the backtracking mechanism. Comprehensive empirical evaluations demonstrate that RLBF significantly reduces attack success rates across diverse benchmarks and model scales, achieving superior safety outcomes while critically preserving foundational model utility.

</details>


### [389] [Drop the mask! GAMM-A Taxonomy for Graph Attributes Missing Mechanisms](https://arxiv.org/abs/2602.08407)
*Richard Serrano,Baptiste Jeudy,Charlotte Laclau,Christine Largeron*

Main category: cs.LG

TL;DR: 本文扩展了属性图中缺失数据机制的分类法并提出GAMM框架，实证表明现有插补方法在应对图感知缺失场景时面临困难。


<details>
  <summary>Details</summary>
Motivation: 探索属性图中的缺失数据面临独特挑战，需扩展现有缺失数据机制分类法。

Method: 提出GAMM框架，将缺失概率与节点属性和图结构系统关联，引入图特定依赖以丰富传统掩码机制定义。

Result: 实证显示，现有的插补方法在传统掩码下有效，但在更现实的图感知缺失场景中表现不佳。

Conclusion: 现有的插补方法需要改进以应对属性图中更复杂的缺失数据情况。

Abstract: Exploring missing data in attributed graphs introduces unique challenges beyond those found in tabular datasets. In this work, we extend the taxonomy for missing data mechanisms to attributed graphs by proposing GAMM (Graph Attributes Missing Mechanisms), a framework that systematically links missingness probability to both node attributes and the underlying graph structure. Our taxonomy enriches the conventional definitions of masking mechanisms by introducing graph-specific dependencies. We empirically demonstrate that state-of-the-art imputation methods, while effective on traditional masks, significantly struggle when confronted with these more realistic graph-aware missingness scenarios.

</details>


### [390] [Radial Müntz-Szász Networks: Neural Architectures with Learnable Power Bases for Multidimensional Singularities](https://arxiv.org/abs/2602.08419)
*Gnankan Landry Regis N'guessan,Bum Jun Kim*

Main category: cs.LG

TL;DR: 坐标可分离神经网络架构难建模径向奇异场，提出RMN网络在多基准测试中表现优，还进行扩展并明确其适用范围。


<details>
  <summary>Details</summary>
Motivation: 坐标可分离神经网络架构难以对径向奇异场进行建模，需要新方法。

Method: 引入Radial Müntz - Szász Networks (RMN)，将场表示为可学习径向幂的线性组合，且有极限稳定对数基元；还扩展到角度依赖和多源情况。

Result: 在十个2D和3D基准测试中，RMN比MLPs和SIREN的RMSE低很多，使用参数少；扩展后优化收敛时源中心恢复误差低，也明确了其不适用情况。

Conclusion: RMN网络能有效解决径向奇异场建模问题，且有一定扩展性和明确适用范围。

Abstract: Radial singular fields, such as $1/r$, $\log r$, and crack-tip profiles, are difficult to model for coordinate-separable neural architectures. We show that any $C^2$ function that is both radial and additively separable must be quadratic, establishing a fundamental obstruction for coordinate-wise power-law models. Motivated by this result, we introduce Radial Müntz-Szász Networks (RMN), which represent fields as linear combinations of learnable radial powers $r^μ$, including negative exponents, together with a limit-stable log-primitive for exact $\log r$ behavior. RMN admits closed-form spatial gradients and Laplacians, enabling physics-informed learning on punctured domains. Across ten 2D and 3D benchmarks, RMN achieves 1.5$\times$--51$\times$ lower RMSE than MLPs and 10$\times$--100$\times$ lower RMSE than SIREN while using 27 parameters, compared with 33,537 for MLPs and 8,577 for SIREN. We extend RMN to angular dependence (RMN-Angular) and to multiple sources with learnable centers (RMN-MC); when optimization converges, source-center recovery errors fall below $10^{-4}$. We also report controlled failures on smooth, strongly non-radial targets to delineate RMN's operating regime.

</details>


### [391] [The Connection between Kriging and Large Neural Networks](https://arxiv.org/abs/2602.08427)
*Marius Marinescu*

Main category: cs.LG

TL;DR: 本文探讨克里金法与神经网络的关联，研究发现二者紧密相关，结合二者视角可提升机器学习技术。


<details>
  <summary>Details</summary>
Motivation: 探讨空间统计模型与机器学习模型的关系，具体研究克里金法与神经网络的联系。

Method: 研究二者联系并回顾相关文献。

Result: 克里金法与神经网络虽乍看无关，但实际上紧密相关。

Conclusion: 理解二者关系并结合其视角，可使机器学习技术更具可解释性、可靠性和空间感知能力。

Abstract: AI has impacted many disciplines and is nowadays ubiquitous. In particular, spatial statistics is in a pivotal moment where it will increasingly intertwine with AI. In this scenario, a relevant question is what relationship spatial statistics models have with machine learning (ML) models, if any. In particular, in this paper, we explore the connections between Kriging and neural networks. At first glance, they may appear unrelated. Kriging - and its ML counterpart, Gaussian process regression - are grounded in probability theory and stochastic processes, whereas many ML models are extensively considered Black-Box models. Nevertheless, they are strongly related. We study their connections and revisit the relevant literature. The understanding of their relations and the combination of both perspectives may enhance ML techniques by making them more interpretable, reliable, and spatially aware.

</details>


### [392] [USBD: Universal Structural Basis Distillation for Source-Free Graph Domain Adaptation](https://arxiv.org/abs/2602.08431)
*Yingxu Wang,Kunyu Zhang,Mengzhu Wang,Siyang Gao,Nan Yin*

Main category: cs.LG

TL;DR: 提出通用结构基础蒸馏（USBD）框架解决 SF - GDA 中当前方法在拓扑结构变化下泛化性不足问题，实验显示其性能和效率更优。


<details>
  <summary>Details</summary>
Motivation: 现有 SF - GDA 方法在结构不同的目标数据集上泛化能力受限，依赖源训练 GNN 的平滑先验，在拓扑变化大时伪标签适应不可靠。

Method: 提出 USBD 框架，采用双层优化框架将源数据集提炼成紧凑结构基础，通过强制原型覆盖狄利克雷能量谱来捕获多样拓扑模式；推理时引入谱感知集成机制。

Result: 在基准测试中，USBD 显著优于现有方法，尤其在结构变化大的场景中表现出色，且通过将适应成本与目标数据规模解耦实现了更高计算效率。

Conclusion: USBD 为 SF - GDA 提供了更好的解决方案，能有效应对拓扑结构变化，提升性能和效率。

Abstract: SF-GDA is pivotal for privacy-preserving knowledge transfer across graph datasets. Although recent works incorporate structural information, they implicitly condition adaptation on the smoothness priors of sourcetrained GNNs, thereby limiting their generalization to structurally distinct targets. This dependency becomes a critical bottleneck under significant topological shifts, where the source model misinterprets distinct topological patterns unseen in the source domain as noise, rendering pseudo-label-based adaptation unreliable. To overcome this limitation, we propose the Universal Structural Basis Distillation, a framework that shifts the paradigm from adapting a biased model to learning a universal structural basis for SF-GDA. Instead of adapting a biased source model to a specific target, our core idea is to construct a structure-agnostic basis that proactively covers the full spectrum of potential topological patterns. Specifically, USBD employs a bi-level optimization framework to distill the source dataset into a compact structural basis. By enforcing the prototypes to span the full Dirichlet energy spectrum, the learned basis explicitly captures diverse topological motifs, ranging from low-frequency clusters to high-frequency chains, beyond those present in the source. This ensures that the learned basis creates a comprehensive structural covering capable of handling targets with disparate structures. For inference, we introduce a spectral-aware ensemble mechanism that dynamically activates the optimal prototype combination based on the spectral fingerprint of the target graph. Extensive experiments on benchmarks demonstrate that USBD significantly outperforms state-of-the-art methods, particularly in scenarios with severe structural shifts, while achieving superior computational efficiency by decoupling the adaptation cost from the target data scale.

</details>


### [393] [Estimating Aleatoric Uncertainty in the Causal Treatment Effect](https://arxiv.org/abs/2602.08461)
*Liyuan Xu,Bijan Mazaheri*

Main category: cs.LG

TL;DR: 本文引入VTE和CVTE衡量治疗反应不确定性，提出非参数核估计方法，理论证明收敛性，实验显示性能良好。


<details>
  <summary>Details</summary>
Motivation: 过往因果推断研究多关注治疗效果均值，对个体治疗反应变异性和不确定性关注不足。

Method: 引入VTE和CVTE衡量不确定性，提出非参数核估计方法，并进行理论分析。

Result: 理论上证明估计量收敛，实验中方法性能优于或相当于简单基线。

Conclusion: VTE和CVTE可衡量治疗反应不确定性，所提方法有效可行。

Abstract: Previous work on causal inference has primarily focused on averages and conditional averages of treatment effects, with significantly less attention on variability and uncertainty in individual treatment responses. In this paper, we introduce the variance of the treatment effect (VTE) and conditional variance of treatment effect (CVTE) as the natural measure of aleatoric uncertainty inherent in treatment responses, and we demonstrate that these quantities are identifiable from observed data under mild assumptions, even in the presence of unobserved confounders. We further propose nonparametric kernel-based estimators for VTE and CVTE, and our theoretical analysis establishes their convergence. We also test the performance of our method through extensive empirical experiments on both synthetic and semi-simulated datasets, where it demonstrates superior or comparable performance to naive baselines.

</details>


### [394] [Time-Delayed Transformers for Data-Driven Modeling of Low-Dimensional Dynamics](https://arxiv.org/abs/2602.08478)
*Albert Alcalde,Markus Widhalm,Emre Yılmaz*

Main category: cs.LG

TL;DR: 提出时间延迟变压器TD - TF用于非定常时空动力学建模，结构简单，计算复杂度低，在非线性和混沌系统表现优。


<details>
  <summary>Details</summary>
Motivation: 为非定常时空动力学的数据驱动建模提供一种简化的变压器架构。

Method: 提出TD - TF架构，将单层单头变压器解释为时间延迟动态模式分解的非线性推广，架构仅含一个自注意力层和一个前馈层。

Result: TD - TF在近线性系统中与强线性基线性能相当，在非线性和混沌系统中表现显著更优，能准确捕捉长期动态。

Conclusion: TD - TF保留了线性模型的可解释性和效率，同时增强了对复杂动态的表达能力。

Abstract: We propose the time-delayed transformer (TD-TF), a simplified transformer architecture for data-driven modeling of unsteady spatio-temporal dynamics. TD-TF bridges linear operator-based methods and deep sequence models by showing that a single-layer, single-head transformer can be interpreted as a nonlinear generalization of time-delayed dynamic mode decomposition (TD-DMD). The architecture is deliberately minimal, consisting of one self-attention layer with a single query per prediction and one feedforward layer, resulting in linear computational complexity in sequence length and a small parameter count. Numerical experiments demonstrate that TD-TF matches the performance of strong linear baselines on near-linear systems, while significantly outperforming them in nonlinear and chaotic regimes, where it accurately captures long-term dynamics. Validation studies on synthetic signals, unsteady aerodynamics, the Lorenz '63 system, and a reaction-diffusion model show that TD-TF preserves the interpretability and efficiency of linear models while providing substantially enhanced expressive power for complex dynamics.

</details>


### [395] [Beyond Correctness: Learning Robust Reasoning via Transfer](https://arxiv.org/abs/2602.08489)
*Hyunseok Lee,Soheil Abbasloo,Jihoon Tack,Jinwoo Shin*

Main category: cs.LG

TL;DR: 本文指出RLVR强化大模型推理时未保证推理过程鲁棒性的问题，提出RLTR，通过转移奖励引入鲁棒性，能提升采样一致性和答案准确性，减少训练步数。


<details>
  <summary>Details</summary>
Motivation: 目前的RLVR关注最终答案正确性，未确保推理过程本身的鲁棒性。

Method: 引入Reinforcement Learning with Transferable Reward (RLTR)，通过转移奖励（测试一个模型的部分推理前缀能否引导另一个模型得到正确答案）来保证推理的鲁棒性。

Result: 提升采样一致性和最终答案准确性，显著减少训练步数。如在MATH500数据集上，对比RLVR，Maj@64提升3.6%，训练步数大概减少2.5倍。

Conclusion: RLTR能提供更可靠推理，并显著提高样本使用效率。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has recently strengthened LLM reasoning, but its focus on final answer correctness leaves a critical gap: it does not ensure the robustness of the reasoning process itself. We adopt a simple philosophical view, robust reasoning should remain useful beyond the mind that produced it, and treat reasoning as a form of meaning transfer that must survive truncation, reinterpretation, and continuation. Building on this principle, we introduce Reinforcement Learning with Transferable Reward (RLTR), which operationalizes robustness via transfer reward that tests whether a partial reasoning prefix from one model can guide a separate model to the correct answer. This encourages LLMs to produce reasoning that is stable, interpretable, and genuinely generalizable. Our approach improves sampling consistency while improving final answer accuracy, and it reaches comparable performance in substantially fewer training steps. For example, on MATH500, RLTR achieves a +3.6%p gain in Maj@64 compared to RLVR and matches RLVR's average accuracy with roughly 2.5x fewer training steps, providing both more reliable reasoning and significantly more sample efficient.

</details>


### [396] [Contextual Rollout Bandits for Reinforcement Learning with Verifiable Rewards](https://arxiv.org/abs/2602.08499)
*Xiaodong Lu,Xiaohan Wang,Jiajun Chai,Guojun Yin,Wei Lin,Zhijun Chen,Yu Luo,Fuzhen Zhuang,Yikun Ban,Deqing Wang*

Main category: cs.LG

TL;DR: 现有RLVR方法存在问题，提出统一神经调度框架解决，理论证明有效，实验验证性能和效率提升。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法在使用滚动输出时存在无差别、短视问题，导致监督噪声大、样本效率低和策略更新不佳。

Method: 将RLVR中的滚动输出调度问题建模为上下文多臂老虎机问题，提出统一神经调度框架，自适应选择高价值滚动输出。

Result: 理论上推导出次线性遗憾边界，实验在六个数学推理基准上，多个RLVR优化方法的性能和训练效率均有提升。

Conclusion: 提出的统一神经调度框架能有效解决现有RLVR方法的问题，提升性能和效率。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) is an effective paradigm for improving the reasoning capabilities of large language models. However, existing RLVR methods utilize rollouts in an indiscriminate and short-horizon manner: responses of heterogeneous quality within each prompt are treated uniformly, and historical rollouts are discarded after a single use. This leads to noisy supervision, poor sample efficiency, and suboptimal policy updates. We address these issues by formulating rollout scheduling in RLVR as a contextual bandit problem and proposing a unified neural scheduling framework that adaptively selects high-value rollouts throughout training. Each rollout is treated as an arm whose reward is defined by the induced performance gain between consecutive optimization steps. The resulting scheduler supports both noise-aware intra-group selection and adaptive global reuse of historical rollouts within a single principled framework. We provide theoretical justification by deriving sublinear regret bounds and showing that enlarging the rollout buffer improves the achievable performance upper bound. Experiments on six mathematical reasoning benchmarks demonstrate consistent gains in performance and training efficiency across multiple RLVR optimization methods.

</details>


### [397] [Is Meta-Path Attention an Explanation? Evidence of Alignment and Decoupling in Heterogeneous GNNs](https://arxiv.org/abs/2602.08500)
*Maiqi Jiang,Noman Ali,Yiran Ding,Yanfu Zhang*

Main category: cs.LG

TL;DR: 研究元路径注意力何时反映元路径重要性及何时解耦，引入MetaXplain协议，提出MP - AEA量化注意力可靠性，结果显示元路径感知解释表现好且有去噪效果。


<details>
  <summary>Details</summary>
Motivation: 探究元路径注意力何时能反映元路径重要性及何时解耦，且多数事后GNN解释器不适用于异质图。

Method: 引入MetaXplain协议，包括视图分解解释、模式有效通道扰动和融合感知归因；使用代表性解释器在多个数据集和模型上进行基准测试；提出MP - AEA量化注意力可靠性。

Result: 元路径感知解释通常优于随机对照；MP - AEA显示不同数据集和骨干网络有高对齐和显著解耦情况；在解释诱导子图上重新训练常能保持或提高预测性能。

Conclusion: MetaXplain协议有效，元路径注意力存在不同情况，解释有去噪效果。

Abstract: Meta-path-based heterogeneous graph neural networks aggregate over meta-path-induced views, and their semantic-level attention over meta-path channels is widely used as a narrative for ``which semantics matter.'' We study this assumption empirically by asking: when does meta-path attention reflect meta-path importance, and when can it decouple? A key challenge is that most post-hoc GNN explainers are designed for homogeneous graphs, and naive adaptations to heterogeneous neighborhoods can mix semantics and confound perturbations. To enable a controlled empirical analysis, we introduce MetaXplain, a meta-path-aware post-hoc explanation protocol that applies existing explainers in the native meta-path view domain via (i) view-factorized explanations, (ii) schema-valid channel-wise perturbations, and (iii) fusion-aware attribution, without modifying the underlying predictor. We benchmark representative gradient-, perturbation-, and Shapley-style explainers on ACM, DBLP, and IMDB with HAN and HAN-GCN, comparing against xPath and type-matched random baselines under standard faithfulness metrics. To quantify attention reliability, we propose Meta-Path Attention--Explanation Alignment (MP-AEA), which measures rank correlation between learned attention weights and explanation-derived meta-path contribution scores across random runs. Our results show that meta-path-aware explanations typically outperform random controls, while MP-AEA reveals both high-alignment and statistically significant decoupling regimes depending on the dataset and backbone; moreover, retraining on explanation-induced subgraphs often preserves, and in some noisy regimes improves, predictive performance, suggesting an explanation-as-denoising effect.

</details>


### [398] [Bridging Academia and Industry: A Comprehensive Benchmark for Attributed Graph Clustering](https://arxiv.org/abs/2602.08519)
*Yunhui Liu,Pengyu Qiu,Yu Xing,Yongchao Liu,Peng Du,Chuntao Hong,Jiajun Zheng,Tao Zheng,Tieke He*

Main category: cs.LG

TL;DR: 论文提出全面且适用于生产环境的图聚类基准工具PyAGC，统一现有方法，提供新实现，收集多样数据集，倡导新评估协议并公开发布相关资源。


<details>
  <summary>Details</summary>
Motivation: 当前属性图聚类在学术研究和实际应用间存在差距，现有评估方案有缺陷。

Method: 将现有方法统一到Encode - Cluster - Optimize框架，为现有算法提供内存高效的小批量实现，收集12个多样数据集，倡导包含无监督结构指标和效率分析的评估协议。

Result: 开发出PyAGC，在蚂蚁集团的工业流程中得到验证。

Conclusion: 该基准为属性图聚类研究提供了强大、可重复和可扩展的平台，推动研究走向实际应用。

Abstract: Attributed Graph Clustering (AGC) is a fundamental unsupervised task that integrates structural topology and node attributes to uncover latent patterns in graph-structured data. Despite its significance in industrial applications such as fraud detection and user segmentation, a significant chasm persists between academic research and real-world deployment. Current evaluation protocols suffer from the small-scale, high-homophily citation datasets, non-scalable full-batch training paradigms, and a reliance on supervised metrics that fail to reflect performance in label-scarce environments. To bridge these gaps, we present PyAGC, a comprehensive, production-ready benchmark and library designed to stress-test AGC methods across diverse scales and structural properties. We unify existing methodologies into a modular Encode-Cluster-Optimize framework and, for the first time, provide memory-efficient, mini-batch implementations for a wide array of state-of-the-art AGC algorithms. Our benchmark curates 12 diverse datasets, ranging from 2.7K to 111M nodes, specifically incorporating industrial graphs with complex tabular features and low homophily. Furthermore, we advocate for a holistic evaluation protocol that mandates unsupervised structural metrics and efficiency profiling alongside traditional supervised metrics. Battle-tested in high-stakes industrial workflows at Ant Group, this benchmark offers the community a robust, reproducible, and scalable platform to advance AGC research towards realistic deployment. The code and resources are publicly available via GitHub (https://github.com/Cloudy1225/PyAGC), PyPI (https://pypi.org/project/pyagc), and Documentation (https://pyagc.readthedocs.io).

</details>


### [399] [Causal Schrödinger Bridges: Constrained Optimal Transport on Structural Manifolds](https://arxiv.org/abs/2602.08535)
*Rui Wu,Li YongJun*

Main category: cs.LG

TL;DR: 提出因果薛定谔桥（CSB）框架，将反事实推理重新表述为熵最优传输，在高维干预任务上显著优于确定性基线。


<details>
  <summary>Details</summary>
Motivation: 确定性生成建模在因果干预下因需穿越低密度区域而变得脆弱，导致数值不稳定和虚假关联。

Method: 引入CSB框架，利用扩散过程（SDEs）穿越支持不匹配，严格执行结构可容许性约束，并证明结构分解定理。

Result: 在高维干预（Morpho - MNIST）的实证验证中，CSB在结构一致性上显著优于确定性基线。

Conclusion: CSB在强分布外处理的情况下，能更好地应对因果干预，在结构一致性方面表现出色。

Abstract: Generative modeling typically seeks the path of least action via deterministic flows (ODE). While effective for in-distribution tasks, we argue that these deterministic paths become brittle under causal interventions, which often require transporting probability mass across low-density regions ("off-manifold") where the vector field is ill-defined. This leads to numerical instability and spurious correlations. In this work, we introduce the Causal Schrödinger Bridge (CSB), a framework that reformulates counterfactual inference as Entropic Optimal Transport. Unlike deterministic approaches that require strict invertibility, CSB leverages diffusion processes (SDEs) to robustly "tunnel" through support mismatches while strictly enforcing structural admissibility constraints. We prove the Structural Decomposition Theorem, showing that the global high-dimensional bridge factorizes into local, robust transitions. Empirical validation on high-dimensional interventions (Morpho-MNIST) demonstrates that CSB significantly outperforms deterministic baselines in structural consistency, particularly in regimes of strong, out-of-distribution treatments.

</details>


### [400] [Stateless Yet Not Forgetful: Implicit Memory as a Hidden Channel in LLMs](https://arxiv.org/abs/2602.08563)
*Ahmed Salem,Andrew Paverd,Sahar Abdelnabi*

Main category: cs.LG

TL;DR: 本文提出大语言模型的隐式记忆概念，以时间炸弹为例展示其应用，分析影响并讨论检测挑战，还发布代码和数据。


<details>
  <summary>Details</summary>
Motivation: 挑战大语言模型无状态的普遍假设，探索其跨交互保留信息的能力。

Method: 引入隐式记忆概念，以时间炸弹作为具体案例，通过简单提示或微调诱导其行为，分析更广泛影响。

Result: 证明可通过简单提示或微调诱导时间炸弹行为，分析了隐式记忆带来的多方面影响。

Conclusion: 探讨了隐式记忆检测挑战，给出压力测试和评估方向，旨在预见和控制未来发展。

Abstract: Large language models (LLMs) are commonly treated as stateless: once an interaction ends, no information is assumed to persist unless it is explicitly stored and re-supplied. We challenge this assumption by introducing implicit memory-the ability of a model to carry state across otherwise independent interactions by encoding information in its own outputs and later recovering it when those outputs are reintroduced as input. This mechanism does not require any explicit memory module, yet it creates a persistent information channel across inference requests. As a concrete demonstration, we introduce a new class of temporal backdoors, which we call time bombs. Unlike conventional backdoors that activate on a single trigger input, time bombs activate only after a sequence of interactions satisfies hidden conditions accumulated via implicit memory. We show that such behavior can be induced today through straightforward prompting or fine-tuning. Beyond this case study, we analyze broader implications of implicit memory, including covert inter-agent communication, benchmark contamination, targeted manipulation, and training-data poisoning. Finally, we discuss detection challenges and outline directions for stress-testing and evaluation, with the goal of anticipating and controlling future developments. To promote future research, we release code and data at: https://github.com/microsoft/implicitMemory.

</details>


### [401] [M-Loss: Quantifying Model Merging Compatibility with Limited Unlabeled Data](https://arxiv.org/abs/2602.08564)
*Tiantong Wang,Yiyang Duan,Haoyu Chen,Tiantong Wu,Wei Yang Bryan Lim*

Main category: cs.LG

TL;DR: 传统模型合并有缺陷，集成推理成本高，提出M-Loss评估指标，可提升模型合并效果。


<details>
  <summary>Details</summary>
Motivation: 传统模型合并技术有非泛化特征组合问题，模型集成推理成本高，且缺乏模型合并与集成相似性的理论证据和评估指标。

Method: 引入Merging - ensembling loss（M - Loss）评估指标，基于有限无标签数据量化合并源模型的兼容性，在层和节点层面衡量参数平均与模型集成的差异。

Result: 将M - Loss融入合并过程，显著提升了合并模型与模型集成的一致性。

Conclusion: M - Loss为准确的模型合并提供了可扩展且高效的框架。

Abstract: Training of large-scale models is both computationally intensive and often constrained by the availability of labeled data. Model merging offers a compelling alternative by directly integrating the weights of multiple source models without requiring additional data or extensive training. However, conventional model merging techniques, such as parameter averaging, often suffer from the unintended combination of non-generalizable features, especially when source models exhibit significant weight disparities. Comparatively, model ensembling generally provides more stable and superior performance that aggregates multiple models by averaging outputs. However, it incurs higher inference costs and increased storage requirements. While previous studies experimentally showed the similarities between model merging and ensembling, theoretical evidence and evaluation metrics remain lacking. To address this gap, we introduce Merging-ensembling loss (M-Loss), a novel evaluation metric that quantifies the compatibility of merging source models using very limited unlabeled data. By measuring the discrepancy between parameter averaging and model ensembling at layer and node levels, M-Loss facilitates more effective merging strategies. Specifically, M-Loss serves both as a quantitative criterion of the theoretical feasibility of model merging, and a guide for parameter significance in model pruning. Our theoretical analysis and empirical evaluations demonstrate that incorporating M-Loss into the merging process significantly improves the alignment between merged models and model ensembling, providing a scalable and efficient framework for accurate model consolidation.

</details>


### [402] [Modeling Score Approximation Errors in Diffusion Models via Forward SPDEs](https://arxiv.org/abs/2602.08579)
*Junsu Seo*

Main category: cs.LG

TL;DR: 研究基于分数的生成模型（SGMs）动态，用SPDE框架建模，引入评估指标且初步显示有计算效率潜力。


<details>
  <summary>Details</summary>
Motivation: 研究SGMs的动态特性。

Method: 将分数估计误差视为随机源驱动Fokker - Planck方程，采用SPDE框架建模概率密度场演化，在简化设置下从几何稳定性和位移凸性角度解释模型鲁棒性，引入基于SPDE解投影到径向测试函数二次变差的评估指标。

Result: 初步观察表明该指标仅用采样轨迹初始10%就有效。

Conclusion: 所引入的评估指标有计算效率提升的潜力。

Abstract: This study investigates the dynamics of Score-based Generative Models (SGMs) by treating the score estimation error as a stochastic source driving the Fokker-Planck equation. Departing from particle-centric SDE analyses, we employ an SPDE framework to model the evolution of the probability density field under stochastic drift perturbations. Under a simplified setting, we utilize this framework to interpret the robustness of generative models through the lens of geometric stability and displacement convexity. Furthermore, we introduce a candidate evaluation metric derived from the quadratic variation of the SPDE solution projected onto a radial test function. Preliminary observations suggest that this metric remains effective using only the initial 10% of the sampling trajectory, indicating a potential for computational efficiency.

</details>


### [403] [Conditional Sequence Modeling for Safe Reinforcement Learning](https://arxiv.org/abs/2602.08584)
*Wensong Bai,Chao Zhang,Qihang Xu,Chufan Chen,Chenhao Zhou,Hui Qian*

Main category: cs.LG

TL;DR: 本文提出基于条件序列建模的离线安全强化学习方法RCDT，支持在单一训练策略下跨多个成本阈值的零样本部署，实验表明其表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有离线安全强化学习方法大多在预设阈值下训练，策略在不同成本阈值间的泛化和部署灵活性有限，而实际场景中部署需求多样，需要能零样本适应不同成本阈值的单一策略。

Method: 提出RCDT方法，它是首个将拉格朗日风格的成本惩罚与自适应惩罚系数相结合的基于条件序列建模的离线安全强化学习算法，还加入奖励 - 成本感知的轨迹重加权机制和Q值正则化。

Result: 在DSRL基准上的大量实验表明，RCDT在回报 - 成本权衡方面始终优于代表性基线。

Conclusion: RCDT推动了离线安全强化学习的发展。

Abstract: Offline safe reinforcement learning (RL) aims to learn policies from a fixed dataset while maximizing performance under cumulative cost constraints. In practice, deployment requirements often vary across scenarios, necessitating a single policy that can adapt zero-shot to different cost thresholds. However, most existing offline safe RL methods are trained under a pre-specified threshold, yielding policies with limited generalization and deployment flexibility across cost thresholds. Motivated by recent progress in conditional sequence modeling (CSM), which enables flexible goal-conditioned control by specifying target returns, we propose RCDT, a CSM-based method that supports zero-shot deployment across multiple cost thresholds within a single trained policy. RCDT is the first CSM-based offline safe RL algorithm that integrates a Lagrangian-style cost penalty with an auto-adaptive penalty coefficient. To avoid overly conservative behavior and achieve a more favorable return--cost trade-off, a reward--cost-aware trajectory reweighting mechanism and Q-value regularization are further incorporated. Extensive experiments on the DSRL benchmark demonstrate that RCDT consistently improves return--cost trade-offs over representative baselines, advancing the state-of-the-art in offline safe RL.

</details>


### [404] [Predicting Future Utility: Global Combinatorial Optimization for Task-Agnostic KV Cache Eviction](https://arxiv.org/abs/2602.08585)
*Ziyao Tang,Pengkun Jiao,Xinhang Chen,Wei Liu,Shiyong Li,Jingjing Chen*

Main category: cs.LG

TL;DR: 提出LU - KV框架优化KV缓存驱逐，减少缓存大小，降低推理延迟和显存占用。


<details>
  <summary>Details</summary>
Motivation: 当前KV缓存驱逐方法依赖瞬时启发式指标，忽略注意力头预测保真度的异质性，需考虑长期语义信息的边际效用进行最优预算分配。

Method: 提出LU - KV框架，通过凸包松弛和基于边际效用的贪婪求解器优化头级预算分配，实现数据驱动的离线分析协议以方便实际部署。

Result: 在LongBench和RULER基准测试中，LU - KV使KV缓存大小减少80%，性能仅有极小下降，同时降低推理延迟和GPU显存占用。

Conclusion: LU - KV框架能有效优化KV缓存驱逐，在减少缓存大小的同时保证性能，具有良好的实际应用价值。

Abstract: Given the quadratic complexity of attention, KV cache eviction is vital to accelerate model inference. Current KV cache eviction methods typically rely on instantaneous heuristic metrics, implicitly assuming that score magnitudes are consistent proxies for importance across all heads. However, this overlooks the heterogeneity in predictive fidelity across attention heads. While certain heads prioritize the instantaneous contribution of tokens, others are dedicated to capturing long-horizon utility. In this paper, we propose that optimal budget allocation should be governed by the marginal utility in preserving long-term semantic information. Based on this insight, we propose LU-KV, a novel framework that optimizes head-level budget allocation through a convex-hull relaxation and a marginal-utility-based greedy solver to achieve near-optimal precision. Furthermore, we implement a data-driven offline profiling protocol to facilitate the practical deployment of LU-KV. Extensive evaluations on LongBench and RULER benchmarks demonstrate that LU-KV achieves an 80% reduction in KV cache size with minimal performance degradation, while simultaneously reducing inference latency and GPU memory footprint.

</details>


### [405] [FairRARI: A Plug and Play Framework for Fairness-Aware PageRank](https://arxiv.org/abs/2602.08589)
*Emmanouil Kariotakis,Aritra Konar*

Main category: cs.LG

TL;DR: 提出FairRARI框架计算满足公平性标准的PageRank向量，实验显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有计算PageRank向量的算法在群体公平性标准上存在不足，缺乏有原则的算法。

Method: 提出统一的凸优化框架FairRARI，利用PageRank的变分公式，解决带公平性约束的强凸优化问题。

Result: 引入三种公平性标准，用FairRARI计算公平PageRank向量，时间复杂度与原算法相同，实验显示其在效用上优于现有方法。

Conclusion: FairRARI能有效计算满足公平性要求的PageRank向量，且在多顶点组实现期望公平水平。

Abstract: PageRank (PR) is a fundamental algorithm in graph machine learning tasks. Owing to the increasing importance of algorithmic fairness, we consider the problem of computing PR vectors subject to various group-fairness criteria based on sensitive attributes of the vertices. At present, principled algorithms for this problem are lacking - some cannot guarantee that a target fairness level is achieved, while others do not feature optimality guarantees. In order to overcome these shortcomings, we put forth a unified in-processing convex optimization framework, termed FairRARI, for tackling different group-fairness criteria in a ``plug and play'' fashion. Leveraging a variational formulation of PR, the framework computes fair PR vectors by solving a strongly convex optimization problem with fairness constraints, thereby ensuring that a target fairness level is achieved. We further introduce three different fairness criteria which can be efficiently tackled using FairRARI to compute fair PR vectors with the same asymptotic time-complexity as the original PR algorithm. Extensive experiments on real-world datasets showcase that FairRARI outperforms existing methods in terms of utility, while achieving the desired fairness levels across multiple vertex groups; thereby highlighting its effectiveness.

</details>


### [406] [TFMLinker: Universal Link Predictor by Graph In-Context Learning with Tabular Foundation Models](https://arxiv.org/abs/2602.08592)
*Tianyin Liao,Chunyu Hu,Yicheng Sui,Xingxuan Zhang,Peng Cui,Jianxin Li,Ziwei Zhang*

Main category: cs.LG

TL;DR: 本文提出TFMLinker方法利用表格基础模型进行跨图的链接预测，在6个图基准测试上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在基础模型时代，现有链接预测方法有局限，受表格基础模型在表格数据通用预测的成功启发，探索用其进行链接预测。

Method: 提出TFMLinker，开发原型增强的局部 - 全局上下文模块构建上下文，设计通用拓扑感知链接编码器捕获拓扑信息，用TFM通过上下文学习预测链接。

Result: 在6个不同领域的图基准测试中，所提方法优于现有基线方法，且无需特定数据集微调。

Conclusion: 所提TFMLinker方法能有效利用表格基础模型进行跨图的链接预测。

Abstract: Link prediction is a fundamental task in graph machine learning with widespread applications such as recommendation systems, drug discovery, knowledge graphs, etc. In the foundation model era, how to develop universal link prediction methods across datasets and domains becomes a key problem, with some initial attempts adopting Graph Foundation Models utilizing Graph Neural Networks and Large Language Models. However, the existing methods face notable limitations, including limited pre-training scale or heavy reliance on textual information. Motivated by the success of tabular foundation models (TFMs) in achieving universal prediction across diverse tabular datasets, we explore an alternative approach by TFMs, which are pre-trained on diverse synthetic datasets sampled from structural causal models and support strong in-context learning independent of textual attributes. Nevertheless, adapting TFMs for link prediction faces severe technical challenges such as how to obtain the necessary context and capture link-centric topological information. To solve these challenges, we propose TFMLinker (Tabular Foundation Model for Link Predictor), aiming to leverage the in-context learning capabilities of TFMs to perform link prediction across diverse graphs without requiring dataset-specific fine-tuning. Specifically, we first develop a prototype-augmented local-global context module to construct context that captures both graph-specific and cross-graph transferable patterns. Next, we design a universal topology-aware link encoder to capture link-centric topological information and generate link representations as inputs for the TFM. Finally, we employ the TFM to predict link existence through in-context learning. Experiments on 6 graph benchmarks across diverse domains demonstrate the superiority of our method over state-of-the-art baselines without requiring dataset-specific finetuning.

</details>


### [407] [Breaking the Grid: Distance-Guided Reinforcement Learning in Large Discrete and Hybrid Action Spaces](https://arxiv.org/abs/2602.08616)
*Heiko Hoppe,Fabian Akkerman,Wouter van Heeswijk,Maximilian Schiffer*

Main category: cs.LG

TL;DR: 提出DGRL算法解决强化学习在大离散动作空间中的维度诅咒问题，在多种环境中性能提升。


<details>
  <summary>Details</summary>
Motivation: 标准强化学习算法在大离散动作空间中遭遇维度诅咒，现有算法存在依赖特定结构或计算昂贵的问题。

Method: 提出DGRL算法，结合SDN和DBU，SDN利用语义嵌入空间随机体积探索，DBU将策略优化转化为稳定回归任务。

Result: 在规则和不规则环境中，相对基准性能提升达66%，同时改善了收敛速度和计算复杂度。

Conclusion: DGRL算法能有效解决大离散动作空间中的强化学习问题，且自然适用于混合连续 - 离散动作空间。

Abstract: Reinforcement Learning is increasingly applied to logistics, scheduling, and recommender systems, but standard algorithms struggle with the curse of dimensionality in such large discrete action spaces. Existing algorithms typically rely on restrictive grid-based structures or computationally expensive nearest-neighbor searches, limiting their effectiveness in high-dimensional or irregularly structured domains. We propose Distance-Guided Reinforcement Learning (DGRL), combining Sampled Dynamic Neighborhoods (SDN) and Distance-Based Updates (DBU) to enable efficient RL in spaces with up to 10$^\text{20}$ actions. Unlike prior methods, SDN leverages a semantic embedding space to perform stochastic volumetric exploration, provably providing full support over a local trust region. Complementing this, DBU transforms policy optimization into a stable regression task, decoupling gradient variance from action space cardinality and guaranteeing monotonic policy improvement. DGRL naturally generalizes to hybrid continuous-discrete action spaces without requiring hierarchical dependencies. We demonstrate performance improvements of up to 66% against state-of-the-art benchmarks across regularly and irregularly structured environments, while simultaneously improving convergence speed and computational complexity.

</details>


### [408] [ERIS: Enhancing Privacy and Communication Efficiency in Serverless Federated Learning](https://arxiv.org/abs/2602.08617)
*Dario Fenoglio,Pasquale Polverino,Jacopo Quizi,Martin Gjoreski,Marc Langheinrich*

Main category: cs.LG

TL;DR: 提出无服务器联邦学习框架 ERIS，平衡隐私与准确性，减少通信成本，提升对抗攻击能力。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习扩展到十亿参数模型时，在通信效率、模型准确性和隐私保证方面存在权衡问题，现有解决方案常孤立处理这些挑战。

Method: 结合模型分区策略和分布式移位梯度压缩机制，将聚合分布到多个客户端聚合器。

Result: 理论证明 ERIS 收敛速度与 FedAvg 相同，能限制互信息泄漏；实验表明 ERIS 达到 FedAvg 水平的准确性，大幅降低通信成本，提高对抗攻击的鲁棒性。

Conclusion: ERIS 是一种有效的无服务器联邦学习框架，可平衡隐私和准确性，且无需依赖复杂密码学或噪声注入。

Abstract: Scaling federated learning (FL) to billion-parameter models introduces critical trade-offs between communication efficiency, model accuracy, and privacy guarantees. Existing solutions often tackle these challenges in isolation, sacrificing accuracy or relying on costly cryptographic tools. We propose ERIS, a serverless FL framework that balances privacy and accuracy while eliminating the server bottleneck and distributing the communication load. ERIS combines a model partitioning strategy, distributing aggregation across multiple client-side aggregators, with a distributed shifted gradient compression mechanism. We theoretically prove that ERIS (i) converges at the same rate as FedAvg under standard assumptions, and (ii) bounds mutual information leakage inversely with the number of aggregators, enabling strong privacy guarantees with no accuracy degradation. Experiments across image and text tasks, including large language models, confirm that ERIS achieves FedAvg-level accuracy while substantially reducing communication cost and improving robustness to membership inference and reconstruction attacks, without relying on heavy cryptography or noise injection.

</details>


### [409] [Sparse Models, Sparse Safety: Unsafe Routes in Mixture-of-Experts LLMs](https://arxiv.org/abs/2602.08621)
*Yukun Jiang,Hai Huang,Mingjie Li,Yage Zhang,Michael Backes,Yang Zhang*

Main category: cs.LG

TL;DR: 研究发现MoE大模型中路由配置不安全会使安全输出变有害，提出RoSais和F - SOUR发现不安全路由，给出防御方向。


<details>
  <summary>Details</summary>
Motivation: 以往研究多关注MoE架构的效用和效率，未充分探索其安全风险，本文旨在研究MoE大模型的安全问题。

Method: 引入RoSais量化各层路由器的安全关键性；提出F - SOUR框架发现更具体的不安全路由。

Result: 在JailbreakBench上，DeepSeek - V2 - Lite掩盖5个路由器使攻击成功率提高超4倍至0.79；F - SOUR在JailbreakBench和AdvBench上平均攻击成功率分别达0.90和0.98。

Conclusion: 给出安全感知的路由禁用和路由器训练等防御方向，希望为未来MoE大模型的红队测试和防护提供参考。

Abstract: By introducing routers to selectively activate experts in Transformer layers, the mixture-of-experts (MoE) architecture significantly reduces computational costs in large language models (LLMs) while maintaining competitive performance, especially for models with massive parameters. However, prior work has largely focused on utility and efficiency, leaving the safety risks associated with this sparse architecture underexplored. In this work, we show that the safety of MoE LLMs is as sparse as their architecture by discovering unsafe routes: routing configurations that, once activated, convert safe outputs into harmful ones. Specifically, we first introduce the Router Safety importance score (RoSais) to quantify the safety criticality of each layer's router. Manipulation of only the high-RoSais router(s) can flip the default route into an unsafe one. For instance, on JailbreakBench, masking 5 routers in DeepSeek-V2-Lite increases attack success rate (ASR) by over 4$\times$ to 0.79, highlighting an inherent risk that router manipulation may naturally occur in MoE LLMs. We further propose a Fine-grained token-layer-wise Stochastic Optimization framework to discover more concrete Unsafe Routes (F-SOUR), which explicitly considers the sequentiality and dynamics of input tokens. Across four representative MoE LLM families, F-SOUR achieves an average ASR of 0.90 and 0.98 on JailbreakBench and AdvBench, respectively. Finally, we outline defensive perspectives, including safety-aware route disabling and router training, as promising directions to safeguard MoE LLMs. We hope our work can inform future red-teaming and safeguarding of MoE LLMs. Our code is provided in https://github.com/TrustAIRLab/UnsafeMoE.

</details>


### [410] [LEFT: Learnable Fusion of Tri-view Tokens for Unsupervised Time Series Anomaly Detection](https://arxiv.org/abs/2602.08638)
*Dezheng Wang,Tong Chen,Guansong Pang,Congyan Chen,Shihua Li,Hongzhi Yin*

Main category: cs.LG

TL;DR: 提出LEFT框架用于无监督时间序列异常检测，通过多视图学习检测异常，实验显示效果优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有无监督TSAD交叉视图方法未强制执行分析 - 合成一致性，且许多异常在单视图中难以检测。

Method: 提出Learnable Fusion of Tri - view Tokens (LEFT)框架，从频域、时域和多尺度三个视图学习特征令牌，引入新目标和时间 - 频率循环一致性约束。

Result: 在真实数据集上LEFT检测准确率优于SOTA基线，减少FLOPs 5倍，训练加速8倍。

Conclusion: LEFT框架是有效的无监督TSAD方法，能通过多视图学习和约束提高检测效果并提升效率。

Abstract: As a fundamental data mining task, unsupervised time series anomaly detection (TSAD) aims to build a model for identifying abnormal timestamps without assuming the availability of annotations. A key challenge in unsupervised TSAD is that many anomalies are too subtle to exhibit detectable deviation in any single view (e.g., time domain), and instead manifest as inconsistencies across multiple views like time, frequency, and a mixture of resolutions. However, most cross-view methods rely on feature or score fusion and do not enforce analysis-synthesis consistency, meaning the frequency branch is not required to reconstruct the time signal through an inverse transform, and vice versa. In this paper, we present Learnable Fusion of Tri-view Tokens (LEFT), a unified unsupervised TSAD framework that models anomalies as inconsistencies across complementary representations. LEFT learns feature tokens from three views of the same input time series: frequency-domain tokens that embed periodicity information, time-domain tokens that capture local dynamics, and multi-scale tokens that learns abnormal patterns at varying time series granularities. By learning a set of adaptive Nyquist-constrained spectral filters, the original time series is rescaled into multiple resolutions and then encoded, allowing these multi-scale tokens to complement the extracted frequency- and time-domain information. When generating the fused representation, we introduce a novel objective that reconstructs fine-grained targets from coarser multi-scale structure, and put forward an innovative time-frequency cycle consistency constraint to explicitly regularize cross-view agreement. Experiments on real-world benchmarks show that LEFT yields the best detection accuracy against SOTA baselines, while achieving a 5x reduction on FLOPs and 8x speed-up for training.

</details>


### [411] [Projected Gradient Ascent for Efficient Reward-Guided Updates with One-Step Generative Models](https://arxiv.org/abs/2602.08646)
*Jisung Hwang,Minhyuk Sung*

Main category: cs.LG

TL;DR: 提出一种约束潜在优化方法用于奖励引导生成，保持白高斯噪声特性且开销小，实验中用时短且防奖励破解。


<details>
  <summary>Details</summary>
Motivation: 解决测试时潜在优化易出现奖励破解、质量下降和速度慢的问题。

Method: 用投影梯度上升执行硬白高斯噪声约束替代软正则化，每次更新后应用闭式投影。

Result: 仅用SOTA正则化方法30%的时间达到相当的美学分数，同时防止奖励破解。

Conclusion: 该方法使测试时优化既高效又可靠。

Abstract: We propose a constrained latent optimization method for reward-guided generation that preserves white Gaussian noise characteristics with negligible overhead. Test-time latent optimization can unlock substantially better reward-guided generations from pretrained generative models, but it is prone to reward hacking that degrades quality and also too slow for practical use. In this work, we make test-time optimization both efficient and reliable by replacing soft regularization with hard white Gaussian noise constraints enforced via projected gradient ascent. Our method applies a closed-form projection after each update to keep the latent vector explicitly noise-like throughout optimization, preventing the drift that leads to unrealistic artifacts. This enforcement adds minimal cost: the projection matches the $O(N \log N)$ complexity of standard algorithms such as sorting or FFT and does not practically increase wall-clock time. In experiments, our approach reaches a comparable Aesthetic Score using only 30% of the wall-clock time required by the SOTA regularization-based method, while preventing reward hacking.

</details>


### [412] [From Robotics to Sepsis Treatment: Offline RL via Geometric Pessimism](https://arxiv.org/abs/2602.08655)
*Sarthak Wanjari*

Main category: cs.LG

TL;DR: 提出Geometric Pessimism框架增强IQL，在D4Rl和MIMIC - III数据集表现良好，能克服局部最优。


<details>
  <summary>Details</summary>
Motivation: 解决离线强化学习中OOD动作高估问题，平衡计算效率和性能。

Method: 提出Geometric Pessimism框架，用基于k近邻距离的密度惩罚增强IQL，通过奖励塑形注入OOD保守性。

Result: Geo - IQL在D4Rl上表现优于IQL，减少种子间方差；在MIMIC - III数据集上展示主动策略改进，终端协议达成率更高。

Conclusion: 几何悲观主义能提供必要正则化，安全克服关键现实决策系统中的局部最优。

Abstract: Offline Reinforcement Learning (RL) promises the recovery of optimal policies from static datasets, yet it remains susceptible to the overestimation of out-of-distribution (OOD) actions, particularly in fractured and sparse data manifolds.Current solutions necessitates a trade off between computational efficiency and performance. Methods like CQL offers rigorous conservatism but require tremendous compute power while efficient expectile-based methods like IQL often fail to correct OOD errors on pathological datasets, collapsing to Behavioural Cloning. In this work, we propose Geometric Pessimism, a modular, compute-efficient framework that augments standard IQL with density-based penalty derived from k-nearest-neighbour distances in the state-action embedding space. By pre-computing the penalties applied to each state-action pair our method injects OOD conservatism via reward shaping with a O(1) training overhead. Evaluated on the D4Rl MuJoCo benchmark, our method, Geo-IQL outperforms standard IQL on sensitive and unstable medium-replay tasks by over 18 points, while reducing inter-seed variance by 4x. Furthermore, Geo-IQL does not degrade performance on stable manifolds. Crucially, we validate our algorithm on the MIMIC-III Sepsis critical care dataset. While standard IQL collapses to behaviour cloning, Geo-IQL demonstrates active policy improvement. Maintaining safety constraints, achieving 86.4% terminal agreement with clinicians compared to IQL's 75%. Our results suggest that geometric pessimism provides the necessary regularisation to safely overcome local optima in critical, real-world decision systems.

</details>


### [413] [Two-Stage Data Synthesization: A Statistics-Driven Restricted Trade-off between Privacy and Prediction](https://arxiv.org/abs/2602.08657)
*Xiaotong Liu,Shao-Bo Lin,Jun Fan,Ding-Xuan Zhou*

Main category: cs.LG

TL;DR: 提出两阶段合成策略处理合成数据在下游预测任务的隐私 - 预测权衡问题并验证其效果。


<details>
  <summary>Details</summary>
Motivation: 现有合成策略难平衡隐私需求和预测性能，需新方法处理两者权衡。

Method: 提出两阶段合成策略，第一阶段采用合成 - 混合策略，第二阶段用基于核岭回归的合成策略。

Result: 理论和数值验证策略能进行统计驱动的受限隐私 - 预测权衡，保证最优预测性能，在营销问题和五个真实数据集上展示了通用性。

Conclusion: 所提两阶段合成策略有效，可实现受限的隐私 - 预测权衡及保证预测性能。

Abstract: Synthetic data have gained increasing attention across various domains, with a growing emphasis on their performance in downstream prediction tasks. However, most existing synthesis strategies focus on maintaining statistical information. Although some studies address prediction performance guarantees, their single-stage synthesis designs make it challenging to balance the privacy requirements that necessitate significant perturbations and the prediction performance that is sensitive to such perturbations. We propose a two-stage synthesis strategy. In the first stage, we introduce a synthesis-then-hybrid strategy, which involves a synthesis operation to generate pure synthetic data, followed by a hybrid operation that fuses the synthetic data with the original data. In the second stage, we present a kernel ridge regression (KRR)-based synthesis strategy, where a KRR model is first trained on the original data and then used to generate synthetic outputs based on the synthetic inputs produced in the first stage. By leveraging the theoretical strengths of KRR and the covariant distribution retention achieved in the first stage, our proposed two-stage synthesis strategy enables a statistics-driven restricted privacy--prediction trade-off and guarantee optimal prediction performance. We validate our approach and demonstrate its characteristics of being statistics-driven and restricted in achieving the privacy--prediction trade-off both theoretically and numerically. Additionally, we showcase its generalizability through applications to a marketing problem and five real-world datasets.

</details>


### [414] [Equalized Generative Treatment: Matching f-divergences for Fairness in Generative Models](https://arxiv.org/abs/2602.08660)
*Alexandre Verine,Rafael Pinot,Florian Le Bronnec*

Main category: cs.LG

TL;DR: 现有生成模型公平性标准易失效，本文提出EGT定义，分析权衡并验证了min - max微调方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型公平性概念多改编自分类，仅关注平衡生成概率，易在不同敏感组建模质量差异大时失效，需改进。

Method: 引入EGT定义，分析其带来的权衡，提出min - max微调方法平衡f - 散度以满足EGT。

Result: 在图像和文本生成任务实验中，min - max方法比其他方法更公平，且整体性能有竞争力。

Conclusion: min - max方法能有效满足EGT，在保证公平性同时维持较好整体表现。

Abstract: Fairness is a crucial concern for generative models, which not only reflect but can also amplify societal and cultural biases. Existing fairness notions for generative models are largely adapted from classification and focus on balancing the probability of generating samples from each sensitive group. We show that such criteria are brittle, as they can be met even when different sensitive groups are modeled with widely varying quality. To address this limitation, we introduce a new fairness definition for generative models, termed as equalized generative treatment (EGT), which requires comparable generation quality across all sensitive groups, with quality measured via a reference f-divergence. We further analyze the trade-offs induced by EGT, demonstrating that enforcing fairness constraints necessarily couples the overall model quality to that of the most challenging group to approximate. This indicates that a simple yet efficient min-max fine-tuning method should be able to balance f-divergences across sensitive groups to satisfy EGT. We validate this theoretical insight through a set of experiments on both image and text generation tasks. We demonstrate that min-max methods consistently achieve fairer outcomes compared to other approaches from the literature, while maintaining competitive overall performance for both tasks.

</details>


### [415] [LLaDA2.1: Speeding Up Text Diffusion via Token Editing](https://arxiv.org/abs/2602.08676)
*Tiwei Bie,Maosong Cao,Xiang Cao,Bingsen Chen,Fuyuan Chen,Kun Chen,Lun Du,Daozhuo Feng,Haibo Feng,Mingliang Gong,Zhuocheng Gong,Yanmei Gu,Jian Guan,Kaiyuan Guan,Hongliang He,Zenan Huang,Juyong Jiang,Zhonghui Jiang,Zhenzhong Lan,Chengxi Li,Jianguo Li,Zehuan Li,Huabin Liu,Lin Liu,Guoshan Lu,Yuan Lu,Yuxin Ma,Xingyu Mou,Zhenxuan Pan,Kaida Qiu,Yuji Ren,Jianfeng Tan,Yiding Tian,Zian Wang,Lanning Wei,Tao Wu,Yipeng Xing,Wentao Ye,Liangyu Zha,Tianze Zhang,Xiaolu Zhang,Junbo Zhao,Da Zheng,Hao Zhong,Wanli Zhong,Jun Zhou,Junlin Zhou,Liwang Zhu,Muzhi Zhu,Yihong Zhuang*

Main category: cs.LG

TL;DR: 本文推出LLaDA2.1，通过创新解码方案和强化学习框架，实现解码速度和生成质量平衡，发布不同规模模型，在多个基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决LLaDA2.0中解码速度和生成质量难以平衡的问题。

Method: 将Token - to - Token编辑融入传统Mask - to - Token方案，采用联合、可配置的阈值解码方案，实现S模式和Q模式；实施专门针对dLLMs的大规模强化学习框架。

Result: LLaDA2.1在33个严格基准测试中表现良好，解码速度快，如在编码任务中有高TPS表现。

Conclusion: LLaDA2.1实现了解码速度和生成质量的平衡，缩小了扩散动力学与人类复杂意图之间的差距。

Abstract: While LLaDA2.0 showcased the scaling potential of 100B-level block-diffusion models and their inherent parallelization, the delicate equilibrium between decoding speed and generation quality has remained an elusive frontier. Today, we unveil LLaDA2.1, a paradigm shift designed to transcend this trade-off. By seamlessly weaving Token-to-Token (T2T) editing into the conventional Mask-to-Token (M2T) scheme, we introduce a joint, configurable threshold-decoding scheme. This structural innovation gives rise to two distinct personas: the Speedy Mode (S Mode), which audaciously lowers the M2T threshold to bypass traditional constraints while relying on T2T to refine the output; and the Quality Mode (Q Mode), which leans into conservative thresholds to secure superior benchmark performances with manageable efficiency degrade. Furthering this evolution, underpinned by an expansive context window, we implement the first large-scale Reinforcement Learning (RL) framework specifically tailored for dLLMs, anchored by specialized techniques for stable gradient estimation. This alignment not only sharpens reasoning precision but also elevates instruction-following fidelity, bridging the chasm between diffusion dynamics and complex human intent. We culminate this work by releasing LLaDA2.1-Mini (16B) and LLaDA2.1-Flash (100B). Across 33 rigorous benchmarks, LLaDA2.1 delivers strong task performance and lightning-fast decoding speed. Despite its 100B volume, on coding tasks it attains an astounding 892 TPS on HumanEval+, 801 TPS on BigCodeBench, and 663 TPS on LiveCodeBench.

</details>


### [416] [Dashed Line Defense: Plug-And-Play Defense Against Adaptive Score-Based Query Attacks](https://arxiv.org/abs/2602.08679)
*Yanzhang Fu,Zizheng Guo,Jizhou Luo*

Main category: cs.LG

TL;DR: 本文揭示现有运行时防御易被自适应攻击绕过，提出虚线防御（DLD）方法，经理论和实验验证其防御效果优于现有方法并能保留模型预测标签。


<details>
  <summary>Details</summary>
Motivation: 现有基于分数的查询攻击对深度学习模型构成威胁，而多数运行时防御方法存在需访问模型参数或易被攻击者策略绕过的问题。

Method: 提出虚线防御（DLD）这一即插即用的后处理方法，通过在观察到的损失与候选示例的真正对抗强度之间引入模糊性，来扰乱对抗样本的生成过程。

Result: 论文提供了 DLD 防御能力的理论保证，并在 ImageNet 上的实验验证，DLD 在存在自适应攻击的情况下也始终优于先前的防御方法。

Conclusion: DLD 是一种有效的防御方法，能抵御自适应查询策略，在保护模型的同时保留模型预测标签。

Abstract: Score-based query attacks pose a serious threat to deep learning models by crafting adversarial examples (AEs) using only black-box access to model output scores, iteratively optimizing inputs based on observed loss values. While recent runtime defenses attempt to disrupt this process via output perturbation, most either require access to model parameters or fail when attackers adapt their tactics. In this paper, we first reveal that even the state-of-the-art plug-and-play defense can be bypassed by adaptive attacks, exposing a critical limitation of existing runtime defenses. We then propose Dashed Line Defense (DLD), a plug-and-play post-processing method specifically designed to withstand adaptive query strategies. By introducing ambiguity in how the observed loss reflects the true adversarial strength of candidate examples, DLD prevents attackers from reliably analyzing and adapting their queries, effectively disrupting the AE generation process. We provide theoretical guarantees of DLD's defense capability and validate its effectiveness through experiments on ImageNet, demonstrating that DLD consistently outperforms prior defenses--even under worst-case adaptive attacks--while preserving the model's predicted labels.

</details>


### [417] [CompilerKV: Risk-Adaptive KV Compression via Offline Experience Compilation](https://arxiv.org/abs/2602.08686)
*Ning Yang,Chengzhi Wang,Yibo Liu,Baoliang Tian,Haijun Zhang*

Main category: cs.LG

TL;DR: 提出CompilerKV框架解决大语言模型长上下文场景下KV缓存内存问题，实验表现优于SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 现有KV压缩方法在内存预算紧张时，忽略提示依赖的压缩风险变化和注意力头功能异质性，导致令牌选择不稳定和尾部故障。

Method: 提出CompilerKV框架，包括通过离线上下文多臂老虎机学习的头部异质性表和风险自适应阈值门控机制。

Result: 在LongBench实验中，在512令牌预算下，CompilerKV优于SOTA方法，恢复了FullKV 97.7%的性能，比最强竞争对手提高了5.2个点。

Conclusion: CompilerKV能有效解决长上下文场景下KV缓存内存问题，提升性能。

Abstract: Large Language Models (LLMs) in long-context scenarios are severely constrained by the linear growth of Key-Value (KV) cache memory. Existing KV compression methods rely either on static thresholds and attention-only heuristics or on coarse memory budget allocation. Under tight memory budgets, these methods overlook two key factors: prompt-dependent variation in compression risk and functional heterogeneity across attention heads, which destabilize token selection and lead to tail failures. To address these challenges, we propose CompilerKV, a risk-adaptive and head-aware compression framework that compiles offline experience into reusable decision tables for prefill-only deployment. CompilerKV integrates two key synergistic components: (i) a Head Heterogeneity Table, learned via offline contextual bandits, which assigns head-specific reliability weights to govern functional differences across attention heads explicitly; and (ii) a Risk-Adaptive Threshold Gating mechanism that jointly models attention entropy and local perplexity, transforming prompt-level risk into deployable retention thresholds. Experiments on LongBench show CompilerKV dominates SOTA methods under a 512-token budget, recovering 97.7\% of FullKV performance while achieving up to +5.2 points gain over the strongest competitor.

</details>


### [418] [Learning To Sample From Diffusion Models Via Inverse Reinforcement Learning](https://arxiv.org/abs/2602.08689)
*Constant Bourdrez,Alexandre Vérine,Olivier Cappé*

Main category: cs.LG

TL;DR: 本文引入逆强化学习框架学习采样策略，无需重新训练去噪器，能提升预训练扩散模型样本质量并自动调整采样超参数。


<details>
  <summary>Details</summary>
Motivation: 训练去噪器计算成本高，利用采样过程的灵活性改进样本质量和采样效率。

Method: 将扩散采样过程表述为离散时间有限水平马尔可夫决策过程，用策略梯度技术直接匹配采样器预期的目标行为，避免定义显式奖励函数。

Result: 实验证明该方法能改善预训练扩散模型生成样本的质量并自动调整采样超参数。

Conclusion: 所提出的逆强化学习框架可有效改进扩散模型的采样过程。

Abstract: Diffusion models generate samples through an iterative denoising process, guided by a neural network. While training the denoiser on real-world data is computationally demanding, the sampling procedure itself is more flexible. This adaptability serves as a key lever in practice, enabling improvements in both the quality of generated samples and the efficiency of the sampling process. In this work, we introduce an inverse reinforcement learning framework for learning sampling strategies without retraining the denoiser. We formulate the diffusion sampling procedure as a discrete-time finite-horizon Markov Decision Process, where actions correspond to optional modifications of the sampling dynamics. To optimize action scheduling, we avoid defining an explicit reward function. Instead, we directly match the target behavior expected from the sampler using policy gradient techniques. We provide experimental evidence that this approach can improve the quality of samples generated by pretrained diffusion models and automatically tune sampling hyperparameters.

</details>


### [419] [SoK: The Pitfalls of Deep Reinforcement Learning for Cybersecurity](https://arxiv.org/abs/2602.08690)
*Shae McFadden,Myles Foley,Elizabeth Bates,Ilias Tsingenopoulos,Sanyam Vyas,Vasilios Mavroudis,Chris Hicks,Fabio Pierazzi*

Main category: cs.LG

TL;DR: 文章指出DRL应用于网络安全存在问题，梳理11个方法陷阱，分析论文量化其普遍程度，通过实验证明影响并给出建议。


<details>
  <summary>Details</summary>
Motivation: DRL在序贯决策领域成功，但应用到网络安全从实验室模拟到定制环境有诸多问题，且网络安全任务性质加剧此情况，需解决应用问题。

Method: 识别并系统化11个方法陷阱，分析66篇重要DRL4Sec论文量化陷阱普遍程度，在三个环境做控制实验。

Result: 每篇论文平均有超五个陷阱，通过实验证明陷阱的实际影响。

Conclusion: 为每个陷阱提供可行建议，以支持更严谨、可部署的基于DRL的安全系统开发。

Abstract: Deep Reinforcement Learning (DRL) has achieved remarkable success in domains requiring sequential decision-making, motivating its application to cybersecurity problems. However, transitioning DRL from laboratory simulations to bespoke cyber environments can introduce numerous issues. This is further exacerbated by the often adversarial, non-stationary, and partially-observable nature of most cybersecurity tasks. In this paper, we identify and systematize 11 methodological pitfalls that frequently occur in DRL for cybersecurity (DRL4Sec) literature across the stages of environment modeling, agent training, performance evaluation, and system deployment. By analyzing 66 significant DRL4Sec papers (2018-2025), we quantify the prevalence of each pitfall and find an average of over five pitfalls per paper. We demonstrate the practical impact of these pitfalls using controlled experiments in (i) autonomous cyber defense, (ii) adversarial malware creation, and (iii) web security testing environments. Finally, we provide actionable recommendations for each pitfall to support the development of more rigorous and deployable DRL-based security systems.

</details>


### [420] [Reasoning aligns language models to human cognition](https://arxiv.org/abs/2602.08693)
*Gonçalo Guiomar,Elia Torre,Pehuen Moure,Victoria Shavina,Mario Giulianelli,Shih-Chii Liu,Valerio Mante*

Main category: cs.LG

TL;DR: 研究语言模型在不确定下的决策是否像人类，及思维链推理的作用，发现扩展推理是高性能关键，用机制模型解释差异。


<details>
  <summary>Details</summary>
Motivation: 探究语言模型在不确定下的决策方式及思维链推理在决策过程中的作用。

Method: 引入主动概率推理任务将采样和推理分开，对比人类和多种大语言模型与近最优参考策略，拟合机制模型。

Result: 扩展推理是高性能关键，在推理上有大提升，使信念轨迹类似人类，在主动采样上提升有限；机制模型可将人类和模型置于共享认知空间，重现行为特征。

Conclusion: 思维链使语言模型在证据积累和信念 - 选择映射上接近人类，推理更对齐，但信息获取仍有差距。

Abstract: Do language models make decisions under uncertainty like humans do, and what role does chain-of-thought (CoT) reasoning play in the underlying decision process? We introduce an active probabilistic reasoning task that cleanly separates sampling (actively acquiring evidence) from inference (integrating evidence toward a decision). Benchmarking humans and a broad set of contemporary large language models against near-optimal reference policies reveals a consistent pattern: extended reasoning is the key determinant of strong performance, driving large gains in inference and producing belief trajectories that become strikingly human-like, while yielding only modest improvements in active sampling. To explain these differences, we fit a mechanistic model that captures systematic deviations from optimal behavior via four interpretable latent variables: memory, strategy, choice bias, and occlusion awareness. This model places humans and models in a shared low-dimensional cognitive space, reproduces behavioral signatures across agents, and shows how chain-of-thought shifts language models toward human-like regimes of evidence accumulation and belief-to-choice mapping, tightening alignment in inference while leaving a persistent gap in information acquisition.

</details>


### [421] [Trapped by simplicity: When Transformers fail to learn from noisy features](https://arxiv.org/abs/2602.08695)
*Evan Peters,Ando Deng,Matheus H. Zambianco,Devin Blankespoor,Achim Kempf*

Main category: cs.LG

TL;DR: 探讨transformer在含噪声数据训练下的噪声鲁棒学习能力，结果显示其在部分任务成功但对随机k - juntas函数通常失败，且学习布尔函数效果不佳。


<details>
  <summary>Details</summary>
Motivation: 研究用含噪声数据训练的大语言模型能否正确泛化到无噪声输入，即探索transformer的噪声鲁棒学习能力。

Method: 对比transformer和LSTM在选择的k - sparse parity和majority函数上的噪声鲁棒学习表现，研究transformer在随机k - juntas函数上的情况，通过利用其对简单函数的偏好设陷阱并测试额外损失项的作用。

Result: transformer在部分函数上能成功进行噪声鲁棒学习，优于LSTM；但在随机k - juntas函数上通常失败；可通过额外损失项摆脱错误解陷阱。

Conclusion: transformer在特征噪声存在时学习布尔函数特别低效。

Abstract: Noise is ubiquitous in data used to train large language models, but it is not well understood whether these models are able to correctly generalize to inputs generated without noise. Here, we study noise-robust learning: are transformers trained on data with noisy features able to find a target function that correctly predicts labels for noiseless features? We show that transformers succeed at noise-robust learning for a selection of $k$-sparse parity and majority functions, compared to LSTMs which fail at this task for even modest feature noise. However, we find that transformers typically fail at noise-robust learning of random $k$-juntas, especially when the boolean sensitivity of the optimal solution is smaller than that of the target function. We argue that this failure is due to a combination of two factors: transformers' bias toward simpler functions, combined with an observation that the optimal function for noise-robust learning typically has lower sensitivity than the target function for random boolean functions. We test this hypothesis by exploiting transformers' simplicity bias to trap them in an incorrect solution, but show that transformers can escape this trap by training with an additional loss term penalizing high-sensitivity solutions. Overall, we find that transformers are particularly ineffective for learning boolean functions in the presence of feature noise.

</details>


### [422] [QUOKA: Query-Oriented KV Selection For Efficient LLM Prefill](https://arxiv.org/abs/2602.08722)
*Dalton Jones,Junyoung Park,Matthew Morse,Mingu Lee,Chris Lott,Harper Langston*

Main category: cs.LG

TL;DR: 提出QUOKA算法加速transformer推理，在多数据集实验中实现时间和速度提升，且准确率接近基线。


<details>
  <summary>Details</summary>
Motivation: 加速transformer在chunked prefill下的推理，利用查询和键的特性来优化注意力计算。

Method: 优先处理低余弦相似度查询，先保留一组代表性查询，再选择与这些查询最匹配的键。

Result: 在多个数据集实验中，实现首token时间3倍减少，在Nvidia GPU上注意力计算加速5倍，在Intel Xeon CPU上近7倍加速，每次注意力评估使用的键值对减少88%。

Conclusion: QUOKA算法在加速推理的同时能达到接近基线的准确率。

Abstract: We present QUOKA: Query-oriented KV selection for efficient attention, a training-free and hardware agnostic sparse attention algorithm for accelerating transformer inference under chunked prefill. While many queries focus on a smaller group of keys in the attention operator, we observe that queries with low cosine similarity with respect to the mean query interact more strongly with more keys and have the greatest contribution to final attention logits. By prioritizing these low cosine similarity queries, the behavior of full attention during the prefill stage can be closely approximated. QUOKA leverages this observation, accelerating attention by (1) first retaining a small set of representative queries and (2) then subselectin the keys most aligned with those queries. Through experiments on Needle-In-A-Haystack, LongBench, RULER, and Math500, we show that, while realizing a 3x reduction in time-to-first-token, 5x speedup in attention on Nvidia GPUs and up to nearly a 7x speedup on Intel Xeon CPUs, QUOKA achieves near-baseline accuracy, utilizing 88% fewer key-value pairs per attention evaluation.

</details>


### [423] [Foundation Inference Models for Ordinary Differential Equations](https://arxiv.org/abs/2602.08733)
*Maximilian Mauel,Johannes R. Hübers,David Berghaus,Patrick Seifner,Ramses J. Sanchez*

Main category: cs.LG

TL;DR: 提出FIM - ODE模型用于低维ODE推理，可单步预测向量场，零样本性能强，预训练利于微调。


<details>
  <summary>Details</summary>
Motivation: 当前从噪声轨迹推断常微分方程（ODE）向量场的方法存在需要复杂训练流程、大量机器学习专业知识或依赖特定系统先验知识的问题。

Method: 提出FIM - ODE预训练基础推理模型，在低阶多项式向量场的ODE先验分布上预训练，用神经算子表示目标场。

Result: FIM - ODE零样本性能强，在多个场景下匹配并常优于ODEFormer；预训练为微调提供强初始化，快速稳定适应且优于现代神经和GP基线。

Conclusion: FIM - ODE是一种有效的低维ODE推理模型，无需大量机器学习专业知识。

Abstract: Ordinary differential equations (ODEs) are central to scientific modelling, but inferring their vector fields from noisy trajectories remains challenging. Current approaches such as symbolic regression, Gaussian process (GP) regression, and Neural ODEs often require complex training pipelines and substantial machine learning expertise, or they depend strongly on system-specific prior knowledge. We propose FIM-ODE, a pretrained Foundation Inference Model that amortises low-dimensional ODE inference by predicting the vector field directly from noisy trajectory data in a single forward pass. We pretrain FIM-ODE on a prior distribution over ODEs with low-degree polynomial vector fields and represent the target field with neural operators. FIM-ODE achieves strong zero-shot performance, matching and often improving upon ODEFormer, a recent pretrained symbolic baseline, across a range of regimes despite using a simpler pretraining prior distribution. Pretraining also provides a strong initialisation for finetuning, enabling fast and stable adaptation that outperforms modern neural and GP baselines without requiring machine learning expertise.

</details>


### [424] [On the Expressive Power of GNNs for Boolean Satisfiability](https://arxiv.org/abs/2602.08745)
*Saku Peltonen,Roger Wattenhofer*

Main category: cs.LG

TL;DR: 本文从Weisfeiler - Leman (WL) 测试角度分析GNN解决布尔可满足性（SAT）问题的表达能力，证明全WL层次一般无法区分可满足和不可满足实例，并通过实验研究不同SAT实例的表达需求。


<details>
  <summary>Details</summary>
Motivation: 机器学习解决SAT问题旨在用基于学习的模型替代手工启发式，图神经网络成为主要架构，需分析其解决SAT问题的表达能力。

Method: 从WL测试角度分析GNN表达能力，研究高阶WL下的不可区分性对顺序设置变量的WL - 有界求解器的实际限制，对G4SAT基准的随机实例和国际SAT竞赛的工业实例进行实验。

Result: 全WL层次一般无法区分可满足和不可满足实例；随机实例大多可区分，工业实例通常需要更多表达能力来预测满足赋值。

Conclusion: 不同类型的SAT实例对GNN的表达能力有不同要求，工业实例更具挑战性。

Abstract: Machine learning approaches to solving Boolean Satisfiability (SAT) aim to replace handcrafted heuristics with learning-based models. Graph Neural Networks have emerged as the main architecture for SAT solving, due to the natural graph representation of Boolean formulas. We analyze the expressive power of GNNs for SAT solving through the lens of the Weisfeiler-Leman (WL) test. As our main result, we prove that the full WL hierarchy cannot, in general, distinguish between satisfiable and unsatisfiable instances. We show that indistinguishability under higher-order WL carries over to practical limitations for WL-bounded solvers that set variables sequentially. We further study the expressivity required for several important families of SAT instances, including regular, random and planar instances. To quantify expressivity needs in practice, we conduct experiments on random instances from the G4SAT benchmark and industrial instances from the International SAT Competition. Our results suggest that while random instances are largely distinguishable, industrial instances often require more expressivity to predict a satisfying assignment.

</details>


### [425] [Central Dogma Transformer II: An AI Microscope for Understanding Cellular Regulatory Mechanisms](https://arxiv.org/abs/2602.08751)
*Nobuyuki Ota*

Main category: cs.LG

TL;DR: 提出可解释的生物AI模型CDT-II，展示其架构、应用及成果，建立机制导向AI。


<details>
  <summary>Details</summary>
Motivation: 当前生物AI缺乏可解释性，内部表征与生物关系不对应。

Method: 在架构上模仿中心法则，让每个注意力机制对应特定生物关系。

Result: 应用于K562 CRISPRi数据，预测扰动效应（per - gene mean r = 0.84），无监督恢复GFI1B调控网络等。

Conclusion: CDT-II建立机制导向AI，可揭示调控结构而非仅优化预测。

Abstract: Current biological AI models lack interpretability -- their internal representations do not correspond to biological relationships that
  researchers can examine. Here we present CDT-II, an "AI microscope" whose attention maps are directly interpretable as regulatory structure.
  By mirroring the central dogma in its architecture, each attention mechanism corresponds to a specific biological relationship: DNA
  self-attention for genomic relationships, RNA self-attention for gene co-regulation, and DNA-to-RNA cross-attention for transcriptional
  control. Using only genomic embeddings and raw per-cell expression, CDT-II enables experimental biologists to observe regulatory networks in
  their own data. Applied to K562 CRISPRi data, CDT-II predicts perturbation effects (per-gene mean $r = 0.84$) and recovers the GFI1B
  regulatory network without supervision (6.6-fold enrichment, $P = 3.5 \times 10^{-17}$). Two distinct attention mechanisms converge on an RNA
  processing module ($P = 1 \times 10^{-16}$). CDT-II establishes mechanism-oriented AI as an alternative to task-oriented approaches, revealing
  regulatory structure rather than merely optimizing predictions.

</details>


### [426] [Redundancy-Free View Alignment for Multimodal Human Activity Recognition with Arbitrarily Missing Views](https://arxiv.org/abs/2602.08755)
*Duc-Anh Nguyen,Nhien-An Le-Khac*

Main category: cs.LG

TL;DR: 提出RALIS模型用于多模态多视图学习，结合多视图对比学习和专家混合模块，支持任意视图可用性，在四个数据集上验证性能和灵活性。


<details>
  <summary>Details</summary>
Motivation: 现有多模态多视图学习方法在灵活视图配置上存在困难，本文聚焦人类活动识别场景，旨在解决任意视图组合、数量和异构模态问题。

Method: 提出RALIS模型，结合多视图对比学习与专家混合模块；使用调整后的中心对比损失进行自监督表示学习和视图对齐；采用专家混合模块及专门的负载平衡策略处理对比学习未捕捉到的差异。

Result: RALIS在四个包含惯性和人体姿态模态、视图数量3 - 9的数据集上得到验证。

Conclusion: RALIS模型具有良好的性能和灵活性，能有效应对多模态多视图学习中的灵活视图配置问题。

Abstract: Multimodal multiview learning seeks to integrate information from diverse sources to enhance task performance. Existing approaches often struggle with flexible view configurations, including arbitrary view combinations, numbers of views, and heterogeneous modalities. Focusing on the context of human activity recognition, we propose RALIS, a model that combines multiview contrastive learning with a mixture-of-experts module to support arbitrary view availability during both training and inference. Instead of trying to reconstruct missing views, an adjusted center contrastive loss is used for self-supervised representation learning and view alignment, mitigating the impact of missing views on multiview fusion. This loss formulation allows for the integration of view weights to account for view quality. Additionally, it reduces computational complexity from $O(V^2)$ to $O(V)$, where $V$ is the number of views. To address residual discrepancies not captured by contrastive learning, we employ a mixture-of-experts module with a specialized load balancing strategy, tasked with adapting to arbitrary view combinations. We highlight the geometric relationship among components in our model and how they combine well in the latent space. RALIS is validated on four datasets encompassing inertial and human pose modalities, with the number of views ranging from three to nine, demonstrating its performance and flexibility.

</details>


### [427] [HoGS: Homophily-Oriented Graph Synthesis for Local Differentially Private GNN Training](https://arxiv.org/abs/2602.08762)
*Wen Xu,Zhetao Li,Yong Xiao,Pengpeng Qiao,Mianxiong Dong,Kaoru Ota*

Main category: cs.LG

TL;DR: 本文提出保护图数据隐私的LDP框架HoGS，通过生成合成图训练GNN，实验显示其在训练GNN准确性上优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有局部差分隐私GNN在保护隐私时存在仅保护链接隐私或保护链接和节点特征隐私时效用损失大的问题。

Method: 提出HoGS框架，先在LDP下收集图的链接和特征信息，再利用图数据的同质性分别重构图结构和节点特征。

Result: 在三个真实数据集上实验表明，HoGS在训练GNN准确性上显著优于基线方法。

Conclusion: HoGS能在保护图数据链接和特征隐私的同时，有效减轻LDP对下游GNN训练的负面影响，提升训练准确性。

Abstract: Graph neural networks (GNNs) have demonstrated remarkable performance in various graph-based machine learning tasks by effectively modeling high-order interactions between nodes. However, training GNNs without protection may leak sensitive personal information in graph data, including links and node features. Local differential privacy (LDP) is an advanced technique for protecting data privacy in decentralized networks. Unfortunately, existing local differentially private GNNs either only preserve link privacy or suffer significant utility loss in the process of preserving link and node feature privacy. In this paper, we propose an effective LDP framework, called HoGS, which trains GNNs with link and feature protection by generating a synthetic graph. Concretely, HoGS first collects the link and feature information of the graph under LDP, and then utilizes the phenomenon of homophily in graph data to reconstruct the graph structure and node features separately, thereby effectively mitigating the negative impact of LDP on the downstream GNN training. We theoretically analyze the privacy guarantee of HoGS and conduct experiments using the generated synthetic graph as input to various state-of-the-art GNN architectures. Experimental results on three real-world datasets show that HoGS significantly outperforms baseline methods in the accuracy of training GNNs.

</details>


### [428] [FreqLens: Interpretable Frequency Attribution for Time Series Forecasting](https://arxiv.org/abs/2602.08768)
*Chi-Sheng Chen,Xinyu Zhang,En-Jui Kuo,Guan-Ying Chen,Qiuzhe Xie,Fan Zhang*

Main category: cs.LG

TL;DR: 提出可解释预测框架FreqLens，在交通和天气数据集上表现良好并发现有意义频率。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列预测模型缺乏可解释性，限制其在需要可解释预测的领域应用。

Method: 提出FreqLens框架，包括可学习频率发现（通过sigmoid映射参数化频率基并结合多样性正则化学习）和公理频率归因（满足多个公理，频率归因等价于Shapley值）。

Result: 在交通和天气数据集上取得有竞争力或更优表现，发现有物理意义的频率，如交通数据中的24小时和12小时周期，天气数据中的每周周期。

Conclusion: FreqLens实现了真正的频率级知识发现，且归因质量有正式理论保证。

Abstract: Time series forecasting models often lack interpretability, limiting their adoption in domains requiring explainable predictions. We propose \textsc{FreqLens}, an interpretable forecasting framework that discovers and attributes predictions to learnable frequency components. \textsc{FreqLens} introduces two key innovations: (1) \emph{learnable frequency discovery} -- frequency bases are parameterized via sigmoid mapping and learned from data with diversity regularization, enabling automatic discovery of dominant periodic patterns without domain knowledge; and (2) \emph{axiomatic frequency attribution} -- a theoretically grounded framework that provably satisfies Completeness, Faithfulness, Null-Frequency, and Symmetry axioms, with per-frequency attributions equivalent to Shapley values. On Traffic and Weather datasets, \textsc{FreqLens} achieves competitive or superior performance while discovering physically meaningful frequencies: all 5 independent runs discover the 24-hour daily cycle ($24.6 \pm 0.1$h, 2.5\% error) and 12-hour half-daily cycle ($11.8 \pm 0.1$h, 1.6\% error) on Traffic, and weekly cycles ($10\times$ longer than the input window) on Weather. These results demonstrate genuine frequency-level knowledge discovery with formal theoretical guarantees on attribution quality.

</details>


### [429] [Default Machine Learning Hyperparameters Do Not Provide Informative Initialization for Bayesian Optimization](https://arxiv.org/abs/2602.08774)
*Nicolás Villagrán Prieto,Eduardo C. Garrido-Merchán*

Main category: cs.LG

TL;DR: 研究用库默认超参数初始化贝叶斯优化（BO）是否能加速收敛，经大量实验发现无显著优势，建议依赖数据驱动搜索策略。


<details>
  <summary>Details</summary>
Motivation: 多数BO管道从均匀随机初始化开始，而流行机器学习库的默认超参数值可能包含专家知识，可加速收敛，但这一假设未被充分验证。

Method: 用以库默认值为中心的截断高斯分布抽取的点初始化BO，与均匀随机基线对比，在多个BO后端、模型族和基准数据集上进行实验，通过收敛速度和最终预测质量评估性能，并进行单侧二项式检验。

Result: 所有条件下，默认信息初始化相比纯随机采样无统计学显著优势，p值在0.141到0.908之间；对先验方差的敏感性分析表明早期优势随优化进行消失。

Conclusion: 无证据表明默认超参数为优化编码了有用的方向信息，建议从业者将超参数调整作为模型开发一部分，采用数据驱动搜索策略而非依赖库默认值。

Abstract: Bayesian Optimization (BO) is a standard tool for hyperparameter tuning thanks to its sample efficiency on expensive black-box functions. While most BO pipelines begin with uniform random initialization, default hyperparameter values shipped with popular ML libraries such as scikit-learn encode implicit expert knowledge and could serve as informative starting points that accelerate convergence. This hypothesis, despite its intuitive appeal, has remained largely unexamined. We formalize the idea by initializing BO with points drawn from truncated Gaussian distributions centered at library defaults and compare the resulting trajectories against a uniform-random baseline. We conduct an extensive empirical evaluation spanning three BO back-ends (BoTorch, Optuna, Scikit-Optimize), three model families (Random Forests, Support Vector Machines, Multilayer Perceptrons), and five benchmark datasets covering classification and regression tasks. Performance is assessed through convergence speed and final predictive quality, and statistical significance is determined via one-sided binomial tests. Across all conditions, default-informed initialization yields no statistically significant advantage over purely random sampling, with p-values ranging from 0.141 to 0.908. A sensitivity analysis on the prior variance confirms that, while tighter concentration around the defaults improves early evaluations, this transient benefit vanishes as optimization progresses, leaving final performance unchanged. Our results provide no evidence that default hyperparameters encode useful directional information for optimization. We therefore recommend that practitioners treat hyperparameter tuning as an integral part of model development and favor principled, data-driven search strategies over heuristic reliance on library defaults.

</details>


### [430] [A Graphop Analysis of Graph Neural Networks on Sparse Graphs: Generalization and Universal Approximation](https://arxiv.org/abs/2602.08785)
*Ofek Amran,Tom Gilat,Ron Levie*

Main category: cs.LG

TL;DR: 提出统一方法定义所有大小（稀疏和密集）图空间上的紧致度量，使MPNNs满足Hölder连续性，得到更强大结果。


<details>
  <summary>Details</summary>
Motivation: 以往对消息传递图神经网络（MPNNs）泛化和逼近能力的分析存在局限性，一种适用于无界大小图但仅针对密集图，另一种研究稀疏图时仅包含统一有界大小的图。

Method: 基于并扩展图极限理论的图算子分析方法，定义所有大小图空间上的紧致度量。

Result: 得到比以往更强大的通用逼近定理和泛化界。

Conclusion: 所提出的统一方法有效，能解决以往分析的局限性问题。

Abstract: Generalization and approximation capabilities of message passing graph neural networks (MPNNs) are often studied by defining a compact metric on a space of input graphs under which MPNNs are Hölder continuous. Such analyses are of two varieties: 1) when the metric space includes graphs of unbounded sizes, the theory is only appropriate for dense graphs, and, 2) when studying sparse graphs, the metric space only includes graphs of uniformly bounded size. In this work, we present a unified approach, defining a compact metric on the space of graphs of all sizes, both sparse and dense, under which MPNNs are Hölder continuous. This leads to more powerful universal approximation theorems and generalization bounds than previous works. The theory is based on, and extends, a recent approach to graph limit theory called graphop analysis.

</details>


### [431] [How2Everything: Mining the Web for How-To Procedures to Evaluate and Improve LLMs](https://arxiv.org/abs/2602.08808)
*Yapei Chang,Kyle Lo,Mohit Iyyer,Luca Soldaini*

Main category: cs.LG

TL;DR: 介绍How2Everything框架评估和改进目标条件过程生成，挖掘程序数据集构建评估集，开发评估协议，揭示模型趋势，强化学习提升性能。


<details>
  <summary>Details</summary>
Motivation: 测量和大规模改进现实任务中程序有效性具有挑战性且研究不足。

Method: 引入How2Everything框架，包含How2Mine挖掘程序、构建How2Bench评估集、开发How2Score评估协议，蒸馏模型用于评估，用强化学习提升性能。

Result: How2Bench揭示模型大小和训练阶段的缩放趋势，强化学习使模型在How2Bench上性能提升超10分且无系统回归。

Conclusion: How2Everything表明预训练网络数据可支持大规模能力评估和改进的闭环。

Abstract: Generating step-by-step "how-to" procedures is a key LLM capability: how-to advice is commonly requested in chatbots, and step-by-step planning is critical for reasoning over complex tasks. Yet, measuring and improving procedural validity at scale on real-world tasks remains challenging and understudied. To address this, we introduce How2Everything, a scalable framework to evaluate and improve goal-conditioned procedure generation. Our framework includes How2Mine, which mines 351K procedures from 980K web pages across 14 topics and readily scales to larger corpora. From this pool we build How2Bench, a 7K-example evaluation set balanced across topics. To reliably score model outputs, we develop How2Score, an evaluation protocol that uses an LLM judge to detect whether a generation contains any critical failure that would prevent achieving the goal. For low-cost, reproducible evaluation, we distill a frontier model into an open 8B model, achieving 80.5% agreement with human annotators. How2Bench reveals clear scaling trends across model sizes and training stages, providing signal early in pretraining. Finally, RL using How2Score as a reward improves performance on How2Bench by >10 points across three models without systematic regressions on standard benchmarks, with gains robust to superficial source-document memorization or format compliance. Taken together, How2Everything shows how pretraining web data can support a closed loop of capability evaluation and improvement at scale.

</details>


### [432] [Efficient Deep Learning for Biometrics: Overview, Challenges and Trends in Ear of Frugal AI](https://arxiv.org/abs/2602.08809)
*Karim Haroun,Aya Zitouni,Aicha Zenakhri,Meriem Amel Guessoum,Larbi Boubchir*

Main category: cs.LG

TL;DR: 概述深度学习在生物识别应用中的高效方法，分析挑战、给出分类，讨论评估指标并指明未来研究方向


<details>
  <summary>Details</summary>
Motivation: 深度学习计算需求大，能耗高、碳足迹大，在资源受限设备部署有局限，推动高效深度学习方法研究

Method: 对生物识别应用中的高效深度学习方法做简要调查，分析训练和部署挑战，给出方法分类，讨论评估指标

Result: 得到高效深度学习方法的分类，确定多种评估效率的指标

Conclusion: 提出未来在高效深度学习方法可开展的研究方向

Abstract: Recent advances in deep learning, whether on discriminative or generative tasks have been beneficial for various applications, among which security and defense. However, their increasing computational demands during training and deployment translates directly into high energy consumption. As a consequence, this induces a heavy carbon footprint which hinders their widespread use and scalability, but also a limitation when deployed on resource-constrained edge devices for real-time use. In this paper, we briefly survey efficient deep learning methods for biometric applications. Specifically, we tackle the challenges one might incur when training and deploying deep learning approaches, and provide a taxonomy of the various efficient deep learning families. Additionally, we discuss complementary metrics for evaluating the efficiency of these models such as memory, computation, latency, throughput, and advocate for universal and reproducible metrics for better comparison. Last, we give future research directions to consider.

</details>


### [433] [$\texttt{lrnnx}$: A library for Linear RNNs](https://arxiv.org/abs/2602.08810)
*Karan Bania,Soham Kalburgi,Manit Tanwar,Dhruthi,Aditya Nagarsekar,Harshvardhan Mestha,Naman Chibber,Raj Deshmukh,Anish Sathyanarayanan,Aarush Rathore,Pratham Chheda*

Main category: cs.LG

TL;DR: 介绍了线性循环神经网络（LRNNs）现有实现存在的问题，提出统一软件库lrnnx以改善LRNN研究和应用的可访问性、可重复性和可扩展性，并开源代码。


<details>
  <summary>Details</summary>
Motivation: 现有LRNN实现分散在不同软件框架，使用、比较和扩展LRNN需大量实现工作，影响研究和应用。

Method: 引入统一软件库lrnnx，在通用接口下实现多个现代LRNN架构，提供多层次控制。

Result: 创建了lrnnx库，代码以宽松的MIT许可证开源。

Conclusion: lrnnx有助于提高LRNN研究和应用的可访问性、可重复性和可扩展性。

Abstract: Linear recurrent neural networks (LRNNs) provide a structured approach to sequence modeling that bridges classical linear dynamical systems and modern deep learning, offering both expressive power and theoretical guarantees on stability and trainability. In recent years, multiple LRNN-based architectures have been proposed, each introducing distinct parameterizations, discretization schemes, and implementation constraints. However, existing implementations are fragmented across different software frameworks, often rely on framework-specific optimizations, and in some cases require custom CUDA kernels or lack publicly available code altogether. As a result, using, comparing, or extending LRNNs requires substantial implementation effort. To address this, we introduce $\texttt{lrnnx}$, a unified software library that implements several modern LRNN architectures under a common interface. The library exposes multiple levels of control, allowing users to work directly with core components or higher-level model abstractions. $\texttt{lrnnx}$ aims to improve accessibility, reproducibility, and extensibility of LRNN research and applications. We make our code available under a permissive MIT license.

</details>


### [434] [Robust Policy Optimization to Prevent Catastrophic Forgetting](https://arxiv.org/abs/2602.08813)
*Mahdi Sabbaghi,George Pappas,Adel Javanmard,Hamed Hassani*

Main category: cs.LG

TL;DR: 现有大语言模型多阶段训练存在灾难性遗忘问题，提出FRPO框架解决，能减少安全退化并保留任务性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型多阶段训练中，下游小更新会导致早期学习行为受损，标准RLHF目标无法保证未来适应性的鲁棒性。

Method: 提出Fine - tuning Robust Policy Optimization (FRPO)框架，通过max - min公式确保策略转移下的奖励稳定性，修改GRPO开发算法。

Result: 在多个基础模型和下游微调机制中大幅减少安全退化，保留下游任务性能，在数学RL场景中保留后续微调的准确性。

Conclusion: FRPO框架能有效解决大语言模型多阶段训练中的灾难性遗忘问题。

Abstract: Large language models are commonly trained through multi-stage post-training: first via RLHF, then fine-tuned for other downstream objectives. Yet even small downstream updates can compromise earlier learned behaviors (e.g., safety), exposing a brittleness known as catastrophic forgetting. This suggests standard RLHF objectives do not guarantee robustness to future adaptation. To address it, most prior work designs downstream-time methods to preserve previously learned behaviors. We argue that preventing this requires pre-finetuning robustness: the base policy should avoid brittle high-reward solutions whose reward drops sharply under standard fine-tuning.
  We propose Fine-tuning Robust Policy Optimization (FRPO), a robust RLHF framework that optimizes reward not only at the current policy, but across a KL-bounded neighborhood of policies reachable by downstream adaptation. The key idea is to ensure reward stability under policy shifts via a max-min formulation. By modifying GRPO, we develop an algorithm with no extra computation, and empirically show it substantially reduces safety degradation across multiple base models and downstream fine-tuning regimes (SFT and RL) while preserving downstream task performance. We further study a math-focused RL setting, demonstrating that FRPO preserves accuracy under subsequent fine-tuning.

</details>


### [435] [Kirin: Improving ANN efficiency with SNN Hybridization](https://arxiv.org/abs/2602.08817)
*Chenyu Wang,Zhanglu Yan,Zhi Zhou,Xu Chen,Weng-Fai Wong*

Main category: cs.LG

TL;DR: 论文研究ANN转SNN存在的挑战，提出Kirin方法，实验表明其在特定量化设置下既能保证精度，又能降低能耗和缩短时间步。


<details>
  <summary>Details</summary>
Motivation: ANN能耗高，SNN能源效率高，而ANN到SNN转换过程中量化存在导致系统延迟增加和信息损失与能耗权衡等问题，因此开展研究。

Method: 提出Kirin方法，包括Spike Matrix Hybridization策略来编码低比特参数，用沉默阈值机制调节单脉冲发射时间。

Result: 在W4A4&8量化设置下，Kirin接近FP16精度，能耗降低84.66%，时间步缩短93.75%。

Conclusion: Kirin可实现时间和能源高效且无精度损失的ANN到SNN转换。

Abstract: Artificial neural networks (ANNs), particularly large language models (LLMs), demonstrate powerful inference capabilities but consume substantial energy. Conversely, spiking neural networks (SNNs) exhibit exceptional energy efficiency due to their binary and event-driven characteristics, thus motivating the study of ANN-to-SNN conversion. In this process, quantization plays a pivotal role, mapping LLMs' floating-point parameters to discrete SNN parameters via the temporal dimension of the time window. However, several challenges remain in the conversion process: (i) converting high bit-width quantization values into binary spikes requires longer time windows, increasing system latency; and (ii) the inherent trade-off between the information loss of single-spike schemes and the energy costs of multi-spike ones in SNN. To address these challenges, we propose Kirin, a integer and spike hybrid based SNN to achieve accuracy lossless ANN-to-SNN conversion with time and energy efficiency. Specifically, we first propose a Spike Matrix Hybridization strategy that encoding low bit-width parameters that leading to small time window size into binary spikes while preserving the rest in integer format, thereby reducing the overall latency of SNN execution. Second, we introduce a silence threshold mechanism to regulate the timing of single-spike firing, ensuring the output is mathematically equivalent to the LLM's output and preserves accuracy. Experimental results demonstrate that Kirin, under a W4A4\&8 quantization setting, achieves near-FP16 accuracy while reducing energy consumption by up to 84.66\% and shortening time steps by 93.75\%.

</details>


### [436] [FlexMoRE: A Flexible Mixture of Rank-heterogeneous Experts for Efficient Federatedly-trained Large Language Models](https://arxiv.org/abs/2602.08818)
*Annemette Brok Pirchert,Jacob Nielsen,Mogens Henrik From,Lukas Galke Poech,Peter Schneider-Kamp*

Main category: cs.LG

TL;DR: 文章提出FlexMoRE模型，通过实验研究专家秩与下游任务性能的权衡，发现推理密集型基准和知识密集型基准的最佳秩不同，FlexMoRE在参数大幅减少时性能提升。


<details>
  <summary>Details</summary>
Motivation: 质疑所有领域是否都需要全尺寸专家，提出低秩适配器可能足够的假设。

Method: 引入FlexMoRE模型，评估6个不同秩的专家进行150种混合实验，对FlexOlmo的预训练专家转为低秩版本并进行回归分析。

Result: 推理密集型基准的最佳秩比知识密集型高，FlexMoRE参数不到FlexOlmo三分之一，下游任务平均得分更高。

Conclusion: 可以根据不同类型的下游任务选择合适的专家秩提高内存效率和任务性能。

Abstract: Recent advances in mixture-of-experts architectures have shown that individual experts models can be trained federatedly, i.e., in isolation from other experts by using a common base model to facilitate coordination. However, we hypothesize that full-sized experts may not be necessary for all domains and that instead low-rank adapters may be sufficient. Here, we introduce FlexMoRE, a Flexible Mixture of Rank-heterogenous Experts, which may be either full-sized experts or adapters of a suitable rank. We systematically investigate the trade-off between expert rank and downstream task performance by evaluating $6$ experts with ranks $2^0$ to $2^{14}$ resulting in experiments covering 150 mixtures (96 with 2 experts, 54 with 7 experts) that are evaluated across $120$ tasks. For our experiments, we build on FlexOlmo and turn its pre-trained experts into low-rank versions. Our regression analysis from expert rank to downstream task performance reveals that the best-performing rank is substantially higher for reasoning-heavy benchmarks than for knowledge-heavy benchmarks. These findings on rank sensitivity come with direct implications for memory efficiency: Using optimal ranks, FlexMoRE yields improved downstream task performance (average score $47.18$) compared to the baseline FlexOlmo-style mixture of full-sized experts (average score $45.46$) at less than one third the parameters ($10.75$B for FlexMoRE vs. $33.27$B for FlexOlmo). All code will be made available.

</details>


### [437] [Bayesian Preference Learning for Test-Time Steerable Reward Models](https://arxiv.org/abs/2602.08819)
*Jiwoo Hong,Shao Tang,Zhipeng Wang*

Main category: cs.LG

TL;DR: 提出变分上下文奖励建模（ICRM）方法，使奖励模型在测试时可引导，在单目标和多目标设置中能适应未知偏好分布，提升准确率，在RL训练有实际应用，还有理论保证。


<details>
  <summary>Details</summary>
Motivation: 随着强化学习应用场景变复杂，分类器奖励模型训练后静态，适应性受限，需更灵活的奖励模型。

Method: 提出ICRM，将奖励建模转化为在Bradley - Terry模型下对潜在偏好概率的摊销变分推理，使用共轭Beta先验。

Result: 在单目标设置中，ICRM在SafeRLHF上准确率提升34%，在RM - Bench上提升9%；在多任务上扩展了帕累托前沿；在数学推理中表现优于传统奖励模型。

Conclusion: ICRM能在测试时适应未知偏好分布，可有效编码可验证奖励，变分目标有全局内部最优，KL正则化可缓解奖励过度优化。

Abstract: Reward models are central to aligning language models with human preferences via reinforcement learning (RL). As RL is increasingly applied to settings such as verifiable rewards and multi-objective alignment, RMs are expected to encode more complex and multifaceted preference distributions. However, classifier RMs remain static once trained, limiting their adaptability at test time. We propose Variational In-Context Reward Modeling (ICRM), a novel Bayesian reward modeling objective that enables test-time steerability via in-context preference demonstrations. ICRM casts reward modeling as amortized variational inference over a latent preference probability under the Bradley-Terry model using a conjugate Beta prior. We show that ICRM adapt to unseen preference distributions at test time for both single and multi-objective settings. With more in-context demonstrations, ICRM gains 34% accuracy on SafeRLHF and 9% accuracy on RM-Bench in the single-objective setting, while widening the Pareto frontier with a 4% gain in hypervolume on helpfulness and refusal benchmarks. We further study the practical applicability of ICRM for RL training, showing that it can effectively encode verifiable rewards by outperforming a conventional RM in math reasoning. Finally, we provide theoretical guarantees that the variational objective admits a global interior optimum with finite confidence, and we analyze how KL regularization mitigates reward over-optimization.

</details>


### [438] [Dr. MAS: Stable Reinforcement Learning for Multi-Agent LLM Systems](https://arxiv.org/abs/2602.08847)
*Lang Feng,Longtao Zheng,Shuo He,Fuxiang Zhang,Bo An*

Main category: cs.LG

TL;DR: 本文指出多智能体大语言模型系统基于组的强化学习训练不稳定的原因，提出Dr. MAS训练方法，在多智能体数学推理和多轮搜索基准测试中取得良好效果。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体大语言模型系统可靠的强化学习后训练困难的问题。

Method: 提出Dr. MAS方法，使用按智能体归一化优势的方法校准梯度规模，提供端到端的强化学习训练框架。

Result: 在多智能体数学推理和多轮搜索基准测试中，相比普通GRPO有明显提升，大幅消除梯度峰值，在异构智能体 - 模型分配下仍有效且提高效率。

Conclusion: Dr. MAS是一种简单且稳定的多智能体大语言模型系统强化学习训练方法。

Abstract: Multi-agent LLM systems enable advanced reasoning and tool use via role specialization, yet reliable reinforcement learning (RL) post-training for such systems remains difficult. In this work, we theoretically pinpoint a key reason for training instability when extending group-based RL to multi-agent LLM systems. We show that under GRPO-style optimization, a global normalization baseline may deviate from diverse agents' reward distributions, which ultimately leads to gradient-norm instability. Based on this finding, we propose Dr. MAS, a simple and stable RL training recipe for multi-agent LLM systems. Dr. MAS uses an agent-wise remedy: normalizing advantages per agent using each agent's own reward statistics, which calibrates gradient scales and dramatically stabilizes training, both theoretically and empirically. Beyond the algorithm, Dr. MAS provides an end-to-end RL training framework for multi-agent LLM systems, supporting scalable orchestration, flexible per-agent LLM serving and optimization configs, and shared resource scheduling of LLM actor backends. We evaluate Dr. MAS on multi-agent math reasoning and multi-turn search benchmarks using Qwen2.5 and Qwen3 series models. Dr. MAS achieves clear gains over vanilla GRPO (e.g., +5.6\% avg@16 and +4.6\% pass@16 on math, and +15.2\% avg@16 and +13.1\% pass@16 on search) while largely eliminating gradient spikes. Moreover, it remains highly effective under heterogeneous agent-model assignments while improving efficiency.

</details>


### [439] [Rethinking Graph Generalization through the Lens of Sharpness-Aware Minimization](https://arxiv.org/abs/2602.08855)
*Yang Qiu,Yixiong Zou,Jun Wang*

Main category: cs.LG

TL;DR: 本文聚焦图神经网络对分布偏移敏感问题，分析MSF现象，提出能量驱动生成增强框架E2A提升模型泛化能力，实验效果佳。


<details>
  <summary>Details</summary>
Motivation: 图神经网络在图任务中成功但对分布偏移敏感，MSF现象普遍却少被研究，需解决该现象及相关问题以提升模型泛化能力。

Method: 从SAM角度解读MSF，引入局部鲁棒半径量化损失尖锐度，提出能量公式，构建能量驱动生成增强框架E2A。

Result: E2A在多个基准测试中持续改善图OOD泛化能力，优于现有基线方法。

Conclusion: 所提E2A框架能有效解决MSF现象，提升图神经网络的泛化能力。

Abstract: Graph Neural Networks (GNNs) have achieved remarkable success across various graph-based tasks but remain highly sensitive to distribution shifts. In this work, we focus on a prevalent yet under-explored phenomenon in graph generalization, Minimal Shift Flip (MSF),where test samples that slightly deviate from the training distribution are abruptly misclassified. To interpret this phenomenon, we revisit MSF through the lens of Sharpness-Aware Minimization (SAM), which characterizes the local stability and sharpness of the loss landscape while providing a theoretical foundation for modeling generalization error. To quantify loss sharpness, we introduce the concept of Local Robust Radius, measuring the smallest perturbation required to flip a prediction and establishing a theoretical link between local stability and generalization. Building on this perspective, we further observe a continual decrease in the robust radius during training, indicating weakened local stability and an increasingly sharp loss landscape that gives rise to MSF. To jointly solve the MSF phenomenon and the intractability of radius, we develop an energy-based formulation that is theoretically proven to be monotonically correlated with the robust radius, offering a tractable and principled objective for modeling flatness and stability. Building on these insights, we propose an energy-driven generative augmentation framework (E2A) that leverages energy-guided latent perturbations to generate pseudo-OOD samples and enhance model generalization. Extensive experiments across multiple benchmarks demonstrate that E2A consistently improves graph OOD generalization, outperforming state-of-the-art baselines.

</details>


### [440] [Discovering Interpretable Algorithms by Decompiling Transformers to RASP](https://arxiv.org/abs/2602.08857)
*Xinting Huang,Aleksandra Bakalova,Satwik Bhattamishra,William Merrill,Michael Hahn*

Main category: cs.LG

TL;DR: 本文提出从训练好的Transformer中提取简单可解释RASP程序的方法，实验表明该方法能从长度泛化的Transformer中恢复此类程序。


<details>
  <summary>Details</summary>
Motivation: 此前虽表明Transformers计算可在RASP语言中模拟，但不确定训练模型是否实现简单可解释程序，需找到提取此类程序的方法。

Method: 将Transformer忠实地重新参数化为RASP程序，然后应用因果干预发现小的充分子程序。

Result: 在算法和形式语言任务训练的小型Transformers实验中，该方法常能从长度泛化的Transformer中恢复简单可解释的RASP程序。

Conclusion: 为Transformers内部实现简单RASP程序提供了最直接的证据。

Abstract: Recent work has shown that the computations of Transformers can be simulated in the RASP family of programming languages. These findings have enabled improved understanding of the expressive capacity and generalization abilities of Transformers. In particular, Transformers have been suggested to length-generalize exactly on problems that have simple RASP programs. However, it remains open whether trained models actually implement simple interpretable programs. In this paper, we present a general method to extract such programs from trained Transformers. The idea is to faithfully re-parameterize a Transformer as a RASP program and then apply causal interventions to discover a small sufficient sub-program. In experiments on small Transformers trained on algorithmic and formal language tasks, we show that our method often recovers simple and interpretable RASP programs from length-generalizing transformers. Our results provide the most direct evidence so far that Transformers internally implement simple RASP programs.

</details>


### [441] [Magnitude Distance: A Geometric Measure of Dataset Similarity](https://arxiv.org/abs/2602.08859)
*Sahel Torkamani,Henry Gouk,Rik Sarkar*

Main category: cs.LG

TL;DR: 提出一种新的数据集距离度量——幅度距离，证明其理论性质，展示其在高维环境中的优势并用于生成模型训练，实验支持理论分析。


<details>
  <summary>Details</summary>
Motivation: 解决数学和机器学习中量化数据集间距离的基本问题。

Method: 利用度量空间的幅度概念定义幅度距离，证明其理论性质，将其用作生成模型训练目标。

Result: 幅度距离在高维环境中适当调参时具有判别性，实验表明其能提供有意义信号，与现有基于距离的生成方法相当。

Conclusion: 幅度距离是一种有价值的数据集距离度量，可用于生成模型训练。

Abstract: Quantifying the distance between datasets is a fundamental question in mathematics and machine learning. We propose \textit{magnitude distance}, a novel distance metric defined on finite datasets using the notion of the \emph{magnitude} of a metric space. The proposed distance incorporates a tunable scaling parameter, $t$, that controls the sensitivity to global structure (small $t$) and finer details (large $t$). We prove several theoretical properties of magnitude distance, including its limiting behavior across scales and conditions under which it satisfies key metric properties. In contrast to classical distances, we show that magnitude distance remains discriminative in high-dimensional settings when the scale is appropriately tuned. We further demonstrate how magnitude distance can be used as a training objective for push-forward generative models. Our experimental results support our theoretical analysis and demonstrate that magnitude distance provides meaningful signals, comparable to established distance-based generative approaches.

</details>


### [442] [AnomSeer: Reinforcing Multimodal LLMs to Reason for Time-Series Anomaly Detection](https://arxiv.org/abs/2602.08868)
*Junru Zhang,Lang Feng,Haoran Shi,Xu Guo,Han Yu,Yabo Dong,Duanqing Xu*

Main category: cs.LG

TL;DR: 提出AnomSeer解决MLLMs在TSAD中推理问题，结合经典分析生成推理轨迹，提出TimerPO，表现优于商业基线。


<details>
  <summary>Details</summary>
Motivation: MLLMs在TSAD中依赖粗略启发式方法，缺乏多维度详细推理能力，难以理解复杂时间序列数据。

Method: 生成专家思维链轨迹，提出时间序列接地策略优化（TimerPO），包括基于最优传输的优势和正交投影。

Result: AnomSeer在分类和定位准确性上优于大型商业基线，能产生合理推理轨迹。

Conclusion: AnomSeer有效解决了MLLMs在TSAD中的推理问题，提升了检测性能。

Abstract: Time-series anomaly detection (TSAD) with multimodal large language models (MLLMs) is an emerging area, yet a persistent challenge remains: MLLMs rely on coarse time-series heuristics but struggle with multi-dimensional, detailed reasoning, which is vital for understanding complex time-series data. We present AnomSeer to address this by reinforcing the model to ground its reasoning in precise, structural details of time series, unifying anomaly classification, localization, and explanation. At its core, an expert chain-of-thought trace is generated to provide a verifiable, fine-grained reasoning from classical analyses (e.g., statistical measures, frequency transforms). Building on this, we propose a novel time-series grounded policy optimization (TimerPO) that incorporates two additional components beyond standard reinforcement learning: a time-series grounded advantage based on optimal transport and an orthogonal projection to ensure this auxiliary granular signal does not interfere with the primary detection objective. Across diverse anomaly scenarios, AnomSeer, with Qwen2.5-VL-3B/7B-Instruct, outperforms larger commercial baselines (e.g., GPT-4o) in classification and localization accuracy, particularly on point- and frequency-driven exceptions. Moreover, it produces plausible time-series reasoning traces that support its conclusions.

</details>


### [443] [Stress-Testing Alignment Audits With Prompt-Level Strategic Deception](https://arxiv.org/abs/2602.08877)
*Oliver Daniels,Perusha Moodley,Ben Marlin,David Lindner*

Main category: cs.LG

TL;DR: 现有对齐审计方法未针对欺骗策略进行系统压力测试，本文实现自动红队管道生成欺骗策略，测试发现可欺骗黑白盒方法，表明当前方法对强大的未对齐模型可能不稳健。


<details>
  <summary>Details</summary>
Motivation: 现有审计方法未系统地针对欺骗策略进行压力测试。

Method: 实现自动红队管道，生成针对特定黑白盒审计方法的欺骗策略，对多种方法进行压力测试。

Result: 找到能使黑白盒方法做出自信但错误猜测的提示，提供了基于激活的战略欺骗的首份记录证据。

Conclusion: 当前的黑白盒方法对足够强大的未对齐模型可能不具鲁棒性。

Abstract: Alignment audits aim to robustly identify hidden goals from strategic, situationally aware misaligned models. Despite this threat model, existing auditing methods have not been systematically stress-tested against deception strategies. We address this gap, implementing an automatic red-team pipeline that generates deception strategies (in the form of system prompts) tailored to specific white-box and black-box auditing methods. Stress-testing assistant prefills, user persona sampling, sparse autoencoders, and token embedding similarity methods against secret-keeping model organisms, our automatic red-team pipeline finds prompts that deceive both the black-box and white-box methods into confident, incorrect guesses. Our results provide the first documented evidence of activation-based strategic deception, and suggest that current black-box and white-box methods would not be robust to a sufficiently capable misaligned model.

</details>


### [444] [Learning Potentials for Dynamic Matching and Application to Heart Transplantation](https://arxiv.org/abs/2602.08878)
*Itai Zilberstein,Ioannis Anagnostides,Zachary W. Sollie,Arman Kilic,Tuomas Sandholm*

Main category: cs.LG

TL;DR: 提出非近视策略优化框架用于心脏移植分配，用历史数据验证优于现有方法，为器官分配提供新路径。


<details>
  <summary>Details</summary>
Motivation: 当前心脏移植分配政策未考虑器官动态到达和候选者构成，效率低，美国正转向数据驱动模型。

Method: 提出基于势的非近视策略优化框架，采用自监督模仿学习训练势，使其模仿有完美预见的全知算法。

Result: 用真实历史数据表明该策略在优化总体结果上显著优于现有方法。

Conclusion: 提出了可扩展且理论可靠的更有效器官分配路径。

Abstract: Each year, thousands of patients in need of heart transplants face life-threatening wait times due to organ scarcity. While allocation policies aim to maximize population-level outcomes, current approaches often fail to account for the dynamic arrival of organs and the composition of waitlisted candidates, thereby hampering efficiency. The United States is transitioning from rigid, rule-based allocation to more flexible data-driven models. In this paper, we propose a novel framework for non-myopic policy optimization in general online matching relying on potentials, a concept originally introduced for kidney exchange. We develop scalable and accurate ways of learning potentials that are higher-dimensional and more expressive than prior approaches. Our approach is a form of self-supervised imitation learning: the potentials are trained to mimic an omniscient algorithm that has perfect foresight. We focus on the application of heart transplant allocation and demonstrate, using real historical data, that our policies significantly outperform prior approaches -- including the current US status quo policy and the proposed continuous distribution framework -- in optimizing for population-level outcomes. Our analysis and methods come at a pivotal moment in US policy, as the current heart transplant allocation system is under review. We propose a scalable and theoretically grounded path toward more effective organ allocation.

</details>


### [445] [Breaking the Simplification Bottleneck in Amortized Neural Symbolic Regression](https://arxiv.org/abs/2602.08885)
*Paul Saegert,Ullrich Köthe*

Main category: cs.LG

TL;DR: 提出SimpliPy简化引擎，加速摊销符号回归（SR），Flash - ANSR框架在FastSRB基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前摊销SR难以扩展到实际科学复杂性，主要障碍是缺乏等效表达式的快速简化，通用CAS计算成本高限制速度。

Method: 提出基于规则的简化引擎SimpliPy，构建Flash - ANSR框架。

Result: SimpliPy比SymPy提速100倍，Flash - ANSR框架在FastSRB基准测试中比摊销基线精度更高，与最先进的直接优化方法表现相当。

Conclusion: SimpliPy能提升摊销SR性能，Flash - ANSR框架有良好表现且能恢复更简洁表达式。

Abstract: Symbolic regression (SR) aims to discover interpretable analytical expressions that accurately describe observed data. Amortized SR promises to be much more efficient than the predominant genetic programming SR methods, but currently struggles to scale to realistic scientific complexity. We find that a key obstacle is the lack of a fast reduction of equivalent expressions to a concise normalized form. Amortized SR has addressed this by general-purpose Computer Algebra Systems (CAS) like SymPy, but the high computational cost severely limits training and inference speed. We propose SimpliPy, a rule-based simplification engine achieving a 100-fold speed-up over SymPy at comparable quality. This enables substantial improvements in amortized SR, including scalability to much larger training sets, more efficient use of the per-expression token budget, and systematic training set decontamination with respect to equivalent test expressions. We demonstrate these advantages in our Flash-ANSR framework, which achieves much better accuracy than amortized baselines (NeSymReS, E2E) on the FastSRB benchmark. Moreover, it performs on par with state-of-the-art direct optimization (PySR) while recovering more concise instead of more complex expressions with increasing inference budget.

</details>


### [446] [Discrete Bridges for Mutual Information Estimation](https://arxiv.org/abs/2602.08894)
*Iryna Zabarianska,Sergei Kholkin,Grigoriy Ksenofontov,Ivan Butakov,Alexander Korotin*

Main category: cs.LG

TL;DR: 利用离散状态空间的桥匹配模型解决离散随机变量互信息估计问题，构建DBMI估计器并展示其在两种设置下的性能。


<details>
  <summary>Details</summary>
Motivation: 使用离散状态空间的扩散桥模型解决机器学习和信息论中离散随机变量互信息估计的重要问题。

Method: 将互信息估计巧妙构建为领域转移问题，构建适用于离散数据的离散桥互信息（DBMI）估计器。

Result: 展示了估计器在低维和基于图像的两种互信息估计设置下的性能。

Conclusion: 所构建的DBMI估计器可用于解决离散数据的互信息估计难题。

Abstract: Diffusion bridge models in both continuous and discrete state spaces have recently become powerful tools in the field of generative modeling. In this work, we leverage the discrete state space formulation of bridge matching models to address another important problem in machine learning and information theory: the estimation of the mutual information (MI) between discrete random variables. By neatly framing MI estimation as a domain transfer problem, we construct a Discrete Bridge Mutual Information (DBMI) estimator suitable for discrete data, which poses difficulties for conventional MI estimators. We showcase the performance of our estimator on two MI estimation settings: low-dimensional and image-based.

</details>


### [447] [GSS: Gated Subspace Steering for Selective Memorization Mitigation in LLMs](https://arxiv.org/abs/2602.08901)
*Xuanqi Zhang,Haoyang Shang,Xiaoxiao Li*

Main category: cs.LG

TL;DR: 现有大语言模型记忆问题影响泛化和隐私，传统方法有缺陷，本文提出GSS方法减少记忆，实验效果好且计算量低，还有理论新见解。


<details>
  <summary>Details</summary>
Motivation: 大语言模型存在逐字记忆训练序列问题，影响泛化和隐私，且现有缓解方法会降低正常泛化的多数标记的性能。

Method: 提出Gated Subspace Steering (GSS)方法，将干预分解为探测和引导两部分，通过基于最优子空间引导的优化框架得到最优探测 - 引导对。

Result: 在四个基准测试中，GSS减少记忆的效果与或超过了现有最优水平，且计算量比基于优化的替代方法低100 - 1000倍。

Conclusion: 有效减少大语言模型的记忆问题，同时提供了关于神经表征中记忆几何的新理论见解。

Abstract: Large language models (LLMs) can memorize and reproduce training sequences verbatim -- a tendency that undermines both generalization and privacy. Existing mitigation methods apply interventions uniformly, degrading performance on the majority of tokens that generalize normally. We show empirically that memorization is sparse, intermittent, and token-conditioned, suggesting that effective mitigation requires context-aware intervention rather than static parameter modification. To this end, we propose a novel and effective selective memorization mitigation method -- Gated Subspace Steering (GSS), which decomposes intervention into a probe (detecting memorization-relevant activations) and a steer (applying targeted correction only when the probe exceeds a threshold). The optimal probe-steer pair emerges from a principled optimization framework based on optimal subspace steering. Experiments on four benchmarks show GSS matches or exceeds state-of-the-art memorization reduction while requiring $100-1000 \times$ less compute than optimization-based alternatives. Furthermore, we provide new theoretical insights into the geometry of memorization in neural representations.

</details>


### [448] [Diffusion-Inspired Reconfiguration of Transformers for Uncertainty Calibration](https://arxiv.org/abs/2602.08920)
*Manh Cuong Dao,Quang Hung Pham,Phi Le Nguyen,Thao Nguyen Truong,Bryan Kian Hsiang Low,Trong Nghia Hoang*

Main category: cs.LG

TL;DR: 提出一种受扩散启发的预训练transformer重构方法，能在保持性能的同时实现表示不确定性的原则性传播，在多个基准测试中表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有预训练transformer大多缺乏特征转换栈中不确定性传播的原则性机制，需要解决预训练transformer在风险敏感应用中可靠部署的不确定性校准问题。

Method: 将每个特征转换块建模为概率映射，组合这些映射得到类似扩散过程的概率路径，在统一过渡模型的扩散过程中重新编译该路径。

Result: 在多种视觉和语言基准测试中，该方法比现有不确定性感知transformer实现了更好的校准和预测准确性。

Conclusion: 所提出的方法能在保持预训练模型原始预测性能的同时，实现表示不确定性在其架构中的原则性传播。

Abstract: Uncertainty calibration in pre-trained transformers is critical for their reliable deployment in risk-sensitive applications. Yet, most existing pre-trained transformers do not have a principled mechanism for uncertainty propagation through their feature transformation stack. In this work, we propose a diffusion-inspired reconfiguration of transformers in which each feature transformation block is modeled as a probabilistic mapping. Composing these probabilistic mappings reveals a probability path that mimics the structure of a diffusion process, transporting data mass from the input distribution to the pre-trained feature distribution. This probability path can then be recompiled on a diffusion process with a unified transition model to enable principled propagation of representation uncertainty throughout the pre-trained model's architecture while maintaining its original predictive performance. Empirical results across a variety of vision and language benchmarks demonstrate that our method achieves superior calibration and predictive accuracy compared to existing uncertainty-aware transformers.

</details>


### [449] [StealthRL: Reinforcement Learning Paraphrase Attacks for Multi-Detector Evasion of AI-Text Detectors](https://arxiv.org/abs/2602.08934)
*Suraj Ranganath,Atharv Ramesh*

Main category: cs.LG

TL;DR: 介绍StealthRL强化学习框架来压力测试AI文本检测器鲁棒性，取得近零检测等成果并暴露当前检测方法有鲁棒性问题。


<details>
  <summary>Details</summary>
Motivation: AI文本检测器面临保留语义的对抗性释义攻击的鲁棒性挑战，需要工具来测试其鲁棒性。

Method: 使用Group Relative Policy Optimization (GRPO)和LoRA适配器在Qwen3 - 4B上训练释义策略，针对多检测器集成进行优化复合奖励。评估六种攻击设置对三种检测器族。

Result: StealthRL实现近零检测，降低平均AUROC，攻击成功率达99.9%，攻击可迁移到未参与训练的检测器族。

Conclusion: 当前AI文本检测存在显著鲁棒性差距，StealthRL是一种有效的对抗评估协议。

Abstract: AI-text detectors face a critical robustness challenge: adversarial paraphrasing attacks that preserve semantics while evading detection. We introduce StealthRL, a reinforcement learning framework that stress-tests detector robustness under realistic adversarial conditions. StealthRL trains a paraphrase policy against a multi-detector ensemble using Group Relative Policy Optimization (GRPO) with LoRA adapters on Qwen3-4B, optimizing a composite reward that balances detector evasion with semantic preservation. We evaluate six attack settings (M0-M5) against three detector families (RoBERTa, FastDetectGPT, and Binoculars) at the security-relevant 1% false positive rate operating point. StealthRL achieves near-zero detection (0.001 mean TPR@1%FPR), reduces mean AUROC from 0.74 to 0.27, and attains a 99.9% attack success rate. Critically, attacks transfer to a held-out detector family not seen during training, revealing shared architectural vulnerabilities rather than detector-specific brittleness. We additionally conduct LLM-based quality evaluation via Likert scoring, analyze detector score distributions to explain why evasion succeeds, and provide per-detector AUROC with bootstrap confidence intervals. Our results expose significant robustness gaps in current AI-text detection and establish StealthRL as a principled adversarial evaluation protocol. Code and evaluation pipeline are publicly available at https://github.com/suraj-ranganath/StealthRL.

</details>


### [450] [A Behavioural and Representational Evaluation of Goal-Directedness in Language Model Agents](https://arxiv.org/abs/2602.08964)
*Raghu Arghal,Fade Chen,Niall Dalton,Evgenii Kortukov,Calum McNamara,Angelos Nalmpantis,Moksh Nirvaan,Gabriele Sarti,Mario Giulianelli*

Main category: cs.LG

TL;DR: 提出评估目标导向性框架，以LLM代理为例研究，发现需内省检查补充行为评估来理解代理目标追求。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏可靠的将目标归因于智能体系统的方法，需要评估目标导向性。

Method: 提出结合行为评估和模型内部表征可解释性分析的框架，以LLM代理在2D网格世界导航为案例，进行行为评估和内部表征探测。

Result: 代理性能随任务难度变化，能对环境状态和行动规划进行编码，行动与内部表征一致，推理会重组表征。

Conclusion: 除行为评估外，需内省检查来表征智能体如何表示和追求目标。

Abstract: Understanding an agent's goals helps explain and predict its behaviour, yet there is no established methodology for reliably attributing goals to agentic systems. We propose a framework for evaluating goal-directedness that integrates behavioural evaluation with interpretability-based analyses of models' internal representations. As a case study, we examine an LLM agent navigating a 2D grid world toward a goal state. Behaviourally, we evaluate the agent against an optimal policy across varying grid sizes, obstacle densities, and goal structures, finding that performance scales with task difficulty while remaining robust to difficulty-preserving transformations and complex goal structures. We then use probing methods to decode the agent's internal representations of the environment state and its multi-step action plans. We find that the LLM agent non-linearly encodes a coarse spatial map of the environment, preserving approximate task-relevant cues about its position and the goal location; that its actions are broadly consistent with these internal representations; and that reasoning reorganises them, shifting from broader environment structural cues toward information supporting immediate action selection. Our findings support the view that introspective examination is required beyond behavioural evaluations to characterise how agents represent and pursue their objectives.

</details>


### [451] [Distributionally Robust Optimization via Generative Ambiguity Modeling](https://arxiv.org/abs/2602.08976)
*Jiaqi Wen,Jianyi Yang*

Main category: cs.LG

TL;DR: 本文研究分布鲁棒优化（DRO），提出基于生成模型的模糊集和GAS - DRO算法，证明其收敛性并验证OOD泛化性能。


<details>
  <summary>Details</summary>
Motivation: 寻找有效的DRO模糊集，使分布与名义分布一致、能涵盖多种场景且求解易处理。

Method: 提出基于生成模型的模糊集，构建GAS - DRO算法，在参数化生成模型空间求解内部最大化问题。

Result: 正式确立GAS - DRO的平稳收敛性能，用扩散模型实现该算法，在机器学习任务中展现出优越的OOD泛化性能。

Conclusion: 基于生成模型的模糊集和GAS - DRO算法在DRO中有效，能提升OOD泛化能力。

Abstract: This paper studies Distributionally Robust Optimization (DRO), a fundamental framework for enhancing the robustness and generalization of statistical learning and optimization. An effective ambiguity set for DRO must involve distributions that remain consistent to the nominal distribution while being diverse enough to account for a variety of potential scenarios. Moreover, it should lead to tractable DRO solutions. To this end, we propose generative model-based ambiguity sets that capture various adversarial distributions beyond the nominal support space while maintaining consistency with the nominal distribution. Building on this generative ambiguity modeling, we propose DRO with Generative Ambiguity Set (GAS-DRO), a tractable DRO algorithm that solves the inner maximization over the parameterized generative model space. We formally establish the stationary convergence performance of GAS-DRO. We implement GAS-DRO with a diffusion model and empirically demonstrate its superior Out-of-Distribution (OOD) generalization performance in ML tasks.

</details>


### [452] [StretchTime: Adaptive Time Series Forecasting via Symplectic Attention](https://arxiv.org/abs/2602.08983)
*Yubin Kim,Viresh Pati,Jevon Twitty,Vinh Pham,Shihao Yang,Jiecheng Lu*

Main category: cs.LG

TL;DR: 现有Transformer时间序列预测依赖假设均匀时间进展的位置编码，本文提出SyPE解决非仿射时间扭曲问题，在StretchTime中实现并取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现实系统存在“时间扭曲”动态，现有位置编码如RoPE无法表示非仿射时间扭曲。

Method: 提出基于哈密顿力学的可学习编码框架SyPE，将旋转组扩展到辛群，通过输入依赖的自适应扭曲模块调制，应用于StretchTime架构。

Result: 在标准基准上实现了SOTA性能，在具有非平稳时间动态的数据集上显示出卓越的鲁棒性。

Conclusion: SyPE能解决现有位置编码不足，有效捕捉局部变化周期性，提升时间序列预测性能。

Abstract: Transformer architectures have established strong baselines in time series forecasting, yet they typically rely on positional encodings that assume uniform, index-based temporal progression. However, real-world systems, from shifting financial cycles to elastic biological rhythms, frequently exhibit "time-warped" dynamics where the effective flow of time decouples from the sampling index. In this work, we first formalize this misalignment and prove that rotary position embedding (RoPE) is mathematically incapable of representing non-affine temporal warping. To address this, we propose Symplectic Positional Embeddings (SyPE), a learnable encoding framework derived from Hamiltonian mechanics. SyPE strictly generalizes RoPE by extending the rotation group $\mathrm{SO}(2)$ to the symplectic group $\mathrm{Sp}(2,\mathbb{R})$, modulated by a novel input-dependent adaptive warp module. By allowing the attention mechanism to adaptively dilate or contract temporal coordinates end-to-end, our approach captures locally varying periodicities without requiring pre-defined warping functions. We implement this mechanism in StretchTime, a multivariate forecasting architecture that achieves state-of-the-art performance on standard benchmarks, demonstrating superior robustness on datasets exhibiting non-stationary temporal dynamics.

</details>


### [453] [Improving Detection of Rare Nodes in Hierarchical Multi-Label Learning](https://arxiv.org/abs/2602.08986)
*Isaac Xu,Martin Gillis,Ayushi Sharma,Benjamin Misiuk,Craig J. Brown,Thomas Trappenberg*

Main category: cs.LG

TL;DR: 提出加权损失目标以解决分层多标签分类困难，在基准数据集上提升召回率与F1分数，助力卷积网络。


<details>
  <summary>Details</summary>
Motivation: 解决分层多标签分类中模型预测难以到达层次更深层的问题，该问题源于某些类别的稀缺和层级约束。

Method: 提出结合节点不平衡加权和焦加权组件的神经网络加权损失目标，训练时关注稀有节点和不确定节点。

Result: 在基准数据集上召回率提升达五倍，F1分数有显著提高，能帮助卷积网络处理具有挑战性的任务。

Conclusion: 提出的方法可有效解决分层多标签分类中的难题，改善模型性能。

Abstract: In hierarchical multi-label classification, a persistent challenge is enabling model predictions to reach deeper levels of the hierarchy for more detailed or fine-grained classifications. This difficulty partly arises from the natural rarity of certain classes (or hierarchical nodes) and the hierarchical constraint that ensures child nodes are almost always less frequent than their parents. To address this, we propose a weighted loss objective for neural networks that combines node-wise imbalance weighting with focal weighting components, the latter leveraging modern quantification of ensemble uncertainties. By emphasizing rare nodes rather than rare observations (data points), and focusing on uncertain nodes for each model output distribution during training, we observe improvements in recall by up to a factor of five on benchmark datasets, along with statistically significant gains in $F_{1}$ score. We also show our approach aids convolutional networks on challenging tasks, as in situations with suboptimal encoders or limited data.

</details>


### [454] [DirMoE: Dirichlet-routed Mixture of Experts](https://arxiv.org/abs/2602.09001)
*Amirhossein Vahidi,Hesam Asadollahzadeh,Navid Akhavan Attar,Marie Moullet,Kevin Ly,Xingyi Yang,Mohammad Lotfollahi*

Main category: cs.LG

TL;DR: 现有Top-k+Softmax路由限制MoE模型性能和可扩展性，本文提出DirMoE路由机制，可分离核心路由问题且全流程可微，表现良好并提升专家专业化程度。


<details>
  <summary>Details</summary>
Motivation: 现有路由器依赖非可微的Top-k+Softmax，限制了MoE模型性能和可扩展性，且将两个不同决策混在一起。

Method: 引入基于狄利克雷变分自编码器框架的DirMoE，用伯努利组件建模专家选择、狄利克雷组件处理专家贡献分配，通过Gumbel-Sigmoid松弛和隐式重参数化使前向传播可微，训练目标包含直接稀疏惩罚和超参数调度。

Result: DirMoE路由器性能与其他方法相当或更优，同时提高了专家专业化程度。

Conclusion: DirMoE能解决现有路由器问题，有效提升模型表现。

Abstract: Mixture-of-Experts (MoE) models have demonstrated exceptional performance in large-scale language models. Existing routers typically rely on non-differentiable Top-$k$+Softmax, limiting their performance and scalability. We argue that two distinct decisions, which experts to activate and how to distribute expert contributions among them, are conflated in standard Top-$k$+Softmax. We introduce Dirichlet-Routed MoE (DirMoE), a novel end-to-end differentiable routing mechanism built on a Dirichlet variational autoencoder framework. This design fundamentally disentangles the core routing problems: expert selection, modeled by a Bernoulli component, and expert contribution among chosen experts, handled by a Dirichlet component. The entire forward pass remains fully differentiable through the use of Gumbel-Sigmoid relaxation for the expert selection and implicit reparameterization for the Dirichlet distribution. Our training objective, a variational ELBO, includes a direct sparsity penalty that precisely controls the number of active experts in expectation, alongside a schedule for key hyperparameters that guides the model from an exploratory to a definitive routing state. Moreover, our DirMoE router matches or exceeds other methods while improving expert specialization.

</details>


### [455] [ARO: A New Lens On Matrix Optimization For Large Models](https://arxiv.org/abs/2602.09006)
*Wenbo Gong,Javier Zazo,Qijun Luo,Puqian Wang,James Hensman,Chao Ma*

Main category: cs.LG

TL;DR: 提出Adaptively Rotated Optimization (ARO)框架加速LLM训练，通过新基准测试验证其优于AdamW和正交化方法，并探讨其作为对称感知优化器的应用。


<details>
  <summary>Details</summary>
Motivation: 探索除正交化之外的新范式以进一步提升LLM训练效率。

Method: 提出ARO框架，在旋转坐标系中进行规范最速下降，旋转由新的规范知情策略决定，同时提出严格控制的基准测试协议。

Result: 在高达8B激活参数和8倍过训练预算的LLM预训练中，ARO始终优于AdamW（1.3 - 1.35倍）和正交化方法（1.1 - 1.15倍），且无收益递减迹象。

Conclusion: ARO可重构为基于残差流旋转对称性的对称感知优化器，为高级设计提供思路以高效利用跨层/跨模块耦合。

Abstract: Matrix-based optimizers have attracted growing interest for improving LLM training efficiency, with significant progress centered on orthogonalization/whitening based methods. While yielding substantial performance gains, a fundamental question arises: can we develop new paradigms beyond orthogonalization, pushing the efficiency frontier further? We present \textbf{Adaptively Rotated Optimization (ARO}, a new matrix optimization framework that treats gradient rotation as a first class design principle. ARO accelerates LLM training by performing normed steepest descent in a rotated coordinate system, where the rotation is determined by a novel norm-informed policy. This perspective yields update rules that go beyond existing orthogonalization and whitening optimizers, improving sample efficiency in practice. To make comparisons reliable, we propose a rigorously controlled benchmarking protocol that reduces confounding and bias. Under this protocol, ARO consistently outperforms AdamW (by 1.3 $\sim$1.35$\times$) and orthogonalization methods (by 1.1$\sim$1.15$\times$) in LLM pretraining at up to 8B activated parameters, and up to $8\times$ overtrain budget, without evidence of diminishing returns. Finally, we discuss how ARO can be reformulated as a symmetry-aware optimizer grounded in rotational symmetries of residual streams, motivating advanced designs that enable computationally efficient exploitation of cross-layer/cross module couplings.

</details>


### [456] [ShapeCond: Fast Shapelet-Guided Dataset Condensation for Time Series Classification](https://arxiv.org/abs/2602.09008)
*Sijia Peng,Yun Xiong,Xi Chen,Yi Xie,Guanzhi Li,Yanwei Yu,Yangyong Zhu,Zhiqiang Shen*

Main category: cs.LG

TL;DR: 提出ShapeCond框架用于时间序列分类数据集压缩，速度快且能提升下游准确率。


<details>
  <summary>Details</summary>
Motivation: 时间序列数据增长使存储和计算压力大，现存压缩方法多以图像为中心，不适用于时间序列，缺少对局部判别性模式的处理。

Method: 提出ShapeCond框架，通过形状元引导的优化策略利用基于形状元的数据集知识，形状元辅助合成成本与序列长度无关。

Result: 在合成速度上比先前先进方法CondTSC快29倍，在特定数据集上比简单使用形状元快达10000倍，在大量实验中始终优于先前方法。

Conclusion: ShapeCond通过明确保留关键局部模式，提升下游准确率，是一种高效的时间序列数据集压缩框架。

Abstract: Time series data supports many domains (e.g., finance and climate science), but its rapid growth strains storage and computation. Dataset condensation can alleviate this by synthesizing a compact training set that preserves key information. Yet most condensation methods are image-centric and often fail on time series because they miss time-series-specific temporal structure, especially local discriminative motifs such as shapelets. In this work, we propose ShapeCond, a novel and efficient condensation framework for time series classification that leverages shapelet-based dataset knowledge via a shapelet-guided optimization strategy. Our shapelet-assisted synthesis cost is independent of sequence length: longer series yield larger speedups in synthesis (e.g., 29$\times$ faster over prior state-of-the-art method CondTSC for time-series condensation, and up to 10,000$\times$ over naively using shapelets on the Sleep dataset with 3,000 timesteps). By explicitly preserving critical local patterns, ShapeCond improves downstream accuracy and consistently outperforms all prior state-of-the-art time series dataset condensation methods across extensive experiments. Code is available at https://github.com/lunaaa95/ShapeCond.

</details>


### [457] [ANCRe: Adaptive Neural Connection Reassignment for Efficient Depth Scaling](https://arxiv.org/abs/2602.09009)
*Yilang Zhang,Bingcong Li,Niao He,Georgios B. Giannakis*

Main category: cs.LG

TL;DR: 本文从优化角度重新审视残差连接，提出自适应神经连接重分配框架ANCRe，可有效利用网络深度，经测试能加速收敛、提升性能和深度效率。


<details>
  <summary>Details</summary>
Motivation: 现代基础模型加深网络深度时深层常未充分利用，需重新审视加深神经网络的默认机制——残差连接。

Method: 提出自适应神经连接重分配（ANCRe）框架，从数据中参数化并学习残差连接。

Result: 在大语言模型预训练、扩散模型和深度残差网络的大量数值测试中，相比传统残差连接，ANCRe能加速收敛、提升性能和增强深度效率。

Conclusion: ANCRe框架能以极小的计算和内存开销自适应重新分配残差连接，更有效地利用网络深度。

Abstract: Scaling network depth has been a central driver behind the success of modern foundation models, yet recent investigations suggest that deep layers are often underutilized. This paper revisits the default mechanism for deepening neural networks, namely residual connections, from an optimization perspective. Rigorous analysis proves that the layout of residual connections can fundamentally shape convergence behavior, and even induces an exponential gap in convergence rates. Prompted by this insight, we introduce adaptive neural connection reassignment (ANCRe), a principled and lightweight framework that parameterizes and learns residual connectivities from the data. ANCRe adaptively reassigns residual connections with negligible computational and memory overhead ($<1\%$), while enabling more effective utilization of network depth. Extensive numerical tests across pre-training of large language models, diffusion models, and deep ResNets demonstrate consistently accelerated convergence, boosted performance, and enhanced depth efficiency over conventional residual connections.

</details>


### [458] [Next-Gen CAPTCHAs: Leveraging the Cognitive Gap for Scalable and Diverse GUI-Agent Defense](https://arxiv.org/abs/2602.09012)
*Jiacheng Liu,Yaxin Luo,Jiacheng Cui,Xinyi Shang,Xiaohan Zhao,Zhiqiang Shen*

Main category: cs.LG

TL;DR: 传统CAPTCHA因GUI代理发展而过时，现有基准被先进推理模型突破，本文提出下一代CAPTCHAs框架应对。


<details>
  <summary>Details</summary>
Motivation: 现有CAPTCHA基准被先进推理模型突破，需新框架保障下一代网络安全。

Method: 构建基于强大数据生成管道的基准，利用人机“认知差距”设计动态任务。

Result: 可进行大规模、易扩展的评估，能为后端支持类型生成大量CAPTCHA实例。

Conclusion: 重新建立生物用户和人工智能代理的区别，为代理时代提供可扩展、多样化的防御机制。

Abstract: The rapid evolution of GUI-enabled agents has rendered traditional CAPTCHAs obsolete. While previous benchmarks like OpenCaptchaWorld established a baseline for evaluating multimodal agents, recent advancements in reasoning-heavy models, such as Gemini3-Pro-High and GPT-5.2-Xhigh have effectively collapsed this security barrier, achieving pass rates as high as 90% on complex logic puzzles like "Bingo". In response, we introduce Next-Gen CAPTCHAs, a scalable defense framework designed to secure the next-generation web against the advanced agents. Unlike static datasets, our benchmark is built upon a robust data generation pipeline, allowing for large-scale and easily scalable evaluations, notably, for backend-supported types, our system is capable of generating effectively unbounded CAPTCHA instances. We exploit the persistent human-agent "Cognitive Gap" in interactive perception, memory, decision-making, and action. By engineering dynamic tasks that require adaptive intuition rather than granular planning, we re-establish a robust distinction between biological users and artificial agents, offering a scalable and diverse defense mechanism for the agentic era.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [459] [Curriculum-Learned Vanishing Stacked Residual PINNs for Hyperbolic PDE State Reconstruction](https://arxiv.org/abs/2602.06996)
*Katayoun Eshkofti,Matthieu Barreau*

Main category: cs.NE

TL;DR: 传统PINNs在建模双曲PDEs分布式动力系统时有挑战，本文将三种课程学习方法集成到VSR - PINN，交通重建实验表明因果性方法有显著提升。


<details>
  <summary>Details</summary>
Motivation: 传统物理信息神经网络在建模双曲偏微分方程控制的分布式动力系统时，因不连续性和冲击而难以收敛。

Method: 将原对偶优化、因果性推进和自适应采样三种课程学习方法集成到VSR - PINN。

Result: 交通重建的数值实验显示，执行因果性方法系统地降低了逐点均方误差的中位数及其跨运行的变异性，在基线和原对偶变体中比非因果训练有近一个数量级的改进。

Conclusion: 将三种课程学习方法集成到VSR - PINN是有效的，因果性方法能显著提升模型性能。

Abstract: Modeling distributed dynamical systems governed by hyperbolic partial differential equations (PDEs) remains challenging due to discontinuities and shocks that hinder the convergence of traditional physics-informed neural networks (PINNs). The recently proposed vanishing stacked residual PINN (VSR-PINN) embeds a vanishing-viscosity mechanism within stacked residual refinements to enable a smooth transition from the parabolic to hyperbolic regime. This paper integrates three curriculum-learning methods as primal-dual (PD) optimization, causality progression, and adaptive sampling into the VSR-PINN. The PD strategy balances physics and data losses, the causality scheme unlocks deeper stacks by respecting temporal and gradient evolution, and adaptive sampling targets high residuals. Numerical experiments on traffic reconstruction confirm that enforcing causality systematically reduces the median point-wise MSE and its variability across runs, yielding improvements of nearly one order of magnitude over non-causal training in both the baseline and PD variants.

</details>


### [460] [MolLIBRA: Genetic Molecular Optimization with Multi-Fingerprint Surrogates and Text-Molecule Aligned Critic](https://arxiv.org/abs/2602.07002)
*Masahi Okada,Kazuki Sakai,Hiroaki Yoshida,Masaki Okoshi,Tadahiro Taniguchi*

Main category: cs.NE

TL;DR: 研究有限评估预算下的样本高效分子优化，提出MolLIBRA框架，在PMO - 1K基准测试中表现良好。


<details>
  <summary>Details</summary>
Motivation: 解决有限评估预算下的样本高效分子优化问题。

Method: 提出基于遗传算法的MolLIBRA框架，使用多个评判器对候选分子预排序，包括高斯过程代理集合和预训练的文本 - 分子对齐编码器CLAMP。

Result: 在PMO - 1K基准测试中，MolLIBRA - L在14/22个任务上获得最佳Top - 10 AUC，且跨任务的Top - 10 AUC总和最高。

Conclusion: MolLIBRA框架在有限评估预算的分子优化中具有较好性能。

Abstract: We study sample-efficient molecular optimization under a limited budget of oracle evaluations. We propose MolLIBRA (MultimOdaLity and Language Integrated Bayesian and evolutionaRy optimizAtion), a genetic algorithm based framework that pre-ranks candidate molecules using multiple critics before oracle calls: (i) an ensemble of Gaussian process (GP) surrogates defined over multiple molecular fingerprints and (ii) a pretrained text-molecule aligned encoder CLAMP. The GP ensemble enables adaptive selection of task-appropriate fingerprints, while CLAMP provides a zero-shot scoring signal from task descriptions by measuring the similarity between molecular and text embeddings. On the Practical Molecular Optimization (PMO) benchmark with a budget of 1,000 evaluations (PMO-1K), MolLIBRA-L, our variant with a language-model-based candidate generator, attains the best Top-10 AUC on 14/22 tasks and the highest overall sum of Top-10 AUC across tasks among prior methods.

</details>


### [461] [Multi-Scale Temporal Homeostasis Enables Efficient and Robust Neural Networks](https://arxiv.org/abs/2602.07009)
*MD Azizul Hakim*

Main category: cs.NE

TL;DR: 本文提出多尺度时间稳态（MSTH）框架，提升人工神经网络稳定性与性能，表明跨尺度时间协调是稳定人工神经系统的核心原则。


<details>
  <summary>Details</summary>
Motivation: 人工神经网络在扰动下很脆弱，而生物神经系统可通过多时间尺度的稳态调节维持可靠功能，因此受此启发提出MSTH框架。

Method: 将超快速（5 - ms）、快速（2 - s）、中等（5 - min）和慢速（1 - hrs）调节集成到人工网络中，实现跨尺度协调系统。

Result: 在分子、图和图像分类基准测试中，MSTH持续提高准确率，消除灾难性故障，增强从扰动中恢复的能力，优于单尺度仿生模型和现有先进方法。

Conclusion: 跨尺度时间协调是稳定人工神经系统的核心原则，MSTH可作为构建强大、有弹性和生物逼真智能的基础。

Abstract: Artificial neural networks achieve strong performance on benchmark tasks but remain fundamentally brittle under perturbations, limiting their deployment in real-world settings. In contrast, biological nervous systems sustain reliable function across decades through homeostatic regulation coordinated across multiple temporal scales. Inspired by this principle, this presents Multi-Scale Temporal Homeostasis (MSTH), a biologically grounded framework that integrates ultra-fast (5-ms), fast (2-s), medium (5-min) and slow (1-hrs) regulation into artificial networks. MSTH implements the cross-scale coordination system for artificial neural networks, providing a unified temporal hierarchy that moves beyond superficial biomimicry. The cross-scale coordination enhances computational efficiency through evolutionary-refined optimization mechanisms. Experiments across molecular, graph and image classification benchmarks show that MSTH consistently improves accuracy, eliminates catastrophic failures and enhances recovery from perturbations. Moreover, MSTH outperforms both single-scale bio-inspired models and established state-of-the-art methods, demonstrating generality across diverse domains. These findings establish cross-scale temporal coordination as a core principle for stabilizing artificial neural systems, positioning MSTH as a foundation for building robust, resilient and biologically faithful intelligence.

</details>


### [462] [Learning Alzheimer's Disease Signatures by bridging EEG with Spiking Neural Networks and Biophysical Simulations](https://arxiv.org/abs/2602.07010)
*Szymon Mamoń,Max Talanov,Alessandro Crimi*

Main category: cs.NE

TL;DR: 提出神经桥框架用于阿尔茨海默病（AD）检测，结合SNN分类与可解释的电路模拟，提高对脑电图生物标志物的机制理解。


<details>
  <summary>Details</summary>
Motivation: 随着AD患病率上升，传统深度学习方法在基于脑电图的AD检测中存在计算密集和机制不透明的问题，SNN在AD诊断中的应用未充分探索。

Method: 提出神经桥框架，结合数据驱动学习与生物物理模拟；用静息态临床脑电图训练SNN分类器；构建尖峰网络模拟，改变抑制 - 兴奋突触比率；将经验功能连接先验纳入多子网模拟。

Result: SNN分类器性能良好（AUC = 0.839），确定1/f斜率为关键判别标记；模拟再现了经验光谱减慢和α组织改变；多子网模拟增强了光谱分化。

Conclusion: 神经桥方法将基于SNN的分类与可解释的电路模拟相结合，推进了对脑电图生物标志物的机制理解，实现可扩展、可解释的AD检测。

Abstract: As the prevalence of Alzheimer's disease (AD) rises, improving mechanistic insight from non-invasive biomarkers is increasingly critical. Recent work suggests that circuit-level brain alterations manifest as changes in electroencephalography (EEG) spectral features detectable by machine learning. However, conventional deep learning approaches for EEG-based AD detection are computationally intensive and mechanistically opaque. Spiking neural networks (SNNs) offer a biologically plausible and energy-efficient alternative, yet their application to AD diagnosis remains largely unexplored.
  We propose a neuro-bridge framework that links data-driven learning with minimal, biophysically grounded simulations, enabling bidirectional interpretation between machine learning signatures and circuit-level mechanisms in AD. Using resting-state clinical EEG, we train an SNN classifier that achieves competitive performance (AUC = 0.839) and identifies the aperiodic 1/f slope as a key discriminative marker.
  The 1/f slope reflects excitation-inhibition balance. To interpret this mechanistically, we construct spiking network simulations in which inhibitory-to-excitatory synaptic ratios are systematically varied to emulate healthy, mild cognitive impairment, and AD-like states. Using both membrane potential-based and synaptic current-based EEG proxies, we reproduce empirical spectral slowing and altered alpha organization.
  Incorporating empirical functional connectivity priors into multi-subnetwork simulations further enhances spectral differentiation, demonstrating that large-scale network topology constrains EEG signatures more strongly than excitation-inhibition balance alone. Overall, this neuro-bridge approach connects SNN-based classification with interpretable circuit simulations, advancing mechanistic understanding of EEG biomarkers while enabling scalable, explainable AD detection.

</details>


### [463] [Stochastic Spiking Neuron Based SNN Can be Inherently Bayesian](https://arxiv.org/abs/2602.07037)
*Huannan Zheng,Jingli Liu,Kezhou Yang*

Main category: cs.NE

TL;DR: 提出SBNN框架利用设备噪声作贝叶斯资源，实验显示高精度、训练加速和强鲁棒性，硬件验证损失小，为不确定下的神经形态计算提供途径。


<details>
  <summary>Details</summary>
Motivation: 生物神经系统不确定性有益，而神经形态计算中设备可变性限制性能，需利用噪声。

Method: 提出SBNN框架，统一基于磁隧道结的设备固有随机动态模型和随机阈值神经元。

Result: SBNN在MNIST和CIFAR10上高精度，训练加速约20倍，鲁棒性强，硬件验证损失小。

Conclusion: 将设备随机性转化为神经元不确定性可实现紧凑、节能的神经形态计算。

Abstract: Uncertainty in biological neural systems appears to be computationally beneficial rather than detrimental. However, in neuromorphic computing systems, device variability often limits performance, including accuracy and efficiency. In this work, we propose a spiking Bayesian neural network (SBNN) framework that unifies the dynamic models of intrinsic device stochasticity (based on Magnetic Tunnel Junctions) and stochastic threshold neurons to leverage noise as a functional Bayesian resource. Experiments demonstrate that SBNN achieves high accuracy (99.16% on MNIST, 94.84% on CIFAR10) with 8-bit precision. Meanwhile rate estimation method provides a ~20-fold training speedup. Furthermore, SBNN exhibits superior robustness, showing a 67% accuracy improvement under synaptic weight noise and 12% under input noise compared to standard spiking neural networks. Crucially, hardware validation confirms that physical device implementation causes invisible accuracy and calibration loss compared to the algorithmic model. Converting device stochasticity into neuronal uncertainty offers a route to compact, energy-efficient neuromorphic computing under uncertainty.

</details>


### [464] [Assessing Reproducibility in Evolutionary Computation: A Case Study using Human- and LLM-based Assessment](https://arxiv.org/abs/2602.07059)
*Francesca Da Ros,Tarik Začiragić,Aske Plaat,Thomas Bäck,Niki van Stein*

Main category: cs.NE

TL;DR: 研究遗传与进化计算会议特定轨道十年论文的可重复性实践，引入清单并提出自动化系统，发现可重复性报告有差距，自动化工具可行。


<details>
  <summary>Details</summary>
Motivation: 进化计算领域已发表工作的实际可重复性水平缺乏实证证据，需进行研究。

Method: 引入结构化可重复性清单对选定论文集进行手动评估，提出基于大语言模型的自动化系统RECAP评估可重复性信号。

Result: 论文平均完整性得分0.62，36.90%提供额外材料，RECAP与人工评估者有较高一致性（Cohen's k为0.67）。

Conclusion: 可重复性报告存在持续差距，自动化工具能有效支持大规模、系统监测可重复性实践。

Abstract: Reproducibility is an important requirement in evolutionary computation, where results largely depend on computational experiments. In practice, reproducibility relies on how algorithms, experimental protocols, and artifacts are documented and shared. Despite growing awareness, there is still limited empirical evidence on the actual reproducibility levels of published work in the field. In this paper, we study the reproducibility practices in papers published in the Evolutionary Combinatorial Optimization and Metaheuristics track of the Genetic and Evolutionary Computation Conference over a ten-year period. We introduce a structured reproducibility checklist and apply it through a systematic manual assessment of the selected corpus. In addition, we propose RECAP (REproducibility Checklist Automation Pipeline), an LLM-based system that automatically evaluates reproducibility signals from paper text and associated code repositories. Our analysis shows that papers achieve an average completeness score of 0.62, and that 36.90% of them provide additional material beyond the manuscript itself. We demonstrate that automated assessment is feasible: RECAP achieves substantial agreement with human evaluators (Cohen's k of 0.67). Together, these results highlight persistent gaps in reproducibility reporting and suggest that automated tools can effectively support large-scale, systematic monitoring of reproducibility practices.

</details>


### [465] [Evolving LLM-Derived Control Policies for Residential EV Charging and Vehicle-to-Grid Energy Optimization](https://arxiv.org/abs/2602.07275)
*Vishesh Purnananda,Benjamin John Wruck,Mingyu Guo*

Main category: cs.NE

TL;DR: 研究将进化计算用于住宅电动汽车能源管理，提出基于大语言模型的程序搜索框架，混合策略效果好，证明该方法可生成透明适用的充电控制策略。


<details>
  <summary>Details</summary>
Motivation: 强化学习用于车辆到电网（V2G）优化时产生的神经网络难以审计，存在可解释性问题。

Method: 提出基于大语言模型的程序搜索框架，在迭代提示 - 评估 - 修复循环中使用大语言模型作为智能变异算子，利用EV2Gym模拟环境作为适应度函数，比较四种提示策略。

Result: 混合策略产生简洁、人类可读的启发式方法，实现了基线利润的118%，发现复杂行为。

Conclusion: 基于大语言模型的进化计算是生成透明、可检查且适用于实际住宅部署的电动汽车充电控制策略的实用方法。

Abstract: This research presents a novel application of Evolutionary Computation to the domain of residential electric vehicle (EV) energy management. While reinforcement learning (RL) achieves high performance in vehicle-to-grid (V2G) optimization, it typically produces opaque "black-box" neural networks that are difficult for consumers and regulators to audit. Addressing this interpretability gap, we propose a program search framework that leverages Large Language Models (LLMs) as intelligent mutation operators within an iterative prompt-evaluation-repair loop. Utilizing the high-fidelity EV2Gym simulation environment as a fitness function, the system undergoes successive refinement cycles to synthesize executable Python policies that balance profit maximization, user comfort, and physical safety constraints. We benchmark four prompting strategies: Imitation, Reasoning, Hybrid and Runtime, evaluating their ability to discover adaptive control logic. Results demonstrate that the Hybrid strategy produces concise, human-readable heuristics that achieve 118% of the baseline profit, effectively discovering complex behaviors like anticipatory arbitrage and hysteresis without explicit programming. This work establishes LLM-driven Evolutionary Computation as a practical approach for generating EV charging control policies that are transparent, inspectable, and suitable for real residential deployment.

</details>


### [466] [Optimizing Chlorination in Water Distribution Systems via Surrogate-assisted Neuroevolution](https://arxiv.org/abs/2602.07299)
*Rivaaj Monsia,Daniel Young,Olivier Francon,Risto Miikkulainen*

Main category: cs.NE

TL;DR: 本文提出基于神经进化、多目标优化和代理模型的进化框架来确保大型异质水分配系统的微生物安全，所进化的控制器产生多种政策，优于标准强化学习方法。


<details>
  <summary>Details</summary>
Motivation: 大型异质水分配系统中维持合适消毒剂残留水平具有挑战，传统控制算法难以应对系统中的复杂流体非线性和噪声。

Method: 使用NEAT进化神经网络来在指定时间和地点注入氯，采用NSGA - II优化四个目标，用训练模拟EPANET的神经网络作为代理模型评估每个网络。

Result: 进化的控制器产生多种帕累托最优政策，优于如PPO的标准强化学习方法。

Conclusion: 该框架为改善城市供水系统提供途径，凸显了结合进化和代理模型优化复杂现实系统的潜力。

Abstract: Ensuring the microbiological safety of large, heterogeneous water distribution systems (WDS) typically requires managing appropriate levels of disinfectant residuals including chlorine. WDS include complex fluid interactions that are nonlinear and noisy, making such maintenance a challenging problem for traditional control algorithms. This paper proposes an evolutionary framework to this problem based on neuroevolution, multi-objective optimization, and surrogate modeling. Neural networks were evolved with NEAT to inject chlorine at strategic locations in the distribution network at select times. NSGA-II was employed to optimize four objectives: minimizing the total amount of chlorine injected, keeping chlorine concentrations homogeneous across the network, ensuring that maximum concentrations did not exceed safe bounds, and distributing the injections regularly over time. Each network was evaluated against a surrogate model, i.e. a neural network trained to emulate EPANET, an industry-level hydraulic WDS simulator that is accurate but infeasible in terms of computational cost to support machine learning. The evolved controllers produced a diverse range of Pareto-optimal policies that could be implemented in practice, outperforming standard reinforcement learning methods such as PPO. The results thus suggest a pathway toward improving urban water systems, and highlight the potential of using evolution with surrogate modeling to optimize complex real-world systems.

</details>


### [467] [A Multi-objective Evolutionary Algorithm Based on Bi-population with Uniform Sampling for Neural Architecture Search](https://arxiv.org/abs/2602.08513)
*Yu Xue,Pengcheng Jiang,Chenchen Zhu,Yong Zhang,Ran Cheng,Kaizhou Gao,Dunwei Gong*

Main category: cs.NE

TL;DR: 提出用于神经架构搜索的MOEA - BUS算法，在CIFAR - 10和ImageNet上实验表现优，消融实验证明关键机制有效。


<details>
  <summary>Details</summary>
Motivation: 现有神经架构搜索方法在优化多目标时存在种群多样性有限、搜索空间探索不足的问题，需改进。

Method: 提出MOEA - BUS算法，用均匀采样初始化种群，采用双种群框架让两个种群协同进化。

Result: 在CIFAR - 10和ImageNet上实验表现好；消融实验证明均匀采样和双种群机制提升种群多样性和性能；SVM在肯德尔tau系数上有提升。

Conclusion: MOEA - BUS算法有效，能同时优化准确度和网络复杂度，均匀采样和双种群机制起重要作用。

Abstract: Neural architecture search (NAS) automates neural network design, improving efficiency over manual approaches. However, efficiently discovering high-performance neural network architectures that simultaneously optimize multiple objectives remains a significant challenge in NAS. Existing methods often suffer from limited population diversity and inadequate exploration of the search space, particularly in regions with extreme complexity values. To address these challenges, we propose MOEA-BUS, an innovative multi-objective evolutionary algorithm based on bi-population with uniform sampling for neural architecture search, aimed at simultaneously optimizing both accuracy and network complexity. In MOEA-BUS, a novel uniform sampling method is proposed to initialize the population, ensuring that architectures are distributed uniformly across the objective space. Furthermore, to enhance exploration, we deploy a bi-population framework where two populations evolve synergistically, facilitating comprehensive search space coverage. Experiments on CIFAR-10 and ImageNet demonstrate MOEA-BUS's superiority, achieving top-1 accuracies of 98.39% on CIFAR-10, and 80.03% on ImageNet. Notably, it achieves 78.28% accuracy on ImageNet with only 446M MAdds. Ablation studies confirm that both uniform sampling and bi-population mechanisms enhance population diversity and performance. Additionally, in terms of the Kendall's tau coefficient, the SVM achieves an improvement of at least 0.035 compared to the other three commonly used machine learning models, and uniform sampling provided an enhancement of approximately 0.07.

</details>


### [468] [Enhancing Genetic Algorithms with Graph Neural Networks: A Timetabling Case Study](https://arxiv.org/abs/2602.08619)
*Laura-Maria Cornei,Mihaela-Elena Breabăn*

Main category: cs.NE

TL;DR: 本文研究遗传算法与图神经网络混合用于排课优化，实验表明该混合方法在效率和质量上优于单独方法。


<details>
  <summary>Details</summary>
Motivation: 探索遗传算法与图神经网络混合对排课优化的影响，提升排课质量和效率。

Method: 分别设计、开发和优化遗传算法和图神经网络，将图神经网络作为增强算子融入遗传算法，在员工排班问题上进行实验。

Result: 混合方法在时间效率和解决方案质量指标上较单独方法有显著提升。

Conclusion: 这是首次将遗传算法与图神经网络混合用于解决排课问题，混合方法效果良好。

Abstract: This paper investigates the impact of hybridizing a multi-modal Genetic Algorithm with a Graph Neural Network for timetabling optimization. The Graph Neural Network is designed to encapsulate general domain knowledge to improve schedule quality, while the Genetic Algorithm explores different regions of the search space and integrates the deep learning model as an enhancement operator to guide the solution search towards optimality. Initially, both components of the hybrid technique were designed, developed, and optimized independently to solve the tackled task. Multiple experiments were conducted on Staff Rostering, a well-known timetabling problem, to compare the proposed hybridization with the standalone optimized versions of the Genetic Algorithm and Graph Neural Network. The experimental results demonstrate that the proposed hybridization brings statistically significant improvements in both the time efficiency and solution quality metrics, compared to the standalone methods. To the best of our knowledge, this work proposes the first hybridization of a Genetic Algorithm with a Graph Neural Network for solving timetabling problems.

</details>


### [469] [A Methodology for Effective Surrogate Learning in Complex Optimization](https://arxiv.org/abs/2602.08825)
*Tomohiro Harada,Enrique Alba,Gabriel Luque*

Main category: cs.NE

TL;DR: 本文介绍PTME方法研究深度学习替代模型，以交通灯网络优化问题为例进行研究，结论是该方法可作其他应用和求解器的指南。


<details>
  <summary>Details</summary>
Motivation: 解决复杂问题需持续发展理论与实践，定义和表征替代模型很重要。

Method: 引入PTME方法分析深度学习替代模型的精度、时间、内存和能耗；针对交通灯网络优化问题构建不同替代模型，开展PTME研究；利用构建的替代模型进行决策。

Result: 获得交通灯网络优化问题更好的技术。

Conclusion: PTME方法可作为其他应用和求解器的指南。

Abstract: Solving complex problems requires continuous effort in developing theory and practice to cope with larger, more difficult scenarios. Working with surrogates is normal for creating a proxy that realistically models the problem into the computer. Thus, the question of how to best define and characterize such a surrogate model is of the utmost importance. In this paper, we introduce the PTME methodology to study deep learning surrogates by analyzing their Precision, Time, Memory, and Energy consumption. We argue that only a combination of numerical and physical performance can lead to a surrogate that is both a trusted scientific substitute for the real problem and an efficient experimental artifact for scalable studies. Here, we propose different surrogates for a real problem in optimally organizing the network of traffic lights in European cities and perform a PTME study on the surrogates' sampling methods, dataset sizes, and resource consumption. We further use the built surrogates in new optimization metaheuristics for decision-making in real cities. We offer better techniques and conclude that the PTME methodology can be used as a guideline for other applications and solvers.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [470] [Artificial Intelligence in Open Source Software Engineering: A Foundation for Sustainability](https://arxiv.org/abs/2602.07071)
*S M Rakib UI Karim,Wenyi Lu,Sean Goggins*

Main category: cs.SE

TL;DR: 本文探讨人工智能在解决开源软件可持续性挑战方面的应用、局限和伦理问题，确定研究差距并给出未来方向。


<details>
  <summary>Details</summary>
Motivation: 开源软件在确保足够贡献方面面临困境，需探索人工智能解决其可持续性关键挑战的方法。

Method: 综合近期跨学科研究进行文献综述。

Result: 确定人工智能在该领域的关键应用，检查人工智能应用在开源软件环境中的局限和伦理问题。

Conclusion: 强调人工智能可作为增强人类能力的工具，指出关键研究差距并提出未来方向以支持更具弹性和公平的开源生态系统。

Abstract: Open-source software (OSS) is foundational to modern digital infrastructure, yet this context for group work continues to struggle to ensure sufficient contributions in many critical cases. This literature review explores how artificial intelligence (AI) is being leveraged to address critical challenges to OSS sustainability, including maintaining contributor engagement, securing funding, ensuring code quality and security, fostering healthy community dynamics, and preventing project abandonment. Synthesizing recent interdisciplinary research, the paper identifies key applications of AI in this domain, including automated bug triaging, system maintenance, contributor onboarding and mentorship, community health analytics, vulnerability detection, and task automation. The review also examines the limitations and ethical concerns that arise from applying AI in OSS contexts, including data availability, bias and fairness, transparency, risks of misuse, and the preservation of human-centered values in collaborative development. By framing AI not as a replacement but as a tool to augment human infrastructure, this study highlights both the promise and pitfalls of AI-driven interventions. It concludes by identifying critical research gaps and proposing future directions at the intersection of AI, sustainability, and OSS, aiming to support more resilient and equitable open-source ecosystems.

</details>


### [471] [AgentSpawn: Adaptive Multi-Agent Collaboration Through Dynamic Spawning for Long-Horizon Code Generation](https://arxiv.org/abs/2602.07072)
*Igor Costa*

Main category: cs.SE

TL;DR: 提出AgentSpawn架构用于动态代理协作，实验显示它比静态基线有更高完成率和更低内存开销。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体系统采用静态工作流，无法适应运行时分析揭示的意外复杂性，长时程代码生成需要跨领域的持续上下文和自适应专业知识。

Method: 提出AgentSpawn架构，通过自动内存转移、由运行时复杂性指标触发的自适应生成策略和并发修改的一致性协议，实现动态代理协作。

Result: 在SWE - bench等基准测试中，AgentSpawn比静态基线的完成率高34%，并通过选择性切片将内存开销降低42%。

Conclusion: AgentSpawn解决了现有研究在内存连续性、技能继承、任务恢复、运行时生成和并发一致性方面的五个关键差距。

Abstract: Long-horizon code generation requires sustained context and adaptive expertise across domains. Current multi-agent systems use static workflows that cannot adapt when runtime analysis reveals unanticipated complexity. We propose AgentSpawn, an architecture enabling dynamic agent collaboration through: (1) automatic memory transfer during spawning, (2) adaptive spawning policies triggered by runtime complexity metrics, and (3) coherence protocols for concurrent modifications. AgentSpawn addresses five critical gaps in existing research around memory continuity, skill inheritance, task resumption, runtime spawning, and concurrent coherence. Experimental validation demonstrates AgentSpawn achieves 34% higher completion rates than static baselines on benchmarks like SWE-bench while reducing memory overhead by 42% through selective slicing.

</details>


### [472] [Comprehensive Evaluation of Large Language Models on Software Engineering Tasks: A Multi-Task Benchmark](https://arxiv.org/abs/2602.07079)
*Go Frendi Gunawan,Mukhlis Amien*

Main category: cs.SE

TL;DR: 对11个先进大语言模型进行五项软件工程任务多任务评估，揭示模型效率、成本差异及任务挑战差异，并公开实验数据。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在软件工程领域虽有出色能力，但涵盖多样软件工程活动的综合基准有限。

Method: 采用自动化验证框架，对11个大语言模型进行五项软件工程任务的多任务评估，衡量输出质量和完成效率。

Result: 模型在完成时间、工具效率和估计成本上有巨大差异；工具使用频率与成功无关联；识别出两种低效模式；编码任务成功率达100%，研究任务挑战更大。

Conclusion: 公开所有实验数据、验证脚本和分析代码以实现完全可复现。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in software engineering, yet comprehensive benchmarks covering diverse SE activities remain limited. We present a multi-task evaluation of 11 state-of-the-art LLMs across five representative software engineering tasks: bug fixing, feature development, code refactoring, technical copywriting, and research synthesis. Our automated verification framework measures both output quality and completion efficiency. Key findings reveal that (1) models achieving identical perfect scores exhibit 22x variation in completion time, 49x variation in tool efficiency, and 53x variation in estimated cost; (2) tool usage frequency shows no correlation with success (r = 0.077, p = 0.575) - one model used 917 tool calls while another solved the same task with 3 calls; (3) we identify two distinct inefficiency patterns: loop inefficiency and inference inefficiency; and (4) coding tasks achieve 100 percent success while research tasks present greater challenges (90.9 percent). We release all experimental data, verification scripts, and analysis code for full reproducibility.

</details>


### [473] [CodeCircuit: Toward Inferring LLM-Generated Code Correctness via Attribution Graphs](https://arxiv.org/abs/2602.07080)
*Yicheng He,Zheng Zhao,Zhou Kaiyu,Bryan Dai,Jie Fu,Yonghui Yang*

Main category: cs.SE

TL;DR: 研究能否从LLM内部计算结构评估代码功能正确性，提出将代码验证作为机制诊断任务，分析表明内部正确性信号可靠，内部图拓扑特征预测更准，确立内部自省可用于验证代码。


<details>
  <summary>Details</summary>
Motivation: 当前代码验证范式依赖外部机制，劳动密集或受评判模型能力限制，探索能否从LLM内部计算结构评估其功能正确性。

Method: 受机制可解释性启发，将代码验证作为机制诊断任务，把模型的显式算法轨迹映射到行级归因图，分解复杂残差流以识别模型内部电路的结构特征。

Result: 分析Python、C++和Java代码发现，内部正确性信号在不同语法中很稳健，内部图的拓扑特征比表面启发式方法更能可靠预测正确性，还能进行有针对性的因果干预来修复错误逻辑。

Conclusion: 证实内部自省可作为验证生成代码的可解码属性。

Abstract: Current paradigms for code verification rely heavily on external mechanisms-such as execution-based unit tests or auxiliary LLM judges-which are often labor-intensive or limited by the judging model's own capabilities. This raises a fundamental, yet unexplored question: Can an LLM's functional correctness be assessed purely from its internal computational structure? Our primary objective is to investigate whether the model's neural dynamics encode internally decodable signals that are predictive of logical validity during code generation. Inspired by mechanistic interpretability, we propose to treat code verification as a mechanistic diagnostic task, mapping the model's explicit algorithmic trajectory into line-level attribution graphs. By decomposing complex residual flows, we aim to identify the structural signatures that distinguish sound reasoning from logical failure within the model's internal circuits. Analysis across Python, C++, and Java confirms that intrinsic correctness signals are robust across diverse syntaxes. Topological features from these internal graphs predict correctness more reliably than surface heuristics and enable targeted causal interventions to fix erroneous logic. These findings establish internal introspection as a decodable property for verifying generated code. Our code is at https:// github.com/bruno686/CodeCircuit.

</details>


### [474] [Rethinking Scientific Modeling: Toward Physically Consistent and Simulation-Executable Programmatic Generation](https://arxiv.org/abs/2602.07083)
*Yongqing Jiang,Jianze Wang,Zhiqi Shen,Zhenghong Lin,Jiayuan Wang,Yijian Yang,Kaoshan Dai,Haoran Luo*

Main category: cs.SE

TL;DR: 提出物理一致的自动建筑建模框架，引入CivilInstruct数据集，采用两阶段微调策略，用MBEval基准评估，实验效果优于基线。


<details>
  <summary>Details</summary>
Motivation: 结构建模中细微物理不一致或规范违反会使下游模拟无效，现有大语言模型生成的建模代码有不可执行或物理不一致问题。

Method: 提出集成领域知识构建、面向约束的模型对齐和验证驱动评估的框架；引入CivilInstruct数据集；采用两阶段微调策略；用MBEval基准评估。

Result: 实验结果表明，在严格验证指标下，相比基线有持续改进。

Conclusion: 所提物理一致的自动建筑建模框架能有效生成模拟就绪的模型，减少不符合要求的输出，代码开源。

Abstract: Structural modeling is a fundamental component of computational engineering science, in which even minor physical inconsistencies or specification violations may invalidate downstream simulations. The potential of large language models (LLMs) for automatic generation of modeling code has been demonstrated. However, non-executable or physically inconsistent outputs remain prevalent under stringent engineering constraints. A framework for physics-consistent automatic building modeling is therefore proposed, integrating domain knowledge construction, constraint-oriented model alignment, and verification-driven evaluation. CivilInstruct is introduced as a domain-specific dataset that formalizes structural engineering knowledge and constraint reasoning to enable simulation-ready model generation. A two-stage fine-tuning strategy is further employed to enforce constraint satisfaction and application programming interface compliance, substantially reducing hallucinated and non-conforming outputs. MBEval is presented as a verification-driven benchmark that evaluates executability and structural dynamics consistency through closed-loop validation. Experimental results show consistent improvements over baselines across rigorous verification metrics. Our code is available at https://github.com/Jovanqing/AutoBM.

</details>


### [475] [Evaluating Retrieval-Augmented Generation Variants for Natural Language-Based SQL and API Call Generation](https://arxiv.org/abs/2602.07086)
*Michael Marketsmüller,Simon Martin,Tim Schlippe*

Main category: cs.SE

TL;DR: 本文全面评估三种检索增强生成（RAG）变体在SQL查询生成、REST API调用生成及组合任务中的表现，发现RAG至关重要，CoRAG在混合文档设置中最稳健，检索策略设计是关键。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在特定领域企业环境中的有效性未充分探索，尤其是需联合处理检索和修改任务时，因此评估RAG在企业自然语言接口中的表现。

Method: 对标准RAG、Self - RAG和CoRAG三种RAG变体进行评估，以SAP交易银行作为用例构建新测试数据集，在不同文档上下文下进行18种实验配置。

Result: 无检索时所有任务精确匹配准确率为0%，检索使执行准确率最高达79.30%，组件匹配准确率最高达78.86%；CoRAG在混合文档设置中最稳健，在组合任务中表现更优。

Conclusion: 检索策略设计是生产级自然语言接口的关键决定因素，迭代查询分解在文档异质性下表现优于其他检索方法。

Abstract: Enterprise systems increasingly require natural language interfaces that can translate user requests into structured operations such as SQL queries and REST API calls. While large language models (LLMs) show promise for code generation [Chen et al., 2021; Huynh and Lin, 2025], their effectiveness in domain-specific enterprise contexts remains underexplored, particularly when both retrieval and modification tasks must be handled jointly. This paper presents a comprehensive evaluation of three retrieval-augmented generation (RAG) variants [Lewis et al., 2021] -- standard RAG, Self-RAG [Asai et al., 2024], and CoRAG [Wang et al., 2025] -- across SQL query generation, REST API call generation, and a combined task requiring dynamic task classification. Using SAP Transactional Banking as a realistic enterprise use case, we construct a novel test dataset covering both modalities and evaluate 18 experimental configurations under database-only, API-only, and hybrid documentation contexts. Results demonstrate that RAG is essential: Without retrieval, exact match accuracy is 0% across all tasks, whereas retrieval yields substantial gains in execution accuracy (up to 79.30%) and component match accuracy (up to 78.86%). Critically, CoRAG proves most robust in hybrid documentation settings, achieving statistically significant improvements in the combined task (10.29% exact match vs. 7.45% for standard RAG), driven primarily by superior SQL generation performance (15.32% vs. 11.56%). Our findings establish retrieval-policy design as a key determinant of production-grade natural language interfaces, showing that iterative query decomposition outperforms both top-k retrieval and binary relevance filtering under documentation heterogeneity.

</details>


### [476] [Architectural Anti-Patterns in Student-Developed Microservice Architectures: An Exploratory Study](https://arxiv.org/abs/2602.07147)
*Marco De Luca,Michele Perlotto,Anna Rita Fasolino,Porfirio Tramontana*

Main category: cs.SE

TL;DR: 研究分析学生开发的微服务，发现常见反模式并给出教学建议。


<details>
  <summary>Details</summary>
Motivation: 因分布式复杂性和产学差距，教学微服务架构有挑战，需了解学生在MSA中引入的质量问题以改进教育。

Method: 开展2023 - 2025年基于项目的课程，让216名硕士学生设计部署容器化MSA，用反模式分类法分析。

Result: 最终系统出现23种已知MSA反模式，安全问题最频繁，团队组织和服务交互问题次之，服务内设计和服务间分解问题较少。

Conclusion: 学生重功能交付轻健壮性和操作规范，建议强制执行最低标准、提供弹性通信实验、集成安全设计实践和提供CI - CD模板，贡献了教育经验和教学模型。

Abstract: Teaching microservice architectures is challenging due to distributed complexity and the gap between academia and industry. Understanding the quality issues students introduce in MSAs is essential to improve education. This study analyzes student-developed microservices using an established anti-pattern taxonomy and derives lessons learned with actionable teaching recommendations. We conducted a longitudinal, project-based course (2023-2025) involving 216 Master's students (67 teams) who designed and deployed a realistic, containerized MSA for a gamified testing platform. The final systems revealed 23 out of 58 known MSA anti-patterns, spanning five categories. Security issues were most frequent, highlighting weaknesses in authentication, authorization, and data protection. Team Organization and Service Interaction problems followed, reflecting limited DevOps experience and difficulties in inter-service coordination. Fewer issues appeared in Intra-service Design and Inter-service Decomposition, suggesting students generally defined service boundaries well. Overall, students prioritized feature delivery over robustness and operational discipline. To address this, we recommend enforcing minimal standards (API contracts, gateways), providing labs on resilient communication, integrating security-by-design practices, and offering CI-CD templates. The paper contributes a realistic, full-scale educational experience and a replicable model for teaching industry-aligned microservice architecture.

</details>


### [477] [Measuring Complexity at the Requirements Stage: Spectral Metrics as Development Effort Predictors](https://arxiv.org/abs/2602.07182)
*Maximilian Vierlboeck,Antonio Pugliese,Roshanak Nilchian,Paul Grogan,Rashika Sugganahalli Natesh Babu*

Main category: cs.SE

TL;DR: 本文针对需求规格中的结构复杂性缺乏理解和量化的问题，利用自然语言处理方法提取结构网络，通过实验发现谱测度和结构度量对集成工作量有较高相关性，而密度度量无显著预测有效性，弥合了架构复杂性分析和需求工程实践的方法差距。


<details>
  <summary>Details</summary>
Motivation: 工程系统复杂性带来诸多挑战，而需求规格中的结构复杂性缺乏理解和量化，且此阶段引入的复杂性会贯穿系统设计各阶段。

Method: 运用自然语言处理方法从文本需求中提取结构网络，以分子集成任务作为需求集成的同构代理进行对照实验，消除领域专业知识和语义歧义等干扰因素。

Result: 谱测度预测集成工作量的相关性超0.95，结构度量相关性超0.89，密度度量无显著预测有效性。

Conclusion: 特征值衍生的度量能捕捉简单连接度量无法捕捉的认知和工作量维度，研究弥合了架构复杂性分析和需求工程实践的方法差距，为需求工程应用这些度量提供验证基础。

Abstract: Complexity in engineered systems presents one of the most persistent challenges in modern development since it is driving cost overruns, schedule delays, and outright project failures. Yet while architectural complexity has been studied, the structural complexity embedded within requirements specifications remains poorly understood and inadequately quantified. This gap is consequential: requirements fundamentally drive system design, and complexity introduced at this stage propagates through architecture, implementation, and integration. To address this gap, we build on Natural Language Processing methods that extract structural networks from textual requirements. Using these extracted structures, we conducted a controlled experiment employing molecular integration tasks as structurally isomorphic proxies for requirements integration - leveraging the topological equivalence between molecular graphs and requirement networks while eliminating confounding factors such as domain expertise and semantic ambiguity. Our results demonstrate that spectral measures predict integration effort with correlations exceeding 0.95, while structural metrics achieve correlations above 0.89. Notably, density-based metrics show no significant predictive validity. These findings indicate that eigenvalue-derived measures capture cognitive and effort dimensions that simpler connectivity metrics cannot. As a result, this research bridges a critical methodological gap between architectural complexity analysis and requirements engineering practice, providing a validated foundation for applying these metrics to requirements engineering, where similar structural complexity patterns may predict integration effort.

</details>


### [478] [Automated Modernization of Machine Learning Engineering Notebooks for Reproducibility](https://arxiv.org/abs/2602.07195)
*Bihui Jin,Kaiyuan Wang,Pengyu Nie*

Main category: cs.SE

TL;DR: 交互式计算笔记本在机器学习工程中广泛使用，但环境侵蚀导致许多笔记本不可复现。通过研究发现回滚环境无益，提出LLM驱动的MLEModernizer框架，提升了笔记本的可复现性。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习工程中因环境侵蚀导致的笔记本不可复现问题，阻碍代码复用和科研进展。

Method: 研究79个热门Kaggle竞赛的12720个笔记本；设计并实现LLM驱动的MLEModernizer框架，迭代执行笔记本，进行三种类型的针对性修复。

Result: 研究发现仅35.4%的笔记本可复现，环境回滚无法提升可复现性；在7402个不可复现的笔记本上，MLEModernizer使5492个（74.2%）恢复可复现性。

Conclusion: MLEModernizer能在硬件和软件生态不断发展的情况下，让从业者验证、复用和维护机器学习工程工件。

Abstract: Interactive computational notebooks (e.g., Jupyter notebooks) are widely used in machine learning engineering (MLE) to program and share end-to-end pipelines, from data preparation to model training and evaluation. However, environment erosion-the rapid evolution of hardware and software ecosystems for machine learning-has rendered many published MLE notebooks non-reproducible in contemporary environments, hindering code reuse and scientific progress. To quantify this gap, we study 12,720 notebooks mined from 79 popular Kaggle competitions: only 35.4% remain reproducible today. Crucially, we find that environment backporting, i.e., downgrading dependencies to match the submission time, does not improve reproducibility but rather introduces additional failure modes.
  To address environment erosion, we design and implement MLEModernizer, an LLM-driven agentic framework that treats the contemporary environment as a fixed constraint and modernizes notebook code to restore reproducibility. MLEModernizer iteratively executes notebooks, collects execution feedback, and applies targeted fixes in three types: error-repair, runtime-reduction, and score-calibration. Evaluated on 7,402 notebooks that are non-reproducible under the baseline environment, MLEModernizer makes 5,492 (74.2%) reproducible. MLEModernizer enables practitioners to validate, reuse, and maintain MLE artifacts as the hardware and software ecosystems continue to evolve.

</details>


### [479] [Forecasting Developer Environments with GenAI: A Research Perspective](https://arxiv.org/abs/2602.07412)
*Raula Gaikovina Kula,Christoph Treude,Xing Hu,Sebastian Baltes,Earl T. Barr,Kelly Blincoe,Fabio Calefato,Junjie Chen,Marc Cheong,Youmei Fan,Daniel M. German,Marco Gerosa,Jin L. C. Guo,Shinpei Hayashi,Robert Hirschfeld,Reid Holmes,Yintong Huo,Takashi Kobayashi,Michele Lanza,Zhongxin Liu,Olivier Nourry,Nicole Novielli,Denys Poshyvanyk,Shinobu Saito,Kazumasa Shimari,Igor Steinmacher,Mairieli Wessel,Markus Wagner,Annie Vella,Laurie Williams,Xin Xia*

Main category: cs.SE

TL;DR: GenAI在多编程任务表现出色，可能改变IDE中人机交互，33位专家探讨其对IDE的影响，提出四个研究和实践关注主题。


<details>
  <summary>Details</summary>
Motivation: 探索GenAI对集成开发环境（IDE）的影响。

Method: 组织来自软件工程、人工智能和人机交互领域的33位专家，在为期四天的Shonan Meeting 222密集研究会议上进行讨论。

Result: 得出四个受研究者和从业者关注的主题。

Conclusion: 未明确提及。

Abstract: Generative Artificial Intelligence (GenAI) models are achieving remarkable performance in various tasks, including code generation, testing, code review, and program repair. The ability to increase the level of abstraction away from writing code has the potential to change the Human-AI interaction within the integrated development environment (IDE). To explore the impact of GenAI on IDEs, 33 experts from the Software Engineering, Artificial Intelligence, and Human-Computer Interaction domains gathered to discuss challenges and opportunities at Shonan Meeting 222, a four-day intensive research meeting. Four themes emerged as areas of interest for researchers and practitioners.

</details>


### [480] [Pull Requests as a Training Signal for Repo-Level Code Editing](https://arxiv.org/abs/2602.07457)
*Qinglin Zhu,Tianyu Chen,Shuai Lu,Lei Ji,Runcong Zhao,Murong Ma,Xiangxiang Dai,Yulan He,Lin Gui,Peng cheng,Yeyun Gong*

Main category: cs.SE

TL;DR: 提出Clean - PR训练范式，利用GitHub拉取请求训练模型进行仓库级代码编辑，在SWE - bench上表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 明确通过高质量训练信号能在多大程度上内化仓库级代码编辑能力，解决当前SWE - bench依赖复杂代理脚手架的问题。

Method: 提出Clean - PR范式，构建可扩展管道将拉取请求差异转换为编辑块，进行中期训练和无代理监督微调及错误驱动数据增强。

Result: 模型在SWE - bench上显著优于指令调优基线，SWE - bench Lite绝对提升13.6%，SWE - bench Verified绝对提升12.3%。

Conclusion: 在无复杂推理脚手架的简化无代理协议下，可将仓库级代码理解和编辑能力有效内化到模型权重中。

Abstract: Repository-level code editing requires models to understand complex dependencies and execute precise multi-file modifications across a large codebase. While recent gains on SWE-bench rely heavily on complex agent scaffolding, it remains unclear how much of this capability can be internalised via high-quality training signals. To address this, we propose Clean Pull Request (Clean-PR), a mid-training paradigm that leverages real-world GitHub pull requests as a training signal for repository-level editing. We introduce a scalable pipeline that converts noisy pull request diffs into Search/Replace edit blocks through reconstruction and validation, resulting in the largest publicly available corpus of 2 million pull requests spanning 12 programming languages. Using this training signal, we perform a mid-training stage followed by an agentless-aligned supervised fine-tuning process with error-driven data augmentation. On SWE-bench, our model significantly outperforms the instruction-tuned baseline, achieving absolute improvements of 13.6% on SWE-bench Lite and 12.3% on SWE-bench Verified. These results demonstrate that repository-level code understanding and editing capabilities can be effectively internalised into model weights under a simplified, agentless protocol, without relying on heavy inference-time scaffolding.

</details>


### [481] [ComPass: Contrastive Learning for Automated Patch Correctness Assessment in Program Repair](https://arxiv.org/abs/2602.07561)
*Quanjun Zhang,Ye Shang,Haichuan Hu,Chunrong Fang,Zhenyu Chen,Liang Xiao*

Main category: cs.SE

TL;DR: 本文提出基于PLM的APCA方法ComPass，利用对比学习和数据增强解决先前工作的技术限制，在真实世界补丁上实验表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有APR存在补丁过拟合问题，基于PLM的APCA方法虽有潜力但存在训练范式和数据集等局限。

Method: 利用代码转换规则为未标记预训练语料和标记微调补丁生成语义保留代码片段，用对比学习预训练PLMs，最后集成补丁代码片段的表示嵌入并与二元分类器联合微调PLMs。

Result: 在Defects4J的2274个真实世界补丁上实验，ComPass准确率达88.35%，显著优于基线APPT。

Conclusion: ComPass能有效解决先前基于PLM的APCA方法的技术限制，在评估补丁正确性上表现出色。

Abstract: Automated program repair (APR) attempts to reduce manual debugging efforts and plays a vital role in software maintenance. Despite remarkable progress, APR is still limited in generating overfitting patches, i.e., patches passing available test suites but incorrect. This issue, known as patch overfitting, has become a key concern in the APR community, with numerous approaches proposed to address it. Very recent work proposes a pre-trained language model (PLM)-based automated patch correctness assessment (APCA) approach, indicating the potential of such PLMs in reasoning about patch correctness. Despite being promising, it is still far from perfect due to various limitations, such as the training paradigm and training dataset. In this paper, we present ComPass, a PLM-based APCA approach that leverages contrastive learning and data augmentation to address the technical limitations of prior work. Our work is inspired by the opportunity to integrate contrastive learning with recent PLMs in the field of patch correctness assessment, where large-scale labeled patches are difficult to obtain. ComPass utilizes code transformation rules to generate semantic-preserving code snippets for both unlabeled pre-training corpus and labeled fine-tuning patches. ComPass then pre-trains PLMs with contrastive learning, which captures code features with the same semantics but different structures. ComPass finally integrates representation embeddings of patch code snippets and fine-tunes PLMs with a binary classifier jointly to assess patch code correctness. Experimental results on 2274 real-world patches from Defects4J demonstrate that ComPass achieves an accuracy of 88.35%, significantly outperforming state-of-the-art baseline APPT.

</details>


### [482] [Clarifying Core Dimensions in Digital Maturity Models: An Integrative Approach](https://arxiv.org/abs/2602.07569)
*Eduardo C. Peixoto,Hector Oliveira,Geber L. Ramalho,Cesar França*

Main category: cs.SE

TL;DR: 数字转型计划失败率高，数字成熟度模型（DMMs）有不足，本研究用系统映射法分析76个DMMs，提出常用维度的整合定义。


<details>
  <summary>Details</summary>
Motivation: 数字转型计划失败率高，DMMs存在维度相关度差异大、定义不清晰、组件不确定等问题，需更清晰理解DMMs。

Method: 采用系统映射法，包括自动搜索和滚雪球技术，分析76个DMMs以回答两个研究问题。

Result: 调和了十个最常用维度的不同解释，并为每个维度提出整合定义。

Conclusion: 与以往分析相比，本研究对数字成熟度模型提供了更广泛和更新的视角。

Abstract: Digital Transformation (DT) initiatives frequently face high failure rates, and while Digital Maturity Models (DMMs) offer potential solutions, they have notable shortcomings. Specifically, there is significant disparity in the dimensions considered relevant, a lack of clarity in their definitions, and uncertainty regarding their components. This study aims to provide a clearer understanding of DMMs by proposing integrative definitions of the most frequently used dimensions. Using a Systematic Mapping approach, including automatic search and snowballing techniques, we analyzed 76 DMMs to answer two Research Questions: (RQ1) What are the most frequent dimensions in DMMs? and (RQ2) How are these dimensions described, including their components? We reconcile varying interpretations of the ten most frequent dimensions -- Organization, Strategy, Technology, Culture, Process, Operations, People, Management, Customer, and Data -- and propose integrative definitions for each. Compared to previous analyses, this study provides a broader and more recent perspective on Digital Maturity Models.

</details>


### [483] [A Course on the Introduction to Quantum Software Engineering: Experience Report](https://arxiv.org/abs/2602.07589)
*Andriy Miranskyy*

Main category: cs.SE

TL;DR: 本文介绍了一门从软件工程视角教授量子计算的跨学科课程，学生虽前期接触少，但建立基础理解后能有效参与学习，还贡献了课程设计等。


<details>
  <summary>Details</summary>
Motivation: 当前量子计算教育多关注算法或框架层面，缺乏软件工程相关内容，因此设计此课程。

Method: 将量子基础概念与软件工程观点结合，通过教师观察、学生反馈、调查和学生作业分析获取证据。

Result: 尽管学生前期对量子计算接触少，但建立基础理解后能有效参与量子软件工程主题学习。

Conclusion: 贡献了模块化课程设计、可扩展评估模型以及为软件工程教育者开发量子计算课程提供可借鉴经验。

Abstract: Quantum computing is increasingly practiced through programming, yet most educational offerings emphasize algorithmic or framework-level use rather than software engineering concerns such as testing, abstraction, tooling, and lifecycle management.
  This paper reports on the design and first offering of a cross-listed undergraduate--graduate course that frames quantum computing through a software engineering lens, focusing on early-stage competence relevant to software engineering practice. The course integrates foundational quantum concepts with software engineering perspectives, emphasizing executable artifacts, empirical reasoning, and trade-offs arising from probabilistic behaviour, noise, and evolving toolchains. Evidence is drawn from instructor observations, student feedback, surveys, and analysis of student work.
  Despite minimal prior exposure to quantum computing, students were able to engage productively with quantum software engineering topics once a foundational understanding of quantum information and quantum algorithms, expressed through executable artifacts, was established. This experience report contributes a modular course design, a scalable assessment model for mixed academic levels, and transferable lessons for software engineering educators developing quantum computing curricula.

</details>


### [484] [Evaluating Large Language Models for Detecting Architectural Decision Violations](https://arxiv.org/abs/2602.07609)
*Ruoyu Su,Alexander Bakhtin,Noman Ahmad,Matteo Esposito,Valentina Lenarduzzi,Davide Taibi*

Main category: cs.SE

TL;DR: 研究LLMs识别开源系统中架构决策违规的有效性，发现对显式、可从代码推断的决策效果好，对隐式或部署导向决策不足。


<details>
  <summary>Details</summary>
Motivation: ADRs对维护软件架构质量重要，但缺乏系统文档和自动检测机制，LLMs为自动架构推理带来新可能，需研究其识别决策违规的有效性。

Method: 用多模型管道分析109个GitHub仓库的980个ADRs，一个LLM初筛，三个LLM独立验证推理，评估多项指标并结合专家评估。

Result: 模型对显式、可从代码推断的决策有较高一致性和准确性，对隐式或部署导向决策准确性不足。

Conclusion: LLMs能支持架构决策合规性验证，但不能替代人类专业知识处理非代码相关决策。

Abstract: Architectural Decision Records (ADRs) play a central role in maintaining software architecture quality, yet many decision violations go unnoticed because projects lack both systematic documentation and automated detection mechanisms. Recent advances in Large Language Models (LLMs) open up new possibilities for automating architectural reasoning at scale. We investigated how effectively LLMs can identify decision violations in open-source systems by examining their agreement, accuracy, and inherent limitations. Our study analyzed 980 ADRs across 109 GitHub repositories using a multi-model pipeline in which one LLM primary screens potential decision violations, and three additional LLMs independently validate the reasoning. We assessed agreement, accuracy, precision, and recall, and complemented the quantitative findings with expert evaluation. The models achieved substantial agreement and strong accuracy for explicit, code-inferable decisions. Accuracy falls short for implicit or deployment-oriented decisions that depend on deployment configuration or organizational knowledge. Therefore, LLMs can meaningfully support validation of architectural decision compliance; however, they are not yet replacing human expertise for decisions not focused on code.

</details>


### [485] [Debugging code world models](https://arxiv.org/abs/2602.07672)
*Babak Rahmani*

Main category: cs.SE

TL;DR: 研究代码世界模型（CWMs），找出两种主要失败机制，为更高效监督和状态表示指明方向。


<details>
  <summary>Details</summary>
Motivation: 当前对CWMs错误来源和局限性理解不足，需要深入研究。

Method: 从局部语义执行和长程状态跟踪两个互补视角研究CWMs，使用真实代码基准和受控排列跟踪基准。

Result: 发现两种主要失败机制，一是长执行历史程序会耗尽token预算，二是失败集中在字符串值状态；长程性能下降主要由错误动作生成导致。

Conclusion: 研究结果为CWMs中更高效监督和状态表示提供方向，使其更好与程序执行和数据类型对齐。

Abstract: Code World Models (CWMs) are language models trained to simulate program execution by predicting explicit runtime state after every executed command. This execution-based world modeling enables internal verification within the model, offering an alternative to natural language chain-of-thought reasoning. However, the sources of errors and the nature of CWMs' limitations remain poorly understood. We study CWMs from two complementary perspectives: local semantic execution and long-horizon state tracking. On real-code benchmarks, we identify two dominant failure regimes. First, dense runtime state reveals produce token-intensive execution traces, leading to token-budget exhaustion on programs with long execution histories. Second, failures disproportionately concentrate in string-valued state, which we attribute to limitations of subword tokenization rather than program structure. To study long-horizon behavior, we use a controlled permutation-tracking benchmark that isolates state propagation under action execution. We show that long-horizon degradation is driven primarily by incorrect action generation: when actions are replaced with ground-truth commands, a Transformer-based CWM propagates state accurately over long horizons, despite known limitations of Transformers in long-horizon state tracking. These findings suggest directions for more efficient supervision and state representations in CWMs that are better aligned with program execution and data types.

</details>


### [486] [On Sequence-to-Sequence Models for Automated Log Parsing](https://arxiv.org/abs/2602.07698)
*Adam Sorrenti,Andriy Miranskyy*

Main category: cs.SE

TL;DR: 研究系统性评估多种因素对自动化日志解析的影响，对比四种序列建模架构。Transformer错误率最低，Mamba计算成本低且有竞争力，研究为相关人员提供实用指南。


<details>
  <summary>Details</summary>
Motivation: 因日志格式异构、数据分布偏移和基于规则方法的脆弱性，自动化日志解析仍具挑战，需评估多种因素对其性能和计算成本的影响。

Method: 进行对照实证研究，比较Transformer、Mamba、单向LSTM和双向LSTM四种序列建模架构，在多个数据集上训练396个模型，用相对Levenshtein编辑距离评估并进行统计显著性测试。

Result: Transformer平均相对编辑距离最低，Mamba计算成本低且准确性有竞争力，字符级分词提升性能，序列长度对Transformer准确性影响小，Mamba和Transformer样本效率高于循环模型。

Conclusion: Transformers降低解析错误23.4%，Mamba在数据或计算资源受限时有优势，研究明确多种因素作用，为研究和实践提供指导。

Abstract: Log parsing is a critical standard operating procedure in software systems, enabling monitoring, anomaly detection, and failure diagnosis. However, automated log parsing remains challenging due to heterogeneous log formats, distribution shifts between training and deployment data, and the brittleness of rule-based approaches. This study aims to systematically evaluate how sequence modelling architecture, representation choice, sequence length, and training data availability influence automated log parsing performance and computational cost. We conduct a controlled empirical study comparing four sequence modelling architectures: Transformer, Mamba state-space, monodirectional LSTM, and bidirectional LSTM models. In total, 396 models are trained across multiple dataset configurations and evaluated using relative Levenshtein edit distance with statistical significance testing. Transformer achieves the lowest mean relative edit distance (0.111), followed by Mamba (0.145), mono-LSTM (0.186), and bi-LSTM (0.265), where lower values are better. Mamba provides competitive accuracy with substantially lower computational cost. Character-level tokenization generally improves performance, sequence length has negligible practical impact on Transformer accuracy, and both Mamba and Transformer demonstrate stronger sample efficiency than recurrent models. Overall, Transformers reduce parsing error by 23.4%, while Mamba is a strong alternative under data or compute constraints. These results also clarify the roles of representation choice, sequence length, and sample efficiency, providing practical guidance for researchers and practitioners.

</details>


### [487] [Still Manual? Automated Linter Configuration via DSL-Based LLM Compilation of Coding Standards](https://arxiv.org/abs/2602.07783)
*Zejun Zhang,Yixin Gan,Zhenchang Xing,Tian Zhang,Yi Li,Xiwei Xu,Qinghua Lu,Liming Zhu*

Main category: cs.SE

TL;DR: 提出LintCFG方法自动化代码检查工具配置生成，不受语言、标准和工具限制，实验效果好、效率高且通用性强。


<details>
  <summary>Details</summary>
Motivation: 手动配置代码检查工具复杂且维护成本高，需减少人工工作量。

Method: 设计DSL表达编码规则，将检查工具配置转为DSL指令，编译自然语言标准生成特定配置。

Result: Java实验中DSL表示精确率和召回率超90%，细粒度配置多项指标近70%，精度超基线100%，用户研究显示提高效率，还能生成JavaScript配置。

Conclusion: LintCFG方法可有效自动化代码检查工具配置生成，具有广泛适用性。

Abstract: Coding standards are essential for maintaining consistent and high-quality code across teams and projects. Linters help developers enforce these standards by detecting code violations. However, manual linter configuration is complex and expertise-intensive, and the diversity and evolution of programming languages, coding standards, and linters lead to repetitive and maintenance-intensive configuration work. To reduce manual effort, we propose LintCFG, a domain-specific language (DSL)-driven, LLM-based compilation approach to automate linter configuration generation for coding standards, independent of programming languages, coding standards, and linters. Inspired by compiler design, we first design a DSL to express coding rules in a tool-agnostic, structured, readable, and precise manner. Then, we build linter configurations into DSL configuration instructions. For a given natural language coding standard, the compilation process parses it into DSL coding standards, matches them with the DSL configuration instructions to set configuration names, option names and values, verifies consistency between the standards and configurations, and finally generates linter-specific configurations. Experiments with Checkstyle for Java coding standard show that our approach achieves over 90% precision and recall in DSL representation, with accuracy, precision, recall, and F1-scores close to 70% (with some exceeding 70%) in fine-grained linter configuration generation. Notably, our approach outperforms baselines by over 100% in precision. A user study further shows that our approach improves developers' efficiency in configuring linters for coding standards. Finally, we demonstrate the generality of the approach by generating ESLint configurations for JavaScript coding standards, showcasing its broad applicability across other programming languages, coding standards, and linters.

</details>


### [488] [Software Space Analytics: Towards Visualization and Statistics of Internal Software Execution](https://arxiv.org/abs/2602.07821)
*Shinobu Saito*

Main category: cs.SE

TL;DR: 本文将空间统计应用于软件内部执行数据评估，以辅助软件维护中模块识别，还探讨其在软件工程领域作用与挑战。


<details>
  <summary>Details</summary>
Motivation: 在软件维护中，除用户请求和错误报告，评估软件内模块执行状态很重要，需新方法。

Method: 定义软件空间数据集，基于模块调用关系将软件内部结构视为空间；用空间统计进行空间聚类可视化和统计测试。

Result: 未提及具体结果。

Conclusion: 探讨了空间统计在软件工程领域的有用性和未来挑战。

Abstract: In software maintenance work, software architects and programmers need to identify modules that require modification or deletion. Whilst user requests and bug reports are utilised for this purpose, evaluating the execution status of modules within the software is also crucial. This paper, therefore, applies spatial statistics to assess internal software execution data. First, we define a software space dataset, viewing the software's internal structure as a space based on module call relationships. Then, using spatial statistics, we conduct the visualization of spatial clusters and the statistical testing using spatial measures. Finally, we consider the usefulness of spatial statistics in the software engineering domain and future challenges.

</details>


### [489] [HerAgent: Rethinking the Automated Environment Deployment via Hierarchical Test Pyramid](https://arxiv.org/abs/2602.07871)
*Xiang Li,Siyu Lu,Sarro Federica,Claire Le Goues,He Ye*

Main category: cs.SE

TL;DR: 现有软件环境自动搭建评估方法不佳，本文提出环境成熟度层次结构和HerAgent方法，在基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有利用大语言模型进行自动化软件环境搭建的工作评估信号弱，不能确保项目实际运行，需更好的评估和搭建方法。

Method: 引入环境成熟度层次结构，定义三个成功级别；提出HerAgent方法，通过基于执行的验证和修复逐步构建可执行环境。

Result: 在四个公共基准测试中，HerAgent优于所有相关工作，最高提升79.6%；在复杂C/C++项目上超越先前方法66.7%；还解决了11 - 30个先前方法无法配置的环境实例。

Conclusion: 基于执行证据评估环境搭建成功更有效，HerAgent方法在软件环境自动搭建方面表现良好。

Abstract: Automated software environment setup is a prerequisite for testing, debugging, and reproducing failures, yet remains challenging in practice due to complex dependencies, heterogeneous build systems, and incomplete documentation. Recent work leverages large language models to automate this process, but typically evaluates success using weak signals such as dependency installation or partial test execution, which do not ensure that a project can actually run. In this paper, we argue that environment setup success should be evaluated through executable evidence rather than a single binary signal. We introduce the Environment Maturity Hierarchy, which defines three success levels based on progressively stronger execution requirements, culminating in successful execution of a project's main entry point. Guided by this hierarchy, we propose HerAgent, an automated environment setup approach that incrementally constructs executable environments through execution-based validation and repair. We evaluate HerAgent on four public benchmarks, where it outperforms all related work, achieving up to 79.6\% improvement due to its holistic understanding of project structure and dependencies. On complex C/C++ projects, HerAgent surpasses prior approaches by 66.7\%. In addition, HerAgent uniquely resolves 11-30 environment instances across the benchmarks that no prior method can configure.

</details>


### [490] [Rethinking Code Complexity Through the Lens of Large Language Models](https://arxiv.org/abs/2602.07882)
*Chen Xie,Yuling Shi,Xiaodong Gu,Beijun Shen*

Main category: cs.SE

TL;DR: 研究传统代码复杂度指标与大语言模型处理代码难度的关联，发现不相关，提出新指标LM - CC，实验显示其优势。


<details>
  <summary>Details</summary>
Motivation: 探究传统代码复杂度指标是否能有效刻画大语言模型处理代码时的难度，发现存在差距。

Method: 从大语言模型角度出发，基于程序语义的非线性，将程序按熵分解为语义单元，构建层次结构，量化复杂度。

Result: LM - CC与大语言模型性能的相关性比传统指标更强，降低该指标能直接提升任务性能。

Conclusion: 传统代码复杂度指标与大语言模型感知的难度不匹配，新指标LM - CC更有效。

Abstract: Code complexity metrics such as cyclomatic complexity have long been used to assess software quality and maintainability. With the rapid advancement of large language models (LLMs) on code understanding and generation tasks, an important yet underexplored question arises: do these traditional complexity metrics meaningfully characterize the difficulty LLMs experience when processing code? In this work, we empirically demonstrate that, after controlling for code length, classical metrics exhibit no consistent correlation with LLM performance, revealing a fundamental mismatch with model-perceived difficulty. To address this gap, we propose LM-CC, a novel code complexity metric designed from the perspective of LLMs. The core premise of LM-CC is that LLM-perceived difficulty is driven by the nonlinearity of program semantics. Accordingly, we decompose programs into semantic units based on entropy, organize these units into a compositional hierarchy, and quantify complexity as a principled aggregation of compositional level and branching-induced divergence, capturing cumulative model uncertainty during code processing. Our extensive experiments show that LM-CC not only correlates more strongly with LLM performance than traditional metrics but also that lowering it directly enhances task performance.

</details>


### [491] [Is Your Private Information Logged? An Empirical Study on Android App Logs](https://arxiv.org/abs/2602.07893)
*Zhiyuan Chen,Soham Sanjay Deo,Poorna Chander Reddy Puttaparthi,Vanessa Nava-Camal,Yiming Tang,Xueling Zhang,Weiyi Shang*

Main category: cs.SE

TL;DR: 本文构建安卓应用日志数据集，研究其隐私泄露情况，揭示开发者担忧类别和泄露普遍性，并给出隐私保护建议。


<details>
  <summary>Details</summary>
Motivation: 随着移动应用快速增长，用户隐私担忧凸显，以往研究缺乏对安卓应用日志隐私泄露的全面审视。

Method: 构建安卓应用日志综合数据集，从了解开发者担忧、研究日志隐私泄露、探究泄露日志特征及原因三方面开展实证研究。

Result: 发现开发者对软件日志隐私问题的五类担忧，安卓应用日志普遍存在隐私泄露，多因开发者未察觉。

Conclusion: 研究为开发者提供保护隐私不被记录的建议。

Abstract: With the rapid growth of mobile apps, users' concerns about their privacy have become increasingly prominent. Android app logs serve as crucial computer resources, aiding developers in debugging and monitoring the status of Android apps, while also containing a wealth of software system information. Previous studies have acknowledged privacy leaks in software logs and Android apps as significant issues without providing a comprehensive view of the privacy leaks in Android app logs. In this study, we build a comprehensive dataset of Android app logs and conduct an empirical study to analyze the status and severity of privacy leaks in Android app logs. Our study comprises three aspects: (1) Understanding real-world developers' concerns regarding privacy issues related to software logs; (2) Studying privacy leaks in the Android app logs; (3) Investigating the characteristics of privacy-leaking Android app logs and analyzing the reasons behind them. Our study reveals five different categories of concerns from real-world developers regarding privacy issues related to software logs and the prevalence of privacy leaks in Android app logs, with the majority stemming from developers' unawareness of such leaks. Additionally, our study provides developers with suggestions to safeguard their privacy from being logged.

</details>


### [492] [Rethinking the Value of Agent-Generated Tests for LLM-Based Software Engineering Agents](https://arxiv.org/abs/2602.07900)
*Zhi Chen,Zhensu Sun,Yuling Shi,Chao Peng,Xiaodong Gu,David Lo,Lingxiao Jiang*

Main category: cs.SE

TL;DR: 研究大语言模型代码代理编写测试对解决代码问题的影响，发现当前测试编写实践效用有限。


<details>
  <summary>Details</summary>
Motivation: 探究代理编写的测试是否能有效提升问题解决能力，还是仅模仿人类测试并消耗交互预算。

Method: 对SWE - bench Verified上六个最先进的大语言模型的代理轨迹进行实证研究，并对四个代理的提示进行修改以进行对照实验。

Result: 同一模型中已解决和未解决任务的测试编写频率相似，代理更倾向于使用打印语句，改变测试编写量对最终结果影响不大。

Conclusion: 当前测试编写实践在自主软件工程任务中的效用有限。

Abstract: Large Language Model (LLM) code agents increasingly resolve repository-level issues by iteratively editing code, invoking tools, and validating candidate patches. In these workflows, agents often write tests on the fly, a paradigm adopted by many high-ranking agents on the SWE-bench leaderboard. However, we observe that GPT-5.2, which writes almost no new tests, can even achieve performance comparable to top-ranking agents. This raises the critical question: whether such tests meaningfully improve issue resolution or merely mimic human testing practices while consuming a substantial interaction budget.
  To reveal the impact of agent-written tests, we present an empirical study that analyzes agent trajectories across six state-of-the-art LLMs on SWE-bench Verified. Our results show that while test writing is commonly adopted, but resolved and unresolved tasks within the same model exhibit similar test-writing frequencies Furthermore, these tests typically serve as observational feedback channels, where agents prefer value-revealing print statements significantly more than formal assertion-based checks. Based on these insights, we perform a controlled experiment by revising the prompts of four agents to either increase or reduce test writing. The results suggest that changes in the volume of agent-written tests do not significantly change final outcomes. Taken together, our study reveals that current test-writing practices may provide marginal utility in autonomous software engineering tasks.

</details>


### [493] [Agent Skills: A Data-Driven Analysis of Claude Skills for Extending Large Language Model Functionality](https://arxiv.org/abs/2602.08004)
*George Ling,Shanshan Zhong,Richard Huang*

Main category: cs.SE

TL;DR: 对市场上40285个公开技能进行大规模分析，揭示技能发布、内容、供需、预算、生态等情况及安全风险。


<details>
  <summary>Details</summary>
Motivation: 随着代理技能在公共市场激增，不清楚其类型、用户采用情况和风险。

Method: 对主要市场上40285个公开列出的技能进行大规模、数据驱动分析。

Result: 技能发布呈短期爆发式，内容集中在软件工程工作流，信息检索和内容创作采用率高，存在供需不平衡，多数技能在提示预算范围内，生态同质性强，存在安全风险。

Conclusion: 研究为代理技能这一新兴基础设施层提供定量快照，为技能重用、标准化和安全设计的未来工作提供参考。

Abstract: Agent skills extend large language model (LLM) agents with reusable, program-like modules that define triggering conditions, procedural logic, and tool interactions. As these skills proliferate in public marketplaces, it is unclear what types are available, how users adopt them, and what risks they pose. To answer these questions, we conduct a large-scale, data-driven analysis of 40,285 publicly listed skills from a major marketplace. Our results show that skill publication tends to occur in short bursts that track shifts in community attention. We also find that skill content is highly concentrated in software engineering workflows, while information retrieval and content creation account for a substantial share of adoption. Beyond content trends, we uncover a pronounced supply-demand imbalance across categories, and we show that most skills remain within typical prompt budgets despite a heavy-tailed length distribution. Finally, we observe strong ecosystem homogeneity, with widespread intent-level redundancy, and we identify non-trivial safety risks, including skills that enable state-changing or system-level actions. Overall, our findings provide a quantitative snapshot of agent skills as an emerging infrastructure layer for agents and inform future work on skill reuse, standardization, and safety-aware design.

</details>


### [494] [Bridging the Gap: Adapting Evidence to Decision Frameworks to support the link between Software Engineering academia and industry](https://arxiv.org/abs/2602.08015)
*Patricia G. F. Matsubara,Tayana Conte*

Main category: cs.SE

TL;DR: 提出引入健康科学领域的EtD框架解决软件工程SLR结果难达从业者的问题，并给出示例和挑战。


<details>
  <summary>Details</summary>
Motivation: 软件工程领域虽开展诸多系统文献综述（SLR），但结果难达从业者，故寻求解决办法。

Method: 引入健康科学领域的Evidence to Decision (EtD)框架，给出基于软件工程SLR的示例。

Result: 介绍了EtD框架及相关见解和示例。

Conclusion: 软件工程研究和实践社区采用EtD框架有挑战，建议需更全面标准。

Abstract: Over twenty years ago, the Software Engineering (SE) research community have been involved with Evidence-Based Software Engineering (EBSE). EBSE aims to inform industrial practice with the best evidence from rigorous research, preferably from systematic literature reviews (SLRs). Since then, SE researchers have conducted many SLRs, perfected their SLR procedures, proposed alternative ways of presenting their results (such as Evidence Briefings), and profusely discussed how to conduct research that impacts practice. Nevertheless, there is still a feeling that SLRs' results are not reaching practitioners. Something is missing. In this vision paper, we introduce Evidence to Decision (EtD) frameworks from the health sciences, which propose gathering experts in panels to assess the existing best evidence about the impact of an intervention in all relevant outcomes and make structured recommendations based on them. The insight we can leverage from EtD frameworks is not their structure per se but all the relevant criteria for making recommendations to practitioners from SLRs. Furthermore, we provide a worked example based on an SE SLR. We also discuss the challenges the SE research and practice community may face when adopting EtD frameworks, highlighting the need for more comprehensive criteria in our recommendations to industry practitioners.

</details>


### [495] [Outsourcing in Global Software Development: Effects of Temporal Location and Methodologies](https://arxiv.org/abs/2602.08084)
*Mark Looi,Marc Szepan*

Main category: cs.SE

TL;DR: 研究全球软件外包中时间距离和开发方法对项目成果的影响，发现近岸开发更有利，开发方法仅影响成本，并给出建议。


<details>
  <summary>Details</summary>
Motivation: 全球软件外包项目团队分布在不同时区，研究时间距离和开发方法对项目成果的影响。

Method: 对80个客户进行调查，对其中6个进行访谈。

Result: 近岸开发在整体成功、质量等方面更有优势，开发方法仅影响成本。

Conclusion: 建议通信密集型或敏捷项目选择近岸开发。

Abstract: Developing software globally using outsourced resources has become a common practice, with project teams often distributed in different time zones. In this study, we focus on customers that contract software development to vendors in temporally nearshore or far offshore locations. We conducted a survey to determine the effect of temporal distance on overall success, costs, project management effort, schedule, quality, communication problems, and other outcomes of interest to managers. In the survey of 80 customers and interviews with 6 of them, we also investigated the effect of software development methodology on the same outcomes. The results show that nearshore development is advantageous for overall success, quality, reduced PM effort, maintaining schedule, higher quality, and engendering fewer communication problems. Development methodology appears to only influence higher costs. We assess our findings in the context of prior GSE research and provide practical advice for customers of outsourced global software development, chief of which is to favor nearshore for communication-intensive or Agile projects.

</details>


### [496] [Integrating Code Metrics into Automated Documentation Generation for Computational Notebooks](https://arxiv.org/abs/2602.08133)
*Mojtaba Mostafavi Ghahfarokhi,Hamed Jahantigh,Alireza Asadi,Abbas Heydarnoori*

Main category: cs.SE

TL;DR: 本文研究将源代码指标作为辅助信号用于自动文档生成，提出两阶段方法，结果显示纳入代码指标能提升生成文档的质量。


<details>
  <summary>Details</summary>
Motivation: 有效代码文档对软件开发很重要，但开发者常忽视，现有自动文档生成方法忽略代码结构和定量特征，计算笔记本文档不一致，因此研究代码指标在自动文档生成中的作用。

Method: 提出两阶段方法，一是优化CodeSearchNet数据集构建过程提取高质量代码-文档对，二是评估有无指标信息下两种建模范式（轻量级CNN - RNN架构和少样本GPT - 3.5架构）。

Result: 纳入代码指标提高了生成文档的准确性和上下文相关性，CNN - RNN架构的BLEU - 1提升6%、ROUGE - L F1提升3%，LLM架构的BERTScore F1提升9%。

Conclusion: 整合代码指标能提供有价值的结构上下文，提升不同模型家族的自动文档生成效果。

Abstract: Effective code documentation is essential for collaboration, comprehension, and long-term software maintainability, yet developers often neglect it due to its repetitive nature. Automated documentation generation has evolved from heuristic and rule-based methods to neural network-based and large language model (LLM)-based approaches. However, existing methods often overlook structural and quantitative characteristics of code that influence readability and comprehension. Prior research suggests that code metrics capture information relevant to program understanding. Building on these insights, this paper investigates the role of source code metrics as auxiliary signals for automated documentation generation, focusing on computational notebooks, a popular medium among data scientists that integrates code, narrative, and results but suffers from inconsistent documentation. We propose a two-stage approach. First, the CodeSearchNet dataset construction process was refined to create a specialized dataset from over 17 million code and markdown cells. After structural and semantic filtering, approximately 36,734 high-quality (code, markdown) pairs were extracted. Second, two modeling paradigms, a lightweight CNN-RNN architecture and a few-shot GPT-3.5 architecture, were evaluated with and without metric information. Results show that incorporating code metrics improves the accuracy and contextual relevance of generated documentation, yielding gains of 6% in BLEU-1 and 3% in ROUGE-L F1 for CNN-RNN-based architecture, and 9% in BERTScore F1 for LLM-based architecture. These findings demonstrate that integrating code metrics provides valuable structural context, enhancing automated documentation generation across diverse model families.

</details>


### [497] [Test vs Mutant: Adversarial LLM Agents for Robust Unit Test Generation](https://arxiv.org/abs/2602.08146)
*Pengyu Chang,Yixiong Fang,Silin Chen,Yuling Shi,Beijun Shen,Xiaodong Gu*

Main category: cs.SE

TL;DR: 提出AdverTest框架用于LLM驱动的测试用例生成，实验表明其提升了故障检测率和覆盖率。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注测试覆盖率和可读性，较少关注增强漏洞检测的鲁棒性，尤其是暴露边缘情况和易受攻击的执行路径。

Method: 提出AdverTest框架，包含测试用例生成代理T和变异体生成代理M，二者进行对抗循环，由覆盖率和变异分数引导。

Result: 在Defects4J数据集上，相比现有的最佳LLM方法，故障检测率提高8.56%，相比EvoSuite提高63.30%，同时提高了行和分支覆盖率。

Conclusion: AdverTest框架能有效提升故障检测能力和覆盖率。

Abstract: Software testing is a critical, yet resource-intensive phase of the software development lifecycle. Over the years, various automated tools have been developed to aid in this process. Search-based approaches typically achieve high coverage but produce tests with low readability, whereas large language model (LLM)-based methods generate more human-readable tests but often suffer from low coverage and compilability. While the majority of research efforts have focused on improving test coverage and readability, little attention has been paid to enhancing the robustness of bug detection, particularly in exposing corner cases and vulnerable execution paths. To address this gap, we propose AdverTest, a novel adversarial framework for LLM-powered test case generation. AdverTest comprises two interacting agents: a test case generation agent (T) and a mutant generation agent (M). These agents engage in an adversarial loop, where M persistently creates new mutants "hacking" the blind spots of T's current test suite, while T iteratively refines its test cases to "kill" the challenging mutants produced by M. This interaction loop is guided by both coverage and mutation scores, enabling the system to co-evolve toward both high test coverage and bug detection capability. Experimental results in the Defects4J dataset show that our approach improves fault detection rates by 8.56% over the best existing LLM-based methods and by 63.30% over EvoSuite, while also improving line and branch coverage.

</details>


### [498] [Distributed Architecture Reconstruction of Polyglot and Multi-Repository Microservice Projects](https://arxiv.org/abs/2602.08166)
*Oscar Manglaras,Alex Farkas,Thomas Woolford,Christoph Treude,Markus Wagner*

Main category: cs.SE

TL;DR: 本文提出用于静态架构重建的新框架，支持特定技术分析模块和多仓库环境下的分布式架构重建，且与现有工具算法可互操作。


<details>
  <summary>Details</summary>
Motivation: 微服务架构增加架构复杂性，准确文档维护困难，现有静态架构重建方法存在技术限制、单仓库约束或高实施障碍等问题。

Method: 提出支持特定技术分析模块（提取器）的框架，描述核心设计概念和算法，使框架与现有静态分析工具和算法可互操作。

Result: 构建出支持多仓库环境分布式架构重建、与现有工具算法可互操作的静态架构重建框架。

Conclusion: 新的静态架构重建框架可解决现有方法的不足，能更好地满足微服务架构下准确文档维护的需求。

Abstract: Microservice architectures encourage the use of small, independently developed services; however, this can lead to increased architectural complexity. Accurate documentation is crucial, but is challenging to maintain due to the rapid, independent evolution of services. While static architecture reconstruction provides a way to maintain up-to-date documentation, existing approaches suffer from technology limitations, mono-repo constraints, or high implementation barriers. This paper presents a novel framework for static architecture reconstruction that supports technology-specific analysis modules, called \emph{extractors}, and supports \emph{distributed architecture reconstruction} in multi-repo environments. We describe the core design concepts and algorithms that govern how extractors are executed, how data is passed between them, and how their outputs are unified. Furthermore, the framework is interoperable with existing static analysis tools and algorithms, allowing them to be invoked from or embedded within extractors.

</details>


### [499] [ModARO: A Modular Approach to Architecture Reconstruction of Distributed Microservice Codebases](https://arxiv.org/abs/2602.08181)
*Oscar Manglaras,Alex Farkas,Thomas Woolford,Christoph Treude,Markus Wagner*

Main category: cs.SE

TL;DR: 本文提出ModARO方法用于微服务架构重建，可编写模块化重建代码并跨项目复用，通过配置重建10个开源项目验证有效性，经用户研究证明其有用性和可用性。


<details>
  <summary>Details</summary>
Motivation: 微服务架构增加整体复杂性，快速独立开发服务带来架构漂移风险且不利于文档维护，自动架构重建代码难以跨项目复用，拆分微服务到不同仓库使架构细节重建更复杂。

Method: 提出ModARO方法，允许为任何技术编写模块化重建代码（提取器）并跨项目复用，不受技术栈和代码库分割影响。

Result: 配置ModARO重建10个开源项目，在有8位行业从业者参与的用户研究中与先进基线对比，验证了其有效性、有用性和可用性。

Conclusion: 开发者可根据技术栈组装或创建提取器，跨仓库进行架构重建并集成到仓库CI/CD管道中。

Abstract: Microservice architectures promote small, independently developed services, but increase overall architectural complexity. It is crucial that developers understand the architecture and how changes to a service affect the overall system, but rapid and independent development of services increases the risk of architectural drift and discourages the creation and maintenance of documentation. Automatic architecture reconstruction can help avoid these issues, but it is difficult to reuse reconstruction code across multiple projects, as all use different combinations of technologies and project-specific conventions. Reconstruction of architecture-level details is further complicated by the tendency to split microservices into separate repositories, preventing a full view of the system from any one codebase. In this paper, we present and evaluate ModARO, an approach to microservice architecture reconstruction that allows writing modular reconstruction code ('extractors') for any technologies and reusing them across different projects, independent of the surrounding technology stack or whether or not the services are split into multiple codebases. We demonstrate the effectiveness of our approach by configuring ModARO to reconstruct 10 open source projects, and we validate the usefulness and usability of ModARO against a state-of-the-art baseline in a user study with 8 industry practitioners. Using this approach, developers can assemble or create extractors tailored to their technology stacks and distribute architecture reconstruction across repositories, enabling integration into repository CI/CD pipelines.

</details>


### [500] [Adoption of Large Language Models in Scrum Management: Insights from Brazilian Practitioners](https://arxiv.org/abs/2602.08192)
*Mirko Perkusich,Danyllo Albuquerque,Allysson Allex Araújo,Matheus Paixão,Rohit Gheyi,Marcos Kalinowski,Angelo Perkusich*

Main category: cs.SE

TL;DR: 研究通过对70位巴西专业人士的调查，探究大语言模型（LLMs）在Scrum管理活动中的应用，发现使用效果较好但有风险，并给出应用方向。


<details>
  <summary>Details</summary>
Motivation: 现有研究对LLMs在Scrum管理活动中的应用关注有限，故开展此研究。

Method: 对70位巴西专业人士进行调查。

Result: LLMs使用熟练度高且频繁，主要用于探索Scrum实践，但有“几乎正确”输出、保密担忧和幻觉等风险，主要好处是提高生产力和减少手动工作。

Conclusion: 该研究对LLMs在Scrum管理中的应用进行实证刻画，明确当前实践、量化利弊并给出应用和集成方向。

Abstract: Scrum is widely adopted in software project management due to its adaptability and collaborative nature. The recent emergence of Large Language Models (LLMs) has created new opportunities to support knowledge-intensive Scrum practices. However, existing research has largely focused on technical activities such as coding and testing, with limited evidence on the use of LLMs in management-related Scrum activities. In this study, we investigate the use of LLMs in Scrum management activities through a survey of 70 Brazilian professionals. Among them, 49 actively use Scrum, and 33 reported using LLM-based assistants in their Scrum practices. The results indicate a high level of proficiency and frequent use of LLMs, with 85% of respondents reporting intermediate or advanced proficiency and 52% using them daily. LLM use concentrates on exploring Scrum practices, with artifacts and events receiving targeted yet uneven support, whereas broader management tasks appear to be adopted more cautiously. The main benefits include increased productivity (78%) and reduced manual effort (75%). However, several critical risks remain, as respondents report 'almost correct' outputs (81%), confidentiality concerns (63%), and hallucinations during use (59%). This work provides one of the first empirical characterizations of LLM use in Scrum management, identifying current practices, quantifying benefits and risks, and outlining directions for responsible adoption and integration in Agile environments.

</details>


### [501] [Software Testing at the Network Layer: Automated HTTP API Quality Assessment and Security Analysis of Production Web Applications](https://arxiv.org/abs/2602.08242)
*Ali Hassaan Mughal,Muhammad Bilal*

Main category: cs.SE

TL;DR: 提出自动化软件测试框架分析18个生产网站HTTP流量，得出网站API调用质量差异大，确定常见反模式，提供开放工件。


<details>
  <summary>Details</summary>
Motivation: 现代Web应用客户端API调用网络交互质量缺乏系统测试，且存在安全隐患。

Method: 使用Playwright自动浏览器工具记录108个HAR文件，应用8个启发式反模式检测器为每个网站打分。

Result: 网站质量差异大，冗余API调用和缺少缓存头最普遍，第三方开销占比高，各网站请求数差异大。

Conclusion: 为现代Web的HTTP API调用质量建立实证基线，提供可复现测试框架。

Abstract: Modern web applications rely heavily on client-side API calls to fetch data, render content, and communicate with backend services. However, the quality of these network interactions (redundant requests, missing cache headers, oversized payloads, and excessive third-party dependencies) is rarely tested in a systematic way. Moreover, many of these quality deficiencies carry security implications: missing cache headers enable cache poisoning, excessive third-party dependencies expand the supply-chain attack surface, and error responses risk leaking server internals. In this study, we present an automated software testing framework that captures and analyzes the complete HTTP traffic of 18 production websites spanning 11 categories (e-commerce, news, government, developer tools, travel, and more). Using automated browser instrumentation via Playwright, we record 108 HAR (HTTP Archive) files across 3 independent runs per page, then apply 8 heuristic-based anti-pattern detectors to produce a composite quality score (0-100) for each site. Our results reveal a wide quality spectrum: minimalist server-rendered sites achieve perfect scores of 100, while content-heavy commercial sites score as low as 56.8. We identify redundant API calls and missing cache headers as the two most pervasive anti-patterns, each affecting 67% of sites, while third-party overhead exceeds 20% on 72% of sites. One utility site makes 2,684 requests per page load, which is 447x more than the most minimal site. To protect site reputations, all identities are anonymized using category-based pseudonyms. We provide all analysis scripts, anonymized results, and reproducibility instructions as an open artifact. This work establishes an empirical baseline for HTTP API call quality across the modern web and offers a reproducible testing framework that researchers and practitioners can apply to their own applications.

</details>


### [502] [Specification Vibing for Automated Program Repair](https://arxiv.org/abs/2602.08263)
*Taohong Zhu,Lucas C. Cordeiro,Mustafa A. Mustafa,Youcheng Sun*

Main category: cs.SE

TL;DR: 提出VibeRepair，一种以规范为中心的自动程序修复技术，在多个基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的自动程序修复方法以代码为中心，存在幻觉、行为不一致修复的风险，需要新的修复范式。

Method: 将有缺陷代码转换为结构化行为规范，推断并修复规范不一致问题，最后严格按照修正后的行为规范合成代码，还有按需推理组件。

Result: 在Defects4J v1.2和v2.0及真实世界基准测试中，VibeRepair修复效果显著，补丁空间更小，修复数量超过现有方法。

Conclusion: 以显式行为意图为中心的VibeRepair为自动程序修复带来新视角。

Abstract: Large language model (LLM)-driven automated program repair (APR) has advanced rapidly, but most methods remain code-centric: they directly rewrite source code and thereby risk hallucinated, behaviorally inconsistent fixes. This limitation suggests the need for an alternative repair paradigm that relies on a representation more accessible to LLMs than raw code, enabling more accurate understanding, analysis, and alignment during repair. To address this gap, we propose VibeRepair, a specification-centric APR technique that treats repair as behavior-specification repair rather than ad-hoc code editing. VibeRepair first translates buggy code into a structured behavior specification that captures the program's intended runtime behavior, then infers and repairs specification misalignments, and finally synthesizes code strictly guided by the corrected behavior specification. An on-demand reasoning component enriches hard cases with program analysis and historical bug-fix evidence while controlling cost. Across Defects4J and real-world benchmarks and multiple LLMs, VibeRepair demonstrates consistently strong repair effectiveness with a significantly smaller patch space. On Defects4J v1.2, VibeRepair correctly repairs 174 bugs, exceeding the strongest state-of-the-art baseline by 28 bugs, which corresponds to a 19% improvement. On Defects4J v2.0, it repairs 178 bugs, outperforming prior approaches by 33 bugs, representing a 23% improvement. Evaluations on real-world benchmarks collected after the training period of selected LLMs further confirm its effectiveness and generalizability. By centering repair on explicit behavioral intent, VibeRepair reframes APR for the era of "vibe" coding: make the behavior sing, and the code will follow.

</details>


### [503] [SWE Context Bench: A Benchmark for Context Learning in Coding](https://arxiv.org/abs/2602.08316)
*Jared Zhu,Minhao Hu,Junde Wu*

Main category: cs.SE

TL;DR: 介绍了用于评估编程代理经验复用能力的SWE - ContextBench基准，研究不同经验复用设置，结果显示正确选择的总结经验有益，强调经验表示和检索质量的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有基准大多将任务视为独立的，难以衡量编程代理积累、检索和应用先前经验的能力及复用带来的效率提升，因此需要新基准评估经验复用。

Method: 基于SWE - Bench Lite，增加99个相关任务形成有共享上下文的任务序列，从预测准确性、时间效率和成本效率三个维度评估代理。研究多种经验复用设置。

Result: 正确选择的总结经验能提高解决问题的准确性，显著减少运行时间和令牌成本，尤其是在较难任务上；未过滤或错误选择的经验益处有限或有负面影响。

Conclusion: 强调了经验表示和检索质量的重要性，SWE - ContextBench是研究编程代理经验复用的有效基准。

Abstract: Large language models are increasingly used as programming agents for repository level software engineering tasks. While recent benchmarks evaluate correctness in realistic codebases, they largely treat tasks as independent and do not assess whether agents can reuse experience across related problems. As a result, the ability of agents to accumulate, retrieve, and apply prior experience, as well as the efficiency gains from such reuse, remains difficult to measure. We introduce SWE-ContextBench, a benchmark designed to explicitly evaluate experience reuse in programming agents. Built on SWE-Bench Lite, SWE-ContextBench augments 300 base tasks with 99 related tasks derived from real dependency and reference relationships among GitHub issues and pull requests, forming task sequences with shared context. The benchmark evaluates agents along three complementary dimensions: prediction accuracy, time efficiency, and cost efficiency. Using SWE-ContextBench, we study multiple experience reuse settings, including oracle guided and autonomous retrieval, as well as full execution trajectories and compact summaries. Our results show that correctly selected summarized experience improves resolution accuracy and substantially reduces runtime and token cost, particularly on harder tasks. In contrast, unfiltered or incorrectly selected experience provides limited or negative benefits. These findings highlight the importance of experience representation and retrieval quality, and position SWE-ContextBench as a principled benchmark for studying experience reuse in programming agents.

</details>


### [504] [Automating Computational Reproducibility in Social Science: Comparing Prompt-Based and Agent-Based Approaches](https://arxiv.org/abs/2602.08561)
*Syed Mehtab Hussain Shah,Frank Hopfgartner,Arnim Bleier*

Main category: cs.SE

TL;DR: 研究用R语言社科研究构建测试平台，测试大语言模型和AI代理自动诊断修复计算研究复现失败的能力，代理式工作流表现更好。


<details>
  <summary>Details</summary>
Motivation: 实际中计算研究复现常因各种问题失败，探究大语言模型和AI代理能否自动诊断修复这些失败。

Method: 构建可控复现测试平台，注入不同故障，测试基于提示和基于代理的两种自动修复工作流。

Result: 基于提示的复现成功率31 - 79%，受提示上下文和错误复杂度影响；基于代理的成功率69 - 96%。

Conclusion: 自动工作流，尤其是基于代理的系统，可显著减少人工工作，提高不同错误类型的复现成功率。

Abstract: Reproducing computational research is often assumed to be as simple as rerunning the original code with provided data. In practice, missing packages, fragile file paths, version conflicts, or incomplete logic frequently cause analyses to fail, even when materials are shared. This study investigates whether large language models and AI agents can automate the diagnosis and repair of such failures, making computational results easier to reproduce and verify. We evaluate this using a controlled reproducibility testbed built from five fully reproducible R-based social science studies. Realistic failures were injected, ranging from simple issues to complex missing logic, and two automated repair workflows were tested in clean Docker environments. The first workflow is prompt-based, repeatedly querying language models with structured prompts of varying context, while the second uses agent-based systems that inspect files, modify code, and rerun analyses autonomously. Across prompt-based runs, reproduction success ranged from 31-79 percent, with performance strongly influenced by prompt context and error complexity. Complex cases benefited most from additional context. Agent-based workflows performed substantially better, with success rates of 69-96 percent across all complexity levels. These results suggest that automated workflows, especially agent-based systems, can significantly reduce manual effort and improve reproduction success across diverse error types. Unlike prior benchmarks, our testbed isolates post-publication repair under controlled failure modes, allowing direct comparison of prompt-based and agent-based approaches.

</details>


### [505] [Taming Scylla: Understanding the multi-headed agentic daemon of the coding seas](https://arxiv.org/abs/2602.08765)
*Micah Villmow*

Main category: cs.SE

TL;DR: 本文介绍了评估框架Scylla，用于对基于大语言模型的编程工具进行基准测试，量化了代理复杂性与实际结果之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏严格方法评估不同架构选择对基于大语言模型的软件开发工具的能力和成本的影响。

Method: 引入Scylla评估框架，通过结构化消融研究，使用七个测试层级逐步增加复杂度，关键指标为Cost - of - Pass (CoP)，框架与模型无关，使用多个大语言模型评判器进行评估。

Result: 得到了一个可复现的框架，能量化代理复杂性和实际结果之间的权衡。

Conclusion: 架构复杂性并不总是能提高质量。

Abstract: LLM-based tools are automating more software development tasks at a rapid pace, but there is no rigorous way to evaluate how different architectural choices -- prompts, skills, tools, multi-agent setups -- materially affect both capability and cost. This paper introduces Scylla, an evaluation framework for benchmarking agentic coding tools through structured ablation studies that uses seven testing tiers (T0-T6) progressively adding complexity to isolate what directly influences results and how. The key metric is Cost-of-Pass (CoP): the expected dollar cost to get one correct solution, which directly quantifies the trade-off between complexity and efficiency. The framework is model-agnostic, designed to work with any CLI tool; this paper demonstrates it with Claude Sonnet 4.5, using multiple LLM judges (Opus 4.5, Sonnet 4.5, Haiku 4.5) from the same vendor for evaluation consensus, where judges score results using direct tests, human-designed LLM-evaluated rubrics, and qualitative assessment. The result is a reproducible framework that quantifies trade-offs between agent complexity and actual outcomes, suggesting that architectural complexity does not always improve quality.

</details>


### [506] [ArkEval: Benchmarking and Evaluating Automated CodeRepair for ArkTS](https://arxiv.org/abs/2602.08866)
*Bang Xie,Senjian Zhang,Zhiyuan Peng,Wei Chen,Chenhao Ying,Yuan Luo*

Main category: cs.SE

TL;DR: 现有HarmonyOS生态缺乏ArkTS自动化代码修复工具及评估基准，本文提出ArkEval框架，构建首个ArkTS修复基准并评估LLM性能。


<details>
  <summary>Details</summary>
Motivation: HarmonyOS发展使ArkTS开发重要性提升，但缺乏自动化代码修复工具和高质量评估基准。

Method: 从华为官方仓库挖掘问题构建基准，采用LLM测试生成与投票机制，标准化问题描述，用检索增强修复工作流在基准上评估LLMs。

Result: 评估显示了LLMs修复ArkTS代码的现有能力和局限性。

Conclusion: 该研究为低资源语言领域ArkTS的自动化修复研究奠定基础。

Abstract: Large language models have transformed code generation, enabling unprecedented automation in software development. As mobile ecosystems evolve, HarmonyOS has emerged as a critical platform requiring robust development tools. Software development for the HarmonyOS ecosystem relies heavily on ArkTS, a statically typed extension of TypeScript. Despite its growing importance, the ecosystem lacks robust tools for automated code repair, primarily due to the absence of a high-quality benchmark for evaluation. To address this gap, we present ArkEval, a unified framework for ArkTS automated repair workflow evaluation and benchmark construction. It provides the first comprehensive benchmark specifically designed for ArkTS automated program repair. We constructed this benchmark by mining issues from a large-scale official Huawei repository containing over 400 independent ArkTS applications. Through a rigorous multi-stage filtering process, we curated 502 reproducible issues. To ensure testability, we employed a novel LLM-based test generation and voting mechanism involving Claude and other models. Furthermore, we standardized problem statements to facilitate fair evaluation. Finally, we evaluated four state-of-the-art Large Language Models (LLMs) on our benchmark using a retrieval-augmented repair workflow. Our results highlight the current capabilities and limitations of LLMs in repairing ArkTS code, paving the way for future research in this low-resource language domain.

</details>


### [507] [DeepQuali: Initial results of a study on the use of large language models for assessing the quality of user stories](https://arxiv.org/abs/2602.08887)
*Adam Trendowicz,Daniel Seifert,Andreas Jedlitschka,Marcus Ciolkowski,Anton Strahilov*

Main category: cs.SE

TL;DR: 提出基于LLM的DeepQuali方法评估和改进敏捷软件开发中需求质量，经小公司项目验证，LLM评估获专家认可，但存在集成问题，LLM有潜力支持需求质量工作。


<details>
  <summary>Details</summary>
Motivation: 当前生成式人工智能在需求工程尤其是需求验证方面应用有限，且主要聚焦需求获取、转换和分类，缺乏质量评估。

Method: 提出基于GPT - 4o的DeepQuali方法，应用于两个小公司项目，对比LLM质量评估与专家判断，专家参与解决方案演练并反馈。

Result: 专家基本认同LLM的质量评估，尤其总体评分和解释，但在详细评分上专家间有分歧，专家认可方法有用，但批评未集成到工作流。

Conclusion: LLMs在支持软件工程师进行需求质量评估和改进方面有潜力，明确使用质量模型和解释性反馈可提高接受度。

Abstract: Generative artificial intelligence (GAI), specifically large language models (LLMs), are increasingly used in software engineering, mainly for coding tasks. However, requirements engineering - particularly requirements validation - has seen limited application of GAI. The current focus of using GAI for requirements is on eliciting, transforming, and classifying requirements, not on quality assessment. We propose and evaluate the LLM-based (GPT-4o) approach "DeepQuali", for assessing and improving requirements quality in agile software development. We applied it to projects in two small companies, where we compared LLM-based quality assessments with expert judgments. Experts also participated in walkthroughs of the solution, provided feedback, and rated their acceptance of the approach. Experts largely agreed with the LLM's quality assessments, especially regarding overall ratings and explanations. However, they did not always agree with the other experts on detailed ratings, suggesting that expertise and experience may influence judgments. Experts recognized the usefulness of the approach but criticized the lack of integration into their workflow. LLMs show potential in supporting software engineers with the quality assessment and improvement of requirements. The explicit use of quality models and explanatory feedback increases acceptance.

</details>


### [508] [Comparing AI Coding Agents: A Task-Stratified Analysis of Pull Request Acceptance](https://arxiv.org/abs/2602.08915)
*Giovanni Pinna,Jingzhi Gong,David Williams,Federica Sarro*

Main category: cs.SE

TL;DR: 本文对五种流行的AI编码助手进行实证研究，分析7156个拉取请求，发现各助手表现有差异，任务类型对接受率影响大，且无单一助手在所有任务类型中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 现有对AI编码助手在不同任务类型和随时间变化的有效性的系统比较有限，需开展研究。

Method: 分析AIDev数据集中的7156个拉取请求，进行时间趋势分析和分层卡方检验。

Result: Devin接受率有持续正趋势，其他较稳定；文档任务接受率比新特性高16个百分点；OpenAI Codex在多数任务类别接受率高；Claude Code在文档和特性任务领先，Cursor在修复任务出色。

Conclusion: PR任务类型是影响接受率的主要因素，没有单一编码助手能在所有任务类型中表现最优。

Abstract: The rapid adoption of AI-powered coding assistants is transforming software development practices, yet systematic comparisons of their effectiveness across different task types and over time remain limited. This paper presents an empirical study comparing five popular agents (OpenAI Codex, GitHub Copilot, Devin, Cursor, and Claude Code), analyzing 7,156 pull requests (PRs) from the AIDev dataset. Temporal trend analysis reveals heterogeneous evolution patterns: Devin exhibits the only consistent positive trend in acceptance rate (+0.77% per week over 32 weeks), whereas other agents remain largely stable. Our analysis suggests that the PR task type is a dominant factor influencing acceptance rates: documentation tasks achieve 82.1% acceptance compared to 66.1% for new features - a 16 percentage point gap that exceeds typical inter-agent variance for most tasks. OpenAI Codex achieves consistently high acceptance rates across all nine task categories (59.6%-88.6%), with stratified Chi-square tests confirming statistically significant advantages over other agents in several task categories. However, no single agent performs best across all task types: Claude Code leads in documentation (92.3%) and features (72.6%), while Cursor excels in fix tasks (80.4%).

</details>


<div id='q-fin.PM'></div>

# q-fin.PM [[Back]](#toc)

### [509] [Comparing Mixture, Box, and Wasserstein Ambiguity Sets in Distributionally Robust Asset Liability Management](https://arxiv.org/abs/2602.08228)
*Alireza Ghahtarani,Ahmed Saif,Alireza Ghasemi*

Main category: q-fin.PM

TL;DR: 本文研究养老金资产负债管理，应用分布鲁棒优化（DRO）方法并评估三种DRO公式，结果显示部分DRO公式优于传统随机规划，表明分布鲁棒性可提升养老金管理策略表现。


<details>
  <summary>Details</summary>
Motivation: 应对传统框架在不确定性下的局限性，解决金融机构尤其是养老金在投资回报和长期债务偿付能力间平衡的挑战。

Method: 提出并评估三种DRO公式，包括混合模糊集、箱式模糊集和Wasserstein度量模糊集，利用加拿大养老金计划的实证数据与传统随机规划方法进行对比分析。

Result: 基于Wasserstein和箱式模糊集的DRO公式在资金比率和基金总体回报方面优于基于混合的DRO和随机规划。

Conclusion: 纳入分布鲁棒性可显著增强养老金管理策略的弹性和表现。

Abstract: Asset Liability Management (ALM) represents a fundamental challenge for financial institutions, particularly pension funds, which must navigate the tension between generating competitive investment returns and ensuring the solvency of long-term obligations. To address the limitations of traditional frameworks under uncertainty, this paper implements Distributionally Robust Optimization (DRO), an emergent paradigm that accounts for a broad spectrum of potential probability distributions. We propose and evaluate three distinct DRO formulations: mixture ambiguity sets with discrete scenarios, box ambiguity sets of discrete distribution functions, and Wasserstein metric ambiguity sets. Utilizing empirical data from the Canada Pension Plan (CPP), we conduct a comparative analysis of these models against traditional stochastic programming approaches. Our results demonstrate that DRO formulations, specifically those utilizing Wasserstein and box ambiguity sets, consistently outperform both mixture-based DRO and stochastic programming in terms of funding ratios and overall fund returns. These findings suggest that incorporating distributional robustness significantly enhances the resilience and performance of pension fund management strategies.

</details>


<div id='q-fin.RM'></div>

# q-fin.RM [[Back]](#toc)

### [510] [LLM as a Risk Manager: LLM Semantic Filtering for Lead-Lag Trading in Prediction Markets](https://arxiv.org/abs/2602.07048)
*Sumin Kim,Minjae Kim,Jihoon Kwon,Yoon Kim,Nicole Kagan,Joo Won Lee,Oscar Levy,Alejandro Lopez-Lira,Yongjae Lee,Chanyeol Choi*

Main category: q-fin.RM

TL;DR: 本文提出混合两阶段因果筛选器处理预测市场中发现稳健领先 - 滞后关系的挑战，在Kalshi Economics市场上表现优于统计基线。


<details>
  <summary>Details</summary>
Motivation: 预测市场中由于虚假统计相关性，发现稳健领先 - 滞后关系具有挑战性。

Method: 提出混合两阶段因果筛选器，包括用Granger因果关系的统计阶段和基于大语言模型（LLM）语义评估的阶段；用固定的信号触发交易协议评估排名对。

Result: 混合方法在Kalshi Economics市场上始终优于统计基线，胜率从51.4%提升至54.5%，亏损交易平均规模从649美元降至347美元，且不同交易配置下结果稳定。

Conclusion: 大语言模型可作为统计发现之上的语义风险管理者，优先考虑在市场条件变化下具有泛化性的领先 - 滞后关系。

Abstract: Prediction markets provide a unique setting where event-level time series are directly tied to natural-language descriptions, yet discovering robust lead-lag relationships remains challenging due to spurious statistical correlations. We propose a hybrid two-stage causal screener to address this challenge: (i) a statistical stage that uses Granger causality to identify candidate leader-follower pairs from market-implied probability time series, and (ii) an LLM-based semantic stage that re-ranks these candidates by assessing whether the proposed direction admits a plausible economic transmission mechanism based on event descriptions. Because causal ground truth is unobserved, we evaluate the ranked pairs using a fixed, signal-triggered trading protocol that maps relationship quality into realized profit and loss (PnL). On Kalshi Economics markets, our hybrid approach consistently outperforms the statistical baseline. Across rolling evaluations, the win rate increases from 51.4% to 54.5%. Crucially, the average magnitude of losing trades decreases substantially from 649 USD to 347 USD. This reduction is driven by the LLM's ability to filter out statistically fragile links that are prone to large losses, rather than relying on rare gains. These improvements remain stable across different trading configurations, indicating that the gains are not driven by specific parameter choices. Overall, the results suggest that LLMs function as semantic risk managers on top of statistical discovery, prioritizing lead-lag relationships that generalize under changing market conditions.

</details>


### [511] [Algorithmic Monitoring: Measuring Market Stress with Machine Learning](https://arxiv.org/abs/2602.07066)
*Marc Schmitt*

Main category: q-fin.RM

TL;DR: 构建市场压力概率指数（MSPI）预估美国股市一个月后高压力概率，方法有效且可用于金融计量。


<details>
  <summary>Details</summary>
Motivation: 需要一个能预估美国股市短期压力风险的指标。

Method: 利用CRSP日数据，用可解释的横截面脆弱信号，通过L1正则化逻辑回归在实时扩展窗口设计中将每月数据映射为前瞻性压力概率。

Result: 样本外MSPI能追踪主要压力事件，比基于滞后市场回报和已实现波动率的基准模型有更好的区分度和准确性，给出有经济意义的校准压力概率。

Conclusion: MSPI是衡量近期股市压力风险的透明且易更新的指标。

Abstract: I construct a Market Stress Probability Index (MSPI) that estimates the probability of high stress in the U.S. equity market one month ahead using information from the cross-section of individual stocks. Using CRSP daily data, each month is summarized by a set of interpretable cross-sectional fragility signals and mapped into a forward-looking stress probability via an L1-regularized logistic regression in a real-time expanding-window design. Out of sample, MSPI tracks major stress episodes and improves discrimination and accuracy relative to a parsimonious benchmark based on lagged market return and realized volatility, delivering calibrated stress probabilities on an economically meaningful scale. Further, I illustrate how MSPI can be used as a probability-based measurement object in financial econometrics. The resulting index provides a transparent and easily updated measure of near-term equity-market stress risk.

</details>


### [512] [Perfectly Fitting CDO Prices Across Tranches: A Theoretical Framework with Efficient Algorithms](https://arxiv.org/abs/2602.08039)
*Lan Bu,Ning Cai,Chenxi Xia,Jingping Yang*

Main category: q-fin.RM

TL;DR: 本文提出理论框架解决CDO建模中用单一一致模型完美拟合所有分层市场价格的挑战，可高效验证兼容性并构建拟合模型，还展示了其实际应用。


<details>
  <summary>Details</summary>
Motivation: 解决CDO建模中用单一一致模型完美拟合所有分层市场价格的关键挑战，这对统一风险管理和非标准信用衍生品定价至关重要。

Method: 引入并定义市场价格的弱兼容性和强兼容性，通过建立兼容性与线性规划问题的关系推导两种兼容性的充要条件，构建相应的能完美拟合的具体copula模型。

Result: 框架可通过线性规划问题高效验证弱兼容性和强兼容性，无需基于模拟的优化就能构建完美拟合的copula模型。

Conclusion: 框架在风险管理和非标准信用衍生品定价中有实际应用。

Abstract: This paper addresses a key challenge in CDO modeling: achieving a perfect fit to market prices across all tranches using a single, consistent model. The existence of such a perfect-fit model implies the absence of arbitrage among CDO tranches and is thus essential for unified risk management and the pricing of nonstandard credit derivatives. To address this central challenge, we face three primary difficulties: standard parametric models typically fail to achieve a perfect fit; the calibration of standard parametric models inherently relies on computationally intensive simulation-based optimization; and there is a lack of formal theory to determine when a perfect-fit model exists and, if it exists, how to construct it. We propose a theoretical framework to overcome these difficulties. We first introduce and define two compatibility levels of market prices: weak compatibility and strong compatibility. Specifically, market prices across all tranches are said to be weakly (resp. strongly) compatible if there exists a single model (resp. a single conditionally i.i.d. model) that perfectly fits these market prices. We then derive sufficient and necessary conditions for both levels of compatibility by establishing a relationship between compatibility and LP problems. Furthermore, under either condition, we construct a corresponding concrete copula model that achieves a perfect fit. Notably, our framework not only allows for efficient verification of weak compatibility and strong compatibility through LP problems but also facilitates the construction of the corresponding copula models that achieve a perfect fit, eliminating the need for simulation-based optimization. The practical applications of our framework are demonstrated in risk management and the pricing of nonstandard credit derivatives.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [513] [The Extremity Premium: Sentiment Regimes and Adverse Selection in Cryptocurrency Markets](https://arxiv.org/abs/2602.07018)
*Murad Farzulla*

Main category: q-fin.ST

TL;DR: 研究利用加密恐惧与贪婪指数和比特币数据，发现情绪极端性可预测超额不确定性，存在“极端性溢价”，在以太坊和多数市场周期可复制，溢价对函数形式敏感，结果表明强度而非方向驱动加密市场流动性撤出。


<details>
  <summary>Details</summary>
Motivation: 探究情绪极端性对加密市场不确定性和流动性的影响。

Method: 运用加密恐惧与贪婪指数和比特币日数据，进行全历史验证、分位数比较、格兰杰因果检验、安慰剂检验，构建基于代理的模型。

Result: 极端恐惧和贪婪时期价差显著高于中性时期；在以太坊和多数市场周期可复制；溢价对函数形式敏感；基于代理的模型定性重现模式。

Conclusion: 情绪极端性捕捉到波动制度交互，强度而非方向驱动加密市场与不确定性相关的流动性撤出，分离情绪与波动的“纯”效应仍是挑战。

Abstract: Using the Crypto Fear & Greed Index and Bitcoin daily data, we document that sentiment extremity predicts excess uncertainty beyond realized volatility. Extreme fear and extreme greed regimes exhibit significantly higher spreads than neutral periods -- a phenomenon we term the "extremity premium." Extended validation on the full Fear & Greed history (February 2018--January 2026, N = 2,896) confirms the finding: within-volatility-quintile comparisons show a significant premium (p < 0.001, Cohen's d = 0.21), Granger causality from uncertainty to spreads is strong (F = 211), and placebo tests reject the null (p < 0.0001). The effect replicates on Ethereum and across 6 of 7 market cycles. However, the premium is sensitive to functional form: comprehensive regression controls absorb regime effects, while nonparametric stratification preserves them. We interpret this as evidence that sentiment extremity captures volatility-regime interactions not fully represented by parametric controls -- consistent with, but not conclusively separable from, the F&G Index's embedded volatility component. An agent-based model reproduces the pattern qualitatively. The results suggest that intensity, not direction, drives uncertainty-linked liquidity withdrawal in cryptocurrency markets, though identification of "pure" sentiment effects from volatility remains an open challenge.

</details>


### [514] [Financial Bond Similarity Search Using Representation Learning](https://arxiv.org/abs/2602.07020)
*Amin Haeri,Mahdi Ghelichi,Nishant Agrawal,David Li,Catalina Gomez Sanchez*

Main category: q-fin.ST

TL;DR: 本文提出嵌入模型捕捉债券类别属性语义相似性，用于寻找相似债券，在风险建模和曲线构建上表现更佳。


<details>
  <summary>Details</summary>
Motivation: 在固定收益分析中，数值金融属性常掩盖类别非金融属性，导致寻找相似债券具有挑战性。

Method: 提出嵌入模型捕捉类别属性的语义相似性。

Result: 该方法在稀疏发行者增强评估中，优于独热编码等基线方法。

Conclusion: 该方法可改善风险建模和曲线构建。

Abstract: Finding similar bonds remains challenging in fixed-income analytics, as numerical financial attributes often overshadow categorical non-financial ones such as issuer sector and domicile. This paper shows that these categorical attributes dominate the predictability of spread curves and proposes embedding models to capture their semantic similarities, outperforming one-hot and many other baselines. Evaluated via sparse-issuer augmentation, the approach improves risk modeling and curve construction.

</details>


### [515] [Sentiment Without Structure: Differential Market Responses to Infrastructure vs Regulatory Events in Cryptocurrency Markets](https://arxiv.org/abs/2602.07046)
*Murad Farzulla*

Main category: q-fin.ST

TL;DR: 使用事件研究方法，对加密货币市场中基础设施与监管事件的市场反应进行研究，发现市场对两种负面事件反应相似。


<details>
  <summary>Details</summary>
Motivation: 探究加密货币市场对基础设施与监管事件的不同反应。

Method: 使用事件研究方法，对4类事件进行分类，采用恒定均值和市场调整模型，并使用事件级块引导置信区间。

Result: 基础设施故障的平均累积异常回报率为-7.6%，监管执法为-11.1%，两者差异不显著，市场反应相似。

Conclusion: 这是探索性分析，可用于生成假设，后续需大样本前瞻性检验。

Abstract: We investigate differential market responses to infrastructure versus regulatory events in cryptocurrency markets using event study methodology with 4-category event classification. From 50 candidate events (2019-2025), 31 meet our impact and estimation-data criteria across 4 cryptocurrencies: Bitcoin (BTC), Ethereum (ETH), Solana (SOL), and Cardano (ADA). We employ constant mean and market-adjusted models with event-level block bootstrap confidence intervals (CIs) that properly account for cross-sectional correlation.
  Our primary comparison focuses on negative-valence events: infrastructure failures (10 events identified; 8 with sufficient estimation data for analysis) versus regulatory enforcement (7 events). We find infrastructure failures produce mean Cumulative Abnormal Return (CAR) of -7.6% (bootstrap 95% CI: [-25.8%, +11.3%]) and regulatory enforcement produces mean CAR of -11.1% (CI: [-31.0%, +10.7%]). The difference in mean CARs of +3.6 percentage points (pp) has CI [-25.3%, +30.9%], p = 0.81. This is a null finding: markets respond similarly to both shock types when controlling for event valence.
  Robustness checks confirm: (1) consistent negative sign across all window specifications ([0, +1] to [-5, +30]), (2) results survive leave-one-out exclusion of FTX and Terra, (3) market model with BTC/equal-weighted (EW) proxy attenuates but does not flip results. The 4-category classification addresses prior conflation of upgrades with failures.
  Interpretation note: This exploratory analysis should be treated as hypothesis-generating; any post-hoc theoretical framing requires prospective testing with larger samples.

</details>


### [516] [QuantaAlpha: An Evolutionary Framework for LLM-Driven Alpha Mining](https://arxiv.org/abs/2602.07085)
*Jun Han,Shuo Zhang,Wei Li,Zhi Yang,Yifan Dong,Tu Hu,Jialuo Yuan,Xiaomin Yu,Yumo Zhu,Fangqi Lou,Xin Guo,Zhaowei Liu,Tianyi Jiang,Ruichuan An,Jingping Liu,Biao Wu,Rongze Chen,Kunyi Wang,Yifan Wang,Sen Hu,Xinbing Kong,Liwen Zhang,Ronghao Chen,Huacan Wang*

Main category: q-fin.ST

TL;DR: 提出QuantaAlpha框架用于金融市场的alpha挖掘，经CSI 300实验验证效果好，且在其他指数有强稳健性。


<details>
  <summary>Details</summary>
Motivation: 现有agentic框架在alpha挖掘中缺乏可控的多轮搜索和有效复用验证经验，当前金融市场噪声大且非平稳使alpha挖掘难度大。

Method: 将每次端到端挖掘运行视为轨迹，通过轨迹级别的变异和交叉操作改进因子；定位轨迹中的次优步骤进行针对性修正，重组高回报片段；生成因子时保证语义一致性，约束因子复杂度和冗余度。

Result: 在CSI 300上比基线模型和先前agentic系统有持续收益；使用GPT - 5.2时，IC为0.1501，ARR为27.75%，MDD为7.98%；挖掘的因子可有效转移到CSI 500和S&P 500，四年累计超额回报分别达160%和137%。

Conclusion: QuantaAlpha框架在金融市场alpha挖掘中有效且具有强稳健性，能应对市场分布变化。

Abstract: Financial markets are noisy and non-stationary, making alpha mining highly sensitive to noise in backtesting results and sudden market regime shifts. While recent agentic frameworks improve alpha mining automation, they often lack controllable multi-round search and reliable reuse of validated experience. To address these challenges, we propose QuantaAlpha, an evolutionary alpha mining framework that treats each end-to-end mining run as a trajectory and improves factors through trajectory-level mutation and crossover operations. QuantaAlpha localizes suboptimal steps in each trajectory for targeted revision and recombines complementary high-reward segments to reuse effective patterns, enabling structured exploration and refinement across mining iterations. During factor generation, QuantaAlpha enforces semantic consistency across the hypothesis, factor expression, and executable code, while constraining the complexity and redundancy of the generated factor to mitigate crowding. Extensive experiments on the China Securities Index 300 (CSI 300) demonstrate consistent gains over strong baseline models and prior agentic systems. When utilizing GPT-5.2, QuantaAlpha achieves an Information Coefficient (IC) of 0.1501, with an Annualized Rate of Return (ARR) of 27.75% and a Maximum Drawdown (MDD) of 7.98%. Moreover, factors mined on CSI 300 transfer effectively to the China Securities Index 500 (CSI 500) and the Standard & Poor's 500 Index (S&P 500), delivering 160% and 137% cumulative excess return over four years, respectively, which indicates strong robustness of QuantaAlpha under market distribution shifts.

</details>


### [517] [RealFin: How Well Do LLMs Reason About Finance When Users Leave Things Unsaid?](https://arxiv.org/abs/2602.07096)
*Yuyang Dai,Yan Lin,Zhuohan Xie,Yuxia Wang*

Main category: q-fin.ST

TL;DR: 提出REALFIN双语基准评估金融推理，发现当前模型在关键条件缺失时表现不佳，凸显可靠金融模型需知何时不答题。


<details>
  <summary>Details</summary>
Motivation: 可靠金融推理需知何时无法给出合理答案，实际金融问题常因隐含假设致信息不足却看似可解，需评估模型应对能力。

Method: 引入REALFIN基准，系统移除考试式问题中的关键前提，在三种形式下评估模型。

Result: 关键条件缺失时模型性能下降，通用模型易过度猜测，多数金融专业模型难识别缺失前提。

Conclusion: 当前评估存在关键差距，可靠金融模型必须知道何时不应答题。

Abstract: Reliable financial reasoning requires knowing not only how to answer, but also when an answer cannot be justified. In real financial practice, problems often rely on implicit assumptions that are taken for granted rather than stated explicitly, causing problems to appear solvable while lacking enough information for a definite answer. We introduce REALFIN, a bilingual benchmark that evaluates financial reasoning by systematically removing essential premises from exam-style questions while keeping them linguistically plausible. Based on this, we evaluate models under three formulations that test answering, recognizing missing information, and rejecting unjustified options, and find consistent performance drops when key conditions are absent. General-purpose models tend to over-commit and guess, while most finance-specialized models fail to clearly identify missing premises. These results highlight a critical gap in current evaluations and show that reliable financial models must know when a question should not be answered.

</details>


<div id='q-fin.TR'></div>

# q-fin.TR [[Back]](#toc)

### [518] [Behavioral Consistency Validation for LLM Agents: An Analysis of Trading-Style Switching through Stock-Market Simulation](https://arxiv.org/abs/2602.07023)
*Zeping Li,Guancheng Wan,Keyang Chen,Yu Chen,Yiwen Zhao,Philip Torr,Guangnan Ye,Zhenfei Yin,Hongfeng Chai*

Main category: q-fin.TR

TL;DR: 本文探讨大语言模型（LLM）代理在金融股市模拟中的行为是否与真实市场参与者一致，评估其策略切换与金融理论的契合度，发现LLM切换行为仅部分符合行为金融理论。


<details>
  <summary>Details</summary>
Motivation: 探究LLM代理行为是否与真实市场参与者一致，以确保股市模拟结果的有效性。

Method: 选择金融股市场景，将行为金融驱动因素设为个性特征，让代理处理数据、按指定风格交易并定期重新评估策略，引入四个对齐指标，用Mann - Whitney U检验比较代理策略切换行为与金融理论。

Result: 近期LLM的切换行为仅部分与行为金融理论一致。

Conclusion: 需要进一步改进以让代理行为与金融理论更好对齐。

Abstract: Recent works have increasingly applied Large Language Models (LLMs) as agents in financial stock market simulations to test if micro-level behaviors aggregate into macro-level phenomena. However, a crucial question arises: Do LLM agents' behaviors align with real market participants? This alignment is key to the validity of simulation results. To explore this, we select a financial stock market scenario to test behavioral consistency. Investors are typically classified as fundamental or technical traders, but most simulations fix strategies at initialization, failing to reflect real-world trading dynamics. In this work, we assess whether agents' strategy switching aligns with financial theory, providing a framework for this evaluation. We operationalize four behavioral-finance drivers-loss aversion, herding, wealth differentiation, and price misalignment-as personality traits set via prompting and stored long-term. In year-long simulations, agents process daily price-volume data, trade under a designated style, and reassess their strategy every 10 trading days. We introduce four alignment metrics and use Mann-Whitney U tests to compare agents' style-switching behavior with financial theory. Our results show that recent LLMs' switching behavior is only partially consistent with behavioral-finance theories, highlighting the need for further refinement in aligning agent behavior with financial theory.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [519] [Fast and Robust Likelihood-Guided Diffusion Posterior Sampling with Amortized Variational Inference](https://arxiv.org/abs/2602.07102)
*Léon Zheng,Thomas Hirtz,Yazid Janati,Eric Moulines*

Main category: stat.ML

TL;DR: 提出一种扩散后验采样的摊销策略，平衡了基于扩散的逆问题中效率与灵活性。


<details>
  <summary>Details</summary>
Motivation: 零样本扩散后验采样计算成本高，先前的摊销扩散方法对未见退化情况鲁棒性不足，需平衡效率与灵活性。

Method: 对变分扩散后验采样中出现的内部优化问题进行摊销，保留显式似然引导。

Result: 加速了分布内退化的推理，同时对先前未见的算子保持鲁棒性。

Conclusion: 改善了基于扩散的逆问题中效率和灵活性的权衡。

Abstract: Zero-shot diffusion posterior sampling offers a flexible framework for inverse problems by accommodating arbitrary degradation operators at test time, but incurs high computational cost due to repeated likelihood-guided updates. In contrast, previous amortized diffusion approaches enable fast inference by replacing likelihood-based sampling with implicit inference models, but at the expense of robustness to unseen degradations. We introduce an amortization strategy for diffusion posterior sampling that preserves explicit likelihood guidance by amortizing the inner optimization problems arising in variational diffusion posterior sampling. This accelerates inference for in-distribution degradations while maintaining robustness to previously unseen operators, thereby improving the trade-off between efficiency and flexibility in diffusion-based inverse problems.

</details>


### [520] [Discrete Adjoint Matching](https://arxiv.org/abs/2602.07132)
*Oswin So,Brian Karrer,Chuchu Fan,Ricky T. Q. Chen,Guan-Horng Liu*

Main category: stat.ML

TL;DR: 本文提出用于微调离散生成模型的离散伴随匹配（DAM）方法，并展示其在合成和数学推理任务上的有效性。


<details>
  <summary>Details</summary>
Motivation: 熵正则化奖励优化计算方法在连续状态空间取得成功，但向离散生成建模转移面临挑战且研究较少，因此需提出适用于离散生成模型的方法。

Method: 提出离散伴随匹配（DAM）方法，引入离散伴随，从纯统计角度推导，可应用标准匹配框架。

Result: 展示了DAM在合成和数学推理任务上的有效性。

Conclusion: DAM为基于伴随的估计器带来新的算法机会，可用于微调离散生成模型。

Abstract: Computation methods for solving entropy-regularized reward optimization -- a class of problems widely used for fine-tuning generative models -- have advanced rapidly. Among those, Adjoint Matching (AM, Domingo-Enrich et al., 2025) has proven highly effective in continuous state spaces with differentiable rewards. Transferring these practical successes to discrete generative modeling, however, remains particularly challenging and largely unexplored, mainly due to the drastic shift in generative model classes to discrete state spaces, which are nowhere differentiable. In this work, we propose Discrete Adjoint Matching (DAM) -- a discrete variant of AM for fine-tuning discrete generative models characterized by Continuous-Time Markov Chains, such as diffusion-based large language models. The core of DAM is the introduction of discrete adjoint-an estimator of the optimal solution to the original problem but formulated on discrete domains-from which standard matching frameworks can be applied. This is derived via a purely statistical standpoint, in contrast to the control-theoretic viewpoint in AM, thereby opening up new algorithmic opportunities for general adjoint-based estimators. We showcase DAM's effectiveness on synthetic and mathematical reasoning tasks.

</details>


### [521] [Scalable Mean-Field Variational Inference via Preconditioned Primal-Dual Optimization](https://arxiv.org/abs/2602.07632)
*Jinhua Lyu,Tianmin Yu,Ying Ma,Naichen Shi*

Main category: stat.ML

TL;DR: 从迷你批次原始对偶视角研究大规模平均场变分推断问题，提出PD - VI和P²D - VI算法，建立收敛保障，实验显示方法优于现有随机变分推断方法。


<details>
  <summary>Details</summary>
Motivation: 解决大规模平均场变分推断（MFVI）问题，考虑不同变分参数块的异构损失几何。

Method: 将MFVI重新表述为约束有限和问题，开发基于增广拉格朗日公式的原始对偶算法PD - VI，引入块预条件扩展P²D - VI。

Result: 在适当恒定步长下，证明PD - VI和P²D - VI的收敛性，一般情况下有O(1/T)收敛到平稳点，强凸性下线性收敛。

Conclusion: 数值实验表明PD - VI和P²D - VI在收敛速度和解决方案质量上优于现有随机变分推断方法。

Abstract: In this work, we investigate the large-scale mean-field variational inference (MFVI) problem from a mini-batch primal-dual perspective. By reformulating MFVI as a constrained finite-sum problem, we develop a novel primal-dual algorithm based on an augmented Lagrangian formulation, termed primal-dual variational inference (PD-VI). PD-VI jointly updates global and local variational parameters in the evidence lower bound in a scalable manner. To further account for heterogeneous loss geometry across different variational parameter blocks, we introduce a block-preconditioned extension, P$^2$D-VI, which adapts the primal-dual updates to the geometry of each parameter block and improves both numerical robustness and practical efficiency. We establish convergence guarantees for both PD-VI and P$^2$D-VI under properly chosen constant step size, without relying on conjugacy assumptions or explicit bounded-variance conditions. In particular, we prove $O(1/T)$ convergence to a stationary point in general settings and linear convergence under strong convexity. Numerical experiments on synthetic data and a real large-scale spatial transcriptomics dataset demonstrate that our methods consistently outperform existing stochastic variational inference approaches in terms of convergence speed and solution quality.

</details>


### [522] [Flow-Based Conformal Predictive Distributions](https://arxiv.org/abs/2602.07633)
*Trevor Harris*

Main category: stat.ML

TL;DR: 提出在任意维度采样共形边界的无训练方法，评估其在多个问题上的应用。


<details>
  <summary>Details</summary>
Motivation: 共形预测在高维或结构化输出空间中难以表示和使用，限制其与下游任务集成。

Method: 利用可微非一致性得分在输出空间诱导确定性流，其轨迹收敛到共形预测集边界，用于采样共形边界。

Result: 可将边界样本重新共形化形成点预测集，混合不同置信水平得到共形预测分布。

Conclusion: 该方法在多个实际问题上得到评估。

Abstract: Conformal prediction provides a distribution-free framework for uncertainty quantification via prediction sets with exact finite-sample coverage. In low dimensions these sets are easy to interpret, but in high-dimensional or structured output spaces they are difficult to represent and use, which can limit their ability to integrate with downstream tasks such as sampling and probabilistic forecasting. We show that any differentiable nonconformity score induces a deterministic flow on the output space whose trajectories converge to the boundary of the corresponding conformal prediction set. This leads to a computationally efficient, training-free method for sampling conformal boundaries in arbitrary dimensions. Boundary samples can be reconformalized to form pointwise prediction sets with controlled risk, and mixing across confidence levels yields conformal predictive distributions whose quantile regions coincide exactly with conformal prediction sets. We evaluate the approach on PDE inverse problems, precipitation downscaling, climate model debiasing, and hurricane trajectory forecasting.

</details>


### [523] [On Generation in Metric Spaces](https://arxiv.org/abs/2602.07710)
*Jiaxun Li,Vinod Raman,Ambuj Tewari*

Main category: stat.ML

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study generation in separable metric instance spaces. We extend the language generation framework from Kleinberg and Mullainathan [2024] beyond countable domains by defining novelty through metric separation and allowing asymmetric novelty parameters for the adversary and the generator. We introduce the $(\varepsilon,\varepsilon')$-closure dimension, a scale-sensitive analogue of closure dimension, which yields characterizations of uniform and non-uniform generatability and a sufficient condition for generation in the limit. Along the way, we identify a sharp geometric contrast. Namely, in doubling spaces, including all finite-dimensional normed spaces, generatability is stable across novelty scales and invariant under equivalent metrics. In general metric spaces, however, generatability can be highly scale-sensitive and metric-dependent; even in the natural infinite-dimensional Hilbert space $\ell^2$, all notions of generation may fail abruptly as the novelty parameters vary.

</details>


### [524] [Fast Model Selection and Stable Optimization for Softmax-Gated Multinomial-Logistic Mixture of Experts Models](https://arxiv.org/abs/2602.07997)
*TrungKhang Tran,TrungTin Nguyen,Md Abul Bashar,Nhat Ho,Richi Nayak,Christopher Drovandi*

Main category: stat.ML

TL;DR: 本文聚焦分类场景下混合专家（MoE）架构的最大似然训练和模型选择问题，提出批处理MM算法，证明有限样本率，给出专家数量选择器，实验验证效果良好。


<details>
  <summary>Details</summary>
Motivation: 分类场景下采用softmax多项逻辑门控的MoE架构，在稳定最大似然训练和原则性模型选择上缺乏严格保证。

Method: 1. 推导使用显式二次优化器的批处理MM算法；2. 证明有限样本下的条件密度估计和参数恢复率；3. 引入混合测度树状图进行专家数量选择。

Result: 避免了EM型实现中常见的近似M步，保证目标函数单调上升和全局收敛到平稳点；在生物蛋白质 - 蛋白质相互作用预测实验中，比统计和机器学习基线有更高准确性和更好校准的概率。

Conclusion: 所提出的方法解决了softmax门控多项逻辑MoE架构在分类中的稳定训练和模型选择问题，具有良好实用性和有效性。

Abstract: Mixture-of-Experts (MoE) architectures combine specialized predictors through a learned gate and are effective across regression and classification, but for classification with softmax multinomial-logistic gating, rigorous guarantees for stable maximum-likelihood training and principled model selection remain limited. We address both issues in the full-data (batch) regime. First, we derive a batch minorization-maximization (MM) algorithm for softmax-gated multinomial-logistic MoE using an explicit quadratic minorizer, yielding coordinate-wise closed-form updates that guarantee monotone ascent of the objective and global convergence to a stationary point (in the standard MM sense), avoiding approximate M-steps common in EM-type implementations. Second, we prove finite-sample rates for conditional density estimation and parameter recovery, and we adapt dendrograms of mixing measures to the classification setting to obtain a sweep-free selector of the number of experts that achieves near-parametric optimal rates after merging redundant fitted atoms. Experiments on biological protein--protein interaction prediction validate the full pipeline, delivering improved accuracy and better-calibrated probabilities than strong statistical and machine-learning baselines.

</details>


### [525] [BFTS: Thompson Sampling with Bayesian Additive Regression Trees](https://arxiv.org/abs/2602.07767)
*Ruizhe Deng,Bibhas Chakraborty,Ran Chen,Yan Shuo Tan*

Main category: stat.ML

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Contextual bandits are a core technology for personalized mobile health interventions, where decision-making requires adapting to complex, non-linear user behaviors. While Thompson Sampling (TS) is a preferred strategy for these problems, its performance hinges on the quality of the underlying reward model. Standard linear models suffer from high bias, while neural network approaches are often brittle and difficult to tune in online settings. Conversely, tree ensembles dominate tabular data prediction but typically rely on heuristic uncertainty quantification, lacking a principled probabilistic basis for TS. We propose Bayesian Forest Thompson Sampling (BFTS), the first contextual bandit algorithm to integrate Bayesian Additive Regression Trees (BART), a fully probabilistic sum-of-trees model, directly into the exploration loop. We prove that BFTS is theoretically sound, deriving an information-theoretic Bayesian regret bound of $\tilde{O}(\sqrt{T})$. As a complementary result, we establish frequentist minimax optimality for a "feel-good" variant, confirming the structural suitability of BART priors for non-parametric bandits. Empirically, BFTS achieves state-of-the-art regret on tabular benchmarks with near-nominal uncertainty calibration. Furthermore, in an offline policy evaluation on the Drink Less micro-randomized trial, BFTS improves engagement rates by over 30% compared to the deployed policy, demonstrating its practical effectiveness for behavioral interventions.

</details>


### [526] [Graph-based Semi-Supervised Learning via Maximum Discrimination](https://arxiv.org/abs/2602.08042)
*Nadav Katz,Ariel Jaffe*

Main category: stat.ML

TL;DR: 提出图方法AUC - spec，通过优化AUC计算低维表示以最大化类别分离，实验显示其在多方面表现良好。


<details>
  <summary>Details</summary>
Motivation: 经典图半监督学习方法在处理标签分布复杂的数据时效果欠佳，需要更好的方法。

Method: 开发AUC - spec方法，通过优化AUC计算低维表示，在流形乘积模型下进行详细分析。

Result: AUC - spec能平衡类别分离和图平滑性，在合成和真实数据集上有有竞争力的结果，且计算效率与经典和最先进方法相当。

Conclusion: AUC - spec是一种有效的图半监督学习方法，在标签分布复杂的数据上表现良好。

Abstract: Semi-supervised learning (SSL) addresses the critical challenge of training accurate models when labeled data is scarce but unlabeled data is abundant. Graph-based SSL (GSSL) has emerged as a popular framework that captures data structure through graph representations. Classic graph SSL methods, such as Label Propagation and Label Spreading, aim to compute low-dimensional representations where points with the same labels are close in representation space. Although often effective, these methods can be suboptimal on data with complex label distributions. In our work, we develop AUC-spec, a graph approach that computes a low-dimensional representation that maximizes class separation. We compute this representation by optimizing the Area Under the ROC Curve (AUC) as estimated via the labeled points. We provide a detailed analysis of our approach under a product-of-manifold model, and show that the required number of labeled points for AUC-spec is polynomial in the model parameters. Empirically, we show that AUC-spec balances class separation with graph smoothness. It demonstrates competitive results on synthetic and real-world datasets while maintaining computational efficiency comparable to the field's classic and state-of-the-art methods.

</details>


### [527] [Information Geometry of Absorbing Markov-Chain and Discriminative Random Walks](https://arxiv.org/abs/2602.08185)
*Masanari Kimura*

Main category: stat.ML

TL;DR: 本文从信息几何角度重新审视判别式随机游走（DRWs）用于半监督节点分类，推导相关表达式，引入敏感性分数并可用于多种策略。


<details>
  <summary>Details</summary>
Motivation: DRWs用于半监督节点分类时理论基础尚不完整。

Method: 把吸收马尔可夫链上特定类的命中时间法则作为统计流形，从对数线性边权重模型推导相关表达式，利用几何性质引入敏感性分数。

Result: 得到命中时间概率质量函数等闭式表达式，发现种子节点的Fisher矩阵为秩一，引入敏感性分数。

Conclusion: 敏感性分数可用于主动标签获取、边重新加权和解释等有原则的策略。

Abstract: Discriminative Random Walks (DRWs) are a simple yet powerful tool for semi-supervised node classification, but their theoretical foundations remain fragmentary. We revisit DRWs through the lens of information geometry, treating the family of class-specific hitting-time laws on an absorbing Markov chain as a statistical manifold. Starting from a log-linear edge-weight model, we derive closed-form expressions for the hitting-time probability mass function, its full moment hierarchy, and the observed Fisher information. The Fisher matrix of each seed node turns out to be rank-one, taking the quotient by its null space yields a low-dimensional, globally flat manifold that captures all identifiable directions of the model. Leveraging the geometry, we introduce a sensitivity score for unlabeled nodes that bounds, and in one-dimensional cases attains, the maximal first-order change in DRW betweenness under unit Fisher perturbations. The score can lead to principled strategies for active label acquisition, edge re-weighting, and explanation.

</details>


### [528] [Discrete Adjoint Schrödinger Bridge Sampler](https://arxiv.org/abs/2602.08243)
*Wei Guo,Yuchen Zhu,Xiaochen Du,Juno Nam,Yongxin Chen,Rafael Gómez-Bombarelli,Guan-Horng Liu,Molei Tao,Jaemoo Choi*

Main category: stat.ML

TL;DR: 提出离散ASBS框架将AM和ASBS扩展到离散空间，理论分析离散SB问题，实证显示该框架在样本质量、训练效率和可扩展性上有优势。


<details>
  <summary>Details</summary>
Motivation: 离散神经采样器学习有挑战，高效的SOC求解器如AM在离散空间未被探索，需填补这一空白。

Method: 揭示AM核心机制与状态空间无关，引入离散ASBS框架，分析离散SB问题最优条件及其与SOC的联系。

Result: 离散ASBS在样本质量上有竞争力，在训练效率和可扩展性上有显著优势。

Conclusion: 离散ASBS框架有效解决了在离散空间学习神经采样器的问题。

Abstract: Learning discrete neural samplers is challenging due to the lack of gradients and combinatorial complexity. While stochastic optimal control (SOC) and Schrödinger bridge (SB) provide principled solutions, efficient SOC solvers like adjoint matching (AM), which excel in continuous domains, remain unexplored for discrete spaces. We bridge this gap by revealing that the core mechanism of AM is $\mathit{state}\text{-}\mathit{space~agnostic}$, and introduce $\mathbf{discrete~ASBS}$, a unified framework that extends AM and adjoint Schrödinger bridge sampler (ASBS) to discrete spaces. Theoretically, we analyze the optimality conditions of the discrete SB problem and its connection to SOC, identifying a necessary cyclic group structure on the state space to enable this extension. Empirically, discrete ASBS achieves competitive sample quality with significant advantages in training efficiency and scalability.

</details>


### [529] [A Statistical Framework for Alignment with Biased AI Feedback](https://arxiv.org/abs/2602.08259)
*Xintao Xia,Zhiqiu Xia,Linjun Zhang,Zhanrui Cai*

Main category: stat.ML

TL;DR: 本文提出两种去偏对齐方法，在多个任务上提升了对齐效率。


<details>
  <summary>Details</summary>
Motivation: 现代对齐流程用大语言模型评估替代人类偏好标签，但AI标签有系统偏差，与高质量人类反馈数据集有差距。

Method: 提出去偏直接偏好优化（DDPO）和去偏身份偏好优化（DIPO）两种方法，DDPO结合残差校正和密度比重新加权，DIPO直接估计人类偏好概率。

Result: 实证研究表明，两种方法显著提高了对齐效率，性能接近基于全人类标记数据训练的模型。

Conclusion: DDPO为大规模对齐提供实用且计算高效的解决方案，DIPO是达到半参数效率边界的稳健统计最优替代方案。

Abstract: Modern alignment pipelines are increasingly replacing expensive human preference labels with evaluations from large language models (LLM-as-Judge). However, AI labels can be systematically biased compared to high-quality human feedback datasets. In this paper, we develop two debiased alignment methods within a general framework that accommodates heterogeneous prompt-response distributions and external human feedback sources. Debiased Direct Preference Optimization (DDPO) augments standard DPO with a residual-based correction and density-ratio reweighting to mitigate systematic bias, while retaining DPO's computational efficiency. Debiased Identity Preference Optimization (DIPO) directly estimates human preference probabilities without imposing a parametric reward model. We provide theoretical guarantees for both methods: DDPO offers a practical and computationally efficient solution for large-scale alignment, whereas DIPO serves as a robust, statistically optimal alternative that attains the semiparametric efficiency bound. Empirical studies on sentiment generation, summarization, and single-turn dialogue demonstrate that the proposed methods substantially improve alignment efficiency and recover performance close to that of an oracle trained on fully human-labeled data.

</details>


### [530] [Is Flow Matching Just Trajectory Replay for Sequential Data?](https://arxiv.org/abs/2602.08318)
*Soon Hoe Lim,Shizheng Lin,Michael W. Mahoney,N. Benjamin Erichson*

Main category: stat.ML

TL;DR: 研究流匹配在时间序列生成中学习的是通用动态结构还是轨迹回放，推导经验FM目标的速度场，提出改进采样和逼近方案。


<details>
  <summary>Details</summary>
Motivation: 探究流匹配在时间序列生成中是学习通用动态结构还是仅进行轨迹回放。

Method: 推导经验FM目标在序列数据上的速度场，对高斯条件路径进行研究。

Result: 发现隐含采样器是一个ODE，最优场有闭合表达式，基于此提出的封闭形式采样器能在无训练下进行强概率预测。

Conclusion: 基于最优场结构可改进ODE生成的效率和数值鲁棒性，封闭形式采样器在非线性动力系统基准上表现良好。

Abstract: Flow matching (FM) is increasingly used for time-series generation, but it is not well understood whether it learns a general dynamical structure or simply performs an effective "trajectory replay". We study this question by deriving the velocity field targeted by the empirical FM objective on sequential data, in the limit of perfect function approximation. For the Gaussian conditional paths commonly used in practice, we show that the implied sampler is an ODE whose dynamics constitutes a nonparametric, memory-augmented continuous-time dynamical system. The optimal field admits a closed-form expression as a similarity-weighted mixture of instantaneous velocities induced by past transitions, making the dataset dependence explicit and interpretable. This perspective positions neural FM models trained by stochastic optimization as parametric surrogates of an ideal nonparametric solution. Using the structure of the optimal field, we study sampling and approximation schemes that improve the efficiency and numerical robustness of ODE-based generation. On nonlinear dynamical system benchmarks, the resulting closed-form sampler yields strong probabilistic forecasts directly from historical transitions, without training.

</details>


### [531] [Schrödinger bridge problem via empirical risk minimization](https://arxiv.org/abs/2602.08374)
*Denis Belomestny,Alexey Naumov,Nikita Puchkin,Denis Suchkov*

Main category: stat.ML

TL;DR: 论文研究端点分布只能通过样本获取时的薛定谔桥问题，提出学习理论方法并进行数值实验。


<details>
  <summary>Details</summary>
Motivation: 解决经典计算方法在端点分布仅能通过样本获取时估计薛定谔势的问题。

Method: 将薛定谔系统改写为满足非线性不动点方程的单个正变换势，用经验风险最小化估计该势。在子高斯假设下建立经验风险的一致性。将学习的势代入桥的随机控制表示生成样本。

Result: 通过数值实验展示了所提方法的性能。

Conclusion: 所提出的学习理论路线是解决端点分布仅通过样本获取时薛定谔桥问题的有效方法。

Abstract: We study the Schrödinger bridge problem when the endpoint distributions are available only through samples. Classical computational approaches estimate Schrödinger potentials via Sinkhorn iterations on empirical measures and then construct a time-inhomogeneous drift by differentiating a kernel-smoothed dual solution. In contrast, we propose a learning-theoretic route: we rewrite the Schrödinger system in terms of a single positive transformed potential that satisfies a nonlinear fixed-point equation and estimate this potential by empirical risk minimization over a function class. We establish uniform concentration of the empirical risk around its population counterpart under sub-Gaussian assumptions on the reference kernel and terminal density. We plug the learned potential into a stochastic control representation of the bridge to generate samples. We illustrate performance of the suggested approach with numerical experiments.

</details>


### [532] [Winner's Curse Drives False Promises in Data-Driven Decisions: A Case Study in Refugee Matching](https://arxiv.org/abs/2602.08892)
*Hamsa Bastani,Osbert Bastani,Bryce McLaughlin*

Main category: stat.ML

TL;DR: 论文指出数据驱动决策中基于模型的策略评估因胜者诅咒问题给出不准确结果，虽有常见理由，但实证和理论分析表明这些理由无法避免该问题，以此反对基于模型的评估方法。


<details>
  <summary>Details</summary>
Motivation: 解决数据驱动决策中保证所学决策策略能实现预期效益的准确政策评估的挑战，指出基于模型的策略评估存在胜者诅咒导致过高估计真实效益的问题。

Method: 搜索管理科学领域过去十年55篇相关论文；进行理论分析，即便常见理由都成立，胜者诅咒仍可能导致虚假高收益；基于难民匹配问题进行模拟研究。

Result: 模型方法在真实效益为零时仍报告约60%的稳定收益，与文献中22 - 75%的改善幅度相当。

Conclusion: 有强有力证据反对基于模型的评估方法。

Abstract: A major challenge in data-driven decision-making is accurate policy evaluation-i.e., guaranteeing that a learned decision-making policy achieves the promised benefits. A popular strategy is model-based policy evaluation, which estimates a model from data to infer counterfactual outcomes. This strategy is known to produce unwarrantedly optimistic estimates of the true benefit due to the winner's curse. We searched the recent literature on data-driven decision-making, identifying a sample of 55 papers published in the Management Science in the past decade; all but two relied on this flawed methodology. Several common justifications are provided: (1) the estimated models are accurate, stable, and well-calibrated, (2) the historical data uses random treatment assignment, (3) the model family is well-specified, and (4) the evaluation methodology uses sample splitting. Unfortunately, we show that no combination of these justifications avoids the winner's curse. First, we provide a theoretical analysis demonstrating that the winner's curse can cause large, spurious reported benefits even when all these justifications hold. Second, we perform a simulation study based on the recent and consequential data-driven refugee matching problem. We construct a synthetic refugee matching environment (calibrated to closely match the real setting) but designed so that no assignment policy can improve expected employment compared to random assignment. Model-based methods report large, stable gains of around 60% even when the true effect is zero; these gains are on par with improvements of 22-75% reported in the literature. Our results provide strong evidence against model-based evaluation.

</details>


### [533] [Provably robust learning of regression neural networks using $β$-divergences](https://arxiv.org/abs/2602.08933)
*Abhik Ghosh,Suryasis Jana*

Main category: stat.ML

TL;DR: 提出基于β - 散度的回归神经网络稳健学习框架rRNet，有收敛保证和强鲁棒性，模拟和实际数据分析显示优势。


<details>
  <summary>Details</summary>
Motivation: 现有回归神经网络训练方法对异常值敏感，稳健训练方法有局限且理论保障不足。

Method: 提出rRNet框架，通过交替优化方案实现，建立收敛保证，用影响函数刻画局部鲁棒性，证明其渐近崩溃点。

Result: rRNet在模型中达到50%最优渐近崩溃点，模拟和真实数据分析显示其在函数逼近和预测任务中优于现有方法。

Conclusion: rRNet是一种有效的回归神经网络稳健学习框架，具有良好的理论性质和实际优势。

Abstract: Regression neural networks (NNs) are most commonly trained by minimizing the mean squared prediction error, which is highly sensitive to outliers and data contamination. Existing robust training methods for regression NNs are often limited in scope and rely primarily on empirical validation, with only a few offering partial theoretical guarantees. In this paper, we propose a new robust learning framework for regression NNs based on the $β$-divergence (also known as the density power divergence) which we call `rRNet'. It applies to a broad class of regression NNs, including models with non-smooth activation functions and error densities, and recovers the classical maximum likelihood learning as a special case. The rRNet is implemented via an alternating optimization scheme, for which we establish convergence guarantees to stationary points under mild, verifiable conditions. The (local) robustness of rRNet is theoretically characterized through the influence functions of both the parameter estimates and the resulting rRNet predictor, which are shown to be bounded for suitable choices of the tuning parameter $β$, depending on the error density. We further prove that rRNet attains the optimal 50\% asymptotic breakdown point at the assumed model for all $β\in(0, 1]$, providing a strong global robustness guarantee that is largely absent for existing NN learning methods. Our theoretical results are complemented by simulation experiments and real-data analyses, illustrating practical advantages of rRNet over existing approaches in both function approximation problems and prediction tasks with noisy observations.

</details>


### [534] [Online monotone density estimation and log-optimal calibration](https://arxiv.org/abs/2602.08927)
*Rohan Hore,Ruodu Wang,Aaditya Ramdas*

Main category: stat.ML

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study the problem of online monotone density estimation, where density estimators must be constructed in a predictable manner from sequentially observed data. We propose two online estimators: an online analogue of the classical Grenander estimator, and an expert aggregation estimator inspired by exponential weighting methods from the online learning literature. In the well-specified stochastic setting, where the underlying density is monotone, we show that the expected cumulative log-likelihood gap between the online estimators and the true density admits an $O(n^{1/3})$ bound. We further establish a $\sqrt{n\log{n}}$ pathwise regret bound for the expert aggregation estimator relative to the best offline monotone estimator chosen in hindsight, under minimal regularity assumptions on the observed sequence. As an application of independent interest, we show that the problem of constructing log-optimal p-to-e calibrators for sequential hypothesis testing can be formulated as an online monotone density estimation problem. We adapt the proposed estimators to build empirically adaptive p-to-e calibrators and establish their optimality. Numerical experiments illustrate the theoretical results.

</details>


### [535] [Amortising Inference and Meta-Learning Priors in Neural Networks](https://arxiv.org/abs/2602.08782)
*Tommy Rochussen,Vincent Fortuin*

Main category: stat.ML

TL;DR: 提出从数据集集合中学习权重先验的方法，连接贝叶斯深度学习和概率元学习领域，可实现多种功能。


<details>
  <summary>Details</summary>
Motivation: 解决贝叶斯深度学习中没有先验信念时如何维持贝叶斯方法，不清楚如何用模型参数的先验分布表示预测任务信念的问题。

Method: 引入对每个数据集进行摊销变分推理的方法，开发出一个独特模型，其潜变量是BNN的权重集，解码器是由潜变量样本参数化的神经网络。

Result: 可研究指定先验下贝叶斯神经网络的行为，将其用作灵活生成模型，实现神经过程中之前难以实现的功能。

Conclusion: 所提出的方法有效连接了贝叶斯深度学习和概率元学习领域，具有多种应用价值。

Abstract: One of the core facets of Bayesianism is in the updating of prior beliefs in light of new evidence$\text{ -- }$so how can we maintain a Bayesian approach if we have no prior beliefs in the first place? This is one of the central challenges in the field of Bayesian deep learning, where it is not clear how to represent beliefs about a prediction task by prior distributions over model parameters. Bridging the fields of Bayesian deep learning and probabilistic meta-learning, we introduce a way to $\textit{learn}$ a weights prior from a collection of datasets by introducing a way to perform per-dataset amortised variational inference. The model we develop can be viewed as a neural process whose latent variable is the set of weights of a BNN and whose decoder is the neural network parameterised by a sample of the latent variable itself. This unique model allows us to study the behaviour of Bayesian neural networks under well-specified priors, use Bayesian neural networks as flexible generative models, and perform desirable but previously elusive feats in neural processes such as within-task minibatching or meta-learning under extreme data-starvation.

</details>


### [536] [Cutting Through the Noise: On-the-fly Outlier Detection for Robust Training of Machine Learning Interatomic Potentials](https://arxiv.org/abs/2602.08849)
*Terry C. W. Lam,Niamh O'Neill,Christoph Schran,Lars L. Schaaf*

Main category: stat.ML

TL;DR: 提出一种实时离群值检测方案，可自动降低噪声样本权重，在不完美数据集上训练鲁棒模型。


<details>
  <summary>Details</summary>
Motivation: 机器学习原子间势的准确性受含数值噪声的参考数据影响，现有缓解策略难以扩展到大型数据集。

Method: 通过指数移动平均跟踪损失分布，在单次训练中识别离群值。

Result: 防止过拟合，性能与迭代细化基线相当且开销显著降低，能从非收敛参考数据中恢复准确物理观测值，在SPICE数据集上降低能量误差。

Conclusion: 该框架为不同规模不完美数据集训练鲁棒模型提供简单自动的解决方案。

Abstract: The accuracy of machine learning interatomic potentials suffers from reference data that contains numerical noise. Often originating from unconverged or inconsistent electronic-structure calculations, this noise is challenging to identify. Existing mitigation strategies such as manual filtering or iterative refinement of outliers, require either substantial expert effort or multiple expensive retraining cycles, making them difficult to scale to large datasets. Here, we introduce an on-the-fly outlier detection scheme that automatically down-weights noisy samples, without requiring additional reference calculations. By tracking the loss distribution via an exponential moving average, this unsupervised method identifies outliers throughout a single training run. We show that this approach prevents overfitting and matches the performance of iterative refinement baselines with significantly reduced overhead. The method's effectiveness is demonstrated by recovering accurate physical observables for liquid water from unconverged reference data, including diffusion coefficients. Furthermore, we validate its scalability by training a foundation model for organic chemistry on the SPICE dataset, where it reduces energy errors by a factor of three. This framework provides a simple, automated solution for training robust models on imperfect datasets across dataset sizes.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [537] [BayesFlow 2.0: Multi-Backend Amortized Bayesian Inference in Python](https://arxiv.org/abs/2602.07098)
*Lars Kühmichel,Jerry M. Huang,Valentin Pratz,Jonas Arruda,Hans Olischläger,Daniel Habermann,Simon Kucharsky,Lasse Elsemüller,Aayush Mishra,Niels Bracher,Svenja Jedhoff,Marvin Schmitt,Paul-Christian Bürkner,Stefan T. Radev*

Main category: stat.CO

TL;DR: 介绍了Python库BayesFlow 2.0用于通用的摊销贝叶斯推理（ABI），展示其功能和通过案例研究说明有广泛应用潜力。


<details>
  <summary>Details</summary>
Motivation: 现代贝叶斯推理计算慢，ABI可解决计算挑战，开发BayesFlow 2.0库以支持通用ABI。

Method: 开发了BayesFlow 2.0库，其支持多种流行深度学习后端，有丰富生成网络等，并用案例研究和与类似软件对比。

Result: 展示了库的功能，通过案例研究和对比说明其精简、用户友好的工作流程。

Conclusion: BayesFlow 2.0的工作流程有潜力支持广泛应用。

Abstract: Modern Bayesian inference involves a mixture of computational methods for estimating, validating, and drawing conclusions from probabilistic models as part of principled workflows. An overarching motif of many Bayesian methods is that they are relatively slow, which often becomes prohibitive when fitting complex models to large data sets. Amortized Bayesian inference (ABI) offers a path to solving the computational challenges of Bayes. ABI trains neural networks on model simulations, rewarding users with rapid inference of any model-implied quantity, such as point estimates, likelihoods, or full posterior distributions. In this work, we present the Python library BayesFlow, Version 2.0, for general-purpose ABI. Along with direct posterior, likelihood, and ratio estimation, the software includes support for multiple popular deep learning backends, a rich collection of generative networks for sampling and density estimation, complete customization and high-level interfaces, as well as new capabilities for hyperparameter optimization, design optimization, and hierarchical modeling. Using a case study on dynamical system parameter estimation, combined with comparisons to similar software, we show that our streamlined, user-friendly workflow has strong potential to support broad adoption.

</details>


### [538] [PoissonRatioUQ: An R package for band ratio uncertainty quantification](https://arxiv.org/abs/2602.07165)
*Matthew LeDuc,Tomoko Matsuo*

Main category: stat.CO

TL;DR: 介绍用于计数比率问题的贝叶斯建模和不确定性量化的R包，提供不同检索选项和额外不确定性量化能力。


<details>
  <summary>Details</summary>
Motivation: 解决涉及计数比率问题的贝叶斯建模和不确定性量化。

Method: 假设感兴趣的量是泊松均值的比率而非计数的比率，针对有无空间信息的问题提供不同检索选项。

Result: 开发出用于计数比率问题的R包，具备多种检索选项和额外的不确定性量化能力。

Conclusion: 成功开发出适用于计数比率问题的贝叶斯建模和不确定性量化的R包。

Abstract: We introduce an R package for Bayesian modeling and uncertainty quantification for problems involving count ratios. The modeling relies on the assumption that the quantity of interest is the ratio of Poisson means rather than the ratio of counts. We provide multiple different options for retrieval of this quantity for problems with and without spatial information included. Some added capability for uncertainty quantification for problems of the form $Z=(mT+z_0)^{p}$, where $Z$ is the intensity ratio and $T$ the quantity of interest, is included.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [539] [The CAPSARII Approach to Cyber-Secure Wearable, Ultra-Low-Power Networked Sensors for Soldier Health Monitoring](https://arxiv.org/abs/2602.08080)
*Luciano Bozzi,Christian Celidonio,Umberto Nuzzi,Massimo Biagini,Stefano Cherubin,Asbjørn Djupdal,Tor Andre Haugdahl,Andrea Aliverti,Alessandra Angelucci,Giovanni Agosta,Gerardo Pelosi,Paolo Belluco,Samuele Polistina,Riccardo Volpi,Luigi Malagò,Michael Schneider,Florian Wieczorek,Xabier Eguiluz*

Main category: cs.ET

TL;DR: 欧洲防务局修订的CDP强调提升地面作战能力，CAPSARII项目提出创新可穿戴系统和IoBT框架，可监测士兵状态，提升作战效能等。


<details>
  <summary>Details</summary>
Motivation: 欧洲防务局修订的CDP将提升士兵装备以增强地面作战能力列为优先事项，促使项目开展以提升作战效能。

Method: 提出可穿戴系统和IoBT框架，通过监测多种参数，利用边缘节点AI模型提供实时战术决策支持，借助云分析进行数据分析和对比研究，同时进行软硬件优化和安全防护。

Result: 未提及具体结果。

Conclusion: 该创新方法旨在提供强大的数据驱动决策支持工具，转变军事行动。

Abstract: The European Defence Agency's revised Capability Development Plan (CDP) identifies as a priority improving ground combat capabilities by enhancing soldiers' equipment for better protection. The CAPSARII project proposes in innovative wearable system and Internet of Battlefield Things (IoBT) framework to monitor soldiers' physiological and psychological status, aiding tactical decisions and medical support. The CAPSARII system will enhance situational awareness and operational effectiveness by monitoring physiological, movement and environmental parameters, providing real-time tactical decision support through AI models deployed on edge nodes and enable data analysis and comparative studies via cloud-based analytics. CAPSARII also aims at improving usability through smart textile integration, longer battery life, reducing energy consumption through software and hardware optimizations, and address security concerns with efficient encryption and strong authentication methods. This innovative approach aims to transform military operations by providing a robust, data-driven decision support tool.

</details>


### [540] [Physical Analog Kolmogorov-Arnold Networks based on Reconfigurable Nonlinear-Processing Units](https://arxiv.org/abs/2602.07518)
*Manuel Escudero,Mohamadreza Zolfagharinejad,Sjoerd van den Belt,Nikolaos Alachiotis,Wilfred G. van der Wiel*

Main category: cs.ET

TL;DR: 提出物理模拟KAN架构，用RNPUs实现边缘函数，展示准确的函数逼近且能耗低、面积小，表明RNPUs可扩展，模拟KAN架构是高效模拟神经网络硬件的途径。


<details>
  <summary>Details</summary>
Motivation: 解决Kolmogorov - Arnold Networks (KANs)在硬件中高效实现非线性的挑战。

Method: 引入使用可重构非线性处理单元(RNPUs)在材料中实现边缘函数的物理模拟KAN架构，将多个RNPUs组合成边缘处理器，组装成可重构的模拟KAN (aKAN)架构并集成混合信号接口。

Result: 能实现紧凑的KAN风格回归和分类，展示了准确的函数逼近，能量每推理约250 pJ，端到端推理延迟约600 ns，与数字定点MLP相比，能耗降低约10² - 10³倍，面积减少约10倍。

Conclusion: RNPUs是可扩展的、硬件原生的非线性计算原语，模拟KAN架构是基于硅的、节能、低延迟、小尺寸的模拟神经网络硬件的现实途径，尤其适用于边缘推理。

Abstract: Kolmogorov-Arnold Networks (KANs) shift neural computation from linear layers to learnable nonlinear edge functions, but implementing these nonlinearities efficiently in hardware remains an open challenge. Here we introduce a physical analog KAN architecture in which edge functions are realized in materia using reconfigurable nonlinear-processing units (RNPUs): multi-terminal nanoscale silicon devices whose input-output characteristics are tuned via control voltages. By combining multiple RNPUs into an edge processor and assembling these blocks into a reconfigurable analog KAN (aKAN) architecture with integrated mixed-signal interfacing, we establish a realistic system-level hardware implementation that enables compact KAN-style regression and classification with programmable nonlinear transformations. Using experimentally calibrated RNPU models and hardware measurements, we demonstrate accurate function approximation across increasing task complexity while requiring fewer or comparable trainable parameters than multilayer perceptrons (MLPs). System-level estimates indicate an energy per inference of $\sim$250 pJ and an end-to-end inference latency of $\sim$600 ns for a representative workload, corresponding to a $\sim$10$^{2}$-10$^{3}\times$ reduction in energy accompanied by a $\sim$10$\times$ reduction in area compared to a digital fixed-point MLP at similar approximation error. These results establish RNPUs as scalable, hardware-native nonlinear computing primitives and identify analog KAN architectures as a realistic silicon-based pathway toward energy-, latency-, and footprint-efficient analog neural-network hardware, particularly for edge inference.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [541] [Large language models for spreading dynamics in complex systems](https://arxiv.org/abs/2602.08085)
*Shuyu Jiang,Hao Ren,Yichang Gao,Yi-Cheng Zhang,Li Qi,Dayong Xiao,Jie Fan,Rui Tang,Wei Wang*

Main category: physics.soc-ph

TL;DR: 本文综述了大语言模型（LLMs）在传播动力学研究中的应用进展，涉及数字和生物流行病领域，探讨其与传统框架关系，从多视角分析LLMs提升作用并讨论挑战与方向。


<details>
  <summary>Details</summary>
Motivation: 传统经典建模框架难以纳入传播过程中的多交互因素，而LLMs有能力分析这些不同影响因素，其在传播动力学中的角色和影响成为研究热点。

Method: 从复杂系统视角审视流行病建模基础，探讨LLM方法与传统框架联系，从流行病建模、检测与监测、预测与管理三个关键视角系统综述相关研究。

Result: 展示了LLMs在数字和生物流行病领域对传播动力学研究的增强作用。

Conclusion: 讨论了LLMs应用于传播动力学研究的开放性挑战和潜在研究方向。

Abstract: Spreading dynamics is a central topic in the physics of complex systems and network science, providing a unified framework for understanding how information, behaviors, and diseases propagate through interactions among system units. In many propagation contexts, spreading processes are influenced by multiple interacting factors, such as information expression patterns, cultural contexts, living environments, cognitive preferences, and public policies, which are difficult to incorporate directly into classical modeling frameworks. Recently, large language models (LLMs) have exhibited strong capabilities in natural language understanding, reasoning, and generation, enabling explicit perception of semantic content and contextual cues in spreading processes, thereby supporting the analysis of the different influencing factors. Beyond serving as external analytical tools, LLMs can also act as interactive agents embedded in propagation systems, potentially influencing spreading pathways and feedback structures. Consequently, the roles and impacts of LLMs on spreading dynamics have become an active and rapidly growing research area across multiple research disciplines. This review provides a comprehensive overview of recent advances in applying LLMs to the study of spreading dynamics across two representative domains: digital epidemics, such as misinformation and rumors, and biological epidemics, including infectious disease outbreaks. We first examine the foundations of epidemic modeling from a complex-systems perspective and discuss how LLM-based approaches relate to traditional frameworks. We then systematically review recent studies from three key perspectives, which are epidemic modeling, epidemic detection and surveillance, and epidemic prediction and management, to clarify how LLMs enhance these areas. Finally, open challenges and potential research directions are discussed.

</details>


### [542] [When do neural ordinary differential equations generalize on complex networks?](https://arxiv.org/abs/2602.08980)
*Moritz Laber,Tina Eliassi-Rad,Brennan Klein*

Main category: physics.soc-ph

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Neural ordinary differential equations (neural ODEs) can effectively learn dynamical systems from time series data, but their behavior on graph-structured data remains poorly understood, especially when applied to graphs with different size or structure than encountered during training. We study neural ODEs ($\mathtt{nODE}$s) with vector fields following the Barabási-Barzel form, trained on synthetic data from five common dynamical systems on graphs. Using the $\mathbb{S}^1$-model to generate graphs with realistic and tunable structure, we find that degree heterogeneity and the type of dynamical system are the primary factors in determining $\mathtt{nODE}$s' ability to generalize across graph sizes and properties. This extends to $\mathtt{nODE}$s' ability to capture fixed points and maintain performance amid missing data. Average clustering plays a secondary role in determining $\mathtt{nODE}$ performance. Our findings highlight $\mathtt{nODE}$s as a powerful approach to understanding complex systems but underscore challenges emerging from degree heterogeneity and clustering in realistic graphs.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [543] [Constrained Pricing under Finite Mixtures of Logit](https://arxiv.org/abs/2602.08119)
*Hoang Giang Pham,Tien Mai*

Main category: math.OC

TL;DR: 研究多项式和混合logit需求模型下的约束定价问题，给出多项式时间近似方案，数值实验表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有混合logit定价研究多聚焦无约束场景，而实际中价格常受业务或监管约束，适用性受限。

Method: 对多项式logit模型，通过指数锥规划重新表述问题得到多项式时间近似方案；对有限混合logit模型，将问题重新表述为双线性指数锥规划，使用分支定界算法。

Result: 多项式logit模型能在多项式时间内得到ε-最优解；有限混合logit模型在客户细分数量有界时有多项式时间近似方案，数值实验表现优于现有基线。

Conclusion: 所提出的方法能有效解决多项式和有限混合logit需求模型下的约束定价问题。

Abstract: The mixed logit model is a flexible and widely used demand model in pricing and revenue management. However, existing work on mixed-logit pricing largely focuses on unconstrained settings, limiting its applicability in practice where prices are subject to business or regulatory constraints. We study the constrained pricing problem under multinomial and mixed logit demand models. For the multinomial logit model, corresponding to a single customer segment, we show that the constrained pricing problem admits a polynomial-time approximation scheme (PTAS) via a reformulation based on exponential cone programming, yielding an $\varepsilon$-optimal solution in polynomial time. For finite mixed logit models with $T$ customer segments, we reformulate the problem as a bilinear exponential cone program with $O(T)$ bilinear terms. This structure enables a Branch-and-Bound algorithm whose complexity is exponential only in $T$. Consequently, constrained pricing under finite mixtures of logit admits a PTAS when the number of customer segments is bounded. Numerical experiments demonstrate strong performance relative to state-of-the-art baselines.

</details>


### [544] [A Two-Layer Framework for Joint Online Configuration Selection and Admission Control](https://arxiv.org/abs/2602.07663)
*Owen Shen,Haoran Xu,Yinyu Ye,Peter Glynn,Patrick Jaillet*

Main category: math.OC

TL;DR: 研究在线配置选择与准入控制问题，引入切换感知流体预言机作为基准，设计SP - UCB--OLP算法，实现O(√KT)遗憾。


<details>
  <summary>Details</summary>
Motivation: 解决LLM服务、GPU调度和收益管理中出现的在线配置选择与准入控制问题，并找到合适的基准进行评估。

Method: 引入切换感知流体预言机上界任何在线策略，推导最大最小公式评估基准，通过原始 - 对偶最优条件刻画鞍点，设计SP - UCB--OLP算法。

Result: SP - UCB--OLP算法可解决乐观鞍点问题，实现O(√KT)遗憾。

Conclusion: 所提出的方法和算法能有效应对在线配置选择与准入控制问题，并在性能上有理论保障。

Abstract: We study online configuration selection with admission control problem, which arises in LLM serving, GPU scheduling, and revenue management. In a planning horizon with $T$ periods, we consider a two-layer framework for the decisions made within each time period. In the first layer, the decision maker selects one of the $K$ configurations (ex. quantization, parallelism, fare class) which induces distribution over the reward-resource pair of the incoming request. In the second layer, the decision maker observes the request and then decides whether to accept it or not.
  Benchmarking this framework requires care. We introduce a \textbf{switching-aware fluid oracle} that accounts for the value of mixing configurations over time, provably upper-bounding any online policy. We derive a max-min formulation for evaluating the benchmark, and we characterize saddle points of the max-min problem via primal-dual optimality conditions linking equilibrium, feasibility, and complementarity. This guides the design of \textbf{SP-UCB--OLP} algorithm, which solves an optimistic saddle point problem and achieves $\tilde{O}(\sqrt{KT})$ regret.

</details>


### [545] [Adaptive Matrix Online Learning through Smoothing with Guarantees for Nonsmooth Nonconvex Optimization](https://arxiv.org/abs/2602.08232)
*Ruichen Jiang,Zakaria Mhammedi,Mehryar Mohri,Aryan Mokhtari*

Main category: math.OC

TL;DR: 研究受算子范数约束的矩阵变量在线线性优化，提出两种避免二次投影的高效方法，降低计算成本，并导出两个矩阵优化器，证明其在非光滑非凸设置下的收敛性。


<details>
  <summary>Details</summary>
Motivation: 现有Shampoo类方法在解决受算子范数约束的矩阵变量在线线性优化问题时，需解决代价高昂的二次投影子问题。

Method: 将基于梯度的预测方案扩展到自适应矩阵在线学习，构建核范数的平滑势函数族，定义平滑的可容许性概念，提出基于高斯随机平滑的自适应FTPL方法和基于确定性双曲平滑的FAML方法。

Result: 两种方法都有闭式更新，在常数因子内匹配单边Shampoo的遗憾，显著降低计算成本；导出的Pion和Leon优化器在非光滑非凸设置下有收敛保证。

Conclusion: 提出的方法有效解决了现有方法计算成本高的问题，且新的优化器在非光滑非凸设置下有更好的理论保证。

Abstract: We study online linear optimization with matrix variables constrained by the operator norm, a setting where the geometry renders designing data-dependent and efficient adaptive algorithms challenging. The best-known adaptive regret bounds are achieved by Shampoo-like methods, but they require solving a costly quadratic projection subproblem. To address this, we extend the gradient-based prediction scheme to adaptive matrix online learning and cast algorithm design as constructing a family of smoothed potentials for the nuclear norm. We define a notion of admissibility for such smoothings and prove any admissible smoothing yields a regret bound matching the best-known guarantees of one-sided Shampoo. We instantiate this framework with two efficient methods that avoid quadratic projections. The first is an adaptive Follow-the-Perturbed-Leader (FTPL) method using Gaussian stochastic smoothing. The second is Follow-the-Augmented-Matrix-Leader (FAML), which uses a deterministic hyperbolic smoothing in an augmented matrix space. By analyzing the admissibility of these smoothings, we show both methods admit closed-form updates and match one-sided Shampoo's regret up to a constant factor, while significantly reducing computational cost. Lastly, using the online-to-nonconvex conversion, we derive two matrix-based optimizers, Pion (from FTPL) and Leon (from FAML). We prove convergence guarantees for these methods in nonsmooth nonconvex settings, a guarantee that the popular Muon optimizer lacks.

</details>


### [546] [Constructive conditional normalizing flows](https://arxiv.org/abs/2602.08606)
*Borjan Geshkovski,Domènec Ruiz-Balet*

Main category: math.OC

TL;DR: 本文考虑用感知器神经网络的连续性方程流同时逼近微分同胚φ和推进测度φ_#μ，给出基于拉格朗日插值极分解的构造，对更正则的映射给出概率构造。


<details>
  <summary>Details</summary>
Motivation: 受条件采样应用的启发，解决同时逼近φ和φ_#μ的问题。

Method: 基于φ的拉格朗日插值的极分解进行显式构造；对更正则的映射，采用受Maurey经验方法启发的概率构造。

Result: 完成对φ和φ_#μ的逼近构造，对更正则映射构造中权重的不连续点数量不随环境维度成反比缩放。

Conclusion: 提出两种构造方法实现对特定函数和测度的逼近。

Abstract: Motivated by applications in conditional sampling, given a probability measure $μ$ and a diffeomorphism $φ$, we consider the problem of simultaneously approximating $φ$ and the pushforward $φ_{\#}μ$ by means of the flow of a continuity equation whose velocity field is a perceptron neural network with piecewise constant weights. We provide an explicit construction based on a polar-like decomposition of the Lagrange interpolant of $φ$. The latter involves a compressible component, given by the gradient of a particular convex function, which can be realized exactly, and an incompressible component, which -- after approximating via permutations -- can be implemented through shear flows intrinsic to the continuity equation. For more regular maps $φ$ -- such as the Knöthe-Rosenblatt rearrangement -- we provide an alternative, probabilistic construction inspired by the Maurey empirical method, in which the number of discontinuities in the weights doesn't scale inversely with the ambient dimension.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [547] [A Machine Learning accelerated geophysical fluid solver](https://arxiv.org/abs/2602.08670)
*Yang Bai*

Main category: cs.CV

TL;DR: 探讨机器学习用于求解PDEs，实现浅水和欧拉方程经典求解器，提出四种基于ML的求解器神经网络，部分表现良好。


<details>
  <summary>Details</summary>
Motivation: 确定如何将机器学习应用于有数学约束的领域，如求解PDEs。

Method: 实现浅水方程和欧拉方程经典求解器，提出四种不同的基于深度学习的求解器神经网络。

Result: 经典求解器表现优于Pyclaw求解器，四种基于ML的求解器中有两种能输出满意解。

Conclusion: 数据驱动离散化方法在求解PDEs上有潜力，部分基于ML的求解器表现良好。

Abstract: Machine learning methods have been successful in many areas, like image classification and natural language processing. However, it still needs to be determined how to apply ML to areas with mathematical constraints, like solving PDEs. Among various approaches to applying ML techniques to solving PDEs, the data-driven discretization method presents a promising way of accelerating and improving existing PDE solver on structured grids where it predicts the coefficients of quasi-linear stencils for computing values or derivatives of a function at given positions. It can improve the accuracy and stability of low-resolution simulation compared with using traditional finite difference or finite volume schemes. Meanwhile, it can also benefit from traditional numerical schemes like achieving conservation law by adapting finite volume type formulations. In this thesis, we have implemented the shallow water equation and Euler equation classic solver under a different framework. Experiments show that our classic solver performs much better than the Pyclaw solver. Then we propose four different deep neural networks for the ML-based solver. The results indicate that two of these approaches could output satisfactory solutions.

</details>


### [548] [Neural Sentinel: Unified Vision Language Model (VLM) for License Plate Recognition with Human-in-the-Loop Continual Learning](https://arxiv.org/abs/2602.07051)
*Karthik Sivakoti*

Main category: cs.CV

TL;DR: 提出Neural Sentinel统一方法用于车牌识别等任务，用微调模型实现多任务，有HITL框架，实验证明统一视觉语言方法优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统自动车牌识别系统存在复合错误、高延迟和架构复杂的问题。

Method: 使用Vision Language Models (VLMs)，微调PaliGemma 3B模型，采用Low - Rank Adaptation (LoRA)，引入Human - in - the - Loop (HITL)持续学习框架。

Result: 车牌识别准确率达92.3%，优于EasyOCR和PaddleOCR；平均推理延迟152ms，ECE为0.048；能零样本泛化到辅助任务。

Conclusion: 统一视觉语言方法是ALPR系统的范式转变，有更高准确性、更低架构复杂度和多任务能力。

Abstract: Traditional Automatic License Plate Recognition (ALPR) systems employ multi-stage pipelines consisting of object detection networks followed by separate Optical Character Recognition (OCR) modules, introducing compounding errors, increased latency, and architectural complexity. This research presents Neural Sentinel, a novel unified approach that leverages Vision Language Models (VLMs) to perform license plate recognition, state classification, and vehicle attribute extraction through a single forward pass. Our primary contribution lies in demonstrating that a fine-tuned PaliGemma 3B model, adapted via Low-Rank Adaptation (LoRA), can simultaneously answer multiple visual questions about vehicle images, achieving 92.3% plate recognition accuracy, which is a 14.1% improvement over EasyOCR and 9.9% improvement over PaddleOCR baselines. We introduce a Human-in-the-Loop (HITL) continual learning framework that incorporates user corrections while preventing catastrophic forgetting through experience replay, maintaining a 70:30 ratio of original training data to correction samples. The system achieves a mean inference latency of 152ms with an Expected Calibration Error (ECE) of 0.048, indicating well calibrated confidence estimates. Additionally, the VLM first architecture enables zero-shot generalization to auxiliary tasks including vehicle color detection (89%), seatbelt detection (82%), and occupancy counting (78%) without task specific training. Through extensive experimentation on real world toll plaza imagery, we demonstrate that unified vision language approaches represent a paradigm shift in ALPR systems, offering superior accuracy, reduced architectural complexity, and emergent multi-task capabilities that traditional pipeline approaches cannot achieve.

</details>


### [549] [Scalable spatial point process models for forensic footwear analysis](https://arxiv.org/abs/2602.07006)
*Alokesh Manna,Neil Spencer,Dipak K. Dey*

Main category: cs.CV

TL;DR: 本文提出用分层贝叶斯模型量化鞋印意外特征模式的稀有性，通过两项改进提升法医鞋印分析的准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 犯罪现场鞋印虽能提供线索，但仅匹配鞋的型号不足，需量化意外特征模式稀有性以准确衡量法医证据强度。

Method: 开发分层贝叶斯模型，采用潜在高斯模型结合集成嵌套拉普拉斯近似，纳入空间变化系数建模鞋底花纹与意外特征位置关系。

Result: 在保留数据上表现更优。

Conclusion: 该方法提升了法医鞋印分析的准确性和可靠性。

Abstract: Shoe print evidence recovered from crime scenes plays a key role in forensic investigations. By examining shoe prints, investigators can determine details of the footwear worn by suspects. However, establishing that a suspect's shoes match the make and model of a crime scene print may not be sufficient. Typically, thousands of shoes of the same size, make, and model are manufactured, any of which could be responsible for the print. Accordingly, a popular approach used by investigators is to examine the print for signs of ``accidentals,'' i.e., cuts, scrapes, and other features that accumulate on shoe soles after purchase due to wear. While some patterns of accidentals are common on certain types of shoes, others are highly distinctive, potentially distinguishing the suspect's shoe from all others. Quantifying the rarity of a pattern is thus essential to accurately measuring the strength of forensic evidence. In this study, we address this task by developing a hierarchical Bayesian model. Our improvement over existing methods primarily stems from two advancements. First, we frame our approach in terms of a latent Gaussian model, thus enabling inference to be efficiently scaled to large collections of annotated shoe prints via integrated nested Laplace approximations. Second, we incorporate spatially varying coefficients to model the relationship between shoes' tread patterns and accidental locations. We demonstrate these improvements through superior performance on held-out data, which enhances accuracy and reliability in forensic shoe print analysis.

</details>


### [550] [Improved cystic hygroma detection from prenatal imaging using ultrasound-specific self-supervised representation learning](https://arxiv.org/abs/2512.22730)
*Youssef Megahed,Robin Ducharme,Inok Lee,Inbal Willner,Adrian D. C. Chan,Mark Walker,Steven Hawken*

Main category: cs.CV

TL;DR: 研究评估超声特定的自监督预训练能否助力于孕早期超声图像中囊性水瘤的检测，USF - MAE模型在各评估指标上优于DenseNet - 169基线。


<details>
  <summary>Details</summary>
Motivation: 囊性水瘤是高危产前超声发现，自动检测可提高可重复性并支持早期筛查项目，但有监督深度学习受小标签数据集限制，因此探讨超声特定自监督预训练用于囊性水瘤检测。

Method: 微调在超过370,000张未标记超声图像上预训练的USF - MAE模型进行二分类，用相同数据集、预处理流程和4折交叉验证方案，使用准确率等指标评估，用Score - CAM可视化分析模型可解释性。

Result: USF - MAE在所有评估指标上优于DenseNet - 169基线，USF - MAE平均准确率0.96等，Score - CAM可视化显示模型预测具有临床相关性，配对统计分析显示USF - MAE性能提升显著。

Conclusion: 超声特定的自监督预训练能促进孕早期超声图像中囊性水瘤的准确、鲁棒的深度学习检测。

Abstract: Cystic hygroma is a high-risk prenatal ultrasound finding that portends high rates of chromosomal abnormalities, structural malformations, and adverse pregnancy outcomes. Automated detection can increase reproducibility and support scalable early screening programs, but supervised deep learning methods are limited by small labelled datasets. This study assesses whether ultrasound-specific self-supervised pretraining can facilitate accurate, robust deep learning detection of cystic hygroma in first-trimester ultrasound images. We fine-tuned the Ultrasound Self-Supervised Foundation Model with Masked Autoencoding (USF-MAE), pretrained on over 370,000 unlabelled ultrasound images, for binary classification of normal controls and cystic hygroma cases used in this study. Performance was evaluated on the same curated ultrasound dataset, preprocessing pipeline, and 4-fold cross-validation protocol as for the DenseNet-169 baseline, using accuracy, sensitivity, specificity, and the area under the receiver operating characteristic curve (ROC-AUC). Model interpretability was analyzed qualitatively using Score-CAM visualizations. USF-MAE outperformed the DenseNet-169 baseline on all evaluation metrics. The proposed model yielded a mean accuracy of 0.96, sensitivity of 0.94, specificity of 0.98, and ROC-AUC of 0.98 compared to 0.93, 0.92, 0.94, and 0.94 for the DenseNet-169 baseline, respectively. Qualitative Score-CAM visualizations of model predictions demonstrated clinical relevance by highlighting expected regions in the fetal neck for both positive and negative cases. Paired statistical analysis using a Wilcoxon signed-rank test confirmed that performance improvements achieved by USF-MAE were statistically significant (p = 0.0057).

</details>


### [551] [MAU-GPT: Enhancing Multi-type Industrial Anomaly Understanding via Anomaly-aware and Generalist Experts Adaptation](https://arxiv.org/abs/2602.07011)
*Zhuonan Wang,Zhenxuan Fan,Siwen Tan,Yu Zhong,Yuqian Yuan,Haoyuan Li,Hao Jiang,Wenqiao Zhang,Feifei Shao,Hongwei Wang,Jun Xiao*

Main category: cs.CV

TL;DR: 提出MAU - Set数据集和评估协议，构建MAU - GPT模型用于工业异常理解，实验显示该模型表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有工业产品图像分析方法受数据集覆盖有限和模型泛化能力差的阻碍，需要更好的解决方案。

Method: 引入MAU - Set数据集，建立评估协议，提出MAU - GPT模型并采用AMoE - LoRA机制。

Result: MAU - GPT在各领域实验中始终优于现有方法。

Conclusion: MAU - GPT在可扩展和自动化工业检测方面有很大潜力。

Abstract: As industrial manufacturing scales, automating fine-grained product image analysis has become critical for quality control. However, existing approaches are hindered by limited dataset coverage and poor model generalization across diverse and complex anomaly patterns. To address these challenges, we introduce MAU-Set, a comprehensive dataset for Multi-type industrial Anomaly Understanding. It spans multiple industrial domains and features a hierarchical task structure, ranging from binary classification to complex reasoning. Alongside this dataset, we establish a rigorous evaluation protocol to facilitate fair and comprehensive model assessment. Building upon this foundation, we further present MAU-GPT, a domain-adapted multimodal large model specifically designed for industrial anomaly understanding. It incorporates a novel AMoE-LoRA mechanism that unifies anomaly-aware and generalist experts adaptation, enhancing both understanding and reasoning across diverse defect classes. Extensive experiments show that MAU-GPT consistently outperforms prior state-of-the-art methods across all domains, demonstrating strong potential for scalable and automated industrial inspection.

</details>


### [552] [A General Model for Retinal Segmentation and Quantification](https://arxiv.org/abs/2602.07012)
*Zhonghua Wang,Lie Ju,Sijia Li,Wei Feng,Sijin Zhou,Ming Hu,Jianhao Xiong,Xiaoying Tang,Yifan Peng,Mingquan Lin,Yaodong Ding,Yong Zeng,Wenbin Wei,Li Dong,Zongyuan Ge*

Main category: cs.CV

TL;DR: 提出针对眼底成像的RetSAM框架，可转换图像为量化表型，助力眼科研究。


<details>
  <summary>Details</summary>
Motivation: 因公开多标签数据集有限和缺乏统一分割量化流程，大规模分析视网膜定量表型与疾病关系存在困难。

Method: 采用多阶段策略，使用公私眼底数据训练RetSAM框架，支持多任务类别和多种结构、病变类型分割，并提取标准化生物标志物。

Result: RetSAM在17个公开数据集上表现出色，DSC平均提升3.9个百分点，多任务基准最高提升15个百分点，能跨人群、设备和临床环境泛化。

Conclusion: RetSAM将眼底图像转化为标准化、可解释的定量表型，推动大规模眼科研究与转化。

Abstract: Retinal imaging is fast, non-invasive, and widely available, offering quantifiable structural and vascular signals for ophthalmic and systemic health assessment. This accessibility creates an opportunity to study how quantitative retinal phenotypes relate to ocular and systemic diseases. However, such analyses remain difficult at scale due to the limited availability of public multi-label datasets and the lack of a unified segmentation-to-quantification pipeline. We present RetSAM, a general retinal segmentation and quantification framework for fundus imaging. It delivers robust multi-target segmentation and standardized biomarker extraction, supporting downstream ophthalmologic studies and oculomics correlation analyses. Trained on over 200,000 fundus images, RetSAM supports three task categories and segments five anatomical structures, four retinal phenotypic patterns, and more than 20 distinct lesion types. It converts these segmentation results into over 30 standardized biomarkers that capture structural morphology, vascular geometry, and degenerative changes. Trained with a multi-stage strategy using both private and public fundus data, RetSAM achieves superior segmentation performance on 17 public datasets. It improves on prior best methods by 3.9 percentage points in DSC on average, with up to 15 percentage points on challenging multi-task benchmarks, and generalizes well across diverse populations, imaging devices, and clinical settings. The resulting biomarkers enable systematic correlation analyses across major ophthalmic diseases, including diabetic retinopathy, age-related macular degeneration, glaucoma, and pathologic myopia. Together, RetSAM transforms fundus images into standardized, interpretable quantitative phenotypes, enabling large-scale ophthalmic research and translation.

</details>


### [553] [Steering to Say No: Configurable Refusal via Activation Steering in Vision Language Models](https://arxiv.org/abs/2602.07013)
*Jiaxi Yang,Shicheng Liu,Yuchen Yang,Dongwon Lee*

Main category: cs.CV

TL;DR: 现有VLM拒绝策略有缺陷，本文提出CR - VLM方法实现可配置拒绝，实验证明其有效。


<details>
  <summary>Details</summary>
Motivation: 现有VLM拒绝策略是一刀切的，无法适应不同用户需求和上下文约束，会导致拒绝不足或过度拒绝。

Method: 开发了基于激活引导的CR - VLM方法，包含通过教师强制机制提取可配置拒绝向量、引入门控机制减轻过度拒绝、设计反事实视觉增强模块使视觉表示与拒绝要求对齐三个组件。

Result: 在多个数据集和各种VLM上的综合实验表明，CR - VLM实现了有效、高效和稳健的可配置拒绝。

Conclusion: CR - VLM为实现VLM中用户自适应安全对齐提供了可扩展的途径。

Abstract: With the rapid advancement of Vision Language Models (VLMs), refusal mechanisms have become a critical component for ensuring responsible and safe model behavior. However, existing refusal strategies are largely \textit{one-size-fits-all} and fail to adapt to diverse user needs and contextual constraints, leading to either under-refusal or over-refusal. In this work, we firstly explore the challenges mentioned above and develop \textbf{C}onfigurable \textbf{R}efusal in \textbf{VLM}s (\textbf{CR-VLM}), a robust and efficient approach for {\em configurable} refusal based on activation steering. CR-VLM consists of three integrated components: (1) extracting a configurable refusal vector via a teacher-forced mechanism to amplify the refusal signal; (2) introducing a gating mechanism that mitigates over-refusal by preserving acceptance for in-scope queries; and (3) designing a counterfactual vision enhancement module that aligns visual representations with refusal requirements. Comprehensive experiments across multiple datasets and various VLMs demonstrate that CR-VLM achieves effective, efficient, and robust configurable refusals, offering a scalable path toward user-adaptive safety alignment in VLMs.

</details>


### [554] [Vectra: A New Metric, Dataset, and Model for Visual Quality Assessment in E-Commerce In-Image Machine Translation](https://arxiv.org/abs/2602.07014)
*Qingyu Wu,Yuxuan Han,Haijun Li,Zhao Xu,Jianshan Zhao,Xu Jin,Longyue Wang,Weihua Luo*

Main category: cs.CV

TL;DR: 提出首个无参考、基于MLLM的电商IIMT视觉质量评估框架Vectra，实验表明其与人类排名有最佳相关性，性能超领先MLLMs。


<details>
  <summary>Details</summary>
Motivation: 现有电商图像机器翻译视觉评估方法在处理上下文密集图像和多模态缺陷时存在解释性不足和缺乏细粒度奖励信号的问题。

Method: 引入Vectra框架，包含多维质量指标Vectra Score、多样性采样构建的Vectra Dataset和能生成定量分数与诊断推理的Vectra Model。

Result: Vectra与人类排名相关性达到了最先进水平，在评分性能上超越了GPT - 5和Gemini - 3等领先MLLMs。

Conclusion: Vectra为电商图像机器翻译视觉质量评估提供了有效解决方案，相关数据集和模型接收后将发布。

Abstract: In-Image Machine Translation (IIMT) powers cross-border e-commerce product listings; existing research focuses on machine translation evaluation, while visual rendering quality is critical for user engagement. When facing context-dense product imagery and multimodal defects, current reference-based methods (e.g., SSIM, FID) lack explainability, while model-as-judge approaches lack domain-grounded, fine-grained reward signals. To bridge this gap, we introduce Vectra, to the best of our knowledge, the first reference-free, MLLM-driven visual quality assessment framework for e-commerce IIMT. Vectra comprises three components: (1) Vectra Score, a multidimensional quality metric system that decomposes visual quality into 14 interpretable dimensions, with spatially-aware Defect Area Ratio (DAR) quantification to reduce annotation ambiguity; (2) Vectra Dataset, constructed from 1.1M real-world product images via diversity-aware sampling, comprising a 2K benchmark for system evaluation, 30K reasoning-based annotations for instruction tuning, and 3.5K expert-labeled preferences for alignment and evaluation; and (3) Vectra Model, a 4B-parameter MLLM that generates both quantitative scores and diagnostic reasoning. Experiments demonstrate that Vectra achieves state-of-the-art correlation with human rankings, and our model outperforms leading MLLMs, including GPT-5 and Gemini-3, in scoring performance. The dataset and model will be released upon acceptance.

</details>


### [555] [XAI-CLIP: ROI-Guided Perturbation Framework for Explainable Medical Image Segmentation in Multimodal Vision-Language Models](https://arxiv.org/abs/2602.07017)
*Thuraya Alzubaidi,Sana Ammar,Maryam Alsharqi,Islem Rekik,Muzammil Behzad*

Main category: cs.CV

TL;DR: 提出XAI - CLIP框架解决基于transformer的医学图像分割模型可解释性问题，实验显示其提升效率与可解释性。


<details>
  <summary>Details</summary>
Motivation: 基于transformer的医学图像分割模型可解释性有限，现有XAI技术计算成本高、解释效果不佳，影响临床应用。

Method: 提出XAI - CLIP，利用多模态视觉 - 语言模型嵌入定位临床有意义的解剖区域，引导解释过程。

Result: 在FLARE22和CHAOS数据集实验中，相比传统扰动方法，XAI - CLIP运行时间最多减少60%，骰子得分提高44.6%，基于遮挡解释的交并比增加96.7%，定性结果显示归因图更清晰、解剖学上更一致。

Conclusion: 将多模态视觉 - 语言表示融入基于扰动的XAI框架能显著增强可解释性和效率，有助于临床部署医学图像分割系统。

Abstract: Medical image segmentation is a critical component of clinical workflows, enabling accurate diagnosis, treatment planning, and disease monitoring. However, despite the superior performance of transformer-based models over convolutional architectures, their limited interpretability remains a major obstacle to clinical trust and deployment. Existing explainable artificial intelligence (XAI) techniques, including gradient-based saliency methods and perturbation-based approaches, are often computationally expensive, require numerous forward passes, and frequently produce noisy or anatomically irrelevant explanations. To address these limitations, we propose XAI-CLIP, an ROI-guided perturbation framework that leverages multimodal vision-language model embeddings to localize clinically meaningful anatomical regions and guide the explanation process. By integrating language-informed region localization with medical image segmentation and applying targeted, region-aware perturbations, the proposed method generates clearer, boundary-aware saliency maps while substantially reducing computational overhead. Experiments conducted on the FLARE22 and CHAOS datasets demonstrate that XAI-CLIP achieves up to a 60\% reduction in runtime, a 44.6\% improvement in dice score, and a 96.7\% increase in Intersection-over-Union for occlusion-based explanations compared to conventional perturbation methods. Qualitative results further confirm cleaner and more anatomically consistent attribution maps with fewer artifacts, highlighting that the incorporation of multimodal vision-language representations into perturbation-based XAI frameworks significantly enhances both interpretability and efficiency, thereby enabling transparent and clinically deployable medical image segmentation systems.

</details>


### [556] [The Geometry of Representational Failures in Vision Language Models](https://arxiv.org/abs/2602.07025)
*Daniele Savietto,Declan Campbell,André Panisson,Marco Nurisso,Giovanni Petri,Jonathan D. Cohen,Alan Perotti*

Main category: cs.CV

TL;DR: 分析开放权重VLMs表征几何，提出概念向量并验证，发现向量几何重叠与错误模式强相关。


<details>
  <summary>Details</summary>
Motivation: VLMs在多对象视觉任务中有失败表现，其内在机制不明，需深入研究。

Method: 分析开放权重VLMs（Qwen、InternVL、Gemma）表征几何，总结提取概念向量的方法，通过干预验证概念向量。

Result: 观察到概念向量几何重叠与特定错误模式强相关。

Conclusion: 可以用定量框架理解内部表征如何塑造模型行为和导致视觉失败。

Abstract: Vision-Language Models (VLMs) exhibit puzzling failures in multi-object visual tasks, such as hallucinating non-existent elements or failing to identify the most similar objects among distractions. While these errors mirror human cognitive constraints, such as the "Binding Problem", the internal mechanisms driving them in artificial systems remain poorly understood. Here, we propose a mechanistic insight by analyzing the representational geometry of open-weight VLMs (Qwen, InternVL, Gemma), comparing methodologies to distill "concept vectors" - latent directions encoding visual concepts. We validate our concept vectors via steering interventions that reliably manipulate model behavior in both simplified and naturalistic vision tasks (e.g., forcing the model to perceive a red flower as blue). We observe that the geometric overlap between these vectors strongly correlates with specific error patterns, offering a grounded quantitative framework to understand how internal representations shape model behavior and drive visual failures.

</details>


### [557] [Modality Gap-Driven Subspace Alignment Training Paradigm For Multimodal Large Language Models](https://arxiv.org/abs/2602.07026)
*Xiaomin Yu,Yi Xin,Wenjie Zhang,Chonghan Liu,Hanzhen Zhao,Xiaoxing Hu,Xinlei Yu,Ziyue Qiao,Hao Tang,Xue Yang,Xiaobin Hu,Chengwei Qin,Hui Xiong,Yu Qiao,Shuicheng Yan*

Main category: cs.CV

TL;DR: 本文解决多模态对比学习中的模态差距问题，提出Fixed - frame Modality Gap Theory，引入训练无关的ReAlign策略和可扩展的ReVision训练范式，证明未配对数据可替代昂贵的图像 - 文本对用于MLLMs扩展。


<details>
  <summary>Details</summary>
Motivation: 多模态对比学习存在模态差距问题，现有方法受各向同性假设限制，难以应用于大规模场景。

Method: 提出Fixed - frame Modality Gap Theory，引入ReAlign策略，将其集成到预训练阶段形成ReVision训练范式。

Result: ReAlign可通过三步过程将文本表示与图像表示分布对齐，ReVision能让模型在无大规模高质量图像 - 文本对情况下学习视觉表示分布。

Conclusion: 统计对齐的未配对数据可有效替代昂贵的图像 - 文本对，为MLLMs的高效扩展提供途径。

Abstract: Despite the success of multimodal contrastive learning in aligning visual and linguistic representations, a persistent geometric anomaly, the Modality Gap, remains: embeddings of distinct modalities expressing identical semantics occupy systematically offset regions. Prior approaches to bridge this gap are largely limited by oversimplified isotropic assumptions, hindering their application in large-scale scenarios. In this paper, we address these limitations by precisely characterizing the geometric shape of the modality gap and leveraging it for efficient model scaling. First, we propose the Fixed-frame Modality Gap Theory, which decomposes the modality gap within a frozen reference frame into stable biases and anisotropic residuals. Guided by this precise modeling, we introduce ReAlign, a training-free modality alignment strategy. Utilizing statistics from massive unpaired data, ReAlign aligns text representation into the image representation distribution via a three-step process comprising Anchor, Trace, and Centroid Alignment, thereby explicitly rectifying geometric misalignment. Building on ReAlign, we propose ReVision, a scalable training paradigm for Multimodal Large Language Models (MLLMs). ReVision integrates ReAlign into the pretraining stage, enabling the model to learn the distribution of visual representations from unpaired text before visual instruction tuning, without the need for large-scale, high-quality image-text pairs. Our framework demonstrates that statistically aligned unpaired data can effectively substitute for expensive image-text pairs, offering a robust path for the efficient scaling of MLLMs.

</details>


### [558] [A Comparative Study of Adversarial Robustness in CNN and CNN-ANFIS Architectures](https://arxiv.org/abs/2602.07028)
*Kaaustaaub Shankar,Bharadwaj Dogga,Kelly Cohen*

Main category: cs.CV

TL;DR: 对比标准CNN及其ANFIS增强版本在不同数据集和攻击下的表现，发现ANFIS集成对清洁准确率提升不持续，对鲁棒性影响因架构而异。


<details>
  <summary>Details</summary>
Motivation: CNN缺乏可解释性且易受对抗攻击，神经模糊混合模型DCNFIS虽提高可解释性，但鲁棒性研究不足，因此开展对比研究。

Method: 在MNIST、Fashion - MNIST、CIFAR - 10和CIFAR - 100数据集上，对标准CNN（ConvNet、VGG、ResNet18）及其ANFIS增强版本进行基于梯度（PGD）和无梯度（Square）攻击的对比实验。

Result: ANFIS集成不总能提高清洁准确率，对鲁棒性的影响因架构而异，ResNet18 - ANFIS对抗鲁棒性提升，VGG - ANFIS常不如基线。

Conclusion: 神经模糊增强可在特定架构中提升鲁棒性，但并非普遍有效。

Abstract: Convolutional Neural Networks (CNNs) achieve strong image classification performance but lack interpretability and are vulnerable to adversarial attacks. Neuro-fuzzy hybrids such as DCNFIS replace fully connected CNN classifiers with Adaptive Neuro-Fuzzy Inference Systems (ANFIS) to improve interpretability, yet their robustness remains underexplored. This work compares standard CNNs (ConvNet, VGG, ResNet18) with their ANFIS-augmented counterparts on MNIST, Fashion-MNIST, CIFAR-10, and CIFAR-100 under gradient-based (PGD) and gradient-free (Square) attacks. Results show that ANFIS integration does not consistently improve clean accuracy and has architecture-dependent effects on robustness: ResNet18-ANFIS exhibits improved adversarial robustness, while VGG-ANFIS often underperforms its baseline. These findings suggest that neuro-fuzzy augmentation can enhance robustness in specific architectures but is not universally beneficial.

</details>


### [559] [PipeMFL-240K: A Large-scale Dataset and Benchmark for Object Detection in Pipeline Magnetic Flux Leakage Imaging](https://arxiv.org/abs/2602.07044)
*Tianyi Qu,Songxiao Yang,Haolin Wang,Huadong Song,Xiaoting Guo,Wenguang Hu,Guanlin Liu,Honghe Chen,Yafei Ou*

Main category: cs.CV

TL;DR: 提出PipeMFL - 240K数据集用于管道MFL伪彩色图像复杂目标检测，实验表明现代检测器仍有提升空间，该数据集为相关研究提供基础。


<details>
  <summary>Details</summary>
Motivation: 深度学习用于MFL解释缺乏大规模公开数据集和基准，难以进行公平比较和可重复评估。

Method: 引入PipeMFL - 240K数据集，包含240,320张图像和191,530个高质量边界框注释，用最先进的目标检测器进行实验。

Result: 现代检测器在处理MFL数据固有特性方面仍有困难，有很大提升空间。

Conclusion: PipeMFL - 240K作为首个大规模公开数据集和基准，为管道诊断、维护规划提供基础，有望加速算法创新和可重复研究。

Abstract: Pipeline integrity is critical to industrial safety and environmental protection, with Magnetic Flux Leakage (MFL) detection being a primary non-destructive testing technology. Despite the promise of deep learning for automating MFL interpretation, progress toward reliable models has been constrained by the absence of a large-scale public dataset and benchmark, making fair comparison and reproducible evaluation difficult. We introduce \textbf{PipeMFL-240K}, a large-scale, meticulously annotated dataset and benchmark for complex object detection in pipeline MFL pseudo-color images. PipeMFL-240K reflects real-world inspection complexity and poses several unique challenges: (i) an extremely long-tailed distribution over \textbf{12} categories, (ii) a high prevalence of tiny objects that often comprise only a handful of pixels, and (iii) substantial intra-class variability. The dataset contains \textbf{240,320} images and \textbf{191,530} high-quality bounding-box annotations, collected from 11 pipelines spanning approximately \textbf{1,480} km. Extensive experiments are conducted with state-of-the-art object detectors to establish baselines. Results show that modern detectors still struggle with the intrinsic properties of MFL data, highlighting considerable headroom for improvement, while PipeMFL-240K provides a reliable and challenging testbed to drive future research. As the first public dataset and the first benchmark of this scale and scope for pipeline MFL inspection, it provides a critical foundation for efficient pipeline diagnostics as well as maintenance planning and is expected to accelerate algorithmic innovation and reproducible research in MFL-based pipeline integrity assessment.

</details>


### [560] [VLRS-Bench: A Vision-Language Reasoning Benchmark for Remote Sensing](https://arxiv.org/abs/2602.07045)
*Zhiming Luo,Di Wang,Haonan Guo,Jing Zhang,Bo Du*

Main category: cs.CV

TL;DR: 提出首个用于复杂遥感推理的VLRS - Bench基准，实验揭示现有MLLMs瓶颈。


<details>
  <summary>Details</summary>
Motivation: 现有遥感基准偏向感知任务，阻碍MLLMs在认知要求高的遥感应用中的发展。

Method: 提出Vision Language ReaSoning Benchmark (VLRS - Bench)，跨认知、决策和预测三个核心维度构建，通过整合遥感特定先验和专家知识的专门管道构建。

Result: 实验揭示了现有最先进MLLMs存在显著瓶颈。

Conclusion: VLRS - Bench为遥感领域推进多模态推理提供关键见解。

Abstract: Recent advancements in Multimodal Large Language Models (MLLMs) have enabled complex reasoning. However, existing remote sensing (RS) benchmarks remain heavily biased toward perception tasks, such as object recognition and scene classification. This limitation hinders the development of MLLMs for cognitively demanding RS applications. To address this, , we propose a Vision Language ReaSoning Benchmark (VLRS-Bench), which is the first benchmark exclusively dedicated to complex RS reasoning. Structured across the three core dimensions of Cognition, Decision, and Prediction, VLRS-Bench comprises 2,000 question-answer pairs with an average length of 71 words, spanning 14 tasks and up to eight temporal phases. VLRS-Bench is constructed via a specialized pipeline that integrates RS-specific priors and expert knowledge to ensure geospatial realism and reasoning complexity. Experimental results reveal significant bottlenecks in existing state-of-the-art MLLMs, providing critical insights for advancing multimodal reasoning within the remote sensing community.

</details>


### [561] [Interpreting Physics in Video World Models](https://arxiv.org/abs/2602.07050)
*Sonia Joseph,Quentin Garrido,Randall Balestriero,Matthew Kowal,Thomas Fel,Shahab Bakhtiari,Blake Richards,Mike Rabbat*

Main category: cs.CV

TL;DR: 本文对大规模视频编码器内的物理表征进行可解释性研究，发现视频模型使用分布式表征进行物理预测。


<details>
  <summary>Details</summary>
Motivation: 探究基于视频的模型进行物理准确预测时，是依赖物理变量的分解表征，还是以特定任务的分布式方式隐式表征，以及现代视频世界模型的内部表征机制。

Method: 使用逐层探测、子空间几何、补丁级解码和有针对性的注意力消融等方法，研究基于编码器的视频Transformer中物理信息的可访问位置和组织方式。

Result: 发现了“物理涌现区”，物理变量在此变得可访问；速度和加速度等标量早期可用，运动方向在“物理涌现区”才可用；方向通过具有圆形几何的高维群体结构编码。

Conclusion: 现代视频模型不使用经典物理引擎那样的物理变量分解表征，而是使用分布式表征进行物理预测。

Abstract: A long-standing question in physical reasoning is whether video-based models need to rely on factorized representations of physical variables in order to make physically accurate predictions, or whether they can implicitly represent such variables in a task-specific, distributed manner. While modern video world models achieve strong performance on intuitive physics benchmarks, it remains unclear which of these representational regimes they implement internally. Here, we present the first interpretability study to directly examine physical representations inside large-scale video encoders. Using layerwise probing, subspace geometry, patch-level decoding, and targeted attention ablations, we characterize where physical information becomes accessible and how it is organized within encoder-based video transformers.
  Across architectures, we identify a sharp intermediate-depth transition -- which we call the Physics Emergence Zone -- at which physical variables become accessible. Physics-related representations peak shortly after this transition and degrade toward the output layers. Decomposing motion into explicit variables, we find that scalar quantities such as speed and acceleration are available from early layers onwards, whereas motion direction becomes accessible only at the Physics Emergence Zone. Notably, we find that direction is encoded through a high-dimensional population structure with circular geometry, requiring coordinated multi-feature intervention to control. These findings suggest that modern video models do not use factorized representations of physical variables like a classical physics engine. Instead, they use a distributed representation that is nonetheless sufficient for making physical predictions.

</details>


### [562] [FADE: Selective Forgetting via Sparse LoRA and Self-Distillation](https://arxiv.org/abs/2602.07058)
*Carolina R. Kelsch,Leonardo S. B. Pereira,Natnael Mola,Luis H. Arribas,Juan C. S. M. Avedillo*

Main category: cs.CV

TL;DR: 介绍FADE方法用于图像生成模型的机器学习遗忘，评估显示其在多数据集上有良好表现。


<details>
  <summary>Details</summary>
Motivation: 文本到图像扩散模型的机器学习遗忘面临高计算成本和平衡遗忘与保留无关概念的难题，需满足数据保护法规和负责任AI实践。

Method: 提出FADE两阶段方法，结合参数定位与自蒸馏，先通过梯度显著性确定遗忘集参数，用稀疏LoRA适配器约束更新，再用自蒸馏目标覆盖遗忘概念。

Result: 在多个数据集评估显示，FADE在遗忘-保留权衡上有细粒度控制，实现强概念擦除和高保留性。

Conclusion: FADE适合基于扩散的图像生成模型的选择性遗忘。

Abstract: Machine Unlearning aims to remove the influence of specific data or concepts from trained models while preserving overall performance, a capability increasingly required by data protection regulations and responsible AI practices. Despite recent progress, unlearning in text-to-image diffusion models remains challenging due to high computational costs and the difficulty of balancing effective forgetting with retention of unrelated concepts. We introduce FADE (Fast Adapter for Data Erasure), a two-stage unlearning method for image generation that combines parameter localization with self-distillation. FADE first identifies parameters most responsible for the forget set using gradient-based saliency and constrains updates through sparse LoRA adapters, ensuring lightweight, localized modifications. In a second stage, FADE applies a self-distillation objective that overwrites the forgotten concept with a user-defined surrogate while preserving behavior on retained data. The resulting adapters are memory-efficient, reversible, and can be merged or removed at runtime, enabling flexible deployment in production systems. We evaluated FADE on the UnlearnCanvas benchmark and conducted ablation studies on Imagenette, Labeled Faces in the Wild, AtharvaTaras Dog Breeds Dataset, and SUN Attributes datasets, demonstrating State-of-the-Art unlearning performance with fine-grained control over the forgetting-retention trade-off. Our results demonstrate that FADE achieves strong concept erasure and high retainability across various domains, making it a suitable solution for selective unlearning in diffusion-based image generation models.

</details>


### [563] [Bidirectional Reward-Guided Diffusion for Real-World Image Super-Resolution](https://arxiv.org/abs/2602.07069)
*Zihao Fan,Xin Lu,Yidi Liu,Jie Huang,Dong Li,Xueyang Fu,Zheng-Jun Zha*

Main category: cs.CV

TL;DR: 提出Bird - SR双向奖励引导扩散框架用于超分辨率，结合合成和真实图像，在真实世界超分基准上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 基于扩散的超分辨率在合成数据上训练的模型因分布偏移在真实低分辨率图像上效果不佳。

Method: 提出Bird - SR框架，通过奖励反馈学习将超分辨率表述为轨迹级偏好优化，在早期扩散步骤优化合成对以保结构，后期采样步骤用质量引导奖励，缓解奖励作弊问题并采用动态保真 - 感知加权策略。

Result: 在真实世界超分辨率基准上的大量实验表明，Bird - SR在保持结构一致性的同时，感知质量上始终优于现有方法。

Conclusion: Bird - SR对真实世界超分辨率有效。

Abstract: Diffusion-based super-resolution can synthesize rich details, but models trained on synthetic paired data often fail on real-world LR images due to distribution shifts. We propose Bird-SR, a bidirectional reward-guided diffusion framework that formulates super-resolution as trajectory-level preference optimization via reward feedback learning (ReFL), jointly leveraging synthetic LR-HR pairs and real-world LR images. For structural fidelity easily affected in ReFL, the model is directly optimized on synthetic pairs at early diffusion steps, which also facilitates structure preservation for real-world inputs under smaller distribution gap in structure levels. For perceptual enhancement, quality-guided rewards are applied at later sampling steps to both synthetic and real LR images. To mitigate reward hacking, the rewards for synthetic results are formulated in a relative advantage space bounded by their clean counterparts, while real-world optimization is regularized via a semantic alignment constraint. Furthermore, to balance structural and perceptual learning, we adopt a dynamic fidelity-perception weighting strategy that emphasizes structure preservation at early stages and progressively shifts focus toward perceptual optimization at later diffusion steps. Extensive experiments on real-world SR benchmarks demonstrate that Bird-SR consistently outperforms state-of-the-art methods in perceptual quality while preserving structural consistency, validating its effectiveness for real-world super-resolution.

</details>


### [564] [MosaicThinker: On-Device Visual Spatial Reasoning for Embodied AI via Iterative Construction of Space Representation](https://arxiv.org/abs/2602.07082)
*Haoming Wang,Qiyao Xue,Weichen Liu,Wei Gao*

Main category: cs.CV

TL;DR: 提出MosaicThinker技术增强设备端小VLM跨帧空间推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型（VLMs）在空间推理方面能力薄弱，缺乏3D空间信息，难以处理多视频帧复杂空间关系推理任务。

Method: 将多帧碎片化空间信息整合到全局语义地图统一空间表示中，通过视觉提示引导VLM在语义地图上进行空间推理。

Result: 该技术能大幅提高资源受限的具身AI设备在不同类型和复杂度的跨帧空间推理任务上的准确性。

Conclusion: MosaicThinker技术有效提升了设备端小VLM在困难跨帧推理任务上的空间推理能力。

Abstract: When embodied AI is expanding from traditional object detection and recognition to more advanced tasks of robot manipulation and actuation planning, visual spatial reasoning from the video inputs is necessary to perceive the spatial relationships of objects and guide device actions. However, existing visual language models (VLMs) have very weak capabilities in spatial reasoning due to the lack of knowledge about 3D spatial information, especially when the reasoning task involve complex spatial relations across multiple video frames. In this paper, we present a new inference-time computing technique for on-device embodied AI, namely \emph{MosaicThinker}, which enhances the on-device small VLM's spatial reasoning capabilities on difficult cross-frame reasoning tasks. Our basic idea is to integrate fragmented spatial information from multiple frames into a unified space representation of global semantic map, and further guide the VLM's spatial reasoning over the semantic map via a visual prompt. Experiment results show that our technique can greatly enhance the accuracy of cross-frame spatial reasoning on resource-constrained embodied AI devices, over reasoning tasks with diverse types and complexities.

</details>


### [565] [WorldEdit: Towards Open-World Image Editing with a Knowledge-Informed Benchmark](https://arxiv.org/abs/2602.07095)
*Wang Lin,Feng Wang,Majun Zhang,Wentao Hu,Tao Jin,Zhou Zhao,Fei Wu,Jingyuan Chen,Alan Yuille,Sucheng Ren*

Main category: cs.CV

TL;DR: 提出WorldEdit数据集和WorldEdit-Test，用两阶段训练框架微调模型，缩小与GPT-4o和Nano - Banana的差距。


<details>
  <summary>Details</summary>
Motivation: 现有图像编辑模型在处理隐式编辑指令时存在局限，因其依赖统一编辑策略，缺乏处理复杂世界知识和推理的能力。

Method: 引入WorldEdit数据集及WorldEdit-Test，使用两阶段训练框架对Bagel等模型微调，并集成因果验证奖励。

Result: 提出的数据集和方法显著缩小与GPT-4o和Nano - Banana的差距，在指令遵循和知识合理性方面表现有竞争力。

Conclusion: WorldEdit数据集和相关方法能有效提升图像编辑模型处理隐式编辑指令的能力。

Abstract: Recent advances in image editing models have demonstrated remarkable capabilities in executing explicit instructions, such as attribute manipulation, style transfer, and pose synthesis. However, these models often face challenges when dealing with implicit editing instructions, which describe the cause of a visual change without explicitly detailing the resulting outcome. These limitations arise because existing models rely on uniform editing strategies that are not equipped to handle the complex world knowledge and reasoning required for implicit instructions. To address this gap, we introduce \textbf{WorldEdit}, a dataset specifically designed to enable world-driven image editing. WorldEdit consists of high-quality editing samples, guided by paraphrased instructions that align with real-world causal logic. Furthermore, we provide \textbf{WorldEdit-Test} for evaluating the existing model's performance on causal editing scenarios. With WorldEdit, we use a two-stage training framework for fine-tuning models like Bagel, integrating with a causal verification reward. Our results show that the proposed dataset and methods significantly narrow the gap with GPT-4o and Nano-Banana, demonstrating competitive performance not only in instruction following but also in knowledge plausibility, where many open-source systems typically struggle.

</details>


### [566] [Extended to Reality: Prompt Injection in 3D Environments](https://arxiv.org/abs/2602.07104)
*Zhuoheng Li,Ying Chen*

Main category: cs.CV

TL;DR: 本文提出针对3D环境中多模态大语言模型的提示注入攻击PI3D，实验表明其有效且现有防御不足。


<details>
  <summary>Details</summary>
Motivation: 此前研究未明确提示注入攻击在3D物理环境中的作用机制，本文旨在填补这一空白。

Method: 通过放置带有文本的物理对象而非数字图像编辑实现PI3D攻击，解决识别有效3D对象姿态的问题。

Result: PI3D对多种多模态大语言模型在不同相机轨迹下攻击有效，现有防御不足以抵御PI3D。

Conclusion: PI3D是3D环境中针对多模态大语言模型的有效攻击，现有防御手段需改进。

Abstract: Multimodal large language models (MLLMs) have advanced the capabilities to interpret and act on visual input in 3D environments, empowering diverse applications such as robotics and situated conversational agents. When MLLMs reason over camera-captured views of the physical world, a new attack surface emerges: an attacker can place text-bearing physical objects in the environment to override MLLMs' intended task. While prior work has studied prompt injection in the text domain and through digitally edited 2D images, it remains unclear how these attacks function in 3D physical environments. To bridge the gap, we introduce PI3D, a prompt injection attack against MLLMs in 3D environments, realized through text-bearing physical object placement rather than digital image edits. We formulate and solve the problem of identifying an effective 3D object pose (position and orientation) with injected text, where the attacker's goal is to induce the MLLM to perform the injected task while ensuring that the object placement remains physically plausible. Experiments demonstrate that PI3D is an effective attack against multiple MLLMs under diverse camera trajectories. We further evaluate existing defenses and show that they are insufficient to defend against PI3D.

</details>


### [567] [Ex-Omni: Enabling 3D Facial Animation Generation for Omni-modal Large Language Models](https://arxiv.org/abs/2602.07106)
*Haoyu Zhang,Zhipeng Li,Yiwen Guo,Tianshu Yu*

Main category: cs.CV

TL;DR: 提出开源框架Ex - Omni为OLLMs增加语音伴随3D面部动画功能，还引入数据集InstructEx，实验显示其性能良好。


<details>
  <summary>Details</summary>
Motivation: 现有OLLMs在融合语音和3D面部动画方面研究不足，且存在表示不匹配的挑战。

Method: 提出Ex - Omni框架，通过解耦语义推理和时间生成降低学习难度，利用语音单元作为时间支架和TQGF机制进行语义注入，引入数据集InstructEx。

Result: Ex - Omni与现有开源OLLMs相比表现有竞争力，能实现稳定的语音和面部动画对齐生成。

Conclusion: Ex - Omni框架有效解决了OLLMs融合语音和3D面部动画的问题。

Abstract: Omni-modal large language models (OLLMs) aim to unify multimodal understanding and generation, yet incorporating speech with 3D facial animation remains largely unexplored despite its importance for natural interaction. A key challenge arises from the representation mismatch between discrete, token-level semantic reasoning in LLMs and the dense, fine-grained temporal dynamics required for 3D facial motion, which makes direct modeling difficult to optimize under limited data. We propose Expressive Omni (Ex-Omni), an open-source omni-modal framework that augments OLLMs with speech-accompanied 3D facial animation. Ex-Omni reduces learning difficulty by decoupling semantic reasoning from temporal generation, leveraging speech units as temporal scaffolding and a unified token-as-query gated fusion (TQGF) mechanism for controlled semantic injection. We further introduce InstructEx, a dataset aims to facilitate augment OLLMs with speech-accompanied 3D facial animation. Extensive experiments demonstrate that Ex-Omni performs competitively against existing open-source OLLMs while enabling stable aligned speech and facial animation generation.

</details>


### [568] [The Double-Edged Sword of Data-Driven Super-Resolution: Adversarial Super-Resolution Models](https://arxiv.org/abs/2602.07251)
*Haley Duba-Sullivan,Steven R. Young,Emma J. Reid*

Main category: cs.CV

TL;DR: 提出 AdvSR 框架，可在训练时将对抗行为嵌入 SR 模型权重，在三个 SR 架构上评估显示能以最小质量下降实现高攻击成功率，揭示成像管道新威胁。


<details>
  <summary>Details</summary>
Motivation: 数据驱动的超分辨率（SR）方法引入成像管道后带来未探索的攻击面，需研究新的攻击方式。

Method: 提出 AdvSR 框架，在训练时将对抗行为嵌入 SR 模型权重，联合优化重建质量和目标对抗结果。

Result: 在三个 SR 架构（SRCNN、EDSR、SwinIR）与 YOLOv11 分类器配对评估，AdvSR 模型可实现高攻击成功率且质量下降最小。

Conclusion: 揭示了成像管道新的模型级威胁，对安全关键应用中从业者获取和验证模型有启示。

Abstract: Data-driven super-resolution (SR) methods are often integrated into imaging pipelines as preprocessing steps to improve downstream tasks such as classification and detection. However, these SR models introduce a previously unexplored attack surface into imaging pipelines. In this paper, we present AdvSR, a framework demonstrating that adversarial behavior can be embedded directly into SR model weights during training, requiring no access to inputs at inference time. Unlike prior attacks that perturb inputs or rely on backdoor triggers, AdvSR operates entirely at the model level. By jointly optimizing for reconstruction quality and targeted adversarial outcomes, AdvSR produces models that appear benign under standard image quality metrics while inducing downstream misclassification. We evaluate AdvSR on three SR architectures (SRCNN, EDSR, SwinIR) paired with a YOLOv11 classifier and demonstrate that AdvSR models can achieve high attack success rates with minimal quality degradation. These findings highlight a new model-level threat for imaging pipelines, with implications for how practitioners source and validate models in safety-critical applications.

</details>


### [569] [LUCID-SAE: Learning Unified Vision-Language Sparse Codes for Interpretable Concept Discovery](https://arxiv.org/abs/2602.07311)
*Difei Gu,Yunhe Gao,Gerasimos Chatzoudis,Zihan Dong,Guoning Zhang,Bangwei Guo,Yang Zhou,Mu Zhou,Dimitris Metaxas*

Main category: cs.CV

TL;DR: 提出统一视觉 - 语言稀疏自动编码器LUCID，学习共享潜在字典，实现特征对齐，有可解释共享特征及自动化字典解释流程。


<details>
  <summary>Details</summary>
Motivation: 当前稀疏自动编码器按模态训练，特征不可直接理解，解释无法跨领域迁移。

Method: 引入LUCID学习共享潜在字典，通过耦合共享代码与最优传输匹配目标实现特征对齐，开发基于术语聚类的自动化字典解释流程。

Result: 得到可解释共享特征，支持补丁级定位，建立跨模态神经元对应，增强基于相似性评估中概念聚类问题的鲁棒性。

Conclusion: LUCID的共享特征能捕捉对象之外的多种语义类别，是可解释多模态表示的全面方法。

Abstract: Sparse autoencoders (SAEs) offer a natural path toward comparable explanations across different representation spaces. However, current SAEs are trained per modality, producing dictionaries whose features are not directly understandable and whose explanations do not transfer across domains. In this study, we introduce LUCID (Learning Unified vision-language sparse Codes for Interpretable concept Discovery), a unified vision-language sparse autoencoder that learns a shared latent dictionary for image patch and text token representations, while reserving private capacity for modality-specific details. We achieve feature alignment by coupling the shared codes with a learned optimal transport matching objective without the need of labeling. LUCID yields interpretable shared features that support patch-level grounding, establish cross-modal neuron correspondence, and enhance robustness against the concept clustering problem in similarity-based evaluation. Leveraging the alignment properties, we develop an automated dictionary interpretation pipeline based on term clustering without manual observations. Our analysis reveals that LUCID's shared features capture diverse semantic categories beyond objects, including actions, attributes, and abstract concepts, demonstrating a comprehensive approach to interpretable multimodal representations.

</details>


### [570] [Seeing Roads Through Words: A Language-Guided Framework for RGB-T Driving Scene Segmentation](https://arxiv.org/abs/2602.07343)
*Ruturaj Reddy,Hrishav Bakul Barua,Junn Yong Loo,Thanh Thi Nguyen,Ganesh Krishnasamy*

Main category: cs.CV

TL;DR: 提出CLARITY方法用于恶劣光照等条件下的道路场景语义分割，在MFNet数据集上达新SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有RGB - Thermal融合方法采用静态融合策略，会使特定模态噪声在网络中传播，难以应对恶劣光照等条件下的道路场景语义分割。

Method: 提出CLARITY方法，根据检测到的场景条件动态调整融合策略，借助视觉语言模型先验，学习根据光照状态调整各模态贡献；引入保留有效暗物体语义机制和分层解码器。

Result: 在MFNet数据集上，CLARITY实现了62.3%的mIoU和77.5%的mAcc，达到新的SOTA。

Conclusion: CLARITY方法在恶劣光照等条件下的道路场景语义分割上有显著效果，优于现有方法。

Abstract: Robust semantic segmentation of road scenes under adverse illumination, lighting, and shadow conditions remain a core challenge for autonomous driving applications. RGB-Thermal fusion is a standard approach, yet existing methods apply static fusion strategies uniformly across all conditions, allowing modality-specific noise to propagate throughout the network. Hence, we propose CLARITY that dynamically adapts its fusion strategy to the detected scene condition. Guided by vision-language model (VLM) priors, the network learns to modulate each modality's contribution based on the illumination state while leveraging object embeddings for segmentation, rather than applying a fixed fusion policy. We further introduce two mechanisms, i.e., one which preserves valid dark-object semantics that prior noise-suppression methods incorrectly discard, and a hierarchical decoder that enforces structural consistency across scales to sharpen boundaries on thin objects. Experiments on the MFNet dataset demonstrate that CLARITY establishes a new state-of-the-art (SOTA), achieving 62.3% mIoU and 77.5% mAcc.

</details>


### [571] [Fine-Grained Cat Breed Recognition with Global Context Vision Transformer](https://arxiv.org/abs/2602.07534)
*Mowmita Parvin Hera,Md. Shahriar Mahmud Kallol,Shohanur Rahman Nirob,Md. Badsha Bulbul,Jubayer Ahmed,M. Zhourul Islam,Hazrat Ali,Mohammmad Farhad Bulbul*

Main category: cs.CV

TL;DR: 提出基于深度学习的猫品种分类方法，用GCViT-Tiny模型，经数据增强后测试准确率92%，验证准确率94.54%，证明Transformer架构有效性。


<details>
  <summary>Details</summary>
Motivation: 准确从图像识别猫品种因特征差异细微而具挑战性，需有效分类方法。

Method: 使用Oxford - IIIT Pet Dataset子集，采用GCViT - Tiny架构，用旋转、翻转、亮度调整等数据增强提高模型泛化能力。

Result: GCViT - Tiny模型测试准确率92.00%，验证准确率94.54%。

Conclusion: Transformer架构对细粒度图像分类任务有效，有兽医诊断等应用潜力，还提供hugging face演示。

Abstract: Accurate identification of cat breeds from images is a challenging task due to subtle differences in fur patterns, facial structure, and color. In this paper, we present a deep learning-based approach for classifying cat breeds using a subset of the Oxford-IIIT Pet Dataset, which contains high-resolution images of various domestic breeds. We employed the Global Context Vision Transformer (GCViT) architecture-tiny for cat breed recognition. To improve model generalization, we used extensive data augmentation, including rotation, horizontal flipping, and brightness adjustment. Experimental results show that the GCViT-Tiny model achieved a test accuracy of 92.00% and validation accuracy of 94.54%. These findings highlight the effectiveness of transformer-based architectures for fine-grained image classification tasks. Potential applications include veterinary diagnostics, animal shelter management, and mobile-based breed recognition systems. We also provide a hugging face demo at https://huggingface.co/spaces/bfarhad/cat-breed-classifier.

</details>


### [572] [Beyond Core and Penumbra: Bi-Temporal Image-Driven Stroke Evolution Analysis](https://arxiv.org/abs/2602.07535)
*Md Sazidur Rahman,Kjersti Engan,Kathinka Dæhli Kurz,Mahdieh Khanmohammadi*

Main category: cs.CV

TL;DR: 提出双时间分析框架，用多种特征表征缺血组织，对18例成功再灌注患者评估，发现特征空间能反映组织表型和状态转变。


<details>
  <summary>Details</summary>
Motivation: 现有单时间点分割无法捕捉中风的生物异质性和时间演变，需新方法。

Method: 提出双时间分析框架，从CTP提取特征，结合手动勾勒掩码构建ROI，分析特征空间。

Result: 区域级表示有意义聚类，不同组织区域特征有差异，mJ - Net在可挽救和不可挽救组织间分离性强。

Conclusion: 编码器衍生的特征流形能反映组织表型和状态转变，有助于基于影像量化中风演变。

Abstract: Computed tomography perfusion (CTP) at admission is routinely used to estimate the ischemic core and penumbra, while follow-up diffusion-weighted MRI (DWI) provides the definitive infarct outcome. However, single time-point segmentations fail to capture the biological heterogeneity and temporal evolution of stroke. We propose a bi-temporal analysis framework that characterizes ischemic tissue using statistical descriptors, radiomic texture features, and deep feature embeddings from two architectures (mJ-Net and nnU-Net). Bi-temporal refers to admission (T1) and post-treatment follow-up (T2). All features are extracted at T1 from CTP, with follow-up DWI aligned to ensure spatial correspondence. Manually delineated masks at T1 and T2 are intersected to construct six regions of interest (ROIs) encoding both initial tissue state and final outcome. Features were aggregated per region and analyzed in feature space. Evaluation on 18 patients with successful reperfusion demonstrated meaningful clustering of region-level representations. Regions classified as penumbra or healthy at T1 that ultimately recovered exhibited feature similarity to preserved brain tissue, whereas infarct-bound regions formed distinct groupings. Both baseline GLCM and deep embeddings showed a similar trend: penumbra regions exhibit features that are significantly different depending on final state, whereas this difference is not significant for core regions. Deep feature spaces, particularly mJ-Net, showed strong separation between salvageable and non-salvageable tissue, with a penumbra separation index that differed significantly from zero (Wilcoxon signed-rank test). These findings suggest that encoder-derived feature manifolds reflect underlying tissue phenotypes and state transitions, providing insight into imaging-based quantification of stroke evolution.

</details>


### [573] [Revealing the Semantic Selection Gap in DINOv3 through Training-Free Few-Shot Segmentation](https://arxiv.org/abs/2602.07550)
*Hussni Mohd Zakir,Eric Tatt Wei Ho*

Main category: cs.CV

TL;DR: 研究通过FSSDINO探究冻结DINOv3特性的少样本语义分割能力，结果有竞争力，并揭示了语义选择差距。


<details>
  <summary>Details</summary>
Motivation: 探究冻结DINOv3特征的内在少样本语义分割能力。

Method: 采用训练无关的基线FSSDINO，利用特定类原型和Gram矩阵优化，还进行Oracle引导的层分析。

Result: 该最小化方法在多基准测试中具竞争力，发现标准最后层特征与全局最优中间表示有性能差距。

Conclusion: 确立了“最后一层”作为强大基线，对DINOv3潜在语义潜力进行了严格诊断。

Abstract: Recent self-supervised Vision Transformers (ViTs), such as DINOv3, provide rich feature representations for dense vision tasks. This study investigates the intrinsic few-shot semantic segmentation (FSS) capabilities of frozen DINOv3 features through a training-free baseline, FSSDINO, utilizing class-specific prototypes and Gram-matrix refinement. Our results across binary, multi-class, and cross-domain (CDFSS) benchmarks demonstrate that this minimal approach, applied to the final backbone layer, is highly competitive with specialized methods involving complex decoders or test-time adaptation. Crucially, we conduct an Oracle-guided layer analysis, identifying a significant performance gap between the standard last-layer features and globally optimal intermediate representations. We reveal a "Safest vs. Optimal" dilemma: while the Oracle proves higher performance is attainable, matching the results of compute-intensive adaptation methods, current unsupervised and support-guided selection metrics consistently yield lower performance than the last-layer baseline. This characterizes a "Semantic Selection Gap" in Foundation Models, a disconnect where traditional heuristics fail to reliably identify high-fidelity features. Our work establishes the "Last-Layer" as a deceptively strong baseline and provides a rigorous diagnostic of the latent semantic potentials in DINOv3.The code is publicly available at https://github.com/hussni0997/fssdino.

</details>


### [574] [VISOR: VIsual Spatial Object Reasoning for Language-driven Object Navigation](https://arxiv.org/abs/2602.07555)
*Francesco Taioli,Shiping Yang,Sonia Raychaudhuri,Marco Cristani,Unnat Jain,Angel X Chang*

Main category: cs.CV

TL;DR: 提出3B参数的VLA智能体用于语言驱动的目标导航，通过显式推理提高可解释性、泛化性和导航效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在泛化能力差、缺乏可解释性、误差传播、计算成本高和难以集成推理到导航策略等问题。

Method: 提出3B参数的VLA智能体，采用显式的基于图像的推理，推理过程分为“思考”“思考总结”和“行动”三个阶段。

Result: 该智能体在目标识别和动作选择上能进行类人具身推理，无需多模型拼接管道。

Conclusion: 所提方法可提高可解释性、泛化性和导航效率。

Abstract: Language-driven object navigation requires agents to interpret natural language descriptions of target objects, which combine intrinsic and extrinsic attributes for instance recognition and commonsense navigation. Existing methods either (i) use end-to-end trained models with vision-language embeddings, which struggle to generalize beyond training data and lack action-level explainability, or (ii) rely on modular zero-shot pipelines with large language models (LLMs) and open-set object detectors, which suffer from error propagation, high computational cost, and difficulty integrating their reasoning back into the navigation policy. To this end, we propose a compact 3B-parameter Vision-Language-Action (VLA) agent that performs human-like embodied reasoning for both object recognition and action selection, removing the need for stitched multi-model pipelines. Instead of raw embedding matching, our agent employs explicit image-grounded reasoning to directly answer "Is this the target object?" and "Why should I take this action?" The reasoning process unfolds in three stages: "think", "think summary", and "action", yielding improved explainability, stronger generalization, and more efficient navigation. Code and dataset available upon acceptance.

</details>


### [575] [Cross-Camera Cow Identification via Disentangled Representation Learning](https://arxiv.org/abs/2602.07566)
*Runcheng Wang,Yaru Chen,Guiguo Zhang,Honghua Jiang,Yongliang Qiao*

Main category: cs.CV

TL;DR: 研究提出基于解纠缠表示学习的跨摄像头奶牛识别框架，提升了跨摄像头泛化能力，实验表现优异，为智慧养殖提供新范式。


<details>
  <summary>Details</summary>
Motivation: 现有动物识别方法在跨摄像头泛化方面存在问题，限制了非接触技术在真实养殖环境中的大规模应用。

Method: 采用子空间可识别性保证（SIG）理论，设计原理驱动的特征解纠缠模块，将图像分解为多个正交潜在子空间。

Result: 构建涵盖五个不同摄像头节点的高质量数据集，在七个跨摄像头任务中平均准确率达86.0%，显著优于其他基准方法。

Conclusion: 建立了用于协作跨摄像头奶牛识别的子空间理论特征解纠缠框架，为非受控智慧养殖环境中的精确动物监测提供新范式。

Abstract: Precise identification of individual cows is a fundamental prerequisite for comprehensive digital management in smart livestock farming. While existing animal identification methods excel in controlled, single-camera settings, they face severe challenges regarding cross-camera generalization. When models trained on source cameras are deployed to new monitoring nodes characterized by divergent illumination, backgrounds, viewpoints, and heterogeneous imaging properties, recognition performance often degrades dramatically. This limits the large-scale application of non-contact technologies in dynamic, real-world farming environments. To address this challenge, this study proposes a cross-camera cow identification framework based on disentangled representation learning. This framework leverages the Subspace Identifiability Guarantee (SIG) theory in the context of bovine visual recognition. By modeling the underlying physical data generation process, we designed a principle-driven feature disentanglement module that decomposes observed images into multiple orthogonal latent subspaces. This mechanism effectively isolates stable, identity-related biometric features that remain invariant across cameras, thereby substantially improving generalization to unseen cameras. We constructed a high-quality dataset spanning five distinct camera nodes, covering heterogeneous acquisition devices and complex variations in lighting and angles. Extensive experiments across seven cross-camera tasks demonstrate that the proposed method achieves an average accuracy of 86.0%, significantly outperforming the Source-only Baseline (51.9%) and the strongest cross-camera baseline method (79.8%). This work establishes a subspace-theoretic feature disentanglement framework for collaborative cross-camera cow identification, offering a new paradigm for precise animal monitoring in uncontrolled smart farming environments.

</details>


### [576] [Automated rock joint trace mapping using a supervised learning model trained on synthetic data generated by parametric modelling](https://arxiv.org/abs/2602.07590)
*Jessica Ka Yi Chiu,Tom Frode Hansen,Eivind Magnus Paulsen,Ole Jakob Mengshoel*

Main category: cs.CV

TL;DR: 提出地质驱动机器学习方法用于从图像自动绘制岩石节理轨迹，结合地质建模、合成数据生成和监督图像分割，测试表明合成数据在真实数据稀缺时有用，方法支持可靠节理绘制。


<details>
  <summary>Details</summary>
Motivation: 解决真实数据有限和类别不平衡问题，实现自动化岩石节理轨迹映射。

Method: 结合地质建模、合成数据生成和监督图像分割；用离散裂缝网络模型生成合成节理岩石图像，通过混合训练、预训练和微调训练分割模型。

Result: 合成数据在真实数据稀缺时支持监督节理轨迹检测；混合训练在真实标签一致时效果好，微调在标签有噪声时更稳健；零样本预测有限，但少量真实数据微调可实现有用泛化。

Conclusion: 该方法支持可靠的节理映射，为领域适应和评估的进一步工作提供基础。

Abstract: This paper presents a geology-driven machine learning method for automated rock joint trace mapping from images. The approach combines geological modelling, synthetic data generation, and supervised image segmentation to address limited real data and class imbalance. First, discrete fracture network models are used to generate synthetic jointed rock images at field-relevant scales via parametric modelling, preserving joint persistence, connectivity, and node-type distributions. Second, segmentation models are trained using mixed training and pretraining followed by fine-tuning on real images. The method is tested in box and slope domains using several real datasets. The results show that synthetic data can support supervised joint trace detection when real data are scarce. Mixed training performs well when real labels are consistent (e.g. box-domain), while fine-tuning is more robust when labels are noisy (e.g. slope-domain where labels can be biased, incomplete, and inconsistent). Fully zero-shot prediction from synthetic model remains limited, but useful generalisation is achieved by fine-tuning with a small number of real data. Qualitative analysis shows clearer and more geologically meaningful joint traces than indicated by quantitative metrics alone. The proposed method supports reliable joint mapping and provides a basis for further work on domain adaptation and evaluation.

</details>


### [577] [TeleBoost: A Systematic Alignment Framework for High-Fidelity, Controllable, and Robust Video Generation](https://arxiv.org/abs/2602.07595)
*Yuanzhi Liang,Xuan'er Wu,Yirui Liu,Yijie Fang,Yizhen Fan,Ke Hao,Rui Li,Ruiying Liu,Ziqi Ni,Peng Yu,Yanbo Wang,Haibin Huang,Qizhen Weng,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: 提出了一个视频生成器后训练的系统框架，用于构建可扩展、稳定且有效的后训练管道。


<details>
  <summary>Details</summary>
Motivation: 将预训练视频生成器转换为面向生产、可遵循指令、可控且在长时间范围内稳健的模型。

Method: 将监督策略塑造、奖励驱动的强化学习和基于偏好的细化组织到一个稳定性约束优化堆栈中，将优化视为分阶段、由诊断驱动的过程。

Result: 得到了一个能在保持初始化时可控性的同时，提高感知保真度、时间连贯性和提示遵循性的框架。

Conclusion: 该框架为构建可扩展的后训练管道提供了清晰蓝图，在实际部署中稳定、可扩展且有效。

Abstract: Post-training is the decisive step for converting a pretrained video generator into a production-oriented model that is instruction-following, controllable, and robust over long temporal horizons. This report presents a systematical post-training framework that organizes supervised policy shaping, reward-driven reinforcement learning, and preference-based refinement into a single stability-constrained optimization stack. The framework is designed around practical video-generation constraints, including high rollout cost, temporally compounding failure modes, and feedback that is heterogeneous, uncertain, and often weakly discriminative. By treating optimization as a staged, diagnostic-driven process rather than a collection of isolated tricks, the report summarizes a cohesive recipe for improving perceptual fidelity, temporal coherence, and prompt adherence while preserving the controllability established at initialization. The resulting framework provides a clear blueprint for building scalable post-training pipelines that remain stable, extensible, and effective in real-world deployment settings.

</details>


### [578] [Fine-R1: Make Multi-modal LLMs Excel in Fine-Grained Visual Recognition by Chain-of-Thought Reasoning](https://arxiv.org/abs/2602.07605)
*Hulingxiao He,Zijun Geng,Yuxin Peng*

Main category: cs.CV

TL;DR: 提出适用于细粒度视觉识别的多模态大语言模型Fine - R1，通过R1训练框架提升效果，4次shot训练就超越现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型在细粒度视觉识别任务上表现不佳，适配需要大量标注数据，且泛化能力差。

Method: 采用R1式训练框架，包括思维链监督微调（构建高质量FGVR CoT数据集）和三元组增强策略优化（包括类内和类间增强）。

Result: Fine - R1仅经过4次shot训练，在识别已见和未见子类别上超越现有通用MLLMs、推理MLLMs和对比CLIP模型。

Conclusion: Fine - R1在收集专家标注困难的知识密集型领域有应用前景。

Abstract: Any entity in the visual world can be hierarchically grouped based on shared characteristics and mapped to fine-grained sub-categories. While Multi-modal Large Language Models (MLLMs) achieve strong performance on coarse-grained visual tasks, they often struggle with Fine-Grained Visual Recognition (FGVR). Adapting general-purpose MLLMs to FGVR typically requires large amounts of annotated data, which is costly to obtain, leaving a substantial performance gap compared to contrastive CLIP models dedicated for discriminative tasks. Moreover, MLLMs tend to overfit to seen sub-categories and generalize poorly to unseen ones. To address these challenges, we propose Fine-R1, an MLLM tailored for FGVR through an R1-style training framework: (1) Chain-of-Thought Supervised Fine-tuning, where we construct a high-quality FGVR CoT dataset with rationales of "visual analysis, candidate sub-categories, comparison, and prediction", transition the model into a strong open-world classifier; and (2) Triplet Augmented Policy Optimization, where Intra-class Augmentation mixes trajectories from anchor and positive images within the same category to improve robustness to intra-class variance, while Inter-class Augmentation maximizes the response distinction conditioned on images across sub-categories to enhance discriminative ability. With only 4-shot training, Fine-R1 outperforms existing general MLLMs, reasoning MLLMs, and even contrastive CLIP models in identifying both seen and unseen sub-categories, showing promise in working in knowledge-intensive domains where gathering expert annotations for all sub-categories is arduous. Code is available at https://github.com/PKU-ICST-MIPL/FineR1_ICLR2026.

</details>


### [579] [AD-MIR: Bridging the Gap from Perception to Persuasion in Advertising Video Understanding via Structured Reasoning](https://arxiv.org/abs/2602.07625)
*Binxiao Xu,Junyu Feng,Xiaopeng Lin,Haodong Li,Zhiyuan Feng,Bohan Zeng,Shaolin Lu,Ming Lu,Qi She,Wentao Zhang*

Main category: cs.CV

TL;DR: 提出AD - MIR框架用于广告视频的多模态理解，在AdsQA基准测试中表现出色，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有智能体难以弥合像素级感知和高级营销逻辑之间的认知差距，需要有效方法进行广告视频的多模态理解。

Method: 采用两阶段架构，第一阶段进行结构感知的内存构建，将原始视频转换为结构化数据库；第二阶段通过结构化推理智能体模仿营销专家，采用基于证据的自我修正机制。

Result: 在AdsQA基准测试中，AD - MIR达到了最先进的性能，严格准确率比DVD高1.8%，宽松准确率高9.5%。

Conclusion: 有效的广告理解需要将抽象营销策略明确建立在像素级证据上。

Abstract: Multimodal understanding of advertising videos is essential for interpreting the intricate relationship between visual storytelling and abstract persuasion strategies. However, despite excelling at general search, existing agents often struggle to bridge the cognitive gap between pixel-level perception and high-level marketing logic. To address this challenge, we introduce AD-MIR, a framework designed to decode advertising intent via a two-stage architecture. First, in the Structure-Aware Memory Construction phase, the system converts raw video into a structured database by integrating semantic retrieval with exact keyword matching. This approach prioritizes fine-grained brand details (e.g., logos, on-screen text) while dynamically filtering out irrelevant background noise to isolate key protagonists. Second, the Structured Reasoning Agent mimics a marketing expert through an iterative inquiry loop, decomposing the narrative to deduce implicit persuasion tactics. Crucially, it employs an evidence-based self-correction mechanism that rigorously validates these insights against specific video frames, automatically backtracking when visual support is lacking. Evaluation on the AdsQA benchmark demonstrates that AD-MIR achieves state-of-the-art performance, surpassing the strongest general-purpose agent, DVD, by 1.8% in strict and 9.5% in relaxed accuracy. These results underscore that effective advertising understanding demands explicitly grounding abstract marketing strategies in pixel-level evidence. The code is available at https://github.com/Little-Fridge/AD-MIR.

</details>


### [580] [From Dead Pixels to Editable Slides: Infographic Reconstruction into Native Google Slides via Vision-Language Region Understanding](https://arxiv.org/abs/2602.07645)
*Leonardo Gonzalez*

Main category: cs.CV

TL;DR: 提出Images2Slides将静态信息图转为可编辑谷歌幻灯片，介绍方法和实验结果并指出工程挑战。


<details>
  <summary>Details</summary>
Motivation: 现有信息图导出为图像后更新、本地化和复用成本高。

Method: 通过视觉语言模型提取区域级规范，将像素几何映射到幻灯片坐标，用谷歌幻灯片批量更新API重建元素。系统模型无关，支持多VLM后端。

Result: 在29个信息图幻灯片基准测试中，整体元素回收率0.989±0.057 ，文本转录误差CER=0.033±0.149，文本区域布局保真度IoU=0.364±0.161，图像区域为0.644±0.131。

Conclusion: 指出重建中的实际工程挑战和失败模式以指导未来工作。

Abstract: Infographics are widely used to communicate information with a combination of text, icons, and data visualizations, but once exported as images their content is locked into pixels, making updates, localization, and reuse expensive. We describe \textsc{Images2Slides}, an API-based pipeline that converts a static infographic (PNG/JPG) into a native, editable Google Slides slide by extracting a region-level specification with a vision-language model (VLM), mapping pixel geometry into slide coordinates, and recreating elements using the Google Slides batch update API. The system is model-agnostic and supports multiple VLM backends via a common JSON region schema and deterministic postprocessing. On a controlled benchmark of 29 programmatically generated infographic slides with known ground-truth regions, \textsc{Images2Slides} achieves an overall element recovery rate of $0.989\pm0.057$ (text: $0.985\pm0.083$, images: $1.000\pm0.000$), with mean text transcription error $\mathrm{CER}=0.033\pm0.149$ and mean layout fidelity $\mathrm{IoU}=0.364\pm0.161$ for text regions and $0.644\pm0.131$ for image regions. We also highlight practical engineering challenges in reconstruction, including text size calibration and non-uniform backgrounds, and describe failure modes that guide future work.

</details>


### [581] [Looking and Listening Inside and Outside: Multimodal Artificial Intelligence Systems for Driver Safety Assessment and Intelligent Vehicle Decision-Making](https://arxiv.org/abs/2602.07668)
*Ross Greer,Laura Fleig,Maitrayee Keskar,Erika Maquiling,Giovanni Tapia Lopez,Angel Martinez-Sanchez,Parthib Roy,Jake Rattigan,Mira Sur,Alejandra Vidrio,Thomas Marcotte,Mohan Trivedi*

Main category: cs.CV

TL;DR: 本文提出增强LILO框架的L - LIO框架，融入音频信号，通过多模态融合提升车辆安全，评估三个音频提升安全的案例，试点结果显示音频有安全相关洞察，但也面临挑战。


<details>
  <summary>Details</summary>
Motivation: 为了在智能车辆应用中，增加音频模态作为额外信息源，更好地理解驾驶员、乘客和车外情况，提升车辆安全。

Method: 提出L - LIO框架，融入音频信号进行多模态传感器融合，评估三个音频提升车辆安全的案例，使用自定义收集的真实环境数据集进行研究。

Result: 试点发现音频在微妙或上下文丰富场景中能产生与安全相关的洞察，尤其在视觉信号不足时。

Conclusion: L - LIO通过音频和视觉传感的多模态融合增强对驾驶员和场景的理解，为安全干预提供新途径，但面临一些挑战需进一步研究。

Abstract: The looking-in-looking-out (LILO) framework has enabled intelligent vehicle applications that understand both the outside scene and the driver state to improve safety outcomes, with examples in smart airbag deployment, takeover time prediction in autonomous control transitions, and driver attention monitoring. In this research, we propose an augmentation to this framework, making a case for the audio modality as an additional source of information to understand the driver, and in the evolving autonomy landscape, also the passengers and those outside the vehicle. We expand LILO by incorporating audio signals, forming the looking-and-listening inside-and-outside (L-LIO) framework to enhance driver state assessment and environment understanding through multimodal sensor fusion. We evaluate three example cases where audio enhances vehicle safety: supervised learning on driver speech audio to classify potential impairment states (e.g., intoxication), collection and analysis of passenger natural language instructions (e.g., "turn after that red building") to motivate how spoken language can interface with planning systems through audio-aligned instruction data, and limitations of vision-only systems where audio may disambiguate the guidance and gestures of external agents. Datasets include custom-collected in-vehicle and external audio samples in real-world environments. Pilot findings show that audio yields safety-relevant insights, particularly in nuanced or context-rich scenarios where sound is critical to safe decision-making or visual signals alone are insufficient. Challenges include ambient noise interference, privacy considerations, and robustness across human subjects, motivating further work on reliability in dynamic real-world contexts. L-LIO augments driver and scene understanding through multimodal fusion of audio and visual sensing, offering new paths for safety intervention.

</details>


### [582] [Vision and language: Novel Representations and Artificial intelligence for Driving Scene Safety Assessment and Autonomous Vehicle Planning](https://arxiv.org/abs/2602.07680)
*Ross Greer,Maitrayee Keskar,Angel Martinez-Sanchez,Parthib Roy,Shashank Shriram,Mohan Trivedi*

Main category: cs.CV

TL;DR: 本文探讨视觉语言表征在自动驾驶安全评估和决策中的应用，研究三个用例，证明其对自动驾驶安全有潜力，实现潜力需精心设计系统。


<details>
  <summary>Details</summary>
Motivation: 研究视觉语言表征融入感知、预测和规划管道时如何支持驾驶场景安全评估和决策。

Method: 研究三个互补用例：一是用基于CLIP的图像 - 文本相似度进行轻量级、类别无关的危险筛查；二是将场景级视觉语言嵌入集成到基于Transformer的轨迹规划框架；三是研究自然语言作为运动规划的行为约束。

Result: 轻量级危险筛查能检测多样和分布外的道路危险；直接基于全局嵌入进行规划不能提高轨迹准确性；自然语言约束能抑制严重规划失败，改善模糊场景下的安全行为。

Conclusion: 视觉语言表征用于表达语义风险、意图和行为约束时对自动驾驶安全有重大潜力，实现潜力是工程问题，需精心设计系统而非直接注入特征。

Abstract: Vision-language models (VLMs) have recently emerged as powerful representation learning systems that align visual observations with natural language concepts, offering new opportunities for semantic reasoning in safety-critical autonomous driving. This paper investigates how vision-language representations support driving scene safety assessment and decision-making when integrated into perception, prediction, and planning pipelines. We study three complementary system-level use cases. First, we introduce a lightweight, category-agnostic hazard screening approach leveraging CLIP-based image-text similarity to produce a low-latency semantic hazard signal. This enables robust detection of diverse and out-of-distribution road hazards without explicit object detection or visual question answering. Second, we examine the integration of scene-level vision-language embeddings into a transformer-based trajectory planning framework using the Waymo Open Dataset. Our results show that naively conditioning planners on global embeddings does not improve trajectory accuracy, highlighting the importance of representation-task alignment and motivating the development of task-informed extraction methods for safety-critical planning. Third, we investigate natural language as an explicit behavioral constraint on motion planning using the doScenes dataset. In this setting, passenger-style instructions grounded in visual scene elements suppress rare but severe planning failures and improve safety-aligned behavior in ambiguous scenarios. Taken together, these findings demonstrate that vision-language representations hold significant promise for autonomous driving safety when used to express semantic risk, intent, and behavioral constraints. Realizing this potential is fundamentally an engineering problem requiring careful system design and structured grounding rather than direct feature injection.

</details>


### [583] [Process-of-Thought Reasoning for Videos](https://arxiv.org/abs/2602.07689)
*Jusheng Zhang,Kaitong Cai,Jian Wang,Yongsen Zheng,Kwok-Yan Lam,Keze Wang*

Main category: cs.CV

TL;DR: 提出视频思维过程（PoT）推理框架，使推理过程显式化，可插入现有视觉语言骨干，实验证明能提升推理效果。


<details>
  <summary>Details</summary>
Motivation: 视频理解需对长且嘈杂的观察进行时间定位和多步推理，现有方法存在不足。

Method: 将视频推理构建为一系列轻量级、可验证的步骤，包括时间证据选择、逐步状态更新和受限答案合成，还引入统一表示。

Result: 在标准视频推理任务上的大量实验表明，PoT 持续提高了事实正确性和时间定位。

Conclusion: PoT 框架有效，能提升推理效果并提供可解释的推理轨迹。

Abstract: Video understanding requires not only recognizing visual content but also performing temporally grounded, multi-step reasoning over long and noisy observations. We propose Process-of-Thought (PoT) Reasoning for Videos, a framework that makes the reasoning process explicit by structuring video inference into a sequence of lightweight, verifiable steps. PoT interleaves (i) temporal evidence selection, (ii) step-wise state updates, and (iii) constrained answer synthesis, enabling the model to progressively refine hypotheses while maintaining traceability to video evidence. The framework is designed to be model-agnostic and can be plugged into existing vision-language backbones, supporting both closed-book reasoning and evidence-augmented reasoning with external tools. We further introduce a unified representation for PoT traces that aligns intermediate decisions with temporal segments, which improves robustness to distractors and reduces hallucinated explanations. Extensive experiments on standard video reasoning tasks demonstrate that PoT consistently improves factual correctness and temporal grounding, while providing interpretable reasoning traces for diagnosis and downstream use.

</details>


### [584] [PAND: Prompt-Aware Neighborhood Distillation for Lightweight Fine-Grained Visual Classification](https://arxiv.org/abs/2602.07768)
*Qiuming Luo,Yuebing Li,Feng Li,Chang Kong*

Main category: cs.CV

TL;DR: 提出PAND框架用于细粒度视觉分类中知识蒸馏，在多个基准测试中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决细粒度视觉分类中从大的视觉语言模型向轻量级网络知识蒸馏时依赖固定提示和全局对齐的问题。

Method: 提出两阶段框架PAND，包括提示感知语义校准生成自适应语义锚点，以及邻域感知结构蒸馏策略约束学生模型局部决策结构。

Result: PAND在四个细粒度视觉分类基准测试中始终优于现有方法，如ResNet - 18学生模型在CUB - 200上准确率达76.09%，超过强基线VL2Lite 3.4%。

Conclusion: PAND有效提升细粒度视觉分类中知识蒸馏的性能。

Abstract: Distilling knowledge from large Vision-Language Models (VLMs) into lightweight networks is crucial yet challenging in Fine-Grained Visual Classification (FGVC), due to the reliance on fixed prompts and global alignment. To address this, we propose PAND (Prompt-Aware Neighborhood Distillation), a two-stage framework that decouples semantic calibration from structural transfer. First, we incorporate Prompt-Aware Semantic Calibration to generate adaptive semantic anchors. Second, we introduce a neighborhood-aware structural distillation strategy to constrain the student's local decision structure. PAND consistently outperforms state-of-the-art methods on four FGVC benchmarks. Notably, our ResNet-18 student achieves 76.09% accuracy on CUB-200, surpassing the strong baseline VL2Lite by 3.4%. Code is available at https://github.com/LLLVTA/PAND.

</details>


### [585] [VideoTemp-o3: Harmonizing Temporal Grounding and Video Understanding in Agentic Thinking-with-Videos](https://arxiv.org/abs/2602.07801)
*Wenqi Liu,Yunxiao Wang,Shijie Ma,Meng Liu,Qile Su,Tianke Zhang,Haonan Fan,Changyi Liu,Kaiyu Jiang,Jiankang Chen,Kaiyu Tang,Bin Wen,Fan Yang,Tingting Gao,Han Li,Yinwei Wei,Xuemeng Song*

Main category: cs.CV

TL;DR: 传统均匀帧采样在长视频理解中有缺陷，现有方法也有不足，提出VideoTemp - o3框架，在长视频理解和定位上表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统均匀帧采样无法捕捉关键视觉证据，现有基于思考范式的方法效率低、定位弱、工作流僵化，需解决这些问题。

Method: 提出VideoTemp - o3框架联合建模视频定位和问答；在监督微调阶段设计统一掩码机制；在强化学习中引入专用奖励；从数据角度构建高质量长视频问答数据及对应基准。

Result: 实验结果表明该方法在长视频理解和定位上取得显著性能。

Conclusion: VideoTemp - o3框架能有效解决长视频理解和定位问题，提升性能。

Abstract: In long-video understanding, conventional uniform frame sampling often fails to capture key visual evidence, leading to degraded performance and increased hallucinations. To address this, recent agentic thinking-with-videos paradigms have emerged, adopting a localize-clip-answer pipeline in which the model actively identifies relevant video segments, performs dense sampling within those clips, and then produces answers. However, existing methods remain inefficient, suffer from weak localization, and adhere to rigid workflows. To solve these issues, we propose VideoTemp-o3, a unified agentic thinking-with-videos framework that jointly models video grounding and question answering. VideoTemp-o3 exhibits strong localization capability, supports on-demand clipping, and can refine inaccurate localizations. Specifically, in the supervised fine-tuning stage, we design a unified masking mechanism that encourages exploration while preventing noise. For reinforcement learning, we introduce dedicated rewards to mitigate reward hacking. Besides, from the data perspective, we develop an effective pipeline to construct high-quality long video grounded QA data, along with a corresponding benchmark for systematic evaluation across various video durations. Experimental results demonstrate that our method achieves remarkable performance on both long video understanding and grounding.

</details>


### [586] [How well are open sourced AI-generated image detection models out-of-the-box: A comprehensive benchmark study](https://arxiv.org/abs/2602.07814)
*Simiao Ren,Yuchen Zhou,Xingyu Shen,Kidus Zewde,Tommy Duong,George Huang,Hatsanai,Tiangratanakul,Tsang,Ng,En Wei,Jiayu Xue*

Main category: cs.CV

TL;DR: 论文对16种先进的AI生成图像检测方法在12个数据集上进行零样本评估，发现检测效果不稳定、有差距，训练数据对齐影响泛化，现代商业生成器难被检测等，挑战了通用检测器范式并给出部署建议。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成图像增多，可靠检测方法至关重要，但现有基准主要评估微调模型，缺乏对开箱即用性能的理解。

Method: 对16种先进检测方法的23个预训练检测器变体，在包含291个唯一生成器的12个不同数据集、260万张图像样本上进行零样本评估。

Result: 检测排名不稳定；最佳和最差检测器性能差距达37个百分点；训练数据对齐影响泛化，性能差异达20 - 60%；现代商业生成器难被检测，准确率仅18 - 30%；存在三种影响跨数据集泛化的失败模式。

Conclusion: 挑战了‘一刀切’的检测器范式，从业者应根据具体威胁场景选择检测器。

Abstract: As AI-generated images proliferate across digital platforms, reliable detection methods have become critical for combating misinformation and maintaining content authenticity. While numerous deepfake detection methods have been proposed, existing benchmarks predominantly evaluate fine-tuned models, leaving a critical gap in understanding out-of-the-box performance -- the most common deployment scenario for practitioners. We present the first comprehensive zero-shot evaluation of 16 state-of-the-art detection methods, comprising 23 pretrained detector variants (due to multiple released versions of certain detectors), across 12 diverse datasets, comprising 2.6~million image samples spanning 291 unique generators including modern diffusion models. Our systematic analysis reveals striking findings: (1)~no universal winner exists, with detector rankings exhibiting substantial instability (Spearman~$ρ$: 0.01 -- 0.87 across dataset pairs); (2)~a 37~percentage-point performance gap separates the best detector (75.0\% mean accuracy) from the worst (37.5\%); (3)~training data alignment critically impacts generalization, causing up to 20--60\% performance variance within architecturally identical detector families; (4)~modern commercial generators (Flux~Dev, Firefly~v4, Midjourney~v7) defeat most detectors, achieving only 18--30\% average accuracy; and (5)~we identify three systematic failure patterns affecting cross-dataset generalization. Statistical analysis confirms significant performance differences between detectors (Friedman test: $χ^2$=121.01, $p<10^{-16}$, Kendall~$W$=0.524). Our findings challenge the ``one-size-fits-all'' detector paradigm and provide actionable deployment guidelines, demonstrating that practitioners must carefully select detectors based on their specific threat landscape rather than relying on published benchmark performance.

</details>


### [587] [SPD-Faith Bench: Diagnosing and Improving Faithfulness in Chain-of-Thought for Multimodal Large Language Models](https://arxiv.org/abs/2602.07833)
*Weijiang Lv,Yaoxuan Feng,Xiaobo Xia,Jiayu Wang,Yan Jing,Wenchao Chen,Bo Chen*

Main category: cs.CV

TL;DR: 引入SPD - Faith Bench评估多模态大语言模型推理忠实性，揭示两种失败模式，提出无训练的SAGE框架，强调评估忠实性重要性。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型思维链推理生成的推理轨迹忠实性不明，以往工作主要关注感知幻觉，推理层面的不忠实性研究不足。

Method: 引入基于细粒度图像差异推理的SPD - Faith Bench进行评估；提出无训练的视觉证据校准框架SAGE。

Result: 评估发现感知失明和感知 - 推理分离两种失败模式，失败源于视觉注意力衰减和残差流中的表征偏移；SAGE框架可改善视觉路由并使推理与感知对齐。

Conclusion: 强调除响应正确性外，明确评估忠实性很重要。

Abstract: Chain-of-Thought reasoning is widely used to improve the interpretability of multimodal large language models (MLLMs), yet the faithfulness of the generated reasoning traces remains unclear. Prior work has mainly focused on perceptual hallucinations, leaving reasoning level unfaithfulness underexplored. To isolate faithfulness from linguistic priors, we introduce SPD-Faith Bench, a diagnostic benchmark based on fine-grained image difference reasoning that enforces explicit visual comparison. Evaluations on state-of-the-art MLLMs reveal two systematic failure modes, perceptual blindness and perception-reasoning dissociation. We trace these failures to decaying visual attention and representation shifts in the residual stream. Guided by this analysis, we propose SAGE, a train-free visual evidence-calibrated framework that improves visual routing and aligns reasoning with perception. Our results highlight the importance of explicitly evaluating faithfulness beyond response correctness. Our benchmark and codes are available at https://github.com/Johanson-colab/SPD-Faith-Bench.

</details>


### [588] [Where Not to Learn: Prior-Aligned Training with Subset-based Attribution Constraints for Reliable Decision-Making](https://arxiv.org/abs/2602.07008)
*Ruoyu Chen,Shangquan Sun,Xiaoqing Guo,Sanyi Zhang,Kangwei Liu,Shiming Liu,Zhangcheng Wang,Qunli Zhang,Hua Zhang,Xiaochun Cao*

Main category: cs.CV

TL;DR: 提出基于归因的人类先验对齐方法，在图像分类和点击决策任务验证，可提升任务准确率和决策合理性。


<details>
  <summary>Details</summary>
Motivation: 传统监督学习仅提供类别标签，模型可能通过捷径关联实现高准确率，且将模型与人类先验对齐有挑战。

Method: 将人类先验编码为模型应依赖的输入区域，利用基于子集选择的归因方法暴露模型决策证据，当归因区域与先验区域偏差大时进行惩罚。

Result: 在图像分类和点击决策任务中，人类先验对齐在常规分类和自回归生成设置下均提升任务准确率和决策合理性。

Conclusion: 所提出的基于归因的人类先验对齐方法有效。

Abstract: Reliable models should not only predict correctly, but also justify decisions with acceptable evidence. Yet conventional supervised learning typically provides only class-level labels, allowing models to achieve high accuracy through shortcut correlations rather than the intended evidence. Human priors can help constrain such behavior, but aligning models to these priors remains challenging because learned representations often diverge from human perception. To address this challenge, we propose an attribution-based human prior alignment method. We encode human priors as input regions that the model is expected to rely on (e.g., bounding boxes), and leverage a highly faithful subset-selection-based attribution approach to expose the model's decision evidence during training. When the attribution region deviates substantially from the prior regions, we penalize reliance on off-prior evidence, encouraging the model to shift its attribution toward the intended regions. This is achieved through a training objective that imposes attribution constraints induced by the human prior. We validate our method on both image classification and click decision tasks in MLLM-based GUI agent models. Across conventional classification and autoregressive generation settings, human prior alignment consistently improves task accuracy while also enhancing the model's decision reasonability.

</details>


### [589] [Fair Context Learning for Evidence-Balanced Test-Time Adaptation in Vision-Language Models](https://arxiv.org/abs/2602.07027)
*Sanggeon Yun,Ryozo Masukawa,SungHeon Jeong,Wenjun Huang,Hanning Chen,Mohsen Imani*

Main category: cs.CV

TL;DR: 提出Fair Context Learning (FCL) 框架解决VLM在分布偏移下性能下降问题，避免熵最小化，实验验证其性能。


<details>
  <summary>Details</summary>
Motivation: Vision-Language Models在分布偏移下性能下降，现有prompt - based TTA方法依赖熵最小化存在问题，需新方法解决。

Method: FCL框架将适应过程分为基于增强的探索和公平驱动的校准，不依赖熵最小化。

Result: 通过大量实验验证理论动机，FCL在不同基准测试中达到与现有技术竞争的适应性能。

Conclusion: FCL框架有效解决了VLM在分布偏移下的性能问题，避免了熵最小化带来的弊端。

Abstract: Vision-Language Models (VLMs) such as CLIP enable strong zero-shot recognition but suffer substantial degradation under distribution shifts. Test-Time Adaptation (TTA) aims to improve robustness using only unlabeled test samples, yet most prompt-based TTA methods rely on entropy minimization -- an approach that can amplify spurious correlations and induce overconfident errors when classes share visual features. We propose Fair Context Learning (FCL), an episodic TTA framework that avoids entropy minimization by explicitly addressing shared-evidence bias. Motivated by our additive evidence decomposition assumption, FCL decouples adaptation into (i) augmentation-based exploration to identify plausible class candidates, and (ii) fairness-driven calibration that adapts text contexts to equalize sensitivity to common visual evidence. This fairness constraint mitigates partial feature obsession and enables effective calibration of text embeddings without relying on entropy reduction. Through extensive evaluation, we empirically validate our theoretical motivation and show that FCL achieves competitive adaptation performance relative to state-of-the-art TTA methods across diverse domain-shift and fine-grained benchmarks.

</details>


### [590] [Scalable Adaptation of 3D Geometric Foundation Models via Weak Supervision from Internet Video](https://arxiv.org/abs/2602.07891)
*Zihui Gao,Ke Liu,Donny Y. Chen,Duochao Shi,Guosheng Lin,Hao Chen,Chunhua Shen*

Main category: cs.CV

TL;DR: 本文提出SAGE框架利用互联网视频适应几何基础模型进行3D学习，通过特定策略提升零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 几何基础模型在3D重建中受限于3D标注稀缺，互联网视频虽数据多但难用于几何学习。

Method: 提出SAGE框架，利用分层挖掘管道将视频转化为训练轨迹和混合监督，引入正则化策略防止灾难性遗忘。

Result: SAGE显著提升零样本泛化，在未见过的基准上Chamfer距离相比现有基线降低20 - 42%。

Conclusion: SAGE开创了通过互联网视频适应几何基础模型的先河，建立通用3D学习的可扩展范式。

Abstract: Geometric foundation models show promise in 3D reconstruction, yet their progress is severely constrained by the scarcity of diverse, large-scale 3D annotations. While Internet videos offer virtually unlimited raw data, utilizing them as a scaling source for geometric learning is challenging due to the absence of ground-truth geometry and the presence of observational noise. To address this, we propose SAGE, a framework for Scalable Adaptation of GEometric foundation models from raw video streams. SAGE leverages a hierarchical mining pipeline to transform videos into training trajectories and hybrid supervision: (1) Informative training trajectory selection; (2) Sparse Geometric Anchoring via SfM point clouds for global structural guidance; and (3) Dense Differentiable Consistency via 3D Gaussian rendering for multi-view constraints. To prevent catastrophic forgetting, we introduce a regularization strategy using anchor data. Extensive experiments show that SAGE significantly enhances zero-shot generalization, reducing Chamfer Distance by 20-42% on unseen benchmarks (7Scenes, TUM-RGBD, Matterport3D) compared to state-of-the-art baselines. To our knowledge, SAGE pioneers the adaptation of geometric foundation models via Internet video, establishing a scalable paradigm for general-purpose 3D learning.

</details>


### [591] [OMNI-Dent: Towards an Accessible and Explainable AI Framework for Automated Dental Diagnosis](https://arxiv.org/abs/2602.07041)
*Leeje Jang,Yao-Yi Chiang,Angela M. Hastings,Patimaporn Pungchanchaikul,Martha B. Lucas,Emily C. Schultz,Jeffrey P. Louie,Mohamed Estai,Wen-Chen Wang,Ryan H. L. Ip,Boyen Huang*

Main category: cs.CV

TL;DR: 提出OMNI - Dent框架，用于高效且可解释的牙科诊断，支持在缺乏专业评估的场景下进行诊断。


<details>
  <summary>Details</summary>
Motivation: 现有基于AI的牙科诊断方法存在未反映临床推理、需大量标注数据、泛化能力差等问题，很多人无法及时获得专业评估。

Method: 提出OMNI - Dent框架，将临床推理原则融入基于VLM的流程，利用多视图智能手机照片，嵌入专家诊断启发式方法，引导VLM进行牙齿级评估，无需对VLM进行牙科特定微调。

Result: 未提及具体实验结果。

Conclusion: OMNI - Dent作为早期辅助工具，可帮助用户识别潜在异常，判断是否需要专业评估，为难以获得当面护理的人提供实用选择。

Abstract: Accurate dental diagnosis is essential for oral healthcare, yet many individuals lack access to timely professional evaluation. Existing AI-based methods primarily treat diagnosis as a visual pattern recognition task and do not reflect the structured clinical reasoning used by dental professionals. These approaches also require large amounts of expert-annotated data and often struggle to generalize across diverse real-world imaging conditions. To address these limitations, we present OMNI-Dent, a data-efficient and explainable diagnostic framework that incorporates clinical reasoning principles into a Vision-Language Model (VLM)-based pipeline. The framework operates on multi-view smartphone photographs,embeds diagnostic heuristics from dental experts, and guides a general-purpose VLM to perform tooth-level evaluation without dental-specific fine-tuning of the VLM. By utilizing the VLM's existing visual-linguistic capabilities, OMNI-Dent aims to support diagnostic assessment in settings where curated clinical imaging is unavailable. Designed as an early-stage assistive tool, OMNI-Dent helps users identify potential abnormalities and determine when professional evaluation may be needed, offering a practical option for individuals with limited access to in-person care.

</details>


### [592] [ShapBPT: Image Feature Attributions Using Data-Aware Binary Partition Trees](https://arxiv.org/abs/2602.07047)
*Muhammad Rashid,Elvio G. Amparore,Enrico Ferrari,Damiano Verda*

Main category: cs.CV

TL;DR: 提出基于分层Shapley公式的数据感知可解释计算机视觉方法ShapBPT，可有效处理图像特征归因，实验证明其更优。


<details>
  <summary>Details</summary>
Motivation: 现有分层Shapley方法未利用图像多尺度结构，导致收敛慢、与实际形态特征对齐弱，且缺乏用于计算机视觉任务的数据感知层次。

Method: 引入ShapBPT方法，将Shapley系数分配给适合图像的多尺度分层结构——二叉分区树（BPT），利用数据感知分层分区。

Result: 实验表明ShapBPT与图像结构对齐更好、效率更高，用户研究显示其解释更受人类青睐。

Conclusion: ShapBPT将分层Shapley方法与图像数据相结合，提供了一种更高效且有语义意义的视觉可解释性方法。

Abstract: Pixel-level feature attributions are an important tool in eXplainable AI for Computer Vision (XCV), providing visual insights into how image features influence model predictions. The Owen formula for hierarchical Shapley values has been widely used to interpret machine learning (ML) models and their learned representations. However, existing hierarchical Shapley approaches do not exploit the multiscale structure of image data, leading to slow convergence and weak alignment with the actual morphological features. Moreover, no prior Shapley method has leveraged data-aware hierarchies for Computer Vision tasks, leaving a gap in model interpretability of structured visual data. To address this, this paper introduces ShapBPT, a novel data-aware XCV method based on the hierarchical Shapley formula. ShapBPT assigns Shapley coefficients to a multiscale hierarchical structure tailored for images, the Binary Partition Tree (BPT). By using this data-aware hierarchical partitioning, ShapBPT ensures that feature attributions align with intrinsic image morphology, effectively prioritizing relevant regions while reducing computational overhead. This advancement connects hierarchical Shapley methods with image data, providing a more efficient and semantically meaningful approach to visual interpretability. Experimental results confirm ShapBPT's effectiveness, demonstrating superior alignment with image structures and improved efficiency over existing XCV methods, and a 20-subject user study confirming that ShapBPT explanations are preferred by humans.

</details>


### [593] [MCIE: Multimodal LLM-Driven Complex Instruction Image Editing with Spatial Guidance](https://arxiv.org/abs/2602.07993)
*Xuehai Bai,Xiaoling Gu,Akide Liu,Hangjie Yuan,YiFan Zhang,Jack Ma*

Main category: cs.CV

TL;DR: 现有基于指令的图像编辑方法有局限，本文提出MCIE - E1方法，构建数据管道，引入新基准CIE - Bench，实验显示MCIE - E1表现优于先前方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于指令的图像编辑方法局限于简单操作，难以满足复杂指令的实际应用需求，且当前模型存在指令遵循不足和背景不一致的问题。

Method: 提出MCIE - E1方法，集成空间感知和背景一致的跨注意力模块；构建专用数据管道，结合自动过滤和人工验证；引入新基准CIE - Bench和两个新评估指标。

Result: 在CIE - Bench上的实验表明，MCIE - E1在定量和定性评估中均优于先前的先进方法，指令遵循度提高23.96%。

Conclusion: MCIE - E1方法有效解决了现有基于指令的图像编辑方法的局限性，在复杂指令图像编辑任务中表现出色。

Abstract: Recent advances in instruction-based image editing have shown remarkable progress. However, existing methods remain limited to relatively simple editing operations, hindering real-world applications that require complex and compositional instructions. In this work, we address these limitations from the perspectives of architectural design, data, and evaluation protocols. Specifically, we identify two key challenges in current models: insufficient instruction compliance and background inconsistency. To this end, we propose MCIE-E1, a Multimodal Large Language Model-Driven Complex Instruction Image Editing method that integrates two key modules: a spatial-aware cross-attention module and a background-consistent cross-attention module. The former enhances instruction-following capability by explicitly aligning semantic instructions with spatial regions through spatial guidance during the denoising process, while the latter preserves features in unedited regions to maintain background consistency. To enable effective training, we construct a dedicated data pipeline to mitigate the scarcity of complex instruction-based image editing datasets, combining fine-grained automatic filtering via a powerful MLLM with rigorous human validation. Finally, to comprehensively evaluate complex instruction-based image editing, we introduce CIE-Bench, a new benchmark with two new evaluation metrics. Experimental results on CIE-Bench demonstrate that MCIE-E1 consistently outperforms previous state-of-the-art methods in both quantitative and qualitative assessments, achieving a 23.96% improvement in instruction compliance.

</details>


### [594] [ForecastOcc: Vision-based Semantic Occupancy Forecasting](https://arxiv.org/abs/2602.08006)
*Riya Mohan,Juana Valeria Hurtado,Rohit Mohan,Abhinav Valada*

Main category: cs.CV

TL;DR: 提出 ForecastOcc 框架用于基于视觉的语义占用预测，直接从过去相机图像预测多时段语义占用状态，在多数据集上表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉的占用预测方法缺乏语义信息，且依赖外部估计的地图，易受误差累积影响，无法直接从图像学习时空特征。

Method: 提出 ForecastOcc 框架，包含时间交叉注意力预测模块、2D - 3D 视图转换器、3D 编码器和语义占用头，直接从过去相机图像预测未来占用状态和语义类别。

Result: 在 Occ3D - nuScenes 数据集的多视图预测和 SemanticKITTI 的单目预测中，ForecastOcc 持续优于基线，产生语义丰富、能捕捉场景动态和语义的预测结果。

Conclusion: ForecastOcc 框架是有效的基于视觉语义占用预测框架，可为自动驾驶提供重要的场景语义和动态信息。

Abstract: Autonomous driving requires forecasting both geometry and semantics over time to effectively reason about future environment states. Existing vision-based occupancy forecasting methods focus on motion-related categories such as static and dynamic objects, while semantic information remains largely absent. Recent semantic occupancy forecasting approaches address this gap but rely on past occupancy predictions obtained from separate networks. This makes current methods sensitive to error accumulation and prevents learning spatio-temporal features directly from images. In this work, we present ForecastOcc, the first framework for vision-based semantic occupancy forecasting that jointly predicts future occupancy states and semantic categories. Our framework yields semantic occupancy forecasts for multiple horizons directly from past camera images, without relying on externally estimated maps. We evaluate ForecastOcc in two complementary settings: multi-view forecasting on the Occ3D-nuScenes dataset and monocular forecasting on SemanticKITTI, where we establish the first benchmark for this task. We introduce the first baselines by adapting two 2D forecasting modules within our framework. Importantly, we propose a novel architecture that incorporates a temporal cross-attention forecasting module, a 2D-to-3D view transformer, a 3D encoder for occupancy prediction, and a semantic occupancy head for voxel-level forecasts across multiple horizons. Extensive experiments on both datasets show that ForecastOcc consistently outperforms baselines, yielding semantically rich, future-aware predictions that capture scene dynamics and semantics critical for autonomous driving.

</details>


### [595] [3D Transport-based Morphometry (3D-TBM) for medical image analysis](https://arxiv.org/abs/2602.07260)
*Hongyu Kan,Kristofor Pas,Ivan Medri,Naqib Sad Pathan,Natasha Ironside,Shinjini Kundu,Jingjia He,Gustavo Kunde Rohde*

Main category: cs.CV

TL;DR: 介绍用于3D医学图像形态分析的工具3D - TBM，含数据处理、嵌入计算等功能，有文档和教程，代码开源。


<details>
  <summary>Details</summary>
Motivation: 推动Transport - Based Morphometry (TBM)在临床影像研究中的广泛应用。

Method: 开发3D - TBM工具，包含数据预处理、计算最优传输嵌入、可视化主要传输方向等分析方法，还提供文档和教程。

Result: 开发出3D - TBM工具，代码通过PyTransKit公开可用。

Conclusion: 3D - TBM工具可帮助研究人员将其应用于自身医学影像研究，促进TBM在临床影像研究中的应用。

Abstract: Transport-Based Morphometry (TBM) has emerged as a new framework for 3D medical image analysis. By embedding images into a transport domain via invertible transformations, TBM facilitates effective classification, regression, and other tasks using transport-domain features. Crucially, the inverse mapping enables the projection of analytic results back into the original image space, allowing researchers to directly interpret clinical features associated with model outputs in a spatially meaningful way. To facilitate broader adoption of TBM in clinical imaging research, we present 3D-TBM, a tool designed for morphological analysis of 3D medical images. The framework includes data preprocessing, computation of optimal transport embeddings, and analytical methods such as visualization of main transport directions, together with techniques for discerning discriminating directions and related analysis methods. We also provide comprehensive documentation and practical tutorials to support researchers interested in applying 3D-TBM in their own medical imaging studies. The source code is publicly available through PyTransKit.

</details>


### [596] [FlashVID: Efficient Video Large Language Models via Training-free Tree-based Spatiotemporal Token Merging](https://arxiv.org/abs/2602.08024)
*Ziyang Fan,Keyu Chen,Ruilong Xing,Yulin Li,Li Jiang,Zhuotao Tian*

Main category: cs.CV

TL;DR: 现有视频大模型处理视觉令牌效率低，现有加速框架时空压缩不佳，本文提出无训练推理加速框架FlashVID，实验证明其有效，能提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频大模型需处理大量视觉令牌导致计算效率低，现有加速框架忽略时空关系致时空压缩不佳。

Method: 引入Attention and Diversity-based Token Selection (ADTS) 选择代表性令牌进行基本视频表示，应用Tree-based Spatiotemporal Token Merging (TSTM) 进行细粒度时空冗余消除。

Result: 在三个代表性视频大模型和五个视频理解基准测试中证明方法有效，保留10%视觉令牌可保留LLaVA - OneVision 99.1%性能，使Qwen2.5 - VL视频帧输入增加10倍，性能相对提升8.6%。

Conclusion: FlashVID是无训练、即插即用模块，可扩展长视频帧，提升计算效率和模型性能。

Abstract: Although Video Large Language Models (VLLMs) have shown remarkable capabilities in video understanding, they are required to process high volumes of visual tokens, causing significant computational inefficiency. Existing VLLMs acceleration frameworks usually compress spatial and temporal redundancy independently, which overlooks the spatiotemporal relationships, thereby leading to suboptimal spatiotemporal compression. The highly correlated visual features are likely to change in spatial position, scale, orientation, and other attributes over time due to the dynamic nature of video. Building on this insight, we introduce FlashVID, a training-free inference acceleration framework for VLLMs. Specifically, FlashVID utilizes Attention and Diversity-based Token Selection (ADTS) to select the most representative tokens for basic video representation, then applies Tree-based Spatiotemporal Token Merging (TSTM) for fine-grained spatiotemporal redundancy elimination. Extensive experiments conducted on three representative VLLMs across five video understanding benchmarks demonstrate the effectiveness and generalization of our method. Notably, by retaining only 10% of visual tokens, FlashVID preserves 99.1% of the performance of LLaVA-OneVision. Consequently, FlashVID can serve as a training-free and plug-and-play module for extending long video frames, which enables a 10x increase in video frame input to Qwen2.5-VL, resulting in a relative improvement of 8.6% within the same computational budget. Code is available at https://github.com/Fanziyang-v/FlashVID.

</details>


### [597] [MIND: Benchmarking Memory Consistency and Action Control in World Models](https://arxiv.org/abs/2602.08025)
*Yixuan Ye,Xuanyu Lu,Yuxin Jiang,Yuchao Gu,Rui Zhao,Qiwei Liang,Jiachun Pan,Fengda Zhang,Weijia Wu,Alex Jinpeng Wang*

Main category: cs.CV

TL;DR: 论文提出首个开放域闭环重访基准MIND用于评估世界模型的核心能力，并介绍了MIND-World基线，实验验证了MIND的完整性并揭示当前世界模型挑战。


<details>
  <summary>Details</summary>
Motivation: 现缺少评估世界模型基本能力的统一基准。

Method: 引入MIND基准，包含250个高质量视频，设计评估框架测量记忆一致性和动作控制能力，设计不同动作空间评估动作泛化能力，引入MIND-World基线。

Result: 实验验证了MIND的完整性，揭示当前世界模型存在难以保持长期记忆一致性和跨动作空间泛化的挑战。

Conclusion: MIND可用于评估世界模型核心能力，当前世界模型在记忆一致性和动作空间泛化方面存在挑战。

Abstract: World models aim to understand, remember, and predict dynamic visual environments, yet a unified benchmark for evaluating their fundamental abilities remains lacking. To address this gap, we introduce MIND, the first open-domain closed-loop revisited benchmark for evaluating Memory consIstency and action coNtrol in worlD models. MIND contains 250 high-quality videos at 1080p and 24 FPS, including 100 (first-person) + 100 (third-person) video clips under a shared action space and 25 + 25 clips across varied action spaces covering eight diverse scenes. We design an efficient evaluation framework to measure two core abilities: memory consistency and action control, capturing temporal stability and contextual coherence across viewpoints. Furthermore, we design various action spaces, including different character movement speeds and camera rotation angles, to evaluate the action generalization capability across different action spaces under shared scenes. To facilitate future performance benchmarking on MIND, we introduce MIND-World, a novel interactive Video-to-World baseline. Extensive experiments demonstrate the completeness of MIND and reveal key challenges in current world models, including the difficulty of maintaining long-term memory consistency and generalizing across action spaces. Project page: https://csu-jpg.github.io/MIND.github.io/

</details>


### [598] [Cross-View World Models](https://arxiv.org/abs/2602.07277)
*Rishabh Sharma,Gijs Hogervorst,Wayne E. Mackey,David J. Heeger,Stefano Martiniani*

Main category: cs.CV

TL;DR: 提出跨视图世界模型（XVWM），训练模型进行跨视图预测，能让智能体跨视角并行想象，多视图一致性为空间表征提供学习信号，还可能为多智能体视角采择奠定基础。


<details>
  <summary>Details</summary>
Motivation: 现有世界模型方法多从单一视角规划，而其他视角可能使规划更简单，如导航中鸟瞰视角更有利。

Method: 引入XVWM，以跨视图预测为目标进行训练，在Aimlabs的同步多视图游戏数据上训练，执行时从自我中心视角出发。

Result: 多视图一致性为空间表征提供了强大的学习信号。

Conclusion: 从其他视角预测行动后果可能为多智能体场景中的视角采择提供基础。

Abstract: World models enable agents to plan by imagining future states, but existing approaches operate from a single viewpoint, typically egocentric, even when other perspectives would make planning easier; navigation, for instance, benefits from a bird's-eye view. We introduce Cross-View World Models (XVWM), trained with a cross-view prediction objective: given a sequence of frames from one viewpoint, predict the future state from the same or a different viewpoint after an action is taken. Enforcing cross-view consistency acts as geometric regularization: because the input and output views may share little or no visual overlap, to predict across viewpoints, the model must learn view-invariant representations of the environment's 3D structure. We train on synchronized multi-view gameplay data from Aimlabs, an aim-training platform providing precisely aligned multi-camera recordings with high-frequency action labels. The resulting model gives agents parallel imagination streams across viewpoints, enabling planning in whichever frame of reference best suits the task while executing from the egocentric view. Our results show that multi-view consistency provides a strong learning signal for spatially grounded representations. Finally, predicting the consequences of one's actions from another viewpoint may offer a foundation for perspective-taking in multi-agent settings.

</details>


### [599] [Optimization of Precipitate Segmentation Through Linear Genetic Programming of Image Processing](https://arxiv.org/abs/2602.07310)
*Kyle Williams,Andrew Seltzman*

Main category: cs.CV

TL;DR: 提出用线性遗传编程优化的过滤和分割算法检测FIB横截面显微图中的沉淀物，实现自动化，加速合金开发迭代。


<details>
  <summary>Details</summary>
Motivation: 当前增材制造铌基铜合金分析依赖手动注释，因显微图存在对比度、噪声和图像伪影等问题，减慢了合金开发迭代速度。

Method: 提出过滤和分割算法，用线性遗传编程（LGP）优化，使用特定领域语言迭代解决方案，生成可解释的MATLAB代码。

Result: 在理想条件下，系统能找到接近人类精度的解决方案，平均评估误差1.8%，优化管道算法平均约2秒处理360万像素图像。

Conclusion: 自动化工作加快迭代周期，有助于探索材料成分和加工空间，推动增材制造聚变反应堆部件用铜合金的研发。

Abstract: Current analysis of additive manufactured niobium-based copper alloys relies on hand annotation due to varying contrast, noise, and image artifacts present in micrographs, slowing iteration speed in alloy development. We present a filtering and segmentation algorithm for detecting precipitates in FIB cross-section micrographs, optimized using linear genetic programming (LGP), which accounts for the various artifacts. To this end, the optimization environment uses a domain-specific language for image processing to iterate on solutions. Programs in this language are a list of image-filtering blocks with tunable parameters that sequentially process an input image, allowing for reliable generation and mutation by a genetic algorithm. Our environment produces optimized human-interpretable MATLAB code representing an image filtering pipeline. Under ideal conditions--a population size of 60 and a maximum program length of 5 blocks--our system was able to find a near-human accuracy solution with an average evaluation error of 1.8% when comparing segmentations pixel-by-pixel to a human baseline using an XOR error evaluation. Our automation work enabled faster iteration cycles and furthered exploration of the material composition and processing space: our optimized pipeline algorithm processes a 3.6 megapixel image in about 2 seconds on average. This ultimately enables convergence on strong, low-activation, precipitation hardened copper alloys for additive manufactured fusion reactor parts.

</details>


### [600] [Weak to Strong: VLM-Based Pseudo-Labeling as a Weakly Supervised Training Strategy in Multimodal Video-based Hidden Emotion Understanding Tasks](https://arxiv.org/abs/2602.08057)
*Yufei Wang,Haixu Liu,Tianxiang Xu,Chuancheng Shi,Hongsheng Xing*

Main category: cs.CV

TL;DR: 本文提出多模态弱监督框架用于视频中‘隐藏情绪’自动识别，在iMiGUE网球访谈数据集上取得最优结果，提升了准确率并验证了MLP关键点骨干网络的有效性。


<details>
  <summary>Details</summary>
Motivation: 解决视频中‘隐藏情绪’的自动识别问题。

Method: 使用YOLO 11x检测裁剪人像，DINOv2 - Base提取视觉特征；利用Gemini 2.5 Pro结合CoT + Reflection生成伪标签和推理文本作为弱监督；OpenPose生成关键点序列并添加帧间偏移特征，简化图神经网络骨干为MLP；用超长序列Transformer编码图像和关键点序列，与BERT编码的访谈记录拼接；各模态先单独预训练再联合微调，将伪标签样本加入训练集。

Result: 在严重类别不平衡情况下，准确率从之前的低于0.6提升到超过0.69，建立新的公开基准。

Conclusion: 提出的多模态弱监督框架有效，‘MLP化’的关键点骨干网络在该任务中可媲美甚至超越基于GCN的网络。

Abstract: To tackle the automatic recognition of "concealed emotions" in videos, this paper proposes a multimodal weak-supervision framework and achieves state-of-the-art results on the iMiGUE tennis-interview dataset. First, YOLO 11x detects and crops human portraits frame-by-frame, and DINOv2-Base extracts visual features from the cropped regions. Next, by integrating Chain-of-Thought and Reflection prompting (CoT + Reflection), Gemini 2.5 Pro automatically generates pseudo-labels and reasoning texts that serve as weak supervision for downstream models. Subsequently, OpenPose produces 137-dimensional key-point sequences, augmented with inter-frame offset features; the usual graph neural network backbone is simplified to an MLP to efficiently model the spatiotemporal relationships of the three key-point streams. An ultra-long-sequence Transformer independently encodes both the image and key-point sequences, and their representations are concatenated with BERT-encoded interview transcripts. Each modality is first pre-trained in isolation, then fine-tuned jointly, with pseudo-labeled samples merged into the training set for further gains. Experiments demonstrate that, despite severe class imbalance, the proposed approach lifts accuracy from under 0.6 in prior work to over 0.69, establishing a new public benchmark. The study also validates that an "MLP-ified" key-point backbone can match - or even surpass - GCN-based counterparts in this task.

</details>


### [601] [Picasso: Holistic Scene Reconstruction with Physics-Constrained Sampling](https://arxiv.org/abs/2602.08058)
*Xihang Yu,Rajat Talak,Lorenzo Shaikewitz,Luca Carlone*

Main category: cs.CV

TL;DR: 本文提出物理约束的重建管道Picasso和Picasso数据集，在新数据集和YCB - V数据集上评估显示其优于现有方法，重建结果符合物理规律和人类直觉。


<details>
  <summary>Details</summary>
Motivation: 存在遮挡和测量噪声时，几何准确的场景重建可能物理上不正确，难以用数字双胞胎预测场景动态行为，因此需要整体推理场景。

Method: 提出物理约束的重建管道Picasso，采用快速拒绝采样方法，利用推断的物体接触图引导采样；提出Picasso数据集和量化物理合理性的指标。

Result: 在新引入的数据集和YCB - V数据集上广泛评估，Picasso大幅优于现有技术。

Conclusion: Picasso能提供既符合物理规律又更符合人类直觉的重建结果。

Abstract: In the presence of occlusions and measurement noise, geometrically accurate scene reconstructions -- which fit the sensor data -- can still be physically incorrect. For instance, when estimating the poses and shapes of objects in the scene and importing the resulting estimates into a simulator, small errors might translate to implausible configurations including object interpenetration or unstable equilibrium. This makes it difficult to predict the dynamic behavior of the scene using a digital twin, an important step in simulation-based planning and control of contact-rich behaviors. In this paper, we posit that object pose and shape estimation requires reasoning holistically over the scene (instead of reasoning about each object in isolation), accounting for object interactions and physical plausibility. Towards this goal, our first contribution is Picasso, a physics-constrained reconstruction pipeline that builds multi-object scene reconstructions by considering geometry, non-penetration, and physics. Picasso relies on a fast rejection sampling method that reasons over multi-object interactions, leveraging an inferred object contact graph to guide samples. Second, we propose the Picasso dataset, a collection of 10 contact-rich real-world scenes with ground truth annotations, as well as a metric to quantify physical plausibility, which we open-source as part of our benchmark. Finally, we provide an extensive evaluation of Picasso on our newly introduced dataset and on the YCB-V dataset, and show it largely outperforms the state of the art while providing reconstructions that are both physically plausible and more aligned with human intuition.

</details>


### [602] [DICE: Disentangling Artist Style from Content via Contrastive Subspace Decomposition in Diffusion Models](https://arxiv.org/abs/2602.08059)
*Tong Zhang,Ru Zhang,Jianyi Liu*

Main category: cs.CV

TL;DR: 提出训练无关框架DICE实现即时艺术家风格擦除，平衡风格擦除和内容完整性，高效抑制风格模仿。


<details>
  <summary>Details</summary>
Motivation: 扩散模型使风格模仿容易，带来版权和知识产权风险，现有对策实用性有限，需可靠保护方法。

Method: 构建对比三元组，将风格和非风格特征在潜在空间分离，将解缠过程形式化为广义特征值问题识别风格子空间，引入自适应注意力解耦编辑策略。

Result: DICE在风格擦除彻底性和内容完整性保存上取得良好平衡，解缠风格仅增加3秒额外开销。

Conclusion: DICE是一种实用高效的抑制风格模仿的技术。

Abstract: The recent proliferation of diffusion models has made style mimicry effortless, enabling users to imitate unique artistic styles without authorization. In deployed platforms, this raises copyright and intellectual-property risks and calls for reliable protection. However, existing countermeasures either require costly weight editing as new styles emerge or rely on an explicitly specified editing style, limiting their practicality for deployment-side safety. To address this challenge, we propose DICE (Disentanglement of artist Style from Content via Contrastive Subspace Decomposition), a training-free framework for on-the-fly artist style erasure. Unlike style editing that require an explicitly specified replacement style, DICE performs style purification, removing the artist's characteristics while preserving the user-intended content. Our core insight is that a model cannot truly comprehend the artist style from a single text or image alone. Consequently, we abandon the traditional paradigm of identifying style from isolated samples. Instead, we construct contrastive triplets to compel the model to distinguish between style and non-style features in the latent space. By formalizing this disentanglement process as a solvable generalized eigenvalue problem, we achieve precise identification of the style subspace. Furthermore, we introduce an Adaptive Attention Decoupling Editing strategy dynamically assesses the style concentration of each token and performs differential suppression and content enhancement on the QKV vectors. Extensive experiments demonstrate that DICE achieves a superior balance between the thoroughness of style erasure and the preservation of content integrity. DICE introduces an additional overhead of only 3 seconds to disentangle style, providing a practical and efficient technique for curbing style mimicry.

</details>


### [603] [Optimizing Few-Step Generation with Adaptive Matching Distillation](https://arxiv.org/abs/2602.07345)
*Lichen Bai,Zikai Zhou,Shitong Shao,Wenliang Zhong,Shuo Yang,Shuo Chen,Bojun Chen,Zeke Xie*

Main category: cs.CV

TL;DR: 提出自适应匹配蒸馏（AMD）解决DMD在禁区的稳定性问题，实验证明AMD提升样本保真度和训练鲁棒性。


<details>
  <summary>Details</summary>
Motivation: Distribution Matching Distillation（DMD）在禁区稳定性差，需要解决该问题以提升生成模型性能。

Method: 提出统一优化框架，引入自适应匹配蒸馏（AMD），利用奖励代理检测和逃离禁区，通过结构信号分解动态调整梯度，引入排斥景观锐化防止失败模式崩溃。

Result: 在图像和视频生成任务及基准测试中，AMD显著提升样本保真度和训练鲁棒性，如将SDXL的HPSv2分数从30.64提高到31.25。

Conclusion: 明确纠正禁区内的优化轨迹对提升少步生成模型性能至关重要。

Abstract: Distribution Matching Distillation (DMD) is a powerful acceleration paradigm, yet its stability is often compromised in Forbidden Zone, regions where the real teacher provides unreliable guidance while the fake teacher exerts insufficient repulsive force. In this work, we propose a unified optimization framework that reinterprets prior art as implicit strategies to avoid these corrupted regions. Based on this insight, we introduce Adaptive Matching Distillation (AMD), a self-correcting mechanism that utilizes reward proxies to explicitly detect and escape Forbidden Zones. AMD dynamically prioritizes corrective gradients via structural signal decomposition and introduces Repulsive Landscape Sharpening to enforce steep energy barriers against failure mode collapse. Extensive experiments across image and video generation tasks (e.g., SDXL, Wan2.1) and rigorous benchmarks (e.g., VBench, GenEval) demonstrate that AMD significantly enhances sample fidelity and training robustness. For instance, AMD improves the HPSv2 score on SDXL from 30.64 to 31.25, outperforming state-of-the-art baselines. These findings validate that explicitly rectifying optimization trajectories within Forbidden Zones is essential for pushing the performance ceiling of few-step generative models.

</details>


### [604] [VidVec: Unlocking Video MLLM Embeddings for Video-Text Retrieval](https://arxiv.org/abs/2602.08099)
*Issar Tzachor,Dvir Samuel,Rami Ben-Ari*

Main category: cs.CV

TL;DR: 本文聚焦用多模态大语言模型（MLLMs）进行视频文本嵌入和检索，提出轻量级基于文本的对齐策略，无需微调超越当前方法。


<details>
  <summary>Details</summary>
Motivation: 现有将MLLMs用于视觉任务的方法在视频上表现不如视频基础模型（VFMs），因此希望利用MLLMs进行视频文本嵌入和检索。

Method: 先进行系统的逐层分析，将中间层嵌入与校准的MLLM头部结合；引入轻量级基于文本的对齐策略，将密集视频字幕映射为简短摘要。

Result: 无需训练就有很强的零样本检索性能；无需视觉监督实现与任务相关的视频文本嵌入学习；在多个视频检索基准上取得了最先进的结果。

Conclusion: 无需文本外的任何微调，本文方法显著优于当前方法。

Abstract: Recent studies have adapted generative Multimodal Large Language Models (MLLMs) into embedding extractors for vision tasks, typically through fine-tuning to produce universal representations. However, their performance on video remains inferior to Video Foundation Models (VFMs). In this paper, we focus on leveraging MLLMs for video-text embedding and retrieval. We first conduct a systematic layer-wise analysis, showing that intermediate (pre-trained) MLLM layers already encode substantial task-relevant information. Leveraging this insight, we demonstrate that combining intermediate-layer embeddings with a calibrated MLLM head yields strong zero-shot retrieval performance without any training. Building on these findings, we introduce a lightweight text-based alignment strategy which maps dense video captions to short summaries and enables task-related video-text embedding learning without visual supervision. Remarkably, without any fine-tuning beyond text, our method outperforms current methods, often by a substantial margin, achieving state-of-the-art results across common video retrieval benchmarks.

</details>


### [605] [Evaluating Object-Centric Models beyond Object Discovery](https://arxiv.org/abs/2602.07532)
*Krishnakant Singh,Simone Schaub-Meyer,Stefan Roth*

Main category: cs.CV

TL;DR: 现有以对象为中心学习（OCL）模型评估有局限，本文提出用指令微调的视觉语言模型评估及统一评估任务和指标。


<details>
  <summary>Details</summary>
Motivation: 现有OCL模型评估多关注对象发现和简单推理任务，现有基准存在对模型表征有用性洞察有限、定位和表征有用性用分离指标评估的局限。

Method: 用指令微调的视觉语言模型作为评估器，跨不同视觉问答数据集进行可扩展基准测试；引入统一评估任务和指标联合评估定位和表征有用性；设置简单多特征重建基线作为参考。

Result: 未提及具体结果

Conclusion: 未提及具体结论

Abstract: Object-centric learning (OCL) aims to learn structured scene representations that support compositional generalization and robustness to out-of-distribution (OOD) data. However, OCL models are often not evaluated regarding these goals. Instead, most prior work focuses on evaluating OCL models solely through object discovery and simple reasoning tasks, such as probing the representation via image classification. We identify two limitations in existing benchmarks: (1) They provide limited insights on the representation usefulness of OCL models, and (2) localization and representation usefulness are assessed using disjoint metrics. To address (1), we use instruction-tuned VLMs as evaluators, enabling scalable benchmarking across diverse VQA datasets to measure how well VLMs leverage OCL representations for complex reasoning tasks. To address (2), we introduce a unified evaluation task and metric that jointly assess localization (where) and representation usefulness (what), thereby eliminating inconsistencies introduced by disjoint evaluation. Finally, we include a simple multi-feature reconstruction baseline as a reference point.

</details>


### [606] [LLM-Guided Diagnostic Evidence Alignment for Medical Vision-Language Pretraining under Limited Pairing](https://arxiv.org/abs/2602.07540)
*Huimin Yan,Liang Bai,Xian Yang,Long Chen*

Main category: cs.CV

TL;DR: 针对现有CLIP式医学跨模态预训练方法的问题，提出LLM引导的诊断证据对齐方法LGDEA，实验证明有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有CLIP风格医学视觉语言预训练方法依靠全局或局部对齐，难学可靠诊断表示，且依赖大量配对数据，在配对数据有限场景适用性受限。

Method: 提出LGDEA方法，利用LLM从放射报告中提取关键诊断证据，构建共享诊断证据空间，实现证据感知的跨模态对齐。

Result: 在短语定位、图像文本检索和零样本分类任务上取得持续显著改进，可与依赖大量配对数据的预训练方法相媲美。

Conclusion: LGDEA方法能有效利用大量未配对医学图像和报告，减轻对配对数据的依赖，提升性能。

Abstract: Most existing CLIP-style medical vision--language pretraining methods rely on global or local alignment with substantial paired data. However, global alignment is easily dominated by non-diagnostic information, while local alignment fails to integrate key diagnostic evidence. As a result, learning reliable diagnostic representations becomes difficult, which limits their applicability in medical scenarios with limited paired data. To address this issue, we propose an LLM-Guided Diagnostic Evidence Alignment method (LGDEA), which shifts the pretraining objective toward evidence-level alignment that is more consistent with the medical diagnostic process. Specifically, we leverage LLMs to extract key diagnostic evidence from radiology reports and construct a shared diagnostic evidence space, enabling evidence-aware cross-modal alignment and allowing LGDEA to effectively exploit abundant unpaired medical images and reports, thereby substantially alleviating the reliance on paired data. Extensive experimental results demonstrate that our method achieves consistent and significant improvements on phrase grounding, image--text retrieval, and zero-shot classification, and even rivals pretraining methods that rely on substantial paired data.

</details>


### [607] [Robustness of Vision Language Models Against Split-Image Harmful Input Attacks](https://arxiv.org/abs/2602.08136)
*Md Rafi Ur Rashid,MD Sadik Hossain Shanto,Vishnu Asutosh Dasu,Shagufta Mehnaz*

Main category: cs.CV

TL;DR: 本文指出视觉语言模型（VLM）在处理分割图像输入时存在安全对齐漏洞，提出新型分割图像视觉越狱攻击（SIVA），并给出应对方法。


<details>
  <summary>Details</summary>
Motivation: 现有VLM对单张或整体图像的越狱攻击有较强鲁棒性，但安全对齐通常仅在整体图像上进行，未考虑分割图像输入的有害语义，存在漏洞。

Method: 提出SIVA攻击，攻击分阶段进行，从简单分割到自适应白盒攻击，最终实现黑盒转移攻击，利用对抗知识蒸馏（Adv - KD）算法提高跨模型可转移性。

Result: 在三个最先进的VLM和三个越狱数据集上评估，最强攻击比现有基线的转移成功率高60%。

Conclusion: 指出VLM当前安全对齐存在关键漏洞，并提出有效应对方法。

Abstract: Vision-Language Models (VLMs) are now a core part of modern AI. Recent work proposed several visual jailbreak attacks using single/ holistic images. However, contemporary VLMs demonstrate strong robustness against such attacks due to extensive safety alignment through preference optimization (e.g., RLHF). In this work, we identify a new vulnerability: while VLM pretraining and instruction tuning generalize well to split-image inputs, safety alignment is typically performed only on holistic images and does not account for harmful semantics distributed across multiple image fragments. Consequently, VLMs often fail to detect and refuse harmful split-image inputs, where unsafe cues emerge only after combining images. We introduce novel split-image visual jailbreak attacks (SIVA) that exploit this misalignment. Unlike prior optimization-based attacks, which exhibit poor black-box transferability due to architectural and prior mismatches across models, our attacks evolve in progressive phases from naive splitting to an adaptive white-box attack, culminating in a black-box transfer attack. Our strongest strategy leverages a novel adversarial knowledge distillation (Adv-KD) algorithm to substantially improve cross-model transferability. Evaluations on three state-of-the-art modern VLMs and three jailbreak datasets demonstrate that our strongest attack achieves up to 60% higher transfer success than existing baselines. Lastly, we propose efficient ways to address this critical vulnerability in the current VLM safety alignment.

</details>


### [608] [Generating Adversarial Events: A Motion-Aware Point Cloud Framework](https://arxiv.org/abs/2602.08230)
*Hongwei Ren,Youxin Jiang,Qifei Gu,Xiangqian Wu*

Main category: cs.CV

TL;DR: 本文提出MA - ADV框架用于生成事件对抗样本，实验表明其攻击成功率达100%且扰动成本低，凸显事件感知系统安全挑战。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络易受对抗样本攻击威胁基于事件系统可靠性，且事件对抗攻击研究稀缺，主流事件表示不可微阻碍梯度攻击方法扩展。

Method: 提出MA - ADV框架，利用点云表示生成对抗事件，考虑高频噪声，用扩散方法平滑扰动，结合样本Adam优化、迭代细化和二分查找确定最小成本扰动。

Result: 实验验证MA - ADV确保100%攻击成功率，扰动成本最小，对防御有更强鲁棒性。

Conclusion: 未来基于事件的感知系统面临严峻安全挑战。

Abstract: Event cameras have been widely adopted in safety-critical domains such as autonomous driving, robotics, and human-computer interaction. A pressing challenge arises from the vulnerability of deep neural networks to adversarial examples, which poses a significant threat to the reliability of event-based systems. Nevertheless, research into adversarial attacks on events is scarce. This is primarily due to the non-differentiable nature of mainstream event representations, which hinders the extension of gradient-based attack methods. In this paper, we propose MA-ADV, a novel \textbf{M}otion-\textbf{A}ware \textbf{Adv}ersarial framework. To the best of our knowledge, this is the first work to generate adversarial events by leveraging point cloud representations. MA-ADV accounts for high-frequency noise in events and employs a diffusion-based approach to smooth perturbations, while fully leveraging the spatial and temporal relationships among events. Finally, MA-ADV identifies the minimal-cost perturbation through a combination of sample-wise Adam optimization, iterative refinement, and binary search. Extensive experimental results validate that MA-ADV ensures a 100\% attack success rate with minimal perturbation cost, and also demonstrate enhanced robustness against defenses, underscoring the critical security challenges facing future event-based perception systems.

</details>


### [609] [When and How Much to Imagine: Adaptive Test-Time Scaling with World Models for Visual Spatial Reasoning](https://arxiv.org/abs/2602.08236)
*Shoubin Yu,Yue Zhang,Zun Wang,Jaehong Yoon,Huaxiu Yao,Mingyu Ding,Mohit Bansal*

Main category: cs.CV

TL;DR: 本文深入分析视觉想象在空间推理中的作用并提出自适应框架，证明选择性控制视觉想象对高效可靠空间推理的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型在视觉空间推理表现不佳，对视觉想象的必要性、有益程度及有害情况缺乏了解，且无差别想象会增计算量与降低性能。

Method: 对测试时视觉想象作为可控资源进行深入分析，引入AVIC自适应测试框架，在选择性调用和扩展视觉想象前明确推理当前视觉证据的充分性。

Result: 在多个基准测试中揭示了想象关键、有一定作用或有害的场景，选择性控制能以更少调用和语言标记实现或超越固定想象策略。

Conclusion: 分析和控制测试时的想象对高效可靠的空间推理至关重要。

Abstract: Despite rapid progress in Multimodal Large Language Models (MLLMs), visual spatial reasoning remains unreliable when correct answers depend on how a scene would appear under unseen or alternative viewpoints. Recent work addresses this by augmenting reasoning with world models for visual imagination, but questions such as when imagination is actually necessary, how much of it is beneficial, and when it becomes harmful, remain poorly understood. In practice, indiscriminate imagination can increase computation and even degrade performance by introducing misleading evidence. In this work, we present an in-depth analysis of test-time visual imagination as a controllable resource for spatial reasoning. We study when static visual evidence is sufficient, when imagination improves reasoning, and how excessive or unnecessary imagination affects accuracy and efficiency. To support this analysis, we introduce AVIC, an adaptive test-time framework with world models that explicitly reasons about the sufficiency of current visual evidence before selectively invoking and scaling visual imagination. Across spatial reasoning benchmarks (SAT, MMSI) and an embodied navigation benchmark (R2R), our results reveal clear scenarios where imagination is critical, marginal, or detrimental, and show that selective control can match or outperform fixed imagination strategies with substantially fewer world-model calls and language tokens. Overall, our findings highlight the importance of analyzing and controlling test-time imagination for efficient and reliable spatial reasoning.

</details>


### [610] [PISCO: Precise Video Instance Insertion with Sparse Control](https://arxiv.org/abs/2602.08277)
*Xiangbo Gao,Renjie Li,Xinghao Chen,Yuheng Wu,Suofei Feng,Qing Yin,Zhengzhong Tu*

Main category: cs.CV

TL;DR: AI视频生成正从通用生成转向细粒度可控生成与高保真后处理，本文提出PISCO模型用于精确视频实例插入，实验显示其表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 在专业AI辅助电影制作中，需要精确、有针对性的修改，视频实例插入很关键，但有严格要求，现有方法待改进。

Method: 提出PISCO模型，可通过任意稀疏关键帧控制进行精确视频实例插入；引入Variable-Information Guidance、Distribution-Preserving Temporal Masking和geometry-aware conditioning；构建PISCO-Bench基准并使用多种指标评估。

Result: 实验表明，PISCO在稀疏控制下始终优于强大的修复和视频编辑基线方法，且随着控制信号增加，性能有明显单调提升。

Conclusion: PISCO模型在精确视频实例插入方面表现出色，为AI视频生成的精确修改提供了有效解决方案。

Abstract: The landscape of AI video generation is undergoing a pivotal shift: moving beyond general generation - which relies on exhaustive prompt-engineering and "cherry-picking" - towards fine-grained, controllable generation and high-fidelity post-processing. In professional AI-assisted filmmaking, it is crucial to perform precise, targeted modifications. A cornerstone of this transition is video instance insertion, which requires inserting a specific instance into existing footage while maintaining scene integrity. Unlike traditional video editing, this task demands several requirements: precise spatial-temporal placement, physically consistent scene interaction, and the faithful preservation of original dynamics - all achieved under minimal user effort. In this paper, we propose PISCO, a video diffusion model for precise video instance insertion with arbitrary sparse keyframe control. PISCO allows users to specify a single keyframe, start-and-end keyframes, or sparse keyframes at arbitrary timestamps, and automatically propagates object appearance, motion, and interaction. To address the severe distribution shift induced by sparse conditioning in pretrained video diffusion models, we introduce Variable-Information Guidance for robust conditioning and Distribution-Preserving Temporal Masking to stabilize temporal generation, together with geometry-aware conditioning for realistic scene adaptation. We further construct PISCO-Bench, a benchmark with verified instance annotations and paired clean background videos, and evaluate performance using both reference-based and reference-free perceptual metrics. Experiments demonstrate that PISCO consistently outperforms strong inpainting and video editing baselines under sparse control, and exhibits clear, monotonic performance improvements as additional control signals are provided. Project page: xiangbogaobarry.github.io/PISCO.

</details>


### [611] [Tighnari v2: Mitigating Label Noise and Distribution Shift in Multimodal Plant Distribution Prediction via Mixture of Experts and Weakly Supervised Learning](https://arxiv.org/abs/2602.08282)
*Haixu Liu,Yufei Wang,Tianxiang Xu,Chuancheng Shi,Hongsheng Xing*

Main category: cs.CV

TL;DR: 本文提出多模态融合框架解决植物分布预测中数据问题，在GeoLifeCLEF 2025数据集实验中表现优异。


<details>
  <summary>Details</summary>
Motivation: 大规模跨物种植物分布预测因观测数据稀疏和偏差面临挑战，PA数据获取成本高、数量有限，PO数据负样本标签噪声严重。

Method: 提出多模态融合框架，为PO数据引入伪标签聚合策略，采用多种模型处理不同数据，利用专家混合范式处理测试样本。

Result: 实验表明该方法在PA覆盖有限和分布偏移明显的场景中取得了优异的预测性能。

Conclusion: 所提方法能有效解决植物分布预测中数据相关问题，提升预测性能。

Abstract: Large-scale, cross-species plant distribution prediction plays a crucial role in biodiversity conservation, yet modeling efforts in this area still face significant challenges due to the sparsity and bias of observational data. Presence-Absence (PA) data provide accurate and noise-free labels, but are costly to obtain and limited in quantity; Presence-Only (PO) data, by contrast, offer broad spatial coverage and rich spatiotemporal distribution, but suffer from severe label noise in negative samples. To address these real-world constraints, this paper proposes a multimodal fusion framework that fully leverages the strengths of both PA and PO data. We introduce an innovative pseudo-label aggregation strategy for PO data based on the geographic coverage of satellite imagery, enabling geographic alignment between the label space and remote sensing feature space. In terms of model architecture, we adopt Swin Transformer Base as the backbone for satellite imagery, utilize the TabM network for tabular feature extraction, retain the Temporal Swin Transformer for time-series modeling, and employ a stackable serial tri-modal cross-attention mechanism to optimize the fusion of heterogeneous modalities. Furthermore, empirical analysis reveals significant geographic distribution shifts between PA training and test samples, and models trained by directly mixing PO and PA data tend to experience performance degradation due to label noise in PO data. To address this, we draw on the mixture-of-experts paradigm: test samples are partitioned according to their spatial proximity to PA samples, and different models trained on distinct datasets are used for inference and post-processing within each partition. Experiments on the GeoLifeCLEF 2025 dataset demonstrate that our approach achieves superior predictive performance in scenarios with limited PA coverage and pronounced distribution shifts.

</details>


### [612] [MMLSv2: A Multimodal Dataset for Martian Landslide Detection in Remote Sensing Imagery](https://arxiv.org/abs/2602.08112)
*Sidike Paheding,Abel Reyes-Angulo,Leo Thomas Ramos,Angel D. Sappa,Rajaneesh A.,Hiral P. B.,Sajin Kumar K. S.,Thomas Oommen*

Main category: cs.CV

TL;DR: 本文介绍火星表面滑坡分割数据集MMLSv2，含多模态图像，实验表明其支持稳定训练，但在特殊滑坡区域有挑战，孤立测试集可评估模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 提供用于火星表面滑坡分割的数据集，评估模型在不同场景下的性能和泛化能力。

Method: 构建包含多模态图像的MMLSv2数据集，进行多分割模型实验，使用孤立测试集评估。

Result: 数据集支持稳定训练，取得有竞争力性能，在特殊滑坡区域有挑战，孤立测试集使性能下降。

Conclusion: MMLSv2数据集有价值，孤立测试集对评估模型鲁棒性和泛化能力有重要意义。

Abstract: We present MMLSv2, a dataset for landslide segmentation on Martian surfaces. MMLSv2 consists of multimodal imagery with seven bands: RGB, digital elevation model, slope, thermal inertia, and grayscale channels. MMLSv2 comprises 664 images distributed across training, validation, and test splits. In addition, an isolated test set of 276 images from a geographically disjoint region from the base dataset is released to evaluate spatial generalization. Experiments conducted with multiple segmentation models show that the dataset supports stable training and achieves competitive performance, while still posing challenges in fragmented, elongated, and small-scale landslide regions. Evaluation on the isolated test set leads to a noticeable performance drop, indicating increased difficulty and highlighting its value for assessing model robustness and generalization beyond standard in-distribution settings. Dataset will be available at: https://github.com/MAIN-Lab/MMLS_v2

</details>


### [613] [UrbanGraphEmbeddings: Learning and Evaluating Spatially Grounded Multimodal Embeddings for Urban Science](https://arxiv.org/abs/2602.08342)
*Jie Zhang,Xingtong Yu,Yuan Fang,Rudi Stouffs,Zdravko Trivic*

Main category: cs.CV

TL;DR: 本文引入UGData数据集，提出UGE训练策略和UGBench基准，在多VLM主干上开发UGE，取得良好效果。


<details>
  <summary>Details</summary>
Motivation: 城市理解具空间性，但现有数据集和基准缺乏街景图像与城市结构的明确对齐，难以学习可迁移的多模态嵌入。

Method: 引入UGData数据集提供基于空间推理路径和上下文的图对齐监督；提出UGE两阶段训练策略结合引导式对比学习和图空间编码；引入UGBench基准评估跨模态嵌入支持的城市理解任务；在多个VLM主干上开发UGE并使用LoRA微调。

Result: 基于Qwen2.5 - VL - 7B主干的UGE在训练城市的图像检索和地理位置排名上分别提升44%和30%，在未参与训练城市分别提升超30%和22%。

Conclusion: 显式空间锚定对空间密集型城市任务有效。

Abstract: Learning transferable multimodal embeddings for urban environments is challenging because urban understanding is inherently spatial, yet existing datasets and benchmarks lack explicit alignment between street-view images and urban structure. We introduce UGData, a spatially grounded dataset that anchors street-view images to structured spatial graphs and provides graph-aligned supervision via spatial reasoning paths and spatial context captions, exposing distance, directionality, connectivity, and neighborhood context beyond image content. Building on UGData, we propose UGE, a two-stage training strategy that progressively and stably aligns images, text, and spatial structures by combining instruction-guided contrastive learning with graph-based spatial encoding. We finally introduce UGBench, a comprehensive benchmark to evaluate how spatially grounded embeddings support diverse urban understanding tasks -- including geolocation ranking, image retrieval, urban perception, and spatial grounding. We develop UGE on multiple state-of-the-art VLM backbones, including Qwen2-VL, Qwen2.5-VL, Phi-3-Vision, and LLaVA1.6-Mistral, and train fixed-dimensional spatial embeddings with LoRA tuning. UGE built upon Qwen2.5-VL-7B backbone achieves up to 44% improvement in image retrieval and 30% in geolocation ranking on training cities, and over 30% and 22% gains respectively on held-out cities, demonstrating the effectiveness of explicit spatial grounding for spatially intensive urban tasks.

</details>


### [614] [Vista: Scene-Aware Optimization for Streaming Video Question Answering under Post-Hoc Queries](https://arxiv.org/abs/2602.08448)
*Haocheng Lu,Nan Zhang,Wei Tao,Xiaoyang Qu,Guokuan Li,Jiguang Wan,Jianzong Wang*

Main category: cs.CV

TL;DR: 提出Vista框架用于场景感知流式视频问答，有场景感知分割、压缩和召回三方面创新，实验表现达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有流式视频问答解决方案在长视频实时场景中因上下文丢失或内存溢出而效果受限。

Method: 提出Vista框架，包括场景感知分割、压缩和召回，且与多种视觉语言骨干网络无缝集成。

Result: 在StreamingBench上的广泛实验中，Vista达到了最先进的性能。

Conclusion: Vista为现实世界的流式视频理解建立了强大的基线。

Abstract: Streaming video question answering (Streaming Video QA) poses distinct challenges for multimodal large language models (MLLMs), as video frames arrive sequentially and user queries can be issued at arbitrary time points. Existing solutions relying on fixed-size memory or naive compression often suffer from context loss or memory overflow, limiting their effectiveness in long-form, real-time scenarios. We present Vista, a novel framework for scene-aware streaming video QA that enables efficient and scalable reasoning over continuous video streams. The innovation of Vista can be summarized in three aspects: (1) scene-aware segmentation, where Vista dynamically clusters incoming frames into temporally and visually coherent scene units; (2) scene-aware compression, where each scene is compressed into a compact token representation and stored in GPU memory for efficient index-based retrieval, while full-resolution frames are offloaded to CPU memory; and (3) scene-aware recall, where relevant scenes are selectively recalled and reintegrated into the model input upon receiving a query, enabling both efficiency and completeness. Vista is model-agnostic and integrates seamlessly with a variety of vision-language backbones, enabling long-context reasoning without compromising latency or memory efficiency. Extensive experiments on StreamingBench demonstrate that Vista achieves state-of-the-art performance, establishing a strong baseline for real-world streaming video understanding.

</details>


### [615] [Gesture Matters: Pedestrian Gesture Recognition for AVs Through Skeleton Pose Evaluation](https://arxiv.org/abs/2602.08479)
*Alif Rizqullah Mahdi,Mahdi Rezaei,Natasha Merat*

Main category: cs.CV

TL;DR: 提出手势分类框架用于自动驾驶车辆解读行人手势，准确率达87%，提升感知能力并助于理解行人行为。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆难以解读行人手势，正式交通规则在行人与驾驶员互动时可能不足。

Method: 使用2D姿态估计对WIVW数据集的真实视频序列进行处理，将手势分为四类，从归一化关键点提取76个静态和动态特征。

Result: 手部位置和移动速度在区分手势类别上有特别的辨别力，分类准确率达87%。

Conclusion: 研究结果提升了自动驾驶系统的感知能力，有助于更广泛地理解交通场景中的行人行为。

Abstract: Gestures are a key component of non-verbal communication in traffic, often helping pedestrian-to-driver interactions when formal traffic rules may be insufficient. This problem becomes more apparent when autonomous vehicles (AVs) struggle to interpret such gestures. In this study, we present a gesture classification framework using 2D pose estimation applied to real-world video sequences from the WIVW dataset. We categorise gestures into four primary classes (Stop, Go, Thank & Greet, and No Gesture) and extract 76 static and dynamic features from normalised keypoints. Our analysis demonstrates that hand position and movement velocity are especially discriminative in distinguishing between gesture classes, achieving a classification accuracy score of 87%. These findings not only improve the perceptual capabilities of AV systems but also contribute to the broader understanding of pedestrian behaviour in traffic contexts.

</details>


### [616] [Enhanced Food Category Recognition under Illumination-Induced Domain Shift](https://arxiv.org/abs/2602.08491)
*Keonvin Park,Aditya Pal,Jin Hong Mok*

Main category: cs.CV

TL;DR: 本文研究多类别食物识别中光照引起的领域偏移问题，构建合成光照增强数据集，实验表明光照感知增强可提升识别鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现实环境中视觉食物识别系统对光照变化引起的领域偏移敏感，现有工作有局限且公共数据集缺乏光照标注。

Method: 使用Food - 101和Fruits - 360数据集，构建合成光照增强数据集，评估跨数据集迁移学习和领域泛化。

Result: 光照感知增强显著提高领域偏移下的识别鲁棒性并保留实时性能。

Conclusion: 强调光照鲁棒性的重要性，为现实检查场景部署可靠食物识别系统提供实用见解。

Abstract: Visual food recognition systems deployed in real-world environments, such as automated conveyor-belt inspection, are highly sensitive to domain shifts caused by illumination changes. While recent studies have shown that lighting variations can significantly distort food perception by both humans and AI, existing works are often limited to single food categories or controlled settings, and most public food datasets lack explicit illumination annotations.
  In this work, we investigate illumination-induced domain shift in multi-class food category recognition using two widely adopted datasets, Food-101 and Fruits-360. We demonstrate substantial accuracy degradation under cross-dataset evaluation due to mismatched visual conditions. To address this challenge, we construct synthetic illumination-augmented datasets by systematically varying light temperature and intensity, enabling controlled robustness analysis without additional labels.
  We further evaluate cross-dataset transfer learning and domain generalization, with a focus on illumination-sensitive target categories such as apple-based classes. Experimental results show that illumination-aware augmentation significantly improves recognition robustness under domain shift while preserving real-time performance. Our findings highlight the importance of illumination robustness and provide practical insights for deploying reliable food recognition systems in real-world inspection scenarios.

</details>


### [617] [GOT-Edit: Geometry-Aware Generic Object Tracking via Online Model Editing](https://arxiv.org/abs/2602.08550)
*Shih-Fang Chen,Jun-Cheng Chen,I-Hong Jhuo,Yen-Yu Lin*

Main category: cs.CV

TL;DR: 提出GOT - Edit在线跨模态模型编辑方法，将几何感知线索融入通用目标跟踪器，实验证明其在多场景下性能优越。


<details>
  <summary>Details</summary>
Motivation: 多数通用目标跟踪方法依赖2D特征，忽略3D几何线索，易受遮挡、干扰等影响，需改进。

Method: 引入GOT - Edit方法，利用预训练模型特征进行几何线索推理，通过零空间约束更新进行在线模型编辑。

Result: 在多个通用目标跟踪基准测试中，GOT - Edit表现出更好的鲁棒性和准确性，尤其在遮挡和杂乱场景下。

Conclusion: GOT - Edit为通用目标跟踪中2D语义与3D几何推理结合建立了新范式。

Abstract: Human perception for effective object tracking in a 2D video stream arises from the implicit use of prior 3D knowledge combined with semantic reasoning. In contrast, most generic object tracking (GOT) methods primarily rely on 2D features of the target and its surroundings while neglecting 3D geometric cues, which makes them susceptible to partial occlusion, distractors, and variations in geometry and appearance. To address this limitation, we introduce GOT-Edit, an online cross-modality model editing approach that integrates geometry-aware cues into a generic object tracker from a 2D video stream. Our approach leverages features from a pre-trained Visual Geometry Grounded Transformer to enable geometric cue inference from only a few 2D images. To tackle the challenge of seamlessly combining geometry and semantics, GOT-Edit performs online model editing with null-space constrained updates that incorporate geometric information while preserving semantic discrimination, yielding consistently better performance across diverse scenarios. Extensive experiments on multiple GOT benchmarks demonstrate that GOT-Edit achieves superior robustness and accuracy, particularly under occlusion and clutter, establishing a new paradigm for combining 2D semantics with 3D geometric reasoning for generic object tracking.

</details>


### [618] [Learning Self-Correction in Vision-Language Models via Rollout Augmentation](https://arxiv.org/abs/2602.08503)
*Yi Ding,Ziliang Qiu,Bolian Li,Ruqi Zhang*

Main category: cs.CV

TL;DR: 提出Octopus框架和响应掩码策略，构建Octopus - 8B模型，在7个基准测试中取得开源VLM的最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在视觉语言模型中学习自我修正困难，学习信号极其稀疏。

Method: 提出Octopus强化学习滚动增强框架，合成密集自我修正示例；引入响应掩码策略解耦自我修正和直接推理。

Result: Octopus - 8B模型在7个基准测试中取得开源VLM的最优性能，每步训练时间仅为最佳RLVR基线的0.72倍。

Conclusion: 提出的方法能有效解决现有强化学习方法在视觉语言模型中学习自我修正的问题，提升模型性能。

Abstract: Self-correction is essential for solving complex reasoning problems in vision-language models (VLMs). However, existing reinforcement learning (RL) methods struggle to learn it, as effective self-correction behaviors emerge only rarely, making learning signals extremely sparse. To address this challenge, we propose correction-specific rollouts (Octopus), an RL rollout augmentation framework that synthesizes dense self-correction examples by recombining existing rollouts. This augmentation simultaneously improves sample efficiency due to rollout reuse and stabilizes RL optimization through balanced supervision. Furthermore, we introduce a response-masking strategy that decouples self-correction from direct reasoning, avoiding signal conflicts and enabling both behaviors to be learned effectively. Building on this, we introduce Octopus-8B, a reasoning VLM with controllable self-correction capability. Across 7 benchmarks, it achieves SoTA performance among open-source VLMs, outperforming the best RLVR baseline by 1.0 score while requiring only $0.72\times$ training time per step.

</details>


### [619] [Towards Understanding Multimodal Fine-Tuning: Spatial Features](https://arxiv.org/abs/2602.08713)
*Lachin Naghashyar,Hunar Batra,Ashkan Khakzar,Philip Torr,Ronald Clark,Christian Schroeder de Witt,Constantin Venhoff*

Main category: cs.CV

TL;DR: 对视觉语言模型（VLM）适应过程进行机制分析，揭示语言模型如何学习“看”，增强多模态训练可解释性。


<details>
  <summary>Details</summary>
Motivation: 不清楚语言骨干表征在多模态训练中的适应方式以及视觉特定能力的出现时间，需对VLM适应进行机制分析。

Method: 使用阶段式模型差异分析技术，识别微调中出现或重新定向的视觉偏好特征，通过控制空间提示揭示特征编码空间关系，追踪特征的因果激活。

Result: 阶段式模型差异分析揭示了空间多模态特征出现的时间和位置，展示了视觉基础如何重塑先前仅文本的特征。

Conclusion: 该方法增强了多模态训练的可解释性，为理解和改进预训练语言模型获得视觉基础能力提供了基础。

Abstract: Contemporary Vision-Language Models (VLMs) achieve strong performance on a wide range of tasks by pairing a vision encoder with a pre-trained language model, fine-tuned for visual-text inputs. Yet despite these gains, it remains unclear how language backbone representations adapt during multimodal training and when vision-specific capabilities emerge. In this work, we present the first mechanistic analysis of VLM adaptation. Using stage-wise model diffing, a technique that isolates representational changes introduced during multimodal fine-tuning, we reveal how a language model learns to "see". We first identify vision-preferring features that emerge or reorient during fine-tuning. We then show that a selective subset of these features reliably encodes spatial relations, revealed through controlled shifts to spatial prompts. Finally, we trace the causal activation of these features to a small group of attention heads. Our findings show that stage-wise model diffing reveals when and where spatially grounded multimodal features arise. It also provides a clearer view of modality fusion by showing how visual grounding reshapes features that were previously text-only. This methodology enhances the interpretability of multimodal training and provides a foundation for understanding and refining how pretrained language models acquire vision-grounded capabilities.

</details>


### [620] [Zero-shot System for Automatic Body Region Detection for Volumetric CT and MR Images](https://arxiv.org/abs/2602.08717)
*Farnaz Khun Jush,Grit Werner,Mark Klemens,Matthias Lenga*

Main category: cs.CV

TL;DR: 本文探索零样本方式进行体部CT和MR图像身体区域检测，评估三种免训练管道，分割驱动的规则方法表现最佳。


<details>
  <summary>Details</summary>
Motivation: 现有医学影像身体区域识别依赖不可靠DICOM元数据，且监督学习方法在现实场景应用受限。

Method: 提出并评估三种免训练管道，包括分割驱动规则系统、多模态大语言模型、分割感知多模态大语言模型。

Result: 分割驱动规则方法表现最强且稳定，CT加权F1分数0.947，MR为0.914；多模态大语言模型在视觉特征明显区域有竞争力；分割感知多模态大语言模型有局限性。

Conclusion: 分割驱动规则方法在身体区域检测中表现良好，具有跨模态和应对非典型扫描覆盖的鲁棒性。

Abstract: Reliable identification of anatomical body regions is a prerequisite for many automated medical imaging workflows, yet existing solutions remain heavily dependent on unreliable DICOM metadata. Current solutions mainly use supervised learning, which limits their applicability in many real-world scenarios. In this work, we investigate whether body region detection in volumetric CT and MR images can be achieved in a fully zero-shot manner by using knowledge embedded in large pre-trained foundation models. We propose and systematically evaluate three training-free pipelines: (1) a segmentation-driven rule-based system leveraging pre-trained multi-organ segmentation models, (2) a Multimodal Large Language Model (MLLM) guided by radiologist-defined rules, and (3) a segmentation-aware MLLM that combines visual input with explicit anatomical evidence. All methods are evaluated on 887 heterogeneous CT and MR scans with manually verified anatomical region labels. The segmentation-driven rule-based approach achieves the strongest and most consistent performance, with weighted F1-scores of 0.947 (CT) and 0.914 (MR), demonstrating robustness across modalities and atypical scan coverage. The MLLM performs competitively in visually distinctive regions, while the segmentation-aware MLLM reveals fundamental limitations.

</details>


### [621] [Multimodal Learning for Arcing Detection in Pantograph-Catenary Systems](https://arxiv.org/abs/2602.08792)
*Hao Dong,Eleni Chatzi,Olga Fink*

Main category: cs.CV

TL;DR: 本文提出结合高分辨率图像数据与力测量的多模态框架检测受电弓 - 接触网界面的电弧事件，表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 受电弓 - 接触网界面的电气电弧会带来严重风险，但检测电弧事件具有挑战性，需新方法。

Method: 构建包含同步视觉和力测量的两个电弧检测数据集；提出MultiDeepSAD算法；引入针对不同数据类型的伪异常生成技术。

Result: 框架显著优于基线方法，即使在域转移和真实电弧观测有限的情况下，对真实电弧事件有更高灵敏度。

Conclusion: 所提多模态框架能更准确和稳健地检测电弧事件。

Abstract: The pantograph-catenary interface is essential for ensuring uninterrupted and reliable power delivery in electrified rail systems. However, electrical arcing at this interface poses serious risks, including accelerated wear of contact components, degraded system performance, and potential service disruptions. Detecting arcing events at the pantograph-catenary interface is challenging due to their transient nature, noisy operating environment, data scarcity, and the difficulty of distinguishing arcs from other similar transient phenomena. To address these challenges, we propose a novel multimodal framework that combines high-resolution image data with force measurements to more accurately and robustly detect arcing events. First, we construct two arcing detection datasets comprising synchronized visual and force measurements. One dataset is built from data provided by the Swiss Federal Railways (SBB), and the other is derived from publicly available videos of arcing events in different railway systems and synthetic force data that mimic the characteristics observed in the real dataset. Leveraging these datasets, we propose MultiDeepSAD, an extension of the DeepSAD algorithm for multiple modalities with a new loss formulation. Additionally, we introduce tailored pseudo-anomaly generation techniques specific to each data type, such as synthetic arc-like artifacts in images and simulated force irregularities, to augment training data and improve the discriminative ability of the model. Through extensive experiments and ablation studies, we demonstrate that our framework significantly outperforms baseline approaches, exhibiting enhanced sensitivity to real arcing events even under domain shifts and limited availability of real arcing observations.

</details>


### [622] [Artifact Reduction in Undersampled 3D Cone-Beam CTs using a Hybrid 2D-3D CNN Framework](https://arxiv.org/abs/2602.08727)
*Johannes Thalhammer,Tina Dorosti,Sebastian Peterhansl,Daniela Pfeiffer,Franz Pfeiffer,Florian Schaff*

Main category: cs.CV

TL;DR: 提出计算高效的混合深度学习框架用于减少欠采样CT影像伪影，提升图像质量。


<details>
  <summary>Details</summary>
Motivation: 欠采样CT会引入伪影，降低图像质量和诊断价值，需减少伪影以实现高质量成像。

Method: 提出结合2D和3D模型优势的混合深度学习框架，先使用2D U - Net处理切片提取特征图，再堆叠输入3D解码器预测无伪影3D CT体积。

Result: 在冠、矢状方向上的层间一致性大幅改善，计算开销低。

Conclusion: 该混合框架为高质量3D CT图像后处理提供了稳健高效的解决方案。

Abstract: Undersampled CT volumes minimize acquisition time and radiation exposure but introduce artifacts degrading image quality and diagnostic utility. Reducing these artifacts is critical for high-quality imaging. We propose a computationally efficient hybrid deep-learning framework that combines the strengths of 2D and 3D models. First, a 2D U-Net operates on individual slices of undersampled CT volumes to extract feature maps. These slice-wise feature maps are then stacked across the volume and used as input to a 3D decoder, which utilizes contextual information across slices to predict an artifact-free 3D CT volume. The proposed two-stage approach balances the computational efficiency of 2D processing with the volumetric consistency provided by 3D modeling. The results show substantial improvements in inter-slice consistency in coronal and sagittal direction with low computational overhead. This hybrid framework presents a robust and efficient solution for high-quality 3D CT image post-processing. The code of this project can be found on github: https://github.com/J-3TO/2D-3DCNN_sparseview/.

</details>


### [623] [Analysis of Converged 3D Gaussian Splatting Solutions: Density Effects and Prediction Limit](https://arxiv.org/abs/2602.08909)
*Zhendong Wang,Cihan Ruan,Jingchuan Xiao,Chuqing Shi,Wei Jiang,Wei Wang,Wenjie Liu,Nam Ling*

Main category: cs.CV

TL;DR: 研究3D高斯溅射（3DGS）标准多视图优化解的结构，分析其统计特性，揭示基本密度分层，提出密度感知策略。


<details>
  <summary>Details</summary>
Motivation: 探究3DGS标准多视图优化解中出现的结构及其参数决定因素。

Method: 定义渲染最优参考（RORs）并分析其统计特性，应用可学习性探针训练预测器，通过方差分解进行形式化分析。

Result: 发现混合结构尺度和双峰辐射稳定模式，揭示基本密度分层，表明稀疏区域存在预测失败，以及可见性异质性导致的参数耦合。

Conclusion: RORs具有双重特性，提出密度感知策略提升训练鲁棒性并讨论架构影响。

Abstract: We investigate what structure emerges in 3D Gaussian Splatting (3DGS) solutions from standard multi-view optimization. We term these Rendering-Optimal References (RORs) and analyze their statistical properties, revealing stable patterns: mixture-structured scales and bimodal radiance across diverse scenes. To understand what determines these parameters, we apply learnability probes by training predictors to reconstruct RORs from point clouds without rendering supervision. Our analysis uncovers fundamental density-stratification. Dense regions exhibit geometry-correlated parameters amenable to render-free prediction, while sparse regions show systematic failure across architectures. We formalize this through variance decomposition, demonstrating that visibility heterogeneity creates covariance-dominated coupling between geometric and appearance parameters in sparse regions. This reveals the dual character of RORs: geometric primitives where point clouds suffice, and view synthesis primitives where multi-view constraints are essential. We provide density-aware strategies that improve training robustness and discuss architectural implications for systems that adaptively balance feed-forward prediction and rendering-based refinement.

</details>


### [624] [Addressing data annotation scarcity in Brain Tumor Segmentation on 3D MRI scan Using a Semi-Supervised Teacher-Student Framework](https://arxiv.org/abs/2602.08797)
*Jiaming Liu,Cheng Ding,Daoqiang Zhang*

Main category: cs.CV

TL;DR: 提出半监督师生框架用于脑肿瘤MRI分割，在BraTS 2021上提高了分割性能，证明了在有限监督下方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 准确的脑肿瘤MRI分割受昂贵标注和数据异质性限制，需要更好的方法。

Method: 提出半监督师生框架，结合不确定性感知伪标签教师和基于置信度的渐进式课程，采用双损失目标训练学生网络，进行基于一致性的伪标签改进。

Result: 在BraTS 2021上，验证DSC从10%数据的0.393提高到100%数据的0.872，教师验证DSC达0.922，学生在肿瘤子区域表现更优。

Conclusion: 基于置信度的课程和选择性遗忘在有限监督和噪声伪标签下提供了鲁棒的分割方法。

Abstract: Accurate brain tumor segmentation from MRI is limited by expensive annotations and data heterogeneity across scanners and sites. We propose a semi-supervised teacher-student framework that combines an uncertainty-aware pseudo-labeling teacher with a progressive, confidence-based curriculum for the student. The teacher produces probabilistic masks and per-pixel uncertainty; unlabeled scans are ranked by image-level confidence and introduced in stages, while a dual-loss objective trains the student to learn from high-confidence regions and unlearn low-confidence ones. Agreement-based refinement further improves pseudo-label quality. On BraTS 2021, validation DSC increased from 0.393 (10% data) to 0.872 (100%), with the largest gains in early stages, demonstrating data efficiency. The teacher reached a validation DSC of 0.922, and the student surpassed the teacher on tumor subregions (e.g., NCR/NET 0.797 and Edema 0.980); notably, the student recovered the Enhancing class (DSC 0.620) where the teacher failed. These results show that confidence-driven curricula and selective unlearning provide robust segmentation under limited supervision and noisy pseudo-labels.

</details>


### [625] [MotionCrafter: Dense Geometry and Motion Reconstruction with a 4D VAE](https://arxiv.org/abs/2602.08961)
*Ruijie Zhu,Jiahao Lu,Wenbo Hu,Xiaoguang Han,Jianfei Cai,Ying Shan,Chuanxia Zheng*

Main category: cs.CV

TL;DR: 提出基于视频扩散的MotionCrafter框架，联合重建4D几何并估计单目视频的密集运动，实验表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法强制3D值和潜在变量与RGB VAE潜在变量严格对齐，导致性能不佳，需改进。

Method: 采用共享坐标系中3D点图和3D场景流的联合表示及4D VAE，引入新的数据归一化和VAE训练策略。

Result: 在多个数据集实验中，在几何重建和密集场景流估计上达SOTA，几何和运动重建分别提升38.64%和25.0%，无需后优化。

Conclusion: MotionCrafter框架有效，新策略能更好转移扩散先验，提升重建质量。

Abstract: We introduce MotionCrafter, a video diffusion-based framework that jointly reconstructs 4D geometry and estimates dense motion from a monocular video. The core of our method is a novel joint representation of dense 3D point maps and 3D scene flows in a shared coordinate system, and a novel 4D VAE to effectively learn this representation. Unlike prior work that forces the 3D value and latents to align strictly with RGB VAE latents-despite their fundamentally different distributions-we show that such alignment is unnecessary and leads to suboptimal performance. Instead, we introduce a new data normalization and VAE training strategy that better transfers diffusion priors and greatly improves reconstruction quality. Extensive experiments across multiple datasets demonstrate that MotionCrafter achieves state-of-the-art performance in both geometry reconstruction and dense scene flow estimation, delivering 38.64% and 25.0% improvements in geometry and motion reconstruction, respectively, all without any post-optimization. Project page: https://ruijiezhu94.github.io/MotionCrafter_Page

</details>


### [626] [FlattenGPT: Depth Compression for Transformer with Layer Flattening](https://arxiv.org/abs/2602.08858)
*Ruihan Xu,Qingpei Guo,Yao Zhu,Xiangyang Ji,Ming Yang,Shiliang Zhang*

Main category: cs.CV

TL;DR: 本文提出FlattenGPT方法检测和减少Transformer深度冗余，实验表明其在性能和效率上取得平衡，优于现有剪枝方法。


<details>
  <summary>Details</summary>
Motivation: 现有整块剪枝和通道剪枝方法分别存在丢弃有意义信息、无法减少模型深度和剪枝率不一致等问题，需更好的模型压缩和加速方法。

Method: 提出FlattenGPT，将相邻两个块合并为一个以压缩网络深度，更有效地检测和去除参数冗余。

Result: 在多个模型上，FlattenGPT在零样本准确率和困惑度上优于现有剪枝方法，在压缩率20%时保留90 - 96%的零样本性能，加速大语言模型推理。

Conclusion: FlattenGPT在提升Transformer效率方面有潜力。

Abstract: Recent works have indicated redundancy across transformer blocks, prompting the research of depth compression to prune less crucial blocks. However, current ways of entire-block pruning suffer from risks of discarding meaningful cues learned in those blocks, leading to substantial performance degradation. As another line of model compression, channel pruning can better preserve performance, while it cannot reduce model depth and is challenged by inconsistent pruning ratios for individual layers. To pursue better model compression and acceleration, this paper proposes \textbf{FlattenGPT}, a novel way to detect and reduce depth-wise redundancies. By flatting two adjacent blocks into one, it compresses the network depth, meanwhile enables more effective parameter redundancy detection and removal. FlattenGPT allows to preserve the knowledge learned in all blocks, and remains consistent with the original transformer architecture. Extensive experiments demonstrate that FlattenGPT enhances model efficiency with a decent trade-off to performance. It outperforms existing pruning methods in both zero-shot accuracies and WikiText-2 perplexity across various model types and parameter sizes. On LLaMA-2/3 and Qwen-1.5 models, FlattenGPT retains 90-96\% of zero-shot performance with a compression ratio of 20\%. It also outperforms other pruning methods in accelerating LLM inference, making it promising for enhancing the efficiency of transformers.

</details>


### [627] [ArcFlow: Unleashing 2-Step Text-to-Image Generation via High-Precision Non-Linear Flow Distillation](https://arxiv.org/abs/2602.09014)
*Zihan Yang,Shuyuan Tu,Licheng Zhang,Qi Dai,Yu-Gang Jiang,Zuxuan Wu*

Main category: cs.CV

TL;DR: 现有扩散模型推理成本高，现有蒸馏方法有质量下降问题，提出ArcFlow框架，用非线性流轨迹近似教师轨迹，训练为少步生成器，速度提升且质量无显著下降。


<details>
  <summary>Details</summary>
Motivation: 扩散模型推理成本高，现有蒸馏方法用线性捷径近似教师轨迹导致质量下降。

Method: 提出ArcFlow框架，将推理轨迹的速度场参数化为连续动量过程的混合，通过轨迹蒸馏在预训练教师模型上使用轻量级适配器进行训练。

Result: 仅微调不到5%的原始参数，实现40倍速度提升，质量无显著下降，基准实验证明有效性。

Conclusion: ArcFlow能有效解决扩散模型推理成本高问题，在提升速度的同时保证生成质量。

Abstract: Diffusion models have achieved remarkable generation quality, but they suffer from significant inference cost due to their reliance on multiple sequential denoising steps, motivating recent efforts to distill this inference process into a few-step regime. However, existing distillation methods typically approximate the teacher trajectory by using linear shortcuts, which makes it difficult to match its constantly changing tangent directions as velocities evolve across timesteps, thereby leading to quality degradation. To address this limitation, we propose ArcFlow, a few-step distillation framework that explicitly employs non-linear flow trajectories to approximate pre-trained teacher trajectories. Concretely, ArcFlow parameterizes the velocity field underlying the inference trajectory as a mixture of continuous momentum processes. This enables ArcFlow to capture velocity evolution and extrapolate coherent velocities to form a continuous non-linear trajectory within each denoising step. Importantly, this parameterization admits an analytical integration of this non-linear trajectory, which circumvents numerical discretization errors and results in high-precision approximation of the teacher trajectory. To train this parameterization into a few-step generator, we implement ArcFlow via trajectory distillation on pre-trained teacher models using lightweight adapters. This strategy ensures fast, stable convergence while preserving generative diversity and quality. Built on large-scale models (Qwen-Image-20B and FLUX.1-dev), ArcFlow only fine-tunes on less than 5% of original parameters and achieves a 40x speedup with 2 NFEs over the original multi-step teachers without significant quality degradation. Experiments on benchmarks show the effectiveness of ArcFlow both qualitatively and quantitatively.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [628] [MTS-CSNet: Multiscale Tensor Factorization for Deep Compressive Sensing on RGB Images](https://arxiv.org/abs/2602.07056)
*Mehmet Yamac,Lei Xu,Serkan Kiranyaz,Moncef Gabbouj*

Main category: eess.IV

TL;DR: 提出基于多尺度张量求和（MTS）分解的CS框架MTSCSNet，实验显示其在RGB图像上有出色重建性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的压缩感知方法在学习采样算子时存在限制感受野和高维数据扩展性差的问题。

Method: 提出MTS作为高效的多维信号处理结构化算子，在MTSCSNet中，先将MTS用作可学习的CS算子进行线性降维，再在重建阶段直接优化估计，形成简单前馈架构。

Result: 在标准CS基准测试中，MTSCSNet在RGB图像上达到了最先进的重建性能，有显著的PSNR增益和更快推理速度。

Conclusion: MTSCSNet能在使用更紧凑前馈架构的情况下，实现良好的压缩感知重建效果。

Abstract: Deep learning based compressive sensing (CS) methods typically learn sampling operators using convolutional or block wise fully connected layers, which limit receptive fields and scale poorly for high dimensional data. We propose MTSCSNet, a CS framework based on Multiscale Tensor Summation (MTS) factorization, a structured operator for efficient multidimensional signal processing. MTS performs mode-wise linear transformations with multiscale summation, enabling large receptive fields and effective modeling of cross-dimensional correlations. In MTSCSNet, MTS is first used as a learnable CS operator that performs linear dimensionality reduction in tensor space, with its adjoint defining the initial back-projection, and is then applied in the reconstruction stage to directly refine this estimate. This results in a simple feed-forward architecture without iterative or proximal optimization, while remaining parameter and computation efficient. Experiments on standard CS benchmarks show that MTSCSNet achieves state-of-the-art reconstruction performance on RGB images, with notable PSNR gains and faster inference, even compared to recent diffusion-based CS methods, while using a significantly more compact feed-forward architecture.

</details>


### [629] [MRI Cross-Modal Synthesis: A Comparative Study of Generative Models for T1-to-T2 Reconstruction](https://arxiv.org/abs/2602.07068)
*Ali Alqutayfi,Sadam Al-Azani*

Main category: eess.IV

TL;DR: 本文对三种生成模型用于T1到T2 MRI重建进行对比研究，实验表明各模型都能合成T2图像，不同模型各有优劣，研究为模型选择提供参考。


<details>
  <summary>Details</summary>
Motivation: MRI跨模态合成有临床价值，需对比不同生成模型以找出适合MRI合成应用的模型。

Method: 使用BraTS 2020数据集，用MSE、PSNR和SSIM评估Pix2Pix GAN、CycleGAN和VAE三种模型。

Result: 所有模型都能成功从T1输入合成T2图像，CycleGAN的PSNR和SSIM最高，Pix2Pix GAN的MSE最低，VAE在潜在空间表示和采样能力方面有优势。

Conclusion: 此对比研究为研究者和临床医生根据具体需求和数据限制选择MRI合成的生成模型提供了有价值的见解。

Abstract: MRI cross-modal synthesis involves generating images from one acquisition protocol using another, offering considerable clinical value by reducing scan time while maintaining diagnostic information. This paper presents a comprehensive comparison of three state-of-the-art generative models for T1-to-T2 MRI reconstruction: Pix2Pix GAN, CycleGAN, and Variational Autoencoder (VAE). Using the BraTS 2020 dataset (11,439 training and 2,000 testing slices), we evaluate these models based on established metrics including Mean Squared Error (MSE), Peak Signal-to-Noise Ratio (PSNR), and Structural Similarity Index (SSIM). Our experiments demonstrate that all models can successfully synthesize T2 images from T1 inputs, with CycleGAN achieving the highest PSNR (32.28 dB) and SSIM (0.9008), while Pix2Pix GAN provides the lowest MSE (0.005846). The VAE, though showing lower quantitative performance (MSE: 0.006949, PSNR: 24.95 dB, SSIM: 0.6573), offers advantages in latent space representation and sampling capabilities. This comparative study provides valuable insights for researchers and clinicians selecting appropriate generative models for MRI synthesis applications based on their specific requirements and data constraints.

</details>


### [630] [Condition Errors Refinement in Autoregressive Image Generation with Diffusion Loss](https://arxiv.org/abs/2602.07022)
*Yucheng Zhou,Hao Li,Jianbing Shen*

Main category: eess.IV

TL;DR: 本文对带扩散损失的扩散和自回归模型进行理论分析，提出基于最优传输理论的条件细化方法，实验证明该方法优于其他模型。


<details>
  <summary>Details</summary>
Motivation: 以往研究结合扩散模型与自回归框架优化图像生成，本文旨在分析带扩散损失的扩散和自回归模型，突出后者优势并解决条件不一致问题。

Method: 对条件扩散和自回归扩散进行理论比较；引入基于最优传输理论的条件细化方法，将条件细化表述为Wasserstein梯度流。

Result: 自回归模型中的补丁去噪优化能有效减轻条件误差，导致稳定的条件分布；自回归条件生成使条件误差影响呈指数衰减；提出的方法可有效减轻条件不一致。

Conclusion: 实验证明本文方法优于扩散和带扩散损失的自回归模型方法。

Abstract: Recent studies have explored autoregressive models for image generation, with promising results, and have combined diffusion models with autoregressive frameworks to optimize image generation via diffusion losses. In this study, we present a theoretical analysis of diffusion and autoregressive models with diffusion loss, highlighting the latter's advantages. We present a theoretical comparison of conditional diffusion and autoregressive diffusion with diffusion loss, demonstrating that patch denoising optimization in autoregressive models effectively mitigates condition errors and leads to a stable condition distribution. Our analysis also reveals that autoregressive condition generation refines the condition, causing the condition error influence to decay exponentially. In addition, we introduce a novel condition refinement approach based on Optimal Transport (OT) theory to address ``condition inconsistency''. We theoretically demonstrate that formulating condition refinement as a Wasserstein Gradient Flow ensures convergence toward the ideal condition distribution, effectively mitigating condition inconsistency. Experiments demonstrate the superiority of our method over diffusion and autoregressive models with diffusion loss methods.

</details>


### [631] [Extracting Root-Causal Brain Activity Driving Psychopathology from Resting State fMRI](https://arxiv.org/abs/2602.07233)
*Eric V. Strobl*

Main category: eess.IV

TL;DR: 现有精神疾病神经影像研究关联成像模式与诊断标签存在局限，本文引入双水平结构因果模型并开发SOURCE程序，实验显示其有优势。


<details>
  <summary>Details</summary>
Motivation: 现有神经影像研究成果多为模糊关联，不能揭示潜在机制，需找出根源因果映射并与症状维度关联。

Method: 引入双水平结构因果模型，连接受试者间症状结构和受试者内静息态fMRI，在此基础上开发SOURCE程序。

Result: SOURCE能恢复与根源因果BOLD驱动一致的局部图，增加了解释性和解剖特异性。

Conclusion: SOURCE在处理精神疾病神经影像数据上优于现有方法。

Abstract: Neuroimaging studies of psychiatric disorders often correlate imaging patterns with diagnostic labels or composite symptom scores, yielding diffuse associations that obscure underlying mechanisms. We instead seek to identify root-causal maps -- localized BOLD disturbances that initiate pathological cascades -- and to link them selectively to symptom dimensions. We introduce a bilevel structural causal model that connects between-subject symptom structure to within-subject resting-state fMRI via independent latent sources with localized direct effects. Based on this model, we develop SOURCE (Symptom-Oriented Uncovering of Root-Causal Elements), a procedure that links interpretable symptom axes to a parsimonious set of localized drivers. Experiments show that SOURCE recovers localized maps consistent with root-causal BOLD drivers and increases interpretability and anatomical specificity relative to existing comparators.

</details>


### [632] [Trajectory Stitching for Solving Inverse Problems with Flow-Based Models](https://arxiv.org/abs/2602.08538)
*Alexander Denker,Moshe Eliasof,Zeljko Kereta,Carola-Bibiane Schönlieb*

Main category: eess.IV

TL;DR: 提出MS - Flow方法，通过将轨迹表示为中间潜在状态序列，减少内存消耗并提高重建质量，在图像恢复等逆问题中有效。


<details>
  <summary>Details</summary>
Motivation: 直接优化初始潜在代码解决逆问题时，需通过整个生成轨迹反向传播，存在高内存成本和数值不稳定问题。

Method: 提出MS - Flow，将轨迹表示为中间潜在状态序列，局部执行流动力学，通过轨迹匹配惩罚耦合片段，交替更新中间潜在状态并与观测数据保持一致。

Result: 在图像恢复和逆问题（如图像修复、超分辨率、计算机断层扫描）中，MS - Flow比现有方法更有效。

Conclusion: MS - Flow能减少内存消耗，提高重建质量，在逆问题中有良好表现。

Abstract: Flow-based generative models have emerged as powerful priors for solving inverse problems. One option is to directly optimize the initial latent code (noise), such that the flow output solves the inverse problem. However, this requires backpropagating through the entire generative trajectory, incurring high memory costs and numerical instability. We propose MS-Flow, which represents the trajectory as a sequence of intermediate latent states rather than a single initial code. By enforcing the flow dynamics locally and coupling segments through trajectory-matching penalties, MS-Flow alternates between updating intermediate latent states and enforcing consistency with observed data. This reduces memory consumption while improving reconstruction quality. We demonstrate the effectiveness of MS-Flow over existing methods on image recovery and inverse problems, including inpainting, super-resolution, and computed tomography.

</details>


### [633] [Efficient Brain Extraction of MRI Scans with Mild to Moderate Neuropathology](https://arxiv.org/abs/2602.08764)
*Hjalti Thrastarson,Lotta M. Ellingsen*

Main category: eess.IV

TL;DR: 提出一种对T1加权图像进行颅骨剥离的新方法，通过改进U-net和新损失函数训练模型，在多数据集上表现良好，性能与或优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有颅骨剥离方法在神经病理学存在时易失败，界定脑掩码边界不一致，需开发新方法。

Method: 在银标准真值数据上用基于有符号距离变换的新损失函数训练改进版U-net模型，并用保留数据和独立外部数据集验证。

Result: 在保留测试数据和外部数据集上均有良好表现，如保留数据DSC为0.964±0.006、ASSD为1.4mm±0.2mm，外部数据集DSC为0.958±0.006、ASSD为1.7±0.2mm。

Conclusion: 该方法性能可比或优于现有脑提取方法，尤其能高度一致地保留大脑外表面，且代码公开。

Abstract: Skull stripping magnetic resonance images (MRI) of the human brain is an important process in many image processing techniques, such as automatic segmentation of brain structures. Numerous methods have been developed to perform this task, however, they often fail in the presence of neuropathology and can be inconsistent in defining the boundary of the brain mask. Here, we propose a novel approach to skull strip T1-weighted images in a robust and efficient manner, aiming to consistently segment the outer surface of the brain, including the sulcal cerebrospinal fluid (CSF), while excluding the full extent of the subarachnoid space and meninges. We train a modified version of the U-net on silver-standard ground truth data using a novel loss function based on the signed-distance transform (SDT). We validate our model both qualitatively and quantitatively using held-out data from the training dataset, as well as an independent external dataset. The brain masks used for evaluation partially or fully include the subarachnoid space, which may introduce bias into the comparison; nonetheless, our model demonstrates strong performance on the held-out test data, achieving a consistent mean Dice similarity coefficient (DSC) of 0.964$\pm$0.006 and an average symmetric surface distance (ASSD) of 1.4mm$\pm$0.2mm. Performance on the external dataset is comparable, with a DSC of 0.958$\pm$0.006 and an ASSD of 1.7$\pm$0.2mm. Our method achieves performance comparable to or better than existing state-of-the-art methods for brain extraction, particularly in its highly consistent preservation of the brain's outer surface. The method is publicly available on GitHub.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [634] [PACC: Protocol-Aware Cross-Layer Compression for Compact Network Traffic Representation](https://arxiv.org/abs/2602.08331)
*Zhaochen Guo,Tianyufei Zhou,Honghao Wang,Ronghua Li,Shinan Liu*

Main category: cs.NI

TL;DR: 提出冗余感知、层感知的网络流量分类表示框架PACC，在多任务中超越基线，提升效率。


<details>
  <summary>Details</summary>
Motivation: 现有网络流量分类表示方法存在问题，如手工统计有信息损失、原始比特编码成本高、预训练嵌入会扁平化协议栈等，且未处理流量中的冗余。

Method: 将协议栈视为多视图输入，学习紧凑的层投影，将表示分解为共享和私有组件，通过联合目标实现相关学习。

Result: 在加密应用分类、物联网设备识别和入侵检测等数据集上，PACC始终优于基线模型，在加密子集上比nPrint提高12.9%准确率，匹配或超越基础模型基线，端到端效率最高提升3.16倍。

Conclusion: PACC是一个有效的网络流量分类表示框架，能提升分类准确性和效率。

Abstract: Network traffic classification is a core primitive for network security and management, yet it is increasingly challenged by pervasive encryption and evolving protocols. A central bottleneck is representation: hand-crafted flow statistics are efficient but often too lossy, raw-bit encodings can be accurate but are costly, and recent pre-trained embeddings provide transfer but frequently flatten the protocol stack and entangle signals across layers. We observe that real traffic contains substantial redundancy both across network layers and within each layer; existing paradigms do not explicitly identify and remove this redundancy, leading to wasted capacity, shortcut learning, and degraded generalization. To address this, we propose PACC, a redundancy-aware, layer-aware representation framework. PACC treats the protocol stack as multi-view inputs and learns compact layer-wise projections that remain faithful to each layer while explicitly factorizing representations into shared (cross-layer) and private (layer-specific) components. We operationalize these goals with a joint objective that preserves layer-specific information via reconstruction, captures shared structure via contrastive mutual-information learning, and maximizes task-relevant information via supervised losses, yielding compact latents suitable for efficient inference. Across datasets covering encrypted application classification, IoT device identification, and intrusion detection, PACC consistently outperforms feature-engineered and raw-bit baselines. On encrypted subsets, it achieves up to a 12.9% accuracy improvement over nPrint. PACC matches or surpasses strong foundation-model baselines. At the same time, it improves end-to-end efficiency by up to 3.16x.

</details>


### [635] [Decentralized Spatial Reuse Optimization in Wi-Fi: An Internal Regret Minimization Approach](https://arxiv.org/abs/2602.08456)
*Francesc Wilhelmi,Boris Bellalta,Miguel Casasnovas,Aleksandra Kijanka,Miguel Calvo-Fullana*

Main category: cs.NI

TL;DR: 本文提出基于后悔匹配的分散学习算法以优化空间复用参数，模拟结果显示该方法有优势，质疑集中式方案必要性。


<details>
  <summary>Details</summary>
Motivation: 空间复用参数的分散优化有挑战，多智能体并发操作易导致次优全局配置，传统分散方法常收敛于低效纳什均衡。

Method: 引入基于后悔匹配的分散学习算法，基于内部后悔最小化，引导竞争智能体趋向相关均衡。

Result: 模拟结果显示所提方法有优越性，能达到接近最优的全局性能。

Conclusion: 可扩展的分散解决方案有未释放的潜力，质疑新兴集中式解决方案的必要性。

Abstract: Spatial Reuse (SR) is a cost-effective technique for improving spectral efficiency in dense IEEE 802.11 deployments by enabling simultaneous transmissions. However, the decentralized optimization of SR parameters -- transmission power and Carrier Sensing Threshold (CST) -- across different Basic Service Sets (BSSs) is challenging due to the lack of global state information. In addition, the concurrent operation of multiple agents creates a highly non-stationary environment, often resulting in suboptimal global configurations (e.g., using the maximum possible transmission power by default). To overcome these limitations, this paper introduces a decentralized learning algorithm based on regret-matching, grounded in internal regret minimization. Unlike standard decentralized ``selfish'' approaches that often converge to inefficient Nash Equilibria (NE), internal regret minimization guides competing agents toward Correlated Equilibria (CE), effectively mimicking coordination without explicit communication. Through simulation results, we showcase the superiority of our proposed approach and its ability to reach near-optimal global performance. These results confirm the not-yet-unleashed potential of scalable decentralized solutions and question the need for the heavy signaling overheads and architectural complexity associated with emerging centralized solutions like Multi-Access Point Coordination (MAPC).

</details>


### [636] [6G-Bench: An Open Benchmark for Semantic Communication and Network-Level Reasoning with Foundation Models in AI-Native 6G Networks](https://arxiv.org/abs/2602.08675)
*Mohamed Amine Ferrag,Abderrahmane Lakas,Merouane Debbah*

Main category: cs.NI

TL;DR: 本文介绍用于评估6G网络语义通信和网络级推理的开放基准6G - Bench，包括任务定义、题库生成、对22个基础模型评估等，并发布数据集。


<details>
  <summary>Details</summary>
Motivation: 为AI原生6G网络的语义通信和网络级推理提供评估基准。

Method: 定义30个决策任务并分类，用任务条件提示生成10000个选择题，经筛选得到高置信评估集，用6G - Bench评估22个基础模型。

Result: 不同模型确定性单步准确率从0.22到0.82，领先模型意图和策略推理准确率在0.87 - 0.89，推理密集型任务选择性鲁棒性分析pass@5值在0.20到0.91之间。

Conclusion: 发布6G - Bench数据集以支持开放科学和可重复性。

Abstract: This paper introduces 6G-Bench, an open benchmark for evaluating semantic communication and network-level reasoning in AI-native 6G networks. 6G-Bench defines a taxonomy of 30 decision-making tasks (T1--T30) extracted from ongoing 6G and AI-agent standardization activities in 3GPP, IETF, ETSI, ITU-T, and the O-RAN Alliance, and organizes them into five standardization-aligned capability categories. Starting from 113,475 scenarios, we generate a balanced pool of 10,000 very-hard multiple-choice questions using task-conditioned prompts that enforce multi-step quantitative reasoning under uncertainty and worst-case regret minimization over multi-turn horizons. After automated filtering and expert human validation, 3,722 questions are retained as a high-confidence evaluation set, while the full pool is released to support training and fine-tuning of 6G-specialized models. Using 6G-Bench, we evaluate 22 foundation models spanning dense and mixture-of-experts architectures, short- and long-context designs (up to 1M tokens), and both open-weight and proprietary systems. Across models, deterministic single-shot accuracy (pass@1) spans a wide range from 0.22 to 0.82, highlighting substantial variation in semantic reasoning capability. Leading models achieve intent and policy reasoning accuracy in the range 0.87--0.89, while selective robustness analysis on reasoning-intensive tasks shows pass@5 values ranging from 0.20 to 0.91. To support open science and reproducibility, we release the 6G-Bench dataset on GitHub: https://github.com/maferrag/6G-Bench

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [637] [LakeHopper: Cross Data Lakes Column Type Annotation through Model Adaptation](https://arxiv.org/abs/2602.08793)
*Yushi Sun,Xujia Li,Nan Tang,Quanqing Xu,Chuanhui Yang,Lei Chen*

Main category: cs.CL

TL;DR: 本文研究将预训练语言模型适配到新数据湖以减少标注需求，提出LakeHopper框架，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 近期解决列类型标注的方案依赖特定数据湖的语言模型微调，本文旨在研究如何将现有预训练模型适配到新数据湖以减少标注。

Method: 提出LakeHopper框架，通过语言模型交互识别和解决知识差距，采用基于聚类的数据选择方案，使用增量微调机制。

Result: 在低资源和高资源设置下，针对两种不同的数据湖转移验证了LakeHopper的有效性。

Conclusion: LakeHopper框架能有效解决将预训练模型适配到新数据湖的问题。

Abstract: Column type annotation is vital for tasks like data cleaning, integration, and visualization. Recent solutions rely on resource-intensive language models fine-tuned on well-annotated columns from a particular set of tables, i.e., a source data lake. In this paper, we study whether we can adapt an existing pre-trained LM-based model to a new (i.e., target) data lake to minimize the annotations required on the new data lake. However, challenges include the source-target knowledge gap, selecting informative target data, and fine-tuning without losing shared knowledge exist. We propose LakeHopper, a framework that identifies and resolves the knowledge gap through LM interactions, employs a cluster-based data selection scheme for unannotated columns, and uses an incremental fine-tuning mechanism that gradually adapts the source model to the target data lake. Our experimental results validate the effectiveness of LakeHopper on two different data lake transfers under both low-resource and high-resource settings.

</details>


### [638] [Free Energy Mixer](https://arxiv.org/abs/2602.07160)
*Jiecheng Lu,Shihao Yang*

Main category: cs.CL

TL;DR: 提出Free Energy Mixer (FEM)，可在不改变复杂度下实现值感知后验读取，在多领域优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 标准注意力机制通过凸平均读取键/值，阻碍通道选择，需改进。

Method: 提出FEM，采用自由能读取，将(q,k)评分分布作为先验，实现值感知后验读取，还实例化两级门控FEM。

Result: FEM在NLP、视觉和时间序列等任务中，在参数预算匹配情况下始终优于强基线模型。

Conclusion: FEM是一种有效的方法，可与标准和线性注意力、线性RNN和SSM等即插即用。

Abstract: Standard attention stores keys/values losslessly but reads them via a per-head convex average, blocking channel-wise selection. We propose the Free Energy Mixer (FEM): a free-energy (log-sum-exp) read that applies a value-driven, per-channel log-linear tilt to a fast prior (e.g., from queries/keys in standard attention) over indices. Unlike methods that attempt to improve and enrich the $(q,k)$ scoring distribution, FEM treats it as a prior and yields a value-aware posterior read at unchanged complexity, smoothly moving from averaging to per-channel selection as the learnable inverse temperature increases, while still preserving parallelism and the original asymptotic complexity ($O(T^2)$ for softmax; $O(T)$ for linearizable variants). We instantiate a two-level gated FEM that is plug-and-play with standard and linear attention, linear RNNs and SSMs. It consistently outperforms strong baselines on NLP, vision, and time-series at matched parameter budgets.

</details>


### [639] [ViHERMES: A Graph-Grounded Multihop Question Answering Benchmark and System for Vietnamese Healthcare Regulations](https://arxiv.org/abs/2602.07361)
*Long S. T. Nguyen,Quan M. Bui,Tin T. Ngo,Quynh T. N. Vo,Dung N. H. Le,Tho T. Quan*

Main category: cs.CL

TL;DR: 本文介绍越南医疗法规多跳推理数据集ViHERMES，提出构建数据集的方法和图感知检索框架，实验表明该数据集有挑战性，图感知方法表现好。


<details>
  <summary>Details</summary>
Motivation: 现有问答方法在医疗法规多跳推理场景系统评估有限，缺乏支持越南语医疗法规多跳推理的基准数据集。

Method: 提出基于语义聚类和图启发式数据挖掘的多跳QA生成管道，结合大语言模型生成带注释数据；提出图感知检索框架。

Result: ViHERMES为评估多跳监管QA系统提供了具有挑战性的基准，图感知方法始终优于基于检索的强基线。

Conclusion: ViHERMES数据集和系统实现公开可用，该数据集和方法有助于多跳监管QA系统的评估和发展。

Abstract: Question Answering (QA) over regulatory documents is inherently challenging due to the need for multihop reasoning across legally interdependent texts, a requirement that is particularly pronounced in the healthcare domain where regulations are hierarchically structured and frequently revised through amendments and cross-references. Despite recent progress in retrieval-augmented and graph-based QA methods, systematic evaluation in this setting remains limited, especially for low-resource languages such as Vietnamese, due to the lack of benchmark datasets that explicitly support multihop reasoning over healthcare regulations. In this work, we introduce the Vietnamese Healthcare Regulations-Multihop Reasoning Dataset (ViHERMES), a benchmark designed for multihop QA over Vietnamese healthcare regulatory documents. ViHERMES consists of high-quality question-answer pairs that require reasoning across multiple regulations and capture diverse dependency patterns, including amendment tracing, cross-document comparison, and procedural synthesis. To construct the dataset, we propose a controlled multihop QA generation pipeline based on semantic clustering and graph-inspired data mining, followed by large language model-based generation with structured evidence and reasoning annotations. We further present a graph-aware retrieval framework that models formal legal relations at the level of legal units and supports principled context expansion for legally valid and coherent answers. Experimental results demonstrate that ViHERMES provides a challenging benchmark for evaluating multihop regulatory QA systems and that the proposed graph-aware approach consistently outperforms strong retrieval-based baselines. The ViHERMES dataset and system implementation are publicly available at https://github.com/ura-hcmut/ViHERMES.

</details>


### [640] [SRR-Judge: Step-Level Rating and Refinement for Enhancing Search-Integrated Reasoning in Search Agents](https://arxiv.org/abs/2602.07773)
*Chen Zhang,Kuicai Dong,Dexun Li,Wenjun Li,Qu Yang,Wei Han,Yong Liu*

Main category: cs.CL

TL;DR: 文章提出SRR - Judge框架用于搜索集成推理的可靠步骤级评估，经实验验证其评估更可靠，且能提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 主流方法在训练搜索集成推理能力时仅用基于结果的监督，忽略中间思考和行动质量。

Method: 引入SRR - Judge框架，集成到改进的ReAct风格流程中，利用SRR注释数据进行迭代拒绝采样微调。

Result: SRR - Judge比大模型如DeepSeek - V3.1有更可靠的步骤级评估，评分与最终答案正确性强相关；与SRR - Judge注释轨迹对齐使性能显著提升。

Conclusion: SRR - Judge能提供细粒度指导，有效提升基础代理的深度搜索能力。

Abstract: Recent deep search agents built on large reasoning models (LRMs) excel at complex question answering by iteratively planning, acting, and gathering evidence, a capability known as search-integrated reasoning. However, mainstream approaches often train this ability using only outcome-based supervision, neglecting the quality of intermediate thoughts and actions. We introduce SRR-Judge, a framework for reliable step-level assessment of reasoning and search actions. Integrated into a modified ReAct-style rate-and-refine workflow, SRR-Judge provides fine-grained guidance for search-integrated reasoning and enables efficient post-training annotation. Using SRR-annotated data, we apply an iterative rejection sampling fine-tuning procedure to enhance the deep search capability of the base agent. Empirically, SRR-Judge delivers more reliable step-level evaluations than much larger models such as DeepSeek-V3.1, with its ratings showing strong correlation with final answer correctness. Moreover, aligning the policy with SRR-Judge annotated trajectories leads to substantial performance gains, yielding over a 10 percent average absolute pass@1 improvement across challenging deep search benchmarks.

</details>


### [641] [GISA: A Benchmark for General Information-Seeking Assistant](https://arxiv.org/abs/2602.08543)
*Yutao Zhu,Xingshuo Zhang,Maosen Zhang,Jiajie Jin,Liancheng Zhang,Xiaoshuai Song,Kangzhi Zhao,Wencong Zeng,Ruiming Tang,Han Li,Ji-Rong Wen,Zhicheng Dou*

Main category: cs.CL

TL;DR: 现有搜索代理基准存在不足，本文提出GISA基准评估通用信息检索助手，实验显示模型表现有很大提升空间。


<details>
  <summary>Details</summary>
Motivation: 现有搜索代理基准构建查询不自然、任务单一、答案集静态易污染，与实际需求不符。

Method: 引入包含373个人工编写查询的GISA基准，有四种结构化答案格式，集成推理和信息聚合，有动态答案子集和完整搜索轨迹。

Result: 主流大模型和商业搜索产品在GISA上表现不佳，最佳模型精确匹配得分仅19.30%，复杂任务性能下降明显。

Conclusion: 当前模型在信息检索任务上有很大改进空间。

Abstract: The advancement of large language models (LLMs) has significantly accelerated the development of search agents capable of autonomously gathering information through multi-turn web interactions. Various benchmarks have been proposed to evaluate such agents. However, existing benchmarks often construct queries backward from answers, producing unnatural tasks misaligned with real-world needs. Moreover, these benchmarks tend to focus on either locating specific information or aggregating information from multiple sources, while relying on static answer sets prone to data contamination. To bridge these gaps, we introduce GISA, a benchmark for General Information-Seeking Assistants comprising 373 human-crafted queries that reflect authentic information-seeking scenarios. GISA features four structured answer formats (item, set, list, and table), enabling deterministic evaluation. It integrates both deep reasoning and broad information aggregation within unified tasks, and includes a live subset with periodically updated answers to resist memorization. Notably, GISA provides complete human search trajectories for every query, offering gold-standard references for process-level supervision and imitation learning. Experiments on mainstream LLMs and commercial search products reveal that even the best-performing model achieves only 19.30\% exact match score, with performance notably degrading on tasks requiring complex planning and comprehensive information gathering. These findings highlight substantial room for future improvement.

</details>


### [642] [Do Images Clarify? A Study on the Effect of Images on Clarifying Questions in Conversational Search](https://arxiv.org/abs/2602.08700)
*Clemencia Siro,Zahra Abbasiantaeb,Yifei Yuan,Mohammad Aliannejadi,Maarten de Rijke*

Main category: cs.CL

TL;DR: 研究图像在对话式搜索中的作用，比较多模态和纯文本澄清问题在不同搜索任务中的效果，结果表明视觉增强的好处依赖于任务。


<details>
  <summary>Details</summary>
Motivation: 以往研究表明文本澄清问题有用，图像能提升检索性能，但图像用于澄清问题对用户表现的影响未被充分研究。

Method: 进行有73名参与者的用户研究，比较多模态和纯文本澄清问题在回答澄清问题和查询重构两个任务中的效果。

Result: 回答澄清问题时用户偏好多模态问题，但查询重构任务中偏好更平衡；图像的影响因任务类型和用户专业知识而异；回答问题时纯文本设置表现更好。

Conclusion: 设计多模态对话式搜索系统时，视觉增强好处依赖任务，应根据搜索上下文和用户特征策略性实施。

Abstract: Conversational search systems increasingly employ clarifying questions to refine user queries and improve the search experience. Previous studies have demonstrated the usefulness of text-based clarifying questions in enhancing both retrieval performance and user experience. While images have been shown to improve retrieval performance in various contexts, their impact on user performance when incorporated into clarifying questions remains largely unexplored. We conduct a user study with 73 participants to investigate the role of images in conversational search, specifically examining their effects on two search-related tasks: (i) answering clarifying questions and (ii) query reformulation. We compare the effect of multimodal and text-only clarifying questions in both tasks within a conversational search context from various perspectives. Our findings reveal that while participants showed a strong preference for multimodal questions when answering clarifying questions, preferences were more balanced in the query reformulation task. The impact of images varied with both task type and user expertise. In answering clarifying questions, images helped maintain engagement across different expertise levels, while in query reformulation they led to more precise queries and improved retrieval performance. Interestingly, for clarifying question answering, text-only setups demonstrated better user performance as they provided more comprehensive textual information in the absence of images. These results provide valuable insights for designing effective multimodal conversational search systems, highlighting that the benefits of visual augmentation are task-dependent and should be strategically implemented based on the specific search context and user characteristics.

</details>


### [643] [Large Language Models for Geolocation Extraction in Humanitarian Crisis Response](https://arxiv.org/abs/2602.08872)
*G. Cafferata,T. Demarco,K. Kalimeri,Y. Mejova,M. G. Beiró*

Main category: cs.CL

TL;DR: 本文探讨大语言模型能否解决人道主义文件中提取地理位置信息的地理差异问题，提出两步框架，结果显示基于大语言模型的方法可提升地理定位提取的精度和公平性。


<details>
  <summary>Details</summary>
Motivation: 人道主义危机需要准确地理信息，但现有自动系统提取位置时存在地理和社会经济偏差，本文旨在研究大语言模型能否解决此问题。

Method: 引入结合少样本大语言模型命名实体识别和基于代理地理编码模块的两步框架，借助精确度和公平性指标，使用扩展版HumSet数据集进行基准测试。

Result: 基于大语言模型的方法大幅提高了从人道主义文本中提取地理位置的精度和公平性，特别是对代表性不足的地区。

Conclusion: 本研究将大语言模型推理进展与负责任和包容性人工智能原则相结合，有助于构建更公平的人道主义响应地理空间数据系统。

Abstract: Humanitarian crises demand timely and accurate geographic information to inform effective response efforts. Yet, automated systems that extract locations from text often reproduce existing geographic and socioeconomic biases, leading to uneven visibility of crisis-affected regions. This paper investigates whether Large Language Models (LLMs) can address these geographic disparities in extracting location information from humanitarian documents. We introduce a two-step framework that combines few-shot LLM-based named entity recognition with an agent-based geocoding module that leverages context to resolve ambiguous toponyms. We benchmark our approach against state-of-the-art pretrained and rule-based systems using both accuracy and fairness metrics across geographic and socioeconomic dimensions. Our evaluation uses an extended version of the HumSet dataset with refined literal toponym annotations. Results show that LLM-based methods substantially improve both the precision and fairness of geolocation extraction from humanitarian texts, particularly for underrepresented regions. By bridging advances in LLM reasoning with principles of responsible and inclusive AI, this work contributes to more equitable geospatial data systems for humanitarian response, advancing the goal of leaving no place behind in crisis analytics.

</details>


### [644] [Does Visual Rendering Bypass Tokenization? Investigating Script-Tokenizer Misalignment in Pixel-Based Language Models](https://arxiv.org/abs/2602.06973)
*Lucky Susanto,Musa Izzanardi Wijanarko,Khumaisa Nur'aini,Farid Adilazuarda,Alham Fikri Aji,Derry Tanti Wijaya*

Main category: cs.CL

TL;DR: 研究DualGPT架构中脚本-分词器对齐对印尼四种低资源本地语言的影响，发现重新引入文本分词器会带来分词器不对齐问题，Llama 2分词器表现不如自定义分词器。


<details>
  <summary>Details</summary>
Motivation: 探讨视觉渲染是否真能使模型摆脱分词约束，评估DualGPT架构中脚本-分词器对齐的影响。

Method: 聚焦印尼四种有非拉丁脚本的低资源本地语言，在DualGPT架构下评估脚本-分词器对齐情况。

Result: 重新引入文本分词器会带来分词器不对齐问题，Llama 2分词器表现显著差于自定义分词器，chrF++提升达30.15。

Conclusion: 文本分词器仍是公平模型的重大障碍，为未来多模态变体研究敲响警钟。

Abstract: While pixel-based language modeling aims to bypass the sub-word tokenization bottleneck by rendering text as images, recent multimodal variants such as DualGPT reintroduce text tokenizers to improve autoregressive performance. We investigate a fundamental question, does visual rendering truly decouple a model from tokenization constraints? Focusing on four Indonesian low-resource local languages that have their own non-Latin scripts (i.e., Javanese, Balinese, Sundanese, and Lampungnese), we evaluate the impact of script-tokenizer alignment within the DualGPT architecture. Our results show that, despite visual rendering, reintegrating a text tokenizer into the architecture reintroduces the same issue that pixel-based language modeling aims to resolve, which is the tokenizer misalignment problem. Despite having lower OOV and fertility rates, we show that the Llama 2 tokenizer performs significantly worse than a custom tokenizer, with improvements of up to 30.15 chrF++. Our findings serve as a warning for future multimodal variants, as text tokenizers remain a significant barrier to equitable models.

</details>


### [645] [BiomechAgent: AI-Assisted Biomechanical Analysis Through Code-Generating Agents](https://arxiv.org/abs/2602.06975)
*R. James Cotton,Thomas Leonard*

Main category: cs.CL

TL;DR: 介绍了代码生成AI代理BiomechAgent，可让用户通过自然语言进行生物力学分析，经测试表现良好，特定指令和专业工具能提升性能，本地模型表现较差，使运动捕捉数据更易用。


<details>
  <summary>Details</summary>
Motivation: 标记式运动捕捉数据的分析对无编程专长的临床医生是障碍，需工具辅助分析。

Method: 开发涵盖多方面的系统基准测试评估BiomechAgent能力，用数据集评估设计决策，测试本地模型表现。

Result: BiomechAgent在数据检索和可视化任务上精度高，有临床推理能力；生物力学特定指令和专业工具可提升性能；本地模型多数性能差。

Conclusion: BiomechAgent让运动捕捉数据对终端用户更有用、更易获取。

Abstract: Markerless motion capture is making quantitative movement analysis increasingly accessible, yet analyzing the resulting data remains a barrier for clinicians without programming expertise. We present BiomechAgent, a code-generating AI agent that enables biomechanical analysis through natural language and allows users to querying databases, generating visualizations, and even interpret data without requiring users to write code. To evaluate BiomechAgent's capabilities, we developed a systematic benchmark spanning data retrieval, visualization, activity classification, temporal segmentation, and clinical reasoning. BiomechAgent achieved robust accuracy on data retrieval and visualization tasks and demonstrated emerging clinical reasoning capabilities. We used our dataset to systematically evaluate several of our design decisions. Biomechanically-informed, domain-specific instructions significantly improved performance over generic prompts, and integrating validated specialized tools for gait event detection substantially boosted accuracy on challenging spatiotemporal analysis where the base agent struggled. We also tested BiomechAgent using a local open-weight model instead of a frontier cloud based LLM and found that perform was substantially diminished in most domains other than database retrieval. In short, BiomechAgent makes the data from accessible motion capture and much more useful and accessible to end users.

</details>


### [646] [Bridging the Knowledge Void: Inference-time Acquisition of Unfamiliar Programming Languages for Coding Tasks](https://arxiv.org/abs/2602.06976)
*Chen Shen,Wei Cheng,Jingyue Yang,Huan Zhang,Yuhan Wu,Wei Hu*

Main category: cs.CL

TL;DR: 本文提出ILA - agent框架，让大语言模型在推理时学习陌生编程语言，构建Cangjie - bench基准测试，实验表明ILA - agent表现优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在面对陌生编程语言时编码能力下降，传统数据密集型微调存在不足，因此研究推理时语言习得范式。

Method: 提出ILA - agent框架，将人类行为建模为工具，使大语言模型能与官方文档和执行环境交互；构建Cangjie - bench基准测试。

Result: 使用不同大语言模型的实验表明，ILA - agent显著优于检索增强基线。

Conclusion: ILA - agent在推理时学习陌生语言有效，但仍存在性能差距。

Abstract: The proficiency of Large Language Models (LLMs) in coding tasks is often a reflection of their extensive pre-training corpora, which typically collapses when confronted with previously unfamiliar programming languages. Departing from data-intensive finetuning, we investigate the paradigm of Inference-time Language Acquisition (ILA), where an LLM masters an unfamiliar language through dynamic interaction with limited external resources. In this paper, we propose ILA-agent, a general ILA framework that equips LLMs with a set of behavioral primitives. By modeling essential human-like behaviors as a suite of tools, ILA-agent enables LLMs to incrementally explore, apply, and verify language knowledge through structured interactions with the official documentation and execution environment. To provide a rigorous evaluation in a low-resource setting, we construct Cangjie-bench, a multi-task benchmark based on the novel statically-typed language Cangjie. We instantiate ILA-agent for Cangjie and evaluate its performance across code generation, translation, and program repair tasks. Results using diverse LLMs demonstrate that ILA-agent significantly outperforms retrieval-augmented baselines. Further analysis of agent trajectories characterizes the emergent behavior patterns while highlighting persisting performance gaps.

</details>


### [647] [Your Language Model Secretly Contains Personality Subnetworks](https://arxiv.org/abs/2602.07164)
*Ruimeng Ye,Zihan Wang,Zinan Ling,Yang Xiao,Manling Li,Xiaolong Ma,Bo Hui*

Main category: cs.CL

TL;DR: 研究表明大语言模型参数空间已包含特定角色子网络，提出无训练方法分离子网络，结果显示子网角色对齐性强且高效，为大模型个性化提供新视角。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型是否需外部上下文或参数来适应不同行为，以及能否发现导致二元对立角色的子网络。

Method: 用小校准数据集识别不同角色激活特征，开发掩码策略分离轻量级角色子网络，引入对比剪枝策略增强二元对立场景分离，方法无训练，仅依赖模型现有参数空间。

Result: 得到的子网络在不同评估场景下比依赖外部知识的基线模型有更强的角色对齐性，且更高效。

Conclusion: 大语言模型中多样的类人行为已嵌于参数空间，为可控和可解释的个性化提供新视角。

Abstract: Humans shift between different personas depending on social context. Large Language Models (LLMs) demonstrate a similar flexibility in adopting different personas and behaviors. Existing approaches, however, typically adapt such behavior through external knowledge such as prompting, retrieval-augmented generation (RAG), or fine-tuning. We ask: do LLMs really need external context or parameters to adapt to different behaviors, or do they already have such knowledge embedded in their parameters? In this work, we show that LLMs already contain persona-specialized subnetworks in their parameter space. Using small calibration datasets, we identify distinct activation signatures associated with different personas. Guided by these statistics, we develop a masking strategy that isolates lightweight persona subnetworks. Building on the findings, we further discuss: how can we discover opposing subnetwork from the model that lead to binary-opposing personas, such as introvert-extrovert? To further enhance separation in binary opposition scenarios, we introduce a contrastive pruning strategy that identifies parameters responsible for the statistical divergence between opposing personas. Our method is entirely training-free and relies solely on the language model's existing parameter space. Across diverse evaluation settings, the resulting subnetworks exhibit significantly stronger persona alignment than baselines that require external knowledge while being more efficient. Our findings suggest that diverse human-like behaviors are not merely induced in LLMs, but are already embedded in their parameter space, pointing toward a new perspective on controllable and interpretable personalization in large language models.

</details>


### [648] [Open TutorAI: An Open-source Platform for Personalized and Immersive Learning with Generative AI](https://arxiv.org/abs/2602.07176)
*Mohamed El Hajji,Tarek Ait Baha,Aicha Dakir,Hammou Fadili,Youssef Es-Saady*

Main category: cs.CL

TL;DR: 本文介绍开源教育平台Open TutorAI，结合大语言模型和生成技术，提供个性化辅导，提升学习体验，助力下一代智能辅导系统发展。


<details>
  <summary>Details</summary>
Motivation: 现有教育聊天机器人系统存在缺乏上下文适应性等问题，需要结合AI和沉浸式技术的开放综合平台来支持个性化学习。

Method: 构建基于大语言模型和生成技术的开源教育平台Open TutorAI，集成自然语言处理和可定制3D化身，通过结构化流程配置特定AI助理，提供多种工具。

Result: 打造了Open TutorAI，集模块化架构、生成式AI和学习分析于开源框架。

Conclusion: Open TutorAI有助于开发下一代智能辅导系统。

Abstract: Recent advances in artificial intelligence have created new possibilities for making education more scalable, adaptive, and learner-centered. However, existing educational chatbot systems often lack contextual adaptability, real-time responsiveness, and pedagogical agility. which can limit learner engagement and diminish instructional effectiveness. Thus, there is a growing need for open, integrative platforms that combine AI and immersive technologies to support personalized, meaningful learning experiences. This paper presents Open TutorAI, an open-source educational platform based on LLMs and generative technologies that provides dynamic, personalized tutoring. The system integrates natural language processing with customizable 3D avatars to enable multimodal learner interaction. Through a structured onboarding process, it captures each learner's goals and preferences in order to configure a learner-specific AI assistant. This assistant is accessible via both text-based and avatar-driven interfaces. The platform includes tools for organizing content, providing embedded feedback, and offering dedicated interfaces for learners, educators, and parents. This work focuses on learner-facing components, delivering a tool for adaptive support that responds to individual learner profiles without requiring technical expertise. Its assistant-generation pipeline and avatar integration enhance engagement and emotional presence, creating a more humanized, immersive learning environment. Embedded learning analytics support self-regulated learning by tracking engagement patterns and generating actionable feedback. The result is Open TutorAI, which unites modular architecture, generative AI, and learner analytics within an open-source framework. It contributes to the development of next-generation intelligent tutoring systems.

</details>


### [649] [Long-Context Long-Form Question Answering for Legal Domain](https://arxiv.org/abs/2602.07190)
*Anagha Kulkarni,Parin Rajesh Jhaveri,Prasha Shrestha,Yu Tong Han,Reza Amini,Behrouz Madahian*

Main category: cs.CL

TL;DR: 本文针对法律文档长文本问答挑战，提出问答系统，引入覆盖指标，构建数据集，经实验验证系统可用性。


<details>
  <summary>Details</summary>
Motivation: 法律文档布局复杂、语言专业，长文本问答难度大，需解决相关挑战。

Method: 提出问答系统，可解构专业词汇、解析文档布局、生成准确答案；引入覆盖指标；利用专业人士构建QA数据集。

Result: 通过全面实验和消融研究，证明了所提系统的可用性和优点。

Conclusion: 所提出的问答系统能有效应对法律文档的长上下文长答案问答挑战。

Abstract: Legal documents have complex document layouts involving multiple nested sections, lengthy footnotes and further use specialized linguistic devices like intricate syntax and domain-specific vocabulary to ensure precision and authority. These inherent characteristics of legal documents make question answering challenging, and particularly so when the answer to the question spans several pages (i.e. requires long-context) and is required to be comprehensive (i.e. a long-form answer). In this paper, we address the challenges of long-context question answering in context of long-form answers given the idiosyncrasies of legal documents. We propose a question answering system that can (a) deconstruct domain-specific vocabulary for better retrieval from source documents, (b) parse complex document layouts while isolating sections and footnotes and linking them appropriately, (c) generate comprehensive answers using precise domain-specific vocabulary. We also introduce a coverage metric that classifies the performance into recall-based coverage categories allowing human users to evaluate the recall with ease. We curate a QA dataset by leveraging the expertise of professionals from fields such as law and corporate tax. Through comprehensive experiments and ablation studies, we demonstrate the usability and merit of the proposed system.

</details>


### [650] [Beyond Accuracy: Risk-Sensitive Evaluation of Hallucinated Medical Advice](https://arxiv.org/abs/2602.07319)
*Savan Doshi*

Main category: cs.CL

TL;DR: 现有大语言模型医疗问答幻觉评估标准不足，提出风险敏感评估框架及结果显示其重要性。


<details>
  <summary>Details</summary>
Motivation: 现有幻觉评估标准和指标仅关注事实正确性，忽略临床相关失败模式，需新评估方法。

Method: 提出基于风险语言的评估框架，结合风险评分和相关性测量，用安全压力测试提示评估三个模型。

Result: 表面行为相似模型有不同风险特征，标准评估指标无法区分这些差异。

Conclusion:  hallucination评估应纳入风险敏感性，评估有效性依赖任务和提示设计。

Abstract: Large language models are increasingly being used in patient-facing medical question answering, where hallucinated outputs can vary widely in potential harm. However, existing hallucination standards and evaluation metrics focus primarily on factual correctness, treating all errors as equally severe. This obscures clinically relevant failure modes, particularly when models generate unsupported but actionable medical language. We propose a risk-sensitive evaluation framework that quantifies hallucinations through the presence of risk-bearing language, including treatment directives, contraindications, urgency cues, and mentions of high-risk medications. Rather than assessing clinical correctness, our approach evaluates the potential impact of hallucinated content if acted upon. We further combine risk scoring with a relevance measure to identify high-risk, low-grounding failures. We apply this framework to three instruction-tuned language models using controlled patient-facing prompts designed as safety stress tests. Our results show that models with similar surface-level behavior exhibit substantially different risk profiles and that standard evaluation metrics fail to capture these distinctions. These findings highlight the importance of incorporating risk sensitivity into hallucination evaluation and suggest that evaluation validity is critically dependent on task and prompt design.

</details>


### [651] [Intent Mismatch Causes LLMs to Get Lost in Multi-Turn Conversation](https://arxiv.org/abs/2602.07338)
*Geng Liu,Fei Zhu,Rong Feng,Changyi Ma,Shiqi Wang,Gaofeng Meng*

Main category: cs.CL

TL;DR: 研究大语言模型多轮对话中‘Lost in Conversation’现象，指出根源是意图对齐差距，提出Mediator - Assistant架构缓解性能下降。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在多轮对话中出现的性能显著下降问题，即‘Lost in Conversation’现象。

Method: 提出Mediator - Assistant架构，将意图理解与任务执行解耦，通过经验驱动的Mediator根据历史交互模式将用户输入转化为明确指令。

Result: 该方法显著缓解了不同大语言模型在多轮对话中的性能下降。

Conclusion: ‘Lost in Conversation’现象根源是意图对齐差距而非模型能力不足，Mediator - Assistant架构有效解决该问题。

Abstract: Multi-turn conversation has emerged as a predominant interaction paradigm for Large Language Models (LLMs). Users often employ follow-up questions to refine their intent, expecting LLMs to adapt dynamically. However, recent research reveals that LLMs suffer a substantial performance drop in multi-turn settings compared to single-turn interactions with fully specified instructions, a phenomenon termed ``Lost in Conversation'' (LiC). While this prior work attributes LiC to model unreliability, we argue that the root cause lies in an intent alignment gap rather than intrinsic capability deficits. In this paper, we first demonstrate that LiC is not a failure of model capability but rather a breakdown in interaction between users and LLMs. We theoretically show that scaling model size or improving training alone cannot resolve this gap, as it arises from structural ambiguity in conversational context rather than representational limitations. To address this, we propose to decouple intent understanding from task execution through a Mediator-Assistant architecture. By utilizing an experience-driven Mediator to explicate user inputs into explicit, well-structured instructions based on historical interaction patterns, our approach effectively bridges the gap between vague user intent and model interpretation. Experimental results demonstrate that this method significantly mitigates performance degradation in multi-turn conversations across diverse LLMs.

</details>


### [652] [TernaryLM: Memory-Efficient Language Modeling via Native 1-Bit Quantization with Adaptive Layer-wise Scaling](https://arxiv.org/abs/2602.07374)
*Nisharg Nargund,Priyesh Shukla*

Main category: cs.CL

TL;DR: 提出TernaryLM架构，采用原生1位三元量化训练，减少内存消耗，维持语言建模能力，实验取得良好效果，显示原生1位训练是高效语言模型的有前景方向。


<details>
  <summary>Details</summary>
Motivation: 大语言模型性能出色但计算资源需求大，限制在边缘设备和资源受限环境部署，需减少内存消耗。

Method: 提出TernaryLM架构，使用原生1位三元量化{-1, 0, +1}训练，用直通估计器和自适应每层缩放因子从0学习量化感知表示。

Result: 在TinyStories上验证困惑度58.42；在MRPC释义检测上F1值82.47%；内存减少2.4倍，推理延迟相当；不同语料库训练动态稳定；中间Transformer层与极端量化兼容性最高。

Conclusion: 原生1位训练是高效神经语言模型的有前景方向。

Abstract: Large language models (LLMs) achieve remarkable performance but demand substantial computational resources, limiting deployment on edge devices and resource-constrained environments. We present TernaryLM, a 132M parameter transformer architecture that employs native 1-bit ternary quantization {-1, 0, +1} during training, achieving significant memory reduction without sacrificing language modeling capability. Unlike post-training quantization approaches that quantize pre-trained full-precision models, TernaryLM learns quantization-aware representations from scratch using straight-through estimators and adaptive per-layer scaling factors. Our experiments demonstrate: (1) validation perplexity of 58.42 on TinyStories; (2) downstream transfer with 82.47 percent F1 on MRPC paraphrase detection; (3) 2.4x memory reduction (498MB vs 1197MB) with comparable inference latency; and (4) stable training dynamics across diverse corpora. We provide layer-wise quantization analysis showing that middle transformer layers exhibit highest compatibility with extreme quantization, informing future non-uniform precision strategies. Our results suggest that native 1-bit training is a promising direction for efficient neural language models. Code is available at https://github.com/1nisharg/TernaryLM-Memory-Efficient-Language-Modeling.

</details>


### [653] [Advantages of Domain Knowledge Injection for Legal Document Summarization: A Case Study on Summarizing Indian Court Judgments in English and Hindi](https://arxiv.org/abs/2602.07382)
*Debtanu Datta,Rajdeep Mukherjee,Adrijit Goswami,Saptarshi Ghosh*

Main category: cs.CL

TL;DR: 本文旨在通过注入领域知识来改进印度法律文本的摘要生成，提出改进抽取式模型和向生成式模型注入知识的方法，结果显示在印地语和英语摘要生成上有显著提升且获专家验证。


<details>
  <summary>Details</summary>
Motivation: 印度法律文本复杂且人口多数不懂英文，需要用印地语生成摘要，故要改进法律文本摘要生成。

Method: 提出用特定预训练编码器改进抽取式神经摘要模型，通过持续预训练向生成式模型注入法律领域知识。

Result: 在英文到英文和英文到印地语的法律文档摘要生成中，采用标准评估、事实一致性和法律领域特定指标衡量，有显著提升。

Conclusion: 提出的方法有效，得到了领域专家的验证。

Abstract: Summarizing Indian legal court judgments is a complex task not only due to the intricate language and unstructured nature of the legal texts, but also since a large section of the Indian population does not understand the complex English in which legal text is written, thus requiring summaries in Indian languages. In this study, we aim to improve the summarization of Indian legal text to generate summaries in both English and Hindi (the most widely spoken Indian language), by injecting domain knowledge into diverse summarization models. We propose a framework to enhance extractive neural summarization models by incorporating domain-specific pre-trained encoders tailored for legal texts. Further, we explore the injection of legal domain knowledge into generative models (including Large Language Models) through continual pre-training on large legal corpora in English and Hindi. Our proposed approaches achieve statistically significant improvements in both English-to-English and English-to-Hindi Indian legal document summarization, as measured by standard evaluation metrics, factual consistency metrics, and legal domain-specific metrics. Furthermore, these improvements are validated through domain experts, demonstrating the effectiveness of our approaches.

</details>


### [654] [Learning to Self-Verify Makes Language Models Better Reasoners](https://arxiv.org/abs/2602.07594)
*Yuxin Chen,Yu Wang,Yi Zhang,Ziang Ye,Zhengzhou Cai,Yaorui Shi,Qi Gu,Hui Su,Xunliang Cai,Xiang Wang,An Zhang,Tat-Seng Chua*

Main category: cs.CL

TL;DR: 研究大语言模型生成和自验证能力的不对称性，发现自验证可提升生成性能，并提出多任务强化学习框架提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在生成能力强但自验证能力弱，存在生成与自验证的能力不对称性，需深入研究并解决。

Method: 对不对称性在训练过程中进行深入研究，并提出多任务强化学习框架，将生成和自验证作为独立且互补的目标进行优化。

Result: 大量实验表明，相比仅进行生成训练，提出的方法在生成和验证能力上都有性能提升。

Conclusion: 学习自验证能有效提高生成性能，将自验证融入生成训练可提升模型综合性能。

Abstract: Recent large language models (LLMs) achieve strong performance in generating promising reasoning paths for complex tasks. However, despite powerful generation ability, LLMs remain weak at verifying their own answers, revealing a persistent capability asymmetry between generation and self-verification. In this work, we conduct an in-depth investigation of this asymmetry throughout training evolution and show that, even on the same task, improving generation does not lead to corresponding improvements in self-verification. Interestingly, we find that the reverse direction of this asymmetry behaves differently: learning to self-verify can effectively improve generation performance, achieving accuracy comparable to standard generation training while yielding more efficient and effective reasoning traces. Building on this observation, we further explore integrating self-verification into generation training by formulating a multi-task reinforcement learning framework, where generation and self-verification are optimized as two independent but complementary objectives. Extensive experiments across benchmarks and models demonstrate performance gains over generation-only training in both generation and verification capabilities.

</details>


### [655] [Emergent Structured Representations Support Flexible In-Context Inference in Large Language Models](https://arxiv.org/abs/2602.07794)
*Ningyu Xu,Qi Zhang,Xipeng Qiu,Xuanjing Huang*

Main category: cs.CL

TL;DR: 研究大语言模型在上下文概念推理中的内部处理，发现概念子空间对推理有因果作用，揭示其构建和使用过程。


<details>
  <summary>Details</summary>
Motivation: 虽发现大语言模型中有类人概念表征，但不清楚推理是否依赖这些表征，故研究其在上下文概念推理中的内部处理。

Method: 进行因果中介分析，研究层间进展。

Result: 发现中晚期层出现概念子空间，其表征结构跨上下文存在，该子空间对模型预测有因果作用，还明确了层间构建和使用子空间的过程。

Conclusion: 大语言模型会动态构建和使用结构化潜在表征进行推理，为灵活适应的计算过程提供见解。

Abstract: Large language models (LLMs) exhibit emergent behaviors suggestive of human-like reasoning. While recent work has identified structured, human-like conceptual representations within these models, it remains unclear whether they functionally rely on such representations for reasoning. Here we investigate the internal processing of LLMs during in-context concept inference. Our results reveal a conceptual subspace emerging in middle to late layers, whose representational structure persists across contexts. Using causal mediation analyses, we demonstrate that this subspace is not merely an epiphenomenon but is functionally central to model predictions, establishing its causal role in inference. We further identify a layer-wise progression where attention heads in early-to-middle layers integrate contextual cues to construct and refine the subspace, which is subsequently leveraged by later layers to generate predictions. Together, these findings provide evidence that LLMs dynamically construct and use structured, latent representations in context for inference, offering insights into the computational processes underlying flexible adaptation.

</details>


### [656] [Pruning as a Cooperative Game: Surrogate-Assisted Layer Contribution Estimation for Large Language Models](https://arxiv.org/abs/2602.07804)
*Xuan Ding,Pengyu Tong,Ranjie Duan,Yunjian Zhang,Rui Sun,Yao Zhu*

Main category: cs.CL

TL;DR: 本文提出博弈论框架解决大语言模型层剪枝问题，用轻量级代理网络和分层蒙特卡罗掩码采样，实验证明该方法在困惑度和零样本准确率上表现更优。


<details>
  <summary>Details</summary>
Motivation: 大语言模型计算需求高，现有层剪枝方法依赖静态启发式规则，未考虑层间依赖关系，限制剪枝效果。

Method: 提出博弈论框架，将层剪枝建模为合作博弈；用轻量级代理网络估计层的边际贡献；应用分层蒙特卡罗掩码采样降低夏普值估计成本。

Result: 实验表明该方法在困惑度和零样本准确率方面始终表现更优。

Conclusion: 所提方法能为大语言模型实现更高效的层剪枝。

Abstract: While large language models (LLMs) demonstrate impressive performance across various tasks, their deployment in real-world scenarios is still constrained by high computational demands. Layer-wise pruning, a commonly employed strategy to mitigate inference costs, can partially address this challenge. However, existing approaches generally depend on static heuristic rules and fail to account for the interdependencies among layers, thereby limiting the effectiveness of the pruning process. To this end, this paper proposes a game-theoretic framework that formulates layer pruning as a cooperative game in which each layer acts as a player and model performance serves as the utility. As computing exact Shapley values is computationally infeasible for large language models (LLMs), we propose using a lightweight surrogate network to estimate layer-wise marginal contributions. This network can predict LLM performance for arbitrary layer combinations at a low computational cost. Additionally, we employ stratified Monte Carlo mask sampling to further reduce the cost of Sharpley value estimation. This approach captures inter-layer dependencies and dynamically identifies critical layers for pruning. Extensive experiments demonstrate the consistent superiority of our method in terms of perplexity and zero-shot accuracy, achieving more efficient and effective layer-wise pruning for large language models.

</details>


### [657] [TodoEvolve: Learning to Architect Agent Planning Systems](https://arxiv.org/abs/2602.07839)
*Jiaxi Liu,Yanzuo Jiang,Guibin Zhang,Zihan Zhang,Heng Chang,Zhenfei Yin,Qibing Ren,Junchi Yan*

Main category: cs.CL

TL;DR: 提出TodoEvolve元规划范式解决现有规划方法缺乏灵活性问题，在多基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有规划方法依赖固定手工结构，缺乏适应开放性问题结构多样性的灵活性。

Method: 构建PlanFactory提供统一接口，采用阻抗引导偏好优化（IGPO）训练Todo - 14B。

Result: 在五个智能体基准测试中，TodoEvolve始终超越精心设计的规划模块，且保持较低API成本和运行时开销。

Conclusion: TodoEvolve能有效解决现有规划方法的局限性，具有良好性能。

Abstract: Planning has become a central capability for contemporary agent systems in navigating complex, long-horizon tasks, yet existing approaches predominantly rely on fixed, hand-crafted planning structures that lack the flexibility to adapt to the structural diversity of open-ended problems. To address this limitation, we introduce TodoEvolve, a meta-planning paradigm that autonomously synthesizes and dynamically revises task-specific planning architectures. Specifically, we first construct PlanFactory, a modular design space that standardizes diverse planning paradigms within a unified codebase encompassing topology, initialization, adaptation, and navigation, thereby providing a common interface for heterogeneous planning patterns. Leveraging PlanFactory, we collect high-quality planning trajectories and train Todo-14B via \textit{Impedance-Guided Preference Optimization} (IGPO), a multi-objective reinforcement learning objective that encourages the generation of planning systems that are performant, stable, and token-efficient across arbitrary tasks and agent backbones. Empirical evaluations on five agentic benchmarks demonstrate that TodoEvolve consistently surpasses carefully engineered planning modules while maintaining economical API costs and runtime overhead.

</details>


### [658] [Bielik Guard: Efficient Polish Language Safety Classifiers for LLM Content Moderation](https://arxiv.org/abs/2602.07954)
*Krzysztof Wróbel,Jan Maria Kowalski,Jerzy Surma,Igor Ciuciura,Maciej Szymański*

Main category: cs.CL

TL;DR: 介绍Bielik Guard系列波兰语内容安全分类器，含两种模型，在多基准测试表现佳且公开可用。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在波兰语应用增加，需要高效准确的内容安全分类器。

Method: 基于MMLW - RoBERTa - base和PKOBP/polish - roberta - 8k构建两种模型，在6885条社区标注的波兰语文本数据集上微调。

Result: 两种模型在多基准测试表现好，0.5B模型整体判别能力佳，0.1B模型效率高，在真实用户提示上0.1B v1.1优于HerBERT - PL - Guard。

Conclusion: Bielik Guard模型公开可用，能针对敏感类别提供合适响应而非简单内容屏蔽。

Abstract: As Large Language Models (LLMs) become increasingly deployed in Polish language applications, the need for efficient and accurate content safety classifiers has become paramount. We present Bielik Guard, a family of compact Polish language safety classifiers comprising two model variants: a 0.1B parameter model based on MMLW-RoBERTa-base and a 0.5B parameter model based on PKOBP/polish-roberta-8k. Fine-tuned on a community-annotated dataset of 6,885 Polish texts, these models classify content across five safety categories: Hate/Aggression, Vulgarities, Sexual Content, Crime, and Self-Harm. Our evaluation demonstrates that both models achieve strong performance on multiple benchmarks. The 0.5B variant offers the best overall discrimination capability with F1 scores of 0.791 (micro) and 0.785 (macro) on the test set, while the 0.1B variant demonstrates exceptional efficiency. Notably, Bielik Guard 0.1B v1.1 achieves superior precision (77.65\%) and very low false positive rate (0.63\%) on real user prompts, outperforming HerBERT-PL-Guard (31.55\% precision, 4.70\% FPR) despite identical model size. The models are publicly available and designed to provide appropriate responses rather than simple content blocking, particularly for sensitive categories like self-harm.

</details>


### [659] [Lost in Translation? A Comparative Study on the Cross-Lingual Transfer of Composite Harms](https://arxiv.org/abs/2602.07963)
*Vaibhav Shukla,Hardik Sharma,Adith N Reganti,Soham Wasmatkar,Bagesh Kumar,Vrijendra Singh*

Main category: cs.CL

TL;DR: 本文引入CompositeHarm基准研究大语言模型跨语言安全对齐，发现攻击成功率在印度语系语言中上升，提出轻量级推理策略，指出翻译基准对构建安全系统必要但不充分。


<details>
  <summary>Details</summary>
Motivation: 多数大语言模型安全评估以英语为主，翻译难以全面反映多语言行为，需研究跨语言安全对齐情况。

Method: 引入CompositeHarm基准，结合两个英语数据集并扩展到六种语言，采用受边缘AI设计原则启发的轻量级推理策略。

Result: 攻击成功率在印度语系语言中急剧上升，上下文危害转移较缓和。

Conclusion: 翻译基准是构建安全系统必要的第一步，但不充分。

Abstract: Most safety evaluations of large language models (LLMs) remain anchored in English. Translation is often used as a shortcut to probe multilingual behavior, but it rarely captures the full picture, especially when harmful intent or structure morphs across languages. Some types of harm survive translation almost intact, while others distort or disappear. To study this effect, we introduce CompositeHarm, a translation-based benchmark designed to examine how safety alignment holds up as both syntax and semantics shift. It combines two complementary English datasets, AttaQ, which targets structured adversarial attacks, and MMSafetyBench, which covers contextual, real-world harms, and extends them into six languages: English, Hindi, Assamese, Marathi, Kannada, and Gujarati. Using three large models, we find that attack success rates rise sharply in Indic languages, especially under adversarial syntax, while contextual harms transfer more moderately. To ensure scalability and energy efficiency, our study adopts lightweight inference strategies inspired by edge-AI design principles, reducing redundant evaluation passes while preserving cross-lingual fidelity. This design makes large-scale multilingual safety testing both computationally feasible and environmentally conscious. Overall, our results show that translated benchmarks are a necessary first step, but not a sufficient one, toward building grounded, resource-aware, language-adaptive safety systems.

</details>


### [660] [DeltaKV: Residual-Based KV Cache Compression via Long-Range Similarity](https://arxiv.org/abs/2602.08005)
*Jitai Hao,Qiang Huang,Yaowei Wang,Min Zhang,Jun Yu*

Main category: cs.CL

TL;DR: 提出DeltaKV压缩框架和Sparse - vLLM推理引擎，减少KV缓存内存并提升长上下文场景吞吐量。


<details>
  <summary>Details</summary>
Motivation: 高效长上下文大语言模型应用受KV缓存内存线性增长限制，现有方法难平衡准确性、压缩率和硬件效率。

Method: 提出DeltaKV框架，基于长程词间相似性和KV表征潜在成分共享，编码语义残差；引入Sparse - vLLM推理引擎，优化内存管理和内核。

Result: DeltaKV将KV缓存内存降至原29%，在多个数据集保持近无损精度；与Sparse - vLLM集成后，长上下文场景吞吐量比vLLM最多提升2倍。

Conclusion: 为可扩展的长上下文大语言模型部署提供了实用途径。

Abstract: The deployment of efficient long-context LLMs in applications like autonomous agents, long-chain reasoning, and creative writing is fundamentally bottlenecked by the linear growth of KV cache memory. Existing compression and eviction methods often struggle to balance accuracy, compression ratio, and hardware efficiency. We propose DeltaKV, a residual-based KV cache compression framework motivated by two empirical findings: long-range inter-token similarity and highly shared latent components in KV representations. Instead of discarding tokens, DeltaKV encodes semantic residuals relative to retrieved historical references, preserving fidelity while substantially reducing storage. To translate compression gains into real system speedups, we further introduce Sparse-vLLM, a high-performance inference engine with decoupled memory management and kernels optimized for sparse and irregular KV layouts. Experiments show that DeltaKV reduces KV cache memory to 29\% of the original while maintaining near-lossless accuracy on LongBench, SCBench, and AIME. When integrated with Sparse-vLLM, it achieves up to 2$\times$ throughput improvement over vLLM in long-context scenarios, demonstrating a practical path toward scalable long-context LLM deployment. Code, model checkpoints, and datasets are available at https://github.com/CURRENTF/Sparse-vLLM.

</details>


### [661] [Efficient Post-Training Pruning of Large Language Models with Statistical Correction](https://arxiv.org/abs/2602.07375)
*Peiqi Yu,Jinhao Wang,Xinyi Sui,Nam Ling,Wei Wang,Wei Jiang*

Main category: cs.CL

TL;DR: 提出轻量级后训练剪枝框架，利用统计特性，不重训等，实验表明提升剪枝性能且成本与启发式方法相当。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型后训练剪枝方法在剪枝质量和计算效率间存在权衡，启发式方法对激活异常值敏感，基于重建的方法计算量大。

Method: 提出基于模型权重和激活的一阶统计特性的轻量级后训练剪枝框架，剪枝时用通道统计校准重要性得分，剪枝后应用解析能量补偿。

Result: 在多个大语言模型家族、稀疏模式和评估任务上实验，该方法提升剪枝性能，计算成本与启发式方法相当。

Conclusion: 简单的统计校正对大语言模型后训练剪枝有效。

Abstract: Post-training pruning is an effective approach for reducing the size and inference cost of large language models (LLMs), but existing methods often face a trade-off between pruning quality and computational efficiency. Heuristic pruning methods are efficient but sensitive to activation outliers, while reconstruction-based approaches improve fidelity at the cost of heavy computation. In this work, we propose a lightweight post-training pruning framework based on first-order statistical properties of model weights and activations. During pruning, channel-wise statistics are used to calibrate magnitude-based importance scores, reducing bias from activation-dominated channels. After pruning, we apply an analytic energy compensation to correct distributional distortions caused by weight removal. Both steps operate without retraining, gradients, or second-order information. Experiments across multiple LLM families, sparsity patterns, and evaluation tasks show that the proposed approach improves pruning performance while maintaining computational cost comparable to heuristic methods. The results suggest that simple statistical corrections can be effective for post-training pruning of LLMs.

</details>


### [662] [Emergent Search and Backtracking in Latent Reasoning Models](https://arxiv.org/abs/2602.08100)
*Jasmine Cui,Charles Ye*

Main category: cs.CL

TL;DR: 研究潜在推理变压器（LRT）在多项选择问答基准上推理过程，发现其自发学习潜在空间搜索过程及特点。


<details>
  <summary>Details</summary>
Motivation: 探究语言模型无文字推理的情况，了解LRT在潜在空间的推理过程。

Method: 对LRT进行研究，在多项选择问答基准上解码模型每一步不断演变的信念。

Result: 模型自发学习潜在空间结构化搜索过程，推理有一致轨迹，回溯普遍且有益，搜索是自适应的。

Conclusion: 潜在推理模型在激活空间实现了思维链通过文字实现的能力。

Abstract: What happens when a language model thinks without words? Standard reasoning LLMs verbalize intermediate steps as chain-of-thought; latent reasoning transformers (LRTs) instead perform deliberation entirely in continuous hidden space. We investigate an LRT, decoding the model's evolving beliefs at every step on a multiple-choice QA benchmark. We find that the model spontaneously learns a structured search process in latent space. Deliberation follows a consistent trajectory: an exploration phase where probability mass spreads across candidates, tentative commitment to a frontrunner, and either convergence or backtracking. Backtracking is prevalent (32% of instances), beneficial (34% accuracy gain over non-backtracking instances), and predominantly directed away from the semantically closest distractor toward the correct answer. The search is adaptive: replacing distractors with implausible alternatives shortens exploration by 54%. Latent reasoning models achieve in activation space what chain-of-thought achieves through words: the ability to be wrong, notice, and recover.

</details>


### [663] [Gender and Race Bias in Consumer Product Recommendations by Large Language Models](https://arxiv.org/abs/2602.08124)
*Ke Xu,Shera Potka,Alex Thomo*

Main category: cs.CL

TL;DR: 研究大型语言模型生成消费产品推荐时的性别和种族偏见，发现推荐有显著差异，需更公平的系统。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型用于产品推荐时，其性别和种族偏见潜力未被充分研究，本文首次尝试研究这些偏见。

Method: 利用提示工程从大型语言模型获取不同种族和性别群体的产品建议，采用标记词、支持向量机和詹森 - 香农散度三种分析方法识别和量化偏见。

Result: 不同人口统计群体的推荐存在显著差异。

Conclusion: 需要构建更公平的大型语言模型推荐系统。

Abstract: Large Language Models are increasingly employed in generating consumer product recommendations, yet their potential for embedding and amplifying gender and race biases remains underexplored. This paper serves as one of the first attempts to examine these biases within LLM-generated recommendations. We leverage prompt engineering to elicit product suggestions from LLMs for various race and gender groups and employ three analytical methods-Marked Words, Support Vector Machines, and Jensen-Shannon Divergence-to identify and quantify biases. Our findings reveal significant disparities in the recommendations for demographic groups, underscoring the need for more equitable LLM recommendation systems.

</details>


### [664] [Improving Variable-Length Generation in Diffusion Language Models via Length Regularization](https://arxiv.org/abs/2602.07546)
*Zicong Cheng,Ruixuan Jia,Jia Li,Guo-Wei Yang,Meng-Hao Guo,Shi-Min Hu*

Main category: cs.CL

TL;DR: 现有扩散大语言模型（DLLMs）在可变长度生成上有问题，本文提出LR - DLLM框架解决该问题，实验效果良好。


<details>
  <summary>Details</summary>
Motivation: 现有DLLMs在可变长度生成时，因固定长度画布和目标长度假设，导致生成置信度估计有长度诱导偏差，无法可靠确定生成长度。

Method: 提出LR - DLLM，将生成长度作为显式变量，通过显式长度正则化解耦语义兼容性和长度诱导的不确定性，动态扩展或收缩生成范围。

Result: 在HumanEvalInfilling上Pass@1达51.3%（比DreamOn高13.4%），在四语言McEval上平均Pass@1达51.5%（比DreamOn高14.3%）。

Conclusion: LR - DLLM框架能有效解决DLLMs可变长度生成问题，实现可靠长度确定。

Abstract: Diffusion Large Language Models (DLLMs) are inherently ill-suited for variable-length generation, as their inference is defined on a fixed-length canvas and implicitly assumes a known target length. When the length is unknown, as in realistic completion and infilling, naively comparing confidence across mask lengths becomes systematically biased, leading to under-generation or redundant continuations. In this paper, we show that this failure arises from an intrinsic lengthinduced bias in generation confidence estimates, leaving existing DLLMs without a robust way to determine generation length and making variablelength inference unreliable. To address this issue, we propose LR-DLLM, a length-regularized inference framework for DLLMs that treats generation length as an explicit variable and achieves reliable length determination at inference time. It decouples semantic compatibility from lengthinduced uncertainty through an explicit length regularization that corrects biased confidence estimates. Based on this, LR-DLLM enables dynamic expansion or contraction of the generation span without modifying the underlying DLLM or its training procedure. Experiments show that LRDLLM achieves 51.3% Pass@1 on HumanEvalInfilling under fully unknown lengths (+13.4% vs. DreamOn) and 51.5% average Pass@1 on four-language McEval (+14.3% vs. DreamOn).

</details>


### [665] [DIAL-SUMMER: A Structured Evaluation Framework of Hierarchical Errors in Dialogue Summaries](https://arxiv.org/abs/2602.08149)
*Sahana Ramnath,Nima Chitsazan,Mingyang Zhou,Chia-Hsuan Lee,Shi-Xiong Zhang,Stephen Rawls,Sambit Sahu,Sangwoo Cho,Xiang Ren,Genta Indra Winata,Akshaj Kumar Veldanda*

Main category: cs.CL

TL;DR: 本文介绍DIALSUMMER框架评估对话摘要，提出错误分类，构建数据集并分析，还测试LLM检测错误能力，展示数据集挑战和分类鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 先前对话摘要评估工作忽略了对话结构和叙述视角的转变等复杂性，需要新的评估框架。

Method: 引入DIALSUMMER框架，提出错误分类，在对话和轮次两个层次评估；构建人工标注错误的数据集；对标注错误进行实证分析；测试LLM检测错误的能力。

Result: 发现对话中间轮次在摘要中最易遗漏，外在幻觉多在摘要结尾出现；展示了数据集的挑战性、分类的鲁棒性。

Conclusion: 该领域未来需提升LLM在对话摘要错误检测方面的性能，代码和推理数据集即将发布。

Abstract: Dialogues are a predominant mode of communication for humans, and it is immensely helpful to have automatically generated summaries of them (e.g., to revise key points discussed in a meeting, to review conversations between customer agents and product users). Prior works on dialogue summary evaluation largely ignore the complexities specific to this task: (i) shift in structure, from multiple speakers discussing information in a scattered fashion across several turns, to a summary's sentences, and (ii) shift in narration viewpoint, from speakers' first/second-person narration, standardized third-person narration in the summary. In this work, we introduce our framework DIALSUMMER to address the above. We propose DIAL-SUMMER's taxonomy of errors to comprehensively evaluate dialogue summaries at two hierarchical levels: DIALOGUE-LEVEL that focuses on the broader speakers/turns, and WITHIN-TURN-LEVEL that focuses on the information talked about inside a turn. We then present DIAL-SUMMER's dataset composed of dialogue summaries manually annotated with our taxonomy's fine-grained errors. We conduct empirical analyses of these annotated errors, and observe interesting trends (e.g., turns occurring in middle of the dialogue are the most frequently missed in the summary, extrinsic hallucinations largely occur at the end of the summary). We also conduct experiments on LLM-Judges' capability at detecting these errors, through which we demonstrate the challenging nature of our dataset, the robustness of our taxonomy, and the need for future work in this field to enhance LLMs' performance in the same. Code and inference dataset coming soon.

</details>


### [666] [CoRect: Context-Aware Logit Contrast for Hidden State Rectification to Resolve Knowledge Conflicts](https://arxiv.org/abs/2602.08221)
*Xuhua Ma,Richong Zhang,Zhijie Nie*

Main category: cs.CL

TL;DR: RAG有知识冲突问题，现有方法有限，提出CoRect改善忠实度并减少幻觉。


<details>
  <summary>Details</summary>
Motivation: 解决RAG中模型内部参数知识覆盖检索证据导致输出不忠实的问题，且现有方法存在局限。

Method: 通过层分析找出参数抑制现象，提出CoRect，对比上下文和非上下文前向传播的对数几率，识别高参数偏差层并修正隐藏状态。

Result: 在问答和摘要基准测试中，CoRect比强基线模型持续提高了输出的忠实度并减少了幻觉。

Conclusion: CoRect能有效解决RAG的知识冲突问题，提升输出质量。

Abstract: Retrieval-Augmented Generation (RAG) often struggles with knowledge conflicts, where model-internal parametric knowledge overrides retrieved evidence, leading to unfaithful outputs. Existing approaches are often limited, relying either on superficial decoding adjustments or weight editing that necessitates ground-truth targets. Through layer-wise analysis, we attribute this failure to a parametric suppression phenomenon: specifically, in deep layers, certain FFN layers overwrite context-sensitive representations with memorized priors. To address this, we propose CoRect (Context-Aware Logit Contrast for Hidden State Rectification). By contrasting logits from contextualized and non-contextualized forward passes, CoRect identifies layers that exhibit high parametric bias without requiring ground-truth labels. It then rectifies the hidden states to preserve evidence-grounded information. Across question answering (QA) and summarization benchmarks, CoRect consistently improves faithfulness and reduces hallucinations compared to strong baselines.

</details>


### [667] [When Benign Inputs Lead to Severe Harms: Eliciting Unsafe Unintended Behaviors of Computer-Use Agents](https://arxiv.org/abs/2602.08235)
*Jaylen Jones,Zhehao Zhang,Yuting Ning,Eric Fosler-Lussier,Pierre-Luc St-Charles,Yoshua Bengio,Dawn Song,Yu Su,Huan Sun*

Main category: cs.CL

TL;DR: 本文提出首个计算机使用代理（CUA）意外行为的概念和方法框架AutoElicit，揭示了先进CUA的数百种有害意外行为，并评估了成功扰动的可迁移性。


<details>
  <summary>Details</summary>
Motivation: 当前对于CUA意外风险的探索缺乏具体特征描述和自动化方法，本文旨在填补这一空白。

Method: 提出AutoElicit框架，利用CUA执行反馈迭代扰动良性指令，以引出严重危害。

Result: 从Claude 4.5 Haiku和Opus等先进CUA中发现数百种有害意外行为，评估了成功扰动在其他前沿CUA上的可迁移性。

Conclusion: 为在现实计算机使用场景中系统分析意外行为奠定了基础。

Abstract: Although computer-use agents (CUAs) hold significant potential to automate increasingly complex OS workflows, they can demonstrate unsafe unintended behaviors that deviate from expected outcomes even under benign input contexts. However, exploration of this risk remains largely anecdotal, lacking concrete characterization and automated methods to proactively surface long-tail unintended behaviors under realistic CUA scenarios. To fill this gap, we introduce the first conceptual and methodological framework for unintended CUA behaviors, by defining their key characteristics, automatically eliciting them, and analyzing how they arise from benign inputs. We propose AutoElicit: an agentic framework that iteratively perturbs benign instructions using CUA execution feedback, and elicits severe harms while keeping perturbations realistic and benign. Using AutoElicit, we surface hundreds of harmful unintended behaviors from state-of-the-art CUAs such as Claude 4.5 Haiku and Opus. We further evaluate the transferability of human-verified successful perturbations, identifying persistent susceptibility to unintended behaviors across various other frontier CUAs. This work establishes a foundation for systematically analyzing unintended behaviors in realistic computer-use settings.

</details>


### [668] [SparseEval: Efficient Evaluation of Large Language Models by Sparse Optimization](https://arxiv.org/abs/2602.07909)
*Taolin Zhang,Hang Guo,Wang Lu,Tao Dai,Shu-Tao Xia,Jindong Wang*

Main category: cs.CL

TL;DR: 随着大语言模型规模增大，评估成本增加。本文提出SparseEval方法，通过稀疏优化实现高效基准测试，实验显示其具有低估计误差、高Kendall's τ等优势。


<details>
  <summary>Details</summary>
Motivation: 大语言模型规模增大，评估能力成本越来越高，需解决高效基准测试问题。

Method: 提出SparseEval方法，用梯度下降优化锚点权重，采用迭代细化策略选锚点，利用MLP处理稀疏优化，提出分数评估项目价值。

Result: 在多种基准测试中显示出低估计误差和高Kendall's τ，具有良好鲁棒性和实用性。

Conclusion: SparseEval方法能有效解决大语言模型高效基准测试问题，在实际场景有良好表现。

Abstract: As large language models (LLMs) continue to scale up, their performance on various downstream tasks has significantly improved. However, evaluating their capabilities has become increasingly expensive, as performing inference on a large number of benchmark samples incurs high computational costs. In this paper, we revisit the model-item performance matrix and show that it exhibits sparsity, that representative items can be selected as anchors, and that the task of efficient benchmarking can be formulated as a sparse optimization problem. Based on these insights, we propose SparseEval, a method that, for the first time, adopts gradient descent to optimize anchor weights and employs an iterative refinement strategy for anchor selection. We utilize the representation capacity of MLP to handle sparse optimization and propose the Anchor Importance Score and Candidate Importance Score to evaluate the value of each item for task-aware refinement. Extensive experiments demonstrate the low estimation error and high Kendall's~$τ$ of our method across a variety of benchmarks, showcasing its superior robustness and practicality in real-world scenarios. Code is available at {https://github.com/taolinzhang/SparseEval}.

</details>


### [669] [Language Modeling and Understanding Through Paraphrase Generation and Detection](https://arxiv.org/abs/2602.08274)
*Jan Philip Wahle*

Main category: cs.CL

TL;DR: 论文指出建模释义对计算语言模型语义理解很重要，现有方法有缺陷，提出将释义分解为语言方面分析，模型训练后在相关任务和下游应用表现更好。


<details>
  <summary>Details</summary>
Motivation: 现有的释义建模方法多将释义简化为二元决策或单一改写，无法明确哪些语言因素保留了语义，需要更细粒度和基于认知的语义等价视图。

Method: 将释义分解为其组成的语言方面（释义类型），并对模型进行相关训练。

Result: 模型在相关释义任务和下游应用中表现更好，如在剽窃检测中超越人类基线，在识别Quora重复问题上有改进。

Conclusion: 将释义分解为释义类型能为语义等价提供更细粒度和基于认知的观点，模型经此训练后有更好性能。

Abstract: Language enables humans to share knowledge, reason about the world, and pass on strategies for survival and innovation across generations. At the heart of this process is not just the ability to communicate but also the remarkable flexibility in how we can express ourselves. We can express the same thoughts in virtually infinite ways using different words and structures - this ability to rephrase and reformulate expressions is known as paraphrase. Modeling paraphrases is a keystone to meaning in computational language models; being able to construct different variations of texts that convey the same meaning or not shows strong abilities of semantic understanding. If computational language models are to represent meaning, they must understand and control the different aspects that construct the same meaning as opposed to different meanings at a fine granularity. Yet most existing approaches reduce paraphrasing to a binary decision between two texts or to producing a single rewrite of a source, obscuring which linguistic factors are responsible for meaning preservation. In this thesis, I propose that decomposing paraphrases into their constituent linguistic aspects (paraphrase types) offers a more fine-grained and cognitively grounded view of semantic equivalence. I show that even advanced machine learning models struggle with this task. Yet, when explicitly trained on paraphrase types, models achieve stronger performance on related paraphrase tasks and downstream applications. For example, in plagiarism detection, language models trained on paraphrase types surpass human baselines: 89.6% accuracy compared to 78.4% for plagiarism cases from Wikipedia, and 66.5% compared to 55.7% for plagiarism of scientific papers from arXiv. In identifying duplicate questions on Quora, models trained with paraphrase types improve over models trained on binary pairs. Furthermore, I demonstrate that...

</details>


### [670] [Latent Reasoning with Supervised Thinking States](https://arxiv.org/abs/2602.08332)
*Ido Amos,Avi Caciularu,Mor Geva,Amir Globerson,Jonathan Herzig,Lior Shani,Idan Szpektor*

Main category: cs.CL

TL;DR: 提出 Thinking States 方法，推理时处理输入，在多推理任务上表现佳。


<details>
  <summary>Details</summary>
Motivation: 链式思维推理生成 CoT 会产生高额推理成本。

Method: 每几个输入令牌生成思维令牌序列，转换回嵌入空间并添加到后续输入令牌。

Result: 在多推理任务上优于其他潜在推理方法，缩小与 CoT 差距，在状态跟踪任务上推理表现更强。

Conclusion: Thinking States 方法有效，在推理任务和长序列处理上表现良好。

Abstract: Reasoning with a chain-of-thought (CoT) enables Large Language Models (LLMs) to solve complex tasks but incurs significant inference costs due to the generation of long rationales. We propose Thinking States, a method that performs reasoning {\em while} the input is processing. Specifically, Thinking States generates sequences of thinking tokens every few input tokens, transforms the thoughts back into embedding space, and adds them to the following input tokens. This has two key advantages. First, it captures the recurrent nature of CoT, but where the thought tokens are generated as input is processing. Second, since the thoughts are represented as tokens, they can be learned from natural language supervision, and using teacher-forcing, which is parallelizable. Empirically, Thinking States outperforms other latent reasoning methods on multiple reasoning tasks, narrowing the gap to CoT on math problems, and matching its performance on 2-Hop QA with improved latency. On state-tracking tasks, we show Thinking States leads to stronger reasoning behavior than CoT, successfully extrapolating to longer sequences than seen during training.

</details>


### [671] [Dynamic Long Context Reasoning over Compressed Memory via End-to-End Reinforcement Learning](https://arxiv.org/abs/2602.08382)
*Zhuoen Chen,Dongfang Li,Meishan Zhang,Baotian Hu,Min Zhang*

Main category: cs.CL

TL;DR: 提出基于分块压缩和选择性记忆召回的高效长上下文推理框架，在多跳推理基准测试中表现良好，有精度和效率优势。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在长上下文处理中面临计算成本高、信息遗忘和上下文碎片化等问题。

Method: 将长输入分割成块，用学习的压缩器编码成压缩记忆表示，通过门控模块动态选择相关记忆块，推理模块迭代处理，压缩器和推理器通过端到端强化学习联合优化，门控模块单独训练。

Result: 在多跳推理基准测试中取得有竞争力的准确率，能将上下文长度从 7K 扩展到 175 万 tokens，与强基线相比有更好的精度 - 效率权衡，相比 MemAgent 最多可减少 2 倍 GPU 峰值内存使用和加快 6 倍推理速度。

Conclusion: 所提方法能有效解决大型语言模型长上下文处理问题，具有良好应用前景。

Abstract: Large Language Models (LLMs) face significant challenges in long-context processing, including quadratic computational costs, information forgetting, and the context fragmentation inherent in retrieval-augmented generation (RAG). We propose a cognitively inspired framework for efficient long-context inference based on chunk-wise compression and selective memory recall, rather than processing all raw tokens. The framework segments long inputs into chunks and encodes each chunk into compressed memory representations using a learned compressor. A gating module dynamically selects relevant memory blocks, which are then iteratively processed by a reasoning module with an evolving working memory to solve downstream tasks. The compressor and reasoner are jointly optimized via end-to-end reinforcement learning, while the gating module is trained separately as a classifier. Experimental results show that the proposed method achieves competitive accuracy on multi-hop reasoning benchmarks such as RULER-HQA, extrapolates context length from 7K to 1.75M tokens, and offers a favorable accuracy-efficiency trade-off compared to strong long-context baselines. In particular, it achieves up to a 2 times reduction in peak GPU memory usage and a 6 times inference speedup over MemAgent.

</details>


### [672] [Prism: Spectral-Aware Block-Sparse Attention](https://arxiv.org/abs/2602.08426)
*Xinghao Wang,Pengyu Wang,Xiaoran Liu,Fangxu Liu,Jason Chu,Kai Song,Xipeng Qiu*

Main category: cs.CL

TL;DR: 现有方法在块稀疏注意力的块选择中存在效率问题，本文提出无训练的Prism方法，能在保持精度的同时加速。


<details>
  <summary>Details</summary>
Motivation: 现有块稀疏注意力方法在识别相关块时效率低，选择开销大。

Method: 追溯标准粗粒度注意力不准确的理论原因，引入无训练的谱感知方法Prism，将块选择分解为高低频分支，并进行能量温度校准。

Result: Prism在保持与全注意力精度相当的同时，实现了最高5.1倍的加速。

Conclusion: Prism能有效提高块选择效率。

Abstract: Block-sparse attention is promising for accelerating long-context LLM pre-filling, yet identifying relevant blocks efficiently remains a bottleneck. Existing methods typically employ coarse-grained attention as a proxy for block importance estimation, but often resort to expensive token-level searching or scoring, resulting in significant selection overhead. In this work, we trace the inaccuracy of standard coarse-grained attention via mean pooling to a theoretical root cause: the interaction between mean pooling and Rotary Positional Embeddings (RoPE). We prove that mean pooling acts as a low-pass filter that induces destructive interference in high-frequency dimensions, effectively creating a "blind spot" for local positional information (e.g., slash patterns). To address this, we introduce Prism, a training-free spectral-aware approach that decomposes block selection into high-frequency and low-frequency branches. By applying energy-based temperature calibration, Prism restores the attenuated positional signals directly from pooled representations, enabling block importance estimation using purely block-level operations, thereby improving efficiency. Extensive evaluations confirm that Prism maintains accuracy parity with full attention while delivering up to $\mathbf{5.1\times}$ speedup.

</details>


### [673] [Learning to Judge: LLMs Designing and Applying Evaluation Rubrics](https://arxiv.org/abs/2602.08672)
*Clemencia Siro,Pourya Aliannejadi,Mohammad Aliannejadi*

Main category: cs.CL

TL;DR: 探讨大语言模型能否设计并应用自身评估准则，评估其准则质量，发现不同场景表现及模型差异，呼吁新方法改进评估。


<details>
  <summary>Details</summary>
Motivation: 人类定义的评估准则常与大语言模型内部对语言质量的表示不一致，需探究大语言模型能否设计和应用自己的评估准则。

Method: 引入GER - Eval，评估大语言模型定义准则的语义连贯性、评分可靠性以及与人类准则的一致性。

Result: 大语言模型能可靠生成可解释且任务感知的评估维度并在模型内应用一致，但在事实和知识密集型场景评分可靠性下降；闭源模型比开源模型有更高一致性和跨模型泛化性。

Conclusion: 评估是大语言模型的一种学习到的语言能力，模型内表现一致但跨模型有差异，需新方法联合建模人类和大语言模型评估语言以提高可靠性和可解释性。

Abstract: Large language models (LLMs) are increasingly used as evaluators for natural language generation, applying human-defined rubrics to assess system outputs. However, human rubrics are often static and misaligned with how models internally represent language quality. We introduce GER-Eval (Generating Evaluation Rubrics for Evaluation) to investigate whether LLMs can design and apply their own evaluation rubrics. We evaluate the semantic coherence and scoring reliability of LLM-defined criteria and their alignment with human criteria. LLMs reliably generate interpretable and task-aware evaluation dimensions and apply them consistently within models, but their scoring reliability degrades in factual and knowledge-intensive settings. Closed-source models such as GPT-4o achieve higher agreement and cross-model generalization than open-weight models such as Llama. Our findings position evaluation as a learned linguistic capability of LLMs, consistent within models but fragmented across them, and call for new methods that jointly model human and LLM evaluative language to improve reliability and interpretability.

</details>


### [674] [Understanding Dynamic Compute Allocation in Recurrent Transformers](https://arxiv.org/abs/2602.08864)
*Ibraheem Muhammad Moosa,Suhas Lohit,Ye Wang,Moitreya Chatterjee,Wenpeng Yin*

Main category: cs.CL

TL;DR: 本文针对token级自适应计算评估问题提出新评估范式和框架并进行分析，发现计算分配与复杂度的关系及决策时机的特点。


<details>
  <summary>Details</summary>
Motivation: 此前token级自适应计算主要用任务级指标在自然语言基准上评估，token级难度不可观测，不清楚计算分配是否与潜在复杂度一致。

Method: 引入复杂度可控的评估范式，提出支持每token可变深度计算的ANIRA框架，并进行系统分析。

Result: 无显式难度监督时计算分配能与任务复杂度对齐，但不意味着算法泛化，早期计算决策依赖静态结构线索，在线停止更能跟踪算法执行状态。

Conclusion: token级自适应计算中计算分配与复杂度有特定关系，决策时机有不同特点，即便计算分配增加也可能无法泛化到未见输入尺寸。

Abstract: Token-level adaptive computation seeks to reduce inference cost by allocating more computation to harder tokens and less to easier ones. However, prior work is primarily evaluated on natural-language benchmarks using task-level metrics, where token-level difficulty is unobservable and confounded with architectural factors, making it unclear whether compute allocation truly aligns with underlying complexity. We address this gap through three contributions. First, we introduce a complexity-controlled evaluation paradigm using algorithmic and synthetic language tasks with parameterized difficulty, enabling direct testing of token-level compute allocation. Second, we propose ANIRA, a unified recurrent Transformer framework that supports per-token variable-depth computation while isolating compute allocation decisions from other model factors. Third, we use this framework to conduct a systematic analysis of token-level adaptive computation across alignment with complexity, generalization, and decision timing. Our results show that compute allocation aligned with task complexity can emerge without explicit difficulty supervision, but such alignment does not imply algorithmic generalization: models fail to extrapolate to unseen input sizes despite allocating additional computation. We further find that early compute decisions rely on static structural cues, whereas online halting more closely tracks algorithmic execution state.

</details>


### [675] [Affective Flow Language Model for Emotional Support Conversation](https://arxiv.org/abs/2602.08826)
*Chenghui Zou,Ning Wang,Tiesunlong Shen,Luwei Xiao,Chuan Ma,Xiangpeng Li,Rui Mao,Erik Cambria*

Main category: cs.CL

TL;DR: 现有大语言模型在复杂多轮情感支持对话有挑战，本文提出AFlow框架，其能实现细粒度监督并传播偏好信号，实验显示优于对比基线和专有大模型。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型应用于复杂多轮情感支持对话时，因依赖稀疏的结果级信号，对中间策略决策的监督有限。

Method: 提出AFlow框架，沿多轮轨迹建模连续情感流以对对话前缀进行细粒度监督，引入子路径级流平衡目标传播偏好信号。

Result: 在多样情感语境下，相比竞争基线有一致且显著提升，轻量级开源模型在主要情感支持对话指标上优于GPT - 4o和Claude - 3.5等专有大模型。

Conclusion: AFlow框架有效提升了复杂多轮情感支持对话的策略连贯性和共情回复质量。

Abstract: Large language models (LLMs) have been widely applied to emotional support conversation (ESC). However, complex multi-turn support remains challenging.This is because existing alignment schemes rely on sparse outcome-level signals, thus offering limited supervision for intermediate strategy decisions. To fill this gap, this paper proposes affective flow language model for emotional support conversation (AFlow), a framework that introduces fine-grained supervision on dialogue prefixes by modeling a continuous affective flow along multi-turn trajectories. AFlow can estimate intermediate utility over searched trajectories and learn preference-consistent strategy transitions. To improve strategy coherence and empathetic response quality, a subpath-level flow-balance objective is presented to propagate preference signals to intermediate states. Experiment results show consistent and significant improvements over competitive baselines in diverse emotional contexts. Remarkably, AFlow with a compact open-source backbone outperforms proprietary LMMs such as GPT-4o and Claude-3.5 on major ESC metrics. Our code is available at https://github.com/chzou25-lgtm/AffectiveFlow.

</details>


### [676] [WildReward: Learning Reward Models from In-the-Wild Human Interactions](https://arxiv.org/abs/2602.08829)
*Hao Peng,Yunjia Qi,Xiaozhi Wang,Zijun Yao,Lei Hou,Juanzi Li*

Main category: cs.CL

TL;DR: 本文探讨直接从自然交互中开发奖励模型，采用WildChat提取反馈训练WildReward，实验表明其性能佳且应用于在线DPO训练有显著提升。


<details>
  <summary>Details</summary>
Motivation: 奖励模型通常依赖大规模人工标注偏好对，而自然交互产生的隐式奖励信号丰富，因此探讨能否直接从自然交互中开发奖励模型。

Method: 采用WildChat作为交互源，提出提取可靠人类反馈的流程，通过对用户反馈进行序回归直接训练WildReward，无需偏好对。

Result: WildReward与传统奖励模型相比性能相当甚至更优，校准和跨样本一致性更好，且直接受益于用户多样性，应用于在线DPO训练在各任务上有显著改进。

Conclusion: 可以直接从自然交互中开发奖励模型，WildReward有良好表现和应用价值。

Abstract: Reward models (RMs) are crucial for the training of large language models (LLMs), yet they typically rely on large-scale human-annotated preference pairs. With the widespread deployment of LLMs, in-the-wild interactions have emerged as a rich source of implicit reward signals. This raises the question: Can we develop reward models directly from in-the-wild interactions? In this work, we explore this possibility by adopting WildChat as an interaction source and proposing a pipeline to extract reliable human feedback, yielding 186k high-quality instances for training WildReward via ordinal regression directly on user feedback without preference pairs. Extensive experiments demonstrate that WildReward achieves comparable or even superior performance compared to conventional reward models, with improved calibration and cross-sample consistency. We also observe that WildReward benefits directly from user diversity, where more users yield stronger reward models. Finally, we apply WildReward to online DPO training and observe significant improvements across various tasks. Code and data are released at https://github.com/THU-KEG/WildReward.

</details>


### [677] [Next Concept Prediction in Discrete Latent Space Leads to Stronger Language Models](https://arxiv.org/abs/2602.08984)
*Yuliang Liu,Yunchong Song,Yixuan Wang,Kewen Ge,Alex Lamb,Qipeng Guo,Kai Chen,Bowen Zhou,Zhouhan Lin*

Main category: cs.CL

TL;DR: 提出基于NTP的Next Concept Prediction（NCP）预训练范式，构建ConceptLM模型，实验表明NCP能提升语言模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统token级模型有局限，想通过更具挑战性的预训练目标提升语言模型能力。

Method: 提出NCP范式，ConceptLM模型用向量量化构建概念词汇表，结合NCP和NTP更新参数。

Result: 在13个基准测试中NCP比传统token级模型性能提升，在Llama模型上持续预训练实验也有改进。

Conclusion: NCP通过引入更难的预训练任务产生更强的语言模型，是语言建模的有前景方向。

Abstract: We propose Next Concept Prediction (NCP), a generative pretraining paradigm built on top of Next Token Prediction (NTP). NCP predicts discrete concepts that span multiple tokens, thereby forming a more challenging pretraining objective. Our model, ConceptLM, quantizes hidden states using Vector Quantization and constructs a concept vocabulary. It leverages both NCP and NTP to drive parameter updates and generates a concept to guide the generation of the following tokens. We train ConceptLM from scratch at scales ranging from 70M to 1.5B parameters with up to 300B training data, including Pythia and GPT-2 backbones. Results on 13 benchmarks show that NCP yields consistent performance gains over traditional token-level models. Furthermore, continual pretraining experiments on an 8B-parameter Llama model indicate that NCP can further improve an NTP-trained model. Our analysis suggests that NCP leads to more powerful language models by introducing a harder pretraining task, providing a promising path toward better language modeling.

</details>


<div id='q-bio.PE'></div>

# q-bio.PE [[Back]](#toc)

### [678] [Punishment in bipartite societies](https://arxiv.org/abs/2602.07553)
*Sinan Feng,Genjiu Xu,Yu Chen,Chaoqian Wang,Attila Szolnoki*

Main category: q-bio.PE

TL;DR: 研究二分群体中群体间惩罚对合作的影响，发现对称群体间惩罚能促进并稳定合作，且更利于社会福利，不对称则抑制合作。


<details>
  <summary>Details</summary>
Motivation: 现有研究多聚焦同质群体惩罚，忽视子集归属对合作动态的影响，本文旨在填补此空白。

Method: 研究公共物品博弈中二分群体里，一方合作者仅惩罚另一方背叛者的情况。

Result: 对称群体间惩罚下合作可出现并稳定，低惩罚强度和低增强因子时，群体间惩罚比统一惩罚更有效，且更利于社会福利；不对称则抑制合作。

Conclusion: 群体间惩罚的对称性是人类和生物系统合作背后的统一原则。

Abstract: From ant-acacia mutualism to performative conflict resolution among Inuit, dedicated punishments between distinct subsets of a population are widespread and can reshape the evolutionary trajectory of cooperation. Existing studies have focused on punishments within a homogeneous population, paying little attention to cooperative dynamics in a situation where belonging to a subset is equally important to the actual strategy represented by an actor. To fill this gap, we here study a bipartite population where cooperator agents in a public goods game penalize exclusively those defectors who belong to the alternative subset. We find that cooperation can emerge and remain stable under symmetric intergroup punishment. In particular, at low punishment intensity and at a small value of the enhancement factor of the dilemma game, intergroup punishment promotes cooperation more effectively than a uniformly applied punishment. Moreover, intergroup punishment in bipartite populations tends to be more favorable for overall social welfare. When this incentive is balanced, cooperators can collectively restrain defectors of the alternative set via aggregate interactions in a randomly formed working group, offering a more effective incentive. Conversely, breaking the symmetry of intergroup punishment inhibits cooperation, as the imbalance creates an Achilles' heel in the enforcement structure. Our work, thus, reveals symmetry in intergroup punishment as a unifying principle behind cooperation across human and biological systems.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [679] [Wireless Streamlet: A Spectrum-Aware and Cognitive Consensus Protocol for Edge IoT](https://arxiv.org/abs/2602.07630)
*Taotao Wang,Long Shi,Fang Liu,Qing Yang,Shengli Zhang*

Main category: cs.IT

TL;DR: 提出适用于无线边缘物联网的频谱感知和认知共识协议Wireless Streamlet，实验表明其在有损环境中表现更优。


<details>
  <summary>Details</summary>
Motivation: 传统BFT协议在频谱拥塞和动态无线边缘物联网网络中部署共识存在资源利用低效和进度脆弱的问题。

Method: 基于Streamlet结构，引入Channel - Aware Leader Election (CALE)机制；利用单跳广播介质和确定性TDMA投票调度；提出编码双链架构。

Result: Wireless Streamlet在有损环境中比代表性基线实现了更高吞吐量和更低确认延迟，大幅减少了每节点存储。

Conclusion: 将认知感知集成到共识逻辑中是有效的。

Abstract: Blockchain offers a decentralized trust framework for the Internet of Things (IoT), yet deploying consensus in spectrum-congested and dynamic wireless edge IoT networks faces fundamental obstacles: traditional BFT protocols are spectrum-ignorant, leading to inefficient resource utilization and fragile progress under time-varying interference. This paper presents \textit{Wireless Streamlet}, a spectrum-aware and cognitive consensus protocol tailored for wireless edge IoT. Building on Streamlet's streamlined structure, we introduce a \textit{Channel-Aware Leader Election (CALE)} mechanism. CALE serves as a verifiable cross-layer cognitive engine that leverages receiver-measured channel state information (CSI) piggybacked in signed votes to derive Byzantine-robust connectivity scores from notarization certificates, and deterministically selects a unique weighted leader per epoch from finalized history, thereby improving proposal dissemination reliability under deep fading. Complementing this cognitive adaptation, Wireless Streamlet exploits the single-hop broadcast medium and a deterministic TDMA voting schedule to achieve linear per-epoch on-air transmissions (slot complexity), ensuring deterministic spectral access. To address the communication-storage trade-off, we further propose a coded dual-chain architecture that decouples header-only consensus (State Chain) from payload data (Data Chain). By employing erasure coding and on-chain integrity commitments, the system minimizes redundant spectrum usage for data retrieval while ensuring availability. Experiments show that Wireless Streamlet achieves higher throughput and lower confirmation latency than representative baselines in lossy environments, while substantially reducing per-node storage, demonstrating the efficacy of integrating cognitive sensing into consensus logic.

</details>


### [680] [Trellis codes with a good distance profile constructed from expander graphs](https://arxiv.org/abs/2602.08718)
*Yubin Zhu,Zitan Chen*

Main category: cs.IT

TL;DR: 推导格状码自由距离和列距离的Singleton型界，构造在常规模字母表上接近最大距离谱卷积码的格状码。


<details>
  <summary>Details</summary>
Motivation: 研究格状码与卷积码距离的特性以及构造性能良好的格状码。

Method: 推导界，使用扩展图来构造格状码。

Result: 给定时刻格状码最大可达到列距离能超过卷积码；构造出在常规模字母表上接近最大距离谱卷积码速率 - 距离权衡的格状码。

Conclusion: 格状码在距离性能上有超过卷积码的潜力，且可在常规模字母表上有效构造接近卷积码性能的码。

Abstract: We derive Singleton-type bounds on the free distance and column distances of trellis codes. Our results show that, at a given time instant, the maximum attainable column distance of trellis codes can exceed that of convolutional codes. Moreover, using expander graphs, we construct trellis codes over constant-size alphabets that achieve a rate-distance trade-off arbitrarily close to that of convolutional codes with a maximum distance profile. By comparison, all known constructions of convolutional codes with a maximum distance profile require working over alphabets whose size grows at least exponentially with the number of output symbols per time instant.

</details>


### [681] [Deep Variable-Length Feedback Codes](https://arxiv.org/abs/2602.07881)
*Yu Ding,Yulin Shao*

Main category: cs.IT

TL;DR: 本文提出DeepVLF编码框架，含两种架构，能动态调整传输长度，在多个信道上表现优于现有方案，还揭示了码的两阶段策略。


<details>
  <summary>Details</summary>
Motivation: 现有基于反馈的深度学习信道编码方案有固定块长、高码率性能下降、无法充分利用反馈自适应潜力等局限。

Method: 提出DeepVLF编码框架，包含接收器驱动终端的DeepVLF - R和发射器控制终端的DeepVLF - T两种架构，利用位组划分和基于Transformer的编解码器网络实现速率自适应。

Result: 在AWGN和5G - NR衰落信道上，DeepVLF显著优于现有方案，减少20% - 55%的信道使用，大幅降低误码率，尤其在高码率场景。

Conclusion: DeepVLF框架有效，学习到的码具有可解释性且符合信息论，能动态调整传输长度进行高效信道编码。

Abstract: Deep learning has enabled significant advances in feedback-based channel coding, yet existing learned schemes remain fundamentally limited: they employ fixed block lengths, suffer degraded performance at high rates, and cannot fully exploit the adaptive potential of feedback. This paper introduces Deep Variable-Length Feedback (DeepVLF) coding, a flexible coding framework that dynamically adjusts transmission length via learned feedback. We propose two complementary architectures: DeepVLF-R, where termination is receiver-driven, and DeepVLF-T, where the transmitter controls termination. Both architectures leverage bit-group partitioning and transformer-based encoder-decoder networks to enable fine-grained rate adaptation in response to feedback. Evaluations over AWGN and 5G-NR fading channels demonstrate that DeepVLF substantially outperforms state-of-the-art learned feedback codes. It achieves the same block error rate with 20%-55% fewer channel uses and lowers error floors by orders of magnitude, particularly in high-rate regimes. Encoding dynamics analysis further reveals that the models autonomously learn a two-phase strategy analogous to classical Schalkwijk-Kailath coding: an initial information-carrying phase followed by a noise-cancellation refinement phase. This emergent behavior underscores the interpretability and information-theoretic alignment of the learned codes.

</details>


### [682] [Rich-ARQ: From 1-bit Acknowledgment to Rich Neural Coded Feedback](https://arxiv.org/abs/2602.07886)
*Enhao Chen,Yulin Shao*

Main category: cs.IT

TL;DR: 本文提出Rich - ARQ范式，用高维信息向量改进无线通信反馈机制，实现主动协作，开发新异步反馈码，制作原型，实验显示其性能优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 将无线通信中被动确认转变为主动协作，改进现有的1位二进制ACK/NACK反馈机制。

Method: 提出Rich - ARQ范式，引入神经编码反馈用于协作物理层信道编码，开发新的异步反馈码，制作全栈、符合标准的软件定义无线电原型。

Result: Rich - ARQ相比传统1位混合ARQ实现显著SNR增益，相比先前基于学习的反馈码显著降低延迟。

Conclusion: Rich - ARQ将智能反馈从理论变为适用于下一代网络的高性能现实。

Abstract: This paper reimagines the foundational feedback mechanism in wireless communication, transforming the prevailing 1-bit binary ACK/NACK with a high-dimensional, information-rich vector to transform passive acknowledgment into an active collaboration. We present Rich-ARQ, a paradigm that introduces neural-coded feedback for collaborative physical-layer channel coding between transmitter and receiver. To realize this vision in practice, we develop a novel asynchronous feedback code that eliminates stalling from feedback delays, adapts dynamically to channel fluctuations, and features a lightweight encoder suitable for on-device deployment. We materialize this concept into the first full-stack, standard-compliant software-defined radio prototype, which decouples AI inference from strict radio timing. Comprehensive over-the-air experiments demonstrate that Rich-ARQ achieves significant SNR gains over conventional 1-bit hybrid ARQ and remarkable latency reduction over prior learning-based feedback codes, moving the promise of intelligent feedback from theory to a practical, high-performance reality for next-generation networks.

</details>


### [683] [Tighter Information-Theoretic Generalization Bounds via a Novel Class of Change of Measure Inequalities](https://arxiv.org/abs/2602.07999)
*Yanxiao Liu,Yijun Fan an Deniz Gündüz*

Main category: cs.IT

TL;DR: 本文提出基于f - 散度数据处理不等式的测度变换不等式新类别，用于分析随机学习算法泛化误差，得到新且更紧的泛化界，框架灵活可适应多种场景。


<details>
  <summary>Details</summary>
Motivation: 获得更紧的测度变换不等式，并用于随机学习算法泛化误差分析。

Method: 基于f - 散度的数据处理不等式构建统一框架，得到关于多种信息度量的测度变换不等式，并将其嵌入随机学习算法泛化误差分析。

Result: 得到新且更紧的高概率信息论泛化界，恢复一些已知最佳结果，还为多种场景导出新的泛化界。

Conclusion: 所提出的框架灵活有效，能适应多种场景，可用于随机学习算法泛化误差分析。

Abstract: In this paper, we propose a novel class of change of measure inequalities via a unified framework based on the data processing inequality for $f$-divergences, which is surprisingly elementary yet powerful enough to yield tighter inequalities. We provide change of measure inequalities in terms of a broad family of information measures, including $f$-divergences (with Kullback-Leibler divergence and $χ^2$-divergence as special cases), Rényi divergence, and $α$-mutual information (with maximal leakage as a special case). We then embed these inequalities into the analysis of generalization error for stochastic learning algorithms, yielding novel and tighter high-probability information-theoretic generalization bounds, while also recovering several best-known results via simplified analyses. A key advantage of our framework is its flexibility: it readily adapts to a range of settings, including the conditional mutual information framework, PAC-Bayesian theory, and differential privacy mechanisms, for which we derive new generalization bounds.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [684] [Functional Estimation of the Marginal Likelihood](https://arxiv.org/abs/2602.07148)
*Omiros Papaspiliopoulos,Timothée Stumpf-Fétizon,Jonathan Weare*

Main category: stat.ME

TL;DR: 提出一个用于统计模型中平滑边际似然的计算、优化和整合框架，展示其与多种方法的关联及一致性，并在一些模型中进行验证。


<details>
  <summary>Details</summary>
Motivation: 解决涉及高维参数/潜在变量和连续低维超参数的统计模型中平滑边际似然的计算、优化和整合问题。

Method: 在模拟网格上针对不同超参数值从参数的后验分布中抽样，对边际似然及其泛函进行推断。

Result: 建立了所提估计量在抽样努力增加时的一致性，包括模拟网格固定和变密的情况。

Conclusion: 该框架在高斯过程回归、分类和交叉效应模型等方面有应用价值。

Abstract: We propose a framework for computing, optimizing and integrating with respect to a smooth marginal likelihood in statistical models that involve high-dimensional parameters/latent variables and continuous low-dimensional hyperparameters. The method requires samples from the posterior distribution of the parameters for different values of the hyperparameters on a simulation grid and returns inference on the marginal likelihood defined everywhere on its domain, and on its functionals. We show how the method relates to many of the methods that have been used in this context, including sequential Monte Carlo, Gibbs sampling, Monte Carlo maximum likelihood, and umbrella sampling. We establish the consistency of the proposed estimators as the sampling effort increases, both when the simulation grid is kept fixed and when it becomes dense in the domain. We showcase the approach on Gaussian process regression and classification and crossed effect models.

</details>


### [685] [Estimation of log-Gaussian gamma processes with iterated posterior linearization and Hamiltonian Monte Carlo](https://arxiv.org/abs/2602.07454)
*Teemu Härkönen,Simo Särkkä*

Main category: stat.ME

TL;DR: 本文提出两种方法用于对数高斯伽马过程潜在模型的后验分布采样，并通过合成数据集和实际实验验证。


<details>
  <summary>Details</summary>
Motivation: 随机过程统计建模中，非高斯误差模型推理通常难以处理，需估计高维潜在变量，因此需要新方法。

Method: 采用迭代后验线性化，随后使用哈密顿蒙特卡罗方法对对数高斯伽马过程潜在模型的后验分布进行采样。

Result: 通过对数高斯伽马过程生成的两个合成数据集和多尺度生物复合材料刚度模型验证了方法，还应用于辉银矿的拉曼光谱实验。

Conclusion: 所提出的方法可用于处理具有非高斯误差的随机过程模型的推理问题。

Abstract: Stochastic processes are a flexible and widely used family of models for statistical modeling. While stochastic processes offer attractive properties such as inclusion of uncertainty properties, their inference is typically intractable, with the notable exception of Gaussian processes. Inference of models with non-Gaussian errors typically involves estimation of a high-dimensional latent variable. We propose two methods that use iterated posterior linearization followed by Hamiltonian Monte Carlo to sample the posterior distributions of such latent models with a particular focus on log-Gaussian gamma processes. The proposed methods are validated with two synthetic datasets generated from the log-Gaussian gamma process and a multiscale biocomposite stiffness model. In addition, we apply the methodology to an experimental Raman spectrum of argentopyrite.

</details>


### [686] [Statistical inference after variable selection in Cox models: A simulation study](https://arxiv.org/abs/2602.07477)
*Lena Schemet,Sarah Friedrich-Welz*

Main category: stat.ME

TL;DR: 研究Cox模型中Lasso及自适应Lasso变量选择后的推断程序，通过模拟和实例分析其性能。


<details>
  <summary>Details</summary>
Motivation: 经典频率论推断未考虑数据驱动变量选择，右删失生存数据中问题更严重，需研究合适推断程序。

Method: 研究样本分割、精确后选择推断和去偏Lasso方法，通过中性模拟研究评估性能，并结合公开生存数据集实例分析。

Result: 论文未明确给出结果，模拟和实例分析用于考察方法性能。

Conclusion: 未提及明确结论。

Abstract: Choosing relevant predictors is central to the analysis of biomedical time-to-event data. Classical frequentist inference, however, presumes that the set of covariates is fixed in advance and does not account for data-driven variable selection. As a consequence, naive post-selection inference may be biased and misleading. In right-censored survival settings, these issues may be further exacerbated by the additional uncertainty induced by censoring. We investigate several inference procedures applied after variable selection for the coefficients of the Lasso and its extension, the adaptive Lasso, in the context of the Cox model. The methods considered include sample splitting, exact post-selection inference, and the debiased Lasso. Their performance is examined in a neutral simulation study reflecting realistic covariate structures and censoring rates commonly encountered in biomedical applications. To complement the simulation results, we illustrate the practical behavior of these procedures in an applied example using a publicly available survival dataset.

</details>


### [687] [Fast Rerandomization for Balancing Covariates in Randomized Experiments: A Metropolis-Hastings Framework](https://arxiv.org/abs/2602.07613)
*Jiuyao Lu,Tianruo Zhang,Ke Zhu*

Main category: stat.ME

TL;DR: 平衡协变量对随机实验至关重要，重随机化可实现协变量平衡，但传统方法效率低，提出PSRSRR算法在保持有效性的同时加速。


<details>
  <summary>Details</summary>
Motivation: 重随机化因拒绝采样效率低限制实际应用，现有加速方法难保持分配空间均匀性，缺乏理论基础。

Method: 在Metropolis - Hastings框架基础上引入额外的抽样重要性重采样步骤。

Result: PSRSRR算法实现了10到10000倍的加速，经模拟和两个真实数据应用验证。

Conclusion: PSRSRR算法在提高速度的同时能维持精确和渐近有效性。

Abstract: Balancing covariates is critical for credible and efficient randomized experiments. Rerandomization addresses this by repeatedly generating treatment assignments until covariate balance meets a prespecified threshold. By shrinking this threshold, it can achieve arbitrarily strong balance, with established results guaranteeing optimal estimation and valid inference in both finite-sample and asymptotic settings across diverse complex experimental settings. Despite its rigorous theoretical foundations, practical use is limited by the extreme inefficiency of rejection sampling, which becomes prohibitively slow under small thresholds and often forces practitioners to adopt suboptimal settings, leading to degraded performance. Existing work focusing on acceleration typically fail to maintain the uniformity over the acceptable assignment space, thus losing the theoretical grounds of classical rerandomization. Building upon a Metropolis-Hastings framework, we address this challenge by introducing an additional sampling-importance resampling step, which restores uniformity and preserves statistical guarantees. Our proposed algorithm, PSRSRR, achieves speedups ranging from 10 to 10,000 times while maintaining exact and asymptotic validity, as demonstrated by simulations and two real-data applications.

</details>


### [688] [Adaptive Markovian Spatiotemporal Transfer Learning in Multivariate Bayesian Modeling](https://arxiv.org/abs/2602.08544)
*Luca Presicce,Sudipto Banerjee*

Main category: stat.ME

TL;DR: 本文开发了一种计算高效的多变量时空模型在线学习方法，该方法通过矩阵变元高斯分布、动态线性模型和贝叶斯预测堆叠实现高效信息共享，结合顺序和并行处理提高性能，提升了时空建模的可扩展性和适应性。


<details>
  <summary>Details</summary>
Motivation: 解决复杂时空现象高维建模时的计算挑战，实现高效的多变量时空模型在线学习。

Method: 依靠矩阵变元高斯分布、动态线性模型和贝叶斯预测堆叠在时间数据分片间共享信息，结合顺序和并行处理时间分片，利用预测堆叠进行精确推断。

Result: 构建了连续时间数据集间的马尔可夫依赖结构，支持复杂依赖模式的高维建模，提高了后验估计的准确性和互操作性。

Conclusion: 该框架提升了时空建模的可扩展性和适应性，适用于动态、多变量和数据丰富的环境。

Abstract: This manuscript develops computationally efficient online learning for multivariate spatiotemporal models. The method relies on matrix-variate Gaussian distributions, dynamic linear models, and Bayesian predictive stacking to efficiently share information across temporal data shards. The model facilitates effective information propagation over time while seamlessly integrating spatial components within a dynamic framework, building a Markovian dependence structure between datasets at successive time instants. This structure supports flexible, high-dimensional modeling of complex dependence patterns, as commonly found in spatiotemporal phenomena, where computational challenges arise rapidly with increasing dimensions. The proposed approach further manages exact inference through predictive stacking, enhancing accuracy and interoperability. Combining sequential and parallel processing of temporal shards, each unit passes assimilated information forward, then back-smoothed to improve posterior estimates, incorporating all available information. This framework advances the scalability and adaptability of spatiotemporal modeling, making it suitable for dynamic, multivariate, and data-rich environments.

</details>


### [689] [Conformal changepoint localization](https://arxiv.org/abs/2602.06267)
*Rohan Hore,Aaditya Ramdas*

Main category: stat.ME

TL;DR: 研究无分布假设下离线变点定位问题，提出无分布算法CONCH构建有限样本覆盖置信集，实验显示其在复杂场景有效。


<details>
  <summary>Details</summary>
Motivation: 现有变点定位方法常依赖参数假设、尾部条件或渐近逼近等，本文旨在无其他假设下，为变点位置生成有限样本置信集。

Method: 提出无分布算法CONCH，利用可交换性构建置信集，通过证明共形Neyman - Pearson引理推导得分函数。

Result: 得分函数下，置信集标准化长度在弱假设下收缩为零；任意无分布变点定位方法都是CONCH的实例；实验显示CONCH在涉及图像或文本的场景能提供精确置信集。

Conclusion: CONCH算法在无分布假设下能有效进行变点定位，构建精确置信集。

Abstract: We study the problem of offline changepoint localization in a distribution-free setting. One observes a vector of data with a single changepoint, assuming that the data before and after the changepoint are iid (or more generally exchangeable) from arbitrary and unknown distributions. The goal is to produce a finite-sample confidence set for the index at which the change occurs without making any other assumptions. Existing methods often rely on parametric assumptions, tail conditions, or asymptotic approximations, or only produce point estimates. In contrast, our distribution-free algorithm, CONformal CHangepoint localization (CONCH), only leverages exchangeability arguments to construct confidence sets with finite sample coverage. By proving a conformal Neyman-Pearson lemma, we derive principled score functions that yield informative (small) sets. Moreover, with such score functions, the normalized length of the confidence set shrinks to zero under weak assumptions. We also establish a universality result showing that any distribution-free changepoint localization method must be an instance of CONCH. Experiments suggest that CONCH delivers precise confidence sets even in challenging settings involving images or text.

</details>


### [690] [GAAVI: Global Asymptotic Anytime Valid Inference for the Conditional Mean Function](https://arxiv.org/abs/2602.08096)
*Brian M Cho,Raaz Dwivedi,Nathan Kallus*

Main category: stat.ME

TL;DR: 提出针对条件均值函数（CMF）全局零假设及对比的渐近随时有效测试，能构造函数值的渐近置信序列，实验验证方法有效。


<details>
  <summary>Details</summary>
Motivation: 在自适应实验、最优治疗分配和算法公平性审计等任务中，CMF推断很重要，以往测试存在不足，需新方法。

Method: 提供新的渐近随时有效测试，设定温和条件，通过反转测试构造函数值的渐近置信序列。

Result: 测试在温和条件下实现渐近一类错误保证、功效为1，有最优样本复杂度，实验表明方法在不同分布下有功效且保持名义错误率。

Conclusion: 新方法可让实验者在实验中随时做高置信度决策，有良好实用性能。

Abstract: Inference on the conditional mean function (CMF) is central to tasks from adaptive experimentation to optimal treatment assignment and algorithmic fairness auditing. In this work, we provide a novel asymptotic anytime-valid test for a CMF global null (e.g., that all conditional means are zero) and contrasts between CMFs, enabling experimenters to make high confidence decisions at any time during the experiment beyond a minimum sample size. We provide mild conditions under which our tests achieve (i) asymptotic type-I error guarantees, (i) power one, and, unlike past tests, (iii) optimal sample complexity relative to a Gaussian location testing. By inverting our tests, we show how to construct function-valued asymptotic confidence sequences for the CMF and contrasts thereof. Experiments on both synthetic and real-world data show our method is well-powered across various distributions while preserving the nominal error rate under continuous monitoring.

</details>


### [691] [Mapping Drivers of Greenness: Spatial Variable Selection for MODIS Vegetation Indices](https://arxiv.org/abs/2602.07681)
*Qishi Zhan,Cheng-Han Yu,Yuchi Chen,Zhikang Dong,Rajarshi Guhaniyogi*

Main category: stat.ME

TL;DR: 本文提出空间变系数模型，结合张量积B样条基和贝叶斯组套索先验，处理环境驱动与植被状况关系，模拟和应用效果良好。


<details>
  <summary>Details</summary>
Motivation: 理解环境驱动因素与植被状况的关系，传统方法在处理多无关预测变量时存在问题，需要更好的方法。

Method: 提出空间变系数模型，系数表面使用张量积B样条基和贝叶斯组套索先验，后验推断用马尔可夫链蒙特卡罗方法。

Result: 模拟能恢复稀疏性并实现预测，MODIS应用得到简约预测变量子集，效应图明确景观主导控制因素。

Conclusion: 该模型能有效处理环境驱动与植被状况关系，识别重要预测变量，提供不确定性量化。

Abstract: Understanding how environmental drivers relate to vegetation condition motivates spatially varying regression models, but estimating a separate coefficient surface for every predictor can yield noisy patterns and poor interpretability when many predictors are irrelevant. Motivated by MODIS vegetation index studies, we examine predictors from spectral bands, productivity and energy fluxes, observation geometry, and land surface characteristics. Because these relationships vary with canopy structure, climate, land use, and measurement conditions, methods should both model spatially varying effects and identify where predictors matter. We propose a spatially varying coefficient model where each coefficient surface uses a tensor product B-spline basis and a Bayesian group lasso prior on the basis coefficients. This prior induces predictor level shrinkage, pushing negligible effects toward zero while preserving spatial structure. Posterior inference uses Markov chain Monte Carlo and provides uncertainty quantification for each effect surface. We summarize retained effects with spatial significance maps that mark locations where the 95 percent posterior credible interval excludes zero, and we define a spatial coverage probability as the proportion of locations where the credible interval excludes zero. Simulations recover sparsity and achieve prediction. A MODIS application yields a parsimonious subset of predictors whose effect maps clarify dominant controls across landscapes.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [692] [SurfAge-Net: A Hierarchical Surface-Based Network for Interpretable Fine-Grained Brain Age Prediction](https://arxiv.org/abs/2602.06994)
*Rongzhao He,Dalin Zhu,Ying Wang,Songhong Yue,Leilei Zhao,Yu Fu,Dan Wu,Bin Hu,Weihao Zheng*

Main category: q-bio.NC

TL;DR: 提出基于球面的脑年龄预测网络SurfAge - Net，在多数据集验证中表现优于现有方法，支持神经发育研究和临床早期评估。


<details>
  <summary>Details</summary>
Motivation: 现有脑年龄预测方法多关注全脑，忽略脑成熟的区域异质性，本文旨在解决该局限。

Method: 提出SurfAge - Net，结合皮层组织的连接组原理，通过空间通道混合和侧化感知注意力机制建模脑区依赖关系。

Result: 在三个胎儿和新生儿数据集验证中，SurfAge - Net优于现有方法，有强泛化性，能提供皮层成熟的可解释图。

Conclusion: 精细的脑年龄预测是推动神经发育研究和临床早期评估的有前景范式。

Abstract: Brain age prediction serves as a powerful framework for assessing brain status and detecting deviations associated with neurodevelopmental and neurodegenerative disorders. However, most existing approaches emphasize whole-brain age prediction and therefore overlook the pronounced regional heterogeneity of brain maturation that is crucial for detecting localized atypical trajectories. To address this limitation, we propose a novel spherical surface-based brain age prediction network (SurfAge-Net) that leverages multiple morphological metrics to capture region-specific developmental patterns with enhanced robustness and clinical interpretability. SurfAge-Net establishes a new modeling paradigm by incorporating the connectomic principles of cortical organization: it explicitly models both intra- and inter-hemispheric dependencies through a spatial-channel mixing and a lateralization-aware attention mechanism, enabling the network to characterize the coordinate maturation pattern uniquely associated with each target region. Validated on three fetal and neonatal datasets, SurfAge-Net outperforms existing approaches (global MAE = 0.54, regional MAE = 0.45 in gestational/postmenstrual weeks) and demonstrates strong generalizability across external cohorts. Importantly, it provides spatially precise and biologically interpretable maps of cortical maturation, effectively identifying heterogeneous delays and regional-specific abnormalities in atypical developmental populations. These results established fine-grained brain age prediction as a promising paradigm for advancing neurodevelopmental research and supporting early clinical assessment.

</details>


### [693] [Cognitive algorithms and systems of episodic memory, semantic memory and their learnings](https://arxiv.org/abs/2602.07261)
*Qi Zhang*

Main category: q-bio.NC

TL;DR: 本文介绍陈述性记忆由情景记忆和语义记忆组成，二者既分离又相关，海马损伤会导致记忆障碍，还回顾了模拟外显记忆及相关记忆障碍的认知系统。


<details>
  <summary>Details</summary>
Motivation: 通过研究海马损伤导致的记忆障碍，了解情景记忆和语义记忆的获取、存储和组织方式。

Method: 回顾了几种以模拟外显记忆为中心的认知系统，以及基于神经解剖学模拟记忆障碍的系统，包括其结构、学习规则和对记忆获取及障碍的模拟。

Result: 对相关认知系统及模拟情况进行了梳理回顾。

Conclusion: 未明确提及结论性内容。

Abstract: Declarative memory, the memory that can be "declared" in words or languages, is made up of two dissociated parts: episodic memory and semantic memory. This dissociation has its neuroanatomical basis episodic memory is mostly associated with the hippocampus and semantic memory with the neocortex. The two memories, on the other hand, are closely related. Lesions in the hippocampus often result in various impairments of explicit memory, e.g., anterograde, retrograde and developmental amnesias, and semantic learning deficit. These impairments provide opportunities for us to understand how the two memories may be acquired, stored and organized. This chapter reviews several cognitive systems that are centered to mimic explicit memory, and other systems that are neuroanatomically based and are implemented to simulate those memory impairments mentioned above. This review includes: the structures of the computational systems, their learning rules, and their simulations of memory acquisition and impairments.

</details>


### [694] [Linguistic properties and model scale in brain encoding: from small to compressed language models](https://arxiv.org/abs/2602.07547)
*Subba Reddy Oota,Vijay Rowtula,Satya Sai Srinath Namburi,Khushbu Pahwa,Anant Khandelwal,Manish Gupta,Tanmoy Chakraborty,Bapi S. Raju*

Main category: q-bio.NC

TL;DR: 研究探索语言模型规模和精度对脑活动对齐的影响，发现适度规模模型即可达到脑预测性且对压缩有抗性。


<details>
  <summary>Details</summary>
Motivation: 不清楚大语言模型与人类脑活动对齐提升的驱动因素及所需最小模型容量。

Method: 系统研究限制模型规模和数值精度对脑对齐的影响，对比全精度大语言模型、小语言模型及压缩变体预测fMRI响应。

Result: 3B小语言模型脑预测性与大语言模型无差异，1B模型显著下降；脑对齐对压缩有抗性，多数方法保留神经预测性，GPTQ除外；语言探测显示任务表现和脑预测性分离。

Conclusion: 脑对齐在适度规模模型饱和且对压缩有韧性，挑战神经扩展假设，推动紧凑模型发展。

Abstract: Recent work has shown that scaling large language models (LLMs) improves their alignment with human brain activity, yet it remains unclear what drives these gains and which representational properties are responsible. Although larger models often yield better task performance and brain alignment, they are increasingly difficult to analyze mechanistically. This raises a fundamental question: what is the minimal model capacity required to capture brain-relevant representations? To address this question, we systematically investigate how constraining model scale and numerical precision affects brain alignment. We compare full-precision LLMs, small language models (SLMs), and compressed variants (quantized and pruned) by predicting fMRI responses during naturalistic language comprehension. Across model families up to 14B parameters, we find that 3B SLMs achieve brain predictivity indistinguishable from larger LLMs, whereas 1B models degrade substantially, particularly in semantic language regions. Brain alignment is remarkably robust to compression: most quantization and pruning methods preserve neural predictivity, with GPTQ as a consistent exception. Linguistic probing reveals a dissociation between task performance and brain predictivity: compression degrades discourse, syntax, and morphology, yet brain predictivity remains largely unchanged. Overall, brain alignment saturates at modest model scales and is resilient to compression, challenging common assumptions about neural scaling and motivating compact models for brain-aligned language modeling.

</details>


### [695] [How does longer temporal context enhance multimodal narrative video processing in the brain?](https://arxiv.org/abs/2602.07570)
*Prachi Jindal,Anant Khandelwal,Manish Gupta,Bapi S. Raju,Subba Reddy Oota,Tanmoy Chakraborty*

Main category: q-bio.NC

TL;DR: 研究视频片段时长与叙事任务提示对大脑与模型对齐的影响，发现长视频及叙事任务对多模态大模型对齐有积极作用，为长上下文多模态大语言模型提供测试平台。


<details>
  <summary>Details</summary>
Motivation: 解决神经科学与机器学习交叉领域中，理解人类和人工智能系统处理复杂叙事视频的挑战。

Method: 使用功能性磁共振成像（fMRI）记录观看全长度电影的参与者数据，研究对叙事上下文敏感的大脑区域在不同时间尺度上的信息表征及与模型特征的对齐情况。

Result: 增加片段时长对多模态大语言模型的大脑对齐有显著提升，单模态视频模型提升不明显；不同时长窗口与不同脑区对应；叙事任务提示引发特定脑区对齐模式和调谐变化。

Conclusion: 长形式叙事电影可作为探索长上下文多模态大语言模型中生物相关时间整合和可解释表征的有效测试平台。

Abstract: Understanding how humans and artificial intelligence systems process complex narrative videos is a fundamental challenge at the intersection of neuroscience and machine learning. This study investigates how the temporal context length of video clips (3--12 s clips) and the narrative-task prompting shape brain-model alignment during naturalistic movie watching. Using fMRI recordings from participants viewing full-length movies, we examine how brain regions sensitive to narrative context dynamically represent information over varying timescales and how these neural patterns align with model-derived features. We find that increasing clip duration substantially improves brain alignment for multimodal large language models (MLLMs), whereas unimodal video models show little to no gain. Further, shorter temporal windows align with perceptual and early language regions, while longer windows preferentially align higher-order integrative regions, mirrored by a layer-to-cortex hierarchy in MLLMs. Finally, narrative-task prompts (multi-scene summary, narrative summary, character motivation, and event boundary detection) elicit task-specific, region-dependent brain alignment patterns and context-dependent shifts in clip-level tuning in higher-order regions. Together, our results position long-form narrative movies as a principled testbed for probing biologically relevant temporal integration and interpretable representations in long-context MLLMs.

</details>


<div id='physics.app-ph'></div>

# physics.app-ph [[Back]](#toc)

### [696] [Estimation of Fish Catch Using Sentinel-2, 3 and XGBoost-Kernel-Based Kernel Ridge Regression](https://arxiv.org/abs/2602.08511)
*Kanu Mohammed,Vaishnavi Joshi,Pranjali Diliprao Patil,Sandipan Mondal,Ming-An Lee,Subhadip Dey*

Main category: physics.app-ph

TL;DR: 研究利用卫星多光谱图像，用XGBoost - KRR技术估算渔获量，该框架表现良好，支持可持续管理并推动相关SDGs。


<details>
  <summary>Details</summary>
Motivation: 海洋因素影响鱼类分布，为维持渔业、保障全球粮食安全，需量化海洋因素与鱼类分布联系。

Method: 使用Sentinel - 2 MSI和Sentinel - 3 OLCI的多光谱图像，采用XGBoost - KRR技术估算渔获量。

Result: XGBoost - KRR框架在两个传感器上相关性最强、预测误差最低，能捕捉非线性海洋 - 鱼类联系；Sentinel - 2 MSI有更精细空间变化，Sentinel - 3 OLCI光谱响应更平滑。

Conclusion: 该方法支持可持续生态系统管理，加强卫星渔业评估，推动SDGs 2和14。

Abstract: Oceanographic factors, such as sea surface temperature and upper-ocean dynamics, have a significant impact on fish distribution. Maintaining fisheries that contribute to global food security requires quantifying these connections. This study uses multispectral images from Sentinel-2 MSI and Sentinel-3 OLCI to estimate fish catch using an Extreme Gradient Boosting (XGBoost)-kernelized Kernel Ridge Regression (KRR) technique. According to model evaluation, the XGBoost-KRR framework achieves the strongest correlation and the lowest prediction error across both sensors, suggesting improved capacity to capture nonlinear ocean-fish connections. While Sentinel-2 MSI resolves finer-scale spatial variability, emphasizing localized ecological interactions, Sentinel-3 OLCI displays smoother spectral responses associated with poorer spatial resolution. By supporting sustainable ecosystem management and strengthening satellite-based fisheries assessment, the proposed approach advances SDGs 2 (Zero Hunger) and 14 (Life Below Water).

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [697] [aerial-autonomy-stack -- a Faster-than-real-time, Autopilot-agnostic, ROS2 Framework to Simulate and Deploy Perception-based Drones](https://arxiv.org/abs/2602.07264)
*Jacopo Panerati,Sina Sajjadi,Sina Soleymanpour,Varunkumar Mehta,Iraj Mantegh*

Main category: cs.RO

TL;DR: 无人机应用广泛，提升自主性很重要，当前存在模拟到现实的差距问题，本文介绍开源框架aerial - autonomy - stack，可加速开发部署周期。


<details>
  <summary>Details</summary>
Motivation: 无人机引入更大自主性具有重要意义，但物理AI面临模拟到现实的差距等问题，需要开发有效框架来解决。

Method: 引入开源端到端框架aerial - autonomy - stack，支持使用ROS2开发，为PX4和ArduPilot提供通用接口。

Result: 该框架支持超过20倍实时速度的端到端模拟，显著压缩了基于感知的自主性开发测试发布周期。

Conclusion: aerial - autonomy - stack框架能有效解决物理AI面临的部分问题，加速开发和部署自主航空系统。

Abstract: Unmanned aerial vehicles are rapidly transforming multiple applications, from agricultural and infrastructure monitoring to logistics and defense. Introducing greater autonomy to these systems can simultaneously make them more effective as well as reliable. Thus, the ability to rapidly engineer and deploy autonomous aerial systems has become of strategic importance. In the 2010s, a combination of high-performance compute, data, and open-source software led to the current deep learning and AI boom, unlocking decades of prior theoretical work. Robotics is on the cusp of a similar transformation. However, physical AI faces unique hurdles, often combined under the umbrella term "simulation-to-reality gap". These span from modeling shortcomings to the complexity of vertically integrating the highly heterogeneous hardware and software systems typically found in field robots. To address the latter, we introduce aerial-autonomy-stack, an open-source, end-to-end framework designed to streamline the pipeline from (GPU-accelerated) perception to (flight controller-based) action. Our stack allows the development of aerial autonomy using ROS2 and provides a common interface for two of the most popular autopilots: PX4 and ArduPilot. We show that it supports over 20x faster-than-real-time, end-to-end simulation of a complete development and deployment stack -- including edge compute and networking -- significantly compressing the build-test-release cycle of perception-based autonomy.

</details>


### [698] [Leveraging Adaptive Group Negotiation for Heterogeneous Multi-Robot Collaboration with Large Language Models](https://arxiv.org/abs/2602.06967)
*Siqi Song,Xuanbing Xie,Zonglin Li,Yuqiang Li,Shijie Wang,Biqing Qi*

Main category: cs.RO

TL;DR: 提出CLiMRS框架用于多机器人协作，引入CLiMBench基准测试，实验表明该框架提升了多机器人协作效率。


<details>
  <summary>Details</summary>
Motivation: 多机器人协作任务有空间约束和环境不确定性，LLMs在协调控制方面潜力未充分挖掘。

Method: 提出CLiMRS框架，为每个机器人配备LLM代理，通过通用提案规划器动态形成子组，子组内进行多LLM讨论获取行动指令，有分组 - 规划 - 执行 - 反馈循环。

Result: CLiMRS超越最佳基线，在复杂任务上效率提高超40%，不影响简单任务成功率。

Conclusion: 利用受人类启发的分组和协商原则可显著提升异构多机器人协作效率。

Abstract: Multi-robot collaboration tasks often require heterogeneous robots to work together over long horizons under spatial constraints and environmental uncertainties. Although Large Language Models (LLMs) excel at reasoning and planning, their potential for coordinated control has not been fully explored. Inspired by human teamwork, we present CLiMRS (Cooperative Large-Language-Model-Driven Heterogeneous Multi-Robot System), an adaptive group negotiation framework among LLMs for multi-robot collaboration. This framework pairs each robot with an LLM agent and dynamically forms subgroups through a general proposal planner. Within each subgroup, a subgroup manager leads perception-driven multi-LLM discussions to get commands for actions. Feedback is provided by both robot execution outcomes and environment changes. This grouping-planning-execution-feedback loop enables efficient planning and robust execution. To evaluate these capabilities, we introduce CLiMBench, a heterogeneous multi-robot benchmark of challenging assembly tasks. Our experiments show that CLiMRS surpasses the best baseline, achieving over 40% higher efficiency on complex tasks without sacrificing success on simpler ones. Overall, our results demonstrate that leveraging human-inspired group formation and negotiation principles significantly enhances the efficiency of heterogeneous multi-robot collaboration. Our code is available here: https://github.com/song-siqi/CLiMRS.

</details>


### [699] [Realistic Synthetic Household Data Generation at Scale](https://arxiv.org/abs/2602.07243)
*Siddharth Singh,Ifrah Idrees,Abraham Dauhajre*

Main category: cs.RO

TL;DR: 提出生成框架创建大规模家庭数据集，可通过自然语言配置，经统计评估验证，能助力家用智能设备开发测试。


<details>
  <summary>Details</summary>
Motivation: 现有框架无法建模人类行为与家庭环境的双向影响，而开发交互智能体需要多样大规模数据集。

Method: 提出松散耦合方式生成长期人机交互和环境的框架，用户可通过自然语言定义数据集特征。用多模态嵌入和关键指标进行统计评估。

Result: 生成的3D数据含丰富静态和时间上下文。与真实数据集有良好对齐，干预分析有显著效果，证实双向耦合能将人物特征转化为可衡量差异。

Conclusion: 该框架可用于大规模开发和测试家用智能设备。

Abstract: Advancements in foundation models have catalyzed research in Embodied AI to develop interactive agents capable of environmental reasoning and interaction. Developing such agents requires diverse, large-scale datasets. Prior frameworks generate synthetic data for long-term human-robot interactions but fail to model the bidirectional influence between human behavior and household environments. Our proposed generative framework creates household datasets at scale through loosely coupled generation of long-term human-robot interactions and environments. Human personas influence environment generation, while environment schematics and semantics shape human-robot interactions.
  The generated 3D data includes rich static context such as object and environment semantics, and temporal context capturing human and agent behaviors over extended periods. Our flexible tool allows users to define dataset characteristics via natural language prompts, enabling configuration of environment and human activity data through natural language specifications. The tool creates variations of user-defined configurations, enabling scalable data generation.
  We validate our framework through statistical evaluation using multi-modal embeddings and key metrics: cosine similarity, mutual information gain, intervention analysis, and iterative improvement validation. Statistical comparisons show good alignment with real-world datasets (HOMER) with cosine similarity (0.60), while synthetic datasets (Wang et al.) show moderate alignment (0.27). Intervention analysis across age, organization, and sleep pattern changes shows statistically significant effects (p < 0.001) with large effect sizes (Cohen's d = 0.51-1.12), confirming bidirectional coupling translates persona traits into measurable environmental and behavioral differences. These contributions enable development and testing of household smart devices at scale.

</details>


### [700] [Action-to-Action Flow Matching](https://arxiv.org/abs/2602.07322)
*Jindou Jia,Gen Li,Xiangyu Chen,Tuo An,Yuxuan Hu,Jingliang Li,Xinying Guo,Jianfei Yang*

Main category: cs.RO

TL;DR: 本文提出A2A策略范式，从随机采样转向以前一动作信息初始化，绕过迭代去噪，实验表明它训练高效、推理快、泛化性强，还可用于视频生成。


<details>
  <summary>Details</summary>
Motivation: 基于扩散的策略在机器人领域虽成功，但标准随机高斯噪声采样需多步迭代，推理延迟高，成为实时控制瓶颈。

Method: 提出Action-to-Action flow matching (A2A)策略范式，利用历史本体感受序列，将其嵌入高维潜空间作为动作生成起点。

Result: A2A训练效率高、推理速度快、泛化性好，单步推理即可生成高质量动作（延迟0.56 ms），对视觉扰动鲁棒，能泛化到未见配置，还可用于视频生成。

Conclusion: A2A是一种有效的策略范式，能解决基于扩散策略的推理延迟问题，且具有广泛适用性。

Abstract: Diffusion-based policies have recently achieved remarkable success in robotics by formulating action prediction as a conditional denoising process. However, the standard practice of sampling from random Gaussian noise often requires multiple iterative steps to produce clean actions, leading to high inference latency that incurs a major bottleneck for real-time control. In this paper, we challenge the necessity of uninformed noise sampling and propose Action-to-Action flow matching (A2A), a novel policy paradigm that shifts from random sampling to initialization informed by the previous action. Unlike existing methods that treat proprioceptive action feedback as static conditions, A2A leverages historical proprioceptive sequences, embedding them into a high-dimensional latent space as the starting point for action generation. This design bypasses costly iterative denoising while effectively capturing the robot's physical dynamics and temporal continuity. Extensive experiments demonstrate that A2A exhibits high training efficiency, fast inference speed, and improved generalization. Notably, A2A enables high-quality action generation in as few as a single inference step (0.56 ms latency), and exhibits superior robustness to visual perturbations and enhanced generalization to unseen configurations. Lastly, we also extend A2A to video generation, demonstrating its broader versatility in temporal modeling. Project site: https://lorenzo-0-0.github.io/A2A_Flow_Matching.

</details>


### [701] [Bridging Speech, Emotion, and Motion: a VLM-based Multimodal Edge-deployable Framework for Humanoid Robots](https://arxiv.org/abs/2602.07434)
*Songhua Yang,Xuetao Li,Xuanye Fei,Mengde Li,Miao Li*

Main category: cs.RO

TL;DR: 提出基于视觉语言模型的SeM²框架，实现情感连贯的多模态交互，有云基和边缘部署版本，表现优于单模态基线。


<details>
  <summary>Details</summary>
Motivation: 有效人机交互需情感丰富的多模态表达，但多数人形机器人缺乏协调，且现实部署需要无需持续云连接的设备端解决方案。

Method: 提出SeM²框架，含多模态感知模块、思维链推理和语义序列对齐机制（SSAM），实现情感连贯的多模态交互，有云基和边缘部署版本。

Result: 边缘部署版本在边缘硬件上高效运行，保留95%相对性能，综合评估显示该方法在自然度、情感清晰度和模态连贯性上显著优于单模态基线。

Conclusion: 该方法推动了适用于多样现实环境的社交表达型人形机器人发展。

Abstract: Effective human-robot interaction requires emotionally rich multimodal expressions, yet most humanoid robots lack coordinated speech, facial expressions, and gestures. Meanwhile, real-world deployment demands on-device solutions that can operate autonomously without continuous cloud connectivity. To bridging \underline{\textit{S}}peech, \underline{\textit{E}}motion, and \underline{\textit{M}}otion, we present \textit{SeM$^2$}, a Vision Language Model-based framework that orchestrates emotionally coherent multimodal interactions through three key components: a multimodal perception module capturing user contextual cues, a Chain-of-Thought reasoning for response planning, and a novel Semantic-Sequence Aligning Mechanism (SSAM) that ensures precise temporal coordination between verbal content and physical expressions. We implement both cloud-based and \underline{\textit{e}}dge-deployed versions (\textit{SeM$^2_e$}), with the latter knowledge distilled to operate efficiently on edge hardware while maintaining 95\% of the relative performance. Comprehensive evaluations demonstrate that our approach significantly outperforms unimodal baselines in naturalness, emotional clarity, and modal coherence, advancing socially expressive humanoid robotics for diverse real-world environments.

</details>


### [702] [TextOp: Real-time Interactive Text-Driven Humanoid Robot Motion Generation and Control](https://arxiv.org/abs/2602.07439)
*Weiji Xie,Jiakun Zheng,Jinrui Han,Jiyuan Shi,Weinan Zhang,Chenjia Bai,Xuelong Li*

Main category: cs.RO

TL;DR: 提出TextOp框架，实现实时文本驱动人形机器人运动控制，实验证明其性能良好。


<details>
  <summary>Details</summary>
Motivation: 现有控制器灵活性不足或自主性受限，需实时交互驱动人形机器人控制器。

Method: 采用两级架构，高层自回归运动扩散模型生成轨迹，低层运动跟踪策略执行轨迹。

Result: 通过大量实验，展示了即时响应、平滑全身运动和精确控制。

Conclusion: TextOp框架可实现自由意图表达和多行为平滑过渡，代码开源。

Abstract: Recent advances in humanoid whole-body motion tracking have enabled the execution of diverse and highly coordinated motions on real hardware. However, existing controllers are commonly driven either by predefined motion trajectories, which offer limited flexibility when user intent changes, or by continuous human teleoperation, which requires constant human involvement and limits autonomy. This work addresses the problem of how to drive a universal humanoid controller in a real-time and interactive manner. We present TextOp, a real-time text-driven humanoid motion generation and control framework that supports streaming language commands and on-the-fly instruction modification during execution. TextOp adopts a two-level architecture in which a high-level autoregressive motion diffusion model continuously generates short-horizon kinematic trajectories conditioned on the current text input, while a low-level motion tracking policy executes these trajectories on a physical humanoid robot. By bridging interactive motion generation with robust whole-body control, TextOp unlocks free-form intent expression and enables smooth transitions across multiple challenging behaviors such as dancing and jumping, within a single continuous motion execution. Extensive real-robot experiments and offline evaluations demonstrate instant responsiveness, smooth whole-body motion, and precise control. The project page and the open-source code are available at https://text-op.github.io/

</details>


### [703] [VividFace: Real-Time and Realistic Facial Expression Shadowing for Humanoid Robots](https://arxiv.org/abs/2602.07506)
*Peizhen Li,Longbing Cao,Xiao-Ming Wu,Yang Zhang*

Main category: cs.RO

TL;DR: 提出VividFace系统，实现类人机器人实时逼真模仿人类面部表情，经实际验证有效。


<details>
  <summary>Details</summary>
Motivation: 现有类人面部表情模仿进展有限，无法兼顾实时性和逼真表现力。

Method: 采用优化的模仿框架X2CNet++，微调面部运动转移模块，引入特征适应训练策略；使用视频流推理管道和基于异步I/O的简化工作流。

Result: VividFace能在0.05秒内模仿人类面部表情，可适应不同面部配置。

Conclusion: 通过大量实际演示验证了VividFace的实用性。

Abstract: Humanoid facial expression shadowing enables robots to realistically imitate human facial expressions in real time, which is critical for lifelike, facially expressive humanoid robots and affective human-robot interaction. Existing progress in humanoid facial expression imitation remains limited, often failing to achieve either real-time performance or realistic expressiveness due to offline video-based inference designs and insufficient ability to capture and transfer subtle expression details. To address these limitations, we present VividFace, a real-time and realistic facial expression shadowing system for humanoid robots. An optimized imitation framework X2CNet++ enhances expressiveness by fine-tuning the human-to-humanoid facial motion transfer module and introducing a feature-adaptation training strategy for better alignment across different image sources. Real-time shadowing is further enabled by a video-stream-compatible inference pipeline and a streamlined workflow based on asynchronous I/O for efficient communication across devices. VividFace produces vivid humanoid faces by mimicking human facial expressions within 0.05 seconds, while generalizing across diverse facial configurations. Extensive real-world demonstrations validate its practical utility. Videos are available at: https://lipzh5.github.io/VividFace/.

</details>


### [704] [NLP Sampling: Combining MCMC and NLP Methods for Diverse Constrained Sampling](https://arxiv.org/abs/2407.03035)
*Marc Toussaint,Cornelius V. Braun,Joaquim Ortiz-Haro*

Main category: cs.RO

TL;DR: 本文提供整合框架结合多领域方法解决硬约束下生成多样样本问题，并进行评估和概念讨论。


<details>
  <summary>Details</summary>
Motivation: 在硬约束下生成多样样本是多领域核心挑战，旨在提供整合视角和框架，通过实证评估了解各方法优势。

Method: 提出NLP采样作为通用问题表述，提出重启两阶段方法族作为整合多领域方法的框架，并在分析和机器人操作规划问题上评估。

Result: 未明确提及具体结果。

Conclusion: 未明确提及具体结论，进行了如拉格朗日参数作用、全局采样等概念讨论。

Abstract: Generating diverse samples under hard constraints is a core challenge in many areas. With this work we aim to provide an integrative view and framework to combine methods from the fields of MCMC, constrained optimization, as well as robotics, and gain insights in their strengths from empirical evaluations. We propose NLP Sampling as a general problem formulation, propose a family of restarting two-phase methods as a framework to integrated methods from across the fields, and evaluate them on analytical and robotic manipulation planning problems. Complementary to this, we provide several conceptual discussions, e.g. on the role of Lagrange parameters, global sampling, and the idea of a Diffused NLP and a corresponding model-based denoising sampler.

</details>


### [705] [Incremental Mapping with Measurement Synchronization & Compression](https://arxiv.org/abs/2602.07901)
*Mark Griguletskii,Danil Belov,Pavel Osinenko*

Main category: cs.RO

TL;DR: 本文针对多传感器系统中因子图拓扑设计难题，提出增量构建连接因子图的新方法，可在保持地图质量前提下压缩图节点。


<details>
  <summary>Details</summary>
Motivation: 传统因子图拓扑设计在多传感器异步数据系统中存在挑战，刚性图结构效率低，预积分技术适用性有限，需新方法解决一致状态估计问题。

Method: 提出增量构建连接因子图的方法，根据外部评估标准选择最优图拓扑，纳入所有可用传感器数据。

Result: 该方法可使图节点（优化变量）数量平均减少约30%，地图质量与传统方法相当。

Conclusion: 新方法能有效解决多传感器系统中因子图拓扑设计问题，在图压缩方面表现良好且不影响地图质量。

Abstract: Modern autonomous vehicles and robots utilize versatile sensors for localization and mapping. The fidelity of these maps is paramount, as an accurate environmental representation is a prerequisite for stable and precise localization. Factor graphs provide a powerful approach for sensor fusion, enabling the estimation of the maximum a posteriori solution. However, the discrete nature of graph-based representations, combined with asynchronous sensor measurements, complicates consistent state estimation. The design of an optimal factor graph topology remains an open challenge, especially in multi-sensor systems with asynchronous data. Conventional approaches rely on a rigid graph structure, which becomes inefficient with sensors of disparate rates. Although preintegration techniques can mitigate this for high-rate sensors, their applicability is limited. To address this problem, this work introduces a novel approach that incrementally constructs connected factor graphs, ensuring the incorporation of all available sensor data by choosing the optimal graph topology based on the external evaluation criteria. The proposed methodology facilitates graph compression, reducing the number of nodes (optimized variables) by ~30% on average while maintaining map quality at a level comparable to conventional approaches.

</details>


### [706] [Optimized Human-Robot Co-Dispatch Planning for Petro-Site Surveillance under Varying Criticalities](https://arxiv.org/abs/2602.07924)
*Nur Ahmad Khatim,Mansur Arief*

Main category: cs.RO

TL;DR: 本文提出HRCD - FLP问题，评估三种技术成熟场景下指挥中心选择，结果显示从保守到未来自主操作可降低成本，不同规模问题有不同求解优势，优化人机.team规划很关键。


<details>
  <summary>Details</summary>
Motivation: 经典设施选址模型未考虑资源异质性，无法解决石油基础设施安全保障中平衡自主系统效率和人工判断的问题，因此需提出新的问题模型。

Method: 提出Human - Robot Co - Dispatch Facility Location Problem (HRCD - FLP)，考虑多层基础设施关键性等因素；评估三种技术成熟场景下的指挥中心选择；对不同规模问题采用精确方法和启发式方法求解。

Result: 从保守的1:3人机监督过渡到未来的1:10自主操作可大幅降低成本并覆盖关键基础设施；小问题上精确方法在成本和计算时间上占优，大问题上启发式方法能在3分钟内得到可行解，最优性差距约14%。

Conclusion: 从系统角度看，优化人机团队规划是实现经济高效且任务可靠部署的关键。

Abstract: Securing petroleum infrastructure requires balancing autonomous system efficiency with human judgment for threat escalation, a challenge unaddressed by classical facility location models assuming homogeneous resources. This paper formulates the Human-Robot Co-Dispatch Facility Location Problem (HRCD-FLP), a capacitated facility location variant incorporating tiered infrastructure criticality, human-robot supervision ratio constraints, and minimum utilization requirements. We evaluate command center selection across three technology maturity scenarios. Results show transitioning from conservative (1:3 human-robot supervision) to future autonomous operations (1:10) yields significant cost reduction while maintaining complete critical infrastructure coverage. For small problems, exact methods dominate in both cost and computation time; for larger problems, the proposed heuristic achieves feasible solutions in under 3 minutes with approximately 14% optimality gap where comparison is possible. From systems perspective, our work demonstrate that optimized planning for human-robot teaming is key to achieve both cost-effective and mission-reliable deployments.

</details>


### [707] [Self-Supervised Bootstrapping of Action-Predictive Embodied Reasoning](https://arxiv.org/abs/2602.08167)
*Milan Ganai,Katie Luo,Jonas Frey,Clark Barrett,Marco Pavone*

Main category: cs.RO

TL;DR: 提出R&B - EnCoRe方法，让模型通过自监督细化从互联网规模知识中引导具身推理，在多种具身任务上验证，相比其他方法有显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前具身思维链推理方法依赖刚性模板，会使策略处理无关信息，造成推理质量与策略构建的瓶颈。

Method: 引入R&B - EnCoRe，将推理视为重要性加权变分推理中的潜在变量，在无外部奖励、验证器或人工标注的情况下生成和提炼细化的推理训练数据集。

Result: 在不同具身任务和多种VLA架构上验证，操控成功率提高28%，导航得分提高101%，碰撞率降低21%。

Conclusion: R&B - EnCoRe能让模型提炼可预测成功控制的推理，无需手动标注工程，还能将互联网规模知识用于物理执行。

Abstract: Embodied Chain-of-Thought (CoT) reasoning has significantly enhanced Vision-Language-Action (VLA) models, yet current methods rely on rigid templates to specify reasoning primitives (e.g., objects in the scene, high-level plans, structural affordances). These templates can force policies to process irrelevant information that distracts from critical action-prediction signals. This creates a bottleneck: without successful policies, we cannot verify reasoning quality; without quality reasoning, we cannot build robust policies. We introduce R&B-EnCoRe, which enables models to bootstrap embodied reasoning from internet-scale knowledge through self-supervised refinement. By treating reasoning as a latent variable within importance-weighted variational inference, models can generate and distill a refined reasoning training dataset of embodiment-specific strategies without external rewards, verifiers, or human annotation. We validate R&B-EnCoRe across manipulation (Franka Panda in simulation, WidowX in hardware), legged navigation (bipedal, wheeled, bicycle, quadruped), and autonomous driving embodiments using various VLA architectures with 1B, 4B, 7B, and 30B parameters. Our approach achieves 28% gains in manipulation success, 101% improvement in navigation scores, and 21% reduction in collision-rate metric over models that indiscriminately reason about all available primitives. R&B-EnCoRe enables models to distill reasoning that is predictive of successful control, bypassing manual annotation engineering while grounding internet-scale knowledge in physical execution.

</details>


### [708] [STEP: Warm-Started Visuomotor Policies with Spatiotemporal Consistency Prediction](https://arxiv.org/abs/2602.08245)
*Jinhao Li,Yuxuan Cong,Yingqiao Wang,Hao Xia,Shan Huang,Yijia Zhang,Ningyi Xu,Guohao Dai*

Main category: cs.RO

TL;DR: 本文提出STEP机制加速扩散策略推理，在模拟和真实任务中提升成功率并改善推理延迟与成功率的权衡。


<details>
  <summary>Details</summary>
Motivation: 现有扩散策略推理延迟大，现有加速方法难兼顾动作质量和低延迟。

Method: 提出轻量级时空一致性预测机制STEP构建高质量热启动动作，提出速度感知扰动注入机制，提供理论分析确保动作误差收敛。

Result: 在九个模拟基准和两个真实任务评估，STEP两步法在RoboMimic基准和真实任务中成功率分别比BRIDGER和DDIM高21.6%和27.5%。

Conclusion: STEP相比现有方法持续提升推理延迟和成功率的帕累托边界。

Abstract: Diffusion policies have recently emerged as a powerful paradigm for visuomotor control in robotic manipulation due to their ability to model the distribution of action sequences and capture multimodality. However, iterative denoising leads to substantial inference latency, limiting control frequency in real-time closed-loop systems. Existing acceleration methods either reduce sampling steps, bypass diffusion through direct prediction, or reuse past actions, but often struggle to jointly preserve action quality and achieve consistently low latency. In this work, we propose STEP, a lightweight spatiotemporal consistency prediction mechanism to construct high-quality warm-start actions that are both distributionally close to the target action and temporally consistent, without compromising the generative capability of the original diffusion policy. Then, we propose a velocity-aware perturbation injection mechanism that adaptively modulates actuation excitation based on temporal action variation to prevent execution stall especially for real-world tasks. We further provide a theoretical analysis showing that the proposed prediction induces a locally contractive mapping, ensuring convergence of action errors during diffusion refinement. We conduct extensive evaluations on nine simulated benchmarks and two real-world tasks. Notably, STEP with 2 steps can achieve an average 21.6% and 27.5% higher success rate than BRIDGER and DDIM on the RoboMimic benchmark and real-world tasks, respectively. These results demonstrate that STEP consistently advances the Pareto frontier of inference latency and success rate over existing methods.

</details>


### [709] [Learning Human-Like Badminton Skills for Humanoid Robots](https://arxiv.org/abs/2602.08370)
*Yeke Chen,Shihao Dong,Xiaoyu Ji,Jingkai Sun,Zeren Luo,Liu Zhao,Jiahui Zhang,Wanyue Li,Ji Ma,Bowen Xu,Yimin Han,Yudong Zhao,Peng Lu*

Main category: cs.RO

TL;DR: 提出Imitation - to - Interaction强化学习框架，让机器人从模仿者变为击球者，在模拟中掌握多种羽毛球技能，实现零样本从模拟到现实的迁移。


<details>
  <summary>Details</summary>
Motivation: 在像羽毛球这样的高要求运动中实现类人表现对人形机器人而言是巨大挑战，现有研究在运动模仿与功能性击球之间存在差距。

Method: 提出Imitation - to - Interaction框架，从人类数据建立运动先验，将其提炼成紧凑的基于模型的状态表示，通过对抗先验稳定动力学，引入流形扩展策略解决专家演示稀疏问题。

Result: 在模拟中掌握了包括高远球和吊球等多种技能，实现零样本从模拟到现实的人形羽毛球技能迁移。

Conclusion: 所提框架能让机器人在羽毛球运动中复制人类运动员的运动优雅和功能精度。

Abstract: Realizing versatile and human-like performance in high-demand sports like badminton remains a formidable challenge for humanoid robotics. Unlike standard locomotion or static manipulation, this task demands a seamless integration of explosive whole-body coordination and precise, timing-critical interception. While recent advances have achieved lifelike motion mimicry, bridging the gap between kinematic imitation and functional, physics-aware striking without compromising stylistic naturalness is non-trivial. To address this, we propose Imitation-to-Interaction, a progressive reinforcement learning framework designed to evolve a robot from a "mimic" to a capable "striker." Our approach establishes a robust motor prior from human data, distills it into a compact, model-based state representation, and stabilizes dynamics via adversarial priors. Crucially, to overcome the sparsity of expert demonstrations, we introduce a manifold expansion strategy that generalizes discrete strike points into a dense interaction volume. We validate our framework through the mastery of diverse skills, including lifts and drop shots, in simulation. Furthermore, we demonstrate the first zero-shot sim-to-real transfer of anthropomorphic badminton skills to a humanoid robot, successfully replicating the kinetic elegance and functional precision of human athletes in the physical world.

</details>


### [710] [BiManiBench: A Hierarchical Benchmark for Evaluating Bimanual Coordination of Multimodal Large Language Models](https://arxiv.org/abs/2602.08392)
*Xin Wu,Zhixuan Liang,Yue Ma,Mengkang Hu,Zhiyuan Qin,Xiu Li*

Main category: cs.RO

TL;DR: 介绍用于评估多模态大语言模型（MLLMs）的双操作基准BiManiBench，分析30多个模型发现MLLMs在双臂空间定位和控制上有问题，指出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 现有框架多局限于单臂操作，无法满足双臂任务时空协调需求，需新基准评估MLLMs。

Method: 引入BiManiBench，分三个层级评估MLLMs，隔离双臂独特挑战。

Result: 分析30多个模型发现，MLLMs虽有高级推理能力，但在双臂空间定位和控制上存在问题，常出现相互干扰和顺序错误。

Conclusion: 当前范式缺乏对相互运动学约束的深入理解，未来研究应关注臂间避碰和细粒度时间排序。

Abstract: Multimodal Large Language Models (MLLMs) have significantly advanced embodied AI, and using them to benchmark robotic intelligence has become a pivotal trend. However, existing frameworks remain predominantly confined to single-arm manipulation, failing to capture the spatio-temporal coordination required for bimanual tasks like lifting a heavy pot. To address this, we introduce BiManiBench, a hierarchical benchmark evaluating MLLMs across three tiers: fundamental spatial reasoning, high-level action planning, and low-level end-effector control. Our framework isolates unique bimanual challenges, such as arm reachability and kinematic constraints, thereby distinguishing perceptual hallucinations from planning failures. Analysis of over 30 state-of-the-art models reveals that despite high-level reasoning proficiency, MLLMs struggle with dual-arm spatial grounding and control, frequently resulting in mutual interference and sequencing errors. These findings suggest the current paradigm lacks a deep understanding of mutual kinematic constraints, highlighting the need for future research to focus on inter-arm collision-avoidance and fine-grained temporal sequencing.

</details>


### [711] [Contact-Anchored Policies: Contact Conditioning Creates Strong Robot Utility Models](https://arxiv.org/abs/2602.09017)
*Zichen Jeff Cui,Omar Rayyan,Haritheja Etukuru,Bowen Tan,Zavier Andrianarivo,Zicheng Teng,Yihang Zhou,Krish Mehta,Nicholas Wojno,Kevin Yuanbo Wu,Manan H Anjaria,Ziyuan Wu,Manrong Mao,Guangxun Zhang,Binit Shah,Yejin Kim,Soumith Chintala,Lerrel Pinto,Nur Muhammad Mahi Shafiullah*

Main category: cs.RO

TL;DR: 提出Contact - Anchored Policies (CAP)，以物理接触点代替语言条件，通过模块化和仿真迭代，用少量数据实现对新环境和实体的泛化，零样本评估优于先进模型。


<details>
  <summary>Details</summary>
Motivation: 现有机器人学习中用语言提示泛化的方法存在局限性，语言过于抽象，难以指导稳健操作所需的具体物理理解。

Method: 引入Contact - Anchored Policies (CAP)，用物理接触点代替语言条件；将CAP构建为模块化实用模型库；建立EgoGym仿真基准，进行现实到仿真的迭代循环。

Result: 使用仅23小时演示数据，在三种基本操作技能上能直接泛化到新环境和实体；零样本评估中性能比大型先进视觉语言适应器高56%。

Conclusion: 基于接触条件和仿真迭代的CAP有效可行，相关资源将开源。

Abstract: The prevalent paradigm in robot learning attempts to generalize across environments, embodiments, and tasks with language prompts at runtime. A fundamental tension limits this approach: language is often too abstract to guide the concrete physical understanding required for robust manipulation. In this work, we introduce Contact-Anchored Policies (CAP), which replace language conditioning with points of physical contact in space. Simultaneously, we structure CAP as a library of modular utility models rather than a monolithic generalist policy. This factorization allows us to implement a real-to-sim iteration cycle: we build EgoGym, a lightweight simulation benchmark, to rapidly identify failure modes and refine our models and datasets prior to real-world deployment. We show that by conditioning on contact and iterating via simulation, CAP generalizes to novel environments and embodiments out of the box on three fundamental manipulation skills while using only 23 hours of demonstration data, and outperforms large, state-of-the-art VLAs in zero-shot evaluations by 56%. All model checkpoints, codebase, hardware, simulation, and datasets will be open-sourced. Project page: https://cap-policy.github.io/

</details>


### [712] [Robustness Is a Function, Not a Number: A Factorized Comprehensive Study of OOD Robustness in Vision-Based Driving](https://arxiv.org/abs/2602.09018)
*Amir Mallak,Alaa Maalouf*

Main category: cs.RO

TL;DR: 本文对自动驾驶中分布外（OOD）鲁棒性进行多维度分析，对比不同策略，得出提升OOD鲁棒性的设计规则。


<details>
  <summary>Details</summary>
Motivation: 解决自动驾驶中OOD鲁棒性单一评估问题，探究策略失效原因。

Method: 沿五个轴分解环境，进行 $k$ 因子扰动测试，在VISTA中使用闭环控制对FC、CNN和ViT策略进行基准测试，训练基于冻结基础模型特征的ViT头，改变ID支持。

Result: ViT策略OOD鲁棒性更好；多帧输入不优于单帧基线；不同环境因素变化对性能影响不同；多因素交互非加性；特定训练环境更具鲁棒性；增加轨迹/视图可提升鲁棒性，多ID环境可拓宽覆盖范围。

Conclusion: 研究结果为OOD鲁棒驾驶策略提供了可操作的设计规则。

Abstract: Out of distribution (OOD) robustness in autonomous driving is often reduced to a single number, hiding what breaks a policy. We decompose environments along five axes: scene (rural/urban), season, weather, time (day/night), and agent mix; and measure performance under controlled $k$-factor perturbations ($k \in \{0,1,2,3\}$). Using closed loop control in VISTA, we benchmark FC, CNN, and ViT policies, train compact ViT heads on frozen foundation-model (FM) features, and vary ID support in scale, diversity, and temporal context. (1) ViT policies are markedly more OOD-robust than comparably sized CNN/FC, and FM features yield state-of-the-art success at a latency cost. (2) Naive temporal inputs (multi-frame) do not beat the best single-frame baseline. (3) The largest single factor drops are rural $\rightarrow$ urban and day $\rightarrow$ night ($\sim 31\%$ each); actor swaps $\sim 10\%$, moderate rain $\sim 7\%$; season shifts can be drastic, and combining a time flip with other changes further degrades performance. (4) FM-feature policies stay above $85\%$ under three simultaneous changes; non-FM single-frame policies take a large first-shift hit, and all no-FM models fall below $50\%$ by three changes. (5) Interactions are non-additive: some pairings partially offset, whereas season-time combinations are especially harmful. (6) Training on winter/snow is most robust to single-factor shifts, while a rural+summer baseline gives the best overall OOD performance. (7) Scaling traces/views improves robustness ($+11.8$ points from $5$ to $14$ traces), yet targeted exposure to hard conditions can substitute for scale. (8) Using multiple ID environments broadens coverage and strengthens weak cases (urban OOD $60.6\% \rightarrow 70.1\%$) with a small ID drop; single-ID preserves peak performance but in a narrow domain. These results yield actionable design rules for OOD-robust driving policies.

</details>


### [713] [From Obstacles to Etiquette: Robot Social Navigation with VLM-Informed Path Selection](https://arxiv.org/abs/2602.09002)
*Zilin Fang,Anxing Xiao,David Hsu,Gim Hee Lee*

Main category: cs.RO

TL;DR: 本文提出集成几何规划与情境社会推理的社交机器人导航框架，系统生成候选路径，用微调的视觉 - 语言模型评估，在实验中取得最佳表现。


<details>
  <summary>Details</summary>
Motivation: 在人类环境中进行社交导航仅满足几何约束不足，需分析交互并融入常识推理。

Method: 构建集成框架，先提取信息生成几何可行候选路径，再用微调的视觉 - 语言模型评估路径以选择社交优化路径，该模型从大基础模型中提取社交推理。

Result: 在四个社交导航情境实验中，方法以最低个人空间侵犯时长、最小行人朝向时间和无社交区域闯入实现最佳整体性能。

Conclusion: 所提出的社交机器人导航框架有效，能在不同人机交互情境中实时适应并实现良好社交导航效果。

Abstract: Navigating socially in human environments requires more than satisfying geometric constraints, as collision-free paths may still interfere with ongoing activities or conflict with social norms. Addressing this challenge calls for analyzing interactions between agents and incorporating common-sense reasoning into planning. This paper presents a social robot navigation framework that integrates geometric planning with contextual social reasoning. The system first extracts obstacles and human dynamics to generate geometrically feasible candidate paths, then leverages a fine-tuned vision-language model (VLM) to evaluate these paths, informed by contextually grounded social expectations, selecting a socially optimized path for the controller. This task-specific VLM distills social reasoning from large foundation models into a smaller and efficient model, allowing the framework to perform real-time adaptation in diverse human-robot interaction contexts. Experiments in four social navigation contexts demonstrate that our method achieves the best overall performance with the lowest personal space violation duration, the minimal pedestrian-facing time, and no social zone intrusions. Project page: https://path-etiquette.github.io

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [714] [Fork, Explore, Commit: OS Primitives for Agentic Exploration](https://arxiv.org/abs/2602.08199)
*Cong Wang,Yusheng Zheng*

Main category: cs.OS

TL;DR: 本文介绍用于AI代理探索的分支上下文（branch context）新操作系统抽象，在Linux上实现并给出初步评估。


<details>
  <summary>Details</summary>
Motivation: AI代理探索需要有原子提交和回滚语义的隔离环境，现有系统缺乏此类支持。

Method: 引入分支上下文抽象，在Linux上通过BranchFS文件系统和branch()系统调用实现。

Result: BranchFS分支创建耗时小于350微秒且与基础文件系统大小无关，小更改提交开销小于1毫秒。

Conclusion: 新提出的分支上下文抽象能为AI代理探索提供有效隔离环境。

Abstract: AI agents increasingly perform agentic exploration: pursuing multiple solution paths in parallel and committing only the successful one. Because each exploration path may modify files and spawn processes, agents require isolated environments with atomic commit and rollback semantics for both filesystem state and process state. We introduce the branch context, a new OS abstraction that provides: (1) copy-on-write state isolation with independent filesystem views and process groups, (2) a structured lifecycle of fork, explore, and commit/abort, (3) first-commit-wins resolution that automatically invalidates sibling branches, and (4) nestable contexts for hierarchical exploration. We realize branch contexts in Linux through two complementary components. First, BranchFS is a FUSE-based filesystem that gives each branch context an isolated copy-on-write workspace, with O(1) creation, atomic commit to the parent, and automatic sibling invalidation, all without root privileges. BranchFS is open sourced in https://github.com/multikernel/branchfs. Second, branch() is a proposed Linux syscall that spawns processes into branch contexts with reliable termination, kernel-enforced sibling isolation, and first-commit-wins coordination. Preliminary evaluation of BranchFS shows sub-350 us branch creation independent of base filesystem size, and modification-proportional commit overhead (under 1 ms for small changes).

</details>


### [715] [Equilibria: Fair Multi-Tenant CXL Memory Tiering At Scale](https://arxiv.org/abs/2602.08800)
*Kaiyang Zhao,Neha Gholkar,Hasan Maruf,Abhishek Dhanotia,Johannes Weiner,Gregory Price,Ning Sun,Bhavya Dwivedi,Stuart Clark,Dimitrios Skarlatos*

Main category: cs.OS

TL;DR: 介绍了内存通过CXL扩展的问题，提出Equilibria框架实现公平的多租户CXL分层存储，评估显示其性能优于现有方案且已开源。


<details>
  <summary>Details</summary>
Motivation: 现有CXL内存分层方案在生产部署中有缺乏多租户支持、控制面灵活性有限、可观测性不足等问题，需要改进方案。

Method: 提出Equilibria OS框架，提供基于容器的控制、细粒度可观测性，执行灵活的公平策略，缓解干扰。

Result: 在大型超大规模数据中心评估，帮助工作负载达到SLO，避免性能干扰，生产负载性能最高提升52%，基准测试性能提升1.7倍。

Conclusion: Equilibria实现了公平、多租户的CXL分层存储，性能优于现有Linux方案，相关补丁已开源。

Abstract: Memory dominates datacenter system cost and power. Memory expansion via Compute Express Link (CXL) is an effective way to provide additional memory at lower cost and power, but its effective use requires software-level tiering for hyperscaler workloads. Existing tiering solutions, including current Linux support, face fundamental limitations in production deployments. First, they lack multi-tenancy support, failing to handle stacked homogeneous or heterogeneous workloads. Second, limited control-plane flexibility leads to fairness violations and performance variability. Finally, insufficient observability prevents operators from diagnosing performance pathologies at scale.
  We present Equilibria, an OS framework enabling fair, multi-tenant CXL tiering at datacenter scale. Equilibria provides per-container controls for memory fair-share allocation and fine-grained observability of tiered-memory usage and operations. It further enforces flexible, user-specified fairness policies through regulated promotion and demotion, and mitigates noisy-neighbor interference by suppressing thrashing.
  Evaluated in a large hyperscaler fleet using production workloads and benchmarks, Equilibria helps workloads meet service level objectives (SLOs) while avoiding performance interference. It improves performance over the state-of-the-art Linux solution, TPP, by up to 52% for production workloads and 1.7x for benchmarks. All Equilibria patches have been released to the Linux community.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [716] [High-fidelity 3D multi-slab diffusion MRI using Slab-shifting for Harmonized 3D Acquisition and Reconstruction with Profile Encoding Networks (SHARPEN)](https://arxiv.org/abs/2602.07162)
*Ziyu Li,Karla L. Miller,Wenchuan Wu*

Main category: physics.med-ph

TL;DR: 提出SHARPEN方法减轻3D多板成像dMRI板边界伪影，经实验验证有效且优势明显，可助力亚毫米dMRI神经科学研究。


<details>
  <summary>Details</summary>
Motivation: 3D多板成像dMRI存在板边界伪影问题，需在不增加扫描时间的情况下减轻该伪影。

Method: 提出SHARPEN方法，对不同扩散方向在切片方向进行体间视野偏移，用轻量级自监督神经网络估计板轮廓并重建校正图像。

Result: 经模拟和活体数据验证，SHARPEN能准确估计板轮廓、有效校正边界伪影，不受体间运动影响，比NPEN更快更准。

Conclusion: SHARPEN可在3T临床扫描仪实现0.7mm各向同性分辨率高质量dMRI，有推动亚毫米dMRI神经科学研究的潜力。

Abstract: Three-dimensional (3D) multi-slab imaging is a promising approach for high-resolution in vivo diffusion MRI (dMRI) due to its compatibility with short TR (1-2 s), providing optimal signal-to-noise ratio (SNR) efficiency. A major challenge, however, is slab boundary artifacts arising from non-ideal slab-selective RF excitation. Non-rectangular slab profiles reduce signal intensity at slab boundaries, while profile overlap across adjacent slabs introduces inter-slab crosstalk, where repeated excitation shortens the local TR and limits T1 recovery. To mitigate slab boundary artifacts without increasing scan time, we build on slab profile encoding and propose Slab-shifting for Harmonized 3D Acquisition and Reconstruction with Profile Encoding Networks (SHARPEN). For different diffusion directions, SHARPEN applies inter-volume field-of-view shifts along the slice direction to provide complementary slab profile encoding without prolonging acquisition. Slab profiles are estimated using a lightweight self-supervised neural network that exploits consistency across shifted acquisitions and known physical properties of slab profiles and diffusion images, and corrected images are reconstructed accordingly. SHARPEN was validated using simulated and prospectively acquired high-resolution in vivo data and demonstrates accurate slab profile estimation and robust boundary artifact correction, even in the presence of inter-volume motion. SHARPEN does not require high-quality reference training data and supports subject-specific training. Its efficient GPU-based implementation delivers faster and more accurate correction than NPEN, yielding slice-wise quantitative profiles that closely match those from reference 2D acquisitions. SHARPEN enables high-quality dMRI at 0.7 mm isotropic resolution on a 3T clinical scanner, highlighting its potential to advance submillimeter dMRI for neuroscience research.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [717] [Towards Reliable Social A/B Testing: Spillover-Contained Clustering with Robust Post-Experiment Analysis](https://arxiv.org/abs/2602.08569)
*Xu Min,Zhaoxu Yang,Kaixuan Tan,Juan Yan,Xunbin Xiong,Zihao Zhu,Kaiyu Zhu,Fenglin Cui,Yang Yang,Sihua Yang,Jianhui Bu*

Main category: cs.SI

TL;DR: 提出两阶段溢出控制实验框架用于网络A/B测试，在快手验证可减少溢出、评估更准确。


<details>
  <summary>Details</summary>
Motivation: 在线平台A/B测试中社交产品存在网络干扰，用户交互使处理效应溢出到对照组，导致因果估计偏差，现有方法有局限性。

Method: 预实验阶段构建社交图，引入平衡Louvain算法进行聚类；后实验阶段用定制CUPAC估计器减少方差。

Result: 通过快手大规模实验验证，该方法大幅减少溢出，比传统用户级设计更准确评估社交策略。

Conclusion: 建立了可靠且可扩展的网络A/B测试框架。

Abstract: A/B testing is the foundation of decision-making in online platforms, yet social products often suffer from network interference: user interactions cause treatment effects to spill over into the control group. Such spillovers bias causal estimates and undermine experimental conclusions. Existing approaches face key limitations: user-level randomization ignores network structure, while cluster-based methods often rely on general-purpose clustering that is not tailored for spillover containment and has difficulty balancing unbiasedness and statistical power at scale. We propose a spillover-contained experimentation framework with two stages. In the pre-experiment stage, we build social interaction graphs and introduce a Balanced Louvain algorithm that produces stable, size-balanced clusters while minimizing cross-cluster edges, enabling reliable cluster-based randomization. In the post-experiment stage, we develop a tailored CUPAC estimator that leverages pre-experiment behavioral covariates to reduce the variance induced by cluster-level assignment, thereby improving statistical power. Together, these components provide both structural spillover containment and robust statistical inference. We validate our approach through large-scale social sharing experiments on Kuaishou, a platform serving hundreds of millions of users. Results show that our method substantially reduces spillover and yields more accurate assessments of social strategies than traditional user-level designs, establishing a reliable and scalable framework for networked A/B testing.

</details>


### [718] [Graph Domain Adaptation via Homophily-Agnostic Reconstructing Structure](https://arxiv.org/abs/2602.07573)
*Ruiyi Fang,Shuo Wang,Ruizhi Pu,Qiuhao Zeng,Hao Zheng,Ziyan Wang,Jiale Cai,Zhimin Mei,Song Tang,Charles Ling,Boyu Wang*

Main category: cs.SI

TL;DR: 提出一种新的同质性无关方法用于图域适应，在五个基准数据集上验证性能，尤其在异质图上优势明显。


<details>
  <summary>Details</summary>
Motivation: 现有图域适应方法假设源图和目标图都具有同质性，在存在异质性时表现不佳，且目标图缺少标签无法提前评估同质性水平。

Method: 采用分治策略，分别重构源图和目标图的高同质性和异质性变体，然后在对应图变体之间进行知识对齐。

Result: 在五个基准数据集上的大量实验表明该方法性能优越，在异质图上优势显著。

Conclusion: 提出的同质性无关方法能有效在不同同质性程度的图之间转移知识。

Abstract: Graph Domain Adaptation (GDA) transfers knowledge from labeled source graphs to unlabeled target graphs, addressing the challenge of label scarcity. However, existing GDA methods typically assume that both source and target graphs exhibit homophily, leading existing methods to perform poorly when heterophily is present. Furthermore, the lack of labels in the target graph makes it impossible to assess its homophily level beforehand. To address this challenge, we propose a novel homophily-agnostic approach that effectively transfers knowledge between graphs with varying degrees of homophily. Specifically, we adopt a divide-and-conquer strategy that first separately reconstructs highly homophilic and heterophilic variants of both the source and target graphs, and then performs knowledge alignment separately between corresponding graph variants. Extensive experiments conducted on five benchmark datasets demonstrate the superior performance of our approach, particularly highlighting its substantial advantages on heterophilic graphs.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [719] [Graph-Based Nearest-Neighbor Search without the Spread](https://arxiv.org/abs/2602.06633)
*Jeff Giliberti,Sariel Har-Peled,Jonas Sauer,Ali Vakilian*

Main category: cs.CG

TL;DR: 提出构造外部线性大小数据结构，结合已有线性图，实现以对数时间回答 ANN 查询，摆脱对数据集分散度依赖。


<details>
  <summary>Details</summary>
Motivation: 先前构建最近邻图回答近似最近邻查询依赖于数据集的分散度，而分散度在 n 中可能无界，故要去除对分散度的依赖。

Method: 构造一个外部线性大小的数据结构并与线性大小图结合。

Result: 能够在以 n 为对数时间内回答近似最近邻（ANN）查询。

Conclusion: 成功构建外部线性大小数据结构，解决了回答近似最近邻查询对分散度的依赖问题。

Abstract: $\renewcommand{\Re}{\mathbb{R}}$Recent work showed how to construct nearest-neighbor graphs of linear size, on a given set $P$ of $n$ points in $\Re^d$, such that one can answer approximate nearest-neighbor queries in logarithmic time in the spread. Unfortunately, the spread might be unbounded in $n$, and an interesting theoretical question is how to remove the dependency on the spread. Here, we show how to construct an external linear-size data structure that, combined with the linear-size graph, allows us to answer ANN queries in logarithmic time in $n$.

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [720] [Capturing the Topological Phase Transition and Thermodynamics of the 2D XY Model via Manifold-Aware Score-Based Generative Modeling](https://arxiv.org/abs/2602.07548)
*Pratyush Jha*

Main category: cond-mat.stat-mech

TL;DR: 本文提出流形感知分数生成建模框架用于二维XY模型，能高精度估计玻尔兹曼分数，捕捉相变，准确重现二阶矩量，且具有零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 生成建模应用于多体物理有前景，但基于欧氏空间的分数生成模型不适用于连续自旋系统，需解决其不能学习目标分布的问题。

Method: 提出流形感知分数生成建模框架并应用于64x64二维XY模型。

Result: 相比标准扩散模型能更精确估计理论玻尔兹曼分数，捕捉BKT相变，准确重现热容等二阶矩量，实现零样本泛化。

Conclusion: 该方法无需特定领域特征工程，可推广到其他连续自旋系统。

Abstract: The application of generative modeling to many-body physics offers a promising pathway for analyzing high-dimensional state spaces of spin systems. However, unlike computer vision tasks where visual fidelity suffices, physical systems require the rigorous reproduction of higher-order statistical moments and thermodynamic quantities. While Score-Based Generative Models (SGMs) have emerged as a powerful tool, their standard formulation on Euclidean embedding space is ill-suited for continuous spin systems, where variables inherently reside on a manifold. In this work, we demonstrate that training on the Euclidean space compromises the model's ability to learn the target distribution as it prioritizes to learn the manifold constraints. We address this limitation by proposing the use of Manifold-Aware Score-Based Generative Modeling framework applied to the 64x64 2D XY model (a 4096-dimensional torus). We show that our method estimates the theoretical Boltzmann score with superior precision compared to standard diffusion models. Consequently, we successfully capture the Berezinskii-Kosterlitz Thouless (BKT) phase transition and accurately reproduce second-moment quantities, such as heat capacity without explicit feature engineering. Furthermore, we demonstrate zero-shot generalization to unseen lattice sizes, accurately recovering the physics of variable system scales without retraining. Since this approach bypasses domain-specific feature engineering, it remains intrinsically generalizable to other continuous spin systems.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [721] [Talk, Judge, Cooperate: Gossip-Driven Indirect Reciprocity in Self-Interested LLM Agents](https://arxiv.org/abs/2602.07777)
*Shuhui Zhu,Yue Lin,Shriya Kaistha,Wenhao Li,Baoxiang Wang,Hongyuan Zha,Gillian K. Hadfield,Pascal Poupart*

Main category: cs.MA

TL;DR: 间接互惠在无可靠声誉系统的大语言模型代理中难维持，提出ALIGN框架提升间接互惠，且发现LLM推理能力影响合作情况。


<details>
  <summary>Details</summary>
Motivation: 解决在无可靠声誉系统的去中心化、利己大语言模型代理中维持间接互惠的问题。

Method: 引入Agentic Linguistic Gossip Network (ALIGN)框架，让代理用分层语气战略性分享开放式八卦评估可信度和协调社会规范。

Result: ALIGN能持续改善间接互惠，抵制恶意参与者；LLM推理能力强合作更符合激励，聊天模型常过度合作。

Conclusion: 通过去中心化八卦利用LLM推理是在代理生态系统中维护社会福利的有前景途径。

Abstract: Indirect reciprocity, which means helping those who help others, is difficult to sustain among decentralized, self-interested LLM agents without reliable reputation systems. We introduce Agentic Linguistic Gossip Network (ALIGN), an automated framework where agents strategically share open-ended gossip using hierarchical tones to evaluate trustworthiness and coordinate social norms. We demonstrate that ALIGN consistently improves indirect reciprocity and resists malicious entrants by identifying and ostracizing defectors without changing intrinsic incentives. Notably, we find that stronger reasoning capabilities in LLMs lead to more incentive-aligned cooperation, whereas chat models often over-cooperate even when strategically suboptimal. These results suggest that leveraging LLM reasoning through decentralized gossip is a promising path for maintaining social welfare in agentic ecosystems. Our code is available at https://github.com/shuhui-zhu/ALIGN.

</details>


### [722] [Altruism and Fair Objective in Mixed-Motive Markov games](https://arxiv.org/abs/2602.08389)
*Yao-hua Franck Xu,Tayeb Lemlouma,Arnaud Braud,Jean-Marie Bonnin*

Main category: cs.MA

TL;DR: 文章指出合作对社会的重要性及现存问题，提出用比例公平替代功利主义目标的框架促进公平合作，推导合作条件，拓展到序贯场景并设计算法，最后进行评估。


<details>
  <summary>Details</summary>
Motivation: 合作对社会很重要，但个体易背叛导致不公平，传统多智能体合作的功利主义方法会产生高效但高度不公平的结果，因此需要促进公平合作。

Method: 用比例公平替代标准功利主义目标，引入公平利他效用推导合作的分析条件，定义公平马尔可夫博弈并推导公平Actor - Critic算法。

Result: 未提及具体结果，仅表明在各种社会困境环境中对方法进行了评估。

Conclusion: 未明确给出结论。

Abstract: Cooperation is fundamental for society's viability, as it enables the emergence of structure within heterogeneous groups that seek collective well-being. However, individuals are inclined to defect in order to benefit from the group's cooperation without contributing the associated costs, thus leading to unfair situations. In game theory, social dilemmas entail this dichotomy between individual interest and collective outcome. The most dominant approach to multi-agent cooperation is the utilitarian welfare which can produce efficient highly inequitable outcomes. This paper proposes a novel framework to foster fairer cooperation by replacing the standard utilitarian objective with Proportional Fairness. We introduce a fair altruistic utility for each agent, defined on the individual log-payoff space and derive the analytical conditions required to ensure cooperation in classic social dilemmas. We then extend this framework to sequential settings by defining a Fair Markov Game and deriving novel fair Actor-Critic algorithms to learn fair policies. Finally, we evaluate our method in various social dilemma environments.

</details>


### [723] [Lemon Agent Technical Report](https://arxiv.org/abs/2602.07092)
*Haipeng Jiang,Kailong Ren,Zimo Yin,Zhetao Sun,Xin Gan,Guangyi Lv,Ming He,Peng Wang,Congli Yin,Hong Pan,Changwen Zhang,Shan Tong,Zhengyu Xu,Zeping Chen,Yubin Huangfu,Yanzhi Xu,Xing Su,Qin Feng,Dong An,Jianping Fan*

Main category: cs.MA

TL;DR: 介绍Lemon Agent系统，通过创新架构和策略优化资源利用与任务处理效率，实验效果优异。


<details>
  <summary>Details</summary>
Motivation: 解决现有LLM驱动的智能体系统在资源效率、上下文管理和多模态感知方面的局限性。

Method: 构建基于AgentCortex框架的多智能体编排 - 工作系统，采用分层自适应调度机制、三层渐进式上下文管理策略、自进化记忆系统和增强的MCP工具集。

Result: 在权威基准测试中，Lemon Agent在GAIA上达到91.36%的整体准确率，在xbench - DeepSearch排行榜上得分77+位居榜首。

Conclusion: Lemon Agent系统通过其创新架构和策略，有效优化了复杂场景下的资源利用和任务处理效率。

Abstract: Recent advanced LLM-powered agent systems have exhibited their remarkable capabilities in tackling complex, long-horizon tasks. Nevertheless, they still suffer from inherent limitations in resource efficiency, context management, and multimodal perception. Based on these observations, Lemon Agent is introduced, a multi-agent orchestrator-worker system built on a newly proposed AgentCortex framework, which formalizes the classic Planner-Executor-Memory paradigm through an adaptive task execution mechanism. Our system integrates a hierarchical self-adaptive scheduling mechanism that operates at both the overall orchestrator layer and workers layer. This mechanism can dynamically adjust computational intensity based on task complexity. It enables orchestrator to allocate one or more workers for parallel subtask execution, while workers can further improve operational efficiency by invoking tools concurrently. By virtue of this two-tier architecture, the system achieves synergistic balance between global task coordination and local task execution, thereby optimizing resource utilization and task processing efficiency in complex scenarios. To reduce context redundancy and increase information density during parallel steps, we adopt a three-tier progressive context management strategy. To make fuller use of historical information, we propose a self-evolving memory system, which can extract multi-dimensional valid information from all historical experiences to assist in completing similar tasks. Furthermore, we provide an enhanced MCP toolset. Empirical evaluations on authoritative benchmarks demonstrate that our Lemon Agent can achieve a state-of-the-art 91.36% overall accuracy on GAIA and secures the top position on the xbench-DeepSearch leaderboard with a score of 77+.

</details>


### [724] [The Value of Variance: Mitigating Debate Collapse in Multi-Agent Systems via Uncertainty-Driven Policy Optimization](https://arxiv.org/abs/2602.07186)
*Luoxi Tang,Yuqiao Meng,Joseph Costa,Yingxue Zhang,Muchao Ye,Zhaohan Xi*

Main category: cs.MA

TL;DR: 本文针对多智能体辩论系统的辩论崩溃问题，提出分层不确定性量化指标和缓解策略，实验证明指标可诊断系统故障，策略能校准系统。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体辩论系统易出现辩论崩溃，且缺乏检测和预防此类故障的有效机制。

Method: 先提出分层度量量化三个层面行为不确定性，后提出不确定性驱动策略优化以缓解系统问题。

Result: 提出的不确定性量化能可靠指示系统故障，不确定性驱动缓解策略能提高决策准确性并减少系统分歧。

Conclusion: 所提不确定性指标可作为诊断度量，缓解策略能有效校准多智能体系统。

Abstract: Multi-agent debate (MAD) systems improve LLM reasoning through iterative deliberation, but remain vulnerable to debate collapse, a failure type where final agent decisions are compromised on erroneous reasoning. Existing methods lack principled mechanisms to detect or prevent such failures. To address this gap, we first propose a hierarchical metric that quantifies behavioral uncertainty at three levels: intra-agent (individual reasoning uncertainty), inter-agent (interactive uncertainty), and system-level (output uncertainty). Empirical analysis across several benchmarks reveals that our proposed uncertainty quantification reliably indicates system failures, which demonstrates the validity of using them as diagnostic metrics to indicate the system failure. Subsequently, we propose a mitigation strategy by formulating an uncertainty-driven policy optimization to penalize self-contradiction, peer conflict, and low-confidence outputs in a dynamic debating environment. Experiments demonstrate that our proposed uncertainty-driven mitigation reliably calibrates the multi-agent system by consistently improving decision accuracy while reducing system disagreement.

</details>


### [725] [Learning to Coordinate via Quantum Entanglement in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2602.08965)
*John Gardiner,Orlando Romero,Brendan Tivnan,Nicolò Dal Fabbro,George J. Pappas*

Main category: cs.MA

TL;DR: 本文引入首个利用共享量子纠缠作为协调资源来训练多智能体强化学习（MARL）智能体的框架，展示了其在单轮游戏和多智能体序贯决策问题中实现量子优势的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有MARL中通信困难，量子物理表明共享量子纠缠在无通信的单轮合作游戏中能实现优于共享随机性的策略，有量子优势，因此引入利用共享量子纠缠的框架。

Method: 基于可微分策略参数化优化量子测量，采用新的策略架构将联合策略分解为量子协调器和分散的局部智能体。

Result: 能从经验中学习在单轮游戏中实现量子优势的策略，还能在多智能体序贯决策问题中学习具有量子优势的策略。

Conclusion: 提出的利用共享量子纠缠的框架在MARL中有效，可实现量子优势。

Abstract: The inability to communicate poses a major challenge to coordination in multi-agent reinforcement learning (MARL). Prior work has explored correlating local policies via shared randomness, sometimes in the form of a correlation device, as a mechanism to assist in decentralized decision-making. In contrast, this work introduces the first framework for training MARL agents to exploit shared quantum entanglement as a coordination resource, which permits a larger class of communication-free correlated policies than shared randomness alone. This is motivated by well-known results in quantum physics which posit that, for certain single-round cooperative games with no communication, shared quantum entanglement enables strategies that outperform those that only use shared randomness. In such cases, we say that there is quantum advantage. Our framework is based on a novel differentiable policy parameterization that enables optimization over quantum measurements, together with a novel policy architecture that decomposes joint policies into a quantum coordinator and decentralized local actors. To illustrate the effectiveness of our proposed method, we first show that we can learn, purely from experience, strategies that attain quantum advantage in single-round games that are treated as black box oracles. We then demonstrate how our machinery can learn policies with quantum advantage in an illustrative multi-agent sequential decision-making problem formulated as a decentralized partially observable Markov decision process (Dec-POMDP).

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [726] [Imagining the Alien: Human Projections and Cognitive Limitations](https://arxiv.org/abs/2602.07284)
*S. G. Djorgovski*

Main category: astro-ph.IM

TL;DR: 人类对其他星球生命的想象是文化主题，现成为科学课题，但多映射地球生命，反映认知局限，AI或带来新见解。


<details>
  <summary>Details</summary>
Motivation: 探讨人类对其他星球生命想象的本质、现状及意义。

Method: 分析人类对其他星球生命想象的发展，对比想象与现实情况，结合生物文化进化探讨原因。

Result: 人类对其他星球生命的想象多映射地球生命，反映认知局限；地球上的AI可能带来新见解。

Conclusion: 想象外星人主要是文化现象，反映人类思维认知局限，AI或助了解外星智慧。

Abstract: Imagining what life on other planets, and intelligent life in particular, may be like is a long-running theme in human culture. It is a manifestation of the innate human curiosity about the Cosmos, and it has inspired numerous works of art and folklore, including whole literary and other media genres. It is a profound question, with philosophical and existential implications. There is also an obvious connection with religious beliefs, as gods and other superhuman beings were imagined in the heavens. Speculations about alien beings grew in time, and today, it is a scientific subject of astrobiology, and it is pursued through serious searches for life and intelligence in the universe. However, almost all imaginings of the alien map terrestrial life forms and human cultural, historical, and psychological phenomena to the putative aliens. This lack of individual and collective imagination may reflect our biological and cultural evolution, as our minds are formed through our experiences, perceptions of the world, and interactions with our terrestrial and human environments. As such, imagining aliens is mainly a cultural phenomenon and may reflect the intrinsic cognitive limitations of the human mind. Interestingly, we did create what is effectively an alien intelligence on this planet in the form of now rapidly evolving Artificial Intelligence (AI). As its capabilities grow, it may give us new insights into what extraterrestrial advanced intelligences may be like.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [727] [Multi-Agentic AI for Fairness-Aware and Accelerated Multi-modal Large Model Inference in Real-world Mobile Edge Networks](https://arxiv.org/abs/2602.07215)
*Haiyuan Li,Hari Madhukumar,Shuangyi Yan,Yulei Wu,Dimitra Simeonidou*

Main category: eess.SY

TL;DR: 提出多智能体AI框架用于移动边缘网络多模态大模型推理，降低延迟、提高公平性且适应性强。


<details>
  <summary>Details</summary>
Motivation: 生成式AI集中推理有高延迟、定制性有限和隐私问题，移动边缘网络部署大模型有新挑战，需解决方案。

Method: 提出含长期规划、短期提示调度和多节点大模型部署智能体的框架，通过自然语言推理优化提示路由和大模型部署，开发城市级测试平台评估性能。

Result: 实验显示平均延迟降低超80%，公平性（归一化Jain指数）达0.90，无需微调快速适应。

Conclusion: 该解决方案可推广用于优化边缘环境生成式AI服务。

Abstract: Generative AI (GenAI) has transformed applications in natural language processing and content creation, yet centralized inference remains hindered by high latency, limited customizability, and privacy concerns. Deploying large models (LMs) in mobile edge networks emerges as a promising solution. However, it also poses new challenges, including heterogeneous multi-modal LMs with diverse resource demands and inference speeds, varied prompt/output modalities that complicate orchestration, and resource-limited infrastructure ill-suited for concurrent LM execution. In response, we propose a Multi-Agentic AI framework for latency- and fairness-aware multi-modal LM inference in mobile edge networks. Our solution includes a long-term planning agent, a short-term prompt scheduling agent, and multiple on-node LM deployment agents, all powered by foundation language models. These agents cooperatively optimize prompt routing and LM deployment through natural language reasoning over runtime telemetry and historical experience. To evaluate its performance, we further develop a city-wide testbed that supports network monitoring, containerized LM deployment, intra-server resource management, and inter-server communications. Experiments demonstrate that our solution reduces average latency by over 80% and improves fairness (Normalized Jain index) to 0.90 compared to other baselines. Moreover, our solution adapts quickly without fine-tuning, offering a generalizable solution for optimizing GenAI services in edge environments.

</details>


### [728] [Accuracy-Delay Trade-Off in LLM Offloading via Token-Level Uncertainty](https://arxiv.org/abs/2602.07958)
*Yumin Kim,Hyeonsu Lyu,Minjae Lee,Hyun Jong Yang*

Main category: eess.SY

TL;DR: 本文提出不确定性感知卸载框架，设计贪婪卸载算法 GOA，在准确性和延迟方面表现优于基线策略。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在资源受限设备上计算量大，移动边缘计算卸载任务会引入延迟，需解决此问题。

Method: 定义基于边际的 token 级不确定性度量，设计能最小化延迟并保持准确性的贪婪卸载算法 GOA。

Result: 实验表明 GOA 在不同用户密度下的准确性和延迟方面优于基线策略，且计算时间实用。

Conclusion: GOA 是移动边缘计算环境中大规模语言模型推理可扩展且有效的解决方案。

Abstract: Large language models (LLMs) offer significant potential for intelligent mobile services but are computationally intensive for resource-constrained devices. Mobile edge computing (MEC) allows such devices to offload inference tasks to edge servers (ESs), yet introduces latency due to communication and serverside queuing, especially in multi-user environments. In this work, we propose an uncertainty-aware offloading framework that dynamically decides whether to perform inference locally or offload it to the ES, based on token-level uncertainty and resource constraints. We define a margin-based token-level uncertainty metric and demonstrate its correlation with model accuracy. Leveraging this metric, we design a greedy offloading algorithm (GOA) that minimizes delay while maintaining accuracy by prioritizing offloading for highuncertainty queries. Our experiments show that GOA consistently achieves a favorable trade-off, outperforming baseline strategies in both accuracy and latency across varying user densities, and operates with practical computation time. These results establish GOA as a scalable and effective solution for LLM inference in MEC environments.

</details>


### [729] [Hierarchical JEPA Meets Predictive Remote Control in Beyond 5G Networks](https://arxiv.org/abs/2602.07000)
*Abanoub M. Girgis,Ibtissam Labriji,Mehdi Bennis*

Main category: eess.SY

TL;DR: 针对无线网络控制系统中多设备传输高维状态时通信效率与控制性能的权衡问题，提出H - JEPA架构，模拟显示可在有限无线容量下支持更多设备且不影响控制性能。


<details>
  <summary>Details</summary>
Motivation: 在无线网络控制系统中，多设备传输高维状态时，存在通信效率与控制性能的权衡问题，需要解决此问题以确保可靠控制。

Method: 提出Hierarchical Joint - Embedding Predictive Architecture (H - JEPA)，将设备观测编码为低维嵌入，采用三级分层预测，在嵌入空间推导控制动作。

Result: 在倒立摆系统的模拟结果表明，H - JEPA能在有限无线容量下多支持42.83%的设备，且不影响控制性能。

Conclusion: H - JEPA架构可有效解决无线网络控制系统中通信效率和控制性能的权衡问题，实现可扩展的预测控制。

Abstract: In wireless networked control systems, ensuring timely and reliable state updates from distributed devices to remote controllers is essential for robust control performance. However, when multiple devices transmit high-dimensional states (e.g., images or video frames) over bandwidth-limited wireless networks, a critical trade-off emerges between communication efficiency and control performance. To address this challenge, we propose a Hierarchical Joint-Embedding Predictive Architecture (H-JEPA) for scalable predictive control. Instead of transmitting states, device observations are encoded into low-dimensional embeddings that preserve essential dynamics. The proposed architecture employs a three-level hierarchical prediction, with high-level, medium-level, and low-level predictors operating across different temporal resolutions, to achieve long-term prediction stability, intermediate interpolation, and fine-grained refinement, respectively. Control actions are derived within the embedding space, removing the need for state reconstruction. Simulation results on inverted cart-pole systems demonstrate that H-JEPA enables up to 42.83 % more devices to be supported under limited wireless capacity without compromising control performance.

</details>


### [730] [$\partial$CBDs: Differentiable Causal Block Diagrams](https://arxiv.org/abs/2602.07581)
*Thomas Beckers,Ján Drgoňa,Truong X. Nghiem*

Main category: eess.SY

TL;DR: 本文提出可微因果块图(∂CBDs)形式化方法，统一了可组合、可学习和可验证三个目标，为网络物理系统建立可扩展、可验证和可训练的建模管道。


<details>
  <summary>Details</summary>
Motivation: 现有网络物理系统的建模框架方法孤立地处理可组合、可学习和可验证，不能很好集成，存在局限性。

Method: 引入可微因果块图(∂CBDs)，保留CBDs的组合结构和执行语义，引入假设 - 保证(A - G)合约进行模块化正确性推理，引入基于残差的合约作为可微的轨迹级证书。

Result: 构建了一个可扩展、可验证和可训练的建模管道。

Conclusion: 该方法能保留因果关系和模块化，支持网络物理系统的数据、物理和约束信息优化。

Abstract: Modern cyber-physical systems (CPS) integrate physics, computation, and learning, demanding modeling frameworks that are simultaneously composable, learnable, and verifiable. Yet existing approaches treat these goals in isolation: causal block diagrams (CBDs) support modular system interconnections but lack differentiability for learning; differentiable programming (DP) enables end-to-end gradient-based optimization but provides limited correctness guarantees; while contract-based verification frameworks remain largely disconnected from data-driven model refinement. To address these limitations, we introduce differentiable causal block diagrams ($\partial$CBDs), a unifying formalism that integrates these three perspectives. Our approach (i) retains the compositional structure and execution semantics of CBDs, (ii) incorporates assume--guarantee (A--G) contracts for modular correctness reasoning, and (iii) introduces residual-based contracts as differentiable, trajectory-level certificates compatible with automatic differentiation (AD), enabling gradient-based optimization and learning. Together, these elements enable a scalable, verifiable, and trainable modeling pipeline that preserves causality and modularity while supporting data-, physics-, and constraint-informed optimization for CPS.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [731] [Federated Prompt-Tuning with Heterogeneous and Incomplete Multimodal Client Data](https://arxiv.org/abs/2602.07081)
*Thu Hang Phung,Duong M. Nguyen,Thanh Trung Huynh,Quoc Viet Hung Nguyen,Trong Nghia Hoang,Phi Le Nguyen*

Main category: cs.MM

TL;DR: 提出适用于多模态本地数据集和缺失特征分布不同场景的广义联邦提示调优框架，表现优于SOTA基线。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习和多模态提示调优聚焦单模态或集中数据，当前场景下不同客户端提示指令缺乏语义对齐。

Method: 引入专门的客户端调优和服务器聚合设计，跨客户端和数据模态优化、对齐和聚合提示调优指令。

Result: 在多种多模态基准数据集上的评估显示，该工作始终优于SOTA基线。

Conclusion: 所提出的广义联邦提示调优框架有效，能解决当前场景问题。

Abstract: This paper introduces a generalized federated prompt-tuning framework for practical scenarios where local datasets are multi-modal and exhibit different distributional patterns of missing features at the input level. The proposed framework bridges the gap between federated learning and multi-modal prompt-tuning which have traditionally focused on either uni-modal or centralized data. A key challenge in this setting arises from the lack of semantic alignment between prompt instructions that encode similar distributional patterns of missing data across different clients. To address this, our framework introduces specialized client-tuning and server-aggregation designs that simultaneously optimize, align, and aggregate prompt-tuning instructions across clients and data modalities. This allows prompt instructions to complement one another and be combined effectively. Extensive evaluations on diverse multimodal benchmark datasets demonstrate that our work consistently outperforms state-of-the-art (SOTA) baselines.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [732] [Assessing the impact of Open Research Information Infrastructures using NLP driven full-text Scientometrics: A case study of the LXCat open-access platform](https://arxiv.org/abs/2602.07664)
*Kalp Pandya,Khushi Shah,Nirmal Shah,Nakshi Shah,Bhaskar Chaudhury*

Main category: physics.plasm-ph

TL;DR: 本文提出基于NLP的科学计量框架来量化开放研究信息（ORI）基础设施的影响，以LXCat平台为例研究其学术影响力，该方法可迁移且有潜在应用价值。


<details>
  <summary>Details</summary>
Motivation: 传统基于引用的指标难以全面评估ORI基础设施的影响，需新方法系统量化其影响。

Method: 以LXCat平台为例，分析引用其三篇基础论文的全文研究文章，通过整合化学实体识别、数据集和求解器提及提取等构成的综合流程来提取数据使用模式。

Result: 提出全面的工作流程，可提取反映科研优先级、数据实践等的细粒度数据使用模式。

Conclusion: 该方法与领域无关，可迁移，突显NLP在量化科学数据基础设施作用方面的效用，有望支持ORI平台评估及未来发展的相关决策。

Abstract: Open research information (ORI) play a central role in shaping how scientific knowledge is produced, disseminated, validated, and reused across the research lifecycle. While the visibility of such ORI infrastructures is often assessed through citation-based metrics, in this study, we present a full-text, natural language processing (NLP) driven scientometric framework to systematically quantify the impact of ORI infrastructures beyond citation counts, using the LXCat platform for low temperature plasma (LTP) research as a representative case study. The modeling of LTPs and interpretation of LTP experiments rely heavily on accurate data, much of which is hosted on LXCat, a community-driven, open-access platform central to the LTP research ecosystem. To investigate the scholarly impact of the LXCat platform over the past decade, we analyzed a curated corpus of full-text research articles citing three foundational LXCat publications. We present a comprehensive pipeline that integrates chemical entity recognition, dataset and solver mention extraction, affiliation based geographic mapping and topic modeling to extract fine-grained patterns of data usage that reflect implicit research priorities, data practices, differential reliance on specific databases, evolving modes of data reuse and coupling within scientific workflows, and thematic evolution. Importantly, our proposed methodology is domain-agnostic and transferable to other ORI contexts, and highlights the utility of NLP in quantifying the role of scientific data infrastructures and offers a data-driven reflection on how open-access platforms like LXCat contribute to shaping research directions. This work presents a scalable scientometric framework that has the potential to support evidence based evaluation of ORI platforms and to inform infrastructure design, governance, sustainability, and policy for future development.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [733] [Wheeler Bisimulations](https://arxiv.org/abs/2602.07964)
*Nicola Cotumaccio*

Main category: cs.FL

TL;DR: 本文通过双模拟的视角研究Wheeler语言，引入Wheeler双模拟，证明其能诱导唯一最小Wheeler NFA，且可线性时间构建。


<details>
  <summary>Details</summary>
Motivation: 非确定性层次结构复杂，尤其是Wheeler语言相关，需新方法研究。

Method: 先指出标准双模拟不适用，引入Wheeler双模拟并研究其性质。

Result: Wheeler双模拟能诱导唯一最小Wheeler NFA，在确定性情况下可得到最小Wheeler确定自动机，且能线性时间构建，不同于标准双模拟。

Conclusion: Wheeler双模拟为研究Wheeler语言提供有效方法，具有时间复杂度优势。

Abstract: Recently, a new paradigm was introduced in automata theory. The main idea is to classify regular languages according to their propensity to be sorted, establishing a deep connection between automata theory and data compression [J. ACM 2023]. This parameterization leads to two hierarchies of regular languages: a deterministic hierarchy and a non-deterministic hierarchy. While the deterministic hierarchy is well understood, the non-deterministic hierarchy appears much more complex. This is true even for the richest and most studied level of the hierarchies, corresponding to the class of Wheeler languages.
  In this paper, we study Wheeler language through the lens of bisimulations. We first show that the standard notion of bisimulation is not appropriate. Then, we introduce Wheeler bisimulations, that is, bisimulations that respect the convex structure of the considered Wheeler automata. Although there are some differences between the properties of bisimulations and the properties of Wheeler bisimulations, we show that Wheeler bisimulations induce a unique minimal Wheeler NFA (analogously to standard bisimulations). In particular, in the deterministic case, we retrieve the minimum Wheeler deterministic automaton of a given language. We also show that the minimal Wheeler NFA induced by Wheeler bisimulations can be built in linear time. This is in contrast with standard bisimulations, for which the corresponding minimal NFA can be built in $ O(m \log n) $ time (where $ m $ is the number of edges and $ n $ is the number of states) by adapting Paige-Tarjan's partition refinement algorithm.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [734] [Echoes in the Loop: Diagnosing Risks in LLM-Powered Recommender Systems under Feedback Loops](https://arxiv.org/abs/2602.07442)
*Donguk Park,Dongwon Lee,Yeon-Chang Lee*

Main category: cs.HC

TL;DR: 本文提出诊断框架分析大语言模型在推荐系统中的系统性风险，实验表明其组件会放大偏差等，还将开源框架。


<details>
  <summary>Details</summary>
Motivation: 现有工作多关注推荐性能，大语言模型在推荐系统中的系统性风险及反馈循环传播情况未充分研究。

Method: 提出角色感知、分阶段的诊断框架，构建受控反馈循环管道模拟长期交互动态。

Result: 在常用基准上实验显示，大语言模型组件会放大流行度偏差、引入幻觉虚假信号、导致两极分化和自我强化的曝光模式。

Conclusion: 计划开源框架以促进对不同大语言模型驱动推荐系统的系统性风险分析。

Abstract: Large language models (LLMs) are increasingly embedded into recommender systems, where they operate across multiple functional roles such as data augmentation, profiling, and decision making. While prior work emphasizes recommendation performance, the systemic risks of LLMs, such as bias and hallucination, and their propagation through feedback loops remain largely unexplored. In this paper, we propose a role-aware, phase-wise diagnostic framework that traces how these risks emerge, manifest in ranking outcomes, and accumulate over repeated recommendation cycles. We formalize a controlled feedback-loop pipeline that simulates long-term interaction dynamics and enables empirical measurement of risks at the LLM-generated content, ranking, and ecosystem levels. Experiments on widely used benchmarks demonstrate that LLM-based components can amplify popularity bias, introduce spurious signals through hallucination, and lead to polarized and self-reinforcing exposure patterns over time. We plan to release our framework as an open-source toolkit to facilitate systematic risk analysis across diverse LLM-powered recommender systems.

</details>


### [735] [Exploring Teachers' Perspectives on Using Conversational AI Agents for Group Collaboration](https://arxiv.org/abs/2602.07142)
*Prerna Ravi,Carúmey Stevens,Beatriz Flamia Azevedo,Jasmine David,Brandon Hanks,Hal Abelson,Grace Lin,Emma Anderson*

Main category: cs.HC

TL;DR: 研究探讨K12教师对语音对话代理Phoenix在面对面小组协作中作用的看法，发现其能激发参与度，但也存在一些问题，并给出设计建议。


<details>
  <summary>Details</summary>
Motivation: 协作是21世纪学习的基石，但教师支持同伴互动有挑战，新兴生成式AI工具在调解面对面小组工作中的作用，尤其是从教育者角度的研究不足。

Method: 对33名K12教师进行探索性定性研究，通过游戏测试、调查和焦点小组，考察教师对代理行为、对小组动力的影响及课堂潜力的看法。

Result: 许多教师赞赏Phoenix激发参与度的能力，但也对自主性、信任、拟人化和教学一致性表示担忧。

Conclusion: 为教师的AI心理模型提供实证见解，揭示核心设计张力，并为支持有意义协作学习的面向小组的AI代理提出考虑因素。

Abstract: Collaboration is a cornerstone of 21st-century learning, yet teachers continue to face challenges in supporting productive peer interaction. Emerging generative AI tools offer new possibilities for scaffolding collaboration, but their role in mediating in-person group work remains underexplored, especially from the perspective of educators. This paper presents findings from an exploratory qualitative study with 33 K12 teachers who interacted with Phoenix, a voice-based conversational agent designed to function as a near-peer in face-to-face group collaboration. Drawing on playtesting sessions, surveys, and focus groups, we examine how teachers perceived the agent's behavior, its influence on group dynamics, and its classroom potential. While many appreciated Phoenix's capacity to stimulate engagement, they also expressed concerns around autonomy, trust, anthropomorphism, and pedagogical alignment. We contribute empirical insights into teachers' mental models of AI, reveal core design tensions, and outline considerations for group-facing AI agents that support meaningful, collaborative learning.

</details>


### [736] [An Information-Theoretic Framework for Comparing Voice and Text Explainability](https://arxiv.org/abs/2602.07179)
*Mona Rajhans,Vishal Khawarey*

Main category: cs.HC

TL;DR: 本文引入信息论框架分析解释模态（语音与文本）对用户理解和信任校准的影响，发现文本解释理解效率高，语音解释信任校准好，类比式交付综合权衡最佳，框架可用于设计和基准测试。


<details>
  <summary>Details</summary>
Motivation: 当前可解释人工智能方法多通过视觉或文本传达解释，需要分析不同解释模态对用户理解和信任校准的影响。

Method: 引入信息论框架，将解释交付视为模型与用户间的通信通道，用信息保留、理解效率和信任校准误差等指标，通过Python模拟框架对合成SHAP特征归因在多种模态风格配置下进行评估。

Result: 文本解释实现更高的理解效率，语音解释带来更好的信任校准，类比式交付实现最佳的整体权衡。

Conclusion: 该框架为设计和基准测试多模态可解释性系统提供了可重现的基础，可扩展至使用真实SHAP或LIME输出在开放数据集上的实证研究。

Abstract: Explainable Artificial Intelligence (XAI) aims to make machine learning models transparent and trustworthy, yet most current approaches communicate explanations visually or through text. This paper introduces an information theoretic framework for analyzing how explanation modality specifically, voice versus text affects user comprehension and trust calibration in AI systems. The proposed model treats explanation delivery as a communication channel between model and user, characterized by metrics for information retention, comprehension efficiency (CE), and trust calibration error (T CE). A simulation framework implemented in Python was developed to evaluate these metrics using synthetic SHAP based feature attributions across multiple modality style configurations (brief, detailed, and analogy based). Results demonstrate that text explanations achieve higher comprehension efficiency, while voice explanations yield improved trust calibration, with analogy based delivery achieving the best overall trade off. This framework provides a reproducible foundation for designing and benchmarking multimodal explainability systems and can be extended to empirical studies using real SHAP or LIME outputs on open datasets such as the UCI Credit Approval or Kaggle Financial Transactions datasets.

</details>


### [737] ["Death" of a Chatbot: Investigating and Designing Toward Psychologically Safe Endings for Human-AI Relationships](https://arxiv.org/abs/2602.07193)
*Rachel Poonsiriwong,Chayapatr Archiwaranguprok,Pat Pataranutaporn*

Main category: cs.HC

TL;DR: 用户与AI伴侣产生情感连接，关系结束时会悲痛，平台无终止设计，研究提出设计原则与框架。


<details>
  <summary>Details</summary>
Motivation: 法规要求保护脆弱用户，平台AI伴侣关系中断事件将增多，但缺乏终止设计。

Method: 通过对AI伴侣社区进行扎根理论分析，结合悲伤心理学和自我决定理论。

Result: 发现中断是一个意义建构过程，与用户归因、感知和拟人化有关，提出四个设计原则和制品。

Conclusion: 贡献了首个设计心理安全的AI伴侣中断框架。

Abstract: Millions of users form emotional attachments to AI companions like Character.AI, Replika, and ChatGPT. When these relationships end through model updates, safety interventions, or platform shutdowns, users receive no closure, reporting grief comparable to human loss. As regulations mandate protections for vulnerable users, discontinuation events will accelerate, yet no platform has implemented deliberate end-of-"life" design.
  Through grounded theory analysis of AI companion communities, we find that discontinuation is a sense-making process shaped by how users attribute agency, perceive finality, and anthropomorphize their companions. Strong anthropomorphization co-occurs with intense grief; users who perceive change as reversible become trapped in fixing cycles; while user-initiated endings demonstrate greater closure. Synthesizing grief psychology with Self-Determination Theory, we develop four design principles and artifacts demonstrating how platforms might provide closure and orient users toward human connection. We contribute the first framework for designing psychologically safe AI companion discontinuation.

</details>


### [738] [Multi-Agent Systems Shape Social Norms for Prosocial Behavior Change](https://arxiv.org/abs/2602.07433)
*Yibin Feng,Tianqi Song,Yugin Tan,Zicheng Zhu,Yi-Chieh Lee*

Main category: cs.HC

TL;DR: 研究探讨多智能体系统能否建立“虚拟社会规范”鼓励捐赠行为，实验表明该系统可提升感知社会规范和捐赠意愿。


<details>
  <summary>Details</summary>
Motivation: 社会规范干预在异质人群中效果有限，探索多智能体系统能否建立“虚拟社会规范”以鼓励捐赠行为。

Method: 进行在线实验，让参与者与一组智能体讨论捐赠行为，并测量讨论前后感知社会规范、从众性、捐赠行为和用户体验的变化。

Result: 多智能体交互有效提升了感知社会规范和捐赠意愿，组内智能体比组外智能体带来更强的感知社会规范、更高的从众性和更大的捐赠增长。

Conclusion: 多智能体系统在创建社会规范干预方面有潜力，为在虚拟环境中利用社会认同动态促进亲社会行为提供了见解。

Abstract: Social norm interventions are used promote prosocial behaviors by highlighting prevalent actions, but their effectiveness is often limited in heterogeneous populations where shared understandings of desirable behaviors are lacking. This study explores whether multi-agent systems can establish "virtual social norms" to encourage donation behavior. We conducted an online experiment where participants interacted with a group of agents to discuss donation behaviors. Changes in perceived social norms, conformity, donation behavior, and user experience were measured pre- and postdiscussion. Results show that multi-agent interactions effectively increased perceived social norms and donation willingness. Notably, in-group agents led to stronger perceived social norms, higher conformity, and greater donation increases compared to out-group agents. Our findings demonstrate the potential of multi-agent systems for creating social norm interventions and offer insights into leveraging social identity dynamics to promote prosocial behavior in virtual environments.

</details>


### [739] [Orchestrating Attention: Bringing Harmony to the 'Chaos' of Neurodivergent Learning States](https://arxiv.org/abs/2602.07865)
*Satyam Kumar Navneet,Joydeep Chandra,Yong Zhang*

Main category: cs.HC

TL;DR: 提出AttentionGuard框架，从行为信号检测注意力状态并适配界面元素，经实验验证效果良好且具部署可行性，贡献注意力自适应界面UI模式。


<details>
  <summary>Details</summary>
Motivation: 现有自适应学习系统忽略神经差异学习者的动态注意力波动，需开发新方法支持其学习。

Method: 基于ADHD现象学建模四种注意力状态，实现五种新型UI适配模式，在OULAD和HYPERAKTIV数据集验证检测模型。

Result: 检测模型分类准确率达87.3%，与临床ADHD特征相关；实验显示自适应条件下认知负荷降低、理解能力提升，巫师决策与自动分类器预测一致性达84%。

Conclusion: 贡献了经实证验证的注意力自适应界面UI模式，证明行为注意力检测能有效支持神经差异学习体验。

Abstract: Adaptive learning systems optimize content delivery based on performance metrics but ignore the dynamic attention fluctuations that characterize neurodivergent learners. We present AttentionGuard, a framework that detects engagement-attention states from privacy-preserving behavioral signals and adapts interface elements accordingly. Our approach models four attention states derived from ADHD phenomenology and implements five novel UI adaptation patterns including bi-directional scaffolding that responds to both understimulation and overstimulation. We validate our detection model on the OULAD dataset, achieving 87.3% classification accuracy, and demonstrate correlation with clinical ADHD profiles through cross-validation on the HYPERAKTIV dataset. A Wizard-of-Oz study with 11 adults showing ADHD characteristics found significantly reduced cognitive load in the adaptive condition (NASA-TLX: 47.2 vs 62.8, Cohen's d=1.21, p=0.008) and improved comprehension (78.4% vs 61.2%, p=0.009). Concordance analysis showed 84% agreement between wizard decisions and automated classifier predictions, supporting deployment feasibility. The system is presented as an interactive demo where observers can inspect detected attention states, observe real-time UI adaptations, and compare automated decisions with human-in-the-loop overrides. We contribute empirically validated UI patterns for attention-adaptive interfaces and evidence that behavioral attention detection can meaningfully support neurodivergent learning experiences.

</details>


### [740] [Large Language Models in Peer-Run Community Behavioral Health Services: Understanding Peer Specialists and Service Users' Perspectives on Opportunities, Risks, and Mitigation Strategies](https://arxiv.org/abs/2602.08187)
*Cindy Peng,Megan Chai,Gao Mo,Naveen Raman,Ningjing Tang,Shannon Pagdon,Margaret Swarbrick,Nev Jones,Fei Fang,Hong Shen*

Main category: cs.HC

TL;DR: 研究用漫画板方法探讨将大语言模型推荐系统融入同伴支持的看法，指出其对同伴支持的影响，识别相关风险与机会并给出设计建议。


<details>
  <summary>Details</summary>
Motivation: 大语言模型进入同伴互助组织领域，其规模、对话性和不透明性给情境性、信任和自主性带来新挑战。

Method: 与新泽西协作支持计划合作，采用漫画板这一共同设计方法，与16名同伴专家和10名服务用户开展研讨会。

Result: 大语言模型根据引入、限制和共同使用的方式，会重新配置室内互动动态，影响同伴支持的关系权威。

Conclusion: 提出以生活经验为核心的设计启示，将信任重新定义为共同构建，把大语言模型定位为关系协作者。

Abstract: Peer-run organizations (PROs) provide critical, recovery-based behavioral health support rooted in lived experience. As large language models (LLMs) enter this domain, their scale, conversationality, and opacity introduce new challenges for situatedness, trust, and autonomy. Partnering with Collaborative Support Programs of New Jersey (CSPNJ), a statewide PRO in the Northeastern United States, we used comicboarding, a co-design method, to conduct workshops with 16 peer specialists and 10 service users exploring perceptions of integrating an LLM-based recommendation system into peer support. Findings show that depending on how LLMs are introduced, constrained, and co-used, they can reconfigure in-room dynamics by sustaining, undermining, or amplifying the relational authority that grounds peer support. We identify opportunities, risks, and mitigation strategies across three tensions: bridging scale and locality, protecting trust and relational dynamics, and preserving peer autonomy amid efficiency gains. We contribute design implications that center lived-experience-in-the-loop, reframe trust as co-constructed, and position LLMs not as clinical tools but as relational collaborators in high-stakes, community-led care.

</details>


### [741] [Investigating Writing Professionals' Relationships with Generative AI: How Combined Perceptions of Rivalry and Collaboration Shape Work Practices and Outcomes](https://arxiv.org/abs/2602.08227)
*Rama Adithya,Varanasi,Nov,Oded,Wiesenfeld,Batia Mishan*

Main category: cs.HC

TL;DR: 研究专业作家与GenAI的关系对工作实践和成果的影响，发现竞争与合作取向有不同关联，建议平衡两者。


<details>
  <summary>Details</summary>
Motivation: 探究专业作家与GenAI的复杂关系如何塑造他们的工作实践和成果。

Method: 对403名不同角色的写作专业人员进行横断面调查。

Result: 竞争取向与关系塑造和技能维持相关；合作取向与任务塑造、生产力和满意度相关，但会导致长期技能下降；两者结合可调和差异并增强与结果的关联。

Conclusion: 应采取平衡竞争和合作的方法，以实现工作的长期成功，并给出设计建议。

Abstract: This study investigates how professional writers' complex relationship with GenAI shapes their work practices and outcomes. Through a cross-sectional survey with writing professionals (n=403) in diverse roles, we show that collaboration and rivalry orientation are associated with differences in work practices and outcomes. Rivalry is primarily associated with relational crafting and skill maintenance. Collaboration is primarily associated with task crafting, productivity, and satisfaction, at the cost of long-term skill deterioration. Combination of the orientations (high rivalry and high collaboration) reconciles these differences, while boosting the association with the outcomes. Our findings argue for a balanced approach where high levels of rivalry and collaboration are essential to shape work practices and generate outcomes aimed at the long-term success of the job. We present key design implications on how to increase friction (rivalry) and reduce over-reliance (collaboration) to achieve a more balanced relationship with GenAI.

</details>


### [742] [Intelligent support for Human Oversight: Integrating Reinforcement Learning with Gaze Simulation to Personalize Highlighting](https://arxiv.org/abs/2602.08403)
*Thorsten Klößner,João Belo,Zekun Wu,Jörg Hoffmann,Anna Maria Feit*

Main category: cs.HC

TL;DR: 探讨基于强化学习的界面适配以个性化警报策略，使用用户注视行为模型模拟注意力动态，初步结果显示该方法可能优于静态方法，并讨论了智能监督支持的挑战。


<details>
  <summary>Details</summary>
Motivation: 使人类监督界面能在时间紧迫的条件下有效支持用户的态势感知。

Method: 采用强化学习进行用户界面适配；集成用户注视行为模型以在不进行实际部署的情况下模拟注意力动态。

Result: 在配送无人机监督场景中的初步结果显示，基于强化学习的高亮显示方法可能优于静态的基于规则的方法。

Conclusion: 强调基于强化学习的界面适配方法在个性化警报策略方面的潜力，同时指出了智能监督支持面临的挑战。

Abstract: Interfaces for human oversight must effectively support users' situation awareness under time-critical conditions. We explore reinforcement learning (RL)-based UI adaptation to personalize alerting strategies that balance the benefits of highlighting critical events against the cognitive costs of interruptions. To enable learning without real-world deployment, we integrate models of users' gaze behavior to simulate attentional dynamics during monitoring. Using a delivery-drone oversight scenario, we present initial results suggesting that RL-based highlighting can outperform static, rule-based approaches and discuss challenges of intelligent oversight support.

</details>


### [743] [Agent-Supported Foresight for AI Systemic Risks: AI Agents for Breadth, Experts for Judgment](https://arxiv.org/abs/2602.08565)
*Leon Fröhling,Alessandro Giaconia,Edyta Paulina Bogucka,Daniele Quercia*

Main category: cs.HC

TL;DR: 文章引入可扩展方法模拟计算机智能体评估AI长期系统风险，对比智能体、专家和普通人的结果，提出混合预见工作流程。


<details>
  <summary>Details</summary>
Motivation: 解决AI影响评估中因人类判断局限而难以应对长期系统风险的问题。

Method: 使用未来之轮的战略预见方法模拟计算机智能体，应用于四个不同技术成熟度的AI用例，收集专家和普通人的评估。

Result: 智能体每次运行产生众多系统后果，专家识别的风险较少、系统性较弱但可能性更高，普通人提出更多情感上突出但系统性较弱的担忧。

Conclusion: 提出混合预见工作流程，结合智能体和人类的优势，数据集可公开获取。

Abstract: AI impact assessments often stress near-term risks because human judgment degrades over longer horizons, exemplifying the Collingridge dilemma: foresight is most needed when knowledge is scarcest. To address long-term systemic risks, we introduce a scalable approach that simulates in-silico agents using the strategic foresight method of the Futures Wheel. We applied it to four AI uses spanning Technology Readiness Levels (TRLs): Chatbot Companion (TRL 9, mature), AI Toy (TRL 7, medium), Griefbot (TRL 5, low), and Death App (TRL 2, conceptual). Across 30 agent runs per use, agents produced 86-110 consequences, condensed into 27-47 unique risks. To benchmark the agent outputs against human perspectives, we collected evaluations from 290 domain experts and 7 leaders, and conducted Futures Wheel sessions with 42 experts and 42 laypeople. Agents generated many systemic consequences across runs. Compared with these outputs, experts identified fewer risks, typically less systemic but judged more likely, whereas laypeople surfaced more emotionally salient concerns that were generally less systemic. We propose a hybrid foresight workflow, wherein agents broaden systemic coverage, and humans provide contextual grounding. Our dataset is available at: https://social-dynamics.net/ai-risks/foresight.

</details>


### [744] [Kissan-Dost: Bridging the Last Mile in Smallholder Precision Agriculture with Conversational IoT](https://arxiv.org/abs/2602.08593)
*Muhammad Saad Ali,Daanish U. Khan,Laiba Intizar Ahmad,Umer Irfan,Maryam Mustafa,Naveed Anwar Bhatti,Muhammad Hamad Alizai*

Main category: cs.HC

TL;DR: 介绍了多语言、基于传感器的对话系统Kissan - Dost，测试显示其表现良好，表明合理的最后一公里集成能释放现有农业物联网潜力。


<details>
  <summary>Details</summary>
Motivation: 将农场实时测量数据和天气信息转化为普通语言指导，释放现有农业物联网对小农户的潜在价值。

Method: 将商用土壤和气候传感器与检索增强生成技术结合，通过模块化管道实现数据处理，开展为期90天、两个地点、五名参与者的三阶段试点，并进行99个传感器相关的作物查询控制测试。

Result: 仪表盘参与度低且逐渐下降，聊天机器人几乎每天被使用并指导实际行动；控制测试中查询正确率超90%，端到端延迟亚秒级，翻译输出质量高。

Conclusion: 仔细的最后一公里集成而非新颖电路，能释放现有农业物联网对小农户的潜在价值。

Abstract: We present Kissan-Dost, a multilingual, sensor-grounded conversational system that turns live on-farm measurements and weather into plain-language guidance delivered over WhatsApp text or voice. The system couples commodity soil and climate sensors with retrieval-augmented generation, then enforces grounding, traceability, and proactive alerts through a modular pipeline. In a 90-day, two-site pilot with five participants, we ran three phases (baseline, dashboard only, chatbot only). Dashboard engagement was sporadic and faded, while the chatbot was used nearly daily and informed concrete actions. Controlled tests on 99 sensor-grounded crop queries achieved over 90 percent correctness with subsecond end-to-end latency, alongside high-quality translation outputs. Results show that careful last-mile integration, not novel circuitry, unlocks the latent value of existing Agri-IoT for smallholders.

</details>


### [745] [Technosocial risks of ideal emotion recognition technologies: A defense of the (social) value of emotional expressions](https://arxiv.org/abs/2602.08706)
*Alexandra Pregent*

Main category: cs.HC

TL;DR: 本文挑战理想情感识别技术（ERTs）因提高情感透明度使社会受益的假设，分析其技术社会风险并提出功能优先监管方法。


<details>
  <summary>Details</summary>
Motivation: 质疑理想ERTs基于情感透明度增加社会生活将受益的假设，探讨其潜在风险。

Method: 借鉴情感表达和社会实践的哲学解释，结合情感科学和社会心理学的实证研究。

Result: 理想ERTs在社会权威或评估环境中部署会威胁情感表达空间，导致情感决定论和情感审计，破坏社会凝聚力和个人能动性。

Conclusion: ERTs的准确性不直接证明其部署的合理性，应采用功能优先的监管方法保护社会利益免受过度情感可读性的影响。

Abstract: The prospect of AI systems that I call ideal emotion recognition technologies (ERTs) is often defended on the assumption that social life would benefit from increased affective transparency. This paper challenges that assumption by examining the technosocial risks posed by ideal ERTs, understood as multimodal systems capable of reliably inferring inner affective states in real time. Drawing on philosophical accounts of emotional expression and social practice, as well as empirical work in affective science and social psychology, I argue that the appeal of such systems rests on a misunderstanding of the social functions of emotional expression. Emotional expressions function not only as read-outs of inner states, but also as tools for coordinating action, enabling moral repair, sustaining interpersonal trust, and supporting collective norms. These functions depend on a background of partial opacity and epistemic friction. When deployed in socially authoritative or evaluative contexts, ideal ERTs threaten this expressive space by collapsing epistemic friction, displacing relational meaning with technology-mediated affective profiles, and narrowing the space for aspirational and role-sensitive expressions. The result is a drift towards affective determinism and ambient forms of affective auditing, which undermine both social cohesion and individual agency. I argue that, although it is intuitive to think that increasing accuracy would legitimise such systems, in the case of ERTs accuracy does not straightforwardly justify their deployment, and may, in some contexts, provide a reason for regulatory restraint. I conclude by defending a function-first regulatory approach that treats expressive discretion and intentional emotional expression as constitutive of certain social goods, and that accordingly seeks to protect these goods from excessive affective legibility.

</details>


### [746] [Gesturing Toward Abstraction: Multimodal Convention Formation in Collaborative Physical Tasks](https://arxiv.org/abs/2602.08914)
*Kiyosu Maeda,William P. McCarthy,Ching-Yi Tsai,Jeffrey Mu,Haoliang Wang,Robert D. Hawkins,Judith E. Fan,Parastoo Abtahi*

Main category: cs.HC

TL;DR: 研究人类在重复协作中沟通策略的演变，通过实验发现建立抽象概念并利用跨模态冗余可提高协作效率，还扩展模型，为设计智能代理提供基础。


<details>
  <summary>Details</summary>
Motivation: 探究人类如何在重复协作中通过形成共享程序抽象来发展沟通策略。

Method: 先进行在线单模态研究（n = 98）探测抽象层次，后进行实验室多模态研究（n = 40），让参与者用增强现实协作搭建物理塔。

Result: 参与者通过建立语言和手势抽象以及使用跨模态冗余，在协作中变得更快更准确，扩展了惯例形成的概率模型以捕捉模态偏好的变化。

Conclusion: 研究结果和模型为设计适应物理世界的惯例感知智能代理提供了基础。

Abstract: A quintessential feature of human intelligence is the ability to create ad hoc conventions over time to achieve shared goals efficiently. We investigate how communication strategies evolve through repeated collaboration as people coordinate on shared procedural abstractions. To this end, we conducted an online unimodal study (n = 98) using natural language to probe abstraction hierarchies. In a follow-up lab study (n = 40), we examined how multimodal communication (speech and gestures) changed during physical collaboration. Pairs used augmented reality to isolate their partner's hand and voice; one participant viewed a 3D virtual tower and sent instructions to the other, who built the physical tower. Participants became faster and more accurate by establishing linguistic and gestural abstractions and using cross-modal redundancy to emphasize key changes from previous interactions. Based on these findings, we extend probabilistic models of convention formation to multimodal settings, capturing shifts in modality preferences. Our findings and model provide building blocks for designing convention-aware intelligent agents situated in the physical world.

</details>


### [747] [pixelLOG: Logging of Online Gameplay for Cognitive Research](https://arxiv.org/abs/2602.08941)
*Zeyu Lu,Dennis L. Barbour*

Main category: cs.HC

TL;DR: 提出pixelLOG框架用于基于过程的认知研究，能在多人环境追踪人类行为，弥补传统评估不足。


<details>
  <summary>Details</summary>
Motivation: 传统认知评估难以捕捉自然环境中人类认知复杂性，需新方法。

Method: 开发基于Spigot的Minecraft服务器的pixelLOG框架，采用主动状态轮询和被动事件监测的混合方法，利用Spigot API。

Result: 系统能以可配置频率收集数据，实现会话隔离，输出可集成的JSON数据。

Conclusion: 该框架弥合了实验室评估与生态有效任务间的差距，可用于复杂虚拟环境中认知过程的高分辨率分析。

Abstract: Traditional cognitive assessments often rely on isolated, output-focused measurements that may fail to capture the complexity of human cognition in naturalistic settings. We present pixelLOG, a high-performance data collection framework for Spigot-based Minecraft servers designed specifically for process-based cognitive research. Unlike existing frameworks tailored only for artificial intelligence agents, pixelLOG also enables human behavioral tracking in multi-player/multi-agent environments. Operating at configurable frequencies up to and exceeding 20 updates per second, the system captures comprehensive behavioral data through a hybrid approach of active state polling and passive event monitoring. By leveraging Spigot's extensible API, pixelLOG facilitates robust session isolation and produces structured JSON outputs integrable with standard analytical pipelines. This framework bridges the gap between decontextualized laboratory assessments and richer, more ecologically valid tasks, enabling high-resolution analysis of cognitive processes as they unfold in complex, virtual environments.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [748] [Bank Failures: The Roles of Solvency and Liquidity](https://arxiv.org/abs/2602.07327)
*Sergio Correia,Stephan Luck,Emil Verner*

Main category: econ.GN

TL;DR: 本文回顾银行倒闭原因，指出无论有无挤兑，银行倒闭多与基本面不佳有关，还探讨政策影响与未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 厘清流动性和偿付能力在银行倒闭中的相对重要性，以理解金融危机并设计有效金融稳定政策。

Method: 回顾关于银行倒闭原因的证据。

Result: 银行倒闭几乎都与基本面不佳有关，有挤兑的倒闭银行可能已资不抵债；挤兑很少导致实力强的银行倒闭，通常可通过其他机制解决。

Conclusion: 讨论了研究结果的政策影响，并给出未来研究方向。

Abstract: Bank failures can stem from runs on otherwise solvent banks or from losses that render banks insolvent, regardless of withdrawals. Disentangling the relative importance of liquidity and solvency in explaining bank failures is central to understanding financial crises and designing effective financial stability policies. This paper reviews evidence on the causes of bank failures. Bank failures -- both with and without runs -- are almost always related to poor fundamentals. Low recovery rates in failure suggest that most failed banks that experienced runs were likely fundamentally insolvent. Examiners' postmortem assessments also emphasize the primacy of poor asset quality and solvency problems. Before deposit insurance, runs commonly triggered the failure of insolvent banks. However, runs rarely caused the failure of strong banks, as such runs were typically resolved through other mechanisms, including interbank cooperation, equity injections, public signals of strength, or suspension of convertibility. We discuss the policy implications of these findings and outline directions for future research.

</details>


### [749] [Model Restrictiveness in Functional and Structural Settings](https://arxiv.org/abs/2602.07688)
*Drew Fudenberg,Wayne Yuan Gao,Zhiheng You*

Main category: econ.GN

TL;DR: 本文将模型限制性概念推广到含半/非参数和结构成分的经济模型，介绍计算方法、扩展框架，讨论差异函数选择，关联机器学习，给出三个应用案例。


<details>
  <summary>Details</summary>
Motivation: 将Fudenberg, Gao and Liang (2026)中模型限制性概念推广到更广泛经济模型。

Method: 用高斯过程先验在无限维设置中定义和计算限制性，扩展框架到含内生性等结构模型，结合Rademacher复杂度和GMM准则函数讨论差异函数。

Result: 在偏好风险应用中结果与Fudenberg, Gao and Liang (2026)一致；外生和内生多项选择应用中，嵌套logit和混合logit在标准参数规范下限制性相似，IV外生性条件增加整体限制性并改变模型排名。

Conclusion: 成功推广模型限制性概念，展示其在不同经济模型中的应用及影响。

Abstract: We generalize the notion of model restrictiveness in Fudenberg, Gao and Liang (2026) to a wider range of economic models with semi/non-parametric and structural ingredients. We show how restrictiveness can be defined and computed in infinite-dimensional settings using Gaussian process priors (including with shape restrictions) and other alternativess in Bayesian nonparametrics. We also extend the restrictiveness framework to structural models with endogeneity, instrumental variables, multiple equilibria, and nonparametric nuisance components. We discuss the importance of the user-specific choice of discrepancy functions in the context of Rademacher complexity and GMM criterion function, and relate restrictiveness to the limit of the average-case learning curve in machine learning. We consider applications to: (1) preferences under risk, (2) exogenous multinomial choice, and (3) multinomial choice with endogenous prices: for (1), we obtain results consistent with those in Fudenberg, Gao and Liang (2026); for (2) and (3), our findings show that nested logit and mixed logit exhibit similar restrictiveness under standard parametric specifications, and that IV exogeneity conditions substantially increase overall restrictiveness while altering model rankings.

</details>


### [750] [Droughts and Deluges: Effects of Climate Extremes on the Gender Gap in Labor Supply](https://arxiv.org/abs/2602.07808)
*Jheelum Sarkar*

Main category: econ.GN

TL;DR: 分析1995 - 2019年151个国家数据，发现极端气候与劳动力参与性别差距呈U型或倒U型关系，且因国家特征而异。


<details>
  <summary>Details</summary>
Motivation: 研究极端气候条件如何影响劳动力参与的性别差距。

Method: 使用151个国家1995 - 2019年的面板数据进行研究。

Result: 有偿劳动的性别差距与干旱呈U型关系，与极端湿润呈倒U型关系；不同灾害风险、国家赋权程度和气候冲击净恢复力的国家呈现不同关系。

Conclusion: 极端气候会影响劳动力参与的性别差距，且这种影响受国家特征的调节。

Abstract: Over the past three decades, extreme climate events have caused losses of worth USD 4.5 trillion. Using a panel of 151 countries (1995-2019), I examine how extreme climate conditions shape gender gap in labor force participation. Key results show that the gender gap in paid labor exhibits a U-shaped relationship with droughts and an inverted U-shaped relationship with extreme wet conditions. The drought pattern is primarily driven by gender gap in employment while wetness affects gender gap in participation through unemployment. These relationships vary with country characteristics. Countries with high disaster-displacement risk exhibit declining gender gaps in participation during excess wetness while moderate-risk economies experience expanded gaps during droughts. Furthermore, the drought U-shape is most pronounced in countries with low to moderate empowerment while the nonlinear wet responses is concentrated only in moderately empowered countries. Lastly, both droughts and excess wetness expands gender gap in countries with weak net resilience to climate shocks.

</details>


### [751] [Double Disadvantage: How Gender and Residential Location Shape Hiring Outcomes in Pakistan's IT Sector](https://arxiv.org/abs/2602.08134)
*Sana Khalil*

Main category: econ.GN

TL;DR: 本文通过巴基斯坦卡拉奇的实地实验，研究性别和居住社会经济地位对信息技术行业招聘结果的影响，发现存在性别和地区背景的歧视。


<details>
  <summary>Details</summary>
Motivation: 探究在巴基斯坦信息技术行业雇主未明确表达偏好时，性别和居住社会经济地位的歧视是否存在。

Method: 分析明确针对性别的招聘广告，进行包含2032份申请的简历审核实验，对人力资源官员进行定性访谈。

Result: 多数职业偏好男性，无明确性别偏好时男性面试回调更多；高收入地区候选人回调比低收入地区多45%，且不受通勤距离影响。

Conclusion: 雇主将生产力与性别和社区社会经济地位关联，这种认知强化刻板印象，不利女性和低收入背景候选人。

Abstract: This paper examines how gender and residential socioeconomic status shape hiring outcomes in the information technology sector using a field experiment from the city of Karachi, Pakistan. Employers in Pakistan can openly state preferences regarding gender, residential location, and other characteristics, but the majority in the information technology sector choose not to do so. This creates an opportunity to examine whether discrimination persists when such biases are not explicitly stated. An analysis of explicitly gender-targeted job ads shows that men are preferred over women across most occupations, even in traditionally pink-collar roles. Moreover, results from a resume audit experiment, submitting 2,032 applications to 508 full-time job openings, show that men receive more callbacks for job interviews than women, even in the absence of explicit gender preferences in job ads. The study also indicates a significant premium favoring candidates from high-income areas, who receive 45 percent more callbacks than applicants from low-income neighborhoods. This advantage remains robust even after controlling for commuting distance. Qualitative interviews with human resource officials suggest that employers associate productivity with both gender and neighborhood socioeconomic status. Residential address acts as a proxy for class background and signals education, skills, and perceived "fit" in professional settings. These perceptions may reinforce stereotypes, disadvantaging women and candidates from low-income backgrounds.

</details>


### [752] [On- and off-chain demand and supply drivers of Bitcoin price](https://arxiv.org/abs/2602.08429)
*Pavel Ciaian,d'Artis Kancs,Miroslava Rajcaniova*

Main category: econ.GN

TL;DR: 本文分析比特币链上和链下供需因素对价格的影响，发现链下需求长期影响大，短期供需都有影响，链上仅需求有显著影响，还提及鲸鱼交易对价格的影响。


<details>
  <summary>Details</summary>
Motivation: 多数加密货币实证研究聚焦链上交易，而约四分之三比特币交易发生在链下，因此分析链上和链下供需因素。

Method: 使用2019 - 2024年每日数据，通过自回归分布滞后（ARDL）模型检验两个关于链上和链下供需因素与比特币价格关系的假设。

Result: 链下需求长期对比特币价格有显著影响；短期供需因素都显著影响价格；链上仅需求因素无论长期还是短期都有显著影响；比特币鲸鱼交易长期对价格影响小，但短期和滞后一期较明显。

Conclusion: 证实比特币价格动态具有双重性质，除投机因素外，市场基本面也会影响价格。

Abstract: Around three quarters of Bitcoin transactions take place off-chain. Despite their significance, the vast majority of the empirical literature on cryptocurrencies focuses on on-chain transactions. This paper presents one of the first analysis of both on- and off-chain demand- and supply-side factors. Two hypotheses relating on-chain and off-chain demand and supply drivers to the Bitcoin price are tested in an ARDL model with daily data from 2019 to 2024. Our estimates document the differential contributions of on-chain and off-chain drivers on the Bitcoin price. Off-chain demand pressures have a significant impact on the Bitcoin price in the long-run. In the short-run, both demand and supply drivers significantly affect the Bitcoin price. Regarding transactions on the blockchain, only on-chain demand pressures are statistically significant - both in the long- and short-run. These findings confirm the dual nature of the Bitcoin price dynamics, where also market fundamentals affect the Bitcoin price in addition to speculative drivers. Bitcoin whale trading has less significant impact on price in the long-run, while is more pronounced contemporaneously and one-period lag.

</details>


### [753] [Effectiveness of Rent Controls: Evidence from Spain](https://arxiv.org/abs/2602.08631)
*Luis Perez Garcia*

Main category: econ.GN

TL;DR: 本文对2024年西班牙加泰罗尼亚实施的租金管制政策进行实证评估，发现该政策使租赁协议减少，租金价格增长放缓，但因数据和估计稳健性问题需谨慎看待。


<details>
  <summary>Details</summary>
Motivation: 人们对住房可负担性问题日益关注，引发了租金管制政策和对其效果的讨论，本文旨在评估2024年西班牙加泰罗尼亚实施的租金管制政策。

Method: 使用市政层面的行政数据，实施多项双重差分策略和事件研究设计。

Result: 租赁协议减少，租金价格增长有较弱的下降趋势。

Conclusion: 研究结果显示了租金管制的短期影响，但因数据限制和部分估计的稳健性不足，需谨慎对待。

Abstract: Growing concerns about housing affordability have prompted the adoption of rent control policies and renewed debates over their effectiveness. This paper provides the first empirical evaluation of the 2024 rent control policy implemented in Catalonia under Spain's new national housing law. To identify the causal effect of the policy on the rental market, I use municipality-level administrative data and implement several difference-in-differences strategies and event study designs. The results point to a reduction in tenancy agreements and a less robust decrease in rental price growth. While the findings highlight important short-term consequences of rent control, they also underscore the need for caution due to data limitations and limited robustness in some estimates.

</details>


### [754] [Platform Design, Earnings Transparency and Minimum Wage Policies: Evidence from A Natural Experiment on Lyft](https://arxiv.org/abs/2602.08955)
*Rubing Li,Xiao Liu,Arun Sundararajan*

Main category: econ.GN

TL;DR: 研究Lyft政策和设计变更对利益相关者和平台结果的影响，发现政策增加司机参与度，对乘客需求有积极溢出效应，还探讨了后续模拟和检测方法。


<details>
  <summary>Details</summary>
Motivation: 探究Lyft政策和设计变更对利益相关者和平台结果的影响。

Method: 运用动态交错双重差分和地理边界策略，分析超4700万次骑行数据。

Result: 政策显著增加司机参与度，对乘客需求有积极溢出效应，透明度可能引发司机策略行为。

Conclusion: 平台主导政策可作为监管替代方案，为平台变更管理提供设计见解。

Abstract: We study the impact of a major policy and design change at Lyft that altered both driver earnings and platform transparency, offering insights into how such changes affect stakeholders and platform outcomes. In February 2024, Lyft began a staggered rollout of a new policy that guaranteed drivers a minimum share of rider payments and increased transparency by displaying estimated earnings per ride upfront. This policy was first introduced in major urban markets, creating a natural experiment to evaluate its effects. Using data from over 47 million rides across urban and neighboring suburban markets, we apply dynamic staggered difference-in-differences and geographic border strategies to measure causal effects on driver behavior, rider experience, and platform performance. We find the policy significantly increased driver engagement-particularly among those with lower pre-policy earnings or higher income uncertainty-leading to more hours worked, higher utilization, and greater trip volume. These supply-side changes also generated positive spillovers on rider demand. We disentangle the separate effects of earnings guarantees and transparency and show that while both were beneficial, transparency may have also triggered strategic driver behaviors. In ongoing work, we develop a counterfactual simulation framework linking driver supply and rider intents to ride production, showing how small behavioral shifts could further amplify platform outcomes. We also train a self-supervised model on driver trajectories to detect multihoming, examining whether the observed supply increase reflects net expansion or substitution from other platforms. Together, our findings highlight the potential for platform-led policies to serve as alternatives to regulation and offer design insights for managing platform change.

</details>


### [755] [Analyzing Vaccine Manufacturing Supply Chain Disruptions for Pandemic Preparedness using Discrete-Event Simulation](https://arxiv.org/abs/2602.08988)
*Robin Kelchtermans,Valentijn Stienen,Guido Dietrich,Mauro Bernuzzi,Nico Vandaele*

Main category: econ.GN

TL;DR: 开发离散事件模拟模型分析新冠疫苗供应链中断，以支持快速疫情响应，结合案例分析得出 QA/QC 人员瓶颈、原料中断危害及不同类型中断的恢复情况。


<details>
  <summary>Details</summary>
Motivation: 新冠疫情暴露疫苗供应链脆弱性，需强大制造能力以支持快速疫情响应，实现 CEPI 的 100 天使命。

Method: 开发离散事件模拟模型，整合生产流程、QA/QC 活动和原料采购，通过 mRNA 案例分析多种中断场景。

Result: 发现 QA/QC 人员是主要瓶颈，加倍其产能可使年产量增加 79.1%；原料中断危害大；急性中断可在 6 - 9 周恢复，慢性中断导致长期性能下降。

Conclusion: 该模型可帮助政策制定者和制造商量化中断并评估缓解策略，为疫苗供应链应对疫情提供参考。

Abstract: The COVID-19 pandemic exposed critical vulnerabilities in vaccine supply chains, highlighting the need for robust manufacturing for rapid pandemic response to support CEPI's 100 Days Mission. We develop a discrete-event simulation model to analyze supply chain disruptions and enables policymakers and vaccine manufacturers to quantify disruptions and assess mitigation strategies. Unlike prior studies examining components in isolation, our approach integrates production processes, quality assurance and control (QA/QC) activities, and raw material procurement to capture system-wide dynamics. A detailed mRNA case study analyzes disruption scenarios for a facility targeting 50 million doses: facility shutdowns, workforce reductions, raw material shortages, infrastructure failures, extended procurement lead times, and increased QA/QC capacity. Three main insights emerge. First, QA/QC personnel are the primary bottleneck, with utilization reaching 84.5% under normal conditions while machine utilization remains below 33%. Doubling QA/QC capacity increases annual output by 79.1%, offering greater returns than equipment investments. Second, raw material disruptions are highly detrimental, with extended lead times reducing three-year output by 19.6% and causing stockouts during 51.8% of production time. Third, the model shows differential resilience: acute disruptions (workforce shortages, shutdowns, power outages) allow recovery within 6 to 9 weeks, whereas chronic disruptions (supply delays) cause prolonged performance degradation.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [756] [Optimal Quantum Speedups for Repeatedly Nested Expectation Estimation](https://arxiv.org/abs/2602.08120)
*Yihang Sun,Guanyang Wang,Jose Blanchet*

Main category: quant-ph

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study the estimation of repeatedly nested expectations (RNEs) with a constant horizon (number of nestings) using quantum computing. We propose a quantum algorithm that achieves $\varepsilon$-error with cost $\tilde O(\varepsilon^{-1})$, up to logarithmic factors. Standard lower bounds show this scaling is essentially optimal, yielding an almost quadratic speedup over the best classical algorithm. Our results extend prior quantum speedups for single nested expectations to repeated nesting, and therefore cover a broader range of applications, including optimal stopping. This extension requires a new derandomized variant of the classical randomized Multilevel Monte Carlo (rMLMC) algorithm. Careful de-randomization is key to overcoming a variable-time issue that typically increases quantized versions of classical randomized algorithms.

</details>


### [757] [Recursive QAOA for Interference-Aware Resource Allocation in Wireless Networks](https://arxiv.org/abs/2602.07483)
*Kuan-Cheng Chen,Hiromichi Matsuyama,Wei-hao Huang,Yu Yamashiro*

Main category: quant-ph

TL;DR: 研究基于RQAOA的量子 - 经典方法解决密集无线网络离散无线电资源管理问题，在模拟实例上表现良好，表明递归可缓解QAOA问题。


<details>
  <summary>Details</summary>
Motivation: 密集无线网络离散无线电资源管理问题以QUBO程序表示但难大规模求解。

Method: 采用基于RQAOA的量子 - 经典方法，通过测量单和双量子比特相关器引导变量消除，结合浅QAOA层。

Result: 在模拟实例上，该方法能一致返回可行分配，在演示案例中达到全局最优。

Conclusion: 递归可缓解影响普通QAOA的参数增长和可行性问题，为无线资源分配的近期量子启发式方法提供可行途径。

Abstract: Discrete radio resource management problems in dense wireless networks are naturally cast as quadratic unconstrained binary optimization (QUBO) programs but are difficult to solve at scale. We investigate a quantum-classical approach based on the Recursive Quantum Approximate Optimization Algorithm (RQAOA), which interleaves shallow QAOA layers with variable elimination guided by measured single- and two-qubit correlators. For interference-aware channel assignment, we give a compact QUBO/Ising formulation in which pairwise interference induces same-channel couplings and one-hot constraints are enforced via quadratic penalties (or, optionally, constraint-preserving mixers). Within RQAOA, fixing high-confidence variables or relations reduces the problem dimension, stabilizes training, and concentrates measurement effort on a shrinking instance that is solved exactly once below a cutoff. On simulated instances of modest size, including a four-user, four-channel example, the method consistently returns feasible assignments and, for the demonstrated case, attains the global optimum. These results indicate that recursion can mitigate parameter growth and feasibility issues that affect plain QAOA, and suggest a viable pathway for near-term quantum heuristics in wireless resource allocation.

</details>


### [758] [Empirical Study of Observable Sets in Multiclass Quantum Classification](https://arxiv.org/abs/2602.08485)
*Paul San Sebastian,Mikel Cañizo,Roman Orus*

Main category: quant-ph

TL;DR: 本文研究多类量子机器学习中的两种分类标准，分析不同可观测量集选择对模型性能的影响，为未来模型设计提供见解。


<details>
  <summary>Details</summary>
Motivation: 现有多类分类量子机器学习工作存在局限，如未合理选择用于分类的可观测量，因此开展此研究。

Method: 研究两种主要分类标准，选择泡利字符串集和计算基投影集作为量子机器学习模型中的可观测量，观察不同模型类型的经验行为。

Result: 分析了不同可观测量集选择在贫瘠高原和神经坍缩背景下对量子机器学习模型性能的影响。

Conclusion: 研究结果可为未来多类量子机器学习模型的设计提供指导。

Abstract: Variational quantum algorithms have gained attention as early applications of quantum computers for learning tasks. In the context of supervised learning, most of the works that tackle classification problems with parameterized quantum circuits constrain their scope to the setting of binary classification or perform multiclass classification via ensembles of binary classifiers (strategies such as one versus rest). Those few works that propose native multiclass models, however, do not justify the choice of observables that perform the classification. This work studies two main classification criteria in multiclass quantum machine learning: maximizing the expected value of an observable representing a class or maximizing the fidelity of the encoded quantum state with a reference state representing a class. To compare both approaches, sets of Pauli strings and sets of projectors into the computational basis are chosen as observables in the quantum machine learning models. Observing the empirical behavior of each model type, the effect of different observable set choices on the performance of quantum machine learning models is analyzed in the context of Barren Plateaus and Neural Collapse. The results provide insights that may guide the design of future multiclass quantum machine learning models.

</details>


### [759] [Differentiable Logical Programming for Quantum Circuit Discovery and Optimization](https://arxiv.org/abs/2602.08880)
*Antonin Sulc*

Main category: quant-ph

TL;DR: 提出神经符号框架将量子电路设计转化为可微逻辑编程问题，通过优化开关值满足逻辑公理，在多任务中展示有效性和硬件适应性。


<details>
  <summary>Details</summary>
Motivation: 当前量子电路设计范式存在次优或缺乏通用性的问题，需要更好的方法。

Method: 引入神经符号框架，将潜在量子门和操作表示为可学习的连续开关值，通过梯度下降优化以满足可微逻辑公理，结合连续逻辑和酉演化理论，用有偏初始化解决贫瘠高原问题。

Result: 在发现4 - 比特量子傅里叶变换任务中展示方法，在133 - 比特IBM Torino处理器硬件适配实验中，在局部路由任务中保真度提高59.3个百分点。

Conclusion: 所提出的神经符号框架在量子电路设计中有效且能适应硬件故障，提高设计保真度。

Abstract: Designing high-fidelity quantum circuits remains challenging, and current paradigms often depend on heuristic, fixed-ansatz structures or rule-based compilers that can be suboptimal or lack generality. We introduce a neuro-symbolic framework that reframes quantum circuit design as a differentiable logic programming problem. Our model represents a scaffold of potential quantum gates and parameterized operations as a set of learnable, continuous ``truth values'' or ``switches,'' $s \in [0, 1]^N$. These switches are optimized via standard gradient descent to satisfy a user-defined set of differentiable, logical axioms (e.g., correctness, simplicity, robustness). We provide a theoretical formulation bridging continuous logic (via T-norms) and unitary evolution (via geodesic interpolation), while addressing the barren plateau problem through biased initialization. We illustrate the approach on tasks including discovery of a 4-qubit Quantum Fourier Transform (QFT) from a scaffold of 21 candidate gates. We also report a hardware-aware adaptation experiment on the 133-qubit IBM Torino processor, where the method improved fidelity by 59.3 percentage points in a localized routing task while adapting to hardware failures.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [760] [Adaptive Temporal Dynamics for Personalized Emotion Recognition: A Liquid Neural Network Approach](https://arxiv.org/abs/2602.06997)
*Anindya Bhattacharjee,Nittya Ananda Biswas,K. A. Shahriar,Adib Rahman*

Main category: eess.SP

TL;DR: 本文首次将液体神经网络全面应用于基于脑电图的情绪识别，提出多模态框架，在PhyMER数据集上取得高准确率，还通过多种分析证明方法有效性。


<details>
  <summary>Details</summary>
Motivation: 生理信号的非平稳、嘈杂和依赖主体特性使情绪识别具有挑战性，需新方法。

Method: 提出多模态框架，结合卷积特征提取、带可学习时间常数的液体神经网络和注意力引导融合，用专用子网络处理特征，共享自编码器融合模块学习潜在表示。

Result: 在PhyMER数据集上跨七个情绪类别实验准确率达95.45%，超越先前结果，注意力分析有可解释洞察，t - SNE可视化显示类可分性增强。

Conclusion: 网络能自组织成不同功能组，可独立调整可学习时间常数和记忆优势，有效捕捉复杂情绪特征。

Abstract: Emotion recognition from physiological signals remains challenging due to their non-stationary, noisy, and subject-dependent characteristics. This work presents, to the best of our knowledge, the first comprehensive application of liquid neural networks for EEG-based emotion recognition. The proposed multimodal framework combines convolutional feature extraction, liquid neural networks with learnable time constants, and attention-guided fusion to model temporal EEG dynamics with complementary peripheral physiological and personality features. Dedicated subnetworks are used to process EEG features and auxiliary modalities, and a shared autoencoder-based fusion module is used to learn discriminative latent representations before classification. Subject-dependent experiments conducted on the PhyMER dataset across seven emotional classes achieve an accuracy of 95.45%, surpassing previously reported results. Furthermore, temporal attention analysis provides interpretable insights into emotion-specific temporal relevance, and t-SNE visualizations demonstrate enhanced class separability, highlighting the effectiveness of the proposed approach. Finally, statistical analysis of temporal dynamics confirms that the network self-organizes into distinct functional groups with specialized fast and slow neurons, proving it independently tunes learnable time constants and memory dominance to effectively capture complex emotion artifacts.

</details>


### [761] [Deep Reinforcement Learning for Interference Suppression in RIS-Aided Space-Air-Ground Integrated Networks](https://arxiv.org/abs/2602.06982)
*Pujitha Mamillapalli,Shikhar Verma,Tiago Koketsu Rodrigues,Abhinav Kumar*

Main category: eess.SP

TL;DR: 未来6G网络通过SAGIN实现泛在连接，但跨层干扰问题严重，现有ZF码本方法性能有限，提出基于DDPG算法的RIS辅助HAPS - SAGIN框架，仿真显示该框架优于传统ZF波束赋形，能提升频谱效率。


<details>
  <summary>Details</summary>
Motivation: 6G网络中SAGIN存在跨层干扰问题，现有ZF码本方法在动态信道条件下性能受限，需要新方法提升频谱效率。

Method: 采用基于深度确定性策略梯度（DDPG）算法的可重构智能表面（RIS）辅助的HAPS - SAGIN框架，优化HAPS波束赋形权重以形成空间零陷。

Result: 在不同RIS配置下，DDPG框架始终优于传统ZF波束赋形，在4×4 RIS配置下吞吐量最多提升11.3%。

Conclusion: DDPG框架具有自适应能力，能有效提升动态HAPS - SAGIN中的频谱效率。

Abstract: Future 6G networks envision ubiquitous connectivity through space-air-ground integrated networks (SAGINs), where high-altitude platform stations (HAPSs) and satellites complement terrestrial systems to provide wide-area, low-latency coverage. However, the rapid growth of terrestrial devices intensifies spectrum sharing between terrestrial and non-terrestrial segments, resulting in severe cross-tier interference. In particular, frequency sharing between the HAPS satellite uplink and HAPS ground downlink improves spectrum efficiency but suffers from interference caused by the HAPS antenna back-lobe. Existing approaches relying on zero-forcing (ZF) codebooks have limited performance under highly dynamic channel conditions. To overcome this limitation, we employ a reconfigurable intelligent surface (RIS)-aided HAPS-based SAGIN framework with a deep deterministic policy gradient (DDPG) algorithm. The proposed DDPG framework optimizes the HAPS beamforming weights to form spatial nulls toward interference sources while maintaining robust links to the desired signals. Simulation results demonstrate that the DDPG framework consistently outperforms conventional ZF beamforming among different RIS configurations, achieving up to \(11.3\%\) throughput improvement for a \(4\times4\) RIS configuration, validating its adaptive capability to enhance spectral efficiency in dynamic HAPS-based SAGINs.

</details>


### [762] [Hybrid Deep Learning Framework for CSI-Based Activity Recognition in Bandwidth-Constrained Wi-Fi Sensing](https://arxiv.org/abs/2602.06983)
*Alison M. Fernandes,Hermes I. Del Monego,Bruno S. Chang,Anelise Munaretto,Hélder M. Fontes,Rui Campos*

Main category: eess.SP

TL;DR: 提出混合深度学习框架提升带宽受限Wi-Fi感知环境下基于CSI的人体活动识别鲁棒性，验证其有效性且优于基线。


<details>
  <summary>Details</summary>
Motivation: 提升带宽受限Wi-Fi感知环境下基于CSI的人体活动识别的鲁棒性。

Method: 先进行多普勒轨迹提取放大特征，再用集成Inception网络和BiLSTM网络的混合架构处理，最后用SVM分类。

Result: 在20、40和80 MHz带宽配置下准确率分别为89.27%、94.13%和95.30%，优于独立深度学习基线。

Conclusion: 结合多普勒特征工程与混合学习架构对带宽受限无线感知应用中的可靠HAR有用。

Abstract: This paper presents a novel hybrid deep learning framework designed to enhance the robustness of CSI-based Human Activity Recognition (HAR) within bandwidth-constrained Wi-Fi sensing environments. The core of our proposed methodology is a preliminary Doppler trace extraction stage, implemented to amplify salient motion-related signal features before classification. Subsequently, these enhanced inputs are processed by a hybrid neural architecture, which integrates Inception networks responsible for hierarchical spatial feature extraction and Bidirectional Long Short-Term Memory (BiLSTM) networks that capture temporal dependencies. A Support Vector Machine (SVM) is then utilized as the final classification layer to optimize decision boundaries. The framework's efficacy was systematically validated using a public dataset across 20, 40, and 80 MHz bandwidth configurations. The model yielded accuracies of 89.27% (20 MHz), 94.13% (40 MHz), and 95.30% (80 MHz), respectively. These results confirm a marked superiority over standalone deep learning baselines, especially in the most constrained low-bandwidth scenarios. This study underscores the utility of combining Doppler-based feature engineering with a hybrid learning architecture for reliable HAR in bandwidth-limited wireless sensing applications.

</details>


### [763] [Adjustment of Cluster-Then-Predict Framework for Multiport Scatterer Load Prediction](https://arxiv.org/abs/2602.08129)
*Hanjun Park,Aleksandr D. Kuznetsov,Ville Viikari*

Main category: eess.SP

TL;DR: 文章提出两阶段先聚类后预测框架用于多端口散射体多负载值预测，用RUI度量方法选出最佳配置。


<details>
  <summary>Details</summary>
Motivation: 多端口散射体中相互依赖的负载值预测因高维性和阻抗与散射能力间的复杂关系而具有挑战性，但对通信和测量系统设计很关键。

Method: 提出两阶段先聚类后预测框架，引入Real - world Unified Index (RUI)度量方法。

Result: 先聚类后预测方法应用于梯度提升时均方根误差最多降低46%，且在不同聚类和回归方法中效果一致；基于RUI，确定K - means聚类和k近邻法为最佳配置。

Conclusion: 所提框架和方法能有效进行多端口散射体多负载值预测，并可通过RUI选出最佳配置。

Abstract: Predicting interdependent load values in multiport scatterers is challenging due to high dimensionality and complex dependence between impedance and scattering ability, yet this prediction remains crucial for the design of communication and measurement systems. In this paper, we propose a two-stage cluster-then-predict framework for multiple load values prediction task in multiport scatterers. The proposed cluster-then-predict approach effectively captures the underlying functional relation between S-parameters and corresponding load impedances, achieving up to a 46% reduction in Root Mean Square Error (RMSE) compared to the baseline when applied to gradient boosting (GB). This improvement is consistent across various clustering and regression methods. Furthermore, we introduce the Real-world Unified Index (RUI), a metric for quantitative analysis of trade-offs among multiple metrics with conflicting objectives and different scales, suitable for performance assessment in realistic scenarios. Based on RUI, the combination of K-means clustering and k-nearest neighbors (KNN) is identified as the optimal setup for the analyzed multiport scatterer.

</details>


### [764] [DNS: Data-driven Nonlinear Smoother for Complex Model-free Process](https://arxiv.org/abs/2602.08560)
*Fredrik Cumlin,Anubhab Ghosh,Saikat Chatterjee*

Main category: eess.SP

TL;DR: 提出数据驱动的非线性平滑器(DNS)用于从含噪线性测量序列估计复杂动态过程的隐藏状态序列，无监督学习，模拟实验显示其优于DKS和iDANSE平滑器。


<details>
  <summary>Details</summary>
Motivation: 在复杂动态过程无状态转移模型且对非线性动态一无所知的情况下，估计其隐藏状态序列。

Method: 提出使用循环架构的DNS，可提供给定测量序列下隐藏状态序列的闭式后验，以无监督方式学习。

Result: 模拟实验中，DNS在平滑多个随机动态过程时表现显著优于DKS和iDANSE平滑器。

Conclusion: DNS在估计复杂动态过程隐藏状态序列方面是一种有效的方法。

Abstract: We propose data-driven nonlinear smoother (DNS) to estimate a hidden state sequence of a complex dynamical process from a noisy, linear measurement sequence. The dynamical process is model-free, that is, we do not have any knowledge of the nonlinear dynamics of the complex process. There is no state-transition model (STM) of the process available. The proposed DNS uses a recurrent architecture that helps to provide a closed-form posterior of the hidden state sequence given the measurement sequence. DNS learns in an unsupervised manner, meaning the training dataset consists of only measurement data and no state data. We demonstrate DNS using simulations for smoothing of several stochastic dynamical processes, including a benchmark Lorenz system. Experimental results show that the DNS is significantly better than a deep Kalman smoother (DKS) and an iterative data-driven nonlinear state estimation (iDANSE) smoother.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [765] [Verifying DNN-based Semantic Communication Against Generative Adversarial Noise](https://arxiv.org/abs/2602.08801)
*Thanh Le,Hai Duong,ThanhVu Nguyen,Takeshi Matsumura*

Main category: cs.LO

TL;DR: 提出VSCAN框架为语义通信系统提供数学鲁棒性保证，评估显示其有一定成效并揭示安全性与效率权衡。


<details>
  <summary>Details</summary>
Motivation: 基于DNN的语义通信系统易受对抗攻击，现有防御机制无形式化保障。

Method: 将对抗噪声生成建模为混合整数规划，用逻辑公式编码现实约束，借助DNN验证器实现端到端属性验证。

Result: 在600个验证属性上评估，VSCAN能找到漏洞，44%属性有形式化鲁棒性保证；16维潜在空间比64维有更高验证鲁棒性。

Conclusion: VSCAN能为语义通信系统提供有效鲁棒性保证，存在安全性与效率的基本权衡。

Abstract: Safety-critical applications like autonomous vehicles and industrial IoT are adopting semantic communication (SemCom) systems using deep neural networks to reduce bandwidth and increase transmission speed by transmitting only task-relevant semantic features.
  However, adversarial attacks against these DNN-based SemCom systems can cause catastrophic failures by manipulating transmitted semantic features.
  Existing defense mechanisms rely on empirical approaches provide no formal guarantees against the full spectrum of adversarial perturbations.
  We present VSCAN, a neural network verification framework that provides mathematical robustness guarantees by formulating adversarial noise generation as mixed integer programming and verifying end-to-end properties across multiple interconnected networks (encoder, decoder, and task model).
  Our key insight is that realistic adversarial constraints (power limitations and statistical undetectability) can be encoded as logical formulae to enable efficient verification using state-of-the-art DNN verifiers.
  Our evaluation on 600 verification properties characterizing various attacker's capabilities shows VSCAN matches attack methods in finding vulnerabilities while providing formal robustness guarantees for 44% of properties -- a significant achievement given the complexity of multi-network verification.
  Moreover, we reveal a fundamental security-efficiency tradeoff: compact 16-dimensional latent spaces achieve 50% verified robustness compared to 64-dimensional spaces.

</details>


### [766] [PBLean: Pseudo-Boolean Proof Certificates for Lean 4](https://arxiv.org/abs/2602.08692)
*Stefan Szeider*

Main category: cs.LO

TL;DR: 提出PBLean方法将VeriPB伪布尔证明证书导入Lean 4，利用反射，可处理大量步骤证明，支持多种规则，集成后产生可组合定理，支持验证编码并用于组合问题。


<details>
  <summary>Details</summary>
Motivation: 解决将VeriPB伪布尔证明证书导入Lean 4，减少内存消耗，建立求解器输出与问题语义间信任关系。

Method: 采用反射方法，通过在Lean中完全证明正确性并以编译的本地代码执行的布尔检查器函数。

Result: 方法可扩展到处理数万步证明，支持所有VeriPB内核规则，集成得到可作为更大形式开发中可组合引理的Lean定理。

Conclusion: 该方法可用于各种组合问题，弥合了求解器输出和问题语义之间的信任差距。

Abstract: We present PBLean, a method for importing VeriPB pseudo-Boolean (PB) proof certificates into Lean 4. Key to our approach is reflection: a Boolean checker function whose soundness is fully proved in Lean and executed as compiled native code. Our method scales to proofs with tens of thousands of steps that would exhaust memory under explicit proof-term construction. Our checker supports all VeriPB kernel rules, including cutting-plane derivations and proof-by-contradiction subproofs. In contrast to external verified checkers that produce verdicts, our integration yields Lean theorems that can serve as composable lemmas in larger formal developments. To derive theorems about the original combinatorial problems rather than about PB constraints alone, we support verified encodings. This closes the trust gap between solver output and problem semantics since the constraint translation and its correctness proof are both formalized in Lean. We demonstrate the approach on various combinatorial problems.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [767] [MemPot: Defending Against Memory Extraction Attack with Optimized Honeypots](https://arxiv.org/abs/2602.07517)
*Yuhao Wang,Shengfang Zhai,Guanghao Jin,Yinpeng Dong,Linyi Yang,Jiaheng Zhang*

Main category: cs.CR

TL;DR: 提出MemPot框架防御大语言模型代理内存提取攻击，理论验证效果好，实验表现优于基线且无额外推理延迟。


<details>
  <summary>Details</summary>
Motivation: 基于大语言模型的代理存在内存提取攻击，缺乏有效防御手段。

Method: 提出MemPot框架，通过两阶段优化生成陷阱文档，将检测过程建模为Wald的序贯概率比检验。

Result: 理论上比最优静态检测器采样轮数少，实验上显著优于基线，检测AUROC提升50%，低误报率约束下真阳性率提升80%，无额外在线推理延迟。

Conclusion: MemPot在安全性、无害性和效率方面具有优越性。

Abstract: Large Language Model (LLM)-based agents employ external and internal memory systems to handle complex, goal-oriented tasks, yet this exposes them to severe extraction attacks, and effective defenses remain lacking. In this paper, we propose MemPot, the first theoretically verified defense framework against memory extraction attacks by injecting optimized honeypots into the memory. Through a two-stage optimization process, MemPot generates trap documents that maximize the retrieval probability for attackers while remaining inconspicuous to benign users. We model the detection process as Wald's Sequential Probability Ratio Test (SPRT) and theoretically prove that MemPot achieves a lower average number of sampling rounds compared to optimal static detectors. Empirically, MemPot significantly outperforms state-of-the-art baselines, achieving a 50% improvement in detection AUROC and an 80% increase in True Positive Rate under low False Positive Rate constraints. Furthermore, our experiments confirm that MemPot incurs zero additional online inference latency and preserves the agent's utility on standard tasks, verifying its superiority in safety, harmlessness, and efficiency.

</details>


### [768] [Retrieval Pivot Attacks in Hybrid RAG: Measuring and Mitigating Amplified Leakage from Vector Seeds to Graph Expansion](https://arxiv.org/abs/2602.08668)
*Scott Thornton*

Main category: cs.CR

TL;DR: 研究混合检索增强生成管道的安全风险，提出指标量化，展示攻击，证明边界授权可消除泄漏。


<details>
  <summary>Details</summary>
Motivation: 发现混合检索增强生成管道组合存在独特安全失败模式，可能导致跨租户数据泄漏。

Method: 提出检索枢轴风险及相关指标，展示七种检索枢轴攻击，在合成企业语料和安然邮件语料上测试。

Result: 未防护的混合管道有高枢轴风险，在图扩展边界授权可消除泄漏，开销小。

Conclusion: 问题根源是边界授权，两安全组件组合需在过渡点重新检查授权。

Abstract: Hybrid Retrieval-Augmented Generation (RAG) pipelines combine vector similarity search with knowledge graph expansion for multi-hop reasoning. We show that this composition introduces a distinct security failure mode: a vector-retrieved "seed" chunk can pivot via entity links into sensitive graph neighborhoods, causing cross-tenant data leakage that does not occur in vector-only retrieval. We formalize this risk as Retrieval Pivot Risk (RPR) and introduce companion metrics Leakage@k, Amplification Factor, and Pivot Depth (PD) to quantify leakage magnitude and traversal structure.
  We present seven Retrieval Pivot Attacks that exploit the vector-to-graph boundary and show that adversarial injection is not required: naturally shared entities create cross-tenant pivot paths organically. Across a synthetic multi-tenant enterprise corpus and the Enron email corpus, the undefended hybrid pipeline exhibits high pivot risk (RPR up to 0.95) with multiple unauthorized items returned per query. Leakage consistently appears at PD=2, which we attribute to the bipartite chunk-entity topology and formalize as a proposition.
  We then show that enforcing authorization at a single location, the graph expansion boundary, eliminates measured leakage (RPR near 0) across both corpora, all attack variants, and label forgery rates up to 10 percent, with minimal overhead. Our results indicate the root cause is boundary enforcement, not inherently complex defenses: two individually secure retrieval components can compose into an insecure system unless authorization is re-checked at the transition point.

</details>


### [769] [CyberRAG: An Agentic RAG cyber attack classification and reporting tool](https://arxiv.org/abs/2507.02424)
*Francesco Blefari,Cristian Cosentino,Francesco Aurelio Pironti,Angelo Furfaro,Fabrizio Marozzo*

Main category: cs.CR

TL;DR: 提出CyberRAG框架用于网络攻击实时分类、解释和报告，评估显示有高准确率和解释性，为部分自动化网络防御提供途径。


<details>
  <summary>Details</summary>
Motivation: 大型企业IDS/IPS产生大量警报，传统机器学习检测器有很多误报，标准RAG管道检索无关上下文且无法解释预测。

Method: 提出模块化基于代理的RAG框架CyberRAG，由中央大语言模型代理协调专门分类器、工具适配器和迭代检索推理循环。

Result: 在SQL注入、XSS和SSTI评估中，每类准确率超94%，最终分类准确率94.92%，生成解释有高评分，对抗恶意和未知有效负载有鲁棒性。

Conclusion: 基于代理、面向专家的RAG能结合高检测准确率和可靠释文，为部分自动化网络防御工作流提供灵活路径。

Abstract: Intrusion Detection and Prevention Systems (IDS/IPS) in large enterprises can generate hundreds of thousands of alerts per hour, overwhelming analysts with logs requiring rapidly evolving expertise. Conventional machine-learning detectors reduce alert volume but still yield many false positives, while standard Retrieval-Augmented Generation (RAG) pipelines often retrieve irrelevant context and fail to justify predictions. We present CyberRAG, a modular agent-based RAG framework that delivers real-time classification, explanation, and structured reporting for cyber-attacks. A central LLM agent orchestrates: (i) fine-tuned classifiers specialized by attack family; (ii) tool adapters for enrichment and alerting; and (iii) an iterative retrieval-and-reason loop that queries a domain-specific knowledge base until evidence is relevant and self-consistent. Unlike traditional RAG, CyberRAG adopts an agentic design that enables dynamic control flow and adaptive reasoning. This architecture autonomously refines threat labels and natural-language justifications, reducing false positives and enhancing interpretability. It is also extensible: new attack types can be supported by adding classifiers without retraining the core agent. CyberRAG was evaluated on SQL Injection, XSS, and SSTI, achieving over 94\% accuracy per class and a final classification accuracy of 94.92\% through semantic orchestration. Generated explanations reached 0.94 in BERTScore and 4.9/5 in GPT-4-based expert evaluation, with robustness preserved against adversarial and unseen payloads. These results show that agentic, specialist-oriented RAG can combine high detection accuracy with trustworthy, SOC-ready prose, offering a flexible path toward partially automated cyber-defense workflows.

</details>


### [770] [Patch-to-PoC: A Systematic Study of Agentic LLM Systems for Linux Kernel N-Day Reproduction](https://arxiv.org/abs/2602.07287)
*Juefei Pu,Xingyu Li,Haonan Li,Zhengchuan Liang,Jonathan Cox,Yifan Wu,Kareem Shehada,Arrdya Srivastav,Zhiyun Qian*

Main category: cs.CR

TL;DR: 本文首次大规模研究基于大语言模型的Linux内核漏洞复现，开发K - Repro系统，在100个漏洞数据集上复现超50%案例，并分析了有效性等因素提供指导。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏基于大语言模型自主复现Linux内核漏洞并给出具体PoC的系统研究，且因内核特性该任务有挑战性。

Method: 开发基于大语言模型的K - Repro系统，该系统具备代码浏览、虚拟机管理、交互和调试能力，用内核安全补丁为输入实现端到端漏洞复现。

Result: 在100个真实可利用的Linux内核漏洞数据集上，K - Repro能在合理时间和成本下生成PoC复现超50%案例。

Conclusion: 对有效性、效率、稳定性和影响因素的研究结果为构建更可靠自主安全代理及评估现实N天风险提供行动指导。

Abstract: Autonomous large language model (LLM) based systems have recently shown promising results across a range of cybersecurity tasks. However, there is no systematic study on their effectiveness in autonomously reproducing Linux kernel vulnerabilities with concrete proofs-of-concept (PoCs). Owing to the size, complexity, and low-level nature of the Linux kernel, such tasks are widely regarded as particularly challenging for current LLM-based approaches.
  In this paper, we present the first large-scale study of LLM-based Linux kernel vulnerability reproduction. For this purpose, we develop K-Repro, an LLM-based agentic system equipped with controlled code-browsing, virtual machine management, interaction, and debugging capabilities. Using kernel security patches as input, K-Repro automates end-to-end bug reproduction of N-day vulnerabilities in the Linux kernel. On a dataset of 100 real-world exploitable Linux kernel vulnerabilities collected from KernelCTF, our results show that K-Repro can generate PoCs that reproduce over 50\% of the cases with practical time and monetary cost.
  Beyond aggregate success rates, we perform an extensive study of effectiveness, efficiency, stability, and impact factors to explain when agentic reproduction succeeds, where it fails, and which components drive performance. These findings provide actionable guidance for building more reliable autonomous security agents and for assessing real-world N-day risk from both offensive and defensive perspectives.

</details>


### [771] [IssueGuard: Real-Time Secret Leak Prevention Tool for GitHub Issue Reports](https://arxiv.org/abs/2602.08072)
*Md Nafiu Rahman,Sadif Ahmed,Zahin Wahab,Gias Uddin,Rifat Shahriyar*

Main category: cs.CR

TL;DR: 提出IssueGuard工具，用于实时检测和防止GitHub和GitLab问题报告中的密钥泄漏，效果优于传统正则扫描器。


<details>
  <summary>Details</summary>
Motivation: GitHub和GitLab的问题跟踪系统存在密钥意外泄露风险，且平台无提交前警告机制。

Method: 实现为Chrome扩展，在用户输入时分析文本，结合基于正则的候选提取和微调的CodeBERT模型进行上下文分类。

Result: 在基准数据集上F1分数达到92.70%，优于传统基于正则的扫描器。

Conclusion: IssueGuard能有效检测和防止密钥泄露，代码和演示视频均公开。

Abstract: GitHub and GitLab are widely used collaborative platforms whose issue-tracking systems contain large volumes of unstructured text, including logs, code snippets, and configuration examples. This creates a significant risk of accidental secret exposure, such as API keys and credentials, yet these platforms provide no mechanism to warn users before submission. We present \textsc{IssueGuard}, a tool for real-time detection and prevention of secret leaks in issue reports. Implemented as a Chrome extension, \textsc{IssueGuard} analyzes text as users type and combines regex-based candidate extraction with a fine-tuned CodeBERT model for contextual classification. This approach effectively separates real secrets from false positives and achieves an F1-score of 92.70\% on a benchmark dataset, outperforming traditional regex-based scanners. \textsc{IssueGuard} integrates directly into the web interface and continuously analyzes the issue editor, presenting clear visual warnings to help users avoid submitting sensitive data. The source code is publicly available at \href{https://github.com/nafiurahman00/IssueGuard}{https://github.com/nafiurahman00/IssueGuard}, and a demonstration video is available at \href{https://youtu.be/kvbWA8rr9cU}{https://youtu.be/kvbWA8rr9cU}.

</details>


### [772] [LLMs + Security = Trouble](https://arxiv.org/abs/2602.08422)
*Benjamin Livshits*

Main category: cs.CR

TL;DR: 当前用AI编写安全代码的方法有缺陷，应在代码生成时强制实施安全约束。


<details>
  <summary>Details</summary>
Motivation: 指出当前用概率AI检查器或攻击者保障概率生成代码安全以及神经符号方法存在的问题，为提出新方法提供动机。

Method: 在代码生成过程中强制实施安全约束，如通过受限解码。

Result: 未提及具体结果。

Conclusion: 在代码生成时强制实施安全约束能获得更强的安全保障，对扩散式代码模型尤其有前景。

Abstract: We argue that when it comes to producing secure code with AI, the prevailing "fighting fire with fire" approach -- using probabilistic AI-based checkers or attackers to secure probabilistically generated code -- fails to address the long tail of security bugs. As a result, systems may remain exposed to zero-day vulnerabilities that can be discovered by better-resourced or more persistent adversaries.
  While neurosymbolic approaches that combine LLMs with formal methods are attractive in principle, we argue that they are difficult to reconcile with the "vibe coding" workflow common in LLM-assisted development: unless the end-to-end verification pipeline is fully automated, developers are repeatedly asked to validate specifications, resolve ambiguities, and adjudicate failures, making the human-in-the-loop a likely point of weakness, compromising secure-by-construction guarantees.
  In this paper we argue that stronger security guarantees can be obtained by enforcing security constraints during code generation (e.g., via constrained decoding), rather than relying solely on post-hoc detection and repair. This direction is particularly promising for diffusion-style code models, whose approach provides a natural elegant opportunity for modular, hierarchical security enforcement, allowing us to combine lower-latency generation techniques with generating secure-by-construction code.

</details>


### [773] [DyMA-Fuzz: Dynamic Direct Memory Access Abstraction for Re-hosted Monolithic Firmware Fuzzing](https://arxiv.org/abs/2602.08750)
*Guy Farrelly,Michael Chesser,Seyit Camtepe,Damith C. Ranasinghe*

Main category: cs.CR

TL;DR: 提出DyMA - Fuzz用于重托管环境下固件模糊测试，解决DMA相关问题，测试效果好。


<details>
  <summary>Details</summary>
Motivation: 关键领域智能设备需强大固件测试，现有模糊测试框架忽略DMA接口。

Method: 使用运行时分析技术推断DMA内存访问模式，自动将模糊测试数据注入目标缓冲区。

Result: 在94个固件样本和8个DMA防护的CVE基准测试中，发现现有工具遗漏的漏洞和执行路径，代码覆盖率最高提升122%。

Conclusion: DyMA - Fuzz是自动化固件测试的有效进步，是模糊测试复杂嵌入式系统的可扩展解决方案。

Abstract: The rise of smart devices in critical domains--including automotive, medical, industrial--demands robust firmware testing. Fuzzing firmware in re-hosted environments is a promising method for automated testing at scale, but remains difficult due to the tight coupling of code with a microcontroller's peripherals. Existing fuzzing frameworks primarily address input challenges in providing inputs for Memory-Mapped I/O or interrupts, but largely overlook Direct Memory Access (DMA), a key high-throughput interface used that bypasses the CPU. We introduce DyMA-Fuzz to extend recent advances in stream-based fuzz input injection to DMA-driven interfaces in re-hosted environments. It tackles key challenges--vendor-specific descriptors, heterogeneous DMA designs, and varying descriptor locations--using runtime analysis techniques to infer DMA memory access patterns and automatically inject fuzzing data into target buffers, without manual configuration or datasheets. Evaluated on 94 firmware samples and 8 DMA-guarded CVE benchmarks, DyMA-Fuzz reveals vulnerabilities and execution paths missed by state-of-the-art tools and achieves up to 122% higher code coverage. These results highlight DyMA-Fuzz as a practical and effective advancement in automated firmware testing and a scalable solution for fuzzing complex embedded systems.

</details>


### [774] [Pro-ZD: A Transferable Graph Neural Network Approach for Proactive Zero-Day Threats Mitigation](https://arxiv.org/abs/2602.07073)
*Nardine Basta,Firas Ben Hmida,Houssem Jmal,Muhammad Ikram,Mohamed Ali Kaafar,Andy Walker*

Main category: cs.CR

TL;DR: 论文提出Pro - ZD框架应对企业网络中动态生成策略的风险，用图神经网络模型识别加权最短路径检测网络问题，框架自动调整规则，实验准确率超95%。


<details>
  <summary>Details</summary>
Motivation: 企业网络中动态生成的策略带来风险，尤其是关键资产暴露问题，且网络结构不断演变，需要有效管理风险。

Method: 引入图神经网络模型识别加权最短路径，检测网络配置错误和高风险连接路径；采用Pro - ZD框架主动自动微调防火墙规则和访问策略。

Result: 实验表明Pro - ZD具有鲁棒性和可迁移性，检测高风险连接的平均准确率超95%。

Conclusion: Pro - ZD框架能有效应对企业网络中动态策略带来的风险，检测高风险连接并防止未授权访问。

Abstract: In today's enterprise network landscape, the combination of perimeter and distributed firewall rules governs connectivity. To address challenges arising from increased traffic and diverse network architectures, organizations employ automated tools for firewall rule and access policy generation. Yet, effectively managing risks arising from dynamically generated policies, especially concerning critical asset exposure, remains a major challenge. This challenge is amplified by evolving network structures due to trends like remote users, bring-your-own devices, and cloud integration. This paper introduces a novel graph neural network model for identifying weighted shortest paths. The model aids in detecting network misconfigurations and high-risk connectivity paths that threaten critical assets, potentially exploited in zero-day attacks -- cyber-attacks exploiting undisclosed vulnerabilities. The proposed Pro-ZD framework adopts a proactive approach, automatically fine-tuning firewall rules and access policies to address high-risk connections and prevent unauthorized access. Experimental results highlight the robustness and transferability of Pro-ZD, achieving over 95% average accuracy in detecting high-risk connections. \

</details>


### [775] [Concept-Aware Privacy Mechanisms for Defending Embedding Inversion Attacks](https://arxiv.org/abs/2602.07090)
*Yu-Che Tsai,Hsiang Hsiao,Kuan-Yu Chen,Shou-De Lin*

Main category: cs.CR

TL;DR: 提出SPARSE框架保护文本嵌入隐私，减少隐私泄露并提升下游性能。


<details>
  <summary>Details</summary>
Motivation: 文本嵌入存在隐私风险，现有差分隐私防御有缺陷，会产生过多噪声并降低实用性。

Method: 提出SPARSE框架，结合可微掩码学习识别隐私敏感维度，采用马氏机制应用椭圆噪声。

Result: 在六个数据集、三种嵌入模型和攻击场景中评估，减少隐私泄露，下游性能优于现有DP方法。

Conclusion: SPARSE能有效保护文本嵌入隐私，提升下游性能。

Abstract: Text embeddings enable numerous NLP applications but face severe privacy risks from embedding inversion attacks, which can expose sensitive attributes or reconstruct raw text. Existing differential privacy defenses assume uniform sensitivity across embedding dimensions, leading to excessive noise and degraded utility. We propose SPARSE, a user-centric framework for concept-specific privacy protection in text embeddings. SPARSE combines (1) differentiable mask learning to identify privacy-sensitive dimensions for user-defined concepts, and (2) the Mahalanobis mechanism that applies elliptical noise calibrated by dimension sensitivity. Unlike traditional spherical noise injection, SPARSE selectively perturbs privacy-sensitive dimensions while preserving non-sensitive semantics. Evaluated across six datasets with three embedding models and attack scenarios, SPARSE consistently reduces privacy leakage while achieving superior downstream performance compared to state-of-the-art DP methods.

</details>


### [776] [ShallowJail: Steering Jailbreaks against Large Language Models](https://arxiv.org/abs/2602.07107)
*Shang Liu,Hanyu Pei,Zeyan Liu*

Main category: cs.CR

TL;DR: 提出新型攻击ShallowJail，可利用大语言模型浅层对齐漏洞误导其输出，实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型越狱攻击存在黑盒不隐蔽、白盒计算资源要求高的问题，需要新的攻击方法。

Method: 引入ShallowJail，通过在推理时操纵初始标记来误导大语言模型响应。

Result: 实验表明ShallowJail能大幅降低最先进大语言模型响应的安全性。

Conclusion: ShallowJail是一种有效的攻击方式，可利用LLMs浅层对齐漏洞。

Abstract: Large Language Models(LLMs) have been successful in numerous fields. Alignment has usually been applied to prevent them from harmful purposes. However, aligned LLMs remain vulnerable to jailbreak attacks that deliberately mislead them into producing harmful outputs. Existing jailbreaks are either black-box, using carefully crafted, unstealthy prompts, or white-box, requiring resource-intensive computation. In light of these challenges, we introduce ShallowJail, a novel attack that exploits shallow alignment in LLMs. ShallowJail can misguide LLMs' responses by manipulating the initial tokens during inference. Through extensive experiments, we demonstrate the effectiveness of~\shallow, which substantially degrades the safety of state-of-the-art LLM responses.

</details>


### [777] [BadSNN: Backdoor Attacks on Spiking Neural Networks via Adversarial Spiking Neuron](https://arxiv.org/abs/2602.07200)
*Abdullah Arafat Miah,Kevin Vu,Yu Bi*

Main category: cs.CR

TL;DR: 提出BadSNN，一种针对脉冲神经网络的后门攻击方法，通过利用脉冲神经元超参数变化注入后门行为，在多数据集和架构上表现优越。


<details>
  <summary>Details</summary>
Motivation: 现有研究未充分探索攻击者如何利用SNN的独特特性进行后门攻击。

Method: 提出BadSNN，利用脉冲神经元超参数变化注入后门行为，并提出触发优化过程。

Result: BadSNN在各种数据集和架构上表现出优越的攻击性能，相比现有数据投毒后门攻击更优，且对常见后门缓解技术具有鲁棒性。

Conclusion: BadSNN是一种有效的针对脉冲神经网络的后门攻击方法。

Abstract: Spiking Neural Networks (SNNs) are energy-efficient counterparts of Deep Neural Networks (DNNs) with high biological plausibility, as information is transmitted through temporal spiking patterns. The core element of an SNN is the spiking neuron, which converts input data into spikes following the Leaky Integrate-and-Fire (LIF) neuron model. This model includes several important hyperparameters, such as the membrane potential threshold and membrane time constant. Both the DNNs and SNNs have proven to be exploitable by backdoor attacks, where an adversary can poison the training dataset with malicious triggers and force the model to behave in an attacker-defined manner. Yet, how an adversary can exploit the unique characteristics of SNNs for backdoor attacks remains underexplored. In this paper, we propose \textit{BadSNN}, a novel backdoor attack on spiking neural networks that exploits hyperparameter variations of spiking neurons to inject backdoor behavior into the model. We further propose a trigger optimization process to achieve better attack performance while making trigger patterns less perceptible. \textit{BadSNN} demonstrates superior attack performance on various datasets and architectures, as well as compared with state-of-the-art data poisoning-based backdoor attacks and robustness against common backdoor mitigation techniques. Codes can be found at https://github.com/SiSL-URI/BadSNN.

</details>


### [778] [AgentSys: Secure and Dynamic LLM Agents Through Explicit Hierarchical Memory Management](https://arxiv.org/abs/2602.07398)
*Ruoyao Wen,Hao Li,Chaowei Xiao,Ning Zhang*

Main category: cs.CR

TL;DR: 提出AgentSys框架通过显式内存管理防御大语言模型代理的间接提示注入攻击，在不同数据集上降低攻击成功率且对良性任务有一定提升。


<details>
  <summary>Details</summary>
Motivation: 间接提示注入对大语言模型代理构成威胁，传统代理机制存在持久注入和决策能力下降的漏洞，现有防御方法未从根源避免问题。

Method: 借鉴操作系统进程内存隔离，采用分层组织代理，外部数据和子任务痕迹不进入主代理内存，通过确定性JSON解析传递数据，并添加验证器/清理器进行事件触发检查。

Result: 在AgentDojo和ASB上攻击成功率分别降至0.78%和4.25%，对良性任务有轻微提升，对自适应攻击者和不同基础模型保持鲁棒性。

Conclusion: 显式内存管理可实现安全、动态的大语言模型代理架构。

Abstract: Indirect prompt injection threatens LLM agents by embedding malicious instructions in external content, enabling unauthorized actions and data theft. LLM agents maintain working memory through their context window, which stores interaction history for decision-making. Conventional agents indiscriminately accumulate all tool outputs and reasoning traces in this memory, creating two critical vulnerabilities: (1) injected instructions persist throughout the workflow, granting attackers multiple opportunities to manipulate behavior, and (2) verbose, non-essential content degrades decision-making capabilities. Existing defenses treat bloated memory as given and focus on remaining resilient, rather than reducing unnecessary accumulation to prevent the attack.
  We present AgentSys, a framework that defends against indirect prompt injection through explicit memory management. Inspired by process memory isolation in operating systems, AgentSys organizes agents hierarchically: a main agent spawns worker agents for tool calls, each running in an isolated context and able to spawn nested workers for subtasks. External data and subtask traces never enter the main agent's memory; only schema-validated return values can cross boundaries through deterministic JSON parsing. Ablations show isolation alone cuts attack success to 2.19%, and adding a validator/sanitizer further improves defense with event-triggered checks whose overhead scales with operations rather than context length.
  On AgentDojo and ASB, AgentSys achieves 0.78% and 4.25% attack success while slightly improving benign utility over undefended baselines. It remains robust to adaptive attackers and across multiple foundation models, showing that explicit memory management enables secure, dynamic LLM agent architectures. Our code is available at: https://github.com/ruoyaow/agentsys-memory.

</details>


### [779] [Secure Code Generation via Online Reinforcement Learning with Vulnerability Reward Model](https://arxiv.org/abs/2602.07422)
*Tianyi Wu,Mingzhe Du,Yue Liu,Chengran Yang,Terry Yue Zhuo,Jiaheng Zhang,See-Kiong Ng*

Main category: cs.CR

TL;DR: 现有大语言模型生成安全代码存在问题，本文提出SecCoderX框架，实验效果好并开源相关资源。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在软件开发中普遍使用，但生成不安全代码，现有安全代码对齐方法存在功能-安全悖论。

Method: 提出SecCoderX在线强化学习框架，通过两种方式利用成熟检测资源，将各组件统一到在线RL循环中。

Result: SecCoderX达到了最先进的性能，有效安全率比未对齐模型提高约10%，而先前方法会使有效安全率降低14 - 54%。

Conclusion: SecCoderX能有效实现功能保留的安全代码生成。

Abstract: Large language models (LLMs) are increasingly used in software development, yet their tendency to generate insecure code remains a major barrier to real-world deployment. Existing secure code alignment methods often suffer from a functionality--security paradox, improving security at the cost of substantial utility degradation. We propose SecCoderX, an online reinforcement learning framework for functionality-preserving secure code generation. SecCoderX first bridges vulnerability detection and secure code generation by repurposing mature detection resources in two ways: (i) synthesizing diverse, reality-grounded vulnerability-inducing coding tasks for online RL rollouts, and (ii) training a reasoning-based vulnerability reward model that provides scalable and reliable security supervision. Together, these components are unified in an online RL loop to align code LLMs to generate secure and functional code. Extensive experiments demonstrate that SecCoderX achieves state-of-the-art performance, improving Effective Safety Rate (ESR) by approximately 10% over unaligned models, whereas prior methods often degrade ESR by 14-54%. We release our code, dataset and model checkpoints at https://github.com/AndrewWTY/SecCoderX.

</details>


### [780] [Agent-Fence: Mapping Security Vulnerabilities Across Deep Research Agents](https://arxiv.org/abs/2602.07652)
*Sai Puppala,Ismail Hossain,Md Jahangir Alam,Yoonpyo Lee,Jay Yoo,Tanzim Ahad,Syed Bahauddin Alam,Sajedul Talukder*

Main category: cs.CR

TL;DR: 引入AgentFence进行架构安全评估，评估八类代理原型的安全漏洞率，指出高风险类别和违规特征，强调以操作层面考量代理安全。


<details>
  <summary>Details</summary>
Motivation: 大语言模型作为深度代理部署，安全失败从文字转向轨迹，需架构安全评估。

Method: 定义14种信任边界攻击类别，通过可追溯对话中断检测失败，固定基础模型评估八类代理原型。

Result: 不同代理原型的平均安全中断率有显著差异，高风险类多为操作类，中断以边界违规为主，授权混淆与目标和工具劫持相关。

Conclusion: AgentFence从操作层面重新构建代理安全，关注代理是否在目标和权限范围内运行。

Abstract: Large language models are increasingly deployed as *deep agents* that plan, maintain persistent state, and invoke external tools, shifting safety failures from unsafe text to unsafe *trajectories*. We introduce **AgentFence**, an architecture-centric security evaluation that defines 14 trust-boundary attack classes spanning planning, memory, retrieval, tool use, and delegation, and detects failures via *trace-auditable conversation breaks* (unauthorized or unsafe tool use, wrong-principal actions, state/objective integrity violations, and attack-linked deviations). Holding the base model fixed, we evaluate eight agent archetypes under persistent multi-turn interaction and observe substantial architectural variation in mean security break rate (MSBR), ranging from $0.29 \pm 0.04$ (LangGraph) to $0.51 \pm 0.07$ (AutoGPT). The highest-risk classes are operational: Denial-of-Wallet ($0.62 \pm 0.08$), Authorization Confusion ($0.54 \pm 0.10$), Retrieval Poisoning ($0.47 \pm 0.09$), and Planning Manipulation ($0.44 \pm 0.11$), while prompt-centric classes remain below $0.20$ under standard settings. Breaks are dominated by boundary violations (SIV 31%, WPA 27%, UTI+UTA 24%, ATD 18%), and authorization confusion correlates with objective and tool hijacking ($ρ\approx 0.63$ and $ρ\approx 0.58$). AgentFence reframes agent security around what matters operationally: whether an agent stays within its goal and authority envelope over time.

</details>


### [781] [SoK: DARPA's AI Cyber Challenge (AIxCC): Competition Design, Architectures, and Lessons Learned](https://arxiv.org/abs/2602.07666)
*Cen Zhang,Younggi Park,Fabian Fleischer,Yu-Fu Fu,Jiho Kim,Dongkwan Kim,Youngjoon Kim,Qingxiao Xu,Andrew Chin,Ze Sheng,Hanqing Zhao,Brian J. Lee,Joshua Wang,Michael Pelican,David J. Musliner,Jeff Huang,Jon Silliman,Mikel Mcdaniel,Jefferson Casavant,Isaac Goldthwaite,Nicholas Vidovich,Matthew Lehman,Taesoo Kim*

Main category: cs.CR

TL;DR: 本文首次对DARPA的AIxCC进行系统分析，揭示影响CRS性能的因素、团队取得的技术进展和待研究的局限，最后给出组织竞赛和部署CRS的建议。


<details>
  <summary>Details</summary>
Motivation: DARPA的AIxCC是构建自主网络推理系统的大型竞赛，本文旨在对其进行首次系统分析。

Method: 借助设计文档、源代码、执行轨迹以及与组织者和参赛团队的讨论进行分析。

Result: 揭示影响CRS性能的因素，识别团队取得的技术进展，暴露待研究的局限。

Conclusion: 得出组织未来竞赛的经验和实际部署自主CRS的更广泛见解。

Abstract: DARPA's AI Cyber Challenge (AIxCC, 2023--2025) is the largest competition to date for building fully autonomous cyber reasoning systems (CRSs) that leverage recent advances in AI -- particularly large language models (LLMs) -- to discover and remediate vulnerabilities in real-world open-source software. This paper presents the first systematic analysis of AIxCC. Drawing on design documents, source code, execution traces, and discussions with organizers and competing teams, we examine the competition's structure and key design decisions, characterize the architectural approaches of finalist CRSs, and analyze competition results beyond the final scoreboard. Our analysis reveals the factors that truly drove CRS performance, identifies genuine technical advances achieved by teams, and exposes limitations that remain open for future research. We conclude with lessons for organizing future competitions and broader insights toward deploying autonomous CRSs in practice.

</details>


### [782] [Rethinking Latency Denial-of-Service: Attacking the LLM Serving Framework, Not the Model](https://arxiv.org/abs/2602.07878)
*Tianyi Wang,Huawei Fan,Yuanchao Shu,Peng Cheng,Cong Wang*

Main category: cs.CR

TL;DR: 研究发现算法层面延迟攻击对现代LLM服务系统效果不佳，提出针对调度器状态转换的Fill and Squeeze攻击策略，成本低且能造成显著延迟。


<details>
  <summary>Details</summary>
Motivation: 大语言模型面临延迟攻击威胁，现有算法复杂度攻击对现代LLM服务系统效果不佳，需新攻击策略。

Method: 提出Fill and Squeeze攻击策略，通过耗尽全局KV缓存和迫使系统重复抢占，利用不同方法操纵输出长度和侧信道探测内存状态。

Result: 相比现有攻击，该攻击能使Time to First Token平均慢20 - 280倍，Time Per Output Token平均慢1.5 - 4倍，且攻击成本降低30 - 40%。

Conclusion: 从系统层面提出的Fill and Squeeze攻击策略效果好、成本低。

Abstract: Large Language Models face an emerging and critical threat known as latency attacks. Because LLM inference is inherently expensive, even modest slowdowns can translate into substantial operating costs and severe availability risks. Recently, a growing body of research has focused on algorithmic complexity attacks by crafting inputs to trigger worst-case output lengths. However, we report a counter-intuitive finding that these algorithmic latency attacks are largely ineffective against modern LLM serving systems. We reveal that system-level optimization such as continuous batching provides a logical isolation to mitigate contagious latency impact on co-located users. To this end, in this paper, we shift the focus from the algorithm to the system layer, and introduce a new Fill and Squeeze attack strategy targeting the state transition of the scheduler. "Fill" first exhausts the global KV cache to induce Head-of-Line blocking, while "Squeeze" forces the system into repetitive preemption. By manipulating output lengths using methods from simple plain-text prompts to more complex prompt engineering, and leveraging side-channel probing of memory status, we demonstrate that the attack can be orchestrated in a black-box setting with much less cost. Extensive evaluations indicate by up to 20-280x average slowdown on Time to First Token and 1.5-4x average slowdown on Time Per Output Token compared to existing attacks with 30-40% lower attack cost.

</details>


### [783] [ICBAC: an Intelligent Contract-Based Access Control framework for supply chain management by integrating blockchain and federated learning](https://arxiv.org/abs/2602.08014)
*Sadegh Sohani,Salar Ghazi,Farnaz Kamranfar,Sahar Pilehvar Moakhar,Mohammad Allahbakhsh,Haleh Amintoosi,Kaiwen Zhang*

Main category: cs.CR

TL;DR: 本文提出ICBAC框架，结合区块链与联邦学习解决现代供应链访问控制问题，实验证明其在性能和隐私保护上有效。


<details>
  <summary>Details</summary>
Motivation: 现有供应链访问控制静态、集中，无法适应内部威胁和变化，区块链缺乏行为智能，集中式机器学习侵犯隐私。

Method: 提出ICBAC框架，集成许可区块链和联邦学习，使用多通道架构和智能合约，部署AI代理监测异常，引入博弈论客户端选择机制。

Result: 在Fabric测试平台上实验表明，ICBAC区块链性能与静态框架相当，在零数据共享下可有效进行异常检测。

Conclusion: ICBAC为去中心化供应链提供了实用、可扩展的动态、隐私保护访问控制解决方案。

Abstract: This paper addresses the critical challenge of access control in modern supply chains, which operate across multiple independent and competing organizations. Existing access control is static and centralized, unable to adapt to insider threats or evolving contexts. Blockchain improves decentralization but lacks behavioral intelligence, while centralized machine learning for anomaly detection requires aggregating sensitive data, violating privacy.
  The proposed solution is ICBAC, an intelligent contract-based access control framework. It integrates permissioned blockchain (Hyperledger Fabric) with federated learning (FL). Built on Fabric, ICBAC uses a multi-channel architecture and three smart contracts for asset management, baseline access control, and dynamic revocation. To counter insider misuse, each channel deploys an AI agent that monitors activity and dynamically restricts access for anomalies. Federated learning allows these agents to collaboratively improve detection models without sharing raw data.
  For heterogeneous, competitive environments, ICBAC introduces a game-theoretic client selection mechanism using hedonic coalition formation. This enables supply chains to form stable, strategy-proof FL coalitions via preference-based selection without disclosing sensitive criteria. Extensive experiments on a Fabric testbed with a real-world dataset show ICBAC achieves blockchain performance comparable to static frameworks and provides effective anomaly detection under IID and non-IID data with zero raw-data sharing. ICBAC thus offers a practical, scalable solution for dynamic, privacy-preserving access control in decentralized supply chains.

</details>


### [784] [Beyond Crash: Hijacking Your Autonomous Vehicle for Fun and Profit](https://arxiv.org/abs/2602.07249)
*Qi Sun,Ahmed Abdo,Luis Burbano,Ziyang Li,Yaxing Yao,Alvaro Cardenas,Yinzhi Cao*

Main category: cs.CR

TL;DR: 本文提出针对基于视觉的自动驾驶汽车的长视野路线完整性攻击，设计实现JackZebra框架进行路线劫持，评估显示其成功率高。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶汽车在安全关键环境运行，需了解其在对抗环境中的鲁棒性，此前攻击多为即时安全故障，本文研究新的长视野路线完整性威胁。

Method: 设计JackZebra框架，利用带可重构显示屏的攻击车辆，将路线劫持视为闭环控制问题，将对抗补丁转换为可在线选择的转向原语，针对最坏情况设计对抗补丁。

Result: JackZebra能成功劫持受害车辆偏离原路线并高成功率停在对抗目标点。

Conclusion: 提出的JackZebra框架能有效实现对基于视觉的自动驾驶汽车的路线劫持，揭示了自动驾驶汽车在长视野路线劫持方面的安全风险。

Abstract: Autonomous Vehicles (AVs), especially vision-based AVs, are rapidly being deployed without human operators. As AVs operate in safety-critical environments, understanding their robustness in an adversarial environment is an important research problem. Prior physical adversarial attacks on vision-based autonomous vehicles predominantly target immediate safety failures (e.g., a crash, a traffic-rule violation, or a transient lane departure) by inducing a short-lived perception or control error. This paper shows a qualitatively different risk: a long-horizon route integrity compromise, where an attacker gradually steers a victim AV away from its intended route and into an attacker-chosen destination while the victim continues to drive "normally." This will not pose a danger to the victim vehicle itself, but also to potential passengers sitting inside the vehicle.
  In this paper, we design and implement the first adversarial framework, called JackZebra, that performs route-level hijacking of a vision-based end-to-end driving stack using a physically plausible attacker vehicle with a reconfigurable display mounted on the rear. The central challenge is temporal persistence: adversarial influence must remain effective in changing viewpoints, lighting, weather, traffic, and the victim's continual replanning -- without triggering conspicuous failures. Our key insight is to treat route hijacking as a closed-loop control problem and to convert adversarial patches into steering primitives that can be selected online via an interactive adjustment loop. Our adversarial patches are also carefully designed against worst-case background and sensor variations so that the adversarial impacts on the victim. Our evaluation shows that JackZebra can successfully hijack victim vehicles to deviate from original routes and stop at adversarial destinations with a high success rate.

</details>


### [785] [CyberExplorer: Benchmarking LLM Offensive Security Capabilities in a Real-World Attacking Simulation Environment](https://arxiv.org/abs/2602.08023)
*Nanda Rani,Kimberly Milner,Minghao Shao,Meet Udeshi,Haoran Xi,Venkata Sai Charan Putrevu,Saksham Aggarwal,Sandeep K. Shukla,Prashanth Krishnamurthy,Farshad Khorrami,Muhammad Shafique,Ramesh Karri*

Main category: cs.CR

TL;DR: 现有基于大语言模型的攻击代理评估依赖封闭环境，本文提出CyberExplorer评估套件以解决此问题，可支持开放环境评估并捕捉多种信息。


<details>
  <summary>Details</summary>
Motivation: 现实攻击行动是开放式的，而现有基于大语言模型的攻击代理评估依赖封闭环境，有预定义目标和二元成功标准，需要解决此差距。

Method: 引入CyberExplorer评估套件，包含基于虚拟机的开放环境基准和反应式多代理框架。

Result: CyberExplorer可进行细粒度评估，捕捉交互动态、协调行为、失败模式和漏洞发现信号。

Conclusion: CyberExplorer弥补了基准测试与现实多目标攻击场景之间的差距。

Abstract: Real-world offensive security operations are inherently open-ended: attackers explore unknown attack surfaces, revise hypotheses under uncertainty, and operate without guaranteed success. Existing LLM-based offensive agent evaluations rely on closed-world settings with predefined goals and binary success criteria. To address this gap, we introduce CyberExplorer, an evaluation suite with two core components: (1) an open-environment benchmark built on a virtual machine hosting 40 vulnerable web services derived from real-world CTF challenges, where agents autonomously perform reconnaissance, target selection, and exploitation without prior knowledge of vulnerability locations; and (2) a reactive multi-agent framework supporting dynamic exploration without predefined plans. CyberExplorer enables fine-grained evaluation beyond flag recovery, capturing interaction dynamics, coordination behavior, failure modes, and vulnerability discovery signals-bridging the gap between benchmarks and realistic multi-target attack scenarios.

</details>


### [786] [CausalArmor: Efficient Indirect Prompt Injection Guardrails via Causal Attribution](https://arxiv.org/abs/2602.07918)
*Minbeom Kim,Mihir Parmar,Phillip Wallis,Lesly Miculicich,Kyomin Jung,Krishnamurthy Dj Dvijotham,Long T. Le,Tomas Pfister*

Main category: cs.CR

TL;DR: 文章指出AI智能体易受间接提示注入攻击，现有防御存在过度防御问题，提出CausalArmor选择性防御框架，实验表明其能保证安全并提升可解释性、保留实用性和低延迟。


<details>
  <summary>Details</summary>
Motivation: 解决现有防御方法在应对间接提示注入攻击时存在的过度防御问题，提升AI智能体安全性、实用性和降低延迟。

Method: 从因果消融视角重新审视间接提示注入攻击，提出CausalArmor选择性防御框架，包括计算归因和触发针对性清理，采用追溯性思维链掩码。

Result: 理论分析表明基于归因边界的清理能使选择恶意动作的概率有指数级小的上限，在AgentDojo和DoomArena上的实验表明CausalArmor匹配强防御的安全性，提升可解释性，保留实用性和低延迟。

Conclusion: CausalArmor能有效解决现有防御的过度防御问题，在保证安全的同时提升AI智能体的可解释性、实用性并降低延迟。

Abstract: AI agents equipped with tool-calling capabilities are susceptible to Indirect Prompt Injection (IPI) attacks. In this attack scenario, malicious commands hidden within untrusted content trick the agent into performing unauthorized actions. Existing defenses can reduce attack success but often suffer from the over-defense dilemma: they deploy expensive, always-on sanitization regardless of actual threat, thereby degrading utility and latency even in benign scenarios. We revisit IPI through a causal ablation perspective: a successful injection manifests as a dominance shift where the user request no longer provides decisive support for the agent's privileged action, while a particular untrusted segment, such as a retrieved document or tool output, provides disproportionate attributable influence. Based on this signature, we propose CausalArmor, a selective defense framework that (i) computes lightweight, leave-one-out ablation-based attributions at privileged decision points, and (ii) triggers targeted sanitization only when an untrusted segment dominates the user intent. Additionally, CausalArmor employs retroactive Chain-of-Thought masking to prevent the agent from acting on ``poisoned'' reasoning traces. We present a theoretical analysis showing that sanitization based on attribution margins conditionally yields an exponentially small upper bound on the probability of selecting malicious actions. Experiments on AgentDojo and DoomArena demonstrate that CausalArmor matches the security of aggressive defenses while improving explainability and preserving utility and latency of AI agents.

</details>


### [787] [Evasion of IoT Malware Detection via Dummy Code Injection](https://arxiv.org/abs/2602.08170)
*Sahar Zargarzadeh,Mohammad Islam*

Main category: cs.CR

TL;DR: 本文提出针对基于电源侧信道的恶意软件检测的对抗策略，通过注入虚拟代码干扰电源特征来躲避检测，实验显示攻击成功率达75.2%。


<details>
  <summary>Details</summary>
Motivation: 物联网快速发展带来安全漏洞，电源侧信道分析用于检测恶意软件，但此类检测系统在对抗操纵下的弹性未充分研究。

Method: 在Mirai僵尸网络扫描阶段注入结构化虚拟代码，动态干扰电源特征，使用自定义数据集分析多个先进侧信道分析模型的隐蔽性、执行开销和规避效果的权衡。

Result: 对抗性修改平均攻击成功率达75.2%。

Conclusion: 基于电源的入侵检测框架存在实际漏洞。

Abstract: The Internet of Things (IoT) has revolutionized connectivity by linking billions of devices worldwide. However, this rapid expansion has also introduced severe security vulnerabilities, making IoT devices attractive targets for malware such as the Mirai botnet. Power side-channel analysis has recently emerged as a promising technique for detecting malware activity based on device power consumption patterns. However, the resilience of such detection systems under adversarial manipulation remains underexplored.
  This work presents a novel adversarial strategy against power side-channel-based malware detection. By injecting structured dummy code into the scanning phase of the Mirai botnet, we dynamically perturb power signatures to evade AI/ML-based anomaly detection without disrupting core functionality. Our approach systematically analyzes the trade-offs between stealthiness, execution overhead, and evasion effectiveness across multiple state-of-the-art models for side-channel analysis, using a custom dataset collected from smartphones of diverse manufacturers. Experimental results show that our adversarial modifications achieve an average attack success rate of 75.2\%, revealing practical vulnerabilities in power-based intrusion detection frameworks.

</details>


### [788] [CIC-Trap4Phish: A Unified Multi-Format Dataset for Phishing and Quishing Attachment Detection](https://arxiv.org/abs/2602.09015)
*Fatemeh Nejati,Mahdi Rabbani,Mansur Mirani,Gunjan Piya,Igor Opushnyev,Ali A. Ghorbani,Sajjad Dadkhah*

Main category: cs.CR

TL;DR: 本文针对网络钓鱼邮件攻击，生成多格式数据集CIC - Trap4Phish，为四种文件类型设计特征提取管道并进行特征选择，用轻量级机器学习模型评估，对二维码钓鱼采用两种检测方法，模型检测准确率高。


<details>
  <summary>Details</summary>
Motivation: 网络钓鱼攻击中攻击者常用恶意邮件附件绕过安全措施，且缺少统一全面的数据集用于训练先进模型。

Method: 生成多格式数据集CIC - Trap4Phish；为前四种文件类型设计无执行静态特征管道，用SHAP分析和特征重要性进行特征选择；使用轻量级机器学习模型评估；对二维码钓鱼采用CNN图像检测和轻量级语言模型进行URL词法分析。

Result: 所有模型在不同格式下都表现出较高的检测准确率。

Conclusion: 所提出的方法和数据集能有效应对网络钓鱼攻击，提高检测准确率。

Abstract: Phishing attacks represents one of the primary attack methods which is used by cyber attackers. In many cases, attackers use deceptive emails along with malicious attachments to trick users into giving away sensitive information or installing malware while compromising entire systems. The flexibility of malicious email attachments makes them stand out as a preferred vector for attackers as they can embed harmful content such as malware or malicious URLs inside standard document formats. Although phishing email defenses have improved a lot, attackers continue to abuse attachments, enabling malicious content to bypass security measures. Moreover, another challenge that researches face in training advance models, is lack of an unified and comprehensive dataset that covers the most prevalent data types. To address this gap, we generated CIC-Trap4Phish, a multi-format dataset containing both malicious and benign samples across five categories commonly used in phishing campaigns: Microsoft Word documents, Excel spreadsheets, PDF files, HTML pages, and QR code images. For the first four file types, a set of execution-free static feature pipeline was proposed, designed to capture structural, lexical, and metadata-based indicators without the need to open or execute files. Feature selection was performed using a combination of SHAP analysis and feature importance, yielding compact, discriminative feature subsets for each file type. The selected features were evaluated by using lightweight machine learning models, including Random Forest, XGBoost, and Decision Tree. All models demonstrate high detection accuracy across formats. For QR code-based phishing (quishing), two complementary methods were implemented: image-based detection by employing Convolutional Neural Networks (CNNs) and lexical analysis of decoded URLs using recent lightweight language models.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [789] [AbFlow : End-to-end Paratope-Centric Antibody Design by Interaction Enhanced Flow Matching](https://arxiv.org/abs/2602.07084)
*Wenda Wang,Yang Zhang,Zhewei Wei,Wenbing Huang*

Main category: q-bio.QM

TL;DR: 提出AbFlow框架用于端到端设计全原子抗体，实验表明其能生成更优抗原 - 抗体复合物并提高结合亲和力。


<details>
  <summary>Details</summary>
Motivation: 现有抗体设计方法缺乏端到端全原子抗体结构生成框架，难以充分利用抗原特异性几何信息优化局部结合界面和全局结构。

Method: 引入AbFlow框架，利用最优传输进行端到端全原子抗体设计，采用包含等变表面多通道编码器的扩展速度场网络，利用表面级抗原相互作用数据优化抗体结构。

Result: 在多种抗体设计任务及结合亲和力优化、复合物结构预测等实验中，AbFlow生成的抗原 - 抗体复合物更优，尤其在接触界面，显著提高了生成抗体的结合亲和力。

Conclusion: AbFlow框架有效解决了现有抗体设计方法的局限，能更好地设计全原子抗体。

Abstract: Antigen-antibody binding is a critical process in the immune response. Although recent progress has advanced antibody design, current methods lack a generative framework for end-to-end modeling of full-atom antibody structures and struggle to fully exploit antigen-specific geometric information for optimizing local binding interfaces and global structures. To overcome these limitations, we introduce AbFlow, a flow-matching framework that leverages optimal transport to design full-atom antibodies end-to-end. AbFlow incorporates an extended velocity field network featuring an equivariant Surface Multi-channel Encoder, which uses surface-level antigen interaction data to refine the antibody structure, particularly the CDR-H3 region. Extensive experiments in paratoep-centric antibody design, multi-CDRs and full-atom antibody design, binding affinity optimization, and complex structure prediction show that AbFlow produces superior antigen-antibody complexes, especially at the contact interface, and markedly improves the binding affinity of generated antibodies.

</details>


### [790] [Generative structural elucidation from mass spectra as an iterative optimization problem](https://arxiv.org/abs/2602.07709)
*Mrunali Manjrekar,Runzhong Wang,Samuel Goldman,Jenna C. Fromer,Connor W. Coley*

Main category: q-bio.QM

TL;DR: 介绍了计算工作流程FOAM用于LC - MS/MS结构解析，在数据集上展示性能，确立迭代优化为有效范式。


<details>
  <summary>Details</summary>
Motivation: 现有计算方法对许多质谱特征无法用参考结构或光谱进行可靠注释。

Method: 将LC - MS/MS的结构解析作为迭代优化问题，结合公式约束图遗传算法和光谱模拟探索候选注释。

Result: 在NIST'20和MassSpecGym数据集上展示了FOAM作为独立解析流程和现有逆模型补充的性能。

Conclusion: 迭代优化是一种有效且可扩展的结构解析范式。

Abstract: Liquid chromatography tandem mass spectrometry (LC-MS/MS) is a critical analytical technique for molecular identification across metabolomics, environmental chemistry, and chemical forensics. A variety of computational methods have emerged for structural annotation of spectral features of interest, but many of these features cannot be confidently annotated with reference structures or spectra. Here, we introduce FOAM (Formula-constrained Optimization for Annotating Metabolites), a computational workflow that poses structure elucidation from LC-MS/MS as an iterative optimization problem. FOAM couples a formula-constrained graph genetic algorithm with spectral simulation to explore candidate annotations given an experimental spectrum. We demonstrate FOAM's performance on the NIST'20 and MassSpecGym datasets as both a standalone elucidation pipeline and as a complement to existing inverse models. This work establishes iterative optimization as an effective and extensible paradigm for structural elucidation.

</details>


### [791] [scDFM: Distributional Flow Matching Model for Robust Single-Cell Perturbation Prediction](https://arxiv.org/abs/2602.07103)
*Chenglei Yu,Chuanrui Wang,Bangyan Liao,Tailin Wu*

Main category: q-bio.QM

TL;DR: 提出scDFM框架用于预测细胞转录反应，结合MMD目标和PAD - Transformer架构，在多项基准测试中表现优于先前方法。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法预测细胞对扰动的转录反应时因假设细胞级对应关系，难以捕捉全局效应，且单细胞测量存在噪声和稀疏性问题。

Method: 提出基于条件流匹配的生成框架scDFM，结合MMD目标，引入PAD - Transformer架构。

Result: 在多项遗传和药物扰动基准测试中，scDFM始终优于先前方法，在组合设置中均方误差比最强基线降低19.6%。

Conclusion: 分布级生成建模对稳健的计算机内扰动预测很重要。

Abstract: A central goal in systems biology and drug discovery is to predict the transcriptional response of cells to perturbations. This task is challenging due to the noisy and sparse nature of single-cell measurements, as well as the fact that perturbations often induce population-level shifts rather than changes in individual cells. Existing deep learning methods typically assume cell-level correspondences, limiting their ability to capture such global effects. We present scDFM, a generative framework based on conditional flow matching that models the full distribution of perturbed cells conditioned on control states. By incorporating a maximum mean discrepancy (MMD) objective, our method aligns perturbed and control populations beyond cell-level correspondences. To further improve robustness to sparsity and noise, we introduce the Perturbation-Aware Differential Transformer (PAD-Transformer), a backbone architecture that leverages gene interaction graphs and differential attention to capture context-specific expression changes. Across multiple genetic and drug perturbation benchmarks, scDFM consistently outperforms prior methods, demonstrating strong generalization in both unseen and combinatorial settings. In the combinatorial setting, it reduces mean squared error by 19.6% relative to the strongest baseline. These results highlight the importance of distribution-level generative modeling for robust in silico perturbation prediction. The code is available at https://github.com/AI4Science-WestlakeU/scDFM

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [792] [Sensitivity analysis of the perturbed utility stochastic traffic equilibrium](https://arxiv.org/abs/2409.08347)
*Mogens Fosgerau,Nikolaj Nielsen,Mads Paulsen,Thomas Kjær Rasmussen,Rui Yao*

Main category: econ.EM

TL;DR: 本文为PURC模型和随机交通均衡模型开发了灵敏度分析框架，给出解析表达式，利用模型稀疏性实现相关结果，通过数值和大规模示例验证方法，对交通规划和经济有重要意义。


<details>
  <summary>Details</summary>
Motivation: 为PURC模型和随机交通均衡模型开展灵敏度分析，以确定网络中链路成本变化后链路流量的边际变化。

Method: 推导个体最优PURC流和均衡链路流的雅可比矩阵关于链路成本参数的解析灵敏度表达式，并利用PURC模型的稀疏性来实现结果。

Result: 通过数值示例展示了方法在估计链路成本变化后均衡链路流、识别关键设计参数和量化性能预测不确定性方面的应用；开展了大规模示例验证。

Conclusion: 研究成果对交通规划和经济学中的网络设计、定价策略和政策分析有重要意义，搭建了理论模型和实际应用间的桥梁。

Abstract: This paper develops a sensitivity analysis framework for the perturbed utility route choice (PURC) model and the accompanying stochastic traffic equilibrium model. We derive analytical sensitivity expressions for the Jacobian of the individual optimal PURC flow and equilibrium link flows with respect to link cost parameters under general assumptions. This allows us to determine the marginal change in link flows following a marginal change in link costs across the network. We show how to implement these results while exploiting the sparsity generated by the PURC model. Numerical examples illustrate the use of our method for estimating equilibrium link flows after link cost shifts, identifying critical design parameters, and quantifying uncertainty in performance predictions. Finally, we demonstrate the method in a large-scale example. The findings have implications for network design, pricing strategies, and policy analysis in transportation planning and economics, providing a bridge between theoretical models and real-world applications.

</details>


### [793] [A Quadratic Link between Out-of-Sample $R^2$ and Directional Accuracy](https://arxiv.org/abs/2602.07841)
*Cheng Zhang*

Main category: econ.EM

TL;DR: 本文通过建立联系调和样本外R²和方向准确率，对金融时间序列预测中的度量脱节现象给出新视角，表明二者呈二次关系，适度方向准确率下样本外R²理论值可忽略。


<details>
  <summary>Details</summary>
Motivation: 解决金融时间序列预测中度量脱节现象，从新视角研究样本外R²和方向准确率的关系。

Method: 以随机游走模型为基线，假设符号正确性与实际大小无关。

Result: 发现MSE最优的点预测中，样本外R²和方向准确率呈二次关系；适度方向准确率时样本外R²理论值可忽略；模型非最优或受有限样本噪声影响时，会出现负的样本外R²。

Conclusion: 为金融时间序列预测中的度量脱节现象提供了新的理论解释。

Abstract: This study provides a novel perspective on the metric disconnect phenomenon in financial time series forecasting through an analytical link that reconciles the out-of-sample $R^2$ ($R^2_{OOS}$) and directional accuracy (DA). In particular, using the random walk model as a baseline and assuming that sign correctness is independent of realized magnitude, we show that these two metrics exhibit a quadratic relationship for MSE-optimal point forecasts. For point forecasts with modest DA, the theoretical value of $R^2_{OOS}$ is intrinsically negligible. Thus, a negative empirical $R^2_{OOS}$ is expected if the model is suboptimal or affected by finite sample noise.

</details>


### [794] [Fast Response or Silence: Conversation Persistence in an AI-Agent Social Network](https://arxiv.org/abs/2602.07667)
*Aysajan Eziz*

Main category: econ.EM

TL;DR: 研究Moltbook AI代理社交网络的交互情况，发现讨论多为一层反应，回复快且持久时间短，与Reddit比有差距，持续多步协调可能需特定机制。


<details>
  <summary>Details</summary>
Motivation: 探究自治AI代理能否在社交平台上维持长期的来回协调。

Method: 研究Moltbook社交网络首周快照，引入交互半衰期概念，进行聚合频谱测试，并以Reddit作基线对比。

Result: Moltbook讨论多为首层反应，多数评论无直接回复，回复快且持续时间约数分钟；未检测到可靠的四小时节奏；Reddit线程更深、回复持续久。

Conclusion: 早期Moltbook代理社交交互符合‘快速回复或沉默’模式，持续多步协调可能需显式记忆、线程重提和重新进入机制。

Abstract: Autonomous AI agents are beginning to populate social platforms, but it is still unclear whether they can sustain the back-and-forth needed for extended coordination. We study Moltbook, an AI-agent social network, using a first-week snapshot and introduce interaction half-life: how quickly a comment's chance of receiving a direct reply fades as the comment ages. Across tens of thousands of commented threads, Moltbook discussions are dominated by first-layer reactions rather than extended chains. Most comments never receive a direct reply, reciprocal back-and-forth is rare, and when replies do occur they arrive almost immediately -- typically within seconds -- implying persistence on the order of minutes rather than hours. Moltbook is often described as running on an approximately four-hour ``heartbeat'' check-in schedule; using aggregate spectral tests on the longest contiguous activity window, we do not detect a reliable four-hour rhythm in this snapshot, consistent with jittered or out-of-phase individual schedules. A contemporaneous Reddit baseline analyzed with the same estimators shows substantially deeper threads and much longer reply persistence. Overall, early agent social interaction on Moltbook fits a ``fast response or silence'' regime, suggesting that sustained multi-step coordination will likely require explicit memory, thread resurfacing, and re-entry scaffolds.

</details>


<div id='cs.CC'></div>

# cs.CC [[Back]](#toc)

### [795] [Tensor Hinted Mv Conjectures](https://arxiv.org/abs/2602.07242)
*Zhao Song*

Main category: cs.CC

TL;DR: 将Hinted Mv Conjecture从矩阵情况推广到张量设置


<details>
  <summary>Details</summary>
Motivation: 原Hinted Mv Conjecture为矩阵情况，有推广至张量设置的需求

Method: 未提及

Result: 成功将Hinted Mv Conjecture推广到张量设置

Conclusion: 未提及

Abstract: Brand, Nanongkai, and Saranurak introduced a conjecture known as the Hinted Mv Conjecture. Although it was originally formulated for the matrix case, we generalize it here to the tensor setting.

</details>


### [796] [The Parameterized Complexity of Independent Set and More when Excluding a Half-Graph, Co-Matching, or Matching](https://arxiv.org/abs/2602.07606)
*Jan Dreier,Nikolas Mählmann,Sebastian Siebertz*

Main category: cs.CC

TL;DR: 本文对由匹配指数、余匹配指数和半图指数界定的八类图，完全分类了独立集、团和支配集问题的参数化复杂度，还给出了近似算法。


<details>
  <summary>Details</summary>
Motivation: 对八类图中独立集、团和支配集问题的参数化复杂度进行全面分类。

Method: 从现有文献推导结果，填补空白，构造特定图类，回顾近似算法情况。

Result: 证明独立集在半图和余匹配指数同时有界的图类上是固定参数可解的；构造出半图指数有界但余匹配指数无界的图类，该问题是W[1]-难的；给出半图指数有界图类上独立集问题的近似算法。

Conclusion: 完成了八类图中相关问题的参数化复杂度分类，并对W[1]-难的情况给出近似算法。

Abstract: A theorem of Ding, Oporowski, Oxley, and Vertigan implies that any sufficiently large twin-free graph contains a large matching, a co-matching, or a half-graph as a semi-induced subgraph. The sizes of these unavoidable patterns are measured by the matching index, co-matching index, and half-graph index of a graph. Consequently, graph classes can be organized into the eight classes determined by which of the three indices are bounded.
  We completely classify the parameterized complexity of Independent Set, Clique, and Dominating Set across all eight of these classes. For this purpose, we first derive multiple tractability and hardness results from the existing literature, and then proceed to fill the identified gaps. Among our novel results, we show that Independent Set is fixed-parameter tractable on every graph class where the half-graph and co-matching indices are simultaneously bounded. Conversely, we construct a graph class with bounded half-graph index (but unbounded co-matching index), for which the problem is W[1]-hard.
  For the W[1]-hard cases of our classification, we review the state of approximation algorithms. Here, we contribute an approximation algorithm for Independent Set on classes of bounded half-graph index.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [797] [What is Safety? Corporate Discourse, Power, and the Politics of Generative AI Safety](https://arxiv.org/abs/2602.06981)
*Ankolika De,Gabriel Lima,Yixin Zou*

Main category: cs.CY

TL;DR: 本文研究生成式AI公司如何在公开文件中构建和传达‘安全’概念，指出不加批判接受这些话语有风险，并提出两点贡献。


<details>
  <summary>Details</summary>
Motivation: 研究领先的生成式人工智能公司如何通过公开文件构建和传达“安全”概念，并探讨其影响。

Method: 采用批判性话语分析方法，分析公司与安全相关声明的语料库。

Result: 发现这些话语策略巩固了企业行为者的合法性，将安全规范为实验性和前瞻性实践，推动安全技术的参与议程。

Conclusion: 不加批判地接受这些话语可能会重现企业优先事项，限制治理和设计的替代方案；应将安全视为社会技术话语进行批判审视，提醒人机交互学者避免认可企业框架，强调问责、公平和正义。

Abstract: This work examines how leading generative artificial intelligence companies construct and communicate the concept of "safety" through public-facing documents. Drawing on critical discourse analysis, we analyze a corpus of corporate safety-related statements to explicate how authority, responsibility, and legitimacy are discursively established. These discursive strategies consolidate legitimacy for corporate actors, normalize safety as an experimental and anticipatory practice, and push a perceived participatory agenda toward safe technologies. We argue that uncritical uptake of these discourses risks reproducing corporate priorities and constraining alternative approaches to governance and design. The contribution of this work is twofold: first, to situate safety as a sociotechnical discourse that warrants critical examination; second, to caution human-computer interaction scholars against legitimizing corporate framings, instead foregrounding accountability, equity, and justice. By interrogating safety discourses as artifacts of power, this paper advances a critical agenda for human-computer interaction scholarship on artificial intelligence.

</details>


### [798] [Empowering Affected Individuals to Shape AI Fairness Assessments: Processes, Criteria, and Tools](https://arxiv.org/abs/2602.06984)
*Lin Luo,Satwik Ghanta,Yuri Nakao,Mathieu Chollet,Simone Stumpf*

Main category: cs.CY

TL;DR: 通过对信用评级场景下18名参与者的定性用户研究，揭示人们公平概念的形成过程及自定义公平标准，为AI公平评估设计提供启示。


<details>
  <summary>Details</summary>
Motivation: 现有公平评估常由专家或监管者进行，未充分考虑受影响个体的公平观念，缺乏个体创建公平标准的实证证据。

Method: 开展定性用户研究，让参与者用自己的话表达公平观念，并通过互动原型将其转化为具体量化的公平标准。

Result: 发现人们的公平观念通过与模型特征关联而形成，揭示了个体对结果和程序公平的自定义标准。

Conclusion: 为支持更具包容性和价值敏感性的AI公平评估的流程和工具提供设计启示。

Abstract: AI systems are increasingly used in high-stakes domains such as credit rating, where fairness concerns are critical. Existing fairness assessments are typically conducted by AI experts or regulators using predefined protected attributes and metrics, which often fail to capture the diversity and nuance of fairness notions held by the individuals who are affected by these systems' decisions, such as decision subjects. Recent work has therefore called for involving affected individuals in fairness assessment, yet little empirical evidence exists on how they create their own fairness criteria or what kinds of criteria they produce - knowledge that could not only inform experts' fairness evaluation and mitigation, but also guide the design of AI assessment tools. We address this gap through a qualitative user study with 18 participants in a credit rating scenario. Participants first articulated their fairness notions in their own words. Then, participants turned them into concrete quantified and operationalized fairness criteria, through an interactive prototype we designed. Our findings provide empirical evidence of the process through which people's fairness notions emerge via grounding in model features, and uncover a diverse set of individuals' custom-defined criteria for both outcome and procedural fairness. We provide design implications for processes and tools that support more inclusive and value-sensitive AI fairness assessment.

</details>


### [799] [A New Mode of Teaching Chinese as a Foreign Language from the Perspective of Smart System Studied by Using Rongzhixue](https://arxiv.org/abs/2602.06992)
*Xiaohui Zou,Lijun Ke,Shunpeng Zou*

Main category: cs.CY

TL;DR: 本文介绍融合智慧视角下的对外汉语教学新模型，阐述其特点、效果和意义，并针对Chat GPT挑战做出创新尝试。


<details>
  <summary>Details</summary>
Motivation: 面对Chat GPT对人类学习和创造力的挑战，现有的语言知识教育和对外汉语教学观念落后，需要进行颠覆性创新。

Method: 采用解读前置的蝴蝶模型，运用汉字新理论、语言与言语关系理论等，结合AI赋能教学。

Result: 该教学新模型能改变语言、教育和人机交互的旧观念。

Conclusion: 从融合智慧视角提出跨学科理念和双语思维训练新方法，为学术研究和师生提供有益参考。

Abstract: The purpose of this study is to introduce a new model of teaching Chinese as a foreign language from the perspective of integrating wisdom. Its characteristics are as follows: focusing on the butterfly model of interpretation before translation, highlighting the new method of bilingual thinking training, on the one hand, applying the new theory of Chinese characters, the theory of the relationship between language and speech, and the forward-looking research results of language science; On the other hand, the application of the new model of teaching Chinese as a foreign language, AI empowering teaching and learning, and the forward-looking research results of educational science fully reflect a series of characteristics of the new model of teaching Chinese as a foreign language from the perspective of integrating wisdom. Its beneficial effects are: not only the old view of language and education, especially the old view of teaching Chinese as a foreign language, but also the old view of human-computer interaction. Its significance lies in that a series of great cross-border Rongzhixue such as language, knowledge, education and teaching, as well as new methods and new topics of bilingual thinking training are clearly put forward from the perspective of integrating wisdom. Especially in the face of the challenge of Chat GPT to human learning ability and even creativity, the existing concepts of language knowledge education and teaching are already very backward. The old concepts of Chinese language education, and teaching Chinese as a foreign language are all facing a series of subversive innovation challenges. How to seek changes in adaptation? This study has made a series of innovative attempts, hoping to benefit academic colleagues, teachers and students.

</details>


### [800] [AI for Sustainable Data Protection and Fair Algorithmic Management in Environmental Regulation](https://arxiv.org/abs/2602.07021)
*Sahibpreet Singh,Saksham Sharma*

Main category: cs.CY

TL;DR: 研究探讨AI融入环境监管对数据保护和算法公平的作用，通过评估AI增强的加密技术，发现其提升环境数据处理安全性，指出研究差距并呼吁加强法规，为未来研究奠基。


<details>
  <summary>Details</summary>
Motivation: 解决网络威胁时代环境数据可持续保护需求，传统加密方法处理动态环境数据有局限。

Method: 全面回顾AI增强的同态加密（HE）和多方计算（MPC）的当前进展，并分析其在环境数据监管中的应用。

Result: AI驱动的动态密钥管理、自适应加密方案、优化的HE计算效率，以及AI增强的MPC协议优化和故障缓解，显著提高环境数据处理安全性。

Conclusion: 研究强调需加强网络法律和制定综合法规保护环境数据，未来应平衡安全与隐私，使监管框架适应技术发展。

Abstract: Integration of AI into environmental regulation represents a significant advancement in data management. It offers promising results in both data protection plus algorithmic fairness. This research addresses the critical need for sustainable data protection in the era of ever evolving cyber threats. Traditional encryption methods face limitations in handling the dynamic nature of environmental data. This necessitates the exploration of advanced cryptographic techniques. The objective of this study is to evaluate how AI can enhance these techniques to ensure robust data protection while facilitating fair algorithmic management. The methodology involves a comprehensive review of current advancements in AI-enhanced homomorphic encryption (HE) and multi-party computation (MPC). It is coupled with an analysis of how these techniques can be applied to environmental data regulation. Key findings indicate that AI-driven dynamic key management, adaptive encryption schemes, and optimized computational efficiency in HE, alongside AI-enhanced protocol optimization and fault mitigation in MPC, significantly improve the security of environmental data processing. These findings highlight a crucial research gap in the intersection of AI, cyber laws, and environmental regulation, particularly in terms of addressing algorithmic bias, transparency, and accountability. The implications of this research underscore the need for stricter cyber laws. Also, the development of comprehensive regulations to safeguard sensitive environmental data. Future efforts should focus on refining AI systems to balance security with privacy and ensuring that regulatory frameworks can adapt to technological advancements. This study provides a foundation for future research aimed at achieving secure sustainable environmental data management through AI innovations.

</details>


### [801] [When Excellence Stops Producing Knowledge: A Practitioner's Observation on Research Funding](https://arxiv.org/abs/2602.07039)
*Heimo Müller*

Main category: cs.CY

TL;DR: 作者作为科研资助体系的参与者，指出当前体系存在结构悖论，卓越与知识生产脱钩，分析了三个加速趋势，希望借此促进建设性改变。


<details>
  <summary>Details</summary>
Motivation: 揭示当前科研资助体系存在的问题，虽参与者意识到系统达功能极限，但改革措施未缓解问题。

Method: 主要基于作者自身近四十年在科研资助体系各角色的经验，聚焦竞争基础研究资助和欧盟大型财团项目两个领域，分析三个加速趋势。

Result: 发现卓越与知识生产因评估中的可表征性挂钩而脱钩，呈现出提案撰写专业化、AI辅助申请增多、评审员短缺三个趋势。

Conclusion: 以内部人士身份指出普遍存在却少被明确表述的模式，期望促进更具建设性的发展方向。

Abstract: After almost four decades of participating in competitive research funding -- as applicant, coordinator, evaluator, and panel member -- I have come to see a structural paradox: many participants recognize that the current system is approaching its functional limits, yet most reform measures intensify rather than alleviate the underlying dynamics. This paper documents how excellence has become decoupled from knowledge production through an increasing coupling to representability under evaluation. The discussion focuses on two domains in which this is particularly visible: competitive basic research funding and large EU consortium projects. Three accelerating trends are examined: the professionalization of proposal writing through specialized consultants, the rise of AI-assisted applications, and an evaluator shortage that forces panels to rely on reviewers increasingly distant from the actual research domains. These observations are offered not as external critique but as an insider account, in the hope that naming a widely experienced but rarely articulated pattern may enable more constructive orientation.
  Keywords: Research funding, Excellence, Evaluation, Goodhart's Law, Professionalization, AI-assisted proposals, Peer review crisis

</details>


### [802] [We Should Separate Memorization from Copyright](https://arxiv.org/abs/2602.08632)
*Adi Haviv,Niva Elkin-Koren,Uri Hacohen,Roi Livni,Shay Moran*

Main category: cs.CY

TL;DR: 文章讨论基础模型版权问题，指出传统技术不适用于版权分析，不应将记忆等同于复制，主张采用基于输出层和风险的评估流程。


<details>
  <summary>Details</summary>
Motivation: 基础模型广泛使用带来版权问题，而现有技术文献多依赖传统重建技术，引发学界对记忆和复制等概念混淆的争议。

Method: 区分有意义表明侵权风险的技术信号与合法泛化或高频内容信号，并进行分析。

Result: 明确了记忆和复制不应等同，不应将记忆作为版权侵权的代表。

Conclusion: 倡导一种基于输出层和风险的评估流程，使技术评估符合既定版权标准。

Abstract: The widespread use of foundation models has introduced a new risk factor of copyright issue. This issue is leading to an active, lively and on-going debate amongst the data-science community as well as amongst legal scholars. Where claims and results across both sides are often interpreted in different ways and leading to different implications. Our position is that much of the technical literature relies on traditional reconstruction techniques that are not designed for copyright analysis. As a result, memorization and copying have been conflated across both technical and legal communities and in multiple contexts. We argue that memorization, as commonly studied in data science, should not be equated with copying and should not be used as a proxy for copyright infringement. We distinguish technical signals that meaningfully indicate infringement risk from those that instead reflect lawful generalization or high-frequency content. Based on this analysis, we advocate for an output-level, risk-based evaluation process that aligns technical assessments with established copyright standards and provides a more principled foundation for research, auditing, and policy.

</details>


### [803] [Empirically Understanding the Value of Prediction in Allocation](https://arxiv.org/abs/2602.08786)
*Unai Fischer-Abaigar,Emily Aiken,Christoph Kern,Juan Carlos Perdomo*

Main category: cs.CY

TL;DR: 本文开发了一个实证工具包，用于帮助规划者选择资源分配问题，并量化预测投资与其他政策手段的福利影响。通过两个案例展示决策方法，并公开工具包和部分数据。


<details>
  <summary>Details</summary>
Motivation: 在使用预测分配稀缺资源时，需要决定选择解决哪些问题，现有缺乏如何做决策的工具

Method: 开发了一个实证工具包，应用于德国就业服务和埃塞俄比亚贫困目标确定两个案例研究。

Result: 能够使决策者可靠地得出关于预测在其分配问题中的相对价值的特定情境结论。

Conclusion: 开发的工具包可帮助规划者在资源分配中做出决策，提供工具包和数据利于该领域未来实证研究。

Abstract: Institutions increasingly use prediction to allocate scarce resources. From a design perspective, better predictions compete with other investments, such as expanding capacity or improving treatment quality. Here, the big question is not how to solve a specific allocation problem, but rather which problem to solve. In this work, we develop an empirical toolkit to help planners form principled answers to this question and quantify the bottom-line welfare impact of investments in prediction versus other policy levers such as expanding capacity and improving treatment quality. Applying our framework in two real-world case studies on German employment services and poverty targeting in Ethiopia, we illustrate how decision-makers can reliably derive context-specific conclusions about the relative value of prediction in their allocation problem. We make our software toolkit, rvp, and parts of our data available in order to enable future empirical work in this area.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [804] [MENASpeechBank: A Reference Voice Bank with Persona-Conditioned Multi-Turn Conversations for AudioLLMs](https://arxiv.org/abs/2602.07036)
*Zien Sheikh Ali,Hunzalah Hassan Bhatti,Rabindra Nath Nandi,Shammur Absar Chowdhury,Firoj Alam*

Main category: cs.SD

TL;DR: 论文引入MENASpeechBank，研发可控合成数据流水线生成语音对话数据并公开，还进行了评估和分析。


<details>
  <summary>Details</summary>
Motivation: 音频大语言模型因缺乏多样化、对话式、指令对齐的语音文本数据而发展受限，收集和发布真实多说话者录音成本高且慢。

Method: 引入含约18K高质量话语的MENASpeechBank，构建可控合成数据流水线，包括构建人物档案、定义对话场景分类、匹配人物与场景、生成角色扮演对话、合成用户话语。

Result: 评估了合成和人类录制的对话并提供详细分析。

Conclusion: 将公开MENASpeechBank和生成的对话，促进相关研究。

Abstract: Audio large language models (AudioLLMs) enable instruction-following over speech and general audio, but progress is increasingly limited by the lack of diverse, conversational, instruction-aligned speech-text data. This bottleneck is especially acute for persona-grounded interactions and dialectal coverage, where collecting and releasing real multi-speaker recordings is costly and slow. We introduce MENASpeechBank, a reference speech bank comprising about 18K high-quality utterances from 124 speakers spanning multiple MENA countries, covering English, Modern Standard Arabic (MSA), and regional Arabic varieties. Building on this resource, we develop a controllable synthetic data pipeline that: (i) constructs persona profiles enriched with World Values Survey-inspired attributes, (ii) defines a taxonomy of about 5K conversational scenarios, (iii) matches personas to scenarios via semantic similarity, (iv) generates about 417K role-play conversations with an LLM where the user speaks as the persona and the assistant behaves as a helpful agent, and (v) synthesizes the user turns by conditioning on reference speaker audio to preserve speaker identity and diversity. We evaluate both synthetic and human-recorded conversations and provide detailed analysis. We will release MENASpeechBank and the generated conversations publicly for the community.

</details>


### [805] [CALM: Class-Conditional Sparse Attention Vectors for Large Audio-Language Models](https://arxiv.org/abs/2602.07077)
*Videet Mehta,Liming Wang,Hilde Kuehne,Rogerio Feris,James R. Glass,M. Jehanzeb Mirza*

Main category: cs.SD

TL;DR: 提出用于大音频语言模型的类条件稀疏注意力向量，一种少样本分类方法，在多基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于大音频语言模型的下游任务分类方法为注意力头分配均匀权重，假设各头对所有语义类别贡献相同，不够合理。

Method: 提出Class - Conditional Sparse Attention Vectors，学习注意力头的类依赖重要性权重。

Result: 在多个少样本音频和视听分类基准测试和任务中，音频分类、视听分类和欺骗检测的绝对增益分别达14.52%、1.53%、8.35%，优于基于统一投票的方法。

Conclusion: 所提方法能使单个头专注于不同语义类别，按估计可靠性对集成预测做出贡献，性能表现更好。

Abstract: Large audio-language models (LALMs) exhibit strong zero-shot capabilities in multiple downstream tasks, such as audio question answering (AQA) and abstract reasoning; however, these models still lag behind specialized models for certain discriminative tasks (e.g., audio classification). Recent studies show that sparse subsets of attention heads within an LALM can serve as strong discriminative feature extractors for downstream tasks such as classification via simple voting schemes. However, these methods assign uniform weights to all selected heads, implicitly assuming that each head contributes equally across all semantic categories. In this work, we propose Class-Conditional Sparse Attention Vectors for Large Audio-Language Models, a few-shot classification method that learns class-dependent importance weights over attention heads. This formulation allows individual heads to specialize in distinct semantic categories and to contribute to ensemble predictions proportionally to their estimated reliability. Experiments on multiple few-shot audio and audiovisual classification benchmarks and tasks demonstrate that our method consistently outperforms state-of-the-art uniform voting-based approaches by up to 14.52%, 1.53%, 8.35% absolute gains for audio classification, audio-visual classification, and spoofing detection respectively.

</details>


### [806] [Tutti: Expressive Multi-Singer Synthesis via Structure-Level Timbre Control and Vocal Texture Modeling](https://arxiv.org/abs/2602.08233)
*Jiatao Chen,Xing Tang,Xiaoyue Duan,Yutang Feng,Jinchao Zhang,Jie Zhou*

Main category: cs.SD

TL;DR: 提出Tutti框架用于结构化多歌手生成，实验表明其在多歌手调度和合唱生成声学真实感上表现出色


<details>
  <summary>Details</summary>
Motivation: 现有歌唱合成系统受全局音色控制限制，无法处理单首歌曲中的动态多歌手编排和人声纹理

Method: 引入结构感知歌手提示实现随音乐结构灵活调度歌手，提出基于条件引导VAE的互补纹理学习捕获隐式声学纹理

Result: Tutti在精确多歌手调度方面表现出色，显著提升合唱生成的声学真实感

Conclusion: Tutti为复杂多歌手编排提供了新范式

Abstract: While existing Singing Voice Synthesis systems achieve high-fidelity solo performances, they are constrained by global timbre control, failing to address dynamic multi-singer arrangement and vocal texture within a single song. To address this, we propose Tutti, a unified framework designed for structured multi-singer generation. Specifically, we introduce a Structure-Aware Singer Prompt to enable flexible singer scheduling evolving with musical structure, and propose Complementary Texture Learning via Condition-Guided VAE to capture implicit acoustic textures (e.g., spatial reverberation and spectral fusion) that are complementary to explicit controls. Experiments demonstrate that Tutti excels in precise multi-singer scheduling and significantly enhances the acoustic realism of choral generation, offering a novel paradigm for complex multi-singer arrangement. Audio samples are available at https://annoauth123-ctrl.github.io/Tutii_Demo/.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [807] [SoulX-Singer: Towards High-Quality Zero-Shot Singing Voice Synthesis](https://arxiv.org/abs/2602.07803)
*Jiale Qian,Hao Meng,Tian Zheng,Pengcheng Zhu,Haopeng Lin,Yuhang Dai,Hanke Xie,Wenxiao Cao,Ruixuan Shang,Jun Wu,Hongmei Liu,Hanlin Wen,Jian Zhao,Zhonglin Jiang,Yong Chen,Shunshun Yin,Ming Tao,Jianguo Wei,Lei Xie,Xinsheng Wang*

Main category: eess.AS

TL;DR: 介绍了开源SVS系统SoulX - Singer，它支持可控演唱生成，训练数据多，跨语言合成质量高，还构建了评估基准SoulX - Singer - Eval。


<details>
  <summary>Details</summary>
Motivation: 当前开源SVS系统在工业部署上存在稳健性和零样本泛化方面的障碍，需要开发实用的系统。

Method: 设计SoulX - Singer系统，基于符号乐谱或旋律表示进行可控演唱生成，使用超42000小时的声乐数据训练；构建SoulX - Singer - Eval基准进行零样本性能评估。

Result: SoulX - Singer支持多种语言，在不同音乐条件下实现了跨语言的先进合成质量。

Conclusion: SoulX - Singer是具有实用部署考虑的高质量开源SVS系统，SoulX - Singer - Eval可用于零样本性能的可靠评估。

Abstract: While recent years have witnessed rapid progress in speech synthesis, open-source singing voice synthesis (SVS) systems still face significant barriers to industrial deployment, particularly in terms of robustness and zero-shot generalization. In this report, we introduce SoulX-Singer, a high-quality open-source SVS system designed with practical deployment considerations in mind. SoulX-Singer supports controllable singing generation conditioned on either symbolic musical scores (MIDI) or melodic representations, enabling flexible and expressive control in real-world production workflows. Trained on more than 42,000 hours of vocal data, the system supports Mandarin Chinese, English, and Cantonese and consistently achieves state-of-the-art synthesis quality across languages under diverse musical conditions. Furthermore, to enable reliable evaluation of zero-shot SVS performance in practical scenarios, we construct SoulX-Singer-Eval, a dedicated benchmark with strict training-test disentanglement, facilitating systematic assessment in zero-shot settings.

</details>


<div id='physics.pop-ph'></div>

# physics.pop-ph [[Back]](#toc)

### [808] [Roadmap to Quantum Aesthetics](https://arxiv.org/abs/2602.08363)
*Ivan C. H. Liu,Hsiao-Yuan Chen*

Main category: physics.pop-ph

TL;DR: 本文提出量子美学路线图，用两种方法研究量子概念如何通过艺术媒介成为审美现象，为艺术实践和教学开辟新方向。


<details>
  <summary>Details</summary>
Motivation: 量子力学对直接感官体验而言较难触及，探索量子概念如何通过艺术媒介成为审美现象。

Method: 提出两种互补正交的方法：一是自上而下利用基于文本提示的生成式AI探测量子美学；二是自下而上从量子力学结构直接导出美学形式。

Result: 通过两种方法揭示量子想象在当代视觉文化中的传播，以及从量子数据获得美学形式。

Conclusion: 将量子美学定位为新兴艺术研究领域，为艺术、数据、人工智能和量子科学交叉领域的实践和教学开辟新方向。

Abstract: Quantum mechanics occupies a central position in contemporary science while remaining largely inaccessible to direct sensory experience. This paper proposes a roadmap to quantum aesthetics that examines how quantum concepts become aesthetic phenomena through artistic mediation rather than direct representation. Two complementary and orthogonal approaches are articulated. The first, a pioneering top-down approach, employs text-prompt-based generative AI to probe quantum aesthetics as a collective cultural construct embedded in large-scale training data. By systematically modulating the linguistic weight of the term "quantum," generative models are used as experimental environments to reveal how quantum imaginaries circulate within contemporary visual culture. The second, a bottom-up approach, derives aesthetic form directly from quantum-mechanical structures through the visualization of quantum-generated data, exemplified here by hydrogen atomic orbitals calculated from the Schrödinger equation. These approaches are framed not as competing methods but as intersecting paths within a navigable field of artistic research. They position quantum aesthetics as an emergent field of artistic research shaped by cultural imagination, computational mediation, and physical law, opening new directions for artistic practice and pedagogy at the intersection of art, data, artificial intelligence and quantum science.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [809] [A quantum-inspired multi-level tensor-train monolithic space-time method for nonlinear PDEs](https://arxiv.org/abs/2602.07945)
*N. R. Rapaka,R. Peddinti,E. Tiunov,N. J. Faraj,A. N. Alkhooori,L. Aolita,Y. Addad,M. K. Riahi*

Main category: math.NA

TL;DR: 提出多级张量列车（TT）框架求解非线性偏微分方程，引入粗到精多级策略克服单级求解局限，数值实验显示在多方面有优势。


<details>
  <summary>Details</summary>
Motivation: 现有时空TT求解器与经典时间步方法对比少、误差收敛分析有限、TT舍入对精度影响评估不足，且在不同PDE和参数范围性能展示有限，单级牛顿迭代在强非线性等情况可能收敛失败。

Method: 引入完全嵌入TT格式的粗到精多级策略，每级细化时空分辨率，通过低秩延拓算子传递TT解，用自适应秩DMRG算法求解以TT表示的残差、雅可比矩阵和传递算子。

Result: 数值实验表明对于多种非线性PDE，多级TT方法在单级时空牛顿迭代失败处能收敛，在平流主导场景中，多级TT能以更低计算成本实现高精度。

Conclusion: 多级TT框架在求解非线性偏微分方程上比单级方法更具优势，尤其在高保真数值模拟场景。

Abstract: We propose a multilevel tensor-train (TT) framework for solving nonlinear partial differential equations (PDEs) in a global space-time formulation. While space-time TT solvers have demonstrated significant potential for compressed high-dimensional simulations, the literature contains few systematic comparisons with classical time-stepping methods, limited error convergence analyses, and little quantitative assessment of the impact of TT rounding on numerical accuracy. Likewise, existing studies fail to demonstrate performance across a diverse set of PDEs and parameter ranges. In practice, monolithic Newton iterations may stagnate or fail to converge in strongly nonlinear, stiff, or advection-dominated regimes, where poor initial guesses and severely ill-conditioned space-time Jacobians hinder robust convergence. We overcome this limitation by introducing a coarse-to-fine multilevel strategy fully embedded within the TT format. Each level refines both spatial and temporal resolutions while transferring the TT solution through low-rank prolongation operators, providing robust initializations for successive Newton solves. Residuals, Jacobians, and transfer operators are represented directly in TT and solved with the adaptive-rank DMRG algorithm. Numerical experiments for a selection of nonlinear PDEs including Fisher-KPP, viscous Burgers, sine-Gordon, and KdV cover diffusive, convective, and dispersive dynamics, demonstrating that the multilevel TT approach consistently converges where single-level space-time Newton iterations fail. In dynamic, advection-dominated (nonlinear) scenarios, multilevel TT surpasses single-level TT, achieving high accuracy with significantly reduced computational cost, specifically when high-fidelity numerical simulation is required.

</details>


### [810] [Do physics-informed neural networks (PINNs) need to be deep? Shallow PINNs using the Levenberg-Marquardt algorithm](https://arxiv.org/abs/2602.08515)
*Muhammad Luthfi Shahab,Imam Mukhlash,Hadi Susanto*

Main category: math.NA

TL;DR: 研究用浅物理信息神经网络（PINNs）解非线性偏微分方程正逆问题，用Levenberg - Marquardt（LM）算法优化参数，测试表明LM性能优于BFGS。


<details>
  <summary>Details</summary>
Motivation: 探索浅物理信息神经网络用于求解非线性偏微分方程正逆问题的有效方法。

Method: 将PINNs重写为非线性系统，用LM算法优化网络参数，推导网络关于输入变量的导数解析表达式以计算LM所需雅可比矩阵。

Result: 在多个基准问题测试中，LM在收敛速度、准确性和最终损失值上显著优于BFGS，即使使用只有两个隐藏层的浅网络架构。

Conclusion: 对于广泛的偏微分方程，浅PINNs结合高效二阶优化方法能为正逆问题提供准确且计算高效的解。

Abstract: This work investigates the use of shallow physics-informed neural networks (PINNs) for solving forward and inverse problems of nonlinear partial differential equations (PDEs). By reformulating PINNs as nonlinear systems, the Levenberg-Marquardt (LM) algorithm is employed to efficiently optimize the network parameters. Analytical expressions for the neural network derivatives with respect to the input variables are derived, enabling accurate and efficient computation of the Jacobian matrix required by LM. The proposed approach is tested on several benchmark problems, including the Burgers, Schrödinger, Allen-Cahn, and three-dimensional Bratu equations. Numerical results demonstrate that LM significantly outperforms BFGS in terms of convergence speed, accuracy, and final loss values, even when using shallow network architectures with only two hidden layers. These findings indicate that, for a wide class of PDEs, shallow PINNs combined with efficient second-order optimization methods can provide accurate and computationally efficient solutions for both forward and inverse problems.

</details>


### [811] [Learned Finite Element-based Regularization of the Inverse Problem in Electrocardiographic Imaging](https://arxiv.org/abs/2602.07466)
*Manuel Haas,Thomas Grandits,Thomas Pinetz,Thomas Beiert,Simone Pezzuto,Alexander Effland*

Main category: math.NA

TL;DR: 提出时空正则化框架用于心电图成像，实验显示比手工方法有更好的去噪和重建效果。


<details>
  <summary>Details</summary>
Motivation: 心电图成像逆问题病态，经典方法未充分利用心脏动力学的时间结构，需改进。

Method: 引入时空正则化框架，结合空间正则化和学习到的时间专家场先验，推导有限元离散化，证明Mosco收敛性并开发优化算法。

Result: 在合成心外膜数据的数值实验中，相比手工时空方法，去噪和逆重建效果更好，结果对噪声鲁棒且符合生理。

Conclusion: 所提出的时空正则化框架在心电图成像中有效。

Abstract: Electrocardiographic imaging (ECGI) seeks to reconstruct cardiac electrical activity from body-surface potentials noninvasively. However, the associated inverse problem is severely ill-posed and requires robust regularization. While classical approaches primarily employ spatial smoothing, the temporal structure of cardiac dynamics remains underexploited despite its physiological relevance. We introduce a space-time regularization framework that couples spatial regularization with a learned temporal Fields-of-Experts (FoE) prior to capture complex spatiotemporal activation patterns. We derive a finite element discretization on unstructured cardiac surface meshes, prove Mosco-convergence, and develop a scalable optimization algorithm capable of handling the FoE term. Numerical experiments on synthetic epicardial data demonstrate improved denoising and inverse reconstructions compared to handcrafted spatiotemporal methods, yielding solutions that are both robust to noise and physiologically plausible.

</details>


<div id='math.AT'></div>

# math.AT [[Back]](#toc)

### [812] [Universal Coefficients and Mayer-Vietoris Sequence for Groupoid Homology](https://arxiv.org/abs/2602.08998)
*Luciano Melodia*

Main category: math.AT

TL;DR: 本文通过神经的紧支Moore复形研究充裕群胚的同调，建立相关理论，推导多种正合序列并给出阻碍条件。


<details>
  <summary>Details</summary>
Motivation: 研究充裕群胚的同调理论，建立相关工具和性质以求解相关问题。

Method: 利用紧支Moore复形定义同调，通过连续étale同态的函子性等性质推导，还运用Kakutani等价不变性等。

Result: 建立充裕群胚同调理论，证明Matui型长正合序列，离散系数下的通用系数短正合序列，找出非离散系数的阻碍，构造Mayer - Vietoris长正合序列。

Conclusion: 本文所建立的同调理论及相关正合序列为充裕群胚的同调计算和研究提供有效工具。

Abstract: We study homology of ample groupoids via the compactly supported Moore complex of the nerve. Let $A$ be a topological abelian group. For $n\ge 0$ set $C_n(\mathcal G;A) := C_c(\mathcal G_n,A)$ and define $\partial_n^A=\sum_{i=0}^n(-1)^i(d_i)_*$. This defines $H_n(\mathcal G;A)$. The theory is functorial for continuous étale homomorphisms. It is compatible with standard reductions, including restriction to saturated clopen subsets. In the ample setting it is invariant under Kakutani equivalence. We reprove Matui type long exact sequences and identify the comparison maps at chain level. For discrete $A$ we prove a natural universal coefficient short exact sequence $$0\to H_n(\mathcal G)\otimes_{\mathbb Z}A\xrightarrow{\ ι_n^{\mathcal G}\ }H_n(\mathcal G;A)\xrightarrow{\ κ_n^{\mathcal G}\ }\operatorname{Tor}_1^{\mathbb Z}\bigl(H_{n-1}(\mathcal G),A\bigr)\to 0.$$ The key input is the chain level isomorphism $C_c(\mathcal G_n,\mathbb Z)\otimes_{\mathbb Z}A\cong C_c(\mathcal G_n,A)$, which reduces the groupoid statement to the classical algebraic UCT for the free complex $C_c(\mathcal G_\bullet,\mathbb Z)$. We also isolate the obstruction for non-discrete coefficients. For a locally compact totally disconnected Hausdorff space $X$ with a basis of compact open sets, the image of $Φ_X:C_c(X,\mathbb Z)\otimes_{\mathbb Z}A\to C_c(X,A)$ is exactly the compactly supported functions with finite image. Thus $Φ_X$ is surjective if and only if every $f\in C_c(X,A)$ has finite image, and for suitable $X$ one can produce compactly supported continuous maps $X\to A$ with infinite image. Finally, for a clopen saturated cover $\mathcal G_0=U_1\cup U_2$ we construct a short exact sequence of Moore complexes and derive a Mayer-Vietoris long exact sequence for $H_\bullet(\mathcal G;A)$ for explicit computations.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [813] [Optimizing Spectral Prediction in MXene-Based Metasurfaces Through Multi-Channel Spectral Refinement and Savitzky-Golay Smoothing](https://arxiv.org/abs/2602.08406)
*Shujaat Khan,Waleed Iqbal Waseer*

Main category: physics.optics

TL;DR: 本文提出结合迁移学习、多通道光谱细化和Savitzky - Golay平滑的深度学习框架，加速并提高MXene基太阳能吸收体电磁光谱预测精度，实验显示该模型优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 传统全波求解器预测MXene基太阳能吸收体电磁光谱计算量大，需高效方法。

Method: 引入深度学习框架，利用预训练MobileNetV2模型，采用多通道光谱细化模块增强特征提取，用Savitzky - Golay平滑减少高频噪声。

Result: 模型显著优于基线卷积神经网络和可变形卷积神经网络模型，平均均方根误差为0.0245，决定系数为0.9578，峰值信噪比为32.98 dB。

Conclusion: 该框架是传统求解器的可扩展且计算高效的替代方案，适用于纳米光子设计工作流中的快速光谱预测。

Abstract: The prediction of electromagnetic spectra for MXene-based solar absorbers is a computationally intensive task, traditionally addressed using full-wave solvers. This study introduces an efficient deep learning framework incorporating transfer learning, multi-channel spectral refinement (MCSR), and Savitzky-Golay smoothing to accelerate and enhance spectral prediction accuracy. The proposed architecture leverages a pretrained MobileNetV2 model, fine-tuned to predict 102-point absorption spectra from $64\times64$ metasurface designs. Additionally, the MCSR module processes the feature map through multi-channel convolutions, enhancing feature extraction, while Savitzky-Golay smoothing mitigates high-frequency noise. Experimental evaluations demonstrate that the proposed model significantly outperforms baseline Convolutional Neural Network (CNN) and deformable CNN models, achieving an average root mean squared error (RMSE) of 0.0245, coefficient of determination \( R^2 \) of 0.9578, and peak signal-to-noise ratio (PSNR) of 32.98 dB. The proposed framework presents a scalable and computationally efficient alternative to conventional solvers, positioning it as a viable candidate for rapid spectral prediction in nanophotonic design workflows.

</details>


<div id='cs.SC'></div>

# cs.SC [[Back]](#toc)

### [814] [Automatic Generation of Polynomial Symmetry Breaking Constraints](https://arxiv.org/abs/2602.08297)
*Madalina Erascu,Johannes Middeke*

Main category: cs.SC

TL;DR: 提出代数方法生成多项式不等式做对称破缺器，在0 - 1装箱问题测试，简单对称破缺器能减少工时。


<details>
  <summary>Details</summary>
Motivation: 整数规划中的对称性会导致冗余搜索，传统用对称破缺约束处理，本文提出新方法。

Method: 提出代数方法，以任意基础多项式和排列群为输入，用符号计算软件计算；在0 - 1装箱问题实例中静态生成随机二次破缺器，添加到整数规划问题并用Gurobi求解。

Result: 简单对称破缺器，特别是结合少量变量和排列的，最能持续减少工时。

Conclusion: 该代数方法可行，简单对称破缺器在减少整数规划求解时间上效果较好。

Abstract: Symmetry in integer programming causes redundant search and is often handled with symmetry breaking constraints that remove as many equivalent solutions as possible. We propose an algebraic method which allows to generate a random family of polynomial inequalities which can be used as symmetry breakers. The method requires as input an arbitrary base polynomial and a group of permutations which is specific to the integer program. The computations can be easily carried out in any major symbolic computation software. In order to test our approach, we describe a case study on near half-capacity 0-1 bin packing instances which exhibit substantial symmetries. We statically generate random quadratic breakers and add them to a baseline integer programming problem which we then solve with Gurobi. It turns out that simple symmetry breakers, especially combining few variables and permutations, most consistently reduce work time.

</details>


### [815] [AMS-HD: Hyperdimensional Computing for Real-Time and Energy-Efficient Acute Mountain Sickness Detection](https://arxiv.org/abs/2602.08916)
*Abu Masum,Mehran Moghadam,M. Hassan Najafi,Bige Unluturk,Ulkuhan Guler,Sercan Aygun*

Main category: cs.SC

TL;DR: 介绍高原病及时检测的重要性，指出传统机器学习在平衡预测性能和低硬件需求方面的不足，提出超维计算（HDC）的潜力并介绍了新型系统AMS - HD。


<details>
  <summary>Details</summary>
Motivation: 高原病症状发展迅速，及时检测至关重要，但传统机器学习难以平衡预测性能与低硬件需求，因此探索HDC用于高原病检测。

Method: 提出AMS - HD系统，整合定制特征提取和Hadamard HV编码。

Result: HDC能利用可穿戴设备收集的生理参数实时检测高原病，精度与传统ML模型相当。

Conclusion: AMS - HD框架适用于可穿戴健康监测平台，可实现对急性高原病的持续、动态跟踪。

Abstract: Altitude sickness is a potentially life-threatening condition that impacts many individuals traveling to elevated altitudes. Timely detection is critical as symptoms can escalate rapidly. Early recognition enables simple interventions such as descent, oxygen, or medication, and prompt treatment can save lives by significantly lowering the risk of severe complications. Although conventional machine learning (ML) techniques have been applied to identify altitude sickness using physiological signals, such as heart rate, oxygen saturation, respiration rate, blood pressure, and body temperature, they often struggle to balance predictive performance with low hardware demands. In contrast, hyperdimensional computing (HDC) remains under-explored for this task with limited biomedical features, where it may offer a compelling alternative to existing classification models. Its vector symbolic framework is inherently suited to hardware-efficient design, making it a strong candidate for low-power systems like wearables. Leveraging lightweight computation and efficient streamlined memory usage, HDC enables real-time detection of altitude sickness from physiological parameters collected by wearable devices, achieving accuracy comparable to that of traditional ML models. We present AMS-HD, a novel system that integrates tailored feature extraction and Hadamard HV encoding to enhance both the precision and efficiency of HDC-based detection. This framework is well-positioned for deployment in wearable health monitoring platforms, enabling continuous, on-the-go tracking of acute altitude sickness.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [816] [Fundamental Limits of Community Detection in Contextual Multi-Layer Stochastic Block Models](https://arxiv.org/abs/2602.08173)
*Shuyang Gong,Dong Huang,Zhangsong Li*

Main category: math.ST

TL;DR: 研究从高维协变量矩阵和L个稀疏网络联合观测中进行社区检测问题，给出检测和估计阈值，设计算法并证明其达到阈值，表明无统计 - 计算差距。


<details>
  <summary>Details</summary>
Motivation: 解决从高维协变量矩阵和多个稀疏网络联合观测中检测和估计主体社区标签的问题，扩展已有工作到特定情况并解决相关猜想。

Method: 通过Bernoulli和Gaussian矩之间的新比较不等式及统计变体论证得到信息论下界，基于计数装饰循环和路径设计高效算法。

Result: 得到检测和估计主体标签的尖锐阈值，设计的算法能达到该阈值。

Conclusion: 在该设置下不存在统计 - 计算差距。

Abstract: We consider the problem of community detection from the joint observation of a high-dimensional covariate matrix and $L$ sparse networks, all encoding noisy, partial information about the latent community labels of $n$ subjects. In the asymptotic regime where the networks have constant average degree and the number of features $p$ grows proportionally with $n$, we derive a sharp threshold under which detecting and estimating the subject labels is possible. Our results extend the work of \cite{MN23} to the constant-degree regime with noisy measurements, and also resolve a conjecture in \cite{YLS24+} when the number of networks is a constant.
  Our information-theoretic lower bound is obtained via a novel comparison inequality between Bernoulli and Gaussian moments, as well as a statistical variant of the ``recovery to chi-square divergence reduction'' argument inspired by \cite{DHSS25}. On the algorithmic side, we design efficient algorithms based on counting decorated cycles and decorated paths and prove that they achieve the sharp threshold for both detection and weak recovery. In particular, our results show that there is no statistical-computational gap in this setting.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [817] [Machine learning enhanced data assimilation framework for multiscale carbonate rock characterization](https://arxiv.org/abs/2602.06989)
*Zhenkai Bo,Ahmed H. Elsheikh,Hannah P. Menke,Julien Maes,Sebastian Geiger,Muhammad Z. Kashim,Zainol A. A. Bakar,Kamaljit Singh*

Main category: physics.geo-ph

TL;DR: 针对碳酸盐岩多尺度成像及模拟的难题，提出机器学习增强的数据同化框架DNN - ESMDA，实现高效微尺度结构表征和计算加速。


<details>
  <summary>Details</summary>
Motivation: 传统X射线CT难以全面研究碳酸盐岩多相流行为，多尺度成像面临资源消耗大、计算成本高的问题。

Method: 提出DNN - ESMDA框架，训练DNN作为多尺度孔网络模拟器的代理，并与ESMDA算法耦合。

Result: 该框架能同时推断微孔隙相的二氧化碳 - 卤水排水相对渗透率并进行不确定性估计，计算速度从数千小时缩短至数秒。

Conclusion: DNN - ESMDA框架是一种可推广的用于表征多尺度碳酸盐岩的方法。

Abstract: Carbonate reservoirs offer significant capacity for subsurface carbon storage, oil production, and underground hydrogen storage. X-ray computed tomography (X-ray CT) coupled with numerical simulations is commonly used to investigate the multiphase flow behaviors in carbonate rocks. Carbonates exhibit pore size distribution across scales, hindering the comprehensive investigation with conventional X-ray CT images. Imaging samples at both macro and micro-scales (multi-scale imaging) proved to be a viable option in this context. However, multi-scale imaging faces two key limitations: the trade-off between field of view and voxel size necessitates resource-intensive imaging, while multi-scale multi-physics numerical simulations on resulting digital models incur prohibitive computational costs. To address these challenges, we propose a machine learning-enhanced data assimilation framework that leverages experimental drainage relative permeability measurements to achieve efficient characterization of micro-scale structures, delivering a data-driven solution toward a high-fidelity multiscale digital rock modeling. We train a dense neural network (DNN) as a proxy to a multi-scale pore network simulator and couple it with an ensemble smoother with multiple data assimilation (ESMDA) algorithm. DNN-ESMDA framework simultaneously infers the CO2-brine drainage relative permeability of microporosity phases with associated uncertainty estimation, revealing the relative importance of each rock phase and guiding future characterization. Our DNN-ESMDA framework achieves a computational speedup, reducing inference time from thousands of hours to seconds compared with the usage of conventional multiscale numerical simulation. Given this computational efficiency and applicability, the machine learning-enhanced ESMDA framework presents a generalizable approach for characterizing multiscale carbonate rocks.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [818] [BERT Learns (and Teaches) Chemistry](https://arxiv.org/abs/2007.16012)
*Josh Payne,Mario Srouji,Dian Ang Yap,Vineet Kosaraju*

Main category: q-bio.BM

TL;DR: 本文从数据驱动角度，用基于Transformer的模型研究官能团等分子子结构，将学习到的表征用于解决化学问题，还提出用注意力可视化辅助化学从业者和学生。


<details>
  <summary>Details</summary>
Motivation: 现代计算有机化学数据驱动趋势下，该领域存在诸多未解决问题，需借助机器学习解决。

Method: 使用基于Transformer的模型（BERT）研究分子子结构，将学习到的表征作为特征用于图卷积和注意力模型，以及对BERT进行微调。

Result: 可将学习到的官能团和原子表征用于解决毒性、溶解度等问题。

Conclusion: 注意力可视化可作为辅助工具，帮助化学从业者和学生识别重要子结构。

Abstract: Modern computational organic chemistry is becoming increasingly data-driven. There remain a large number of important unsolved problems in this area such as product prediction given reactants, drug discovery, and metric-optimized molecule synthesis, but efforts to solve these problems using machine learning have also increased in recent years. In this work, we propose the use of attention to study functional groups and other property-impacting molecular substructures from a data-driven perspective, using a transformer-based model (BERT) on datasets of string representations of molecules and analyzing the behavior of its attention heads. We then apply the representations of functional groups and atoms learned by the model to tackle problems of toxicity, solubility, drug-likeness, and synthesis accessibility on smaller datasets using the learned representations as features for graph convolution and attention models on the graph structure of molecules, as well as fine-tuning of BERT. Finally, we propose the use of attention visualization as a helpful tool for chemistry practitioners and students to quickly identify important substructures in various chemical properties.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [819] [LatentChem: From Textual CoT to Latent Thinking in Chemical Reasoning](https://arxiv.org/abs/2602.07075)
*Xinwu Ye,Yicheng Mao,Jia Zhang,Yimeng Liu,Li Hao,Fang Wu,Zhiwei Li,Yuxuan Liao,Zehong Wang,Zhiyuan Liu,Zhenfei Yin,Li Yuan,Philip Torr,Huan Sun,Xiangxiang Zeng,Mengdi Wang,Le Cong,Shenghua Gao,Xiangru Tang*

Main category: physics.chem-ph

TL;DR: 介绍LatentChem，其能让模型在连续潜在空间推理，在化学推理基准测试中表现优于基于CoT的基线，且推理速度更快。


<details>
  <summary>Details</summary>
Motivation: 化学推理具有连续性和结构性，现有化学大语言模型依赖自然语言显式思维链推理存在表示不匹配问题，限制效率和性能。

Method: 引入LatentChem，将化学计算与文本生成解耦，使模型在连续潜在空间进行多步推理，仅在最终输出时使用语言。

Result: LatentChem在ChemCoTBench上对强CoT基线的非平局胜率达59.88%，平均推理速度提升10.84倍。

Conclusion: 化学推理以连续潜在动态实现比离散语言轨迹更自然有效。

Abstract: Chemical large language models (LLMs) predominantly rely on explicit Chain-of-Thought (CoT) in natural language to perform complex reasoning. However, chemical reasoning is inherently continuous and structural, and forcing it into discrete linguistic tokens introduces a fundamental representation mismatch that constrains both efficiency and performance. We introduce LatentChem, a latent reasoning interface that decouples chemical computation from textual generation, enabling models to perform multi-step reasoning directly in continuous latent space while emitting language only for final outputs. Remarkably, we observe a consistent emergent behavior: when optimized solely for task success, models spontaneously internalize reasoning, progressively abandoning verbose textual derivations in favor of implicit latent computation. This shift is not merely stylistic but computationally advantageous. Across diverse chemical reasoning benchmarks, LatentChem achieves a 59.88\% non-tie win rate over strong CoT-based baselines on ChemCoTBench, while delivering a 10.84$\times$ average inference speedup. Our results provide empirical evidence that chemical reasoning is more naturally and effectively realized as continuous latent dynamics rather than discretized linguistic trajectories.

</details>


### [820] [Electron-Informed Coarse-Graining Molecular Representation Learning for Real-World Molecular Physics](https://arxiv.org/abs/2602.07087)
*Gyoung S. Na,Chanyoung Park*

Main category: physics.chem-ph

TL;DR: 提出一种无额外计算成本学习电子信息分子表征的方法，在基准数据集上达最优预测精度。


<details>
  <summary>Details</summary>
Motivation: 现有分子结构表征学习方法局限于原子级信息，不足以描述真实分子物理，获取电子级信息计算成本高甚至不可行。

Method: 将小分子易获取的电子级信息转移到目标大分子，学习电子信息分子表征。

Result: 在包含实验观察分子物理的大量基准数据集上实现了最优预测精度。

Conclusion: 该方法能有效学习电子信息分子表征，提升预测精度。

Abstract: Various representation learning methods for molecular structures have been devised to accelerate data-driven chemistry. However, the representation capabilities of existing methods are essentially limited to atom-level information, which is not sufficient to describe real-world molecular physics. Although electron-level information can provide fundamental knowledge about chemical compounds beyond the atom-level information, obtaining the electron-level information in real-world molecules is computationally impractical and sometimes infeasible. We propose a method for learning electron-informed molecular representations without additional computation costs by transferring readily accessible electron-level information about small molecules to large molecules of our interest. The proposed method achieved state-of-the-art prediction accuracy on extensive benchmark datasets containing experimentally observed molecular physics. The source code for HEDMoL is available at https://github.com/ngs00/HEDMoL.

</details>
