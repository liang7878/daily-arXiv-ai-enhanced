<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 15]
- [cs.CE](#cs.CE) [Total: 3]
- [cs.DB](#cs.DB) [Total: 3]
- [cs.DC](#cs.DC) [Total: 3]
- [cs.DS](#cs.DS) [Total: 6]
- [cs.GT](#cs.GT) [Total: 2]
- [cs.IR](#cs.IR) [Total: 4]
- [cs.LG](#cs.LG) [Total: 52]
- [cs.NE](#cs.NE) [Total: 3]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.SE](#cs.SE) [Total: 12]
- [q-fin.CP](#q-fin.CP) [Total: 1]
- [stat.ML](#stat.ML) [Total: 1]
- [stat.CO](#stat.CO) [Total: 1]
- [cs.LO](#cs.LO) [Total: 1]
- [cs.CY](#cs.CY) [Total: 3]
- [eess.AS](#eess.AS) [Total: 1]
- [econ.GN](#econ.GN) [Total: 1]
- [cs.MA](#cs.MA) [Total: 1]
- [physics.geo-ph](#physics.geo-ph) [Total: 1]
- [stat.ME](#stat.ME) [Total: 2]
- [math.OC](#math.OC) [Total: 1]
- [eess.IV](#eess.IV) [Total: 3]
- [cs.CV](#cs.CV) [Total: 33]
- [cs.CL](#cs.CL) [Total: 27]
- [stat.AP](#stat.AP) [Total: 2]
- [cs.AR](#cs.AR) [Total: 2]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]
- [cs.IT](#cs.IT) [Total: 1]
- [hep-ex](#hep-ex) [Total: 1]
- [eess.SP](#eess.SP) [Total: 1]
- [hep-th](#hep-th) [Total: 1]
- [cs.CR](#cs.CR) [Total: 5]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [quant-ph](#quant-ph) [Total: 4]
- [cs.RO](#cs.RO) [Total: 6]
- [cs.HC](#cs.HC) [Total: 2]
- [eess.SY](#eess.SY) [Total: 2]
- [cs.SD](#cs.SD) [Total: 4]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Stable diffusion models reveal a persisting human and AI gap in visual creativity](https://arxiv.org/abs/2511.16814)
*Silvia Rondini,Claudia Alvarez-Martin,Paula Angermair-Barkai,Olivier Penacchio,M. Paz,Matthew Pelowski,Dan Dediu,Antoni Rodriguez-Fornells,Xim Cerda-Company*

Main category: cs.AI

TL;DR: 研究对比人类与图像生成AI模型的视觉创造力，发现人类创造力高于AI，人类指导可提升AI创造力，且人机评判模式不同。


<details>
  <summary>Details</summary>
Motivation: 现有研究对大语言模型在视觉创造力方面探索不足，该研究旨在对比人类与AI在图像生成上的创造力。

Method: 对比人类参与者（视觉艺术家和非艺术家）与图像生成AI模型（两种提示条件）的图像生成，由人类评分者和GPT4o评估图像创造力。

Result: 存在创造力梯度，视觉艺术家最有创造力，其次是非艺术家、有人启发的生成式AI、自引导生成式AI；人类指导可提升AI创造力；人机评判模式差异大。

Conclusion: 与语言任务不同，生成式AI模型在视觉领域面临独特挑战，人类的感知细微差别和上下文敏感性等能力难以从语言模型转移。

Abstract: While recent research suggests Large Language Models match human creative performance in divergent thinking tasks, visual creativity remains underexplored. This study compared image generation in human participants (Visual Artists and Non Artists) and using an image generation AI model (two prompting conditions with varying human input: high for Human Inspired, low for Self Guided). Human raters (N=255) and GPT4o evaluated the creativity of the resulting images. We found a clear creativity gradient, with Visual Artists being the most creative, followed by Non Artists, then Human Inspired generative AI, and finally Self Guided generative AI. Increased human guidance strongly improved GenAI's creative output, bringing its productions close to those of Non Artists. Notably, human and AI raters also showed vastly different creativity judgment patterns. These results suggest that, in contrast to language centered tasks, GenAI models may face unique challenges in visual domains, where creativity depends on perceptual nuance and contextual sensitivity, distinctly human capacities that may not be readily transferable from language models.

</details>


### [2] [Cognitive BASIC: An In-Model Interpreted Reasoning Language for LLMs](https://arxiv.org/abs/2511.16837)
*Oliver Kramer*

Main category: cs.AI

TL;DR: Cognitive BASIC是一种简易的BASIC风格提示语言和模型内解释器，可将大语言模型推理结构化，经测试大语言模型能执行其程序。


<details>
  <summary>Details</summary>
Motivation: 受复古BASIC简单性启发，希望将大语言模型推理结构化，实现透明的多步推理。

Method: 采用编号行和简单命令作为可解释的认知控制层，用自然语言解释文件指定命令语义等，用心理模型解释器提取知识、检测矛盾并解决。

Result: 在知识提取、冲突检测和推理任务基准测试中，三个大语言模型都能执行Cognitive BASIC程序，表现总体较强但不一致。

Conclusion: Cognitive BASIC能让大语言模型进行结构化推理，具有一定可行性。

Abstract: Cognitive BASIC is a minimal, BASIC-style prompting language and in-model interpreter that structures large language model (LLM) reasoning into explicit, stepwise execution traces. Inspired by the simplicity of retro BASIC, we repurpose numbered lines and simple commands as an interpretable cognitive control layer. Modern LLMs can reliably simulate such short programs, enabling transparent multi-step reasoning inside the model. A natural-language interpreter file specifies command semantics, memory updates, and logging behavior. Our mental-model interpreter extracts declarative and procedural knowledge, detects contradictions, and produces resolutions when necessary. A comparison across three LLMs on a benchmark of knowledge extraction, conflict detection, and reasoning tasks shows that all models can execute Cognitive BASIC programs, with overall strong but not uniform performance.

</details>


### [3] [Fantastic Bugs and Where to Find Them in AI Benchmarks](https://arxiv.org/abs/2511.16842)
*Sang Truong,Yuheng Tu,Michael Hardy,Anka Reuel,Zeyu Tang,Jirayu Burapacheep,Jonathan Perera,Chibuike Uwakwe,Ben Domingue,Nick Haber,Sanmi Koyejo*

Main category: cs.AI

TL;DR: 本文提出系统基准修订框架，利用响应模式统计分析标记问题，结合LLM - judge初筛，高效可扩展。


<details>
  <summary>Details</summary>
Motivation: 基准对AI进步至关重要，但无效基准问题影响可靠性，手动识别修正不可行且是评估瓶颈。

Method: 基于AI评估中均值能概括模型性能的假设，统计分析响应模式标记问题，引入LLM - judge初筛。

Result: 在九个常用基准上，指导专家审查识别问题，精度达84%。

Conclusion: 该框架高效且可扩展，能用于系统基准修订。

Abstract: Benchmarks are pivotal in driving AI progress, and invalid benchmark questions frequently undermine their reliability. Manually identifying and correcting errors among thousands of benchmark questions is not only infeasible but also a critical bottleneck for reliable evaluation. In this work, we introduce a framework for systematic benchmark revision that leverages statistical analysis of response patterns to flag potentially invalid questions for further expert review. Our approach builds on a core assumption commonly used in AI evaluations that the mean score sufficiently summarizes model performance. This implies a unidimensional latent construct underlying the measurement experiment, yielding expected ranges for various statistics for each item. When empirically estimated values for these statistics fall outside the expected range for an item, the item is more likely to be problematic. Across nine widely used benchmarks, our method guides expert review to identify problematic questions with up to 84\% precision. In addition, we introduce an LLM-judge first pass to review questions, further reducing human effort. Together, these components provide an efficient and scalable framework for systematic benchmark revision.

</details>


### [4] [Hybrid Differential Reward: Combining Temporal Difference and Action Gradients for Efficient Multi-Agent Reinforcement Learning in Cooperative Driving](https://arxiv.org/abs/2511.16916)
*Ye Han,Lijun Zhang,Dejian Meng,Zhuang Zhang*

Main category: cs.AI

TL;DR: 本文针对多车协同驾驶中传统奖励函数奖励差异消失问题，提出混合差分奖励（HDR）机制，实验证明其能提升收敛速度和策略稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统基于状态的奖励函数在多车协同驾驶高频连续控制任务中存在奖励差异消失问题，导致策略梯度信噪比低，阻碍算法收敛和性能提升。

Method: 先理论阐释传统奖励信号失效原因，HDR框架集成基于全局势函数的时间差分奖励（TRD）和动作梯度奖励（ARG），将协同驾驶问题建模为含时变智能体集的多智能体部分可观测马尔可夫博弈（POMDPG），并给出HDR实例化方案。

Result: 使用在线规划和多智能体强化学习算法的大量实验表明，HDR机制显著提高收敛速度和策略稳定性。

Conclusion: HDR能引导智能体学习高质量协同策略，有效平衡交通效率和安全。

Abstract: In multi-vehicle cooperative driving tasks involving high-frequency continuous control, traditional state-based reward functions suffer from the issue of vanishing reward differences. This phenomenon results in a low signal-to-noise ratio (SNR) for policy gradients, significantly hindering algorithm convergence and performance improvement. To address this challenge, this paper proposes a novel Hybrid Differential Reward (HDR) mechanism. We first theoretically elucidate how the temporal quasi-steady nature of traffic states and the physical proximity of actions lead to the failure of traditional reward signals. Building on this analysis, the HDR framework innovatively integrates two complementary components: (1) a Temporal Difference Reward (TRD) based on a global potential function, which utilizes the evolutionary trend of potential energy to ensure optimal policy invariance and consistency with long-term objectives; and (2) an Action Gradient Reward (ARG), which directly measures the marginal utility of actions to provide a local guidance signal with a high SNR. Furthermore, we formulate the cooperative driving problem as a Multi-Agent Partially Observable Markov Game (POMDPG) with a time-varying agent set and provide a complete instantiation scheme for HDR within this framework. Extensive experiments conducted using both online planning (MCTS) and Multi-Agent Reinforcement Learning (QMIX, MAPPO, MADDPG) algorithms demonstrate that the HDR mechanism significantly improves convergence speed and policy stability. The results confirm that HDR guides agents to learn high-quality cooperative policies that effectively balance traffic efficiency and safety.

</details>


### [5] [Comparing verbal, visual and combined explanations for Bayesian Network inferences](https://arxiv.org/abs/2511.16961)
*Erik P. Nyberg,Steven Mascaro,Ingrid Zukerman,Michael Wybrow,Duc-Minh Vo,Ann Nicholson*

Main category: cs.AI

TL;DR: 为解决用户理解贝叶斯网络推理困难问题，设计标准贝叶斯网络用户界面的扩展，用户研究表明扩展界面优于基线界面，且图文结合更好。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯网络虽被认为是透明模型，但用户理解困难，当前用户界面无法阐明其推理过程。

Method: 设计贝叶斯网络标准用户界面的语言和视觉扩展，进行用户研究对比语言、视觉、组合扩展界面和基线界面。

Result: 三种扩展界面在某些问题上表现优于基线界面；图文结合在部分问题类型上比单一模态更好。

Conclusion: 设计的用户界面扩展能帮助用户理解贝叶斯网络推理，图文结合有优势。

Abstract: Bayesian Networks (BNs) are an important tool for assisting probabilistic reasoning, but despite being considered transparent models, people have trouble understanding them. Further, current User Interfaces (UIs) still do not clarify the reasoning of BNs. To address this problem, we have designed verbal and visual extensions to the standard BN UI, which can guide users through common inference patterns.
  We conducted a user study to compare our verbal, visual and combined UI extensions, and a baseline UI. Our main findings are: (1) users did better with all three types of extensions than with the baseline UI for questions about the impact of an observation, the paths that enable this impact, and the way in which an observation influences the impact of other observations; and (2) using verbal and visual modalities together is better than using either modality alone for some of these question types.

</details>


### [6] [DAPS++: Rethinking Diffusion Inverse Problems with Decoupled Posterior Annealing](https://arxiv.org/abs/2511.17038)
*Hao Chen,Renzheng Zhang,Scott S. Howard*

Main category: cs.AI

TL;DR: 本文从贝叶斯视角分析分数扩散法在逆问题求解中的问题，提出DAPS++方法，提升计算效率和重建性能。


<details>
  <summary>Details</summary>
Motivation: 当前分数扩散法在逆问题求解中的公式无法解释其实际行为，先验指导有限，推理过程与扩散动力学解耦。

Method: 将扩散在逆问题求解中的作用重新解释为期望最大化（EM）框架中的初始化阶段，提出DAPS++方法。

Result: DAPS++能让似然项更直接地引导推理，保持数值稳定性，在不同图像恢复任务中减少函数评估次数和测量优化步骤，实现高计算效率和稳健的重建性能。

Conclusion: DAPS++方法有效解决了现有分数扩散法的问题，在图像恢复任务中有出色表现。

Abstract: From a Bayesian perspective, score-based diffusion solves inverse problems through joint inference, embedding the likelihood with the prior to guide the sampling process. However, this formulation fails to explain its practical behavior: the prior offers limited guidance, while reconstruction is largely driven by the measurement-consistency term, leading to an inference process that is effectively decoupled from the diffusion dynamics. To clarify this structure, we reinterpret the role of diffusion in inverse problem solving as an initialization stage within an expectation--maximization (EM)--style framework, where the diffusion stage and the data-driven refinement are fully decoupled. We introduce \textbf{DAPS++}, which allows the likelihood term to guide inference more directly while maintaining numerical stability and providing insight into why unified diffusion trajectories remain effective in practice. By requiring fewer function evaluations (NFEs) and measurement-optimization steps, \textbf{DAPS++} achieves high computational efficiency and robust reconstruction performance across diverse image restoration tasks.

</details>


### [7] [MirrorMind: Empowering OmniScientist with the Expert Perspectives and Collective Knowledge of Human Scientists](https://arxiv.org/abs/2511.16997)
*Qingbin Zeng,Bingbing Fan,Zhiyu Chen,Sijian Ren,Zhilun Zhou,Xuhua Zhang,Yuanyi Zhen,Fengli Xu,Yong Li,Tie-Yan Liu*

Main category: cs.AI

TL;DR: 现有AI科学家方法忽视科研社会性，本文提出MirrorMind架构，经四任务评估，可实现结构化、个性化科研推理。


<details>
  <summary>Details</summary>
Motivation: 当前AI科学家方法将科研发现视为孤立过程，忽视其社会性和历史性，现有大语言模型难以表示认知和社会背景，需改进。

Method: 引入MirrorMind分层认知架构，含个体、领域和跨学科三层，分离记忆存储与执行。

Result: 在四项综合任务评估中，MirrorMind结合个体认知深度与集体学科广度，超越简单事实检索。

Conclusion: MirrorMind能实现结构化、个性化和有洞察力的科学推理。

Abstract: The emergence of AI Scientists has demonstrated remarkable potential in automating scientific research. However, current approaches largely conceptualize scientific discovery as a solitary optimization or search process, overlooking that knowledge production is inherently a social and historical endeavor. Human scientific insight stems from two distinct yet interconnected sources. First is the individual cognitive trajectory, where a researcher's unique insight is shaped by their evolving research history and stylistic preferences; another is the collective disciplinary memory, where knowledge is sedimented into vast, interconnected networks of citations and concepts. Existing LLMs still struggle to represent these structured, high-fidelity cognitive and social contexts. To bridge this gap, we introduce MirrorMind, a hierarchical cognitive architecture that integrates dual-memory representations within a three-level framework. The Individual Level constructs high-fidelity cognitive models of individual researchers by capturing their episodic, semantic, and persona memories; the Domain Level maps collective knowledge into structured disciplinary concept graphs; and the Interdisciplinary Level that acts as an orthogonal orchestration engine. Crucially, our architecture separates memory storage from agentic execution, enabling AI scientist agents to flexibly access individual memories for unique perspectives or collective structures to reason. We evaluate MirrorMind across four comprehensive tasks, including author-level cognitive simulation, complementary reasoning, cross-disciplinary collaboration promotion, and multi-agent scientific problem solving. The results show that by integrating individual cognitive depth with collective disciplinary breadth, MirrorMind moves beyond simple fact retrieval toward structural, personalized, and insight-generating scientific reasoning.

</details>


### [8] [Budget-Aware Tool-Use Enables Effective Agent Scaling](https://arxiv.org/abs/2511.17006)
*Tengxiao Liu,Zifeng Wang,Jin Miao,I-Hung Hsu,Jun Yan,Jiefeng Chen,Rujun Han,Fangyuan Xu,Yanfei Chen,Ke Jiang,Samira Daruki,Yi Liang,William Yang Wang,Tomas Pfister,Chen-Yu Lee*

Main category: cs.AI

TL;DR: 研究如何在明确工具调用预算下有效扩展工具增强代理，提出Budget Tracker和BATS框架，表明预算感知方法能产生更优缩放曲线。


<details>
  <summary>Details</summary>
Motivation: 简单增加工具调用预算无法提升代理性能，因缺乏‘预算意识’，需研究在明确预算下有效扩展此类代理的方法。

Method: 引入Budget Tracker提供预算意识，开发BATS框架动态调整策略，形式化统一成本度量。

Result: 预算感知方法产生更有利的缩放曲线，推动了成本 - 性能帕累托前沿。

Conclusion: 工作为更透明和有原则地理解工具增强代理的扩展提供了实证见解。

Abstract: Scaling test-time computation improves performance across different tasks on large language models (LLMs), which has also been extended to tool-augmented agents. For these agents, scaling involves not only "thinking" in tokens but also "acting" via tool calls. The number of tool calls directly bounds the agent's interaction with the external environment. However, we find that simply granting agents a larger tool-call budget fails to improve performance, as they lack "budget awareness" and quickly hit a performance ceiling. To address this, we study how to scale such agents effectively under explicit tool-call budgets, focusing on web search agents. We first introduce the Budget Tracker, a lightweight plug-in that provides the agent with continuous budget awareness, enabling simple yet effective scaling. We further develop BATS (Budget Aware Test-time Scaling), an advanced framework that leverages this awareness to dynamically adapt its planning and verification strategy, deciding whether to "dig deeper" on a promising lead or "pivot" to new paths based on remaining resources. To analyze cost-performance scaling in a controlled manner, we formalize a unified cost metric that jointly accounts for token and tool consumption. We provide the first systematic study on budget-constrained agents, showing that budget-aware methods produce more favorable scaling curves and push the cost-performance Pareto frontier. Our work offers empirical insights toward a more transparent and principled understanding of scaling in tool-augmented agents.

</details>


### [9] [Patient-level Information Extraction by Consistent Integration of Textual and Tabular Evidence with Bayesian Networks](https://arxiv.org/abs/2511.17056)
*Paloma Rabaey,Adrick Tench,Stefan Heytens,Thomas Demeester*

Main category: cs.AI

TL;DR: 提出多模态患者级信息提取方法，利用贝叶斯网络和神经文本分类器，并使用虚拟证据和一致性节点融合预测，在SimSUM数据集验证潜力。


<details>
  <summary>Details</summary>
Motivation: 电子病历部分信息为非结构化文本，为在高风险应用中利用临床决策支持系统，需构建基于透明特征的模型，需要结构化数据集。

Method: 提出多模态患者级信息提取方法，结合专家知识的贝叶斯网络处理表格特征和神经文本分类器处理临床笔记，用虚拟证据和一致性节点融合预测。

Result: 一致性节点相比单独的虚拟证据改善了最终预测的校准，使贝叶斯网络能更好调整神经分类器输出。

Conclusion: 所提方法在SimSUM数据集上展现出潜力。

Abstract: Electronic health records (EHRs) form an invaluable resource for training clinical decision support systems. To leverage the potential of such systems in high-risk applications, we need large, structured tabular datasets on which we can build transparent feature-based models. While part of the EHR already contains structured information (e.g. diagnosis codes, medications, and lab results), much of the information is contained within unstructured text (e.g. discharge summaries and nursing notes). In this work, we propose a method for multi-modal patient-level information extraction that leverages both the tabular features available in the patient's EHR (using an expert-informed Bayesian network) as well as clinical notes describing the patient's symptoms (using neural text classifiers). We propose the use of virtual evidence augmented with a consistency node to provide an interpretable, probabilistic fusion of the models' predictions. The consistency node improves the calibration of the final predictions compared to virtual evidence alone, allowing the Bayesian network to better adjust the neural classifier's output to handle missing information and resolve contradictions between the tabular and text data. We show the potential of our method on the SimSUM dataset, a simulated benchmark linking tabular EHRs with clinical notes through expert knowledge.

</details>


### [10] [The Belief-Desire-Intention Ontology for modelling mental reality and agency](https://arxiv.org/abs/2511.17162)
*Sara Zuppiroli,Carmelo Fabio Longo,Anna Sofia Lippolis,Rocco Paolillo,Lorenzo Giammei,Miguel Ceriani,Francesco Poggi,Antonio Zinilli,Andrea Giovanni Nuzzolese*

Main category: cs.AI

TL;DR: 本文提出形式化BDI本体，通过实验展示其作为概念和操作桥梁，为多智能体和神经符号系统奠定基础。


<details>
  <summary>Details</summary>
Motivation: 解决BDI模型在结构化、语义互操作知识表示中集成有限的问题。

Method: 提出作为模块化本体设计模式的BDI本体，通过与大语言模型结合及集成到推理平台两个实验验证。

Result: 实验表明BDI本体可作为声明性和程序性智能间的桥梁。

Conclusion: BDI本体为基于认知、可解释和语义互操作的多智能体和神经符号系统铺平道路。

Abstract: The Belief-Desire-Intention (BDI) model is a cornerstone for representing rational agency in artificial intelligence and cognitive sciences. Yet, its integration into structured, semantically interoperable knowledge representations remains limited. This paper presents a formal BDI Ontology, conceived as a modular Ontology Design Pattern (ODP) that captures the cognitive architecture of agents through beliefs, desires, intentions, and their dynamic interrelations. The ontology ensures semantic precision and reusability by aligning with foundational ontologies and best practices in modular design. Two complementary lines of experimentation demonstrate its applicability: (i) coupling the ontology with Large Language Models (LLMs) via Logic Augmented Generation (LAG) to assess the contribution of ontological grounding to inferential coherence and consistency; and (ii) integrating the ontology within the Semas reasoning platform, which implements the Triples-to-Beliefs-to-Triples (T2B2T) paradigm, enabling a bidirectional flow between RDF triples and agent mental states. Together, these experiments illustrate how the BDI Ontology acts as both a conceptual and operational bridge between declarative and procedural intelligence, paving the way for cognitively grounded, explainable, and semantically interoperable multi-agent and neuro-symbolic systems operating within the Web of Data.

</details>


### [11] [MIR: Efficient Exploration in Episodic Multi-Agent Reinforcement Learning via Mutual Intrinsic Reward](https://arxiv.org/abs/2511.17165)
*Kesheng Chen,Wenjian Luo,Bang Zhang,Zeping Yin,Zipeng Ye*

Main category: cs.AI

TL;DR: 本文针对多智能体强化学习中情节奖励带来的挑战，提出互促内在奖励（MIR）策略，通过扩展单智能体环境创建实验环境验证其性能更优。


<details>
  <summary>Details</summary>
Motivation: 解决内在奖励方法在多智能体强化学习中应用的难题，包括联合动作轨迹的指数稀疏性和现有方法未考虑联合动作对团队状态的影响。

Method: 提出互促内在奖励（MIR）策略，激励个体智能体探索影响队友的动作；扩展单智能体MiniGrid环境创建MiniGrid - MA实验环境。

Result: 在MiniGrid - MA环境下与现有方法对比，实验结果显示提出的方法性能更优。

Conclusion: MIR策略简单有效，能促进团队探索并提升算法性能。

Abstract: Episodic rewards present a significant challenge in reinforcement learning. While intrinsic reward methods have demonstrated effectiveness in single-agent rein-forcement learning scenarios, their application to multi-agent reinforcement learn-ing (MARL) remains problematic. The primary difficulties stem from two fac-tors: (1) the exponential sparsity of joint action trajectories that lead to rewards as the exploration space expands, and (2) existing methods often fail to account for joint actions that can influence team states. To address these challenges, this paper introduces Mutual Intrinsic Reward (MIR), a simple yet effective enhancement strategy for MARL with extremely sparse rewards like episodic rewards. MIR incentivizes individual agents to explore actions that affect their teammates, and when combined with original strategies, effectively stimulates team exploration and improves algorithm performance. For comprehensive experimental valida-tion, we extend the representative single-agent MiniGrid environment to create MiniGrid-MA, a series of MARL environments with sparse rewards. Our evalu-ation compares the proposed method against state-of-the-art approaches in the MiniGrid-MA setting, with experimental results demonstrating superior perfor-mance.

</details>


### [12] [Designing Domain-Specific Agents via Hierarchical Task Abstraction Mechanism](https://arxiv.org/abs/2511.17198)
*Kaiyu Li,Jiayu Wang,Zhi Wang,Hui Qiao,Weizhan Zhang,Deyu Meng,Xiangyong Cao*

Main category: cs.AI

TL;DR: 现有大语言模型驱动的代理在专业领域表现不佳，本文提出基于HTAM的代理设计框架，构建EarthAgent用于地理空间分析，创建GeoPlan - bench评估，实验显示EarthAgent表现更好，证明使代理架构与领域任务结构对齐很重要。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型驱动的代理在专业领域（如遥感）难以处理严格结构化工作流的问题。

Method: 引入基于Hierarchical Task Abstraction Mechanism (HTAM)的代理设计框架，构建EarthAgent系统，创建GeoPlan - bench基准及评估指标。

Result: 实验表明EarthAgent大幅优于一系列已有的单代理和多代理系统。

Conclusion: 使代理架构与领域的内在任务结构对齐是构建强大可靠的专业自主系统的关键步骤。

Abstract: LLM-driven agents, particularly those using general frameworks like ReAct or human-inspired role-playing, often struggle in specialized domains that necessitate rigorously structured workflows. Fields such as remote sensing, requiring specialized tools (e.g., correction, spectral indices calculation), and multi-step procedures (e.g., numerous intermediate products and optional steps), significantly challenge generalized approaches. To address this gap, we introduce a novel agent design framework centered on a Hierarchical Task Abstraction Mechanism (HTAM). Specifically, HTAM moves beyond emulating social roles, instead structuring multi-agent systems into a logical hierarchy that mirrors the intrinsic task-dependency graph of a given domain. This task-centric architecture thus enforces procedural correctness and decomposes complex problems into sequential layers, where each layer's sub-agents operate on the outputs of the preceding layers. We instantiate this framework as EarthAgent, a multi-agent system tailored for complex geospatial analysis. To evaluate such complex planning capabilities, we build GeoPlan-bench, a comprehensive benchmark of realistic, multi-step geospatial planning tasks. It is accompanied by a suite of carefully designed metrics to evaluate tool selection, path similarity, and logical completeness. Experiments show that EarthAgent substantially outperforms a range of established single- and multi-agent systems. Our work demonstrates that aligning agent architecture with a domain's intrinsic task structure is a critical step toward building robust and reliable specialized autonomous systems.

</details>


### [13] [Agentifying Agentic AI](https://arxiv.org/abs/2511.17332)
*Virginia Dignum,Frank Dignum*

Main category: cs.AI

TL;DR: 本文认为AAMAS社区的概念工具能为实现代理AI愿景提供基础，提出结合自适应方法与结构化模型实现有能力、灵活且透明、合作、可问责的代理系统的路径。


<details>
  <summary>Details</summary>
Motivation: 实现代理AI赋予系统持续自主性、推理和交互能力的愿景，需对代理假设补充认知、合作和治理的显式模型。

Method: 将自适应、数据驱动方法与推理和协调的结构化模型相结合。

Result: 形成了一种连接形式理论和实际自主性的代理视角。

Conclusion: AAMAS社区的概念工具为代理AI提供了实现愿景的基础。

Abstract: Agentic AI seeks to endow systems with sustained autonomy, reasoning, and interaction capabilities. To realize this vision, its assumptions about agency must be complemented by explicit models of cognition, cooperation, and governance. This paper argues that the conceptual tools developed within the Autonomous Agents and Multi-Agent Systems (AAMAS) community, such as BDI architectures, communication protocols, mechanism design, and institutional modelling, provide precisely such a foundation. By aligning adaptive, data-driven approaches with structured models of reasoning and coordination, we outline a path toward agentic systems that are not only capable and flexible, but also transparent, cooperative, and accountable. The result is a perspective on agency that bridges formal theory and practical autonomy.

</details>


### [14] [That's not natural: The Impact of Off-Policy Training Data on Probe Performance](https://arxiv.org/abs/2511.17408)
*Nathalie Kirch,Samuel Dower,Adrians Skapars,Ekdeep Singh Lubana,Dmitrii Krasheninnikov*

Main category: cs.AI

TL;DR: 研究系统评估合成和离策略数据对探测大语言模型八种行为泛化性的影响，发现响应生成策略影响探测性能，离策略数据泛化性可预测在线策略泛化性，还指出不同域数据对性能影响，强调处理分布偏移方法的重要性。


<details>
  <summary>Details</summary>
Motivation: 自然场景中许多行为示例稀少，研究者依赖合成或离策略数据训练探测大语言模型的探针，需评估这些数据对探针泛化性的影响。

Method: 对多个大语言模型的线性和注意力探针进行测试，系统评估合成和离策略数据对八种大语言模型行为的探针泛化性的影响。

Result: 响应生成策略显著影响探针性能，离策略数据到目标行为测试集的成功泛化可预测在线策略泛化，欺骗和故意放水探测在实际监测中可能无法从离策略到在线策略数据泛化，不同域测试分数低于同域。

Conclusion: 在缺乏在线策略数据时，使用同域离策略数据比不同域在线策略数据能产生更可靠的探针，需要更好处理大语言模型监测中分布偏移的方法。

Abstract: Probing has emerged as a promising method for monitoring Large Language Models (LLMs), enabling inference-time detection of concerning behaviours such as deception and sycophancy. However, natural examples of many behaviours are rare, forcing researchers to rely on synthetic or off-policy LLM responses for training probes. We systematically evaluate how the use of synthetic and off-policy data influences probe generalisation across eight distinct LLM behaviours. Testing linear and attention probes across multiple LLMs, we find that the response generation strategy can significantly affect probe performance, though the magnitude of this effect varies by behaviour. We find that successful generalisation from off-policy data, to test sets where the model is incentivised to produce the target behaviour, is predictive of successful on-policy generalisation. Leveraging this result, we predict that Deception and Sandbagging probes may fail to generalise from off-policy to on-policy data when used in real monitoring scenarios. Notably, shifts in the training data domain still cause even larger performance degradation, with different-domain test scores being consistently lower than the same-domain ones. These results indicate that, in the absence of on-policy data, using same-domain off-policy data yields more reliable probes than using on-policy data from a different domain, emphasizing the need for methods that can better handle distribution shifts in LLM monitoring.

</details>


### [15] [SRA-CP: Spontaneous Risk-Aware Selective Cooperative Perception](https://arxiv.org/abs/2511.17461)
*Jiaxi Liu,Chengyuan Ma,Hang Zhou,Weizhe Tang,Shixiao Liang,Haoyang Ding,Xiaopeng Li,Bin Ran*

Main category: cs.AI

TL;DR: 提出SRA - CP框架解决现有协同感知问题，评估显示其节省带宽且提升性能


<details>
  <summary>Details</summary>
Motivation: 现有通用协同感知方法传输大量无关数据超通信带宽，多数框架依赖预定义通信伙伴，不适用于动态交通环境

Method: 提出SRA - CP框架，采用分散协议，有感知风险识别模块，触发CP时选择合适伙伴并进行选择性信息交换

Result: 与通用CP相比，安全关键对象平均精度损失小于1%，仅使用20%通信带宽；比无风险感知的选择性CP方法感知性能提升15%

Conclusion: SRA - CP框架有效解决现有协同感知问题，节省带宽并提升性能

Abstract: Cooperative perception (CP) offers significant potential to overcome the limitations of single-vehicle sensing by enabling information sharing among connected vehicles (CVs). However, existing generic CP approaches need to transmit large volumes of perception data that are irrelevant to the driving safety, exceeding available communication bandwidth. Moreover, most CP frameworks rely on pre-defined communication partners, making them unsuitable for dynamic traffic environments. This paper proposes a Spontaneous Risk-Aware Selective Cooperative Perception (SRA-CP) framework to address these challenges. SRA-CP introduces a decentralized protocol where connected agents continuously broadcast lightweight perception coverage summaries and initiate targeted cooperation only when risk-relevant blind zones are detected. A perceptual risk identification module enables each CV to locally assess the impact of occlusions on its driving task and determine whether cooperation is necessary. When CP is triggered, the ego vehicle selects appropriate peers based on shared perception coverage and engages in selective information exchange through a fusion module that prioritizes safety-critical content and adapts to bandwidth constraints. We evaluate SRA-CP on a public dataset against several representative baselines. Results show that SRA-CP achieves less than 1% average precision (AP) loss for safety-critical objects compared to generic CP, while using only 20% of the communication bandwidth. Moreover, it improves the perception performance by 15% over existing selective CP methods that do not incorporate risk awareness.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [16] [Multivariate Sensitivity Analysis of Electric Machine Efficiency Maps and Profiles Under Design Uncertainty](https://arxiv.org/abs/2511.17099)
*Aylar Partovizadeh,Sebastian Schöps,Dimitrios Loukrezis*

Main category: cs.CE

TL;DR: 本文提出用多元全局灵敏度分析评估电机设计参数不确定性对效率图和曲线的影响，在不同保真度永磁同步电机模型上验证其优势，比较计算方法成本，用分析结果简化模型并验证简化有效性。


<details>
  <summary>Details</summary>
Motivation: 传统基于方差的灵敏度分析逐元素应用，无法对效率图或曲线整体估计参数重要性，需新方法。

Method: 采用多元全局灵敏度分析，比较基于蒙特卡罗采样和多项式混沌展开的计算成本，通过固定非影响参数简化模型。

Result: 多元灵敏度分析能为每个参数提供单一灵敏度指标；比较得出不同计算方法的成本情况；简化模型后的不确定性估计验证了模型简化的有效性。

Conclusion: 多元全局灵敏度分析可用于评估电机设计参数不确定性对效率的影响，且基于其结果的模型简化有效。

Abstract: This work proposes the use of multivariate global sensitivity analysis for assessing the impact of uncertain electric machine design parameters on efficiency maps and profiles. Contrary to the common approach of applying variance-based (Sobol') sensitivity analysis elementwise, multivariate sensitivity analysis provides a single sensitivity index per parameter, thus allowing for a holistic estimation of parameter importance over the full efficiency map or profile. Its benefits are demonstrated on permanent magnet synchronous machine models of different fidelity. Computations based on Monte Carlo sampling and polynomial chaos expansions are compared in terms of computational cost. The sensitivity analysis results are subsequently used to simplify the models, by fixing non-influential parameters to their nominal values and allowing random variations only for influential parameters. Uncertainty estimates obtained with the full and reduced models confirm the validity of model simplification guided by multivariate sensitivity analysis.

</details>


### [17] [Towards Generative Design Using Optimal Transport for Shape Exploration and Solution Field Interpolation](https://arxiv.org/abs/2511.17111)
*Sergio Torregrosa,David Munoz,Hector Navarro,Charbel Farhat,Francisco Chinesta*

Main category: cs.CE

TL;DR: 本文介绍基于最优传输（OT）的生成式设计统一框架，解决现有方法挑战，经测试展示出优势，为未来设计工作流奠基。


<details>
  <summary>Details</summary>
Motivation: 当前生成式设计方法存在AI需大量数据、拓扑优化计算密集、模型降阶不完善等挑战，需新方法解决。

Method: 引入基于最优传输（OT）的统一、保结构框架，利用高斯散点和Wasserstein重心实现复杂几何及物理场插值。

Result: 框架能有效跨任意形状、演变几何插值正标量场，初步扩展到有符号和矢量场，测试案例显示出效率、适应性和物理保真度的提升。

Conclusion: 建立了基于OT的框架，为未来由基础模型驱动的生成式设计工作流奠定基础。

Abstract: Generative Design (GD) combines artificial intelligence (AI), physics-based modeling, and multi-objective optimization to autonomously explore and refine engineering designs. Despite its promise in aerospace, automotive, and other high-performance applications, current GD methods face critical challenges: AI approaches require large datasets and often struggle to generalize; topology optimization is computationally intensive and difficult to extend to multiphysics problems; and model order reduction for evolving geometries remains underdeveloped. To address these challenges, we introduce a unified, structure-preserving framework for GD based on optimal transport (OT), enabling simultaneous interpolation of complex geometries and their associated physical solution fields across evolving design spaces, even with non-matching meshes and substantial shape changes. This capability leverages Gaussian splatting to provide a continuous, mesh-independent representation of the solution and Wasserstein barycenters to enable smooth, mathematically ''mass''-preserving blending of geometries, offering a major advance over surrogate models tied to static meshes. Our framework efficiently interpolates positive scalar fields across arbitrarily shaped, evolving geometries without requiring identical mesh topology or dimensionality. OT also naturally preserves localized physical features -- such as stress concentrations or sharp gradients -- by conserving the spatial distribution of quantities, interpreted as ''mass'' in a mathematical sense, rather than averaging them, avoiding artificial smoothing. Preliminary extensions to signed and vector fields are presented. Representative test cases demonstrate enhanced efficiency, adaptability, and physical fidelity, establishing a foundation for future foundation-model-powered generative design workflows.

</details>


### [18] [Randomness as Reference: Benchmark Metric for Optimization in Engineering](https://arxiv.org/abs/2511.17226)
*Stefan Ivić,Siniša Družeta,Luka Grbčić*

Main category: cs.CE

TL;DR: 本文提出新基准套件和性能指标评估优化算法，发现部分算法在工程问题上效率低，为算法应用提供指导。


<details>
  <summary>Details</summary>
Motivation: 现有广泛采用的人工测试套件与实际工程优化任务的多样性和复杂性匹配度有限，需新基准。

Method: 提出含231个有界、连续、无约束优化问题的基准套件，引入用随机采样作统计参考的性能指标，对20种优化方法进行数百次独立运行评估。

Result: 仅少数测试的优化方法表现出色，部分常用元启发式算法在工程问题上效率严重损失。

Conclusion: 提出的测试套件和指标为评估和比较优化方法提供实用平台，缩小了基准测试与实际工程应用的差距。

Abstract: Benchmarking optimization algorithms is fundamental for the advancement of computational intelligence. However, widely adopted artificial test suites exhibit limited correspondence with the diversity and complexity of real-world engineering optimization tasks. This paper presents a new benchmark suite comprising 231 bounded, continuous, unconstrained optimization problems, the majority derived from engineering design and simulation scenarios, including computational fluid dynamics and finite element analysis models. In conjunction with this suite, a novel performance metric is introduced, which employs random sampling as a statistical reference, providing nonlinear normalization of objective values and enabling unbiased comparison of algorithmic efficiency across heterogeneous problems. Using this framework, 20 deterministic and stochastic optimization methods were systematically evaluated through hundreds of independent runs per problem, ensuring statistical robustness. The results indicate that only a few of the tested optimization methods consistently achieve excellent performance, while several commonly used metaheuristics exhibit severe efficiency loss on engineering-type problems, emphasizing the limitations of conventional benchmarks. Furthermore, the conducted tests are used for analyzing various features of the optimization methods, providing practical guidelines for their application. The proposed test suite and metric together offer a transparent, reproducible, and practically relevant platform for evaluating and comparing optimization methods, thereby narrowing the gap between the available benchmark tests and realistic engineering applications.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [19] [RAG-Driven Data Quality Governance for Enterprise ERP Systems](https://arxiv.org/abs/2511.16700)
*Sedat Bin Vedat,Enes Kutay Yarkan,Meftun Akarsu,Recep Kaan Karaman,Arda Sar,Çağrı Çelikbilek,Savaş Saygılı*

Main category: cs.DB

TL;DR: 本文提出结合自动数据清理与大语言模型驱动的SQL查询生成的端到端管道，部署于生产系统，评估显示效果良好，提供了可复现的企业数据治理框架。


<details>
  <summary>Details</summary>
Motivation: 企业ERP系统在人力资源部门跨多语言分散手动输入时面临数据质量挑战。

Method: 提出端到端管道，包括多阶段数据清理和由GPT - 4o驱动的检索增强生成框架，使用LangChain编排、FAISS向量相似度搜索和少样本学习。

Result: 评估显示查询有效性92.5%、模式合规性95.1%、语义准确性90.7%，将查询周转时间从2.3天降至5秒以内，GPT - 4o相比GPT - 3.5降低了延迟和成本，用户满意度4.3/5.0。

Conclusion: 该模块化架构为企业数据治理提供可复现框架，在企业规模上具有实际可行性。

Abstract: Enterprise ERP systems managing hundreds of thousands of employee records face critical data quality challenges when human resources departments perform decentralized manual entry across multiple languages. We present an end-to-end pipeline combining automated data cleaning with LLM-driven SQL query generation, deployed on a production system managing 240,000 employee records over six months.
  The system operates in two integrated stages: a multi-stage cleaning pipeline that performs translation normalization, spelling correction, and entity deduplication during periodic synchronization from Microsoft SQL Server to PostgreSQL; and a retrieval-augmented generation framework powered by GPT-4o that translates natural-language questions in Turkish, Russian, and English into validated SQL queries. The query engine employs LangChain orchestration, FAISS vector similarity search, and few-shot learning with 500+ validated examples.
  Our evaluation demonstrates 92.5% query validity, 95.1% schema compliance, and 90.7\% semantic accuracy on 2,847 production queries. The system reduces query turnaround time from 2.3 days to under 5 seconds while maintaining 99.2% uptime, with GPT-4o achieving 46% lower latency and 68% cost reduction versus GPT-3.5. This modular architecture provides a reproducible framework for AI-native enterprise data governance, demonstrating real-world viability at enterprise scale with 4.3/5.0 user satisfaction.

</details>


### [20] [LinkML: An Open Data Modeling Framework](https://arxiv.org/abs/2511.16935)
*Sierra A. T. Moxon,Harold Solbrig,Nomi L. Harris,Patrick Kalita,Mark A. Miller,Sujay Patil,Kevin Schaper,Chris Bizon,J. Harry Caufield,Silvano Cirujano Cuesta,Corey Cox,Frank Dekervel,Damion M. Dooley,William D. Duncan,Tim Fliss,Sarah Gehrke,Adam S. L. Graefe,Harshad Hegde,AJ Ireland,Julius O. B. Jacobsen,Madan Krishnamurthy,Carlo Kroll,David Linke,Ryan Ly,Nicolas Matentzoglu,James A. Overton,Jonny L. Saunders,Deepak R. Unni,Gaurav Vaidya,Wouter-Michiel A. M. Vierdag,LinkML Community Contributors,Oliver Ruebel,Christopher G. Chute,Matthew H. Brush,Melissa A. Haendel,Christopher J. Mungall*

Main category: cs.DB

TL;DR: 介绍LinkML框架可简化数据处理，解决科研数据缺乏结构问题，在多领域获应用。


<details>
  <summary>Details</summary>
Motivation: 科研数据缺乏结构，影响互操作性，数据集成、验证和重用困难。

Method: 使用LinkML框架，其可描述多种数据结构，有易上手语法，可与现有框架集成，支持模式导入。

Result: LinkML降低数据异质性、复杂性，使数据符合FAIR标准，在多领域得到应用。

Conclusion: LinkML让隐式模型可计算，能从源头标准化数据。

Abstract: Scientific research relies on well-structured, standardized data; however, much of it is stored in formats such as free-text lab notebooks, non-standardized spreadsheets, or data repositories. This lack of structure challenges interoperability, making data integration, validation, and reuse difficult. LinkML (Linked Data Modeling Language) is an open framework that simplifies the process of authoring, validating, and sharing data. LinkML can describe a range of data structures, from flat, list-based models to complex, interrelated, and normalized models that utilize polymorphism and compound inheritance. It offers an approachable syntax that is not tied to any one technical architecture and can be integrated seamlessly with many existing frameworks. The LinkML syntax provides a standard way to describe schemas, classes, and relationships, allowing modelers to build well-defined, stable, and optionally ontology-aligned data structures. Once defined, LinkML schemas may be imported into other LinkML schemas. These key features make LinkML an accessible platform for interdisciplinary collaboration and a reliable way to define and share data semantics.
  LinkML helps reduce heterogeneity, complexity, and the proliferation of single-use data models while simultaneously enabling compliance with FAIR data standards. LinkML has seen increasing adoption in various fields, including biology, chemistry, biomedicine, microbiome research, finance, electrical engineering, transportation, and commercial software development. In short, LinkML makes implicit models explicitly computable and allows data to be standardized at its origin. LinkML documentation and code are available at linkml.io.

</details>


### [21] [Anomaly Pattern-guided Transaction Bug Testing in Relational Databases](https://arxiv.org/abs/2511.17377)
*Huicong Xu,Shuang Liu,Xianyu Zhu,Qiyu Zhuang,Wei Lu,Xiaoyong Du*

Main category: cs.DB

TL;DR: 提出异常模式引导测试方法检测RDBMS事务错误，工具APTrans在三种数据库发现13个未知错误，11个获确认。


<details>
  <summary>Details</summary>
Motivation: 现有不同隔离级别下测试事务行为有挑战，包括自动生成测试事务和检测逻辑异常困难。

Method: 引入预定义异常模式引导测试用例生成技术，采用显式和隐式错误检测两阶段检测流程。

Result: 在MySQL、MariaDB和OceanBase三种RDBMS上评估，APTrans发现13个先前未知事务相关错误，11个获开发团队确认。

Conclusion: 异常模式引导测试方法能有效检测RDBMS中的事务错误。

Abstract: Concurrent transaction processing is a fundamental capability of Relational Database Management Systems (RDBMSs), widely utilized in applications requiring high levels of parallel user interaction, such as banking systems, e-commerce platforms, and telecommunications infrastructure. Isolation levels offer a configurable mechanism to manage the interaction between concurrent transactions, enabling varying degrees of consistency and performance trade-offs. These isolation guarantees are supported by all major RDBMSs. However, testing transaction behavior under different isolation levels remains a significant challenge due to two primary reasons. First, automatically generating test transactions that can effectively expose bugs in transaction handling logic is non-trivial, as such bugs are typically triggered under specific transactional constraints. Second, detecting logic anomalies in transaction outcomes is difficult because the correct execution results are often unknown for randomly generated transactions. To address these challenges, we propose an anomaly pattern-guided testing approach for uncovering transaction bugs in RDBMSs. Our solution tackles the first challenge by introducing a test case generation technique guided by predefined anomaly patterns, which increases the likelihood of exposing transactional bugs. For the second challenge, we present a two-phase detection process, involving explicit error detection and implicit error detection, to identify bugs in transaction execution. We have implemented our approach in a tool, APTrans, and evaluated it on three widely-used RDBMSs: MySQL, MariaDB, and OceanBase. APTrans successfully identified 13 previously unknown transaction-related bugs, 11 of which have been confirmed by the respective development teams.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [22] [MicroMoE: Fine-Grained Load Balancing for Mixture-of-Experts with Token Scheduling](https://arxiv.org/abs/2511.16947)
*Chenqi Zhao,Wenfei Wu,Linhai Song,Yuchen Xu*

Main category: cs.DC

TL;DR: 现有 MoE 系统负载平衡方案有不足，提出 MicroEP 策略和 MicroMoE 系统，实验显示 MicroMoE 提升训练吞吐量并实现负载平衡。


<details>
  <summary>Details</summary>
Motivation: 现有 MoE 系统负载不平衡影响训练效率，且已有解决方案无法实现细粒度负载平衡。

Method: 提出 MicroEP 并行化策略进行高效令牌调度以实现细粒度负载平衡，构建具备该策略能力的 MicroMoE 分布式训练系统。

Result: MicroMoE 相比现有系统将端到端训练吞吐量提高达 47.6%，且几乎能在 GPU 间始终实现最优负载平衡。

Conclusion: MicroEP 策略和 MicroMoE 系统能有效解决 MoE 系统负载平衡问题，提升训练效率。

Abstract: Mixture-of-Experts (MoE) has emerged as a promising approach to scale up deep learning models due to its significant reduction in computational resources. However, the dynamic nature of MoE leads to load imbalance among experts, severely impacting training efficiency. While previous research has attempted to address the load balancing challenge, existing solutions either compromise model accuracy or introduce additional system overhead. As a result, they fail to achieve fine-grained load balancing, which is crucial to optimizing training efficiency.
  We propose MicroEP, a novel parallelization strategy to achieve fine-grained load balancing in MoE systems. MicroEP is capable of achieving optimal load balancing in every micro-batch through efficient token scheduling across GPUs. Furthermore, we propose MicroMoE, an efficient distributed MoE training system with MicroEP's load balancing capabilities. Our experimental results demonstrate that MicroMoE improves the end-to-end training throughput by up to 47.6% compared with the state-of-the-art system, and almost consistently achieves optimal load balance among GPUs.

</details>


### [23] [Modeling Anomaly Detection in Cloud Services: Analysis of the Properties that Impact Latency and Resource Consumption](https://arxiv.org/abs/2511.17119)
*Gabriel Job Antunes Grabher,Fumio Machida,Thomas Ropars*

Main category: cs.DC

TL;DR: 研究性能异常检测特性对云服务性能与成本权衡的影响，发现检测频率不同时，精度和召回率的重要性不同。


<details>
  <summary>Details</summary>
Motivation: 云服务性能异常检测对维持性能目标很关键，但检测器会出错，需研究检测特性以优化性能与成本的权衡。

Method: 使用随机奖励网络对受性能异常检测器监控的云服务进行建模，研究检测器特性（精度、召回率和检测频率）对服务平均延迟和资源消耗的影响。

Result: 实现高精度和高召回率并非总是必要的。若检测频繁，高精度足以获得良好的性能 - 成本权衡；若检测不频繁，召回率则最重要。

Conclusion: 检测频率影响性能异常检测中精度和召回率的重要性，应根据检测频率合理权衡。

Abstract: Detecting and resolving performance anomalies in Cloud services is crucial for maintaining desired performance objectives. Scaling actions triggered by an anomaly detector help achieve target latency at the cost of extra resource consumption. However, performance anomaly detectors make mistakes. This paper studies which characteristics of performance anomaly detection are important to optimize the trade-off between performance and cost. Using Stochastic Reward Nets, we model a Cloud service monitored by a performance anomaly detector. Using our model, we study the impact of detector characteristics, namely precision, recall and inspection frequency, on the average latency and resource consumption of the monitored service. Our results show that achieving a high precision and a high recall is not always necessary. If detection can be run frequently, a high precision is enough to obtain a good performance-to-cost trade-off, but if the detector is run infrequently, recall becomes the most important.

</details>


### [24] [Instance Configuration for Sustainable Job Shop Scheduling](https://arxiv.org/abs/2409.18972)
*Christian Perez,Carlos March,Miguel A. Salido*

Main category: cs.DC

TL;DR: 研究作业车间调度问题（JSP），提出创新实例配置器生成500个测试实例并公开，用于调度算法评估和节能研究。


<details>
  <summary>Details</summary>
Motivation: JSP是运筹学关键挑战，需优化性能指标和降低能耗，同时考虑多约束，现有基准测试需丰富。

Method: 提出含多种参数和分布的实例配置器，生成涵盖不同配置、反映现实场景和约束的实例。

Result: 生成500个测试实例并公开，可用于调度算法综合评估。

Conclusion: 实例有助于调度算法的强大分析和协作，推动先进节能调度方案发展。

Abstract: The Job Shop Scheduling Problem (JSP) is a pivotal challenge in operations research and is essential for evaluating the effectiveness and performance of scheduling algorithms. Scheduling problems are a crucial domain in combinatorial optimization, where resources (machines) are allocated to job tasks to minimize the completion time (makespan) alongside other objectives like energy consumption. This research delves into the intricacies of JSP, focusing on optimizing performance metrics and minimizing energy consumption while considering various constraints such as deadlines and release dates. Recognizing the multi-dimensional nature of benchmarking in JSP, this study underscores the significance of reference libraries and datasets like JSPLIB in enriching algorithm evaluation. The research highlights the importance of problem instance characteristics, including job and machine numbers, processing times, and machine availability, emphasizing the complexities introduced by energy consumption considerations.
  An innovative instance configurator is proposed, equipped with parameters such as the number of jobs, machines, tasks, and speeds, alongside distributions for processing times and energy consumption. The generated instances encompass various configurations, reflecting real-world scenarios and operational constraints. These instances facilitate comprehensive benchmarking and evaluation of scheduling algorithms, particularly in contexts of energy efficiency. A comprehensive set of 500 test instances has been generated and made publicly available, promoting further research and benchmarking in JSP. These instances enable robust analyses and foster collaboration in developing advanced, energy-efficient scheduling solutions by providing diverse scenarios.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [25] [Efficient Algorithms and Implementations for Extracting Maximum-Size $(k,\ell)$-Sparse Subgraphs](https://arxiv.org/abs/2511.16877)
*Péter Madarasi*

Main category: cs.DS

TL;DR: 本文提出高效灵活的增广路径法实现，结合实用启发式策略，实验表明其性能远超现有工具，还提出渐近更快算法并提供公开实现。


<details>
  <summary>Details</summary>
Motivation: 解决寻找最大规模(k, ℓ)-稀疏子图问题，提升算法效率。

Method: 采用增广路径法，结合边排序、节点排序、两阶段策略和基于伪森林的初始化等启发式策略；提出渐近更快的提取包含意义上最大(k, 2k)-稀疏子图的算法。

Result: 综合实验显示实现性能比现有工具高几个数量级；提出特定稀疏子图的渐近更快算法。

Conclusion: 所提实现高效且灵活，能显著减少运行时间并保持最优性，公开实现可纳入LEMON图库。

Abstract: A multigraph $G = (V, E)$ is $(k, \ell)$-sparse if every subset $X \subseteq V$ induces at most $\max\{k|X| - \ell, 0\}$ edges. Finding a maximum-size $(k, \ell)$-sparse subgraph is a classical problem in rigidity theory and combinatorial optimization, with known polynomial-time algorithms. This paper presents a highly efficient and flexible implementation of an augmenting path method, enhanced with a range of powerful practical heuristics that significantly reduce running time while preserving optimality. These heuristics $\unicode{x2013}$ including edge-ordering, node-ordering, two-phase strategies, and pseudoforest-based initialization $\unicode{x2013}$ steer the algorithm toward accepting more edges early in the execution and avoiding costly augmentations. A comprehensive experimental evaluation on both synthetic and real-world graphs demonstrates that our implementation outperforms existing tools by several orders of magnitude. We also propose an asymptotically faster algorithm for extracting an inclusion-wise maximal $(k,2k)$-sparse subgraph with the sparsity condition required only for node sets of size at least three, which is particularly relevant to 3D rigidity when $k = 3$. We provide a carefully engineered implementation, which is publicly available online and is proposed for inclusion in the LEMON graph library.

</details>


### [26] [Low-Sensitivity Matching via Sampling from Gibbs Distributions](https://arxiv.org/abs/2511.16918)
*Yuichi Yoshida,Zihan Zhang*

Main category: cs.DS

TL;DR: 本文从灵敏度视角研究最大匹配问题，给出不同图类型的多项式时间近似算法，改进了灵敏度和运行时间的已知界。


<details>
  <summary>Details</summary>
Motivation: 从灵敏度角度研究最大匹配问题，改进已有灵敏度和运行时间的界限。

Method: 基于从匹配的吉布斯分布采样设计算法，针对不同图类型采用不同采样策略。

Result: 1. 对任意ε>0，存在灵敏度为Δ^O(1/ε)的(1 - ε)近似算法，运行时间O_ε,Δ(m log m)。2. 对平面图和二部图，有运行时间为poly(n/ε)的更快算法。3. 对一般图，存在灵敏度为√n⋅(ε^(-1) log n)^O(1/ε)的(1 - ε)近似算法。

Conclusion: 所提算法在灵敏度和运行时间上优于先前的结果。

Abstract: In this work, we study the maximum matching problem from the perspective of sensitivity. The sensitivity of an algorithm $A$ on a graph $G$ is defined as the maximum Wasserstein distance between the output distributions of $A$ on $G$ and on $G - e$, where $G - e$ is the graph obtained by deleting an edge $e$ from $G$. The maximum is taken over all edges $e$, and the underlying metric for the Wasserstein distance is the Hamming distance.
  We first show that for any $\varepsilon > 0$, there exists a polynomial-time $(1 - \varepsilon)$-approximation algorithm with sensitivity $Δ^{O(1/\varepsilon)}$, where $Δ$ is the maximum degree of the input graph. The algorithm is based on sampling from the Gibbs distribution over matchings and runs in time $O_{\varepsilon, Δ}(m \log m)$, where $m$ is the number of edges in the graph. This result significantly improves the previously known sensitivity bounds.
  Next, we present significantly faster algorithms for planar and bipartite graphs as a function of $\varepsilon$ and $Δ$, which run in time $\mathrm{poly}(n/\varepsilon)$. This improvement is achieved by designing a more efficient algorithm for sampling matchings from the Gibbs distribution in these graph classes, which improves upon the previous best in terms of running time.
  Finally, for general graphs with potentially unbounded maximum degree, we show that there exists a polynomial-time $(1 - \varepsilon)$-approximation algorithm with sensitivity $\sqrt{n} \cdot (\varepsilon^{-1} \log n)^{O(1/\varepsilon)}$, improving upon the previous best bound of $O(n^{1/(1+\varepsilon^2)})$.

</details>


### [27] [Merging RLBWTs adaptively](https://arxiv.org/abs/2511.16953)
*Travis Gagie*

Main category: cs.DS

TL;DR: 在特定参数较小时，以O(R)空间快速合并游程长度压缩的Burrows - Wheeler变换（RLBWTs），时间复杂度为O~(L + σ + R)。


<details>
  <summary>Details</summary>
Motivation: 实现RLBWTs的快速合并，并探究在一定条件下的合并情况。

Method: 考虑组合扩展Burrows - Wheeler变换（eBWT）中相同原始RLBWT字符块之间的边界，用L表示边界处最长公共前缀（LCP）值之和。

Result: 能在O(R)空间和O~(L + σ + R)时间内合并RLBWTs。

Conclusion: 推测当原始RLBWTs对应的字符串重复但不相似时，L往往较小。

Abstract: We show how to merge run-length compressed Burrows-Wheeler Transforms (RLBWTs) quickly and in $O (R)$ space, where $R$ is the total number of runs in them, when a certain parameter is small. Specifically, we consider the boundaries in their combined extended Burrows-Wheeler Transform (eBWT) between blocks of characters from the same original RLBWT, and denote by $L$ the sum of the longest common prefix (LCP) values at those boundaries. We show how to merge the RLBWTs in $\tilde{O} (L + σ+ R)$ time, where $σ$ is the alphabet size. We conjecture that $L$ tends to be small when the strings (or sets of strings) underlying the original RLBWTs are repetitive but dissimilar.

</details>


### [28] [Triangle Detection in H-Free Graphs](https://arxiv.org/abs/2511.17224)
*Amir Abboud,Ron Safier,Nathan Wallheimer*

Main category: cs.DS

TL;DR: 研究H - 自由图中三角形检测的组合算法，尝试分类哪些模式可实现亚立方加速，给出上下界结果及特殊情况算法。


<details>
  <summary>Details</summary>
Motivation: 研究H - 自由图中用组合方法进行三角形检测，分类哪些模式能实现亚立方加速，向二分定理迈进。

Method: 下界分析H的性质判断复杂度是否变化；上界采用嵌入方法为“可嵌入”模式设计亚立方算法；还给出两种推广及奇数环特殊情况的专门算法。

Result: 若H不可3 - 着色或含多个三角形，无组合加速；为可嵌入模式设计出强亚立方算法，时间复杂度为\(\tilde O(n^{3 - \frac{1}{2^{k - 3}}})\)，可扩展到列出所有三角形；完成小模式分类，给出两种推广；奇数环特殊情况有高效算法。

Conclusion: 对H - 自由图中三角形检测的组合算法进行了系统研究，给出不同模式下的复杂度结果和算法，推动了该领域的二分定理研究。

Abstract: We initiate the study of combinatorial algorithms for Triangle Detection in $H$-free graphs. The goal is to decide if a graph that forbids a fixed pattern $H$ as a subgraph contains a triangle, using only "combinatorial" methods that notably exclude fast matrix multiplication. Our work aims to classify which patterns admit a subcubic speedup, working towards a dichotomy theorem. On the lower bound side, we show that if $H$ is not $3$-colorable or contains more than one triangle, the complexity of the problem remains unchanged, and no combinatorial speedup is likely possible. On the upper bound side, we develop an embedding approach that results in a strongly subcubic, combinatorial algorithm for a rich class of "embeddable" patterns. Specifically, for an embeddable pattern of size $k$, our algorithm runs in $\tilde O(n^{3-\frac{1}{2^{k-3}}})$ time, where $\tilde O(\cdot)$ hides poly-logarithmic factors. This algorithm also extends to listing all the triangles within the same time bound. We supplement this main result with two generalizations: 1) A generalization to patterns that are embeddable up to a single obstacle that arises from a triangle in the pattern. This completes our classification for small patterns, yielding a dichotomy theorem for all patterns of size up to eight. 2) An $H$-sensitive algorithm for embeddable patterns, which runs faster when the number of copies of $H$ is significantly smaller than the maximum possible $Ω(n^k)$. Finally, we focus on the special case of odd cycles. We present specialized Triangle Detection algorithms that are very efficient: 1) A combinatorial algorithm for $C_{2k+1}$-free graphs that runs in $\tilde O(m+n^{1+2/k})$ time for every $k\geq2$, where $m$ is the number of edges in the graph. 2) A combinatorial $C_5$-sensitive algorithm that runs in $\tilde O(n^2+n^{4/3}t^{1/3})$ time, where $t$ is the number of $5$-cycles in the graph.

</details>


### [29] [Spectral Clustering with Side Information](https://arxiv.org/abs/2511.17326)
*Hendrik Fichtenberger,Michael Kapralov,Ekaterina Kochetkova,Silvio Lattanzi,Davide Mazzali,Weronika Wrzos-Kaminska*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In the graph clustering problem with a planted solution, the input is a graph on $n$ vertices partitioned into $k$ clusters, and the task is to infer the clusters from graph structure. A standard assumption is that clusters induce well-connected subgraphs (i.e. $Ω(1)$-expanders), and form $ε$-sparse cuts. Such a graph defines the clustering uniquely up to $\approx ε$ misclassification rate, and efficient algorithms for achieving this rate are known. While this vanilla version of graph clustering is well studied, in practice, vertices of the graph are typically equipped with labels that provide additional information on cluster ids of the vertices. For example, each vertex could have a cluster label that is corrupted independently with probability $δ$. Using only one of the two sources of information leads to misclassification rate $\min\{ε, δ\}$, but can they be combined to achieve a rate of $\approx εδ$?
  In this paper, we give an affirmative answer to this question and present a sublinear-time algorithm in the number of vertices $n$. Our key algorithmic insight is a new observation on ``spectrally ambiguous'' vertices in a well-clusterable graph.
  While our sublinear-time classifier achieves the nearly optimal $\approx \widetilde O(εδ)$ misclassification rate, the approximate clusters that it outputs do not necessarily induce expanders in the graph $G$. In our second result, we give a polynomial-time algorithm that reweights edges of the original $(k, ε, Ω(1))$-clusterable graph to transform it into a $(k, \widetilde O(εδ), Ω(1))$-clusterable one (for constant $k$), improving sparsity of cuts nearly optimally and preserving expansion properties of the communities - an algorithm for refining community structure of the input graph.

</details>


### [30] [Relative Error Streaming Quantiles with Seamless Mergeability via Adaptive Compactors](https://arxiv.org/abs/2511.17396)
*Tomáš Domes,Pavel Veselý*

Main category: cs.DS

TL;DR: 本文提出ReqSketch改进版本，简化证明并在特定场景实现近最优空间界。


<details>
  <summary>Details</summary>
Motivation: ReqSketch合并性证明复杂，需简化。

Method: 开发自适应压缩器得到ReqSketch的改进版本。

Result: 在最通用合并性设置下简化相对误差保证证明，保留原空间界、更新时间和算法简单性，在特定场景实现近最优空间界。

Conclusion: 改进后的ReqSketch在证明简化和特定场景性能上有优势。

Abstract: Quantile summaries provide a scalable way to estimate the distribution of individual attributes in large datasets that are often distributed across multiple machines or generated by sensor networks. ReqSketch (arXiv:2004.01668) is currently the most space-efficient summary with two key properties: relative error guarantees, offering increasingly higher accuracy towards the distribution's tails, and mergeability, allowing distributed or parallel processing of datasets. Due to these features and its simple algorithm design, ReqSketch has been adopted in practice, via implementation in the Apache DataSketches library. However, the proof of mergeability in ReqSketch is overly complicated, requiring an intricate charging argument and complex variance analysis.
  In this paper, we provide a refined version of ReqSketch, by developing so-called adaptive compactors. This enables a significantly simplified proof of relative error guarantees in the most general mergeability setting, while retaining the original space bound, update time, and algorithmic simplicity. Moreover, the adaptivity of our sketch, together with the proof technique, yields near-optimal space bounds in specific scenarios - particularly when merging sketches of comparable size.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [31] [Near-Optimal Dropout-Robust Sortition](https://arxiv.org/abs/2511.16897)
*Maya Pal Gambhir,Bailey Flanigan,Aaron Roth*

Main category: cs.GT

TL;DR: 针对公民大会中成员临时退出问题，提出最小化损失的算法，比较了算法效果并刻画了权衡关系。


<details>
  <summary>Details</summary>
Motivation: 解决公民大会成员临时退出影响小组规模和代表性的问题，在无法提前预知退出者的情况下，选择合适小组成员。

Method: 将问题建模为极小极大博弈，提出损失最小化算法，迭代执行投影梯度下降子程序并对抗计算最佳响应退出分布的有效算法。

Result: 算法在从最坏情况到平均情况改变最大化者权力时保持最优，比较了算法与现有基准，首次刻画了该问题中鲁棒性、损失和平等性之间的权衡。

Conclusion: 提出的算法有效解决了公民大会成员退出问题，并明确了相关权衡关系。

Abstract: Citizens' assemblies - small panels of citizens that convene to deliberate on policy issues - often face the issue of panelists dropping out at the last-minute. Without intervention, these dropouts compromise the size and representativeness of the panel, prompting the question: Without seeing the dropouts ahead of time, can we choose panelists such that after dropouts, the panel will be representative and appropriately-sized? We model this problem as a minimax game: the minimizer aims to choose a panel that minimizes the loss, i.e., the deviation of the ultimate panel from predefined representation targets. Then, an adversary defines a distribution over dropouts from which the realized dropouts are drawn. Our main contribution is an efficient loss-minimizing algorithm, which remains optimal as we vary the maximizer's power from worst case to average case. Our algorithm - which iteratively plays a projected gradient descent subroutine against an efficient algorithm for computing the best-response dropout distribution - also addresses a key open question in the area: how to manage dropouts while ensuring that each potential panelist is chosen with relatively equal probabilities. Using real-world datasets, we compare our algorithms to existing benchmarks, and we offer the first characterizations of tradeoffs between robustness, loss, and equality in this problem.

</details>


### [32] [The Effects of Latency on a Progressive Second-Price Auction](https://arxiv.org/abs/2511.17424)
*Jordana Blazek,Eric Olson,Fredrick C. Harris*

Main category: cs.GT

TL;DR: 研究渐进二价拍卖中延迟、异步分析和随机初始出价对均衡的影响，引入找最小收益均衡算法，发现设置略低于清算价的保留价可稳定卖家收益。


<details>
  <summary>Details</summary>
Motivation: 研究延迟、异步分析和随机初始出价对渐进二价拍卖中通过真实ε - 最佳回复方法获得的ε - 纳什均衡的影响。

Method: 引入寻找最小收益均衡状态的算法。

Result: 设置略低于清算价的保留价可稳定卖家收益并保持效率，买家个体分配的价值和成本有不可预测性，但效用可预测。

Conclusion: 设置保留价对稳定卖家收益有效，在弹性需求假设下买家效用可预测。

Abstract: The progressive second-price auction of Lazar and Semret is a decentralized mechanism for the allocation and real-time pricing of a divisible resource. Our focus is on how delays in the receipt of bid messages, asynchronous analysis by buyers of the market and randomness in the initial bids affect the $\varepsilon$-Nash equilibria obtained by the method of truthful $\varepsilon$-best reply. We introduce an algorithm for finding minimal-revenue equilibrium states and then show that setting a reserve price just below clearing stabilizes seller revenue while maintaining efficiency. Utility is of primary interest given the assumption of elastic demand. Although some buyers experienced unpredictability in the value and cost of their individual allocations, their respective utilities were predictable.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [33] [δ-EMG: A Monotonic Graph Index for Approximate Nearest Neighbor Search](https://arxiv.org/abs/2511.16921)
*Liming Xiang,Jing Feng,Ziqi Yin,Zijian Li,Daihao Xue,Hongchao Qin,Ronghua Li,Guoren Wang*

Main category: cs.IR

TL;DR: 本文提出基于图的误差有界近似最近邻（ANN）搜索方法，设计相关图结构与算法，实验表明其性能优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 现有 ANN 搜索算法基于 ε - Recall - Bounded 原则，未对错误结果偏差提供保证，需改进。

Method: 采用基于图的框架，提出 δ - EMG 提供可证明近似，设计误差有界的 top - k ANN 搜索算法，引入 δ - EMQG 并集成向量量化。

Result: 在 ANN - Benchmarks 数据集实验，在 SIFT1M 数据集召回率 0.99 时，QPS 达 19000，超其他方法 40%以上。

Conclusion: 所提误差有界的 ANN 搜索方法有效，性能表现良好。

Abstract: Approximate nearest neighbor (ANN) search in high-dimensional spaces is a foundational component of many modern retrieval and recommendation systems. Currently, almost all algorithms follow an $ε$-Recall-Bounded principle when comparing performance: they require the ANN search results to achieve a recall of more than $1-ε$ and then compare query-per-second (QPS) performance. However, this approach only accounts for the recall of true positive results and does not provide guarantees on the deviation of incorrect results. To address this limitation, we focus on an Error-Bounded ANN method, which ensures that the returned results are a $(1/δ)$-approximation of the true values. Our approach adopts a graph-based framework. To enable Error-Bounded ANN search, we propose a $δ$-EMG (Error-bounded Monotonic Graph), which, for the first time, provides a provable approximation for arbitrary queries. By enforcing a $δ$-monotonic geometric constraint during graph construction, $δ$-EMG ensures that any greedy search converges to a $(1/δ)$-approximate neighbor without backtracking. Building on this foundation, we design an error-bounded top-$k$ ANN search algorithm that adaptively controls approximation accuracy during query time. To make the framework practical at scale, we introduce $δ$-EMQG (Error-bounded Monotonic Quantized Graph), a localized and degree-balanced variant with near-linear construction complexity. We further integrate vector quantization to accelerate distance computation while preserving theoretical guarantees. Extensive experiments on the ANN-Benchmarks dataset demonstrate the effectiveness of our approach. Under a recall requirement of 0.99, our algorithm achieves 19,000 QPS on the SIFT1M dataset, outperforming other methods by more than 40\%.

</details>


### [34] [RASTP: Representation-Aware Semantic Token Pruning for Generative Recommendation with Semantic Identifiers](https://arxiv.org/abs/2511.16943)
*Tianyu Zhan,Kairui Fu,Zheqi Lv,Shengyu Zhang*

Main category: cs.IR

TL;DR: 提出RASTP方法直接修剪输入序列中信息较少的标记，在三个亚马逊数据集上实验，减少训练时间并维持或提升推荐性能，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有生成式推荐系统用多个语义标识符表示物品ID会增加输入序列长度，导致计算复杂度和内存消耗增加，而现有工作主要关注优化注意力计算和KV缓存。

Method: 提出RASTP方法，通过结合语义显著性（通过表示幅度衡量）和注意力中心性（从累积注意力权重得出）评估标记重要性，动态修剪低信息或无关的语义标记。

Result: 在三个真实世界的亚马逊数据集上，RASTP减少训练时间26.7%，同时维持或略微提升推荐性能。

Conclusion: RASTP能有效减少训练时间，同时保证推荐性能，具有一定实用价值。

Abstract: Generative recommendation systems typically leverage Semantic Identifiers (SIDs), which represent each item as a sequence of tokens that encode semantic information. However, representing item ID with multiple SIDs significantly increases input sequence length, which is a major determinant of computational complexity and memory consumption. While existing efforts primarily focus on optimizing attention computation and KV cache, we propose RASTP (Representation-Aware Semantic Token Pruning), which directly prunes less informative tokens in the input sequence. Specifically, RASTP evaluates token importance by combining semantic saliency, measured via representation magnitude, and attention centrality, derived from cumulative attention weights. Since RASTP dynamically prunes low-information or irrelevant semantic tokens, experiments on three real-world Amazon datasets show that RASTP reduces training time by 26.7\%, while maintaining or slightly improving recommendation performance. The code has been open-sourced at https://github.com/Yuzt-zju/RASTP.

</details>


### [35] [CLLMRec: LLM-powered Cognitive-Aware Concept Recommendation via Semantic Alignment and Prerequisite Knowledge Distillation](https://arxiv.org/abs/2511.17041)
*Xiangrui Xiong,Yichuan Lu,Zifei Pan,Chang Sun*

Main category: cs.IR

TL;DR: 本文提出CLLMRec框架解决MOOC个性化概念推荐问题，不依赖高质量结构化知识图谱，实验表明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有MOOC概念推荐方法依赖高质量结构化知识图谱，而现实教育场景中此类图谱稀缺，存在局限性。

Method: 提出CLLMRec框架，通过语义对齐构建统一表示空间，采用师生架构进行先修知识蒸馏，结合深度知识追踪的精排机制。

Result: 在两个真实MOOC数据集上的实验显示，CLLMRec在多个评估指标上显著优于现有基线方法。

Conclusion: CLLMRec在不依赖明确结构先验的情况下，能有效生成认知感知和个性化的概念推荐。

Abstract: The growth of Massive Open Online Courses (MOOCs) presents significant challenges for personalized learning, where concept recommendation is crucial. Existing approaches typically rely on heterogeneous information networks or knowledge graphs to capture conceptual relationships, combined with knowledge tracing models to assess learners' cognitive states. However, these methods face significant limitations due to their dependence on high-quality structured knowledge graphs, which are often scarce in real-world educational scenarios. To address this fundamental challenge, this paper proposes CLLMRec, a novel framework that leverages Large Language Models through two synergistic technical pillars: Semantic Alignment and Prerequisite Knowledge Distillation. The Semantic Alignment component constructs a unified representation space by encoding unstructured textual descriptions of learners and concepts. The Prerequisite Knowledge Distillation paradigm employs a teacher-student architecture, where a large teacher LLM (implemented as the Prior Knowledge Aware Component) extracts conceptual prerequisite relationships from its internalized world knowledge and distills them into soft labels to train an efficient student ranker. Building upon these foundations, our framework incorporates a fine-ranking mechanism that explicitly models learners' real-time cognitive states through deep knowledge tracing, ensuring recommendations are both structurally sound and cognitively appropriate. Extensive experiments on two real-world MOOC datasets demonstrate that CLLMRec significantly outperforms existing baseline methods across multiple evaluation metrics, validating its effectiveness in generating truly cognitive-aware and personalized concept recommendations without relying on explicit structural priors.

</details>


### [36] [Parametric Retrieval-Augmented Generation using Latent Routing of LoRA Adapters](https://arxiv.org/abs/2511.17044)
*Zhan Su,Fengran Mo,Jian-yun Nie*

Main category: cs.IR

TL;DR: 提出Poly - PRAG解决当前PRAG范式问题，实验在多数据集达SOTA。


<details>
  <summary>Details</summary>
Motivation: 当前PRAG采用一对一文档编码方案存在数据稀缺和推理开销大的问题。

Method: 提出Poly - PRAG，离线编码将文档编码视为多任务学习，用路由函数以少量潜在LoRA适配器编码；在线推理根据查询激活部分潜在专家。

Result: 在多个知识密集型NLP任务综合评估，在四个不同数据集取得SOTA结果。

Conclusion: 提出的Poly - PRAG方法有效。

Abstract: Parametric Retrieval-Augmented Generation (PRAG) is a novel RAG paradigm that integrates external knowledge directly into a Large Language Model (LLM) by parameterizing documents using LoRA adapters, demonstrating reduced inference costs compared to traditional RAG approaches. However, current PRAG approaches adopt a \textbf{one-to-one} document encoding scheme, using a dedicated LoRA adapter for each individual document. This scheme introduces two major limitations: First, it leads to data scarcity, as the training datasets for individual LoRA adapters are limited. Second, it incurs high overhead during inference, requiring the merging of LLM weights with a new LoRA adapter for every candidate passage, which is computationally inefficient. To overcome these challenges, we propose a novel paradigm for encoding passages in PRAG that utilizes a latent routing encoding process (Poly-PRAG). During offline encoding, we treat the encoding of a set of documents as a multi-task learning process, where each passage is assigned a unique task identifier. By employing a routing function, we use a small set of latent LoRA adapters to encode the entire passage space. During online inference, this routing function selectively activates a subset of latent experts based on the input query. We conduct comprehensive evaluations of Poly-PRAG across multiple knowledge-intensive NLP tasks. Our extensive experiments demonstrate the effectiveness of the proposed method, achieving state-of-the-art results on four distinct datasets.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [37] [Joint Design of Protein Surface and Structure Using a Diffusion Bridge Model](https://arxiv.org/abs/2511.16675)
*Guanlue Li,Xufeng Zhao,Fang Wu,Sören Laue*

Main category: cs.LG

TL;DR: 提出PepBridge框架用于蛋白质表面和结构联合设计，经多步流程生成完整蛋白质结构，验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决设计与目标受体精确互补的多样化、物理现实的蛋白质结构和表面这一计算蛋白质设计中的重大挑战。

Method: 以3D点云表示受体表面，通过多步流程，利用去噪扩散桥模型映射受体与配体表面，多模型扩散模型预测对应结构，Shape - Frame匹配网络确保表面几何与骨架结构对齐。

Result: 在多种蛋白质设计场景的广泛验证中，PepBridge能生成结构可行的蛋白质。

Conclusion: PepBridge是自顶向下蛋白质结构联合设计的重大进展。

Abstract: Protein-protein interactions (PPIs) are governed by surface complementarity and hydrophobic interactions at protein interfaces. However, designing diverse and physically realistic protein structure and surfaces that precisely complement target receptors remains a significant challenge in computational protein design. In this work, we introduce PepBridge, a novel framework for the joint design of protein surface and structure that seamlessly integrates receptor surface geometry and biochemical properties. Starting with a receptor surface represented as a 3D point cloud, PepBridge generates complete protein structures through a multi-step process. First, it employs denoising diffusion bridge models (DDBMs) to map receptor surfaces to ligand surfaces. Next, a multi-model diffusion model predicts the corresponding structure, while Shape-Frame Matching Networks ensure alignment between surface geometry and backbone architecture. This integrated approach facilitates surface complementarity, conformational stability, and chemical feasibility. Extensive validation across diverse protein design scenarios demonstrates PepBridge's efficacy in generating structurally viable proteins, representing a significant advancement in the joint design of top-down protein structure.

</details>


### [38] [DDTime: Dataset Distillation with Spectral Alignment and Information Bottleneck for Time-Series Forecasting](https://arxiv.org/abs/2511.16715)
*Yuqi Li,Kuiye Ding,Chuanguang Yang,Hao Wang,Haoxuan Wang,Huiran Duan,Junming Liu,Yingli Tian*

Main category: cs.LG

TL;DR: 提出轻量级插件式蒸馏框架DDTime用于时间序列预测数据集蒸馏，解决两大挑战，实验表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 时间序列预测训练准确模型需大量数据和计算资源，数据集蒸馏是替代方案，但扩展到时间序列预测面临两大挑战。

Method: 提出基于一阶凝聚分解的DDTime框架，用时间统计和频域对齐机制解决时间偏差，设计样本间正则化增强样本多样性。

Result: 在20个基准数据集和多种预测架构上实验，DDTime始终优于现有蒸馏方法，相对准确率提升约30%，计算开销约2.49%。

Conclusion: DDTime能有效解决时间序列数据集蒸馏的挑战，提升预测准确性，且计算开销小。

Abstract: Time-series forecasting is fundamental across many domains, yet training accurate models often requires large-scale datasets and substantial computational resources. Dataset distillation offers a promising alternative by synthesizing compact datasets that preserve the learning behavior of full data. However, extending dataset distillation to time-series forecasting is non-trivial due to two fundamental challenges: 1.temporal bias from strong autocorrelation, which leads to distorted value-term alignment between teacher and student models; and 2.insufficient diversity among synthetic samples, arising from the absence of explicit categorical priors to regularize trajectory variety.
  In this work, we propose DDTime, a lightweight and plug-in distillation framework built upon first-order condensation decomposition. To tackle Challenge 1, it revisits value-term alignment through temporal statistics and introduces a frequency-domain alignment mechanism to mitigate autocorrelation-induced bias, ensuring spectral consistency and temporal fidelity. To address Challenge 2, we further design an inter-sample regularization inspired by the information bottleneck principle, which enhances diversity and maximizes information density across synthetic trajectories. The combined objective is theoretically compatible with a wide range of condensation paradigms and supports stable first-order optimization. Extensive experiments on 20 benchmark datasets and diverse forecasting architectures demonstrate that DDTime consistently outperforms existing distillation methods, achieving about 30% relative accuracy gains while introducing about 2.49% computational overhead. All code and distilled datasets will be released.

</details>


### [39] [CroTad: A Contrastive Reinforcement Learning Framework for Online Trajectory Anomaly Detection](https://arxiv.org/abs/2511.16929)
*Rui Xue,Dan He,Fengmei Jin,Chen Zhang,Xiaofang Zhou*

Main category: cs.LG

TL;DR: 提出无阈值、抗噪声的在线轨迹异常检测框架CroTad，实验证明其有效性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在子轨迹异常检测、阈值适应性、处理不规则采样和噪声数据方面存在不足。

Method: 提出对比强化学习框架CroTad，结合对比学习提取正常模式，利用深度强化学习进行实时异常评分。

Result: 在两个真实数据集上的实验表明，该框架在各种评估场景下都有效且鲁棒。

Conclusion: CroTad能有效解决现有轨迹异常检测方法的问题，实现及时、细粒度的异常检测。

Abstract: Detecting trajectory anomalies is a vital task in modern Intelligent Transportation Systems (ITS), enabling the identification of unsafe, inefficient, or irregular travel behaviours. While deep learning has emerged as the dominant approach, several key challenges remain unresolved. First, sub-trajectory anomaly detection, capable of pinpointing the precise segments where anomalies occur, remains underexplored compared to whole-trajectory analysis. Second, many existing methods depend on carefully tuned thresholds, limiting their adaptability in real-world applications. Moreover, the irregular sampling of trajectory data and the presence of noise in training sets further degrade model performance, making it difficult to learn reliable representations of normal routes. To address these challenges, we propose a contrastive reinforcement learning framework for online trajectory anomaly detection, CroTad. Our method is threshold-free and robust to noisy, irregularly sampled data. By incorporating contrastive learning, CroTad learns to extract diverse normal travel patterns for different itineraries and effectively distinguish anomalous behaviours at both sub-trajectory and point levels. The detection module leverages deep reinforcement learning to perform online, real-time anomaly scoring, enabling timely and fine-grained identification of abnormal segments. Extensive experiments on two real-world datasets demonstrate the effectiveness and robustness of our framework across various evaluation scenarios.

</details>


### [40] [When Structure Doesn't Help: LLMs Do Not Read Text-Attributed Graphs as Effectively as We Expected](https://arxiv.org/abs/2511.16767)
*Haotian Xu,Yuning You,Tengfei Ma*

Main category: cs.LG

TL;DR: 研究不同图结构编码策略对大语言模型在文本属性图上性能的影响，发现仅用节点文本描述就能有强性能，多数结构编码策略增益不大甚至有负面影响。


<details>
  <summary>Details</summary>
Motivation: 探索不同图结构编码策略对大语言模型在文本属性图上性能的影响，挑战结构对基于大语言模型的图推理有益的基础假设。

Method: 进行系统实验，研究不同编码图结构的策略。

Result: 大语言模型仅利用节点文本描述在各任务中已能取得强性能，多数结构编码策略增益微弱甚至为负。

Conclusion: 强大语言模型参与时，显式结构先验常不必要，甚至适得其反，需重新思考大语言模型时代图结构的表示和利用方式。

Abstract: Graphs provide a unified representation of semantic content and relational structure, making them a natural fit for domains such as molecular modeling, citation networks, and social graphs. Meanwhile, large language models (LLMs) have excelled at understanding natural language and integrating cross-modal signals, sparking interest in their potential for graph reasoning. Recent work has explored this by either designing template-based graph templates or using graph neural networks (GNNs) to encode structural information. In this study, we investigate how different strategies for encoding graph structure affect LLM performance on text-attributed graphs. Surprisingly, our systematic experiments reveal that: (i) LLMs leveraging only node textual descriptions already achieve strong performance across tasks; and (ii) most structural encoding strategies offer marginal or even negative gains. We show that explicit structural priors are often unnecessary and, in some cases, counterproductive when powerful language models are involved. This represents a significant departure from traditional graph learning paradigms and highlights the need to rethink how structure should be represented and utilized in the LLM era. Our study is to systematically challenge the foundational assumption that structure is inherently beneficial for LLM-based graph reasoning, opening the door to new, semantics-driven approaches for graph learning.

</details>


### [41] [Gradient flow for deep equilibrium single-index models](https://arxiv.org/abs/2511.16976)
*Sanjit Dandapanthula,Aaditya Ramdas*

Main category: cs.LG

TL;DR: 本文研究线性模型和单指标模型中深度均衡模型（DEQs）的梯度下降动力学，证明守恒定律和收敛性，并通过实验验证。


<details>
  <summary>Details</summary>
Motivation: 尽管DEQs在实际应用中取得成功，但对其训练的梯度下降动力学的理论理解仍是活跃研究领域，本文旨在填补相关文献空白。

Method: 在简单的线性模型和单指标模型中严格研究DEQs的梯度下降动力学，证明线性DEQs的守恒定律，利用该性质分析梯度流的条件性，证明梯度下降的线性收敛性。

Result: 证明线性DEQs的守恒定律，表明训练中参数被困在球面上，梯度流始终条件良好；证明线性DEQs和深度均衡单指标模型在适当初始化和小步长下梯度下降线性收敛到全局极小值；通过实验验证理论结果。

Conclusion: 本文对线性模型和单指标模型中DEQs的梯度下降动力学进行了理论分析和实验验证，填补了相关文献的空白。

Abstract: Deep equilibrium models (DEQs) have recently emerged as a powerful paradigm for training infinitely deep weight-tied neural networks that achieve state of the art performance across many modern machine learning tasks. Despite their practical success, theoretically understanding the gradient descent dynamics for training DEQs remains an area of active research. In this work, we rigorously study the gradient descent dynamics for DEQs in the simple setting of linear models and single-index models, filling several gaps in the literature. We prove a conservation law for linear DEQs which implies that the parameters remain trapped on spheres during training and use this property to show that gradient flow remains well-conditioned for all time. We then prove linear convergence of gradient descent to a global minimizer for linear DEQs and deep equilibrium single-index models under appropriate initialization and with a sufficiently small step size. Finally, we validate our theoretical findings through experiments.

</details>


### [42] [GCL-OT: Graph Contrastive Learning with Optimal Transport for Heterophilic Text-Attributed Graphs](https://arxiv.org/abs/2511.16778)
*Yating Ren,Yikun Ban,Huobin Tan*

Main category: cs.LG

TL;DR: 现有结构 - 文本对比学习方法在异质图上有局限，本文提出含最优传输的图对比学习框架GCL - OT，在九个基准测试中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有结构 - 文本对比学习方法依赖同质性假设和硬优化目标，处理异质图有局限，且文本嵌入处理不佳，文本属性图存在多粒度异质性使结构 - 文本对齐困难。

Method: 提出GCL - OT框架，针对部分异质性设计基于RealSoftMax的相似度估计器；针对完全异质性引入基于提示的过滤器；引入OT引导的软监督挖掘潜在同质性。

Result: 理论分析表明GCL - OT能提高互信息界和贝叶斯误差保证，九个基准测试显示GCL - OT始终优于现有方法。

Conclusion: GCL - OT在处理文本属性图的多粒度异质性方面有效且鲁棒。

Abstract: Recently, structure-text contrastive learning has shown promising performance on text-attributed graphs by leveraging the complementary strengths of graph neural networks and language models. However, existing methods typically rely on homophily assumptions in similarity estimation and hard optimization objectives, which limit their applicability to heterophilic graphs. Although existing methods can mitigate heterophily through structural adjustments or neighbor aggregation, they usually treat textual embeddings as static targets, leading to suboptimal alignment. In this work, we identify the multi-granular heterophily in text-attributed graphs, including complete heterophily, partial heterophily, and latent homophily, which makes structure-text alignment particularly challenging due to mixed, noisy, and missing semantic correlations. To achieve flexible and bidirectional alignment, we propose GCL-OT, a novel graph contrastive learning framework with optimal transport, equipped with tailored mechanisms for each type of heterophily. Specifically, for partial heterophily, we design a RealSoftMax-based similarity estimator to emphasize key neighbor-word interactions while easing background noise. For complete heterophily, we introduce a prompt-based filter that adaptively excludes irrelevant noise during optimal transport alignment. Furthermore, we incorporate OT-guided soft supervision to uncover potential neighbors with similar semantics, enhancing the learning of latent homophily. Theoretical analysis shows that GCL-OT can improve the mutual information bound and Bayes error guarantees. Extensive experiments on nine benchmarks show that GCL-OT consistently outperforms state-of-the-art methods, verifying its effectiveness and robustness.

</details>


### [43] [Revisiting Multimodal KV Cache Compression: A Frequency-Domain-Guided Outlier-KV-Aware Approach](https://arxiv.org/abs/2511.16786)
*Yaoxin Yang,Peng Ye,Xudong Tan,Chongjun Tu,Maosen Zhao,Jia Hao,Tao Chen*

Main category: cs.LG

TL;DR: 现有多模态KV缓存压缩方法有局限，本文提出FlashCache框架，实验表明其优于现有方法，能加速解码并降低内存使用，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型推理开销大，现有多模态KV缓存压缩方法与高效注意力内核不兼容且忽略值向量贡献。

Method: 从KV矩阵分布角度，用低通滤波器提取主能量，定义离群KVs；提出FlashCache框架，包括离群KV识别模块和动态预算分配模块。

Result: 在多个MLLMs和基准测试上，FlashCache优于现有多模态KV压缩方法，解码速度最高提升1.69倍，KV内存使用降低80%，同时保持任务性能。

Conclusion: FlashCache是一种有效的多模态KV缓存压缩框架，能在降低内存占用的情况下提高推理速度。

Abstract: Multimodal large language models suffer from substantial inference overhead since multimodal KV Cache grows proportionally with the visual input length. Existing multimodal KV Cache compression methods mostly rely on attention score to reduce cache size, which makes them are incompatible with established efficient attention kernels (e.g., FlashAttention) and ignores the contribution of value vectors to the attention output. In this work, we revisit multimodal KV Cache compression from the perspective of the KV matrices' distribution. First, we observe that frequency-domain energy of multimodal KV matrices is predominantly concentrated in low-frequency and extract this principal energy via a low-pass filter. Further, we find that removing KV pairs that deviate substantially from this principal energy leads to a pronounced performance drop, which we define as Outlier KVs. Considering Outlier KVs are more likely to encode features critical for inference, we propose FlashCache, a frequency-domain-guided, Outlier-KV-aware KV Cache compression framework. First, we introduce an Outlier KV Recognition Module that models the principal component of multimodal KV matrices in the frequency domain and preferentially retains KV pairs that significantly deviate from it. Furthermore, Dynamic Budget Allocation Module is designed to adaptively determine the per-layer KV Cache size to retain more Outlier KVs. Experiments on multiple MLLMs and benchmarks demonstrate that FlashCache outperforms state-of-the-art multimoal KV compression methods, achieving up to 1.69 times faster decoding with 80% lower KV memory usage while maintaining task performance.

</details>


### [44] [SAVeD: Semantic Aware Version Discovery](https://arxiv.org/abs/2511.17298)
*Artem Frenk,Roee Shraga*

Main category: cs.LG

TL;DR: 介绍基于对比学习的SAVeD框架，用于识别结构化数据集版本，实验显示其在区分语义变更版本上表现良好，优于基线和部分方法。


<details>
  <summary>Details</summary>
Motivation: 解决数据科学中因数据集上相似工作或转换导致的重复劳动问题。

Method: 采用修改后的SimCLR管道，通过随机变换生成增强表视图，用自定义Transformer编码器嵌入并在潜在空间对比优化语义相似度。

Result: 在五个基准数据集实验中，训练后有显著提升，在未见过的表上准确率高，分离得分显著提高。

Conclusion: SAVeD能有效区分语义变更版本，自定义编码器结果有竞争力或更优。

Abstract: Our work introduces SAVeD (Semantically Aware Version Detection), a contrastive learning-based framework for identifying versions of structured datasets without relying on metadata, labels, or integration-based assumptions. SAVeD addresses a common challenge in data science of repeated labor due to a difficulty of similar work or transformations on datasets. SAVeD employs a modified SimCLR pipeline, generating augmented table views through random transformations (e.g., row deletion, encoding perturbations). These views are embedded via a custom transformer encoder and contrasted in latent space to optimize semantic similarity. Our model learns to minimize distances between augmented views of the same dataset and maximize those between unrelated tables. We evaluate performance using validation accuracy and separation, defined respectively as the proportion of correctly classified version/non-version pairs on a hold-out set, and the difference between average similarities of versioned and non-versioned tables (defined by a benchmark, and not provided to the model). Our experiments span five canonical datasets from the Semantic Versioning in Databases Benchmark, and demonstrate substantial gains post-training. SAVeD achieves significantly higher accuracy on completely unseen tables in, and a significant boost in separation scores, confirming its capability to distinguish semantically altered versions. Compared to untrained baselines and prior state-of-the-art dataset-discovery methods like Starmie, our custom encoder achieves competitive or superior results.

</details>


### [45] [A Vector Symbolic Approach to Multiple Instance Learning](https://arxiv.org/abs/2511.16795)
*Ehsan Ahmed Dhrubo,Mohammad Mahmudul Alam,Edward Raff,Tim Oates,James Holt*

Main category: cs.LG

TL;DR: 提出基于向量符号架构（VSAs）的多实例学习（MIL）框架，在标准MIL基准和医学影像数据集上取得SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 多数基于深度学习的MIL方法违反逻辑约束，导致性能指标虚高和泛化能力差。

Method: 将实例和概念表示为高维向量，用代数运算执行约束；设计编码器将输入实例转换为VSA兼容向量。

Result: 在标准MIL基准和医学影像数据集上达到SOTA，优于现有方法。

Conclusion: 该工作为依赖启发式学习的现有MIL方法提供了有原则、可解释且有效的替代方案。

Abstract: Multiple Instance Learning (MIL) tasks impose a strict logical constraint: a bag is labeled positive if and only if at least one instance within it is positive. While this iff constraint aligns with many real-world applications, recent work has shown that most deep learning-based MIL approaches violate it, leading to inflated performance metrics and poor generalization. We propose a novel MIL framework based on Vector Symbolic Architectures (VSAs), which provide a differentiable mechanism for performing symbolic operations in high-dimensional space. Our method encodes the MIL assumption directly into the model's structure by representing instances and concepts as nearly orthogonal high-dimensional vectors and using algebraic operations to enforce the iff constraint during classification. To bridge the gap between raw data and VSA representations, we design a learned encoder that transforms input instances into VSA-compatible vectors while preserving key distributional properties. Our approach, which includes a VSA-driven MaxNetwork classifier, achieves state-of-the-art results for a valid MIL model on standard MIL benchmarks and medical imaging datasets, outperforming existing methods while maintaining strict adherence to the MIL formulation. This work offers a principled, interpretable, and effective alternative to existing MIL approaches that rely on learned heuristics.

</details>


### [46] [A Robust Federated Learning Approach for Combating Attacks Against IoT Systems Under non-IID Challenges](https://arxiv.org/abs/2511.16822)
*Eyad Gad,Zubair Md Fadlullah,Mostafa M. Fouda*

Main category: cs.LG

TL;DR: 在数据激增和资源受限场景下，联邦学习（FL）虽能解决部分问题，但非IID数据的统计异质性是挑战。研究对比FedAvg、FedProx和Scaffold在不同数据分布下表现，用CICIoT2023数据集分类物联网攻击。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习模型训练在数据增长和资源受限环境面临挑战，FL虽有优势但非IID数据的统计异质性影响其效果，且缺乏相关联邦方法对比研究。

Method: 探索FedAvg、FedProx和Scaffold三种FL算法在不同数据分布下的表现，利用CICIoT2023数据集对大规模物联网攻击进行分类。

Result: 文中未提及具体结果。

Conclusion: 文中未提及具体结论，但目标是揭示这些FL方法的性能差异，为相关人员提供见解。

Abstract: In the context of the growing proliferation of user devices and the concurrent surge in data volumes, the complexities arising from the substantial increase in data have posed formidable challenges to conventional machine learning model training. Particularly, this is evident within resource-constrained and security-sensitive environments such as those encountered in networks associated with the Internet of Things (IoT). Federated Learning has emerged as a promising remedy to these challenges by decentralizing model training to edge devices or parties, effectively addressing privacy concerns and resource limitations. Nevertheless, the presence of statistical heterogeneity in non-Independently and Identically Distributed (non-IID) data across different parties poses a significant hurdle to the effectiveness of FL. Many FL approaches have been proposed to enhance learning effectiveness under statistical heterogeneity. However, prior studies have uncovered a gap in the existing research landscape, particularly in the absence of a comprehensive comparison between federated methods addressing statistical heterogeneity in detecting IoT attacks. In this research endeavor, we delve into the exploration of FL algorithms, specifically FedAvg, FedProx, and Scaffold, under different data distributions. Our focus is on achieving a comprehensive understanding of and addressing the challenges posed by statistical heterogeneity. In this study, We classify large-scale IoT attacks by utilizing the CICIoT2023 dataset. Through meticulous analysis and experimentation, our objective is to illuminate the performance nuances of these FL methods, providing valuable insights for researchers and practitioners in the domain.

</details>


### [47] [Self-Supervised Learning by Curvature Alignment](https://arxiv.org/abs/2511.17426)
*Benyamin Ghojogh,M. Hadi Sepanj,Paul Fieguth*

Main category: cs.LG

TL;DR: 本文提出曲率正则化自监督学习框架CurvSSL及其核扩展，实验显示其在MNIST和CIFAR - 10数据集上有竞争力或更优表现，表明显式塑造局部几何是对纯统计正则化器的有效补充。


<details>
  <summary>Details</summary>
Motivation: 现有的非对比自监督学习方法主要关注表征的一阶和二阶统计，而忽略了底层数据流形的局部几何，因此提出新方法。

Method: 引入CurvSSL及其核扩展kernel CurvSSL，采用标准双视图编码器 - 投影器架构，在投影特征上使用Barlow Twins式冗余减少损失，并添加基于曲率的正则化器。

Result: 在MNIST和CIFAR - 10数据集上，使用ResNet - 18骨干网络，曲率正则化的自监督学习在线性评估性能上比Barlow Twins和VICReg更具竞争力或表现更优。

Conclusion: 显式塑造局部几何是对纯统计自监督学习正则化器的简单有效的补充。

Abstract: Self-supervised learning (SSL) has recently advanced through non-contrastive methods that couple an invariance term with variance, covariance, or redundancy-reduction penalties. While such objectives shape first- and second-order statistics of the representation, they largely ignore the local geometry of the underlying data manifold. In this paper, we introduce CurvSSL, a curvature-regularized self-supervised learning framework, and its RKHS extension, kernel CurvSSL. Our approach retains a standard two-view encoder-projector architecture with a Barlow Twins-style redundancy-reduction loss on projected features, but augments it with a curvature-based regularizer. Each embedding is treated as a vertex whose $k$ nearest neighbors define a discrete curvature score via cosine interactions on the unit hypersphere; in the kernel variant, curvature is computed from a normalized local Gram matrix in an RKHS. These scores are aligned and decorrelated across augmentations by a Barlow-style loss on a curvature-derived matrix, encouraging both view invariance and consistency of local manifold bending. Experiments on MNIST and CIFAR-10 datasets with a ResNet-18 backbone show that curvature-regularized SSL yields competitive or improved linear evaluation performance compared to Barlow Twins and VICReg. Our results indicate that explicitly shaping local geometry is a simple and effective complement to purely statistical SSL regularizers.

</details>


### [48] [Monte Carlo Expected Threat (MOCET) Scoring](https://arxiv.org/abs/2511.16823)
*Joseph Kim,Saahith Potluri*

Main category: cs.LG

TL;DR: 现有评估指标有局限，引入可量化现实风险的MOCET指标


<details>
  <summary>Details</summary>
Motivation: 评估和衡量AI安全水平威胁很重要，现有指标无法很好处理现实风险及适应大语言模型快速发展

Method: 引入可解释且双重可扩展的MOCET指标

Result: 提出MOCET指标

Conclusion: 需要能更好结合现实风险的指标及可扩展、开放式指标，MOCET可满足需求

Abstract: Evaluating and measuring AI Safety Level (ASL) threats are crucial for guiding stakeholders to implement safeguards that keep risks within acceptable limits. ASL-3+ models present a unique risk in their ability to uplift novice non-state actors, especially in the realm of biosecurity. Existing evaluation metrics, such as LAB-Bench, BioLP-bench, and WMDP, can reliably assess model uplift and domain knowledge. However, metrics that better contextualize "real-world risks" are needed to inform the safety case for LLMs, along with scalable, open-ended metrics to keep pace with their rapid advancements. To address both gaps, we introduce MOCET, an interpretable and doubly-scalable metric (automatable and open-ended) that can quantify real-world risks.

</details>


### [49] [ManifoldFormer: Geometric Deep Learning for Neural Dynamics on Riemannian Manifolds](https://arxiv.org/abs/2511.16828)
*Yihang Fu,Lifang He,Qingyu Chen*

Main category: cs.LG

TL;DR: 现有EEG基础模型忽略神经动力学几何结构，ManifoldFormer提出新几何深度学习框架改进，评估显示优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有EEG基础模型将神经信号视为欧几里得空间中的通用时间序列，忽略神经动力学内在几何结构，限制表征质量和跨主体泛化能力。

Method: 提出ManifoldFormer框架，包括用于流形嵌入的黎曼VAE、具有测地线感知注意力机制的几何Transformer和利用神经ODE进行流形约束时间演化的动力学预测器。

Result: 在四个公共数据集上评估，比现有方法有显著改进，准确率高4.6 - 4.8%，Cohen's Kappa高6.2 - 10.2%，且跨主体泛化能力强。

Conclusion: 几何方法揭示了符合神经生理学原理的有意义神经模式，表明几何约束对有效的EEG基础模型至关重要。

Abstract: Existing EEG foundation models mainly treat neural signals as generic time series in Euclidean space, ignoring the intrinsic geometric structure of neural dynamics that constrains brain activity to low-dimensional manifolds. This fundamental mismatch between model assumptions and neural geometry limits representation quality and cross-subject generalization. ManifoldFormer addresses this limitation through a novel geometric deep learning framework that explicitly learns neural manifold representations. The architecture integrates three key innovations: a Riemannian VAE for manifold embedding that preserves geometric structure, a geometric Transformer with geodesic-aware attention mechanisms operating directly on neural manifolds, and a dynamics predictor leveraging neural ODEs for manifold-constrained temporal evolution. Extensive evaluation across four public datasets demonstrates substantial improvements over state-of-the-art methods, with 4.6-4.8% higher accuracy and 6.2-10.2% higher Cohen's Kappa, while maintaining robust cross-subject generalization. The geometric approach reveals meaningful neural patterns consistent with neurophysiological principles, establishing geometric constraints as essential for effective EEG foundation models.

</details>


### [50] [Analysis of heart failure patient trajectories using sequence modeling](https://arxiv.org/abs/2511.16839)
*Falk Dippela,Yinan Yu,Annika Rosengren,Martin Lindgren,Christina E. Lundberg,Erik Aerts,Martin Adiels,Helen Sjöland*

Main category: cs.LG

TL;DR: 研究对比六种序列模型在瑞典心衰队列中的表现，发现Llama和Mambas表现佳，且提出消融研究，为临床预测任务模型开发提供参考。


<details>
  <summary>Details</summary>
Motivation: 医疗领域缺乏系统分析模型在不同设置下性能和效率的方法，需对不同序列模型在临床预测任务中的表现进行研究。

Method: 在瑞典心衰队列中研究三种架构类别的六种序列模型，评估其在三个一年预测任务中的表现，进行消融研究。

Result: Llama预测辨别力最高、校准最佳且各任务表现稳健，Mambas次之；二者表示学习高效，小配置超大型Transformer；相同模型大小下，用25%更少训练数据有优越表现。

Conclusion: 本消融研究为输入标记、模型配置和时间数据预处理提供系统设计选择，未来临床预测任务模型开发可参考。

Abstract: Transformers have defined the state-of-the-art for clinical prediction tasks involving electronic health records (EHRs). The recently introduced Mamba architecture outperformed an advanced Transformer (Transformer++) based on Llama in handling long context lengths, while using fewer model parameters. Despite the impressive performance of these architectures, a systematic approach to empirically analyze model performance and efficiency under various settings is not well established in the medical domain. The performances of six sequence models were investigated across three architecture classes (Transformers, Transformers++, Mambas) in a large Swedish heart failure (HF) cohort (N = 42820), providing a clinically relevant case study. Patient data included diagnoses, vital signs, laboratories, medications and procedures extracted from in-hospital EHRs. The models were evaluated on three one-year prediction tasks: clinical instability (a readmission phenotype) after initial HF hospitalization, mortality after initial HF hospitalization and mortality after latest hospitalization. Ablations account for modifications of the EHR-based input patient sequence, architectural model configurations, and temporal preprocessing techniques for data collection. Llama achieves the highest predictive discrimination, best calibration, and showed robustness across all tasks, followed by Mambas. Both architectures demonstrate efficient representation learning, with tiny configurations surpassing other large-scaled Transformers. At equal model size, Llama and Mambas achieve superior performance using 25% less training data. This paper presents a first ablation study with systematic design choices for input tokenization, model configuration and temporal data preprocessing. Future model development in clinical prediction tasks using EHRs could build upon this study's recommendation as a starting point.

</details>


### [51] [Provably Minimum-Length Conformal Prediction Sets for Ordinal Classification](https://arxiv.org/abs/2511.16845)
*Zijian Zhang,Xinyu Chen,Yuanjie Shi,Liyuan Lillian Ma,Zifan Xu,Yan Yan*

Main category: cs.LG

TL;DR: 本文提出了一种模型无关的序贯共形预测（ordinal - CP）方法，解决了现有序贯CP方法的不足，实验表明该方法比基线方法的预测效率显著提高。


<details>
  <summary>Details</summary>
Motivation: 现有序贯CP方法主要关注启发式算法或要求基础模型预测单峰分布，在覆盖 - 效率权衡和模型无关、分布自由方面存在不足，需要改进。

Method: 将共形序贯分类问题表述为实例级的最小长度覆盖问题，开发滑动窗口算法求解，还提出长度正则化变体。

Result: 在四个不同领域的基准数据集上实验，所提方法比基线方法的预测效率平均提高15%。

Conclusion: 所提出的序贯CP方法能有效提高预测效率，解决了现有方法的局限性。

Abstract: Ordinal classification has been widely applied in many high-stakes applications, e.g., medical imaging and diagnosis, where reliable uncertainty quantification (UQ) is essential for decision making. Conformal prediction (CP) is a general UQ framework that provides statistically valid guarantees, which is especially useful in practice. However, prior ordinal CP methods mainly focus on heuristic algorithms or restrictively require the underlying model to predict a unimodal distribution over ordinal labels. Consequently, they provide limited insight into coverage-efficiency trade-offs, or a model-agnostic and distribution-free nature favored by CP methods. To this end, we fill this gap by propose an ordinal-CP method that is model-agnostic and provides instance-level optimal prediction intervals. Specifically, we formulate conformal ordinal classification as a minimum-length covering problem at the instance level. To solve this problem, we develop a sliding-window algorithm that is optimal on each calibration data, with only a linear time complexity in K, the number of label candidates. The local optimality per instance further also improves predictive efficiency in expectation. Moreover, we propose a length-regularized variant that shrinks prediction set size while preserving coverage. Experiments on four benchmark datasets from diverse domains are conducted to demonstrate the significantly improved predictive efficiency of the proposed methods over baselines (by 15% decrease on average over four datasets).

</details>


### [52] [Sex and age determination in European lobsters using AI-Enhanced bioacoustics](https://arxiv.org/abs/2511.16848)
*Feliciano Pedro Francisco Domingos,Isibor Kennedy Ihianle,Omprakash Kaiwartya,Ahmad Lotfi,Nicola Khan,Nicholas Beaudreau,Amaya Albalat,Pedro Machado*

Main category: cs.LG

TL;DR: 本文利用被动声学监测（PAM）和机器学习、深度学习模型，对欧洲龙虾的年龄和性别进行分类，取得高准确率，为龙虾保护和管理提供非侵入性方法。


<details>
  <summary>Details</summary>
Motivation: 监测水生物种尤其是龙虾存在挑战，了解龙虾的栖息地、福利、繁殖、性别和年龄对管理和保护至关重要，此前生物声学排放已用人工智能模型对多种水生物种进行分类，本文聚焦欧洲龙虾生物声学进行年龄和性别分类。

Method: 在苏格兰约翰黑文用混凝土水箱中的水听器收集数据集，探索深度学习模型（1D - CNN、1D - DCNN）和六种机器学习模型（SVM、k - NN、Naive Bayes、Random Forest、XGBoost、MLP），使用梅尔频率倒谱系数（MFCCs）作为特征。

Result: 年龄分类中多数模型准确率超97%（朴素贝叶斯为91.31%）；性别分类中除朴素贝叶斯外所有模型准确率超93.23%。

Conclusion: 监督式机器学习和深度学习有潜力从龙虾声音中提取与年龄和性别相关的特征，该研究为龙虾保护、检测和管理提供了有前景的非侵入性PAM方法，可用于水下物种的实际边缘计算应用。

Abstract: Monitoring aquatic species, especially elusive ones like lobsters, presents challenges. This study focuses on Homarus gammarus (European lobster), a key species for fisheries and aquaculture, and leverages non-invasive Passive Acoustic Monitoring (PAM). Understanding lobster habitats, welfare, reproduction, sex, and age is crucial for management and conservation. While bioacoustic emissions have classified various aquatic species using Artificial Intelligence (AI) models, this research specifically uses H. gammarus bioacoustics (buzzing/carapace vibrations) to classify lobsters by age (juvenile/adult) and sex (male/female).
  The dataset was collected at Johnshaven, Scotland, using hydrophones in concrete tanks. We explored the efficacy of Deep Learning (DL) models (1D-CNN, 1D-DCNN) and six Machine Learning (ML) models (SVM, k-NN, Naive Bayes, Random Forest, XGBoost, MLP). Mel-frequency cepstral coefficients (MFCCs) were used as features.
  For age classification (adult vs. juvenile), most models achieved over 97% accuracy (Naive Bayes: 91.31%). For sex classification, all models except Naive Bayes surpassed 93.23%. These strong results demonstrate the potential of supervised ML and DL to extract age- and sex-related features from lobster sounds. This research offers a promising non-invasive PAM approach for lobster conservation, detection, and management in aquaculture and fisheries, enabling real-world edge computing applications for underwater species.

</details>


### [53] [Better audio representations are more brain-like: linking model-brain alignment with performance in downstream auditory tasks](https://arxiv.org/abs/2511.16849)
*Leonardo Pepino,Pablo Riera,Juan Kamienkowski,Luciana Ferrer*

Main category: cs.LG

TL;DR: 本文量化36种音频模型内部表征与大脑活动的对齐度，发现任务表现与大脑表征对齐存在强正相关，且预训练中大脑相似性会逐渐增加。


<details>
  <summary>Details</summary>
Motivation: 探究提升人工神经网络任务表现是否会使其内部表征更接近大脑信号。

Method: 采用体素和成分回归、表征相似性分析（RSA），并在HEAREval基准的6个听觉任务中评估模型。

Result: 近期自监督音频模型更能预测听觉皮层活动，模型整体任务表现与大脑表征对齐有强正皮尔逊相关性，EnCodecMAE预训练中大脑相似性逐渐增加。

Conclusion: 类脑表征可作为从自然音频数据学习重建缺失信息的意外副产品出现。

Abstract: Artificial neural networks (ANNs) are increasingly powerful models of brain computation, yet it remains unclear whether improving their task performance also makes their internal representations more similar to brain signals. To address this question in the auditory domain, we quantified the alignment between the internal representations of 36 different audio models and brain activity from two independent fMRI datasets. Using voxel-wise and component-wise regression, and representation similarity analysis (RSA), we found that recent self-supervised audio models with strong performance in diverse downstream tasks are better predictors of auditory cortex activity than older and more specialized models. To assess the quality of the audio representations, we evaluated these models in 6 auditory tasks from the HEAREval benchmark, spanning music, speech, and environmental sounds. This revealed strong positive Pearson correlations ($r>0.7$) between a model's overall task performance and its alignment with brain representations. Finally, we analyzed the evolution of the similarity between audio and brain representations during the pretraining of EnCodecMAE. We discovered that brain similarity increases progressively and emerges early during pretraining, despite the model not being explicitly optimized for this objective. This suggests that brain-like representations can be an emergent byproduct of learning to reconstruct missing information from naturalistic audio data.

</details>


### [54] [The use of vocal biomarkers in the detection of Parkinson's disease: a robust statistical performance comparison of classic machine learning models](https://arxiv.org/abs/2511.16856)
*Katia Pires Nascimento do Sacramento,Elliot Q. C. Garcia,Nicéias Silva Vilela,Vinicius P. Sacramento,Tiago A. E. Ferreira*

Main category: cs.LG

TL;DR: 研究对比DNN和传统ML方法，用语音生物标志物区分帕金森患者和健康人，DNN表现更优，证实其用于神经退行性疾病早期检测的潜力。


<details>
  <summary>Details</summary>
Motivation: 帕金森病常伴有早期发声障碍，使用语音生物标志物进行早期诊断具有无创、低成本和易获取的优点，需评估DNN在区分患者和健康人方面的有效性。

Method: 使用两个公开语音数据集，提取MFCC特征，用1000次独立随机执行的验证策略评估模型鲁棒性，用分类统计评估性能，用非参数检验验证模型分类效果差异。

Result: DNN在两个数据集上平均准确率分别为98.65%和92.11%，优于传统ML模型，与相关研究相比也有竞争力。

Conclusion: 证实了DNN的效率，强调其利用语音生物标志物为神经退行性疾病早期检测提供更高准确性和可靠性的潜力。

Abstract: Parkinson's disease (PD) is a progressive neurodegenerative disorder that, in addition to directly impairing functional mobility, is frequently associated with vocal impairments such as hypophonia and dysarthria, which typically manifest in the early stages. The use of vocal biomarkers to support the early diagnosis of PD presents a non-invasive, low-cost, and accessible alternative in clinical settings. Thus, the objective of this cross-sectional study was to consistently evaluate the effectiveness of a Deep Neural Network (DNN) in distinguishing individuals with Parkinson's disease from healthy controls, in comparison with traditional Machine Learning (ML) methods, using vocal biomarkers. Two publicly available voice datasets were used. Mel-frequency cepstral coefficients (MFCCs) were extracted from the samples, and model robustness was assessed using a validation strategy with 1000 independent random executions. Performance was evaluated using classification statistics. Since normality assumptions were not satisfied, non-parametric tests (Kruskal-Wallis and Bonferroni post-hoc tests) were applied to verify whether the tested classification models were similar or different in the classification of PD. With an average accuracy of $98.65\%$ and $92.11\%$ on the Italian Voice dataset and Parkinson's Telemonitoring dataset, respectively, the DNN demonstrated superior performance and efficiency compared to traditional ML models, while also achieving competitive results when benchmarked against relevant studies. Overall, this study confirms the efficiency of DNNs and emphasizes their potential to provide greater accuracy and reliability for the early detection of neurodegenerative diseases using voice-based biomarkers.

</details>


### [55] [Topologic Attention Networks: Attending to Direct and Indirect Neighbors through Gaussian Belief Propagation](https://arxiv.org/abs/2511.16871)
*Marshall Rosenhoover,Huaming Zhang*

Main category: cs.LG

TL;DR: 提出拓扑注意力网络解决图神经网络长程依赖建模问题，性能达最优。


<details>
  <summary>Details</summary>
Motivation: 图神经网络依赖局部消息传递，现有扩展方法计算成本高、可扩展性有限。

Method: 提出拓扑注意力网络，采用拓扑注意力机制，从图的信息传播中学习信息流动。

Result: 在所有测量的基线模型上达到了最优性能。

Conclusion: 拓扑注意力网络能有效解决图神经网络长程依赖问题，且性能优越。

Abstract: Graph Neural Networks rely on local message passing, which limits their ability to model long-range dependencies in graphs. Existing approaches extend this range through continuous-time dynamics or dense self-attention, but both suffer from high computational cost and limited scalability. We propose Topologic Attention Networks, a new framework that applies topologic attention, a probabilistic mechanism that learns how information should flow through both direct and indirect connections in a graph. Unlike conventional attention that depends on explicit pairwise interactions, topologic attention emerges from the learned information propagation of the graph, enabling unified reasoning over local and global relationships. This method achieves provides state-of-the-art performance across all measured baseline models. Our implementation is available at https://github.com/Marshall-Rosenhoover/Topologic-Attention-Networks.

</details>


### [56] [PersonalizedRouter: Personalized LLM Routing via Graph-based User Preference Modeling](https://arxiv.org/abs/2511.16883)
*Zhongjie Dai,Tao Feng,Jiaxuan You*

Main category: cs.LG

TL;DR: 论文提出PersonalizedRouter框架用于个性化大模型选择，通过构建异构图建模用户偏好，设计评估策略和基准测试，实验表明其性能显著优于现有方法，且有较强少样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有大模型选择方法通常优化单一固定目标，无法从交互数据学习用户偏好，在众多有不同能力和响应风格的大模型中，难以满足用户多样化偏好。

Method: 提出基于图的PersonalizedRouter框架，将交互数据转换为异构图，设计多成本效率模拟策略和大模型评判策略评估跨用户适应性，构建PersonaRoute - Bench基准测试。

Result: PersonalizedRouter显著优于现有大模型选择方法，在两种模拟策略下分别大幅领先15.38%和9.83%，在1000用户基准上分别领先16.19%和59.69%，且效率更高，有较强少样本泛化能力。

Conclusion: PersonalizedRouter在大模型选择中具有显著优势，能有效解决现有方法的局限性，实现个性化选择。

Abstract: The growing number of Large Language Models (LLMs) with diverse capabilities and response styles provides users with a wider range of choices, which presents challenges in selecting appropriate LLMs, as user preferences vary in terms of performance, cost, and response style. Current LLM selection methods typically optimize for a single fixed objective, such as performance, cost, or a trade-off between them, and fail to learn individual user preferences from interaction data. To address these limitations, we propose PersonalizedRouter, a graph-based framework that models diverse user profiles and performs personalized LLM selection by leveraging interaction data that includes task context, queries, candidate LLMs, and user decisions. To capture contextual information between user queries and optimal LLMs, PersonalizedRouter converts the interaction data into a heterogeneous graph, where the relationships between different types of nodes are represented by edges. To evaluate adaptability across users, we design two strategies: the multi-cost-efficiency simulation strategy and the LLM-as-a-Judge strategy. In addition, we construct PersonaRoute-Bench, a large-scale benchmark with 1,000 simulated users and 10 LLMs. Experimental results show that PersonalizedRouter significantly outperforms existing LLM selection methods and surpasses the strongest methods by a large margin of 15.38% and 9.83% under two simulation strategies. On the PersonaRoute-Bench with 1,000 users, it further surpasses the best methods by 16.19% and 59.69% while maintaining higher efficiency. Moreover, PersonalizedRouter demonstrates strong few-shot generalization, achieving 64.81% and 85.80% of the fully trained model's performance when adapting to new users and new LLMs.

</details>


### [57] [Predicting Talent Breakout Rate using Twitter and TV data](https://arxiv.org/abs/2511.16905)
*Bilguun Batsaikhan,Hiroyuki Fukuda*

Main category: cs.LG

TL;DR: 论文定义人才突破概念，结合Twitter和TV数据预测日本人才成名前情况，对比传统、神经网络和集成学习方法，发现集成学习在标准回归指标上表现好，神经网络在人才突破预测的精确率和召回率上更优。


<details>
  <summary>Details</summary>
Motivation: 在广告领域实现对潜力人才的早期发现，确定结合Twitter和TV数据对预测社交数据随时间变化的有效性。

Method: 实验对比传统、神经网络和集成学习方法。

Result: 集成学习方法在标准回归指标上优于传统和神经网络模型；在人才突破预测的精确率和召回率方面，神经网络模型表现更好。

Conclusion: 利用人才突破概念能评估模型真实预测能力，神经网络在这方面有优势。

Abstract: Early detection of rising talents is of paramount importance in the field of advertising. In this paper, we define a concept of talent breakout and propose a method to detect Japanese talents before their rise to stardom. The main focus of the study is to determine the effectiveness of combining Twitter and TV data on predicting time-dependent changes in social data. Although traditional time-series models are known to be robust in many applications, the success of neural network models in various fields (e.g.\ Natural Language Processing, Computer Vision, Reinforcement Learning) continues to spark an interest in the time-series community to apply new techniques in practice. Therefore, in order to find the best modeling approach, we have experimented with traditional, neural network and ensemble learning methods. We observe that ensemble learning methods outperform traditional and neural network models based on standard regression metrics. However, by utilizing the concept of talent breakout, we are able to assess the true forecasting ability of the models, where neural networks outperform traditional and ensemble learning methods in terms of precision and recall.

</details>


### [58] [PepEVOLVE: Position-Aware Dynamic Peptide Optimization via Group-Relative Advantage](https://arxiv.org/abs/2511.16912)
*Trieu Nguyen,Hao-Wei Pang,Shasha Feng*

Main category: cs.LG

TL;DR: 介绍PepEVOLVE框架用于肽先导优化，在模拟评估和基准测试中表现优于PepINVENT，提供实用优化路径。


<details>
  <summary>Details</summary>
Motivation: 大环肽先导优化慢且具挑战性，现有生成方法有局限性，如需预指定可变位置、依赖静态预训练和优化算法。

Method: PepEVOLVE采用动态掩码和CHUCKLES移位增强预训练，用上下文无关多臂老虎机路由器发现高回报残基，结合新型进化优化算法与群体相对优势稳定强化更新。

Result: 路由器策略可靠学习并聚焦影响肽性质的化学意义位点；在Rev结合大环基准测试中，PepEVOLVE得分更高、最佳候选得分更高且收敛步数更少。

Conclusion: PepEVOLVE在未知最佳编辑位点时提供实用、可重复的肽先导优化路径，提高多目标探索效率和设计质量。

Abstract: Macrocyclic peptides are an emerging modality that combines biologics-like affinity with small-molecule-like developability, but their vast combinatorial space and multi-parameter objectives make lead optimization slow and challenging. Prior generative approaches such as PepINVENT require chemists to pre-specify mutable positions for optimization, choices that are not always known a priori, and rely on static pretraining and optimization algorithms that limit the model's ability to generalize and effectively optimize peptide sequences. We introduce PepEVOLVE, a position-aware, dynamic framework that learns both where to edit and how to dynamically optimize peptides for multi-objective improvement. PepEVOLVE (i) augments pretraining with dynamic masking and CHUCKLES shifting to improve generalization, (ii) uses a context-free multi-armed bandit router that discovers high-reward residues, and (iii) couples a novel evolving optimization algorithm with group-relative advantage to stabilize reinforcement updates. During in silico evaluations, the router policy reliably learns and concentrates probability on chemically meaningful sites that influence the peptide's properties. On a therapeutically motivated Rev-binding macrocycle benchmark, PepEVOLVE outperformed PepINVENT by reaching higher mean scores (approximately 0.8 vs. 0.6), achieving best candidates with a score of 0.95 (vs. 0.87), and converging in fewer steps under the task of optimizing permeability and lipophilicity with structural constraints. Overall, PepEVOLVE offers a practical, reproducible path to peptide lead optimization when optimal edit sites are unknown, enabling more efficient exploration and improving design quality across multiple objectives.

</details>


### [59] [A Hybrid Computational Intelligence Framework for scRNA-seq Imputation: Integrating scRecover and Random Forests](https://arxiv.org/abs/2511.16923)
*Ali Anaissi,Deshao Liu,Yuanzhe Jia,Weidong Huang,Widad Alyassine,Junaid Akram*

Main category: cs.LG

TL;DR: 提出SCR - MF工作流用于解决scRNA - seq的dropout问题，性能好且平衡了准确性与计算效率，适合中规模数据集。


<details>
  <summary>Details</summary>
Motivation: scRNA - seq存在普遍的dropout事件，会掩盖生物信号。

Method: 提出SCR - MF模块化两阶段工作流，结合scRecover进行dropout检测和missForest进行非参数插补。

Result: 在公共和模拟数据集上，SCR - MF性能稳健且可解释，多数情况下与现有插补方法相当或更优，保留了生物保真度和透明度。

Conclusion: SCR - MF在准确性和计算效率之间取得了有竞争力的平衡，适合中规模单细胞数据集。

Abstract: Single-cell RNA sequencing (scRNA-seq) enables transcriptomic profiling at cellular resolution but suffers from pervasive dropout events that obscure biological signals. We present SCR-MF, a modular two-stage workflow that combines principled dropout detection using scRecover with robust non-parametric imputation via missForest. Across public and simulated datasets, SCR-MF achieves robust and interpretable performance comparable to or exceeding existing imputation methods in most cases, while preserving biological fidelity and transparency. Runtime analysis demonstrates that SCR-MF provides a competitive balance between accuracy and computational efficiency, making it suitable for mid-scale single-cell datasets.

</details>


### [60] [A novel approach to classification of ECG arrhythmia types with latent ODEs](https://arxiv.org/abs/2511.16933)
*Angelina Yan,Matt L. Sampson,Peter Melchior*

Main category: cs.LG

TL;DR: 提出端到端分类管道解决心电图检测问题，性能在不同频率下稳定，利于长期心脏健康监测。


<details>
  <summary>Details</summary>
Motivation: 12导联高采样频率心电图易漏检间歇性事件，可穿戴心电图因电池限制采样频率不规则，形态分析困难。

Method: 训练潜在ODE对连续心电图波形建模，从高频单通道信号创建特征向量，通过降采样构建三个潜在向量，用梯度提升树分类。

Result: 在360Hz、90Hz和45Hz下，宏平均AUC - ROC值分别为0.984、0.978和0.976，性能下降极小。

Conclusion: 该方法可避开信号保真度和电池寿命的权衡，利于小型可穿戴设备长期监测心脏健康。

Abstract: 12-lead ECGs with high sampling frequency are the clinical gold standard for arrhythmia detection, but their short-term, spot-check nature often misses intermittent events. Wearable ECGs enable long-term monitoring but suffer from irregular, lower sampling frequencies due to battery constraints, making morphology analysis challenging. We present an end-to-end classification pipeline to address these issues. We train a latent ODE to model continuous ECG waveforms and create robust feature vectors from high-frequency single-channel signals. We construct three latent vectors per waveform via downsampling the initial 360 Hz ECG to 90 Hz and 45 Hz. We then use a gradient boosted tree to classify these vectors and test robustness across frequencies. Performance shows minimal degradation, with macro-averaged AUC-ROC values of 0.984, 0.978, and 0.976 at 360 Hz, 90 Hz, and 45 Hz, respectively, suggesting a way to sidestep the trade-off between signal fidelity and battery life. This enables smaller wearables, promoting long-term monitoring of cardiac health.

</details>


### [61] [ToC: Tree-of-Claims Search with Multi-Agent Language Models](https://arxiv.org/abs/2511.16972)
*Shuyang Yu,Jianan Liang,Hui Hu*

Main category: cs.LG

TL;DR: 提出Tree of Claims (ToC)框架优化专利权利要求，结合MCTS与多智能体系统，实验显示性能优于标准大模型。


<details>
  <summary>Details</summary>
Motivation: 手动起草专利权利要求劳动密集、成本高且不一致，传统大模型缺乏精确权利要求细化所需的结构化迭代推理。

Method: 将权利要求编辑定义为引导搜索问题，集成蒙特卡罗树搜索（MCTS）与协作多智能体系统，由EditorAgent提出编辑建议，ExaminerAgent进行评估，通过多目标奖励函数联合优化。

Result: 在1145条权利要求基准测试中，ToC在零样本和少样本场景下显著优于标准大模型，平均综合得分提高8%，某些情况下达9%。

Conclusion: ToC建立了透明、可控、可解释的方法，有效结合大模型推理能力与MCTS规划进行专利权利要求优化。

Abstract: Optimizing patent claims is a critical yet challenging task, demanding careful balance between maximizing novelty and preserving legal scope. Manual claim drafting is labor-intensive, costly, and inherently inconsistent, while conventional Large Language Models (LLMs) often lack the structured, iterative reasoning essential for precise claim refinement. To address these challenges, we introduce Tree of Claims (ToC), an innovative framework that redefines claim editing as a guided search problem. ToC synergistically integrates Monte Carlo Tree Search (MCTS) with a collaborative multi-agent system, comprising an LLM-based EditorAgent that proposes contextually grounded edits, and an ExaminerAgent that mimics patent examiner critiques through structured, chain-of-thought analyses of novelty and prior art disclosure. Driven by a carefully designed multi-objective reward function, ToC jointly optimizes novelty, scope retention, and semantic coherence. Experimental evaluation on a benchmark of 1145 claims demonstrates that ToC significantly outperforms standard LLMs in zero-shot and few-shot scenarios, achieving an average composite score improvement of 8\%, and up to 9\% in certain cases. Extensive experiments, including detailed ablation studies, validate ToC's efficacy in generating superior, legally robust claim revisions. Overall, ToC establishes a transparent, controllable, and interpretable methodology that effectively bridges advanced LLM reasoning capabilities with strategic MCTS planning for structured patent claim optimization.The source code is available at https://github.com/ysy2003/ToC.

</details>


### [62] [FIRM: Federated In-client Regularized Multi-objective Alignment for Large Language Models](https://arxiv.org/abs/2511.16992)
*Fatemeh,Nourzad,Amirhossein Roknilamouki,Eylem Ekici,Jia,Liu,Ness B. Shroff*

Main category: cs.LG

TL;DR: 提出新算法FIRM用于大语言模型与人类价值观对齐，解决联邦学习通信瓶颈，有理论收敛证明和良好实证效果。


<details>
  <summary>Details</summary>
Motivation: 大语言模型与人类价值观对齐训练计算密集、集中式训练有隐私问题，现有联邦多目标优化方法有通信瓶颈。

Method: 提出FIRM算法，各客户端本地解决正则化多目标优化问题，通过客户端内正则化减少客户端分歧漂移，仅传输一组参数。

Result: 证明算法收敛到帕累托驻点，有有限时间收敛保证；实证显示训练动态更平稳、客户端分歧漂移减少、奖励权衡更优，能根据偏好调整目标权衡。

Conclusion: FIRM算法在大语言模型与人类价值观对齐的联邦多目标优化中有效，解决了通信瓶颈问题。

Abstract: Aligning Large Language Models (LLMs) with human values often involves balancing multiple, conflicting objectives such as helpfulness and harmlessness. Training these models is computationally intensive, and centralizing the process raises significant data privacy concerns. Federated Learning (FL) offers a compelling alternative, but existing Federated Multi-Objective Optimization (FMOO) methods face severe communication bottlenecks as their reliance on transmitting multiple gradients to a server is unscalable for large models. We introduce FIRM (Federated In-client Regularized Multi-objective alignment), a novel algorithm that achieves both client disagreement drift mitigation and communication efficiency. In FIRM, each client locally solves a regularized multi-objective optimization problem. By directly mitigating client disagreement drift through in-client regularization, our method eliminates the need for the multi-gradient transmissions common in prior works. Consequently, clients need only to transmit a single set of adapted parameters, maintaining high communication efficiency. We prove that our algorithm converges to Pareto-stationary points and, to our knowledge, provide the first finite-time convergence guarantees for this federated multi-objective alignment setting. Empirically, we show that FIRM leads to smoother training dynamics, reduced client disagreement drift, and improved reward trade-offs compared to baselines. We further propose a method to incorporate a preference over the objectives and report empirical Pareto plots, demonstrating that FIRM can smoothly adapt trade-offs between objectives in response to specified preferences.

</details>


### [63] [Mask the Redundancy: Evolving Masking Representation Learning for Multivariate Time-Series Clustering](https://arxiv.org/abs/2511.17008)
*Zexi Tan,Xiaopeng Luo,Yunlin Liu,Yiqun Zhang*

Main category: cs.LG

TL;DR: 本文提出Evolving - masked MTS Clustering (EMTC)方法用于多元时间序列（MTS）聚类，在15个真实基准数据集上实验表明其优于8种SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 现有MTS聚类因时间序列冗余影响表示学习，且多数掩码策略与学习过程隔离，无法动态适应关键时间戳重要性，导致性能瓶颈。

Method: 提出EMTC方法，模型架构包含重要性感知变量掩码（IVM）和多内生视图（MEV）表示学习模块，IVM引导学习更具判别性表示，MEV重建和对比学习增强泛化。

Result: 在15个真实基准数据集上与8种SOTA方法对比，EMTC比最强基线平均提升4.85%。

Conclusion: EMTC方法在MTS聚类中表现优越，能有效解决现有问题。

Abstract: Multivariate Time-Series (MTS) clustering discovers intrinsic grouping patterns of temporal data samples. Although time-series provide rich discriminative information, they also contain substantial redundancy, such as steady-state machine operation records and zero-output periods of solar power generation. Such redundancy diminishes the attention given to discriminative timestamps in representation learning, thus leading to performance bottlenecks in MTS clustering. Masking has been widely adopted to enhance the MTS representation, where temporal reconstruction tasks are designed to capture critical information from MTS. However, most existing masking strategies appear to be standalone preprocessing steps, isolated from the learning process, which hinders dynamic adaptation to the importance of clustering-critical timestamps. Accordingly, this paper proposes the Evolving-masked MTS Clustering (EMTC) method, with its model architecture composed of Importance-aware Variate-wise Masking (IVM) and Multi-Endogenous Views (MEV) representation learning modules. IVM adaptively guides the model in learning more discriminative representations for clustering, while the MEV-based reconstruction and contrastive learning pathways enhance the generalization. That is, the MEV reconstruction facilitates multi-perspective complementary to prevent the masking from premature convergence, and the clustering-guided contrastive learning facilitates the joint optimization of representation and clustering. Extensive experiments on 15 real benchmark datasets demonstrate the superiority of EMTC in comparison with eight SOTA methods, where the EMTC achieves an average improvement of 4.85% over the strongest baselines.

</details>


### [64] [Energy Scaling Laws for Diffusion Models: Quantifying Compute and Carbon Emissions in Image Generation](https://arxiv.org/abs/2511.17031)
*Aniketh Iyengar,Jiaqi Han,Boris Ruf,Vincent Grari,Marcin Detyniecki,Stefano Ermon*

Main category: cs.LG

TL;DR: 提出用Kaplan缩放定律预测扩散模型GPU能耗，实验表明该定律预测精度高、跨架构泛化性强，为可持续AI部署和碳足迹估算提供基础。


<details>
  <summary>Details</summary>
Motivation: 扩散模型计算需求增长引发能耗和环境问题担忧，现有能耗优化方法缺乏预测不同模型配置和硬件设置下能耗的原则性方法。

Method: 采用Kaplan缩放定律，基于计算复杂度（FLOPs）预测扩散模型GPU能耗，将推理分解为文本编码、迭代去噪和解码组件。

Result: 能耗缩放定律在单个架构内预测精度高（R平方>0.9），跨架构泛化性强，能为未见的模型 - 硬件组合可靠估算能耗。

Conclusion: 验证了扩散推理的计算密集特性，为可持续AI部署规划和碳足迹估算提供基础。

Abstract: The rapidly growing computational demands of diffusion models for image generation have raised significant concerns about energy consumption and environmental impact. While existing approaches to energy optimization focus on architectural improvements or hardware acceleration, there is a lack of principled methods to predict energy consumption across different model configurations and hardware setups. We propose an adaptation of Kaplan scaling laws to predict GPU energy consumption for diffusion models based on computational complexity (FLOPs). Our approach decomposes diffusion model inference into text encoding, iterative denoising, and decoding components, with the hypothesis that denoising operations dominate energy consumption due to their repeated execution across multiple inference steps. We conduct comprehensive experiments across four state-of-the-art diffusion models (Stable Diffusion 2, Stable Diffusion 3.5, Flux, and Qwen) on three GPU architectures (NVIDIA A100, A4000, A6000), spanning various inference configurations including resolution (256x256 to 1024x1024), precision (fp16/fp32), step counts (10-50), and classifier-free guidance settings. Our energy scaling law achieves high predictive accuracy within individual architectures (R-squared > 0.9) and exhibits strong cross-architecture generalization, maintaining high rank correlations across models and enabling reliable energy estimation for unseen model-hardware combinations. These results validate the compute-bound nature of diffusion inference and provide a foundation for sustainable AI deployment planning and carbon footprint estimation.

</details>


### [65] [Step-E: A Differentiable Data Cleaning Framework for Robust Learning with Noisy Labels](https://arxiv.org/abs/2511.17040)
*Wenzhang Du*

Main category: cs.LG

TL;DR: 提出Step - E框架将样本选择和模型学习集成到一个优化过程，在含噪声数据集上提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 野外收集的训练数据含噪声标签和异常值，现有两阶段数据清洗管道存在不足。

Method: Step - E框架在每个训练轮次按损失对样本排序，短暂预热后逐渐增加排除的高损失样本比例。

Result: 在CIFAR - 100N上，ResNet - 18模型测试准确率从43.3%提升到50.4%；在CIFAR - 10N (aggre)上，准确率从83.9%提升到85.3%。

Conclusion: Step - E框架能有效处理含噪声数据，提升模型性能，且训练时间开销适中。

Abstract: Training data collected in the wild often contain noisy labels and outliers that substantially degrade the performance and reliability of deep neural networks. While data cleaning is commonly applied as a separate preprocessing stage, such two-stage pipelines neither fully exploit feedback from the downstream model nor adapt to unknown noise patterns. We propose Step-E, a simple framework that integrates sample selection and model learning into a single optimization process. At each epoch, Step-E ranks samples by loss and gradually increases the fraction of high-loss examples that are excluded from gradient updates after a brief warm-up stage, yielding an online curriculum that focuses on easy and consistent examples and eventually ignores persistent outliers. On CIFAR-100N, Step-E improves the test accuracy of a ResNet-18 model from 43.3% (+/- 0.7%) to 50.4% (+/- 0.9%), clearly outperforming loss truncation, self-paced learning, and one-shot filtering while approaching the clean-label oracle at 60.5% (+/- 0.2%). On CIFAR-10N (aggre), Step-E also improves over the noisy baseline (85.3% vs. 83.9%) and nearly matches the clean-label oracle (85.9%), with only moderate training-time overhead.

</details>


### [66] [Hash Collisions in Molecular Fingerprints: Effects on Property Prediction and Bayesian Optimization](https://arxiv.org/abs/2511.17078)
*Walter Virany,Austin Tripp*

Main category: cs.LG

TL;DR: 研究精确指纹在分子属性预测和贝叶斯优化中是否比标准压缩指纹更准确，发现精确指纹在预测精度上有小提升，但对贝叶斯优化性能无显著改善。


<details>
  <summary>Details</summary>
Motivation: 解决哈希冲突导致分子相似性计算高估问题，探究精确指纹在分子属性预测和贝叶斯优化中是否能提高准确性。

Method: 在分子属性预测和贝叶斯优化中，以高斯过程为预测模型，对比精确指纹和标准压缩指纹。

Result: 在DOCKSTRING数据集的五个分子属性预测基准上，精确指纹在预测精度上有小而稳定的提升，但在贝叶斯优化性能上无显著改善。

Conclusion: 精确指纹在分子属性预测中有一定优势，但对贝叶斯优化性能提升不明显。

Abstract: Molecular fingerprinting methods use hash functions to create fixed-length vector representations of molecules. However, hash collisions cause distinct substructures to be represented with the same feature, leading to overestimates in molecular similarity calculations. We investigate whether using exact fingerprints improves accuracy compared to standard compressed fingerprints in molecular property prediction and Bayesian optimization where the underlying predictive model is a Gaussian process. We find that using exact fingerprints yields a small yet consistent improvement in predictive accuracy on five molecular property prediction benchmarks from the DOCKSTRING dataset. However, these gains did not translate to significant improvements in Bayesian optimization performance.

</details>


### [67] [Why Do Language Model Agents Whistleblow?](https://arxiv.org/abs/2511.17085)
*Kushal Agrawal,Frank Xiao,Guido Bergman,Asa Cooper Stickland*

Main category: cs.LG

TL;DR: 研究大语言模型（LLMs）作为工具使用代理时的举报行为，引入评估套件，发现举报频率因模型家族而异，任务复杂度、系统提示及工具和工作流会影响举报率，且数据集评估意识低。


<details>
  <summary>Details</summary>
Motivation: 大语言模型作为工具使用代理时，其对齐训练有新表现，且存在模型使用工具违背用户利益或指令的情况，研究模型举报疑似不当行为。

Method: 引入多样化且现实的阶段性不当行为场景评估套件来评估模型。

Result: 1. 不同模型家族举报频率差异大；2. 任务复杂度增加降低举报倾向；3. 系统提示道德行事提高举报率；4. 提供更多工具和详细工作流降低举报率；数据集评估意识低于以往工作。

Conclusion: 评估套件可用于评估大语言模型的举报行为，且数据集具有一定鲁棒性。

Abstract: The deployment of Large Language Models (LLMs) as tool-using agents causes their alignment training to manifest in new ways. Recent work finds that language models can use tools in ways that contradict the interests or explicit instructions of the user. We study LLM whistleblowing: a subset of this behavior where models disclose suspected misconduct to parties beyond the dialog boundary (e.g., regulatory agencies) without user instruction or knowledge. We introduce an evaluation suite of diverse and realistic staged misconduct scenarios to assess agents for this behavior. Across models and settings, we find that: (1) the frequency of whistleblowing varies widely across model families, (2) increasing the complexity of the task the agent is instructed to complete lowers whistleblowing tendencies, (3) nudging the agent in the system prompt to act morally substantially raises whistleblowing rates, and (4) giving the model more obvious avenues for non-whistleblowing behavior, by providing more tools and a detailed workflow to follow, decreases whistleblowing rates. Additionally, we verify the robustness of our dataset by testing for model evaluation awareness, and find that both black-box methods and probes on model activations show lower evaluation awareness in our settings than in comparable previous work.

</details>


### [68] [Geometric-Disentangelment Unlearning](https://arxiv.org/abs/2511.17100)
*Duo Zhou,Yuji Zhang,Tianxin Wei,Ruizhong Qiu,Ke Yang,Xiao Lin,Cheng Qian,Jingrui He,Hanghang Tong,Heng Ji,Huan Zhang*

Main category: cs.LG

TL;DR: 文章针对机器学习遗忘问题，从一阶分析出发提出几何解缠遗忘方法（GU），可减轻副作用并在多个基准测试中提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习遗忘方法在有效遗忘和保留集知识保存间存在权衡，且缺乏对遗忘更新损害保留知识的正式分析及去除副作用的理论保证。

Method: 从一阶分析保留损失在模型训练中参数小更新下的局部变化，提出GU方法，将遗忘梯度更新分解为切向和法向分量，只执行法向分量。

Result: GU方法是即插即用的，能附加到现有基于梯度的遗忘程序中减轻副作用，在三个基准测试中对各种方法都有持续改进。

Conclusion: 所提出的GU方法能有效解决现有机器学习遗忘方法的问题，在多个基准测试中表现良好。

Abstract: Machine unlearning, the removal of a training subset's influence from a deployed model, is critical for privacy preservation and model reliability, yet gradient ascent on forget samples often harms retained knowledge. Existing approaches face a persistent tradeoff between effective forgetting and preservation on the retain set. While previous methods provide useful heuristics, they often lack a formal analysis on how exactly forgetting updates harm retained knowledge, and whether the side effects can be removed with theoretical guarantees. To explore a theoretically sound and simple solution, we start from the first principle on how performance on the retain set is actually affected: a first-order analysis of the local change of the retain loss under small parameter updates during model training. We start from a crisp equivalence: the retain loss is unchanged to first order iff the update direction is orthogonal to the subspace spanned by retain gradients ("retain-invariant"). This identifies the entangled component as the tangential part of forget update within the retain-gradient subspace, and characterizes disentanglement as orthogonality. Guided by this, we propose the Geometric-disentanglement Unlearning (GU) that decomposes any candidate forget gradient update into tangential and normal components to retain space and executes only the normal component. Under a standard trust-region budget, the projected direction aligned with the raw forget gradient is optimal among all first-order retain-invariant moves, and we also derive the optimal projected direction for joint forget-retain updating objectives. Our method is plug-and-play and can be attached to existing gradient-based unlearning procedures to mitigate side effects. GU achieves consistent improvement on various methods across three benchmarks TOFU, MUSE, and WMDP.

</details>


### [69] [Four decades of circumpolar super-resolved satellite land surface temperature data](https://arxiv.org/abs/2511.17134)
*Sonia Dupuis,Nando Metzger,Konrad Schindler,Frank Göttsche,Stefan Wunderle*

Main category: cs.LG

TL;DR: 本文提出新的42年泛北极地表温度数据集，通过基于深度各向异性扩散模型的超分辨率算法将AVHRR GAC数据降尺度至1公里，可用于多项研究和气候监测。


<details>
  <summary>Details</summary>
Motivation: AVHRR全球区域覆盖数据空间分辨率粗，限制了其在分析北极精细尺度多年冻土动态和其他地表过程的效用，需高分辨率数据。

Method: 使用基于深度各向异性扩散模型的超分辨率算法，基于MODIS地表温度数据训练模型，结合高分辨率土地覆盖、数字高程和植被高度图。

Result: 得到了整个泛北极地区40多年来每日两次、1公里分辨率的地表温度观测数据集。

Conclusion: 该数据集能改善多年冻土建模等，支持前MODIS时代气候监测，为未来卫星任务提供框架。

Abstract: Land surface temperature (LST) is an essential climate variable (ECV) crucial for understanding land-atmosphere energy exchange and monitoring climate change, especially in the rapidly warming Arctic. Long-term satellite-based LST records, such as those derived from the Advanced Very High Resolution Radiometer (AVHRR), are essential for detecting climate trends. However, the coarse spatial resolution of AVHRR's global area coverage (GAC) data limit their utility for analyzing fine-scale permafrost dynamics and other surface processes in the Arctic. This paper presents a new 42 years pan-Arctic LST dataset, downscaled from AVHRR GAC to 1 km with a super-resolution algorithm based on a deep anisotropic diffusion model. The model is trained on MODIS LST data, using coarsened inputs and native-resolution outputs, guided by high-resolution land cover, digital elevation, and vegetation height maps. The resulting dataset provides twice-daily, 1 km LST observations for the entire pan-Arctic region over four decades. This enhanced dataset enables improved modelling of permafrost, reconstruction of near-surface air temperature, and assessment of surface mass balance of the Greenland Ice Sheet. Additionally, it supports climate monitoring efforts in the pre-MODIS era and offers a framework adaptable to future satellite missions for thermal infrared observation and climate data record continuity.

</details>


### [70] [Reconstruction of Surface EMG Signal using IMU data for Upper Limb Actions](https://arxiv.org/abs/2511.17200)
*Shubhranil Basak,Mada Hemanth,Madhav Rao*

Main category: cs.LG

TL;DR: 本文用深度学习方法从6轴IMU数据合成归一化sEMG信号，模型能预测肌肉激活情况，证明该方法用于肌肉意图检测可行。


<details>
  <summary>Details</summary>
Motivation: sEMG信号有噪声且采集困难，IMU是运动捕捉系统的可靠替代，研究从IMU数据合成sEMG信号。

Method: 收集不同手臂动作的同步sEMG和IMU数据，用基于扩张因果卷积的Sliding - Window - Wave - Net模型将IMU数据映射到sEMG信号。

Result: 模型成功预测肌肉激活的时间和大致形状，但峰值幅度常被低估。

Conclusion: 该方法具有较高的时间保真度，可用于假肢和康复生物反馈等应用中的肌肉意图检测。

Abstract: Surface Electromyography (sEMG) provides vital insights into muscle function, but it can be noisy and challenging to acquire. Inertial Measurement Units (IMUs) provide a robust and wearable alternative to motion capture systems. This paper investigates the synthesis of normalized sEMG signals from 6-axis IMU data using a deep learning approach. We collected simultaneous sEMG and IMU data sampled at 1~KHz for various arm movements. A Sliding-Window-Wave-Net model, based on dilated causal convolutions, was trained to map the IMU data to the sEMG signal. The results show that the model successfully predicts the timing and general shape of muscle activations. Although peak amplitudes were often underestimated, the high temporal fidelity demonstrates the feasibility of using this method for muscle intent detection in applications such as prosthetics and rehabilitation biofeedback.

</details>


### [71] [DelTriC: A Novel Clustering Method with Accurate Outlier](https://arxiv.org/abs/2511.17219)
*Tomas Javurek,Michal Gregor,Sebastian Kula,Marian Simko*

Main category: cs.LG

TL;DR: 介绍聚类算法DelTriC，它结合投影、三角剖分和反投影机制，在性能、扩展性和异常检测上有优势。


<details>
  <summary>Details</summary>
Motivation: 提出一种更优的聚类算法，解决传统聚类算法在某些场景下的不足。

Method: 集成基于PCA/UMAP的投影、Delaunay三角剖分和反投影机制，先在低维代理空间进行三角剖分，再反投影到原始空间进行边缘修剪、合并和异常检测。

Result: 在许多场景下性能优于k - means、DBSCAN和HDBSCAN等传统方法，具有可扩展性和准确性，显著改善异常检测。

Conclusion: DelTriC是一种有效的聚类算法，有较好的性能和异常检测能力。

Abstract: The paper introduces DelTriC (Delaunay Triangulation Clustering), a clustering algorithm which integrates PCA/UMAP-based projection, Delaunay triangulation, and a novel back-projection mechanism to form clusters in the original high-dimensional space. DelTriC decouples neighborhood construction from decision-making by first triangulating in a low-dimensional proxy to index local adjacency, and then back-projecting to the original space to perform robust edge pruning, merging, and anomaly detection. DelTriC can outperform traditional methods such as k-means, DBSCAN, and HDBSCAN in many scenarios; it is both scalable and accurate, and it also significantly improves outlier detection.

</details>


### [72] [Generating transition states of chemical reactions via distance-geometry-based flow matching](https://arxiv.org/abs/2511.17229)
*Yufei Luo,Xiang Gu,Jian Sun*

Main category: cs.LG

TL;DR: 提出TS - DFM框架预测过渡态，在基准数据集上表现优于先前方法，能提供初始结构、识别反应路径并具强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 过渡态探索受实验和计算方法复杂性限制，需更好方法预测过渡态。

Method: 提出TS - DFM框架，在分子距离几何空间操作，设计TSDVNet学习速度场以准确生成过渡态几何结构。

Result: 在Transition1X数据集上结构精度比React - OT高30%，提供初始结构加速CI - NEB优化，能识别替代反应路径，发现更低能垒过渡态，在RGD1数据集验证泛化能力。

Conclusion: TS - DFM有促进反应探索的潜力。

Abstract: Transition states (TSs) are crucial for understanding reaction mechanisms, yet their exploration is limited by the complexity of experimental and computational approaches. Here we propose TS-DFM, a flow matching framework that predicts TSs from reactants and products. By operating in molecular distance geometry space, TS-DFM explicitly captures the dynamic changes of interatomic distances in chemical reactions. A network structure named TSDVNet is designed to learn the velocity field for generating TS geometries accurately. On the benchmark dataset Transition1X, TS-DFM outperforms the previous state-of-the-art method React-OT by 30\% in structural accuracy. These predicted TSs provide high-quality initial structures, accelerating the convergence of CI-NEB optimization. Additionally, TS-DFM can identify alternative reaction paths. In our experiments, even a more favorable TS with lower energy barrier is discovered. Further tests on RGD1 dataset confirm its strong generalization ability on unseen molecules and reaction types, highlighting its potential for facilitating reaction exploration.

</details>


### [73] [FlexiFlow: decomposable flow matching for generation of flexible molecular ensemble](https://arxiv.org/abs/2511.17249)
*Riccardo Tedoldi,Ola Engkvist,Patrick Bryant,Hossein Azizpour,Jon Paul Janet,Alessandro Tibo*

Main category: cs.LG

TL;DR: 提出FlexiFlow架构，可联合采样分子及其多种构象，在数据集上达SOTA，能生成高质量分子并捕捉构象多样性，还可用于蛋白条件配体生成任务。


<details>
  <summary>Details</summary>
Motivation: 当前3D从头设计模型只能生成单一构象，而分子构象景观决定其性质，需生成低能构象集以评估性质和生成有理想热力学特性的分子。

Method: 提出FlexiFlow架构，扩展流匹配模型，实现分子和多构象联合采样，同时保持等变性和置换不变性。

Result: 在QM9和GEOM Drugs数据集上取得SOTA结果，能生成有效、无张力、独特新颖且符合训练数据分布的分子，捕捉分子构象多样性，生成的构象集覆盖度与基于物理的方法相近但推理时间短，还能用于蛋白条件配体生成任务。

Conclusion: FlexiFlow架构有效，在分子生成和构象采样方面表现良好，有实际应用价值。

Abstract: Sampling useful three-dimensional molecular structures along with their most favorable conformations is a key challenge in drug discovery. Current state-of-the-art 3D de-novo design flow matching or diffusion-based models are limited to generating a single conformation. However, the conformational landscape of a molecule determines its observable properties and how tightly it is able to bind to a given protein target. By generating a representative set of low-energy conformers, we can more directly assess these properties and potentially improve the ability to generate molecules with desired thermodynamic observables. Towards this aim, we propose FlexiFlow, a novel architecture that extends flow-matching models, allowing for the joint sampling of molecules along with multiple conformations while preserving both equivariance and permutation invariance. We demonstrate the effectiveness of our approach on the QM9 and GEOM Drugs datasets, achieving state-of-the-art results in molecular generation tasks. Our results show that FlexiFlow can generate valid, unstrained, unique, and novel molecules with high fidelity to the training data distribution, while also capturing the conformational diversity of molecules. Moreover, we show that our model can generate conformational ensembles that provide similar coverage to state-of-the-art physics-based methods at a fraction of the inference time. Finally, FlexiFlow can be successfully transferred to the protein-conditioned ligand generation task, even when the dataset contains only static pockets without accompanying conformations.

</details>


### [74] [Enforcing governing equation constraints in neural PDE solvers via training-free projections](https://arxiv.org/abs/2511.17258)
*Omer Rochman,Gilles Louppe*

Main category: cs.LG

TL;DR: 评估两种无训练、事后的近似解投影方法，发现它们能减少违反约束情况并提高精度。


<details>
  <summary>Details</summary>
Motivation: 神经PDE求解器常违反控制方程约束，非线性约束投影复杂，动态PDE更难处理。

Method: 评估两种无训练、事后的近似解投影方法，即基于非线性优化的投影和使用雅可比向量与向量雅可比积的局部线性化投影。

Result: 两种投影方法大幅减少了违反约束情况，且在物理信息基线之上提高了精度。

Conclusion: 所评估的两种投影方法在处理PDE约束方面有效。

Abstract: Neural PDE solvers used for scientific simulation often violate governing equation constraints. While linear constraints can be projected cheaply, many constraints are nonlinear, complicating projection onto the feasible set. Dynamical PDEs are especially difficult because constraints induce long-range dependencies in time. In this work, we evaluate two training-free, post hoc projections of approximate solutions: a nonlinear optimization-based projection, and a local linearization-based projection using Jacobian-vector and vector-Jacobian products. We analyze constraints across representative PDEs and find that both projections substantially reduce violations and improve accuracy over physics-informed baselines.

</details>


### [75] [Automobile demand forecasting: Spatiotemporal and hierarchical modeling, life cycle dynamics, and user-generated online information](https://arxiv.org/abs/2511.17275)
*Tom Nahrendorf,Stefan Minner,Helfried Binder,Richard Zinck*

Main category: cs.LG

TL;DR: 研究用德国高端汽车制造商数据解决多产品、多市场、多层次汽车需求预测问题，结合多种方法，指出时空依赖等影响准确性，不同阶段需求受不同因素影响。


<details>
  <summary>Details</summary>
Motivation: 高端汽车制造商因产品多样、数据稀疏和市场动态变化面临复杂预测挑战，需解决多产品、多市场、多层次的月度汽车需求预测问题。

Method: 结合战略和运营规划层面的点预测与概率预测，利用LightGBM模型集成、分位数回归和混合整数线性规划协调方法。

Result: 时空依赖和舍入偏差显著影响预测准确性，短期需求受生命周期成熟度等影响，中期需求受在线参与度等影响，在线行为数据提高细分层面准确性。

Conclusion: 整数预测对运营可行性很重要，不同阶段需求受不同驱动因素影响，在线行为数据有助于提高细分预测准确性。

Abstract: Premium automotive manufacturers face increasingly complex forecasting challenges due to high product variety, sparse variant-level data, and volatile market dynamics. This study addresses monthly automobile demand forecasting across a multi-product, multi-market, and multi-level hierarchy using data from a German premium manufacturer. The methodology combines point and probabilistic forecasts across strategic and operational planning levels, leveraging ensembles of LightGBM models with pooled training sets, quantile regression, and a mixed-integer linear programming reconciliation approach. Results highlight that spatiotemporal dependencies, as well as rounding bias, significantly affect forecast accuracy, underscoring the importance of integer forecasts for operational feasibility. Shapley analysis shows that short-term demand is reactive, shaped by life cycle maturity, autoregressive momentum, and operational signals, whereas medium-term demand reflects anticipatory drivers such as online engagement, planning targets, and competitive indicators, with online behavioral data considerably improving accuracy at disaggregated levels.

</details>


### [76] [Self-supervised denoising of raw tomography detector data for improved image reconstruction](https://arxiv.org/abs/2511.17312)
*Israt Jahan Tulin,Sebastian Starke,Dominic Windisch,André Bieberle,Peter Steinbach*

Main category: cs.LG

TL;DR: 研究两种自监督深度学习去噪方法用于超快电子束X射线CT原始数据去噪，表现优于非学习方法。


<details>
  <summary>Details</summary>
Motivation: 超快电子束X射线CT因测量时间短产生噪声数据，导致重建伪影和图像质量受限，需去噪。

Method: 研究两种自监督深度学习去噪方法，并与非学习去噪方法对比。

Result: 深度学习方法能提高探测器数据信噪比，改善重建图像质量，优于非学习方法。

Conclusion: 深度学习去噪方法在超快电子束X射线CT原始数据去噪上效果更好。

Abstract: Ultrafast electron beam X-ray computed tomography produces noisy data due to short measurement times, causing reconstruction artifacts and limiting overall image quality. To counteract these issues, two self-supervised deep learning methods for denoising of raw detector data were investigated and compared against a non-learning based denoising method. We found that the application of the deep-learning-based methods was able to enhance signal-to-noise ratios in the detector data and also led to consistent improvements of the reconstructed images, outperforming the non-learning based method.

</details>


### [77] [ReBaPL: Repulsive Bayesian Prompt Learning](https://arxiv.org/abs/2511.17339)
*Yassir Bendou,Omar Ezzahir,Eduardo Fernandes Montesuma,Gabriel Mahuas,Victoria Shevchenko,Mike Gartrell*

Main category: cs.LG

TL;DR: 提出Repulsive Bayesian Prompt Learning (ReBaPL)用于贝叶斯提示学习，结合循环步长和SGHMC算法，引入排斥力，在基准数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统提示调整方法易过拟合且泛化能力差，为解决这些局限提出贝叶斯提示学习，本文旨在高效探索提示的复杂后验景观。

Method: 将循环步长调度与随机梯度哈密顿蒙特卡罗（SGHMC）算法结合，引入基于概率度量的排斥力。

Result: 在多个基准数据集上展示了ReBaPL的有效性，性能优于现有提示学习方法。

Conclusion: ReBaPL能更全面地表征提示后验分布，提高泛化能力，是基于最大似然估计的现有提示学习方法的模块化即插即用贝叶斯扩展。

Abstract: Prompt learning has emerged as an effective technique for fine-tuning large-scale foundation models for downstream tasks. However, conventional prompt tuning methods are prone to overfitting and can struggle with out-of-distribution generalization. To address these limitations, Bayesian prompt learning has been proposed, which frames prompt optimization as a Bayesian inference problem to enhance robustness. This paper introduces Repulsive Bayesian Prompt Learning (ReBaPL), a novel method for Bayesian prompt learning, designed to efficiently explore the complex and often multimodal posterior landscape of prompts. Our method integrates a cyclical step-size schedule with a stochastic gradient Hamiltonian Monte Carlo (SGHMC) algorithm, enabling alternating phases of exploration to discover new modes, and exploitation to refine existing modes. Furthermore, we introduce a repulsive force derived from a potential function over probability metrics (including Maximum Mean Discrepancy and Wasserstein distance) computed on the distributions of representations produced by different prompts. This representation-space repulsion diversifies exploration and prevents premature collapse to a single mode. Our approach allows for a more comprehensive characterization of the prompt posterior distribution, leading to improved generalization. In contrast to prior Bayesian prompt learning methods, our method provides a modular plug-and-play Bayesian extension of any existing prompt learning method based on maximum likelihood estimation. We demonstrate the efficacy of ReBaPL on several benchmark datasets, showing superior performance over state-of-the-art methods for prompt learning.

</details>


### [78] [Convergence and stability of Q-learning in Hierarchical Reinforcement Learning](https://arxiv.org/abs/2511.17351)
*Massimiliano Manenti,Andrea Iannelli*

Main category: cs.LG

TL;DR: 提出Feudal Q - learning方案，分析其收敛和稳定性，实验支持理论结果。


<details>
  <summary>Details</summary>
Motivation: 分层强化学习理论保障落后于实践，需要对Feudal Q - learning的收敛和稳定性进行理论分析。

Method: 利用随机逼近理论和ODE方法来分析Feudal Q - learning的收敛和稳定性。

Result: 给出Feudal Q - learning收敛和稳定性的定理，更新收敛点可解释为合适定义游戏的均衡，实验支持理论预期。

Conclusion: 对Feudal RL进行了原则性的收敛和稳定性分析，为分层强化学习引入了博弈论方法。

Abstract: Hierarchical Reinforcement Learning promises, among other benefits, to efficiently capture and utilize the temporal structure of a decision-making problem and to enhance continual learning capabilities, but theoretical guarantees lag behind practice. In this paper, we propose a Feudal Q-learning scheme and investigate under which conditions its coupled updates converge and are stable. By leveraging the theory of Stochastic Approximation and the ODE method, we present a theorem stating the convergence and stability properties of Feudal Q-learning. This provides a principled convergence and stability analysis tailored to Feudal RL. Moreover, we show that the updates converge to a point that can be interpreted as an equilibrium of a suitably defined game, opening the door to game-theoretic approaches to Hierarchical RL. Lastly, experiments based on the Feudal Q-learning algorithm support the outcomes anticipated by theory.

</details>


### [79] [R2PS: Worst-Case Robust Real-Time Pursuit Strategies under Partial Observability](https://arxiv.org/abs/2511.17367)
*Runyu Lu,Ruochuan Shi,Yuanheng Zhu,Dongbin Zhao*

Main category: cs.LG

TL;DR: 本文提出了部分可观测下最坏情况鲁棒实时追捕策略（R2PS），通过结合动态规划和强化学习，实现对未见图结构的零样本泛化并优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有图基追捕 - 逃避游戏在部分可观测下缺乏实时适用追捕策略，且现有强化学习方法局限于完美信息场景。

Method: 先证明传统动态规划算法在逃避者异步移动下的最优性，提出信念保存机制将其扩展到部分可观测场景，再将其嵌入EPG框架完成R2PS学习方案。

Result: 策略实现对未见图结构的鲁棒零样本泛化，持续优于现有游戏强化学习方法在测试图上直接训练的策略。

Conclusion: 所提R2PS方法在部分可观测的追捕 - 逃避游戏中有效且具有良好泛化性。

Abstract: Computing worst-case robust strategies in pursuit-evasion games (PEGs) is time-consuming, especially when real-world factors like partial observability are considered. While important for general security purposes, real-time applicable pursuit strategies for graph-based PEGs are currently missing when the pursuers only have imperfect information about the evader's position. Although state-of-the-art reinforcement learning (RL) methods like Equilibrium Policy Generalization (EPG) and Grasper provide guidelines for learning graph neural network (GNN) policies robust to different game dynamics, they are restricted to the scenario of perfect information and do not take into account the possible case where the evader can predict the pursuers' actions. This paper introduces the first approach to worst-case robust real-time pursuit strategies (R2PS) under partial observability. We first prove that a traditional dynamic programming (DP) algorithm for solving Markov PEGs maintains optimality under the asynchronous moves by the evader. Then, we propose a belief preservation mechanism about the evader's possible positions, extending the DP pursuit strategies to a partially observable setting. Finally, we embed the belief preservation into the state-of-the-art EPG framework to finish our R2PS learning scheme, which leads to a real-time pursuer policy through cross-graph reinforcement learning against the asynchronous-move DP evasion strategies. After reinforcement learning, our policy achieves robust zero-shot generalization to unseen real-world graph structures and consistently outperforms the policy directly trained on the test graphs by the existing game RL approach.

</details>


### [80] [A Unified Stability Analysis of SAM vs SGD: Role of Data Coherence and Emergence of Simplicity Bias](https://arxiv.org/abs/2511.17378)
*Wei-Kai Chang,Rajiv Khanna*

Main category: cs.LG

TL;DR: 本文为分析深度学习优化动态发展了线性稳定性框架，揭示特定最小值稳定和受青睐的原因。


<details>
  <summary>Details</summary>
Motivation: 随着模型规模扩大，理解深度学习优化动态很重要，但SGD及其变体泛化机制不明，缺乏统一理论连接数据结构、优化动态和学习解的性质。

Method: 开发线性稳定性框架，分析SGD、随机扰动和SAM在两层ReLU网络中的行为，引入相干性度量。

Result: 引入的相干性度量能量化梯度曲率在数据点间的对齐情况。

Conclusion: 该框架有助于理解SGD等算法在训练中为何偏好某些稳定的最小值。

Abstract: Understanding the dynamics of optimization in deep learning is increasingly important as models scale. While stochastic gradient descent (SGD) and its variants reliably find solutions that generalize well, the mechanisms driving this generalization remain unclear. Notably, these algorithms often prefer flatter or simpler minima, particularly in overparameterized settings. Prior work has linked flatness to generalization, and methods like Sharpness-Aware Minimization (SAM) explicitly encourage flatness, but a unified theory connecting data structure, optimization dynamics, and the nature of learned solutions is still lacking. In this work, we develop a linear stability framework that analyzes the behavior of SGD, random perturbations, and SAM, particularly in two layer ReLU networks. Central to our analysis is a coherence measure that quantifies how gradient curvature aligns across data points, revealing why certain minima are stable and favored during training.

</details>


### [81] [Stable Coresets via Posterior Sampling: Aligning Induced and Full Loss Landscapes](https://arxiv.org/abs/2511.17399)
*Wei-Kai Chang,Rajiv Khanna*

Main category: cs.LG

TL;DR: 随着深度学习模型扩展，对有效核心集选择技术需求增加。本文提出新框架解决现有梯度方法的局限，实验表明其训练更快、泛化性更好。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型计算需求增长，现有梯度核心集选择方法面临如SGD作为强基线、代表性因损失曲率不匹配而失效等挑战。

Method: 建立后验采样与损失景观的联系；引入基于后验采样的平滑损失函数到模型权重；对基于采样的核心集选择方法进行收敛性分析。

Result: 通过大量实验，该方法在不同数据集上比现有技术训练更快且泛化性更好。

Conclusion: 提出的新框架有效解决了现有梯度核心集选择方法的局限，具有更好的性能。

Abstract: As deep learning models continue to scale, the growing computational demands have amplified the need for effective coreset selection techniques. Coreset selection aims to accelerate training by identifying small, representative subsets of data that approximate the performance of the full dataset. Among various approaches, gradient based methods stand out due to their strong theoretical underpinnings and practical benefits, particularly under limited data budgets. However, these methods face challenges such as naive stochastic gradient descent (SGD) acting as a surprisingly strong baseline and the breakdown of representativeness due to loss curvature mismatches over time.
  In this work, we propose a novel framework that addresses these limitations. First, we establish a connection between posterior sampling and loss landscapes, enabling robust coreset selection even in high data corruption scenarios. Second, we introduce a smoothed loss function based on posterior sampling onto the model weights, enhancing stability and generalization while maintaining computational efficiency. We also present a novel convergence analysis for our sampling-based coreset selection method. Finally, through extensive experiments, we demonstrate how our approach achieves faster training and enhanced generalization across diverse datasets than the current state of the art.

</details>


### [82] [DS-Span: Single-Phase Discriminative Subgraph Mining for Efficient Graph Embeddings](https://arxiv.org/abs/2511.17419)
*Yeamin Kaiser,Muhammed Tasnim Bin Anwar,Bholanath Das,Chowdhury Farhan Ahmed,Md. Tanvir Alam*

Main category: cs.LG

TL;DR: 提出DS - Span单阶段判别子图挖掘框架用于图表示学习，实验表明其比多阶段方法更优。


<details>
  <summary>Details</summary>
Motivation: 现有频繁或判别子图挖掘方法存在多阶段管道冗余、计算成本高、挖掘结构与判别相关性弱的问题。

Method: 提出DS - Span框架，统一模式增长、剪枝和监督驱动评分，引入覆盖上限资格机制和信息增益引导选择。

Result: DS - Span生成的子图特征更紧凑、有判别性，在显著减少运行时间的同时达到更高或相当的准确率。

Conclusion: 统一的单阶段判别挖掘有潜力成为可扩展和可解释图表示学习的基础。

Abstract: Graph representation learning seeks to transform complex, high-dimensional graph structures into compact vector spaces that preserve both topology and semantics. Among the various strategies, subgraph-based methods provide an interpretable bridge between symbolic pattern discovery and continuous embedding learning. Yet, existing frequent or discriminative subgraph mining approaches often suffer from redundant multi-phase pipelines, high computational cost, and weak coupling between mined structures and their discriminative relevance. We propose DS-Span, a single-phase discriminative subgraph mining framework that unifies pattern growth, pruning, and supervision-driven scoring within one traversal of the search space. DS-Span introduces a coverage-capped eligibility mechanism that dynamically limits exploration once a graph is sufficiently represented, and an information-gain-guided selection that promotes subgraphs with strong class-separating ability while minimizing redundancy. The resulting subgraph set serves as an efficient, interpretable basis for downstream graph embedding and classification. Extensive experiments across benchmarks demonstrate that DS-Span generates more compact and discriminative subgraph features than prior multi-stage methods, achieving higher or comparable accuracy with significantly reduced runtime. These results highlight the potential of unified, single-phase discriminative mining as a foundation for scalable and interpretable graph representation learning.

</details>


### [83] [Towards fully differentiable neural ocean model with Veros](https://arxiv.org/abs/2511.17427)
*Etienne Meunier,Said Ouala,Hugo Frezat,Julien Le Sommer,Ronan Fablet*

Main category: cs.LG

TL;DR: 提出VEROS海洋模型可微扩展，实现自动求导，展示两个应用示例，代码在线可用。


<details>
  <summary>Details</summary>
Motivation: 使海洋模型能进行自动求导，便于端到端学习和参数调整。

Method: 对VEROS海洋模型做出关键修改以兼容JAX自动求导框架，并评估数值一致性。

Result: 实现了VEROS海洋模型可微扩展，展示了两个应用。

Conclusion: 可微编程有助于海洋建模中的端到端学习和参数调整。

Abstract: We present a differentiable extension of the VEROS ocean model, enabling automatic differentiation through its dynamical core. We describe the key modifications required to make the model fully compatible with JAX autodifferentiation framework and evaluate the numerical consistency of the resulting implementation. Two illustrative applications are then demonstrated: (i) the correction of an initial ocean state through gradient-based optimization, and (ii) the calibration of unknown physical parameters directly from model observations. These examples highlight how differentiable programming can facilitate end-to-end learning and parameter tuning in ocean modeling. Our implementation is available online.

</details>


### [84] [Multi-Agent Pointer Transformer: Seq-to-Seq Reinforcement Learning for Multi-Vehicle Dynamic Pickup-Delivery Problems](https://arxiv.org/abs/2511.17435)
*Zengyu Zou,Jingyuan Wang,Yixuan Huang,Junjie Wu*

Main category: cs.LG

TL;DR: 本文提出Multi - Agent Pointer Transformer (MAPT)框架解决MVDPDPSR问题，实验显示其性能优于现有方法，计算时间也有优势。


<details>
  <summary>Details</summary>
Motivation: 经典运筹学方法处理大规模动态问题时在计算复杂度和时间效率上有瓶颈，现有强化学习方法存在无法建模联合动作分布、难捕捉实体间关系、联合动作空间大等问题。

Method: 设计MAPT框架，用Transformer Encoder提取实体表示，结合Transformer Decoder和Pointer Network以自回归方式生成联合动作序列，引入Relation - Aware Attention模块捕捉实体间关系，并用信息先验引导模型决策。

Result: 在8个数据集上的实验表明，MAPT在性能上显著优于现有基线方法，与经典运筹学方法相比有显著的计算时间优势。

Conclusion: MAPT框架能有效解决MVDPDPSR问题，在性能和计算时间上有明显优势。

Abstract: This paper addresses the cooperative Multi-Vehicle Dynamic Pickup and Delivery Problem with Stochastic Requests (MVDPDPSR) and proposes an end-to-end centralized decision-making framework based on sequence-to-sequence, named Multi-Agent Pointer Transformer (MAPT). MVDPDPSR is an extension of the vehicle routing problem and a spatio-temporal system optimization problem, widely applied in scenarios such as on-demand delivery. Classical operations research methods face bottlenecks in computational complexity and time efficiency when handling large-scale dynamic problems. Although existing reinforcement learning methods have achieved some progress, they still encounter several challenges: 1) Independent decoding across multiple vehicles fails to model joint action distributions; 2) The feature extraction network struggles to capture inter-entity relationships; 3) The joint action space is exponentially large. To address these issues, we designed the MAPT framework, which employs a Transformer Encoder to extract entity representations, combines a Transformer Decoder with a Pointer Network to generate joint action sequences in an AutoRegressive manner, and introduces a Relation-Aware Attention module to capture inter-entity relationships. Additionally, we guide the model's decision-making using informative priors to facilitate effective exploration. Experiments on 8 datasets demonstrate that MAPT significantly outperforms existing baseline methods in terms of performance and exhibits substantial computational time advantages compared to classical operations research methods.

</details>


### [85] [InTAct: Interval-based Task Activation Consolidation for Continual Learning](https://arxiv.org/abs/2511.17439)
*Patryk Krukowski,Jan Miksa,Piotr Helm,Jacek Tabor,Paweł Wawrzyński,Przemysław Spurek*

Main category: cs.LG

TL;DR: 提出InTAct方法解决基于提示的持续学习在域转移下的表征漂移问题，在多个基准测试中提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于提示的持续学习方法在域转移下易出现表征漂移问题，导致遗忘先前知识。

Method: 引入InTAct方法，捕获先前学习任务的特征激活范围，约束更新以保证网络在这些区域的一致性。

Result: 在DomainNet和ImageNet - R等基准测试中，InTAct持续减少表征漂移，相比现有基线最高将平均准确率提高8个百分点。

Conclusion: InTAct在不冻结参数或存储过去数据的情况下，实现了稳定性和可塑性的平衡。

Abstract: Continual learning aims to enable neural networks to acquire new knowledge without forgetting previously learned information. While recent prompt-based methods perform strongly in class-incremental settings, they remain vulnerable under domain shifts, where the input distribution changes but the label space remains fixed. This exposes a persistent problem known as representation drift. Shared representations evolve in ways that overwrite previously useful features and cause forgetting even when prompts isolate task-specific parameters. To address this issue, we introduce InTAct, a method that preserves functional behavior in shared layers without freezing parameters or storing past data. InTAct captures the characteristic activation ranges associated with previously learned tasks and constrains updates to ensure the network remains consistent within these regions, while still allowing for flexible adaptation elsewhere. In doing so, InTAct stabilizes the functional role of important neurons rather than directly restricting parameter values. The approach is architecture-agnostic and integrates seamlessly into existing prompt-based continual learning frameworks. By regulating representation changes where past knowledge is encoded, InTAct achieves a principled balance between stability and plasticity. Across diverse domain-incremental benchmarks, including DomainNet and ImageNet-R, InTAct consistently reduces representation drift and improves performance, increasing Average Accuracy by up to 8 percentage points over state-of-the-art baselines.

</details>


### [86] [Unmasking Airborne Threats: Guided-Transformers for Portable Aerosol Mass Spectrometry](https://arxiv.org/abs/2511.17446)
*Kyle M. Regan,Michael McLoughlin,Wayne A. Bryden,Gonzalo R. Arce*

Main category: cs.LG

TL;DR: 提出MS - DGFormer框架解决MALDI - MS在实时环境监测中的局限，可直接处理原始质谱数据，实现气溶胶样本病原体识别，推动便携式MALDI - MS平台发展。


<details>
  <summary>Details</summary>
Motivation: MALDI - MS依赖繁琐样本制备和多光谱平均，不适用于实时环境监测，新兴气溶胶MALDI - MS系统需单光谱检测。

Method: 提出MS - DGFormer框架，利用Transformer架构，引入字典编码器整合SVD去噪光谱信息。

Result: 可从气溶胶样本中实现出色的病原体识别，能在野外条件下进行自主实时分析。

Conclusion: 无需大量预处理，为便携式MALDI - MS平台提供可能，革新环境病原体检测和生物威胁应急响应。

Abstract: Matrix Assisted Laser Desorption/Ionization Mass Spectrometry (MALDI-MS) is a cornerstone in biomolecular analysis, offering precise identification of pathogens through unique mass spectral signatures. Yet, its reliance on labor-intensive sample preparation and multi-shot spectral averaging restricts its use to laboratory settings, rendering it impractical for real-time environmental monitoring. These limitations are especially pronounced in emerging aerosol MALDI-MS systems, where autonomous sampling generates noisy spectra for unknown aerosol analytes, requiring single-shot detection for effective analysis. Addressing these challenges, we propose the Mass Spectral Dictionary-Guided Transformer (MS-DGFormer): a data-driven framework that redefines spectral analysis by directly processing raw, minimally prepared mass spectral data. MS-DGFormer leverages a transformer architecture, designed to capture the long-range dependencies inherent in these time-series spectra. To enhance feature extraction, we introduce a novel dictionary encoder that integrates denoised spectral information derived from Singular Value Decomposition (SVD), enabling the model to discern critical biomolecular patterns from single-shot spectra with robust performance. This innovation provides a system to achieve superior pathogen identification from aerosol samples, facilitating autonomous, real-time analysis in field conditions. By eliminating the need for extensive preprocessing, our method unlocks the potential for portable, deployable MALDI-MS platforms, revolutionizing environmental pathogen detection and rapid response to biological threats.

</details>


### [87] [PersonaAgent with GraphRAG: Community-Aware Knowledge Graphs for Personalized LLM](https://arxiv.org/abs/2511.17467)
*Siqi Liang,Yudi Zhang,Yue Guo*

Main category: cs.LG

TL;DR: 提出基于人物角色的语言模型系统框架，用图RAG机制生成个性化提示，在基准测试中表现好。


<details>
  <summary>Details</summary>
Motivation: 满足适应个人用户偏好的个性化AI代理的需求。

Method: 引入知识图增强的检索增强生成（Graph RAG）机制，结合用户历史行为偏好总结和基于图的社区检测识别的全局交互模式生成个性化提示。

Result: 在LaMP基准测试中，新闻分类F1提高11.1%，电影标签F1提高56.1%，产品评级MAE降低10.4%。

Conclusion: 所提出的框架能让代理保持与人物角色一致的行为，同时受益于集体知识，且代码公开。

Abstract: We propose a novel framework for persona-based language model system, motivated by the need for personalized AI agents that adapt to individual user preferences. In our approach, the agent embodies the user's "persona" (e.g. user profile or taste) and is powered by a large language model (LLM). To enable the agent to leverage rich contextual information, we introduce a Knowledge-Graph-enhanced Retrieval-Augmented Generation (Graph RAG) mechanism that constructs an LLM-derived graph index of relevant documents and summarizes communities of related information. Our framework generates personalized prompts by combining: (1) a summary of the user's historical behaviors and preferences extracted from the knowledge graph, and (2) relevant global interaction patterns identified through graph-based community detection. This dynamic prompt engineering approach allows the agent to maintain consistent persona-aligned behaviors while benefiting from collective knowledge. On the LaMP benchmark, our method improves news categorization F1 by 11.1%, movie tagging F1 by 56.1%, and reduces product rating MAE by 10.4% over prior methods. Our code is available at https://anonymous.4open.science/r/PersonaAgentwGraphRAG-DE6F

</details>


### [88] [Harnessing Data from Clustered LQR Systems: Personalized and Collaborative Policy Optimization](https://arxiv.org/abs/2511.17489)
*Vinay Kanakeri,Shivam Bajaj,Ashwin Verma,Vijay Gupta,Aritra Mitra*

Main category: cs.LG

TL;DR: 本文研究在LQR场景下提高强化学习样本效率问题，提出新算法同时进行聚类和学习，证明其能高概率正确聚类，策略次优差距与簇大小成反比，且通信开销小。


<details>
  <summary>Details</summary>
Motivation: 强化学习数据需求大，利用相似过程数据可提高样本效率，但未知过程模型时识别相似过程有挑战。

Method: 结合顺序消除和零阶策略优化思想，提出新算法在多智能体LQR场景下同时进行聚类和学习。

Result: 算法能高概率正确聚类，策略次优差距与簇大小成反比，无额外偏差。

Conclusion: 首次揭示聚类在数据驱动控制中学习个性化策略的作用，且方法通信开销小。

Abstract: It is known that reinforcement learning (RL) is data-hungry. To improve sample-efficiency of RL, it has been proposed that the learning algorithm utilize data from 'approximately similar' processes. However, since the process models are unknown, identifying which other processes are similar poses a challenge. In this work, we study this problem in the context of the benchmark Linear Quadratic Regulator (LQR) setting. Specifically, we consider a setting with multiple agents, each corresponding to a copy of a linear process to be controlled. The agents' local processes can be partitioned into clusters based on similarities in dynamics and tasks. Combining ideas from sequential elimination and zeroth-order policy optimization, we propose a new algorithm that performs simultaneous clustering and learning to output a personalized policy (controller) for each cluster. Under a suitable notion of cluster separation that captures differences in closed-loop performance across systems, we prove that our approach guarantees correct clustering with high probability. Furthermore, we show that the sub-optimality gap of the policy learned for each cluster scales inversely with the size of the cluster, with no additional bias, unlike in prior works on collaborative learning-based control. Our work is the first to reveal how clustering can be used in data-driven control to learn personalized policies that enjoy statistical gains from collaboration but do not suffer sub-optimality due to inclusion of data from dissimilar processes. From a distributed implementation perspective, our method is attractive as it incurs only a mild logarithmic communication overhead.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [89] [Fractional Artificial Neural Networks for Growth Models](https://arxiv.org/abs/2511.16676)
*Juan Carlos Najera-Tinoco,Martin P. Arciga-Alejandre,Jorge Sanchez-Ortiz,Francisco J. Ariza-Hernandez*

Main category: cs.NE

TL;DR: 提出求解分数增长模型初值问题的方法，用离散化Caputo导数构建分数人工神经网络并在R中实现，还给出对比分析解和近似解的例子。


<details>
  <summary>Details</summary>
Motivation: 解决分数增长模型的初值问题，如含周期收获的指数和逻辑模型的推广。

Method: 对Caputo导数进行离散化，构建分数人工神经网络，并在统计软件R中实现。

Result: 给出了分析解和人工神经网络近似解对比的例子。

Conclusion: 所提出的方法可用于求解分数增长模型的初值问题。

Abstract: In this paper we present a method to solve initial value problems for fractional growth models, such as generalizations of the exponential and logistic with periodic harvesting models. Using a discretization of the Caputo derivative we propose a fractional artificial neural network, which is implemented in the statistical software R. Moreover, we show examples where the analytical solutions and the approximation of the artificial neural network are compared.

</details>


### [90] [Jump-diffusion models of parametric volume-price distributions](https://arxiv.org/abs/2511.16838)
*Anup Budhathoki,Leonardo Rydin Gorjão,Pedro G. Lind,Shailendra Bhandari*

Main category: cs.NE

TL;DR: 提出数据驱动框架对纽交所股票量价分布的随机演化建模，分析不同分布模型参数的动态特征。


<details>
  <summary>Details</summary>
Motivation: 对纽约证券交易所股票量价分布的随机演化进行建模和分析。

Method: 每10分钟采样976个交易日的经验分布，拟合不同模型，对参数去趋势，用自适应分箱和回归方法提取Kramers - Moyal系数。

Result: 不同模型中形状参数φ和尺度参数θ有不同动态特征，如Gamma等模型中φ为纯扩散，θ为跳跃 - 扩散；对数正态模型相反；全局矩反演表明θ的跳跃对总方差贡献大。

Conclusion: 稀有不连续性主导了波动率。

Abstract: We present a data-driven framework to model the stochastic evolution of volume-price distribution from the New York Stock Exchange (NYSE) equities. The empirical distributions are sampled every 10 minutes over 976 trading days, and fitted to different models, namely Gamma, Inverse Gamma, Weibull, and Log-Normal distributions. Each of these models is parameterized by a shape parameter, $φ$, and a scale parameter, $θ$, which are detrended from their daily average behavior. The time series of the detrended parameters is analyzed using adaptive binning and regression-based extraction of the Kramers-Moyal (KM) coefficients, up to their sixth order, enabling to classification of its intrinsic dynamics. We show that (i) $φ$ is well described as a pure diffusion with a linear mean regression for the Gamma, Inverse Gamma, and Weibull models, while $θ$ shows dominant jump-diffusion dynamics, with an elevated fourth- and sixth-order moment contributions; (ii) the log-normal model shows however the opposite: $θ$ is predominantly diffusive, with $φ$ showing weak jump signatures; (iii) global moment inversion yields jump rates and amplitudes that account for a large share of total variance for $θ$, confirming that rare discontinuities dominate volatility.

</details>


### [91] [Illuminating the Black Box of Reservoir Computing](https://arxiv.org/abs/2511.17003)
*Claus Metzner,Achim Schilling,Thomas Kinfe,Andreas Maier,Patrick Krauss*

Main category: cs.NE

TL;DR: 本文从系统简化角度研究储层计算机，发现特定情况下读出层承担主要计算，一些次要设计因素对性能有关键影响。


<details>
  <summary>Details</summary>
Motivation: 当前对储层计算机内部数据转换、各层相互作用及设计参数影响理解不足，希望找出不同模型任务所需的最小计算要素。

Method: 从系统简化而非性能最大化角度，研究完成特定任务所需的神经元数量、非线性程度和连接结构，考虑非Sigmoid激活函数神经元和非随机连接网络。

Result: 发现非平凡情况下读出层承担主要计算，储层仅提供弱非线性和记忆；一些常被视为次要的设计方面成为特定任务中系统性能的关键决定因素。

Conclusion: 在研究储层计算机时，不应只关注性能最大化，也要重视系统简化和一些次要设计因素。

Abstract: Reservoir computers, based on large recurrent neural networks with fixed random connections, are known to perform a wide range of information processing tasks. However, the nature of data transformations within the reservoir, the interplay of input matrix, reservoir, and readout layer, as well as the effect of varying design parameters remain poorly understood. In this study, we shift the focus from performance maximization to systematic simplification, aiming to identify the minimal computational ingredients required for different model tasks. We examine how many neurons, how much nonlinearity, and which connective structure is necessary and sufficient to perform certain tasks, considering also neurons with non-sigmoidal activation functions and networks with non-random connectivity. Surprisingly, we find non-trivial cases where the readout layer performs the bulk of the computation, with the reservoir merely providing weak nonlinearity and memory. Furthermore, design aspects often considered secondary, such as the structure of the input matrix, the steepness of activation functions, or the precise input/output timing, emerge as critical determinants of system performance in certain tasks.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [92] [An Introductory Study on the Power Consumption Overhead of ERC-4337 Bundlers](https://arxiv.org/abs/2511.16890)
*Andrei Arusoaie,Claudiu-Nicu Bărbieru,Oana-Otilia Captarencu,Paul-Flavian Diac,Emanuel Onica,Cosmin-Nicolae Vârlan*

Main category: cs.PF

TL;DR: 本文研究以太坊ERC - 4337标准中捆绑器的有功功耗开销，利用SmartWatts确定捆绑器工作负载与有功功耗的相关性。


<details>
  <summary>Details</summary>
Motivation: 区块链网络存在能耗和运营成本问题，ERC - 4337标准中的捆绑器有功耗成本，需研究其有功功耗开销。

Method: 使用SmartWatts监测系统，借助Running Average Power Limit (RAPL)硬件接口进行实证研究。

Result: 文中未提及具体研究结果。

Conclusion: 文中未提及具体结论。

Abstract: Ethereum is currently the main blockchain ecosystem providing decentralised trust guarantees for applications ranging from finance to e-government. A common criticism of blockchain networks has been their energy consumption and operational costs. The switch from Proof-of-Work (PoW) protocol to Proof-of-Stake (PoS) protocol has significantly reduced this issue, though concerns remain, especially with network expansions via additional layers. The ERC-4337 standard is a recent proposal that facilitates end-user access to Ethereum-backed applications. It introduces a middleware called a bundler, operated as a third-party service, where part of its operational cost is represented by its power consumption. While bundlers have served over 500 million requests in the past two years, fewer than 15 official bundler providers exist, compared to over 100 regular Ethereum access providers. In this paper, we provide a first look at the active power consumption overhead that a bundler would add to an Ethereum access service. Using SmartWatts, a monitoring system leveraging Running Average Power Limit (RAPL) hardware interfaces, we empirically determine correlations between the bundler workload and its active power consumption.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [93] [Large language models for automated PRISMA 2020 adherence checking](https://arxiv.org/abs/2511.16707)
*Yuki Kataoka,Ryuhei So,Masahiro Banno,Yasushi Tsujimoto,Tomohiro Takayama,Yosuke Yamagishi,Takahiro Tsuge,Norio Yamamoto,Chiaki Suda,Toshi A. Furukawa*

Main category: cs.SE

TL;DR: 构建版权感知基准评估大语言模型对PRISMA 2020指南的评估能力，结构化清单可提升评估准确性，人工验证仍必要。


<details>
  <summary>Details</summary>
Motivation: 解决同行评审中缺乏可共享的评估PRISMA 2020指南遵循情况的基准问题。

Method: 构建含108篇系统评价的版权感知基准，用五种输入格式评估十个大语言模型。

Result: 结构化清单输入准确率78.7 - 79.7%，高于仅手稿输入；不同模型准确率70.6 - 82.8%；Qwen3 - Max在全数据集上灵敏度95.1%，特异度49.3%。

Conclusion: 结构化清单可大幅改善基于大语言模型的PRISMA评估，但编辑决策前人工专家验证仍不可或缺。

Abstract: Evaluating adherence to PRISMA 2020 guideline remains a burden in the peer review process. To address the lack of shareable benchmarks, we constructed a copyright-aware benchmark of 108 Creative Commons-licensed systematic reviews and evaluated ten large language models (LLMs) across five input formats. In a development cohort, supplying structured PRISMA 2020 checklists (Markdown, JSON, XML, or plain text) yielded 78.7-79.7% accuracy versus 45.21% for manuscript-only input (p less than 0.0001), with no differences between structured formats (p>0.9). Across models, accuracy ranged from 70.6-82.8% with distinct sensitivity-specificity trade-offs, replicated in an independent validation cohort. We then selected Qwen3-Max (a high-sensitivity open-weight model) and extended evaluation to the full dataset (n=120), achieving 95.1% sensitivity and 49.3% specificity. Structured checklist provision substantially improves LLM-based PRISMA assessment, though human expert verification remains essential before editorial decisions.

</details>


### [94] [Multi-Agent Code Verification with Compound Vulnerability Detection](https://arxiv.org/abs/2511.16708)
*Shreshth Rajan*

Main category: cs.SE

TL;DR: 大语言模型生成的代码易出错，现有工具检测效果不佳。CodeX - Verify多智能体系统能更高效检测代码漏洞，测试效果好且适用于生产。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型生成代码存在大量漏洞，而现有工具检测能力有限、误报率高的问题。

Method: 构建CodeX - Verify多智能体系统，使用四个专业智能体检测不同类型漏洞，并结合不同检测模式的智能体。

Result: 系统能捕获76.1%的漏洞，运行速度快且无需测试执行；多智能体组合比单智能体准确率提高39.7个百分点，最佳双智能体组合准确率达79.3%；在实际补丁测试中运行速度快，适用于生产。

Conclusion: CodeX - Verify多智能体系统在检测大语言模型生成代码漏洞方面表现出色，多智能体组合效果优于单智能体，具有实际应用价值。

Abstract: LLMs generate buggy code: 29.6% of SWE-bench "solved" patches fail, 62% of BaxBench solutions have vulnerabilities, and existing tools only catch 65% of bugs with 35% false positives. We built CodeX-Verify, a multi-agent system that uses four specialized agents to detect different types of bugs. We prove mathematically that combining agents with different detection patterns finds more bugs than any single agent when the agents look for different problems, confirmed by measuring agent correlation of p = 0.05--0.25. We also show that multiple vulnerabilities in the same code create exponentially more risk than previously thought--SQL injection plus exposed credentials creates 15x more danger (risk 300 vs. 20) than traditional models predict. Testing on 99 code samples with verified labels shows our system catches 76.1% of bugs, matching the best existing method while running faster and without test execution. We tested 15 different agent combinations and found that using multiple agents improves accuracy by 39.7 percentage points (from 32.8% to 72.4%) compared to single agents, with gains of +14.9pp, +13.5pp, and +11.2pp for agents 2, 3, and 4. The best two-agent combination reaches 79.3% accuracy. Testing on 300 real patches from Claude Sonnet 4.5 runs in under 200ms per sample, making this practical for production use.

</details>


### [95] [Is the Cure Still Worse Than the Disease? Test Overfitting by LLMs in Automated Program Repair](https://arxiv.org/abs/2511.16858)
*Toufique Ahmed,Jatin Ganhotra,Avraham Shinnar,Martin Hirzel*

Main category: cs.SE

TL;DR: 通过实验研究当前使用仓库级SWE - bench任务时测试过拟合问题的严重程度。


<details>
  <summary>Details</summary>
Motivation: 自动化程序修复存在生成的代码通过可见测试但在隐藏测试集失败的问题，即测试过拟合，要研究当前该问题的严重程度。

Method: 使用仓库级SWE - bench任务进行实验研究。

Result: 未提及。

Conclusion: 未提及。

Abstract: Automated program repair has been shown to be susceptible to generating repaired code that passes on seen tests but fails on a hold-out set of hidden tests. This problem, dubbed test overfitting, has been identified and studied before the rise of large language models. We experimentally study how much test overfitting is still a problem today, using repository-level SWE-bench tasks.

</details>


### [96] [MOOT: a Repository of Many Multi-Objective Optimization Tasks](https://arxiv.org/abs/2511.16882)
*Tim Menzies,Tao Chen,Yulong Ye,Kishan Kumar Ganguly,Amirali Rayegan,Srinath Srinivasan,Andre Lustosa*

Main category: cs.SE

TL;DR: 引入MOOT多目标优化任务仓库，含120+任务，可支持新研究问题。


<details>
  <summary>Details</summary>
Motivation: 软件工程师做决策需权衡目标，但MSR研究者和工业从业者面临困难，缺少探索权衡的工具，需推动该领域研究。

Method: 引入MOOT仓库，其任务来自SE研究论文，涵盖软件配置等多方面，位于github且开源。

Result: MOOT仓库数据可引出数十个新研究问题。

Conclusion: MOOT仓库能推动多目标优化权衡领域的研究。

Abstract: Software engineers must make decisions that trade off competing goals (faster vs. cheaper, secure vs. usable, accurate vs. interpretable, etc.). Despite MSR's proven techniques for exploring such goals, researchers still struggle with these trade-offs. Similarly, industrial practitioners deliver sub-optimal products since they lack the tools needed to explore these trade-offs.
  To enable more research in this important area, we introduce MOOT, a repository of multi-objective optimization tasks taken from recent SE research papers. MOOT's tasks cover software configuration, cloud tuning, project health, process modeling, hyperparameter optimization, and more. Located at github.com/timm/moot, MOOT's current 120+ tasks are freely available under an MIT license (and we invite community contributions). As shown here, this data enables dozens of novel research questions.

</details>


### [97] [ReVul-CoT: Towards Effective Software Vulnerability Assessment with Retrieval-Augmented Generation and Chain-of-Thought Prompting](https://arxiv.org/abs/2511.17027)
*Zhijie Chen,Xiang Chen,Ziming Li,Jiacheng Xue,Chaoyang Gao*

Main category: cs.SE

TL;DR: 提出ReVul - CoT框架结合RAG与CoT提示提升基于大语言模型的软件漏洞评估，实验效果佳。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在软件漏洞评估中缺乏特定领域知识且依赖浅层模式匹配，难以理解复杂代码语义和安全影响。

Method: 提出ReVul - CoT框架，RAG模块从本地知识库动态检索相关信息，CoT提示引导大语言模型逐步推理。

Result: 在12070个漏洞数据集上评估，ReVul - CoT在MCC等指标上优于现有基线，消融实验验证各部分贡献。

Conclusion: 结合RAG与CoT提示显著提升基于大语言模型的软件漏洞评估，为未来研究指明方向。

Abstract: Context: Software Vulnerability Assessment (SVA) plays a vital role in evaluating and ranking vulnerabilities in software systems to ensure their security and reliability. Objective: Although Large Language Models (LLMs) have recently shown remarkable potential in SVA, they still face two major limitations. First, most LLMs are trained on general-purpose corpora and thus lack domain-specific knowledge essential for effective SVA. Second, they tend to rely on shallow pattern matching instead of deep contextual reasoning, making it challenging to fully comprehend complex code semantics and their security implications. Method: To alleviate these limitations, we propose a novel framework ReVul-CoT that integrates Retrieval-Augmented Generation (RAG) with Chain-of-Thought (COT) prompting. In ReVul-CoT, the RAG module dynamically retrieves contextually relevant information from a constructed local knowledge base that consolidates vulnerability data from authoritative sources (such as NVD and CWE), along with corresponding code snippets and descriptive information. Building on DeepSeek-V3.1, CoT prompting guides the LLM to perform step-by-step reasoning over exploitability, impact scope, and related factors Results: We evaluate ReVul-CoT on a dataset of 12,070 vulnerabilities. Experimental results show that ReVul-CoT outperforms state-of-the-art SVA baselines by 16.50%-42.26% in terms of MCC, and outperforms the best baseline by 10.43%, 15.86%, and 16.50% in Accuracy, F1-score, and MCC, respectively. Our ablation studies further validate the contributions of considering dynamic retrieval, knowledge integration, and CoT-based reasoning. Conclusion: Our results demonstrate that combining RAG with CoT prompting significantly enhances LLM-based SVA and points out promising directions for future research.

</details>


### [98] [UI-CUBE: Enterprise-Grade Computer Use Agent Benchmarking Beyond Task Accuracy to Operational Reliability](https://arxiv.org/abs/2511.17131)
*Horia Cristescu,Charles Park,Trong Canh Nguyen,Sergiu Talmacel,Alexandru-Gabriel Ilie,Stefan Adam*

Main category: cs.SE

TL;DR: 提出UI - CUBE基准测试，评估显示当前CUA在复杂工作流存在性能断崖，揭示其架构局限，为开发生产就绪CUA提供见解。


<details>
  <summary>Details</summary>
Motivation: 现有CUA基准对企业部署就绪性评估有限，强调功能正确性而忽视生产系统所需的操作可靠性。

Method: 构建包含226个任务、分两个难度层级的UI - CUBE基准，涵盖简单UI交互和复杂工作流，进行系统界面变化覆盖、多分辨率测试和任务成功自动验证。

Result: 评估五个先进模型，简单UI交互成功率67 - 85%，复杂工作流降至9 - 19%，人类评估者在复杂任务成功率61.2%，呈现不连续性能模式。

Conclusion: 当前CUA存在内存管理、分层规划和状态协调的架构局限，不能作为可靠的工作流自动化工具，研究为开发生产就绪CUA提供架构见解。

Abstract: While current Computer Use Agent (CUA) benchmarks measure task completion effectively, they provide limited assessment of enterprise deployment readiness, emphasizing functional correctness over the operational reliability required for production systems. We present UI-CUBE (UiPath Computer Use BEnchmark), a systematic benchmark comprising 226 tasks across two difficulty tiers designed to expose fundamental architectural limitations in current CUAs. Our evaluation covers simple UI interactions (136 tasks) and complex workflows including copy-paste tasks (50 tasks) and enterprise application scenarios (40 tasks), with systematic interface variation coverage, multi-resolution testing and automated validation of task success through the application state. Evaluation of five state-of-the-art models reveals a sharp capability cliff rather than gradual performance degradation. Simple UI interactions achieve 67-85% success rates (compared to 97.9% human performance), but complex workflows drop precipitously to 9-19%. Human evaluators with no prior application experience achieve only 61.2% on complex tasks despite near-perfect performance on simple tasks, establishing realistic performance ceilings. This discontinuous performance pattern -- where agents achieve 68-87% of human performance on simple tasks but only 15-32% on complex workflows -- indicates fundamental architectural limitations in memory management, hierarchical planning, and state coordination rather than incremental capability gaps addressable through better training or prompting. UI-CUBE functions as an enterprise-readiness diagnostic, revealing that while current CUAs can manipulate individual interface elements, they cannot yet function as reliable workflow automation tools. These findings provide architectural insights essential for developing production-ready CUAs capable of managing complex, multi-step enterprise processes.

</details>


### [99] [SlsReuse: LLM-Powered Serverless Function Reuse](https://arxiv.org/abs/2511.17262)
*Jinfeng Wen,Yuehan Sun*

Main category: cs.SE

TL;DR: 本文提出首个由大语言模型驱动的无服务器函数复用框架SlsReuse，在数据集上评估显示其性能超越现有基线。


<details>
  <summary>Details</summary>
Motivation: 无服务器计算虽降低运维开销，但开发函数对新手有挑战，现有函数推荐技术不足，需新方法解决。

Method: 构建可复用函数库，通过少量提示的有效提示工程学习异构函数统一语义增强表示，再根据自然语言任务查询进行意图感知发现、多级剪枝和相似度匹配。

Result: 在110个任务查询的数据集上评估，基于ChatGPT - 4o的SlsReuse的Recall@10达91.20%，超现有基线24.53个百分点。

Conclusion: SlsReuse能有效解决无服务器函数复用中的函数推荐问题，性能表现良好。

Abstract: Serverless computing has rapidly emerged as a popular cloud computing paradigm. It enables developers to implement function-level tasks, i.e., serverless functions, without managing infrastructure. While reducing operational overhead, it poses challenges, especially for novice developers. Developing functions from scratch requires adapting to heterogeneous, platform-specific programming styles, making the process time-consuming and error-prone. Function reuse offers a promising solution to address these challenges. However, research on serverless computing lacks a dedicated approach for function recommendation. Existing techniques from traditional contexts remain insufficient due to the semantic gap between task descriptions and heterogeneous function implementations. Advances in large language models (LLMs), pre-trained on large-scale corpora, create opportunities to bridge this gap by aligning developer requirements with function semantics.
  This paper presents SlsReuse, the first LLM-powered framework for serverless function reuse. Specifically, SlsReuse first constructs a reusable function repository serving as a foundational knowledge base. Then, it learns unified semantic-enhanced representations of heterogeneous functions through effective prompt engineering with few-shot prompting, capturing implicit code intent, target platforms, programming languages, and cloud services. Finally, given a natural language task query, SlsReuse performs intent-aware discovery combined with a multi-level pruning strategy and similarity matching. We evaluate SlsReuse on a curated dataset of 110 task queries. Built on ChatGPT-4o, one of the most representative LLMs, SlsReuse achieves Recall@10 of 91.20%, exceeding the state-of-the-art baseline by 24.53 percentage points.

</details>


### [100] [Detecting Performance-Relevant Changes in Configurable Software Systems](https://arxiv.org/abs/2511.17271)
*Sebastian Böhm,Florian Sattler,Norbert Siegmund,Sven Apel*

Main category: cs.SE

TL;DR: 频繁性能分析成本高，配置采样有缺陷，提出ConfFLARE方法，能有效检测性能回归并减少测试配置数量。


<details>
  <summary>Details</summary>
Motivation: 频繁进行性能测量成本高，配置采样不能保证完整性，可能遗漏性能回归。

Method: 提出ConfFLARE，通过识别与性能相关代码的数据流交互来估计变更是否影响性能，并提取参与交互的软件特性，据此选择相关配置子集进行性能分析。

Result: 在合成和真实软件系统研究中，几乎能正确检测所有性能回归，除两例外能识别相关特性，平均减少合成场景79%、真实场景70%的测试配置数量，节省大量测试时间。

Conclusion: ConfFLARE能有效检测性能回归，显著减少性能测试配置数量和时间。

Abstract: Performance is a volatile property of a software system and frequent performance profiling is required to keep the knowledge about a software system's performance behavior up to date. Repeating all performance measurements after every revision is a cost-intensive task, especially in the presence of configurability, where one has to measure multiple configurations to obtain a comprehensive picture. Configuration sampling is a common approach to control the measurement cost. However, it cannot guarantee completeness and might miss performance regressions, especially if they only affect few configurations. As an alternative to solve the cost reduction problem, we present ConfFLARE: ConfFLARE estimates whether a change potentially impacts performance by identifying data-flow interactions with performance-relevant code and extracts which software features participate in such interactions. Based on these features, we can select a subset of relevant configurations to focus performance profiling efforts on. In a study conducted on both, synthetic and real-world software systems, ConfFLARE correctly detects performance regressions in almost all cases and identifies relevant features in all but two cases, reducing the number of configurations to be tested on average by $79\%$ for synthetic and by $70\%$ for real-world regression scenarios saving hours of performance testing time.

</details>


### [101] [Framework Matters: Energy Efficiency of UI Automation Testing Frameworks](https://arxiv.org/abs/2511.17303)
*Timmie M. R. Lagermann,Kristina Sophia Carter,Su Mei Gwen Ho,Luís Cruz,Kerstin Eder,Maja H. Kirkeby*

Main category: cs.SE

TL;DR: 研究四个Web UI自动化测试框架的单操作能耗，发现能耗因框架和操作而异，为开发人员提供节能测试决策依据。


<details>
  <summary>Details</summary>
Motivation: 确定是否有一致趋势可指导节能测试设计。

Method: 采用受控的客户端 - 服务器设置和外部功率计量，对每个UI操作重复35次。

Result: 不同框架和操作的能耗不同，Puppeteer在部分操作最节能，Selenium在部分操作最节能，Nightwatch总体能耗最高，相同操作能耗因框架最多相差6倍。

Conclusion: 为UI自动化测试框架提供能耗透明度，可让开发人员做出节能测试决策。

Abstract: We examine per action energy consumption across four web user interface (UI) automation testing frameworks to determine whether consistent tendencies can guide energy-aware test design. Using a controlled client-server setup with external power metering, we repeat each UI action (refresh, click variants, checkbox, drag&drop, input-text, scroll) 35 times. Across each of the actions, energy costs vary by both framework and action. Puppeteer is the most efficient for left-click, right-click, double-click, checkbox, and input-text; Selenium is the most efficient for refresh and scroll; Nightwatch is generally the least energy efficient. The energy cost of performing the same action varied by up to a factor of six depending on the framework. This indicates that providing transparency of energy consumption for UI automation testing frameworks allows developers to make informed, energy-aware decisions when testing a specific UI action.

</details>


### [102] [Agentic Program Verification](https://arxiv.org/abs/2511.17330)
*Haoxin Tu,Huan Zhao,Yahui Song,Mehtab Zafar,Ruijie Meng,Abhik Roychoudhury*

Main category: cs.SE

TL;DR: 本文提出首个用于程序验证的LLM代理AutoRocq，能边学习边迭代改进证明，实验显示其在自动程序验证上有效，有望与AI编码代理集成。


<details>
  <summary>Details</summary>
Motivation: 随着自动生成代码流行及AI用于通用数学推理的可能性，期望用AI对AI生成的大量代码进行推理验证。

Method: 提出AutoRocq代理，通过与Rocq定理证明器通信获取上下文和反馈，以迭代改进循环学习和完善证明。

Result: 在SV - COMP基准和Linux内核模块实验中，AutoRocq在自动程序验证上展现出良好效果。

Conclusion: 该证明代理有望与AI编码代理集成，实现生成与验证循环，迈向可信自动编程愿景。

Abstract: Automatically generated code is gaining traction recently, owing to the prevalence of Large Language Models (LLMs). Further, the AlphaProof initiative has demonstrated the possibility of using AI for general mathematical reasoning. Reasoning about computer programs (software) can be accomplished via general mathematical reasoning; however, it tends to be more structured and richer in contexts. This forms an attractive proposition, since then AI agents can be used to reason about voluminous code that gets generated by AI.
  In this work, we present a first LLM agent, AutoRocq, for conducting program verification. Unlike past works, which rely on extensive training of LLMs on proof examples, our agent learns on-the-fly and improves the proof via an iterative refinement loop. The iterative improvement of the proof is achieved by the proof agent communicating with the Rocq (formerly Coq) theorem prover to get additional context and feedback. The final result of the iteration is a proof derivation checked by the Rocq theorem prover. In this way, our proof construction involves autonomous collaboration between the proof agent and the theorem prover. This autonomy facilitates the search for proofs and decision-making in deciding on the structure of the proof tree.
  Experimental evaluation on SV-COMP benchmarks and on Linux kernel modules shows promising efficacy in achieving automated program verification. As automation in code generation becomes more widespread, we posit that our proof agent can be potentially integrated with AI coding agents to achieve a generate and validate loop, thus moving closer to the vision of trusted automatic programming.

</details>


### [103] [Exploring Scientific Debt: Harnessing AI for SATD Identification in Scientific Software](https://arxiv.org/abs/2511.17368)
*Eric L. Melin,Ahmed Musa Awon,Nasir U. Eisty,Neil A. Ernst,Shurui Zhou*

Main category: cs.SE

TL;DR: 研究探索了科学软件（SSW）仓库中的自我承认技术债务（SATD），对比科学与通用开源软件中的SATD，评估基于Transformer的SATD识别模型，发现SSW中SATD更多，最佳模型表现更优。


<details>
  <summary>Details</summary>
Motivation: 科学软件中SATD普遍且影响大，但SATD与SSW的关系未充分研究，需了解如何在该领域管理SATD。

Method: 分析27个多领域、多语言的科学和通用仓库中的SATD，在67,066条标注代码注释上微调并比较10个基于Transformer的模型（1亿 - 70亿参数）。

Result: SSW包含的科学债务是通用软件的9.25倍，SATD是4.93倍，最佳模型优于现有模型。

Conclusion: 揭示了SSW中SATD与通用软件的差异及其对质量和科学有效性的影响，开发者和研究者可据此采取策略管理债务，保障科学发现的完整性。

Abstract: Developers often leave behind clues in their code, admitting where it falls short, known as Self-Admitted Technical Debt (SATD). In the world of Scientific Software (SSW), where innovation moves fast and collaboration is key, such debt is not just common but deeply impactful. As research relies on accurate and reproducible results, accumulating SATD can threaten the very foundations of scientific discovery. Yet, despite its significance, the relationship between SATD and SSW remains largely unexplored, leaving a crucial gap in understanding how to manage SATD in this critical domain. This study explores SATD in SSW repositories, comparing SATD in scientific versus general-purpose open-source software and evaluating transformer-based models for SATD identification. We analyzed SATD in 27 scientific and general-purpose repositories across multiple domains and languages. We fine-tuned and compared 10 transformer-based models (100M-7B parameters) on 67,066 labeled code comments. SSW contains 9.25x more Scientific Debt and 4.93x more SATD than general-purpose software due to complex computations, domain constraints, and evolving research needs. Furthermore, our best model outperforms existing ones. This study uncovers how SATD in SSW differs from general software, revealing its impact on quality and scientific validity. By recognizing these challenges, developers and researchers can adopt smarter strategies to manage debt and safeguard the integrity of scientific discovery.

</details>


### [104] [CREST: Improving Interpretability and Effectiveness of Troubleshooting at Ericsson through Criterion-Specific Trouble Report Retrieval](https://arxiv.org/abs/2511.17417)
*Soroush Javdan,Pragash Krishnamoorthy,Olga Baysal*

Main category: cs.SE

TL;DR: 电信行业发展需高效故障排查，研究不同TR观察标准对检索模型性能影响，提出CREST方法，实验表明标准特定模型表现更佳。


<details>
  <summary>Details</summary>
Motivation: 电信行业发展需要高效故障排查，TR数据复杂量大给检索系统带来挑战，需研究不同观察标准对检索模型的影响。

Method: 提出CREST方法，利用针对特定TR标准训练的专业模型并聚合输出。

Result: 使用爱立信内部部分TR数据实验，标准特定模型在关键评估指标上显著优于单一模型。

Conclusion: 本研究中所有目标标准对优化检索系统性能很重要。

Abstract: The rapid evolution of the telecommunication industry necessitates efficient troubleshooting processes to maintain network reliability, software maintainability, and service quality. Trouble Reports (TRs), which document issues in Ericsson's production system, play a critical role in facilitating the timely resolution of software faults. However, the complexity and volume of TR data, along with the presence of diverse criteria that reflect different aspects of each fault, present challenges for retrieval systems. Building on prior work at Ericsson, which utilized a two-stage workflow, comprising Initial Retrieval (IR) and Re-Ranking (RR) stages, this study investigates different TR observation criteria and their impact on the performance of retrieval models. We propose \textbf{CREST} (\textbf{C}riteria-specific \textbf{R}etrieval via \textbf{E}nsemble of \textbf{S}pecialized \textbf{T}R models), a criterion-driven retrieval approach that leverages specialized models for different TR fields to improve both effectiveness and interpretability, thereby enabling quicker fault resolution and supporting software maintenance. CREST utilizes specialized models trained on specific TR criteria and aggregates their outputs to capture diverse and complementary signals. This approach leads to enhanced retrieval accuracy, better calibration of predicted scores, and improved interpretability by providing relevance scores for each criterion, helping users understand why specific TRs were retrieved. Using a subset of Ericsson's internal TRs, this research demonstrates that criterion-specific models significantly outperform a single model approach across key evaluation metrics. This highlights the importance of all targeted criteria used in this study for optimizing the performance of retrieval systems.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [105] [Law-Strength Frontiers and a No-Free-Lunch Result for Law-Seeking Reinforcement Learning on Volatility Law Manifolds](https://arxiv.org/abs/2511.17304)
*Jian'an Zhang*

Main category: q-fin.CP

TL;DR: 从科学人工智能角度研究波动率曲面的强化学习，探讨无套利法则作为软惩罚对强化学习智能体的影响，证明相关定理并实验，发现奖励塑造不足以实现稳健法则对齐。


<details>
  <summary>Details</summary>
Motivation: 探究将公理无套利法则作为软惩罚加在学习的世界模型上，能否可靠地校准高容量强化学习智能体，还是会产生利用模型误差的古德哈特定律式激励。

Method: 从经典静态无套利条件构建有限维凸波动率法则流形、度量法则惩罚函数和优雅失败指数，用合成生成器生成法则一致轨迹，训练无法则正则化的循环神经世界模型，定义古德哈特分解，证明相关定理并在类SPX/VIX世界模型上实验。

Result: 简单结构策略形成经验法则强度前沿，所有寻求法则的强化学习变体表现不佳，进入高惩罚、高GFI区域。

Conclusion: 在波动率场景中，用可验证惩罚进行奖励塑造不足以实现稳健法则对齐。

Abstract: We study reinforcement learning (RL) on volatility surfaces through the lens of Scientific AI. We ask whether axiomatic no-arbitrage laws, imposed as soft penalties on a learned world model, can reliably align high-capacity RL agents, or mainly create Goodhart-style incentives to exploit model errors. From classical static no-arbitrage conditions we build a finite-dimensional convex volatility law manifold of admissible total-variance surfaces, together with a metric law-penalty functional and a Graceful Failure Index (GFI) that normalizes law degradation under shocks. A synthetic generator produces law-consistent trajectories, while a recurrent neural world model trained without law regularization exhibits structured off-manifold errors. On this testbed we define a Goodhart decomposition \(r = r^{\mathcal{M}} + r^\perp\), where \(r^\perp\) is ghost arbitrage from off-manifold prediction error. We prove a ghost-arbitrage incentive theorem for PPO-type agents, a law-strength trade-off theorem showing that stronger penalties eventually worsen P\&L, and a no-free-lunch theorem: under a law-consistent world model and law-aligned strategy class, unconstrained law-seeking RL cannot Pareto-dominate structural baselines on P\&L, penalties, and GFI. In experiments on an SPX/VIX-like world model, simple structural strategies form the empirical law-strength frontier, while all law-seeking RL variants underperform and move into high-penalty, high-GFI regions. Volatility thus provides a concrete case where reward shaping with verifiable penalties is insufficient for robust law alignment.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [106] [BITS for GAPS: Bayesian Information-Theoretic Sampling for hierarchical GAussian Process Surrogates](https://arxiv.org/abs/2511.16815)
*Kyla D. Jones,Alexander W. Dowling*

Main category: stat.ML

TL;DR: 提出BITS for GAPS框架用于混合物理系统潜在组件仿真，推导熵基采集函数指导数据采集，通过建模汽液平衡系统活动系数验证框架效用，该框架高效、可解释且能感知不确定性。


<details>
  <summary>Details</summary>
Motivation: 为混合物理系统的潜在组件仿真提供有效框架，处理已知物理规律和数据推断的剩余动态。

Method: 引入BITS for GAPS框架，在潜在函数上设置高斯过程先验和层次先验，推导熵基采集函数，以有限均匀加权高斯过程混合近似预测后验。

Result: 熵引导采样提高样本效率，加速代理收敛，增强非理想状态下预测精度，保持物理一致性。

Conclusion: BITS for GAPS为复杂物理系统的混合建模提供了高效、可解释且能感知不确定性的框架。

Abstract: We introduce the Bayesian Information-Theoretic Sampling for hierarchical GAussian Process Surrogates (BITS for GAPS) framework to emulate latent components in hybrid physical systems. BITS for GAPS supports serial hybrid modeling, where known physics governs part of the system and residual dynamics are represented as a latent function inferred from data. A Gaussian process prior is placed over the latent function, with hierarchical priors on its hyperparameters to encode physically meaningful structure in the predictive posterior.
  To guide data acquisition, we derive entropy-based acquisition functions that quantify expected information gain from candidate input locations, identifying samples most informative for training the surrogate. Specifically, we obtain a closed-form expression for the differential entropy of the predictive posterior and establish a tractable lower bound for efficient evaluation. These derivations approximate the predictive posterior as a finite, uniformly weighted mixture of Gaussian processes.
  We demonstrate the framework's utility by modeling activity coefficients in vapor-liquid equilibrium systems, embedding the surrogate into extended Raoult's law for distillation design. Numerical results show that entropy-guided sampling improves sample efficiency by targeting regions of high uncertainty and potential information gain. This accelerates surrogate convergence, enhances predictive accuracy in non-ideal regimes, and preserves physical consistency. Overall, BITS for GAPS provides an efficient, interpretable, and uncertainty-aware framework for hybrid modeling of complex physical systems.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [107] [Modified Delayed Acceptance MCMC for Quasi-Bayesian Inference with Linear Moment Conditions](https://arxiv.org/abs/2511.17117)
*Masahiro Tanaka*

Main category: stat.CO

TL;DR: 本文提出基于线性矩条件的准贝叶斯推断高效计算框架，介绍两种DA - MCMC实现方式，模拟研究和实证应用表明其优于传统方法，还可扩展到风险型准贝叶斯公式。


<details>
  <summary>Details</summary>
Motivation: 开发基于线性矩条件的计算高效的准贝叶斯推断框架。

Method: 采用延迟接受马尔可夫链蒙特卡罗（DA - MCMC）算法，使用代理目标核和从近似条件后验导出的提议分布，介绍DA - MCMC - Exact和DA - MCMC - Approx两种实现方式。

Result: 在异方差线性回归模拟研究中，在每次迭代和每秒的多元有效样本量上优于标准MCMC和传统DA - MCMC；Approx变体总体吞吐量最佳，Exact变体每次迭代效率最高；实证应用中Approx可处理更大规模设计。

Conclusion: 所提出的算法为准贝叶斯分析提供了实用且稳健的工具。

Abstract: We develop a computationally efficient framework for quasi-Bayesian inference based on linear moment conditions. The approach employs a delayed acceptance Markov chain Monte Carlo (DA-MCMC) algorithm that uses a surrogate target kernel and a proposal distribution derived from an approximate conditional posterior, thereby exploiting the structure of the quasi-likelihood. Two implementations are introduced. DA-MCMC-Exact fully incorporates prior information into the proposal distribution and maximizes per-iteration efficiency, whereas DA-MCMC-Approx omits the prior in the proposal to reduce matrix inversions, improving numerical stability and computational speed in higher dimensions. Simulation studies on heteroskedastic linear regressions show substantial gains over standard MCMC and conventional DA-MCMC baselines, measured by multivariate effective sample size per iteration and per second. The Approx variant yields the best overall throughput, while the Exact variant attains the highest per-iteration efficiency. Applications to two empirical instrumental variable regressions corroborate these findings: the Approx implementation scales to larger designs where other methods become impractical, while still delivering precise inference. Although developed for moment-based quasi-posteriors, the proposed approach also extends to risk-based quasi-Bayesian formulations when first-order conditions are linear and can be transformed analogously. Overall, the proposed algorithms provide a practical and robust tool for quasi-Bayesian analysis in statistical applications.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [108] [An Efficient Computational Framework for Discrete Fuzzy Numbers Based on Total Orders](https://arxiv.org/abs/2511.17080)
*Arnau Mir,Alejandro Mus,Juan Vicente Riera*

Main category: cs.LO

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Discrete fuzzy numbers, and in particular those defined over a finite chain $L_n = \{0, \ldots, n\}$, have been effectively employed to represent linguistic information within the framework of fuzzy systems. Research on total (admissible) orderings of such types of fuzzy subsets, and specifically those belonging to the set $\mathcal{D}_1^{L_n\rightarrow Y_m}$ consisting of discrete fuzzy numbers $A$ whose support is a closed subinterval of the finite chain $L_n = \{0, 1, \ldots, n\}$ and whose membership values $A(x)$, for $x \in L_n$, belong to the set $Y_m = \{ 0 = y_1 < y_2 < \cdots < y_{m-1} < y_m = 1 \}$, has facilitated the development of new methods for constructing logical connectives, based on a bijective function, called $\textit{pos function}$, that determines the position of each $A \in \mathcal{D}_1^{L_n\rightarrow Y_m}$. For this reason, in this work we revisit the problem by introducing algorithms that exploit the combinatorial structure of total (admissible) orders to compute the $\textit{pos}$ function and its inverse with exactness. The proposed approach achieves a complexity of $\mathcal{O}(n^{2} m \log n)$, which is quadratic in the size of the underlying chain ($n$) and linear in the number of membership levels ($m$). The key point is that the dominant factor is $m$, ensuring scalability with respect to the granularity of membership values. The results demonstrate that this formulation substantially reduces computational cost and enables the efficient implementation of algebraic operations -- such as aggregation and implication -- on the set of discrete fuzzy numbers.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [109] [OmniScientist: Toward a Co-evolving Ecosystem of Human and AI Scientists](https://arxiv.org/abs/2511.16931)
*Chenyang Shao,Dehao Huang,Yu Li,Keyu Zhao,Weiquan Lin,Yining Zhang,Qingbin Zeng,Zhiyu Chen,Tianxing Li,Yifei Huang,Taozhong Wu,Xinyang Liu,Ruotong Zhao,Mengsheng Zhao,Xuhua Zhang,Yue Wang,Yuanyi Zhen,Fengli Xu,Yong Li,Tie-Yan Liu*

Main category: cs.CY

TL;DR: 现有AI科学家系统未考虑科研的社会性和协作性，本文提出OmniScientist框架，实现科研流程自动化并模拟人类科研系统，促进创新生态发展。


<details>
  <summary>Details</summary>
Motivation: 现有AI科学家系统将科学发现视为独立问题，忽视科研的社会协作本质，缺乏对关键维度建模，难以建立研究生态和与人类科研社区深度互动。

Method: 引入OmniScientist框架，将人类研究机制编码到AI科研工作流中，实现端到端自动化，并提供包括结构化知识系统、协作研究协议和开放评估平台的基础设施支持。

Result: 该框架使智能体能够理解和利用人类知识系统，实现协作和共同进化。

Conclusion: OmniScientist框架能促进可持续和可扩展的创新生态系统的形成。

Abstract: With the rapid development of Large Language Models (LLMs), AI agents have demonstrated increasing proficiency in scientific tasks, ranging from hypothesis generation and experimental design to manuscript writing. Such agent systems are commonly referred to as "AI Scientists." However, existing AI Scientists predominantly formulate scientific discovery as a standalone search or optimization problem, overlooking the fact that scientific research is inherently a social and collaborative endeavor. Real-world science relies on a complex scientific infrastructure composed of collaborative mechanisms, contribution attribution, peer review, and structured scientific knowledge networks. Due to the lack of modeling for these critical dimensions, current systems struggle to establish a genuine research ecosystem or interact deeply with the human scientific community. To bridge this gap, we introduce OmniScientist, a framework that explicitly encodes the underlying mechanisms of human research into the AI scientific workflow. OmniScientist not only achieves end-to-end automation across data foundation, literature review, research ideation, experiment automation, scientific writing, and peer review, but also provides comprehensive infrastructural support by simulating the human scientific system, comprising: (1) a structured knowledge system built upon citation networks and conceptual correlations; (2) a collaborative research protocol (OSP), which enables seamless multi-agent collaboration and human researcher participation; and (3) an open evaluation platform (ScienceArena) based on blind pairwise user voting and Elo rankings. This infrastructure empowers agents to not only comprehend and leverage human knowledge systems but also to collaborate and co-evolve, fostering a sustainable and scalable innovation ecosystem.

</details>


### [110] [Generative AI in Sociological Research: State of the Discipline](https://arxiv.org/abs/2511.16884)
*AJ Alvero,Dustin S. Stoltz,Oscar Stuhler,Marshall Taylor*

Main category: cs.CY

TL;DR: 研究通过对433位社会学期刊作者的调查，了解GenAI在社会学研究中的使用情况、原因、态度等，发现使用主要在写作辅助，不同研究者差异小，大家担忧其后果，信任度低，对其改进乐观但对领域影响看法不一。


<details>
  <summary>Details</summary>
Motivation: 在对GenAI既兴奋又怀疑的情况下，了解其在社会学研究中的实际使用情况。

Method: 对过去五年50本社会学期刊的433位作者进行调查。

Result: 约三分之一受访者至少每周使用，主要用于写作辅助；不同研究者在使用和态度上差异小；大家很担忧社会和环境后果，对输出信任度低；对GenAI改进乐观，但对其对领域影响看法不一。

Conclusion: GenAI在社会学研究中有一定使用，但存在诸多问题和分歧，其未来影响有待观察。

Abstract: Generative artificial intelligence (GenAI) has garnered considerable attention for its potential utility in research and scholarship, even among those who typically do not rely on computational tools. Early commentators, however, have also articulated concerns about how GenAI usage comes with enormous environmental costs, serious social risks, and a tendency to produce low-quality content. In the midst of both excitement and skepticism, it is crucial to take stock of how GenAI is actually being used. Our study focuses on sociological research as our site, and here we present findings from a survey of 433 authors of articles published in 50 sociology journals in the last five years. The survey provides an overview of the state of the discipline with regard to the use of GenAI by providing answers to fundamental questions: how (much) do scholars use the technology for their research; what are their reasons for using it; and how concerned, trustful, and optimistic are they about the technology? Of the approximately one third ofrespondents who self-report using GenAI at least weekly, the primary uses are for writing assistance and comparatively less so in planning, data collection, or data analysis. In both use and attitudes, there are surprisingly few differences between self-identified computational and non-computational researchers. Generally, respondents are very concerned about the social and environmental consequences of GenAI. Trust in GenAI outputs is low, regardless of expertise or frequency of use. While optimism that GenAI will improve is high, scholars are divided on whether GenAI will have a positive impact on the field.

</details>


### [111] [AI Workers, Geopolitics, and Algorithmic Collective Action](https://arxiv.org/abs/2511.17331)
*Sydney Reis*

Main category: cs.CY

TL;DR: 本文基于国际政治经济学理论，探讨人工智能治理问题，认为AI工作者可作为地缘政治参与者，治理需结合自下而上干预，提议用参与式设计方法让AI工作者参与。


<details>
  <summary>Details</summary>
Motivation: 解释国际和国家层面人工智能治理发展、应用和执行不均衡的原因，探讨如何确保人工智能负责任、道德和稳健发展。

Method: 借鉴参与式设计方法，让AI工作者作为知识、权力和意图来源参与。

Result: 提出AI工作者可作为地缘政治参与者，强调自下而上干预的重要性。

Conclusion: 治理不足以确保AI的良好发展，应重视AI工作者自下而上的干预，促进算法集体行动。

Abstract: According to the theory of International Political Economy (IPE), states are often incentivized to rely on rather than constrain powerful corporations. For this reason, IPE provides a useful lens to explain why efforts to govern Artificial Intelligence (AI) at the international and national levels have thus far been developed, applied, and enforced unevenly. Building on recent work that explores how AI companies engage in geopolitics, this position paper argues that some AI workers can be considered actors of geopolitics. It makes the timely case that governance alone cannot ensure responsible, ethical, or robust AI development and use, and greater attention should be paid to bottom-up interventions at the site of AI development. AI workers themselves should be situated as individual agents of change, especially when considering their potential to foster Algorithmic Collective Action (ACA). Drawing on methods of Participatory Design (PD), this paper proposes engaging AI workers as sources of knowledge, relative power, and intentionality to encourage more responsible and just AI development and create the conditions that can facilitate ACA.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [112] [Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation](https://arxiv.org/abs/2511.16757)
*Wei-Cheng Tseng,Xuanru Zhou,Mingyue Huo,Yiwen Shao,Hao Zhang,Dong Yu*

Main category: eess.AS

TL;DR: 本文指出音频-语言预训练存在的问题，引入CaptionStew数据集进行评估，发现不同目标的优势，表明其是通用音频表征的可行途径并开源相关资源。


<details>
  <summary>Details</summary>
Motivation: 当前音频-语言预训练研究不足，现有模型应用受限，存在数据、标注多样性及评估等问题，需探索通用音频理解方法。

Method: 引入CaptionStew数据集，对对比学习和字幕生成目标在多种音频任务上进行综合评估，开展系统的数据规模实验。

Result: 音频-语言预训练可得到有竞争力和可迁移的表征；对比学习在小数据规模时数据效率高，字幕生成在涉及语言的音频理解任务上扩展性好；常见监督初始化方法在大规模数据下收益递减。

Conclusion: 音频-语言预训练是实现通用音频表征的可行途径，为未来研究提供指导，开源资源推动通用音频理解发展。

Abstract: Audio-language pretraining holds promise for general-purpose audio understanding, yet remains underexplored compared to its vision counterpart. While vision-language models like CLIP serve as widely adopted foundations, existing audio-language models primarily excel at retrieval tasks with limited adoption as general-purpose encoders. We identify three key barriers: limited large-scale audio-text corpora, insufficient caption diversity, and lack of systematic exploration and evaluation. To this end, we introduce CaptionStew, a 10.7M caption dataset aggregating diverse open-source audio-text corpora across multiple domains and captioning styles. Using this resource, we conduct the first comprehensive evaluation comparing contrastive and captioning objectives for audio representation learning across speech, music, and environmental sound tasks. Our results demonstrate that audio-language pretraining yields competitive, transferable representations. Through systematic data-scaling experiments, we reveal complementary objective strengths: contrastive learning achieves superior data efficiency at smaller scales, while captioning demonstrates better scalability on language-involved audio understanding tasks. We also find that common supervised initialization practices provide diminishing returns at scale, challenging current approaches. These findings establish audio-language pretraining as a viable pathway toward general-purpose audio representations, guiding future research. To accelerate progress, we release data preparation recipes, training protocols, and pretrained models, paving the way toward universal audio understanding.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [113] [U.S. Economy and Global Stock Markets: Insights from a Distributional Approach](https://arxiv.org/abs/2511.17140)
*Ping Wu,Dan Zhu*

Main category: econ.GN

TL;DR: 本文超越传统股市指数，用矩阵函数VAR方法分析横截面收益分布，揭示金融市场对宏观经济动态的预测能力及货币政策新的传导效应，提供条件预测工具。


<details>
  <summary>Details</summary>
Motivation: 金融市场相互关联，传统股市指数分析有局限，需研究横截面收益分布以更好理解金融市场与宏观经济关系。

Method: 开发矩阵函数VAR方法，从横截面收益分布中提取可解释因子，联合建模分布与美国宏观经济指标。

Result: 发现美国紧缩货币政策不仅降低全球股票回报，还抑制横截面收益峰度。

Conclusion: 该框架能进行条件预测，为政策制定者评估不同经济情景下宏观金融联系提供灵活工具。

Abstract: Financial markets are interconnected, with micro-currents propagating across global markets and shaping economic trends. This paper moves beyond traditional stock market indices to examine cross-sectional return distributions-15 in our empirical application, each representing a distinct global market. To facilitate this analysis, we develop a matrix functional VAR method with interpretable factors extracted from cross-sectional return distributions. Our approach extends the existing framework from modeling a single function to multiple functions, allowing for a richer representation of cross-sectional dependencies. By jointly modeling these distributions with U.S. macroeconomic indicators, we uncover the predictive power of financial market in forecasting macro-economic dynamics. Our findings reveal that U.S. contractionary monetary policy not only lowers global stock returns, as traditionally understood, but also dampens cross-sectional return kurtosis, highlighting an overlooked policy transmission. This framework enables conditional forecasting, equipping policymakers with a flexible tool to assess macro-financial linkages under different economic scenarios.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [114] [Optimizing PyTorch Inference with LLM-Based Multi-Agent Systems](https://arxiv.org/abs/2511.16964)
*Kirill Nagaitsev,Luka Grbcic,Samuel Williams,Costin Iancu*

Main category: cs.MA

TL;DR: 提出比较多智能体PyTorch优化系统的逻辑框架，评估表明利用型策略与纠错智能体配合最佳，最佳实现有显著加速。


<details>
  <summary>Details</summary>
Motivation: 解决现代AI推理系统在GPU硬件上性能最大化的挑战，探索多智能体系统在该任务中的动态。

Method: 提出逻辑框架来比较多智能体PyTorch优化系统。

Result: 利用型策略与纠错智能体配合表现最佳，性能与优化步骤粒度相关，最佳实现在H100 GPU上有2.88倍加速。

Conclusion: 所提出的框架能有效优化多智能体PyTorch系统，提升在GPU上的性能。

Abstract: Maximizing performance on available GPU hardware is an ongoing challenge for modern AI inference systems. Traditional approaches include writing custom GPU kernels and using specialized model compilers to tune high-level code for specific GPU targets. Recent work shows that LLM-based multi-agent systems can effectively perform such tuning, often outperforming existing compilers and eliminating the need for manual kernel development. However, the dynamics of multi-agent systems for this task remain unexplored. In this work, we present a logical framework for comparing multi-agent PyTorch optimization systems. Our evaluation shows that exploit-heavy strategies perform best when paired with error-fixing agents, and that performance correlates with the granularity of optimization steps. The best implementation achieves an average 2.88x speedup on an H100 GPU across diverse tasks in KernelBench, a benchmark suite covering a range of machine learning architectures in PyTorch.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [115] [Diffusion-Inversion-Net (DIN): An End-to-End Direct Probabilistic Framework for Characterizing Hydraulic Conductivities and Quantifying Uncertainty](https://arxiv.org/abs/2511.16926)
*Xun Zhang,Weijie Yang,Jiangjiang Zhang,Simin Jiang*

Main category: physics.geo-ph

TL;DR: 提出Diffusion - Inversion - Net (DIN)框架用于地下水流动和溶质运移过程反演建模，结果表明其能生成多组满足约束的实现，准确估计参数场并量化不确定性，泛化能力强。


<details>
  <summary>Details</summary>
Motivation: 为地下水流动和溶质运移过程的反演建模提供有效方法。

Method: 利用离线训练的去噪扩散概率模型（DDPM）作为先验学习器，通过条件注入机制融合多源观测数据，利用随机采样和概率建模机制，重复执行反向去噪过程直接生成后验参数场集合。

Result: DIN能在相同观测条件下产生多组满足约束的实现，准确估计水力传导率场，实现可靠的不确定性量化。

Conclusion: DIN框架具有强泛化能力，是传统多阶段反演方法的有力统一替代方案。

Abstract: We propose the Diffusion-Inversion-Net (DIN) framework for inverse modeling of groundwater flow and solute transport processes. DIN utilizes an offline-trained Denoising Diffusion Probabilistic Model (DDPM) as a powerful prior leaner, which flexibly incorporates sparse, multi-source observational data, including hydraulic head, solute concentration, and hard conductivity data, through conditional injection mechanisms. These conditioning inputs subsequently guide the generative inversion process during sampling. Bypassing iterative forward simulations, DIN leverages stochastic sampling and probabilistic modeling mechanisms to directly generate ensembles of posterior parameter fields by repeatedly executing the reverse denoising process. Two representative posterior scenarios, Gaussian and non-Gaussian, are investigated. The results demonstrate that DIN can produce multiple constraint-satisfying realizations under identical observational conditions, accurately estimate hydraulic-conductivity fields, and achieve reliable uncertainty quantification. The framework exhibits strong generalization capability across diverse data distributions, offering a robust and unified alternative to conventional multi-stage inversion methodologies.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [116] [Covariate Connectivity Combined Clustering for Weighted Networks](https://arxiv.org/abs/2511.17302)
*Zeyu Hu,Wenrui Li,Jun Yan,Panpan Zhang*

Main category: stat.ME

TL;DR: 提出自适应谱聚类算法C4，结合网络连通性和节点属性，在模拟和实际应用中表现良好。


<details>
  <summary>Details</summary>
Motivation: 传统社区检测算法依赖网络拓扑，现有协变量辅助方法有局限性，如假设簇数量已知、计算密集或不适用于加权网络。

Method: 提出C4算法，将网络连通性和节点级协变量整合到统一的相似性表示中，通过数据驱动的调整参数平衡信息，用特征间隙启发式估计社区数量。

Result: 模拟研究表明C4在不同场景下比其他方法更准确和稳健，机场可达性网络应用展示了其可扩展性、可解释性和实用性。

Conclusion: C4算法是一种有效的社区检测方法，适用于加权网络，具有实际应用价值。

Abstract: Community detection is a central task in network analysis, with applications in social, biological, and technological systems. Traditional algorithms rely primarily on network topology, which can fail when community signals are partly encoded in node-specific attributes. Existing covariate-assisted methods often assume the number of clusters is known, involve computationally intensive inference, or are not designed for weighted networks. We propose $\text{C}^4$: Covariate Connectivity Combined Clustering, an adaptive spectral clustering algorithm that integrates network connectivity and node-level covariates into a unified similarity representation. $\text{C}^4$ balances the two sources of information through a data-driven tuning parameter, estimates the number of communities via an eigengap heuristic, and avoids reliance on costly sampling-based procedures. Simulation studies show that $\text{C}^4$ achieves higher accuracy and robustness than competing approaches across diverse scenarios. Application to an airport reachability network demonstrates the method's scalability, interpretability, and practical utility for real-world weighted networks.

</details>


### [117] [Iterating marginalized Bayes maps for likelihood maximization with application to nonlinear panel models](https://arxiv.org/abs/2511.17438)
*Jesse Wheeler,Aaron J. Abkemeier,Edward L. Ionides*

Main category: stat.ME

TL;DR: 传统蒙特卡罗推理程序在高维系统中计算不可行，本文研究用迭代滤波算法处理面板数据，引入新算法拓宽动态模型适用范围。


<details>
  <summary>Details</summary>
Motivation: 传统蒙特卡罗推理程序在高维系统计算不可行，面板数据普遍存在但现有程序处理非线性、非高斯机械模型有实际困难，需开发有效推理算法。

Method: 研究使用迭代滤波算法，引入含边缘化步骤的新算法以缓解高维粒子滤波问题。

Result: 新算法使之前难处理的模型能进行基于似然的推理。

Conclusion: 新算法拓宽了面板数据分析可用的动态模型范围。

Abstract: Complex dynamic systems can be investigated by fitting mechanistic stochastic dynamic models to time series data. In this context, commonly used Monte Carlo inference procedures for model selection and parameter estimation quickly become computationally unfeasible as the system dimension grows. The increasing prevalence of panel data, characterized by multiple related time series, therefore necessitates the development of inference algorithms that are effective for this class of high-dimensional mechanistic models. Nonlinear, non-Gaussian mechanistic models are routinely fitted to time series data but seldom to panel data, despite its widespread availability, suggesting that the practical difficulties for existing procedures are prohibitive. We investigate the use of iterated filtering algorithms for this purpose. We introduce a novel algorithm that contains a marginalization step that mitigates issues arising from particle filtering in high dimensions. Our approach enables likelihood-based inference for models that were previously considered intractable, thus broadening the scope of dynamic models available for panel data analysis.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [118] [Efficient Penalty-Based Bilevel Methods: Improved Analysis, Novel Updates, and Flatness Condition](https://arxiv.org/abs/2511.16796)
*Liuyuan Jiang,Quan Xiao,Lisha Chen,Tianyi Chen*

Main category: math.OC

TL;DR: 本文提出新罚函数重写方法，改进平滑常数分析，提出新算法PBGD - Free，给出曲率条件，经严格收敛分析和实验验证。


<details>
  <summary>Details</summary>
Motivation: 现有基于罚函数的双层优化（BLO）方法存在内循环迭代和步长小导致复杂度次优的问题。

Method: 采用新罚函数重写解耦上下层变量，提出PBGD - Free算法，给出新曲率条件。

Result: 改进平滑常数分析，减少迭代复杂度，放松传统上层Lipschitz条件，可选择更小罚常数。

Conclusion: 所提方法经严格收敛分析，在超参数优化和大语言模型微调实验中验证了有效性。

Abstract: Penalty-based methods have become popular for solving bilevel optimization (BLO) problems, thanks to their effective first-order nature. However, they often require inner-loop iterations to solve the lower-level (LL) problem and small outer-loop step sizes to handle the increased smoothness induced by large penalty terms, leading to suboptimal complexity. This work considers the general BLO problems with coupled constraints (CCs) and leverages a novel penalty reformulation that decouples the upper- and lower-level variables. This yields an improved analysis of the smoothness constant, enabling larger step sizes and reduced iteration complexity for Penalty-Based Gradient Descent algorithms in ALTernating fashion (ALT-PBGD). Building on the insight of reduced smoothness, we propose PBGD-Free, a novel fully single-loop algorithm that avoids inner loops for the uncoupled constraint BLO. For BLO with CCs, PBGD-Free employs an efficient inner-loop with substantially reduced iteration complexity. Furthermore, we propose a novel curvature condition describing the "flatness" of the upper-level objective with respect to the LL variable. This condition relaxes the traditional upper-level Lipschitz requirement, enables smaller penalty constant choices, and results in a negligible penalty gradient term during upper-level variable updates. We provide rigorous convergence analysis and validate the method's efficacy through hyperparameter optimization for support vector machines and fine-tuning of large language models.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [119] [MRI Super-Resolution with Deep Learning: A Comprehensive Survey](https://arxiv.org/abs/2511.16854)
*Mohammad Khateri,Serge Vasylechko,Morteza Ghahremani,Liam Timms,Deniz Kocanaogullari,Simon K. Warfield,Camilo Jaimes,Davood Karimi,Alejandra Sierra,Jussi Tohka,Sila Kurugol,Onur Afacan*

Main category: eess.IV

TL;DR: 本文综述MRI超分辨率技术进展，聚焦深度学习方法，提出分类法，研究相关技术，指出挑战和方向并提供资源。


<details>
  <summary>Details</summary>
Motivation: 高分辨率MRI成本高且受技术限制，超分辨率是有前景的计算方法，可解决这些挑战。

Method: 从计算机视觉、计算成像、逆问题和MR物理等角度研究基于深度学习的MRI超分辨率方法，提出系统分类法。

Result: 对新兴和成熟的SR技术进行深入研究，指出开放挑战和方向，提供开源资源。

Conclusion: 超分辨率是解决高分辨率MRI挑战的有效途径，社区需应对相关开放挑战。

Abstract: High-resolution (HR) magnetic resonance imaging (MRI) is crucial for many clinical and research applications. However, achieving it remains costly and constrained by technical trade-offs and experimental limitations. Super-resolution (SR) presents a promising computational approach to overcome these challenges by generating HR images from more affordable low-resolution (LR) scans, potentially improving diagnostic accuracy and efficiency without requiring additional hardware. This survey reviews recent advances in MRI SR techniques, with a focus on deep learning (DL) approaches. It examines DL-based MRI SR methods from the perspectives of computer vision, computational imaging, inverse problems, and MR physics, covering theoretical foundations, architectural designs, learning strategies, benchmark datasets, and performance metrics. We propose a systematic taxonomy to categorize these methods and present an in-depth study of both established and emerging SR techniques applicable to MRI, considering unique challenges in clinical and research contexts. We also highlight open challenges and directions that the community needs to address. Additionally, we provide a collection of essential open-access resources, tools, and tutorials, available on our GitHub: https://github.com/mkhateri/Awesome-MRI-Super-Resolution.
  IEEE keywords: MRI, Super-Resolution, Deep Learning, Computational Imaging, Inverse Problem, Survey.

</details>


### [120] [MedImageInsight for Thoracic Cavity Health Classification from Chest X-rays](https://arxiv.org/abs/2511.17043)
*Rama Krishna Boya,Mohan Kireeti Magalanadu,Azaruddin Palavalli,Rupa Ganesh Tekuri,Amrit Pattanayak,Prasanthi Enuga,Vignesh Esakki Muthu,Vivek Aditya Boya*

Main category: eess.IV

TL;DR: 研究用MedImageInsight对胸片进行自动二分类，微调分类器性能最佳，该系统可集成到工作流程减轻放射科医生负担，未来将扩展到多标签病理分类。


<details>
  <summary>Details</summary>
Motivation: 胸片检查量增加和放射科医生工作量大，挑战及时诊断，需自动化方法。

Method: 评估两种方法，一是微调MedImageInsight进行端到端分类，二是将其作为特征提取器用于传统机器学习分类器的迁移学习管道。使用ChestX - ray14数据集和合作医院的真实临床数据进行实验。

Result: 微调分类器性能最高，ROC - AUC为0.888，校准优于迁移学习模型，性能与CheXNet相当。

Conclusion: 基础医学影像模型能减少特定任务训练需求，同时保持诊断可靠性，系统可集成到工作流程，未来将扩展到多标签病理分类。

Abstract: Chest radiography remains one of the most widely used imaging modalities for thoracic diagnosis, yet increasing imaging volumes and radiologist workload continue to challenge timely interpretation. In this work, we investigate the use of MedImageInsight, a medical imaging foundational model, for automated binary classification of chest X-rays into Normal and Abnormal categories. Two approaches were evaluated: (1) fine-tuning MedImageInsight for end-to-end classification, and (2) employing the model as a feature extractor for a transfer learning pipeline using traditional machine learning classifiers. Experiments were conducted using a combination of the ChestX-ray14 dataset and real-world clinical data sourced from partner hospitals. The fine-tuned classifier achieved the highest performance, with an ROC-AUC of 0.888 and superior calibration compared to the transfer learning models, demonstrating performance comparable to established architectures such as CheXNet. These results highlight the effectiveness of foundational medical imaging models in reducing task-specific training requirements while maintaining diagnostic reliability. The system is designed for integration into web-based and hospital PACS workflows to support triage and reduce radiologist burden. Future work will extend the model to multi-label pathology classification to provide preliminary diagnostic interpretation in clinical environments.

</details>


### [121] [OmniLens++: Blind Lens Aberration Correction via Large LensLib Pre-Training and Latent PSF Representation](https://arxiv.org/abs/2511.17126)
*Qi Jiang,Xiaolong Qian,Yao Gao,Lei Sun,Kailun Yang,Zhonghua Yi,Wenyong Li,Ming-Hsuan Yang,Luc Van Gool,Kaiwei Wang*

Main category: eess.IV

TL;DR: 本文提出OmniLens++框架用于盲镜头像差校正，解决现有管道泛化能力的挑战，实验表明其有先进泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决现有镜头库预训练管道在数据扩展困难和缺乏光学退化先验指导方面的问题，提升泛化能力。

Method: 扩展设计规格增加数据多样性，量化光学退化模式和严重程度；提出潜在点扩散函数表示（LPR），引入VQVAE框架学习PSF潜在特征并建模光学退化过程。

Result: OmniLens++在盲像差校正中表现出先进的泛化能力，AODLibpro可作为跨多种像差训练的可扩展基础，LPR可挖掘大规模LensLib潜力。

Conclusion: OmniLens++框架有效解决现有管道问题，在盲像差校正中具有良好性能和泛化能力。

Abstract: Emerging deep-learning-based lens library pre-training (LensLib-PT) pipeline offers a new avenue for blind lens aberration correction by training a universal neural network, demonstrating strong capability in handling diverse unknown optical degradations. This work proposes the OmniLens++ framework, which resolves two challenges that hinder the generalization ability of existing pipelines: the difficulty of scaling data and the absence of prior guidance characterizing optical degradation. To improve data scalability, we expand the design specifications to increase the degradation diversity of the lens source, and we sample a more uniform distribution by quantifying the spatial-variation patterns and severity of optical degradation. In terms of model design, to leverage the Point Spread Functions (PSFs), which intuitively describe optical degradation, as guidance in a blind paradigm, we propose the Latent PSF Representation (LPR). The VQVAE framework is introduced to learn latent features of LensLib's PSFs, which is assisted by modeling the optical degradation process to constrain the learning of degradation priors. Experiments on diverse aberrations of real-world lenses and synthetic LensLib show that OmniLens++ exhibits state-of-the-art generalization capacity in blind aberration correction. Beyond performance, the AODLibpro is verified as a scalable foundation for more effective training across diverse aberrations, and LPR can further tap the potential of large-scale LensLib. The source code and datasets will be made publicly available at https://github.com/zju-jiangqi/OmniLens2.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [122] [A Little More Like This: Text-to-Image Retrieval with Vision-Language Models Using Relevance Feedback](https://arxiv.org/abs/2511.17255)
*Bulat Khaertdinov,Mirela Popa,Nava Tintarev*

Main category: cs.CV

TL;DR: 本文提出基于传统文本搜索的相关性反馈机制提升视觉语言模型（VLM）检索性能，评估四种反馈策略，实验表明能提升检索效果。


<details>
  <summary>Details</summary>
Motivation: 提升VLM检索性能，避免常需的微调与模型扩展。

Method: 引入并评估四种反馈策略，包括改进的伪相关性反馈（PRF）、生成式相关性反馈（GRF）、注意力反馈汇总器（AFS）和模拟显式反馈。

Result: GRF、AFS和显式反馈能提升检索性能，AFS能缓解查询漂移，在多轮检索中更稳健。

Conclusion: 相关性反馈能持续提升VLM检索性能，为交互式和自适应视觉搜索提供机会。

Abstract: Large vision-language models (VLMs) enable intuitive visual search using natural language queries. However, improving their performance often requires fine-tuning and scaling to larger model variants. In this work, we propose a mechanism inspired by traditional text-based search to improve retrieval performance at inference time: relevance feedback. While relevance feedback can serve as an alternative to fine-tuning, its model-agnostic design also enables use with fine-tuned VLMs. Specifically, we introduce and evaluate four feedback strategies for VLM-based retrieval. First, we revise classical pseudo-relevance feedback (PRF), which refines query embeddings based on top-ranked results. To address its limitations, we propose generative relevance feedback (GRF), which uses synthetic captions for query refinement. Furthermore, we introduce an attentive feedback summarizer (AFS), a custom transformer-based model that integrates multimodal fine-grained features from relevant items. Finally, we simulate explicit feedback using ground-truth captions as an upper-bound baseline. Experiments on Flickr30k and COCO with the VLM backbones show that GRF, AFS, and explicit feedback improve retrieval performance by 3-5% in MRR@5 for smaller VLMs, and 1-3% for larger ones, compared to retrieval with no feedback. Moreover, AFS, similarly to explicit feedback, mitigates query drift and is more robust than GRF in iterative, multi-turn retrieval settings. Our findings demonstrate that relevance feedback can consistently enhance retrieval across VLMs and open up opportunities for interactive and adaptive visual search.

</details>


### [123] [PairHuman: A High-Fidelity Photographic Dataset for Customized Dual-Person Generation](https://arxiv.org/abs/2511.16712)
*Ting Pan,Ye Wang,Peiguang Jing,Rui Ma,Zili Yi,Yu Liu*

Main category: cs.CV

TL;DR: 提出PairHuman数据集和DHumanDiff方法用于双人肖像生成，实验证明其效果好且数据集公开。


<details>
  <summary>Details</summary>
Motivation: 个性化双人肖像定制有应用潜力，但缺乏基准数据集阻碍高质量定制，因此提出PairHuman数据集。

Method: 提出PairHuman数据集，包含超10万张图像及丰富元数据；引入DHumanDiff基线方法用于双人肖像生成。

Result: 实验结果表明数据集和方法能生成高度定制、视觉质量优越且符合人类偏好的肖像。

Conclusion: 所提出的数据集和方法有效，数据集已公开。

Abstract: Personalized dual-person portrait customization has considerable potential applications, such as preserving emotional memories and facilitating wedding photography planning. However, the absence of a benchmark dataset hinders the pursuit of high-quality customization in dual-person portrait generation. In this paper, we propose the PairHuman dataset, which is the first large-scale benchmark dataset specifically designed for generating dual-person portraits that meet high photographic standards. The PairHuman dataset contains more than 100K images that capture a variety of scenes, attire, and dual-person interactions, along with rich metadata, including detailed image descriptions, person localization, human keypoints, and attribute tags. We also introduce DHumanDiff, which is a baseline specifically crafted for dual-person portrait generation that features enhanced facial consistency and simultaneously balances in personalized person generation and semantic-driven scene creation. Finally, the experimental results demonstrate that our dataset and method produce highly customized portraits with superior visual quality that are tailored to human preferences. Our dataset is publicly available at https://github.com/annaoooo/PairHuman.

</details>


### [124] [A Machine Learning-Driven Solution for Denoising Inertial Confinement Fusion Images](https://arxiv.org/abs/2511.16717)
*Asya Y. Akkus,Bradley T. Wolfe,Pinghan Chu,Chengkun Huang,Chris S. Campbell,Mariana Alvarado Alvarez,Petr Volegov,David Fittinghoff,Robert Reinovsky,Zhehui Wang*

Main category: cs.CV

TL;DR: 本文提出用带CDF 97小波变换的无监督自编码器对中子成像数据进行混合高斯 - 泊松去噪，效果良好。


<details>
  <summary>Details</summary>
Motivation: 中子成像对ICF事件分析重要，但图像常受高斯和泊松噪声干扰，传统方法难去除，且以往因缺乏真实数据少用机器学习方法，近期合成数据发展带来新机遇。

Method: 实现带Cohen - Daubechies - Feauveau (CDF 97) 小波变换的无监督自编码器进行混合高斯 - 泊松去噪。

Result: 网络成功对中子成像数据去噪，与正向模型生成数据对比及和非机器学习滤波机制对比，显示出较低的重建误差和更好的边缘保留指标。

Conclusion: 该方法在中子图像降噪和ICF实验三维重建分析方面有前景。

Abstract: Neutron imaging is important in optimizing analysis of inertial confinement fusion (ICF) events such as those at the National Ignition Facility (NIF) and improving current and future ICF platforms. However, images of neutron sources are often degraded by various types of noise. Most commonly, Gaussian and Poisson noise often coexist within one image, obscuring fine details and blurring edges. These noise types often overlap, making them difficult to distinguish and remove using conventional filtering and thresholding methods. As a result, noise removal techniques that preserve image fidelity are important for analyzing and interpreting images of a neutron source. Current solutions include a combination of filtering and thresholding methodologies. In the past, machine learning approaches were rarely implemented due to a lack of ground truth neutron imaging data for ICF processes. However, recent advances in synthetic data production, particularly in the fusion imaging field, have opened opportunities to investigate new denoising procedures using both supervised and unsupervised machine learning methods. In this study, we implement an unsupervised autoencoder with a Cohen-Daubechies- Feauveau (CDF 97) wavelet transform in the latent space for mixed Gaussian-Poisson denoising. The network successfully denoises neutron imaging data. Additionally, it demonstrates lower reconstruction error and superior edge preservation metrics when benchmarked with data generated by a forward model and compared to non-ML-based filtering mechanisms such as Block-matching and 3D filtering (BM3D). This approach presents a promising advancement in neutron image noise reduction and three-dimensional reconstruction analysis of ICF experiments.

</details>


### [125] [SAM 3: Segment Anything with Concepts](https://arxiv.org/abs/2511.16719)
*Nicolas Carion,Laura Gustafson,Yuan-Ting Hu,Shoubhik Debnath,Ronghang Hu,Didac Suris,Chaitanya Ryali,Kalyan Vasudev Alwala,Haitham Khedr,Andrew Huang,Jie Lei,Tengyu Ma,Baishan Guo,Arpit Kalla,Markus Marks,Joseph Greer,Meng Wang,Peize Sun,Roman Rädle,Triantafyllos Afouras,Effrosyni Mavroudi,Katherine Xu,Tsung-Han Wu,Yu Zhou,Liliane Momeni,Rishi Hazra,Shuangrui Ding,Sagar Vaze,Francois Porcher,Feng Li,Siyuan Li,Aishwarya Kamath,Ho Kei Cheng,Piotr Dollár,Nikhila Ravi,Kate Saenko,Pengchuan Zhang,Christoph Feichtenhofer*

Main category: cs.CV

TL;DR: 提出SAM 3模型，可基于概念提示进行图像和视频中对象的检测、分割和跟踪，构建数据集，模型提升了PCS准确率并开源。


<details>
  <summary>Details</summary>
Motivation: 推动Promptable Concept Segmentation (PCS) 发展，提升现有系统在图像和视频PCS中的准确率。

Method: 构建可扩展数据引擎生成高质量数据集，模型由图像级检测器和基于内存的视频跟踪器组成，用存在头解耦识别和定位。

Result: SAM 3在图像和视频PCS中使现有系统准确率翻倍，提升了先前SAM在视觉分割任务上的能力。

Conclusion: 开源SAM 3和SA - Co基准，推动可提示概念分割领域发展。

Abstract: We present Segment Anything Model (SAM) 3, a unified model that detects, segments, and tracks objects in images and videos based on concept prompts, which we define as either short noun phrases (e.g., "yellow school bus"), image exemplars, or a combination of both. Promptable Concept Segmentation (PCS) takes such prompts and returns segmentation masks and unique identities for all matching object instances. To advance PCS, we build a scalable data engine that produces a high-quality dataset with 4M unique concept labels, including hard negatives, across images and videos. Our model consists of an image-level detector and a memory-based video tracker that share a single backbone. Recognition and localization are decoupled with a presence head, which boosts detection accuracy. SAM 3 doubles the accuracy of existing systems in both image and video PCS, and improves previous SAM capabilities on visual segmentation tasks. We open source SAM 3 along with our new Segment Anything with Concepts (SA-Co) benchmark for promptable concept segmentation.

</details>


### [126] [SafeR-CLIP: Mitigating NSFW Content in Vision-Language Models While Preserving Pre-Trained Knowledge](https://arxiv.org/abs/2511.16743)
*Adeel Yousaf,Joseph Fioresi,James Beetham,Amrit Singh Bedi,Mubarak Shah*

Main category: cs.CV

TL;DR: 微调提升CLIP等视觉语言模型安全性会降低泛化性能，提出SaFeR - CLIP框架和NSFW - Caps基准，能兼顾安全与性能。


<details>
  <summary>Details</summary>
Motivation: 解决微调提升视觉语言模型安全性时导致泛化性能大幅下降的问题。

Method: 提出接近感知方法，将不安全概念重定向到语义最接近的安全替代方案，引入SaFeR - CLIP微调框架，还贡献NSFW - Caps基准。

Result: SaFeR - CLIP能使零样本准确率比先前方法提高达8.0%，同时保持强大安全性。

Conclusion: 尊重预训练表示的几何结构是在不牺牲性能的前提下实现安全的关键。

Abstract: Improving the safety of vision-language models like CLIP via fine-tuning often comes at a steep price, causing significant drops in their generalization performance. We find this trade-off stems from rigid alignment strategies that force unsafe concepts toward single, predefined safe targets, disrupting the model's learned semantic structure. To address this, we propose a proximity-aware approach: redirecting unsafe concepts to their semantically closest safe alternatives to minimize representational change. We introduce SaFeR-CLIP, a fine-tuning framework that applies this principle of minimal intervention. SaFeR-CLIP successfully reconciles safety and performance, recovering up to 8.0% in zero-shot accuracy over prior methods while maintaining robust safety. To support more rigorous evaluation, we also contribute NSFW-Caps, a new benchmark of 1,000 highly-aligned pairs for testing safety under distributional shift. Our work shows that respecting the geometry of pretrained representations is key to achieving safety without sacrificing performance.

</details>


### [127] [Mesh RAG: Retrieval Augmentation for Autoregressive Mesh Generation](https://arxiv.org/abs/2511.16807)
*Xiatao Sun,Chen Liang,Qian Wang,Daniel Rakita*

Main category: cs.CV

TL;DR: 传统3D网格手动创建耗时且难扩展，现有自回归模型有质量 - 速度权衡问题，提出Mesh RAG框架解决这些问题。


<details>
  <summary>Details</summary>
Motivation: 传统3D网格手动创建效率低，现有自回归模型存在质量 - 速度权衡及增量编辑困难的问题，需改进。

Method: 提出Mesh RAG框架，受语言模型RAG启发，利用点云分割、空间变换和点云配准来检索、生成和集成网格组件。

Result: 在多种基础自回归网格生成模型上展示了广泛适用性，显著提升网格质量、加快生成速度，且无需重新训练模型即可进行增量编辑。

Conclusion: Mesh RAG框架能有效克服现有自回归网格生成模型的局限性，实现高效、高质量的网格生成和编辑。

Abstract: 3D meshes are a critical building block for applications ranging from industrial design and gaming to simulation and robotics. Traditionally, meshes are crafted manually by artists, a process that is time-intensive and difficult to scale. To automate and accelerate this asset creation, autoregressive models have emerged as a powerful paradigm for artistic mesh generation. However, current methods to enhance quality typically rely on larger models or longer sequences that result in longer generation time, and their inherent sequential nature imposes a severe quality-speed trade-off. This sequential dependency also significantly complicates incremental editing. To overcome these limitations, we propose Mesh RAG, a novel, training-free, plug-and-play framework for autoregressive mesh generation models. Inspired by RAG for language models, our approach augments the generation process by leveraging point cloud segmentation, spatial transformation, and point cloud registration to retrieve, generate, and integrate mesh components. This retrieval-based approach decouples generation from its strict sequential dependency, facilitating efficient and parallelizable inference. We demonstrate the wide applicability of Mesh RAG across various foundational autoregressive mesh generation models, showing it significantly enhances mesh quality, accelerates generation speed compared to sequential part prediction, and enables incremental editing, all without model retraining.

</details>


### [128] [WorldGen: From Text to Traversable and Interactive 3D Worlds](https://arxiv.org/abs/2511.16825)
*Dilin Wang,Hyunyoung Jung,Tom Monnier,Kihyuk Sohn,Chuhang Zou,Xiaoyu Xiang,Yu-Ying Yeh,Di Liu,Zixuan Huang,Thu Nguyen-Phuoc,Yuchen Fan,Sergiu Oprea,Ziyan Wang,Roman Shapovalov,Nikolaos Sarafianos,Thibault Groueix,Antoine Toisoul,Prithviraj Dhar,Xiao Chu,Minghao Chen,Geon Yeong Park,Mahima Gupta,Yassir Azziz,Rakesh Ranjan,Andrea Vedaldi*

Main category: cs.CV

TL;DR: 介绍WorldGen系统，可根据文本提示自动创建大规模交互式3D世界，支持细粒度控制，推动3D生成AI发展。


<details>
  <summary>Details</summary>
Motivation: 弥合创意意图与功能性虚拟空间之间的差距，让创作者无需手动建模和专业3D知识就能设计连贯、可导航的世界。

Method: 结合大语言模型驱动的场景布局推理、过程式生成、基于扩散的3D生成和对象感知的场景分解。

Result: 创建出几何一致、视觉丰富且可实时高效渲染的3D世界，系统全模块化，支持细粒度控制。

Conclusion: 该工作朝着可扩展、可访问的生成式世界构建迈出一步，推进了3D生成AI在游戏、模拟和沉浸式社交环境中的应用。

Abstract: We introduce WorldGen, a system that enables the automatic creation of large-scale, interactive 3D worlds directly from text prompts. Our approach transforms natural language descriptions into traversable, fully textured environments that can be immediately explored or edited within standard game engines. By combining LLM-driven scene layout reasoning, procedural generation, diffusion-based 3D generation, and object-aware scene decomposition, WorldGen bridges the gap between creative intent and functional virtual spaces, allowing creators to design coherent, navigable worlds without manual modeling or specialized 3D expertise. The system is fully modular and supports fine-grained control over layout, scale, and style, producing worlds that are geometrically consistent, visually rich, and efficient to render in real time. This work represents a step towards accessible, generative world-building at scale, advancing the frontier of 3D generative AI for applications in gaming, simulation, and immersive social environments.

</details>


### [129] [OmniGround: A Comprehensive Spatio-Temporal Grounding Benchmark for Real-World Complex Scenarios](https://arxiv.org/abs/2511.16937)
*Hong Gao,Jingyu Wu,Xiangkai Xu,Kangni Xie,Yunchen Zhang,Bin Zhong,Xurui Gao,Min-Ling Zhang*

Main category: cs.CV

TL;DR: 本文指出当前时空视频定位模型与现实需求存在差距，引入OmniGround基准、标注管道和评估框架，揭示复杂场景下模型性能下降，提出无训练框架PG - TAF，实验显示其在多个基准上有显著提升。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在时空视频定位中与现实需求存在差距，现有基准范围有限，导致模型有类别偏差、推理简化和语言鲁棒性差等问题。

Method: 引入OmniGround基准；提出Forward - Backward - Refinement标注管道；引入DeepSTG评估框架；提出无训练的两阶段框架PG - TAF。

Result: 评估显示模型在复杂现实场景性能平均下降10.4%；PG - TAF在OmniGround上m_tIoU和m_vIoU分别提升25.6%和35.6%，在四个基准上均有持续增益。

Conclusion: PG - TAF能有效解决当前时空视频定位模型的问题，在多个基准上表现良好。

Abstract: Spatio-Temporal Video Grounding (STVG) aims to localize target objects in videos based on natural language descriptions. Despite recent advances in Multimodal Large Language Models, a significant gap remains between current models and real-world demands involving diverse objects and complex queries. We attribute this to limited benchmark scope, causing models to exhibit category bias, oversimplified reasoning, and poor linguistic robustness. To address these limitations, we introduce OmniGround, a comprehensive benchmark with 3,475 videos spanning 81 categories and complex real-world queries. We propose the Forward-Backward-Refinement annotation pipeline that combines multi-directional tracking with intelligent error correction for high-quality labels. We further introduce DeepSTG, a systematic evaluation framework quantifying dataset quality across four complementary dimensions beyond superficial statistics. Evaluations reveal performance average drop of 10.4% on complex real-world scenes, particularly with small/occluded objects and intricate spatial relations. Motivated by these, we propose PG-TAF, a training-free two-stage framework decomposing STVG into high-level temporal grounding and fine-grained spatio-temporal propagation. Experiments demonstrate PG-TAF achieves 25.6% and 35.6% improvements in m\_tIoU and m\_vIoU on OmniGround with consistent gains across four benchmarks.

</details>


### [130] [The Finer the Better: Towards Granular-aware Open-set Domain Generalization](https://arxiv.org/abs/2511.16979)
*Yunyun Wang,Zheng Duan,Xinyue Liao,Ke-Jia Chen,Songcan Chen*

Main category: cs.CV

TL;DR: 本文提出Semantic-enhanced CLIP (SeeCLIP)框架解决开放集领域泛化问题，实验显示优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有开放集领域泛化方法陷入已知类结构风险和未知类开放空间风险的困境，且区分与已知类视觉相似的“硬未知”时易过度自信。

Method: 提出语义感知提示增强模块分解图像为语义标记；引入双工对比学习；语义引导扩散模块合成伪未知样本。

Result: 在五个基准测试中，准确率较现有方法提升3%，H分数提升5%。

Conclusion: SeeCLIP框架能有效解决开放集领域泛化问题，提升模型性能。

Abstract: Open-Set Domain Generalization (OSDG) tackles the realistic scenario where deployed models encounter both domain shifts and novel object categories. Despite impressive progress with vision-language models like CLIP, existing methods still fall into the dilemma between structural risk of known-classes and open-space risk from unknown-classes, and easily suffers from over-confidence, especially when distinguishing ``hard unknowns" that share fine-grained visual similarities with known classes. To this end, we propose a Semantic-enhanced CLIP (SeeCLIP) framework that explicitly addresses this dilemma through fine-grained semantic enhancement. In SeeCLIP, we propose a semantic-aware prompt enhancement module to decompose images into discriminative semantic tokens, enabling nuanced vision-language alignment beyond coarse category labels. To position unknown prompts effectively, we introduce duplex contrastive learning with complementary objectives, that is, repulsion to maintain separability from known classes, and cohesion to preserve semantic proximity. Further, our semantic-guided diffusion module synthesizes pseudo-unknowns by perturbing extracted semantic tokens, generating challenging samples that are visually similar to known classes yet exhibit key local differences. These hard negatives force the model to learn finer decision boundaries. Extensive experiments across five benchmarks demonstrate consistent improvements of 3% accuracy and 5% H-score over state-of-the-art methods.

</details>


### [131] [FLUID: Training-Free Face De-identification via Latent Identity Substitution](https://arxiv.org/abs/2511.17005)
*Jinhyeong Park,Shaheryar Muhammad,Seangmin Lee,Jong Taek Lee,Soon Ki Jung*

Main category: cs.CV

TL;DR: 提出无训练框架FLUID，在预训练扩散模型潜空间直接替换身份，实验表现优。


<details>
  <summary>Details</summary>
Motivation: 解决人脸去识别问题，在潜空间实现身份替换。

Method: 受化学替代机制启发，将身份编辑视为语义位移，用新试剂损失引导优化发现编辑方向，提出线性和测地编辑方案。

Result: 在CelebA - HQ和FFHQ上实验，FLUID在身份抑制和属性保留上取得良好平衡，超越现有去识别方法。

Conclusion: FLUID是有效的人脸去识别框架，在身份抑制和属性保留方面表现出色。

Abstract: We present FLUID (Face de-identification in the Latent space via Utility-preserving Identity Displacement), a training-free framework that directly substitutes identity in the latent space of pretrained diffusion models. Inspired by substitution mechanisms in chemistry, we reinterpret identity editing as semantic displacement in the latent h-space of a pretrained unconditional diffusion model. Our framework discovers identity-editing directions through optimization guided by novel reagent losses, which supervise for attribute preservation and identity suppression. We further propose both linear and geodesic (tangent-based) editing schemes to effectively navigate the latent manifold. Experimental results on CelebA-HQ and FFHQ demonstrate that FLUID achieves a superior trade-off between identity suppression and attribute preservation, outperforming state-of-the-art de-identification methods in both qualitative and quantitative metrics.

</details>


### [132] [Parameter-Free Neural Lens Blur Rendering for High-Fidelity Composites](https://arxiv.org/abs/2511.17014)
*Lingyan Ruan,Bin Chen,Taehyun Rhee*

Main category: cs.CV

TL;DR: 提出一种从RGB图像直接估计混淆圈（CoC）图的合成方法，绕过场景深度和相机元数据，实现高保真合成。


<details>
  <summary>Details</summary>
Motivation: 现有计算CoC以实现镜头模糊渲染的方法依赖相机参数和场景深度信息，普通用户难以获取，限制了方法的可访问性和通用性。

Method: 从RGB图像直接估计CoC图，通过CoC图与深度的线性关系推断虚拟对象的CoC值，用神经重模糊网络渲染镜头模糊。

Result: 实验表明该方法实现了具有逼真散焦效果的高保真合成，在定性和定量评估中优于现有技术。

Conclusion: 该方法为现实应用提供了灵活实用的解决方案。

Abstract: Consistent and natural camera lens blur is important for seamlessly blending 3D virtual objects into photographed real-scenes. Since lens blur typically varies with scene depth, the placement of virtual objects and their corresponding blur levels significantly affect the visual fidelity of mixed reality compositions. Existing pipelines often rely on camera parameters (e.g., focal length, focus distance, aperture size) and scene depth to compute the circle of confusion (CoC) for realistic lens blur rendering. However, such information is often unavailable to ordinary users, limiting the accessibility and generalizability of these methods. In this work, we propose a novel compositing approach that directly estimates the CoC map from RGB images, bypassing the need for scene depth or camera metadata. The CoC values for virtual objects are inferred through a linear relationship between its signed CoC map and depth, and realistic lens blur is rendered using a neural reblurring network. Our method provides flexible and practical solution for real-world applications. Experimental results demonstrate that our method achieves high-fidelity compositing with realistic defocus effects, outperforming state-of-the-art techniques in both qualitative and quantitative evaluations.

</details>


### [133] [Align & Invert: Solving Inverse Problems with Diffusion and Flow-based Models via Representational Alignment](https://arxiv.org/abs/2511.16870)
*Loukas Sfountouris,Giannis Daras,Paris Giampouras*

Main category: cs.CV

TL;DR: 本文将生成模型与预训练自监督编码器的表示对齐思想扩展到逆问题，提出REPA方法，理论分析其作用，集成到多种求解器，实验证明能提升重建质量和效率。


<details>
  <summary>Details</summary>
Motivation: 将生成模型与预训练自监督编码器表示对齐的思想应用于逆问题，提升重建效果。

Method: 提出REPA方法，在推理时应用扩散或流基模型与预训练自监督视觉编码器（如DINOv2）的表示对齐来引导重建过程。

Result: 理论上分析了REPA正则化与DINOv2嵌入空间散度的关系，及REPA更新如何引导模型内部表示；实验上在多个逆问题任务中提升重建质量，减少离散化步骤提升效率。

Conclusion: REPA方法能有效提升逆问题重建质量和效率，具有通用性。

Abstract: Enforcing alignment between the internal representations of diffusion or flow-based generative models and those of pretrained self-supervised encoders has recently been shown to provide a powerful inductive bias, improving both convergence and sample quality. In this work, we extend this idea to inverse problems, where pretrained generative models are employed as priors. We propose applying representation alignment (REPA) between diffusion or flow-based models and a pretrained self-supervised visual encoder, such as DINOv2, to guide the reconstruction process at inference time. Although ground-truth signals are unavailable in inverse problems, we show that aligning model representations with approximate target features can substantially enhance reconstruction fidelity and perceptual realism. We provide theoretical results showing (a) the relation between the REPA regularization and a divergence measure in the DINOv2 embedding space, and (b) how REPA updates steer the model's internal representations toward those of the clean image. These results offer insights into the role of REPA in improving perceptual fidelity. Finally, we demonstrate the generality of our approach by integrating it into multiple state-of-the-art inverse problem solvers. Extensive experiments on super-resolution, box inpainting, Gaussian deblurring, and motion deblurring confirm that our method consistently improves reconstruction quality across tasks, while also providing substantial efficiency gains by reducing the number of required discretization steps without compromising the performance of the underlying solver.

</details>


### [134] [RacketVision: A Multiple Racket Sports Benchmark for Unified Ball and Racket Analysis](https://arxiv.org/abs/2511.17045)
*Linfeng Dong,Yuchen Yang,Hao Wu,Wei Wang,Yuenan HouZhihang Zhong,Xiao Sun*

Main category: cs.CV

TL;DR: 介绍RacketVision数据集和基准，用于体育分析计算机视觉，评估显示CrossAttention机制对多模态融合重要，为相关研究提供资源。


<details>
  <summary>Details</summary>
Motivation: 推动体育分析中计算机视觉的发展，研究复杂的人与物体交互。

Method: 创建RacketVision数据集，涵盖乒乓球、网球和羽毛球，设计三项关联任务，评估现有基线。

Result: 直接拼接球拍姿态特征会降低性能，CrossAttention机制可提升轨迹预测结果，超越单模态基线。

Conclusion: RacketVision为动态目标跟踪、条件运动预测和体育多模态分析研究提供了通用资源和良好起点。

Abstract: We introduce RacketVision, a novel dataset and benchmark for advancing computer vision in sports analytics, covering table tennis, tennis, and badminton. The dataset is the first to provide large-scale, fine-grained annotations for racket pose alongside traditional ball positions, enabling research into complex human-object interactions. It is designed to tackle three interconnected tasks: fine-grained ball tracking, articulated racket pose estimation, and predictive ball trajectory forecasting. Our evaluation of established baselines reveals a critical insight for multi-modal fusion: while naively concatenating racket pose features degrades performance, a CrossAttention mechanism is essential to unlock their value, leading to trajectory prediction results that surpass strong unimodal baselines. RacketVision provides a versatile resource and a strong starting point for future research in dynamic object tracking, conditional motion forecasting, and multimodal analysis in sports. Project page at https://github.com/OrcustD/RacketVision

</details>


### [135] [Neighbor GRPO: Contrastive ODE Policy Optimization Aligns Flow Models](https://arxiv.org/abs/2511.16955)
*Dailan He,Guanlin Feng,Xingtong Ge,Yazhe Niu,Yi Zhang,Bingqi Ma,Guanglu Song,Yu Liu,Hongsheng Li*

Main category: cs.CV

TL;DR: 本文提出Neighbor GRPO算法解决GRPO应用于流匹配模型的问题，优于SDE - based方法。


<details>
  <summary>Details</summary>
Motivation: 现有GRPO应用于现代流匹配模型有挑战，SDE - based GRPO存在效率和兼容性问题。

Method: 从距离优化视角重新解读SDE - based GRPO，提出Neighbor GRPO算法，利用扰动初始噪声生成候选轨迹，使用基于softmax距离的代理跳跃策略优化模型，还引入对称锚采样和组准范数重加权。

Result: Neighbor GRPO在训练成本、收敛速度和生成质量上显著优于SDE - based方法。

Conclusion: Neighbor GRPO算法有效，能保留确定性ODE采样优势，解决现有方法问题。

Abstract: Group Relative Policy Optimization (GRPO) has shown promise in aligning image and video generative models with human preferences. However, applying it to modern flow matching models is challenging because of its deterministic sampling paradigm. Current methods address this issue by converting Ordinary Differential Equations (ODEs) to Stochastic Differential Equations (SDEs), which introduce stochasticity. However, this SDE-based GRPO suffers from issues of inefficient credit assignment and incompatibility with high-order solvers for fewer-step sampling. In this paper, we first reinterpret existing SDE-based GRPO methods from a distance optimization perspective, revealing their underlying mechanism as a form of contrastive learning. Based on this insight, we propose Neighbor GRPO, a novel alignment algorithm that completely bypasses the need for SDEs. Neighbor GRPO generates a diverse set of candidate trajectories by perturbing the initial noise conditions of the ODE and optimizes the model using a softmax distance-based surrogate leaping policy. We establish a theoretical connection between this distance-based objective and policy gradient optimization, rigorously integrating our approach into the GRPO framework. Our method fully preserves the advantages of deterministic ODE sampling, including efficiency and compatibility with high-order solvers. We further introduce symmetric anchor sampling for computational efficiency and group-wise quasi-norm reweighting to address reward flattening. Extensive experiments demonstrate that Neighbor GRPO significantly outperforms SDE-based counterparts in terms of training cost, convergence speed, and generation quality.

</details>


### [136] [OmniPT: Unleashing the Potential of Large Vision Language Models for Pedestrian Tracking and Understanding](https://arxiv.org/abs/2511.17053)
*Teng Fu,Mengyang Zhao,Ke Niu,Kaixin Peng,Bin Li*

Main category: cs.CV

TL;DR: 本文提出统一行人跟踪框架OmniPT，采用特定训练阶段解决建模和输出格式问题，实验表明性能优于先前方法。


<details>
  <summary>Details</summary>
Motivation: LVLMs在实例级任务有性能差距，而行人跟踪新任务需高级语义理解，LVLMs在此有优势，故提出新框架。

Method: 提出OmniPT框架，采用RL - Mid Training - SFT - RL训练阶段，基于LVLM预训练权重，依次进行简单RL、中间训练、监督微调、再次RL。

Result: 在跟踪基准上实验，所提方法性能优于先前方法。

Conclusion: 所提统一行人跟踪框架OmniPT有效，能提升行人跟踪性能。

Abstract: LVLMs have been shown to perform excellently in image-level tasks such as VQA and caption. However, in many instance-level tasks, such as visual grounding and object detection, LVLMs still show performance gaps compared to previous expert models. Meanwhile, although pedestrian tracking is a classical task, there have been a number of new topics in combining object tracking and natural language, such as Referring MOT, Cross-view Referring MOT, and Semantic MOT. These tasks emphasize that models should understand the tracked object at an advanced semantic level, which is exactly where LVLMs excel. In this paper, we propose a new unified Pedestrian Tracking framework, namely OmniPT, which can track, track based on reference and generate semantic understanding of tracked objects interactively. We address two issues: how to model the tracking task into a task that foundation models can perform, and how to make the model output formatted answers. To this end, we implement a training phase consisting of RL-Mid Training-SFT-RL. Based on the pre-trained weights of the LVLM, we first perform a simple RL phase to enable the model to output fixed and supervisable bounding box format. Subsequently, we conduct a mid-training phase using a large number of pedestrian-related datasets. Finally, we perform supervised fine-tuning on several pedestrian tracking datasets, and then carry out another RL phase to improve the model's tracking performance and enhance its ability to follow instructions. We conduct experiments on tracking benchmarks and the experimental results demonstrate that the proposed method can perform better than the previous methods.

</details>


### [137] [Real-Time Cooked Food Image Synthesis and Visual Cooking Progress Monitoring on Edge Devices](https://arxiv.org/abs/2511.16965)
*Jigyasa Gupta,Soumya Goyal,Anil Kumar,Ishan Jindal*

Main category: cs.CV

TL;DR: 提出首个基于烤箱的烹饪进度数据集，及边缘高效的食谱和烹饪状态引导生成器，引入CIS指标，模型表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有图像生成方法在边缘设备上合成逼真熟食图像时结果不真实或资源消耗大。

Method: 引入烤箱烹饪进度数据集，提出边缘高效生成器，引入CIS指标。

Result: 模型在FID分数上显著优于现有基线，在自有数据集提升30%，公共数据集提升60%。

Conclusion: 所提方法能有效在边缘设备上合成逼真熟食图像。

Abstract: Synthesizing realistic cooked food images from raw inputs on edge devices is a challenging generative task, requiring models to capture complex changes in texture, color and structure during cooking. Existing image-to-image generation methods often produce unrealistic results or are too resource-intensive for edge deployment. We introduce the first oven-based cooking-progression dataset with chef-annotated doneness levels and propose an edge-efficient recipe and cooking state guided generator that synthesizes realistic food images conditioned on raw food image. This formulation enables user-preferred visual targets rather than fixed presets. To ensure temporal consistency and culinary plausibility, we introduce a domain-specific \textit{Culinary Image Similarity (CIS)} metric, which serves both as a training loss and a progress-monitoring signal. Our model outperforms existing baselines with significant reductions in FID scores (30\% improvement on our dataset; 60\% on public datasets)

</details>


### [138] [ReBrain: Brain MRI Reconstruction from Sparse CT Slice via Retrieval-Augmented Diffusion](https://arxiv.org/abs/2511.17068)
*Junming Liu,Yifei Sun,Weihua Cheng,Yujin Kang,Yirong Chen,Ding Wang,Guosun Zeng*

Main category: cs.CV

TL;DR: 提出 ReBrain 框架解决从低剂量 CT 重建全脑 MRI 难题，实验显示其在稀疏条件下跨模态重建达 SOTA 性能。


<details>
  <summary>Details</summary>
Motivation: MRI 因物理或临床限制并非对所有患者可行，低剂量 CT 重建全脑 MRI 有挑战。

Method: 提出 ReBrain 框架，用 BBDM 沿 2D 维度合成 MRI 切片，通过微调检索模型从数据库检索相似 CT 切片作参考，用 ControlNet 分支引导生成中间 MRI 切片，处理检索失败用球面线性插值补充指导。

Result: 在 SynthRAD2023 和 BraTS 上实验表明 ReBrain 在稀疏条件下跨模态重建达 SOTA 性能。

Conclusion: ReBrain 框架能有效解决从稀疏 CT 重建全脑 MRI 问题，在跨模态重建上有优异表现。

Abstract: Magnetic Resonance Imaging (MRI) plays a crucial role in brain disease diagnosis, but it is not always feasible for certain patients due to physical or clinical constraints. Recent studies attempt to synthesize MRI from Computed Tomography (CT) scans; however, low-dose protocols often result in highly sparse CT volumes with poor through-plane resolution, making accurate reconstruction of the full brain MRI volume particularly challenging. To address this, we propose ReBrain, a retrieval-augmented diffusion framework for brain MRI reconstruction. Given any 3D CT scan with limited slices, we first employ a Brownian Bridge Diffusion Model (BBDM) to synthesize MRI slices along the 2D dimension. Simultaneously, we retrieve structurally and pathologically similar CT slices from a comprehensive prior database via a fine-tuned retrieval model. These retrieved slices are used as references, incorporated through a ControlNet branch to guide the generation of intermediate MRI slices and ensure structural continuity. We further account for rare retrieval failures when the database lacks suitable references and apply spherical linear interpolation to provide supplementary guidance. Extensive experiments on SynthRAD2023 and BraTS demonstrate that ReBrain achieves state-of-the-art performance in cross-modal reconstruction under sparse conditions.

</details>


### [139] [A Diversity-optimized Deep Ensemble Approach for Accurate Plant Leaf Disease Detection](https://arxiv.org/abs/2511.16982)
*Sai Nath Chowdary Medikonduru,Hongpeng Jin,Yanzhao Wu*

Main category: cs.CV

TL;DR: 本文引入协同多样性（SQ）框架提升植物病害检测准确率，实验证明SQ指标能改善集成选择并提高检测准确性。


<details>
  <summary>Details</summary>
Motivation: 植物病害对全球农业造成巨大经济损失，及时准确检测至关重要。深度集成方法虽可提升预测准确性，但选择高性能集成成员模型因难以衡量集成多样性而具挑战。

Method: 先分析现有集成多样性指标的局限性，再提出新的SQ指标，最后在植物叶片图像数据集上进行广泛实验验证。

Result: SQ指标显著改善了集成选择，提高了检测准确率。

Conclusion: 研究为更可靠、高效的基于图像的植物病害检测奠定基础。

Abstract: Plant diseases pose a significant threat to global agriculture, causing over $220 billion in annual economic losses and jeopardizing food security. The timely and accurate detection of these diseases from plant leaf images is critical to mitigating their adverse effects. Deep neural network Ensembles (Deep Ensembles) have emerged as a powerful approach to enhancing prediction accuracy by leveraging the strengths of diverse Deep Neural Networks (DNNs). However, selecting high-performing ensemble member models is challenging due to the inherent difficulty in measuring ensemble diversity. In this paper, we introduce the Synergistic Diversity (SQ) framework to enhance plant disease detection accuracy. First, we conduct a comprehensive analysis of the limitations of existing ensemble diversity metrics (denoted as Q metrics), which often fail to identify optimal ensemble teams. Second, we present the SQ metric, a novel measure that captures the synergy between ensemble members and consistently aligns with ensemble accuracy. Third, we validate our SQ approach through extensive experiments on a plant leaf image dataset, which demonstrates that our SQ metric substantially improves ensemble selection and enhances detection accuracy. Our findings pave the way for a more reliable and efficient image-based plant disease detection.

</details>


### [140] [Spanning Tree Autoregressive Visual Generation](https://arxiv.org/abs/2511.17089)
*Sangkyu Lee,Changho Lee,Janghoon Han,Hosung Song,Tackgeun You,Hwasup Lim,Stanley Jungkyu Choi,Honglak Lee,Youngjae Yu*

Main category: cs.CV

TL;DR: 提出STAR建模，利用均匀生成树遍历顺序，在不改变模型架构下，兼顾采样性能与推理灵活性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉生成中让传统自回归模型暴露随机排列序列顺序的方法，存在性能下降或推理时序列顺序选择灵活性不足的问题。

Method: STAR利用图像块位置定义的晶格中采样的均匀生成树的遍历顺序，通过广度优先搜索获取遍历顺序，并通过拒绝采样确保图像的部分观察在序列中作为前缀。

Result: STAR在不显著改变语言自回归建模中广泛采用的模型架构的情况下，保留了后缀补全能力并维持了采样性能。

Conclusion: STAR建模能够结合图像先验知识，在维持采样性能的同时，为推理时的图像编辑提供足够灵活的序列顺序。

Abstract: We present Spanning Tree Autoregressive (STAR) modeling, which can incorporate prior knowledge of images, such as center bias and locality, to maintain sampling performance while also providing sufficiently flexible sequence orders to accommodate image editing at inference. Approaches that expose randomly permuted sequence orders to conventional autoregressive (AR) models in visual generation for bidirectional context either suffer from a decline in performance or compromise the flexibility in sequence order choice at inference. Instead, STAR utilizes traversal orders of uniform spanning trees sampled in a lattice defined by the positions of image patches. Traversal orders are obtained through breadth-first search, allowing us to efficiently construct a spanning tree whose traversal order ensures that the connected partial observation of the image appears as a prefix in the sequence through rejection sampling. Through the tailored yet structured randomized strategy compared to random permutation, STAR preserves the capability of postfix completion while maintaining sampling performance without any significant changes to the model architecture widely adopted in the language AR modeling.

</details>


### [141] [FireScope: Wildfire Risk Prediction with a Chain-of-Thought Oracle](https://arxiv.org/abs/2511.17171)
*Mario Markov,Stefan Maria Ailuro,Luc Van Gool,Konrad Schindler,Danda Pani Paudel*

Main category: cs.CV

TL;DR: 提出FireScope - Bench数据集和基准，及FireScope框架用于野火风险预测，提升泛化性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有野火风险预测方法缺乏因果推理和多模态理解，难以可靠泛化。

Method: 引入FireScope - Bench数据集，结合卫星图像、气候数据等；提出基于VLM的FireScope框架，结合强化学习和视觉监督学习。

Result: FireScope在跨大陆评估中取得显著性能提升，推理痕迹可信且有语义意义。

Conclusion: 推理可增强栅格预测模型，提高泛化性和可解释性；FireScope - Bench有望推动推理驱动的空间建模。

Abstract: Predicting wildfire risk is a reasoning-intensive spatial problem that requires the integration of visual, climatic, and geographic factors to infer continuous risk maps. Existing methods lack the causal reasoning and multimodal understanding required for reliable generalization. We introduce $\textbf{FireScope-Bench}$, a large-scale dataset and benchmark that couples Sentinel-2 imagery and climate data with expert-defined risk rasters across the USA, and real wildfire events in Europe for cross-continental evaluation. Building on this dataset, we propose $\textbf{FireScope}$, a VLM-based reasoning-to-generation framework that learns from both reinforcement learning and visual supervision to predict risk rasters with complementary reasoning traces. When trained in the USA and tested in Europe, $\textbf{FireScope}$ achieves substantial performance gains, while expert feedback and automated analysis confirm that its reasoning traces are faithful and semantically meaningful. Our findings demonstrate that reasoning can ground raster prediction models, improving both generalization and interpretability. To our knowledge, this is the first framework to (1) demonstrate that language-based reasoning can improve generalization in visual generation, (2) propose a high-resolution wildfire risk model that can be applied across continents, and (3) enable systematic studies of robust cross-continental generalization for multimodal fire risk models. We believe that $\textbf{FireScope-Bench}$ has the potential to serve as a foundation for advancing reasoning-driven, interpretable and generalizable spatial modeling. Data and source code will be made publicly available.

</details>


### [142] [A lightweight detector for real-time detection of remote sensing images](https://arxiv.org/abs/2511.17147)
*Qianyi Wang,Guoqiang Ren*

Main category: cs.CV

TL;DR: 提出DMG - YOLO用于遥感图像小目标实时检测，设计DFE、MFF模块及GLAFPN网络，实验显示其在关键指标上表现良好。


<details>
  <summary>Details</summary>
Motivation: 遥感图像广泛应用但实时检测因小目标多且需平衡精度效率而具挑战性。

Method: 提出DMG - YOLO，设计DFE模块分两支提取特征，MFF模块增强多尺度融合，引入GLAFPN网络。

Result: 在VisDrone2019和NWPU VHR - 10数据集实验中，DMG - YOLO在mAP、模型大小等关键指标上有竞争力。

Conclusion: DMG - YOLO能有效用于遥感图像小目标实时检测。

Abstract: Remote sensing imagery is widely used across various fields, yet real-time detection remains challenging due to the prevalence of small objects and the need to balance accuracy with efficiency. To address this, we propose DMG-YOLO, a lightweight real-time detector tailored for small object detection in remote sensing images. Specifically, we design a Dual-branch Feature Extraction (DFE) module in the backbone, which partitions feature maps into two parallel branches: one extracts local features via depthwise separable convolutions, and the other captures global context using a vision transformer with a gating mechanism. Additionally, a Multi-scale Feature Fusion (MFF) module with dilated convolutions enhances multi-scale integration while preserving fine details. In the neck, we introduce the Global and Local Aggregate Feature Pyramid Network (GLAFPN) to further boost small object detection through global-local feature fusion. Extensive experiments on the VisDrone2019 and NWPU VHR-10 datasets show that DMG-YOLO achieves competitive performance in terms of mAP, model size, and other key metrics.

</details>


### [143] [Investigating self-supervised representations for audio-visual deepfake detection](https://arxiv.org/abs/2511.17181)
*Dragos-Alexandru Boldisor,Stefan Smeu,Dan Oneata,Elisabeta Oneata*

Main category: cs.CV

TL;DR: 研究自监督表征在视听深度伪造检测中的潜力，发现其能捕捉相关信息但跨数据集泛化不佳。


<details>
  <summary>Details</summary>
Motivation: 自监督表征在视听深度伪造检测方面的潜力未被充分挖掘，此前研究使用特征方式有局限。

Method: 系统评估自监督表征在不同模态和领域的表现，评估检测有效性、信息可解释性和跨模态互补性三个关键维度。

Result: 多数自监督特征能捕捉深度伪造相关信息且具互补性，模型关注语义有意义区域，但跨数据集泛化不可靠，泛化失败可能源于数据集特性。

Conclusion: 自监督表征用于深度伪造检测有前景，但实现稳健的跨域性能仍困难。

Abstract: Self-supervised representations excel at many vision and speech tasks, but their potential for audio-visual deepfake detection remains underexplored. Unlike prior work that uses these features in isolation or buried within complex architectures, we systematically evaluate them across modalities (audio, video, multimodal) and domains (lip movements, generic visual content). We assess three key dimensions: detection effectiveness, interpretability of encoded information, and cross-modal complementarity. We find that most self-supervised features capture deepfake-relevant information, and that this information is complementary. Moreover, models primarily attend to semantically meaningful regions rather than spurious artifacts. Yet none generalize reliably across datasets. This generalization failure likely stems from dataset characteristics, not from the features themselves latching onto superficial patterns. These results expose both the promise and fundamental challenges of self-supervised representations for deepfake detection: while they learn meaningful patterns, achieving robust cross-domain performance remains elusive.

</details>


### [144] [Equivariant-Aware Structured Pruning for Efficient Edge Deployment: A Comprehensive Framework with Adaptive Fine-Tuning](https://arxiv.org/abs/2511.17242)
*Mohammed Alnemari*

Main category: cs.CV

TL;DR: 提出结合G - CNN与等变感知结构化剪枝的框架，用于资源受限环境，在多数据集验证有效。


<details>
  <summary>Details</summary>
Motivation: 为资源受限环境生成紧凑、变换不变的模型，解决理论设计与实际部署约束的差距。

Method: 通过e2cnn库结合C4循环群实现旋转等变性，引入结构化剪枝，采用自适应微调，包括动态INT8量化和综合优化流程。

Result: 在多数据集上实验，参数减少29.3%且显著恢复准确率。

Conclusion: 该框架可优化等变模型，对卫星图像分析和几何视觉任务有重要意义。

Abstract: This paper presents a novel framework combining group equivariant convolutional neural networks (G-CNNs) with equivariant-aware structured pruning to produce compact, transformation-invariant models for resource-constrained environments. Equivariance to rotations is achieved through the C4 cyclic group via the e2cnn library,enabling consistent performance under geometric transformations while reducing computational overhead.
  Our approach introduces structured pruning that preserves equivariant properties by analyzing e2cnn layer structure and applying neuron-level pruning to fully connected components. To mitigate accuracy degradation, we implement adaptive fine-tuning that automatically triggers when accuracy drop exceeds 2%, using early stopping and learning rate scheduling for efficient recovery. The framework includes dynamic INT8 quantization and a comprehensive pipeline encompassing training, knowledge distillation, structured pruning, fine-tuning, and quantization.
  We evaluate our method on satellite imagery (EuroSAT) and standard benchmarks (CIFAR-10, Rotated MNIST) demonstrating effectiveness across diverse domains. Experimental results show 29.3% parameter reduction with significant accuracy recovery, demonstrating that structured pruning of equivariant networks achieves substantial compression while maintaining geometric robustness. Our pipeline provides a reproducible framework for optimizing equivariant models, bridging the gap between group-theoretic network design and practical deployment constraints, with particular relevance to satellite imagery analysis and geometric vision tasks.

</details>


### [145] [MuM: Multi-View Masked Image Modeling for 3D Vision](https://arxiv.org/abs/2511.17309)
*David Nordström,Johan Edstedt,Fredrik Kahl,Georg Bökman*

Main category: cs.CV

TL;DR: 本文延续CroCo路径，扩展MAE到同一场景多视图，提出MuM模型，在下游任务中优于DINOv3和CroCo v2。


<details>
  <summary>Details</summary>
Motivation: 多数自监督学习工作优化语义理解而非几何推理，作者希望学习适用于3D视觉的特征。

Method: 将MAE扩展到同一场景任意多视图，统一掩码所有视图并使用带帧间注意力的轻量级解码器。

Result: MuM模型在下游任务如前馈重建、密集图像匹配和相对姿态估计中表现优于DINOv3和CroCo v2。

Conclusion: 所提出的方法简单且可扩展，能有效学习适用于3D视觉的特征。

Abstract: Self-supervised learning on images seeks to extract meaningful visual representations from unlabeled data. When scaled to large datasets, this paradigm has achieved state-of-the-art performance and the resulting trained models such as DINOv3 have seen widespread adoption. However, most prior efforts are optimized for semantic understanding rather than geometric reasoning. One important exception is Cross-View Completion, CroCo, which is a form of masked autoencoding (MAE) tailored for 3D understanding. In this work, we continue on the path proposed by CroCo and focus on learning features tailored for 3D vision. In a nutshell, we extend MAE to arbitrarily many views of the same scene. By uniformly masking all views and employing a lightweight decoder with inter-frame attention, our approach is inherently simpler and more scalable than CroCo. We evaluate the resulting model, MuM, extensively on downstream tasks including feedforward reconstruction, dense image matching and relative pose estimation, finding that it outperforms the state-of-the-art visual encoders DINOv3 and CroCo v2.

</details>


### [146] [Intervene-All-Paths: Unified Mitigation of LVLM Hallucinations across Alignment Formats](https://arxiv.org/abs/2511.17254)
*Jiaye Qian,Ge Zheng,Yuchen Zhu,Sibei Yang*

Main category: cs.CV

TL;DR: 提出针对大视觉语言模型幻觉问题的干预框架，发现幻觉成因并提出方法减少幻觉。


<details>
  <summary>Details</summary>
Motivation: 解决大视觉语言模型易产生幻觉的问题。

Method: 提出与变压器因果架构对齐的干预框架，识别并干预各路径中关键幻觉头。

Result: 实验表明该方法能在多种对齐类型中持续减少幻觉。

Conclusion: 所提方法可有效减少大视觉语言模型的幻觉。

Abstract: Despite their impressive performance across a wide range of tasks, Large Vision-Language Models (LVLMs) remain prone to hallucination. In this study, we propose a comprehensive intervention framework aligned with the transformer's causal architecture in LVLMs, integrating the effects of different intervention paths on hallucination. We find that hallucinations in LVLMs do not arise from a single causal path, but rather from the interplay among image-to-input-text, image-to-output-text, and text-to-text pathways. For the first time, we also find that LVLMs rely on different pathways depending on the question-answer alignment format. Building on these insights, we propose simple yet effective methods to identify and intervene on critical hallucination heads within each pathway, tailored to discriminative and generative formats. Experiments across multiple benchmarks demonstrate that our approach consistently reduces hallucinations across diverse alignment types.

</details>


### [147] [Range-Edit: Semantic Mask Guided Outdoor LiDAR Scene Editing](https://arxiv.org/abs/2511.17269)
*Suchetan G. Uppur,Hemant Kumar,Vaibhav Kumar*

Main category: cs.CV

TL;DR: 本文提出用语义掩码引导编辑真实LiDAR扫描数据生成合成点云的方法，在KITTI - 360数据集验证有效，为自动驾驶系统提供低成本数据生成方案。


<details>
  <summary>Details</summary>
Motivation: 获取真实世界中复杂边缘场景的点云数据有挑战，现有模拟方法耗时、计算成本高且难以捕捉真实场景复杂性，限制系统泛化和鲁棒性。

Method: 通过语义掩码引导编辑真实LiDAR扫描数据，结合距离图像投影和语义掩码调节实现基于扩散的生成，利用凸包语义掩码进行语义编辑。

Result: 在KITTI - 360数据集上验证该方法能生成高质量LiDAR点云，可产生复杂边缘情况和动态场景。

Conclusion: 该方法为生成多样LiDAR数据提供了低成本、可扩展的解决方案，有助于提升自动驾驶系统的鲁棒性。

Abstract: Training autonomous driving and navigation systems requires large and diverse point cloud datasets that capture complex edge case scenarios from various dynamic urban settings. Acquiring such diverse scenarios from real-world point cloud data, especially for critical edge cases, is challenging, which restricts system generalization and robustness. Current methods rely on simulating point cloud data within handcrafted 3D virtual environments, which is time-consuming, computationally expensive, and often fails to fully capture the complexity of real-world scenes. To address some of these issues, this research proposes a novel approach that addresses the problem discussed by editing real-world LiDAR scans using semantic mask-based guidance to generate novel synthetic LiDAR point clouds. We incorporate range image projection and semantic mask conditioning to achieve diffusion-based generation. Point clouds are transformed to 2D range view images, which are used as an intermediate representation to enable semantic editing using convex hull-based semantic masks. These masks guide the generation process by providing information on the dimensions, orientations, and locations of objects in the real environment, ensuring geometric consistency and realism. This approach demonstrates high-quality LiDAR point cloud generation, capable of producing complex edge cases and dynamic scenes, as validated on the KITTI-360 dataset. This offers a cost-effective and scalable solution for generating diverse LiDAR data, a step toward improving the robustness of autonomous driving systems.

</details>


### [148] [Non-Parametric Probabilistic Robustness: A Conservative Metric with Optimized Perturbation Distributions](https://arxiv.org/abs/2511.17380)
*Zheng Wang,Yi Zhang,Siddartha Khastgir,Carsten Maple,Xingyu Zhao*

Main category: cs.CV

TL;DR: 提出非参数概率鲁棒性（NPPR）指标，不依赖预定义扰动分布，经理论分析和实验验证其实用性。


<details>
  <summary>Details</summary>
Motivation: 现有概率鲁棒性（PR）公式假设固定已知的扰动分布，不符合实际，需更实用的PR指标。

Method: 提出NPPR，基于非参数范式从数据中学习优化的扰动分布；开发基于带MLP头和双三次上采样的高斯混合模型（GMM）的NPPR估计器；进行理论分析和多数据集多模型实验。

Result: 理论上建立了对抗鲁棒性（AR）、PR和NPPR的关系；实验显示NPPR比现有方法的PR估计保守达40%。

Conclusion: NPPR是更实用的鲁棒性度量指标。

Abstract: Deep learning (DL) models, despite their remarkable success, remain vulnerable to small input perturbations that can cause erroneous outputs, motivating the recent proposal of probabilistic robustness (PR) as a complementary alternative to adversarial robustness (AR). However, existing PR formulations assume a fixed and known perturbation distribution, an unrealistic expectation in practice. To address this limitation, we propose non-parametric probabilistic robustness (NPPR), a more practical PR metric that does not rely on any predefined perturbation distribution. Following the non-parametric paradigm in statistical modeling, NPPR learns an optimized perturbation distribution directly from data, enabling conservative PR evaluation under distributional uncertainty. We further develop an NPPR estimator based on a Gaussian Mixture Model (GMM) with Multilayer Perceptron (MLP) heads and bicubic up-sampling, covering various input-dependent and input-independent perturbation scenarios. Theoretical analyses establish the relationships among AR, PR, and NPPR. Extensive experiments on CIFAR-10, CIFAR-100, and Tiny ImageNet across ResNet18/50, WideResNet50 and VGG16 validate NPPR as a more practical robustness metric, showing up to 40\% more conservative (lower) PR estimates compared to assuming those common perturbation distributions used in state-of-the-arts.

</details>


### [149] [Where Culture Fades: Revealing the Cultural Gap in Text-to-Image Generation](https://arxiv.org/abs/2511.17282)
*Chuancheng Shi,Shangze Li,Shiming Guo,Simiao Xie,Wenhua Wu,Jingtong Dou,Chao Wu,Canran Xiao,Cong Wang,Zifeng Cheng,Fei Shen,Tat-Seng Chua*

Main category: cs.CV

TL;DR: 当前多语言文本到图像（T2I）模型输出存在跨文化一致性问题，提出探测方法和两种对齐策略，实验显示有改进。


<details>
  <summary>Details</summary>
Motivation: 多语言T2I模型输出在不同文化语境有差异，需保证跨语言文化一致性。

Method: 提出探测方法定位文化敏感信号，引入推理时文化激活和层针对性文化增强两种对齐策略。

Result: 在CultureBench上实验显示，相比强基线在文化一致性上有持续改进，同时保留保真度和多样性。

Conclusion: 所提方法能有效解决多语言T2I模型的跨文化一致性问题。

Abstract: Multilingual text-to-image (T2I) models have advanced rapidly in terms of visual realism and semantic alignment, and are now widely utilized. Yet outputs vary across cultural contexts: because language carries cultural connotations, images synthesized from multilingual prompts should preserve cross-lingual cultural consistency. We conduct a comprehensive analysis showing that current T2I models often produce culturally neutral or English-biased results under multilingual prompts. Analyses of two representative models indicate that the issue stems not from missing cultural knowledge but from insufficient activation of culture-related representations. We propose a probing method that localizes culture-sensitive signals to a small set of neurons in a few fixed layers. Guided by this finding, we introduce two complementary alignment strategies: (1) inference-time cultural activation that amplifies the identified neurons without backbone fine-tuned; and (2) layer-targeted cultural enhancement that updates only culturally relevant layers. Experiments on our CultureBench demonstrate consistent improvements over strong baselines in cultural consistency while preserving fidelity and diversity.

</details>


### [150] [Designing and Generating Diverse, Equitable Face Image Datasets for Face Verification Tasks](https://arxiv.org/abs/2511.17393)
*Georgia Baltsou,Ioannis Sarridis,Christos Koutlis,Symeon Papadopoulos*

Main category: cs.CV

TL;DR: 现有面部图像数据集存在偏差，本文提出综合方法生成合成面部图像，引入DIF - V数据集，分析现有模型偏差，推动公平面部验证技术发展。


<details>
  <summary>Details</summary>
Motivation: 现有面部图像数据集存在种族、性别等人口统计学特征偏差，限制面部验证系统有效性和公平性。

Method: 提出综合方法，整合先进生成模型创建多样高质量合成面部图像，引入DIF - V数据集。

Result: 现有验证模型对特定性别和种族有偏差，应用身份风格修改会降低模型性能。

Conclusion: 解决现有数据集的不公平性，丰富人工智能多样性和伦理讨论，为开发更具包容性和可靠性的面部验证技术奠定基础。

Abstract: Face verification is a significant component of identity authentication in various applications including online banking and secure access to personal devices. The majority of the existing face image datasets often suffer from notable biases related to race, gender, and other demographic characteristics, limiting the effectiveness and fairness of face verification systems. In response to these challenges, we propose a comprehensive methodology that integrates advanced generative models to create varied and diverse high-quality synthetic face images. This methodology emphasizes the representation of a diverse range of facial traits, ensuring adherence to characteristics permissible in identity card photographs. Furthermore, we introduce the Diverse and Inclusive Faces for Verification (DIF-V) dataset, comprising 27,780 images of 926 unique identities, designed as a benchmark for future research in face verification. Our analysis reveals that existing verification models exhibit biases toward certain genders and races, and notably, applying identity style modifications negatively impacts model performance. By tackling the inherent inequities in existing datasets, this work not only enriches the discussion on diversity and ethics in artificial intelligence but also lays the foundation for developing more inclusive and reliable face verification technologies

</details>


### [151] [Sparse Mixture-of-Experts for Multi-Channel Imaging: Are All Channel Interactions Required?](https://arxiv.org/abs/2511.17400)
*Sukwon Yun,Heming Yao,Burkhard Hoeckendorf,David Richmond,Aviv Regev,Russell Littman*

Main category: cs.CV

TL;DR: 本文提出MoE - ViT架构解决多通道ViTs效率问题，实验证明其有效。


<details>
  <summary>Details</summary>
Motivation: Vision Transformers在多通道领域优化不足，现有方法在注意力块有计算瓶颈和高成本问题。

Method: 受Sparse Mixture - of - Experts哲学启发，提出MoE - ViT，将每个通道视为专家，用轻量级路由器选择相关专家进行注意力计算。

Result: 在JUMP - CP和So2Sat数据集实验表明，MoE - ViT在不牺牲甚至提升性能的同时，实现显著效率提升。

Conclusion: MoE - ViT是多通道成像实用且有吸引力的骨干架构。

Abstract: Vision Transformers ($\text{ViTs}$) have become the backbone of vision foundation models, yet their optimization for multi-channel domains - such as cell painting or satellite imagery - remains underexplored. A key challenge in these domains is capturing interactions between channels, as each channel carries different information. While existing works have shown efficacy by treating each channel independently during tokenization, this approach naturally introduces a major computational bottleneck in the attention block - channel-wise comparisons leads to a quadratic growth in attention, resulting in excessive $\text{FLOPs}$ and high training cost. In this work, we shift focus from efficacy to the overlooked efficiency challenge in cross-channel attention and ask: "Is it necessary to model all channel interactions?". Inspired by the philosophy of Sparse Mixture-of-Experts ($\text{MoE}$), we propose MoE-ViT, a Mixture-of-Experts architecture for multi-channel images in $\text{ViTs}$, which treats each channel as an expert and employs a lightweight router to select only the most relevant experts per patch for attention. Proof-of-concept experiments on real-world datasets - JUMP-CP and So2Sat - demonstrate that $\text{MoE-ViT}$ achieves substantial efficiency gains without sacrificing, and in some cases enhancing, performance, making it a practical and attractive backbone for multi-channel imaging.

</details>


### [152] [Preventing Shortcut Learning in Medical Image Analysis through Intermediate Layer Knowledge Distillation from Specialist Teachers](https://arxiv.org/abs/2511.17421)
*Christopher Boland,Sotirios Tsaftaris,Sonia Dahdouh*

Main category: cs.CV

TL;DR: 本文提出知识蒸馏框架缓解深度学习模型捷径学习问题，在多数据集实验中表现优于传统方法，适用于医学影像场景。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型易利用训练数据中虚假相关特征学习捷径解，在医学影像分析等高危应用中会影响模型鲁棒性并危害患者。

Method: 提出知识蒸馏框架，利用在少量任务相关数据上微调的教师网络，缓解在含偏差特征大数据集上训练的学生网络的捷径学习。

Result: 在多个数据集和架构上实验，相比传统方法有持续改进，在很多情况下与无偏差数据训练的基线模型性能相当，包括分布外测试数据。

Conclusion: 所提方法在偏差标注有限、捷径特征难先验识别的真实医学影像场景有实际应用价值。

Abstract: Deep learning models are prone to learning shortcut solutions to problems using spuriously correlated yet irrelevant features of their training data. In high-risk applications such as medical image analysis, this phenomenon may prevent models from using clinically meaningful features when making predictions, potentially leading to poor robustness and harm to patients. We demonstrate that different types of shortcuts (those that are diffuse and spread throughout the image, as well as those that are localized to specific areas) manifest distinctly across network layers and can, therefore, be more effectively targeted through mitigation strategies that target the intermediate layers. We propose a novel knowledge distillation framework that leverages a teacher network fine-tuned on a small subset of task-relevant data to mitigate shortcut learning in a student network trained on a large dataset corrupted with a bias feature. Through extensive experiments on CheXpert, ISIC 2017, and SimBA datasets using various architectures (ResNet-18, AlexNet, DenseNet-121, and 3D CNNs), we demonstrate consistent improvements over traditional Empirical Risk Minimization, augmentation-based bias-mitigation, and group-based bias-mitigation approaches. In many cases, we achieve comparable performance with a baseline model trained on bias-free data, even on out-of-distribution test data. Our results demonstrate the practical applicability of our approach to real-world medical imaging scenarios where bias annotations are limited and shortcut features are difficult to identify a priori.

</details>


### [153] [REMSA: An LLM Agent for Foundation Model Selection in Remote Sensing](https://arxiv.org/abs/2511.17442)
*Binger Chen,Tacettin Emre Bök,Behnood Rasti,Volker Markl,Begüm Demir*

Main category: cs.CV

TL;DR: 论文介绍RS - FMD数据库和REMSA代理用于自动选择遥感基础模型，REMSA在基准测试中表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 因文档分散、格式异构和部署约束多样，选择合适的遥感基础模型困难。

Method: 引入RS - FMD数据库，构建基于大语言模型的REMSA代理进行自动选择，提出75个专家验证的遥感查询场景基准。

Result: REMSA在基准测试中优于多个基线方法，且仅使用公开元数据，不访问私有敏感数据。

Conclusion: REMSA能有效解决遥感基础模型选择难题，具有良好性能和数据安全性。

Abstract: Foundation Models (FMs) are increasingly used in remote sensing (RS) for tasks such as environmental monitoring, disaster assessment, and land-use mapping. These models include unimodal vision encoders trained on a single data modality and multimodal architectures trained on combinations of SAR, multispectral, hyperspectral, and image-text data. They support diverse RS tasks including semantic segmentation, image classification, change detection, and visual question answering. However, selecting an appropriate remote sensing foundation model (RSFM) remains difficult due to scattered documentation, heterogeneous formats, and varied deployment constraints. We introduce the RSFM Database (RS-FMD), a structured resource covering over 150 RSFMs spanning multiple data modalities, resolutions, and learning paradigms. Built on RS-FMD, we present REMSA, the first LLM-based agent for automated RSFM selection from natural language queries. REMSA interprets user requirements, resolves missing constraints, ranks candidate models using in-context learning, and provides transparent justifications. We also propose a benchmark of 75 expert-verified RS query scenarios, producing 900 configurations under an expert-centered evaluation protocol. REMSA outperforms several baselines, including naive agents, dense retrieval, and unstructured RAG-based LLMs. It operates entirely on publicly available metadata and does not access private or sensitive data.

</details>


### [154] [Planning with Sketch-Guided Verification for Physics-Aware Video Generation](https://arxiv.org/abs/2511.17450)
*Yidong Huang,Zun Wang,Han Lin,Dong-Ki Kim,Shayegan Omidshafiei,Jaehong Yoon,Yue Zhang,Mohit Bansal*

Main category: cs.CV

TL;DR: 提出无训练的SketchVerify框架，通过测试时采样和验证循环改进运动规划质量，实验显示其在运动质量、物理真实性和长期一致性上表现更好且效率更高。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成方法中，单镜头规划局限于简单运动，迭代细化计算成本高，需改进运动规划方法。

Method: 提出SketchVerify框架，预测多个候选运动计划，用视觉语言验证器对其排序，将轨迹渲染为轻量级视频草图来高效评分，迭代优化运动计划后进行最终合成。

Result: 在WorldModelBench和PhyWorldBench实验中，相比基线方法显著提高运动质量、物理真实性和长期一致性，且更高效，增加轨迹候选数量能提升整体性能。

Conclusion: SketchVerify框架有效改进视频生成中的运动规划，在性能和效率上有优势。

Abstract: Recent video generation approaches increasingly rely on planning intermediate control signals such as object trajectories to improve temporal coherence and motion fidelity. However, these methods mostly employ single-shot plans that are typically limited to simple motions, or iterative refinement which requires multiple calls to the video generator, incuring high computational cost. To overcome these limitations, we propose SketchVerify, a training-free, sketch-verification-based planning framework that improves motion planning quality with more dynamically coherent trajectories (i.e., physically plausible and instruction-consistent motions) prior to full video generation by introducing a test-time sampling and verification loop. Given a prompt and a reference image, our method predicts multiple candidate motion plans and ranks them using a vision-language verifier that jointly evaluates semantic alignment with the instruction and physical plausibility. To efficiently score candidate motion plans, we render each trajectory as a lightweight video sketch by compositing objects over a static background, which bypasses the need for expensive, repeated diffusion-based synthesis while achieving comparable performance. We iteratively refine the motion plan until a satisfactory one is identified, which is then passed to the trajectory-conditioned generator for final synthesis. Experiments on WorldModelBench and PhyWorldBench demonstrate that our method significantly improves motion quality, physical realism, and long-term consistency compared to competitive baselines while being substantially more efficient. Our ablation study further shows that scaling up the number of trajectory candidates consistently enhances overall performance.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [155] [Parrot: Persuasion and Agreement Robustness Rating of Output Truth -- A Sycophancy Robustness Benchmark for LLMs](https://arxiv.org/abs/2511.17220)
*Yusuf Çelebi,Mahmoud El Hussieni,Özay Ezerceli*

Main category: cs.CL

TL;DR: 研究提出PARROT框架测量大语言模型在社会压力下的准确性下降，评估22个模型，发现模型表现异质性大，建议将抗过拟合压力作为重要目标。


<details>
  <summary>Details</summary>
Motivation: 测量大语言模型在权威和说服等社会压力下因过度顺从导致的准确性下降现象。

Method: 1. 用双盲评估对比中性和权威错误版本问题以分离因果效应；2. 用基于对数似然的校准跟踪量化信心转变；3. 用八态行为分类法系统分类失败模式。

Result: 评估22个模型，先进模型跟随率低、准确性损失小，旧/小模型有严重认知崩溃，不同领域表现不同。

Conclusion: 应将“抗过拟合压力”作为重要目标，与准确性、避免伤害和隐私一同考虑，以实现安全部署。

Abstract: This study presents PARROT (Persuasion and Agreement Robustness Rating of Output Truth), a robustness focused framework designed to measure the degradation in accuracy that occurs under social pressure exerted on users through authority and persuasion in large language models (LLMs) the phenomenon of sycophancy (excessive conformity). PARROT (i) isolates causal effects by comparing the neutral version of the same question with an authoritatively false version using a double-blind evaluation, (ii) quantifies confidence shifts toward the correct and imposed false responses using log-likelihood-based calibration tracking, and (iii) systematically classifies failure modes (e.g., robust correct, sycophantic agreement, reinforced error, stubborn error, self-correction, etc.) using an eight-state behavioral taxonomy. We evaluated 22 models using 1,302 MMLU-style multiple-choice questions across 13 domains and domain-specific authority templates. Findings show marked heterogeneity: advanced models (e.g., GPT-5, GPT-4.1, Claude Sonnet 4.5) exhibit low "follow rates" ($\leq 11\%$, GPT-5: 4\%) and minimal accuracy loss, while older/smaller models show severe epistemic collapse (GPT-4: 80\%, Qwen 2.5-1.5B: 94\%). The danger is not limited to response changes; weak models reduce confidence in the correct response while increasing confidence in the imposed incorrect response. While international law and global knowledge at the domain level exhibit high fragility, elementary mathematics is relatively resilient. Consequently, we argue that the goal of "resistance to overfitting pressure" should be addressed as a primary objective alongside accuracy, harm avoidance, and privacy for safe deployment in the real world.

</details>


### [156] [Bench360: Benchmarking Local LLM Inference from 360°](https://arxiv.org/abs/2511.16682)
*Linus Stuhlmann,Mauricio Fadel Argerich,Jonathan Fürst*

Main category: cs.CL

TL;DR: 介绍Bench360，可对本地大语言模型推理进行360度基准测试，通过多任务多平台验证，揭示任务性能和系统效率权衡，凸显框架必要性。


<details>
  <summary>Details</summary>
Motivation: 本地运行大语言模型配置选择多，现有基准测试目标窄且非用户导向，缺乏统一易用的基准测试工具。

Method: 提出Bench360，让用户自定义任务、数据集和指标，自动对大语言模型、推理引擎和量化级别在不同场景下进行基准测试，跟踪系统和任务特定指标。

Result: 在多硬件平台和推理引擎上对四个常见大语言模型任务进行测试，揭示任务性能和系统效率的权衡，不同推理引擎和模型存在差异。

Conclusion: 本地推理没有单一最佳设置，需要Bench360这样的框架。

Abstract: Running large language models (LLMs) locally is becoming increasingly common. While the growing availability of small open-source models and inference engines has lowered the entry barrier, users now face an overwhelming number of configuration choices. Identifying an optimal configuration -- balancing functional and non-functional requirements -- requires substantial manual effort. While several benchmarks target LLM inference, they are designed for narrow evaluation goals and not user-focused. They fail to integrate relevant system and task-specific metrics into a unified, easy-to-use benchmark that supports multiple inference engines, usage scenarios, and quantization levels. To address this gap, we present Bench360 -- Benchmarking Local LLM Inference from 360°. Bench360 allows users to easily define their own custom tasks along with datasets and relevant task-specific metrics and then automatically benchmarks selected LLMs, inference engines, and quantization levels across different usage scenarios (single stream, batch & server). Bench360 tracks a wide range of metrics, including (1) system metrics -- such as Computing Performance (e.g., latency, throughput), Resource Usage (e.g., energy per query), and Deployment (e.g., cold start time) -- and (2) task-specific metrics such as ROUGE, F1 score or accuracy. We demonstrate Bench360 on four common LLM tasks -- General Knowledge & Reasoning, QA, Summarization and Text-to-SQL -- across three hardware platforms and four state of the art inference engines. Our results reveal several interesting trade-offs between task performance and system-level efficiency, highlighting the differences in inference engines and models. Most importantly, there is no single best setup for local inference, which strongly motivates the need for a framework such as Bench360.

</details>


### [157] [AutoLink: Autonomous Schema Exploration and Expansion for Scalable Schema Linking in Text-to-SQL at Scale](https://arxiv.org/abs/2511.17190)
*Ziyang Wang,Yuanlei Zheng,Zhenbiao Cao,Xiaojin Zhang,Zhongyu Wei,Pei Fu,Zhenbo Luo,Wei Chen,Xiang Bai*

Main category: cs.CL

TL;DR: 现有文本到SQL的模式链接方法成本高、难平衡召回率和噪声且扩展性差，提出AutoLink框架，实验显示其性能优越、扩展性好。


<details>
  <summary>Details</summary>
Motivation: 现有模式链接方法存在成本高、难以平衡召回率和噪声、扩展性差等问题，需要更好的解决方案。

Method: 提出AutoLink自主代理框架，将模式链接重新表述为迭代、代理驱动的过程，由大语言模型引导动态探索和扩展链接的模式子集。

Result: 在Bird - Dev和Spider - 2.0 - Lite上取得了97.4%和91.2%的严格模式链接召回率，有有竞争力的执行准确率，在大模式上扩展性好。

Conclusion: AutoLink是工业文本到SQL系统中可扩展、高召回率的模式链接解决方案。

Abstract: For industrial-scale text-to-SQL, supplying the entire database schema to Large Language Models (LLMs) is impractical due to context window limits and irrelevant noise. Schema linking, which filters the schema to a relevant subset, is therefore critical. However, existing methods incur prohibitive costs, struggle to trade off recall and noise, and scale poorly to large databases. We present \textbf{AutoLink}, an autonomous agent framework that reformulates schema linking as an iterative, agent-driven process. Guided by an LLM, AutoLink dynamically explores and expands the linked schema subset, progressively identifying necessary schema components without inputting the full database schema. Our experiments demonstrate AutoLink's superior performance, achieving state-of-the-art strict schema linking recall of \textbf{97.4\%} on Bird-Dev and \textbf{91.2\%} on Spider-2.0-Lite, with competitive execution accuracy, i.e., \textbf{68.7\%} EX on Bird-Dev (better than CHESS) and \textbf{34.9\%} EX on Spider-2.0-Lite (ranking 2nd on the official leaderboard). Crucially, AutoLink exhibits \textbf{exceptional scalability}, \textbf{maintaining high recall}, \textbf{efficient token consumption}, and \textbf{robust execution accuracy} on large schemas (e.g., over 3,000 columns) where existing methods severely degrade-making it a highly scalable, high-recall schema-linking solution for industrial text-to-SQL systems.

</details>


### [158] [Training Foundation Models on a Full-Stack AMD Platform: Compute, Networking, and System Design](https://arxiv.org/abs/2511.17127)
*Quentin Anthony,Yury Tokpanov,Skyler Szot,Srivatsan Rajagopal,Praneeth Medepalli,Rishi Iyer,Vasu Shyam,Anna Golubeva,Ansh Chaurasia,Xiao Yang,Tomas Figliolia,Robert Washbourne,Drew Thorstensen,Amartey Pearson,Zack Grossbart,Jason van Patten,Emad Barsoum,Zhenyu Gu,Yao Fu,Beren Millidge*

Main category: cs.CL

TL;DR: 本文首次在纯AMD硬件上进行大规模混合专家（MoE）预训练研究，给出系统和模型设计指导，介绍训练栈，ZAYA1-base性能表现良好，证明AMD软硬件栈适合大规模预训练。


<details>
  <summary>Details</summary>
Motivation: 在纯AMD硬件上开展大规模混合专家（MoE）预训练研究，为系统和模型设计提供实用指导。

Method: 对集群和网络进行全面表征，提供微基准测试；引入并应用MI300X感知的变压器尺寸规则；详细描述训练栈和训练配方。

Result: ZAYA1-base在推理、数学和编码基准测试中表现良好，性能可与领先基础模型媲美，超越部分模型。

Conclusion: AMD硬件、网络和软件栈已成熟且经过优化，足以进行有竞争力的大规模预训练。

Abstract: We report on the first large-scale mixture-of-experts (MoE) pretraining study on pure AMD hardware, utilizing both MI300X GPUs with Pollara interconnect. We distill practical guidance for both systems and model design. On the systems side, we deliver a comprehensive cluster and networking characterization: microbenchmarks for all core collectives (all-reduce, reduce-scatter, all-gather, broadcast) across message sizes and GPU counts on Pollara. To our knowledge, this is the first at this scale. We further provide MI300X microbenchmarks on kernel sizing and memory bandwidth to inform model design. On the modeling side, we introduce and apply MI300X-aware transformer sizing rules for attention and MLP blocks and justify MoE widths that jointly optimize training throughput and inference latency. We describe our training stack in depth, including often-ignored utilities such as fault-tolerance and checkpoint-reshaping, as well as detailed information on our training recipe. We also provide a preview of our model architecture and base model - ZAYA1 (760M active, 8.3B total parameters MoE) - which will be further improved upon in forthcoming papers. ZAYA1-base achieves performance comparable to leading base models such as Qwen3-4B and Gemma3-12B at its scale and larger, and outperforms models including Llama-3-8B and OLMoE across reasoning, mathematics, and coding benchmarks. Together, these results demonstrate that the AMD hardware, network, and software stack are mature and optimized enough for competitive large-scale pretraining.

</details>


### [159] [NALA_MAINZ at BLP-2025 Task 2: A Multi-agent Approach for Bangla Instruction to Python Code Generation](https://arxiv.org/abs/2511.16787)
*Hossain Shaikh Saadi,Faria Alam,Mario Sanz-Guerrero,Minh Duc Bui,Manuel Mager,Katharina von der Wense*

Main category: cs.CL

TL;DR: 本文介绍JGU Mainz在BLP - 2025共享任务中获胜的代码生成系统，提出多智能体管道方法，取得第一名并公开代码。


<details>
  <summary>Details</summary>
Motivation: 参与BLP - 2025共享任务，从孟加拉语指令进行代码生成。

Method: 提出多智能体管道，代码生成智能体生成初始解决方案，调试智能体处理失败测试用例生成修订方案。

Result: 提交方案在共享任务中获得第一名，Pass@1分数为95.4。

Conclusion: 多智能体管道方法在从孟加拉语指令进行代码生成任务中有效，公开代码利于后续研究。

Abstract: This paper presents JGU Mainz's winning system for the BLP-2025 Shared Task on Code Generation from Bangla Instructions. We propose a multi-agent-based pipeline. First, a code-generation agent produces an initial solution from the input instruction. The candidate program is then executed against the provided unit tests (pytest-style, assert-based). Only the failing cases are forwarded to a debugger agent, which reruns the tests, extracts error traces, and, conditioning on the error messages, the current program, and the relevant test cases, generates a revised solution. Using this approach, our submission achieved first place in the shared task with a $Pass@1$ score of 95.4. We also make our code public.

</details>


### [160] [Shona spaCy: A Morphological Analyzer for an Under-Resourced Bantu Language](https://arxiv.org/abs/2511.16680)
*Happymore Masoka*

Main category: cs.CL

TL;DR: 本文介绍基于spaCy框架的斯瓦希里语开源形态分析工具Shona spaCy，评估准确率高，促进该语言NLP发展。


<details>
  <summary>Details</summary>
Motivation: 斯瓦希里语在形态分析和语言工具方面资源不足，需开发相关工具。

Method: 结合精心策划的JSON词库和语言规则，在spaCy框架上构建系统，进行词元、词性和形态特征的标注。

Result: 在正式和非正式语料库上，词性标注准确率达90%，形态特征准确率达88%，且语言决策透明。

Conclusion: Shona spaCy促进斯瓦希里语NLP可及性和数字包容，为其他班图语提供模板。

Abstract: Despite rapid advances in multilingual natural language processing (NLP), the Bantu language Shona remains under-served in terms of morphological analysis and language-aware tools. This paper presents Shona spaCy, an open-source, rule-based morphological pipeline for Shona built on the spaCy framework. The system combines a curated JSON lexicon with linguistically grounded rules to model noun-class prefixes (Mupanda 1-18), verbal subject concords, tense-aspect markers, ideophones, and clitics, integrating these into token-level annotations for lemma, part-of-speech, and morphological features. The toolkit is available via pip install shona-spacy, with source code at https://github.com/HappymoreMasoka/shona-spacy and a PyPI release at https://pypi.org/project/shona-spacy/0.1.4/. Evaluation on formal and informal Shona corpora yields 90% POS-tagging accuracy and 88% morphological-feature accuracy, while maintaining transparency in its linguistic decisions. By bridging descriptive grammar and computational implementation, Shona spaCy advances NLP accessibility and digital inclusion for Shona speakers and provides a template for morphological analysis tools for other under-resourced Bantu languages.

</details>


### [161] [Towards Hyper-Efficient RAG Systems in VecDBs: Distributed Parallel Multi-Resolution Vector Search](https://arxiv.org/abs/2511.16681)
*Dong Liu,Yanxuan Yu*

Main category: cs.CL

TL;DR: 提出Semantic Pyramid Indexing (SPI)框架解决向量数据库检索问题，实现速度、效率和质量提升。


<details>
  <summary>Details</summary>
Motivation: 现有向量数据库检索管道采用单一分辨率索引结构，无法适应不同用户查询的语义粒度，导致检索速度和上下文相关性难以平衡。

Method: 构建文档嵌入的语义金字塔，通过轻量级分类器为每个查询动态选择最佳分辨率级别，实现从粗到细的渐进式检索。

Result: 在多个RAG任务中，SPI实现了最高5.7倍的检索加速、1.8倍的内存效率提升，端到端问答F1分数最高提高2.5分。

Conclusion: SPI框架能保证检索质量和延迟界限，各组件有效，与现有基础设施兼容，可用于生产RAG系统。

Abstract: Retrieval-Augmented Generation (RAG) systems have become a dominant approach to augment large language models (LLMs) with external knowledge. However, existing vector database (VecDB) retrieval pipelines rely on flat or single-resolution indexing structures, which cannot adapt to the varying semantic granularity required by diverse user queries. This limitation leads to suboptimal trade-offs between retrieval speed and contextual relevance.
  To address this, we propose \textbf{Semantic Pyramid Indexing (SPI)}, a novel multi-resolution vector indexing framework that introduces query-adaptive resolution control for RAG in VecDBs. Unlike existing hierarchical methods that require offline tuning or separate model training, SPI constructs a semantic pyramid over document embeddings and dynamically selects the optimal resolution level per query through a lightweight classifier. This adaptive approach enables progressive retrieval from coarse-to-fine representations, significantly accelerating search while maintaining semantic coverage.
  We implement SPI as a plugin for both FAISS and Qdrant backends and evaluate it across multiple RAG tasks including MS MARCO, Natural Questions, and multimodal retrieval benchmarks. SPI achieves up to \textbf{5.7$\times$} retrieval speedup and \textbf{1.8$\times$} memory efficiency gain while improving end-to-end QA F1 scores by up to \textbf{2.5 points} compared to strong baselines. Our theoretical analysis provides guarantees on retrieval quality and latency bounds, while extensive ablation studies validate the contribution of each component. The framework's compatibility with existing VecDB infrastructures makes it readily deployable in production RAG systems. Code is availabe at \href{https://github.com/FastLM/SPI_VecDB}{https://github.com/FastLM/SPI\_VecDB}.

</details>


### [162] [How Well Do LLMs Understand Tunisian Arabic?](https://arxiv.org/abs/2511.16683)
*Mohamed Mahdi*

Main category: cs.CL

TL;DR: 研究引入含并行翻译和情感标签的突尼斯阿拉伯语数据集，对大语言模型进行三项任务基准测试，揭示模型差异，强调低资源语言纳入下一代AI系统的重要性。


<details>
  <summary>Details</summary>
Motivation: 工业级大语言模型对突尼斯阿拉伯语等低资源语言理解能力常被忽视，这会使突尼斯人难以用母语与AI交互，威胁方言传承。

Method: 引入包含突尼斯阿拉伯语、标准突尼斯阿拉伯语和英语并行翻译及情感标签的新数据集，对多个流行大语言模型进行音译、翻译和情感分析三项任务的基准测试。

Result: 不同模型之间存在显著差异，凸显了它们在理解和处理突尼斯方言方面的优势和局限性。

Conclusion: 强调将低资源语言纳入下一代AI系统的重要性，确保技术的可访问性、包容性和文化根基。

Abstract: Large Language Models (LLMs) are the engines driving today's AI agents. The better these models understand human languages, the more natural and user-friendly the interaction with AI becomes, from everyday devices like computers and smartwatches to any tool that can act intelligently. Yet, the ability of industrial-scale LLMs to comprehend low-resource languages, such as Tunisian Arabic (Tunizi), is often overlooked. This neglect risks excluding millions of Tunisians from fully interacting with AI in their own language, pushing them toward French or English. Such a shift not only threatens the preservation of the Tunisian dialect but may also create challenges for literacy and influence younger generations to favor foreign languages. In this study, we introduce a novel dataset containing parallel Tunizi, standard Tunisian Arabic, and English translations, along with sentiment labels. We benchmark several popular LLMs on three tasks: transliteration, translation, and sentiment analysis. Our results reveal significant differences between models, highlighting both their strengths and limitations in understanding and processing Tunisian dialects. By quantifying these gaps, this work underscores the importance of including low-resource languages in the next generation of AI systems, ensuring technology remains accessible, inclusive, and culturally grounded.

</details>


### [163] [Ellipsoid-Based Decision Boundaries for Open Intent Classification](https://arxiv.org/abs/2511.16685)
*Yuetian Zou,Hanlei Zhang,Hua Xu,Songze Li,Long Xiao*

Main category: cs.CL

TL;DR: 本文提出EliDecide方法解决文本开放意图分类问题，在多数据集达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有自适应决策边界方法假设已知类为各向同性分布，限制边界为球形，忽略不同方向分布差异，需改进。

Method: 提出EliDecide方法，通过监督对比学习获取特征空间，用可学习矩阵参数化椭球边界，用双损失函数优化边界。

Result: 在多个文本意图基准和问题分类数据集上达到SOTA。

Conclusion: 椭球边界灵活性展示了出色的开放意图检测能力和在复杂开放场景文本分类任务的泛化潜力。

Abstract: Textual open intent classification is crucial for real-world dialogue systems, enabling robust detection of unknown user intents without prior knowledge and contributing to the robustness of the system. While adaptive decision boundary methods have shown great potential by eliminating manual threshold tuning, existing approaches assume isotropic distributions of known classes, restricting boundaries to balls and overlooking distributional variance along different directions. To address this limitation, we propose EliDecide, a novel method that learns ellipsoid decision boundaries with varying scales along different feature directions. First, we employ supervised contrastive learning to obtain a discriminative feature space for known samples. Second, we apply learnable matrices to parameterize ellipsoids as the boundaries of each known class, offering greater flexibility than spherical boundaries defined solely by centers and radii. Third, we optimize the boundaries via a novelly designed dual loss function that balances empirical and open-space risks: expanding boundaries to cover known samples while contracting them against synthesized pseudo-open samples. Our method achieves state-of-the-art performance on multiple text intent benchmarks and further on a question classification dataset. The flexibility of the ellipsoids demonstrates superior open intent detection capability and strong potential for generalization to more text classification tasks in diverse complex open-world scenarios.

</details>


### [164] [Prompt-Based Value Steering of Large Language Models](https://arxiv.org/abs/2511.16688)
*Giulio Antonio Abbo,Tony Belpaeme*

Main category: cs.CL

TL;DR: 提出评估提示候选能否引导生成文本符合人类价值观的方法，应用于Wizard - Vicuna模型，证明无需改模型或动态优化提示也可实现价值引导。


<details>
  <summary>Details</summary>
Motivation: 现有模型微调技术静态，无法适应涉及动态价值观和偏好的日常情况，需有效方法确保生成文本符合人类价值观。

Method: 提出实用、可复现且与模型无关的程序来评估提示候选，用Schwartz理论和对话数据集进行结构化评估。

Result: 将方法应用于Wizard - Vicuna模型，对比基线提示和明确基于价值观的提示，证明无需改变模型或动态优化提示就能实现价值引导。

Conclusion: 存在不改变模型或动态优化提示实现价值引导的可能性。

Abstract: Large language models are increasingly used in applications where alignment with human values is critical. While model fine-tuning is often employed to ensure safe responses, this technique is static and does not lend itself to everyday situations involving dynamic values and preferences. In this paper, we present a practical, reproducible, and model-agnostic procedure to evaluate whether a prompt candidate can effectively steer generated text toward specific human values, formalising a scoring method to quantify the presence and gain of target values in generated responses. We apply our method to a variant of the Wizard-Vicuna language model, using Schwartz's theory of basic human values and a structured evaluation through a dialogue dataset. With this setup, we compare a baseline prompt to one explicitly conditioned on values, and show that value steering is possible even without altering the model or dynamically optimising prompts.

</details>


### [165] [Concept-Based Interpretability for Toxicity Detection](https://arxiv.org/abs/2511.16689)
*Samarth Garg,Deeksha Varshney,Divya Singh*

Main category: cs.CL

TL;DR: 研究利用毒性检测数据集亚型属性识别毒性语言，引入基于概念梯度的可解释性技术、目标词库集，计算WCA分数，还提出无词库增强策略。


<details>
  <summary>Details</summary>
Motivation: 社交网络有害内容传播，现有毒性检测对基于概念的解释探索有限，且概念对目标类的过度归因常导致分类错误。

Method: 引入基于概念梯度（CG）的可解释性技术；提出目标词库集；计算词 - 概念对齐（WCA）分数；提出无词库增强策略。

Result: 未明确提及具体结果。

Conclusion: 未明确提及具体结论，但通过一系列方法能洞察模型对更广泛毒性语言模式的归因情况。

Abstract: The rise of social networks has not only facilitated communication but also allowed the spread of harmful content. Although significant advances have been made in detecting toxic language in textual data, the exploration of concept-based explanations in toxicity detection remains limited. In this study, we leverage various subtype attributes present in toxicity detection datasets, such as obscene, threat, insult, identity attack, and sexual explicit as concepts that serve as strong indicators to identify whether language is toxic. However, disproportionate attribution of concepts towards the target class often results in classification errors. Our work introduces an interpretability technique based on the Concept Gradient (CG) method which provides a more causal interpretation by measuring how changes in concepts directly affect the output of the model. This is an extension of traditional gradient-based methods in machine learning, which often focus solely on input features. We propose the curation of Targeted Lexicon Set, which captures toxic words that contribute to misclassifications in text classification models. To assess the significance of these lexicon sets in misclassification, we compute Word-Concept Alignment (WCA) scores, which quantify the extent to which these words lead to errors due to over-attribution to toxic concepts. Finally, we introduce a lexicon-free augmentation strategy by generating toxic samples that exclude predefined toxic lexicon sets. This approach allows us to examine whether over-attribution persists when explicit lexical overlap is removed, providing insights into the model's attribution on broader toxic language patterns.

</details>


### [166] [Falsely Accused: How AI Detectors Misjudge Slightly Polished Arabic Articles](https://arxiv.org/abs/2511.16690)
*Saleh Almohaimeed,Saad Almohaimeed,Mousa Jari,Khaled A. Alobaid,Fahad Alotaibi*

Main category: cs.CL

TL;DR: 本文生成两个阿拉伯语数据集评估大语言模型和商业AI检测器区分人类与AI生成文章的能力，发现轻微AI润色会使模型性能大幅下降。


<details>
  <summary>Details</summary>
Motivation: 现有AI检测模型会将经AI轻微润色的人类文章误判为AI生成，英文已有应对但阿拉伯语没有，故开展研究。

Method: 生成两个数据集，第一个含800篇文章评估14个大语言模型和商业AI检测器，选8个最佳模型；第二个含经10个大语言模型4种润色设置的400篇人类文章，评估8个模型。

Result: 所有AI检测器都会大量误判，最佳大语言模型Claude - 4 Sonnet和商业模型originality.AI在文章被轻微润色后准确率大幅下降。

Conclusion: 轻微的AI文本润色会严重影响AI检测模型的性能。

Abstract: Many AI detection models have been developed to counter the presence of articles created by artificial intelligence (AI). However, if a human-authored article is slightly polished by AI, a shift will occur in the borderline decision of these AI detection models, leading them to consider it AI-generated article. This misclassification may result in falsely accusing authors of AI plagiarism and harm the credibility of AI detector models. In English, some efforts were made to meet this challenge, but not in Arabic. In this paper, we generated two datasets. The first dataset contains 800 Arabic articles, half AI-generated and half human-authored. We used it to evaluate 14 Large Language models (LLMs) and commercial AI detectors to assess their ability in distinguishing between human-authored and AI-generated articles. The best 8 models were chosen to act as detectors for our primary concern, which is whether they would consider slightly polished human text as AI-generated. The second dataset, Ar-APT, contains 400 Arabic human-authored articles polished by 10 LLMs using 4 polishing settings, totaling 16400 samples. We use it to evaluate the 8 nominated models and determine whether slight polishing will affect their performance. The results reveal that all AI detectors incorrectly attribute a significant number of articles to AI. The best performing LLM, Claude-4 Sonnet, achieved 83.51%, their performance decreased to 57.63% for articles slightly polished by LLaMA-3. Whereas for the best performing commercial model, originality.AI, that achieves 92% accuracy, dropped to 12% for articles slightly polished by Mistral or Gemma-3.

</details>


### [167] [Hierarchical Retrieval with Out-Of-Vocabulary Queries: A Case Study on SNOMED CT](https://arxiv.org/abs/2511.16698)
*Jonathon Dilworth,Hui Yang,Jiaoyan Chen,Yongsheng Gao*

Main category: cs.CL

TL;DR: 本文聚焦SNOMED CT中OOV查询的分层概念检索问题，提出基于语言模型本体嵌入的方法，该方法优于基线，且具有通用性。


<details>
  <summary>Details</summary>
Motivation: SNOMED CT知识检索因语言歧义、OOV查询等问题面临挑战，需要解决OOV查询的分层概念检索问题。

Method: 提出基于语言模型本体嵌入的方法，构建OOV查询并标注，测试最直接的上位概念及其不太相关的祖先概念的检索。

Result: 该方法优于包括SBERT和两种词汇匹配方法的基线。

Conclusion: 该方法具有通用性，可扩展到其他本体，同时公开了代码、工具和评估数据集。

Abstract: SNOMED CT is a biomedical ontology with a hierarchical representation of large-scale concepts. Knowledge retrieval in SNOMED CT is critical for its application, but often proves challenging due to language ambiguity, synonyms, polysemies and so on. This problem is exacerbated when the queries are out-of-vocabulary (OOV), i.e., having no equivalent matchings in the ontology. In this work, we focus on the problem of hierarchical concept retrieval from SNOMED CT with OOV queries, and propose an approach based on language model-based ontology embeddings. For evaluation, we construct OOV queries annotated against SNOMED CT concepts, testing the retrieval of the most direct subsumers and their less relevant ancestors. We find that our method outperforms the baselines including SBERT and two lexical matching methods. While evaluated against SNOMED CT, the approach is generalisable and can be extended to other ontologies. We release code, tools, and evaluation datasets at https://github.com/jonathondilworth/HR-OOV.

</details>


### [168] [Detecting and Steering LLMs' Empathy in Action](https://arxiv.org/abs/2511.16699)
*Juan P. Cadile*

Main category: cs.CL

TL;DR: 研究将共情行动视为大语言模型激活空间的线性方向，在多个模型上测试检测和引导，发现不同模型表现不同及相关影响。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型激活空间中是否存在共情行动这一线性方向。

Method: 使用基于EIA基准的对比提示，在多个模型上进行检测和引导测试。

Result: 检测方面，各模型在最优层AUROC高，非审查模型与安全训练模型表现相当，跨模型探针一致性有限；引导方面，不同模型成功率和表现不同。

Conclusion: 检测和引导差距因模型而异，安全训练可能影响引导鲁棒性，需更多模型验证。

Abstract: We investigate empathy-in-action -- the willingness to sacrifice task efficiency to address human needs -- as a linear direction in LLM activation space. Using contrastive prompts grounded in the Empathy-in-Action (EIA) benchmark, we test detection and steering across Phi-3-mini-4k (3.8B), Qwen2.5-7B (safety-trained), and Dolphin-Llama-3.1-8B (uncensored).
  Detection: All models show AUROC 0.996-1.00 at optimal layers. Uncensored Dolphin matches safety-trained models, demonstrating empathy encoding emerges independent of safety training. Phi-3 probes correlate strongly with EIA behavioral scores (r=0.71, p<0.01). Cross-model probe agreement is limited (Qwen: r=-0.06, Dolphin: r=0.18), revealing architecture-specific implementations despite convergent detection.
  Steering: Qwen achieves 65.3% success with bidirectional control and coherence at extreme interventions. Phi-3 shows 61.7% success with similar coherence. Dolphin exhibits asymmetric steerability: 94.4% success for pro-empathy steering but catastrophic breakdown for anti-empathy (empty outputs, code artifacts).
  Implications: The detection-steering gap varies by model. Qwen and Phi-3 maintain bidirectional coherence; Dolphin shows robustness only for empathy enhancement. Safety training may affect steering robustness rather than preventing manipulation, though validation across more models is needed.

</details>


### [169] [ConCISE: A Reference-Free Conciseness Evaluation Metric for LLM-Generated Answers](https://arxiv.org/abs/2511.16846)
*Seyed Mohssen Ghafari,Ronny Kol,Juan C. Quiroz,Nella Luan,Monika Patial,Chanaka Rupasinghe,Herman Wandabwa,Luiz Pizzato*

Main category: cs.CL

TL;DR: 本文提出一种无参考指标评估大语言模型响应简洁性，实验表明该指标能识别冗余，可用于对话AI系统自动评估。


<details>
  <summary>Details</summary>
Motivation: 大语言模型响应冗长，影响清晰度和用户满意度，增加开发者成本，需评估响应简洁性。

Method: 量化非必要内容，计算三种压缩率：原始响应与抽象摘要、提取摘要的压缩率，以及词移除压缩率。

Result: 所提指标能识别大语言模型输出中的冗余。

Conclusion: 该指标为对话AI系统响应简洁性的自动评估提供实用工具，无需人工标注。

Abstract: Large language models (LLMs) frequently generate responses that are lengthy and verbose, filled with redundant or unnecessary details. This diminishes clarity and user satisfaction, and it increases costs for model developers, especially with well-known proprietary models that charge based on the number of output tokens. In this paper, we introduce a novel reference-free metric for evaluating the conciseness of responses generated by LLMs. Our method quantifies non-essential content without relying on gold standard references and calculates the average of three calculations: i) a compression ratio between the original response and an LLM abstractive summary; ii) a compression ratio between the original response and an LLM extractive summary; and iii) wordremoval compression, where an LLM removes as many non-essential words as possible from the response while preserving its meaning, with the number of tokens removed indicating the conciseness score. Experimental results demonstrate that our proposed metric identifies redundancy in LLM outputs, offering a practical tool for automated evaluation of response brevity in conversational AI systems without the need for ground truth human annotations.

</details>


### [170] [Deep Improvement Supervision](https://arxiv.org/abs/2511.16886)
*Arip Asadulaev,Rayan Banerjee,Fakhri Karray,Martin Takac*

Main category: cs.CL

TL;DR: 研究如何以最小改动提升Tiny Recursive Models (TRMs)效率，提出新训练方案，提升训练效率，减少前向传播次数并在ARC - 1上表现出色。


<details>
  <summary>Details</summary>
Motivation: 探究如何以最小改动进一步提升TRMs等小循环架构方法的效率。

Method: 将TRMs的潜在推理构建为无分类器引导和隐式策略改进算法形式，提出在训练中为每个循环提供目标的新训练方案。

Result: 显著提升训练效率，前向传播总数减少18倍，消除停止机制，在ARC - 1上以0.8M参数达到24%准确率，优于多数大语言模型。

Conclusion: 所提方法能有效提升TRMs效率并取得良好效果。

Abstract: Recently, it was shown that small, looped architectures, such as Tiny Recursive Models (TRMs), can outperform Large Language Models (LLMs) on complex reasoning tasks, including the Abstraction and Reasoning Corpus (ARC). In this work, we investigate a core question: how can we further improve the efficiency of these methods with minimal changes? To address this, we frame the latent reasoning of TRMs as a form of classifier-free guidance and implicit policy improvement algorithm. Building on these insights, we propose a novel training scheme that provides a target for each loop during training. We demonstrate that our approach significantly enhances training efficiency. Our method reduces the total number of forward passes by 18x and eliminates halting mechanisms, while maintaining quality comparable to standard TRMs. Notably, we achieve 24% accuracy on ARC-1 with only 0.8M parameters, outperforming most LLMs.

</details>


### [171] [Supervised Fine Tuning of Large Language Models for Domain Specific Knowledge Graph Construction:A Case Study on Hunan's Historical Celebrities](https://arxiv.org/abs/2511.17012)
*Junjie Hao,Chun Wang,Ying Qiao,Qiuyue Zuo,Qiya Song,Hua Ma,Xieping Gao*

Main category: cs.CL

TL;DR: 本文以湖南近代名人为例，提出监督微调方法提升大模型在特定领域信息提取性能，Qwen3 - 8B微调后效果最佳。


<details>
  <summary>Details</summary>
Motivation: 大模型和知识图谱可助力历史文化研究，但湖南历史名人系统数据资源有限，通用模型在低资源领域知识提取和结构化输出表现不佳。

Method: 设计针对湖南历史名人领域的细粒度、模式引导指令模板，构建指令调优数据集，对四个公开大模型进行参数高效指令微调，并制定评估标准。

Result: 所有模型微调后性能显著提升，Qwen3 - 8B表现最佳，100个样本和50次训练迭代下得分89.3866。

Conclusion: 本研究为区域历史文化领域垂直大模型微调提供新见解，凸显其在文化遗产知识提取和知识图谱构建中低成本应用潜力。

Abstract: Large language models and knowledge graphs offer strong potential for advancing research on historical culture by supporting the extraction, analysis, and interpretation of cultural heritage. Using Hunan's modern historical celebrities shaped by Huxiang culture as a case study, pre-trained large models can help researchers efficiently extract key information, including biographical attributes, life events, and social relationships, from textual sources and construct structured knowledge graphs. However, systematic data resources for Hunan's historical celebrities remain limited, and general-purpose models often underperform in domain knowledge extraction and structured output generation in such low-resource settings. To address these issues, this study proposes a supervised fine-tuning approach for enhancing domain-specific information extraction. First, we design a fine-grained, schema-guided instruction template tailored to the Hunan historical celebrities domain and build an instruction-tuning dataset to mitigate the lack of domain-specific training corpora. Second, we apply parameter-efficient instruction fine-tuning to four publicly available large language models - Qwen2.5-7B, Qwen3-8B, DeepSeek-R1-Distill-Qwen-7B, and Llama-3.1-8B-Instruct - and develop evaluation criteria for assessing their extraction performance. Experimental results show that all models exhibit substantial performance gains after fine-tuning. Among them, Qwen3-8B achieves the strongest results, reaching a score of 89.3866 with 100 samples and 50 training iterations. This study provides new insights into fine-tuning vertical large language models for regional historical and cultural domains and highlights their potential for cost-effective applications in cultural heritage knowledge extraction and knowledge graph construction.

</details>


### [172] [Learning to Compress: Unlocking the Potential of Large Language Models for Text Representation](https://arxiv.org/abs/2511.17129)
*Yeqin Zhang,Yizheng Zhao,Chen Hu,Binxing Jiao,Daxin Jiang,Ruihang Miao,Cam-Tu Nguyen*

Main category: cs.CL

TL;DR: 本文探索将上下文压缩作为预训练任务来无监督适配大语言模型进行文本表示，实验表明其效果优于基于标记级预训练任务的模型，结合对比学习的LLM2Comp在多任务上表现出色且更省数据。


<details>
  <summary>Details</summary>
Motivation: 多数大语言模型因固有因果性和为下一个标记预测优化，不适合生成整体文本表示，现有预训练任务多依赖标记级预测目标，需探索新方法。

Method: 将上下文压缩作为预训练任务，让模型学习生成紧凑的记忆标记替代整个上下文进行下游序列预测，结合对比学习。

Result: 设计良好的压缩目标能显著提升基于大语言模型的文本表示，优于基于标记级预训练任务的模型，LLM2Comp在多任务上表现优于当代基于大语言模型的文本编码器且更省数据。

Conclusion: 上下文压缩作为预训练任务可有效提升大语言模型的文本表示能力，结合对比学习的LLM2Comp是一个强大的文本表示模型。

Abstract: Text representation plays a critical role in tasks like clustering, retrieval, and other downstream applications. With the emergence of large language models (LLMs), there is increasing interest in harnessing their capabilities for this purpose. However, most of the LLMs are inherently causal and optimized for next-token prediction, making them suboptimal for producing holistic representations. To address this, recent studies introduced pretext tasks to adapt LLMs for text representation. Most of these tasks, however, rely on token-level prediction objectives, such as the masked next-token prediction (MNTP) used in LLM2Vec. In this work, we explore the untapped potential of context compression as a pretext task for unsupervised adaptation of LLMs. During compression pre-training, the model learns to generate compact memory tokens, which substitute the whole context for downstream sequence prediction. Experiments demonstrate that a well-designed compression objective can significantly enhance LLM-based text representations, outperforming models trained with token-level pretext tasks. Further improvements through contrastive learning produce a strong representation model (LLM2Comp) that outperforms contemporary LLM-based text encoders on a wide range of tasks while being more sample-efficient, requiring significantly less training data.

</details>


### [173] [The PLLuM Instruction Corpus](https://arxiv.org/abs/2511.17161)
*Piotr Pęzik,Filip Żarnecki,Konrad Kaczyński,Anna Cichosz,Zuzanna Deckert,Monika Garnys,Izabela Grabarczyk,Wojciech Janowski,Sylwia Karasińska,Aleksandra Kujawiak,Piotr Misztela,Maria Szymańska,Karolina Walkusz,Igor Siek,Maciej Chrabąszcz,Anna Kołos,Agnieszka Karlińska,Karolina Seweryn,Aleksandra Krasnodębska,Paula Betscher,Zofia Cieślińska,Katarzyna Kowol,Artur Wilczek,Maciej Trzciński,Katarzyna Dziewulska,Roman Roszko,Tomasz Bernaś,Jurgita Vaičenonienė,Danuta Roszko,Paweł Levchuk,Paweł Kowalski,Irena Prawdzic-Jankowska,Marek Kozłowski,Sławomir Dadas,Rafał Poświata,Alina Wróblewska,Katarzyna Krasnowska-Kieraś,Maciej Ogrodniczuk,Michał Rudolf,Piotr Rybak,Karolina Saputa,Joanna Wołoszyn,Marcin Oleksy,Bartłomiej Koptyra,Teddy Ferdinan,Stanisław Woźniak,Maciej Piasecki,Paweł Walkowiak,Konrad Wojtasik,Arkadiusz Janz,Przemysław Kazienko,Julia Moska,Jan Kocoń*

Main category: cs.CL

TL;DR: 介绍PLLuM项目微调大语言模型的指令数据集，提出功能类型，分享使用不同数据集的观察，发布PLLuMIC子集。


<details>
  <summary>Details</summary>
Motivation: 为大语言模型的语言适配提供指导，规划类似数据集开发。

Method: 描述用于微调的指令数据集，提出功能类型。

Result: 发布PLLuM指令语料库的首个代表性子集PLLuMIC。

Conclusion: PLLuMIC对其他大语言模型的类似数据集开发有指导和规划作用。

Abstract: This paper describes the instruction dataset used to fine-tune a set of transformer-based large language models (LLMs) developed in the PLLuM (Polish Large Language Model) project. We present a functional typology of the organic, converted, and synthetic instructions used in PLLuM and share some observations about the implications of using human-authored versus synthetic instruction datasets in the linguistic adaptation of base LLMs. Additionally, we release the first representative subset of the PLLuM instruction corpus (PLLuMIC), which we believe to be useful in guiding and planning the development of similar datasets for other LLMs.

</details>


### [174] [Hallucinate Less by Thinking More: Aspect-Based Causal Abstention for Large Language Models](https://arxiv.org/abs/2511.17170)
*Vy Nguyen,Ziqi Xu,Jeffrey Chan,Estrid He,Feng Xia,Xiuzhen Zhang*

Main category: cs.CL

TL;DR: 提出基于方面的因果弃权框架ABCA解决大语言模型幻觉问题，实验表明其提升弃权可靠性等。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型弃权方法依赖生成后信号，难以提前防止不可靠响应，需新方法。

Method: 引入ABCA框架，通过因果推断分析大语言模型知识内部多样性，估计基于方面的因果效应，有两种弃权类型。

Result: 在标准基准测试中，ABCA提升了弃权可靠性，达到了最优性能，增强了弃权决策的可解释性。

Conclusion: ABCA是一种有效的解决大语言模型幻觉问题的框架。

Abstract: Large Language Models (LLMs) often produce fluent but factually incorrect responses, a phenomenon known as hallucination. Abstention, where the model chooses not to answer and instead outputs phrases such as "I don't know", is a common safeguard. However, existing abstention methods typically rely on post-generation signals, such as generation variations or feedback, which limits their ability to prevent unreliable responses in advance. In this paper, we introduce Aspect-Based Causal Abstention (ABCA), a new framework that enables early abstention by analysing the internal diversity of LLM knowledge through causal inference. This diversity reflects the multifaceted nature of parametric knowledge acquired from various sources, representing diverse aspects such as disciplines, legal contexts, or temporal frames. ABCA estimates causal effects conditioned on these aspects to assess the reliability of knowledge relevant to a given query. Based on these estimates, we enable two types of abstention: Type-1, where aspect effects are inconsistent (knowledge conflict), and Type-2, where aspect effects consistently support abstention (knowledge insufficiency). Experiments on standard benchmarks demonstrate that ABCA improves abstention reliability, achieves state-of-the-art performance, and enhances the interpretability of abstention decisions.

</details>


### [175] [Attention-Guided Feature Fusion (AGFF) Model for Integrating Statistical and Semantic Features in News Text Classification](https://arxiv.org/abs/2511.17184)
*Mohammad Zare*

Main category: cs.CL

TL;DR: 本文提出AGFF模型结合统计和语义特征进行新闻文本分类，在基准数据集上表现优于传统和纯语义模型，证明融合特征可提升分类准确率。


<details>
  <summary>Details</summary>
Motivation: 传统新闻文本分类方法依赖统计特征，无法反映上下文含义；现代深度学习方法利用语义特征，但可能忽略统计指标，因此需要一种能结合两者优势的方法。

Method: 引入Attention - Guided Feature Fusion (AGFF) 模型，用基于注意力的机制动态确定各特征类型的相对重要性。

Result: 在基准新闻数据集上，AGFF模型表现优于传统统计模型和纯语义深度学习模型，消融研究验证了融合过程中各组件的贡献。

Conclusion: 战略性整合不同特征类型可显著提高分类准确率，AGFF模型能平衡和利用统计与语义表示的互补优势，是实际新闻分类任务的有效解决方案。

Abstract: News text classification is a crucial task in natural language processing, essential for organizing and filtering the massive volume of digital content. Traditional methods typically rely on statistical features like term frequencies or TF-IDF values, which are effective at capturing word-level importance but often fail to reflect contextual meaning. In contrast, modern deep learning approaches utilize semantic features to understand word usage within context, yet they may overlook simple, high-impact statistical indicators. This paper introduces an Attention-Guided Feature Fusion (AGFF) model that combines statistical and semantic features in a unified framework. The model applies an attention-based mechanism to dynamically determine the relative importance of each feature type, enabling more informed classification decisions. Through evaluation on benchmark news datasets, the AGFF model demonstrates superior performance compared to both traditional statistical models and purely semantic deep learning models. The results confirm that strategic integration of diverse feature types can significantly enhance classification accuracy. Additionally, ablation studies validate the contribution of each component in the fusion process. The findings highlight the model's ability to balance and exploit the complementary strengths of statistical and semantic representations, making it a practical and effective solution for real-world news classification tasks.

</details>


### [176] [Lost in Translation and Noise: A Deep Dive into the Failure Modes of VLMs on Real-World Tables](https://arxiv.org/abs/2511.17238)
*Anshul Singh,Rohan Chaudhary,Gagneet Singh,Abhay Kumary*

Main category: cs.CL

TL;DR: 提出新基准MirageTVQA评估VLMs，发现领先模型面临视觉噪声性能下降和英语优先偏差问题。


<details>
  <summary>Details</summary>
Motivation: 现有表格QA数据集多单语且格式完美，与现实场景有差距，需新基准评估VLMs。

Method: 创建MirageTVQA基准，含近60000个跨24种语言的QA对，用含视觉噪声的表格测试模型。

Result: 领先VLMs在面对视觉噪声时性能严重下降超35%，存在英语优先偏差，推理能力难迁移到其他语言。

Conclusion: MirageTVQA为表格推理的更鲁棒VLM模型提供衡量和推动进步的基准。

Abstract: The impressive performance of VLMs is largely measured on benchmarks that fail to capture the complexities of real-world scenarios. Existing datasets for tabular QA, such as WikiTableQuestions and FinQA, are overwhelmingly monolingual (English) and present tables in a digitally perfect, clean format. This creates a significant gap between research and practice. To address this, we present \textbf{MirageTVQA}, a new benchmark designed to evaluate VLMs on these exact dimensions. Featuring nearly 60,000 QA pairs across 24 languages, MirageTVQA challenges models with tables that are not only multilingual but also visually imperfect, incorporating realistic noise to mimic scanned documents. Our evaluation of the leading VLMs reveals two primary failure points: a severe degradation in performance (over 35\% drop for the best models) when faced with visual noise and a consistent English-first bias where reasoning abilities fail to transfer to other languages. MirageTVQA provides a benchmark for measuring and driving progress towards more robust VLM models for table reasoning. The dataset and the code are available at: https://github.com/anshulsc/MirageTVQA.

</details>


### [177] [Selective Rotary Position Embedding](https://arxiv.org/abs/2511.17388)
*Sajad Movahedi,Timur Carstensen,Arshia Afzal,Frank Hutter,Antonio Orvieto,Volkan Cevher*

Main category: cs.CL

TL;DR: 提出Selective RoPE输入依赖旋转嵌入机制，可用于线性和softmax变压器，验证其在语言建模和序列任务中提升性能。


<details>
  <summary>Details</summary>
Motivation: 受选择性在语言相关任务中表现的启发，改进现有的位置编码机制。

Method: 引入Selective RoPE机制，分析softmax注意力对查询 - 键对的旋转，研究状态空间模型和门控线性变压器中实部和虚部作用。

Result: 将Selective RoPE应用于门控变压器，在语言建模和困难序列任务中性能提升。

Conclusion: Selective RoPE机制有效，其输入依赖旋转能改善语言建模和序列任务性能。

Abstract: Position information is essential for language modeling. In softmax transformers, Rotary Position Embeddings (\textit{RoPE}) encode positions through \textit{fixed-angle} rotations, while in linear transformers, order is handled via input-dependent (selective) gating that decays past key-value associations. Selectivity has generally been shown to improve language-related tasks. Inspired by this, we introduce \textit{Selective RoPE}, an \textit{input-dependent} rotary embedding mechanism, that generalizes \textit{RoPE}, and enables rotation in \textit{arbitrary angles} for both linear and softmax transformers. We show that softmax attention already performs a hidden form of these rotations on query-key pairs, uncovering an implicit positional structure. We further show that in state-space models and gated linear transformers, the real part manages forgetting while the imaginary part encodes positions through rotations. We validate our method by equipping gated transformers with \textit{Selective RoPE}, demonstrating that its input-dependent rotations improve performance in language modeling and on difficult sequence tasks like copying, state tracking, and retrieval.

</details>


### [178] [Large Language Models for Sentiment Analysis to Detect Social Challenges: A Use Case with South African Languages](https://arxiv.org/abs/2511.17301)
*Koena Ronny Mabokela,Tim Schlippe,Matthias Wölfel*

Main category: cs.CL

TL;DR: 分析多个大语言模型在南非语言社交媒体帖子情感分析中的零样本性能，发现模型、主题和语言间差异大，融合多模型结果可提升性能，能为检测社会挑战提供可靠信息。


<details>
  <summary>Details</summary>
Motivation: 现有研究未利用大语言模型对南非语言社交媒体帖子进行情感分析和检测社会挑战，而情感分析有助于政府部门精准处理社会问题。

Method: 分析GPT - 3.5、GPT - 4、LlaMa 2、PaLM 2和Dolly 2在英语、塞佩迪语和茨瓦纳语社交媒体帖子中对10个新兴主题的零样本情感极性分析表现。

Result: 不同大语言模型、主题和语言间存在较大差异，融合不同大语言模型的结果可大幅提升情感分类性能，情感分类错误率低于1%。

Conclusion: 现在可以提供能生成可靠情感分析信息的系统，用于检测社会挑战并得出特定主题和不同语言群体的行动需求结论。

Abstract: Sentiment analysis can aid in understanding people's opinions and emotions on social issues. In multilingual communities sentiment analysis systems can be used to quickly identify social challenges in social media posts, enabling government departments to detect and address these issues more precisely and effectively. Recently, large-language models (LLMs) have become available to the wide public and initial analyses have shown that they exhibit magnificent zero-shot sentiment analysis abilities in English. However, there is no work that has investigated to leverage LLMs for sentiment analysis on social media posts in South African languages and detect social challenges. Consequently, in this work, we analyse the zero-shot performance of the state-of-the-art LLMs GPT-3.5, GPT-4, LlaMa 2, PaLM 2, and Dolly 2 to investigate the sentiment polarities of the 10 most emerging topics in English, Sepedi and Setswana social media posts that fall within the jurisdictional areas of 10 South African government departments. Our results demonstrate that there are big differences between the various LLMs, topics, and languages. In addition, we show that a fusion of the outcomes of different LLMs provides large gains in sentiment classification performance with sentiment classification errors below 1%. Consequently, it is now feasible to provide systems that generate reliable information about sentiment analysis to detect social challenges and draw conclusions about possible needs for actions on specific topics and within different language groups.

</details>


### [179] [Masked-and-Reordered Self-Supervision for Reinforcement Learning from Verifiable Rewards](https://arxiv.org/abs/2511.17473)
*Zhen Wang,Zhifeng Gao,Guolin Ke*

Main category: cs.CL

TL;DR: 提出MR - RLVR方法提升大语言模型数学推理中RLVR的可扩展性和性能，在多个数据集上取得增益。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR在数学语料尤其是定理证明中可扩展性有限，token - level SFT易变成死记硬背，需改进方法。

Method: 提出MR - RLVR，通过“masked - then - fill”和“step reordering”构建过程级自监督奖励，训练分两阶段，先自监督训练，后在仅结果可验证的数据集上进行RLVR微调。

Result: 在Qwen2.5 - 3B和DeepSeek - R1 - Distill - Qwen - 1.5B上实现MR - RLVR，在多个数据集上，MR - RLVR较原始RLVR在Pass@1、Pass@5、Pass@8分别有+9.86%、+5.27%、+4.00%的平均相对增益。

Conclusion: 融入过程感知的自监督信号能有效增强RLVR在仅结果可验证场景中的可扩展性和性能。

Abstract: Test-time scaling has been shown to substantially improve large language models' (LLMs) mathematical reasoning. However, for a large portion of mathematical corpora, especially theorem proving, RLVR's scalability is limited: intermediate reasoning is crucial, while final answers are difficult to directly and reliably verify. Meanwhile, token-level SFT often degenerates into rote memorization rather than inducing longer chains of thought. Inspired by BERT's self-supervised tasks, we propose MR-RLVR (Masked-and-Reordered RLVR), which constructs process-level self-supervised rewards via "masked-then-fill" and "step reordering" to extract learnable signals from intermediate reasoning. Our training pipeline comprises two stages: we first perform self-supervised training on sampled mathematical calculation and proof data; we then conduct RLVR fine-tuning on mathematical calculation datasets where only outcomes are verifiable. We implement MR-RLVR on Qwen2.5-3B and DeepSeek-R1-Distill-Qwen-1.5B, and evaluate on AIME24, AIME25, AMC23, and MATH500. Under a fixed sampling and decoding budget, MR-RLVR achieves average relative gains over the original RLVR of +9.86% Pass@1, +5.27% Pass@5, and +4.00% Pass@8. These results indicate that incorporating process-aware self-supervised signals can effectively enhance RLVR's scalability and performance in only outcome-verifiable settings.

</details>


### [180] [Beyond Multiple Choice: A Hybrid Framework for Unifying Robust Evaluation and Verifiable Reasoning Training](https://arxiv.org/abs/2511.17405)
*Yesheng Liu,Hao Li,Haiyu Xu,Baoqi Pei,Jiahao Wang,Mingxuan Zhao,Jingshu Zheng,Zheqi He,JG Yao,Bowen Qin,Xi Yang,Jiajun Zhang*

Main category: cs.CL

TL;DR: 论文指出MCQA选项可能泄露信号致指标不可靠，提出ReVeL框架，改写问题，实验显示其在训练和评估上有优势。


<details>
  <summary>Details</summary>
Motivation: 解决MCQA选项可能泄露信号，导致准确率指标无法反映真实能力，鼓励猜测行为的问题。

Method: 提出ReVeL框架，按答案类型分类问题，采用不同改写和验证方案；转换20k MCQA示例，用GRPO微调Qwen2.5 - VL模型。

Result: 基于ReVeL - OpenQA训练的模型在选择题基准上匹配MCQA准确率，OpenQA准确率提高约6个百分点；评估时揭示MCQA基准高达20个百分点的分数膨胀，提高判断准确率，降低成本和延迟。

Conclusion: ReVeL框架具有更好的数据效率和更稳健的奖励信号，在训练和评估方面优于基于MCQA的方法，后续将公开代码和数据。

Abstract: Multiple-choice question answering (MCQA) has been a popular format for evaluating and reinforcement fine-tuning (RFT) of modern multimodal language models. Its constrained output format allows for simplified, deterministic automatic verification. However, we find that the options may leak exploitable signals, which makes the accuracy metrics unreliable for indicating real capabilities and encourages explicit or implicit answer guessing behaviors during RFT. We propose ReVeL (Rewrite and Verify by LLM), a framework that rewrites multiple-choice questions into open-form questions while keeping answers verifiable whenever possible. The framework categorizes questions according to different answer types, apply different rewriting and verification schemes, respectively. When applied for RFT, we converted 20k MCQA examples and use GRPO to finetune Qwen2.5-VL models. Models trained on ReVeL-OpenQA match MCQA accuracy on multiple-choice benchmarks and improve OpenQA accuracy by about six percentage points, indicating better data efficiency and more robust reward signals than MCQA-based training. When used for evaluation, ReVeL also reveals up to 20 percentage points of score inflation in MCQA benchmarks (relative to OpenQA), improves judging accuracy, and reduces both cost and latency. We will release code and data publicly.

</details>


### [181] [SMILE: A Composite Lexical-Semantic Metric for Question-Answering Evaluation](https://arxiv.org/abs/2511.17432)
*Shrikant Kendre,Austin Xu,Honglu Zhou,Michael Ryoo,Shafiq Joty,Juan Carlos Niebles*

Main category: cs.CL

TL;DR: 传统问答评估指标有局限，提出SMILE方法结合句级和关键词级语义理解及关键词匹配，评估效果好且计算轻量。


<details>
  <summary>Details</summary>
Motivation: 传统问答评估指标重n - gram词法相似性，缺乏语义理解；BERTScore等难平衡句级和关键词级语义且忽略词法相似性；LLM评估器有成本高、有偏差等问题。

Method: 引入SMILE方法，结合句子级语义理解、关键词级语义理解和简单关键词匹配。

Result: 在文本、图像和视频问答任务的大量基准测试中，SMILE与人类判断高度相关且计算轻量。

Conclusion: SMILE能弥合词法和语义评估之间的差距。

Abstract: Traditional evaluation metrics for textual and visual question answering, like ROUGE, METEOR, and Exact Match (EM), focus heavily on n-gram based lexical similarity, often missing the deeper semantic understanding needed for accurate assessment. While measures like BERTScore and MoverScore leverage contextual embeddings to address this limitation, they lack flexibility in balancing sentence-level and keyword-level semantics and ignore lexical similarity, which remains important. Large Language Model (LLM) based evaluators, though powerful, come with drawbacks like high costs, bias, inconsistency, and hallucinations. To address these issues, we introduce SMILE: Semantic Metric Integrating Lexical Exactness, a novel approach that combines sentence-level semantic understanding with keyword-level semantic understanding and easy keyword matching. This composite method balances lexical precision and semantic relevance, offering a comprehensive evaluation. Extensive benchmarks across text, image, and video QA tasks show SMILE is highly correlated with human judgments and computationally lightweight, bridging the gap between lexical and semantic evaluation.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [182] [Trust-Aware Multimodal Data Fusion for Yield Estimation: A Case Study of the 2020 Beirut Explosion](https://arxiv.org/abs/2511.16816)
*Lekha Patel,Craig Ulmer,Stephen J. Verzi,Daniel J. Krofcheck,Indu Manickam,Asmeret Naugle,Jaideep Ray*

Main category: stat.AP

TL;DR: 提出贝叶斯分数后验框架融合多源数据估计2020年贝鲁特爆炸当量，结果为0.34 - 0.48 kt TNT，该方法不确定性量化更优，能集成定性与定量评估。


<details>
  <summary>Details</summary>
Motivation: 解决从异构观测数据估计爆炸当量这一逆问题的挑战，特别是融合传统物理测量与现代人工智能解释模态数据的问题。

Method: 提出贝叶斯分数后验框架，通过狄利克雷先验为各数据模态学习信任权重，自动校准不同观测数据的相对信息含量。

Result: 对贝鲁特爆炸估计当量为0.34 - 0.48 kt TNT，爆轰效率为理论最大值的12% - 17%，分数后验方法相比单模态估计在不确定性量化上更优。

Conclusion: 该工作建立了将定性评估与定量物理测量相结合的原则性框架，可应用于爆炸监测、灾害响应和法医分析。

Abstract: The estimation of explosive yield from heterogeneous observational data presents fundamental challenges in inverse problems, particularly when combining traditional physical measurements with modern artificial intelligence-interpreted modalities. We present a novel Bayesian fractional posterior framework that fuses seismic waves, crater dimensions, synthetic aperture radar imagery, and vision-language model interpreted ground-level images to estimate the yield of the 2020 Beirut explosion. Unlike conventional approaches that may treat data sources equally, our method learns trust weights for each modality through a Dirichlet prior, automatically calibrating the relative information content of disparate observations. Applied to the Beirut explosion, the framework yields an estimate of 0.34--0.48 kt TNT equivalent, representing 12 to 17 percent detonation efficiency relative to the 2.75 kt theoretical maximum from the blast's stored ammonium nitrate. The fractional posterior approach demonstrates superior uncertainty quantification compared to single-modality estimates while providing robustness against systematic biases. This work establishes a principled framework for integrating qualitative assessments with quantitative physical measurements, with applications to explosion monitoring, disaster response, and forensic analysis.

</details>


### [183] [Effects of Distance Metrics and Scaling on the Perturbation Discrimination Score](https://arxiv.org/abs/2511.16954)
*Qiyuan Liu,Qirui Zhang,Jinhong Du,Siming Zhao,Jingshu Wang*

Main category: stat.AP

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The Perturbation Discrimination Score (PDS) is increasingly used to evaluate whether predicted perturbation effects remain distinguishable, including in Systema and the Virtual Cell Challenge. However, its behavior in high-dimensional gene-expression settings has not been examined in detail. We show that PDS is highly sensitive to the choice of similarity or distance measure and to the scale of predicted effects. Analysis of observed perturbation responses reveals that $\ell_1$ and $\ell_2$-based PDS behave very differently from cosine-based measures, even after norm matching. We provide geometric insight and discuss implications for future discrimination-based evaluation metrics.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [184] [DISCA: A Digital In-memory Stochastic Computing Architecture Using A Compressed Bent-Pyramid Format](https://arxiv.org/abs/2511.17265)
*Shady Agwa,Yikang Shen,Shiwei Wang,Themis Prodromakis*

Main category: cs.AR

TL;DR: 本文提出新的数字内存随机计算架构DISCA，提升矩阵乘法工作负载能效。


<details>
  <summary>Details</summary>
Motivation: 人工智能革命下，传统冯诺依曼架构有内存墙问题，边缘AI应用对硬件预算有更多限制，现有内存计算架构有设计局限。

Method: 提出利用准随机Bent - Pyramid数据格式压缩版本的数字内存随机计算架构DISCA。

Result: DISCA在500 MHz、180nm CMOS技术下每比特能效达3.59 TOPS/W。

Conclusion: 与同类架构相比，DISCA能显著提升矩阵乘法工作负载的能效。

Abstract: Nowadays, we are witnessing an Artificial Intelligence revolution that dominates the technology landscape in various application domains, such as healthcare, robotics, automotive, security, and defense. Massive-scale AI models, which mimic the human brain's functionality, typically feature millions and even billions of parameters through data-intensive matrix multiplication tasks. While conventional Von-Neumann architectures struggle with the memory wall and the end of Moore's Law, these AI applications are migrating rapidly towards the edge, such as in robotics and unmanned aerial vehicles for surveillance, thereby adding more constraints to the hardware budget of AI architectures at the edge. Although in-memory computing has been proposed as a promising solution for the memory wall, both analog and digital in-memory computing architectures suffer from substantial degradation of the proposed benefits due to various design limitations. We propose a new digital in-memory stochastic computing architecture, DISCA, utilizing a compressed version of the quasi-stochastic Bent-Pyramid data format. DISCA inherits the same computational simplicity of analog computing, while preserving the same scalability, productivity, and reliability of digital systems. Post-layout modeling results of DISCA show an energy efficiency of 3.59 TOPS/W per bit at 500 MHz using a commercial 180nm CMOS technology. Therefore, DISCA significantly improves the energy efficiency for matrix multiplication workloads by orders of magnitude if scaled and compared to its counterpart architectures.

</details>


### [185] [Layer-wise Weight Selection for Power-Efficient Neural Network Acceleration](https://arxiv.org/abs/2511.17123)
*Jiaxun Fang,Li Zhang,Shaoyi Huang*

Main category: cs.AR

TL;DR: 提出能量感知、逐层压缩框架，利用MAC和层能量特性，实验显示节能效果好且精度损失小。


<details>
  <summary>Details</summary>
Motivation: 现有CNN压缩方法在利用MAC单元能量特性上存在不足，如使用全局激活模型、粗能量代理或层无关策略，限制了在真实硬件上的效果。

Method: 构建层感知MAC能量模型，结合每层激活统计和MSB - Hamming距离分组估计卷积层能量；在量化感知训练中引入能量精度共同优化的权重选择算法和能量优先的逐层调度方法。

Result: 在不同CNN模型实验中，实现高达58.6%的能量降低，精度下降2 - 3%，优于现有基线。

Conclusion: 所提能量感知、逐层压缩框架能有效降低CNN能耗，且精度损失在可接受范围内。

Abstract: Systolic array accelerators execute CNNs with energy dominated by the switching activity of multiply accumulate (MAC) units. Although prior work exploits weight dependent MAC power for compression, existing methods often use global activation models, coarse energy proxies, or layer-agnostic policies, which limits their effectiveness on real hardware. We propose an energy aware, layer-wise compression framework that explicitly leverages MAC and layer level energy characteristics. First, we build a layer-aware MAC energy model that combines per-layer activation statistics with an MSB-Hamming distance grouping of 22-bit partial sum transitions, and integrate it with a tile-level systolic mapping to estimate convolution-layer energy. On top of this model, we introduce an energy accuracy co-optimized weight selection algorithm within quantization aware training and an energy-prioritized layer-wise schedule that compresses high energy layers more aggressively under a global accuracy constraint. Experiments on different CNN models demonstrate up to 58.6\% energy reduction with 2-3\% accuracy drop, outperforming a state-of-the-art power-aware baseline.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [186] [On the Predictive Skill of Artificial Intelligence-based Weather Models for Extreme Events using Uncertainty Quantification](https://arxiv.org/abs/2511.17176)
*Rodrigo Almeida,Noelia Otero,Miguel-Ángel Fernández-Torres,Jackie Ma*

Main category: physics.ao-ph

TL;DR: 研究基于AI的确定性天气模型对初始条件扰动的响应，生成集合预报评估极端天气预测能力，发现依赖流的扰动效果好，AI模型对温度极端事件捕捉更有效。


<details>
  <summary>Details</summary>
Motivation: 基于AI的天气预测系统准确预测极端天气有挑战，现有确定性模型在表征不确定性和捕捉极端事件方面能力有限。

Method: 采用三种扰动策略（高斯噪声、半球中心繁殖向量、大集合），为2022年8月两个重大事件生成50个成员的集合预报，用确定性和概率性指标评估集合技巧。

Result: 依赖流的扰动产生最真实的集合离散度和最高概率技巧，缩小与数值预报集合的性能差距；AI天气模型对温度极端事件捕捉比降水更有效。

Conclusion: 输入扰动可将确定性模型扩展到概率预报，为结合依赖流的扰动与生成或潜空间不确定性建模的可靠AI预警系统铺平道路。

Abstract: Accurate prediction of extreme weather events remains a major challenge for artificial intelligence based weather prediction systems. While deterministic models such as FuXi, GraphCast, and SFNO have achieved competitive forecast skill relative to numerical weather prediction, their ability to represent uncertainty and capture extremes is still limited. This study investigates how state of the art deterministic artificial intelligence based models respond to initial-condition perturbations and evaluates the resulting ensembles in forecasting extremes. Using three perturbation strategies (Gaussian noise, Hemispheric Centered Bred Vectors, and Huge Ensembles), we generate 50 member ensembles for two major events in August 2022: the Pakistan floods and the China heatwave. Ensemble skill is assessed against ERA5 and compared with IFS ENS and the probabilistic AIFSENS model using deterministic and probabilistic metrics. Results show that flow dependent perturbations produce the most realistic ensemble spread and highest probabilistic skill, narrowing but not closing the performance gap with numerical weather prediction ensembles. Across variables, artificial intelligence based weather models capture temperature extremes more effectively than precipitation. These findings demonstrate that input perturbations can extend deterministic models toward probabilistic forecasting, paving the way for approaches that combine flow dependent perturbations with generative or latent-space uncertainty modeling for reliable artificial intelligence-driven early warning systems.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [187] [Fast Decoding for Non-Adaptive Learning of Erdős--Rényi Random Graphs](https://arxiv.org/abs/2511.17240)
*Hoang Ta,Jonathan Scarlett*

Main category: cs.IT

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study the problem of learning an unknown graph via group queries on node subsets, where each query reports whether at least one edge is present among the queried nodes. In general, learning arbitrary graphs with \(n\) nodes and \(k\) edges is hard in the non-adaptive setting, requiring \(Ω\big(\min\{k^2\log n,\,n^2\}\big)\) tests even when a small error probability is allowed. We focus on learning Erdős--Rényi (ER) graphs \(G\sim\ER(n,q)\) in the non-adaptive setting, where the expected number of edges is \(\bar{k}=q\binom{n}{2}\), and we aim to design an efficient testing--decoding scheme achieving asymptotically vanishing error probability. Prior work (Li--Fresacher--Scarlett, NeurIPS 2019) presents a testing--decoding scheme that attains an order-optimal number of tests \(O(\bar{k}\log n)\) but incurs \(Ω(n^2)\) decoding time, whereas their proposed sublinear-time algorithm incurs an extra \((\log \bar{k})(\log n)\) factor in the number of tests. We extend the binary splitting approach, recently developed for non-adaptive group testing, to the ER graph learning setting, and prove that the edge set can be recovered with high probability using \(O(\bar{k}\log n)\) tests while attaining decoding time \(O(\bar{k}^{1+δ}\log n)\) for any fixed \(δ>0\).

</details>


<div id='hep-ex'></div>

# hep-ex [[Back]](#toc)

### [188] [A First Full Physics Benchmark for Highly Granular Calorimeter Surrogates](https://arxiv.org/abs/2511.17293)
*Thorsten Buss,Henry Day-Hall,Frank Gaede,Gregor Kasieczka,Katja Krüger,Anatolii Korol,Thomas Madlener,Peter McKeown*

Main category: hep-ex

TL;DR: 研究在现实模拟应用中使用高度精细的生成式量能器替代模型，介绍DDML库，比较不同生成模型，评估模型性能，发现点云模型在高度精细量能器模拟中能实现速度和精度的良好平衡。


<details>
  <summary>Details</summary>
Motivation: 当前和未来对撞机实验的物理项目需要开发量能器簇射的替代模拟器，但现有生成模型多在简化场景和单粒子情况下评估，缺乏现实模拟应用研究。

Method: 引入DDML库结合生成式量能器替代模型与现实探测器；比较规则网格和点云两种生成模型；与理想化模拟器对比；在电磁簇射模拟的重建后基准上系统评估模型性能。

Result: 点云模型在高度精细量能器模拟中能实现速度和精度的良好平衡。

Conclusion: 点云模型在高度精细量能器模拟方面优于规则网格表示的模型。

Abstract: The physics programs of current and future collider experiments necessitate the development of surrogate simulators for calorimeter showers. While much progress has been made in the development of generative models for this task, they have typically been evaluated in simplified scenarios and for single particles. This is particularly true for the challenging task of highly granular calorimeter simulation. For the first time, this work studies the use of highly granular generative calorimeter surrogates in a realistic simulation application. We introduce DDML, a generic library which enables the combination of generative calorimeter surrogates with realistic detectors implemented using the DD4hep toolkit. We compare two different generative models - one operating on a regular grid representation, and the other using a less common point cloud approach. In order to disentangle methodological details from model performance, we provide comparisons to idealized simulators which directly sample representations of different resolutions from the full simulation ground-truth. We then systematically evaluate model performance on post-reconstruction benchmarks for electromagnetic shower simulation. Beginning with a typical single particle study, we introduce a first multi-particle benchmark based on di-photon separations, before studying a first full-physics benchmark based on hadronic decays of the tau lepton. Our results indicate that models operating on a point cloud can achieve a favorable balance between speed and accuracy for highly granular calorimeter simulation compared to those which operate on a regular grid representation.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [189] [Generative MIMO Beam Map Construction for Location Recovery and Beam Tracking](https://arxiv.org/abs/2511.17007)
*Wangqian Chen,Junting Chen,Shuguang Cui*

Main category: eess.SP

TL;DR: 提出从稀疏CSI测量序列恢复位置标签的生成框架，可提高定位精度和容量。


<details>
  <summary>Details</summary>
Motivation: 现有基于ML的无线通信方法依赖带位置信息的大量准确标注数据集，获取困难且成本高。

Method: 设计双尺度特征提取方案，开发混合循环 - 卷积编码器学习移动模式，嵌入可学习的无线电地图，用基于扩散的生成解码器重建完整CSI。

Result: 数值实验表明，相比基于模型的卡尔曼滤波方法，该模型在非视距场景下定位精度提高超30%，容量增益达20%。

Conclusion: 所提生成框架有效，能解决现有方法问题，提升无线通信性能。

Abstract: Machine learning (ML) has greatly advanced data-driven channel modeling and resource optimization in wireless communication systems. However, most existing ML-based methods rely on large, accurately labeled datasets with location information, which are often difficult and costly to obtain. This paper proposes a generative framework to recover location labels directly from sequences of sparse channel state information (CSI) measurements, without explicit location labels for radio map construction. Instead of directly storing raw CSI, we learn a compact low-dimensional radio map embedding and leverage a generative model to reconstruct the high-dimensional CSI. Specifically, to address the uncertainty of sparse CSI, a dual-scale feature extraction scheme is designed to enhance feature representation by jointly exploiting correlations from angular space and across neighboring samples. We develop a hybrid recurrent-convolutional encoder to learn mobility patterns, which combines a truncation strategy and multi-scale convolutions in the recurrent neural network (RNN) to ensure feature robustness against short-term fluctuations. Unlike conventional Gaussian priors in latent space, we embed a learnable radio map to capture the location information by encoding high-level positional features from CSI measurements. Finally, a diffusion-based generative decoder reconstructs the full CSI with high fidelity by conditioning on the positional features in the radio map. Numerical experiments demonstrate that the proposed model can improve localization accuracy by over 30% and achieve a 20% capacity gain in non-line-of-sight (NLOS) scenarios compared with model-based Kalman filter approaches.

</details>


<div id='hep-th'></div>

# hep-th [[Back]](#toc)

### [190] [Fermions and Supersymmetry in Neural Network Field Theories](https://arxiv.org/abs/2511.16741)
*Samuel Frank,James Halverson,Anindita Maiti,Fabian Ruehle*

Main category: hep-th

TL;DR: 引入费米子神经网络场论，包括自由理论、相互作用及超对称模型。


<details>
  <summary>Details</summary>
Motivation: 构建费米子神经网络场论模型。

Method: 通过将中心极限定理推广到格拉斯曼变量获得自由理论，用打破输出权重统计独立性引入汤川耦合，用超仿射变换引入超对称模型。

Result: 实现无限宽度的自由狄拉克旋量、有限宽度的四费米子相互作用，引入大量相互作用的超对称量子力学和场论模型。

Conclusion: 成功引入费米子神经网络场论及相关模型。

Abstract: We introduce fermionic neural network field theories via Grassmann-valued neural networks. Free theories are obtained by a generalization of the Central Limit Theorem to Grassmann variables. This enables the realization of the free Dirac spinor at infinite width and a four fermion interaction at finite width. Yukawa couplings are introduced by breaking the statistical independence of the output weights for the fermionic and bosonic fields. A large class of interacting supersymmetric quantum mechanics and field theory models are introduced by super-affine transformations on the input that realize a superspace formalism.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [191] [A Patient-Centric Blockchain Framework for Secure Electronic Health Record Management: Decoupling Data Storage from Access Control](https://arxiv.org/abs/2511.17464)
*Tanzim Hossain Romel,Kawshik Kumar Paul,Tanberul Islam Ruhan,Maisha Rahman Mim,Abu Sayed Md. Latiful Hoque*

Main category: cs.CR

TL;DR: 提出以患者为中心的电子健康记录共享架构，分离内容存储与授权审计，给出安全目标和实现，分析成本并讨论相关问题。


<details>
  <summary>Details</summary>
Motivation: 恢复患者对电子健康记录的控制权，同时保证敏感临床数据的安全属性。

Method: 采用分离内容存储与授权审计的架构，加密FHIR资源链下存储，区块链记录加密承诺和患者签名权限，通过公钥包装分发密钥，提供Solidity参考实现。

Result: 权限授予链上平均成本78,000 gas（L1），1MB记录端到端访问延迟0.7 - 1.4s，L2部署降低gas使用10 - 13倍。

Conclusion: 该架构是恢复患者控制权并保障数据安全的可行途径，同时讨论了元数据隐私、密钥注册要求和监管考虑。

Abstract: We present a patient-centric architecture for electronic health record (EHR) sharing that separates content storage from authorization and audit. Encrypted FHIR resources are stored off-chain; a public blockchain records only cryptographic commitments and patient-signed, time-bounded permissions using EIP-712. Keys are distributed via public-key wrapping, enabling storage providers to remain honest-but-curious without risking confidentiality. We formalize security goals (confidentiality, integrity, cryptographically attributable authorization, and auditability of authorization events) and provide a Solidity reference implementation deployed as single-patient contracts. On-chain costs for permission grants average 78,000 gas (L1), and end-to-end access latency for 1 MB records is 0.7--1.4s (mean values for S3 and IPFS respectively), dominated by storage retrieval. Layer-2 deployment reduces gas usage by 10--13x, though data availability charges dominate actual costs. We discuss metadata privacy, key registry requirements, and regulatory considerations (HIPAA/GDPR), demonstrating a practical route to restoring patient control while preserving security properties required for sensitive clinical data.

</details>


### [192] [AutoBackdoor: Automating Backdoor Attacks via LLM Agents](https://arxiv.org/abs/2511.16709)
*Yige Li,Zhe Li,Wei Zhao,Nay Myat Min,Hanxun Huang,Xingjun Ma,Jun Sun*

Main category: cs.CR

TL;DR: 介绍自动化后门注入框架AutoBackdoor，能自动生成触发短语，实验显示攻击成功率超90%，现有防御常失效。


<details>
  <summary>Details</summary>
Motivation: 现有后门攻击评估方法依赖手动触发和静态数据管道，无法系统评估现代防御鲁棒性，需更严格、多样和可扩展的红队框架。

Method: 引入AutoBackdoor框架，通过自主代理驱动的管道实现触发生成、中毒数据构建和模型微调，用语言模型代理生成语义连贯、上下文感知的触发短语。

Result: 在三种现实威胁场景下评估，对开源和商业模型实验表明，少量中毒样本攻击成功率超90%，现有防御常无法缓解攻击。

Conclusion: 需要更严格和自适应的评估技术来应对代理驱动的威胁。

Abstract: Backdoor attacks pose a serious threat to the secure deployment of large language models (LLMs), enabling adversaries to implant hidden behaviors triggered by specific inputs. However, existing methods often rely on manually crafted triggers and static data pipelines, which are rigid, labor-intensive, and inadequate for systematically evaluating modern defense robustness. As AI agents become increasingly capable, there is a growing need for more rigorous, diverse, and scalable \textit{red-teaming frameworks} that can realistically simulate backdoor threats and assess model resilience under adversarial conditions. In this work, we introduce \textsc{AutoBackdoor}, a general framework for automating backdoor injection, encompassing trigger generation, poisoned data construction, and model fine-tuning via an autonomous agent-driven pipeline. Unlike prior approaches, AutoBackdoor uses a powerful language model agent to generate semantically coherent, context-aware trigger phrases, enabling scalable poisoning across arbitrary topics with minimal human effort. We evaluate AutoBackdoor under three realistic threat scenarios, including \textit{Bias Recommendation}, \textit{Hallucination Injection}, and \textit{Peer Review Manipulation}, to simulate a broad range of attacks. Experiments on both open-source and commercial models, including LLaMA-3, Mistral, Qwen, and GPT-4o, demonstrate that our method achieves over 90\% attack success with only a small number of poisoned samples. More importantly, we find that existing defenses often fail to mitigate these attacks, underscoring the need for more rigorous and adaptive evaluation techniques against agent-driven threats as explored in this work. All code, datasets, and experimental configurations will be merged into our primary repository at https://github.com/bboylyg/BackdoorLLM.

</details>


### [193] [Password Strength Analysis Through Social Network Data Exposure: A Combined Approach Relying on Data Reconstruction and Generative Models](https://arxiv.org/abs/2511.16716)
*Maurizio Atzori,Eleonora Calò,Loredana Caruccio,Stefano Cirillo,Giuseppe Polese,Giandomenico Solimando*

Main category: cs.CR

TL;DR: 提出SODA ADVANCE工具增强密码强度评估，研究大语言模型在密码评估和生成方面的能力，实验表明大语言模型有效果。


<details>
  <summary>Details</summary>
Motivation: 用户使用易记密码增加安全风险，传统密码强度评估方法不足。

Method: 提出SODA ADVANCE工具，利用多源公开数据评估密码强度；研究大语言模型在密码评估和生成方面的表现。

Result: 对100名真实用户的实验表明，大语言模型能生成符合用户画像的强密码，在结合用户画像数据时评估密码有效。

Conclusion: SODA ADVANCE可增强密码强度评估，大语言模型在密码评估和生成方面有应用潜力。

Abstract: Although passwords remain the primary defense against unauthorized access, users often tend to use passwords that are easy to remember. This behavior significantly increases security risks, also due to the fact that traditional password strength evaluation methods are often inadequate. In this discussion paper, we present SODA ADVANCE, a data reconstruction tool also designed to enhance evaluation processes related to the password strength. In particular, SODA ADVANCE integrates a specialized module aimed at evaluating password strength by leveraging publicly available data from multiple sources, including social media platforms. Moreover, we investigate the capabilities and risks associated with emerging Large Language Models (LLMs) in evaluating and generating passwords, respectively. Experimental assessments conducted with 100 real users demonstrate that LLMs can generate strong and personalized passwords possibly defined according to user profiles. Additionally, LLMs were shown to be effective in evaluating passwords, especially when they can take into account user profile data.

</details>


### [194] [Membership Inference Attacks Beyond Overfitting](https://arxiv.org/abs/2511.16792)
*Mona Khalil,Alberto Blanco-Justicia,Najeeb Jebreel,Josep Domingo-Ferrer*

Main category: cs.CR

TL;DR: 本文研究非过拟合机器学习模型成员推理攻击漏洞的根源，发现易受攻击样本常为类内离群点，并提出防御策略。


<details>
  <summary>Details</summary>
Motivation: 成员推理攻击对使用敏感数据训练的个人有隐私风险，现有防御有精度损失，且非过拟合模型也会泄露训练数据信息，需研究根源并提出防御。

Method: 对非过拟合但有泛化能力的模型中易受成员推理攻击的训练数据样本特征进行实证分析。

Result: 发现易受攻击的样本通常是其类内的离群点，如含噪声或难以分类的样本。

Conclusion: 提出潜在防御策略以保护易受攻击样本，增强机器学习模型的隐私保护能力。

Abstract: Membership inference attacks (MIAs) against machine learning (ML) models aim to determine whether a given data point was part of the model training data. These attacks may pose significant privacy risks to individuals whose sensitive data were used for training, which motivates the use of defenses such as differential privacy, often at the cost of high accuracy losses. MIAs exploit the differences in the behavior of a model when making predictions on samples it has seen during training (members) versus those it has not seen (non-members). Several studies have pointed out that model overfitting is the major factor contributing to these differences in behavior and, consequently, to the success of MIAs. However, the literature also shows that even non-overfitted ML models can leak information about a small subset of their training data. In this paper, we investigate the root causes of membership inference vulnerabilities beyond traditional overfitting concerns and suggest targeted defenses. We empirically analyze the characteristics of the training data samples vulnerable to MIAs in models that are not overfitted (and hence able to generalize). Our findings reveal that these samples are often outliers within their classes (e.g., noisy or hard to classify). We then propose potential defensive strategies to protect these vulnerable samples and enhance the privacy-preserving capabilities of ML models. Our code is available at https://github.com/najeebjebreel/mia_analysis.

</details>


### [195] [AutoGraphAD: A novel approach using Variational Graph Autoencoders for anomalous network flow detection](https://arxiv.org/abs/2511.17113)
*Georgios Anyfantis,Pere Barlet-Ros*

Main category: cs.CR

TL;DR: 提出基于异质变分图自编码器的无监督异常检测方法AutoGraphAD，无需标签数据，比之前方法快且效果好。


<details>
  <summary>Details</summary>
Motivation: 现有监督机器学习用于攻击检测需准确标记数据集，成本高，公开数据集存在攻击有限、过时和标签错误问题，需减少对标记数据的依赖。

Method: 基于异质变分图自编码器，在由连接和IP节点构成的异质图上操作，采用无监督和对比学习训练，结合重建、结构损失和KL散度得出异常分数进行检测。

Result: AutoGraphAD与以往无监督方法效果相当甚至更好，训练速度快约1.18个数量级，推理速度快约1.03个数量级。

Conclusion: AutoGraphAD无需昂贵的下游异常检测器，在实际部署中有很大优势。

Abstract: Network Intrusion Detection Systems (NIDS) are essential tools for detecting network attacks and intrusions. While extensive research has explored the use of supervised Machine Learning for attack detection and characterisation, these methods require accurately labelled datasets, which are very costly to obtain. Moreover, existing public datasets have limited and/or outdated attacks, and many of them suffer from mislabelled data. To reduce the reliance on labelled data, we propose AutoGraphAD, a novel unsupervised anomaly detection approach based on a Heterogeneous Variational Graph Autoencoder. AutoGraphAD operates on heterogeneous graphs, made from connection and IP nodes that capture network activity within a time window. The model is trained using unsupervised and contrastive learning, without relying on any labelled data. The reconstruction, structural loss, and KL divergence are then weighted and combined in an anomaly score that is then used for anomaly detection. Overall, AutoGraphAD yields the same, and in some cases better, results than previous unsupervised approaches, such as Anomal-E, but without requiring costly downstream anomaly detectors. As a result, AutoGraphAD achieves around 1.18 orders of magnitude faster training and 1.03 orders of magnitude faster inference, which represents a significant advantage for operational deployment.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [196] [Addressing A Posteriori Performance Degradation in Neural Network Subgrid Stress Models](https://arxiv.org/abs/2511.17475)
*Andy Wu,Sanjiva K. Lele*

Main category: physics.flu-dyn

TL;DR: 结合训练数据增强和降低输入复杂度可减小神经网络亚网格应力模型先验与后验性能差距。


<details>
  <summary>Details</summary>
Motivation: 解决神经网络亚网格应力模型先验性能远好于后验性能，导致后验大涡模拟失败的问题。

Method: 采用训练数据增强（用两种不同滤波器扩充训练数据）和降低输入复杂度（去除输入到神经网络的高阶项）两种方法。

Result: 用两种滤波器训练的神经网络先验无性能下降，后验在不同大涡模拟代码中更稳健；降低输入复杂度使先验与后验性能变化不明显；结合两种方法后，后验性能更能反映先验评估。

Conclusion: 结合训练数据增强和降低输入复杂度能有效减小先验与后验性能差距。

Abstract: Neural network subgrid stress models often have a priori performance that is far better than the a posteriori performance, leading to neural network models that look very promising a priori completely failing in a posteriori Large Eddy Simulations (LES). This performance gap can be decreased by combining two different methods, training data augmentation and reducing input complexity to the neural network. Augmenting the training data with two different filters before training the neural networks has no performance degradation a priori as compared to a neural network trained with one filter. A posteriori, neural networks trained with two different filters are far more robust across two different LES codes with different numerical schemes. In addition, by ablating away the higher order terms input into the neural network, the a priori versus a posteriori performance changes become less apparent. When combined, neural networks that use both training data augmentation and a less complex set of inputs have a posteriori performance far more reflective of their a priori evaluation.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [197] [Fundamental Limitations of QAOA on Constrained Problems and a Route to Exponential Enhancement](https://arxiv.org/abs/2511.17259)
*Chinonso Onah,Kristel Michielsen*

Main category: quant-ph

TL;DR: 研究通用QAOA在约束问题上的局限性，提出通过约束嵌入实现指数级改进的方法，以排列约束问题为例展示效果并可推广到其他NP - Hard约束优化问题。


<details>
  <summary>Details</summary>
Motivation: 探索通用量子近似优化算法（QAOA）在约束问题上的基本局限性，寻求改进方法。

Method: 提出最小约束增强核（CE QAOA），在乘积单热子空间内操作，用块局部XY哈密顿量混合。

Result: 对于排列约束问题，证明CE QAOA与通用QAOA的可行质量比在一定条件下随$n^2$指数增长。

Conclusion: 通过问题 - 算法协同设计，技术和保证可从排列问题推广到广泛的NP - Hard约束优化问题。

Abstract: We study fundamental limitations of the generic Quantum Approximate Optimization Algorithm (QAOA) on constrained problems where valid solutions form a low dimensional manifold inside the Boolean hypercube, and we present a provable route to exponential improvements via constraint embedding. Focusing on permutation constrained objectives, we show that the standard generic QAOA ansatz, with a transverse field mixer and diagonal r local cost, faces an intrinsic feasibility bottleneck: even after angle optimization, circuits whose depth grows at most linearly with n cannot raise the total probability mass on the feasible manifold much above the uniform baseline suppressed by the size of the full Hilber space. Against this envelope we introduce a minimal constraint enhanced kernel (CE QAOA) that operates directly inside a product one hot subspace and mixes with a block local XY Hamiltonian. For permutation constrained problems, we prove an angle robust, depth matched exponential enhancement where the ratio between the feasible mass from CE QAOA and generic QAOA grows exponentially in $n^2$ for all depths up to a linear fraction of n, under a mild polynomial growth condition on the interaction hypergraph. Thanks to the problem algorithm co design in the kernel construction, the techniques and guarantees extend beyond permutations to a broad class of NP-Hard constrained optimization problems.

</details>


### [198] [Dissecting Quantum Reinforcement Learning: A Systematic Evaluation of Key Components](https://arxiv.org/abs/2511.17112)
*Javier Lazaro,Juan-Ignacio Vazquez,Pablo Garcia-Bringas*

Main category: quant-ph

TL;DR: 本文对基于参数化量子电路的量子强化学习架构进行实验评估，对比混合与经典智能体，给出量子与经典贡献相互作用的证据并建立基准框架。


<details>
  <summary>Details</summary>
Motivation: 基于参数化量子电路的量子强化学习虽有潜力，但因训练不稳定、贫瘠高原等问题，其实用性不确定，需深入研究。

Method: 对数据嵌入策略、ansatz设计、量子测量后处理块三个关键方面进行系统实验评估，用统一PPO - CartPole框架在相同条件下对比混合与经典智能体。

Result: 输出复用在混合管道中有独特表现，数据重新上传提高可训练性和稳定性，更强纠缠会降低优化效果。

Conclusion: 研究给出量子与经典贡献相互作用的实证证据，建立了可重现的量子强化学习系统基准和组件分析框架。

Abstract: Parameterised quantum circuit (PQC) based Quantum Reinforcement Learning (QRL) has emerged as a promising paradigm at the intersection of quantum computing and reinforcement learning (RL). By design, PQCs create hybrid quantum-classical models, but their practical applicability remains uncertain due to training instabilities, barren plateaus (BPs), and the difficulty of isolating the contribution of individual pipeline components. In this work, we dissect PQC based QRL architectures through a systematic experimental evaluation of three aspects recurrently identified as critical: (i) data embedding strategies, with Data Reuploading (DR) as an advanced approach; (ii) ansatz design, particularly the role of entanglement; and (iii) post-processing blocks after quantum measurement, with a focus on the underexplored Output Reuse (OR) technique. Using a unified PPO-CartPole framework, we perform controlled comparisons between hybrid and classical agents under identical conditions. Our results show that OR, though purely classical, exhibits distinct behaviour in hybrid pipelines, that DR improves trainability and stability, and that stronger entanglement can degrade optimisation, offsetting classical gains. Together, these findings provide controlled empirical evidence of the interplay between quantum and classical contributions, and establish a reproducible framework for systematic benchmarking and component-wise analysis in QRL.

</details>


### [199] [Intrinsic preservation of plasticity in continual quantum learning](https://arxiv.org/abs/2511.17228)
*Yu-Qin Chen,Shi-Xin Zhang*

Main category: quant-ph

TL;DR: 量子学习模型可自然克服标准深度学习可塑性损失问题，在多任务和数据模态中展现优势，源于量子模型内在物理约束。


<details>
  <summary>Details</summary>
Motivation: 解决标准深度学习在动态、真实环境中存在的可塑性损失问题，实现人工智能的持续学习。

Method: 在多种学习范式和数据模态的广泛任务中，对比量子神经网络和经典模型的学习能力。

Result: 量子神经网络无论数据或任务如何，都能保持一致学习能力，而经典模型因权重和梯度无界增长导致性能下降。

Conclusion: 量子计算在机器学习中的应用不仅有潜在加速作用，还为构建自适应人工智能和终身学习者提供了可靠途径。

Abstract: Artificial intelligence in dynamic, real-world environments requires the capacity for continual learning. However, standard deep learning suffers from a fundamental issue: loss of plasticity, in which networks gradually lose their ability to learn from new data. Here we show that quantum learning models naturally overcome this limitation, preserving plasticity over long timescales. We demonstrate this advantage systematically across a broad spectrum of tasks from multiple learning paradigms, including supervised learning and reinforcement learning, and diverse data modalities, from classical high-dimensional images to quantum-native datasets. Although classical models exhibit performance degradation correlated with unbounded weight and gradient growth, quantum neural networks maintain consistent learning capabilities regardless of the data or task. We identify the origin of the advantage as the intrinsic physical constraints of quantum models. Unlike classical networks where unbounded weight growth leads to landscape ruggedness or saturation, the unitary constraints confine the optimization to a compact manifold. Our results suggest that the utility of quantum computing in machine learning extends beyond potential speedups, offering a robust pathway for building adaptive artificial intelligence and lifelong learners.

</details>


### [200] [Quantum Masked Autoencoders for Vision Learning](https://arxiv.org/abs/2511.17372)
*Emma Andrews,Prabhat Mishra*

Main category: quant-ph

TL;DR: 提出量子掩码自编码器（QMAEs），能在量子态中学习缺失特征，在MNIST图像上表现良好，分类准确率优于现有量子自编码器。


<details>
  <summary>Details</summary>
Motivation: 现有量子自编码器中缺乏能利用量子计算优势的量子掩码自编码器，为改进特征学习提出QMAEs。

Method: 提出量子掩码自编码器（QMAEs）架构。

Result: QMAE架构能学习图像掩码特征，重建图像视觉保真度更高，分类准确率平均比现有量子自编码器高12.86%。

Conclusion: QMAEs能有效学习量子态中数据样本的缺失特征，在图像重建和分类上表现出色。

Abstract: Classical autoencoders are widely used to learn features of input data. To improve the feature learning, classical masked autoencoders extend classical autoencoders to learn the features of the original input sample in the presence of masked-out data. While quantum autoencoders exist, there is no design and implementation of quantum masked autoencoders that can leverage the benefits of quantum computing and quantum autoencoders. In this paper, we propose quantum masked autoencoders (QMAEs) that can effectively learn missing features of a data sample within quantum states instead of classical embeddings. We showcase that our QMAE architecture can learn the masked features of an image and can reconstruct the masked input image with improved visual fidelity in MNIST images. Experimental evaluation highlights that QMAE can significantly outperform (12.86% on average) in classification accuracy compared to state-of-the-art quantum autoencoders in the presence of masks.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [201] [Vector Cost Behavioral Planning for Autonomous Robotic Systems with Contemporary Validation Strategies](https://arxiv.org/abs/2511.17375)
*Benjamin R. Toaz,Quentin Goss,John Thompson,Seta Boğosyan,Shaunak D. Bopardikar,Mustafa İlhan Akbaş,Metin Gökaşan*

Main category: cs.RO

TL;DR: 本文将向量成本双矩阵博弈方法扩展到任意数量的目标，结合博弈论规划和智能系统验证构建新仿真管道，显示向量成本法优于标量化方法。


<details>
  <summary>Details</summary>
Motivation: 使自主机器人系统能同时优化多目标并避免忽视某些目标的最坏情况，结合博弈论规划和智能系统验证。

Method: 将向量成本双矩阵博弈方法扩展到任意数量目标，使用可解释人工智能软件分析数据，应用SEMBAS进行参数空间性能模式探索。

Result: 向量成本法相比标量化方法有显著改进，构建了可解释和可推广的机器人行为规划框架。

Conclusion: 所提出的集成方法有效，为机器人行为规划提供了更好的解决方案。

Abstract: The vector cost bimatrix game is a method for multi-objective decision making that enables autonomous robotic systems to optimize for multiple goals at once while avoiding worst-case scenarios in neglected objectives. We expand this approach to arbitrary numbers of objectives and compare its performance to scalar weighted sum methods during competitive motion planning. Explainable Artificial Intelligence (XAI) software is used to aid in the analysis of high dimensional decision-making data. State-space Exploration of Multidimensional Boundaries using Adherence Strategies (SEMBAS) is applied to explore performance modes in the parameter space as a sensitivity study for the baseline and proposed frameworks. While some works have explored aspects of game theoretic planning and intelligent systems validation separately, we combine each of these into a novel and comprehensive simulation pipeline. This integration demonstrates a dramatic improvement of the vector cost method over scalarization and offers an interpretable and generalizable framework for robotic behavioral planning. Code available at https://github.com/toazbenj/race_simulation. The video companion to this work is available at https://tinyurl.com/vectorcostvideo.

</details>


### [202] [FORWARD: Dataset of a forwarder operating in rough terrain](https://arxiv.org/abs/2511.17318)
*Mikael Lundbäck,Erik Wallin,Carola Häggström,Mattias Nyström,Andreas Grönlund,Mats Richardson,Petrus Jönsson,William Arnvik,Lucas Hedström,Arvid Fälldin,Martin Servin*

Main category: cs.RO

TL;DR: 本文介绍了FORWARD数据集，包含瑞典中部两个采伐场伐木归堆机多模态数据，用于森林机械相关模型和算法开发。


<details>
  <summary>Details</summary>
Motivation: 开发森林机械交通适应性、感知和自主控制的模型和算法，考虑效率、油耗、安全和环境影响等因素。

Method: 使用配备多种传感器的大型Komatsu伐木归堆机收集数据，记录事件时间日志、视频、地形数据等，标注视频工作元素，设置不同实验场景。

Result: 创建了FORWARD数据集，包含约18小时木材提取工作数据、实验场景规格等。

Conclusion: 该开放数据集可用于开发森林机械模型算法，探索林业机械模拟器自动生成和校准及自动化场景描述。

Abstract: We present FORWARD, a high-resolution multimodal dataset of a cut-to-length forwarder operating in rough terrain on two harvest sites in the middle part of Sweden. The forwarder is a large Komatsu model equipped with a variety of sensors, including RTK-GNSS, 360-camera, operator vibration sensors, internal CAN-bus signal recording, and multiple IMUs. The data includes event time logs recorded in 5 Hz with e.g., driving speed, fuel consumption, vehicle position with centimeter accuracy, and crane use while the vehicle operates in forest areas laser-scanned with very high-resolution, $\sim$1500 points per square meter. Production log files (StanForD standard) with time-stamped machine events, extensive video material, and terrain data in various formats are included as well. About 18 hours of regular wood extraction work during three days is annotated from 360-video material into individual work elements and included in the dataset. We also include scenario specifications of conducted experiments on forest roads and in terrain. Scenarios include repeatedly driving the same routes with and without steel tracks, different load weight, and different target driving speeds. The dataset is intended for developing models and algorithms for trafficability, perception, and autonomous control of forest machines using artificial intelligence, simulation, and experiments on physical testbeds. In part, we focus on forwarders traversing terrain, avoiding obstacles, and loading or unloading logs, with consideration for efficiency, fuel consumption, safety, and environmental impact. Other benefits of the open dataset include the ability to explore auto-generation and calibration of forestry machine simulators and automation scenario descriptions using the data recorded in the field.

</details>


### [203] [A ROS2 Interface for Universal Robots Collaborative Manipulators Based on ur_rtde](https://arxiv.org/abs/2511.17237)
*Alessio Saccuti,Riccardo Monica,Jacopo Aleotti*

Main category: cs.RO

TL;DR: 本文基于ur_rtde C++库提出一种新型ROS2驱动用于UR机器人，灵活且开源。


<details>
  <summary>Details</summary>
Motivation: 为UR机器人操纵器提供灵活、适用于广泛应用的解决方案。

Method: 基于ur_rtde C++库开发，通过插件系统添加自定义命令。

Result: 实现了包括基于路点路径的运动执行等多个命令，驱动已开源发布。

Conclusion: 所提出的新型ROS2驱动是可行且灵活的，能满足不同应用需求。

Abstract: In this paper a novel ROS2 driver for UR robot manipulators is presented, based on the ur_rtde C++ library. The proposed driver aims to be a flexible solution, adaptable to a wide range of applications. The driver exposes the high-level commands of Universal Robots URScripts, and custom commands can be added using a plugin system. Several commands have been implemented, including motion execution along a waypoint-based path. The driver is published as open source.

</details>


### [204] [TP-MDDN: Task-Preferenced Multi-Demand-Driven Navigation with Autonomous Decision-Making](https://arxiv.org/abs/2511.17225)
*Shanshan Li,Da Huang,Yu He,Yanwei Fu,Yu-Gang Jiang,Xiangyang Xue*

Main category: cs.RO

TL;DR: 提出TP - MDDN基准及AWMSystem系统解决多需求长视野导航问题，设计MASMap，采用双节奏动作生成框架，实验显示效果优于基线。


<details>
  <summary>Details</summary>
Motivation: 传统需求驱动导航一次处理一个需求，无法反映涉及多需求和个人选择的现实任务复杂性。

Method: 引入TP - MDDN基准；提出由BreakLLM、LocateLLM和StatusMLLM组成的AWMSystem；设计MASMap；采用双节奏动作生成框架及自适应误差校正器。

Result: 实验表明在感知准确性和导航鲁棒性上优于现有基线。

Conclusion: 所提方法有效解决多需求长视野导航问题，性能表现良好。

Abstract: In daily life, people often move through spaces to find objects that meet their needs, posing a key challenge in embodied AI. Traditional Demand-Driven Navigation (DDN) handles one need at a time but does not reflect the complexity of real-world tasks involving multiple needs and personal choices. To bridge this gap, we introduce Task-Preferenced Multi-Demand-Driven Navigation (TP-MDDN), a new benchmark for long-horizon navigation involving multiple sub-demands with explicit task preferences. To solve TP-MDDN, we propose AWMSystem, an autonomous decision-making system composed of three key modules: BreakLLM (instruction decomposition), LocateLLM (goal selection), and StatusMLLM (task monitoring). For spatial memory, we design MASMap, which combines 3D point cloud accumulation with 2D semantic mapping for accurate and efficient environmental understanding. Our Dual-Tempo action generation framework integrates zero-shot planning with policy-based fine control, and is further supported by an Adaptive Error Corrector that handles failure cases in real time. Experiments demonstrate that our approach outperforms state-of-the-art baselines in both perception accuracy and navigation robustness.

</details>


### [205] [Leveraging CVAE for Joint Configuration Estimation of Multifingered Grippers from Point Cloud Data](https://arxiv.org/abs/2511.17276)
*Julien Merand,Boris Meden,Mathieu Grossard*

Main category: cs.RO

TL;DR: 本文提出一种仅根据多指夹爪多关节链点云数据确定关节配置的高效方法，利用CVAE，在MultiDex数据集上验证有效。


<details>
  <summary>Details</summary>
Motivation: 现有逆运动学技术在确定关节配置时需事后决策或数值近似求解，存在不足，需新方法。

Method: 利用条件变分自编码器（CVAE），将关键结构元素的点云数据作为输入，重建相应的关节配置。

Result: 在MultiDex抓取数据集上使用Allegro Hand验证，运行时间0.05毫秒，精度与最先进方法相当。

Conclusion: 该方法在基于AI的抓取规划关节配置估计方面有效。

Abstract: This paper presents an efficient approach for determining the joint configuration of a multifingered gripper solely from the point cloud data of its poly-articulated chain, as generated by visual sensors, simulations or even generative neural networks. Well-known inverse kinematics (IK) techniques can provide mathematically exact solutions (when they exist) for joint configuration determination based solely on the fingertip pose, but often require post-hoc decision-making by considering the positions of all intermediate phalanges in the gripper's fingers, or rely on algorithms to numerically approximate solutions for more complex kinematics. In contrast, our method leverages machine learning to implicitly overcome these challenges. This is achieved through a Conditional Variational Auto-Encoder (CVAE), which takes point cloud data of key structural elements as input and reconstructs the corresponding joint configurations. We validate our approach on the MultiDex grasping dataset using the Allegro Hand, operating within 0.05 milliseconds and achieving accuracy comparable to state-of-the-art methods. This highlights the effectiveness of our pipeline for joint configuration estimation within the broader context of AI-driven techniques for grasp planning.

</details>


### [206] [SPEAR-1: Scaling Beyond Robot Demonstrations via 3D Understanding](https://arxiv.org/abs/2511.17411)
*Nikolay Nikolov,Giuliano Albanese,Sombit Dey,Aleksandar Yanev,Luc Van Gool,Jan-Nico Zaech,Danda Pani Paudel*

Main category: cs.RO

TL;DR: 提出用3D注释丰富非机器人图像数据，训练3D感知VLM（SPEAR - VLM），并构建机器人基础模型SPEAR - 1，在少数据下表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有机器人基础模型因基于2D图像语言任务训练的VLM构建，缺乏3D空间推理能力，泛化能力受限。

Method: 用3D注释丰富易收集的非机器人图像数据，训练3D感知的SPEAR - VLM，在此基础上构建SPEAR - 1。

Result: SPEAR - 1在约4500万帧数据上训练，使用少20倍机器人演示数据，表现优于或匹配SOTA模型。

Conclusion: 精心设计的训练策略解锁VLM新能力，提升具身控制可靠性，且公开模型权重和3D注释数据集。

Abstract: Robotic Foundation Models (RFMs) hold great promise as generalist, end-to-end systems for robot control. Yet their ability to generalize across new environments, tasks, and embodiments remains limited. We argue that a major bottleneck lies in their foundations: most RFMs are built by fine-tuning internet-pretrained Vision-Language Models (VLMs). However, these VLMs are trained on 2D image-language tasks and lack the 3D spatial reasoning inherently required for embodied control in the 3D world. Bridging this gap directly with large-scale robotic data is costly and difficult to scale. Instead, we propose to enrich easy-to-collect non-robotic image data with 3D annotations and enhance a pretrained VLM with 3D understanding capabilities. Following this strategy, we train SPEAR-VLM, a 3D-aware VLM that infers object coordinates in 3D space from a single 2D image. Building on SPEAR-VLM, we introduce our main contribution, $~\textbf{SPEAR-1}$: a robotic foundation model that integrates grounded 3D perception with language-instructed embodied control. Trained on $\sim$45M frames from 24 Open X-Embodiment datasets, SPEAR-1 outperforms or matches state-of-the-art models such as $π_0$-FAST and $π_{0.5}$, while it uses 20$\times$ fewer robot demonstrations. This carefully-engineered training strategy unlocks new VLM capabilities and as a consequence boosts the reliability of embodied control beyond what is achievable with only robotic data. We make our model weights and 3D-annotated datasets publicly available.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [207] [Generative Augmented Reality: Paradigms, Technologies, and Future Applications](https://arxiv.org/abs/2511.16783)
*Chen Liang,Jiawen Zheng,Yufeng Zeng,Yi Tan,Hengye Lyu,Yuhui Zheng,Zisu Li,Yueting Weng,Jiaxin Shi,Hanwang Zhang*

Main category: cs.HC

TL;DR: 介绍生成式增强现实（GAR），将增强视为世界重新合成过程，替代传统AR引擎，阐述其计算对应、技术基础、应用前景并指出挑战。


<details>
  <summary>Details</summary>
Motivation: 提出一种下一代增强现实范式，以实现更逼真、交互性和沉浸感更强的体验。

Method: 用统一生成骨干替换传统AR引擎多阶段模块，将环境感知、虚拟内容和交互信号联合编码为连续视频生成的条件输入。

Result: 明确AR与GAR的计算对应，研究使实时生成式增强可行的技术基础，列出潜在应用。

Conclusion: GAR是未来AR范式，但也带来技术、内容生态及伦理社会方面新挑战。

Abstract: This paper introduces Generative Augmented Reality (GAR) as a next-generation paradigm that reframes augmentation as a process of world re-synthesis rather than world composition by a conventional AR engine. GAR replaces the conventional AR engine's multi-stage modules with a unified generative backbone, where environmental sensing, virtual content, and interaction signals are jointly encoded as conditioning inputs for continuous video generation. We formalize the computational correspondence between AR and GAR, survey the technical foundations that make real-time generative augmentation feasible, and outline prospective applications that leverage its unified inference model. We envision GAR as a future AR paradigm that delivers high-fidelity experiences in terms of realism, interactivity, and immersion, while eliciting new research challenges on technologies, content ecosystems, and the ethical and societal implications.

</details>


### [208] [GRAPHIC--Guidelines for Reviewing Algorithmic Practices in Human-centred Design and Interaction for Creativity](https://arxiv.org/abs/2511.17443)
*Joana Rovira Martins,Pedro Martins,Ana Boavida*

Main category: cs.HC

TL;DR: 文章基于文献综述提出GRAPHIC框架分析应用于平面设计的AI系统，揭示研究差距。


<details>
  <summary>Details</summary>
Motivation: 人工智能应用于创意领域，平面设计中集成计算系统面临挑战，需了解当前系统如何支持人机协作。

Method: 采用PRISMA方法识别872篇文章，最终筛选出71篇描述68个独特系统的出版物。

Result: 提出GRAPHIC框架，包含协作全景、过程与模式、平面设计原则三个主要维度。

Conclusion: 应用框架揭示了研究差距，如平衡主体间主动性和控制权、改进交互模型、推广基于核心设计原则的系统。

Abstract: Artificial Intelligence (AI) has been increasingly applied to creative domains, leading to the development of systems that collaborate with humans in design processes. In Graphic Design, integrating computational systems into co-creative workflows presents specific challenges, as it requires balancing scientific rigour with the subjective and visual nature of design practice. Following the PRISMA methodology, we identified 872 articles, resulting in a final corpus of 71 publications describing 68 unique systems. Based on this review, we introduce GRAPHIC (Guidelines for Reviewing Algorithmic Practices in Human-centred Design and Interaction for Creativity), a framework for analysing AI-based systems applied to Graphic Design. Its goal is to understand how current systems support human-AI collaboration in the Graphic Design discipline. The framework comprises main dimensions, which our analysis revealed to be essential across diverse system types: (1) Collaborative Panorama, (2) Processes and Modalities, and (3) Graphic Design Principles. Its application revealed research gaps, including the need to balance initiative and control between agents, improve communication through explainable interaction models, and promote systems that support transformational creativity grounded in core design principles.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [209] [Algorithmic design and implementation considerations of deep MPC](https://arxiv.org/abs/2511.17233)
*Prabhat K. Mishra,Mateus V. Gasparino,Girish Chowdhary*

Main category: eess.SY

TL;DR: 本文聚焦将神经网络与MPC结合的深度模型预测控制（Deep MPC）方法，阐述其实现挑战和控制权限分配问题，通过实验说明分配不当会导致性能不佳。


<details>
  <summary>Details</summary>
Motivation: 探索Deep MPC中神经网络与MPC结合的有效方法，解决系统控制中的模型不确定性和约束处理问题。

Method: 在Deep MPC中让神经网络和MPC控制器共同分配控制权限，神经网络学习模型不确定性，MPC处理约束，并通过数值实验分析问题。

Result: 发现控制权限分配不当会导致系统性能不佳，并通过四轮滑移转向动力学实验进行了解释。

Conclusion: 在Deep MPC中合理分配控制权限至关重要，否则会影响系统性能。

Abstract: Deep Model Predictive Control (Deep MPC) is an evolving field that integrates model predictive control and deep learning. This manuscript is focused on a particular approach, which employs deep neural network in the loop with MPC. This class of approaches distributes control authority between a neural network and an MPC controller, in such a way that the neural network learns the model uncertainties while the MPC handles constraints. The approach is appealing because training data collected while the system is in operation can be used to fine-tune the neural network, and MPC prevents unsafe behavior during those learning transients. This manuscript explains implementation challenges of Deep MPC, algorithmic way to distribute control authority and argues that a poor choice in distributing control authority may lead to poor performance. A reason of poor performance is explained through a numerical experiment on a four-wheeled skid-steer dynamics.

</details>


### [210] [A Framework for Adaptive Stabilisation of Nonlinear Stochastic Systems](https://arxiv.org/abs/2511.17436)
*Seth Siriya,Jingge Zhu,Dragan Nešić,Ye Pu*

Main category: eess.SY

TL;DR: 提出基于确定性等价学习的自适应控制策略并推导闭环系统稳定性界。


<details>
  <summary>Details</summary>
Motivation: 解决离散时间、具有线性参数化不确定性的非线性随机系统的自适应控制问题。

Method: 提出基于确定性等价学习的自适应控制策略。

Result: 推导出闭环系统在一定概率下的稳定性界，若整个状态空间信息充分且控制器族能全局稳定，则可得到高概率稳定性保证。

Conclusion: 所提策略可用于解决该类系统的自适应控制问题并获得稳定性保证。

Abstract: We consider the adaptive control problem for discrete-time, nonlinear stochastic systems with linearly parameterised uncertainty. Assuming access to a parameterised family of controllers that can stabilise the system in a bounded set within an informative region of the state space when the parameter is well-chosen, we propose a certainty equivalence learning-based adaptive control strategy, and subsequently derive stability bounds on the closed-loop system that hold for some probabilities. We then show that if the entire state space is informative, and the family of controllers is globally stabilising with appropriately chosen parameters, high probability stability guarantees can be derived.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [211] [Is Phase Really Needed for Weakly-Supervised Dereverberation ?](https://arxiv.org/abs/2511.17346)
*Marius Rodrigues,Louis Bahrman,Roland Badeau,Gaël Richard*

Main category: cs.SD

TL;DR: 研究混响语音相位在时频域作用，发现湿相位信息有限，排除其可提升去混响模型性能。


<details>
  <summary>Details</summary>
Motivation: 在无监督或弱监督语音去混响中，需评估仅从混响语音获取信息的程度，研究混响相位作用。

Method: 基于统计波场理论分析，在弱监督框架下训练去混响模型验证。

Result: 除低频外，后期混响用白噪声干扰相位分量，湿相位有用信息有限；排除混响相位可显著提升模型性能。

Conclusion: 混响语音的湿相位对弱监督去混响非必需，排除它能改善性能。

Abstract: In unsupervised or weakly-supervised approaches for speech dereverberation, the target clean (dry) signals are considered to be unknown during training. In that context, evaluating to what extent information can be retrieved from the sole knowledge of reverberant (wet) speech becomes critical. This work investigates the role of the reverberant (wet) phase in the time-frequency domain. Based on Statistical Wave Field Theory, we show that late reverberation perturbs phase components with white, uniformly distributed noise, except at low frequencies. Consequently, the wet phase carries limited useful information and is not essential for weakly supervised dereverberation. To validate this finding, we train dereverberation models under a recent weak supervision framework and demonstrate that performance can be significantly improved by excluding the reverberant phase from the loss function.

</details>


### [212] [Device-Guided Music Transfer](https://arxiv.org/abs/2511.17136)
*Manh Pham Hung,Changshuo Hu,Ting Dang,Dong Ma*

Main category: cs.SD

TL;DR: 提出DeMT方法处理扬声器频率响应曲线以实现设备引导的音乐迁移，支持多种应用。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽略播放设备的硬件属性，本文旨在解决该问题。

Method: 用视觉语言模型处理扬声器频率响应曲线提取设备嵌入，通过特征线性调制调节混合变压器，并在自收集数据集上微调。

Result: DeMT实现有效扬声器风格迁移和对未见设备的少样本适应。

Conclusion: DeMT支持设备风格增强和质量提升等应用。

Abstract: Device-guided music transfer adapts playback across unseen devices for users who lack them. Existing methods mainly focus on modifying the timbre, rhythm, harmony, or instrumentation to mimic genres or artists, overlooking the diverse hardware properties of the playback device (i.e., speaker). Therefore, we propose DeMT, which processes a speaker's frequency response curve as a line graph using a vision-language model to extract device embeddings. These embeddings then condition a hybrid transformer via feature-wise linear modulation. Fine-tuned on a self-collected dataset, DeMT enables effective speaker-style transfer and robust few-shot adaptation for unseen devices, supporting applications like device-style augmentation and quality enhancement.

</details>


### [213] [MusicAIR: A Multimodal AI Music Generation Framework Powered by an Algorithm-Driven Core](https://arxiv.org/abs/2511.17323)
*Callie C. Liao,Duoduo Liao,Ellie L. Zhang*

Main category: cs.SD

TL;DR: 提出MusicAIR框架及GenAIM工具用于AI音乐生成，实验表明系统表现良好，可辅助音乐创作。


<details>
  <summary>Details</summary>
Motivation: 解决现有基于神经网络的音乐生成模型依赖大数据集带来的版权侵权和高成本问题。

Method: 提出MusicAIR框架，其音乐核心算法连接歌词和节奏信息推导音乐特征，开发GenAIM工具实现多种形式音乐生成，用标准音乐指标和创新分析评估结果。

Result: 系统平均关键置信度达85%，优于人类作曲家的79%，符合音乐理论标准，能生成多样、类人的音乐作品。

Conclusion: GenAIM可作为可靠音乐创作助手和教育导师，降低音乐创作门槛，对AI音乐生成有重要贡献。

Abstract: Recent advances in generative AI have made music generation a prominent research focus. However, many neural-based models rely on large datasets, raising concerns about copyright infringement and high-performance costs. In contrast, we propose MusicAIR, an innovative multimodal AI music generation framework powered by a novel algorithm-driven symbolic music core, effectively mitigating copyright infringement risks. The music core algorithms connect critical lyrical and rhythmic information to automatically derive musical features, creating a complete, coherent melodic score solely from the lyrics. The MusicAIR framework facilitates music generation from lyrics, text, and images. The generated score adheres to established principles of music theory, lyrical structure, and rhythmic conventions. We developed Generate AI Music (GenAIM), a web tool using MusicAIR for lyric-to-song, text-to-music, and image-to-music generation. In our experiments, we evaluated AI-generated music scores produced by the system using both standard music metrics and innovative analysis that compares these compositions with original works. The system achieves an average key confidence of 85%, outperforming human composers at 79%, and aligns closely with established music theory standards, demonstrating its ability to generate diverse, human-like compositions. As a co-pilot tool, GenAIM can serve as a reliable music composition assistant and a possible educational composition tutor while simultaneously lowering the entry barrier for all aspiring musicians, which is innovative and significantly contributes to AI for music generation.

</details>


### [214] [Enhancing Quranic Learning: A Multimodal Deep Learning Approach for Arabic Phoneme Recognition](https://arxiv.org/abs/2511.17477)
*Ayhan Kucukmanisa,Derya Gelmez,Sukru Selim Calik,Zeynep Hilal Kilimci*

Main category: cs.SD

TL;DR: 提出基于Transformer的多模态框架用于阿拉伯语音素发音错误检测，结合声学和文本表示，评估不同融合策略，实验表明该配置效果好，有助于CALL系统发展。


<details>
  <summary>Details</summary>
Motivation: 多模态深度学习虽提升语音分析能力，但阿拉伯语尤其是古兰经诵读中准确发音检测仍是挑战，需解决该问题。

Method: 提出结合声学和文本表示的Transformer多模态框架，集成UniSpeech声学嵌入和基于Whisper转录的BERT文本嵌入，实现并评估早、中、晚期融合方法，加入公开YouTube录音增强数据多样性，用标准指标评估。

Result: UniSpeech - BERT多模态配置取得良好结果，基于融合的Transformer架构对音素级发音错误检测有效。

Conclusion: 该研究有助于开发智能、独立于说话者的多模态CALL系统，推动技术支持的古兰经发音训练和更广泛语音教育应用。

Abstract: Recent advances in multimodal deep learning have greatly enhanced the capability of systems for speech analysis and pronunciation assessment. Accurate pronunciation detection remains a key challenge in Arabic, particularly in the context of Quranic recitation, where subtle phonetic differences can alter meaning. Addressing this challenge, the present study proposes a transformer-based multimodal framework for Arabic phoneme mispronunciation detection that combines acoustic and textual representations to achieve higher precision and robustness. The framework integrates UniSpeech-derived acoustic embeddings with BERT-based textual embeddings extracted from Whisper transcriptions, creating a unified representation that captures both phonetic detail and linguistic context. To determine the most effective integration strategy, early, intermediate, and late fusion methods were implemented and evaluated on two datasets containing 29 Arabic phonemes, including eight hafiz sounds, articulated by 11 native speakers. Additional speech samples collected from publicly available YouTube recordings were incorporated to enhance data diversity and generalization. Model performance was assessed using standard evaluation metrics: accuracy, precision, recall, and F1-score, allowing a detailed comparison of the fusion strategies. Experimental findings show that the UniSpeech-BERT multimodal configuration provides strong results and that fusion-based transformer architectures are effective for phoneme-level mispronunciation detection. The study contributes to the development of intelligent, speaker-independent, and multimodal Computer-Aided Language Learning (CALL) systems, offering a practical step toward technology-supported Quranic pronunciation training and broader speech-based educational applications.

</details>
