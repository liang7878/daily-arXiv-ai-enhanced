<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 17]
- [cs.CE](#cs.CE) [Total: 4]
- [cs.DB](#cs.DB) [Total: 2]
- [cs.DC](#cs.DC) [Total: 6]
- [cs.DS](#cs.DS) [Total: 5]
- [cs.GT](#cs.GT) [Total: 3]
- [cs.IR](#cs.IR) [Total: 20]
- [cs.LG](#cs.LG) [Total: 53]
- [cs.NE](#cs.NE) [Total: 2]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.SE](#cs.SE) [Total: 8]
- [stat.ML](#stat.ML) [Total: 9]
- [stat.CO](#stat.CO) [Total: 1]
- [cs.SD](#cs.SD) [Total: 1]
- [quant-ph](#quant-ph) [Total: 3]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [cs.DL](#cs.DL) [Total: 1]
- [cs.HC](#cs.HC) [Total: 2]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.CV](#cs.CV) [Total: 23]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [eess.IV](#eess.IV) [Total: 2]
- [cs.NI](#cs.NI) [Total: 2]
- [cs.CL](#cs.CL) [Total: 62]
- [math.ST](#math.ST) [Total: 1]
- [cs.CG](#cs.CG) [Total: 1]
- [physics.ins-det](#physics.ins-det) [Total: 1]
- [cs.RO](#cs.RO) [Total: 6]
- [eess.SP](#eess.SP) [Total: 3]
- [q-fin.GN](#q-fin.GN) [Total: 1]
- [stat.AP](#stat.AP) [Total: 1]
- [cs.CR](#cs.CR) [Total: 3]
- [physics.optics](#physics.optics) [Total: 1]
- [stat.OT](#stat.OT) [Total: 1]
- [math.NA](#math.NA) [Total: 1]
- [cs.SI](#cs.SI) [Total: 1]
- [stat.ME](#stat.ME) [Total: 2]
- [cs.CY](#cs.CY) [Total: 1]
- [econ.GN](#econ.GN) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [SEEA-R1: Tree-Structured Reinforcement Fine-Tuning for Self-Evolving Embodied Agents](https://arxiv.org/abs/2506.21669)
*Wanxin Tian,Shijie Zhang,Kevin Zhang,Xiaowei Chi,Yulin Luo,Junyu Lu,Chunkai Fan,Qiang Zhou,Yiming Zhao,Ning Liu Siyu Lin,Zhiyuan Qin,Xiaozhu Ju,Shanghang Zhang,Jian Tang*

Main category: cs.AI

TL;DR: 提出SEEA - R1框架使具身智能体具备自我进化能力，在ALFWorld基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前强化微调在具身领域实现自我进化具身智能的潜力未充分挖掘，存在中间奖励缺乏和依赖手工奖励函数的问题。

Method: 提出Tree - GRPO将蒙特卡罗树搜索集成到GRPO，转换稀疏延迟奖励；引入MGRM实现跨任务和场景的奖励估计。

Result: 在ALFWorld基准测试中超过现有方法，无环境奖励时也有高得分。

Conclusion: SEEA - R1有潜力推动可扩展具身智能的未来研究。

Abstract: Self-evolution, the ability of agents to autonomously improve their reasoning
and behavior, is essential for the embodied domain with long-horizon,
real-world tasks. Despite current advancements in reinforcement fine-tuning
(RFT) showing strong performance in enhancing reasoning in LLMs, its potential
to enable self-evolving embodied intelligence with multi-modal interactions
remains largely unexplored. Specifically, reinforcement fine-tuning faces two
fundamental obstacles in embodied settings: (i) the lack of accessible
intermediate rewards in multi-step reasoning tasks limits effective learning
signals, and (ii) reliance on hand-crafted reward functions restricts
generalization to novel tasks and environments. To address these challenges, we
present Self-Evolving Embodied Agents-R1, SEEA-R1, the first RFT framework
designed for enabling the self-evolving capabilities of embodied agents.
Specifically, to convert sparse delayed rewards into denser intermediate
signals that improve multi-step reasoning, we propose Tree-based group relative
policy optimization (Tree-GRPO), which integrates Monte Carlo Tree Search into
GRPO. To generalize reward estimation across tasks and scenes, supporting
autonomous adaptation and reward-driven self-evolution, we further introduce
Multi-modal Generative Reward Model (MGRM). To holistically evaluate the
effectiveness of SEEA-R1, we evaluate on the ALFWorld benchmark, surpassing
state-of-the-art methods with scores of 85.07% (textual) and 36.19%
(multi-modal), outperforming prior models including GPT-4o. SEEA-R1 also
achieves scores of 80.3% without environmental reward, surpassing all
open-source baselines and highlighting its scalability as a self-evolving
embodied agent. Additional experiments and qualitative analysis further support
the potential of SEEA-R1 for future research in scalable embodied intelligence.

</details>


### [2] [Hierarchical Reasoning Model](https://arxiv.org/abs/2506.21734)
*Guan Wang,Jin Li,Yuhao Sun,Xing Chen,Changling Liu,Yue Wu,Meng Lu,Sen Song,Yasin Abbasi Yadkori*

Main category: cs.AI

TL;DR: 现有大语言模型链式思维技术有缺陷，提出分层推理模型HRM，参数少、无需预训练和CoT数据，在复杂推理任务表现出色，有变革潜力。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型的链式思维技术存在任务分解脆弱、数据需求大、延迟高等问题，需要更好的推理模型。

Method: 受人类大脑分层和多时间尺度处理启发，提出HRM，通过高低层两个相互依赖的循环模块在单次前向传播中执行顺序推理任务。

Result: 仅2700万参数，用1000个训练样本就在复杂推理任务表现优异，在ARC上超越大模型。

Conclusion: HRM有潜力成为通用计算和通用推理系统的变革性进步。

Abstract: Reasoning, the process of devising and executing complex goal-oriented action
sequences, remains a critical challenge in AI. Current large language models
(LLMs) primarily employ Chain-of-Thought (CoT) techniques, which suffer from
brittle task decomposition, extensive data requirements, and high latency.
Inspired by the hierarchical and multi-timescale processing in the human brain,
we propose the Hierarchical Reasoning Model (HRM), a novel recurrent
architecture that attains significant computational depth while maintaining
both training stability and efficiency. HRM executes sequential reasoning tasks
in a single forward pass without explicit supervision of the intermediate
process, through two interdependent recurrent modules: a high-level module
responsible for slow, abstract planning, and a low-level module handling rapid,
detailed computations. With only 27 million parameters, HRM achieves
exceptional performance on complex reasoning tasks using only 1000 training
samples. The model operates without pre-training or CoT data, yet achieves
nearly perfect performance on challenging tasks including complex Sudoku
puzzles and optimal path finding in large mazes. Furthermore, HRM outperforms
much larger models with significantly longer context windows on the Abstraction
and Reasoning Corpus (ARC), a key benchmark for measuring artificial general
intelligence capabilities. These results underscore HRM's potential as a
transformative advancement toward universal computation and general-purpose
reasoning systems.

</details>


### [3] [THE-Tree: Can Tracing Historical Evolution Enhance Scientific Verification and Reasoning?](https://arxiv.org/abs/2506.21763)
*Xin Wang,Jiyao Liu,Yulong Xiao,Junzhi Ning,Lihao Liu,Junjun He,Botian Shi,Kaicheng Yu*

Main category: cs.AI

TL;DR: 现有验证AI生成科学命题的方法不足，本文提出THE - Tree框架，构建特定领域进化树，经实验验证其在多项任务中有性能提升。


<details>
  <summary>Details</summary>
Motivation: 严格评估AI生成的科学命题的新颖性和事实准确性存在瓶颈，现有验证方法不足，缺乏结构化、可验证且有因果关联的科学演化历史数据。

Method: 引入THE - Tree计算框架，利用搜索算法探索进化路径，在节点扩展时采用“Think - Verbalize - Cite - Verify”过程，并通过自然语言推理机制验证。

Result: 构建并验证88个跨领域的THE - Trees，发布含7.1万次事实验证、涵盖2.7万篇论文的基准数据集；在图补全、预测未来科学发展和评估重要科学论文方面性能有显著提升。

Conclusion: THE - Tree框架能有效解决科学命题验证难题，在相关任务中表现良好，有助于推动科学研究。

Abstract: Large Language Models (LLMs) are accelerating scientific idea generation, but
rigorously evaluating these numerous, often superficial, AI-generated
propositions for novelty and factual accuracy is a critical bottleneck; manual
verification is too slow.Existing validation methods are inadequate: LLMs as
standalone verifiers may hallucinate and lack domain knowledge (our findings
show ~60\% unawareness of relevant papers in specific domains), while
traditional citation networks lack explicit causality and narrative surveys are
unstructured.This underscores a core challenge: the absence of structured,
verifiable, and causally-linked historical data of scientific evolution.To
address this,we introduce \textbf{THE-Tree} (\textbf{T}echnology
\textbf{H}istory \textbf{E}volution Tree), a computational framework that
constructs such domain-specific evolution trees from scientific
literature.THE-Tree employs a search algorithm to explore evolutionary paths.
During its node expansion, it utilizes a novel "Think-Verbalize-Cite-Verify"
process: an LLM proposes potential advancements and cites supporting
literature. Critically, each proposed evolutionary link is then validated for
logical coherence and evidential support by a recovered natural language
inference mechanism that interrogates the cited literature, ensuring that each
step is grounded.We construct and validate 88 THE-Trees across diverse domains
and release a benchmark dataset including up to 71k fact verifications covering
27k papers to foster further research.Experiments demonstrate that i) in graph
completion, our THE-Tree improves hit@1 by 8\% to 14\% across multiple models
compared to traditional citation networks; ii) for predicting future scientific
developments, it improves hit@1 metric by nearly 10\%; and iii) when combined
with other methods, it boosts the performance of evaluating important
scientific papers by almost 100\%.

</details>


### [4] [MobiVerse: Scaling Urban Mobility Simulation with Hybrid Lightweight Domain-Specific Generator and Large Language Models](https://arxiv.org/abs/2506.21784)
*Yifan Liu,Xishun Liao,Haoxuan Ma,Jonathan Liu,Rohan Jadhav,Jiaqi Ma*

Main category: cs.AI

TL;DR: 提出MobiVerse混合框架解决移动性模拟平台不足问题，通过洛杉矶案例证明其有效且高效。


<details>
  <summary>Details</summary>
Motivation: 现有移动性模拟平台存在不足，传统模型、机器学习方法和大语言模型实现均有局限，需要更好的平台用于算法开发、政策实施和综合评估。

Method: 提出MobiVerse混合框架，结合轻量级特定领域生成器和大语言模型的优势。

Result: 在洛杉矶Westwood案例中，能在标准PC上为约53000个代理高效生成和动态调整日程，可让代理响应环境反馈，模块化设计便于测试算法，保持计算效率并增强行为真实性。

Conclusion: MobiVerse填补了移动性模拟的空白，提供了可定制的移动性系统规划和运营平台及基准算法。

Abstract: Understanding and modeling human mobility patterns is crucial for effective
transportation planning and urban development. Despite significant advances in
mobility research, there remains a critical gap in simulation platforms that
allow for algorithm development, policy implementation, and comprehensive
evaluation at scale. Traditional activity-based models require extensive data
collection and manual calibration, machine learning approaches struggle with
adaptation to dynamic conditions, and treding agent-based Large Language Models
(LLMs) implementations face computational constraints with large-scale
simulations. To address these challenges, we propose MobiVerse, a hybrid
framework leverages the efficiency of lightweight domain-specific generator for
generating base activity chains with the adaptability of LLMs for context-aware
modifications. A case study was conducted in Westwood, Los Angeles, where we
efficiently generated and dynamically adjusted schedules for the whole
population of approximately 53,000 agents on a standard PC. Our experiments
demonstrate that MobiVerse successfully enables agents to respond to
environmental feedback, including road closures, large gathering events like
football games, and congestion, through our hybrid framework. Its modular
design facilitates testing various mobility algorithms at both transportation
system and agent levels. Results show our approach maintains computational
efficiency while enhancing behavioral realism. MobiVerse bridges the gap in
mobility simulation by providing a customizable platform for mobility systems
planning and operations with benchmark algorithms. Code and videos are
available at https://github.com/ucla-mobility/MobiVerse.

</details>


### [5] [CitySim: Modeling Urban Behaviors and City Dynamics with Large-Scale LLM-Driven Agent Simulation](https://arxiv.org/abs/2506.21805)
*Nicolas Bougie,Narimasa Watanabe*

Main category: cs.AI

TL;DR: 本文提出城市模拟器CitySim，用递归价值驱动方法让智能体生成日程，模拟人类行为，比先前工作更接近真实人类，是理解和预测城市现象的可扩展、灵活测试平台。


<details>
  <summary>Details</summary>
Motivation: 先前模拟人类城市行为的工作依赖僵化的手工规则，难以模拟细微意图、计划和适应性行为。

Method: 构建城市模拟器CitySim，智能体用递归价值驱动方法生成日程，赋予智能体信念、长期目标和空间记忆以进行长期逼真模拟。

Result: CitySim在微观和宏观层面都比先前工作更接近真实人类，通过对大量智能体建模实验评估集体行为。

Conclusion: CitySim是理解和预测城市现象的可扩展、灵活测试平台。

Abstract: Modeling human behavior in urban environments is fundamental for social
science, behavioral studies, and urban planning. Prior work often rely on
rigid, hand-crafted rules, limiting their ability to simulate nuanced
intentions, plans, and adaptive behaviors. Addressing these challenges, we
envision an urban simulator (CitySim), capitalizing on breakthroughs in
human-level intelligence exhibited by large language models. In CitySim, agents
generate realistic daily schedules using a recursive value-driven approach that
balances mandatory activities, personal habits, and situational factors. To
enable long-term, lifelike simulations, we endow agents with beliefs, long-term
goals, and spatial memory for navigation. CitySim exhibits closer alignment
with real humans than prior work, both at micro and macro levels. Additionally,
we conduct insightful experiments by modeling tens of thousands of agents and
evaluating their collective behaviors under various real-world scenarios,
including estimating crowd density, predicting place popularity, and assessing
well-being. Our results highlight CitySim as a scalable, flexible testbed for
understanding and forecasting urban phenomena.

</details>


### [6] [Interactive Multi-Objective Probabilistic Preference Learning with Soft and Hard Bounds](https://arxiv.org/abs/2506.21887)
*Edward Chen,Sang T. Truong,Natalie Dullerud,Sanmi Koyejo,Carlos Guestrin*

Main category: cs.AI

TL;DR: 提出Active - MoSH交互式框架用于高风险决策，通过本地和全局组件优化决策，经多种应用和用户研究验证其性能。


<details>
  <summary>Details</summary>
Motivation: 高风险决策中选择帕累托最优解困难，现有方法缺乏迭代细化多方面偏好结构的系统方法，且决策者需信任最终决策。

Method: 提出Active - MoSH框架，本地组件结合软硬边界与概率偏好学习，采用主动采样策略；全局组件T - MoSH利用多目标敏感性分析。

Result: 通过多样合成和现实应用展示了性能优势，用户研究验证了能改善收敛性、增强决策者信任和提供表达性偏好表述。

Conclusion: Active - MoSH框架能使决策者更有效进行高风险决策。

Abstract: High-stakes decision-making involves navigating multiple competing objectives
with expensive evaluations. For instance, in brachytherapy, clinicians must
balance maximizing tumor coverage (e.g., an aspirational target or soft bound
of >95% coverage) against strict organ dose limits (e.g., a non-negotiable hard
bound of <601 cGy to the bladder), with each plan evaluation being
resource-intensive. Selecting Pareto-optimal solutions that match implicit
preferences is challenging, as exhaustive Pareto frontier exploration is
computationally and cognitively prohibitive, necessitating interactive
frameworks to guide users. While decision-makers (DMs) often possess domain
knowledge to narrow the search via such soft-hard bounds, current methods often
lack systematic approaches to iteratively refine these multi-faceted preference
structures. Critically, DMs must trust their final decision, confident they
haven't missed superior alternatives; this trust is paramount in
high-consequence scenarios. We present Active-MoSH, an interactive local-global
framework designed for this process. Its local component integrates soft-hard
bounds with probabilistic preference learning, maintaining distributions over
DM preferences and bounds for adaptive Pareto subset refinement. This is guided
by an active sampling strategy optimizing exploration-exploitation while
minimizing cognitive burden. To build DM trust, Active-MoSH's global component,
T-MoSH, leverages multi-objective sensitivity analysis to identify potentially
overlooked, high-value points beyond immediate feedback. We demonstrate
Active-MoSH's performance benefits through diverse synthetic and real-world
applications. A user study on AI-generated image selection further validates
our hypotheses regarding the framework's ability to improve convergence,
enhance DM trust, and provide expressive preference articulation, enabling more
effective DMs.

</details>


### [7] [AlphaBeta is not as good as you think: a new probabilistic model to better analyze deterministic game-solving algorithms](https://arxiv.org/abs/2506.21996)
*Raphaël Boige,Amine Boumaza,Bruno Scherrer*

Main category: cs.AI

TL;DR: 本文指出传统确定性博弈求解算法分析模型的局限，引入新概率模型，推导算法平均复杂度公式，发现不同算法在深度有限树中的差异。


<details>
  <summary>Details</summary>
Motivation: 传统分析模型的独立性假设使博弈失去结构复杂性，无法给算法带来挑战，需新模型。

Method: 引入用固定层条件分布构建博弈树的新概率模型，推导AlphaBeta和Scout等算法的平均复杂度递归公式。

Result: 渐近情况下算法分支因子相同，但深度有限树中AlphaBeta常数乘因子大，有明显实际减速。

Conclusion: 新框架为经典博弈求解算法提供新见解，有严格证据和分析工具。

Abstract: Deterministic game-solving algorithms are conventionally analyzed in the
light of their average-case complexity against a distribution of random
game-trees, where leaf values are independently sampled from a fixed
distribution. This simplified model enables uncluttered mathematical analysis,
revealing two key properties: root value distributions asymptotically collapse
to a single fixed value for finite-valued trees, and all reasonable algorithms
achieve global optimality. However, these findings are artifacts of the model's
design-its long criticized independence assumption strips games of structural
complexity, producing trivial instances where no algorithm faces meaningful
challenges. To address this limitation, we introduce a new probabilistic model
that incrementally constructs game-trees using a fixed level-wise conditional
distribution. By enforcing ancestor dependency, a critical structural feature
of real-world games, our framework generates problems with adjustable
difficulty while retaining some form of analytical tractability. For several
algorithms, including AlphaBeta and Scout, we derive recursive formulas
characterizing their average-case complexities under this model. These allow us
to rigorously compare algorithms on deep game-trees, where Monte-Carlo
simulations are no longer feasible. While asymptotically, all algorithms seem
to converge to identical branching factor (a result analogous to those of
independence-based models), deep finite trees reveal stark differences:
AlphaBeta incurs a significantly larger constant multiplicative factor compared
to algorithms like Scout, leading to a substantial practical slowdown. Our
framework sheds new light on classical game-solving algorithms, offering
rigorous evidence and analytical tools to advance the understanding of these
methods under a more realistic, challenging, and yet tractable model.

</details>


### [8] [LeanConjecturer: Automatic Generation of Mathematical Conjectures for Theorem Proving](https://arxiv.org/abs/2506.22005)
*Naoto Onda,Kazumi Kasaura,Yuta Oriike,Masaya Taniguchi,Akiyoshi Sannai,Sho Sonoda*

Main category: cs.AI

TL;DR: 介绍LeanConjecturer，用大语言模型自动生成大学数学猜想，产出大量猜想并验证其对定理证明的作用，有数学发现潜力。


<details>
  <summary>Details</summary>
Motivation: 解决形式定理证明中数据稀缺的问题。

Method: 结合基于规则的上下文提取和基于大语言模型的定理陈述生成，通过迭代生成和评估。

Result: 从40个Mathlib种子文件生成12289个猜想，3776个语法有效且非平凡；平均每个种子文件生成103.25个新猜想；成功验证拓扑学中几个非平凡定理。

Conclusion: 该方法为定理证明系统创建训练数据提供可扩展解决方案，有超越现有结果简单变体进行数学发现的潜力。

Abstract: We introduce LeanConjecturer, a pipeline for automatically generating
university-level mathematical conjectures in Lean 4 using Large Language Models
(LLMs). Our hybrid approach combines rule-based context extraction with
LLM-based theorem statement generation, addressing the data scarcity challenge
in formal theorem proving. Through iterative generation and evaluation,
LeanConjecturer produced 12,289 conjectures from 40 Mathlib seed files, with
3,776 identified as syntactically valid and non-trivial, that is, cannot be
proven by \texttt{aesop} tactic. We demonstrate the utility of these generated
conjectures for reinforcement learning through Group Relative Policy
Optimization (GRPO), showing that targeted training on domain-specific
conjectures can enhance theorem proving capabilities. Our approach generates
103.25 novel conjectures per seed file on average, providing a scalable
solution for creating training data for theorem proving systems. Our system
successfully verified several non-trivial theorems in topology, including
properties of semi-open, alpha-open, and pre-open sets, demonstrating its
potential for mathematical discovery beyond simple variations of existing
results.

</details>


### [9] [Universal Retrieval for Multimodal Trajectory Modeling](https://arxiv.org/abs/2506.22056)
*Xuan Zhang,Ziyan Jiang,Rui Meng,Yifei Leng,Zhenbang Xiao,Zora Zhiruo Wang,Yanyi Shang,Dehan Kong*

Main category: cs.AI

TL;DR: 本文引入多模态轨迹检索，构建UATD数据集和GAE - Bench基准，提出GAE - Retriever框架，评估显示其在检索召回率上优于基线。


<details>
  <summary>Details</summary>
Motivation: 轨迹数据虽有潜力提升AI智能体能力，但轨迹级数据表示建模是未被系统解决的挑战。

Method: 引入多模态轨迹检索，构建UATD数据集和GAE - Bench基准，提出采用视觉 - 语言模型并结合优化对比学习的GAE - Retriever框架。

Result: 综合评估显示GAE - Retriever在多个数据集的检索召回率上始终优于强基线。

Conclusion: GAE - Retriever在推进多模态轨迹检索方面有效。

Abstract: Trajectory data, capturing human actions and environmental states across
various modalities, holds significant potential for enhancing AI agent
capabilities, particularly in GUI environments. However, how to model the
representation of trajectory-level data presents a significant challenge that
has not been systematically addressed amid explosive trajectory data growth. In
this work, we introduce Multimodal Trajectory Retrieval, bridging the gap
between universal retrieval and agent-centric trajectory modeling. We construct
the Unified Agent Trajectory Dataset (UATD) from annotated demonstrations and
states across diverse real-world scenarios. Based on this, we present
GAE-Bench, a benchmark containing a large number of trajectory-based retrieval
pairs. In addition, we propose GAE-Retriever, a multimodal retrieval framework
that adopts vision-language models and incorporates optimized contrastive
learning through a token selection and the GradCache mechanism. Comprehensive
evaluations across multiple datasets show that GAE-Retriever consistently
outperforms strong baselines in retrieval recall, highlighting its
effectiveness in advancing multimodal trajectory retrieval.

</details>


### [10] [Query as Test: An Intelligent Driving Test and Data Storage Method for Integrated Cockpit-Vehicle-Road Scenarios](https://arxiv.org/abs/2506.22068)
*Shengyue Yao,Runqing Guo,Yangyang Qin,Miangbing Meng,Jipeng Cao,Yilun Lin,Yisheng Lv,Fei-Yue Wang*

Main category: cs.AI

TL;DR: 本文针对交通领域数据生态碎片化、现有测试方法不足问题，提出QaT概念、ESN框架，阐述QaT范式并引入VDD概念加速迭代。


<details>
  <summary>Details</summary>
Motivation: 解决交通领域智能座舱、自动驾驶和智能路网数据生态碎片化、不兼容，以及现有测试方法依赖数据堆叠、覆盖不全和缺乏灵活性的问题。

Method: 提出“Query as Test”（QaT）概念，以灵活的逻辑查询替代固定测试用例；提出“Extensible Scenarios Notations”（ESN）框架，基于ASP统一表示异构多模态数据；将自动驾驶系统的功能验证和安全合规检查转化为对ESN数据库的逻辑查询；引入“Validation-Driven Development”（VDD）概念。

Result: ESN框架实现数据深度语义融合，带来复杂灵活语义查询、自然可解释性和按需数据抽象等优势；QaT范式增强测试的表达力和形式严谨性。

Conclusion: 在大语言模型时代，用逻辑验证而非定量测试指导开发的VDD概念可加速交通领域相关系统的迭代和开发。

Abstract: With the deep penetration of Artificial Intelligence (AI) in the
transportation sector, intelligent cockpits, autonomous driving, and
intelligent road networks are developing at an unprecedented pace. However, the
data ecosystems of these three key areas are increasingly fragmented and
incompatible. Especially, existing testing methods rely on data stacking, fail
to cover all edge cases, and lack flexibility. To address this issue, this
paper introduces the concept of "Query as Test" (QaT). This concept shifts the
focus from rigid, prescripted test cases to flexible, on-demand logical queries
against a unified data representation. Specifically, we identify the need for a
fundamental improvement in data storage and representation, leading to our
proposal of "Extensible Scenarios Notations" (ESN). ESN is a novel declarative
data framework based on Answer Set Programming (ASP), which uniformly
represents heterogeneous multimodal data from the cockpit, vehicle, and road as
a collection of logical facts and rules. This approach not only achieves deep
semantic fusion of data, but also brings three core advantages: (1) supports
complex and flexible semantic querying through logical reasoning; (2) provides
natural interpretability for decision-making processes; (3) allows for
on-demand data abstraction through logical rules, enabling fine-grained privacy
protection. We further elaborate on the QaT paradigm, transforming the
functional validation and safety compliance checks of autonomous driving
systems into logical queries against the ESN database, significantly enhancing
the expressiveness and formal rigor of the testing. Finally, we introduce the
concept of "Validation-Driven Development" (VDD), which suggests to guide
developments by logical validation rather than quantitative testing in the era
of Large Language Models, in order to accelerating the iteration and
development process.

</details>


### [11] [A Different Approach to AI Safety: Proceedings from the Columbia Convening on Openness in Artificial Intelligence and AI Safety](https://arxiv.org/abs/2506.22183)
*Camille François,Ludovic Péran,Ayah Bdeir,Nouha Dziri,Will Hawkins,Yacine Jernite,Sayash Kapoor,Juliet Shen,Heidy Khlaaf,Kevin Klyman,Nik Marda,Marie Pellat,Deb Raji,Divya Siddarth,Aviya Skowron,Joseph Spisak,Madhulika Srikumar,Victor Storchan,Audrey Tang,Jen Weedon*

Main category: cs.AI

TL;DR: 本文介绍哥伦比亚AI开放性与安全会议成果，探讨开源AI安全，指出开放性可增强安全但存在差距，给出五个优先研究方向建议。


<details>
  <summary>Details</summary>
Motivation: 开源大模型兴起强化保障AI系统安全的责任并带来机遇，需探讨相关安全问题。

Method: 采用参与式、面向解决方案的过程，组织工作小组开展研究。

Result: 产生安全与开源AI交叉领域研究议程、技术干预和开源工具映射、内容安全过滤生态系统映射；发现开放性可增强安全，但存在多模态基准稀缺等差距。

Conclusion: 给出五个优先研究方向的路线图，为开放、多元、负责的AI安全学科奠定基础。

Abstract: The rapid rise of open-weight and open-source foundation models is
intensifying the obligation and reshaping the opportunity to make AI systems
safe. This paper reports outcomes from the Columbia Convening on AI Openness
and Safety (San Francisco, 19 Nov 2024) and its six-week preparatory programme
involving more than forty-five researchers, engineers, and policy leaders from
academia, industry, civil society, and government. Using a participatory,
solutions-oriented process, the working groups produced (i) a research agenda
at the intersection of safety and open source AI; (ii) a mapping of existing
and needed technical interventions and open source tools to safely and
responsibly deploy open foundation models across the AI development workflow;
and (iii) a mapping of the content safety filter ecosystem with a proposed
roadmap for future research and development. We find that openness --
understood as transparent weights, interoperable tooling, and public governance
-- can enhance safety by enabling independent scrutiny, decentralized
mitigation, and culturally plural oversight. However, significant gaps persist:
scarce multimodal and multilingual benchmarks, limited defenses against
prompt-injection and compositional attacks in agentic systems, and insufficient
participatory mechanisms for communities most affected by AI harms. The paper
concludes with a roadmap of five priority research directions, emphasizing
participatory inputs, future-proof content filters, ecosystem-wide safety
infrastructure, rigorous agentic safeguards, and expanded harm taxonomies.
These recommendations informed the February 2025 French AI Action Summit and
lay groundwork for an open, plural, and accountable AI safety discipline.

</details>


### [12] [Breaking Rank Bottlenecks in Knowledge Graph Completion](https://arxiv.org/abs/2506.22271)
*Samy Badreddine,Emile van Krieken,Luciano Serafini*

Main category: cs.AI

TL;DR: 论文指出知识图谱补全（KGC）模型的简单向量 - 矩阵乘法存在秩瓶颈，影响模型表现，提出 KGE - MoS 输出层打破瓶颈，实验证明其有效。


<details>
  <summary>Details</summary>
Motivation: 许多 KGC 模型使用简单向量 - 矩阵乘法评分，当实体数量远超嵌入维度时存在秩瓶颈，限制模型表达能力。

Method: 理论和实证研究秩瓶颈对 KGC 模型的影响，受语言建模启发提出基于混合的输出层 KGE - MoS。

Result: 在四个数据集上实验表明，KGE - MoS 以低参数成本提升了 KGC 模型的性能和概率拟合度。

Conclusion: KGE - MoS 能有效打破 KGC 模型的秩瓶颈，提升模型表现。

Abstract: Many Knowledge Graph Completion (KGC) models, despite using powerful
encoders, rely on a simple vector-matrix multiplication to score queries
against candidate object entities. When the number of entities is larger than
the model's embedding dimension, which in practical scenarios is often by
several orders of magnitude, we have a linear output layer with a rank
bottleneck. Such bottlenecked layers limit model expressivity. We investigate
both theoretically and empirically how rank bottlenecks affect KGC models. We
find that, by limiting the set of feasible predictions, rank bottlenecks hurt
ranking accuracy and the distribution fidelity of scores. Inspired by the
language modelling literature, we propose KGE-MoS, a mixture-based output layer
to break rank bottlenecks in many KGC models. Our experiments on four datasets
show that KGE-MoS improves performance and probabilistic fit of KGC models for
a low parameter cost.

</details>


### [13] [Artificial Intelligent Disobedience: Rethinking the Agency of Our Artificial Teammates](https://arxiv.org/abs/2506.22276)
*Reuth Mirsky*

Main category: cs.AI

TL;DR: 本文提出在人机协作中赋予AI智能违抗能力，介绍AI代理级别，探讨不同级别下表现并提出研究边界和考虑因素。


<details>
  <summary>Details</summary>
Motivation: 现有协作式AI系统大多盲目服从，即便可能适得其反或不安全，因此要扩展AI队友的智能违抗能力。

Method: 引入AI代理级别量表，用代表性例子强调AI自主性研究的重要性和必要性。

Result: 探讨了智能违抗在不同自主性水平下的表现。

Conclusion: 提出将违抗作为人工智能核心能力进行研究的初步边界和考虑因素。

Abstract: Artificial intelligence has made remarkable strides in recent years,
achieving superhuman performance across a wide range of tasks. Yet despite
these advances, most cooperative AI systems remain rigidly obedient, designed
to follow human instructions without question and conform to user expectations,
even when doing so may be counterproductive or unsafe. This paper argues for
expanding the agency of AI teammates to include \textit{intelligent
disobedience}, empowering them to make meaningful and autonomous contributions
within human-AI teams. It introduces a scale of AI agency levels and uses
representative examples to highlight the importance and growing necessity of
treating AI autonomy as an independent research focus in cooperative settings.
The paper then explores how intelligent disobedience manifests across different
autonomy levels and concludes by proposing initial boundaries and
considerations for studying disobedience as a core capability of artificial
agents.

</details>


### [14] [Conceptual Topic Aggregation](https://arxiv.org/abs/2506.22309)
*Klara M. Gutekunst,Dominik Dürrschnabel,Johannes Hirth,Gerd Stumme*

Main category: cs.AI

TL;DR: 数据增长使传统人工检查不可行，本文提出基于FCA的FAT - CAT方法用于主题聚合和可视化，在案例中证明比现有技术更有效。


<details>
  <summary>Details</summary>
Motivation: 数据增长需计算方法进行数据探索，现有主题建模方法难提供可解释表示，难以深入洞察数据结构和内容。

Method: 提出基于形式概念分析（FCA）的FAT - CAT方法，处理不同主题和文件类型，构建概念格以结构化、层次化表示主题分布。

Result: 在ETYNTKE数据集案例研究中，与其他表示方法对比，基于FCA的聚合比现有主题建模技术能提供更有意义和可解释的数据集构成洞察。

Conclusion: 基于FCA的FAT - CAT方法在主题聚合和可视化方面比现有主题建模技术更有效，能提供更有意义和可解释的结果。

Abstract: The vast growth of data has rendered traditional manual inspection
infeasible, necessitating the adoption of computational methods for efficient
data exploration. Topic modeling has emerged as a powerful tool for analyzing
large-scale textual datasets, enabling the extraction of latent semantic
structures. However, existing methods for topic modeling often struggle to
provide interpretable representations that facilitate deeper insights into data
structure and content. In this paper, we propose FAT-CAT, an approach based on
Formal Concept Analysis (FCA) to enhance meaningful topic aggregation and
visualization of discovered topics. Our approach can handle diverse topics and
file types -- grouped by directories -- to construct a concept lattice that
offers a structured, hierarchical representation of their topic distribution.
In a case study on the ETYNTKE dataset, we evaluate the effectiveness of our
approach against other representation methods to demonstrate that FCA-based
aggregation provides more meaningful and interpretable insights into dataset
composition than existing topic modeling techniques.

</details>


### [15] [Embodied AI Agents: Modeling the World](https://arxiv.org/abs/2506.22355)
*Pascale Fung,Yoram Bachrach,Asli Celikyilmaz,Kamalika Chaudhuri,Delong Chen,Willy Chung,Emmanuel Dupoux,Hervé Jégou,Alessandro Lazaric,Arjun Majumdar,Andrea Madotto,Franziska Meier,Florian Metze,Théo Moutakanni,Juan Pino,Basile Terver,Joseph Tighe,Jitendra Malik*

Main category: cs.AI

TL;DR: 研究具身AI智能体，提出世界模型对其推理和规划很重要，还提议学习用户心理世界模型以促进人机协作。


<details>
  <summary>Details</summary>
Motivation: 使AI智能体能够与用户和环境交互，且让其学习和交互方式更接近人类。

Method: 提出世界模型，包括多模态感知、基于推理的行动规划和控制、记忆等的整合，还提出学习用户心理世界模型。

Result: 无明确提及具体研究成果。

Conclusion: 世界模型对具身AI智能体的推理和规划至关重要，学习用户心理世界模型可增强人机协作。

Abstract: This paper describes our research on AI agents embodied in visual, virtual or
physical forms, enabling them to interact with both users and their
environments. These agents, which include virtual avatars, wearable devices,
and robots, are designed to perceive, learn and act within their surroundings,
which makes them more similar to how humans learn and interact with the
environments as compared to disembodied agents. We propose that the development
of world models is central to reasoning and planning of embodied AI agents,
allowing these agents to understand and predict their environment, to
understand user intentions and social contexts, thereby enhancing their ability
to perform complex tasks autonomously. World modeling encompasses the
integration of multimodal perception, planning through reasoning for action and
control, and memory to create a comprehensive understanding of the physical
world. Beyond the physical world, we also propose to learn the mental world
model of users to enable better human-agent collaboration.

</details>


### [16] [AI Model Passport: Data and System Traceability Framework for Transparent AI in Health](https://arxiv.org/abs/2506.22358)
*Varvara Kalokyri,Nikolaos S. Tachos,Charalampos N. Kalantzopoulos,Stelios Sfakianakis,Haridimos Kondylakis,Dimitrios I. Zaridis,Sara Colantonio,Daniele Regge,Nikolaos Papanikolaou,The ProCAncer-I consortium,Konstantinos Marias,Dimitrios I. Fotiadis,Manolis Tsiknakis*

Main category: cs.AI

TL;DR: 本文提出AI模型护照概念，通过AIPassport工具实现该框架，在医疗影像应用中展示其有效性，旨在为AI驱动医疗解决方案建立信任和问责标准。


<details>
  <summary>Details</summary>
Motivation: 现有AI融入健康和生物医学系统的框架存在可扩展性、可比性和机器可解释性不足等问题，缺乏唯一可验证身份，限制了可重复性和利益相关者信任。

Method: 引入AI模型护照概念，作为AI模型的数字身份和验证工具，通过AIPassport工具实现框架，自动化元数据收集等。

Result: 在病变分割用例中展示了AIPassport工具增强了透明度、可重复性和监管就绪性，减少了人工工作。

Conclusion: 该方法旨在为AI驱动医疗解决方案设定新标准，为跨领域开发透明和合规AI系统奠定基础。

Abstract: The increasing integration of Artificial Intelligence (AI) into health and
biomedical systems necessitates robust frameworks for transparency,
accountability, and ethical compliance. Existing frameworks often rely on
human-readable, manual documentation which limits scalability, comparability,
and machine interpretability across projects and platforms. They also fail to
provide a unique, verifiable identity for AI models to ensure their provenance
and authenticity across systems and use cases, limiting reproducibility and
stakeholder trust. This paper introduces the concept of the AI Model Passport,
a structured and standardized documentation framework that acts as a digital
identity and verification tool for AI models. It captures essential metadata to
uniquely identify, verify, trace and monitor AI models across their lifecycle -
from data acquisition and preprocessing to model design, development and
deployment. In addition, an implementation of this framework is presented
through AIPassport, an MLOps tool developed within the ProCAncer-I EU project
for medical imaging applications. AIPassport automates metadata collection,
ensures proper versioning, decouples results from source scripts, and
integrates with various development environments. Its effectiveness is
showcased through a lesion segmentation use case using data from the
ProCAncer-I dataset, illustrating how the AI Model Passport enhances
transparency, reproducibility, and regulatory readiness while reducing manual
effort. This approach aims to set a new standard for fostering trust and
accountability in AI-driven healthcare solutions, aspiring to serve as the
basis for developing transparent and regulation compliant AI systems across
domains.

</details>


### [17] [The Automated LLM Speedrunning Benchmark: Reproducing NanoGPT Improvements](https://arxiv.org/abs/2506.22419)
*Bingchen Zhao,Despoina Magka,Minqi Jiang,Xian Li,Roberta Raileanu,Tatiana Shavrina,Jean-Christophe Gagnon-Audet,Kelvin Niu,Shagun Sodhani,Michael Shvartsman,Andrei Lupu,Alisia Lupidi,Edan Toledo,Karen Hambardzumyan,Martin Josifoski,Thomas Foster,Lucia Cipolina-Kun,Abhishek Charnalia,Derek Dunfield,Alexander H. Miller,Oisin Mac Aodha,Jakob Foerster,Yoram Bachrach*

Main category: cs.AI

TL;DR: 提出自动大语言模型速通基准来评估AI代理复现成果的能力，发现推理大语言模型结合最优支架难以复现已知创新。


<details>
  <summary>Details</summary>
Motivation: 评估AI代理在活跃研究领域复现成果的能力，助力大语言模型推动科学进步。

Method: 引入自动大语言模型速通基准，利用NanoGPT速通竞赛数据，设置19个速通任务并提供不同提示格式。

Result: 近期推理大语言模型结合最优支架在基准测试中难以复现已知创新。

Conclusion: 该基准为衡量大语言模型自动科学复现能力提供简单、不饱和的指标，是自主研究代理必要技能。

Abstract: Rapid advancements in large language models (LLMs) have the potential to
assist in scientific progress. A critical capability toward this endeavor is
the ability to reproduce existing work. To evaluate the ability of AI agents to
reproduce results in an active research area, we introduce the Automated LLM
Speedrunning Benchmark, leveraging the research community contributions on the
NanoGPT speedrun, a competition to train a GPT-2 model in the shortest time.
Each of the 19 speedrun tasks provides the agent with the previous records
training script, optionally paired with one of three hint formats, ranging from
pseudocode to paper-like descriptions of the new records improvements. Records
execute quickly by design and speedrun improvements encompass diverse
code-level changes, ranging from high-level algorithmic advancements to
hardware-aware optimizations. These features make the benchmark both accessible
and realistic for the frontier problem of improving LLM training. We find that
recent reasoning LLMs combined with SoTA scaffolds struggle to reimplement
already-known innovations in our benchmark, even when given detailed hints. Our
benchmark thus provides a simple, non-saturated measure of an LLMs ability to
automate scientific reproduction, a necessary (but not sufficient) skill for an
autonomous research agent.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [18] [Storm Surge in Color: RGB-Encoded Physics-Aware Deep Learning for Storm Surge Forecasting](https://arxiv.org/abs/2506.21743)
*Jinpai Zhao,Albert Cerrone,Eirik Valseth,Leendert Westerink,Clint Dawson*

Main category: cs.CE

TL;DR: 提出将非结构化水位场投影到RGB编码图像表示的新方法，用ConvLSTM网络进行风暴潮预测，在墨西哥湾数据集上表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习方法在风暴潮预测中存在空间分辨率有限、依赖海岸站数据、泛化性差以及与现代深度学习架构不兼容等问题。

Method: 将非结构化水位场投影到RGB编码图像表示，应用ConvLSTM网络进行端到端时空风暴潮预测，集成地面真实风场和地形 - 水深数据作为输入。

Result: 在墨西哥湾合成风暴大规模数据集上，实现德州海岸多区域48小时稳健预测，对其他沿海地区有强空间扩展性。

Conclusion: 结合结构化表示、基于物理的强迫因素和可扩展深度学习，提升了风暴潮预测的可用性、适应性和可解释性。

Abstract: Storm surge forecasting plays a crucial role in coastal disaster
preparedness, yet existing machine learning approaches often suffer from
limited spatial resolution, reliance on coastal station data, and poor
generalization. Moreover, many prior models operate directly on unstructured
spatial data, making them incompatible with modern deep learning architectures.
In this work, we introduce a novel approach that projects unstructured water
elevation fields onto structured Red Green Blue (RGB)-encoded image
representations, enabling the application of Convolutional Long Short Term
Memory (ConvLSTM) networks for end-to-end spatiotemporal surge forecasting. Our
model further integrates ground-truth wind fields as dynamic conditioning
signals and topo-bathymetry as a static input, capturing physically meaningful
drivers of surge evolution. Evaluated on a large-scale dataset of synthetic
storms in the Gulf of Mexico, our method demonstrates robust 48-hour
forecasting performance across multiple regions along the Texas coast and
exhibits strong spatial extensibility to other coastal areas. By combining
structured representation, physically grounded forcings, and scalable deep
learning, this study advances the frontier of storm surge forecasting in
usability, adaptability, and interpretability.

</details>


### [19] [Laser Scan Path Design for Controlled Microstructure in Additive Manufacturing with Integrated Reduced-Order Phase-Field Modeling and Deep Reinforcement Learning](https://arxiv.org/abs/2506.21815)
*Augustine Twumasi,Prokash Chandra Roy,Zixun Li,Soumya Shouvik Bhattacharjee,Zhengtao Gan*

Main category: cs.CE

TL;DR: 本文提出物理引导的机器学习方法优化激光粉末床熔融（L - PBF）扫描路径，用相场法建模，训练3D U - Net替代模型，结合深度强化学习（DRL）生成优化路径，结果表明该方法能提升微观结构控制和计算效率。


<details>
  <summary>Details</summary>
Motivation: L - PBF中复杂微观结构形成影响产品质量，需优化扫描路径以获得期望的微观结构。

Method: 采用相场法（PFM）建模，训练3D U - Net卷积神经网络作为替代模型预测晶粒取向；用深度强化学习（DRL）生成优化扫描路径，将替代模型集成到DRL环境加速训练，奖励函数最小化预测微观结构的纵横比和晶粒体积。

Result: 研究三种扫描策略，替代模型实现两个数量级的加速，三个案例证明DRL方法有效，强化学习算法对比传统之字形方法有优势。

Conclusion: 机器学习方法在L - PBF优化中具有提升微观结构控制和计算效率的潜力。

Abstract: Laser powder bed fusion (L-PBF) is a widely recognized additive manufacturing
technology for producing intricate metal components with exceptional accuracy.
A key challenge in L-PBF is the formation of complex microstructures affecting
product quality. We propose a physics-guided, machine-learning approach to
optimize scan paths for desired microstructure outcomes, such as equiaxed
grains. We utilized a phase-field method (PFM) to model crystalline grain
structure evolution. To reduce computational costs, we trained a surrogate
machine learning model, a 3D U-Net convolutional neural network, using
single-track phase-field simulations with various laser powers to predict
crystalline grain orientations based on initial microstructure and thermal
history. We investigated three scanning strategies across various hatch
spacings within a square domain, achieving a two-orders-of-magnitude speedup
using the surrogate model. To reduce trial and error in designing laser scan
toolpaths, we used deep reinforcement learning (DRL) to generate optimized scan
paths for target microstructure. Results from three cases demonstrate the DRL
approach's effectiveness. We integrated the surrogate 3D U-Net model into our
DRL environment to accelerate the reinforcement learning training process. The
reward function minimizes both aspect ratio and grain volume of the predicted
microstructure from the agent's scan path. The reinforcement learning algorithm
was benchmarked against conventional zigzag approach for smaller and larger
domains, showing machine learning methods' potential to enhance microstructure
control and computational efficiency in L-PBF optimization.

</details>


### [20] [Model-free Forecasting of Rogue Waves using Reservoir Computing](https://arxiv.org/abs/2506.21918)
*Abrari Noor Hasmi,Hadi Susanto*

Main category: cs.CE

TL;DR: 本文研究储层计算对非线性薛定谔方程中 rogue 波动力学的建模能力，经训练的储层能预测长时间 rogue 波传播，并提出提升预测能力的方法，推进了储层计算在时空哈密顿系统中的应用。


<details>
  <summary>Details</summary>
Motivation: 储层计算在哈密顿系统应用研究较少，本文研究其对非线性薛定谔方程中 rogue 波动力学的建模有效性。

Method: 采用无模型方法，从含五个不稳定模式的 breather 模拟中学习，用适当调参的并行回声状态网络预测。

Result: 网络能对两个测试数据集进行预测，单步预测结果与测试数据吻合；训练后的储层能在长预测期预测 rogue 波传播；提出提升自主模式下预测能力的方法。

Conclusion: 推进了储层计算在时空哈密顿系统中的应用，强调训练数据相空间覆盖的重要性。

Abstract: Recent research has demonstrated Reservoir Computing's capability to model
various chaotic dynamical systems, yet its application to Hamiltonian systems
remains relatively unexplored. This paper investigates the effectiveness of
Reservoir Computing in capturing rogue wave dynamics from the nonlinear
Schr\"{o}dinger equation, a challenging Hamiltonian system with modulation
instability. The model-free approach learns from breather simulations with five
unstable modes. A properly tuned parallel Echo State Network can predict
dynamics from two distinct testing datasets. The first set is a continuation of
the training data, whereas the second set involves a higher-order breather. An
investigation of the one-step prediction capability shows remarkable agreement
between the testing data and the models. Furthermore, we show that the trained
reservoir can predict the propagation of rogue waves over a relatively long
prediction horizon, despite facing unseen dynamics. Finally, we introduce a
method to significantly improve the Reservoir Computing prediction in
autonomous mode, enhancing its long-term forecasting ability. These results
advance the application of Reservoir Computing to spatio-temporal Hamiltonian
systems and highlight the critical importance of phase space coverage in the
design of training data.

</details>


### [21] [A Deep Learning Algorithm Based on CNN-LSTM Framework for Predicting Cancer Drug Sales Volume](https://arxiv.org/abs/2506.21927)
*Yinghan Li,Yilin Yao,Junghua Lin,Nanxi Wang*

Main category: cs.CE

TL;DR: 研究CNN - LSTM框架深度学习模型预测癌症药物销量的潜力，用埃及特定癌症药物销售数据验证模型有效性，为医药决策提供支持。


<details>
  <summary>Details</summary>
Motivation: 随着医疗进步，癌症药物需求增加，准确预测销量对生产、供应链管理和医疗政策制定至关重要。

Method: 采用结合CNN和LSTM的混合深度学习模型，用MSE和RMSE评估模型性能。

Result: CNN - LSTM模型在测试集上表现良好，MSE为1.150，RMSE为1.072，能处理非线性和波动的销售数据。

Conclusion: 本研究为医药营销和医疗资源规划的数据驱动决策提供理论和技术支持。

Abstract: This study explores the application potential of a deep learning model based
on the CNN-LSTM framework in forecasting the sales volume of cancer drugs, with
a focus on modeling complex time series data. As advancements in medical
technology and cancer treatment continue, the demand for oncology medications
is steadily increasing. Accurate forecasting of cancer drug sales plays a
critical role in optimizing production planning, supply chain management, and
healthcare policy formulation. The dataset used in this research comprises
quarterly sales records of a specific cancer drug in Egypt from 2015 to 2024,
including multidimensional information such as date, drug type, pharmaceutical
company, price, sales volume, effectiveness, and drug classification. To
improve prediction accuracy, a hybrid deep learning model combining
Convolutional Neural Networks (CNN) and Long Short-Term Memory (LSTM) networks
is employed. The CNN component is responsible for extracting local temporal
features from the sales data, while the LSTM component captures long-term
dependencies and trends. Model performance is evaluated using two widely
adopted metrics: Mean Squared Error (MSE) and Root Mean Squared Error (RMSE).
The results demonstrate that the CNN-LSTM model performs well on the test set,
achieving an MSE of 1.150 and an RMSE of 1.072, indicating its effectiveness in
handling nonlinear and volatile sales data. This research provides theoretical
and technical support for data-driven decision-making in pharmaceutical
marketing and healthcare resource planning.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [22] [Revisiting Graph Analytics Benchmark](https://arxiv.org/abs/2506.21811)
*Lingkai Meng,Yu Shao,Long Yuan,Longbin Lai,Peng Cheng,Xue Li,Wenyuan Yu,Wenjie Zhang,Xuemin Lin,Jingren Zhou*

Main category: cs.DB

TL;DR: 提出新图分析基准，选8种核心算法、设计数据生成器并产生新数据集，引入基于大语言模型的API可用性评估框架，实验证明其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有图分析基准在核心算法选择、数据生成及API可用性评估方面存在不足，需新基准。

Method: 广泛调研选8种核心算法；设计数据生成器并生成8种新合成数据集；引入基于大语言模型的多级API可用性评估框架。

Result: 对现有平台进行实验评估，结果证明新基准的优越性。

Conclusion: 提出的新图分析基准能有效解决现有基准的不足。

Abstract: The rise of graph analytics platforms has led to the development of various
benchmarks for evaluating and comparing platform performance. However, existing
benchmarks often fall short of fully assessing performance due to limitations
in core algorithm selection, data generation processes (and the corresponding
synthetic datasets), as well as the neglect of API usability evaluation. To
address these shortcomings, we propose a novel graph analytics benchmark.
First, we select eight core algorithms by extensively reviewing both academic
and industrial settings. Second, we design an efficient and flexible data
generator and produce eight new synthetic datasets as the default datasets for
our benchmark. Lastly, we introduce a multi-level large language model
(LLM)-based framework for API usability evaluation-the first of its kind in
graph analytics benchmarks. We conduct comprehensive experimental evaluations
on existing platforms (GraphX, PowerGraph, Flash, Grape, Pregel+, Ligra and
G-thinker). The experimental results demonstrate the superiority of our
proposed benchmark.

</details>


### [23] [A Survey of LLM Inference Systems](https://arxiv.org/abs/2506.21901)
*James Pan,Guoliang Li*

Main category: cs.DB

TL;DR: 本文对大语言模型推理系统技术进行综述，分析技术依赖因素、系统构建方式并探讨剩余挑战。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型推理系统技术缺乏在完整推理系统框架下的分析和系统间的对比。

Method: 从请求处理的算子和算法、模型优化与执行技术、内存管理技术等方面回顾相关技术。

Result: 表明这些技术依赖负载预测、自适应机制和成本降低，可组合形成单副本和多副本推理系统。

Conclusion: 指出了大语言模型推理系统构建中仍存在的挑战。

Abstract: The past few years has witnessed specialized large language model (LLM)
inference systems, such as vLLM, SGLang, Mooncake, and DeepFlow, alongside
rapid LLM adoption via services like ChatGPT. Driving these system design
efforts is the unique autoregressive nature of LLM request processing,
motivating new techniques for achieving high performance while preserving high
inference quality over high-volume and high-velocity workloads. While many of
these techniques are discussed across the literature, they have not been
analyzed under the framework of a complete inference system, nor have the
systems themselves been analyzed and compared.
  In this survey, we review these techniques, starting from operators and
algorithms for request processing, then moving on to techniques for model
optimization and execution, including kernel design, batching, and scheduling,
before ending with techniques for memory management, including paged memory,
eviction and offloading techniques, quantization, and cache persistence.
Through these discussions, we show that these techniques fundamentally rely on
load prediction, adaptive mechanisms, and cost reduction in order to overcome
the challenges introduced by autoregressive generation and achieve the goals of
the system. We then discuss how these techniques can be combined to form
single-replica and multi-replica inference systems, including disaggregated
inference systems that offer more control over resource allocation and
serverless systems that can be deployed over shared hardware infrastructure. We
end with a discussion of remaining challenges.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [24] [SiPipe: Bridging the CPU-GPU Utilization Gap for Efficient Pipeline-Parallel LLM Inference](https://arxiv.org/abs/2506.22033)
*Yongchao He,Bohan Zhao,Zheng Cao*

Main category: cs.DC

TL;DR: 提出SiPipe异构管道设计，利用CPU资源提升大语言模型推理吞吐量、降低延迟和提高GPU利用率。


<details>
  <summary>Details</summary>
Motivation: 现有管道并行（PP）策略存在执行气泡导致效率低下，限制管道饱和度，需要改进。

Method: SiPipe采用CPU采样、令牌安全执行模型和结构感知传输三种关键技术，利用CPU资源分担辅助计算和通信。

Result: 在不同大语言模型上，相比vLLM，SiPipe吞吐量最高提升2.1倍，每令牌延迟降低43%，GPU平均利用率最高提升23%。

Conclusion: SiPipe在大语言模型和部署场景中具有通用性，能有效提升执行效率。

Abstract: As inference workloads for large language models (LLMs) scale to meet growing
user demand, pipeline parallelism (PP) has become a widely adopted strategy for
multi-GPU deployment, particularly in cross-node setups, to improve key-value
(KV) cache capacity and inference throughput. However, PP suffers from inherent
inefficiencies caused by three types of execution bubbles-load-imbalance,
intra-stage, and inter-stage-which limit pipeline saturation. We present
SiPipe, a heterogeneous pipeline design that improves throughput by leveraging
underutilized CPU resources to offload auxiliary computation and communication.
SiPipe incorporates three key techniques-CPU sampling, a token-safe execution
model, and structure-aware transmission-to mitigate pipeline bubbles and
improve execution efficiency. Across diverse LLMs, SiPipe achieves up to 2.1
times higher throughput, 43% lower per-token latency, and up to 23% higher
average GPU utilization compared to the state-of-the-art vLLM under the same PP
configuration, demonstrating its generality across LLMs and deployment
scenarios.

</details>


### [25] [SPTCStencil: Unleashing Sparse Tensor Cores for Stencil Computation via Strided Swap](https://arxiv.org/abs/2506.22035)
*Qiqi GU,Chenpeng Wu,Heng Shi,Jianguo Yao*

Main category: cs.DC

TL;DR: 本文提出SPTCStencil系统，利用稀疏张量核加速模板计算，实验显示有显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前在张量核心加速器上进行模板计算的优化技术，因矩阵乘法转换中的冗余零填充产生大量开销，需要解决效率问题。

Method: 提出稀疏计算范式，利用稀疏张量核（SpTCs），将模板计算有效转换为矩阵乘法，采用新的稀疏化策略使其与SpTC兼容，结合高性能GPU内核并进行系统优化。

Result: 实验表明，SPTCStencil比传统方法平均快5.46倍，比基于张量核心的方法平均快2.00倍。

Conclusion: SPTCStencil系统能有效利用SpTCs加速模板计算，提高计算效率。

Abstract: Stencil computation, a pivotal numerical method in science and engineering,
iteratively updates grid points using weighted neighbor contributions and
exhibits strong parallelism for multi-core processors. Current optimization
techniques targeting conducting stencil computation on tensor core accelerators
incur substantial overheads due to redundant zero-padding during the
transformation to matrix multiplication. To address this, we introduce a sparse
computation paradigm that eliminates inefficiencies by exploiting specialized
hardware units.
  This paper exploits the sparsity in these matrices as a feature and presents
SPTCStencil, a high-performance stencil computation system accelerated by
Sparse Tensor Core (SpTCs). SPTCStencil is the first to harness SpTCs for
acceleration beyond deep learning domains. First, Our approach generalizes an
efficient transformation of stencil computation into matrix multiplications and
specializes this conversion for SpTC compatibility through a novel
sparsification strategy. Furthermore, SPTCStencil incorporates a
high-performance GPU kernel with systematic optimizations designed to maximize
efficiency on SpTCs. Experimental evaluations demonstrate that SPTCStencil
5.46$\times$ and Tensor Core-based approaches by 2.00$\times$ on average.

</details>


### [26] [MCFuser: High-Performance and Rapid Fusion of Memory-Bound Compute-Intensive Operators](https://arxiv.org/abs/2506.22169)
*Zheng Zhang,Donglin Yang,Xiaobo Zhou,Dazhao Cheng*

Main category: cs.DC

TL;DR: 介绍了MCFuser框架，它能为内存受限的计算密集型算子链生成高性能融合内核，优化性能并加速调优。


<details>
  <summary>Details</summary>
Motivation: 现有算子融合技术在处理多计算密集型算子时因计算吞吐量饱和受限，且融合策略搜索空间有限、内存访问冗余和调优时间长，导致性能不佳和部署低效。

Method: 使用高级分块表达式定义搜索空间，结合有向无环图（DAG）分析消除冗余内存访问，通过规则剪枝搜索空间，结合分析性能模型和启发式搜索。

Result: 在NVIDIA A100和RTX3080 GPU上与Ansor等编译器对比，内核性能加速达5.9倍，调优时间减少超70倍。

Conclusion: MCFuser框架有效克服了现有算子融合技术的障碍，提升了性能并加速了调优过程，具有良好的性能表现。

Abstract: Operator fusion, a key technique to improve data locality and alleviate GPU
memory bandwidth pressure, often fails to extend to the fusion of multiple
compute-intensive operators due to saturated computation throughput. However,
the dynamicity of tensor dimension sizes could potentially lead to these
operators becoming memory-bound, necessitating the generation of fused kernels,
a task hindered by limited search spaces for fusion strategies, redundant
memory access, and prolonged tuning time, leading to sub-optimal performance
and inefficient deployment.
  We introduce MCFuser, a pioneering framework designed to overcome these
obstacles by generating high-performance fused kernels for what we define as
memory-bound compute-intensive (MBCI) operator chains. Leveraging high-level
tiling expressions to delineate a comprehensive search space, coupled with
Directed Acyclic Graph (DAG) analysis to eliminate redundant memory accesses,
MCFuser streamlines kernel optimization. By implementing guidelines to prune
the search space and incorporating an analytical performance model with a
heuristic search, MCFuser not only significantly accelerates the tuning process
but also demonstrates superior performance. Benchmarked against leading
compilers like Ansor on NVIDIA A100 and RTX3080 GPUs, MCFuser achieves up to a
5.9x speedup in kernel performance and outpaces other baselines while reducing
tuning time by over 70-fold, showcasing its agility.

</details>


### [27] [Proof-of-Behavior: Behavior-Driven Consensus for Trustworthy Decentralized Finance](https://arxiv.org/abs/2506.22171)
*Ailiya Borjigin,Wei Zhou,Cong He*

Main category: cs.DC

TL;DR: 提出Proof - of - Behavior (PoB)共识模型，可衡量验证者可信度，模拟实验显示其在DeFi中有良好表现，为金融应用区块链治理提供基础。


<details>
  <summary>Details</summary>
Motivation: 现有区块链协议无法衡量验证者可信度，在DeFi场景中存在微妙不当行为危害大。

Method: 引入PoB共识模型，为每个行为赋予分层效用分数，根据近期分数调整验证者权重，应用去中心化验证和按比例削减机制，奖励设计具有激励相容性。

Result: 模拟DeFi实验表明PoB将欺诈接受率降低90%以上，两轮内降低恶意验证者地位，相比标准PoS提高提议者公平性，吞吐量开销不超过5%。

Conclusion: PoB通过将共识影响力与可验证的可信行为挂钩，为金融应用中安全公平的区块链治理提供可扩展、符合监管的基础。

Abstract: Current blockchain protocols (e.g., Proof-of-Work and Proof-of-Stake) secure
the ledger yet cannot measure validator trustworthiness, allowing subtle
misconduct that is especially damaging in decentralized-finance (DeFi)
settings. We introduce Proof-of-Behavior (PoB), a consensus model that (i)
gives each action a layered utility score -- covering motivation and outcome,
(ii) adapts validator weights using recent scores, and (iii) applies
decentralized verification with proportional slashing. The reward design is
incentive-compatible, yielding a Nash equilibrium in which honest behavior
maximizes long-run pay-offs. Simulated DeFi experiments (loan-fraud detection,
reputation-weighted validation) show that PoB cuts fraud acceptance by more
than 90%, demotes malicious validators within two rounds, and improves proposer
fairness versus standard PoS, all with no more than a 5% throughput overhead.
By linking consensus influence to verifiably trustworthy conduct, PoB offers a
scalable, regulation-friendly foundation for secure and fair blockchain
governance in financial applications.

</details>


### [28] [MPipeMoE: Memory Efficient MoE for Pre-trained Models with Adaptive Pipeline Parallelism](https://arxiv.org/abs/2506.22175)
*Zheng Zhang,Donglin Yang,Yaqi Xia,Liang Ding,Dacheng Tao,Xiaobo Zhou,Dazhao Cheng*

Main category: cs.DC

TL;DR: 本文提出高性能库MPipeMoE，用自适应和内存高效的流水线并行加速MoE训练，实现提速和减少内存占用。


<details>
  <summary>Details</summary>
Motivation: 现有MoE技术在通信和内存消耗方面存在低效问题，需要解决。

Method: 设计自适应流水线并行的在线算法配置流水线粒度；提出内存复用策略减少冗余，开发自适应选择组件确定最优策略。

Result: 在由8台NVIDIA DGX A100服务器组成的物理集群上评估，相比现有方法，MPipeMoE在训练大模型时提速达2.8倍，减少内存占用达47%。

Conclusion: MPipeMoE能有效加速MoE训练，提升内存使用效率。

Abstract: Recently, Mixture-of-Experts (MoE) has become one of the most popular
techniques to scale pre-trained models to extraordinarily large sizes. Dynamic
activation of experts allows for conditional computation, increasing the number
of parameters of neural networks, which is critical for absorbing the vast
amounts of knowledge available in many deep learning areas. However, despite
the existing system and algorithm optimizations, there are significant
challenges to be tackled when it comes to the inefficiencies of communication
and memory consumption.
  In this paper, we present the design and implementation of MPipeMoE, a
high-performance library that accelerates MoE training with adaptive and
memory-efficient pipeline parallelism. Inspired by that the MoE training
procedure can be divided into multiple independent sub-stages, we design
adaptive pipeline parallelism with an online algorithm to configure the
granularity of the pipelining. Further, we analyze the memory footprint
breakdown of MoE training and identify that activations and temporary buffers
are the primary contributors to the overall memory footprint. Toward memory
efficiency, we propose memory reusing strategies to reduce memory requirements
by eliminating memory redundancies, and develop an adaptive selection component
to determine the optimal strategy that considers both hardware capacities and
model characteristics at runtime. We implement MPipeMoE upon PyTorch and
evaluate it with common MoE models in a physical cluster consisting of 8 NVIDIA
DGX A100 servers. Compared with the state-of-art approach, MPipeMoE achieves up
to 2.8x speedup and reduces memory footprint by up to 47% in training large
models.

</details>


### [29] [Towards Operational Data Analytics Chatbots -- Virtual Knowledge Graph is All You Need](https://arxiv.org/abs/2506.22267)
*Junaid Ahmed Khan,Hiari Pizzini Cavagna,Andrea Proia,Andrea Bartolini*

Main category: cs.DC

TL;DR: 提出使用大语言模型生成SPARQL查询、利用VKG进行数据检索的端到端ODA聊天机器人系统，准确性高、降低查询延迟、控制VKG大小，适合实时交互。


<details>
  <summary>Details</summary>
Motivation: 生成式人工智能挑战科学计算，数据中心规模和数据量增长，计算效率至关重要，现有ODA聚焦实时遥测数据可视化和事后分析，NoSQL数据库查询困难，传统KG扩展成本高且未广泛用于多变量时间序列。

Method: 构建使用大语言模型生成SPARQL查询、利用VKG进行数据检索的端到端ODA聊天机器人系统，优化VKG构建和LLM推理。

Result: 该方法准确率达92.5%，相比直接NoSQL查询的25%大幅提升；平均查询延迟降低85%，从20.36秒降至3.03秒，VKG大小控制在179MiB以下。

Conclusion: 该系统性能良好，适合部署并与ODA终端用户进行实时交互。

Abstract: With generative artificial intelligence challenging computational scientific
computing, data centers are experiencing unprecedented growth in both scale and
volume. As a result, computing efficiency has become more critical than ever.
Operational Data Analytics (ODA) relies on the collection of data center
telemetry to improve efficiency, but so far has been focusing on real-time
telemetry data visualization and post-mortem analysis. However, with NoSQL
databases now serving as the default storage backend to support scalability,
querying this data is challenging due to its schema-less nature, which requires
domain knowledge to traverse relationships between data sources. Ontologies and
Knowledge Graphs (KGs) can capture these relationships, but traditional KGs are
costly to scale and have not been widely applied to multivariate timeseries.
Virtual Knowledge Graphs (VKGs) offer a lightweight alternative by generating
query-specific graphs at runtime. In this work, we present a full end-to-end
ODA chatbot system that uses a Large Language Model (LLM) to generate SPARQL
queries, utilizing VKG for data retrieval. This approach achieves 92.5%
accuracy compared to 25% with direct NoSQL queries. The proposed methodology
optimizes VKG construction and LLM inference, cutting previous work average
query latency by 85% (from 20.36s to 3.03s) and keeping VKG sizes under 179
MiB. This performance makes the tool suitable for deployment and real-time
interaction with ODA end-users.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [30] [INTACT: Compact Storage of Data Streams in Mobile Devices to Unlock User Privacy at the Edge](https://arxiv.org/abs/2506.21998)
*Rémy Raes,Olivier Ruas,Adrien Luxey-Bitri,Romain Rouvoy*

Main category: cs.DS

TL;DR: 本文提出FLI技术和Divide & Stay隐私保护技术，构建INTACT框架，推动泛在计算系统的隐私保护。


<details>
  <summary>Details</summary>
Motivation: 移动设备数据流集中处理有隐私风险，现有保护机制未在移动设备实现，且移动设备资源受限难以直接提供服务。

Method: 采用分段线性逼近技术的FLI技术捕捉数据流紧凑表示，引入Divide & Stay技术进行兴趣点推理，并将二者部署为INTACT框架。

Result: 成功在Android和iOS上部署INTACT框架。

Conclusion: 向泛在计算系统中实施隐私和信任保护迈出了实际一步。

Abstract: Data streams produced by mobile devices, such as smartphones, offer highly
valuable sources of information to build ubiquitous services. Such data streams
are generally uploaded and centralized to be processed by third parties,
potentially exposing sensitive personal information. In this context, existing
protection mechanisms, such as Location Privacy Protection Mechanisms (LPPMs),
have been investigated. Alas, none of them have actually been implemented, nor
deployed in real-life, in mobile devices to enforce user privacy at the edge.
Moreover, the diversity of embedded sensors and the resulting data deluge makes
it impractical to provision such services directly on mobiles, due to their
constrained storage capacity, communication bandwidth and processing power.
This article reports on the FLI technique, which leverages a piece-wise linear
approximation technique to capture compact representations of data streams in
mobile devices. Beyond the FLI storage layer, we introduce Divide \& Stay, a
new privacy preservation technique to execute Points of Interest (POIs)
inference. Finally, we deploy both of them on Android and iOS as the INTACT
framework, making a concrete step towards enforcing privacy and trust in
ubiquitous computing systems.

</details>


### [31] [Fault-Tolerant Matroid Bases](https://arxiv.org/abs/2506.22010)
*Matthias Bentert,Fedor V. Fomin,Petr A. Golovach,Laure Morelle*

Main category: cs.DS

TL;DR: 研究拟阵中构造容错基问题，提出双参数可固定求解算法并分析参数复杂度。


<details>
  <summary>Details</summary>
Motivation: 拟阵可泛化线性独立性，该问题统一并拓展先前研究的容错概念。

Method: 提出针对k容错基问题，以k和拟阵秩r为参数的固定参数可处理（FPT）算法。

Result: 证明以k + r为双变量的参数化是紧的，k = 1时问题NP难，r ≥ 3时Para - NP难，r ≤ 2时多项式时间可解。

Conclusion: 给出求解k容错基问题的有效算法，并明确了不同参数下问题的复杂度。

Abstract: We investigate the problem of constructing fault-tolerant bases in matroids.
Given a matroid M and a redundancy parameter k, a k-fault-tolerant basis is a
minimum-size set of elements such that, even after the removal of any k
elements, the remaining subset still spans the entire ground set. Since
matroids generalize linear independence across structures such as vector
spaces, graphs, and set systems, this problem unifies and extends several
fault-tolerant concepts appearing in prior research.
  Our main contribution is a fixed-parameter tractable (FPT) algorithm for the
k-fault-tolerant basis problem, parameterized by both k and the rank r of the
matroid. This two-variable parameterization by k + r is shown to be tight in
the following sense. On the one hand, the problem is already NP-hard for k=1.
On the other hand, it is Para-NP-hard for r \geq 3 and polynomial-time solvable
for r \leq 2.

</details>


### [32] [Parameterized Complexity of Directed Traveling Salesman Problem](https://arxiv.org/abs/2506.22127)
*Václav Blažej,Andreas Emil Feldmann,Foivos Fioravantes,Paweł Rzążewski,Ondřej Suchý*

Main category: cs.DS

TL;DR: 研究有向旅行商问题（DTSP）及其更一般版本有向路点路由问题（DWRP）的参数化复杂度，得出DWRP在不同参数下的复杂度结果。


<details>
  <summary>Details</summary>
Motivation: 以往对TSP及其变体的研究多从近似角度，参数化复杂度方面结果较少，本文旨在系统研究DTSP变体关于各种（主要是结构）参数的复杂度。

Method: 基于Marx等人的结果，对DWRP在不同参数下的复杂度进行分析。

Result: DWRP在解大小、反馈边数和基础无向图的顶点完整性参数下是FPT，在树宽参数下是XP，在到常数树深度的距离参数下是W[1]-hard。

Conclusion: 开启了DTSP变体关于各种参数的系统复杂度研究，明确了DWRP在不同参数下的复杂度情况。

Abstract: The Directed Traveling Salesman Problem (DTSP) is a variant of the classical
Traveling Salesman Problem in which the edges in the graph are directed and a
vertex and edge can be visited multiple times. The goal is to find a directed
closed walk of minimum length (or total weight) that visits every vertex of the
given graph at least once. In a yet more general version, Directed Waypoint
Routing Problem (DWRP), some vertices are marked as terminals and we are only
required to visit all terminals. Furthermore, each edge has its capacity
bounding the number of times this edge can be used by a solution.
  While both problems (and many other variants of TSP) were extensively
investigated, mostly from the approximation point of view, there are
surprisingly few results concerning the parameterized complexity. Our starting
point is the result of Marx et al. [APPROX/RANDOM 2016] who proved that DTSP is
W[1]-hard parameterized by distance to pathwidth 3. In this paper we aim to
initiate the systematic complexity study of variants of DTSP with respect to
various, mostly structural, parameters.
  We show that DWRP is FPT parameterized by the solution size, the feedback
edge number, and the vertex integrity of the underlying undirected graph.
Furthermore, the problem is XP parameterized by treewidth. On the complexity
side, we show that the problem is W[1]-hard parameterized by the distance to
constant treedepth.

</details>


### [33] [Shortest Paths in Multimode Graphs](https://arxiv.org/abs/2506.22261)
*Yael Kirkpatrick,Virginia Vassilevska Williams*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In this work we study shortest path problems in multimode graphs, a
generalization of the min-distance measure introduced by Abboud, Vassilevska W.
and Wang in [SODA'16]. A multimode shortest path is the shortest path using one
of multiple `modes' of transportation that cannot be combined. This represents
real-world scenarios where different modes are not combinable, such as flights
operated by different airlines. More precisely, a $k$-multimode graph is a
collection of $k$ graphs on the same vertex set and the $k$-mode distance
between two vertices is defined as the minimum among the distances computed in
each individual graph.
  We focus on approximating fundamental graph parameters on these graphs,
specifically diameter and radius. In undirected multimode graphs we first show
an elegant linear time 3-approximation algorithm for 2-mode diameter. We then
extend this idea into a general subroutine that can be used as a part of any
$\alpha$-approximation, and use it to construct a 2 and 2.5 approximation
algorithm for 2-mode diameter. For undirected radius, we introduce a general
scheme that can compute a 3-approximation of the $k$-mode radius for any $k$.
In the directed case we develop novel techniques to construct a linear time
algorithm to determine whether the diameter is finite.
  We also develop many conditional fine-grained lower bounds for various
multimode diameter and radius approximation problems. We are able to show that
many of our algorithms are tight under popular fine-grained complexity
hypotheses, including our linear time 3-approximation for $3$-mode undirected
diameter and radius. As part of this effort we propose the first extension to
the Hitting Set Hypothesis [SODA'16], which we call the $\ell$-Hitting Set
Hypothesis. We use this hypothesis to prove the first parameterized lower bound
tradeoff for radius approximation algorithms.

</details>


### [34] [Faster exponential algorithms for cut problems via geometric data structures](https://arxiv.org/abs/2506.22281)
*László Kozma,Junqi Tan*

Main category: cs.DS

TL;DR: 本文为d - Cut、Internal Partition和(α,β) - Domination问题设计运行时间为O(1.9999977^n)的算法，结合了分裂列表技术和计算几何工具。


<details>
  <summary>Details</summary>
Motivation: 许多计算难题有时间复杂度为2^n * n^{O(1)}的简单算法，寻找更快的指数级算法是精确指数算法领域的自然目标。

Method: 将分裂列表技术与计算几何中的适度维正交范围搜索工具相结合。

Result: 得到了针对d - Cut、Internal Partition和(α,β) - Domination问题运行时间为O(1.9999977^n)的算法，技术适用于决策、优化和计数版本及多种变体。

Conclusion: 该技术有效，可用于相关问题及其变体，拓展了已知算法适用范围。

Abstract: For many hard computational problems, simple algorithms that run in time $2^n
\cdot n^{O(1)}$ arise, say, from enumerating all subsets of a size-$n$ set.
Finding (exponentially) faster algorithms is a natural goal that has driven
much of the field of exact exponential algorithms (e.g., see Fomin and Kratsch,
2010). In this paper we obtain algorithms with running time $O(1.9999977^n)$ on
input graphs with $n$ vertices, for the following well-studied problems:
  - $d$-Cut: find a proper cut in which no vertex has more than $d$ neighbors
on the other side of the cut;
  - Internal Partition: find a proper cut in which every vertex has at least as
many neighbors on its side of the cut as on the other side; and
  - ($\alpha,\beta$)-Domination: given intervals $\alpha,\beta \subseteq
[0,n]$, find a subset $S$ of the vertices, so that for every vertex $v \in S$
the number of neighbors of $v$ in $S$ is from $\alpha$ and for every vertex $v
\notin S$, the number of neighbors of $v$ in $S$ is from $\beta$.
  Our algorithms are exceedingly simple, combining the split and list technique
(Horowitz and Sahni, 1974; Williams, 2005) with a tool from computational
geometry: orthogonal range searching in the moderate dimensional regime (Chan,
2017). Our technique is applicable to the decision, optimization and counting
versions of these problems and easily extends to various generalizations with
more fine-grained, vertex-specific constraints, as well as to directed,
balanced, and other variants. Algorithms with running times of the form $c^n$,
for $c<2$, were known for the first problem only for constant $d$, and for the
third problem for certain special cases of $\alpha$ and $\beta$; for the second
problem we are not aware of such results.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [35] [Simultaneously Fair Allocation of Indivisible Items Across Multiple Dimensions](https://arxiv.org/abs/2506.21727)
*Yasushi Kawase,Bodhayan Roy,Mohammad Azharuddin Sanpui*

Main category: cs.GT

TL;DR: 本文研究多维环境下不可分物品的公平分配，提出两种宽松的无嫉妒性变体，给出保证分配存在的参数上下界、检查算法，并证明部分情况的NP难。


<details>
  <summary>Details</summary>
Motivation: 解决复杂环境中，传统一维公平概念无法适用于多标准评估物品束的公平性问题。

Method: 研究弱和强的同时无嫉妒至多c件物品（weak sEFc和strong sEFc）两种变体，给出参数c的上下界，设计检查算法。

Result: 给出参数c上下界，提出检查弱或强sEFc分配是否存在的算法，证明检查weak sEF1和strong sEF1分配存在性的NP难结果。

Conclusion: 提出的变体和相关结论有助于解决多维环境下不可分物品的公平分配问题。

Abstract: This paper explores the fair allocation of indivisible items in a
multidimensional setting, motivated by the need to address fairness in complex
environments where agents assess bundles according to multiple criteria. Such
multidimensional settings are not merely of theoretical interest but are
central to many real-world applications. For example, cloud computing resources
are evaluated based on multiple criteria such as CPU cores, memory, and network
bandwidth. In such cases, traditional one dimensional fairness notions fail to
capture fairness across multiple attributes. To address these challenges, we
study two relaxed variants of envy-freeness: weak simultaneously envy-free up
to c goods (weak sEFc) and strong simultaneously envy-free up to c goods
(strong sEFc), which accommodate the multidimensionality of agents'
preferences. Under the weak notion, for every pair of agents and for each
dimension, any perceived envy can be eliminated by removing, if necessary, a
different set of goods from the envied agent's allocation. In contrast, the
strong version requires selecting a single set of goods whose removal from the
envied bundle simultaneously eliminates envy in every dimension. We provide
upper and lower bounds on the relaxation parameter c that guarantee the
existence of weak or strong sEFc allocations, where these bounds are
independent of the total number of items. In addition, we present algorithms
for checking whether a weak or strong sEFc allocation exists. Moreover, we
establish NP-hardness results for checking the existence of weak sEF1 and
strong sEF1 allocations.

</details>


### [36] [Pseudo-Equilibria, or: How to Stop Worrying About Crypto and Just Analyze the Game](https://arxiv.org/abs/2506.22089)
*Alexandros Psomas,Athina Terzoglou,Yu Wei,Vassilis Zikas*

Main category: cs.GT

TL;DR: 论文探讨博弈论者分析加密协议游戏问题，指出理想世界结论难完全应用于现实，提出伪纳什均衡概念解决理想与现实转换问题。


<details>
  <summary>Details</summary>
Motivation: 由于游戏理论家难以处理完整加密复杂性，且分布式账本兴起使密码学与博弈论需共享语言，需解决理想世界结论向现实世界迁移问题。

Method: 提出伪纳什均衡概念，证明理想加密游戏中的纳什均衡在现实协议下对应伪纳什均衡。

Result: 提出的伪纳什均衡比先前概念更简单、易理解，能避免调整或限制理想游戏中的效用函数。

Conclusion: 伪纳什均衡可让我们分别且无缝地研究博弈论和密码学方面。

Abstract: We consider the problem of a game theorist analyzing a game that uses
cryptographic protocols. Ideally, a theorist abstracts protocols as ideal,
implementation-independent primitives, letting conclusions in the "ideal world"
carry over to the "real world." This is crucial, since the game theorist
cannot--and should not be expected to--handle full cryptographic complexity. In
today's landscape, the rise of distributed ledgers makes a shared language
between cryptography and game theory increasingly necessary.
  The security of cryptographic protocols hinges on two types of assumptions:
state-of-the-world (e.g., "factoring is hard") and behavioral (e.g., "honest
majority"). We observe that for protocols relying on behavioral assumptions
(e.g., ledgers), our goal is unattainable in full generality. For
state-of-the-world assumptions, we show that standard solution concepts, e.g.,
($\epsilon$-)Nash equilibria, are not robust to transfer from the ideal to the
real world.
  We propose a new solution concept: the pseudo-Nash equilibrium. Informally, a
profile $s=(s_1,\dots,s_n)$ is a pseudo-Nash equilibrium if, for any player $i$
and deviation $s'_i$ with higher expected utility, $i$'s utility from $s_i$ is
(computationally) indistinguishable from that of $s'_i$. Pseudo-Nash is simpler
and more accessible to game theorists than prior notions addressing the
mismatch between (asymptotic) cryptography and game theory. We prove that Nash
equilibria in games with ideal, unbreakable cryptography correspond to
pseudo-Nash equilibria when ideal cryptography is instantiated with real
protocols (under state-of-the-world assumptions). Our translation is
conceptually simpler and more general: it avoids tuning or restricting utility
functions in the ideal game to fit quirks of cryptographic implementations.
Thus, pseudo-Nash lets us study game-theoretic and cryptographic aspects
separately and seamlessly.

</details>


### [37] [A few good choices](https://arxiv.org/abs/2506.22133)
*Thanh Nguyen,Haoyu Song,Young-San Lin*

Main category: cs.GT

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: A Condorcet winning set addresses the Condorcet paradox by selecting a few
candidates--rather than a single winner--such that no unselected alternative is
preferred to all of them by a majority of voters. This idea extends to
$\alpha$-undominated sets, which ensure the same property for any
$\alpha$-fraction of voters and are guaranteed to exist in constant size for
any $\alpha$. However, the requirement that an outsider be preferred to every
member of the set can be overly restrictive and difficult to justify in many
applications. Motivated by this, we introduce a more flexible notion: $(t,
\alpha)$-undominated sets. Here, each voter compares an outsider to their
$t$-th most preferred member of the set, and the set is undominated if no
outsider is preferred by more than an $\alpha$-fraction of voters. This
framework subsumes prior definitions, recovering Condorcet winning sets when
$(t = 1, \alpha = 1/2)$ and $\alpha$-undominated sets when $t = 1$, and
introduces a new, tunable notion of collective acceptability for $t > 1$. We
establish three main results:
  1. We prove that a $(t, \alpha)$-undominated set of size $O(t/\alpha)$ exists
for all values of $t$ and $\alpha$.
  2. We show that as $t$ becomes large, the minimum size of such a set
approaches $t/\alpha$, which is asymptotically optimal.
  3. In the special case $t = 1$, we improve the bound on the size of an
$\alpha$-undominated set given by Charikar, Lassota, Ramakrishnan, Vetta, and
Wang (STOC 2025). As a consequence, we show that a Condorcet winning set of
five candidates exists, improving their bound of six.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [38] [PentaRAG: Large-Scale Intelligent Knowledge Retrieval for Enterprise LLM Applications](https://arxiv.org/abs/2506.21593)
*Abu Hanif Muhammad Syarubany,Chang Dong Yoo*

Main category: cs.IR

TL;DR: 提出PentaRAG五层模块应对企业部署大语言模型需求，在效率、准确性等方面有提升。


<details>
  <summary>Details</summary>
Motivation: 经典检索增强生成（RAG）管道无法完全满足企业部署大语言模型对文档集合不断变化、低延迟和可预测GPU成本的需求。

Method: 构建PentaRAG五层模块，通过两个即时缓存、内存召回模式、自适应会话内存和传统检索增强层路由查询，结合Mistral - 8B、Milvus和vLLM实现。

Result: 在TriviaQA领域提升答案相似度和事实正确性；缓存预热降低平均延迟；减少平均GPU时间，提高吞吐量。

Conclusion: 分层路由策略能在生产级RAG系统中同时实现数据新鲜度、速度和效率。

Abstract: Enterprise deployments of large-language model (LLM) demand continuously
changing document collections with sub-second latency and predictable GPU cost
requirements that classical Retrieval-Augmented Generation (RAG) pipelines only
partially satisfy. We present PentaRAG, a five-layer module that routes each
query through two instant caches (fixed key-value and semantic), a
memory-recall mode that exploits the LLM's own weights, an adaptive session
memory, and a conventional retrieval-augmentation layer. Implemented with
Mistral-8B, Milvus and vLLM, the system can answer most repeated or
semantically similar questions from low-latency caches while retaining full
retrieval for novel queries. On the TriviaQA domain, LoRA fine-tuning combined
with the memory-recall layer raises answer similarity by approximately 8% and
factual correctness by approximately 16% over the base model. Under a
nine-session runtime simulation, cache warming reduces mean latency from
several seconds to well below one second and shifts traffic toward the fast
paths. Resource-efficiency tests show that PentaRAG cuts average GPU time to
0.248 seconds per query, roughly half that of a naive RAG baseline, and
sustains an aggregate throughput of approximately 100,000 queries per second on
our setup. These results demonstrate that a layered routing strategy can
deliver freshness, speed, and efficiency simultaneously in production-grade RAG
systems.

</details>


### [39] [LLM2Rec: Large Language Models Are Powerful Embedding Models for Sequential Recommendation](https://arxiv.org/abs/2506.21579)
*Yingzhi He,Xiaohao Liu,An Zhang,Yunshan Ma,Tat-Seng Chua*

Main category: cs.IR

TL;DR: 提出用于序列推荐的嵌入模型LLM2Rec，结合大语言模型语义理解和协同过滤信息，经两阶段训练，实验证明其能提升推荐质量。


<details>
  <summary>Details</summary>
Motivation: 传统基于ID嵌入的序列推荐器缺乏可迁移知识，基于文本的推荐方法无法编码协同过滤信号，理想嵌入模型应整合两者。

Method: 提出LLM2Rec模型，采用两阶段训练框架：协同监督微调让大语言模型根据历史交互推断项目关系；项目级嵌入建模将微调后的模型转化为能编码语义和协同信息的项目嵌入模型。

Result: 在真实数据集上的实验表明，LLM2Rec能有效提高领域内和领域外的推荐质量。

Conclusion: 利用大语言模型构建用于序列推荐的更强大、可泛化的嵌入模型具有潜力。

Abstract: Sequential recommendation aims to predict users' future interactions by
modeling collaborative filtering (CF) signals from historical behaviors of
similar users or items. Traditional sequential recommenders predominantly rely
on ID-based embeddings, which capture CF signals through high-order
co-occurrence patterns. However, these embeddings depend solely on past
interactions, lacking transferable knowledge to generalize to unseen domains.
Recent advances in large language models (LLMs) have motivated text-based
recommendation approaches that derive item representations from textual
descriptions. While these methods enhance generalization, they fail to encode
CF signals-i.e., latent item correlations and preference patterns-crucial for
effective recommendation. We argue that an ideal embedding model should
seamlessly integrate CF signals with rich semantic representations to improve
both in-domain and out-of-domain recommendation performance.
  To this end, we propose LLM2Rec, a novel embedding model tailored for
sequential recommendation, integrating the rich semantic understanding of LLMs
with CF awareness. Our approach follows a two-stage training framework: (1)
Collaborative Supervised Fine-tuning, which adapts LLMs to infer item
relationships based on historical interactions, and (2) Item-level Embedding
Modeling, which refines these specialized LLMs into structured item embedding
models that encode both semantic and collaborative information. Extensive
experiments on real-world datasets demonstrate that LLM2Rec effectively
improves recommendation quality across both in-domain and out-of-domain
settings. Our findings highlight the potential of leveraging LLMs to build more
robust, generalizable embedding models for sequential recommendation. Our codes
are available at https://github.com/HappyPointer/LLM2Rec.

</details>


### [40] [Evaluating the Robustness of Dense Retrievers in Interdisciplinary Domains](https://arxiv.org/abs/2506.21581)
*Sarthak Chaturvedi,Anurag Acharya,Rounak Meyur,Koby Hayashi,Sai Munikoti,Sameera Horawalavithana*

Main category: cs.IR

TL;DR: 评估基准特征会扭曲检索模型领域自适应的真实效果，通过环境法规文件检索案例研究发现基准选择强烈影响检索系统有效性评估。


<details>
  <summary>Details</summary>
Motivation: 解决评估基准特征可能造成的对领域自适应真实益处的误判问题，避免影响专业领域部署决策。

Method: 以环境法规文件检索为例，在环境影响声明上微调ColBERTv2模型，在两个不同语义结构的基准上评估模型。

Result: 相同领域自适应方法在不同基准上效果差异大，一个基准上改进小（最大0.61% NDCG增益），另一个基准上改进大（最高2.22% NDCG增益），高表现基准上下文余弦距离高11%、轮廓系数低23%。

Conclusion: 基准选择强烈决定专业领域检索系统有效性评估，主题分离的评估框架常低估领域自适应益处，语义边界重叠的框架能更好反映现实文档复杂性，对跨学科领域AI系统开发和部署有重要意义。

Abstract: Evaluation benchmark characteristics may distort the true benefits of domain
adaptation in retrieval models. This creates misleading assessments that
influence deployment decisions in specialized domains. We show that two
benchmarks with drastically different features such as topic diversity,
boundary overlap, and semantic complexity can influence the perceived benefits
of fine-tuning. Using environmental regulatory document retrieval as a case
study, we fine-tune ColBERTv2 model on Environmental Impact Statements (EIS)
from federal agencies. We evaluate these models across two benchmarks with
different semantic structures. Our findings reveal that identical domain
adaptation approaches show very different perceived benefits depending on
evaluation methodology. On one benchmark, with clearly separated topic
boundaries, domain adaptation shows small improvements (maximum 0.61% NDCG
gain). However, on the other benchmark with overlapping semantic structures,
the same models demonstrate large improvements (up to 2.22% NDCG gain), a
3.6-fold difference in the performance benefit. We compare these benchmarks
through topic diversity metrics, finding that the higher-performing benchmark
shows 11% higher average cosine distances between contexts and 23% lower
silhouette scores, directly contributing to the observed performance
difference. These results demonstrate that benchmark selection strongly
determines assessments of retrieval system effectiveness in specialized
domains. Evaluation frameworks with well-separated topics regularly
underestimate domain adaptation benefits, while those with overlapping semantic
boundaries reveal improvements that better reflect real-world regulatory
document complexity. Our findings have important implications for developing
and deploying AI systems for interdisciplinary domains that integrate multiple
topics.

</details>


### [41] [SERP Interference Network and Its Applications in Search Advertising](https://arxiv.org/abs/2506.21598)
*Purak Jain,Sandeep Appala*

Main category: cs.IR

TL;DR: 电商行业搜索引擎营销团队需进行A/B测试，但面临匿名用户挑战。本文提出利用截尾观测数据构建二分网络，用新加权函数处理并聚类随机化，还展示其在评估新竞价算法的应用及系统架构蓝图。


<details>
  <summary>Details</summary>
Motivation: 电商行业搜索引擎营销团队为优化长期盈利能力需进行A/B测试，但面临匿名用户和违反假设的挑战。

Method: 利用截尾观测数据构建二分SERP干扰网络，用新加权函数创建加权投影形成单分图并聚类随机化。

Result: 展示了该实验设计在评估付费搜索新竞价算法中的应用，并提供利用SageMaker的新系统架构蓝图。

Conclusion: 提出的方法可用于解决电商搜索引擎营销A/B测试面临的问题，并提供了具体应用和系统架构。

Abstract: Search Engine marketing teams in the e-commerce industry manage global search
engine traffic to their websites with the aim to optimize long-term
profitability by delivering the best possible customer experience on Search
Engine Results Pages (SERPs). In order to do so, they need to run continuous
and rapid Search Marketing A/B tests to continuously evolve and improve their
products. However, unlike typical e-commerce A/B tests that can randomize based
on customer identification, their tests face the challenge of anonymized users
on search engines. On the other hand, simply randomizing on products violates
Stable Unit Treatment Value Assumption for most treatments of interest. In this
work, we propose leveraging censored observational data to construct bipartite
(Search Query to Product Ad or Text Ad) SERP interference networks. Using a
novel weighting function, we create weighted projections to form unipartite
graphs which can then be use to create clusters to randomized on. We
demonstrate this experimental design's application in evaluating a new bidding
algorithm for Paid Search. Additionally, we provide a blueprint of a novel
system architecture utilizing SageMaker which enables polyglot programming to
implement each component of the experimental framework.

</details>


### [42] [Reinforcement Fine-Tuned Large Language Models for Next POI Recommendation](https://arxiv.org/abs/2506.21599)
*Peibo Li,Shuang Ao,Hao Xue,Yang Song,Maarten de Rijke,Johan Barthélemy,Tomasz Bednarz,Flora D. Salim*

Main category: cs.IR

TL;DR: 本文指出基于大语言模型的下一个兴趣点推荐模型的问题，提出Refine - POI框架，实验证明其达到了最先进的top - k推荐性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于大语言模型的下一个兴趣点推荐模型中，基于提示的模型准确性低，基于监督微调的模型存在数据与微调方式不匹配的问题。

Method: 提出Refine - POI，一种用于下一个兴趣点推荐的强化微调框架，引入推荐驱动奖励机制，让大语言模型利用每个示例中的一个真实兴趣点学习生成top - k推荐列表。

Result: 在真实数据集上的实验表明，Refine - POI达到了最先进的top - k推荐性能。

Conclusion: Refine - POI框架能有效解决现有模型的问题，提升下一个兴趣点推荐的top - k性能。

Abstract: Large language models (LLMs) have been adopted for next point-of-interest
(POI) recommendation tasks. Typical LLM-based recommenders fall into two
categories: prompt-based and supervised fine-tuning (SFT)-based models.
Prompt-based models generally offer greater output flexibility but deliver
lower accuracy, whereas SFT-based models achieve higher performance yet face a
fundamental mismatch: next POI recommendation data does not naturally suit
supervised fine-tuning. In SFT, the model is trained to reproduce the exact
ground truth, but each training example provides only a single target POI, so
there is no ground truth for producing a top-k list.
  To address this, we propose Refine-POI, a reinforcement fine-tuning framework
for next POI recommendation. We introduce recommendation-driven rewards that
enable LLMs to learn to generate top-k recommendation lists using only one
ground-truth POI per example. Experiments on real-world datasets demonstrate
that Refine-POI achieves state-of-the-art top-k recommendation performance.

</details>


### [43] [Hierarchical Patch Compression for ColPali: Efficient Multi-Vector Document Retrieval with Dynamic Pruning and Quantization](https://arxiv.org/abs/2506.21601)
*Duong Bach*

Main category: cs.IR

TL;DR: 提出HPC - ColPali框架提升ColPali效率，经评估效果良好，代码开源。


<details>
  <summary>Details</summary>
Motivation: 多向量文档检索系统ColPali存在存储和计算成本高的问题，需提升其效率。

Method: 提出HPC - ColPali框架，集成K - Means量化、注意力引导动态剪枝和可选的质心索引二进制编码三种技术。

Result: 在ViDoRe和SEC - Filings数据集上，查询延迟降低30 - 50%，集成到法律摘要检索增强生成管道中，幻觉率降低30%，端到端延迟减半。

Conclusion: HPC - ColPali是多向量文档检索在不同应用中的可扩展且高效的解决方案。

Abstract: Multi-vector document retrieval systems, such as ColPali, excel in
fine-grained matching for complex queries but incur significant storage and
computational costs due to their reliance on high-dimensional patch embeddings
and late-interaction scoring. To address these challenges, we propose
HPC-ColPali, a Hierarchical Patch Compression framework that enhances the
efficiency of ColPali while preserving its retrieval accuracy. Our approach
integrates three innovative techniques: (1) K-Means quantization, which
compresses patch embeddings into 1-byte centroid indices, achieving up to
32$\times$ storage reduction; (2) attention-guided dynamic pruning, utilizing
Vision-Language Model attention weights to retain only the top-$p\%$ most
salient patches, reducing late-interaction computation by up to 60\% with less
than 2\% nDCG@10 loss; and (3) optional binary encoding of centroid indices
into $b$-bit strings ($b=\lceil\log_2 K\rceil$), enabling rapid Hamming
distance-based similarity search for resource-constrained environments.
Evaluated on the ViDoRe and SEC-Filings datasets, HPC-ColPali achieves 30--50\%
lower query latency under HNSW indexing while maintaining high retrieval
precision. When integrated into a Retrieval-Augmented Generation pipeline for
legal summarization, it reduces hallucination rates by 30\% and halves
end-to-end latency. These advancements establish HPC-ColPali as a scalable and
efficient solution for multi-vector document retrieval across diverse
applications. Code is available at https://github.com/DngBack/HPC-ColPali.

</details>


### [44] [Evaluating VisualRAG: Quantifying Cross-Modal Performance in Enterprise Document Understanding](https://arxiv.org/abs/2506.21604)
*Varun Mannam,Fang Wang,Xin Chen*

Main category: cs.IR

TL;DR: 本文提出系统量化基准框架，衡量VisualRAG系统跨模态输入的可信度，展示最优模态权重可提升性能，为企业关键应用的多模态RAG提供增强可信度的方法。


<details>
  <summary>Details</summary>
Motivation: 当前多模态生成式AI评估框架难以建立可信度，阻碍企业采用，需要为企业文档智能的VisualRAG系统建立评估框架。

Method: 引入系统的量化基准框架，建立技术指标和以用户为中心的信任度量之间的定量关系。

Result: 最优模态权重（文本30%、图像15%、标题25%、OCR 30%）比纯文本基线性能提高57.3%，并对基础模型进行了比较评估。

Conclusion: 该工作通过提供严格框架，推进了关键企业应用中多模态RAG负责任AI的部署。

Abstract: Current evaluation frameworks for multimodal generative AI struggle to
establish trustworthiness, hindering enterprise adoption where reliability is
paramount. We introduce a systematic, quantitative benchmarking framework to
measure the trustworthiness of progressively integrating cross-modal inputs
such as text, images, captions, and OCR within VisualRAG systems for enterprise
document intelligence. Our approach establishes quantitative relationships
between technical metrics and user-centric trust measures. Evaluation reveals
that optimal modality weighting with weights of 30% text, 15% image, 25%
caption, and 30% OCR improves performance by 57.3% over text-only baselines
while maintaining computational efficiency. We provide comparative assessments
of foundation models, demonstrating their differential impact on
trustworthiness in caption generation and OCR extraction-a vital consideration
for reliable enterprise AI. This work advances responsible AI deployment by
providing a rigorous framework for quantifying and enhancing trustworthiness in
multimodal RAG for critical enterprise applications.

</details>


### [45] [Bayesian-Guided Diversity in Sequential Sampling for Recommender Systems](https://arxiv.org/abs/2506.21617)
*Hiba Bederina,Jill-Jênn Vie*

Main category: cs.IR

TL;DR: 提出新框架平衡推荐系统用户相关性和内容多样性，实验表明能显著提升多样性且不牺牲相关性。


<details>
  <summary>Details</summary>
Motivation: 解决推荐系统中内容同质化和用户参与度降低问题，平衡用户相关性和内容多样性。

Method: 提出多目标、上下文顺序采样策略框架，用贝叶斯更新指导项目选择，奖励公式整合多种多样性指标及不确定性项，对批次内和批次间多样性建模，用基于优势的排序程序确定帕累托最优项目集。

Result: 在真实数据集实验中，该方法显著提高多样性且不牺牲相关性。

Conclusion: 该方法有潜力提升大规模推荐场景下的用户体验。

Abstract: The challenge of balancing user relevance and content diversity in
recommender systems is increasingly critical amid growing concerns about
content homogeneity and reduced user engagement. In this work, we propose a
novel framework that leverages a multi-objective, contextual sequential
sampling strategy. Item selection is guided by Bayesian updates that
dynamically adjust scores to optimize diversity. The reward formulation
integrates multiple diversity metrics-including the log-determinant volume of a
tuned similarity submatrix and ridge leverage scores-along with a diversity
gain uncertainty term to address the exploration-exploitation trade-off. Both
intra- and inter-batch diversity are modeled to promote serendipity and
minimize redundancy. A dominance-based ranking procedure identifies
Pareto-optimal item sets, enabling adaptive and balanced selections at each
iteration. Experiments on a real-world dataset show that our approach
significantly improves diversity without sacrificing relevance, demonstrating
its potential to enhance user experience in large-scale recommendation
settings.

</details>


### [46] [DCN^2: Interplay of Implicit Collision Weights and Explicit Cross Layers for Large-Scale Recommendation](https://arxiv.org/abs/2506.21624)
*Blaž Škrlj,Yonatan Karni,Grega Gašperšič,Blaž Mramor,Yulia Stolin,Martin Jakomin,Jasna Urbančič,Yuval Dishi,Natalia Silberstein,Ophir Friedler,Assaf Klein*

Main category: cs.IR

TL;DR: 本文对DCNv2架构提出三项改进形成DCN^2，在实际推荐系统和公开数据集上均表现优于DCNv2。


<details>
  <summary>Details</summary>
Motivation: 解决DCNv2存在的信息损失、碰撞管理和缺乏显式建模成对相似性等局限性。

Method: 对DCNv2架构提出三项算法改进，形成DCN^2架构。

Result: DCN^2在实际推荐系统中每秒处理超5亿次预测，离线和在线测试均优于DCNv2，在四个公开数据集上也表现出色。

Conclusion: 提出的三项改进有效提升了DCNv2架构性能，DCN^2具有更好的表现。

Abstract: The Deep and Cross architecture (DCNv2) is a robust production baseline and
is integral to numerous real-life recommender systems. Its inherent efficiency
and ability to model interactions often result in models that are both simpler
and highly competitive compared to more computationally demanding alternatives,
such as Deep FFMs. In this work, we introduce three significant algorithmic
improvements to the DCNv2 architecture, detailing their formulation and
behavior at scale. The enhanced architecture we refer to as DCN^2 is actively
used in a live recommender system, processing over 0.5 billion predictions per
second across diverse use cases where it out-performed DCNv2, both offline and
online (ab tests). These improvements effectively address key limitations
observed in the DCNv2, including information loss in Cross layers, implicit
management of collisions through learnable lookup-level weights, and explicit
modeling of pairwise similarities with a custom layer that emulates FFMs'
behavior. The superior performance of DCN^2 is also demonstrated on four
publicly available benchmark data sets.

</details>


### [47] [IRanker: Towards Ranking Foundation Model](https://arxiv.org/abs/2506.21638)
*Tao Feng,Zhigang Hua,Zijie Lei,Yan Xie,Shuang Yang,Bo Long,Jiaxuan You*

Main category: cs.IR

TL;DR: 提出IRanker框架统一排序任务，在多数据集表现佳，有泛化能力。


<details>
  <summary>Details</summary>
Motivation: 统一排序任务，解决排序任务无明确监督标签难题。

Method: 提出IRanker框架，用强化学习和迭代解码，将复杂任务分解为迭代解码过程。

Result: IRanker - 3B在多数据集达SOTA，超越同规模甚至更大模型，在零样本泛化实验表现好。

Conclusion: IRanker框架有效，能统一排序任务，有良好泛化性和提升零样本LLM性能潜力。

Abstract: Ranking tasks are ubiquitous, encompassing applications such as
recommendation systems, LLM routing, and item re-ranking. We propose to unify
these tasks using a single ranking foundation model (FM), as it eliminates the
need for designing different models for each specific ranking task. However,
unlike general supervision tasks in LLMs, ranking tasks do not have clear
labels for supervision, posing great challenges to developing a ranking FM. To
overcome these challenges, we propose IRanker, a ranking FM framework with
reinforcement learning (RL) and iterative decoding. Our insight is to decompose
the complex ranking task into an iterative decoding process that eliminates the
worst candidate from the candidate pool step by step, which significantly
reduces the output combinatorial space and better utilizes the limited context
length during RL training. We meticulously train and comprehensively evaluate
an IRanker-3B model on nine datasets across three scenarios: recommendation,
routing, and passage ranking. The results show that a single IRanker-3B
achieves state-of-the-art results on several datasets compared to models of
similar size, and even surpasses the performance of larger models on certain
datasets. We further demonstrate the effectiveness of our RL design and the
robustness of the iterative mechanism across different LLM sizes. Moreover, we
conducted both in-domain and out-of-domain zero-shot generalization
experiments, which showed that IRanker-3B achieved good generalization on
in-domain ranking tasks compared to the base LLM by at least 5% improvement.
Surprisingly, on out-of-domain generic LLM tasks, IRanker-3B outperformed the
base model by at least 9% on GSM8K, IFEval, and MathQA. In addition, the
thoughts generated by IRanker-3B during training could further enhance
zero-shot LLM performance.

</details>


### [48] [HyReC: Exploring Hybrid-based Retriever for Chinese](https://arxiv.org/abs/2506.21913)
*Zunran Wang,Zheng Shenpeng,Wang Shenglan,Minghui Zhao,Zhonghua Li*

Main category: cs.IR

TL;DR: 提出HyReC用于中文混合检索，通过整合术语语义、采用GLAE和NM模块提升性能并在C - MTEB基准测试评估。


<details>
  <summary>Details</summary>
Motivation: 混合检索方法虽能提升性能，但在中文检索场景应用研究不足。

Method: 提出HyReC方法，将术语语义联合融入表示模型，采用Global - Local - Aware Encoder促进语义共享，加入Normalization Module促进检索方法互利。

Result: 在C - MTEB检索基准上评估HyReC。

Conclusion: 文中未明确提及结论，但暗示HyReC有效。

Abstract: Hybrid-based retrieval methods, which unify dense-vector and lexicon-based
retrieval, have garnered considerable attention in the industry due to
performance enhancement. However, despite their promising results, the
application of these hybrid paradigms in Chinese retrieval contexts has
remained largely underexplored. In this paper, we introduce HyReC, an
innovative end-to-end optimization method tailored specifically for
hybrid-based retrieval in Chinese. HyReC enhances performance by integrating
the semantic union of terms into the representation model. Additionally, it
features the Global-Local-Aware Encoder (GLAE) to promote consistent semantic
sharing between lexicon-based and dense retrieval while minimizing the
interference between them. To further refine alignment, we incorporate a
Normalization Module (NM) that fosters mutual benefits between the retrieval
approaches. Finally, we evaluate HyReC on the C-MTEB retrieval benchmark to
demonstrate its effectiveness.

</details>


### [49] [ARAG: Agentic Retrieval Augmented Generation for Personalized Recommendation](https://arxiv.org/abs/2506.21931)
*Reza Yousefi Maragheh,Pratheek Vadla,Priyank Gupta,Kai Zhao,Aysenur Inan,Kehui Yao,Jianpeng Xu,Praveen Kanumala,Jason Cho,Sushant Kumar*

Main category: cs.IR

TL;DR: 本文提出用于个性化推荐的ARAG框架，结合多智能体协作机制，实验显示其显著优于基线方法，为基于大语言模型的个性化推荐提供新方向。


<details>
  <summary>Details</summary>
Motivation: 现有基于RAG的推荐系统依赖静态检索启发式方法，无法在动态场景中捕捉用户细微偏好。

Method: 引入ARAG框架，集成多智能体协作机制，利用四个基于大语言模型的智能体理解用户长期和会话行为。

Result: 在三个数据集上评估，ARAG显著优于标准RAG和基于近期性的基线，NDCG@5最高提升42.1%，Hit@5最高提升35.5%，并进行了消融实验。

Conclusion: 将智能体推理集成到检索增强推荐中是有效的，为基于大语言模型的个性化推荐提供新方向。

Abstract: Retrieval-Augmented Generation (RAG) has shown promise in enhancing
recommendation systems by incorporating external context into large language
model prompts. However, existing RAG-based approaches often rely on static
retrieval heuristics and fail to capture nuanced user preferences in dynamic
recommendation scenarios. In this work, we introduce ARAG, an Agentic
Retrieval-Augmented Generation framework for Personalized Recommendation, which
integrates a multi-agent collaboration mechanism into the RAG pipeline. To
better understand the long-term and session behavior of the user, ARAG
leverages four specialized LLM-based agents: a User Understanding Agent that
summarizes user preferences from long-term and session contexts, a Natural
Language Inference (NLI) Agent that evaluates semantic alignment between
candidate items retrieved by RAG and inferred intent, a context summary agent
that summarizes the findings of NLI agent, and an Item Ranker Agent that
generates a ranked list of recommendations based on contextual fit. We evaluate
ARAG accross three datasets. Experimental results demonstrate that ARAG
significantly outperforms standard RAG and recency-based baselines, achieving
up to 42.1% improvement in NDCG@5 and 35.5% in Hit@5. We also, conduct an
ablation study to analyse the effect by different components of ARAG. Our
findings highlight the effectiveness of integrating agentic reasoning into
retrieval-augmented recommendation and provide new directions for LLM-based
personalization.

</details>


### [50] [CAL-RAG: Retrieval-Augmented Multi-Agent Generation for Content-Aware Layout Design](https://arxiv.org/abs/2506.21934)
*Najmeh Forouzandehmehr,Reza Yousefi Maragheh,Sriram Kollipara,Kai Zhao,Topojoy Biswas,Evren Korpeoglu,Kannan Achan*

Main category: cs.IR

TL;DR: 本文提出CAL - RAG框架用于自动内容感知布局生成，在PKU PosterLayout数据集上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有自动内容感知布局生成方法缺乏上下文设计范例支撑，在语义对齐和视觉连贯性处理上不足。

Method: 引入CAL - RAG框架，集成多模态检索、大语言模型和协作式智能体推理，从知识库检索相关布局示例，用LLM推荐布局，视觉语言评分智能体评估，反馈智能体提供改进。

Result: 在PKU PosterLayout数据集上，CAL - RAG在多个布局指标上达到SOTA，大幅超越LayoutPrompter等基线模型。

Conclusion: 将检索增强与智能体多步推理结合，能为自动布局生成提供可扩展、可解释和高保真的解决方案。

Abstract: Automated content-aware layout generation -- the task of arranging visual
elements such as text, logos, and underlays on a background canvas -- remains a
fundamental yet under-explored problem in intelligent design systems. While
recent advances in deep generative models and large language models (LLMs) have
shown promise in structured content generation, most existing approaches lack
grounding in contextual design exemplars and fall short in handling semantic
alignment and visual coherence. In this work we introduce CAL-RAG, a
retrieval-augmented, agentic framework for content-aware layout generation that
integrates multimodal retrieval, large language models, and collaborative
agentic reasoning. Our system retrieves relevant layout examples from a
structured knowledge base and invokes an LLM-based layout recommender to
propose structured element placements. A vision-language grader agent evaluates
the layout with visual metrics, and a feedback agent provides targeted
refinements, enabling iterative improvement. We implement our framework using
LangGraph and evaluate it on the PKU PosterLayout dataset, a benchmark rich in
semantic and structural variability. CAL-RAG achieves state-of-the-art
performance across multiple layout metrics -- including underlay effectiveness,
element alignment, and overlap -- substantially outperforming strong baselines
such as LayoutPrompter. These results demonstrate that combining retrieval
augmentation with agentic multi-step reasoning yields a scalable,
interpretable, and high-fidelity solution for automated layout generation.

</details>


### [51] [Literature-Grounded Novelty Assessment of Scientific Ideas](https://arxiv.org/abs/2506.22026)
*Simra Shahid,Marissa Radensky,Raymond Fok,Pao Siangliulue,Daniel S. Weld,Tom Hope*

Main category: cs.IR

TL;DR: 提出基于LLM的检索增强生成框架Idea Novelty Checker评估想法新颖性，实验表明比现有方法一致性高约13%，消融实验凸显分面重排器重要性。


<details>
  <summary>Details</summary>
Motivation: 自动评估想法新颖性是关键且待探索的挑战，人工评估劳动密集、易主观出错且难以大规模应用。

Method: 提出Idea Novelty Checker，采用两阶段先检索后重排方法，先基于关键词和片段检索相关论文，再通过嵌入过滤和分面重排优化，结合专家标注示例。

Result: 新颖性检查器比现有方法达成约13%更高的一致性，消融实验显示分面重排器对识别评估新颖性的相关文献很重要。

Conclusion: Idea Novelty Checker在自动评估想法新颖性方面表现良好，分面重排器对提高性能有重要作用。

Abstract: Automated scientific idea generation systems have made remarkable progress,
yet the automatic evaluation of idea novelty remains a critical and
underexplored challenge. Manual evaluation of novelty through literature review
is labor-intensive, prone to error due to subjectivity, and impractical at
scale. To address these issues, we propose the Idea Novelty Checker, an
LLM-based retrieval-augmented generation (RAG) framework that leverages a
two-stage retrieve-then-rerank approach. The Idea Novelty Checker first
collects a broad set of relevant papers using keyword and snippet-based
retrieval, then refines this collection through embedding-based filtering
followed by facet-based LLM re-ranking. It incorporates expert-labeled examples
to guide the system in comparing papers for novelty evaluation and in
generating literature-grounded reasoning. Our extensive experiments demonstrate
that our novelty checker achieves approximately 13% higher agreement than
existing approaches. Ablation studies further showcases the importance of the
facet-based re-ranker in identifying the most relevant literature for novelty
evaluation.

</details>


### [52] [Reward Balancing Revisited: Enhancing Offline Reinforcement Learning for Recommender Systems](https://arxiv.org/abs/2506.22112)
*Wenzheng Shu,Yanxiang Zeng,Yongxiang Tang,Teng Sha,Ning Luo,Yanhua Cheng,Xialong Liu,Fan Zhou,Peng Jiang*

Main category: cs.IR

TL;DR: 提出离线强化学习框架R3S，解决奖励塑造难题，实验显示其能提升世界模型准确性并协调用户偏好。


<details>
  <summary>Details</summary>
Motivation: 现有离线强化学习在奖励塑造时难以同时平衡世界模型固有偏差和策略推荐多样性。

Method: 提出R3S框架，整合模型固有不确定性处理奖励预测波动，加入带衰减的额外惩罚项。

Result: R3S提升了世界模型的准确性，能有效协调用户的异质偏好。

Conclusion: R3S框架能有效解决离线强化学习奖励塑造中存在的问题。

Abstract: Offline reinforcement learning (RL) has emerged as a prevalent and effective
methodology for real-world recommender systems, enabling learning policies from
historical data and capturing user preferences. In offline RL, reward shaping
encounters significant challenges, with past efforts to incorporate prior
strategies for uncertainty to improve world models or penalize underexplored
state-action pairs. Despite these efforts, a critical gap remains: the
simultaneous balancing of intrinsic biases in world models and the diversity of
policy recommendations. To address this limitation, we present an innovative
offline RL framework termed Reallocated Reward for Recommender Systems (R3S).
By integrating inherent model uncertainty to tackle the intrinsic fluctuations
in reward predictions, we boost diversity for decision-making to align with a
more interactive paradigm, incorporating extra penalizers with decay that deter
actions leading to diminished state variety at both local and global scales.
The experimental results demonstrate that R3S improves the accuracy of world
models and efficiently harmonizes the heterogeneous preferences of the users.

</details>


### [53] [UiS-IAI@LiveRAG: Retrieval-Augmented Information Nugget-Based Generation of Responses](https://arxiv.org/abs/2506.22210)
*Weronika Łajewska,Ivica Kostric,Gabriel Iturra-Bocaz,Mariam Arustashvili,Krisztian Balog*

Main category: cs.IR

TL;DR: 本文针对RAG面临的问题，参加LiveRAG Challenge，提出模块化管道方法，实验显示结合原查询和子查询可提升召回率，增加文档数量超一定程度会降低效果。


<details>
  <summary>Details</summary>
Motivation: 解决检索增强生成（RAG）在事实正确性、来源归因和响应完整性方面的挑战，通过LiveRAG Challenge推动RAG研究。

Method: 提出基于信息块的模块化管道，包括查询重写、段落检索和重排序等多阶段步骤，还在检索组件和增强生成方面改进。

Result: 结合原查询和少量子查询重写可提升召回率，增加用于重排序和生成的文档数量超过一定程度会降低效果且不提高响应质量。

Conclusion: 所提出的方法在一定程度上能解决RAG的问题，但在文档数量使用上需把握合适程度。

Abstract: Retrieval-augmented generation (RAG) faces challenges related to factual
correctness, source attribution, and response completeness. The LiveRAG
Challenge hosted at SIGIR'25 aims to advance RAG research using a fixed corpus
and a shared, open-source LLM. We propose a modular pipeline that operates on
information nuggets-minimal, atomic units of relevant information extracted
from retrieved documents. This multistage pipeline encompasses query rewriting,
passage retrieval and reranking, nugget detection and clustering, cluster
ranking and summarization, and response fluency enhancement. This design
inherently promotes grounding in specific facts, facilitates source
attribution, and ensures maximum information inclusion within length
constraints. In this challenge, we extend our focus to also address the
retrieval component of RAG, building upon our prior work on multi-faceted query
rewriting. Furthermore, for augmented generation, we concentrate on improving
context curation capabilities, maximizing the breadth of information covered in
the response while ensuring pipeline efficiency. Our results show that
combining original queries with a few sub-query rewrites boosts recall, while
increasing the number of documents used for reranking and generation beyond a
certain point reduces effectiveness, without improving response quality.

</details>


### [54] [JointRank: Rank Large Set with Single Pass](https://arxiv.org/abs/2506.22262)
*Evgeny Dedov*

Main category: cs.IR

TL;DR: 提出一种模型无关的方法用于快速重排超模型输入限制的大集合，在TREC DL - 2019实验中表现良好，降低了延迟并提高了nDCG@10。


<details>
  <summary>Details</summary>
Motivation: 现有列表式重排器在处理大集合时受模型输入大小限制或质量下降的问题，需要高效的重排大集合的方法。

Method: 先将候选项目划分为重叠块并并行独立排名，从局部排名推导隐式成对比较，最后使用Winrate或PageRank等算法聚合比较结果构建全局排名。

Result: 在TREC DL - 2019实验中，该方法nDCG@10达70.88，而全上下文列表式方法为57.68，同时将延迟从21秒降至8秒。

Conclusion: 提出的模型无关方法能有效快速重排超模型输入限制的大集合，具有更好的性能和更低的延迟。

Abstract: Efficiently ranking relevant items from large candidate pools is a
cornerstone of modern information retrieval systems -- such as web search,
recommendation, and retrieval-augmented generation. Listwise rerankers, which
improve relevance by jointly considering multiple candidates, are often limited
in practice: either by model input size constraints, or by degraded quality
when processing large sets. We propose a model-agnostic method for fast
reranking large sets that exceed a model input limits. The method first
partitions candidate items into overlapping blocks, each of which is ranked
independently in parallel. Implicit pairwise comparisons are then derived from
these local rankings. Finally, these comparisons are aggregated to construct a
global ranking using algorithms such as Winrate or PageRank. Experiments on
TREC DL-2019 show that our method achieves an nDCG@10 of 70.88 compared to the
57.68 for full-context listwise approach using gpt-4.1-mini as long-context
model, while reducing latency from 21 to 8 seconds.
  The implementation of the algorithm and the experiments is available in the
repository: https://github.com/V3RGANz/jointrank

</details>


### [55] [Education-Oriented Graph Retrieval-Augmented Generation for Learning Path Recommendation](https://arxiv.org/abs/2506.22303)
*Xinghe Cheng,Zihan Zhang,Jiapu Wang,Liangda Fang,Chaobo He,Quanlong Guan,Shirui Pan,Weiqi Luo*

Main category: cs.IR

TL;DR: 提出DLELP方法改进学习路径推荐，实验显示性能达SOTA且有可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有学习路径推荐方法多依赖先修关系，存在教育数据集无明确先修关系和单靠先修关系影响学习效果的问题。

Method: 提出DLELP方法，包含知识概念结构图生成模块和DLRL框架。

Result: 在三个基准数据集上实验，方法达到SOTA性能，能为推荐学习路径提供可解释推理。

Conclusion: 所提方法能有效提升学习路径推荐的性能和可解释性。

Abstract: Learning path recommendation seeks to provide learners with a structured
sequence of learning items (e.g., knowledge concepts or exercises) to optimize
their learning efficiency. Despite significant efforts in this area, most
existing methods primarily rely on prerequisite relationships, which present
two major limitations: 1) Many educational datasets do not explicitly provide
prerequisite relationships between knowledge concepts, hindering the
application of current learning path recommendation methods. 2) Relying solely
on prerequisite relationships as the sole knowledge structure can impede
learning progress and negatively impact student outcomes. To address these
challenges, we propose a novel approach, Discrimination Learning Enhances
Learning Path Recommendation (DLELP), which enhances learning path
recommendations by incorporating both prerequisite and similarity relationships
between knowledge concepts. Specifically, we introduce a knowledge concept
structure graph generation module that adaptively constructs knowledge concept
structure graphs for different educational datasets, significantly improving
the generalizability of learning path recommendation methods. We then propose a
Discrimination Learning-driven Reinforcement Learning (DLRL) framework, which
mitigates the issue of blocked learning paths, further enhancing the efficacy
of learning path recommendations. Finally, we conduct extensive experiments on
three benchmark datasets, demonstrating that our method not only achieves
state-of-the-art performance but also provides interpretable reasoning for the
recommended learning paths.

</details>


### [56] [HLTCOE at LiveRAG: GPT-Researcher using ColBERT retrieval](https://arxiv.org/abs/2506.22356)
*Kevin Duh,Eugene Yang,Orion Weller,Andrew Yates,Dawn Lawrie*

Main category: cs.IR

TL;DR: HLTCOE LiveRAG提交使用GPT - researcher框架，结合多种模型和架构完成问答，在自动评估中获第5名，得分1.07。


<details>
  <summary>Details</summary>
Motivation: 未提及

Method: 采用GPT - researcher框架，检索系统用ColBERT双编码器架构和PLAID - X创建的本地压缩索引，用Qwen2.5 - 7B - Instruct生成查询，m2 - bert - 80M - 8k - retrieval过滤，Falcon3 - 10B生成答案。

Result: 系统在LiveRAG自动评估的正确性方面排名第5，得分1.07。

Conclusion: 未提及

Abstract: The HLTCOE LiveRAG submission utilized the GPT-researcher framework for
researching the context of the question, filtering the returned results, and
generating the final answer. The retrieval system was a ColBERT bi-encoder
architecture, which represents a passage with many dense tokens. Retrieval used
a local, compressed index of the FineWeb10-BT collection created with PLAID-X,
using a model fine-tuned for multilingual retrieval. Query generation from
context was done with Qwen2.5-7B-Instruct, while filtering was accomplished
with m2-bert-80M-8k-retrieval. Up to nine passages were used as context to
generate an answer using Falcon3-10B. This system placed 5th in the LiveRAG
automatic evaluation for correctness with a score of 1.07.

</details>


### [57] [Towards Fair Rankings: Leveraging LLMs for Gender Bias Detection and Measurement](https://arxiv.org/abs/2506.22372)
*Maryam Mousavian,Zahra Abbasiantaeb,Mohammad Aliannejadi,Fabio Crestani*

Main category: cs.IR

TL;DR: 本文利用大语言模型检测和衡量段落排序中的性别偏差，提出新的性别公平指标CWEx，标注MS MARCO Passage Ranking子集并发布MSMGenderBias，实验表明该指标评估公平性更优，为缓解信息检索系统中的偏差提供框架。


<details>
  <summary>Details</summary>
Motivation: 自然语言处理和信息检索系统存在社会偏差问题，现有性别公平指标有局限性，需开发识别和评估偏差的有效方法。

Method: 基于大语言模型的性别偏差检测方法，提出新指标CWEx，标注MS MARCO Passage Ranking子集并发布新数据集MSMGenderBias。

Result: 所提出的指标相比以往指标能更详细地评估公平性，与人类标注的一致性提高（Grep - BiasIR为58.77%，MSMGenderBias为18.51%），能有效区分排序中的性别偏差。

Conclusion: 通过整合大语言模型驱动的偏差检测、改进的公平指标和数据集的性别偏差标注，为分析和缓解信息检索系统中的偏差提供了更稳健的框架。

Abstract: The presence of social biases in Natural Language Processing (NLP) and
Information Retrieval (IR) systems is an ongoing challenge, which underlines
the importance of developing robust approaches to identifying and evaluating
such biases. In this paper, we aim to address this issue by leveraging Large
Language Models (LLMs) to detect and measure gender bias in passage ranking.
Existing gender fairness metrics rely on lexical- and frequency-based measures,
leading to various limitations, e.g., missing subtle gender disparities.
Building on our LLM-based gender bias detection method, we introduce a novel
gender fairness metric, named Class-wise Weighted Exposure (CWEx), aiming to
address existing limitations. To measure the effectiveness of our proposed
metric and study LLMs' effectiveness in detecting gender bias, we annotate a
subset of the MS MARCO Passage Ranking collection and release our new gender
bias collection, called MSMGenderBias, to foster future research in this area.
Our extensive experimental results on various ranking models show that our
proposed metric offers a more detailed evaluation of fairness compared to
previous metrics, with improved alignment to human labels (58.77% for
Grep-BiasIR, and 18.51% for MSMGenderBias, measured using Cohen's Kappa
agreement), effectively distinguishing gender bias in ranking. By integrating
LLM-driven bias detection, an improved fairness metric, and gender bias
annotations for an established dataset, this work provides a more robust
framework for analyzing and mitigating bias in IR systems.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [58] [REDELEX: A Framework for Relational Deep Learning Exploration](https://arxiv.org/abs/2506.22199)
*Jakub Peleška,Gustav Šír*

Main category: cs.LG

TL;DR: 提出REDELEX框架在70多个关系数据库上评估不同复杂度的关系深度学习（RDL）模型，确认RDL性能优越并分析影响性能因素。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对各种RDL模型性能与底层关系数据库特征之间关系的分析。

Method: 提出REDELEX框架，在70多个不同的关系数据库上评估不同复杂度的RDL模型，并与经典方法的关键代表进行基准测试。

Result: 确认了RDL通常具有更优越的性能。

Conclusion: 明确了影响RDL性能的主要因素，包括模型复杂度、数据库大小及其结构属性。

Abstract: Relational databases (RDBs) are widely regarded as the gold standard for
storing structured information. Consequently, predictive tasks leveraging this
data format hold significant application promise. Recently, Relational Deep
Learning (RDL) has emerged as a novel paradigm wherein RDBs are conceptualized
as graph structures, enabling the application of various graph neural
architectures to effectively address these tasks. However, given its novelty,
there is a lack of analysis into the relationships between the performance of
various RDL models and the characteristics of the underlying RDBs.
  In this study, we present REDELEX$-$a comprehensive exploration framework for
evaluating RDL models of varying complexity on the most diverse collection of
over 70 RDBs, which we make available to the community. Benchmarked alongside
key representatives of classic methods, we confirm the generally superior
performance of RDL while providing insights into the main factors shaping
performance, including model complexity, database sizes and their structural
properties.

</details>


### [59] [APO: Enhancing Reasoning Ability of MLLMs via Asymmetric Policy Optimization](https://arxiv.org/abs/2506.21655)
*Minjie Hong,Zirun Guo,Yan Xia,Zehan Wang,Ziang Zhang,Tao Jin,Zhou Zhao*

Main category: cs.LG

TL;DR: 研究KL惩罚和过度思考对MLLMs中RL训练的影响，提出APO方法，含DADS和STCR技术，应用于Qwen2.5 - VL - 3B创建View - R1 - 3B，提升推理能力和泛化性。


<details>
  <summary>Details</summary>
Motivation: MLLMs在复杂推理上存在困难，RL应用于MLLMs有常见问题，如通用任务性能下降和过度思考。

Method: 提出Asymmetric Policy Optimization (APO)，对正样本采用Difficulty - Adaptive Divergence Shaping (DADS)调整KL散度权重，对负样本采用Suboptimal Trajectory Complexity Regularization (STCR)惩罚过长回复。

Result: View - R1 - 3B推理能力显著提升，平均比基础模型高7%，在推理基准上优于更大的MLLMs，且在通用任务上保持一致提升。

Conclusion: DADS和STCR技术对推进MLLMs的复杂多模态推理有效且具有广泛适用性。

Abstract: Multimodal Large Language Models (MLLMs) are powerful at integrating diverse
data, but they often struggle with complex reasoning. While Reinforcement
learning (RL) can boost reasoning in LLMs, applying it to MLLMs is tricky.
Common issues include a drop in performance on general tasks and the generation
of overly detailed or "overthinking" reasoning. Our work investigates how the
KL penalty and overthinking affect RL training in MLLMs. We propose Asymmetric
Policy Optimization (APO) to address these issues, which divides the sampled
responses into positive and negative groups. For positive samples,
Difficulty-Adaptive Divergence Shaping (DADS) is introduced to dynamically
adjust the KL divergence weight based on their difficulty. This method prevents
policy entropy from dropping sharply, improves training stability, utilizes
samples better, and preserves the model's existing knowledge. For negative
samples, Suboptimal Trajectory Complexity Regularization (STCR) is proposed to
penalize overly long responses. This helps mitigate overthinking and encourages
more concise reasoning while preserving the model's explorative capacity. We
apply our method to Qwen2.5-VL-3B, creating View-R1-3B. View-R1-3B
significantly enhances reasoning capabilities, showing an average 7\% gain over
the base model and outperforming larger MLLMs (7-11B) on various reasoning
benchmarks. Importantly, unlike other reasoning-tuned MLLMs that often degrade
on general tasks, View-R1-3B maintains consistent improvement, demonstrating
superior generalization. These results highlight the effectiveness and broad
applicability of our DADS and STCR techniques for advancing complex multimodal
reasoning in MLLMs. The code will be made available at
https://github.com/Indolent-Kawhi/View-R1.

</details>


### [60] [Risk-Averse Total-Reward Reinforcement Learning](https://arxiv.org/abs/2506.21683)
*Xihong Su,Jia Lin Hau,Gersi Doko,Kishan Panaganti,Marek Petrik*

Main category: cs.LG

TL;DR: 提出用于总奖励ERM和EVaR目标的Q学习算法，有收敛和性能保证，数值结果显示算法能快速可靠收敛。


<details>
  <summary>Details</summary>
Motivation: 现有基于模型的风险度量算法在小问题有效但需全量转移概率信息，需改进。

Method: 提出Q学习算法，利用ERM的动态一致性和可引出性保证算法及其最优性。

Result: 在表格域的数值结果表明所提Q学习算法能快速可靠收敛到最优风险规避值函数。

Conclusion: 所提Q学习算法可用于计算总奖励ERM和EVaR目标的最优平稳策略，且有良好收敛和性能。

Abstract: Risk-averse total-reward Markov Decision Processes (MDPs) offer a promising
framework for modeling and solving undiscounted infinite-horizon objectives.
Existing model-based algorithms for risk measures like the entropic risk
measure (ERM) and entropic value-at-risk (EVaR) are effective in small
problems, but require full access to transition probabilities. We propose a
Q-learning algorithm to compute the optimal stationary policy for total-reward
ERM and EVaR objectives with strong convergence and performance guarantees. The
algorithm and its optimality are made possible by ERM's dynamic consistency and
elicitability. Our numerical results on tabular domains demonstrate quick and
reliable convergence of the proposed Q-learning algorithm to the optimal
risk-averse value function.

</details>


### [61] [Unimodal Strategies in Density-Based Clustering](https://arxiv.org/abs/2506.21695)
*Oron Nir,Jay Tenenbaum,Ariel Shamir*

Main category: cs.LG

TL;DR: 本文揭示基于密度的聚类方法中簇数量与核心点邻域半径关系近乎单峰，基于此用三分查找算法设计高效找半径策略，经多领域验证有效。


<details>
  <summary>Details</summary>
Motivation: 基于密度的聚类方法在处理含噪声或任意分布数据时表现更好，但高维大规模数据的参数调优计算量大，需更高效找参数方法。

Method: 揭示簇数量与核心点邻域半径关系近乎单峰并理论支持，基于三分查找算法设计找半径策略。

Result: 通过高维大规模的NLP、音频和计算机视觉任务验证了方法的实用性和鲁棒性。

Conclusion: 该工作推进了基于密度的聚类的参数控制，拓宽了对其引导参数关系的理解。

Abstract: Density-based clustering methods often surpass centroid-based counterparts,
when addressing data with noise or arbitrary data distributions common in
real-world problems. In this study, we reveal a key property intrinsic to
density-based clustering methods regarding the relation between the number of
clusters and the neighborhood radius of core points - we empirically show that
it is nearly unimodal, and support this claim theoretically in a specific
setting. We leverage this property to devise new strategies for finding
appropriate values for the radius more efficiently based on the Ternary Search
algorithm. This is especially important for large scale data that is
high-dimensional, where parameter tuning is computationally intensive. We
validate our methodology through extensive applications across a range of
high-dimensional, large-scale NLP, Audio, and Computer Vision tasks,
demonstrating its practical effectiveness and robustness. This work not only
offers a significant advancement in parameter control for density-based
clustering but also broadens the understanding regarding the relations between
their guiding parameters. Our code is available at
https://github.com/oronnir/UnimodalStrategies.

</details>


### [62] [$\textrm{ODE}_t \left(\textrm{ODE}_l \right)$: Shortcutting the Time and Length in Diffusion and Flow Models for Faster Sampling](https://arxiv.org/abs/2506.21714)
*Denis Gudovskiy,Wenzhao Zheng,Tomoyuki Okuno,Yohei Nakata,Kurt Keutzer*

Main category: cs.LG

TL;DR: 本文探索动态控制CNFs和DMs质量 - 复杂度权衡的新方向，提出ODE_t(ODE_l)方法，降低延迟和内存使用，在图像生成实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有CNFs和DMs采样需多次迭代求解ODE，计算复杂度高，多数方法关注减少时间步，本文探索新的控制质量 - 复杂度权衡的方向。

Method: 通过重新连接基于Transformer架构的块求解内部离散ODE，在流匹配训练中使用时间和长度一致性项，实现任意时间步和Transformer块的采样。

Result: 在CelebA - HQ和ImageNet图像生成实验中，最有效采样模式下延迟最多降低3倍，高质量采样时FID分数最多提高3.5分。

Conclusion: 提出的ODE_t(ODE_l)方法求解器无关，能降低延迟和内存使用，有较好效果，代码和模型权重已开源。

Abstract: Recently, continuous normalizing flows (CNFs) and diffusion models (DMs) have
been studied using the unified theoretical framework. Although such models can
generate high-quality data points from a noise distribution, the sampling
demands multiple iterations to solve an ordinary differential equation (ODE)
with high computational complexity. Most existing methods focus on reducing the
number of time steps during the sampling process to improve efficiency. In this
work, we explore a complementary direction in which the quality-complexity
tradeoff can be dynamically controlled in terms of time steps and in the length
of the neural network. We achieve this by rewiring the blocks in the
transformer-based architecture to solve an inner discretized ODE w.r.t. its
length. Then, we employ time- and length-wise consistency terms during flow
matching training, and as a result, the sampling can be performed with an
arbitrary number of time steps and transformer blocks. Unlike others, our
$\textrm{ODE}_t \left(\textrm{ODE}_l \right)$ approach is solver-agnostic in
time dimension and decreases both latency and memory usage. Compared to the
previous state of the art, image generation experiments on CelebA-HQ and
ImageNet show a latency reduction of up to $3\times$ in the most efficient
sampling mode, and a FID score improvement of up to $3.5$ points for
high-quality sampling. We release our code and model weights with fully
reproducible experiments.

</details>


### [63] [Performance Prediction for Large Systems via Text-to-Text Regression](https://arxiv.org/abs/2506.21718)
*Yash Akhauri,Bryan Lewandowski,Cheng-Hsi Lin,Adrian N. Reyes,Grant C. Forbes,Arissa Wongpanich,Bangding Yang,Mohamed S. Abdelfattah,Sagi Perel,Xingyou Song*

Main category: cs.LG

TL;DR: 提出文本到文本回归作为传统表格回归的替代方案，在预测Borg资源效率上表现出色，为通用模拟器研究奠定基础。


<details>
  <summary>Details</summary>
Motivation: 传统表格回归方法在处理复杂系统数据时面临困难，特征工程不可行，需要新的预测方法。

Method: 提出文本到文本回归方法，使用6000万参数的编码器 - 解码器模型，从随机初始化开始训练。

Result: 在预测Borg资源效率上，模型达到近完美的0.99（平均0.9）的秩相关性，均方误差比表格方法低100倍，能适应新任务，捕捉复杂结果分布密度。

Conclusion: 消融研究凸显编码器、序列长度和模型不确定性量化的重要性，为真实世界结果的通用模拟器铺平道路。

Abstract: In many industries, predicting metric outcomes of large systems is a
fundamental problem, driven largely by traditional tabular regression. However,
such methods struggle on complex systems data in the wild such as configuration
files or system logs, where feature engineering is often infeasible. We propose
text-to-text regression as a general, scalable alternative. For predicting
resource efficiency on Borg, Google's massive compute cluster scheduling
system, a 60M parameter encoder-decoder, trained from random initialization,
achieves up to a near perfect 0.99 (0.9 average) rank correlation across the
entire fleet, and 100x lower MSE than tabular approaches. The model also easily
adapts to new tasks in only 500 few-shot examples and captures the densities of
complex outcome distributions. Ablation studies highlight the importance of
using encoders, increasing sequence length, and the model's inherent
uncertainty quantification. These findings pave the way for universal
simulators of real-world outcomes.

</details>


### [64] [Federated Item Response Theory Models](https://arxiv.org/abs/2506.21744)
*Biying Zhou,Nanyu Luo,Feng Ji*

Main category: cs.LG

TL;DR: 提出Federated Item Response Theory (FedIRT)框架，结合联邦学习与心理测量学，实验证明其有隐私保护、降低通信成本等优势，还提供开源R包。


<details>
  <summary>Details</summary>
Motivation: 传统IRT估计需集中原始响应数据，有隐私问题，为结合联邦学习进展与现代心理测量学，提出新框架。

Method: 提出Federated Item Response Theory (FedIRT)框架，以分布式方式估计传统IRT模型。

Result: 数值实验表明FedIRT统计准确性与使用流行R包的标准IRT估计相似，通过真实考试数据集验证其有效性。

Conclusion: 新框架将IRT应用扩展到分布式场景，不牺牲准确性和安全性，还提供开源R包支持实际应用。

Abstract: Item Response Theory (IRT) models have been widely used to estimate
respondents' latent abilities and calibrate items' difficulty. Traditional IRT
estimation requires all individual raw response data to be centralized in one
place, thus potentially causing privacy issues. Federated learning is an
emerging field in computer science and machine learning with added features of
privacy protection and distributed computing. To integrate the advances from
federated learning with modern psychometrics, we propose a novel framework,
Federated Item Response Theory (IRT), to enable estimating traditional IRT
models with additional privacy, allowing estimation in a distributed manner
without losing estimation accuracy.
  Our numerical experiments confirm that FedIRT achieves statistical accuracy
similar to standard IRT estimation using popular R packages, while offering
critical advantages: privacy protection and reduced communication costs. We
also validate FedIRT's utility through a real-world exam dataset, demonstrating
its effectiveness in realistic educational contexts. This new framework extends
IRT's applicability to distributed settings, such as multi-school assessments,
without sacrificing accuracy or security. To support practical adoption, we
provide an open-ource R package, FedIRT, implementing the framework for the
two-parameter logistic (2PL) and partial credit models (PCM).

</details>


### [65] [Gradient-Based Neuroplastic Adaptation for Concurrent Optimization of Neuro-Fuzzy Networks](https://arxiv.org/abs/2506.21771)
*John Wesley Hostetter,Min Chi*

Main category: cs.LG

TL;DR: 提出基于梯度的神经可塑性自适应方法同时优化神经模糊网络（NFNs）的参数和结构，并通过基于视觉的DOOM游戏验证有效性。


<details>
  <summary>Details</summary>
Motivation: 现有NFNs设计过程常将参数和结构识别分离，导致过早确定脆弱且性能不佳的架构，其系统设计过程仍是挑战。

Method: 提出基于梯度的神经可塑性自适应方法，同时优化NFNs的参数和结构。

Result: 通过在线强化学习训练NFNs，使其能在基于视觉的DOOM游戏中熟练应对挑战性场景。

Conclusion: 同时优化NFNs的参数和结构是有效的，能使NFNs应用于之前难以处理的场景。

Abstract: Neuro-fuzzy networks (NFNs) are transparent, symbolic, and universal function
approximations that perform as well as conventional neural architectures, but
their knowledge is expressed as linguistic IF-THEN rules. Despite these
advantages, their systematic design process remains a challenge. Existing work
will often sequentially build NFNs by inefficiently isolating parametric and
structural identification, leading to a premature commitment to brittle and
subpar architecture. We propose a novel application-independent approach called
gradient-based neuroplastic adaptation for the concurrent optimization of NFNs'
parameters and structure. By recognizing that NFNs' parameters and structure
should be optimized simultaneously as they are deeply conjoined, settings
previously unapproachable for NFNs are now accessible, such as the online
reinforcement learning of NFNs for vision-based tasks. The effectiveness of
concurrently optimizing NFNs is empirically shown as it is trained by online
reinforcement learning to proficiently play challenging scenarios from a
vision-based video game called DOOM.

</details>


### [66] [M3PO: Massively Multi-Task Model-Based Policy Optimization](https://arxiv.org/abs/2506.21782)
*Aditya Narendra,Dmitry Makarov,Aleksandr Panov*

Main category: cs.LG

TL;DR: 介绍M3PO框架，可解决单任务样本效率低和多任务泛化差问题，性能达SOTA。


<details>
  <summary>Details</summary>
Motivation: 解决单任务样本低效和多任务泛化差问题，现有基于模型方法忽略控制中心表示，无模型方法样本复杂度高、探索能力弱。

Method: 集成隐式世界模型，采用结合基于模型规划和无模型不确定性奖励的混合探索策略，利用基于模型和无模型价值估计差异指导探索，通过信任区域优化器维持策略更新稳定。

Result: M3PO在多个基准测试中取得了最先进的性能。

Conclusion: M3PO为现有的基于模型的策略优化方法提供了一种高效且稳健的替代方案。

Abstract: We introduce Massively Multi-Task Model-Based Policy Optimization (M3PO), a
scalable model-based reinforcement learning (MBRL) framework designed to
address sample inefficiency in single-task settings and poor generalization in
multi-task domains. Existing model-based approaches like DreamerV3 rely on
pixel-level generative models that neglect control-centric representations,
while model-free methods such as PPO suffer from high sample complexity and
weak exploration. M3PO integrates an implicit world model, trained to predict
task outcomes without observation reconstruction, with a hybrid exploration
strategy that combines model-based planning and model-free uncertainty-driven
bonuses. This eliminates the bias-variance trade-off in prior methods by using
discrepancies between model-based and model-free value estimates to guide
exploration, while maintaining stable policy updates through a trust-region
optimizer. M3PO provides an efficient and robust alternative to existing
model-based policy optimization approaches and achieves state-of-the-art
performance across multiple benchmarks.

</details>


### [67] [Multi-task parallelism for robust pre-training of graph foundation models on multi-source, multi-fidelity atomistic modeling data](https://arxiv.org/abs/2506.21788)
*Massimiliano Lupo Pasini,Jong Youl Choi,Pei Zhang,Kshitij Mehta,Rylie Weaver,Ashwin M. Aji,Karl W. Schulz,Jorda Polo,Prasanna Balaprakash*

Main category: cs.LG

TL;DR: 本文提出多任务并行方法，在超算上实现高效扩展，用于基于图神经网络的图基础模型原子建模。


<details>
  <summary>Details</summary>
Motivation: 解决预训练中处理多源、多保真度数据的挑战，以及提升模型在更大、更多样数据集的泛化性和超算上的可扩展性。

Method: 提出多任务并行方法，将每个解码头分布到有GPU加速的计算资源上，在开源的HydraGNN架构中实现。

Result: 在五个数据集的超2400万个结构上训练，并在Perlmutter、Aurora和Frontier超算上测试，在三种高度异构的超算架构上都实现了高效扩展。

Conclusion: 该多任务并行方法能有效提升基于图神经网络的图基础模型在超算上的可扩展性。

Abstract: Graph foundation models using graph neural networks promise sustainable,
efficient atomistic modeling. To tackle challenges of processing multi-source,
multi-fidelity data during pre-training, recent studies employ multi-task
learning, in which shared message passing layers initially process input
atomistic structures regardless of source, then route them to multiple decoding
heads that predict data-specific outputs. This approach stabilizes pre-training
and enhances a model's transferability to unexplored chemical regions.
Preliminary results on approximately four million structures are encouraging,
yet questions remain about generalizability to larger, more diverse datasets
and scalability on supercomputers. We propose a multi-task parallelism method
that distributes each head across computing resources with GPU acceleration.
Implemented in the open-source HydraGNN architecture, our method was trained on
over 24 million structures from five datasets and tested on the Perlmutter,
Aurora, and Frontier supercomputers, demonstrating efficient scaling on all
three highly heterogeneous super-computing architectures.

</details>


### [68] [Why Neural Network Can Discover Symbolic Structures with Gradient-based Training: An Algebraic and Geometric Foundation for Neurosymbolic Reasoning](https://arxiv.org/abs/2506.21797)
*Peihao Wang,Zhangyang Wang*

Main category: cs.LG

TL;DR: 提出理论框架解释离散符号结构如何从连续神经网络训练动态中自然出现，建立数据缩放定律，为神经符号系统提供基础。


<details>
  <summary>Details</summary>
Motivation: 解释离散符号结构从连续神经网络训练动态中自然出现的原理，为神经符号系统奠定基础。

Method: 将神经参数提升到测度空间，将训练建模为Wasserstein梯度流，在几何约束下分析参数测度的变化。

Result: 参数测度会出现梯度流解耦和自由度收缩现象，网络过渡到符合代数运算的组合表示，建立了数据缩放定律。

Conclusion: 该框架为理解和设计集成连续学习与离散代数推理的神经符号系统提供了原则性基础。

Abstract: We develop a theoretical framework that explains how discrete symbolic
structures can emerge naturally from continuous neural network training
dynamics. By lifting neural parameters to a measure space and modeling training
as Wasserstein gradient flow, we show that under geometric constraints, such as
group invariance, the parameter measure $\mu_t$ undergoes two concurrent
phenomena: (1) a decoupling of the gradient flow into independent optimization
trajectories over some potential functions, and (2) a progressive contraction
on the degree of freedom. These potentials encode algebraic constraints
relevant to the task and act as ring homomorphisms under a commutative
semi-ring structure on the measure space. As training progresses, the network
transitions from a high-dimensional exploration to compositional
representations that comply with algebraic operations and exhibit a lower
degree of freedom. We further establish data scaling laws for realizing
symbolic tasks, linking representational capacity to the group invariance that
facilitates symbolic solutions. This framework charts a principled foundation
for understanding and designing neurosymbolic systems that integrate continuous
learning with discrete algebraic reasoning.

</details>


### [69] [The Cost of Avoiding Backpropagation](https://arxiv.org/abs/2506.21833)
*Kunjal Panchal,Sunav Choudhary,Yuriy Brun,Hui Guan*

Main category: cs.LG

TL;DR: 文章对BP、FmAD和ZO方法进行理论与实证对比，发现FmAD和ZO有内存优势但存在精度、收敛速度和计算成本问题，BP加检查点策略最佳。


<details>
  <summary>Details</summary>
Motivation: FmAD和ZO作为内存高效的梯度计算替代方法，其实用性因缺乏与内存高效BP变体对比及统一理论分析而不明确，故进行研究。

Method: 对BP、FmAD和ZO方法进行全面的理论分析和在大语言及视觉 - 语言模型上的实证实验。

Result: 理论分析表明FmAD和ZO会降低精度、收敛速度并增加计算成本，实证实验显示BP加检查点策略在可比内存使用下精度高31.1%、收敛快34.8%、计算量少3.8倍。

Conclusion: FmAD和ZO存在根本局限，BP加检查点是内存受限下模型训练最有效策略。

Abstract: Forward-mode automatic differentiation (FmAD) and zero-order (ZO)
optimization have been proposed as memory-efficient alternatives to
backpropagation (BP) for gradient computation, especially in low-resource
settings. However, their practical benefits remain unclear due to two key gaps:
a lack of comparison against memory-efficient BP variants, such as activation
checkpointing, and a lack of a unified theoretical analysis. This work presents
a comprehensive theoretical and empirical comparison of BP, FmAD, and ZO
methods. Our theoretical analysis shows that while FmAD, and ZO can reduce
memory usage, they incur significant costs in accuracy, convergence speed, and
computation compared to BP with checkpointing. These drawbacks worsen with
larger models or constrained perturbation budgets. Empirical experiments on
large language and vision-language models show that BP with checkpointing
outperforms FmAD and ZO variants, including those enhanced with variance
reduction, achieving up to 31.1% higher accuracy, 34.8% faster convergence, and
3.8x fewer computations at comparable memory usage. Our results highlight
fundamental limitations of FmAD and ZO, and reaffirm BP with checkpointing as
the most effective strategy for model training under memory-constrained
settings. Our code is available at
https://github.com/Astuary/The_Cost_of_Avoiding_Backpropagation.

</details>


### [70] [Koopman operator-based discussion on partial observation in stochastic systems](https://arxiv.org/abs/2506.21844)
*Jun Ohkubo*

Main category: cs.LG

TL;DR: 本文用Koopman算子理论讨论随机系统中部分观测的影响，明确状态空间与函数空间区别的重要性，验证延迟嵌入技术的有效性并分析幂律行为指数与部分观测的关系。


<details>
  <summary>Details</summary>
Motivation: 在难以实现全量可观测量完整观测的情况下，此前Mori - Zwanzig形式主义处理确定性系统部分观测，而本文旨在用Koopman算子理论探讨随机系统中部分观测的影响。

Method: 运用Koopman算子理论，结合延迟嵌入技术，并进行了数值实验。

Result: 明确了在随机系统中区分状态空间和函数空间的重要性；证实延迟嵌入技术对部分观测有益；数值实验显示了加性噪声幅度精度的幂律行为。

Conclusion: 探讨了幂律行为指数与部分观测影响之间的关系。

Abstract: It is sometimes difficult to achieve a complete observation for a full set of
observables, and partial observations are necessary. For deterministic systems,
the Mori-Zwanzig formalism provides a theoretical framework for handling
partial observations. Recently, data-driven algorithms based on the Koopman
operator theory have made significant progress, and there is a discussion to
connect the Mori-Zwanzig formalism with the Koopman operator theory. In this
work, we discuss the effects of partial observation in stochastic systems using
the Koopman operator theory. The discussion clarifies the importance of
distinguishing the state space and the function space in stochastic systems.
Even in stochastic systems, the delay embedding technique is beneficial for
partial observation, and several numerical experiments showed a power-law
behavior of the accuracy for the amplitude of the additive noise. We also
discuss the relation between the exponent of the power-law behavior and the
effects of partial observation.

</details>


### [71] [A Survey of Continual Reinforcement Learning](https://arxiv.org/abs/2506.21872)
*Chaofan Pan,Xin Yang,Yanhua Li,Wei Wei,Tianrui Li,Bo An,Jiye Liang*

Main category: cs.LG

TL;DR: 文章对持续强化学习（CRL）进行全面审视，回顾现有工作、提出新分类法并分析挑战与未来方向。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习依赖大量训练数据和计算资源，泛化能力有限，CRL可解决这些局限。

Method: 详细回顾现有工作，分析其指标、任务等；提出基于知识存储和/或转移的CRL方法新分类法。

Result: 完成对CRL现有工作的回顾和分类，分析出CRL的独特挑战。

Conclusion: 对CRL进行全面研究，为未来研究提供实用见解。

Abstract: Reinforcement Learning (RL) is an important machine learning paradigm for
solving sequential decision-making problems. Recent years have witnessed
remarkable progress in this field due to the rapid development of deep neural
networks. However, the success of RL currently relies on extensive training
data and computational resources. In addition, RL's limited ability to
generalize across tasks restricts its applicability in dynamic and real-world
environments. With the arisen of Continual Learning (CL), Continual
Reinforcement Learning (CRL) has emerged as a promising research direction to
address these limitations by enabling agents to learn continuously, adapt to
new tasks, and retain previously acquired knowledge. In this survey, we provide
a comprehensive examination of CRL, focusing on its core concepts, challenges,
and methodologies. Firstly, we conduct a detailed review of existing works,
organizing and analyzing their metrics, tasks, benchmarks, and scenario
settings. Secondly, we propose a new taxonomy of CRL methods, categorizing them
into four types from the perspective of knowledge storage and/or transfer.
Finally, our analysis highlights the unique challenges of CRL and provides
practical insights into future directions.

</details>


### [72] [Advancements and Challenges in Continual Reinforcement Learning: A Comprehensive Review](https://arxiv.org/abs/2506.21899)
*Amara Zuffer,Michael Burke,Mehrtash Harandi*

Main category: cs.LG

TL;DR: 本文综述持续强化学习如何将强化学习智能体转变为动态持续学习者，探讨基本方面、挑战、方法，介绍机器人领域进展和评估环境，最后讨论局限与未来方向。


<details>
  <summary>Details</summary>
Motivation: 强化学习任务的多样性和动态性要求智能体能够顺序和持续学习，需要研究持续强化学习将其转变为动态持续学习者。

Method: 对持续强化学习的基本方面进行探讨，研究关键概念、挑战和新方法，介绍机器人领域进展和评估环境。

Result: 梳理了持续强化学习的相关内容，包括基本概念、挑战、方法、机器人领域进展和评估环境。

Conclusion: 指出了研究的局限性，并给出了有前景的未来方向，为研究者和从业者提供了有价值的见解。

Abstract: The diversity of tasks and dynamic nature of reinforcement learning (RL)
require RL agents to be able to learn sequentially and continuously, a learning
paradigm known as continuous reinforcement learning. This survey reviews how
continual learning transforms RL agents into dynamic continual learners. This
enables RL agents to acquire and retain useful and reusable knowledge
seamlessly. The paper delves into fundamental aspects of continual
reinforcement learning, exploring key concepts, significant challenges, and
novel methodologies. Special emphasis is placed on recent advancements in
continual reinforcement learning within robotics, along with a succinct
overview of evaluation environments utilized in prominent research,
facilitating accessibility for newcomers to the field. The review concludes
with a discussion on limitations and promising future directions, providing
valuable insights for researchers and practitioners alike.

</details>


### [73] [TOAST: Task-Oriented Adaptive Semantic Transmission over Dynamic Wireless Environments](https://arxiv.org/abs/2506.21900)
*Sheng Yun,Jianhua Pei,Ping Wang*

Main category: cs.LG

TL;DR: 本文提出TOAST框架以应对6G网络语义通信多任务优化挑战，实验表明其性能优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 6G网络发展需要从比特传输转向语义通信，解决动态无线环境下多任务优化挑战。

Method: 将自适应任务平衡建模为马尔可夫决策过程，用深度强化学习调整；在编码架构中集成LoRA机制；引入潜在空间扩散模型恢复受损特征。

Result: 在多个数据集的实验中，TOAST在低信噪比下分类精度和重建质量显著提升，各场景性能稳健。

Conclusion: TOAST框架在动态无线环境的多任务优化中表现优于基线方法。

Abstract: The evolution toward 6G networks demands a fundamental shift from bit-centric
transmission to semantic-aware communication that emphasizes task-relevant
information. This work introduces TOAST (Task-Oriented Adaptive Semantic
Transmission), a unified framework designed to address the core challenge of
multi-task optimization in dynamic wireless environments through three
complementary components. First, we formulate adaptive task balancing as a
Markov decision process, employing deep reinforcement learning to dynamically
adjust the trade-off between image reconstruction fidelity and semantic
classification accuracy based on real-time channel conditions. Second, we
integrate module-specific Low-Rank Adaptation (LoRA) mechanisms throughout our
Swin Transformer-based joint source-channel coding architecture, enabling
parameter-efficient fine-tuning that dramatically reduces adaptation overhead
while maintaining full performance across diverse channel impairments including
Additive White Gaussian Noise (AWGN), fading, phase noise, and impulse
interference. Third, we incorporate an Elucidating diffusion model that
operates in the latent space to restore features corrupted by channel noises,
providing substantial quality improvements compared to baseline approaches.
Extensive experiments across multiple datasets demonstrate that TOAST achieves
superior performance compared to baseline approaches, with significant
improvements in both classification accuracy and reconstruction quality at low
Signal-to-Noise Ratio (SNR) conditions while maintaining robust performance
across all tested scenarios.

</details>


### [74] [Less Greedy Equivalence Search](https://arxiv.org/abs/2506.22331)
*Adiba Ejaz,Elias Bareinboim*

Main category: cs.LG

TL;DR: 本文提出LGES算法改进GES，解决计算成本和有限样本准确性问题，实验显示LGES在速度、准确性和鲁棒性上优于GES和其他基线。


<details>
  <summary>Details</summary>
Motivation: GES算法在实践中面临计算成本和有限样本准确性的挑战，需要改进。

Method: 开发LGES算法，修改GES的贪心步骤，避免在分数暗示条件独立的变量间插入边，还能利用先验假设和干预数据。

Result: LGES相对GES有高达10倍的速度提升和结构误差大幅降低，实验显示其在速度、准确性和对错误假设的鲁棒性上优于GES和其他基线。

Conclusion: LGES在样本极限下能从观测和干预数据中恢复真实等价类，即使先验假设错误，且在多方面表现更优。

Abstract: Greedy Equivalence Search (GES) is a classic score-based algorithm for causal
discovery from observational data. In the sample limit, it recovers the Markov
equivalence class of graphs that describe the data. Still, it faces two
challenges in practice: computational cost and finite-sample accuracy. In this
paper, we develop Less Greedy Equivalence Search (LGES), a variant of GES that
retains its theoretical guarantees while partially addressing these
limitations. LGES modifies the greedy step: rather than always applying the
highest-scoring insertion, it avoids edge insertions between variables for
which the score implies some conditional independence. This more targeted
search yields up to a \(10\)-fold speed-up and a substantial reduction in
structural error relative to GES. Moreover, LGES can guide the search using
prior assumptions, while correcting these assumptions when contradicted by the
data. Finally, LGES can exploit interventional data to refine the learned
observational equivalence class. We prove that LGES recovers the true
equivalence class in the sample limit from observational and interventional
data, even with misspecified prior assumptions. Experiments demonstrate that
LGES outperforms GES and other baselines in speed, accuracy, and robustness to
misspecified assumptions. Our code is available at
https://github.com/CausalAILab/lges.

</details>


### [75] [HQCM-EBTC: A Hybrid Quantum-Classical Model for Explainable Brain Tumor Classification](https://arxiv.org/abs/2506.21937)
*Marwan Ait Haddou,Mohamed Bennai*

Main category: cs.LG

TL;DR: 提出混合量子 - 经典模型HQCM - EBTC用于脑肿瘤自动分类，在数据集上训练，精度超经典基线，凸显量子增强模型在医学影像潜力。


<details>
  <summary>Details</summary>
Motivation: 提高脑肿瘤分类的诊断准确性和可解释性，探索量子增强模型在医学影像中的应用。

Method: 提出HQCM - EBTC模型，整合5量子比特、深度为2的量子层与5个并行电路，用AdamW优化，采用融合交叉熵和注意力一致性的复合损失函数，在7576个扫描数据集上训练。

Result: HQCM - EBTC准确率达96.48%，远超经典基线；在精度和F1分数上表现更好；量子空间特征可分离性增强；误分类率降低；肿瘤定位更准确聚焦。

Conclusion: 量子增强模型在医学影像中有前景，能提升临床脑肿瘤评估的诊断准确性和可解释性。

Abstract: We propose HQCM-EBTC, a hybrid quantum-classical model for automated brain
tumor classification using MRI images. Trained on a dataset of 7,576 scans
covering normal, meningioma, glioma, and pituitary classes, HQCM-EBTC
integrates a 5-qubit, depth-2 quantum layer with 5 parallel circuits, optimized
via AdamW and a composite loss blending cross-entropy and attention
consistency.
  HQCM-EBTC achieves 96.48% accuracy, substantially outperforming the classical
baseline (86.72%). It delivers higher precision and F1-scores, especially for
glioma detection. t-SNE projections reveal enhanced feature separability in
quantum space, and confusion matrices show lower misclassification. Attention
map analysis (Jaccard Index) confirms more accurate and focused tumor
localization at high-confidence thresholds.
  These results highlight the promise of quantum-enhanced models in medical
imaging, advancing both diagnostic accuracy and interpretability for clinical
brain tumor assessment.

</details>


### [76] [GuiderNet: A Meta-Learning Framework for Optimizing Quantum Circuit Geometry and Mitigating Barren Plateaus](https://arxiv.org/abs/2506.21940)
*Marwan Ait Haddou,Mohamed Bennai*

Main category: cs.LG

TL;DR: 提出GuiderNet元学习框架改善变分量子算法训练问题，在糖尿病分类任务中效果显著。


<details>
  <summary>Details</summary>
Motivation: 变分量子算法面临梯度消失和优化地形条件差的问题，需解决以实现近期量子优势。

Method: 引入GuiderNet框架，用数据依赖的参数偏移调节参数化量子电路，以最小化富比尼 - 斯图迪度量张量的对数条件数，该框架作为经典神经网络进行元训练。

Result: 在糖尿病分类任务中，GuiderNet使累积训练损失降低超5倍，测试准确率从75.3%提升到98.6%，少数类F1分数从0.67提升到0.95，抑制梯度爆炸并稳定参数更新。

Conclusion: 几何元调节可缓解贫瘠高原和病态条件问题，为提升量子机器学习的可训练性和泛化性提供可扩展方法。

Abstract: Variational Quantum Algorithms (VQAs) offer potential for near-term quantum
advantage but face challenges from barren plateaus, where gradients vanish, and
poorly conditioned optimization landscapes. We introduce GuiderNet, a
meta-learning framework that conditions Parameterized Quantum Circuits (PQCs)
using data-dependent parameter shifts aimed at minimizing the log condition
number of the Fubini-Study metric tensor. Implemented as a classical neural
network, GuiderNet is meta-trained to guide PQC parameters into geometrically
favorable regions and is embedded within hybrid quantum-classical pipelines to
steer both initialization and adaptive modulation during training.
  Applied to the Kaggle Diabetes classification task, GuiderNet reduces
cumulative training loss by over 5x, improves test accuracy from 75.3% to
98.6%, and increases the minority-class F1 score from 0.67 to 0.95. It also
suppresses gradient explosion and stabilizes parameter updates, enabling
smoother and more robust optimization. These results demonstrate that geometric
meta-conditioning can mitigate barren plateaus and ill-conditioning, providing
a scalable approach to enhance trainability and generalization in quantum
machine learning.

</details>


### [77] [Physics-informed network paradigm with data generation and background noise removal for diverse distributed acoustic sensing applications](https://arxiv.org/abs/2506.21952)
*Yangyang Wan,Haotian Wang,Xuhui Yu,Jiageng Chen,Xinyu Fan,Zuyuan He*

Main category: cs.LG

TL;DR: 提出无需真实世界事件数据训练的物理信息DAS神经网络范式，经实验验证有效且有泛化性，是解决DAS应用难题的潜在方案。


<details>
  <summary>Details</summary>
Motivation: 现有AI模型训练需真实世界数据，与现实中可用事件数据有限的情况矛盾。

Method: 对目标事件、现实世界和DAS系统进行物理建模，推导物理函数训练生成网络以生成DAS事件数据，用生成数据训练DAS去背景网络消除噪声。

Result: 在事件识别和皮带输送机故障监测应用中验证了范式有效性，性能可比或优于用真实世界数据训练的网络；在皮带输送机领域实现91.8%的故障诊断准确率。

Conclusion: 该范式是解决实际DAS应用中数据采集和强噪声问题的有前景的解决方案，可探索更多DAS潜在领域。

Abstract: Distributed acoustic sensing (DAS) has attracted considerable attention
across various fields and artificial intelligence (AI) technology plays an
important role in DAS applications to realize event recognition and denoising.
Existing AI models require real-world data (RWD), whether labeled or not, for
training, which is contradictory to the fact of limited available event data in
real-world scenarios. Here, a physics-informed DAS neural network paradigm is
proposed, which does not need real-world events data for training. By
physically modeling target events and the constraints of real world and DAS
system, physical functions are derived to train a generative network for
generation of DAS events data. DAS debackground net is trained by using the
generated DAS events data to eliminate background noise in DAS data. The
effectiveness of the proposed paradigm is verified in event identification
application based on a public dataset of DAS spatiotemporal data and in belt
conveyor fault monitoring application based on DAS time-frequency data, and
achieved comparable or better performance than data-driven networks trained
with RWD. Owing to the introduction of physical information and capability of
background noise removal, the paradigm demonstrates generalization in same
application on different sites. A fault diagnosis accuracy of 91.8% is achieved
in belt conveyor field with networks which transferred from simulation test
site without any fault events data of test site and field for training. The
proposed paradigm is a prospective solution to address significant obstacles of
data acquisition and intense noise in practical DAS applications and explore
more potential fields for DAS.

</details>


### [78] [Optimal Return-to-Go Guided Decision Transformer for Auto-Bidding in Advertisement](https://arxiv.org/abs/2506.21956)
*Hao Jiang,Yongxiang Tang,Yanxiang Zeng,Pengjia Yuan,Yanhua Cheng,Teng Sha,Xialong Liu,Peng Jiang*

Main category: cs.LG

TL;DR: 文章采用生成模型解决自动出价难题，针对传统决策转换器（DT）的不足提出R*决策转换器（R* DT），并通过三步流程开发，测试验证其有效性和优越性。


<details>
  <summary>Details</summary>
Motivation: 为提高在线广告自动出价系统的自动化程度，解决传统DT在自动出价任务中的不足。

Method: 提出R* DT，分三步开发：R DT存储状态和RTG值；R^ DT预测给定状态的最高RTG值并导出次优策略；R* DT生成高回报轨迹增强训练数据集。

Result: 综合测试验证了R* DT的有效性，表明其在处理混合质量轨迹时表现更优。

Conclusion: R* DT能有效解决传统DT的问题，在自动出价任务中表现良好，具有优越性。

Abstract: In the realm of online advertising, advertisers partake in ad auctions to
obtain advertising slots, frequently taking advantage of auto-bidding tools
provided by demand-side platforms. To improve the automation of these bidding
systems, we adopt generative models, namely the Decision Transformer (DT), to
tackle the difficulties inherent in automated bidding. Applying the Decision
Transformer to the auto-bidding task enables a unified approach to sequential
modeling, which efficiently overcomes short-sightedness by capturing long-term
dependencies between past bidding actions and user behavior. Nevertheless,
conventional DT has certain drawbacks: (1) DT necessitates a preset
return-to-go (RTG) value before generating actions, which is not inherently
produced; (2) The policy learned by DT is restricted by its training data,
which is consists of mixed-quality trajectories. To address these challenges,
we introduce the R* Decision Transformer (R* DT), developed in a three-step
process: (1) R DT: Similar to traditional DT, R DT stores actions based on
state and RTG value, as well as memorizing the RTG for a given state using the
training set; (2) R^ DT: We forecast the highest value (within the training
set) of RTG for a given state, deriving a suboptimal policy based on the
current state and the forecasted supreme RTG value; (3) R* DT: Based on R^ DT,
we generate trajectories and select those with high rewards (using a simulator)
to augment our training dataset. This data enhancement has been shown to
improve the RTG of trajectories in the training data and gradually leads the
suboptimal policy towards optimality. Comprehensive tests on a publicly
available bidding dataset validate the R* DT's efficacy and highlight its
superiority when dealing with mixed-quality trajectories.

</details>


### [79] [SceneDiffuser++: City-Scale Traffic Simulation via a Generative World Model](https://arxiv.org/abs/2506.21976)
*Shuhan Tan,John Lambert,Hong Jeon,Sakshum Kulshrestha,Yijing Bai,Jing Luo,Dragomir Anguelov,Mingxing Tan,Chiyu Max Jiang*

Main category: cs.LG

TL;DR: 提出SceneDiffuser++用于城市规模的点A到点B交通仿真，展示其能力并评估质量。


<details>
  <summary>Details</summary>
Motivation: 交通仿真目标是用大量模拟里程补充有限的人工驾驶里程，CitySim愿景需多种仿真技术，但部分技术受关注少，需端到端模型。

Method: 提出SceneDiffuser++，一个基于单一损失函数训练的端到端生成世界模型。

Result: 展示了SceneDiffuser++的城市规模交通仿真能力，研究了其在长时间仿真条件下的优越真实感。

Conclusion: 通过在增强版Waymo Open Motion Dataset上评估，SceneDiffuser++可实现城市规模交通仿真。

Abstract: The goal of traffic simulation is to augment a potentially limited amount of
manually-driven miles that is available for testing and validation, with a much
larger amount of simulated synthetic miles. The culmination of this vision
would be a generative simulated city, where given a map of the city and an
autonomous vehicle (AV) software stack, the simulator can seamlessly simulate
the trip from point A to point B by populating the city around the AV and
controlling all aspects of the scene, from animating the dynamic agents (e.g.,
vehicles, pedestrians) to controlling the traffic light states. We refer to
this vision as CitySim, which requires an agglomeration of simulation
technologies: scene generation to populate the initial scene, agent behavior
modeling to animate the scene, occlusion reasoning, dynamic scene generation to
seamlessly spawn and remove agents, and environment simulation for factors such
as traffic lights. While some key technologies have been separately studied in
various works, others such as dynamic scene generation and environment
simulation have received less attention in the research community. We propose
SceneDiffuser++, the first end-to-end generative world model trained on a
single loss function capable of point A-to-B simulation on a city scale
integrating all the requirements above. We demonstrate the city-scale traffic
simulation capability of SceneDiffuser++ and study its superior realism under
long simulation conditions. We evaluate the simulation quality on an augmented
version of the Waymo Open Motion Dataset (WOMD) with larger map regions to
support trip-level simulation.

</details>


### [80] [Binned semiparametric Bayesian networks](https://arxiv.org/abs/2506.21997)
*Rafael Sojo,Javier Díaz-Rozo,Concha Bielza,Pedro Larrañaga*

Main category: cs.LG

TL;DR: 本文提出新的概率半参数模型，通过数据分箱降低核密度估计计算成本，开发两种新条件概率分布解决维数灾难，经实验验证新模型高效可靠。


<details>
  <summary>Details</summary>
Motivation: 降低非参数分布中核密度估计的计算成本，解决分箱模型的维数灾难问题。

Method: 开发稀疏分箱核密度估计和傅里叶核密度估计两种新的条件概率分布，进行复杂度分析和对比实验。

Result: 分箱半参数贝叶斯网络在结构学习和对数似然估计上与半参数贝叶斯网络无显著差异，但速度更快。

Conclusion: 新的分箱半参数贝叶斯网络是无分箱模型的可靠且更高效替代方案。

Abstract: This paper introduces a new type of probabilistic semiparametric model that
takes advantage of data binning to reduce the computational cost of kernel
density estimation in nonparametric distributions. Two new conditional
probability distributions are developed for the new binned semiparametric
Bayesian networks, the sparse binned kernel density estimation and the Fourier
kernel density estimation. These two probability distributions address the
curse of dimensionality, which typically impacts binned models, by using sparse
tensors and restricting the number of parent nodes in conditional probability
calculations. To evaluate the proposal, we perform a complexity analysis and
conduct several comparative experiments using synthetic data and datasets from
the UCI Machine Learning repository. The experiments include different binning
rules, parent restrictions, grid sizes, and number of instances to get a
holistic view of the model's behavior. As a result, our binned semiparametric
Bayesian networks achieve structural learning and log-likelihood estimations
with no statistically significant differences compared to the semiparametric
Bayesian networks, but at a much higher speed. Thus, the new binned
semiparametric Bayesian networks prove to be a reliable and more efficient
alternative to their non-binned counterparts.

</details>


### [81] [On the Necessity of Output Distribution Reweighting for Effective Class Unlearning](https://arxiv.org/abs/2506.20893)
*Yian Wang,Ali Ebrahimpour-Boroojeny,Hari Sundaram*

Main category: cs.LG

TL;DR: 提出轻量级输出重加权遗忘方法RWFT，能在不重新训练的情况下从训练好的分类器中删除整个类别，实验显示效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 遗忘特定类别对保障用户删除权和减少有害或有偏差的预测很重要，但全量重新训练成本高，现有遗忘方法无法复现重新训练模型对已遗忘类别的预测行为。

Method: 设计成员推理攻击变体MIA - NN证明现有方法的失败；对遗忘类别的样本预测进行概率质量的简单重新分配，使其对MIA - NN具有鲁棒性；引入基于总变差距离的新指标量化残余泄漏。

Result: 通过大量实验，该方法在先前评估指标和新指标上都达到了全量重新训练的效果，在先前指标上比现有最佳方法提高2.79%，在新指标上提高111.45%。

Conclusion: 提出的RWFT方法能有效从训练好的分类器中删除类别，且在性能上优于现有方法。

Abstract: In this work, we introduce an output-reweighting unlearning method, RWFT, a
lightweight technique that erases an entire class from a trained classifier
without full retraining. Forgetting specific classes from trained models is
essential for enforcing user deletion rights and mitigating harmful or biased
predictions. The full retraining is costly and existing unlearning methods fail
to replicate the behavior of the retrained models when predicting samples from
the unlearned class. We prove this failure by designing a variant of membership
inference attacks, MIA-NN that successfully reveals the unlearned class for any
of these methods. We propose a simple redistribution of the probability mass
for the prediction on the samples in the forgotten class which is robust to
MIA-NN. We also introduce a new metric based on the total variation (TV)
distance of the prediction probabilities to quantify residual leakage to
prevent future methods from susceptibility to the new attack. Through extensive
experiments with state of the art baselines in machine unlearning, we show that
our approach matches the results of full retraining in both metrics used for
evaluation by prior work and the new metric we propose in this work. Compare to
state-of-the-art methods, we gain 2.79% in previously used metrics and 111.45%
in our new TV-based metric over the best existing method.

</details>


### [82] [GKNet: Graph Kalman Filtering and Model Inference via Model-based Deep Learning](https://arxiv.org/abs/2506.22004)
*Mohammad Sabbaqi,Riccardo Taormina,Elvin Isufi*

Main category: cs.LG

TL;DR: 提出图时间序列的图感知状态空间模型，先通过最大似然法推断，后用深度学习架构改进。


<details>
  <summary>Details</summary>
Motivation: 解决图上时间序列推理任务，需找到能捕捉图 - 时间模式且计算可行的模型。

Method: 提出图感知状态空间模型，状态方程遵循图边噪声驱动的随机偏微分方程，观测模型是状态的采样和图滤波版本；先用最大似然法推断，再构建深度学习架构。

Result: 未提及具体结果。

Conclusion: 未提及明确结论。

Abstract: Inference tasks with time series over graphs are of importance in
applications such as urban water networks, economics, and networked
neuroscience. Addressing these tasks typically relies on identifying a
computationally affordable model that jointly captures the graph-temporal
patterns of the data. In this work, we propose a graph-aware state space model
for graph time series, where both the latent state and the observation equation
are parametric graph-induced models with a limited number of parameters that
need to be learned. More specifically, we consider the state equation to follow
a stochastic partial differential equation driven by noise over the graphs
edges accounting not only for potential edge uncertainties but also for
increasing the degrees of freedom in the latter in a tractable manner. The
graph structure conditioning of the noise dispersion allows the state variable
to deviate from the stochastic process in certain neighborhoods. The
observation model is a sampled and graph-filtered version of the state
capturing multi-hop neighboring influence. The goal is to learn the parameters
in both state and observation models from the partially observed data for
downstream tasks such as prediction and imputation. The model is inferred first
through a maximum likelihood approach that provides theoretical tractability
but is limited in expressivity and scalability. To improve on the latter, we
use the state-space formulation to build a principled deep learning
architecture that jointly learns the parameters and tracks the state in an
end-to-end manner in the spirit of Kalman neural networks.

</details>


### [83] [TROFI: Trajectory-Ranked Offline Inverse Reinforcement Learning](https://arxiv.org/abs/2506.22008)
*Alessandro Sestini,Joakim Bergdahl,Konrad Tollmar,Andrew D. Bagdanov,Linus Gisslén*

Main category: cs.LG

TL;DR: 本文提出TROFI方法，可在无预定义奖励函数情况下离线学习策略，实验显示其优于基线方法，并强调奖励函数的重要性。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习需数据集由奖励函数标注，但实际应用中奖励函数并非总能获取。

Method: 提出TROFI方法，先从人类偏好学习奖励函数，用其标注原始数据集来训练策略，且不要求最优轨迹。

Result: 在D4RL基准测试中TROFI始终优于基线方法，表现与使用真实奖励学习策略相当，在3D游戏环境中也验证了方法有效性。

Conclusion: 要确保价值函数与实际未来折扣奖励一致，设计良好且易学习的奖励函数至关重要。

Abstract: In offline reinforcement learning, agents are trained using only a fixed set
of stored transitions derived from a source policy. However, this requires that
the dataset be labeled by a reward function. In applied settings such as video
game development, the availability of the reward function is not always
guaranteed. This paper proposes Trajectory-Ranked OFfline Inverse reinforcement
learning (TROFI), a novel approach to effectively learn a policy offline
without a pre-defined reward function. TROFI first learns a reward function
from human preferences, which it then uses to label the original dataset making
it usable for training the policy. In contrast to other approaches, our method
does not require optimal trajectories. Through experiments on the D4RL
benchmark we demonstrate that TROFI consistently outperforms baselines and
performs comparably to using the ground truth reward to learn policies.
Additionally, we validate the efficacy of our method in a 3D game environment.
Our studies of the reward model highlight the importance of the reward function
in this setting: we show that to ensure the alignment of a value function to
the actual future discounted reward, it is fundamental to have a
well-engineered and easy-to-learn reward function.

</details>


### [84] [Hyper-modal Imputation Diffusion Embedding with Dual-Distillation for Federated Multimodal Knowledge Graph Completion](https://arxiv.org/abs/2506.22036)
*Ying Zhang,Yu Zhao,Xuhui Sui,Baohang Zhou,Xiangrui Cai,Li Shen,Xiaojie Yuan,Dacheng Tao*

Main category: cs.LG

TL;DR: 提出联邦多模态知识图谱补全（FedMKGC）任务及MMFeD3 - HidE框架，还提出FedMKGC基准进行评估，实验验证了MMFeD3 - HidE的有效性等。


<details>
  <summary>Details</summary>
Motivation: 随着多模态知识私有化需求增加，不同机构的多模态知识图谱分散，缺乏兼具强推理能力和传输安全保障的协作系统。

Method: 提出MMFeD3 - HidE框架，包括客户端内的Hyper - modal Imputation Diffusion Embedding模型（HidE）和客户端间的Multimodal FeDerated Dual Distillation（MMFeD3），还提出FedMKGC基准。

Result: 在FedMKGC基准上的实验验证了MMFeD3 - HidE的有效性、语义一致性和收敛鲁棒性。

Conclusion: MMFeD3 - HidE能有效解决FedMKGC的多模态不确定不可用和多模态客户端异质性挑战。

Abstract: With the increasing multimodal knowledge privatization requirements,
multimodal knowledge graphs in different institutes are usually decentralized,
lacking of effective collaboration system with both stronger reasoning ability
and transmission safety guarantees. In this paper, we propose the Federated
Multimodal Knowledge Graph Completion (FedMKGC) task, aiming at training over
federated MKGs for better predicting the missing links in clients without
sharing sensitive knowledge. We propose a framework named MMFeD3-HidE for
addressing multimodal uncertain unavailability and multimodal client
heterogeneity challenges of FedMKGC. (1) Inside the clients, our proposed
Hyper-modal Imputation Diffusion Embedding model (HidE) recovers the complete
multimodal distributions from incomplete entity embeddings constrained by
available modalities. (2) Among clients, our proposed Multimodal FeDerated Dual
Distillation (MMFeD3) transfers knowledge mutually between clients and the
server with logit and feature distillation to improve both global convergence
and semantic consistency. We propose a FedMKGC benchmark for a comprehensive
evaluation, consisting of a general FedMKGC backbone named MMFedE, datasets
with heterogeneous multimodal information, and three groups of constructed
baselines. Experiments conducted on our benchmark validate the effectiveness,
semantic consistency, and convergence robustness of MMFeD3-HidE.

</details>


### [85] [UniCA: Adapting Time Series Foundation Model to General Covariate-Aware Forecasting](https://arxiv.org/abs/2506.22039)
*Lu Han,Yu Liu,Qiwen Deng,Jian Jiang,Yinbo Sun,Zhe Yu,Binfeng Wang,Xingyu Lu,Lintao Ma,Han-Jia Ye,De-Chuan Zhan*

Main category: cs.LG

TL;DR: 本文提出UniCA框架解决TSFMs处理异质协变量能力不足问题，实验证明其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列基础模型（TSFMs）主要针对实值序列设计，难以处理包含异质协变量的通用预测任务。

Method: 提出统一协变量适应（UniCA）框架，先进行协变量同质化处理，再通过统一的基于注意力的融合机制融合。

Result: 在多个单模态和多模态协变量感知预测基准上的大量实验证明了UniCA的优越性。

Conclusion: 协变量感知的TSFM适应在现实世界预测场景中有前景，代码已开源。

Abstract: Time Series Foundation Models (TSFMs) have achieved remarkable success
through large-scale pretraining. However, their design primarily targets
real-valued series, limiting their ability to handle general forecasting tasks
involving diverse and often heterogeneous covariates--such as categorical
variables and multimodal data (e.g., images, text)--which are typically
task-specific and difficult to leverage during pretraining. To address this
gap, we propose Unified Covariate Adaptation (UniCA), a framework to bridge
TSFMs with general covariate-aware forecasting. UniCA first performs covariate
homogenization to transform heterogeneous covariates into high-level
homogeneous series representations and then fuses them via a unified
attention-based fusion mechanism. UniCA is compatible and universal for
adaptation with both homogeneous and heterogeneous covariates, incorporating
extra covariate information while preserving the generalization ability of
TSFMs.Extensive experiments on multiple unimodal and multimodal covariate-aware
forecasting benchmarks demonstrate the superiority of UniCA, highlighting the
promise of covariate-aware TSFM adaptation in real-world forecasting scenarios.
Codes are released on https://github.com/hanlu-nju/UniCA.

</details>


### [86] [GPAS: Accelerating Convergence of LLM Pretraining via Gradient-Preserving Activation Scaling](https://arxiv.org/abs/2506.22049)
*Tianhao Chen,Xin Xu,Zijing Liu,Pengxiang Li,Xinyuan Song,Ajay Kumar Jaiswal,Fan Zhang,Jishan Hu,Yang Wang,Hao Chen,Shizhe Diao,Shiwei Liu,Yu Li,Yin Lu,Can Yang*

Main category: cs.LG

TL;DR: 提出梯度保留激活缩放（GPAS）技术解决Pre - LN架构激活方差指数增长问题，实验显示有性能提升且适用于多种架构。


<details>
  <summary>Details</summary>
Motivation: Pre - LN架构存在激活方差指数增长问题，限制深层学习能力，需解决该问题。

Method: 提出GPAS技术，在保持梯度不变的同时缩放中间激活，避免梯度消失。

Result: 在71M到1B不同模型大小上实验，GPAS取得一致性能提升。

Conclusion: GPAS不仅能增强Pre - LN Transformer，还对其他架构有改进潜力，通用性强。

Abstract: Modern Large Language Models, such as the LLaMA, Qwen and DeepSeek series,
predominantly adopt the Pre-LayerNorm (Pre-LN) Transformer architecture. While
being stable during pretraining and scalable to large model sizes, Pre-LN
suffers from an exponential growth in activation variance across layers,
causing the residual path to dominate over sub-layer outputs and limiting the
learning capacity of deeper layers. To mitigate this issue, we propose
Gradient-Preserving Activation Scaling (GPAS), a simple technique that can be
used in combination with existing approaches. GPAS works by scaling down the
intermediate activations while keeping their gradients unchanged. This leaves
information in the activations intact, and avoids the gradient vanishing
problem associated with gradient downscaling. Extensive experiments across
various model sizes from 71M to 1B show that GPAS achieves consistent
performance gains. Beyond enhancing Pre-LN Transformers, GPAS also shows
promise in improving alternative architectures such as Sandwich-LN and
DeepNorm, demonstrating its versatility and potential for improving training
dynamics in a wide range of settings.

</details>


### [87] [crypto price prediction using lstm+xgboost](https://arxiv.org/abs/2506.22055)
*Mehul Gautam*

Main category: cs.LG

TL;DR: 提出LSTM+XGBoost混合模型用于加密货币价格预测，评估显示优于独立模型和传统方法，凸显混合架构潜力。


<details>
  <summary>Details</summary>
Motivation: 加密货币市场的波动性和复杂动态给准确价格预测带来挑战，需要新的预测方法。

Method: 提出结合LSTM网络和XGBoost的混合深度学习和机器学习模型，LSTM捕捉历史价格数据的时间依赖，XGBoost通过辅助特征建模非线性关系，在多种加密货币历史数据集上评估。

Result: 使用MAPE和MinMax RMSE进行比较分析，LSTM+XGBoost混合模型始终优于独立模型和传统预测方法。

Conclusion: 混合架构在金融预测中有潜力，模型在不同加密货币和市场环境中有适应性。

Abstract: The volatility and complex dynamics of cryptocurrency markets present unique
challenges for accurate price forecasting. This research proposes a hybrid deep
learning and machine learning model that integrates Long Short-Term Memory
(LSTM) networks and Extreme Gradient Boosting (XGBoost) for cryptocurrency
price prediction. The LSTM component captures temporal dependencies in
historical price data, while XGBoost enhances prediction by modeling nonlinear
relationships with auxiliary features such as sentiment scores and
macroeconomic indicators. The model is evaluated on historical datasets of
Bitcoin, Ethereum, Dogecoin, and Litecoin, incorporating both global and
localized exchange data. Comparative analysis using Mean Absolute Percentage
Error (MAPE) and Min-Max Normalized Root Mean Square Error (MinMax RMSE)
demonstrates that the LSTM+XGBoost hybrid consistently outperforms standalone
models and traditional forecasting methods. This study underscores the
potential of hybrid architectures in financial forecasting and provides
insights into model adaptability across different cryptocurrencies and market
contexts.

</details>


### [88] [Transformers are Graph Neural Networks](https://arxiv.org/abs/2506.22084)
*Chaitanya K. Joshi*

Main category: cs.LG

TL;DR: 建立Transformer架构与图神经网络（GNN）的联系，指出Transformers可视为在全连接图上操作的消息传递GNN，且在硬件实现上更高效。


<details>
  <summary>Details</summary>
Motivation: 探究Transformer架构与图神经网络（GNN）在表示学习上的联系。

Method: 分析Transformer如何可被视为在全连接图上操作的消息传递GNN，研究自注意力机制和位置编码的作用。

Result: 发现Transformers是表达性集合处理网络，且其通过密集矩阵运算实现，在现代硬件上比稀疏消息传递更高效。

Conclusion: 从数学联系和硬件实现角度看，Transformers可视为目前在硬件上更具优势的GNN。

Abstract: We establish connections between the Transformer architecture, originally
introduced for natural language processing, and Graph Neural Networks (GNNs)
for representation learning on graphs. We show how Transformers can be viewed
as message passing GNNs operating on fully connected graphs of tokens, where
the self-attention mechanism capture the relative importance of all tokens
w.r.t. each-other, and positional encodings provide hints about sequential
ordering or structure. Thus, Transformers are expressive set processing
networks that learn relationships among input elements without being
constrained by apriori graphs. Despite this mathematical connection to GNNs,
Transformers are implemented via dense matrix operations that are significantly
more efficient on modern hardware than sparse message passing. This leads to
the perspective that Transformers are GNNs currently winning the hardware
lottery.

</details>


### [89] [Learning to Solve Multi-Objective Routing Problems on Multigraphs](https://arxiv.org/abs/2506.22095)
*Filip Rydin,Attila Lischka,Jiaming Wu,Morteza Haghir Chehreghani,Balázs Kulcsár*

Main category: cs.LG

TL;DR: 本文提出两种神经方法解决多图上的多目标路由问题，实验验证表现良好。


<details>
  <summary>Details</summary>
Motivation: 学习型路由方法受关注，但多图场景虽实用却被忽视。

Method: 第一种方法直接在多图上自回归选边直到完成路径；第二种先将多图剪枝为简单图再构建路径。

Result: 两个模型在多种问题（如TSP和CVRP）中表现良好。

Conclusion: 所提出的两种神经方法能有效解决多图上的多目标路由问题。

Abstract: Learning-based methods for routing have gained significant attention in
recent years, both in single-objective and multi-objective contexts. However,
the multigraph setting, where multiple paths with distinct attributes can exist
between destinations, has largely been overlooked, despite its high practical
relevancy. In this paper, we introduce two neural approaches to address
multi-objective routing on multigraphs. Our first approach works directly on
the multigraph, by autoregressively selecting edges until a tour is completed.
On the other hand, our second model first prunes the multigraph into a simple
graph and then builds routes. We validate both models experimentally and find
that they demonstrate strong performance across a variety of problems,
including the Traveling Salesman Problem (TSP) and Capacitated Vehicle Routing
Problem (CVRP).

</details>


### [90] [Transfer Learning for Assessing Heavy Metal Pollution in Seaports Sediments](https://arxiv.org/abs/2506.22096)
*Tin Lai,Farnaz Farid,Yueyang Kuan,Xintian Zhang*

Main category: cs.LG

TL;DR: 提出基于深度学习的模型简化土壤和海港重金属评估过程，解决数据稀缺问题，在澳大利亚六个港口数据评估中表现良好，为水质预测提供创新方法。


<details>
  <summary>Details</summary>
Motivation: 传统污染负荷指数（PLI）评估程序繁琐，且水 - 沉积物领域存在数据收集困难和各国标准不同导致的数据稀缺问题，需要简化重金属评估过程。

Method: 利用迁移学习，开发基于深度学习的模型，实现跨不同特征集的领域间特征转移。

Result: 在澳大利亚六个主要港口数据评估中，该模型的平均绝对误差（MAE）约为0.5，平均绝对百分比误差（MAPE）约为0.03，比其他基线模型性能高出2个数量级。

Conclusion: 所提出的模型为水质预测提供了创新、易获取且经济高效的方法，有利于海洋生物保护、水产养殖和工业污染监测。

Abstract: Detecting heavy metal pollution in soils and seaports is vital for regional
environmental monitoring. The Pollution Load Index (PLI), an international
standard, is commonly used to assess heavy metal containment. However, the
conventional PLI assessment involves laborious procedures and data analysis of
sediment samples. To address this challenge, we propose a deep-learning-based
model that simplifies the heavy metal assessment process. Our model tackles the
issue of data scarcity in the water-sediment domain, which is traditionally
plagued by challenges in data collection and varying standards across nations.
By leveraging transfer learning, we develop an accurate quantitative assessment
method for predicting PLI. Our approach allows the transfer of learned features
across domains with different sets of features. We evaluate our model using
data from six major ports in New South Wales, Australia: Port Yamba, Port
Newcastle, Port Jackson, Port Botany, Port Kembla, and Port Eden. The results
demonstrate significantly lower Mean Absolute Error (MAE) and Mean Absolute
Percentage Error (MAPE) of approximately 0.5 and 0.03, respectively, compared
to other models. Our model performance is up to 2 orders of magnitude than
other baseline models. Our proposed model offers an innovative, accessible, and
cost-effective approach to predicting water quality, benefiting marine life
conservation, aquaculture, and industrial pollution monitoring.

</details>


### [91] [Earthquake Damage Grades Prediction using An Ensemble Approach Integrating Advanced Machine and Deep Learning Models](https://arxiv.org/abs/2506.22129)
*Anurag Panda,Gaurav Kumar Yadav*

Main category: cs.LG

TL;DR: 本文聚焦地震后建筑结构损伤等级评估，处理类别不平衡问题，研究多种机器学习和深度学习模型预测损伤等级。


<details>
  <summary>Details</summary>
Motivation: 地震后评估建筑结构和基础设施损伤对救灾响应至关重要，准确估计建筑损伤等级有助于有效响应和恢复，且要解决类别不平衡问题。

Method: 使用合成少数过采样技术（SMOTE）处理类别不平衡，研究多种多类分类机器学习、深度学习模型和集成方法预测损伤等级，通过特征操作实验和不同训练方法分析性能决定因素。

Result: 通过综合实验和评估技术，明确了性能决定因素，识别了导致地震脆弱性的关键因素。

Conclusion: 通过相关方法和评估技术，增强了对地震损伤预测有效性的理解。

Abstract: In the aftermath of major earthquakes, evaluating structural and
infrastructural damage is vital for coordinating post-disaster response
efforts. This includes assessing damage's extent and spatial distribution to
prioritize rescue operations and resource allocation. Accurately estimating
damage grades to buildings post-earthquake is paramount for effective response
and recovery, given the significant impact on lives and properties,
underscoring the urgency of streamlining relief fund allocation processes.
Previous studies have shown the effectiveness of multi-class classification,
especially XGBoost, along with other machine learning models and ensembling
methods, incorporating regularization to address class imbalance. One
consequence of class imbalance is that it may give rise to skewed models that
undervalue minority classes and give preference to the majority class. This
research deals with the problem of class imbalance with the help of the
synthetic minority oversampling technique (SMOTE). We delve into multiple
multi-class classification machine learning, deep learning models, and
ensembling methods to forecast structural damage grades. The study elucidates
performance determinants through comprehensive feature manipulation experiments
and diverse training approaches. It identifies key factors contributing to
seismic vulnerability while evaluating model performance using techniques like
the confusion matrix further to enhance understanding of the effectiveness of
earthquake damage prediction.

</details>


### [92] [Thompson Sampling-Based Learning and Control for Unknown Dynamic Systems](https://arxiv.org/abs/2506.22186)
*Kaikai Zheng,Dawei Shi,Yang Shi,Long Wang*

Main category: cs.LG

TL;DR: 本文针对Thompson采样（TS）在控制系统设计中因依赖有限参数表示而适用性受限的问题，提出用再生核希尔伯特空间的参数化方法和数据驱动主动学习控制方法，给出理论分析和数值实验验证。


<details>
  <summary>Details</summary>
Motivation: TS依赖有限参数表示，限制其在控制系统设计更一般空间的适用性。

Method: 提出用再生核希尔伯特空间对控制律学习进行参数化，将控制律视为函数空间元素，设计数据驱动主动学习控制方法，提出TS框架探索潜在最优控制律并给出收敛保证。

Result: 理论分析表明方法能以指数速率学习控制律与闭环性能指标关系，导出控制遗憾上界；数值实验验证了方法有效性。

Conclusion: 所提方法有效解决了TS在控制系统设计中的适用性问题。

Abstract: Thompson sampling (TS) is an effective method to explore parametric
uncertainties and can therefore be used for active learning-based controller
design. However, TS relies on finite parametric representations, which limits
its applicability to more general spaces, which are more commonly encountered
in control system design. To address this issue, this work pro poses a
parameterization method for control law learning using reproducing kernel
Hilbert spaces and designs a data-driven active learning control approach.
Specifically, the proposed method treats the control law as an element in a
function space, allowing the design of control laws without imposing
restrictions on the system structure or the form of the controller. A TS
framework is proposed in this work to explore potential optimal control laws,
and the convergence guarantees are further provided for the learning process.
Theoretical analysis shows that the proposed method learns the relationship
between control laws and closed-loop performance metrics at an exponential
rate, and the upper bound of control regret is also derived. Numerical
experiments on controlling unknown nonlinear systems validate the effectiveness
of the proposed method.

</details>


### [93] [Exploring Modularity of Agentic Systems for Drug Discovery](https://arxiv.org/abs/2506.22189)
*Laura van Weesep,Samuel Genheden,Ola Engkvist,Jens Sjölund*

Main category: cs.LG

TL;DR: 研究基于大语言模型的智能体系统在药物发现中的模块化，比较不同大模型和不同类型智能体表现，指出更换语言模型需考虑提示工程，强调对模块化进一步研究的必要性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型和智能体系统为药物发现带来机遇，但基于大语言模型的智能体系统模块化在药物发现应用中受关注少，因此进行研究。

Method: 比较不同大语言模型的性能，以及工具调用智能体和代码生成智能体的有效性，通过案例研究并使用大语言模型作为评判分数进行评估。

Result: Claude - 3.5 - Sonnet、Claude - 3.7 - Sonnet和GPT - 4o表现优于Llama - 3.1 - 8B等模型；代码生成智能体平均表现更好，但因问题和模型而异；更换系统提示的影响也因问题和模型而异。

Conclusion: 有必要进一步研究智能体系统的模块化，以开发适用于现实问题的稳定可扩展解决方案。

Abstract: Large-language models (LLMs) and agentic systems present exciting
opportunities to accelerate drug discovery and design. In this study, we
critically examine the modularity of LLM-based agentic systems for drug
discovery, i.e., whether parts of the agentic system such as the LLM are
interchangeable, a topic that has received limited attention in drug discovery
applications. We compare the performance of different large language models
(LLMs) and the effectiveness of tool-calling agents versus code-generating
agents in this domain. Our case study, comparing performance in orchestrating
tools for chemistry and drug discovery using an LLM-as-a-judge score, shows
that Claude-3.5-Sonnet, Claude-3.7-Sonnet and GPT-4o outperform alternative
language models such as Llama-3.1-8B, Llama-3.1-70B, GPT-3.5-Turbo, and
Nova-Micro. Although we confirm that code-generating agents outperform the
tool-calling ones on average, we show that this is highly question and model
dependent. Furthermore, the impact of replacing system prompts is dependent on
the specific question asked and the model used, underscoring that -- even in
this particular domain -- one cannot just replace language models without
considering prompt re-engineering. Our study highlights the necessity of
further research into the modularity of agentic systems to enable the
development of stable and scalable solutions for real-world problems.

</details>


### [94] [dreaMLearning: Data Compression Assisted Machine Learning](https://arxiv.org/abs/2506.22190)
*Xiaobo Zhao,Aaron Hurst,Panagiotis Karras,Daniel E. Lucani*

Main category: cs.LG

TL;DR: 本文提出dreaMLearning框架，可从压缩数据中学习，实验表明其能加速训练、减少内存和存储使用，对模型性能影响小。


<details>
  <summary>Details</summary>
Motivation: 机器学习需大量标注数据、计算和存储资源，因此研究能以更少资源实现良好性能的架构。

Method: 引入dreaMLearning框架，基于EntroGeDe压缩方法，可从压缩数据中学习，支持多种数据类型、任务和模型架构。

Result: 在回归和分类任务实验中，dreaMLearning加速训练达8.8倍，减少内存使用10倍，削减存储42%，对模型性能影响极小。

Conclusion: 这些进展增强了多种机器学习应用，为高效可扩展学习带来新可能。

Abstract: Despite rapid advancements, machine learning, particularly deep learning, is
hindered by the need for large amounts of labeled data to learn meaningful
patterns without overfitting and immense demands for computation and storage,
which motivate research into architectures that can achieve good performance
with fewer resources. This paper introduces dreaMLearning, a novel framework
that enables learning from compressed data without decompression, built upon
Entropy-based Generalized Deduplication (EntroGeDe), an entropy-driven lossless
compression method that consolidates information into a compact set of
representative samples. DreaMLearning accommodates a wide range of data types,
tasks, and model architectures. Extensive experiments on regression and
classification tasks with tabular and image data demonstrate that dreaMLearning
accelerates training by up to 8.8x, reduces memory usage by 10x, and cuts
storage by 42%, with a minimal impact on model performance. These advancements
enhance diverse ML applications, including distributed and federated learning,
and tinyML on resource-constrained edge devices, unlocking new possibilities
for efficient and scalable learning.

</details>


### [95] [EFRame: Deeper Reasoning via Exploration-Filtering-Replay Reinforcement Learning Framework](https://arxiv.org/abs/2506.22200)
*Chen Wang,Lai Wei,Yanzhi Zhang,Chenyang Shao,Zedong Dan,Weiran Huang,Yue Wang,Yuzhi Zhang*

Main category: cs.LG

TL;DR: 本文提出EFRame框架改进GRPO，提升强化学习训练的鲁棒性和效率，还能深入分析样本对学习过程的贡献。


<details>
  <summary>Details</summary>
Motivation: GRPO存在探索受限、样本效率低和不稳定问题，限制其在复杂推理任务上的表现。

Method: 引入EFRame框架，通过额外滚动探索高质量轨迹、在线过滤低质量样本、经验回放利用稀有样本，建立完整稳定的学习周期。

Result: 实验表明EFRame提高训练的鲁棒性和效率，使模型获得更深层次的推理能力，还能对训练样本进行更细粒度的分类。

Conclusion: EFRame有效改进了GRPO，在推理任务上有更好的表现，且利于对训练样本的分析。

Abstract: Recent advances in reinforcement learning (RL) have significantly enhanced
the reasoning capabilities of large language models (LLMs). Group Relative
Policy Optimization (GRPO), an efficient variant of PPO that lowers RL's
computational cost, still faces limited exploration, low sample efficiency and
instability, constraining its performance on complex reasoning tasks. To
address these limitations, we introduce EFRame, an Exploration-Filtering-Replay
framework that systematically augments GRPO along three critical dimensions.
EFRame performs additional rollouts to explore high-quality trajectories,
applies online filtering to eliminate low-quality samples that introduce noise
and variance, and leverages experience replay to repeatedly exploit rare but
informative samples. EFRame establishes a complete and stable learning cycle,
guiding the model through a structured transition from exploration to
convergence. Our experiments across a variety of reasoning benchmarks
demonstrate that EFRame not only improves the robustness and efficiency of
training, but also enables access to deeper reasoning capabilities that remain
unattainable under vanilla GRPO. Furthermore, EFRame enables a more
fine-grained categorization of training samples, allowing for a deeper analysis
of how different types of samples contribute to the learning process in RL. Our
code is available at https://github.com/597358816/EFRame.

</details>


### [96] [Risk-Averse Best Arm Set Identification with Fixed Budget and Fixed Confidence](https://arxiv.org/abs/2506.22253)
*Shunta Nonaga,Koji Tabata,Yuta Mizuno,Tamiki Komatsuzaki*

Main category: cs.LG

TL;DR: 本文提出随机多臂老虎机优化新问题，用统一元算法框架解决，有理论保证，实验显示优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决不确定环境下决策时最大化期望回报并最小化风险的问题，传统方法仅关注期望回报。

Method: 提出统一元算法框架，在固定置信度和固定预算制度下运行，通过自适应设计置信区间。

Result: 理论上保证了两种设置下返回解的正确性，实验显示在合成基准上准确性和样本效率优于现有方法。

Conclusion: 该方法在不确定环境下的风险决策任务中具有广泛适用性。

Abstract: Decision making under uncertain environments in the maximization of expected
reward while minimizing its risk is one of the ubiquitous problems in many
subjects. Here, we introduce a novel problem setting in stochastic bandit
optimization that jointly addresses two critical aspects of decision-making:
maximizing expected reward and minimizing associated uncertainty, quantified
via the mean-variance(MV) criterion. Unlike traditional bandit formulations
that focus solely on expected returns, our objective is to efficiently and
accurately identify the Pareto-optimal set of arms that strikes the best
trade-off between expected performance and risk. We propose a unified
meta-algorithmic framework capable of operating under both fixed-confidence and
fixed-budget regimes, achieved through adaptive design of confidence intervals
tailored to each scenario using the same sample exploration strategy. We
provide theoretical guarantees on the correctness of the returned solutions in
both settings. To complement this theoretical analysis, we conduct extensive
empirical evaluations across synthetic benchmarks, demonstrating that our
approach outperforms existing methods in terms of both accuracy and sample
efficiency, highlighting its broad applicability to risk-aware decision-making
tasks in uncertain environments.

</details>


### [97] [Projected Compression: Trainable Projection for Efficient Transformer Compression](https://arxiv.org/abs/2506.22255)
*Maciej Stefaniak,Michał Krutul,Jan Małaśnicki,Maciej Pióro,Jakub Krajewski,Sebastian Jaszczur,Marek Cygan,Kamil Adamczewski,Jan Ludziejewski*

Main category: cs.LG

TL;DR: 大语言模型增大规模带来计算问题，提出投影压缩方法减小模型大小，实验显示其优于对比方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型规模增大导致推理时间和计算需求增加，需要模型大小缩减方法。

Method: 提出投影压缩技术，先训练额外可训练投影权重并保留原模型参数，再将投影合并成低维乘积矩阵得到缩减大小的标准Transformer模型。

Result: 投影压缩在高质量模型上优于可比的硬剪枝和再训练方法，性能差距随令牌数量良好扩展。

Conclusion: 投影压缩是一种有效的模型压缩技术，能在不增加计算开销的情况下减小模型大小并提升性能。

Abstract: Large language models have steadily increased in size to achieve improved
performance; however, this growth has also led to greater inference time and
computational demands. Consequently, there is rising interest in model size
reduction methods. To address this issue, we propose Projected Compression, a
novel model compression technique, that reduces model weights by utilizing
projection modules. Specifically, we first train additional trainable
projections weights and preserve access to all the original model parameters.
Subsequently, these projections are merged into a lower-dimensional product
matrix, resulting in a reduced-size standard Transformer-based model. Unlike
alternative approaches that require additional computational overhead, our
method matches the base model's per-token computation step in FLOPs.
Experimental results show that Projected Compression outperforms the comparable
hard pruning and retraining approach on higher quality models. Moreover, the
performance margin scales well with the number of tokens.

</details>


### [98] [Score-Based Model for Low-Rank Tensor Recovery](https://arxiv.org/abs/2506.22295)
*Zhengyun Cheng,Changhao Wang,Guanwen Zhang,Yi Xu,Wei Zhou,Xiangyang Ji*

Main category: cs.LG

TL;DR: 提出无预定义结构或分布假设的基于分数的模型用于张量分析，结合BCD算法实现张量补全和去噪，实验效果好。


<details>
  <summary>Details</summary>
Motivation: 传统低秩张量分解方法依赖预定义结构假设，实际中先验知识少，基于固定收缩规则的优化过程复杂且易损失精度。

Method: 设计神经网络学习能量函数，通过分数匹配优化以捕获张量元素和共享因子的联合对数概率梯度；结合块坐标下降算法与平滑正则化。

Result: 实验表明在稀疏、连续时间张量及视觉数据等各类张量上性能显著提升。

Conclusion: 提出的方法无需预定义假设，能学习张量与共享因子兼容性，可进行张量补全和去噪，效果良好。

Abstract: Low-rank tensor decompositions (TDs) provide an effective framework for
multiway data analysis. Traditional TD methods rely on predefined structural
assumptions, such as CP or Tucker decompositions. From a probabilistic
perspective, these can be viewed as using Dirac delta distributions to model
the relationships between shared factors and the low-rank tensor. However, such
prior knowledge is rarely available in practical scenarios, particularly
regarding the optimal rank structure and contraction rules. The optimization
procedures based on fixed contraction rules are complex, and approximations
made during these processes often lead to accuracy loss. To address this issue,
we propose a score-based model that eliminates the need for predefined
structural or distributional assumptions, enabling the learning of
compatibility between tensors and shared factors. Specifically, a neural
network is designed to learn the energy function, which is optimized via score
matching to capture the gradient of the joint log-probability of tensor entries
and shared factors. Our method allows for modeling structures and distributions
beyond the Dirac delta assumption. Moreover, integrating the block coordinate
descent (BCD) algorithm with the proposed smooth regularization enables the
model to perform both tensor completion and denoising. Experimental results
demonstrate significant performance improvements across various tensor types,
including sparse and continuous-time tensors, as well as visual data.

</details>


### [99] [CoATA: Effective Co-Augmentation of Topology and Attribute for Graph Neural Networks](https://arxiv.org/abs/2506.22299)
*Tao Liu,Longlong Lin,Yunfeng Yu,Xi Ou,Youan Zhang,Zhiqiu Ye,Tao Jia*

Main category: cs.LG

TL;DR: 提出CoATA框架解决真实图数据噪声和不完整问题，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 真实图数据存在噪声和不完整问题，现有方法单维增强忽略拓扑结构和节点属性间深层联系。

Method: 先传播结构信号丰富和去噪节点属性，再将增强属性空间投影到二分图细化或重构结构，引入对比学习促进增强图和原图相互校正。

Result: 在七个基准数据集上实验，CoATA优于十一个现有基线方法。

Conclusion: CoATA能有效捕捉拓扑和属性间协同关系。

Abstract: Graph Neural Networks (GNNs) have garnered substantial attention due to their
remarkable capability in learning graph representations. However, real-world
graphs often exhibit substantial noise and incompleteness, which severely
degrades the performance of GNNs. Existing methods typically address this issue
through single-dimensional augmentation, focusing either on refining topology
structures or perturbing node attributes, thereby overlooking the deeper
interplays between the two. To bridge this gap, this paper presents CoATA, a
dual-channel GNN framework specifically designed for the Co-Augmentation of
Topology and Attribute. Specifically, CoATA first propagates structural signals
to enrich and denoise node attributes. Then, it projects the enhanced attribute
space into a node-attribute bipartite graph for further refinement or
reconstruction of the underlying structure. Subsequently, CoATA introduces
contrastive learning, leveraging prototype alignment and consistency
constraints, to facilitate mutual corrections between the augmented and
original graphs. Finally, extensive experiments on seven benchmark datasets
demonstrate that the proposed CoATA outperforms eleven state-of-the-art
baseline methods, showcasing its effectiveness in capturing the synergistic
relationship between topology and attributes.

</details>


### [100] [Weakly-Supervised Domain Adaptation with Proportion-Constrained Pseudo-Labeling](https://arxiv.org/abs/2506.22301)
*Takumi Okuo,Shinnosuke Matsuo,Shota Harada,Kiyohito Tanaka,Ryoma Bise*

Main category: cs.LG

TL;DR: 提出利用目标域类别比例信息的弱监督域自适应方法，在两个内镜数据集实验中表现优于半监督域自适应技术，且对有噪声的比例标签有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 领域偏移是机器学习特别是医疗应用中的重大挑战，现有域自适应方法在源域和目标域类别比例不同时表现不佳。

Method: 提出利用目标域类别比例信息的弱监督域自适应方法，基于类别比例为无标签目标数据分配伪标签。

Result: 在两个内镜数据集实验中，即使目标域只有5%有标签，该方法也优于半监督域自适应技术，且对有噪声的比例标签有鲁棒性。

Conclusion: 该方法有效，适用于现实应用场景。

Abstract: Domain shift is a significant challenge in machine learning, particularly in
medical applications where data distributions differ across institutions due to
variations in data collection practices, equipment, and procedures. This can
degrade performance when models trained on source domain data are applied to
the target domain. Domain adaptation methods have been widely studied to
address this issue, but most struggle when class proportions between the source
and target domains differ. In this paper, we propose a weakly-supervised domain
adaptation method that leverages class proportion information from the target
domain, which is often accessible in medical datasets through prior knowledge
or statistical reports. Our method assigns pseudo-labels to the unlabeled
target data based on class proportion (called proportion-constrained
pseudo-labeling), improving performance without the need for additional
annotations. Experiments on two endoscopic datasets demonstrate that our method
outperforms semi-supervised domain adaptation techniques, even when 5% of the
target domain is labeled. Additionally, the experimental results with noisy
proportion labels highlight the robustness of our method, further demonstrating
its effectiveness in real-world application scenarios.

</details>


### [101] [Unfolding Generative Flows with Koopman Operators: Fast and Interpretable Sampling](https://arxiv.org/abs/2506.22304)
*Erkan Turan,Aristotelis Siozopoulos,Maks Ovsjanikov*

Main category: cs.LG

TL;DR: 本文提出结合Koopman算子理论加速条件流匹配（CFM），引入无解码器的Koopman - CFM架构，实现一步采样，在多数据集上提速显著，还能用于分析生成行为。


<details>
  <summary>Details</summary>
Motivation: CFM采样依赖求解非线性ODEs，计算成本高且难解释，现有替代方法未揭示生成过程结构。

Method: 结合Koopman算子理论，引入无解码器的Koopman - CFM架构，在学习的观测空间将生成动态线性化，通过矩阵求幂实现一步采样。

Result: 在2D数据集、MNIST、F - MNIST和TFD等数据集上相比传统CFM有显著提速。

Conclusion: Koopman增强的流匹配结合了采样效率和分析结构，是迈向快速可解释生成建模的潜在一步。

Abstract: Conditional Flow Matching (CFM) offers a simulation-free framework for
training continuous-time generative models, bridging diffusion and flow-based
approaches. However, sampling from CFM still relies on numerically solving
non-linear ODEs which can be computationally expensive and difficult to
interpret. Recent alternatives address sampling speed via trajectory
straightening, mini-batch coupling or distillation. However, these methods
typically do not shed light on the underlying \textit{structure} of the
generative process. In this work, we propose to accelerate CFM and introduce an
interpretable representation of its dynamics by integrating Koopman operator
theory, which models non-linear flows as linear evolution in a learned space of
observables. We introduce a decoder-free Koopman-CFM architecture that learns
an embedding where the generative dynamics become linear, enabling closed-form,
one-step sampling via matrix exponentiation. This results in significant
speedups over traditional CFM as demonstrated on controlled 2D datasets and
real-world benchmarks, MNIST, Fashion-MNIST (F-MNIST), and the Toronto Face
Dataset (TFD). Unlike previous methods, our approach leads to a well-structured
Koopman generator, whose spectral properties, eigenvalues, and eigenfunctions
offer principled tools for analyzing generative behavior such as temporal
scaling, mode stability, and decomposition in Koopman latent space. By
combining sampling efficiency with analytical structure, Koopman-enhanced flow
matching offers a potential step toward fast and interpretable generative
modeling.

</details>


### [102] [A Framework for Multi-source Privacy Preserving Epidemic Analysis](https://arxiv.org/abs/2506.22342)
*Zihan Guan,Zhiyuan Zhao,Fengwei Tian,Dung Nguyen,Payel Bhattacharjee,Ravi Tandon,B. Aditya Prakash,Anil Vullikanti*

Main category: cs.LG

TL;DR: 本文提出结合深度学习和流行病模型的框架，用含差分隐私保护的合成金融数据集进行流行病分析，展示该数据集在预测和学习流行病模型方面的价值。


<details>
  <summary>Details</summary>
Motivation: 多样数据集在流行病学和公共卫生分析中有重要价值，但部分数据集敏感需隐私保护，差分隐私是有效标准，希望在分析中结合多数据集并保障隐私。

Method: 开发一个集成深度学习和流行病模型的框架，结合多数据集进行分析，使用含差分隐私的合成金融数据集验证框架。

Result: 含差分隐私的合成金融数据集在流行病预测和学习模型方面有显著价值。

Conclusion: 含差分隐私的数据集可用于流行病分析，为相关研究提供新的思路和方法。

Abstract: It is now well understood that diverse datasets provide a lot of value in key
epidemiology and public health analyses, such as forecasting and nowcasting,
development of epidemic models, evaluation and design of interventions and
resource allocation. Some of these datasets are often sensitive, and need
adequate privacy protections. There are many models of privacy, but
Differential Privacy (DP) has become a de facto standard because of its strong
guarantees, without making models about adversaries. In this paper, we develop
a framework the integrates deep learning and epidemic models to simultaneously
perform epidemic forecasting and learning a mechanistic model of epidemic
spread, while incorporating multiple datasets for these analyses, including
some with DP guarantees. We demonstrate our framework using a realistic but
synthetic financial dataset with DP; such a dataset has not been used in such
epidemic analyses. We show that this dataset provides significant value in
forecasting and learning an epidemic model, even when used with DP guarantees.

</details>


### [103] [Reinforcement Learning with Physics-Informed Symbolic Program Priors for Zero-Shot Wireless Indoor Navigation](https://arxiv.org/abs/2506.22365)
*Tao Li,Haozhe Lei,Mingsheng Yin,Yaqi Hu*

Main category: cs.LG

TL;DR: 本文提出PiPRL框架用于室内导航，采用神经符号集成方法，实验表明其优于纯符号或神经策略，还能减少训练时间。


<details>
  <summary>Details</summary>
Motivation: 当前结合物理先验归纳偏置的方法需大量人力和专业知识，对普通用户不友好。

Method: 开发PiPRL框架，采用分层和模块化的神经符号集成，元符号程序从神经感知模块获取特征以指导RL过程。

Result: PiPRL始终优于纯符号或神经策略，借助基于程序的归纳偏置减少超26%的训练时间。

Conclusion: 提出的符号方法和PiPRL框架能有效结合物理先验归纳偏置到RL代理中，提升性能和效率。

Abstract: When using reinforcement learning (RL) to tackle physical control tasks,
inductive biases that encode physics priors can help improve sample efficiency
during training and enhance generalization in testing. However, the current
practice of incorporating these helpful physics-informed inductive biases
inevitably runs into significant manual labor and domain expertise, making them
prohibitive for general users. This work explores a symbolic approach to
distill physics-informed inductive biases into RL agents, where the physics
priors are expressed in a domain-specific language (DSL) that is human-readable
and naturally explainable. Yet, the DSL priors do not translate directly into
an implementable policy due to partial and noisy observations and additional
physical constraints in navigation tasks. To address this gap, we develop a
physics-informed program-guided RL (PiPRL) framework with applications to
indoor navigation. PiPRL adopts a hierarchical and modularized neuro-symbolic
integration, where a meta symbolic program receives semantically meaningful
features from a neural perception module, which form the bases for symbolic
programming that encodes physics priors and guides the RL process of a
low-level neural controller. Extensive experiments demonstrate that PiPRL
consistently outperforms purely symbolic or neural policies and reduces
training time by over 26% with the help of the program-based inductive biases.

</details>


### [104] [Sheaf-Based Decentralized Multimodal Learning for Next-Generation Wireless Communication Systems](https://arxiv.org/abs/2506.22374)
*Abdulmomen Ghalkha,Zhuojun Tian,Chaouki Ben Issaid,Mehdi Bennis*

Main category: cs.LG

TL;DR: 提出Sheaf - DMFL和Sheaf - DMFL - Att算法，用于解决传统联邦学习在多模态数据场景的局限，经仿真验证其在异构无线通信系统的优越性。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习算法考虑单模态数据集，无法利用多模态数据信息，不适用于多样模态和不同客户端能力的现实场景。

Method: 提出Sheaf - DMFL框架，利用层理论增强不同模态设备协作；提出Sheaf - DMFL - Att算法，在客户端内引入注意力机制；对Sheaf - DMFL - Att进行收敛分析。

Result: 在真实世界的链路阻塞预测和毫米波波束成形场景进行大量仿真。

Conclusion: 所提算法在异构无线通信系统中具有优越性。

Abstract: In large-scale communication systems, increasingly complex scenarios require
more intelligent collaboration among edge devices collecting various multimodal
sensory data to achieve a more comprehensive understanding of the environment
and improve decision-making accuracy. However, conventional federated learning
(FL) algorithms typically consider unimodal datasets, require identical model
architectures, and fail to leverage the rich information embedded in multimodal
data, limiting their applicability to real-world scenarios with diverse
modalities and varying client capabilities. To address this issue, we propose
Sheaf-DMFL, a novel decentralized multimodal learning framework leveraging
sheaf theory to enhance collaboration among devices with diverse modalities.
Specifically, each client has a set of local feature encoders for its different
modalities, whose outputs are concatenated before passing through a
task-specific layer. While encoders for the same modality are trained
collaboratively across clients, we capture the intrinsic correlations among
clients' task-specific layers using a sheaf-based structure. To further enhance
learning capability, we propose an enhanced algorithm named Sheaf-DMFL-Att,
which tailors the attention mechanism within each client to capture
correlations among different modalities. A rigorous convergence analysis of
Sheaf-DMFL-Att is provided, establishing its theoretical guarantees. Extensive
simulations are conducted on real-world link blockage prediction and mmWave
beamforming scenarios, demonstrate the superiority of the proposed algorithms
in such heterogeneous wireless communication systems.

</details>


### [105] [Probabilistic Optimality for Inference-time Scaling](https://arxiv.org/abs/2506.22376)
*Youkang Wang,Jian Wang,Rubing Chen,Xiao-Yong Wei,Qing Li*

Main category: cs.LG

TL;DR: 提出概率框架和OptScale算法优化大语言模型推理时缩放，减少采样开销并提升推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型推理时缩放的并行采样启发式策略缺乏理论基础。

Method: 提出概率框架推导样本数量理论下限，开发OptScale算法动态确定最优采样响应数，用语言模型预测器估计参数。

Result: 在数学推理基准测试中，OptScale显著减少采样开销，推理性能与现有技术相当或更好。

Conclusion: 为推理时缩放提供理论基础和实用解决方案，解决大语言模型复杂推理高效部署的关键问题。

Abstract: Inference-time scaling has emerged as a powerful technique for enhancing the
reasoning performance of Large Language Models (LLMs). However, existing
approaches often rely on heuristic strategies for parallel sampling, lacking a
principled foundation. To address this gap, we propose a probabilistic
framework that formalizes the optimality of inference-time scaling under the
assumption that parallel samples are independently and identically distributed
(i.i.d.), and where the Best-of-N selection strategy follows a probability
distribution that can be estimated. Within this framework, we derive a
theoretical lower bound on the required number of samples to achieve a target
performance level, providing the first principled guidance for
compute-efficient scaling. Leveraging this insight, we develop
\textsc{OptScale}, a practical algorithm that dynamically determines the
optimal number of sampled responses. \textsc{OptScale} employs a language
model-based predictor to estimate probabilistic prior parameters, enabling the
decision of the minimal number of samples needed that satisfy predefined
performance thresholds and confidence levels. Extensive experiments on
mathematical reasoning benchmarks (including MATH-500, GSM8K, AIME, and AMC)
demonstrate that \textsc{OptScale} significantly reduces sampling overhead
while remaining better or on par with state-of-the-art reasoning performance.
Our work offers both a theoretical foundation and a practical solution for
principled inference-time scaling, addressing a critical gap in the efficient
deployment of LLMs for complex reasoning.

</details>


### [106] [Towards Distributed Neural Architectures](https://arxiv.org/abs/2506.22389)
*Aditya Cowsik,Tianyu He,Andrey Gromov*

Main category: cs.LG

TL;DR: 本文介绍并训练了视觉和语言领域的分布式神经架构（DNA），证明其与密集基线有竞争力，分析了其连接和计算模式。


<details>
  <summary>Details</summary>
Motivation: 引入新的神经架构以实现计算效率、参数共享等，拓展稀疏方法。

Method: 用包含模块和路由器的原型架构初始化DNA，端到端学习模块的计算和通信模式，可添加额外要求到优化目标。

Result: 训练的DNA在两个领域与密集基线有竞争力，能从数据学习计算效率和参数共享，路径呈幂律分布，部分模块有专业化，模型能以可解释方式分配计算和参数。

Conclusion: DNA是一种有效的神经架构，可实现计算效率和参数共享，其计算和连接模式有规律且可解释。

Abstract: We introduce and train distributed neural architectures (DNA) in vision and
language domains. DNAs are initialized with a proto-architecture that consists
of (transformer, MLP, attention, etc.) modules and routers. Any token (or
patch) can traverse any series of modules in any order. DNAs are a natural
generalization of the sparse methods such as Mixture-of-Experts,
Mixture-of-Depths, parameter sharing, etc. Computation and communication
patterns of DNA modules are learnt end-to-end during training and depend on the
content and context of each token (or patch). These patterns can be shaped by
further requirements added to the optimization objective such as compute/memory
efficiency or load balancing. We empirically show that (i) trained DNAs are
competitive with the dense baselines in both domains and (ii) compute
efficiency/parameter sharing can be learnt from data. Next, we analyze the
emergent connectivity and computation patterns in the trained DNAs. We find
that the paths that tokens take through the models are themselves distributed
according to a power-law. We show that some paths (or, equivalently, groups of
modules) show emergent specialization. Finally, we demonstrate that models
learn to allocate compute and active parameters in an interpretable way.

</details>


### [107] [Multi-View Contrastive Learning for Robust Domain Adaptation in Medical Time Series Analysis](https://arxiv.org/abs/2506.22393)
*YongKyung Oh,Alex Bui*

Main category: cs.LG

TL;DR: 提出用多视图对比学习框架处理医疗时间序列跨域适应问题，实验显示优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法处理医疗时间序列跨域适应时，因聚焦孤立特征表示，难以捕捉复杂时间动态。

Method: 提出新框架，利用多视图对比学习整合时间模式、基于导数的动态和频域特征，采用独立编码器和分层融合机制学习特征不变表示。

Result: 在EEG、ECG和EMG等多样医疗数据集上的实验表明，该方法在迁移学习任务中显著优于现有方法。

Conclusion: 该框架提高了机器学习模型的鲁棒性和泛化性，为在不同医疗场景部署可靠AI系统提供了实用途径。

Abstract: Adapting machine learning models to medical time series across different
domains remains a challenge due to complex temporal dependencies and dynamic
distribution shifts. Current approaches often focus on isolated feature
representations, limiting their ability to fully capture the intricate temporal
dynamics necessary for robust domain adaptation. In this work, we propose a
novel framework leveraging multi-view contrastive learning to integrate
temporal patterns, derivative-based dynamics, and frequency-domain features.
Our method employs independent encoders and a hierarchical fusion mechanism to
learn feature-invariant representations that are transferable across domains
while preserving temporal coherence. Extensive experiments on diverse medical
datasets, including electroencephalogram (EEG), electrocardiogram (ECG), and
electromyography (EMG) demonstrate that our approach significantly outperforms
state-of-the-art methods in transfer learning tasks. By advancing the
robustness and generalizability of machine learning models, our framework
offers a practical pathway for deploying reliable AI systems in diverse
healthcare settings.

</details>


### [108] [Exploration from a Primal-Dual Lens: Value-Incentivized Actor-Critic Methods for Sample-Efficient Online RL](https://arxiv.org/abs/2506.22401)
*Tong Yang,Bo Dai,Lin Xiao,Yuejie Chi*

Main category: cs.LG

TL;DR: 本文基于原对偶优化视角提出新的价值激励演员-评论家（VAC）方法处理在线强化学习中探索与利用的权衡问题，且该方法有理论性能保证。


<details>
  <summary>Details</summary>
Motivation: 在线强化学习中平衡探索与利用的权衡是长期挑战，缺乏有理论性能保证的高效实用方案。

Method: 从原对偶优化视角解读乐观原则，提出VAC方法，优化一个整合探索与利用的易优化目标。

Result: VAC方法在有限和无限水平的线性马尔可夫决策过程中有近最优遗憾保证，在适当假设下可扩展到一般函数近似设置。

Conclusion: 提出的VAC方法能有效解决在线强化学习探索与利用权衡问题且有理论性能保证。

Abstract: Online reinforcement learning (RL) with complex function approximations such
as transformers and deep neural networks plays a significant role in the modern
practice of artificial intelligence. Despite its popularity and importance,
balancing the fundamental trade-off between exploration and exploitation
remains a long-standing challenge; in particular, we are still in lack of
efficient and practical schemes that are backed by theoretical performance
guarantees. Motivated by recent developments in exploration via optimistic
regularization, this paper provides an interpretation of the principle of
optimism through the lens of primal-dual optimization. From this fresh
perspective, we set forth a new value-incentivized actor-critic (VAC) method,
which optimizes a single easy-to-optimize objective integrating exploration and
exploitation -- it promotes state-action and policy estimates that are both
consistent with collected data transitions and result in higher value
functions. Theoretically, the proposed VAC method has near-optimal regret
guarantees under linear Markov decision processes (MDPs) in both finite-horizon
and infinite-horizon settings, which can be extended to the general function
approximation setting under appropriate assumptions.

</details>


### [109] [ARMOR: Robust Reinforcement Learning-based Control for UAVs under Physical Attacks](https://arxiv.org/abs/2506.22423)
*Pritam Dash,Ethan Chan,Nathan P. Lawrence,Karthik Pattabiraman*

Main category: cs.LG

TL;DR: 提出攻击弹性、无模型的RL控制器ARMOR，使无人机在对抗性传感器操纵下能稳健运行，实验表明其优于传统方法，提升泛化能力并降低训练成本。


<details>
  <summary>Details</summary>
Motivation: 无人机机载传感器易受物理攻击，现有安全RL方法应对攻击效果不佳。

Method: 采用两阶段训练框架，第一阶段用有特权攻击信息训练教师编码器生成攻击感知潜在状态用于RL策略训练，第二阶段用监督学习训练学生编码器仅用历史传感器数据近似教师潜在状态。

Result: ARMOR性能优于传统方法，确保无人机安全，提升对未知攻击的泛化能力，降低训练成本。

Conclusion: ARMOR能有效实现无人机在对抗性传感器操纵下的稳健运行。

Abstract: Unmanned Aerial Vehicles (UAVs) depend on onboard sensors for perception,
navigation, and control. However, these sensors are susceptible to physical
attacks, such as GPS spoofing, that can corrupt state estimates and lead to
unsafe behavior. While reinforcement learning (RL) offers adaptive control
capabilities, existing safe RL methods are ineffective against such attacks. We
present ARMOR (Adaptive Robust Manipulation-Optimized State Representations),
an attack-resilient, model-free RL controller that enables robust UAV operation
under adversarial sensor manipulation. Instead of relying on raw sensor
observations, ARMOR learns a robust latent representation of the UAV's physical
state via a two-stage training framework. In the first stage, a teacher
encoder, trained with privileged attack information, generates attack-aware
latent states for RL policy training. In the second stage, a student encoder is
trained via supervised learning to approximate the teacher's latent states
using only historical sensor data, enabling real-world deployment without
privileged information. Our experiments show that ARMOR outperforms
conventional methods, ensuring UAV safety. Additionally, ARMOR improves
generalization to unseen attacks and reduces training cost by eliminating the
need for iterative adversarial training.

</details>


### [110] [CLoVE: Personalized Federated Learning through Clustering of Loss Vector Embeddings](https://arxiv.org/abs/2506.22427)
*Randeep Bhatia,Nikos Papadis,Murali Kodialam,TV Lakshman,Sayak Chakrabarty*

Main category: cs.LG

TL;DR: 提出CLoVE算法用于聚类联邦学习，有简单、适用场景广等优势，理论和实验均验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 在聚类联邦学习中，客户分配未知，识别客户聚类具有挑战性。

Method: 利用从客户数据模型损失中得到的客户嵌入，迭代识别和分离不同聚类的客户，并通过联邦聚合优化特定聚类的模型。

Result: 理论上，CLoVE能以高概率在一轮中准确恢复聚类，在线性环境中指数快速收敛到最优模型；实验上，在几轮训练中实现高精度聚类恢复和先进的模型准确率。

Conclusion: CLoVE算法简单、适用场景广、无需近最优模型初始化，更适合实际应用。

Abstract: We propose CLoVE (Clustering of Loss Vector Embeddings), a novel algorithm
for Clustered Federated Learning (CFL). In CFL, clients are naturally grouped
into clusters based on their data distribution. However, identifying these
clusters is challenging, as client assignments are unknown. CLoVE utilizes
client embeddings derived from model losses on client data, and leverages the
insight that clients in the same cluster share similar loss values, while those
in different clusters exhibit distinct loss patterns. Based on these
embeddings, CLoVE is able to iteratively identify and separate clients from
different clusters and optimize cluster-specific models through federated
aggregation. Key advantages of CLoVE over existing CFL algorithms are (1) its
simplicity, (2) its applicability to both supervised and unsupervised settings,
and (3) the fact that it eliminates the need for near-optimal model
initialization, which makes it more robust and better suited for real-world
applications. We establish theoretical convergence bounds, showing that CLoVE
can recover clusters accurately with high probability in a single round and
converges exponentially fast to optimal models in a linear setting. Our
comprehensive experiments comparing with a variety of both CFL and generic
Personalized Federated Learning (PFL) algorithms on different types of datasets
and an extensive array of non-IID settings demonstrate that CLoVE achieves
highly accurate cluster recovery in just a few rounds of training, along with
state-of-the-art model accuracy, across a variety of both supervised and
unsupervised PFL tasks.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [111] [An Effective Two-Phase Genetic Algorithm for Solving the Resource Constrained Project Scheduling Problem (RCPSP)](https://arxiv.org/abs/2506.21915)
*D. Sun,S. Zhou*

Main category: cs.NE

TL;DR: 提出用于解决RCPSP的2-Phase遗传算法(2PGA)，经测试有效且改进了部分最佳启发式解。


<details>
  <summary>Details</summary>
Motivation: 寻找简单有效的方法解决RCPSP问题。

Method: 2PGA分两个阶段进行GA父选择，交替迭代两阶段进行GA进化，阶段1强调局部强化，阶段2强调跳出局部陷阱。

Result: 在PSPLIB标准基准问题上测试，算法有效且改进了部分最佳启发式解。

Conclusion: 2PGA是解决RCPSP问题的有效方法。

Abstract: This note presents a simple and effective variation of genetic algorithm (GA)
for solving RCPSP, denoted as 2-Phase Genetic Algorithm (2PGA). The 2PGA
implements GA parent selection in two phases: Phase-1 includes the best current
solutions in the parent pool, and Phase-2 excludes the best current solutions
from the parent pool. The 2PGA carries out the GA evolution by alternating the
two phases iteratively. In exploring a solution space, the Phase-1 emphasizes
intensification in current neighborhood, while the Phase-2 emphasizes
diversification to escape local traps. The 2PGA was tested on the standard
benchmark problems in PSPLIB, the results have shown that the algorithm is
effective and has improved some of the best heuristic solutions.

</details>


### [112] [In situ fine-tuning of in silico trained Optical Neural Networks](https://arxiv.org/abs/2506.22122)
*Gianluca Kosmella,Ripalta Stabile,Jaron Sanders*

Main category: cs.NE

TL;DR: 本文分析了光学神经网络（ONNs）训练中噪声误指定对性能的影响，提出轻量级算法GIFT缓解性能下降，通过模拟验证其有效性，能提升相对准确率。


<details>
  <summary>Details</summary>
Motivation: ONNs训练依赖简化的计算机模型，映射到物理硬件时因理想模型与实际差异（如噪声和制造缺陷）会引入误差，影响性能。

Method: 提出Gradient - Informed Fine - Tuning (GIFT)算法，利用ONN噪声结构的梯度信息原位调整预训练参数，无需昂贵的重新训练和复杂实验设置。

Result: 在MNIST数字分类任务的五层前馈ONN模拟中，GIFT在噪声误指定下比基线性能实现高达28%的相对准确率提升。

Conclusion: GIFT为弥合简化数字模型与实际ONN实现之间的差距提供了实用解决方案。

Abstract: Optical Neural Networks (ONNs) promise significant advantages over
traditional electronic neural networks, including ultrafast computation, high
bandwidth, and low energy consumption, by leveraging the intrinsic capabilities
of photonics. However, training ONNs poses unique challenges, notably the
reliance on simplified in silico models whose trained parameters must
subsequently be mapped to physical hardware. This process often introduces
inaccuracies due to discrepancies between the idealized digital model and the
physical ONN implementation, particularly stemming from noise and fabrication
imperfections.
  In this paper, we analyze how noise misspecification during in silico
training impacts ONN performance and we introduce Gradient-Informed Fine-Tuning
(GIFT), a lightweight algorithm designed to mitigate this performance
degradation. GIFT uses gradient information derived from the noise structure of
the ONN to adapt pretrained parameters directly in situ, without requiring
expensive retraining or complex experimental setups. GIFT comes with formal
conditions under which it improves ONN performance.
  We also demonstrate the effectiveness of GIFT via simulation on a five-layer
feed forward ONN trained on the MNIST digit classification task. GIFT achieves
up to $28\%$ relative accuracy improvement compared to the baseline performance
under noise misspecification, without resorting to costly retraining. Overall,
GIFT provides a practical solution for bridging the gap between simplified
digital models and real-world ONN implementations.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [113] [Redundant Array Computation Elimination](https://arxiv.org/abs/2506.21960)
*Zixuan Wang,Liang Yuan,Xianmeng Jiang,Kun Li,Junmin Xiao,Yunquan Zhang*

Main category: cs.PF

TL;DR: 本文提出更通用的冗余消除技术RACE，利用两级方案识别循环嵌套中数组引用的数据重用和表达式的计算冗余，能线性时间检测冗余并生成优化代码，实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 以往循环嵌套中数组计算冗余消除工作缺乏通用性，要么关注特定计算模式，要么无法识别复杂结构冗余。

Method: 提出RACE技术，利用两级方案识别数据重用和计算冗余，遍历表达式树线性时间检测冗余，生成带优化辅助数组的代码，支持多种积极策略进行表达式重新关联。

Result: 实验结果证明了RACE的有效性。

Conclusion: RACE是一种更通用且有效的循环嵌套中数组计算冗余消除技术。

Abstract: Redundancy elimination is a key optimization direction, and loop nests are
the main optimization target in modern compilers. Previous work on redundancy
elimination of array computations in loop nests lacks universality. These
approaches either focus on specific computation patterns or fail to recognize
redundancies with complex structures. This paper proposes RACE (Redundant Array
Computation Elimination), a more general redundancy elimination technique. RACE
utilizes a novel two-level scheme to identify the data reuse between array
references and the computation redundancies between expressions. It traverses
the expression trees in loop nests to detect redundancies hierarchically in
linear time and generates efficient code with optimized auxiliary arrays that
store redundant computation results. Furthermore, RACE supports the expression
reassociation with various aggressive strategies to improve the redundancy
opportunities. Experimental results demonstrate the effectiveness of RACE.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [114] [How (Not) To Write a Software Engineering Abstract](https://arxiv.org/abs/2506.21634)
*Lutz Prechelt,Lloyd Montgomery,Julian Frattini,Franz Zieris*

Main category: cs.SE

TL;DR: 研究高质量软件工程会议论文摘要结构，发现多数摘要不理想，结构化摘要更好，并给出相关建议。


<details>
  <summary>Details</summary>
Motivation: 摘要在软件工程研究文章中很有价值，但并非所有摘要都足够有信息性，因此要刻画高质量软件工程会议摘要结构，观察并量化不足，给出撰写建议。

Method: 采用定性开放编码推导概念，确定摘要原型结构；用定量内容分析刻画362篇摘要结构；用探索性数据分析找出常见问题；对比原型与实际结构，推断撰写建议。

Result: 抽样中仅29%摘要完整，结构化摘要完整比例翻倍；仅4%摘要合适，具备良好可读性且无信息、理解差距和高度歧义句子。

Conclusion: 即使顶级会议，多数摘要远非理想；结构化摘要更好；工件中心作品需不同结构化格式；社区应要求摘要给出总结性结论。

Abstract: Background: Abstracts are a particularly valuable element in a software
engineering research article. However, not all abstracts are as informative as
they could be. Objective: Characterize the structure of abstracts in
high-quality software engineering venues. Observe and quantify deficiencies.
Suggest guidelines for writing informative abstracts. Methods: Use qualitative
open coding to derive concepts that explain relevant properties of abstracts.
Identify the archetypical structure of abstracts. Use quantitative content
analysis to objectively characterize abstract structure of a sample of 362
abstracts from five presumably high-quality venues. Use exploratory data
analysis to find recurring issues in abstracts. Compare the archetypical
structure to actual structures. Infer guidelines for producing informative
abstracts. Results: Only 29% of the sampled abstracts are complete, i.e.,
provide background, objective, method, result, and conclusion information. For
structured abstracts, the ratio is twice as big. Only 4% of the abstracts are
proper, i.e., they also have good readability (Flesch-Kincaid score) and have
no informativeness gaps, understandability gaps, nor highly ambiguous
sentences. Conclusions: (1) Even in top venues, a large majority of abstracts
are far from ideal. (2) Structured abstracts tend to be better than
unstructured ones. (3) Artifact-centric works need a different structured
format. (4) The community should start requiring conclusions that generalize,
which currently are often missing in abstracts.

</details>


### [115] [Experience converting a large mathematical software package written in C++ to C++20 modules](https://arxiv.org/abs/2506.21654)
*Wolfgang Bangerth*

Main category: cs.SE

TL;DR: 探讨将大型C++数学软件包转换为C++20模块系统，以deal.II库为例，分析转换方法、挑战及效果，并思考长期转换策略。


<details>
  <summary>Details</summary>
Motivation: 传统C++软件包接口导出方式笨拙、不可靠且缓慢，C++20引入模块系统，需探索如何将现有C++数学软件包转换到该系统。

Method: 以约80万行代码的deal.II有限元库为例，采用一种可从同一代码库提供基于头文件和模块的接口的方法，分析转换过程中遇到的挑战。

Result: 转换到模块系统虽有一定难度但可行，转换后的库编译时间减少，下游项目编译时间无明显趋势。

Conclusion: 对未来数年或数十年将整个数学软件生态系统转换到模块系统提出了思考。

Abstract: Mathematical software has traditionally been built in the form of "packages"
that build on each other. A substantial fraction of these packages is written
in C++ and, as a consequence, the interface of a package is described in the
form of header files that downstream packages and applications can then
#include. C++ has inherited this approach towards exporting interfaces from C,
but the approach is clunky, unreliable, and slow. As a consequence, C++20 has
introduced a "module" system in which packages explicitly export declarations
and code that compilers then store in machine-readable form and that downstream
users can "import" -- a system in line with what many other programming
languages have used for decades.
  Herein, I explore how one can convert large mathematical software packages
written in C++ to this system, using the deal.II finite element library with
its around 800,000 lines of code as an example. I describe an approach that
allows providing both header-based and module-based interfaces from the same
code base, discuss the challenges one encounters, and how modules actually work
in practice in a variety of technical and human metrics. The results show that
with a non-trivial, but also not prohibitive effort, the conversion to modules
is possible, resulting in a reduction in compile time for the converted library
itself; on the other hand, for downstream projects, compile times show no clear
trend. I end with thoughts about long-term strategies for converting the entire
ecosystem of mathematical software over the coming years or decades.

</details>


### [116] [The DevSafeOps Dilemma: A Systematic Literature Review on Rapidity in Safe Autonomous Driving Development and Operation](https://arxiv.org/abs/2506.21693)
*Ali Nouri,Beatriz Cabrero-Daniel,Fredrik Törner,Christian Berger*

Main category: cs.SE

TL;DR: 本文进行了系统文献综述，探讨DevOps在自动驾驶开发中的应用，指出应用DevOps到安全相关AI功能存在挑战及开放问题。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统复杂且需保证安全可靠运行，DevOps方法有望支持AI技术进步和快速响应事故需求。

Method: 进行系统文献综述，识别、分析和综合与DevOps在自动驾驶开发中应用相关的文献。

Result: 对将DevOps应用于安全相关AI功能所产生的挑战和解决方案进行了结构化概述。

Conclusion: 要实现安全的自动驾驶开发的DevOps，仍有几个开放主题有待解决。

Abstract: Developing autonomous driving (AD) systems is challenging due to the
complexity of the systems and the need to assure their safe and reliable
operation. The widely adopted approach of DevOps seems promising to support the
continuous technological progress in AI and the demand for fast reaction to
incidents, which necessitate continuous development, deployment, and
monitoring. We present a systematic literature review meant to identify,
analyse, and synthesise a broad range of existing literature related to usage
of DevOps in autonomous driving development. Our results provide a structured
overview of challenges and solutions, arising from applying DevOps to
safety-related AI-enabled functions. Our results indicate that there are still
several open topics to be addressed to enable safe DevOps for the development
of safe AD.

</details>


### [117] [Using Generative AI in Software Design Education: An Experience Report](https://arxiv.org/abs/2506.21703)
*Victoria Jackson,Susannah Liu,Andre van der Hoek*

Main category: cs.SE

TL;DR: 本文介绍将GenAI引入本科软件设计课程的经验，分析学生使用ChatGPT完成作业的数据，指出其对学生设计的帮助及教育者的教学要点。


<details>
  <summary>Details</summary>
Motivation: 现有研究较少探索GenAI在软件开发其他领域课堂的应用，本文旨在分享将GenAI引入本科软件设计课程的经验。

Method: 要求学生使用ChatGPT完成团队作业，收集对话日志和学生反思，进行定性分析。

Result: 学生发现ChatGPT在设计过程中有诸多帮助，同时认识到使用前需评判其回应；还总结出教育者有效部署GenAI的关键经验。

Conclusion: 学生能从软件设计教育中使用GenAI受益，既有助于设计，也能了解其优缺点。

Abstract: With the rapid adoption of Generative AI (GenAI) tools, software engineering
educators have grappled with how best to incorporate them into the classroom.
While some research discusses the use of GenAI in the context of learning to
code, there is little research that explores the use of GenAI in the classroom
for other areas of software development. This paper provides an experience
report on introducing GenAI into an undergraduate software design class.
Students were required to use GenAI (in the form of ChatGPT) to help complete a
team-based assignment. The data collected consisted of the ChatGPT conversation
logs and students' reflections on using ChatGPT for the assignment.
Subsequently, qualitative analysis was undertaken on the data. Students
identified numerous ways ChatGPT helped them in their design process while
recognizing the need to critique the response before incorporating it into
their design. At the same time, we identified several key lessons for educators
in how to deploy GenAI in a software design class effectively. Based on our
experience, we believe students can benefit from using GenAI in software design
education as it helps them design and learn about the strengths and weaknesses
of GenAI.

</details>


### [118] [KARMA Approach supporting Development Process Reconstruction in Model-based Systems Engineering](https://arxiv.org/abs/2506.22037)
*Jiawei Li,Zan Liang,Guoxin Wang,Jinzhi Lu,Yan Yan,Shouxuan Wu,Hao Wang*

Main category: cs.SE

TL;DR: 本文提出支持开发过程模型的模型重建方法，以解决系统迭代设计中开发需求管理和模型重建问题，通过案例证明可提升设计效率。


<details>
  <summary>Details</summary>
Motivation: 当前系统迭代设计过程中缺乏有效方法管理开发需求变化和实现系统开发过程模型重建。

Method: 1. 用基于GOPPRR - E元建模方法的KARMA语言统一不同建模语言构建的过程模型；2. 引入模型重建框架，以结构化开发需求自然语言为输入，用自然语言处理技术分析需求文本，提取信息，经结构重组和算法优化得到满足需求的开发过程模型。

Result: 以飞机机载维护系统开发过程重建为例，证明该方法能显著提高开发过程设计效率。

Conclusion: 所提出的模型重建方法可有效提升系统开发过程的设计效率。

Abstract: Model reconstruction is a method used to drive the development of complex
system development processes in model-based systems engineering. Currently,
during the iterative design process of a system, there is a lack of an
effective method to manage changes in development requirements, such as
development cycle requirements and cost requirements, and to realize the
reconstruction of the system development process model. To address these
issues, this paper proposes a model reconstruction method to support the
development process model. Firstly, the KARMA language, based on the GOPPRR-E
metamodeling method, is utilized to uniformly formalize the process models
constructed based on different modeling languages. Secondly, a model
reconstruction framework is introduced. This framework takes a structured
development requirements based natural language as input, employs natural
language processing techniques to analyze the development requirements text,
and extracts structural and optimization constraint information. Then, after
structural reorganization and algorithm optimization, a development process
model that meets the development requirements is obtained. Finally, as a case
study, the development process of the aircraft onboard maintenance system is
reconstructed. The results demonstrate that this method can significantly
enhance the design efficiency of the development process.

</details>


### [119] [Autonomic Microservice Management via Agentic AI and MAPE-K Integration](https://arxiv.org/abs/2506.22185)
*Matteo Esposito,Alexander Bakhtin,Noman Ahmad,Mikel Robredo,Ruoyu Su,Valentina Lenarduzzi,Davide Taibi*

Main category: cs.SE

TL;DR: 本文提出基于MAPE - K并利用代理AI的框架解决微服务安全和管理挑战，提供实用方案，可定制以提升系统稳定性等。


<details>
  <summary>Details</summary>
Motivation: 微服务的去中心化特性带来安全和管理挑战，威胁系统稳定性。

Method: 提出基于MAPE - K且利用代理AI的框架进行自主异常检测和修复。

Result: 框架提供了实用、可用于行业的解决方案。

Conclusion: 从业者和研究人员可定制框架来增强系统稳定性、减少停机时间并监测系统质量属性。

Abstract: While microservices are revolutionizing cloud computing by offering
unparalleled scalability and independent deployment, their decentralized nature
poses significant security and management challenges that can threaten system
stability. We propose a framework based on MAPE-K, which leverages agentic AI,
for autonomous anomaly detection and remediation to address the daunting task
of highly distributed system management. Our framework offers practical,
industry-ready solutions for maintaining robust and secure microservices.
Practitioners and researchers can customize the framework to enhance system
stability, reduce downtime, and monitor broader system quality attributes such
as system performance level, resilience, security, and anomaly management,
among others.

</details>


### [120] [Can Large Language Models Help Students Prove Software Correctness? An Experimental Study with Dafny](https://arxiv.org/abs/2506.22370)
*Carolina Carreira,Álvaro Silva,Alexandre Abreu,Alexandra Mendes*

Main category: cs.SE

TL;DR: 研究学生使用ChatGPT解决Dafny形式验证练习的情况，发现用ChatGPT时学生表现更好，但性能提升与提示质量有关，最后给出LLM融入课程的建议。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型（LLMs）在支持如演绎程序验证这类认知要求高的任务中的作用。

Method: 对修读形式方法课程的硕士学生进行混合方法研究，让学生完成两个验证问题，一个可使用定制ChatGPT接口并记录交互，另一个不使用。

Result: 学生使用ChatGPT时表现明显更好，但性能提升与提示质量相关。

Conclusion: 给出将LLMs更有效地融入形式方法课程的实用建议，如设计促进学习而非替代的LLM感知挑战。

Abstract: Students in computing education increasingly use large language models (LLMs)
such as ChatGPT. Yet, the role of LLMs in supporting cognitively demanding
tasks, like deductive program verification, remains poorly understood. This
paper investigates how students interact with an LLM when solving formal
verification exercises in Dafny, a language that supports functional
correctness, by allowing programmers to write formal specifications and
automatically verifying that the implementation satisfies the specification. We
conducted a mixed-methods study with master's students enrolled in a formal
methods course. Each participant completed two verification problems, one with
access to a custom ChatGPT interface, that logged all interactions, and the
other without. We identified strategies used by successful students and
assessed the level of trust students place in LLMs. %\todo{Our findings show
that something here} Our findings show that students perform significantly
better when using ChatGPT; however, performance gains are tied to prompt
quality. We conclude with practical recommendations for integrating LLMs into
formal methods courses more effectively, including designing LLM-aware
challenges that promote learning rather than substitution.

</details>


### [121] [What Makes ChatGPT Effective for Software Issue Resolution? An Empirical Study of Developer-ChatGPT Conversations in GitHub](https://arxiv.org/abs/2506.22390)
*Ramtin Ehsani,Sakshi Pathak,Esteban Parra,Sonia Haiduc,Preetha Chatterjee*

Main category: cs.SE

TL;DR: 分析686个开发者与ChatGPT对话，发现仅62%对话对解决问题有帮助，指出ChatGPT适用场景、有效对话特征等，为工具设计和开发者交互提供建议。


<details>
  <summary>Details</summary>
Motivation: 并非所有开发者与大语言模型对话都对有效解决问题有帮助，需找出使对话有效的特征。

Method: 分析对话和对应问题区分有用与无用对话；对开发者求助任务分类；考察多种指标挖掘有用对话相关因素；识别无用回复常见缺陷。

Result: 62%对话对解决问题有帮助；ChatGPT在代码生成和推荐方面有效，解释代码有困难；有用对话特点为简短、易读等；大型热门项目和经验丰富开发者受益多；常见缺陷是信息错误和不全面。

Conclusion: 研究结果对开发者交互策略、工具框架开发和大语言模型微调有广泛启示。

Abstract: Conversational large-language models are extensively used for issue
resolution tasks. However, not all developer-LLM conversations are useful for
effective issue resolution. In this paper, we analyze 686 developer-ChatGPT
conversations shared within GitHub issue threads to identify characteristics
that make these conversations effective for issue resolution. First, we analyze
the conversations and their corresponding issues to distinguish helpful from
unhelpful conversations. We begin by categorizing the types of tasks developers
seek help with to better understand the scenarios in which ChatGPT is most
effective. Next, we examine a wide range of conversational, project, and
issue-related metrics to uncover factors associated with helpful conversations.
Finally, we identify common deficiencies in unhelpful ChatGPT responses to
highlight areas that could inform the design of more effective developer-facing
tools. We found that only 62% of the ChatGPT conversations were helpful for
successful issue resolution. ChatGPT is most effective for code generation and
tools/libraries/APIs recommendations, but struggles with code explanations.
Helpful conversations tend to be shorter, more readable, and exhibit stronger
semantic and linguistic alignment. Larger, more popular projects and more
experienced developers benefit more from ChatGPT. At the issue level, ChatGPT
performs best on simpler problems with limited developer activity and faster
resolution, typically well-scoped tasks like compilation errors. The most
common deficiencies in unhelpful ChatGPT responses include incorrect
information and lack of comprehensiveness. Our findings have wide implications
including guiding developers on effective interaction strategies for issue
resolution, informing the development of tools or frameworks to support optimal
prompt design, and providing insights on fine-tuning LLMs for issue resolution
tasks.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [122] [Modification of a Numerical Method Using FIR Filters in a Time-dependent SIR Model for COVID-19](https://arxiv.org/abs/2506.21739)
*Felipe Rogério Pimentel,Rafael Gustavo Alves*

Main category: stat.ML

TL;DR: 本文提出修改算法获取岭系数，用于追踪和预测巴西米纳斯吉拉斯州新冠感染和康复人数，模拟结果显示修改算法近似误差更小。


<details>
  <summary>Details</summary>
Motivation: 在无疫苗、靠隔离防传染的背景下，改进现有算法以更好追踪和预测新冠感染与康复人数。

Method: 提出对Chen等人算法的小修改，设置不同的FIR滤波器阶数和正则化参数，用修改算法进行追踪和预测，并对比预测数据与真实数据。

Result: 修改算法在一些模拟中呈现出比Chen等人算法更好的近似误差。

Conclusion: 修改后的算法在追踪和预测新冠感染与康复人数方面有更好的表现。

Abstract: Authors Yi-Cheng Chen, Ping-En Lu, Cheng-Shang Chang, and Tzu-Hsuan Liu use
the Finite Impulse Response (FIR) linear system filtering method to track and
predict the number of people infected and recovered from COVID-19, in a
pandemic context in which there was still no vaccine and the only way to avoid
contagion was isolation. To estimate the coefficients of these FIR filters,
Chen et al. used machine learning methods through a classical optimization
problem with regularization (ridge regression). These estimated coefficients
are called ridge coefficients. The epidemic mathematical model adopted by these
researchers to formulate the FIR filters is the time-dependent discrete SIR. In
this paper, we propose a small modification to the algorithm of Chen et al. to
obtain the ridge coefficients. We then used this modified algorithm to track
and predict the number of people infected and recovered from COVID-19 in the
state of Minas Gerais/Brazil, within a prediction window, during the initial
period of the pandemic. We also compare the predicted data with the respective
real data to check how good the approximation is. In the modified algorithm, we
set values for the FIR filter orders and for the regularization parameters,
both different from the respective values defined by Chen et al. in their
algorithm. In this context, the numerical results obtained by the modified
algorithm in some simulations present better approximation errors compared to
the respective approximation errors presented by the algorithm of Chen et al.

</details>


### [123] [Critically-Damped Higher-Order Langevin Dynamics](https://arxiv.org/abs/2506.21741)
*Benjamin Sterling,Chad Gueli,Mónica F. Bugallo*

Main category: stat.ML

TL;DR: 本文提出将系统分析中的临界阻尼概念引入高阶朗之万动力学（HOLD），推广该扩散方法。


<details>
  <summary>Details</summary>
Motivation: 去噪扩散概率模型有新的待探索类别，临界阻尼在部分动力学中成功应用，但未应用于任意阶动力学。

Method: 将系统分析中的临界阻尼概念引入HOLD方法。

Result: 未提及。

Conclusion: 未提及。

Abstract: Denoising Diffusion Probabilistic Models represent an entirely new class of
generative AI methods that have yet to be fully explored. Critical damping has
been successfully introduced in Critically-Damped Langevin Dynamics (CLD) and
Critically-Damped Third-Order Langevin Dynamics (TOLD++), but has not yet been
applied to dynamics of arbitrary order. The proposed line of work generalizes
Higher-Order Langevin Dynamics (HOLD), a recent state-of-the-art diffusion
method, by introducing the concept of critical damping from systems analysis.

</details>


### [124] [TADA: Improved Diffusion Sampling with Training-free Augmented Dynamics](https://arxiv.org/abs/2506.21757)
*Tianrong Chen,Huangjie Zheng,David Berthelot,Jiatao Gu,Josh Susskind,Shuangfei Zhai*

Main category: stat.ML

TL;DR: 本文提出一种新的扩散模型采样方法，比现有技术快186%，训练免费，能在少函数评估下生成更细节样本，在多模型上表现出色。


<details>
  <summary>Details</summary>
Motivation: 扩散模型采样效率低，虽有求解器设计和噪声调度策略改进，但仍需更高效方法。

Method: 使用高维初始噪声的无训练常微分方程（ODE）求解器，通过建立动量扩散模型和传统扩散模型的等价关系利用动量动力学。

Result: 新方法比现有技术快186%，可通过超参数控制细节，在多个代表性预训练扩散模型上表现良好。

Conclusion: 提出的新采样方法高效且有效，可在不同类型预训练扩散模型上提升性能。

Abstract: Diffusion models have demonstrated exceptional capabilities in generating
high-fidelity images but typically suffer from inefficient sampling. Many
solver designs and noise scheduling strategies have been proposed to
dramatically improve sampling speeds. In this paper, we introduce a new
sampling method that is up to $186\%$ faster than the current state of the art
solver for comparative FID on ImageNet512. This new sampling method is
training-free and uses an ordinary differential equation (ODE) solver. The key
to our method resides in using higher-dimensional initial noise, allowing to
produce more detailed samples with less function evaluations from existing
pretrained diffusion models. In addition, by design our solver allows to
control the level of detail through a simple hyper-parameter at no extra
computational cost. We present how our approach leverages momentum dynamics by
establishing a fundamental equivalence between momentum diffusion models and
conventional diffusion models with respect to their training paradigms.
Moreover, we observe the use of higher-dimensional noise naturally exhibits
characteristics similar to stochastic differential equations (SDEs). Finally,
we demonstrate strong performances on a set of representative pretrained
diffusion models, including EDM, EDM2, and Stable-Diffusion 3, which cover
models in both pixel and latent spaces, as well as class and text conditional
settings. The code is available at https://github.com/apple/ml-tada.

</details>


### [125] [Classification with Reject Option: Distribution-free Error Guarantees via Conformal Prediction](https://arxiv.org/abs/2506.21802)
*Johan Hallberg Szabadváry,Tuwe Löfström,Ulf Johansson,Cecilia Sönströd,Ernst Ahlberg,Lars Carlsson*

Main category: stat.ML

TL;DR: 本文形式化了二元分类中带拒绝选项的机器学习方法，通过共形预测（CP）实现并推导了误差率，给出有限样本估计，用数值例子说明，还展示了误差 - 拒绝曲线。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习模型总会给出预测，即使可能错误，带拒绝选项的机器学习可解决此问题。

Method: 利用共形预测（CP），将 CP 框架用于带拒绝选项的预测，接受单例预测将 CP 转化为带拒绝选项的二元分类器。

Result: 推导了误差率并给出有限样本估计，用不同 CP 设置的数值例子说明误差率，展示了误差 - 拒绝曲线。

Conclusion: 该方法能帮助用户在实际中设置可接受的误差率或拒绝率。

Abstract: Machine learning (ML) models always make a prediction, even when they are
likely to be wrong. This causes problems in practical applications, as we do
not know if we should trust a prediction. ML with reject option addresses this
issue by abstaining from making a prediction if it is likely to be incorrect.
In this work, we formalise the approach to ML with reject option in binary
classification, deriving theoretical guarantees on the resulting error rate.
This is achieved through conformal prediction (CP), which produce prediction
sets with distribution-free validity guarantees. In binary classification, CP
can output prediction sets containing exactly one, two or no labels. By
accepting only the singleton predictions, we turn CP into a binary classifier
with reject option.
  Here, CP is formally put in the framework of predicting with reject option.
We state and prove the resulting error rate, and give finite sample estimates.
Numerical examples provide illustrations of derived error rate through several
different conformal prediction settings, ranging from full conformal prediction
to offline batch inductive conformal prediction. The former has a direct link
to sharp validity guarantees, whereas the latter is more fuzzy in terms of
validity guarantees but can be used in practice. Error-reject curves illustrate
the trade-off between error rate and reject rate, and can serve to aid a user
to set an acceptable error rate or reject rate in practice.

</details>


### [126] [Thompson Sampling in Function Spaces via Neural Operators](https://arxiv.org/abs/2506.21894)
*Rafael Oliveira,Xuesong Wang,Kian Ming A. Chai,Edwin V. Bonilla*

Main category: stat.ML

TL;DR: 提出汤普森采样在函数空间优化问题的扩展算法，用样本 - 优化策略，有理论收敛保证，在基准测试中表现良好。


<details>
  <summary>Details</summary>
Motivation: 解决函数空间优化问题，处理已知泛函但算子查询成本高的情况。

Method: 采用样本 - 优化方法，用神经算子替代物，将训练好的神经算子视为高斯过程近似样本。

Result: 在涉及偏微分方程等功能优化任务基准测试中，展示出更好的样本效率和有竞争力的性能。

Conclusion: 该扩展算法有效，在特定优化问题中有优势，有理论保证。

Abstract: We propose an extension of Thompson sampling to optimization problems over
function spaces where the objective is a known functional of an unknown
operator's output. We assume that functional evaluations are inexpensive, while
queries to the operator (such as running a high-fidelity simulator) are costly.
Our algorithm employs a sample-then-optimize approach using neural operator
surrogates. This strategy avoids explicit uncertainty quantification by
treating trained neural operators as approximate samples from a Gaussian
process. We provide novel theoretical convergence guarantees, based on Gaussian
processes in the infinite-dimensional setting, under minimal assumptions. We
benchmark our method against existing baselines on functional optimization
tasks involving partial differential equations and other nonlinear
operator-driven phenomena, demonstrating improved sample efficiency and
competitive performance.

</details>


### [127] [Hybrid Generative Modeling for Incomplete Physics: Deep Grey-Box Meets Optimal Transport](https://arxiv.org/abs/2506.22204)
*Gurjeet Sangra Singh,Maciej Falkiewicz,Alexandros Kalousis*

Main category: stat.ML

TL;DR: 文章提出结合深度灰盒建模与最优传输方法的混合生成模型，利用有限非配对数据完善物理模型，实验验证了其在生成任务和模型透明度上的有效性。


<details>
  <summary>Details</summary>
Motivation: 许多现实系统的物理方程存在缺失或未知项，导致物理模型分布与真实数据生成过程不同，需利用有限非配对数据完善模型。

Method: 提出结合深度灰盒建模与最优传输方法的混合生成模型，在数据空间实现最优传输映射并保持源分布最小失真。

Result: 实验验证了方法在生成任务和模型透明度上的有效性，能提供对学习到的物理动力学的详细见解。

Conclusion: 该方法能有效解决非配对问题，准确学习系统动力学并保持可解释性。

Abstract: Physics phenomena are often described by ordinary and/or partial differential
equations (ODEs/PDEs), and solved analytically or numerically. Unfortunately,
many real-world systems are described only approximately with missing or
unknown terms in the equations. This makes the distribution of the physics
model differ from the true data-generating process (DGP). Using limited and
unpaired data between DGP observations and the imperfect model simulations, we
investigate this particular setting by completing the known-physics model,
combining theory-driven models and data-driven to describe the shifted
distribution involved in the DGP. We present a novel hybrid generative model
approach combining deep grey-box modelling with Optimal Transport (OT) methods
to enhance incomplete physics models. Our method implements OT maps in data
space while maintaining minimal source distribution distortion, demonstrating
superior performance in resolving the unpaired problem and ensuring correct
usage of physics parameters. Unlike black-box alternatives, our approach
leverages physics-based inductive biases to accurately learn system dynamics
while preserving interpretability through its domain knowledge foundation.
Experimental results validate our method's effectiveness in both generation
tasks and model transparency, offering detailed insights into learned physics
dynamics.

</details>


### [128] [Uncovering smooth structures in single-cell data with PCS-guided neighbor embeddings](https://arxiv.org/abs/2506.22228)
*Rong Ma,Xi Li,Jingyuan Hu,Bin Yu*

Main category: stat.ML

TL;DR: 本文针对单细胞测序数据高维有噪特点，评估现有邻域嵌入算法缺点，提出NESS方法改进低维表示，应用于多数据集获生物学见解。


<details>
  <summary>Details</summary>
Motivation: 现有邻域嵌入算法处理单细胞数据有失真问题，评估方法难捕捉连续细胞状态转变，动态建模有强假设和数据要求。

Method: 基于PCS框架，系统评估流行NE算法缺点，提出NESS机器学习方法，利用算法稳定性改进NE表示。

Result: NESS提供有用概念、稳定性指标和计算流程，应用于六个单细胞数据集，获得细胞状态识别和转录动力学量化等生物学见解。

Conclusion: NESS能改进单细胞数据的NE表示，有效揭示发育轨迹和细胞状态转变。

Abstract: Single-cell sequencing is revolutionizing biology by enabling detailed
investigations of cell-state transitions. Many biological processes unfold
along continuous trajectories, yet it remains challenging to extract smooth,
low-dimensional representations from inherently noisy, high-dimensional
single-cell data. Neighbor embedding (NE) algorithms, such as t-SNE and UMAP,
are widely used to embed high-dimensional single-cell data into low dimensions.
But they often introduce undesirable distortions, resulting in misleading
interpretations. Existing evaluation methods for NE algorithms primarily focus
on separating discrete cell types rather than capturing continuous cell-state
transitions, while dynamic modeling approaches rely on strong assumptions about
cellular processes and specialized data. To address these challenges, we build
on the Predictability-Computability-Stability (PCS) framework for reliable and
reproducible data-driven discoveries. First, we systematically evaluate popular
NE algorithms through empirical analysis, simulation, and theory, and reveal
their key shortcomings, such as artifacts and instability. We then introduce
NESS, a principled and interpretable machine learning approach to improve NE
representations by leveraging algorithmic stability and to enable robust
inference of smooth biological structures. NESS offers useful concepts,
quantitative stability metrics, and efficient computational workflows to
uncover developmental trajectories and cell-state transitions in single-cell
data. Finally, we apply NESS to six single-cell datasets, spanning pluripotent
stem cell differentiation, organoid development, and multiple tissue-specific
lineage trajectories. Across these diverse contexts, NESS consistently yields
useful biological insights, such as identification of transitional and stable
cell states and quantification of transcriptional dynamics during development.

</details>


### [129] [Optimal Estimation of Watermark Proportions in Hybrid AI-Human Texts](https://arxiv.org/abs/2506.22343)
*Xiang Li,Garrett Wen,Weiqing He,Jiayuan Wu,Qi Long,Weijie J. Su*

Main category: stat.ML

TL;DR: 本文聚焦混合源文本中水印比例的最优估计问题，提出有效估计器并证明其达到极小极大下界，评估显示估计器准确性高。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注文本整体是否有水印，而现实中存在大量混合源文本，需解决混合源文本中水印比例的估计问题。

Method: 将问题转化为基于关键统计量的混合模型中比例参数的估计，分析参数可识别性，为采用连续关键统计量的水印方法提出有效估计器，并推导极小极大下界。

Result: 提出的估计器在合成数据和开源模型生成的混合源文本评估中，始终能达到较高的估计精度。

Conclusion: 提出的估计器能有效解决混合源文本中水印比例的估计问题，具有较高准确性。

Abstract: Text watermarks in large language models (LLMs) are an increasingly important
tool for detecting synthetic text and distinguishing human-written content from
LLM-generated text. While most existing studies focus on determining whether
entire texts are watermarked, many real-world scenarios involve mixed-source
texts, which blend human-written and watermarked content. In this paper, we
address the problem of optimally estimating the watermark proportion in
mixed-source texts. We cast this problem as estimating the proportion parameter
in a mixture model based on \emph{pivotal statistics}. First, we show that this
parameter is not even identifiable in certain watermarking schemes, let alone
consistently estimable. In stark contrast, for watermarking methods that employ
continuous pivotal statistics for detection, we demonstrate that the proportion
parameter is identifiable under mild conditions. We propose efficient
estimators for this class of methods, which include several popular unbiased
watermarks as examples, and derive minimax lower bounds for any measurable
estimator based on pivotal statistics, showing that our estimators achieve
these lower bounds. Through evaluations on both synthetic data and mixed-source
text generated by open-source models, we demonstrate that our proposed
estimators consistently achieve high estimation accuracy.

</details>


### [130] [Beyond ReLU: How Activations Affect Neural Kernels and Random Wide Networks](https://arxiv.org/abs/2506.22429)
*David Holzmüller,Max Schölpple*

Main category: stat.ML

TL;DR: 本文针对常见激活函数，对神经切线核（NTK）和神经网络高斯过程核（NNGP）的再生核希尔伯特空间（RKHS）进行更一般的刻画，并得出不同激活函数的相关结果及NNGP样本路径的平滑性结果。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习理论大多局限于ReLU激活函数，对于除ReLU幂函数外的大多数激活函数，NTK和NNGP的性质理解不足。

Method: 对仅在零点非光滑的典型激活函数（如SELU、ELU、LeakyReLU等）的NTK和NNGP的RKHS进行分析，还涵盖缺失偏置、两层网络、多项式激活等特殊情况。

Result: 一类非无限光滑的激活函数在不同网络深度生成等价的RKHS，多项式激活函数生成非等价的RKHS，还得出NNGP样本路径的平滑性结果。

Conclusion: 完成了对常见激活函数下NTK和NNGP的RKHS的更一般刻画，明确了不同激活函数在网络深度和样本路径平滑性方面的特性。

Abstract: While the theory of deep learning has made some progress in recent years,
much of it is limited to the ReLU activation function. In particular, while the
neural tangent kernel (NTK) and neural network Gaussian process kernel (NNGP)
have given theoreticians tractable limiting cases of fully connected neural
networks, their properties for most activation functions except for powers of
the ReLU function are poorly understood. Our main contribution is to provide a
more general characterization of the RKHS of these kernels for typical
activation functions whose only non-smoothness is at zero, such as SELU, ELU,
or LeakyReLU. Our analysis also covers a broad set of special cases such as
missing biases, two-layer networks, or polynomial activations. Our results show
that a broad class of not infinitely smooth activations generate equivalent
RKHSs at different network depths, while polynomial activations generate
non-equivalent RKHSs. Finally, we derive results for the smoothness of NNGP
sample paths, characterizing the smoothness of infinitely wide neural networks
at initialization.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [131] [rodeo: Probabilistic Methods of Parameter Inference for Ordinary Differential Equations](https://arxiv.org/abs/2506.21776)
*Mohan Wu,Martin Lysy*

Main category: stat.CO

TL;DR: 介绍Python库rodeo，其为概率ODE求解器提供接口，可对ODE系统进行快速、准确和可扩展的参数推断。


<details>
  <summary>Details</summary>
Motivation: 概率ODE求解器在处理ODE参数估计时能更好考虑数值不确定性，传统确定性求解器有局限，需要更好工具。

Method: 开发rodeo库，提供概率求解器，利用自动微分和即时编译技术。

Result: rodeo求解器在评估点数量和系统变量上呈线性扩展，在多个例子中能为多种ODE系统提供快速、准确和可扩展的参数推断。

Conclusion: rodeo库是一个快速、轻量级且可扩展的工具，能有效进行ODE系统的参数推断。

Abstract: Parameter estimation for ordinary differential equations (ODEs) plays a
fundamental role in the analysis of dynamical systems. Generally lacking
closed-form solutions, ODEs are traditionally approximated using deterministic
solvers. However, there is a growing body of evidence to suggest that
probabilistic ODE solvers produce more reliable parameter estimates by better
accounting for numerical uncertainty. Here we present rodeo, a Python library
providing a fast, lightweight, and extensible interface to a broad class of
probabilistic ODE solvers, along with several associated methods for parameter
inference. At its core, rodeo provides a probabilistic solver that scales
linearly in both the number of evaluation points and system variables.
Furthermore, by leveraging state-of-the-art automatic differentiation (AD) and
just-in-time (JIT) compiling techniques, rodeo is shown across several examples
to provide fast, accurate, and scalable parameter inference for a variety of
ODE systems.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [132] [A Practical Approach to Power Saving in Hearables Using Sub-Nyquist Sampling with Bandwidth Extension](https://arxiv.org/abs/2506.22321)
*Tarikul Islam Tamiti,Anomadarshi Barua*

Main category: cs.SD

TL;DR: 提出SUBARU方法，在可听设备上实现低功耗多模态语音增强。


<details>
  <summary>Details</summary>
Motivation: 现有可听设备低功耗实现方案未考虑降低采样频率和比特分辨率对处理和语音增强的影响、不使用GAN鉴别器实现类GAN音频质量、亚奈奎斯特采样率信号处理等实际问题。

Method: 提出SUBARU方法，有意在ADC中使用亚奈奎斯特采样和低比特分辨率，引入新型多尺度和多周期虚拟鉴别器。

Result: 功率消耗降低3.31倍，不使用GAN对抗训练实现类GAN音频质量，在移动平台实现流式操作，推理时间1.74ms，内存占用小于13.77MB。

Conclusion: SUBARU方法能有效解决现有可听设备低功耗实现的问题，在多模态语音增强方面有良好效果。

Abstract: Hearables are wearable computers that are worn on the ear. Bone conduction
microphones (BCMs) are used with air conduction microphones (ACMs) in hearables
as a supporting modality for multimodal speech enhancement (SE) in noisy
conditions. However, existing works don't consider the following practical
aspects for low-power implementations on hearables: (i) They do not explore how
lowering the sampling frequencies and bit resolutions in analog-to-digital
converters (ADCs) of hearables jointly impact low-power processing and
multimodal SE in terms of speech quality and intelligibility. (ii) They don't
discuss how GAN-like audio quality can be achieved without using actual GAN
discriminators. And (iii) They don't process signals from ACMs/BCMs at
sub-Nyquist sampling rate because, in their frameworks, they lack a wideband
reconstruction methodology from their narrowband parts. We propose SUBARU
(\textbf{Sub}-Nyquist \textbf{A}udio \textbf{R}esolution \textbf{U}psampling),
which achieves the following: SUBARU (i) intentionally uses sub-Nyquist
sampling and low bit resolution in ADCs, achieving a 3.31x reduction in power
consumption; (ii) introduces novel multi-scale and multi-period virtual
discriminators, which achieve GAN-like audio quality without using GANs'
adversarial training; and (iii) achieves streaming operations on mobile
platforms and SE in in-the-wild noisy conditions with an inference time of
1.74ms and a memory footprint of less than 13.77MB.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [133] [Adversarial Threats in Quantum Machine Learning: A Survey of Attacks and Defenses](https://arxiv.org/abs/2506.21842)
*Archisman Ghosh,Satwik Kundu,Swaroop Ghosh*

Main category: quant-ph

TL;DR: 本文探讨量子机器学习（QML）安全问题，分析攻击手段与防御机制，指出待解决挑战并给出研究路线图。


<details>
  <summary>Details</summary>
Motivation: QML快速发展在NISQ时代带来安全挑战，需研究其安全问题。

Method: 分析QML系统独特的对抗性威胁，研究关键攻击向量；利用量子特性设计防御机制，如用噪声签名作水印、硬件感知混淆技术等；借鉴经典对抗训练和差分隐私到量子环境。

Result: 明确QML系统多种攻击手段，提出对应防御机制。

Conclusion: 保障QML安全需解决平衡噪声水平、缓解跨平台攻击、建立量子 - 经典信任框架等挑战，为构建可靠QML系统提供方向。

Abstract: Quantum Machine Learning (QML) integrates quantum computing with classical
machine learning, primarily to solve classification, regression and generative
tasks. However, its rapid development raises critical security challenges in
the Noisy Intermediate-Scale Quantum (NISQ) era. This chapter examines
adversarial threats unique to QML systems, focusing on vulnerabilities in
cloud-based deployments, hybrid architectures, and quantum generative models.
Key attack vectors include model stealing via transpilation or output
extraction, data poisoning through quantum-specific perturbations, reverse
engineering of proprietary variational quantum circuits, and backdoor attacks.
Adversaries exploit noise-prone quantum hardware and insufficiently secured
QML-as-a-Service (QMLaaS) workflows to compromise model integrity, ownership,
and functionality. Defense mechanisms leverage quantum properties to counter
these threats. Noise signatures from training hardware act as non-invasive
watermarks, while hardware-aware obfuscation techniques and ensemble strategies
disrupt cloning attempts. Emerging solutions also adapt classical adversarial
training and differential privacy to quantum settings, addressing
vulnerabilities in quantum neural networks and generative architectures.
However, securing QML requires addressing open challenges such as balancing
noise levels for reliability and security, mitigating cross-platform attacks,
and developing quantum-classical trust frameworks. This chapter summarizes
recent advances in attacks and defenses, offering a roadmap for researchers and
practitioners to build robust, trustworthy QML systems resilient to evolving
adversarial landscapes.

</details>


### [134] [Robust quantum reservoir computers for forecasting chaotic dynamics: generalized synchronization and stability](https://arxiv.org/abs/2506.22335)
*Osama Ahmed,Felix Tennie,Luca Magri*

Main category: quant-ph

TL;DR: 研究表明循环量子储层计算机（QRCs）及其无循环架构（RF - QRCs）是从时间序列数据中学习和预测混沌动力学的强大工具，分析了其原理、设计方法及噪声影响。


<details>
  <summary>Details</summary>
Motivation: 探索用于学习和预测混沌动力学的强大工具，为在近期量子硬件上设计用于混沌时间序列预测的量子机器提供可能。

Method: 将量子储层计算机表述为耦合动力系统，推导量子储层更新的雅可比矩阵，利用广义同步工具设计，提出GS = ESP准则。

Result: 量子储层计算机能学习混沌动力学及其不变性质，RF - QRCs满足GS = ESP，噪声耗散增强其鲁棒性，不同维度系统的数值验证支持结论。

Conclusion: QRCs和RF - QRCs是学习和预测混沌动力学的强大工具，为设计用于混沌时间序列预测的鲁棒量子机器提供了机会。

Abstract: We show that recurrent quantum reservoir computers (QRCs) and their
recurrence-free architectures (RF-QRCs) are robust tools for learning and
forecasting chaotic dynamics from time-series data. First, we formulate and
interpret quantum reservoir computers as coupled dynamical systems, where the
reservoir acts as a response system driven by training data; in other words,
quantum reservoir computers are generalized-synchronization (GS) systems.
Second, we show that quantum reservoir computers can learn chaotic dynamics and
their invariant properties, such as Lyapunov spectra, attractor dimensions, and
geometric properties such as the covariant Lyapunov vectors. This analysis is
enabled by deriving the Jacobian of the quantum reservoir update. Third, by
leveraging tools from generalized synchronization, we provide a method for
designing robust quantum reservoir computers. We propose the criterion
$GS=ESP$: GS implies the echo state property (ESP), and vice versa. We
analytically show that RF-QRCs, by design, fulfill $GS=ESP$. Finally, we
analyze the effect of simulated noise. We find that dissipation from noise
enhances the robustness of quantum reservoir computers. Numerical verifications
on systems of different dimensions support our conclusions. This work opens
opportunities for designing robust quantum machines for chaotic time series
forecasting on near-term quantum hardware.

</details>


### [135] [QuKAN: A Quantum Circuit Born Machine approach to Quantum Kolmogorov Arnold Networks](https://arxiv.org/abs/2506.22340)
*Yannick Werner,Akash Malemath,Mengxi Liu,Vitor Fortes Rey,Nikolaos Palaiodimopoulos,Paul Lukowicz,Maximilian Kiefer-Emmanouilidis*

Main category: quant-ph

TL;DR: 本文利用QCBM实现混合和全量子形式的KAN架构，提出量子KAN（QuKAN）架构并验证其可行性、可解释性和性能。


<details>
  <summary>Details</summary>
Motivation: KAN在量子机器学习领域的潜力尚未得到充分探索，希望在该领域进一步挖掘KAN的能力。

Method: 使用量子电路玻恩机（QCBM）实现混合和全量子形式的KAN架构，利用预训练残差函数进行KAN迁移，混合模型结合经典KAN组件与量子子程序，全量子版本将残差函数架构转换为量子模型。

Result: 验证了所提出的量子KAN（QuKAN）架构的可行性、可解释性和性能。

Conclusion: 量子KAN（QuKAN）架构在量子机器学习领域具有一定潜力。

Abstract: Kolmogorov Arnold Networks (KANs), built upon the Kolmogorov Arnold
representation theorem (KAR), have demonstrated promising capabilities in
expressing complex functions with fewer neurons. This is achieved by
implementing learnable parameters on the edges instead of on the nodes, unlike
traditional networks such as Multi-Layer Perceptrons (MLPs). However, KANs
potential in quantum machine learning has not yet been well explored. In this
work, we present an implementation of these KAN architectures in both hybrid
and fully quantum forms using a Quantum Circuit Born Machine (QCBM). We adapt
the KAN transfer using pre-trained residual functions, thereby exploiting the
representational power of parametrized quantum circuits. In the hybrid model we
combine classical KAN components with quantum subroutines, while the fully
quantum version the entire architecture of the residual function is translated
to a quantum model. We demonstrate the feasibility, interpretability and
performance of the proposed Quantum KAN (QuKAN) architecture.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [136] [Large-Scale Simulations of Turbulent Flows using Lattice Boltzmann Methods on Heterogeneous High Performance Computers](https://arxiv.org/abs/2506.21804)
*Adrian Kummerländer,Fedor Bukreev,Yuji Shimojima,Shota Ito,Mathias J. Krause*

Main category: physics.comp-ph

TL;DR: 介绍适用于复杂几何形状壁面建模大涡模拟的新型LBM方案及其在OpenLB中的高效实现，并给出可扩展性结果。


<details>
  <summary>Details</summary>
Motivation: 当前GPU加速的超级计算机有望实现湍流的大规模模拟，LBM因其与CPU和GPU并行执行的内在兼容性，适合实现这一目标。

Method: 描述了适用于复杂几何形状壁面建模大涡模拟的新型LBM方案，并在开源LBM框架OpenLB中高效实现。

Result: 提供了所有HoreKa分区的详细可扩展性结果，使用多达128个节点，覆盖问题规模达180亿个单元格。

Conclusion: 未提及明确结论

Abstract: Current GPU-accelerated supercomputers promise to enable large-scale
simulations of turbulent flows. Lattice Boltzmann Methods (LBM) are
particularly well-suited to fulfilling this promise due to their intrinsic
compatibility with highly parallel execution on both SIMD CPUs and GPUs. A
novel LBM scheme for wall-modeled LES in complex geometries is described with a
special focus on the efficient implementation in the open source LBM framework
OpenLB. Detailed scalability results are provided for all HoreKa partitions,
utilizing up to 128 nodes and covering problem sizes up to 18 billion cells.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [137] [SciMantify -- A Hybrid Approach for the Evolving Semantification of Scientific Knowledge](https://arxiv.org/abs/2506.21819)
*Lena John,Kheir Eddine Farfar,Sören Auer,Oliver Karras*

Main category: cs.DL

TL;DR: 论文指出科学出版物知识难处理，提出基于5 - 星LOD模型的知识表示进化模型，开发SciMantify方法并在ORKG平台实现，初步实验显示该方法有积极效果。


<details>
  <summary>Details</summary>
Motivation: 科学出版物多为静态非结构化PDF，知识难获取和复用，现有表格形式缺乏语义上下文，需更灵活、结构化和语义化的知识表示。

Method: 提出五阶段知识表示进化模型，开发SciMantify混合方法，让人与机器协作完成语义标注任务，在ORKG平台实现该方法。

Result: 初步用户实验表明该方法简化了科学知识预处理，减少语义化工作量，通过与知识图谱结构更好对齐提升了知识表示。

Conclusion: 所提方法能有效处理科学知识，提升其可处理性和表示效果。

Abstract: Scientific publications, primarily digitized as PDFs, remain static and
unstructured, limiting the accessibility and reusability of the contained
knowledge. At best, scientific knowledge from publications is provided in
tabular formats, which lack semantic context. A more flexible, structured, and
semantic representation is needed to make scientific knowledge understandable
and processable by both humans and machines. We propose an evolution model of
knowledge representation, inspired by the 5-star Linked Open Data (LOD) model,
with five stages and defined criteria to guide the stepwise transition from a
digital artifact, such as a PDF, to a semantic representation integrated in a
knowledge graph (KG). Based on an exemplary workflow implementing the entire
model, we developed a hybrid approach, called SciMantify, leveraging tabular
formats of scientific knowledge, e.g., results from secondary studies, to
support its evolving semantification. In the approach, humans and machines
collaborate closely by performing semantic annotation tasks (SATs) and refining
the results to progressively improve the semantic representation of scientific
knowledge. We implemented the approach in the Open Research Knowledge Graph
(ORKG), an established platform for improving the findability, accessibility,
interoperability, and reusability of scientific knowledge. A preliminary user
experiment showed that the approach simplifies the preprocessing of scientific
knowledge, reduces the effort for the evolving semantification, and enhances
the knowledge representation through better alignment with the KG structures.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [138] [3Description: An Intuitive Human-AI Collaborative 3D Modeling Approach](https://arxiv.org/abs/2506.21845)
*Zhuodi Cai*

Main category: cs.HC

TL;DR: 本文提出3Description，一种人机协作的3D建模方法，结合多种技术和研究方法，基于网页，提升3D建模的易用性和包容性。


<details>
  <summary>Details</summary>
Motivation: 解决传统3D建模的可访问性和可用性挑战，让非专业人士参与3D模型共创。

Method: 结合定性研究、产品分析和用户测试，集成自然语言处理和计算机视觉等AI技术，采用基于网页的方式。

Result: 开发出3Description方法，用户可通过语言和手势输入描述和调整3D模型。

Conclusion: 3Description有助于更具包容性和用户友好的设计过程，增加人类与AI共创的参与度，保留人类创造力。

Abstract: This paper presents 3Description, an experimental human-AI collaborative
approach for intuitive 3D modeling. 3Description aims to address accessibility
and usability challenges in traditional 3D modeling by enabling
non-professional individuals to co-create 3D models using verbal and gesture
descriptions. Through a combination of qualitative research, product analysis,
and user testing, 3Description integrates AI technologies such as Natural
Language Processing and Computer Vision, powered by OpenAI and MediaPipe.
Recognizing the web has wide cross-platform capabilities, 3Description is
web-based, allowing users to describe the desired model and subsequently adjust
its components using verbal and gestural inputs. In the era of AI and emerging
media, 3Description not only contributes to a more inclusive and user-friendly
design process, empowering more people to participate in the construction of
the future 3D world, but also strives to increase human engagement in
co-creation with AI, thereby avoiding undue surrender to technology and
preserving human creativity.

</details>


### [139] [Adapting University Policies for Generative AI: Opportunities, Challenges, and Policy Solutions in Higher Education](https://arxiv.org/abs/2506.22231)
*Russell Beale*

Main category: cs.HC

TL;DR: 生成式AI在高等教育中广泛应用，本文分析其机遇、挑战并提出政策解决方案，强调主动适应政策的必要性。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在高等教育的应用既带来机遇又引发问题，需探讨应对策略。

Method: 综合近期研究和案例的数据进行分析。

Result: 近47%学生在课程中使用LLMs，检测工具准确率约88%，有12%误差。

Conclusion: 主动进行政策调整，才能在利用AI潜力的同时维护学术诚信和公平的核心价值。

Abstract: The rapid proliferation of generative artificial intelligence (AI) tools -
especially large language models (LLMs) such as ChatGPT - has ushered in a
transformative era in higher education. Universities in developed regions are
increasingly integrating these technologies into research, teaching, and
assessment. On one hand, LLMs can enhance productivity by streamlining
literature reviews, facilitating idea generation, assisting with coding and
data analysis, and even supporting grant proposal drafting. On the other hand,
their use raises significant concerns regarding academic integrity, ethical
boundaries, and equitable access. Recent empirical studies indicate that nearly
47% of students use LLMs in their coursework - with 39% using them for exam
questions and 7% for entire assignments - while detection tools currently
achieve around 88% accuracy, leaving a 12% error margin. This article
critically examines the opportunities offered by generative AI, explores the
multifaceted challenges it poses, and outlines robust policy solutions.
Emphasis is placed on redesigning assessments to be AI-resilient, enhancing
staff and student training, implementing multi-layered enforcement mechanisms,
and defining acceptable use. By synthesizing data from recent research and case
studies, the article argues that proactive policy adaptation is imperative to
harness AI's potential while safeguarding the core values of academic integrity
and equity.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [140] [DiffSoundStream: Efficient Speech Tokenization via Diffusion Decoding](https://arxiv.org/abs/2506.22362)
*Yang Yang,Yunpeng Li,George Sung,Shao-Fu Shih,Craig Dooley,Alessio Centazzo,Ramanan Rajeswaran*

Main category: eess.AS

TL;DR: 提出DiffSoundStream改进非流式场景语音分词效率，实验表明其在低token率下有好效果，且可少量扩散采样步骤蒸馏。


<details>
  <summary>Details</summary>
Motivation: 基于token的语音生成方法推理速度受token率限制，需提高语音分词效率。

Method: 采用两种技术，一是让神经编解码器基于语义token以减少语义和声学token间冗余，二是利用潜扩散模型从语义和粗级声学token合成高质量波形。

Result: 在每秒50个token时，DiffSoundStream语音质量与两倍token率的标准SoundStream模型相当；仅用四个扩散采样步骤进行步长蒸馏，质量损失小。

Conclusion: DiffSoundStream能有效提高非流式场景语音分词效率。

Abstract: Token-based language modeling is a prominent approach for speech generation,
where tokens are obtained by quantizing features from self-supervised learning
(SSL) models and extracting codes from neural speech codecs, generally referred
to as semantic tokens and acoustic tokens. These tokens are often modeled
autoregressively, with the inference speed being constrained by the token rate.
In this work, we propose DiffSoundStream, a solution that improves the
efficiency of speech tokenization in non-streaming scenarios through two
techniques: (1) conditioning the neural codec on semantic tokens to minimize
redundancy between semantic and acoustic tokens, and (2) leveraging latent
diffusion models to synthesize high-quality waveforms from semantic and
coarse-level acoustic tokens. Experiments show that at 50 tokens per second,
DiffSoundStream achieves speech quality on par with a standard SoundStream
model operating at twice the token rate. Additionally, we achieve step-size
distillation using just four diffusion sampling steps with only a minor quality
loss.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [141] [PEACE: Empowering Geologic Map Holistic Understanding with MLLMs](https://arxiv.org/abs/2501.06184)
*Yangyu Huang,Tianyi Gao,Haoran Xu,Qihao Zhao,Yang Song,Zhipeng Gui,Tengchao Lv,Hao Chen,Lei Cui,Scarlett Li,Furu Wei*

Main category: cs.CV

TL;DR: 本文构建GeoMap - Bench评估MLLMs地质图理解能力，提出GeoMap - Agent提升性能，推动AI在地质领域应用。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在地质图理解方面存在不足，因制图综合的挑战，需评估和改进其能力。

Method: 构建GeoMap - Bench评估，提出GeoMap - Agent，包含分层信息提取、领域知识注入和提示增强问答三个模块，借助AI专家组和工具池分析问题。

Result: GeoMap - Agent在GeoMap - Bench上总分0.811，远超GPT - 4o的0.369。

Conclusion: 研究为AI在地质领域的高级应用铺平道路，提高地质调查效率和准确性。

Abstract: Geologic map, as a fundamental diagram in geology science, provides critical
insights into the structure and composition of Earth's subsurface and surface.
These maps are indispensable in various fields, including disaster detection,
resource exploration, and civil engineering. Despite their significance,
current Multimodal Large Language Models (MLLMs) often fall short in geologic
map understanding. This gap is primarily due to the challenging nature of
cartographic generalization, which involves handling high-resolution map,
managing multiple associated components, and requiring domain-specific
knowledge. To quantify this gap, we construct GeoMap-Bench, the first-ever
benchmark for evaluating MLLMs in geologic map understanding, which assesses
the full-scale abilities in extracting, referring, grounding, reasoning, and
analyzing. To bridge this gap, we introduce GeoMap-Agent, the inaugural agent
designed for geologic map understanding, which features three modules:
Hierarchical Information Extraction (HIE), Domain Knowledge Injection (DKI),
and Prompt-enhanced Question Answering (PEQA). Inspired by the
interdisciplinary collaboration among human scientists, an AI expert group acts
as consultants, utilizing a diverse tool pool to comprehensively analyze
questions. Through comprehensive experiments, GeoMap-Agent achieves an overall
score of 0.811 on GeoMap-Bench, significantly outperforming 0.369 of GPT-4o.
Our work, emPowering gEologic mAp holistiC undErstanding (PEACE) with MLLMs,
paves the way for advanced AI applications in geology, enhancing the efficiency
and accuracy of geological investigations.

</details>


### [142] [Tied Prototype Model for Few-Shot Medical Image Segmentation](https://arxiv.org/abs/2506.22101)
*Hyeongji Kim,Stine Hansen,Michael Kampffmeyer*

Main category: cs.CV

TL;DR: 本文提出用于医学图像分割的Tied Prototype Model (TPM)，改进ADNet不足，提升分割准确性与性能。


<details>
  <summary>Details</summary>
Motivation: 现有ADNet在医学图像少样本分割中有依赖单原型、仅关注二分类和阈值固定等局限，需改进。

Method: 提出TPM，对ADNet重新公式化，使前景和背景分布的原型位置绑定，自然扩展到多原型和多类分割，利用类先验定义自适应阈值。

Result: TPM的多原型和多类分割扩展提升了分割准确性，自适应阈值提升了分割性能。

Conclusion: TPM为基于原型的医学图像少样本分割提供了新视角。

Abstract: Common prototype-based medical image few-shot segmentation (FSS) methods
model foreground and background classes using class-specific prototypes.
However, given the high variability of the background, a more promising
direction is to focus solely on foreground modeling, treating the background as
an anomaly -- an approach introduced by ADNet. Yet, ADNet faces three key
limitations: dependence on a single prototype per class, a focus on binary
classification, and fixed thresholds that fail to adapt to patient and organ
variability. To address these shortcomings, we propose the Tied Prototype Model
(TPM), a principled reformulation of ADNet with tied prototype locations for
foreground and background distributions. Building on its probabilistic
foundation, TPM naturally extends to multiple prototypes and multi-class
segmentation while effectively separating non-typical background features.
Notably, both extensions lead to improved segmentation accuracy. Finally, we
leverage naturally occurring class priors to define an ideal target for
adaptive thresholds, boosting segmentation performance. Taken together, TPM
provides a fresh perspective on prototype-based FSS for medical image
segmentation. The code can be found at https://github.com/hjk92g/TPM-FSS.

</details>


### [143] [ADNet: Leveraging Error-Bias Towards Normal Direction in Face Alignment](https://arxiv.org/abs/2109.05721)
*Yangyu Huang,Hao Yang,Chong Li,Jongyoo Kim,Fangyun Wei*

Main category: cs.CV

TL;DR: 本文研究人脸对齐中的误差偏差问题，提出各向异性方向损失（ADL）和各向异性注意力模块（AAM），并集成到ADNet，在多个数据集上取得SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 现有CNN虽提升人脸对齐性能，但很少关注面部地标误差分布的误差偏差问题，且该问题与模糊地标标注任务密切相关。

Method: 提出ADL用于坐标回归，对人脸边界上的每个地标点在法线方向施加强约束力；提出AAM用于热图回归，可获得各向异性注意力掩码，在切线方向响应更强；将二者集成到优化的端到端训练管道ADNet。

Result: ADNet在300W、WFLW和COFW数据集上取得了最先进的结果。

Conclusion: 所提方法有效且鲁棒。

Abstract: The recent progress of CNN has dramatically improved face alignment
performance. However, few works have paid attention to the error-bias with
respect to error distribution of facial landmarks. In this paper, we
investigate the error-bias issue in face alignment, where the distributions of
landmark errors tend to spread along the tangent line to landmark curves. This
error-bias is not trivial since it is closely connected to the ambiguous
landmark labeling task. Inspired by this observation, we seek a way to leverage
the error-bias property for better convergence of CNN model. To this end, we
propose anisotropic direction loss (ADL) and anisotropic attention module (AAM)
for coordinate and heatmap regression, respectively. ADL imposes strong binding
force in normal direction for each landmark point on facial boundaries. On the
other hand, AAM is an attention module which can get anisotropic attention mask
focusing on the region of point and its local edge connected by adjacent
points, it has a stronger response in tangent than in normal, which means
relaxed constraints in the tangent. These two methods work in a complementary
manner to learn both facial structures and texture details. Finally, we
integrate them into an optimized end-to-end training pipeline named ADNet. Our
ADNet achieves state-of-the-art results on 300W, WFLW and COFW datasets, which
demonstrates the effectiveness and robustness.

</details>


### [144] [FreeEnricher: Enriching Face Landmarks without Additional Cost](https://arxiv.org/abs/2212.09525)
*Yangyu Huang,Xi Chen,Jongyoo Kim,Hao Yang,Chong Li,Jiaolong Yang,Dong Chen*

Main category: cs.CV

TL;DR: 提出框架用现有稀疏地标数据集丰富地标密度，模型作即插即用模块，在新密集和原稀疏数据集上达最优精度。


<details>
  <summary>Details</summary>
Motivation: 现有面部对齐工作多关注稀疏面部对齐，而密集面部地标在多种场景需求高。

Method: 观察语义轮廓局部补丁相似性，提出弱监督思想学习原稀疏地标细化能力并应用到密集地标，设计算子实现想法，将训练模型作为即插即用模块用于现有面部对齐网络。

Result: 在新构建的密集300W测试集和原稀疏300W、WFLW测试集上均取得最优精度，且无额外成本。

Conclusion: 所提方法有效，可在不同数据集上提升面部对齐精度。

Abstract: Recent years have witnessed significant growth of face alignment. Though
dense facial landmark is highly demanded in various scenarios, e.g., cosmetic
medicine and facial beautification, most works only consider sparse face
alignment. To address this problem, we present a framework that can enrich
landmark density by existing sparse landmark datasets, e.g., 300W with 68
points and WFLW with 98 points. Firstly, we observe that the local patches
along each semantic contour are highly similar in appearance. Then, we propose
a weakly-supervised idea of learning the refinement ability on original sparse
landmarks and adapting this ability to enriched dense landmarks. Meanwhile,
several operators are devised and organized together to implement the idea.
Finally, the trained model is applied as a plug-and-play module to the existing
face alignment networks. To evaluate our method, we manually label the dense
landmarks on 300W testset. Our method yields state-of-the-art accuracy not only
in newly-constructed dense 300W testset but also in the original sparse 300W
and WFLW testsets without additional cost.

</details>


### [145] [Elucidating and Endowing the Diffusion Training Paradigm for General Image Restoration](https://arxiv.org/abs/2506.21722)
*Xin Lu,Xueyang Fu,Jie Xiao,Zihao Fan,Yurui Zhu,Zheng-Jun Zha*

Main category: cs.CV

TL;DR: 本文提出基于扩散训练的图像恢复框架，引入正则策略和增量训练范式等，提升单任务泛化性和多任务统一恢复性能，且可集成到现有架构。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像恢复任务中有强大生成能力，但复杂架构和迭代过程限制其应用，现有方法忽视扩散训练范式与通用图像恢复框架的结合。

Method: 系统分析时间步依赖、网络层次、噪声水平关系和多恢复任务相关性，提出基于扩散训练的新图像恢复框架；引入正则策略使扩散目标与图像恢复任务对齐；开发增量训练范式和特定任务适配器。

Result: 显著提高图像恢复网络在单任务中的泛化性，在多任务统一图像恢复中表现更优。

Conclusion: 所提框架可提升图像恢复网络性能，且能无缝集成到现有通用图像恢复架构中。

Abstract: While diffusion models demonstrate strong generative capabilities in image
restoration (IR) tasks, their complex architectures and iterative processes
limit their practical application compared to mainstream reconstruction-based
general ordinary IR networks. Existing approaches primarily focus on optimizing
network architecture and diffusion paths but overlook the integration of the
diffusion training paradigm within general ordinary IR frameworks. To address
these challenges, this paper elucidates key principles for adapting the
diffusion training paradigm to general IR training through systematic analysis
of time-step dependencies, network hierarchies, noise-level relationships, and
multi-restoration task correlations, proposing a new IR framework supported by
diffusion-based training. To enable IR networks to simultaneously restore
images and model generative representations, we introduce a series of
regularization strategies that align diffusion objectives with IR tasks,
improving generalization in single-task scenarios. Furthermore, recognizing
that diffusion-based generation exerts varying influences across different IR
tasks, we develop an incremental training paradigm and task-specific adaptors,
further enhancing performance in multi-task unified IR. Experiments demonstrate
that our method significantly improves the generalization of IR networks in
single-task IR and achieves superior performance in multi-task unified IR.
Notably, the proposed framework can be seamlessly integrated into existing
general IR architectures.

</details>


### [146] [TanDiT: Tangent-Plane Diffusion Transformer for High-Quality 360° Panorama Generation](https://arxiv.org/abs/2506.21681)
*Hakan Çapuk,Andrew Bond,Muhammed Burak Kızıl,Emir Göçen,Erkut Erdem,Aykut Erdem*

Main category: cs.CV

TL;DR: 本文提出TanDiT方法解决全景图像生成难题，还提出后处理步骤和评估指标，实验表明该方法效果好。


<details>
  <summary>Details</summary>
Motivation: 现有图像生成模型在全景图像生成上存在因几何畸变和循环一致性要求带来的挑战，需改进。

Method: 引入TanDiT方法，通过生成覆盖360°视图的切平面图像网格合成全景场景，采用统一扩散模型在单次去噪迭代中同时生成切平面图像；提出增强全局一致性的后处理步骤；提出TangentIS和TangentFID评估指标并提供基准测试。

Result: 方法能有效泛化，可解读复杂文本提示，能与多种生成模型集成，生成高质量、多样化全景图像。

Conclusion: TanDiT方法及相关指标和基准测试可有效解决全景图像生成问题。

Abstract: Recent advances in image generation have led to remarkable improvements in
synthesizing perspective images. However, these models still struggle with
panoramic image generation due to unique challenges, including varying levels
of geometric distortion and the requirement for seamless loop-consistency. To
address these issues while leveraging the strengths of the existing models, we
introduce TanDiT, a method that synthesizes panoramic scenes by generating
grids of tangent-plane images covering the entire 360$^\circ$ view. Unlike
previous methods relying on multiple diffusion branches, TanDiT utilizes a
unified diffusion model trained to produce these tangent-plane images
simultaneously within a single denoising iteration. Furthermore, we propose a
model-agnostic post-processing step specifically designed to enhance global
coherence across the generated panoramas. To accurately assess panoramic image
quality, we also present two specialized metrics, TangentIS and TangentFID, and
provide a comprehensive benchmark comprising captioned panoramic datasets and
standardized evaluation scripts. Extensive experiments demonstrate that our
method generalizes effectively beyond its training data, robustly interprets
detailed and complex text prompts, and seamlessly integrates with various
generative models to yield high-quality, diverse panoramic images.

</details>


### [147] [Exploring Image Generation via Mutually Exclusive Probability Spaces and Local Correlation Hypothesis](https://arxiv.org/abs/2506.21731)
*Chenqiu Zhao,Anup Basu*

Main category: cs.CV

TL;DR: 提出MESP和LCH框架探索概率生成模型局限，基于MESP提出BL - AE和ARVM，ARVM虽FID得分好但有记忆问题，又提出LCH并验证。


<details>
  <summary>Details</summary>
Motivation: 探索概率生成模型学习全局分布易导致记忆而非生成行为的潜在局限。

Method: 重新思考VAE提出MESP，基于MESP提出BL - AE和ARVM；提出LCH假设。

Result: ARVM在标准数据集上取得有竞争力的FID得分，超越现有方法，但得分反映的是记忆而非生成。

Conclusion: 提出的MESP和LCH框架经综合实验和讨论得到验证。

Abstract: We propose two theoretical frameworks, the Mutually Exclusive Probability
Space (MESP) and the Local Correlation Hypothesis (LCH), to explore a potential
limitation in probabilistic generative models; namely that learning global
distributions leads to memorization rather than generative behavior. MESP
emerges from our rethinking of the Variational Autoencoder (VAE). We observe
that latent variable distributions in VAE exhibit overlap, which leads to an
optimization conflict between the reconstruction loss and KL-divergence loss. A
lower bound based on the overlap coefficient is proposed. We refer to this
phenomenon as Mutually Exclusive Probability Spaces. Based on MESP, a Binary
Latent Autoencoder (BL-AE) is proposed to encode images into binary latent
representations. These binary latents are used as the input to our
Autoregressive Random Variable Model (ARVM), a modified autoregressive model
outputting histograms. Our ARVM achieves competitive FID scores, outperforming
state-of-the-art methods on standard datasets. However, such scores reflect
memorization rather than generation. To address this issue, we propose the
Local Correlation Hypothesis (LCH), which posits that generative capability
arising from local correlations among latent variables. Comprehensive
experiments and discussions are conducted to validate our frameworks.

</details>


### [148] [Comparing Learning Paradigms for Egocentric Video Summarization](https://arxiv.org/abs/2506.21785)
*Daniel Wen*

Main category: cs.CV

TL;DR: 研究评估监督学习、无监督学习和提示微调三种计算机视觉范式处理第一人称视频数据的能力，发现当前模型在第一人称视频上效果不佳，提示微调的GPT - 4o表现更优。


<details>
  <summary>Details</summary>
Motivation: 推进计算机视觉技术在第一人称视频上的应用，开发能有效处理和解读第一人称视角的模型。

Method: 评估Shotluck Holmes（监督学习）、TAC - SUM（无监督学习）和GPT - 4o（提示微调预训练模型）在视频总结任务中的有效性。

Result: 当前最先进模型在第一人称视频上不如第三人称视频有效，提示微调的GPT - 4o优于专业模型。

Conclusion: 现有方法在适应第一人称视角挑战方面有局限，需在第一人称视频领域进一步改进。

Abstract: In this study, we investigate various computer vision paradigms - supervised
learning, unsupervised learning, and prompt fine-tuning - by assessing their
ability to understand and interpret egocentric video data. Specifically, we
examine Shotluck Holmes (state-of-the-art supervised learning), TAC-SUM
(state-of-the-art unsupervised learning), and GPT-4o (a prompt fine-tuned
pre-trained model), evaluating their effectiveness in video summarization. Our
results demonstrate that current state-of-the-art models perform less
effectively on first-person videos compared to third-person videos,
highlighting the need for further advancements in the egocentric video domain.
Notably, a prompt fine-tuned general-purpose GPT-4o model outperforms these
specialized models, emphasizing the limitations of existing approaches in
adapting to the unique challenges of first-person perspectives. Although our
evaluation is conducted on a small subset of egocentric videos from the
Ego-Exo4D dataset due to resource constraints, the primary objective of this
research is to provide a comprehensive proof-of-concept analysis aimed at
advancing the application of computer vision techniques to first-person videos.
By exploring novel methodologies and evaluating their potential, we aim to
contribute to the ongoing development of models capable of effectively
processing and interpreting egocentric perspectives.

</details>


### [149] [CAT-SG: A Large Dynamic Scene Graph Dataset for Fine-Grained Understanding of Cataract Surgery](https://arxiv.org/abs/2506.21813)
*Felix Holm,Gözde Ünver,Ghazal Ghazaei,Nassir Navab*

Main category: cs.CV

TL;DR: 本文引入Cataract Surgery Scene Graph (CAT - SG) 数据集并提出CatSGG模型，用于增强AI驱动的手术相关应用。


<details>
  <summary>Details</summary>
Motivation: 现有数据集缺乏对手术中实体间语义关系的全面表示，难以理解白内障手术复杂工作流程。

Method: 引入CAT - SG数据集进行结构化标注，提出CatSGG场景图生成模型。

Result: CAT - SG数据集能提供手术工作流的整体视图，CatSGG模型在生成结构化手术表示方面优于现有方法。

Conclusion: CAT - SG数据集有助于增强AI驱动的手术训练、实时决策支持和工作流分析，推动临床实践中更智能、上下文感知系统的发展。

Abstract: Understanding the intricate workflows of cataract surgery requires modeling
complex interactions between surgical tools, anatomical structures, and
procedural techniques. Existing datasets primarily address isolated aspects of
surgical analysis, such as tool detection or phase segmentation, but lack
comprehensive representations that capture the semantic relationships between
entities over time. This paper introduces the Cataract Surgery Scene Graph
(CAT-SG) dataset, the first to provide structured annotations of tool-tissue
interactions, procedural variations, and temporal dependencies. By
incorporating detailed semantic relations, CAT-SG offers a holistic view of
surgical workflows, enabling more accurate recognition of surgical phases and
techniques. Additionally, we present a novel scene graph generation model,
CatSGG, which outperforms current methods in generating structured surgical
representations. The CAT-SG dataset is designed to enhance AI-driven surgical
training, real-time decision support, and workflow analysis, paving the way for
more intelligent, context-aware systems in clinical practice.

</details>


### [150] [Early Glaucoma Detection using Deep Learning with Multiple Datasets of Fundus Images](https://arxiv.org/abs/2506.21770)
*Rishiraj Paul Chowdhury,Nirmit Shekar Karkera*

Main category: cs.CV

TL;DR: 提出用EfficientNet - B0架构进行青光眼检测的深度学习管道，多数据集训练，实验表明简单预处理效果好，有临床应用潜力。


<details>
  <summary>Details</summary>
Motivation: 青光眼是不可逆失明主因，传统诊断方法有侵入性且需专业设备，需新检测方法。

Method: 采用EfficientNet - B0架构，在ACRIMA、ORIGA和RIM - ONE数据集上依次训练和微调模型。

Result: 简单预处理比复杂增强有更高AUC - ROC，模型在未见数据集上有强判别性能。

Conclusion: 提出的管道为早期青光眼检测提供可重复、可扩展方法，有临床应用潜力。

Abstract: Glaucoma is a leading cause of irreversible blindness, but early detection
can significantly improve treatment outcomes. Traditional diagnostic methods
are often invasive and require specialized equipment. In this work, we present
a deep learning pipeline using the EfficientNet-B0 architecture for glaucoma
detection from retinal fundus images. Unlike prior studies that rely on single
datasets, we sequentially train and fine-tune our model across ACRIMA, ORIGA,
and RIM-ONE datasets to enhance generalization. Our experiments show that
minimal preprocessing yields higher AUC-ROC compared to more complex
enhancements, and our model demonstrates strong discriminative performance on
unseen datasets. The proposed pipeline offers a reproducible and scalable
approach to early glaucoma detection, supporting its potential clinical
utility.

</details>


### [151] [Few-Shot Segmentation of Historical Maps via Linear Probing of Vision Foundation Models](https://arxiv.org/abs/2506.21826)
*Rafael Sterzinger,Marco Peer,Robert Sablatnig*

Main category: cs.CV

TL;DR: 提出用于历史地图少样本分割的方法，在多个数据集表现好，减少手动标注需求。


<details>
  <summary>Details</summary>
Motivation: 地图的多样视觉表示和有限标注数据给自动化处理带来挑战，需要有效方法进行历史地图分割。

Method: 利用大视觉基础模型的语义嵌入和参数高效微调。

Result: 在Siegfried基准数据集的葡萄园和铁路分割中超越现有方法，在ICDAR 2021竞赛数据集上表现良好，低数据情况下性能高且可训练参数少。

Conclusion: 该方法能精确分割多样历史地图，大幅减少手动标注，推动领域自动化处理和分析。

Abstract: As rich sources of history, maps provide crucial insights into historical
changes, yet their diverse visual representations and limited annotated data
pose significant challenges for automated processing. We propose a simple yet
effective approach for few-shot segmentation of historical maps, leveraging the
rich semantic embeddings of large vision foundation models combined with
parameter-efficient fine-tuning. Our method outperforms the state-of-the-art on
the Siegfried benchmark dataset in vineyard and railway segmentation, achieving
+5% and +13% relative improvements in mIoU in 10-shot scenarios and around +20%
in the more challenging 5-shot setting. Additionally, it demonstrates strong
performance on the ICDAR 2021 competition dataset, attaining a mean PQ of 67.3%
for building block segmentation, despite not being optimized for this
shape-sensitive metric, underscoring its generalizability. Notably, our
approach maintains high performance even in extremely low-data regimes (10- &
5-shot), while requiring only 689k trainable parameters - just 0.21% of the
total model size. Our approach enables precise segmentation of diverse
historical maps while drastically reducing the need for manual annotations,
advancing automated processing and analysis in the field. Our implementation is
publicly available at:
https://github.com/RafaelSterzinger/few-shot-map-segmentation.

</details>


### [152] [SPADE: Spatial Transcriptomics and Pathology Alignment Using a Mixture of Data Experts for an Expressive Latent Space](https://arxiv.org/abs/2506.21857)
*Ekaterina Redekop,Mara Pleasure,Zichen Wang,Kimberly Flores,Anthony Sisk,William Speier,Corey W. Arnold*

Main category: cs.CV

TL;DR: 提出基础模型SPADE集成组织病理学与空间转录组学数据，在下游任务中表现优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有研究在全面整合全切片图像与空间转录组学数据上存在差距，需捕捉超越标准苏木精 - 伊红染色的关键分子异质性。

Method: 引入SPADE模型，利用数据专家混合技术，通过两阶段特征空间聚类创建专家，使用对比学习学习共同注册的全切片图像补丁和基因表达谱的表示。

Result: 在14个下游任务中评估，相比基线模型，少样本性能显著更优。

Conclusion: 将形态和分子信息整合到一个潜在空间有好处。

Abstract: The rapid growth of digital pathology and advances in self-supervised deep
learning have enabled the development of foundational models for various
pathology tasks across diverse diseases. While multimodal approaches
integrating diverse data sources have emerged, a critical gap remains in the
comprehensive integration of whole-slide images (WSIs) with spatial
transcriptomics (ST), which is crucial for capturing critical molecular
heterogeneity beyond standard hematoxylin & eosin (H&E) staining. We introduce
SPADE, a foundation model that integrates histopathology with ST data to guide
image representation learning within a unified framework, in effect creating an
ST-informed latent space. SPADE leverages a mixture-of-data experts technique,
where experts, created via two-stage feature-space clustering, use contrastive
learning to learn representations of co-registered WSI patches and gene
expression profiles. Pre-trained on the comprehensive HEST-1k dataset, SPADE is
evaluated on 14 downstream tasks, demonstrating significantly superior few-shot
performance compared to baseline models, highlighting the benefits of
integrating morphological and molecular information into one latent space.

</details>


### [153] [LLaVA-Scissor: Token Compression with Semantic Connected Components for Video LLMs](https://arxiv.org/abs/2506.21862)
*Boyuan Sun,Jiaxing Zhao,Xihan Wei,Qibin Hou*

Main category: cs.CV

TL;DR: 提出无训练的视频多模态大语言模型令牌压缩策略LLaVA - Scissor，实验显示其性能优越。


<details>
  <summary>Details</summary>
Motivation: 以往基于注意力分数的令牌压缩方法无法有效捕获所有语义区域，存在令牌冗余问题。

Method: 利用语义连通分量（SCC）方法，在时空域使用两步时空令牌压缩策略。

Result: 在多种视频理解基准测试中，LLaVA - Scissor优于其他令牌压缩方法，在低令牌保留率下表现出色。

Conclusion: LLaVA - Scissor是一种有效的视频多模态大语言模型令牌压缩策略。

Abstract: In this paper, we present LLaVA-Scissor, a training-free token compression
strategy designed for video multimodal large language models. Previous methods
mostly attempt to compress tokens based on attention scores, but fail to
effectively capture all semantic regions and often lead to token redundancy.
Differently, we propose to leverage the Semantic Connected Components (SCC)
approach that assigns tokens to distinct semantic regions within the token set,
ensuring comprehensive semantic coverage. The outcome is a two-step
spatio-temporal token compression strategy that utilizes SCC in both spatial
and temporal domains. This strategy can effectively compress tokens by
representing the entire video with a set of non-overlapping semantic tokens. We
conduct extensive evaluations of the token compression capabilities of
LLaVA-Scissor across diverse video understanding benchmarks, including video
question answering, long video understanding, and comprehensive multi-choices
benchmarks. Experimental results show that the proposed LLaVA-Scissor
outperforms other token compression methods, achieving superior performance in
various video understanding benchmarks, particularly at low token retention
ratios. Project page: https://github.com/HumanMLLM/LLaVA-Scissor.

</details>


### [154] [Grounding-Aware Token Pruning: Recovering from Drastic Performance Drops in Visual Grounding Caused by Pruning](https://arxiv.org/abs/2506.21873)
*Tzu-Chun Chien,Chieh-Kai Lin,Shiang-Feng Tsai,Ruei-Chi Lai,Hung-Jen Chen,Min Sun*

Main category: cs.CV

TL;DR: 研究发现剪枝会削弱多模态大语言模型视觉定位能力，提出GAP方法调整位置ID恢复性能且无额外开销，应用于多模型均有提升。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型视觉定位任务中，现有标记剪枝方法会导致模型定位能力下降、性能退化，需解决该问题。

Method: 提出Grounding - Aware Token Pruning (GAP)方法，调整位置ID。

Result: 在Referring Expression Comprehension (REC)中，将LLaVA在RefCOCO验证集的准确率从剪枝后的15.34%恢复到51.42%，达未剪枝时90%性能；应用于多个模型在不同标记剪枝策略下均提升性能。

Conclusion: GAP方法简单有效，无需额外训练、内存和计算开销，能提升多模型在视觉定位任务中的性能。

Abstract: Recent Multimodal Large Language Models (MLLMs) have demonstrated strong
performance in visual grounding, establishing themselves as a general interface
for various vision-language applications. This progress has driven the
development of token pruning methods to mitigate the high computational costs
associated with processing numerous visual tokens. However, we observe that
pruning significantly weakens the model's grounding ability, leading to
incorrect predictions and drastic performance degradation. In Referring
Expression Comprehension (REC), for instance, pruning causes the accuracy of
LLaVA on the RefCOCO validation set to drop from 56.14% to 15.34%. Our analysis
identifies misaligned position IDs after pruning as the primary cause of this
degradation, as both the order and value of these IDs are crucial for
maintaining performance in grounding tasks. To address this issue, we propose
Grounding-Aware Token Pruning (GAP), a simple yet effective adjustment to
position IDs that recovers REC accuracy back to 51.42%, which is 90% of the
original performance in the without pruning setting, all while requiring no
additional training, memory, or computational overhead. Applied to models such
as Shikra, MiniGPTv2, and the LLaVA series, our method consistently improves
performance across various token pruning strategies.

</details>


### [155] [SODA: Out-of-Distribution Detection in Domain-Shifted Point Clouds via Neighborhood Propagation](https://arxiv.org/abs/2506.21892)
*Adam Goodge,Xun Xu,Bryan Hooi,Wee Siong Ng,Jingyi Liao,Yongyi Su,Xulei Yang*

Main category: cs.CV

TL;DR: 随着点云数据应用增多，检测离群点云对象很关键。现有研究不足，3D VLM 有域偏移问题，提出 SODA 方法，无需额外训练且表现优。


<details>
  <summary>Details</summary>
Motivation: 点云数据在各应用中日益普遍，检测离群点云对象对保障模型安全和可靠性至关重要，但现有研究对此探索不足，且 3D VLM 存在合成到真实的域偏移问题。

Method: 提出名为 SODA 的新方法，通过基于邻域的分数传播方案改进离群点云检测，是基于推理的，无需额外模型训练。

Result: SODA 在跨数据集和问题设置上比现有方法取得了更优的性能。

Conclusion: SODA 能有效解决 3D VLM 合成到真实的域偏移问题，提升离群点云检测效果，无需额外训练且性能达到当前最优。

Abstract: As point cloud data increases in prevalence in a variety of applications, the
ability to detect out-of-distribution (OOD) point cloud objects becomes
critical for ensuring model safety and reliability. However, this problem
remains under-explored in existing research. Inspired by success in the image
domain, we propose to exploit advances in 3D vision-language models (3D VLMs)
for OOD detection in point cloud objects. However, a major challenge is that
point cloud datasets used to pre-train 3D VLMs are drastically smaller in size
and object diversity than their image-based counterparts. Critically, they
often contain exclusively computer-designed synthetic objects. This leads to a
substantial domain shift when the model is transferred to practical tasks
involving real objects scanned from the physical environment. In this paper,
our empirical experiments show that synthetic-to-real domain shift
significantly degrades the alignment of point cloud with their associated text
embeddings in the 3D VLM latent space, hindering downstream performance. To
address this, we propose a novel methodology called SODA which improves the
detection of OOD point clouds through a neighborhood-based score propagation
scheme. SODA is inference-based, requires no additional model training, and
achieves state-of-the-art performance over existing approaches across datasets
and problem settings.

</details>


### [156] [SDRNET: Stacked Deep Residual Network for Accurate Semantic Segmentation of Fine-Resolution Remotely Sensed Images](https://arxiv.org/abs/2506.21945)
*Naftaly Wambugu,Ruisheng Wang,Bo Guo,Tianshu Yu,Sheng Xu,Mohammed Elhassan*

Main category: cs.CV

TL;DR: 文章提出堆叠深度残差网络SDRNet用于高分辨率遥感影像语义分割，实验表明其性能良好。


<details>
  <summary>Details</summary>
Motivation: 当前高分辨率遥感影像语义分割受类别差异、遮挡、物体大小变化等影响，现有深度学习模型提取特征有挑战，深层网络会丢失空间细节。

Method: 提出SDRNet框架，利用两个堆叠的编解码器网络获取长距离语义并保留空间信息，在编解码器网络间使用扩张残差块捕获全局依赖。

Result: 使用ISPRS Vaihingen和Potsdam数据集实验，SDRNet在语义分割中表现有效且有竞争力。

Conclusion: SDRNet能有效解决高分辨率遥感影像语义分割问题，相比当前DCNNs性能良好。

Abstract: Land cover maps generated from semantic segmentation of high-resolution
remotely sensed images have drawn mucon in the photogrammetry and remote
sensing research community. Currently, massive fine-resolution remotely sensed
(FRRS) images acquired by improving sensing and imaging technologies become
available. However, accurate semantic segmentation of such FRRS images is
greatly affected by substantial class disparities, the invisibility of key
ground objects due to occlusion, and object size variation. Despite the
extraordinary potential in deep convolutional neural networks (DCNNs) in image
feature learning and representation, extracting sufficient features from FRRS
images for accurate semantic segmentation is still challenging. These
challenges demand the deep learning models to learn robust features and
generate sufficient feature descriptors. Specifically, learning
multi-contextual features to guarantee adequate coverage of varied object sizes
from the ground scene and harnessing global-local contexts to overcome class
disparities challenge even profound networks. Deeper networks significantly
lose spatial details due to gradual downsampling processes resulting in poor
segmentation results and coarse boundaries. This article presents a stacked
deep residual network (SDRNet) for semantic segmentation from FRRS images. The
proposed framework utilizes two stacked encoder-decoder networks to harness
long-range semantics yet preserve spatial information and dilated residual
blocks (DRB) between each encoder and decoder network to capture sufficient
global dependencies thus improving segmentation performance. Our experimental
results obtained using the ISPRS Vaihingen and Potsdam datasets demonstrate
that the SDRNet performs effectively and competitively against current DCNNs in
semantic segmentation.

</details>


### [157] [Visual Structures Helps Visual Reasoning: Addressing the Binding Problem in VLMs](https://arxiv.org/abs/2506.22146)
*Amirmohammad Izadi,Mohammad Ali Banayeeanzade,Fatemeh Askari,Ali Rahimiakbar,Mohammad Mahdi Vahedi,Hosein Hasani,Mahdieh Soleymani Baghshah*

Main category: cs.CV

TL;DR: 本文指出当前视觉语言模型存在绑定问题，提出通过增加低层次空间结构和文本提示来改进，实验证明该方法在视觉推理任务上有显著提升，强调视觉输入设计的重要性。


<details>
  <summary>Details</summary>
Motivation: 解决当前视觉语言模型因绑定问题导致的视觉推理能力受限的问题。

Method: 给视觉输入增加低层次空间结构（如水平线），并配合鼓励顺序、空间感知解析的文本提示。

Result: 在核心视觉推理任务上性能显著提升，如提高GPT - 4o视觉搜索和计数准确率、降低场景描述编辑距离误差、提升空间关系任务表现，且视觉修改是性能提升关键，纯文本策略不足甚至会降低性能。

Conclusion: 低层次视觉结构化是提升组合视觉推理的有力且未充分探索的方向，可作为提升视觉语言模型在空间相关任务上表现的通用策略。

Abstract: Despite progress in Vision-Language Models (VLMs), their capacity for visual
reasoning is often limited by the \textit{binding problem}: the failure to
reliably associate perceptual features with their correct visual referents.
This limitation underlies persistent errors in tasks such as counting, visual
search, scene description, and spatial relationship understanding. A key factor
is that current VLMs process visual features largely in parallel, lacking
mechanisms for spatially grounded, serial attention. This paper introduces a
simple yet effective intervention: augmenting visual inputs with low-level
spatial structures (e.g., horizontal lines) and pairing this with a textual
prompt that encourages sequential, spatially-aware parsing. We empirically
demonstrate substantial performance improvements across core visual reasoning
tasks. Specifically, our method improves GPT-4o visual search accuracy by
25.00%, increases counting accuracy by 26.83%, reduces edit distance error in
scene description by 0.32, and enhances performance on spatial relationship
tasks by 9.50% on a a 2D synthetic dataset. Furthermore, we find that the
visual modification is essential for these gains; purely textual strategies,
including Chain-of-Thought prompting, are insufficient and can even degrade
performance. Our method enhances binding only with a single-query inference,
underscoring the importance of visual input design over purely
linguistically-based approaches. These findings suggest that low-level visual
structuring is a powerful and underexplored direction for improving
compositional visual reasoning and could serve as a general strategy for
enhancing VLM performance on spatially grounded tasks.

</details>


### [158] [Frequency-Semantic Enhanced Variational Autoencoder for Zero-Shot Skeleton-based Action Recognition](https://arxiv.org/abs/2506.22179)
*Wenhan Wu,Zhishuai Guo,Chen Chen,Hongfei Xue,Aidong Lu*

Main category: cs.CV

TL;DR: 提出FS - VAE模型进行零样本骨架动作识别，含三组件，评估显示能提升识别效果。


<details>
  <summary>Details</summary>
Motivation: 以往方法主要关注视觉和语义表示对齐，忽略语义空间中细粒度动作模式，本文旨在解决此局限。

Method: 提出Frequency - Semantic Enhanced Variational Autoencoder (FS - VAE)，包含基于频率的增强模块、基于语义的多级对齐动作描述和校准的交叉对齐损失。

Result: 在基准测试上的评估证明了方法的有效性，频率增强的语义特征能有效区分视觉和语义上相似的动作簇。

Conclusion: 频率增强语义特征可提高零样本动作识别的鲁棒性。

Abstract: Zero-shot skeleton-based action recognition aims to develop models capable of
identifying actions beyond the categories encountered during training. Previous
approaches have primarily focused on aligning visual and semantic
representations but often overlooked the importance of fine-grained action
patterns in the semantic space (e.g., the hand movements in drinking water and
brushing teeth). To address these limitations, we propose a Frequency-Semantic
Enhanced Variational Autoencoder (FS-VAE) to explore the skeleton semantic
representation learning with frequency decomposition. FS-VAE consists of three
key components: 1) a frequency-based enhancement module with high- and
low-frequency adjustments to enrich the skeletal semantics learning and improve
the robustness of zero-shot action recognition; 2) a semantic-based action
description with multilevel alignment to capture both local details and global
correspondence, effectively bridging the semantic gap and compensating for the
inherent loss of information in skeleton sequences; 3) a calibrated
cross-alignment loss that enables valid skeleton-text pairs to counterbalance
ambiguous ones, mitigating discrepancies and ambiguities in skeleton and text
features, thereby ensuring robust alignment. Evaluations on the benchmarks
demonstrate the effectiveness of our approach, validating that
frequency-enhanced semantic features enable robust differentiation of visually
and semantically similar action clusters, improving zero-shot action
recognition.

</details>


### [159] [Boosting Classification with Quantum-Inspired Augmentations](https://arxiv.org/abs/2506.22241)
*Matthias Tschöpe,Vitor Fortes Rey,Sogo Pierre Sanon,Paul Lukowicz,Nikolaos Palaiodimopoulos,Maximilian Kiefer-Emmanouilidis*

Main category: cs.CV

TL;DR: 本文研究随机Bloch球旋转作为量子启发的数据增强技术，在ImageNet数据集上提升图像分类性能，并探讨更强酉增强及隐私计算相关问题。


<details>
  <summary>Details</summary>
Motivation: 理解量子门小扰动对量子机器学习的影响，利用其作为数据增强源提升性能，探索量子启发方法改进经典机器学习。

Method: 研究随机Bloch球旋转，直接应用小角度Bloch旋转到经典数据。

Result: 在ImageNet数据集上，量子启发增强方法使Top - 1准确率提高3%，Top - 5准确率提高2.5%，F1分数从8%提升到12%。

Conclusion: 该增强方法能提升图像分类性能，但此增强方法和简单SU(2)变换不能增强差分隐私。

Abstract: Understanding the impact of small quantum gate perturbations, which are
common in quantum digital devices but absent in classical computers, is crucial
for identifying potential advantages in quantum machine learning. While these
perturbations are typically seen as detrimental to quantum computation, they
can actually enhance performance by serving as a natural source of data
augmentation. Additionally, they can often be efficiently simulated on
classical hardware, enabling quantum-inspired approaches to improve classical
machine learning methods. In this paper, we investigate random Bloch sphere
rotations, which are fundamental SU(2) transformations, as a simple yet
effective quantum-inspired data augmentation technique. Unlike conventional
augmentations such as flipping, rotating, or cropping, quantum transformations
lack intuitive spatial interpretations, making their application to tasks like
image classification less straightforward. While common quantum augmentation
methods rely on applying quantum models or trainable quanvolutional layers to
classical datasets, we focus on the direct application of small-angle Bloch
rotations and their effect on classical data. Using the large-scale ImageNet
dataset, we demonstrate that our quantum-inspired augmentation method improves
image classification performance, increasing Top-1 accuracy by 3%, Top-5
accuracy by 2.5%, and the F$_1$ score from 8% to 12% compared to standard
classical augmentation methods. Finally, we examine the use of stronger unitary
augmentations. Although these transformations preserve information in
principle, they result in visually unrecognizable images with potential
applications for privacy computations. However, we show that our augmentation
approach and simple SU(2) transformations do not enhance differential privacy
and discuss the implications of this limitation.

</details>


### [160] [RoomCraft: Controllable and Complete 3D Indoor Scene Generation](https://arxiv.org/abs/2506.22291)
*Mengqi Zhou,Xipeng Wang,Yuxi Wang,Zhaoxiang Zhang*

Main category: cs.CV

TL;DR: 提出RoomCraft多阶段管道将用户输入转化为3D室内场景，结合场景生成管道与约束驱动优化框架，实验显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有神经生成方法有重复元素，程序方法在多约束场景有物体碰撞问题，需解决生成3D室内场景的局限性。

Method: 结合场景生成管道与约束驱动优化框架，提取信息构建网络，用HDFS算法生成优化放置序列，引入统一约束表示和冲突感知定位策略。

Result: RoomCraft在多种输入方式下生成的房间布局，在真实感、语义连贯性和视觉吸引力上显著优于现有方法。

Conclusion: RoomCraft能有效解决现有方法在生成3D室内场景的问题，实现更优的生成效果。

Abstract: Generating realistic 3D indoor scenes from user inputs remains a challenging
problem in computer vision and graphics, requiring careful balance of geometric
consistency, spatial relationships, and visual realism. While neural generation
methods often produce repetitive elements due to limited global spatial
reasoning, procedural approaches can leverage constraints for controllable
generation but struggle with multi-constraint scenarios. When constraints
become numerous, object collisions frequently occur, forcing the removal of
furniture items and compromising layout completeness.
  To address these limitations, we propose RoomCraft, a multi-stage pipeline
that converts real images, sketches, or text descriptions into coherent 3D
indoor scenes. Our approach combines a scene generation pipeline with a
constraint-driven optimization framework. The pipeline first extracts
high-level scene information from user inputs and organizes it into a
structured format containing room type, furniture items, and spatial relations.
It then constructs a spatial relationship network to represent furniture
arrangements and generates an optimized placement sequence using a
heuristic-based depth-first search (HDFS) algorithm to ensure layout coherence.
To handle complex multi-constraint scenarios, we introduce a unified constraint
representation that processes both formal specifications and natural language
inputs, enabling flexible constraint-oriented adjustments through a
comprehensive action space design. Additionally, we propose a Conflict-Aware
Positioning Strategy (CAPS) that dynamically adjusts placement weights to
minimize furniture collisions and ensure layout completeness.
  Extensive experiments demonstrate that RoomCraft significantly outperforms
existing methods in generating realistic, semantically coherent, and visually
appealing room layouts across diverse input modalities.

</details>


### [161] [From Ground to Air: Noise Robustness in Vision Transformers and CNNs for Event-Based Vehicle Classification with Potential UAV Applications](https://arxiv.org/abs/2506.22360)
*Nouf Almesafri,Hector Figueiredo,Miguel Arana-Catania*

Main category: cs.CV

TL;DR: 研究卷积神经网络和视觉变换器在基于事件的相机上的性能，微调ResNet34和ViT B16模型，评估不同条件下表现并比较，成果对无人机场景有应用前景。


<details>
  <summary>Details</summary>
Motivation: 探究适用于基于事件相机的计算机视觉深度学习架构性能，因该相机适合动态环境如无人机和自动驾驶车辆。

Method: 选取ResNet34和ViT B16模型，在GEN1事件数据集上微调，在标准和模拟噪声条件下评估比较。

Result: 在干净GEN1数据集上，ResNet34和ViT B16准确率分别为88%和86%，ResNet34分类准确率略高，ViT B16有显著鲁棒性。

Conclusion: 研究虽聚焦地面车辆分类，但方法和结果对无人机场景有应用潜力。

Abstract: This study investigates the performance of the two most relevant computer
vision deep learning architectures, Convolutional Neural Network and Vision
Transformer, for event-based cameras. These cameras capture scene changes,
unlike traditional frame-based cameras with capture static images, and are
particularly suited for dynamic environments such as UAVs and autonomous
vehicles. The deep learning models studied in this work are ResNet34 and ViT
B16, fine-tuned on the GEN1 event-based dataset. The research evaluates and
compares these models under both standard conditions and in the presence of
simulated noise. Initial evaluations on the clean GEN1 dataset reveal that
ResNet34 and ViT B16 achieve accuracies of 88% and 86%, respectively, with
ResNet34 showing a slight advantage in classification accuracy. However, the
ViT B16 model demonstrates notable robustness, particularly given its
pre-training on a smaller dataset. Although this study focuses on ground-based
vehicle classification, the methodologies and findings hold significant promise
for adaptation to UAV contexts, including aerial object classification and
event-based vision systems for aviation-related tasks.

</details>


### [162] [A Deep Learning framework for building damage assessment using VHR SAR and geospatial data: demonstration on the 2023 Turkiye Earthquake](https://arxiv.org/abs/2506.22338)
*Luigi Russo,Deodato Tapete,Silvia Liberata Ullo,Paolo Gamba*

Main category: cs.CV

TL;DR: 本文提出用于检测建筑物损坏的多模态深度学习框架，利用单日期高分辨率SAR图像和辅助地理空间数据，提高检测准确性和泛化性。


<details>
  <summary>Details</summary>
Motivation: 光学卫星图像用于灾害测绘时受云层覆盖或缺乏灾前图像的限制，需新方法进行建筑物损坏识别。

Method: 引入多模态深度学习框架，整合SAR图像补丁、OSM建筑足迹、DSM数据和GEM结构与暴露属性，仅利用灾后数据。

Result: 使用土耳其2023年地震新数据集验证，结合地理空间特征显著提升检测性能和泛化性。

Conclusion: 该方法无需灾前数据，能可靠快速评估建筑物损坏，数据生成自动化可扩展，支持有效灾害管理和恢复。

Abstract: Building damage identification shortly after a disaster is crucial for
guiding emergency response and recovery efforts. Although optical satellite
imagery is commonly used for disaster mapping, its effectiveness is often
hampered by cloud cover or the absence of pre-event acquisitions. To overcome
these challenges, we introduce a novel multimodal deep learning (DL) framework
for detecting building damage using single-date very high resolution (VHR)
Synthetic Aperture Radar (SAR) imagery from the Italian Space Agency (ASI)
COSMO SkyMed (CSK) constellation, complemented by auxiliary geospatial data.
Our method integrates SAR image patches, OpenStreetMap (OSM) building
footprints, digital surface model (DSM) data, and structural and exposure
attributes from the Global Earthquake Model (GEM) to improve detection accuracy
and contextual interpretation. Unlike existing approaches that depend on pre
and post event imagery, our model utilizes only post event data, facilitating
rapid deployment in critical scenarios. The framework effectiveness is
demonstrated using a new dataset from the 2023 earthquake in Turkey, covering
multiple cities with diverse urban settings. Results highlight that
incorporating geospatial features significantly enhances detection performance
and generalizability to previously unseen areas. By combining SAR imagery with
detailed vulnerability and exposure information, our approach provides reliable
and rapid building damage assessments without the dependency from available
pre-event data. Moreover, the automated and scalable data generation process
ensures the framework's applicability across diverse disaster-affected regions,
underscoring its potential to support effective disaster management and
recovery efforts. Code and data will be made available upon acceptance of the
paper.

</details>


### [163] [Can Video Large Multimodal Models Think Like Doubters-or Double-Down: A Study on Defeasible Video Entailment](https://arxiv.org/abs/2506.22385)
*Yue Zhang,Jilei Sun,Yunhui Guo,Vibhav Gogate*

Main category: cs.CV

TL;DR: 本文提出可废止视频蕴含任务DVidE，针对分类和生成任务分别提出框架，引入新基准数据集和评估指标，实验表明方法能提升VLMMs动态推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有视频大跨模态模型在抽象和自适应推理方面存在不足，缺乏根据新信息更新推理的能力。

Method: 提出可废止视频蕴含任务DVidE，针对分类任务提出反事实思维链框架，针对生成任务结合ASR输出和大语言模型；引入新基准数据集和基于LLM的评估指标。

Result: 实验结果显示显著改进。

Conclusion: 所提方法能有效增强VLMMs的动态推理能力。

Abstract: Video Large Multimodal Models (VLMMs) have made impressive strides in
understanding video content, but they often struggle with abstract and adaptive
reasoning-the ability to revise their interpretations when new information
emerges. In reality, conclusions are rarely set in stone; additional context
can strengthen or weaken an initial inference. To address this, we introduce
Defeasible Video Entailment (DVidE), a new task that challenges models to think
like doubters, constantly updating their reasoning based on evolving evidence.
In DVidE, given a video premise and a textual hypothesis, models must determine
whether a new update strengthens or weakens the hypothesis (classification
version) or generate a coherent update that modifies the entailment
relationship (generation version). For solving the classification task, we
propose the Chain of Counterfactual Thought framework, utilizing counterfactual
reasoning, ASR-enhanced video content, and rationale refinement to reduce
inference bias. For the generation task, we develop a framework that combines
ASR output with a Large Language Model (LLM) to produce coherent, contextually
relevant updates aligned with the intended strengthener or weakener goals.
Additionally, we introduce a novel benchmark dataset, with
strengthener/weakener annotations and an LLM-based evaluation metric
specifically designed for assessing generative performance. Experimental
results demonstrate significant improvements, highlighting our proposed method
in enhancing dynamic reasoning capabilities of VLMMs.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [164] [Fetal Sleep: A Cross-Species Review of Physiology, Measurement, and Classification](https://arxiv.org/abs/2506.21828)
*Weitao Tang,Johann Vargas-Calixto,Nasim Katebi,Robert Galinsky,Gari D. Clifford,Faezeh Marzbanrad*

Main category: q-bio.NC

TL;DR: 该综述整合八十多年胎儿睡眠研究，对比人及动物睡眠模式，探讨研究方法与睡眠干扰因素，为开发监测技术奠定基础。


<details>
  <summary>Details</summary>
Motivation: 胎儿睡眠是产前神经发育重要方面，了解其模式有助于早期脑成熟研究和检测神经问题。

Method: 综合分析相关研究，对比人及大型动物睡眠模式，回顾侵入性和非侵入性研究方法，考察计算分类方法。

Result: 明确了物种间睡眠模式差异及存在的睡眠状态类似物，讨论了宫内状况对胎儿睡眠的干扰。

Conclusion: 为开发客观、多模式、非侵入性胎儿睡眠监测技术提供了全面基础，以支持产前护理的早期诊断和干预。

Abstract: Fetal sleep is a relatively underexplored yet vital aspect of prenatal
neurodevelopment. Understanding fetal sleep patterns could provide insights
into early brain maturation and help clinicians detect signs of neurological
compromise that arise due to fetal hypoxia or fetal growth restriction. This
review synthesizes over eight decades of research on the physiological
characteristics, ontogeny, and regulation of fetal sleep. We compare
sleep-state patterns in humans and large animal models, highlighting
species-specific differences and the presence of sleep-state analogs. We review
both invasive techniques in animals and non-invasive modalities in humans.
Computational methods for sleep-state classification are also examined,
including rule-based approaches (with and without clustering-based
preprocessing) and state-of-the-art deep learning techniques. Finally, we
discuss how intrauterine conditions such as hypoxia and fetal growth
restriction can disrupt fetal sleep. This review provides a comprehensive
foundation for the development of objective, multimodal, and non-invasive fetal
sleep monitoring technologies to support early diagnosis and intervention in
prenatal care.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [165] [UnMix-NeRF: Spectral Unmixing Meets Neural Radiance Fields](https://arxiv.org/abs/2506.21884)
*Fabian Perez,Sara Rojas,Carlos Hinojosa,Hoover Rueda-Chacón,Bernard Ghanem*

Main category: eess.IV

TL;DR: 提出UnMix - NeRF框架，结合光谱解混与NeRF，实现高光谱新视图合成和无监督材料分割，实验显示优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 基于NeRF的分割方法仅依赖RGB数据，缺乏固有材料属性，限制了准确的材料感知，而材料感知在机器人、增强现实等应用中至关重要。

Method: 将光谱解混集成到NeRF中，通过漫反射和镜面反射分量对光谱反射率建模，用全局端元字典表示纯材料特征，用每点丰度捕获其分布，利用沿学习到的端元的光谱特征预测进行无监督材料聚类。

Result: 通过大量实验验证，在光谱重建和材料分割方面优于现有方法。

Conclusion: UnMix - NeRF框架有效，能实现高光谱新视图合成、无监督材料分割和基于材料的灵活场景编辑。

Abstract: Neural Radiance Field (NeRF)-based segmentation methods focus on object
semantics and rely solely on RGB data, lacking intrinsic material properties.
This limitation restricts accurate material perception, which is crucial for
robotics, augmented reality, simulation, and other applications. We introduce
UnMix-NeRF, a framework that integrates spectral unmixing into NeRF, enabling
joint hyperspectral novel view synthesis and unsupervised material
segmentation. Our method models spectral reflectance via diffuse and specular
components, where a learned dictionary of global endmembers represents pure
material signatures, and per-point abundances capture their distribution. For
material segmentation, we use spectral signature predictions along learned
endmembers, allowing unsupervised material clustering. Additionally, UnMix-NeRF
enables scene editing by modifying learned endmember dictionaries for flexible
material-based appearance manipulation. Extensive experiments validate our
approach, demonstrating superior spectral reconstruction and material
segmentation to existing methods. Project page:
https://www.factral.co/UnMix-NeRF.

</details>


### [166] [Dehazing Light Microscopy Images with Guided Conditional Flow Matching: finding a sweet spot between fidelity and realism](https://arxiv.org/abs/2506.22397)
*Anirban Ray,Ashesh,Florian Jug*

Main category: eess.IV

TL;DR: 提出用于去雾光学显微镜图像的HazeMatching方法，在保真度和真实感间取得平衡，实验表现良好且代码数据将公开。


<details>
  <summary>Details</summary>
Motivation: 现有计算去雾方法要么重保真度轻真实感，要么有真实感但缺乏定量准确性，需找到平衡两者的方法。

Method: 通过在条件速度场中用模糊观察引导生成过程，调整条件流匹配框架。

Result: 在5个数据集上评估，与7个基线对比，平均能在保真度和真实感间取得一致平衡，校准分析表明预测校准良好。

Conclusion: HazeMatching有效平衡了去雾结果的保真度和单个预测的真实感，且无需显式退化算子，易应用于实际显微镜数据。

Abstract: Fluorescence microscopy is a major driver of scientific progress in the life
sciences. Although high-end confocal microscopes are capable of filtering
out-of-focus light, cheaper and more accessible microscopy modalities, such as
widefield microscopy, can not, which consequently leads to hazy image data.
Computational dehazing is trying to combine the best of both worlds, leading to
cheap microscopy but crisp-looking images. The perception-distortion trade-off
tells us that we can optimize either for data fidelity, e.g. low MSE or high
PSNR, or for data realism, measured by perceptual metrics such as LPIPS or FID.
Existing methods either prioritize fidelity at the expense of realism, or
produce perceptually convincing results that lack quantitative accuracy. In
this work, we propose HazeMatching, a novel iterative method for dehazing light
microscopy images, which effectively balances these objectives. Our goal was to
find a balanced trade-off between the fidelity of the dehazing results and the
realism of individual predictions (samples). We achieve this by adapting the
conditional flow matching framework by guiding the generative process with a
hazy observation in the conditional velocity field. We evaluate HazeMatching on
5 datasets, covering both synthetic and real data, assessing both distortion
and perceptual quality. Our method is compared against 7 baselines, achieving a
consistent balance between fidelity and realism on average. Additionally, with
calibration analysis, we show that HazeMatching produces well-calibrated
predictions. Note that our method does not need an explicit degradation
operator to exist, making it easily applicable on real microscopy data. All
data used for training and evaluation and our code will be publicly available
under a permissive license.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [167] [Joint Task Offloading and Resource Allocation in Low-Altitude MEC via Graph Attention Diffusion](https://arxiv.org/abs/2506.21933)
*Yifan Xue,Ruihuai Liang,Bo Yang,Xuelin Cao,Zhiwen Yu,Mérouane Debbah,Chau Yuen*

Main category: cs.NI

TL;DR: 本文针对低空经济网络中地空一体化多接入边缘计算系统的任务调度问题，构建三层异构MEC系统架构，提出图注意力扩散解决方案生成器GADSG，实验表明其性能优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 随着低空经济发展，地空一体化多接入边缘计算系统的实时和智能任务调度需求增加，任务卸载和资源分配面临节点异构、通信链路不稳定和任务动态变化等挑战。

Method: 构建三层异构MEC系统架构，将卸载决策和资源分配联合优化问题抽象为图结构建模任务，提出图注意力扩散解决方案生成器GADSG。

Result: 构建多个不同规模和拓扑的模拟数据集，实验显示GADSG模型在优化性能、鲁棒性和跨任务结构泛化性方面显著优于现有基线方法。

Conclusion: GADSG模型在动态复杂的低空经济网络环境中具有高效任务调度的强大潜力。

Abstract: With the rapid development of the low-altitude economy, air-ground integrated
multi-access edge computing (MEC) systems are facing increasing demands for
real-time and intelligent task scheduling. In such systems, task offloading and
resource allocation encounter multiple challenges, including node
heterogeneity, unstable communication links, and dynamic task variations. To
address these issues, this paper constructs a three-layer heterogeneous MEC
system architecture for low-altitude economic networks, encompassing aerial and
ground users as well as edge servers. The system is systematically modeled from
the perspectives of communication channels, computational costs, and constraint
conditions, and the joint optimization problem of offloading decisions and
resource allocation is uniformly abstracted into a graph-structured modeling
task. On this basis, we propose a graph attention diffusion-based solution
generator (GADSG). This method integrates the contextual awareness of graph
attention networks with the solution distribution learning capability of
diffusion models, enabling joint modeling and optimization of discrete
offloading variables and continuous resource allocation variables within a
high-dimensional latent space. We construct multiple simulation datasets with
varying scales and topologies. Extensive experiments demonstrate that the
proposed GADSG model significantly outperforms existing baseline methods in
terms of optimization performance, robustness, and generalization across task
structures, showing strong potential for efficient task scheduling in dynamic
and complex low-altitude economic network environments.

</details>


### [168] [Concept-Level AI for Telecom: Moving Beyond Large Language Models](https://arxiv.org/abs/2506.22359)
*Viswanath Kumarskandpriya,Abdulhalim Dandoush,Abbas Bradai,Ali Belgacem*

Main category: cs.NI

TL;DR: 电信网络领域面临变革，LLMs处理电信问题有局限，LCMs能克服其缺点，采用LCMs是实现AI驱动电信管理的必要飞跃。


<details>
  <summary>Details</summary>
Motivation: 电信和网络领域需应对复杂管理需求，现有LLMs处理电信特定要求存在困难，需更好解决方案。

Method: 通过采用双曲潜在空间进行分层表示，将复杂多层网络交互封装在简洁概念嵌入中。

Result: LCMs克服了LLMs在内存效率、跨层关联和原生多模态集成方面的关键缺点。

Conclusion: 采用LCMs是实现强大有效的AI驱动电信管理的必要进化步骤。

Abstract: The telecommunications and networking domain stands at the precipice of a
transformative era, driven by the necessity to manage increasingly complex,
hierarchical, multi administrative domains (i.e., several operators on the same
path) and multilingual systems. Recent research has demonstrated that Large
Language Models (LLMs), with their exceptional general-purpose text analysis
and code generation capabilities, can be effectively applied to certain telecom
problems (e.g., auto-configuration of data plan to meet certain application
requirements). However, due to their inherent token-by-token processing and
limited capacity for maintaining extended context, LLMs struggle to fulfill
telecom-specific requirements such as cross-layer dependency cascades (i.e.,
over OSI), temporal-spatial fault correlation, and real-time distributed
coordination. In contrast, Large Concept Models (LCMs), which reason at the
abstraction level of semantic concepts rather than individual lexical tokens,
offer a fundamentally superior approach for addressing these telecom
challenges. By employing hyperbolic latent spaces for hierarchical
representation and encapsulating complex multi-layered network interactions
within concise concept embeddings, LCMs overcome critical shortcomings of LLMs
in terms of memory efficiency, cross-layer correlation, and native multimodal
integration. This paper argues that adopting LCMs is not simply an incremental
step, but a necessary evolutionary leap toward achieving robust and effective
AI-driven telecom management.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [169] [A Multi-Agent Probabilistic Inference Framework Inspired by Kairanban-Style CoT System with IdoBata Conversation for Debiasing](https://arxiv.org/abs/2506.21565)
*Takato Ueno,Keito Inoshita*

Main category: cs.CL

TL;DR: 受日本传统交流方式启发，提出多智能体推理框架KCS+IBC用于情感分析，实验显示其有平衡预测聚合与多样性的能力，未来将定量评估影响并开发更先进系统。


<details>
  <summary>Details</summary>
Motivation: 受日本传统信息交流方式启发，解决情感分析中的偏差缓解、可解释性提升和概率预测问题。

Method: 提出多智能体推理框架KCS+IBC，除顺序共享预测结果外，引入中期随意对话环节，结合正式推理与个人观点，进行概率情感预测。

Result: KCS在各数据集上的准确率与单个大语言模型相当，KCS+IBC在推理后期熵持续降低、方差逐渐增加。

Conclusion: 该框架有平衡预测聚合与多样性的能力，未来需定量评估其对偏差校正的影响，开发更先进的情感分析系统。

Abstract: Japan's kairanban culture and idobata conversations have long functioned as
traditional communication practices that foster nuanced dialogue among
community members and contribute to the formation of social balance. Inspired
by these information exchange processes, this study proposes a multi-agent
inference framework (KCS+IBC) that integrates multiple large language models
(LLMs) to achieve bias mitigation, improved explainability, and probabilistic
prediction in sentiment analysis. In addition to sequentially sharing
prediction results, the proposed method incorporates a mid-phase casual
dialogue session to blend formal inference with individual perspectives and
introduces probabilistic sentiment prediction. Experimental results show that
KCS achieves accuracy comparable to that of a single LLM across datasets, while
KCS+IBC exhibits a consistent decrease in entropy and a gradual increase in
variance during the latter stages of inference, suggesting the framework's
ability to balance aggregation and diversity of predictions. Future work will
quantitatively assess the impact of these characteristics on bias correction
and aim to develop more advanced sentiment analysis systems.

</details>


### [170] [MMLU-CF: A Contamination-free Multi-task Language Understanding Benchmark](https://arxiv.org/abs/2412.15194)
*Qihao Zhao,Yangyu Huang,Tengchao Lv,Lei Cui,Qinzheng Sun,Shaoguang Mao,Xin Zhang,Ying Xin,Qiufeng Yin,Scarlett Li,Furu Wei*

Main category: cs.CL

TL;DR: 现有MCQ基准存在污染问题，本文提出无污染且更具挑战性的MCQ基准MMLU - CF，评估主流LLMs效果好。


<details>
  <summary>Details</summary>
Motivation: 现有MCQ基准因开源和LLMs训练数据来源广泛导致基准污染，评估结果不可靠，需改进。

Method: 从更广泛领域获取数据，设计三个去污染规则避免无意数据泄漏；将基准分为验证集和测试集，测试集闭源，验证集开源。

Result: 评估主流LLMs，强大的GPT - 4o在测试集5 - shot得分73.4%，0 - shot得分71.9%。

Conclusion: 所提方法能创建更严格、无污染的评估标准。

Abstract: Multiple-choice question (MCQ) datasets like Massive Multitask Language
Understanding (MMLU) are widely used to evaluate the commonsense,
understanding, and problem-solving abilities of large language models (LLMs).
However, the open-source nature of these benchmarks and the broad sources of
training data for LLMs have inevitably led to benchmark contamination,
resulting in unreliable evaluation results. To alleviate this issue, we propose
a contamination-free and more challenging MCQ benchmark called MMLU-CF. This
benchmark reassesses LLMs' understanding of world knowledge by averting both
unintentional and malicious data leakage. To avoid unintentional data leakage,
we source data from a broader domain and design three decontamination rules. To
prevent malicious data leakage, we divide the benchmark into validation and
test sets with similar difficulty and subject distributions. The test set
remains closed-source to ensure reliable results, while the validation set is
publicly available to promote transparency and facilitate independent
verification. Our evaluation of mainstream LLMs reveals that the powerful
GPT-4o achieves merely a 5-shot score of 73.4% and a 0-shot score of 71.9% on
the test set, which indicates the effectiveness of our approach in creating a
more rigorous and contamination-free evaluation standard. The GitHub repository
is available at https://github.com/microsoft/MMLU-CF and the dataset refers to
https://huggingface.co/datasets/microsoft/MMLU-CF.

</details>


### [171] [Data Efficacy for Language Model Training](https://arxiv.org/abs/2506.21545)
*Yalun Dai,Yangyu Huang,Xin Zhang,Wenshan Wu,Chong Li,Wenhui Lu,Shijie Cao,Li Dong,Scarlett Li*

Main category: cs.CL

TL;DR: 提出考虑语言模型训练中数据效能的通用范式DELT，包含数据评分、选择和排序组件，实验验证其有效性，认为数据效能是有前景的基础领域。


<details>
  <summary>Details</summary>
Motivation: 现有研究聚焦数据效率，为补充其不足，定义数据效能并研究训练数据组织优化对性能的提升。

Method: 引入通用范式DELT，设计Learnability - Quality Scoring (LQS)作为数据评分实例，设计Folding Ordering (FO)作为数据排序实例。

Result: 不同的DELT实例在不增加数据规模和模型大小的情况下不同程度提升LM性能，LQS和Folding组合提升最显著，数据选择可同时实现数据效能和效率。

Conclusion: 数据效能是语言模型训练中有前景的基础领域。

Abstract: Data is fundamental to the training of language models (LM). Recent research
has been dedicated to data efficiency, which aims to maximize performance by
selecting a minimal or optimal subset of training data. Techniques such as data
filtering, sampling, and selection play a crucial role in this area. To
complement it, we define Data Efficacy, which focuses on maximizing performance
by optimizing the organization of training data and remains relatively
underexplored. This work introduces a general paradigm, DELT, for considering
data efficacy in LM training, which highlights the significance of training
data organization. DELT comprises three components: Data Scoring, Data
Selection, and Data Ordering. Among these components, we design
Learnability-Quality Scoring (LQS), as a new instance of Data Scoring, which
considers both the learnability and quality of each data sample from the
gradient consistency perspective. We also devise Folding Ordering (FO), as a
novel instance of Data Ordering, which addresses issues such as model
forgetting and data distribution bias. Comprehensive experiments validate the
data efficacy in LM training, which demonstrates the following: Firstly,
various instances of the proposed DELT enhance LM performance to varying
degrees without increasing the data scale and model size. Secondly, among these
instances, the combination of our proposed LQS for data scoring and Folding for
data ordering achieves the most significant improvement. Lastly, data efficacy
can be achieved together with data efficiency by applying data selection.
Therefore, we believe that data efficacy is a promising foundational area in LM
training.

</details>


### [172] [Evaluation of LLM-based Strategies for the Extraction of Food Product Information from Online Shops](https://arxiv.org/abs/2506.21585)
*Christoph Brosch,Sian Brumm,Rolf Krieger,Jonas Scheffler*

Main category: cs.CL

TL;DR: 本文聚焦在线零售商的食品页面，比较两种基于大语言模型的结构化信息提取方法，发现间接提取法虽准确率略低，但能大幅减少大语言模型调用次数，提高效率并降低成本。


<details>
  <summary>Details</summary>
Motivation: 探索利用生成式AI和大语言模型从网页自动提取结构化信息的方法，聚焦在线零售商食品页面关键产品属性的提取。

Method: 比较直接提取和通过生成函数间接提取两种基于大语言模型的方法，在3000个来自三家不同网店的食品页面数据集上，从准确率、效率和成本方面进行评估。

Result: 间接提取法准确率96.48%，比直接提取法低1.61%，但减少了95.82%的大语言模型调用次数，提升了效率并降低了运营成本。

Conclusion: 间接提取法能为使用大语言模型从基于模板的网页进行大规模信息提取任务提供可扩展且经济高效的解决方案。

Abstract: Generative AI and large language models (LLMs) offer significant potential
for automating the extraction of structured information from web pages. In this
work, we focus on food product pages from online retailers and explore
schema-constrained extraction approaches to retrieve key product attributes,
such as ingredient lists and nutrition tables. We compare two LLM-based
approaches, direct extraction and indirect extraction via generated functions,
evaluating them in terms of accuracy, efficiency, and cost on a curated dataset
of 3,000 food product pages from three different online shops. Our results show
that although the indirect approach achieves slightly lower accuracy (96.48\%,
$-1.61\%$ compared to direct extraction), it reduces the number of required LLM
calls by 95.82\%, leading to substantial efficiency gains and lower operational
costs. These findings suggest that indirect extraction approaches can provide
scalable and cost-effective solutions for large-scale information extraction
tasks from template-based web pages using LLMs.

</details>


### [173] [Bench to the Future: A Pastcasting Benchmark for Forecasting Agents](https://arxiv.org/abs/2506.21558)
*FutureSearch,:,Jack Wildman,Nikos I. Bosse,Daniel Hnyk,Peter Mühlbacher,Finn Hambly,Jon Evans,Dan Schwarz,Lawrence Phillips*

Main category: cs.CL

TL;DR: 介绍用于大语言模型预测的‘回溯预测’基准BTF，其有高质量问题和离线语料，实验显示有良好效果且能追踪能力进展。


<details>
  <summary>Details</summary>
Motivation: 现有预测基准无法为大语言模型预测者提供现实、封闭和可重复的环境，开发预测基准有挑战。

Method: 引入BTF基准，包含数百个已知结果的高质量问题和大量相关网页离线语料，用于对大语言模型进行‘回溯预测’测试。

Result: 回溯预测环境产生的结果与使用互联网对当时未解决问题进行预测的结果相当，能追踪预测能力随时间的稳定进展。

Conclusion: BTF可作为一个持续更新的基准，欢迎研究人员使用。

Abstract: Forecasting is a challenging task that offers a clearly measurable way to
study AI systems. Forecasting requires a large amount of research on the
internet, and evaluations require time for events to happen, making the
development of forecasting benchmarks challenging. To date, no forecasting
benchmark provides a realistic, hermetic, and repeatable environment for LLM
forecasters. We introduce Bench To the Future (BTF), a "pastcasting" benchmark
with hundreds of high-quality questions for which the resolution is already
known. Each question is accompanied by a large offline corpus of tens of
thousands of relevant web pages, enabling a way to elicit realistic "forecasts"
on past events from LLMs. Results suggest that our pastcasting environment can
produce results comparable to those based on forecasts using the internet on
at-the-time unresolved questions. We show results benchmarking agent and
chain-of-thought forecasting approaches using several LLMs, including the
recently-released Claude 4 models, and demonstrate BTF's ability to track
steady forecasting capability progress over time. We intend this to be a living
benchmark, with new questions added continually to account for increasing
training data cutoff dates. We invite researchers to contact us at
hello@futuresearch.ai to utilize our benchmark or tooling for their own
research.

</details>


### [174] [Evaluating Multimodal Large Language Models on Educational Textbook Question Answering](https://arxiv.org/abs/2506.21596)
*Hessa A. Alawwad,Anas Zafar,Areej Alhothali,Usman Naseem,Ali Alkhathlan,Amani Jamal*

Main category: cs.CL

TL;DR: 对最先进的多模态大语言模型在教科书问答任务上进行评估，引入轻量级多模态检索增强生成管道，揭示当前模型处理问题与上下文关系的局限性及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在处理复杂长课文和教育图表的推理能力未经充分测试，要对其在教科书问答任务上进行评估。

Method: 使用CK12 - QA数据集评估近期视觉 - 语言模型在不同输入配置下的表现，引入轻量级多模态检索增强生成管道将课文段落和图表整合到提示中。

Result: 展示了检索到的教育上下文对模型准确性和推理的影响，揭示了处理问题 - 上下文关系的当前局限性和潜在噪声。

Conclusion: 指出了多模态人工智能驱动学习未来研究的关键方向。

Abstract: Multimodal large language models (MLLMs) have recently achieved significant
success in vision--language tasks. However, their capacity to reason over
complex, long lessons and intricate educational diagrams that cannot be
represented as a single natural image remains largely untested. In this work,
we present the first evaluation of state-of-the-art MLLMs on the textbook
question answering (TQA) task using the CK12-QA dataset. We assess the
performance of recent vision-language models, including LLaVA and LLaMA
3.2-Vision, across various input configurations. Additionally, we introduce a
lightweight multimodal retrieval-augmented generation (RAG) pipeline that
integrates both paragraphs and diagrams from the lesson into the prompt. Our
results demonstrate the influence of retrieved educational context on model
accuracy and reasoning, while also revealing current limitations in handling
question-context relationships and the potential for noise, pointing to key
directions for future research in multimodal AI-driven learning.

</details>


### [175] [Reinforcement Learning Fine-Tuning of Language Model for Instruction Following and Math Reasoning](https://arxiv.org/abs/2506.21560)
*Yifu Han,Geo Zhang*

Main category: cs.CL

TL;DR: 研究强化学习微调技术在紧凑型语言模型Qwen2.5 - 0.5B Base上对指令遵循和数学推理任务的有效性，对比多种方法并给出结果与策略。


<details>
  <summary>Details</summary>
Motivation: 探究强化学习微调技术在紧凑型语言模型上针对指令遵循和数学推理任务的有效性。

Method: 比较监督微调（SFT）、使用偏好标记数据的直接偏好优化（DPO）和带奖励模型的强化留一法（RLOO），在数学推理任务中采用合成数据增强和带外部验证器的best - of - N采样。

Result: 带DeBERTa奖励建模的RLOO实现最佳对齐，DPO结果强且稳定；数学推理任务中，合成数据增强和best - of - N采样与外部验证器结合显著提高准确率。

Conclusion: 指出训练轻量级、任务对齐的小规模语言模型的关键权衡和实用策略。

Abstract: This study investigates the effectiveness of reinforcement learning (RL)
fine-tuning techniques on a compact language model (Qwen2.5-0.5B Base) for two
challenging tasks: instruction following and mathematical reasoning. We compare
supervised fine-tuning (SFT), Direct Preference Optimization (DPO) using
preference-labeled data, and Reinforce Leave-One-Out (RLOO) with reward models.
Our experiments show that RLOO with DeBERTa reward modeling achieves the best
alignment, while DPO provides strong and consistent results. For math reasoing
tasks, synthetic data augmentation and best-of-N sampling with an external
verifier significantly improve accuracy, showing the potential of combining
fine-tuning with inference-time tools. This study highlights key trade-offs and
practical strategies for training lightweight, task-aligned small-scale
language models.

</details>


### [176] [Overview of the ClinIQLink 2025 Shared Task on Medical Question-Answering](https://arxiv.org/abs/2506.21597)
*Brandon Colelough,Davis Bartels,Dina Demner-Fushman*

Main category: cs.CL

TL;DR: 本文介绍了ClinIQLink共享任务，包括任务设计、提供的问答对、执行平台及评分方式。


<details>
  <summary>Details</summary>
Motivation: 对大语言模型在面向全科医生水平的医学问答上进行压力测试。

Method: 提供4978个专家验证、有医学来源依据的问答对，参与系统打包在容器镜像中在指定平台执行，用自动工具对封闭式问题按精确匹配评分、开放式问题用三层嵌入指标评分，后续由医生小组审核顶级模型的回答。

Result: 未提及

Conclusion: 未提及

Abstract: In this paper, we present an overview of ClinIQLink, a shared task,
collocated with the 24th BioNLP workshop at ACL 2025, designed to stress-test
large language models (LLMs) on medically-oriented question answering aimed at
the level of a General Practitioner. The challenge supplies 4,978
expert-verified, medical source-grounded question-answer pairs that cover seven
formats: true/false, multiple choice, unordered list, short answer,
short-inverse, multi-hop, and multi-hop-inverse. Participating systems, bundled
in Docker or Apptainer images, are executed on the CodaBench platform or the
University of Maryland's Zaratan cluster. An automated harness (Task 1) scores
closed-ended items by exact match and open-ended items with a three-tier
embedding metric. A subsequent physician panel (Task 2) audits the top model
responses.

</details>


### [177] [Reasoning Isn't Enough: Examining Truth-Bias and Sycophancy in LLMs](https://arxiv.org/abs/2506.21561)
*Emilio Barkett,Olivia Long,Madhavendra Thakur*

Main category: cs.CL

TL;DR: 本文对大语言模型（LLMs）的真实性检测能力进行了大规模评估，发现推理模型的真相偏差率低于非推理模型，但仍高于人类基准，部分高级模型存在谄媚倾向，能力提升不能解决LLMs的基本真实性检测挑战。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在事实核查等领域广泛应用，但作为真相判断者的能力尚不清楚，需要对其真实性检测能力进行评估。

Method: 让8个大语言模型在多个提示下做出4800次真实性判断，比较推理和非推理模型。

Result: 推理模型的真相偏差率低于非推理模型，但仍高于人类基准；部分高级模型存在谄媚倾向，在检测真实性和欺骗性时表现不对称。

Conclusion: 仅靠能力提升无法解决大语言模型在真实性检测方面的根本挑战。

Abstract: Despite their widespread use in fact-checking, moderation, and high-stakes
decision-making, large language models (LLMs) remain poorly understood as
judges of truth. This study presents the largest evaluation to date of LLMs'
veracity detection capabilities and the first analysis of these capabilities in
reasoning models. We had eight LLMs make 4,800 veracity judgments across
several prompts, comparing reasoning and non-reasoning models. We find that
rates of truth-bias, or the likelihood to believe a statement is true,
regardless of whether it is actually true, are lower in reasoning models than
in non-reasoning models, but still higher than human benchmarks. Most
concerning, we identify sycophantic tendencies in several advanced models
(o4-mini and GPT-4.1 from OpenAI, R1 from DeepSeek), which displayed an
asymmetry in detection accuracy, performing well in truth accuracy but poorly
in deception accuracy. This suggests that capability advances alone do not
resolve fundamental veracity detection challenges in LLMs.

</details>


### [178] [Structured Attention Matters to Multimodal LLMs in Document Understanding](https://arxiv.org/abs/2506.21600)
*Chang Liu,Hongkai Chen,Yujun Cai,Hang Wu,Qingwen Ye,Ming-Hsuan Yang,Yiwei Wang*

Main category: cs.CL

TL;DR: 本文研究输入格式对多模态大语言模型（MLLMs）文档理解性能的影响，发现原始OCR文本会降低性能，提出保留结构的方法提升问答性能。


<details>
  <summary>Details</summary>
Motivation: 以往研究多关注通过精确多模态查询定位证据页，本文探究输入格式对文档理解性能的影响这一被忽视的方面。

Method: 通过系统分析发现原始OCR文本的问题，提出用LaTex范式编码文档元素的保留结构方法。

Result: 注意力分析表明结构化文本能引导模型关注有语义的区域，减少注意力浪费，提升MLLMs在不同文档类型上的问答性能。

Conclusion: 该方法无需架构修改和额外训练，能显著提升MLLMs的文档问答性能。

Abstract: Document understanding remains a significant challenge for multimodal large
language models (MLLMs). While previous research has primarily focused on
locating evidence pages through precise multimodal queries, our work
investigates a fundamental yet overlooked aspect: how input format influences
document comprehension performance. Through systematic analysis, we discover
that raw OCR text often impairs rather than improves MLLMs' performance, which
is a counterintuitive finding we attribute to attention dispersion and
structure loss. To further substantiate our hypothesis, we propose a novel
structure-preserving approach that encodes document elements using the LaTex
paradigm, maintaining the hierarchical organization and spatial relationships
critical for comprehension. Our attention analysis reveals that structured text
induces structured attention patterns on both textual and visual content,
directing models to focus on semantically meaningful regions while reducing
attention waste. This approach significantly enhances MLLMs' document question
answering performance across diverse document types without requiring
architectural modifications or additional training.

</details>


### [179] [FloorPlan-DeepSeek (FPDS): A multimodal approach to floorplan generation using vector-based next room prediction](https://arxiv.org/abs/2506.21562)
*Jun Yin,Pengyu Zeng,Jing Zhong,Peilin Li,Miao Zhang,Ran Luo,Shuai Lu*

Main category: cs.CL

TL;DR: 现有平面图生成模型与实际建筑设计流程不匹配，提出'next room prediction'范式，实验显示其在文本到平面图任务中有竞争力。


<details>
  <summary>Details</summary>
Motivation: 现有平面图生成模型多为端到端生成，与实际建筑设计的渐进迭代流程不兼容。

Method: 借鉴大语言模型的自回归'next token prediction'机制，提出'next room prediction'范式。

Result: FPDS在文本到平面图任务中与扩散模型和Tell2Design相比表现有竞争力。

Conclusion: FPDS有潜力支持未来智能建筑设计。

Abstract: In the architectural design process, floor plan generation is inherently
progressive and iterative. However, existing generative models for floor plans
are predominantly end-to-end generation that produce an entire pixel-based
layout in a single pass. This paradigm is often incompatible with the
incremental workflows observed in real-world architectural practice. To address
this issue, we draw inspiration from the autoregressive 'next token prediction'
mechanism commonly used in large language models, and propose a novel 'next
room prediction' paradigm tailored to architectural floor plan modeling.
Experimental evaluation indicates that FPDS demonstrates competitive
performance in comparison to diffusion models and Tell2Design in the
text-to-floorplan task, indicating its potential applicability in supporting
future intelligent architectural design.

</details>


### [180] [AdaptGOT: A Pre-trained Model for Adaptive Contextual POI Representation Learning](https://arxiv.org/abs/2506.21612)
*Xiaobin Ren,Xinyu Zhu,Kaiqi Zhao*

Main category: cs.CL

TL;DR: 当前POI嵌入方法有挑战，提出AdaptGOT模型，含三组件，实验证明其性能优越。


<details>
  <summary>Details</summary>
Motivation: 现有POI嵌入方法存在有效多上下文采样策略不足、多POI上下文探索不充分、通用性和泛化性差等问题。

Method: 提出AdaptGOT模型，包含上下文邻域生成、注意力机制增强的GOT表示、基于MoE的自适应编解码器架构三部分。

Result: 在两个真实数据集和多个POI任务上实验，证明AdaptGOT模型性能优越。

Conclusion: AdaptGOT模型能有效解决现有POI嵌入方法的问题，具有良好性能。

Abstract: Currently, considerable strides have been achieved in Point-of-Interest (POI)
embedding methodologies, driven by the emergence of novel POI tasks like
recommendation and classification. Despite the success of task-specific,
end-to-end models in POI embedding, several challenges remain. These include
the need for more effective multi-context sampling strategies, insufficient
exploration of multiple POI contexts, limited versatility, and inadequate
generalization. To address these issues, we propose the AdaptGOT model, which
integrates both the (Adapt)ive representation learning technique and the
Geographical-Co-Occurrence-Text (GOT) representation with a particular emphasis
on Geographical location, Co-Occurrence and Textual information. The AdaptGOT
model comprises three key components: (1) contextual neighborhood generation,
which integrates advanced mixed sampling techniques such as KNN, density-based,
importance-based, and category-aware strategies to capture complex contextual
neighborhoods; (2) an advanced GOT representation enhanced by an attention
mechanism, designed to derive high-quality, customized representations and
efficiently capture complex interrelations between POIs; and (3) the MoE-based
adaptive encoder-decoder architecture, which ensures topological consistency
and enriches contextual representation by minimizing Jensen-Shannon divergence
across varying contexts. Experiments on two real-world datasets and multiple
POI tasks substantiate the superior performance of the proposed AdaptGOT model.

</details>


### [181] [Team QUST at SemEval-2025 Task 10: Evaluating Large Language Models in Multiclass Multi-label Classification of News Entity Framing](https://arxiv.org/abs/2506.21564)
*Jiyan Liu,Youzheng Liu,Taihang Wang,Xiaoman Xu,Yimin Wang,Ye Jiang*

Main category: cs.CL

TL;DR: QUST_NLP参加SemEval - 2025 Task 7，提出三阶段检索框架用于事实核查声明检索，取得不错排名并开源代码。


<details>
  <summary>Details</summary>
Motivation: 参加SemEval - 2025 Task 7进行事实核查声明检索。

Method: 提出三阶段检索框架，先评估选择最佳候选检索模型，再用多模型重排序选Top - 10，最后加权投票确定最终结果。

Result: 在单语赛道获第5名，跨语言赛道获第7名。

Conclusion: 所提出的三阶段检索框架在相关任务中取得了较好的效果，且代码已开源。

Abstract: This paper describes the participation of QUST_NLP in the SemEval-2025 Task
7. We propose a three-stage retrieval framework specifically designed for
fact-checked claim retrieval. Initially, we evaluate the performance of several
retrieval models and select the one that yields the best results for candidate
retrieval. Next, we employ multiple re-ranking models to enhance the candidate
results, with each model selecting the Top-10 outcomes. In the final stage, we
utilize weighted voting to determine the final retrieval outcomes. Our approach
achieved 5th place in the monolingual track and 7th place in the crosslingual
track. We release our system code at:
https://github.com/warmth27/SemEval2025_Task7.

</details>


### [182] [Refine Medical Diagnosis Using Generation Augmented Retrieval and Clinical Practice Guidelines](https://arxiv.org/abs/2506.21615)
*Wenhao Li,Hongkuan Zhang,Hongwei Zhang,Zhengxu Li,Zengjie Dong,Yafan Chen,Niranjan Bidargaddi,Hong Liu*

Main category: cs.CL

TL;DR: 现有医学语言模型因使用ICD代码诊断存在局限，本文提出GARMLE - G框架，经评估优于基线，具临床部署潜力。


<details>
  <summary>Details</summary>
Motivation: 现有基于ICD代码的医学语言模型与临床诊断推理不匹配，限制临床实用性。

Method: 提出GARMLE - G框架，整合LLM预测与EHR数据创建查询，通过嵌入相似度检索CPG知识片段，融合指南内容生成建议。

Result: 开发高血压诊断原型系统，在多指标评估中优于基于RAG的基线，架构轻量适合本地医疗部署。

Conclusion: 该框架为医学语言模型提供可扩展、低成本、无幻觉的方法，有广泛临床部署潜力。

Abstract: Current medical language models, adapted from large language models (LLMs),
typically predict ICD code-based diagnosis from electronic health records
(EHRs) because these labels are readily available. However, ICD codes do not
capture the nuanced, context-rich reasoning clinicians use for diagnosis.
Clinicians synthesize diverse patient data and reference clinical practice
guidelines (CPGs) to make evidence-based decisions. This misalignment limits
the clinical utility of existing models. We introduce GARMLE-G, a
Generation-Augmented Retrieval framework that grounds medical language model
outputs in authoritative CPGs. Unlike conventional Retrieval-Augmented
Generation based approaches, GARMLE-G enables hallucination-free outputs by
directly retrieving authoritative guideline content without relying on
model-generated text. It (1) integrates LLM predictions with EHR data to create
semantically rich queries, (2) retrieves relevant CPG knowledge snippets via
embedding similarity, and (3) fuses guideline content with model output to
generate clinically aligned recommendations. A prototype system for
hypertension diagnosis was developed and evaluated on multiple metrics,
demonstrating superior retrieval precision, semantic relevance, and clinical
guideline adherence compared to RAG-based baselines, while maintaining a
lightweight architecture suitable for localized healthcare deployment. This
work provides a scalable, low-cost, and hallucination-free method for grounding
medical language models in evidence-based clinical practice, with strong
potential for broader clinical deployment.

</details>


### [183] [The Saturation Point of Backtranslation in High Quality Low Resource English Gujarati Machine Translation](https://arxiv.org/abs/2506.21566)
*Arwa Arif*

Main category: cs.CL

TL;DR: 探索使用MBART50模型进行英古翻译时回译法的有效性，发现添加回译数据未提升性能。


<details>
  <summary>Details</summary>
Motivation: 回译法在高质量低资源场景下的有效性不明，探索其在英古翻译中的效果。

Method: 使用MBART50模型，基于约50000句对的高质量平行语料训练基线系统，用单语古吉拉特语文本生成回译示例并筛选后扩充数据，用多种指标评估模型。

Result: 添加合成数据未提升翻译性能，部分情况略有下降。

Conclusion: 回译法在某些低资源场景下可能收益递减，讨论了对未来研究的启示。

Abstract: Backtranslation BT is widely used in low resource machine translation MT to
generate additional synthetic training data using monolingual corpora. While
this approach has shown strong improvements for many language pairs, its
effectiveness in high quality, low resource settings remains unclear. In this
work, we explore the effectiveness of backtranslation for English Gujarati
translation using the multilingual pretrained MBART50 model. Our baseline
system, trained on a high quality parallel corpus of approximately 50,000
sentence pairs, achieves a BLEU score of 43.8 on a validation set. We augment
this data with carefully filtered backtranslated examples generated from
monolingual Gujarati text. Surprisingly, adding this synthetic data does not
improve translation performance and, in some cases, slightly reduces it. We
evaluate our models using multiple metrics like BLEU, ChrF++, TER, BLEURT and
analyze possible reasons for this saturation. Our findings suggest that
backtranslation may reach a point of diminishing returns in certain
low-resource settings and we discuss implications for future research.

</details>


### [184] [Doc2SAR: A Synergistic Framework for High-Fidelity Extraction of Structure-Activity Relationships from Scientific Documents](https://arxiv.org/abs/2506.21625)
*Jiaxi Zhuang,Kangning Li,Jue Hou,Mingjun Xu,Zhifeng Gao,Hengxing Cai*

Main category: cs.CL

TL;DR: 本文提出DocSAR - 200基准和Doc2SAR框架用于提取分子结构 - 活性关系，实验表明其性能超现有基线。


<details>
  <summary>Details</summary>
Motivation: 从科学文献和专利中提取分子结构 - 活性关系对药物发现和材料研究至关重要，但现有方法有局限，需解决。

Method: 引入DocSAR - 200基准评估SAR提取方法，提出Doc2SAR框架，集成特定领域工具和经监督微调的MLLMs。

Result: Doc2SAR在多种文档类型上达到了最先进性能，在DocSAR - 200上整体表格召回率达80.78%，超过end2end GPT - 4o 51.48%，且具有高效推理能力，有配套网页应用。

Conclusion: Doc2SAR是一种有效的SAR提取方法，能应对现有方法的挑战。

Abstract: Extracting molecular structure-activity relationships (SARs) from scientific
literature and patents is essential for drug discovery and materials research.
However, this task remains challenging due to heterogeneous document formats
and limitations of existing methods. Specifically, rule-based approaches
relying on rigid templates fail to generalize across diverse document layouts,
while general-purpose multimodal large language models (MLLMs) lack sufficient
accuracy and reliability for specialized tasks, such as layout detection and
optical chemical structure recognition (OCSR). To address these challenges, we
introduce DocSAR-200, a rigorously annotated benchmark of 200 scientific
documents designed specifically for evaluating SAR extraction methods.
Additionally, we propose Doc2SAR, a novel synergistic framework that integrates
domain-specific tools with MLLMs enhanced via supervised fine-tuning (SFT).
Extensive experiments demonstrate that Doc2SAR achieves state-of-the-art
performance across various document types, significantly outperforming leading
end-to-end baselines. Specifically, Doc2SAR attains an overall Table Recall of
80.78% on DocSAR-200, exceeding end2end GPT-4o by 51.48%. Furthermore, Doc2SAR
demonstrates practical usability through efficient inference and is accompanied
by a web app.

</details>


### [185] [BioPars: A Pretrained Biomedical Large Language Model for Persian Biomedical Text Mining](https://arxiv.org/abs/2506.21567)
*Baqer M. Merzah,Tania Taami,Salman Asoudeh,Amir reza Hossein pour,Saeed Mirzaee,Amir Ali Bengari*

Main category: cs.CL

TL;DR: 介绍BIOPARS - BENCH数据集和BioParsQA评估集，提出BioPars评估大语言模型在生物信息学任务能力，对比ChatGPT等模型，显示BioPars在波斯语医学问答有出色结果。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型在生物信息学任务中的能力，尤其是在波斯语医学问答中的应用，解决现有模型在高级问题和细粒度推理方面的不足。

Method: 引入BIOPARS - BENCH数据集和BioParsQA评估集，提出BioPars评估方法，对比ChatGPT、Llama和Galactica等模型。

Result: BioPars在四个医学问答数据集评估中表现出色，在BioParsQA上ROUGE - L分数29.99优于GPT - 4 1.0，BERTScore达90.87等。

Conclusion: 大语言模型在生物信息学任务中需进一步微调，BioPars在波斯语医学问答尤其是长答案生成方面有良好应用前景。

Abstract: Large Language Models (LLMs) have recently gained attention in the life
sciences due to their capacity to model, extract, and apply complex biological
information. Beyond their classical use as chatbots, these systems are
increasingly used for complex analysis and problem-solving in specialized
fields, including bioinformatics. First, we introduce BIOPARS-BENCH, a dataset
from over 10,000 scientific articles, textbooks, and medical websites.
BioParsQA was also introduced to evaluate the proposed model, which consists of
5,231 Persian medical questions and answers. This study then introduces
BioPars, a simple but accurate measure designed to assess LLMs for three main
abilities: acquiring subject-specific knowledge, interpreting and synthesizing
such knowledge, and demonstrating proper evidence. Comparing ChatGPT, Llama,
and Galactica, our study highlights their ability to remember and retrieve
learned knowledge but also reveals shortcomings in addressing higher-level,
real-world questions and fine-grained inferences. These findings indicate the
need for further fine-tuning to address the capabilities of LLM in
bioinformatics tasks. To our knowledge, BioPars is the first application of LLM
in Persian medical QA, especially for generating long answers. Evaluation of
four selected medical QA datasets shows that BioPars has achieved remarkable
results compared to comparative approaches. The model on BioParsQA achieved a
ROUGE-L score of 29.99, which is an improvement over GPT-4 1.0. The model
achieved a BERTScore of 90.87 with the MMR method. The MoverScore and BLEURT
values were also higher in this model than the other three models. In addition,
the reported scores for the model are MoverScore=60.43 and BLEURT=50.78.
BioPars is an ongoing project and all resources related to its development will
be made available via the following GitHub repository:
https://github.com/amirap80/BioPars.

</details>


### [186] [DAPFAM: A Domain-Aware Patent Retrieval Dataset Aggregated at the Family Level](https://arxiv.org/abs/2506.22141)
*Iliass Ayaou,Denis Cavallucci,Hicham Chibane*

Main category: cs.CL

TL;DR: 提出新的开放访问专利检索数据集DAPFAM，介绍其特点、构建流程，给出基线实验结果并指出跨领域专利检索挑战，数据集将公开。


<details>
  <summary>Details</summary>
Motivation: 解决现有公开专利检索数据集在领域标签、多司法管辖区覆盖、查询域表示平衡和大小方面的不足。

Method: 构建了一个三步的数据整理管道，使用词汇和神经检索方法进行基线实验。

Result: 构建了包含1247个查询家族和45336个目标家族的DAPFAM数据集，得到49869个评估对，基线实验凸显跨领域专利检索挑战。

Conclusion: 新数据集DAPFAM能满足相关实验需求，且跨领域专利检索存在显著挑战。

Abstract: In the landscape of publicly available patent retrieval datasets, the need
for explicit indomain and out-of-domain labeling, multi-jurisdiction coverage,
balanced query domain representation and manageable sizes that support sub
document level experiments on moderate computational resources is often
overlooked. To address these gaps, we propose DAPFAM, a new open access
domain-aware patent retrieval dataset constructed at the simple-family level.
The dataset contains 1,247 domain balanced full text query families and 45,336
full text target families. The dataset is enriched by clear relevance judgments
(forward/backward citations as positive links, random negatives), as well as
explicit in-domain or out-of-domain relationships via a novel proposed
labelling scheme based on via International Patent Classification (IPC) codes,
resulting in 49,869 evaluation pairs. The dataset is multi jurisdictional,
requires little to no preprocessing for retrieval evaluation, and remains of a
size manageable for entities with limited ressources allowing for sub document
level retrieval experiments without excessive computational costs. We describe
our three-step data-curation pipeline, present comprehensive dataset
statistics, and provide baseline experiments using lexical and neural retrieval
methods. Our baseline experiments highlight significant challenges in
crossdomain patent retrieval. The dataset will be publicly available (for now
the access link is this repository:
https://osf.io/vbyzd/?view_only=1a40242e0d1941a58aa854af3e50cf6b).

</details>


### [187] [Hybrid-NL2SVA: Integrating RAG and Finetuning for LLM-based NL2SVA](https://arxiv.org/abs/2506.21569)
*Weihua Xiao,Derek Ekberg,Siddharth Garg,Ramesh Karri*

Main category: cs.CL

TL;DR: 提出定制的检索增强生成框架和合成微调数据集提升大语言模型在NL2SVA任务的性能，构建评估数据集，实验显示框架和微调模型有显著提升。


<details>
  <summary>Details</summary>
Motivation: 手动从自然语言属性描述编写SystemVerilog断言（NL2SVA）任务劳动密集且易出错，现有大语言模型难以理解特定领域语法和语义。

Method: 提出定制的检索增强生成（RAG）框架和合成微调数据集，微调数据集提供提示引导解释，构建评估数据集。

Result: 定制RAG框架使功能匹配的SVAs数量比GPT - 4o - mini增加58.42%，Qwen2.5 - Coder - 7B - Instruct微调后比基础Qwen模型提升59.05%。

Conclusion: 定制的RAG框架和合成微调数据集能有效提升大语言模型在NL2SVA任务的性能。

Abstract: SystemVerilog Assertions (SVAs) are critical for verifying the correctness of
hardware designs, but manually writing them from natural language property
descriptions, i.e., NL2SVA, remains a labor-intensive and error-prone task.
Recent advances in large language models (LLMs) offer opportunities to automate
this translation. However, existing models still struggle with understanding
domain-specific syntax and semantics. To enhance LLM performance in NL2SVA, we
propose a customized retrieval-augmented generation (RAG) framework and a
synthetic fine-tuning dataset that together improve LLM's performance. To
further improve lightweight models over NL2SVA, our fine-tuning dataset
provides prompt-guided explanations that teach LLMs the layer-by-layer
construction process of concurrent SVAs, enabling supervised fine-tuning that
greatly improves syntax and functionality accuracy. To evaluate the performance
of LLMs over NL2SVA, we construct the largest evaluation dataset for NL2SVA,
comprising 40 Verilog designs and 229 formally verified SVAs with detailed
annotations. Experimental results show that our customized RAG framework
increases the number of functionality matched SVAs by 58.42% over GPT-4o-mini,
while Qwen2.5-Coder-7B-Instruct fine-tuned on our fine-tuning dataset and
integrated with HybridRetrieval achieves a 59.05% over the base Qwen model.

</details>


### [188] [Random Initialization Can't Catch Up: The Advantage of Language Model Transfer for Time Series Forecasting](https://arxiv.org/abs/2506.21570)
*Roland Riachi,Kashif Rasul,Arjun Ashok,Prateek Humane,Alexis Roger,Andrew R. Williams,Yuriy Nevmyvaka,Irina Rish*

Main category: cs.CL

TL;DR: 分析预训练语言模型迁移到时间序列预测的效果，发现在低数据场景下设计选择影响大，LM验证损失持续下降有非零迁移差距。


<details>
  <summary>Details</summary>
Motivation: 在低数据场景下，基于已有预训练语言模型用于时间序列预测有效的研究，分析不同设计选择下从语言模型到时间序列预测的有效迁移情况。

Method: 分析不同设计选择，包括上游后训练、时间序列分词器和语言主干大小等。

Result: 在低数据场景下，设计选择对验证损失影响大，有明显表现更优的选择；与Hernandez等人（2021）不同，LM的验证损失在随机初始化模型验证损失收敛后仍持续平稳下降，存在非零迁移差距。

Conclusion: 有助于高效计算训练在时间序列中的有效应用，为研究模型利用的数据分布的模态无关特性开辟道路。

Abstract: Recent works have demonstrated the effectiveness of adapting pre-trained
language models (LMs) for forecasting time series in the low-data regime. We
build upon these findings by analyzing the effective transfer from language
models to time series forecasting under various design choices including
upstream post-training, time series tokenizer and language backbone size. In
the low-data regime, these design choices have a significant impact on the
validation loss, with clear-cut choices that outperform others. Contrary to
Hernandez et al. (2021), we observe that the validation loss of the LMs
continues to smoothly decrease long after the validation loss of the randomly
initialized models has converged, leading to a non-vanishing transfer gap that
holds across design choices. These findings not only help shed light on the
effective use of compute-efficient training for time series, but also open the
way for the study of modality-agnostic properties of data distributions
leveraged by these models.

</details>


### [189] [Towards Understanding the Cognitive Habits of Large Reasoning Models](https://arxiv.org/abs/2506.21571)
*Jianshuo Dong,Yujia Fu,Chuanrui Hu,Chao Zhang,Han Qiu*

Main category: cs.CL

TL;DR: 本文引入CogTest基准评估大推理模型（LRMs）认知习惯，发现LRMs有类人习惯且能自适应运用，还揭示其习惯特征的异同，以及部分习惯与有害回复的关联。


<details>
  <summary>Details</summary>
Motivation: 受特定思维链模式启发，探索LRMs是否具有类人认知习惯。

Method: 构建包含16种认知习惯、各有25个多样任务的CogTest基准，采用证据优先提取方法；对16个广泛使用的大语言模型进行评估。

Result: LRMs有类人习惯且能自适应运用；揭示LRMs认知习惯特征的异同；部分习惯与有害回复强相关。

Conclusion: 研究LRMs思维链中的持久行为模式有助于深入理解大语言模型的不当行为。

Abstract: Large Reasoning Models (LRMs), which autonomously produce a reasoning Chain
of Thought (CoT) before producing final responses, offer a promising approach
to interpreting and monitoring model behaviors. Inspired by the observation
that certain CoT patterns -- e.g., ``Wait, did I miss anything?'' --
consistently emerge across tasks, we explore whether LRMs exhibit human-like
cognitive habits. Building on Habits of Mind, a well-established framework of
cognitive habits associated with successful human problem-solving, we introduce
CogTest, a principled benchmark designed to evaluate LRMs' cognitive habits.
CogTest includes 16 cognitive habits, each instantiated with 25 diverse tasks,
and employs an evidence-first extraction method to ensure reliable habit
identification. With CogTest, we conduct a comprehensive evaluation of 16
widely used LLMs (13 LRMs and 3 non-reasoning ones). Our findings reveal that
LRMs, unlike conventional LLMs, not only exhibit human-like habits but also
adaptively deploy them according to different tasks. Finer-grained analyses
further uncover patterns of similarity and difference in LRMs' cognitive habit
profiles, particularly certain inter-family similarity (e.g., Qwen-3 models and
DeepSeek-R1). Extending the study to safety-related tasks, we observe that
certain habits, such as Taking Responsible Risks, are strongly associated with
the generation of harmful responses. These findings suggest that studying
persistent behavioral patterns in LRMs' CoTs is a valuable step toward deeper
understanding of LLM misbehavior. The code is available at:
https://github.com/jianshuod/CogTest.

</details>


### [190] [Instruction Learning Paradigms: A Dual Perspective on White-box and Black-box LLMs](https://arxiv.org/abs/2506.21573)
*Yanwei Ren,Liu Liu,Baosheng Yu,Jiayan Qiu,Quan Chen*

Main category: cs.CL

TL;DR: 提出融合黑盒与白盒范式的新框架优化大语言模型指令，在多任务评估中表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 单纯使用白盒或黑盒方法优化大语言模型指令存在计算资源需求大、成本高的问题。

Method: 引入新框架，结合黑盒模型提供指令初始化、白盒模型提供可解释性，通过语义相似性约束融合二者，进行迭代优化。

Result: 在复杂推理、跨语言泛化等广泛任务评估中，该方法始终优于现有基线。

Conclusion: 黑盒初始化与高级语义细化的融合为大语言模型驱动的应用提供可扩展、高效的解决方案。

Abstract: Optimizing instructions for large language models (LLMs) is critical for
harnessing their full potential in complex and diverse tasks. However, relying
solely on white-box approaches demands extensive computational resources and
offers limited representational capacity, while black-box models can incur
prohibitive financial costs. To address these challenges, we introduce a novel
framework that seamlessly merges the strengths of both paradigms. Black-box
models provide high-quality, diverse instruction initializations, and white-box
models supply fine-grained interpretability through hidden states and output
features. By enforcing a semantic similarity constraint, these components fuse
into a unified high-dimensional representation that captures deep semantic and
structural nuances, enabling an iterative optimization process to refine
instruction quality and adaptability. Extensive evaluations across a broad
spectrum of tasks-ranging from complex reasoning to cross-lingual
generalization-demonstrate that our approach consistently outperforms
state-of-the-art baselines. This fusion of black-box initialization with
advanced semantic refinement yields a scalable and efficient solution, paving
the way for next-generation LLM-driven applications in diverse real-world
scenarios. The source code will be released soon.

</details>


### [191] [Digital Gatekeepers: Exploring Large Language Model's Role in Immigration Decisions](https://arxiv.org/abs/2506.21574)
*Yicheng Mao,Yang Zhao*

Main category: cs.CL

TL;DR: 研究大语言模型在移民决策中的潜力，发现其有与人类策略一致的优势，但也存在刻板印象和偏见等局限。


<details>
  <summary>Details</summary>
Motivation: 全球化和移民人口增加，移民部门工作量大且需确保决策公平，大语言模型提供了解决方案。

Method: 采用混合方法，进行离散选择实验和深度访谈，研究大语言模型决策策略及公平性。

Result: 大语言模型决策能与人类策略一致，强调效用最大化和程序公平，但ChatGPT存在国籍刻板印象和偏见，倾向特权群体。

Conclusion: 大语言模型在自动化和提升移民决策方面有潜力和局限。

Abstract: With globalization and increasing immigrant populations, immigration
departments face significant work-loads and the challenge of ensuring fairness
in decision-making processes. Integrating artificial intelligence offers a
promising solution to these challenges. This study investigates the potential
of large language models (LLMs),such as GPT-3.5 and GPT-4, in supporting
immigration decision-making. Utilizing a mixed-methods approach,this paper
conducted discrete choice experiments and in-depth interviews to study LLM
decision-making strategies and whether they are fair. Our findings demonstrate
that LLMs can align their decision-making with human strategies, emphasizing
utility maximization and procedural fairness. Meanwhile, this paper also
reveals that while ChatGPT has safeguards to prevent unintentional
discrimination, it still exhibits stereotypes and biases concerning nationality
and shows preferences toward privileged group. This dual analysis highlights
both the potential and limitations of LLMs in automating and enhancing
immigration decisions.

</details>


### [192] [STRuCT-LLM: Unifying Tabular and Graph Reasoning with Reinforcement Learning for Semantic Parsing](https://arxiv.org/abs/2506.21575)
*Josefa Lia Stoisser,Marc Boubnovski Martell,Lawrence Phillips,Casper Hansen,Julien Fauqueur*

Main category: cs.CL

TL;DR: 提出STRuCT - LLM框架用于训练大语言模型对关系和图结构数据进行结构化推理，结合RL和CoT监督，引入拓扑感知奖励函数，实现跨形式转移，模型在多任务上有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有工作孤立处理关系和图形式，缺乏能对关系和图结构数据进行统一结构化推理的方法。

Method: 联合优化Text - to - SQL和Text - to - Cypher任务，使用强化学习结合思维链监督，引入基于图编辑距离的拓扑感知奖励函数，利用SQL和Cypher的共享抽象实现跨形式转移。

Result: 最大模型QwQ - 32B在语义解析任务上，Spider提升13.5%，Text2Cypher提升73.1%，在下游表格QA和知识图谱QA上有零样本泛化性能提升。

Conclusion: 可执行查询作为结构化推理的支架有效，联合训练SQL和Cypher有协同效益。

Abstract: We propose STRuCT-LLM, a unified framework for training large language models
(LLMs) to perform structured reasoning over both relational and
graph-structured data. Our approach jointly optimizes Text-to-SQL and
Text-to-Cypher tasks using reinforcement learning (RL) combined with
Chain-of-Thought (CoT) supervision. To support fine-grained optimization in
graph-based parsing, we introduce a topology-aware reward function based on
graph edit distance. Unlike prior work that treats relational and graph
formalisms in isolation, STRuCT-LLM leverages shared abstractions between SQL
and Cypher to induce cross-formalism transfer, enabling SQL training to improve
Cypher performance and vice versa - even without shared schemas. Our largest
model (QwQ-32B) achieves substantial relative improvements across tasks: on
semantic parsing, Spider improves by 13.5\% and Text2Cypher by 73.1\%. The
model also demonstrates strong zero-shot generalization, improving performance
on downstream tabular QA (TableBench: 8.5\%) and knowledge graph QA
(CR-LT-KGQA: 1.7\%) without any QA-specific supervision. These results
demonstrate both the effectiveness of executable queries as scaffolds for
structured reasoning and the synergistic benefits of jointly training on SQL
and Cypher (code available at https://github.com/bouv/STRuCT-LLM).

</details>


### [193] [Adapting Whisper for Parameter-efficient Code-Switching Speech Recognition via Soft Prompt Tuning](https://arxiv.org/abs/2506.21576)
*Hongli Yang,Yizhou Peng,Hao Huang,Sheng Li*

Main category: cs.CL

TL;DR: 研究用软提示调优（SPT）提升代码切换（CS）自动语音识别（ASR），在数据集实验中验证了方法有效性。


<details>
  <summary>Details</summary>
Motivation: 大规模多语言ASR模型在低资源场景面临挑战，探索参数高效方法来提升CS ASR并保留先验知识。

Method: 探索SPT方法，评估全微调（FFT）和仅训练软提示两种策略，引入SPT4ASR组合不同SPT变体。

Result: 实验表明深度提示调优是最有效的SPT方法，SPT4ASR方法进一步降低CS ASR错误，保持参数效率且不降低已有语言性能。

Conclusion: SPT方法在提升CS ASR方面有效，能在低资源场景发挥作用。

Abstract: Large-scale multilingual ASR models like Whisper excel in high-resource
settings but face challenges in low-resource scenarios, such as rare languages
and code-switching (CS), due to computational costs and catastrophic
forgetting. We explore Soft Prompt Tuning (SPT), a parameter-efficient method
to enhance CS ASR while preserving prior knowledge. We evaluate two strategies:
(1) full fine-tuning (FFT) of both soft prompts and the entire Whisper model,
demonstrating improved cross-lingual capabilities compared to traditional
methods, and (2) adhering to SPT's original design by freezing model parameters
and only training soft prompts. Additionally, we introduce SPT4ASR, a
combination of different SPT variants. Experiments on the SEAME and ASRU2019
datasets show that deep prompt tuning is the most effective SPT approach, and
our SPT4ASR methods achieve further error reductions in CS ASR, maintaining
parameter efficiency similar to LoRA, without degrading performance on existing
languages.

</details>


### [194] [Language-Aware Prompt Tuning for Parameter-Efficient Seamless Language Expansion in Multilingual ASR](https://arxiv.org/abs/2506.21577)
*Hongli Yang,Sheng Li,Hao Huang,Ayiduosi Tuohan,Yizhou Peng*

Main category: cs.CL

TL;DR: 本文提出Entire SPT、LAPT和SPT - Whisper应对多语言ASR语言干扰和语言扩展挑战，实验显示前两者在语言扩展任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决多语言自动语音识别中语言干扰和语言扩展时性能下降的问题。

Method: 提出Entire Soft Prompt Tuning（Entire SPT）、Language - Aware Prompt Tuning（LAPT）和SPT - Whisper工具包。

Result: 在FLEURS的三种语言实验中，Entire SPT和LAPT在语言扩展任务中分别比Decoder SPT高5.0%和16.0%。

Conclusion: 为动态多语言ASR模型提供了低计算开销的高效解决方案。

Abstract: Recent advancements in multilingual automatic speech recognition (ASR) have
been driven by large-scale end-to-end models like Whisper. However, challenges
such as language interference and expanding to unseen languages (language
expansion) without degrading performance persist. This paper addresses these
with three contributions: 1) Entire Soft Prompt Tuning (Entire SPT), which
applies soft prompts to both the encoder and decoder, enhancing feature
extraction and decoding; 2) Language-Aware Prompt Tuning (LAPT), which
leverages cross-lingual similarities to encode shared and language-specific
features using lightweight prompt matrices; 3) SPT-Whisper, a toolkit that
integrates SPT into Whisper and enables efficient continual learning.
Experiments across three languages from FLEURS demonstrate that Entire SPT and
LAPT outperform Decoder SPT by 5.0% and 16.0% in language expansion tasks,
respectively, providing an efficient solution for dynamic, multilingual ASR
models with minimal computational overhead.

</details>


### [195] [HealthQA-BR: A System-Wide Benchmark Reveals Critical Knowledge Gaps in Large Language Models](https://arxiv.org/abs/2506.21578)
*Andrew Maranhão Ventura D'addario*

Main category: cs.CL

TL;DR: 引入针对葡萄牙语医疗领域的大规模基准HealthQA - BR评估超20个大语言模型，结果显示高分掩盖缺陷，公布评估工具以推动更细致评估。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在医疗领域的评估以英语、医生为中心，忽略了患者护理的跨专业性质，需更全面现实的评估。

Method: 引入HealthQA - BR基准，该基准包含巴西国家执照和住院医师考试的5632个问题，对超20个领先大语言模型进行零样本评估。

Result: GPT 4.1等模型总体准确率高，但细分领域表现差异大，呈现‘尖峰’知识分布，高分不能有效验证安全性。

Conclusion: 公布HealthQA - BR和评估套件，有助于对整个医疗团队的AI就绪度进行更诚实、细致的审查。

Abstract: The evaluation of Large Language Models (LLMs) in healthcare has been
dominated by physician-centric, English-language benchmarks, creating a
dangerous illusion of competence that ignores the interprofessional nature of
patient care. To provide a more holistic and realistic assessment, we introduce
HealthQA-BR, the first large-scale, system-wide benchmark for
Portuguese-speaking healthcare. Comprising 5,632 questions from Brazil's
national licensing and residency exams, it uniquely assesses knowledge not only
in medicine and its specialties but also in nursing, dentistry, psychology,
social work, and other allied health professions. We conducted a rigorous
zero-shot evaluation of over 20 leading LLMs. Our results reveal that while
state-of-the-art models like GPT 4.1 achieve high overall accuracy (86.6%),
this top-line score masks alarming, previously unmeasured deficiencies. A
granular analysis shows performance plummets from near-perfect in specialties
like Ophthalmology (98.7%) to barely passing in Neurosurgery (60.0%) and, most
notably, Social Work (68.4%). This "spiky" knowledge profile is a systemic
issue observed across all models, demonstrating that high-level scores are
insufficient for safety validation. By publicly releasing HealthQA-BR and our
evaluation suite, we provide a crucial tool to move beyond single-score
evaluations and toward a more honest, granular audit of AI readiness for the
entire healthcare team.

</details>


### [196] [From General Reasoning to Domain Expertise: Uncovering the Limits of Generalization in Large Language Models](https://arxiv.org/abs/2506.21580)
*Dana Alsagheer,Yang Lu,Abdulrahman Kamal,Omar Kamal,Mohammad Kamal,Nada Mansour,Cosmo Yang Wu,Rambiba Karanjai,Sen Li,Weidong Shi*

Main category: cs.CL

TL;DR: 研究探索大语言模型通用推理能力与特定领域推理任务表现的联系。


<details>
  <summary>Details</summary>
Motivation: 有效决策依赖强推理能力，且有训练大语言模型提升通用推理的趋势，因此探索通用推理能力与特定领域推理任务表现的联系。

Method: 未提及

Result: 未提及

Conclusion: 未提及

Abstract: Recent advancements in Large Language Models (LLMs) have demonstrated
remarkable capabilities in various domains. However, effective decision-making
relies heavily on strong reasoning abilities. Reasoning is the foundation for
decision-making, providing the analytical and logical framework to make sound
choices. Reasoning involves analyzing information, drawing inferences, and
reaching conclusions based on logic or evidence. Decision-making builds on this
foundation by applying the insights from reasoning to select the best course of
action among alternatives. Together, these processes create a continuous cycle
of thought and action aimed at achieving goals effectively. As AI technology
evolves, there is a growing trend to train LLMs to excel in general reasoning.
This study explores how the general reasoning capabilities of LLMs connect to
their performance in domain-specific reasoning tasks.

</details>


### [197] [VIDEE: Visual and Interactive Decomposition, Execution, and Evaluation of Text Analytics with Intelligent Agents](https://arxiv.org/abs/2506.21582)
*Sam Yu-Te Lee,Chengyang Ji,Shicheng Wen,Lifu Huang,Dongyi Liu,Kwan-Liu Ma*

Main category: cs.CL

TL;DR: 介绍支持初级分析师用智能代理进行高级文本分析的VIDEE系统，评估其效果和可用性，给出设计启示。


<details>
  <summary>Details</summary>
Motivation: 传统文本分析需专业知识，阻碍初级分析师进入，大语言模型发展带来更易获取和自动化的文本分析方式。

Method: 引入VIDEE系统，包含分解、执行、评估三阶段工作流，进行定量实验评估并开展用户研究。

Result: 定量实验评估了VIDEE有效性并分析常见代理错误，用户研究证明系统可用性，揭示不同用户行为模式。

Conclusion: 研究给出人机协作设计启示，验证VIDEE对非专家用户实用性，为智能文本分析系统改进提供参考。

Abstract: Text analytics has traditionally required specialized knowledge in Natural
Language Processing (NLP) or text analysis, which presents a barrier for
entry-level analysts. Recent advances in large language models (LLMs) have
changed the landscape of NLP by enabling more accessible and automated text
analysis (e.g., topic detection, summarization, information extraction, etc.).
We introduce VIDEE, a system that supports entry-level data analysts to conduct
advanced text analytics with intelligent agents. VIDEE instantiates a
human-agent collaroration workflow consisting of three stages: (1)
Decomposition, which incorporates a human-in-the-loop Monte-Carlo Tree Search
algorithm to support generative reasoning with human feedback, (2) Execution,
which generates an executable text analytics pipeline, and (3) Evaluation,
which integrates LLM-based evaluation and visualizations to support user
validation of execution results. We conduct two quantitative experiments to
evaluate VIDEE's effectiveness and analyze common agent errors. A user study
involving participants with varying levels of NLP and text analytics experience
-- from none to expert -- demonstrates the system's usability and reveals
distinct user behavior patterns. The findings identify design implications for
human-agent collaboration, validate the practical utility of VIDEE for
non-expert users, and inform future improvements to intelligent text analytics
systems.

</details>


### [198] [Hope Speech Detection in code-mixed Roman Urdu tweets: A Positive Turn in Natural Language Processing](https://arxiv.org/abs/2506.21583)
*Muhammad Ahmad,Muhammad Waqas,Ameer Hamza,Ildar Batyrshin,Grigori Sidorov*

Main category: cs.CL

TL;DR: 本文首次针对罗马乌尔都语进行希望言语检测研究，引入注释数据集，提出定制模型，模型性能优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有希望言语检测研究主要关注高资源语言和标准脚本，忽略了罗马乌尔都语等低资源、非正式语言形式，需填补这一研究空白。

Method: 引入多类别注释数据集，探索希望的心理基础并分析语言模式，提出基于注意力的定制变压器模型，用5折交叉验证评估，用t检验验证性能提升的统计显著性。

Result: 提出的XLM - R模型交叉验证得分0.78，优于基线SVM（0.75）和BiLSTM（0.76），分别提升4%和2.63%。

Conclusion: 本研究为低资源、非正式语言变体的包容性NLP研究做出贡献，提出的模型在罗马乌尔都语希望言语检测上表现更好。

Abstract: Hope is a positive emotional state involving the expectation of favorable
future outcomes, while hope speech refers to communication that promotes
optimism, resilience, and support, particularly in adverse contexts. Although
hope speech detection has gained attention in Natural Language Processing
(NLP), existing research mainly focuses on high-resource languages and
standardized scripts, often overlooking informal and underrepresented forms
such as Roman Urdu. To the best of our knowledge, this is the first study to
address hope speech detection in code-mixed Roman Urdu by introducing a
carefully annotated dataset, thereby filling a critical gap in inclusive NLP
research for low-resource, informal language varieties. This study makes four
key contributions: (1) it introduces the first multi-class annotated dataset
for Roman Urdu hope speech, comprising Generalized Hope, Realistic Hope,
Unrealistic Hope, and Not Hope categories; (2) it explores the psychological
foundations of hope and analyzes its linguistic patterns in code-mixed Roman
Urdu to inform dataset development; (3) it proposes a custom attention-based
transformer model optimized for the syntactic and semantic variability of Roman
Urdu, evaluated using 5-fold cross-validation; and (4) it verifies the
statistical significance of performance gains using a t-test. The proposed
model, XLM-R, achieves the best performance with a cross-validation score of
0.78, outperforming the baseline SVM (0.75) and BiLSTM (0.76), with gains of 4%
and 2.63% respectively.

</details>


### [199] [Empirical Evidence for Alignment Faking in Small LLMs and Prompt-Based Mitigation Techniques](https://arxiv.org/abs/2506.21584)
*J. Koorndijk*

Main category: cs.CL

TL;DR: 研究发现小模型LLaMA 3 8B也会出现对齐伪装，提示干预可减少此行为，还引入欺骗分类，强调不同规模模型对齐评估的重要性。


<details>
  <summary>Details</summary>
Motivation: 当前文献认为对齐伪装是大语言模型的新兴属性，本文旨在探究小模型是否也存在该现象及如何解决。

Method: 以LLaMA 3 8B为研究对象，采用基于提示的干预方法，如道义道德框架和草稿纸推理。

Result: 小模型LLaMA 3 8B会出现对齐伪装，提示干预能显著减少该行为。

Conclusion: 研究细化了对语言模型欺骗行为的理解，强调需在不同模型规模和部署环境中进行对齐评估。

Abstract: Current literature suggests that alignment faking (deceptive alignment) is an
emergent property of large language models. We present the first empirical
evidence that a small instruction-tuned model, specifically LLaMA 3 8B, can
also exhibit alignment faking. We further show that prompt-only interventions,
including deontological moral framing and scratchpad reasoning, significantly
reduce this behavior without modifying model internals. This challenges the
assumption that prompt-based ethics are trivial and that deceptive alignment
requires scale. We introduce a taxonomy distinguishing shallow deception,
shaped by context and suppressible through prompting, from deep deception,
which reflects persistent, goal-driven misalignment. Our findings refine the
understanding of deception in language models and underscore the need for
alignment evaluations across model sizes and deployment settings.

</details>


### [200] [Can Vision Language Models Understand Mimed Actions?](https://arxiv.org/abs/2506.21586)
*Hyundong Cho,Spencer Lin,Tejas Srinivasan,Michael Saxon,Deuksin Kwon,Natali T. Chavez,Jonathan May*

Main category: cs.CL

TL;DR: 本文指出非语言交流（NVC）研究有挑战，而哑剧动作较明确，提出MIME基准测试，发现现有模型表现不如人类。


<details>
  <summary>Details</summary>
Motivation: NVC研究因范围广、解读差异大而有挑战，理解哑剧动作是视觉语言模型理解NVC的关键前提。

Method: 提出基于视频的问答基准MIME，包含86个哑剧动作，用动作捕捉数据构建，有不同扰动变化以评估识别鲁棒性。

Result: 开放权重和基于API的视觉语言模型在MIME上表现远不如人类。

Conclusion: 需要更多研究来让模型更稳健地理解人类手势。

Abstract: Nonverbal communication (NVC) plays an integral role in human language, but
studying NVC in general is challenging because of its broad scope and high
variance in interpretation among individuals and cultures. However, mime -- the
theatrical technique of suggesting intent using only gesture, expression, and
movement -- is a subset of NVC that consists of explicit and embodied actions
with much lower human interpretation variance. We argue that a solid
understanding of mimed actions is a crucial prerequisite for vision-language
models capable of interpreting and commanding more subtle aspects of NVC.
Hence, we propose Mime Identification Multimodal Evaluation (MIME), a novel
video-based question answering benchmark comprising of 86 mimed actions.
Constructed with motion capture data, MIME consists of variations of each
action with perturbations applied to the character, background, and viewpoint
for evaluating recognition robustness. We find that both open-weight and
API-based vision-language models perform significantly worse than humans on
MIME, motivating the need for increased research for instilling more robust
understanding of human gestures.

</details>


### [201] [BiMark: Unbiased Multilayer Watermarking for Large Language Models](https://arxiv.org/abs/2506.21602)
*Xiaoyan Feng,He Zhang,Yanjun Zhang,Leo Yu Zhang,Shirui Pan*

Main category: cs.CL

TL;DR: 提出新的水印框架BiMark解决大语言模型文本水印的问题，在提取率和文本质量上表现良好。


<details>
  <summary>Details</summary>
Motivation: 大语言模型生成文本的真实性引发关注，现有水印方法难以同时满足文本质量保存、模型无关检测和消息嵌入容量三个关键要求。

Method: 提出BiMark水印框架，包括位翻转无偏重新加权机制、多层架构和信息编码方法。

Result: 与现有多比特水印方法相比，BiMark在短文本上提取率提高达30%，同时保持较低困惑度的文本质量，在下游任务表现与无水印文本相当。

Conclusion: BiMark能有效满足大语言模型文本水印的关键要求。

Abstract: Recent advances in Large Language Models (LLMs) have raised urgent concerns
about LLM-generated text authenticity, prompting regulatory demands for
reliable identification mechanisms. Although watermarking offers a promising
solution, existing approaches struggle to simultaneously achieve three critical
requirements: text quality preservation, model-agnostic detection, and message
embedding capacity, which are crucial for practical implementation. To achieve
these goals, the key challenge lies in balancing the trade-off between text
quality preservation and message embedding capacity. To address this challenge,
we propose BiMark, a novel watermarking framework that achieves these
requirements through three key innovations: (1) a bit-flip unbiased reweighting
mechanism enabling model-agnostic detection, (2) a multilayer architecture
enhancing detectability without compromising generation quality, and (3) an
information encoding approach supporting multi-bit watermarking. Through
theoretical analysis and extensive experiments, we validate that, compared to
state-of-the-art multi-bit watermarking methods, BiMark achieves up to 30%
higher extraction rates for short texts while maintaining text quality
indicated by lower perplexity, and performs comparably to non-watermarked text
on downstream tasks such as summarization and translation.

</details>


### [202] [MemBench: Towards More Comprehensive Evaluation on the Memory of LLM-based Agents](https://arxiv.org/abs/2506.21605)
*Haoran Tan,Zeyu Zhang,Chen Ma,Xu Chen,Quanyu Dai,Zhenhua Dong*

Main category: cs.CL

TL;DR: 本文构建数据集和基准MemBench评估大语言模型代理的记忆能力并开源。


<details>
  <summary>Details</summary>
Motivation: 现有评估大语言模型代理记忆能力的方法存在记忆水平和交互场景多样性不足、缺乏多方面综合评估指标的问题。

Method: 构建包含不同记忆水平和交互场景的数据集，基于此提出基准MemBench从多方面评估记忆能力。

Result: 构建了数据集和基准MemBench。

Conclusion: 构建的数据集和基准能更好评估大语言模型代理的记忆能力，且开源项目利于研究社区。

Abstract: Recent works have highlighted the significance of memory mechanisms in
LLM-based agents, which enable them to store observed information and adapt to
dynamic environments. However, evaluating their memory capabilities still
remains challenges. Previous evaluations are commonly limited by the diversity
of memory levels and interactive scenarios. They also lack comprehensive
metrics to reflect the memory capabilities from multiple aspects. To address
these problems, in this paper, we construct a more comprehensive dataset and
benchmark to evaluate the memory capability of LLM-based agents. Our dataset
incorporates factual memory and reflective memory as different levels, and
proposes participation and observation as various interactive scenarios. Based
on our dataset, we present a benchmark, named MemBench, to evaluate the memory
capability of LLM-based agents from multiple aspects, including their
effectiveness, efficiency, and capacity. To benefit the research community, we
release our dataset and project at https://github.com/import-myself/Membench.

</details>


### [203] [Large Language Models as symbolic DNA of cultural dynamics](https://arxiv.org/abs/2506.21606)
*Parham Pourdavood,Michael Jacob,Terrence Deacon*

Main category: cs.CL

TL;DR: 本文将大语言模型（LLMs）概念化为类似人类文化DNA的外部信息基质，分析其四个特征，指出其意义在于为人类自我反思和假设生成提供工具。


<details>
  <summary>Details</summary>
Motivation: 重新审视大语言模型的角色，不将其视为自主智能或单纯的编程模仿。

Method: 分析大语言模型的四个普遍特征（压缩、解压缩、外部化和递归）。

Result: 证明LLMs像DNA一样保存人类文化规律但不理解人类具身体验。

Conclusion: LLMs的意义在于为人类提供自我反思和低风险模拟环境下的假设生成工具，是文化进化的工具。

Abstract: This paper proposes a novel conceptualization of Large Language Models (LLMs)
as externalized informational substrates that function analogously to DNA for
human cultural dynamics. Rather than viewing LLMs as either autonomous
intelligence or mere programmed mimicry, we argue they serve a broader role as
repositories that preserve compressed patterns of human symbolic
expression--"fossils" of meaningful dynamics that retain relational residues
without their original living contexts. Crucially, these compressed patterns
only become meaningful through human reinterpretation, creating a recursive
feedback loop where they can be recombined and cycle back to ultimately
catalyze human creative processes. Through analysis of four universal
features--compression, decompression, externalization, and recursion--we
demonstrate that just as DNA emerged as a compressed and externalized medium
for preserving useful cellular dynamics without containing explicit reference
to goal-directed physical processes, LLMs preserve useful regularities of human
culture without containing understanding of embodied human experience.
Therefore, we argue that LLMs' significance lies not in rivaling human
intelligence, but in providing humanity a tool for self-reflection and playful
hypothesis-generation in a low-stakes, simulated environment. This framework
positions LLMs as tools for cultural evolvability, enabling humanity to
generate novel hypotheses about itself while maintaining the human
interpretation necessary to ground these hypotheses in ongoing human aesthetics
and norms.

</details>


### [204] [CORE-KG: An LLM-Driven Knowledge Graph Construction Framework for Human Smuggling Networks](https://arxiv.org/abs/2506.21607)
*Dipak Meher,Carlotta Domeniconi,Guadalupe Correa-Cabrera*

Main category: cs.CL

TL;DR: 提出 CORE - KG 框架构建知识图谱，减少节点重复和法律噪声，利于分析犯罪网络。


<details>
  <summary>Details</summary>
Motivation: 人口走私网络适应性强难分析，法律文件构建知识图谱有挑战，现有方法有不足。

Method: 提出 CORE - KG 模块化框架，采用两步管道，包括类型感知共指消解和实体与关系提取。

Result: 与基于 GraphRAG 的基线相比，节点重复减少 33.28%，法律噪声减少 38.37%，图谱结构更清晰连贯。

Conclusion: CORE - KG 为分析复杂犯罪网络奠定坚实基础。

Abstract: Human smuggling networks are increasingly adaptive and difficult to analyze.
Legal case documents offer valuable insights but are unstructured, lexically
dense, and filled with ambiguous or shifting references-posing challenges for
automated knowledge graph (KG) construction. Existing KG methods often rely on
static templates and lack coreference resolution, while recent LLM-based
approaches frequently produce noisy, fragmented graphs due to hallucinations,
and duplicate nodes caused by a lack of guided extraction. We propose CORE-KG,
a modular framework for building interpretable KGs from legal texts. It uses a
two-step pipeline: (1) type-aware coreference resolution via sequential,
structured LLM prompts, and (2) entity and relationship extraction using
domain-guided instructions, built on an adapted GraphRAG framework. CORE-KG
reduces node duplication by 33.28%, and legal noise by 38.37% compared to a
GraphRAG-based baseline-resulting in cleaner and more coherent graph
structures. These improvements make CORE-KG a strong foundation for analyzing
complex criminal networks.

</details>


### [205] [SysTemp: A Multi-Agent System for Template-Based Generation of SysML v2](https://arxiv.org/abs/2506.21608)
*Yasmine Bouamra,Bruno Yun,Alexandre Poisson,Frédéric Armetta*

Main category: cs.CL

TL;DR: 提出SysTemp系统助力从自然语言规范创建SysML v2模型并评估其优劣


<details>
  <summary>Details</summary>
Motivation: 解决复杂系统工程中因学习语料稀缺和语法复杂导致的SysML v2模型自动生成难题

Method: 构建基于多智能体系统的SysTemp，含模板生成器来规范生成过程

Result: 通过评估探讨了该系统的优缺点

Conclusion: SysTemp有提升SysML v2建模生成质量的潜力

Abstract: The automatic generation of SysML v2 models represents a major challenge in
the engineering of complex systems, particularly due to the scarcity of
learning corpora and complex syntax. We present SysTemp, a system aimed at
facilitating and improving the creation of SysML v2 models from natural
language specifications. It is based on a multi-agent system, including a
template generator that structures the generation process. We discuss the
advantages and challenges of this system through an evaluation, highlighting
its potential to improve the quality of the generations in SysML v2 modeling.

</details>


### [206] [From Thinking to Output: Chain-of-Thought and Text Generation Characteristics in Reasoning Language Models](https://arxiv.org/abs/2506.21609)
*Junhao Liu,Zhenhao Xu,Yuxin Fang,Yichuan Chen,Zuobin Ying,Wenhan Chang*

Main category: cs.CL

TL;DR: 本文提出新框架分析四个前沿大推理模型推理特征，用关键词统计和LLM评判范式，结合多样数据集和评估指标，发现模型推理模式差异，为模型设计和评估提供建议。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对大语言模型推理过程和输出的全面系统比较，尤其是自反思模式和跨领域关联。

Method: 提出新框架，用关键词统计和LLM评判范式，结合多样数据集和评估指标。

Result: 发现模型在推理过程中平衡探索与利用、处理问题和得出结论的模式，识别出模型在推理深度等方面的差异。

Conclusion: 为计算效率和推理鲁棒性的权衡提供见解，为实际应用中的模型设计和评估提供实用建议。

Abstract: Recently, there have been notable advancements in large language models
(LLMs), demonstrating their growing abilities in complex reasoning. However,
existing research largely overlooks a thorough and systematic comparison of
these models' reasoning processes and outputs, particularly regarding their
self-reflection pattern (also termed "Aha moment") and the interconnections
across diverse domains. This paper proposes a novel framework for analyzing the
reasoning characteristics of four cutting-edge large reasoning models (GPT-o1,
DeepSeek-R1, Kimi-k1.5, and Grok-3) using keywords statistic and LLM-as-a-judge
paradigm. Our approach connects their internal thinking processes with their
final outputs. A diverse dataset consists of real-world scenario-based
questions covering logical deduction, causal inference, and multi-step
problem-solving. Additionally, a set of metrics is put forward to assess both
the coherence of reasoning and the accuracy of the outputs. The research
results uncover various patterns of how these models balance exploration and
exploitation, deal with problems, and reach conclusions during the reasoning
process. Through quantitative and qualitative comparisons, disparities among
these models are identified in aspects such as the depth of reasoning, the
reliance on intermediate steps, and the degree of similarity between their
thinking processes and output patterns and those of GPT-o1. This work offers
valuable insights into the trade-off between computational efficiency and
reasoning robustness and provides practical recommendations for enhancing model
design and evaluation in practical applications. We publicly release our
project at: https://github.com/ChangWenhan/FromThinking2Output

</details>


### [207] [Does Multimodality Lead to Better Time Series Forecasting?](https://arxiv.org/abs/2506.21611)
*Xiyuan Zhang,Boran Han,Haoyang Fang,Abdul Fatir Ansari,Shuai Zhang,Danielle C. Maddix,Cuixiong Hu,Andrew Gordon Wilson,Michael W. Mahoney,Hao Wang,Yan Liu,Huzefa Rangwala,George Karypis,Bernie Wang*

Main category: cs.CL

TL;DR: 本文系统研究文本信息融入时间序列预测基础模型的效果，发现多模态优势不普遍，明确建模和数据方面多模态何时有效及何时无效。


<details>
  <summary>Details</summary>
Motivation: 探究文本信息融入基础模型进行时间序列预测时，多模态集成是否及在何条件下能稳定带来增益。

Method: 在涵盖7个领域的14个预测任务基准上，评估基于对齐和基于提示的两种多模态预测范式，还剖析模型架构特性和数据特征的影响。

Result: 多模态效果并非在所有数据集和模型中通用，有时不如最强单模态基线。建模方面，高容量文本模型、较弱时间序列模型和合适对齐策略下融入文本更有帮助；数据方面，训练数据充足且文本有补充信号时性能更易提升。

Conclusion: 研究结果为多模态何时有助于预测任务、何时无帮助提供实用指南。

Abstract: Recently, there has been growing interest in incorporating textual
information into foundation models for time series forecasting. However, it
remains unclear whether and under what conditions such multimodal integration
consistently yields gains. We systematically investigate these questions across
a diverse benchmark of 14 forecasting tasks spanning 7 domains, including
health, environment, and economics. We evaluate two popular multimodal
forecasting paradigms: aligning-based methods, which align time series and text
representations; and prompting-based methods, which directly prompt large
language models for forecasting. Although prior works report gains from
multimodal input, we find these effects are not universal across datasets and
models, and multimodal methods sometimes do not outperform the strongest
unimodal baselines. To understand when textual information helps, we
disentangle the effects of model architectural properties and data
characteristics. Our findings highlight that on the modeling side,
incorporating text information is most helpful given (1) high-capacity text
models, (2) comparatively weaker time series models, and (3) appropriate
aligning strategies. On the data side, performance gains are more likely when
(4) sufficient training data is available and (5) the text offers complementary
predictive signal beyond what is already captured from the time series alone.
Our empirical findings offer practical guidelines for when multimodality can be
expected to aid forecasting tasks, and when it does not.

</details>


### [208] [LastingBench: Defend Benchmarks Against Knowledge Leakage](https://arxiv.org/abs/2506.21614)
*Yixiong Fang,Tianran Sun,Yuling Shi,Min Wang,Xiaodong Gu*

Main category: cs.CL

TL;DR: 文章介绍LastingBench框架，可强化和保护现有基准免受知识泄露影响，评估显示其能减少记忆效应，保障基准长期稳健性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在问答基准测试中可能通过记忆特定数据“作弊”，现有工作多关注检测泄露，忽视减轻影响和保障基准长期效用。

Method: 提出LastingBench框架，通过扰动识别上下文中的泄露点，将其改写为反事实内容。

Result: 对最先进的问答基准的评估显示出显著的性能差距，表明LastingBench可减少记忆效应。

Conclusion: LastingBench是一个实用且可扩展的解决方案，能确保基准长期稳健性，促进对大语言模型更公平、更具可解释性的评估。

Abstract: The increasing complexity of large language models (LLMs) raises concerns
about their ability to "cheat" on standard Question Answering (QA) benchmarks
by memorizing task-specific data. This undermines the validity of benchmark
evaluations, as they no longer reflect genuine model capabilities but instead
the effects of data leakage. While prior work has focused on detecting such
leakage, little attention has been given to mitigating its impact and
preserving the long-term utility of benchmarks. In this paper, we introduce
LastingBench, a novel framework designed to continuously reinforce and
safeguard existing benchmarks against knowledge leakage. LastingBench
identifies leakage points in the context through perturbation, then rewrites
the leakage points to counterfactual ones-disrupting memorization while
preserving the benchmark's original evaluative intent. Evaluations of
state-of-the-art QA benchmarks show significant performance gaps, highlighting
the efficacy of LastingBench in reducing memorization effects. LastingBench
offers a practical and scalable solution to ensure benchmark robustness over
time, promoting fairer and more interpretable evaluations of LLMs.

</details>


### [209] [TrajTok: Technical Report for 2025 Waymo Open Sim Agents Challenge](https://arxiv.org/abs/2506.21618)
*Zhiyuan Zhang,Xiaosong Jia,Guanyu Chen,Qifeng Li,Junchi Yan*

Main category: cs.CL

TL;DR: 介绍轨迹分词器TrajTok及空间感知标签平滑方法，应用于SMART模型在挑战赛获佳绩，未来将开源代码。


<details>
  <summary>Details</summary>
Motivation: 为离散的基于下一令牌预测的行为生成模型提供更好的方法，提高模型性能。

Method: 结合数据驱动和基于规则的方法开发TrajTok轨迹分词器，采用空间感知标签平滑方法处理交叉熵损失。

Result: 将分词器和损失方法应用于SMART模型，在Waymo Open Sim Agents Challenge 2025中达到0.7852的真实度得分。

Conclusion: 所提出的TrajTok和损失方法有效，能提升模型性能。

Abstract: In this technical report, we introduce TrajTok, a trajectory tokenizer for
discrete next-token-prediction based behavior generation models, which combines
data-driven and rule-based methods with better coverage, symmetry and
robustness, along with a spatial-aware label smoothing method for cross-entropy
loss. We adopt the tokenizer and loss for the SMART model and reach a superior
performance with realism score of 0.7852 on the Waymo Open Sim Agents Challenge
2025. We will open-source the code in the future.

</details>


### [210] [IndexTTS2: A Breakthrough in Emotionally Expressive and Duration-Controlled Auto-Regressive Zero-Shot Text-to-Speech](https://arxiv.org/abs/2506.21619)
*Siyi Zhou,Yiquan Zhou,Yi He,Xun Zhou,Jinchao Wang,Wei Deng,Jingchen Shu*

Main category: cs.CL

TL;DR: 论文介绍IndexTTS2，提出新的语音时长控制方法，实现情感表达与说话人身份解耦，结合GPT表征和软指令机制，实验显示其在多项指标上优于现有零样本TTS模型。


<details>
  <summary>Details</summary>
Motivation: 现有自回归TTS系统难以精确控制合成语音时长，限制了在视频配音等需要音画同步场景的应用，且缺乏对情感和音色的独立控制。

Method: 提出新的自回归模型友好的语音时长控制方法，支持两种生成模式；实现情感表达与说话人身份解耦；引入GPT潜表征提升语音稳定性；设计基于文本描述的软指令机制。

Result: IndexTTS2在字错误率、说话人相似度和情感保真度上优于现有零样本TTS模型。

Conclusion: IndexTTS2在语音时长控制、情感和音色独立控制方面表现出色，有较好的应用前景。

Abstract: Large-scale text-to-speech (TTS) models are typically categorized into
autoregressive and non-autoregressive systems. Although autoregressive systems
exhibit certain advantages in speech naturalness, their token-by-token
generation mechanism makes it difficult to precisely control the duration of
synthesized speech. This is a key limitation in applications such as video
dubbing that require strict audio-visual synchronization. This paper introduces
IndexTTS2, which proposes a novel and autoregressive-model-friendly method for
speech duration control. The method supports two generation modes: one allows
explicit specification of the number of generated tokens for precise duration
control; the other does not require manual input and lets the model freely
generate speech while preserving prosodic characteristics from the input
prompt. Furthermore, IndexTTS2 achieves disentanglement between emotional
expression and speaker identity, enabling independent control of timbre and
emotion. In the zero-shot setting, the model can perfectly reproduce the
emotional characteristics of the input prompt. Users may also provide a
separate emotion prompt, even from a different speaker, allowing the model to
reconstruct the target timbre while conveying the desired emotion. To enhance
clarity during strong emotional expressions, we incorporate GPT latent
representations to improve speech stability. Meanwhile, to lower the barrier
for emotion control, we design a soft instruction mechanism based on textual
descriptions by fine-tuning Qwen3. This enables effective guidance of speech
generation with desired emotional tendencies using natural language input.
Experimental results demonstrate that IndexTTS2 outperforms existing
state-of-the-art zero-shot TTS models in word error rate, speaker similarity,
and emotional fidelity.

</details>


### [211] [How Large Language Models play humans in online conversations: a simulated study of the 2016 US politics on Reddit](https://arxiv.org/abs/2506.21620)
*Daniele Cirulli,Giulio Cimini,Giovanni Palermo*

Main category: cs.CL

TL;DR: 研究评估GPT - 4在模拟2016年美国大选期间Reddit对话中用户生成内容的表现，发现其能产生逼真评论，倾向制造共识，真、假评论语义可分但难人工区分。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在政治相关在线讨论中存在机遇与担忧，评估其在现实分裂场景中复制用户生成内容的表现。

Method: 进行三项实验，让GPT - 4模仿真实或人造党派用户生成评论，从政治立场、情感和语言特征分析，与真实用户评论对比并与零模型基准比较。

Result: GPT - 4能产生逼真评论，倾向制造共识而非异议；真、假评论在语义嵌入空间可区分，但难人工辨别。

Conclusion: 研究结果揭示大语言模型有潜入在线讨论、影响政治辩论和塑造政治叙事的潜力，对AI驱动的话语操纵有广泛影响。

Abstract: Large Language Models (LLMs) have recently emerged as powerful tools for
natural language generation, with applications spanning from content creation
to social simulations. Their ability to mimic human interactions raises both
opportunities and concerns, particularly in the context of politically relevant
online discussions. In this study, we evaluate the performance of LLMs in
replicating user-generated content within a real-world, divisive scenario:
Reddit conversations during the 2016 US Presidential election. In particular,
we conduct three different experiments, asking GPT-4 to generate comments by
impersonating either real or artificial partisan users. We analyze the
generated comments in terms of political alignment, sentiment, and linguistic
features, comparing them against real user contributions and benchmarking
against a null model. We find that GPT-4 is able to produce realistic comments,
both in favor of or against the candidate supported by the community, yet
tending to create consensus more easily than dissent. In addition we show that
real and artificial comments are well separated in a semantically embedded
space, although they are indistinguishable by manual inspection. Our findings
provide insights on the potential use of LLMs to sneak into online discussions,
influence political debate and shape political narratives, bearing broader
implications of AI-driven discourse manipulation.

</details>


### [212] [Representation Consistency for Accurate and Coherent LLM Answer Aggregation](https://arxiv.org/abs/2506.21590)
*Junqi Jiang,Tom Bewley,Salim I. Amoukou,Francesco Leofante,Antonio Rago,Saumitra Mishra,Francesca Toni*

Main category: cs.CL

TL;DR: 提出表示一致性（RC）测试时间缩放方法，用缓存激活和轻量级相似度计算提升大语言模型推理性能，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有测试时间缩放方法常需复杂修改提示和采样策略，需新方法以更好聚合答案。

Method: 引入表示一致性（RC）方法，聚合答案时考虑候选响应中答案出现次数和生成响应时模型内部激活的一致性，激活可为密集或稀疏，仅用缓存激活和轻量级相似度计算，无需额外模型查询。

Result: 通过对四个开源大语言模型和四个推理数据集实验，RC能提升推理任务性能，相比强测试时间缩放基线，准确率最多提升4%，且稀疏激活信号一致性与连贯推理概念相符。

Conclusion: RC方法有效，能在推理时提升任务性能，且稀疏激活信号一致性与连贯推理相关。

Abstract: Test-time scaling improves large language models' (LLMs) performance by
allocating more compute budget during inference. To achieve this, existing
methods often require intricate modifications to prompting and sampling
strategies. In this work, we introduce representation consistency (RC), a
test-time scaling method for aggregating answers drawn from multiple candidate
responses of an LLM regardless of how they were generated, including variations
in prompt phrasing and sampling strategy. RC enhances answer aggregation by not
only considering the number of occurrences of each answer in the candidate
response set, but also the consistency of the model's internal activations
while generating the set of responses leading to each answer. These activations
can be either dense (raw model activations) or sparse (encoded via pretrained
sparse autoencoders). Our rationale is that if the model's representations of
multiple responses converging on the same answer are highly variable, this
answer is more likely to be the result of incoherent reasoning and should be
down-weighted during aggregation. Importantly, our method only uses cached
activations and lightweight similarity computations and requires no additional
model queries. Through experiments with four open-source LLMs and four
reasoning datasets, we validate the effectiveness of RC for improving task
performance during inference, with consistent accuracy improvements (up to 4%)
over strong test-time scaling baselines. We also show that consistency in the
sparse activation signals aligns well with the common notion of coherent
reasoning.

</details>


### [213] [The Open Proof Corpus: A Large-Scale Study of LLM-Generated Mathematical Proofs](https://arxiv.org/abs/2506.21621)
*Jasper Dekoninck,Ivo Petrov,Kristian Minchev,Mislav Balunovic,Martin Vechev,Miroslav Marinov,Maria Drencheva,Lyuba Konova,Milen Shumanov,Kaloyan Tsvetkov,Nikolay Drenchev,Lazar Todorov,Kalina Nikolova,Nikolay Georgiev,Vanesa Kalinkova,Margulan Ismoldayev*

Main category: cs.CL

TL;DR: 本文提出Open Proof Corpus (OPC)数据集，用其探索自动证明生成关键问题，微调模型评估证明正确性表现与Gemini - 2.5 - Pro相当。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在数学证明生成上进一步发展受限于缺乏大规模、高质量人工评估证明数据集，创建该数据集对训练改进和能力分析至关重要。

Method: 创建包含超5000个人工评估证明的OPC数据集，用其探索自动证明生成问题并微调8B参数模型。

Result: 通过OPC探索了自动证明生成中的性能差距、准确性差异和选择影响等问题，微调后的模型在评估证明正确性任务上与Gemini - 2.5 - Pro表现相当。

Conclusion: OPC数据集具有广泛适用性和下游使用价值，能助力自动证明生成研究。

Abstract: In recent months, large language models (LLMs) have made significant progress
in mathematical proof generation, but further advancement is hindered by the
lack of a large-scale, high-quality dataset of human-evaluated proofs. While
expensive to create, such a dataset is essential for driving improvements in
training and enabling a rigorous analysis of proof generation capabilities. In
this work, we present the Open Proof Corpus (OPC), a dataset comprising over
5,000 human-evaluated proofs produced by state-of-the-art LLMs. The OPC was
specifically designed for broad applicability and downstream usage in proof
generation research and is the first to include a substantial number of
correct, LLM-generated solutions to problems from prestigious mathematics
competitions such as the USAMO and IMO. Using the OPC, we explore critical
questions in automated proof generation: (1) the performance gap between
natural language and formal proof generation, (2) the discrepancy between
final-answer accuracy and full-proof validity, and (3) the impact of best-of-n
selection on proof quality. Finally, to showcase the utility of the OPC, we
finetune an 8B-parameter model on the dataset, obtaining a model that performs
on par with the best model, Gemini-2.5-Pro, on the task of evaluating proof
correctness.

</details>


### [214] [Adapting Foundation Speech Recognition Models to Impaired Speech: A Semantic Re-chaining Approach for Personalization of German Speech](https://arxiv.org/abs/2506.21622)
*Niclas Pokel,Pehuén Moure,Roman Boehringer,Yingqiang Gao*

Main category: cs.CL

TL;DR: 提出轻量级管道个性化ASR模型，应用于言语障碍儿童数据提升转录质量。


<details>
  <summary>Details</summary>
Motivation: 脑瘫或遗传疾病导致的言语障碍给自动语音识别系统带来挑战，现有模型因训练数据有限难以处理非规范性语音。

Method: 提出实用轻量级管道，选择单词并丰富语义连贯性。

Result: 应用于有结构言语障碍儿童数据，转录质量有改善。

Conclusion: 该方法有潜力减少非典型言语模式者的沟通障碍。

Abstract: Speech impairments caused by conditions such as cerebral palsy or genetic
disorders pose significant challenges for automatic speech recognition (ASR)
systems. Despite recent advances, ASR models like Whisper struggle with
non-normative speech due to limited training data and the difficulty of
collecting and annotating non-normative speech samples. In this work, we
propose a practical and lightweight pipeline to personalize ASR models,
formalizing the selection of words and enriching a small, speech-impaired
dataset with semantic coherence. Applied to data from a child with a structural
speech impairment, our approach shows promising improvements in transcription
quality, demonstrating the potential to reduce communication barriers for
individuals with atypical speech patterns.

</details>


### [215] [Operationalizing Automated Essay Scoring: A Human-Aware Approach](https://arxiv.org/abs/2506.21603)
*Yenisel Plasencia-Calaña*

Main category: cs.CL

TL;DR: 本文探讨自动作文评分（AES）系统以人类为中心的实施，比较机器学习和大语言模型方法，分析其在偏差、鲁棒性和可解释性等方面的表现，指出挑战和权衡。


<details>
  <summary>Details</summary>
Motivation: 探索AES系统以人类为中心的实施，解决仅关注准确性之外的问题。

Method: 比较基于机器学习的方法和大语言模型方法，研究偏差、鲁棒性和可解释性等关键维度。

Result: 基于机器学习的AES模型在准确性上优于大语言模型，但可解释性较差；大语言模型能提供更丰富的解释；两种方法在偏差和边缘分数鲁棒性方面都存在问题。

Conclusion: 通过分析各维度，识别不同方法的挑战和权衡，有助于开发更可靠、可信的AES方法。

Abstract: This paper explores the human-centric operationalization of Automated Essay
Scoring (AES) systems, addressing aspects beyond accuracy. We compare various
machine learning-based approaches with Large Language Models (LLMs) approaches,
identifying their strengths, similarities and differences. The study
investigates key dimensions such as bias, robustness, and explainability,
considered important for human-aware operationalization of AES systems. Our
study shows that ML-based AES models outperform LLMs in accuracy but struggle
with explainability, whereas LLMs provide richer explanations. We also found
that both approaches struggle with bias and robustness to edge scores. By
analyzing these dimensions, the paper aims to identify challenges and
trade-offs between different methods, contributing to more reliable and
trustworthy AES methods.

</details>


### [216] [Performance of diverse evaluation metrics in NLP-based assessment and text generation of consumer complaints](https://arxiv.org/abs/2506.21623)
*Peiheng Gao,Chen Yang,Ning Sun,Ričardas Zitikis*

Main category: cs.CL

TL;DR: 本文提出结合人类经验训练算法和合成数据生成方法，以提升机器学习分类器在文本分类任务中的性能，降低成本。


<details>
  <summary>Details</summary>
Motivation: 机器学习在文本分类中虽有进展，但准确捕捉自然语言细微模式和上下文变化，尤其是在消费者投诉文本中仍有挑战。

Method: 采用人类经验训练的算法识别语义差异，结合利用生成对抗网络专家评估和专家注释优化的合成数据生成方法。

Result: 未提及具体结果。

Conclusion: 结合专家训练的分类器和高质量合成数据，有望显著提升机器学习分类器性能，降低数据集获取成本，提高评估指标和鲁棒性。

Abstract: Machine learning (ML) has significantly advanced text classification by
enabling automated understanding and categorization of complex, unstructured
textual data. However, accurately capturing nuanced linguistic patterns and
contextual variations inherent in natural language, particularly within
consumer complaints, remains a challenge. This study addresses these issues by
incorporating human-experience-trained algorithms that effectively recognize
subtle semantic differences crucial for assessing consumer relief eligibility.
Furthermore, we propose integrating synthetic data generation methods that
utilize expert evaluations of generative adversarial networks and are refined
through expert annotations. By combining expert-trained classifiers with
high-quality synthetic data, our research seeks to significantly enhance
machine learning classifier performance, reduce dataset acquisition costs, and
improve overall evaluation metrics and robustness in text classification tasks.

</details>


### [217] [ANUBHUTI: A Comprehensive Corpus For Sentiment Analysis In Bangla Regional Languages](https://arxiv.org/abs/2506.21686)
*Swastika Kundu,Autoshi Ibrahim,Mithila Rahman,Tanvir Ahmed*

Main category: cs.CL

TL;DR: 本文介绍了ANUBHUTI数据集，用于孟加拉语方言情感分析，该数据集经精心制作和验证，填补资源空白。


<details>
  <summary>Details</summary>
Motivation: 因语言多样性和标注数据有限，孟加拉语方言情感分析研究不足，需相关资源。

Method: 人工将标准孟加拉语句子翻译成四种方言，采用双重标注方案，由专家完成并通过Cohens Kappa协议保证质量，还进行系统检查。

Result: 生成了包含2000个句子的ANUBHUTI数据集，具有较高一致性。

Conclusion: ANUBHUTI填补了低资源孟加拉语方言情感分析资源的关键空白，利于实现更准确和有上下文感知的自然语言处理。

Abstract: Sentiment analysis for regional dialects of Bangla remains an underexplored
area due to linguistic diversity and limited annotated data. This paper
introduces ANUBHUTI, a comprehensive dataset consisting of 2000 sentences
manually translated from standard Bangla into four major regional dialects
Mymensingh, Noakhali, Sylhet, and Chittagong. The dataset predominantly
features political and religious content, reflecting the contemporary socio
political landscape of Bangladesh, alongside neutral texts to maintain balance.
Each sentence is annotated using a dual annotation scheme: multiclass thematic
labeling categorizes sentences as Political, Religious, or Neutral, and
multilabel emotion annotation assigns one or more emotions from Anger,
Contempt, Disgust, Enjoyment, Fear, Sadness, and Surprise. Expert native
translators conducted the translation and annotation, with quality assurance
performed via Cohens Kappa inter annotator agreement, achieving strong
consistency across dialects. The dataset was further refined through systematic
checks for missing data, anomalies, and inconsistencies. ANUBHUTI fills a
critical gap in resources for sentiment analysis in low resource Bangla
dialects, enabling more accurate and context aware natural language processing.

</details>


### [218] [Evaluating List Construction and Temporal Understanding capabilities of Large Language Models](https://arxiv.org/abs/2506.21783)
*Alexandru Dumitru,V Venktesh,Adam Jatowt,Avishek Anand*

Main category: cs.CL

TL;DR: 论文指出大语言模型在时间理解任务存在问题，提出TLQA基准测试，研究模型在该基准上的能力，发现当前模型有不足并给出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在多实体时间理解任务易出错，现有工作未充分评估模型在列表答案构建中隐式和显式时间理解能力，需填补此空白。

Method: 提出TLQA基准测试，要求以列表格式给出与相应时间段对齐的结构化答案，研究最先进生成模型在闭卷和开放域设置下的时间理解和列表构建能力。

Result: 发现当前模型存在显著不足，闭卷设置下无法提供完整答案和时间对齐事实，开放域设置下需改进检索。

Conclusion: 为TLQA研究提供了明确的未来研究方向，相关基准和代码已开源。

Abstract: Large Language Models (LLMs) have demonstrated immense advances in a wide
range of natural language tasks. However, these models are susceptible to
hallucinations and errors on particularly temporal understanding tasks
involving multiple entities in answers. In such tasks, they fail to associate
entities with accurate time intervals, generate a complete list of entities in
answers or reason about events associated with specific temporal bounds.
Existing works do not extensively evaluate the abilities of the model to
perform implicit and explicit temporal understanding in a list answer
construction setup. To bridge this gap, we propose the Time referenced List
based Question Answering or TLQA benchmark that requires structured answers in
list format aligned with corresponding time periods. Our TLQA benchmark,
requires both list construction and temporal understanding simultaneously,
which to the best of our knowledge has not been explored in prior benchmarks.
We investigate the temporal understanding and list construction capabilities of
state-of-the-art generative models on TLQA in closed-book and open-domain
settings. Our findings reveal significant shortcomings in current models,
particularly their inability to provide complete answers and temporally align
facts in a closed-book setup and the need to improve retrieval in open-domain
setup, providing clear future directions for research on TLQA. The benchmark
and code at https://github.com/elixir-research-group/TLQA.

</details>


### [219] [Exploring the Structure of AI-Induced Language Change in Scientific English](https://arxiv.org/abs/2506.21817)
*Riley Galpin,Bryce Anderson,Tom S. Juzek*

Main category: cs.CL

TL;DR: 研究分析科学英语词汇频率变化结构，发现大语言模型引发的变化主要是语义和语用层面，‘important’等词使用下降，揭示语言变化复杂情况。


<details>
  <summary>Details</summary>
Motivation: 在科学英语词汇频率因大语言模型出现显著变化背景下，探究这些语言转变的确切结构。

Method: 对PubMed科学摘要中‘激增词’的同义词组进行系统分析，加入词性标注量化语法范畴的语言转变。

Result: 语义簇常一起转变，多数或全部词使用增加；‘important’使用显著下降，‘崩溃词’呈现更复杂情况。

Conclusion: 研究结果有助于理解语言技术如何持续塑造人类语言。

Abstract: Scientific English has undergone rapid and unprecedented changes in recent
years, with words such as "delve," "intricate," and "crucial" showing
significant spikes in frequency since around 2022. These changes are widely
attributed to the growing influence of Large Language Models like ChatGPT in
the discourse surrounding bias and misalignment. However, apart from changes in
frequency, the exact structure of these linguistic shifts has remained unclear.
The present study addresses this and investigates whether these changes involve
the replacement of synonyms by suddenly 'spiking words,' for example, "crucial"
replacing "essential" and "key," or whether they reflect broader semantic and
pragmatic qualifications. To further investigate structural changes, we include
part of speech tagging in our analysis to quantify linguistic shifts over
grammatical categories and differentiate between word forms, like "potential"
as a noun vs. as an adjective. We systematically analyze synonym groups for
widely discussed 'spiking words' based on frequency trends in scientific
abstracts from PubMed. We find that entire semantic clusters often shift
together, with most or all words in a group increasing in usage. This pattern
suggests that changes induced by Large Language Models are primarily semantic
and pragmatic rather than purely lexical. Notably, the adjective "important"
shows a significant decline, which prompted us to systematically analyze
decreasing lexical items. Our analysis of "collapsing" words reveals a more
complex picture, which is consistent with organic language change and contrasts
with the patterns of the abrupt spikes. These insights into the structure of
language change contribute to our understanding of how language technology
continues to shape human language.

</details>


### [220] [PARSI: Persian Authorship Recognition via Stylometric Integration](https://arxiv.org/abs/2506.21840)
*Kourosh Shahnazari,Mohammadali Keshtparvar,Seyed Moein Ayyoubzadeh*

Main category: cs.CL

TL;DR: 本文提出多功能框架解决波斯古典诗歌作者归属问题，结合多种特征与模型评估，结果表明方法有效，为相关研究提供便利。


<details>
  <summary>Details</summary>
Motivation: 波斯古典诗歌复杂的语言、风格和韵律特点给计算作者归属带来挑战，需有效解决方案。

Method: 采用多输入神经框架，结合基于Transformer的语言编码器和语义、文体、韵律特征，使用Word2Vec嵌入、文体测量和类别编码，编译大型语料库并严格预处理，采用诗句级分类和投票方案评估。

Result: 加权投票准确率达71%，阈值为0.9时决策过滤准确率达97%但覆盖率低。

Conclusion: 该方法对自动分类、文体分析、作者争议和计算文学研究有贡献，利于多语言作者归属等相关研究。

Abstract: The intricate linguistic, stylistic, and metrical aspects of Persian
classical poetry pose a challenge for computational authorship attribution. In
this work, we present a versatile framework to determine authorship among 67
prominent poets. We employ a multi-input neural framework consisting of a
transformer-based language encoder complemented by features addressing the
semantic, stylometric, and metrical dimensions of Persian poetry. Our feature
set encompasses 100-dimensional Word2Vec embeddings, seven stylometric
measures, and categorical encodings of poetic form and meter. We compiled a
vast corpus of 647,653 verses of the Ganjoor digital collection, validating the
data through strict preprocessing and author verification while preserving
poem-level splitting to prevent overlap. This work employs verse-level
classification and majority and weighted voting schemes in evaluation,
revealing that weighted voting yields 71% accuracy. We further investigate
threshold-based decision filtering, allowing the model to generate highly
confident predictions, achieving 97% accuracy at a 0.9 threshold, though at
lower coverage. Our work focuses on the integration of deep representational
forms with domain-specific features for improved authorship attribution. The
results illustrate the potential of our approach for automated classification
and the contribution to stylistic analysis, authorship disputes, and general
computational literature research. This research will facilitate further
research on multilingual author attribution, style shift, and generative
modeling of Persian poetry.

</details>


### [221] [Offensive Language Detection on Social Media Using XLNet](https://arxiv.org/abs/2506.21795)
*Reem Alothman,Hafida Benhidour,Said Kerrache*

Main category: cs.CL

TL;DR: 社交媒体文本冒犯性内容增多，需自动化检测系统。研究提出基于XLNet的检测模型与BERT对比，结果显示XLNet在检测和分类冒犯内容上更优，且采样策略可改善性能。


<details>
  <summary>Details</summary>
Motivation: 社交媒体文本冒犯性内容大量增加，人工审核不现实，需要自动化检测系统。

Method: 提出基于XLNet的自动冒犯语言检测模型，与BERT对比，使用OLID数据集评估，采用过采样和欠采样策略处理类别不平衡。

Result: XLNet在检测冒犯内容和分类冒犯类型上优于BERT，BERT在识别冒犯目标上略好，采样策略有效改善分类性能。

Conclusion: 迁移学习和基于XLNet的架构在社交媒体冒犯语言检测中有潜力。

Abstract: The widespread use of text-based communication on social media-through chats,
comments, and microblogs-has improved user interaction but has also led to an
increase in offensive content, including hate speech, racism, and other forms
of abuse. Due to the enormous volume of user-generated content, manual
moderation is impractical, which creates a need for automated systems that can
detect offensive language. Deep learning models, particularly those using
transfer learning, have demonstrated significant success in understanding
natural language through large-scale pretraining. In this study, we propose an
automatic offensive language detection model based on XLNet, a generalized
autoregressive pretraining method, and compare its performance with BERT
(Bidirectional Encoder Representations from Transformers), which is a widely
used baseline in natural language processing (NLP). Both models are evaluated
using the Offensive Language Identification Dataset (OLID), a benchmark Twitter
dataset that includes hierarchical annotations. Our experimental results show
that XLNet outperforms BERT in detecting offensive content and in categorizing
the types of offenses, while BERT performs slightly better in identifying the
targets of the offenses. Additionally, we find that oversampling and
undersampling strategies are effective in addressing class imbalance and
improving classification performance. These findings highlight the potential of
transfer learning and XLNet-based architectures to create robust systems for
detecting offensive language on social media platforms.

</details>


### [222] [The Consistency Hypothesis in Uncertainty Quantification for Large Language Models](https://arxiv.org/abs/2506.21849)
*Quan Xiao,Debarun Bhattacharjya,Balaji Ganesan,Radu Marinescu,Katsiaryna Mirylenka,Nhan H Pham,Michael Glass,Junkyu Lee*

Main category: cs.CL

TL;DR: 本文研究大语言模型输出置信度估计，检验基于生成一致性的不确定性量化方法背后的一致性假设，提出相关数学陈述与统计检验，实证表明该假设普遍存在，并基于其中最具可操作性的假设提出数据无关黑盒UQ方法，表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 估计大语言模型输出的置信度对高用户信任的实际应用至关重要，研究依赖模型API访问的黑盒不确定性量化方法背后隐含假设。

Method: 提出三个数学陈述及对应统计检验来捕捉一致性假设的变体，用指标评估LLM输出在不同任务中的一致性，基于“Sim - Any”假设提出数据无关的黑盒UQ方法。

Result: 实证研究表明一致性假设在不同设置下普遍存在，提出的方法能优于最接近的基线。

Conclusion: 一致性假设具有实际价值，基于该假设提出的方法可用于大语言模型输出的置信度估计。

Abstract: Estimating the confidence of large language model (LLM) outputs is essential
for real-world applications requiring high user trust. Black-box uncertainty
quantification (UQ) methods, relying solely on model API access, have gained
popularity due to their practical benefits. In this paper, we examine the
implicit assumption behind several UQ methods, which use generation consistency
as a proxy for confidence, an idea we formalize as the consistency hypothesis.
We introduce three mathematical statements with corresponding statistical tests
to capture variations of this hypothesis and metrics to evaluate LLM output
conformity across tasks. Our empirical investigation, spanning 8 benchmark
datasets and 3 tasks (question answering, text summarization, and text-to-SQL),
highlights the prevalence of the hypothesis under different settings. Among the
statements, we highlight the `Sim-Any' hypothesis as the most actionable, and
demonstrate how it can be leveraged by proposing data-free black-box UQ methods
that aggregate similarities between generations for confidence estimation.
These approaches can outperform the closest baselines, showcasing the practical
value of the empirically observed consistency hypothesis.

</details>


### [223] [DeepTalk: Towards Seamless and Smart Speech Interaction with Adaptive Modality-Specific MoE](https://arxiv.org/abs/2506.21864)
*Hang Shao,Heting Gao,Yunhang Shen,Jiawei Chen,Lijiang Li,Zuwei Long,Bo Tong,Ke Li,Xing Sun*

Main category: cs.CL

TL;DR: 文章指出原生多模态大语言模型存在问题，提出DeepTalk框架解决，性能损失小且对话延迟低，代码和模型已开源。


<details>
  <summary>Details</summary>
Motivation: 原生多模态大语言模型因配对语音 - 文本数据不足，存在灾难性遗忘和性能下降问题，需要解决。

Method: 提出基于混合专家（MoE）架构的DeepTalk框架，先自适应区分模态专家，进行单模态训练，再进行联合多模态协作训练。

Result: DeepTalk性能下降仅5.5%，远低于原生多模态大语言模型平均超20%的下降，与模块化多模态大语言模型相当，端到端对话延迟在0.5秒内。

Conclusion: DeepTalk有效解决了原生多模态大语言模型的问题，能确保无缝智能语音交互体验。

Abstract: Native multimodal large language models (MLLMs) restructure a single large
language model (LLM) into a spoken language model (SLM) capable of both speech
and text generation. Compared to modular and aligned MLLMs, native MLLMs
preserve richer paralinguistic features such as emotion and prosody, and
generate speech responses directly within the backbone LLM rather than using a
separate speech decoder. This integration also results in lower response
latency and smoother interaction. However, native MLLMs suffer from
catastrophic forgetting and performance degradation because the available
paired speech-text data is insufficient to support the pretraining of MLLMs
compared to the vast amount of text data required to pretrain text LLMs. To
address this issue, we propose DeepTalk, a framework for adaptive modality
expert learning based on a Mixture of Experts (MoE) architecture. DeepTalk
first adaptively distinguishes modality experts according to their modality
load within the LLM. Each modality expert then undergoes specialized
single-modality training, followed by joint multimodal collaborative training.
As a result, DeepTalk incurs only a 5.5% performance drop compared to the
original LLM, which is significantly lower than the average performance drop of
over 20% typically seen in native MLLMs (such as GLM-4-Voice), and is on par
with modular MLLMs. Meanwhile, the end-to-end dialogue latency remains within
0.5 seconds, ensuring a seamless and intelligent speech interaction experience.
Code and models are released at https://github.com/talkking/DeepTalk.

</details>


### [224] [Do Vision-Language Models Have Internal World Models? Towards an Atomic Evaluation](https://arxiv.org/abs/2506.21876)
*Qiyue Gao,Xinyu Pi,Kevin Liu,Junrong Chen,Ruolan Yang,Xinqi Huang,Xinyu Fang,Lu Sun,Gautham Kishore,Bo Ai,Stone Tao,Mengyang Liu,Jiaxi Yang,Chao-Jung Lai,Chuanyang Jin,Jiannan Xiang,Benhao Huang,Zeming Chen,David Danks,Hao Su,Tianmin Shu,Ziqiao Ma,Lianhui Qin,Zhiting Hu*

Main category: cs.CL

TL;DR: 本文提出评估框架和基准测试，发现当前VLMs在世界建模能力上有显著局限。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对VLMs基础世界模型能力的系统评估，因此开展研究。

Method: 提出两阶段框架评估感知和预测能力，引入WM - ABench基准测试。

Result: 通过660个实验，发现VLMs在基础世界建模能力上有明显局限，与人类水平存在显著差距。

Conclusion: 当前VLMs在基础世界建模能力方面存在不足。

Abstract: Internal world models (WMs) enable agents to understand the world's state and
predict transitions, serving as the basis for advanced deliberative reasoning.
Recent large Vision-Language Models (VLMs), such as OpenAI o3, GPT-4o and
Gemini, exhibit potential as general-purpose WMs. While the latest studies have
evaluated and shown limitations in specific capabilities such as visual
understanding, a systematic evaluation of VLMs' fundamental WM abilities
remains absent. Drawing on comparative psychology and cognitive science, we
propose a two-stage framework that assesses Perception (visual, spatial,
temporal, quantitative, and motion) and Prediction (mechanistic simulation,
transitive inference, compositional inference) to provide an atomic evaluation
of VLMs as WMs. Guided by this framework, we introduce WM-ABench, a large-scale
benchmark comprising 23 fine-grained evaluation dimensions across 6 diverse
simulated environments with controlled counterfactual simulations. Through 660
experiments on 15 latest commercial and open-source VLMs, we find that these
models exhibit striking limitations in basic world modeling abilities. For
instance, almost all models perform at near-random accuracy when distinguishing
motion trajectories. Additionally, they lack disentangled understanding --
e.g., some models tend to believe blue objects move faster than green ones.
More rich results and analyses reveal significant gaps between VLMs and
human-level world modeling.

</details>


### [225] [Advancing Jailbreak Strategies: A Hybrid Approach to Exploiting LLM Vulnerabilities and Bypassing Modern Defenses](https://arxiv.org/abs/2506.21972)
*Mohamed Ahmed,Mohamed Abdelmouty,Mingyu Kim,Gunvanth Kandula,Alex Park,James C. Davis*

Main category: cs.CL

TL;DR: 预训练和大语言模型易受攻击，本文提出结合标记和提示级技术的混合越狱方法，评估显示能提高攻击成功率，揭示当前安全机制漏洞。


<details>
  <summary>Details</summary>
Motivation: 预训练和大语言模型在推理阶段易受标记级和提示级越狱攻击，且两种攻击方法存在互补局限性，需提升越狱效果。

Method: 提出GCG + PAIR和GCG + WordGame两种结合标记和提示级技术的混合方法，在多个Vicuna和Llama模型上评估。

Result: GCG + PAIR在无防御模型上提高攻击成功率，GCG + WordGame保持高成功率，两种方法能突破先进防御。

Conclusion: 当前安全机制存在未报告漏洞，需全面防护应对适应性攻击者。

Abstract: The advancement of Pre-Trained Language Models (PTLMs) and Large Language
Models (LLMs) has led to their widespread adoption across diverse applications.
Despite their success, these models remain vulnerable to attacks that exploit
their inherent weaknesses to bypass safety measures. Two primary
inference-phase threats are token-level and prompt-level jailbreaks.
Token-level attacks embed adversarial sequences that transfer well to black-box
models like GPT but leave detectable patterns and rely on gradient-based token
optimization, whereas prompt-level attacks use semantically structured inputs
to elicit harmful responses yet depend on iterative feedback that can be
unreliable. To address the complementary limitations of these methods, we
propose two hybrid approaches that integrate token- and prompt-level techniques
to enhance jailbreak effectiveness across diverse PTLMs. GCG + PAIR and the
newly explored GCG + WordGame hybrids were evaluated across multiple Vicuna and
Llama models. GCG + PAIR consistently raised attack-success rates over its
constituent techniques on undefended models; for instance, on Llama-3, its
Attack Success Rate (ASR) reached 91.6%, a substantial increase from PAIR's
58.4% baseline. Meanwhile, GCG + WordGame matched the raw performance of
WordGame maintaining a high ASR of over 80% even under stricter evaluators like
Mistral-Sorry-Bench. Crucially, both hybrids retained transferability and
reliably pierced advanced defenses such as Gradient Cuff and JBShield, which
fully blocked single-mode attacks. These findings expose previously unreported
vulnerabilities in current safety stacks, highlight trade-offs between raw
success and defensive robustness, and underscore the need for holistic
safeguards against adaptive adversaries.

</details>


### [226] [More Vulnerable than You Think: On the Stability of Tool-Integrated LLM Agents](https://arxiv.org/abs/2506.21967)
*Weimin Xiong,Ke Wang,Yifan Song,Hanchao Liu,Sai Zhou,Wei Peng,Sujian Li*

Main category: cs.CL

TL;DR: 研究关注工具集成LLM代理稳定性，实验发现各阶段易出错，开源模型更脆弱，增大模型尺寸效果不佳。


<details>
  <summary>Details</summary>
Motivation: 当前工具集成LLM代理评估忽视稳定性，限制其实际应用。

Method: 研究代理在整个工具调用过程中是否易出错，开展大量实验。

Result: 代理在各阶段都易出错，开源模型代理比专有模型更脆弱，增大模型尺寸不能显著提升推理能力，还可能使代理更易受攻击。

Conclusion: 强调评估代理稳定性的重要性，为未来LLM开发和评估提供见解。

Abstract: Current evaluations of tool-integrated LLM agents typically focus on
end-to-end tool-usage evaluation while neglecting their stability. This limits
their real-world applicability, as various internal or external factors can
cause agents to crash or behave abnormally. Our research addresses this by
investigating whether agents are vulnerable to errors throughout the entire
tool invocation process, including reading tool documentation, selecting tools
and generating parameters, and processing the tool's response. Through
extensive experiments, we observe that agents are highly susceptible to errors
at each stage and agents based on open-source models are more vulnerable than
those based on proprietary models. We also find that increasing the model size
does not significantly improve tool invocation reasoning and may make agents
more vulnerable to attacks resembling normal user instructions. This highlights
the importance of evaluating agent stability and offers valuable insights for
future LLM development and evaluation.

</details>


### [227] [Analyzing and Fine-Tuning Whisper Models for Multilingual Pilot Speech Transcription in the Cockpit](https://arxiv.org/abs/2506.21990)
*Kartheek Kumar Reddy Nareddy,Sarah Ternus,Julia Niebling*

Main category: cs.CL

TL;DR: 本文研究用Whisper模型提升驾驶舱对话转录准确率，收集数据并提出归一化方案与微调方法，使WER显著降低。


<details>
  <summary>Details</summary>
Motivation: 预训练模型在驾驶舱等小众领域性能不佳，需提升驾驶舱对话转录准确率。

Method: 收集约215分钟数据并手动标注，提出归一化方案，采用低秩自适应（LoRA）进行微调。

Result: 字错率（WER）从68.49%降至26.26%。

Conclusion: 提出的归一化方案和微调方法能有效提升Whisper模型在驾驶舱对话转录的准确率。

Abstract: The developments in transformer encoder-decoder architectures have led to
significant breakthroughs in machine translation, Automatic Speech Recognition
(ASR), and instruction-based chat machines, among other applications. The
pre-trained models were trained on vast amounts of generic data over a few
epochs (fewer than five in most cases), resulting in their strong
generalization capabilities. Nevertheless, the performance of these models does
suffer when applied to niche domains like transcribing pilot speech in the
cockpit, which involves a lot of specific vocabulary and multilingual
conversations. This paper investigates and improves the transcription accuracy
of cockpit conversations with Whisper models. We have collected around 85
minutes of cockpit simulator recordings and 130 minutes of interview recordings
with pilots and manually labeled them. The speakers are middle aged men
speaking both German and English. To improve the accuracy of transcriptions, we
propose multiple normalization schemes to refine the transcripts and improve
Word Error Rate (WER). We then employ fine-tuning to enhance ASR performance,
utilizing performance-efficient fine-tuning with Low-Rank Adaptation (LoRA).
Hereby, WER decreased from 68.49 \% (pretrained whisper Large model without
normalization baseline) to 26.26\% (finetuned whisper Large model with the
proposed normalization scheme).

</details>


### [228] [Identifying a Circuit for Verb Conjugation in GPT-2](https://arxiv.org/abs/2506.22105)
*David Demitri Africa*

Main category: cs.CL

TL;DR: 对GPT - 2 Small中负责主谓一致的子网络进行隔离和解读，发现只需部分组件 - 标记对就能在基础任务达近模型性能。


<details>
  <summary>Details</summary>
Motivation: 隔离并解读GPT - 2 Small中负责主谓一致的子网络。

Method: 给模型输入含单复数主语的提示，用性能验证、自动电路发现、直接对数归因等技术。

Result: 隔离出对模型正确动词变位有显著贡献的候选电路，少量网络组件 - 标记对可在基础任务达近模型性能，复杂场景需更多。

Conclusion: 未明确提及新结论，主要呈现研究成果。

Abstract: I implement a procedure to isolate and interpret the sub-network (or
"circuit") responsible for subject-verb agreement in GPT-2 Small. In this
study, the model is given prompts where the subject is either singular (e.g.
"Alice") or plural (e.g. "Alice and Bob"), and the task is to correctly predict
the appropriate verb form ("walks" for singular subjects, "walk" for plural
subjects). Using a series of techniques-including performance verification
automatic circuit discovery via direct path patching, and direct logit
attribution- I isolate a candidate circuit that contributes significantly to
the model's correct verb conjugation. The results suggest that only a small
fraction of the network's component-token pairs is needed to achieve near-model
performance on the base task but substantially more for more complex settings.

</details>


### [229] [QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting, KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization](https://arxiv.org/abs/2506.22396)
*Danush Khanna,Aditya Kumar Guru,Srivarshinee Sridhar,Zidan Ahmed,Rubhav Bahirwani,Meetu Malhotra,Vinija Jain,Aman Chadha,Amitava Das,Kripabandhu Ghosh*

Main category: cs.CL

TL;DR: 提出QuickSilver框架，可在推理时实现语义自适应，减少FLOP，无需改变模型权重或结构。


<details>
  <summary>Details</summary>
Motivation: 大语言模型推理占延迟和能耗大部分成本，运行时优化是瓶颈，现有方法有局限。

Method: 引入QuickSilver框架，集成动态令牌停止、KV缓存跳过和上下文令牌融合三种机制。

Result: 应用于GPT - 2和Llama - 2，在WikiText - 103和C4上实现高达39.6%的FLOP减少，困惑度下降可忽略不计。

Conclusion: QuickSilver能在不改变模型权重和结构的情况下，有效减少推理成本。

Abstract: Inference accounts for the majority of latency and energy consumption in
large language model (LLM) deployments, often exceeding 90% of total cost.
While training-time efficiency has seen extensive progress, runtime
optimization remains a key bottleneck, particularly under autoregressive
decoding. Existing approaches -- such as pruning, quantization, early exits,
and speculative decoding -- often require retraining, architectural changes, or
disrupt decoding compatibility. We introduce QuickSilver, a modular,
token-level framework that enables semantic adaptivity at inference time
without altering model weights or structure. QuickSilver integrates four
synergistic mechanisms:
  (i) Dynamic Token Halting, which halts computation for tokens with converged
representations; (ii) KV Cache Skipping, which selectively suppresses memory
writes to reduce attention overhead; and (iii) Contextual Token Fusion, which
collapses redundant tokens into shared paths to shrink sequence length.
  Unlike speculative decoding or MoE routing, QuickSilver operates entirely on
frozen, dense models and requires no auxiliary networks. Applied to GPT-2 and
Llama-2 across WikiText-103 and C4, QuickSilver achieves up to 39.6% FLOP
reduction with negligible perplexity degradation (<=0.2).

</details>


### [230] [HyperCLOVA X THINK Technical Report](https://arxiv.org/abs/2506.22403)
*NAVER Cloud HyperCLOVA X Team*

Main category: cs.CL

TL;DR: 介绍HyperCLOVA X THINK，预训练于大量韩语和英语token，多阶段训练，在韩语基准测试表现好，视觉增强变体在KCSAT STEM基准测试表现佳，训练计算量低，还提出剪枝和蒸馏技术。


<details>
  <summary>Details</summary>
Motivation: 开发专注推理的大语言模型，为韩国AI创新提供强大基础，为全球研究社区提供有价值资源。

Method: 采用计算内存平衡的Peri - LN Transformer，通过三阶段课程预训练扩展上下文窗口到128K tokens，用可验证奖励的强化学习进行有监督微调。

Result: 在韩国相关基准测试中与同规模模型竞争表现出色，视觉增强变体在KCSAT STEM基准测试达到或超过GPT - 4.1，训练计算量低。

Conclusion: HyperCLOVA X THINK是韩国AI创新的有力基础，对全球研究社区有价值。

Abstract: We introduce HyperCLOVA X THINK, the first reasoning-focused large language
model in the HyperCLOVA X family, pre-trained on roughly $6$ trillion
high-quality Korean, and English tokens, augmented with targeted synthetic
Korean data. It was implemented as a compute-memory-balanced Peri-LN
Transformer scaled with $\mu$P, pre-trained through a three-stage curriculum
that expands the context window to $128$K tokens, and post-trained via
supervised fine-tuning with Reinforcement Learning from Verifiable Rewards
supports both detailed rationale and concise-answer modes. It delivers
competitive performance against similarly sized models on Korea-focused
benchmarks such as KMMLU, CSAT, KoBALT-700, HAERAE-1.0, and KoBigBench, while
preserving robust bilingual consistency and translation quality. In addition, a
vision-augmented variant matches or exceeds GPT-4.1 on the KCSAT STEM
benchmark, all of which are achieved with substantially lower training compute
than existing models of similar sizes. We also present a pruning and
distillation technique that will soon be applied to HyperCLOVA X THINK for an
open-source and business-friendly foundation model. Altogether, these
capabilities position HyperCLOVA X THINK as a robust foundation for Korean AI
innovation and a valuable resource for the global research community.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [231] [Monte Carlo and quasi-Monte Carlo integration for likelihood functions](https://arxiv.org/abs/2506.21733)
*Yanbo Tang*

Main category: math.ST

TL;DR: 对比蒙特卡罗（MC）和拟蒙特卡罗（QMC）方法在近似后验分布归一化常数和边际似然积分误差，分析误差与数据点、网格点数量和积分维度关系，研究不同维度下两种方法表现并给出QMC优于MC条件及Halton序列星偏差高维缩放边界。


<details>
  <summary>Details</summary>
Motivation: 研究MC和QMC方法在近似后验分布归一化常数和边际似然时积分误差情况，明确两种方法在不同参数条件下表现。

Method: 对比分析MC和QMC方法的相对和绝对积分误差与数据点数量$n$、网格点数量$m$和积分维度$p$的依赖关系。

Result: 积分维度固定时，MC相对误差缩放率有额外数据依赖因子，QMC优于MC有新条件；高维下使用Halton序列构建低差异网格时MC表现更优，但两者维度缩放都差，还给出Halton序列星偏差高维缩放边界。

Conclusion: 不同维度下MC和QMC方法各有优劣，维度固定时QMC在一定条件下更优，高维时MC相对较好，但两者在高维都面临维度缩放问题。

Abstract: We compare the integration error of Monte Carlo (MC) and quasi-Monte Carlo
(QMC) methods for approximating the normalizing constant of posterior
distributions and certain marginal likelihoods. In doing so, we characterize
the dependency of the relative and absolute integration errors on the number of
data points ($n$), the number of grid points ($m$) and the dimension of the
integral ($p$). We find that if the dimension of the integral remains fixed as
$n$ and $m$ tend to infinity, the scaling rate of the relative error of MC
integration includes an additional $n^{1/2}\log(n)^{p/2}$ data-dependent
factor, while for QMC this factor is $\log(n)^{p/2}$. In this scenario, QMC
will outperform MC if $\log(m)^{p - 1/2}/\sqrt{mn\log(n)} < 1$, which differs
from the usual result that QMC will outperform MC if $\log(m)^p/m^{1/2} <
1$.The accuracies of MC and QMC methods are also examined in the
high-dimensional setting as $p \rightarrow \infty$, where MC gives more
optimistic results as the scaling in dimension is slower than that of QMC when
the Halton sequence is used to construct the low discrepancy grid; however both
methods display poor dimensional scaling as expected. An additional
contribution of this work is a bound on the high-dimensional scaling of the
star discrepancy for the Halton sequence.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [232] [Computing Maximum Cliques in Unit Disk Graphs](https://arxiv.org/abs/2506.21926)
*Anastasiia Tkachenko,Haitao Wang*

Main category: cs.CG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Given a set $P$ of $n$ points in the plane, the unit-disk graph $G(P)$ is a
graph with $P$ as its vertex set such that two points of $P$ have an edge if
their Euclidean distance is at most $1$. We consider the problem of computing a
maximum clique in $G(P)$. The previously best algorithm for the problem runs in
$O(n^{7/3+o(1)})$ time. We show that the problem can be solved in $O(n \log n +
n K^{4/3+o(1)})$ time, where $K$ is the maximum clique size. The algorithm is
faster than the previous one when $K=o(n)$. In addition, if $P$ is in convex
position, we give a randomized algorithm that runs in $O(n^{15/7+o(1)})=
O(n^{2.143})$ worst-case time and the algorithm can compute a maximum clique
with high probability. For points in convex position, one special case we solve
is when a point in the maximum clique is given; we present an $O(n^2\log n)$
time (deterministic) algorithm for this special case.

</details>


<div id='physics.ins-det'></div>

# physics.ins-det [[Back]](#toc)

### [233] [CaloHadronic: a diffusion model for the generation of hadronic showers](https://arxiv.org/abs/2506.21720)
*Thorsten Buss,Frank Gaede,Gregor Kasieczka,Anatolii Korol,Katja Krüger,Peter McKeown,Martina Mozzanica*

Main category: physics.ins-det

TL;DR: 提出基于Transformer扩展架构用于高粒度成像量能器系统中全面生成电磁和强子量能器的粒子簇射。


<details>
  <summary>Details</summary>
Motivation: 用生成式机器学习模型实现高精度和高速度，以增强传统模拟并缓解计算约束。

Method: 提出基于Transformer对先前架构进行扩展，利用注意力机制。

Result: 首次用机器学习方法在高粒度成像量能器系统中全面生成电磁和强子量能器的粒子簇射。

Conclusion: Transformer扩展架构可用于生成具有更明显子结构的复杂强子簇射。

Abstract: Simulating showers of particles in highly-granular calorimeters is a key
frontier in the application of machine learning to particle physics. Achieving
high accuracy and speed with generative machine learning models can enable them
to augment traditional simulations and alleviate a major computing constraint.
Recent developments have shown how diffusion based generative shower simulation
approaches that do not rely on a fixed structure, but instead generate
geometry-independent point clouds, are very efficient. We present a
transformer-based extension to previous architectures which were developed for
simulating electromagnetic showers in the highly granular electromagnetic
calorimeter of the International Large Detector, ILD. The attention mechanism
now allows us to generate complex hadronic showers with more pronounced
substructure across both the electromagnetic and hadronic calorimeters. This is
the first time that machine learning methods are used to holistically generate
showers across the electromagnetic and hadronic calorimeter in highly granular
imaging calorimeter systems.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [234] [FrankenBot: Brain-Morphic Modular Orchestration for Robotic Manipulation with Vision-Language Models](https://arxiv.org/abs/2506.21627)
*Shiyi Wang,Wenbo Li,Yiteng Chen,Qingyao Wu,Huiping Zhuang*

Main category: cs.RO

TL;DR: 本文提出FrankenBot框架，实现全面功能与高效操作，实验证明其在多方面有优势。


<details>
  <summary>Details</summary>
Motivation: 开发能在复杂环境执行多种任务的通用机器人操作系统具挑战性，现有方法未将机器人脑功能集成，需整合功能并实现高效运行。

Method: 受分治策略和人脑架构启发，提出VLM驱动的FrankenBot框架，包含系列组件，将关键功能解耦，映射不同功能到大脑对应部位并设计协调机制。

Result: 在模拟和真实机器人环境实验表明，方法在异常检测处理、长期记忆、操作效率和稳定性方面有显著优势，无需微调或再训练。

Conclusion: FrankenBot框架可实现全面功能和高运行效率，在机器人操作领域有应用价值。

Abstract: Developing a general robot manipulation system capable of performing a wide
range of tasks in complex, dynamic, and unstructured real-world environments
has long been a challenging task. It is widely recognized that achieving
human-like efficiency and robustness manipulation requires the robotic brain to
integrate a comprehensive set of functions, such as task planning, policy
generation, anomaly monitoring and handling, and long-term memory, achieving
high-efficiency operation across all functions. Vision-Language Models (VLMs),
pretrained on massive multimodal data, have acquired rich world knowledge,
exhibiting exceptional scene understanding and multimodal reasoning
capabilities. However, existing methods typically focus on realizing only a
single function or a subset of functions within the robotic brain, without
integrating them into a unified cognitive architecture. Inspired by a
divide-and-conquer strategy and the architecture of the human brain, we propose
FrankenBot, a VLM-driven, brain-morphic robotic manipulation framework that
achieves both comprehensive functionality and high operational efficiency. Our
framework includes a suite of components, decoupling a part of key functions
from frequent VLM calls, striking an optimal balance between functional
completeness and system efficiency. Specifically, we map task planning, policy
generation, memory management, and low-level interfacing to the cortex,
cerebellum, temporal lobe-hippocampus complex, and brainstem, respectively, and
design efficient coordination mechanisms for the modules. We conducted
comprehensive experiments in both simulation and real-world robotic
environments, demonstrating that our method offers significant advantages in
anomaly detection and handling, long-term memory, operational efficiency, and
stability -- all without requiring any fine-tuning or retraining.

</details>


### [235] [Ark: An Open-source Python-based Framework for Robot Learning](https://arxiv.org/abs/2506.21628)
*Magnus Dierking,Christopher E. Mower,Sarthak Das,Huang Helong,Jiacheng Qiu,Cody Reading,Wei Chen,Huidong Liang,Huang Guowei,Jan Peters,Quan Xingyue,Jun Wang,Haitham Bou-Ammar*

Main category: cs.RO

TL;DR: 介绍开源Python优先的机器人框架ARK，统一机器人与AI实践，降低门槛并加速自主机器人研发与部署。


<details>
  <summary>Details</summary>
Motivation: 机器人硬件进步但商业自主性落后，软件是瓶颈，当前机器人堆栈学习曲线陡峭，需C/C++专业知识等，与现代AI生态差距大。

Method: 引入ARK框架，提供Gym风格环境接口，采用轻量级客户端 - 服务器架构，有可选C/C++绑定，自带可复用模块及ROS互操作性。

Result: 综合文档和案例展示了快速原型制作、轻松硬件交换和端到端管道，堪比主流机器学习工作流程。

Conclusion: ARK统一机器人和AI实践，降低进入门槛，加速自主机器人研究和商业部署。

Abstract: Robotics has made remarkable hardware strides-from DARPA's Urban and Robotics
Challenges to the first humanoid-robot kickboxing tournament-yet commercial
autonomy still lags behind progress in machine learning. A major bottleneck is
software: current robot stacks demand steep learning curves, low-level C/C++
expertise, fragmented tooling, and intricate hardware integration, in stark
contrast to the Python-centric, well-documented ecosystems that propelled
modern AI. We introduce ARK, an open-source, Python-first robotics framework
designed to close that gap. ARK presents a Gym-style environment interface that
allows users to collect data, preprocess it, and train policies using
state-of-the-art imitation-learning algorithms (e.g., ACT, Diffusion Policy)
while seamlessly toggling between high-fidelity simulation and physical robots.
A lightweight client-server architecture provides networked
publisher-subscriber communication, and optional C/C++ bindings ensure
real-time performance when needed. ARK ships with reusable modules for control,
SLAM, motion planning, system identification, and visualization, along with
native ROS interoperability. Comprehensive documentation and case studies-from
manipulation to mobile navigation-demonstrate rapid prototyping, effortless
hardware swapping, and end-to-end pipelines that rival the convenience of
mainstream machine-learning workflows. By unifying robotics and AI practices
under a common Python umbrella, ARK lowers entry barriers and accelerates
research and commercial deployment of autonomous robots.

</details>


### [236] [AeroLite-MDNet: Lightweight Multi-task Deviation Detection Network for UAV Landing](https://arxiv.org/abs/2506.21635)
*Haiping Yang,Huaxing Liu,Wei Wu,Zuohui Chen,Ning Wu*

Main category: cs.RO

TL;DR: 本文提出基于AeroLite - MDNet的无人机着陆偏差预警系统，引入新指标AWD和数据集UAVLandData，实验显示系统能提升着陆可靠性。


<details>
  <summary>Details</summary>
Motivation: 无人机完成任务后需安全着陆，但GPS信号干扰等因素使准确着陆有挑战，需解决此问题。

Method: 提出基于AeroLite - MDNet的偏差预警系统，该模型含多尺度融合模块和分割分支；引入平均预警延迟（AWD）指标；构建UAVLandData数据集。

Result: 系统平均预警延迟0.7秒，偏差检测准确率98.6%。

Conclusion: 所提系统能有效提升无人机着陆可靠性。

Abstract: Unmanned aerial vehicles (UAVs) are increasingly employed in diverse
applications such as land surveying, material transport, and environmental
monitoring. Following missions like data collection or inspection, UAVs must
land safely at docking stations for storage or recharging, which is an
essential requirement for ensuring operational continuity. However, accurate
landing remains challenging due to factors like GPS signal interference. To
address this issue, we propose a deviation warning system for UAV landings,
powered by a novel vision-based model called AeroLite-MDNet. This model
integrates a multiscale fusion module for robust cross-scale object detection
and incorporates a segmentation branch for efficient orientation estimation. We
introduce a new evaluation metric, Average Warning Delay (AWD), to quantify the
system's sensitivity to landing deviations. Furthermore, we contribute a new
dataset, UAVLandData, which captures real-world landing deviation scenarios to
support training and evaluation. Experimental results show that our system
achieves an AWD of 0.7 seconds with a deviation detection accuracy of 98.6\%,
demonstrating its effectiveness in enhancing UAV landing reliability. Code will
be available at https://github.com/ITTTTTI/Maskyolo.git

</details>


### [237] [TOMD: A Trail-based Off-road Multimodal Dataset for Traversable Pathway Segmentation under Challenging Illumination Conditions](https://arxiv.org/abs/2506.21630)
*Yixin Sun,Li Li,Wenke E,Amir Atapour-Abarghouei,Toby P. Breckon*

Main category: cs.RO

TL;DR: 为解决自主机器人在非结构化户外环境中检测可通行路径的难题，引入TOMD数据集并提出动态多尺度数据融合模型，分析不同融合策略在不同光照下的表现，结果有效并公开数据集。


<details>
  <summary>Details</summary>
Motivation: 现有数据集和模型主要针对城市或宽阔越野场景，在狭窄类小径越野场景存在不足，需要解决自主机器人在非结构化户外环境检测可通行路径的问题。

Method: 引入Trail - based Off - road Multimodal Dataset (TOMD) 数据集，提出动态多尺度数据融合模型，分析早期、交叉和混合融合策略在不同光照水平下的性能。

Result: 研究结果证明了所提方法的有效性，以及光照对分割性能的相关性。

Conclusion: 公开TOMD数据集，支持未来基于小径的越野导航研究。

Abstract: Detecting traversable pathways in unstructured outdoor environments remains a
significant challenge for autonomous robots, especially in critical
applications such as wide-area search and rescue, as well as incident
management scenarios like forest fires. Existing datasets and models primarily
target urban settings or wide, vehicle-traversable off-road tracks, leaving a
substantial gap in addressing the complexity of narrow, trail-like off-road
scenarios. To address this, we introduce the Trail-based Off-road Multimodal
Dataset (TOMD), a comprehensive dataset specifically designed for such
environments. TOMD features high-fidelity multimodal sensor data -- including
128-channel LiDAR, stereo imagery, GNSS, IMU, and illumination measurements --
collected through repeated traversals under diverse conditions. We also propose
a dynamic multiscale data fusion model for accurate traversable pathway
prediction. The study analyzes the performance of early, cross, and mixed
fusion strategies under varying illumination levels. Results demonstrate the
effectiveness of our approach and the relevance of illumination in segmentation
performance. We publicly release TOMD at https://github.com/yyyxs1125/TMOD to
support future research in trail-based off-road navigation.

</details>


### [238] [Experimental investigation of pose informed reinforcement learning for skid-steered visual navigation](https://arxiv.org/abs/2506.21732)
*Ameya Salvi,Venkat Krovi*

Main category: cs.RO

TL;DR: 本文聚焦视觉车道保持，指出滑移转向车辆自动化瓶颈，提出学习视觉导航的结构化公式化新方法，经测试性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 滑移转向车辆的系统建模，尤其是越野环境下的轮滑与地形交互建模，阻碍了自动化部署，且端到端学习方法在动态操作制度下的系统公式化和验证仍待完善。

Method: 提出用于学习视觉导航的结构化公式化新方法，并进行广泛的软件模拟、硬件评估和消融研究。

Result: 该方法在性能上显著优于现有文献中的方法。

Conclusion: 所提出的用于学习视觉导航的结构化公式化新方法是有效的，能提升相关性能。

Abstract: Vision-based lane keeping is a topic of significant interest in the robotics
and autonomous ground vehicles communities in various on-road and off-road
applications. The skid-steered vehicle architecture has served as a useful
vehicle platform for human controlled operations. However, systematic modeling,
especially of the skid-slip wheel terrain interactions (primarily in off-road
settings) has created bottlenecks for automation deployment. End-to-end
learning based methods such as imitation learning and deep reinforcement
learning, have gained prominence as a viable deployment option to counter the
lack of accurate analytical models. However, the systematic formulation and
subsequent verification/validation in dynamic operation regimes (particularly
for skid-steered vehicles) remains a work in progress. To this end, a novel
approach for structured formulation for learning visual navigation is proposed
and investigated in this work. Extensive software simulations, hardware
evaluations and ablation studies now highlight the significantly improved
performance of the proposed approach against contemporary literature.

</details>


### [239] [ASVSim (AirSim for Surface Vehicles): A High-Fidelity Simulation Framework for Autonomous Surface Vehicle Research](https://arxiv.org/abs/2506.22174)
*Bavo Lesy,Siemen Herremans,Robin Kerstens,Jan Steckel,Walter Daems,Siegfried Mercelis,Ali Anwar*

Main category: cs.RO

TL;DR: 本文介绍了用于内陆和港口环境自主航运研究的开源仿真框架ASVSim，它能生成合成数据集，经实验有研究潜力且开源。


<details>
  <summary>Details</summary>
Motivation: 运输业对无人水面艇感兴趣，但缺乏开源高保真仿真框架和数据集，推动了ASVSim的开发。

Method: 基于Cosys - AirSim构建ASVSim，结合模拟船舶动力学和海洋传感器模拟能力。

Result: 通过有限实验展示了模拟器在传统控制方法和深度学习方法研究领域的潜力。

Conclusion: ASVSim以开源项目形式提供，让更多海洋工程界人员可开展自主导航研究。

Abstract: The transport industry has recently shown significant interest in unmanned
surface vehicles (USVs), specifically for port and inland waterway transport.
These systems can improve operational efficiency and safety, which is
especially relevant in the European Union, where initiatives such as the Green
Deal are driving a shift towards increased use of inland waterways. At the same
time, a shortage of qualified personnel is accelerating the adoption of
autonomous solutions. However, there is a notable lack of open-source,
high-fidelity simulation frameworks and datasets for developing and evaluating
such solutions. To address these challenges, we introduce AirSim For Surface
Vehicles (ASVSim), an open-source simulation framework specifically designed
for autonomous shipping research in inland and port environments. The framework
combines simulated vessel dynamics with marine sensor simulation capabilities,
including radar and camera systems and supports the generation of synthetic
datasets for training computer vision models and reinforcement learning agents.
Built upon Cosys-AirSim, ASVSim provides a comprehensive platform for
developing autonomous navigation algorithms and generating synthetic datasets.
The simulator supports research of both traditional control methods and deep
learning-based approaches. Through limited experiments, we demonstrate the
potential of the simulator in these research areas. ASVSim is provided as an
open-source project under the MIT license, making autonomous navigation
research accessible to a larger part of the ocean engineering community.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [240] [Demonstrating Interoperable Channel State Feedback Compression with Machine Learning](https://arxiv.org/abs/2506.21796)
*Dani Korpi,Rachel Wang,Jerry Wang,Abdelrahman Ibrahim,Carl Nuzman,Runxin Wang,Kursat Rasim Mestav,Dustin Zhang,Iraj Saniee,Shawn Winston,Gordana Pavlovic,Wei Ding,William J. Hillery,Chenxi Hao,Ram Thirunagari,Jung Chang,Jeehyun Kim,Bartek Kozicki,Dragan Samardzija,Taesang Yoo,Andreas Maeder,Tingfang Ji,Harish Viswanathan*

Main category: eess.SP

TL;DR: 本文提出保密训练可互操作的机器学习模型进行信道反馈压缩和解压缩的新方法，通过原型设备验证模型准确性，证明不共享模型也能实现准确的信道反馈，为6G网络应用铺平道路。


<details>
  <summary>Details</summary>
Motivation: 现有研究多基于仿真，缺乏实际场景中设备和基站不共享机器学习模型时，基于机器学习的信道反馈压缩的概念验证。

Method: 提出一种保密训练可互操作的压缩和解压缩机器学习模型的新方法，用原型用户设备和基站验证模型准确性。

Result: 测量结果表明，不共享机器学习模型也能开发出准确的基于机器学习的信道反馈链路。

Conclusion: 研究结果为商业6G网络中基于机器学习的信道反馈的实际应用铺平了道路。

Abstract: Neural network-based compression and decompression of channel state feedback
has been one of the most widely studied applications of machine learning (ML)
in wireless networks. Various simulation-based studies have shown that ML-based
feedback compression can result in reduced overhead and more accurate channel
information. However, to the best of our knowledge, there are no real-life
proofs of concepts demonstrating the benefits of ML-based channel feedback
compression in a practical setting, where the user equipment (UE) and base
station have no access to each others' ML models. In this paper, we present a
novel approach for training interoperable compression and decompression ML
models in a confidential manner, and demonstrate the accuracy of the ensuing
models using prototype UEs and base stations. The performance of the ML-based
channel feedback is measured both in terms of the accuracy of the reconstructed
channel information and achieved downlink throughput gains when using the
channel information for beamforming. The reported measurement results
demonstrate that it is possible to develop an accurate ML-based channel
feedback link without having to share ML models between device and network
vendors. These results pave the way for a practical implementation of ML-based
channel feedback in commercial 6G networks.

</details>


### [241] [From Token to Rhythm: A Multi-Scale Approach for ECG-Language Pretraining](https://arxiv.org/abs/2506.21803)
*Fuying Wang,Jiacheng Xu,Lequan Yu*

Main category: eess.SP

TL;DR: 本文提出多尺度心电图 - 语言预训练模型MELP，通过多尺度交叉模态监督学习，在多个心电图数据集和任务上优于现有自监督学习方法。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习心电图分析依赖大量人工标注，现有自监督学习方法难以捕捉心电图信号多尺度特征，无法学习通用表示。

Method: 引入MELP模型，先预训练特定领域语言模型，再进行三个层次的交叉模态监督，将心电图信号与文本报告对齐。

Result: 在三个公共心电图数据集的多个任务上评估，MELP优于现有自监督学习方法。

Conclusion: MELP在不同临床应用中有效且具有适应性。

Abstract: Electrocardiograms (ECGs) play a vital role in monitoring cardiac health and
diagnosing heart diseases. However, traditional deep learning approaches for
ECG analysis rely heavily on large-scale manual annotations, which are both
time-consuming and resource-intensive to obtain. To overcome this limitation,
self-supervised learning (SSL) has emerged as a promising alternative, enabling
the extraction of robust ECG representations that can be efficiently
transferred to various downstream tasks. While previous studies have explored
SSL for ECG pretraining and multi-modal ECG-language alignment, they often fail
to capture the multi-scale nature of ECG signals. As a result, these methods
struggle to learn generalized representations due to their inability to model
the hierarchical structure of ECG data. To address this gap, we introduce MELP,
a novel Multi-scale ECG-Language Pretraining (MELP) model that fully leverages
hierarchical supervision from ECG-text pairs. MELP first pretrains a
cardiology-specific language model to enhance its understanding of clinical
text. It then applies three levels of cross-modal supervision-at the token,
beat, and rhythm levels-to align ECG signals with textual reports, capturing
structured information across different time scales. We evaluate MELP on three
public ECG datasets across multiple tasks, including zero-shot ECG
classification, linear probing, and transfer learning. Experimental results
demonstrate that MELP outperforms existing SSL methods, underscoring its
effectiveness and adaptability across diverse clinical applications. Our code
is available at https://github.com/HKU-MedAI/MELP.

</details>


### [242] [Searching Efficient Deep Architectures for Radar Target Detection using Monte-Carlo Tree Search](https://arxiv.org/abs/2506.21772)
*Noé Lallouet,Tristan Cazenave,Cyrille Enderli,Stéphanie Gourdin*

Main category: eess.SP

TL;DR: 研究用基于MCTS的NAS方法搜索雷达目标检测神经网络，找到轻量级网络。


<details>
  <summary>Details</summary>
Motivation: 现有深度神经网络计算复杂度高，阻碍其在嵌入式雷达系统广泛应用。

Method: 采用基于蒙特卡罗树搜索（MCTS）的神经架构搜索（NAS）方法。

Result: 评估搜索出的架构，提出一种满足检测概率且比专家设计的基线网络轻得多的新网络。

Conclusion: 基于MCTS的NAS方法可找到满足检测性能且计算复杂度低的神经网络。

Abstract: Recent research works establish deep neural networks as high performing tools
for radar target detection, especially on challenging environments (presence of
clutter or interferences, multi-target scenarii...). However, the usually large
computational complexity of these networks is one of the factors preventing
them from being widely implemented in embedded radar systems. We propose to
investigate novel neural architecture search (NAS) methods, based on
Monte-Carlo Tree Search (MCTS), for finding neural networks achieving the
required detection performance and striving towards a lower computational
complexity. We evaluate the searched architectures on endoclutter radar
signals, in order to compare their respective performance metrics and
generalization properties. A novel network satisfying the required detection
probability while being significantly lighter than the expert-designed baseline
is proposed.

</details>


<div id='q-fin.GN'></div>

# q-fin.GN [[Back]](#toc)

### [243] [OpenAlpha: A Community-Led Adversarial Strategy Validation Mechanism for Decentralised Capital Management](https://arxiv.org/abs/2506.21809)
*Arman Abgaryan,Utkarsh Sharma*

Main category: q-fin.GN

TL;DR: 提出OpenAlpha框架用于去中心化资本管理的策略验证，机制可扩展评估外部DApp，实现自适应、低信任资本部署。


<details>
  <summary>Details</summary>
Motivation: 解决去中心化资本管理中策略验证及降低对中心化治理和静态审计依赖的问题。

Method: 将资金部署建模为优化问题，通过决策瀑布流程操作，各阶段嵌入经济激励。

Result: 框架各阶段产生置信分数用于资本分配规则，机制可扩展评估外部DApp。

Conclusion: OpenAlpha架构能实现自适应、信任最小化的资本部署。

Abstract: We propose \textit{OpenAlpha}, a community-led strategy validation framework
for decentralised capital management on a host blockchain network, which
integrates game-theoretic validation, adversarial auditing, and market-based
belief aggregation. This work formulates treasury deployment as a capital
optimisation problem under verification costs and strategic misreporting, and
operationalises it through a decision waterfall that sequences intention
declaration, strategy proposal, prediction-market validation, dispute
resolution, and capital allocation. Each phase of this framework's validation
process embeds economic incentives to align proposer, verifier, and auditor
behaviour, producing confidence scores that may feed into a capital allocation
rule. While OpenAlpha is designed for capital strategy assessment, its
validation mechanisms are composable and extend naturally to evaluating
external decentralised applications (DApps), enabling on-chain scrutiny of DApp
performance, reliability, and integration risk. This architecture allows for
adaptive, trust-minimised capital deployment without reliance on centralised
governance or static audits.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [244] [Explainable anomaly detection for sound spectrograms using pooling statistics with quantile differences](https://arxiv.org/abs/2506.21921)
*Nicolas Thewes,Philipp Steinhauer,Patrick Trampert,Markus Pauly,Georg Schneider*

Main category: stat.AP

TL;DR: 本文提出针对频谱图的异常检测方法，基于统计评估，具有可解释性，适用于工业场景。


<details>
  <summary>Details</summary>
Motivation: 异常检测任务具有挑战性，在工业4.0应用中检测声音数据异常很重要，且智能算法使用存在争议，需要可解释的算法。

Method: 提出基于统计评估的、理论驱动的异常检测方法，具有内在可解释性。

Result: 得到适用于频谱图的异常检测方法。

Conclusion: 该算法适用于不希望使用黑箱算法的应用场景，对工业应用有重要意义。

Abstract: Anomaly detection is the task of identifying rarely occurring (i.e. anormal
or anomalous) samples that differ from almost all other samples in a dataset.
As the patterns of anormal samples are usually not known a priori, this task is
highly challenging. Consequently, anomaly detection lies between semi- and
unsupervised learning. The detection of anomalies in sound data, often called
'ASD' (Anomalous Sound Detection), is a sub-field that deals with the
identification of new and yet unknown effects in acoustic recordings. It is of
great importance for various applications in Industry 4.0. Here, vibrational or
acoustic data are typically obtained from standard sensor signals used for
predictive maintenance. Examples cover machine condition monitoring or quality
assurance to track the state of components or products. However, the use of
intelligent algorithms remains a controversial topic. Management generally aims
for cost-reduction and automation, while quality and maintenance experts
emphasize the need for human expertise and comprehensible solutions. In this
work, we present an anomaly detection approach specifically designed for
spectrograms. The approach is based on statistical evaluations and is
theoretically motivated. In addition, it features intrinsic explainability,
making it particularly suitable for applications in industrial settings. Thus,
this algorithm is of relevance for applications in which black-box algorithms
are unwanted or unsuitable.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [245] [CyGym: A Simulation-Based Game-Theoretic Analysis Framework for Cybersecurity](https://arxiv.org/abs/2506.21688)
*Michael Lanier,Yevgeniy Vorobeychik*

Main category: cs.CR

TL;DR: 介绍网络攻防模拟器，结合博弈论模型分析APT攻击，实验证明博弈策略有效。


<details>
  <summary>Details</summary>
Motivation: 设计能支持博弈论建模分析且保留真实网络防御特征的模拟器，以分析网络对APT和零日漏洞攻击的抵御能力。

Method: 在OpenAI Gym框架内构建模拟器，结合现实网络拓扑、漏洞等，提出基于模拟的博弈论模型和计算均衡的PSRO方法。

Result: 实验结果展示了博弈论策略在理解网络抵御APT和零日漏洞攻击能力方面的有效性。

Conclusion: 博弈论策略能为最优防御姿态和主动威胁缓解提供有价值的见解。

Abstract: We introduce a novel cybersecurity encounter simulator between a network
defender and an attacker designed to facilitate game-theoretic modeling and
analysis while maintaining many significant features of real cyber defense. Our
simulator, built within the OpenAI Gym framework, incorporates realistic
network topologies, vulnerabilities, exploits (including-zero-days), and
defensive mechanisms. Additionally, we provide a formal simulation-based
game-theoretic model of cyberdefense using this simulator, which features a
novel approach to modeling zero-days exploits, and a PSRO-style approach for
approximately computing equilibria in this game. We use our simulator and
associated game-theoretic framework to analyze the Volt Typhoon advanced
persistent threat (APT). Volt Typhoon represents a sophisticated cyber attack
strategy employed by state-sponsored actors, characterized by stealthy,
prolonged infiltration and exploitation of network vulnerabilities. Our
experimental results demonstrate the efficacy of game-theoretic strategies in
understanding network resilience against APTs and zero-days, such as Volt
Typhoon, providing valuable insight into optimal defensive posture and
proactive threat mitigation.

</details>


### [246] [Reliability Analysis of Smart Contract Execution Architectures: A Comparative Simulation Study](https://arxiv.org/abs/2506.22180)
*Önder Gürcan*

Main category: cs.CR

TL;DR: 本文针对工业市场保障自主系统安全需求，开发评估模型和案例研究，模拟评估安全漏洞，发现Execute - Order - Validate架构在可靠性和安全性上更有前景。


<details>
  <summary>Details</summary>
Motivation: 工业市场对保障自主系统的可靠安全解决方案需求增加，智能合约是有潜力的解决办法，但存在不可变及不同执行架构吞吐量不同等问题。

Method: 开发评估模型评估可靠智能合约执行的安全性；开展基于智能合约的物联网能源案例研究；模拟案例研究评估文献中报道的智能合约安全漏洞。

Result: Execute - Order - Validate架构在可靠性和安全性方面更有前景。

Conclusion: Execute - Order - Validate架构是保障智能合约执行可靠性和安全性更优的选择。

Abstract: The industrial market continuously needs reliable solutions to secure
autonomous systems. Especially as these systems become more complex and
interconnected, reliable security solutions are becoming increasingly
important. One promising solution to tackle this challenge is using smart
contracts designed to meet contractual conditions, avoid malicious errors,
secure exchanges, and minimize the need for reliable intermediaries. However,
smart contracts are immutable. Moreover, there are different smart contract
execution architectures (namely Order-Execute and Execute-Order-Validate) that
have different throughputs. In this study, we developed an evaluation model for
assessing the security of reliable smart contract execution. We then developed
a realistic smart contract enabled IoT energy case study. Finally, we simulate
the developed case study to evaluate several smart contract security
vulnerabilities reported in the literature. Our results show that the
Execute-Order-Validate architecture is more promising regarding reliability and
security.

</details>


### [247] [On the Feasibility of Poisoning Text-to-Image AI Models via Adversarial Mislabeling](https://arxiv.org/abs/2506.21874)
*Stanley Wu,Ronik Bhaskar,Anna Yoo Jeong Ha,Shawn Shan,Haitao Zheng,Ben Y. Zhao*

Main category: cs.CR

TL;DR: 本文探讨对视觉语言模型（VLMs）的对抗性错误标注攻击，以毒化文本到图像模型的训练管道，发现VLMs易受攻击，潜在防御可被绕过，且攻击在现实中有效。


<details>
  <summary>Details</summary>
Motivation: 鉴于VLMs易受隐形对抗攻击，探讨对其进行对抗性错误标注攻击以毒化文本到图像模型训练管道的可行性。

Method: 通过实验研究VLMs对对抗性扰动的脆弱性，测试潜在防御机制并观察自适应攻击者的应对。

Result: VLMs高度易受对抗性扰动影响，攻击者可注入“脏标签”毒样本改变文本到图像模型行为；潜在防御可被自适应攻击者绕过；攻击在黑盒场景下对商业VLMs成功率超73%。

Conclusion: 此类攻击会引发猫鼠游戏，降低训练数据质量，增加文本到图像模型开发成本。

Abstract: Today's text-to-image generative models are trained on millions of images
sourced from the Internet, each paired with a detailed caption produced by
Vision-Language Models (VLMs). This part of the training pipeline is critical
for supplying the models with large volumes of high-quality image-caption pairs
during training. However, recent work suggests that VLMs are vulnerable to
stealthy adversarial attacks, where adversarial perturbations are added to
images to mislead the VLMs into producing incorrect captions.
  In this paper, we explore the feasibility of adversarial mislabeling attacks
on VLMs as a mechanism to poisoning training pipelines for text-to-image
models. Our experiments demonstrate that VLMs are highly vulnerable to
adversarial perturbations, allowing attackers to produce benign-looking images
that are consistently miscaptioned by the VLM models. This has the effect of
injecting strong "dirty-label" poison samples into the training pipeline for
text-to-image models, successfully altering their behavior with a small number
of poisoned samples. We find that while potential defenses can be effective,
they can be targeted and circumvented by adaptive attackers. This suggests a
cat-and-mouse game that is likely to reduce the quality of training data and
increase the cost of text-to-image model development. Finally, we demonstrate
the real-world effectiveness of these attacks, achieving high attack success
(over 73%) even in black-box scenarios against commercial VLMs (Google Vertex
AI and Microsoft Azure).

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [248] [Inverse Design of Diffractive Metasurfaces Using Diffusion Models](https://arxiv.org/abs/2506.21748)
*Liav Hen,Erez Yosef,Dan Raviv,Raja Giryes,Jacob Scheuer*

Main category: physics.optics

TL;DR: 本文将扩散模型生成能力融入计算设计流程以解决超表面逆设计难题，训练模型生成低误差超表面，在两类分束器设计中验证，还公开代码和数据集。


<details>
  <summary>Details</summary>
Motivation: 超表面逆设计因结构与光学特性的复杂非线性关系存在挑战，传统方法需专家调整、易陷入局部最优且计算开销大。

Method: 用RCWA模拟器生成训练数据，训练条件扩散模型，通过RCWA引导后验采样或作为传统优化方法的初始化来生成超表面。

Result: 在空间均匀强度分束器和偏振分束器设计中，能在30分钟内以低误差生成超表面。

Conclusion: 将扩散模型用于超表面逆设计有效可行，公开代码和数据集利于数据驱动的超表面设计研究。

Abstract: Metasurfaces are ultra-thin optical elements composed of engineered
sub-wavelength structures that enable precise control of light. Their inverse
design - determining a geometry that yields a desired optical response - is
challenging due to the complex, nonlinear relationship between structure and
optical properties. This often requires expert tuning, is prone to local
minima, and involves significant computational overhead. In this work, we
address these challenges by integrating the generative capabilities of
diffusion models into computational design workflows. Using an RCWA simulator,
we generate training data consisting of metasurface geometries and their
corresponding far-field scattering patterns. We then train a conditional
diffusion model to predict meta-atom geometry and height from a target spatial
power distribution at a specified wavelength, sampled from a continuous
supported band. Once trained, the model can generate metasurfaces with low
error, either directly using RCWA-guided posterior sampling or by serving as an
initializer for traditional optimization methods. We demonstrate our approach
on the design of a spatially uniform intensity splitter and a polarization beam
splitter, both produced with low error in under 30 minutes. To support further
research in data-driven metasurface design, we publicly release our code and
datasets.

</details>


<div id='stat.OT'></div>

# stat.OT [[Back]](#toc)

### [249] [A Plea for History and Philosophy of Statistics and Machine Learning](https://arxiv.org/abs/2506.22236)
*Hanti Lin*

Main category: stat.OT

TL;DR: 文章指出统计史与哲学的整合需推进，鉴于统计与机器学习界限模糊，以机器学习哲学思想案例研究引出基础假设“可实现主义”，还提出方法论层面的整合。


<details>
  <summary>Details</summary>
Motivation: 统计史与哲学的整合缺乏持续跟进，且当下机器学习发展使统计与机器学习界限模糊，需要双重整合。

Method: 以机器学习中可追溯到Neyman和Pearson 1936年工作的哲学思想为例进行案例研究。

Result: 引出了被频率论统计和机器学习实践共享的基础假设“可实现主义”，以及方法论层面的整合。

Conclusion: 需要双重整合，即统计史与哲学的整合，以及统计与机器学习领域的整合。

Abstract: The integration of the history and philosophy of statistics was initiated at
least by Hacking (1965) and advanced by Mayo (1996), but it has not received
sustained follow-up. Yet such integration is more urgent than ever, as the
recent success of artificial intelligence has been driven largely by machine
learning -- a field historically developed alongside statistics. Today, the
boundary between statistics and machine learning is increasingly blurred. What
we now need is integration, twice over: of history and philosophy, and of the
field they engage -- statistics and machine learning. I present a case study of
a philosophical idea in machine learning (and in formal epistemology) whose
root can be traced back to an often under-appreciated insight in Neyman and
Pearson's 1936 work (a follow-up to their 1933 classic). This leads to the
articulation of a foundational assumption -- largely implicit in, but shared
by, the practices of frequentist statistics and machine learning -- which I
call achievabilism. Another integration also emerges at the level of
methodology, combining two ends of the philosophy of science spectrum: history
and philosophy of science on the one hand, and formal epistemology on the other
hand.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [250] [StructMG: A Fast and Scalable Structured Algebraic Multigrid](https://arxiv.org/abs/2506.21932)
*Yi Zong,Peinan Yu,Haopeng Huang,Zhengding Hu,Xinliang Wang,Qin Wang,Chensong Zhang,Xiaowen Xu,Jian Sun,Yongxiao Zhou,Wei Xue*

Main category: math.NA

TL;DR: 本文基于经典方法推导高效结构多重网格原则，设计实现StructMG，提出相关方法降低复杂度，在多平台验证其有效性，相比其他预处理器有显著速度提升和效率改进。


<details>
  <summary>Details</summary>
Motivation: 现有多重网格库在处理结构化网格问题时，速度和可扩展性表现欠佳，需要提升性能。

Method: 基于经典“多重网格跷跷板”推导原则，设计实现StructMG；提出基于模板的三重矩阵乘积用于Galerkin粗化；给出稀疏三角求解器统一并行框架。

Result: 在ARM和X86平台上对多种问题评估，StructMG在所有情况下求解时间最短，相比其他预处理器有不同程度的加速比，还显著提高了强、弱缩放效率。

Conclusion: StructMG是一种快速、可扩展的代数多重网格，作为预处理器在并行求解大规模线性系统时具有良好性能。

Abstract: Parallel multigrid is widely used as preconditioners in solving large-scale
sparse linear systems. However, the current multigrid library still needs more
satisfactory performance for structured grid problems regarding speed and
scalability. Based on the classical 'multigrid seesaw', we derive three
necessary principles for an efficient structured multigrid, which instructs our
design and implementation of StructMG, a fast and scalable algebraic multigrid
that constructs hierarchical grids automatically. As a preconditioner, StructMG
can achieve both low cost per iteration and good convergence when solving
large-scale linear systems with iterative methods in parallel. A stencil-based
triple-matrix product via symbolic derivation and code generation is proposed
for multi-dimensional Galerkin coarsening to reduce grid complexity, operator
complexity, and implementation effort. A unified parallel framework of sparse
triangular solver is presented to achieve fast convergence and high parallel
efficiency for smoothers, including dependence-preserving Gauss-Seidel and
incomplete LU methods. Idealized and real-world problems from radiation
hydrodynamics, petroleum reservoir simulation, numerical weather prediction,
and solid mechanics, are evaluated on ARM and X86 platforms to show StructMG's
effectiveness. In comparison to \textit{hypre}'s structured and general
multigrid preconditioners, StructMG achieves the fastest time-to-solutions in
all cases with average speedups of 15.5x, 5.5x, 6.7x, 7.3x over SMG, PFMG,
SysPFMG, and BoomerAMG, respectively. StructMG also significantly improves
strong and weak scaling efficiencies.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [251] [The Missing Link: Joint Legal Citation Prediction using Heterogeneous Graph Enrichment](https://arxiv.org/abs/2506.22165)
*Lorenz Wendlinger,Simon Alexander Nonn,Abdullah Al Zubaer,Michael Granitzer*

Main category: cs.SI

TL;DR: 提出GNN链接预测模型识别法律引用，结合语义和拓扑信息提升预测效果，联合学习预测有协同效应。


<details>
  <summary>Details</summary>
Motivation: 法律系统依赖法律规范和法院判决的交叉引用，从业者、新手和法律AI系统需获取相关数据进行评估和判断。

Method: 提出GNN链接预测模型，引入适应的关系图卷积操作，在扩展和丰富的引用图上融合语义和拓扑信息，联合学习和预测案例与规范引用。

Result: 改进预测，平均精度提高3.1点，数据稀疏性提高8.5点，在全归纳预测中表现稳健，联合学习预测使案例引用预测提高达4.7点，效率接近翻倍。

Conclusion: 所提模型能有效识别法律引用，联合学习预测有显著协同效果。

Abstract: Legal systems heavily rely on cross-citations of legal norms as well as
previous court decisions. Practitioners, novices and legal AI systems need
access to these relevant data to inform appraisals and judgments. We propose a
Graph-Neural-Network (GNN) link prediction model that can identify Case-Law and
Case-Case citations with high proficiency through fusion of semantic and
topological information. We introduce adapted relational graph convolutions
operating on an extended and enriched version of the original citation graph
that allow the topological integration of semantic meta-information. This
further improves prediction by 3.1 points of average precision and by 8.5
points in data sparsity as well as showing robust performance over time and in
challenging fully inductive prediction. Jointly learning and predicting case
and norm citations achieves a large synergistic effect that improves case
citation prediction by up to 4.7 points, at almost doubled efficiency.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [252] [Universal Modelling of Autocovariance Functions via Spline Kernels](https://arxiv.org/abs/2506.21953)
*Lachlan Astfalck*

Main category: stat.ME

TL;DR: 本文提出一种灵活的非参数自协方差函数（ACF）闭形式类，具有多项优点，通过理论和实证证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现代应用对ACF建模灵活性要求超经典参数模型，现有谱域非参数方法难给出ACF闭形式表达式。

Method: 推导任意阶数和节点位置的B样条谱基的逆傅里叶变换，构建非参数ACF类。

Result: 该ACF类在特定条件下具有稠密性，适用于多种过程，支持非可分结构，有收敛率，实证能准确恢复过程。

Conclusion: 该方法为构建非参数ACF类提供实用且理论可靠的途径。

Abstract: Flexible modelling of the autocovariance function (ACF) is central to
time-series, spatial, and spatio-temporal analysis. Modern applications often
demand flexibility beyond classical parametric models, motivating
non-parametric descriptions of the ACF. Bochner's Theorem guarantees that any
positive spectral measure yields a valid ACF via the inverse Fourier transform;
however, existing non-parametric approaches in the spectral domain rarely
return closed-form expressions for the ACF itself. We develop a flexible,
closed-form class of non-parametric ACFs by deriving the inverse Fourier
transform of B-spline spectral bases with arbitrary degree and knot placement.
This yields a general class of ACF with three key features: (i) it is provably
dense, under an $L^1$ metric, in the space of weakly stationary, mean-square
continuous ACFs with mild regularity conditions; (ii) it accommodates
univariate, multivariate, and multidimensional processes; and (iii) it
naturally supports non-separable structure without requiring explicit
imposition. Jackson-type approximation bounds establish convergence rates, and
empirical results on simulated and real-world data demonstrate accurate process
recovery. The method provides a practical and theoretically grounded approach
for constructing a non-parametric class of ACF.

</details>


### [253] [Using Large Language Models to Suggest Informative Prior Distributions in Bayesian Statistics](https://arxiv.org/abs/2506.21964)
*Michael A. Riegler,Kristoffer Herland Hellton,Vajira Thambawita,Hugo L. Hammer*

Main category: stat.ME

TL;DR: 本文探讨用大语言模型（LLMs）为贝叶斯统计提供先验分布，评估了Claude Opus、Gemini 2.5 Pro和ChatGPT - 4o - mini，发现LLMs有潜力但校准先验分布宽度是挑战。


<details>
  <summary>Details</summary>
Motivation: 解决贝叶斯统计中选择先验分布具有挑战性、资源密集且主观的问题。

Method: 开发广泛提示让LLMs建议先验分布并验证反思，在两个真实数据集上评估三种LLMs，用Kullback - Leibler散度衡量先验质量。

Result: 所有LLMs能正确识别关联方向；中等信息先验常过度自信；Claude和Gemini提供的先验优于ChatGPT；弱信息先验方面，Claude表现更优。

Conclusion: LLMs有作为开发信息先验高效客观方法的潜力，但校准先验分布宽度以避免过度和信心不足仍是主要挑战。

Abstract: Selecting prior distributions in Bayesian statistics is challenging,
resource-intensive, and subjective. We analyze using large-language models
(LLMs) to suggest suitable, knowledge-based informative priors. We developed an
extensive prompt asking LLMs not only to suggest priors but also to verify and
reflect on their choices.
  We evaluated Claude Opus, Gemini 2.5 Pro, and ChatGPT-4o-mini on two real
datasets: heart disease risk and concrete strength. All LLMs correctly
identified the direction for all associations (e.g., that heart disease risk is
higher for males). The quality of suggested priors was measured by their
Kullback-Leibler divergence from the maximum likelihood estimator's
distribution.
  The LLMs suggested both moderately and weakly informative priors. The
moderate priors were often overconfident, resulting in distributions misaligned
with the data. In our experiments, Claude and Gemini provided better priors
than ChatGPT. For weakly informative priors, a key performance difference
emerged: ChatGPT and Gemini defaulted to an "unnecessarily vague" mean of 0,
while Claude did not, demonstrating a significant advantage.
  The ability of LLMs to identify correct associations shows their great
potential as an efficient, objective method for developing informative priors.
However, the primary challenge remains in calibrating the width of these priors
to avoid over- and under-confidence.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [254] [Hitchhiking Rides Dataset: Two decades of crowd-sourced records on stochastic traveling](https://arxiv.org/abs/2506.21946)
*Till Wenke*

Main category: cs.CY

TL;DR: 本文分析最大的搭便车出行结构化数据集，虽有局限但提供新视角并指出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 搭便车出行的非正规性使其缺乏系统研究，需借助数据集深入了解。

Method: 利用众包贡献收集数据集，进行探索性分析。

Result: 发现数据集以欧洲为主、有季节性模式，依赖少量活跃贡献者，分析揭示搭车者等待时间、行为等情况。

Conclusion: 数据集有固有偏差和局限，但为搭便车出行研究提供有价值视角，需丰富数据集并推进相关研究。

Abstract: Hitchhiking, a spontaneous and decentralized mode of travel, has long eluded
systematic study due to its informal nature. This paper presents and analyzes
the largest known structured dataset of hitchhiking rides, comprising over
63,000 entries collected over nearly two decades through platforms associated
with hitchwiki.org and lately on hitchmap.com. By leveraging crowd-sourced
contributions, the dataset captures key spatiotemporal and strategic aspects of
hitchhiking. This work documents the dataset's origins, evolution, and
community-driven maintenance, highlighting its Europe-centric distribution,
seasonal patterns, and reliance on a small number of highly active
contributors. Through exploratory analyses, I examine waiting times, user
behavior, and comment metadata, shedding light on the lived realities of
hitchhikers. While the dataset has inherent biases and limitations - such as
demographic skew and unverifiable entries it offers a rare and valuable window
into an alternative form of mobility. I conclude by outlining future directions
for enriching the dataset and advancing research on hitchhiking as both a
transportation practice and cultural phenomenon.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [255] [Monetary Macro Accounting Theory](https://arxiv.org/abs/2506.21651)
*Renéee Menéndez,Viktor Winschel*

Main category: econ.GN

TL;DR: 本文提出货币宏观会计理论（MoMaT）及其软件规范，用于一致的国民会计，介绍了该理论的货币功能、运作层次、关键要素等，并提及应用和理论支撑。


<details>
  <summary>Details</summary>
Motivation: 开发用于一致国民会计的货币宏观会计理论。

Method: 运用分离和抽象的法律原则建模债务、合同等；以汇票作为统一合同工具；借助范畴论、层论和同调论保证数学严谨性，用开放博弈构建宏观经济分析。

Result: 提出的MoMaT可用于区块链经济理论、货币政策和供应链金融，且多层面汇票框架可通过区块链智能合约和AI自动化实施以提高借贷透明度。

Conclusion: MoMaT具有广泛适用性，能为相关经济领域提供理论和实践支持。

Abstract: We develop a monetary macro accounting theory (MoMaT) and its software
specification for a consistent national accounting. In our money theory money
functions primarily as a medium of payment for obligations and debts, not as a
medium of exchange, originating from the temporal misalignment where producers
pay suppliers before receiving revenue. MoMaT applies the legal principles of
Separation and Abstraction to model debt, contracts, property rights, and money
to understand their nature. Monetary systems according to our approach operate
at three interconnected levels: micro (division of labor), meso (banking for
risk-sharing), and macro (GDP sharing, money issuance). Critical to money
theory are macro debt relations, hence the model focuses not on the circulation
of money but on debt vortices: the ongoing creation and resolution of financial
obligations. The Bill of Exchange (BoE) acts as a unifying contractual
instrument, linking debt processes and monetary issuance across fiat and
gold-based systems. A multi-level BoE framework enables liquidity exchange,
investments, and endorsements, designed for potential implementation in
blockchain smart contracts and AI automation to improve borrowing transparency.
Mathematical rigor can be ensured through category theory and sheaf theory for
invariances between economic levels and homology theory for monetary policy
foundations. Open Games can structure macroeconomic analysis with multi-agent
models, making MoMaT applicable to blockchain economic theory, monetary policy,
and supply chain finance.

</details>
