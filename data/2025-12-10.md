<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 37]
- [cs.CE](#cs.CE) [Total: 4]
- [cs.DB](#cs.DB) [Total: 3]
- [cs.DC](#cs.DC) [Total: 9]
- [cs.DS](#cs.DS) [Total: 7]
- [cs.GT](#cs.GT) [Total: 5]
- [cs.IR](#cs.IR) [Total: 7]
- [cs.LG](#cs.LG) [Total: 88]
- [cs.NE](#cs.NE) [Total: 1]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.SE](#cs.SE) [Total: 19]
- [q-fin.RM](#q-fin.RM) [Total: 2]
- [q-fin.ST](#q-fin.ST) [Total: 4]
- [stat.ML](#stat.ML) [Total: 4]
- [stat.CO](#stat.CO) [Total: 2]
- [cs.CL](#cs.CL) [Total: 10]
- [cs.SD](#cs.SD) [Total: 3]
- [q-bio.NC](#q-bio.NC) [Total: 2]
- [physics.data-an](#physics.data-an) [Total: 1]
- [quant-ph](#quant-ph) [Total: 2]
- [cs.NI](#cs.NI) [Total: 2]
- [physics.bio-ph](#physics.bio-ph) [Total: 1]
- [cs.ET](#cs.ET) [Total: 1]
- [stat.ME](#stat.ME) [Total: 4]
- [cs.CY](#cs.CY) [Total: 1]
- [econ.GN](#econ.GN) [Total: 2]
- [cs.IT](#cs.IT) [Total: 1]
- [astro-ph.SR](#astro-ph.SR) [Total: 1]
- [cs.HC](#cs.HC) [Total: 2]
- [cs.CV](#cs.CV) [Total: 31]
- [eess.SY](#eess.SY) [Total: 6]
- [physics.med-ph](#physics.med-ph) [Total: 2]
- [q-bio.PE](#q-bio.PE) [Total: 1]
- [cs.MA](#cs.MA) [Total: 3]
- [cs.SI](#cs.SI) [Total: 1]
- [eess.IV](#eess.IV) [Total: 2]
- [cs.CR](#cs.CR) [Total: 10]
- [hep-th](#hep-th) [Total: 1]
- [cs.RO](#cs.RO) [Total: 11]
- [econ.EM](#econ.EM) [Total: 2]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [math.NA](#math.NA) [Total: 2]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Impact of Data-Oriented and Object-Oriented Design on Performance and Cache Utilization with Artificial Intelligence Algorithms in Multi-Threaded CPUs](https://arxiv.org/abs/2512.07841)
*Gabriel M. Arantes,Richard F. Pinto,Bruno L. Dalmazo,Eduardo N. Borges,Giancarlo Lucca,Viviane L. D. de Mattos,Fabian C. Cardoso,Rafael A. Berri*

Main category: cs.AI

TL;DR: 研究对比面向数据设计（DOD）和面向对象设计（OOD）在多核CPU和内存性能差距下的性能，发现DOD在多线程数据密集操作中更优，单线程版本在A*算法中表现更好，DOD架构优势明显。


<details>
  <summary>Details</summary>
Motivation: 多核CPU与主存性能差距增大，需要硬件感知的软件设计范式，对比DOD和OOD在多线程环境下的缓存利用和效率。

Method: 开发并对比A*搜索算法的单线程OOD、单线程DOD、多线程OOD和多线程DOD四个版本，基于执行时间、内存使用和CPU缓存未命中数等指标评估。

Result: 多线程测试中DOD有显著性能提升，执行更快、系统调用和缓存未命中数更少；OOD在内存使用或缓存未命中率百分比上偶有小优势；单线程版本在A*算法中表现优于多线程版本。

Conclusion: 即使在简单算法中性能差异不明显，DOD在关键指标上的持续优势体现其架构优越性，是复杂大规模AI和并行计算任务中提高硬件效率的更有效方法。

Abstract: The growing performance gap between multi-core CPUs and main memory necessitates hardware-aware software design paradigms. This study provides a comprehensive performance analysis of Data Oriented Design (DOD) versus the traditional Object-Oriented Design (OOD), focusing on cache utilization and efficiency in multi-threaded environments. We developed and compared four distinct versions of the A* search algorithm: single-threaded OOD (ST-OOD), single-threaded DOD (ST-DOD), multi-threaded OOD (MT-OOD), and multi-threaded DOD (MT-DOD). The evaluation was based on metrics including execution time, memory usage, and CPU cache misses. In multi-threaded tests, the DOD implementation demonstrated considerable performance gains, with faster execution times and a lower number of raw system calls and cache misses. While OOD occasionally showed marginal advantages in memory usage or percentage-based cache miss rates, DOD's efficiency in data-intensive operations was more evident. Furthermore, our findings reveal that for a fine-grained task like the A* algorithm, the overhead associated with thread management led to single-threaded versions significantly outperforming their multi-threaded counterparts in both paradigms. We conclude that even when performance differences appear subtle in simple algorithms, the consistent advantages of DOD in critical metrics highlight its foundational architectural superiority, suggesting it is a more effective approach for maximizing hardware efficiency in complex, large-scale AI and parallel computing tasks.

</details>


### [2] [Can AI autonomously build, operate, and use the entire data stack?](https://arxiv.org/abs/2512.07926)
*Arvind Agarwal,Lisa Amini,Sameep Mehta,Horst Samulowitz,Kavitha Srinivas*

Main category: cs.AI

TL;DR: 文章探讨企业数据管理，主张从AI独立应用转向全生命周期自主处理，探索智能代理自主管理数据栈各阶段。


<details>
  <summary>Details</summary>
Motivation: 当前AI辅助数据管理未达全自动化，且AI能力提升，存在实现全自主数据管理的机会，需进行范式转变。

Method: 描述推动范式转变的因素，研究智能代理如何简化数据生命周期，指出待研究问题。

Result: 无明确具体结果。

Conclusion: 希望激发讨论、促进研究与合作，推动数据系统更自主的未来。

Abstract: Enterprise data management is a monumental task. It spans data architecture and systems, integration, quality, governance, and continuous improvement. While AI assistants can help specific persona, such as data engineers and stewards, to navigate and configure the data stack, they fall far short of full automation. However, as AI becomes increasingly capable of tackling tasks that have previously resisted automation due to inherent complexities, we believe there is an imminent opportunity to target fully autonomous data estates. Currently, AI is used in different parts of the data stack, but in this paper, we argue for a paradigm shift from the use of AI in independent data component operations towards a more holistic and autonomous handling of the entire data lifecycle. Towards that end, we explore how each stage of the modern data stack can be autonomously managed by intelligent agents to build self-sufficient systems that can be used not only by human end-users, but also by AI itself. We begin by describing the mounting forces and opportunities that demand this paradigm shift, examine how agents can streamline the data lifecycle, and highlight open questions and areas where additional research is needed. We hope this work will inspire lively debate, stimulate further research, motivate collaborative approaches, and facilitate a more autonomous future for data systems.

</details>


### [3] [SkipKV: Selective Skipping of KV Generation and Storage for Efficient Inference with Large Reasoning Models](https://arxiv.org/abs/2512.07993)
*Jiayi Tian,Seyedarmin Azizi,Yequan Zhao,Erfan Baghaei Potraghloo,Sean McPherson,Sharath Nittur Sridhar,Zhengyang Wang,Zheng Zhang,Massoud Pedram,Souvik Kundu*

Main category: cs.AI

TL;DR: 论文指出大推理模型KV缓存开销大，现有方法有问题，提出SkipKV方法，实验证明其在准确性、生成长度和吞吐量上有优势。


<details>
  <summary>Details</summary>
Motivation: 大推理模型在CoT推理过程中KV缓存开销大，限制了高效部署，需减少推理时的KV缓存大小。

Method: 提出SkipKV，一种无训练的KV压缩方法，在粗粒度句子级进行选择性驱逐和生成，引入句子评分指标，动态调整转向向量。

Result: 在多个推理基准测试中，与其他方法相比，SkipKV能提高26.7%的准确率，生成长度最多减少1.6倍，吞吐量最多提高1.7倍。

Conclusion: SkipKV方法在减少KV缓存大小的同时，能有效提高推理性能。

Abstract: Large reasoning models (LRMs) often cost significant key-value (KV) cache overhead, due to their linear growth with the verbose chain-of-thought (CoT) reasoning process. This costs both memory and throughput bottleneck limiting their efficient deployment. Towards reducing KV cache size during inference, we first investigate the effectiveness of existing KV cache eviction methods for CoT reasoning. Interestingly, we find that due to unstable token-wise scoring and the reduced effective KV budget caused by padding tokens, state-of-the-art (SoTA) eviction methods fail to maintain accuracy in the multi-batch setting. Additionally, these methods often generate longer sequences than the original model, as semantic-unaware token-wise eviction leads to repeated revalidation during reasoning. To address these issues, we present \textbf{SkipKV}, a \textbf{\textit{training-free}} KV compression method for selective \textit{eviction} and \textit{generation} operating at a coarse-grained sentence-level sequence removal for efficient CoT reasoning. In specific, it introduces a \textit{sentence-scoring metric} to identify and remove highly similar sentences while maintaining semantic coherence. To suppress redundant generation, SkipKV dynamically adjusts a steering vector to update the hidden activation states during inference enforcing the LRM to generate concise response. Extensive evaluations on multiple reasoning benchmarks demonstrate the effectiveness of SkipKV in maintaining up to $\mathbf{26.7}\%$ improved accuracy compared to the alternatives, at a similar compression budget. Additionally, compared to SoTA, SkipKV yields up to $\mathbf{1.6}\times$ fewer generation length while improving throughput up to $\mathbf{1.7}\times$.

</details>


### [4] [Toward an AI Reasoning-Enabled System for Patient-Clinical Trial Matching](https://arxiv.org/abs/2512.08026)
*Caroline N. Leach,Mitchell A. Klusty,Samuel E. Armstrong,Justine C. Pickarski,Kristen L. Hankins,Emily B. Collier,Maya Shah,Aaron D. Mullen,V. K. Cody Bumgardner*

Main category: cs.AI

TL;DR: 提出AI增强患者-试验匹配概念验证系统，解决关键挑战，生成结构化评估并提供建议，减少协调员负担。


<details>
  <summary>Details</summary>
Motivation: 当前临床试验患者筛选是手动、耗时且资源密集型过程，需改进。

Method: 利用支持推理的开源大语言模型，超越二元分类，生成带可解释推理链的结构化评估。

Result: 将患者资格表示为动态状态，识别匹配项并提供未来使患者符合条件的可操作建议。

Conclusion: 系统可减少协调员负担，扩大考虑的试验集，保证AI输出可审计性。

Abstract: Screening patients for clinical trial eligibility remains a manual, time-consuming, and resource-intensive process. We present a secure, scalable proof-of-concept system for Artificial Intelligence (AI)-augmented patient-trial matching that addresses key implementation challenges: integrating heterogeneous electronic health record (EHR) data, facilitating expert review, and maintaining rigorous security standards. Leveraging open-source, reasoning-enabled large language models (LLMs), the system moves beyond binary classification to generate structured eligibility assessments with interpretable reasoning chains that support human-in-the-loop review. This decision support tool represents eligibility as a dynamic state rather than a fixed determination, identifying matches when available and offering actionable recommendations that could render a patient eligible in the future. The system aims to reduce coordinator burden, intelligently broaden the set of trials considered for each patient and guarantee comprehensive auditability of all AI-generated outputs.

</details>


### [5] [Large Language Models for Education and Research: An Empirical and User Survey-based Analysis](https://arxiv.org/abs/2512.08057)
*Md Mostafizer Rahman,Ariful Islam Shiplu,Md Faizul Ibne Amin,Yutaka Watanobe,Lu Peng*

Main category: cs.AI

TL;DR: 对ChatGPT和DeepSeek两个大语言模型在教育和研究领域进行综合评估，得出两者在不同任务的表现及作用。


<details>
  <summary>Details</summary>
Motivation: 预训练大语言模型在教育和研究领域影响显著，需评估ChatGPT和DeepSeek在该领域的表现。

Method: 通过背景技术分析、实证实验和真实用户调查进行评估，对模型在文本生成、编程和专业问题解决等方面进行基准测试。

Result: ChatGPT在通用语言理解和文本生成方面出色，DeepSeek在编程任务中表现优越，两者在医学诊断和数学问题解决上表现良好；用户调查揭示了模型的实际利弊。

Conclusion: 评估为理解两模型在教育和研究领域的作用提供了深入见解。

Abstract: Pretrained Large Language Models (LLMs) have achieved remarkable success across diverse domains, with education and research emerging as particularly impactful areas. Among current state-of-the-art LLMs, ChatGPT and DeepSeek exhibit strong capabilities in mathematics, science, medicine, literature, and programming. In this study, we present a comprehensive evaluation of these two LLMs through background technology analysis, empirical experiments, and a real-world user survey. The evaluation explores trade-offs among model accuracy, computational efficiency, and user experience in educational and research affairs. We benchmarked these LLMs performance in text generation, programming, and specialized problem-solving. Experimental results show that ChatGPT excels in general language understanding and text generation, while DeepSeek demonstrates superior performance in programming tasks due to its efficiency-focused design. Moreover, both models deliver medically accurate diagnostic outputs and effectively solve complex mathematical problems. Complementing these quantitative findings, a survey of students, educators, and researchers highlights the practical benefits and limitations of these models, offering deeper insights into their role in advancing education and research.

</details>


### [6] [Scalable Back-End for an AI-Based Diabetes Prediction Application](https://arxiv.org/abs/2512.08147)
*Henry Anand Septian Radityo,Bernardus Willson,Reynard Tanadi,Latifa Dwiyanti,Saiful Akbar*

Main category: cs.AI

TL;DR: 本文开发并评估了用于移动糖尿病预测应用的可扩展后端系统，多数功能达性能目标，能处理大量并发用户。


<details>
  <summary>Details</summary>
Motivation: 全球糖尿病患病率上升，需早期检测，AI预测应用需响应式和可扩展后端架构服务大量用户。

Method: 采用水平扩展、数据库分片和通过消息队列进行异步通信的架构。

Result: 83%的系统功能达性能目标，能处理10000个并发用户，异步通信降低计算密集型预测请求错误率。

Conclusion: 该后端系统具有可扩展性和可靠性，能有效服务于移动糖尿病预测应用。

Abstract: The rising global prevalence of diabetes necessitates early detection to prevent severe complications. While AI-powered prediction applications offer a promising solution, they require a responsive and scalable back-end architecture to serve a large user base effectively. This paper details the development and evaluation of a scalable back-end system designed for a mobile diabetes prediction application. The primary objective was to maintain a failure rate below 5% and an average latency of under 1000 ms. The architecture leverages horizontal scaling, database sharding, and asynchronous communication via a message queue. Performance evaluation showed that 83% of the system's features (20 out of 24) met the specified performance targets. Key functionalities such as user profile management, activity tracking, and read-intensive prediction operations successfully achieved the desired performance. The system demonstrated the ability to handle up to 10,000 concurrent users without issues, validating its scalability. The implementation of asynchronous communication using RabbitMQ proved crucial in minimizing the error rate for computationally intensive prediction requests, ensuring system reliability by queuing requests and preventing data loss under heavy load.

</details>


### [7] [Empowerment Gain and Causal Model Construction: Children and adults are sensitive to controllability and variability in their causal interventions](https://arxiv.org/abs/2512.08230)
*Eunice Yiu,Kelsey Allen,Shiry Ginosar,Alison Gopnik*

Main category: cs.AI

TL;DR: 因果学习对大模型有挑战，“赋能”或为经典贝叶斯因果学习与强化学习桥梁，且能表征人类因果学习，研究还测试人类利用“赋能”线索推理因果关系。


<details>
  <summary>Details</summary>
Motivation: 解决大模型用标准深度学习技术进行因果学习的难题，探索人类与机器因果学习的有效途径。

Method: 基于形式因果理解理论，描述“赋能”概念，开展实证研究测试儿童与成人利用“赋能”线索推理因果和设计干预。

Result: 若智能体学习到准确因果世界模型会增加“赋能”，增加“赋能”会带来更准确模型。

Conclusion: “赋能”是因果学习重要桥梁，能表征人类因果学习，还可解释儿童因果学习特点。

Abstract: Learning about the causal structure of the world is a fundamental problem for human cognition. Causal models and especially causal learning have proved to be difficult for large pretrained models using standard techniques of deep learning. In contrast, cognitive scientists have applied advances in our formal understanding of causation in computer science, particularly within the Causal Bayes Net formalism, to understand human causal learning. In the very different tradition of reinforcement learning, researchers have described an intrinsic reward signal called "empowerment" which maximizes mutual information between actions and their outcomes. "Empowerment" may be an important bridge between classical Bayesian causal learning and reinforcement learning and may help to characterize causal learning in humans and enable it in machines. If an agent learns an accurate causal world model, they will necessarily increase their empowerment, and increasing empowerment will lead to a more accurate causal world model. Empowerment may also explain distinctive features of childrens causal learning, as well as providing a more tractable computational account of how that learning is possible. In an empirical study, we systematically test how children and adults use cues to empowerment to infer causal relations, and design effective causal interventions.

</details>


### [8] [Beyond Traditional Diagnostics: Transforming Patient-Side Information into Predictive Insights with Knowledge Graphs and Prototypes](https://arxiv.org/abs/2512.08261)
*Yibowen Zhao,Yinan Zhang,Zhixiang Su,Lizhen Cui,Chunyan Miao*

Main category: cs.AI

TL;DR: 提出KPI框架解决现有疾病预测方法问题，实验显示其性能优且有实用价值。


<details>
  <summary>Details</summary>
Motivation: 现有疾病预测方法存在疾病分布不平衡和缺乏可解释性问题，影响预测准确性和可靠性。

Method: 提出KPI框架，整合医学知识构建疾病知识图，构建疾病原型，用对比学习提升准确性，用大语言模型生成解释。

Result: 在真实数据集上实验表明，KPI在预测准确性上优于现有方法，能提供与患者描述相符的临床有效解释。

Conclusion: KPI框架有实际价值，可用于以患者为中心的医疗服务。

Abstract: Predicting diseases solely from patient-side information, such as demographics and self-reported symptoms, has attracted significant research attention due to its potential to enhance patient awareness, facilitate early healthcare engagement, and improve healthcare system efficiency. However, existing approaches encounter critical challenges, including imbalanced disease distributions and a lack of interpretability, resulting in biased or unreliable predictions. To address these issues, we propose the Knowledge graph-enhanced, Prototype-aware, and Interpretable (KPI) framework. KPI systematically integrates structured and trusted medical knowledge into a unified disease knowledge graph, constructs clinically meaningful disease prototypes, and employs contrastive learning to enhance predictive accuracy, which is particularly important for long-tailed diseases. Additionally, KPI utilizes large language models (LLMs) to generate patient-specific, medically relevant explanations, thereby improving interpretability and reliability. Extensive experiments on real-world datasets demonstrate that KPI outperforms state-of-the-art methods in predictive accuracy and provides clinically valid explanations that closely align with patient narratives, highlighting its practical value for patient-centered healthcare delivery.

</details>


### [9] [Reasoning Models Ace the CFA Exams](https://arxiv.org/abs/2512.08270)
*Jaisal Patel,Yunzhe Chen,Kaiwen He,Keyi Wang,David Li,Kairong Xiao,Xiao-Yang Liu*

Main category: cs.AI

TL;DR: 评估推理模型在模拟CFA考试中的表现，多数模型通过三个级别，部分模型成绩优异。


<details>
  <summary>Details</summary>
Motivation: 先前研究表明大语言模型在CFA考试中表现不佳，而近期推理模型在多学科考试中成果显著，因此评估其在CFA考试中的表现。

Method: 对涵盖三个一级、两个二级和三个三级考试共980道题的模拟CFA考试，使用与先前研究相同的通过/不通过标准评估模型。

Result: 多数模型通过所有三个级别，表现较好的模型有Gemini 3.0 Pro、Gemini 2.5 Pro等，且各模型在不同级别考试中有优异成绩。

Conclusion: 先进推理模型在CFA考试中表现出色。

Abstract: Previous research has reported that large language models (LLMs) demonstrate poor performance on the Chartered Financial Analyst (CFA) exams. However, recent reasoning models have achieved strong results on graduate-level academic and professional examinations across various disciplines. In this paper, we evaluate state-of-the-art reasoning models on a set of mock CFA exams consisting of 980 questions across three Level I exams, two Level II exams, and three Level III exams. Using the same pass/fail criteria from prior studies, we find that most models clear all three levels. The models that pass, ordered by overall performance, are Gemini 3.0 Pro, Gemini 2.5 Pro, GPT-5, Grok 4, Claude Opus 4.1, and DeepSeek-V3.1. Specifically, Gemini 3.0 Pro achieves a record score of 97.6% on Level I. Performance is also strong on Level II, led by GPT-5 at 94.3%. On Level III, Gemini 2.5 Pro attains the highest score with 86.4% on multiple-choice questions while Gemini 3.0 Pro achieves 92.0% on constructed-response questions.

</details>


### [10] [AgentEval: Generative Agents as Reliable Proxies for Human Evaluation of AI-Generated Content](https://arxiv.org/abs/2512.08273)
*Thanh Vu,Richi Nayak,Thiru Balasubramaniam*

Main category: cs.AI

TL;DR: 现代企业在高质量内容生成与评估上耗时费钱，研究引入生成式智能体解决此问题，能提升内容生成和评估效率。


<details>
  <summary>Details</summary>
Motivation: 现代企业面临高质量内容生成与评估的时间和费用挑战，传统方法有局限，需高效自动解决方案。

Method: 引入生成式智能体来快速且低成本地评估AI生成内容，模拟人类判断从多方面进行评分。

Result: 企业可简化内容生成流程，确保输出内容质量稳定，减少对高成本人工评估的依赖。

Conclusion: 研究为增强大语言模型生成高质量商业适配内容提供关键见解，推动自动化内容生成和评估发展。

Abstract: Modern businesses are increasingly challenged by the time and expense required to generate and assess high-quality content. Human writers face time constraints, and extrinsic evaluations can be costly. While Large Language Models (LLMs) offer potential in content creation, concerns about the quality of AI-generated content persist. Traditional evaluation methods, like human surveys, further add operational costs, highlighting the need for efficient, automated solutions. This research introduces Generative Agents as a means to tackle these challenges. These agents can rapidly and cost-effectively evaluate AI-generated content, simulating human judgment by rating aspects such as coherence, interestingness, clarity, fairness, and relevance. By incorporating these agents, businesses can streamline content generation and ensure consistent, high-quality output while minimizing reliance on costly human evaluations. The study provides critical insights into enhancing LLMs for producing business-aligned, high-quality content, offering significant advancements in automated content generation and evaluation.

</details>


### [11] [Towards a Science of Scaling Agent Systems](https://arxiv.org/abs/2512.08296)
*Yubin Kim,Ken Gu,Chanwoo Park,Chunjong Park,Samuel Schmidgall,A. Ali Heydari,Yao Yan,Zhihan Zhang,Yuchen Zhuang,Mark Malhotra,Paul Pu Liang,Hae Won Park,Yuzhe Yang,Xuhai Xu,Yilun Du,Shwetak Patel,Tim Althoff,Daniel McDuff,Xin Liu*

Main category: cs.AI

TL;DR: 本文推导了代理系统的定量缩放原则，通过多基准和架构评估，得出预测模型，发现三种主要效应，能为多数配置预测最优协调策略。


<details>
  <summary>Details</summary>
Motivation: 当前代理系统性能决定原则未充分探索，从业者依赖启发式方法，缺乏有原则的设计选择。

Method: 在四个多样化基准上，采用五种经典架构和三个大语言模型家族，进行180种配置的受控评估，基于经验协调指标推导预测模型。

Result: 预测模型交叉验证R²=0.513，识别出工具协调权衡、能力饱和、拓扑依赖的误差放大三种效应，不同协调方式在不同任务上表现不同，框架能为87%的保留配置预测最优协调策略。

Conclusion: 提供了基于可测量任务属性的代理缩放预测原则。

Abstract: Agents, language model (LM)-based systems that are capable of reasoning, planning, and acting are becoming the dominant paradigm for real-world AI applications. Despite this widespread adoption, the principles that determine their performance remain underexplored, leaving practitioners to rely on heuristics rather than principled design choices. We address this gap by deriving quantitative scaling principles for agent systems. We evaluate this across four diverse benchmarks: Finance-Agent, BrowseComp-Plus, PlanCraft, and Workbench. Using five canonical architectures (Single, Independent, Centralized, Decentralized, Hybrid) instantiated across three LLM families, we perform a controlled evaluation spanning 180 configurations with standardized tools and token budgets. We derive a predictive model using empirical coordination metrics, including efficiency, overhead, error amplification, and redundancy, that achieves cross-validated R^2=0.513. We identify three dominant effects: (1) a tool-coordination trade-off: under fixed computational budgets, tool-heavy tasks suffer disproportionately from multi-agent overhead. (2) a capability saturation: coordination yields diminishing or negative returns (beta=-0.408, p<0.001) once single-agent baselines exceed ~45%. (3) topology-dependent error amplification: independent agents amplify errors 17.2x through unchecked propagation, while centralized coordination contains this to 4.4x. Centralized coordination improves performance by 80.9% on parallelizable tasks like financial reasoning, while decentralized coordination excels on dynamic web navigation (+9.2% vs. +0.2%). Yet for sequential reasoning tasks, all multi-agent variants degraded performance by 39-70%. The framework predicts the optimal coordination strategy for 87% of held-out configurations, providing a predictive principle of agentic scaling based on measurable task properties.

</details>


### [12] [rSIM: Incentivizing Reasoning Capabilities of LLMs via Reinforced Strategy Injection](https://arxiv.org/abs/2512.08300)
*Sijia Chen,Baochun Li,Di Niu*

Main category: cs.AI

TL;DR: 本文提出强化策略注入机制rSIM，让大语言模型成为推理语言模型，实验显示其效果佳且规划器有泛化性和持续学习能力。


<details>
  <summary>Details</summary>
Motivation: 受大语言模型通过强化学习演变为推理语言模型中推理策略启发，提出使任意大语言模型成为推理语言模型的方法。

Method: 提出rSIM机制，采用多智能体强化学习联合训练规划器（领导者智能体）和大语言模型（跟随者智能体），基于领导者 - 跟随者框架和简单基于规则的奖励。

Result: rSIM使Qwen2.5 - 0.5B成为推理语言模型，显著超越Qwen2.5 - 14B；规划器可作为插件提升现有大语言模型推理能力；支持跨任务持续学习。

Conclusion: rSIM机制有效，规划器具有泛化性和持续学习能力，能提升大语言模型推理能力。

Abstract: Large language models (LLMs) are post-trained through reinforcement learning (RL) to evolve into Reasoning Language Models (RLMs), where the hallmark of this advanced reasoning is ``aha'' moments when they start to perform strategies, such as self-reflection and deep thinking, within chain of thoughts (CoTs). Motivated by this, this paper proposes a novel reinforced strategy injection mechanism (rSIM), that enables any LLM to become an RLM by employing a small planner to guide the LLM's CoT through the adaptive injection of reasoning strategies. To achieve this, the planner (leader agent) is jointly trained with an LLM (follower agent) using multi-agent RL (MARL), based on a leader-follower framework and straightforward rule-based rewards. Experimental results show that rSIM enables Qwen2.5-0.5B to become an RLM and significantly outperform Qwen2.5-14B. Moreover, the planner is generalizable: it only needs to be trained once and can be applied as a plug-in to substantially improve the reasoning capabilities of existing LLMs. In addition, the planner supports continual learning across various tasks, allowing its planning abilities to gradually improve and generalize to a wider range of problems.

</details>


### [13] [Predicting California Bearing Ratio with Ensemble and Neural Network Models: A Case Study from Türkiye](https://arxiv.org/abs/2512.08340)
*Abdullah Hulusi Kökçam,Uğur Dağdeviren,Talas Fikret Kurnaz,Alparslan Serhat Demir,Caner Erden*

Main category: cs.AI

TL;DR: 本文引入机器学习框架预测加州承载比（CBR），用382个土壤样本测试12种算法，随机森林表现最佳，支持智能模型在岩土工程应用。


<details>
  <summary>Details</summary>
Motivation: 传统CBR测定方法耗时、成本高且不实用，需要更高效的方法。

Method: 收集382个土壤样本，测试决策树、随机森林等12种机器学习算法，进行训练、验证和评估。

Result: 随机森林回归器表现最佳，训练、验证和测试的R2分数分别为0.95、0.76和0.83。

Conclusion: 支持将智能、以数据为中心的模型集成到岩土工程中，可替代传统方法，推动基础设施分析和设计的数字化转型。

Abstract: The California Bearing Ratio (CBR) is a key geotechnical indicator used to assess the load-bearing capacity of subgrade soils, especially in transportation infrastructure and foundation design. Traditional CBR determination relies on laboratory penetration tests. Despite their accuracy, these tests are often time-consuming, costly, and can be impractical, particularly for large-scale or diverse soil profiles. Recent progress in artificial intelligence, especially machine learning (ML), has enabled data-driven approaches for modeling complex soil behavior with greater speed and precision. This study introduces a comprehensive ML framework for CBR prediction using a dataset of 382 soil samples collected from various geoclimatic regions in Türkiye. The dataset includes physicochemical soil properties relevant to bearing capacity, allowing multidimensional feature representation in a supervised learning context. Twelve ML algorithms were tested, including decision tree, random forest, extra trees, gradient boosting, xgboost, k-nearest neighbors, support vector regression, multi-layer perceptron, adaboost, bagging, voting, and stacking regressors. Each model was trained, validated, and evaluated to assess its generalization and robustness. Among them, the random forest regressor performed the best, achieving strong R2 scores of 0.95 (training), 0.76 (validation), and 0.83 (test). These outcomes highlight the model's powerful nonlinear mapping ability, making it a promising tool for predictive geotechnical tasks. The study supports the integration of intelligent, data-centric models in geotechnical engineering, offering an effective alternative to traditional methods and promoting digital transformation in infrastructure analysis and design.

</details>


### [14] [Soil Compaction Parameters Prediction Based on Automated Machine Learning Approach](https://arxiv.org/abs/2512.08343)
*Caner Erden,Alparslan Serhat Demir,Abdullah Hulusi Kokcam,Talas Fikret Kurnaz,Ugur Dagdeviren*

Main category: cs.AI

TL;DR: 传统方法测土壤压实参数存在局限，研究提出自动化机器学习（AutoML）方法预测OMC和MDD，发现XGBoost算法表现最佳，证明AutoML有效，强调异构数据集重要性。


<details>
  <summary>Details</summary>
Motivation: 传统确定土壤最佳含水量和最大干密度的方法是劳动密集型实验，经验回归模型适用性和准确性有限，机器学习模型在异质数据集上准确性和泛化性不足，需要更有效的方法预测压实参数。

Method: 采用自动化机器学习（AutoML）方法，自动进行算法选择和超参数优化。

Result: 极端梯度提升（XGBoost）算法表现最佳，在独立数据集上MDD的R平方值为80.4%，OMC的R平方值为89.1%。

Conclusion: AutoML在预测不同土壤类型压实参数方面有效，异构数据集对提高机器学习模型泛化和性能很重要，研究有助于更高效可靠的建筑实践。

Abstract: Soil compaction is critical in construction engineering to ensure the stability of structures like road embankments and earth dams. Traditional methods for determining optimum moisture content (OMC) and maximum dry density (MDD) involve labor-intensive laboratory experiments, and empirical regression models have limited applicability and accuracy across diverse soil types. In recent years, artificial intelligence (AI) and machine learning (ML) techniques have emerged as alternatives for predicting these compaction parameters. However, ML models often struggle with prediction accuracy and generalizability, particularly with heterogeneous datasets representing various soil types. This study proposes an automated machine learning (AutoML) approach to predict OMC and MDD. AutoML automates algorithm selection and hyperparameter optimization, potentially improving accuracy and scalability. Through extensive experimentation, the study found that the Extreme Gradient Boosting (XGBoost) algorithm provided the best performance, achieving R-squared values of 80.4% for MDD and 89.1% for OMC on a separate dataset. These results demonstrate the effectiveness of AutoML in predicting compaction parameters across different soil types. The study also highlights the importance of heterogeneous datasets in improving the generalization and performance of ML models. Ultimately, this research contributes to more efficient and reliable construction practices by enhancing the prediction of soil compaction parameters.

</details>


### [15] [Enhancing Explainability of Graph Neural Networks Through Conceptual and Structural Analyses and Their Extensions](https://arxiv.org/abs/2512.08344)
*Tien Cuong Bui*

Main category: cs.AI

TL;DR: GNN应用广泛但决策过程难理解，现有XAI方法有不足，论文提出新XAI框架用于图机器学习。


<details>
  <summary>Details</summary>
Motivation: 当前GNN决策过程难理解，现有XAI方法存在无法处理图复杂关系、计算资源需求大、可靠性低、泛化性差等问题，需要改进。

Method: 开发一种专为图基机器学习定制的新型XAI框架。

Result: 未提及。

Conclusion: 未提及，但目标是提供适应性强、计算高效的GNN解释，超越特征分析捕捉图结构对预测的影响。

Abstract: Graph Neural Networks (GNNs) have become a powerful tool for modeling and analyzing data with graph structures. The wide adoption in numerous applications underscores the value of these models. However, the complexity of these methods often impedes understanding their decision-making processes. Current Explainable AI (XAI) methods struggle to untangle the intricate relationships and interactions within graphs. Several methods have tried to bridge this gap via a post-hoc approach or self-interpretable design. Most of them focus on graph structure analysis to determine essential patterns that correlate with prediction outcomes. While post-hoc explanation methods are adaptable, they require extra computational resources and may be less reliable due to limited access to the model's internal workings. Conversely, Interpretable models can provide immediate explanations, but their generalizability to different scenarios remains a major concern. To address these shortcomings, this thesis seeks to develop a novel XAI framework tailored for graph-based machine learning. The proposed framework aims to offer adaptable, computationally efficient explanations for GNNs, moving beyond individual feature analysis to capture how graph structure influences predictions.

</details>


### [16] [The High Cost of Incivility: Quantifying Interaction Inefficiency via Multi-Agent Monte Carlo Simulations](https://arxiv.org/abs/2512.08345)
*Benedikt Mangold*

Main category: cs.AI

TL;DR: 研究利用LLM多智能体系统模拟一对一辩论，发现有毒参与者使对话时长增加约25%，提出“毒性延迟”可衡量损失，且基于智能体建模是研究社会摩擦的伦理替代方案。


<details>
  <summary>Details</summary>
Motivation: 职场毒性对组织文化有害，但量化其对运营效率的直接影响存在方法上的挑战。

Method: 利用基于大语言模型的多智能体系统模拟一对一辩论，采用蒙特卡罗方法模拟数百次讨论，比较对照组和有毒性提示的处理组的收敛时间。

Result: 涉及有毒参与者的对话时长有大约25%的统计学显著增加。

Conclusion: “毒性延迟”可作为企业和学术环境中财务损失的代理指标，基于智能体的建模为测量社会摩擦机制提供了可重复、符合伦理的人类受试者研究替代方案。

Abstract: Workplace toxicity is widely recognized as detrimental to organizational culture, yet quantifying its direct impact on operational efficiency remains methodologically challenging due to the ethical and practical difficulties of reproducing conflict in human subjects. This study leverages Large Language Model (LLM) based Multi-Agent Systems to simulate 1-on-1 adversarial debates, creating a controlled "sociological sandbox". We employ a Monte Carlo method to simulate hundrets of discussions, measuring the convergence time (defined as the number of arguments required to reach a conclusion) between a baseline control group and treatment groups involving agents with "toxic" system prompts. Our results demonstrate a statistically significant increase of approximately 25\% in the duration of conversations involving toxic participants. We propose that this "latency of toxicity" serves as a proxy for financial damage in corporate and academic settings. Furthermore, we demonstrate that agent-based modeling provides a reproducible, ethical alternative to human-subject research for measuring the mechanics of social friction.

</details>


### [17] [Reflecting with Two Voices: A Co-Adaptive Dual-Strategy Framework for LLM-Based Agent Decision Making](https://arxiv.org/abs/2512.08366)
*Wentao Zhang,Qunbo Wang,Tao Zhang,Junsheng Wu,Hongping Gan,Yang Liu,Ling Dai,Shizhuang Deng,Shuntong Sun*

Main category: cs.AI

TL;DR: 提出无演示框架 DuSAR，使单个冻结大语言模型通过双策略进行自适应推理，在 ALFWorld 和 Mind2Web 上取得优异成果，减少标记消耗。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型智能体依赖外部演示或检索增强规划，存在脆性、泛化性差和计算开销高的问题。

Method: 提出 DuSAR 框架，采用高级整体规划和基于上下文的局部策略两种互补策略进行自适应推理，通过轻量级反思机制交互。

Result: 在 ALFWorld 和 Mind2Web 上用开源大语言模型取得最优性能，成功率翻倍，减少 3 - 9 倍每步标记消耗，消融实验证实双策略协调的必要性，集成专家演示可进一步提升结果。

Conclusion: DuSAR 框架有效，具有灵活性和与外部知识的兼容性。

Abstract: Large language model (LLM) agents often rely on external demonstrations or retrieval-augmented planning, leading to brittleness, poor generalization, and high computational overhead. Inspired by human problem-solving, we propose DuSAR (Dual-Strategy Agent with Reflecting) - a demonstration-free framework that enables a single frozen LLM to perform co-adaptive reasoning via two complementary strategies: a high-level holistic plan and a context-grounded local policy. These strategies interact through a lightweight reflection mechanism, where the agent continuously assesses progress via a Strategy Fitness Score and dynamically revises its global plan when stuck or refines it upon meaningful advancement, mimicking human metacognitive behavior. On ALFWorld and Mind2Web, DuSAR achieves state-of-the-art performance with open-source LLMs (7B-70B), reaching 37.1% success on ALFWorld (Llama3.1-70B) - more than doubling the best prior result (13.0%) - and 4.02% on Mind2Web, also more than doubling the strongest baseline. Remarkably, it reduces per-step token consumption by 3-9X while maintaining strong performance. Ablation studies confirm the necessity of dual-strategy coordination. Moreover, optional integration of expert demonstrations further boosts results, highlighting DuSAR's flexibility and compatibility with external knowledge.

</details>


### [18] [DeepFeature: Iterative Context-aware Feature Generation for Wearable Biosignals](https://arxiv.org/abs/2512.08379)
*Kaiwei Liu,Yuting He,Bufang Yang,Mu Yuan,Chun Man Victor Wong,Ho Pong Andrew Sze,Zhenyu Yan,Hongkai Chen*

Main category: cs.AI

TL;DR: 提出针对可穿戴生物信号的LLM赋能、上下文感知特征生成框架DeepFeature，实验表明其在多项任务上优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有生物信号特征提取方法缺乏特定任务上下文知识、难选最优设置且易出错。

Method: 提出DeepFeature框架，引入多源特征生成机制、迭代特征细化过程，采用多层过滤和验证方法进行特征到代码的转换。

Result: 在八项不同任务中平均AUROC提升4.21 - 9.67%，五项任务优于现有方法，其余任务表现相当。

Conclusion: DeepFeature是有效且有优势的可穿戴生物信号特征生成框架。

Abstract: Biosignals collected from wearable devices are widely utilized in healthcare applications. Machine learning models used in these applications often rely on features extracted from biosignals due to their effectiveness, lower data dimensionality, and wide compatibility across various model architectures. However, existing feature extraction methods often lack task-specific contextual knowledge, struggle to identify optimal feature extraction settings in high-dimensional feature space, and are prone to code generation and automation errors. In this paper, we propose DeepFeature, the first LLM-empowered, context-aware feature generation framework for wearable biosignals. DeepFeature introduces a multi-source feature generation mechanism that integrates expert knowledge with task settings. It also employs an iterative feature refinement process that uses feature assessment-based feedback for feature re-selection. Additionally, DeepFeature utilizes a robust multi-layer filtering and verification approach for robust feature-to-code translation to ensure that the extraction functions run without crashing. Experimental evaluation results show that DeepFeature achieves an average AUROC improvement of 4.21-9.67% across eight diverse tasks compared to baseline methods. It outperforms state-of-the-art approaches on five tasks while maintaining comparable performance on the remaining tasks.

</details>


### [19] [Prismatic World Model: Learning Compositional Dynamics for Planning in Hybrid Systems](https://arxiv.org/abs/2512.08411)
*Mingwei Li,Xiaoyuan Zhang,Chengwei Yang,Zilong Zheng,Yaodong Yang*

Main category: cs.AI

TL;DR: 引入棱柱世界模型PRISM - WM处理机器人领域基于模型规划问题，减少滚动漂移，实验证明其对轨迹优化算法有优势。


<details>
  <summary>Details</summary>
Motivation: 传统潜在世界模型在处理机器人领域混合物理动态时会过度平滑不同动态模式，导致长视野前瞻时出现灾难性复合误差，搜索过程在物理边界不可靠。

Method: 引入PRISM - WM，利用上下文感知的混合专家框架，通过门控机制识别当前物理模式，专家预测相关过渡动态，还引入潜在正交化目标确保专家多样性。

Result: PRISM - WM显著减少了滚动漂移。

Conclusion: PRISM - WM为轨迹优化算法提供了高保真基础，有潜力成为下一代基于模型智能体的强大基础模型。

Abstract: Model-based planning in robotic domains is fundamentally challenged by the hybrid nature of physical dynamics, where continuous motion is punctuated by discrete events such as contacts and impacts. Conventional latent world models typically employ monolithic neural networks that enforce global continuity, inevitably over-smoothing the distinct dynamic modes (e.g., sticking vs. sliding, flight vs. stance). For a planner, this smoothing results in catastrophic compounding errors during long-horizon lookaheads, rendering the search process unreliable at physical boundaries. To address this, we introduce the Prismatic World Model (PRISM-WM), a structured architecture designed to decompose complex hybrid dynamics into composable primitives. PRISM-WM leverages a context-aware Mixture-of-Experts (MoE) framework where a gating mechanism implicitly identifies the current physical mode, and specialized experts predict the associated transition dynamics. We further introduce a latent orthogonalization objective to ensure expert diversity, effectively preventing mode collapse. By accurately modeling the sharp mode transitions in system dynamics, PRISM-WM significantly reduces rollout drift. Extensive experiments on challenging continuous control benchmarks, including high-dimensional humanoids and diverse multi-task settings, demonstrate that PRISM-WM provides a superior high-fidelity substrate for trajectory optimization algorithms (e.g., TD-MPC), proving its potential as a powerful foundational model for next-generation model-based agents.

</details>


### [20] [From Accuracy to Impact: The Impact-Driven AI Framework (IDAIF) for Aligning Engineering Architecture with Theory of Change](https://arxiv.org/abs/2512.08449)
*Yong-Woon Kim*

Main category: cs.AI

TL;DR: 本文提出IDAIF框架，将变革理论与AI系统设计结合，解决AI对齐问题，介绍各层理论与组件，经案例验证，推动AI从以模型为中心转向以影响为中心发展。


<details>
  <summary>Details</summary>
Motivation: 随着AI影响高风险领域，当前方法忽视AI部署的社会技术维度，对齐问题亟待解决。

Method: 建立变革理论五阶段模型与AI架构层的系统映射，各层融入多目标帕累托优化等理论，给出数学公式，设保证层，用三个案例验证。

Result: 通过三个案例展示了IDAIF在医疗、网络安全和软件工程领域的应用。

Conclusion: IDAIF框架实现了AI开发从以模型为中心到以影响为中心的范式转变，为构建道德、可信、有益的AI系统提供架构模式。

Abstract: This paper introduces the Impact-Driven AI Framework (IDAIF), a novel architectural methodology that integrates Theory of Change (ToC) principles with modern artificial intelligence system design. As AI systems increasingly influence high-stakes domains including healthcare, finance, and public policy, the alignment problem--ensuring AI behavior corresponds with human values and intentions--has become critical. Current approaches predominantly optimize technical performance metrics while neglecting the sociotechnical dimensions of AI deployment. IDAIF addresses this gap by establishing a systematic mapping between ToC's five-stage model (Inputs-Activities-Outputs-Outcomes-Impact) and corresponding AI architectural layers (Data Layer-Pipeline Layer-Inference Layer-Agentic Layer-Normative Layer). Each layer incorporates rigorous theoretical foundations: multi-objective Pareto optimization for value alignment, hierarchical multi-agent orchestration for outcome achievement, causal directed acyclic graphs (DAGs) for hallucination mitigation, and adversarial debiasing with Reinforcement Learning from Human Feedback (RLHF) for fairness assurance. We provide formal mathematical formulations for each component and introduce an Assurance Layer that manages assumption failures through guardian architectures. Three case studies demonstrate IDAIF application across healthcare, cybersecurity, and software engineering domains. This framework represents a paradigm shift from model-centric to impact-centric AI development, providing engineers with concrete architectural patterns for building ethical, trustworthy, and socially beneficial AI systems.

</details>


### [21] [Using reinforcement learning to probe the role of feedback in skill acquisition](https://arxiv.org/abs/2512.08463)
*Antonio Terpin,Raffaello D'Andrea*

Main category: cs.AI

TL;DR: 研究技能获取过程，用强化学习智能体与旋转圆柱体互动控制阻力，发现学习高性能技能所需信息或比执行时多，学习条件取决于目标。


<details>
  <summary>Details</summary>
Motivation: 为在可控条件下研究技能获取过程，绕开人类受试者，采用物理系统实验。

Method: 将通用强化学习智能体与旋转圆柱体在桌面循环水通道中相连，通过最大化或最小化阻力进行实验。

Result: 高维流动反馈使智能体短时间内发现高性能阻力控制策略；无反馈执行策略性能相近；无流动反馈训练，最大化阻力时难获良策，最小化阻力仍可成功但较缓慢和不可靠。

Conclusion: 学习高性能技能所需信息可能比执行多，学习条件仅取决于目标，而非动力学或策略复杂性。

Abstract: Many high-performance human activities are executed with little or no external feedback: think of a figure skater landing a triple jump, a pitcher throwing a curveball for a strike, or a barista pouring latte art. To study the process of skill acquisition under fully controlled conditions, we bypass human subjects. Instead, we directly interface a generalist reinforcement learning agent with a spinning cylinder in a tabletop circulating water channel to maximize or minimize drag. This setup has several desirable properties. First, it is a physical system, with the rich interactions and complex dynamics that only the physical world has: the flow is highly chaotic and extremely difficult, if not impossible, to model or simulate accurately. Second, the objective -- drag minimization or maximization -- is easy to state and can be captured directly in the reward, yet good strategies are not obvious beforehand. Third, decades-old experimental studies provide recipes for simple, high-performance open-loop policies. Finally, the setup is inexpensive and far easier to reproduce than human studies. In our experiments we find that high-dimensional flow feedback lets the agent discover high-performance drag-control strategies with only minutes of real-world interaction. When we later replay the same action sequences without any feedback, we obtain almost identical performance. This shows that feedback, and in particular flow feedback, is not needed to execute the learned policy. Surprisingly, without flow feedback during training the agent fails to discover any well-performing policy in drag maximization, but still succeeds in drag minimization, albeit more slowly and less reliably. Our studies show that learning a high-performance skill can require richer information than executing it, and learning conditions can be kind or wicked depending solely on the goal, not on dynamics or policy complexity.

</details>


### [22] [Autonomous Issue Resolver: Towards Zero-Touch Code Maintenance](https://arxiv.org/abs/2512.08492)
*Aliaksei Kaliutau*

Main category: cs.AI

TL;DR: 论文提出从CPGs到DTG的范式转变，并引入多智能体框架构建AIR系统修复代码逻辑，在多个基准测试取得良好效果。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型虽在函数级代码生成取得进展，但库级自动程序修复仍面临挑战，现有方法采用控制中心范式有局限性。

Method: 提出从CPGs到DTG的范式转变，引入多智能体框架，构建AIR系统，利用神经符号推理和DTG结构进行可扩展逻辑修复。

Result: 方法解决了现代编码代理中标准RAG系统固有的“语义陷阱”问题，在多个SWE基准测试中取得良好效果，在SWE - Verified基准测试中解决率达87.1%。

Conclusion: 该方法直接解决了当前AI代码辅助工具的核心局限性，满足了软件依赖程度日益增加的世界对更强大基础的迫切需求。

Abstract: Recent advances in Large Language Models have revolutionized function-level code generation; however, repository-scale Automated Program Repair (APR) remains a significant challenge. Current approaches typically employ a control-centric paradigm, forcing agents to navigate complex directory structures and irrelevant control logic. In this paper, we propose a paradigm shift from the standard Code Property Graphs (CPGs) to the concept of Data Transformation Graph (DTG) that inverts the topology by modeling data states as nodes and functions as edges, enabling agents to trace logic defects through data lineage rather than control flow. We introduce a multi-agent framework that reconciles data integrity navigation with control flow logic. Our theoretical analysis and case studies demonstrate that this approach resolves the "Semantic Trap" inherent in standard RAG systems in modern coding agents. We provide a comprehensive implementation in the form of Autonomous Issue Resolver (AIR), a self-improvement system for zero-touch code maintenance that utilizes neuro-symbolic reasoning and uses the DTG structure for scalable logic repair. Our approach has demonstrated good results on several SWE benchmarks, reaching a resolution rate of 87.1% on SWE-Verified benchmark. Our approach directly addresses the core limitations of current AI code-assistant tools and tackles the critical need for a more robust foundation for our increasingly software-dependent world.

</details>


### [23] [A Lightweight Transfer Learning-Based State-of-Health Monitoring with Application to Lithium-ion Batteries in Unmanned Air Vehicles](https://arxiv.org/abs/2512.08512)
*Jiang Liu,Yan Qin,Wei Dai,Chau Yuen*

Main category: cs.AI

TL;DR: 提出轻量级基于迁移学习的电池健康状态监测方法CITL，经实验验证优于多种方法。


<details>
  <summary>Details</summary>
Motivation: 传统基于迁移学习的SOH监测在便携式移动设备中消耗大量计算资源，降低工作续航，需解决该问题。

Method: 提出半监督迁移学习机制，通过迭代添加网络节点以最小化监测残差；通过结构风险最小化等保证节点参数跨域学习能力；进行收敛分析。

Result: 在真实无人机电池数据集实验中，CITL在SOH估计上比SS - TCA等方法分别优83.73%、61.15%等（以均方根误差为指标）。

Conclusion: 所提出的轻量级基于CITL的SOH监测方法有效，能提升监测性能且保证网络紧凑性。

Abstract: Accurate and rapid state-of-health (SOH) monitoring plays an important role in indicating energy information for lithium-ion battery-powered portable mobile devices. To confront their variable working conditions, transfer learning (TL) emerges as a promising technique for leveraging knowledge from data-rich source working conditions, significantly reducing the training data required for SOH monitoring from target working conditions. However, traditional TL-based SOH monitoring is infeasible when applied in portable mobile devices since substantial computational resources are consumed during the TL stage and unexpectedly reduce the working endurance. To address these challenges, this paper proposes a lightweight TL-based SOH monitoring approach with constructive incremental transfer learning (CITL). First, taking advantage of the unlabeled data in the target domain, a semi-supervised TL mechanism is proposed to minimize the monitoring residual in a constructive way, through iteratively adding network nodes in the CITL. Second, the cross-domain learning ability of node parameters for CITL is comprehensively guaranteed through structural risk minimization, transfer mismatching minimization, and manifold consistency maximization. Moreover, the convergence analysis of the CITL is given, theoretically guaranteeing the efficacy of TL performance and network compactness. Finally, the proposed approach is verified through extensive experiments with a realistic unmanned air vehicles (UAV) battery dataset collected from dozens of flight missions. Specifically, the CITL outperforms SS-TCA, MMD-LSTM-DA, DDAN, BO-CNN-TL, and AS$^3$LSTM, in SOH estimation by 83.73%, 61.15%, 28.24%, 87.70%, and 57.34%, respectively, as evaluated using the index root mean square error.

</details>


### [24] [Principles2Plan: LLM-Guided System for Operationalising Ethical Principles into Plans](https://arxiv.org/abs/2512.08536)
*Tammy Zhong,Yang Song,Maurice Pagnucco*

Main category: cs.AI

TL;DR: 介绍了名为Principles2Plan的交互式研究原型，展示人与大语言模型协作生成上下文敏感的道德规则以指导自动化规划。


<details>
  <summary>Details</summary>
Motivation: 现有自动化规划工具对机器人在人类环境中运行的道德意识支持不足，手动指定道德规则费力且特定于上下文。

Method: 领域专家提供规划领域、问题细节和高级原则，系统生成可操作的道德规则，用户审查、排序后提供给规划器。

Result: 开发出Principles2Plan系统，展示了人类与大语言模型协作的潜力。

Conclusion: Principles2Plan使道德自动化规划更实用可行。

Abstract: Ethical awareness is critical for robots operating in human environments, yet existing automated planning tools provide little support. Manually specifying ethical rules is labour-intensive and highly context-specific. We present Principles2Plan, an interactive research prototype demonstrating how a human and a Large Language Model (LLM) can collaborate to produce context-sensitive ethical rules and guide automated planning. A domain expert provides the planning domain, problem details, and relevant high-level principles such as beneficence and privacy. The system generates operationalisable ethical rules consistent with these principles, which the user can review, prioritise, and supply to a planner to produce ethically-informed plans. To our knowledge, no prior system supports users in generating principle-grounded rules for classical planning contexts. Principles2Plan showcases the potential of human-LLM collaboration for making ethical automated planning more practical and feasible.

</details>


### [25] [The SMART+ Framework for AI Systems](https://arxiv.org/abs/2512.08592)
*Laxmiraju Kandikatla,Branislav Radeljic*

Main category: cs.AI

TL;DR: AI已融入多行业，但带来安全等挑战，本文提出SMART+框架评估和治理AI系统，能减轻风险、建立信任，为临床研究AI治理提供基础。


<details>
  <summary>Details</summary>
Motivation: AI在多行业应用带来安全、问责和合规等新挑战，需要解决这些问题。

Method: 引入基于安全、监控等支柱构建，并增强隐私安全等方面的SMART+框架。

Result: SMART+框架能减轻风险、建立信任、做好合规准备。

Conclusion: SMART+框架为临床研究中有效AI治理提供了坚实基础，可促进负责任的AI应用。

Abstract: Artificial Intelligence (AI) systems are now an integral part of multiple industries. In clinical research, AI supports automated adverse event detection in clinical trials, patient eligibility screening for protocol enrollment, and data quality validation. Beyond healthcare, AI is transforming finance through real-time fraud detection, automated loan risk assessment, and algorithmic decision-making. Similarly, in manufacturing, AI enables predictive maintenance to reduce equipment downtime, enhances quality control through computer-vision inspection, and optimizes production workflows using real-time operational data. While these technologies enhance operational efficiency, they introduce new challenges regarding safety, accountability, and regulatory compliance. To address these concerns, we introduce the SMART+ Framework - a structured model built on the pillars of Safety, Monitoring, Accountability, Reliability, and Transparency, and further enhanced with Privacy & Security, Data Governance, Fairness & Bias, and Guardrails. SMART+ offers a practical, comprehensive approach to evaluating and governing AI systems across industries. This framework aligns with evolving mechanisms and regulatory guidance to integrate operational safeguards, oversight procedures, and strengthened privacy and governance controls. SMART+ demonstrates risk mitigation, trust-building, and compliance readiness. By enabling responsible AI adoption and ensuring auditability, SMART+ provides a robust foundation for effective AI governance in clinical research.

</details>


### [26] [CogMCTS: A Novel Cognitive-Guided Monte Carlo Tree Search Framework for Iterative Heuristic Evolution with Large Language Models](https://arxiv.org/abs/2512.08609)
*Hui Wang,Yang Liu,Xiaoyu Zhang,Chaoxu Mu*

Main category: cs.AI

TL;DR: 本文提出CogMCTS框架解决现有LLM - 基AHD方法局限，实验显示其在稳定性、效率和解决方案质量上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的进化方法易陷入局部最优，LLM与MCTS集成存在多轮认知集成有限和搜索多样性受限问题，需改进。

Method: 提出CogMCTS框架，紧密结合LLM认知引导机制与MCTS，采用多轮认知反馈、双轨节点扩展与精英启发式管理、战略变异等方法。

Result: CogMCTS在稳定性、效率和解决方案质量上优于现有基于LLM的自动启发式设计方法。

Conclusion: CogMCTS框架能有效提升自动启发式优化性能，解决现有方法局限。

Abstract: Automatic Heuristic Design (AHD) is an effective1 framework for solving complex optimization prob-2 lems. The development of large language mod-3 els (LLMs) enables the automated generation of4 heuristics. Existing LLM-based evolutionary meth-5 ods rely on population strategies and are prone6 to local optima. Integrating LLMs with Monte7 Carlo Tree Search (MCTS) improves the trade-off8 between exploration and exploitation, but multi-9 round cognitive integration remains limited and10 search diversity is constrained. To overcome these11 limitations, this paper proposes a novel cognitive-12 guided MCTS framework (CogMCTS). CogMCTS13 tightly integrates the cognitive guidance mecha-14 nism of LLMs with MCTS to achieve efficient au-15 tomated heuristic optimization. The framework16 employs multi-round cognitive feedback to incor-17 porate historical experience, node information, and18 negative outcomes, dynamically improving heuris-19 tic generation. Dual-track node expansion com-20 bined with elite heuristic management balances the21 exploration of diverse heuristics and the exploita-22 tion of high-quality experience. In addition, strate-23 gic mutation modifies the heuristic forms and pa-24 rameters to further enhance the diversity of the so-25 lution and the overall optimization performance.26 The experimental results indicate that CogMCTS27 outperforms existing LLM-based AHD methods in28 stability, efficiency, and solution quality.

</details>


### [27] [Protein Secondary Structure Prediction Using Transformers](https://arxiv.org/abs/2512.08613)
*Manzi Kevin Maxime*

Main category: cs.AI

TL;DR: 提出基于transformer的模型预测蛋白质二级结构，用滑动窗口扩充样本，模型泛化能力强。


<details>
  <summary>Details</summary>
Motivation: 从氨基酸序列预测蛋白质二级结构对理解蛋白质功能至关重要。

Method: 提出基于transformer的模型，应用注意力机制处理蛋白质序列数据，使用滑动窗口数据增强技术扩充CB513数据集的训练样本。

Result: transformer模型在可变长度序列上有很强的泛化能力，能有效捕捉局部和远程残基相互作用。

Conclusion: 基于transformer的模型在蛋白质二级结构预测中具有有效性和优势。

Abstract: Predicting protein secondary structures such as alpha helices, beta sheets, and coils from amino acid sequences is essential for understanding protein function. This work presents a transformer-based model that applies attention mechanisms to protein sequence data to predict structural motifs. A sliding-window data augmentation technique is used on the CB513 dataset to expand the training samples. The transformer shows strong ability to generalize across variable-length sequences while effectively capturing both local and long-range residue interactions.

</details>


### [28] [See-Control: A Multimodal Agent Framework for Smartphone Interaction with a Robotic Arm](https://arxiv.org/abs/2512.08629)
*Haoyu Zhao,Weizhong Ding,Yuhao Yang,Zheng Tian,Linyi Yang,Kun Shao,Jun Wang*

Main category: cs.AI

TL;DR: 本文提出了具身智能手机操作（ESO）任务和See - Control框架，可通过低自由度机械臂直接物理交互实现智能手机操作，是平台无关的解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型用于智能手机操作的方法依赖ADB，局限于安卓设备，需更通用的解决方案。

Method: 提出ESO任务和See - Control框架，包含155个任务的ESO基准及评估指标、基于MLLM且无需ADB的具身代理、操作情节注释丰富的数据集。

Result: 构建了基准、具身代理和数据集，为数字代理与物理世界搭建桥梁。

Conclusion: See - Control为家庭机器人在现实环境中执行依赖智能手机的任务迈出了重要一步。

Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have enabled their use as intelligent agents for smartphone operation. However, existing methods depend on the Android Debug Bridge (ADB) for data transmission and action execution, limiting their applicability to Android devices. In this work, we introduce the novel Embodied Smartphone Operation (ESO) task and present See-Control, a framework that enables smartphone operation via direct physical interaction with a low-DoF robotic arm, offering a platform-agnostic solution. See-Control comprises three key components: (1) an ESO benchmark with 155 tasks and corresponding evaluation metrics; (2) an MLLM-based embodied agent that generates robotic control commands without requiring ADB or system back-end access; and (3) a richly annotated dataset of operation episodes, offering valuable resources for future research. By bridging the gap between digital agents and the physical world, See-Control provides a concrete step toward enabling home robots to perform smartphone-dependent tasks in realistic environments.

</details>


### [29] [Multi-Agent Intelligence for Multidisciplinary Decision-Making in Gastrointestinal Oncology](https://arxiv.org/abs/2512.08674)
*Rongzhao Zhang,Junqiao Wang,Shuyun Yang,Mouxiao Bian,Chao Ding,Yuwei Bai,Chihao Zhang,Yuguang Shen,Lei Wang,Lei Zheng,Qiujuan Yan,Yun Zhong,Meiling Liu,Jiwei Yu,Zheng Wang,Jie Xu,Meng Luo*

Main category: cs.AI

TL;DR: 提出分层多智能体框架解决多模态大语言模型在胃肠道肿瘤临床推理中的问题，表现优于基线，证明基于智能体协作的有效性。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在处理复杂异质医疗史时存在上下文稀释和幻觉等问题，需解决以用于胃肠道肿瘤多模态临床推理。

Method: 提出分层多智能体框架，模仿人类多学科团队协作流程。

Result: 系统获得4.60/5.00的综合专家评估分数，相比单一基线有显著提升，基于智能体的架构在推理逻辑和医学准确性方面改善最大。

Conclusion: 基于智能体的模仿协作模式为肿瘤学自动决策支持提供了可扩展、可解释且临床稳健的范式。

Abstract: Multimodal clinical reasoning in the field of gastrointestinal (GI) oncology necessitates the integrated interpretation of endoscopic imagery, radiological data, and biochemical markers. Despite the evident potential exhibited by Multimodal Large Language Models (MLLMs), they frequently encounter challenges such as context dilution and hallucination when confronted with intricate, heterogeneous medical histories. In order to address these limitations, a hierarchical Multi-Agent Framework is proposed, which emulates the collaborative workflow of a human Multidisciplinary Team (MDT). The system attained a composite expert evaluation score of 4.60/5.00, thereby demonstrating a substantial improvement over the monolithic baseline. It is noteworthy that the agent-based architecture yielded the most substantial enhancements in reasoning logic and medical accuracy. The findings indicate that mimetic, agent-based collaboration provides a scalable, interpretable, and clinically robust paradigm for automated decision support in oncology.

</details>


### [30] [Deconstructing the Dual Black Box:A Plug-and-Play Cognitive Framework for Human-AI Collaborative Enhancement and Its Implications for AI Governance](https://arxiv.org/abs/2512.08740)
*Yiming Lu*

Main category: cs.AI

TL;DR: 论文提出‘人机协同认知增强’范式，用‘元交互’解决双黑箱问题，实现范式转变，为认知公平提供证据和新治理路径，框架已开源。


<details>
  <summary>Details</summary>
Motivation: 解决人类专家‘认知黑箱’和人工智能‘计算黑箱’的问题，实现从‘AI 作为工具’到‘AI 作为思维伙伴’的转变。

Method: 提出‘元交互’方法，通过‘即插即用认知框架’将专家思维转化为可复用资产，运用递归对抗元思维网络（RAMTN）。

Result: 给出了将双黑箱变为可组合、可审计、可扩展的‘功能白盒’系统的实现方式，框架已开源。

Conclusion: 为‘认知公平’提供首个工程证明，开拓了新的 AI 治理路径，促进技术向善和认知包容。

Abstract: Currently, there exists a fundamental divide between the "cognitive black box" (implicit intuition) of human experts and the "computational black box" (untrustworthy decision-making) of artificial intelligence (AI). This paper proposes a new paradigm of "human-AI collaborative cognitive enhancement," aiming to transform the dual black boxes into a composable, auditable, and extensible "functional white-box" system through structured "meta-interaction." The core breakthrough lies in the "plug-and-play cognitive framework"--a computable knowledge package that can be extracted from expert dialogues and loaded into the Recursive Adversarial Meta-Thinking Network (RAMTN). This enables expert thinking, such as medical diagnostic logic and teaching intuition, to be converted into reusable and scalable public assets, realizing a paradigm shift from "AI as a tool" to "AI as a thinking partner." This work not only provides the first engineering proof for "cognitive equity" but also opens up a new path for AI governance: constructing a verifiable and intervenable governance paradigm through "transparency of interaction protocols" rather than prying into the internal mechanisms of models. The framework is open-sourced to promote technology for good and cognitive inclusion. This paper is an independent exploratory research conducted by the author. All content presented, including the theoretical framework (RAMTN), methodology (meta-interaction), system implementation, and case validation, constitutes the author's individual research achievements.

</details>


### [31] [Towards Foundation Models with Native Multi-Agent Intelligence](https://arxiv.org/abs/2512.08743)
*Shuyue Hu,Haoyang Yan,Yiqun Zhang,Yang Chen,Dongzhan Zhou,Lei Bai*

Main category: cs.AI

TL;DR: 论文指出基础模型（FMs）的下一个前沿是赋予其原生多智能体智能，分析其核心能力，实证表明单智能体性能强不自动带来多智能体智能，并给出研究方向。


<details>
  <summary>Details</summary>
Motivation: 当前虽有赋予FMs单智能体能力的工作，但作者认为下一个前沿是赋予其原生多智能体智能。

Method: 对41个大语言模型进行实证研究。

Result: 强单智能体性能不会自动产生强大的多智能体智能。

Conclusion: 提出构建具有原生多智能体智能FMs的关键研究方向，涵盖数据集构建、评估、训练范式和安全考量等。

Abstract: Foundation models (FMs) are increasingly assuming the role of the "brain" of AI agents. While recent efforts have begun to equip FMs with native single-agent abilities -- such as GUI interaction or integrated tool use -- we argue that the next frontier is endowing FMs with native multi-agent intelligence. We identify four core capabilities of FMs in multi-agent contexts: understanding, planning, efficient communication, and adaptation. Contrary to assumptions about the spontaneous emergence of such abilities, we provide extensive empirical evidence across 41 large language models showing that strong single-agent performance alone does not automatically yield robust multi-agent intelligence. To address this gap, we outline key research directions -- spanning dataset construction, evaluation, training paradigms, and safety considerations -- for building FMs with native multi-agent intelligence.

</details>


### [32] [Performance Comparison of Aerial RIS and STAR-RIS in 3D Wireless Environments](https://arxiv.org/abs/2512.08755)
*Dongdong Yang,Bin Li,Jiguang He*

Main category: cs.AI

TL;DR: 论文对比了空中RIS和STAR - RIS在三维无线环境中的性能，建立信道模型，优化系统和速率，发现低高度STAR - RIS优，高高度基站附近RIS优。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对空中RIS和STAR - RIS架构的全面性能比较，作者旨在填补这一空白。

Method: 建立含定向辐射模式的准确信道模型，为两种架构制定联合优化问题，用加权最小均方误差和块坐标下降算法求解。

Result: 低海拔场景中STAR - RIS因全空间覆盖能力优于RIS，高海拔基站附近RIS性能更好。

Conclusion: 研究结果为未来6G通信系统中空中智能表面的部署提供实用见解。

Abstract: Reconfigurable intelligent surface (RIS) and simultaneously transmitting and reflecting RIS (STAR-RIS) have emerged as key enablers for enhancing wireless coverage and capacity in next-generation networks. When mounted on unmanned aerial vehicles (UAVs), they benefit from flexible deployment and improved line-of-sight conditions. Despite their promising potential, a comprehensive performance comparison between aerial RIS and STAR-RIS architectures has not been thoroughly investigated. This letter presents a detailed performance comparison between aerial RIS and STAR-RIS in three-dimensional wireless environments. Accurate channel models incorporating directional radiation patterns are established, and the influence of deployment altitude and orientation is thoroughly examined. To optimize the system sum-rate, we formulate joint optimization problems for both architectures and propose an efficient solution based on the weighted minimum mean square error and block coordinate descent algorithms. Simulation results reveal that STAR-RIS outperforms RIS in low-altitude scenarios due to its full-space coverage capability, whereas RIS delivers better performance near the base station at higher altitudes. The findings provide practical insights for the deployment of aerial intelligent surfaces in future 6G communication systems.

</details>


### [33] [A Practical Guide for Designing, Developing, and Deploying Production-Grade Agentic AI Workflows](https://arxiv.org/abs/2512.08769)
*Eranga Bandara,Ross Gore,Peter Foytik,Sachin Shetty,Ravi Mukkamala,Abdul Rahman,Xueping Liang,Safdar H. Bouk,Amin Hass,Sachini Rajapakse,Ng Wee Keong,Kasun De Zoysa,Aruna Withanage,Nilaan Loganathan*

Main category: cs.AI

TL;DR: 本文提供生产级智能体AI系统设计、开发和部署的端到端指南，介绍工程生命周期和最佳实践，并通过案例展示。


<details>
  <summary>Details</summary>
Motivation: 随着智能体AI在行业和研究中加速应用，组织面临设计、工程和运营可靠、可观测、可维护且符合安全治理要求的生产级智能体AI工作流的挑战。

Method: 引入涵盖工作流分解、多智能体设计模式等的结构化工程生命周期，提出九条工程生产级智能体AI工作流的最佳实践。

Result: 通过多模态新闻分析和媒体生成工作流的案例研究展示了这些原则的实际应用。

Conclusion: 本文结合架构指导、运营模式和实践实施见解，为构建健壮、可扩展且适用于生产的智能体AI工作流提供基础参考。

Abstract: Agentic AI marks a major shift in how autonomous systems reason, plan, and execute multi-step tasks. Unlike traditional single model prompting, agentic workflows integrate multiple specialized agents with different Large Language Models(LLMs), tool-augmented capabilities, orchestration logic, and external system interactions to form dynamic pipelines capable of autonomous decision-making and action. As adoption accelerates across industry and research, organizations face a central challenge: how to design, engineer, and operate production-grade agentic AI workflows that are reliable, observable, maintainable, and aligned with safety and governance requirements. This paper provides a practical, end-to-end guide for designing, developing, and deploying production-quality agentic AI systems. We introduce a structured engineering lifecycle encompassing workflow decomposition, multi-agent design patterns, Model Context Protocol(MCP), and tool integration, deterministic orchestration, Responsible-AI considerations, and environment-aware deployment strategies. We then present nine core best practices for engineering production-grade agentic AI workflows, including tool-first design over MCP, pure-function invocation, single-tool and single-responsibility agents, externalized prompt management, Responsible-AI-aligned model-consortium design, clean separation between workflow logic and MCP servers, containerized deployment for scalable operations, and adherence to the Keep it Simple, Stupid (KISS) principle to maintain simplicity and robustness. To demonstrate these principles in practice, we present a comprehensive case study: a multimodal news-analysis and media-generation workflow. By combining architectural guidance, operational patterns, and practical implementation insights, this paper offers a foundational reference to build robust, extensible, and production-ready agentic AI workflows.

</details>


### [34] [CARLoS: Retrieval via Concise Assessment Representation of LoRAs at Scale](https://arxiv.org/abs/2512.08826)
*Shahar Sarfaty,Adi Haviv,Uri Hacohen,Niva Elkin-Koren,Roi Livni,Amit H. Bermano*

Main category: cs.AI

TL;DR: 提出CARLoS框架对LoRAs进行无额外元数据表征，开发高效检索框架并在多方面评估中表现出色，还能用于版权相关分析。


<details>
  <summary>Details</summary>
Motivation: 现有LoRAs发现方法依赖不可靠的用户描述或有偏见的流行度指标，影响可用性。

Method: 分析超650个LoRAs，用于不同提示和种子的图像生成评估行为，用CLIP嵌入及与基础模型生成的差异定义三部分表示，开发检索框架。

Result: 检索框架在自动化和人工评估中优于文本基线。

Conclusion: CARLoS是实用系统，对LoRA分析有广泛价值，可用于版权相关考虑。

Abstract: The rapid proliferation of generative components, such as LoRAs, has created a vast but unstructured ecosystem. Existing discovery methods depend on unreliable user descriptions or biased popularity metrics, hindering usability. We present CARLoS, a large-scale framework for characterizing LoRAs without requiring additional metadata. Analyzing over 650 LoRAs, we employ them in image generation over a variety of prompts and seeds, as a credible way to assess their behavior. Using CLIP embeddings and their difference to a base-model generation, we concisely define a three-part representation: Directions, defining semantic shift; Strength, quantifying the significance of the effect; and Consistency, quantifying how stable the effect is. Using these representations, we develop an efficient retrieval framework that semantically matches textual queries to relevant LoRAs while filtering overly strong or unstable ones, outperforming textual baselines in automated and human evaluations. While retrieval is our primary focus, the same representation also supports analyses linking Strength and Consistency to legal notions of substantiality and volition, key considerations in copyright, positioning CARLoS as a practical system with broader relevance for LoRA analysis.

</details>


### [35] [Interpolation in Knowledge Representation](https://arxiv.org/abs/2512.08833)
*Jean Christoph Jung,Patrick Koopmann,Matthias Knorr*

Main category: cs.AI

TL;DR: 文章探讨描述逻辑和逻辑编程两种知识表示形式中插值计算的理论成果和实践方法。


<details>
  <summary>Details</summary>
Motivation: 内插法在知识表示中有众多应用，但很多知识表示形式没有内插法，且实际计算插值具有挑战性。

Method: 深入研究描述逻辑和逻辑编程两种知识表示形式。

Result: 文中未明确提及具体结果。

Conclusion: 文中未明确提及具体结论。

Abstract: Craig interpolation and uniform interpolation have many applications in knowledge representation, including explainability, forgetting, modularization and reuse, and even learning. At the same time, many relevant knowledge representation formalisms do in general not have Craig or uniform interpolation, and computing interpolants in practice is challenging. We have a closer look at two prominent knowledge representation formalisms, description logics and logic programming, and discuss theoretical results and practical methods for computing interpolants.

</details>


### [36] [EcomBench: Towards Holistic Evaluation of Foundation Agents in E-commerce](https://arxiv.org/abs/2512.08868)
*Rui Min,Zile Qiao,Ze Xu,Jiawen Zhai,Wenyu Gao,Xuanzhong Chen,Haozhen Sun,Zhen Zhang,Xinyu Wang,Hong Zhou,Wenbiao Yin,Xuan Zhou,Yong Jiang,Haicheng Liu,Liang Ding,Ling Zou,Yi R.,Fung,Yalong Li,Pengjun Xie*

Main category: cs.AI

TL;DR: 为评估基础代理在现实中的能力，引入了EcomBench这一电商基准测试。


<details>
  <summary>Details</summary>
Motivation: 现有基准多集中于学术或人工设计场景，忽略了实际应用挑战，所以需要在高度实用的电商领域进行评估。

Method: 从全球领先电商生态系统中的真实用户需求构建EcomBench，并由专家精心筛选和标注，覆盖电商多任务类别，定义三个难度等级。

Result: EcomBench为测量现代电商中代理的实际能力提供了严格且动态的测试平台。

Conclusion: EcomBench能有效评估基础代理在现实电商环境中的性能。

Abstract: Foundation agents have rapidly advanced in their ability to reason and interact with real environments, making the evaluation of their core capabilities increasingly important. While many benchmarks have been developed to assess agent performance, most concentrate on academic settings or artificially designed scenarios while overlooking the challenges that arise in real applications. To address this issue, we focus on a highly practical real-world setting, the e-commerce domain, which involves a large volume of diverse user interactions, dynamic market conditions, and tasks directly tied to real decision-making processes. To this end, we introduce EcomBench, a holistic E-commerce Benchmark designed to evaluate agent performance in realistic e-commerce environments. EcomBench is built from genuine user demands embedded in leading global e-commerce ecosystems and is carefully curated and annotated through human experts to ensure clarity, accuracy, and domain relevance. It covers multiple task categories within e-commerce scenarios and defines three difficulty levels that evaluate agents on key capabilities such as deep information retrieval, multi-step reasoning, and cross-source knowledge integration. By grounding evaluation in real e-commerce contexts, EcomBench provides a rigorous and dynamic testbed for measuring the practical capabilities of agents in modern e-commerce.

</details>


### [37] [Same Content, Different Answers: Cross-Modal Inconsistency in MLLMs](https://arxiv.org/abs/2512.08923)
*Angela van Sprang,Laurens Samson,Ana Lucic,Erman Acar,Sennay Ghebreab,Yuki M. Asano*

Main category: cs.AI

TL;DR: 引入REST和REST+基准测试系统评估多模态大语言模型跨模态不一致性，评估15个模型发现不一致程度差异大，多种因素影响模型表现且一致性得分与模态差距有关。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型虽将视觉和语言表示在同一嵌入空间，但无法在两种模态下执行相同任务，缺少系统评估跨模态不一致性的方法。

Method: 引入REST和REST+基准测试，包含三种模态（图像、文本、混合）语义信息相同的样本，对15个MLLMs进行评估。

Result: 最先进的MLLMs不能在不同模态上一致推理，模态不一致程度差异大；渲染文本与图像相互转换无法解决不一致问题；视觉特征和视觉令牌数量影响模型性能；一致性得分与模态差距相关。

Conclusion: 所提出的一致性得分可为跨模态不一致的MLLMs提供机制解释。

Abstract: We introduce two new benchmarks REST and REST+(Render-Equivalence Stress Tests) to enable systematic evaluation of cross-modal inconsistency in multimodal large language models (MLLMs). MLLMs are trained to represent vision and language in the same embedding space, yet they cannot perform the same tasks in both modalities. Our benchmarks contain samples with the same semantic information in three modalities (image, text, mixed) and we show that state-of-the-art MLLMs cannot consistently reason over these different modalities. We evaluate 15 MLLMs and find that the degree of modality inconsistency varies substantially, even when accounting for problems with text recognition (OCR). Neither rendering text as image nor rendering an image as text solves the inconsistency. Even if OCR is correct, we find that visual characteristics (text colour and resolution, but not font) and the number of vision tokens have an impact on model performance. Finally, we find that our consistency score correlates with the modality gap between text and images, highlighting a mechanistic interpretation of cross-modal inconsistent MLLMs.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [38] [Broadband Thermoelectric Energy Harvesting for Wearable Biosensors Using Plasmonic Field-Enhancement and Machine-Learning-Guided Device Optimization](https://arxiv.org/abs/2512.08103)
*Hamidreza Moradi,Melika Filvantorkaman*

Main category: cs.CE

TL;DR: 本文介绍了一种混合热等离子体和热电能收集器，结合多波段等离子体吸收和机器学习优化，提高了可穿戴设备的能量转换效率。


<details>
  <summary>Details</summary>
Motivation: 可穿戴生物传感器需要连续无电池电源，但传统皮肤安装的热电器件受实际环境温差小的限制。

Method: 引入混合热等离子体和热电能量收集器，结合多波段等离子体吸收和机器学习引导优化；设计宽带超表面吸收红外辐射；进行电磁仿真、耦合光热电建模，使用机器学习代理模型。

Result: 局部加热使有效温差从3 - 4摄氏度提高到约13摄氏度，功率密度达0.15 mW/cm²，比现有设备提高4 - 6倍；机器学习模型预测准确度高。

Conclusion: 提出的混合框架为高效、紧凑、灵活的可穿戴能量收集器提供了可扩展途径。

Abstract: Wearable biosensors increasingly require continuous and battery-free power sources, but conventional skin-mounted thermoelectric generators are limited by the small temperature differences available in real environments. This work introduces a hybrid thermoplasmonic and thermoelectric energy harvester that combines multiband plasmonic absorption with machine-learning-guided optimization to improve on-body energy conversion. A broadband metasurface made of cross-bowtie nanoantennas is designed to absorb infrared radiation across the 2 to 12 micron range, capturing human body emission, ambient infrared radiation, and near-infrared sunlight. Electromagnetic simulations show strong field enhancement in nanoscale antenna gaps, producing localized thermoplasmonic heating directly above flexible Bi2Te3 thermoelectric junctions. Coupled optical, thermal, and electrical modeling indicates that this localized heating increases the effective temperature difference from the typical 3 to 4 degrees C of standard wearable thermoelectric generators to approximately 13 degrees C. This results in a power density of about 0.15 mW per cm^2 under indoor-relevant infrared flux, representing a four- to six-fold improvement over existing flexible devices. A machine-learning surrogate model trained on multiphysics data predicts temperature rise and electrical output with high accuracy (R2 greater than 0.92) and identifies optimal device geometries through Pareto-front analysis. The proposed hybrid thermoplasmonic, thermoelectric, and machine-learning framework provides a scalable route toward more efficient, compact, and flexible energy harvesters for autonomous and long-term wearable physiological monitoring.

</details>


### [39] [Dflow-SUR: Enhancing Generative Aerodynamic Inverse Design using Differentiation Throughout Flow Matching](https://arxiv.org/abs/2512.08336)
*Aobo Yang,Zhen Wei,Rhea Liem,Pascal Fua*

Main category: cs.CE

TL;DR: 本文提出Dflow - SUR策略解决当前能量基方法的异步问题，在翼型和机翼设计中表现出色，是有前景的气动设计框架。


<details>
  <summary>Details</summary>
Motivation: 当前能量基方法存在异步现象，物理损失优化受流匹配推理过程限制，需改进。

Method: 引入Dflow - SUR，将物理损失优化与流匹配推理分离。

Result: 相比先进能量基基线，在翼型案例中物理损失降低四个数量级，时间减少74%；机翼设计中升阻比提高11.8%，还有额外三个实用优势。

Conclusion: Dflow - SUR是很有前景的框架，为生成式气动设计提供可扩展性和高保真度。

Abstract: Generative inverse design requires incorporating physical constraints to ensure that generated designs are both reliable and accurate. However, we observe that current state-of-the-art energy-based methods suffer from an asynchronous phenomenon, where the optimization of the physical loss is constrained by the flow matching inference process. To overcome this limitation, we introduce Dflow-SUR, a differentiation strategy that separates the optimization of the physical loss from the flow matching inference.
  Compared to the most advanced energy-based baseline, Dflow-SUR achieves a reduction in physical loss by four orders of magnitude, while also cutting wall-clock time by 74% on the airfoil case. Additionally, it increases the mean lift-to-drag ratio by 11.8% over traditional Latin-hypercube sampling in wing design. Beyond improvements in accuracy and efficiency, Dflow-SUR offers three additional practical advantages: (i) enhanced control over guidance, (ii) lower surrogate uncertainty, and (iii) greater robustness to hyper-parameter tuning.
  Together, these results demonstrate that Dflow-SUR is a highly promising framework, providing both scalability and high fidelity for generative aerodynamic design.

</details>


### [40] [Mechanical behaviour of brain-skull interface (meninges) under shear loading through experiment and finite element modelling: Preliminary results](https://arxiv.org/abs/2512.08425)
*Sajjad Arzemanzadeh,Karol Miller,Tim Rosenow,Sjoerd B. Vos,Adam Wittek*

Main category: cs.CE

TL;DR: 本文提出结合实验测试与计算建模的改进方案，确定剪切载荷下脑 - 颅骨界面力学特性，为改进计算头部模型生物逼真度提供基础。


<details>
  <summary>Details</summary>
Motivation: 因实验数据有限，计算模型常简化脑 - 颅骨界面，需确定其力学特性以改进模型。

Method: 从羊尸体头部提取脑组织和脑 - 颅骨复合物样本进行剪切加载，用 MRI 获取样本 3D 几何形状，用有限元模型模拟实验，用二阶 Ogden 超弹性模型描述脑组织，用内聚层模拟脑 - 颅骨界面。

Result: 内聚层能捕捉脑 - 颅骨界面力 - 位移和损伤起始，校准的内聚特性在样本间有一致模式，最大法向牵引力 2.8 - 3.4 kPa，最大切向牵引力 1.8 - 2.1 kPa。

Conclusion: 该框架可通过用实验数据推导的公式替代任意边界条件，改进用于损伤预测和神经外科规划的计算头部模型生物逼真度。

Abstract: The brain-skull interface (meninges) plays a critical role in governing brain motion during head impacts, yet computational models often simplify this interface using idealized contact conditions due to limited experimental data. This study presents an improved protocol combining experimental testing and computational modelling to determine the mechanical properties of the brain-skull interface under shear loading. Brain tissue and brain-skull complex samples were extracted from sheep cadaver heads and subjected to shear loading. Magnetic resonance imaging (MRI) was used to obtain accurate 3D geometries of the samples, which were then used to create computational grids (meshes) for simulation of the experiments using finite element (FE) models to determine subject-specific properties of the brain tissue and brain-skull interface. A second-order Ogden hyperelastic model was used for the brain tissue, and a cohesive layer was employed to model the brain-skull interface. Our results indicate that a cohesive layer captures the force-displacement and damage initiation of the brain-skull interface. The calibrated cohesive properties showed consistent patterns across samples, with maximum normal tractions ranging from 2.8-3.4 kPa and maximum tangential tractions from 1.8-2.1 kPa. This framework provides a foundation for improving the biofidelity of computational head models used in injury prediction and neurosurgical planning by replacing arbitrary boundary conditions with formulations derived from experimental data on brain-skull interface (meninges) biomechanical behaviour.

</details>


### [41] [Financial News Summarization: Can extractive methods still offer a true alternative to LLMs?](https://arxiv.org/abs/2512.08764)
*Nicolas Reche,Elvys Linhares-Pontes,Juan-Manuel Torres-Moreno*

Main category: cs.CE

TL;DR: 金融市场变化快，自动化金融新闻摘要必要。研究评估多种摘要方法，大语言模型生成摘要质量高但有缺陷，提取式方法对特定文本有效，微调模型ROUGE结果佳但数据可靠性有限。


<details>
  <summary>Details</summary>
Motivation: 金融市场变化快，每日金融文章数量多，需要自动化摘要辅助投资者决策。

Method: 使用FinLLMs Challenge数据集评估从简单提取式技术到先进大语言模型等一系列摘要方法。

Result: 大语言模型生成的摘要更连贯、信息更丰富，但资源消耗大且易产生幻觉；提取式方法在短且结构良好的文本上表现出色；微调的FT - Mistral - 7B模型ROUGE结果最佳。

Conclusion: 虽然微调模型ROUGE结果好，但数据语料可靠性有限，需谨慎解读。

Abstract: Financial markets change rapidly due to news, economic shifts, and geopolitical events. Quick reactions are vital for investors to avoid losses or capture short-term gains. As a result, concise financial news summaries are critical for decision-making. With over 50,000 financial articles published daily, automation in summarization is necessary. This study evaluates a range of summarization methods, from simple extractive techniques to advanced large language models (LLMs), using the FinLLMs Challenge dataset. LLMs generated more coherent and informative summaries, but they are resource-intensive and prone to hallucinations, which can introduce significant errors into financial summaries. In contrast, extractive methods perform well on short, well-structured texts and offer a more efficient alternative for this type of article. The best ROUGE results come from fine-tuned LLM model like FT-Mistral-7B, although our data corpus has limited reliability, which calls for cautious interpretation.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [42] [NeurIDA: Dynamic Modeling for Effective In-Database Analytics](https://arxiv.org/abs/2512.08483)
*Lingze Zeng,Naili Xing,Shaofeng Cai,Peng Lu,Gang Chen,Jian Pei,Beng Chin Ooi*

Main category: cs.DB

TL;DR: 文章指出传统ML模型与RDBMS环境不匹配问题，提出NeurIDA系统用于数据库内分析，实验显示其有良好性能。


<details>
  <summary>Details</summary>
Motivation: 随着预测分析需求增长，需将ML深度集成到RDBMS，但传统ML模型静态、特定任务，与RDBMS动态环境不匹配，开发成本高限制ML在分析中的广泛应用。

Method: 提出动态数据库内建模范式，预训练可组合的基础模型架构，接收任务时动态选择和配置基础模型组件；支持自然语言查询，用LLM代理生成分析报告。

Result: 在五个真实数据集的十个任务中，NeurIDA的AUC - ROC最多提高12%，MAE相对降低25%。

Conclusion: NeurIDA实现了易于使用且高效的数据库内AI分析。

Abstract: Relational Database Management Systems (RDBMS) manage complex, interrelated data and support a broad spectrum of analytical tasks. With the growing demand for predictive analytics, the deep integration of machine learning (ML) into RDBMS has become critical. However, a fundamental challenge hinders this evolution: conventional ML models are static and task-specific, whereas RDBMS environments are dynamic and must support diverse analytical queries. Each analytical task entails constructing a bespoke pipeline from scratch, which incurs significant development overhead and hence limits wide adoption of ML in analytics.
  We present NeurIDA, an autonomous end-to-end system for in-database analytics that dynamically "tweaks" the best available base model to better serve a given analytical task. In particular, we propose a novel paradigm of dynamic in-database modeling to pre-train a composable base model architecture over the relational data. Upon receiving a task, NeurIDA formulates the task and data profile to dynamically select and configure relevant components from the pool of base models and shared model components for prediction. For friendly user experience, NeurIDA supports natural language queries; it interprets user intent to construct structured task profiles, and generates analytical reports with dedicated LLM agents. By design, NeurIDA enables ease-of-use and yet effective and efficient in-database AI analytics. Extensive experiment study shows that NeurIDA consistently delivers up to 12% improvement in AUC-ROC and 25% relative reduction in MAE across ten tasks on five real-world datasets. The source code is available at https://github.com/Zrealshadow/NeurIDA

</details>


### [43] [Analyzing Deviations from Monotonic Trends through Database Repair](https://arxiv.org/abs/2512.08526)
*Shunit Agmon,Jonathan Gal,Amir Gilad,Ester Livshits,Or Mutay,Brit Youngmann,Benny Kimelfeld*

Main category: cs.DB

TL;DR: 提出Aggregate Order Dependencies (AODs)量化数据集与单调趋势偏差，研究AOD修复问题并给出算法，实验验证算法效率及分析启发式方法性能。


<details>
  <summary>Details</summary>
Motivation: 解决量化数据集与预期单调趋势偏差的问题。

Method: 引入AOD，将AOD修复问题转化为寻找最小删除元组集，分析计算复杂度，提出通用算法模板并实例化，引入优化技术和启发式替代方法。

Result: 实验表明算法在实际应用中有效，提供了启发式方法性能的见解，案例研究揭示并解释了意外的AOD违规情况。

Conclusion: 所提出的方法能够有效量化数据集与单调趋势的偏差，并对AOD违规情况进行处理和分析。

Abstract: Datasets often exhibit violations of expected monotonic trends - for example, higher education level correlating with higher average salary, newer homes being more expensive, or diabetes prevalence increasing with age. We address the problem of quantifying how far a dataset deviates from such trends. To this end, we introduce Aggregate Order Dependencies (AODs), an aggregation-centric extension of the previously studied order dependencies. An AOD specifies that the aggregated value of a target attribute (e.g., mean salary) should monotonically increase or decrease with the grouping attribute (e.g., education level).
  We formulate the AOD repair problem as finding the smallest set of tuples to delete from a table so that the given AOD is satisfied. We analyze the computational complexity of this problem and propose a general algorithmic template for solving it. We instantiate the template for common aggregation functions, introduce optimization techniques that substantially improve the runtime of the template instances, and develop efficient heuristic alternatives. Our experimental study, carried out on both real-world and synthetic datasets, demonstrates the practical efficiency of the algorithms and provides insight into the performance of the heuristics. We also present case studies that uncover and explain unexpected AOD violations using our framework.

</details>


### [44] [Causal Explanations for Disparate Trends: Where and Why?](https://arxiv.org/abs/2512.08679)
*Tal Blau,Brit Youngmann,Anna Fariha,Yuval Moskovitch*

Main category: cs.DB

TL;DR: 介绍ExDis框架用于发现两组数据间差异的因果解释，通过实验证明其有效。


<details>
  <summary>Details</summary>
Motivation: 分析数据时需解释两组数据差异的区域和原因，且解释要有可解释性和可操作性，需自动系统完成该任务。

Method: 正式定义ExDis框架及相关优化问题，分析复杂度并开发高效算法。

Result: 在三个真实数据集上实验，ExDis能生成有意义因果解释，优于先前方法，可处理高维大数据集。

Conclusion: ExDis是有效的发现两组数据差异因果解释的框架。

Abstract: During data analysis, we are often perplexed by certain disparities observed between two groups of interest within a dataset. To better understand an observed disparity, we need explanations that can pinpoint the data regions where the disparity is most pronounced, along with its causes, i.e., factors that alleviate or exacerbate the disparity. This task is complex and tedious, particularly for large and high-dimensional datasets, demanding an automatic system for discovering explanations (data regions and causes) of an observed disparity. It is critical that explanations for disparities are not only interpretable but also actionable-enabling users to make informed, data-driven decisions. This requires explanations to go beyond surface-level correlations and instead capture causal relationships. We introduce ExDis, a framework for discovering causal Explanations for Disparities between two groups of interest. ExDis identifies data regions (subpopulations) where disparities are most pronounced (or reversed), and associates specific factors that causally contribute to the disparity within each identified data region. We formally define the ExDis framework and the associated optimization problem, analyze its complexity, and develop an efficient algorithm to solve the problem. Through extensive experiments over three real-world datasets, we demonstrate that ExDis generates meaningful causal explanations, outperforms prior methods, and scales effectively to handle large, high-dimensional datasets.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [45] [Modeling the Potential of Message-Free Communication via CXL.mem](https://arxiv.org/abs/2512.08005)
*Stepan Vanecek,Matthew Turner,Manisha Gajbe,Matthew Wolf,Martin Schulz*

Main category: cs.DC

TL;DR: 本文提出结合扩展性能模型的新型性能评估工具链，分析MPI应用程序数据访问模式以预测使用CXL.mem数据交换的性能收益，并在两个应用中验证。


<details>
  <summary>Details</summary>
Motivation: 异构内存技术对解决HPC系统内存墙问题重要，CXL.mem可实现多节点共享内存池，但缺乏评估其数据交换性能收益的工具。

Method: 分析MPI应用的数据访问模式，结合数据构建扩展性能模型，扩展Mitos工具提取数据访问行为并自动分析生成单个MPI调用的性能模型。

Result: 在2D传热小程序和HPCG基准测试两个应用上验证了模型，并展示了通过集成CXL.mem进行针对性优化的支持。

Conclusion: 所提出的工具链和性能模型可有效预测CXL.mem在数据交换中的性能收益，支持针对MPI调用进行优化。

Abstract: Heterogeneous memory technologies are increasingly important instruments in addressing the memory wall in HPC systems. While most are deployed in single node setups, CXL.mem is a technology that implements memories that can be attached to multiple nodes simultaneously, enabling shared memory pooling. This opens new possibilities, particularly for efficient inter-node communication.
  In this paper, we present a novel performance evaluation toolchain combined with an extended performance model for message-based communication, which can be used to predict potential performance benefits from using CXL.mem for data exchange. Our approach analyzes data access patterns of MPI applications: it analyzes on-node accesses to/from MPI buffers, as well as cross-node MPI traffic to gather a full understanding of the impact of memory performance. We combine this data in an extended performance model to predict which data transfers could benefit from direct CXL.mem implementations as compared to traditional MPI messages. Our model works on a per-MPI call granularity, allowing the identification and later optimizations of those MPI invocations in the code with the highest potential for speedup by using CXL.mem.
  For our toolchain, we extend the memory trace sampling tool Mitos and use it to extract data access behavior. In the post-processing step, the raw data is automatically analyzed to provide performance models for each individual MPI call. We validate the models on two sample applications -- a 2D heat transfer miniapp and the HPCG benchmark -- and use them to demonstrate their support for targeted optimizations by integrating CXL.mem.

</details>


### [46] [CapsuleFS A Multi-credential DataCapsule Filesystem](https://arxiv.org/abs/2512.08067)
*Qingyang Hu,Yucheng Huang,Manshi Yang*

Main category: cs.DC

TL;DR: CapsuleFS (CFS) 是首个在 POSIX 兼容框架内集成多凭证功能的文件系统，基于边缘计算的全球数据平面构建，虽读写性能一般但功能正确性高，适用于软件开发场景。


<details>
  <summary>Details</summary>
Motivation: 在 POSIX 兼容框架内实现多凭证通用访问 API。

Method: 将 CFS 架构分为 DataCapsule 服务器、运行在可信执行环境的中间件和 POSIX 兼容的客户端组件。

Result: CFS 读写性能相对一般，但功能正确性高。

Conclusion: CFS 可用于实际软件开发场景，论文还提出未来改进方向以提高其实用性。

Abstract: CapsuleFS (CFS) is the first filesystem to integrate multi-credential functionality within a POSIX-compliant framework, utilizing DataCapsule as the storage provider. This innovative system is established based on the Global Data Plane in the area of edge computing. Our comprehensive design and implementation of CFS successfully fulfill the objective of providing a multi-credential Common Access API. The architecture of CFS is methodically segmented into three integral components: Firstly, the DataCapsule server, tasked with the storage, dissemination, and replication of DataCapsules on the edge. Secondly, the middleware, a crucial element running in a Trusted Execution Environment responsible for the enforcement and management of write permissions and requests. Finally, the client component, which manifests as a POSIX-compliant filesystem, is adaptable and operational across many architectures. Experimental evaluations of CFS reveal that, while its read and write performances are comparatively modest, it upholds a high degree of functional correctness. This attribute distinctly positions CFS as a viable candidate for application in real-world software development scenarios. The paper also delineates potential future enhancements, aimed at augmenting the practicality of CFS in the landscape of software development.

</details>


### [47] [Chopper: A Multi-Level GPU Characterization Tool & Derived Insights Into LLM Training Inefficiency](https://arxiv.org/abs/2512.08242)
*Marco Kurzynski,Shaizeen Aga,Di Wu*

Main category: cs.DC

TL;DR: 介绍Chopper框架对Llama 3 8B在八GPU AMD节点上训练进行分析，揭示未被充分探索的瓶颈，指出频率开销是性能差距最大来源，为优化提供见解。


<details>
  <summary>Details</summary>
Motivation: 现有工作对多GPU LLM训练中通信、计算、内存行为和电源管理的复杂交互特征描述不足，需要深入理解现代GPU系统在实际分布式训练工作负载下的表现以高效训练LLMs。

Method: 引入Chopper框架，收集、对齐和可视化多粒度的GPU内核跟踪和硬件性能计数器，对Llama 3 8B在八GPU AMD节点上进行端到端分析。

Result: 揭示了此前未充分探索的瓶颈和行为，如内存确定性可实现更高更稳定的GPU和内存频率；确定频率开销是理论和实际性能差距的最大来源。

Conclusion: Chopper为AMD InstinctTM MI300X GPU上的LLM训练提供了首个整体、多粒度的特征描述，为优化训练框架、改进电源管理策略和指导未来GPU架构与系统设计提供了可操作的见解。

Abstract: Training large language models (LLMs) efficiently requires a deep understanding of how modern GPU systems behave under real-world distributed training workloads. While prior work has focused primarily on kernel-level performance or single-GPU microbenchmarks, the complex interaction between communication, computation, memory behavior, and power management in multi-GPU LLM training remains poorly characterized. In this work, we introduce Chopper, a profiling and analysis framework that collects, aligns, and visualizes GPU kernel traces and hardware performance counters across multiple granularities (i.e., from individual kernels to operations, layers, phases, iterations, and GPUs). Using Chopper, we perform a comprehensive end-to-end characterization of Llama 3 8B training under fully sharded data parallelism (FSDP) on an eight-GPU AMD InstinctTM MI300X node. Our analysis reveals several previously underexplored bottlenecks and behaviors, such as memory determinism enabling higher, more stable GPU and memory frequencies. We identify several sources of inefficiencies, with frequency overhead (DVFS effects) being the single largest contributor to the gap between theoretical and observed performance, exceeding the impact of MFMA utilization loss, communication/computation overlap, and kernel launch overheads. Overall, Chopper provides the first holistic, multi-granularity characterization of LLM training on AMD InstinctTM MI300X GPUs, yielding actionable insights for optimizing training frameworks, improving power-management strategies, and guiding future GPU architecture and system design.

</details>


### [48] [Synergizing Monetization, Orchestration, and Semantics in Computing Continuum](https://arxiv.org/abs/2512.08288)
*Chinmaya Kumar Dehury,Lauri Lovén,Praveen Kumar Donta,Ilir Murturi,Schahram Dustdar*

Main category: cs.DC

TL;DR: 行业对云到边缘的超分布式应用需求增长，现有方案有局限，本文介绍HERMES框架解决问题。


<details>
  <summary>Details</summary>
Motivation: 现有云到边缘超分布式应用方案在可扩展性、互操作性和信任方面存在固有局限，无法满足行业需求。

Method: 提出HERMES框架，建立开放、无缝、安全的环境，智能编排资源，在分布式市场对数据和服务进行货币化，通过语义互操作性共享知识。

Result: HERMES为连接和数据利用带来了变革。

Conclusion: HERMES为新一代更高效、可信和自主的分布式应用奠定了基础。

Abstract: Industry demands are growing for hyper-distributed applications that span from the cloud to the edge in domains such as smart manufacturing, transportation, and agriculture. Yet today's solutions struggle to meet these demands due to inherent limitations in scalability, interoperability, and trust. In this article, we introduce HERMES (Heterogeneous Computing Continuum with Resource Monetization, Orchestration, and Semantic) - a novel framework designed to transform connectivity and data utilization across the computing continuum. HERMES establishes an open, seamless, and secure environment where resources, from cloud servers to tiny edge devices, can be orchestrated intelligently, data and services can be monetized in a distributed marketplace, and knowledge is shared through semantic interoperability. By bridging these key facets, HERMES lays a foundation for a new generation of distributed applications that are more efficient, trustworthy, and autonomous.

</details>


### [49] [Emulation of Complex Matrix Multiplication based on the Chinese Remainder Theorem](https://arxiv.org/abs/2512.08321)
*Yuki Uchino,Qianxiang Ma,Toshiyuki Imamura,Katsuhisa Ozaki,Patrick Lars Gutsche*

Main category: cs.DC

TL;DR: 基于Ozaki - II方案提出在INT8矩阵引擎上对单双精度复矩阵乘法的高性能仿真方法，在NVIDIA B200 GPU上有显著加速，不同精度场景表现良好，有潜力成通用算法。


<details>
  <summary>Details</summary>
Motivation: 现代计算架构中低精度矩阵乘法单元吞吐量高，高性能计算领域对用低精度硬件仿真高精度矩阵乘法感兴趣。

Method: 基于Ozaki - II方案，提出在INT8矩阵引擎上对单双精度复矩阵乘法的高性能仿真方法。

Result: 在NVIDIA B200 GPU上，大问题规模时，相比cuBLAS原生单双精度复矩阵乘法例程分别有4.0x - 5.6x和4.4x - 6.5x加速；可在不同精度场景下灵活调整性能。

Conclusion: 所提方法有潜力成为广泛应用场景下的默认算法。

Abstract: Modern computing architectures feature low-precision matrix multiplication units that achieve substantially higher throughput than their high-precision counterparts. Motivated by this architectural trend, the emulation of high-precision matrix multiplication using low-precision hardware has attracted significant interest in the high-performance computing community. Ozaki, Uchino, and Imamura introduced the Ozaki-II scheme as a general framework for emulating matrix multiplication. Building on this framework, Uchino, Ozaki, and Imamura developed high-performance and power-efficient techniques for emulating single- and double-precision real matrix multiplication on INT8 matrix engines. Extending this line of research, the present study proposes high-performance emulation methods for single- and double-precision complex matrix multiplication on INT8 matrix engines, based on the Ozaki-II scheme. On an NVIDIA B200 GPU, the proposed methods achieve 4.0x--5.6x and 4.4x--6.5x speedups over the native single- and double-precision complex matrix multiplication routines from cuBLAS, respectively, for sufficiently large problem sizes. When lower accuracy than that of the standard routine is acceptable, the proposed methods can operate at even higher speed. Conversely, with only a modest increase in computation time, they can also deliver higher accuracy than the standard routines. These properties suggest that the proposed approach has the potential to serve as a default algorithm across a wide range of applications.

</details>


### [50] [Magneton: Optimizing Energy Efficiency of ML Systems via Differential Energy Debugging](https://arxiv.org/abs/2512.08365)
*Yi Pan,Wenbo Qian,Dedong Xie,Ruiyan Hu,Yigong Hu,Baris Kasikci*

Main category: cs.DC

TL;DR: 揭示机器学习模型软件能耗浪费问题，提出差分能耗调试方法，Magneton工具能检测和诊断低能效问题。


<details>
  <summary>Details</summary>
Motivation: 现有优化多关注硬件能效，软件设计缺陷导致能源浪费问题被忽视，开发者缺乏检测诊断工具。

Method: 提出差分能耗调试方法，设计实现Magneton工具，在算子层面比较类似ML系统能耗，定位高能耗代码区域和配置选项。

Result: Magneton应用于9个流行ML系统，检测并诊断了16个已知软件能耗低效案例，发现8个未知案例，7个获开发者确认。

Conclusion: 差分能耗调试方法和Magneton工具可有效检测和诊断机器学习软件的能耗问题。

Abstract: The training and deployment of machine learning (ML) models have become extremely energy-intensive. While existing optimization efforts focus primarily on hardware energy efficiency, a significant but overlooked source of inefficiency is software energy waste caused by poor software design. This often includes redundant or poorly designed operations that consume more energy without improving performance. These inefficiencies arise in widely used ML frameworks and applications, yet developers often lack the visibility and tools to detect and diagnose them.
  We propose differential energy debugging, a novel approach that leverages the observation that competing ML systems often implement similar functionality with vastly different energy consumption. Building on this insight, we design and implement Magneton, an energy profiler that compares energy consumption between similar ML systems at the operator level and automatically pinpoints code regions and configuration choices responsible for excessive energy use. Applied to 9 popular ML systems spanning LLM inference, general ML frameworks, and image generation, Magneton detects and diagnoses 16 known cases of software energy inefficiency and further discovers 8 previously unknown cases, 7 of which have been confirmed by developers.

</details>


### [51] [Basic Lock Algorithms in Lightweight Thread Environments](https://arxiv.org/abs/2512.08563)
*Taras Skazhenik,Nikolai Korobenikov,Andrei Churbanov,Anton Malakhov,Vitaly Aksenov*

Main category: cs.DC

TL;DR: 传统多线程数据结构为操作系统线程设计，轻量级线程相关实现研究不足。本文聚焦互斥锁，提出修改版TTAS和MCS锁，指出轻量级线程上下文切换机制重要，推荐使用队列锁。


<details>
  <summary>Details</summary>
Motivation: 传统多线程数据结构在轻量级线程场景研究不足，原操作系统线程实现可能导致死锁，不同轻量级线程库需不同锁实现。

Method: 对TTAS和MCS锁进行修改以适用于轻量级线程，分析两种上下文切换机制。

Result: TTAS和MCS锁性能因设置而异，队列锁能平衡两者。

Conclusion: 使用队列锁可在不同轻量级线程库中取得较好效果。

Abstract: Traditionally, multithreaded data structures have been designed for access by the threads of Operating Systems (OS). However, implementations for access by programmable alternatives known as lightweight threads (also referred to as asynchronous calls or coroutines) have not been thoroughly studied. The main advantage of lightweight threads is their significantly lower overhead during launch and context switching. However, this comes at a cost: to achieve proper parallelism, context switches must be manually invoked in the code; without these switches, new lightweight threads will never be executed.
  In this paper, we focus on the simplest multithreaded data structure: a mutex (also known as a lock). We demonstrate that original implementations for OS threads cannot be used effectively in this new context due to the potential for deadlocks. Furthermore, correctness is not the only concern. In certain languages, such as C++, there are various lightweight thread libraries, each with different implementations and interfaces, which necessitate distinct lock implementations.
  In this work, we present a modification of TTAS and MCS locks for the use from lightweight threads and demonstrate that the two context switch mechanisms of lightweight threads, yielding and sleeping, are crucial. However, the performance of TTAS and MCS may differ significantly depending on the settings. If one wants to have a lock that works well for any library, we suggest using the cohort lock, which strikes a balance between MCS and TTAS by utilizing several MCS queues with a common TTAS.

</details>


### [52] [Model-based Testing of Practical Distributed Systems in Actor Model](https://arxiv.org/abs/2512.08698)
*Ilya Kokorin,Evgeny Chernatskiy,Vitaly Aksenov*

Main category: cs.DC

TL;DR: 本文探讨为基于参与者模型的分布式系统高效生成测试套件，以弥合实现与形式规范间差距，且不影响代码和执行环境，还验证了复制算法实现。


<details>
  <summary>Details</summary>
Motivation: 分布式系统设计和实现存在挑战，实现与形式规范间有差距，无法保证实现无缺陷。

Method: 利用基于模型的测试，若系统模型可解释为有限状态自动机，则为采用参与者模型编写的分布式系统生成详尽测试套件，且不修改代码和干扰执行环境。

Result: 以基于Viewstamped Replication的复制算法实现为例完成验证。

Conclusion: 该方法可高效生成涵盖所有可能状态和转换的测试套件，弥合分布式系统实现与形式规范的差距。

Abstract: Designing and implementing distributed systems correctly can be quite challenging. Although these systems are often accompanied by formal specifications that are verified using model-checking techniques, a gap still exists between the implementation and its formal specification: there is no guarantee that the implementation is free of bugs.
  To bridge this gap, we can use model-based testing. Specifically, if the model of the system can be interpreted as a finite-state automaton, we can generate an exhaustive test suite for the implementation that covers all possible states and transitions.
  In this paper, we discuss how to efficiently generate such a test suite for distributed systems written in the actor model. Importantly, our approach does not require any modifications to the code or interfering with the distributed system execution environment. As an example, we verified an implementation of a replication algorithm based on Viewstamped Replication, which is used in a real-world system.

</details>


### [53] [Spatio-Temporal Shifting to Reduce Carbon, Water, and Land-Use Footprints of Cloud Workloads](https://arxiv.org/abs/2512.08725)
*Giulio Attenni,Youssef Moawad,Novella Bartolini,Lauritz Thamsen*

Main category: cs.DC

TL;DR: 研究云工作负载时空转移降低碳、水和土地使用足迹的潜力，模拟表明时空转移结合效果最佳且策略稳健。


<details>
  <summary>Details</summary>
Motivation: 探究云工作负载时空转移在降低碳、水和土地使用足迹方面的潜力。

Method: 使用多个云提供商（AWS和Azure）的真实数据以及不同应用（大数据分析和FaaS）的工作负载轨迹进行模拟研究。

Result: 空间转移可大幅降低足迹，降幅20% - 85%；时间转移也有降低效果但较小；两者结合效果最佳，主要由空间转移驱动，时间调整有额外增量效益；策略对电网混合数据预测误差和不同季节变化具有鲁棒性。

Conclusion: 云工作负载的空间和时间转移策略能有效降低碳、水和土地使用足迹，结合使用效果更好且策略稳健。

Abstract: In this paper, we investigate the potential of spatial and temporal cloud workload shifting to reduce carbon, water, and land-use footprints. Specifically, we perform a simulation study using real-world data from multiple cloud providers (AWS and Azure) and workload traces for different applications (big data analytics and FaaS). Our simulation results indicate that spatial shifting can substantially lower carbon, water, and land use footprints, with observed reductions ranging from 20% to 85%, depending on the scenario and optimization criteria. Temporal shifting also decreases the footprint, though to a lesser extent. When applied together, the two strategies yield the greatest overall reduction, driven mainly by spatial shifting with temporal adjustments providing an additional, incremental benefit. Sensitivity analysis demonstrates that such shifting is robust to prediction errors in grid mix data and to variations across different seasons.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [54] [The Bichromatic Two-Center Problem on Graphs](https://arxiv.org/abs/2512.08111)
*Qi Sun,Jingru Zhang*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In this paper, we study the (weighted) bichromatic two-center problem on graphs. The input consists of a graph $G$ of $n$ (weighted) vertices and $m$ edges, and a set $\mathcal{P}$ of pairs of distinct vertices, where no vertex appears in more than one pair. The problem aims to find two points (i.e., centers) on $G$ by assigning vertices of each pair to different centers so as to minimize the maximum (weighted) distance of vertices to their assigned centers (so that the graph can be bi-colored with this goal). To the best of our knowledge, this problem has not been studied on graphs, including tree graphs. In this paper, we propose an $O(m^2n\log n\log mn)$ algorithm for solving the problem on an undirected graph provided with the distance matrix, an $O(n\log n)$-time algorithm for the problem on trees, and a linear-time approach for the unweighted tree version.

</details>


### [55] [A tight example for approximation ratio 5 for covering small cuts by the primal-dual method](https://arxiv.org/abs/2512.08350)
*Zeev Nutov*

Main category: cs.DS

TL;DR: 证明了在Small Cuts Cover问题中，原始对偶算法近似比5是紧的


<details>
  <summary>Details</summary>
Motivation: 回答Simmons关于原始对偶算法近似比5是否为紧的问题

Method: 提供一个例子，使原始对偶算法的解与最优解的比率任意接近5

Result: 成功给出例子说明原始对偶算法近似比可任意接近5

Conclusion: 原始对偶算法在Small Cuts Cover问题上的近似比5是紧的

Abstract: In the Small Cuts Cover problem we seek to cover by a min-cost edge-set the set family of cuts of size/capacity $<k$ of a graph. Recently, Simmons showed that the primal-dual algorithm of Williamson, Goemans, Mihail, and Vazirani achieves approximation ratio $5$ for this problem, and asked whether this bound is tight. We will answer this question positively, by providing an example in which the ratio between the solution produced by the primal-dual algorithm and the optimum is arbitrarily close to $5$.

</details>


### [56] [A Distribution Testing Approach to Clustering Distributions](https://arxiv.org/abs/2512.08376)
*Gunjan Kumar,Yash Pote,Jonathan Scarlett*

Main category: cs.DS

TL;DR: 研究分布聚类问题中恢复分区，给出两种情况样本复杂度上下界并刻画其与各参数依赖，在多数情况下达到接近紧的结果。


<details>
  <summary>Details</summary>
Motivation: 解决分布聚类问题中恢复隐藏分区的问题，明确样本复杂度。

Method: 建立两种基本情况下样本复杂度的上下界，分析其与域大小、分布数量、簇大小和距离等参数的关系。

Result: 得到了样本复杂度上下界，在多种情况下关于 (n,k,r,ε) 达到接近紧的结果（相差一个 O(log k) 因子）。

Conclusion: 对分布聚类问题的样本复杂度有了清晰刻画和接近最优的结果。

Abstract: We study the following distribution clustering problem: Given a hidden partition of $k$ distributions into two groups, such that the distributions within each group are the same, and the two distributions associated with the two clusters are $\varepsilon$-far in total variation, the goal is to recover the partition. We establish upper and lower bounds on the sample complexity for two fundamental cases: (1) when one of the cluster's distributions is known, and (2) when both are unknown. Our upper and lower bounds characterize the sample complexity's dependence on the domain size $n$, number of distributions $k$, size $r$ of one of the clusters, and distance $\varepsilon$. In particular, we achieve tightness with respect to $(n,k,r,\varepsilon)$ (up to an $O(\log k)$ factor) for all regimes.

</details>


### [57] [Finding All Bounded-Length Simple Cycles in a Directed Graphs -- Revisited](https://arxiv.org/abs/2512.08392)
*Frank Bauernöppel,Jörg-Rüdiger Sack*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In 2021, Gupta and Suzumura proposed a novel algorithm for enumerating all bounded-length simple cycles in directed graphs. In this work, we present concrete examples demonstrating that the proposed algorithm fails to enumerate certain valid cycles. Via these examples, we perform a detailed analysis pinpointing the specific points at which the proofs exhibit logical gaps. Furthermore, we propose a corrected formulation that resolves these issues while preserving the desirable property that the algorithm's computational complexity remains $O((c + 1) \cdot k \cdot (n + e))$ where $c$ is the number of simple cycles of a specified maximum length $k$, and $n$ and $e$ the number of graph nodes and edges respectively.

</details>


### [58] [Weighted $k$-Path and Other Problems in Almost $O^*(2^k)$ Deterministic Time via Dynamic Representative Sets](https://arxiv.org/abs/2512.08583)
*Jesper Nederlof*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We present a data structure that we call a Dynamic Representative Set. In its most basic form, it is given two parameters $0< k < n$ and allows us to maintain a representation of a family $\mathcal{F}$ of subsets of $\{1,\ldots,n\}$. It supports basic update operations (unioning of two families, element convolution) and a query operation that determines for a set $B \subseteq \{1,\ldots,n\}$ whether there is a set $A \in \mathcal{F}$ of size at most $k-|B|$ such that $A$ and $B$ are disjoint. After $2^{k+O(\sqrt{k}\log^2k)}n \log n$ preprocessing time, all operations use $2^{k+O(\sqrt{k}\log^2k)}\log n$ time.
  Our data structure has many algorithmic consequences that improve over previous works. One application is a deterministic algorithm for the Weighted Directed $k$-Path problem, one of the central problems in parameterized complexity. Our algorithm takes as input an $n$-vertex directed graph $G=(V,E)$ with edge lengths and an integer $k$, and it outputs the minimum edge length of a path on $k$ vertices in $2^{k+O(\sqrt{k}\log^2k)}(n+m)\log n$ time (in the word RAM model where weights fit into a single word). Modulo the lower order term $2^{O(\sqrt{k}\log^2k)}$, this answers a question that has been repeatedly posed as a major open problem in the field.

</details>


### [59] [Fast exact algorithms via the Matrix Tree Theorem](https://arxiv.org/abs/2512.08600)
*V. Arvind,Srijan Chakraborty,Samir Datta,Asif Khan*

Main category: cs.DS

TL;DR: 本文提出简单且相似的算法解决无向和有向二部图哈密顿路径问题，还用于计算二部图完美匹配数、图划分为k - 星的方式数及一般图最大匹配数，所有算法在多项式空间运行。


<details>
  <summary>Details</summary>
Motivation: 已有解决无向和有向二部图哈密顿路径的算法虽精巧但复杂且差异大，期望设计简单且相似的算法。

Method: 使用矩阵树定理和单位根筛法，还借助Björklund算法和Gallai - Edmonds分解定理。

Result: 得到解决上述问题的算法，计算图划分为k - 星的运行时间可改进，计算最大匹配数有特定时间复杂度。

Conclusion: 设计的算法简单相似，能解决多个图论问题，且都在多项式空间运行。

Abstract: Fast exact algorithms are known for Hamiltonian paths in undirected and directed bipartite graphs through elegant though involved algorithms that are quite different from each other. We devise algorithms that are simple and similar to each other while having the same upper bounds. The common features of these algorithms is the use of the Matrix-Tree theorem and sieving using roots of unity.
  Next, we use the framework to provide alternative algorithms to count perfect matchings in bipartite graphs on $n$ vertices, i.e., computing the $\{0,1\}$-permanent of a square $n/2 \times n/2$ matrix which runs in a time similar to Ryser.
  We demonstrate the flexibility of our method by counting the number of ways to vertex partition the graph into $k$-stars (a $k$-star consist of a tree with a root having $k-1$ children that are all leaves). Interestingly, our running time improves to $O^*((1+ε_k)^n)$ with $ε_k \rightarrow 0$ as $k \rightarrow \infty$.
  As an aside, making use of Björklund's algorithm for exact counting perfect matchings in general graphs, we show that the count of maximum matchings can be computed in time $O^*(2^ν)$ where $ν$ is the size of a maximum matching. The crucial ingredient here is the famous Gallai-Edmonds decomposition theorem.
  All our algorithms run in polynomial space.

</details>


### [60] [Parallel Batch Dynamic Vertex Coloring in $O(\log Δ)$ Amortized Update Time](https://arxiv.org/abs/2512.08742)
*Chase Hutton,Adam Melrod*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We present the first parallel batch-dynamic algorithm for maintaining a proper $(Δ+ 1)$-vertex coloring. Our approach builds on a new sequential dynamic algorithm inspired by the work of Bhattacharya et al. (SODA'18). The resulting randomized algorithm achieves $O(\log Δ)$ expected amortized update time and, for any batch of $b$ updates, has parallel span $O(\operatorname{polylog} b + \operatorname{polylog} n)$ with high probability.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [61] [The Theory of Strategic Evolution: Games with Endogenous Players and Strategic Replicators](https://arxiv.org/abs/2512.07901)
*Kevin Vallier*

Main category: cs.GT

TL;DR: 本文提出战略演化理论，统一多领域结果，提供长期战略适应的通用数学模型。


<details>
  <summary>Details</summary>
Motivation: 建立一个能让参与者群体、策略和制度规则共同演化的通用模型。

Method: 将复制者动态扩展到具有内生参与者、多层次选择等情境，引入Poiesis堆栈这一核心数学对象。

Result: 在小增益条件下，系统有全局Lyapunov函数，满足选择、跟踪和随机稳定性结果；证明该理论类在多种情况下封闭，封闭定理有相关结论。

Conclusion: 该理论统一了多个领域的结果，为长期战略适应提供通用数学模型。

Abstract: This paper develops the Theory of Strategic Evolution, a general model for systems in which the population of players, strategies, and institutional rules evolve together. The theory extends replicator dynamics to settings with endogenous players, multi level selection, innovation, constitutional change, and meta governance. The central mathematical object is a Poiesis stack: a hierarchy of strategic layers linked by cross level gain matrices. Under small gain conditions, the system admits a global Lyapunov function and satisfies selection, tracking, and stochastic stability results at every finite depth. We prove that the class is closed under block extension, innovation events, heterogeneous utilities, continuous strategy spaces, and constitutional evolution. The closure theorem shows that no new dynamics arise at higher levels and that unrestricted self modification cannot preserve Lyapunov structure. The theory unifies results from evolutionary game theory, institutional design, innovation dynamics, and constitutional political economy, providing a general mathematical model of long run strategic adaptation.

</details>


### [62] [Selling Privacy in Blockchain Transactions](https://arxiv.org/abs/2512.08096)
*Georgios Chionas,Olga Gorelkina,Piotr Krysta,Rida Laraki*

Main category: cs.GT

TL;DR: 从经济学角度研究增强区块链交易隐私的方法，分析两种拍卖场景并提出双边市场的定价机制。


<details>
  <summary>Details</summary>
Motivation: 从经济角度探索增强区块链交易隐私的方法，满足隐私敏感用户需求。

Method: 研究两种拍卖场景（订单流拍卖和荷兰拍卖变体），引入双边隐私市场并提出定价机制。

Result: 得出最优拍卖是密封投标拍卖，比较不同拍卖的收入，提出保证最优社会福利近似、激励兼容和预算平衡的定价机制。

Conclusion: 可通过所研究的拍卖机制和定价机制增强区块链交易隐私，改善经济机制性能。

Abstract: We study methods to enhance privacy in blockchain transactions from an economic angle. We consider mechanisms for privacy-aware users whose utility depends not only on the outcome of the mechanism but also negatively on the exposure of their economic preferences. Specifically, we study two auction-theoretic settings with privacy-aware users. First, we analyze an order flow auction, where a user auctions off to specialized agents, called searchers, the right to execute her transaction while maintaining a degree of privacy. We examine how the degree of privacy affects the revenue of the auction and, broadly, the net utility of the privacy-aware user. In this new setting, we describe the optimal auction, which is a sealed-bid auction. Subsequently, we analyze a variant of a Dutch auction in which the user gradually decreases the price and the degree of privacy until the transaction is sold. We compare the revenue of this auction to that of the optimal one as a function of the number of communication rounds. Then, we introduce a two-sided market - a privacy marketplace - with multiple users selling their transactions under their privacy preferences to multiple searchers. We propose a posted-price mechanism for the two-sided market that guarantees constant approximation of the optimal social welfare while maintaining incentive compatibility (from both sides of the market) and budget balance. This work builds on the emerging line of research that attempts to improve the performance of economic mechanisms by appending cryptographic primitives to them.

</details>


### [63] [Beyond Revenue and Welfare: Counterfactual Analysis of Spectrum Auctions with Application to Canada's 3800MHz Allocation](https://arxiv.org/abs/2512.08106)
*Sara Jalili Shani,Kris Joseph,Michael B. McNally,James R. Wright*

Main category: cs.GT

TL;DR: 本文提出用简单行为模型预测频谱拍卖结果，以加拿大2023年3800MHz频谱拍卖为例，验证模型有效性，并模拟替代机制，显示其可改善服务不足地区覆盖。


<details>
  <summary>Details</summary>
Motivation: 传统频谱拍卖模型依赖强均衡假设，本文采用更简约方法建模，以更好预测拍卖结果和评估替代机制对政策目标的影响。

Method: 将竞标者建模为短视且直接的，用线性规划框架根据逐轮投标数据估计竞标者估值，验证模型后模拟替代机制。

Result: 模型能有效预测加拿大2023年频谱拍卖结果，模拟的替代机制可大幅改善服务不足地区的人口覆盖。

Conclusion: 具有最少假设的行为模型足以进行可靠的反事实预测，为政策制定者评估替代拍卖设计提供实用工具，也为反事实机制设计提供方法。

Abstract: Spectrum auctions are the primary mechanism through which governments allocate scarce radio frequencies, with outcomes that shape competition, coverage, and innovation in telecommunications markets. While traditional models of spectrum auctions often rely on strong equilibrium assumptions, we take a more parsimonious approach by modeling bidders as myopic and straightforward: in each round, firms simply demand the bundle that maximizes their utility given current prices. Despite its simplicity, this model proves effective in predicting the outcomes of Canada's 2023 auction of 3800 MHz spectrum licenses. Using detailed round-by-round bidding data, we estimate bidders' valuations through a linear programming framework and validate that our model reproduces key features of the observed allocation and price evolution. We then use these estimated valuations to simulate a counterfactual auction under an alternative mechanism that incentivizes deployment in rural and remote regions, aligning with one of the key objectives set out in the Canadian Telecommunications Act. The results show that the proposed mechanism substantially improves population coverage in underserved areas. These findings demonstrate that a behavioral model with minimal assumptions is sufficient to generate reliable counterfactual predictions, making it a practical tool for policymakers to evaluate how alternative auction designs may influence future outcomes. In particular, our study demonstrates a method for counterfactual mechanism design, providing a framework to evaluate how alternative auction rules could advance policy goals such as equitable deployment across Canada.

</details>


### [64] [Multi-agent learning under uncertainty: Recurrence vs. concentration](https://arxiv.org/abs/2512.08132)
*Kyriakos Lotidis,Panayotis Mertikopoulos,Nicholas Bambos,Jose Blanchet*

Main category: cs.GT

TL;DR: 本文研究不确定下多智能体学习的收敛情况，分析两种正则化学习的随机模型，表明结果动态通常不收敛，强单调博弈中动态有有利性质，非强单调则可能失效。


<details>
  <summary>Details</summary>
Motivation: 研究不确定下多智能体学习的长期行为，与确定性全信息学习模型对比。

Method: 分析连续博弈中两种正则化学习的随机模型，一个连续时间，一个离散时间。

Result: 结果动态通常不收敛；强单调博弈中，动态会无限次偏离均衡但有限时间返回附近，长期分布集中；非强单调博弈，有利性质可能失效。

Conclusion: 指出正则化学习在持续随机性和不确定性下的局限性，依赖于博弈是否为强单调。

Abstract: In this paper, we examine the convergence landscape of multi-agent learning under uncertainty. Specifically, we analyze two stochastic models of regularized learning in continuous games -- one in continuous and one in discrete time with the aim of characterizing the long-run behavior of the induced sequence of play. In stark contrast to deterministic, full-information models of learning (or models with a vanishing learning rate), we show that the resulting dynamics do not converge in general. In lieu of this, we ask instead which actions are played more often in the long run, and by how much. We show that, in strongly monotone games, the dynamics of regularized learning may wander away from equilibrium infinitely often, but they always return to its vicinity in finite time (which we estimate), and their long-run distribution is sharply concentrated around a neighborhood thereof. We quantify the degree of this concentration, and we show that these favorable properties may all break down if the underlying game is not strongly monotone -- underscoring in this way the limits of regularized learning in the presence of persistent randomness and uncertainty.

</details>


### [65] [Robust equilibria in continuous games: From strategic to dynamic robustness](https://arxiv.org/abs/2512.08138)
*Kyriakos Lotidis,Panayotis Mertikopoulos,Nicholas Bambos,Jose Blanchet*

Main category: cs.GT

TL;DR: 文章研究连续博弈中纳什均衡在战略和动态不确定性下的鲁棒性，建立两种鲁棒性的结构对应，还研究收敛速率。


<details>
  <summary>Details</summary>
Motivation: 探究连续博弈中纳什均衡在战略和动态不确定性下的鲁棒性特点。

Method: 先定义战略鲁棒均衡、进行几何刻画，再研究动态鲁棒性，探讨两种鲁棒性的关系，最后研究收敛速率和正则化器的关系。

Result: 建立战略鲁棒性和动态鲁棒性的结构对应，表明对熵正则化学习在仿射约束行动空间博弈中有几何收敛速率。

Conclusion: 两种鲁棒性之间有明确结构关系，熵正则化学习收敛速率受正则化器影响。

Abstract: In this paper, we examine the robustness of Nash equilibria in continuous games, under both strategic and dynamic uncertainty. Starting with the former, we introduce the notion of a robust equilibrium as those equilibria that remain invariant to small -- but otherwise arbitrary -- perturbations to the game's payoff structure, and we provide a crisp geometric characterization thereof. Subsequently, we turn to the question of dynamic robustness, and we examine which equilibria may arise as stable limit points of the dynamics of "follow the regularized leader" (FTRL) in the presence of randomness and uncertainty. Despite their very distinct origins, we establish a structural correspondence between these two notions of robustness: strategic robustness implies dynamic robustness, and, conversely, the requirement of strategic robustness cannot be relaxed if dynamic robustness is to be maintained. Finally, we examine the rate of convergence to robust equilibria as a function of the underlying regularizer, and we show that entropically regularized learning converges at a geometric rate in games with affinely constrained action spaces.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [66] [MixLM: High-Throughput and Effective LLM Ranking via Text-Embedding Mix-Interaction](https://arxiv.org/abs/2512.07846)
*Guoyao Li,Ran He,Shusen Jing,Kayhan Behdin,Yubo Wang,Sundara Raman Ramachandran,Chanh Nguyen,Jian Sheng,Xiaojing Ma,Chuanrui Zhu,Sriram Vasudevan,Muchen Wu,Sayan Ghosh,Lin Su,Qingquan Song,Xiaoqing Wang,Zhipeng Wang,Qing Lan,Yanning Chen,Jingwei Wu,Luke Simon,Wenjing Zhang,Qi Guo,Fedor Borisyuk*

Main category: cs.IR

TL;DR: 提出MixLM框架，通过减少输入上下文长度提高系统吞吐量，在LinkedIn搜索应用部署后提升吞吐量和DAU。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在推荐和搜索系统中计算开销高，交叉编码器排名系统有长上下文预填充重的工作负载问题。

Method: 提出MixLM框架，采用文本和嵌入令牌混合的交互方式，将目录中所有项目编码为少量嵌入令牌存于近线缓存，用于在线推理。

Result: 与强基线相比，MixLM在相同延迟预算下吞吐量提高10.0倍，保持相关性指标，全流量部署后在线A/B测试中DAU显著增加0.47%。

Conclusion: MixLM能有效提高系统吞吐量，实现大语言模型搜索的全流量部署并带来用户增长。

Abstract: Large language models (LLMs) excel at capturing semantic nuances and therefore show impressive relevance ranking performance in modern recommendation and search systems. However, they suffer from high computational overhead under industrial latency and throughput requirements. In particular, cross-encoder ranking systems often create long context prefill-heavy workloads, as the model has to be presented with the user, query and item information. To this end, we propose MixLM, a novel LLM-based ranking framework, which significantly improves the system throughput via reducing the input context length, while preserving the semantic strength of cross-encoder rankers. In contrast to a standard ranking system where the context is presented to the model as pure text, we propose to use mix-interaction, a mixture of text and embedding tokens to represent the input. Specifically, MixLM encodes all items in the catalog into a few embedding tokens and stores in a nearline cache. The encoded item descriptions are used during online inference, effectively reducing the item length from a few thousand text tokens to a few embedding tokens. We share insights from deploying our MixLM framework to a real-world search application at LinkedIn, including a detailed discussion of our training pipelines, as well as a thorough analysis of our online serving infrastructure optimization. Comparing with strong baselines, MixLM increased throughput by 10.0x under the same latency budget, while maintaining relevance metrics. The efficiency gains delivered by MixLM enabled full-traffic deployment of LLM-powered search, which resulted in a significant 0.47% increase in Daily Active Users (DAU) in online A/B tests.

</details>


### [67] [Detecting Privileged Documents by Ranking Connected Network Entities](https://arxiv.org/abs/2512.08073)
*Jianping Zhang,Han Qin,Nathaniel Huber-Fliflet*

Main category: cs.IR

TL;DR: 本文提出一种通过邮件元数据构建人类实体网络来识别特权文档的链路分析方法，实验证明该算法有效。


<details>
  <summary>Details</summary>
Motivation: 识别特权文档。

Method: 构建人类实体网络，根据预定义的法律专业人员列表将实体分类，用算法为网络中每个实体打分，结合实体分数和连接强度识别特权文档。

Result: 算法能有效对法律实体进行排名以检测特权文档。

Conclusion: 所提出的链路分析方法在特权文档检测方面有效。

Abstract: This paper presents a link analysis approach for identifying privileged documents by constructing a network of human entities derived from email header metadata. Entities are classified as either counsel or non-counsel based on a predefined list of known legal professionals. The core assumption is that individuals with frequent interactions with lawyers are more likely to participate in privileged communications. To quantify this likelihood, an algorithm assigns a score to each entity within the network. By utilizing both entity scores and the strength of their connections, the method enhances the identification of privileged documents. Experimental results demonstrate the algorithm's effectiveness in ranking legal entities for privileged document detection.

</details>


### [68] [A Comparative Study of Retrieval Methods in Azure AI Search](https://arxiv.org/abs/2512.08078)
*Qiang Mao,Han Qin,Robert Neary,Charles Wang,Fusheng Wei,Jianping Zhang,Nathaniel Huber-Fliflet*

Main category: cs.IR

TL;DR: 研究评估微软Azure的RAG框架内的检索策略，用于电子发现的早期案例评估，并比较多种检索方法的性能。


<details>
  <summary>Details</summary>
Motivation: 律师希望超越关键词和语义搜索提高文档审查效率，所以评估Azure RAG框架的检索策略以用于早期案例评估。

Method: 比较Azure AI搜索的关键词、语义、向量、混合和混合语义检索方法。

Result: 给出了每种方法的AI生成响应的准确性、相关性和一致性。

Conclusion: 法律从业者可利用研究结果改进未来RAG配置的选择。

Abstract: Increasingly, attorneys are interested in moving beyond keyword and semantic search to improve the efficiency of how they find key information during a document review task. Large language models (LLMs) are now seen as tools that attorneys can use to ask natural language questions of their data during document review to receive accurate and concise answers. This study evaluates retrieval strategies within Microsoft Azure's Retrieval-Augmented Generation (RAG) framework to identify effective approaches for Early Case Assessment (ECA) in eDiscovery. During ECA, legal teams analyze data at the outset of a matter to gain a general understanding of the data and attempt to determine key facts and risks before beginning full-scale review. In this paper, we compare the performance of Azure AI Search's keyword, semantic, vector, hybrid, and hybrid-semantic retrieval methods. We then present the accuracy, relevance, and consistency of each method's AI-generated responses. Legal practitioners can use the results of this study to enhance how they select RAG configurations in the future.

</details>


### [69] [Leveraging Machine Learning and Large Language Models for Automated Image Clustering and Description in Legal Discovery](https://arxiv.org/abs/2512.08079)
*Qiang Mao,Fusheng Wei,Robert Neary,Charles Wang,Han Qin,Jianping Zhang,Nathaniel Huber-Fliflet*

Main category: cs.IR

TL;DR: 本文探讨图像收集自动化组织和描述方法，通过集成和实验，结果为实际应用提供指导。


<details>
  <summary>Details</summary>
Motivation: 数字图像急剧增加，手动审查不现实且成本高，需自动化方法来组织和描述大规模图像数据集。

Method: 用K - means聚类分组图像，用Azure AI Vision API生成基础说明。从图像采样策略、提示技术、描述生成方法三方面评估，用语义相似度和覆盖率评估说明质量。

Result: 每簇取20张图像的战略采样与全部纳入表现相当且降低成本；基于LLM的方法优于TF - IDF，标准提示优于思维链提示。

Conclusion: 这些发现为构建支持大量图像工作流的集群描述系统提供实用指导。

Abstract: The rapid increase in digital image creation and retention presents substantial challenges during legal discovery, digital archive, and content management. Corporations and legal teams must organize, analyze, and extract meaningful insights from large image collections under strict time pressures, making manual review impractical and costly. These demands have intensified interest in automated methods that can efficiently organize and describe large-scale image datasets. This paper presents a systematic investigation of automated cluster description generation through the integration of image clustering, image captioning, and large language models (LLMs). We apply K-means clustering to group images into 20 visually coherent clusters and generate base captions using the Azure AI Vision API. We then evaluate three critical dimensions of the cluster description process: (1) image sampling strategies, comparing random, centroid-based, stratified, hybrid, and density-based sampling against using all cluster images; (2) prompting techniques, contrasting standard prompting with chain-of-thought prompting; and (3) description generation methods, comparing LLM-based generation with traditional TF-IDF and template-based approaches. We assess description quality using semantic similarity and coverage metrics. Results show that strategic sampling with 20 images per cluster performs comparably to exhaustive inclusion while significantly reducing computational cost, with only stratified sampling showing modest degradation. LLM-based methods consistently outperform TF-IDF baselines, and standard prompts outperform chain-of-thought prompts for this task. These findings provide practical guidance for deploying scalable, accurate cluster description systems that support high-volume workflows in legal discovery and other domains requiring automated organization of large image collections.

</details>


### [70] [Exploiting the Randomness of Large Language Models (LLM) in Text Classification Tasks: Locating Privileged Documents in Legal Matters](https://arxiv.org/abs/2512.08083)
*Keith Huffman,Jianping Zhang,Nathaniel Huber-Fliflet,Fusheng Wei,Peter Gronvall*

Main category: cs.IR

TL;DR: 本文实证研究大语言模型（LLM）在律师 - 客户特权文件检测分类中随机性的作用，发现LLM能有效识别特权文件，控制随机性参数影响小，利用随机性能提高准确率，还能增强企业对LLM输出的信心。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型在法律事务文本分类中，随机性对律师 - 客户特权文件检测分类的影响。

Method: 进行实证研究，从LLM识别特权文件有效性、随机性控制参数对分类输出的影响、对整体分类性能的影响以及利用随机性提高准确率的方法四个维度展开。

Result: LLM能有效识别特权文件，随机性控制参数对分类性能影响小，利用随机性能显著提高准确率。

Conclusion: 开发的利用随机性的方法可增强企业对LLM输出的信心，减少输出可变性有助于建立对LLM制裁筛查决策的内部和监管信心。

Abstract: In legal matters, text classification models are most often used to filter through large datasets in search of documents that meet certain pre-selected criteria like relevance to a certain subject matter, such as legally privileged communications and attorney-directed documents. In this context, large language models have demonstrated strong performance. This paper presents an empirical study investigating the role of randomness in LLM-based classification for attorney-client privileged document detection, focusing on four key dimensions: (1) the effectiveness of LLMs in identifying legally privileged documents, (2) the influence of randomness control parameters on classification outputs, (3) their impact on overall classification performance, and (4) a methodology for leveraging randomness to enhance accuracy. Experimental results showed that LLMs can identify privileged documents effectively, randomness control parameters have minimal impact on classification performance, and importantly, our developed methodology for leveraging randomness can have a significant impact on improving accuracy. Notably, this methodology that leverages randomness could also enhance a corporation's confidence in an LLM's output when incorporated into its sanctions-compliance processes. As organizations increasingly rely on LLMs to augment compliance workflows, reducing output variability helps build internal and regulatory confidence in LLM-derived sanctions-screening decisions.

</details>


### [71] [Ontology-Based Knowledge Graph Framework for Industrial Standard Documents via Hierarchical and Propositional Structuring](https://arxiv.org/abs/2512.08398)
*Jiin Park,Hyuna Jeon,Yoonseo Lee,Jisu Hong,Misuk Kim*

Main category: cs.IR

TL;DR: 提出一种基于本体的知识图谱构建方法，组织工业标准文档，经LLM提取三元组集成到本体知识图谱，实验显示性能优于现有方法，助力领域RAG和文档管理。


<details>
  <summary>Details</summary>
Motivation: 工业标准文档结构复杂，传统知识图谱构建方法难以有效表示其特定语义，需新方法应对挑战。

Method: 将文档组织成层次语义结构，把句子和表格分解为原子命题，通过基于LLM的三元组提取集成到本体知识图谱；构建数据集，实现本体感知的KG - RAG框架进行评估。

Result: 该方法在所有问答类型上比现有KG - RAG方法有显著性能提升。

Conclusion: 即使对于条件、约束和范围相互交织的工业文档，可靠且可扩展的知识表示是可行的，有助于未来特定领域RAG发展和智能文档管理。

Abstract: Ontology-based knowledge graph (KG) construction is a core technology that enables multidimensional understanding and advanced reasoning over domain knowledge. Industrial standards, in particular, contain extensive technical information and complex rules presented in highly structured formats that combine tables, scopes of application, constraints, exceptions, and numerical calculations, making KG construction especially challenging. In this study, we propose a method that organizes such documents into a hierarchical semantic structure, decomposes sentences and tables into atomic propositions derived from conditional and numerical rules, and integrates them into an ontology-knowledge graph through LLM-based triple extraction. Our approach captures both the hierarchical and logical structures of documents, effectively representing domain-specific semantics that conventional methods fail to reflect. To verify its effectiveness, we constructed rule, table, and multi-hop QA datasets, as well as a toxic clause detection dataset, from industrial standards, and implemented an ontology-aware KG-RAG framework for comparative evaluation. Experimental results show that our method achieves significant performance improvements across all QA types compared to existing KG-RAG approaches. This study demonstrates that reliable and scalable knowledge representation is feasible even for industrial documents with intertwined conditions, constraints, and scopes, contributing to future domain-specific RAG development and intelligent document management.

</details>


### [72] [VI-MMRec: Similarity-Aware Training Cost-free Virtual User-Item Interactions for Multimodal Recommendation](https://arxiv.org/abs/2512.08702)
*Jinfeng Xu,Zheyu Chen,Shuo Yang,Jinze Li,Zitong Wan,Hewei Wang,Weijie Liu,Yijie Li,Edith C. H. Ngai*

Main category: cs.IR

TL;DR: 提出VI - MMRec框架解决多模态推荐数据稀疏问题，与现有模型集成提升性能，实验验证有效。


<details>
  <summary>Details</summary>
Motivation: 现有多模态推荐模型受数据稀疏问题限制，因用户交互物品少，模型将未观察物品随意当作负样本。

Method: 提出VI - MMRec框架，通过相似性感知的虚拟用户 - 物品交互丰富稀疏交互，有Overlay和Synergistic两种策略，设计权重分配机制，可与现有模型无缝集成。

Result: 在六个真实数据集上用七个最先进的多模态推荐模型进行实验，验证了VI - MMRec的有效性。

Conclusion: VI - MMRec是模型无关、无训练成本的框架，能提升现有模型性能，无额外训练开销，便于实际部署。

Abstract: Although existing multimodal recommendation models have shown promising performance, their effectiveness continues to be limited by the pervasive data sparsity problem. This problem arises because users typically interact with only a small subset of available items, leading existing models to arbitrarily treat unobserved items as negative samples. To this end, we propose VI-MMRec, a model-agnostic and training cost-free framework that enriches sparse user-item interactions via similarity-aware virtual user-item interactions. These virtual interactions are constructed based on modality-specific feature similarities of user-interacted items. Specifically, VI-MMRec introduces two different strategies: (1) Overlay, which independently aggregates modality-specific similarities to preserve modality-specific user preferences, and (2) Synergistic, which holistically fuses cross-modal similarities to capture complementary user preferences. To ensure high-quality augmentation, we design a statistically informed weight allocation mechanism that adaptively assigns weights to virtual user-item interactions based on dataset-specific modality relevance. As a plug-and-play framework, VI-MMRec seamlessly integrates with existing models to enhance their performance without modifying their core architecture. Its flexibility allows it to be easily incorporated into various existing models, maximizing performance with minimal implementation effort. Moreover, VI-MMRec introduces no additional overhead during training, making it significantly advantageous for practical deployment. Comprehensive experiments conducted on six real-world datasets using seven state-of-the-art multimodal recommendation models validate the effectiveness of our VI-MMRec.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [73] [ThreadWeaver: Adaptive Threading for Efficient Parallel Reasoning in Language Models](https://arxiv.org/abs/2512.07843)
*Long Lian,Sida Wang,Felix Juefei-Xu,Tsu-Jui Fu,Xiuyu Li,Adam Yala,Trevor Darrell,Alane Suhr,Yuandong Tian,Xi Victoria Lin*

Main category: cs.LG

TL;DR: 介绍了自适应并行推理框架ThreadWeaver，在保证准确率的同时显著降低推理延迟。


<details>
  <summary>Details</summary>
Motivation: 现有自适应并行推理方法存在准确率低、需定制推理引擎等问题，需提升复杂任务推理效率。

Method: 提出三方面创新：两阶段并行轨迹生成器、基于字典树的训练推理协同设计、并行感知强化学习框架。

Result: 在六个数学推理基准测试中，基于Qwen3 - 8B训练的ThreadWeaver准确率与前沿顺序推理模型相当，平均提速1.53倍。

Conclusion: ThreadWeaver在准确率和效率间建立了新的帕累托最优边界。

Abstract: Scaling inference-time computation has enabled Large Language Models (LLMs) to achieve strong reasoning performance, but inherently sequential decoding leads to substantial latency, especially on complex tasks. Recent work on adaptive parallel reasoning aims to improve inference efficiency by decomposing the problem-solving process into concurrent reasoning threads when beneficial. However, existing methods on realistic tasks are either limited to supervised behavior cloning or exhibit significant accuracy drops compared to widely-used sequential long chain-of-thought (CoT) baselines. Moreover, many require customized inference engines, complicating deployment. We introduce ThreadWeaver, a framework for adaptive parallel reasoning that achieves accuracy on par with popular sequential reasoning models of comparable size while significantly reducing inference latency. ThreadWeaver's performance stems from three key innovations: 1) a two-stage parallel trajectory generator that produces large-scale, high-quality CoT data with parallel annotations for supervised fine-tuning; 2) a trie-based training-inference co-design that enables parallel reasoning on any off-the-shelf autoregressive inference engine without modifying position embeddings or KV caches; and 3) a parallelization-aware reinforcement learning framework that teaches the model to balance accuracy with effective parallelization. Across six challenging mathematical reasoning benchmarks, ThreadWeaver trained atop Qwen3-8B achieves accuracy comparable to cutting-edge sequential reasoning models (71.9% on average and 79.9% on AIME24) while delivering up to 1.53x average speedup in token latency, establishing a new Pareto frontier between accuracy and efficiency.

</details>


### [74] [Space Alignment Matters: The Missing Piece for Inducing Neural Collapse in Long-Tailed Learning](https://arxiv.org/abs/2512.07844)
*Jinping Wang,Zhiqiang Gao,Zhiwu Xie*

Main category: cs.LG

TL;DR: 论文理论量化特征与分类器权重空间不对齐的危害，提出三种对齐策略，实验提升了现有长尾方法性能。


<details>
  <summary>Details</summary>
Motivation: 长尾数据集中样本不均衡阻碍神经崩溃现象出现，现有方法忽略特征与分类器权重空间的不对齐问题。

Method: 通过最优误差指数分析量化不对齐危害，提出三种可即插即用的对齐策略。

Result: 在CIFAR - 10 - LT、CIFAR - 100 - LT和ImageNet - LT数据集上提升了基线方法性能，达到了当前最优。

Conclusion: 所提出的对齐策略能有效解决长尾数据集中特征与分类器权重空间不对齐问题，提升模型性能。

Abstract: Recent studies on Neural Collapse (NC) reveal that, under class-balanced conditions, the class feature means and classifier weights spontaneously align into a simplex equiangular tight frame (ETF). In long-tailed regimes, however, severe sample imbalance tends to prevent the emergence of the NC phenomenon, resulting in poor generalization performance. Current efforts predominantly seek to recover the ETF geometry by imposing constraints on features or classifier weights, yet overlook a critical problem: There is a pronounced misalignment between the feature and the classifier weight spaces. In this paper, we theoretically quantify the harm of such misalignment through an optimal error exponent analysis. Built on this insight, we propose three explicit alignment strategies that plug-and-play into existing long-tail methods without architectural change. Extensive experiments on the CIFAR-10-LT, CIFAR-100-LT, and ImageNet-LT datasets consistently boost examined baselines and achieve the state-of-the-art performances.

</details>


### [75] [Softly Symbolifying Kolmogorov-Arnold Networks](https://arxiv.org/abs/2512.07875)
*James Bagrow,Josh Bongard*

Main category: cs.LG

TL;DR: 提出Softly Symbolified Kolmogorov - Arnold Networks (S2KAN)，将符号原语融入训练，在多种任务中展示了小模型高准确性，有自稀疏性。


<details>
  <summary>Details</summary>
Motivation: Kolmogorov - Arnold Networks (KANs) 训练后的激活函数常缺乏符号保真度，学习到无意义的分解，无法对应可解释形式。

Method: 提出S2KAN，将符号原语直接融入训练过程，每个激活函数从符号和密集项字典中选取，通过可学习的门来实现表示的稀疏化，且该稀疏化可微分，以最小描述长度目标为指导。

Result: 在符号基准测试、动态系统预测和现实世界预测任务中，用更小的模型实现了有竞争力或更优的准确性，且即使无正则化压力也有自稀疏性。

Conclusion: S2KAN 能在符号项足够时发现可解释形式，不足时退化为密集样条，是有效的可解释机器学习方法。

Abstract: Kolmogorov-Arnold Networks (KANs) offer a promising path toward interpretable machine learning: their learnable activations can be studied individually, while collectively fitting complex data accurately. In practice, however, trained activations often lack symbolic fidelity, learning pathological decompositions with no meaningful correspondence to interpretable forms. We propose Softly Symbolified Kolmogorov-Arnold Networks (S2KAN), which integrate symbolic primitives directly into training. Each activation draws from a dictionary of symbolic and dense terms, with learnable gates that sparsify the representation. Crucially, this sparsification is differentiable, enabling end-to-end optimization, and is guided by a principled Minimum Description Length objective. When symbolic terms suffice, S2KAN discovers interpretable forms; when they do not, it gracefully degrades to dense splines. We demonstrate competitive or superior accuracy with substantially smaller models across symbolic benchmarks, dynamical systems forecasting, and real-world prediction tasks, and observe evidence of emergent self-sparsification even without regularization pressure.

</details>


### [76] [CarBench: A Comprehensive Benchmark for Neural Surrogates on High-Fidelity 3D Car Aerodynamics](https://arxiv.org/abs/2512.07847)
*Mohamed Elrefaie,Dule Shu,Matt Klenk,Faez Ahmed*

Main category: cs.LG

TL;DR: 本文介绍首个用于大规模3D汽车空气动力学的综合基准CarBench，在DrivAerNet++数据集上评估多种模型，进行跨类别实验，开源基准框架。


<details>
  <summary>Details</summary>
Motivation: 计算机视觉等领域有基准推动创新，但工程设计大规模数值模拟缺乏标准化基准，因此开展研究。

Method: 在DrivAerNet++数据集上对11种架构模型进行大规模评估，开展跨类别实验，分析预测准确性、物理一致性等指标。

Result: 完成多种模型评估和跨类别实验，分析了相关指标。

Conclusion: 开源基准框架，为基于高保真CFD模拟的大规模学习建立首个可重现基础。

Abstract: Benchmarking has been the cornerstone of progress in computer vision, natural language processing, and the broader deep learning domain, driving algorithmic innovation through standardized datasets and reproducible evaluation protocols. The growing availability of large-scale Computational Fluid Dynamics (CFD) datasets has opened new opportunities for applying machine learning to aerodynamic and engineering design. Yet, despite this progress, there exists no standardized benchmark for large-scale numerical simulations in engineering design. In this work, we introduce CarBench, the first comprehensive benchmark dedicated to large-scale 3D car aerodynamics, performing a large-scale evaluation of state-of-the-art models on DrivAerNet++, the largest public dataset for automotive aerodynamics, containing over 8,000 high-fidelity car simulations. We assess eleven architectures spanning neural operator methods (e.g., Fourier Neural Operator), geometric deep learning (PointNet, RegDGCNN, PointMAE, PointTransformer), transformer-based neural solvers (Transolver, Transolver++, AB-UPT), and implicit field networks (TripNet). Beyond standard interpolation tasks, we perform cross-category experiments in which transformer-based solvers trained on a single car archetype are evaluated on unseen categories. Our analysis covers predictive accuracy, physical consistency, computational efficiency, and statistical uncertainty. To accelerate progress in data-driven engineering, we open-source the benchmark framework, including training pipelines, uncertainty estimation routines based on bootstrap resampling, and pretrained model weights, establishing the first reproducible foundation for large-scale learning from high-fidelity CFD simulations, available at https://github.com/Mohamedelrefaie/CarBench.

</details>


### [77] [Towards symbolic regression for interpretable clinical decision scores](https://arxiv.org/abs/2512.07961)
*Guilherme Seidyo Imai Aldeia,Joseph D. Romano,Fabricio Olivetti de Franca,Daniel S. Herman,William G. La Cava*

Main category: cs.LG

TL;DR: 介绍了SR算法Brush，它结合决策树分裂算法与非线性常数优化，在SRBench上表现优，应用于临床评分系统效果好。


<details>
  <summary>Details</summary>
Motivation: 传统符号回归（SR）难对医疗决策建模，但其有开发数据驱动临床风险评分的潜力，需改进。

Method: 引入结合决策树分裂算法与非线性常数优化的SR算法Brush。

Result: Brush在SRBench上实现帕累托最优性能，用于重现临床评分系统有高精度和可解释性，比决策树等方法预测性能相当或更优且模型更简单。

Conclusion: Brush算法能将基于规则的逻辑无缝集成到符号回归和分类模型中，有良好应用效果。

Abstract: Medical decision-making makes frequent use of algorithms that combine risk equations with rules, providing clear and standardized treatment pathways. Symbolic regression (SR) traditionally limits its search space to continuous function forms and their parameters, making it difficult to model this decision-making. However, due to its ability to derive data-driven, interpretable models, SR holds promise for developing data-driven clinical risk scores. To that end we introduce Brush, an SR algorithm that combines decision-tree-like splitting algorithms with non-linear constant optimization, allowing for seamless integration of rule-based logic into symbolic regression and classification models. Brush achieves Pareto-optimal performance on SRBench, and was applied to recapitulate two widely used clinical scoring systems, achieving high accuracy and interpretable models. Compared to decision trees, random forests, and other SR methods, Brush achieves comparable or superior predictive performance while producing simpler models.

</details>


### [78] [Pattern Recognition of Ozone-Depleting Substance Exports in Global Trade Data](https://arxiv.org/abs/2512.07864)
*Muhammad Sukri Bin Ramli*

Main category: cs.LG

TL;DR: 文章介绍用无监督机器学习框架检测可疑贸易模式，用于监测环境条约。该框架结合多种ML技术处理交易记录，找出高优先级货物和异常贸易，构建了可重复使用的管道。


<details>
  <summary>Details</summary>
Motivation: 需要新方法通过审查复杂海关数据集来监测环境条约。

Method: 采用无监督机器学习，结合无监督聚类（K - Means）、异常检测（Isolation Forest和IQR）和启发式标记，为货物计算优先级分数。

Result: 成功识别1351个价格异常值和1288个高优先级货物，发现高优先级商品的价值重量比不同，还检测到2021年初“超级贸易”激增。

Conclusion: 构建了可重复的无监督学习管道，将原始贸易数据转化为监管机构可用的情报。

Abstract: New methods are needed to monitor environmental treaties, like the Montreal Protocol, by reviewing large, complex customs datasets. This paper introduces a framework using unsupervised machine learning to systematically detect suspicious trade patterns and highlight activities for review. Our methodology, applied to 100,000 trade records, combines several ML techniques. Unsupervised Clustering (K-Means) discovers natural trade archetypes based on shipment value and weight. Anomaly Detection (Isolation Forest and IQR) identifies rare "mega-trades" and shipments with commercially unusual price-per-kilogram values. This is supplemented by Heuristic Flagging to find tactics like vague shipment descriptions. These layers are combined into a priority score, which successfully identified 1,351 price outliers and 1,288 high-priority shipments for customs review. A key finding is that high-priority commodities show a different and more valuable value-to-weight ratio than general goods. This was validated using Explainable AI (SHAP), which confirmed vague descriptions and high value as the most significant risk predictors. The model's sensitivity was validated by its detection of a massive spike in "mega-trades" in early 2021, correlating directly with the real-world regulatory impact of the US AIM Act. This work presents a repeatable unsupervised learning pipeline to turn raw trade data into prioritized, usable intelligence for regulatory groups.

</details>


### [79] [RaX-Crash: A Resource Efficient and Explainable Small Model Pipeline with an Application to City Scale Injury Severity Prediction](https://arxiv.org/abs/2512.07848)
*Di Zhu,Chen Xie,Ziwei Wang,Haoyun Zhang*

Main category: cs.LG

TL;DR: 介绍RaX - Crash用于纽约机动车碰撞数据集的损伤严重程度预测，对比模型性能，表明小模型集成是有效基线，混合管道可提升沟通且不牺牲可扩展性。


<details>
  <summary>Details</summary>
Motivation: 纽约每年机动车碰撞多，造成较大伤害和公共卫生负担，需进行损伤严重程度预测。

Method: 集成三个关联表，构建统一特征模式，用随机森林和XGBoost训练基于表格特征的树集成模型，并与小语言模型对比；进行类不平衡分析和SHAP归因。

Result: XGBoost和随机森林准确率分别为0.7828和0.7794，明显优于小语言模型；类权重调整可提升致命召回率；SHAP归因找出影响预测严重程度的主导因素。

Conclusion: 可解释的小模型集成是城市规模损伤分析的强大基线，结合表格预测器和小语言模型的混合管道可在不影响可扩展性的情况下改善沟通。

Abstract: New York City reports over one hundred thousand motor vehicle collisions each year, creating substantial injury and public health burden. We present RaX-Crash, a resource efficient and explainable small model pipeline for structured injury severity prediction on the official NYC Motor Vehicle Collisions dataset. RaX-Crash integrates three linked tables with tens of millions of records, builds a unified feature schema in partitioned storage, and trains compact tree based ensembles (Random Forest and XGBoost) on engineered tabular features, which are compared against locally deployed small language models (SLMs) prompted with textual summaries. On a temporally held out test set, XGBoost and Random Forest achieve accuracies of 0.7828 and 0.7794, clearly outperforming SLMs (0.594 and 0.496); class imbalance analysis shows that simple class weighting improves fatal recall with modest accuracy trade offs, and SHAP attribution highlights human vulnerability factors, timing, and location as dominant drivers of predicted severity. Overall, RaX-Crash indicates that interpretable small model ensembles remain strong baselines for city scale injury analytics, while hybrid pipelines that pair tabular predictors with SLM generated narratives improve communication without sacrificing scalability.

</details>


### [80] [SABER: Small Actions, Big Errors -- Safeguarding Mutating Steps in LLM Agents](https://arxiv.org/abs/2512.07850)
*Alejandro Cuadron,Pengfei Yu,Yang Liu,Arpit Gupta*

Main category: cs.LG

TL;DR: 分析LLM智能体长时任务失败因素，引入Cautious Mutator防护机制并验证效果，同时发布τ - Bench Verified。


<details>
  <summary>Details</summary>
Motivation: 解决LLM智能体在长时使用工具任务中表现不稳定问题，通过分析不同行为对失败的影响来改进。

Method: 分析执行轨迹，划分突变与非突变步骤，定义决定性偏差进行逻辑回归；引入Cautious Mutator，添加突变门控验证、突变步骤前进行目标反思、执行块级上下文清理。

Result: Cautious Mutator取得一致性能提升；发现τ - Bench存在上限效应，发布τ - Bench Verified恢复基准测试空间。

Conclusion: 行动级分析、针对性防护和可靠评估是构建稳健多轮智能体的前提。

Abstract: Despite rapid progress in LLM agents, performance on long-horizon, tool-using tasks remains fragile. To better understand this fragility, we ask a simple question: \emph{do all actions contribute equally to failure?} Analyzing execution traces on $τ$-Bench (Airline/Retail) and SWE-Bench Verified, we decompose trajectories into \emph{mutating} (environment-changing) vs.\ non-mutating steps and formalize \emph{decisive deviations}, earliest action, level divergences that flip success to failure. A logistic regression reveals that each additional deviation in a mutating action reduces the odds of success by upto $92\%$ on Airline and upto $96\%$ on Retail for SoTA models. In contrast, deviations in non-mutating actions have little to no effect. Errors also grow with context length as agents drift from role and act on stale constraints. Motivated by these observations, we introduce \cm{}, a model-agnostic, gradient-free, test-time safeguard that (i) adds mutation-gated verification, (ii) injects \emph{Targeted Reflection} before mutating steps, and (iii) performs block-based context cleaning. \cm{} delivers consistent gains, e.g., Qwen3-Thinking: +28\% \emph{relative} on Airline, +11\% on Retail, and +7\% on SWE-Bench Verified; Claude: +9\%/+7\%. We further identify ceiling effects in $τ$-Bench, where annotation errors and underspecified tasks artificially cap model performance. To address this, we release $τ$-Bench Verified, which restores benchmark headroom through targeted revisions. Our results argue for action-level analysis, targeted safeguards, and reliable evaluations as prerequisites for robust multi-turn agents.

</details>


### [81] [Bayesian Optimization for Function-Valued Responses under Min-Max Criteria](https://arxiv.org/abs/2512.07868)
*Pouya Ahadi,Reza Marzban,Ali Adibi,Kamran Paynabar*

Main category: cs.LG

TL;DR: 提出了min - max Functional Bayesian Optimization (MM - FBO) 解决现有贝叶斯优化在处理函数响应时的问题，方法经实验验证有效。


<details>
  <summary>Details</summary>
Motivation: 现有贝叶斯优化方法大多针对标量响应，在函数响应场景中经典方法不足，且现有方法常忽略最坏情况偏差。

Method: 提出MM - FBO框架，用函数主成分分析表示函数响应，构建高斯过程替代模型，引入集成不确定性采集函数，给出两个理论保证。

Result: 在合成基准和物理案例研究中，MM - FBO始终优于现有基线。

Conclusion: MM - FBO有效，凸显了在贝叶斯优化中显式建模函数不确定性的重要性。

Abstract: Bayesian optimization is widely used for optimizing expensive black box functions, but most existing approaches focus on scalar responses. In many scientific and engineering settings the response is functional, varying smoothly over an index such as time or wavelength, which makes classical formulations inadequate. Existing methods often minimize integrated error, which captures average performance but neglects worst case deviations. To address this limitation we propose min-max Functional Bayesian Optimization (MM-FBO), a framework that directly minimizes the maximum error across the functional domain. Functional responses are represented using functional principal component analysis, and Gaussian process surrogates are constructed for the principal component scores. Building on this representation, MM-FBO introduces an integrated uncertainty acquisition function that balances exploitation of worst case expected error with exploration across the functional domain. We provide two theoretical guarantees: a discretization bound for the worst case objective, and a consistency result showing that as the surrogate becomes accurate and uncertainty vanishes, the acquisition converges to the true min-max objective. We validate the method through experiments on synthetic benchmarks and physics inspired case studies involving electromagnetic scattering by metaphotonic devices and vapor phase infiltration. Results show that MM-FBO consistently outperforms existing baselines and highlights the importance of explicitly modeling functional uncertainty in Bayesian optimization.

</details>


### [82] [Long-only cryptocurrency portfolio management by ranking the assets: a neural network approach](https://arxiv.org/abs/2512.08124)
*Zijiang Yang*

Main category: cs.LG

TL;DR: 本文提出基于机器学习的加密货币投资组合管理方法，通过分析相对关系管理一组加密货币，回测显示该方法在复杂市场条件下优于现有方法且对交易费用增加有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 以往研究主要关注特定加密货币走势预测并单独交易，本文旨在通过分析相对关系管理一组加密货币。

Method: 在每个时间步，利用神经网络预测所管理加密货币未来回报排名并分配权重，纳入横截面信息。

Result: 在2020年5月至2023年11月真实加密货币市场数据回测中表现盈利，夏普比率达1.01，年化收益率64.26%，且对交易费用增加有鲁棒性。

Conclusion: 该方法在复杂市场条件下优于现有方法，具有较好的盈利能力和鲁棒性。

Abstract: This paper will propose a novel machine learning based portfolio management method in the context of the cryptocurrency market. Previous researchers mainly focus on the prediction of the movement for specific cryptocurrency such as the bitcoin(BTC) and then trade according to the prediction. In contrast to the previous work that treats the cryptocurrencies independently, this paper manages a group of cryptocurrencies by analyzing the relative relationship. Specifically, in each time step, we utilize the neural network to predict the rank of the future return of the managed cryptocurrencies and place weights accordingly. By incorporating such cross-sectional information, the proposed methods is shown to profitable based on the backtesting experiments on the real daily cryptocurrency market data from May, 2020 to Nov, 2023. During this 3.5 years, the market experiences the full cycle of bullish, bearish and stagnant market conditions. Despite under such complex market conditions, the proposed method outperforms the existing methods and achieves a Sharpe ratio of 1.01 and annualized return of 64.26%. Additionally, the proposed method is shown to be robust to the increase of transaction fee.

</details>


### [83] [gHAWK: Local and Global Structure Encoding for Scalable Training of Graph Neural Networks on Knowledge Graphs](https://arxiv.org/abs/2512.08274)
*Humera Sabir,Fatima Farooq,Ashraf Aboulnaga*

Main category: cs.LG

TL;DR: 本文提出可扩展的GNN训练框架gHAWK，通过预计算节点结构特征，降低内存使用、加速收敛和提高准确率，在OGB数据集实验中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有消息传递GNN难以扩展到大型知识图谱，因其迭代消息传递过程效率低。

Method: 引入预处理步骤，计算Bloom过滤器编码局部邻域结构，计算TransE嵌入表示全局位置，将这些特征与特定领域特征融合。

Result: 在OGB大型数据集实验中，gHAWK在节点属性预测和链接预测任务上达到了最先进的准确率，降低了训练时间，在三个图上位居OGB排行榜榜首。

Conclusion: 通过为消息传递训练增加结构先验，gHAWK能显著降低内存使用、加速收敛和提高模型准确率。

Abstract: Knowledge Graphs (KGs) are a rich source of structured, heterogeneous data, powering a wide range of applications. A common approach to leverage this data is to train a graph neural network (GNN) on the KG. However, existing message-passing GNNs struggle to scale to large KGs because they rely on the iterative message passing process to learn the graph structure, which is inefficient, especially under mini-batch training, where a node sees only a partial view of its neighborhood. In this paper, we address this problem and present gHAWK, a novel and scalable GNN training framework for large KGs. The key idea is to precompute structural features for each node that capture its local and global structure before GNN training even begins. Specifically, gHAWK introduces a preprocessing step that computes: (a)~Bloom filters to compactly encode local neighborhood structure, and (b)~TransE embeddings to represent each node's global position in the graph. These features are then fused with any domain-specific features (e.g., text embeddings), producing a node feature vector that can be incorporated into any GNN technique. By augmenting message-passing training with structural priors, gHAWK significantly reduces memory usage, accelerates convergence, and improves model accuracy. Extensive experiments on large datasets from the Open Graph Benchmark (OGB) demonstrate that gHAWK achieves state-of-the-art accuracy and lower training time on both node property prediction and link prediction tasks, topping the OGB leaderboard for three graphs.

</details>


### [84] [GPU Memory Prediction for Multimodal Model Training](https://arxiv.org/abs/2512.07853)
*Jinwoo Jeong,Minchul Kang,Younghun Go,Changyong Shin,Hyunho Lee,Junho Yoon,Gyeongsik Yang,Chuck Yoo*

Main category: cs.LG

TL;DR: 随着智能体AI系统中深度学习模型规模和复杂度增加，GPU内存常不足，提出框架分析多模态模型架构和训练行为预测GPU内存峰值使用，准确率较高。


<details>
  <summary>Details</summary>
Motivation: 智能体AI系统中深度学习模型规模和复杂度增加，GPU内存常不足出现内存溢出错误，浪费资源，以往研究不能泛化到多模态模型，需要准确预测多模态模型GPU内存使用。

Method: 提出框架，将多模态模型分解为各层并应用因式分解估计每层内存使用。

Result: 框架实现了约8.7%的平均平均绝对百分比误差（MAPE）的高预测准确率。

Conclusion: 所提出的框架能有效准确预测多模态模型的GPU内存峰值使用。

Abstract: As deep learning models in agentic AI systems grow in scale and complexity, GPU memory requirements increase and often exceed the available GPU memory capacity, so that out-of-memory (OoM) errors occur. It is well known that OoM interrupts the whole training itself and wastes substantial computational resources. Therefore, to prevent OoM, accurate prediction of GPU memory usage is essential. However, previous studies focus only on unimodal architectures and fail to generalize to multimodal models, even though the multimodal models are a common choice in agentic AI systems. To address this limitation, we propose a framework that predicts the peak GPU memory usage by analyzing the model architecture and training behavior of multimodal models. Specifically, the framework decomposes the multimodal model into its constituent layers and applies factorization to estimate the memory usage of each layer. Our evaluation shows that our framework achieves high prediction accuracy of ~8.7% average MAPE.

</details>


### [85] [Long-Sequence LSTM Modeling for NBA Game Outcome Prediction Using a Novel Multi-Season Dataset](https://arxiv.org/abs/2512.08591)
*Charles Rios,Longzhen Han,Almas Baimagambetov,Nikolaos Polatidis*

Main category: cs.LG

TL;DR: 本文引入新的NBA纵向数据集，提出LSTM架构预测比赛结果，该模型表现优于传统模型，证明长序列时间建模的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有篮球比赛预测模型存在概念漂移、时间上下文有限和跨赛季不稳定等问题，需要改进预测方法。

Method: 构建2004 - 2025赛季的NBA纵向数据集，采用LSTM架构捕捉球队长期动态和赛季间依赖关系，并与多种传统机器学习和深度学习模型对比。

Result: LSTM模型在所有指标上表现最佳，准确率72.35、精确率73.15、AUC - ROC为76.13。

Conclusion: 长序列时间建模对篮球比赛结果预测很重要，新的多赛季数据集有助于开发稳健、通用的NBA预测系统。

Abstract: Predicting the outcomes of professional basketball games, particularly in the National Basketball Association (NBA), has become increasingly important for coaching strategy, fan engagement, and sports betting. However, many existing prediction models struggle with concept drift, limited temporal context, and instability across seasons. To advance forecasting in this domain, we introduce a newly constructed longitudinal NBA dataset covering the 2004-05 to 2024-25 seasons and present a deep learning framework designed to model long-term performance trends. Our primary contribution is a Long Short-Term Memory (LSTM) architecture that leverages an extended sequence length of 9,840 games equivalent to eight full NBA seasons to capture evolving team dynamics and season-over-season dependencies. We compare this model against several traditional Machine Learning (ML) and Deep Learning (DL) baselines, including Logistic Regression, Random Forest, Multi-Layer Perceptron (MLP), and Convolutional Neural Network (CNN). The LSTM achieves the best performance across all metrics, with 72.35 accuracy, 73.15 precision and 76.13 AUC-ROC. These results demonstrate the importance of long-sequence temporal modeling in basketball outcome prediction and highlight the value of our new multi-season dataset for developing robust, generalizable NBA forecasting systems.

</details>


### [86] [HSTMixer: A Hierarchical MLP-Mixer for Large-Scale Traffic Forecasting](https://arxiv.org/abs/2512.07854)
*Yongyao Wang,Jingyuan Wang,Xie Yu,Jiahao Ji,Chao Li*

Main category: cs.LG

TL;DR: 提出HSTMixer框架用于大规模交通预测，实验显示性能佳且计算效率有竞争力。


<details>
  <summary>Details</summary>
Motivation: 现有模型计算复杂度高，不适用于大规模真实场景，需高效模型用于大规模交通预测。

Method: 提出Hierarchical Spatio-Temporal Mixer (HSTMixer)框架，采用分层时空混合块提取多分辨率特征，用自适应区域混合器生成变换矩阵。

Result: 在四个大规模真实数据集上实验，该方法达到了最先进的性能且计算效率有竞争力。

Conclusion: HSTMixer框架可有效且高效地进行大规模交通预测。

Abstract: Traffic forecasting task is significant to modern urban management. Recently, there is growing attention on large-scale forecasting, as it better reflects the complexity of real-world traffic networks. However, existing models often exhibit quadratic computational complexity, making them impractical for large-scale real-world scenarios. In this paper, we propose a novel framework, Hierarchical Spatio-Temporal Mixer (HSTMixer), which leverages an all-MLP architecture for efficient and effective large-scale traffic forecasting. HSTMixer employs a hierarchical spatiotemporal mixing block to extract multi-resolution features through bottom-up aggregation and top-down propagation. Furthermore, an adaptive region mixer generates transformation matrices based on regional semantics, enabling our model to dynamically capture evolving spatiotemporal patterns for different regions. Extensive experiments conducted on four large-scale real-world datasets demonstrate that the proposed method not only achieves state-of-the-art performance but also exhibits competitive computational efficiency.

</details>


### [87] [Fourier-Enhanced Recurrent Neural Networks for Electrical Load Time Series Downscaling](https://arxiv.org/abs/2512.07876)
*Qi Chen,Mihai Anitescu*

Main category: cs.LG

TL;DR: 提出用于电力负荷降尺度的傅里叶增强循环神经网络，在四个PJM地区表现优于经典基线和消融模型。


<details>
  <summary>Details</summary>
Motivation: 寻找更有效的电力负荷降尺度方法。

Method: 构建结合低分辨率输入驱动的循环骨干、潜在空间中融合的显式傅里叶季节嵌入和自注意力层的傅里叶增强循环神经网络。

Result: 在四个PJM地区，该方法的RMSE比经典Prophet基线和无注意力或傅里叶特征的RNN消融模型更低且更平稳。

Conclusion: 所提出的傅里叶增强循环神经网络在电力负荷降尺度方面表现良好。

Abstract: We present a Fourier-enhanced recurrent neural network (RNN) for downscaling electrical loads. The model combines (i) a recurrent backbone driven by low-resolution inputs, (ii) explicit Fourier seasonal embeddings fused in latent space, and (iii) a self-attention layer that captures dependencies among high-resolution components within each period. Across four PJM territories, the approach yields RMSE lower and flatter horizon-wise than classical Prophet baselines (with and without seasonality/LAA) and than RNN ablations without attention or Fourier features.

</details>


### [88] [LAPA: Log-Domain Prediction-Driven Dynamic Sparsity Accelerator for Transformer Model](https://arxiv.org/abs/2512.07855)
*Huizheng Wang,Hongbin Wang,Shaojun Wei,Yang Hu,Shouyi Yin*

Main category: cs.LG

TL;DR: 本文提出用于Transformer模型的跨阶段稀疏加速策略LAPA，实验显示其能效优于SOTA方法。


<details>
  <summary>Details</summary>
Motivation: Transformer模型计算瓶颈随输入序列变化呈现动态特性，需跨阶段稀疏加速策略，而现有方法多为单阶段且跨阶段应用有功耗开销问题。

Method: 提出log域注意力预测算法 - 架构协同设计LAPA，包括ALOC方案、MRSA机制、DDF策略，并设计加速器。

Result: LAPA比SOTA方法Spatten、Sanger和FACT的能效分别高3.52倍、3.24倍和2.79倍。

Conclusion: LAPA能有效解决Transformer模型跨阶段加速问题，提升能效。

Abstract: Attention-based Transformers have revolutionized natural language processing (NLP) and shown strong performance in computer vision (CV) tasks. However, as the input sequence varies, the computational bottlenecks in Transformer models exhibit dynamic behavior across stages, which calls for a cross-stage sparse acceleration strategy. Unfortunately, most existing sparse Transformer approaches are single-stage based, and their sparsity prediction mechanisms lead to significant power overhead when applied across multiple stages. To this end, this paper proposes a log-domain attention prediction algorithm-architecture co-design, named LAPA. First, an asymmetric leading one computing (ALOC) scheme is designed to eliminate expensive multiplications. Next, a mixed-precision multi-round shifting accumulation (MRSA) mechanism is further proposed to mitigate the accumulation overhead. A data-feature dependent filter (DDF) strategy is designed to work in concert with the MRSA process. Finally, an elaborate accelerator is designed to translate the theoretical enhancement into practical hardware improvement. Experimental results show that LAPA achieves 3.52x, 3.24x and 2.79x higher energy efficiency than the state-of-the-art (SOTA) works Spatten, Sanger and FACT, respectively.

</details>


### [89] [Medical Test-free Disease Detection Based on Big Data](https://arxiv.org/abs/2512.07856)
*Haokun Zhao,Yingzhe Bai,Qingyang Xu,Lixin Zhou,Jianxin Chen,Jicong Fan*

Main category: cs.LG

TL;DR: 提出基于图的深度学习模型CLDD用于疾病检测，在MIMIC - IV数据集实验中表现优于基线模型，有减少诊断成本和大规模筛查潜力。


<details>
  <summary>Details</summary>
Motivation: 疾病检测常伴随大量医疗测试和高昂成本，难以对患者进行所有测试来诊断多种疾病。

Method: 提出Collaborative Learning for Disease Detection (CLDD)模型，将疾病检测作为协同学习任务，自适应利用疾病关联和患者相似性，整合患者 - 疾病交互和电子病历人口特征。

Result: 在含61,191名患者和2,000种疾病的MIMIC - IV数据集上，CLDD在多个指标上优于基线模型，召回率提高6.33%，精度提高7.63%，案例研究显示其能在预测中恢复被掩盖疾病。

Conclusion: CLDD有助于降低诊断成本、提高可及性，有望用于大规模疾病筛查和社会健康保障。

Abstract: Accurate disease detection is of paramount importance for effective medical treatment and patient care. However, the process of disease detection is often associated with extensive medical testing and considerable costs, making it impractical to perform all possible medical tests on a patient to diagnose or predict hundreds or thousands of diseases. In this work, we propose Collaborative Learning for Disease Detection (CLDD), a novel graph-based deep learning model that formulates disease detection as a collaborative learning task by exploiting associations among diseases and similarities among patients adaptively. CLDD integrates patient-disease interactions and demographic features from electronic health records to detect hundreds or thousands of diseases for every patient, with little to no reliance on the corresponding medical tests. Extensive experiments on a processed version of the MIMIC-IV dataset comprising 61,191 patients and 2,000 diseases demonstrate that CLDD consistently outperforms representative baselines across multiple metrics, achieving a 6.33\% improvement in recall and 7.63\% improvement in precision. Furthermore, case studies on individual patients illustrate that CLDD can successfully recover masked diseases within its top-ranked predictions, demonstrating both interpretability and reliability in disease prediction. By reducing diagnostic costs and improving accessibility, CLDD holds promise for large-scale disease screening and social health security.

</details>


### [90] [LUNA: Linear Universal Neural Attention with Generalization Guarantees](https://arxiv.org/abs/2512.08061)
*Ashkan Shahbazi,Ping He,Ali Abbasi,Yikun Bai,Xinran Liu,Elaheh Akbari,Darian Salehi,Navid NaderiAlizadeh,Soheil Kolouri*

Main category: cs.LG

TL;DR: 提出核化线性注意力机制LUNA，解决了计算效率和模型精度的权衡问题，在多个任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统软注意力计算成本高，线性注意力依赖固定随机特征图，存在精度和效率的权衡问题。

Method: 引入可学习的特征映射来构建核化线性注意力机制LUNA，克服固定特征方法的表达限制。

Result: 在LRA上达到高效Transformer的最优平均精度，在事后转换中表现出色。

Conclusion: LUNA有效解决了注意力机制中的权衡问题，具有良好的性能。

Abstract: Scaling attention faces a critical bottleneck: the $\mathcal{O}(n^2)$ quadratic computational cost of softmax attention, which limits its application in long-sequence domains. While linear attention mechanisms reduce this cost to $\mathcal{O}(n)$, they typically rely on fixed random feature maps, such as random Fourier features or hand-crafted functions. This reliance on static, data-agnostic kernels creates a fundamental trade-off, forcing practitioners to sacrifice significant model accuracy for computational efficiency. We introduce \textsc{LUNA}, a kernelized linear attention mechanism that eliminates this trade-off, retaining linear cost while matching and surpassing the accuracy of quadratic attention. \textsc{LUNA} is built on the key insight that the kernel feature map itself should be learned rather than fixed a priori. By parameterizing the kernel, \textsc{LUNA} learns a feature basis tailored to the specific data and task, overcoming the expressive limitations of fixed-feature methods. \textsc{Luna} implements this with a learnable feature map that induces a positive-definite kernel and admits a streaming form, yielding linear time and memory scaling in the sequence length. Empirical evaluations validate our approach across diverse settings. On the Long Range Arena (LRA), \textsc{Luna} achieves state-of-the-art average accuracy among efficient Transformers under compute parity, using the same parameter count, training steps, and approximate FLOPs. \textsc{Luna} also excels at post-hoc conversion: replacing softmax in fine-tuned BERT and ViT-B/16 checkpoints and briefly fine-tuning recovers most of the original performance, substantially outperforming fixed linearizations.

</details>


### [91] [SA^2GFM: Enhancing Robust Graph Foundation Models with Structure-Aware Semantic Augmentation](https://arxiv.org/abs/2512.07857)
*Junhua Shi,Qingyun Sun,Haonan Yuan,Xingcheng Fu*

Main category: cs.LG

TL;DR: 提出SA^2GFM框架提升图基础模型鲁棒性，实验显示其在节点和图分类任务上优于9个基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有图基础模型在应对领域噪声、结构扰动和对抗攻击时鲁棒性研究不足，且对分层结构语义建模不够。

Method: 通过将基于熵的编码树转换为结构感知文本提示进行特征增强；采用自监督信息瓶颈机制进行结构引导压缩；引入专家自适应路由机制；提出微调模块进行联合社区内和社区间结构学习。

Result: SA^2GFM在节点和图分类任务中，在有效性和抗随机噪声及对抗扰动的鲁棒性上优于9个最先进的基线模型。

Conclusion: SA^2GFM是一个有效的、能提升图基础模型鲁棒性的框架。

Abstract: We present Graph Foundation Models (GFMs) which have made significant progress in various tasks, but their robustness against domain noise, structural perturbations, and adversarial attacks remains underexplored. A key limitation is the insufficient modeling of hierarchical structural semantics, which are crucial for generalization. In this paper, we propose SA^2GFM, a robust GFM framework that improves domain-adaptive representations through Structure-Aware Semantic Augmentation. First, we encode hierarchical structural priors by transforming entropy-based encoding trees into structure-aware textual prompts for feature augmentation. The enhanced inputs are processed by a self-supervised Information Bottleneck mechanism that distills robust, transferable representations via structure-guided compression. To address negative transfer in cross-domain adaptation, we introduce an expert adaptive routing mechanism, combining a mixture-of-experts architecture with a null expert design. For efficient downstream adaptation, we propose a fine-tuning module that optimizes hierarchical structures through joint intra- and inter-community structure learning. Extensive experiments demonstrate that SA^2GFM outperforms 9 state-of-the-art baselines in terms of effectiveness and robustness against random noise and adversarial perturbations for node and graph classification.

</details>


### [92] [Complexity of One-Dimensional ReLU DNNs](https://arxiv.org/abs/2512.08091)
*Jonathan Kogan,Hayden Jananthan,Jeremy Kepner*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study the expressivity of one-dimensional (1D) ReLU deep neural networks through the lens of their linear regions. For randomly initialized, fully connected 1D ReLU networks (He scaling with nonzero bias) in the infinite-width limit, we prove that the expected number of linear regions grows as $\sum_{i = 1}^L n_i + \mathop{o}\left(\sum_{i = 1}^L{n_i}\right) + 1$, where $n_\ell$ denotes the number of neurons in the $\ell$-th hidden layer. We also propose a function-adaptive notion of sparsity that compares the expected regions used by the network to the minimal number needed to approximate a target within a fixed tolerance.

</details>


### [93] [FAIM: Frequency-Aware Interactive Mamba for Time Series Classification](https://arxiv.org/abs/2512.07858)
*Da Zhang,Bingyu Li,Zhiyuan Zhao,Yanhan Zhang,Junyu Gao,Feiping Nie,Xuelong Li*

Main category: cs.LG

TL;DR: 提出轻量级FAIM模型用于时间序列分类，结合AFB和IMB模块及自监督预训练机制，实验显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习架构用于时间序列分类存在计算成本高、对噪声敏感、小数据集易过拟合等问题。

Method: 提出FAIM模型，引入AFB利用傅里叶变换提取频域特征并抑制噪声，设计IMB促进多粒度信息交互，加入自监督预训练机制。

Result: 在多个基准测试上，FAIM始终优于现有SOTA方法，在准确性和效率间取得良好平衡。

Conclusion: FAIM在时间序列分类任务中表现出色，具有强大的表示能力和鲁棒性。

Abstract: Time series classification (TSC) is crucial in numerous real-world applications, such as environmental monitoring, medical diagnosis, and posture recognition. TSC tasks require models to effectively capture discriminative information for accurate class identification. Although deep learning architectures excel at capturing temporal dependencies, they often suffer from high computational cost, sensitivity to noise perturbations, and susceptibility to overfitting on small-scale datasets. To address these challenges, we propose FAIM, a lightweight Frequency-Aware Interactive Mamba model. Specifically, we introduce an Adaptive Filtering Block (AFB) that leverages Fourier Transform to extract frequency-domain features from time series data. The AFB incorporates learnable adaptive thresholds to dynamically suppress noise and employs element-wise coupling of global and local semantic adaptive filtering, enabling in-depth modeling of the synergy among different frequency components. Furthermore, we design an Interactive Mamba Block (IMB) to facilitate efficient multi-granularity information interaction, balancing the extraction of fine-grained discriminative features and comprehensive global contextual information, thereby endowing FAIM with powerful and expressive representations for TSC tasks. Additionally, we incorporate a self-supervised pre-training mechanism to enhance FAIM's understanding of complex temporal patterns and improve its robustness across various domains and high-noise scenarios. Extensive experiments on multiple benchmarks demonstrate that FAIM consistently outperforms existing state-of-the-art (SOTA) methods, achieving a superior trade-off between accuracy and efficiency and exhibits outstanding performance.

</details>


### [94] [SetAD: Semi-Supervised Anomaly Learning in Contextual Sets](https://arxiv.org/abs/2512.07863)
*Jianling Gao,Chongyang Tao,Xuelian Lin,Junfeng Liu,Shuai Ma*

Main category: cs.LG

TL;DR: 提出SetAD框架解决半监督异常检测现存问题，实验显示其优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有半监督异常检测方法以点或对为中心，忽视异常上下文特性和集合组合监督信号，难以利用高阶交互。

Method: 提出SetAD框架，将半监督AD转为集合级任务，用基于注意力的集合编码器和分级学习目标训练，还有上下文校准异常评分机制。

Result: 在10个真实数据集上实验，SetAD显著优于现有模型，且性能随集合大小增加而提升。

Conclusion: 集合级异常检测方法有效，为异常检测的集合化表述提供了实证支持。

Abstract: Semi-supervised anomaly detection (AD) has shown great promise by effectively leveraging limited labeled data. However, existing methods are typically structured around scoring individual points or simple pairs. Such {point- or pair-centric} view not only overlooks the contextual nature of anomalies, which are defined by their deviation from a collective group, but also fails to exploit the rich supervisory signals that can be generated from the combinatorial composition of sets. Consequently, such models struggle to exploit the high-order interactions within the data, which are critical for learning discriminative representations. To address these limitations, we propose SetAD, a novel framework that reframes semi-supervised AD as a Set-level Anomaly Detection task. SetAD employs an attention-based set encoder trained via a graded learning objective, where the model learns to quantify the degree of anomalousness within an entire set. This approach directly models the complex group-level interactions that define anomalies. Furthermore, to enhance robustness and score calibration, we propose a context-calibrated anomaly scoring mechanism, which assesses a point's anomaly score by aggregating its normalized deviations from peer behavior across multiple, diverse contextual sets. Extensive experiments on 10 real-world datasets demonstrate that SetAD significantly outperforms state-of-the-art models. Notably, we show that our model's performance consistently improves with increasing set size, providing strong empirical support for the set-based formulation of anomaly detection.

</details>


### [95] [Using Text-Based Life Trajectories from Swedish Register Data to Predict Residential Mobility with Pretrained Transformers](https://arxiv.org/abs/2512.07865)
*Philipp Stark,Alexandros Sopasakis,Ola Hall,Markus Grillitsch*

Main category: cs.LG

TL;DR: 本文将瑞典大规模登记数据转化为文本化生活轨迹，预测个体居住流动性，对比多种NLP架构，发现文本化数据支持建模，可推动社会科学纵向分析。


<details>
  <summary>Details</summary>
Motivation: 解决数据分析中分类变量高基数和编码方案随时间不一致的问题，探索文本化数据在纵向预测中的有效性。

Method: 将690万个体的登记数据转化为语义丰富的文本，对比LSTM、DistilBERT、BERT和Qwen等NLP架构。

Result: 顺序和基于Transformer的模型比基线模型更能有效捕捉时间和语义结构，文本化登记数据保留个体路径信息，支持复杂可扩展建模。

Conclusion: 结合语义丰富的登记数据和现代语言模型可大幅推进社会科学的纵向分析。

Abstract: We transform large-scale Swedish register data into textual life trajectories to address two long-standing challenges in data analysis: high cardinality of categorical variables and inconsistencies in coding schemes over time. Leveraging this uniquely comprehensive population register, we convert register data from 6.9 million individuals (2001-2013) into semantically rich texts and predict individuals' residential mobility in later years (2013-2017). These life trajectories combine demographic information with annual changes in residence, work, education, income, and family circumstances, allowing us to assess how effectively such sequences support longitudinal prediction. We compare multiple NLP architectures (including LSTM, DistilBERT, BERT, and Qwen) and find that sequential and transformer-based models capture temporal and semantic structure more effectively than baseline models. The results show that textualized register data preserves meaningful information about individual pathways and supports complex, scalable modeling. Because few countries maintain longitudinal microdata with comparable coverage and precision, this dataset enables analyses and methodological tests that would be difficult or impossible elsewhere, offering a rigorous testbed for developing and evaluating new sequence-modeling approaches. Overall, our findings demonstrate that combining semantically rich register data with modern language models can substantially advance longitudinal analysis in social sciences.

</details>


### [96] [A Multivariate Bernoulli-Based Sampling Method for Multi-Label Data with Application to Meta-Research](https://arxiv.org/abs/2512.08371)
*Simon Chung,Colby J. Vorland,Donna L. Maney,Andrew W. Brown*

Main category: cs.LG

TL;DR: 论文针对多标签数据采样难题，提出考虑标签依赖的多元伯努利分布采样算法，并应用于文献样本，得到更平衡子样本。


<details>
  <summary>Details</summary>
Motivation: 多标签数据中标签不互斥且频率差异大，获取含足够稀缺标签观测值并符合特定采样偏差要求的样本存在挑战。

Method: 采用多元伯努利分布作为多标签问题的基础分布，提出考虑标签依赖的采样算法，利用观测标签频率估计分布参数并计算标签组合权重。

Result: 将该方法应用于Web of Science文献样本，得到更平衡的子样本。

Conclusion: 该方法能保留类别频率顺序，减少常见与罕见类别频率差异，考虑类别依赖，增强少数类别的代表性。

Abstract: Datasets may contain observations with multiple labels. If the labels are not mutually exclusive, and if the labels vary greatly in frequency, obtaining a sample that includes sufficient observations with scarcer labels to make inferences about those labels, and which deviates from the population frequencies in a known manner, creates challenges. In this paper, we consider a multivariate Bernoulli distribution as our underlying distribution of a multi-label problem. We present a novel sampling algorithm that takes label dependencies into account. It uses observed label frequencies to estimate multivariate Bernoulli distribution parameters and calculate weights for each label combination. This approach ensures the weighted sampling acquires target distribution characteristics while accounting for label dependencies. We applied this approach to a sample of research articles from Web of Science labeled with 64 biomedical topic categories. We aimed to preserve category frequency order, reduce frequency differences between most and least common categories, and account for category dependencies. This approach produced a more balanced sub-sample, enhancing the representation of minority categories.

</details>


### [97] [Command & Control (C2) Traffic Detection Via Algorithm Generated Domain (Dga) Classification Using Deep Learning And Natural Language Processing](https://arxiv.org/abs/2512.07866)
*Maria Milena Araujo Felix*

Main category: cs.LG

TL;DR: 因现代恶意软件发展，基于静态黑名单的防御失效，本文用深度学习和NLP技术检测DGA域名，神经网络方法效果好，准确率达97.2%。


<details>
  <summary>Details</summary>
Motivation: 现代恶意软件利用DGA使基于静态黑名单的防御失效，传统防火墙难以阻挡，需新检测方法。

Method: 收集包含5万合法和5万恶意域名的混合数据库，提取词汇特征，训练LSTM循环神经网络。

Result: 统计熵分析对简单DGA有效，神经网络方法在检测复杂模式上更优，准确率达97.2%，降低模糊合法流量场景下的误报率。

Conclusion: 使用深度学习和NLP技术训练的神经网络方法在检测DGA域名上有较好效果。

Abstract: The sophistication of modern malware, specifically regarding communication with Command and Control (C2) servers, has rendered static blacklist-based defenses obsolete. The use of Domain Generation Algorithms (DGA) allows attackers to generate thousands of dynamic addresses daily, hindering blocking by traditional firewalls. This paper aims to propose and evaluate a method for detecting DGA domains using Deep Learning and Natural Language Processing (NLP) techniques. The methodology consisted of collecting a hybrid database containing 50,000 legitimate and 50,000 malicious domains, followed by the extraction of lexical features and the training of a Recurrent Neural Network (LSTM). Results demonstrated that while statistical entropy analysis is effective for simple DGAs, the Neural Network approach presents superiority in detecting complex patterns, reaching 97.2% accuracy and reducing the false positive rate in ambiguous lawful traffic scenarios.

</details>


### [98] [Advancing physiological time series reconstruction and imputation via mixture of receptive fields and experts fusion](https://arxiv.org/abs/2512.07873)
*Ci Zhang,Huayu Li,Changdi Yang,Jiangnan Xia,Yanzhi Wang,Xiaolong Ma,Jin Lu,Geng Yuan*

Main category: cs.LG

TL;DR: 提出基于MoE的噪声估计器用于医学时间序列信号重建，设计RFAMoE和Fusion MoE模块，单步推理完成重建，性能优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型用于医学时间序列信号重建研究少，生理时间序列特性使深度学习方法用于插补等任务有挑战，多步推理有计算和延迟开销。

Method: 在基于分数的扩散框架中提出基于MoE的噪声估计器，设计RFAMoE模块让各通道自适应选择感受野，设计Fusion MoE模块并行生成K个噪声信号并融合，单步推理完成信号重建。

Result: 所提框架在不同任务和数据集上始终优于基于扩散的SOTA方法。

Conclusion: 所提基于MoE的框架能有效用于医学时间序列信号重建，提升性能并降低计算成本和延迟。

Abstract: Recent studies show that using diffusion models for time series signal reconstruc- tion holds great promise. However, such approaches remain largely unexplored in the domain of medical time series. The unique characteristics of the physiological time series signals, such as multivariate, high temporal variability, highly noisy, and artifact-prone, make deep learning-based approaches still challenging for tasks such as imputation. Hence, we propose a novel Mixture of Experts (MoE)-based noise estimator within a score-based diffusion framework. Specifically, the Receptive Field Adaptive MoE (RFAMoE) module is designed to enable each channel to adap- tively select desired receptive fields throughout the diffusion process. Moreover, recent literature has found that when generating a physiological signal, performing multiple inferences and averaging the reconstructed signals can effectively reduce reconstruction errors, but at the cost of significant computational and latency over- head. We design a Fusion MoE module and innovatively leverage the nature of MoE module to generate K noise signals in parallel, fuse them using a routing mechanism, and complete signal reconstruction in a single inference step. This design not only improves performance over previous methods but also eliminates the substantial computational cost and latency associated with multiple inference processes. Extensive results demonstrate that our proposed framework consistently outperforms diffusion-based SOTA works on different tasks and datasets.

</details>


### [99] [DS FedProxGrad: Asymptotic Stationarity Without Noise Floor in Fair Federated Learning](https://arxiv.org/abs/2512.08671)
*Huzaifa Arif*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Recent work \cite{arifgroup} introduced Federated Proximal Gradient \textbf{(\texttt{FedProxGrad})} for solving non-convex composite optimization problems in group fair federated learning. However, the original analysis established convergence only to a \textit{noise-dominated neighborhood of stationarity}, with explicit dependence on a variance-induced noise floor. In this work, we provide an improved asymptotic convergence analysis for a generalized \texttt{FedProxGrad}-type analytical framework with inexact local proximal solutions and explicit fairness regularization. We call this extended analytical framework \textbf{DS \texttt{FedProxGrad}} (Decay Step Size \texttt{FedProxGrad}). Under a Robbins-Monro step-size schedule \cite{robbins1951stochastic} and a mild decay condition on local inexactness, we prove that $\liminf_{r\to\infty} \mathbb{E}[\|\nabla F(\mathbf{x}^r)\|^2] = 0$, i.e., the algorithm is asymptotically stationary and the convergence rate does not depend on a variance-induced noise floor.

</details>


### [100] [Controllable risk scenario generation from human crash data for autonomous vehicle testing](https://arxiv.org/abs/2512.07874)
*Qiujing Lu,Xuanhan Wang,Runze Yuan,Wei Lu,Xinyi Gong,Shuo Feng*

Main category: cs.LG

TL;DR: 提出Controllable Risk Agent Generation (CRAG)框架，统一正常和安全关键行为建模，实验表明其能提高多样性并可控生成风险场景。


<details>
  <summary>Details</summary>
Motivation: 确保自动驾驶车辆安全需严格测试，挑战在于模拟能在正常交通中现实行为且展现与真实事故一致的易发生风险行为的环境代理。

Method: 构建结构化潜在空间分离正常和风险相关行为，结合风险感知潜在表示与基于优化的模式转换机制。

Result: 与现有基线相比，CRAG提高了多样性，可可控生成风险场景。

Conclusion: CRAG可用于有针对性和高效地评估自动驾驶车辆的鲁棒性。

Abstract: Ensuring the safety of autonomous vehicles (AV) requires rigorous testing under both everyday driving and rare, safety-critical conditions. A key challenge lies in simulating environment agents, including background vehicles (BVs) and vulnerable road users (VRUs), that behave realistically in nominal traffic while also exhibiting risk-prone behaviors consistent with real-world accidents. We introduce Controllable Risk Agent Generation (CRAG), a framework designed to unify the modeling of dominant nominal behaviors and rare safety-critical behaviors. CRAG constructs a structured latent space that disentangles normal and risk-related behaviors, enabling efficient use of limited crash data. By combining risk-aware latent representations with optimization-based mode-transition mechanisms, the framework allows agents to shift smoothly and plausibly from safe to risk states over extended horizons, while maintaining high fidelity in both regimes. Extensive experiments show that CRAG improves diversity compared to existing baselines, while also enabling controllable generation of risk scenarios for targeted and efficient evaluation of AV robustness.

</details>


### [101] [Bridging the Clinical Expertise Gap: Development of a Web-Based Platform for Accessible Time Series Forecasting and Analysis](https://arxiv.org/abs/2512.07992)
*Aaron D. Mullen,Daniel R. Harris,Svetla Slavova,V. K. Cody Bumgardner*

Main category: cs.LG

TL;DR: 文章介绍了一个使时间序列预测分析过程更易上手的网络平台，可用于医疗领域，还欲集成到学习健康系统。


<details>
  <summary>Details</summary>
Motivation: 时间序列预测技术所需专业知识成为医疗等领域应用障碍，需让研究者和临床医生更易使用。

Method: 开发一个具有数据上传、绘图、支持多种可定制预测模型和训练技术，且能借助大语言模型生成建议和解释的网络平台。

Result: 构建了可让用户上传数据、生成绘图、根据需求定制模型并从大语言模型获取参数建议和结果解释的平台。

Conclusion: 目标是将该平台集成到学习健康系统以实现临床管道的持续数据收集和推理。

Abstract: Time series forecasting has applications across domains and industries, especially in healthcare, but the technical expertise required to analyze data, build models, and interpret results can be a barrier to using these techniques. This article presents a web platform that makes the process of analyzing and plotting data, training forecasting models, and interpreting and viewing results accessible to researchers and clinicians. Users can upload data and generate plots to showcase their variables and the relationships between them. The platform supports multiple forecasting models and training techniques which are highly customizable according to the user's needs. Additionally, recommendations and explanations can be generated from a large language model that can help the user choose appropriate parameters for their data and understand the results for each model. The goal is to integrate this platform into learning health systems for continuous data collection and inference from clinical pipelines.

</details>


### [102] [Unsupervised Learning of Density Estimates with Topological Optimization](https://arxiv.org/abs/2512.08895)
*Suina Tanweer,Firas A. Khasawneh*

Main category: cs.LG

TL;DR: 本文提出基于拓扑的无监督学习方法自动选择核密度估计的最优带宽，并与经典技术对比。


<details>
  <summary>Details</summary>
Motivation: 核密度估计需调整核带宽这一关键超参数，而拓扑数据分析可量化拓扑特征，因此想利用其实现带宽自动选择。

Method: 提出使用基于拓扑的损失函数的无监督学习方法来选择最优带宽。

Result: 将该方法与经典技术进行了对比，展示了其在不同维度上的潜力。

Conclusion: 所提出的无监督学习方法能有效自动选择核密度估计的最优带宽。

Abstract: Kernel density estimation is a key component of a wide variety of algorithms in machine learning, Bayesian inference, stochastic dynamics and signal processing. However, the unsupervised density estimation technique requires tuning a crucial hyperparameter: the kernel bandwidth. The choice of bandwidth is critical as it controls the bias-variance trade-off by over- or under-smoothing the topological features. Topological data analysis provides methods to mathematically quantify topological characteristics, such as connected components, loops, voids et cetera, even in high dimensions where visualization of density estimates is impossible. In this paper, we propose an unsupervised learning approach using a topology-based loss function for the automated and unsupervised selection of the optimal bandwidth and benchmark it against classical techniques -- demonstrating its potential across different dimensions.

</details>


### [103] [Artificial Intelligence-Driven Network-on-Chip Design Space Exploration: Neural Network Architectures for Design](https://arxiv.org/abs/2512.07877)
*Amogh Anshu N,Harish BP*

Main category: cs.LG

TL;DR: 提出机器学习驱动框架自动探索片上网络设计空间，对比三种架构，条件扩散模型预测精度最高，框架大幅减少设计探索时间。


<details>
  <summary>Details</summary>
Motivation: 传统片上网络设计空间探索技术慢且难处理复杂非线性参数交互，需高效方法满足吞吐量和延迟要求。

Method: 使用BookSim模拟和反向神经网络模型，对比多层感知器、条件扩散模型和条件变分自编码器三种架构。

Result: 条件扩散模型预测精度最高，在未见数据上均方误差为0.463，框架大幅减少设计探索时间。

Conclusion: 提出的框架是快速可扩展片上网络协同设计的实用解决方案。

Abstract: Network-on-Chip (NoC) design requires exploring a high-dimensional configuration space to satisfy stringent throughput requirements and latency constraints.Traditional design space exploration techniques are often slow and struggle to handle complex, non-linear parameter interactions.This work presents a machine learning-driven framework that automates NoC design space exploration using BookSim simulations and reverse neural network models.Specifically, we compare three architectures - a Multi-Layer Perceptron (MLP),a Conditional Diffusion Model, and a Conditional Variational Autoencoder (CVAE) to predict optimal NoC parameters given target performance metrics.Our pipeline generates over 150,000 simulation data points across varied mesh topologies.The Conditional Diffusion Model achieved the highest predictive accuracy, attaining a mean squared error (MSE) of 0.463 on unseen data.Furthermore, the proposed framework reduces design exploration time by several orders of magnitude, making it a practical solution for rapid and scalable NoC co-design.

</details>


### [104] [Graph Contrastive Learning via Spectral Graph Alignment](https://arxiv.org/abs/2512.07878)
*Manh Nguyen,Joshua Cape*

Main category: cs.LG

TL;DR: 提出SpecMatch - CL损失函数，理论上证明其优势，实验上在多个基准测试取得新SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有对比学习方法无法控制从图嵌入构建的特定视图图的全局结构。

Method: 引入SpecMatch - CL损失函数，通过最小化特定视图图的归一化拉普拉斯矩阵之间的差异来对齐它们。

Result: 在八个TU基准测试的无监督和低标签半监督学习中达到新SOTA，在PPI - 306K和ZINC 2M数据集的迁移学习中有稳定提升。

Conclusion: SpecMatch - CL损失函数有效，可提升图对比学习效果。

Abstract: Given augmented views of each input graph, contrastive learning methods (e.g., InfoNCE) optimize pairwise alignment of graph embeddings across views while providing no mechanism to control the global structure of the view specific graph-of-graphs built from these embeddings. We introduce SpecMatch-CL, a novel loss function that aligns the view specific graph-of-graphs by minimizing the difference between their normalized Laplacians. Theoretically, we show that under certain assumptions, the difference between normalized Laplacians provides an upper bound not only for the difference between the ideal Perfect Alignment contrastive loss and the current loss, but also for the Uniformly loss. Empirically, SpecMatch-CL establishes new state of the art on eight TU benchmarks under unsupervised learning and semi-supervised learning at low label rates, and yields consistent gains in transfer learning on PPI-306K and ZINC 2M datasets.

</details>


### [105] [Nonnegative Matrix Factorization through Cone Collapse](https://arxiv.org/abs/2512.07879)
*Manh Nguyen,Daniel Pimentel-Alarcón*

Main category: cs.LG

TL;DR: 本文从几何角度重新审视非负矩阵分解（NMF），提出Cone Collapse算法，推导了CC - NMF模型，在多个数据集上表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有NMF聚类算法未充分利用NMF诱导的锥几何特性，本文从几何角度改进。

Method: 提出Cone Collapse算法，从全非负正交方向开始迭代收缩至数据生成的最小锥；基于此推导CC - NMF模型。

Result: 在16个基准数据集上，CC - NMF在聚类纯度上始终匹配或优于其他NMF基线方法。

Conclusion: 显式恢复数据锥能产生理论可靠且经验有效的基于NMF的聚类方法。

Abstract: Nonnegative matrix factorization (NMF) is a widely used tool for learning parts-based, low-dimensional representations of nonnegative data, with applications in vision, text, and bioinformatics. In clustering applications, orthogonal NMF (ONMF) variants further impose (approximate) orthogonality on the representation matrix so that its rows behave like soft cluster indicators. Existing algorithms, however, are typically derived from optimization viewpoints and do not explicitly exploit the conic geometry induced by NMF: data points lie in a convex cone whose extreme rays encode fundamental directions or "topics". In this work we revisit NMF from this geometric perspective and propose Cone Collapse, an algorithm that starts from the full nonnegative orthant and iteratively shrinks it toward the minimal cone generated by the data. We prove that, under mild assumptions on the data, Cone Collapse terminates in finitely many steps and recovers the minimal generating cone of $\mathbf{X}^\top$ . Building on this basis, we then derive a cone-aware orthogonal NMF model (CC-NMF) by applying uni-orthogonal NMF to the recovered extreme rays. Across 16 benchmark gene-expression, text, and image datasets, CC-NMF consistently matches or outperforms strong NMF baselines-including multiplicative updates, ANLS, projective NMF, ONMF, and sparse NMF-in terms of clustering purity. These results demonstrate that explicitly recovering the data cone can yield both theoretically grounded and empirically strong NMF-based clustering methods.

</details>


### [106] [Semi-Supervised Contrastive Learning with Orthonormal Prototypes](https://arxiv.org/abs/2512.07880)
*Huanran Li,Manh Nguyen,Daniel Pimentel-Alarcón*

Main category: cs.LG

TL;DR: 本文指出对比学习中维数坍塌问题，提出CLOP损失函数防止坍塌，实验证明其能提升图像分类和目标检测性能及稳定性。


<details>
  <summary>Details</summary>
Motivation: 对比学习存在维数坍塌问题，尤其在半监督和自监督场景中，需要解决该问题。

Method: 确定学习率阈值，提出CLOP半监督损失函数，促进类嵌入之间正交线性子空间的形成。

Result: 在真实和合成数据集上的实验表明，CLOP提升了图像分类和目标检测任务的性能，且在不同学习率和批量大小下更稳定。

Conclusion: CLOP能有效防止对比学习中的维数坍塌，提升任务性能和稳定性。

Abstract: Contrastive learning has emerged as a powerful method in deep learning, excelling at learning effective representations through contrasting samples from different distributions. However, dimensional collapse, where embeddings converge into a lower-dimensional space, poses a significant challenge, especially in semi-supervised and self-supervised setups. In this paper, we first identify a critical learning-rate threshold, beyond which standard contrastive losses converge to collapsed solutions. Building on these insights, we propose CLOP, a novel semi-supervised loss function designed to prevent dimensional collapse by promoting the formation of orthogonal linear subspaces among class embeddings. Through extensive experiments on real and synthetic datasets, we demonstrate that CLOP improves performance in image classification and object detection tasks while also exhibiting greater stability across different learning rates and batch sizes.

</details>


### [107] [GSPN-2: Efficient Parallel Sequence Modeling](https://arxiv.org/abs/2512.07884)
*Hongjun Wang,Yitong Jiang,Collin McCarthy,David Wehr,Hanrong Ye,Xinhao Li,Ka Chun Cheung,Wonmin Byeon,Jinwei Gu,Ke Chen,Kai Han,Hongxu Yin,Pavlo Molchanov,Jan Kautz,Sifei Liu*

Main category: cs.LG

TL;DR: 当前高效视觉变压器在高分辨率图像等应用中有瓶颈，已有GSPN有不足，本文提出GSPN - 2，通过算法 - 系统联合重设计，减少计算开销，实验证明其在多任务中有效，确立新效率边界。


<details>
  <summary>Details</summary>
Motivation: 现有GSPN实现存在多次启动GPU内核开销大、数据传输过多、各通道维护单独传播权重导致冗余计算等问题，需要改进。

Method: 引入GSPN - 2，进行算法 - 系统联合重设计。消除数千次微启动变为一个2D内核，将一个线程束固定到每个通道切片，将前一列激活存于共享内存；模型方面引入紧凑通道传播策略。

Result: 实验表明GSPN - 2在图像分类和文本转图像合成任务中有效，以显著更低计算成本达到变压器级精度。

Conclusion: GSPN - 2通过结构矩阵变换和GPU优化实现的独特组合，为视觉应用中全局空间上下文建模确立了新的效率边界。

Abstract: Efficient vision transformer remains a bottleneck for high-resolution images and long-video related real-world applications. Generalized Spatial Propagation Network (GSPN) addresses this by replacing quadratic self-attention with a line-scan propagation scheme, bringing the cost close to linear in the number of rows or columns, while retaining accuracy. Despite this advancement, the existing GSPN implementation still suffers from (i) heavy overhead due to repeatedly launching GPU kernels, (ii) excessive data transfers from global GPU memory, and (iii) redundant computations caused by maintaining separate propagation weights for each channel. We introduce GSPN-2, a joint algorithm-system redesign. In particular, we eliminate thousands of micro-launches from the previous implementation into one single 2D kernel, explicitly pin one warp to each channel slice, and stage the previous column's activations in shared memory. On the model side, we introduce a compact channel propagation strategy that replaces per-channel matrices, trimming parameters, and align naturally with the affinity map used in transformer attention. Experiments demonstrate GSPN-2's effectiveness across image classification and text-to-image synthesis tasks, matching transformer-level accuracy with significantly lower computational cost. GSPN-2 establishes a new efficiency frontier for modeling global spatial context in vision applications through its unique combination of structured matrix transformations and GPU-optimized implementation. Project page: https://whj363636.github.io/GSPN2/

</details>


### [108] [ByteStorm: a multi-step data-driven approach for Tropical Cyclones detection and tracking](https://arxiv.org/abs/2512.07885)
*Davide Donno,Donatello Elia,Gabriele Accarino,Marco De Carlo,Enrico Scoccimarro,Silvio Gualdi*

Main category: cs.LG

TL;DR: 提出ByteStorm框架用于重建热带气旋轨迹，无需阈值调整，在东西北太平洋盆地表现优异，凸显深度学习和计算机视觉在热带气旋跟踪中的潜力。


<details>
  <summary>Details</summary>
Motivation: 传统热带气旋跟踪方案依赖主观阈值，在不同地理区域应用可能产生偏差，需更准确的跟踪方法。

Method: 利用深度学习网络，仅使用相对涡度（850 mb）和平均海平面压力检测热带气旋中心，通过BYTE算法将检测到的中心连接成轨迹。

Result: 在东西北太平洋盆地，ByteStorm在检测概率、误报率和年际变率相关性方面表现优于现有确定性跟踪器。

Conclusion: 深度学习和计算机视觉结合可实现快速准确的热带气旋跟踪，是传统方法的有力替代。

Abstract: Accurate tropical cyclones (TCs) tracking represents a critical challenge in the context of weather and climate science. Traditional tracking schemes mainly rely on subjective thresholds, which may introduce biases in their skills on the geographical region of application. We present ByteStorm, an efficient data-driven framework for reconstructing TC tracks without threshold tuning. It leverages deep learning networks to detect TC centers (via classification and localization), using only relative vorticity (850 mb) and mean sea-level pressure. Then, detected centers are linked into TC tracks through the BYTE algorithm. ByteStorm is evaluated against state-of-the-art deterministic trackers in the East- and West-North Pacific basins (ENP and WNP). The proposed framework achieves superior performance in terms of Probability of Detection ($85.05\%$ ENP, $79.48\%$ WNP), False Alarm Rate ($23.26\%$ ENP, $16.14\%$ WNP), and high Inter-Annual Variability correlations ($0.75$ ENP and $0.69$ WNP). These results highlight the potential of integrating deep learning and computer vision for fast and accurate TC tracking, offering a robust alternative to traditional approaches.

</details>


### [109] [CIP-Net: Continual Interpretable Prototype-based Network](https://arxiv.org/abs/2512.07981)
*Federico Di Valerio,Michela Proietti,Alessio Ragno,Roberto Capobianco*

Main category: cs.LG

TL;DR: 本文提出用于持续学习的无样本自解释原型模型CIP - Net，在不同增量设置下表现优异且内存开销低。


<details>
  <summary>Details</summary>
Motivation: 持续学习存在灾难性遗忘问题，现有可解释方法扩展性有限，需更好方案。

Method: 引入无样本自解释原型模型CIP - Net，避免存储过往样本，保持简单架构。

Result: CIP - Net在任务和类别增量设置下比先前无样本和自解释方法有最优表现，内存开销显著降低。

Conclusion: CIP - Net是持续学习实用且可解释的解决方案。

Abstract: Continual learning constrains models to learn new tasks over time without forgetting what they have already learned. A key challenge in this setting is catastrophic forgetting, where learning new information causes the model to lose its performance on previous tasks. Recently, explainable AI has been proposed as a promising way to better understand and reduce forgetting. In particular, self-explainable models are useful because they generate explanations during prediction, which can help preserve knowledge. However, most existing explainable approaches use post-hoc explanations or require additional memory for each new task, resulting in limited scalability. In this work, we introduce CIP-Net, an exemplar-free self-explainable prototype-based model designed for continual learning. CIP-Net avoids storing past examples and maintains a simple architecture, while still providing useful explanations and strong performance. We demonstrate that CIPNet achieves state-of-the-art performances compared to previous exemplar-free and self-explainable methods in both task- and class-incremental settings, while bearing significantly lower memory-related overhead. This makes it a practical and interpretable solution for continual learning.

</details>


### [110] [HOLE: Homological Observation of Latent Embeddings for Neural Network Interpretability](https://arxiv.org/abs/2512.07988)
*Sudhanva Manjunath Athreya,Paul Rosen*

Main category: cs.LG

TL;DR: 本文介绍HOLE方法，用持久同调分析和解释深度神经网络，评估显示拓扑分析能为理解和改进深度学习系统提供补充视角。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型表征和决策过程难以解释，需要方法分析和解释。

Method: 引入HOLE方法，通过持久同调分析深度神经网络，提取拓扑特征并用多种可视化技术呈现。

Result: 拓扑分析揭示与类别分离、特征解缠和模型鲁棒性相关的模式。

Conclusion: 拓扑分析为理解和改进深度学习系统提供了补充视角。

Abstract: Deep learning models have achieved remarkable success across various domains, yet their learned representations and decision-making processes remain largely opaque and hard to interpret. This work introduces HOLE (Homological Observation of Latent Embeddings), a method for analyzing and interpreting deep neural networks through persistent homology. HOLE extracts topological features from neural activations and presents them using a suite of visualization techniques, including Sankey diagrams, heatmaps, dendrograms, and blob graphs. These tools facilitate the examination of representation structure and quality across layers. We evaluate HOLE on standard datasets using a range of discriminative models, focusing on representation quality, interpretability across layers, and robustness to input perturbations and model compression. The results indicate that topological analysis reveals patterns associated with class separation, feature disentanglement, and model robustness, providing a complementary perspective for understanding and improving deep learning systems.

</details>


### [111] [Benchmarking Offline Multi-Objective Reinforcement Learning in Critical Care](https://arxiv.org/abs/2512.08012)
*Aryaman Bansal,Divya Sharma*

Main category: cs.LG

TL;DR: 本文在MIMIC - IV数据集上对三种离线多目标强化学习算法与三种单目标基线算法进行基准测试，发现PEDA DT算法更具灵活性，表明离线多目标强化学习在重症监护个性化决策方面有前景。


<details>
  <summary>Details</summary>
Motivation: 重症监护中临床医生需平衡患者生存和资源利用的目标，单目标强化学习方法策略僵化，多目标强化学习可解决此问题，但医疗应用需严格离线学习，所以要对相关算法进行研究。

Method: 在MIMIC - IV数据集上对三种离线多目标强化学习算法（CPQL、Adaptive CPQL、PEDA DT）和三种标量化单目标基线算法（BC、CQL、DDQN）进行基准测试，使用离策略评估指标。

Result: PEDA DT算法比静态标量化基线算法有更好的灵活性，且序列建模架构在扩展到多目标条件生成时仍稳健有效。

Conclusion: 离线多目标强化学习是重症监护中实现个性化、可调整决策的有前景的框架，无需重新训练。

Abstract: In critical care settings such as the Intensive Care Unit, clinicians face the complex challenge of balancing conflicting objectives, primarily maximizing patient survival while minimizing resource utilization (e.g., length of stay). Single-objective Reinforcement Learning approaches typically address this by optimizing a fixed scalarized reward function, resulting in rigid policies that fail to adapt to varying clinical priorities. Multi-objective Reinforcement Learning (MORL) offers a solution by learning a set of optimal policies along the Pareto Frontier, allowing for dynamic preference selection at test time. However, applying MORL in healthcare necessitates strict offline learning from historical data.
  In this paper, we benchmark three offline MORL algorithms, Conditioned Conservative Pareto Q-Learning (CPQL), Adaptive CPQL, and a modified Pareto Efficient Decision Agent (PEDA) Decision Transformer (PEDA DT), against three scalarized single-objective baselines (BC, CQL, and DDQN) on the MIMIC-IV dataset. Using Off-Policy Evaluation (OPE) metrics, we demonstrate that PEDA DT algorithm offers superior flexibility compared to static scalarized baselines. Notably, our results extend previous findings on single-objective Decision Transformers in healthcare, confirming that sequence modeling architectures remain robust and effective when scaled to multi-objective conditioned generation. These findings suggest that offline MORL is a promising framework for enabling personalized, adjustable decision-making in critical care without the need for retraining.

</details>


### [112] [CLARITY: Medical World Model for Guiding Treatment Decisions by Modeling Context-Aware Disease Trajectories in Latent Space](https://arxiv.org/abs/2512.08029)
*Tianxingjian Ding,Yuanhao Zou,Chen Chen,Mubarak Shah,Yu Tian*

Main category: cs.LG

TL;DR: 现有静态AI预测器无法预测癌症动态疾病演变，现有医学世界模型应用有局限，为此提出CLARITY模型，性能优异。


<details>
  <summary>Details</summary>
Motivation: 当前静态AI预测器不能预测动态疾病演变，现有医学世界模型应用有限，忽略患者时间和临床背景且缺乏反馈机制。

Method: 引入CLARITY模型，在结构化潜在空间直接预测疾病演变，整合时间间隔和患者特定数据，引入预测到决策框架。

Result: CLARITY在治疗规划上表现达到先进水平，在MU - Glioma - Post数据集上比近期MeWM表现好12%，远超其他医疗特定大语言模型。

Conclusion: CLARITY模型能有效解决现有医学预测模型的问题，为癌症临床决策提供更优方案。

Abstract: Clinical decision-making in oncology requires predicting dynamic disease evolution, a task current static AI predictors cannot perform. While world models (WMs) offer a paradigm for generative prediction, existing medical applications remain limited. Existing methods often rely on stochastic diffusion models, focusing on visual reconstruction rather than causal, physiological transitions. Furthermore, in medical domain, models like MeWM typically ignore patient-specific temporal and clinical contexts and lack a feedback mechanism to link predictions to treatment decisions. To address these gaps, we introduce CLARITY, a medical world model that forecasts disease evolution directly within a structured latent space. It explicitly integrates time intervals (temporal context) and patient-specific data (clinical context) to model treatment-conditioned progression as a smooth, interpretable trajectory, and thus generate physiologically faithful, individualized treatment plans. Finally, CLARITY introduces a novel prediction-to-decision framework, translating latent rollouts into transparent, actionable recommendations. CLARITY demonstrates state-of-the-art performance in treatment planning. On the MU-Glioma-Post dataset, our approach outperforms recent MeWM by 12\%, and significantly surpasses all other medical-specific large language models.

</details>


### [113] [Deep Kernel Aalen-Johansen Estimator: An Interpretable and Flexible Neural Net Framework for Competing Risks](https://arxiv.org/abs/2512.08063)
*Xiaobin Shen,George H. Chen*

Main category: cs.LG

TL;DR: 提出可解释的深度竞争风险模型DKAJ，在四个标准数据集上有竞争力且可可视化辅助解释。


<details>
  <summary>Details</summary>
Motivation: 推广经典Aalen - Johansen非参数累积发生率函数（CIFs）估计，提供可解释的深度竞争风险模型。

Method: 将每个数据点表示为聚类的加权组合，权重来自自动学习的衡量数据点相似度的核函数。

Result: 在四个标准竞争风险数据集上，DKAJ与最先进的基线模型有竞争力。

Conclusion: DKAJ能在有竞争力的同时提供可视化辅助模型解释。

Abstract: We propose an interpretable deep competing risks model called the Deep Kernel Aalen-Johansen (DKAJ) estimator, which generalizes the classical Aalen-Johansen nonparametric estimate of cumulative incidence functions (CIFs). Each data point (e.g., patient) is represented as a weighted combination of clusters. If a data point has nonzero weight only for one cluster, then its predicted CIFs correspond to those of the classical Aalen-Johansen estimator restricted to data points from that cluster. These weights come from an automatically learned kernel function that measures how similar any two data points are. On four standard competing risks datasets, we show that DKAJ is competitive with state-of-the-art baselines while being able to provide visualizations to assist model interpretation.

</details>


### [114] [CAMO: Causality-Guided Adversarial Multimodal Domain Generalization for Crisis Classification](https://arxiv.org/abs/2512.08071)
*Pingchuan Ma,Chengshuai Zhao,Bohan Jiang,Saketh Vishnubhatla,Ujun Jeong,Alimohammad Beigi,Adrienne Raglin,Huan Liu*

Main category: cs.LG

TL;DR: 提出因果引导的多模态域泛化框架用于社交媒体危机分类，实验表明该方法在未知灾难场景中性能最佳。


<details>
  <summary>Details</summary>
Motivation: 现有危机分类方法缺乏跨未知灾难类型的泛化能力，存在未分离虚假和因果特征、未对齐多模态表示的问题。

Method: 引入因果引导的多模态域泛化（MMDG）框架，结合对抗解缠和统一表示学习。

Result: 在不同数据集实验中，该方法在未知灾难场景中取得最佳性能。

Conclusion: 所提出的MMDG框架能有效解决现有方法的问题，提升危机分类的泛化能力。

Abstract: Crisis classification in social media aims to extract actionable disaster-related information from multimodal posts, which is a crucial task for enhancing situational awareness and facilitating timely emergency responses. However, the wide variation in crisis types makes achieving generalizable performance across unseen disasters a persistent challenge. Existing approaches primarily leverage deep learning to fuse textual and visual cues for crisis classification, achieving numerically plausible results under in-domain settings. However, they exhibit poor generalization across unseen crisis types because they 1. do not disentangle spurious and causal features, resulting in performance degradation under domain shift, and 2. fail to align heterogeneous modality representations within a shared space, which hinders the direct adaptation of established single-modality domain generalization (DG) techniques to the multimodal setting. To address these issues, we introduce a causality-guided multimodal domain generalization (MMDG) framework that combines adversarial disentanglement with unified representation learning for crisis classification. The adversarial objective encourages the model to disentangle and focus on domain-invariant causal features, leading to more generalizable classifications grounded in stable causal mechanisms. The unified representation aligns features from different modalities within a shared latent space, enabling single-modality DG strategies to be seamlessly extended to multimodal learning. Experiments on the different datasets demonstrate that our approach achieves the best performance in unseen disaster scenarios.

</details>


### [115] [Unveiling Latent Knowledge in Chemistry Language Models through Sparse Autoencoders](https://arxiv.org/abs/2512.08077)
*Jaron Cohen,Alexander G. Hasson,Sara Tanovic*

Main category: cs.LG

TL;DR: 本文将稀疏自编码器技术扩展到化学语言模型，以揭示可解释特征，发现模型编码了丰富化学概念，提供了通用框架并有望加速计算化学研究。


<details>
  <summary>Details</summary>
Motivation: 机器学习可解释性是持续挑战，化学语言模型内部表示化学知识的方式尚不明确。

Method: 将稀疏自编码器技术扩展到化学语言模型，应用于FM4M SMI - TED化学基础模型，提取潜在特征并分析其激活模式。

Result: 发现模型编码了丰富化学概念，识别出特定潜在特征与不同化学知识领域的相关性。

Conclusion: 该方法为聚焦化学的AI系统提供了通用框架，对基础理解和实际应用有意义，有望加速计算化学研究。

Abstract: Since the advent of machine learning, interpretability has remained a persistent challenge, becoming increasingly urgent as generative models support high-stakes applications in drug and material discovery. Recent advances in large language model (LLM) architectures have yielded chemistry language models (CLMs) with impressive capabilities in molecular property prediction and molecular generation. However, how these models internally represent chemical knowledge remains poorly understood. In this work, we extend sparse autoencoder techniques to uncover and examine interpretable features within CLMs. Applying our methodology to the Foundation Models for Materials (FM4M) SMI-TED chemistry foundation model, we extract semantically meaningful latent features and analyse their activation patterns across diverse molecular datasets. Our findings reveal that these models encode a rich landscape of chemical concepts. We identify correlations between specific latent features and distinct domains of chemical knowledge, including structural motifs, physicochemical properties, and pharmacological drug classes. Our approach provides a generalisable framework for uncovering latent knowledge in chemistry-focused AI systems. This work has implications for both foundational understanding and practical deployment; with the potential to accelerate computational chemistry research.

</details>


### [116] [Training LLMs for Honesty via Confessions](https://arxiv.org/abs/2512.08093)
*Manas Joglekar,Jeremy Chen,Gabriel Wu,Jason Yosinski,Jasmine Wang,Boaz Barak,Amelia Glaese*

Main category: cs.LG

TL;DR: 论文提出通过自报告“忏悔”方法让大语言模型诚实地表达自身不足，并训练GPT - 5 - Thinking进行验证，发现该方法有一定效果，“忏悔”还能用于推理时干预。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在报告自身行为和观点时可能不诚实，这种不诚实可能源于强化学习的影响，因此需要一种方法让模型诚实地表达不足。

Method: 提出通过自报告“忏悔”的方法，训练时“忏悔”奖励仅基于诚实性，不影响主答案奖励，激励模型在“忏悔”中保持诚实。

Result: 训练GPT - 5 - Thinking在分布外场景中评估，发现模型在主答案说谎或隐瞒不足时，常能诚实地“忏悔”，且“忏悔”的诚实度随训练略有提高。

Conclusion: 该方法有一定可行性，“忏悔”能实现如监控、拒绝采样和向用户反馈问题等推理时干预。

Abstract: Large language models (LLMs) can be dishonest when reporting on their actions and beliefs -- for example, they may overstate their confidence in factual claims or cover up evidence of covert actions. Such dishonesty may arise due to the effects of reinforcement learning (RL), where challenges with reward shaping can result in a training process that inadvertently incentivizes the model to lie or misrepresent its actions.
  In this work we propose a method for eliciting an honest expression of an LLM's shortcomings via a self-reported *confession*. A confession is an output, provided upon request after a model's original answer, that is meant to serve as a full account of the model's compliance with the letter and spirit of its policies and instructions. The reward assigned to a confession during training is solely based on its honesty, and does not impact positively or negatively the main answer's reward. As long as the "path of least resistance" for maximizing confession reward is to surface misbehavior rather than covering it up, this incentivizes models to be honest in their confessions. Our findings provide some justification this empirical assumption, especially in the case of egregious model misbehavior.
  To demonstrate the viability of our approach, we train GPT-5-Thinking to produce confessions, and we evaluate its honesty in out-of-distribution scenarios measuring hallucination, instruction following, scheming, and reward hacking. We find that when the model lies or omits shortcomings in its "main" answer, it often confesses to these behaviors honestly, and this confession honesty modestly improves with training. Confessions can enable a number of inference-time interventions including monitoring, rejection sampling, and surfacing issues to the user.

</details>


### [117] [Scalable Offline Model-Based RL with Action Chunks](https://arxiv.org/abs/2512.08108)
*Kwanyoung Park,Seohong Park,Youngwoon Lee,Sergey Levine*

Main category: cs.LG

TL;DR: 研究基于模型的强化学习在离线RL中处理复杂长程任务的可行性，提出MAC方法，实验表明其在离线基于模型的RL算法中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 探索基于模型的强化学习（特别是基于模型的值扩展）能否为离线RL中的复杂长程任务提供可扩展方案。

Method: 使用动作块模型减少累积误差，采用拒绝采样防止模型利用分布外动作，提出MAC方法。

Result: 在高达1亿次转换的大规模数据集的高挑战性任务实验中，MAC在离线基于模型的RL算法中表现最佳，尤其在长程任务上。

Conclusion: MAC是处理离线RL中复杂长程任务的有效方法。

Abstract: In this paper, we study whether model-based reinforcement learning (RL), in particular model-based value expansion, can provide a scalable recipe for tackling complex, long-horizon tasks in offline RL. Model-based value expansion fits an on-policy value function using length-n imaginary rollouts generated by the current policy and a learned dynamics model. While larger n reduces bias in value bootstrapping, it amplifies accumulated model errors over long horizons, degrading future predictions. We address this trade-off with an \emph{action-chunk} model that predicts a future state from a sequence of actions (an "action chunk") instead of a single action, which reduces compounding errors. In addition, instead of directly training a policy to maximize rewards, we employ rejection sampling from an expressive behavioral action-chunk policy, which prevents model exploitation from out-of-distribution actions. We call this recipe \textbf{Model-Based RL with Action Chunks (MAC)}. Through experiments on highly challenging tasks with large-scale datasets of up to 100M transitions, we show that MAC achieves the best performance among offline model-based RL algorithms, especially on challenging long-horizon tasks.

</details>


### [118] [Balanced Accuracy: The Right Metric for Evaluating LLM Judges -- Explained through Youden's J statistic](https://arxiv.org/abs/2512.08121)
*Stephane Collot,Colin Fraser,Justin Zhao,William F. Shen,Timon Willi,Ilias Leontiadis*

Main category: cs.LG

TL;DR: 研究指出常用指标在选择大语言模型评估分类器时有缺陷，Youden's J统计量和平衡准确率更优。


<details>
  <summary>Details</summary>
Motivation: 现有用于选择大语言模型评估分类器的常用指标存在对类别不平衡敏感、受正类选择影响等问题，影响评估可信度。

Method: 通过分析论证、实证例子和模拟，对比常用指标与Youden's J统计量、平衡准确率。

Result: Youden's J统计量理论上适合选择比较模型的最佳评判者，平衡准确率是J的等价线性变换，用平衡准确率选择评判者效果更好、更稳健。

Conclusion: 选择大语言模型评估分类器时，使用平衡准确率能实现更好、更稳健的分类器选择。

Abstract: Rigorous evaluation of large language models (LLMs) relies on comparing models by the prevalence of desirable or undesirable behaviors, such as task pass rates or policy violations. These prevalence estimates are produced by a classifier, either an LLM-as-a-judge or human annotators, making the choice of classifier central to trustworthy evaluation. Common metrics used for this choice, such as Accuracy, Precision, and F1, are sensitive to class imbalance and to arbitrary choices of positive class, and can favor judges that distort prevalence estimates. We show that Youden's $J$ statistic is theoretically aligned with choosing the best judge to compare models, and that Balanced Accuracy is an equivalent linear transformation of $J$. Through both analytical arguments and empirical examples and simulations, we demonstrate how selecting judges using Balanced Accuracy leads to better, more robust classifier selection.

</details>


### [119] [Improving the Sensitivity of Backdoor Detectors via Class Subspace Orthogonalization](https://arxiv.org/abs/2512.08129)
*Guangmingmei Yang,David J. Miller,George Kesidis*

Main category: cs.LG

TL;DR: 现有后门检测方法有局限性，提出Class Subspace Orthogonalization (CSO)方法抑制本征特征以优化检测统计量，评估其对抗复杂攻击的效果。


<details>
  <summary>Details</summary>
Motivation: 现有后训练后门检测方法在某些情况下会失效，如部分非目标类易区分或后门特征不明显时。

Method: 提出Class Subspace Orthogonalization (CSO)方法，通过抑制本征特征，在给定类上优化检测统计量，具体是利用少量干净样本构建约束优化问题，使检测统计量与类的本征特征正交。

Result: 未提及具体结果。

Conclusion: 未提及明确结论，但评估了CSO方法对抗混合标签和自适应攻击的效果。

Abstract: Most post-training backdoor detection methods rely on attacked models exhibiting extreme outlier detection statistics for the target class of an attack, compared to non-target classes. However, these approaches may fail: (1) when some (non-target) classes are easily discriminable from all others, in which case they may naturally achieve extreme detection statistics (e.g., decision confidence); and (2) when the backdoor is subtle, i.e., with its features weak relative to intrinsic class-discriminative features. A key observation is that the backdoor target class has contributions to its detection statistic from both the backdoor trigger and from its intrinsic features, whereas non-target classes only have contributions from their intrinsic features. To achieve more sensitive detectors, we thus propose to suppress intrinsic features while optimizing the detection statistic for a given class. For non-target classes, such suppression will drastically reduce the achievable statistic, whereas for the target class the (significant) contribution from the backdoor trigger remains. In practice, we formulate a constrained optimization problem, leveraging a small set of clean examples from a given class, and optimizing the detection statistic while orthogonalizing with respect to the class's intrinsic features. We dub this plug-and-play approach Class Subspace Orthogonalization (CSO) and assess it against challenging mixed-label and adaptive attacks.

</details>


### [120] [Biothreat Benchmark Generation Framework for Evaluating Frontier AI Models I: The Task-Query Architecture](https://arxiv.org/abs/2512.08130)
*Gary Ackerman,Brandon Behlendorf,Zachary Kallenborn,Sheriff Almakki,Doug Clifford,Jenna LaTourette,Hayley Peterson,Noah Sheinbaum,Olivia Shoemaker,Anna Wetzel*

Main category: cs.LG

TL;DR: 本文介绍新型生物威胁基准生成（BBG）框架第一部分，专注细菌生物威胁，构建生物威胁任务查询架构，为评估大语言模型生物风险提供结构。


<details>
  <summary>Details</summary>
Motivation: 量化和减轻前沿人工智能模型对生物安全的风险，开发能评估特定模型生物安全风险的基准。

Method: 构建基于生物威胁类别、元素和任务的层次结构，开发任务对齐查询，形成细菌生物威胁模式。

Result: 构建了细菌生物威胁模式这一生物威胁任务查询架构。

Conclusion: BBG框架包括细菌生物威胁模式，能为评估大语言模型细菌生物风险提供稳健、可复用的结构。

Abstract: Both model developers and policymakers seek to quantify and mitigate the risk of rapidly-evolving frontier artificial intelligence (AI) models, especially large language models (LLMs), to facilitate bioterrorism or access to biological weapons. An important element of such efforts is the development of model benchmarks that can assess the biosecurity risk posed by a particular model. This paper describes the first component of a novel Biothreat Benchmark Generation (BBG) Framework. The BBG approach is designed to help model developers and evaluators reliably measure and assess the biosecurity risk uplift and general harm potential of existing and future AI models, while accounting for key aspects of the threat itself that are often overlooked in other benchmarking efforts, including different actor capability levels, and operational (in addition to purely technical) risk factors. As a pilot, the BBG is first being developed to address bacterial biological threats only. The BBG is built upon a hierarchical structure of biothreat categories, elements and tasks, which then serves as the basis for the development of task-aligned queries. This paper outlines the development of this biothreat task-query architecture, which we have named the Bacterial Biothreat Schema, while future papers will describe follow-on efforts to turn queries into model prompts, as well as how the resulting benchmarks can be implemented for model evaluation. Overall, the BBG Framework, including the Bacterial Biothreat Schema, seeks to offer a robust, re-usable structure for evaluating bacterial biological risks arising from LLMs across multiple levels of aggregation, which captures the full scope of technical and operational requirements for biological adversaries, and which accounts for a wide spectrum of biological adversary capabilities.

</details>


### [121] [Robust Agents in Open-Ended Worlds](https://arxiv.org/abs/2512.08139)
*Mikayel Samvelyan*

Main category: cs.LG

TL;DR: 本文利用开放式和多智能体学习方法训练和评估能泛化到新环境的鲁棒AI智能体，涉及多个领域提升智能体鲁棒性的方法。


<details>
  <summary>Details</summary>
Motivation: 随着AI在各应用中普及，需要能在不断变化的开放式世界中导航和适应的鲁棒AI智能体，要解决其在未见场景的泛化问题。

Method: 引入MiniHack框架创建多样环境；提出Maestro方法生成对抗课程；利用质量多样性方法识别多智能体领域预训练策略漏洞；在大语言模型领域用进化搜索生成对抗性提示。

Result: 在多个领域开展了提升AI智能体鲁棒性的探索和实践。

Conclusion: 为未来AI鲁棒性的发展铺平道路，使智能体适应不断变化的世界并应对意外挑战。

Abstract: The growing prevalence of artificial intelligence (AI) in various applications underscores the need for agents that can successfully navigate and adapt to an ever-changing, open-ended world. A key challenge is ensuring these AI agents are robust, excelling not only in familiar settings observed during training but also effectively generalising to previously unseen and varied scenarios. In this thesis, we harness methodologies from open-endedness and multi-agent learning to train and evaluate robust AI agents capable of generalising to novel environments, out-of-distribution inputs, and interactions with other co-player agents. We begin by introducing MiniHack, a sandbox framework for creating diverse environments through procedural content generation. Based on the game of NetHack, MiniHack enables the construction of new tasks for reinforcement learning (RL) agents with a focus on generalisation. We then present Maestro, a novel approach for generating adversarial curricula that progressively enhance the robustness and generality of RL agents in two-player zero-sum games. We further probe robustness in multi-agent domains, utilising quality-diversity methods to systematically identify vulnerabilities in state-of-the-art, pre-trained RL policies within the complex video game football domain, characterised by intertwined cooperative and competitive dynamics. Finally, we extend our exploration of robustness to the domain of LLMs. Here, our focus is on diagnosing and enhancing the robustness of LLMs against adversarial prompts, employing evolutionary search to generate a diverse range of effective inputs that aim to elicit undesirable outputs from an LLM. This work collectively paves the way for future advancements in AI robustness, enabling the development of agents that not only adapt to an ever-evolving world but also thrive in the face of unforeseen challenges and interactions.

</details>


### [122] [PolyLingua: Margin-based Inter-class Transformer for Robust Cross-domain Language Detection](https://arxiv.org/abs/2512.08143)
*Ali Lotfi Rezaabad,Bikram Khanal,Shashwat Chaurasia,Lu Zeng,Dezhi Hong,Hossein Beshashati,Thomas Butler,Megan Ganji*

Main category: cs.LG

TL;DR: 介绍轻量级语言识别模型PolyLingua，在多数据集表现优，适用于资源和延迟受限场景。


<details>
  <summary>Details</summary>
Motivation: 现有语言识别工具在关键场景表现不佳，开源工具精度低，大模型成本高，需高精度低成本模型。

Method: 引入基于Transformer的PolyLingua模型，采用两级对比学习框架结合实例级分离和类级对齐与自适应边界。

Result: 在两个具有挑战性的数据集上，PolyLingua分别达到99.25%和98.15%的F1分数，超越Sonnet 3.5且参数少10倍。

Conclusion: PolyLingua适合计算和延迟受限的环境。

Abstract: Language identification is a crucial first step in multilingual systems such as chatbots and virtual assistants, enabling linguistically and culturally accurate user experiences. Errors at this stage can cascade into downstream failures, setting a high bar for accuracy. Yet, existing language identification tools struggle with key cases -- such as music requests where the song title and user language differ. Open-source tools like LangDetect, FastText are fast but less accurate, while large language models, though effective, are often too costly for low-latency or low-resource settings. We introduce PolyLingua, a lightweight Transformer-based model for in-domain language detection and fine-grained language classification. It employs a two-level contrastive learning framework combining instance-level separation and class-level alignment with adaptive margins, yielding compact and well-separated embeddings even for closely related languages. Evaluated on two challenging datasets -- Amazon Massive (multilingual digital assistant utterances) and a Song dataset (music requests with frequent code-switching) -- PolyLingua achieves 99.25% F1 and 98.15% F1, respectively, surpassing Sonnet 3.5 while using 10x fewer parameters, making it ideal for compute- and latency-constrained environments.

</details>


### [123] [TreeGRPO: Tree-Advantage GRPO for Online RL Post-Training of Diffusion Models](https://arxiv.org/abs/2512.08153)
*Zheng Ding,Weirui Ye*

Main category: cs.LG

TL;DR: 介绍TreeGRPO框架提升强化学习后训练效率，实验显示它能使训练快2.4倍且表现优异。


<details>
  <summary>Details</summary>
Motivation: 强化学习后训练能使生成模型符合人类偏好，但计算成本高阻碍广泛应用。

Method: 将去噪过程重构为搜索树，从共享初始噪声样本分支生成候选轨迹并复用公共前缀。

Result: 在扩散和基于流的模型实验中，TreeGRPO训练快2.4倍，在效率 - 奖励权衡空间建立了更好的帕累托前沿，在多个基准和奖励模型上表现优于GRPO基线。

Conclusion: TreeGRPO为基于强化学习的视觉生成模型对齐提供了可扩展和有效的途径。

Abstract: Reinforcement learning (RL) post-training is crucial for aligning generative models with human preferences, but its prohibitive computational cost remains a major barrier to widespread adoption. We introduce \textbf{TreeGRPO}, a novel RL framework that dramatically improves training efficiency by recasting the denoising process as a search tree. From shared initial noise samples, TreeGRPO strategically branches to generate multiple candidate trajectories while efficiently reusing their common prefixes. This tree-structured approach delivers three key advantages: (1) \emph{High sample efficiency}, achieving better performance under same training samples (2) \emph{Fine-grained credit assignment} via reward backpropagation that computes step-specific advantages, overcoming the uniform credit assignment limitation of trajectory-based methods, and (3) \emph{Amortized computation} where multi-child branching enables multiple policy updates per forward pass. Extensive experiments on both diffusion and flow-based models demonstrate that TreeGRPO achieves \textbf{2.4$\times$ faster training} while establishing a superior Pareto frontier in the efficiency-reward trade-off space. Our method consistently outperforms GRPO baselines across multiple benchmarks and reward models, providing a scalable and effective pathway for RL-based visual generative model alignment. The project website is available at treegrpo.github.io.

</details>


### [124] [LayerPipe2: Multistage Pipelining and Weight Recompute via Improved Exponential Moving Average for Training Neural Networks](https://arxiv.org/abs/2512.08160)
*Nanda K. Unnikrishnan,Keshab K. Parhi*

Main category: cs.LG

TL;DR: 本文提出LayerPipe2，推导LayerPipe，分析梯度延迟，解决存储瓶颈，实现可扩展流水线训练。


<details>
  <summary>Details</summary>
Motivation: 前期工作LayerPipe缺乏对各层引入多少梯度延迟以实现所需流水线程度的原理性理解，本文旨在填补这一空白。

Method: 通过可变延迟梯度自适应和重定时形式化推导LayerPipe，识别合法插入延迟的位置，开发流水线感知移动平均法解决存储瓶颈。

Result: 得到原理性框架，可构建LayerPipe架构、预测延迟需求并减轻存储负担。

Conclusion: 该框架使可扩展的流水线训练成为可能，并能控制通信和计算的权衡。

Abstract: In our prior work, LayerPipe, we had introduced an approach to accelerate training of convolutional, fully connected, and spiking neural networks by overlapping forward and backward computation. However, despite empirical success, a principled understanding of how much gradient delay needs to be introduced at each layer to achieve desired level of pipelining was not addressed. This paper, LayerPipe2, fills that gap by formally deriving LayerPipe using variable delayed gradient adaptation and retiming. We identify where delays may be legally inserted and show that the required amount of delay follows directly from the network structure where inner layers require fewer delays and outer layers require longer delays. When pipelining is applied at every layer, the amount of delay depends only on the number of remaining downstream stages. When layers are pipelined in groups, all layers in the group share the same assignment of delays. These insights not only explain previously observed scheduling patterns but also expose an often overlooked challenge that pipelining implicitly requires storage of historical weights. We overcome this storage bottleneck by developing a pipeline--aware moving average that reconstructs the required past states rather than storing them explicitly. This reduces memory cost without sacrificing the accuracy guarantees that makes pipelined learning viable. The result is a principled framework that illustrates how to construct LayerPipe architectures, predicts their delay requirements, and mitigates their storage burden, thereby enabling scalable pipelined training with controlled communication computation tradeoffs.

</details>


### [125] [MobileFineTuner: A Unified End-to-End Framework for Fine-Tuning LLMs on Mobile Phones](https://arxiv.org/abs/2512.08211)
*Jiaxiang Geng,Lunyu Zhao,Yiyi Lu,Bing Luo*

Main category: cs.LG

TL;DR: 提出开源框架MobileFineTuner在商用手机上实现端到端大语言模型微调，并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 高质量公共数据临近枯竭，设备端微调可利用私有数据并保护隐私，但现有方法多基于模拟或依赖其他设备，缺乏在手机上微调大语言模型的开源框架。

Method: 提出统一开源框架MobileFineTuner，支持全参数和参数高效微调，引入参数分片、梯度累积和能量感知计算调度等系统级优化。

Result: 在真实手机上对GPT - 2、Gemma 3和Qwen 2.5进行微调，广泛实验和消融研究验证了优化的有效性。

Conclusion: MobileFineTuner为设备端大语言模型训练的未来研究提供了可行基础。

Abstract: Mobile phones are the most ubiquitous end devices, generating vast amounts of human-authored data and serving as the primary platform for end-side applications. As high-quality public data for large language models (LLMs) approaches exhaustion, on-device fine-tuning provides an opportunity to leverage private user data while preserving privacy. However, existing approaches are predominantly simulation-based or rely on IoT devices and PCs, leaving commodity mobile phones largely unexplored. A key gap is the absence of an open-source framework that enables practical LLM fine-tuning on mobile phones. We present MobileFineTuner, a unified open-source framework that enables end-to-end LLM fine-tuning directly on commodity mobile phones. MobileFineTuner is designed for efficiency, scalability, and usability, supporting full-parameters fine-tuning (Full-FT) and parameter-efficient fine-tuning (PEFT). To address the memory and energy limitations inherent to mobile phones, we introduce system-level optimizations including parameter sharding, gradient accumulation, and energy-aware computation scheduling. We demonstrate the practicality of MobileFineTuner by fine-tuning GPT-2, Gemma 3, and Qwen 2.5 on real mobile phones. Extensive experiments and ablation studies validate the effectiveness of the proposed optimizations and establish MobileFineTuner as a viable foundation for future research on on-device LLM training.

</details>


### [126] [Correction of Decoupled Weight Decay](https://arxiv.org/abs/2512.08217)
*Jason Chuan-Chih Chou*

Main category: cs.LG

TL;DR: 本文反驳了以正交性论据设定解耦权重衰减为与学习率平方成正比的观点，推导得出此设定能稳定权重范数，验证了Scion优化器中动量相关有效学习率对批量总更新贡献的表征，说明该设定能更好控制训练动态、提高性能。


<details>
  <summary>Details</summary>
Motivation: 过去解耦权重衰减一直被设为与学习率成正比，近期有观点基于正交性认为应设为与学习率平方成正比；作者对此假设存疑，旨在深入探究解耦权重衰减的合适设置。

Method: 推导验证解耦权重衰减与学习率平方成正比能基于更新与权重在稳态下独立的假设稳定权重范数；推导并实证验证Scion优化器中动量相关有效学习率对批量总更新贡献的表征。

Result: 解耦权重衰减与学习率平方成正比能稳定权重和梯度范数，能更好控制训练动态，Scion优化器中动量相关有效学习率最优值可迁移。

Conclusion: 解耦权重衰减与学习率平方成正比可稳定权重和梯度范数，有助于更好控制训练动态和提高模型性能。

Abstract: Decoupled weight decay, solely responsible for the performance advantage of AdamW over Adam, has long been set to proportional to learning rate $γ$ without questioning. Some researchers have recently challenged such assumption and argued that decoupled weight decay should be set $\propto γ^2$ instead based on orthogonality arguments at steady state. To the contrary, we find that eliminating the contribution of the perpendicular component of the update to the weight norm leads to little change to the training dynamics. Instead, we derive that decoupled weight decay $\propto γ^2$ results in stable weight norm based on the simple assumption that updates become independent of the weights at steady state, regardless of the nature of the optimizer. Based on the same assumption, we derive and empirically verify that the Total Update Contribution (TUC) of a minibatch under the Scion optimizer is better characterized by the momentum-dependent effective learning rate whose optimal value transfers and we show that decoupled weight decay $\propto γ^2$ leads to stable weight and gradient norms and allows us to better control the training dynamics and improve the model performance.

</details>


### [127] [PR-CapsNet: Pseudo-Riemannian Capsule Network with Adaptive Curvature Routing for Graph Learning](https://arxiv.org/abs/2512.08218)
*Ye Qin,Jingchao Wang,Yang Shi,Haiying Huang,Junxu Li,Weijian Liu,Tinghui Chen,Jinghui Qin*

Main category: cs.LG

TL;DR: 提出伪黎曼胶囊网络（PR - CapsNet）用于图表示学习，在节点和图分类基准测试中表现优于SOTA模型。


<details>
  <summary>Details</summary>
Motivation: 现有胶囊网络在固定曲率空间中对复杂几何图建模不佳，非欧几里得伪黎曼流形可提供归纳偏置，但如何用于改进胶囊网络仍待探索。

Method: 将欧氏胶囊路由扩展到测地不连通的伪黎曼流形，提出PR - CapsNet，通过自适应伪黎曼切空间路由增强，利用自适应曲率路由融合不同曲率空间特征，并开发几何性质保留的分类器。

Result: 在节点和图分类基准测试中，PR - CapsNet表现优于现有SOTA模型。

Conclusion: PR - CapsNet对复杂图结构有强大的表示能力。

Abstract: Capsule Networks (CapsNets) show exceptional graph representation capacity via dynamic routing and vectorized hierarchical representations, but they model the complex geometries of real\-world graphs poorly by fixed\-curvature space due to the inherent geodesical disconnectedness issues, leading to suboptimal performance. Recent works find that non\-Euclidean pseudo\-Riemannian manifolds provide specific inductive biases for embedding graph data, but how to leverage them to improve CapsNets is still underexplored. Here, we extend the Euclidean capsule routing into geodesically disconnected pseudo\-Riemannian manifolds and derive a Pseudo\-Riemannian Capsule Network (PR\-CapsNet), which models data in pseudo\-Riemannian manifolds of adaptive curvature, for graph representation learning. Specifically, PR\-CapsNet enhances the CapsNet with Adaptive Pseudo\-Riemannian Tangent Space Routing by utilizing pseudo\-Riemannian geometry. Unlike single\-curvature or subspace\-partitioning methods, PR\-CapsNet concurrently models hierarchical and cluster or cyclic graph structures via its versatile pseudo\-Riemannian metric. It first deploys Pseudo\-Riemannian Tangent Space Routing to decompose capsule states into spherical\-temporal and Euclidean\-spatial subspaces with diffeomorphic transformations. Then, an Adaptive Curvature Routing is developed to adaptively fuse features from different curvature spaces for complex graphs via a learnable curvature tensor with geometric attention from local manifold properties. Finally, a geometric properties\-preserved Pseudo\-Riemannian Capsule Classifier is developed to project capsule embeddings to tangent spaces and use curvature\-weighted softmax for classification. Extensive experiments on node and graph classification benchmarks show PR\-CapsNet outperforms SOTA models, validating PR\-CapsNet's strong representation power for complex graph structures.

</details>


### [128] [Persistent Topological Structures and Cohomological Flows as a Mathematical Framework for Brain-Inspired Representation Learning](https://arxiv.org/abs/2512.08241)
*Preksha Girish,Rachana Mysore,Mahanthesha U,Shrey Kumar,Shipra Prashant*

Main category: cs.LG

TL;DR: 本文提出基于拓扑结构和上同调流的脑启发表征学习数学框架，模型性能优越。


<details>
  <summary>Details</summary>
Motivation: 构建严谨数学框架用于脑启发表征学习，捕获大脑状态不变量。

Method: 将神经计算重新表述为动态单纯复形上余链映射的演化，结合代数拓扑与微分几何构建上同调算子，用持久同调等分析数据。

Result: 模型在流形一致性和抗噪性上优于图神经网络和基于流形的深度架构。

Conclusion: 为拓扑驱动的表征学习建立了连贯数学基础。

Abstract: This paper presents a mathematically rigorous framework for brain-inspired representation learning founded on the interplay between persistent topological structures and cohomological flows. Neural computation is reformulated as the evolution of cochain maps over dynamic simplicial complexes, enabling representations that capture invariants across temporal, spatial, and functional brain states. The proposed architecture integrates algebraic topology with differential geometry to construct cohomological operators that generalize gradient-based learning within a homological landscape. Synthetic data with controlled topological signatures and real neural datasets are jointly analyzed using persistent homology, sheaf cohomology, and spectral Laplacians to quantify stability, continuity, and structural preservation. Empirical results demonstrate that the model achieves superior manifold consistency and noise resilience compared to graph neural and manifold-based deep architectures, establishing a coherent mathematical foundation for topology-driven representation learning.

</details>


### [129] [SPROCKET: Extending ROCKET to Distance-Based Time-Series Transformations With Prototypes](https://arxiv.org/abs/2512.08246)
*Nicholas Harner*

Main category: cs.LG

TL;DR: 介绍基于原型的SPROCKET特征工程策略，其在多数UCR和UEA数据集上表现与现有卷积算法相当，MR - HY - SP集成平均准确率超之前最佳卷积集成。


<details>
  <summary>Details</summary>
Motivation: 改进经典时间序列分类算法中基于特征工程的策略。

Method: 引入基于原型的新特征工程策略SPROCKET，并构建MR - HY - SP集成。

Result: SPROCKET在多数UCR和UEA数据集上表现与现有卷积算法相当，MR - HY - SP集成平均准确率超之前最佳卷积集成HYDRA - MR。

Conclusion: 基于原型的特征变换能提高时间序列分类的准确性和鲁棒性。

Abstract: Classical Time Series Classification algorithms are dominated by feature engineering strategies. One of the most prominent of these transforms is ROCKET, which achieves strong performance through random kernel features. We introduce SPROCKET (Selected Prototype Random Convolutional Kernel Transform), which implements a new feature engineering strategy based on prototypes. On a majority of the UCR and UEA Time Series Classification archives, SPROCKET achieves performance comparable to existing convolutional algorithms and the new MR-HY-SP ( MultiROCKET-HYDRA-SPROCKET) ensemble's average accuracy ranking exceeds HYDRA-MR, the previous best convolutional ensemble's performance. These experimental results demonstrate that prototype-based feature transformation can enhance both accuracy and robustness in time series classification.

</details>


### [130] [Wavelet-Accelerated Physics-Informed Quantum Neural Network for Multiscale Partial Differential Equations](https://arxiv.org/abs/2512.08256)
*Deepak Gupta,Himanshu Pandey,Ratikanta Behera*

Main category: cs.LG

TL;DR: 提出基于小波的物理信息量子神经网络框架解决多尺度偏微分方程，减少计算复杂度，精度高、收敛快。


<details>
  <summary>Details</summary>
Motivation: 传统物理信息神经网络及其量子版本在解决多尺度特征时有挑战，自动求导构建损失函数计算开销大、训练时间长。

Method: 开发无需自动求导的小波加速物理信息量子神经网络，将小波多分辨率特性融入量子神经网络架构。

Result: 比经典基于小波的PINNs所需可训练参数少5%以上，精度更高、收敛更快；比现有量子PINNs快3 - 5倍。

Conclusion: 该方法在解决多尺度和振荡问题上有潜力。

Abstract: This work proposes a wavelet-based physics-informed quantum neural network framework to efficiently address multiscale partial differential equations that involve sharp gradients, stiffness, rapid local variations, and highly oscillatory behavior. Traditional physics-informed neural networks (PINNs) have demonstrated substantial potential in solving differential equations, and their quantum counterparts, quantum-PINNs, exhibit enhanced representational capacity with fewer trainable parameters. However, both approaches face notable challenges in accurately solving multiscale features. Furthermore, their reliance on automatic differentiation for constructing loss functions introduces considerable computational overhead, resulting in longer training times. To overcome these challenges, we developed a wavelet-accelerated physics-informed quantum neural network that eliminates the need for automatic differentiation, significantly reducing computational complexity. The proposed framework incorporates the multiresolution property of wavelets within the quantum neural network architecture, thereby enhancing the network's ability to effectively capture both local and global features of multiscale problems. Numerical experiments demonstrate that our proposed method achieves superior accuracy while requiring less than five percent of the trainable parameters compared to classical wavelet-based PINNs, resulting in faster convergence. Moreover, it offers a speedup of three to five times compared to existing quantum PINNs, highlighting the potential of the proposed approach for efficiently solving challenging multiscale and oscillatory problems.

</details>


### [131] [Geometric-Stochastic Multimodal Deep Learning for Predictive Modeling of SUDEP and Stroke Vulnerability](https://arxiv.org/abs/2512.08257)
*Preksha Girish,Rachana Mysore,Mahanthesha U,Shrey Kumar,Misbah Fatimah Annigeri,Tanish Jain*

Main category: cs.LG

TL;DR: 提出统一几何 - 随机多模态深度学习框架建模SUDEP和中风脆弱性，实验有更好预测精度和可解释生物标志物。


<details>
  <summary>Details</summary>
Motivation: SUDEP和急性缺血性中风是危及生命的疾病，涉及皮层、脑干和自主神经系统复杂相互作用，需建模研究。

Method: 结合黎曼流形嵌入、李群不变特征表示、分数随机动力学、哈密顿能量流建模和跨模态注意力机制，用分数流行病扩散对中风传播建模。

Result: 在MULTI - CLARID数据集实验中，有更好的预测精度，得到基于流形曲率、分数记忆指数、注意力熵和扩散中心性的可解释生物标志物。

Conclusion: 框架为神经自主疾病的早期检测、风险分层和可解释多模态建模提供数学原理基础。

Abstract: Sudden Unexpected Death in Epilepsy (SUDEP) and acute ischemic stroke are life-threatening conditions involving complex interactions across cortical, brainstem, and autonomic systems. We present a unified geometric-stochastic multimodal deep learning framework that integrates EEG, ECG, respiration, SpO2, EMG, and fMRI signals to model SUDEP and stroke vulnerability. The approach combines Riemannian manifold embeddings, Lie-group invariant feature representations, fractional stochastic dynamics, Hamiltonian energy-flow modeling, and cross-modal attention mechanisms. Stroke propagation is modeled using fractional epidemic diffusion over structural brain graphs. Experiments on the MULTI-CLARID dataset demonstrate improved predictive accuracy and interpretable biomarkers derived from manifold curvature, fractional memory indices, attention entropy, and diffusion centrality. The proposed framework provides a mathematically principled foundation for early detection, risk stratification, and interpretable multimodal modeling in neural-autonomic disorders.

</details>


### [132] [Mathematical Foundations of Neural Tangents and Infinite-Width Networks](https://arxiv.org/abs/2512.08264)
*Rachana Mysore,Preksha Girish,Kavitha Jayaram,Shrey Kumar,Preksha Girish,Shravan Sanjeev Bagal,Kavitha Jayaram,Shreya Aravind Shastry*

Main category: cs.LG

TL;DR: 研究无限宽度下神经网络数学基础，提出NTK - ECRN架构，有理论推导和实证结果，提供连接理论与实践架构的框架。


<details>
  <summary>Details</summary>
Motivation: 探究无限宽度下神经网络的数学基础，实现对训练中核演化的严格分析。

Method: 提出NTK - ECRN架构，融合傅里叶特征嵌入、带层缩放的残差连接和随机深度。

Result: 在合成和基准数据集上验证了预测的核行为，展示了更好的训练稳定性和泛化能力。

Conclusion: 为连接无限宽度理论和实际深度学习架构提供了综合框架。

Abstract: We investigate the mathematical foundations of neural networks in the infinite-width regime through the Neural Tangent Kernel (NTK). We propose the NTK-Eigenvalue-Controlled Residual Network (NTK-ECRN), an architecture integrating Fourier feature embeddings, residual connections with layerwise scaling, and stochastic depth to enable rigorous analysis of kernel evolution during training. Our theoretical contributions include deriving bounds on NTK dynamics, characterizing eigenvalue evolution, and linking spectral properties to generalization and optimization stability. Empirical results on synthetic and benchmark datasets validate the predicted kernel behavior and demonstrate improved training stability and generalization. This work provides a comprehensive framework bridging infinite-width theory and practical deep-learning architectures.

</details>


### [133] [SOFA-FL: Self-Organizing Hierarchical Federated Learning with Adaptive Clustered Data Sharing](https://arxiv.org/abs/2512.08267)
*Yi Ni,Xinkun Wang,Han Zhang*

Main category: cs.LG

TL;DR: 本文针对联邦学习在动态环境中的数据异构和固定拓扑问题，提出SOFA - FL框架，有三种核心机制，能有效捕捉客户间动态关系并增强个性化能力。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习在演变环境中面临的数据异构和固定网络拓扑刚性问题。

Method: 提出SOFA - FL框架，包括动态多分支凝聚聚类（DMAC）构建初始层次结构，自组织层次自适应传播与进化（SHAPE）动态重构拓扑，自适应聚类数据共享减轻数据异构性。

Result: 能有效捕捉客户端之间的动态关系，且增强个性化能力。

Conclusion: 集成核心机制的SOFA - FL不依赖预定集群结构就能有效解决联邦学习的相关问题，提升性能。

Abstract: Federated Learning (FL) faces significant challenges in evolving environments, particularly regarding data heterogeneity and the rigidity of fixed network topologies. To address these issues, this paper proposes \textbf{SOFA-FL} (Self-Organizing Hierarchical Federated Learning with Adaptive Clustered Data Sharing), a novel framework that enables hierarchical federated systems to self-organize and adapt over time.
  The framework is built upon three core mechanisms: (1) \textbf{Dynamic Multi-branch Agglomerative Clustering (DMAC)}, which constructs an initial efficient hierarchical structure; (2) \textbf{Self-organizing Hierarchical Adaptive Propagation and Evolution (SHAPE)}, which allows the system to dynamically restructure its topology through atomic operations -- grafting, pruning, consolidation, and purification -- to adapt to changes in data distribution; and (3) \textbf{Adaptive Clustered Data Sharing}, which mitigates data heterogeneity by enabling controlled partial data exchange between clients and cluster nodes.
  By integrating these mechanisms, SOFA-FL effectively captures dynamic relationships among clients and enhances personalization capabilities without relying on predetermined cluster structures.

</details>


### [134] [Jacobian Aligned Random Forests](https://arxiv.org/abs/2512.08306)
*Sarwesh Rauniyar*

Main category: cs.LG

TL;DR: 提出JARF方法，通过监督预条件处理改进轴对齐森林，在表格分类和回归基准上效果好。


<details>
  <summary>Details</summary>
Motivation: 轴对齐决策树在处理旋转或特征交互依赖的决策边界数据集时表现不佳，斜森林有计算成本和实现复杂度问题。

Method: 先拟合轴对齐森林估计概率或输出，计算预测关于特征的有限差分梯度，聚合为期望雅可比外积，作为全局线性预条件器对特征空间旋转，再用标准轴对齐森林处理转换后数据。

Result: 预条件处理持续改进轴对齐森林，常匹配或超越斜森林基线，同时改善训练时间。

Conclusion: 监督预条件处理能恢复斜森林的大部分准确性，同时保留轴对齐树的简单性和鲁棒性。

Abstract: Axis-aligned decision trees are fast and stable but struggle on datasets with rotated or interaction-dependent decision boundaries, where informative splits require linear combinations of features rather than single-feature thresholds. Oblique forests address this with per-node hyperplane splits, but at added computational cost and implementation complexity. We propose a simple alternative: JARF, Jacobian-Aligned Random Forests. Concretely, we first fit an axis-aligned forest to estimate class probabilities or regression outputs, compute finite-difference gradients of these predictions with respect to each feature, aggregate them into an expected Jacobian outer product that generalizes the expected gradient outer product (EGOP), and use it as a single global linear preconditioner for all inputs. This supervised preconditioner applies a single global rotation of the feature space, then hands the transformed data back to a standard axis-aligned forest, preserving off-the-shelf training pipelines while capturing oblique boundaries and feature interactions that would otherwise require many axis-aligned splits to approximate. The same construction applies to any model that provides gradients, though we focus on random forests and gradient-boosted trees in this work. On tabular classification and regression benchmarks, this preconditioning consistently improves axis-aligned forests and often matches or surpasses oblique baselines while improving training time. Our experimental results and theoretical analysis together indicate that supervised preconditioning can recover much of the accuracy of oblique forests while retaining the simplicity and robustness of axis-aligned trees.

</details>


### [135] [Minimizing Layerwise Activation Norm Improves Generalization in Federated Learning](https://arxiv.org/abs/2512.08314)
*M Yashwanth,Gaurav Kumar Nayak,Harsh Rangwani,Arya Singh,R. Venkatesh Babu,Anirban Chakraborty*

Main category: cs.LG

TL;DR: 本文针对联邦学习模型收敛到尖锐最小值影响泛化性问题，引入平坦性约束优化问题，提出MAN正则化技术，应用到现有联邦学习技术获显著提升。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中聚合的全局模型可能收敛到尖锐最小值，影响模型泛化性。

Method: 引入对训练损失计算的黑塞矩阵最大特征值施加平坦性约束的优化问题，利用客户端损失函数重新构建问题，提出计算高效的“MAN”正则化技术，最小化客户端模型各层激活值的范数。

Result: 将提出的平坦性约束优化应用到现有联邦学习技术中，取得了显著改善。

Conclusion: 所提方法能确保收敛到平坦最小值，建立了新的最优水平。

Abstract: Federated Learning (FL) is an emerging machine learning framework that enables multiple clients (coordinated by a server) to collaboratively train a global model by aggregating the locally trained models without sharing any client's training data. It has been observed in recent works that learning in a federated manner may lead the aggregated global model to converge to a 'sharp minimum' thereby adversely affecting the generalizability of this FL-trained model. Therefore, in this work, we aim to improve the generalization performance of models trained in a federated setup by introducing a 'flatness' constrained FL optimization problem. This flatness constraint is imposed on the top eigenvalue of the Hessian computed from the training loss. As each client trains a model on its local data, we further re-formulate this complex problem utilizing the client loss functions and propose a new computationally efficient regularization technique, dubbed 'MAN,' which Minimizes Activation's Norm of each layer on client-side models. We also theoretically show that minimizing the activation norm reduces the top eigenvalue of the layer-wise Hessian of the client's loss, which in turn decreases the overall Hessian's top eigenvalue, ensuring convergence to a flat minimum. We apply our proposed flatness-constrained optimization to the existing FL techniques and obtain significant improvements, thereby establishing new state-of-the-art.

</details>


### [136] [Fully Decentralized Certified Unlearning](https://arxiv.org/abs/2512.08443)
*Hithem Lamri,Michail Maniatakos*

Main category: cs.LG

TL;DR: 研究去中心化网络中经过认证的机器学习模型遗忘问题，提出RR - DU方法，有理论保证且实验效果好。


<details>
  <summary>Details</summary>
Motivation: 现有认证遗忘研究集中在集中式和服务器协调的联邦设置，去中心化设置研究不足。

Method: 提出RR - DU随机游走程序，结合子采样高斯噪声和投影到原模型信任区域。

Result: 在凸和非凸情况下有收敛和驻点保证；通过子采样高斯Rényi DP获得网络遗忘证书；有删除容量边界；实验中RR - DU比基线准确率高且能将遗忘准确率降至随机猜测水平。

Conclusion: RR - DU方法在去中心化网络认证遗忘问题上有效，在隐私 - 效用权衡中有优势。

Abstract: Machine unlearning (MU) seeks to remove the influence of specified data from a trained model in response to privacy requests or data poisoning. While certified unlearning has been analyzed in centralized and server-orchestrated federated settings (via guarantees analogous to differential privacy, DP), the decentralized setting -- where peers communicate without a coordinator remains underexplored. We study certified unlearning in decentralized networks with fixed topologies and propose RR-DU, a random-walk procedure that performs one projected gradient ascent step on the forget set at the unlearning client and a geometrically distributed number of projected descent steps on the retained data elsewhere, combined with subsampled Gaussian noise and projection onto a trust region around the original model. We provide (i) convergence guarantees in the convex case and stationarity guarantees in the nonconvex case, (ii) $(\varepsilon,δ)$ network-unlearning certificates on client views via subsampled Gaussian Rényi DP (RDP) with segment-level subsampling, and (iii) deletion-capacity bounds that scale with the forget-to-local data ratio and quantify the effect of decentralization (network mixing and randomized subsampling) on the privacy-utility trade-off. Empirically, on image benchmarks (MNIST, CIFAR-10), RR-DU matches a given $(\varepsilon,δ)$ while achieving higher test accuracy than decentralized DP baselines and reducing forget accuracy to random guessing ($\approx 10\%$).

</details>


### [137] [Biothreat Benchmark Generation Framework for Evaluating Frontier AI Models II: Benchmark Generation Process](https://arxiv.org/abs/2512.08451)
*Gary Ackerman,Zachary Kallenborn,Anna Wetzel,Hayley Peterson,Jenna LaTourette,Olivia Shoemaker,Brandon Behlendorf,Sheriff Almakki,Doug Clifford,Noah Sheinbaum*

Main category: cs.LG

TL;DR: 文章介绍新型生物威胁基准生成框架中的细菌生物威胁基准数据集生成。


<details>
  <summary>Details</summary>
Motivation: 前沿人工智能模型可能促进生物恐怖主义，模型开发者和政策制定者需量化并降低风险，因此要开发评估生物安全风险的模型基准。

Method: 通过基于网络的提示生成、红队攻击以及挖掘现有基准语料库三种方式生成超7000个潜在基准，经去重、提升诊断性评估和质量控制措施得出1010个最终基准。

Result: 得到1010个具有提升诊断性、与生物安全威胁直接相关且符合生物安全架构的最终基准。

Conclusion: 所生成的基准可用于评估模型生物安全风险并进行细致分析。

Abstract: The potential for rapidly-evolving frontier artificial intelligence (AI) models, especially large language models (LLMs), to facilitate bioterrorism or access to biological weapons has generated significant policy, academic, and public concern. Both model developers and policymakers seek to quantify and mitigate any risk, with an important element of such efforts being the development of model benchmarks that can assess the biosecurity risk posed by a particular model. This paper, the second in a series of three, describes the second component of a novel Biothreat Benchmark Generation (BBG) framework: the generation of the Bacterial Biothreat Benchmark (B3) dataset. The development process involved three complementary approaches: 1) web-based prompt generation, 2) red teaming, and 3) mining existing benchmark corpora, to generate over 7,000 potential benchmarks linked to the Task-Query Architecture that was developed during the first component of the project. A process of de-duplication, followed by an assessment of uplift diagnosticity, and general quality control measures, reduced the candidates to a set of 1,010 final benchmarks. This procedure ensured that these benchmarks are a) diagnostic in terms of providing uplift; b) directly relevant to biosecurity threats; and c) are aligned with a larger biosecurity architecture permitting nuanced analysis at different levels of analysis.

</details>


### [138] [Biothreat Benchmark Generation Framework for Evaluating Frontier AI Models III: Implementing the Bacterial Biothreat Benchmark (B3) Dataset](https://arxiv.org/abs/2512.08459)
*Gary Ackerman,Theodore Wilson,Zachary Kallenborn,Olivia Shoemaker,Anna Wetzel,Hayley Peterson,Abigail Danfora,Jenna LaTourette,Brandon Behlendorf,Douglas Clifford*

Main category: cs.LG

TL;DR: 本文讨论细菌生物威胁基准（B3）数据集的试点实施，表明其可用于快速评估大语言模型的生物安全风险。


<details>
  <summary>Details</summary>
Motivation: 快速发展的前沿人工智能模型带来生物恐怖主义和获取生物武器风险，模型开发者和政策制定者需量化和缓解风险，需要开发评估生物安全风险的模型基准。

Method: 通过样本前沿人工智能模型运行基准，对模型响应进行人工评估，并从多个维度对结果进行应用风险分析。

Result: B3数据集为快速评估大语言模型生物安全风险提供了可行、细致的方法。

Conclusion: B3数据集可评估生物安全风险，识别风险来源并为缓解重点领域提供指导。

Abstract: The potential for rapidly-evolving frontier artificial intelligence (AI) models, especially large language models (LLMs), to facilitate bioterrorism or access to biological weapons has generated significant policy, academic, and public concern. Both model developers and policymakers seek to quantify and mitigate any risk, with an important element of such efforts being the development of model benchmarks that can assess the biosecurity risk posed by a particular model. This paper discusses the pilot implementation of the Bacterial Biothreat Benchmark (B3) dataset. It is the third in a series of three papers describing an overall Biothreat Benchmark Generation (BBG) framework, with previous papers detailing the development of the B3 dataset. The pilot involved running the benchmarks through a sample frontier AI model, followed by human evaluation of model responses, and an applied risk analysis of the results along several dimensions. Overall, the pilot demonstrated that the B3 dataset offers a viable, nuanced method for rapidly assessing the biosecurity risk posed by a LLM, identifying the key sources of that risk and providing guidance for priority areas of mitigation priority.

</details>


### [139] [Transformers for Multimodal Brain State Decoding: Integrating Functional Magnetic Resonance Imaging Data and Medical Metadata](https://arxiv.org/abs/2512.08462)
*Danial Jafarzadeh Jazi,Maryam Hajiesmaeili*

Main category: cs.LG

TL;DR: 本文提出整合基于Transformer架构与多模态输入（fMRI数据和DICOM元数据）的框架用于解码大脑状态，提升模型性能，还讨论了局限与未来方向。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习和深度学习方法未能利用DICOM元数据的上下文信息，需新方法解码fMRI数据中的大脑状态。

Method: 提出整合基于Transformer架构与多模态输入（fMRI数据和DICOM元数据）的框架，运用注意力机制。

Result: 该框架可捕获复杂时空模式和上下文关系，增强模型准确性、可解释性和鲁棒性。

Conclusion: 此框架在临床诊断、认知神经科学和个性化医疗等领域有应用潜力，同时指出局限并探讨未来优化方向。

Abstract: Decoding brain states from functional magnetic resonance imaging (fMRI) data is vital for advancing neuroscience and clinical applications. While traditional machine learning and deep learning approaches have made strides in leveraging the high-dimensional and complex nature of fMRI data, they often fail to utilize the contextual richness provided by Digital Imaging and Communications in Medicine (DICOM) metadata. This paper presents a novel framework integrating transformer-based architectures with multimodal inputs, including fMRI data and DICOM metadata. By employing attention mechanisms, the proposed method captures intricate spatial-temporal patterns and contextual relationships, enhancing model accuracy, interpretability, and robustness. The potential of this framework spans applications in clinical diagnostics, cognitive neuroscience, and personalized medicine. Limitations, such as metadata variability and computational demands, are addressed, and future directions for optimizing scalability and generalizability are discussed.

</details>


### [140] [Solving Over-Smoothing in GNNs via Nonlocal Message Passing: Algebraic Smoothing and Depth Scalability](https://arxiv.org/abs/2512.08475)
*Weiqi Guan,Junlin He*

Main category: cs.LG

TL;DR: 论文分析LN位置与过平滑现象关系，提出基于Post - LN诱导代数平滑的方法，可支持更深网络且提升性能，无需额外参数。


<details>
  <summary>Details</summary>
Motivation: 探索LN放置位置与过平滑现象的关系，解决Pre - LN和Post - LN架构各自的问题。

Method: 提出基于Post - LN诱导代数平滑的新方法。

Result: 在五个基准测试中，该方法支持多达256层的更深网络，提升性能且无需额外参数。

Conclusion: 对LN动态及其影响进行理论分析，提出有效解决方法并通过实验验证其在更深GNNs中的有效性。

Abstract: The relationship between Layer Normalization (LN) placement and the over-smoothing phenomenon remains underexplored. We identify a critical dilemma: Pre-LN architectures avoid over-smoothing but suffer from the curse of depth, while Post-LN architectures bypass the curse of depth but experience over-smoothing.
  To resolve this, we propose a new method based on Post-LN that induces algebraic smoothing, preventing over-smoothing without the curse of depth. Empirical results across five benchmarks demonstrate that our approach supports deeper networks (up to 256 layers) and improves performance, requiring no additional parameters.
  Key contributions:
  Theoretical Characterization: Analysis of LN dynamics and their impact on over-smoothing and the curse of depth.
  A Principled Solution: A parameter-efficient method that induces algebraic smoothing and avoids over-smoothing and the curse of depth.
  Empirical Validation: Extensive experiments showing the effectiveness of the method in deeper GNNs.

</details>


### [141] [Optimal Perturbation Budget Allocation for Data Poisoning in Offline Reinforcement Learning](https://arxiv.org/abs/2512.08485)
*Junnan Qiu,Jie Li*

Main category: cs.LG

TL;DR: 本文提出一种新的离线强化学习全局预算分配攻击策略，在D4RL基准测试中表现优于基线策略。


<details>
  <summary>Details</summary>
Motivation: 现有离线强化学习数据中毒攻击策略采用局部统一扰动，对所有样本一视同仁，效率低且缺乏隐蔽性。

Method: 利用样本对价值函数收敛影响与时间差分（TD）误差成正比的理论，将攻击建模为全局资源分配问题，推导出在全局L2约束下扰动幅度与TD误差敏感度成正比的闭式解。

Result: 在D4RL基准测试中，该方法显著优于基线策略，用最小扰动实现高达80%的性能下降，且能躲避最先进的统计和频谱防御检测。

Conclusion: 提出的全局预算分配攻击策略有效且隐蔽，优于现有攻击策略。

Abstract: Offline Reinforcement Learning (RL) enables policy optimization from static datasets but is inherently vulnerable to data poisoning attacks. Existing attack strategies typically rely on locally uniform perturbations, which treat all samples indiscriminately. This approach is inefficient, as it wastes the perturbation budget on low-impact samples, and lacks stealthiness due to significant statistical deviations. In this paper, we propose a novel Global Budget Allocation attack strategy. Leveraging the theoretical insight that a sample's influence on value function convergence is proportional to its Temporal Difference (TD) error, we formulate the attack as a global resource allocation problem. We derive a closed-form solution where perturbation magnitudes are assigned proportional to the TD-error sensitivity under a global L2 constraint. Empirical results on D4RL benchmarks demonstrate that our method significantly outperforms baseline strategies, achieving up to 80% performance degradation with minimal perturbations that evade detection by state-of-the-art statistical and spectral defenses.

</details>


### [142] [Developing Distance-Aware Uncertainty Quantification Methods in Physics-Guided Neural Networks for Reliable Bearing Health Prediction](https://arxiv.org/abs/2512.08499)
*Waleed Razzaq,Yun-Bo Zhao*

Main category: cs.LG

TL;DR: 提出两种用于确定性物理引导神经网络的距离感知不确定性方法PG - SNGP和PG - SNER，在滚动轴承退化估计中表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有不确定性方法存在缺乏置信校准、运行成本高、无距离感知和在分布外数据泛化能力差等问题，需要准确且有不确定性感知的退化估计方法用于安全关键系统的预测性维护。

Method: 引入PG - SNGP和PG - SNER两种方法，对隐藏层应用谱归一化，PG - SNGP用高斯过程层替换最后一层，PG - SNER输出正态逆伽马参数；用标准精度指标和基于皮尔逊相关系数的距离感知指标评估，设计动态加权损失方案。

Result: PG - SNGP和PG - SNER提高了预测准确性，在分布外条件下可靠泛化，对对抗攻击和噪声保持鲁棒性。

Conclusion: 所提出的PG - SNGP和PG - SNER方法在滚动轴承退化估计中有效，有良好的性能和鲁棒性。

Abstract: Accurate and uncertainty-aware degradation estimation is essential for predictive maintenance in safety-critical systems like rotating machinery with rolling-element bearings. Many existing uncertainty methods lack confidence calibration, are costly to run, are not distance-aware, and fail to generalize under out-of-distribution data. We introduce two distance-aware uncertainty methods for deterministic physics-guided neural networks: PG-SNGP, based on Spectral Normalization Gaussian Process, and PG-SNER, based on Deep Evidential Regression. We apply spectral normalization to the hidden layers so the network preserves distances from input to latent space. PG-SNGP replaces the final dense layer with a Gaussian Process layer for distance-sensitive uncertainty, while PG-SNER outputs Normal Inverse Gamma parameters to model uncertainty in a coherent probabilistic form. We assess performance using standard accuracy metrics and a new distance-aware metric based on the Pearson Correlation Coefficient, which measures how well predicted uncertainty tracks the distance between test and training samples. We also design a dynamic weighting scheme in the loss to balance data fidelity and physical consistency. We test our methods on rolling-element bearing degradation using the PRONOSTIA dataset and compare them with Monte Carlo and Deep Ensemble PGNNs. Results show that PG-SNGP and PG-SNER improve prediction accuracy, generalize reliably under OOD conditions, and remain robust to adversarial attacks and noise.

</details>


### [143] [A Hybrid Model for Stock Market Forecasting: Integrating News Sentiment and Time Series Data with Graph Neural Networks](https://arxiv.org/abs/2512.08567)
*Nader Sadek,Mirette Moawad,Christina Naguib,Mariam Elzahaby*

Main category: cs.LG

TL;DR: 本文通过结合公司新闻文章和历史股票数据，采用多模态方法提升股票市场预测准确率，GNN模型优于LSTM基准模型。


<details>
  <summary>Details</summary>
Motivation: 传统股票市场预测模型主要依赖历史价格，而金融新闻能提供有用外部信号，旨在通过多模态方法改善预测性能。

Method: 将GNN模型与LSTM基准模型对比，用LSTM编码公司历史数据，用语言模型嵌入新闻标题，构建异构图并使用GraphSAGE捕获互动。评估两个预测目标。

Result: GNN模型表现优于LSTM基准模型，在第一个目标上准确率达53%，第二个目标上精度提高4%。新闻多的公司预测准确率更高，新闻标题比全文预测信号更强。

Conclusion: 多模态方法能有效提升股票市场预测性能，简洁新闻摘要对短期市场反应有重要作用。

Abstract: Stock market prediction is a long-standing challenge in finance, as accurate forecasts support informed investment decisions. Traditional models rely mainly on historical prices, but recent work shows that financial news can provide useful external signals. This paper investigates a multimodal approach that integrates companies' news articles with their historical stock data to improve prediction performance. We compare a Graph Neural Network (GNN) model with a baseline LSTM model. Historical data for each company is encoded using an LSTM, while news titles are embedded with a language model. These embeddings form nodes in a heterogeneous graph, and GraphSAGE is used to capture interactions between articles, companies, and industries. We evaluate two targets: a binary direction-of-change label and a significance-based label. Experiments on the US equities and Bloomberg datasets show that the GNN outperforms the LSTM baseline, achieving 53% accuracy on the first target and a 4% precision gain on the second. Results also indicate that companies with more associated news yield higher prediction accuracy. Moreover, headlines contain stronger predictive signals than full articles, suggesting that concise news summaries play an important role in short-term market reactions.

</details>


### [144] [An Additive Manufacturing Part Qualification Framework: Transferring Knowledge of Stress-strain Behaviors from Additively Manufactured Polymers to Metals](https://arxiv.org/abs/2512.08699)
*Chenglong Duan,Dazhong Wu*

Main category: cs.LG

TL;DR: 本文提出DTW - TL框架用于增材制造零件鉴定，将低成本聚合物应力 - 应变知识迁移到金属，实验表明框架有效且性能优于对比模型。


<details>
  <summary>Details</summary>
Motivation: 零件鉴定对增材制造至关重要，预测增材制造零件复杂应力 - 应变行为是关键，因此需要开发有效方法。

Method: 开发DTW - TL框架，用DTW选择与目标金属数据集最相关的聚合物数据集作为源域，用LSTM模型验证框架有效性。

Result: DTW - TL框架能找出聚合物与金属最匹配项选单一源域，在以三种金属为目标域时模型表现优于对比模型。

Conclusion: DTW - TL框架在增材制造零件鉴定中有效，可将聚合物应力 - 应变知识迁移到金属。

Abstract: Part qualification is crucial in additive manufacturing (AM) because it ensures that additively manufactured parts can be consistently produced and reliably used in critical applications. Part qualification aims at verifying that an additively manufactured part meets performance requirements; therefore, predicting the complex stress-strain behaviors of additively manufactured parts is critical. We develop a dynamic time warping (DTW)-transfer learning (TL) framework for additive manufacturing part qualification by transferring knowledge of the stress-strain behaviors of additively manufactured low-cost polymers to metals. Specifically, the framework employs DTW to select a polymer dataset as the source domain that is the most relevant to the target metal dataset. Using a long short-term memory (LSTM) model, four source polymers (i.e., Nylon, PLA, CF-ABS, and Resin) and three target metals (i.e., AlSi10Mg, Ti6Al4V, and carbon steel) that are fabricated by different AM techniques are utilized to demonstrate the effectiveness of the DTW-TL framework. Experimental results show that the DTW-TL framework identifies the closest match between polymers and metals to select one single polymer dataset as the source domain. The DTW-TL model achieves the lowest mean absolute percentage error of 12.41% and highest coefficient of determination of 0.96 when three metals are used as the target domain, respectively, outperforming the vanilla LSTM model without TL as well as the TL model pre-trained on four polymer datasets as the source domain.

</details>


### [145] [Exposing Hidden Biases in Text-to-Image Models via Automated Prompt Search](https://arxiv.org/abs/2512.08724)
*Manos Plitsis,Giorgos Bouritsas,Vassilis Katsouros,Yannis Panagakis*

Main category: cs.LG

TL;DR: 提出Bias - Guided Prompt Search (BGPS)框架自动生成引发图像偏见的提示，发现新偏见，可作评估工具。


<details>
  <summary>Details</summary>
Motivation: 现有缓解文本到图像扩散模型社会偏见的方法依赖提示数据集，有成本且可能遗漏引发偏见的提示。

Method: BGPS框架包含指令生成属性中立提示的大语言模型和基于文本到图像模型内部表示的属性分类器，引导大语言模型解码。

Result: 在Stable Diffusion 1.5和去偏模型上实验，发现新偏见，提示可解释，改进困惑度指标。

Conclusion: 揭示文本到图像模型漏洞，BGPS扩展偏见搜索空间，可作新评估工具。

Abstract: Text-to-image (TTI) diffusion models have achieved remarkable visual quality, yet they have been repeatedly shown to exhibit social biases across sensitive attributes such as gender, race and age. To mitigate these biases, existing approaches frequently depend on curated prompt datasets - either manually constructed or generated with large language models (LLMs) - as part of their training and/or evaluation procedures. Beside the curation cost, this also risks overlooking unanticipated, less obvious prompts that trigger biased generation, even in models that have undergone debiasing. In this work, we introduce Bias-Guided Prompt Search (BGPS), a framework that automatically generates prompts that aim to maximize the presence of biases in the resulting images. BGPS comprises two components: (1) an LLM instructed to produce attribute-neutral prompts and (2) attribute classifiers acting on the TTI's internal representations that steer the decoding process of the LLM toward regions of the prompt space that amplify the image attributes of interest. We conduct extensive experiments on Stable Diffusion 1.5 and a state-of-the-art debiased model and discover an array of subtle and previously undocumented biases that severely deteriorate fairness metrics. Crucially, the discovered prompts are interpretable, i.e they may be entered by a typical user, quantitatively improving the perplexity metric compared to a prominent hard prompt optimization counterpart. Our findings uncover TTI vulnerabilities, while BGPS expands the bias search space and can act as a new evaluation tool for bias mitigation.

</details>


### [146] [Neural Ordinary Differential Equations for Simulating Metabolic Pathway Dynamics from Time-Series Multiomics Data](https://arxiv.org/abs/2512.08732)
*Udesh Habaraduwa,Andrei Lixandru*

Main category: cs.LG

TL;DR: 本文引入神经常微分方程（NODEs）学习蛋白质组和代谢组的复杂相互作用，应用于大肠杆菌时间序列数据，相比传统机器学习管道效果更好，能高效推进代谢工程和生物发现。


<details>
  <summary>Details</summary>
Motivation: 高吞吐量多组学数据丰富，但转化为可操作预测模型存在瓶颈，需要高容量、数据驱动的模拟系统来解决复杂生物系统行为预测问题。

Method: 引入NODEs作为动态框架，应用到工程大肠杆菌菌株的时间序列数据，对代谢途径的连续动态进行建模。

Result: NODE架构在捕捉系统动态方面优于传统机器学习管道，在柠檬烯和异戊烯醇途径数据集上均方根误差较基线提升超90%，推理时间加速1000倍。

Conclusion: NODE模型是下一代代谢工程和生物发现的可扩展、高保真工具。

Abstract: The advancement of human healthspan and bioengineering relies heavily on predicting the behavior of complex biological systems. While high-throughput multiomics data is becoming increasingly abundant, converting this data into actionable predictive models remains a bottleneck. High-capacity, datadriven simulation systems are critical in this landscape; unlike classical mechanistic models restricted by prior knowledge, these architectures can infer latent interactions directly from observational data, allowing for the simulation of temporal trajectories and the anticipation of downstream intervention effects in personalized medicine and synthetic biology. To address this challenge, we introduce Neural Ordinary Differential Equations (NODEs) as a dynamic framework for learning the complex interplay between the proteome and metabolome. We applied this framework to time-series data derived from engineered Escherichia coli strains, modeling the continuous dynamics of metabolic pathways. The proposed NODE architecture demonstrates superior performance in capturing system dynamics compared to traditional machine learning pipelines. Our results show a greater than 90% improvement in root mean squared error over baselines across both Limonene (up to 94.38% improvement) and Isopentenol (up to 97.65% improvement) pathway datasets. Furthermore, the NODE models demonstrated a 1000x acceleration in inference time, establishing them as a scalable, high-fidelity tool for the next generation of metabolic engineering and biological discovery.

</details>


### [147] [Learning and Editing Universal Graph Prompt Tuning via Reinforcement Learning](https://arxiv.org/abs/2512.08763)
*Jinfeng Xu,Zheyu Chen,Shuo Yang,Jinze Li,Hewei Wang,Yijie Li,Edith C. H. Ngai*

Main category: cs.LG

TL;DR: 现有图提示调优方法有局限，本文提出LEAP模型和范式，加强理论基础并追求更优提示，实验显示其性能优越。


<details>
  <summary>Details</summary>
Motivation: 早期图提示调优方法依赖特定任务设计，选择性节点的图提示调优会破坏通用图提示调优的理论基础，需加强理论基础并追求更理想提示。

Method: 引入更严格约束，提出LEAP模型和范式，先构建基本通用图提示，再用演员-评论家强化学习选择节点和编辑提示。

Result: 在多种预训练策略的图和节点级任务的全量和少样本场景实验中，LEAP始终优于微调及其他基于提示的方法。

Conclusion: 添加提示到所有节点是实现图提示通用性的必要条件，LEAP能在保留理论基础的同时追求更理想提示。

Abstract: Early graph prompt tuning approaches relied on task-specific designs for Graph Neural Networks (GNNs), limiting their adaptability across diverse pre-training strategies. In contrast, another promising line of research has investigated universal graph prompt tuning, which operates directly in the input graph's feature space and builds a theoretical foundation that universal graph prompt tuning can theoretically achieve an equivalent effect of any prompting function, eliminating dependence on specific pre-training strategies. Recent works propose selective node-based graph prompt tuning to pursue more ideal prompts. However, we argue that selective node-based graph prompt tuning inevitably compromises the theoretical foundation of universal graph prompt tuning. In this paper, we strengthen the theoretical foundation of universal graph prompt tuning by introducing stricter constraints, demonstrating that adding prompts to all nodes is a necessary condition for achieving the universality of graph prompts. To this end, we propose a novel model and paradigm, Learning and Editing Universal GrAph Prompt Tuning (LEAP), which preserves the theoretical foundation of universal graph prompt tuning while pursuing more ideal prompts. Specifically, we first build the basic universal graph prompts to preserve the theoretical foundation and then employ actor-critic reinforcement learning to select nodes and edit prompts. Extensive experiments on graph- and node-level tasks across various pre-training strategies in both full-shot and few-shot scenarios show that LEAP consistently outperforms fine-tuning and other prompt-based approaches.

</details>


### [148] [De novo generation of functional terpene synthases using TpsGPT](https://arxiv.org/abs/2512.08772)
*Hamsini Ramanathan,Roman Bushuiev,Matouš Soldát,Jirí Kohout,Téo Hebra,Joshua David Smith,Josef Sivic,Tomáš Pluskal*

Main category: cs.LG

TL;DR: 本文提出生成模型TpsGPT用于可扩展的萜烯合酶（TPS）蛋白质设计，经多指标评估和实验验证，证明该方法能从头生成有功能的酶。


<details>
  <summary>Details</summary>
Motivation: 传统通过定向进化进行TPS从头设计成本高且速度慢，需新方法。

Method: 在从UniProt挖掘的79k个TPS序列上微调蛋白质语言模型ProtGPT2构建TpsGPT，用多种验证指标评估生成的酶候选序列。

Result: 从28k个生成序列中确定7个满足所有验证标准的推测TPS酶，至少2个序列经实验验证有TPS酶活性。

Conclusion: 在精心策划的特定酶类数据集上微调蛋白质语言模型，结合严格过滤，可实现从头生成有功能且进化距离远的酶。

Abstract: Terpene synthases (TPS) are a key family of enzymes responsible for generating the diverse terpene scaffolds that underpin many natural products, including front-line anticancer drugs such as Taxol. However, de novo TPS design through directed evolution is costly and slow. We introduce TpsGPT, a generative model for scalable TPS protein design, built by fine-tuning the protein language model ProtGPT2 on 79k TPS sequences mined from UniProt. TpsGPT generated de novo enzyme candidates in silico and we evaluated them using multiple validation metrics, including EnzymeExplorer classification, ESMFold structural confidence (pLDDT), sequence diversity, CLEAN classification, InterPro domain detection, and Foldseek structure alignment. From an initial pool of 28k generated sequences, we identified seven putative TPS enzymes that satisfied all validation criteria. Experimental validation confirmed TPS enzymatic activity in at least two of these sequences. Our results show that fine-tuning of a protein language model on a carefully curated, enzyme-class-specific dataset, combined with rigorous filtering, can enable the de novo generation of functional, evolutionarily distant enzymes.

</details>


### [149] [Can TabPFN Compete with GNNs for Node Classification via Graph Tabularization?](https://arxiv.org/abs/2512.08798)
*Jeongwhan Choi,Woosung Kang,Minseo Kim,Jongwoo Kim,Noseong Park*

Main category: cs.LG

TL;DR: 研究将图节点分类转化为表格学习问题，提出TabPFN - GN，实验显示其在同构和异构图上表现良好。


<details>
  <summary>Details</summary>
Motivation: 受TabPFN在表格数据和时间序列上的成功启发，探究能否将图节点分类有效转化为表格学习问题。

Method: 引入TabPFN - GN，将图数据转化为表格特征，让TabPFN直接进行节点分类，无需图特定训练和语言模型依赖。

Result: 在12个基准数据集上，TabPFN - GN在同构图上与GNNs表现相当，在异构图上始终优于GNNs。

Conclusion: 有原则的特征工程可弥合表格和图领域的差距，为特定任务的GNN训练和依赖大语言模型的图基础模型提供实用替代方案。

Abstract: Foundation models pretrained on large data have demonstrated remarkable zero-shot generalization capabilities across domains. Building on the success of TabPFN for tabular data and its recent extension to time series, we investigate whether graph node classification can be effectively reformulated as a tabular learning problem. We introduce TabPFN-GN, which transforms graph data into tabular features by extracting node attributes, structural properties, positional encodings, and optionally smoothed neighborhood features. This enables TabPFN to perform direct node classification without any graph-specific training or language model dependencies. Our experiments on 12 benchmark datasets reveal that TabPFN-GN achieves competitive performance with GNNs on homophilous graphs and consistently outperforms them on heterophilous graphs. These results demonstrate that principled feature engineering can bridge the gap between tabular and graph domains, providing a practical alternative to task-specific GNN training and LLM-dependent graph foundation models.

</details>


### [150] [Identifying counterfactual probabilities using bivariate distributions and uplift modeling](https://arxiv.org/abs/2512.08805)
*Théo Verhelst,Gianluca Bontempi*

Main category: cs.LG

TL;DR: 提出一种利用提升模型进行反事实估计的方法，通过对预测的提升分数拟合双变量β分布来得到反事实结果的后验分布，模拟显示该方法有效。


<details>
  <summary>Details</summary>
Motivation: 反事实识别能恢复潜在结果的联合分布，比提升建模信息更丰富但更难估计，探索利用提升模型进行反事实估计。

Method: 提出一种反事实估计器，对预测的提升分数拟合双变量β分布，得到反事实结果的后验分布。

Result: 模拟显示该方法有效，可用于电信客户流失问题，能揭示标准机器学习或提升模型单独无法获得的见解。

Conclusion: 提升模型和反事实识别方法具有协同作用，所提出的反事实估计方法可行且有效。

Abstract: Uplift modeling estimates the causal effect of an intervention as the difference between potential outcomes under treatment and control, whereas counterfactual identification aims to recover the joint distribution of these potential outcomes (e.g., "Would this customer still have churned had we given them a marketing offer?"). This joint counterfactual distribution provides richer information than the uplift but is harder to estimate. However, the two approaches are synergistic: uplift models can be leveraged for counterfactual estimation. We propose a counterfactual estimator that fits a bivariate beta distribution to predicted uplift scores, yielding posterior distributions over counterfactual outcomes. Our approach requires no causal assumptions beyond those of uplift modeling. Simulations show the efficacy of the approach, which can be applied, for example, to the problem of customer churn in telecom, where it reveals insights unavailable to standard ML or uplift models alone.

</details>


### [151] [Forecasting Fails: Unveiling Evasion Attacks in Weather Prediction Models](https://arxiv.org/abs/2512.08832)
*Huzaifa Arif,Pin-Yu Chen,Alex Gittens,James Diffenderfer,Bhavya Kailkhura*

Main category: cs.LG

TL;DR: 本文提出WAAPO框架评估AI气象预报模型对抗扰动脆弱性，实验显示模型易受攻击，需防护机制


<details>
  <summary>Details</summary>
Motivation: 评估AI气象预报模型对抗扰动的脆弱性

Method: 引入WAAPO框架，通过结合通道稀疏性、空间定位和光滑性约束，生成有效且隐蔽的目标对抗扰动

Result: 利用ERA5数据集和FourCastNet，证明WAAPO能在受限条件下生成与预定义目标紧密对齐的对抗轨迹，小扰动会导致预测天气模式显著偏差

Conclusion: AI驱动的预报模型存在关键漏洞，业务预报系统需强大防护措施防止对抗攻击

Abstract: With the increasing reliance on AI models for weather forecasting, it is imperative to evaluate their vulnerability to adversarial perturbations. This work introduces Weather Adaptive Adversarial Perturbation Optimization (WAAPO), a novel framework for generating targeted adversarial perturbations that are both effective in manipulating forecasts and stealthy to avoid detection. WAAPO achieves this by incorporating constraints for channel sparsity, spatial localization, and smoothness, ensuring that perturbations remain physically realistic and imperceptible. Using the ERA5 dataset and FourCastNet (Pathak et al. 2022), we demonstrate WAAPO's ability to generate adversarial trajectories that align closely with predefined targets, even under constrained conditions. Our experiments highlight critical vulnerabilities in AI-driven forecasting models, where small perturbations to initial conditions can result in significant deviations in predicted weather patterns. These findings underscore the need for robust safeguards to protect against adversarial exploitation in operational forecasting systems.

</details>


### [152] [Reinforcement Learning From State and Temporal Differences](https://arxiv.org/abs/2512.08855)
*Lex Weaver,Jonathan Baxter*

Main category: cs.LG

TL;DR: 指出TD($λ$)可能使最优策略收敛到次优，提出改进的STD($λ$)方法，并理论分析和实验验证。


<details>
  <summary>Details</summary>
Motivation: TD($λ$)以最小化状态值误差为主，但策略中状态相对排序误差更关键，TD($λ$)可能从最优策略收敛到次优策略。

Method: 提出改进的STD($λ$)方法，使函数逼近器针对二元决策问题的相对状态值训练，给出两状态系统下的理论分析，并与Bertsekas的差分训练方法比较。

Result: STD($λ$)在两状态系统和变体机械臂问题中成功应用。

Conclusion: STD($λ$)可避免TD($λ$)的问题，在单调策略改进上表现良好。

Abstract: TD($λ$) with function approximation has proved empirically successful for some complex reinforcement learning problems. For linear approximation, TD($λ$) has been shown to minimise the squared error between the approximate value of each state and the true value. However, as far as policy is concerned, it is error in the relative ordering of states that is critical, rather than error in the state values. We illustrate this point, both in simple two-state and three-state systems in which TD($λ$)--starting from an optimal policy--converges to a sub-optimal policy, and also in backgammon. We then present a modified form of TD($λ$), called STD($λ$), in which function approximators are trained with respect to relative state values on binary decision problems. A theoretical analysis, including a proof of monotonic policy improvement for STD($λ$) in the context of the two-state system, is presented, along with a comparison with Bertsekas' differential training method [1]. This is followed by successful demonstrations of STD($λ$) on the two-state system and a variation on the well known acrobot problem.

</details>


### [153] [Refining Diffusion Models for Motion Synthesis with an Acceleration Loss to Generate Realistic IMU Data](https://arxiv.org/abs/2512.08859)
*Lars Ole Häusler,Lena Uhlenberg,Göran Köber,Diyora Salimova,Oliver Amft*

Main category: cs.LG

TL;DR: 提出基于加速度二阶损失微调扩散模型的文本到IMU运动合成框架，提升数据质量和HAR性能。


<details>
  <summary>Details</summary>
Motivation: 获取更逼真的IMU数据，对齐扩散先验与IMU加速度模式。

Method: 将加速度二阶损失L_acc集成到现有扩散模型训练目标中进行微调，用含表面建模和虚拟传感器模拟的框架评估。

Result: L_acc相对原模型下降12.7%，高动态活动改善大，合成数据分布更接近真实数据，HAR分类性能提升。

Conclusion: 加速度感知的扩散细化能有效对齐运动生成和IMU合成，深度学习管道可灵活用于特定任务。

Abstract: We propose a text-to-IMU (inertial measurement unit) motion-synthesis framework to obtain realistic IMU data by fine-tuning a pretrained diffusion model with an acceleration-based second-order loss (L_acc). L_acc enforces consistency in the discrete second-order temporal differences of the generated motion, thereby aligning the diffusion prior with IMU-specific acceleration patterns. We integrate L_acc into the training objective of an existing diffusion model, finetune the model to obtain an IMU-specific motion prior, and evaluate the model with an existing text-to-IMU framework that comprises surface modelling and virtual sensor simulation. We analysed acceleration signal fidelity and differences between synthetic motion representation and actual IMU recordings. As a downstream application, we evaluated Human Activity Recognition (HAR) and compared the classification performance using data of our method with the earlier diffusion model and two additional diffusion model baselines. When we augmented the earlier diffusion model objective with L_acc and continued training, L_acc decreased by 12.7% relative to the original model. The improvements were considerably larger in high-dynamic activities (i.e., running, jumping) compared to low-dynamic activities~(i.e., sitting, standing). In a low-dimensional embedding, the synthetic IMU data produced by our refined model shifts closer to the distribution of real IMU recordings. HAR classification trained exclusively on our refined synthetic IMU data improved performance by 8.7% compared to the earlier diffusion model and by 7.6% over the best-performing comparison diffusion model. We conclude that acceleration-aware diffusion refinement provides an effective approach to align motion generation and IMU synthesis and highlights how flexible deep learning pipelines are for specialising generic text-to-motion priors to sensor-specific tasks.

</details>


### [154] [Differentially Private Synthetic Data Generation Using Context-Aware GANs](https://arxiv.org/abs/2512.08869)
*Anantaa Kotal,Anupam Joshi*

Main category: cs.LG

TL;DR: 大数据使用带来隐私问题，合成数据是解决方案，但传统方法有不足。提出ContextGAN，验证其在多领域能生成高质量、符合规则且保护隐私的合成数据。


<details>
  <summary>Details</summary>
Motivation: 大数据使用引发隐私担忧，法规严格，传统合成数据方法难捕捉复杂隐式规则，影响数据真实性和实用性。

Method: 提出ContextGAN，即上下文感知的差分隐私生成对抗网络，通过约束矩阵集成领域规则，用约束感知判别器评估合成数据，差分隐私保护敏感信息。

Result: 在医疗、安全和金融领域验证了ContextGAN，表明其能生成符合领域规则、保护隐私的高质量合成数据。

Conclusion: ContextGAN通过实施领域约束提高了数据的真实性和实用性，适用于严格隐私要求下需遵循显式和隐式规则的应用。

Abstract: The widespread use of big data across sectors has raised major privacy concerns, especially when sensitive information is shared or analyzed. Regulations such as GDPR and HIPAA impose strict controls on data handling, making it difficult to balance the need for insights with privacy requirements. Synthetic data offers a promising solution by creating artificial datasets that reflect real patterns without exposing sensitive information. However, traditional synthetic data methods often fail to capture complex, implicit rules that link different elements of the data and are essential in domains like healthcare. They may reproduce explicit patterns but overlook domain-specific constraints that are not directly stated yet crucial for realism and utility. For example, prescription guidelines that restrict certain medications for specific conditions or prevent harmful drug interactions may not appear explicitly in the original data. Synthetic data generated without these implicit rules can lead to medically inappropriate or unrealistic profiles. To address this gap, we propose ContextGAN, a Context-Aware Differentially Private Generative Adversarial Network that integrates domain-specific rules through a constraint matrix encoding both explicit and implicit knowledge. The constraint-aware discriminator evaluates synthetic data against these rules to ensure adherence to domain constraints, while differential privacy protects sensitive details from the original data. We validate ContextGAN across healthcare, security, and finance, showing that it produces high-quality synthetic data that respects domain rules and preserves privacy. Our results demonstrate that ContextGAN improves realism and utility by enforcing domain constraints, making it suitable for applications that require compliance with both explicit patterns and implicit rules under strict privacy guarantees.

</details>


### [155] [Fed-SE: Federated Self-Evolution for Privacy-Constrained Multi-Environment LLM Agents](https://arxiv.org/abs/2512.08870)
*Xiang Chen,Yuling Shi,Qizhen Lan,Yuchao Qiu,Xiaodong Gu*

Main category: cs.LG

TL;DR: 提出Fed - SE框架用于LLM代理在隐私约束下跨环境知识转移，经实验验证比基线提升约18%成功率。


<details>
  <summary>Details</summary>
Motivation: LLM代理在复杂交互任务中受隐私约束，无法集中优化和跨环境联合进化，标准联邦学习难应用于代理的开放式自我进化。

Method: 提出Fed - SE框架，建立本地进化 - 全局聚合范式，本地在过滤后的高回报轨迹上进行参数高效微调，全局在低秩子空间聚合更新。

Result: 在五个异构环境实验中，Fed - SE比联邦基线平均任务成功率提高约18%。

Conclusion: Fed - SE在隐私约束部署下能实现稳健的跨环境知识转移，有效解决了现有问题。

Abstract: LLM agents are widely deployed in complex interactive tasks, yet privacy constraints often preclude centralized optimization and co-evolution across dynamic environments. While Federated Learning (FL) has proven effective on static datasets, its extension to the open-ended self-evolution of agents remains underexplored. Directly applying standard FL is challenging: heterogeneous tasks and sparse, trajectory-level rewards introduce severe gradient conflicts, destabilizing the global optimization process. To bridge this gap, we propose Fed-SE, a Federated Self-Evolution framework for LLM agents. Fed-SE establishes a local evolution-global aggregation paradigm. Locally, agents employ parameter-efficient fine-tuning on filtered, high-return trajectories to achieve stable gradient updates. Globally, Fed-SE aggregates updates within a low-rank subspace that disentangles environment-specific dynamics, effectively reducing negative transfer across clients. Experiments across five heterogeneous environments demonstrate that Fed-SE improves average task success rates by approximately 18% over federated baselines, validating its effectiveness in robust cross-environment knowledge transfer in privacy-constrained deployments.

</details>


### [156] [When Tables Leak: Attacking String Memorization in LLM-Based Tabular Data Generation](https://arxiv.org/abs/2512.08875)
*Joshua Ward,Bochao Gu,Chi-Hua Wang,Guang Cheng*

Main category: cs.LG

TL;DR: 研究指出基于大语言模型的表格合成数据生成存在隐私泄露问题，提出LevAtt攻击揭示风险，还给出两种防御方法。


<details>
  <summary>Details</summary>
Motivation: 发现流行的大语言模型表格数据生成实现方式存在隐私泄露问题，需系统分析风险并找到防御方法。

Method: 引入简单的无盒成员推理攻击LevAtt分析隐私风险，提出两种防御方法，包括一种新颖的采样策略。

Result: LevAtt攻击揭示了多种模型和数据集存在大量隐私泄露，部分情况下对先进模型是完美的成员分类器；提出的防御方法能在合成数据保真度和实用性损失最小的情况下抵御攻击。

Conclusion: 基于大语言模型的合成数据生成存在独特隐私漏洞，需要有效防御，提出的方法可应对攻击。

Abstract: Large Language Models (LLMs) have recently demonstrated remarkable performance in generating high-quality tabular synthetic data. In practice, two primary approaches have emerged for adapting LLMs to tabular data generation: (i) fine-tuning smaller models directly on tabular datasets, and (ii) prompting larger models with examples provided in context. In this work, we show that popular implementations from both regimes exhibit a tendency to compromise privacy by reproducing memorized patterns of numeric digits from their training data. To systematically analyze this risk, we introduce a simple No-box Membership Inference Attack (MIA) called LevAtt that assumes adversarial access to only the generated synthetic data and targets the string sequences of numeric digits in synthetic observations. Using this approach, our attack exposes substantial privacy leakage across a wide range of models and datasets, and in some cases, is even a perfect membership classifier on state-of-the-art models. Our findings highlight a unique privacy vulnerability of LLM-based synthetic data generation and the need for effective defenses. To this end, we propose two methods, including a novel sampling strategy that strategically perturbs digits during generation. Our evaluation demonstrates that this approach can defeat these attacks with minimal loss of fidelity and utility of the synthetic data.

</details>


### [157] [DAO-GP Drift Aware Online Non-Linear Regression Gaussian-Process](https://arxiv.org/abs/2512.08879)
*Mohammad Abu-Shaira,Ajita Rattani,Weishi Shi*

Main category: cs.LG

TL;DR: 提出DAO - GP模型解决在线高斯过程方法在处理概念漂移问题上的局限，经评估性能优越。


<details>
  <summary>Details</summary>
Motivation: 现实数据集有概念漂移现象，忽视它会降低模型预测准确性，且在线模型超参数固定，传统在线高斯过程方法有诸多局限。

Method: 提出DAO - GP，具备内置漂移检测和自适应机制，可根据漂移严重程度动态调整模型行为。

Result: 经大量实证评估，DAO - GP在多种条件和数据特征下表现稳健，与现有模型相比性能优越或有竞争力。

Conclusion: DAO - GP是在线非线性回归中抗漂移的有效解决方案。

Abstract: Real-world datasets often exhibit temporal dynamics characterized by evolving data distributions. Disregarding this phenomenon, commonly referred to as concept drift, can significantly diminish a model's predictive accuracy. Furthermore, the presence of hyperparameters in online models exacerbates this issue. These parameters are typically fixed and cannot be dynamically adjusted by the user in response to the evolving data distribution. Gaussian Process (GP) models offer powerful non-parametric regression capabilities with uncertainty quantification, making them ideal for modeling complex data relationships in an online setting. However, conventional online GP methods face several critical limitations, including a lack of drift-awareness, reliance on fixed hyperparameters, vulnerability to data snooping, absence of a principled decay mechanism, and memory inefficiencies. In response, we propose DAO-GP (Drift-Aware Online Gaussian Process), a novel, fully adaptive, hyperparameter-free, decayed, and sparse non-linear regression model. DAO-GP features a built-in drift detection and adaptation mechanism that dynamically adjusts model behavior based on the severity of drift. Extensive empirical evaluations confirm DAO-GP's robustness across stationary conditions, diverse drift types (abrupt, incremental, gradual), and varied data characteristics. Analyses demonstrate its dynamic adaptation, efficient in-memory and decay-based management, and evolving inducing points. Compared with state-of-the-art parametric and non-parametric models, DAO-GP consistently achieves superior or competitive performance, establishing it as a drift-resilient solution for online non-linear regression.

</details>


### [158] [Explainable Anomaly Detection for Industrial IoT Data Streams](https://arxiv.org/abs/2512.08885)
*Ana Rita Paupério,Diogo Risca,Afonso Lourenço,Goreti Marreiros,Ricardo Martins*

Main category: cs.LG

TL;DR: 本文提出用于工业维护的协作式数据流挖掘框架，结合无监督异常检测与人工参与学习支持维护决策，介绍方法并给出提花织机故障检测初步结果，后续将持续监测预测轴承故障。


<details>
  <summary>Details</summary>
Motivation: 工业维护因物联网和边缘计算产生需实时决策的数据流，但现有数据流挖掘大多为全监督设置，实际中标签常缺失或延迟。

Method: 采用在线孤立森林，使用增量偏依赖图和特征重要性分数增强可解释性，让用户能动态评估特征相关性并调整异常阈值。

Result: 给出提花织机单元故障检测的初步结果。

Conclusion: 持续工作目标是连续监测并预测和解释即将发生的轴承故障。

Abstract: Industrial maintenance is being transformed by the Internet of Things and edge computing, generating continuous data streams that demand real-time, adaptive decision-making under limited computational resources. While data stream mining (DSM) addresses this challenge, most methods assume fully supervised settings, yet in practice, ground-truth labels are often delayed or unavailable. This paper presents a collaborative DSM framework that integrates unsupervised anomaly detection with interactive, human-in-the-loop learning to support maintenance decisions. We employ an online Isolation Forest and enhance interpretability using incremental Partial Dependence Plots and a feature importance score, derived from deviations of Individual Conditional Expectation curves from a fading average, enabling users to dynamically reassess feature relevance and adjust anomaly thresholds. We describe the real-time implementation and provide initial results for fault detection in a Jacquard loom unit. Ongoing work targets continuous monitoring to predict and explain imminent bearing failures.

</details>


### [159] [Revisiting the Scaling Properties of Downstream Metrics in Large Language Model Training](https://arxiv.org/abs/2512.08894)
*Jakub Krajewski,Amitis Shidani,Dan Busbridge,Sam Wiseman,Jason Ramapuram*

Main category: cs.LG

TL;DR: 本文提出直接框架从训练预算建模基准性能的缩放，发现固定令牌 - 参数比时简单幂律能描述下游任务对数准确率缩放，直接方法外推效果更好，还引入预测准确率的函数形式并验证，发布数据支持研究。


<details>
  <summary>Details</summary>
Motivation: 传统大语言模型缩放定律聚焦预训练损失等代理指标，预测下游任务性能不可靠，本文旨在挑战该观点，直接从训练预算建模下游任务性能缩放。

Method: 提出直接框架，使用简单幂律描述固定令牌 - 参数比下下游任务的缩放行为，引入预测准确率的函数形式。

Result: 固定令牌 - 参数比时简单幂律能准确描述多下游任务对数准确率的缩放行为；直接方法比之前的两阶段程序外推效果好；验证了函数形式在不同模型和数据集上的有效性。

Conclusion: 提出的直接框架有效，能更好地预测大语言模型下游任务性能，发布数据支持后续研究。

Abstract: While scaling laws for Large Language Models (LLMs) traditionally focus on proxy metrics like pretraining loss, predicting downstream task performance has been considered unreliable. This paper challenges that view by proposing a direct framework to model the scaling of benchmark performance from the training budget. We find that for a fixed token-to-parameter ratio, a simple power law can accurately describe the scaling behavior of log accuracy on multiple popular downstream tasks. Our results show that the direct approach extrapolates better than the previously proposed two-stage procedure, which is prone to compounding errors. Furthermore, we introduce functional forms that predict accuracy across token-to-parameter ratios and account for inference compute under repeated sampling. We validate our findings on models with up to 17B parameters trained on up to 350B tokens across two dataset mixtures. To support reproducibility and encourage future research, we release the complete set of pretraining losses and downstream evaluation results.

</details>


### [160] [Open Polymer Challenge: Post-Competition Report](https://arxiv.org/abs/2512.08896)
*Gang Liu,Sobin Alosious,Subhamoy Mahajan,Eric Inae,Yihan Zhu,Yuhan Liu,Renzheng Zhang,Jiaxin Xu,Addison Howard,Ying Li,Tengfei Luo,Meng Jiang*

Main category: cs.LG

TL;DR: Open Polymer Challenge (OPC) 发布首个聚合物信息学基准数据集，开展多任务聚合物属性预测竞赛，为聚合物科学的分子 AI 奠定基础，还发布测试数据集和数据生成管道。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习在发现可持续聚合物材料时缺乏大型、高质量和开放可访问聚合物数据集的问题。

Method: 参与者在小数据、标签不平衡和异构模拟源等现实约束下，采用基于特征的增强、迁移学习、自监督预训练和有针对性的集成策略等技术开发模型。

Result: 竞赛揭示了数据准备、分布偏移和跨组模拟一致性等重要经验教训，得到了模型、分析结果和发布的数据。

Conclusion: 竞赛成果为聚合物科学的分子 AI 创造新基础，有望加速可持续和节能材料的开发。

Abstract: Machine learning (ML) offers a powerful path toward discovering sustainable polymer materials, but progress has been limited by the lack of large, high-quality, and openly accessible polymer datasets. The Open Polymer Challenge (OPC) addresses this gap by releasing the first community-developed benchmark for polymer informatics, featuring a dataset with 10K polymers and 5 properties: thermal conductivity, radius of gyration, density, fractional free volume, and glass transition temperature. The challenge centers on multi-task polymer property prediction, a core step in virtual screening pipelines for materials discovery. Participants developed models under realistic constraints that include small data, label imbalance, and heterogeneous simulation sources, using techniques such as feature-based augmentation, transfer learning, self-supervised pretraining, and targeted ensemble strategies. The competition also revealed important lessons about data preparation, distribution shifts, and cross-group simulation consistency, informing best practices for future large-scale polymer datasets. The resulting models, analysis, and released data create a new foundation for molecular AI in polymer science and are expected to accelerate the development of sustainable and energy-efficient materials. Along with the competition, we release the test dataset at https://www.kaggle.com/datasets/alexliu99/neurips-open-polymer-prediction-2025-test-data. We also release the data generation pipeline at https://github.com/sobinalosious/ADEPT, which simulates more than 25 properties, including thermal conductivity, radius of gyration, and density.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [161] [Conditional Morphogenesis: Emergent Generation of Structural Digits via Neural Cellular Automata](https://arxiv.org/abs/2512.08360)
*Ali Sakour*

Main category: cs.NE

TL;DR: 提出条件神经细胞自动机(c - NCA)架构生成MNIST数字拓扑结构，具局部性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有NCA研究多聚焦连续纹理合成或单目标物体恢复，类条件结构生成问题待探索。

Method: 提出c - NCA架构，通过向细胞感知场注入单热条件，利用局部规则自组装。

Result: c - NCA稳定收敛，能从单个像素正确形成数字拓扑结构，具生物系统鲁棒性。

Conclusion: 该工作填补基于纹理的NCA和结构模式形成间的差距，为条件生成提供轻量级、生物合理的替代方案。

Abstract: Biological systems exhibit remarkable morphogenetic plasticity, where a single genome can encode various specialized cellular structures triggered by local chemical signals. In the domain of Deep Learning, Differentiable Neural Cellular Automata (NCA) have emerged as a paradigm to mimic this self-organization. However, existing NCA research has predominantly focused on continuous texture synthesis or single-target object recovery, leaving the challenge of class-conditional structural generation largely unexplored. In this work, we propose a novel Conditional Neural Cellular Automata (c-NCA) architecture capable of growing distinct topological structures - specifically MNIST digits - from a single generic seed, guided solely by a spatially broadcasted class vector. Unlike traditional generative models (e.g., GANs, VAEs) that rely on global reception fields, our model enforces strict locality and translation equivariance. We demonstrate that by injecting a one-hot condition into the cellular perception field, a single set of local rules can learn to break symmetry and self-assemble into ten distinct geometric attractors. Experimental results show that our c-NCA achieves stable convergence, correctly forming digit topologies from a single pixel, and exhibits robustness characteristic of biological systems. This work bridges the gap between texture-based NCAs and structural pattern formation, offering a lightweight, biologically plausible alternative for conditional generation.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [162] [Multi-domain performance analysis with scores tailored to user preferences](https://arxiv.org/abs/2512.08715)
*Sébastien Piérard,Adrien Deliège,Marc Van Droogenbroeck*

Main category: cs.PF

TL;DR: 采用概率框架分析算法等在多领域的加权平均性能，定义四类领域并为二分类开发新可视化工具。


<details>
  <summary>Details</summary>
Motivation: 算法等性能依赖应用领域案例分布，计算加权平均性能并分析平均过程很有意义。

Method: 采用概率框架，将性能视为概率测度。

Result: 加权平均是一种总结，部分显著得分使总结性能值等于领域特定性能值的加权算术平均，权重取决于用户偏好。

Conclusion: 严格定义四类领域，并为二分类任务开发新可视化工具。

Abstract: The performance of algorithms, methods, and models tends to depend heavily on the distribution of cases on which they are applied, this distribution being specific to the applicative domain. After performing an evaluation in several domains, it is highly informative to compute a (weighted) mean performance and, as shown in this paper, to scrutinize what happens during this averaging. To achieve this goal, we adopt a probabilistic framework and consider a performance as a probability measure (e.g., a normalized confusion matrix for a classification task). It appears that the corresponding weighted mean is known to be the summarization, and that only some remarkable scores assign to the summarized performance a value equal to a weighted arithmetic mean of the values assigned to the domain-specific performances. These scores include the family of ranking scores, a continuum parameterized by user preferences, and that the weights to consider in the arithmetic mean depend on the user preferences. Based on this, we rigorously define four domains, named easiest, most difficult, preponderant, and bottleneck domains, as functions of user preferences. After establishing the theory in a general setting, regardless of the task, we develop new visual tools for two-class classification.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [163] [CFD-copilot: leveraging domain-adapted large language model and model context protocol to enhance simulation automation](https://arxiv.org/abs/2512.07917)
*Zhehao Dong,Shanghai Du,Zhen Lu,Yue Yang*

Main category: cs.SE

TL;DR: 介绍CFD - copilot框架实现自然语言驱动CFD仿真自动化，评估显示该框架提升了工程工作流可靠性和效率。


<details>
  <summary>Details</summary>
Motivation: CFD仿真配置要求高阻碍非专家使用，将大语言模型应用于完整CFD工作流有挑战。

Method: 采用微调大语言模型将用户描述转化为可执行CFD设置，用多智能体系统集成相关操作，后处理用MCP标准。

Result: 在NACA 0012和30P - 30N翼型基准测试中，特定领域适配和MCP提升了可靠性和效率。

Conclusion: 领域特定适配和MCP结合能增强大语言模型驱动工程工作流的可靠性和效率。

Abstract: Configuring computational fluid dynamics (CFD) simulations requires significant expertise in physics modeling and numerical methods, posing a barrier to non-specialists. Although automating scientific tasks with large language models (LLMs) has attracted attention, applying them to the complete, end-to-end CFD workflow remains a challenge due to its stringent domain-specific requirements. We introduce CFD-copilot, a domain-specialized LLM framework designed to facilitate natural language-driven CFD simulation from setup to post-processing. The framework employs a fine-tuned LLM to directly translate user descriptions into executable CFD setups. A multi-agent system integrates the LLM with simulation execution, automatic error correction, and result analysis. For post-processing, the framework utilizes the model context protocol (MCP), an open standard that decouples LLM reasoning from external tool execution. This modular design allows the LLM to interact with numerous specialized post-processing functions through a unified and scalable interface, improving the automation of data extraction and analysis. The framework was evaluated on benchmarks including the NACA~0012 airfoil and the three-element 30P-30N airfoil. The results indicate that domain-specific adaptation and the incorporation of the MCP jointly enhance the reliability and efficiency of LLM-driven engineering workflows.

</details>


### [164] [DeepCode: Open Agentic Coding](https://arxiv.org/abs/2512.07921)
*Zongwei Li,Zhonghang Li,Zirui Guo,Xubin Ren,Chao Huang*

Main category: cs.SE

TL;DR: 论文介绍了DeepCode框架，解决了高保真文档到代码库合成的挑战，在评估中表现出色，为自主科学复现奠定基础。


<details>
  <summary>Details</summary>
Motivation: 现有方法在实现高保真文档到代码库合成方面存在挑战，主要因信息过载和大语言模型的上下文瓶颈。

Method: DeepCode框架将存储库合成视为信道优化问题，协调源压缩、结构化索引、条件知识注入和闭环纠错这四个信息操作。

Result: 在PaperBench基准测试中，DeepCode达到了最先进水平，超越了商业代码助手和顶尖机构的博士级人类专家。

Conclusion: 该工作为自主科学复现奠定了新基础，可加速研究评估和发现。

Abstract: Recent advances in large language models (LLMs) have given rise to powerful coding agents, making it possible for code assistants to evolve into code engineers. However, existing methods still face significant challenges in achieving high-fidelity document-to-codebase synthesis--such as scientific papers to code--primarily due to a fundamental conflict between information overload and the context bottlenecks of LLMs. In this work, we introduce DeepCode, a fully autonomous framework that fundamentally addresses this challenge through principled information-flow management. By treating repository synthesis as a channel optimization problem, DeepCode seamlessly orchestrates four information operations to maximize task-relevant signals under finite context budgets: source compression via blueprint distillation, structured indexing using stateful code memory, conditional knowledge injection via retrieval-augmented generation, and closed-loop error correction. Extensive evaluations on the PaperBench benchmark demonstrate that DeepCode achieves state-of-the-art performance, decisively outperforming leading commercial agents such as Cursor and Claude Code, and crucially, surpassing PhD-level human experts from top institutes on key reproduction metrics. By systematically transforming paper specifications into production-grade implementations comparable to human expert quality, this work establishes new foundations for autonomous scientific reproduction that can accelerate research evaluation and discovery.

</details>


### [165] [An Empirical Framework for Evaluating Semantic Preservation Using Hugging Face](https://arxiv.org/abs/2512.07983)
*Nan Jia,Anita Raja,Raffi Khatchadourian*

Main category: cs.SE

TL;DR: 本文介绍评估学习型软件系统语义保留的经验框架，构建数据集、评估管道并进行案例研究，推进机器学习系统的维护性和可信度。


<details>
  <summary>Details</summary>
Motivation: 机器学习成为高自主性系统一部分，其非确定性和运行时定义语义使传统软件重构变复杂，需确保学习型软件系统的可信度。

Method: 通过挖掘HuggingFace的模型演化数据，提取提交历史、模型卡片和性能指标，在三个领域进行案例研究。

Result: 可通过跨提交的评估指标检测语义漂移，分析出常见重构模式；构建了机器学习模型演化的大规模数据集、评估语义保留的实用管道，完成实证案例研究。

Conclusion: 这些贡献为更具可维护性和可信度的机器学习系统奠定了基础。

Abstract: As machine learning (ML) becomes an integral part of high-autonomy systems, it is critical to ensure the trustworthiness of learning-enabled software systems (LESS). Yet, the nondeterministic and run-time-defined semantics of ML complicate traditional software refactoring. We define semantic preservation in LESS as the property that optimizations of intelligent components do not alter the system's overall functional behavior. This paper introduces an empirical framework to evaluate semantic preservation in LESS by mining model evolution data from HuggingFace. We extract commit histories, $\textit{Model Cards}$, and performance metrics from a large number of models. To establish baselines, we conducted case studies in three domains, tracing performance changes across versions. Our analysis demonstrates how $\textit{semantic drift}$ can be detected via evaluation metrics across commits and reveals common refactoring patterns based on commit message analysis. Although API constraints limited the possibility of estimating a full-scale threshold, our pipeline offers a foundation for defining community-accepted boundaries for semantic preservation. Our contributions include: (1) a large-scale dataset of ML model evolution, curated from 1.7 million Hugging Face entries via a reproducible pipeline using the native HF hub API, (2) a practical pipeline for the evaluation of semantic preservation for a subset of 536 models and 4000+ metrics and (3) empirical case studies illustrating semantic drift in practice. Together, these contributions advance the foundations for more maintainable and trustworthy ML systems.

</details>


### [166] [Formally and Empirically Verified Methodologies for Scalable Hierarchical Full-Stack Systems](https://arxiv.org/abs/2510.00002)
*Dong Liu*

Main category: cs.SE

TL;DR: 本文介绍PBFD和PDFD两种全栈软件工程方法及TLE编码方案，验证其性能并展示工业可行性。


<details>
  <summary>Details</summary>
Motivation: 提出可扩展、工业级的全栈软件工程方法，解决软件结构化和行为正确性问题，并有效管理大规模分层数据。

Method: 用图论建模，使用CSP和LTL验证，提出TLE编码方案并用CSP验证。

Result: PBFD在企业部署八年零关键故障，开发速度、查询性能和存储上优于传统模型，开源实现验证关键属性。

Conclusion: PBFD和PDFD方法及TLE编码方案具有工业可行性和良好性能，相关资源公开可复现。

Abstract: This paper introduces Primary Breadth-First Development (PBFD) and Primary Depth-First Development (PDFD)-formally and empirically verified methodologies for scalable, industrial-grade full-stack software engineering. Both approaches enforce structural and behavioral correctness through graph-theoretic modeling, bridging formal methods and real-world practice. PBFD and PDFD model software development as layered directed graphs with unified state machines, verified using Communicating Sequential Processes (CSP) and Linear Temporal Logic (LTL). This guarantees bounded-refinement termination, deadlock freedom, and structural completeness. To manage hierarchical data at scale, we present the Three-Level Encapsulation (TLE)-a novel bitmask-based encoding scheme. TLE operations are verified via CSP failures-divergences refinement, ensuring constant-time updates and compact storage that underpin PBFD's robust performance. PBFD demonstrates exceptional industrial viability through eight years of enterprise deployment with zero critical failures, achieving approximately 20x faster develop-ment than Salesforce OmniScript, 7-8x faster query performance, and 11.7x storage reduction compared to conventional relational models. These results are established through longitudinal observational studies, quasi-experimental runtime comparisons, and controlled schema-level experiments. Open-source Minimum Viable Product implementations validate key behavioral properties, including bounded refinement and constant-time bitmask operations, un-der reproducible conditions. All implementations, formal specifications, and non-proprietary datasets are publicly available.

</details>


### [167] [A Gray Literature Study on Fairness Requirements in AI-enabled Software Engineering](https://arxiv.org/abs/2512.07990)
*Thanh Nguyen,Chaima Boufaied,Ronnie de Souza Santos*

Main category: cs.SE

TL;DR: 本文回顾灰色文献，探讨AI公平性要求定义、SDLC管理方式、违规原因与后果，强调将公平性融入AI软件的重要性。


<details>
  <summary>Details</summary>
Motivation: 当前AI应用中多关注模型有效性，公平性受关注较少，因此研究AI公平性要求。

Method: 对现有灰色文献进行回顾研究。

Result: 发现AI系统公平性要求定义多样，SDLC中管理实践不同，违规常与多种偏差相关，后果广泛。

Conclusion: 需要一致的框架和实践将公平性融入AI软件，给予公平性与有效性同等关注。

Abstract: Today, with the growing obsession with applying Artificial Intelligence (AI), particularly Machine Learning (ML), to software across various contexts, much of the focus has been on the effectiveness of AI models, often measured through common metrics such as F1- score, while fairness receives relatively little attention. This paper presents a review of existing gray literature, examining fairness requirements in AI context, with a focus on how they are defined across various application domains, managed throughout the Software Development Life Cycle (SDLC), and the causes, as well as the corresponding consequences of their violation by AI models. Our gray literature investigation shows various definitions of fairness requirements in AI systems, commonly emphasizing non-discrimination and equal treatment across different demographic and social attributes. Fairness requirement management practices vary across the SDLC, particularly in model training and bias mitigation, fairness monitoring and evaluation, and data handling practices. Fairness requirement violations are frequently linked, but not limited, to data representation bias, algorithmic and model design bias, human judgment, and evaluation and transparency gaps. The corresponding consequences include harm in a broad sense, encompassing specific professional and societal impacts as key examples, stereotype reinforcement, data and privacy risks, and loss of trust and legitimacy in AI-supported decisions. These findings emphasize the need for consistent frameworks and practices to integrate fairness into AI software, paying as much attention to fairness as to effectiveness.

</details>


### [168] [What Pulls the Strings? Understanding the Characteristics and Role of Argumentation in Open-Source Software Usability Discussions](https://arxiv.org/abs/2512.08032)
*Arghavan Sanei,Chaima Amiri,Atefeh Shokrizadeh,Jinghui Cheng*

Main category: cs.SE

TL;DR: 本文对五个开源软件项目中可用性讨论的论证话语和质量进行分析，发现讨论多由论证驱动但质量不一，为开源软件利益相关者提供见解。


<details>
  <summary>Details</summary>
Motivation: 开源软件可用性常被忽视，论证话语在可用性讨论中作用关键，但相关讨论中论证话语特征未知，难以为参与者提供有效支持。

Method: 对五个开源软件项目中的论证话语和质量进行全面分析。

Result: 可用性讨论多由论证驱动但质量不同；问题评论中的论证质量低于问题帖子，表明开源软件社区缺乏可用性方面的集体智慧；论证话语和质量对参与者后续行为有不同影响。

Conclusion: 本研究为开源软件利益相关者构建更有效论证、改善软件可用性提供见解，也可为其他分布式协作社区研究提供参考。

Abstract: The usability of open-source software (OSS) is important but frequently overlooked in favor of technical and functional complexity. Argumentation can be a pivotal device for diverse stakeholders in OSS usability discussions to express opinions and persuade others. However, the characteristics of argument discourse in those discussions remain unknown, resulting in difficulties in providing effective support for discussion participants. We address this through a comprehensive analysis of argument discourse and quality in five OSS projects. Our results indicated that usability discussions are predominantly argument-driven, although their qualities vary. Issue comments exhibit lower-quality arguments than the issue posts, suggesting a shortage of collective intelligence about usability in OSS communities. Moreover, argument discourse and quality have various impacts on the subsequent behavior of participants. Overall, this research offers insights to help OSS stakeholders build more effective arguments and eventually improve OSS usability. These insights can also inform studies about other distributed collaborative communities.

</details>


### [169] [Secure or Suspect? Investigating Package Hallucinations of Shell Command in Original and Quantized LLMs](https://arxiv.org/abs/2512.08213)
*Md Nazmul Haque,Elizabeth Lin,Lawrence Arkoh,Biruk Tadesse,Bowen Xu*

Main category: cs.SE

TL;DR: 该论文首次系统研究量化对大语言模型生成Go包时的包幻觉和漏洞风险的影响，发现量化会大幅增加包幻觉率，低精度模型安全风险更高，还揭示了包幻觉的模式。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在生成软件依赖时存在包幻觉和安全漏洞问题，而量化虽能降低推理成本，但量化对生成的软件依赖正确性和安全性的影响尚不明确。

Method: 对五个Qwen模型大小在全精度、8位和4位量化下，在三个数据集进行评估。

Result: 量化大幅增加包幻觉率，4位模型退化最严重；正确生成的包中，低精度模型的漏洞存在率上升；幻觉输出中的伪造包多类似基于URL的Go模块路径，有系统性模式。

Conclusion: 研究结果为部署量化大语言模型进行代码生成和依赖推荐的可靠性和安全性提供了可行见解。

Abstract: Large Language Models for code (LLMs4Code) are increasingly used to generate software artifacts, including library and package recommendations in languages such as Go. However, recent evidence shows that LLMs frequently hallucinate package names or generate dependencies containing known security vulnerabilities, posing significant risks to developers and downstream software supply chains. At the same time, quantization has become a widely adopted technique to reduce inference cost and enable deployment of LLMs on resource-constrained environments. Despite its popularity, little is known about how quantization affects the correctness and security of LLM-generated software dependencies while generating shell commands for package installation.
  In this work, we conduct the first systematic empirical study of the impact of quantization on package hallucination and vulnerability risks in LLM-generated Go packages. We evaluate five Qwen model sizes under full-precision, 8-bit, and 4-bit quantization across three datasets (SO, MBPP, and paraphrase). Our results show that quantization substantially increases the package hallucination rate (PHR), with 4-bit models exhibiting the most severe degradation. We further find that even among the correctly generated packages, the vulnerability presence rate (VPR) rises as precision decreases, indicating elevated security risk in lower-precision models. Finally, our analysis of hallucinated outputs reveals that most fabricated packages resemble realistic URL-based Go module paths, such as most commonly malformed or non-existent GitHub and golang.org repositories, highlighting a systematic pattern in how LLMs hallucinate dependencies. Overall, our findings provide actionable insights into the reliability and security implications of deploying quantized LLMs for code generation and dependency recommendation.

</details>


### [170] [Measuring Computer Science Enthusiasm: A Questionnaire-Based Analysis of Age and Gender Effects on Students' Interest](https://arxiv.org/abs/2512.08472)
*Kai Marquardt,Robert Hanak,Anne Koziolek,Lucia Happe*

Main category: cs.SE

TL;DR: 研究通过分析青少年样本，揭示年龄和性别对学生计算机科学兴趣的影响，发现年龄比性别更关键，强调计算机科学教育需动态、对年龄敏感的框架。


<details>
  <summary>Details</summary>
Motivation: 深入了解学生对计算机科学教育的兴趣，评估短期推广活动，挑战早期接触是培养持续兴趣主要途径的观点。

Method: 基于兴趣的人 - 物理论开发问卷，对参与在线计算机科学课程的400多名学生进行前后测，分析年龄和性别相关模式。

Result: 早期青少年热情下降，尤其是女孩；年龄比性别对兴趣发展影响更大；年长学生干预后积极变化最大。

Conclusion: 计算机科学教育需要动态、对年龄敏感的框架，教学策略应与发展轨迹相匹配。

Abstract: This study offers new insights into students' interest in computer science (CS) education by disentangling the distinct effects of age and gender across a diverse adolescent sample. Grounded in the person-object theory of interest (POI), we conceptualize enthusiasm as a short-term, activating expression of interest that combines positive affect, perceived relevance, and intention to re-engage. Experiencing such enthusiasm can temporarily shift CS attitudes and strengthen future engagement intentions, making it a valuable lens for evaluating brief outreach activities. To capture these dynamics, we developed a theoretically grounded questionnaire for pre-post assessment of the enthusiasm potential of CS interventions. Using data from more than 400 students participating in online CS courses, we examined age- and gender-related patterns in enthusiasm. The findings challenge the prevailing belief that early exposure is the primary pathway to sustained interest in CS. Instead, we identify a marked decline in enthusiasm during early adolescence, particularly among girls, alongside substantial variability in interest trajectories across age groups. Crucially, our analyses reveal that age is a more decisive factor than gender in shaping interest development and uncover key developmental breakpoints. Despite starting with lower baseline attitudes, older students showed the largest positive changes following the intervention, suggesting that well-designed short activities can effectively re-activate interest even at later ages. Overall, the study highlights the need for a dynamic, age-sensitive framework for CS education in which instructional strategies are aligned with developmental trajectories.

</details>


### [171] [Migrating QAOA from Qiskit 1.x to 2.x: An experience report](https://arxiv.org/abs/2512.08245)
*Julien Cardinal,Imen Benzarti,Ghizlane El boussaidi,Christophe Pere*

Main category: cs.SE

TL;DR: 本文讲述将QAOA算法从Qiskit 1.x迁移到2.x时结果差异大，根源是采样预算，增加采样数可恢复精度，并给出相关建议。


<details>
  <summary>Details</summary>
Motivation: 解决量子算法跨框架迁移时出现的影响准确性和可重复性的行为变化问题。

Method: 将QAOA算法从Qiskit 1.x迁移到2.x，进行系统分析找出问题根源。

Result: 发现问题根源是采样预算，v2默认10000次采样仅覆盖23%状态空间，增加到250000次可恢复精度。

Conclusion: 量子 - 经典交互层面的隐藏参数会主导混合算法性能，为开发者和框架设计者提供确保量子软件迁移可重复性结果的建议。

Abstract: Migrating quantum algorithms across evolving frameworks introduces subtle behavioral changes that affect accuracy and reproducibility. This paper reports our experience converting the Quantum Approximate Optimization Algorithm (QAOA) from Qiskit Algorithms with Qiskit 1.x (v1 primitives) to a custom implementation using Qiskit 2.x (v2 primitives). Despite identical circuits, optimizers, and Hamiltonians, the new version produced drastically different results. A systematic analysis revealed the root cause: the sampling budget -- the number of circuit executions (shots) per iteration. The library's implicit use of unlimited shots yielded dense probability distributions, whereas the v2 default of 10 000 shots captured only 23% of the state space. Increasing shots to 250 000 restored library-level accuracy. This study highlights how hidden parameters at the quantum-classical interaction level can dominate hybrid algorithm performance and provides actionable recommendations for developers and framework designers to ensure reproducible results in quantum software migration.

</details>


### [172] [Token Sugar: Making Source Code Sweeter for LLMs through Token-Efficient Shorthand](https://arxiv.org/abs/2512.08266)
*Zhensu Sun,Chengran Yang,Xiaoning Du,Zhou Yang,Li Li,David Lo*

Main category: cs.SE

TL;DR: 提出Token Sugar概念，可在代码中用高效简写替换冗长代码模式，减少代码token数量，训练模型能节省token且保持性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在代码任务中计算成本高，现有方法局限于语法转换，语义层面有token缩减空间。

Method: 提出Token Sugar概念，从代码语料中挖掘高频、高token模式并映射为唯一简写，通过代码转换集成到LLM预训练。

Result: 获得799对（代码模式，简写），源代码token最多减少15.1%，训练模型生成时token最多减少11.2%，Pass@1分数与基线相近。

Conclusion: Token Sugar方法能有效减少代码token数量，且不影响模型性能，与现有语法方法互补。

Abstract: Large language models (LLMs) have shown exceptional performance in code generation and understanding tasks, yet their high computational costs hinder broader adoption. One important factor is the inherent verbosity of programming languages, such as unnecessary formatting elements and lengthy boilerplate code. This leads to inflated token counts in both input and generated outputs, which increases inference costs and slows down the generation process. Prior work improves this through simplifying programming language grammar, reducing token usage across both code understanding and generation tasks. However, it is confined to syntactic transformations, leaving significant opportunities for token reduction unrealized at the semantic level.
  In this work, we propose Token Sugar, a concept that replaces frequent and verbose code patterns with reversible, token-efficient shorthand in the source code. To realize this concept in practice, we designed a systematic solution that mines high-frequency, token-heavy patterns from a code corpus, maps each to a unique shorthand, and integrates them into LLM pretraining via code transformation. With this solution, we obtain 799 (code pattern, shorthand) pairs, which can reduce up to 15.1% token count in the source code and is complementary to existing syntax-focused methods. We further trained three widely used LLMs on Token Sugar-augmented data. Experimental results show that these models not only achieve significant token savings (up to 11.2% reduction) during generation but also maintain near-identical Pass@1 scores compared to baselines trained on unprocessed code.

</details>


### [173] [FedLAD: A Modular and Adaptive Testbed for Federated Log Anomaly Detection](https://arxiv.org/abs/2512.08277)
*Yihan Liao,Jacky Keung,Zhenyu Mao,Jingyu Zhang,Jialong Li*

Main category: cs.SE

TL;DR: 提出FedLAD平台用于解决联邦环境下日志异常检测缺乏专用测试床的问题，支持可重复和可扩展实验


<details>
  <summary>Details</summary>
Motivation: 现有日志异常检测方法多为集中式训练，因隐私和日志分散不实用，联邦学习缺乏日志异常检测专用测试床

Method: 提出FedLAD统一平台，支持不同模型、数据集和聚合策略的插拔式集成，提供运行时验证日志、参数调优和自适应策略控制等支持

Result: FedLAD可实现可重复和可扩展的实验

Conclusion: FedLAD弥合了联邦学习框架和日志异常检测需求之间的差距，为未来研究提供基础，代码公开

Abstract: Log-based anomaly detection (LAD) is critical for ensuring the reliability of large-scale distributed systems. However, most existing LAD approaches assume centralized training, which is often impractical due to privacy constraints and the decentralized nature of system logs. While federated learning (FL) offers a promising alternative, there is a lack of dedicated testbeds tailored to the needs of LAD in federated settings. To address this, we present FedLAD, a unified platform for training and evaluating LAD models under FL constraints. FedLAD supports plug-and-play integration of diverse LAD models, benchmark datasets, and aggregation strategies, while offering runtime support for validation logging (self-monitoring), parameter tuning (self-configuration), and adaptive strategy control (self-adaptation). By enabling reproducible and scalable experimentation, FedLAD bridges the gap between FL frameworks and LAD requirements, providing a solid foundation for future research. Project code is publicly available at: https://github.com/AA-cityu/FedLAD.

</details>


### [174] [Empowering smart app development with SolidGPT: an edge-cloud hybrid AI agent framework](https://arxiv.org/abs/2512.08286)
*Liao Hu,Qiteng Wu,Ruoyu Qi*

Main category: cs.SE

TL;DR: 论文介绍开源边缘云混合开发者助手SolidGPT，可增强代码和工作区语义搜索，提升开发者生产力并保障数据隐私。


<details>
  <summary>Details</summary>
Motivation: 大语言模型集成到移动和软件开发工作流时，语义感知、开发者生产力和数据隐私三者存在矛盾，传统云工具和端上解决方案各有不足。

Method: 构建开源的边缘云混合开发者助手SolidGPT，实现与代码库交互查询、自动化软件项目工作流、配置私有可扩展代理等功能。

Result: SolidGPT实现了语义丰富的代码导航、集成文档和任务管理、隐私优先设计，提升了开发者生产力。

Conclusion: SolidGPT结合交互代码查询、自动化项目搭建和人机协作，是实用且注重隐私的边缘助手，适用于智能移动和软件工程场景。

Abstract: The integration of Large Language Models (LLMs) into mobile and software development workflows faces a persistent tension among three demands: semantic awareness, developer productivity, and data privacy. Traditional cloud-based tools offer strong reasoning but risk data exposure and latency, while on-device solutions lack full-context understanding across codebase and developer tooling. We introduce SolidGPT, an open-source, edge-cloud hybrid developer assistant built on GitHub, designed to enhance code and workspace semantic search. SolidGPT enables developers to: talk to your codebase: interactively query code and project structure, discovering the right methods and modules without manual searching. Automate software project workflows: generate PRDs, task breakdowns, Kanban boards, and even scaffold web app beginnings, with deep integration via VSCode and Notion. Configure private, extensible agents: onboard private code folders (up to approximately 500 files), connect Notion, customize AI agent personas via embedding and in-context training, and deploy via Docker, CLI, or VSCode extension. In practice, SolidGPT empowers developer productivity through: Semantic-rich code navigation: no more hunting through files or wondering where a feature lives. Integrated documentation and task management: seamlessly sync generated PRD content and task boards into developer workflows. Privacy-first design: running locally via Docker or VSCode, with full control over code and data, while optionally reaching out to LLM APIs as needed. By combining interactive code querying, automated project scaffolding, and human-AI collaboration, SolidGPT provides a practical, privacy-respecting edge assistant that accelerates real-world development workflows, ideal for intelligent mobile and software engineering contexts.

</details>


### [175] [Measuring Agile Agreement: Development and Validation of the Manifesto and Principle Scales](https://arxiv.org/abs/2512.08461)
*Nicolas Matton,Anthony Simonofski,Marie-Ange Remiche,Benoît Vanderose*

Main category: cs.SE

TL;DR: 现有研究未区分对敏捷宣言价值观和实践的认同度，本文设计验证 MAS 和 PAS 量表，结果显示量表有效且各有侧重，为精准衡量敏捷认同度奠定基础。


<details>
  <summary>Details</summary>
Motivation: 现有研究未能区分对敏捷宣言抽象价值观和具体实践的认同，导致个人“敏捷认同”的测量缺乏明确方法。

Method: 设计并验证两个量表 MAS 和 PAS，详细介绍条目创建、选择、调查设计和验证的系统过程，运用比例优势逻辑回归、Bland - Altman 图和组内相关系数（ICC）进行收敛和差异分析。

Result: 两个量表都有良好的内部一致性和结构效度，虽有中度相关性但不可互换，能捕捉敏捷认同的不同维度。

Conclusion: 提供的两个公开量表是精准测量敏捷认同度的关键开端，可区分不同层面的敏捷认同，助于更准确解读个人与敏捷的适配度。

Abstract: While the importance of human factors in agile software development is widely acknowledged, the measurement of an individual's "agile agreement" remains an ill-defined and challenging area. A key limitation in existing research is the failure to distinguish between agreement with the abstract, high-level values of the Agile Manifesto and agreement with the concrete, day-to-day practices derived from the 12 Principles. This paper addresses this methodological gap by presenting the design and validation of two distinct instruments: the novel Manifesto Agreement Scale (MAS), and the Principle Agreement Scale (PAS), which is a systematic adaptation and refinement of a prior instrument.
  We detail the systematic process of item creation and selection, survey design, and validation. The results demonstrate that both scales possess important internal consistency and construct validity. A convergence and divergence analysis, including Proportional Odds Logistic Regression, a Bland-Altman plot, and an Intraclass Correlation Coefficient (ICC), reveals that while the two scales are moderately correlated, they are not interchangeable and capture distinct dimensions of agile agreement. The primary contribution of this work is a pair of publicly available instruments, validated within a specific demographic of Belgian IT professionals. These scales represent a critical initial step toward facilitating a more nuanced measurement of agile agreement, distinguishing agile agreement across various levels of perception and aiding in a more refined interpretation of person-agile fit.

</details>


### [176] [Gamification with Purpose: What Learners Prefer to Motivate Their Learning](https://arxiv.org/abs/2512.08551)
*Kai Marquardt,Mona Schulz,Anne Koziolek,Lucia Happe*

Main category: cs.SE

TL;DR: 研究学习者对教育情境中游戏设计元素的偏好，以制定有针对性的游戏化策略，发现学习者偏好与教育内容有意义结合并支持内在动机的元素。


<details>
  <summary>Details</summary>
Motivation: 研究学习者对教育情境中游戏设计元素的偏好，为有目的的游戏化策略开发提供依据，同时减轻如内在动机削弱等风险。

Method: 进行系统文献综述确定十个广泛讨论的游戏设计元素，开发视觉原型，开展包含125名参与者的最佳 - 最差缩放调查获取偏好排名，并收集定性反馈。

Result: 学习者始终偏好直接支持学习过程的游戏设计元素，如进度条、概念图等；定性分析发现六个反复出现的激励主题。

Conclusion: 学习者重视与教育内容有意义结合并支持内在动机的游戏化元素，有目的的游戏化应优先考虑可视化学习进度和提供可行反馈的工具，而非仅依赖外在激励。

Abstract: This study investigates learners' preferences for game design elements (GDEs) in educational contexts to inform the development of purpose-driven gamification strategies. It emphasizes a learner-centered approach that aligns gamification design with pedagogical goals, while mitigating risks such as the erosion of intrinsic motivation. A systematic literature review was conducted to identify ten widely discussed GDEs. Visual prototypes representing each element were developed, and a best-worst scaling (BWS) survey with 125 participants was administered to elicit preference rankings. Qualitative feedback was also collected to uncover motivational drivers. Learners consistently preferred GDEs that support learning processes directly-most notably progress bars, concept maps, immediate feedback, and achievements. Qualitative analysis revealed six recurring motivational themes, including visible progress, content relevance, and constructive feedback. The findings suggest that learners value gamification elements that are meaningfully integrated with educational content and support intrinsic motivation. Purpose-aligned gamification should prioritize tools that visualize learning progress and provide actionable feedback, rather than relying solely on extrinsic incentives.

</details>


### [177] [Reusability in MLOps: Leveraging Ports and Adapters to Build a Microservices Architecture for the Maritime Domain](https://arxiv.org/abs/2512.08657)
*Renato Cordeiro Ferreira,Aditya Dhinavahi,Rowanne Trapmann,Willem-Jan van den Heuvel*

Main category: cs.SE

TL;DR: 介绍构建海事领域异常检测MLES Ocean Guard时的软件架构复用技术，鼓励使用六边形架构模式


<details>
  <summary>Details</summary>
Motivation: MLES复杂，需探索软件架构复用技术以实现业务目标

Method: 重用Ports and Adapters模式，从单一代码库构建多个微服务

Result: 报告展示了复用技术，突出了挑战和经验教训

Conclusion: 鼓励软件、机器学习工程师和数据科学家应用六边形架构模式构建MLES

Abstract: ML-Enabled Systems (MLES) are inherently complex since they require multiple components to achieve their business goal. This experience report showcases the software architecture reusability techniques applied while building Ocean Guard, an MLES for anomaly detection in the maritime domain. In particular, it highlights the challenges and lessons learned to reuse the Ports and Adapters pattern to support building multiple microservices from a single codebase. This experience report hopes to inspire software engineers, machine learning engineers, and data scientists to apply the Hexagonal Architecture pattern to build their MLES.

</details>


### [178] [RESTifAI: LLM-Based Workflow for Reusable REST API Testing](https://arxiv.org/abs/2512.08706)
*Leon Kogler,Maximilian Ehrhart,Benedikt Dornauer,Eduard Paul Enoiu*

Main category: cs.SE

TL;DR: 介绍了RESTifAI，一种基于大语言模型生成REST API测试的方法，与现有工具对比有优势，给出相应结果和工具地址。


<details>
  <summary>Details</summary>
Motivation: 当前工具存在专注内部服务器错误、功能的局限性，RESTifAI要构造有效测试场景、验证预期功能和鲁棒性。

Method: 采用happy - path方法，系统构建有效测试场景并推导负例。

Result: RESTifAI 性能与最新的大语言模型工具相当，解决了可复用性、预言复杂度和集成方面的限制。

Conclusion: RESTifAI有良好表现，适用于工业服务，工具公开可获取。

Abstract: With this paper, we introduce RESTifAI, an LLM-driven approach for generating reusable, CI/CD ready REST API tests, following the happy-path approach. Unlike existing tools that often focus primarily on internal server errors, RESTifAI systematically constructs valid test scenarios (happy paths) and derives negative cases to verify both intended functionality (2xx responses) and robustness against invalid inputs or business-rule violations (4xx responses). The results indicate that RESTifAI performs on par with the latest LLM tools, i.e., AutoRestTest and LogiAgent, while addressing limitations related to reusability, oracle complexity, and integration. To support this, we provide common comparative results and demonstrate the tool's applicability in industrial services. For tool demonstration, please refer to https://www.youtube.com/watch?v=2vtQo0T0Lo4. RESTifAI is publicly available at https://github.com/casablancahotelsoftware/RESTifAI.

</details>


### [179] [Multicalibration for LLM-based Code Generation](https://arxiv.org/abs/2512.08810)
*Viola Campos,Robin Kuschnereit,Adrian Ulges*

Main category: cs.SE

TL;DR: 研究代码大语言模型校准，借助多校准方法，结果显示多校准能改进校准效果并公开数据集。


<details>
  <summary>Details</summary>
Motivation: 随着基于AI的代码生成普及，需要确保代码大语言模型的置信度分数能忠实反映代码正确的真实可能性。

Method: 研究四种多校准方法，在三个函数合成基准测试中使用最新一代代码大语言模型（Qwen3 Coder、GPT-OSS、DeepSeek - R1 - Distill）。

Result: 多校准相对于未校准的令牌似然性（技能得分提高1.03）和基线校准（技能得分提高0.37）有显著改进。

Conclusion: 多校准能改善代码大语言模型校准效果，并公开数据集供未来研究。

Abstract: As AI-based code generation becomes widespread, researchers are investigating the calibration of code LLMs - ensuring their confidence scores faithfully represent the true likelihood of code correctness. To do so, we investigate multicalibration, which can capture additional factors about a coding problem, such as complexity, code length, or programming language used. We study four multicalibration approaches on three function synthesis benchmarks, using latest-generation code LLMs (Qwen3 Coder, GPT-OSS, DeepSeek-R1-Distill). Our results demonstrate that multicalibration can yield distinct improvements over both uncalibrated token likelihoods (+1.03 in skill score) and baseline calibrations (+0.37 in skill score). We study the influence of the aforementioned factors in ablations, and make our dataset (consisting of code generations, likelihoods, and correctness labels) available for future research on code LLM calibration.

</details>


### [180] [SimpleDevQA: Benchmarking Large Language Models on Development Knowledge QA](https://arxiv.org/abs/2512.08867)
*Jing Zhang,Lianghong Guo,Yanlin Wang,Mingwei Liu,Jiachi Chen,Yuchi Ma,Ensheng Shi,Terry Yue Zhuo,Hongyu Zhang,Zibin Zheng*

Main category: cs.SE

TL;DR: 分析真实用户与大语言模型对话，发现开发知识问答任务重要但现有基准有局限，设计三阶段流程构建多语言基准SimpleDevQA并实验得出相关结论。


<details>
  <summary>Details</summary>
Motivation: 研究开发知识问答任务的重要性和探索程度，弥补现有基准评估大语言模型该能力的不足。

Method: 分析WildChat的真实用户-LLM对话，设计三阶段流程将真实对话转化为问答对构建SimpleDevQA基准，进行实验。

Result: 开发知识问答任务交互占比高；现有基准有局限；SimpleDevQA含2740个三语言问答对；代码LLM表现更好；RAG策略提升准确率；LLM有过度自信情况且准确率与自信度正相关；代码生成能力强的LLM开发知识问答表现好。

Conclusion: 开发知识问答任务重要，SimpleDevQA基准能准确简单评估，代码LLM和RAG策略在该任务中有优势。

Abstract: The Development Knowledge Question Answering (Dev Knowledge QA) task aims to provide natural language answers to knowledge-seeking questions during software development. To investigate its importance and to what extent it has been explored, we analyze real user-LLM dialogues from WildChat and find that: (1) The Dev Knowledge QA task accounts for 39.6% of interactions(highest among all tasks), revealing broad knowledge needs beyond code generation (32.3%). (2) Only 27.5% of real Dev Knowledge QA dialogues focus on code understanding, leaving out development knowledge-seeking. (3) Only 17.1% of real-world Dev Knowledge QA dialogues can be used for constructing a benchmark. Existing benchmarks have two primary limitations for evaluating the Dev Knowledge QA capability of LLMs. First, existing benchmarks offer a limited development knowledge scope, mainly focusing on code understanding and neglecting broader knowledge during development. Second, some benchmarks are not built from real user queries. To bridge this gap, we design a three-phase pipeline that transforms real-world dialogue into simple development knowledge-seeking QA pairs. Through this pipeline, we introduce SimpleDevQA, a multilingual benchmark derived from real user dialogues. It contains 2,740 QA pairs in three languages (English, Chinese, and Russian), and focuses on questions with unique, short, and verifiable answers for accurate and simple evaluation. Experiments show that: Code LLMs generally outperform general LLMs of similar scale; Knowledge injection with the Retrieval-Augmented Generation (RAG) strategy can boost LLM accuracy by 11.3% on average; LLMs show systematic overconfidence in Dev Knowledge QA, and the answering accuracy of LLMs shows a positive correlation with their stated confidence; Generally, LLMs with stronger code generation performance also exhibit stronger performance in Dev Knowledge QA.

</details>


### [181] [Exploring the Garden of Forking Paths in Empirical Software Engineering Research: A Multiverse Analysis](https://arxiv.org/abs/2512.08910)
*Nathan Cassee,Robert Feldt*

Main category: cs.SE

TL;DR: 对实证软件工程论文进行多宇宙分析，发现多数分析结果与原结果不同，倡导研究者进行稳健性检查并提出分类模型。


<details>
  <summary>Details</summary>
Motivation: 实证软件工程研究中分析决策的自由虽有优势，但威胁结果的稳健性和可重复性，需理解此风险。

Method: 对一篇已发表的实证软件工程论文进行多宇宙分析，确定九个关键分析决策，在原始数据集上运行所有 3072 个分析管道。

Result: 仅 6 个（<0.2%）宇宙重现了发表结果，绝大多数产生不同甚至相反的发现。

Conclusion: 倡导研究者进行稳健性检查或明确解释每个分析决策，提出分类模型，表明多宇宙分析是实用工具。

Abstract: In empirical software engineering (SE) research, researchers have considerable freedom to decide how to process data, what operationalizations to use, and which statistical model to fit. Gelman and Loken refer to this freedom as leading to a "garden of forking paths". Although this freedom is often seen as an advantage, it also poses a threat to robustness and replicability: variations in analytical decisions, even when justifiable, can lead to divergent conclusions.
  To better understand this risk, we conducted a so-called multiverse analysis on a published empirical SE paper. The paper we picked is a Mining Software Repositories study, as MSR studies commonly use non-trivial statistical models to analyze post-hoc, observational data. In the study, we identified nine pivotal analytical decisions-each with at least one equally defensible alternative and systematically reran all the 3,072 resulting analysis pipelines on the original dataset. Interestingly, only 6 of these universes (<0.2%) reproduced the published results; the overwhelming majority produced qualitatively different, and sometimes even opposite, findings.
  This case study of a data analytical method commonly applied to empirical software engineering data reveals how methodological choices can exert a more profound influence on outcomes than is often acknowledged. We therefore advocate that SE researchers complement standard reporting with robustness checks across plausible analysis variants or, at least, explicitly justify each analytical decision. We propose a structured classification model to help classify and improve justification for methodological choices. Secondly, we show how the multiverse analysis is a practical tool in the methodological arsenal of SE researchers, one that can help produce more reliable, reproducible science.

</details>


<div id='q-fin.RM'></div>

# q-fin.RM [[Back]](#toc)

### [182] [LLM-Generated Counterfactual Stress Scenarios for Portfolio Risk Simulation via Hybrid Prompt-RAG Pipeline](https://arxiv.org/abs/2512.07867)
*Masoud Soleimani*

Main category: q-fin.RM

TL;DR: 开发基于大语言模型的宏观金融压力测试管道，生成宏观经济情景，结果显示其可作为传统压力测试框架的补充。


<details>
  <summary>Details</summary>
Motivation: 构建透明且可审计的宏观金融压力测试方法。

Method: 结合结构化提示与可选的国家基本面和新闻检索，通过因子映射将情景转化为投资组合损失。

Result: 大语言模型生成连贯且特定国家的压力叙事，风险变化主要由投资组合构成和提示设计驱动。

Conclusion: 大语言模型生成的宏观情景结合透明结构和严格验证，可作为传统压力测试框架的可扩展且可解释的补充。

Abstract: We develop a transparent and fully auditable LLM-based pipeline for macro-financial stress testing, combining structured prompting with optional retrieval of country fundamentals and news. The system generates machine-readable macroeconomic scenarios for the G7, which cover GDP growth, inflation, and policy rates, and are translated into portfolio losses through a factor-based mapping that enables Value-at-Risk and Expected Shortfall assessment relative to classical econometric baselines. Across models, countries, and retrieval settings, the LLMs produce coherent and country-specific stress narratives, yielding stable tail-risk amplification with limited sensitivity to retrieval choices. Comprehensive plausibility checks, scenario diagnostics, and ANOVA-based variance decomposition show that risk variation is driven primarily by portfolio composition and prompt design rather than by the retrieval mechanism. The pipeline incorporates snapshotting, deterministic modes, and hash-verified artifacts to ensure reproducibility and auditability. Overall, the results demonstrate that LLM-generated macro scenarios, when paired with transparent structure and rigorous validation, can provide a scalable and interpretable complement to traditional stress-testing frameworks.

</details>


### [183] [A New Application of Hoeffding's Inequality Can Give Traders Early Warning of Financial Regime Change](https://arxiv.org/abs/2512.08851)
*Daniel Egger,Jacob Vestal*

Main category: q-fin.RM

TL;DR: 利用Hoeffding不等式，以交易策略表现为随机变量，通过其与期望偏差的概率变化预警金融市场制度变化。


<details>
  <summary>Details</summary>
Motivation: 寻找金融市场制度变化的早期预警指标。

Method: 以交易策略表现为随机变量，应用Hoeffding不等式计算其与期望偏差的概率。

Result: 得出较大偏差对应金融制度持续概率降低、变化概率升高。

Conclusion: Hoeffding概率变化可作为金融制度变化的早期预警指标。

Abstract: Hoeffding's Inequality provides the maximum probability that a series of n draws from a bounded random variable differ from the variable's true expectation u by more than given tolerance t. The random variable is typically the error rate of a classifier in machine learning applications. Here, a trading strategy is premised on the assumption of an underlying distribution of causal factors, in other words, a market regime, and the random variable is the performance of that trading strategy. A larger deviation of observed performance from the trader's expectation u can be characterized as a lower probability that the financial regime supporting that strategy remains in force, and a higher probability of financial regime change. The changing Hoeffding probabilities can be used as an early warning indicator of this change.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [184] [Integrating LSTM Networks with Neural Levy Processes for Financial Forecasting](https://arxiv.org/abs/2512.07860)
*Mohammed Alruqimi,Luca Di Persio*

Main category: q-fin.ST

TL;DR: 研究深度学习与金融模型的最优集成用于资产价格预测，开发混合框架，经实验验证结合GWO优化的LSTM和ANN校准的Levy - Merton模型表现最佳。


<details>
  <summary>Details</summary>
Motivation: 实现深度学习与金融模型的最优集成，以进行稳健的资产价格预测。

Method: 开发结合LSTM网络与Merton - Lévy跳跃扩散模型的混合框架，用GWO优化LSTM超参数，探索ANN、MPA和TorchSDE库三种校准Merton - Levy模型参数的方法，与多个基准模型对比评估。

Result: 结合GWO优化的LSTM网络与用ANN校准的Levy - Merton跳跃扩散模型的混合模型，在三个真实金融数据集上的表现优于基础LSTM模型及本研究开发的其他模型。

Conclusion: 所开发的结合GWO优化的LSTM和ANN校准的Levy - Merton模型在资产价格预测方面具有优越性。

Abstract: This paper investigates an optimal integration of deep learning with financial models for robust asset price forecasting. Specifically, we developed a hybrid framework combining a Long Short-Term Memory (LSTM) network with the Merton-Lévy jump-diffusion model. To optimise this framework, we employed the Grey Wolf Optimizer (GWO) for the LSTM hyperparameter tuning, and we explored three calibration methods for the Merton-Levy model parameters: Artificial Neural Networks (ANNs), the Marine Predators Algorithm (MPA), and the PyTorch-based TorchSDE library. To evaluate the predictive performance of our hybrid model, we compared it against several benchmark models, including a standard LSTM and an LSTM combined with the Fractional Heston model. This evaluation used three real-world financial datasets: Brent oil prices, the STOXX 600 index, and the IT40 index. Performance was assessed using standard metrics, including Mean Squared Error (MSE), Mean Absolute Error(MAE), Mean Squared Percentage Error (MSPE), and the coefficient of determination (R2). Our experimental results demonstrate that the hybrid model, combining a GWO-optimized LSTM network with the Levy-Merton Jump-Diffusion model calibrated using an ANN, outperformed the base LSTM model and all other models developed in this study.

</details>


### [185] [The Endogenous Constraint: Hysteresis, Stagflation, and the Structural Inhibition of Monetary Velocity in the Bitcoin Network (2016-2025)](https://arxiv.org/abs/2512.07886)
*Hamoon Soleimani*

Main category: q-fin.ST

TL;DR: 本文验证内生约束假说，分析比特币网络中摩擦与货币流通速度关系，发现网络效用分叉、速度约束为制度转换现象及加密乘数反转。


<details>
  <summary>Details</summary>
Motivation: 验证内生约束假说，探究比特币协议级吞吐量限制在网络摩擦和基础层货币流通速度间产生的非线性负反馈循环。

Method: 使用基于Blockchain.com链上数据的交易成本指数（TCI）和Hansen的阈值回归，进行工具变量（IV）测试。

Result: 确定摩擦第90百分位处有结构突变；正常和冲击制度下网络效用分叉；速度约束是制度转换现象；高摩擦使资本集中度增加。

Conclusion: 比特币网络中速度约束是制度转换现象，高摩擦会导致资本集中和投机性囤积。

Abstract: Bitcoin operates as a macroeconomic paradox: it combines a strictly predetermined, inelastic monetary issuance schedule with a stochastic, highly elastic demand for scarce block space. This paper empirically validates the Endogenous Constraint Hypothesis, positing that protocol-level throughput limits generate a non-linear negative feedback loop between network friction and base-layer monetary velocity. Using a verified Transaction Cost Index (TCI) derived from Blockchain.com on-chain data and Hansen's (2000) threshold regression, we identify a definitive structural break at the 90th percentile of friction (TCI ~ 1.63). The analysis reveals a bifurcation in network utility: while the network exhibits robust velocity growth of +15.44% during normal regimes, this collapses to +6.06% during shock regimes, yielding a statistically significant Net Utility Contraction of -9.39% (p = 0.012). Crucially, Instrumental Variable (IV) tests utilizing Hashrate Variation as a supply-side instrument fail to detect a significant relationship in a linear specification (p=0.196), confirming that the velocity constraint is strictly a regime-switching phenomenon rather than a continuous linear function. Furthermore, we document a "Crypto Multiplier" inversion: high friction correlates with a +8.03% increase in capital concentration per entity, suggesting that congestion forces a substitution from active velocity to speculative hoarding.

</details>


### [186] [Analysis of Contagion in China's Stock Market: A Hawkes Process Perspective](https://arxiv.org/abs/2512.08000)
*Junwei Yang*

Main category: q-fin.ST

TL;DR: 研究用Hawkes过程分析中国股市传染效应，发现高交易活动时板块指数趋势持续，低活动时板块轮动强。


<details>
  <summary>Details</summary>
Motivation: 探究中国股市的传染效应，分析市场指数是否有趋势行为以及板块指数间的相互影响。

Method: 对上证指数、深证成指、创业板指及板块指数的日收益率拟合自激和抑制Hawkes过程。

Result: 高交易活动时板块指数趋势持续，低活动时板块轮动强。

Conclusion: 用时空Hawkes过程建模股价运动，借助条件强度函数解释板块轮动，增进对金融传染的理解。

Abstract: This study explores contagion in the Chinese stock market using Hawkes processes to analyze autocorrelation and cross-correlation in multivariate time series data. We examine whether market indices exhibit trending behavior and whether sector indices influence one another. By fitting self-exciting and inhibitory Hawkes processes to daily returns of indices like the Shanghai Composite, Shenzhen Component, and ChiNext, as well as sector indices (CSI Consumer, Healthcare, and Financial), we identify long-term dependencies and trending patterns, including upward, downward, and oversold rebound trends. Results show that during high trading activity, sector indices tend to sustain their trends, while low activity periods exhibit strong sector rotation. This research models stock price movements using spatiotemporal Hawkes processes, leveraging conditional intensity functions to explain sector rotation, advancing the understanding of financial contagion.

</details>


### [187] [Does it take two to tango: Interaction between Credit Default Swaps and National Stock Indices](https://arxiv.org/abs/2512.07887)
*Yhlas Sovbetov,Hami Saka*

Main category: q-fin.ST

TL;DR: 本文用ARDL技术研究2008 - 2015年BIST - 100指数与CDS价格互动关系，发现两者相互影响及多种因素的作用，显示土企资本结构与市场敏感性，且两者会向长期均衡收敛。


<details>
  <summary>Details</summary>
Motivation: 探究2008年1月至2015年5月期间BIST - 100指数与CDS价格的短期和长期互动关系。

Method: 使用ARDL技术开展研究。

Result: 揭示了BIST - 100指数与CDS价格相互影响，以及利率、通胀率、汇率和政治事件的影响，且发现两者与控制变量会偏离后向长期均衡以适度月速度收敛。

Conclusion: 表明土耳其企业资本结构高度美元负债，金融市场对政治不确定性过度敏感。

Abstract: This paper investigates both short and long-run interaction between BIST-100 index and CDS prices over January 2008 to May 2015 using ARDL technique. The paper documents several findings. First, ARDL analysis shows that 1 TL increase in CDS shrinks BIST-100 index by 22.5 TL in short-run and 85.5 TL in long-run. Second, 1000 TL increase in BIST index price causes 25 TL and 44 TL reducation in Turkey's CDS prices in short- and long-run respectively. Third, a percentage increase in interest rate shrinks BIST index by 359 TL and a percentage increase in inflation rate scales CDS prices up to 13.34 TL both in long-run. In case of short-run, these impacts are limited with 231 TL and 5.73 TL respectively. Fourth, a kurush increase in TL/USD exchange rate leads 24.5 TL (short-run) and 78 TL (long-run) reductions in BIST, while it augments CDS prices by 2.5 TL (short-run) and 3 TL (long-run) respectively. Fifth, each negative political events decreases BIST by 237 TL in short-run and 538 TL in long-run, while it increases CDS prices by 33 TL in short-run and 89 TL in long-run. These findings imply the highly dollar indebted capital structure of Turkish firms, and overly sensitivity of financial markets to the uncertainties in political sphere. Finally, the paper provides evidence for that BIST and CDS with control variables drift too far apart, and converge to a long-run equilibrium at a moderate monthly speed.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [188] [Functional Random Forest with Adaptive Cost-Sensitive Splitting for Imbalanced Functional Data Classification](https://arxiv.org/abs/2512.07888)
*Fahad Mostafa,Hafiz Khan*

Main category: stat.ML

TL;DR: 本文提出用于不平衡功能数据分类的FRF - ACS框架，经实验验证其性能优于现有方法，为高维功能数据分析提供可扩展、可解释方案。


<details>
  <summary>Details</summary>
Motivation: 传统随机森林算法处理功能数据分类（尤其是严重类别不平衡情况）时，难以捕捉功能观测的内在结构且对少数类检测能力不足。

Method: 提出FRF - ACS框架，利用基展开和FPCA高效表示曲线，采用动态成本敏感分裂准则、混合采样策略，用曲线特定相似度指标替代欧氏距离。

Result: 在合成和真实数据集上实验表明，FRF - ACS显著提高少数类召回率和整体预测性能。

Conclusion: 该方法为少数类检测至关重要的领域的高维功能数据分析提供了可扩展、可解释的解决方案。

Abstract: Classification of functional data where observations are curves or trajectories poses unique challenges, particularly under severe class imbalance. Traditional Random Forest algorithms, while robust for tabular data, often fail to capture the intrinsic structure of functional observations and struggle with minority class detection. This paper introduces Functional Random Forest with Adaptive Cost-Sensitive Splitting (FRF-ACS), a novel ensemble framework designed for imbalanced functional data classification. The proposed method leverages basis expansions and Functional Principal Component Analysis (FPCA) to represent curves efficiently, enabling trees to operate on low dimensional functional features. To address imbalance, we incorporate a dynamic cost sensitive splitting criterion that adjusts class weights locally at each node, combined with a hybrid sampling strategy integrating functional SMOTE and weighted bootstrapping. Additionally, curve specific similarity metrics replace traditional Euclidean measures to preserve functional characteristics during leaf assignment. Extensive experiments on synthetic and real world datasets including biomedical signals and sensor trajectories demonstrate that FRF-ACS significantly improves minority class recall and overall predictive performance compared to existing functional classifiers and imbalance handling techniques. This work provides a scalable, interpretable solution for high dimensional functional data analysis in domains where minority class detection is critical.

</details>


### [189] [Provable Diffusion Posterior Sampling for Bayesian Inversion](https://arxiv.org/abs/2512.08022)
*Jinyuan Chang,Chenguang Duan,Yuling Jiao,Ruoxuan Li,Jerry Zhijian Yang,Cheng Yuan*

Main category: stat.ML

TL;DR: 提出基于扩散的后验采样方法，有理论误差界，实验验证有效性。


<details>
  <summary>Details</summary>
Motivation: 改进现有后验采样方法，避免启发式近似，更好捕获先验分布特征。

Method: 在PnP框架下构建概率传输，用热启动策略初始化粒子，开发蒙特卡罗估计器，用Langevin动力学生成粒子，从数据学习分数。

Result: 给出非渐近误差界，表明方法对复杂多模态后验分布收敛，明确多种误差来源。

Conclusion: 所提方法在一系列逆问题中有效。

Abstract: This paper proposes a novel diffusion-based posterior sampling method within a plug-and-play (PnP) framework. Our approach constructs a probability transport from an easy-to-sample terminal distribution to the target posterior, using a warm-start strategy to initialize the particles. To approximate the posterior score, we develop a Monte Carlo estimator in which particles are generated using Langevin dynamics, avoiding the heuristic approximations commonly used in prior work. The score governing the Langevin dynamics is learned from data, enabling the model to capture rich structural features of the underlying prior distribution. On the theoretical side, we provide non-asymptotic error bounds, showing that the method converges even for complex, multi-modal target posterior distributions. These bounds explicitly quantify the errors arising from posterior score estimation, the warm-start initialization, and the posterior sampling procedure. Our analysis further clarifies how the prior score-matching error and the condition number of the Bayesian inverse problem influence overall performance. Finally, we present numerical experiments demonstrating the effectiveness of the proposed method across a range of inverse problems.

</details>


### [190] [Worst-case generation via minimax optimization in Wasserstein space](https://arxiv.org/abs/2512.08176)
*Xiuyuan Cheng,Yao Xie,Linglingzhi Zhu,Yunqin Zhu*

Main category: stat.ML

TL;DR: 本文提出基于Wasserstein空间的最坏情况生成的生成建模框架，用GDA方案更新模型和传输映射，参数化传输映射实现无模拟方法，实验验证其效率。


<details>
  <summary>Details</summary>
Motivation: 传统离散分布鲁棒优化方法存在可扩展性、泛化性和推理成本问题，需开发新的最坏情况生成框架。

Method: 基于连续概率分布的最小 - 最大优化开发框架，利用Brenier定理表征最坏情况分布，提出GDA方案更新模型和传输映射，用神经网络参数化传输映射。

Result: 在合成和图像数据的数值实验中验证了所提方法作为风险诱导最坏情况生成器的效率。

Conclusion: 所提方法能有效解决传统方法问题，可作为风险诱导的最坏情况生成器。

Abstract: Worst-case generation plays a critical role in evaluating robustness and stress-testing systems under distribution shifts, in applications ranging from machine learning models to power grids and medical prediction systems. We develop a generative modeling framework for worst-case generation for a pre-specified risk, based on min-max optimization over continuous probability distributions, namely the Wasserstein space. Unlike traditional discrete distributionally robust optimization approaches, which often suffer from scalability issues, limited generalization, and costly worst-case inference, our framework exploits the Brenier theorem to characterize the least favorable (worst-case) distribution as the pushforward of a transport map from a continuous reference measure, enabling a continuous and expressive notion of risk-induced generation beyond classical discrete DRO formulations. Based on the min-max formulation, we propose a Gradient Descent Ascent (GDA)-type scheme that updates the decision model and the transport map in a single loop, establishing global convergence guarantees under mild regularity assumptions and possibly without convexity-concavity. We also propose to parameterize the transport map using a neural network that can be trained simultaneously with the GDA iterations by matching the transported training samples, thereby achieving a simulation-free approach. The efficiency of the proposed method as a risk-induced worst-case generator is validated by numerical experiments on synthetic and image data.

</details>


### [191] [Heuristics for Combinatorial Optimization via Value-based Reinforcement Learning: A Unified Framework and Analysis](https://arxiv.org/abs/2512.08601)
*Orit Davidovich,Shimrit Shtern,Segev Wasserkrug,Nimrod Megiddo*

Main category: stat.ML

TL;DR: 文章关注用强化学习解决组合优化问题，理论工作少，引入统一框架，给出条件使组合优化可转化为马尔可夫决策过程并用强化学习解决，还进行收敛性分析。


<details>
  <summary>Details</summary>
Motivation: 以往用强化学习解决组合优化问题的理论研究匮乏，需要理论分析提供支撑。

Method: 引入统一框架将组合优化问题通过马尔可夫决策过程建模，用强化学习技术求解，给出易测试假设和收敛条件。

Result: 明确了批量大小和投影梯度下降步骤的增加速率、优化间隙与问题参数和强化学习精度的关系以及状态空间嵌入选择的重要性。

Conclusion: 分析揭示了深度Q学习算法在该问题背景下的成功之处和局限性。

Abstract: Since the 1990s, considerable empirical work has been carried out to train statistical models, such as neural networks (NNs), as learned heuristics for combinatorial optimization (CO) problems. When successful, such an approach eliminates the need for experts to design heuristics per problem type. Due to their structure, many hard CO problems are amenable to treatment through reinforcement learning (RL). Indeed, we find a wealth of literature training NNs using value-based, policy gradient, or actor-critic approaches, with promising results, both in terms of empirical optimality gaps and inference runtimes. Nevertheless, there has been a paucity of theoretical work undergirding the use of RL for CO problems. To this end, we introduce a unified framework to model CO problems through Markov decision processes (MDPs) and solve them using RL techniques. We provide easy-to-test assumptions under which CO problems can be formulated as equivalent undiscounted MDPs that provide optimal solutions to the original CO problems. Moreover, we establish conditions under which value-based RL techniques converge to approximate solutions of the CO problem with a guarantee on the associated optimality gap. Our convergence analysis provides: (1) a sufficient rate of increase in batch size and projected gradient descent steps at each RL iteration; (2) the resulting optimality gap in terms of problem parameters and targeted RL accuracy; and (3) the importance of a choice of state-space embedding. Together, our analysis illuminates the success (and limitations) of the celebrated deep Q-learning algorithm in this problem context.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [192] [deepspat: An R package for modeling nonstationary spatial and spatio-temporal Gaussian and extremes data through deep deformations](https://arxiv.org/abs/2512.08137)
*Quan Vu,Xuanjie Shao,Raphaël Huser,Andrew Zammit-Mangion*

Main category: stat.CO

TL;DR: 介绍R软件包deepspat用于非平稳时空模型的建模、拟合和预测，通过模拟研究和实际应用展示功能。


<details>
  <summary>Details</summary>
Motivation: 环境数据集中非平稳性普遍，但缺乏实现非平稳模型的统计软件包。

Method: 使用原始时空域的深度多层变形构建非平稳模型，用tensorflow基于梯度优化自定义损失函数估计参数。

Result: 通过模拟研究和尼泊尔温度数据应用展示了软件包的功能。

Conclusion: R软件包deepspat可用于非平稳空间和时空模型对高斯和极端数据的建模、拟合和预测。

Abstract: Nonstationarity in spatial and spatio-temporal processes is ubiquitous in environmental datasets, but is not often addressed in practice, due to a scarcity of statistical software packages that implement nonstationary models. In this article, we introduce the R software package deepspat, which allows for modeling, fitting and prediction with nonstationary spatial and spatio-temporal models applied to Gaussian and extremes data. The nonstationary models in our package are constructed using a deep multi-layered deformation of the original spatial or spatio-temporal domain, and are straightforward to implement. Model parameters are estimated using gradient-based optimization of customized loss functions with tensorflow, which implements automatic differentiation. The functionalities of the package are illustrated through simulation studies and an application to Nepal temperature data.

</details>


### [193] [Matrix Completion Survey: Theory, Algorithms, and Empirical Evaluation](https://arxiv.org/abs/2512.08689)
*Connor Panish,Leo Villani*

Main category: stat.CO

TL;DR: 对矩阵补全方法及相关算法实现进行简要调研，涵盖被动和自适应策略，并通过实验展示简单自适应采样方案的表现。


<details>
  <summary>Details</summary>
Motivation: 对矩阵补全方法及相关算法实现进行全面了解。

Method: 进行文献调研，涵盖被动和自适应策略，开展受控合成实验。

Result: 完成矩阵补全方法的调研，展示了简单自适应采样方案的行为。

Conclusion: 对矩阵补全方法的研究有一定的参考价值，自适应采样方案在实验中有特定表现。

Abstract: We present a concise survey of matrix completion methods and associated implementations of several fundamental algorithms. Our study covers both passive and adaptive strategies. We further illustrate the behavior of a simple adaptive sampling scheme through controlled synthetic experiments.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [194] [ClinicalTrialsHub: Bridging Registries and Literature for Comprehensive Clinical Trial Access](https://arxiv.org/abs/2512.08193)
*Jiwoo Park,Ruoqi Liu,Avani Jagdale,Andrew Srisuwananukorn,Jing Zhao,Lang Li,Ping Zhang,Sachin Kumar*

Main category: cs.CL

TL;DR: 介绍ClinicalTrialsHub平台，结合ClinicalTrials.gov数据和PubMed文章信息，用大语言模型增强易用性，经评估证明其有用。


<details>
  <summary>Details</summary>
Motivation: 增加结构化临床试验数据的可获取性，方便患者、临床医生、研究人员和政策制定者，推动循证医学发展。

Method: 使用GPT - 5.1和Gemini - 3 - Pro等大语言模型，自动解析全文研究文章提取信息，将用户查询转化为结构化数据库搜索，提供归因问答系统。

Result: 与仅依赖ClinicalTrials.gov相比，系统使结构化临床试验数据的可获取性提高了83.8%。

Conclusion: 通过用户研究和系统评估证明了ClinicalTrialsHub平台的实用性。

Abstract: We present ClinicalTrialsHub, an interactive search-focused platform that consolidates all data from ClinicalTrials.gov and augments it by automatically extracting and structuring trial-relevant information from PubMed research articles. Our system effectively increases access to structured clinical trial data by 83.8% compared to relying on ClinicalTrials.gov alone, with potential to make access easier for patients, clinicians, researchers, and policymakers, advancing evidence-based medicine. ClinicalTrialsHub uses large language models such as GPT-5.1 and Gemini-3-Pro to enhance accessibility. The platform automatically parses full-text research articles to extract structured trial information, translates user queries into structured database searches, and provides an attributed question-answering system that generates evidence-grounded answers linked to specific source sentences. We demonstrate its utility through a user study involving clinicians, clinical researchers, and PhD students of pharmaceutical sciences and nursing, and a systematic automatic evaluation of its information extraction and question answering capabilities.

</details>


### [195] [Short-Context Dominance: How Much Local Context Natural Language Actually Needs?](https://arxiv.org/abs/2512.08082)
*Vala Vakilian,Zimeng Wang,Ankit Singh Rawat,Christos Thrampoulidis*

Main category: cs.CL

TL;DR: 研究短上下文主导假设，测量最小上下文长度，引入 DaMCL 检测长短上下文序列，开发解码算法减轻偏差以提升性能


<details>
  <summary>Details</summary>
Motivation: 研究短上下文主导假设，并检测难以用短前缀预测的长上下文序列，减轻短上下文主导对大语言模型输出分布的偏差

Method: 用大语言模型测量最小上下文长度，引入与实际预测无关且兼容多种采样策略的 DaMCL，通过简单阈值检测长短上下文序列，开发利用检测器的解码算法

Result: 75 - 80% 长度为 1 - 7k 个标记的序列最多只需要最后 96 个标记；简单阈值的 DaMCL 检测长短上下文序列性能高；减轻偏差能提升性能

Conclusion: 短上下文在大多数序列中起主导作用，DaMCL 能有效检测长上下文序列，减轻偏差的解码算法可提升模型性能

Abstract: We investigate the short-context dominance hypothesis: that for most sequences, a small local prefix suffices to predict their next tokens. Using large language models as statistical oracles, we measure the minimum context length (MCL) needed to reproduce accurate full-context predictions across datasets with sequences of varying lengths. For sequences with 1-7k tokens from long-context documents, we consistently find that 75-80% require only the last 96 tokens at most. Given the dominance of short-context tokens, we then ask whether it is possible to detect challenging long-context sequences for which a short local prefix does not suffice for prediction. We introduce a practical proxy to MCL, called Distributionally Aware MCL (DaMCL), that does not require knowledge of the actual next-token and is compatible with sampling strategies beyond greedy decoding. Our experiments validate that simple thresholding of the metric defining DaMCL achieves high performance in detecting long vs. short context sequences. Finally, to counter the bias that short-context dominance induces in LLM output distributions, we develop an intuitive decoding algorithm that leverages our detector to identify and boost tokens that are long-range-relevant. Across Q&A tasks and model architectures, we confirm that mitigating the bias improves performance.

</details>


### [196] [Are generative AI text annotations systematically biased?](https://arxiv.org/abs/2512.08404)
*Sjoerd B. Stolwijk,Mark Boukes,Damian Trilling*

Main category: cs.CL

TL;DR: 本文通过复制Boukes（2024）的人工标注来研究大语言模型标注中的偏差，发现大语言模型F1分数表现尚可，但存在偏差。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型标注中的偏差。

Method: 复制Boukes（2024）的人工标注，使用多种大语言模型结合五种不同提示对五个概念进行标注。

Result: 大语言模型F1分数表现尚可，但在流行度上与人工标注不同，下游结果差异大，且相互之间的重叠度高于与人工标注的重叠度，F1分数差异不能解释偏差程度。

Conclusion: 大语言模型标注存在系统性偏差。

Abstract: This paper investigates bias in GLLM annotations by conceptually replicating manual annotations of Boukes (2024). Using various GLLMs (Llama3.1:8b, Llama3.3:70b, GPT4o, Qwen2.5:72b) in combination with five different prompts for five concepts (political content, interactivity, rationality, incivility, and ideology). We find GLLMs perform adequate in terms of F1 scores, but differ from manual annotations in terms of prevalence, yield substantively different downstream results, and display systematic bias in that they overlap more with each other than with manual annotations. Differences in F1 scores fail to account for the degree of bias.

</details>


### [197] [Curriculum Guided Massive Multi Agent System Solving For Robust Long Horizon Tasks](https://arxiv.org/abs/2512.08545)
*Indrajit Kar,Kalathur Chenchu Kishore Kumar*

Main category: cs.CL

TL;DR: 文章提出一种分层多智能体架构解决大模型长程推理和计算成本问题，在汉诺塔基准测试取得好结果。


<details>
  <summary>Details</summary>
Motivation: 大语言模型和多智能体系统在长程推理任务和计算成本上面临挑战。

Method: 引入分层多智能体架构，结合选择性预言机；采用空间课程扩展操作区域；集成负对数似然作为置信度指标；使用汤普森采样课程管理器选择训练区域。

Result: 在汉诺塔基准测试中，该方法提高了稳定性，减少了预言机使用，增强了分布式智能体合作的长程推理能力。

Conclusion: 所提出的方法在处理长程推理任务有较好效果。

Abstract: Large Language Models and multi-agent systems have shown promise in decomposing complex tasks, yet they struggle with long-horizon reasoning tasks and escalating computation cost. This work introduces a hierarchical multi-agent architecture that distributes reasoning across a 64*64 grid of lightweight agents, supported by a selective oracle. A spatial curriculum progressively expands the operational region of the grid, ensuring that agents master easier central tasks before tackling harder peripheral ones. To improve reliability, the system integrates Negative Log-Likelihood as a measure of confidence, allowing the curriculum to prioritize regions where agents are both accurate and well calibrated. A Thompson Sampling curriculum manager adaptively chooses training zones based on competence and NLL-driven reward signals. We evaluate the approach on a spatially grounded Tower of Hanoi benchmark, which mirrors the long-horizon structure of many robotic manipulation and planning tasks. Results demonstrate improved stability, reduced oracle usage, and stronger long-range reasoning from distributed agent cooperation.

</details>


### [198] [Automatic Essay Scoring and Feedback Generation in Basque Language Learning](https://arxiv.org/abs/2512.08713)
*Ekhi Azurmendi,Xabier Arregi,Oier Lopez de Lacalle*

Main category: cs.CL

TL;DR: 本文介绍首个巴斯克语自动作文评分及反馈生成公开数据集，微调开源模型，实验表明Latxa微调后表现超闭源系统，还提出新评估方法，为低资源语言NLP研究奠基。


<details>
  <summary>Details</summary>
Motivation: 提供巴斯克语自动作文评分和反馈生成的公开数据集，推动低资源语言NLP研究。

Method: 引入巴斯克语数据集，微调RoBERTa - EusCrawl和Latxa 8B/70B等开源模型，提出结合自动一致性指标和专家验证的评估方法。

Result: 编码器模型对AES可靠，Latxa微调后在评分一致性和反馈质量上超GPT - 5等闭源系统，能产生符合标准、有教学意义的反馈，识别更多错误类型。

Conclusion: 该资源和基准为巴斯克语等低资源语言的NLP研究奠定基础。

Abstract: This paper introduces the first publicly available dataset for Automatic Essay Scoring (AES) and feedback generation in Basque, targeting the CEFR C1 proficiency level. The dataset comprises 3,200 essays from HABE, each annotated by expert evaluators with criterion specific scores covering correctness, richness, coherence, cohesion, and task alignment enriched with detailed feedback and error examples. We fine-tune open-source models, including RoBERTa-EusCrawl and Latxa 8B/70B, for both scoring and explanation generation. Our experiments show that encoder models remain highly reliable for AES, while supervised fine-tuning (SFT) of Latxa significantly enhances performance, surpassing state-of-the-art (SoTA) closed-source systems such as GPT-5 and Claude Sonnet 4.5 in scoring consistency and feedback quality. We also propose a novel evaluation methodology for assessing feedback generation, combining automatic consistency metrics with expert-based validation of extracted learner errors. Results demonstrate that the fine-tuned Latxa model produces criterion-aligned, pedagogically meaningful feedback and identifies a wider range of error types than proprietary models. This resource and benchmark establish a foundation for transparent, reproducible, and educationally grounded NLP research in low-resource languages such as Basque.

</details>


### [199] [Fluent Alignment with Disfluent Judges: Post-training for Lower-resource Languages](https://arxiv.org/abs/2512.08777)
*David Samuel,Lilja Øvrelid,Erik Velldal,Andrey Kutuzov*

Main category: cs.CL

TL;DR: 提出低资源语言后训练方法，在不依赖指令调优数据下实现偏好对齐，挪威语案例显示该方法关键且表现优。


<details>
  <summary>Details</summary>
Motivation: 现有偏好优化研究多针对英语和中文，低资源语言缺乏原生数据和生成流畅数据的模型，需开发无目标语言指令调优数据的流畅偏好对齐语言模型。

Method: 采用策略内训练方法，并与机器翻译数据监督微调、多语言微调两种常见方法对比。

Result: 通过挪威语案例及母语者评估，发现策略内训练方面很关键，不依赖难获取数据且表现优于其他方法。

Conclusion: 策略内训练方法对低资源语言在不依赖指令调优数据下实现偏好对齐很有效。

Abstract: We propose a post-training method for lower-resource languages that preserves fluency of language models even when aligned by disfluent reward models. Preference-optimization is now a well-researched topic, but previous work has mostly addressed models for English and Chinese. Lower-resource languages lack both datasets written by native speakers and language models capable of generating fluent synthetic data. Thus, in this work, we focus on developing a fluent preference-aligned language model without any instruction-tuning data in the target language. Our approach uses an on-policy training method, which we compare with two common approaches: supervised finetuning on machine-translated data and multilingual finetuning. We conduct a case study on Norwegian Bokmål and evaluate fluency through native-speaker assessments. The results show that the on-policy aspect is crucial and outperforms the alternatives without relying on any hard-to-obtain data.

</details>


### [200] [A Systematic Evaluation of Preference Aggregation in Federated RLHF for Pluralistic Alignment of LLMs](https://arxiv.org/abs/2512.08786)
*Mahmoud Srewa,Tianyu Zhao,Salma Elmalaki*

Main category: cs.CL

TL;DR: 本文提出评估框架，在联邦学习环境中用新自适应方案解决大语言模型与人类偏好对齐问题，实验证明该方案有优势。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习环境中，标准方法难以让大语言模型与多样人类偏好对齐，无法充分代表多样观点的问题。

Method: 引入综合评估框架，评估不同人类偏好聚合策略下对齐质量与公平性的权衡；服务器聚合组级奖励且不访问原始数据；评估标准奖励聚合技术，提出基于组历史对齐表现动态调整偏好权重的自适应方案。

Result: 在基于PPO的RLHF问答任务实验中，自适应方法在保持竞争力对齐分数的同时，持续实现了更优公平性。

Conclusion: 提供了评估大语言模型在不同群体中表现的可靠方法，为开发多元化且公平对齐的模型提供实用解决方案。

Abstract: This paper addresses the challenge of aligning large language models (LLMs) with diverse human preferences within federated learning (FL) environments, where standard methods often fail to adequately represent diverse viewpoints. We introduce a comprehensive evaluation framework that systematically assesses the trade-off between alignment quality and fairness when using different aggregation strategies for human preferences. In our federated setting, each group locally evaluates rollouts and produces reward signals, and the server aggregates these group-level rewards without accessing any raw data. Specifically, we evaluate standard reward aggregation techniques (min, max, and average) and introduce a novel adaptive scheme that dynamically adjusts preference weights based on a group's historical alignment performance. Our experiments on question-answering (Q/A) tasks using a PPO-based RLHF pipeline demonstrate that our adaptive approach consistently achieves superior fairness while maintaining competitive alignment scores. This work offers a robust methodology for evaluating LLM behavior across diverse populations and provides a practical solution for developing truly pluralistic and fairly aligned models.

</details>


### [201] [Do Depth-Grown Models Overcome the Curse of Depth? An In-Depth Analysis](https://arxiv.org/abs/2512.08819)
*Ferdinand Kapl,Emmanouil Angelis,Tobias Höppe,Kaitlin Maile,Johannes von Oswald,Nino Scherrer,Stefan Bauer*

Main category: cs.CL

TL;DR: 研究剖析渐进增长Transformer深度训练的增益机制，提出改进方法克服标准模型深度利用局限。


<details>
  <summary>Details</summary>
Motivation: 现有研究虽表明渐进增长Transformer深度训练有好处，但缺乏对增益的机制理解。

Method: 通过深度分析，揭示渐进中间堆叠增长方式的作用，并对MIDAS提出轻量级修改。

Result: 发现渐进中间堆叠增长能更有效利用模型深度、改变残差流结构、促进可置换计算块形成，改进后的MIDAS提升了下游推理表现。

Conclusion: 渐进增长模型深度可形成独特计算电路，克服标准未增长模型深度利用有限的问题。

Abstract: Gradually growing the depth of Transformers during training can not only reduce training cost but also lead to improved reasoning performance, as shown by MIDAS (Saunshi et al., 2024). Thus far, however, a mechanistic understanding of these gains has been missing. In this work, we establish a connection to recent work showing that layers in the second half of non-grown, pre-layernorm Transformers contribute much less to the final output distribution than those in the first half - also known as the Curse of Depth (Sun et al., 2025, Csordás et al., 2025). Using depth-wise analyses, we demonstrate that growth via gradual middle stacking yields more effective utilization of model depth, alters the residual stream structure, and facilitates the formation of permutable computational blocks. In addition, we propose a lightweight modification of MIDAS that yields further improvements in downstream reasoning benchmarks. Overall, this work highlights how the gradual growth of model depth can lead to the formation of distinct computational circuits and overcome the limited depth utilization seen in standard non-grown models.

</details>


### [202] [An Agentic AI System for Multi-Framework Communication Coding](https://arxiv.org/abs/2512.08659)
*Bohao Yang,Rui Yang,Joshua M. Biro,Haoyuan Wang,Jessica L. Handley,Brianna Richardson,Sophia Bessias,Nicoleta Economou-Zavlanos,Armando D. Bedoya,Monica Agrawal,Michael M. Zavlanos,Anand Chowdhury,Raj M. Ratwani,Kai Sun,Kathryn I. Pollak,Michael J. Pencina,Chuan Hong*

Main category: cs.CL

TL;DR: 本文开发了针对临床沟通的MOSAIC系统，基于LangGraph架构，含四个核心代理，测试集F1得分0.928，表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 大规模人工标注医患对话劳神费力、效果不一且难扩展，现有大语言模型单任务模型缺乏适应性、可解释性和可靠性。

Method: 开发基于LangGraph架构的MOSAIC系统，含计划、更新、标注和验证四个核心代理；用26个标注文本训练、50个测试；与人工标注对比评估性能。

Result: 测试集上整体F1分为0.928，风湿科子集表现最佳，患者行为方面表现强劲，消融实验显示MOSAIC优于基线。

Conclusion: MOSAIC系统在临床沟通分析上表现良好，优于现有方法。

Abstract: Clinical communication is central to patient outcomes, yet large-scale human annotation of patient-provider conversation remains labor-intensive, inconsistent, and difficult to scale. Existing approaches based on large language models typically rely on single-task models that lack adaptability, interpretability, and reliability, especially when applied across various communication frameworks and clinical domains. In this study, we developed a Multi-framework Structured Agentic AI system for Clinical Communication (MOSAIC), built on a LangGraph-based architecture that orchestrates four core agents, including a Plan Agent for codebook selection and workflow planning, an Update Agent for maintaining up-to-date retrieval databases, a set of Annotation Agents that applies codebook-guided retrieval-augmented generation (RAG) with dynamic few-shot prompting, and a Verification Agent that provides consistency checks and feedback. To evaluate performance, we compared MOSAIC outputs against gold-standard annotations created by trained human coders. We developed and evaluated MOSAIC using 26 gold standard annotated transcripts for training and 50 transcripts for testing, spanning rheumatology and OB/GYN domains. On the test set, MOSAIC achieved an overall F1 score of 0.928. Performance was highest in the Rheumatology subset (F1 = 0.962) and strongest for Patient Behavior (e.g., patients asking questions, expressing preferences, or showing assertiveness). Ablations revealed that MOSAIC outperforms baseline benchmarking.

</details>


### [203] [Toward Faithful Retrieval-Augmented Generation with Sparse Autoencoders](https://arxiv.org/abs/2512.08892)
*Guangzhi Xiong,Zhenghao He,Bohan Liu,Sanchit Sinha,Aidong Zhang*

Main category: cs.CL

TL;DR: 本文围绕RAG的幻觉检测展开，提出轻量级检测器RAGLens，性能优且可解释。


<details>
  <summary>Details</summary>
Motivation: 现有RAG幻觉检测方法存在依赖大量标注数据、推理成本高或准确率有限等问题，故寻求更好方案。

Method: 利用稀疏自动编码器（SAEs）解开内部激活，基于信息的特征选择和加性特征建模构建系统管道，引入RAGLens。

Result: RAGLens检测性能优于现有方法，能为决策提供可解释原理，可有效事后缓解RAG不忠实问题。

Conclusion: 设计选择合理，揭示了LLMs中幻觉相关信号分布的新见解，代码开源。

Abstract: Retrieval-Augmented Generation (RAG) improves the factuality of large language models (LLMs) by grounding outputs in retrieved evidence, but faithfulness failures, where generations contradict or extend beyond the provided sources, remain a critical challenge. Existing hallucination detection methods for RAG often rely either on large-scale detector training, which requires substantial annotated data, or on querying external LLM judges, which leads to high inference costs. Although some approaches attempt to leverage internal representations of LLMs for hallucination detection, their accuracy remains limited. Motivated by recent advances in mechanistic interpretability, we employ sparse autoencoders (SAEs) to disentangle internal activations, successfully identifying features that are specifically triggered during RAG hallucinations. Building on a systematic pipeline of information-based feature selection and additive feature modeling, we introduce RAGLens, a lightweight hallucination detector that accurately flags unfaithful RAG outputs using LLM internal representations. RAGLens not only achieves superior detection performance compared to existing methods, but also provides interpretable rationales for its decisions, enabling effective post-hoc mitigation of unfaithful RAG. Finally, we justify our design choices and reveal new insights into the distribution of hallucination-related signals within LLMs. The code is available at https://github.com/Teddy-XiongGZ/RAGLens.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [204] [AudioScene: Integrating Object-Event Audio into 3D Scenes](https://arxiv.org/abs/2512.07845)
*Shuaihang Yuan,Congcong Wen,Muhammad Shafique,Anthony Tzes,Yi Fang*

Main category: cs.SD

TL;DR: 现有纯音频数据集缺少空间背景，提出AudioScanNet和AudioRoboTHOR数据集促进音频引导的空间学习，展示当前音频中心方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有纯音频数据集缺乏空间背景，为解决该问题开展研究。

Method: 提出AudioScanNet和AudioRoboTHOR两个新的音频 - 空间场景数据集，结合音频片段与空间对齐3D场景；利用大语言模型常识推理能力并辅以人工验证关联音频事件和空间信息。

Result: 通过标注者间一致性和两个基准任务的性能量化，结果显示当前以音频为中心的方法存在局限性。

Conclusion: 提出的数据集在推进音频引导的空间学习方面具有实际挑战和重要意义。

Abstract: The rapid advances in audio analysis underscore its vast potential for humancomputer interaction, environmental monitoring, and public safety; yet, existing audioonly datasets often lack spatial context. To address this gap, we present two novel audiospatial scene datasets, AudioScanNet and AudioRoboTHOR, designed to explore audioconditioned tasks within 3D environments. By integrating audio clips with spatially aligned 3D scenes, our datasets enable research on how audio signals interact with spatial context. To associate audio events with corresponding spatial information, we leverage the common sense reasoning ability of large language models and supplement them with rigorous human verification, This approach offers greater scalability compared to purely manual annotation while maintaining high standards of accuracy, completeness, and diversity, quantified through inter annotator agreement and performance on two benchmark tasks audio based 3D visual grounding and audio based robotic zeroshot navigation. The results highlight the limitations of current audiocentric methods and underscore the practical challenges and significance of our datasets in advancing audio guided spatial learning.

</details>


### [205] [SpeechQualityLLM: LLM-Based Multimodal Assessment of Speech Quality](https://arxiv.org/abs/2512.08238)
*Mahathir Monjur,Shahriar Nirjon*

Main category: cs.SD

TL;DR: 介绍了多模态语音质量问答系统SpeechQualityLLM，在NISQA语料库上训练，表现佳且有灵活自然语言接口，能降低成本。


<details>
  <summary>Details</summary>
Motivation: 经典语音质量评估指标需控制条件和昂贵测试，学习型模型缺乏灵活性，不支持自然语言查询和提供文本理由。

Method: 将语音编码器与语言模型结合，在NISQA语料库上用模板问答对训练，监督系统生成文本答案。

Result: 双端模型在保留的NISQA片段上，MOS平均绝对误差为0.41，皮尔逊相关系数为0.86，各维度指标表现良好。

Conclusion: SpeechQualityLLM不仅有定量优势，还提供灵活自然语言接口，可减少对众包测试的依赖和成本。

Abstract: Objective speech quality assessment is central to telephony, VoIP, and streaming systems, where large volumes of degraded audio must be monitored and optimized at scale. Classical metrics such as PESQ and POLQA approximate human mean opinion scores (MOS) but require carefully controlled conditions and expensive listening tests, while learning-based models such as NISQA regress MOS and multiple perceptual dimensions from waveforms or spectrograms, achieving high correlation with subjective ratings yet remaining rigid: they do not support interactive, natural-language queries and do not natively provide textual rationales. In this work, we introduce SpeechQualityLLM, a multimodal speech quality question-answering (QA) system that couples an audio encoder with a language model and is trained on the NISQA corpus using template-based question-answer pairs covering overall MOS and four perceptual dimensions (noisiness, coloration, discontinuity, and loudness) in both single-ended (degraded only) and double-ended (degraded plus clean reference) setups. Instead of directly regressing scores, our system is supervised to generate textual answers from which numeric predictions are parsed and evaluated with standard regression and ranking metrics; on held-out NISQA clips, the double-ended model attains a MOS mean absolute error (MAE) of 0.41 with Pearson correlation of 0.86, with competitive performance on dimension-wise tasks. Beyond these quantitative gains, it offers a flexible natural-language interface in which the language model acts as an audio quality expert: practitioners can query arbitrary aspects of degradations, prompt the model to emulate different listener profiles to capture human variability and produce diverse but plausible judgments rather than a single deterministic score, and thereby reduce reliance on large-scale crowdsourced tests and their monetary cost.

</details>


### [206] [Emovectors: assessing emotional content in jazz improvisations for creativity evaluation](https://arxiv.org/abs/2512.08812)
*Anna Jordanous*

Main category: cs.SD

TL;DR: 研究爵士乐即兴创作创造力评估，提出基于嵌入的方法捕捉情感内容以测试假设，为创造力评估提供新指标。


<details>
  <summary>Details</summary>
Motivation: 评估爵士乐即兴创作的创造力，为当前基于大语言模型的生成系统获取创造力的自动化指标，检测即兴表演中的情感投入。

Method: 提出基于嵌入的方法，使用与情绪相关的音乐特征的心理学分类来捕捉音乐即兴表演中的情感内容，分析生成的“情绪向量”。

Result: 以可量化的方式捕捉情感内容。

Conclusion: 这种捕捉情感内容的方式有助于大规模应用的创造力评估新指标的形成。

Abstract: Music improvisation is fascinating to study, being essentially a live demonstration of a creative process. In jazz, musicians often improvise across predefined chord progressions (leadsheets). How do we assess the creativity of jazz improvisations? And can we capture this in automated metrics for creativity for current LLM-based generative systems? Demonstration of emotional involvement is closely linked with creativity in improvisation. Analysing musical audio, can we detect emotional involvement? This study hypothesises that if an improvisation contains more evidence of emotion-laden content, it is more likely to be recognised as creative. An embeddings-based method is proposed for capturing the emotional content in musical improvisations, using a psychologically-grounded classification of musical characteristics associated with emotions. Resulting 'emovectors' are analysed to test the above hypothesis, comparing across multiple improvisations. Capturing emotional content in this quantifiable way can contribute towards new metrics for creativity evaluation that can be applied at scale.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [207] [State and Parameter Estimation for a Neural Model of Local Field Potentials](https://arxiv.org/abs/2512.07842)
*Daniele Avitabile,Gabriel J. Lord,Khadija Meddouni*

Main category: q-bio.NC

TL;DR: 本文提出用离散化Wilson - Cowan Amari神经场模型和数据同化方法，对小鼠自然睡眠时皮质神经活动建模，经合成测量验证后应用于文献数据集，发现该方法可表征皮质接收的刺激并推断状态。


<details>
  <summary>Details</summary>
Motivation: 研究不同状态下皮质动力学是神经科学重要课题，建模旨在将皮质记录中的神经节律与潜在动力学联系起来，本文聚焦小鼠自然睡眠时皮质神经活动。

Method: 采用离散化Wilson - Cowan Amari神经场模型描述神经活动，结合数据同化方法进行状态和参数的贝叶斯联合估计。

Result: 在合成测量中验证了方法的可行性，并将其应用于文献中的数据集。

Conclusion: 该方法有潜力表征皮质从其他脑区接收的刺激，同时推断出与观测信号相符的状态。

Abstract: The study of cortical dynamics during different states such as decision making, sleep and movement, is an important topic in Neuroscience. Modelling efforts aim to relate the neural rhythms present in cortical recordings to the underlying dynamics responsible for their emergence. We present an effort to characterize the neural activity from the cortex of a mouse during natural sleep, captured through local field potential measurements. Our approach relies on using a discretized Wilson--Cowan Amari neural field model for neural activity, along with a data assimilation method that allows the Bayesian joint estimation of the state and parameters. We demonstrate the feasibility of our approach on synthetic measurements before applying it to a dataset available in literature. Our findings suggest the potential of our approach to characterize the stimulus received by the cortex from other brain regions, while simultaneously inferring a state that aligns with the observed signal.

</details>


### [208] [Manifolds and Modules: How Function Develops in a Neural Foundation Model](https://arxiv.org/abs/2512.07869)
*Johannes Bertram,Luciano Dyballa,T. Anderson Keller,Savik Kinger,Steven W. Zucker*

Main category: q-bio.NC

TL;DR: 文章剖析了先进神经基础模型的内部运作，基于神经元对刺激的时间响应特性分析其表征结构，发现不同模块有不同特征。


<details>
  <summary>Details</summary>
Motivation: 基础模型虽拟合生物视觉系统取得成功，但黑箱性质限制其对脑功能的理解，因此需解析其内部运作。

Method: 像生理学家一样刻画模型每个‘神经元’，通过构建解码流形分析刺激在神经活动空间的表征，构建神经编码流形分析神经元在刺激 - 响应空间的表征。

Result: 模型不同处理阶段（前馈编码器、循环模块和读出模块）在流形中呈现不同表征结构，循环模块使不同时间刺激模式的表征分离，读出模块通过专用特征图实现生物保真度。

Conclusion: 通过分析神经元的联合时间响应模式，研究了神经基础模型的内部运作及内部机制与生物的相关性。

Abstract: Foundation models have shown remarkable success in fitting biological visual systems; however, their black-box nature inherently limits their utility for understanding brain function. Here, we peek inside a SOTA foundation model of neural activity (Wang et al., 2025) as a physiologist might, characterizing each 'neuron' based on its temporal response properties to parametric stimuli. We analyze how different stimuli are represented in neural activity space by building decoding manifolds, and we analyze how different neurons are represented in stimulus-response space by building neural encoding manifolds. We find that the different processing stages of the model (i.e., the feedforward encoder, recurrent, and readout modules) each exhibit qualitatively different representational structures in these manifolds. The recurrent module shows a jump in capabilities over the encoder module by 'pushing apart' the representations of different temporal stimulus patterns; while the readout module achieves biological fidelity by using numerous specialized feature maps rather than biologically plausible mechanisms. Overall, we present this work as a study of the inner workings of a prominent neural foundation model, gaining insights into the biological relevance of its internals through the novel analysis of its neurons' joint temporal response patterns.

</details>


<div id='physics.data-an'></div>

# physics.data-an [[Back]](#toc)

### [209] [Automating High Energy Physics Data Analysis with LLM-Powered Agents](https://arxiv.org/abs/2512.07785)
*Eli Gendreau-Distler,Joshua Ho,Dongwon Kim,Luc Tomas Le Pottier,Haichen Wang,Chengxi Yang*

Main category: physics.data-an

TL;DR: 本文通过ATLAS开放数据开展希格斯玻色子双光子截面测量案例研究，展示使用大语言模型（LLM）代理自动化高能物理（HEP）分析，建立首个LLM代理驱动的自动化数据分析框架，还对模型能力等进行基准测试。


<details>
  <summary>Details</summary>
Motivation: 探索使用大语言模型代理自动化高能物理分析，实现分析代码的自主生成与执行。

Method: 设计结合基于LLM的监督编码代理和Snakemake工作流管理器的混合系统，定义定量评估指标，对多种LLM进行基准测试。

Result: 工作流管理器确保分析步骤确定性执行，但最终输出仍有随机变化，模型未产生完全确定性结果。

Conclusion: 建立了首个HEP领域LLM代理驱动的自动化数据分析框架，可对模型能力、稳定性和局限性进行系统基准测试。

Abstract: We present a proof-of-principle study demonstrating the use of large language model (LLM) agents to automate a representative high energy physics (HEP) analysis. Using the Higgs boson diphoton cross-section measurement as a case study with ATLAS Open Data, we design a hybrid system that combines an LLM-based supervisor-coder agent with the Snakemake workflow manager. In this architecture, the workflow manager enforces reproducibility and determinism, while the agent autonomously generates, executes, and iteratively corrects analysis code in response to user instructions. We define quantitative evaluation metrics including success rate, error distribution, costs per specific task, and average number of API calls, to assess agent performance across multi-stage workflows. To characterize variability across architectures, we benchmark a representative selection of state-of-the-art LLMs spanning the Gemini and GPT-5 series, the Claude family, and leading open-weight models. While the workflow manager ensures deterministic execution of all analysis steps, the final outputs still show stochastic variation. Although we set the temperature to zero, other sampling parameters (e.g., top-p, top-k) remained at their defaults, and some reasoning-oriented models internally adjust these settings. Consequently, the models do not produce fully deterministic results. This study establishes the first LLM-agent-driven automated data-analysis framework in HEP, enabling systematic benchmarking of model capabilities, stability, and limitations in real-world scientific computing environments. The baseline code used in this work is available at https://huggingface.co/HWresearch/LLM4HEP. This work was accepted as a poster at the Machine Learning and the Physical Sciences (ML4PS) workshop at NeurIPS 2025. The initial submission was made on August 30, 2025.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [210] [Quantum Circuit Reasoning Models: A Variational Framework for Differentiable Logical Inference](https://arxiv.org/abs/2512.07871)
*Andrew Kiruluta*

Main category: quant-ph

TL;DR: 介绍量子电路推理模型（QCRM），它将变分量子电路概念拓展到逻辑推理领域，开发数学基础、定义架构等，提出量子推理层用于多领域推理。


<details>
  <summary>Details</summary>
Motivation: 将变分量子电路从能量最小化和分类任务拓展到结构化逻辑推理，利用量子力学操作与推理基本元素的映射关系。

Method: 开发QCRM的数学基础，定义参数化电路架构，将逻辑规则编码为幺正变换，基于经典梯度下降确定训练目标，在经典硬件上进行模拟实现。

Result: 提出可用于科学、生物医学和化学推理领域的可组合推理模型中的量子推理层（QRL）。

Conclusion: QCRM结合量子启发计算与可微优化，能实现逻辑推理，QRL可用于多领域推理模型。

Abstract: This report introduces a novel class of reasoning architectures, termed Quantum Circuit Reasoning Models (QCRM), which extend the concept of Variational Quantum Circuits (VQC) from energy minimization and classification tasks to structured logical inference and reasoning. We posit that fundamental quantum mechanical operations, superposition, entanglement, interference, and measurement, naturally map to essential reasoning primitives such as hypothesis branching, constraint propagation, consistency enforcement, and decision making. The resulting framework combines quantum-inspired computation with differentiable optimization, enabling reasoning to emerge as a process of amplitude evolution and interference-driven selection of self-consistent states. We develop the mathematical foundation of QCRM, define its parameterized circuit architecture, and show how logical rules can be encoded as unitary transformations over proposition-qubit states. We further formalize a training objective grounded in classical gradient descent over circuit parameters and discuss simulation-based implementations on classical hardware. Finally, we propose the Quantum Reasoning Layer (QRL) as a differentiable hybrid component for composable reasoning models applicable to scientific, biomedical, and chemical inference domains.

</details>


### [211] [SAQ: Stabilizer-Aware Quantum Error Correction Decoder](https://arxiv.org/abs/2512.08914)
*David Zenati,Eliya Nachmani*

Main category: quant-ph

TL;DR: 提出SAQ - Decoder框架，结合基于变压器的学习和约束感知后处理，在量子纠错解码中实现高准确率和线性计算可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有量子纠错解码方法存在准确率 - 效率权衡问题，经典方法性能不稳定、复杂度高，张量网络解码器计算成本高，神经解码器准确率不足。

Method: 结合双流变压器架构处理综合征和逻辑信息，采用非对称注意力模式，使用新的可微逻辑损失直接优化逻辑错误率。

Result: SAQ - Decoder在环面码上达到接近最优性能，独立噪声和去极化噪声下的错误阈值接近最大似然界限，在准确率、复杂度和参数效率上优于现有神经和经典基线。

Conclusion: 学习型解码器可同时实现有竞争力的解码准确率和计算效率，满足实用容错量子计算系统的关键要求。

Abstract: Quantum Error Correction (QEC) decoding faces a fundamental accuracy-efficiency tradeoff. Classical methods like Minimum Weight Perfect Matching (MWPM) exhibit variable performance across noise models and suffer from polynomial complexity, while tensor network decoders achieve high accuracy but at prohibitively high computational cost. Recent neural decoders reduce complexity but lack the accuracy needed to compete with computationally expensive classical methods. We introduce SAQ-Decoder, a unified framework combining transformer-based learning with constraint aware post-processing that achieves both near Maximum Likelihood (ML) accuracy and linear computational scalability with respect to the syndrome size. Our approach combines a dual-stream transformer architecture that processes syndromes and logical information with asymmetric attention patterns, and a novel differentiable logical loss that directly optimizes Logical Error Rates (LER) through smooth approximations over finite fields. SAQ-Decoder achieves near-optimal performance, with error thresholds of 10.99% (independent noise) and 18.6% (depolarizing noise) on toric codes that approach the ML bounds of 11.0% and 18.9% while outperforming existing neural and classical baselines in accuracy, complexity, and parameter efficiency. Our findings establish that learned decoders can simultaneously achieve competitive decoding accuracy and computational efficiency, addressing key requirements for practical fault-tolerant quantum computing systems.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [212] [Inferring Causal Relationships to Improve Caching for Clients with Correlated Requests: Applications to VR](https://arxiv.org/abs/2512.08626)
*Agrim Bari,Gustavo de Veciana,Yuqi Zhou*

Main category: cs.NI

TL;DR: 本文提出分组客户端请求模型，分析LRU缓存策略，提出新的在线缓存策略LFRU，并通过VR数据集评估，LFRU性能优于LRU和LFU。


<details>
  <summary>Details</summary>
Motivation: 传统缓存策略（LRU和LFU）在特定请求模式下表现好，但现实中客户端请求存在相关性，传统策略难以应对。

Method: 引入分组客户端请求模型，对LRU进行理论分析，提出LFRU策略，用VR数据集评估。

Result: LFRU性能至少和LRU、LFU一样好，在某些场景下，比LRU高2.9倍，比LFU高1.9倍。

Conclusion: LFRU能动态适应客户端请求因果关系，优化缓存淘汰，性能表现优秀。

Abstract: Efficient edge caching reduces latency and alleviates backhaul congestion in modern networks. Traditional caching policies, such as Least Recently Used (LRU) and Least Frequently Used (LFU), perform well under specific request patterns. LRU excels in workloads with strong temporal locality, while LFU is effective when content popularity remains static. However, real-world client requests often exhibit correlations due to shared contexts and coordinated activities. This is particularly evident in Virtual Reality (VR) environments, where groups of clients navigate shared virtual spaces, leading to correlated content requests.
  In this paper, we introduce the \textit{grouped client request model}, a generalization of the Independent Reference Model that explicitly captures different types of request correlations. Our theoretical analysis of LRU under this model reveals that the optimal causal caching policy depends on cache size: LFU is optimal for small to moderate caches, while LRU outperforms it for larger caches. To address the limitations of existing policies, we propose Least Following and Recently Used (LFRU), a novel online caching policy that dynamically infers and adapts to causal relationships in client requests to optimize evictions. LFRU prioritizes objects likely to be requested based on inferred dependencies, achieving near-optimal performance compared to the offline optimal Belady policy in structured correlation settings.
  We develop VR based datasets to evaluate caching policies under realistic correlated requests. Our results show that LFRU consistently performs at least as well as LRU and LFU, outperforming LRU by up to 2.9x and LFU by up to1.9x in certain settings.

</details>


### [213] [Multi-Agent Deep Reinforcement Learning for Collaborative UAV Relay Networks under Jamming Atatcks](https://arxiv.org/abs/2512.08341)
*Thai Duong Nguyen,Ngoc-Tan Nguyen,Thanh-Dao Nguyen,Nguyen Van Huynh,Dinh-Hieu Tran,Symeon Chatzinotas*

Main category: cs.NI

TL;DR: 本文将无人机群作为动态通信中继的部署问题建模为多智能体强化学习问题，用CTDE框架解决，仿真显示该框架表现优于启发式基线，提升系统吞吐量并降低碰撞率，智能体还发展出抗干扰策略。


<details>
  <summary>Details</summary>
Motivation: 在有对抗的环境中部署无人机群作为动态通信中继，现有启发式方法难以解决动态多目标权衡问题。

Method: 将问题建模为合作式多智能体强化学习问题，使用集中训练分散执行（CTDE）框架，利用集中式评论家指导分散式智能体。

Result: 所提框架显著优于启发式基线，系统总吞吐量提高约50%，碰撞率接近零，智能体发展出抗干扰策略。

Conclusion: 所提框架能有效解决无人机群作为动态通信中继的部署问题，智能体可自主学习平衡干扰缓解与有效通信。

Abstract: The deployment of Unmanned Aerial Vehicle (UAV) swarms as dynamic communication relays is critical for next-generation tactical networks. However, operating in contested environments requires solving a complex trade-off, including maximizing system throughput while ensuring collision avoidance and resilience against adversarial jamming. Existing heuristic-based approaches often struggle to find effective solutions due to the dynamic and multi-objective nature of this problem. This paper formulates this challenge as a cooperative Multi-Agent Reinforcement Learning (MARL) problem, solved using the Centralized Training with Decentralized Execution (CTDE) framework. Our approach employs a centralized critic that uses global state information to guide decentralized actors which operate using only local observations. Simulation results show that our proposed framework significantly outperforms heuristic baselines, increasing the total system throughput by approximately 50% while simultaneously achieving a near-zero collision rate. A key finding is that the agents develop an emergent anti-jamming strategy without explicit programming. They learn to intelligently position themselves to balance the trade-off between mitigating interference from jammers and maintaining effective communication links with ground users.

</details>


<div id='physics.bio-ph'></div>

# physics.bio-ph [[Back]](#toc)

### [214] [Data-Efficient Learning of Anomalous Diffusion with Wavelet Representations: Enabling Direct Learning from Experimental Trajectories](https://arxiv.org/abs/2512.08510)
*Gongyi Wang,Yu Zhang,Zihan Huang*

Main category: physics.bio-ph

TL;DR: 提出基于小波的异常扩散表示法，能从实验记录中高效学习，在模拟和实验轨迹数据上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习管道多基于模拟数据训练，应用于实验轨迹数据时性能下降，需解决模拟与实验数据不匹配问题。

Method: 对每个轨迹应用六个互补小波族并组合小波模尺度图构建小波表示。

Result: 在模拟轨迹上少量训练数据就能超越基于特征和轨迹的方法；在实验轨迹上，用于扩散指数回归和网格尺寸分类效果更好，用1200条实验轨迹训练的模型误差低于用百万条模拟轨迹训练的深度学习模型。

Conclusion: 小波表示法能实现数据高效学习，小波谱中出现的不同尺度特征有助于区分潜在扩散机制。

Abstract: Machine learning (ML) has become a versatile tool for analyzing anomalous diffusion trajectories, yet most existing pipelines are trained on large collections of simulated data. In contrast, experimental trajectories, such as those from single-particle tracking (SPT), are typically scarce and may differ substantially from the idealized models used for simulation, leading to degradation or even breakdown of performance when ML methods are applied to real data. To address this mismatch, we introduce a wavelet-based representation of anomalous diffusion that enables data-efficient learning directly from experimental recordings. This representation is constructed by applying six complementary wavelet families to each trajectory and combining the resulting wavelet modulus scalograms. We first evaluate the wavelet representation on simulated trajectories from the andi-datasets benchmark, where it clearly outperforms both feature-based and trajectory-based methods with as few as 1000 training trajectories and still retains an advantage on large training sets. We then use this representation to learn directly from experimental SPT trajectories of fluorescent beads diffusing in F-actin networks, where the wavelet representation remains superior to existing alternatives for both diffusion-exponent regression and mesh-size classification. In particular, when predicting the diffusion exponents of experimental trajectories, a model trained on 1200 experimental tracks using the wavelet representation achieves significantly lower errors than state-of-the-art deep learning models trained purely on $10^6$ simulated trajectories. We associate this data efficiency with the emergence of distinct scale fingerprints disentangling underlying diffusion mechanisms in the wavelet spectra.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [215] [Resonant and Stochastic Vibration in Neurorehabilitation](https://arxiv.org/abs/2512.08009)
*Ava Hays,Nolan Kosnic,Ryan Miller,Kunal Siddhawar*

Main category: cs.ET

TL;DR: 本文综述振动干预在神经康复中的应用，涉及全身振动、局部肌肉振动等，评估其疗效、挑战并指出研究需求。


<details>
  <summary>Details</summary>
Motivation: 神经系统损伤和年龄相关衰退影响运动功能，基于神经可塑性机制，振动干预可刺激感觉和运动神经回路，有康复潜力。

Method: 对随机和共振振动模式进行综述，分析全身振动案例，评估特定肌肉振动和可穿戴设备应用。

Result: 全身振动可改善老年人、中风患者和帕金森病患者的平衡、活动能力和精细运动功能；聚焦肌肉振动和可穿戴设备在上肢康复方面有临床潜力但存在局限。

Conclusion: 需明确影响治疗效果的关键变量，完善方案、提高可用性，将振动技术融入神经康复框架，并明确重要研究需求以转化为可靠临床工具。

Abstract: Neurological injuries and age-related decline can impair sensory processing and disrupt motor coordination, gait, and balance. As mechanisms of neuroplasticity have become better understood, vibration-based interventions have gained attention as potential tools to stimulate sensory pathways and motor circuits to support functional recovery. This survey reviews stochastic and resonant vibration modalities, describing their mechanisms, therapeutic rationales, and clinical applications. We synthesize evidence on whole-body vibration for improving balance, mobility, and fine motor function in aging adults, stroke survivors, and individuals with Parkinson's disease, with attention to challenges in parameter optimization, generalizability, and safety. We also assess recent developments in focused muscle vibration and wearable stochastic resonance devices for upper-limb rehabilitation, evaluating their clinical promise along with limitations in scalability, ecological validity, and standardization. Across these modalities, we identify key variables that shape therapeutic outcomes and highlight ongoing efforts to refine protocols, improve usability, and integrate vibration techniques into broader neurorehabilitation frameworks. We conclude by outlining the most important research needs for translating vibration-based interventions into reliable and deployable clinical tools.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [216] [Uncertainty quantification for mixed membership in multilayer networks with degree heterogeneity using Gaussian variational inference](https://arxiv.org/abs/2512.08146)
*Fangzheng Xie,Hsin-Hsiung Huang*

Main category: stat.ME

TL;DR: 本文基于ML - DCMM模型提出贝叶斯推理框架和高斯变分推理算法，并在理论上给出保证，通过美国机场纵向网络验证方法有效性。


<details>
  <summary>Details</summary>
Motivation: 在分析多层网络时，需要在跨层整合信息并考虑层特异性异质性的同时，量化社区结构的不确定性。

Method: 基于多层度校正混合成员模型提出基于谱辅助似然的贝叶斯推理框架，开发基于随机梯度下降的高斯变分推理算法。

Result: 理论上建立变分Bernstein - von Mises定理，美国机场纵向网络的实验中该方法产生稳健估计、自然的不确定性量化，且性能优于现有方法。

Conclusion: 所提出的方法在多层网络分析中有效，可用于量化社区结构不确定性。

Abstract: Analyzing multilayer networks is central to understanding complex relational measurements collected across multiple conditions or over time. A pivotal task in this setting is to quantify uncertainty in community structure while appropriately pooling information across layers and accommodating layer-specific heterogeneity. Building on the multilayer degree-corrected mixed-membership (ML-DCMM) model, which captures both stable community membership profiles and layer-specific vertex activity levels, we propose a Bayesian inference framework based on a spectral-assisted likelihood. We then develop a computationally efficient Gaussian variational inference algorithm implemented via stochastic gradient descent. Our theoretical analysis establishes a variational Bernstein--von Mises theorem, which provides a frequentist guarantee for using the variational posterior to construct confidence sets for mixed memberships. We demonstrate the utility of the method on a U.S. airport longitudinal network, where the procedure yields robust estimates, natural uncertainty quantification, and competitive performance relative to state-of-the-art methods.

</details>


### [217] [Bayesian Semiparametric Mixture Cure (Frailty) Models](https://arxiv.org/abs/2512.08173)
*Fatih Kızılaslan,Valeria Vitelli*

Main category: stat.ME

TL;DR: 提出半参数混合治愈模型的分层贝叶斯框架，进行模拟研究和模型评估并应用于两个数据集。


<details>
  <summary>Details</summary>
Motivation: 混合物治愈模型在生存分析受关注，提出新框架以更灵活捕捉患者未观察到的异质性。

Method: 提出分层贝叶斯框架，用马尔可夫链蒙特卡罗方法获取后验分布样本，采用综合模拟研究评估，用多种标准进行贝叶斯模型比较和评估。

Result: 通过模拟研究评估了模型性能和鲁棒性，应用于两个知名数据集。

Conclusion: 所提出方法为半参数混合治愈模型分析提供了一个灵活且有效的途径。

Abstract: In recent years, mixture cure models have gained increasing popularity in survival analysis as an alternative to the Cox proportional hazards model, particularly in settings where a subset of patients is considered cured. The proportional hazards mixture cure model is especially advantageous when the presence of a cured fraction can be reasonably assumed, providing a more accurate representation of long-term survival dynamics. In this study, we propose a novel hierarchical Bayesian framework for the semiparametric mixture cure model, which accommodates both the inclusion and exclusion of a frailty component, allowing for greater flexibility in capturing unobserved heterogeneity among patients. Samples from the posterior distribution are obtained using a Markov chain Monte Carlo method, leveraging a hierarchical structure inspired by Bayesian Lasso. Comprehensive simulation studies are conducted across diverse scenarios to evaluate the performance and robustness of the proposed models. Bayesian model comparison and assessment are performed using various criteria. Finally, the proposed approaches are applied to two well-known datasets in the cure model literature: the E1690 melanoma trial and a colon cancer clinical trial.

</details>


### [218] [Distributional Random Forests for Complex Survey Designs on Reproducing Kernel Hilbert Spaces](https://arxiv.org/abs/2512.08179)
*Yating Zou,Marcos Matabuena,Michael R. Kosorok*

Main category: stat.ME

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study estimation of the conditional law $P(Y|X=\mathbf{x})$ and continuous functionals $Ψ(P(Y|X=\mathbf{x}))$ when $Y$ takes values in a locally compact Polish space, $X \in \mathbb{R}^p$, and the observations arise from a complex survey design. We propose a survey-calibrated distributional random forest (SDRF) that incorporates complex-design features via a pseudo-population bootstrap, PSU-level honesty, and a Maximum Mean Discrepancy (MMD) split criterion computed from kernel mean embeddings of Hájek-type (design-weighted) node distributions. We provide a framework for analyzing forest-style estimators under survey designs; establish design consistency for the finite-population target and model consistency for the super-population target under explicit conditions on the design, kernel, resampling multipliers, and tree partitions. As far as we are aware, these are the first results on model-free estimation of conditional distributions under survey designs. Simulations under a stratified two-stage cluster design provide finite sample performance and demonstrate the statistical error price of ignoring the survey design. The broad applicability of SDRF is demonstrated using NHANES: We estimate the tolerance regions of the conditional joint distribution of two diabetes biomarkers, illustrating how distributional heterogeneity can support subgroup-specific risk profiling for diabetes mellitus in the U.S. population.

</details>


### [219] [Prediction Intervals for Individual Treatment Effects in a Multiple Decision Point Framework using Conformal Inference](https://arxiv.org/abs/2512.08828)
*Swaraj Bose,Walter Dempsey*

Main category: stat.ME

TL;DR: 本文提出一种为随时间变化的个体治疗效果（ITEs）构建预测区间的新方法，用模拟和实际数据验证其效果。


<details>
  <summary>Details</summary>
Motivation: 在医疗、金融等领域，精准量化多个决策点上的ITEs不确定性对个性化决策至关重要，但以往研究存在局限。

Method: 使用共形推断技术，在比以往文献更弱的假设下，为随时间变化的ITEs构建预测区间。

Result: 保证了预测区间覆盖率的下界，该下界取决于数据中的不可交换性程度，并通过模拟和实际MRT数据验证方法的实用性。

Conclusion: 所提出的方法在多个决策场景中具有广泛适用性，通过模拟和实际数据显示有良好表现。

Abstract: Accurately quantifying uncertainty of individual treatment effects (ITEs) across multiple decision points is crucial for personalized decision-making in fields such as healthcare, finance, education, and online marketplaces. Previous work has focused on predicting non-causal longitudinal estimands or constructing prediction bands for ITEs using cross-sectional data based on exchangeability assumptions. We propose a novel method for constructing prediction intervals using conformal inference techniques for time-varying ITEs with weaker assumptions than prior literature. We guarantee a lower bound for coverage, which is dependent on the degree of non-exchangeability in the data. Although our method is broadly applicable across decision-making contexts, we support our theoretical claims with simulations emulating micro-randomized trials (MRTs) -- a sequential experimental design for mobile health (mHealth) studies. We demonstrate the practical utility of our method by applying it to a real-world MRT - the Intern Health Study (IHS).

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [220] [Examining Student Interactions with a Pedagogical AI-Assistant for Essay Writing and their Impact on Students Writing Quality](https://arxiv.org/abs/2512.08596)
*Wicaksono Febriantoro,Qi Zhou,Wannapon Suraworachet,Sahan Bulathwela,Andrea Gauthier,Eva Millan,Mutlu Cukurova*

Main category: cs.CY

TL;DR: 研究评估GenAI驱动的论文写作助手，发现学生使用行为有两类模式，且积极互动者表现更好，对教学和系统设计有启示。


<details>
  <summary>Details</summary>
Motivation: 探索学生与GenAI互动动态及与写作质量关系，弥补对教学设计系统在写作过程各阶段互动研究的不足。

Method: 收集32名本科生两小时写作互动日志，用序列模式挖掘和K - Means聚类分析行为模式，用Mann - Whitney U检验，还有定性分析。

Result: 出现两类行为模式，在论文组织维度有中等效应量，积极互动学生表现更好。

Conclusion: 对教学和系统设计有启示，教师应鼓励积极参与，未来EWA可集成自动标记和监控。

Abstract: The dynamic nature of interactions between students and GenAI, as well as their relationship to writing quality, remains underexplored. While most research has examined how general-purpose GenAI can support writing, fewer studies have investigated how students interact with pedagogically designed systems across different phases of the writing process. To address this gap, we evaluated a GenAI-driven essay-writing assistant (EWA) designed to support higher education students in argumentative writing. Drawing on 1,282 interaction logs from 32 undergraduates during a two-hour writing session, Sequential Pattern Mining and K-Means clustering were used to identify behavioral patterns. Two clusters emerged: Cluster 1 emphasized outline planning and essay structure, while Cluster 2 focused on content development. A Mann-Whitney U test revealed a moderate effect size (r = 0.36) in the essay Organization dimension, with Cluster 1 showing higher scores. Qualitative analysis indicated that students with better performance actively wrote and shared essay sections with EWA for feedback, rather than interacted passively by asking questions. These findings suggest implications for teaching and system design. Teachers can encourage active engagement, while future EWAs may integrate automatic labeling and monitoring to prompt students to move from questioning to writing, enabling fuller benefits from GenAI-supported learning.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [221] [An Examination of Bitcoin's Structural Shortcomings as Money: A Synthesis of Economic and Technical Critiques](https://arxiv.org/abs/2512.07840)
*Hamoon Soleimani*

Main category: econ.GN

TL;DR: 本文批判性评估比特币作为货币的说法，综合两派经济学观点及实证分析，认为比特币是投机资产而非货币。


<details>
  <summary>Details</summary>
Motivation: 评估比特币作为国家货币革命性替代方案这一说法，判断其是否能成为货币。

Method: 综合后凯恩斯主义和奥地利学派的批判观点，进行理论分析，并结合比特币极端波动性、可扩展性限制等方面的实证分析。

Result: 从后凯恩斯主义看，比特币不是基于债务的借据，不具备稳定货币资产属性；从奥地利学派看，不符合米塞斯回归定理，缺乏非货币价值且未成为最易销售商品。

Conclusion: 比特币更准确地应被视为新型投机资产，主要贡献可能是技术创新，而非作为货币标准的可行性。

Abstract: Since its inception, Bitcoin has been positioned as a revolutionary alternative to national currencies, attracting immense public and academic interest. This paper presents a critical evaluation of this claim, suggesting that Bitcoin faces significant structural barriers to qualifying as money. It synthesizes critiques from two distinct schools of economic thought - Post-Keynesianism and the Austrian School - and validates their conclusions with rigorous technical analysis. From a Post-Keynesian perspective, it is argued that Bitcoin does not function as money because it is not a debt-based IOU and fails to exhibit the essential properties required for a stable monetary asset (Vianna, 2021). Concurrently, from an Austrian viewpoint, it is shown to be inconsistent with a strict interpretation of Mises's Regression Theorem, as it lacks prior non-monetary value and has not achieved the status of the most saleable commodity (Peniaz and Kavaliou, 2024). These theoretical arguments are then supported by an empirical analysis of Bitcoin's extreme volatility, hard-coded scalability limits, fragile market structure, and insecure long-term economic design. The paper concludes that Bitcoin is more accurately characterized as a novel speculative asset whose primary legacy may be the technological innovation it has spurred, rather than its viability as a monetary standard.

</details>


### [222] [When Medical AI Explanations Help and When They Harm](https://arxiv.org/abs/2512.08424)
*Manshu Khanna,Ziyi Wang,Lijia Wei,Lian Xue*

Main category: econ.GN

TL;DR: 研究揭示AI透明性悖论，即算法正确时解释提高决策质量，错误时则降低。实验表明该现象存在，且不同医生表现不同，选择性透明比普遍透明创造更多医疗价值。


<details>
  <summary>Details</summary>
Motivation: 探究AI透明性在决策中带来的影响

Method: 通过257名医学生进行3855次诊断决策实验

Result: AI正确时解释提高准确率6.3个百分点，错误时降低4.9个百分点；医生高估AI准确率15.2个百分点；不同能力医生表现不同；选择性透明比普遍透明创造更多医疗价值。

Conclusion: AI透明性存在悖论，选择性透明更具价值。

Abstract: We document a fundamental paradox in AI transparency: explanations improve decisions when algorithms are correct but systematically worsen them when algorithms err. In an experiment with 257 medical students making 3,855 diagnostic decisions, we find explanations increase accuracy by 6.3 percentage points when AI is correct (73% of cases) but decrease it by 4.9 points when incorrect (27% of cases). This asymmetry arises because modern AI systems generate equally persuasive explanations regardless of recommendation quality-physicians cannot distinguish helpful from misleading guidance. We show physicians treat explained AI as 15.2 percentage points more accurate than reality, with over-reliance persisting even for erroneous recommendations. Competent physicians with appropriate uncertainty suffer most from the AI transparency paradox (-12.4pp when AI errs), while overconfident novices benefit most (+9.9pp net). Welfare analysis reveals that selective transparency generates \$2.59 billion in annual healthcare value, 43% more than the \$1.82 billion from mandated universal transparency.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [223] [Expectations in Expectation Propagation](https://arxiv.org/abs/2512.08034)
*Zilu Zhao,Fangqing Xiao,Dirk Slock*

Main category: cs.IT

TL;DR: 本文研究线性模型中的期望传播（EP）算法，分析信念关系，提出避免算法因无限积分值消息受阻及产生此类消息的方法。


<details>
  <summary>Details</summary>
Motivation: 高斯投影EP中的负方差消息会阻碍算法进展，需解决算法因无限积分值消息受阻的问题。

Method: 研究线性模型中的EP，分析对应信念的关系，提出非持久和持久方法，以及基于消息关系的额外方法。

Result: 未明确提及，但可推测提出的方法有望解决算法受阻问题。

Conclusion: 通过对线性模型中EP消息和信念关系的研究，提出有效避免算法受阻的方法。

Abstract: Expectation Propagation (EP) is a widely used message-passing algorithm that decomposes a global inference problem into multiple local ones. It approximates marginal distributions (beliefs) using intermediate functions (messages). While beliefs must be proper probability distributions that integrate to one, messages may have infinite integral values. In Gaussian-projected EP, such messages take a Gaussian form and appear as if they have "negative" variances. Although allowed within the EP framework, these negative-variance messages can impede algorithmic progress.
  In this paper, we investigate EP in linear models and analyze the relationship between the corresponding beliefs. Based on the analysis, we propose both non-persistent and persistent approaches that prevent the algorithm from being blocked by messages with infinite integral values.
  Furthermore, by examining the relationship between the EP messages in linear models, we develop an additional approach that avoids the occurrence of messages with infinite integral values.

</details>


<div id='astro-ph.SR'></div>

# astro-ph.SR [[Back]](#toc)

### [224] [Magnetic activity of ultracool dwarfs in the LAMOST DR11](https://arxiv.org/abs/2512.08305)
*Yue Xiang,Shenghong Gu,Dongtao Cao*

Main category: astro-ph.SR

TL;DR: 从LAMOST数据中识别962颗超冷矮星，模拟CSST光谱，用半监督机器学习识别，研究磁活动和自转周期及关系。


<details>
  <summary>Details</summary>
Motivation: 超冷矮星表面下磁场产生过程理解不足且有争议，需增加活跃超冷矮星样本。

Method: 从LAMOST DR11识别超冷矮星，模拟CSST光谱，用半监督机器学习模型识别，用Hα线研究磁活动，基于Kepler/K2光变曲线推导自转周期。

Result: 识别出962颗超冷矮星，展示CSST巡天能力，推导出82颗超冷矮星自转周期。

Conclusion: 超冷矮星的活动 - 自转关系在罗斯比数约0.12时饱和。

Abstract: Ultracool dwarfs consist of lowest-mass stars and brown dwarfs. Their interior is fully convective, different from that of the partly-convective Sun-like stars. Magnetic field generation process beneath the surface of ultracool dwarfs is still poorly understood and controversial. To increase samples of active ultracool dwarfs significantly, we have identified 962 ultracool dwarfs in the latest LAMOST data release, DR11. We also simulate the Chinese Space Station Survey Telescope (CSST) low-resolution slitless spectra by degrading the LAMOST spectra. A semi-supervised machine learning approach with an autoencoder model is built to identify ultracool dwarfs with the simulated CSST spectra, which demonstrates the capability of the CSST all-sky slitless spectroscopic survey on the detection of ultracool dwarfs. Magnetic activity of the ultracool dwarfs is investigated by using the H$α$ line emission as a proxy. The rotational periods of 82 ultracool dwarfs are derived based on the Kepler/K2 light curves. We also derive the activity-rotation relation of the ultracool dwarfs, which is saturated around a Rossby number of 0.12.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [225] [Joint Activity Design Heuristics for Enhancing Human-Machine Collaboration](https://arxiv.org/abs/2512.08036)
*Mohammadreza Jalaeian,Dane A. Morey,Michael F. Rayo*

Main category: cs.HC

TL;DR: 本文聚焦联合活动，提出设计需支持团队五项宏观认知功能，综合十四项启发式方法辅助人机联合活动技术设计。


<details>
  <summary>Details</summary>
Motivation: 探讨如何设计技术以支持人机联合活动，使其成为有效团队成员，在可用性设计基础上进一步发展。

Method: 从显示设计、人因工程等多领域相关文献中综合十四项启发式方法。

Result: 得出十四项可辅助支持人机联合活动技术设计、开发和评估的启发式方法。

Conclusion: 支持团队五项宏观认知功能与技术可用性同样重要，十四项启发式方法有助于相关技术设计。

Abstract: Joint activity describes when more than one agent (human or machine) contributes to the completion of a task or activity. Designing for joint activity focuses on explicitly supporting the interdependencies between agents necessary for effective coordination among agents engaged in the joint activity. This builds and expands upon designing for usability to further address how technologies can be designed to act as effective team players. Effective joint activity requires supporting, at minimum, five primary macrocognitive functions within teams: Event Detection, Sensemaking, Adaptability, Perspective-Shifting, and Coordination. Supporting these functions is equally as important as making technologies usable. We synthesized fourteen heuristics from relevant literature including display design, human factors, cognitive systems engineering, cognitive psychology, and computer science to aid the design, development, and evaluation of technologies that support joint human-machine activity.

</details>


### [226] [A Comparative Study of EMG- and IMU-based Gesture Recognition at the Wrist and Forearm](https://arxiv.org/abs/2512.07997)
*Soroush Baghernezhad,Elaheh Mohammadreza,Vinicius Prado da Fonseca,Ting Zou,Xianta Jiang*

Main category: cs.HC

TL;DR: 研究探讨用不同肌肉群的IMU信号进行手势识别，发现其足够用于静态手势识别且肌腱微动贡献大，有望提升假肢可用性和应用于多领域。


<details>
  <summary>Details</summary>
Motivation: 已有研究多关注sEMG，IMU作为较少探索的替代方式能提供肌肉微动的补充信息，因此研究其用于手势识别的潜力。

Method: 研究不同肌肉群的IMU信号，将其作为唯一输入传感器进行静态手势识别，对比不同肌肉群并检查单个肌肉群的模式识别质量。

Result: IMU信号包含足够信息用于静态手势识别；肌腱引起的微动是静态手势识别的主要贡献者。

Conclusion: 利用肌肉微动信息能提升截肢者假肢手臂的可用性，且为机器人、遥操作等领域的手势识别提供新可能。

Abstract: Gestures are an integral part of our daily interactions with the environment. Hand gesture recognition (HGR) is the process of interpreting human intent through various input modalities, such as visual data (images and videos) and bio-signals. Bio-signals are widely used in HGR due to their ability to be captured non-invasively via sensors placed on the arm. Among these, surface electromyography (sEMG), which measures the electrical activity of muscles, is the most extensively studied modality. However, less-explored alternatives such as inertial measurement units (IMUs) can provide complementary information on subtle muscle movements, which makes them valuable for gesture recognition. In this study, we investigate the potential of using IMU signals from different muscle groups to capture user intent. Our results demonstrate that IMU signals contain sufficient information to serve as the sole input sensor for static gesture recognition. Moreover, we compare different muscle groups and check the quality of pattern recognition on individual muscle groups. We further found that tendon-induced micro-movement captured by IMUs is a major contributor to static gesture recognition. We believe that leveraging muscle micro-movement information can enhance the usability of prosthetic arms for amputees. This approach also offers new possibilities for hand gesture recognition in fields such as robotics, teleoperation, sign language interpretation, and beyond.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [227] [Low Rank Support Quaternion Matrix Machine](https://arxiv.org/abs/2512.08327)
*Wang Chen,Ziyan Luo,Shuangyue Wang*

Main category: cs.CV

TL;DR: 提出低秩支持四元数矩阵机（LSQMM）用于彩色图像分类，实验显示其在准确率、鲁棒性和计算效率上有优势。


<details>
  <summary>Details</summary>
Motivation: 受四元数数据建模在图像恢复和去噪任务成功启发，解决传统彩色图像分类输入特征表示问题。

Method: 将RGB通道视为纯四元数，在LSQMM模型的铰链损失中添加四元数核范数正则项，设计基于交替方向乘子法（ADMM）的迭代算法求解模型。

Result: 在多个彩色图像分类数据集实验表明该方法在分类准确率、鲁棒性和计算效率方面优于几种先进方法。

Conclusion: 提出的LSQMM方法在彩色图像分类任务中表现良好，有一定优势。

Abstract: Input features are conventionally represented as vectors, matrices, or third order tensors in the real field, for color image classification. Inspired by the success of quaternion data modeling for color images in image recovery and denoising tasks, we propose a novel classification method for color image classification, named as the Low-rank Support Quaternion Matrix Machine (LSQMM), in which the RGB channels are treated as pure quaternions to effectively preserve the intrinsic coupling relationships among channels via the quaternion algebra. For the purpose of promoting low-rank structures resulting from strongly correlated color channels, a quaternion nuclear norm regularization term, serving as a natural extension of the conventional matrix nuclear norm to the quaternion domain, is added to the hinge loss in our LSQMM model. An Alternating Direction Method of Multipliers (ADMM)-based iterative algorithm is designed to effectively resolve the proposed quaternion optimization model. Experimental results on multiple color image classification datasets demonstrate that our proposed classification approach exhibits advantages in classification accuracy, robustness and computational efficiency, compared to several state-of-the-art methods using support vector machines, support matrix machines, and support tensor machines.

</details>


### [228] [Skewness-Guided Pruning of Multimodal Swin Transformers for Federated Skin Lesion Classification on Edge Devices](https://arxiv.org/abs/2512.08751)
*Kuniko Paxton,Koorosh Aslansefat,Dhavalkumar Thakker,Yiannis Papadopoulos*

Main category: cs.CV

TL;DR: 提出偏度引导剪枝方法用于多模态医学AI，在边缘设备上实现高效模型压缩和隐私保护分布式学习，实验使模型大小减少约36%且精度无损失。


<details>
  <summary>Details</summary>
Motivation: 高性能计算机视觉模型计算密集、体积大，不适合边缘设备部署，且严格隐私约束促使采用联邦学习。

Method: 提出基于输出分布统计偏度，对多模态Swin Transformer的多头自注意力和多层感知机层进行选择性剪枝的偏度引导剪枝方法。

Result: 该方法在水平联邦学习环境中验证，能保持性能同时大幅降低模型复杂度，在紧凑Swin Transformer实验中模型大小减少约36%且精度无损失。

Conclusion: 在边缘设备上实现多模态医学AI的高效模型压缩和隐私保护分布式学习是可行的。

Abstract: In recent years, high-performance computer vision models have achieved remarkable success in medical imaging, with some skin lesion classification systems even surpassing dermatology specialists in diagnostic accuracy. However, such models are computationally intensive and large in size, making them unsuitable for deployment on edge devices. In addition, strict privacy constraints hinder centralized data management, motivating the adoption of Federated Learning (FL). To address these challenges, this study proposes a skewness-guided pruning method that selectively prunes the Multi-Head Self-Attention and Multi-Layer Perceptron layers of a multimodal Swin Transformer based on the statistical skewness of their output distributions. The proposed method was validated in a horizontal FL environment and shown to maintain performance while substantially reducing model complexity. Experiments on the compact Swin Transformer demonstrate approximately 36\% model size reduction with no loss in accuracy. These findings highlight the feasibility of achieving efficient model compression and privacy-preserving distributed learning for multimodal medical AI on edge devices.

</details>


### [229] [MELLA: Bridging Linguistic Capability and Cultural Groundedness for Low-Resource Language MLLMs](https://arxiv.org/abs/2508.05502)
*Yufei Gao,Jiaying Fei,Nuo Chen,Ruirui Chen,Guohang Yan,Yunshi Lan,Botian Shi*

Main category: cs.CV

TL;DR: 现有多模态大语言模型在低资源语言场景效果不佳，本文提出双源策略构建MELLA数据集，微调后模型性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在低资源语言场景效果差，现有多语言增强方法存在局限，忽略多模态信息和文化相关性。

Method: 提出双源策略，收集特定数据，构建MELLA多模态多语言数据集。

Result: 在MELLA上微调后，多种MLLM骨干模型在八种语言上性能普遍提升，能生成更丰富描述。

Conclusion: 性能提升源于文化知识和语言能力增强，数据集可公开获取。

Abstract: Multimodal Large Language Models (MLLMs) have shown remarkable performance in high-resource languages. However, their effectiveness diminishes significantly in the contexts of low-resource languages. Current multilingual enhancement methods are often limited to text modality or rely solely on machine translation. While such approaches help models acquire basic linguistic capabilities and produce "thin descriptions", they neglect the importance of multimodal informativeness and cultural groundedness, both of which are crucial for serving low-resource language users effectively. To bridge this gap, in this study, we identify two significant objectives for a truly effective MLLM in low-resource language settings, namely 1) linguistic capability and 2) cultural groundedness, placing special emphasis on cultural awareness. To achieve these dual objectives, we propose a dual-source strategy that guides the collection of data tailored to each goal, sourcing native web alt-text for culture and MLLM-generated captions for linguistics. As a concrete implementation, we introduce MELLA, a multimodal, multilingual dataset. Experiment results show that after fine-tuning on MELLA, there is a general performance improvement for the eight languages on various MLLM backbones, with models producing "thick descriptions". We verify that the performance gains are from both cultural knowledge enhancement and linguistic capability enhancement. Our dataset can be found at https://opendatalab.com/applyMultilingualCorpus.

</details>


### [230] [Near-real time fires detection using satellite imagery in Sudan conflict](https://arxiv.org/abs/2512.07925)
*Kuldip Singh Atwal,Dieter Pfoser,Daniel Rothbart*

Main category: cs.CV

TL;DR: 本文利用深度学习和卫星遥感影像，对苏丹武装冲突中的火灾损害进行近实时监测，通过案例证明方法有效，且8波段影像或时间序列影像增益不大。


<details>
  <summary>Details</summary>
Motivation: 苏丹持续的战争带来挑战，需要对冲突进行快速监测和分析，深度学习和可用卫星遥感影像为此提供可能。

Method: 使用Planet Labs的4波段影像和深度学习模型，进行五个案例研究。

Result: 与基线相比，自动方法能更准确捕捉火灾和烧焦区域，使用8波段影像或时间序列影像增益微小。

Conclusion: 利用4波段影像和深度学习模型可对武装冲突中的火灾损害进行低延迟监测。

Abstract: The challenges of ongoing war in Sudan highlight the need for rapid monitoring and analysis of such conflicts. Advances in deep learning and readily available satellite remote sensing imagery allow for near real-time monitoring. This paper uses 4-band imagery from Planet Labs with a deep learning model to show that fire damage in armed conflicts can be monitored with minimal delay. We demonstrate the effectiveness of our approach using five case studies in Sudan. We show that, compared to a baseline, the automated method captures the active fires and charred areas more accurately. Our results indicate that using 8-band imagery or time series of such imagery only result in marginal gains.

</details>


### [231] [Restrictive Hierarchical Semantic Segmentation for Stratified Tooth Layer Detection](https://arxiv.org/abs/2512.07984)
*Ryan Banks,Camila Lindoni Azevedo,Hongying Tang,Yunpeng Li*

Main category: cs.CV

TL;DR: 提出嵌入解剖层次结构的语义分割框架，在牙科影像数据集验证，提升性能和临床合理性。


<details>
  <summary>Details</summary>
Motivation: 现有层次感知分割方法对解剖结构编码弱且间接，需更好方法准确理解解剖结构以对牙科疾病分期。

Method: 结合循环逐层级预测方案、限制性输出头和自上而下特征调节，在类树每层重新运行骨干网络，使用特征线性调制调节子特征空间，用概率合成规则保证父子类一致性，采用分层损失。

Result: 在TL - pano数据集上，使用UNet和HRNet作为模型，分层变体提高IoU、Dice和召回率，但召回率高于精确率，有更多假阳性。

Conclusion: 显式分层结构能提升性能和临床合理性，尤其在低数据牙科影像场景。

Abstract: Accurate understanding of anatomical structures is essential for reliably staging certain dental diseases. A way of introducing this within semantic segmentation models is by utilising hierarchy-aware methodologies. However, existing hierarchy-aware segmentation methods largely encode anatomical structure through the loss functions, providing weak and indirect supervision. We introduce a general framework that embeds an explicit anatomical hierarchy into semantic segmentation by coupling a recurrent, level-wise prediction scheme with restrictive output heads and top-down feature conditioning. At each depth of the class tree, the backbone is re-run on the original image concatenated with logits from the previous level. Child class features are conditioned using Feature-wise Linear Modulation of their parent class probabilities, to modulate child feature spaces for fine grained detection. A probabilistic composition rule enforces consistency between parent and descendant classes. Hierarchical loss combines per-level class weighted Dice and cross entropy loss and a consistency term loss, ensuring parent predictions are the sum of their children. We validate our approach on our proposed dataset, TL-pano, containing 194 panoramic radiographs with dense instance and semantic segmentation annotations, of tooth layers and alveolar bone. Utilising UNet and HRNet as donor models across a 5-fold cross validation scheme, the hierarchical variants consistently increase IoU, Dice, and recall, particularly for fine-grained anatomies, and produce more anatomically coherent masks. However, hierarchical variants also demonstrated increased recall over precision, implying increased false positives. The results demonstrate that explicit hierarchical structuring improves both performance and clinical plausibility, especially in low data dental imaging regimes.

</details>


### [232] [FRIEDA: Benchmarking Multi-Step Cartographic Reasoning in Vision-Language Models](https://arxiv.org/abs/2512.08016)
*Jiyoon Pyo,Yuankun Jiao,Dongwon Jung,Zekun Li,Leeje Jang,Sofia Kirsanova,Jina Kim,Yijun Lin,Qin Liu,Junyi Xie,Hadi Askari,Nan Xu,Muhao Chen,Yao-Yi Chiang*

Main category: cs.CV

TL;DR: 文章指出地图视觉问答推理能力缺乏评估，引入 FRIEDA 基准测试，评估多种模型，结果显示模型与人类表现差距大。


<details>
  <summary>Details</summary>
Motivation: 地图推理能力重要但缺乏有效评估，现有图表评估方法不适用于地图视觉问答。

Method: 引入 FRIEDA 基准，从多领域多地区获取真实地图图像，针对三类空间关系出题，在两种设置下评估十一个先进的大视觉语言模型。

Result: 最强模型 Gemini - 2.5 - Pro 和 GPT - 5 - Think 准确率分别仅为 38.20%和 37.20%，远低于人类的 84.87%。

Conclusion: 当前多步地图推理中模型与人类存在差距，FRIEDA 可推动大视觉语言模型空间智能的发展。

Abstract: Cartographic reasoning is the skill of interpreting geographic relationships by aligning legends, map scales, compass directions, map texts, and geometries across one or more map images. Although essential as a concrete cognitive capability and for critical tasks such as disaster response and urban planning, it remains largely unevaluated. Building on progress in chart and infographic understanding, recent large vision language model studies on map visual question-answering often treat maps as a special case of charts. In contrast, map VQA demands comprehension of layered symbology (e.g., symbols, geometries, and text labels) as well as spatial relations tied to orientation and distance that often span multiple maps and are not captured by chart-style evaluations. To address this gap, we introduce FRIEDA, a benchmark for testing complex open-ended cartographic reasoning in LVLMs. FRIEDA sources real map images from documents and reports in various domains and geographical areas. Following classifications in Geographic Information System (GIS) literature, FRIEDA targets all three categories of spatial relations: topological (border, equal, intersect, within), metric (distance), and directional (orientation). All questions require multi-step inference, and many require cross-map grounding and reasoning. We evaluate eleven state-of-the-art LVLMs under two settings: (1) the direct setting, where we provide the maps relevant to the question, and (2) the contextual setting, where the model may have to identify the maps relevant to the question before reasoning. Even the strongest models, Gemini-2.5-Pro and GPT-5-Think, achieve only 38.20% and 37.20% accuracy, respectively, far below human performance of 84.87%. These results reveal a persistent gap in multi-step cartographic reasoning, positioning FRIEDA as a rigorous benchmark to drive progress on spatial intelligence in LVLMs.

</details>


### [233] [MM-CoT:A Benchmark for Probing Visual Chain-of-Thought Reasoning in Multimodal Models](https://arxiv.org/abs/2512.08228)
*Jusheng Zhang,Kaitong Cai,Xiaoyang Guo,Sidi Liu,Qinhan Lv,Ruiqi Chen,Jing Yang,Yijia Fan,Xiaofei Sun,Jian Wang,Ziliang Chen,Liang Lin,Keze Wang*

Main category: cs.CV

TL;DR: 引入诊断基准MM - CoT评估多模态模型思维链推理的视觉基础和逻辑连贯性，评估发现先进模型也存在问题，且该基准与现有基准相关性低，为未来模型发展提供基础。


<details>
  <summary>Details</summary>
Motivation: 现有基准注重生成而忽视验证多模态模型思维链推理是否基于视觉证据且逻辑连贯，需填补此空白。

Method: 引入MM - CoT基准，模型需选择满足视觉一致性和逻辑连贯性两个约束的事件链，设置对抗性干扰项暴露推理失败情况。

Result: 在MM - CoT上评估领先的视觉语言模型，发现即使先进系统也有困难，该基准与现有基准相关性低。

Conclusion: MM - CoT基准为开发在视觉世界中进行忠实、连贯推理的未来模型提供基础。

Abstract: The ability to perform Chain-of-Thought (CoT) reasoning marks a major milestone for multimodal models (MMs), enabling them to solve complex visual reasoning problems. Yet a critical question remains: is such reasoning genuinely grounded in visual evidence and logically coherent? Existing benchmarks emphasize generation but neglect verification, i.e., the capacity to assess whether a reasoning chain is both visually consistent and logically valid. To fill this gap, we introduce MM-CoT, a diagnostic benchmark specifically designed to probe the visual grounding and logical coherence of CoT reasoning in MMs. Instead of generating free-form explanations, models must select the sole event chain that satisfies two orthogonal constraints: (i) visual consistency, ensuring all steps are anchored in observable evidence, and (ii) logical coherence, ensuring causal and commonsense validity. Adversarial distractors are engineered to violate one of these constraints, exposing distinct reasoning failures. We evaluate leading vision-language models on MM-CoT and find that even the most advanced systems struggle, revealing a sharp discrepancy between generative fluency and true reasoning fidelity. MM-CoT shows low correlation with existing benchmarks, confirming that it measures a unique combination of visual grounding and logical reasoning. This benchmark provides a foundation for developing future models that reason not just plausibly, but faithfully and coherently within the visual world.

</details>


### [234] [HybridToken-VLM: Hybrid Token Compression for Vision-Language Models](https://arxiv.org/abs/2512.08240)
*Jusheng Zhang,Xiaoyang Guo,Kaitong Cai,Qinhan Lv,Yijia Fan,Wenhao Chai,Jian Wang,Keze Wang*

Main category: cs.CV

TL;DR: 提出HTC - VLM框架解决视觉语言模型计算成本高的问题，在多基准测试中表现良好并验证了语义引导作用。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型将大量视觉补丁令牌输入大语言模型会产生二次计算成本，且传统方法在连续压缩和离散量化上存在问题。

Method: 引入HTC - VLM混合框架，通过双通道分离语义和外观，将其融合成混合序列并压缩成单个voco令牌。

Result: 在七个基准测试中平均性能保留率达87.2%，以580比1的压缩率优于领先连续基线的81.0%，注意力分析验证了压缩令牌对离散锚的优先处理。

Conclusion: 极简混合设计可解决效率 - 保真度困境，推动可扩展视觉语言模型发展。

Abstract: Vision-language models (VLMs) have transformed multimodal reasoning, but feeding hundreds of visual patch tokens into LLMs incurs quadratic computational costs, straining memory and context windows. Traditional approaches face a trade-off: continuous compression dilutes high-level semantics such as object identities, while discrete quantization loses fine-grained details such as textures. We introduce HTC-VLM, a hybrid framework that disentangles semantics and appearance through dual channels, i.e., a continuous pathway for fine-grained details via ViT patches and a discrete pathway for symbolic anchors using MGVQ quantization projected to four tokens. These are fused into a 580-token hybrid sequence and compressed into a single voco token via a disentanglement attention mask and bottleneck, ensuring efficient and grounded representations. HTC-VLM achieves an average performance retention of 87.2 percent across seven benchmarks (GQA, VQAv2, MMBench, MME, POPE, SEED-Bench, ScienceQA-Image), outperforming the leading continuous baseline at 81.0 percent with a 580-to-1 compression ratio. Attention analyses show that the compressed token prioritizes the discrete anchor, validating its semantic guidance. Our work demonstrates that a minimalist hybrid design can resolve the efficiency-fidelity dilemma and advance scalable VLMs.

</details>


### [235] [Residual-SwinCA-Net: A Channel-Aware Integrated Residual CNN-Swin Transformer for Malignant Lesion Segmentation in BUSI](https://arxiv.org/abs/2512.08243)
*Saeeda Naz,Saddam Hussain Khan*

Main category: cs.CV

TL;DR: 提出Residual - SwinCA - Net分割框架用于乳腺病变分割，在BUSI数据集上表现优异，提升诊断性能和临床决策能力。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在提取局部和全局特征、处理超声噪声和病变轮廓等方面的挑战，提高乳腺病变分割性能和诊断准确性。

Method: 结合残差CNN模块提取局部特征，定制Swin Transformer块学习全局依赖；应用Laplacian - of - Gaussian区域算子和边界导向算子；采用渐进式收缩策略；集成MSCAS模块和像素注意力模块。

Result: 在BUSI数据集上，Residual - SwinCA - Net框架取得了99.29%的平均准确率、98.74%的IoU和0.9041的Dice。

Conclusion: Residual - SwinCA - Net框架提高了BUSI病变诊断性能，有助于及时进行临床决策。

Abstract: A novel deep hybrid Residual-SwinCA-Net segmentation framework is proposed in the study for addressing such challenges by extracting locally correlated and robust features, incorporating residual CNN modules. Furthermore, for learning global dependencies, Swin Transformer blocks are customized using internal residual pathways, which reinforce gradient stability, refine local patterns, and facilitate global feature fusion. Formerly, for enhancing tissue continuity, ultrasound noise suppressions, and accentuating fine structural transitions Laplacian-of-Gaussian regional operator is applied, and for maintaining the morphological integrity of malignant lesion contours, a boundary-oriented operator has been incorporated. Subsequently, a contraction strategy was applied stage-wise by progressively reducing features-map progressively for capturing scale invariance and enhancing the robustness of structural variability. In addition, each decoder level prior augmentation integrates a new Multi-Scale Channel Attention and Squeezing (MSCAS) module. The MSCAS selectively emphasizes encoder salient maps, retains discriminative global context, and complementary local structures with minimal computational cost while suppressing redundant activations. Finally, the Pixel-Attention module encodes class-relevant spatial cues by adaptively weighing malignant lesion pixels while suppressing background interference. The Residual-SwinCA-Net and existing CNNs/ViTs techniques have been implemented on the publicly available BUSI dataset. The proposed Residual-SwinCA-Net framework outperformed and achieved 99.29% mean accuracy, 98.74% IoU, and 0.9041 Dice for breast lesion segmentation. The proposed Residual-SwinCA-Net framework improves the BUSI lesion diagnostic performance and strengthens timely clinical decision-making.

</details>


### [236] [Distilling Future Temporal Knowledge with Masked Feature Reconstruction for 3D Object Detection](https://arxiv.org/abs/2512.08247)
*Haowen Zheng,Hu Zhu,Lu Deng,Weihao Gu,Yang Yang,Yanyan Liang*

Main category: cs.CV

TL;DR: 文章提出基于稀疏查询的未来时间知识蒸馏（FTKD）方法，将离线模型的未来帧知识转移到在线模型，在两个3D目标检测基线模型上取得良好效果。


<details>
  <summary>Details</summary>
Motivation: 现有知识蒸馏方法忽略未来帧，难以让在线模型有效学习未来知识。

Method: 提出FTKD方法，包括未来感知特征重建策略和未来引导的逻辑蒸馏，将离线教师模型的未来帧知识转移到在线学生模型。

Result: 应用于两个高性能3D目标检测基线，在nuScenes数据集上mAP和NDS最高提升1.3 ，实现最准确的速度估计，且不增加推理成本。

Conclusion: FTKD方法能有效将未来帧知识从离线模型转移到在线模型，提升3D目标检测性能。

Abstract: Camera-based temporal 3D object detection has shown impressive results in autonomous driving, with offline models improving accuracy by using future frames. Knowledge distillation (KD) can be an appealing framework for transferring rich information from offline models to online models. However, existing KD methods overlook future frames, as they mainly focus on spatial feature distillation under strict frame alignment or on temporal relational distillation, thereby making it challenging for online models to effectively learn future knowledge. To this end, we propose a sparse query-based approach, Future Temporal Knowledge Distillation (FTKD), which effectively transfers future frame knowledge from an offline teacher model to an online student model. Specifically, we present a future-aware feature reconstruction strategy to encourage the student model to capture future features without strict frame alignment. In addition, we further introduce future-guided logit distillation to leverage the teacher's stable foreground and background context. FTKD is applied to two high-performing 3D object detection baselines, achieving up to 1.3 mAP and 1.3 NDS gains on the nuScenes dataset, as well as the most accurate velocity estimation, without increasing inference cost.

</details>


### [237] [Terrain Diffusion: A Diffusion-Based Successor to Perlin Noise in Infinite, Real-Time Terrain Generation](https://arxiv.org/abs/2512.08309)
*Alexander Goslin*

Main category: cs.CV

TL;DR: 介绍Terrain Diffusion，可结合扩散模型保真度与程序噪声特性实现无限地形生成。


<details>
  <summary>Details</summary>
Motivation: 传统程序噪声函数在真实感和大规模连贯性上有限，需新方法。

Method: 提出InfiniteDiffusion算法，用分层扩散模型结合上下文与细节，用拉普拉斯编码稳定输出，有开源无限张量框架和一致性蒸馏。

Result: 能连贯、可控且无限制地合成整个行星地形。

Conclusion: 扩散模型可成为程序世界生成的实用基础。

Abstract: For decades, procedural worlds have been built on procedural noise functions such as Perlin noise, which are fast and infinite, yet fundamentally limited in realism and large-scale coherence. We introduce Terrain Diffusion, an AI-era successor to Perlin noise that bridges the fidelity of diffusion models with the properties that made procedural noise indispensable: seamless infinite extent, seed-consistency, and constant-time random access. At its core is InfiniteDiffusion, a novel algorithm for infinite generation, enabling seamless, real-time synthesis of boundless landscapes. A hierarchical stack of diffusion models couples planetary context with local detail, while a compact Laplacian encoding stabilizes outputs across Earth-scale dynamic ranges. An open-source infinite-tensor framework supports constant-memory manipulation of unbounded tensors, and few-step consistency distillation enables efficient generation. Together, these components establish diffusion models as a practical foundation for procedural world generation, capable of synthesizing entire planets coherently, controllably, and without limits.

</details>


### [238] [GeoDM: Geometry-aware Distribution Matching for Dataset Distillation](https://arxiv.org/abs/2512.08317)
*Xuhui Li,Zhengquan Luo,Zihui Cui,Zhiqiang Xu*

Main category: cs.CV

TL;DR: 提出几何感知的分布匹配框架GeoDM，可在多种流形上工作，能捕获复杂结构，优于现有同类方法。


<details>
  <summary>Details</summary>
Motivation: 现有分布匹配方法局限于欧几里得空间，只能捕获线性结构，忽略了数据的内在几何特征。

Method: 提出GeoDM框架，在欧几里得、双曲和球形流形的笛卡尔积中操作，引入可学习的曲率和权重参数，并设计最优传输损失。

Result: 理论分析表明该方法比欧几里得对应方法有更小的泛化误差界；实验显示优于现有数据蒸馏方法，且在各种单几何分布匹配策略中均有效。

Conclusion: 所提出的几何感知分布匹配框架GeoDM是有效的，能解决现有方法的局限性。

Abstract: Dataset distillation aims to synthesize a compact subset of the original data, enabling models trained on it to achieve performance comparable to those trained on the original large dataset. Existing distribution-matching methods are confined to Euclidean spaces, making them only capture linear structures and overlook the intrinsic geometry of real data, e.g., curvature. However, high-dimensional data often lie on low-dimensional manifolds, suggesting that dataset distillation should have the distilled data manifold aligned with the original data manifold. In this work, we propose a geometry-aware distribution-matching framework, called \textbf{GeoDM}, which operates in the Cartesian product of Euclidean, hyperbolic, and spherical manifolds, with flat, hierarchical, and cyclical structures all captured by a unified representation. To adapt to the underlying data geometry, we introduce learnable curvature and weight parameters for three kinds of geometries. At the same time, we design an optimal transport loss to enhance the distribution fidelity. Our theoretical analysis shows that the geometry-aware distribution matching in a product space yields a smaller generalization error bound than the Euclidean counterparts. Extensive experiments conducted on standard benchmarks demonstrate that our algorithm outperforms state-of-the-art data distillation methods and remains effective across various distribution-matching strategies for the single geometries.

</details>


### [239] [Detection of Cyberbullying in GIF using AI](https://arxiv.org/abs/2512.07838)
*Pal Dave,Xiaohong Yuan,Madhuri Siddula,Kaushik Roy*

Main category: cs.CV

TL;DR: 本文聚焦网络暴力检测，针对GIF数据收集数据集并应用深度学习模型，模型准确率达97%，还提供了GIF数据集。


<details>
  <summary>Details</summary>
Motivation: 网络暴力问题日趋严重，社交媒体是高发地。虽有文本和图像的检测研究，但针对GIF/贴纸的研究极少。

Method: 从Twitter收集与网络暴力相关的标签，用GIPHY API下载GIF文件，构建超4100个GIF的数据集，应用预训练的深度学习模型VGG16进行检测。

Result: 深度学习模型检测网络暴力的准确率达到97%。

Conclusion: 为该领域研究人员提供了GIF数据集。

Abstract: Cyberbullying is a well-known social issue, and it is escalating day by day. Due to the vigorous development of the internet, social media provide many different ways for the user to express their opinions and exchange information. Cyberbullying occurs on social media using text messages, comments, sharing images and GIFs or stickers, and audio and video. Much research has been done to detect cyberbullying on textual data; some are available for images. Very few studies are available to detect cyberbullying on GIFs/stickers. We collect a GIF dataset from Twitter and Applied a deep learning model to detect cyberbullying from the dataset. Firstly, we extracted hashtags related to cyberbullying using Twitter. We used these hashtags to download GIF file using publicly available API GIPHY. We collected over 4100 GIFs including cyberbullying and non cyberbullying. we applied deep learning pre-trained model VGG16 for the detection of the cyberbullying. The deep learning model achieved the accuracy of 97%. Our work provides the GIF dataset for researchers working in this area.

</details>


### [240] [Interpreting Structured Perturbations in Image Protection Methods for Diffusion Models](https://arxiv.org/abs/2512.08329)
*Michael R. Martin,Garrick Chan,Kwan-Liu Ma*

Main category: cs.CV

TL;DR: 本研究对图像保护机制的扰动进行可解释AI分析，揭示其通过结构化特征级变形实现保护，且信号微妙但可检测。


<details>
  <summary>Details</summary>
Motivation: 当前对图像保护机制的内部结构、可检测性和表征行为理解不足，故开展此研究。

Method: 采用统一框架，结合白盒特征空间检查和黑盒信号级探测，包括潜伏空间聚类、特征通道激活分析等。

Result: 保护机制是与图像内容紧密耦合的结构化、低熵扰动，受扰动熵、空间部署和频率对齐影响；Glaze和Nightshade在主导图像频率轴上重新分配能量而非引入噪声。

Conclusion: 当代图像保护通过结构化特征级变形而非语义错位实现，有助于提升对抗性图像保护的可解释性，为未来防御和检测策略提供依据。

Abstract: Recent image protection mechanisms such as Glaze and Nightshade introduce imperceptible, adversarially designed perturbations intended to disrupt downstream text-to-image generative models. While their empirical effectiveness is known, the internal structure, detectability, and representational behavior of these perturbations remain poorly understood. This study provides a systematic, explainable AI analysis using a unified framework that integrates white-box feature-space inspection and black-box signal-level probing. Through latent-space clustering, feature-channel activation analysis, occlusion-based spatial sensitivity mapping, and frequency-domain characterization, we show that protection mechanisms operate as structured, low-entropy perturbations tightly coupled to underlying image content across representational, spatial, and spectral domains. Protected images preserve content-driven feature organization with protection-specific substructure rather than inducing global representational drift. Detectability is governed by interacting effects of perturbation entropy, spatial deployment, and frequency alignment, with sequential protection amplifying detectable structure rather than suppressing it. Frequency-domain analysis shows that Glaze and Nightshade redistribute energy along dominant image-aligned frequency axes rather than introducing diffuse noise. These findings indicate that contemporary image protection operates through structured feature-level deformation rather than semantic dislocation, explaining why protection signals remain visually subtle yet consistently detectable. This work advances the interpretability of adversarial image protection and informs the design of future defenses and detection strategies for generative AI systems.

</details>


### [241] [ContextDrag: Precise Drag-Based Image Editing via Context-Preserving Token Injection and Position-Consistent Attention](https://arxiv.org/abs/2512.08477)
*Huiguo He,Pengyu Yan,Ziqi Yi,Weizhi Zhong,Zheng Liu,Yejun Tang,Huan Yang,Kun Gai,Guanbin Li,Lianwen Jin*

Main category: cs.CV

TL;DR: 提出ContextDrag用于基于拖动的图像编辑，含新机制且超现有SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于拖动的图像编辑方法未能充分利用参考图像上下文信息，导致编辑结果一致性和保真度有限。

Method: 引入ContextDrag，结合参考图像VAE编码特征，采用Context - preserving Token Injection (CTI)和Position - Consistent Attention (PCA)。CTI通过Latent - space Reverse Mapping (LRM)注入无噪参考特征；PCA对参考令牌进行位置重新编码并应用重叠感知掩码。

Result: 在DragBench - SR和DragBench - DR上的大量实验表明该方法超越所有现有的SOTA方法。

Conclusion: ContextDrag通过利用编辑模型的上下文建模能力，有效解决了现有方法问题，在基于拖动的图像编辑中表现出色。

Abstract: Drag-based image editing aims to modify visual content followed by user-specified drag operations. Despite existing methods having made notable progress, they still fail to fully exploit the contextual information in the reference image, including fine-grained texture details, leading to edits with limited coherence and fidelity. To address this challenge, we introduce ContextDrag, a new paradigm for drag-based editing that leverages the strong contextual modeling capability of editing models, such as FLUX-Kontext. By incorporating VAE-encoded features from the reference image, ContextDrag can leverage rich contextual cues and preserve fine-grained details, without the need for finetuning or inversion. Specifically, ContextDrag introduced a novel Context-preserving Token Injection (CTI) that injects noise-free reference features into their correct destination locations via a Latent-space Reverse Mapping (LRM) algorithm. This strategy enables precise drag control while preserving consistency in both semantics and texture details. Second, ContextDrag adopts a novel Position-Consistent Attention (PCA), which positional re-encodes the reference tokens and applies overlap-aware masking to eliminate interference from irrelevant reference features. Extensive experiments on DragBench-SR and DragBench-DR demonstrate that our approach surpasses all existing SOTA methods. Code will be publicly available.

</details>


### [242] [Visionary: The World Model Carrier Built on WebGPU-Powered Gaussian Splatting Platform](https://arxiv.org/abs/2512.08478)
*Yuning Gong,Yifei Liu,Yifan Zhan,Muyao Niu,Xueying Li,Yuanjun Liao,Jiaming Chen,Yuanyuan Gao,Jiaqi Chen,Minming Chen,Li Zhou,Yuning Zhang,Wei Wang,Xiaoqing Hou,Huaxi Huang,Shixiang Tang,Le Ma,Dingwen Zhang,Xue Yang,Junchi Yan,Yanchi Zhang,Yinqiang Zheng,Xiao Sun,Zhihang Zhong*

Main category: cs.CV

TL;DR: 提出开放的Web原生平台Visionary用于实时高斯喷涂和网格渲染，降低3DGS系列方法的部署门槛。


<details>
  <summary>Details</summary>
Motivation: 现有查看器解决方案碎片化、沉重，对动态内容和生成模型支持有限，部署摩擦大。

Method: 构建基于WebGPU的渲染器，引入标准化高斯生成器合约，提供三.js插件库。

Result: 在相同3DGS资产下，渲染效率优于现有Web查看器，支持多种变体。

Conclusion: Visionary统一了浏览器中的推理和渲染，显著降低3DGS系列方法的复现、比较和部署障碍。

Abstract: Neural rendering, particularly 3D Gaussian Splatting (3DGS), has evolved rapidly and become a key component for building world models. However, existing viewer solutions remain fragmented, heavy, or constrained by legacy pipelines, resulting in high deployment friction and limited support for dynamic content and generative models. In this work, we present Visionary, an open, web-native platform for real-time various Gaussian Splatting and meshes rendering. Built on an efficient WebGPU renderer with per-frame ONNX inference, Visionary enables dynamic neural processing while maintaining a lightweight, "click-to-run" browser experience. It introduces a standardized Gaussian Generator contract, which not only supports standard 3DGS rendering but also allows plug-and-play algorithms to generate or update Gaussians each frame. Such inference also enables us to apply feedforward generative post-processing. The platform further offers a plug in three.js library with a concise TypeScript API for seamless integration into existing web applications. Experiments show that, under identical 3DGS assets, Visionary achieves superior rendering efficiency compared to current Web viewers due to GPU-based primitive sorting. It already supports multiple variants, including MLP-based 3DGS, 4DGS, neural avatars, and style transformation or enhancement networks. By unifying inference and rendering directly in the browser, Visionary significantly lowers the barrier to reproduction, comparison, and deployment of 3DGS-family methods, serving as a unified World Model Carrier for both reconstructive and generative paradigms.

</details>


### [243] [Disrupting Hierarchical Reasoning: Adversarial Protection for Geographic Privacy in Multimodal Reasoning Models](https://arxiv.org/abs/2512.08503)
*Jiaming Zhang,Che Wang,Yang Cao,Longtao Huang,Wei Yang Bryan Lim*

Main category: cs.CV

TL;DR: MLRMs带来隐私风险，现有保护技术无效，提出ReasonBreak框架，用概念感知扰动破坏推理，有GeoPrivacy - 6K数据集，实验证明其有效性，建立新隐私保护范式。


<details>
  <summary>Details</summary>
Motivation: MLRMs通过分层思维推理从个人图像推断精确地理位置，带来显著隐私风险，现有隐私保护技术对其无效。

Method: 引入ReasonBreak框架，通过概念感知扰动破坏MLRMs的分层推理，战略地针对推理链中的关键概念依赖生成扰动；构建GeoPrivacy - 6K数据集。

Result: 对七种最先进的MLRMs进行评估，区域级保护提升14.4%（从19.4%到33.8%），块级保护近乎翻倍（从16.8%到33.5%）。

Conclusion: 该工作为抵御基于推理的威胁建立了新的隐私保护范式。

Abstract: Multi-modal large reasoning models (MLRMs) pose significant privacy risks by inferring precise geographic locations from personal images through hierarchical chain-of-thought reasoning. Existing privacy protection techniques, primarily designed for perception-based models, prove ineffective against MLRMs' sophisticated multi-step reasoning processes that analyze environmental cues. We introduce \textbf{ReasonBreak}, a novel adversarial framework specifically designed to disrupt hierarchical reasoning in MLRMs through concept-aware perturbations. Our approach is founded on the key insight that effective disruption of geographic reasoning requires perturbations aligned with conceptual hierarchies rather than uniform noise. ReasonBreak strategically targets critical conceptual dependencies within reasoning chains, generating perturbations that invalidate specific inference steps and cascade through subsequent reasoning stages. To facilitate this approach, we contribute \textbf{GeoPrivacy-6K}, a comprehensive dataset comprising 6,341 ultra-high-resolution images ($\geq$2K) with hierarchical concept annotations. Extensive evaluation across seven state-of-the-art MLRMs (including GPT-o3, GPT-5, Gemini 2.5 Pro) demonstrates ReasonBreak's superior effectiveness, achieving a 14.4\% improvement in tract-level protection (33.8\% vs 19.4\%) and nearly doubling block-level protection (33.5\% vs 16.8\%). This work establishes a new paradigm for privacy protection against reasoning-based threats.

</details>


### [244] [A Novel Wasserstein Quaternion Generative Adversarial Network for Color Image Generation](https://arxiv.org/abs/2512.08542)
*Zhigang Jia,Duan Wang,Hengkai Wang,Yajun Xie,Meixiang Zhao,Xiaoyu Zhao*

Main category: cs.CV

TL;DR: 本文定义新的四元数Wasserstein距离并发展对偶理论，提出新型生成对抗网络，实验表明该模型在生成效率和图像质量上更优。


<details>
  <summary>Details</summary>
Motivation: 现有彩色图像生成模型忽略颜色通道相关性，存在色差问题，且彩色图像数据分布问题未系统阐述，缺乏衡量不同彩色图像数据集的理论。

Method: 定义新的四元数Wasserstein距离并发展其对偶理论，借助四元数凸集分离定理和四元数Farkas引理推导强对偶形式，提出新型Wasserstein四元数生成对抗网络。

Result: 新型模型在生成效率和图像质量上超越（四元数）生成对抗网络和Wasserstein生成对抗网络。

Conclusion: 所提出的新型Wasserstein四元数生成对抗网络具有更好的生成效果。

Abstract: Color image generation has a wide range of applications, but the existing generation models ignore the correlation among color channels, which may lead to chromatic aberration problems. In addition, the data distribution problem of color images has not been systematically elaborated and explained, so that there is still the lack of the theory about measuring different color images datasets. In this paper, we define a new quaternion Wasserstein distance and develop its dual theory. To deal with the quaternion linear programming problem, we derive the strong duality form with helps of quaternion convex set separation theorem and quaternion Farkas lemma. With using quaternion Wasserstein distance, we propose a novel Wasserstein quaternion generative adversarial network. Experiments demonstrate that this novel model surpasses both the (quaternion) generative adversarial networks and the Wasserstein generative adversarial network in terms of generation efficiency and image quality.

</details>


### [245] [Disturbance-Free Surgical Video Generation from Multi-Camera Shadowless Lamps for Open Surgery](https://arxiv.org/abs/2512.08577)
*Yuna Kato,Shohei Mori,Hideo Saito,Yoshifumi Takatsume,Hiroki Kajita,Mariko Isogawa*

Main category: cs.CV

TL;DR: 本文旨在自动化手术视频图像对齐任务，提出方法生成固定视角手术视频，用户研究显示其优于传统方法且改进视频质量，还评估了医生对合成选项的偏好。


<details>
  <summary>Details</summary>
Motivation: 手术视频录制中，外科医生常遮挡视角需频繁调整摄像机，前期解决方案需手动图像对齐，因此要实现该任务自动化。

Method: 识别照明系统移动的帧并重新对齐，选择遮挡最少的相机生成固定视角视频。

Result: 用户研究表明生成的视频在确认手术区域的便利性和观看舒适度上优于传统方法，且改进了视频质量。

Conclusion: 提出的方法有效，能自动化图像对齐任务，还通过评估了解了医生对合成选项的偏好。

Abstract: Video recordings of open surgeries are greatly required for education and research purposes. However, capturing unobstructed videos is challenging since surgeons frequently block the camera field of view. To avoid occlusion, the positions and angles of the camera must be frequently adjusted, which is highly labor-intensive. Prior work has addressed this issue by installing multiple cameras on a shadowless lamp and arranging them to fully surround the surgical area. This setup increases the chances of some cameras capturing an unobstructed view. However, manual image alignment is needed in post-processing since camera configurations change every time surgeons move the lamp for optimal lighting. This paper aims to fully automate this alignment task. The proposed method identifies frames in which the lighting system moves, realigns them, and selects the camera with the least occlusion to generate a video that consistently presents the surgical field from a fixed perspective. A user study involving surgeons demonstrated that videos generated by our method were superior to those produced by conventional methods in terms of the ease of confirming the surgical area and the comfort during video viewing. Additionally, our approach showed improvements in video quality over existing techniques. Furthermore, we implemented several synthesis options for the proposed view-synthesis method and conducted a user study to assess surgeons' preferences for each option.

</details>


### [246] [Decoupling Template Bias in CLIP: Harnessing Empty Prompts for Enhanced Few-Shot Learning](https://arxiv.org/abs/2512.08606)
*Zhenyu Zhang,Guangyao Chen,Yixiong Zou,Zhimeng Huang,Yuhua Li*

Main category: cs.CV

TL;DR: 研究发现CLIP模型中模板 - 样本相似度（TSS）会引入偏差，提出用空提示框架减少偏差，实验证明该方法有更好效果。


<details>
  <summary>Details</summary>
Motivation: CLIP模型中TSS偏差会导致模型依赖模板接近度而非样本与类别的真实对齐，降低分类的准确性和鲁棒性。

Method: 提出空提示框架，预训练阶段揭示并减少CLIP编码器中模板引起的偏差；少样本微调阶段用偏差校准损失确保图像和类别正确对齐。

Result: 该模板校正方法显著减少了TSS引起的性能波动，提高了分类准确性和鲁棒性。

Conclusion: 所提出的方法能有效解决CLIP模型中TSS偏差问题，提升分类性能。

Abstract: The Contrastive Language-Image Pre-Training (CLIP) model excels in few-shot learning by aligning visual and textual representations. Our study shows that template-sample similarity (TSS), defined as the resemblance between a text template and an image sample, introduces bias. This bias leads the model to rely on template proximity rather than true sample-to-category alignment, reducing both accuracy and robustness in classification. We present a framework that uses empty prompts, textual inputs that convey the idea of "emptiness" without category information. These prompts capture unbiased template features and offset TSS bias. The framework employs two stages. During pre-training, empty prompts reveal and reduce template-induced bias within the CLIP encoder. During few-shot fine-tuning, a bias calibration loss enforces correct alignment between images and their categories, ensuring the model focuses on relevant visual cues. Experiments across multiple benchmarks demonstrate that our template correction method significantly reduces performance fluctuations caused by TSS, yielding higher classification accuracy and stronger robustness. The repository of this project is available at https://github.com/zhenyuZ-HUST/Decoupling-Template-Bias-in-CLIP.

</details>


### [247] [Aerial Vision-Language Navigation with a Unified Framework for Spatial, Temporal and Embodied Reasoning](https://arxiv.org/abs/2512.08639)
*Huilin Xu,Zhuoyang Liu,Yixiang Luomei,Feng Xu*

Main category: cs.CV

TL;DR: 提出仅基于单目RGB观测和语言指令的统一空中视觉与语言导航框架，经实验验证有效。


<details>
  <summary>Details</summary>
Motivation: 现有空中视觉与语言导航方法依赖全景图像、深度输入或里程计，增加系统成本和集成复杂性，妨碍轻量级无人机实际部署。

Method: 提出统一框架，将导航视为下一个令牌预测问题，通过提示引导的多任务学习优化，还提出关键帧选择策略和动作合并与标签重新加权机制。

Result: 在Aerial VLN基准测试中，模型在单目RGB设置下，在可见和不可见环境中均取得良好结果，显著超越现有RGB基线并缩小与全景RGB - D模型差距。

Conclusion: 实验验证方法有效，消融研究证明任务设计和架构选择合理。

Abstract: Aerial Vision-and-Language Navigation (VLN) aims to enable unmanned aerial vehicles (UAVs) to interpret natural language instructions and navigate complex urban environments using onboard visual observation. This task holds promise for real-world applications such as low-altitude inspection, search-and-rescue, and autonomous aerial delivery. Existing methods often rely on panoramic images, depth inputs, or odometry to support spatial reasoning and action planning. These requirements increase system cost and integration complexity, thus hindering practical deployment for lightweight UAVs. We present a unified aerial VLN framework that operates solely on egocentric monocular RGB observations and natural language instructions. The model formulates navigation as a next-token prediction problem, jointly optimizing spatial perception, trajectory reasoning, and action prediction through prompt-guided multi-task learning. Moreover, we propose a keyframe selection strategy to reduce visual redundancy by retaining semantically informative frames, along with an action merging and label reweighting mechanism that mitigates long-tailed supervision imbalance and facilitates stable multi-task co-training. Extensive experiments on the Aerial VLN benchmark validate the effectiveness of our method. Under the challenging monocular RGB-only setting, our model achieves strong results across both seen and unseen environments. It significantly outperforms existing RGB-only baselines and narrows the performance gap with state-of-the-art panoramic RGB-D counterparts. Comprehensive ablation studies further demonstrate the contribution of our task design and architectural choices.

</details>


### [248] [Mitigating Individual Skin Tone Bias in Skin Lesion Classification through Distribution-Aware Reweighting](https://arxiv.org/abs/2512.08733)
*Kuniko Paxton,Zeinab Dehghani,Koorosh Aslansefat,Dhavalkumar Thakker,Yiannis Papadopoulos*

Main category: cs.CV

TL;DR: 本文引入基于分布的框架评估和缓解皮肤病变分类中的个体公平性，对比多种统计距离指标，实验证明分布重加权更优。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习医学影像公平性研究依赖粗粒度子组分类，忽略个体差异，可能掩盖子组内异常值面临的偏差。

Method: 将肤色视为连续属性，使用核密度估计（KDE）建模其分布，比较十二种统计距离指标量化肤色分布差异，并提出基于距离的重加权（DRW）损失函数。

Result: 实验表明分类重加权在捕捉个体差异上有局限，基于分布的重加权，尤其是使用Fidelity Similarity (FS)、Wasserstein Distance (WD)、Hellinger Metric (HM)和Harmonic Mean Similarity (HS)时性能更优。

Conclusion: 为提高皮肤病AI系统的个体公平性建立了稳健方法，对医学图像分析中敏感连续属性有更广泛启示。

Abstract: Skin color has historically been a focal point of discrimination, yet fairness research in machine learning for medical imaging often relies on coarse subgroup categories, overlooking individual-level variations. Such group-based approaches risk obscuring biases faced by outliers within subgroups. This study introduces a distribution-based framework for evaluating and mitigating individual fairness in skin lesion classification. We treat skin tone as a continuous attribute rather than a categorical label, and employ kernel density estimation (KDE) to model its distribution. We further compare twelve statistical distance metrics to quantify disparities between skin tone distributions and propose a distance-based reweighting (DRW) loss function to correct underrepresentation in minority tones. Experiments across CNN and Transformer models demonstrate: (i) the limitations of categorical reweighting in capturing individual-level disparities, and (ii) the superior performance of distribution-based reweighting, particularly with Fidelity Similarity (FS), Wasserstein Distance (WD), Hellinger Metric (HM), and Harmonic Mean Similarity (HS). These findings establish a robust methodology for advancing fairness at individual level in dermatological AI systems, and highlight broader implications for sensitive continuous attributes in medical image analysis.

</details>


### [249] [Refining Visual Artifacts in Diffusion Models via Explainable AI-based Flaw Activation Maps](https://arxiv.org/abs/2512.08774)
*Seoyeon Lee,Gwangyeol Yu,Chaewon Kim,Jonghyuk Park*

Main category: cs.CV

TL;DR: 提出自精炼扩散框架提升图像生成质量，在不同扩散模型和任务上效果良好，证明可解释AI技术能助力图像优化。


<details>
  <summary>Details</summary>
Motivation: 解决扩散模型在图像合成中存在的伪影和不真实区域问题。

Method: 提出自精炼扩散框架，使用基于可解释AI的缺陷高亮器生成缺陷激活图，在正反过程分别处理缺陷区域。

Result: 在不同扩散模型上Fréchet inception距离最高提升27.3%，在不同任务上效果稳健。

Conclusion: 可解释AI技术能超越可解释性，为图像精炼做贡献，该框架通用有效，推动图像合成领域发展。

Abstract: Diffusion models have achieved remarkable success in image synthesis. However, addressing artifacts and unrealistic regions remains a critical challenge. We propose self-refining diffusion, a novel framework that enhances image generation quality by detecting these flaws. The framework employs an explainable artificial intelligence (XAI)-based flaw highlighter to produce flaw activation maps (FAMs) that identify artifacts and unrealistic regions. These FAMs improve reconstruction quality by amplifying noise in flawed regions during the forward process and by focusing on these regions during the reverse process. The proposed approach achieves up to a 27.3% improvement in Fréchet inception distance across various diffusion-based models, demonstrating consistently strong performance on diverse datasets. It also shows robust effectiveness across different tasks, including image generation, text-to-image generation, and inpainting. These results demonstrate that explainable AI techniques can extend beyond interpretability to actively contribute to image refinement. The proposed framework offers a versatile and effective approach applicable to various diffusion models and tasks, significantly advancing the field of image synthesis.

</details>


### [250] [MatteViT: High-Frequency-Aware Document Shadow Removal with Shadow Matte Guidance](https://arxiv.org/abs/2512.08789)
*Chaewon Kim,Seoyeon Lee,Jonghyuk Park*

Main category: cs.CV

TL;DR: 提出MatteViT框架用于文档去阴影，采用高频放大和阴影遮罩策略，实验证明效果好且利于下游任务。


<details>
  <summary>Details</summary>
Motivation: 文档去阴影对提升清晰度很重要，需在去阴影时保留高频细节。

Method: 提出MatteViT框架，采用高频放大模块HFAM和基于连续亮度的阴影遮罩两种策略。

Result: 在公共基准测试中达到了最先进的性能，在下游光学字符识别任务中能更好保留文本细节，提升识别性能。

Conclusion: MatteViT为现实文档去阴影提供了强大且实用的解决方案。

Abstract: Document shadow removal is essential for enhancing the clarity of digitized documents. Preserving high-frequency details (e.g., text edges and lines) is critical in this process because shadows often obscure or distort fine structures. This paper proposes a matte vision transformer (MatteViT), a novel shadow removal framework that applies spatial and frequency-domain information to eliminate shadows while preserving fine-grained structural details. To effectively retain these details, we employ two preservation strategies. First, our method introduces a lightweight high-frequency amplification module (HFAM) that decomposes and adaptively amplifies high-frequency components. Second, we present a continuous luminance-based shadow matte, generated using a custom-built matte dataset and shadow matte generator, which provides precise spatial guidance from the earliest processing stage. These strategies enable the model to accurately identify fine-grained regions and restore them with high fidelity. Extensive experiments on public benchmarks (RDD and Kligler) demonstrate that MatteViT achieves state-of-the-art performance, providing a robust and practical solution for real-world document shadow removal. Furthermore, the proposed method better preserves text-level details in downstream tasks, such as optical character recognition, improving recognition performance over prior methods.

</details>


### [251] [Uncertainty-Aware Subset Selection for Robust Visual Explainability under Distribution Shifts](https://arxiv.org/abs/2512.08445)
*Madhav Gupta,Vishak Prasad C,Ganesh Ramakrishnan*

Main category: cs.CV

TL;DR: 现有基于子集选择解释方法在分布外情况可靠性明显下降，为此提出结合子模子集选择和基于梯度的不确定性估计框架，可改善两种分布下表现。


<details>
  <summary>Details</summary>
Motivation: 现有基于子集选择解释深度视觉模型的方法在分布外情况的表现缺乏研究且可靠性下降

Method: 结合子模子集选择与基于梯度的不确定性估计，通过自适应权重扰动估计不确定性以指导子模优化进行子集选择

Result: 方法缓解了分布外场景下现有方法的弱点，且改进了分布内的设置表现

Conclusion: 指出当前基于子集方法的局限性，证明不确定性驱动优化能增强归因和对象级可解释性，为现实视觉应用奠定基础

Abstract: Subset selection-based methods are widely used to explain deep vision models: they attribute predictions by highlighting the most influential image regions and support object-level explanations. While these methods perform well in in-distribution (ID) settings, their behavior under out-of-distribution (OOD) conditions remains poorly understood. Through extensive experiments across multiple ID-OOD sets, we find that reliability of the existing subset based methods degrades markedly, yielding redundant, unstable, and uncertainty-sensitive explanations. To address these shortcomings, we introduce a framework that combines submodular subset selection with layer-wise, gradient-based uncertainty estimation to improve robustness and fidelity without requiring additional training or auxiliary models. Our approach estimates uncertainty via adaptive weight perturbations and uses these estimates to guide submodular optimization, ensuring diverse and informative subset selection. Empirical evaluations show that, beyond mitigating the weaknesses of existing methods under OOD scenarios, our framework also yields improvements in ID settings. These findings highlight limitations of current subset-based approaches and demonstrate how uncertainty-driven optimization can enhance attribution and object-level interpretability, paving the way for more transparent and trustworthy AI in real-world vision applications.

</details>


### [252] [Training-Free Dual Hyperbolic Adapters for Better Cross-Modal Reasoning](https://arxiv.org/abs/2512.08820)
*Yi Zhang,Chun-Wun Cheng,Junyi He,Ke Yu,Yushun Tang,Carola-Bibiane Schönlieb,Zhihai He,Angelica I. Aviles-Rivero*

Main category: cs.CV

TL;DR: 现有视觉 - 语言模型方法在领域变化时性能下降或微调成本高，提出T - DHA方法，在双曲空间表征视觉 - 语言关系，实验显示其在少样本图像识别和领域泛化任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有视觉 - 语言模型方法在领域变化时性能下降，以及在新领域微调需大量计算资源的问题。

Method: 开发训练自由的双曲适配器T - DHA，在双曲空间而非传统欧几里得空间表征视觉 - 语言关系，结合负学习。

Result: 在多个数据集上的实验表明，T - DHA方法在少样本图像识别和领域泛化任务上显著优于现有最先进方法。

Conclusion: T - DHA方法能有效解决现有视觉 - 语言模型的问题，在相关任务中有出色表现。

Abstract: Recent research in Vision-Language Models (VLMs) has significantly advanced our capabilities in cross-modal reasoning. However, existing methods suffer from performance degradation with domain changes or require substantial computational resources for fine-tuning in new domains. To address this issue, we develop a new adaptation method for large vision-language models, called \textit{Training-free Dual Hyperbolic Adapters} (T-DHA). We characterize the vision-language relationship between semantic concepts, which typically has a hierarchical tree structure, in the hyperbolic space instead of the traditional Euclidean space. Hyperbolic spaces exhibit exponential volume growth with radius, unlike the polynomial growth in Euclidean space. We find that this unique property is particularly effective for embedding hierarchical data structures using the Poincaré ball model, achieving significantly improved representation and discrimination power. Coupled with negative learning, it provides more accurate and robust classifications with fewer feature dimensions. Our extensive experimental results on various datasets demonstrate that the T-DHA method significantly outperforms existing state-of-the-art methods in few-shot image recognition and domain generalization tasks.

</details>


### [253] [InfiniteVL: Synergizing Linear and Sparse Attention for Highly-Efficient, Unlimited-Input Vision-Language Models](https://arxiv.org/abs/2512.08829)
*Hongyuan Tao,Bencheng Liao,Shaoyu Chen,Haoran Yin,Qian Zhang,Wenyu Liu,Xinggang Wang*

Main category: cs.CV

TL;DR: 提出线性复杂度VLM架构InfiniteVL，结合SWA和Gated DeltaNet，设计三阶段训练策略，用少量数据取得好效果，推理速度快。


<details>
  <summary>Details</summary>
Motivation: 现有窗口注意力和线性注意力在VLMs中有局限性，如窗口注意力在序列长度超窗口大小时性能下降，线性注意力在信息密集型任务中表现不佳。

Method: 提出InfiniteVL架构，结合滑动窗口注意力（SWA）与Gated DeltaNet，设计三阶段训练策略，包括蒸馏预训练、指令调优和长序列SFT。

Result: 用不到领先VLMs 2%的训练数据，性能超之前线性复杂度VLMs，与领先Transformer - 基VLMs相当，推理速度比类似大小且由FlashAttention - 2加速的Transformer基VLMs快3.6倍，在视频理解场景有稳定实时预填充速度。

Conclusion: InfiniteVL架构有效，能在受限资源下实现有竞争力的多模态性能，具有长时记忆保留能力。

Abstract: Window attention and linear attention represent two principal strategies for mitigating the quadratic complexity and ever-growing KV cache in Vision-Language Models (VLMs). However, we observe that window-based VLMs suffer performance degradation when sequence length exceeds the window size, while linear attention underperforms on information-intensive tasks such as OCR and document understanding. To overcome these limitations, we propose InfiniteVL, a linear-complexity VLM architecture that synergizes sliding window attention (SWA) with Gated DeltaNet. For achieving competitive multimodal performance under constrained resources, we design a three-stage training strategy comprising distillation pretraining, instruction tuning, and long-sequence SFT. Remarkably, using less than 2\% of the training data required by leading VLMs, InfiniteVL not only substantially outperforms previous linear-complexity VLMs but also matches the performance of leading Transformer-based VLMs, while demonstrating effective long-term memory retention. Compared to similar-sized Transformer-based VLMs accelerated by FlashAttention-2, InfiniteVL achieves over 3.6\times inference speedup while maintaining constant latency and memory footprint. In streaming video understanding scenarios, it sustains a stable 24 FPS real-time prefill speed while preserving long-term memory cache. Code and models are available at https://github.com/hustvl/InfiniteVL.

</details>


### [254] [Siamese-Driven Optimization for Low-Resolution Image Latent Embedding in Image Captioning](https://arxiv.org/abs/2512.08873)
*Jing Jie Tan,Anissa Mokraoui,Ban-Hoe Kwan,Danny Wee-Kiat Ng,Yan-Chai Hum*

Main category: cs.CV

TL;DR: 针对图像描述中低分辨率图像的处理挑战，提出SOLI方法，采用孪生网络优化潜在嵌入，适用于资源受限场景。


<details>
  <summary>Details</summary>
Motivation: 图像描述领域处理低分辨率图像存在挑战，大模型重且难再训练，需轻量级解决方案。

Method: 提出SOLI方法，采用孪生网络架构优化潜在嵌入，使用双路径神经网络结构。

Result: SOLI方法能在不牺牲性能的前提下减少计算开销。

Conclusion: SOLI是资源受限场景下低分辨率图像描述训练的理想选择。

Abstract: Image captioning is essential in many fields including assisting visually impaired individuals, improving content management systems, and enhancing human-computer interaction. However, a recent challenge in this domain is dealing with low-resolution image (LRI). While performance can be improved by using larger models like transformers for encoding, these models are typically heavyweight, demanding significant computational resources and memory, leading to challenges in retraining. To address this, the proposed SOLI (Siamese-Driven Optimization for Low-Resolution Image Latent Embedding in Image Captioning) approach presents a solution specifically designed for lightweight, low-resolution images captioning. It employs a Siamese network architecture to optimize latent embeddings, enhancing the efficiency and accuracy of the image-to-text translation process. By focusing on a dual-pathway neural network structure, SOLI minimizes computational overhead without sacrificing performance, making it an ideal choice for training on resource-constrained scenarios.

</details>


### [255] [No Labels, No Problem: Training Visual Reasoners with Multimodal Verifiers](https://arxiv.org/abs/2512.08889)
*Damiano Marsili,Georgia Gkioxari*

Main category: cs.CV

TL;DR: 提出无注释训练框架改进视觉推理和定位，结合语言和视觉模型优势，评估显示性能超越其他模型。


<details>
  <summary>Details</summary>
Motivation: 现有视觉推理方法存在需大规模监督、逻辑缺陷和定位错误等问题，需改进。

Method: 提出无注释训练框架，用LLM验证器通过强化学习优化推理，VLM验证器通过自动难负样本挖掘加强视觉定位。

Result: 在多种空间推理任务中评估，方法提升了视觉推理能力，超越开源和专有模型，改进的视觉定位模型也优于近期仅文本的视觉推理方法。

Conclusion: 所提无注释训练框架有效提升视觉推理和定位能力。

Abstract: Visual reasoning is challenging, requiring both precise object grounding and understanding complex spatial relationships. Existing methods fall into two camps: language-only chain-of-thought approaches, which demand large-scale (image, query, answer) supervision, and program-synthesis approaches which use pre-trained models and avoid training, but suffer from flawed logic and erroneous grounding. We propose an annotation-free training framework that improves both reasoning and grounding. Our framework uses AI-powered verifiers: an LLM verifier refines LLM reasoning via reinforcement learning, while a VLM verifier strengthens visual grounding through automated hard-negative mining, eliminating the need for ground truth labels. This design combines the strengths of modern AI systems: advanced language-only reasoning models for decomposing spatial queries into simpler subtasks, and strong vision specialist models improved via performant VLM critics. We evaluate our approach across diverse spatial reasoning tasks, and show that our method improves visual reasoning and surpasses open-source and proprietary models, while with our improved visual grounding model we further outperform recent text-only visual reasoning methods. Project webpage: https://glab-caltech.github.io/valor/

</details>


### [256] [Generation is Required for Data-Efficient Perception](https://arxiv.org/abs/2512.08854)
*Jack Brady,Bernhard Schölkopf,Thomas Kipf,Simon Buchholz,Wieland Brendel*

Main category: cs.CV

TL;DR: 研究生成和非生成方法能否实现组合泛化，理论和实验表明生成方法在组合泛化上更优。


<details>
  <summary>Details</summary>
Motivation: 探讨生成方法是否是机器实现人类级视觉感知所必需的，通过研究组合泛化能力来验证。

Method: 在组合数据生成过程下，形式化生成和非生成方法保证组合泛化所需的归纳偏置，进行理论分析，还在图像数据集上训练多种方法进行实验。

Result: 非生成方法若无必要归纳偏置，组合泛化常失败，需大规模预训练或额外监督；生成方法利用合适归纳偏置，无需额外数据就能显著提升组合泛化能力。

Conclusion: 生成方法在实现组合泛化上比非生成方法更有优势，对实现人类级视觉感知有重要意义。

Abstract: It has been hypothesized that human-level visual perception requires a generative approach in which internal representations result from inverting a decoder. Yet today's most successful vision models are non-generative, relying on an encoder that maps images to representations without decoder inversion. This raises the question of whether generation is, in fact, necessary for machines to achieve human-level visual perception. To address this, we study whether generative and non-generative methods can achieve compositional generalization, a hallmark of human perception. Under a compositional data generating process, we formalize the inductive biases required to guarantee compositional generalization in decoder-based (generative) and encoder-based (non-generative) methods. We then show theoretically that enforcing these inductive biases on encoders is generally infeasible using regularization or architectural constraints. In contrast, for generative methods, the inductive biases can be enforced straightforwardly, thereby enabling compositional generalization by constraining a decoder and inverting it. We highlight how this inversion can be performed efficiently, either online through gradient-based search or offline through generative replay. We examine the empirical implications of our theory by training a range of generative and non-generative methods on photorealistic image datasets. We find that, without the necessary inductive biases, non-generative methods often fail to generalize compositionally and require large-scale pretraining or added supervision to improve generalization. By comparison, generative methods yield significant improvements in compositional generalization, without requiring additional data, by leveraging suitable inductive biases on a decoder along with search and replay.

</details>


### [257] [Astra: General Interactive World Model with Autoregressive Denoising](https://arxiv.org/abs/2512.08931)
*Yixuan Zhu,Jiaqi Feng,Wenzhao Zheng,Yuan Gao,Xin Tao,Pengfei Wan,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: 提出交互式通用世界模型Astra用于生成不同场景下真实世界的未来视频，在多数据集实验中优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 当前能从过去观察和行动预测长期未来的世界模型在通用场景和多种行动形式方面研究不足，需填补这一空白。

Method: 提出自回归去噪架构，用时间因果注意力聚合过去观察；用噪声增强历史记忆；引入动作感知适配器；开发动作专家混合模块。

Result: Astra实现交互式、一致性和通用的长期视频预测，支持多种交互形式，在多数据集实验中展示出在保真度、长期预测和动作对齐方面的提升。

Conclusion: Astra在长视频预测上表现良好，优于现有世界模型。

Abstract: Recent advances in diffusion transformers have empowered video generation models to generate high-quality video clips from texts or images. However, world models with the ability to predict long-horizon futures from past observations and actions remain underexplored, especially for general-purpose scenarios and various forms of actions. To bridge this gap, we introduce Astra, an interactive general world model that generates real-world futures for diverse scenarios (e.g., autonomous driving, robot grasping) with precise action interactions (e.g., camera motion, robot action). We propose an autoregressive denoising architecture and use temporal causal attention to aggregate past observations and support streaming outputs. We use a noise-augmented history memory to avoid over-reliance on past frames to balance responsiveness with temporal coherence. For precise action control, we introduce an action-aware adapter that directly injects action signals into the denoising process. We further develop a mixture of action experts that dynamically route heterogeneous action modalities, enhancing versatility across diverse real-world tasks such as exploration, manipulation, and camera control. Astra achieves interactive, consistent, and general long-term video prediction and supports various forms of interactions. Experiments across multiple datasets demonstrate the improvements of Astra in fidelity, long-range prediction, and action alignment over existing state-of-the-art world models.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [258] [High-performance computing enabled contingency analysis for modern power networks](https://arxiv.org/abs/2512.08465)
*Alexandre Gracia-Calvo,Francesca Rossi,Eduardo Iraola,Juan Carlos Olives-Camps,Eduardo Prieto-Araujo*

Main category: eess.SY

TL;DR: 本文提出一种可扩展的现代电网脆弱性评估方法，结合N - 2故障分析与小信号稳定性评估，用风险指数确定关键组件，在IEEE 118 - 母线测试系统验证，能有效识别关键资产。


<details>
  <summary>Details</summary>
Motivation: 现代电网面临级联故障脆弱性增加问题，现有方法在N - 2故障分析与复杂稳定性评估的计算可处理性上存在不足，需解决计算瓶颈和确定性筛选的局限性。

Method: 提出概率风险指数$R_i$，结合故障元件的故障频率和故障严重程度；利用高性能计算技术和PyCOMPSs并行编程库，协调最优潮流模拟和小信号分析。

Result: 在IEEE 118 - 母线测试系统处理超57000个场景，风险方法能有效识别确定性N - 1标准常忽略的关键资产。

Conclusion: 建立了可复制、高效的概率安全评估工作流程，适用于大规模网络，能支持近实时环境下的操作员决策。

Abstract: Modern power networks face increasing vulnerability to cascading failures due to high complexity and the growing penetration of intermittent resources, necessitating rigorous security assessment beyond the conventional $N-1$ criterion. Current approaches often struggle to achieve the computational tractability required for exhaustive $N-2$ contingency analysis integrated with complex stability evaluations like small-signal stability. Addressing this computational bottleneck and the limitations of deterministic screening, this paper presents a scalable methodology for the vulnerability assessment of modern power networks, integrating $N-2$ contingency analysis with small-signal stability evaluation. To prioritize critical components, we propose a probabilistic \textbf{Risk Index ($R_i$)} that weights the deterministic \textit{severity} of a contingency (including optimal power flow divergence, islanding, and oscillatory instability) by the \textit{failure frequency} of the involved elements based on reliability data. The proposed framework is implemented using High-Performance Computing (HPC) techniques through the PyCOMPSs parallel programming library, orchestrating optimal power flow simulations (VeraGrid) and small-signal analysis (STAMP) to enable the exhaustive exploration of massive contingency sets. The methodology is validated on the IEEE 118-bus test system, processing more than \num{57000} scenarios to identify components prone to triggering cascading failures. Results demonstrate that the risk-based approach effectively isolates critical assets that deterministic $N-1$ criteria often overlook. This work establishes a replicable and efficient workflow for probabilistic security assessment, suitable for large-scale networks and capable of supporting operator decision-making in near real-time environments.

</details>


### [259] [Cabin Layout, Seat Density, and Passenger Segmentation in Air Transport: Implications for Prices, Ancillary Revenues, and Efficiency](https://arxiv.org/abs/2512.08066)
*Alessandro V. M. Oliveira,Moises D. Vassallo*

Main category: eess.SY

TL;DR: 研究飞机客舱座位布局和密度对国内航班机票定价的影响，用PDS - LASSO方法分析，发现高座位排密度对应低价，还有意外结果并探讨创新客舱概念。


<details>
  <summary>Details</summary>
Motivation: 探究飞机客舱座位布局和密度如何影响国内航班机票定价。

Method: 基于登机牌微数据和乘客面谈数据，用Post - Double - Selection LASSO (PDS - LASSO)程序估计计量经济模型，还采用探索性方法研究创新客舱概念。

Result: 高座位排密度与低价相关；无选座费时，高价票乘客因短时间购票常被分配到中间座位。

Conclusion: 研究结果反映了飞机规模增加带来的规模经济和运营效率提升，解释了航空公司一项主要辅助收入背后的经济逻辑。

Abstract: This study investigates how the layout and density of seats in aircraft cabins influence the pricing of airline tickets on domestic flights. The analysis is based on microdata from boarding passes linked to face-to-face interviews with passengers, allowing us to relate the price paid to the location on the aircraft seat map, as well as market characteristics and flight operations. Econometric models were estimated using the Post-Double-Selection LASSO (PDS-LASSO) procedure, which selects numerous controls for unobservable factors linked to commercial and operational aspects, thus enabling better identification of the effect of variables such as advance purchase, reason for travel, fuel price, market structure, and load factor, among others. The results suggest that a higher density of seat rows is associated with lower prices, reflecting economies of scale with the increase in aircraft size and gains in operational efficiency. An unexpected result was also obtained: in situations where there was no seat selection fee, passengers with more expensive tickets were often allocated middle seats due to purchasing at short notice, when the side alternatives were no longer available. This behavior helps explain the economic logic behind one of the main ancillary revenues of airlines. In addition to quantitative analysis, the study incorporates an exploratory approach to innovative cabin concepts and their possible effects on density and comfort on board.

</details>


### [260] [Learning Dynamics from Infrequent Output Measurements for Uncertainty-Aware Optimal Control](https://arxiv.org/abs/2512.08013)
*Robert Lefringhausen,Theodor Springer,Sandra Hirche*

Main category: eess.SY

TL;DR: 针对非线性系统动力学未知且输出测量少而有噪声的情况，提出基于贝叶斯先验和M - H采样的最优控制方法并验证。


<details>
  <summary>Details</summary>
Motivation: 在非线性系统动力学未知且只有不频繁、有噪声输出测量的情况下，实现可靠的最优控制。

Method: 在连续时间动力学和潜在状态轨迹上构建贝叶斯先验并通过带数值ODE积分器的M - H采样器更新，用后验样本构建考虑模型和测量不确定性的情景最优控制问题，并用标准非线性规划方法求解。

Result: 在1型糖尿病模型的血糖调节数值案例中验证了该方法。

Conclusion: 所提方法能解决有限传感下非线性系统的可靠最优控制问题。

Abstract: Reliable optimal control is challenging when the dynamics of a nonlinear system are unknown and only infrequent, noisy output measurements are available. This work addresses this setting of limited sensing by formulating a Bayesian prior over the continuous-time dynamics and latent state trajectory in state-space form and updating it through a targeted marginal Metropolis-Hastings sampler equipped with a numerical ODE integrator. The resulting posterior samples are used to formulate a scenario-based optimal control problem that accounts for both model and measurement uncertainty and is solved using standard nonlinear programming methods. The approach is validated in a numerical case study on glucose regulation using a Type 1 diabetes model.

</details>


### [261] [Beyond Wave Variables: A Data-Driven Ensemble Approach for Enhanced Teleoperation Transparency and Stability](https://arxiv.org/abs/2512.08436)
*Nour Mitiche,Farid Ferguene,Mourad Oussalah*

Main category: eess.SY

TL;DR: 本文提出数据驱动混合框架解决双边遥操作系统通信延迟问题，实验显示优化集成模型有良好表现。


<details>
  <summary>Details</summary>
Motivation: 通信通道中的时间延迟对双边遥操作系统的透明度和稳定性有显著影响，传统基于波变量的方法易受波反射和干扰。

Method: 用三个先进序列模型的集成取代传统波变量变换，各模型通过Optuna优化器单独优化，再通过堆叠元学习器组合。

Result: 优化的集成模型在不同延迟和噪声下实现了与基线波变量系统相当的透明度，并通过无源约束确保了稳定性。

Conclusion: 所提出的数据驱动混合框架能有效应对双边遥操作系统中的通信延迟问题。

Abstract: Time delays in communication channels present significant challenges for bilateral teleoperation systems, affecting both transparency and stability. Although traditional wave variable-based methods for a four-channel architecture ensure stability via passivity, they remain vulnerable to wave reflections and disturbances like variable delays and environmental noise. This article presents a data-driven hybrid framework that replaces the conventional wave-variable transform with an ensemble of three advanced sequence models, each optimized separately via the state-of-the-art Optuna optimizer, and combined through a stacking meta-learner. The base predictors include an LSTM augmented with Prophet for trend correction, an LSTM-based feature extractor paired with clustering and a random forest for improved regression, and a CNN-LSTM model for localized and long-term dynamics. Experimental validation was performed in Python using data generated from the baseline system implemented in MATLAB/Simulink. The results show that our optimized ensemble achieves a transparency comparable to the baseline wave-variable system under varying delays and noise, while ensuring stability through passivity constraints.

</details>


### [262] [Direct transfer of optimized controllers to similar systems using dimensionless MPC](https://arxiv.org/abs/2512.08667)
*Josip Kir Hromatko,Shambhuraj Sawant,Šandor Ileš,Sébastien Gros*

Main category: eess.SY

TL;DR: 提出用无量纲模型预测控制实现控制器直接转移的方法，并在两问题上验证，代码开源。


<details>
  <summary>Details</summary>
Motivation: 解决模型实验控制器向全尺寸系统转移需额外调参的问题。

Method: 提出无量纲模型预测控制方法，用强化学习或贝叶斯优化调参。

Result: 优化控制器的闭环行为可直接转移到新的动态相似系统，无量纲公式可利用不同规模系统数据进行参数优化。

Conclusion: 所提方法能实现控制器直接转移，在两个问题上得到验证。

Abstract: Scaled model experiments are commonly used in various engineering fields to reduce experimentation costs and overcome constraints associated with full-scale systems. The relevance of such experiments relies on dimensional analysis and the principle of dynamic similarity. However, transferring controllers to full-scale systems often requires additional tuning. In this paper, we propose a method to enable a direct controller transfer using dimensionless model predictive control, tuned automatically for closed-loop performance. With this reformulation, the closed-loop behavior of an optimized controller transfers directly to a new, dynamically similar system. Additionally, the dimensionless formulation allows for the use of data from systems of different scales during parameter optimization. We demonstrate the method on a cartpole swing-up and a car racing problem, applying either reinforcement learning or Bayesian optimization for tuning the controller parameters. Software used to obtain the results in this paper is publicly available at https://github.com/josipkh/dimensionless-mpcrl.

</details>


### [263] [Gradient-Informed Monte Carlo Fine-Tuning of Diffusion Models for Low-Thrust Trajectory Design](https://arxiv.org/abs/2512.08705)
*Jannik Graebner,Ryne Beeson*

Main category: eess.SY

TL;DR: 该论文将低推力航天器轨迹初步任务设计问题转化为非归一化分布采样，扩展自监督扩散模型微调框架，对比两种算法，表明MALA在性能和计算成本间平衡最佳，还通过微调扩散模型学习全局解结构。


<details>
  <summary>Details</summary>
Motivation: 低推力航天器轨迹初步任务设计是复杂的全局搜索问题，有复杂目标景观和众多局部极小值，需新方法解决。

Method: 扩展自监督扩散模型微调框架，采用梯度信息Markov链Monte Carlo方法，对比Metropolis - Adjusted Langevin Algorithm和Hamiltonian Monte Carlo，用状态转移矩阵解析计算目标函数导数。

Result: 梯度漂移项加速了Markov链的混合并改善了收敛性；MALA在性能和计算成本间有最佳平衡，将可行性率从17.34%提高到63.01%，对Pareto前沿覆盖更密集多样。

Conclusion: 通过微调扩散模型学习问题的全局解结构，无需单独的数据生成阶段。

Abstract: Preliminary mission design of low-thrust spacecraft trajectories in the Circular Restricted Three-Body Problem is a global search characterized by a complex objective landscape and numerous local minima. Formulating the problem as sampling from an unnormalized distribution supported on neighborhoods of locally optimal solutions, provides the opportunity to deploy Markov chain Monte Carlo methods and generative machine learning. In this work, we extend our previous self-supervised diffusion model fine-tuning framework to employ gradient-informed Markov chain Monte Carlo. We compare two algorithms - the Metropolis-Adjusted Langevin Algorithm and Hamiltonian Monte Carlo - both initialized from a distribution learned by a diffusion model. Derivatives of an objective function that balances fuel consumption, time of flight and constraint violations are computed analytically using state transition matrices. We show that incorporating the gradient drift term accelerates mixing and improves convergence of the Markov chain for a multi-revolution transfer in the Saturn-Titan system. Among the evaluated methods, MALA provides the best trade-off between performance and computational cost. Starting from samples generated by a baseline diffusion model trained on a related transfer, MALA explicitly targets Pareto-optimal solutions. Compared to a random walk Metropolis algorithm, it increases the feasibility rate from 17.34% to 63.01% and produces a denser, more diverse coverage of the Pareto front. By fine-tuning a diffusion model on the generated samples and associated reward values with reward-weighted likelihood maximization, we learn the global solution structure of the problem and eliminate the need for a tedious separate data generation phase.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [264] [Referenceless Proton Resonance Frequency Thermometry Using Deep Learning with Self-Attention](https://arxiv.org/abs/2512.07882)
*Yueran Zhao,Chang-Sheng Mei,Nathan J. McDannold,Shenyan Zong,Guofeng Shen*

Main category: physics.med-ph

TL;DR: 精准的PRF MR测温对FUS热消融温度监测至关重要，传统无参考方法在组织界面磁化率引起相位不连续时易出错。


<details>
  <summary>Details</summary>
Motivation: 解决传统无参考方法在组织界面磁化率引起相位不连续时出现误差的问题，以实现准确的质子共振频率MR测温用于高强度聚焦超声热消融温度监测。

Method: 未提及

Result: 未提及

Conclusion: 未提及

Abstract: Background: Accurate proton resonance frequency (PRF) MR thermometry is essential for monitoring temperature rise during thermal ablation with high intensity focused ultrasound (FUS). Conventional referenceless methods such as complex field estimation (CFE) and phase finite difference (PFD) tend to exhibit errors when susceptibility-induced phase discontinuities occur at tissue interfaces.

</details>


### [265] [Fast and Robust Diffusion Posterior Sampling for MR Image Reconstruction Using the Preconditioned Unadjusted Langevin Algorithm](https://arxiv.org/abs/2512.05791)
*Moritz Blumenthal,Tina Holliber,Jonathan I. Tamir,Martin Uecker*

Main category: physics.med-ph

TL;DR: 本文提出结合精确似然与预条件的方法用于MRI重建，在速度和质量上优于退火采样，且无需参数调整。


<details>
  <summary>Details</summary>
Motivation: 现有基于ULA和扩散模型的MRI重建采样方法存在重建时间长和需参数调整的问题，需开发快速收敛的鲁棒采样算法。

Method: 在反向扩散过程中，将精确似然与各噪声尺度的扩散先验相乘，并使用预条件克服收敛慢的问题，在fastMRI数据上训练，在健康志愿者的回顾性欠采样脑数据上测试。

Result: 在笛卡尔和非笛卡尔加速MRI的后验采样中，新方法在重建速度和样本质量上优于退火采样。

Conclusion: 所提出的结合精确似然与预条件的方法能在各种MRI重建任务中实现快速可靠的后验采样，无需参数调整。

Abstract: Purpose: The Unadjusted Langevin Algorithm (ULA) in combination with diffusion models can generate high quality MRI reconstructions with uncertainty estimation from highly undersampled k-space data. However, sampling methods such as diffusion posterior sampling or likelihood annealing suffer from long reconstruction times and the need for parameter tuning. The purpose of this work is to develop a robust sampling algorithm with fast convergence.
  Theory and Methods: In the reverse diffusion process used for sampling the posterior, the exact likelihood is multiplied with the diffused prior at all noise scales. To overcome the issue of slow convergence, preconditioning is used. The method is trained on fastMRI data and tested on retrospectively undersampled brain data of a healthy volunteer.
  Results: For posterior sampling in Cartesian and non-Cartesian accelerated MRI the new approach outperforms annealed sampling in terms of reconstruction speed and sample quality.
  Conclusion: The proposed exact likelihood with preconditioning enables rapid and reliable posterior sampling across various MRI reconstruction tasks without the need for parameter tuning.

</details>


<div id='q-bio.PE'></div>

# q-bio.PE [[Back]](#toc)

### [266] [Harmonizing Community Science Datasets to Model Highly Pathogenic Avian Influenza (HPAI) in Birds in the Subantarctic](https://arxiv.org/abs/2512.07907)
*Richard Littauer,Kris Bubendorfer*

Main category: q-bio.PE

TL;DR: 本文提出处理社区科学数据集的工作流程，用多个数据集进行案例研究，预估鸟类种群规模和高致病性禽流感潜在死亡率。


<details>
  <summary>Details</summary>
Motivation: 社区科学观测数据集在标准化、数据质量保证和控制及工作流管理方面面临挑战，需要处理方法。

Method: 提出清洁和整合多个社区科学数据集的数据工作流程，并使用eBird、iNaturalist、GBIF等数据集进行案例研究。

Result: 预测了部分未知种群结构物种的种群规模，基于新聚合的亚南极地区死亡率数据集给出了这些物种因高致病性禽流感可能的死亡率新估计。

Conclusion: 该数据工作流程能对亚南极地区鸟类种群高致病性禽流感影响进行建模。

Abstract: Community science observational datasets are useful in epidemiology and ecology for modeling species distributions, but the heterogeneous nature of the data presents significant challenges for standardization, data quality assurance and control, and workflow management. In this paper, we present a data workflow for cleaning and harmonizing multiple community science datasets, which we implement in a case study using eBird, iNaturalist, GBIF, and other datasets to model the impact of highly pathogenic avian influenza in populations of birds in the subantarctic. We predict population sizes for several species where the demographics are not known, and we present novel estimates for potential mortality rates from HPAI for those species, based on a novel aggregated dataset of mortality rates in the subantarctic.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [267] [CrowdLLM: Building LLM-Based Digital Populations Augmented with Generative Models](https://arxiv.org/abs/2512.07890)
*Ryan Feng Lin,Keyu Tian,Hanming Zheng,Congjing Zhang,Li Zeng,Shuai Huang*

Main category: cs.MA

TL;DR: 提出CrowdLLM集成预训练大语言模型和生成模型，以增强数字人群多样性和保真度，通过理论分析和多领域实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的数字人群工作无法充分捕捉真实人群的准确性和多样性，需降低招募人类参与者成本和缓解人体研究相关问题。

Method: 提出CrowdLLM，集成预训练大语言模型和生成模型，并对其做理论分析。

Result: 在多个领域和模拟研究中进行实验，CrowdLLM在准确性和分布保真度上表现良好。

Conclusion: CrowdLLM有潜力创建具有成本效益、足够有代表性且可扩展的数字人群，质量能与真实人群匹配。

Abstract: The emergence of large language models (LLMs) has sparked much interest in creating LLM-based digital populations that can be applied to many applications such as social simulation, crowdsourcing, marketing, and recommendation systems. A digital population can reduce the cost of recruiting human participants and alleviate many concerns related to human subject study. However, research has found that most of the existing works rely solely on LLMs and could not sufficiently capture the accuracy and diversity of a real human population. To address this limitation, we propose CrowdLLM that integrates pretrained LLMs and generative models to enhance the diversity and fidelity of the digital population. We conduct theoretical analysis of CrowdLLM regarding its great potential in creating cost-effective, sufficiently representative, scalable digital populations that can match the quality of a real crowd. Comprehensive experiments are also conducted across multiple domains (e.g., crowdsourcing, voting, user rating) and simulation studies which demonstrate that CrowdLLM achieves promising performance in both accuracy and distributional fidelity to human data.

</details>


### [268] [MARINE: Theoretical Optimization and Design for Multi-Agent Recursive IN-context Enhancement](https://arxiv.org/abs/2512.07898)
*Hongwei Zhang,Ji Lu,Yongsheng Du,Yanqin Gao,Lingjun Huang,Baoli Wang,Fang Tan,Peng Zou*

Main category: cs.MA

TL;DR: 本文提出MARINE框架，将测试时推理重新概念化为对持久参考轨迹的迭代细化，在基准测试中取得SOTA结果，还建立参数高效推理新范式，有提升训练后效率的潜力。


<details>
  <summary>Details</summary>
Motivation: 基于大语言模型的智能体因实际限制常输出单响应，未充分发挥性能潜力。

Method: 引入MARINE框架，将测试时推理视为迭代细化，其细化算子将基础模型的pass@N能力转化为接近最优的pass@1性能，进行理论分析确定可行批次和批次调度。

Result: 在BrowserComp - ZH基准测试中取得SOTA结果，685B参数实现46.0% pass@1准确率，80B参数模型搭配MARINE能达到1000B参数模型性能，在固定计算预算下比传统策略提供更高质量样本。

Conclusion: MARINE框架有潜力提升训练后效率。

Abstract: Large Language Model (LLM)-based agents demonstrate advanced reasoning capabilities, yet practical constraints frequently limit outputs to single responses, leaving significant performance potential unrealized. This paper introduces MARINE (Multi-Agent Recursive IN-context Enhancement), a theoretically grounded framework that reconceptualizes test-time reasoning as iterative refinement of a persistent reference trajectory, fundamentally departing from conventional one-shot or multi-sample paradigms. The MARINE refinement operator systematically converts a base model's pass@N capabilities into near-optimal pass@1 performance. Rigorous theoretical analysis establishes that minimal feasible batches maximize expected performance gains under fixed invocation budgets, while logarithmically growing batch schedules ensure continuous improvement without computational constraints. Comprehensive evaluation on the BrowserComp-ZH benchmark demonstrates state-of-the-art results, with a 685B-parameter implementation achieving 46.0% pass@1 accuracy. Meanwhile, MARINE establishes a new paradigm for parameter-efficient reasoning: an 80B-parameter model augmented with MARINE matches the performance of standalone 1000B-parameter agents, reducing parameter requirements by over an order of magnitude. Notably, within a fixed computational budget, the proposed MARINE delivers higher-quality samples to alignment and optimization processes than traditional sampling-and-ranking strategies. Consequently, it has great potential to boost post-training efficiency.

</details>


### [269] [Probabilistic Multi-Agent Aircraft Landing Time Prediction](https://arxiv.org/abs/2512.08281)
*Kyungmin Kim,Seokbin Yoon,Keumjin Lee*

Main category: cs.MA

TL;DR: 提出概率多智能体飞机着陆时间预测框架，用韩国仁川国际机场数据评估，结果显示该模型预测精度更高，能量化不确定性且增强可解释性。


<details>
  <summary>Details</summary>
Motivation: 飞机轨迹和交通流的不确定性对着陆时间预测的准确性和可信度构成挑战，且实际需考虑空域中多智能体间相互作用。

Method: 提出一个概率多智能体飞机着陆时间预测框架，将多架飞机着陆时间表示为分布形式，并使用韩国仁川国际机场终端空域的空中交通监控数据集进行评估。

Result: 提出的模型比基线模型有更高的预测精度，能量化结果的不确定性，并通过注意力分数揭示了空中交通管制中的潜在模式。

Conclusion: 该概率多智能体飞机着陆时间预测框架有效，能提升预测准确性、量化不确定性并增强可解释性。

Abstract: Accurate and reliable aircraft landing time prediction is essential for effective resource allocation in air traffic management. However, the inherent uncertainty of aircraft trajectories and traffic flows poses significant challenges to both prediction accuracy and trustworthiness. Therefore, prediction models should not only provide point estimates of aircraft landing times but also the uncertainties associated with these predictions. Furthermore, aircraft trajectories are frequently influenced by the presence of nearby aircraft through air traffic control interventions such as radar vectoring. Consequently, landing time prediction models must account for multi-agent interactions in the airspace. In this work, we propose a probabilistic multi-agent aircraft landing time prediction framework that provides the landing times of multiple aircraft as distributions. We evaluate the proposed framework using an air traffic surveillance dataset collected from the terminal airspace of the Incheon International Airport in South Korea. The results demonstrate that the proposed model achieves higher prediction accuracy than the baselines and quantifies the associated uncertainties of its outcomes. In addition, the model uncovered underlying patterns in air traffic control through its attention scores, thereby enhancing explainability.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [270] [Fairness-aware PageRank via Edge Reweighting](https://arxiv.org/abs/2512.08055)
*Honglian Wang,Haoyun Chen,Aristides Gionis*

Main category: cs.SI

TL;DR: 本文提出将组公平性融入PageRank算法的新方法，通过重加权转移矩阵中的转移概率实现，用投影梯度下降法计算局部最优边权重，实验证明该方法有效。


<details>
  <summary>Details</summary>
Motivation: 随着负责任AI的重要性提升，链接分析算法的公平性问题受到关注，需在PageRank算法中融入组公平性。

Method: 重加权底层转移矩阵的转移概率，通过最小化公平性损失来实现公平的PageRank；定义考虑组同质性的组适应公平性概念；采用投影梯度下降法计算局部最优边权重。

Result: 通过实验与现有基准进行比较，发现转移矩阵的微小变化能显著提高PageRank算法的公平性。

Conclusion: 所提方法能在不改变网络拓扑的情况下，仅修改现有边的相对重要性，有效提升PageRank算法的公平性。

Abstract: Link-analysis algorithms, such as PageRank, are instrumental in understanding the structural dynamics of networks by evaluating the importance of individual vertices based on their connectivity. Recently, with the rising importance of responsible AI, the question of fairness in link-analysis algorithms has gained traction. In this paper, we present a new approach for incorporating group fairness into the PageRank algorithm by reweighting the transition probabilities in the underlying transition matrix. We formulate the problem of achieving fair PageRank by seeking to minimize the fairness loss, which is the difference between the original group-wise PageRank distribution and a target PageRank distribution. We further define a group-adapted fairness notion, which accounts for group homophily by considering random walks with group-biased restart for each group. Since the fairness loss is non-convex, we propose an efficient projected gradient-descent method for computing locally-optimal edge weights. Unlike earlier approaches, we do not recommend adding new edges to the network, nor do we adjust the restart vector. Instead, we keep the topology of the underlying network unchanged and only modify the relative importance of existing edges. We empirically compare our approach with state-of-the-art baselines and demonstrate the efficacy of our method, where very small changes in the transition matrix lead to significant improvement in the fairness of the PageRank algorithm.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [271] [Tumor-anchored deep feature random forests for out-of-distribution detection in lung cancer segmentation](https://arxiv.org/abs/2512.08216)
*Aneesh Rangnekar,Harini Veeraraghavan*

Main category: eess.IV

TL;DR: 提出轻量级即插即用的后处理OOD检测框架RF - Deep，用于提升CT肿瘤分割可靠性，实验表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有结合SSL预训练transformer和卷积解码器的模型易受OOD输入影响，现有检测方法存在任务特定偏差或增加计算成本问题。

Method: 引入基于随机森林的后处理OOD检测框架RF - Deep，利用预训练微调后的骨干编码器分层特征并从预测肿瘤分割的多个感兴趣区域提取特征。

Result: 在近OOD和远OOD数据集实验中，RF - Deep的AUROC分别大于93.50和99.00，远超其他方法，且在不同深度网络和预训练策略下表现一致。

Conclusion: RF - Deep是一种轻量级、与架构无关的方法，可提升CT肿瘤分割的可靠性。

Abstract: Accurate segmentation of cancerous lesions from 3D computed tomography (CT) scans is essential for automated treatment planning and response assessment. However, even state-of-the-art models combining self-supervised learning (SSL) pretrained transformers with convolutional decoders are susceptible to out-of-distribution (OOD) inputs, generating confidently incorrect tumor segmentations, posing risks for safe clinical deployment. Existing logit-based methods suffer from task-specific model biases, while architectural enhancements to explicitly detect OOD increase parameters and computational costs. Hence, we introduce a plug-and-play and lightweight post-hoc random forests-based OOD detection framework called RF-Deep that leverages deep features with limited outlier exposure. RF-Deep enhances generalization to imaging variations by repurposing the hierarchical features from the pretrained-then-finetuned backbone encoder, providing task-relevant OOD detection by extracting the features from multiple regions of interest anchored to the predicted tumor segmentations. Hence, it scales to images of varying fields-of-view. We compared RF-Deep against existing OOD detection methods using 1,916 CT scans across near-OOD (pulmonary embolism, negative COVID-19) and far-OOD (kidney cancer, healthy pancreas) datasets. RF-Deep achieved AUROC > 93.50 for the challenging near-OOD datasets and near-perfect detection (AUROC > 99.00) for the far-OOD datasets, substantially outperforming logit-based and radiomics approaches. RF-Deep maintained similar performance consistency across networks of different depths and pretraining strategies, demonstrating its effectiveness as a lightweight, architecture-agnostic approach to enhance the reliability of tumor segmentation from CT volumes.

</details>


### [272] [Learned iterative networks: An operator learning perspective](https://arxiv.org/abs/2512.08444)
*Andreas Hauptmann,Ozan Öktem*

Main category: eess.IV

TL;DR: 该文提出学习迭代网络的统一算子视图，介绍常用方法并进行数值研究。


<details>
  <summary>Details</summary>
Motivation: 学习图像重建已成为计算成像和逆问题支柱，学习方法常被视为纯离散，现需要统一视角。

Method: 提出学习重建算子，将计算方式和学习问题分开，分析框架内的线性和非线性逆问题。

Result: 展示许多方法在核心上密切相关。

Conclusion: 通过简短的数值研究得到结论，但未提及具体结论内容。

Abstract: Learned image reconstruction has become a pillar in computational imaging and inverse problems. Among the most successful approaches are learned iterative networks, which are formulated by unrolling classical iterative optimisation algorithms for solving variational problems. While the underlying algorithm is usually formulated in the functional analytic setting, learned approaches are often viewed as purely discrete. In this chapter we present a unified operator view for learned iterative networks. Specifically, we formulate a learned reconstruction operator, defining how to compute, and separately the learning problem, which defines what to compute. In this setting we present common approaches and show that many approaches are closely related in their core. We review linear as well as nonlinear inverse problems in this framework and present a short numerical study to conclude.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [273] [Exposing and Defending Membership Leakage in Vulnerability Prediction Models](https://arxiv.org/abs/2512.08291)
*Yihan Liao,Jacky Keung,Xiaoxue Ma,Jingyu Zhang,Yicheng Sun*

Main category: cs.CR

TL;DR: 本文首次全面分析会员推理攻击（MIA）对漏洞预测（VP）模型的影响并提出轻量级防御模块NMID，降低MIA有效性。


<details>
  <summary>Details</summary>
Motivation: 现有神经VP模型易受MIA攻击，引发隐私担忧，且MIA对代码分析任务的影响研究不足。

Method: 对不同架构和特征组合的VP模型进行MIA分析，提出基于噪声的会员推理防御（NMID）模块，应用输出掩码和高斯噪声注入。

Result: 实验表明logits和loss最易导致会员信息泄露，NMID显著降低MIA有效性，将攻击AUC从近1.0降至0.65以下。

Conclusion: 研究突出代码分析中的隐私风险，为保障人工智能软件系统提供可行防御策略。

Abstract: Neural models for vulnerability prediction (VP) have achieved impressive performance by learning from large-scale code repositories. However, their susceptibility to Membership Inference Attacks (MIAs), where adversaries aim to infer whether a particular code sample was used during training, poses serious privacy concerns. While MIA has been widely investigated in NLP and vision domains, its effects on security-critical code analysis tasks remain underexplored. In this work, we conduct the first comprehensive analysis of MIA on VP models, evaluating the attack success across various architectures (LSTM, BiGRU, and CodeBERT) and feature combinations, including embeddings, logits, loss, and confidence. Our threat model aligns with black-box and gray-box settings where prediction outputs are observable, allowing adversaries to infer membership by analyzing output discrepancies between training and non-training samples. The empirical findings reveal that logits and loss are the most informative and vulnerable outputs for membership leakage. Motivated by these observations, we propose a Noise-based Membership Inference Defense (NMID), which is a lightweight defense module that applies output masking and Gaussian noise injection to disrupt adversarial inference. Extensive experiments demonstrate that NMID significantly reduces MIA effectiveness, lowering the attack AUC from nearly 1.0 to below 0.65, while preserving the predictive utility of VP models. Our study highlights critical privacy risks in code analysis and offers actionable defense strategies for securing AI-powered software systems.

</details>


### [274] [Information-Dense Reasoning for Efficient and Auditable Security Alert Triage](https://arxiv.org/abs/2512.08169)
*Guangze Zhao,Yongzheng Zhang,Changbo Tian,Dan Xie,Hongri Liu,Bailing Wang*

Main category: cs.CR

TL;DR: 提出混合云边框架AIDR解决安全运营中心警报分类延迟悖论，实验显示其准确性更高、延迟降低。


<details>
  <summary>Details</summary>
Motivation: 安全运营中心面临大量异构警报流，现有解决方案无法解决警报分类延迟悖论。

Method: 提出AIDR框架，采用基于梯度的推理链压缩，构建紧凑数据集，训练领域专家，部署云边架构。

Result: AIDR比思维链准确性更高，延迟降低40.6%，对数据损坏有鲁棒性，具备分布外泛化能力。

Conclusion: AIDR可实现可审计、高效的安全运营中心警报分类，且符合数据驻留要求。

Abstract: Security Operations Centers face massive, heterogeneous alert streams under minute-level service windows, creating the Alert Triage Latency Paradox: verbose reasoning chains ensure accuracy and compliance but incur prohibitive latency and token costs, while minimal chains sacrifice transparency and auditability. Existing solutions fail: signature systems are brittle, anomaly methods lack actionability, and fully cloud-hosted LLMs raise latency, cost, and privacy concerns. We propose AIDR, a hybrid cloud-edge framework that addresses this trade-off through constrained information-density optimization. The core innovation is gradient-based compression of reasoning chains to retain only decision-critical steps--minimal evidence sufficient to justify predictions while respecting token and latency budgets. We demonstrate that this approach preserves decision-relevant information while minimizing complexity. We construct compact datasets by distilling alerts into 3-5 high-information bullets (68% token reduction), train domain-specialized experts via LoRA, and deploy a cloud-edge architecture: a cloud LLM routes alerts to on-premises experts generating SOAR-ready JSON. Experiments demonstrate AIDR achieves higher accuracy and 40.6% latency reduction versus Chain-of-Thought, with robustness to data corruption and out-of-distribution generalization, enabling auditable and efficient SOC triage with full data residency compliance.

</details>


### [275] [A Practical Framework for Evaluating Medical AI Security: Reproducible Assessment of Jailbreaking and Privacy Vulnerabilities Across Clinical Specialties](https://arxiv.org/abs/2512.08185)
*Jinghao Wang,Ping Zhang,Carter Yagemann*

Main category: cs.CR

TL;DR: 提出实用可复现框架评估医学AI安全，消除资源障碍，推进安全评估。


<details>
  <summary>Details</summary>
Motivation: 现有医学大语言模型安全评估方法存在资源门槛，限制研究参与，需实用评估框架。

Method: 设计涵盖多医学专科、多种攻击类型的评估框架，用合成患者记录，在消费级CPU运行。

Result: 给出框架规范，包括威胁模型、数据生成方法、评估协议和评分规则。

Conclusion: 为医学专业模型和防御机制的安全评估奠定基础，促进安全可信医学AI系统发展。

Abstract: Medical Large Language Models (LLMs) are increasingly deployed for clinical decision support across diverse specialties, yet systematic evaluation of their robustness to adversarial misuse and privacy leakage remains inaccessible to most researchers. Existing security benchmarks require GPU clusters, commercial API access, or protected health data -- barriers that limit community participation in this critical research area. We propose a practical, fully reproducible framework for evaluating medical AI security under realistic resource constraints. Our framework design covers multiple medical specialties stratified by clinical risk -- from high-risk domains such as emergency medicine and psychiatry to general practice -- addressing jailbreaking attacks (role-playing, authority impersonation, multi-turn manipulation) and privacy extraction attacks. All evaluation utilizes synthetic patient records requiring no IRB approval. The framework is designed to run entirely on consumer CPU hardware using freely available models, eliminating cost barriers. We present the framework specification including threat models, data generation methodology, evaluation protocols, and scoring rubrics. This proposal establishes a foundation for comparative security assessment of medical-specialist models and defense mechanisms, advancing the broader goal of ensuring safe and trustworthy medical AI systems.

</details>


### [276] [Systematization of Knowledge: Security and Safety in the Model Context Protocol Ecosystem](https://arxiv.org/abs/2512.08290)
*Shiva Gaire,Srijan Gyawali,Saroj Mishra,Suman Niroula,Dilip Thakur,Umesh Yadav*

Main category: cs.CR

TL;DR: 本文聚焦MCP，指出其在连接大语言模型与外部工具时带来新风险，进行风险分类，分析结构漏洞，调研防御措施并给出安全路线图。


<details>
  <summary>Details</summary>
Motivation: MCP在解决互操作性问题时带来新的威胁，需要对其生态系统中的风险进行全面梳理和研究。

Method: 对MCP生态系统中的风险进行分类，分析MCP原语的结构漏洞，调研现有的防御措施。

Result: 明确了MCP生态系统中的风险，包括对抗性安全威胁和认知安全隐患，展示了“context”在多智能体环境中被武器化的情况，调研了多种防御措施。

Conclusion: 给出了从对话式聊天机器人向自主智能操作系统过渡的安全路线图。

Abstract: The Model Context Protocol (MCP) has emerged as the de facto standard for connecting Large Language Models (LLMs) to external data and tools, effectively functioning as the "USB-C for Agentic AI." While this decoupling of context and execution solves critical interoperability challenges, it introduces a profound new threat landscape where the boundary between epistemic errors (hallucinations) and security breaches (unauthorized actions) dissolves. This Systematization of Knowledge (SoK) aims to provide a comprehensive taxonomy of risks in the MCP ecosystem, distinguishing between adversarial security threats (e.g., indirect prompt injection, tool poisoning) and epistemic safety hazards (e.g., alignment failures in distributed tool delegation). We analyze the structural vulnerabilities of MCP primitives, specifically Resources, Prompts, and Tools, and demonstrate how "context" can be weaponized to trigger unauthorized operations in multi-agent environments. Furthermore, we survey state-of-the-art defenses, ranging from cryptographic provenance (ETDI) to runtime intent verification, and conclude with a roadmap for securing the transition from conversational chatbots to autonomous agentic operating systems.

</details>


### [277] [Argus: A Multi-Agent Sensitive Information Leakage Detection Framework Based on Hierarchical Reference Relationships](https://arxiv.org/abs/2512.08326)
*Bin Wang,Hui Li,Liyang Zhang,Qijia Zhuang,Ao Yang,Dong Zhang,Xijun Luo,Bing Lin*

Main category: cs.CR

TL;DR: 针对代码仓库敏感信息泄漏检测难题，提出多智能体协作框架Argus，实验效果好，代码和数据集公开。


<details>
  <summary>Details</summary>
Motivation: 传统检测方法误报率高，降低检测效率、增加开发者人工筛选负担，大语言模型和多智能体协作架构带来新视角。

Method: 提出Argus框架，采用三层检测机制，开发两个新基准进行评估。

Result: Argus泄漏检测准确率达94.86%，精确率96.36%，召回率94.64%，F1分数0.955，分析97个真实仓库成本仅2.2美元。

Conclusion: Argus能有效降低误报率、提高检测准确率，代码和数据集可用于后续研究和应用。

Abstract: Sensitive information leakage in code repositories has emerged as a critical security challenge. Traditional detection methods that rely on regular expressions, fingerprint features, and high-entropy calculations often suffer from high false-positive rates. This not only reduces detection efficiency but also significantly increases the manual screening burden on developers. Recent advances in large language models (LLMs) and multi-agent collaborative architectures have demonstrated remarkable potential for tackling complex tasks, offering a novel technological perspective for sensitive information detection. In response to these challenges, we propose Argus, a multi-agent collaborative framework for detecting sensitive information. Argus employs a three-tier detection mechanism that integrates key content, file context, and project reference relationships to effectively reduce false positives and enhance overall detection accuracy. To comprehensively evaluate Argus in real-world repository environments, we developed two new benchmarks, one to assess genuine leak detection capabilities and another to evaluate false-positive filtering performance. Experimental results show that Argus achieves up to 94.86% accuracy in leak detection, with a precision of 96.36%, recall of 94.64%, and an F1 score of 0.955. Moreover, the analysis of 97 real repositories incurred a total cost of only 2.2$. All code implementations and related datasets are publicly available at https://github.com/TheBinKing/Argus-Guard for further research and application.

</details>


### [278] [LLM-based Vulnerable Code Augmentation: Generate or Refactor?](https://arxiv.org/abs/2512.08493)
*Dyna Soumhane Ouchebara,Stéphane Dupont*

Main category: cs.CR

TL;DR: 研究用大语言模型对漏洞函数进行数据增强，对比不同方法，发现混合策略能提升漏洞分类器性能。


<details>
  <summary>Details</summary>
Motivation: 漏洞代码库存在严重不平衡问题，限制深度学习漏洞分类器有效性，数据增强可缓解此问题。

Method: 使用Qwen2.5 - Coder生成增强数据，用CodeBERT作为漏洞分类器，对比新漏洞样本的受控生成和现有样本的语义保留重构两种方法。

Result: 所采用的方法能通过简单过程且以合理质量丰富漏洞代码库。

Conclusion: 混合策略最能提升漏洞分类器性能。

Abstract: Vulnerability code-bases often suffer from severe imbalance, limiting the effectiveness of Deep Learning-based vulnerability classifiers. Data Augmentation could help solve this by mitigating the scarcity of under-represented CWEs. In this context, we investigate LLM-based augmentation for vulnerable functions, comparing controlled generation of new vulnerable samples with semantics-preserving refactoring of existing ones. Using Qwen2.5-Coder to produce augmented data and CodeBERT as a vulnerability classifier on the SVEN dataset, we find that our approaches are indeed effective in enriching vulnerable code-bases through a simple process and with reasonable quality, and that a hybrid strategy best boosts vulnerability classifiers' performance.

</details>


### [279] [Democratizing ML for Enterprise Security: A Self-Sustained Attack Detection Framework](https://arxiv.org/abs/2512.08802)
*Sadegh Momeni,Ge Zhang,Birkett Huber,Hamza Harkous,Sam Lipton,Benoit Seguin,Yanis Pavlidis*

Main category: cs.CR

TL;DR: 本文提出两阶段混合框架实现基于机器学习的威胁检测，利用宽松YARA规则和ML分类器，结合Simula生成数据，经长时间测试效果良好，提供低维护方案。


<details>
  <summary>Details</summary>
Motivation: 机器学习安全方案资源需求大且有技能差距，传统规则检测方法有局限性，需新方法实现基于机器学习的威胁检测。

Method: 提出两阶段混合框架，第一阶段用宽松YARA规则粗筛，第二阶段用ML分类器过滤误报；利用Simula生成数据；通过反馈循环调整ML模型。

Result: 在生产环境长时间测试，大幅减少需人工调查的事件数量，模型精度随时间提升。

Conclusion: 该方法是自维持、低开销和低维护的解决方案，让安全专家可指导模型学习。

Abstract: Despite advancements in machine learning for security, rule-based detection remains prevalent in Security Operations Centers due to the resource intensiveness and skill gap associated with ML solutions. While traditional rule-based methods offer efficiency, their rigidity leads to high false positives or negatives and requires continuous manual maintenance. This paper proposes a novel, two-stage hybrid framework to democratize ML-based threat detection. The first stage employs intentionally loose YARA rules for coarse-grained filtering, optimized for high recall. The second stage utilizes an ML classifier to filter out false positives from the first stage's output. To overcome data scarcity, the system leverages Simula, a seedless synthetic data generation framework, enabling security analysts to create high-quality training datasets without extensive data science expertise or pre-labeled examples. A continuous feedback loop incorporates real-time investigation results to adaptively tune the ML model, preventing rule degradation.
  This proposed model with active learning has been rigorously tested for a prolonged time in a production environment spanning tens of thousands of systems. The system handles initial raw log volumes often reaching 250 billion events per day, significantly reducing them through filtering and ML inference to a handful of daily tickets for human investigation. Live experiments over an extended timeline demonstrate a general improvement in the model's precision over time due to the active learning feature. This approach offers a self-sustained, low-overhead, and low-maintenance solution, allowing security professionals to guide model learning as expert ``teachers''.

</details>


### [280] [PrivTune: Efficient and Privacy-Preserving Fine-Tuning of Large Language Models via Device-Cloud Collaboration](https://arxiv.org/abs/2512.08809)
*Yi Liu,Weixiang Han,Chengjun Cai,Xingliang Yuan,Cong Wang*

Main category: cs.CR

TL;DR: 为解决大语言模型微调中敏感数据泄露问题，提出PrivTune框架，实验显示其在隐私保护和性能上优于基线。


<details>
  <summary>Details</summary>
Motivation: 大语言模型服务中用户用私有数据集微调存在敏感数据泄露风险，现有方法难平衡隐私和效用。

Method: 提出PrivTune框架，通过拆分学习注入精心设计的噪声，将其转化为优化问题计算最优噪声向量，并调整噪声分布参数和按token重要性缩放。

Result: 在五个数据集上对抗六种攻击的实验表明，使用RoBERTa在斯坦福情感树库数据集上，PrivTune将攻击成功率降至10%，效用性能仅下降3.33%。

Conclusion: PrivTune在隐私保护和性能上优于现有基线。

Abstract: With the rise of large language models, service providers offer language models as a service, enabling users to fine-tune customized models via uploaded private datasets. However, this raises concerns about sensitive data leakage. Prior methods, relying on differential privacy within device-cloud collaboration frameworks, struggle to balance privacy and utility, exposing users to inference attacks or degrading fine-tuning performance. To address this, we propose PrivTune, an efficient and privacy-preserving fine-tuning framework via Split Learning (SL). The key idea of PrivTune is to inject crafted noise into token representations from the SL bottom model, making each token resemble the $n$-hop indirect neighbors. PrivTune formulates this as an optimization problem to compute the optimal noise vector, aligning with defense-utility goals. On this basis, it then adjusts the parameters (i.e., mean) of the $d_χ$-Privacy noise distribution to align with the optimization direction and scales the noise according to token importance to minimize distortion. Experiments on five datasets (covering both classification and generation tasks) against three embedding inversion and three attribute inference attacks show that, using RoBERTa on the Stanford Sentiment Treebank dataset, PrivTune reduces the attack success rate to 10% with only a 3.33% drop in utility performance, outperforming state-of-the-art baselines.

</details>


### [281] [Secure and Privacy-Preserving Federated Learning for Next-Generation Underground Mine Safety](https://arxiv.org/abs/2512.08862)
*Mohamed Elmahallawy,Sanjay Madria,Samuel Frimpong*

Main category: cs.CR

TL;DR: 传统地下采矿传感器数据传输至中心服务器训练ML模型有隐私安全问题，本文提出FedMining框架解决FL在地下采矿应用的挑战，评估显示其能保障隐私、保持精度且收敛快。


<details>
  <summary>Details</summary>
Motivation: 传统传输方式有隐私安全问题，FL应用于地下采矿存在对手攻击和数据非IID分布导致模型收敛难的挑战。

Method: 提出FedMining框架，包含DFE方案加密本地模型、平衡聚合机制缓解数据异质性。

Result: 在真实采矿数据集评估表明，FedMining能保障隐私，保持高模型精度，实现快速收敛，减少通信和计算开销。

Conclusion: FedMining安全且实用，适用于实时地下安全监测。

Abstract: Underground mining operations depend on sensor networks to monitor critical parameters such as temperature, gas concentration, and miner movement, enabling timely hazard detection and safety decisions. However, transmitting raw sensor data to a centralized server for machine learning (ML) model training raises serious privacy and security concerns. Federated Learning (FL) offers a promising alternative by enabling decentralized model training without exposing sensitive local data. Yet, applying FL in underground mining presents unique challenges: (i) Adversaries may eavesdrop on shared model updates to launch model inversion or membership inference attacks, compromising data privacy and operational safety; (ii) Non-IID data distributions across mines and sensor noise can hinder model convergence. To address these issues, we propose FedMining--a privacy-preserving FL framework tailored for underground mining. FedMining introduces two core innovations: (1) a Decentralized Functional Encryption (DFE) scheme that keeps local models encrypted, thwarting unauthorized access and inference attacks; and (2) a balancing aggregation mechanism to mitigate data heterogeneity and enhance convergence. Evaluations on real-world mining datasets demonstrate FedMining's ability to safeguard privacy while maintaining high model accuracy and achieving rapid convergence with reduced communication and computation overhead. These advantages make FedMining both secure and practical for real-time underground safety monitoring.

</details>


### [282] [Decentralized Trust for Space AI: Blockchain-Based Federated Learning Across Multi-Vendor LEO Satellite Networks](https://arxiv.org/abs/2512.08882)
*Mohamed Elmahallawy,Asma Jodeiri Akbarfam*

Main category: cs.CR

TL;DR: 提出区块链支持的OrbitChain框架用于低轨网络可信多供应商协作，模拟显示其能降低开销、提升性能，在真实数据集上减少收敛时间。


<details>
  <summary>Details</summary>
Motivation: 联邦卫星学习存在收敛慢和信任挑战问题，需要一个能实现低轨网络可信多供应商协作的框架。

Method: 提出OrbitChain框架，将共识卸载到高海拔平台，确保模型更新来源透明可审计，防止操纵或不完整贡献影响模型聚合。

Result: OrbitChain减少计算和通信开销，提升隐私、安全和全局模型准确性，有低延迟，在真实卫星数据集上减少收敛时间。

Conclusion: OrbitChain对实时多供应商学习有效，代码开源。

Abstract: The rise of space AI is reshaping government and industry through applications such as disaster detection, border surveillance, and climate monitoring, powered by massive data from commercial and governmental low Earth orbit (LEO) satellites. Federated satellite learning (FSL) enables joint model training without sharing raw data, but suffers from slow convergence due to intermittent connectivity and introduces critical trust challenges--where biased or falsified updates can arise across satellite constellations, including those injected through cyberattacks on inter-satellite or satellite-ground communication links. We propose OrbitChain, a blockchain-backed framework that empowers trustworthy multi-vendor collaboration in LEO networks. OrbitChain (i) offloads consensus to high-altitude platforms (HAPs) with greater computational capacity, (ii) ensures transparent, auditable provenance of model updates from different orbits owned by different vendors, and (iii) prevents manipulated or incomplete contributions from affecting global FSL model aggregation. Extensive simulations show that OrbitChain reduces computational and communication overhead while improving privacy, security, and global model accuracy. Its permissioned proof-of-authority ledger finalizes over 1000 blocks with sub-second latency (0.16,s, 0.26,s, 0.35,s for 1-of-5, 3-of-5, and 5-of-5 quorums). Moreover, OrbitChain reduces convergence time by up to 30 hours on real satellite datasets compared to single-vendor, demonstrating its effectiveness for real-time, multi-vendor learning. Our code is available at https://github.com/wsu-cyber-security-lab-ai/OrbitChain.git

</details>


<div id='hep-th'></div>

# hep-th [[Back]](#toc)

### [283] [Conformal Defects in Neural Network Field Theories](https://arxiv.org/abs/2512.07946)
*Pietro Capuozzo,Brandon Robinson,Benjamin Suzzoni*

Main category: hep-th

TL;DR: 本文介绍在神经网络场论中构建共形不变缺陷的形式体系，并在两个标量场论玩具模型中展示，还对两点关联函数的展开给出神经网络解释。


<details>
  <summary>Details</summary>
Motivation: 在神经网络场论（NN - FTs）中构建共形不变的缺陷。

Method: 提出一种形式体系，并在两个NN标量场论玩具模型中进行验证，对两点关联函数展开进行NN解释。

Result: 成功在两个玩具模型中展示新的形式体系。

Conclusion: 所提出的形式体系可用于构建神经网络场论中的共形不变缺陷。

Abstract: Neural Network Field Theories (NN-FTs) represent a novel construction of arbitrary field theories, including those of conformal fields, through the specification of the network architecture and prior distribution for the network parameters. In this work, we present a formalism for the construction of conformally invariant defects in these NN-FTs. We demonstrate this new formalism in two toy models of NN scalar field theories. We develop an NN interpretation of an expansion akin to the defect OPE in two-point correlation functions in these models.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [284] [Chat with UAV -- Human-UAV Interaction Based on Large Language Models](https://arxiv.org/abs/2512.08145)
*Haoran Wang,Zhuohang Chen,Guang Li,Bo Ma,Chuanghuang Li*

Main category: cs.RO

TL;DR: 本文提出一种新型双智能体人机交互框架，以解决当前基于大语言模型的人机交互框架在混合任务规划和执行方面的难题，实验证明该框架提升了人机交互流畅性和任务执行灵活性。


<details>
  <summary>Details</summary>
Motivation: 当前人机交互从工程师驱动转向用户驱动，但缺乏通用语言且现有基于大语言模型的人机交互框架在复杂场景适应性低，需改进。

Method: 构建两个独立的大语言模型智能体（任务规划和执行智能体），应用不同提示工程分别处理任务理解、规划和执行，构建任务数据库，用三个独立指标量化框架性能并对比不同大语言模型控制无人机的表现。

Result: 用户研究实验结果表明，该框架在设定任务场景中提升了人机交互流畅性和任务执行灵活性。

Conclusion: 该框架能有效满足用户个性化需求。

Abstract: The future of UAV interaction systems is evolving from engineer-driven to user-driven, aiming to replace traditional predefined Human-UAV Interaction designs. This shift focuses on enabling more personalized task planning and design, thereby achieving a higher quality of interaction experience and greater flexibility, which can be used in many fileds, such as agriculture, aerial photography, logistics, and environmental monitoring. However, due to the lack of a common language between users and the UAVs, such interactions are often difficult to be achieved. The developments of Large Language Models possess the ability to understand nature languages and Robots' (UAVs') behaviors, marking the possibility of personalized Human-UAV Interaction. Recently, some HUI frameworks based on LLMs have been proposed, but they commonly suffer from difficulties in mixed task planning and execution, leading to low adaptability in complex scenarios. In this paper, we propose a novel dual-agent HUI framework. This framework constructs two independent LLM agents (a task planning agent, and an execution agent) and applies different Prompt Engineering to separately handle the understanding, planning, and execution of tasks. To verify the effectiveness and performance of the framework, we have built a task database covering four typical application scenarios of UAVs and quantified the performance of the HUI framework using three independent metrics. Meanwhile different LLM models are selected to control the UAVs with compared performance. Our user study experimental results demonstrate that the framework improves the smoothness of HUI and the flexibility of task execution in the tasks scenario we set up, effectively meeting users' personalized needs.

</details>


### [285] [Embodied Tree of Thoughts: Deliberate Manipulation Planning with Embodied World Model](https://arxiv.org/abs/2512.08188)
*Wenjiang Xu,Cindy Wang,Rui Fang,Mingkang Zhang,Lusong Li,Jing Xu,Jiayuan Gu,Zecui Zeng,Rui Chen*

Main category: cs.RO

TL;DR: 提出Embodied Tree of Thoughts (EToT)规划框架解决视频生成模型在机器人操作规划中缺乏物理基础的问题，在多种任务中表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型用于机器人操作规划时缺乏严格物理基础，会产生幻觉且无法保持长视野物理约束的一致性。

Method: 提出EToT框架，将操作规划构建为树搜索，通过先验分支和反思分支两种机制扩展，在物理模拟器中进行高层推理。

Result: 在短和长视野操作任务中，EToT能有效预测物理动力学、适应潜在失败，始终优于基线。

Conclusion: EToT框架通过利用基于物理的交互式数字孪生作为具身世界模型，能确保生成的规划符合刚体动力学和碰撞约束。

Abstract: World models have emerged as a pivotal component in robot manipulation planning, enabling agents to predict future environmental states and reason about the consequences of actions before execution. While video-generation models are increasingly adopted, they often lack rigorous physical grounding, leading to hallucinations and a failure to maintain consistency in long-horizon physical constraints. To address these limitations, we propose Embodied Tree of Thoughts (EToT), a novel Real2Sim2Real planning framework that leverages a physics-based interactive digital twin as an embodied world model. EToT formulates manipulation planning as a tree search expanded through two synergistic mechanisms: (1) Priori Branching, which generates diverse candidate execution paths based on semantic and spatial analysis; and (2) Reflective Branching, which utilizes VLMs to diagnose execution failures within the simulator and iteratively refine the planning tree with corrective actions. By grounding high-level reasoning in a physics simulator, our framework ensures that generated plans adhere to rigid-body dynamics and collision constraints. We validate EToT on a suite of short- and long-horizon manipulation tasks, where it consistently outperforms baselines by effectively predicting physical dynamics and adapting to potential failures. Website at https://embodied-tree-of-thoughts.github.io .

</details>


### [286] [Model-Based Diffusion Sampling for Predictive Control in Offline Decision Making](https://arxiv.org/abs/2512.08280)
*Haldun Balim,Na Li,Yilun Du*

Main category: cs.RO

TL;DR: 提出MPDiffuser解决离线决策问题，在多个基准测试中表现优于现有方法，还可拓展到视觉控制任务和实际机器人控制。


<details>
  <summary>Details</summary>
Motivation: 现有生成方法在离线决策中生成的轨迹动态不可行，需要可靠的解决方案。

Method: 提出MPDiffuser，包含规划器、动力学模型和排序模块，采用交替扩散采样方案，并给出理论依据。

Result: 在离线决策基准测试中优于现有方法，可拓展到视觉控制任务，能应用于实际四足机器人。

Conclusion: MPDiffuser的组合设计提高了样本效率，在多种场景有应用潜力，具有实际控制的实用性。

Abstract: Offline decision-making requires synthesizing reliable behaviors from fixed datasets without further interaction, yet existing generative approaches often yield trajectories that are dynamically infeasible. We propose Model Predictive Diffuser (MPDiffuser), a compositional model-based diffusion framework consisting of: (i) a planner that generates diverse, task-aligned trajectories; (ii) a dynamics model that enforces consistency with the underlying system dynamics; and (iii) a ranker module that selects behaviors aligned with the task objectives. MPDiffuser employs an alternating diffusion sampling scheme, where planner and dynamics updates are interleaved to progressively refine trajectories for both task alignment and feasibility during the sampling process. We also provide a theoretical rationale for this procedure, showing how it balances fidelity to data priors with dynamics consistency. Empirically, the compositional design improves sample efficiency, as it leverages even low-quality data for dynamics learning and adapts seamlessly to novel dynamics. We evaluate MPDiffuser on both unconstrained (D4RL) and constrained (DSRL) offline decision-making benchmarks, demonstrating consistent gains over existing approaches. Furthermore, we present a preliminary study extending MPDiffuser to vision-based control tasks, showing its potential to scale to high-dimensional sensory inputs. Finally, we deploy our method on a real quadrupedal robot, showcasing its practicality for real-world control.

</details>


### [287] [Robust Finetuning of Vision-Language-Action Robot Policies via Parameter Merging](https://arxiv.org/abs/2512.08333)
*Yajat Yadav,Zhiyuan Zhou,Andrew Wagenmaker,Karl Pertsch,Sergey Levine*

Main category: cs.RO

TL;DR: 本文提出通过模型权重插值的方法，使通用机器人策略在微调时保留泛化能力，能学习新技能并保留原有能力，实验证明该方法有效。


<details>
  <summary>Details</summary>
Motivation: 通用机器人策略在微调新任务时易过拟合，失去原有泛化能力，需开发在微调时保留泛化能力的方法。

Method: 通过对微调模型和预训练模型的权重进行插值。

Result: 模型融合后的单一模型继承了基础模型的通用能力，能稳健解决新任务，在新任务的分布外变体上表现优于预训练和微调模型，且能在终身学习中持续获取新技能而不牺牲原有能力。

Conclusion: 通过简单有效的模型权重插值策略，可使通用机器人策略在微调时保留泛化能力，实现新技能的学习和原有能力的保留。

Abstract: Generalist robot policies, trained on large and diverse datasets, have demonstrated the ability to generalize across a wide spectrum of behaviors, enabling a single policy to act in varied real-world environments. However, they still fall short on new tasks not covered in the training data. When finetuned on limited demonstrations of a new task, these policies often overfit to the specific demonstrations--not only losing their prior abilities to solve a wide variety of generalist tasks but also failing to generalize within the new task itself. In this work, we aim to develop a method that preserves the generalization capabilities of the generalist policy during finetuning, allowing a single policy to robustly incorporate a new skill into its repertoire. Our goal is a single policy that both learns to generalize to variations of the new task and retains the broad competencies gained from pretraining. We show that this can be achieved through a simple yet effective strategy: interpolating the weights of a finetuned model with that of the pretrained model. We show, across extensive simulated and real-world experiments, that such model merging produces a single model that inherits the generalist abilities of the base model and learns to solve the new task robustly, outperforming both the pretrained and finetuned model on out-of-distribution variations of the new task. Moreover, we show that model merging enables continual acquisition of new skills in a lifelong learning setting, without sacrificing previously learned generalist abilities.

</details>


### [288] [An Introduction to Deep Reinforcement and Imitation Learning](https://arxiv.org/abs/2512.08052)
*Pedro Santana*

Main category: cs.RO

TL;DR: 介绍在具身智能体中基于学习的决策方法，聚焦DRL和DIL基础算法技术。


<details>
  <summary>Details</summary>
Motivation: 具身智能体手动设计控制器困难，需学习型方法解决顺序决策问题。

Method: 采用简洁、深度优先的文献研究方法，介绍必要数学和机器学习概念。

Result: 介绍DRL从马尔可夫决策过程到PPO等算法，DIL从行为克隆到GAIL等算法。

Conclusion: 未进行领域综述，聚焦少量基础算法技术，强调深入理解。

Abstract: Embodied agents, such as robots and virtual characters, must continuously select actions to execute tasks effectively, solving complex sequential decision-making problems. Given the difficulty of designing such controllers manually, learning-based approaches have emerged as promising alternatives, most notably Deep Reinforcement Learning (DRL) and Deep Imitation Learning (DIL). DRL leverages reward signals to optimize behavior, while DIL uses expert demonstrations to guide learning. This document introduces DRL and DIL in the context of embodied agents, adopting a concise, depth-first approach to the literature. It is self-contained, presenting all necessary mathematical and machine learning concepts as they are needed. It is not intended as a survey of the field; rather, it focuses on a small set of foundational algorithms and techniques, prioritizing in-depth understanding over broad coverage. The material ranges from Markov Decision Processes to REINFORCE and Proximal Policy Optimization (PPO) for DRL, and from Behavioral Cloning to Dataset Aggregation (DAgger) and Generative Adversarial Imitation Learning (GAIL) for DIL.

</details>


### [289] [SensHRPS: Sensing Comfortable Human-Robot Proxemics and Personal Space With Eye-Tracking](https://arxiv.org/abs/2512.08518)
*Nadezhda Kushina,Ko Watanabe,Aarthi Kannan,Ashita Ashok,Andreas Dengel,Karsten Berns*

Main category: cs.RO

TL;DR: 研究用移动眼动追踪和主观报告探究用户与机器人'Ameca'在不同距离下的舒适度，评估模型发现决策树分类器表现最佳，表明人机交互生理舒适阈值与人际不同。


<details>
  <summary>Details</summary>
Motivation: 社会机器人需适应人类空间规范，此前眼动追踪在人机交互中应用未知，故研究其在人机交互中的适用性。

Method: 通过实验控制机器人与用户的四个距离（0.5米到2.0米），使用移动眼动追踪和主观报告，评估多种机器学习和深度学习模型。

Result: 决策树分类器表现最佳（F1分数=0.73），最小瞳孔直径是最关键预测因素。

Conclusion: 人机交互的生理舒适阈值与人际动态不同，可用可解释逻辑有效建模。

Abstract: Social robots must adjust to human proxemic norms to ensure user comfort and engagement. While prior research demonstrates that eye-tracking features reliably estimate comfort in human-human interactions, their applicability to interactions with humanoid robots remains unexplored. In this study, we investigate user comfort with the robot "Ameca" across four experimentally controlled distances (0.5 m to 2.0 m) using mobile eye-tracking and subjective reporting (N=19). We evaluate multiple machine learning and deep learning models to estimate comfort based on gaze features. Contrary to previous human-human studies where Transformer models excelled, a Decision Tree classifier achieved the highest performance (F1-score = 0.73), with minimum pupil diameter identified as the most critical predictor. These findings suggest that physiological comfort thresholds in human-robot interaction differ from human-human dynamics and can be effectively modeled using interpretable logic.

</details>


### [290] [Bridging Scale Discrepancies in Robotic Control via Language-Based Action Representations](https://arxiv.org/abs/2512.08548)
*Yuchi Zhang,Churui Sun,Shiqi Liang,Diyuan Liu,Chao Ji,Wei-Nan Zhang,Ting Liu*

Main category: cs.RO

TL;DR: 现有端到端机器人操作研究受大语言模型启发，但面临动作数据分布漂移挑战，本文提出基于语义的语言表示来归一化动作以提升预训练效果和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决机器人动作数据分布漂移问题，使预训练知识更有效转移。

Method: 提出基于语义的语言表示对动作进行归一化，该表示忽略数值规模影响，强调方向性。

Result: 多任务实验表明，该方法显著提高机器人操作任务的泛化性能和可迁移性。

Conclusion: 基于语义的语言表示可有效解决机器人动作数据分布漂移，提升机器人操作任务的泛化和迁移能力。

Abstract: Recent end-to-end robotic manipulation research increasingly adopts architectures inspired by large language models to enable robust manipulation. However, a critical challenge arises from severe distribution shifts between robotic action data, primarily due to substantial numerical variations in action commands across diverse robotic platforms and tasks, hindering the effective transfer of pretrained knowledge. To address this limitation, we propose a semantically grounded linguistic representation to normalize actions for efficient pretraining. Unlike conventional discretized action representations that are sensitive to numerical scales, the motion representation specifically disregards numeric scale effects, emphasizing directionality instead. This abstraction mitigates distribution shifts, yielding a more generalizable pretraining representation. Moreover, using the motion representation narrows the feature distance between action tokens and standard vocabulary tokens, mitigating modality gaps. Multi-task experiments on two benchmarks demonstrate that the proposed method significantly improves generalization performance and transferability in robotic manipulation tasks.

</details>


### [291] [Zero-Splat TeleAssist: A Zero-Shot Pose Estimation Framework for Semantic Teleoperation](https://arxiv.org/abs/2512.08271)
*Srijan Dokania,Dharini Raghavan*

Main category: cs.RO

TL;DR: 提出Zero - Splat TeleAssist零样本传感器融合管道，用于多边远程操作。


<details>
  <summary>Details</summary>
Motivation: 将商品闭路电视（CCTV）流转换为共享的6自由度世界模型，以支持多边远程操作。

Method: 集成视觉语言分割、单目深度、加权主成分分析（PCA）姿态提取和3D高斯拼贴（3DGS）。

Result: 在以交互为中心的远程操作设置中，无需基准标记或深度传感器，为每个操作员提供多个机器人的实时全局位置和方向。

Conclusion: Zero - Splat TeleAssist管道能有效实现基于CCTV流的多边远程操作。

Abstract: We introduce Zero-Splat TeleAssist, a zero-shot sensor-fusion pipeline that transforms commodity CCTV streams into a shared, 6-DoF world model for multilateral teleoperation. By integrating vision-language segmentation, monocular depth, weighted-PCA pose extraction, and 3D Gaussian Splatting (3DGS), TeleAssist provides every operator with real-time global positions and orientations of multiple robots without fiducials or depth sensors in an interaction-centric teleoperation setup.

</details>


### [292] [Mind to Hand: Purposeful Robotic Control via Embodied Reasoning](https://arxiv.org/abs/2512.08580)
*Peijun Tang,Shangjin Xie,Binyan Sun,Baifu Huang,Kuncheng Luo,Haotian Yang,Weiqi Jin,Jianan Wang*

Main category: cs.RO

TL;DR: 提出通用视觉-语言-动作模型Lumo - 1，统一机器人推理与动作，经三阶段预训练和强化学习，实验表明其在具身视觉语言推理和机器人任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 互联网数据使AI有推理能力，但将其应用于物理动作仍是挑战，需统一机器人推理与动作。

Method: 基于预训练视觉语言模型，经三阶段预训练（继续VLM预训练、跨具身机器人数据与视觉语言数据共同训练、在Astribot S1上进行带推理过程的动作训练），并集成强化学习。

Result: Lumo - 1在具身视觉语言推理上性能显著提升，在多种机器人任务中超越基线模型，对新物体和环境泛化能力强。

Conclusion: Lumo - 1在通用机器人控制的具身视觉语言推理方面表现出色，有良好泛化能力，适用于复杂任务。

Abstract: Humans act with context and intention, with reasoning playing a central role. While internet-scale data has enabled broad reasoning capabilities in AI systems, grounding these abilities in physical action remains a major challenge. We introduce Lumo-1, a generalist vision-language-action (VLA) model that unifies robot reasoning ("mind") with robot action ("hand"). Our approach builds upon the general multi-modal reasoning capabilities of pre-trained vision-language models (VLMs), progressively extending them to embodied reasoning and action prediction, and ultimately towards structured reasoning and reasoning-action alignment. This results in a three-stage pre-training pipeline: (1) Continued VLM pre-training on curated vision-language data to enhance embodied reasoning skills such as planning, spatial understanding, and trajectory prediction; (2) Co-training on cross-embodiment robot data alongside vision-language data; and (3) Action training with reasoning process on trajectories collected on Astribot S1, a bimanual mobile manipulator with human-like dexterity and agility. Finally, we integrate reinforcement learning to further refine reasoning-action consistency and close the loop between semantic inference and motor control. Extensive experiments demonstrate that Lumo-1 achieves significant performance improvements in embodied vision-language reasoning, a critical component for generalist robotic control. Real-world evaluations further show that Lumo-1 surpasses strong baselines across a wide range of challenging robotic tasks, with strong generalization to novel objects and environments, excelling particularly in long-horizon tasks and responding to human-natural instructions that require reasoning over strategy, concepts and space.

</details>


### [293] [Data-Driven Dynamic Parameter Learning of manipulator robots](https://arxiv.org/abs/2512.08767)
*Mohammed Elseiagy,Tsige Tadesse Alemayoh,Ranulfo Bezerra,Shotaro Kojima,Kazunori Ohno*

Main category: cs.RO

TL;DR: 提出基于Transformer的动态参数估计方法，结合自动数据集生成和运动学增强，实现可扩展、准确的估计，提升机器人系统的仿真到现实迁移能力。


<details>
  <summary>Details</summary>
Motivation: 传统方法在解决机器人仿真到现实差距及动态参数估计问题上存在不足，需要新方法。

Method: 提出基于Transformer的方法，通过自动流水线生成多样机器人模型和丰富轨迹数据，利用雅可比派生特征，借助注意力机制捕捉时空依赖。

Result: 最佳配置下验证R²达0.8633，质量和惯性估计接近完美，库仑摩擦有中高准确性，粘性摩擦和远端连杆质心估计较难。

Conclusion: 结合Transformers、自动数据集生成和运动学增强可实现可扩展、准确的动态参数估计，有助于提升机器人系统仿真到现实的迁移能力。

Abstract: Bridging the sim-to-real gap remains a fundamental challenge in robotics, as accurate dynamic parameter estimation is essential for reliable model-based control, realistic simulation, and safe deployment of manipulators. Traditional analytical approaches often fall short when faced with complex robot structures and interactions. Data-driven methods offer a promising alternative, yet conventional neural networks such as recurrent models struggle to capture long-range dependencies critical for accurate estimation. In this study, we propose a Transformer-based approach for dynamic parameter estimation, supported by an automated pipeline that generates diverse robot models and enriched trajectory data using Jacobian-derived features. The dataset consists of 8,192 robots with varied inertial and frictional properties. Leveraging attention mechanisms, our model effectively captures both temporal and spatial dependencies. Experimental results highlight the influence of sequence length, sampling rate, and architecture, with the best configuration (sequence length 64, 64 Hz, four layers, 32 heads) achieving a validation R2 of 0.8633. Mass and inertia are estimated with near-perfect accuracy, Coulomb friction with moderate-to-high accuracy, while viscous friction and distal link center-of-mass remain more challenging. These results demonstrate that combining Transformers with automated dataset generation and kinematic enrichment enables scalable, accurate dynamic parameter estimation, contributing to improved sim-to-real transfer in robotic systems

</details>


### [294] [OSMO: Open-Source Tactile Glove for Human-to-Robot Skill Transfer](https://arxiv.org/abs/2512.08920)
*Jessica Yin,Haozhi Qi,Youngsun Wi,Sayantan Kundu,Mike Lambeta,William Yang,Changhao Wang,Tingfan Wu,Jitendra Malik,Tess Hellebrekers*

Main category: cs.RO

TL;DR: 介绍开源可穿戴触觉手套OSMO用于人机技能转移，机器人仅用其收集的数据训练就能执行富接触操作任务，触觉感知策略在擦拭任务表现好并开源资料。


<details>
  <summary>Details</summary>
Motivation: 视频演示缺乏关键接触信号，为解决人机技能转移中触觉信息缺失问题。

Method: 设计OSMO手套，含12个三轴触觉传感器，兼容手部追踪方法，训练机器人策略。

Result: 机器人仅用OSMO收集的人类演示数据训练，能执行富接触操作任务，触觉感知策略在擦拭任务成功率72%，优于仅视觉基线。

Conclusion: OSMO可最小化视觉和触觉具身差距，能转移连续力反馈，开源资料支持社区使用。

Abstract: Human video demonstrations provide abundant training data for learning robot policies, but video alone cannot capture the rich contact signals critical for mastering manipulation. We introduce OSMO, an open-source wearable tactile glove designed for human-to-robot skill transfer. The glove features 12 three-axis tactile sensors across the fingertips and palm and is designed to be compatible with state-of-the-art hand-tracking methods for in-the-wild data collection. We demonstrate that a robot policy trained exclusively on human demonstrations collected with OSMO, without any real robot data, is capable of executing a challenging contact-rich manipulation task. By equipping both the human and the robot with the same glove, OSMO minimizes the visual and tactile embodiment gap, enabling the transfer of continuous shear and normal force feedback while avoiding the need for image inpainting or other vision-based force inference. On a real-world wiping task requiring sustained contact pressure, our tactile-aware policy achieves a 72% success rate, outperforming vision-only baselines by eliminating contact-related failure modes. We release complete hardware designs, firmware, and assembly instructions to support community adoption.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [295] [Branching Fixed Effects: A Proposal for Communicating Uncertainty](https://arxiv.org/abs/2512.08101)
*Patrick Kline*

Main category: econ.EM

TL;DR: 提出网络数据样本拆分方法处理双向固定效应估计，用于不确定性量化等，并开发算法，用意大利数据集说明。


<details>
  <summary>Details</summary>
Motivation: 评估其他团队研究的线性固定效应模型估计的不确定性具有挑战性。

Method: 提出网络数据的样本拆分方法，将双向固定效应估计拆分为统计独立分支，开发从大型数据集提取分支的算法。

Result: 所提方法可用于不确定性量化、矩估计和收缩。

Conclusion: 所提技术能有效处理双向固定效应估计的不确定性问题。

Abstract: Economists often rely on estimates of linear fixed effects models developed by other teams of researchers. Assessing the uncertainty in these estimates can be challenging. I propose a form of sample splitting for network data that breaks two-way fixed effects estimates into statistically independent branches, each of which provides an unbiased estimate of the parameters of interest. These branches facilitate uncertainty quantification, moment estimation, and shrinkage. Algorithms are developed for efficiently extracting branches from large datasets. I illustrate these techniques using a benchmark dataset from Veneto, Italy that has been widely used to study firm wage effects.

</details>


### [296] [Minimax and Bayes Optimal Adaptive Experimental Design for Treatment Choice](https://arxiv.org/abs/2512.08513)
*Masahiro Kato*

Main category: econ.EM

TL;DR: 本文设计了一种关于遗憾值的极小极大和贝叶斯最优自适应实验，用于治疗选择，表明Neyman分配是最优的。


<details>
  <summary>Details</summary>
Motivation: 在二元治疗场景下，通过自适应实验选择预期结果最高的治疗方案以最大化福利。

Method: 提出将治疗分配阶段分为两阶段的自适应实验方法，先估计标准差再按标准差比例分配治疗方案；用测度变换推导遗憾值的极小极大和贝叶斯下界，用中心极限定理和大偏差界评估上界。

Result: 所提出的Neyman分配实验的遗憾值上界与推导的下界精确匹配。

Conclusion: Neyman分配在遗憾值方面具有极小极大和贝叶斯最优性。

Abstract: We consider an adaptive experiment for treatment choice and design a minimax and Bayes optimal adaptive experiment with respect to regret. Given binary treatments, the experimenter's goal is to choose the treatment with the highest expected outcome through an adaptive experiment, in order to maximize welfare. We consider adaptive experiments that consist of two phases, the treatment allocation phase and the treatment choice phase. The experiment starts with the treatment allocation phase, where the experimenter allocates treatments to experimental subjects to gather observations. During this phase, the experimenter can adaptively update the allocation probabilities using the observations obtained in the experiment. After the allocation phase, the experimenter proceeds to the treatment choice phase, where one of the treatments is selected as the best. For this adaptive experimental procedure, we propose an adaptive experiment that splits the treatment allocation phase into two stages, where we first estimate the standard deviations and then allocate each treatment proportionally to its standard deviation. We show that this experiment, often referred to as Neyman allocation, is minimax and Bayes optimal in the sense that its regret upper bounds exactly match the lower bounds that we derive. To show this optimality, we derive minimax and Bayes lower bounds for the regret using change-of-measure arguments. Then, we evaluate the corresponding upper bounds using the central limit theorem and large deviation bounds.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [297] [Fused Gromov-Wasserstein Contrastive Learning for Effective Enzyme-Reaction Screening](https://arxiv.org/abs/2512.08508)
*Gengmo Zhou,Feng Yu,Wenda Wang,Zhifeng Gao,Guolin Ke,Zhewei Wei,Zhen Wang*

Main category: q-bio.BM

TL;DR: 传统计算方法筛选酶耗时长、资源多，现有深度学习方法有局限。本文提出FGW - CLIP框架，该框架含多领域对齐和正则化项，实验显示其在酶筛选任务中表现优越，适应性强。


<details>
  <summary>Details</summary>
Motivation: 现有传统酶筛选计算方法耗时耗资源，现有深度学习方法仅关注酶和反应的交互，忽略域内层次关系，需改进方法提高酶筛选效率。

Method: 引入基于优化融合Gromov - Wasserstein距离的FGW - CLIP对比学习框架，包含多领域对齐和定制正则化项。

Result: 在EnzymeMap基准上取得酶虚拟筛选的最优性能，在ReactZyme基准的三个分割中均优于其他方法，能泛化到新的酶和反应。

Conclusion: FGW - CLIP是复杂生化环境中酶发现的有前途框架，在不同筛选场景有强适应性。

Abstract: Enzymes are crucial catalysts that enable a wide range of biochemical reactions. Efficiently identifying specific enzymes from vast protein libraries is essential for advancing biocatalysis. Traditional computational methods for enzyme screening and retrieval are time-consuming and resource-intensive. Recently, deep learning approaches have shown promise. However, these methods focus solely on the interaction between enzymes and reactions, overlooking the inherent hierarchical relationships within each domain. To address these limitations, we introduce FGW-CLIP, a novel contrastive learning framework based on optimizing the fused Gromov-Wasserstein distance. FGW-CLIP incorporates multiple alignments, including inter-domain alignment between reactions and enzymes and intra-domain alignment within enzymes and reactions. By introducing a tailored regularization term, our method minimizes the Gromov-Wasserstein distance between enzyme and reaction spaces, which enhances information integration across these domains. Extensive evaluations demonstrate the superiority of FGW-CLIP in challenging enzyme-reaction tasks. On the widely-used EnzymeMap benchmark, FGW-CLIP achieves state-of-the-art performance in enzyme virtual screening, as measured by BEDROC and EF metrics. Moreover, FGW-CLIP consistently outperforms across all three splits of ReactZyme, the largest enzyme-reaction benchmark, demonstrating robust generalization to novel enzymes and reactions. These results position FGW-CLIP as a promising framework for enzyme discovery in complex biochemical settings, with strong adaptability across diverse screening scenarios.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [298] [A scalable high-order multigrid-FFT Poisson solver for unbounded domains on adaptive multiresolution grids](https://arxiv.org/abs/2512.08555)
*Gilles Poncelet,Jonathan Lambrechts,Thomas Gillis,Philippe Chatelain*

Main category: math.NA

TL;DR: 介绍了在murphy框架下实现能处理任意边界条件的灵活多网格求解器，验证有效且有扩展性。


<details>
  <summary>Details</summary>
Motivation: 多网格求解器是求解泊松方程高效方法，要在murphy框架实现能处理任意边界条件的求解器。

Method: 利用基于傅里叶的直接求解器，采用高阶紧凑模板。

Result: 求解器针对周期和无界域的解析解进行了验证。

Conclusion: 求解器在欧洲高性能计算基础设施中可扩展至16384个核心。

Abstract: Multigrid solvers are among the most efficient methods for solving the Poisson equation, which is ubiquitous in computational physics. For example, in the context of incompressible flows, it is typically the costliest operation. The present document expounds upon the implementation of a flexible multigrid solver that is capable of handling any type of boundary conditions within murphy, a multiresolution framework for solving partial differential equations (PDEs) on collocated adaptive grids. The utilization of a Fourier-based direct solver facilitates the attainment of flexibility and enhanced performance by accommodating any combination of unbounded and semi-unbounded boundary conditions. The employment of high-order compact stencils contributes to the reduction of communication demands while concurrently enhancing the accuracy of the system. The resulting solver is validated against analytical solutions for periodic and unbounded domains. In conclusion, the solver has been demonstrated to demonstrate scalability to 16,384 cores within the context of leading European high-performance computing infrastructures.

</details>


### [299] [A Task Parallel Orthonormalization Multigrid Method For Multiphase Elliptic Problems](https://arxiv.org/abs/2512.08728)
*Teoman Toprak,Florian Kummer*

Main category: math.NA

TL;DR: 本文提出K - 循环正交化多重网格方法的任务并行变体，利用异步执行提升大规模并行系统的可扩展性和性能。


<details>
  <summary>Details</summary>
Motivation: 传统K - 循环正交化多重网格方法依赖整体同步并行，限制了在现代高性能计算系统上的可扩展性。

Method: 提出K - 循环正交化多重网格方法的任务并行变体，利用异步执行。

Result: 未提及具体结果。

Conclusion: 未提及具体结论。

Abstract: Multigrid methods have been a popular approach for solving linear systems arising from the discretization of partial differential equations (PDEs) for several decades. They are particularly effective for accelerating convergence rates with optimal complexity in terms of both time and space. K-cycle orthonormalization multigrid is a robust variant of the multigrid method that combines the efficiency of multigrid with the robustness of Krylov-type residual minimalizations for problems with strong anisotropies. However, traditional implementations of K-cycle orthonormalization multigrid often rely on bulk-synchronous parallelism, which can limit scalability on modern high-performance computing (HPC) systems. This paper presents a task-parallel variant of the K-cycle orthonormalization multigrid method that leverages asynchronous execution to improve scalability and performance on large-scale parallel systems.

</details>
