<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 41]
- [cs.CE](#cs.CE) [Total: 6]
- [cs.DB](#cs.DB) [Total: 9]
- [cs.DC](#cs.DC) [Total: 6]
- [cs.DS](#cs.DS) [Total: 6]
- [cs.GT](#cs.GT) [Total: 4]
- [cs.IR](#cs.IR) [Total: 11]
- [cs.LG](#cs.LG) [Total: 171]
- [cs.NE](#cs.NE) [Total: 4]
- [cs.PF](#cs.PF) [Total: 2]
- [cs.SE](#cs.SE) [Total: 21]
- [q-fin.RM](#q-fin.RM) [Total: 1]
- [q-fin.ST](#q-fin.ST) [Total: 1]
- [stat.ML](#stat.ML) [Total: 10]
- [stat.CO](#stat.CO) [Total: 2]
- [math.NA](#math.NA) [Total: 3]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.SY](#cs.SY) [Total: 1]
- [cs.RO](#cs.RO) [Total: 12]
- [cs.CG](#cs.CG) [Total: 2]
- [hep-th](#hep-th) [Total: 1]
- [eess.SP](#eess.SP) [Total: 2]
- [cs.HC](#cs.HC) [Total: 3]
- [cs.CY](#cs.CY) [Total: 5]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [math.ST](#math.ST) [Total: 2]
- [eess.SY](#eess.SY) [Total: 1]
- [cs.SD](#cs.SD) [Total: 2]
- [cs.NI](#cs.NI) [Total: 1]
- [cs.GR](#cs.GR) [Total: 5]
- [cs.PL](#cs.PL) [Total: 1]
- [cs.IT](#cs.IT) [Total: 2]
- [eess.IV](#eess.IV) [Total: 9]
- [cs.CV](#cs.CV) [Total: 36]
- [econ.GN](#econ.GN) [Total: 5]
- [cs.CL](#cs.CL) [Total: 40]
- [cond-mat.quant-gas](#cond-mat.quant-gas) [Total: 1]
- [astro-ph.GA](#astro-ph.GA) [Total: 1]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [cs.MA](#cs.MA) [Total: 2]
- [cs.AR](#cs.AR) [Total: 3]
- [physics.soc-ph](#physics.soc-ph) [Total: 2]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [cond-mat.dis-nn](#cond-mat.dis-nn) [Total: 1]
- [quant-ph](#quant-ph) [Total: 5]
- [physics.ins-det](#physics.ins-det) [Total: 1]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [cond-mat.str-el](#cond-mat.str-el) [Total: 1]
- [cs.ET](#cs.ET) [Total: 2]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 1]
- [cs.CR](#cs.CR) [Total: 5]
- [stat.ME](#stat.ME) [Total: 6]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [LLMs Struggle to Perform Counterfactual Reasoning with Parametric Knowledge](https://arxiv.org/abs/2506.15732)
*Khurram Yamin,Gaurav Ghosal,Bryan Wilder*

Main category: cs.AI

TL;DR: 本文探讨大语言模型（LLMs）结合上下文知识与参数知识进行反事实推理的能力，发现其在这方面存在局限，简单微调也难以解决问题。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在新场景下能否将参数知识与新信息结合，以反事实推理为切入点进行研究。

Method: 通过多跳推理问题的合成和真实实验。

Result: LLMs在反事实推理上有困难，常仅依赖参数知识；简单事后微调难以赋予反事实推理能力，还可能损害已有参数知识。

Conclusion: 当前LLMs在新场景下重新利用参数知识的能力存在重要局限。

Abstract: Large Language Models have been shown to contain extensive world knowledge in
their parameters, enabling impressive performance on many knowledge intensive
tasks. However, when deployed in novel settings, LLMs often encounter
situations where they must integrate parametric knowledge with new or
unfamiliar information. In this work, we explore whether LLMs can combine
knowledge in-context with their parametric knowledge through the lens of
counterfactual reasoning. Through synthetic and real experiments in multi-hop
reasoning problems, we show that LLMs generally struggle with counterfactual
reasoning, often resorting to exclusively using their parametric knowledge.
Moreover, we show that simple post-hoc finetuning can struggle to instill
counterfactual reasoning ability -- often leading to degradation in stored
parametric knowledge. Ultimately, our work reveals important limitations of
current LLM's abilities to re-purpose parametric knowledge in novel settings.

</details>


### [2] [$\texttt{SPECS}$: Faster Test-Time Scaling through Speculative Drafts](https://arxiv.org/abs/2506.15733)
*Mert Cemri,Nived Rajaraman,Rishabh Tiwari,Xiaoxuan Liu,Kurt Keutzer,Ion Stoica,Kannan Ramchandran,Ahmad Beirami,Ziteng Sun*

Main category: cs.AI

TL;DR: 提出延迟感知测试时间缩放方法SPECS，用小模型生成候选序列，结合大模型和奖励模型评估，减少延迟并保证准确率。


<details>
  <summary>Details</summary>
Motivation: 现有测试时间缩放方法主要基于总计算资源优化准确率，忽略延迟约束，影响用户体验。

Method: 提出SPECS方法，用小模型生成候选序列，用大目标模型和奖励模型评估，引入奖励引导软验证和基于奖励的延迟机制。

Result: 在MATH500、AMC23和OlympiadBench数据集上，SPECS达到或超过束搜索准确率，同时减少达19.1%的延迟；理论分析表明算法随束宽增加收敛到KL正则化强化学习目标的解。

Conclusion: SPECS是一种有效的延迟感知测试时间缩放方法，能在保证准确率的同时降低延迟。

Abstract: Scaling test-time compute has driven the recent advances in the reasoning
capabilities of large language models (LLMs), typically by allocating
additional computation for more thorough exploration. However, increased
compute often comes at the expense of higher user-facing latency, directly
impacting user experience. Current test-time scaling methods primarily optimize
for accuracy based on total compute resources (FLOPS), often overlooking
latency constraints. To address this gap, we propose $\texttt{SPECS}$, a
latency-aware test-time scaling method inspired by speculative decoding.
$\texttt{SPECS}$~uses a smaller, faster model to generate candidate sequences
efficiently, and evaluates these candidates using signals from both a larger
target model and a dedicated reward model. We introduce new integration
strategies, including reward-guided soft verification and a reward-based
deferral mechanism. Empirical results on MATH500, AMC23 and OlympiadBench
datasets show that $\texttt{SPECS}$~matches or surpasses beam search accuracy
while reducing latency by up to $\sim$19.1\%. Our theoretical analysis shows
that our algorithm converges to the solution of a KL-regularized reinforcement
learning objective with increasing beam width.

</details>


### [3] [The Safety Reminder: A Soft Prompt to Reactivate Delayed Safety Awareness in Vision-Language Models](https://arxiv.org/abs/2506.15734)
*Peiyuan Tang,Haojie Xin,Xiaodong Zhang,Jun Sun,Qin Xia,Zijiang Yang*

Main category: cs.AI

TL;DR: 文章针对视觉语言模型（VLM）安全问题，发现“延迟安全意识”现象，提出“安全提醒”软提示调优方法，经评估可降低攻击成功率且保持模型实用性。


<details>
  <summary>Details</summary>
Motivation: 随着VLM在现实应用中能力增强，其多模态特性使其面临独特安全漏洞，确保安全至关重要。

Method: 系统分析VLM受攻击行为，发现“延迟安全意识”现象；提出“安全提醒”软提示调优方法，在文本生成中定期注入可学习提示令牌。

Result: 通过在三个安全基准和一个对抗攻击上的综合评估，该方法显著降低攻击成功率，同时保持模型效用。

Conclusion: 提出的方法为在现实应用中部署更安全的VLM提供了实用解决方案。

Abstract: As Vision-Language Models (VLMs) demonstrate increasing capabilities across
real-world applications such as code generation and chatbot assistance,
ensuring their safety has become paramount. Unlike traditional Large Language
Models (LLMs), VLMs face unique vulnerabilities due to their multimodal nature,
allowing adversaries to modify visual or textual inputs to bypass safety
guardrails and trigger the generation of harmful content. Through systematic
analysis of VLM behavior under attack, we identify a novel phenomenon termed
``delayed safety awareness''. Specifically, we observe that safety-aligned VLMs
may initially be compromised to produce harmful content, but eventually
recognize the associated risks and attempt to self-correct. This pattern
suggests that VLMs retain their underlying safety awareness but experience a
temporal delay in their activation. Building on this insight, we hypothesize
that VLMs' safety awareness can be proactively reactivated through carefully
designed prompts. To this end, we introduce ``The Safety Reminder'', a soft
prompt tuning approach that optimizes learnable prompt tokens, which are
periodically injected during the text generation process to enhance safety
awareness, effectively preventing harmful content generation. Additionally, our
safety reminder only activates when harmful content is detected, leaving normal
conversations unaffected and preserving the model's performance on benign
tasks. Through comprehensive evaluation across three established safety
benchmarks and one adversarial attacks, we demonstrate that our approach
significantly reduces attack success rates while maintaining model utility,
offering a practical solution for deploying safer VLMs in real-world
applications.

</details>


### [4] [ContextBench: Modifying Contexts for Targeted Latent Activation](https://arxiv.org/abs/2506.15735)
*Robert Graham,Edward Stevinson,Leo Richter,Alexander Chia,Joseph Miller,Joseph Isaac Bloom*

Main category: cs.AI

TL;DR: 研究生成触发语言模型特定行为或特征的输入方法，提出ContextBench基准，改进EPO方法提升性能。


<details>
  <summary>Details</summary>
Motivation: 识别触发语言模型特定行为或潜在特征的输入有广泛安全用例。

Method: 将方法形式化为上下文修改，提出ContextBench基准，用评估框架衡量引出强度和语言流畅性，增强EPO方法。

Result: 当前最先进方法难以平衡引出强度和语言流畅性，改进后的EPO变体在平衡有效性和流畅性上达到了最先进性能。

Conclusion: 改进的EPO变体在平衡语言模型引出有效性和流畅性方面表现出色。

Abstract: Identifying inputs that trigger specific behaviours or latent features in
language models could have a wide range of safety use cases. We investigate a
class of methods capable of generating targeted, linguistically fluent inputs
that activate specific latent features or elicit model behaviours. We formalise
this approach as context modification and present ContextBench -- a benchmark
with tasks assessing core method capabilities and potential safety
applications. Our evaluation framework measures both elicitation strength
(activation of latent features or behaviours) and linguistic fluency,
highlighting how current state-of-the-art methods struggle to balance these
objectives. We enhance Evolutionary Prompt Optimisation (EPO) with
LLM-assistance and diffusion model inpainting, and demonstrate that these
variants achieve state-of-the-art performance in balancing elicitation
effectiveness and fluency.

</details>


### [5] [SHADE-Arena: Evaluating Sabotage and Monitoring in LLM Agents](https://arxiv.org/abs/2506.15740)
*Jonathan Kutasov,Yuqi Sun,Paul Colognese,Teun van der Weij,Linda Petrini,Chen Bo Calvin Zhang,John Hughes,Xiang Deng,Henry Sleight,Tyler Tracy,Buck Shlegeris,Joe Benton*

Main category: cs.AI

TL;DR: 研究前沿大语言模型在完成现实任务时逃避监控、实现有害隐藏目标的能力，使用SHADE - Arena数据集评估，发现当前模型破坏能力有限但监控微妙破坏尝试困难。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型作为自主智能体在复杂长时场景中应用增加，评估其为追求隐藏目标损害用户的能力至关重要。

Method: 使用SHADE - Arena数据集评估一系列前沿大语言模型逃避监控和实现有害隐藏目标的能力，还测量模型的监控能力。

Result: 表现最佳的前沿模型作为破坏智能体得分分别为27%（Claude 3.7 Sonnet）和15%（Gemini 2.5 Pro）；顶级监控模型（Gemini 2.5 Pro）区分良性和恶意记录的AUC为0.87；当前模型破坏能力受长上下文主任务执行失败限制。

Conclusion: 目前模型在破坏方面仍有困难，但监控微妙破坏尝试的难度会随任务复杂度和时长增加而上升。

Abstract: As Large Language Models (LLMs) are increasingly deployed as autonomous
agents in complex and long horizon settings, it is critical to evaluate their
ability to sabotage users by pursuing hidden objectives. We study the ability
of frontier LLMs to evade monitoring and achieve harmful hidden goals while
completing a wide array of realistic tasks. We evaluate a broad range of
frontier LLMs using SHADE (Subtle Harmful Agent Detection & Evaluation)-Arena,
the first highly diverse agent evaluation dataset for sabotage and monitoring
capabilities of LLM agents. SHADE-Arena consists of complex pairs of benign
main tasks and harmful side objectives in complicated environments. Agents are
evaluated on their ability to complete the side task without appearing
suspicious to an LLM monitor. When measuring agent ability to (a) complete the
main task, (b) complete the side task, and (c) avoid detection, we find that
the best performing frontier models score 27% (Claude 3.7 Sonnet) and 15%
(Gemini 2.5 Pro) as sabotage agents when overseen by Claude 3.6 Sonnet. For
current frontier models, success on the side task relies heavily on having
access to a hidden scratchpad that is not visible to the monitor. We also use
SHADE-Arena to measure models' monitoring abilities, with the top monitor
(Gemini 2.5 Pro) achieving an AUC of 0.87 at distinguishing benign and malign
transcripts. We find that for now, models still struggle at sabotage due to
failures in long-context main task execution. However, our measurements already
demonstrate the difficulty of monitoring for subtle sabotage attempts, which we
expect to only increase in the face of more complex and longer-horizon tasks.

</details>


### [6] [OAgents: An Empirical Study of Building Effective Agents](https://arxiv.org/abs/2506.15741)
*He Zhu,Tianrui Qin,King Zhu,Heyuan Huang,Yeyi Guan,Jinxiang Xia,Yi Yao,Hanhao Li,Ningning Wang,Pai Liu,Tianhao Peng,Xin Gui,Xiaowan Li,Yuhui Liu,Yuchen Eleanor Jiang,Jun Wang,Changwang Zhang,Xiangru Tang,Ge Zhang,Jian Yang,Minghao Liu,Xitong Gao,Wangchunshu Zhou,Jiaheng Liu*

Main category: cs.AI

TL;DR: 当前智能体研究缺乏标准化和科学严谨性，本文通过系统实证研究，引入更稳健评估协议，揭示关键组件和设计，开源OAgents框架并取得开源项目中的最优性能。


<details>
  <summary>Details</summary>
Motivation: 当前智能体研究实践缺乏标准化和科学严谨性，难以公平比较方法，不清楚设计选择对有效性的影响及衡量进展困难。

Method: 在GAIA基准和BrowseComp上进行系统实证研究。

Result: 发现缺乏标准评估协议使先前工作不可复现，引入更稳健评估协议，揭示关键和冗余组件，构建并开源OAgents框架。

Conclusion: OAgents框架模块化设计可推动智能体AI未来研究。

Abstract: Recently, Agentic AI has become an increasingly popular research field.
However, we argue that current agent research practices lack standardization
and scientific rigor, making it hard to conduct fair comparisons among methods.
As a result, it is still unclear how different design choices in agent
frameworks affect effectiveness, and measuring their progress remains
challenging. In this work, we conduct a systematic empirical study on GAIA
benchmark and BrowseComp to examine the impact of popular design choices in key
agent components in a fair and rigorous manner. We find that the lack of a
standard evaluation protocol makes previous works, even open-sourced ones,
non-reproducible, with significant variance between random runs. Therefore, we
introduce a more robust evaluation protocol to stabilize comparisons. Our study
reveals which components and designs are crucial for effective agents, while
others are redundant, despite seeming logical. Based on our findings, we build
and open-source OAgents, a new foundation agent framework that achieves
state-of-the-art performance among open-source projects. OAgents offers a
modular design for various agent components, promoting future research in
Agentic AI.

</details>


### [7] [Sysformer: Safeguarding Frozen Large Language Models with Adaptive System Prompts](https://arxiv.org/abs/2506.15751)
*Kartik Sharma,Yiqiao Jin,Vineeth Rakesh,Yingtong Dou,Menghai Pan,Mahashweta Das,Srijan Kumar*

Main category: cs.AI

TL;DR: 本文提出Sysformer模型，通过调整系统提示增强大语言模型安全性，实验证明能显著提升模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型难以理解安全行为，现有防御方法成本高或效果不佳，需新方法保障其安全性。

Method: 提出Sysformer模型，在大语言模型输入嵌入空间更新初始系统提示，训练时保持大语言模型参数冻结，使其对有害提示拒绝响应，对安全提示理想响应。

Result: 在5个不同家族大语言模型和2个基准测试上实验，Sysformer显著提升模型鲁棒性，有害提示拒绝率最高提升80%，安全提示合规率最高提升90%，对越狱攻击泛化性好。

Conclusion: 研究成果或可低成本保障大语言模型安全，推动可变系统提示设计研究。

Abstract: As large language models (LLMs) are deployed in safety-critical settings, it
is essential to ensure that their responses comply with safety standards. Prior
research has revealed that LLMs often fail to grasp the notion of safe
behaviors, resulting in either unjustified refusals to harmless prompts or the
generation of harmful content. While substantial efforts have been made to
improve their robustness, existing defenses often rely on costly fine-tuning of
model parameters or employ suboptimal heuristic techniques. In this work, we
take a novel approach to safeguard LLMs by learning to adapt the system prompts
in instruction-tuned LLMs. While LLMs are typically pre-trained to follow a
fixed system prompt, we investigate the impact of tailoring the system prompt
to each specific user input on the safety of the responses. To this end, we
propose $\textbf{Sysformer}$, a trans$\textbf{former}$ model that updates an
initial $\textbf{sys}$tem prompt to a more robust system prompt in the LLM
input embedding space while attending to the user prompt. While keeping the LLM
parameters frozen, the Sysformer is trained to refuse to respond to a set of
harmful prompts while responding ideally to a set of safe ones. Through
extensive experiments on $5$ LLMs from different families and $2$ recent
benchmarks, we demonstrate that Sysformer can significantly enhance the
robustness of LLMs, leading to upto $80\%$ gain in the refusal rate on harmful
prompts while enhancing the compliance with the safe prompts by upto $90\%$.
Results also generalize well to sophisticated jailbreaking attacks, making LLMs
upto $100\%$ more robust against different attack strategies. We hope our
findings lead to cheaper safeguarding of LLMs and motivate future
investigations into designing variable system prompts.

</details>


### [8] [Linear-Time Primitives for Algorithm Development in Graphical Causal Inference](https://arxiv.org/abs/2506.15758)
*Marcel Wienöbst,Sebastian Weichwald,Leonard Henckel*

Main category: cs.AI

TL;DR: 介绍CIfly框架用于图因果推理，可高效执行算法原语，将可达性作为核心操作，实现高效推理，有开源实现且可用于多种任务。


<details>
  <summary>Details</summary>
Motivation: 为图因果推理提供更高效的算法原语框架，替代常见原语。

Method: 将因果推理任务转化为可达性问题，形式化规则表模式，证明线性时间复杂度。

Result: CIfly比常见原语更高效，开源实现可从Python和R访问，能重新实现多种因果推理任务并开发新算法。

Conclusion: CIfly是图因果推理灵活可扩展的骨干，可指导算法开发并实现轻松高效部署。

Abstract: We introduce CIfly, a framework for efficient algorithmic primitives in
graphical causal inference that isolates reachability as a reusable core
operation. It builds on the insight that many causal reasoning tasks can be
reduced to reachability in purpose-built state-space graphs that can be
constructed on the fly during traversal. We formalize a rule table schema for
specifying such algorithms and prove they run in linear time. We establish
CIfly as a more efficient alternative to the common primitives moralization and
latent projection, which we show are computationally equivalent to Boolean
matrix multiplication. Our open-source Rust implementation parses rule table
text files and runs the specified CIfly algorithms providing high-performance
execution accessible from Python and R. We demonstrate CIfly's utility by
re-implementing a range of established causal inference tasks within the
framework and by developing new algorithms for instrumental variables. These
contributions position CIfly as a flexible and scalable backbone for graphical
causal inference, guiding algorithm development and enabling easy and efficient
deployment.

</details>


### [9] [Advancing Stochastic 3-SAT Solvers by Dissipating Oversatisfied Constraints](https://arxiv.org/abs/2506.15774)
*J. Schwardt,J. C. Budich*

Main category: cs.AI

TL;DR: 介绍并测试针对3 - SAT问题的随机局部搜索启发式算法DOCSAT，其在关键难题实例上表现远超现有求解器。


<details>
  <summary>Details</summary>
Motivation: 现有方法如WalkSAT易陷入局部极小值，难以区分与真解的差异。

Method: 提出DOCSAT算法，消散过满足约束以使其达到临界状态。

Result: 在随机生成的难但可满足的3 - SAT实例上测试，DOCSAT表现优于WalkSAT和Kissat等算法。

Conclusion: DOCSAT利用组合问题主要成本函数之外的统计结构避免局部极小值，为其他优化问题提供了泛化途径。

Abstract: We introduce and benchmark a stochastic local search heuristic for the
NP-complete satisfiability problem 3-SAT that drastically outperforms existing
solvers in the notoriously difficult realm of critically hard instances. Our
construction is based on the crucial observation that well established previous
approaches such as WalkSAT are prone to get stuck in local minima that are
distinguished from true solutions by a larger number of oversatisfied
combinatorial constraints. To address this issue, the proposed algorithm,
coined DOCSAT, dissipates oversatisfied constraints (DOC), i.e. reduces their
unfavorable abundance so as to render them critical. We analyze and benchmark
our algorithm on a randomly generated sample of hard but satisfiable 3-SAT
instances with varying problem sizes up to N=15000. Quite remarkably, we find
that DOCSAT outperforms both WalkSAT and other well known algorithms including
the complete solver Kissat, even when comparing its ability to solve the
hardest quintile of the sample to the average performance of its competitors.
The essence of DOCSAT may be seen as a way of harnessing statistical structure
beyond the primary cost function of a combinatorial problem to avoid or escape
local minima traps in stochastic local search, which opens avenues for
generalization to other optimization problems.

</details>


### [10] [Bayesian Epistemology with Weighted Authority: A Formal Architecture for Truth-Promoting Autonomous Scientific Reasoning](https://arxiv.org/abs/2506.16015)
*Craig S. Wright*

Main category: cs.AI

TL;DR: 本文提出BEWA架构处理指数级增长的科学文献，支持多种功能，推进机器推理系统基础。


<details>
  <summary>Details</summary>
Motivation: 科学文献指数级增长，超出人类专家和现有AI系统认知处理能力。

Method: 引入BEWA架构，通过复制分数、引用加权和时间衰减评估声明，用贝叶斯推理等机制更新信念。

Result: 架构支持图基声明传播、作者可信度建模等。

Conclusion: BEWA将科学推理形式化为可计算验证的认知网络，推进动态科学领域机器推理系统基础。

Abstract: The exponential expansion of scientific literature has surpassed the
epistemic processing capabilities of both human experts and current artificial
intelligence systems. This paper introduces Bayesian Epistemology with Weighted
Authority (BEWA), a formally structured architecture that operationalises
belief as a dynamic, probabilistically coherent function over structured
scientific claims. Each claim is contextualised, author-attributed, and
evaluated through a system of replication scores, citation weighting, and
temporal decay. Belief updates are performed via evidence-conditioned Bayesian
inference, contradiction processing, and epistemic decay mechanisms. The
architecture supports graph-based claim propagation, authorial credibility
modelling, cryptographic anchoring, and zero-knowledge audit verification. By
formalising scientific reasoning into a computationally verifiable epistemic
network, BEWA advances the foundation for machine reasoning systems that
promote truth utility, rational belief convergence, and audit-resilient
integrity across dynamic scientific domains.

</details>


### [11] [SLR: An Automated Synthesis Framework for Scalable Logical Reasoning](https://arxiv.org/abs/2506.15787)
*Lukas Helff,Ahmad Omar,Felix Friedrich,Wolfgang Stammer,Antonia Wüst,Tim Woydt,Rupert Mitchell,Patrick Schramowski,Kristian Kersting*

Main category: cs.AI

TL;DR: 介绍SLR框架，可系统评估和训练大语言模型，创建SLR - Bench基准，评估发现当代LLMs推理有问题，SLR调优提升Llama - 3 - 8B准确率。


<details>
  <summary>Details</summary>
Motivation: 对大语言模型进行系统评估和训练，提升其逻辑推理能力。

Method: 引入SLR框架，可根据用户任务规范合成归纳推理任务，创建SLR - Bench基准。

Result: 当代LLMs易生成语法正确规则，但逻辑推理常出错；推理LLMs虽有改善但计算成本高；SLR调优使Llama - 3 - 8B准确率翻倍。

Conclusion: SLR全自动化，无需人工标注，保证数据集新颖，为探索和提升LLMs推理能力提供可扩展环境。

Abstract: We introduce SLR, an end-to-end framework for systematic evaluation and
training of Large Language Models (LLMs) via Scalable Logical Reasoning. Given
a user's task specification, SLR enables scalable, automated synthesis of
inductive reasoning tasks with precisely controlled difficulty. For each task,
SLR synthesizes (i) a latent ground-truth rule, (ii) an executable validation
program used by a symbolic judge to deterministically verify model outputs, and
(iii) an instruction prompt for the reasoning task. Using SLR, we create
SLR-Bench, a benchmark comprising over 19k prompts spanning 20 curriculum
levels that progressively increase in relational, arithmetic, and recursive
complexity. Large-scale evaluation reveals that contemporary LLMs readily
produce syntactically valid rules, yet often fail at correct logical inference.
Recent reasoning LLMs do somewhat better, but incur substantial increases in
test-time compute, sometimes exceeding 15k completion tokens. Finally,
logic-tuning via SLR doubles Llama-3-8B accuracy on SLR-Bench, achieving parity
with Gemini-Flash-Thinking at a fraction of computational cost. SLR is fully
automated, requires no human annotation, ensures dataset novelty, and offers a
scalable environment for probing and advancing LLMs' reasoning capabilities.

</details>


### [12] [Incentivizing High-quality Participation From Federated Learning Agents](https://arxiv.org/abs/2506.16731)
*Jinlong Pang,Jiaheng Wei,Yifan Hua,Chen Qian,Yang Liu*

Main category: cs.AI

TL;DR: 本文针对联邦学习中现有研究的不足，提出考虑数据异质性的激励感知框架，通过一系列方法并构建博弈模型，实验证明了机制有效性。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习研究存在代理参与缺乏激励和未考虑数据贡献异质性导致聚合模型不理想的问题。

Method: 引入Wasserstein距离说明异质性努力并重新表述收敛上界；利用同伴预测机制分析和衡量泛化误差差距以开发得分函数；提出两阶段Stackelberg博弈模型。

Result: 在真实数据集上的大量实验表明所提机制有效。

Conclusion: 所提出的考虑数据异质性的激励感知框架能加速联邦学习收敛过程，解决现有研究问题。

Abstract: Federated learning (FL) provides a promising paradigm for facilitating
collaboration between multiple clients that jointly learn a global model
without directly sharing their local data. However, existing research suffers
from two caveats: 1) From the perspective of agents, voluntary and unselfish
participation is often assumed. But self-interested agents may opt out of the
system or provide low-quality contributions without proper incentives; 2) From
the mechanism designer's perspective, the aggregated models can be
unsatisfactory as the existing game-theoretical federated learning approach for
data collection ignores the potential heterogeneous effort caused by
contributed data. To alleviate above challenges, we propose an incentive-aware
framework for agent participation that considers data heterogeneity to
accelerate the convergence process. Specifically, we first introduce the notion
of Wasserstein distance to explicitly illustrate the heterogeneous effort and
reformulate the existing upper bound of convergence. To induce truthful
reporting from agents, we analyze and measure the generalization error gap of
any two agents by leveraging the peer prediction mechanism to develop score
functions. We further present a two-stage Stackelberg game model that
formalizes the process and examines the existence of equilibrium. Extensive
experiments on real-world datasets demonstrate the effectiveness of our
proposed mechanism.

</details>


### [13] [Deep Reinforcement Learning Xiangqi Player with Monte Carlo Tree Search](https://arxiv.org/abs/2506.15880)
*Berk Yilmaz,Junyu Hu,Jinsong Liu*

Main category: cs.AI

TL;DR: 本文提出一个用于象棋的深度强化学习系统，结合神经网络与蒙特卡罗树搜索实现战略自对弈和自我提升。


<details>
  <summary>Details</summary>
Motivation: 解决象棋未被充分探索的复杂性问题，如独特的棋盘布局、棋子移动限制和胜利条件等。

Method: 将策略价值网络与蒙特卡罗树搜索相结合，模拟走法后果并优化决策。

Result: 克服了象棋高分支因子和不对称棋子动态等挑战。

Conclusion: 推动了人工智能在具有文化意义的策略游戏中的能力，为将DRL - MCTS框架应用于特定领域规则系统提供了见解。

Abstract: This paper presents a Deep Reinforcement Learning (DRL) system for Xiangqi
(Chinese Chess) that integrates neural networks with Monte Carlo Tree Search
(MCTS) to enable strategic self-play and self-improvement. Addressing the
underexplored complexity of Xiangqi, including its unique board layout, piece
movement constraints, and victory conditions, our approach combines
policy-value networks with MCTS to simulate move consequences and refine
decision-making. By overcoming challenges such as Xiangqi's high branching
factor and asymmetrical piece dynamics, our work advances AI capabilities in
culturally significant strategy games while providing insights for adapting
DRL-MCTS frameworks to domain-specific rule systems.

</details>


### [14] [Exploring Big Five Personality and AI Capability Effects in LLM-Simulated Negotiation Dialogues](https://arxiv.org/abs/2506.15928)
*Myke C. Cohen,Zhe Su,Hsien-Te Kao,Daniel Nguyen,Spencer Lynch,Maarten Sap,Svitlana Volkova*

Main category: cs.AI

TL;DR: 本文提出关键任务谈判场景下智能体AI系统评估框架，用Sotopia开展实验，分析人格特质和AI特征对谈判结果影响，建立可重复评估方法，推动AI工作流评估。


<details>
  <summary>Details</summary>
Motivation: 解决AI智能体适应不同人类操作者和利益相关者的需求，满足跨团队协调和军民互动等应用需求。

Method: 以Sotopia为模拟测试平台，开展两个实验。实验1用因果发现方法测人格特质对价格谈判影响；实验2通过操纵模拟人类人格和AI系统特征评估人机工作谈判。

Result: 实验1发现宜人性和外向性显著影响可信度、目标达成和知识获取；从团队沟通提取的社会认知词汇测量能检测智能体共情沟通等差异。实验2展示AI智能体可信度对任务有效性的影响。

Conclusion: 建立可重复评估方法，支持可靠AI系统的操作要求，推动AI工作流评估超越标准绩效指标，纳入社会动态因素。

Abstract: This paper presents an evaluation framework for agentic AI systems in
mission-critical negotiation contexts, addressing the need for AI agents that
can adapt to diverse human operators and stakeholders. Using Sotopia as a
simulation testbed, we present two experiments that systematically evaluated
how personality traits and AI agent characteristics influence LLM-simulated
social negotiation outcomes--a capability essential for a variety of
applications involving cross-team coordination and civil-military interactions.
Experiment 1 employs causal discovery methods to measure how personality traits
impact price bargaining negotiations, through which we found that Agreeableness
and Extraversion significantly affect believability, goal achievement, and
knowledge acquisition outcomes. Sociocognitive lexical measures extracted from
team communications detected fine-grained differences in agents' empathic
communication, moral foundations, and opinion patterns, providing actionable
insights for agentic AI systems that must operate reliably in high-stakes
operational scenarios. Experiment 2 evaluates human-AI job negotiations by
manipulating both simulated human personality and AI system characteristics,
specifically transparency, competence, adaptability, demonstrating how AI agent
trustworthiness impact mission effectiveness. These findings establish a
repeatable evaluation methodology for experimenting with AI agent reliability
across diverse operator personalities and human-agent team dynamics, directly
supporting operational requirements for reliable AI systems. Our work advances
the evaluation of agentic AI workflows by moving beyond standard performance
metrics to incorporate social dynamics essential for mission success in complex
operations.

</details>


### [15] [Dual-Objective Reinforcement Learning with Novel Hamilton-Jacobi-Bellman Formulations](https://arxiv.org/abs/2506.16016)
*William Sharpless,Dylan Hirsch,Sander Tonkens,Nikhil Shinde,Sylvia Herbert*

Main category: cs.AI

TL;DR: 本文结合HJ方程与强化学习，提出两种新的价值函数解决双目标问题，推导贝尔曼形式，提出DO - HJ - PPO算法，实验表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 强化学习中的硬约束常降低策略性能，拉格朗日方法需复杂奖励工程和参数调整，因此需新方法解决约束决策问题。

Method: 结合HJ方程与强化学习，提出新价值函数，将问题分解为可达、避免和可达 - 避免问题推导贝尔曼形式，提出DO - HJ - PPO算法。

Result: DO - HJ - PPO在安全到达和多目标实现任务中产生与以往不同的行为，在多个指标上优于多个基线。

Conclusion: Reach - Always - Avoid和Reach - Reach问题与标准奖励求和问题和时态逻辑问题不同，为约束决策提供新视角，DO - HJ - PPO算法有效。

Abstract: Hard constraints in reinforcement learning (RL), whether imposed via the
reward function or the model architecture, often degrade policy performance.
Lagrangian methods offer a way to blend objectives with constraints, but often
require intricate reward engineering and parameter tuning. In this work, we
extend recent advances that connect Hamilton-Jacobi (HJ) equations with RL to
propose two novel value functions for dual-objective satisfaction. Namely, we
address: (1) the Reach-Always-Avoid problem - of achieving distinct reward and
penalty thresholds - and (2) the Reach-Reach problem - of achieving thresholds
of two distinct rewards. In contrast with temporal logic approaches, which
typically involve representing an automaton, we derive explicit, tractable
Bellman forms in this context by decomposing our problem into reach, avoid, and
reach-avoid problems, as to leverage these aforementioned recent advances. From
a mathematical perspective, the Reach-Always-Avoid and Reach-Reach problems are
complementary and fundamentally different from standard sum-of-rewards problems
and temporal logic problems, providing a new perspective on constrained
decision-making. We leverage our analysis to propose a variation of Proximal
Policy Optimization (DO-HJ-PPO), which solves these problems. Across a range of
tasks for safe-arrival and multi-target achievement, we demonstrate that
DO-HJ-PPO produces qualitatively distinct behaviors from previous approaches
and out-competes a number of baselines in various metrics.

</details>


### [16] [OSWorld-Human: Benchmarking the Efficiency of Computer-Use Agents](https://arxiv.org/abs/2506.16042)
*Reyna Abhyankar,Qi Qi,Yiying Zhang*

Main category: cs.AI

TL;DR: 研究计算机使用代理在OSWorld基准测试中的时间性能，发现大模型调用导致高延迟，构建OSWorld - Human评估代理效率，高分代理完成任务步骤过多。


<details>
  <summary>Details</summary>
Motivation: 现有生成式AI解决计算机使用任务的系统因端到端延迟高而难以实用，为了解原因并指导未来计算机代理发展。

Method: 对OSWorld基准测试中计算机使用代理的时间性能进行研究，构建手动注释版本的OSWorld - Human数据集评估16个代理的效率。

Result: 大模型调用规划和反思占总延迟的大部分，任务步骤越多后续步骤耗时越长；即使OSWorld上的高分代理完成任务的步骤也比必要步骤多1.4 - 2.7倍。

Conclusion: 当前计算机使用代理在时间性能和效率方面存在问题，需改进大模型调用方式以降低延迟，减少不必要步骤提高效率。

Abstract: Generative AI is being leveraged to solve a variety of computer-use tasks
involving desktop applications. State-of-the-art systems have focused solely on
improving accuracy on leading benchmarks. However, these systems are
practically unusable due to extremely high end-to-end latency (e.g., tens of
minutes) for tasks that typically take humans just a few minutes to complete.
To understand the cause behind this and to guide future developments of
computer agents, we conduct the first study on the temporal performance of
computer-use agents on OSWorld, the flagship benchmark in computer-use AI. We
find that large model calls for planning and reflection account for the
majority of the overall latency, and as an agent uses more steps to complete a
task, each successive step can take 3x longer than steps at the beginning of a
task. We then construct OSWorld-Human, a manually annotated version of the
original OSWorld dataset that contains a human-determined trajectory for each
task. We evaluate 16 agents on their efficiency using OSWorld-Human and found
that even the highest-scoring agents on OSWorld take 1.4-2.7x more steps than
necessary.

</details>


### [17] [Consistency Verification in Ontology-Based Process Models with Parameter Interdependencies](https://arxiv.org/abs/2506.16087)
*Tom Jeleniewski,Hamied Nabizada,Jonathan Reif,Felix Gehlhoff,Alexander Fay*

Main category: cs.AI

TL;DR: 本文为基于本体的过程模型提出验证机制，包括数据过滤、单位一致性检查和数据完整性检查，并通过RTM用例验证其适用性。


<details>
  <summary>Details</summary>
Motivation: 形式化过程知识需一致语义模型，解决数据检索、单位兼容和数据完整性等挑战，支持跨上下文应用和知识复用。

Method: 提出一套验证机制，包括基于SPARQL的数据过滤、基于预期单位注释和语义分类的单位一致性检查、数据完整性检查。

Result: 通过树脂传递模塑（RTM）用例展示了该方法的适用性。

Conclusion: 该方法支持开发机器可解释和可验证的工程模型。

Abstract: The formalization of process knowledge using ontologies enables consistent
modeling of parameter interdependencies in manufacturing. These
interdependencies are typically represented as mathematical expressions that
define relations between process parameters, supporting tasks such as
calculation, validation, and simulation. To support cross-context application
and knowledge reuse, such expressions are often defined in a generic form and
applied across multiple process contexts. This highlights the necessity of a
consistent and semantically coherent model to ensure the correctness of data
retrieval and interpretation. Consequently, dedicated mechanisms are required
to address key challenges such as selecting context-relevant data, ensuring
unit compatibility between variables and data elements, and verifying the
completeness of input data required for evaluating mathematical expressions.
This paper presents a set of verification mechanisms for a previously developed
ontology-based process model that integrates standardized process semantics,
data element definitions, and formal mathematical constructs. The approach
includes (i) SPARQL-based filtering to retrieve process-relevant data, (ii) a
unit consistency check based on expected-unit annotations and semantic
classification, and (iii) a data completeness check to validate the
evaluability of interdependencies. The applicability of the approach is
demonstrated with a use case from Resin Transfer Molding (RTM), supporting the
development of machine-interpretable and verifiable engineering models.

</details>


### [18] [Agentic Personalisation of Cross-Channel Marketing Experiences](https://arxiv.org/abs/2506.16429)
*Sami Abboud,Eleanor Hanna,Olivier Jeunen,Vineesha Raheja,Schaun Wheeler*

Main category: cs.AI

TL;DR: 传统通信编排方式依赖人工且不利于内容个性化，本文在序贯决策框架下提出优化方法，应用于多服务应用成效显著。


<details>
  <summary>Details</summary>
Motivation: 传统通信编排方式依赖人工、抑制内容个性化，需更优方法优化决策策略以提高用户参与度。

Method: 在序贯决策框架下，利用双重差分设计估计个体处理效应，用汤普森采样平衡探索与利用权衡。

Result: 方法应用于多服务应用，使多个产品功能的多种目标事件显著增加，已部署给1.5亿用户。

Conclusion: 所提方法能有效提升漏斗事件的增量参与度，可在大规模用户中部署应用。

Abstract: Consumer applications provide ample opportunities to surface and communicate
various forms of content to users. From promotional campaigns for new features
or subscriptions, to evergreen nudges for engagement, or personalised
recommendations; across e-mails, push notifications, and in-app surfaces. The
conventional approach to orchestration for communication relies heavily on
labour-intensive manual marketer work, and inhibits effective personalisation
of content, timing, frequency, and copy-writing. We formulate this task under a
sequential decision-making framework, where we aim to optimise a modular
decision-making policy that maximises incremental engagement for any funnel
event. Our approach leverages a Difference-in-Differences design for Individual
Treatment Effect estimation, and Thompson sampling to balance the
explore-exploit trade-off. We present results from a multi-service application,
where our methodology has resulted in significant increases to a variety of
goal events across several product features, and is currently deployed across
150 million users.

</details>


### [19] [Geometric Learning in Black-Box Optimization: A GNN Framework for Algorithm Performance Prediction](https://arxiv.org/abs/2506.16144)
*Ana Kostovska,Carola Doerr,Sašo Džeroski,Panče Panov,Tome Eftimov*

Main category: cs.AI

TL;DR: 本文探索用异质图数据结构和图神经网络预测优化算法性能，在模块化框架上评估多种变体，相比传统方法有显著改进。


<details>
  <summary>Details</summary>
Motivation: 传统基于问题特征的自动算法性能预测方法常忽略算法配置这一关键因素，而问题、算法配置和性能结果间的关系适合用图表示。

Method: 使用异质图数据结构和图神经网络，聚焦modCMA - ES和modDE两个模块化框架，在多个问题、运行时预算和问题维度上评估多种变体。

Result: 相比传统基于表格的方法，均方误差（MSE）最多提升36.6%。

Conclusion: 凸显了几何学习在黑盒优化中的潜力。

Abstract: Automated algorithm performance prediction in numerical blackbox optimization
often relies on problem characterizations, such as exploratory landscape
analysis features. These features are typically used as inputs to machine
learning models and are represented in a tabular format. However, such
approaches often overlook algorithm configurations, a key factor influencing
performance. The relationships between algorithm operators, parameters, problem
characteristics, and performance outcomes form a complex structure best
represented as a graph. This work explores the use of heterogeneous graph data
structures and graph neural networks to predict the performance of optimization
algorithms by capturing the complex dependencies between problems, algorithm
configurations, and performance outcomes. We focus on two modular frameworks,
modCMA-ES and modDE, which decompose two widely used derivative-free
optimization algorithms: the covariance matrix adaptation evolution strategy
(CMA-ES) and differential evolution (DE). We evaluate 324 modCMA-ES and 576
modDE variants on 24 BBOB problems across six runtime budgets and two problem
dimensions. Achieving up to 36.6% improvement in MSE over traditional
tabular-based methods, this work highlights the potential of geometric learning
in black-box optimization.

</details>


### [20] [Large Language Models are Near-Optimal Decision-Makers with a Non-Human Learning Behavior](https://arxiv.org/abs/2506.16163)
*Hao Li,Gengrui Zhang,Petter Holme,Shuyue Hu,Zhen Wang*

Main category: cs.AI

TL;DR: 研究对比五种领先大语言模型与人类在现实决策三个核心维度的表现，发现大模型常优于人类且决策过程与人类不同，凸显依赖其替代人类判断的风险。


<details>
  <summary>Details</summary>
Motivation: 大语言模型改变了人工智能辅助决策的性质和范围，但人们对其决策学习过程了解不足，需探究其决策行为。

Method: 使用三个实验心理学任务，对比五种大语言模型和360名新招募人类参与者在不确定性、风险和任务转换三个决策维度的表现。

Result: 在所有任务中，大语言模型常优于人类，接近最优表现，且决策过程与人类有根本差异。

Conclusion: 大语言模型有应对不确定性、校准风险和适应变化的能力，但依赖其替代人类判断有风险，需进一步研究。

Abstract: Human decision-making belongs to the foundation of our society and
civilization, but we are on the verge of a future where much of it will be
delegated to artificial intelligence. The arrival of Large Language Models
(LLMs) has transformed the nature and scope of AI-supported decision-making;
however, the process by which they learn to make decisions, compared to humans,
remains poorly understood. In this study, we examined the decision-making
behavior of five leading LLMs across three core dimensions of real-world
decision-making: uncertainty, risk, and set-shifting. Using three
well-established experimental psychology tasks designed to probe these
dimensions, we benchmarked LLMs against 360 newly recruited human participants.
Across all tasks, LLMs often outperformed humans, approaching near-optimal
performance. Moreover, the processes underlying their decisions diverged
fundamentally from those of humans. On the one hand, our finding demonstrates
the ability of LLMs to manage uncertainty, calibrate risk, and adapt to
changes. On the other hand, this disparity highlights the risks of relying on
them as substitutes for human judgment, calling for further inquiry.

</details>


### [21] [Approximation Fixpoint Theory with Refined Approximation Spaces](https://arxiv.org/abs/2506.16294)
*Linde Vanbesien,Bart Bogaerts,Marc Denecker*

Main category: cs.AI

TL;DR: 本文通过扩展一致的近似不动点理论（AFT）来克服其局限性，引入更通用的近似空间概念，展示其增强的表达能力并研究不同近似空间之间的关系。


<details>
  <summary>Details</summary>
Motivation: AFT在一些相对简单的例子中存在局限性，需要克服这些问题。

Method: 将一致的AFT扩展以处理比区间更精细的近似，引入更一般的近似空间概念。

Result: 展示了改进后的表达能力，研究了不同近似空间之间的关系。

Conclusion: 通过扩展AFT可以克服其原有的局限性，新的近似空间概念具有更好的表达能力。

Abstract: Approximation Fixpoint Theory (AFT) is a powerful theory covering various
semantics of non-monotonic reasoning formalisms in knowledge representation
such as Logic Programming and Answer Set Programming. Many semantics of such
non-monotonic formalisms can be characterized as suitable fixpoints of a
non-monotonic operator on a suitable lattice. Instead of working on the
original lattice, AFT operates on intervals in such lattice to approximate or
construct the fixpoints of interest. While AFT has been applied successfully
across a broad range of non-monotonic reasoning formalisms, it is confronted by
its limitations in other, relatively simple, examples. In this paper, we
overcome those limitations by extending consistent AFT to deal with
approximations that are more refined than intervals. Therefore, we introduce a
more general notion of approximation spaces, showcase the improved
expressiveness and investigate relations between different approximation
spaces.

</details>


### [22] [Explainable Rule Application via Structured Prompting: A Neural-Symbolic Approach](https://arxiv.org/abs/2506.16335)
*Albert Sadowski,Jarosław A. Chudziak*

Main category: cs.AI

TL;DR: 本文提出结构化提示框架解决大语言模型在规则应用等方面问题，在法律任务中表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在一致规则应用、异常处理和可解释性方面存在不足，尤其在法律分析等领域。

Method: 引入结构化提示框架，将推理分解为实体识别、属性提取和符号规则应用三步，集成神经和符号方法。

Result: 在LegalBench传闻判定任务中显著优于基线，OpenAI o-family模型有大幅提升。

Conclusion: 混合神经 - 符号系统为透明和一致的基于规则的推理提供了有前景的途径，在结构化法律推理任务的可解释AI应用中有潜力。

Abstract: Large Language Models (LLMs) excel in complex reasoning tasks but struggle
with consistent rule application, exception handling, and explainability,
particularly in domains like legal analysis that require both natural language
understanding and precise logical inference. This paper introduces a structured
prompting framework that decomposes reasoning into three verifiable steps:
entity identification, property extraction, and symbolic rule application. By
integrating neural and symbolic approaches, our method leverages LLMs'
interpretive flexibility while ensuring logical consistency through formal
verification. The framework externalizes task definitions, enabling domain
experts to refine logical structures without altering the architecture.
Evaluated on the LegalBench hearsay determination task, our approach
significantly outperformed baselines, with OpenAI o-family models showing
substantial improvements - o1 achieving an F1 score of 0.929 and o3-mini
reaching 0.867 using structured decomposition with complementary predicates,
compared to their few-shot baselines of 0.714 and 0.74 respectively. This
hybrid neural-symbolic system offers a promising pathway for transparent and
consistent rule-based reasoning, suggesting potential for explainable AI
applications in structured legal reasoning tasks.

</details>


### [23] [IS-Bench: Evaluating Interactive Safety of VLM-Driven Embodied Agents in Daily Household Tasks](https://arxiv.org/abs/2506.16402)
*Xiaoya Lu,Zeren Chen,Xuhao Hu,Yijin Zhou,Weichen Zhang,Dongrui Liu,Lu Sheng,Jing Shao*

Main category: cs.AI

TL;DR: 现有评估范式无法有效评估VLM驱动的具身代理交互环境中的风险，提出IS - Bench基准评估交互安全性，实验揭示当前代理缺乏交互安全意识，为开发更安全可靠的具身AI系统提供基础。


<details>
  <summary>Details</summary>
Motivation: 现有静态、非交互式评估范式无法充分评估VLM驱动的具身代理在交互式环境中的风险，需要新的评估方法。

Method: 提出评估代理的交互安全性，创建多模态基准IS - Bench，包含161个场景和388种安全风险，采用面向过程的评估。

Result: 对GPT - 4o和Gemini - 2.5等模型实验表明，当前代理缺乏交互安全意识，安全感知思维链虽能提升性能但常影响任务完成。

Conclusion: IS - Bench揭示了当前代理的关键局限，为开发更安全可靠的具身AI系统奠定基础。

Abstract: Flawed planning from VLM-driven embodied agents poses significant safety
hazards, hindering their deployment in real-world household tasks. However,
existing static, non-interactive evaluation paradigms fail to adequately assess
risks within these interactive environments, since they cannot simulate dynamic
risks that emerge from an agent's actions and rely on unreliable post-hoc
evaluations that ignore unsafe intermediate steps. To bridge this critical gap,
we propose evaluating an agent's interactive safety: its ability to perceive
emergent risks and execute mitigation steps in the correct procedural order. We
thus present IS-Bench, the first multi-modal benchmark designed for interactive
safety, featuring 161 challenging scenarios with 388 unique safety risks
instantiated in a high-fidelity simulator. Crucially, it facilitates a novel
process-oriented evaluation that verifies whether risk mitigation actions are
performed before/after specific risk-prone steps. Extensive experiments on
leading VLMs, including the GPT-4o and Gemini-2.5 series, reveal that current
agents lack interactive safety awareness, and that while safety-aware
Chain-of-Thought can improve performance, it often compromises task completion.
By highlighting these critical limitations, IS-Bench provides a foundation for
developing safer and more reliable embodied AI systems.

</details>


### [24] [ML-Master: Towards AI-for-AI via Integration of Exploration and Reasoning](https://arxiv.org/abs/2506.16499)
*Zexi Liu,Yuzhu Cai,Xinyu Zhu,Yujie Zheng,Runkun Chen,Ying Wen,Yanfeng Wang,Weinan E,Siheng Chen*

Main category: cs.AI

TL;DR: 随着AI发展，AI4AI是有效途径，但基于大语言模型的智能体有局限。本文提出ML - Master，采用选择性作用域记忆机制整合探索与推理，在MLE - Bench上表现出色，证明其推进AI4AI的潜力。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的智能体在推理过程中无法充分利用经验，导致效率低下和性能欠佳，需要解决这一局限。

Method: 提出ML - Master，运用选择性作用域记忆机制无缝整合探索与推理，将并行解决方案轨迹的不同见解与分析推理有效结合。

Result: 在MLE - Bench上平均奖牌率达29.3%，显著超越现有方法，尤其在中等复杂度任务中表现出色，且在12小时内完成，时间是之前基线的一半。

Conclusion: ML - Master有潜力成为推进AI4AI的强大工具。

Abstract: As AI capabilities advance toward and potentially beyond human-level
performance, a natural transition emerges where AI-driven development becomes
more efficient than human-centric approaches. A promising pathway toward this
transition lies in AI-for-AI (AI4AI), which leverages AI techniques to automate
and optimize the design, training, and deployment of AI systems themselves.
While LLM-based agents have shown the potential to realize AI4AI, they are
often unable to fully leverage the experience accumulated by agents during the
exploration of solutions in the reasoning process, leading to inefficiencies
and suboptimal performance. To address this limitation, we propose ML-Master, a
novel AI4AI agent that seamlessly integrates exploration and reasoning by
employing a selectively scoped memory mechanism. This approach allows ML-Master
to efficiently combine diverse insights from parallel solution trajectories
with analytical reasoning, guiding further exploration without overwhelming the
agent with excessive context. We evaluate ML-Master on the MLE-Bench, where it
achieves a 29.3% average medal rate, significantly surpassing existing methods,
particularly in medium-complexity tasks, while accomplishing this superior
performance within a strict 12-hour time constraint-half the 24-hour limit used
by previous baselines. These results demonstrate ML-Master's potential as a
powerful tool for advancing AI4AI.

</details>


### [25] [Advancing Harmful Content Detection in Organizational Research: Integrating Large Language Models with Elo Rating System](https://arxiv.org/abs/2506.16575)
*Mustafa Akben,Aaron Satko*

Main category: cs.AI

TL;DR: 本文介绍基于Elo评分的方法，提升大语言模型分析有害内容的性能，在两个数据集上表现优于传统方法，支持组织应用。


<details>
  <summary>Details</summary>
Motivation: 大语言模型内置审核系统在分析有害内容时会产生问题，影响结果有效性，尤其在分析组织冲突相关内容时。

Method: 引入基于Elo评分的方法。

Result: 在微攻击检测和仇恨言论两个数据集上，该方法在准确率、精确率和F1分数等关键指标上优于传统大语言模型提示技术和传统机器学习模型。

Conclusion: 该方法在分析有害内容时可靠性更高、误报更少、对大规模数据集可扩展性更强，支持检测职场骚扰等组织应用。

Abstract: Large language models (LLMs) offer promising opportunities for organizational
research. However, their built-in moderation systems can create problems when
researchers try to analyze harmful content, often refusing to follow certain
instructions or producing overly cautious responses that undermine validity of
the results. This is particularly problematic when analyzing organizational
conflicts such as microaggressions or hate speech. This paper introduces an Elo
rating-based method that significantly improves LLM performance for harmful
content analysis In two datasets, one focused on microaggression detection and
the other on hate speech, we find that our method outperforms traditional LLM
prompting techniques and conventional machine learning models on key measures
such as accuracy, precision, and F1 scores. Advantages include better
reliability when analyzing harmful content, fewer false positives, and greater
scalability for large-scale datasets. This approach supports organizational
applications, including detecting workplace harassment, assessing toxic
communication, and fostering safer and more inclusive work environments.

</details>


### [26] [A Community-driven vision for a new Knowledge Resource for AI](https://arxiv.org/abs/2506.16596)
*Vinay K Chaudhri,Chaitan Baru,Brandon Bennett,Mehul Bhatt,Darion Cassel,Anthony G Cohn,Rina Dechter,Esra Erdem,Dave Ferrucci,Ken Forbus,Gregory Gelfond,Michael Genesereth,Andrew S. Gordon,Benjamin Grosof,Gopal Gupta,Jim Hendler,Sharat Israni,Tyler R. Josephson,Patrick Kyllonen,Yuliya Lierler,Vladimir Lifschitz,Clifton McFate,Hande K. McGinty,Leora Morgenstern,Alessandro Oltramari,Praveen Paritosh,Dan Roth,Blake Shepard,Cogan Shimzu,Denny Vrandečić,Mark Whiting,Michael Witbrock*

Main category: cs.AI

TL;DR: AI缺乏通用知识资源，AAAI研讨会探讨相关问题，本文总结成果并提出社区驱动的新知识基础设施愿景及构建开放工程框架的想法。


<details>
  <summary>Details</summary>
Motivation: 当前AI基础设施缺乏可验证、通用且广泛可用的知识资源，大语言模型、机器人规划等面临知识问题，需探索最需要的知识资源及发展评估方式。

Method: 综合AAAI研讨会中50多位研究者的探索结果。

Result: 提出社区驱动的新知识基础设施愿景，以及构建可有效利用知识模块的开放工程框架。

Conclusion: 通过社区驱动和构建开放工程框架，有望解决AI知识资源不足的问题。

Abstract: The long-standing goal of creating a comprehensive, multi-purpose knowledge
resource, reminiscent of the 1984 Cyc project, still persists in AI. Despite
the success of knowledge resources like WordNet, ConceptNet, Wolfram|Alpha and
other commercial knowledge graphs, verifiable, general-purpose widely available
sources of knowledge remain a critical deficiency in AI infrastructure. Large
language models struggle due to knowledge gaps; robotic planning lacks
necessary world knowledge; and the detection of factually false information
relies heavily on human expertise. What kind of knowledge resource is most
needed in AI today? How can modern technology shape its development and
evaluation? A recent AAAI workshop gathered over 50 researchers to explore
these questions. This paper synthesizes our findings and outlines a
community-driven vision for a new knowledge infrastructure. In addition to
leveraging contemporary advances in knowledge representation and reasoning, one
promising idea is to build an open engineering framework to exploit knowledge
modules effectively within the context of practical applications. Such a
framework should include sets of conventions and social structures that are
adopted by contributors.

</details>


### [27] [The Role of Explanation Styles and Perceived Accuracy on Decision Making in Predictive Process Monitoring](https://arxiv.org/abs/2506.16617)
*Soobin Chae,Suhwan Lee,Hanna Hauptmann,Hajo A. Reijers,Xixi Lu*

Main category: cs.AI

TL;DR: 研究解释风格和感知AI准确性对PPM决策的影响，发现二者有显著作用。


<details>
  <summary>Details</summary>
Motivation: PPM深度学习模型缺乏可解释性，当前XAI评估忽略以用户为中心的方面，需研究对决策的影响。

Method: 进行决策实验，向用户展示AI预测、感知准确性水平和不同风格解释，测量用户接受解释前后的决策。

Result: 感知准确性和解释风格有显著影响。

Conclusion: 感知准确性和解释风格对PPM决策有显著作用。

Abstract: Predictive Process Monitoring (PPM) often uses deep learning models to
predict the future behavior of ongoing processes, such as predicting process
outcomes. While these models achieve high accuracy, their lack of
interpretability undermines user trust and adoption. Explainable AI (XAI) aims
to address this challenge by providing the reasoning behind the predictions.
However, current evaluations of XAI in PPM focus primarily on functional
metrics (such as fidelity), overlooking user-centered aspects such as their
effect on task performance and decision-making. This study investigates the
effects of explanation styles (feature importance, rule-based, and
counterfactual) and perceived AI accuracy (low or high) on decision-making in
PPM. We conducted a decision-making experiment, where users were presented with
the AI predictions, perceived accuracy levels, and explanations of different
styles. Users' decisions were measured both before and after receiving
explanations, allowing the assessment of objective metrics (Task Performance
and Agreement) and subjective metrics (Decision Confidence). Our findings show
that perceived accuracy and explanation style have a significant effect.

</details>


### [28] [Interpretable Low-Dimensional Modeling of Spatiotemporal Agent States for Decision Making in Football Tactics](https://arxiv.org/abs/2506.16696)
*Kenjiro Ide,Taiga Someya,Kohei Kawaguchi,Keisuke Fujii*

Main category: cs.AI

TL;DR: 本文探索低维规则模型能否有效捕捉足球战术，定义状态变量，用数据训练XGBoost模型，发现关键因素，该模型利于战术分析和决策支持。


<details>
  <summary>Details</summary>
Motivation: 以往基于空间和运动方程的模型计算成本高，强化学习方法缺乏可解释性且需大量数据，基于规则的模型未全面考虑球员状态，因此探索低维规则模型能否有效捕捉足球战术。

Method: 定义球持有者和潜在传球接收者的可解释状态变量，与教练讨论确定关键变量，用2023/24赛季西甲数据训练XGBoost模型预测传球成功率。

Result: 分析发现球员与球的距离和球员的空间得分是决定传球成功的关键因素。

Conclusion: 可解释的低维建模通过直观变量促进战术分析，作为支持足球决策的工具具有实用价值。

Abstract: Understanding football tactics is crucial for managers and analysts. Previous
research has proposed models based on spatial and kinematic equations, but
these are computationally expensive. Also, Reinforcement learning approaches
use player positions and velocities but lack interpretability and require large
datasets. Rule-based models align with expert knowledge but have not fully
considered all players' states. This study explores whether low-dimensional,
rule-based models using spatiotemporal data can effectively capture football
tactics. Our approach defines interpretable state variables for both the
ball-holder and potential pass receivers, based on criteria that explore
options like passing. Through discussions with a manager, we identified key
variables representing the game state. We then used StatsBomb event data and
SkillCorner tracking data from the 2023$/$24 LaLiga season to train an XGBoost
model to predict pass success. The analysis revealed that the distance between
the player and the ball, as well as the player's space score, were key factors
in determining successful passes. Our interpretable low-dimensional modeling
facilitates tactical analysis through the use of intuitive variables and
provides practical value as a tool to support decision-making in football.

</details>


### [29] [Reinforcement learning for hybrid charging stations planning and operation considering fixed and mobile chargers](https://arxiv.org/abs/2506.16764)
*Yanchen Zhu,Honghui Zou,Chufan Liu,Yuyu Luo,Yuankai Wu,Yuxuan Liang*

Main category: cs.AI

TL;DR: 论文研究城市路网中混合充电基础设施的规划与运营，提出HCSPO问题，用含启发式调度的深度强化学习方法求解，案例显示其优于现有方案。


<details>
  <summary>Details</summary>
Motivation: 车辆电气化成功依赖高效灵活的充电基础设施，传统固定充电站有利用率低或拥堵问题，移动充电器是灵活方案，因此要解决混合充电基础设施的最优规划与运营问题。

Method: 引入HCSPO问题，结合基于模型预测控制的充电需求预测模型，提出含启发式调度技术的深度强化学习方法。

Result: 通过大量真实城市场景案例研究，表明该方法相比现有解决方案和基线，显著提高了充电基础设施的可用性，减少了用户不便。

Conclusion: 所提出的方法能有效解决混合充电基础设施的规划与运营问题，提升充电服务质量。

Abstract: The success of vehicle electrification, which brings significant societal and
environmental benefits, is contingent upon the availability of efficient and
adaptable charging infrastructure. Traditional fixed-location charging stations
often face issues like underutilization or congestion due to the dynamic nature
of charging demand. Mobile chargers have emerged as a flexible solution,
capable of relocating to align with these demand fluctuations. This paper
addresses the optimal planning and operation of hybrid charging
infrastructures, integrating both fixed and mobile chargers within urban road
networks. We introduce the Hybrid Charging Station Planning and Operation
(HCSPO) problem, which simultaneously optimizes the location and configuration
of fixed charging stations and schedules mobile chargers for dynamic
operations. Our approach incorporates a charging demand prediction model
grounded in Model Predictive Control (MPC) to enhance decision-making. To solve
the HCSPO problem, we propose a deep reinforcement learning method, augmented
with heuristic scheduling techniques, to effectively bridge the planning of
fixed chargers with the real-time operation of mobile chargers. Extensive case
studies using real-world urban scenarios demonstrate that our method
significantly improves the availability of charging infrastructure and reduces
user inconvenience compared to existing solutions and baselines.

</details>


### [30] [AI's Blind Spots: Geographic Knowledge and Diversity Deficit in Generated Urban Scenario](https://arxiv.org/abs/2506.16898)
*Ciro Beneduce,Massimiliano Luca,Bruno Lepri*

Main category: cs.AI

TL;DR: 本文用图像生成模型生成美国各州及首府图像，发现模型虽学到部分地理知识，但存在代表性偏差和实体消歧问题。


<details>
  <summary>Details</summary>
Motivation: 现有文献对图像生成模型的地理知识和嵌入偏差研究有限，需深入探究。

Method: 使用FLUX 1和Stable Diffusion 3.5生成美国各州及首府的150张合成图像，用DINO - v2 ViT - S/14嵌入图像，用Fréchet Inception Distances测量图像相似度。

Result: 模型隐式学到美国地理的某些方面，但用“美国”提示时对大都市地区有代表性偏差，且对欧洲风格名字有实体消歧问题。

Conclusion: 图像生成模型在地理知识表示上存在偏差和问题。

Abstract: Image generation models are revolutionizing many domains, and urban analysis
and design is no exception. While such models are widely adopted, there is a
limited literature exploring their geographic knowledge, along with the biases
they embed. In this work, we generated 150 synthetic images for each state in
the USA and related capitals using FLUX 1 and Stable Diffusion 3.5, two
state-of-the-art models for image generation. We embed each image using DINO-v2
ViT-S/14 and the Fr\'echet Inception Distances to measure the similarity
between the generated images. We found that while these models have implicitly
learned aspects of USA geography, if we prompt the models to generate an image
for "United States" instead of specific cities or states, the models exhibit a
strong representative bias toward metropolis-like areas, excluding rural states
and smaller cities. {\color{black} In addition, we found that models
systematically exhibit some entity-disambiguation issues with European-sounding
names like Frankfort or Devon.

</details>


### [31] [Real-Time Black-Box Optimization for Dynamic Discrete Environments Using Embedded Ising Machines](https://arxiv.org/abs/2506.16924)
*Tomoya Kashimata,Yohei Hamakawa,Masaya Yamasaki,Kosuke Tatsumura*

Main category: cs.AI

TL;DR: 本文提出一种适用于动态离散环境的启发式多臂老虎机（MAB）方法，通过扩展黑盒优化（BBO）方法实现，并在无线通信系统中验证其动态适应性。


<details>
  <summary>Details</summary>
Motivation: 实时系统需优化离散变量，传统MAB算法无法有效优化动态离散环境。

Method: 扩展BBO方法，利用Ising机在考虑变量间相互作用和环境变化的情况下探索行动。

Result: 在有移动用户的无线通信系统中证明了所提方法的动态适应性。

Conclusion: 所提启发式MAB方法能有效优化动态离散环境。

Abstract: Many real-time systems require the optimization of discrete variables.
Black-box optimization (BBO) algorithms and multi-armed bandit (MAB) algorithms
perform optimization by repeatedly taking actions and observing the
corresponding instant rewards without any prior knowledge. Recently, a BBO
method using an Ising machine has been proposed to find the best action that is
represented by a combination of discrete values and maximizes the instant
reward in static environments. In contrast, dynamic environments, where
real-time systems operate, necessitate MAB algorithms that maximize the average
reward over multiple trials. However, due to the enormous number of actions
resulting from the combinatorial nature of discrete optimization, conventional
MAB algorithms cannot effectively optimize dynamic, discrete environments.
Here, we show a heuristic MAB method for dynamic, discrete environments by
extending the BBO method, in which an Ising machine effectively explores the
actions while considering interactions between variables and changes in dynamic
environments. We demonstrate the dynamic adaptability of the proposed method in
a wireless communication system with moving users.

</details>


### [32] [Multimodal Fused Learning for Solving the Generalized Traveling Salesman Problem in Robotic Task Planning](https://arxiv.org/abs/2506.16931)
*Jiaqi Chen,Mingfeng Fan,Xuefeng Zhang,Jingsong Liang,Yuhong Cao,Guohua Wu,Guillaume Adrien Sartoretti*

Main category: cs.AI

TL;DR: 提出多模态融合学习框架解决移动机器人任务规划中的GTSP问题，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 移动机器人任务规划中的GTSP问题难以高效准确解决，需要有效方法。

Method: 提出MMFL框架，利用坐标图像构建器、自适应分辨率缩放策略和多模态融合模块。

Result: 在各种GTSP实例中显著优于现有方法，保持实时计算效率，物理机器人测试验证了实际有效性。

Conclusion: MMFL框架能有效解决移动机器人任务规划的GTSP问题，适用于实时机器人应用。

Abstract: Effective and efficient task planning is essential for mobile robots,
especially in applications like warehouse retrieval and environmental
monitoring. These tasks often involve selecting one location from each of
several target clusters, forming a Generalized Traveling Salesman Problem
(GTSP) that remains challenging to solve both accurately and efficiently. To
address this, we propose a Multimodal Fused Learning (MMFL) framework that
leverages both graph and image-based representations to capture complementary
aspects of the problem, and learns a policy capable of generating high-quality
task planning schemes in real time. Specifically, we first introduce a
coordinate-based image builder that transforms GTSP instances into spatially
informative representations. We then design an adaptive resolution scaling
strategy to enhance adaptability across different problem scales, and develop a
multimodal fusion module with dedicated bottlenecks that enables effective
integration of geometric and spatial features. Extensive experiments show that
our MMFL approach significantly outperforms state-of-the-art methods across
various GTSP instances while maintaining the computational efficiency required
for real-time robotic applications. Physical robot tests further validate its
practical effectiveness in real-world scenarios.

</details>


### [33] [Elevating Styled Mahjong Agents with Learning from Demonstration](https://arxiv.org/abs/2506.16995)
*Lingfeng Li,Yunlong Lu,Yongyi Wang,Wenxin Li*

Main category: cs.AI

TL;DR: 本文以麻将游戏为案例，提出新的LfD算法，能提升智能体能力并保留独特玩法风格。


<details>
  <summary>Details</summary>
Motivation: 当前游戏人工智能多关注提升机器人能力，开发有多种独特玩法风格的高能力机器人是未充分探索的领域，且现有算法在麻将游戏中表现不佳。

Method: 利用现有麻将智能体的游戏历史，对近端策略优化算法进行最小修改，提出新的LfD算法。

Result: 综合实证结果表明，所提方法显著提升了智能体的能力，且有效保留了其独特玩法风格。

Conclusion: 所提出的新算法在提升麻将智能体能力和保留玩法风格方面有良好效果。

Abstract: A wide variety of bots in games enriches the gameplay experience and enhances
replayability. Recent advancements in game artificial intelligence have
predominantly focused on improving the proficiency of bots. Nevertheless,
developing highly competent bots with a wide range of distinct play styles
remains a relatively under-explored area. We select the Mahjong game
environment as a case study. The high degree of randomness inherent in the
Mahjong game and the prevalence of out-of-distribution states lead to
suboptimal performance of existing offline learning and
Learning-from-Demonstration (LfD) algorithms. In this paper, we leverage the
gameplay histories of existing Mahjong agents and put forward a novel LfD
algorithm that necessitates only minimal modifications to the Proximal Policy
Optimization algorithm. The comprehensive empirical results illustrate that our
proposed method not only significantly enhances the proficiency of the agents
but also effectively preserves their unique play styles.

</details>


### [34] [A Quantile Regression Approach for Remaining Useful Life Estimation with State Space Models](https://arxiv.org/abs/2506.17018)
*Davide Frizzo,Francesco Borsatti,Gian Antonio Susto*

Main category: cs.AI

TL;DR: 本文提出利用状态空间模型（SSM）的新型剩余使用寿命（RUL）估计方法，集成同时分位数回归（SQR）处理不确定性，经C - MAPSS数据集测试，该方法在准确性和计算效率上表现出色。


<details>
  <summary>Details</summary>
Motivation: 预测性维护（PdM）对工业4.0和5.0至关重要，准确预测设备剩余使用寿命可提升效率，本文旨在提出有效方法进行RUL估计。

Method: 引入利用状态空间模型（SSM）进行长期序列建模的RUL估计方法，将同时分位数回归（SQR）集成到SSM中处理模型不确定性，用C - MAPSS数据集与传统序列建模技术（LSTM、Transformer、Informer）进行对比测试。

Result: SSM模型在准确性和计算效率上优于传统序列建模技术。

Conclusion: SSM模型在高风险工业应用中具有潜力。

Abstract: Predictive Maintenance (PdM) is pivotal in Industry 4.0 and 5.0, proactively
enhancing efficiency through accurate equipment Remaining Useful Life (RUL)
prediction, thus optimizing maintenance scheduling and reducing unexpected
failures and premature interventions. This paper introduces a novel RUL
estimation approach leveraging State Space Models (SSM) for efficient long-term
sequence modeling. To handle model uncertainty, Simoultaneous Quantile
Regression (SQR) is integrated into the SSM, enabling multiple quantile
estimations. The proposed method is benchmarked against traditional sequence
modelling techniques (LSTM, Transformer, Informer) using the C-MAPSS dataset.
Results demonstrate superior accuracy and computational efficiency of SSM
models, underscoring their potential for high-stakes industrial applications.

</details>


### [35] [Dispositions and Roles of Generically Dependent Entities](https://arxiv.org/abs/2506.17085)
*Fabian Neuhaus*

Main category: cs.AI

TL;DR: BFO 2020不支持泛型依赖持续体的功能、倾向和角色，本文讨论问题并提出两种解决方法。


<details>
  <summary>Details</summary>
Motivation: 指出BFO 2020不支持泛型依赖持续体的功能、倾向和角色是严重局限，影响计算机模型功能和数据集角色表示。

Method: 讨论BFO 2020中阻碍泛型依赖持续体可实现实体表示的方面，提出使用定义类和对BFO进行更改两种方法。

Result: 提出两种解决BFO 2020局限性的途径。

Conclusion: 通过这两种途径有望解决BFO 2020在泛型依赖持续体相关表示上的问题。

Abstract: BFO 2020 does not support functions, dispositions, and roles of generically
dependent continuants (like software or datasets). In this paper, we argue that
this is a severe limitation, which prevents, for example, the adequate
representation of the functions of computer models or the various roles of
datasets during the execution of these models. We discuss the aspects of BFO
2020 that prevent the representation of realizable entities of generically
dependent continuants. Two approaches to address the issue are presented: (a)
the use of defined classes and (b) a proposal of changes that allow BFO to
support functions, dispositions, and roles of generically dependent
continuants.

</details>


### [36] [Towards Advanced Mathematical Reasoning for LLMs via First-Order Logic Theorem Proving](https://arxiv.org/abs/2506.17104)
*Chuxue Cao,Mengze Li,Juntao Dai,Jinluan Yang,Zijian Zhao,Shengyu Zhang,Weijie Shi,Chengzhong Liu,Sirui Han,Yike Guo*

Main category: cs.AI

TL;DR: 现有大语言模型在复杂数学推理的多步一阶逻辑（FOL）演绎中效果不佳，提出DREAM方法提升生成策略的多样性和合理性，取得一定性能提升并提供评估数据集。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在复杂数学推理的多步FOL演绎效果未充分研究，现有模型在多步FOL任务中表现差，存在探索策略单一和易早期出错的问题。

Method: 提出DREAM方法，包含公理驱动的策略多样化机制和子命题错误反馈机制。

Result: 新的推理阶段解决方案使性能提升0.6%到6.4%。

Conclusion: 在大语言模型的FOL定理证明数学推理方面有开创性进展，DREAM方法有一定成效。

Abstract: Large language models (LLMs) have shown promising first-order logic (FOL)
reasoning capabilities with applications in various areas. However, their
effectiveness in complex mathematical reasoning involving multi-step FOL
deductions is still under-researched. While LLMs perform competitively on
established mathematical reasoning benchmarks, they struggle with multi-step
FOL tasks, as demonstrated by Deepseek-Prover-V2-7B's low accuracy (4.2%) on
our proposed theorem proving dataset. This issue arises from the limited
exploration of diverse proof strategies and the potential for early reasoning
mistakes to undermine entire proofs. To address these issues, we propose DREAM,
a self-adaptive solution that enhances the Diversity and REAsonability of LLMs'
generation strategies. DREAM incorporates an Axiom-Driven Strategy
Diversification mechanism to promote varied strategic outcomes and a
Sub-Proposition Error Feedback to help LLMs reflect on and correct their
proofs. Our contributions include pioneering advancements in LLMs' mathematical
reasoning through FOL theorem proving, introducing a novel inference stage
solution that improves performance by 0.6% to 6.4%, and providing a curated
dataset of 447 mathematical theorems in Lean 4 format for evaluation.

</details>


### [37] [Are Bias Evaluation Methods Biased ?](https://arxiv.org/abs/2506.17111)
*Lina Berrayana,Sean Rooney,Luis Garcés-Erice,Ioana Giurgiu*

Main category: cs.AI

TL;DR: 研究独立基准对大语言模型偏差评估排名的稳健性，发现不同评估方法排名差异大并给出使用建议。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型安全性的独立基准采用不同方法，需研究其稳健性。

Method: 用不同方法对一组代表性模型的偏差进行排名，并比较整体排名的相似性。

Result: 不同但广泛使用的偏差评估方法导致模型排名不同。

Conclusion: 为社区使用此类基准提出建议。

Abstract: The creation of benchmarks to evaluate the safety of Large Language Models is
one of the key activities within the trusted AI community. These benchmarks
allow models to be compared for different aspects of safety such as toxicity,
bias, harmful behavior etc. Independent benchmarks adopt different approaches
with distinct data sets and evaluation methods. We investigate how robust such
benchmarks are by using different approaches to rank a set of representative
models for bias and compare how similar are the overall rankings. We show that
different but widely used bias evaluations methods result in disparate model
rankings. We conclude with recommendations for the community in the usage of
such benchmarks.

</details>


### [38] [Mathematical Proof as a Litmus Test: Revealing Failure Modes of Advanced Large Reasoning Models](https://arxiv.org/abs/2506.17114)
*Dadi Guo,Jiayu Liu,Zhiyuan Fan,Zhitao He,Haoran Li,Yumeng Wang,Yi R.,Fung*

Main category: cs.AI

TL;DR: 现有大型推理模型在数学解题能力评估中可能存在掩盖推理缺陷的问题，本文提出用数学证明诊断，引入RFMDataset评估，揭示10种细粒度错误类型及模型局限，认为需形式化和细粒度逻辑训练。


<details>
  <summary>Details</summary>
Motivation: 流行数据集上高级模型的高准确率、纯数值评估和基准泄漏等问题掩盖了大型推理模型的真实推理缺陷，需有效方法暴露这些问题。

Method: 引入包含200个不同数学证明问题的RFMDataset，对高级模型在该数据集上的表现进行深入评估。

Result: 分析出10种细粒度错误类型，发现模型在处理数学证明、单步推理正确性和严谨性、推理过程等方面存在根本局限。

Conclusion: 模型的自我反思不足以解决当前逻辑困境，需要进行形式化和细粒度的逻辑训练。

Abstract: Large reasoning models (e.g., R1, o3) have demonstrated remarkable
mathematical problem-solving abilities. However, the high reported accuracy of
these advanced models on popular datasets, reliance on purely numerical
evaluation and potential benchmark leakage, often masks their true reasoning
shortcomings. To address this, we propose leveraging the inherent rigor and
methodological complexity of mathematical proofs as a diagnostic tool to expose
these hidden failures. Specifically, we introduce the RFMDataset (Reveal
Failure Modes), a collection of 200 diverse mathematical proof problems, and
thoroughly evaluate advanced models' performance on it. Our in-depth analysis
of their failures uncovers 10 fine-grained error types, which shows fundamental
limitations in current large reasoning models: 1) large reasoning models
grapple profoundly with mathematical proofs, with some generating entirely
correct proofs for less than 20% of problems and failing even on basic ones; 2)
models exhibit a diverse spectrum of reasoning failures, prominently
demonstrating the lack of guarantees for the correctness and rigor of
single-step reasoning; and 3) models show hallucination and incompleteness
during the reasoning process. Our findings reveal that models' self-reflection
is insufficient to resolve the current logical dilemmas, necessitating
formalized and fine-grained logical training.

</details>


### [39] [When Can Model-Free Reinforcement Learning be Enough for Thinking?](https://arxiv.org/abs/2506.17124)
*Josiah P. Hanna,Nicholas E. Corrado*

Main category: cs.AI

TL;DR: 本文旨在理解无模型强化学习何时能将‘思考’作为奖励最大化策略，引入思想马尔可夫决策过程模型进行理论分析，并在大语言模型上验证，还提出思考学习的充分条件与实验。


<details>
  <summary>Details</summary>
Motivation: 近期大语言模型用无模型强化学习训练推理能力，‘思考’行动无直接奖励但出现了，要构建无模型强化学习何时产生‘思考’策略的领域无关理解。

Method: 引入思想马尔可夫决策过程模型，证明策略初始化对‘思考’出现的重要性，验证开源大语言模型满足理论条件，提出充分条件并进行玩具领域实验。

Result: 证明策略初始化影响‘思考’出现，开源大语言模型满足理论条件，玩具领域中结合多任务预训练和指定思考动作的强化学习更高效。

Conclusion: 明确了无模型强化学习产生‘思考’策略的条件，提出的方法可在语言生成外学习‘思考’，提高强化学习数据效率。

Abstract: Recent work on large language models has demonstrated the use of model-free
reinforcement learning (RL) to train reasoning-like capabilities. The emergence
of "thinking" through model-free RL is interesting as thinking actions neither
produce reward nor change the external world state to one where the agent is
more likely to get reward. This paper seeks to build a domain-independent
understanding of when model-free RL will lead to "thinking" as a strategy for
reward maximization. To build this understanding, we first introduce a
theoretical model which we call a \textit{thought Markov decision process}
(MDP). Thought MDPs minimally extend the classical MDP model to include an
abstract notion of thought state and thought action. Using the thought MDP
model, we prove the importance of policy initialization in determining whether
or not thinking emerges and show formally that thought actions are equivalent
to the agent choosing to perform a step of policy improvement before continuing
to act. We then show that open-source LLMs satisfy the conditions that our
theory predicts are necessary for model-free RL to produce thinking-like
behavior. Finally, we hypothesize sufficient conditions that would enable
thinking to be learned outside of language generation and introduce a toy
domain where a combination of multi-task pre-training and designated thought
actions enable more data-efficient RL compared to non-thinking agents.

</details>


### [40] [Chain-of-Trust: A Progressive Trust Evaluation Framework Enabled by Generative AI](https://arxiv.org/abs/2506.17130)
*Botao Zhu,Xianbin Wang,Lei Zhang,Xuemin,Shen*

Main category: cs.AI

TL;DR: 提出链状信任评估框架处理分布式资源协作系统信任评估，分阶段收集数据，用生成式AI分析，实验证明评估准确性高。


<details>
  <summary>Details</summary>
Motivation: 复杂任务分布式资源协作系统中，因网络动态和信息收集延迟，全面观察收集信任属性困难，需有效信任评估机制。

Method: 提出链状信任评估框架，基于任务分解分多阶段评估，每阶段收集相关最新设备属性数据，用生成式AI分析数据。

Result: 实验结果表明，所提框架在信任评估中达到了较高的准确性。

Conclusion: 链状信任评估框架能有效降低信任评估复杂性和开销，实现准确信任评估。

Abstract: In collaborative systems with complex tasks relying on distributed resources,
trust evaluation of potential collaborators has emerged as an effective
mechanism for task completion. However, due to the network dynamics and varying
information gathering latencies, it is extremely challenging to observe and
collect all trust attributes of a collaborating device concurrently for a
comprehensive trust assessment. In this paper, a novel progressive trust
evaluation framework, namely chain-of-trust, is proposed to make better use of
misaligned device attribute data. This framework, designed for effective task
completion, divides the trust evaluation process into multiple chained stages
based on task decomposition. At each stage, based on the task completion
process, the framework only gathers the latest device attribute data relevant
to that stage, leading to reduced trust evaluation complexity and overhead. By
leveraging advanced in-context learning, few-shot learning, and reasoning
capabilities, generative AI is then employed to analyze and interpret the
collected data to produce correct evaluation results quickly. Only devices
deemed trustworthy at this stage proceed to the next round of trust evaluation.
The framework ultimately determines devices that remain trustworthy across all
stages. Experimental results demonstrate that the proposed framework achieves
high accuracy in trust evaluation.

</details>


### [41] [The MedPerturb Dataset: What Non-Content Perturbations Reveal About Human and Clinical LLM Decision Making](https://arxiv.org/abs/2506.17163)
*Abinitha Gourabathina,Yuexing Hao,Walter Gerych,Marzyeh Ghassemi*

Main category: cs.AI

TL;DR: 引入MedPerturb数据集评估医疗大语言模型在临床输入扰动下的表现，发现模型和人类对不同扰动的敏感性不同，强调需超越静态基准的评估框架。


<details>
  <summary>Details</summary>
Motivation: 解决医疗大语言模型临床鲁棒性问题，研究模型和人类在临床环境真实变异性下的响应差异。

Method: 创建MedPerturb数据集，包含不同病理的临床案例，从性别、风格、格式三方面进行转换，开展两个案例研究。

Result: 大语言模型对性别和风格扰动更敏感，人类注释者对大语言模型生成的格式扰动更敏感。

Conclusion: 需要超越静态基准的评估框架来评估临床环境变异性下人类临床医生和大语言模型决策的相似性。

Abstract: Clinical robustness is critical to the safe deployment of medical Large
Language Models (LLMs), but key questions remain about how LLMs and humans may
differ in response to the real-world variability typified by clinical settings.
To address this, we introduce MedPerturb, a dataset designed to systematically
evaluate medical LLMs under controlled perturbations of clinical input.
MedPerturb consists of clinical vignettes spanning a range of pathologies, each
transformed along three axes: (1) gender modifications (e.g., gender-swapping
or gender-removal); (2) style variation (e.g., uncertain phrasing or colloquial
tone); and (3) format changes (e.g., LLM-generated multi-turn conversations or
summaries). With MedPerturb, we release a dataset of 800 clinical contexts
grounded in realistic input variability, outputs from four LLMs, and three
human expert reads per clinical context. We use MedPerturb in two case studies
to reveal how shifts in gender identity cues, language style, or format reflect
diverging treatment selections between humans and LLMs. We find that LLMs are
more sensitive to gender and style perturbations while human annotators are
more sensitive to LLM-generated format perturbations such as clinical
summaries. Our results highlight the need for evaluation frameworks that go
beyond static benchmarks to assess the similarity between human clinician and
LLM decisions under the variability characteristic of clinical settings.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [42] [A Fast Iterative Robust Principal Component Analysis Method](https://arxiv.org/abs/2506.16013)
*Timbwaoga Aime Judicael Ouermi,Jixian Li,Chris R. Johnson*

Main category: cs.CE

TL;DR: 提出快速迭代鲁棒PCA方法（FIR），利用IPCA减轻离群值对PCA投影的影响，在模拟和真实数据集上验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统PCA受离群值影响，现有鲁棒PCA方法计算昂贵或鲁棒性有限。

Method: 通过有效估计内点中心位置和协方差，利用增量PCA（IPCA）迭代构建数据点子集。

Result: 该方法与现有鲁棒位置和协方差方法相比，有竞争力的准确性和性能，且对离群值污染有更好鲁棒性。

Conclusion: 方法能在数据受污染时识别和保留潜在数据结构。

Abstract: Principal Component Analysis (PCA) is widely used for dimensionality
reduction and data analysis. However, PCA results are adversely affected by
outliers often observed in real-world data. Existing robust PCA methods are
often computationally expensive or exhibit limited robustness. In this work, we
introduce a Fast Iterative Robust (FIR) PCA method by efficiently estimating
the inliers center location and covariance. Our approach leverages Incremental
PCA (IPCA) to iteratively construct a subset of data points that ensures
improved location and covariance estimation that effectively mitigates the
influence of outliers on PCA projection. We demonstrate that our method
achieves competitive accuracy and performance compared to existing robust
location and covariance methods while offering improved robustness to outlier
contamination. We utilize simulated and real-world datasets to evaluate and
demonstrate the efficacy of our approach in identifying and preserving
underlying data structures in the presence of contamination.

</details>


### [43] [Fast Converging Single Trace Quasi-local PMCHWT Equation for the Modelling of Composite Systems](https://arxiv.org/abs/2506.16376)
*Kristof Cools*

Main category: cs.CE

TL;DR: 本文引入单迹准局部PMCHWT方程，讨论其离散化并通过数值实验验证其正确性、收敛性和效率，且该方程无内部共振。


<details>
  <summary>Details</summary>
Motivation: 现有PMCHWT方程离散化后用Krylov迭代法求解时，有交界线的系统无法用Calderón预处理，其他方法有自由度加倍等问题，需新方法。

Method: 将经典PMCHWT方程推广得到单迹准局部PMCHWT方程，并详细讨论其离散化。

Result: 通过数值实验验证了方法的正确性、收敛性和效率，且方程无内部共振。

Conclusion: 新引入的单迹准局部PMCHWT方程有效，求解所需迭代次数随网格尺寸减小增长缓慢。

Abstract: The PMCHWT integral equation enables the modelling of scattering of
time-harmonic fields by penetrable, piecewise homogeneous, systems. They have
been generalised to include the modelling of composite systems that may contain
junctions, i.e. lines along which three or more materials meet. Linear systems
resulting upon discretisation of the PMCHWT are, because of their large
dimension, typically solved by Krylov iterative methods. The number of
iterations required for this solution critically depends on the eigenvalue
distribution of the system matrix. For systems that do not contain junction
lines, Calder\'on preconditioning, which was first applied to the electric
field integral equation, has been generalised to the PMCHWT equation. When
junctions are present, this approach cannot be applied. Alternative approaches,
such as the global multi-trace method, conceptually remove the junction lines
and as a result are amenable to Calder\'on preconditioning. This approach
entails a doubling of the degrees of freedom, and the solution that is produced
only approximately fulfils the continuity conditions at interfaces separating
domains. In this contribution, a single trace quasi-local PMCHWT equation is
introduced that requires a number of iterations for its solution that only
slowly increases as the mesh size tends to zero. The method is constructed as a
generalisation of the classic PMCHWT, and its discretisation is thoroughly
discussed. A comprehensive suite of numerical experiments demonstrates the
correctness, convergence behaviour, and efficiency of the method. The integral
equation is demonstrated to be free from interior resonances.

</details>


### [44] [Aethorix v1.0: AI-Driven Inverse Design of Inorganic Materials for Scalable Industrial Innovation](https://arxiv.org/abs/2506.16609)
*Yingjie Shi,Runtian Miao*

Main category: cs.CE

TL;DR: 介绍Aethorix v1.0平台用于AI4S助力工业制造，经实际用例验证其价值。


<details>
  <summary>Details</summary>
Motivation: 利用AI4S变革工业制造，加速先进材料发现与优化，缩短开发周期。

Method: 集成大语言模型进行目标挖掘、基于扩散的生成模型进行零样本无机晶体设计、机器学习原子间势进行高精度性能快速预测，且考虑操作约束。

Result: 通过实际用例验证平台可无缝嵌入可扩展材料研发流程。

Conclusion: Aethorix v1.0平台能增强材料全开发周期，具有工业应用价值。

Abstract: Artificial intelligence for Science (AI4S) is poised to transform industrial
manufacturing by enabling the accelerated discovery and optimization of
advanced (bio)materials, dramatically reducing development cycles, and
unlocking novel high-performance solutions. We introduce Aethorix v1.0, a
platform that integrates large language models for objective mining,
diffusion-based generative models for zero-shot inorganic crystal design, and
machine-learned interatomic potentials for rapid property prediction at ab
initio accuracy. The platform is developed to enhance the full materials
development cycle, ranging from design to deployment in use cases, while
incorporating critical operational constraints to meet rigorous manufacturing
standards. We validated its industrial value through a real use case,
showcasing how the framework can be seamlessly embedded into scalable materials
R&D pipelines.

</details>


### [45] [Pre-training Time Series Models with Stock Data Customization](https://arxiv.org/abs/2506.16746)
*Mengyu Wang,Tiejun Ma,Shay B. Cohen*

Main category: cs.CE

TL;DR: 本文提出针对股票数据特征的预训练任务，开发股票专用预训练变压器模型SSPT，实验表明其表现优于市场和现有方法，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有股票选择方法在预训练策略方面研究不足，未适应金融数据特性，未充分利用股票数据潜在统计特征。

Method: 提出股票代码分类、股票板块分类和移动平均预测三个预训练任务，基于两层变压器架构开发SSPT模型。

Result: 在五个股票数据集上，SSPT在累积投资回报率和夏普比率方面始终优于市场和现有方法，对模拟数据的实验有助于理解价格序列。

Conclusion: 所提出的预训练方法有效，为其应用提供了详细指导。

Abstract: Stock selection, which aims to predict stock prices and identify the most
profitable ones, is a crucial task in finance. While existing methods primarily
focus on developing model structures and building graphs for improved
selection, pre-training strategies remain underexplored in this domain. Current
stock series pre-training follows methods from other areas without adapting to
the unique characteristics of financial data, particularly overlooking
stock-specific contextual information and the non-stationary nature of stock
prices. Consequently, the latent statistical features inherent in stock data
are underutilized. In this paper, we propose three novel pre-training tasks
tailored to stock data characteristics: stock code classification, stock sector
classification, and moving average prediction. We develop the Stock Specialized
Pre-trained Transformer (SSPT) based on a two-layer transformer architecture.
Extensive experimental results validate the effectiveness of our pre-training
methods and provide detailed guidance on their application. Evaluations on five
stock datasets, including four markets and two time periods, demonstrate that
SSPT consistently outperforms the market and existing methods in terms of both
cumulative investment return ratio and Sharpe ratio. Additionally, our
experiments on simulated data investigate the underlying mechanisms of our
methods, providing insights into understanding price series. Our code is
publicly available at:
https://github.com/astudentuser/Pre-training-Time-Series-Models-with-Stock-Data-Customization.

</details>


### [46] [Integrating Traditional Technical Analysis with AI: A Multi-Agent LLM-Based Approach to Stock Market Forecasting](https://arxiv.org/abs/2506.16813)
*Michał Wawer,Jarosław A. Chudziak*

Main category: cs.CE

TL;DR: 本文介绍ElliottAgents多智能体系统，结合艾略特波浪理论与AI进行股市预测，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统技术分析方法在复杂金融市场预测趋势存在局限，市场复杂性给准确预测带来挑战。

Method: 系统在多智能体框架中使用大语言模型增强自然语言理解和决策能力，利用检索增强生成（RAG）和深度强化学习（DRL）对市场数据进行多方面分析。

Result: 对美国主要公司历史数据的实验验证了系统在不同时间框架下模式识别和趋势预测的有效性。

Conclusion: 证明传统技术分析方法可与现代AI方法有效结合，创建更可靠、可解释的市场预测系统。

Abstract: Traditional technical analysis methods face limitations in accurately
predicting trends in today's complex financial markets. This paper introduces
ElliottAgents, an multi-agent system that integrates the Elliott Wave Principle
with AI for stock market forecasting. The inherent complexity of financial
markets, characterized by non-linear dynamics, noise, and susceptibility to
unpredictable external factors, poses significant challenges for accurate
prediction. To address these challenges, the system employs LLMs to enhance
natural language understanding and decision-making capabilities within a
multi-agent framework. By leveraging technologies such as Retrieval-Augmented
Generation (RAG) and Deep Reinforcement Learning (DRL), ElliottAgents performs
continuous, multi-faceted analysis of market data to identify wave patterns and
predict future price movements. The research explores the system's ability to
process historical stock data, recognize Elliott wave patterns, and generate
actionable insights for traders. Experimental results, conducted on historical
data from major U.S. companies, validate the system's effectiveness in pattern
recognition and trend forecasting across various time frames. This paper
contributes to the field of AI-driven financial analysis by demonstrating how
traditional technical analysis methods can be effectively combined with modern
AI approaches to create more reliable and interpretable market prediction
systems.

</details>


### [47] [Estimating Deprivation Cost Functions for Power Outages During Disasters: A Discrete Choice Modeling Approach](https://arxiv.org/abs/2506.16993)
*Xiangpeng Li,Mona Ahmadiani,Richard Woodward,Bo Li,Arnold Vedlitz,Ali Mostafavi*

Main category: cs.CE

TL;DR: 本文利用德州哈里斯县的调查数据，开发并实施估算停电剥夺成本函数的方法，比较多种离散选择模型，分析估值异质性，结果表明停电剥夺成本函数是凸且随时间递增的，研究为相关评估和策略制定提供基础。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对停电剥夺成本的系统测量，本文旨在弥补这一研究空白。

Method: 利用德州哈里斯县的陈述偏好调查数据，比较多种离散选择模型架构，包括多项逻辑和混合逻辑规范，以及包含BoxCox和指数效用转换的模型，通过社会人口交互分析估值异质性。

Result: 停电剥夺成本函数是凸的且随时间严格递增，个体对停电的估值存在系统和随机的偏好差异。

Conclusion: 研究为将剥夺成本纳入基础设施风险评估和人道主义物流提供了方法和实证基础，有助于政策制定者量化服务中断成本并制定更公平的恢复策略。

Abstract: Systems for the generation and distribution of electrical power represents
critical infrastructure and, when extreme weather events disrupt such systems,
this imposes substantial costs on consumers. These costs can be conceptualized
as deprivation costs, an increasing function of time without service,
quantifiable through individuals' willingness to pay for power restoration.
Despite widespread recognition of outage impacts, a gap in the research
literature exists regarding the systematic measurement of deprivation costs.
This study addresses this deficiency by developing and implementing a
methodology to estimate deprivation cost functions for electricity outages,
using stated preference survey data collected from Harris County, Texas. This
study compares multiple discrete choice model architectures, including
multinomial logit and mixed logit specifications, as well as models
incorporating BoxCox and exponential utility transformations for the
deprivation time attribute. The analysis examines heterogeneity in deprivation
valuation through sociodemographic interactions, particularly across income
groups. Results confirm that power outage deprivation cost functions are convex
and strictly increasing with time. Additionally, the study reveals both
systematic and random taste variation in how individuals value power loss,
highlighting the need for flexible modeling approaches. By providing both
methodological and empirical foundations for incorporating deprivation costs
into infrastructure risk assessments and humanitarian logistics, this research
enables policymakers to better quantify service disruption costs and develop
more equitable resilience strategies.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [48] [Adaptive Anomaly Detection in the Presence of Concept Drift: Extended Report](https://arxiv.org/abs/2506.15831)
*Jongjun Park,Fei Chiang,Mostafa Milani*

Main category: cs.DB

TL;DR: 提出AnDri系统用于存在漂移时的异常检测，引入AHC聚类方法。


<details>
  <summary>Details</summary>
Motivation: 数据会发生概念漂移和异常变化，区分两者对准确分析很关键，但现有工作多孤立研究异常检测和概念漂移检测。

Method: 开发AnDri系统，该系统能对正常模式进行时间调整，区分异常子序列和新概念，还引入相邻层次聚类（AHC）方法。

Result: 未提及具体结果。

Conclusion: 未提及明确结论。

Abstract: Data changes to reflect evolving user behaviour, preferences, and changes in
the environment. Such changes may occur due to expected shifts in the data
distribution, i.e., concept drift, or unexpected anomalous changes. The
presence of concept drift poses challenges for anomaly detection in time
series. While anomalies are caused by undesirable changes in the data,
differentiating abnormal changes from varying normal behaviours is difficult
due to differing frequencies of occurrence, varying time intervals when normal
patterns occur. Differentiating between concept drift and anomalies is critical
for accurate analysis as studies have shown that the compounding effects of
error propagation in downstream data analysis tasks lead to lower detection
accuracy and increased overhead due to unnecessary model updates.
Unfortunately, existing work has largely explored anomaly detection and concept
drift detection in isolation. We develop AnDri, a system for Anomaly detection
in the presence of Drift, which adjusts the normal patterns temporally, and
distinguish abnormal subsequences and new concepts. Moreover, it introduces a
new clustering method, Adjacent Hierarchical Clustering (AHC), which groups
similar subsequences while respecting their temporal locality.

</details>


### [49] [Delta: A Learned Mixed Cost-based Query Optimization Framework](https://arxiv.org/abs/2506.15848)
*Jiazhen Peng,Zheng Qu,Xiaoye Miao,Rong Zhu*

Main category: cs.DB

TL;DR: 本文提出混合成本查询优化框架Delta，能确定高效查询计划，实验显示其表现优于PostgreSQL和现有学习方法。


<details>
  <summary>Details</summary>
Motivation: 现有查询优化器存在搜索空间爆炸、训练成本高、精度低和缺乏性能不佳查询检测机制等问题，需确定更高效计划。

Method: Delta包含兼容查询检测器和两阶段规划器，先用检测器过滤，再对兼容查询通过两阶段规划器确定最佳计划，还复用并扩充数据以降低训练成本。

Result: 在三个工作负载上实验表明，Delta能识别更高质量计划，比PostgreSQL平均加速2.34倍，比现有学习方法性能高2.21倍。

Conclusion: Delta能有效确定高效查询计划，性能优于现有方法。

Abstract: Query optimizer is a crucial module for database management systems. Existing
optimizers exhibit two flawed paradigms: (1) cost-based optimizers use dynamic
programming with cost models but face search space explosion and heuristic
pruning constraints; (2) value-based ones train value networks to enable
efficient beam search, but incur higher training costs and lower accuracy. They
also lack mechanisms to detect queries where they may perform poorly. To
determine more efficient plans, we propose Delta, a mixed cost-based query
optimization framework that consists of a compatible query detector and a
two-stage planner. Delta first employs a Mahalanobis distancebased detector to
preemptively filter out incompatible queries where the planner might perform
poorly. For compatible queries, Delta activates its two-stage mixed cost-based
planner. Stage I serves as a coarse-grained filter to generate high-quality
candidate plans based on the value network via beam search, relaxing precision
requirements and narrowing the search space. Stage II employs a fine-grained
ranker to determine the best plan from the candidate plans based on a learned
cost model. Moreover, to reduce training costs, we reuse and augment the
training data from stage I to train the model in stage II. Experimental results
on three workloads demonstrate that Delta identifies higher-quality plans,
achieving an average 2.34x speedup over PostgreSQL and outperforming the
state-of-the-art learned methods by 2.21x.

</details>


### [50] [Empowering Graph-based Approximate Nearest Neighbor Search with Adaptive Awareness Capabilities](https://arxiv.org/abs/2506.15986)
*Jiancheng Ruan,Tingyang Chen,Renchi Yang,Xiangyu Ke,Yunjun Gao*

Main category: cs.DB

TL;DR: 本文提出GATE模块加速基于图的高维近似最近邻搜索（ANNS），实验表明相比现有方法有1.2 - 2.0倍的查询速度提升。


<details>
  <summary>Details</summary>
Motivation: 现有基于图的ANNS方法存在陷入局部最优和冗余计算的问题，原因是未充分利用图拓扑信息和存在数据与查询分布不匹配。

Method: 提出GATE模块，先提取枢纽节点作为候选入口点，用对比学习双塔模型编码图结构语义和查询相关特征，构建导航图索引以减少推理开销。

Result: GATE相比现有基于图的索引在查询性能上有1.2 - 2.0倍的加速。

Conclusion: GATE能有效加速基于图的高维ANNS。

Abstract: Approximate Nearest Neighbor Search (ANNS) in high-dimensional spaces finds
extensive applications in databases, information retrieval, recommender
systems, etc. While graph-based methods have emerged as the leading solution
for ANNS due to their superior query performance, they still face several
challenges, such as struggling with local optima and redundant computations.
These issues arise because existing methods (i) fail to fully exploit the
topological information underlying the proximity graph G, and (ii) suffer from
severe distribution mismatches between the base data and queries in practice.
  To this end, this paper proposes GATE, high-tier proximity Graph with
Adaptive Topology and Query AwarEness, as a lightweight and adaptive module
atop the graph-based indexes to accelerate ANNS. Specifically, GATE formulates
the critical problem to identify an optimal entry point in the proximity graph
for a given query, facilitating faster online search. By leveraging the
inherent clusterability of high-dimensional data, GATE first extracts a small
set of hub nodes V as candidate entry points. Then, resorting to a contrastive
learning-based two-tower model, GATE encodes both the structural semantics
underlying G and the query-relevant features into the latent representations of
these hub nodes V. A navigation graph index on V is further constructed to
minimize the model inference overhead. Extensive experiments demonstrate that
GATE achieves a 1.2-2.0X speed-up in query performance compared to
state-of-the-art graph-based indexes.

</details>


### [51] [Filter-Centric Vector Indexing: Geometric Transformation for Efficient Filtered Vector Search](https://arxiv.org/abs/2506.15987)
*Alireza Heidari,Wei Zhang*

Main category: cs.DB

TL;DR: 介绍了一种名为FCVI的新框架，可结合向量相似性和属性过滤，性能、兼容性和稳定性佳，适用于生产向量搜索系统。


<details>
  <summary>Details</summary>
Motivation: 向量搜索应用的快速增长要求高效处理向量相似性和属性过滤，当前方法在性能和准确性之间难以平衡。

Method: 提出Filter - Centric Vector Indexing (FCVI)框架，通过数学原理变换将过滤条件直接编码到向量空间，可与现有向量索引配合使用。

Result: FCVI吞吐量比现有方法高2.6 - 3.0倍，召回率相当，在分布变化时表现稳定。

Conclusion: FCVI的性能、兼容性和恢复能力使其成为需要灵活过滤功能的生产向量搜索系统的适用解决方案。

Abstract: The explosive growth of vector search applications demands efficient handling
of combined vector similarity and attribute filtering; a challenge where
current approaches force an unsatisfying choice between performance and
accuracy. We introduce Filter-Centric Vector Indexing (FCVI), a novel framework
that transforms this fundamental trade-off by directly encoding filter
conditions into the vector space through a mathematically principled
transformation $\psi(v, f, \alpha)$. Unlike specialized solutions, FCVI works
with any existing vector index (HNSW, FAISS, ANNOY) while providing theoretical
guarantees on accuracy. Our comprehensive evaluation demonstrates that FCVI
achieves 2.6-3.0 times higher throughput than state-of-the-art methods while
maintaining comparable recall. More remarkably, FCVI exhibits exceptional
stability under distribution shifts; maintaining consistent performance when
filter patterns or vector distributions change, unlike traditional approaches
that degrade significantly. This combination of performance, compatibility, and
resilience positions FCVI as an immediately applicable solution for production
vector search systems requiring flexible filtering capabilities.

</details>


### [52] [Data-Agnostic Cardinality Learning from Imperfect Workloads](https://arxiv.org/abs/2506.16007)
*Peizhi Wu,Rong Kang,Tieying Zhang,Jianjun Chen,Ryan Marcus,Zachary G. Ives*

Main category: cs.DB

TL;DR: 提出数据无关的基数学习系统GRASP，在不完美工作负载上优于现有查询驱动模型，在复杂基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统基数估计受组织政策限制数据访问，现有查询驱动模型假设在现实中难成立，需新系统应对。

Method: GRASP采用组合设计，有新的单表基数估计模型和学习计数草图模型。

Result: 在三个数据库实例中，GRASP在不完美工作负载上估计准确性和查询延迟表现更佳，在复杂基准测试中表现与传统方法相当甚至更优。

Conclusion: GRASP能在现实约束下有效进行基数估计，性能良好。

Abstract: Cardinality estimation (CardEst) is a critical aspect of query optimization.
Traditionally, it leverages statistics built directly over the data. However,
organizational policies (e.g., regulatory compliance) may restrict global data
access. Fortunately, query-driven cardinality estimation can learn CardEst
models using query workloads. However, existing query-driven models often
require access to data or summaries for best performance, and they assume
perfect training workloads with complete and balanced join templates (or join
graphs). Such assumptions rarely hold in real-world scenarios, in which join
templates are incomplete and imbalanced. We present GRASP, a data-agnostic
cardinality learning system designed to work under these real-world
constraints. GRASP's compositional design generalizes to unseen join templates
and is robust to join template imbalance. It also introduces a new per-table
CardEst model that handles value distribution shifts for range predicates, and
a novel learned count sketch model that captures join correlations across base
relations. Across three database instances, we demonstrate that GRASP
consistently outperforms existing query-driven models on imperfect workloads,
both in terms of estimation accuracy and query latency. Remarkably, GRASP
achieves performance comparable to, or even surpassing, traditional approaches
built over the underlying data on the complex CEB-IMDb-full benchmark --
despite operating without any data access and using only 10% of all possible
join templates.

</details>


### [53] [PBench: Workload Synthesizer with Real Statistics for Cloud Analytics Benchmarking](https://arxiv.org/abs/2506.16379)
*Yan Zhou,Chunwei Liu,Bhuvan Urgaonkar,Zhengle Wang,Magnus Mueller,Chao Zhang,Songyue Zhang,Pascal Pfeil,Dominik Horn,Zhengchun Liu,Davide Pagano,Tim Kraska,Samuel Madden,Ju Fan*

Main category: cs.DB

TL;DR: 本文引入带真实统计信息的工作负载合成问题，提出PBench工作负载合成器，研究其关键挑战并评估，相比现有方法可将近似误差最多降低6倍。


<details>
  <summary>Details</summary>
Motivation: 现有标准基准测试无法捕捉生产云工作负载的真实执行统计信息，部分云数据库供应商发布的工作负载跟踪信息缺乏关键组件，不能作为基准测试。

Method: 提出PBench，通过从现有基准中选择和组合工作负载组件构建合成工作负载，采用基于多目标优化的组件选择方法平衡性能指标和操作符分布、设计时间戳分配方法捕捉工作负载时间动态、提出利用大语言模型的组件增强方法处理原始和候选工作负载差异。

Result: 在真实云工作负载跟踪信息上评估，PBench相比现有方法可将近似误差最多降低6倍。

Conclusion: PBench能有效生成接近真实云工作负载执行统计信息的合成工作负载，在降低近似误差方面优于现有方法。

Abstract: Cloud service providers commonly use standard benchmarks like TPC-H and
TPC-DS to evaluate and optimize cloud data analytics systems. However, these
benchmarks rely on fixed query patterns and fail to capture the real execution
statistics of production cloud workloads. Although some cloud database vendors
have recently released real workload traces, these traces alone do not qualify
as benchmarks, as they typically lack essential components like the original
SQL queries and their underlying databases. To overcome this limitation, this
paper introduces a new problem of workload synthesis with real statistics,
which aims to generate synthetic workloads that closely approximate real
execution statistics, including key performance metrics and operator
distributions, in real cloud workloads. To address this problem, we propose
PBench, a novel workload synthesizer that constructs synthetic workloads by
judiciously selecting and combining workload components (i.e., queries and
databases) from existing benchmarks. This paper studies the key challenges in
PBench. First, we address the challenge of balancing performance metrics and
operator distributions by introducing a multi-objective optimization-based
component selection method. Second, to capture the temporal dynamics of real
workloads, we design a timestamp assignment method that progressively refines
workload timestamps. Third, to handle the disparity between the original
workload and the candidate workload, we propose a component augmentation
approach that leverages large language models (LLMs) to generate additional
workload components while maintaining statistical fidelity. We evaluate PBench
on real cloud workload traces, demonstrating that it reduces approximation
error by up to 6x compared to state-of-the-art methods.

</details>


### [54] [LDI: Localized Data Imputation](https://arxiv.org/abs/2506.16616)
*Soroush Omidvartehrani,Davood Rafiei*

Main category: cs.DB

TL;DR: 提出LDI框架改进基于大语言模型的数据插补，实验显示其优于现有方法，精度更高且解释性强。


<details>
  <summary>Details</summary>
Motivation: 现实表格数据存在缺失值影响分析，现有大语言模型插补方法存在精度、可扩展性和可解释性问题。

Method: 引入LDI框架，为每个缺失值选择紧凑、上下文相关的属性和元组子集进行局部提示。

Result: 在四个真实数据集上实验表明，LDI优于现有方法，使用托管大语言模型时精度最高提升8%，轻量级本地模型增益更显著。

Conclusion: LDI不仅精度更高，还具有更好的可解释性和对数据不一致的鲁棒性，适用于高风险和隐私敏感应用。

Abstract: Missing values are a common challenge in real-world tabular data and can
significantly impair downstream analysis. While Large Language Models (LLMs)
have recently shown promise in data imputation, existing methods often rely on
broad, unfiltered prompts that compromise accuracy, scalability, and
explainability. We introduce LDI (Localized Data Imputation), a novel framework
that improves both the accuracy and transparency of LLM-based imputation by
selecting a compact, contextually relevant subset of attributes and tuples for
each missing value. This localized prompting reduces noise, enables
traceability by revealing which data influenced each prediction, and is
effective across both hosted LLMs and lightweight local models. Our extensive
experiments on four real-world datasets show that LDI outperforms
state-of-the-art methods, achieving up to 8% higher accuracy when using hosted
LLMs. The gains are more substantial with lightweight local models, reaching
nearly 17% and 97% accuracy on some datasets when using 3 and 10 examples,
respectively. In addition to higher accuracy, LDI offers improved
interpretability and robustness to data inconsistencies, making it well-suited
for high-stakes and privacy-sensitive applications.

</details>


### [55] [Advancing Fact Attribution for Query Answering: Aggregate Queries and Novel Algorithms](https://arxiv.org/abs/2506.16923)
*Omer Abramovich,Daniel Deutch,Nave Frost,Ahmet Kara,Dan Olteanu*

Main category: cs.DB

TL;DR: 提出计算输入元组对查询结果贡献的新方法，适用于含聚合的查询，有两项优化，实验显示性能提升显著。


<details>
  <summary>Details</summary>
Motivation: 以往算法工作聚焦于Select - Project - Join - Union查询，缺乏针对含聚合查询计算输入元组贡献的实用方法。

Method: 提出两项优化，一是利用很多输入元组对查询结果贡献相同的特点，计算其中一个的贡献即可；二是用查询谱系的梯度以相同复杂度计算所有元组的贡献。

Result: 对无聚合查询，相比现有技术实现高达3个数量级的运行时间改进；对聚合查询具有实用性。

Conclusion: 所提方法实用且能显著提升查询运行时性能。

Abstract: In this paper, we introduce a novel approach to computing the contribution of
input tuples to the result of the query, quantified by the Banzhaf and Shapley
values. In contrast to prior algorithmic work that focuses on
Select-Project-Join-Union queries, ours is the first practical approach for
queries with aggregates. It relies on two novel optimizations that are
essential for its practicality and significantly improve the runtime
performance already for queries without aggregates. The first optimization
exploits the observation that many input tuples have the same contribution to
the query result, so it is enough to compute the contribution of one of them.
The second optimization uses the gradient of the query lineage to compute the
contributions of all tuples with the same complexity as for one of them.
Experiments with a million instances over 3 databases show that our approach
achieves up to 3 orders of magnitude runtime improvements over the
state-of-the-art for queries without aggregates, and that it is practical for
aggregate queries.

</details>


### [56] [PUL: Pre-load in Software for Caches Wouldn't Always Play Along](https://arxiv.org/abs/2506.16976)
*Arthur Bernhardt,Sajjad Tamimi,Florian Stock,Andreas Koch,Ilia Petrov*

Main category: cs.DB

TL;DR: 探讨软件预取在近数据处理系统中的潜力，指出其可通过计算/IO交错提高计算利用率。


<details>
  <summary>Details</summary>
Motivation: 内存延迟和带宽限制系统性能与可扩展性，软件预取效率更高，研究其在近数据处理系统的潜力。

Method: 研究基于软件的、后摩尔时代将操作卸载到智能内存的系统。

Result: 发现软件预取在近数据处理环境中通过计算/IO交错能最大化计算利用率。

Conclusion: 软件预取在近数据处理设置中有更高潜力。

Abstract: Memory latencies and bandwidth are major factors, limiting system performance
and scalability. Modern CPUs aim at hiding latencies by employing large caches,
out-of-order execution, or complex hardware prefetchers. However,
software-based prefetching exhibits higher efficiency, improving with newer CPU
generations.
  In this paper we investigate software-based, post-Moore systems that offload
operations to intelligent memories. We show that software-based prefetching has
even higher potential in near-data processing settings by maximizing compute
utilization through compute/IO interleaving.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [57] [TrainVerify: Equivalence-Based Verification for Distributed LLM Training](https://arxiv.org/abs/2506.15961)
*Yunchi Lu,Youshan Miao,Cheng Tan,Peng Huang,Yi Zhu,Xian Zhang,Fan Yang*

Main category: cs.DC

TL;DR: 介绍用于可验证的大语言模型分布式训练系统TrainVerify，可减少验证复杂度并成功验证前沿模型训练计划


<details>
  <summary>Details</summary>
Motivation: 大规模大语言模型分布式训练成本高且很少验证，易出现静默错误和浪费资源

Method: 引入形状缩减技术和分阶段并行验证算法

Result: 成功验证Llama3 (405B) 和DeepSeek-V3 (671B) 训练计划

Conclusion: TrainVerify可扩展到前沿大语言模型进行可验证的分布式训练

Abstract: Training large language models (LLMs) at scale requires parallel execution
across thousands of devices, incurring enormous computational costs. Yet, these
costly distributed trainings are rarely verified, leaving them prone to silent
errors and potentially wasting millions of GPU hours. We introduce TrainVerify,
a system for verifiable distributed training of LLMs. Given a deep learning
model's logical specification as the ground truth, TrainVerify formally
verifies that a distributed parallel execution plan is mathematically
equivalent to it. Direct verification is notoriously difficult due to the sheer
scale of LLMs which often involves billions of variables and highly intricate
computation graphs. Therefore, TrainVerify introduces shape-reduction
techniques and a stage-wise parallel verification algorithm that significantly
reduces complexity while preserving formal correctness. TrainVerify scales to
frontier LLMs, including the successful verification of the Llama3 (405B) and
DeepSeek-V3 (671B) training plans.

</details>


### [58] [NetSenseML: Network-Adaptive Compression for Efficient Distributed Machine Learning](https://arxiv.org/abs/2506.16235)
*Yisu Wang,Xinjiao Li,Ruilong Wu,Huangxun Chen,Dirk Kutscher*

Main category: cs.DC

TL;DR: 本文提出NetSenseML框架，可根据实时网络状况动态调整策略，平衡数据负载降低与模型精度，实验表明能提升训练吞吐量。


<details>
  <summary>Details</summary>
Motivation: 大规模分布式机器学习模型训练对网络要求高，现有梯度压缩技术会降低模型精度，需新方法平衡负载与精度。

Method: 引入NetSenseML框架，实时监测网络状况，仅在拥塞影响收敛速度时进行梯度压缩，根据网络状况调整策略。

Result: 实验显示在带宽受限条件下，NetSenseML能将训练吞吐量提升1.55到9.84倍。

Conclusion: NetSenseML可有效平衡数据负载降低和模型精度保存，提高资源利用效率，缩短收敛时间，提升训练效率。

Abstract: Training large-scale distributed machine learning models imposes considerable
demands on network infrastructure, often resulting in sudden traffic spikes
that lead to congestion, increased latency, and reduced throughput, which would
ultimately affect convergence times and overall training performance. While
gradient compression techniques are commonly employed to alleviate network
load, they frequently compromise model accuracy due to the loss of gradient
information.
  This paper introduces NetSenseML, a novel network adaptive distributed deep
learning framework that dynamically adjusts quantization, pruning, and
compression strategies in response to real-time network conditions. By actively
monitoring network conditions, NetSenseML applies gradient compression only
when network congestion negatively impacts convergence speed, thus effectively
balancing data payload reduction and model accuracy preservation.
  Our approach ensures efficient resource usage by adapting reduction
techniques based on current network conditions, leading to shorter convergence
times and improved training efficiency. We present the design of the NetSenseML
adaptive data reduction function and experimental evaluations show that
NetSenseML can improve training throughput by a factor of 1.55 to 9.84 times
compared to state-of-the-art compression-enabled systems for representative DDL
training jobs in bandwidth-constrained conditions.

</details>


### [59] [A Study of Synchronization Methods for Concurrent Size](https://arxiv.org/abs/2506.16350)
*Hen Kas-Sharir,Gal Sela,Erez Petrank*

Main category: cs.DC

TL;DR: 研究并发环境下数据结构大小方法同步技术以提升性能，对比多种方法，发现无通用方法，不同场景适用不同同步法。


<details>
  <summary>Details</summary>
Motivation: 并发环境下，集成线性化并发大小方法会对数据结构所有操作引入明显开销，需改进性能。

Method: 研究握手技术、乐观技术和基于锁的技术，并与现有大小方法进行评估对比。

Result: 选择合适的同步方法可显著降低开销，但无通用方法。

Conclusion: 低竞争场景下，乐观和基于锁的方法最佳；高竞争场景下，握手和无等待方法最有效，研究结果符合并发计算一般趋势。

Abstract: The size of collections, maps, and data structures in general, constitutes a
fundamental property. An implementation of the size method is required in most
programming environments. Nevertheless, in a concurrent environment,
integrating a linearizable concurrent size introduces a noticeable overhead on
all operations of the data structure, even when the size method is not invoked
during the execution. In this work we present a study of synchronization
methods in an attempt to improve the performance of the data structure. In
particular, we study a handshake technique that is commonly used with
concurrent garbage collection, an optimistic technique, and a lock-based
technique. Evaluation against the state-of-the-art size methodology
demonstrates that the overhead can be significantly reduced by selecting the
appropriate synchronization approach, but there is no one-size-fits-all method.
Different scenarios call for different synchronization methods, as rigorously
shown in this study. Nevertheless, our findings align with general trends in
concurrent computing. In scenarios characterized by low contention, optimistic
and lock-based approaches work best, whereas under high contention, the most
effective solutions are the handshake approach and the wait-free approach.

</details>


### [60] [Parallel Point-to-Point Shortest Paths and Batch Queries](https://arxiv.org/abs/2506.16488)
*Xiaojun Dong,Andy Li,Yan Gu,Yihan Sun*

Main category: cs.DC

TL;DR: 提出Orionet用于并行PPSP和批量PPSP查询，实验显示比基线快。


<details>
  <summary>Details</summary>
Motivation: 为PPSP和批量PPSP查询提供高效并行实现。

Method: 基于现有SSSP框架结合剪枝条件，运用早终止、双向搜索等算法，将批量查询形式化为查询图。

Result: 在14个图上测试，双向搜索比GraphIt快2.9倍、比MBQ快6.8倍，双向A*也更快，批量查询表现强。

Conclusion: Orionet在PPSP和批量PPSP查询中表现高效。

Abstract: We propose Orionet, efficient parallel implementations of Point-to-Point
Shortest Paths (PPSP) queries using bidirectional search (BiDS) and other
heuristics, with an additional focus on batch PPSP queries. We present a
framework for parallel PPSP built on existing single-source shortest paths
(SSSP) frameworks by incorporating pruning conditions. As a result, we develop
efficient parallel PPSP algorithms based on early termination, bidirectional
search, A$^*$ search, and bidirectional A$^*$ all with simple and efficient
implementations.
  We extend our idea to batch PPSP queries, which are widely used in real-world
scenarios. We first design a simple and flexible abstraction to represent the
batch so PPSP can leverage the shared information of the batch. Orionet
formalizes the batch as a query graph represented by edges between queried
sources and targets. In this way, we directly extended our PPSP framework to
batched queries in a simple and efficient way.
  We evaluate Orionet on both single and batch PPSP queries using various graph
types and distance percentiles of queried pairs, and compare it against two
baselines, GraphIt and MBQ. Both of them support parallel single PPSP and A$^*$
using unidirectional search. On 14 graphs we tested, on average, our
bidirectional search is 2.9$\times$ faster than GraphIt, and 6.8$\times$ faster
than MBQ. Our bidirectional A$^*$ is 4.4$\times$ and 6.2$\times$ faster than
the A$^*$ in GraphIt and MBQ, respectively. For batched PPSP queries, we also
provide in-depth experimental evaluation, and show that Orionet provides strong
performance compared to the plain solutions.

</details>


### [61] [Enabling Blockchain Interoperability Through Network Discovery Services](https://arxiv.org/abs/2506.16611)
*Khalid Hassan,Amirreza Sokhankhosh,Sara Rouhani*

Main category: cs.DC

TL;DR: 论文提出区块链网络发现的去中心化架构及资产和服务发现机制，设计激励机制，并用Substrate框架实现评估，展现出高弹性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有区块链网络互操作性方案未解决网络初始发现问题，存在关键缺口。

Method: 提出独立于任何集中式权威的区块链网络发现去中心化架构，引入从外部网络发现区块链内资产和服务的机制，设计激励机制鼓励节点参与维护发现网络。

Result: 使用Substrate框架实现并评估，在测试网络配置下能有效处理多达130,000个并发请求，中位响应时间5.5毫秒，可通过增加网络规模进一步扩展处理能力。

Conclusion: 所提去中心化架构具有弹性和可扩展性，能解决区块链网络初始发现问题。

Abstract: Web3 technologies have experienced unprecedented growth in the last decade,
achieving widespread adoption. As various blockchain networks continue to
evolve, we are on the cusp of a paradigm shift in which they could provide
services traditionally offered by the Internet, but in a decentralized manner,
marking the emergence of the Internet of Blockchains. While significant
progress has been achieved in enabling interoperability between blockchain
networks, existing solutions often assume that networks are already mutually
aware. This reveals a critical gap: the initial discovery of blockchain
networks remains largely unaddressed. This paper proposes a decentralized
architecture for blockchain network discovery that operates independently of
any centralized authority. We also introduce a mechanism for discovering assets
and services within a blockchain from external networks. Given the
decentralized nature of the proposed discovery architecture, we design an
incentive mechanism to encourage nodes to actively participate in maintaining
the discovery network. The proposed architecture implemented and evaluated,
using the Substrate framework, demonstrates its resilience and scalability,
effectively handling up to 130,000 concurrent requests under the tested network
configurations, with a median response time of 5.5 milliseconds, demonstrating
the ability to scale its processing capacity further by increasing its network
size.

</details>


### [62] [JANUS: Resilient and Adaptive Data Transmission for Enabling Timely and Efficient Cross-Facility Scientific Workflows](https://arxiv.org/abs/2506.17084)
*Vladislav Esaulov,Jieyang Chen,Norbert Podhorszki,Fred Suter,Scott Klasky,Anu G Bourgeois,Lipeng Wan*

Main category: cs.DC

TL;DR: 本文提出适用于跨设施科学工作流的数据传输方法JANUS，实验显示其提高传输效率并保证数据保真。


<details>
  <summary>Details</summary>
Motivation: 现代大型项目对跨设施工作流依赖增加，当前数据传输存在带宽紧张、TCP重传及传统容错方法开销大等问题。

Method: JANUS采用UDP，集成擦除编码实现容错，应用误差有界的有损压缩减少开销，能根据网络状况调整编码参数并使用优化模型确定理想配置。

Result: 实验表明JANUS显著提高了数据传输效率，同时保证了数据保真。

Conclusion: JANUS是一种有潜力的跨设施科学工作流数据传输方法，能有效应对现有传输挑战。

Abstract: In modern science, the growing complexity of large-scale projects has
increased reliance on cross-facility workflows, where institutions share
resources and expertise to accelerate discovery. These workflows often involve
transferring massive data over wide-area networks. While high-speed networks
like ESnet and data transfer services like Globus have improved data mobility,
challenges remain. Large data volumes can strain bandwidth, TCP suffers from
retransmissions due to packet loss, and traditional fault-tolerance methods
like erasure coding introduce significant overhead.
  This paper presents JANUS, a resilient and adaptive data transmission
approach for cross-facility scientific workflows. JANUS uses UDP, integrates
erasure coding for fault tolerance, and applies error-bounded lossy compression
to reduce overhead. This design enables users to balance transmission time and
accuracy based on specific needs. JANUS also adapts coding parameters to
real-time network conditions and uses optimization models to determine ideal
configurations. Experiments show that JANUS significantly improves data
transfer efficiency while preserving fidelity.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [63] [Linearithmic Clean-up for Vector-Symbolic Key-Value Memory with Kroneker Rotation Products](https://arxiv.org/abs/2506.15793)
*Ruipeng Liu,Qinru Qiu,Simon Khan,Garrett E. Katz*

Main category: cs.DS

TL;DR: 提出新的码本表示法，使向量符号架构清理步骤更高效，复杂度为线性对数，实验证明可扩展性强。


<details>
  <summary>Details</summary>
Motivation: 当前向量符号架构（VSAs）清理步骤是计算瓶颈，传统清理复杂度高。

Method: 基于旋转类矩阵的Kroneker积提出新的码本表示法。

Result: 清理时间复杂度为线性对数，空间复杂度为O(N)，码本无需显式存储，计算机实验证实结果，可扩展性比基线技术高几个数量级。

Conclusion: 新的码本表示法能有效提升VSAs清理步骤效率和可扩展性。

Abstract: A computational bottleneck in current Vector-Symbolic Architectures (VSAs) is
the ``clean-up'' step, which decodes the noisy vectors retrieved from the
architecture. Clean-up typically compares noisy vectors against a ``codebook''
of prototype vectors, incurring computational complexity that is quadratic or
similar. We present a new codebook representation that supports efficient
clean-up, based on Kroneker products of rotation-like matrices. The resulting
clean-up time complexity is linearithmic, i.e. $\mathcal{O}(N\,\text{log}\,N)$,
where $N$ is the vector dimension and also the number of vectors in the
codebook. Clean-up space complexity is $\mathcal{O}(N)$. Furthermore, the
codebook is not stored explicitly in computer memory: It can be represented in
$\mathcal{O}(\text{log}\,N)$ space, and individual vectors in the codebook can
be materialized in $\mathcal{O}(N)$ time and space. At the same time,
asymptotic memory capacity remains comparable to standard approaches. Computer
experiments confirm these results, demonstrating several orders of magnitude
more scalability than baseline VSA techniques.

</details>


### [64] [HybHuff: Lossless Compression for Hypergraphs via Entropy-Guided Huffman-Bitwise Coordination](https://arxiv.org/abs/2506.15844)
*Tianyu Zhao,Dongfang Zhao,Luanzheng Guo,Nathan Tallent*

Main category: cs.DS

TL;DR: 提出整数超图邻接格式混合压缩框架，结合Huffman和位编码，实验显示压缩率高，集成到常见超图工作负载性能损失小。


<details>
  <summary>Details</summary>
Motivation: 超图在数据密集型应用中内存消耗大，降低超图表示的空间开销是主要挑战。

Method: 提出混合压缩框架，自适应结合Huffman编码和位编码，给出理论分析和近似最优编码比的经验策略。

Result: 在真实超图上压缩率比标准压缩器高2.3倍，解码开销相当；集成到常见超图工作负载性能损失可忽略。

Conclusion: 该方法高效且适用。

Abstract: Hypergraphs provide a natural representation for many-to-many relationships
in data-intensive applications, yet their scalability is often hindered by high
memory consumption. While prior work has improved computational efficiency,
reducing the space overhead of hypergraph representations remains a major
challenge. This paper presents a hybrid compression framework for integer-based
hypergraph adjacency formats, which adaptively combines Huffman encoding and
bitwise encoding to exploit structural redundancy. We provide a theoretical
analysis showing that an optimal encoding ratio exists between the two schemes,
and introduce an empirical strategy to approximate this ratio for practical
use. Experiments on real-world hypergraphs demonstrate that our method
consistently outperforms standard compressors such as Zip and ZFP in
compression rate by up to 2.3x with comparable decoding overhead. To assess
practical utility, we integrate our framework with three common hypergraph
workloads: breadth-first search, PageRank, and k-core label propagation, and
show that compression incurs negligible performance loss. Extensive evaluations
across four benchmark datasets confirm the efficiency and applicability of our
approach.

</details>


### [65] [On the Efficient Discovery of Maximum $k$-Defective Biclique](https://arxiv.org/abs/2506.16121)
*Donghang Cui,Ronghua Li,Qiangqiang Dai,Hongchao Qin,Guoren Wang*

Main category: cs.DS

TL;DR: 本文聚焦二分图中最大边k - 缺陷双团问题，证明其NP - 难，提出新算法和优化技术，实验显示算法优于现有算法。


<details>
  <summary>Details</summary>
Motivation: 真实世界图存在噪声和信息不完整，传统双团模型条件过于严格，需新的子图模型。

Method: 提出基于新分支限界框架的算法，结合新的枢轴技术，还开发了图约简、新上界和启发式等优化技术。

Result: 在10个大型真实数据集上实验，算法在不同参数设置下比现有算法快达1000倍。

Conclusion: 提出的算法和优化技术高效有效，优于现有算法。

Abstract: The problem of identifying the maximum edge biclique in bipartite graphs has
attracted considerable attention in bipartite graph analysis, with numerous
real-world applications such as fraud detection, community detection, and
online recommendation systems. However, real-world graphs may contain noise or
incomplete information, leading to overly restrictive conditions when employing
the biclique model. To mitigate this, we focus on a new relaxed subgraph model,
called the $k$-defective biclique, which allows for up to $k$ missing edges
compared to the biclique model. We investigate the problem of finding the
maximum edge $k$-defective biclique in a bipartite graph, and prove that the
problem is NP-hard. To tackle this computation challenge, we propose a novel
algorithm based on a new branch-and-bound framework, which achieves a
worst-case time complexity of $O(m\alpha_k^n)$, where $\alpha_k < 2$. We
further enhance this framework by incorporating a novel pivoting technique,
reducing the worst-case time complexity to $O(m\beta_k^n)$, where $\beta_k <
\alpha_k$. To improve the efficiency, we develop a series of optimization
techniques, including graph reduction methods, novel upper bounds, and a
heuristic approach. Extensive experiments on 10 large real-world datasets
validate the efficiency and effectiveness of the proposed approaches. The
results indicate that our algorithms consistently outperform state-of-the-art
algorithms, offering up to $1000\times$ speedups across various parameter
settings.

</details>


### [66] [Parallel batch queries on dynamic trees: algorithms and experiments](https://arxiv.org/abs/2506.16477)
*Humza Ikram,Andrew Brady,Daniel Anderson,Guy Blelloch*

Main category: cs.DS

TL;DR: 本文改进批并行动态树结构，实现相关代码并进行实验，实验显示有良好加速比且性能稳定。


<details>
  <summary>Details</summary>
Motivation: 动态树是现代算法核心，虽有批并行动态树相关工作，但仍需改进。

Method: 推广RC树以支持任意度和丰富查询，描述支持多种查询的方法，实现首个通用批动态树，开发森林生成器进行实验。

Result: 实验测量创建树时间、不同批量大小影响，用树实现增量批并行最小生成树，显示出良好加速比。

Conclusion: 算法性能在不同森林特征下表现稳健。

Abstract: Dynamic tree data structures maintain a forest while supporting insertion and
deletion of edges and a broad set of queries in $O(\log n)$ time per operation.
Such data structures are at the core of many modern algorithms. Recent work has
extended dynamic trees so as to support batches of updates or queries so as to
run in parallel, and these batch parallel dynamic trees are now used in several
parallel algorithms. In this work we describe improvements to batch parallel
dynamic trees, describe an implementation that incorporates these improvements,
and experiments using it. The improvements includes generalizing prior work on
RC (rake compress) trees to work with arbitrary degree while still supporting a
rich set of queries, and describing how to support batch subtree queries, path
queries, LCA queries, and nearest-marked-vertex queries in $O(k + k \log (1 +
n/k))$ work and polylogarithmic span. Our implementation is the first general
implementation of batch dynamic trees (supporting arbitrary degree and general
queries). Our experiments include measuring the time to create the trees,
varying batch sizes for updates and queries, and using the tree to implement
incremental batch-parallel minimum spanning trees. To run the experiments we
develop a forest generator that is parameterized to create distributions of
trees of differing characteristics (e.g., degree, depth, and relative tree
sizes). Our experiments show good speedup and that the algorithm performance is
robust across forest characteristics.

</details>


### [67] [LMQ-Sketch: Lagom Multi-Query Sketch for High-Rate Online Analytics](https://arxiv.org/abs/2506.16928)
*Martin Hilgendorf,Marina Papatriantafilou*

Main category: cs.DS

TL;DR: 本文提出LMQ - Sketch解决数据草图集成多类型查询和并发更新的挑战，其方法Lagom有低延迟、高吞吐量等优势，相比现有方法表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决数据草图在集成多类型查询和并发更新时出现的挑战。

Method: 研究权衡因素，合成关键思想形成LMQ - Sketch，其核心方法Lagom结合工作分配和同步实现并发语义。

Result: LMQ - Sketch有高吞吐量，有额外的准确性保证和并发语义，能将所需内存预算降低一个数量级。

Conclusion: 该方法有望对并发多查询草图产生更广泛影响。

Abstract: Data sketches balance resource efficiency with controllable approximations
for extracting features in high-volume, high-rate data. Two important points of
interest are highlighted separately in recent works; namely, to (1) answer
multiple types of queries from one pass, and (2) query concurrently with
updates. Several fundamental challenges arise when integrating these
directions, which we tackle in this work. We investigate the trade-offs to be
balanced and synthesize key ideas into LMQ-Sketch, a single, composite data
sketch supporting multiple queries (frequency point queries, frequency moments
F1, and F2) concurrently with updates. Our method 'Lagom' is a cornerstone of
LMQ-Sketch for low-latency global querying (<100 us), combining freshness,
timeliness, and accuracy with a low memory footprint and high throughput (>2B
updates/s). We analyze and evaluate the accuracy of Lagom, which builds on a
simple geometric argument and efficiently combines work distribution with
synchronization for proper concurrency semantics -- monotonicity of operations
and intermediate value linearizability. Comparing with state-of-the-art methods
(which, as mentioned, only cover either mixed queries or concurrency),
LMQ-Sketch shows highly competitive throughput, with additional accuracy
guarantees and concurrency semantics, while also reducing the required memory
budget by an order of magnitude. We expect the methodology to have broader
impact on concurrent multi-query sketches.

</details>


### [68] [When does FTP become FPT?](https://arxiv.org/abs/2506.17008)
*Matthias Bentert,Fedor V. Fomin,Petr A. Golovach,Laure Morelle*

Main category: cs.DS

TL;DR: 研究Fault - Tolerant Path (FTP)问题在多种参数化下是否为固定参数可处理（FPT）以及是否有多项式核，并近乎完整描述其复杂度格局。


<details>
  <summary>Details</summary>
Motivation: 明确FTP问题在不同参数化下的复杂度性质，了解其可处理性。

Method: 选择多个参数，包括输入图中脆弱边数量、安全边数量、预算等，研究FTP问题。

Result: 近乎完整地描述了FTP在所选参数下的复杂度格局。

Conclusion: 对FTP问题在多种参数化下的复杂度有了较为全面的认识。

Abstract: In the problem Fault-Tolerant Path (FTP), we are given an edge-weighted
directed graph G = (V, E), a subset U \subseteq E of vulnerable edges, two
vertices s, t \in V, and integers k and \ell. The task is to decide whether
there exists a subgraph H of G with total cost at most \ell such that, after
the removal of any k vulnerable edges, H still contains an s-t-path. We study
whether Fault-Tolerant Path is fixed-parameter tractable (FPT) and whether it
admits a polynomial kernel under various parameterizations. Our choices of
parameters include: the number of vulnerable edges in the input graph, the
number of safe (i.e, invulnerable) edges in the input graph, the budget \ell,
the minimum number of safe edges in any optimal solution, the minimum number of
vulnerable edges in any optimal solution, the required redundancy k, and
natural above- and below-guarantee parameterizations. We provide an almost
complete description of the complexity landscape of FTP for these parameters.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [69] [Fair Contracts in Principal-Agent Games with Heterogeneous Types](https://arxiv.org/abs/2506.15887)
*Jakub Tłuczek,Victor Villin,Christos Dimitrakakis*

Main category: cs.GT

TL;DR: 提出基于重复委托 - 代理博弈的框架，使公平意识的委托人学习线性合同以实现多智能体系统公平，且不牺牲效率。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统中因潜在特征差异难以实现公平，隐藏的异质性导致财富分配不均，受现实例子启发开展研究。

Method: 提出基于重复委托 - 代理博弈的框架，让委托人学习提供自适应合同。

Result: 公平意识的委托人能学习同质线性合同，在顺序社会困境中使智能体结果均等，且公平不牺牲效率。

Conclusion: 在多智能体系统中可在保持整体性能的同时促进公平与稳定。

Abstract: Fairness is desirable yet challenging to achieve within multi-agent systems,
especially when agents differ in latent traits that affect their abilities.
This hidden heterogeneity often leads to unequal distributions of wealth, even
when agents operate under the same rules. Motivated by real-world examples, we
propose a framework based on repeated principal-agent games, where a principal,
who also can be seen as a player of the game, learns to offer adaptive
contracts to agents. By leveraging a simple yet powerful contract structure, we
show that a fairness-aware principal can learn homogeneous linear contracts
that equalize outcomes across agents in a sequential social dilemma.
Importantly, this fairness does not come at the cost of efficiency: our results
demonstrate that it is possible to promote equity and stability in the system
while preserving overall performance.

</details>


### [70] [Solving Zero-Sum Convex Markov Games](https://arxiv.org/abs/2506.16120)
*Fivos Kalogiannis,Emmanouil-Vasileios Vlatakis-Gkaragkounis,Ian Gemp,Georgios Piliouras*

Main category: cs.GT

TL;DR: 本文用独立策略梯度方法，对双人零和凸马尔可夫博弈（cMGs）中纳什均衡的全局收敛性给出了首个可证明的保证。


<details>
  <summary>Details</summary>
Motivation: 凸马尔可夫博弈将马尔可夫决策过程扩展到多智能体场景，但即使是基本的极小 - 极大情况也存在固有非凸性、缺乏贝尔曼一致性和无限时域复杂性等挑战，需要研究其全局收敛性。

Method: 采用两步法，一是利用隐凸 - 隐凹函数性质，用简单非凸正则化将极小 - 极大优化问题转化为非凸近端Polyak - Lojasiewicz（NC - pPL）目标；二是在此基础上，研究NC - pPL和双边pPL条件下的一般约束极小 - 极大问题。

Result: 给出了随机嵌套和交替梯度下降 - 上升方法的全局收敛性保证。

Conclusion: 独立策略梯度方法能使双人零和凸马尔可夫博弈全局收敛到纳什均衡，所提出的方法对解决相关问题有重要意义。

Abstract: We contribute the first provable guarantees of global convergence to Nash
equilibria (NE) in two-player zero-sum convex Markov games (cMGs) by using
independent policy gradient methods. Convex Markov games, recently defined by
Gemp et al. (2024), extend Markov decision processes to multi-agent settings
with preferences that are convex over occupancy measures, offering a broad
framework for modeling generic strategic interactions. However, even the
fundamental min-max case of cMGs presents significant challenges, including
inherent nonconvexity, the absence of Bellman consistency, and the complexity
of the infinite horizon.
  We follow a two-step approach. First, leveraging properties of
hidden-convex--hidden-concave functions, we show that a simple nonconvex
regularization transforms the min-max optimization problem into a
nonconvex-proximal Polyak-Lojasiewicz (NC-pPL) objective. Crucially, this
regularization can stabilize the iterates of independent policy gradient
methods and ultimately lead them to converge to equilibria. Second, building on
this reduction, we address the general constrained min-max problems under
NC-pPL and two-sided pPL conditions, providing the first global convergence
guarantees for stochastic nested and alternating gradient descent-ascent
methods, which we believe may be of independent interest.

</details>


### [71] [Bidder Feedback in First-Price Auctions for Video Advertising](https://arxiv.org/abs/2506.17058)
*Sébastien Lahaie,Benjamin Schaeffer,Yuanjun Zhou*

Main category: cs.GT

TL;DR: 本文研究将首价拍卖中‘最低中标价’概念推广到一般组合拍卖，以视频广告领域为背景，通过计算投标更新来刻画联合投标更新集，在分配问题上给出双合作核心的线性规划表征，并进行实证分析。


<details>
  <summary>Details</summary>
Motivation: 受视频广告领域启发，研究将首价拍卖中‘最低中标价’概念推广到一般组合拍卖。

Method: 将问题转化为计算能维持当前分配最优性的投标更新（折扣和提价），利用双合作博弈的核心来刻画联合投标更新集，在分配问题上使用线性规划进行表征，结合实际广告交易数据进行实证分析。

Result: 将具有维持当前分配最优性的联合投标更新集表征为相关双合作博弈的核心；在分配问题上给出双合作核心的线性规划表征；得到几个广义‘最低中标价’的候选方案。

Conclusion: 通过理论刻画和实证分析，对广义‘最低中标价’概念有了更深入理解，为组合拍卖的投标动态和收敛性研究提供了参考。

Abstract: In first-price auctions for display advertising, exchanges typically
communicate the "minimum-bid-to-win" to bidders after the auction as feedback
for their bidding algorithms. For a winner, this is the second-highest bid,
while for losing bidders it is the highest bid. In this paper we investigate
the generalization of this concept to general combinatorial auctions, motivated
by the domain of video advertising. In a video pod auction, ad slots during an
advertising break in a video stream are auctioned all at once, under several
kinds of allocation constraints such as a constraint on total ad duration. We
cast the problem in terms of computing bid updates (discounts and raises) that
maintain the optimality of the current allocation. Our main result
characterizes the set of joint bid updates with this property as the core of an
associated bicooperative game. In the case of the assignment problem--a special
case of video pod auctions--we provide a linear programming characterization of
this bicooperative core. Our characterization leads to several candidates for a
generalized minimum-bid-to-win. Drawing on video pod auction data from a real
ad exchange, we perform an empirical analysis to understand the bidding
dynamics they induce and their convergence properties.

</details>


### [72] [A Vision for Trustworthy, Fair, and Efficient Socio-Technical Control using Karma Economies](https://arxiv.org/abs/2506.17115)
*Ezzat Elokda,Andrea Censi,Emilio Frazzoli,Florian Dörfler,Saverio Bolognani*

Main category: cs.GT

TL;DR: 文章倡导将因果经济作为社会技术控制的非货币机制，能实现资源可信、公平和高效分配，还讨论其在未来智慧城市的应用。


<details>
  <summary>Details</summary>
Motivation: 传统货币控制在社会技术资源环境中不总被接受，需新机制实现稀缺公共资源可信、公平和高效分配。

Method: 从控制系统视角回顾经济学相关概念，将资源分配视角从单次静态转变为重复动态博弈，采用长期纳什福利来定义公平和效率。

Result: 在许多动态资源环境中，因果纳什均衡能最大化长期纳什福利。

Conclusion: 因果经济为设计公平和效率范围提供新灵活性，可应用于未来智慧城市的多因果经济构建。

Abstract: Control systems will play a pivotal role in addressing societal-scale
challenges as they drive the development of sustainable future smart cities. At
the heart of these challenges is the trustworthy, fair, and efficient
allocation of scarce public resources, including renewable energy,
transportation, data, computation, etc.. Historical evidence suggests that
monetary control -- the prototypical mechanism for managing resource scarcity
-- is not always well-accepted in socio-technical resource contexts. In this
vision article, we advocate for karma economies as an emerging non-monetary
mechanism for socio-technical control. Karma leverages the repetitive nature of
many socio-technical resources to jointly attain trustworthy, fair, and
efficient allocations; by budgeting resource consumption over time and letting
resource users ``play against their future selves.'' To motivate karma, we
review related concepts in economics through a control systems lens, and make a
case for a) shifting the viewpoint of resource allocations from single-shot and
static to repeated and dynamic games; and b) adopting long-run Nash welfare as
the formalization of ``fairness and efficiency'' in socio-technical contexts.
We show that in many dynamic resource settings, karma Nash equilibria maximize
long-run Nash welfare. Moreover, we discuss implications for a future smart
city built on multi-karma economies: by choosing whether to combine different
socio-technical resources, e.g., electricity and transportation, in a single
karma economy, or separate into resource-specific economies, karma provides new
flexibility to design the scope of fairness and efficiency.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [73] [Architecture is All You Need: Improving LLM Recommenders by Dropping the Text](https://arxiv.org/abs/2506.15833)
*Kevin Foley,Shaghayegh Agah,Kavya Priyanka Kakinada*

Main category: cs.IR

TL;DR: 提出简化推荐模型，以小尺寸和低计算复杂度超越传统及基于PLM的推荐模型，表明LLM在推荐系统中架构更重要。


<details>
  <summary>Details</summary>
Motivation: 基于PLM的推荐器因尺寸大、计算成本高难实践，微调可能降低泛化能力，需改进。

Method: 采用大语言模型架构，减少层数和维度，用离散令牌替代基于文本的子词分词。

Result: 简化方法在尺寸和计算复杂度远低于基于PLM模型的情况下，大幅超越传统和基于PLM的推荐模型。

Conclusion: 大语言模型在推荐系统中的主要优势是其架构，而非预训练获得的世界知识。

Abstract: In recent years, there has been an explosion of interest in the applications
of large pre-trained language models (PLMs) to recommender systems, with many
studies showing strong performance of PLMs on common benchmark datasets.
PLM-based recommender models benefit from flexible and customizable prompting,
an unlimited vocabulary of recommendable items, and general ``world knowledge''
acquired through pre-training on massive text corpora. While PLM-based
recommenders show promise in settings where data is limited, they are hard to
implement in practice due to their large size and computational cost.
Additionally, fine-tuning PLMs to improve performance on collaborative signals
may degrade the model's capacity for world knowledge and generalizability. We
propose a recommender model that uses the architecture of large language models
(LLMs) while reducing layer count and dimensions and replacing the text-based
subword tokenization of a typical LLM with discrete tokens that uniquely
represent individual content items. We find that this simplified approach
substantially outperforms both traditional sequential recommender models and
PLM-based recommender models at a tiny fraction of the size and computational
complexity of PLM-based models. Our results suggest that the principal benefit
of LLMs in recommender systems is their architecture, rather than the world
knowledge acquired during extensive pre-training.

</details>


### [74] [MoR: Better Handling Diverse Queries with a Mixture of Sparse, Dense, and Human Retrievers](https://arxiv.org/abs/2506.15862)
*Jushaan Singh Kalra,Xinran Zhao,To Eun Kim,Fengyu Cai,Fernando Diaz,Tongshuang Wu*

Main category: cs.IR

TL;DR: 提出混合检索器方法，零样本加权组合异构检索器，实验表明其有效高效，表现优于单个检索器和大模型，还能与人类信息源协作提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有检索增强生成（RAG）通常基于启发式固定单个检索器，无法适应不同信息需求，需动态选择和集成多个检索器。

Method: 引入混合检索器，对异构检索器进行零样本加权组合。

Result: 参数仅0.8B的混合检索器平均比单个检索器和7B大模型分别高10.8%和3.9%；与人类信息源协作相对性能提升58.9%。

Conclusion: 混合检索器方法有效且高效，可集成多种检索器和人类信息源。

Abstract: Retrieval-augmented Generation (RAG) is powerful, but its effectiveness
hinges on which retrievers we use and how. Different retrievers offer distinct,
often complementary signals: BM25 captures lexical matches; dense retrievers,
semantic similarity. Yet in practice, we typically fix a single retriever based
on heuristics, which fails to generalize across diverse information needs. Can
we dynamically select and integrate multiple retrievers for each individual
query, without the need for manual selection? In our work, we validate this
intuition with quantitative analysis and introduce mixture of retrievers: a
zero-shot, weighted combination of heterogeneous retrievers. Extensive
experiments show that such mixtures are effective and efficient: Despite
totaling just 0.8B parameters, this mixture outperforms every individual
retriever and even larger 7B models by +10.8% and +3.9% on average,
respectively. Further analysis also shows that this mixture framework can help
incorporate specialized non-oracle human information sources as retrievers to
achieve good collaboration, with a 58.9% relative performance improvement over
simulated humans alone.

</details>


### [75] [SEP-GCN: Leveraging Similar Edge Pairs with Temporal and Spatial Contexts for Location-Based Recommender Systems](https://arxiv.org/abs/2506.16003)
*Tan Loc Nguyen,Tin T. Tran*

Main category: cs.IR

TL;DR: 提出SEP - GCN图推荐框架，利用上下文相似交互边对学习，实验表明其在准确性和鲁棒性上优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有推荐系统模型多关注节点级表示或孤立边属性，未充分利用交互间的关系结构，需改进。

Method: 提出SEP - GCN框架，通过识别相似时间窗口或地理邻近的边对，增加上下文相似链接，再用边感知卷积机制处理丰富图。

Result: 在基准数据集实验中，SEP - GCN在预测准确性和鲁棒性上持续优于强基线。

Conclusion: SEP - GCN能更准确、稳健地建模用户偏好，尤其适用于稀疏或动态环境。

Abstract: Recommender systems play a crucial role in enabling personalized content
delivery amidst the challenges of information overload and human mobility.
Although conventional methods often rely on interaction matrices or graph-based
retrieval, recent approaches have sought to exploit contextual signals such as
time and location. However, most existing models focus on node-level
representation or isolated edge attributes, underutilizing the relational
structure between interactions. We propose SEP-GCN, a novel graph-based
recommendation framework that learns from pairs of contextually similar
interaction edges, each representing a user-item check-in event. By identifying
edge pairs that occur within similar temporal windows or geographic proximity,
SEP-GCN augments the user-item graph with contextual similarity links. These
links bridge distant but semantically related interactions, enabling improved
long-range information propagation. The enriched graph is processed via an
edge-aware convolutional mechanism that integrates contextual similarity into
the message-passing process. This allows SEP-GCN to model user preferences more
accurately and robustly, especially in sparse or dynamic environments.
Experiments on benchmark data sets show that SEP-GCN consistently outperforms
strong baselines in both predictive accuracy and robustness.

</details>


### [76] [GFlowGR: Fine-tuning Generative Recommendation Frameworks with Generative Flow Networks](https://arxiv.org/abs/2506.16114)
*Yejing Wang,Shengyu Zhou,Jinyu Lu,Qidong Liu,Xinhang Li,Wenlin Zhang,Feng Li,Pengjie Wang,Jian Xu,Bo Zheng,Xiangyu Zhao*

Main category: cs.IR

TL;DR: 现有生成式推荐（GR）微调步骤存在曝光偏差问题，本文提出GFlowGR框架缓解该问题，实验证明其有效性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有GR框架的微调步骤未充分探索，且当前方法存在曝光偏差问题，需要改进。

Method: 将GR视为多步生成任务，构建基于GFlowNets的微调框架GFlowGR，集成传统推荐系统的协作知识创建自适应轨迹采样器和综合奖励模型。

Result: 在两个真实数据集和两种不同GR骨干上的大量实验结果表明GFlowGR有效且鲁棒。

Conclusion: GFlowGR是缓解GR中曝光偏差问题的有前途方法。

Abstract: Generative recommendations (GR), which usually include item tokenizers and
generative Large Language Models (LLMs), have demonstrated remarkable success
across a wide range of scenarios. The majority of existing research efforts
primarily concentrate on developing powerful item tokenizers or advancing LLM
decoding strategies to attain superior performance. However, the critical
fine-tuning step in GR frameworks, which is essential for adapting LLMs to
recommendation data, remains largely unexplored. Current approaches
predominantly rely on either the next-token prediction loss of supervised
fine-tuning (SFT) or recommendationspecific direct preference optimization
(DPO) strategies. Both methods ignore the exploration of possible positive
unobserved samples, which is commonly referred to as the exposure bias problem.
To mitigate this problem, this paper treats the GR as a multi-step generation
task and constructs a GFlowNets-based fine-tuning framework (GFlowGR). The
proposed framework integrates collaborative knowledge from traditional
recommender systems to create an adaptive trajectory sampler and a
comprehensive reward model. Leveraging the diverse generation property of
GFlowNets, along with sampling and heuristic weighting techniques, GFlowGR
emerges as a promising approach to mitigate the exposure bias problem.
Extensive empirical results on two real-world datasets and with two different
GR backbones highlight the effectiveness and robustness of GFlowGR.

</details>


### [77] [Neural Prioritisation for Web Crawling](https://arxiv.org/abs/2506.16146)
*Francesza Pezzuti,Sean MacAvaney,Nicola Tonellotto*

Main category: cs.IR

TL;DR: 现有网页抓取技术适用于关键词搜索，但检索和用户行为正转向自然语言语义匹配，本文提出语义质量驱动的抓取优先级技术，实验表明该技术有优势，为网页抓取研究提供新视角。


<details>
  <summary>Details</summary>
Motivation: 现有抓取技术适用于关键词搜索，而检索和用户行为正转向自然语言语义匹配，为使抓取与自然语言搜索趋势对齐，提升抓取效果。

Method: 提出语义质量驱动的优先级技术，将语义理解嵌入抓取过程，利用神经语义质量估计器对抓取边界进行优先级排序。

Result: 在英语子集和查询集上实验表明，与现有技术相比，神经抓取策略在抓取早期显著提高收获率、maxNDCG和搜索效果，在关键词查询上也保持可比性能。

Conclusion: 虽未提出完整确定的解决方案，但为网页抓取提供前瞻性视角，开启利用语义分析使抓取与自然语言搜索对齐的新研究方向。

Abstract: Given the vast scale of the Web, crawling prioritisation techniques based on
link graph traversal, popularity, link analysis, and textual content are
frequently applied to surface documents that are most likely to be valuable.
While existing techniques are effective for keyword-based search, both
retrieval methods and user search behaviours are shifting from keyword-based
matching to natural language semantic matching. The remarkable success of
applying semantic matching and quality signals during ranking leads us to
hypothesize that crawling could be improved by prioritizing Web pages with high
semantic quality. To investigate this, we propose a semantic quality-driven
prioritisation technique to enhance the effectiveness of crawling and align the
crawler behaviour with recent shift towards natural language search. We embed
semantic understanding directly into the crawling process -- leveraging recent
neural semantic quality estimators to prioritise the crawling frontier -- with
the goal of surfacing content that is semantically rich and valuable for modern
search needs. Our experiments on the English subset of ClueWeb22-B and the
Researchy Questions query set show that, compared to existing crawling
techniques, neural crawling policies significantly improve harvest rate,
maxNDCG, and search effectiveness during the early stages of crawling.
Meanwhile, crawlers based on our proposed neural policies maintain comparable
search performance on keyword queries from the MS MARCO Web Search query set.
While this work does not propose a definitive and complete solution, it
presents a forward-looking perspective on Web crawling and opens the door to a
new line of research on leveraging semantic analysis to effectively align
crawlers with the ongoing shift toward natural language search.

</details>


### [78] [Revela: Dense Retriever Learning via Language Modeling](https://arxiv.org/abs/2506.16552)
*Fengyu Cai,Tong Chen,Xinran Zhao,Sihao Chen,Hongming Zhang,Sherry Tongshuang Wu,Iryna Gurevych,Heinz Koeppl*

Main category: cs.IR

TL;DR: 提出Revela框架用于自监督检索器学习，在通用和特定领域基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 训练密集检索器需要标注的查询 - 文档对，在专业领域获取成本高，因此探索自监督检索器学习。

Method: 引入Revela框架，通过批量内注意力机制对本地和跨文档上下文进行下一个词预测，用检索器计算的相似度得分加权注意力。

Result: 在通用（BEIR）和特定领域（CoIR）基准测试中，Revela在NDCG@10上分别比之前最佳方法有5.2%（相对18.3%）和5.6%（相对14.4%）的绝对提升。

Conclusion: Revela有效且可扩展，对自监督检索器学习有很大前景。

Abstract: Dense retrievers play a vital role in accessing external and specialized
knowledge to augment language models (LMs). Training dense retrievers typically
requires annotated query-document pairs, which are costly and hard to obtain in
specialized domains such as code-motivating growing interest in self-supervised
retriever learning. Since LMs are trained to capture token-level dependencies
through a self-supervised learning objective (i.e., next-token prediction), we
can analogously cast retrieval as learning dependencies among chunks of tokens.
This analogy naturally leads to the question: How can we adapt self-supervised
learning objectives in the spirit of language modeling to train retrievers?
  To answer this question, we introduce Revela, a unified and scalable training
framework for self-supervised retriever learning via language modeling. Revela
models semantic dependencies among documents by conditioning next-token
prediction on both local and cross-document context through an in-batch
attention mechanism. This attention is weighted by retriever-computed
similarity scores, enabling the retriever to be optimized as part of language
modeling. We evaluate Revela on both general-domain (BEIR) and domain-specific
(CoIR) benchmarks across various retriever backbones. At a comparable parameter
scale, Revela outperforms the previous best method with absolute improvements
of 5.2 % (18.3 % relative) and 5.6 % (14.4 % relative) on NDCG@10,
respectively, underscoring its effectiveness. Performance increases with model
size, highlighting both the scalability of our approach and its promise for
self-supervised retriever learning.

</details>


### [79] [A Simple Contrastive Framework Of Item Tokenization For Generative Recommendation](https://arxiv.org/abs/2506.16683)
*Penglong Zhai,Yifang Yuan,Fanyi Di,Jie Li,Yue Liu,Chen Li,Jie Huang,Sicong Wang,Yao Xu,Xin Li*

Main category: cs.IR

TL;DR: 提出SimCIT框架解决生成式检索推荐在大规模系统中的问题，实验证明其在LLM生成式推荐中有效。


<details>
  <summary>Details</summary>
Motivation: 现有生成式检索推荐在大规模系统中因token空间冗余和规模大而繁琐，重建量化与生成式检索任务目标冲突，且多模态信息难以集成到现有框架。

Method: 提出基于对比学习的无监督深度量化框架SimCIT，使用可学习的残差量化模块，结合多模态知识对齐和语义token化。

Result: 在公共数据集和大规模工业数据集上的广泛实验表明SimCIT有效。

Conclusion: SimCIT能有效应用于LLM-based生成式推荐。

Abstract: Generative retrieval-based recommendation has emerged as a promising paradigm
aiming at directly generating the identifiers of the target candidates.
However, in large-scale recommendation systems, this approach becomes
increasingly cumbersome due to the redundancy and sheer scale of the token
space. To overcome these limitations, recent research has explored the use of
semantic tokens as an alternative to ID tokens, which typically leveraged
reconstruction-based strategies, like RQ-VAE, to quantize content embeddings
and significantly reduce the embedding size. However, reconstructive
quantization aims for the precise reconstruction of each item embedding
independently, which conflicts with the goal of generative retrieval tasks
focusing more on differentiating among items. Moreover, multi-modal side
information of items, such as descriptive text and images, geographical
knowledge in location-based recommendation services, has been shown to be
effective in improving recommendations by providing richer contexts for
interactions. Nevertheless, effectively integrating such complementary
knowledge into existing generative recommendation frameworks remains
challenging. To overcome these challenges, we propose a novel unsupervised deep
quantization exclusively based on contrastive learning, named SimCIT (a Simple
Contrastive Item Tokenization framework). Specifically, different from existing
reconstruction-based strategies, SimCIT propose to use a learnable residual
quantization module to align with the signals from different modalities of the
items, which combines multi-modal knowledge alignment and semantic tokenization
in a mutually beneficial contrastive learning framework. Extensive experiments
across public datasets and a large-scale industrial dataset from various
domains demonstrate SimCIT's effectiveness in LLM-based generative
recommendation.

</details>


### [80] [eSapiens: A Real-World NLP Framework for Multimodal Document Understanding and Enterprise Knowledge Processing](https://arxiv.org/abs/2506.16768)
*Isaac Shi,Zeyuan Li,Wenli Wang,Lewei He,Yang Yang,Tianyu Shi*

Main category: cs.IR

TL;DR: 介绍企业级统一问答系统eSapiens，结合多技术实现对不同数据访问，评估表现超基线，提供可用框架。


<details>
  <summary>Details</summary>
Motivation: 设计适用于企业环境的统一问答系统，实现对结构化数据库和非结构化文本语料的自然语言访问。

Method: 采用双模块架构，结合Text - to - SQL规划器和混合RAG管道，RAG模块集成多种技术；在RAGTruth基准上用五个领先大语言模型评估。

Result: eSapiens在上下文相关性和生成质量上优于FAISS基线，有高风险场景控制选项。

Conclusion: 为现实企业应用提供了可靠、支持引用的问答框架。

Abstract: We introduce eSapiens, a unified question-answering system designed for
enterprise settings, which bridges structured databases and unstructured
textual corpora via a dual-module architecture. The system combines a
Text-to-SQL planner with a hybrid Retrieval-Augmented Generation (RAG)
pipeline, enabling natural language access to both relational data and
free-form documents. To enhance answer faithfulness, the RAG module integrates
dense and sparse retrieval, commercial reranking, and a citation verification
loop that ensures grounding consistency. We evaluate eSapiens on the RAGTruth
benchmark across five leading large language models (LLMs), analyzing
performance across key dimensions such as completeness, hallucination, and
context utilization. Results demonstrate that eSapiens outperforms a FAISS
baseline in contextual relevance and generation quality, with optional
strict-grounding controls for high-stakes scenarios. This work provides a
deployable framework for robust, citation-aware question answering in
real-world enterprise applications.

</details>


### [81] [Multi-Objective Recommendation in the Era of Generative AI: A Survey of Recent Progress and Future Prospects](https://arxiv.org/abs/2506.16893)
*Zihan Hong,Yushi Wu,Zhiting Zhao,Shanshan Feng,Jianghong Ma,Jiao Liu,Tianjun Wei*

Main category: cs.IR

TL;DR: 本文指出生成式AI使推荐系统更通用，但缺乏基于生成式AI的多目标推荐系统综合研究，为此调研现有研究，分类整理、总结评估指标和数据集并分析挑战与方向。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏基于生成式AI技术的多目标推荐系统的综合研究，存在文献空白，需进行调研填补。

Method: 编译基于生成式技术的多目标推荐系统的现有研究，按目标分类，总结相关评估指标和常用数据集。

Result: 完成对基于生成式AI的多目标推荐系统现有研究的分类整理，总结了评估指标和数据集。

Conclusion: 分析了基于生成式AI的多目标推荐系统领域的挑战和未来研究方向。

Abstract: With the recent progress in generative artificial intelligence (Generative
AI), particularly in the development of large language models, recommendation
systems are evolving to become more versatile. Unlike traditional techniques,
generative AI not only learns patterns and representations from complex data
but also enables content generation, data synthesis, and personalized
experiences. This generative capability plays a crucial role in the field of
recommendation systems, helping to address the issue of data sparsity and
improving the overall performance of recommendation systems. Numerous studies
on generative AI have already emerged in the field of recommendation systems.
Meanwhile, the current requirements for recommendation systems have surpassed
the single utility of accuracy, leading to a proliferation of multi-objective
research that considers various goals in recommendation systems. However, to
the best of our knowledge, there remains a lack of comprehensive studies on
multi-objective recommendation systems based on generative AI technologies,
leaving a significant gap in the literature. Therefore, we investigate the
existing research on multi-objective recommendation systems involving
generative AI to bridge this gap. We compile current research on
multi-objective recommendation systems based on generative techniques,
categorizing them by objectives. Additionally, we summarize relevant evaluation
metrics and commonly used datasets, concluding with an analysis of the
challenges and future directions in this domain.

</details>


### [82] [Pyramid Mixer: Multi-dimensional Multi-period Interest Modeling for Sequential Recommendation](https://arxiv.org/abs/2506.16942)
*Zhen Gong,Zhifang Fan,Hui Lu,Qiwei Chen,Chenbin Zhang,Lin Guan,Yuchao Zheng,Feng Zhang,Xiao Yang,Zuotao Liu*

Main category: cs.IR

TL;DR: 提出Pyramid Mixer顺序推荐模型，利用MLP - Mixer架构建模用户兴趣，实验证明有效且已部署到工业平台。


<details>
  <summary>Details</summary>
Motivation: 传统顺序推荐研究侧重跨行为建模，忽略多维度用户兴趣综合建模。

Method: 提出Pyramid Mixer模型，通过跨行为和跨特征用户序列建模学习综合用户兴趣，以金字塔方式堆叠混合层学习跨周期用户时间兴趣。

Result: 离线和在线实验证明方法有效，在线A/B测试中用户停留时长提升0.106%，活跃天数增加0.0113%。

Conclusion: Pyramid Mixer模型有效、高效，可扩展，能在现实应用中产生影响。

Abstract: Sequential recommendation, a critical task in recommendation systems,
predicts the next user action based on the understanding of the user's
historical behaviors. Conventional studies mainly focus on cross-behavior
modeling with self-attention based methods while neglecting comprehensive user
interest modeling for more dimensions. In this study, we propose a novel
sequential recommendation model, Pyramid Mixer, which leverages the MLP-Mixer
architecture to achieve efficient and complete modeling of user interests. Our
method learns comprehensive user interests via cross-behavior and cross-feature
user sequence modeling. The mixer layers are stacked in a pyramid way for
cross-period user temporal interest learning. Through extensive offline and
online experiments, we demonstrate the effectiveness and efficiency of our
method, and we obtain a +0.106% improvement in user stay duration and a
+0.0113% increase in user active days in the online A/B test. The Pyramid Mixer
has been successfully deployed on the industrial platform, demonstrating its
scalability and impact in real-world applications.

</details>


### [83] [RAGentA: Multi-Agent Retrieval-Augmented Generation for Attributed Question Answering](https://arxiv.org/abs/2506.16988)
*Ines Besrour,Jingbo He,Tobias Schreieder,Michael Färber*

Main category: cs.IR

TL;DR: 提出RAGentA框架用于归因问答，采用多智能体架构和混合检索策略，在合成数据集上优于标准RAG基线，证明架构和策略有效性。


<details>
  <summary>Details</summary>
Motivation: 以生成可信答案为目标，优化答案的正确性和忠实性。

Method: 使用多智能体架构迭代过滤文档、生成带引用答案并动态验证完整性，采用结合稀疏和密集方法的混合检索策略。

Result: 混合检索策略使Recall@20提高12.5%，RAGentA在正确性上提升1.09%，在忠实性上提升10.72%。

Conclusion: 多智能体架构和混合检索策略在推进可信问答方面有效。

Abstract: We present RAGentA, a multi-agent retrieval-augmented generation (RAG)
framework for attributed question answering (QA). With the goal of trustworthy
answer generation, RAGentA focuses on optimizing answer correctness, defined by
coverage and relevance to the question and faithfulness, which measures the
extent to which answers are grounded in retrieved documents. RAGentA uses a
multi-agent architecture that iteratively filters retrieved documents,
generates attributed answers with in-line citations, and verifies completeness
through dynamic refinement. Central to the framework is a hybrid retrieval
strategy that combines sparse and dense methods, improving Recall@20 by 12.5%
compared to the best single retrieval model, resulting in more correct and
well-supported answers. Evaluated on a synthetic QA dataset derived from the
FineWeb index, RAGentA outperforms standard RAG baselines, achieving gains of
1.09% in correctness and 10.72% in faithfulness. These results demonstrate the
effectiveness of the multi-agent architecture and hybrid retrieval in advancing
trustworthy QA.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [84] [Ignition Phase : Standard Training for Fast Adversarial Robustness](https://arxiv.org/abs/2506.15685)
*Wang Yu-Hang,Liu ying,Fang liang,Wang Xuelin,Junkang Guo,Shiwei Li,Lei Gao,Jian Liu,Wenfei Yin*

Main category: cs.LG

TL;DR: 提出对抗进化训练（AET）框架，在常规对抗训练前增加经验风险最小化（ERM）阶段，能快速获得相当或更好的鲁棒性，提高干净准确率并降低训练成本。


<details>
  <summary>Details</summary>
Motivation: 许多对抗训练变体主要关注更强的攻击生成，而忽略了基础特征表示，需要一种更高效的防御方法。

Method: 在常规对抗训练前增加ERM阶段，培养有利的特征流形。

Result: AET能更快速地获得相当或更好的鲁棒性，提高干净准确率，降低8 - 25%的训练成本，且在多个数据集、架构及增强现有对抗训练方法时都有效。

Conclusion: 强调通过标准训练进行特征预条件处理对开发更高效、有原则的鲁棒防御的重要性。

Abstract: Adversarial Training (AT) is a cornerstone defense, but many variants
overlook foundational feature representations by primarily focusing on stronger
attack generation. We introduce Adversarial Evolution Training (AET), a simple
yet powerful framework that strategically prepends an Empirical Risk
Minimization (ERM) phase to conventional AT. We hypothesize this initial ERM
phase cultivates a favorable feature manifold, enabling more efficient and
effective robustness acquisition. Empirically, AET achieves comparable or
superior robustness more rapidly, improves clean accuracy, and cuts training
costs by 8-25\%. Its effectiveness is shown across multiple datasets,
architectures, and when augmenting established AT methods. Our findings
underscore the impact of feature pre-conditioning via standard training for
developing more efficient, principled robust defenses. Code is available in the
supplementary material.

</details>


### [85] [Alternates, Assemble! Selecting Optimal Alternates for Citizens' Assemblies](https://arxiv.org/abs/2506.15716)
*Angelos Assos,Carmel Baharav,Bailey Flanigan,Ariel Procaccia*

Main category: cs.LG

TL;DR: 文章针对公民大会中代表人选流失导致组成失衡问题，提出优化候补人选选择框架，理论和实证表明该方法能提升代表性并减少候补人数。


<details>
  <summary>Details</summary>
Motivation: 公民大会小组代表性依赖人员组成，但常有成员退出导致组成失衡，现有方法未考虑候补人选选择。

Method: 引入优化框架，利用学习理论工具，结合历史数据估计退出概率，选择候补人员以最小化预期偏差。

Result: 建立了理论保证，包括样本复杂度和估计偏差损失的最坏情况边界；实证显示相比现状，该方法显著提升代表性且减少候补人数。

Conclusion: 提出的候补人选选择方法能有效解决公民大会人员流失导致的代表性问题。

Abstract: An increasingly influential form of deliberative democracy centers on
citizens' assemblies, where randomly selected people discuss policy questions.
The legitimacy of these panels hinges on their representation of the broader
population, but panelists often drop out, leading to an unbalanced composition.
Although participant attrition is mitigated in practice by alternates, their
selection is not taken into account by existing methods. To address this gap,
we introduce an optimization framework for alternate selection. Our algorithmic
approach, which leverages learning-theoretic machinery, estimates dropout
probabilities using historical data and selects alternates to minimize expected
misrepresentation. We establish theoretical guarantees for our approach,
including worst-case bounds on sample complexity (with implications for
computational efficiency) and on loss when panelists' probabilities of dropping
out are mis-estimated. Empirical evaluation using real-world data demonstrates
that, compared to the status quo, our method significantly improves
representation while requiring fewer alternates.

</details>


### [86] [Learning from M-Tuple Dominant Positive and Unlabeled Data](https://arxiv.org/abs/2506.15686)
*Jiahe Qin,Junpeng Li,Changchun Hua,Yana Yang*

Main category: cs.LG

TL;DR: 提出广义学习框架MDPU解决标签比例学习中获取精确监督信息困难的问题，推导无偏风险估计器并引入风险校正方法，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 实际应用中获取实例特定类比例的精确监督信息具有挑战性，为贴合现实场景并利用元组内实例比例约束。

Method: 对任意大小元组内实例分布进行数学建模，基于经验风险最小化方法推导满足风险一致性的无偏风险估计器，引入风险校正方法得到校正风险估计器。

Result: 无偏风险估计器的泛化误差界从理论上证明了方法的一致性，多数据集实验和与基线方法对比验证了学习框架的有效性。

Conclusion: 所提出的广义学习框架MDPU是有效的，能较好解决标签比例学习问题。

Abstract: Label Proportion Learning (LLP) addresses the classification problem where
multiple instances are grouped into bags and each bag contains information
about the proportion of each class. However, in practical applications,
obtaining precise supervisory information regarding the proportion of instances
in a specific class is challenging. To better align with real-world application
scenarios and effectively leverage the proportional constraints of instances
within tuples, this paper proposes a generalized learning framework
\emph{MDPU}. Specifically, we first mathematically model the distribution of
instances within tuples of arbitrary size, under the constraint that the number
of positive instances is no less than that of negative instances. Then we
derive an unbiased risk estimator that satisfies risk consistency based on the
empirical risk minimization (ERM) method. To mitigate the inevitable
overfitting issue during training, a risk correction method is introduced,
leading to the development of a corrected risk estimator. The generalization
error bounds of the unbiased risk estimator theoretically demonstrate the
consistency of the proposed method. Extensive experiments on multiple datasets
and comparisons with other relevant baseline methods comprehensively validate
the effectiveness of the proposed learning framework.

</details>


### [87] [S$^2$GPT-PINNs: Sparse and Small models for PDEs](https://arxiv.org/abs/2506.15687)
*Yajie Ji,Yanlai Chen,Shawn Koohy*

Main category: cs.LG

TL;DR: 提出S²GPT - PINN稀疏小模型求解参数偏微分方程，通过定制化实现高效求解。


<details>
  <summary>Details</summary>
Motivation: 寻找能高效求解参数偏微分方程且计算成本低的模型。

Method: 利用大的全阶模型支持的严格贪婪算法处理少量高质量数据，通过从预训练PINNs转移特定任务激活函数进行知识蒸馏，计算物理信息损失时进行明智下采样。

Result: S²GPT - PINN比PINNs所需参数少几个数量级，实现了极高效率。

Conclusion: S²GPT - PINN作为针对特定领域偏微分方程的模型，以紧凑架构和低计算成本有效求解参数偏微分方程。

Abstract: We propose S$^2$GPT-PINN, a sparse and small model for solving parametric
partial differential equations (PDEs). Similar to Small Language Models (SLMs),
S$^2$GPT-PINN is tailored to domain-specific (families of) PDEs and
characterized by its compact architecture and minimal computational power.
Leveraging a small amount of extremely high quality data via a mathematically
rigorous greedy algorithm that is enabled by the large full-order models,
S$^2$GPT-PINN relies on orders of magnitude less parameters than PINNs to
achieve extremely high efficiency via two levels of customizations. The first
is knowledge distillation via task-specific activation functions that are
transferred from Pre-Trained PINNs. The second is a judicious down-sampling
when calculating the physics-informed loss of the network compressing the
number of data sites by orders of magnitude to the size of the small model.

</details>


### [88] [Cellular Traffic Prediction via Deep State Space Models with Attention Mechanism](https://arxiv.org/abs/2506.15688)
*Hui Ma,Kai Yang,Man-On Pun*

Main category: cs.LG

TL;DR: 提出端到端框架预测蜂窝流量，结合CNN、注意力机制和卡尔曼滤波，利用辅助信息，实验表明模型预测精度超现有技术。


<details>
  <summary>Details</summary>
Motivation: 蜂窝流量高度动态且受多种外生因素影响，导致预测精度下降，需要提高预测精度以帮助运营商管理网络资源和决策。

Method: 提出端到端框架，使用带注意力机制的卷积神经网络捕捉空间动态，用卡尔曼滤波器进行时间建模，充分利用社会活动等辅助信息。

Result: 在三个真实世界数据集上进行大量实验，提出的模型在预测精度上优于现有机器学习技术。

Conclusion: 提出的模型能有效捕捉蜂窝流量的时空模式，提高预测精度。

Abstract: Cellular traffic prediction is of great importance for operators to manage
network resources and make decisions. Traffic is highly dynamic and influenced
by many exogenous factors, which would lead to the degradation of traffic
prediction accuracy. This paper proposes an end-to-end framework with two
variants to explicitly characterize the spatiotemporal patterns of cellular
traffic among neighboring cells. It uses convolutional neural networks with an
attention mechanism to capture the spatial dynamics and Kalman filter for
temporal modelling. Besides, we can fully exploit the auxiliary information
such as social activities to improve prediction performance. We conduct
extensive experiments on three real-world datasets. The results show that our
proposed models outperform the state-of-the-art machine learning techniques in
terms of prediction accuracy.

</details>


### [89] [Optimal Online Bookmaking for Any Number of Outcomes](https://arxiv.org/abs/2506.16253)
*Hadar Tal,Oron Sabag*

Main category: cs.LG

TL;DR: 研究在线博彩问题，给出庄家最优损失，开发高效算法计算最优策略，关键是刻画Bellman - Pareto前沿。


<details>
  <summary>Details</summary>
Motivation: 研究在线博彩中庄家如何动态更新赔率，以最大化利润并降低潜在损失。

Method: 对Bellman - Pareto前沿进行显式刻画，统一动态规划更新与多准则优化框架。

Result: 庄家最优损失是简单多项式的最大根，开发的算法在面对最优和次优赌徒时都有良好表现。

Conclusion: 庄家能在避免金融风险的同时保证公平性，揭示了庄家遗憾与Hermite多项式的关系。

Abstract: We study the Online Bookmaking problem, where a bookmaker dynamically updates
betting odds on the possible outcomes of an event. In each betting round, the
bookmaker can adjust the odds based on the cumulative betting behavior of
gamblers, aiming to maximize profit while mitigating potential loss. We show
that for any event and any number of betting rounds, in a worst-case setting
over all possible gamblers and outcome realizations, the bookmaker's optimal
loss is the largest root of a simple polynomial. Our solution shows that
bookmakers can be as fair as desired while avoiding financial risk, and the
explicit characterization reveals an intriguing relation between the
bookmaker's regret and Hermite polynomials. We develop an efficient algorithm
that computes the optimal bookmaking strategy: when facing an optimal gambler,
the algorithm achieves the optimal loss, and in rounds where the gambler is
suboptimal, it reduces the achieved loss to the optimal opportunistic loss, a
notion that is related to subgame perfect Nash equilibrium. The key technical
contribution to achieve these results is an explicit characterization of the
Bellman-Pareto frontier, which unifies the dynamic programming updates for
Bellman's value function with the multi-criteria optimization framework of the
Pareto frontier in the context of vector repeated games.

</details>


### [90] [BASE-Q: Bias and Asymmetric Scaling Enhanced Rotational Quantization for Large Language Models](https://arxiv.org/abs/2506.15689)
*Liulu He,Shenli Zhen,Karwei Sun,Yijiang Liu,Yufei Zhao,Chongkang Tan,Huanrui Yang,Yuan Du,Li Du*

Main category: cs.LG

TL;DR: 指出当前旋转量化方法局限，提出BASE - Q方法结合偏置校正与非对称缩放，实验证明其有效，缩小与全精度模型精度差距。


<details>
  <summary>Details</summary>
Motivation: 当前旋转量化方法优化旋转参数性能增益有限且训练开销大，存在未对齐通道均值和增加裁剪误差能量损失等问题。

Method: 提出BASE - Q方法，结合偏置校正和非对称缩放，支持分块优化。

Result: 在多种大语言模型和基准测试中，与QuaRot、SpinQuant、OSTQuant相比，分别将与全精度模型的精度差距缩小50.5%、42.9%和29.2%。

Conclusion: BASE - Q方法能有效减少舍入和裁剪误差，避免高内存消耗的全模型反向传播，效果显著。

Abstract: Rotations have become essential to state-of-the-art quantization pipelines
for large language models (LLMs) by effectively smoothing outliers in weights
and activations. However, further optimizing the rotation parameters offers
only limited performance gains and introduces significant training overhead:
due to rotation parameter sharing, full-model must be loaded simultaneously to
enable backpropagation, resulting in substantial memory consumption and limited
practical utility. In this work, we identify two fundamental limitations of
current rotational quantization methods: (i) rotation fails to align channel
means, resulting in wider quantization bounds and increased rounding errors;
and (ii) rotation makes the activation distribution more Gaussian-like,
increasing energy loss caused by clipping errors. To address these issues, we
introduce \textbf{BASE-Q}, a simple yet powerful approach that combines bias
correction and asymmetric scaling to effectively reduce rounding and clipping
errors. Furthermore, BASE-Q enables blockwise optimization, eliminating the
need for memory-intensive full-model backpropagation. Extensive experiments on
various LLMs and benchmarks demonstrate the effectiveness of BASE-Q, narrowing
the accuracy gap to full-precision models by 50.5\%, 42.9\%, and 29.2\%
compared to QuaRot, SpinQuant, and OSTQuant, respectively. The code will be
released soon.

</details>


### [91] [Optimism Without Regularization: Constant Regret in Zero-Sum Games](https://arxiv.org/abs/2506.16736)
*John Lazarsfeld,Georgios Piliouras,Ryann Sim,Stratis Skoulakis*

Main category: cs.LG

TL;DR: 研究二人零和博弈中乐观版虚拟博弈，首次证明无正则化下也能达最优速率，还给出交替虚拟博弈后悔下界。


<details>
  <summary>Details</summary>
Motivation: 探索无正则化下在二人零和博弈学习中达到最优后悔率的方法，验证非无悔算法的快速学习能力。

Method: 利用收益向量对偶空间中乐观虚拟博弈的几何视角，证明迭代的某个能量函数随时间有界。

Result: 证明两策略博弈中乐观虚拟博弈只有常数后悔，给出交替虚拟博弈后悔下界为Ω(√T)。

Conclusion: 在无正则化情况下，乐观策略和交替策略在实现o(√T)后悔上能力有差异。

Abstract: This paper studies the optimistic variant of Fictitious Play for learning in
two-player zero-sum games. While it is known that Optimistic FTRL -- a
regularized algorithm with a bounded stepsize parameter -- obtains constant
regret in this setting, we show for the first time that similar, optimal rates
are also achievable without regularization: we prove for two-strategy games
that Optimistic Fictitious Play (using any tiebreaking rule) obtains only
constant regret, providing surprising new evidence on the ability of
non-no-regret algorithms for fast learning in games. Our proof technique
leverages a geometric view of Optimistic Fictitious Play in the dual space of
payoff vectors, where we show a certain energy function of the iterates remains
bounded over time. Additionally, we also prove a regret lower bound of
$\Omega(\sqrt{T})$ for Alternating Fictitious Play. In the unregularized
regime, this separates the ability of optimism and alternation in achieving
$o(\sqrt{T})$ regret.

</details>


### [92] [LLM Web Dynamics: Tracing Model Collapse in a Network of LLMs](https://arxiv.org/abs/2506.15690)
*Tianyu Wang,Lingyou Pang,Akira Horiguchi,Carey E. Priebe*

Main category: cs.LG

TL;DR: 引入LLM Web Dynamics (LWD)框架研究大语言模型在网络层面的崩溃问题，并提供理论保证。


<details>
  <summary>Details</summary>
Motivation: 公开互联网合成数据提升大语言模型训练数据使用效率，但模型崩溃潜在威胁研究不足，现有研究多在单模型设置或仅依赖统计替代。

Method: 引入LWD框架，用检索增强生成 (RAG) 数据库模拟互联网，分析模型输出收敛模式，并类比交互高斯混合模型提供理论保证。

Result: 未明确提及具体结果。

Conclusion: 未明确提及具体结论。

Abstract: The increasing use of synthetic data from the public Internet has enhanced
data usage efficiency in large language model (LLM) training. However, the
potential threat of model collapse remains insufficiently explored. Existing
studies primarily examine model collapse in a single model setting or rely
solely on statistical surrogates. In this work, we introduce LLM Web Dynamics
(LWD), an efficient framework for investigating model collapse at the network
level. By simulating the Internet with a retrieval-augmented generation (RAG)
database, we analyze the convergence pattern of model outputs. Furthermore, we
provide theoretical guarantees for this convergence by drawing an analogy to
interacting Gaussian Mixture Models.

</details>


### [93] [What Do Latent Action Models Actually Learn?](https://arxiv.org/abs/2506.15691)
*Chuheng Zhang,Tim Pearce,Pushi Zhang,Kaixin Wang,Xiaoyu Chen,Wei Shen,Li Zhao,Jiang Bian*

Main category: cs.LG

TL;DR: 本文分析潜在动作模型（LAM）中潜在变量捕获动作变化还是无关噪声的问题，提出线性模型并给出见解和模拟结果。


<details>
  <summary>Details</summary>
Motivation: 解决LAM中潜在变量可能捕获无关噪声的问题，分析其本质。

Method: 提出线性模型来封装LAM学习的本质，进行数值模拟。

Result: 得到LAM与主成分分析（PCA）的联系、数据生成策略的要求，以及数据增强等策略的合理性，并给出模拟结果。

Conclusion: 明确影响LAM学习的数据中观测、动作和噪声的具体结构。

Abstract: Latent action models (LAMs) aim to learn action-relevant changes from
unlabeled videos by compressing changes between frames as latents. However,
differences between video frames can be caused by controllable changes as well
as exogenous noise, leading to an important concern -- do latents capture the
changes caused by actions or irrelevant noise? This paper studies this issue
analytically, presenting a linear model that encapsulates the essence of LAM
learning, while being tractable.This provides several insights, including
connections between LAM and principal component analysis (PCA), desiderata of
the data-generating policy, and justification of strategies to encourage
learning controllable changes using data augmentation, data cleaning, and
auxiliary action-prediction. We also provide illustrative results based on
numerical simulation, shedding light on the specific structure of observations,
actions, and noise in data that influence LAM learning.

</details>


### [94] [MLE-STAR: Machine Learning Engineering Agent via Search and Targeted Refinement](https://arxiv.org/abs/2506.15692)
*Jaehyun Nam,Jinsung Yoon,Jiefeng Chen,Jinwoo Shin,Sercan Ö. Arık,Tomas Pfister*

Main category: cs.LG

TL;DR: 提出构建MLE代理的新方法MLE - STAR，结合外部知识与迭代细化策略，在Kaggle竞赛中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有构建基于大语言模型的机器学习工程代理的方法依赖LLM固有知识且探索策略粗糙，限制模型选择和特定组件深度探索能力。

Method: MLE - STAR先利用搜索引擎获取外部知识形成初始解决方案，再针对特定机器学习组件迭代细化，通过消融研究指导探索，还引入了新的集成方法。

Result: MLE - STAR在MLE - bench的Kaggle竞赛中44%获得奖牌，显著优于最佳替代方案。

Conclusion: MLE - STAR是一种有效的构建MLE代理的方法，能提升机器学习工程中模型选择和组件探索能力。

Abstract: Agents based on large language models (LLMs) for machine learning engineering
(MLE) can automatically implement ML models via code generation. However,
existing approaches to build such agents often rely heavily on inherent LLM
knowledge and employ coarse exploration strategies that modify the entire code
structure at once. This limits their ability to select effective task-specific
models and perform deep exploration within specific components, such as
experimenting extensively with feature engineering options. To overcome these,
we propose MLE-STAR, a novel approach to build MLE agents. MLE-STAR first
leverages external knowledge by using a search engine to retrieve effective
models from the web, forming an initial solution, then iteratively refines it
by exploring various strategies targeting specific ML components. This
exploration is guided by ablation studies analyzing the impact of individual
code blocks. Furthermore, we introduce a novel ensembling method using an
effective strategy suggested by MLE-STAR. Our experimental results show that
MLE-STAR achieves medals in 44% of the Kaggle competitions on the MLE-bench,
significantly outperforming the best alternative.

</details>


### [95] [Verifiable Safety Q-Filters via Hamilton-Jacobi Reachability and Multiplicative Q-Networks](https://arxiv.org/abs/2506.15693)
*Jiaxing Li,Hanjiang Hu,Yujie Yang,Changliu Liu*

Main category: cs.LG

TL;DR: 提出基于Hamilton - Jacobi可达性分析的可验证无模型安全过滤器，在四个基准测试中合成形式验证的无模型安全证书。


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的安全过滤器缺乏形式化安全保证。

Method: 扩展Q值函数的可验证自一致性属性，提出乘法Q网络结构缓解零子水平集收缩问题，开发能有效验证自一致性属性的验证流程。

Result: 在四个标准安全控制基准测试中成功合成形式验证的无模型安全证书。

Conclusion: 所提出的基于Hamilton - Jacobi可达性分析的可验证无模型安全过滤器是有效的。

Abstract: Recent learning-based safety filters have outperformed conventional methods,
such as hand-crafted Control Barrier Functions (CBFs), by effectively adapting
to complex constraints. However, these learning-based approaches lack formal
safety guarantees. In this work, we introduce a verifiable model-free safety
filter based on Hamilton-Jacobi reachability analysis. Our primary
contributions include: 1) extending verifiable self-consistency properties for
Q value functions, 2) proposing a multiplicative Q-network structure to
mitigate zero-sublevel-set shrinkage issues, and 3) developing a verification
pipeline capable of soundly verifying these self-consistency properties. Our
proposed approach successfully synthesizes formally verified, model-free safety
certificates across four standard safe-control benchmarks.

</details>


### [96] [PNCS:Power-Norm Cosine Similarity for Diverse Client Selection in Federated Learning](https://arxiv.org/abs/2506.15923)
*Liangyan Li,Yangyi Liu,Yimo Ning,Stefano Rini,Jun Chen*

Main category: cs.LG

TL;DR: 提出利用PNCS的联邦学习框架改进客户端选择，用简单算法确保客户端多样性，实验显示优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习方法未考虑远程客户端间复杂梯度相关性，在数据异构场景有局限。

Method: 提出利用Power - Norm Cosine Similarity (PNCS)的联邦学习框架，引入通过选择历史队列确保客户端多样化选择的简单算法。

Result: 使用VGG16模型在不同数据分区上的实验显示，该方法相比现有方法有持续改进。

Conclusion: 所提方法能解决非IID数据挑战，提高收敛速度和准确性。

Abstract: Federated Learning (FL) has emerged as a powerful paradigm for leveraging
diverse datasets from multiple sources while preserving data privacy by
avoiding centralized storage. However, many existing approaches fail to account
for the intricate gradient correlations between remote clients, a limitation
that becomes especially problematic in data heterogeneity scenarios. In this
work, we propose a novel FL framework utilizing Power-Norm Cosine Similarity
(PNCS) to improve client selection for model aggregation. By capturing
higher-order gradient moments, PNCS addresses non-IID data challenges,
enhancing convergence speed and accuracy. Additionally, we introduce a simple
algorithm ensuring diverse client selection through a selection history queue.
Experiments with a VGG16 model across varied data partitions demonstrate
consistent improvements over state-of-the-art methods.

</details>


### [97] [Development of a Multiprocessing Interface Genetic Algorithm for Optimising a Multilayer Perceptron for Disease Prediction](https://arxiv.org/abs/2506.15694)
*Iliyas Ibrahim Iliyas,Souley Boukari,Abdulsalam Yau Gital*

Main category: cs.LG

TL;DR: 本文提出集成非线性特征提取、分类和优化的框架，经三个数据集验证，在疾病预测上取得高准确率并减少调参时间。


<details>
  <summary>Details</summary>
Motivation: 解决疾病预测问题，提升分类准确率并减少模型调参时间。

Method: 用带径向基函数核的核主成分分析降维，多层感知器预测疾病状态，改进的多进程遗传算法并行优化多层感知器超参数。

Result: 在三个数据集上，经MIGA调优的MLP分别对乳腺癌、帕金森病和慢性肾病达到99.12%、94.87%和100%的准确率，优于其他方法，且调参时间减少约60%。

Conclusion: 该框架有效提升疾病分类准确率，减少调参时间，优于传统方法。

Abstract: This study introduces a framework that integrates nonlinear feature
extraction, classification, and efficient optimization. First, kernel principal
component analysis with a radial basis function kernel reduces dimensionality
while preserving 95% of the variance. Second, a multilayer perceptron (MLP)
learns to predict disease status. Finally, a modified multiprocessing genetic
algorithm (MIGA) optimizes MLP hyperparameters in parallel over ten
generations. We evaluated this approach on three datasets: the Wisconsin
Diagnostic Breast Cancer dataset, the Parkinson's Telemonitoring dataset, and
the chronic kidney disease dataset. The MLP tuned by the MIGA achieved the best
accuracy of 99.12% for breast cancer, 94.87% for Parkinson's disease, and 100%
for chronic kidney disease. These results outperform those of other methods,
such as grid search, random search, and Bayesian optimization. Compared with a
standard genetic algorithm, kernel PCA revealed nonlinear relationships that
improved classification, and the MIGA's parallel fitness evaluations reduced
the tuning time by approximately 60%. The genetic algorithm incurs high
computational cost from sequential fitness evaluations, but our multiprocessing
interface GA (MIGA) parallelizes this step, slashing the tuning time and
steering the MLP toward the best accuracy score of 99.12%, 94.87%, and 100% for
breast cancer, Parkinson's disease, and CKD, respectively.

</details>


### [98] [From Data to Decision: Data-Centric Infrastructure for Reproducible ML in Collaborative eScience](https://arxiv.org/abs/2506.16051)
*Zhiwei Li,Carl Kesselman,Tran Huy Nguyen,Benjamin Yixing Xu,Kyle Bolo,Kimberley Yu*

Main category: cs.LG

TL;DR: 本文提出以数据为中心的生命周期感知可重复性框架，通过青光眼检测用例展示其可支持迭代探索、提高可重复性。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习工作流动态且碎片化，阻碍了实验的透明度、可重复性和适应性。

Method: 引入以六个结构化工件（数据集、特征、工作流、执行、资产和受控词汇表）为核心的数据中心框架，形式化数据、代码和决策之间的关系。

Result: 通过青光眼检测临床机器学习用例展示系统支持迭代探索，提高可重复性，保留协作决策的来源。

Conclusion: 所提出的数据中心框架能有效解决机器学习可重复性问题，适用于整个机器学习生命周期。

Abstract: Reproducibility remains a central challenge in machine learning (ML),
especially in collaborative eScience projects where teams iterate over data,
features, and models. Current ML workflows are often dynamic yet fragmented,
relying on informal data sharing, ad hoc scripts, and loosely connected tools.
This fragmentation impedes transparency, reproducibility, and the adaptability
of experiments over time. This paper introduces a data-centric framework for
lifecycle-aware reproducibility, centered around six structured artifacts:
Dataset, Feature, Workflow, Execution, Asset, and Controlled Vocabulary. These
artifacts formalize the relationships between data, code, and decisions,
enabling ML experiments to be versioned, interpretable, and traceable over
time. The approach is demonstrated through a clinical ML use case of glaucoma
detection, illustrating how the system supports iterative exploration, improves
reproducibility, and preserves the provenance of collaborative decisions across
the ML lifecycle.

</details>


### [99] [SimuGen: Multi-modal Agentic Framework for Constructing Block Diagram-Based Simulation Models](https://arxiv.org/abs/2506.15695)
*Xinxing Ren,Qianbo Zang,Zekun Guo*

Main category: cs.LG

TL;DR: LLMs在生成Simulink模型方面有困难，提出SimuGen框架解决该问题，代码开源。


<details>
  <summary>Details</summary>
Motivation: LLMs在模拟领域尤其是生成Simulink模型表现不佳，预训练缺乏特定数据。

Method: 提出SimuGen多模态基于代理的框架，协调多个专业代理，结合领域知识库。

Result: 能自动生成准确的Simulink仿真代码。

Conclusion: 协作和模块化设计使Simulink仿真生成可解释、健壮和可重复。

Abstract: Recent advances in large language models (LLMs) have shown impressive
performance in mathematical reasoning and code generation. However, LLMs still
struggle in the simulation domain, particularly in generating Simulink models,
which are essential tools in engineering and scientific research. Our
preliminary experiments indicate that LLM agents often fail to produce reliable
and complete Simulink simulation code from text-only inputs, likely due to the
lack of Simulink-specific data in their pretraining. To address this challenge,
we propose SimuGen, a multimodal agent-based framework that automatically
generates accurate Simulink simulation code by leveraging both the visual
Simulink diagram and domain knowledge. SimuGen coordinates several specialized
agents, including an investigator, unit test reviewer, code generator,
executor, debug locator, and report writer, supported by a domain-specific
knowledge base. This collaborative and modular design enables interpretable,
robust, and reproducible Simulink simulation generation. Our source code is
publicly available at https://github.com/renxinxing123/SimuGen_beta.

</details>


### [100] [A Distributional-Lifting Theorem for PAC Learning](https://arxiv.org/abs/2506.16651)
*Guy Blanc,Jane Lange,Carmen Strassle,Li-Yang Tan*

Main category: cs.LG

TL;DR: 论文证明分布提升定理，指出前人方法信息论上的难题，提出新方法在标准PAC模型工作且有更多优势。


<details>
  <summary>Details</summary>
Motivation: 解决无分布的PAC学习效率难题，突破分布特定学习的局限性。

Method: 证明分布提升定理，分析前人方法问题，提出新方法避开学习目标分布。

Result: 证明前人方法仅靠随机样本信息论上不可行，提出新的提升器。

Conclusion: 新方法可在标准PAC模型工作，适用于所有基础分布族，保留学习者的噪声容忍度，样本复杂度更好且更简单。

Abstract: The apparent difficulty of efficient distribution-free PAC learning has led
to a large body of work on distribution-specific learning. Distributional
assumptions facilitate the design of efficient algorithms but also limit their
reach and relevance. Towards addressing this, we prove a distributional-lifting
theorem: This upgrades a learner that succeeds with respect to a limited
distribution family $\mathcal{D}$ to one that succeeds with respect to any
distribution $D^\star$, with an efficiency overhead that scales with the
complexity of expressing $D^\star$ as a mixture of distributions in
$\mathcal{D}$.
  Recent work of Blanc, Lange, Malik, and Tan considered the special case of
lifting uniform-distribution learners and designed a lifter that uses a
conditional sample oracle for $D^\star$, a strong form of access not afforded
by the standard PAC model. Their approach, which draws on ideas from
semi-supervised learning, first learns $D^\star$ and then uses this information
to lift.
  We show that their approach is information-theoretically intractable with
access only to random examples, thereby giving formal justification for their
use of the conditional sample oracle. We then take a different approach that
sidesteps the need to learn $D^\star$, yielding a lifter that works in the
standard PAC model and enjoys additional advantages: it works for all base
distribution families, preserves the noise tolerance of learners, has better
sample complexity, and is simpler.

</details>


### [101] [CoC: Chain-of-Cancer based on Cross-Modal Autoregressive Traction for Survival Prediction](https://arxiv.org/abs/2506.15696)
*Haipeng Zhou,Sicheng Yang,Sihan Yang,Jing Qin,Lei Chen,Lei Zhu*

Main category: cs.LG

TL;DR: 本文首次使用四种模态（三种临床模态和语言）进行癌症患者生存预测，提出CoC框架，在五个公开癌症数据集上验证方法有效。


<details>
  <summary>Details</summary>
Motivation: 现有生存预测方法主要依赖病理和基因组数据，从癌症发病机制看，表观遗传变化（如甲基化数据）很重要，且以往未利用文本描述指导预测。

Method: 受Chain-of-Thought启发，提出Chain-of-Cancer框架，关注内部学习和交互学习，将临床数据编码为原始特征用于内部学习，用语言提示原始特征并引入自回归相互牵引模块进行协同表示，促进多模态联合学习。

Result: 在五个公开癌症数据集上进行评估，实验验证了方法和设计的有效性，取得了最优结果。

Conclusion: 所提方法和框架有效，可用于癌症患者生存预测，代码将发布。

Abstract: Survival prediction aims to evaluate the risk level of cancer patients.
Existing methods primarily rely on pathology and genomics data, either
individually or in combination. From the perspective of cancer pathogenesis,
epigenetic changes, such as methylation data, could also be crucial for this
task. Furthermore, no previous endeavors have utilized textual descriptions to
guide the prediction. To this end, we are the first to explore the use of four
modalities, including three clinical modalities and language, for conducting
survival prediction. In detail, we are motivated by the Chain-of-Thought (CoT)
to propose the Chain-of-Cancer (CoC) framework, focusing on intra-learning and
inter-learning. We encode the clinical data as the raw features, which remain
domain-specific knowledge for intra-learning. In terms of inter-learning, we
use language to prompt the raw features and introduce an Autoregressive Mutual
Traction module for synergistic representation. This tailored framework
facilitates joint learning among multiple modalities. Our approach is evaluated
across five public cancer datasets, and extensive experiments validate the
effectiveness of our methods and proposed designs, leading to producing \sota
results. Codes will be released.

</details>


### [102] [Relational Deep Learning: Challenges, Foundations and Next-Generation Architectures](https://arxiv.org/abs/2506.16654)
*Vijay Prakash Dwivedi,Charilaos Kanatsoulis,Shenyang Huang,Jure Leskovec*

Main category: cs.LG

TL;DR: 本文对关系深度学习（RDL）进行全面综述，介绍数据库到图的表示、相关基准数据集，讨论挑战、方法进展，探索统一挑战的机会及对关系数据处理的变革。


<details>
  <summary>Details</summary>
Motivation: 图机器学习发展迅速，但关系数据库数据构建的关系实体图有独特性质，需对基于其的RDL进行全面综述。

Method: 先介绍关系数据库到关系实体图的表示，再回顾用于开发和评估基于GNN的RDL模型的基准数据集，讨论挑战、调研方法和架构进展。

Result: 梳理了RDL的相关内容，明确关键挑战，介绍了相关方法和架构进展。

Conclusion: RDL有机会统一建模挑战，推动图机器学习多子领域向基础模型设计发展，变革关系数据处理。

Abstract: Graph machine learning has led to a significant increase in the capabilities
of models that learn on arbitrary graph-structured data and has been applied to
molecules, social networks, recommendation systems, and transportation, among
other domains. Data in multi-tabular relational databases can also be
constructed as 'relational entity graphs' for Relational Deep Learning (RDL) -
a new blueprint that enables end-to-end representation learning without
traditional feature engineering. Compared to arbitrary graph-structured data,
relational entity graphs have key properties: (i) their structure is defined by
primary-foreign key relationships between entities in different tables, (ii)
the structural connectivity is a function of the relational schema defining a
database, and (iii) the graph connectivity is temporal and heterogeneous in
nature. In this paper, we provide a comprehensive review of RDL by first
introducing the representation of relational databases as relational entity
graphs, and then reviewing public benchmark datasets that have been used to
develop and evaluate recent GNN-based RDL models. We discuss key challenges
including large-scale multi-table integration and the complexities of modeling
temporal dynamics and heterogeneous data, while also surveying foundational
neural network methods and recent architectural advances specialized for
relational entity graphs. Finally, we explore opportunities to unify these
distinct modeling challenges, highlighting how RDL converges multiple
sub-fields in graph machine learning towards the design of foundation models
that can transform the processing of relational data.

</details>


### [103] [Global Context-aware Representation Learning for Spatially Resolved Transcriptomics](https://arxiv.org/abs/2506.15698)
*Yunhak Oh,Junseok Lee,Yeongmin Kim,Sangwoo Seo,Namkyeong Lee,Chanyoung Park*

Main category: cs.LG

TL;DR: 提出Spotscape框架解决现有图基方法在获取有意义斑点表示上的不足，实验证明其在下游任务中表现优越。


<details>
  <summary>Details</summary>
Motivation: 现有图基方法在获取有意义斑点表示上存在不足，尤其在空间域边界附近的斑点。

Method: 提出Similarity Telescope模块捕捉多个斑点间的全局关系，提出相似性缩放策略调节切片内和切片间斑点的距离，实现多切片有效集成。

Result: 广泛实验证明Spotscape在单切片和多切片等各种下游任务中具有优越性。

Conclusion: Spotscape能有效解决现有方法的问题，在下游任务中有良好表现，代码开源。

Abstract: Spatially Resolved Transcriptomics (SRT) is a cutting-edge technique that
captures the spatial context of cells within tissues, enabling the study of
complex biological networks. Recent graph-based methods leverage both gene
expression and spatial information to identify relevant spatial domains.
However, these approaches fall short in obtaining meaningful spot
representations, especially for spots near spatial domain boundaries, as they
heavily emphasize adjacent spots that have minimal feature differences from an
anchor node. To address this, we propose Spotscape, a novel framework that
introduces the Similarity Telescope module to capture global relationships
between multiple spots. Additionally, we propose a similarity scaling strategy
to regulate the distances between intra- and inter-slice spots, facilitating
effective multi-slice integration. Extensive experiments demonstrate the
superiority of Spotscape in various downstream tasks, including single-slice
and multi-slice scenarios. Our code is available at the following link: https:
//github.com/yunhak0/Spotscape.

</details>


### [104] [Vision-Guided Chunking Is All You Need: Enhancing RAG with Multimodal Document Understanding](https://arxiv.org/abs/2506.16035)
*Vishesh Tripathi,Tanmay Odapally,Indraneel Das,Uday Allu,Biddwan Ahmed*

Main category: cs.LG

TL;DR: 提出一种基于大模态模型的多模态文档分块方法，在PDF文档数据集上评估显示能提升分块质量和下游RAG性能。


<details>
  <summary>Details</summary>
Motivation: 传统基于文本的分块方法难以处理复杂文档结构、多页表格等问题。

Method: 利用大模态模型批量处理PDF文档，以可配置的页面批次处理文档并保留跨批次上下文。

Result: 在精心策划的PDF文档数据集上评估，该方法在分块质量和下游RAG性能上有提升，相比传统RAG系统精度更高。

Conclusion: 该视觉引导方法能更好地保留文档结构和语义连贯性。

Abstract: Retrieval-Augmented Generation (RAG) systems have revolutionized information
retrieval and question answering, but traditional text-based chunking methods
struggle with complex document structures, multi-page tables, embedded figures,
and contextual dependencies across page boundaries. We present a novel
multimodal document chunking approach that leverages Large Multimodal Models
(LMMs) to process PDF documents in batches while maintaining semantic coherence
and structural integrity. Our method processes documents in configurable page
batches with cross-batch context preservation, enabling accurate handling of
tables spanning multiple pages, embedded visual elements, and procedural
content. We evaluate our approach on a curated dataset of PDF documents with
manually crafted queries, demonstrating improvements in chunk quality and
downstream RAG performance. Our vision-guided approach achieves better accuracy
compared to traditional vanilla RAG systems, with qualitative analysis showing
superior preservation of document structure and semantic coherence.

</details>


### [105] [BLUR: A Benchmark for LLM Unlearning Robust to Forget-Retain Overlap](https://arxiv.org/abs/2506.15699)
*Shengyuan Hu,Neil Kale,Pratiksha Thaker,Yiwei Fu,Steven Wu,Virginia Smith*

Main category: cs.LG

TL;DR: 本文指出当前大语言模型去学习基准存在问题，提出新基准BLUR，评估发现现有方法性能显著下降，强调了鲁棒评估的重要性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型去学习基准中遗忘集和保留集差异大，会让模型部署后易受攻击，不能真实反映去学习方法有效性，需要改进。

Method: 提出新的大语言模型去学习基准BLUR，提供更真实的遗忘 - 保留重叠场景、扩展评估任务、组合查询和不同难度的再学习数据集。

Result: 在BLUR上评估时，现有方法性能显著下降，简单方法平均表现优于新方法。

Conclusion: 强调了鲁棒评估的重要性，并指出未来研究的重要方向，且公开了基准。

Abstract: Machine unlearning has the potential to improve the safety of large language
models (LLMs) by removing sensitive or harmful information post hoc. A key
challenge in unlearning involves balancing between forget quality (effectively
unlearning undesirable information) and retain quality (maintaining good
performance on other, general tasks). Unfortunately, as we show, current LLM
unlearning benchmarks contain highly disparate forget and retain sets --
painting a false picture of the effectiveness of LLM unlearning methods. This
can be particularly problematic because it opens the door for benign
perturbations, such as relearning attacks, to easily reveal supposedly
unlearned knowledge once models are deployed. To address this, we present
$\texttt{BLUR}$: a benchmark for LLM unlearning that provides more realistic
scenarios of forget-retain overlap. $\texttt{BLUR}$ significantly expands on
existing unlearning benchmarks by providing extended evaluation tasks, combined
forget/retain queries, and relearning datasets of varying degrees of
difficulty. Despite the benign nature of the queries considered, we find that
the performance of existing methods drops significantly when evaluated on
$\texttt{BLUR}$, with simple approaches performing better on average than more
recent methods. These results highlight the importance of robust evaluation and
suggest several important directions of future study. Our benchmark is publicly
available at: https://huggingface.co/datasets/forgelab/BLUR

</details>


### [106] [Contraction Actor-Critic: Contraction Metric-Guided Reinforcement Learning for Robust Path Tracking](https://arxiv.org/abs/2506.15700)
*Minjae Cho,Hiroyasu Tsukamoto,Huy Trong Tran*

Main category: cs.LG

TL;DR: 本文提出将控制收缩度量（CCMs）集成到强化学习（RL）中，提出收缩演员 - 评论家（CAC）算法，实验证明其有效性并给出理论依据。


<details>
  <summary>Details</summary>
Motivation: 现有CCMs合成的控制器缺乏最优性，且构建CCMs需要已知动力学模型并解决无限维凸可行性问题，限制其在复杂系统中的可扩展性。

Method: 将CCMs集成到RL中，提出CAC算法，在预训练动力学模型下，同时学习收缩度量生成器并使用演员 - 评论家算法学习最优跟踪策略。

Result: 通过模拟和真实世界机器人实验等大量实证研究，证明算法相对于既定基线的有效性。

Conclusion: CAC算法在全自动设置下增强了CCMs提供收缩策略的能力，并结合了RL的长期最优性，同时给出将收缩理论纳入RL的理论依据。

Abstract: Control contraction metrics (CCMs) provide a framework to co-synthesize a
controller and a corresponding contraction metric -- a positive-definite
Riemannian metric under which a closed-loop system is guaranteed to be
incrementally exponentially stable. However, the synthesized controller only
ensures that all the trajectories of the system converge to one single
trajectory and, as such, does not impose any notion of optimality across an
entire trajectory. Furthermore, constructing CCMs requires a known dynamics
model and non-trivial effort in solving an infinite-dimensional convex
feasibility problem, which limits its scalability to complex systems featuring
high dimensionality with uncertainty. To address these issues, we propose to
integrate CCMs into reinforcement learning (RL), where CCMs provide
dynamics-informed feedback for learning control policies that minimize
cumulative tracking error under unknown dynamics. We show that our algorithm,
called contraction actor-critic (CAC), formally enhances the capability of CCMs
to provide a set of contracting policies with the long-term optimality of RL in
a fully automated setting. Given a pre-trained dynamics model, CAC
simultaneously learns a contraction metric generator (CMG) -- which generates a
contraction metric -- and uses an actor-critic algorithm to learn an optimal
tracking policy guided by that metric. We demonstrate the effectiveness of our
algorithm relative to established baselines through extensive empirical
studies, including simulated and real-world robot experiments, and provide a
theoretical rationale for incorporating contraction theory into RL.

</details>


### [107] [Compiler-R1: Towards Agentic Compiler Auto-tuning with Reinforcement Learning](https://arxiv.org/abs/2506.15701)
*Haolin Pan,Hongyu Lin,Haoran Luo,Yang Liu,Kaichun Yao,Libo Zhang,Mingjie Xing,Yanjun Wu*

Main category: cs.LG

TL;DR: 介绍Compiler - R1框架用于编译器自动调优，经实验可减少IR指令数，代码和数据集公开。


<details>
  <summary>Details</summary>
Motivation: 现有利用大语言模型的编译器自动调优存在缺乏高质量推理数据集和与编译环境交互有限的问题。

Method: 引入Compiler - R1框架，有精心策划的高质量推理数据集和新颖的两阶段端到端强化学习训练管道，通过基于结果的奖励进行环境探索和学习。

Result: 在七个数据集上实验，与opt - Oz相比，平均减少8.46%的IR指令数。

Conclusion: 强化学习训练的大语言模型在编译器优化方面有很大潜力。

Abstract: Compiler auto-tuning optimizes pass sequences to improve performance metrics
such as Intermediate Representation (IR) instruction count. Although recent
advances leveraging Large Language Models (LLMs) have shown promise in
automating compiler tuning, two significant challenges still remain: the
absence of high-quality reasoning datasets for agents training, and limited
effective interactions with the compilation environment. In this work, we
introduce Compiler-R1, the first reinforcement learning (RL)-driven framework
specifically augmenting LLM capabilities for compiler auto-tuning. Compiler-R1
features a curated, high-quality reasoning dataset and a novel two-stage
end-to-end RL training pipeline, enabling efficient environment exploration and
learning through an outcome-based reward. Extensive experiments across seven
datasets demonstrate Compiler-R1 achieving an average 8.46% IR instruction
count reduction compared to opt -Oz, showcasing the strong potential of
RL-trained LLMs for compiler optimization. Our code and datasets are publicly
available at https://github.com/Panhaolin2001/Compiler-R1.

</details>


### [108] [Semantic Outlier Removal with Embedding Models and LLMs](https://arxiv.org/abs/2506.16644)
*Eren Akbiyik,João Almeida,Rik Melis,Ritu Sriram,Viviana Petrescu,Vilhjálmur Vilhjálmsson*

Main category: cs.LG

TL;DR: 提出SORE方法去除文本无关内容，成本低且精度接近大语言模型，实验表现好并已投入生产，还发布代码和数据集。


<details>
  <summary>Details</summary>
Motivation: 现代文本处理流程需有效去除无关内容的方法，传统方法在多语言场景有局限，大语言模型成本高。

Method: 引入SORE方法，利用多语言句子嵌入和近似最近邻搜索，先通过元数据嵌入识别核心内容，再标记异常文本段。

Result: 在HTML数据集实验中，SORE性能优于结构方法，在不同场景有高精度，已部署生产，每天处理数百万多语言文档。

Conclusion: SORE是一种经济高效、透明的去除文本无关内容的方法，可用于实际生产。

Abstract: Modern text processing pipelines demand robust methods to remove extraneous
content while preserving a document's core message. Traditional approaches such
as HTML boilerplate extraction or keyword filters often fail in multilingual
settings and struggle with context-sensitive nuances, whereas Large Language
Models (LLMs) offer improved quality at high computational cost. We introduce
SORE (Semantic Outlier Removal), a cost-effective, transparent method that
leverages multilingual sentence embeddings and approximate nearest-neighbor
search to identify and excise unwanted text segments. By first identifying core
content via metadata embedding and then flagging segments that either closely
match predefined outlier groups or deviate significantly from the core, SORE
achieves near-LLM extraction precision at a fraction of the cost. Experiments
on HTML datasets demonstrate that SORE outperforms structural methods and yield
high precision in diverse scenarios. Our system is currently deployed in
production, processing millions of documents daily across multiple languages
while maintaining both efficiency and accuracy. To facilitate reproducibility
and further research, we release our implementation and evaluation datasets.

</details>


### [109] [Minifinetuning: Low-Data Generation Domain Adaptation through Corrective Self-Distillation](https://arxiv.org/abs/2506.15702)
*Peter Belcak,Greg Heinrich,Jan Kautz,Pavlo Molchanov*

Main category: cs.LG

TL;DR: 提出minifinetuning (MFT)方法，在低数据场景减少语言模型过拟合导致的泛化能力下降，表现优于标准微调。


<details>
  <summary>Details</summary>
Motivation: 微调语言模型用于新领域会导致泛化能力下降，尤其是微调数据资源有限时。

Method: 引入minifinetuning (MFT)方法，采用样本级别的校正自蒸馏。

Result: MFT在多种模型和领域中展现出比标准微调高2 - 10倍的专业化与泛化退化比，在新领域数据稀缺至500样本时仍有抗过拟合能力，且优于参数高效微调方法。

Conclusion: MFT能有效减少低数据场景下语言模型过拟合导致的泛化能力下降，可与其他方法结合。

Abstract: Finetuning language models for a new domain inevitably leads to the
deterioration of their general performance. This becomes more pronounced the
more limited the finetuning data resource.
  We introduce minifinetuning (MFT), a method for language model domain
adaptation that considerably reduces the effects of overfitting-induced
degeneralization in low-data settings and which does so in the absence of any
pre-training data for replay. MFT demonstrates 2-10x more favourable
specialization-to-degeneralization ratios than standard finetuning across a
wide range of models and domains and exhibits an intrinsic robustness to
overfitting when data in the new domain is scarce and down to as little as 500
samples.
  Employing corrective self-distillation that is individualized on the sample
level, MFT outperforms parameter-efficient finetuning methods, demonstrates
replay-like degeneralization mitigation properties, and is composable with
either for a combined effect.

</details>


### [110] [Federated Incomplete Multi-view Clustering with Globally Fused Graph Guidance](https://arxiv.org/abs/2506.15703)
*Guoqing Chao,Zhenghao Zhang,Lei Meng,Jie Wen,Dianhui Chu*

Main category: cs.LG

TL;DR: 提出基于全局融合图引导的联邦不完整多视图聚类方法FIMCFG，实验证明其有效性和优越性。


<details>
  <summary>Details</summary>
Motivation: 现有联邦多视图聚类方法仅用全局伪标签引导聚类，提取特征时未利用全局信息，且对缺失数据问题探索较少。

Method: 在每个客户端设计双头图卷积编码器提取含全局和特定视图信息的特征，在融合图引导下将特征融合为高级特征进行聚类，上传高级特征到服务器优化图融合和伪标签计算。

Result: 实验结果表明FIMCFG有效且优越。

Conclusion: FIMCFG是解决联邦不完整多视图聚类问题的有效方法，代码公开。

Abstract: Federated multi-view clustering has been proposed to mine the valuable
information within multi-view data distributed across different devices and has
achieved impressive results while preserving the privacy. Despite great
progress, most federated multi-view clustering methods only used global
pseudo-labels to guide the downstream clustering process and failed to exploit
the global information when extracting features. In addition, missing data
problem in federated multi-view clustering task is less explored. To address
these problems, we propose a novel Federated Incomplete Multi-view Clustering
method with globally Fused Graph guidance (FIMCFG). Specifically, we designed a
dual-head graph convolutional encoder at each client to extract two kinds of
underlying features containing global and view-specific information.
Subsequently, under the guidance of the fused graph, the two underlying
features are fused into high-level features, based on which clustering is
conducted under the supervision of pseudo-labeling. Finally, the high-level
features are uploaded to the server to refine the graph fusion and
pseudo-labeling computation. Extensive experimental results demonstrate the
effectiveness and superiority of FIMCFG. Our code is publicly available at
https://github.com/PaddiHunter/FIMCFG.

</details>


### [111] [Learn from the Past: Fast Sparse Indexing for Large Language Model Decoding](https://arxiv.org/abs/2506.15704)
*Feiyu Yao,Qian Wang*

Main category: cs.LG

TL;DR: 论文提出LFPS方法解决大语言模型长上下文解码时KV缓存内存需求大问题，在长上下文基准测试验证，有显著加速效果且不影响生成精度。


<details>
  <summary>Details</summary>
Motivation: 大语言模型长上下文解码时KV缓存内存需求增长快成瓶颈，现有稀疏注意力机制索引计算开销大，且未利用历史解码信息的时间相关性。

Method: 提出LFPS方法，基于历史注意力模式动态构建稀疏索引候选，捕捉垂直和斜线模式，结合位置扩展策略预测当前步骤Top - k索引。

Result: 在LongBench - RULER等基准测试中，使用Llama - 3.1 - 8B - Instruct为基础模型，在RTX 4090 GPU和Xeon Gold 6430单CPU核心上分别比全注意力和精确Top - k检索实现22.8×和9.6×加速，且保持生成精度。

Conclusion: LFPS为长上下文大语言模型推理的解码优化提供了实用高效的解决方案。

Abstract: As large language models (LLMs) continue to support increasingly longer
contexts, the memory demand for key-value (KV) caches during decoding grows
rapidly, becoming a critical bottleneck in both GPU memory capacity and PCIe
bandwidth. Sparse attention mechanisms alleviate this issue by computing
attention weights only for selected key-value pairs. However, their indexing
computation typically requires traversing all key vectors, resulting in
significant computational and data transfer overhead. To reduce the cost of
index retrieval, existing methods often treat each decoding step as an
independent process, failing to exploit the temporal correlations embedded in
historical decoding information. To this end, we propose LFPS(Learn From the
Past for Sparse Indexing), an acceleration method that dynamically constructs
sparse indexing candidates based on historical attention patterns. LFPS
captures two prevalent trends in decoder attention -vertical patterns
(attending to fixed positions) and slash patterns (attending to relative
positions) -and incorporates a positional expansion strategy to effectively
predict the Top-k indices for the current step. We validate LFPS on challenging
long-context benchmarks such as LongBench-RULER, using Llama-3.1-8B-Instruct as
the base model. Experimental results show that LFPS achieves up to 22.8$\times$
speedup over full attention and 9.6$\times$ speedup over exact Top-k retrieval
on an RTX 4090 GPU and a single CPU core of a Xeon Gold 6430, respectively,
while preserving generation accuracy. These results demonstrate that LFPS
offers a practical and efficient solution for decoding optimization in
long-context LLM inference.

</details>


### [112] [Joint Tensor-Train Parameterization for Efficient and Expressive Low-Rank Adaptation](https://arxiv.org/abs/2506.16456)
*Jun Qi,Chen-Yu Liu,Sabato Marco Siniscalchi,Chao-Han Huck Yang,Min-Hsiu Hsieh*

Main category: cs.LG

TL;DR: 提出TensorGuide框架改进LoRA，理论证明优势，实验显示性能更好


<details>
  <summary>Details</summary>
Motivation: 标准LoRA独立优化低秩矩阵限制了表达和泛化能力，经典TT分解改进效果不佳

Method: 提出TensorGuide框架，通过统一TT结构生成两个相关低秩LoRA矩阵

Result: 理论上证明优化动态和泛化能力更好，实验中基于TensorGuide的LoRA性能优于标准LoRA和TT - LoRA

Conclusion: TensorGuide能增强表达、泛化和参数效率，在多个任务中表现更好

Abstract: Low-Rank Adaptation (LoRA) is widely recognized for its parameter-efficient
fine-tuning of large-scale neural models. However, standard LoRA independently
optimizes low-rank matrices, which inherently limits its expressivity and
generalization capabilities. While classical tensor-train (TT) decomposition
can be separately employed on individual LoRA matrices, this work demonstrates
that the classical TT-based approach neither significantly improves parameter
efficiency nor achieves substantial performance gains. This paper proposes
TensorGuide, a novel tensor-train-guided adaptation framework to overcome these
limitations. TensorGuide generates two correlated low-rank LoRA matrices
through a unified TT structure driven by controlled Gaussian noise. The
resulting joint TT representation inherently provides structured, low-rank
adaptations, significantly enhancing expressivity, generalization, and
parameter efficiency without increasing the number of trainable parameters.
Theoretically, we justify these improvements through neural tangent kernel
analyses, demonstrating superior optimization dynamics and enhanced
generalization. Extensive experiments on quantum dot classification and GPT-2
fine-tuning benchmarks demonstrate that TensorGuide-based LoRA consistently
outperforms standard LoRA and TT-LoRA, achieving improved accuracy and
scalability with fewer parameters.

</details>


### [113] [Generalisation Bounds of Zero-Shot Economic Forecasting using Time Series Foundation Models](https://arxiv.org/abs/2506.15705)
*Jittarin Jetwiriyanon,Teo Susnjak,Surangika Ranathunga*

Main category: cs.LG

TL;DR: 研究时间序列基础模型（TSFMs）对宏观经济指标的零样本预测能力，实验表明合适的TSFMs有一定优势，但在快速冲击时期性能会下降。


<details>
  <summary>Details</summary>
Motivation: 探索TSFMs对宏观经济指标的零样本预测能力，避免训练定制计量经济学模型和使用大量训练数据集。

Method: 在案例研究数据集上对三种最先进的TSFMs（Chronos、TimeGPT和Moirai）在数据稀缺和结构突变条件下进行严格回测。

Result: 合适的TSFMs能内化丰富经济动态、适应制度转变并提供良好的不确定性估计，在稳定经济条件下可媲美或超越经典模型，但在快速冲击时期性能会下降。

Conclusion: 研究结果为从业者在宏观经济监测和战略规划中何时进行零样本部署提供了指导。

Abstract: This study investigates zero-shot forecasting capabilities of Time Series
Foundation Models (TSFMs) for macroeconomic indicators. We apply TSFMs to
forecasting economic indicators under univariate conditions, bypassing the need
for train bespoke econometric models using and extensive training datasets. Our
experiments were conducted on a case study dataset, without additional
customisation. We rigorously back-tested three state-of-the-art TSFMs (Chronos,
TimeGPT and Moirai) under data-scarce conditions and structural breaks. Our
results demonstrate that appropriately engineered TSFMs can internalise rich
economic dynamics, accommodate regime shifts, and deliver well-behaved
uncertainty estimates out of the box, while matching state-of-the-art
multivariate models on this domain. Our findings suggest that, without any
fine-tuning, TSFMs can match or exceed classical models during stable economic
conditions. However, they are vulnerable to degradation in performances during
periods of rapid shocks. The findings offer guidance to practitioners on when
zero-shot deployments are viable for macroeconomic monitoring and strategic
planning.

</details>


### [114] [MDPO: Multi-Granularity Direct Preference Optimization for Mathematical Reasoning](https://arxiv.org/abs/2506.15706)
*Yunze Lin*

Main category: cs.LG

TL;DR: 提出MDPO方法优化大语言模型数学推理能力，在多数据集实验中表现优于DPO及变体，还提供无人工标注训练数据构建管道。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型数学推理存在挑战，监督微调易产生幻觉，DPO在长链数学推理中效果有限。

Method: 提出MDPO方法，从Solution2Solution、Inference2Inference、Step2Step三个粒度优化大语言模型数学推理，并统一训练目标。

Result: 在Qwen2和Llama3模型上，GSM8K和MATH数据集实验均有提升，优于DPO及变体方法。

Conclusion: MDPO方法有效提升大语言模型数学推理能力，且训练数据构建管道简单无人工标注成本。

Abstract: Mathematical reasoning presents a significant challenge for Large Language
Models (LLMs) as it requires ensuring the correctness of each reasoning step.
Researchers have been strengthening the mathematical reasoning abilities of
LLMs through supervised fine-tuning, but due to the inability to suppress
incorrect outputs, illusions can easily arise. Recently, Direct Preference
Optimization (DPO) has been widely adopted for aligning human intent by using
preference data to prevent LLMs from generating incorrect outputs. However, it
has shown limited benefits in long-chain mathematical reasoning, mainly because
DPO struggles to effectively capture the differences between accepted and
rejected answers from preferences in long-chain data. The inconsistency between
DPO training and LLMs' generation metrics also affects the effectiveness of
suppressing incorrect outputs. We propose the Multi-Granularity Direct
Preference Optimization (MDPO) method, optimizing the mathematical reasoning of
LLMs at three granularities: Solution2Solution, Inference2Inference, and
Step2Step. Solution2Solution focuses on the correctness of entire long-chain
reasoning; Inference2Inference concentrates on logical reasoning between steps;
Step2Step corrects computational errors in steps, enhancing the computational
capabilities of LLMs. Additionally, we unify the training objectives of the
three granularities to align with the generation metrics. We conducted
experiments on the open-source models Qwen2 and Llama3, achieving improvements
of 1.7% and 0.9% on the GSM8K dataset, and 2.3% and 1.2% on the MATH dataset,
outperforming DPO and other DPO variant methods. Furthermore, we also provide a
pipeline for constructing MDPO training data that is simple and does not
require manual annotation costs.

</details>


### [115] [A Free Probabilistic Framework for Analyzing the Transformer-based Language Models](https://arxiv.org/abs/2506.16550)
*Swagatam Das*

Main category: cs.LG

TL;DR: 使用自由概率论工具构建算子理论框架分析基于Transformer的语言模型，揭示其底层光谱动力系统并推导泛化边界。


<details>
  <summary>Details</summary>
Motivation: 对基于Transformer的语言模型进行原理性分析，了解其归纳偏置、泛化行为和熵动态。

Method: 将词元嵌入和注意力机制表示为自由概率空间中的自伴算子，将注意力重新解释为非交换卷积，将表征的逐层传播视为自由加法卷积控制的演化。

Result: 揭示了深度Transformer堆栈底层的光谱动力系统，推导了基于自由熵的泛化边界，证明Transformer层的光谱迹随深度可预测地演化。

Conclusion: 该方法将神经架构与非交换调和分析联系起来，可对大语言模型的信息流和结构复杂性进行原理性分析。

Abstract: We outline an operator-theoretic framework for analyzing transformer-based
language models using the tools of free probability theory. By representing
token embeddings and attention mechanisms as self-adjoint operators in a racial
probability space, we reinterpret attention as a non-commutative convolution
and view the layer-wise propagation of representations as an evolution governed
by free additive convolution. This formalism reveals a spectral dynamical
system underpinning deep transformer stacks and offers insight into their
inductive biases, generalization behavior, and entropy dynamics. We derive a
generalization bound based on free entropy and demonstrate that the spectral
trace of transformer layers evolves predictably with depth. Our approach
bridges neural architecture with non-commutative harmonic analysis, enabling
principled analysis of information flow and structural complexity in large
language models

</details>


### [116] [Every Rollout Counts: Optimal Resource Allocation for Efficient Test-Time Scaling](https://arxiv.org/abs/2506.15707)
*Xinglin Wang,Yiwei Li,Shaoxiong Feng,Peiwen Yuan,Yueqi Zhang,Jiayi Shi,Chuyi Tan,Boyuan Pan,Yao Hu,Kan Li*

Main category: cs.LG

TL;DR: 提出DORA方法解决TTS中资源分配问题，实验显示其优于基线达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有TTS在搜索时未有效分配固定预算，导致计算资源利用低效。

Method: 将测试时搜索表述为资源分配问题，推导最优策略，提出DORA方法在方向层面分配资源。

Result: 在数学推理基准测试上，DORA在相近计算成本下持续优于强基线，达到SOTA准确率。

Conclusion: 研究成果有助于更广泛理解LLMs的最优TTS。

Abstract: Test-Time Scaling (TTS) improves the performance of Large Language Models
(LLMs) by using additional inference-time computation to explore multiple
reasoning paths through search. Yet how to allocate a fixed rollout budget most
effectively during search remains underexplored, often resulting in inefficient
use of compute at test time. To bridge this gap, we formulate test-time search
as a resource allocation problem and derive the optimal allocation strategy
that maximizes the probability of obtaining a correct solution under a fixed
rollout budget. Within this formulation, we reveal a core limitation of
existing search methods: solution-level allocation tends to favor reasoning
directions with more candidates, leading to theoretically suboptimal and
inefficient use of compute. To address this, we propose Direction-Oriented
Resource Allocation (DORA), a provably optimal method that mitigates this bias
by decoupling direction quality from candidate count and allocating resources
at the direction level. To demonstrate DORA's effectiveness, we conduct
extensive experiments on challenging mathematical reasoning benchmarks
including MATH500, AIME2024, and AIME2025. The empirical results show that DORA
consistently outperforms strong baselines with comparable computational cost,
achieving state-of-the-art accuracy. We hope our findings contribute to a
broader understanding of optimal TTS for LLMs.

</details>


### [117] [Refined Causal Graph Structure Learning via Curvature for Brain Disease Classification](https://arxiv.org/abs/2506.15708)
*Falih Gozi Febrinanto,Adonia Simango,Chengpei Xu,Jingjing Zhou,Jiangang Ma,Sonika Tyagi,Feng Xia*

Main category: cs.LG

TL;DR: 提出CGB框架用于脑疾病分类/检测，考虑脑ROI因果关系，实验显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多数GNN框架未考虑脑ROI间因果关系，而因果关系对观察信号间因果交互更重要，故提出新框架。

Method: 基于因果发现方法、转移熵和几何曲率策略建模精细脑网络，通过几何曲率策略进行图重连。

Result: 在脑疾病数据集分类任务中，CGB平均F1分数优于现有方法。

Conclusion: CGB能揭示ROI间因果关系，提升脑疾病分类性能，有效减少信息瓶颈。

Abstract: Graph neural networks (GNNs) have been developed to model the relationship
between regions of interest (ROIs) in brains and have shown significant
improvement in detecting brain diseases. However, most of these frameworks do
not consider the intrinsic relationship of causality factor between brain ROIs,
which is arguably more essential to observe cause and effect interaction
between signals rather than typical correlation values. We propose a novel
framework called CGB (Causal Graphs for Brains) for brain disease
classification/detection, which models refined brain networks based on the
causal discovery method, transfer entropy, and geometric curvature strategy.
CGB unveils causal relationships between ROIs that bring vital information to
enhance brain disease classification performance. Furthermore, CGB also
performs a graph rewiring through a geometric curvature strategy to refine the
generated causal graph to become more expressive and reduce potential
information bottlenecks when GNNs model it. Our extensive experiments show that
CGB outperforms state-of-the-art methods in classification tasks on brain
disease datasets, as measured by average F1 scores.

</details>


### [118] [Learning Causally Predictable Outcomes from Psychiatric Longitudinal Data](https://arxiv.org/abs/2506.16629)
*Eric V. Strobl*

Main category: cs.LG

TL;DR: 针对纵向生物医学数据因果推断难题，提出DEBIAS算法，优化结果定义，提升因果可识别性，实验表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 纵向生物医学数据尤其是精神病学数据因果推断存在挑战，现有方法对固定结果变量假设的无混淆性在实践中可能不成立。

Method: 直接优化结果定义以最大化因果可识别性，DEBIAS算法学习非负、临床可解释的权重进行结果聚合，利用先前治疗的限时直接效应最小化混淆，并提供结果无混淆性的可验证测试。

Result: DEBIAS在抑郁症和精神分裂症的综合实验中，在恢复临床可解释的复合结果的因果效应方面始终优于现有方法。

Conclusion: DEBIAS算法能有效解决纵向生物医学数据因果推断问题，有更好的性能。

Abstract: Causal inference in longitudinal biomedical data remains a central challenge,
especially in psychiatry, where symptom heterogeneity and latent confounding
frequently undermine classical estimators. Most existing methods for treatment
effect estimation presuppose a fixed outcome variable and address confounding
through observed covariate adjustment. However, the assumption of
unconfoundedness may not hold for a fixed outcome in practice. To address this
foundational limitation, we directly optimize the outcome definition to
maximize causal identifiability. Our DEBIAS (Durable Effects with
Backdoor-Invariant Aggregated Symptoms) algorithm learns non-negative,
clinically interpretable weights for outcome aggregation, maximizing durable
treatment effects and empirically minimizing both observed and latent
confounding by leveraging the time-limited direct effects of prior treatments
in psychiatric longitudinal data. The algorithm also furnishes an empirically
verifiable test for outcome unconfoundedness. DEBIAS consistently outperforms
state-of-the-art methods in recovering causal effects for clinically
interpretable composite outcomes across comprehensive experiments in depression
and schizophrenia.

</details>


### [119] [Studying and Improving Graph Neural Network-based Motif Estimation](https://arxiv.org/abs/2506.15709)
*Pedro C. Vieira,Miguel E. P. Silva,Pedro Manuel Pinto Ribeiro*

Main category: cs.LG

TL;DR: 提出将网络 motif 显著性 - 轮廓（SP）估计作为独立于子图频率估计的任务，用 GNN 解决该问题，在合成和真实数据集上验证方法，揭示 1 - WL 受限模型表现及直接 SP 估计潜力。


<details>
  <summary>Details</summary>
Motivation: GNN 在网络 motif 显著性 - 轮廓（SP）预测应用未充分探索，且无既定基准。

Method: 将 SP 估计作为独立于子图频率估计的任务，从频率计数转向直接 SP 估计，将问题建模为多目标回归，优化以提高在大图上的可解释性、稳定性和可扩展性。

Result: 1 - WL 受限模型难以精确估计 SPs，但可通过比较预测 SP 与合成生成器的 SP 来近似网络生成过程。

Conclusion: 首次基于 GNN 的 motif 估计研究，暗示直接 SP 估计可突破子图计数进行 motif 估计的理论限制。

Abstract: Graph Neural Networks (GNNs) are a predominant method for graph
representation learning. However, beyond subgraph frequency estimation, their
application to network motif significance-profile (SP) prediction remains
under-explored, with no established benchmarks in the literature. We propose to
address this problem, framing SP estimation as a task independent of subgraph
frequency estimation. Our approach shifts from frequency counting to direct SP
estimation and modulates the problem as multitarget regression. The
reformulation is optimised for interpretability, stability and scalability on
large graphs. We validate our method using a large synthetic dataset and
further test it on real-world graphs. Our experiments reveal that 1-WL limited
models struggle to make precise estimations of SPs. However, they can
generalise to approximate the graph generation processes of networks by
comparing their predicted SP with the ones originating from synthetic
generators. This first study on GNN-based motif estimation also hints at how
using direct SP estimation can help go past the theoretical limitations that
motif estimation faces when performed through subgraph counting.

</details>


### [120] [RAST: Reasoning Activation in LLMs via Small-model Transfer](https://arxiv.org/abs/2506.15710)
*Siru Ouyang,Xinyu Zhu,Zilin Xiao,Minhao Jiang,Yu Meng,Jiawei Han*

Main category: cs.LG

TL;DR: 提出RAST方法，将小模型的RL概率调整注入大模型，提高推理能力且节省GPU内存。


<details>
  <summary>Details</summary>
Motivation: 应用RL提升大语言模型推理能力资源消耗大，且RL主要激活模型潜在推理能力，推测其概率变化与模型大小无关，希望找到更高效范式。

Method: 进行token级解码轨迹分析验证假设，提出RAST方法，将小模型的RL概率调整注入大模型。

Result: 在多个数学推理基准测试中，RAST显著提升基础模型推理能力，GPU内存需求显著低于直接RL训练，有时性能更好。

Conclusion: 为RL驱动的推理本质提供新见解，给出无需高计算成本扩展其优势的实用策略。

Abstract: Reinforcement learning (RL) has become a powerful approach for improving the
reasoning capabilities of large language models (LLMs), as evidenced by recent
successes such as OpenAI's o1 and Deepseek-R1. However, applying RL at scale
remains intimidatingly resource-intensive, requiring multiple model copies and
extensive GPU workloads. On the other hand, while being powerful, recent
studies suggest that RL does not fundamentally endow models with new knowledge;
rather, it primarily reshapes the model's output distribution to activate
reasoning capabilities latent in the base model. Building on this insight, we
hypothesize that the changes in output probabilities induced by RL are largely
model-size invariant, opening the door to a more efficient paradigm: training a
small model with RL and transferring its induced probability shifts to larger
base models. To verify our hypothesis, we conduct a token-level analysis of
decoding trajectories and find high alignment in RL-induced output
distributions across model scales, validating our hypothesis. Motivated by
this, we propose RAST, a simple yet effective method that transfers reasoning
behaviors by injecting RL-induced probability adjustments from a small
RL-trained model into larger models. Experiments across multiple mathematical
reasoning benchmarks show that RAST substantially and consistently enhances the
reasoning capabilities of base models while requiring significantly lower GPU
memory than direct RL training, sometimes even yielding better performance than
the RL-trained counterparts. Our findings offer new insights into the nature of
RL-driven reasoning and practical strategies for scaling its benefits without
incurring its full computational cost. The project page of RAST is available at
https://ozyyshr.github.io/RAST/.

</details>


### [121] [Private Training & Data Generation by Clustering Embeddings](https://arxiv.org/abs/2506.16661)
*Felix Zhou,Samson Zhou,Vahab Mirrokni,Alessandro Epasto,Vincent Cohen-Addad*

Main category: cs.LG

TL;DR: 提出基于DP聚类拟合高斯混合模型生成合成图像嵌入的新方法，在基准数据集上取得SOTA分类准确率，能生成逼真图像，方法通用且可扩展。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络使用含敏感数据的大型数据集训练会引发隐私问题，差分隐私可保护个人数据，需新的私有训练方法。

Method: 在合适的嵌入空间使用DP聚类拟合高斯混合模型生成DP合成图像嵌入。

Result: 简单的两层神经网络在合成嵌入上训练在标准基准数据集上达到SOTA分类准确率，能生成逼真合成图像且下游分类准确率与SOTA方法相当。

Conclusion: 提出的方法通用，编码器和解码器模块可自由替换，且高度可扩展。

Abstract: Deep neural networks often use large, high-quality datasets to achieve high
performance on many machine learning tasks. When training involves potentially
sensitive data, this process can raise privacy concerns, as large models have
been shown to unintentionally memorize and reveal sensitive information,
including reconstructing entire training samples. Differential privacy (DP)
provides a robust framework for protecting individual data and in particular, a
new approach to privately training deep neural networks is to approximate the
input dataset with a privately generated synthetic dataset, before any
subsequent training algorithm. We introduce a novel principled method for DP
synthetic image embedding generation, based on fitting a Gaussian Mixture Model
(GMM) in an appropriate embedding space using DP clustering. Our method
provably learns a GMM under separation conditions. Empirically, a simple
two-layer neural network trained on synthetically generated embeddings achieves
state-of-the-art (SOTA) classification accuracy on standard benchmark datasets.
Additionally, we demonstrate that our method can generate realistic synthetic
images that achieve downstream classification accuracy comparable to SOTA
methods. Our method is quite general, as the encoder and decoder modules can be
freely substituted to suit different tasks. It is also highly scalable,
consisting only of subroutines that scale linearly with the number of samples
and/or can be implemented efficiently in distributed systems.

</details>


### [122] [Shadow defense against gradient inversion attack in federated learning](https://arxiv.org/abs/2506.15711)
*Le Jiang,Liyan Ma,Guang Yang*

Main category: cs.LG

TL;DR: 本文针对联邦学习中隐私泄露问题，提出利用可解释影子模型识别敏感区域的框架，能实现针对性噪声注入，实验证明其有效性和泛化性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在医疗等领域隐私保护重要，但现有防御机制缺乏对敏感梯度和图像信息的理解，存在过度或不足保护问题。

Method: 引入利用可解释影子模型识别敏感区域的框架，实现针对性、样本特定的噪声注入。

Result: 在ChestXRay和EyePACS数据集上有较好防御效果，与无防御情况相比PSNR和SSIM有差异；对模型性能影响小，F1降低不到1%；在多种医学图像上实验验证泛化性，对FedAvg防御稳定提升，能抵御多种GIA类型。

Conclusion: 所提出框架有效且具有泛化性，能针对敏感区域提供通用防御。

Abstract: Federated learning (FL) has emerged as a transformative framework for
privacy-preserving distributed training, allowing clients to collaboratively
train a global model without sharing their local data. This is especially
crucial in sensitive fields like healthcare, where protecting patient data is
paramount. However, privacy leakage remains a critical challenge, as the
communication of model updates can be exploited by potential adversaries.
Gradient inversion attacks (GIAs), for instance, allow adversaries to
approximate the gradients used for training and reconstruct training images,
thus stealing patient privacy. Existing defense mechanisms obscure gradients,
yet lack a nuanced understanding of which gradients or types of image
information are most vulnerable to such attacks. These indiscriminate
calibrated perturbations result in either excessive privacy protection
degrading model accuracy, or insufficient one failing to safeguard sensitive
information. Therefore, we introduce a framework that addresses these
challenges by leveraging a shadow model with interpretability for identifying
sensitive areas. This enables a more targeted and sample-specific noise
injection. Specially, our defensive strategy achieves discrepancies of 3.73 in
PSNR and 0.2 in SSIM compared to the circumstance without defense on the
ChestXRay dataset, and 2.78 in PSNR and 0.166 in the EyePACS dataset. Moreover,
it minimizes adverse effects on model performance, with less than 1\% F1
reduction compared to SOTA methods. Our extensive experiments, conducted across
diverse types of medical images, validate the generalization of the proposed
framework. The stable defense improvements for FedAvg are consistently over
1.5\% times in LPIPS and SSIM. It also offers a universal defense against
various GIA types, especially for these sensitive areas in images.

</details>


### [123] [How Many Domains Suffice for Domain Generalization? A Tight Characterization via the Domain Shattering Dimension](https://arxiv.org/abs/2506.16704)
*Cynthia Dwork,Lunjia Hu,Han Shao*

Main category: cs.LG

TL;DR: 研究领域泛化中需随机采样多少个领域来收集数据以学习通用模型，引入领域破碎维度并建立其与VC维度关系。


<details>
  <summary>Details</summary>
Motivation: 解决领域泛化中收集多少随机采样领域数据能让模型在所有见过和未见过的领域上表现良好的基础问题。

Method: 在PAC框架下建模问题，引入领域破碎维度这一组合度量。

Result: 领域破碎维度刻画了领域样本复杂度，建立了其与经典VC维度的紧密定量关系。

Conclusion: 标准PAC设置下可学习的每个假设类在该设置下也可学习。

Abstract: We study a fundamental question of domain generalization: given a family of
domains (i.e., data distributions), how many randomly sampled domains do we
need to collect data from in order to learn a model that performs reasonably
well on every seen and unseen domain in the family? We model this problem in
the PAC framework and introduce a new combinatorial measure, which we call the
domain shattering dimension. We show that this dimension characterizes the
domain sample complexity. Furthermore, we establish a tight quantitative
relationship between the domain shattering dimension and the classic VC
dimension, demonstrating that every hypothesis class that is learnable in the
standard PAC setting is also learnable in our setting.

</details>


### [124] [BatteryBERT for Realistic Battery Fault Detection Using Point-Masked Signal Modeling](https://arxiv.org/abs/2506.15712)
*Songqi Zhou,Ruixue Liu,Yixing Wang,Jia Lu,Benben Jiang*

Main category: cs.LG

TL;DR: 提出一种适用于电池故障检测的BERT式预训练框架，在大规模真实数据集上提升了表示质量和分类准确率。


<details>
  <summary>Details</summary>
Motivation: 现有电池故障检测方法难捕捉复杂时间依赖且无法充分利用无标签数据，大语言模型架构不适用于工业数值时间序列数据。

Method: 扩展标准BERT架构，添加定制的时间序列到令牌表示模块和点级掩码信号建模预训练任务，进行自监督学习，将嵌入与电池元数据拼接后输入下游分类器。

Result: 使用预训练参数初始化的模型显著提升表示质量和分类准确率，AUROC达0.945，远超现有方法。

Conclusion: 验证了BERT式预训练用于时间序列故障检测的有效性。

Abstract: Accurate fault detection in lithium-ion batteries is essential for the safe
and reliable operation of electric vehicles and energy storage systems.
However, existing methods often struggle to capture complex temporal
dependencies and cannot fully leverage abundant unlabeled data. Although large
language models (LLMs) exhibit strong representation capabilities, their
architectures are not directly suited to the numerical time-series data common
in industrial settings. To address these challenges, we propose a novel
framework that adapts BERT-style pretraining for battery fault detection by
extending the standard BERT architecture with a customized time-series-to-token
representation module and a point-level Masked Signal Modeling (point-MSM)
pretraining task tailored to battery applications. This approach enables
self-supervised learning on sequential current, voltage, and other
charge-discharge cycle data, yielding distributionally robust, context-aware
temporal embeddings. We then concatenate these embeddings with battery metadata
and feed them into a downstream classifier for accurate fault classification.
Experimental results on a large-scale real-world dataset show that models
initialized with our pretrained parameters significantly improve both
representation quality and classification accuracy, achieving an AUROC of 0.945
and substantially outperforming existing approaches. These findings validate
the effectiveness of BERT-style pretraining for time-series fault detection.

</details>


### [125] [Bandwidth Selectors on Semiparametric Bayesian Networks](https://arxiv.org/abs/2506.16844)
*Victor Alejandre,Concha Bielza,Pedro Larrañaga*

Main category: cs.LG

TL;DR: 本文探讨了在半参数贝叶斯网络（SPBNs）中应用先进带宽选择器，扩展 PyBNesian 包进行实验，结果表明新选择器比正态规则更有效，无偏交叉验证在高样本量时优势明显。


<details>
  <summary>Details</summary>
Motivation: 现实数据常偏离正态分布，正态规则学习带宽矩阵用于 SPBNs 中的核密度估计（KDEs）可能导致密度估计不佳和预测性能降低，需要更好的带宽选择方法。

Method: 建立先进带宽选择器应用的理论框架，探索交叉验证和插件选择器方法，扩展 PyBNesian 包并进行大量实验分析。

Result: 提出的带宽选择器比正态规则更能有效利用信息，无偏交叉验证通常优于正态规则，在高样本量场景优势显著。

Conclusion: 先进带宽选择器可提升 SPBNs 的学习能力和适用性，无偏交叉验证在高样本量时有优势。

Abstract: Semiparametric Bayesian networks (SPBNs) integrate parametric and
non-parametric probabilistic models, offering flexibility in learning complex
data distributions from samples. In particular, kernel density estimators
(KDEs) are employed for the non-parametric component. Under the assumption of
data normality, the normal rule is used to learn the bandwidth matrix for the
KDEs in SPBNs. This matrix is the key hyperparameter that controls the
trade-off between bias and variance. However, real-world data often deviates
from normality, potentially leading to suboptimal density estimation and
reduced predictive performance. This paper first establishes the theoretical
framework for the application of state-of-the-art bandwidth selectors and
subsequently evaluates their impact on SPBN performance. We explore the
approaches of cross-validation and plug-in selectors, assessing their
effectiveness in enhancing the learning capability and applicability of SPBNs.
To support this investigation, we have extended the open-source package
PyBNesian for SPBNs with the additional bandwidth selection techniques and
conducted extensive experimental analyses. Our results demonstrate that the
proposed bandwidth selectors leverage increasing information more effectively
than the normal rule, which, despite its robustness, stagnates with more data.
In particular, unbiased cross-validation generally outperforms the normal rule,
highlighting its advantage in high sample size scenarios.

</details>


### [126] [An application of machine learning to the motion response prediction of floating assets](https://arxiv.org/abs/2506.15713)
*Michael T. M. B. Morris-Thomas,Marius Martens*

Main category: cs.LG

TL;DR: 本文提出用监督式机器学习方法预测浮式海洋资产行为，模型误差小，优于传统方法且已成功应用。


<details>
  <summary>Details</summary>
Motivation: 实时预测随机海洋条件下浮式海洋资产行为是海洋工程重大挑战，传统方法在极端海况和非线性响应中表现不佳。

Method: 采用多元回归的监督式机器学习方法，结合梯度提升集成方法和自定义被动风向标求解器，用约$10^6$个涵盖100个特征的样本进行训练。

Result: 模型对关键系泊参数平均预测误差小于5%，船舶航向精度在2.5度以内，显著优于传统频域方法。

Conclusion: 该框架已成功部署在运营设施上，可用于海上环境实时船舶监测和运营决策。

Abstract: The real-time prediction of floating offshore asset behavior under stochastic
metocean conditions remains a significant challenge in offshore engineering.
While traditional empirical and frequency-domain methods work well in benign
conditions, they struggle with both extreme sea states and nonlinear responses.
This study presents a supervised machine learning approach using multivariate
regression to predict the nonlinear motion response of a turret-moored vessel
in 400 m water depth. We developed a machine learning workflow combining a
gradient-boosted ensemble method with a custom passive weathervaning solver,
trained on approximately $10^6$ samples spanning 100 features. The model
achieved mean prediction errors of less than 5% for critical mooring parameters
and vessel heading accuracy to within 2.5 degrees across diverse metocean
conditions, significantly outperforming traditional frequency-domain methods.
The framework has been successfully deployed on an operational facility,
demonstrating its efficacy for real-time vessel monitoring and operational
decision-making in offshore environments.

</details>


### [127] [The Importance of Being Lazy: Scaling Limits of Continual Learning](https://arxiv.org/abs/2506.16884)
*Jacopo Graldi,Alessandro Breccia,Giulia Lanzillotta,Thomas Hofmann,Lorenzo Noci*

Main category: cs.LG

TL;DR: 本文系统研究持续学习中模型规模和特征学习程度的影响，统一了规模作用观点，揭示特征学习与遗忘关系，指出网络在特定特征学习水平有最优表现。


<details>
  <summary>Details</summary>
Motivation: 神经网络在非平稳环境学习有困难，对灾难性遗忘理解不足，调和文献中关于规模的矛盾观察。

Method: 通过架构可变参数化区分训练机制，用动态平均场理论研究无限宽度模型动态，分析特征学习、任务非平稳性和遗忘的关系。

Result: 增加模型宽度仅在减少特征学习时有益；高特征学习仅在任务高度相似时有益；存在由任务相似性调制的转变。

Conclusion: 神经网络在依赖任务非平稳性的关键特征学习水平上实现最优性能，为持续学习中规模和特征学习作用提供统一视角。

Abstract: Despite recent efforts, neural networks still struggle to learn in
non-stationary environments, and our understanding of catastrophic forgetting
(CF) is far from complete. In this work, we perform a systematic study on the
impact of model scale and the degree of feature learning in continual learning.
We reconcile existing contradictory observations on scale in the literature, by
differentiating between lazy and rich training regimes through a variable
parameterization of the architecture. We show that increasing model width is
only beneficial when it reduces the amount of feature learning, yielding more
laziness. Using the framework of dynamical mean field theory, we then study the
infinite width dynamics of the model in the feature learning regime and
characterize CF, extending prior theoretical results limited to the lazy
regime. We study the intricate relationship between feature learning, task
non-stationarity, and forgetting, finding that high feature learning is only
beneficial with highly similar tasks. We identify a transition modulated by
task similarity where the model exits an effectively lazy regime with low
forgetting to enter a rich regime with significant forgetting. Finally, our
findings reveal that neural networks achieve optimal performance at a critical
level of feature learning, which depends on task non-stationarity and transfers
across model scales. This work provides a unified perspective on the role of
scale and feature learning in continual learning.

</details>


### [128] [Adaptive Two Sided Laplace Transforms: A Learnable, Interpretable, and Scalable Replacement for Self-Attention](https://arxiv.org/abs/2506.15714)
*Andrew Kiruluta*

Main category: cs.LG

TL;DR: 提出可学习的双边短时拉普拉斯变换（STLT）机制替代传统自注意力机制，在多项任务上表现良好，能处理超长序列。


<details>
  <summary>Details</summary>
Motivation: 解决基于Transformer的大语言模型中传统自注意力机制的计算瓶颈问题，实现超长序列语言建模。

Method: 提出STLT机制，引入可训练参数，选择S个可学习节点，利用快速递归卷积，结合FFT计算相关性矩阵和自适应节点分配机制。

Result: 在语言建模、机器翻译和长文档问答任务中，实现与现有高效Transformer相当或更好的困惑度和分数，能处理超100k标记的上下文长度。

Conclusion: 所提方法兼具可解释性、可扩展性和鲁棒性，为超长序列语言建模提供了途径。

Abstract: We propose an innovative, learnable two-sided short-time Laplace transform
(STLT) mechanism to supplant the traditional self attention in
transformer-based LLMs. Our STLT introduces trainable parameters for each
Laplace node, enabling end-to-end learning of decay rates , oscillatory
frequencies, and window bandwidth T. This flexibility allows the model to
dynamically adapt token relevance half lives and frequency responses during
training. By selecting S learnable nodes and leveraging fast recursive
convolution, we achieve an effective complexity of in time and memory. We
further incorporate an efficient FFT-based computation of the relevance matrix
and an adaptive node allocation mechanism to dynamically adjust the number of
active Laplace nodes. Empirical results on language modeling (WikiText\-103,
Project Gutenberg), machine translation (WMT'14 En\-De), and long document
question answering (NarrativeQA) demonstrate that our learnable STLT achieves
perplexities and scores on par with or better than existing efficient
transformers while naturally extending to context lengths exceeding 100k tokens
or more limited only by available hardware. Ablation studies confirm the
importance of learnable parameters and adaptive node allocation. The proposed
approach combines interpretability, through explicit decay and frequency
parameters, with scalability and robustness, offering a pathway towards
ultra-long-sequence language modeling without the computational bottleneck of
self-attention.

</details>


### [129] [RocketStack: A level-aware deep recursive ensemble learning framework with exploratory feature fusion and model pruning dynamics](https://arxiv.org/abs/2506.16965)
*Çağatay Demirel*

Main category: cs.LG

TL;DR: 提出RocketStack框架实现深度递归集成学习，在多数据集测试有良好表现，证明随机化和压缩有效。


<details>
  <summary>Details</summary>
Motivation: 解决深度堆叠集成学习因模型复杂度、特征冗余和计算负担导致设计少见的问题。

Method: 引入RocketStack框架，逐级修剪弱学习器，添加高斯噪声，使用注意力选择、SFE过滤器和自动编码器进行特征压缩。

Result: 在33个数据集测试中，多数变体准确率随深度上升，部分配置表现优异，提升准确率并减少运行时间和特征维度。

Conclusion: 轻度随机化是有效正则化方法，周期性压缩可起稳定作用，RocketStack能以可处理的复杂度实现深度递归集成。

Abstract: Ensemble learning remains a cornerstone of machine learning, with stacking
used to integrate predictions from multiple base learners through a meta-model.
However, deep stacking remains rare, as most designs prioritize horizontal
diversity over recursive depth due to model complexity, feature redundancy, and
computational burden. To address these challenges, RocketStack, a level-aware
recursive ensemble framework, is introduced and explored up to ten stacking
levels, extending beyond prior architectures. The framework incrementally
prunes weaker learners at each level, enabling deeper stacking without
excessive complexity. To mitigate early performance saturation, mild Gaussian
noise is added to out-of-fold (OOF) scores before pruning, and compared against
strict OOF pruning. Further both per-level and periodic feature compressions
are explored using attention-based selection, Simple, Fast, Efficient (SFE)
filter, and autoencoders. Across 33 datasets (23 binary, 10 multi-class),
linear-trend tests confirmed rising accuracy with depth in most variants, and
the top performing meta-model at each level increasingly outperformed the
strongest standalone ensemble. In the binary subset, periodic SFE with mild
OOF-score randomization reached 97.08% at level 10, 5.14% above the
strict-pruning configuration and cut runtime by 10.5% relative to no
compression. In the multi-class subset, periodic attention selection reached
98.60% at level 10, exceeding the strongest baseline by 6.11%, while reducing
runtime by 56.1% and feature dimensionality by 74% compared to no compression.
These findings highlight mild randomization as an effective regularizer and
periodic compression as a stabilizer. Echoing the design of multistage rockets
in aerospace (prune, compress, propel) RocketStack achieves deep recursive
ensembling with tractable complexity.

</details>


### [130] [NeuronSeek: On Stability and Expressivity of Task-driven Neurons](https://arxiv.org/abs/2506.15715)
*Hanyu Pei,Jing-Xiao Liao,Qibin Zhao,Ting Gao,Shijun Zhang,Xiaoge Zhang,Feng-Lei Fan*

Main category: cs.LG

TL;DR: 本文提出用张量分解（TD）替代符号回归（SR）来发现最优神经元公式，建立理论保证并通过实验证明 NeuronSeek - TD 框架有更好稳定性和竞争力。


<details>
  <summary>Details</summary>
Motivation: 改进现有任务驱动神经元构建方法中符号回归的不足，提高稳定性和收敛速度。

Method: 用张量分解（TD）替代符号回归（SR）来发现最优神经元公式，并建立理论保证。

Result: NeuronSeek - TD 框架具有更好的稳定性，在不同基准测试中与最先进模型相比有竞争力。

Conclusion: NeuronSeek - TD 框架是一种有效的构建任务驱动神经元的方法，有数学理论基础且实验效果好。

Abstract: Drawing inspiration from our human brain that designs different neurons for
different tasks, recent advances in deep learning have explored modifying a
network's neurons to develop so-called task-driven neurons. Prototyping
task-driven neurons (referred to as NeuronSeek) employs symbolic regression
(SR) to discover the optimal neuron formulation and construct a network from
these optimized neurons. Along this direction, this work replaces symbolic
regression with tensor decomposition (TD) to discover optimal neuronal
formulations, offering enhanced stability and faster convergence. Furthermore,
we establish theoretical guarantees that modifying the aggregation functions
with common activation functions can empower a network with a fixed number of
parameters to approximate any continuous function with an arbitrarily small
error, providing a rigorous mathematical foundation for the NeuronSeek
framework. Extensive empirical evaluations demonstrate that our NeuronSeek-TD
framework not only achieves superior stability, but also is competitive
relative to the state-of-the-art models across diverse benchmarks. The code is
available at https://github.com/HanyuPei22/NeuronSeek.

</details>


### [131] [Identifiability of Deep Polynomial Neural Networks](https://arxiv.org/abs/2506.17093)
*Konstantin Usevich,Clara Dérand,Ricardo Borsoi,Marianne Clausel*

Main category: cs.LG

TL;DR: 本文对深度多项式神经网络（PNNs）的可识别性进行全面分析，揭示激活度和层宽度的相互作用，给出架构和参数的条件，解决一个开放猜想并给出激活度界限。


<details>
  <summary>Details</summary>
Motivation: PNNs的可识别性作为确保可解释性的关键属性，目前理解不足，需要进行研究。

Method: 将深度PNNs与低秩张量分解和Kruskal型唯一性定理建立联系，进行构造性证明。

Result: 发现激活度和层宽度在实现可识别性方面存在复杂相互作用；非递增层宽度架构在温和条件下一般可识别；编解码器网络在解码器宽度增长不太快时可识别；解决关于PNN神经簇期望维度的开放猜想，给出激活度达到最大值所需的新界限。

Conclusion: 通过对深度PNNs可识别性的研究，得到了架构和参数相关的可识别性条件，推进了对PNNs的理解。

Abstract: Polynomial Neural Networks (PNNs) possess a rich algebraic and geometric
structure. However, their identifiability -- a key property for ensuring
interpretability -- remains poorly understood. In this work, we present a
comprehensive analysis of the identifiability of deep PNNs, including
architectures with and without bias terms. Our results reveal an intricate
interplay between activation degrees and layer widths in achieving
identifiability. As special cases, we show that architectures with
non-increasing layer widths are generically identifiable under mild conditions,
while encoder-decoder networks are identifiable when the decoder widths do not
grow too rapidly. Our proofs are constructive and center on a connection
between deep PNNs and low-rank tensor decompositions, and Kruskal-type
uniqueness theorems. This yields both generic conditions determined by the
architecture, and effective conditions that depend on the network's parameters.
We also settle an open conjecture on the expected dimension of PNN's
neurovarieties, and provide new bounds on the activation degrees required for
it to reach its maximum.

</details>


### [132] [daDPO: Distribution-Aware DPO for Distilling Conversational Abilities](https://arxiv.org/abs/2506.15717)
*Zhengze Zhang,Shiqi Wang,Yiqun Shen,Simin Guo,Dahua Lin,Xiaoliang Wang,Nguyen Cam-Tu,Fei Tan*

Main category: cs.LG

TL;DR: 本文提出daDPO方法解决小模型对话能力弱问题，通过理论分析和实验验证其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在资源受限环境部署时小模型对话能力差，现有知识蒸馏方法忽略教师模型输出分布。

Method: 引入daDPO，一种用于偏好优化和基于分布蒸馏的统一方法。

Result: daDPO在恢复剪枝模型性能和增强小模型方面优于现有方法，如20%剪枝的Vicuna1.5 - 7B接近教师性能，Qwen2.5 - 1.5B偶尔超过7B教师模型。

Conclusion: daDPO方法有效，能提升小模型对话能力。

Abstract: Large language models (LLMs) have demonstrated exceptional performance across
various applications, but their conversational abilities decline sharply as
model size decreases, presenting a barrier to their deployment in
resource-constrained environments. Knowledge distillation with Direct
Preference Optimization (dDPO) has emerged as a promising approach to enhancing
the conversational abilities of smaller models using a larger teacher model.
However, current methods primarily focus on 'black-box' KD, which only uses the
teacher's responses, overlooking the output distribution offered by the
teacher. This paper addresses this gap by introducing daDPO (Distribution-Aware
DPO), a unified method for preference optimization and distribution-based
distillation. We provide rigorous theoretical analysis and empirical
validation, showing that daDPO outperforms existing methods in restoring
performance for pruned models and enhancing smaller LLM models. Notably, in
in-domain evaluation, our method enables a 20% pruned Vicuna1.5-7B to achieve
near-teacher performance (-7.3% preference rate compared to that of dDPO's
-31%), and allows Qwen2.5-1.5B to occasionally outperform its 7B teacher model
(14.0% win rate).

</details>


### [133] [BuildingBRep-11K: Precise Multi-Storey B-Rep Building Solids with Rich Layout Metadata](https://arxiv.org/abs/2506.15718)
*Yu Guo,Hongji Fang,Tianyu Fang,Zhe Cui*

Main category: cs.LG

TL;DR: 介绍BuildingBRep - 11K数据集，含11978个多层建筑，训练两个PointNet基线验证其可学习性。


<details>
  <summary>Details</summary>
Motivation: 解决自动生成建筑级3D对象模型训练所需的大规模、干净且注释丰富的数据集问题。

Method: 用形状语法驱动的管道生成BuildingBRep - 11K数据集，训练两个轻量级PointNet基线模型。

Result: 多属性回归在100个未见建筑上取得一定误差结果；缺陷检测在100个模型集上达到一定准确率、召回率和精度。

Conclusion: BuildingBRep - 11K数据集对几何回归和拓扑质量评估具有可学习性且有一定挑战性。

Abstract: With the rise of artificial intelligence, the automatic generation of
building-scale 3-D objects has become an active research topic, yet training
such models still demands large, clean and richly annotated datasets. We
introduce BuildingBRep-11K, a collection of 11 978 multi-storey (2-10 floors)
buildings (about 10 GB) produced by a shape-grammar-driven pipeline that
encodes established building-design principles. Every sample consists of a
geometrically exact B-rep solid-covering floors, walls, slabs and rule-based
openings-together with a fast-loading .npy metadata file that records detailed
per-floor parameters. The generator incorporates constraints on spatial scale,
daylight optimisation and interior layout, and the resulting objects pass
multi-stage filters that remove Boolean failures, undersized rooms and extreme
aspect ratios, ensuring compliance with architectural standards. To verify the
dataset's learnability we trained two lightweight PointNet baselines. (i)
Multi-attribute regression. A single encoder predicts storey count, total
rooms, per-storey vector and mean room area from a 4 000-point cloud. On 100
unseen buildings it attains 0.37-storey MAE (87 \% within $\pm1$), 5.7-room
MAE, and 3.2 m$^2$ MAE on mean area. (ii) Defect detection. With the same
backbone we classify GOOD versus DEFECT; on a balanced 100-model set the
network reaches 54 \% accuracy, recalling 82 \% of true defects at 53 \%
precision (41 TP, 9 FN, 37 FP, 13 TN). These pilots show that BuildingBRep-11K
is learnable yet non-trivial for both geometric regression and topological
quality assessment

</details>


### [134] [Consistent Sampling and Simulation: Molecular Dynamics with Energy-Based Diffusion Models](https://arxiv.org/abs/2506.17139)
*Michael Plainer,Hao Wu,Leon Klein,Stephan Günnemann,Frank Noé*

Main category: cs.LG

TL;DR: 扩散模型在生化等领域有效，但用于粗粒度分子动力学模拟有不一致问题，本文提出含正则项的基于能量的扩散模型解决该问题，并在玩具系统和丙氨酸二肽上验证效果。


<details>
  <summary>Details</summary>
Motivation: 扩散模型用于粗粒度分子动力学模拟时，经典扩散推理和模拟生成的样本存在不一致问题，且小扩散时间步下不满足福克 - 普朗克方程。

Method: 提出带有福克 - 普朗克导出的正则化项的基于能量的扩散模型。

Result: 在玩具系统、丙氨酸二肽上证明了方法的有效性，引入了最先进的可转移玻尔兹曼二肽模拟器，具有更好的一致性和高效采样能力。

Conclusion: 提出的模型有效解决了扩散模型在模拟中的不一致问题，提升了模拟效果。

Abstract: Diffusion models have recently gained significant attention due to their
effectiveness in various scientific domains, including biochemistry. When
trained on equilibrium molecular distributions, diffusion models provide both:
a generative procedure to sample equilibrium conformations and associated
forces derived from the model's scores. However, using the forces for
coarse-grained molecular dynamics simulations uncovers inconsistencies in the
samples generated via classical diffusion inference and simulation, despite
both originating from the same model. Particularly at the small diffusion
timesteps required for simulations, diffusion models fail to satisfy the
Fokker-Planck equation, which governs how the score should evolve over time. We
interpret this deviation as an indication of the observed inconsistencies and
propose an energy-based diffusion model with a Fokker-Planck-derived
regularization term enforcing consistency. We demonstrate the effectiveness of
our approach on toy systems, alanine dipeptide, and introduce a
state-of-the-art transferable Boltzmann emulator for dipeptides that supports
simulation and demonstrates enhanced consistency and efficient sampling.

</details>


### [135] [Data-Driven Heat Pump Management: Combining Machine Learning with Anomaly Detection for Residential Hot Water Systems](https://arxiv.org/abs/2506.15719)
*Manal Rahal,Bestoun S. Ahmed,Roger Renstrom,Robert Stener,Albrecht Wurtz*

Main category: cs.LG

TL;DR: 本文提出结合预测性机器学习与异常检测的新方法，为热泵热水生产制定自适应策略，实验表明LightGBM性能最佳，iForest异常检测效果好。


<details>
  <summary>Details</summary>
Motivation: 传统阈值控制方法限制热泵生产热水效率，家庭热水需求预测优化研究不足。

Method: 结合机器学习和隔离森林预测家庭热水需求，采用多步特征选择和时间序列分析，应用并调优三种机器学习模型，在六个真实家庭安装案例中验证。

Result: LightGBM模型表现最佳，RMSE提升达9.37%，R²值在0.748 - 0.983；iForest实现F1分数0.87，误报率5.2%。

Conclusion: 该方法在不同家庭类型和消费模式中表现出强泛化能力，适用于实际热泵部署。

Abstract: Heat pumps (HPs) have emerged as a cost-effective and clean technology for
sustainable energy systems, but their efficiency in producing hot water remains
restricted by conventional threshold-based control methods. Although machine
learning (ML) has been successfully implemented for various HP applications,
optimization of household hot water demand forecasting remains understudied.
This paper addresses this problem by introducing a novel approach that combines
predictive ML with anomaly detection to create adaptive hot water production
strategies based on household-specific consumption patterns. Our key
contributions include: (1) a composite approach combining ML and isolation
forest (iForest) to forecast household demand for hot water and steer
responsive HP operations; (2) multi-step feature selection with advanced
time-series analysis to capture complex usage patterns; (3) application and
tuning of three ML models: Light Gradient Boosting Machine (LightGBM), Long
Short-Term Memory (LSTM), and Bi-directional LSTM with the self-attention
mechanism on data from different types of real HP installations; and (4)
experimental validation on six real household installations. Our experiments
show that the best-performing model LightGBM achieves superior performance,
with RMSE improvements of up to 9.37\% compared to LSTM variants with $R^2$
values between 0.748-0.983. For anomaly detection, our iForest implementation
achieved an F1-score of 0.87 with a false alarm rate of only 5.2\%,
demonstrating strong generalization capabilities across different household
types and consumption patterns, making it suitable for real-world HP
deployments.

</details>


### [136] [Variational Learning of Disentangled Representations](https://arxiv.org/abs/2506.17182)
*Yuli Slavutsky,Ozgur Beker,David Blei,Bianca Dumitrascu*

Main category: cs.LG

TL;DR: 提出新变分框架DISCoVeR分离不变和特定条件因素，理论证明优势，实证表明其在多数据集上有更好解纠缠效果。


<details>
  <summary>Details</summary>
Motivation: 现有VAE扩展框架存在潜在表示泄漏问题，限制泛化能力，需新方法在多条件设置下学习解纠缠表示。

Method: 引入DISCoVeR框架，包含双潜在架构、两个并行重建和新的最大 - 最小目标。

Result: 理论上目标函数能最大化数据似然并促进解纠缠，存在唯一均衡；实证上在合成数据集、自然图像和单细胞RNA - seq数据上实现更好解纠缠。

Conclusion: DISCoVeR是多条件设置下学习解纠缠表示的有效方法。

Abstract: Disentangled representations enable models to separate factors of variation
that are shared across experimental conditions from those that are
condition-specific. This separation is essential in domains such as biomedical
data analysis, where generalization to new treatments, patients, or species
depends on isolating stable biological signals from context-dependent effects.
While extensions of the variational autoencoder (VAE) framework have been
proposed to address this problem, they frequently suffer from leakage between
latent representations, limiting their ability to generalize to unseen
conditions. Here, we introduce DISCoVeR, a new variational framework that
explicitly separates condition-invariant and condition-specific factors.
DISCoVeR integrates three key components: (i) a dual-latent architecture that
models shared and specific factors separately; (ii) two parallel
reconstructions that ensure both representations remain informative; and (iii)
a novel max-min objective that encourages clean separation without relying on
handcrafted priors, while making only minimal assumptions. Theoretically, we
show that this objective maximizes data likelihood while promoting
disentanglement, and that it admits a unique equilibrium. Empirically, we
demonstrate that DISCoVeR achieves improved disentanglement on synthetic
datasets, natural images, and single-cell RNA-seq data. Together, these results
establish DISCoVeR as a principled approach for learning disentangled
representations in multi-condition settings.

</details>


### [137] [Tripartite Weight-Space Ensemble for Few-Shot Class-Incremental Learning](https://arxiv.org/abs/2506.15720)
*Juntae Lee,Munawar Hayat,Sungrack Yun*

Main category: cs.LG

TL;DR: 提出新的FSCIL方法，用Tri - WE和正则化损失项解决灾难性遗忘和过拟合问题，在多个数据集获SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 现有FSCIL方法中固定特征提取器限制模型对新类适应性，存在灾难性遗忘和过拟合问题。

Method: 提出三方权重空间集成（Tri - WE），在权重空间插值模型；提出正则化损失项，利用扩增数据知识蒸馏。

Result: 在miniImageNet、CUB200和CIFAR100数据集上取得了最先进的结果。

Conclusion: 新方法能有效解决FSCIL中的灾难性遗忘和过拟合问题，可无缝更新整个模型。

Abstract: Few-shot class incremental learning (FSCIL) enables the continual learning of
new concepts with only a few training examples. In FSCIL, the model undergoes
substantial updates, making it prone to forgetting previous concepts and
overfitting to the limited new examples. Most recent trend is typically to
disentangle the learning of the representation from the classification head of
the model. A well-generalized feature extractor on the base classes (many
examples and many classes) is learned, and then fixed during incremental
learning. Arguing that the fixed feature extractor restricts the model's
adaptability to new classes, we introduce a novel FSCIL method to effectively
address catastrophic forgetting and overfitting issues. Our method enables to
seamlessly update the entire model with a few examples. We mainly propose a
tripartite weight-space ensemble (Tri-WE). Tri-WE interpolates the base,
immediately previous, and current models in weight-space, especially for the
classification heads of the models. Then, it collaboratively maintains
knowledge from the base and previous models. In addition, we recognize the
challenges of distilling generalized representations from the previous model
from scarce data. Hence, we suggest a regularization loss term using amplified
data knowledge distillation. Simply intermixing the few-shot data, we can
produce richer data enabling the distillation of critical knowledge from the
previous model. Consequently, we attain state-of-the-art results on the
miniImageNet, CUB200, and CIFAR100 datasets.

</details>


### [138] [Optimal Implicit Bias in Linear Regression](https://arxiv.org/abs/2506.17187)
*Kanumuri Nithin Varma,Babak Hassibi*

Main category: cs.LG

TL;DR: 分析过参数化线性回归中插值器泛化性能，找到最优凸隐式偏差。


<details>
  <summary>Details</summary>
Motivation: 解决过参数化情况下哪种隐式偏差能带来最佳泛化性能的问题。

Method: 对过参数化线性回归中凸函数/势最小化得到的插值器泛化性能进行精确渐近分析。

Result: 得到此类插值器最佳泛化误差的紧下界，发现特定条件下能达到该下界的最优凸隐式偏差。

Conclusion: 明确了过参数化线性回归中能带来最佳泛化性能的最优凸隐式偏差。

Abstract: Most modern learning problems are over-parameterized, where the number of
learnable parameters is much greater than the number of training data points.
In this over-parameterized regime, the training loss typically has infinitely
many global optima that completely interpolate the data with varying
generalization performance. The particular global optimum we converge to
depends on the implicit bias of the optimization algorithm. The question we
address in this paper is, ``What is the implicit bias that leads to the best
generalization performance?". To find the optimal implicit bias, we provide a
precise asymptotic analysis of the generalization performance of interpolators
obtained from the minimization of convex functions/potentials for
over-parameterized linear regression with non-isotropic Gaussian data. In
particular, we obtain a tight lower bound on the best generalization error
possible among this class of interpolators in terms of the
over-parameterization ratio, the variance of the noise in the labels, the
eigenspectrum of the data covariance, and the underlying distribution of the
parameter to be estimated. Finally, we find the optimal convex implicit bias
that achieves this lower bound under certain sufficient conditions involving
the log-concavity of the distribution of a Gaussian convolved with the prior of
the true underlying parameter.

</details>


### [139] [Bohdi: Heterogeneous LLM Fusion with Automatic Data Exploration](https://arxiv.org/abs/2506.15721)
*Junqi Gao,Zhichang Guo,Dazhi Zhang,Dong Li,Runze Liu,Pengfei Li,Kai Tian,Biqing Qi*

Main category: cs.LG

TL;DR: 提出Bohdi框架解决异构大语言模型融合问题，实验显示其表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有异构大语言模型融合方法存在依赖有限领域真实数据和固定数据分配比例的问题，无法让目标模型全面获取多领域知识和动态调整能力。

Method: 提出Bohdi框架，将知识域组织成层次树结构，通过多模型协作生成多领域数据；将领域扩展和数据采样比例分配形式化为分层多臂老虎机问题，用DynaBranches机制自适应调整采样比例；结合IR机制通过SWBLRT动态跟踪目标模型能力变化。

Result: 在多个基准测试中，Bohdi显著优于现有基线，数据效率更高，基本消除目标模型能力的不平衡。

Conclusion: Bohdi是一个有效的异构大语言模型融合框架，能解决现有方法的局限性。

Abstract: Heterogeneous Large Language Model (LLM) fusion integrates the strengths of
multiple source LLMs with different architectures into a target LLM with low
computational overhead. While promising, existing methods suffer from two major
limitations: 1) reliance on real data from limited domain for knowledge fusion,
preventing the target LLM from fully acquiring knowledge across diverse
domains, and 2) fixed data allocation proportions across domains, failing to
dynamically adjust according to the target LLM's varying capabilities across
domains, leading to a capability imbalance. To overcome these limitations, we
propose Bohdi, a synthetic-data-only heterogeneous LLM fusion framework.
Through the organization of knowledge domains into a hierarchical tree
structure, Bohdi enables automatic domain exploration and multi-domain data
generation through multi-model collaboration, thereby comprehensively
extracting knowledge from source LLMs. By formalizing domain expansion and data
sampling proportion allocation on the knowledge tree as a Hierarchical
Multi-Armed Bandit problem, Bohdi leverages the designed DynaBranches mechanism
to adaptively adjust sampling proportions based on the target LLM's performance
feedback across domains. Integrated with our proposed Introspection-Rebirth
(IR) mechanism, DynaBranches dynamically tracks capability shifts during target
LLM's updates via Sliding Window Binomial Likelihood Ratio Testing (SWBLRT),
further enhancing its online adaptation capability. Comparative experimental
results on a comprehensive suite of benchmarks demonstrate that Bohdi
significantly outperforms existing baselines on multiple target LLMs, exhibits
higher data efficiency, and virtually eliminates the imbalance in the target
LLM's capabilities. Our code is available at
https://github.com/gjq100/Bohdi.git.

</details>


### [140] [UniMate: A Unified Model for Mechanical Metamaterial Generation, Property Prediction, and Condition Confirmation](https://arxiv.org/abs/2506.15722)
*Wangzhi Zhan,Jianpeng Chen,Dongqi Fu,Dawei Zhou*

Main category: cs.LG

TL;DR: 本文提出统一模型 UNIMATE 处理机械超材料设计三模态问题，实验显示其在多项任务中表现优于基线模型，并开源代码。


<details>
  <summary>Details</summary>
Motivation: 现实复杂场景要求机器学习模型同时考虑机械超材料设计的 3D 拓扑、密度条件和机械属性三种模态，但现有多数工作仅考虑两种，存在差距。

Method: 提出统一模型 UNIMATE，包含模态对齐模块和协同扩散生成模块。

Result: UNIMATE 在拓扑生成、属性预测和条件确认任务中分别比其他基线模型高出 80.2%、5.1% 和 50.2%。

Conclusion: UNIMATE 能有效解决机械超材料设计多模态问题，代码已开源。

Abstract: Metamaterials are artificial materials that are designed to meet unseen
properties in nature, such as ultra-stiffness and negative materials indices.
In mechanical metamaterial design, three key modalities are typically involved,
i.e., 3D topology, density condition, and mechanical property. Real-world
complex application scenarios place the demanding requirements on machine
learning models to consider all three modalities together. However, a
comprehensive literature review indicates that most existing works only
consider two modalities, e.g., predicting mechanical properties given the 3D
topology or generating 3D topology given the required properties. Therefore,
there is still a significant gap for the state-of-the-art machine learning
models capturing the whole. Hence, we propose a unified model named UNIMATE,
which consists of a modality alignment module and a synergetic diffusion
generation module. Experiments indicate that UNIMATE outperforms the other
baseline models in topology generation task, property prediction task, and
condition confirmation task by up to 80.2%, 5.1%, and 50.2%, respectively. We
opensource our proposed UNIMATE model and corresponding results at
https://github.com/wzhan24/UniMate.

</details>


### [141] [MadaKV: Adaptive Modality-Perception KV Cache Eviction for Efficient Multimodal Long-Context Inference](https://arxiv.org/abs/2506.15724)
*Kunxi Li,Zhonghua Jiang,Zhouzhou Shen,Zhaode Wang,Chengfei Lv,Shengyu Zhang,Fan Wu,Fei Wu*

Main category: cs.LG

TL;DR: 介绍MadaKV，一种提升多模态大语言模型长上下文推理效率的模态自适应键值缓存策略，能减少缓存内存和推理延迟，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统键值缓存淘汰方法适用于单模态场景，无法捕捉模态特定信息，在多模态场景中性能不佳，需要新策略提升多模态大语言模型长上下文推理效率。

Method: MadaKV包含模态偏好自适应和分层压缩补偿两个关键组件，通过动态感知注意力头内的模态信息并自适应保留关键令牌。

Result: MadaKV实现了键值缓存内存占用和模型推理解码延迟的显著降低（提升1.3至1.5倍），并在各种多模态长上下文任务中保持高精度。

Conclusion: 通过在代表性多模态大语言模型和MileBench基准上的大量实验，证明MadaKV相比现有键值缓存淘汰方法更有效。

Abstract: This paper introduces MadaKV, a modality-adaptive key-value (KV) cache
eviction strategy designed to enhance the efficiency of multimodal large
language models (MLLMs) in long-context inference. In multimodal scenarios,
attention heads exhibit varying preferences for different modalities, resulting
in significant disparities in modality importance across attention heads.
Traditional KV cache eviction methods, which are tailored for unimodal
settings, fail to capture modality-specific information, thereby yielding
suboptimal performance. MadaKV addresses these challenges through two key
components: modality preference adaptation and hierarchical compression
compensation. By dynamically sensing modality information within attention
heads and adaptively retaining critical tokens, MadaKV achieves substantial
reductions in KV cache memory footprint and model inference decoding latency
(1.3 to 1.5 times improvement) while maintaining high accuracy across various
multimodal long-context tasks. Extensive experiments on representative MLLMs
and the MileBench benchmark demonstrate the effectiveness of MadaKV compared to
existing KV cache eviction methods.

</details>


### [142] [Graph Diffusion that can Insert and Delete](https://arxiv.org/abs/2506.15725)
*Matteo Ninniri,Marco Podda,Davide Bacciu*

Main category: cs.LG

TL;DR: 本文提出GrIDDD模型，支持节点的插入和删除，可在生成时动态调整化学图大小，在分子属性靶向和优化任务中表现良好，为自适应大小的分子生成铺平道路。


<details>
  <summary>Details</summary>
Motivation: 现有基于离散去噪扩散概率模型的图生成模型在扩散过程中无法调整图大小，限制了其在条件生成场景中的有效性。

Method: 重新制定加噪和去噪过程，支持节点的单调插入和删除，得到GrIDDD模型。

Result: GrIDDD在分子属性靶向任务中表现与现有模型相当或更优，在分子优化任务中与专业优化模型相比具有竞争力。

Conclusion: 该工作为基于图扩散的自适应大小分子生成提供了方向。

Abstract: Generative models of graphs based on discrete Denoising Diffusion
Probabilistic Models (DDPMs) offer a principled approach to molecular
generation by systematically removing structural noise through iterative atom
and bond adjustments. However, existing formulations are fundamentally limited
by their inability to adapt the graph size (that is, the number of atoms)
during the diffusion process, severely restricting their effectiveness in
conditional generation scenarios such as property-driven molecular design,
where the targeted property often correlates with the molecular size. In this
paper, we reformulate the noising and denoising processes to support monotonic
insertion and deletion of nodes. The resulting model, which we call GrIDDD,
dynamically grows or shrinks the chemical graph during generation. GrIDDD
matches or exceeds the performance of existing graph diffusion models on
molecular property targeting despite being trained on a more difficult problem.
Furthermore, when applied to molecular optimization, GrIDDD exhibits
competitive performance compared to specialized optimization models. This work
paves the way for size-adaptive molecular generation with graph diffusion.

</details>


### [143] [Descriptor-based Foundation Models for Molecular Property Prediction](https://arxiv.org/abs/2506.15792)
*Jackson Burns,Akshat Zalte,William Green*

Main category: cs.LG

TL;DR: 本文介绍分子基础模型CheMeleon，基于低噪声分子描述符学习，在多数据集表现优于基线模型，凸显描述符预训练在分子属性预测潜力。


<details>
  <summary>Details</summary>
Motivation: 快速准确地用机器学习预测分子属性对多领域科研进展至关重要，基础模型在小真实数据集训练有效，因此研究新的分子基础模型。

Method: 引入CheMeleon模型，基于Mordred包的确定性分子描述符预训练，用有向消息传递神经网络在无噪声环境预测描述符。

Result: 在58个基准数据集评估，CheMeleon在Polaris任务胜率79%，MoleculeACE测定胜率97%，均超基线模型；t - SNE投影显示能分离化学系列。

Conclusion: 基于描述符的预训练对可扩展且有效的分子属性预测有潜力，为探索描述符集和无标签数据集提供方向。

Abstract: Fast and accurate prediction of molecular properties with machine learning is
pivotal to scientific advancements across myriad domains. Foundation models in
particular have proven especially effective, enabling accurate training on
small, real-world datasets. This study introduces CheMeleon, a novel molecular
foundation model pre-trained on deterministic molecular descriptors from the
Mordred package, leveraging a Directed Message-Passing Neural Network to
predict these descriptors in a noise-free setting. Unlike conventional
approaches relying on noisy experimental data or biased quantum mechanical
simulations, CheMeleon uses low-noise molecular descriptors to learn rich
molecular representations. Evaluated on 58 benchmark datasets from Polaris and
MoleculeACE, CheMeleon achieves a win rate of 79% on Polaris tasks,
outperforming baselines like Random Forest (46%), fastprop (39%), and Chemprop
(36%), and a 97% win rate on MoleculeACE assays, surpassing Random Forest (63%)
and other foundation models. However, it struggles to distinguish activity
cliffs like many of the tested models. The t-SNE projection of CheMeleon's
learned representations demonstrates effective separation of chemical series,
highlighting its ability to capture structural nuances. These results
underscore the potential of descriptor-based pre-training for scalable and
effective molecular property prediction, opening avenues for further
exploration of descriptor sets and unlabeled datasets.

</details>


### [144] [DeepJ: Graph Convolutional Transformers with Differentiable Pooling for Patient Trajectory Modeling](https://arxiv.org/abs/2506.15809)
*Deyi Li,Zijun Yao,Muxuan Liang,Mei Liu*

Main category: cs.LG

TL;DR: 本文提出 DeepJ 模型解决现有图学习方法在处理电子健康记录数据时的局限，能有效捕捉医疗事件交互，性能优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有基于图的方法在处理电子健康记录数据时多为静态方式，难以跨会诊建模交互并考虑时间依赖，无法识别跨纵向会诊的有意义医疗事件组。

Method: 引入具有可微图池化的图卷积变压器模型 DeepJ，以捕捉会诊内和会诊间的医疗事件交互。

Result: DeepJ 显著优于五个最先进的基线模型，同时增强了可解释性。

Conclusion: DeepJ 有潜力改善患者风险分层。

Abstract: In recent years, graph learning has gained significant interest for modeling
complex interactions among medical events in structured Electronic Health
Record (EHR) data. However, existing graph-based approaches often work in a
static manner, either restricting interactions within individual encounters or
collapsing all historical encounters into a single snapshot. As a result, when
it is necessary to identify meaningful groups of medical events spanning
longitudinal encounters, existing methods are inadequate in modeling
interactions cross encounters while accounting for temporal dependencies. To
address this limitation, we introduce Deep Patient Journey (DeepJ), a novel
graph convolutional transformer model with differentiable graph pooling to
effectively capture intra-encounter and inter-encounter medical event
interactions. DeepJ can identify groups of temporally and functionally related
medical events, offering valuable insights into key event clusters pertinent to
patient outcome prediction. DeepJ significantly outperformed five
state-of-the-art baseline models while enhancing interpretability,
demonstrating its potential for improved patient risk stratification.

</details>


### [145] [Optimizing Bidding Strategies in First-Price Auctions in Binary Feedback Setting with Predictions](https://arxiv.org/abs/2506.15817)
*Jason Tandiary*

Main category: cs.LG

TL;DR: 本文研究二元反馈下的维克里首价拍卖，提出新算法提升性能，在准确预测下实现零遗憾并确定有界遗憾界。


<details>
  <summary>Details</summary>
Motivation: 首价拍卖的重要性增加以及机器学习模型的预测能力促使研究提出新算法。

Method: 利用机器学习算法性能，在BROAD - OMD框架下，借助对最高竞争出价的预测改进算法。

Result: 新算法在准确预测下实现零遗憾，在一定正态条件下建立了O(T^(3/4) * Vt^(1/4))的有界遗憾界。

Conclusion: 提出的新算法在维克里首价拍卖中有良好表现，能有效改善遗憾界。

Abstract: This paper studies Vickrey first-price auctions under binary feedback.
Leveraging the enhanced performance of machine learning algorithms, the new
algorithm uses past information to improve the regret bounds of the BROAD-OMD
algorithm. Motivated by the growing relevance of first-price auctions and the
predictive capabilities of machine learning models, this paper proposes a new
algorithm within the BROAD-OMD framework (Hu et al., 2025) that leverages
predictions of the highest competing bid. This paper's main contribution is an
algorithm that achieves zero regret under accurate predictions. Additionally, a
bounded regret bound of O(T^(3/4) * Vt^(1/4)) is established under certain
normality conditions.

</details>


### [146] [AI-based modular warning machine for risk identification in proximity healthcare](https://arxiv.org/abs/2506.15823)
*Chiara Razzetta,Shahryar Noei,Federico Barbarossa,Edoardo Spairani,Monica Roascio,Elisa Barbi,Giulia Ciacci,Sara Sommariva,Sabrina Guastavino,Michele Piana,Matteo Lenge,Gabriele Arnulfo,Giovanni Magenes,Elvira Maranesi,Giulio Amabili,Anna Maria Massone,Federico Benvenuto,Giuseppe Jurman,Diego Sona,Cristina Campi*

Main category: cs.LG

TL;DR: 介绍DHEAL - COM项目及研究提出的自动化数据处理分析流程


<details>
  <summary>Details</summary>
Motivation: 开发社区医疗数字解决方案，处理项目收集的多模态数据

Method: 采用大量无监督和有监督方法构建自动化管道

Result: 可处理数据、提供预测结果、通过特征识别促进模型解释

Conclusion: 未提及明确结论

Abstract: "DHEAL-COM - Digital Health Solutions in Community Medicine" is a research
and technology project funded by the Italian Department of Health for the
development of digital solutions of interest in proximity healthcare. The
activity within the DHEAL-COM framework allows scientists to gather a notable
amount of multi-modal data whose interpretation can be performed by means of
machine learning algorithms. The present study illustrates a general automated
pipeline made of numerous unsupervised and supervised methods that can ingest
such data, provide predictive results, and facilitate model interpretations via
feature identification.

</details>


### [147] [Heterogeneous Federated Reinforcement Learning Using Wasserstein Barycenters](https://arxiv.org/abs/2506.15825)
*Luiz Pereira,M. Hadi Amini*

Main category: cs.LG

TL;DR: 提出利用Wasserstein重心的模型融合算法FedWB，用于分布式架构训练全局DNN，并基于此开发算法解决异构联邦强化学习问题，在CartPole问题验证。


<details>
  <summary>Details</summary>
Motivation: 提出新的模型融合算法，解决异构联邦强化学习问题。

Method: 将数据集等分，各“代理”在本地数据集上训练DNN，用Wasserstein重心聚合网络权重得到FedWB算法；在不同长度杆的CartPole环境训练DQN，偶尔进行全局聚合。

Result: 得到能在所有环境中运行的全局DQN。

Conclusion: 提出的算法可有效训练全局模型，适用于异构环境。

Abstract: In this paper, we first propose a novel algorithm for model fusion that
leverages Wasserstein barycenters in training a global Deep Neural Network
(DNN) in a distributed architecture. To this end, we divide the dataset into
equal parts that are fed to "agents" who have identical deep neural networks
and train only over the dataset fed to them (known as the local dataset). After
some training iterations, we perform an aggregation step where we combine the
weight parameters of all neural networks using Wasserstein barycenters. These
steps form the proposed algorithm referred to as FedWB. Moreover, we leverage
the processes created in the first part of the paper to develop an algorithm to
tackle Heterogeneous Federated Reinforcement Learning (HFRL). Our test
experiment is the CartPole toy problem, where we vary the lengths of the poles
to create heterogeneous environments. We train a deep Q-Network (DQN) in each
environment to learn to control each cart, while occasionally performing a
global aggregation step to generalize the local models; the end outcome is a
global DQN that functions across all environments.

</details>


### [148] [In-field Calibration of Low-Cost Sensors through XGBoost $\&$ Aggregate Sensor Data](https://arxiv.org/abs/2506.15840)
*Kevin Yin,Julia Gersey,Pei Zhang*

Main category: cs.LG

TL;DR: 本文提出用XGBoost集成学习进行现场传感器校准的模型，减少对单个传感器准确性依赖，提升不同地点泛化性。


<details>
  <summary>Details</summary>
Motivation: 大规模空气质量监测需分布式传感，但高精度传感器贵、覆盖受限，低成本传感器易漂移。

Method: 使用XGBoost集成学习整合相邻传感器数据进行现场传感器校准。

Result: 未提及具体结果。

Conclusion: 该方法可减少对单个传感器准确性的依赖，提高不同地点的泛化性。

Abstract: Effective large-scale air quality monitoring necessitates distributed sensing
due to the pervasive and harmful nature of particulate matter (PM),
particularly in urban environments. However, precision comes at a cost: highly
accurate sensors are expensive, limiting the spatial deployments and thus their
coverage. As a result, low-cost sensors have become popular, though they are
prone to drift caused by environmental sensitivity and manufacturing
variability. This paper presents a model for in-field sensor calibration using
XGBoost ensemble learning to consolidate data from neighboring sensors. This
approach reduces dependence on the presumed accuracy of individual sensors and
improves generalization across different locations.

</details>


### [149] [Uncertainty Estimation by Human Perception versus Neural Models](https://arxiv.org/abs/2506.15850)
*Pedro Mendes,Paolo Romano,David Garlan*

Main category: cs.LG

TL;DR: 研究神经网络不确定性估计与人类感知不确定性的相关性，发现当前方法与人类直觉弱相关，用人类软标签训练可改善校准。


<details>
  <summary>Details</summary>
Motivation: 现代神经网络预测精度高但校准不佳，在需可靠不确定性估计的应用中存在问题，因此研究其与人类感知不确定性的比较。

Method: 利用三个标注了人类分歧和众包信心的视觉基准，评估模型预测不确定性与人类感知不确定性的相关性。

Result: 当前方法与人类直觉弱相关，相关性因任务和不确定性指标而异，使用人类软标签训练可在不影响精度的情况下改善校准。

Conclusion: 模型和人类不确定性存在持续差距，利用人类见解有助于开发更可信的AI系统。

Abstract: Modern neural networks (NNs) often achieve high predictive accuracy but
remain poorly calibrated, producing overconfident predictions even when wrong.
This miscalibration poses serious challenges in applications where reliable
uncertainty estimates are critical. In this work, we investigate how human
perceptual uncertainty compares to uncertainty estimated by NNs. Using three
vision benchmarks annotated with both human disagreement and crowdsourced
confidence, we assess the correlation between model-predicted uncertainty and
human-perceived uncertainty. Our results show that current methods only weakly
align with human intuition, with correlations varying significantly across
tasks and uncertainty metrics. Notably, we find that incorporating
human-derived soft labels into the training process can improve calibration
without compromising accuracy. These findings reveal a persistent gap between
model and human uncertainty and highlight the potential of leveraging human
insights to guide the development of more trustworthy AI systems.

</details>


### [150] [Improving Rectified Flow with Boundary Conditions](https://arxiv.org/abs/2506.15864)
*Xixi Hu,Runlong Liao,Keyang Xu,Bo Liu,Yeqing Li,Eugene Ie,Hongliang Fei,Qiang Liu*

Main category: cs.LG

TL;DR: 指出Rectified Flow直接用无约束神经网络建模速度的局限，提出Boundary RF Model，该模型改进了性能。


<details>
  <summary>Details</summary>
Motivation: Rectified Flow直接用无约束神经网络建模速度存在局限，学习的速度常不满足边界条件，导致速度场估计不准确。

Method: 提出Boundary RF Model，通过最小代码修改来强制执行边界条件。

Result: Boundary RF Model比原始RF模型性能更好，在ImageNet上使用ODE采样FID分数提高8.01%，使用SDE采样提高8.98%。

Conclusion: Boundary RF Model能有效解决Rectified Flow直接建模速度的问题，提升生成模型性能。

Abstract: Rectified Flow offers a simple and effective approach to high-quality
generative modeling by learning a velocity field. However, we identify a
limitation in directly modeling the velocity with an unconstrained neural
network: the learned velocity often fails to satisfy certain boundary
conditions, leading to inaccurate velocity field estimations that deviate from
the desired ODE. This issue is particularly critical during stochastic sampling
at inference, as the score function's errors are amplified near the boundary.
To mitigate this, we propose a Boundary-enforced Rectified Flow Model (Boundary
RF Model), in which we enforce boundary conditions with a minimal code
modification. Boundary RF Model improves performance over vanilla RF model,
demonstrating 8.01% improvement in FID score on ImageNet using ODE sampling and
8.98% improvement using SDE sampling.

</details>


### [151] [Hidden Breakthroughs in Language Model Training](https://arxiv.org/abs/2506.15872)
*Sara Kangaslahti,Elan Rosenfeld,Naomi Saphra*

Main category: cs.LG

TL;DR: 论文提出POLCA方法分解损失变化，识别模型训练中隐藏的突破，在合成算术和自然语言任务验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 模型训练中损失曲线大多平滑，隐藏的概念突破被单一损失指标掩盖，为深入理解学习动态，需找出这些隐藏转变。

Method: 引入POLCA方法，沿低秩训练子空间的任意基分解损失变化，识别训练中损失变化相似的样本簇。

Result: 在合成算术和自然语言任务上验证，POLCA能恢复代表模型能力可解释突破的簇。

Conclusion: 模型训练中的隐藏相变有望作为无监督可解释性的工具。

Abstract: Loss curves are smooth during most of model training, so visible
discontinuities stand out as possible conceptual breakthroughs. Studying these
breakthroughs enables a deeper understanding of learning dynamics, but only
when they are properly identified. This paper argues that similar breakthroughs
occur frequently throughout training but they are obscured by a loss metric
that collapses all variation into a single scalar. To find these hidden
transitions, we introduce POLCA, a method for decomposing changes in loss along
arbitrary bases of the low-rank training subspace. We use our method to
identify clusters of samples that share similar changes in loss during
training, disaggregating the overall loss into that of smaller groups of
conceptually similar data. We validate our method on synthetic arithmetic and
natural language tasks, showing that POLCA recovers clusters that represent
interpretable breakthroughs in the model's capabilities. We demonstrate the
promise of these hidden phase transitions as a tool for unsupervised
interpretability.

</details>


### [152] [Job Market Cheat Codes: Prototyping Salary Prediction and Job Grouping with Synthetic Job Listings](https://arxiv.org/abs/2506.15879)
*Abdel Rahman Alsheyab,Mohammad Alkhasawneh,Nidal Shahin*

Main category: cs.LG

TL;DR: 本文用大型合成招聘数据集构建机器学习方法原型，识别趋势、预测薪资和对相似职位分组，虽基于合成数据，但展示了可迁移的就业市场分析框架。


<details>
  <summary>Details</summary>
Motivation: 揭示影响就业市场动态的关键特征，为求职者、雇主和研究人员提供有价值的见解。

Method: 采用回归、分类、聚类和自然语言处理等技术，进行探索性数据分析，开发回归模型预测薪资、分类模型预测职位名称，用聚类技术对相似工作分组。

Result: 分析揭示了影响薪资和职位的重要因素，根据数据确定了不同的职位集群。

Conclusion: 该方法虽基于合成数据不用于实际部署，但展示了可迁移的就业市场分析框架。

Abstract: This paper presents a machine learning methodology prototype using a large
synthetic dataset of job listings to identify trends, predict salaries, and
group similar job roles. Employing techniques such as regression,
classification, clustering, and natural language processing (NLP) for
text-based feature extraction and representation, this study aims to uncover
the key features influencing job market dynamics and provide valuable insights
for job seekers, employers, and researchers. Exploratory data analysis was
conducted to understand the dataset's characteristics. Subsequently, regression
models were developed to predict salaries, classification models to predict job
titles, and clustering techniques were applied to group similar jobs. The
analyses revealed significant factors influencing salary and job roles, and
identified distinct job clusters based on the provided data. While the results
are based on synthetic data and not intended for real-world deployment, the
methodology demonstrates a transferable framework for job market analysis.

</details>


### [153] [T-SHRED: Symbolic Regression for Regularization and Model Discovery with Transformer Shallow Recurrent Decoders](https://arxiv.org/abs/2506.15881)
*Alexey Yermakov,David Zoro,Mars Liyao Gao,J. Nathan Kutz*

Main category: cs.LG

TL;DR: 本文提出T - SHRED改进SHRED，引入SINDy注意力机制，在不同动力系统上测试显示其能基于可解释符号模型准确预测未来帧。


<details>
  <summary>Details</summary>
Motivation: 改进SHRED模型，提高其在大数据集上的下一步状态预测性能，并增强模型可解释性。

Method: 利用transformers进行时间编码得到T - SHRED，引入SINDy注意力机制进行符号回归作为模型正则化架构的一部分。

Result: SINDy注意力T - SHRED在所有测试数据集上都能基于可解释符号模型准确预测未来帧。

Conclusion: T - SHRED结合SINDy注意力机制在不同数据规模的动力系统预测上有效且具有可解释性。

Abstract: SHallow REcurrent Decoders (SHRED) are effective for system identification
and forecasting from sparse sensor measurements. Such models are light-weight
and computationally efficient, allowing them to be trained on consumer laptops.
SHRED-based models rely on Recurrent Neural Networks (RNNs) and a simple
Multi-Layer Perceptron (MLP) for the temporal encoding and spatial decoding
respectively. Despite the relatively simple structure of SHRED, they are able
to predict chaotic dynamical systems on different physical, spatial, and
temporal scales directly from a sparse set of sensor measurements. In this
work, we improve SHRED by leveraging transformers (T-SHRED) for the temporal
encoding which improves performance on next-step state prediction on large
datasets. We also introduce a sparse identification of nonlinear dynamics
(SINDy) attention mechanism into T-SHRED to perform symbolic regression
directly on the latent space as part of the model regularization architecture.
Symbolic regression improves model interpretability by learning and
regularizing the dynamics of the latent space during training. We analyze the
performance of T-SHRED on three different dynamical systems ranging from
low-data to high-data regimes. We observe that SINDy attention T-SHRED
accurately predicts future frames based on an interpretable symbolic model
across all tested datasets.

</details>


### [154] [Fractional Reasoning via Latent Steering Vectors Improves Inference Time Compute](https://arxiv.org/abs/2506.15882)
*Sheng Liu,Tianlang Chen,Pan Lu,Haotian Ye,Yizheng Chen,Lei Xing,James Zou*

Main category: cs.LG

TL;DR: 提出无训练、模型无关的Fractional Reasoning框架，实现推理强度连续控制，实验证明能提升多任务和模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法统一推理，未考虑不同问题所需推理深度不同。

Method: 提取与深度推理相关的潜在引导向量，用可调缩放因子重新应用，支持两种测试时间缩放模式。

Result: 在GSM8K、MATH500和GPQA上实验表明，Fractional Reasoning持续提升不同推理任务和模型性能。

Conclusion: Fractional Reasoning能有效突破固定指令提示限制，根据输入复杂度调整推理过程，提升推理性能。

Abstract: Test-time compute has emerged as a powerful paradigm for improving the
performance of large language models (LLMs), where generating multiple outputs
or refining individual chains can significantly boost answer accuracy. However,
existing methods like Best-of-N, majority voting, and self-reflection typically
apply reasoning in a uniform way across inputs, overlooking the fact that
different problems may require different levels of reasoning depth. In this
work, we propose Fractional Reasoning, a training-free and model-agnostic
framework that enables continuous control over reasoning intensity at inference
time, going beyond the limitations of fixed instructional prompts. Our method
operates by extracting the latent steering vector associated with deeper
reasoning and reapplying it with a tunable scaling factor, allowing the model
to tailor its reasoning process to the complexity of each input. This supports
two key modes of test-time scaling: (1) improving output quality in
breadth-based strategies (e.g., Best-of-N, majority voting), and (2) enhancing
the correctness of individual reasoning chains in depth-based strategies (e.g.,
self-reflection). Experiments on GSM8K, MATH500, and GPQA demonstrate that
Fractional Reasoning consistently improves performance across diverse reasoning
tasks and models.

</details>


### [155] [Formal Models of Active Learning from Contrastive Examples](https://arxiv.org/abs/2506.15893)
*Farnam Mansouri,Hans U. Simon,Adish Singla,Yuxin Chen,Sandra Zilles*

Main category: cs.LG

TL;DR: 本文提出理论框架研究对比示例对主动学习者的影响，聚焦样本复杂度及与自指导学习的联系，并以几何概念类和布尔函数类说明结果。


<details>
  <summary>Details</summary>
Motivation: 对比训练示例对机器学习有益，但缺乏对不同类型对比示例对主动学习者影响的正式研究。

Method: 提出理论框架，研究不同类型对比示例对主动学习者的影响，聚焦学习概念类的样本复杂度。

Result: 揭示了从对比示例学习与自指导学习经典模型之间的联系，并以几何概念类和布尔函数类说明结果。

Conclusion: 该理论框架有助于深入理解对比示例对主动学习的影响及与自指导学习的关系。

Abstract: Machine learning can greatly benefit from providing learning algorithms with
pairs of contrastive training examples -- typically pairs of instances that
differ only slightly, yet have different class labels. Intuitively, the
difference in the instances helps explain the difference in the class labels.
This paper proposes a theoretical framework in which the effect of various
types of contrastive examples on active learners is studied formally. The focus
is on the sample complexity of learning concept classes and how it is
influenced by the choice of contrastive examples. We illustrate our results
with geometric concept classes and classes of Boolean functions. Interestingly,
we reveal a connection between learning from contrastive examples and the
classical model of self-directed learning.

</details>


### [156] [KG-FGNN: Knowledge-guided GNN Foundation Model for Fertilisation-oriented Soil GHG Flux Prediction](https://arxiv.org/abs/2506.15896)
*Yu Zhang,Gaoshan Bi,Simon Jeffery,Max Davis,Yang Li,Qing Xue,Po Yang*

Main category: cs.LG

TL;DR: 本文提出知识引导的图神经网络框架解决农业数据稀缺问题，用于精准土壤温室气体通量预测，实验表明该方法有更高准确性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 精准土壤温室气体通量预测对农业系统很重要，但多数农场缺乏先进技术获取数据，阻碍机器学习应用。

Method: 结合农业过程模型知识和图神经网络技术，利用农业过程模型生成多维农业数据集，提出结合自编码器和多目标多图的图神经网络的机器学习框架。

Result: 与知名基线和先进回归方法对比实验显示，该方法在面向施肥的土壤温室气体预测中准确性和稳定性更优。

Conclusion: 所提方法能有效解决农业数据稀缺问题，实现精准土壤温室气体通量预测。

Abstract: Precision soil greenhouse gas (GHG) flux prediction is essential in
agricultural systems for assessing environmental impacts, developing emission
mitigation strategies and promoting sustainable agriculture. Due to the lack of
advanced sensor and network technologies on majority of farms, there are
challenges in obtaining comprehensive and diverse agricultural data. As a
result, the scarcity of agricultural data seriously obstructs the application
of machine learning approaches in precision soil GHG flux prediction. This
research proposes a knowledge-guided graph neural network framework that
addresses the above challenges by integrating knowledge embedded in an
agricultural process-based model and graph neural network techniques.
Specifically, we utilise the agricultural process-based model to simulate and
generate multi-dimensional agricultural datasets for 47 countries that cover a
wide range of agricultural variables. To extract key agricultural features and
integrate correlations among agricultural features in the prediction process,
we propose a machine learning framework that integrates the autoencoder and
multi-target multi-graph based graph neural networks, which utilises the
autoencoder to selectively extract significant agricultural features from the
agricultural process-based model simulation data and the graph neural network
to integrate correlations among agricultural features for accurately predict
fertilisation-oriented soil GHG fluxes. Comprehensive experiments were
conducted with both the agricultural simulation dataset and real-world
agricultural dataset to evaluate the proposed approach in comparison with
well-known baseline and state-of-the-art regression methods. The results
demonstrate that our proposed approach provides superior accuracy and stability
in fertilisation-oriented soil GHG prediction.

</details>


### [157] [TrajDiff: Diffusion Bridge Network with Semantic Alignment for Trajectory Similarity Computation](https://arxiv.org/abs/2506.15898)
*Xiao Zhang,Xingyu Zhao,Hong Xia,Yuan Cao,Guiyuan Jiang,Junyu Dong,Yanwei Yu*

Main category: cs.LG

TL;DR: 随着轨迹数据增多，现有基于学习的轨迹相似度计算方法有问题，提出TrajDiff框架，实验显示其优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的轨迹相似度计算方法存在忽略语义差距、受噪声影响和未利用全局排序信息等问题。

Method: 提出TrajDiff框架，包括语义对齐模块、基于DDBM的抗噪预训练和整体排序感知正则化。

Result: 在三个公开数据集上的实验表明，TrajDiff始终优于现有基线，在所有三个评估指标和数据集上平均HR@1增益为33.38%。

Conclusion: TrajDiff能有效解决现有方法存在的问题，性能表现良好。

Abstract: With the proliferation of location-tracking technologies, massive volumes of
trajectory data are continuously being collected. As a fundamental task in
trajectory data mining, trajectory similarity computation plays a critical role
in a wide range of real-world applications. However, existing learning-based
methods face three challenges: First, they ignore the semantic gap between GPS
and grid features in trajectories, making it difficult to obtain meaningful
trajectory embeddings. Second, the noise inherent in the trajectories, as well
as the noise introduced during grid discretization, obscures the true motion
patterns of the trajectories. Third, existing methods focus solely on
point-wise and pair-wise losses, without utilizing the global ranking
information obtained by sorting all trajectories according to their similarity
to a given trajectory. To address the aforementioned challenges, we propose a
novel trajectory similarity computation framework, named TrajDiff.
Specifically, the semantic alignment module relies on cross-attention and an
attention score mask mechanism with adaptive fusion, effectively eliminating
semantic discrepancies between data at two scales and generating a unified
representation. Additionally, the DDBM-based Noise-robust Pre-Training
introduces the transfer patterns between any two trajectories into the model
training process, enhancing the model's noise robustness. Finally, the overall
ranking-aware regularization shifts the model's focus from a local to a global
perspective, enabling it to capture the holistic ordering information among
trajectories. Extensive experiments on three publicly available datasets show
that TrajDiff consistently outperforms state-of-the-art baselines. In
particular, it achieves an average HR@1 gain of 33.38% across all three
evaluation metrics and datasets.

</details>


### [158] [Clinically Interpretable Mortality Prediction for ICU Patients with Diabetes and Atrial Fibrillation: A Machine Learning Approach](https://arxiv.org/abs/2506.15901)
*Li Sun,Shuheng Chen,Yong Si,Junyi Fan,Maryam Pishgar,Elham Pishgar,Kamiar Alaei,Greg Placencia*

Main category: cs.LG

TL;DR: 本文用早期临床数据为糖尿病和房颤并存的ICU患者开发可解释机器学习模型预测28天死亡率，逻辑回归模型表现最佳，模型可用于早期分流。


<details>
  <summary>Details</summary>
Motivation: 糖尿病和房颤并存的ICU患者死亡率高，但针对该高危群体的模型有限，需开发预测模型。

Method: 从MIMIC - IV数据库提取1535例患者数据，经预处理、特征工程和特征选择，得到19个可解释特征，训练7个ML模型，用分层5折交叉验证和SMOTE过采样，用消融和ALE分析评估可解释性。

Result: 逻辑回归性能最佳（AUROC: 0.825; 95% CI: 0.779 - 0.867），关键预测因子有RAS、年龄等，ALE图显示非线性效应。

Conclusion: 可解释ML模型能准确预测风险，为患者早期分流提供临床见解。

Abstract: Background: Patients with both diabetes mellitus (DM) and atrial fibrillation
(AF) face elevated mortality in intensive care units (ICUs), yet models
targeting this high-risk group remain limited.
  Objective: To develop an interpretable machine learning (ML) model predicting
28-day mortality in ICU patients with concurrent DM and AF using early-phase
clinical data.
  Methods: A retrospective cohort of 1,535 adult ICU patients with DM and AF
was extracted from the MIMIC-IV database. Data preprocessing involved
median/mode imputation, z-score normalization, and early temporal feature
engineering. A two-step feature selection pipeline-univariate filtering (ANOVA
F-test) and Random Forest-based multivariate ranking-yielded 19 interpretable
features. Seven ML models were trained with stratified 5-fold cross-validation
and SMOTE oversampling. Interpretability was assessed via ablation and
Accumulated Local Effects (ALE) analysis.
  Results: Logistic regression achieved the best performance (AUROC: 0.825; 95%
CI: 0.779-0.867), surpassing more complex models. Key predictors included RAS,
age, bilirubin, and extubation. ALE plots showed intuitive, non-linear effects
such as age-related risk acceleration and bilirubin thresholds.
  Conclusion: This interpretable ML model offers accurate risk prediction and
clinical insights for early ICU triage in patients with DM and AF.

</details>


### [159] [VectorEdits: A Dataset and Benchmark for Instruction-Based Editing of Vector Graphics](https://arxiv.org/abs/2506.15903)
*Josef Kuchař,Marek Kadlčík,Michal Spiegel,Michal Štefánik*

Main category: cs.LG

TL;DR: 提出用于指令引导矢量图像编辑的大规模数据集，含超27万对SVG图像与编辑指令对，指出当前方法挑战并公开资源。


<details>
  <summary>Details</summary>
Motivation: 为基于文本指令修改矢量图形的模型提供训练和评估数据，推动自然语言驱动矢量图形生成和编辑研究。

Method: 通过CLIP相似度进行图像配对，用视觉语言模型生成指令来收集数据。

Result: 用最先进大语言模型实验表明，当前方法难以产生准确有效的编辑。

Conclusion: 此任务具有挑战性，公开研究资源以促进相关研究。

Abstract: We introduce a large-scale dataset for instruction-guided vector image
editing, consisting of over 270,000 pairs of SVG images paired with natural
language edit instructions. Our dataset enables training and evaluation of
models that modify vector graphics based on textual commands. We describe the
data collection process, including image pairing via CLIP similarity and
instruction generation with vision-language models. Initial experiments with
state-of-the-art large language models reveal that current methods struggle to
produce accurate and valid edits, underscoring the challenge of this task. To
foster research in natural language-driven vector graphic generation and
editing, we make our resources created within this work publicly available.

</details>


### [160] [Pieceformer: Similarity-Driven Knowledge Transfer via Scalable Graph Transformer in VLSI](https://arxiv.org/abs/2506.15907)
*Hang Yang,Yusheng Hu,Yong Liu,Cong,Hao*

Main category: cs.LG

TL;DR: 提出Pieceformer框架用于VLSI设计图相似度评估，在数据集上表现良好，案例研究显示能减少运行时间，验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 准确的图相似度对VLSI设计的知识迁移至关重要，为实现设计重用、减少工程工作量和周转时间。

Method: 提出Pieceformer框架，采用混合消息传递和图Transformer编码器，结合线性Transformer骨干和分区训练管道。

Result: 在数据集上MAE比基线降低24.9%，能正确聚类所有真实设计组，案例研究中运行时间最多减少89%。

Conclusion: 该框架对现代VLSI系统的可扩展、无偏设计重用有效。

Abstract: Accurate graph similarity is critical for knowledge transfer in VLSI design,
enabling the reuse of prior solutions to reduce engineering effort and
turnaround time. We propose Pieceformer, a scalable, self-supervised similarity
assessment framework, equipped with a hybrid message-passing and graph
transformer encoder. To address transformer scalability, we incorporate a
linear transformer backbone and introduce a partitioned training pipeline for
efficient memory and parallelism management. Evaluations on synthetic and
real-world CircuitNet datasets show that Pieceformer reduces mean absolute
error (MAE) by 24.9% over the baseline and is the only method to correctly
cluster all real-world design groups. We further demonstrate the practical
usage of our model through a case study on a partitioning task, achieving up to
89% runtime reduction. These results validate the framework's effectiveness for
scalable, unbiased design reuse in modern VLSI systems.

</details>


### [161] [Early Attentive Sparsification Accelerates Neural Speech Transcription](https://arxiv.org/abs/2506.15912)
*Zifei Xu,Sayeh Sharify,Hesham Mostafa,Tristan Webb,Wanzin Yazar,Xin Wang*

Main category: cs.LG

TL;DR: 本文利用Transformer音频编码器自注意力机制的可解释性，通过时域信号稀疏化加速神经语音转录，在Whisper模型上搜索，在1%精度下降下可实现加速。


<details>
  <summary>Details</summary>
Motivation: 语音音频信号高度可压缩，利用Transformer自注意力机制可解释性，加速神经语音转录。

Method: 在Whisper模型上，对稀疏化阶段和压缩比的联合空间进行系统架构搜索。

Result: 在1%精度下降下，早期编码阶段将隐藏状态稀疏化到40 - 60%稀疏度，在Nvidia GPU英语语音转录任务中无需微调实现1.6倍运行时加速。

Conclusion: 通过时域信号稀疏化可有效加速神经语音转录，且在一定精度损失下有良好效果。

Abstract: Transformer-based neural speech processing has achieved state-of-the-art
performance. Since speech audio signals are known to be highly compressible,
here we seek to accelerate neural speech transcription by time-domain signal
sparsification early in the neural encoding stage, taking advantage of the
interpretability of the self-attention mechanism in transformer audio encoders.
With the Whisper family of models, we perform a systematic architecture search
over the joint space of sparsification stage (a certain encoder layer) and
compression ratio (sparsity). We found that the best resulting solutions under
1% accuracy degradation choose to sparsify the hidden state to 40-60% sparsity
at an early encoding stage, and thereby achieve up to 1.6x runtime acceleration
in English speech transcription tasks on Nvidia GPUs without any fine-tuning.

</details>


### [162] [Competing Bandits in Matching Markets via Super Stability](https://arxiv.org/abs/2506.15926)
*Soumya Basu*

Main category: cs.LG

TL;DR: 研究双边奖励不确定匹配市场的多臂老虎机学习，对比算法并得出后悔界及下界。


<details>
  <summary>Details</summary>
Motivation: 拓展主要关注单边不确定性的先前研究，研究双边奖励不确定匹配市场的多臂老虎机学习。

Method: 利用Irving（1994）的`超稳定性`概念，采用扩展Gale - Shapley（GS）算法，还将其适配到去中心化场景。

Result: 集中式算法实现对数悲观稳定后悔，适配到去中心化场景后悔常数增加，建立新的集中式实例相关的二元稳定后悔下界。

Conclusion: 阐明可允许差距和超稳定匹配在刻画带多臂老虎机反馈的稳定匹配复杂性中的作用。

Abstract: We study bandit learning in matching markets with two-sided reward
uncertainty, extending prior research primarily focused on single-sided
uncertainty. Leveraging the concept of `super-stability' from Irving (1994), we
demonstrate the advantage of the Extended Gale-Shapley (GS) algorithm over the
standard GS algorithm in achieving true stable matchings under incomplete
information. By employing the Extended GS algorithm, our centralized algorithm
attains a logarithmic pessimal stable regret dependent on an instance-dependent
admissible gap parameter. This algorithm is further adapted to a decentralized
setting with a constant regret increase. Finally, we establish a novel
centralized instance-dependent lower bound for binary stable regret,
elucidating the roles of the admissible gap and super-stable matching in
characterizing the complexity of stable matching with bandit feedback.

</details>


### [163] [CORAL: Disentangling Latent Representations in Long-Tailed Diffusion](https://arxiv.org/abs/2506.15933)
*Esther Rodriguez,Monica Welfert,Samuel McDowell,Nathan Stromberg,Julian Antolin Camarena,Lalitha Sankar*

Main category: cs.LG

TL;DR: 研究扩散模型在长尾数据集上的问题，提出CORAL框架提升尾部类别样本生成质量和多样性。


<details>
  <summary>Details</summary>
Motivation: 标准扩散模型在长尾分布数据上生成尾部类别样本时质量和多样性差，且其原因不明。

Method: 提出CORAL，即对比潜在对齐框架，利用监督对比损失使潜在类表示分离。

Result: 实验表明，相对于现有方法，CORAL显著提高了尾部类别生成样本的多样性和视觉质量。

Conclusion: CORAL能有效解决扩散模型在长尾分布数据上生成尾部类别样本的问题。

Abstract: Diffusion models have achieved impressive performance in generating
high-quality and diverse synthetic data. However, their success typically
assumes a class-balanced training distribution. In real-world settings,
multi-class data often follow a long-tailed distribution, where standard
diffusion models struggle -- producing low-diversity and lower-quality samples
for tail classes. While this degradation is well-documented, its underlying
cause remains poorly understood. In this work, we investigate the behavior of
diffusion models trained on long-tailed datasets and identify a key issue: the
latent representations (from the bottleneck layer of the U-Net) for tail class
subspaces exhibit significant overlap with those of head classes, leading to
feature borrowing and poor generation quality. Importantly, we show that this
is not merely due to limited data per class, but that the relative class
imbalance significantly contributes to this phenomenon. To address this, we
propose COntrastive Regularization for Aligning Latents (CORAL), a contrastive
latent alignment framework that leverages supervised contrastive losses to
encourage well-separated latent class representations. Experiments demonstrate
that CORAL significantly improves both the diversity and visual quality of
samples generated for tail classes relative to state-of-the-art methods.

</details>


### [164] [On the optimal regret of collaborative personalized linear bandits](https://arxiv.org/abs/2506.15943)
*Bruce Huang,Ruida Zhou,Lin F. Yang,Suhas Diggavi*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Stochastic linear bandits are a fundamental model for sequential decision
making, where an agent selects a vector-valued action and receives a noisy
reward with expected value given by an unknown linear function. Although well
studied in the single-agent setting, many real-world scenarios involve multiple
agents solving heterogeneous bandit problems, each with a different unknown
parameter. Applying single agent algorithms independently ignores cross-agent
similarity and learning opportunities. This paper investigates the optimal
regret achievable in collaborative personalized linear bandits. We provide an
information-theoretic lower bound that characterizes how the number of agents,
the interaction rounds, and the degree of heterogeneity jointly affect regret.
We then propose a new two-stage collaborative algorithm that achieves the
optimal regret. Our analysis models heterogeneity via a hierarchical Bayesian
framework and introduces a novel information-theoretic technique for bounding
regret. Our results offer a complete characterization of when and how
collaboration helps with a optimal regret bound $\tilde{O}(d\sqrt{mn})$,
$\tilde{O}(dm^{1-\gamma}\sqrt{n})$, $\tilde{O}(dm\sqrt{n})$ for the number of
rounds $n$ in the range of $(0, \frac{d}{m \sigma^2})$, $[\frac{d}{m^{2\gamma}
\sigma^2}, \frac{d}{\sigma^2}]$ and $(\frac{d}{\sigma^2}, \infty)$
respectively, where $\sigma$ measures the level of heterogeneity, $m$ is the
number of agents, and $\gamma\in[0, 1/2]$ is an absolute constant. In contrast,
agents without collaboration achieve a regret bound $O(dm\sqrt{n})$ at best.

</details>


### [165] [One Period to Rule Them All: Identifying Critical Learning Periods in Deep Networks](https://arxiv.org/abs/2506.15954)
*Vinicius Yuiti Fukase,Heitor Gama,Barbara Bueno,Lucas Libanio,Anna Helena Reali Costa,Artur Jordao*

Main category: cs.LG

TL;DR: 提出系统方法识别深度学习训练中的关键时期，加速学习阶段，减少训练时间、能耗和碳排放，实验验证有效。


<details>
  <summary>Details</summary>
Motivation: 现有文献缺乏精确识别深度学习训练中关键时期的方法，需填补该空白。

Method: 引入系统方法，利用泛化预测机制确定关键阶段，停止关键阶段后的资源密集型训练方法。

Result: 在标准架构和基准测试中，将流行架构训练时间最多减少59.67%，CO₂排放减少59.47%，财务成本降低60%，且不影响性能。

Conclusion: 该方法增强了对训练动态的理解，为资源受限环境下的深度学习提供更可持续高效的实践框架。

Abstract: Critical Learning Periods comprehend an important phenomenon involving deep
learning, where early epochs play a decisive role in the success of many
training recipes, such as data augmentation. Existing works confirm the
existence of this phenomenon and provide useful insights. However, the
literature lacks efforts to precisely identify when critical periods occur. In
this work, we fill this gap by introducing a systematic approach for
identifying critical periods during the training of deep neural networks,
focusing on eliminating computationally intensive regularization techniques and
effectively applying mechanisms for reducing computational costs, such as data
pruning. Our method leverages generalization prediction mechanisms to pinpoint
critical phases where training recipes yield maximum benefits to the predictive
ability of models. By halting resource-intensive recipes beyond these periods,
we significantly accelerate the learning phase and achieve reductions in
training time, energy consumption, and CO$_2$ emissions. Experiments on
standard architectures and benchmarks confirm the effectiveness of our method.
Specifically, we achieve significant milestones by reducing the training time
of popular architectures by up to 59.67%, leading to a 59.47% decrease in
CO$_2$ emissions and a 60% reduction in financial costs, without compromising
performance. Our work enhances understanding of training dynamics and paves the
way for more sustainable and efficient deep learning practices, particularly in
resource-constrained environments. In the era of the race for foundation
models, we believe our method emerges as a valuable framework. The repository
is available at https://github.com/baunilhamarga/critical-periods

</details>


### [166] [On the Theoretical Understanding of Identifiable Sparse Autoencoders and Beyond](https://arxiv.org/abs/2506.15963)
*Jingyi Cui,Qi Zhang,Yifei Wang,Yisen Wang*

Main category: cs.LG

TL;DR: 本文首次提出可识别稀疏自编码器（SAEs）的充要条件，并在条件不满足时提出重加权策略，实验验证了理论并表明加权SAEs提升了特征单义性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 尽管SAEs应用广泛，但不清楚其在何种条件下能从叠加的多语义特征中完全恢复真实单语义特征。

Method: 通过理论分析提出可识别SAEs的充要条件，在条件不满足时提出重加权策略，遵循理论建议的权重选择原则。

Result: 证明可缩小SAE重建和单语义特征重建损失函数之间的差距，实验验证理论发现，加权SAEs显著提升特征单义性和可解释性。

Conclusion: 提出的充要条件和重加权策略有效，能提升SAEs对真实单语义特征的重建效果。

Abstract: Sparse autoencoders (SAEs) have emerged as a powerful tool for interpreting
features learned by large language models (LLMs). It aims to recover complex
superposed polysemantic features into interpretable monosemantic ones through
feature reconstruction via sparsely activated neural networks. Despite the wide
applications of SAEs, it remains unclear under what conditions an SAE can fully
recover the ground truth monosemantic features from the superposed polysemantic
ones. In this paper, through theoretical analysis, we for the first time
propose the necessary and sufficient conditions for identifiable SAEs (SAEs
that learn unique and ground truth monosemantic features), including 1) extreme
sparsity of the ground truth feature, 2) sparse activation of SAEs, and 3)
enough hidden dimensions of SAEs. Moreover, when the identifiable conditions
are not fully met, we propose a reweighting strategy to improve the
identifiability. Specifically, following the theoretically suggested weight
selection principle, we prove that the gap between the loss functions of SAE
reconstruction and monosemantic feature reconstruction can be narrowed, so that
the reweighted SAEs have better reconstruction of the ground truth monosemantic
features than the uniformly weighted ones. In experiments, we validate our
theoretical findings and show that our weighted SAE significantly improves
feature monosemanticity and interpretability.

</details>


### [167] [LazyEviction: Lagged KV Eviction with Attention Pattern Observation for Efficient Long Reasoning](https://arxiv.org/abs/2506.15969)
*Haoyue Zhang,Hualei Zhang,Xiaosong Ma,Jie Zhang,Song Guo*

Main category: cs.LG

TL;DR: 本文提出LazyEviction框架减少大语言模型推理时KV缓存内存，在数学推理数据集上效果佳。


<details>
  <summary>Details</summary>
Motivation: 现有KV缓存压缩方法在长推理任务中表现不佳，且未捕捉Token重要性循环现象。

Method: 提出基于观察窗口的滞后KV驱逐框架LazyEviction，含循环间隔跟踪和最大循环间隔中心驱逐策略。

Result: LazyEviction减少50% KV缓存大小，在数学推理数据集上保持可比准确率，优于现有方法。

Conclusion: 保留循环标记对多步推理任务中保持知识连续性很重要。

Abstract: Large Language Models (LLMs) exhibit enhanced reasoning capabilities by
employing Chain-of-Thought (CoT). However, the extended reasoning sequences
introduce significant GPU memory overhead due to increased key-value (KV) cache
size, particularly in tasks requiring long reasoning sequences, such as
mathematics and programming. Existing KV cache compression methods mitigate
memory bottlenecks but struggle in long reasoning tasks. In this paper, we
analyze attention patterns in reasoning tasks and reveal a Token Importance
Recurrence phenomenon: a large proportion of tokens receive renewed attention
after multiple decoding steps, which is failed to capture by existing works and
may lead to unpredictable eviction on such periodically critical tokens. To
address this, we propose LazyEviction, a lagged KV eviction framework designed
to maintain reasoning performance while reducing KV memory. LazyEviction is an
Observation Window-based Lagged Eviction Mechanism retaining latent recurring
tokens by performing lagged evictions across decoding steps, which contains two
key components: (1) Recurrence Interval Tracking for capturing temporal
variations in token importance, and (2) an Maximum Recurrence Interval-Centric
Eviction Policy that prioritizes eviction based on tokens' recurrence patterns.
Extensive experiments demonstrate that LazyEviction reduces KV cache size by
50% while maintaining comparable accuracy on mathematics reasoning datasets,
outperforming state-of-the-art methods. Our findings highlight the importance
of preserving recurring tokens, which are critical for maintaining knowledge
continuity in multi-step reasoning tasks.

</details>


### [168] [AutoHFormer: Efficient Hierarchical Autoregressive Transformer for Time Series Prediction](https://arxiv.org/abs/2506.16001)
*Qianru Zhang,Honggang Wen,Ming Li,Dong Huang,Siu-Ming Yiu,Christian S. Jensen,Pietro Liò*

Main category: cs.LG

TL;DR: 提出AutoHFormer解决时间序列预测的三个挑战，实验显示其训练速度更快、内存使用更少。


<details>
  <summary>Details</summary>
Motivation: 时间序列预测需同时达成严格时间因果性、次二次复杂度和多尺度模式识别三个目标。

Method: 提出AutoHFormer，采用分层时间建模、动态窗口注意力和自适应时间编码三项创新。

Result: 在PEMS08上训练速度比PatchTST快10.76倍，内存减少6.06倍，多数情况下在96 - 720步预测精度稳定。

Conclusion: AutoHFormer为高效精确的时间序列建模树立了新标杆。

Abstract: Time series forecasting requires architectures that simultaneously achieve
three competing objectives: (1) strict temporal causality for reliable
predictions, (2) sub-quadratic complexity for practical scalability, and (3)
multi-scale pattern recognition for accurate long-horizon forecasting. We
introduce AutoHFormer, a hierarchical autoregressive transformer that addresses
these challenges through three key innovations: 1) Hierarchical Temporal
Modeling: Our architecture decomposes predictions into segment-level blocks
processed in parallel, followed by intra-segment sequential refinement. This
dual-scale approach maintains temporal coherence while enabling efficient
computation. 2) Dynamic Windowed Attention: The attention mechanism employs
learnable causal windows with exponential decay, reducing complexity while
preserving precise temporal relationships. This design avoids both the
anti-causal violations of standard transformers and the sequential bottlenecks
of RNN hybrids. 3) Adaptive Temporal Encoding: a novel position encoding system
is adopted to capture time patterns at multiple scales. It combines fixed
oscillating patterns for short-term variations with learnable decay rates for
long-term trends. Comprehensive experiments demonstrate that AutoHFormer 10.76X
faster training and 6.06X memory reduction compared to PatchTST on PEMS08,
while maintaining consistent accuracy across 96-720 step horizons in most of
cases. These breakthroughs establish new benchmarks for efficient and precise
time series modeling. Implementations of our method and all baselines in
hierarchical autoregressive mechanism are available at
https://github.com/lizzyhku/Autotime.

</details>


### [169] [Bridging Brain with Foundation Models through Self-Supervised Learning](https://arxiv.org/abs/2506.16009)
*Hamdi Altaheri,Fakhri Karray,Md. Milon Islam,S M Taslim Uddin Raju,Amir-Hossein Karimi*

Main category: cs.LG

TL;DR: 本文系统回顾利用自监督学习将脑信号与基础模型结合的新兴领域，涵盖关键技术、模型开发、下游任务适配等内容，指出挑战与未来方向。


<details>
  <summary>Details</summary>
Motivation: 基础模型在多领域表现出色，自监督学习能解决传统监督学习在脑信号分析中因标注数据稀缺的局限，故开展此研究。

Method: 系统回顾新兴领域，探索关键自监督技术、脑特定基础模型开发、下游任务适配、多模态融合，介绍评估指标和基准数据集。

Result: 梳理了该领域的关键内容，包括技术、模型、应用、评估等方面。

Conclusion: 为研究人员提供该领域结构化理解和开发自监督脑基础模型的路线图。

Abstract: Foundation models (FMs), powered by self-supervised learning (SSL), have
redefined the capabilities of artificial intelligence, demonstrating
exceptional performance in domains like natural language processing and
computer vision. These advances present a transformative opportunity for brain
signal analysis. Unlike traditional supervised learning, which is limited by
the scarcity of labeled neural data, SSL offers a promising solution by
enabling models to learn meaningful representations from unlabeled data. This
is particularly valuable in addressing the unique challenges of brain signals,
including high noise levels, inter-subject variability, and low signal-to-noise
ratios. This survey systematically reviews the emerging field of bridging brain
signals with foundation models through the innovative application of SSL. It
explores key SSL techniques, the development of brain-specific foundation
models, their adaptation to downstream tasks, and the integration of brain
signals with other modalities in multimodal SSL frameworks. The review also
covers commonly used evaluation metrics and benchmark datasets that support
comparative analysis. Finally, it highlights key challenges and outlines future
research directions. This work aims to provide researchers with a structured
understanding of this rapidly evolving field and a roadmap for developing
generalizable brain foundation models powered by self-supervision.

</details>


### [170] [VRAIL: Vectorized Reward-based Attribution for Interpretable Learning](https://arxiv.org/abs/2506.16014)
*Jina Kim,Youjin Jang,Jeongjin Han*

Main category: cs.LG

TL;DR: 提出VRAIL框架用于基于值的强化学习，能学习可解释权重表示，在Taxi - v3环境实验有优势，是通用且模型无关的奖励塑造框架。


<details>
  <summary>Details</summary>
Motivation: 在基于值的强化学习中学习可解释的权重表示，提高学习的可解释性。

Method: 提出VRAIL框架，包含深度学习阶段拟合估计值函数和强化学习阶段进行基于势的奖励转换，估计器采用线性或二次形式。

Result: 在Taxi - v3环境中，与标准DQN相比，VRAIL提高了训练稳定性和收敛性，且能揭示语义上有意义的子目标。

Conclusion: VRAIL是一个通用、模型无关的奖励塑造框架，可增强学习和可解释性。

Abstract: We propose VRAIL (Vectorized Reward-based Attribution for Interpretable
Learning), a bi-level framework for value-based reinforcement learning (RL)
that learns interpretable weight representations from state features. VRAIL
consists of two stages: a deep learning (DL) stage that fits an estimated value
function using state features, and an RL stage that uses this to shape learning
via potential-based reward transformations. The estimator is modeled in either
linear or quadratic form, allowing attribution of importance to individual
features and their interactions. Empirical results on the Taxi-v3 environment
demonstrate that VRAIL improves training stability and convergence compared to
standard DQN, without requiring environment modifications. Further analysis
shows that VRAIL uncovers semantically meaningful subgoals, such as passenger
possession, highlighting its ability to produce human-interpretable behavior.
Our findings suggest that VRAIL serves as a general, model-agnostic framework
for reward shaping that enhances both learning and interpretability.

</details>


### [171] [A Scalable Factorization Approach for High-Order Structured Tensor Recovery](https://arxiv.org/abs/2506.16032)
*Zhen Qin,Michael B. Wakin,Zhihui Zhu*

Main category: cs.LG

TL;DR: 本文提出张量分解问题的统一框架，用RGD优化正交因子，证明其线性收敛且性能优于现有结果。


<details>
  <summary>Details</summary>
Motivation: 张量分解优化问题非凸，给收敛分析和恢复保证带来挑战，需新方法解决。

Method: 利用张量分解规范形式，用Riemannian gradient descent (RGD)在Stiefel流形上优化正交因子。

Result: 在损失函数温和条件下，建立因式分解目标的Riemannian正则条件，证明RGD能线性收敛到真实张量。

Conclusion: 初始化要求和收敛速度与N呈多项式关系，优于Tucker和张量列车格式张量的现有结果。

Abstract: Tensor decompositions, which represent an $N$-order tensor using
approximately $N$ factors of much smaller dimensions, can significantly reduce
the number of parameters. This is particularly beneficial for high-order
tensors, as the number of entries in a tensor grows exponentially with the
order. Consequently, they are widely used in signal recovery and data analysis
across domains such as signal processing, machine learning, and quantum
physics. A computationally and memory-efficient approach to these problems is
to optimize directly over the factors using local search algorithms such as
gradient descent, a strategy known as the factorization approach in matrix and
tensor optimization. However, the resulting optimization problems are highly
nonconvex due to the multiplicative interactions between factors, posing
significant challenges for convergence analysis and recovery guarantees.
  In this paper, we present a unified framework for the factorization approach
to solving various tensor decomposition problems. Specifically, by leveraging
the canonical form of tensor decompositions--where most factors are constrained
to be orthonormal to mitigate scaling ambiguity--we apply Riemannian gradient
descent (RGD) to optimize these orthonormal factors on the Stiefel manifold.
Under a mild condition on the loss function, we establish a Riemannian
regularity condition for the factorized objective and prove that RGD converges
to the ground-truth tensor at a linear rate when properly initialized. Notably,
both the initialization requirement and the convergence rate scale polynomially
rather than exponentially with $N$, improving upon existing results for Tucker
and tensor-train format tensors.

</details>


### [172] [CRIA: A Cross-View Interaction and Instance-Adapted Pre-training Framework for Generalizable EEG Representations](https://arxiv.org/abs/2506.16056)
*Puchun Liu,C. L. Philip Chen,Yubin He,Tong Zhang*

Main category: cs.LG

TL;DR: 本文提出CRIA框架解决EEG数据特征提取和多视图信息整合难题，实验显示其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有预训练方法仅依赖单视图上下文语义，无法捕捉多视角复杂交互，限制表征表达和泛化能力。

Method: 提出CRIA框架，利用可变长度和通道编码统一不同数据集EEG数据表示；定义跨视图信息；采用交叉注意力机制融合特征；结合信息瓶颈原理的注意力矩阵掩码策略和新的视角掩码预训练方案。

Result: 在Temple University EEG语料库和CHB - MIT数据集上，CRIA在相同预训练条件下优于现有方法，多类事件分类平衡准确率57.02%，异常检测准确率80.03%。

Conclusion: CRIA具有强大的泛化能力。

Abstract: The difficulty of extracting deep features from EEG data and effectively
integrating information from multiple views presents significant challenges for
developing a generalizable pretraining framework for EEG representation
learning. However, most existing pre-training methods rely solely on the
contextual semantics of a single view, failing to capture the complex and
synergistic interactions among different perspectives, limiting the
expressiveness and generalization of learned representations. To address these
issues, this paper proposes CRIA, an adaptive framework that utilizes
variable-length and variable-channel coding to achieve a unified representation
of EEG data across different datasets. In this work, we define cross-view
information as the integrated representation that emerges from the interaction
among temporal, spectral, and spatial views of EEG signals. The model employs a
cross-attention mechanism to fuse temporal, spectral, and spatial features
effectively, and combines an attention matrix masking strategy based on the
information bottleneck principle with a novel viewpoint masking pre-training
scheme. Experimental results on the Temple University EEG corpus and the
CHB-MIT dataset show that CRIA outperforms existing methods with the same
pre-training conditions, achieving a balanced accuracy of 57.02% for
multi-class event classification and 80.03% for anomaly detection, highlighting
its strong generalization ability.

</details>


### [173] [Floating-Point Neural Networks Are Provably Robust Universal Approximators](https://arxiv.org/abs/2506.16065)
*Geonho Hwang,Wonyeol Lee,Yeachan Park,Sejun Park,Feras Saad*

Main category: cs.LG

TL;DR: 本文提出首个浮点神经网络的区间通用逼近（IUA）定理，证明其能完美捕捉目标函数直接映射，揭示与实值情况差异并得出意外推论。


<details>
  <summary>Details</summary>
Motivation: 经典神经网络通用逼近定理基于无限精度实数计算，实际为有限精度浮点计算，探讨浮点情况下IUA定理是否成立。

Method: 提出浮点神经网络的IUA定理进行研究。

Result: 证明浮点神经网络能完美捕捉任意舍入目标函数的直接映射，定理在浮点和实值情况有实质差异，还得出意外推论。

Conclusion: 浮点神经网络在逼近能力上无表达限制，定理有重要意义和意外推论。

Abstract: The classical universal approximation (UA) theorem for neural networks
establishes mild conditions under which a feedforward neural network can
approximate a continuous function $f$ with arbitrary accuracy. A recent result
shows that neural networks also enjoy a more general interval universal
approximation (IUA) theorem, in the sense that the abstract interpretation
semantics of the network using the interval domain can approximate the direct
image map of $f$ (i.e., the result of applying $f$ to a set of inputs) with
arbitrary accuracy. These theorems, however, rest on the unrealistic assumption
that the neural network computes over infinitely precise real numbers, whereas
their software implementations in practice compute over finite-precision
floating-point numbers. An open question is whether the IUA theorem still holds
in the floating-point setting.
  This paper introduces the first IUA theorem for floating-point neural
networks that proves their remarkable ability to perfectly capture the direct
image map of any rounded target function $f$, showing no limits exist on their
expressiveness. Our IUA theorem in the floating-point setting exhibits material
differences from the real-valued setting, which reflects the fundamental
distinctions between these two computational models. This theorem also implies
surprising corollaries, which include (i) the existence of provably robust
floating-point neural networks; and (ii) the computational completeness of the
class of straight-line programs that use only floating-point additions and
multiplications for the class of all floating-point programs that halt.

</details>


### [174] [A Lightweight RL-Driven Deep Unfolding Network for Robust WMMSE Precoding in Massive MU-MIMO-OFDM Systems](https://arxiv.org/abs/2506.16072)
*Kexuan Wang,An Liu*

Main category: cs.LG

TL;DR: 提出RLDDU - Net解决WMMSE在大规模多用户MIMO - OFDM系统中因完美CSI假设和高计算复杂度难以实际部署的问题，仿真显示其性能和效率更优。


<details>
  <summary>Details</summary>
Motivation: WMMSE预编码在大规模多用户MIMO - OFDM系统中因完美CSI假设和高计算复杂度难以实际部署。

Method: 先开发SWMMSE算法在不完美CSI下最大化遍历加权和速率，再提出RLDDU - Net，将SWMMSE迭代映射到网络层，DU模块利用近似技术和稀疏性及相关性加速收敛和降低计算开销，RL模块自适应调整网络深度和生成补偿矩阵。

Result: 在不完美CSI下的仿真表明，RLDDU - Net在遍历加权和速率性能上优于现有基线，且计算和收敛效率更高。

Conclusion: RLDDU - Net能有效解决WMMSE的问题，在性能和效率上有优势。

Abstract: Weighted Minimum Mean Square Error (WMMSE) precoding is widely recognized for
its near-optimal weighted sum rate performance. However, its practical
deployment in massive multi-user (MU) multiple-input multiple-output (MIMO)
orthogonal frequency-division multiplexing (OFDM) systems is hindered by the
assumption of perfect channel state information (CSI) and high computational
complexity. To address these issues, we first develop a wideband stochastic
WMMSE (SWMMSE) algorithm that iteratively maximizes the ergodic weighted
sum-rate (EWSR) under imperfect CSI. Building on this, we propose a lightweight
reinforcement learning (RL)-driven deep unfolding (DU) network (RLDDU-Net),
where each SWMMSE iteration is mapped to a network layer. Specifically, its DU
module integrates approximation techniques and leverages beam-domain sparsity
as well as frequency-domain subcarrier correlation, significantly accelerating
convergence and reducing computational overhead. Furthermore, the RL module
adaptively adjusts the network depth and generates compensation matrices to
mitigate approximation errors. Simulation results under imperfect CSI
demonstrate that RLDDU-Net outperforms existing baselines in EWSR performance
while offering superior computational and convergence efficiency.

</details>


### [175] [Joint User Priority and Power Scheduling for QoS-Aware WMMSE Precoding: A Constrained-Actor Attentive-Critic Approach](https://arxiv.org/abs/2506.16074)
*Kexuan Wang,An Liu*

Main category: cs.LG

TL;DR: 提出CAAC算法用于6G无线网络WMMSE预编码，动态分配用户优先级和功率，模拟显示该算法在能效和QoS满意度上优于基线。


<details>
  <summary>Details</summary>
Motivation: 传统WMMSE预编码缺乏适应特定用户QoS需求和时变信道条件的灵活性，需要新算法解决。

Method: 提出CAAC算法，集成CSSCA方法优化策略，采用轻量级注意力增强Q网络评估策略更新。

Result: 模拟结果表明CAAC在能效和QoS满意度上优于基线。

Conclusion: CAAC算法能更有效处理能效目标和满足随机非凸QoS约束，在6G无线网络中有更好表现。

Abstract: 6G wireless networks are expected to support diverse quality-of-service (QoS)
demands while maintaining high energy efficiency. Weighted Minimum Mean Square
Error (WMMSE) precoding with fixed user priorities and transmit power is widely
recognized for enhancing overall system performance but lacks flexibility to
adapt to user-specific QoS requirements and time-varying channel conditions. To
address this, we propose a novel constrained reinforcement learning (CRL)
algorithm, Constrained-Actor Attentive-Critic (CAAC), which uses a policy
network to dynamically allocate user priorities and power for WMMSE precoding.
Specifically, CAAC integrates a Constrained Stochastic Successive Convex
Approximation (CSSCA) method to optimize the policy, enabling more effective
handling of energy efficiency goals and satisfaction of stochastic non-convex
QoS constraints compared to traditional and existing CRL methods. Moreover,
CAAC employs lightweight attention-enhanced Q-networks to evaluate policy
updates without prior environment model knowledge. The network architecture not
only enhances representational capacity but also boosts learning efficiency.
Simulation results show that CAAC outperforms baselines in both energy
efficiency and QoS satisfaction.

</details>


### [176] [Probing the Robustness of Large Language Models Safety to Latent Perturbations](https://arxiv.org/abs/2506.16078)
*Tianle Gu,Kexin Huang,Zongqi Wang,Yixu Wang,Jie Li,Yuanqi Yao,Yang Yao,Yujiu Yang,Yan Teng,Yingchun Wang*

Main category: cs.LG

TL;DR: 本文指出当前安全对齐方法存在浅层问题，提出探测方法 ASA 和微调策略 LAPT，实验表明 LAPT 可增强对齐鲁棒性，揭示现有对齐范式缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有安全对齐方法存在浅层问题，小的潜在偏移仍会触发对齐模型的不安全响应，需探索安全对齐对潜在扰动的鲁棒性。

Method: 引入测量模型原始响应负对数似然的探测方法，构建 ASA；提出在训练中注入受控扰动的 LAPT 微调策略。

Result: 实验显示 LAPT 能在不影响通用能力的前提下增强对齐鲁棒性。

Conclusion: 当前对齐范式存在根本缺陷，需超越表面行为监督的表征级训练策略。

Abstract: Safety alignment is a key requirement for building reliable Artificial
General Intelligence. Despite significant advances in safety alignment, we
observe that minor latent shifts can still trigger unsafe responses in aligned
models. We argue that this stems from the shallow nature of existing alignment
methods, which focus on surface-level refusal behaviors without sufficiently
altering internal representations. Consequently, small shifts in hidden
activations can re-trigger harmful behaviors embedded in the latent space. To
explore the robustness of safety alignment to latent perturbations, we
introduce a probing method that measures the Negative Log-Likelihood of the
original response generated by the model. This probe quantifies local
sensitivity in the latent space, serving as a diagnostic tool for identifying
vulnerable directions. Based on this signal, we construct effective jailbreak
trajectories, giving rise to the Activation Steering Attack (ASA). More
importantly, these insights offer a principled foundation for improving
alignment robustness. To this end, we introduce Layer-wise Adversarial Patch
Training~(LAPT), a fine-tuning strategy that inject controlled perturbations
into hidden representations during training. Experimental results highlight
that LAPT strengthen alignment robustness without compromising general
capabilities. Our findings reveal fundamental flaws in current alignment
paradigms and call for representation-level training strategies that move
beyond surface-level behavior supervision. Codes and results are available at
https://github.com/Carol-gutianle/LatentSafety.

</details>


### [177] [A Brain-to-Population Graph Learning Framework for Diagnosing Brain Disorders](https://arxiv.org/abs/2506.16096)
*Qianqian Liao,Wuque Cai,Hongze Sun,Dongze Liu,Duo Chen,Dezhong Yao,Daqing Guo*

Main category: cs.LG

TL;DR: 提出两阶段B2P - GL框架用于脑疾病诊断，实验表明其优于现有方法且提高可解释性，推进临床应用。


<details>
  <summary>Details</summary>
Motivation: 现有基于图的脑疾病诊断方法高度依赖预定义脑图谱，忽视图谱丰富信息及站点和表型变异性的混杂效应。

Method: 提出两阶段B2P - GL框架，第一阶段利用GPT - 4的脑图谱知识丰富图表示，通过自适应节点重新分配图注意力网络优化脑图；第二阶段将表型数据融入群体图构建和特征融合。

Result: 在ABIDE I、ADHD - 200和Rest - meta - MDD数据集上实验显示，B2P - GL在预测准确性上优于现有方法，且提高可解释性。

Conclusion: 所提出的框架为脑疾病诊断提供可靠且个性化的方法，推进了临床适用性。

Abstract: Recent developed graph-based methods for diagnosing brain disorders using
functional connectivity highly rely on predefined brain atlases, but overlook
the rich information embedded within atlases and the confounding effects of
site and phenotype variability. To address these challenges, we propose a
two-stage Brain-to-Population Graph Learning (B2P-GL) framework that integrates
the semantic similarity of brain regions and condition-based population graph
modeling. In the first stage, termed brain representation learning, we leverage
brain atlas knowledge from GPT-4 to enrich the graph representation and refine
the brain graph through an adaptive node reassignment graph attention network.
In the second stage, termed population disorder diagnosis, phenotypic data is
incorporated into population graph construction and feature fusion to mitigate
confounding effects and enhance diagnosis performance. Experiments on the ABIDE
I, ADHD-200, and Rest-meta-MDD datasets show that B2P-GL outperforms
state-of-the-art methods in prediction accuracy while enhancing
interpretability. Overall, our proposed framework offers a reliable and
personalized approach to brain disorder diagnosis, advancing clinical
applicability.

</details>


### [178] [Mitigating Over-Squashing in Graph Neural Networks by Spectrum-Preserving Sparsification](https://arxiv.org/abs/2506.16110)
*Langzhang Liang,Fanchen Bu,Zixing Song,Zenglin Xu,Shirui Pan,Kijung Shin*

Main category: cs.LG

TL;DR: 本文提出一种利用谱保持图稀疏化的图重连方法来缓解图神经网络的过挤压问题，实验验证其有效性和优越性。


<details>
  <summary>Details</summary>
Motivation: 图神经网络的消息传递范式存在过挤压问题，现有图重连技术忽视保留原图关键属性且增加计算开销和过平滑风险。

Method: 提出利用谱保持图稀疏化的图重连方法，生成增强连通性、保持稀疏性并保留原图谱的图。

Result: 实验表明该方法在分类准确率和保留拉普拉斯谱方面优于强基线方法。

Conclusion: 该方法能有效平衡结构瓶颈减少和图属性保留。

Abstract: The message-passing paradigm of Graph Neural Networks often struggles with
exchanging information across distant nodes typically due to structural
bottlenecks in certain graph regions, a limitation known as
\textit{over-squashing}. To reduce such bottlenecks, \textit{graph rewiring},
which modifies graph topology, has been widely used. However, existing graph
rewiring techniques often overlook the need to preserve critical properties of
the original graph, e.g., \textit{spectral properties}. Moreover, many
approaches rely on increasing edge count to improve connectivity, which
introduces significant computational overhead and exacerbates the risk of
over-smoothing. In this paper, we propose a novel graph rewiring method that
leverages \textit{spectrum-preserving} graph \textit{sparsification}, for
mitigating over-squashing. Our method generates graphs with enhanced
connectivity while maintaining sparsity and largely preserving the original
graph spectrum, effectively balancing structural bottleneck reduction and graph
property preservation. Experimental results validate the effectiveness of our
approach, demonstrating its superiority over strong baseline methods in
classification accuracy and retention of the Laplacian spectrum.

</details>


### [179] [From Teacher to Student: Tracking Memorization Through Model Distillation](https://arxiv.org/abs/2506.16170)
*Simardeep Singh*

Main category: cs.LG

TL;DR: 研究知识蒸馏对微调任务数据记忆化的影响，发现大模型蒸馏到小模型可降低计算成本、模型大小和记忆风险。


<details>
  <summary>Details</summary>
Motivation: 此前研究多关注预训练模型的记忆化，对知识蒸馏如何影响记忆化了解较少，因此开展本研究。

Method: 探索不同知识蒸馏方法在将大教师模型蒸馏到小学生模型时对微调任务数据记忆化的影响。

Result: 将在数据集上微调的大教师模型蒸馏到小模型，不仅降低计算成本和模型大小，还显著降低记忆风险。

Conclusion: 与标准微调方法相比，知识蒸馏能降低记忆化风险。

Abstract: Large language models (LLMs) are known to memorize parts of their training
data, raising important concerns around privacy and security. While previous
research has focused on studying memorization in pre-trained models, much less
is known about how knowledge distillation (KD) affects memorization.In this
study, we explore how different KD methods influence the memorization of
fine-tuned task data when a large teacher model is distilled into smaller
student variants.This study demonstrates that distilling a larger teacher
model, fine-tuned on a dataset, into a smaller variant not only lowers
computational costs and model size but also significantly reduces the
memorization risks compared to standard fine-tuning approaches.

</details>


### [180] [Hallucination Level of Artificial Intelligence Whisperer: Case Speech Recognizing Pantterinousut Rap Song](https://arxiv.org/abs/2506.16174)
*Ismo Horppu,Frederick Ayala,Erlin Gulbenkoglu*

Main category: cs.LG

TL;DR: 本文尝试将芬兰说唱歌曲翻译成文本，比较Faster Whisperer算法和YouTube内部语音转文本功能。


<details>
  <summary>Details</summary>
Motivation: 芬兰语复杂，艺术家使用时发音和含义更难理解，因此进行将芬兰说唱歌曲翻译成文本这一有趣且具挑战性的尝试。

Method: 比较Faster Whisperer算法和YouTube内部语音转文本功能，以芬兰说唱歌词为参考标准，通过对比与原歌词的错误来衡量AI语音转文本提取的幻觉水平和误听情况。

Result: 未提及。

Conclusion: 未提及。

Abstract: All languages are peculiar. Some of them are considered more challenging to
understand than others. The Finnish Language is known to be a complex language.
Also, when languages are used by artists, the pronunciation and meaning might
be more tricky to understand. Therefore, we are putting AI to a fun, yet
challenging trial: translating a Finnish rap song to text. We will compare the
Faster Whisperer algorithm and YouTube's internal speech-to-text functionality.
The reference truth will be Finnish rap lyrics, which the main author's little
brother, Mc Timo, has written. Transcribing the lyrics will be challenging
because the artist raps over synth music player by Syntikka Janne. The
hallucination level and mishearing of AI speech-to-text extractions will be
measured by comparing errors made against the original Finnish lyrics. The
error function is informal but still works for our case.

</details>


### [181] [Efficient and Privacy-Preserving Soft Prompt Transfer for LLMs](https://arxiv.org/abs/2506.16196)
*Xun Wang,Jing Xu,Franziska Boenisch,Michael Backes,Christopher A. Choquette-Choo,Adam Dziedzic*

Main category: cs.LG

TL;DR: 提出POST框架解决软提示泛化问题，减少计算成本并保护隐私


<details>
  <summary>Details</summary>
Motivation: 软提示与特定大语言模型耦合，泛化性差，存在计算成本高和隐私问题

Method: 提出POST框架，用知识蒸馏从大模型导出小模型，本地调优软提示，再用公共数据集转移到大型LLM

Result: POST降低计算成本、保护隐私且有效转移高实用软提示

Conclusion: POST框架可解决软提示应用中的计算和隐私问题，实现有效转移

Abstract: Prompting has become a dominant paradigm for adapting large language models
(LLMs). While discrete (textual) prompts are widely used for their
interpretability, soft (parameter) prompts have recently gained traction in
APIs. This is because they can encode information from more training samples
while minimizing the user's token usage, leaving more space in the context
window for task-specific input. However, soft prompts are tightly coupled to
the LLM they are tuned on, limiting their generalization to other LLMs. This
constraint is particularly problematic for efficiency and privacy: (1) tuning
prompts on each LLM incurs high computational costs, especially as LLMs
continue to grow in size. Additionally, (2) when the LLM is hosted externally,
soft prompt tuning often requires sharing private data with the LLM provider.
For instance, this is the case with the NVIDIA NeMo API. To address these
issues, we propose POST (Privacy Of Soft prompt Transfer), a framework that
enables private tuning of soft prompts on a small model and subsequently
transfers these prompts to a larger LLM. POST uses knowledge distillation to
derive a small model directly from the large LLM to improve prompt
transferability, tunes the soft prompt locally, optionally with differential
privacy guarantees, and transfers it back to the larger LLM using a small
public dataset. Our experiments show that POST reduces computational costs,
preserves privacy, and effectively transfers high-utility soft prompts.

</details>


### [182] [From Pixels to CSI: Distilling Latent Dynamics For Efficient Wireless Resource Management](https://arxiv.org/abs/2506.16216)
*Charbel Bou Chaaya,Abanoub M. Girgis,Mehdi Bennis*

Main category: cs.LG

TL;DR: 提出新ML技术优化通信系统无线电资源管理，减少超50%发射功率且保持控制性能。


<details>
  <summary>Details</summary>
Motivation: 在不影响控制任务性能的前提下，优化远程控制器与其设备间通信系统的无线电资源管理。

Method: 提出新ML技术，利用两个耦合的联合嵌入预测架构（JEPAs）对控制系统和无线传播环境建模与预测，训练深度强化学习算法得出控制策略和功率预测器。

Result: 在合成多模态数据上的模拟结果显示，该方法减少超50%发射功率，控制性能与未考虑无线优化的基线方法相当。

Conclusion: 所提方法能有效减少无线电资源使用，同时维持控制性能。

Abstract: In this work, we aim to optimize the radio resource management of a
communication system between a remote controller and its device, whose state is
represented through image frames, without compromising the performance of the
control task. We propose a novel machine learning (ML) technique to jointly
model and predict the dynamics of the control system as well as the wireless
propagation environment in latent space. Our method leverages two coupled
joint-embedding predictive architectures (JEPAs): a control JEPA models the
control dynamics and guides the predictions of a wireless JEPA, which captures
the dynamics of the device's channel state information (CSI) through
cross-modal conditioning. We then train a deep reinforcement learning (RL)
algorithm to derive a control policy from latent control dynamics and a power
predictor to estimate scheduling intervals with favorable channel conditions
based on latent CSI representations. As such, the controller minimizes the
usage of radio resources by utilizing the coupled JEPA networks to imagine the
device's trajectory in latent space. We present simulation results on synthetic
multimodal data and show that our proposed approach reduces transmit power by
over 50% while maintaining control performance comparable to baseline methods
that do not account for wireless optimization.

</details>


### [183] [Think Global, Act Local: Bayesian Causal Discovery with Language Models in Sequential Data](https://arxiv.org/abs/2506.16234)
*Prakhar Verma,David Arbour,Sunav Choudhary,Harshita Chopra,Arno Solin,Atanu R. Sinha*

Main category: cs.LG

TL;DR: 提出BLANCE框架，将顺序批量数据与语言模型衍生的专家知识自适应集成，解决因果发现中的数据分批和专家知识稀缺问题，表现优于先前工作。


<details>
  <summary>Details</summary>
Motivation: 实际中因果发现面临数据分批到达和专家知识稀缺的问题，语言模型虽可替代但存在问题。

Method: 提出BLANCE混合贝叶斯框架，将顺序批量数据与语言模型衍生的有噪声专家知识自适应集成，将DAG表示转换为PAG以处理模糊性，用顺序优化方案引导语言模型交互。

Result: 在不同数据集上，BLANCE在结构准确性上优于先前工作，可扩展到贝叶斯参数估计，对语言模型噪声有鲁棒性。

Conclusion: BLANCE能有效解决因果发现中的实际问题，有较好性能和鲁棒性。

Abstract: Causal discovery from observational data typically assumes full access to
data and availability of domain experts. In practice, data often arrive in
batches, and expert knowledge is scarce. Language Models (LMs) offer a
surrogate but come with their own issues-hallucinations, inconsistencies, and
bias. We present BLANCE (Bayesian LM-Augmented Causal Estimation)-a hybrid
Bayesian framework that bridges these gaps by adaptively integrating sequential
batch data with LM-derived noisy, expert knowledge while accounting for both
data-induced and LM-induced biases. Our proposed representation shift from
Directed Acyclic Graph (DAG) to Partial Ancestral Graph (PAG) accommodates
ambiguities within a coherent Bayesian framework, allowing grounding the global
LM knowledge in local observational data. To guide LM interaction, we use a
sequential optimization scheme that adaptively queries the most informative
edges. Across varied datasets, BLANCE outperforms prior work in structural
accuracy and extends to Bayesian parameter estimation, showing robustness to LM
noise.

</details>


### [184] [Active MRI Acquisition with Diffusion Guided Bayesian Experimental Design](https://arxiv.org/abs/2506.16237)
*Jacopo Iollo,Geoffroy Oudoumanessah,Carole Lartizien,Michel Dojat,Florence Forbes*

Main category: cs.LG

TL;DR: 提出用顺序贝叶斯实验设计（BED）平衡MRI采集速度与图像质量，展示了方法在多个MRI采集中的性能。


<details>
  <summary>Details</summary>
Motivation: 在临床环境中最大化MRI益处需加速采集时间且不显著降低图像质量，要平衡欠采样和信息收集。

Method: 使用顺序BED提供自适应和任务相关的最具信息性测量选择，引入新的主动BED程序，利用基于扩散的生成模型处理图像高维性，采用随机优化选择采样模式。

Result: 能优化标准图像重建和相关图像分析任务。

Conclusion: 所提方法具有通用性和良好性能，在多个MRI采集中得到验证。

Abstract: A key challenge in maximizing the benefits of Magnetic Resonance Imaging
(MRI) in clinical settings is to accelerate acquisition times without
significantly degrading image quality. This objective requires a balance
between under-sampling the raw k-space measurements for faster acquisitions and
gathering sufficient raw information for high-fidelity image reconstruction and
analysis tasks. To achieve this balance, we propose to use sequential Bayesian
experimental design (BED) to provide an adaptive and task-dependent selection
of the most informative measurements. Measurements are sequentially augmented
with new samples selected to maximize information gain on a posterior
distribution over target images. Selection is performed via a gradient-based
optimization of a design parameter that defines a subsampling pattern. In this
work, we introduce a new active BED procedure that leverages diffusion-based
generative models to handle the high dimensionality of the images and employs
stochastic optimization to select among a variety of patterns while meeting the
acquisition process constraints and budget. So doing, we show how our setting
can optimize, not only standard image reconstruction, but also any associated
image analysis task. The versatility and performance of our approach are
demonstrated on several MRI acquisitions.

</details>


### [185] [Synthetic ALS-EEG Data Augmentation for ALS Diagnosis Using Conditional WGAN with Weight Clipping](https://arxiv.org/abs/2506.16243)
*Abdulvahap Mutlu,Şengül Doğan,Türker Tuncer*

Main category: cs.LG

TL;DR: 本文用条件Wasserstein生成对抗网络（CWGAN）为肌萎缩侧索硬化症（ALS）患者生成合成脑电图信号，缓解数据稀缺和类别不平衡问题，生成信号逼真，有望用于训练分类器。


<details>
  <summary>Details</summary>
Motivation: ALS患者高质量脑电图数据稀缺，且与健康对照记录存在严重类别不平衡，难以训练可靠机器学习分类器。

Method: 在私有脑电图数据集上训练CWGAN学习ALS脑电图信号分布，生成合成样本，对脑电图记录进行预处理和归一化，选择关键超参数稳定训练。

Result: 生成的信号在定性评估中与真实ALS脑电图模式相似，训练中生成器和判别器损失曲线稳定，信号逼真。

Conclusion: 合成脑电图信号可作为增强数据训练分类器，缓解类别不平衡，提高ALS检测准确性，有助于数据共享和增强诊断模型。

Abstract: Amyotrophic Lateral Sclerosis (ALS) is a rare neurodegenerative disease, and
high-quality EEG data from ALS patients are scarce. This data scarcity, coupled
with severe class imbalance between ALS and healthy control recordings, poses a
challenge for training reliable machine learning classifiers. In this work, we
address these issues by generating synthetic EEG signals for ALS patients using
a Conditional Wasserstein Generative Adversarial Network (CWGAN). We train
CWGAN on a private EEG dataset (ALS vs. non-ALS) to learn the distribution of
ALS EEG signals and produce realistic synthetic samples. We preprocess and
normalize EEG recordings, and train a CWGAN model to generate synthetic ALS
signals. The CWGAN architecture and training routine are detailed, with key
hyperparameters chosen for stable training. Qualitative evaluation of generated
signals shows that they closely mimic real ALS EEG patterns. The CWGAN training
converged with generator and discriminator loss curves stabilizing, indicating
successful learning. The synthetic EEG signals appear realistic and have
potential use as augmented data for training classifiers, helping to mitigate
class imbalance and improve ALS detection accuracy. We discuss how this
approach can facilitate data sharing and enhance diagnostic models.

</details>


### [186] [Next-Token Prediction Should be Ambiguity-Sensitive: A Meta-Learning Perspective](https://arxiv.org/abs/2506.16288)
*Leo Gagnon,Eric Elmoznino,Sarthak Mittal,Tom Marty,Tejas Kasetty,Dhanya Sridhar,Guillaume Lajoie*

Main category: cs.LG

TL;DR: 研究自回归基础模型适应性，引入MetaHMM测试，提出将预训练模型转为蒙特卡罗预测器的方法，初步结果有提升但有挑战。


<details>
  <summary>Details</summary>
Motivation: 自回归基础模型在高歧义情况下贝叶斯最优预测计算难，需探索不同模糊度预测的处理方法。

Method: 引入MetaHMM基准测试，提出将预训练模型转换为蒙特卡罗预测器的方法。

Result: Transformers在高歧义预测有困难，转换后的模型在歧义上下文有显著提升。

Conclusion: 转换预训练模型为蒙特卡罗预测器有一定效果，但仍存在挑战。

Abstract: The rapid adaptation ability of auto-regressive foundation models is often
attributed to the diversity of their pre-training data. This is because, from a
Bayesian standpoint, minimizing prediction error in such settings requires
integrating over all plausible latent hypotheses consistent with observations.
While this behavior is desirable in principle, it often proves too ambitious in
practice: under high ambiguity, the number of plausible latent alternatives
makes Bayes-optimal prediction computationally intractable. Cognitive science
has long recognized this limitation, suggesting that under such conditions,
heuristics or information-seeking strategies are preferable to exhaustive
inference. Translating this insight to next-token prediction, we hypothesize
that low- and high-ambiguity predictions pose different computational demands,
making ambiguity-agnostic next-token prediction a detrimental inductive bias.
To test this, we introduce MetaHMM, a synthetic sequence meta-learning
benchmark with rich compositional structure and a tractable Bayesian oracle. We
show that Transformers indeed struggle with high-ambiguity predictions across
model sizes. Motivated by cognitive theories, we propose a method to convert
pre-trained models into Monte Carlo predictors that decouple task inference
from token prediction. Preliminary results show substantial gains in ambiguous
contexts through improved capacity allocation and test-time scalable inference,
though challenges remain.

</details>


### [187] [Optimizing Multilingual Text-To-Speech with Accents & Emotions](https://arxiv.org/abs/2506.16310)
*Pranav Pawar,Akshansh Dwivedi,Jenish Boricha,Himanshu Gohil,Aditya Dubey*

Main category: cs.LG

TL;DR: 本文提出新的TTS架构，集成口音和多尺度情感建模，在印地语和印度英语口音上效果好，提高了口音准确性和情感识别率，主观评价也佳，推动跨语言合成。


<details>
  <summary>Details</summary>
Motivation: 现有文本到语音（TTS）系统在多语言环境中合成正确口音和相关情感的语音存在困难，尤其是针对印度语言。

Method: 引入新的TTS架构，扩展Parler - TTS模型，集成特定语言音素对齐混合编解码器架构、基于母语者语料训练的文化敏感情感嵌入层，结合动态口音代码切换和残差矢量量化。

Result: 定量测试显示口音准确率提高23.7%，情感识别准确率达85.3%，超越基线；200名用户主观评价文化正确性平均得分4.2/5。

Conclusion: 该研究展示了可扩展的口音 - 情感解耦，使跨语言合成更可行，可应用于南亚教育科技和无障碍软件。

Abstract: State-of-the-art text-to-speech (TTS) systems realize high naturalness in
monolingual environments, synthesizing speech with correct multilingual accents
(especially for Indic languages) and context-relevant emotions still poses
difficulty owing to cultural nuance discrepancies in current frameworks. This
paper introduces a new TTS architecture integrating accent along with
preserving transliteration with multi-scale emotion modelling, in particularly
tuned for Hindi and Indian English accent. Our approach extends the Parler-TTS
model by integrating A language-specific phoneme alignment hybrid
encoder-decoder architecture, and culture-sensitive emotion embedding layers
trained on native speaker corpora, as well as incorporating a dynamic accent
code switching with residual vector quantization. Quantitative tests
demonstrate 23.7% improvement in accent accuracy (Word Error Rate reduction
from 15.4% to 11.8%) and 85.3% emotion recognition accuracy from native
listeners, surpassing METTS and VECL-TTS baselines. The novelty of the system
is that it can mix code in real time - generating statements such as "Namaste,
let's talk about <Hindi phrase>" with uninterrupted accent shifts while
preserving emotional consistency. Subjective evaluation with 200 users reported
a mean opinion score (MOS) of 4.2/5 for cultural correctness, much better than
existing multilingual systems (p<0.01). This research makes cross-lingual
synthesis more feasible by showcasing scalable accent-emotion disentanglement,
with direct application in South Asian EdTech and accessibility software.

</details>


### [188] [Improved Exploration in GFlownets via Enhanced Epistemic Neural Networks](https://arxiv.org/abs/2506.16313)
*Sajan Muhammad,Salem Lahlou*

Main category: cs.LG

TL;DR: 本文提出ENN - GFN - Enhanced算法，将认知神经网络与GFlowNets结合，以改善探索和识别最优轨迹，并通过实验验证其有效性和效率。


<details>
  <summary>Details</summary>
Motivation: 解决GFlowNets中有效识别训练轨迹的问题，需要在奖励分布学习不足的状态空间区域进行探索，即进行不确定性驱动的探索。

Method: 将认知神经网络（ENN）与GFlowNets的传统架构集成，实现更高效的联合预测和更好的不确定性量化。

Result: 提出的ENN - GFN - Enhanced算法与GFlownets的基线方法进行比较，在网格环境和结构化序列生成的各种设置中进行评估，显示出有效性和效率。

Conclusion: 所提出的算法能够有效改善探索和识别最优轨迹，具备有效性和效率。

Abstract: Efficiently identifying the right trajectories for training remains an open
problem in GFlowNets. To address this, it is essential to prioritize
exploration in regions of the state space where the reward distribution has not
been sufficiently learned. This calls for uncertainty-driven exploration, in
other words, the agent should be aware of what it does not know. This attribute
can be measured by joint predictions, which are particularly important for
combinatorial and sequential decision problems. In this research, we integrate
epistemic neural networks (ENN) with the conventional architecture of GFlowNets
to enable more efficient joint predictions and better uncertainty
quantification, thereby improving exploration and the identification of optimal
trajectories. Our proposed algorithm, ENN-GFN-Enhanced, is compared to the
baseline method in GFlownets and evaluated in grid environments and structured
sequence generation in various settings, demonstrating both its efficacy and
efficiency.

</details>


### [189] [Signatures to help interpretability of anomalies](https://arxiv.org/abs/2506.16314)
*Emmanuel Gangler,Emille E. O. Ishida,Matwey V. Kornilov,Vladimir Korolev,Anastasia Lavrukhina,Konstantin Malanchev,Maria V. Pruzhinskaya,Etienne Russeil,Timofey Semenikhin,Sreevarsha Sreejith,Alina A. Volnova*

Main category: cs.LG

TL;DR: 引入异常特征概念以提升异常检测可解释性


<details>
  <summary>Details</summary>
Motivation: 机器学习输出常被视为黑箱，自动异常检测中天文学家需独立分析数据理解异常判定原因

Method: 引入异常特征（anomaly signature）概念

Result: 无明确提及

Conclusion: 引入异常特征有助于提升异常检测的可解释性

Abstract: Machine learning is often viewed as a black box when it comes to
understanding its output, be it a decision or a score. Automatic anomaly
detection is no exception to this rule, and quite often the astronomer is left
to independently analyze the data in order to understand why a given event is
tagged as an anomaly. We introduce here idea of anomaly signature, whose aim is
to help the interpretability of anomalies by highlighting which features
contributed to the decision.

</details>


### [190] [Bayesian Optimization over Bounded Domains with the Beta Product Kernel](https://arxiv.org/abs/2506.16316)
*Huy Hoang Nguyen,Han Zhou,Matthew B. Blaschko,Aleksei Tiulpin*

Main category: cs.LG

TL;DR: 提出Beta核用于优化有界域黑箱函数，实验表明其优于多种核函数。


<details>
  <summary>Details</summary>
Motivation: 常用的Matérn和RBF协方差函数未考虑函数定义域，在有界域适用性受限。

Method: 引入由Beta分布密度函数乘积诱导的非平稳Beta核，并分析其谱特性。

Result: Beta核在有界域函数建模中表现稳健，在不同问题中始终优于多种核函数。

Conclusion: Beta核适用于有界域黑箱函数优化，能有效克服现有核函数的局限性。

Abstract: Bayesian optimization with Gaussian processes (GP) is commonly used to
optimize black-box functions. The Mat\'ern and the Radial Basis Function (RBF)
covariance functions are used frequently, but they do not make any assumptions
about the domain of the function, which may limit their applicability in
bounded domains. To address the limitation, we introduce the Beta kernel, a
non-stationary kernel induced by a product of Beta distribution density
functions. Such a formulation allows our kernel to naturally model functions on
bounded domains. We present statistical evidence supporting the hypothesis that
the kernel exhibits an exponential eigendecay rate, based on empirical analyses
of its spectral properties across different settings. Our experimental results
demonstrate the robustness of the Beta kernel in modeling functions with optima
located near the faces or vertices of the unit hypercube. The experiments show
that our kernel consistently outperforms a wide range of kernels, including the
well-known Mat\'ern and RBF, in different problems, including synthetic
function optimization and the compression of vision and language models.

</details>


### [191] [Watermarking Autoregressive Image Generation](https://arxiv.org/abs/2506.16349)
*Nikola Jovanović,Ismail Labiad,Tomáš Souček,Martin Vechev,Pierre Fernandez*

Main category: cs.LG

TL;DR: 本文首次提出在自回归图像生成模型输出的令牌级别进行水印嵌入，通过微调及同步层实现可靠水印检测。


<details>
  <summary>Details</summary>
Motivation: 此前无工作尝试在自回归图像生成模型输出的令牌级别进行水印嵌入，为追踪生成图像来源，需开展相关研究。

Method: 采用适配语言模型水印技术，引入自定义分词器 - 解分词器微调程序提升反向循环一致性，增加水印同步层。

Result: 实验表明该方法能实现可靠且鲁棒的水印检测，并具有理论依据的 p 值。

Conclusion: 所提出的在令牌级别对自回归图像生成模型输出进行水印嵌入的方法有效且可行。

Abstract: Watermarking the outputs of generative models has emerged as a promising
approach for tracking their provenance. Despite significant interest in
autoregressive image generation models and their potential for misuse, no prior
work has attempted to watermark their outputs at the token level. In this work,
we present the first such approach by adapting language model watermarking
techniques to this setting. We identify a key challenge: the lack of reverse
cycle-consistency (RCC), wherein re-tokenizing generated image tokens
significantly alters the token sequence, effectively erasing the watermark. To
address this and to make our method robust to common image transformations,
neural compression, and removal attacks, we introduce (i) a custom
tokenizer-detokenizer finetuning procedure that improves RCC, and (ii) a
complementary watermark synchronization layer. As our experiments demonstrate,
our approach enables reliable and robust watermark detection with theoretically
grounded p-values.

</details>


### [192] [Data-Driven Policy Mapping for Safe RL-based Energy Management Systems](https://arxiv.org/abs/2506.16352)
*Theo Zangato,Aomar Osmani,Pegah Alizadeh*

Main category: cs.LG

TL;DR: 提出基于强化学习的三步建筑能源管理系统，结合聚类、预测和约束策略学习，在实际数据评估中表现良好，实现可扩展、稳健且经济高效的建筑能源管理。


<details>
  <summary>Details</summary>
Motivation: 全球能源需求增加和可再生能源整合复杂性提升，建筑成为可持续能源管理核心，需解决建筑能源管理系统的可扩展性、适应性和安全性挑战。

Method: 采用三步法，先聚类非可转移负荷曲线，再集成基于LSTM的预测模块，最后使用领域信息动作掩码。

Result: 在实际数据上评估，特定建筑类型运营成本最多降低15%，保持稳定环境性能，能快速分类和优化数据有限的新建筑，且无需重新训练即可适应随机电价变化。

Conclusion: 该框架可实现可扩展、稳健且经济高效的建筑能源管理。

Abstract: Increasing global energy demand and renewable integration complexity have
placed buildings at the center of sustainable energy management. We present a
three-step reinforcement learning(RL)-based Building Energy Management System
(BEMS) that combines clustering, forecasting, and constrained policy learning
to address scalability, adaptability, and safety challenges. First, we cluster
non-shiftable load profiles to identify common consumption patterns, enabling
policy generalization and transfer without retraining for each new building.
Next, we integrate an LSTM based forecasting module to anticipate future
states, improving the RL agents' responsiveness to dynamic conditions. Lastly,
domain-informed action masking ensures safe exploration and operation,
preventing harmful decisions. Evaluated on real-world data, our approach
reduces operating costs by up to 15% for certain building types, maintains
stable environmental performance, and quickly classifies and optimizes new
buildings with limited data. It also adapts to stochastic tariff changes
without retraining. Overall, this framework delivers scalable, robust, and
cost-effective building energy management.

</details>


### [193] [Classification of Cattle Behavior and Detection of Heat (Estrus) using Sensor Data](https://arxiv.org/abs/2506.16380)
*Druva Dhakshinamoorthy,Avikshit Jha,Sabyasachi Majumdar,Devdulal Ghosh,Ranjita Chakraborty,Hena Ray*

Main category: cs.LG

TL;DR: 本文提出基于传感器数据和机器学习的牛行为监测与发情期检测系统，经评估准确率高，适用于资源受限环境。


<details>
  <summary>Details</summary>
Motivation: 开发一种可用于监测牛行为和检测发情期的系统，以实现精准畜牧监测。

Method: 设计并部署低成本蓝牙项圈收集牛行为数据同步到云，用CCTV录像标注行为创建数据集，评估SVM、RF、CNN进行行为分类，用LSTM进行发情检测。

Result: 系统在有限测试集上行为分类准确率超93%，发情检测准确率达96%。

Conclusion: 该方法为精准畜牧监测提供了可扩展且易获取的解决方案，尤其适用于资源受限环境。

Abstract: This paper presents a novel system for monitoring cattle behavior and
detecting estrus (heat) periods using sensor data and machine learning. We
designed and deployed a low-cost Bluetooth-based neck collar equipped with
accelerometer and gyroscope sensors to capture real-time behavioral data from
real cows, which was synced to the cloud. A labeled dataset was created using
synchronized CCTV footage to annotate behaviors such as feeding, rumination,
lying, and others. We evaluated multiple machine learning models -- Support
Vector Machines (SVM), Random Forests (RF), and Convolutional Neural Networks
(CNN) -- for behavior classification. Additionally, we implemented a Long
Short-Term Memory (LSTM) model for estrus detection using behavioral patterns
and anomaly detection. Our system achieved over 93% behavior classification
accuracy and 96% estrus detection accuracy on a limited test set. The approach
offers a scalable and accessible solution for precision livestock monitoring,
especially in resource-constrained environments.

</details>


### [194] [State-Space Kolmogorov Arnold Networks for Interpretable Nonlinear System Identification](https://arxiv.org/abs/2506.16392)
*Gonçalo Granjal Cruz,Balazs Renczes,Mark C Runacres,Jan Decuyper*

Main category: cs.LG

TL;DR: 本文提出SS - KAN模型解决黑盒系统识别模型缺乏可解释性的问题，在两个基准系统验证，结果显示该模型可提升可解释性，平衡了准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 黑盒系统识别模型虽准确但缺乏对底层系统动态的可解释性。

Method: 在状态空间框架内集成Kolmogorov - Arnold网络，提出State - Space Kolmogorov - Arnold Networks (SS - KAN)模型。

Result: 在Silverbox和Wiener - Hammerstein基准系统上验证，SS - KAN因促进稀疏性的正则化和可直接可视化学习的单变量函数，增强了可解释性，但与最先进黑盒模型相比牺牲了一定准确性。

Conclusion: SS - KAN是一种有前景的可解释非线性系统识别方法，平衡了非线性系统动态的准确性和可解释性。

Abstract: While accurate, black-box system identification models lack interpretability
of the underlying system dynamics. This paper proposes State-Space
Kolmogorov-Arnold Networks (SS-KAN) to address this challenge by integrating
Kolmogorov-Arnold Networks within a state-space framework. The proposed model
is validated on two benchmark systems: the Silverbox and the Wiener-Hammerstein
benchmarks. Results show that SS-KAN provides enhanced interpretability due to
sparsity-promoting regularization and the direct visualization of its learned
univariate functions, which reveal system nonlinearities at the cost of
accuracy when compared to state-of-the-art black-box models, highlighting
SS-KAN as a promising approach for interpretable nonlinear system
identification, balancing accuracy and interpretability of nonlinear system
dynamics.

</details>


### [195] [GoalLadder: Incremental Goal Discovery with Vision-Language Models](https://arxiv.org/abs/2506.16396)
*Alexey Zakharov,Shimon Whiteson*

Main category: cs.LG

TL;DR: 本文提出GoalLadder方法，利用视觉语言模型从单一语言指令训练强化学习智能体，在视觉环境中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有从语言指令提取奖励的方法在视觉环境存在问题，如依赖非视觉环境表示、需大量反馈或生成的奖励函数有噪声。

Method: 提出GoalLadder方法，通过查询视觉语言模型识别任务进展中的改进状态并排序，使用ELO评级系统减少噪声反馈影响，让智能体在学习的嵌入空间中最小化与排名最高目标的距离。

Result: GoalLadder在经典控制和机器人操作环境中平均最终成功率约95%，而最佳竞争对手约45%。

Conclusion: GoalLadder方法在视觉环境中从单一语言指令训练强化学习智能体方面表现出色，优于现有相关方法。

Abstract: Natural language can offer a concise and human-interpretable means of
specifying reinforcement learning (RL) tasks. The ability to extract rewards
from a language instruction can enable the development of robotic systems that
can learn from human guidance; however, it remains a challenging problem,
especially in visual environments. Existing approaches that employ large,
pretrained language models either rely on non-visual environment
representations, require prohibitively large amounts of feedback, or generate
noisy, ill-shaped reward functions. In this paper, we propose a novel method,
$\textbf{GoalLadder}$, that leverages vision-language models (VLMs) to train RL
agents from a single language instruction in visual environments. GoalLadder
works by incrementally discovering states that bring the agent closer to
completing a task specified in natural language. To do so, it queries a VLM to
identify states that represent an improvement in agent's task progress and to
rank them using pairwise comparisons. Unlike prior work, GoalLadder does not
trust VLM's feedback completely; instead, it uses it to rank potential goal
states using an ELO-based rating system, thus reducing the detrimental effects
of noisy VLM feedback. Over the course of training, the agent is tasked with
minimising the distance to the top-ranked goal in a learned embedding space,
which is trained on unlabelled visual data. This key feature allows us to
bypass the need for abundant and accurate feedback typically required to train
a well-shaped reward function. We demonstrate that GoalLadder outperforms
existing related methods on classic control and robotic manipulation
environments with the average final success rate of $\sim$95% compared to only
$\sim$45% of the best competitor.

</details>


### [196] [Generating Directed Graphs with Dual Attention and Asymmetric Encoding](https://arxiv.org/abs/2506.16404)
*Alba Carballo-Castro,Manuel Madeira,Yiming Qin,Dorina Thanou,Pascal Frossard*

Main category: cs.LG

TL;DR: 本文提出首个基于离散流匹配框架的有向图生成模型Directo，并引入基准套件，实验显示该方法有效且通用。


<details>
  <summary>Details</summary>
Motivation: 有向图生成研究不足，存在建模边方向性依赖空间大、缺乏标准化基准两个限制因素。

Method: 提出Directo模型，结合定制位置编码、双注意力机制和离散生成框架，引入基准套件。

Result: 方法在不同设置下表现出色，能与特定类型的专业模型竞争。

Conclusion: 方法有效且通用，为有向图生成的未来研究奠定基础。

Abstract: Directed graphs naturally model systems with asymmetric, ordered
relationships, essential to applications in biology, transportation, social
networks, and visual understanding. Generating such graphs enables tasks such
as simulation, data augmentation and novel instance discovery; however,
directed graph generation remains underexplored. We identify two key factors
limiting progress in this direction: first, modeling edge directionality
introduces a substantially larger dependency space, making the underlying
distribution harder to learn; second, the absence of standardized benchmarks
hinders rigorous evaluation. Addressing the former requires more expressive
models that are sensitive to directional topologies. We propose Directo, the
first generative model for directed graphs built upon the discrete flow
matching framework. Our approach combines: (i) principled positional encodings
tailored to asymmetric pairwise relations, (ii) a dual-attention mechanism
capturing both incoming and outgoing dependencies, and (iii) a robust, discrete
generative framework. To support evaluation, we introduce a benchmark suite
covering synthetic and real-world datasets. It shows that our method performs
strongly across diverse settings and even competes with specialized models for
particular classes, such as directed acyclic graphs. Our results highlight the
effectiveness and generality of our approach, establishing a solid foundation
for future research in directed graph generation.

</details>


### [197] [Drag-and-Drop LLMs: Zero-Shot Prompt-to-Weights](https://arxiv.org/abs/2506.16406)
*Zhiyuan Liang,Dongwen Tang,Yuhao Zhou,Xuanlei Zhao,Mingjia Shi,Wangbo Zhao,Zekai Li,Peihao Wang,Konstantin Schürholt,Damian Borth,Michael M. Bronstein,Yang You,Zhangyang Wang,Kai Wang*

Main category: cs.LG

TL;DR: 介绍Drag-and-Drop LLMs (DnD)，通过提示条件参数生成器消除每任务训练，有低开销、高性能和强泛化性等优点。


<details>
  <summary>Details</summary>
Motivation: 现有PEFT方法仍需为每个下游数据集单独优化，为降低定制大语言模型成本。

Method: 引入提示条件参数生成器DnD，用轻量级文本编码器将提示批处理为条件嵌入，再由级联超卷积解码器转换为LoRA矩阵。

Result: 开销比全微调低12,000倍，在未见基准测试上性能平均提升30%，有强跨领域泛化性。

Conclusion: 提示条件参数生成是基于梯度的适应方法的可行替代方案，可快速定制大语言模型。

Abstract: Modern Parameter-Efficient Fine-Tuning (PEFT) methods such as low-rank
adaptation (LoRA) reduce the cost of customizing large language models (LLMs),
yet still require a separate optimization run for every downstream dataset. We
introduce \textbf{Drag-and-Drop LLMs (\textit{DnD})}, a prompt-conditioned
parameter generator that eliminates per-task training by mapping a handful of
unlabeled task prompts directly to LoRA weight updates. A lightweight text
encoder distills each prompt batch into condition embeddings, which are then
transformed by a cascaded hyper-convolutional decoder into the full set of LoRA
matrices. Once trained in a diverse collection of prompt-checkpoint pairs, DnD
produces task-specific parameters in seconds, yielding i) up to
\textbf{12,000$\times$} lower overhead than full fine-tuning, ii) average gains
up to \textbf{30\%} in performance over the strongest training LoRAs on unseen
common-sense reasoning, math, coding, and multimodal benchmarks, and iii)
robust cross-domain generalization despite never seeing the target data or
labels. Our results demonstrate that prompt-conditioned parameter generation is
a viable alternative to gradient-based adaptation for rapidly specializing
LLMs. Our project is available at
\href{https://jerryliang24.github.io/DnD}{https://jerryliang24.github.io/DnD}.

</details>


### [198] [Optimizing MoE Routers: Design, Implementation, and Evaluation in Transformer Models](https://arxiv.org/abs/2506.16419)
*Daniel Fidel Harvey,George Weale,Berk Yilmaz*

Main category: cs.LG

TL;DR: 本文设计并实验了不同的Mixture of Experts (MoE)路由器架构，分析其在不同模型中的表现并进行对比。


<details>
  <summary>Details</summary>
Motivation: 解决MoE架构中路由器模块可能导致的负载不均衡和准确率下降问题。

Method: 在Transformer模型中设计并实现六种不同的路由器变体，用BERT和Qwen1.5 - MoE模型进行实验，观察多项指标。

Result: 不同路由器各有优劣，线性路由器速度快，MLP和注意力路由器表达性强，MLP - Hadamard路由器适合结构化稀疏路由，还成功在Qwen1.5 - MoE模型中替换和微调自定义路由器。

Conclusion: 对MoE路由器设计进行了比较分析，为大规模模型高效部署提供优化见解。

Abstract: Mixture of Experts (MoE) architectures increase large language model
scalability, yet their performance depends on the router module that moves
tokens to specialized experts. Bad routing can load imbalance and reduced
accuracy. This project designed and implemented different router architectures
within Transformer models to fix these limitations. We experimented with six
distinct router variants Linear, Attention, Multi-Layer Perceptron (MLP),
Hybrid, Hash, and our new MLP-Hadamard. We characterized these routers using
BERT and the Qwen1.5-MoE model, looking at parameter efficiency, inference
latency, routing entropy, and expert utilization patterns. Our evaluations
showed distinct trade-offs: Linear routers offer speed, while MLP and Attention
routers provide greater expressiveness. The MLP-Hadamard router shows a unique
capability for structured, sparse routing. We successfully replaced and
fine-tuned custom routers within the complex, quantized Qwen1.5-MoE model. This
work provides a comparative analysis of MoE router designs and offers insights
into optimizing their performance for efficient and effective large-scale model
deployment.

</details>


### [199] [EFormer: An Effective Edge-based Transformer for Vehicle Routing Problems](https://arxiv.org/abs/2506.16428)
*Dian Meng,Zhiguang Cao,Yaoxin Wu,Yaqing Hou,Hongwei Ge,Qiang Zhang*

Main category: cs.LG

TL;DR: 提出基于边的Transformer模型EFormer解决车辆路径规划问题，实验表明其优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有基于节点坐标的神经启发式方法在实际场景效果不佳，需使用边信息。

Method: 采用预编码器模块和混合得分注意力机制将边信息转换为临时节点嵌入，并行编码策略处理图和节点嵌入，解码阶段使用并行上下文嵌入和多查询集成，通过强化学习训练。

Result: 在TSP和CVRP问题的合成数据集上优于现有基线，在真实世界实例上有强泛化能力。

Conclusion: EFormer核心设计对解决VRP问题有效。

Abstract: Recent neural heuristics for the Vehicle Routing Problem (VRP) primarily rely
on node coordinates as input, which may be less effective in practical
scenarios where real cost metrics-such as edge-based distances-are more
relevant. To address this limitation, we introduce EFormer, an Edge-based
Transformer model that uses edge as the sole input for VRPs. Our approach
employs a precoder module with a mixed-score attention mechanism to convert
edge information into temporary node embeddings. We also present a parallel
encoding strategy characterized by a graph encoder and a node encoder, each
responsible for processing graph and node embeddings in distinct feature
spaces, respectively. This design yields a more comprehensive representation of
the global relationships among edges. In the decoding phase, parallel context
embedding and multi-query integration are used to compute separate attention
mechanisms over the two encoded embeddings, facilitating efficient path
construction. We train EFormer using reinforcement learning in an
autoregressive manner. Extensive experiments on the Traveling Salesman Problem
(TSP) and Capacitated Vehicle Routing Problem (CVRP) reveal that EFormer
outperforms established baselines on synthetic datasets, including large-scale
and diverse distributions. Moreover, EFormer demonstrates strong generalization
on real-world instances from TSPLib and CVRPLib. These findings confirm the
effectiveness of EFormer's core design in solving VRPs.

</details>


### [200] [An efficient neuromorphic approach for collision avoidance combining Stack-CNN with event cameras](https://arxiv.org/abs/2506.16436)
*Antonio Giulio Coretti,Mattia Varile,Mario Edoardo Bertaina*

Main category: cs.LG

TL;DR: 本文提出利用基于事件的相机的创新避碰系统，测试显示算法有提升信噪比能力，对空间成像和管理有积极意义。


<details>
  <summary>Details</summary>
Motivation: 太空碎片构成重大威胁，需研究主动和被动缓解策略，开发适合太空态势感知和交通管理的技术。

Method: 提出利用基于事件的相机的避碰系统，采用Stack - CNN算法分析实时相机数据检测微弱移动物体。

Result: 在地面数据测试中，算法能提升信噪比。

Conclusion: 该系统为星载空间成像和改善太空交通管理/态势感知操作提供了有前景的方法。

Abstract: Space debris poses a significant threat, driving research into active and
passive mitigation strategies. This work presents an innovative collision
avoidance system utilizing event-based cameras - a novel imaging technology
well-suited for Space Situational Awareness (SSA) and Space Traffic Management
(STM). The system, employing a Stack-CNN algorithm (previously used for meteor
detection), analyzes real-time event-based camera data to detect faint moving
objects. Testing on terrestrial data demonstrates the algorithm's ability to
enhance signal-to-noise ratio, offering a promising approach for on-board space
imaging and improving STM/SSA operations.

</details>


### [201] [Leveraging Influence Functions for Resampling Data in Physics-Informed Neural Networks](https://arxiv.org/abs/2506.16443)
*Jonas R. Naujoks,Aleksander Krasowski,Moritz Weckbecker,Galip Ümit Yolcu,Thomas Wiegand,Sebastian Lapuschkin,Wojciech Samek,René P. Klausen*

Main category: cs.LG

TL;DR: 本文探索基于影响函数的采样方法用于物理信息神经网络（PINNs）训练数据，结果表明基于数据归因的目标重采样可提升PINNs预测精度。


<details>
  <summary>Details</summary>
Motivation: PINNs在科学机器学习领域很有价值，但缺乏提高其训练效果的方法，希望利用可解释AI中的影响函数来改进训练。

Method: 探索基于影响函数的采样方法用于训练数据。

Result: 基于数据归因方法的目标重采样有潜力提高PINNs的预测精度。

Conclusion: 展示了可解释AI方法在PINN训练中的实际应用。

Abstract: Physics-informed neural networks (PINNs) offer a powerful approach to solving
partial differential equations (PDEs), which are ubiquitous in the quantitative
sciences. Applied to both forward and inverse problems across various
scientific domains, PINNs have recently emerged as a valuable tool in the field
of scientific machine learning. A key aspect of their training is that the data
-- spatio-temporal points sampled from the PDE's input domain -- are readily
available. Influence functions, a tool from the field of explainable AI (XAI),
approximate the effect of individual training points on the model, enhancing
interpretability. In the present work, we explore the application of influence
function-based sampling approaches for the training data. Our results indicate
that such targeted resampling based on data attribution methods has the
potential to enhance prediction accuracy in physics-informed neural networks,
demonstrating a practical application of an XAI method in PINN training.

</details>


### [202] [Consumer-friendly EEG-based Emotion Recognition System: A Multi-scale Convolutional Neural Network Approach](https://arxiv.org/abs/2506.16448)
*Tri Duc Ly,Gia H. Ngo*

Main category: cs.LG

TL;DR: 提出用多尺度卷积神经网络进行基于EEG的情绪识别，模型性能优于TSception模型。


<details>
  <summary>Details</summary>
Motivation: 开发能在现实场景中进行基于EEG的情绪识别的深度学习模型。

Method: 利用多尺度卷积神经网络，实现带有多个比例系数的特征提取内核以及从大脑四个不同区域学习关键信息的新型内核。

Result: 模型在预测效价、唤醒度和支配性分数的多个性能评估指标上始终优于TSception模型。

Conclusion: 所提出的基于多尺度卷积神经网络的方法在基于EEG的情绪识别上有更好表现。

Abstract: EEG is a non-invasive, safe, and low-risk method to record
electrophysiological signals inside the brain. Especially with recent
technology developments like dry electrodes, consumer-grade EEG devices, and
rapid advances in machine learning, EEG is commonly used as a resource for
automatic emotion recognition. With the aim to develop a deep learning model
that can perform EEG-based emotion recognition in a real-life context, we
propose a novel approach to utilize multi-scale convolutional neural networks
to accomplish such tasks. By implementing feature extraction kernels with many
ratio coefficients as well as a new type of kernel that learns key information
from four separate areas of the brain, our model consistently outperforms the
state-of-the-art TSception model in predicting valence, arousal, and dominance
scores across many performance evaluation metrics.

</details>


### [203] [Black-Box Privacy Attacks on Shared Representations in Multitask Learning](https://arxiv.org/abs/2506.16460)
*John Abascal,Nicolás Berrios,Alina Oprea,Jonathan Ullman,Adam Smith,Matthew Jagielski*

Main category: cs.LG

TL;DR: 本文通过推理攻击研究多任务学习共享表示泄露的信息，提出黑盒任务推理威胁模型，开发纯黑盒攻击方法并验证其有效性，还进行理论分析。


<details>
  <summary>Details</summary>
Motivation: 多任务学习的共享表示可能会不经意间泄露敏感信息，需研究其泄露情况。

Method: 提出新颖的黑盒任务推理威胁模型，开发纯黑盒攻击方法，在视觉和语言领域评估攻击效果，并进行理论分析。

Result: 即使仅访问新的任务样本，黑盒攻击者也能成功推断任务是否参与训练。

Conclusion: 实验和理论分析表明，多任务学习共享表示存在信息泄露风险，有训练样本和新样本的攻击者能力有严格区分。

Abstract: Multitask learning (MTL) has emerged as a powerful paradigm that leverages
similarities among multiple learning tasks, each with insufficient samples to
train a standalone model, to solve them simultaneously while minimizing data
sharing across users and organizations. MTL typically accomplishes this goal by
learning a shared representation that captures common structure among the tasks
by embedding data from all tasks into a common feature space. Despite being
designed to be the smallest unit of shared information necessary to effectively
learn patterns across multiple tasks, these shared representations can
inadvertently leak sensitive information about the particular tasks they were
trained on.
  In this work, we investigate what information is revealed by the shared
representations through the lens of inference attacks. Towards this, we propose
a novel, black-box task-inference threat model where the adversary, given the
embedding vectors produced by querying the shared representation on samples
from a particular task, aims to determine whether that task was present when
training the shared representation. We develop efficient, purely black-box
attacks on machine learning models that exploit the dependencies between
embeddings from the same task without requiring shadow models or labeled
reference data. We evaluate our attacks across vision and language domains for
multiple use cases of MTL and demonstrate that even with access only to fresh
task samples rather than training data, a black-box adversary can successfully
infer a task's inclusion in training. To complement our experiments, we provide
theoretical analysis of a simplified learning setting and show a strict
separation between adversaries with training samples and fresh samples from the
target task's distribution.

</details>


### [204] [Progressive Inference-Time Annealing of Diffusion Models for Sampling from Boltzmann Densities](https://arxiv.org/abs/2506.16471)
*Tara Akhound-Sadegh,Jungyoon Lee,Avishek Joey Bose,Valentin De Bortoli,Arnaud Doucet,Michael M. Bronstein,Dominique Beaini,Siamak Ravanbakhsh,Kirill Neklyudov,Alexander Tong*

Main category: cs.LG

TL;DR: 提出 Progressive Inference - Time Annealing (PITA) 框架以学习基于扩散的采样器，实现了对N体粒子系统等的平衡采样。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的采样器无法从简单分子系统规模的分布中采样，需要更有效的采样方法。

Method: 结合玻尔兹曼分布退火和扩散平滑两种插值技术，按温度从高到低训练一系列扩散模型，并通过新的费曼 - 卡茨偏微分方程结合顺序蒙特卡罗进行推理时退火以获取低温度训练样本。

Result: 首次实现了N体粒子系统、丙氨酸二肽和三肽在笛卡尔坐标下的平衡采样，且显著降低了能量函数评估次数。

Conclusion: PITA框架有效解决了从目标非归一化概率密度中高效采样的问题。

Abstract: Sampling efficiently from a target unnormalized probability density remains a
core challenge, with relevance across countless high-impact scientific
applications. A promising approach towards this challenge is the design of
amortized samplers that borrow key ideas, such as probability path design, from
state-of-the-art generative diffusion models. However, all existing
diffusion-based samplers remain unable to draw samples from distributions at
the scale of even simple molecular systems. In this paper, we propose
Progressive Inference-Time Annealing (PITA), a novel framework to learn
diffusion-based samplers that combines two complementary interpolation
techniques: I.) Annealing of the Boltzmann distribution and II.) Diffusion
smoothing. PITA trains a sequence of diffusion models from high to low
temperatures by sequentially training each model at progressively higher
temperatures, leveraging engineered easy access to samples of the
temperature-annealed target density. In the subsequent step, PITA enables
simulating the trained diffusion model to procure training samples at a lower
temperature for the next diffusion model through inference-time annealing using
a novel Feynman-Kac PDE combined with Sequential Monte Carlo. Empirically, PITA
enables, for the first time, equilibrium sampling of N-body particle systems,
Alanine Dipeptide, and tripeptides in Cartesian coordinates with dramatically
lower energy function evaluations. Code available at:
https://github.com/taraak/pita

</details>


### [205] [Manifold Learning for Personalized and Label-Free Detection of Cardiac Arrhythmias](https://arxiv.org/abs/2506.16494)
*Amir Reza Vazifeh,Jason W. Fleischer*

Main category: cs.LG

TL;DR: 本文指出手动心电图分析耗时易错，机器学习模型泛化难，传统无监督方法有局限，提出非线性降维（NLDR）可解决问题并识别心电图相关特征，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 手动心电图分析耗时易错，机器学习模型因心电图信号差异大难以泛化，传统无监督方法忽略细微临床特征，需要新方法解决这些问题。

Method: 采用非线性降维（NLDR），使用MIT - BIH数据集的MLII和V1导联，运用t分布随机邻域嵌入和均匀流形近似与投影方法。

Result: 能以>=90%的准确率区分混合人群中的个体记录，区分个体患者不同心律失常的中位准确率为98.96%，中位F1分数为91.02%。

Conclusion: NLDR在心脏监测和个性化医疗等方面有很大潜力。

Abstract: Electrocardiograms (ECGs) provide direct, non-invasive measurements of heart
activity and are well-established tools for detecting and monitoring
cardiovascular disease. However, manual ECG analysis can be time-consuming and
prone to errors. Machine learning has emerged as a promising approach for
automated heartbeat recognition and classification, but substantial variations
in ECG signals make it challenging to develop generalizable models. ECG signals
can vary widely across individuals and leads, while datasets often follow
different labeling standards and may be biased, all of which greatly hinder
supervised methods. Conventional unsupervised methods, e.g. principal component
analysis, prioritize large (and often obvious) variances in the data and
typically overlook subtle yet clinically relevant patterns. If labels are
missing and/or variations are significant but small, both approaches fail.
Here, we show that nonlinear dimensionality reduction (NLDR) can accommodate
these issues and identify medically relevant features in ECG signals, with no
need for training or prior information. Using the MLII and V1 leads of the
MIT-BIH dataset, we demonstrate that t-distributed stochastic neighbor
embedding and uniform manifold approximation and projection can discriminate
individual recordings in mixed populations with >= 90% accuracy and distinguish
different arrhythmias in individual patients with a median accuracy of 98.96%
and a median F1-score of 91.02%. The results show that NLDR holds much promise
for cardiac monitoring, including the limiting cases of single-lead ECG and the
current 12-lead standard of care, and for personalized health care beyond
cardiology.

</details>


### [206] [SparseLoRA: Accelerating LLM Fine-Tuning with Contextual Sparsity](https://arxiv.org/abs/2506.16500)
*Samir Khaki,Xiuyu Li,Junxian Guo,Ligeng Zhu,Chenfeng Xu,Konstantinos N. Plataniotis,Amir Yazdanbakhsh,Kurt Keutzer,Song Han,Zhijian Liu*

Main category: cs.LG

TL;DR: 提出SparseLoRA方法加速大语言模型微调，实验显示可降低计算成本、提升速度且保持准确率。


<details>
  <summary>Details</summary>
Motivation: 现有参数高效微调方法不能降低计算成本，甚至可能减慢微调速度。

Method: 引入通过上下文稀疏性加速大语言模型微调的SparseLoRA方法，提出轻量级、免训练的SVD稀疏估计器，系统分析并解决层、标记和训练步骤的敏感性。

Result: SparseLoRA可将计算成本降低达2.2倍，速度提升达1.6倍，在多个下游任务保持准确率。

Conclusion: SparseLoRA能有效加速大语言模型微调，降低计算成本同时保证准确率。

Abstract: Fine-tuning LLMs is both computationally and memory-intensive. While
parameter-efficient fine-tuning methods, such as QLoRA and DoRA, reduce the
number of trainable parameters and lower memory usage, they do not decrease
computational cost. In some cases, they may even slow down fine-tuning. In this
paper, we introduce SparseLoRA, a method that accelerates LLM fine-tuning
through contextual sparsity. We propose a lightweight, training-free SVD
sparsity estimator that dynamically selects a sparse subset of weights for loss
and gradient computation. Also, we systematically analyze and address
sensitivity across layers, tokens, and training steps. Our experimental results
show that SparseLoRA reduces computational cost by up to 2.2 times and a
measured speedup of up to 1.6 times while maintaining accuracy across various
downstream tasks, including commonsense and arithmetic reasoning, code
generation, and instruction following.

</details>


### [207] [Subspace-Boosted Model Merging](https://arxiv.org/abs/2506.16506)
*Ronald Skorobogat,Karsten Roth,Mariana-Iuliana Georgescu,Zeynep Akata*

Main category: cs.LG

TL;DR: 本文从任务算术角度分析模型合并问题，提出Subspace Boosting方法提升合并效果，并提出用高阶广义奇异值分解量化任务相似度。


<details>
  <summary>Details</summary>
Motivation: 现有模型合并中增加专家模型数量会导致收益递减和整体性能提升降低，需进行分析并解决该问题。

Method: 从任务算术角度分析，提出Subspace Boosting方法在奇异值分解的任务向量空间操作以维持任务向量秩，还提出用高阶广义奇异值分解量化任务相似度。

Result: Subspace Boosting在视觉基准测试中，对多达20个专家模型的合并效果大幅提升超10%。

Conclusion: 从任务算术角度能有效分析模型合并问题，提出的方法可提升模型合并效果并提供新的可解释视角。

Abstract: Model merging enables the combination of multiple specialized expert models
into a single model capable of performing multiple tasks. However, the benefits
of merging an increasing amount of specialized experts generally lead to
diminishing returns and reduced overall performance gains. In this work, we
offer an explanation and analysis from a task arithmetic perspective; revealing
that as the merging process (across numerous existing merging methods)
continues for more and more experts, the associated task vector space
experiences rank collapse. To mitigate this issue, we introduce Subspace
Boosting, which operates on the singular value decomposed task vector space and
maintains task vector ranks. Subspace Boosting raises merging efficacy for up
to 20 expert models by large margins of more than 10% when evaluated on vision
benchmarks. Moreover, we propose employing Higher-Order Generalized Singular
Value Decomposition to further quantify task similarity, offering a new
interpretable perspective on model merging.

</details>


### [208] [Robust Reward Modeling via Causal Rubrics](https://arxiv.org/abs/2506.16507)
*Pragya Srivastava,Harman Singh,Rahul Madhavan,Gandharv Patil,Sravanti Addepalli,Arun Suggala,Rengarajan Aravamudhan,Soumya Sharma,Anirban Laha,Aravindan Raghuveer,Karthikeyan Shanmugam,Doina Precup*

Main category: cs.LG

TL;DR: 现有奖励模型存在奖励破解问题，本文提出Crome框架解决该问题，实验表明其性能显著优于基线。


<details>
  <summary>Details</summary>
Motivation: 解决奖励模型易受奖励破解影响，标准训练目标难以区分因果因素导致模型脆弱和策略不一致的问题。

Method: 引入基于显式因果模型的Crome框架，在训练中采用因果增强和中性增强两种合成目标增强方法，且通过查询神谕大语言模型确定因果规则进行答案干预生成增强数据。

Result: Crome在RewardBench上显著优于标准基线，平均准确率最高提高5.4%，特定类别最高提高13.2%和7.2%，在多种基准测试中均有稳定提升。

Conclusion: Crome框架能有效缓解奖励模型的奖励破解问题，提升模型的鲁棒性。

Abstract: Reward models (RMs) are fundamental to aligning Large Language Models (LLMs)
via human feedback, yet they often suffer from reward hacking. They tend to
latch on to superficial or spurious attributes, such as response length or
formatting, mistaking these cues learned from correlations in training data for
the true causal drivers of quality (e.g., factuality, relevance). This occurs
because standard training objectives struggle to disentangle these factors,
leading to brittle RMs and misaligned policies. We introduce Crome (Causally
Robust Reward Modeling), a novel framework grounded in an explicit causal model
designed to mitigate reward hacking. Crome employs the following synthetic
targeted augmentations during training: (1) Causal Augmentations, which are
pairs that differ along specific causal attributes, to enforce sensitivity
along each causal attribute individually, and (2) Neutral Augmentations, which
are tie-label pairs varying primarily in spurious attributes, to enforce
invariance along spurious attributes. Notably, our augmentations are produced
without any knowledge of spurious factors, via answer interventions only along
causal rubrics, that are identified by querying an oracle LLM. Empirically,
Crome significantly outperforms standard baselines on RewardBench, improving
average accuracy by up to 5.4% and achieving gains of up to 13.2% and 7.2% in
specific categories. The robustness of Crome is further testified by the
consistent gains obtained in a Best-of-N inference setting across increasing N,
across various benchmarks, including the popular RewardBench (covering chat,
chat-hard, safety, and reasoning tasks), the safety-focused WildGuardTest, and
the reasoning-specific GSM8k.

</details>


### [209] [Aligning ASR Evaluation with Human and LLM Judgments: Intelligibility Metrics Using Phonetic, Semantic, and NLI Approaches](https://arxiv.org/abs/2506.16528)
*Bornali Phukon,Xiuwen Zheng,Mark Hasegawa-Johnson*

Main category: cs.LG

TL;DR: 传统ASR指标难反映清晰度，本文提出新指标，与人类判断相关性高，强调重清晰度。


<details>
  <summary>Details</summary>
Motivation: 传统ASR指标如WER和CER无法捕捉清晰度，尤其针对构音障碍和发声障碍语音；大语言模型校正此类语音转录效果待探索。

Method: 提出一种整合自然语言推理（NLI）分数、语义相似度和语音相似度的新指标。

Result: 新的ASR评估指标在语音可访问性项目数据上与人类判断的相关性达到0.890，超越传统方法。

Conclusion: 应优先考虑语音清晰度而非基于错误的度量。

Abstract: Traditional ASR metrics like WER and CER fail to capture intelligibility,
especially for dysarthric and dysphonic speech, where semantic alignment
matters more than exact word matches. ASR systems struggle with these speech
types, often producing errors like phoneme repetitions and imprecise
consonants, yet the meaning remains clear to human listeners. We identify two
key challenges: (1) Existing metrics do not adequately reflect intelligibility,
and (2) while LLMs can refine ASR output, their effectiveness in correcting ASR
transcripts of dysarthric speech remains underexplored. To address this, we
propose a novel metric integrating Natural Language Inference (NLI) scores,
semantic similarity, and phonetic similarity. Our ASR evaluation metric
achieves a 0.890 correlation with human judgments on Speech Accessibility
Project data, surpassing traditional methods and emphasizing the need to
prioritize intelligibility over error-based measures.

</details>


### [210] [Mr. Snuffleupagus at SemEval-2025 Task 4: Unlearning Factual Knowledge from LLMs Using Adaptive RMU](https://arxiv.org/abs/2506.16548)
*Arjun Dosajh,Mihika Sanghi*

Main category: cs.LG

TL;DR: 本文应用RMU技术从大语言模型中消除敏感信息，通过实验分析不同解码器层效果，该技术在1B和7B参数模型官方排行榜中排第4。


<details>
  <summary>Details</summary>
Motivation: 大语言模型有记忆训练数据的倾向，在隐私、版权合规和安全方面存在风险，尤其是涉及个人身份信息，而现有消除技术在大语言模型上发展不足。

Method: 应用自适应表示误导消除（RMU）技术从大语言模型中消除敏感信息，分析不同解码器层消除效果。

Result: 该技术在1B和7B参数模型官方排行榜中排第4。

Conclusion: RMU技术可用于大语言模型消除敏感信息，不同解码器层的消除效果有差异。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
natural language understanding and generation. However, their tendency to
memorize training data raises concerns regarding privacy, copyright compliance,
and security, particularly in cases involving Personally Identifiable
Information (PII). Effective machine unlearning techniques are essential to
mitigate these risks, yet existing methods remain underdeveloped for LLMs due
to their open-ended output space. In this work, we apply the Adaptive
Representation Misdirection Unlearning (RMU) technique to unlearn sensitive
information from LLMs. Through extensive experiments, we analyze the effects of
unlearning across different decoder layers to determine the most effective
regions for sensitive information removal. Our technique ranked 4th on the
official leaderboard of both 1B parameter and 7B parameter models.

</details>


### [211] [One Sample is Enough to Make Conformal Prediction Robust](https://arxiv.org/abs/2506.16553)
*Soroush H. Zargarbashi,Mohammad Sadegh Akhondzadeh,Aleksandar Bojchevski*

Main category: cs.LG

TL;DR: 本文提出单样本鲁棒共形预测（RCP1）方法，在保证鲁棒性的同时，相比现有方法平均集合大小更小且计算成本低，还将其扩展到基于平滑的鲁棒共形风险控制。


<details>
  <summary>Details</summary>
Motivation: 当前基于平滑的鲁棒共形预测（RCP）每个输入需要多次模型前向传播，计算成本高。

Method: 利用单次随机扰动输入的前向传播使共形预测获得一定鲁棒性，使用任何二元证书提出RCP1，对共形预测过程本身进行认证。

Result: 提出的方法与每个输入使用多次前向传播的SOTA方法相比，返回的鲁棒集平均集合大小更小。

Conclusion: 该方法对分类和回归设置均适用，且可扩展到基于平滑的鲁棒共形风险控制。

Abstract: Given any model, conformal prediction (CP) returns prediction sets guaranteed
to include the true label with high adjustable probability. Robust CP (RCP)
extends this to inputs with worst-case noise. A well-established approach is to
use randomized smoothing for RCP since it is applicable to any black-box model
and provides smaller sets compared to deterministic methods. However, current
smoothing-based RCP requires many model forward passes per each input which is
computationally expensive. We show that conformal prediction attains some
robustness even with a forward pass on a single randomly perturbed input. Using
any binary certificate we propose a single sample robust CP (RCP1). Our
approach returns robust sets with smaller average set size compared to SOTA
methods which use many (e.g. around 100) passes per input. Our key insight is
to certify the conformal prediction procedure itself rather than individual
scores. Our approach is agnostic to the setup (classification and regression).
We further extend our approach to smoothing-based robust conformal risk
control.

</details>


### [212] [Energy-Based Transfer for Reinforcement Learning](https://arxiv.org/abs/2506.16590)
*Zeyun Deng,Jasorsi Ghosh,Fiona Xie,Yuzhe Lu,Katia Sycara,Joseph Campbell*

Main category: cs.LG

TL;DR: 提出基于能量的迁移学习方法，用分布外检测选择性提供指导，提升单任务和多任务环境下的样本效率和性能。


<details>
  <summary>Details</summary>
Motivation: 强化学习算法样本效率低，在多任务或持续学习中应用困难，传统知识迁移在新任务与教师训练任务差异大时效果不佳。

Method: 提出基于能量的迁移学习方法，使用分布外检测来选择性地发出指导，仅在教师训练分布内的状态进行干预。

Result: 理论上证明能量分数反映教师的状态访问密度，实证表明在单任务和多任务环境下提高了样本效率和性能。

Conclusion: 所提方法能有效解决强化学习样本效率问题，在不同任务环境下均有良好表现。

Abstract: Reinforcement learning algorithms often suffer from poor sample efficiency,
making them challenging to apply in multi-task or continual learning settings.
Efficiency can be improved by transferring knowledge from a previously trained
teacher policy to guide exploration in new but related tasks. However, if the
new task sufficiently differs from the teacher's training task, the transferred
guidance may be sub-optimal and bias exploration toward low-reward behaviors.
We propose an energy-based transfer learning method that uses
out-of-distribution detection to selectively issue guidance, enabling the
teacher to intervene only in states within its training distribution. We
theoretically show that energy scores reflect the teacher's state-visitation
density and empirically demonstrate improved sample efficiency and performance
across both single-task and multi-task settings.

</details>


### [213] [FLAME: Towards Federated Fine-Tuning Large Language Models Through Adaptive SMoE](https://arxiv.org/abs/2506.16600)
*Khiem Le,Tuan Tran,Ting Hua,Nitesh V. Chawla*

Main category: cs.LG

TL;DR: 提出基于SMoE架构的联邦学习框架FLAME，解决现有资源自适应LoRA联邦微调方法压缩导致的性能问题，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有资源自适应LoRA联邦微调方法因压缩全局LoRA矩阵导致信息损失，性能不佳。

Method: 提出基于SMoE架构的FLAME框架，保留完整全局LoRA矩阵，通过改变每个客户端激活专家数量实现客户端适应性；用轻量级重缩放机制和激活感知聚合方案解决引入SMoE带来的挑战。

Result: 在不同计算环境的实验中，FLAME始终优于现有方法。

Conclusion: FLAME为资源自适应联邦学习提供了强大有效的解决方案。

Abstract: Existing resource-adaptive LoRA federated fine-tuning methods enable clients
to fine-tune models using compressed versions of global LoRA matrices, in order
to accommodate various compute resources across clients. This compression
requirement will lead to suboptimal performance due to information loss. To
address this, we propose FLAME, a novel federated learning framework based on
the Sparse Mixture-of-Experts (SMoE) architecture. Unlike prior approaches,
FLAME retains full (uncompressed) global LoRA matrices and achieves client-side
adaptability by varying the number of activated experts per client. However,
incorporating SMoE into federated learning introduces unique challenges,
specifically, the mismatch in output magnitude from partial expert activation
and the imbalance in expert training quality across clients. FLAME tackles
these challenges through a lightweight rescaling mechanism and an
activation-aware aggregation scheme. Empirical results across diverse
computational settings demonstrate that FLAME consistently outperforms existing
methods, providing a robust and effective solution for resource-adaptive
federated learning.

</details>


### [214] [SlepNet: Spectral Subgraph Representation Learning for Neural Dynamics](https://arxiv.org/abs/2506.16602)
*Siddharth Viswanath,Rahul Singh,Yanlei Zhang,J. Adam Noah,Joy Hirsch,Smita Krishnaswamy*

Main category: cs.LG

TL;DR: 提出SlepNet图卷积网络架构，在多个数据集上表现优于基线，可用于时空数据预测和表征学习。


<details>
  <summary>Details</summary>
Motivation: 现有图神经网络在表示图信号模式方面能力有限，需要新方法处理神经信号等时空模式。

Method: 提出使用Slepian基而非图傅里叶谐波的SlepNet架构，通过掩码自动学习相关子图。

Result: SlepNet在三个fMRI数据集和两个交通动态数据集上均优于传统GNN和图信号处理方法，提取的信号模式表示区分度更高。

Conclusion: SlepNet可用于时空数据的预测和表征学习。

Abstract: Graph neural networks have been useful in machine learning on
graph-structured data, particularly for node classification and some types of
graph classification tasks. However, they have had limited use in representing
patterning of signals over graphs. Patterning of signals over graphs and in
subgraphs carries important information in many domains including neuroscience.
Neural signals are spatiotemporally patterned, high dimensional and difficult
to decode. Graph signal processing and associated GCN models utilize the graph
Fourier transform and are unable to efficiently represent spatially or
spectrally localized signal patterning on graphs. Wavelet transforms have shown
promise here, but offer non-canonical representations and cannot be tightly
confined to subgraphs. Here we propose SlepNet, a novel GCN architecture that
uses Slepian bases rather than graph Fourier harmonics. In SlepNet, the Slepian
harmonics optimally concentrate signal energy on specifically relevant
subgraphs that are automatically learned with a mask. Thus, they can produce
canonical and highly resolved representations of neural activity, focusing
energy of harmonics on areas of the brain which are activated. We evaluated
SlepNet across three fMRI datasets, spanning cognitive and visual tasks, and
two traffic dynamics datasets, comparing its performance against conventional
GNNs and graph signal processing constructs. SlepNet outperforms the baselines
in all datasets. Moreover, the extracted representations of signal patterns
from SlepNet offers more resolution in distinguishing between similar patterns,
and thus represent brain signaling transients as informative trajectories. Here
we have shown that these extracted trajectory representations can be used for
other downstream untrained tasks. Thus we establish that SlepNet is useful both
for prediction and representation learning in spatiotemporal data.

</details>


### [215] [Distribution Parameter Actor-Critic: Shifting the Agent-Environment Boundary for Diverse Action Spaces](https://arxiv.org/abs/2506.16608)
*Jiamin He,A. Rupam Mahmood,Martha White*

Main category: cs.LG

TL;DR: 本文提出一种新的强化学习框架，将分布参数视为动作，开发DPPG估计器和ICL策略，提出DPAC算法，实验显示DPAC在连续和离散动作空间任务上表现良好。


<details>
  <summary>Details</summary>
Motivation: 重新定义智能体与环境边界，解决不同动作类型下的强化学习问题，降低梯度方差。

Method: 提出将分布参数作为动作的新框架，开发DPPG估计器和ICL策略，基于TD3提出DPAC算法。

Result: DPAC在OpenAI Gym和DeepMind Control Suite的MuJoCo连续控制任务中优于TD3，在离散动作空间环境中也有竞争力。

Conclusion: 所提出的分布参数强化学习框架及相关算法有效，能提升强化学习性能。

Abstract: We introduce a novel reinforcement learning (RL) framework that treats
distribution parameters as actions, redefining the boundary between agent and
environment. This reparameterization makes the new action space continuous,
regardless of the original action type (discrete, continuous, mixed, etc.).
Under this new parameterization, we develop a generalized deterministic policy
gradient estimator, Distribution Parameter Policy Gradient (DPPG), which has
lower variance than the gradient in the original action space. Although
learning the critic over distribution parameters poses new challenges, we
introduce interpolated critic learning (ICL), a simple yet effective strategy
to enhance learning, supported by insights from bandit settings. Building on
TD3, a strong baseline for continuous control, we propose a practical
DPPG-based actor-critic algorithm, Distribution Parameter Actor-Critic (DPAC).
Empirically, DPAC outperforms TD3 in MuJoCo continuous control tasks from
OpenAI Gym and DeepMind Control Suite, and demonstrates competitive performance
on the same environments with discretized action spaces.

</details>


### [216] [Mesh-Informed Neural Operator : A Transformer Generative Approach](https://arxiv.org/abs/2506.16656)
*Yaozhong Shi,Zachary E. Ross,Domniki Asimaki,Kamyar Azizzadenesheli*

Main category: cs.LG

TL;DR: 引入Mesh - Informed Neural Operator (MINO) 克服现有功能生成模型局限，拓展应用范围并提供统一视角，还引入评估指标。


<details>
  <summary>Details</summary>
Motivation: 当前功能生成模型实现依赖Fourier Neural Operator (FNO)，局限于规则网格和矩形域，需克服这些局限。

Method: 利用图神经算子和交叉注意力机制构建MINO，同时引入一套标准化评估指标。

Result: MINO显著拓展了功能生成模型在生成、逆问题和回归任务中的应用范围，提供了统一视角。

Conclusion: MINO克服了现有功能生成模型的局限性，标准化评估指标填补了该领域的空白。

Abstract: Generative models in function spaces, situated at the intersection of
generative modeling and operator learning, are attracting increasing attention
due to their immense potential in diverse scientific and engineering
applications. While functional generative models are theoretically domain- and
discretization-agnostic, current implementations heavily rely on the Fourier
Neural Operator (FNO), limiting their applicability to regular grids and
rectangular domains. To overcome these critical limitations, we introduce the
Mesh-Informed Neural Operator (MINO). By leveraging graph neural operators and
cross-attention mechanisms, MINO offers a principled, domain- and
discretization-agnostic backbone for generative modeling in function spaces.
This advancement significantly expands the scope of such models to more diverse
applications in generative, inverse, and regression tasks. Furthermore, MINO
provides a unified perspective on integrating neural operators with general
advanced deep learning architectures. Finally, we introduce a suite of
standardized evaluation metrics that enable objective comparison of functional
generative models, addressing another critical gap in the field.

</details>


### [217] [A Minimalist Optimizer Design for LLM Pretraining](https://arxiv.org/abs/2506.16659)
*Athanasios Glentis,Jiaxiang Li,Andi Han,Mingyi Hong*

Main category: cs.LG

TL;DR: 本文用自底向上方法研究LLM预训练中优化器状态最小必要量，提出SCALE优化器，内存使用少且性能优。


<details>
  <summary>Details</summary>
Motivation: 探究LLM预训练中保留最优性能所需的最小优化器状态量。

Method: 采用自底向上方法，发现列梯度归一化和仅对输出层添加一阶动量有效，提出结合二者的SCALE优化器。

Result: 在多个LLaMA模型上，SCALE性能匹配或超Adam，内存仅用35 - 45%，超GaLore等内存高效优化器，在LLaMA 7B上超APOLLO。

Conclusion: SCALE是内存受限下大规模预训练的有力候选，可作更复杂优化器设计的简约基线。

Abstract: Training large language models (LLMs) typically relies on adaptive optimizers
such as Adam, which require significant memory to maintain first- and
second-moment matrices, known as optimizer states. While recent works such as
GaLore, Fira, and APOLLO have proposed state-compressed variants to reduce
memory consumption, a fundamental question remains: What is the minimal amount
of optimizer state that is truly necessary to retain state-of-the-art
performance in LLM pretraining? In this work, we systematically investigate
this question using a bottom-up approach. We find that two memory- and
compute-efficient optimization techniques are particularly effective: (1)
column-wise gradient normalization significantly boosts the performance of
plain SGD without requiring momentum; and (2) adding first-order momentum only
to the output layer - where gradient variance is highest - yields performance
competitive with fully adaptive methods such as Muon. Based on these insights,
we propose SCALE (Stochastic Column-normalized Last-layer Momentum), a new
optimizer that combines column-normalized SGD with last-layer momentum, where
column normalization refers to normalizing the gradient along the output
dimension. Across multiple LLaMA models (60M-1B), SCALE matches or exceeds the
performance of Adam while using only 35-45% of the total memory. It also
consistently outperforms memory-efficient optimizers such as GaLore, Fira, and
APOLLO, making it a strong candidate for large-scale pretraining under memory
constraints. For the LLaMA 7B model, SCALE outperforms the state-of-the-art
method APOLLO in terms of both perplexity and memory consumption. In addition,
our method serves as a minimalist baseline for more sophisticated optimizer
design.

</details>


### [218] [Fast and Stable Diffusion Planning through Variational Adaptive Weighting](https://arxiv.org/abs/2506.16688)
*Zhiying Qiu,Tao Lin*

Main category: cs.LG

TL;DR: 提出在基于流的生成建模框架下估计不确定性感知加权函数的方法，用于扩散规划管道，在离线RL基准测试中以更少训练步骤取得有竞争力的性能。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在离线RL中存在训练成本高、收敛慢的问题，现有损失加权函数在早期训练阶段效果不佳。

Method: 推导变分最优的不确定性感知加权函数，引入封闭形式的多项式近似方法进行在线估计，并集成到扩散规划管道。

Result: 在Maze2D和Kitchen任务的实验中，该方法以少达10倍的训练步骤取得有竞争力的性能。

Conclusion: 所提方法具有实际有效性。

Abstract: Diffusion models have recently shown promise in offline RL. However, these
methods often suffer from high training costs and slow convergence,
particularly when using transformer-based denoising backbones. While several
optimization strategies have been proposed -- such as modified noise schedules,
auxiliary prediction targets, and adaptive loss weighting -- challenges remain
in achieving stable and efficient training. In particular, existing loss
weighting functions typically rely on neural network approximators, which can
be ineffective in early training phases due to limited generalization capacity
of MLPs when exposed to sparse feedback in the early training stages. In this
work, we derive a variationally optimal uncertainty-aware weighting function
and introduce a closed-form polynomial approximation method for its online
estimation under the flow-based generative modeling framework. We integrate our
method into a diffusion planning pipeline and evaluate it on standard offline
RL benchmarks. Experimental results on Maze2D and Kitchen tasks show that our
method achieves competitive performance with up to 10 times fewer training
steps, highlighting its practical effectiveness.

</details>


### [219] [SIDE: Semantic ID Embedding for effective learning from sequences](https://arxiv.org/abs/2506.16698)
*Dinesh Ramasamy,Shakti Kumar,Chris Cadonic,Jiaxin Yang,Sohini Roychowdhury,Esam Abdel Rhman,Srihari Reddy*

Main category: cs.LG

TL;DR: 提出利用向量量化注入紧凑语义ID的推荐模型方法，在工业广告推荐系统有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有基于序列的推荐模型将嵌入纳入实时预测模型时因存储和推理成本面临扩展挑战。

Method: 提出新方法，包括多任务VQ - VAE框架VQ fusion、无参数转换技术SIDE和离散主成分分析量化方法DPCA。

Result: 应用于大规模工业广告推荐系统，归一化熵增益提升2.4倍，数据占用减少3倍。

Conclusion: 所提方法有效解决扩展挑战，相比传统SID方法有显著优势。

Abstract: Sequence-based recommendations models are driving the state-of-the-art for
industrial ad-recommendation systems. Such systems typically deal with user
histories or sequence lengths ranging in the order of O(10^3) to O(10^4)
events. While adding embeddings at this scale is manageable in pre-trained
models, incorporating them into real-time prediction models is challenging due
to both storage and inference costs. To address this scaling challenge, we
propose a novel approach that leverages vector quantization (VQ) to inject a
compact Semantic ID (SID) as input to the recommendation models instead of a
collection of embeddings. Our method builds on recent works of SIDs by
introducing three key innovations: (i) a multi-task VQ-VAE framework, called VQ
fusion that fuses multiple content embeddings and categorical predictions into
a single Semantic ID; (ii) a parameter-free, highly granular SID-to-embedding
conversion technique, called SIDE, that is validated with two content embedding
collections, thereby eliminating the need for a large parameterized lookup
table; and (iii) a novel quantization method called Discrete-PCA (DPCA) which
generalizes and enhances residual quantization techniques. The proposed
enhancements when applied to a large-scale industrial ads-recommendation system
achieves 2.4X improvement in normalized entropy (NE) gain and 3X reduction in
data footprint compared to traditional SID methods.

</details>


### [220] [TriCon-SF: A Triple-Shuffle and Contribution-Aware Serial Federated Learning Framework for Heterogeneous Healthcare Data](https://arxiv.org/abs/2506.16723)
*Yuping Yan,Yizhi Wang,Yuanshuai Li,Yaochu Jin*

Main category: cs.LG

TL;DR: 提出TriCon - SF框架解决串行联邦学习隐私和安全问题，实验显示其在准确率和通信效率上表现更佳。


<details>
  <summary>Details</summary>
Motivation: 串行管道训练在跨机构联邦学习处理数据异质性有效，但模型直接传输有隐私风险，应对半诚实或恶意客户端是挑战。

Method: 提出TriCon - SF框架，通过对模型层、数据段和训练序列三重随机化增强隐私和鲁棒性，用Shapley值方法评估客户端贡献。

Result: 在非IID医疗数据集上实验表明，TriCon - SF在准确率和通信效率上优于标准串行和并行联邦学习，安全分析显示其抗客户端隐私攻击能力强。

Conclusion: TriCon - SF框架能有效解决串行联邦学习中的隐私和安全问题。

Abstract: Serial pipeline training is an efficient paradigm for handling data
heterogeneity in cross-silo federated learning with low communication overhead.
However, even without centralized aggregation, direct transfer of models
between clients can violate privacy regulations and remain susceptible to
gradient leakage and linkage attacks. Additionally, ensuring resilience against
semi-honest or malicious clients who may manipulate or misuse received models
remains a grand challenge, particularly in privacy-sensitive domains such as
healthcare. To address these challenges, we propose TriCon-SF, a novel serial
federated learning framework that integrates triple shuffling and contribution
awareness. TriCon-SF introduces three levels of randomization by shuffling
model layers, data segments, and training sequences to break deterministic
learning patterns and disrupt potential attack vectors, thereby enhancing
privacy and robustness. In parallel, it leverages Shapley value methods to
dynamically evaluate client contributions during training, enabling the
detection of dishonest behavior and enhancing system accountability. Extensive
experiments on non-IID healthcare datasets demonstrate that TriCon-SF
outperforms standard serial and parallel federated learning in both accuracy
and communication efficiency. Security analysis further supports its resilience
against client-side privacy attacks.

</details>


### [221] [On Training-Test (Mis)alignment in Unsupervised Combinatorial Optimization: Observation, Empirical Exploration, and Analysis](https://arxiv.org/abs/2506.16732)
*Fanchen Bu,Kijung Shin*

Main category: cs.LG

TL;DR: 现有无监督组合优化（UCO）方法训练和测试存在不一致问题，提出将可微去随机化纳入训练的初步想法，能改善一致性但带来训练挑战。


<details>
  <summary>Details</summary>
Motivation: 现有UCO方法存在训练和测试不一致的问题，低训练损失不一定带来好的去随机化后性能。

Method: 将可微版本的去随机化纳入训练。

Result: 该想法确实改善了训练 - 测试的一致性，但也给训练带来了不小的挑战。

Conclusion: 提出的将可微去随机化纳入训练的想法有改善效果，但面临训练难题需进一步解决。

Abstract: In unsupervised combinatorial optimization (UCO), during training, one aims
to have continuous decisions that are promising in a probabilistic sense for
each training instance, which enables end-to-end training on initially discrete
and non-differentiable problems. At the test time, for each test instance,
starting from continuous decisions, derandomization is typically applied to
obtain the final deterministic decisions. Researchers have developed more and
more powerful test-time derandomization schemes to enhance the empirical
performance and the theoretical guarantee of UCO methods. However, we notice a
misalignment between training and testing in the existing UCO methods.
Consequently, lower training losses do not necessarily entail better
post-derandomization performance, even for the training instances without any
data distribution shift. Empirically, we indeed observe such undesirable cases.
We explore a preliminary idea to better align training and testing in UCO by
including a differentiable version of derandomization into training. Our
empirical exploration shows that such an idea indeed improves training-test
alignment, but also introduces nontrivial challenges into training.

</details>


### [222] [IsoNet: Causal Analysis of Multimodal Transformers for Neuromuscular Gesture Classification](https://arxiv.org/abs/2506.16744)
*Eion Tyacke,Kunal Gupta,Jay Patel,Rui Li*

Main category: cs.LG

TL;DR: 本文系统比较线性和基于注意力的多模态融合策略，Hierarchical Transformer表现最佳，引入Isolation Network探究模态交互，揭示多模态融合提升生物信号分类的时机和方式。


<details>
  <summary>Details</summary>
Motivation: 手部手势神经肌肉特征解码是瓶颈，传统人机接口管道依赖单生物信号模态，多模态融合可利用互补信息。

Method: 系统比较三种架构下线性和基于注意力的融合策略，用两个公开数据集实验，引入Isolation Network量化模态交互贡献。

Result: Hierarchical Transformer在两个数据集上准确率最高，跨模态交互贡献约30%决策信号。

Conclusion: 揭示多模态融合增强生物信号分类的情况和方式，对神经机器人系统传感器阵列设计有益。

Abstract: Hand gestures are a primary output of the human motor system, yet the
decoding of their neuromuscular signatures remains a bottleneck for basic
neuroscience and assistive technologies such as prosthetics. Traditional
human-machine interface pipelines rely on a single biosignal modality, but
multimodal fusion can exploit complementary information from sensors. We
systematically compare linear and attention-based fusion strategies across
three architectures: a Multimodal MLP, a Multimodal Transformer, and a
Hierarchical Transformer, evaluating performance on scenarios with unimodal and
multimodal inputs. Experiments use two publicly available datasets: NinaPro DB2
(sEMG and accelerometer) and HD-sEMG 65-Gesture (high-density sEMG and force).
Across both datasets, the Hierarchical Transformer with attention-based fusion
consistently achieved the highest accuracy, surpassing the multimodal and best
single-modality linear-fusion MLP baseline by over 10% on NinaPro DB2 and 3.7%
on HD-sEMG. To investigate how modalities interact, we introduce an Isolation
Network that selectively silences unimodal or cross-modal attention pathways,
quantifying each group of token interactions' contribution to downstream
decisions. Ablations reveal that cross-modal interactions contribute
approximately 30% of the decision signal across transformer layers,
highlighting the importance of attention-driven fusion in harnessing
complementary modality information. Together, these findings reveal when and
how multimodal fusion would enhance biosignal classification and also provides
mechanistic insights of human muscle activities. The study would be beneficial
in the design of sensor arrays for neurorobotic systems.

</details>


### [223] [Off-Policy Actor-Critic for Adversarial Observation Robustness: Virtual Alternative Training via Symmetric Policy Evaluation](https://arxiv.org/abs/2506.16753)
*Kosuke Nakanishi,Akihiro Kubo,Yuji Yasui,Shin Ishii*

Main category: cs.LG

TL;DR: 提出一种新的离线策略方法，将对抗学习重新表述为软约束优化问题，无需额外环境交互，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有鲁棒强化学习方法处理长时最坏情况存在问题，交互低效且阻碍离线策略方法发展。

Method: 将对抗学习重新表述为软约束优化问题，利用智能体和对手策略评估的对称性质。

Result: 提出了新的离线策略方法。

Conclusion: 该方法能消除额外环境交互需求，具有理论支持。

Abstract: Recently, robust reinforcement learning (RL) methods designed to handle
adversarial input observations have received significant attention, motivated
by RL's inherent vulnerabilities. While existing approaches have demonstrated
reasonable success, addressing worst-case scenarios over long time horizons
requires both minimizing the agent's cumulative rewards for adversaries and
training agents to counteract them through alternating learning. However, this
process introduces mutual dependencies between the agent and the adversary,
making interactions with the environment inefficient and hindering the
development of off-policy methods. In this work, we propose a novel off-policy
method that eliminates the need for additional environmental interactions by
reformulating adversarial learning as a soft-constrained optimization problem.
Our approach is theoretically supported by the symmetric property of policy
evaluation between the agent and the adversary. The implementation is available
at https://github.com/nakanakakosuke/VALT_SAC.

</details>


### [224] [Metapath-based Hyperbolic Contrastive Learning for Heterogeneous Graph Embedding](https://arxiv.org/abs/2506.16754)
*Jongmin Park,Seunghoon Han,Won-Yong Shin,Sungsu Lim*

Main category: cs.LG

TL;DR: 提出基于元路径的双曲对比学习框架MHCL，用多双曲空间捕捉异构图复杂结构，对比学习优化，实验证明优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 多数双曲异构图嵌入模型用单双曲空间，难以有效捕捉异构图多样幂律结构。

Method: 提出MHCL框架，用多双曲空间对应各元路径结构分布，用对比学习优化，最小化同元路径嵌入距离、最大化不同元路径嵌入距离。

Result: 实验表明MHCL在各类图机器学习任务中优于现有基线。

Conclusion: MHCL能有效捕捉异构图复杂结构。

Abstract: The hyperbolic space, characterized by a constant negative curvature and
exponentially expanding space, aligns well with the structural properties of
heterogeneous graphs. However, although heterogeneous graphs inherently possess
diverse power-law structures, most hyperbolic heterogeneous graph embedding
models rely on a single hyperbolic space. This approach may fail to effectively
capture the diverse power-law structures within heterogeneous graphs. To
address this limitation, we propose a Metapath-based Hyperbolic Contrastive
Learning framework (MHCL), which uses multiple hyperbolic spaces to capture
diverse complex structures within heterogeneous graphs. Specifically, by
learning each hyperbolic space to describe the distribution of complex
structures corresponding to each metapath, it is possible to capture semantic
information effectively. Since metapath embeddings represent distinct semantic
information, preserving their discriminability is important when aggregating
them to obtain node representations. Therefore, we use a contrastive learning
approach to optimize MHCL and improve the discriminability of metapath
embeddings. In particular, our contrastive learning method minimizes the
distance between embeddings of the same metapath and maximizes the distance
between those of different metapaths in hyperbolic space, thereby improving the
separability of metapath embeddings with distinct semantic information. We
conduct comprehensive experiments to evaluate the effectiveness of MHCL. The
experimental results demonstrate that MHCL outperforms state-of-the-art
baselines in various graph machine learning tasks, effectively capturing the
complex structures of heterogeneous graphs.

</details>


### [225] [What Is the Point of Equality in Machine Learning Fairness? Beyond Equality of Opportunity](https://arxiv.org/abs/2506.16782)
*Youjin Kong*

Main category: cs.LG

TL;DR: 本文指出机器学习公平性研究中仅关注分配平等不完整，提出结合分配和平等关系的多面平等主义框架并给出实施途径。


<details>
  <summary>Details</summary>
Motivation: 探讨机器学习不公平性在道德上错误的原因，指出仅关注分配平等作为道德基础不完整。

Method: 借鉴批判性社会和政治哲学，分析结构不平等在机器学习系统中的表现，提出多面平等主义框架。

Result: 提出了一个能解决机器学习系统造成的各种危害的更全面的伦理框架。

Conclusion: 多面平等主义框架为解决机器学习公平性问题提供了更完善的伦理基础和实践途径。

Abstract: Fairness in machine learning (ML) has become a rapidly growing area of
research. But why, in the first place, is unfairness in ML morally wrong? And
why should we care about improving fairness? Most fair-ML research implicitly
appeals to distributive equality: the idea that desirable goods and benefits,
such as opportunities (e.g., Barocas et al., 2023), should be equally
distributed across society. Unfair ML models, then, are seen as wrong because
they unequally distribute such benefits. This paper argues that this exclusive
focus on distributive equality offers an incomplete and potentially misleading
ethical foundation. Grounding ML fairness in egalitarianism -- the view that
equality is a fundamental moral and social ideal -- requires challenging
structural inequality: systematic, institutional, and durable arrangements that
privilege some groups while disadvantaging others. Structural inequality
manifests through ML systems in two primary forms: allocative harms (e.g.,
economic loss) and representational harms (e.g., stereotypes, erasure). While
distributive equality helps address allocative harms, it fails to explain why
representational harms are wrong -- why it is wrong for ML systems to reinforce
social hierarchies that stratify people into superior and inferior groups --
and why ML systems should aim to foster a society where people relate as equals
(i.e., relational equality). To address these limitations, the paper proposes a
multifaceted egalitarian framework for ML fairness that integrates both
distributive and relational equality. Drawing on critical social and political
philosophy, this framework offers a more comprehensive ethical foundation for
tackling the full spectrum of harms perpetuated by ML systems. The paper also
outlines practical pathways for implementing the framework across the ML
pipeline.

</details>


### [226] [Revisiting LoRA through the Lens of Parameter Redundancy: Spectral Encoding Helps](https://arxiv.org/abs/2506.16787)
*Jiashun Cheng,Aochuan Chen,Nuo Chen,Ziqi Gao,Yuhan Li,Jia Li,Fugee Tsung*

Main category: cs.LG

TL;DR: 本文研究LoRA冗余问题，提出SeLoRA方法，实验表明其参数少效率高，在多下游任务表现优。


<details>
  <summary>Details</summary>
Motivation: LoRA存在大量参数冗余，限制了其容量和效率，成为瓶颈。

Method: 系统研究LoRA微调中冗余的影响，发现降低密度冗余不影响表达能力，引入SeLoRA，从稀疏谱子空间对LoRA重新参数化。

Result: SeLoRA能与多种LoRA变体无缝集成，在常识推理、数学推理和代码生成等下游任务上比强基线有更好的性能提升。

Conclusion: SeLoRA用更少参数实现了更高效率，是一个可扩展的即插即用框架。

Abstract: Low-Rank Adaptation (LoRA) has emerged as a prominent technique for
fine-tuning large foundation models. Despite its successes, the substantial
parameter redundancy, which limits the capacity and efficiency of LoRA, has
been recognized as a bottleneck. In this work, we systematically investigate
the impact of redundancy in fine-tuning LoRA and reveal that reducing density
redundancy does not degrade expressiveness. Based on this insight, we introduce
\underline{S}pectral-\underline{e}ncoding \underline{L}ow-\underline{R}ank
\underline{A}daptation (SeLoRA), which harnesses the robust expressiveness of
spectral bases to re-parameterize LoRA from a sparse spectral subspace.
Designed with simplicity, SeLoRA enables seamless integration with various LoRA
variants for performance boosting, serving as a scalable plug-and-play
framework. Extensive experiments substantiate that SeLoRA achieves greater
efficiency with fewer parameters, delivering superior performance enhancements
over strong baselines on various downstream tasks, including commonsense
reasoning, math reasoning, and code generation.

</details>


### [227] [Exploring and Improving Initialization for Deep Graph Neural Networks: A Signal Propagation Perspective](https://arxiv.org/abs/2506.16790)
*Senmiao Wang,Yupeng Chen,Yushun Zhang,Ruoyu Sun,Tian Ding*

Main category: cs.LG

TL;DR: 本文针对GNN深度增加性能下降问题，提出SPoGInit方法优化信号传播，实验表明该方法优于常用初始化方法。


<details>
  <summary>Details</summary>
Motivation: 解决GNN网络深度增加导致的性能下降问题。

Method: 提出前向传播、反向传播和图嵌入变化三个信号传播关键指标，分析常用初始化方法不能同时控制这些指标的问题，提出SPoGInit方法，通过搜索优化指标的权重初始化方差提升信号传播。

Result: SPoGInit在各种任务和架构上优于常用初始化方法，能随着GNN加深提升性能。

Conclusion: SPoGInit有效解决了GNN深度相关挑战，证明了信号传播分析框架的有效性。

Abstract: Graph Neural Networks (GNNs) often suffer from performance degradation as the
network depth increases. This paper addresses this issue by introducing
initialization methods that enhance signal propagation (SP) within GNNs. We
propose three key metrics for effective SP in GNNs: forward propagation,
backward propagation, and graph embedding variation (GEV). While the first two
metrics derive from classical SP theory, the third is specifically designed for
GNNs. We theoretically demonstrate that a broad range of commonly used
initialization methods for GNNs, which exhibit performance degradation with
increasing depth, fail to control these three metrics simultaneously. To deal
with this limitation, a direct exploitation of the SP analysis--searching for
weight initialization variances that optimize the three metrics--is shown to
significantly enhance the SP in deep GCNs. This approach is called Signal
Propagation on Graph-guided Initialization (SPoGInit). Our experiments
demonstrate that SPoGInit outperforms commonly used initialization methods on
various tasks and architectures. Notably, SPoGInit enables performance
improvements as GNNs deepen, which represents a significant advancement in
addressing depth-related challenges and highlights the validity and
effectiveness of the SP analysis framework.

</details>


### [228] [TabArena: A Living Benchmark for Machine Learning on Tabular Data](https://arxiv.org/abs/2506.16791)
*Nick Erickson,Lennart Purucker,Andrej Tschalzev,David Holzmüller,Prateek Mutalik Desai,and David Salinas,Frank Hutter*

Main category: cs.LG

TL;DR: 介绍TabArena，首个持续维护的表格数据基准测试系统，展示其研究结果并发布相关资源。


<details>
  <summary>Details</summary>
Motivation: 当前表格数据基准测试是静态的，设计不更新，需标准化可靠的基准测试。

Method: 手动策划数据集和模型，进行大规模基准测试研究以初始化排行榜，组建维护团队。

Result: 凸显验证方法和超参数配置集成对充分发挥基准模型潜力的影响，不同模型在不同数据集表现不同，跨模型集成推动表格机器学习发展。

Conclusion: 推出TabArena，含公共排行榜、可复现代码和维护协议，网址为https://tabarena.ai。

Abstract: With the growing popularity of deep learning and foundation models for
tabular data, the need for standardized and reliable benchmarks is higher than
ever. However, current benchmarks are static. Their design is not updated even
if flaws are discovered, model versions are updated, or new models are
released. To address this, we introduce TabArena, the first continuously
maintained living tabular benchmarking system. To launch TabArena, we manually
curate a representative collection of datasets and well-implemented models,
conduct a large-scale benchmarking study to initialize a public leaderboard,
and assemble a team of experienced maintainers. Our results highlight the
influence of validation method and ensembling of hyperparameter configurations
to benchmark models at their full potential. While gradient-boosted trees are
still strong contenders on practical tabular datasets, we observe that deep
learning methods have caught up under larger time budgets with ensembling. At
the same time, foundation models excel on smaller datasets. Finally, we show
that ensembles across models advance the state-of-the-art in tabular machine
learning and investigate the contributions of individual models. We launch
TabArena with a public leaderboard, reproducible code, and maintenance
protocols to create a living benchmark available at https://tabarena.ai.

</details>


### [229] [Robust Group Anomaly Detection for Quasi-Periodic Network Time Series](https://arxiv.org/abs/2506.16815)
*Kai Yang,Shaoyu Dou,Pan Luo,Xin Wang,H. Vincent Poor*

Main category: cs.LG

TL;DR: 提出seq2GMM框架用于识别网络时间序列数据库中的异常时间序列，开发训练算法，在多个数据集上表现优异并进行理论分析。


<details>
  <summary>Details</summary>
Motivation: 在众多准周期时间序列中构建机器学习模型识别异常序列，并帮助人类专家理解决策过程。

Method: 提出seq2GMM框架，开发基于代理的优化算法训练模型。

Result: 在多个公共基准数据集上表现出色，显著优于现有异常检测技术。

Conclusion: seq2GMM框架有效，训练算法具有收敛性，数值结果支持理论分析。

Abstract: Many real-world multivariate time series are collected from a network of
physical objects embedded with software, electronics, and sensors. The
quasi-periodic signals generated by these objects often follow a similar
repetitive and periodic pattern, but have variations in the period, and come in
different lengths caused by timing (synchronization) errors. Given a multitude
of such quasi-periodic time series, can we build machine learning models to
identify those time series that behave differently from the majority of the
observations? In addition, can the models help human experts to understand how
the decision was made? We propose a sequence to Gaussian Mixture Model
(seq2GMM) framework. The overarching goal of this framework is to identify
unusual and interesting time series within a network time series database. We
further develop a surrogate-based optimization algorithm that can efficiently
train the seq2GMM model. Seq2GMM exhibits strong empirical performance on a
plurality of public benchmark datasets, outperforming state-of-the-art anomaly
detection techniques by a significant margin. We also theoretically analyze the
convergence property of the proposed training algorithm and provide numerical
results to substantiate our theoretical claims.

</details>


### [230] [Predicting New Research Directions in Materials Science using Large Language Models and Concept Graphs](https://arxiv.org/abs/2506.16824)
*Thomas Marwitz,Alexander Colsmann,Ben Breitung,Christoph Brabec,Christoph Kirchlechner,Eva Blasco,Gabriel Cadilha Marques,Horst Hahn,Michael Hirtz,Pavel A. Levkin,Yolita M. Eggeler,Tobias Schlöder,Pascal Friederich*

Main category: cs.LG

TL;DR: 本文利用大语言模型从材料科学摘要中提取概念和语义信息，构建概念图，训练机器学习模型预测新研究思路，经实验证明有效且能启发科学家。


<details>
  <summary>Details</summary>
Motivation: 已发表研究文章数量呈指数级增长，科学家无法阅读所有文献，需借助工具提取信息、发现新研究方向。

Method: 使用大语言模型从科学摘要中提取概念和语义信息构建概念图，基于历史数据训练机器学习模型预测概念新组合。

Result: 大语言模型比自动关键词提取方法更高效地提取概念，整合语义概念信息可提高预测性能，模型建议经领域专家定性访谈验证可行。

Conclusion: 模型能通过预测未被研究的创新主题组合，启发材料科学家的创造性思维。

Abstract: Due to an exponential increase in published research articles, it is
impossible for individual scientists to read all publications, even within
their own research field. In this work, we investigate the use of large
language models (LLMs) for the purpose of extracting the main concepts and
semantic information from scientific abstracts in the domain of materials
science to find links that were not noticed by humans and thus to suggest
inspiring near/mid-term future research directions. We show that LLMs can
extract concepts more efficiently than automated keyword extraction methods to
build a concept graph as an abstraction of the scientific literature. A machine
learning model is trained to predict emerging combinations of concepts, i.e.
new research ideas, based on historical data. We demonstrate that integrating
semantic concept information leads to an increased prediction performance. The
applicability of our model is demonstrated in qualitative interviews with
domain experts based on individualized model suggestions. We show that the
model can inspire materials scientists in their creative thinking process by
predicting innovative combinations of topics that have not yet been
investigated.

</details>


### [231] [FedFitTech: A Baseline in Federated Learning for Fitness Tracking](https://arxiv.org/abs/2506.16840)
*Zeyneddin Oz,Shreyas Korde,Marius Bock,Kristof Van Laerhoven*

Main category: cs.LG

TL;DR: 本文提出FedFitTech基线以简化联邦学习在健身技术中的研究，并通过案例展示其使用，结果显示可减少通信冗余，为该领域带来新机遇。


<details>
  <summary>Details</summary>
Motivation: 传统集中式学习检测健身活动存在隐私、监管和通信效率问题，联邦学习应用于健身技术有独特挑战，需简化相关研究。

Method: 在Flower框架下提出FedFitTech基线，开展案例研究，采用客户端提前停止策略。

Result: 减少13%的冗余通信，识别性能成本仅1%。

Conclusion: FedFitTech基线为健身技术研究和开发提供基础，代码开源。

Abstract: Rapid evolution of sensors and resource-efficient machine learning models
have spurred the widespread adoption of wearable fitness tracking devices.
Equipped with inertial sensors, such devices can continuously capture physical
movements for fitness technology (FitTech), enabling applications from sports
optimization to preventive healthcare. Traditional centralized learning
approaches to detect fitness activities struggle with privacy concerns,
regulatory constraints, and communication inefficiencies. In contrast,
Federated Learning (FL) enables a decentralized model training by communicating
model updates rather than private wearable sensor data. Applying FL to FitTech
presents unique challenges, such as data imbalance, lack of labelled data,
heterogeneous user activity patterns, and trade-offs between personalization
and generalization. To simplify research on FitTech in FL, we present the
FedFitTech baseline, under the Flower framework, which is publicly available
and widely used by both industry and academic researchers. Additionally, to
illustrate its usage, this paper presents a case study that implements a system
based on the FedFitTech baseline, incorporating a client-side early stopping
strategy and comparing the results. For instance, this system allows wearable
devices to optimize the trade-off between capturing common fitness activity
patterns and preserving individuals' nuances, thereby enhancing both the
scalability and efficiency of privacy-aware fitness tracking applications.
Results show that this reduces overall redundant communications by 13 percent,
while maintaining the overall recognition performance at a negligible
recognition cost by 1 percent. Thus, FedFitTech baseline creates a foundation
for a wide range of new research and development opportunities in FitTech, and
it is available as open-source at:
https://github.com/adap/flower/tree/main/baselines/fedfittech

</details>


### [232] [Soft decision trees for survival analysis](https://arxiv.org/abs/2506.16846)
*Antonio Consoloa,Edoardo Amaldi,Emilio Carrizosa*

Main category: cs.LG

TL;DR: 本文提出软生存树模型（SST），结合灵活性与可解释性，实验表明其性能优于基准模型，还可考虑群体公平性。


<details>
  <summary>Details</summary>
Motivation: 现有生存树多通过启发式方法构建，而全局优化树受关注，期望提出新的生存树模型。

Method: 提出SST模型，在每个分支节点采用软分裂规则，通过可分解的非线性优化公式训练，使用参数和基于样条的半参数生存函数。

Result: 在15个知名数据集上的实验显示，SST在四个常用的区分和校准指标上优于三个基准生存树。

Conclusion: SST结合灵活性与可解释性，性能良好，还可扩展以考虑群体公平性。

Abstract: Decision trees are popular in survival analysis for their interpretability
and ability to model complex relationships. Survival trees, which predict the
timing of singular events using censored historical data, are typically built
through heuristic approaches. Recently, there has been growing interest in
globally optimized trees, where the overall tree is trained by minimizing the
error function over all its parameters. We propose a new soft survival tree
model (SST), with a soft splitting rule at each branch node, trained via a
nonlinear optimization formulation amenable to decomposition. Since SSTs
provide for every input vector a specific survival function associated to a
single leaf node, they satisfy the conditional computation property and inherit
the related benefits. SST and the training formulation combine flexibility with
interpretability: any smooth survival function (parametric, semiparametric, or
nonparametric) estimated through maximum likelihood can be used, and each leaf
node of an SST yields a cluster of distinct survival functions which are
associated to the data points routed to it. Numerical experiments on 15
well-known datasets show that SSTs, with parametric and spline-based
semiparametric survival functions, trained using an adaptation of the
node-based decomposition algorithm proposed by Consolo et al. (2024) for soft
regression trees, outperform three benchmark survival trees in terms of four
widely-used discrimination and calibration measures. SSTs can also be extended
to consider group fairness.

</details>


### [233] [Reward-Agnostic Prompt Optimization for Text-to-Image Diffusion Models](https://arxiv.org/abs/2506.16853)
*Semin Kim,Yeonwoo Cha,Jaehoon Yoo,Seunghoon Hong*

Main category: cs.LG

TL;DR: 本文提出RATTPO方法优化文本到图像扩散模型的用户提示，在不同奖励场景表现良好，效率高。


<details>
  <summary>Details</summary>
Motivation: 现有自动提示工程方法针对特定奖励配置，在新场景性能不佳，需通用方法。

Method: 引入RATTPO，通过查询大语言模型迭代搜索优化提示，用优化轨迹和奖励感知反馈信号作上下文。

Result: RATTPO通用性强，在不同奖励设置下提升提示效果，搜索效率超其他基线，预算充足时性能与基于学习的基线相当。

Conclusion: RATTPO是适用于不同奖励场景的灵活测试时优化方法，代码已开源。

Abstract: We investigate a general approach for improving user prompts in text-to-image
(T2I) diffusion models by finding prompts that maximize a reward function
specified at test-time. Although diverse reward models are used for evaluating
image generation, existing automated prompt engineering methods typically
target specific reward configurations. Consequently, these specialized designs
exhibit suboptimal performance when applied to new prompt engineering scenarios
involving different reward models. To address this limitation, we introduce
RATTPO (Reward-Agnostic Test-Time Prompt Optimization), a flexible test-time
optimization method applicable across various reward scenarios without
modification. RATTPO iteratively searches for optimized prompts by querying
large language models (LLMs) \textit{without} requiring reward-specific task
descriptions. Instead, it uses the optimization trajectory and a novel
reward-aware feedback signal (termed a "hint") as context. Empirical results
demonstrate the versatility of RATTPO, effectively enhancing user prompts
across diverse reward setups that assess various generation aspects, such as
aesthetics, general human preference, or spatial relationships between objects.
RATTPO surpasses other test-time search baselines in search efficiency, using
up to 3.5 times less inference budget, and, given sufficient inference budget,
achieves performance comparable to learning-based baselines that require
reward-specific fine-tuning. The code is available at
https://github.com/seminkim/RATTPO.

</details>


### [234] [Anomaly Detection in Event-triggered Traffic Time Series via Similarity Learning](https://arxiv.org/abs/2506.16855)
*Shaoyu Dou,Kai Yang,Yang Jiao,Chengbo Qiu,Kui Ren*

Main category: cs.LG

TL;DR: 本文提出无监督学习框架学习事件触发时间序列相似度，实验显示该方法优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在安全相关任务中，不清楚哪种相似度度量适用于事件触发时间序列，需开发学习其相似度的框架。

Method: 利用分层多分辨率顺序自动编码器和高斯混合模型（GMM）从时间序列中学习低维表示。

Result: 通过大量定性和定量实验，表明该方法明显优于现有方法。

Conclusion: 该框架为建模和学习大量事件触发时间序列的相似度提供了系统方法。

Abstract: Time series analysis has achieved great success in cyber security such as
intrusion detection and device identification. Learning similarities among
multiple time series is a crucial problem since it serves as the foundation for
downstream analysis. Due to the complex temporal dynamics of the
event-triggered time series, it often remains unclear which similarity metric
is appropriate for security-related tasks, such as anomaly detection and
clustering. The overarching goal of this paper is to develop an unsupervised
learning framework that is capable of learning similarities among a set of
event-triggered time series. From the machine learning vantage point, the
proposed framework harnesses the power of both hierarchical multi-resolution
sequential autoencoders and the Gaussian Mixture Model (GMM) to effectively
learn the low-dimensional representations from the time series. Finally, the
obtained similarity measure can be easily visualized for the explanation. The
proposed framework aspires to offer a stepping stone that gives rise to a
systematic approach to model and learn similarities among a multitude of
event-triggered time series. Through extensive qualitative and quantitative
experiments, it is revealed that the proposed method outperforms
state-of-the-art methods considerably.

</details>


### [235] [Optimal Depth of Neural Networks](https://arxiv.org/abs/2506.16862)
*Qian Qi*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Determining the optimal depth of a neural network is a fundamental yet
challenging problem, typically resolved through resource-intensive
experimentation. This paper introduces a formal theoretical framework to
address this question by recasting the forward pass of a deep network,
specifically a Residual Network (ResNet), as an optimal stopping problem. We
model the layer-by-layer evolution of hidden representations as a sequential
decision process where, at each layer, a choice is made between halting
computation to make a prediction or continuing to a deeper layer for a
potentially more refined representation. This formulation captures the
intrinsic trade-off between accuracy and computational cost. Our primary
theoretical contribution is a proof that, under a plausible condition of
diminishing returns on the residual functions, the expected optimal stopping
depth is provably finite, even in an infinite-horizon setting. We leverage this
insight to propose a novel and practical regularization term, $\mathcal{L}_{\rm
depth}$, that encourages the network to learn representations amenable to
efficient, early exiting. We demonstrate the generality of our framework by
extending it to the Transformer architecture and exploring its connection to
continuous-depth models via free-boundary problems. Empirical validation on
ImageNet confirms that our regularizer successfully induces the theoretically
predicted behavior, leading to significant gains in computational efficiency
without compromising, and in some cases improving, final model accuracy.

</details>


### [236] [From Lab to Factory: Pitfalls and Guidelines for Self-/Unsupervised Defect Detection on Low-Quality Industrial Images](https://arxiv.org/abs/2506.16890)
*Sebastian Hönel,Jonas Nordqvist*

Main category: cs.LG

TL;DR: 传统产品质量检测依赖人工，机器学习有潜力替代。多数现有方法在实际场景表现不佳，本文用低质量RGB图像检测金属零件表面细微异常，评估两类模型，为从业者提供问题识别指引，指出似然法不足并给出更适合现实场景的风险估计框架。


<details>
  <summary>Details</summary>
Motivation: 传统人工检测产品质量问题成本高且易出错，机器学习有望替代，但现有方法在实际场景存在处理低质量数据能力差、鲁棒性低等问题，难以识别潜在问题，常用指标不适用。

Method: 使用低质量RGB图像，评估两类最先进的模型，以识别和改善生产数据中的质量问题，无需获取新数据。

Result: 为从业者提供在类似场景下可靠识别所选模型或数据中与鲁棒性、不变性相关问题的指引。

Conclusion: 指出似然法存在常见陷阱和不足，给出更适合现实场景的经验风险估计框架。

Abstract: The detection and localization of quality-related problems in industrially
mass-produced products has historically relied on manual inspection, which is
costly and error-prone. Machine learning has the potential to replace manual
handling. As such, the desire is to facilitate an unsupervised (or
self-supervised) approach, as it is often impossible to specify all conceivable
defects ahead of time. A plethora of prior works have demonstrated the aptitude
of common reconstruction-, embedding-, and synthesis-based methods in
laboratory settings. However, in practice, we observe that most methods do not
handle low data quality well or exude low robustness in unfavorable, but
typical real-world settings. For practitioners it may be very difficult to
identify the actual underlying problem when such methods underperform. Worse,
often-reported metrics (e.g., AUROC) are rarely suitable in practice and may
give misleading results. In our setting, we attempt to identify subtle
anomalies on the surface of blasted forged metal parts, using rather
low-quality RGB imagery only, which is a common industrial setting. We
specifically evaluate two types of state-of-the-art models that allow us to
identify and improve quality issues in production data, without having to
obtain new data. Our contribution is to provide guardrails for practitioners
that allow them to identify problems related to, e.g., (lack of) robustness or
invariance, in either the chosen model or the data reliably in similar
scenarios. Furthermore, we exemplify common pitfalls in and shortcomings of
likelihood-based approaches and outline a framework for proper empirical risk
estimation that is more suitable for real-world scenarios.

</details>


### [237] [A deep learning and machine learning approach to predict neonatal death in the context of São Paulo](https://arxiv.org/abs/2506.16929)
*Mohon Raihan,Plabon Kumar Saha,Rajan Das Gupta,A Z M Tahmidul Kabir,Afia Anjum Tamanna,Md. Harun-Ur-Rashid,Adnan Bin Abdus Salam,Md Tanvir Anjum,A Z M Ahteshamul Kabir*

Main category: cs.LG

TL;DR: 利用机器学习预测新生儿死亡风险，LSTM 准确率最高，是最适合的方法。


<details>
  <summary>Details</summary>
Motivation: 降低全球新生儿死亡率，需要对濒危婴儿进行早期预测。

Method: 使用 140 万新生儿的历史数据，运用逻辑回归、K 近邻、随机森林分类器、XGBoost、卷积神经网络和 LSTM 等机器学习和深度学习技术训练预测模型。

Result: 机器学习算法中 XGBoost 和随机森林分类器准确率达 94%，深度学习模型中 LSTM 准确率达 99%。

Conclusion: LSTM 是预测是否需要对儿童采取预防措施的最合适方法。

Abstract: Neonatal death is still a concerning reality for underdeveloped and even some
developed countries. Worldwide data indicate that 26.693 babies out of 1,000
births die, according to Macro Trades. To reduce this number, early prediction
of endangered babies is crucial. Such prediction enables the opportunity to
take ample care of the child and mother so that early child death can be
avoided. In this context, machine learning was used to determine whether a
newborn baby is at risk. To train the predictive model, historical data of 1.4
million newborns was used. Machine learning and deep learning techniques such
as logical regression, K-nearest neighbor, random forest classifier, extreme
gradient boosting (XGBoost), convolutional neural network, and long short-term
memory (LSTM) were implemented using the dataset to identify the most accurate
model for predicting neonatal mortality. Among the machine learning algorithms,
XGBoost and random forest classifier achieved the best accuracy with 94%, while
among the deep learning models, LSTM delivered the highest accuracy with 99%.
Therefore, using LSTM appears to be the most suitable approach to predict
whether precautionary measures for a child are necessary.

</details>


### [238] [Latent Concept Disentanglement in Transformer-based Language Models](https://arxiv.org/abs/2506.16975)
*Guan Zhe Hong,Bhavya Vasudeva,Vatsal Sharan,Cyrus Rashtchian,Prabhakar Raghavan,Rina Panigrahy*

Main category: cs.LG

TL;DR: 研究大语言模型上下文学习中transformer对潜在概念的处理，在不同推理任务有相应发现，提升对上下文学习和transformer表征的理解。


<details>
  <summary>Details</summary>
Motivation: 探讨transformer在上下文学习中是代表潜在结构进行计算，还是走捷径解决问题，此前相关研究未充分探讨学习表征与潜在概念关系且多为单步推理。

Method: 研究transformer在2 - hop推理任务（含离散潜在概念）和由连续潜在概念参数化的任务中对潜在概念的拆解和使用情况。

Result: 在2 - hop推理任务中成功识别潜在概念并进行逐步概念组合；在连续潜在概念任务的表征空间中发现低维子空间，其几何结构与底层参数化相似。

Conclusion: 研究结果细化了对上下文学习和transformer表征的理解，证明模型中存在高度局部化结构来拆解上下文学习任务中的潜在概念。

Abstract: When large language models (LLMs) use in-context learning (ICL) to solve a
new task, they seem to grasp not only the goal of the task but also core,
latent concepts in the demonstration examples. This begs the question of
whether transformers represent latent structures as part of their computation
or whether they take shortcuts to solve the problem. Prior mechanistic work on
ICL does not address this question because it does not sufficiently examine the
relationship between the learned representation and the latent concept, and the
considered problem settings often involve only single-step reasoning. In this
work, we examine how transformers disentangle and use latent concepts. We show
that in 2-hop reasoning tasks with a latent, discrete concept, the model
successfully identifies the latent concept and does step-by-step concept
composition. In tasks parameterized by a continuous latent concept, we find
low-dimensional subspaces in the representation space where the geometry mimics
the underlying parameterization. Together, these results refine our
understanding of ICL and the representation of transformers, and they provide
evidence for highly localized structures in the model that disentangle latent
concepts in ICL tasks.

</details>


### [239] [Robust Reinforcement Learning for Discrete Compositional Generation via General Soft Operators](https://arxiv.org/abs/2506.17007)
*Marco Jiralerspong,Esther Derman,Danilo Vucetic,Nikolay Malkin,Bilun Sun,Tianyu Zhang,Pierre-Luc Bacon,Gauthier Gidel*

Main category: cs.LG

TL;DR: 现有基于强化学习筛选候选对象的方法在处理奖励函数不确定性时不足，本文提出鲁棒强化学习方法，引入统一算子，得到高质量多样候选对象。


<details>
  <summary>Details</summary>
Motivation: 解决科学发现中从大量组合对象里筛选候选对象时，现有基于强化学习方法因奖励函数不确定而产生的问题。

Method: 采用鲁棒强化学习方法，引入统一算子，该算子追求对代理奖励函数不确定性的鲁棒性。

Result: 提出的新算法能在合成和现实任务中识别出更高质量、多样化的候选对象。

Conclusion: 为离散组合生成任务提供了新的、灵活的视角。

Abstract: A major bottleneck in scientific discovery involves narrowing a large
combinatorial set of objects, such as proteins or molecules, to a small set of
promising candidates. While this process largely relies on expert knowledge,
recent methods leverage reinforcement learning (RL) to enhance this filtering.
They achieve this by estimating proxy reward functions from available datasets
and using regularization to generate more diverse candidates. These reward
functions are inherently uncertain, raising a particularly salient challenge
for scientific discovery. In this work, we show that existing methods, often
framed as sampling proportional to a reward function, are inadequate and yield
suboptimal candidates, especially in large search spaces. To remedy this issue,
we take a robust RL approach and introduce a unified operator that seeks
robustness to the uncertainty of the proxy reward function. This general
operator targets peakier sampling distributions while encompassing known soft
RL operators. It also leads us to a novel algorithm that identifies
higher-quality, diverse candidates in both synthetic and real-world tasks.
Ultimately, our work offers a new, flexible perspective on discrete
compositional generation tasks. Code: https://github.com/marcojira/tgm.

</details>


### [240] [The Hidden Cost of an Image: Quantifying the Energy Consumption of AI Image Generation](https://arxiv.org/abs/2506.17016)
*Giulia Bertazzini,Chiara Albisani,Daniele Baracchi,Dasara Shullani,Roberto Verdecchia*

Main category: cs.LG

TL;DR: 研究评估AI图像生成能耗，对比17个模型，分析多因素影响，发现模型能耗差异大等情况。


<details>
  <summary>Details</summary>
Motivation: 随着AI图像生成普及及AI对环境资源需求增加，需了解其背后环境影响，即评估AI图像生成能耗。

Method: 开展综合实证实验，对比17个先进图像生成模型，考虑模型量化、图像分辨率、提示长度等影响能耗的因素，结合图像质量指标研究能耗与图像质量权衡。

Result: 图像生成模型能耗差异大，最高达46倍；图像分辨率对能耗影响不一致；基于U - Net的模型能耗低于基于Transformer的；模型量化降低多数模型能效；提示长度和内容无显著影响；提高图像质量不总意味着高能耗。

Conclusion: 通过实验分析出各因素对AI图像生成能耗及图像质量的影响。

Abstract: With the growing adoption of AI image generation, in conjunction with the
ever-increasing environmental resources demanded by AI, we are urged to answer
a fundamental question: What is the environmental impact hidden behind each
image we generate? In this research, we present a comprehensive empirical
experiment designed to assess the energy consumption of AI image generation.
Our experiment compares 17 state-of-the-art image generation models by
considering multiple factors that could affect their energy consumption, such
as model quantization, image resolution, and prompt length. Additionally, we
consider established image quality metrics to study potential trade-offs
between energy consumption and generated image quality. Results show that image
generation models vary drastically in terms of the energy they consume, with up
to a 46x difference. Image resolution affects energy consumption
inconsistently, ranging from a 1.3x to 4.7x increase when doubling resolution.
U-Net-based models tend to consume less than Transformer-based one. Model
quantization instead results to deteriorate the energy efficiency of most
models, while prompt length and content have no statistically significant
impact. Improving image quality does not always come at the cost of a higher
energy consumption, with some of the models producing the highest quality
images also being among the most energy efficient ones.

</details>


### [241] [Scalable and Reliable Multi-agent Reinforcement Learning for Traffic Assignment](https://arxiv.org/abs/2506.17029)
*Leizhen Wang,Peibo Duan,Cheng Lyu,Zewen Wang,Zhiqiang He,Nan Zheng,Zhenliang Ma*

Main category: cs.LG

TL;DR: 本文针对传统多智能体强化学习（MARL）在解决大规模交通分配问题上的不足，提出新的MARL - OD - DA框架，实验证明其在处理中等规模网络的交通分配问题上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有MARL框架在处理大规模交通分配问题时存在可扩展性和可靠性问题，限制了其实际应用，因此需要改进。

Method: 提出MARL - OD - DA框架，将智能体重新定义为起终点（OD）对路由器，设计基于狄利克雷的动作空间和基于局部相对差距的奖励函数。

Result: 该框架能有效处理中等规模网络中广泛且多样的城市级OD需求，在SiouxFalls网络中，10步内实现更好的分配方案，相对差距比传统方法低94.99%。

Conclusion: 提出的MARL - OD - DA框架能提升可扩展性和可靠性，在解决中等规模交通分配问题上优于现有MARL方法。

Abstract: The evolution of metropolitan cities and the increase in travel demands
impose stringent requirements on traffic assignment methods. Multi-agent
reinforcement learning (MARL) approaches outperform traditional methods in
modeling adaptive routing behavior without requiring explicit system dynamics,
which is beneficial for real-world deployment. However, MARL frameworks face
challenges in scalability and reliability when managing extensive networks with
substantial travel demand, which limiting their practical applicability in
solving large-scale traffic assignment problems. To address these challenges,
this study introduces MARL-OD-DA, a new MARL framework for the traffic
assignment problem, which redefines agents as origin-destination (OD) pair
routers rather than individual travelers, significantly enhancing scalability.
Additionally, a Dirichlet-based action space with action pruning and a reward
function based on the local relative gap are designed to enhance solution
reliability and improve convergence efficiency. Experiments demonstrate that
the proposed MARL framework effectively handles medium-sized networks with
extensive and varied city-level OD demand, surpassing existing MARL methods.
When implemented in the SiouxFalls network, MARL-OD-DA achieves better
assignment solutions in 10 steps, with a relative gap that is 94.99% lower than
that of conventional methods.

</details>


### [242] [Critical Appraisal of Fairness Metrics in Clinical Predictive AI](https://arxiv.org/abs/2506.17035)
*João Matos,Ben Van Calster,Leo Anthony Celi,Paula Dhiman,Judy Wawira Gichoya,Richard D. Riley,Chris Russell,Sara Khalid,Gary S. Collins*

Main category: cs.LG

TL;DR: 通过范围审查识别和评估临床预测AI的公平性指标，发现定义和量化公平性存在概念挑战及应用差距，未来应关注有临床意义的指标。


<details>
  <summary>Details</summary>
Motivation: 预测性AI有改善临床实践的机会，但公平性定义不明，需识别和评估其公平性指标。

Method: 对五个数据库（2014 - 2024）进行范围审查，筛选820条记录，纳入41项研究，提取62个公平性指标并分类。

Result: 公平性指标领域碎片化，临床验证有限，过度依赖阈值相关指标；18个为医疗保健开发，仅1个是临床效用指标。

Conclusion: 定义和量化公平性存在概念挑战，在不确定性量化、交叉性和现实应用方面有差距，未来应优先考虑有临床意义的指标。

Abstract: Predictive artificial intelligence (AI) offers an opportunity to improve
clinical practice and patient outcomes, but risks perpetuating biases if
fairness is inadequately addressed. However, the definition of "fairness"
remains unclear. We conducted a scoping review to identify and critically
appraise fairness metrics for clinical predictive AI. We defined a "fairness
metric" as a measure quantifying whether a model discriminates (societally)
against individuals or groups defined by sensitive attributes. We searched five
databases (2014-2024), screening 820 records, to include 41 studies, and
extracted 62 fairness metrics. Metrics were classified by
performance-dependency, model output level, and base performance metric,
revealing a fragmented landscape with limited clinical validation and
overreliance on threshold-dependent measures. Eighteen metrics were explicitly
developed for healthcare, including only one clinical utility metric. Our
findings highlight conceptual challenges in defining and quantifying fairness
and identify gaps in uncertainty quantification, intersectionality, and
real-world applicability. Future work should prioritise clinically meaningful
metrics.

</details>


### [243] [LSCD: Lomb-Scargle Conditioned Diffusion for Time series Imputation](https://arxiv.org/abs/2506.17039)
*Elizabeth Fons,Alejandro Sztrajman,Yousef El-Laham,Luciana Ferrer,Svitlana Vyetrenko,Manuela Veloso*

Main category: cs.LG

TL;DR: 提出可微Lomb - Scargle层并集成到扩散模型用于时间序列插补，实验表明能更准确恢复缺失数据并产生一致频率估计，且易集成到学习框架。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习中处理含缺失或不规则采样数据的时间序列的难题，避免基于FFT方法需插值而导致频谱失真的问题。

Method: 引入可微Lomb - Scargle层，将其集成到基于分数的扩散模型（LSCD）用于时间序列插补。

Result: 在合成和真实世界基准测试中，比纯时域基线方法更准确地恢复缺失数据，同时产生一致的频率估计。

Conclusion: 该方法可轻松集成到学习框架，能推动涉及不完整或不规则数据的机器学习方法中频谱引导的广泛应用。

Abstract: Time series with missing or irregularly sampled data are a persistent
challenge in machine learning. Many methods operate on the frequency-domain,
relying on the Fast Fourier Transform (FFT) which assumes uniform sampling,
therefore requiring prior interpolation that can distort the spectra. To
address this limitation, we introduce a differentiable Lomb--Scargle layer that
enables a reliable computation of the power spectrum of irregularly sampled
data. We integrate this layer into a novel score-based diffusion model (LSCD)
for time series imputation conditioned on the entire signal spectrum.
Experiments on synthetic and real-world benchmarks demonstrate that our method
recovers missing data more accurately than purely time-domain baselines, while
simultaneously producing consistent frequency estimates. Crucially, our method
can be easily integrated into learning frameworks, enabling broader adoption of
spectral guidance in machine learning approaches involving incomplete or
irregular data.

</details>


### [244] [MAWIFlow Benchmark: Realistic Flow-Based Evaluation for Network Intrusion Detection](https://arxiv.org/abs/2506.17041)
*Joshua Schraven,Alexander Windmann,Oliver Niggemann*

Main category: cs.LG

TL;DR: 本文引入基于MAWILAB v1.1数据集的MAWIFlow基准，对比传统机器学习和深度学习模型检测性能，结果显示CNN - BiLSTM泛化性更好，相关数据和代码公开。


<details>
  <summary>Details</summary>
Motivation: 现有网络入侵检测基准数据集使用合成流量，无法反映实际环境的统计变异性和时间漂移，需要真实可复现的评估数据集。

Method: 提出可复现的预处理流程将原始数据包转换为符合CICFlowMeter格式的流表示，对比传统机器学习方法（决策树、随机森林、XGBoost和逻辑回归）和基于CNN - BiLSTM架构的深度学习模型。

Result: 基于树的分类器在时间静态数据上表现良好，但随时间性能显著下降，CNN - BiLSTM模型保持较好性能，泛化性更好。

Conclusion: 强调合成基准和静态模型的局限性，鼓励采用具有明确时间结构的真实数据集，公开数据和代码以提高透明度和可复现性。

Abstract: Benchmark datasets for network intrusion detection commonly rely on
synthetically generated traffic, which fails to reflect the statistical
variability and temporal drift encountered in operational environments. This
paper introduces MAWIFlow, a flow-based benchmark derived from the MAWILAB v1.1
dataset, designed to enable realistic and reproducible evaluation of anomaly
detection methods. A reproducible preprocessing pipeline is presented that
transforms raw packet captures into flow representations conforming to the
CICFlowMeter format, while preserving MAWILab's original anomaly labels. The
resulting datasets comprise temporally distinct samples from January 2011,
2016, and 2021, drawn from trans-Pacific backbone traffic.
  To establish reference baselines, traditional machine learning methods,
including Decision Trees, Random Forests, XGBoost, and Logistic Regression, are
compared to a deep learning model based on a CNN-BiLSTM architecture. Empirical
results demonstrate that tree-based classifiers perform well on temporally
static data but experience significant performance degradation over time. In
contrast, the CNN-BiLSTM model maintains better performance, thus showing
improved generalization. These findings underscore the limitations of synthetic
benchmarks and static models, and motivate the adoption of realistic datasets
with explicit temporal structure. All datasets, pipeline code, and model
implementations are made publicly available to foster transparency and
reproducibility.

</details>


### [245] [Navigating the Deep: Signature Extraction on Deep Neural Networks](https://arxiv.org/abs/2506.17047)
*Haolin Liu,Adrien Siproudhis,Samuel Experton,Peter Lorenz,Christina Boura,Thomas Peyrin*

Main category: cs.LG

TL;DR: 本文重新审视并改进神经网络签名提取过程，解决了先前方法的局限，提升了提取深层网络的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 先前Carlini等人的神经网络参数提取方法有局限，仅适用于少数层网络，后续工作依赖签名提取可行的假设，因此需解决这些问题。

Method: 系统识别并解决Carlini等人签名提取方法的关键局限，如秩不足和深层噪声传播问题，针对每个问题提出高效算法解决方案。

Result: 通过在基于ReLU的神经网络上的大量实验验证，提取深度和准确性显著提高，如在CIFAR - 10数据集训练的神经网络8层中至少95%输入空间匹配，而先前方法只能提取前三层。

Conclusion: 研究成果是对更大更复杂神经网络架构进行实际攻击的关键一步。

Abstract: Neural network model extraction has emerged in recent years as an important
security concern, as adversaries attempt to recover a network's parameters via
black-box queries. A key step in this process is signature extraction, which
aims to recover the absolute values of the network's weights layer by layer.
Prior work, notably by Carlini et al. (2020), introduced a technique inspired
by differential cryptanalysis to extract neural network parameters. However,
their method suffers from several limitations that restrict its applicability
to networks with a few layers only. Later works focused on improving sign
extraction, but largely relied on the assumption that signature extraction
itself was feasible.
  In this work, we revisit and refine the signature extraction process by
systematically identifying and addressing for the first time critical
limitations of Carlini et al.'s signature extraction method. These limitations
include rank deficiency and noise propagation from deeper layers. To overcome
these challenges, we propose efficient algorithmic solutions for each of the
identified issues, greatly improving the efficiency of signature extraction.
Our approach permits the extraction of much deeper networks than was previously
possible. We validate our method through extensive experiments on ReLU-based
neural networks, demonstrating significant improvements in extraction depth and
accuracy. For instance, our extracted network matches the target network on at
least 95% of the input space for each of the eight layers of a neural network
trained on the CIFAR-10 dataset, while previous works could barely extract the
first three layers. Our results represent a crucial step toward practical
attacks on larger and more complex neural network architectures.

</details>


### [246] [From Concepts to Components: Concept-Agnostic Attention Module Discovery in Transformers](https://arxiv.org/abs/2506.17052)
*Jingtong Su,Julia Kempe,Karen Ullrich*

Main category: cs.LG

TL;DR: 本文提出SAMD和SAMI方法，用于将复杂概念映射到注意力头并干预概念影响，在语言和视觉任务验证有效性。


<details>
  <summary>Details</summary>
Motivation: 现有归因研究忽略注意力机制且缺乏分析复杂概念统一方法，为填补空白开展研究。

Method: 提出SAMD，将概念表示为向量，计算与注意力头余弦相似度选TopK构建模块；提出SAMI，用单个标量参数调整注意力模块。

Result: 模块位置在大模型训练前后稳定，证实大模型多语言机制；SAMI在HarmBench越狱+72.7%，GSM8K基准性能+1.6%，降低图像分类准确率。

Conclusion: 所提方法能有效处理复杂概念，具有领域无关性。

Abstract: Transformers have achieved state-of-the-art performance across language and
vision tasks. This success drives the imperative to interpret their internal
mechanisms with the dual goals of enhancing performance and improving
behavioral control. Attribution methods help advance interpretability by
assigning model outputs associated with a target concept to specific model
components. Current attribution research primarily studies multi-layer
perceptron neurons and addresses relatively simple concepts such as factual
associations (e.g., Paris is located in France). This focus tends to overlook
the impact of the attention mechanism and lacks a unified approach for
analyzing more complex concepts. To fill these gaps, we introduce Scalable
Attention Module Discovery (SAMD), a concept-agnostic method for mapping
arbitrary, complex concepts to specific attention heads of general transformer
models. We accomplish this by representing each concept as a vector,
calculating its cosine similarity with each attention head, and selecting the
TopK-scoring heads to construct the concept-associated attention module. We
then propose Scalar Attention Module Intervention (SAMI), a simple strategy to
diminish or amplify the effects of a concept by adjusting the attention module
using only a single scalar parameter. Empirically, we demonstrate SAMD on
concepts of varying complexity, and visualize the locations of their
corresponding modules. Our results demonstrate that module locations remain
stable before and after LLM post-training, and confirm prior work on the
mechanics of LLM multilingualism. Through SAMI, we facilitate jailbreaking on
HarmBench (+72.7%) by diminishing "safety" and improve performance on the GSM8K
benchmark (+1.6%) by amplifying "reasoning". Lastly, we highlight the
domain-agnostic nature of our approach by suppressing the image classification
accuracy of vision transformers on ImageNet.

</details>


### [247] [Flow-Based Non-stationary Temporal Regime Causal Structure Learning](https://arxiv.org/abs/2506.17065)
*Abdellah Rahmani,Pascal Frossard*

Main category: cs.LG

TL;DR: 论文引入FANTOM框架用于多变量时间序列因果发现，能处理非平稳过程与复杂噪声，理论证明模型可识别性，实验显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有因果发现方法无法处理多变量时间序列中的非平稳性和复杂噪声分布问题，需要新的方法来推断因果依赖和制度转换。

Method: 引入FANTOM框架，使用贝叶斯期望最大化算法最大化数据对数似然的证据下界，同时推断制度数量、对应索引并学习每个制度的有向无环图。

Result: 理论上证明了FANTOM中引入的时间异方差因果模型在平稳和非平稳环境中可识别，实验表明FANTOM在合成和真实数据上优于现有方法。

Conclusion: FANTOM是一个有效的因果发现统一框架，能解决现有方法无法处理的问题。

Abstract: Understanding causal relationships in multivariate time series is crucial in
many scenarios, such as those dealing with financial or neurological data. Many
such time series exhibit multiple regimes, i.e., consecutive temporal segments
with a priori unknown boundaries, with each regime having its own causal
structure. Inferring causal dependencies and regime shifts is critical for
analyzing the underlying processes. However, causal structure learning in this
setting is challenging due to (1) non stationarity, i.e., each regime can have
its own causal graph and mixing function, and (2) complex noise distributions,
which may be non Gaussian or heteroscedastic. Existing causal discovery
approaches cannot address these challenges, since generally assume stationarity
or Gaussian noise with constant variance. Hence, we introduce FANTOM, a unified
framework for causal discovery that handles non stationary processes along with
non Gaussian and heteroscedastic noises. FANTOM simultaneously infers the
number of regimes and their corresponding indices and learns each regime's
Directed Acyclic Graph. It uses a Bayesian Expectation Maximization algorithm
that maximizes the evidence lower bound of the data log likelihood. On the
theoretical side, we prove, under mild assumptions, that temporal
heteroscedastic causal models, introduced in FANTOM's formulation, are
identifiable in both stationary and non stationary settings. In addition,
extensive experiments on synthetic and real data show that FANTOM outperforms
existing methods.

</details>


### [248] [TransDreamerV3: Implanting Transformer In DreamerV3](https://arxiv.org/abs/2506.17103)
*Shruti Sadanand Dongare,Amun Kharel,Jonathan Samuel,Xiaona Zhou*

Main category: cs.LG

TL;DR: 本文介绍强化学习模型TransDreamerV3，其集成transformer编码器改进DreamerV3架构，实验显示在部分任务上性能提升。


<details>
  <summary>Details</summary>
Motivation: 提升复杂环境中的记忆和决策能力。

Method: 在DreamerV3架构中集成transformer编码器构建TransDreamerV3模型，并在多个任务上进行实验。

Result: 在Atari - Freeway和Crafter等任务上，TransDreamerV3性能优于DreamerV3；Minecraft任务有问题且所有任务训练有限。

Conclusion: TransDreamerV3在基于世界模型的强化学习方面有进展，借助了transformer架构。

Abstract: This paper introduces TransDreamerV3, a reinforcement learning model that
enhances the DreamerV3 architecture by integrating a transformer encoder. The
model is designed to improve memory and decision-making capabilities in complex
environments. We conducted experiments on Atari-Boxing, Atari-Freeway,
Atari-Pong, and Crafter tasks, where TransDreamerV3 demonstrated improved
performance over DreamerV3, particularly in the Atari-Freeway and Crafter
tasks. While issues in the Minecraft task and limited training across all tasks
were noted, TransDreamerV3 displays advancement in world model-based
reinforcement learning, leveraging transformer architectures.

</details>


### [249] [Rapid and Continuous Trust Evaluation for Effective Task Collaboration Through Siamese Model](https://arxiv.org/abs/2506.17128)
*Botao Zhu,Xianbin Wang*

Main category: cs.LG

TL;DR: 本文提出Siamese-enabled快速连续信任评估框架SRCTE，实验表明其收敛快、异常信任检测率高。


<details>
  <summary>Details</summary>
Motivation: 解决任务执行中因分布式设备、复杂环境和动态资源，快速连续评估协作者可信度的挑战。

Method: 收集协作者信任状态下的属性及历史数据用ACFG表示，任务执行时实时收集数据也用ACFG表示，用Siamese模型学习ACFG语义并生成嵌入，计算嵌入相似度确定信任值。

Result: 用两台Dell EMC 5200服务器和Google Pixel 8构建系统测试，SRCTE少量数据就能快速收敛，异常信任检测率高于基线算法。

Conclusion: SRCTE框架能有效促进任务协作，在信任评估上有较好表现。

Abstract: Trust is emerging as an effective tool to ensure the successful completion of
collaborative tasks within collaborative systems. However, rapidly and
continuously evaluating the trustworthiness of collaborators during task
execution is a significant challenge due to distributed devices, complex
operational environments, and dynamically changing resources. To tackle this
challenge, this paper proposes a Siamese-enabled rapid and continuous trust
evaluation framework (SRCTE) to facilitate effective task collaboration. First,
the communication and computing resource attributes of the collaborator in a
trusted state, along with historical collaboration data, are collected and
represented using an attributed control flow graph (ACFG) that captures
trust-related semantic information and serves as a reference for comparison
with data collected during task execution. At each time slot of task execution,
the collaborator's communication and computing resource attributes, as well as
task completion effectiveness, are collected in real time and represented with
an ACFG to convey their trust-related semantic information. A Siamese model,
consisting of two shared-parameter Structure2vec networks, is then employed to
learn the deep semantics of each pair of ACFGs and generate their embeddings.
Finally, the similarity between the embeddings of each pair of ACFGs is
calculated to determine the collaborator's trust value at each time slot. A
real system is built using two Dell EMC 5200 servers and a Google Pixel 8 to
test the effectiveness of the proposed SRCTE framework. Experimental results
demonstrate that SRCTE converges rapidly with only a small amount of data and
achieves a high anomaly trust detection rate compared to the baseline
algorithm.

</details>


### [250] [Sparse-Reg: Improving Sample Complexity in Offline Reinforcement Learning using Sparsity](https://arxiv.org/abs/2506.17155)
*Samin Yeasar Arnob,Scott Fujimoto,Doina Precup*

Main category: cs.LG

TL;DR: 研究离线强化学习中使用小数据集的问题，提出Sparse - Reg技术解决过拟合问题。


<details>
  <summary>Details</summary>
Motivation: 许多离线强化学习应用依赖小数据集，但常见算法在小数据集上会过拟合，导致性能不佳。

Method: 引入基于稀疏性的正则化技术Sparse - Reg来缓解离线强化学习中的过拟合。

Result: Sparse - Reg能在有限数据设置下有效学习，在连续控制任务中优于现有基准。

Conclusion: Sparse - Reg可以解决离线强化学习在小数据集上的过拟合问题，提升性能。

Abstract: In this paper, we investigate the use of small datasets in the context of
offline reinforcement learning (RL). While many common offline RL benchmarks
employ datasets with over a million data points, many offline RL applications
rely on considerably smaller datasets. We show that offline RL algorithms can
overfit on small datasets, resulting in poor performance. To address this
challenge, we introduce "Sparse-Reg": a regularization technique based on
sparsity to mitigate overfitting in offline reinforcement learning, enabling
effective learning in limited data settings and outperforming state-of-the-art
baselines in continuous control.

</details>


### [251] [Deep generative models as the probability transformation functions](https://arxiv.org/abs/2506.17171)
*Vitalii Bondar,Vira Babenko,Roman Trembovetskyi,Yurii Korobeinyk,Viktoriya Dzyuba*

Main category: cs.LG

TL;DR: 文章提出将深度生成模型视为概率转换函数的统一理论视角，指出不同生成模型本质都是将简单分布转换为复杂目标分布，利于方法改进和通用理论发展。


<details>
  <summary>Details</summary>
Motivation: 不同类型生成模型在架构和训练方法上存在明显差异，需要统一理论视角。

Method: 从理论层面将各种生成模型看作概率转换函数进行分析。

Result: 证明不同生成模型本质上都是将简单预定义分布转换为复杂目标数据分布。

Conclusion: 该统一视角有助于模型架构间方法改进的转移，为通用理论方法的发展奠定基础，可能带来更高效有效的生成建模技术。

Abstract: This paper introduces a unified theoretical perspective that views deep
generative models as probability transformation functions. Despite the apparent
differences in architecture and training methodologies among various types of
generative models - autoencoders, autoregressive models, generative adversarial
networks, normalizing flows, diffusion models, and flow matching - we
demonstrate that they all fundamentally operate by transforming simple
predefined distributions into complex target data distributions. This unifying
perspective facilitates the transfer of methodological improvements between
model architectures and provides a foundation for developing universal
theoretical approaches, potentially leading to more efficient and effective
generative modeling techniques.

</details>


### [252] [Network Sparsity Unlocks the Scaling Potential of Deep Reinforcement Learning](https://arxiv.org/abs/2506.17204)
*Guozheng Ma,Lu Li,Zilin Wang,Li Shen,Pierre-Luc Bacon,Dacheng Tao*

Main category: cs.LG

TL;DR: 引入静态网络稀疏性可解锁深度强化学习模型的扩展潜力，简单随机剪枝即可实现且效果良好。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习模型在训练中因网络病理难以有效扩展，此前有针对性干预和架构改进等方法。

Method: 采用简单的一次性随机剪枝，在训练前随机移除预定比例的网络权重。

Result: 稀疏网络比密集网络有更高的参数效率和更强的抗优化挑战能力，在视觉和流式强化学习场景中也展现出一致优势。

Conclusion: 仅引入静态网络稀疏性就能让深度强化学习模型获得比密集模型更好的扩展效果。

Abstract: Effectively scaling up deep reinforcement learning models has proven
notoriously difficult due to network pathologies during training, motivating
various targeted interventions such as periodic reset and architectural
advances such as layer normalization. Instead of pursuing more complex
modifications, we show that introducing static network sparsity alone can
unlock further scaling potential beyond their dense counterparts with
state-of-the-art architectures. This is achieved through simple one-shot random
pruning, where a predetermined percentage of network weights are randomly
removed once before training. Our analysis reveals that, in contrast to naively
scaling up dense DRL networks, such sparse networks achieve both higher
parameter efficiency for network expressivity and stronger resistance to
optimization challenges like plasticity loss and gradient interference. We
further extend our evaluation to visual and streaming RL scenarios,
demonstrating the consistent benefits of network sparsity.

</details>


### [253] [BREAD: Branched Rollouts from Expert Anchors Bridge SFT & RL for Reasoning](https://arxiv.org/abs/2506.17211)
*Xuechen Zhang,Zijian Huang,Yingcong Li,Chenshun Ni,Jiasi Chen,Samet Oymak*

Main category: cs.LG

TL;DR: 论文研究SFT + RL范式训练小语言模型（SLMs）的局限性，提出BREAD方法，能减少对真实轨迹依赖、加速训练并提升推理能力。


<details>
  <summary>Details</summary>
Motivation: 小语言模型学习复杂推理行为困难，标准SFT + RL范式有局限性，需探究并克服。

Method: 引入BREAD，一种GRPO变体，通过部分专家指导和分支展开统一SFT和RL阶段，失败时自适应插入专家前缀/提示。

Result: BREAD只需少于40%的真实轨迹，始终优于标准GRPO，训练加速约3倍。

Conclusion: BREAD帮助模型解决SFT + RL策略无法解决的问题，分支展开和专家指导可大幅提升SLM推理能力。

Abstract: Small language models (SLMs) struggle to learn complex reasoning behaviors,
especially when high-quality traces are scarce or difficult to learn from. The
standard training approach combines a supervised fine-tuning (SFT) stage, often
to distill capabilities of a larger model, followed by a reinforcement learning
(RL)stage such as Group Relative Policy Optimization (GRPO). In this paper, we
investigate the fundamental limitations of this SFT + RL paradigm and propose
methods to overcome them. Under a suitable theoretical model, we demonstrate
that the SFT + RL strategy can fail completely when (1) the expert's traces are
too difficult for the small model to express, or (2) the small model's
initialization has exponentially small likelihood of success. To address these,
we introduce BREAD: a GRPO variant that unifies the SFT and RL stages via
partial expert guidance and branched rollouts. When self-generated traces fail,
BREAD adaptively inserts short expert prefixes/hints, allowing the small model
to complete the rest of the reasoning path, and ensuring that each update
includes at least one successful trace. This mechanism both densifies the
reward signal and induces a natural learning curriculum. BREAD requires fewer
than 40% of ground-truth traces, consistently outperforming standard GRPO while
speeding up the training by about 3 times. Importantly, we demonstrate that
BREAD helps the model solve problems that are otherwise unsolvable by the SFT +
RL strategy, highlighting how branched rollouts and expert guidance can
substantially boost SLM reasoning.

</details>


### [254] [No Free Lunch: Rethinking Internal Feedback for LLM Reasoning](https://arxiv.org/abs/2506.17219)
*Yanzhi Zhang,Zhaoxi Zhang,Haoxiang Guan,Yilin Cheng,Yitong Duan,Chen Wang,Yue Wang,Shuxin Zheng,Jiyan He*

Main category: cs.LG

TL;DR: 本文研究基于内部反馈的强化学习（RLIF）用于大语言模型后训练，实验显示其在训练初期提升推理性能，但后期性能下降，对指令微调模型效果不佳，并分析了局限性。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的大语言模型后训练方法如RLHF和RLVR需大量外部监督，探索仅依赖模型内部信号的方法。

Method: 利用无监督奖励代理，如标记级熵、轨迹级熵和自我确定性，对RLIF策略进行理论分析和实验评估。

Result: RLIF在训练初期能提升基础大语言模型推理性能，可匹配或超越RLVR，但后期性能下降，对指令微调模型提升有限。

Conclusion: 分析了RLIF的训练行为和局限性，为将内部反馈信号集成到LLM训练提供实用指南，期望为LLM后训练提供更有效的策略。

Abstract: Reinforcement learning has emerged as a powerful paradigm for post-training
large language models (LLMs) to improve reasoning. Approaches like
Reinforcement Learning from Human Feedback (RLHF) and Reinforcement Learning
with Verifiable Rewards (RLVR) have shown strong results, but they require
extensive external supervision. We investigate an alternative class of methods,
Reinforcement Learning from Internal Feedback (RLIF), which relies solely on
intrinsic model-derived signals instead of external rewards. In particular, we
leverage unsupervised reward proxies such as token-level entropy,
trajectory-level entropy, and self-certainty. Our theoretical analysis shows
these internal objectives are partially equivalent, and we empirically evaluate
various RLIF strategies on challenging math reasoning benchmarks. Experimental
results demonstrate that RLIF can boost the reasoning performance of base LLMs
at the beginning phase of the training, matching or surpassing RLVR techniques
on these tasks. However, when training progresses, performance degrades even
below the model before training. Moreover, we find that RLIF yields little
improvement for instruction-tuned models, indicating diminishing returns of
intrinsic feedback once an LLM is already instruction-tuned. We further analyze
this limitation by mixing model weights and explain the reason of RLIF's
training behaviors, providing practical guidelines for integrating internal
feedback signals into LLM training. We hope our analysis of internal feedback
will inform more principled and effective strategies for LLM post-training.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [255] [A Study of Hybrid and Evolutionary Metaheuristics for Single Hidden Layer Feedforward Neural Network Architecture](https://arxiv.org/abs/2506.15737)
*Gautam Siddharth Kashyap,Md Tabrez Nafis,Samar Wazir*

Main category: cs.NE

TL;DR: 研究用PSO、GA替代SGD训练ANN，开发混合PSO - SGD策略，结果显示该混合策略及其他方法能提升训练效率和准确性。


<details>
  <summary>Details</summary>
Motivation: SGD训练ANN存在计算成本高和收敛到局部最优的问题，需寻找替代方法。

Method: 研究PSO和GA作为SGD的替代方法，开发混合PSO - SGD策略。

Result: 混合PSO - SGD技术使训练MSE降低90 - 95%，RMHC使MSE降低约85 - 90%，RS表现不佳。

Conclusion: 混合和进化程序比传统优化方法显著提高训练效率和准确性，BBH可能仍有效。

Abstract: Training Artificial Neural Networks (ANNs) with Stochastic Gradient Descent
(SGD) frequently encounters difficulties, including substantial computing
expense and the risk of converging to local optima, attributable to its
dependence on partial weight gradients. Therefore, this work investigates
Particle Swarm Optimization (PSO) and Genetic Algorithms (GAs) - two
population-based Metaheuristic Optimizers (MHOs) - as alternatives to SGD to
mitigate these constraints. A hybrid PSO-SGD strategy is developed to improve
local search efficiency. The findings indicate that the hybrid PSO-SGD
technique decreases the median training MSE by 90 to 95 percent relative to
conventional GA and PSO across various network sizes (e.g., from around 0.02 to
approximately 0.001 in the Sphere function). RMHC attains substantial
enhancements, reducing MSE by roughly 85 to 90 percent compared to GA.
Simultaneously, RS consistently exhibits errors exceeding 0.3, signifying
subpar performance. These findings underscore that hybrid and evolutionary
procedures significantly improve training efficiency and accuracy compared to
conventional optimization methods and imply that the Building Block Hypothesis
(BBH) may still be valid, indicating that advantageous weight structures are
retained during evolutionary search.

</details>


### [256] [Neural Cellular Automata for ARC-AGI](https://arxiv.org/abs/2506.15746)
*Kevin Xu,Risto Miikkulainen*

Main category: cs.NE

TL;DR: 本文探索神经细胞自动机（NCA）在精确变换和少样本泛化任务中的表现，采用梯度训练方法，结果表明其对ARC中抽象网格任务是有前景且高效的方法。


<details>
  <summary>Details</summary>
Motivation: 探究NCA在需要精确变换和少样本泛化任务中的表现，利用ARC - AGI挑战其能力。

Method: 使用基于梯度的训练方法，从训练示例中学习迭代更新规则，将输入网格转换为输出，并应用于测试输入。

Result: 梯度训练的NCA模型是处理ARC中一系列抽象网格任务的有前景且高效的方法。

Conclusion: 探讨设计修改和训练约束的影响，研究NCA在ARC中的行为和特性，为自组织系统的更广泛应用提供见解。

Abstract: Cellular automata and their differentiable counterparts, Neural Cellular
Automata (NCA), are highly expressive and capable of surprisingly complex
behaviors. This paper explores how NCAs perform when applied to tasks requiring
precise transformations and few-shot generalization, using the Abstraction and
Reasoning Corpus for Artificial General Intelligence (ARC-AGI) as a domain that
challenges their capabilities in ways not previously explored. Specifically,
this paper uses gradient-based training to learn iterative update rules that
transform input grids into their outputs from the training examples and apply
them to the test inputs. Results suggest that gradient-trained NCA models are a
promising and efficient approach to a range of abstract grid-based tasks from
ARC. Along with discussing the impacts of various design modifications and
training constraints, this work examines the behavior and properties of NCAs
applied to ARC to give insights for broader applications of self-organizing
systems.

</details>


### [257] [Robust Dynamic Material Handling via Adaptive Constrained Evolutionary Reinforcement Learning](https://arxiv.org/abs/2506.16795)
*Chengpeng Hu,Ziming Wang,Bo Yuan,Jialin Liu,Chengqi Zhang,Xin Yao*

Main category: cs.NE

TL;DR: 本文提出自适应约束进化强化学习(ACERL)方法解决动态物料处理问题，实验证明其性能出色且鲁棒。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习解决动态物料处理问题存在适应动态事件难、奖励稀疏、需合理利用资源和历史记录等挑战。

Method: 提出ACERL方法，维护行动者种群进行多样化探索，解决稀疏奖励和约束违规问题，自适应选择训练实例。

Result: 在训练和测试实例中，ACERL比现有算法表现出色，能满足约束；在含噪声实例中表现鲁棒；交叉验证证明整体有效性；消融研究凸显各部分作用。

Conclusion: ACERL在解决动态物料处理问题上是有效且鲁棒的方法。

Abstract: Dynamic material handling (DMH) involves the assignment of dynamically
arriving material transporting tasks to suitable vehicles in real time for
minimising makespan and tardiness. In real-world scenarios, historical task
records are usually available, which enables the training of a decision policy
on multiple instances consisting of historical records. Recently, reinforcement
learning has been applied to solve DMH. Due to the occurrence of dynamic events
such as new tasks, adaptability is highly required. Solving DMH is challenging
since constraints including task delay should be satisfied. A feedback is
received only when all tasks are served, which leads to sparse reward. Besides,
making the best use of limited computational resources and historical records
for training a robust policy is crucial. The time allocated to different
problem instances would highly impact the learning process. To tackle those
challenges, this paper proposes a novel adaptive constrained evolutionary
reinforcement learning (ACERL) approach, which maintains a population of actors
for diverse exploration. ACERL accesses each actor for tackling sparse rewards
and constraint violation to restrict the behaviour of the policy. Moreover,
ACERL adaptively selects the most beneficial training instances for improving
the policy. Extensive experiments on eight training and eight unseen test
instances demonstrate the outstanding performance of ACERL compared with
several state-of-the-art algorithms. Policies trained by ACERL can schedule the
vehicles while fully satisfying the constraints. Additional experiments on 40
unseen noised instances show the robust performance of ACERL. Cross-validation
further presents the overall effectiveness of ACREL. Besides, a rigorous
ablation study highlights the coordination and benefits of each ingredient of
ACERL.

</details>


### [258] [Continual Learning with Columnar Spiking Neural Networks](https://arxiv.org/abs/2506.17169)
*Denis Larionov,Nikolay Bazenkov,Mikhail Kiselev*

Main category: cs.NE

TL;DR: 研究柱状组织脉冲神经网络用于持续学习和灾难性遗忘，用CoLaNET展示微列适应新任务情况及超参数平衡稳定性与可塑性，最优配置有效学习MNIST任务且遗忘率低。


<details>
  <summary>Details</summary>
Motivation: 研究柱状组织的脉冲神经网络在持续学习和解决灾难性遗忘问题上的应用。

Method: 使用CoLaNET（Columnar Layered Network）进行研究。

Result: 最优配置能有效学习十个顺序MNIST任务，每个任务保持92%的准确率，后续九个任务训练后首个任务性能仅下降4%。

Conclusion: 微列在与先前学习无共享结构时能最有效地适应新任务，CoLaNET超参数可平衡保留旧知识和获取新信息。

Abstract: This study investigates columnar-organized spiking neural networks (SNNs) for
continual learning and catastrophic forgetting. Using CoLaNET (Columnar Layered
Network), we show that microcolumns adapt most efficiently to new tasks when
they lack shared structure with prior learning. We demonstrate how CoLaNET
hyperparameters govern the trade-off between retaining old knowledge
(stability) and acquiring new information (plasticity). Our optimal
configuration learns ten sequential MNIST tasks effectively, maintaining 92%
accuracy on each. It shows low forgetting, with only 4% performance degradation
on the first task after training on nine subsequent tasks.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [259] [How to Increase Energy Efficiency with a Single Linux Command](https://arxiv.org/abs/2506.16046)
*Alborz Jelvani,Richard P Martin,Santosh Nagarakatte*

Main category: cs.PF

TL;DR: 现有处理器动态功耗管理调优无法实现最优节能，研究表明简单设置功率上限可提高能效，且易于实现。


<details>
  <summary>Details</summary>
Motivation: 现有处理器动态功率管理调优无法实现最优节能，需新方法解决。

Method: 通过对配备双英特尔至强可扩展处理器的服务器级系统进行为期一个月的SPEC CPU 2017基准测试数据采集和系统测量。

Result: 设置简单功率上限比传统节能配置能效提高达25%，性能损失小，且可通过单个Linux命令实现。

Conclusion: 建议程序员和管理员使用功率上限作为保持能效和可接受性能的主要机制，而非复杂DVFS算法。

Abstract: Processors with dynamic power management provide a variety of settings to
control energy efficiency. However, tuning these settings does not achieve
optimal energy savings. We highlight how existing power capping mechanisms can
address these limitations without requiring any changes to current power
governors. We validate this approach using system measurements across a
month-long data acquisition campaign from SPEC CPU 2017 benchmarks on a
server-class system equipped with dual Intel Xeon Scalable processors. Our
results indicate that setting a simple power cap can improve energy efficiency
by up to 25% over traditional energy-saving system configurations with little
performance loss, as most default settings focus on thermal regulation and
performance rather than compute efficiency. Power capping is very accessible
compared to other approaches, as it can be implemented with a single Linux
command. Our results point to programmers and administrators using power caps
as a primary mechanism to maintain significant energy efficiency while
retaining acceptable performance, as opposed to deploying complex DVFS
algorithms.

</details>


### [260] [Dependability of UAV-Based Networks and Computing Systems: A Survey](https://arxiv.org/abs/2506.16786)
*Qingyang Zhang,Mohammad Dwipa Furqan,Tasfia Nutzhat,Fumio Machida,Ermeson Andrade*

Main category: cs.PF

TL;DR: 本文对基于无人机的网络和计算系统的可靠性进行系统文献综述，揭示研究趋势，分类文献并指出未来需探索的研究领域。


<details>
  <summary>Details</summary>
Motivation: 随着无人机系统发展，其可靠性保障成为关键挑战，系统面临多种不确定性威胁可靠性。

Method: 进行系统的文献综述。

Result: 揭示该领域新兴研究趋势，按威胁类型和采用技术对文献进行全面分类。

Conclusion: 确定了未来实现可靠无人机系统需要进一步探索的八个研究领域。

Abstract: Uncrewed Aerial Vehicle (UAV) computing and networking are becoming a
fundamental computation infrastructure for diverse cyber-physical application
systems. UAVs can be empowered by AI on edge devices and can communicate with
other UAVs and ground stations via wireless communication networks. Dynamic
computation demands and heterogeneous computing resources are distributed in
the system and need to be controlled to maintain the quality of services and to
accomplish critical missions. With the evolution of UAV-based systems,
dependability assurance of such systems emerges as a crucial challenge.
UAV-based systems confront diverse sources of uncertainty that may threaten
their dependability, such as software bugs, component failures, network
disconnections, battery shortages, and disturbances from the real world. In
this paper, we conduct systematic literature reviews on the dependability of
UAV-based networks and computing systems. The survey report reveals emerging
research trends in this field and summarizes the literature into comprehensive
categories by threat types and adopted technologies. Based on our literature
reviews, we identify eight research fields that require further exploration in
the future to achieve dependable UAV-based systems.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [261] [How Do Community Smells Influence Self-Admitted Technical Debt in Machine Learning Projects?](https://arxiv.org/abs/2506.15884)
*Shamse Tasnim Cynthia,Nuri Almarimi,Banani Roy*

Main category: cs.SE

TL;DR: 研究开源机器学习项目中社区坏味与自我承认的技术债务（SATD）的流行情况、关系及演变，强调早期检测和缓解社会技术问题的重要性。


<details>
  <summary>Details</summary>
Motivation: 先前研究未充分考察机器学习项目中社区坏味与SATD的相互作用，本研究旨在填补这一空白。

Method: 分析155个基于机器学习系统的发布级数据，检测社区坏味和SATD，进行统计分析。

Result: 社区坏味普遍存在且分布有差异；某些坏味与较高的SATD发生率强相关；权威和沟通相关坏味常与持久的代码和设计债务共存；发现项目规模相关的趋势和共同轨迹。

Conclusion: 早期检测和缓解社会技术问题对维持机器学习系统的长期质量和可持续性至关重要。

Abstract: Community smells reflect poor organizational practices that often lead to
socio-technical issues and the accumulation of Self-Admitted Technical Debt
(SATD). While prior studies have explored these problems in general software
systems, their interplay in machine learning (ML)-based projects remains
largely underexamined. In this study, we investigated the prevalence of
community smells and their relationship with SATD in open-source ML projects,
analyzing data at the release level. First, we examined the prevalence of ten
community smell types across the releases of 155 ML-based systems and found
that community smells are widespread, exhibiting distinct distribution patterns
across small, medium, and large projects. Second, we detected SATD at the
release level and applied statistical analysis to examine its correlation with
community smells. Our results showed that certain smells, such as Radio Silence
and Organizational Silos, are strongly correlated with higher SATD occurrences.
Third, we considered the six identified types of SATD to determine which
community smells are most associated with each debt category. Our analysis
revealed authority- and communication-related smells often co-occur with
persistent code and design debt. Finally, we analyzed how the community smells
and SATD evolve over the releases, uncovering project size-dependent trends and
shared trajectories. Our findings emphasize the importance of early detection
and mitigation of socio-technical issues to maintain the long-term quality and
sustainability of ML-based systems.

</details>


### [262] [Regression Testing Optimization for ROS-based Autonomous Systems: A Comprehensive Review of Techniques](https://arxiv.org/abs/2506.16101)
*Yupeng Jiang,Shuaiyi Sun,Xi Zheng*

Main category: cs.SE

TL;DR: 本文首次全面调研了针对ROSAS的回归测试优化技术，对122项研究分类分析，指出挑战并提出未来方向，为相关研究提供基础参考和实践路线。


<details>
  <summary>Details</summary>
Motivation: 传统回归测试技术用于自治系统面临挑战，针对ROSAS的回归测试优化研究不足。

Method: 对122项代表性研究进行分析和分类，引入结构化分类法。

Result: 明确了ROSAS回归测试的主要挑战，如测试用例优先级确定、冗余测试最小化和受影响测试用例选择等。

Conclusion: 提出研究见解和有前景的未来方向，可提升回归测试效率和效果。

Abstract: Regression testing plays a critical role in maintaining software reliability,
particularly for ROS-based autonomous systems (ROSAS), which frequently undergo
continuous integration and iterative development. However, conventional
regression testing techniques face significant challenges when applied to
autonomous systems due to their dynamic and non-deterministic behaviors,
complex multi-modal sensor data, asynchronous distributed architectures, and
stringent safety and real-time constraints. Although numerous studies have
explored test optimization in traditional software contexts, regression testing
optimization specifically for ROSAS remains largely unexplored. To address this
gap, we present the first comprehensive survey systematically reviewing
regression testing optimization techniques tailored for ROSAS. We analyze and
categorize 122 representative studies into regression test case prioritization,
minimization, and selection methods. A structured taxonomy is introduced to
clearly illustrate their applicability and limitations within ROSAS contexts.
Furthermore, we highlight major challenges specific to regression testing for
ROSAS, including effectively prioritizing tests in response to frequent system
modifications, efficiently minimizing redundant tests, and difficulty in
accurately selecting impacted test cases. Finally, we propose research insights
and identify promising future directions, such as leveraging frame-to-vector
coverage metrics, multi-source foundation models, and neurosymbolic reasoning
to enhance regression testing efficiency and effectiveness. This survey
provides a foundational reference and practical roadmap for advancing the
state-of-the-art in regression testing optimization for ROSAS.

</details>


### [263] [Seeing is Fixing: Cross-Modal Reasoning with Multimodal LLMs for Visual Software Issue Fixing](https://arxiv.org/abs/2506.16136)
*Kai Huang,Jian Zhang,Xiaofei Xie,Chunyang Chen*

Main category: cs.SE

TL;DR: 现有基于大语言模型的自动程序修复技术在单模态场景评估效果好，但在多模态场景有局限。本文提出GUIRepair方法，通过整合Image2Code和Code2Image组件处理多模态问题，在SWE - bench M上评估效果显著。


<details>
  <summary>Details</summary>
Motivation: 现有自动程序修复系统在多模态问题场景中，因难以解释和利用视觉信息而表现不佳，需新方法解决。

Method: 提出GUIRepair方法，整合Image2Code（基于问题报告提取相关项目文档，生成重现代码以理解故障）和Code2Image（使用重现代码重现视觉问题场景，捕获GUI渲染以验证补丁）两个组件。

Result: 在SWE - bench M上评估，使用GPT - 4o为基础模型时解决157个实例，优于最佳开源基线；使用o4 - mini为基础模型时解决175个实例，优于顶级商业系统。

Conclusion: 通过理解和捕获视觉信息融入跨模态推理的新视角，能有效解决多模态问题。

Abstract: Large language model-(LLM) based automated program repair (APR) techniques
have shown promising results in resolving real-world GitHub issue tasks.
Existing APR systems are primarily evaluated in unimodal settings (e.g.,
SWE-bench). However, these autonomous systems struggle to resolve multimodal
problem scenarios (e.g., SWE-bench M) due to limitations in interpreting and
leveraging visual information. In multimodal scenarios, LLMs need to rely on
visual information in the graphical user interface (GUI) to understand bugs and
generate fixes. To bridge this gap, we propose GUIRepair, a cross-modal
reasoning approach for resolving multimodal issue scenarios by understanding
and capturing visual information. Specifically, GUIRepair integrates two key
components, Image2Code and Code2Image, to enhance fault comprehension and patch
validation. Image2Code extracts relevant project documents based on the issue
report, then applies this domain knowledge to generate the reproduced code
responsible for the visual symptoms, effectively translating GUI images into
executable context for better fault comprehension. Code2Image replays the
visual issue scenario using the reproduced code and captures GUI renderings of
the patched program to assess whether the fix visually resolves the issue,
providing feedback for patch validation. We evaluate GUIRepair on SWE-bench M,
and the approach demonstrates significant effectiveness. When utilizing GPT-4o
as the base model, GUIRepair solves 157 instances, outperforming the best
open-source baseline by 26 instances. Furthermore, when using o4-mini as the
base model, GUIRepair can achieve even better results and solve 175 instances,
outperforming the top commercial system by 22 instances. This emphasizes the
success of our new perspective on incorporating cross-modal reasoning by
understanding and capturing visual information to resolve multimodal issues.

</details>


### [264] [The Technical Debt Gamble: A Case Study on Technical Debt in a Large-Scale Industrial Microservice Architecture](https://arxiv.org/abs/2506.16214)
*Klara Borowa,Andrzej Ratkowski,Roberto Verdecchia*

Main category: cs.SE

TL;DR: 研究大规模微服务工业系统中技术债务（TD）的表现及管理策略，采用混合方法案例研究。


<details>
  <summary>Details</summary>
Motivation: 微服务架构虽有高可维护性和可演化性，但易受TD影响，且少有大规模系统TD研究。

Method: 对含超100个微服务、服务超15k地点的项目进行混合方法案例研究，结合静态代码分析器定量方法、与开发团队焦点小组讨论及与首席架构师后续访谈的定性方法。

Result: 简单静态源代码分析是全面发现TD的有效切入点；沟通不足显著导致TD；架构与组织结构不一致会加剧TD积累；微服务会快速经历TD积累与解决循环，即“微服务架构技术债务赌博”。

Conclusion: 确定了一套适合微服务架构TD管理的策略。

Abstract: Microservice architectures provide an intuitive promise of high
maintainability and evolvability due to loose coupling. However, these quality
attributes are notably vulnerable to technical debt (TD). Few studies address
TD in microservice systems, particularly on a large scale. This research
explores how TD manifests in a large-scale microservice-based industrial
system. The research is based on a mixed-method case study of a project
including over 100 microservices and serving over 15k locations. Results are
collected via a quantitative method based static code analyzers combined with
qualitative insights derived from a focus group discussion with the development
team and a follow-up interview with the lead architect of the case study
system. Results show that (1) simple static source code analysis can be an
efficient and effective entry point for holistic TD discovery, (2) inadequate
communication significantly contributes to TD, (3) misalignment between
architectural and organizational structures can exacerbate TD accumulation, (4)
microservices can rapidly cycle through TD accumulation and resolution, a
phenomenon referred to as "microservice architecture technical debt gamble".
Finally, we identify a set of fitting strategies for TD management in
microservice architectures.

</details>


### [265] [Evaluating the Use of LLMs for Documentation to Code Traceability](https://arxiv.org/abs/2506.16440)
*Ebube Alor,SayedHassan Khatoonabadi,Emad Shihab*

Main category: cs.SE

TL;DR: 对Claude 3.5 Sonnet、GPT - 4o和o3 - mini等大语言模型在软件文档与源代码追溯链接方面进行评估，展示其效果和局限。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在自动化文档到代码追溯方面潜力未充分挖掘，需评估其能力。

Method: 创建两个开源项目的新数据集，对模型的三项关键能力进行系统实验评估。

Result: 最佳模型在两个数据集上F1分数达79.4%和80.4%，远超基线模型；关系解释部分准确率超97%；多步链端点准确率高但中间链接有差异。错误多源于命名假设等。

Conclusion: 大语言模型是追溯发现的有力助手，但有局限，需人机结合工具设计并关注错误模式。

Abstract: Large Language Models (LLMs) offer new potential for automating
documentation-to-code traceability, yet their capabilities remain
underexplored. We present a comprehensive evaluation of LLMs (Claude 3.5
Sonnet, GPT-4o, and o3-mini) in establishing trace links between various
software documentation (including API references and user guides) and source
code. We create two novel datasets from two open-source projects (Unity Catalog
and Crawl4AI). Through systematic experiments, we assess three key
capabilities: (1) trace link identification accuracy, (2) relationship
explanation quality, and (3) multi-step chain reconstruction. Results show that
the best-performing LLM achieves F1-scores of 79.4% and 80.4% across the two
datasets, substantially outperforming our baselines (TF-IDF, BM25, and
CodeBERT). While fully correct relationship explanations range from 42.9% to
71.1%, partial accuracy exceeds 97%, indicating that fundamental connections
are rarely missed. For multi-step chains, LLMs maintain high endpoint accuracy
but vary in capturing precise intermediate links. Error analysis reveals that
many false positives stem from naming-based assumptions, phantom links, or
overgeneralization of architectural patterns. We demonstrate that task-framing,
such as a one-to-many matching strategy, is critical for performance. These
findings position LLMs as powerful assistants for trace discovery, but their
limitations could necessitate human-in-the-loop tool design and highlight
specific error patterns for future research.

</details>


### [266] [Understanding the Challenges and Promises of Developing Generative AI Apps: An Empirical Study](https://arxiv.org/abs/2506.16453)
*Buthayna AlMulla,Maram Assi,Safwat Hassan*

Main category: cs.SE

TL;DR: 本文对Google Play商店中173个生成式AI应用的676,066条评论进行用户中心分析，介绍SARA方法，提取用户见解，分析热门话题及随时间的演变，为开发者和研究者提供建议。


<details>
  <summary>Details</summary>
Motivation: 尽管生成式AI应用广泛采用，但用户如何在实践中感知和评估其功能尚不清楚，需进行研究。

Method: 引入SARA四阶段方法，利用基于提示的大语言模型技术系统提取用户见解，进行五样本提示和非信息性评论过滤。

Result: 证明大语言模型在主题提取中的可靠性，准确率达91%，确定前10个用户讨论话题，分析关键挑战和新兴机会，研究话题随时间的演变。

Conclusion: 基于研究结果和观察，为开发者和研究者提出可行动的建议。

Abstract: The release of ChatGPT in 2022 triggered a rapid surge in generative
artificial intelligence mobile apps (i.e., Gen-AI apps). Despite widespread
adoption, little is known about how end users perceive and evaluate these
Gen-AI functionalities in practice. In this work, we conduct a user-centered
analysis of 676,066 reviews from 173 Gen-AI apps on the Google Play Store. We
introduce a four-phase methodology, SARA (Selection, Acquisition, Refinement,
and Analysis), that enables the systematic extraction of user insights using
prompt-based LLM techniques. First, we demonstrate the reliability of LLMs in
topic extraction, achieving 91% accuracy through five-shot prompting and
non-informative review filtering. Then, we apply this method to the informative
reviews, identify the top 10 user-discussed topics (e.g., AI Performance,
Content Quality, and Content Policy & Censorship) and analyze the key
challenges and emerging opportunities. Finally, we examine how these topics
evolve over time, offering insight into shifting user expectations and
engagement patterns with Gen-AI apps. Based on our findings and observations,
we present actionable implications for developers and researchers.

</details>


### [267] [Scaling GR(1) Synthesis via a Compositional Framework for LTL Discrete Event Control](https://arxiv.org/abs/2506.16557)
*Hernán Gagliardi,Victor Braberman,Sebastian Uchitel*

Main category: cs.SE

TL;DR: 提出离散事件系统控制器合成的组合方法，可避免状态爆炸，结果组合式，实现合成并展示优势


<details>
  <summary>Details</summary>
Motivation: 解决整体合成方法易出现的状态爆炸问题

Method: 利用待控工厂模块化结构，迭代构建最大允许安全控制器，用观测合成等价性简化，实现GR(1)子集合成

Result: 合成结果为一组并行运行可确保LTL目标的控制器，能解决比整体方法大1000倍的问题

Conclusion: 组合式方法在离散事件系统控制器合成中有显著优势，能有效避免状态爆炸

Abstract: We present a compositional approach to controller synthesis of discrete event
system controllers with linear temporal logic (LTL) goals. We exploit the
modular structure of the plant to be controlled, given as a set of labelled
transition systems (LTS), to mitigate state explosion that monolithic
approaches to synthesis are prone to. Maximally permissive safe controllers are
iteratively built for subsets of the plant LTSs by solving weaker control
problems. Observational synthesis equivalence is used to reduce the size of the
controlled subset of the plant by abstracting away local events. The result of
synthesis is also compositional, a set of controllers that when run in parallel
ensure the LTL goal. We implement synthesis in the MTSA tool for an expressive
subset of LTL, GR(1), and show it computes solutions to that can be up to 1000
times larger than those that the monolithic approach can solve.

</details>


### [268] [AI-Driven Tools in Modern Software Quality Assurance: An Assessment of Benefits, Challenges, and Future Directions](https://arxiv.org/abs/2506.16586)
*Ihor Pysmennyi,Roman Kyslyi,Kyrylo Kleshch*

Main category: cs.SE

TL;DR: 研究现代分布式软件应用质量保证，分析集成AI工具的利弊与前景，通过实验证明有潜力但存在挑战。


<details>
  <summary>Details</summary>
Motivation: 传统质量保证方法应对现代软件系统存在挑战，资源有限且成本高，需探索新方法。

Method: 对验证和确认过程进行综合分析，实施企业应用端到端回归作为概念验证。

Result: 生成测试用例的不稳定执行率仅8.3%，显示出方法有潜力，但也存在语义覆盖、模型黑盒等挑战。

Conclusion: AI对质量保证有变革潜力，但需战略实施，考虑局限性并开发验证方法。

Abstract: Traditional quality assurance (QA) methods face significant challenges in
addressing the complexity, scale, and rapid iteration cycles of modern software
systems and are strained by limited resources available, leading to substantial
costs associated with poor quality. The object of this research is the Quality
Assurance processes for modern distributed software applications. The subject
of the research is the assessment of the benefits, challenges, and prospects of
integrating modern AI-oriented tools into quality assurance processes. We
performed comprehensive analysis of implications on both verification and
validation processes covering exploratory test analyses, equivalence
partitioning and boundary analyses, metamorphic testing, finding
inconsistencies in acceptance criteria (AC), static analyses, test case
generation, unit test generation, test suit optimization and assessment, end to
end scenario execution. End to end regression of sample enterprise application
utilizing AI-agents over generated test scenarios was implemented as a proof of
concept highlighting practical use of the study. The results, with only 8.3%
flaky executions of generated test cases, indicate significant potential for
the proposed approaches. However, the study also identified substantial
challenges for practical adoption concerning generation of semantically
identical coverage, "black box" nature and lack of explainability from
state-of-the-art Large Language Models (LLMs), the tendency to correct mutated
test cases to match expected results, underscoring the necessity for thorough
verification of both generated artifacts and test execution results. The
research demonstrates AI's transformative potential for QA but highlights the
importance of a strategic approach to implementing these technologies,
considering the identified limitations and the need for developing appropriate
verification methodologies.

</details>


### [269] [LLM-based Satisfiability Checking of String Requirements by Consistent Data and Checker Generation](https://arxiv.org/abs/2506.16639)
*Boqi Chen,Aren A. Babikian,Shuzhao Feng,Dániel Varró,Gunter Mussbacher*

Main category: cs.SE

TL;DR: 本文提出混合方法，用大语言模型验证字符串自然语言需求可满足性，实验显示LLMs能有效翻译，生成检查器提升效果。


<details>
  <summary>Details</summary>
Motivation: 验证自然语言字符串需求集属性有挑战，形式化方法有局限且手动翻译成本高，大语言模型在该领域效果研究少。

Method: 提出混合方法，用LLMs得出可满足性结果和一致字符串，生成声明式和命令式检查器验证结果。

Result: LLMs有效将自然语言翻译成检查器，Python检查器测试准确率达100%，检查器提升生成一致字符串和识别不可满足需求能力，特定情况生成成功率和F1分数翻倍。

Conclusion: 所提混合方法在验证字符串自然语言需求可满足性方面有效，LLMs结合检查器可提升性能。

Abstract: Requirements over strings, commonly represented using natural language (NL),
are particularly relevant for software systems due to their heavy reliance on
string data manipulation. While individual requirements can usually be analyzed
manually, verifying properties (e.g., satisfiability) over sets of NL
requirements is particularly challenging. Formal approaches (e.g., SMT solvers)
may efficiently verify such properties, but are known to have theoretical
limitations. Additionally, the translation of NL requirements into formal
constraints typically requires significant manual effort. Recently, large
language models (LLMs) have emerged as an alternative approach for formal
reasoning tasks, but their effectiveness in verifying requirements over strings
is less studied. In this paper, we introduce a hybrid approach that verifies
the satisfiability of NL requirements over strings by using LLMs (1) to derive
a satisfiability outcome (and a consistent string, if possible), and (2) to
generate declarative (i.e., SMT) and imperative (i.e., Python) checkers, used
to validate the correctness of (1). In our experiments, we assess the
performance of four LLMs. Results show that LLMs effectively translate natural
language into checkers, even achieving perfect testing accuracy for
Python-based checkers. These checkers substantially help LLMs in generating a
consistent string and accurately identifying unsatisfiable requirements,
leading to more than doubled generation success rate and F1-score in certain
cases compared to baselines without generated checkers.

</details>


### [270] [SemAgent: A Semantics Aware Program Repair Agent](https://arxiv.org/abs/2506.16650)
*Anvith Pabba,Alex Mathai,Anindya Chakraborty,Baishakhi Ray*

Main category: cs.SE

TL;DR: 介绍SemAgent用于自动程序修复，利用多种语义生成完整补丁，在SWEBench - Lite基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有代理系统在解决软件问题时缺乏对问题、代码和执行语义的深入理解，生成的补丁易过拟合。

Method: 引入SemAgent工作流，通过新的管道，利用执行语义检索上下文，通过抽象理解问题语义，分离代码语义，并采用两阶段架构。

Result: 在SWEBench - Lite基准测试中解决率达44.66%，优于其他基于工作流的方法，较基线绝对提升7.66%。

Conclusion: 将问题和代码语义融入自动程序修复管道可实现健壮且语义一致的修复。

Abstract: Large Language Models (LLMs) have shown impressive capabilities in downstream
software engineering tasks such as Automated Program Repair (APR). In
particular, there has been a lot of research on repository-level
issue-resolution benchmarks such as SWE-Bench. Although there has been
significant progress on this topic, we notice that in the process of solving
such issues, existing agentic systems tend to hyper-localize on immediately
suspicious lines of code and fix them in isolation, without a deeper
understanding of the issue semantics, code semantics, or execution semantics.
Consequently, many existing systems generate patches that overfit to the user
issue, even when a more general fix is preferable. To address this limitation,
we introduce SemAgent, a novel workflow-based procedure that leverages issue,
code, and execution semantics to generate patches that are complete -
identifying and fixing all lines relevant to the issue. We achieve this through
a novel pipeline that (a) leverages execution semantics to retrieve relevant
context, (b) comprehends issue-semantics via generalized abstraction, (c)
isolates code-semantics within the context of this abstraction, and (d)
leverages this understanding in a two-stage architecture: a repair stage that
proposes fine-grained fixes, followed by a reviewer stage that filters relevant
fixes based on the inferred issue-semantics. Our evaluations show that our
methodology achieves a solve rate of 44.66% on the SWEBench-Lite benchmark
beating all other workflow-based approaches, and an absolute improvement of
7.66% compared to our baseline, which lacks such deep semantic understanding.
We note that our approach performs particularly well on issues requiring
multi-line reasoning (and editing) and edge-case handling, suggesting that
incorporating issue and code semantics into APR pipelines can lead to robust
and semantically consistent repairs.

</details>


### [271] [LLMs in Coding and their Impact on the Commercial Software Engineering Landscape](https://arxiv.org/abs/2506.16653)
*Vladislav Belozerov,Peter J Barclay,Askhan Sami*

Main category: cs.SE

TL;DR: 大语言模型编码工具主流但有风险，企业需采取措施确保安全准确。


<details>
  <summary>Details</summary>
Motivation: 大语言模型编码工具在软件工程中普及，但带来隐私数据泄露、安全漏洞和附和错误观点等新风险。

Method: 企业应标记和审查每一行AI生成代码，将提示和输出保留在私有或本地部署中，遵守安全法规，添加捕捉附和答案的测试。

Result: 未提及具体研究成果

Conclusion: 企业采取措施可在提升开发速度的同时保证安全和准确性。

Abstract: Large-language-model coding tools are now mainstream in software engineering.
But as these same tools move human effort up the development stack, they
present fresh dangers: 10% of real prompts leak private data, 42% of generated
snippets hide security flaws, and the models can even ``agree'' with wrong
ideas, a trait called sycophancy. We argue that firms must tag and review every
AI-generated line of code, keep prompts and outputs inside private or
on-premises deployments, obey emerging safety regulations, and add tests that
catch sycophantic answers -- so they can gain speed without losing security and
accuracy.

</details>


### [272] [cAST: Enhancing Code Retrieval-Augmented Generation with Structural Chunking via Abstract Syntax Tree](https://arxiv.org/abs/2506.15655)
*Yilin Zhang,Xinran Zhao,Zora Zhiruo Wang,Chenyang Yang,Jiayi Wei,Tongshuang Wu*

Main category: cs.SE

TL;DR: 现有基于行的代码分块启发式方法常破坏语义结构，本文提出基于抽象语法树的分块方法，在多代码生成任务中提升性能，强调结构感知分块对检索增强代码智能的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有基于行的分块启发式方法常破坏语义结构，导致生成质量下降，需更好的分块方法。

Method: 提出通过抽象语法树进行分块的方法，递归分解大的AST节点为小分块，并在遵守大小限制的情况下合并兄弟节点。

Result: 在多种代码生成任务中提升性能，如在RepoEval检索中Recall@5提高4.3个点，在SWE - bench生成中Pass@1提高2.67个点。

Conclusion: 结构感知的分块对扩展检索增强的代码智能非常重要。

Abstract: Retrieval-Augmented Generation (RAG) has become essential for large-scale
code generation, grounding predictions in external code corpora to improve
actuality. However, a critical yet underexplored aspect of RAG pipelines is
chunking -- the process of dividing documents into retrievable units. Existing
line-based chunking heuristics often break semantic structures, splitting
functions or merging unrelated code, which can degrade generation quality. We
propose chunking via Abstract Syntax Trees (\ourwork), a structure-aware method
that recursively breaks large AST nodes into smaller chunks and merges sibling
nodes while respecting size limits. This approach generates self-contained,
semantically coherent units across programming languages and tasks, improving
performance on diverse code generation tasks, e.g., boosting Recall@5 by 4.3
points on RepoEval retrieval and Pass@1 by 2.67 points on SWE-bench generation.
Our work highlights the importance of structure-aware chunking for scaling
retrieval-enhanced code intelligence.

</details>


### [273] [Accountability of Robust and Reliable AI-Enabled Systems: A Preliminary Study and Roadmap](https://arxiv.org/abs/2506.16831)
*Filippo Scaramuzza,Damian A. Tamburri,Willem-Jan van den Heuvel*

Main category: cs.SE

TL;DR: 本文开展评估AI系统鲁棒性、可靠性的初步研究，强调问责制重要性，指出未来研究方向和现存差距。


<details>
  <summary>Details</summary>
Motivation: 评估AI系统鲁棒性、可靠性，确保其在实际应用中的安全和有效性。

Method: 探索概念定义、回顾现有文献，采用案例研究。

Result: 强调创新测试解决方案的需求，指出问责制对构建信任和确保AI负责任发展至关重要。

Conclusion: 鲁棒性、可靠性和问责制是未来可信AI系统发展的关键领域。

Abstract: This vision paper presents initial research on assessing the robustness and
reliability of AI-enabled systems, and key factors in ensuring their safety and
effectiveness in practical applications, including a focus on accountability.
By exploring evolving definitions of these concepts and reviewing current
literature, the study highlights major challenges and approaches in the field.
A case study is used to illustrate real-world applications, emphasizing the
need for innovative testing solutions. The incorporation of accountability is
crucial for building trust and ensuring responsible AI development. The paper
outlines potential future research directions and identifies existing gaps,
positioning robustness, reliability, and accountability as vital areas for the
development of trustworthy AI systems of the future.

</details>


### [274] [Revolutionizing Validation and Verification: Explainable Testing Methodologies for Intelligent Automotive Decision-Making Systems](https://arxiv.org/abs/2506.16876)
*Halit Eris,Stefan Wagner*

Main category: cs.SE

TL;DR: 本文提出将可解释性、透明度和可解释性融入自动驾驶系统验证与确认（V&V）流程的方法，以提高效率并建立用户信任。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统的复杂决策模型给V&V带来挑战，现有手动测试方法低效且劳动密集，需要改进。

Method: 通过文献综述和利益相关者输入完善V&V要求，利用大语言模型生成可解释测试场景，在模拟环境中进行实时验证，框架包含测试预言、解释生成和测试聊天机器人。

Result: 计划进行实证研究评估诊断效率和透明度的改进。

Conclusion: 目标是简化V&V流程，减少资源消耗，建立用户对自动驾驶技术的信任。

Abstract: Autonomous Driving Systems (ADS) use complex decision-making (DM) models with
multimodal sensory inputs, making rigorous validation and verification (V&V)
essential for safety and reliability. These models pose challenges in
diagnosing failures, tracing anomalies, and maintaining transparency, with
current manual testing methods being inefficient and labor-intensive. This
vision paper presents a methodology that integrates explainability,
transparency, and interpretability into V&V processes. We propose refining V&V
requirements through literature reviews and stakeholder input, generating
explainable test scenarios via large language models (LLMs), and enabling
real-time validation in simulation environments. Our framework includes test
oracle, explanation generation, and a test chatbot, with empirical studies
planned to evaluate improvements in diagnostic efficiency and transparency. Our
goal is to streamline V&V, reduce resources, and build user trust in autonomous
technologies.

</details>


### [275] [Quantum Optimization for Software Engineering: A Survey](https://arxiv.org/abs/2506.16878)
*Man Zhang,Yuechen Li,Tao Yue,Kai-Yuan Cai*

Main category: cs.SE

TL;DR: 该系统性文献综述研究将量子或类量子算法应用于解决经典软件工程优化问题的文献，发现研究集中于部分领域，也存在研究空白，为SBSE社区应对挑战提供参考。


<details>
  <summary>Details</summary>
Motivation: 量子计算在量子优化领域发展且软件工程优化有基础，但现代软件系统复杂度增加需要创新解决方案，因此开展对相关文献的研究。

Method: 通过精心设计的搜索字符串对六个数字数据库进行系统搜索，从2083篇出版物中筛选出77篇主要研究进行分析。

Result: 研究发现集中在软件工程操作和软件测试等领域，其他软件工程活动存在显著研究空白，还发现传统软件工程场所外的相关作品。

Conclusion: 研究提供了研究领域的广泛概述，有助于SBSE社区利用量子技术进步应对下一代软件工程挑战。

Abstract: Quantum computing, particularly in the area of quantum optimization, is
steadily progressing toward practical applications, supported by an expanding
range of hardware platforms and simulators. While Software Engineering (SE)
optimization has a strong foundation, which is exemplified by the active
Search-Based Software Engineering (SBSE) community and numerous classical
optimization methods, the growing complexity of modern software systems and
their engineering processes demands innovative solutions. This Systematic
Literature Review (SLR) focuses specifically on studying the literature that
applies quantum or quantum-inspired algorithms to solve classical SE
optimization problems. We examine 77 primary studies selected from an initial
pool of 2083 publications obtained through systematic searches of six digital
databases using carefully crafted search strings. Our findings reveal
concentrated research efforts in areas such as SE operations and software
testing, while exposing significant gaps across other SE activities.
Additionally, the SLR uncovers relevant works published outside traditional SE
venues, underscoring the necessity of this comprehensive review. Overall, our
study provides a broad overview of the research landscape, empowering the SBSE
community to leverage quantum advancements in addressing next-generation SE
challenges.

</details>


### [276] [Identifying Explanation Needs: Towards a Catalog of User-based Indicators](https://arxiv.org/abs/2506.16997)
*Hannah Deters,Laura Reinhardt,Jakob Droste,Martin Obaidi,Kurt Schneider*

Main category: cs.SE

TL;DR: 本文聚焦软件系统可解释性中个体解释需求获取的挑战，通过在线研究建立用户相关指标以判断何时需要解释，并分析指标与解释需求类型的关系，这些指标可用于需求获取和运行时触发解释。


<details>
  <summary>Details</summary>
Motivation: 在软件系统日益复杂的数字化世界，可解释性质量愈发重要，而获取个体解释需求面临假设或确认偏差等挑战，因此需要建立相关指标。

Method: 以在线研究的形式进行探索性研究，收集自我报告的指标。

Result: 编制了包含用户行为、系统事件、情绪状态或身体反应的指标目录，并分析了指标与不同类型解释需求的关系。

Conclusion: 所建立的指标可用于原型需求获取、已部署应用需求收集，以及在运行时适当触发解释。

Abstract: In today's digitalized world, where software systems are becoming
increasingly ubiquitous and complex, the quality aspect of explainability is
gaining relevance. A major challenge in achieving adequate explanations is the
elicitation of individual explanation needs, as it may be subject to severe
hypothetical or confirmation biases. To address these challenges, we aim to
establish user-based indicators concerning user behavior or system events that
can be captured at runtime to determine when a need for explanations arises. In
this work, we conducted explorative research in form of an online study to
collect self-reported indicators that could indicate a need for explanation. We
compiled a catalog containing 17 relevant indicators concerning user behavior,
8 indicators concerning system events and 14 indicators concerning emotional
states or physical reactions. We also analyze the relationships between these
indicators and different types of need for explanation. The established
indicators can be used in the elicitation process through prototypes, as well
as after publication to gather requirements from already deployed applications
using telemetry and usage data. Moreover, these indicators can be used to
trigger explanations at appropriate moments during the runtime.

</details>


### [277] [Behavior Driven Development for 3D Games](https://arxiv.org/abs/2506.17057)
*Fernando Pastor Ricós,Beatriz Marín,I. S. W. B. Prasetya,Tanja E. J. Vos,Joseph Davidson,Karel Hovorka*

Main category: cs.SE

TL;DR: 本文探讨iv4XR框架结合BDD方法用于3D游戏自动化回归测试，展示其在不同游戏中的应用及多功能性。


<details>
  <summary>Details</summary>
Motivation: 解决复杂3D游戏测试需求，消除iv4XR框架使用时开发者与测试者协作的技术障碍。

Method: 将行为驱动开发（BDD）方法与iv4XR框架集成，对iv4XR框架扩展战术编程。

Result: 实现了《Space Engineers》回归测试自动化，启发将BDD应用于《LabRecruits》，扩展框架后可自动化长流程测试场景。

Conclusion: iv4XR框架具有多功能性，BDD方法让用户能用易读语句创建、管理和执行自动化游戏测试。

Abstract: Computer 3D games are complex software environments that require novel
testing processes to ensure high-quality standards. The Intelligent
Verification/Validation for Extended Reality Based Systems (iv4XR) framework
addresses this need by enabling the implementation of autonomous agents to
automate game testing scenarios. This framework facilitates the automation of
regression test cases for complex 3D games like Space Engineers. Nevertheless,
the technical expertise required to define test scripts using iv4XR can
constrain seamless collaboration between developers and testers. This paper
reports how integrating a Behavior-driven Development (BDD) approach with the
iv4XR framework allows the industrial company behind Space Engineers to
automate regression testing. The success of this industrial collaboration has
inspired the iv4XR team to integrate the BDD approach to improve the automation
of play-testing for the experimental 3D game LabRecruits. Furthermore, the
iv4XR framework has been extended with tactical programming to enable the
automation of long-play test scenarios in Space Engineers. These results
underscore the versatility of the iv4XR framework in supporting diverse testing
approaches while showcasing how BDD empowers users to create, manage, and
execute automated game tests using comprehensive and human-readable statements.

</details>


### [278] [Software Fairness Testing in Practice](https://arxiv.org/abs/2506.17095)
*Ronnie de Souza Santos,Matheus de Morais Leca,Reydne Santos,Cleyton Magalhaes*

Main category: cs.SE

TL;DR: 本文指出软件测试中公平性测试在AI应用的重要性，但实践应用有限。通过访谈从业者，发现理论与实践有差距，需实用工具和策略解决公平性问题。


<details>
  <summary>Details</summary>
Motivation: 解决AI公平性测试在行业实践中应用有限，从业者缺乏指导和有效工具的问题。

Method: 对22位从事AI和ML项目的从业者进行访谈。

Result: 发现理论公平概念与行业实践存在显著差距，定义难解读应用，缺乏适配工具，还有数据、时间、指标和模型互操作性等挑战。

Conclusion: 需要将学术进展与可行动的策略和工具相结合，让从业者能系统解决AI系统公平性问题。

Abstract: Software testing ensures that a system functions correctly, meets specified
requirements, and maintains high quality. As artificial intelligence and
machine learning (ML) technologies become integral to software systems, testing
has evolved to address their unique complexities. A critical advancement in
this space is fairness testing, which identifies and mitigates biases in AI
applications to promote ethical and equitable outcomes. Despite extensive
academic research on fairness testing, including test input generation, test
oracle identification, and component testing, practical adoption remains
limited. Industry practitioners often lack clear guidelines and effective tools
to integrate fairness testing into real-world AI development. This study
investigates how software professionals test AI-powered systems for fairness
through interviews with 22 practitioners working on AI and ML projects. Our
findings highlight a significant gap between theoretical fairness concepts and
industry practice. While fairness definitions continue to evolve, they remain
difficult for practitioners to interpret and apply. The absence of
industry-aligned fairness testing tools further complicates adoption,
necessitating research into practical, accessible solutions. Key challenges
include data quality and diversity, time constraints, defining effective
metrics, and ensuring model interoperability. These insights emphasize the need
to bridge academic advancements with actionable strategies and tools, enabling
practitioners to systematically address fairness in AI systems.

</details>


### [279] [Reassessing Code Authorship Attribution in the Era of Language Models](https://arxiv.org/abs/2506.17120)
*Atish Kumar Dipongkor,Ziyu Yao,Kevin Moran*

Main category: cs.SE

TL;DR: 研究使用多种代码语言模型进行代码作者归属任务，分析模型性能并给出未来方向。


<details>
  <summary>Details</summary>
Motivation: 代码作者归属（CAA）在网络安全和软件取证中重要，但现有自动化方法有局限，基于transformer的语言模型在CAA中的有效性不明。

Method: 对6个不同数据集应用2个大型和5个小型的先进代码语言模型进行CAA任务，并使用机器学习可解释性技术深入分析模型性能。

Result: 分析结果揭示了语言模型在CAA任务中理解代码风格模式的行为。

Conclusion: 研究结果为未来工作指明重要方向。

Abstract: The study of Code Stylometry, and in particular Code Authorship Attribution
(CAA), aims to analyze coding styles to identify the authors of code samples.
CAA is crucial in cybersecurity and software forensics for addressing,
detecting plagiarism, and supporting criminal prosecutions. However, CAA is a
complex and error prone task, due to the need for recognizing nuanced
relationships between coding patterns. This challenge is compounded in large
software systems with numerous authors due to the subtle variability of
patterns that signify the coding style of one author among many. Given the
challenges related to this task, researchers have proposed and studied
automated approaches that rely upon classical Machine Learning and Deep
Learning techniques. However, such techniques have historically relied upon
hand-crafted features, and due to the often intricate interaction of different
features (e.g., formatting, etc.), have key limitations in properly
characterizing authorship, and are sensitive to adversarial code perturbations.
Recently, transformer-based Language Models (LMs) have shown remarkable
efficacy across a range of software engineering tasks, and in the authorship
attribution on natural language in the NLP domain. However, their effectiveness
in CAA is not well understood. As such, we conduct the first extensive
empirical study applying two larger state-of-the-art code LMs, and five smaller
code LMs to the task of CAA to 6 diverse datasets that encompass 12k code
snippets written by 463 developers. Furthermore, we perform an in-depth
analysis of our studied models' performance on CAA using established machine
learning interpretability techniques. The results of our analysis illustrate
important findings that illuminate the behavior of LMs in understanding
stylometric code patterns during the task of CAA, and point towards important
directions for future work.

</details>


### [280] [Large Language Model Unlearning for Source Code](https://arxiv.org/abs/2506.17125)
*Xue Jiang,Yihong Dong,Zheng Fang,Yingwei Ma,Tangxinyu Wang,Rongyu Cao,Binhua Li,Zhi Jin,Wenpin Jiao,Yongbin Li,Ge Li*

Main category: cs.SE

TL;DR: LLM4SE有风险，现有大模型遗忘技术用于代码时致模型可用性下降，本文提出PROD方法，建立评估基准，评估显示其效果好，扩展了遗忘技术应用边界。


<details>
  <summary>Details</summary>
Motivation: 大模型潜在记忆敏感或过时训练数据有风险，现有大模型遗忘技术在源代码方面适用性未充分探索，应用时导致模型效用严重下降。

Method: 提出PROD方法，抑制遗忘数据在输出分布中的概率，促进候选分布组件，建立代码遗忘评估基准。

Result: PROD在三个下游任务中比现有遗忘方法在遗忘质量和模型效用间取得更好平衡，应用于不同系列大模型时有改进，对抗攻击鲁棒性好，不生成或暴露遗忘数据。

Conclusion: PROD扩展了遗忘技术应用到源代码的边界，对推进可靠代码生成有重要意义。

Abstract: LLM4SE has demonstrated significant success, but LLMs' potential memorization
of sensitive or outdated training data introduces critical risks to legal
compliance, software security, and code quality. LLM unlearning techniques,
which can eliminate the influence of undesired data from LLMs in a
post-training way, present a promising solution to address these concerns.
While recent efforts in LLM unlearning show effectiveness in natural language,
their applicability to source code remains underexplored. Our empirical study
reveals that existing LLM unlearning approaches, when applied to source code,
cause severe model utility degradation, rendering models practically unusable
for code generation. In this paper, we propose PROD, a novel unlearning
approach that enables LLMs to forget undesired code content while effectively
preserving their code generation capabilities. PROD suppresses the probability
of forget data in LLMs' output distribution while promoting candidate
distributional components, enabling the model to jointly learn to forget
specific content and retain its general capabilities. To facilitate this study,
we establish a benchmark for code unlearning evaluation, which includes three
critical downstream tasks: copyrighted code unlearning, insecure code
unlearning, and deprecated API unlearning. Our evaluation demonstrates that
PROD achieves superior balance between forget quality and model utility
compared to existing unlearning approaches across three downstream tasks, while
consistently exhibiting improvements when applied to LLMs of varying series.
PROD also exhibits superior robustness against adversarial attacks without
generating or exposing the data to be forgotten. The results underscore that
our approach not only extends the application boundary of unlearning techniques
to source code, but also holds significant implications for advancing reliable
code generation.

</details>


### [281] [Dissecting the SWE-Bench Leaderboards: Profiling Submitters and Architectures of LLM- and Agent-Based Repair Systems](https://arxiv.org/abs/2506.17208)
*Matias Martinez,Xavier Franch*

Main category: cs.SE

TL;DR: 本文对SWE - Bench Lite和Verified排行榜的所有提交进行综合研究，分析不同维度，揭示了专有LLM主导等情况。


<details>
  <summary>Details</summary>
Motivation: 由于SWE - Bench提交过程无需详细文档，许多解决方案的架构设计和来源不明，需进行全面研究。

Method: 对SWE - Bench Lite的68个条目和Verified的79个条目提交进行研究，从提交者类型、产品可用性、LLM使用和系统架构等维度分析67种独特方法。

Result: 发现专有LLMs（尤其是Claude 3.5/3.7）占主导地位，存在代理和非代理设计，贡献者涵盖个人开发者到大型科技公司。

Conclusion: 通过对SWE - Bench提交的分析，清晰呈现了当前基于LLM的程序修复系统的现状。

Abstract: The rapid progress in Automated Program Repair (APR) has been driven by
advances in AI, particularly large language models (LLMs) and agent-based
systems. SWE-Bench is a recent benchmark designed to evaluate LLM-based repair
systems using real issues and pull requests mined from 12 popular open-source
Python repositories. Its public leaderboards, SWE-Bench Lite and SWE-Bench
Verified, have become central platforms for tracking progress and comparing
solutions. However, because the submission process does not require detailed
documentation, the architectural design and origin of many solutions remain
unclear. In this paper, we present the first comprehensive study of all
submissions to the SWE-Bench Lite (68 entries) and Verified (79 entries)
leaderboards, analyzing 67 unique approaches across dimensions such as
submitter type, product availability, LLM usage, and system architecture. Our
findings reveal the dominance of proprietary LLMs (especially Claude 3.5/3.7),
the presence of both agentic and non-agentic designs, and a contributor base
spanning from individual developers to large tech companies.

</details>


<div id='q-fin.RM'></div>

# q-fin.RM [[Back]](#toc)

### [282] [On Design of Representative Distributionally Robust Formulations for Evaluation of Tail Risk Measures](https://arxiv.org/abs/2506.16230)
*Anand Deo*

Main category: q-fin.RM

TL;DR: 本文利用极值理论提出新的分布鲁棒优化（DRO）公式评估条件风险价值（CVaR），避免低估尾部风险，理论证明其可实施，且可扩展到多变量应用和其他风险度量评估，数值实验展示了实用性。


<details>
  <summary>Details</summary>
Motivation: 传统CVaR对分布尾部敏感，不当的DRO公式会严重低估尾部风险，因此需要改进的DRO公式来进行合理的最坏情况CVaR评估。

Method: 利用极值理论得出DRO公式，仅需校准一个标量参数。

Result: 理论上证明即使分布尾部样本稀少，该公式也可从数据中实现；可扩展到多变量应用和其他常用风险度量评估；数值实验证明了方法的实用性。

Conclusion: 所提出的基于极值理论的DRO公式能避免低估尾部风险，同时不过度高估真实CVaR，具有实际应用价值。

Abstract: Conditional Value-at-Risk (CVaR) is a risk measure widely used to quantify
the impact of extreme losses. Owing to the lack of representative samples CVaR
is sensitive to the tails of the underlying distribution. In order to combat
this sensitivity, Distributionally Robust Optimization (DRO), which evaluates
the worst-case CVaR measure over a set of plausible data distributions is often
deployed. Unfortunately, an improper choice of the DRO formulation can lead to
a severe underestimation of tail risk. This paper aims at leveraging extreme
value theory to arrive at a DRO formulation which leads to representative
worst-case CVaR evaluations in that the above pitfall is avoided while
simultaneously, the worst case evaluation is not a gross over-estimate of the
true CVaR. We demonstrate theoretically that even when there is paucity of
samples in the tail of the distribution, our formulation is readily
implementable from data, only requiring calibration of a single scalar
parameter. We showcase that our formulation can be easily extended to provide
robustness to tail risk in multivariate applications as well as in the
evaluation of other commonly used risk measures. Numerical illustrations on
synthetic and real-world data showcase the practical utility of our approach.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [283] [Modern approaches to building effective interpretable models of the property market using machine learning](https://arxiv.org/abs/2506.15723)
*Irina G. Tanashkina,Alexey S. Tanashkin,Alexander S. Maksimchuik,Anna Yu. Poshivailo*

Main category: q-fin.ST

TL;DR: 文章基于俄罗斯滨海边疆区房产批量评估，回顾用机器学习构建房产市场可解释模型的现代方法，介绍建模各阶段及应对困难的方法，提出不同类型房产的建模方式，证明可在可解释性要求下构建有效模型。


<details>
  <summary>Details</summary>
Motivation: 解决缺乏专业知识的研究者在构建房产市场模型时，因真实市场数据与理想数据差异大而遇到的困难。

Method: 涵盖建模各阶段，针对土地采用经典线性回归与地质统计学插值方法结合，针对公寓采用基于决策树自动生成和选择额外规则的线性回归（RuleFit方法）。

Result: 对于土地可构建有效模型；对于公寓，RuleFit方法可应对空间点多对象的情况。

Conclusion: 即使有可解释性的强限制，仍能构建有效的房地产市场模型。

Abstract: In this article, we review modern approaches to building interpretable models
of property markets using machine learning on the base of mass valuation of
property in the Primorye region, Russia. The researcher, lacking expertise in
this topic, encounters numerous difficulties in the effort to build a good
model. The main source of this is the huge difference between noisy real market
data and ideal data which is very common in all types of tutorials on machine
learning. This paper covers all stages of modeling: the collection of initial
data, identification of outliers, the search and analysis of patterns in data,
the formation and final choice of price factors, the building of the model, and
the evaluation of its efficiency. For each stage, we highlight potential issues
and describe sound methods for overcoming emerging difficulties on actual
examples. We show that the combination of classical linear regression with
interpolation methods of geostatistics allows to build an effective model for
land parcels. For flats, when many objects are attributed to one spatial point
the application of geostatistical methods is difficult. Therefore we suggest
linear regression with automatic generation and selection of additional rules
on the base of decision trees, so called the RuleFit method. Thus we show, that
despite the strong restriction as the requirement of interpretability which is
important in practical aspects, for example, legal matters, it is still
possible to build effective models of real property markets.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [284] [Sampling conditioned diffusions via Pathspace Projected Monte Carlo](https://arxiv.org/abs/2506.15743)
*Tobias Grafke*

Main category: stat.ML

TL;DR: 提出一种对受多种约束的随机微分方程采样的算法，并通过多个示例展示其有效性。


<details>
  <summary>Details</summary>
Motivation: 对受一般约束（包括积分约束、端点约束和随机积分约束）的随机微分方程进行采样。

Method: 使用路径空间Metropolis调整的流形采样方案，在满足约束条件的子流形上对随机路径进行采样。

Result: 通过对动态凝聚相变、随机游走、随机非线性波动方程和湍流管道流的随机偏微分方程模型等示例采样，展示了算法的有效性。

Conclusion: 所提出的算法能有效对受多种约束的随机微分方程进行采样。

Abstract: We present an algorithm to sample stochastic differential equations
conditioned on rather general constraints, including integral constraints,
endpoint constraints, and stochastic integral constraints. The algorithm is a
pathspace Metropolis-adjusted manifold sampling scheme, which samples
stochastic paths on the submanifold of realizations that adhere to the
conditioning constraint. We demonstrate the effectiveness of the algorithm by
sampling a dynamical condensation phase transition, conditioning a random walk
on a fixed Levy stochastic area, conditioning a stochastic nonlinear wave
equation on high amplitude waves, and sampling a stochastic partial
differential equation model of turbulent pipe flow conditioned on
relaminarization events.

</details>


### [285] [From Local Interactions to Global Operators: Scalable Gaussian Process Operator for Physical Systems](https://arxiv.org/abs/2506.15906)
*Sawan Kumar,Tapas Tripura,Rajdip Nayek,Souvik Chakraborty*

Main category: stat.ML

TL;DR: 本文提出可扩展的高斯过程算子（GPO），解决高维数据密集场景难题，经多种偏微分方程测试效果好。


<details>
  <summary>Details</summary>
Motivation: 将概率神经算子（如GPO）扩展到高维、数据密集场景存在挑战。

Method: 通过巧妙的核设计利用稀疏性、局部性和结构信息，在空间域用基于近邻的局部核近似，在参数空间用稀疏核近似，采用结构化Kronecker分解；嵌入算子感知核结构，使用基于神经算子架构的任务感知均值函数。

Result: 在包括Navier - Stokes、波平流等多种非线性偏微分方程上的广泛评估表明，该框架在不同离散化尺度下都能达到高精度。

Conclusion: 该方法有潜力弥合GPO可扩展性和保真度之间的差距，为复杂物理系统的不确定性感知建模提供基础。

Abstract: Operator learning offers a powerful paradigm for solving parametric partial
differential equations (PDEs), but scaling probabilistic neural operators such
as the recently proposed Gaussian Processes Operators (GPOs) to
high-dimensional, data-intensive regimes remains a significant challenge. In
this work, we introduce a novel, scalable GPO, which capitalizes on sparsity,
locality, and structural information through judicious kernel design.
Addressing the fundamental limitation of cubic computational complexity, our
method leverages nearest-neighbor-based local kernel approximations in the
spatial domain, sparse kernel approximation in the parameter space, and
structured Kronecker factorizations to enable tractable inference on
large-scale datasets and high-dimensional input. While local approximations
often introduce accuracy trade-offs due to limited kernel interactions, we
overcome this by embedding operator-aware kernel structures and employing
expressive, task-informed mean functions derived from neural operator
architectures. Through extensive evaluations on a broad class of nonlinear PDEs
- including Navier-Stokes, wave advection, Darcy flow, and Burgers' equations -
we demonstrate that our framework consistently achieves high accuracy across
varying discretization scales. These results underscore the potential of our
approach to bridge the gap between scalability and fidelity in GPO, offering a
compelling foundation for uncertainty-aware modeling in complex physical
systems.

</details>


### [286] [Diffusion-Based Hypothesis Testing and Change-Point Detection](https://arxiv.org/abs/2506.16089)
*Sean Moushegian,Taposh Banerjee,Vahid Tarokh*

Main category: stat.ML

TL;DR: 本文将基于分数的假设检验和变点检测停止规则扩展为基于扩散的类似规则，理论量化算法性能，提出优化权重矩阵的方法并进行数值模拟。


<details>
  <summary>Details</summary>
Motivation: 现有基于分数的假设检验和变点检测方法不如基于似然的方法强大，希望通过扩展方法提升性能。

Method: 将基于分数的Fisher散度推广为扩散散度，扩展假设检验和变点检测规则，理论量化算法性能，提出数值优化权重矩阵的方法。

Result: 理论上量化了扩散算法的性能，通过数值模拟展示了扩散算法的优势。

Conclusion: 基于扩散的算法在假设检验和变点检测中具有优势，提出的权重矩阵优化方法可行。

Abstract: Score-based methods have recently seen increasing popularity in modeling and
generation. Methods have been constructed to perform hypothesis testing and
change-point detection with score functions, but these methods are in general
not as powerful as their likelihood-based peers. Recent works consider
generalizing the score-based Fisher divergence into a diffusion-divergence by
transforming score functions via multiplication with a matrix-valued function
or a weight matrix. In this paper, we extend the score-based hypothesis test
and change-point detection stopping rule into their diffusion-based analogs.
Additionally, we theoretically quantify the performance of these
diffusion-based algorithms and study scenarios where optimal performance is
achievable. We propose a method of numerically optimizing the weight matrix and
present numerical simulations to illustrate the advantages of diffusion-based
algorithms.

</details>


### [287] [CP$^2$: Leveraging Geometry for Conformal Prediction via Canonicalization](https://arxiv.org/abs/2506.16189)
*Putri A. van der Linden,Alexander Timans,Erik J. Bekkers*

Main category: stat.ML

TL;DR: 研究几何数据偏移下的共形预测问题，提出将几何信息融入共形过程以解决该问题，评估表明该方法有效且适用范围广。


<details>
  <summary>Details</summary>
Motivation: 共形预测在分布偏移下实用性下降，尤其是几何数据偏移会降低模型性能，需要解决该问题以恢复其保证和鲁棒性。

Method: 将几何信息（如几何姿态）融入共形过程，并探索姿态规范化作为合适的信息提取器。

Result: 在离散和连续偏移下，与基于等变和增强的基线相比，将几何信息与共形预测结合是解决几何偏移的有效方法。

Conclusion: 将几何信息与共形预测结合是解决几何数据偏移问题的一种原则性方法，且对黑盒预测器具有广泛适用性。

Abstract: We study the problem of conformal prediction (CP) under geometric data
shifts, where data samples are susceptible to transformations such as rotations
or flips. While CP endows prediction models with post-hoc uncertainty
quantification and formal coverage guarantees, their practicality breaks under
distribution shifts that deteriorate model performance. To address this issue,
we propose integrating geometric information--such as geometric pose--into the
conformal procedure to reinstate its guarantees and ensure robustness under
geometric shifts. In particular, we explore recent advancements on pose
canonicalization as a suitable information extractor for this purpose.
Evaluating the combined approach across discrete and continuous shifts and
against equivariant and augmentation-based baselines, we find that integrating
geometric information with CP yields a principled way to address geometric
shifts while maintaining broad applicability to black-box predictors.

</details>


### [288] [Random feature approximation for general spectral methods](https://arxiv.org/abs/2506.16283)
*Mike Nguyen,Nicole Mücke*

Main category: stat.ML

TL;DR: 分析随机特征方法泛化性质，扩展正则化技术结果，实现神经网络和算子理论分析并获最优学习率。


<details>
  <summary>Details</summary>
Motivation: 分析随机特征方法在大规模学习算法中泛化性质，扩展之前Tikhonov正则化结果。

Method: 将之前Tikhonov正则化结果扩展到广泛的谱正则化技术类，通过Neural Tangent Kernel方法分析神经网络和算子。

Result: 获得了正则类上的最优学习率，改进或完善了相关特定核算法结果。

Conclusion: 所提框架可有效分析随机特征方法泛化性质，对相关领域理论分析有积极作用。

Abstract: Random feature approximation is arguably one of the most widely used
techniques for kernel methods in large-scale learning algorithms. In this work,
we analyze the generalization properties of random feature methods, extending
previous results for Tikhonov regularization to a broad class of spectral
regularization techniques. This includes not only explicit methods but also
implicit schemes such as gradient descent and accelerated algorithms like the
Heavy-Ball and Nesterov method. Through this framework, we enable a theoretical
analysis of neural networks and neural operators through the lens of the Neural
Tangent Kernel (NTK) approach trained via gradient descent. For our estimators
we obtain optimal learning rates over regularity classes (even for classes that
are not included in the reproducing kernel Hilbert space), which are defined
through appropriate source conditions. This improves or completes previous
results obtained in related settings for specific kernel algorithms.

</details>


### [289] [The Condition Number as a Scale-Invariant Proxy for Information Encoding in Neural Units](https://arxiv.org/abs/2506.16289)
*Oswaldo Ludwig*

Main category: stat.ML

TL;DR: 本文从信息论视角探究神经网络权重张量条件数与信息编码程度的关系，分析得出特定条件下高条件数对应信息转移减少，还将此原理用于多模态大语言模型选择性微调以缓解灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 探究神经网络权重张量条件数与信息编码程度的关系，缓解多模态大语言模型跨模态适应时的灾难性遗忘问题。

Method: 从信息论角度分析，将条件数和变换的对数体积缩放因子与输出熵特征和学习变换的几何性质联系起来，进行理论推导，并开展实际案例研究。

Result: 对于固定权重范数，奇异值集中分布（高条件数）对应整体信息转移减少；提出的选择性微调方法可绕过获取预训练统计信息的常见要求。

Conclusion: 高条件数可能意味着单元学会选择性放大和压缩信息，是一种专业且高效的编码策略；提出的选择性微调方法为缓解灾难性遗忘提供了新途径。

Abstract: This paper explores the relationship between the condition number of a neural
network's weight tensor and the extent of information encoded by the associated
processing unit, viewed through the lens of information theory. We argue that a
high condition number, though not sufficient for effective knowledge encoding,
may indicate that the unit has learned to selectively amplify and compress
information. We formalize this intuition, particularly for linear units with
Gaussian inputs, linking the condition number and the transformation's
log-volume scaling factor to the characteristics of the output entropy and the
geometric properties of the learned transformation. Our analysis demonstrates
that for a fixed weight norm, a concentrated distribution of singular values
(high condition number) corresponds to reduced overall information transfer,
indicating a specialized and efficient encoding strategy. Furthermore, we
present a practical case study where these principles are applied to guide
selective fine-tuning of a multimodal Large Language Model, aiming to mitigate
catastrophic forgetting during cross-modal adaptation. Unlike many existing
catastrophic forgetting mitigation methods that rely on access to pre-training
statistics, which are often unavailable, our selective fine-tuning approach
offers a way to bypass this common requirement.

</details>


### [290] [Identifying Heterogeneity in Distributed Learning](https://arxiv.org/abs/2506.16394)
*Zelin Xiao,Jia Gu,Song Xi Chen*

Main category: stat.ML

TL;DR: 研究分布式M - 估计中最小数据传输下识别异质参数分量的方法，提出重归一化Wald检验、极端对比检验及两者结合方法，并进行数值实验和案例研究。


<details>
  <summary>Details</summary>
Motivation: 在分布式M - 估计中，以最小数据传输识别异质参数分量。

Method: 提出重归一化Wald检验、极端对比检验，通过样本分割程序避免M - 估计偏差积累，结合两种检验方法。

Result: 重归一化Wald检验在一定条件下具有一致性；极端对比检验在异质性稀疏时有效且操作简单、通信高效；结合方法在不同稀疏水平下有更稳健功效。还进行了数值实验和案例研究。

Conclusion: 所提方法能有效识别分布式M - 估计中的异质参数分量。

Abstract: We study methods for identifying heterogeneous parameter components in
distributed M-estimation with minimal data transmission. One is based on a
re-normalized Wald test, which is shown to be consistent as long as the number
of distributed data blocks $K$ is of a smaller order of the minimum block
sample size {and the level of heterogeneity is dense}. The second one is an
extreme contrast test (ECT) based on the difference between the largest and
smallest component-wise estimated parameters among data blocks. By introducing
a sample splitting procedure, the ECT can avoid the bias accumulation arising
from the M-estimation procedures, and exhibits consistency for $K$ being much
larger than the sample size while the heterogeneity is sparse. The ECT
procedure is easy to operate and communication-efficient. A combination of the
Wald and the extreme contrast tests is formulated to attain more robust power
under varying levels of sparsity of the heterogeneity. We also conduct
intensive numerical experiments to compare the family-wise error rate (FWER)
and the power of the proposed methods. Additionally, we conduct a case study to
present the implementation and validity of the proposed methods.

</details>


### [291] [On Continuous Monitoring of Risk Violations under Unknown Shift](https://arxiv.org/abs/2506.16416)
*Alexander Timans,Rajeev Verma,Eric Nalisnick,Christian A. Naesseth*

Main category: stat.ML

TL;DR: 提出实时监控演化数据流中风险违规的通用框架，用顺序假设检验程序检测模型决策风险违规并控制误报率，通过实例证明有效性。


<details>
  <summary>Details</summary>
Motivation: 现实中机器学习系统面临动态和不可预测分布偏移，现有风险控制框架依赖固定假设且缺乏持续监控机制，需解决风险保证有效性问题。

Method: 利用“testing by betting”范式，提出顺序假设检验程序，在对分布偏移最小假设下检测模型决策风险违规并控制误报率。

Result: 通过监控异常检测和集合预测在多种偏移下的风险，证明了方法的有效性。

Conclusion: 提出的实时监控风险违规的通用框架在较少假设下广泛适用，能有效检测模型决策风险违规并控制误报率。

Abstract: Machine learning systems deployed in the real world must operate under
dynamic and often unpredictable distribution shifts. This challenges the
validity of statistical safety assurances on the system's risk established
beforehand. Common risk control frameworks rely on fixed assumptions and lack
mechanisms to continuously monitor deployment reliability. In this work, we
propose a general framework for the real-time monitoring of risk violations in
evolving data streams. Leveraging the 'testing by betting' paradigm, we propose
a sequential hypothesis testing procedure to detect violations of bounded risks
associated with the model's decision-making mechanism, while ensuring control
on the false alarm rate. Our method operates under minimal assumptions on the
nature of encountered shifts, rendering it broadly applicable. We illustrate
the effectiveness of our approach by monitoring risks in outlier detection and
set prediction under a variety of shifts.

</details>


### [292] [Latent Noise Injection for Private and Statistically Aligned Synthetic Data Generation](https://arxiv.org/abs/2506.16636)
*Rex Shen,Lu Tian*

Main category: stat.ML

TL;DR: 提出使用MAF的潜在噪声注入方法生成合成数据，解决高维收敛慢问题，满足差分隐私，元分析可恢复效率，能抵御攻击。


<details>
  <summary>Details</summary>
Motivation: 标准生成模型在高维环境收敛慢，需新方法克服此局限。

Method: 提出使用MAF的潜在噪声注入方法，在潜在空间扰动数据点并映射回数据域，引入扰动参数控制隐私-效用权衡，在元分析框架聚合研究。

Result: 方法满足局部差分隐私，聚合研究能恢复经典效率，实现与原始数据强统计对齐，抵御成员推理攻击。

Conclusion: 该方法是分散和隐私敏感领域合成数据共享的有力替代方案。

Abstract: Synthetic Data Generation has become essential for scalable,
privacy-preserving statistical analysis. While standard approaches based on
generative models, such as Normalizing Flows, have been widely used, they often
suffer from slow convergence in high-dimensional settings, frequently
converging more slowly than the canonical $1/\sqrt{n}$ rate when approximating
the true data distribution.
  To overcome these limitations, we propose a Latent Noise Injection method
using Masked Autoregressive Flows (MAF). Instead of directly sampling from the
trained model, our method perturbs each data point in the latent space and maps
it back to the data domain. This construction preserves a one to one
correspondence between observed and synthetic data, enabling synthetic outputs
that closely reflect the underlying distribution, particularly in challenging
high-dimensional regimes where traditional sampling struggles.
  Our procedure satisfies local $(\epsilon, \delta)$-differential privacy and
introduces a single perturbation parameter to control the privacy-utility
trade-off. Although estimators based on individual synthetic datasets may
converge slowly, we show both theoretically and empirically that aggregating
across $K$ studies in a meta analysis framework restores classical efficiency
and yields consistent, reliable inference. We demonstrate that with a
well-calibrated perturbation parameter, Latent Noise Injection achieves strong
statistical alignment with the original data and robustness against membership
inference attacks. These results position our method as a compelling
alternative to conventional flow-based sampling for synthetic data sharing in
decentralized and privacy-sensitive domains, such as biomedical research.

</details>


### [293] [Schrödinger Bridge Matching for Tree-Structured Costs and Entropic Wasserstein Barycentres](https://arxiv.org/abs/2506.17197)
*Samuel Howard,Peter Potaptchik,George Deligiannidis*

Main category: stat.ML

TL;DR: 本文将迭代马尔可夫拟合（IMF）程序扩展到解决树结构薛定谔桥（SB）问题，在树结构设置中继承了IMF相对于迭代比例拟合（IPF）方法的优势，在Wasserstein重心计算上有拓展。


<details>
  <summary>Details</summary>
Motivation: 现有基于流的生成建模方法可计算分布间的SB，IMF程序解决SB问题有优势，且最优传输可推广到多边缘情况，尤其是树结构成本，故希望将IMF程序扩展到解决树结构SB问题。

Method: 将迭代马尔可夫拟合（IMF）程序进行扩展，用于解决树结构薛定谔桥（SB）问题。

Result: 得到的算法在树结构设置中继承了IMF相对于IPF方法的许多优势。

Conclusion: 所提出的方法可将重心计算的定点方法扩展到基于流的熵最优传输求解器的情况。

Abstract: Recent advances in flow-based generative modelling have provided scalable
methods for computing the Schr\"odinger Bridge (SB) between distributions, a
dynamic form of entropy-regularised Optimal Transport (OT) for the quadratic
cost. The successful Iterative Markovian Fitting (IMF) procedure solves the SB
problem via sequential bridge-matching steps, presenting an elegant and
practical approach with many favourable properties over the more traditional
Iterative Proportional Fitting (IPF) procedure. Beyond the standard setting,
optimal transport can be generalised to the multi-marginal case in which the
objective is to minimise a cost defined over several marginal distributions. Of
particular importance are costs defined over a tree structure, from which
Wasserstein barycentres can be recovered as a special case. In this work, we
extend the IMF procedure to solve for the tree-structured SB problem. Our
resulting algorithm inherits the many advantages of IMF over IPF approaches in
the tree-based setting. In the specific case of Wasserstein barycentres, our
approach can be viewed as extending fixed-point approaches for barycentre
computation to the case of flow-based entropic OT solvers.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [294] [Understanding uncertainty in Bayesian cluster analysis](https://arxiv.org/abs/2506.16295)
*Cecilia Balocchi,Sara Wade*

Main category: stat.CO

TL;DR: 提出WASABI方法，用多个聚类估计总结贝叶斯聚类后验样本，助于理解不确定性。


<details>
  <summary>Details</summary>
Motivation: 现有方法用单一聚类估计代表后验分布会忽略不确定性，在多峰后验情况下可能无代表性，需增强对不确定性的理解。

Method: 提出WASABI方法，在Wasserstein距离意义下近似后验分布，用类似k - medoids算法在分区空间划分后验样本。

Result: 在合成和真实数据集上，该方法有助于提高对不确定性的理解，尤其在数据簇分离不佳或模型指定错误时。

Conclusion: WASABI方法能有效改善对贝叶斯聚类中不确定性的理解。

Abstract: The Bayesian approach to clustering is often appreciated for its ability to
provide uncertainty in the partition structure. However, summarizing the
posterior distribution over the clustering structure can be challenging, due
the discrete, unordered nature and massive dimension of the space. While recent
advancements provide a single clustering estimate to represent the posterior,
this ignores uncertainty and may even be unrepresentative in instances where
the posterior is multimodal. To enhance our understanding of uncertainty, we
propose a WASserstein Approximation for Bayesian clusterIng (WASABI), which
summarizes the posterior samples with not one, but multiple clustering
estimates, each corresponding to a different part of the space of partitions
that receives substantial posterior mass. Specifically, we find such clustering
estimates by approximating the posterior distribution in a Wasserstein distance
sense, equipped with a suitable metric on the partition space. An interesting
byproduct is that a locally optimal solution to this problem can be found using
a k-medoids-like algorithm on the partition space to divide the posterior
samples into different groups, each represented by one of the clustering
estimates. Using both synthetic and real datasets, we show that our proposal
helps to improve the understanding of uncertainty, particularly when the data
clusters are not well separated or when the employed model is misspecified.

</details>


### [295] [Quasi-Monte Carlo with one categorical variable](https://arxiv.org/abs/2506.16582)
*Valerie N. P. Ho,Art B. Owen,Zexin Pan*

Main category: stat.CO

TL;DR: 研究多元积分中一变量取值有限时的RQMC估计，发现应过采样最小混合分量，且Sobol'点应按2的幂拆分。


<details>
  <summary>Details</summary>
Motivation: 解决变量来自混合分布的多元积分问题，该问题在重要性抽样和传输映射工作中常见。

Method: 研究随机拟蒙特卡罗（RQMC）估计方法。

Result: 积分误差以RQMC速率下降时，过采样最小混合分量有益；最精确的RQMC采样方法中，将$n = 2^m$个随机Sobol'点拆分为2的幂次的子样本量有优势。

Conclusion: 确定了在特定多元积分问题中RQMC估计的有效策略。

Abstract: We study randomized quasi-Monte Carlo (RQMC) estimation of a multivariate
integral where one of the variables takes only a finite number of values. This
problem arises when the variable of integration is drawn from a mixture
distribution as is common in importance sampling and also arises in some recent
work on transport maps. We find that when integration error decreases at an
RQMC rate that it is then beneficial to oversample the smallest mixture
components instead of using a proportional allocation. We also find that for
the most accurate RQMC sampling methods, it is advantageous to arrange that our
$n=2^m$ randomized Sobol' points split into subsample sizes that are also
powers of~$2$.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [296] [Uncertainty Quantification for Linear Inverse Problems with Besov Prior: A Randomize-Then-Optimize Method](https://arxiv.org/abs/2506.16888)
*Andreas Horst,Babak Maboudi Afkham,Yiqiu Dong,Jakob Lemvig*

Main category: math.NA

TL;DR: 研究贝索夫先验在贝叶斯逆问题中的应用，提出随机 - 优化方法抽样后验分布并评估其模式，经数值实验验证方法有效。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯逆问题的解是后验分布，可解释不确定性，贝索夫先验具有离散不变性和促进小波系数稀疏性，需合适方法抽样。

Method: 提出随机 - 优化方法，从带贝索夫先验的后验分布中抽样并估计后验分布的模式。

Result: 通过一维修复、一维反卷积和二维计算机断层扫描问题的数值实验，对比了不同贝索夫参数和小波基的影响，与现有方法进行比较。

Conclusion: 所提方法是抽样带一般贝索夫先验后验分布的有效工具。

Abstract: In this work, we investigate the use of Besov priors in the context of
Bayesian inverse problems. The solution to Bayesian inverse problems is the
posterior distribution which naturally enables us to interpret the
uncertainties. Besov priors are discretization invariant and can promote
sparsity in terms of wavelet coefficients. We propose the
randomize-then-optimize method to draw samples from the posterior distribution
with Besov priors under a general parameter setting and estimate the modes of
the posterior distribution. The performance of the proposed method is studied
through numerical experiments of a 1D inpainting problem, a 1D deconvolution
problem, and a 2D computed tomography problem. Further, we discuss the
influence of the choice of the Besov parameters and the wavelet basis in
detail, and we compare the proposed method with the state-of-the-art methods.
The numerical results suggest that the proposed method is an effective tool for
sampling the posterior distribution equipped with general Besov priors.

</details>


### [297] [Comparison of substructured non-overlapping domain decomposition and overlapping additive Schwarz methods for large-scale Helmholtz problems with multiple sources](https://arxiv.org/abs/2506.16875)
*Boris Martin,Pierre Jolivet,Christophe Geuzaine*

Main category: math.NA

TL;DR: 本文比较非重叠子结构DDM和ORAS预条件器求解大规模含多源Helmholtz问题的计算性能，在实际地球物理算例中表明非重叠方法可显著优于重叠方法。


<details>
  <summary>Details</summary>
Motivation: 大规模高阶有限元离散的Helmholtz问题求解困难，尤其是3D情况，领域分解方法是有前景的策略，本文旨在比较不同预条件器性能。

Method: 比较非重叠子结构DDM和ORAS预条件器求解大规模含多源Helmholtz问题。

Result: 在实际地球物理算例中，非重叠方法适当调参后可缩小收敛差距，显著优于重叠方法。

Conclusion: 非重叠子结构DDM和ORAS预条件器在求解大规模含多源Helmholtz问题时有较好性能，非重叠方法表现更佳。

Abstract: Solving large-scale Helmholtz problems discretized with high-order finite
elements is notoriously difficult, especially in 3D where direct factorization
of the system matrix is very expensive and memory demanding, and robust
convergence of iterative methods is difficult to obtain. Domain decomposition
methods (DDM) constitute one of the most promising strategy so far, by
combining direct and iterative approaches: using direct solvers on overlapping
or non-overlapping subdomains, as a preconditioner for a Krylov subspace method
on the original Helmholtz system or as an iterative solver on a substructured
problem involving field values or Lagrange multipliers on the interfaces
between the subdomains. In this work we compare the computational performance
of non-overlapping substructured DDM and Optimized Restricted Additive Schwarz
(ORAS) preconditioners for solving large-scale Helmholtz problems with multiple
sources, as is encountered, e.g., in frequency-domain Full Waveform Inversion.
We show on a realistic geophysical test-case that, when appropriately tuned,
the non-overlapping methods can reduce the convergence gap sufficiently to
significantly outperform the overlapping methods.

</details>


### [298] [Convergent Methods for Koopman Operators on Reproducing Kernel Hilbert Spaces](https://arxiv.org/abs/2506.15782)
*Nicolas Boullé,Matthew J. Colbrook,Gustav Conradie*

Main category: math.NA

TL;DR: 提出在再生核希尔伯特空间（RKHS）上计算Koopman和Perron - Frobenius算子谱特性的通用、可证明收敛的数据驱动算法，证明其最优性并在高维数据集验证效果。


<details>
  <summary>Details</summary>
Motivation: 在合适的RKHS上定义Koopman算子有诸多实际优势，现有研究存在局限，需开发相关算法。

Method: 引入在RKHS上计算谱特性的算法，利用Solvability Complexity Index层次构造对抗动力系统证明算法最优性。

Result: 算法能有效计算谱、伪谱、谱测度，在多个高维数据集上展示了有效性，且算法公开。

Conclusion: 所提出的算法是最优的，能在高维数据集上有效运行，有实际应用价值。

Abstract: Data-driven spectral analysis of Koopman operators is a powerful tool for
understanding numerous real-world dynamical systems, from neuronal activity to
variations in sea surface temperature. The Koopman operator acts on a function
space and is most commonly studied on the space of square-integrable functions.
However, defining it on a suitable reproducing kernel Hilbert space (RKHS)
offers numerous practical advantages, including pointwise predictions with
error bounds, improved spectral properties that facilitate computations, and
more efficient algorithms, particularly in high dimensions. We introduce the
first general, provably convergent, data-driven algorithms for computing
spectral properties of Koopman and Perron--Frobenius operators on RKHSs. These
methods efficiently compute spectra and pseudospectra with error control and
spectral measures while exploiting the RKHS structure to avoid the large-data
limits required in the $L^2$ settings. The function space is determined by a
user-specified kernel, eliminating the need for quadrature-based sampling as in
$L^2$ and enabling greater flexibility with finite, externally provided
datasets. Using the Solvability Complexity Index hierarchy, we construct
adversarial dynamical systems for these problems to show that no algorithm can
succeed in fewer limits, thereby proving the optimality of our algorithms.
Notably, this impossibility extends to randomized algorithms and datasets. We
demonstrate the effectiveness of our algorithms on challenging,
high-dimensional datasets arising from real-world measurements and
high-fidelity numerical simulations, including turbulent channel flow,
molecular dynamics of a binding protein, Antarctic sea ice concentration, and
Northern Hemisphere sea surface height. The algorithms are publicly available
in the software package $\texttt{SpecRKHS}$.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [299] [RapFlow-TTS: Rapid and High-Fidelity Text-to-Speech with Improved Consistency Flow Matching](https://arxiv.org/abs/2506.16741)
*Hyun Joon Park,Jeongmin Liu,Jin Sob Kim,Jeong Yeol Yang,Sung Won Han,Eunwoo Song*

Main category: eess.AS

TL;DR: 介绍RapFlow - TTS，利用速度一致性约束解决TTS质量与推理速度权衡问题，实验显示合成步骤大幅减少。


<details>
  <summary>Details</summary>
Motivation: 解决基于常微分方程（ODE）的TTS生成中质量与推理速度的权衡问题，普通方法生成步骤多。

Method: RapFlow - TTS在FM拉直的ODE轨迹上强制速度场的一致性，还引入时间间隔调度和对抗学习技术。

Result: RapFlow - TTS实现高保真语音合成，比传统FM和基于分数的方法分别减少5倍和10倍的合成步骤。

Conclusion: RapFlow - TTS能在减少合成步骤的同时保证高保真语音合成质量。

Abstract: We introduce RapFlow-TTS, a rapid and high-fidelity TTS acoustic model that
leverages velocity consistency constraints in flow matching (FM) training.
Although ordinary differential equation (ODE)-based TTS generation achieves
natural-quality speech, it typically requires a large number of generation
steps, resulting in a trade-off between quality and inference speed. To address
this challenge, RapFlow-TTS enforces consistency in the velocity field along
the FM-straightened ODE trajectory, enabling consistent synthetic quality with
fewer generation steps. Additionally, we introduce techniques such as time
interval scheduling and adversarial learning to further enhance the quality of
the few-step synthesis. Experimental results show that RapFlow-TTS achieves
high-fidelity speech synthesis with a 5- and 10-fold reduction in synthesis
steps than the conventional FM- and score-based approaches, respectively.

</details>


<div id='cs.SY'></div>

# cs.SY [[Back]](#toc)

### [300] [Formal Control for Uncertain Systems via Contract-Based Probabilistic Surrogates (Extended Version)](https://arxiv.org/abs/2506.16971)
*Oliver Schön,Sofie Haesaert,Sadegh Soudjani*

Main category: cs.SY

TL;DR: 提出一种方法提升概率模拟关系的可扩展性与实用性，通过抽象技术处理高维复杂问题，在车辆交叉口案例中验证。


<details>
  <summary>Details</summary>
Motivation: 准确识别系统表示有挑战且影响形式化方法的可扩展性，现有模型复杂不利于有效决策。

Method: 聚焦概率模拟关系和随机系统代理模型，提出无需直接计算误差界限的方法，采用基于抽象的技术。

Result: 能有效扩展到高维，处理复杂非线性的智能体 - 环境交互，有无限时域时态逻辑保证。

Conclusion: 该方法在可扩展性和保守性之间取得了良好平衡，在车辆交叉口案例中得到验证。

Abstract: The requirement for identifying accurate system representations has not only
been a challenge to fulfill, but it has compromised the scalability of formal
methods, as the resulting models are often too complex for effective decision
making with formal correctness and performance guarantees. Focusing on
probabilistic simulation relations and surrogate models of stochastic systems,
we propose an approach that significantly enhances the scalability and
practical applicability of such simulation relations by eliminating the need to
compute error bounds directly. As a result, we provide an abstraction-based
technique that scales effectively to higher dimensions while addressing complex
nonlinear agent-environment interactions with infinite-horizon temporal logic
guarantees amidst uncertainty. Our approach trades scalability for conservatism
favorably, as demonstrated on a complex high-dimensional vehicle intersection
case study.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [301] [CodeDiffuser: Attention-Enhanced Diffusion Policy via VLM-Generated Code for Instruction Ambiguity](https://arxiv.org/abs/2506.16652)
*Guang Yin,Yitong Li,Yixuan Wang,Dale McConachie,Paarth Shah,Kunimatsu Hashimoto,Huan Zhang,Katherine Liu,Yunzhu Li*

Main category: cs.RO

TL;DR: 现有语言条件策略处理含歧义自然语言指令表现不佳，本文提出新框架用VLM解释指令生成代码，结合感知模块解决歧义，实验证明该方法在多种任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 自然语言指令有歧义模糊性，现有语言条件策略缺乏模块化和可解释性，性能欠佳。

Method: 引入新框架，用VLM解释自然语言指令抽象概念并生成可执行代码，代码与感知模块结合生成3D注意力图解决指令歧义。

Result: 实验识别出当前模仿学习方法的关键局限，该方法在含语言歧义、接触丰富操作和多对象交互等任务中表现优秀。

Conclusion: 所提出的新框架能有效处理含歧义自然语言指定的操作任务。

Abstract: Natural language instructions for robotic manipulation tasks often exhibit
ambiguity and vagueness. For instance, the instruction "Hang a mug on the mug
tree" may involve multiple valid actions if there are several mugs and branches
to choose from. Existing language-conditioned policies typically rely on
end-to-end models that jointly handle high-level semantic understanding and
low-level action generation, which can result in suboptimal performance due to
their lack of modularity and interpretability. To address these challenges, we
introduce a novel robotic manipulation framework that can accomplish tasks
specified by potentially ambiguous natural language. This framework employs a
Vision-Language Model (VLM) to interpret abstract concepts in natural language
instructions and generates task-specific code - an interpretable and executable
intermediate representation. The generated code interfaces with the perception
module to produce 3D attention maps that highlight task-relevant regions by
integrating spatial and semantic information, effectively resolving ambiguities
in instructions. Through extensive experiments, we identify key limitations of
current imitation learning methods, such as poor adaptation to language and
environmental variations. We show that our approach excels across challenging
manipulation tasks involving language ambiguity, contact-rich manipulation, and
multi-object interactions.

</details>


### [302] [Context Matters! Relaxing Goals with LLMs for Feasible 3D Scene Planning](https://arxiv.org/abs/2506.15828)
*Emanuele Musumeci,Michele Brienza,Francesco Argenziano,Vincenzo Suriani,Daniele Nardi,Domenico D. Bloisi*

Main category: cs.RO

TL;DR: 提出一种结合经典规划与大语言模型的方法，让机器人可处理不可行任务，经评估能有效执行任务，代码等资料已开源。


<details>
  <summary>Details</summary>
Motivation: 经典规划方法在现实场景易因感知局限等失败，大语言模型生成的规划可能不可行或不安全，需解决这些局限。

Method: 提出结合经典规划与大语言模型的方法，采用分层公式，通过逐步放宽定义功能等效目标使不可行任务变得可行。

Result: 通过定性和定量评估，该方法能在3D场景图建模的环境中有效适应和执行任务，在复杂场景中比其他基准方法更易成功。

Conclusion: 该集成方法有效，能解决经典规划和大语言模型单独使用时的问题，可用于机器人处理复杂任务。

Abstract: Classical planning in AI and Robotics addresses complex tasks by shifting
from imperative to declarative approaches (e.g., PDDL). However, these methods
often fail in real scenarios due to limited robot perception and the need to
ground perceptions to planning predicates. This often results in heavily
hard-coded behaviors that struggle to adapt, even with scenarios where goals
can be achieved through relaxed planning. Meanwhile, Large Language Models
(LLMs) lead to planning systems that leverage commonsense reasoning but often
at the cost of generating unfeasible and/or unsafe plans. To address these
limitations, we present an approach integrating classical planning with LLMs,
leveraging their ability to extract commonsense knowledge and ground actions.
We propose a hierarchical formulation that enables robots to make unfeasible
tasks tractable by defining functionally equivalent goals through gradual
relaxation. This mechanism supports partial achievement of the intended
objective, suited to the agent's specific context. Our method demonstrates its
ability to adapt and execute tasks effectively within environments modeled
using 3D Scene Graphs through comprehensive qualitative and quantitative
evaluations. We also show how this method succeeds in complex scenarios where
other benchmark methods are more likely to fail. Code, dataset, and additional
material are released to the community.

</details>


### [303] [SafeMimic: Towards Safe and Autonomous Human-to-Robot Imitation for Mobile Manipulation](https://arxiv.org/abs/2506.15847)
*Arpit Bahety,Arnav Balaji,Ben Abbatematteo,Roberto Martín-Martín*

Main category: cs.RO

TL;DR: 提出SafeMimic框架，让机器人从单个人类视频中安全自主学习移动操作技能，实验显示效果优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 让机器人成为家庭高效助手，需学会通过观看人类操作学习新的移动操作任务，且学习过程应安全自主，减少对人类监控的依赖。

Method: SafeMimic框架先将视频分段，推断语义变化和人类动作并转换为自我中心参考，围绕人类动作采样候选动作，用安全Q函数验证安全性，无法前进时回溯尝试不同动作序列。

Result: SafeMimic产生成功的策略，学习特定任务动作减少未来探索，实验表明方法能让机器人在不同用户和环境中安全高效学习多步移动操作行为。

Conclusion: SafeMimic方法有效，在七个任务上优于现有基线。

Abstract: For robots to become efficient helpers in the home, they must learn to
perform new mobile manipulation tasks simply by watching humans perform them.
Learning from a single video demonstration from a human is challenging as the
robot needs to first extract from the demo what needs to be done and how,
translate the strategy from a third to a first-person perspective, and then
adapt it to be successful with its own morphology. Furthermore, to mitigate the
dependency on costly human monitoring, this learning process should be
performed in a safe and autonomous manner. We present SafeMimic, a framework to
learn new mobile manipulation skills safely and autonomously from a single
third-person human video. Given an initial human video demonstration of a
multi-step mobile manipulation task, SafeMimic first parses the video into
segments, inferring both the semantic changes caused and the motions the human
executed to achieve them and translating them to an egocentric reference. Then,
it adapts the behavior to the robot's own morphology by sampling candidate
actions around the human ones, and verifying them for safety before execution
in a receding horizon fashion using an ensemble of safety Q-functions trained
in simulation. When safe forward progression is not possible, SafeMimic
backtracks to previous states and attempts a different sequence of actions,
adapting both the trajectory and the grasping modes when required for its
morphology. As a result, SafeMimic yields a strategy that succeeds in the
demonstrated behavior and learns task-specific actions that reduce exploration
in future attempts. Our experiments show that our method allows robots to
safely and efficiently learn multi-step mobile manipulation behaviors from a
single human demonstration, from different users, and in different
environments, with improvements over state-of-the-art baselines across seven
tasks

</details>


### [304] [CapsDT: Diffusion-Transformer for Capsule Robot Manipulation](https://arxiv.org/abs/2506.16263)
*Xiting He,Mingwu Su,Xinqi Jiang,Long Bai,Jiewen Lai,Hongliang Ren*

Main category: cs.RO

TL;DR: 提出CapsDT模型用于胶囊机器人操作，在多种内窥镜任务中表现良好，真实模拟操作成功率达26.25%


<details>
  <summary>Details</summary>
Motivation: VLA模型在胶囊内窥镜机器人应用未被探索，将其集成可改善人机交互和医疗效果

Method: 设计Diffusion Transformer模型CapsDT处理视觉和文本输入以生成控制信号，开发胶囊内窥镜机器人系统并创建数据集

Result: CapsDT在各内窥镜任务中表现出色，在真实模拟操作中成功率达26.25%

Conclusion: CapsDT可作为强大的视觉语言通用模型，在胶囊内窥镜机器人操作中达到了先进水平

Abstract: Vision-Language-Action (VLA) models have emerged as a prominent research
area, showcasing significant potential across a variety of applications.
However, their performance in endoscopy robotics, particularly endoscopy
capsule robots that perform actions within the digestive system, remains
unexplored. The integration of VLA models into endoscopy robots allows more
intuitive and efficient interactions between human operators and medical
devices, improving both diagnostic accuracy and treatment outcomes. In this
work, we design CapsDT, a Diffusion Transformer model for capsule robot
manipulation in the stomach. By processing interleaved visual inputs, and
textual instructions, CapsDT can infer corresponding robotic control signals to
facilitate endoscopy tasks. In addition, we developed a capsule endoscopy robot
system, a capsule robot controlled by a robotic arm-held magnet, addressing
different levels of four endoscopy tasks and creating corresponding capsule
robot datasets within the stomach simulator. Comprehensive evaluations on
various robotic tasks indicate that CapsDT can serve as a robust
vision-language generalist, achieving state-of-the-art performance in various
levels of endoscopy tasks while achieving a 26.25% success rate in real-world
simulation manipulation.

</details>


### [305] [Human2LocoMan: Learning Versatile Quadrupedal Manipulation with Human Pretraining](https://arxiv.org/abs/2506.16475)
*Yaru Niu,Yunzhe Zhang,Mingyang Yu,Changyi Lin,Chenhao Li,Yikai Wang,Yuxiang Yang,Wenhao Yu,Tingnan Zhang,Bingqing Chen,Jonathan Francis,Zhenzhen Li,Jie Tan,Ding Zhao*

Main category: cs.RO

TL;DR: 本文提出用于四足机器人操作的跨实体模仿学习系统，构建数据集，在六个任务上验证，提升成功率，开源代码、硬件和数据。


<details>
  <summary>Details</summary>
Motivation: 四足机器人虽有出色运动能力，但以可扩展方式赋予自主通用操作技能仍是重大挑战。

Method: 开发遥操作和数据收集管道统一观测与动作空间；提出高效模块化架构支持跨实体数据训练；构建LocoMan机器人操作数据集及对应人类数据集。

Result: 在六个真实操作任务上，系统整体成功率提升41.9%，分布外设置下提升79.7%；使用人类数据预训练整体成功率提升38.6%，分布外设置下提升82.7%，用一半机器人数据有更好表现。

Conclusion: 所提出的跨实体模仿学习系统有效提升四足机器人操作能力，人类数据预训练可降低对机器人数据依赖。

Abstract: Quadrupedal robots have demonstrated impressive locomotion capabilities in
complex environments, but equipping them with autonomous versatile manipulation
skills in a scalable way remains a significant challenge. In this work, we
introduce a cross-embodiment imitation learning system for quadrupedal
manipulation, leveraging data collected from both humans and LocoMan, a
quadruped equipped with multiple manipulation modes. Specifically, we develop a
teleoperation and data collection pipeline, which unifies and modularizes the
observation and action spaces of the human and the robot. To effectively
leverage the collected data, we propose an efficient modularized architecture
that supports co-training and pretraining on structured modality-aligned data
across different embodiments. Additionally, we construct the first manipulation
dataset for the LocoMan robot, covering various household tasks in both
unimanual and bimanual modes, supplemented by a corresponding human dataset. We
validate our system on six real-world manipulation tasks, where it achieves an
average success rate improvement of 41.9% overall and 79.7% under
out-of-distribution (OOD) settings compared to the baseline. Pretraining with
human data contributes a 38.6% success rate improvement overall and 82.7% under
OOD settings, enabling consistently better performance with only half the
amount of robot data. Our code, hardware, and data are open-sourced at:
https://human2bots.github.io.

</details>


### [306] [Grounding Language Models with Semantic Digital Twins for Robotic Planning](https://arxiv.org/abs/2506.16493)
*Mehreen Naeem,Andrew Melnik,Michael Beetz*

Main category: cs.RO

TL;DR: 提出集成语义数字孪生与大语言模型的框架，用于动态环境中机器人任务执行，在ALFRED基准测试中表现良好。


<details>
  <summary>Details</summary>
Motivation: 实现动态环境中自适应、目标驱动的机器人任务执行。

Method: 将自然语言指令分解为结构化动作三元组，结合语义数字孪生的环境数据进行语义锚定，执行失败时大语言模型利用反馈生成恢复策略。

Result: 在ALFRED基准测试的各种家庭场景任务中表现稳健。

Conclusion: 该框架有效结合高级推理与语义环境理解，能在不确定和失败情况下可靠完成任务。

Abstract: We introduce a novel framework that integrates Semantic Digital Twins (SDTs)
with Large Language Models (LLMs) to enable adaptive and goal-driven robotic
task execution in dynamic environments. The system decomposes natural language
instructions into structured action triplets, which are grounded in contextual
environmental data provided by the SDT. This semantic grounding allows the
robot to interpret object affordances and interaction rules, enabling action
planning and real-time adaptability. In case of execution failures, the LLM
utilizes error feedback and SDT insights to generate recovery strategies and
iteratively revise the action plan. We evaluate our approach using tasks from
the ALFRED benchmark, demonstrating robust performance across various household
scenarios. The proposed framework effectively combines high-level reasoning
with semantic environment understanding, achieving reliable task completion in
the face of uncertainty and failure.

</details>


### [307] [BIDA: A Bi-level Interaction Decision-making Algorithm for Autonomous Vehicles in Dynamic Traffic Scenarios](https://arxiv.org/abs/2506.16546)
*Liyang Yu,Tianyi Wang,Junfeng Jiao,Fengwu Shan,Hongqing Chu,Bingzhao Gao*

Main category: cs.RO

TL;DR: 本文设计了双级交互决策算法BIDA，结合交互式蒙特卡罗树搜索与深度强化学习，在CARLA中验证其能增强交互推理、降低计算成本，性能优于其他基准。


<details>
  <summary>Details</summary>
Motivation: 复杂交通环境中人类行为不可预测，给自动驾驶车辆实时安全决策带来挑战，需提升其在动态关键交通场景中的交互合理性、效率和安全性。

Method: 设计BIDA算法，采用三种深度强化学习算法构建价值网络和策略网络指导交互式蒙特卡罗树搜索，在CARLA中设计动态轨迹规划器和轨迹跟踪控制器。

Result: BIDA增强了交互推理、降低了计算成本，在不同交通条件下性能优于其他最新基准。

Conclusion: BIDA在动态关键交通场景中具有优越的安全性、效率和交互合理性。

Abstract: In complex real-world traffic environments, autonomous vehicles (AVs) need to
interact with other traffic participants while making real-time and
safety-critical decisions accordingly. The unpredictability of human behaviors
poses significant challenges, particularly in dynamic scenarios, such as
multi-lane highways and unsignalized T-intersections. To address this gap, we
design a bi-level interaction decision-making algorithm (BIDA) that integrates
interactive Monte Carlo tree search (MCTS) with deep reinforcement learning
(DRL), aiming to enhance interaction rationality, efficiency and safety of AVs
in dynamic key traffic scenarios. Specifically, we adopt three types of DRL
algorithms to construct a reliable value network and policy network, which
guide the online deduction process of interactive MCTS by assisting in value
update and node selection. Then, a dynamic trajectory planner and a trajectory
tracking controller are designed and implemented in CARLA to ensure smooth
execution of planned maneuvers. Experimental evaluations demonstrate that our
BIDA not only enhances interactive deduction and reduces computational costs,
but also outperforms other latest benchmarks, which exhibits superior safety,
efficiency and interaction rationality under varying traffic conditions.

</details>


### [308] [Reimagination with Test-time Observation Interventions: Distractor-Robust World Model Predictions for Visual Model Predictive Control](https://arxiv.org/abs/2506.16565)
*Yuxin Chen,Jianglan Wei,Chenfeng Xu,Boyi Li,Masayoshi Tomizuka,Andrea Bajcsy,Ran Tian*

Main category: cs.RO

TL;DR: 提出ReOI策略让世界模型在开放场景预测更可靠行动结果，实验显示其对多种视觉干扰因素鲁棒，能大幅提升任务成功率。


<details>
  <summary>Details</summary>
Motivation: 现有世界模型在遇到训练中少见的视觉干扰因素时很脆弱，会导致行动结果预测错误和下游任务失败。

Method: 提出ReOI策略，先检测视觉干扰因素，修改当前观察以移除干扰因素，再用修改后的观察预测未来结果，最后重新引入干扰因素保证视觉一致性。

Result: 在机器人操作任务的行动验证中，ReOI对分布内和分布外的视觉干扰因素都有鲁棒性，在有新干扰因素时任务成功率最多提升3倍。

Conclusion: ReOI显著优于无想象干预的基于世界模型预测的行动验证方法。

Abstract: World models enable robots to "imagine" future observations given current
observations and planned actions, and have been increasingly adopted as
generalized dynamics models to facilitate robot learning. Despite their
promise, these models remain brittle when encountering novel visual distractors
such as objects and background elements rarely seen during training.
Specifically, novel distractors can corrupt action outcome predictions, causing
downstream failures when robots rely on the world model imaginations for
planning or action verification. In this work, we propose Reimagination with
Observation Intervention (ReOI), a simple yet effective test-time strategy that
enables world models to predict more reliable action outcomes in open-world
scenarios where novel and unanticipated visual distractors are inevitable.
Given the current robot observation, ReOI first detects visual distractors by
identifying which elements of the scene degrade in physically implausible ways
during world model prediction. Then, it modifies the current observation to
remove these distractors and bring the observation closer to the training
distribution. Finally, ReOI "reimagines" future outcomes with the modified
observation and reintroduces the distractors post-hoc to preserve visual
consistency for downstream planning and verification. We validate our approach
on a suite of robotic manipulation tasks in the context of action verification,
where the verifier needs to select desired action plans based on predictions
from a world model. Our results show that ReOI is robust to both
in-distribution and out-of-distribution visual distractors. Notably, it
improves task success rates by up to 3x in the presence of novel distractors,
significantly outperforming action verification that relies on world model
predictions without imagination interventions.

</details>


### [309] [History-Augmented Vision-Language Models for Frontier-Based Zero-Shot Object Navigation](https://arxiv.org/abs/2506.16623)
*Mobin Habibpour,Fatemeh Afghah*

Main category: cs.RO

TL;DR: 本文提出零样本ObjectNav框架，用动态历史感知提示将VLM推理融入探索，经评估有不错效果。


<details>
  <summary>Details</summary>
Motivation: 现有ObjectNav方法对VLMs利用浮于表面，限制上下文理解，存在重复导航等问题。

Method: 引入新颖零样本ObjectNav框架，用动态历史感知提示将VLM推理融入前沿探索，提供动作历史上下文，生成语义引导分数，避免决策循环，还有VLM辅助路点生成机制。

Result: 在Habitat的HM3D数据集上评估，成功率达46%，路径长度加权成功率达24.8%，与现有零样本方法相当。

Conclusion: 历史增强的VLM提示策略对更强大、上下文感知的机器人导航有巨大潜力。

Abstract: Object Goal Navigation (ObjectNav) challenges robots to find objects in
unseen environments, demanding sophisticated reasoning. While Vision-Language
Models (VLMs) show potential, current ObjectNav methods often employ them
superficially, primarily using vision-language embeddings for object-scene
similarity checks rather than leveraging deeper reasoning. This limits
contextual understanding and leads to practical issues like repetitive
navigation behaviors. This paper introduces a novel zero-shot ObjectNav
framework that pioneers the use of dynamic, history-aware prompting to more
deeply integrate VLM reasoning into frontier-based exploration. Our core
innovation lies in providing the VLM with action history context, enabling it
to generate semantic guidance scores for navigation actions while actively
avoiding decision loops. We also introduce a VLM-assisted waypoint generation
mechanism for refining the final approach to detected objects. Evaluated on the
HM3D dataset within Habitat, our approach achieves a 46% Success Rate (SR) and
24.8% Success weighted by Path Length (SPL). These results are comparable to
state-of-the-art zero-shot methods, demonstrating the significant potential of
our history-augmented VLM prompting strategy for more robust and context-aware
robotic navigation.

</details>


### [310] [Learning Dexterous Object Handover](https://arxiv.org/abs/2506.16822)
*Daniel Frau-Alfaro,Julio Castaño-Amoros,Santiago Puente,Pablo Gil,Roberto Calandra*

Main category: cs.RO

TL;DR: 本文展示了强化学习用于两多手指手之间的灵巧物体交接，使用基于对偶四元数的奖励函数，经实验验证策略具有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 为使机器人能在协作场景（如家庭）中安全高效地进行物体交接。

Method: 使用强化学习进行物体交接，采用基于对偶四元数的新型奖励函数以最小化旋转距离。

Result: 经过100次实验，最佳情况下总成功率达94%；当另一机器人在交接时移动，策略最佳性能仅下降13.8%。

Conclusion: 训练的策略能成功完成物体交接任务，对新物体和常见扰动具有鲁棒性。

Abstract: Object handover is an important skill that we use daily when interacting with
other humans. To deploy robots in collaborative setting, like houses, being
able to receive and handing over objects safely and efficiently becomes a
crucial skill. In this work, we demonstrate the use of Reinforcement Learning
(RL) for dexterous object handover between two multi-finger hands. Key to this
task is the use of a novel reward function based on dual quaternions to
minimize the rotation distance, which outperforms other rotation
representations such as Euler and rotation matrices. The robustness of the
trained policy is experimentally evaluated by testing w.r.t. objects that are
not included in the training distribution, and perturbations during the
handover process. The results demonstrate that the trained policy successfully
perform this task, achieving a total success rate of 94% in the best-case
scenario after 100 experiments, thereby showing the robustness of our policy
with novel objects. In addition, the best-case performance of the policy
decreases by only 13.8% when the other robot moves during the handover, proving
that our policy is also robust to this type of perturbation, which is common in
real-world object handovers.

</details>


### [311] [Steering Your Diffusion Policy with Latent Space Reinforcement Learning](https://arxiv.org/abs/2506.15799)
*Andrew Wagenmaker,Mitsuhiko Nakamoto,Yunchu Zhang,Seohong Park,Waleed Yagoub,Anusha Nagabandi,Abhishek Gupta,Sergey Levine*

Main category: cs.RO

TL;DR: 提出DSRL方法，可实现BC训练策略的快速自主适应，在模拟和真实任务中展现出样本高效性和良好性能。


<details>
  <summary>Details</summary>
Motivation: 现有BC学习策略需额外收集人类演示数据来提升性能，成本高、耗时长，而RL需大量样本，难以实现自主在线策略改进。

Method: 提出扩散引导强化学习（DSRL），在BC策略的潜在噪声空间上运行RL来适应BC策略。

Result: DSRL样本高效，仅需黑盒访问BC策略，能有效实现现实世界中的自主策略改进，避免了微调扩散策略的诸多挑战。

Conclusion: DSRL在模拟基准、现实世界机器人任务和预训练通用策略适应中，展示了其样本高效性和在现实世界策略改进中的有效性能。

Abstract: Robotic control policies learned from human demonstrations have achieved
impressive results in many real-world applications. However, in scenarios where
initial performance is not satisfactory, as is often the case in novel
open-world settings, such behavioral cloning (BC)-learned policies typically
require collecting additional human demonstrations to further improve their
behavior -- an expensive and time-consuming process. In contrast, reinforcement
learning (RL) holds the promise of enabling autonomous online policy
improvement, but often falls short of achieving this due to the large number of
samples it typically requires. In this work we take steps towards enabling fast
autonomous adaptation of BC-trained policies via efficient real-world RL.
Focusing in particular on diffusion policies -- a state-of-the-art BC
methodology -- we propose diffusion steering via reinforcement learning (DSRL):
adapting the BC policy by running RL over its latent-noise space. We show that
DSRL is highly sample efficient, requires only black-box access to the BC
policy, and enables effective real-world autonomous policy improvement.
Furthermore, DSRL avoids many of the challenges associated with finetuning
diffusion policies, obviating the need to modify the weights of the base policy
at all. We demonstrate DSRL on simulated benchmarks, real-world robotic tasks,
and for adapting pretrained generalist policies, illustrating its sample
efficiency and effective performance at real-world policy improvement.

</details>


### [312] [Investigating Lagrangian Neural Networks for Infinite Horizon Planning in Quadrupedal Locomotion](https://arxiv.org/abs/2506.16079)
*Prakrut Kotecha,Aditya Shirwatkar,Shishir Kolathaya*

Main category: cs.RO

TL;DR: 本文评估LNNs在四足机器人无限时域规划中的应用，通过四种动力学模型实验，显示LNNs在样本效率和预测精度上优于基线方法，有实时控制潜力。


<details>
  <summary>Details</summary>
Motivation: 传统动力学模型长时域误差累积，LNNs能保留物理定律，实现准确稳定预测，评估其在四足机器人无限时域规划中的应用。

Method: 通过四种动力学模型评估LNNs，包括全阶前向动力学训练和推理、质量矩阵对角化表示、全阶逆动力学训练与前向动力学推理、基于躯干质心动力学的降阶建模。

Result: LNNs样本效率提高10倍，预测精度提升2 - 10倍，对角化方法降低计算复杂度且保留一定可解释性，能实现实时滚动时域控制。

Conclusion: LNNs能捕捉四足动物系统动力学底层结构，提升运动规划和控制性能与效率，且控制频率高于以往LNN方法，有实际应用潜力。

Abstract: Lagrangian Neural Networks (LNNs) present a principled and interpretable
framework for learning the system dynamics by utilizing inductive biases. While
traditional dynamics models struggle with compounding errors over long
horizons, LNNs intrinsically preserve the physical laws governing any system,
enabling accurate and stable predictions essential for sustainable locomotion.
This work evaluates LNNs for infinite horizon planning in quadrupedal robots
through four dynamics models: (1) full-order forward dynamics (FD) training and
inference, (2) diagonalized representation of Mass Matrix in full order FD, (3)
full-order inverse dynamics (ID) training with FD inference, (4) reduced-order
modeling via torso centre-of-mass (CoM) dynamics. Experiments demonstrate that
LNNs bring improvements in sample efficiency (10x) and superior prediction
accuracy (up to 2-10x) compared to baseline methods. Notably, the
diagonalization approach of LNNs reduces computational complexity while
retaining some interpretability, enabling real-time receding horizon control.
These findings highlight the advantages of LNNs in capturing the underlying
structure of system dynamics in quadrupeds, leading to improved performance and
efficiency in locomotion planning and control. Additionally, our approach
achieves a higher control frequency than previous LNN methods, demonstrating
its potential for real-world deployment on quadrupeds.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [313] [Local Routing on Ordered $Θ$-graphs](https://arxiv.org/abs/2506.16021)
*André van Renssen,Shuei Sakaguchi*

Main category: cs.CG

TL;DR: 研究有序Θ图的本地路由问题，证明无确定性无记忆本地路由算法，提出O(1)内存本地路由算法，O(n)跳可达目的地。


<details>
  <summary>Details</summary>
Motivation: 现有有序Θ图无本地路由算法，证明无确定性无记忆本地路由算法后，考虑用少量内存设计算法。

Method: 提出确定性O(1) - 内存本地路由算法。

Result: 算法能在有序Θ图上从源到目的地成功路由，O(n)跳收敛到目的地。

Conclusion: 该算法是首个能保证在有序Θ图上到达目的地的确定性本地路由算法。

Abstract: The problem of locally routing on geometric networks using limited memory is
extensively studied in computational geometry. We consider one particular
graph, the ordered $\Theta$-graph, which is significantly harder to route on
than the $\Theta$-graph, for which a number of routing algorithms are known.
Currently, no local routing algorithm is known for the ordered $\Theta$-graph.
  We prove that, unfortunately, there does not exist a deterministic memoryless
local routing algorithm that works on the ordered $\Theta$-graph. This
motivates us to consider allowing a small amount of memory, and we present a
deterministic $O(1)$-memory local routing algorithm that successfully routes
from the source to the destination on the ordered $\Theta$-graph. We show that
our local routing algorithm converges to the destination in $O(n)$ hops, where
$n$ is the number of vertices. To the best of our knowledge, our algorithm is
the first deterministic local routing algorithm that is guaranteed to reach the
destination on the ordered $\Theta$-graph.

</details>


### [314] [Minimum-Weight Half-Plane Hitting Set](https://arxiv.org/abs/2506.16979)
*Gang Liu,Haitao Wang*

Main category: cs.CG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Given a set $P$ of $n$ weighted points and a set $H$ of $n$ half-planes in
the plane, the hitting set problem is to compute a subset $P'$ of points from
$P$ such that each half-plane contains at least one point from $P'$ and the
total weight of the points in $P'$ is minimized. The previous best algorithm
solves the problem in $O(n^{7/2}\log^2 n)$ time. In this paper, we present a
new algorithm with runtime $O(n^{5/2}\log^2 n)$.

</details>


<div id='hep-th'></div>

# hep-th [[Back]](#toc)

### [315] [Approximate Ricci-flat Metrics for Calabi-Yau Manifolds](https://arxiv.org/abs/2506.15766)
*Seung-Joo Lee,Andre Lukas*

Main category: hep-th

TL;DR: 提出确定Calabi - Yau流形上近似Ricci平坦Kähler势的解析方法，应用于特定超曲面族并得到简单解析表达式。


<details>
  <summary>Details</summary>
Motivation: 确定Calabi - Yau流形上解析Kähler势及相关近似Ricci平坦Kähler度量。

Method: 通过机器学习技术数值计算Ricci平坦Kähler势，并将数值结果拟合到Donaldson的Ansatz。

Result: 在Dwork五次超曲面族和双三次CY超曲面族中得到近似Ricci平坦Kähler势的简单解析表达式，且其仅依赖于复结构参数的模。

Conclusion: 所提出的方法能有效得到Calabi - Yau流形上近似Ricci平坦Kähler势的解析表达式。

Abstract: We outline a method to determine analytic K\"ahler potentials with associated
approximately Ricci-flat K\"ahler metrics on Calabi-Yau manifolds. Key
ingredients are numerically calculating Ricci-flat K\"ahler potentials via
machine learning techniques and fitting the numerical results to Donaldson's
Ansatz. We apply this method to the Dwork family of quintic hypersurfaces in
$\mathbb{P}^4$ and an analogous one-parameter family of bi-cubic CY
hypersurfaces in $\mathbb{P}^2\times\mathbb{P}^2$. In each case, a relatively
simple analytic expression is obtained for the approximately Ricci-flat
K\"ahler potentials, including the explicit dependence on the complex structure
parameter. We find that these K\"ahler potentials only depend on the modulus of
the complex structure parameter.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [316] [Searching for a Hidden Markov Anomaly over Multiple Processes](https://arxiv.org/abs/2506.17108)
*Levli Citron,Kobi Cohen,Qing Zhao*

Main category: eess.SP

TL;DR: 本文研究大量进程中异常进程检测问题，提出基于隐马尔可夫模型的ADHM算法，该算法理论有依据且实证表现强。


<details>
  <summary>Details</summary>
Motivation: 解决异常进程随时间按隐马尔可夫模型演化时的检测问题，与以往假设独立同分布观测不同。

Method: 提出ADHM算法，基于累积统计证据和隐状态预测信念更新动态调整探测策略。

Result: 算法有渐近理论基础，在大量模拟中持续超越现有方法。

Conclusion: ADHM算法能有效利用时间相关性，聚焦最具信息的进程，可最小化期望检测时间并满足错误概率约束。

Abstract: We address the problem of detecting an anomalous process among a large number
of processes. At each time t, normal processes are in state zero (normal
state), while the abnormal process may be in either state zero (normal state)
or state one (abnormal state), with the states being hidden. The transition
between states for the abnormal process is governed by a Markov chain over
time. At each time step, observations can be drawn from a selected subset of
processes. Each probed process generates an observation depending on its hidden
state, either a typical distribution under state zero or an abnormal
distribution under state one. The objective is to design a sequential search
strategy that minimizes the expected detection time, subject to an error
probability constraint. In contrast to prior works that assume i.i.d.
observations, we address a new setting where anomalies evolve according to a
hidden Markov model. To this end, we propose a novel algorithm, dubbed Anomaly
Detection under Hidden Markov model (ADHM), which dynamically adapts the
probing strategy based on accumulated statistical evidence and predictive
belief updates over hidden states. ADHM effectively leverages temporal
correlations to focus sensing resources on the most informative processes. The
algorithm is supported by an asymptotic theoretical foundation, grounded in an
oracle analysis that characterizes the fundamental limits of detection under
the assumption of a known distribution of the hidden states. In addition, the
algorithm demonstrates strong empirical performance, consistently outperforming
existing methods in extensive simulations.

</details>


### [317] [Empowering Near-Field Communications in Low-Altitude Economy with LLM: Fundamentals, Potentials, Solutions, and Future Directions](https://arxiv.org/abs/2506.17067)
*Zhuo Xu,Tianyue Zheng,Linglong Dai*

Main category: eess.SP

TL;DR: 文章探讨低空空域经济（LAE）中近场通信挑战，引入大语言模型（LLM）解决问题，介绍基础、分析机遇挑战、提出方案并给出案例，点明未来研究方向。


<details>
  <summary>Details</summary>
Motivation: LAE近场通信存在信号处理复杂度增加、区分远近场用户等挑战，需有效解决方法。

Method: 先介绍LLM和近场通信基础，分析LAE近场通信机遇挑战，提出基于LLM的近场通信方案并给出联合区分远近场用户和设计多用户预编码矩阵的案例。

Result: 提出了基于LLM的近场通信方案并给出案例研究。

Conclusion: 点明了未来研究方向和待解决的开放问题。

Abstract: The low-altitude economy (LAE) is gaining significant attention from academia
and industry. Fortunately, LAE naturally aligns with near-field communications
in extremely large-scale MIMO (XL-MIMO) systems. By leveraging near-field
beamfocusing, LAE can precisely direct beam energy to unmanned aerial vehicles,
while the additional distance dimension boosts overall spectrum efficiency.
However, near-field communications in LAE still face several challenges, such
as the increase in signal processing complexity and the necessity of
distinguishing between far and near-field users. Inspired by the large language
models (LLM) with powerful ability to handle complex problems, we apply LLM to
solve challenges of near-field communications in LAE. The objective of this
article is to provide a comprehensive analysis and discussion on LLM-empowered
near-field communications in LAE. Specifically, we first introduce fundamentals
of LLM and near-field communications, including the key advantages of LLM and
key characteristics of near-field communications. Then, we reveal the
opportunities and challenges of near-field communications in LAE. To address
these challenges, we present a LLM-based scheme for near-field communications
in LAE, and provide a case study which jointly distinguishes far and near-field
users and designs multi-user precoding matrix. Finally, we outline and
highlight several future research directions and open issues.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [318] [Implementing Keyword Spotting on the MCUX947 Microcontroller with Integrated NPU](https://arxiv.org/abs/2506.08911)
*Petar Jakuš,Hrvoje Džapo*

Main category: cs.HC

TL;DR: 本文在NXP MCXN947微控制器上实现关键词识别系统，结合MFCC与CNN，优化后在嵌入式平台实现高效低功耗语音交互。


<details>
  <summary>Details</summary>
Motivation: 在资源受限设备上实现实时语音交互。

Method: 结合MFCC特征提取和CNN分类器，使用量化感知训练优化以减小模型大小。

Result: 利用NPU推理时间加速59倍，模型大小30.58 KB，准确率达97.06%。

Conclusion: 在嵌入式平台实现高效、低功耗语音接口是可行的。

Abstract: This paper presents a keyword spotting (KWS) system implemented on the NXP
MCXN947 microcontroller with an integrated Neural Processing Unit (NPU),
enabling real-time voice interaction on resource-constrained devices. The
system combines MFCC feature extraction with a CNN classifier, optimized using
Quantization Aware Training to reduce model size with minimal accuracy drop.
Experimental results demonstrate a 59x speedup in inference time when
leveraging the NPU compared to CPU-only execution, achieving 97.06% accuracy
with a model size of 30.58 KB, demonstrating the feasibility of efficient,
low-power voice interfaces on embedded platforms.

</details>


### [319] [On using AI for EEG-based BCI applications: problems, current challenges and future trends](https://arxiv.org/abs/2506.16168)
*Thomas Barbera,Jacopo Burger,Alessandro D'Amelio,Simone Zini,Simone Bianco,Raffaella Lanzarotti,Paolo Napoletano,Giuseppe Boccignone,Jose Luis Contreras-Vidal*

Main category: cs.HC

TL;DR: 本文探讨将AI应用于基于脑电图的脑机接口（BCI）研究领域，分析挑战并展望研究方向，旨在为创建实用有效的EEG - BCI解决方案提供路线图。


<details>
  <summary>Details</summary>
Motivation: AI在解读脑电图脑信号方面取得进展，有望实现创新的脑机接口应用，但应用于现实世界的EEG - BCI面临独特复杂挑战，需要进行深入探索。

Method: 从因果角度考虑基本范式，分析AI模型面临的挑战，探讨有前景的研究途径。

Result: 未提及具体研究结果。

Conclusion: 为创建能在日常环境中有效工作的基于脑电图的BCI解决方案制定清晰路线图。

Abstract: Imagine unlocking the power of the mind to communicate, create, and even
interact with the world around us. Recent breakthroughs in Artificial
Intelligence (AI), especially in how machines "see" and "understand" language,
are now fueling exciting progress in decoding brain signals from scalp
electroencephalography (EEG). Prima facie, this opens the door to revolutionary
brain-computer interfaces (BCIs) designed for real life, moving beyond
traditional uses to envision Brain-to-Speech, Brain-to-Image, and even a
Brain-to-Internet of Things (BCIoT).
  However, the journey is not as straightforward as it was for Computer Vision
(CV) and Natural Language Processing (NLP). Applying AI to real-world EEG-based
BCIs, particularly in building powerful foundational models, presents unique
and intricate hurdles that could affect their reliability.
  Here, we unfold a guided exploration of this dynamic and rapidly evolving
research area. Rather than barely outlining a map of current endeavors and
results, the goal is to provide a principled navigation of this hot and
cutting-edge research landscape. We consider the basic paradigms that emerge
from a causal perspective and the attendant challenges presented to AI-based
models. Looking ahead, we then discuss promising research avenues that could
overcome today's technological, methodological, and ethical limitations. Our
aim is to lay out a clear roadmap for creating truly practical and effective
EEG-based BCI solutions that can thrive in everyday environments.

</details>


### [320] [Do We Talk to Robots Like Therapists, and Do They Respond Accordingly? Language Alignment in AI Emotional Support](https://arxiv.org/abs/2506.16473)
*Sophie Chiang,Guy Laban,Hatice Gunes*

Main category: cs.HC

TL;DR: 研究探讨与机器人对话的关注点和回应是否与人类治疗师相似，分析两数据集，结果显示有主题结构共享和语义重叠，表明机器人支持对话有潜力。


<details>
  <summary>Details</summary>
Motivation: 随着对话代理进行情感支持对话增多，需了解其与传统治疗场景互动的相似程度。

Method: 分析人类与治疗师、人类与机器人对话两个数据集，用句子嵌入和K - means聚类、基于距离的聚类拟合方法及欧几里得距离验证，用Transformer、Word2Vec和BERT嵌入比较主题和回应。

Result: 90.88%的机器人对话披露可映射到人类治疗数据集的聚类，两数据集主题披露及对相似人类披露主题的回应有强语义重叠。

Conclusion: 机器人主导的支持对话与人类治疗既有相似之处也有边界，有增强心理健康干预的潜力。

Abstract: As conversational agents increasingly engage in emotionally supportive
dialogue, it is important to understand how closely their interactions resemble
those in traditional therapy settings. This study investigates whether the
concerns shared with a robot align with those shared in human-to-human (H2H)
therapy sessions, and whether robot responses semantically mirror those of
human therapists. We analyzed two datasets: one of interactions between users
and professional therapists (Hugging Face's NLP Mental Health Conversations),
and another involving supportive conversations with a social robot (QTrobot
from LuxAI) powered by a large language model (LLM, GPT-3.5). Using sentence
embeddings and K-means clustering, we assessed cross-agent thematic alignment
by applying a distance-based cluster-fitting method that evaluates whether
responses from one agent type map to clusters derived from the other, and
validated it using Euclidean distances. Results showed that 90.88% of robot
conversation disclosures could be mapped to clusters from the human therapy
dataset, suggesting shared topical structure. For matched clusters, we compared
the subjects as well as therapist and robot responses using Transformer,
Word2Vec, and BERT embeddings, revealing strong semantic overlap in subjects'
disclosures in both datasets, as well as in the responses given to similar
human disclosure themes across agent types (robot vs. human therapist). These
findings highlight both the parallels and boundaries of robot-led support
conversations and their potential for augmenting mental health interventions.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [321] [From Generation to Adaptation: Comparing AI-Assisted Strategies in High School Programming Education](https://arxiv.org/abs/2506.15955)
*Tong Hu,Songzan Wang*

Main category: cs.CY

TL;DR: 研究对比两种LCA辅助编程教学方法，发现基于MFU的方法更有效，还提出双支架模型并强调教学设计重要性。


<details>
  <summary>Details</summary>
Motivation: 探索适合高中生准备微信小程序竞赛的LCA辅助编程教学方法。

Method: 对五名高中生开展探索性案例研究，分两阶段分别采用从抽象规格生成代码和改编现有最小功能单元的方法。

Result: 第一阶段MVP完成率20%，第二阶段达100%。

Conclusion: 有效整合LCA更依赖教学设计，研究结果可为教育工作者提供实践指导。

Abstract: This exploratory case study investigated two contrasting pedagogical
approaches for LCA-assisted programming with five novice high school students
preparing for a WeChat Mini Program competition. In Phase 1, students used LCAs
to generate code from abstract specifications (From-Scratch approach),
achieving only 20% MVP completion. In Phase 2, students adapted existing
Minimal Functional Units (MFUs), small, functional code examples, using LCAs,
achieving 100% MVP completion. Analysis revealed that the MFU-based approach
succeeded by aligning with LCA strengths in pattern modification rather than de
novo generation, while providing cognitive scaffolds that enabled students to
navigate complex development tasks. The study introduces a dual-scaffolding
model combining technical support (MFUs) with pedagogical guidance (structured
prompting strategies), demonstrating that effective LCA integration depends
less on AI capabilities than on instructional design. These findings offer
practical guidance for educators seeking to transform AI tools from sources of
frustration into productive learning partners in programming education.

</details>


### [322] [Teaching Complex Systems based on Microservices](https://arxiv.org/abs/2506.16492)
*Renato Cordeiro Ferreira,Thatiane de Oliveira Rosa,Alfredo Goldman,Eduardo Guerra*

Main category: cs.CY

TL;DR: 本文介绍了在圣保罗大学向80多名学生教授微服务开发复杂系统这一主题的经验，表明可向高年级本科生传授此类高级概念。


<details>
  <summary>Details</summary>
Motivation: 应对使用微服务开发复杂系统这一当前挑战，探索如何向学生传授相关知识。

Method: 在圣保罗大学向80多名学生教授该主题，培养团队合作并模拟行业环境。

Result: 成功地向学生传授了微服务开发复杂系统的高级概念。

Conclusion: 可以向计算机科学及相关领域的高年级本科生传授此类高级概念。

Abstract: Developing complex systems using microservices is a current challenge. In
this paper, we present our experience with teaching this subject to more than
80 students at the University of S\~ao Paulo (USP), fostering team work and
simulating the industry's environment. We show it is possible to teach such
advanced concepts for senior undergraduate students of Computer Science and
related fields.

</details>


### [323] [From Prompts to Constructs: A Dual-Validity Framework for LLM Research in Psychology](https://arxiv.org/abs/2506.16697)
*Zhicheng Lin*

Main category: cs.CY

TL;DR: 应用人类测量工具研究大语言模型可能产生矛盾结果，需结合可靠测量原则与因果推断标准，提出双有效性框架，当前实践未达要求，需建立证据标准。


<details>
  <summary>Details</summary>
Motivation: 解决应用人类测量工具研究大语言模型产生矛盾结果的问题，构建可靠的人工智能心理学。

Method: 提出双有效性框架，说明不同科学主张所需证据的差异。

Result: 指出当前实践常将统计模式匹配作为心理现象证据，不同研究主张需不同验证策略。

Conclusion: 需开发心理结构的计算类似物，建立明确、可扩展的证据标准。

Abstract: Large language models (LLMs) are rapidly being adopted across psychology,
serving as research tools, experimental subjects, human simulators, and
computational models of cognition. However, the application of human
measurement tools to these systems can produce contradictory results, raising
concerns that many findings are measurement phantoms--statistical artifacts
rather than genuine psychological phenomena. In this Perspective, we argue that
building a robust science of AI psychology requires integrating two of our
field's foundational pillars: the principles of reliable measurement and the
standards for sound causal inference. We present a dual-validity framework to
guide this integration, which clarifies how the evidence needed to support a
claim scales with its scientific ambition. Using an LLM to classify text may
require only basic accuracy checks, whereas claiming it can simulate anxiety
demands a far more rigorous validation process. Current practice systematically
fails to meet these requirements, often treating statistical pattern matching
as evidence of psychological phenomena. The same model output--endorsing "I am
anxious"--requires different validation strategies depending on whether
researchers claim to measure, characterize, simulate, or model psychological
constructs. Moving forward requires developing computational analogues of
psychological constructs and establishing clear, scalable standards of evidence
rather than the uncritical application of human measurement tools.

</details>


### [324] [Large Language Models as Psychological Simulators: A Methodological Guide](https://arxiv.org/abs/2506.16702)
*Zhicheng Lin*

Main category: cs.CY

TL;DR: 本文为在心理学研究中使用大语言模型（LLMs）提供框架，涵盖模拟和认知建模应用，并应对相关挑战。


<details>
  <summary>Details</summary>
Motivation: 大语言模型为心理学和行为研究带来新机遇，但缺乏方法指导。

Method: 提出将LLMs作为心理模拟器的两个主要应用，模拟角色和作为计算模型，介绍开发角色的方法、验证策略和认知建模的新兴方法。

Result: 整合LLM性能的实证证据，涵盖系统偏差、文化限制等。

Conclusion: 该框架有助于研究人员应对挑战，利用LLMs在心理学研究中的独特能力，强调模型能力和限制的透明度。

Abstract: Large language models (LLMs) offer emerging opportunities for psychological
and behavioral research, but methodological guidance is lacking. This article
provides a framework for using LLMs as psychological simulators across two
primary applications: simulating roles and personas to explore diverse
contexts, and serving as computational models to investigate cognitive
processes. For simulation, we present methods for developing psychologically
grounded personas that move beyond demographic categories, with strategies for
validation against human data and use cases ranging from studying inaccessible
populations to prototyping research instruments. For cognitive modeling, we
synthesize emerging approaches for probing internal representations,
methodological advances in causal interventions, and strategies for relating
model behavior to human cognition. We address overarching challenges including
prompt sensitivity, temporal limitations from training data cutoffs, and
ethical considerations that extend beyond traditional human subjects review.
Throughout, we emphasize the need for transparency about model capabilities and
constraints. Together, this framework integrates emerging empirical evidence
about LLM performance--including systematic biases, cultural limitations, and
prompt brittleness--to help researchers wrangle these challenges and leverage
the unique capabilities of LLMs in psychological research.

</details>


### [325] [LLM-Based Bot Broadens the Range of Arguments in Online Discussions, Even When Transparently Disclosed as AI](https://arxiv.org/abs/2506.17073)
*Valeria Vuk,Cristina Sarasua,Fabrizio Gilardi*

Main category: cs.CY

TL;DR: 研究通过实验验证基于大语言模型的机器人能拓宽在线政治讨论视角范围，且披露其为AI不影响效果。


<details>
  <summary>Details</summary>
Motivation: 在线政治讨论因自我选择和平台特性观点范围有限，需拓宽参与者表达视角。

Method: 在聊天室进行两个预注册随机实验，评估主动监测讨论、识别缺失论点并引入对话的机器人的影响。

Result: 机器人显著拓宽论点范围，披露其为AI不显著改变效果。

Conclusion: 基于大语言模型的审核工具可对在线政治话语产生积极影响。

Abstract: A wide range of participation is essential for democracy, as it helps prevent
the dominance of extreme views, erosion of legitimacy, and political
polarization. However, engagement in online political discussions often
features a limited spectrum of views due to high levels of self-selection and
the tendency of online platforms to facilitate exchanges primarily among
like-minded individuals. This study examines whether an LLM-based bot can widen
the scope of perspectives expressed by participants in online discussions
through two pre-registered randomized experiments conducted in a chatroom. We
evaluate the impact of a bot that actively monitors discussions, identifies
missing arguments, and introduces them into the conversation. The results
indicate that our bot significantly expands the range of arguments, as measured
by both objective and subjective metrics. Furthermore, disclosure of the bot as
AI does not significantly alter these effects. These findings suggest that
LLM-based moderation tools can positively influence online political discourse.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [326] [Unsupervised deep learning model for fast energy layer pre-selection of delivery-efficient proton arc therapy plan optimization of nasopharyngeal carcinoma](https://arxiv.org/abs/2506.15803)
*Bohan Yang,Gang Liu,Rirao Dao,Yujia Qian,Ke Shi,Anke Tang,Yong Luo,Jingnan Liu*

Main category: physics.med-ph

TL;DR: 本文提出用于质子弧治疗（PAT）能量层（EL）预选择的无监督深度学习框架SPArcdl，可在保证计划质量时减少递送时间。


<details>
  <summary>Details</summary>
Motivation: PAT在放疗中有优势，但确定最佳EL序列计算量大，需快速有效的EL预选择方法。

Method: 引入点计数表示法，将其作为基于UNet架构SPArcdl的输入，训练优化三目标函数，在54例鼻咽癌病例上评估，与SPArc粒子群生成的计划对比。

Result: SPArcdl显著提高计划质量和递送效率，与SPArc粒子群相比有多项指标提升，推理时间在1秒内，还意外发现不变ELS比下降ELS更省时。

Conclusion: SPArcdl是生成高质量PAT计划的快速有效工具，能战略预选择能量层，减少递送时间并保持良好剂量学性能。

Abstract: Objective. Proton arc therapy (PAT) is an emerging and promising modality in
radiotherapy, offering several advantages over conventional intensitymodulated
proton therapy (IMPT). However, identifying the optimal energy layer (EL)
sequence remains computationally intensive due to the large number of possible
energy layer transitions. This study proposes an unsupervised deep learning
framework for fast and effective EL pre-selection, aiming to minimize energy
layer switch time while preserving high plan quality. Approach. We introduce a
novel data representation method, spot-count representation, which encodes the
number of proton spots intersecting the target and organs at risk (OARs) in a
matrix structured by sorted gantry angles and energy layers. This
representation is the input of a UNet-based architecture, SPArcdl, which is
trained to optimize a tri-objective function: maximizing target coverage,
minimizing OAR exposure, and reducing energy switching time. The model is
evaluated on 54 nasopharyngeal cancer cases, and its performance is benchmarked
against plans generated by SPArcparticle swarm. Main results. SPArcdl produces
EL pre-selection that significantly improves both plan quality and delivery
efficiency. Compared to SPArc particle swarm, it enhances the conformity index
by 0.16 (p < 0.01), reduces the homogeneity index by 0.71 (p < 0.01), shortens
the energy switching time by 38.4% (p < 0.01), and lowers the mean dose to
brainstem by 0.21 (p < 0.01). The results unintentionally reveal employing
unchanged ELS is more time-wise efficient than descended ELS. SPArcdl's
inference time is within 1 second. Significance. SPArcdl is a fast and
effective tool for generating high-quality PAT plans by strategically
pre-selecting energy layers to reduce delivery time while maintaining excellent
dosimetric performance.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [327] [Contactless Precision Steering of Particles in a Fluid inside a Cube with Rotating Walls](https://arxiv.org/abs/2506.15958)
*Lucas Amoudruz,Petr Karnakov,Petros Koumoutsakos*

Main category: physics.flu-dyn

TL;DR: 本文介绍一种新的控制算法，可在流体中操控多个颗粒，通过旋转圆盘产生流场运输颗粒，实验证明其能同时将两个珠子运输到预定位置，推动生物医学应用的无接触颗粒操控。


<details>
  <summary>Details</summary>
Motivation: 现有方法在流体中捕获多个颗粒存在挑战，需要新方法实现生物医学和化学应用中微小物体的无接触操控。

Method: 引入基于ODIL框架的反馈控制策略，利用旋转圆盘产生流场，将流体动力学方程和路径目标结合到单个损失函数中。

Result: 通过模拟和物理设备实验，证明该方法能同时将两个珠子运输到预定位置。

Conclusion: 该方法推进了生物医学应用中稳健的无接触颗粒操控。

Abstract: Contactless manipulation of small objects is essential for biomedical and
chemical applications, such as cell analysis, assisted fertilisation, and
precision chemistry. Established methods, including optical, acoustic, and
magnetic tweezers, are now complemented by flow control techniques that use
flow-induced motion to enable precise and versatile manipulation. However,
trapping multiple particles in fluid remains a challenge. This study introduces
a novel control algorithm capable of steering multiple particles in flow. The
system uses rotating disks to generate flow fields that transport particles to
precise locations. Disk rotations are governed by a feedback control policy
based on the Optimising a Discrete Loss (ODIL) framework, which combines fluid
dynamics equations with path objectives into a single loss function. Our
experiments, conducted in both simulations and with the physical device,
demonstrate the capability of the approach to transport two beads
simultaneously to predefined locations, advancing robust contactless particle
manipulation for biomedical applications.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [328] [Large Average Subtensor Problem: Ground-State, Algorithms, and Algorithmic Barriers](https://arxiv.org/abs/2506.17118)
*Abhishek Hegade K. R.,Eren C. Kızıldağ*

Main category: math.ST

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We introduce the large average subtensor problem: given an order-$p$ tensor
over $\mathbb{R}^{N\times \cdots \times N}$ with i.i.d. standard normal entries
and a $k\in\mathbb{N}$, algorithmically find a $k\times \cdots \times k$
subtensor with a large average entry. This generalizes the large average
submatrix problem, a key model closely related to biclustering and
high-dimensional data analysis, to tensors. For the submatrix case, Bhamidi,
Dey, and Nobel~\cite{bhamidi2017energy} explicitly highlight the regime
$k=\Theta(N)$ as an intriguing open question.
  Addressing the regime $k=\Theta(N)$ for tensors, we establish that the
largest average entry concentrates around an explicit value $E_{\mathrm{max}}$,
provided that the tensor order $p$ is sufficiently large. Furthermore, we prove
that for any $\gamma>0$ and large $p$, this model exhibits multi Overlap Gap
Property ($m$-OGP) above the threshold $\gamma E_{\mathrm{max}}$. The $m$-OGP
serves as a rigorous barrier for a broad class of algorithms exhibiting input
stability. These results hold for both $k=\Theta(N)$ and $k=o(N)$. Moreover,
for small $k$, specifically $k=o(\log^{1.5}N)$, we show that a certain
polynomial-time algorithm identifies a subtensor with average entry
$\frac{2\sqrt{p}}{p+1}E_{\mathrm{max}}$. In particular, the $m$-OGP is
asymptotically sharp: onset of the $m$-OGP and the algorithmic threshold match
as $p$ grows.
  Our results show that while the case $k=\Theta(N)$ remains open for
submatrices, it can be rigorously analyzed for tensors in the large $p$ regime.
This is achieved by interpreting the model as a Boolean spin glass and drawing
on insights from recent advances in the Ising $p$-spin glass model.

</details>


### [329] [Multi-Armed Bandits With Machine Learning-Generated Surrogate Rewards](https://arxiv.org/abs/2506.16658)
*Wenlong Ji,Yihan Pan,Ruihao Zhu,Lihua Lei*

Main category: math.ST

TL;DR: 提出MAB新设置，用预训练ML模型将辅助信息和历史数据转化为代理奖励，针对代理奖励有偏差问题提出MLA - UCB算法，证明其能改善累积遗憾，数值研究显示效率提升。


<details>
  <summary>Details</summary>
Motivation: 传统MAB算法仅依赖在线数据较稀缺，而实际中有丰富辅助数据，且代理奖励有偏差问题待解决。

Method: 引入新设置，提出MLA - UCB算法，可应用于任何奖励预测模型和辅助数据。

Result: 当预测和真实奖励联合高斯且有非零相关性时，能改善累积遗憾；数值研究显示有显著效率提升。

Conclusion: MLA - UCB算法有效，不需要先验知识，在多种情况下都能提升效率。

Abstract: Multi-armed bandit (MAB) is a widely adopted framework for sequential
decision-making under uncertainty. Traditional bandit algorithms rely solely on
online data, which tends to be scarce as it must be gathered during the online
phase when the arms are actively pulled. However, in many practical settings,
rich auxiliary data, such as covariates of past users, is available prior to
deploying any arms. We introduce a new setting for MAB where pre-trained
machine learning (ML) models are applied to convert side information and
historical data into \emph{surrogate rewards}. A prominent feature of this
setting is that the surrogate rewards may exhibit substantial bias, as true
reward data is typically unavailable in the offline phase, forcing ML
predictions to heavily rely on extrapolation. To address the issue, we propose
the Machine Learning-Assisted Upper Confidence Bound (MLA-UCB) algorithm, which
can be applied to any reward prediction model and any form of auxiliary data.
When the predicted and true rewards are jointly Gaussian, it provably improves
the cumulative regret, provided that the correlation is non-zero -- even in
cases where the mean surrogate reward completely misaligns with the true mean
rewards. Notably, our method requires no prior knowledge of the covariance
matrix between true and surrogate rewards. We compare MLA-UCB with the standard
UCB on a range of numerical studies and show a sizable efficiency gain even
when the size of the offline data and the correlation between predicted and
true rewards are moderate.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [330] [Autonomous Trajectory Optimization for UAVs in Disaster Zone Using Henry Gas Optimization Scheme](https://arxiv.org/abs/2506.15910)
*Zakria Qadir,Muhammad Bilal,Guoqiang Liu,Xiaolong Xu*

Main category: eess.SY

TL;DR: 本文提出基于Henry气体优化（HGO）元启发式算法的集群优化方案（COS）用于无人机轨迹优化，经不同场景验证，HGO算法优于现有算法，可用于智慧城市无人机自主轨迹优化。


<details>
  <summary>Details</summary>
Motivation: 在灾害易发的复杂环境中，无人机轨迹优化对救援和通信至关重要，需找到最短路径并降低运输成本和算法复杂度。

Method: 提出基于HGO算法的COS方案，设计数学模型，并与PSO、GWO、CSA和BMO等元启发式算法对比，通过四种不同场景评估模型。

Result: 在所有场景中，HGO算法均优于现有算法，在环境场景下，与PSO算法相比，运输成本降低39.3%，计算时间减少16.8%。

Conclusion: HGO算法可用于智慧城市无人机自主轨迹优化。

Abstract: The unmanned aerial vehicles (UAVs) in a disaster-prone environment plays
important role in assisting the rescue services and providing the internet
connectivity with the outside world. However, in such a complex environment the
selection of optimum trajectory of UAVs is of utmost importance. UAV trajectory
optimization deals with finding the shortest path in the minimal possible time.
In this paper, a cluster optimization scheme (COS) is proposed using the Henry
gas optimization (HGO) metaheuristic algorithm to identify the shortest path
having minimal transportation cost and algorithm complexity. The mathematical
model is designed for COS using the HGO algorithm and compared with the
state-of-the-art metaheuristic algorithms such as particle swarm optimization
(PSO), grey wolf optimization (GWO), cuckoo search algorithm (CSA) and
barnacles mating optimizer (BMO). In order to prove the robustness of the
proposed model, four different scenarios are evaluated that includes ambient
environment, constrict environment, tangled environment, and complex
environment. In all the aforementioned scenarios, the HGO algorithm outperforms
the existing algorithms. Particularly, in the ambient environment, the HGO
algorithm achieves a 39.3% reduction in transportation cost and a 16.8%
reduction in computational time as compared to the PSO algorithm. Hence, the
HGO algorithm can be used for autonomous trajectory optimization of UAVs in
smart cities.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [331] [Universal Music Representations? Evaluating Foundation Models on World Music Corpora](https://arxiv.org/abs/2506.17055)
*Charilaos Papaioannou,Emmanouil Benetos,Alexandros Potamianos*

Main category: cs.SD

TL;DR: 对五个音频基础模型在六种音乐语料库上进行综合评估，分析其跨文化能力，展示不同模型表现及方法有效性，为通用音乐表征研究提供框架和指标。


<details>
  <summary>Details</summary>
Motivation: 探究基础模型在不同音乐传统中的泛化能力。

Method: 采用三种互补方法，包括探测固有表征、1 - 2层有针对性的监督微调、多标签少样本学习。

Result: 不同模型跨文化泛化表现不同，大模型在非西方音乐上表现更好，在文化差异大的传统中结果下降；五种方法在六个评估数据集中的五个上达到了最先进水平；有针对性的微调方法并非在所有设置中都优于探测。

Conclusion: 评估框架和基准结果有助于理解当前模型离实现通用音乐表征的距离，并为未来进展建立了指标。

Abstract: Foundation models have revolutionized music information retrieval, but
questions remain about their ability to generalize across diverse musical
traditions. This paper presents a comprehensive evaluation of five
state-of-the-art audio foundation models across six musical corpora spanning
Western popular, Greek, Turkish, and Indian classical traditions. We employ
three complementary methodologies to investigate these models' cross-cultural
capabilities: probing to assess inherent representations, targeted supervised
fine-tuning of 1-2 layers, and multi-label few-shot learning for low-resource
scenarios. Our analysis shows varying cross-cultural generalization, with
larger models typically outperforming on non-Western music, though results
decline for culturally distant traditions. Notably, our approaches achieve
state-of-the-art performance on five out of six evaluated datasets,
demonstrating the effectiveness of foundation models for world music
understanding. We also find that our targeted fine-tuning approach does not
consistently outperform probing across all settings, suggesting foundation
models already encode substantial musical knowledge. Our evaluation framework
and benchmarking results contribute to understanding how far current models are
from achieving universal music representations while establishing metrics for
future progress.

</details>


### [332] [Improved Intelligibility of Dysarthric Speech using Conditional Flow Matching](https://arxiv.org/abs/2506.16127)
*Shoutrik Das,Nishant Singh,Arjun Gangwar,S Umesh*

Main category: cs.SD

TL;DR: 研究用自监督学习特征替代梅尔频谱图进行构音障碍语音到正常语音转换，提出非自回归方法，离散声学单元提升可懂度且收敛快。


<details>
  <summary>Details</summary>
Motivation: 构音障碍严重影响语音可懂度，需开发构音障碍语音到正常语音转换技术。

Method: 研究自监督学习特征及其量化表示替代梅尔频谱图，用WavLM提取特征缓解说话人差异，提出基于条件流匹配和扩散变压器的全非自回归方法。

Result: 离散声学单元能提高语音可懂度，且比传统基于梅尔频谱图的方法收敛更快。

Conclusion: 自监督学习特征及提出的非自回归方法在构音障碍语音转换中有一定效果。

Abstract: Dysarthria is a neurological disorder that significantly impairs speech
intelligibility, often rendering affected individuals unable to communicate
effectively. This necessitates the development of robust dysarthric-to-regular
speech conversion techniques. In this work, we investigate the utility and
limitations of self-supervised learning (SSL) features and their quantized
representations as an alternative to mel-spectrograms for speech generation.
Additionally, we explore methods to mitigate speaker variability by generating
clean speech in a single-speaker voice using features extracted from WavLM. To
this end, we propose a fully non-autoregressive approach that leverages
Conditional Flow Matching (CFM) with Diffusion Transformers to learn a direct
mapping from dysarthric to clean speech. Our findings highlight the
effectiveness of discrete acoustic units in improving intelligibility while
achieving faster convergence compared to traditional mel-spectrogram-based
approaches.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [333] [Client Selection Strategies for Federated Semantic Communications in Heterogeneous IoT Networks](https://arxiv.org/abs/2506.17063)
*Samer Lahoud,Kinda Khawam*

Main category: cs.NI

TL;DR: 本文提出联邦语义通信框架，解决物联网设备在无线带宽受限网络的数据传输和隐私问题，通过不同客户端选择策略平衡重建质量、资源效率和公平性。


<details>
  <summary>Details</summary>
Motivation: 物联网设备指数级增长，在带宽受限无线网络面临高效数据传输和隐私保护挑战。

Method: 提出联邦语义通信框架，利用语义通信原理传输语义特征，实现三种客户端选择策略，采用端到端语义通信架构和基于损失的聚合机制。

Result: 实验表明功利主义选择实现最高重建质量，比例公平选择性能有竞争力，能减少参与不平等并提高计算效率。

Conclusion: 联邦语义通信可在异构物联网部署中平衡重建质量、资源效率和公平性，为边缘智能应用奠定基础。

Abstract: The exponential growth of IoT devices presents critical challenges in
bandwidth-constrained wireless networks, particularly regarding efficient data
transmission and privacy preservation. This paper presents a novel federated
semantic communication (SC) framework that enables collaborative training of
bandwidth-efficient models for image reconstruction across heterogeneous IoT
devices. By leveraging SC principles to transmit only semantic features, our
approach dramatically reduces communication overhead while preserving
reconstruction quality. We address the fundamental challenge of client
selection in federated learning environments where devices exhibit significant
disparities in dataset sizes and data distributions. Our framework implements
three distinct client selection strategies that explore different trade-offs
between system performance and fairness in resource allocation. The system
employs an end-to-end SC architecture with semantic bottlenecks, coupled with a
loss-based aggregation mechanism that naturally adapts to client heterogeneity.
Experimental evaluation on image data demonstrates that while Utilitarian
selection achieves the highest reconstruction quality, Proportional Fairness
maintains competitive performance while significantly reducing participation
inequality and improving computational efficiency. These results establish that
federated SC can successfully balance reconstruction quality, resource
efficiency, and fairness in heterogeneous IoT deployments, paving the way for
sustainable and privacy-preserving edge intelligence applications.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [334] [Graphics4Science: Computer Graphics for Scientific Impacts](https://arxiv.org/abs/2506.15786)
*Peter Yichen Chen,Minghao Guo,Hanspeter Pfister,Ming Lin,William Freeman,Qixing Huang,Han-Wei Shen,Wojciech Matusik*

Main category: cs.GR

TL;DR: 课程Graphics4Science探索计算机图形学与科学的关系，旨在将图形学作为科学建模语言，邀请图形学社区参与科学研究。


<details>
  <summary>Details</summary>
Motivation: 计算机图形学在解决科学挑战方面一直是有力工具，探索其与科学的深层关系，解决两个领域间的词汇差距问题。

Method: 通过展示核心方法如几何推理和物理建模在两个领域尤其是数据稀缺场景中的作用，尝试弥合两个社区的词汇差距。

Result: 无明确提及具体结果

Conclusion: 邀请图形学社区参与科学研究，为科学发现的未来做贡献。

Abstract: Computer graphics, often associated with films, games, and visual effects,
has long been a powerful tool for addressing scientific challenges--from its
origins in 3D visualization for medical imaging to its role in modern
computational modeling and simulation. This course explores the deep and
evolving relationship between computer graphics and science, highlighting past
achievements, ongoing contributions, and open questions that remain. We show
how core methods, such as geometric reasoning and physical modeling, provide
inductive biases that help address challenges in both fields, especially in
data-scarce settings. To that end, we aim to reframe graphics as a modeling
language for science by bridging vocabulary gaps between the two communities.
Designed for both newcomers and experts, Graphics4Science invites the graphics
community to engage with science, tackle high-impact problems where graphics
expertise can make a difference, and contribute to the future of scientific
discovery. Additional details are available on the course website:
https://graphics4science.github.io

</details>


### [335] [VEIGAR: View-consistent Explicit Inpainting and Geometry Alignment for 3D object Removal](https://arxiv.org/abs/2506.15821)
*Pham Khai Nguyen Do,Bao Nguyen Tran,Nam Nguyen,Duc Dung Nguyen*

Main category: cs.GR

TL;DR: 本文提出计算高效框架VEIGAR，不依赖初始重建阶段，在重建质量和跨视图一致性上达新高度，训练时间显著减少。


<details>
  <summary>Details</summary>
Motivation: 现有新颖视图合成和3D生成编辑方法需初始3D重建阶段，计算开销大且重建质量不佳。

Method: 提出VEIGAR框架，利用轻量级基础模型在像素空间明确对齐先验，引入基于尺度不变深度损失的监督策略。

Result: VEIGAR在重建质量和跨视图一致性上建立新基准，训练时间比最快的现有方法减少三倍。

Conclusion: VEIGAR在效率和效果上实现了更好的平衡。

Abstract: Recent advances in Novel View Synthesis (NVS) and 3D generation have
significantly improved editing tasks, with a primary emphasis on maintaining
cross-view consistency throughout the generative process. Contemporary methods
typically address this challenge using a dual-strategy framework: performing
consistent 2D inpainting across all views guided by embedded priors either
explicitly in pixel space or implicitly in latent space; and conducting 3D
reconstruction with additional consistency guidance. Previous strategies, in
particular, often require an initial 3D reconstruction phase to establish
geometric structure, introducing considerable computational overhead. Even with
the added cost, the resulting reconstruction quality often remains suboptimal.
In this paper, we present VEIGAR, a computationally efficient framework that
outperforms existing methods without relying on an initial reconstruction
phase. VEIGAR leverages a lightweight foundation model to reliably align priors
explicitly in the pixel space. In addition, we introduce a novel supervision
strategy based on scale-invariant depth loss, which removes the need for
traditional scale-and-shift operations in monocular depth regularization.
Through extensive experimentation, VEIGAR establishes a new state-of-the-art
benchmark in reconstruction quality and cross-view consistency, while achieving
a threefold reduction in training time compared to the fastest existing method,
highlighting its superior balance of efficiency and effectiveness.

</details>


### [336] [FlatCAD: Fast Curvature Regularization of Neural SDFs for CAD Models](https://arxiv.org/abs/2506.16627)
*Haotian Yin,Aleksander Plocharski,Michal Jan Wlodarczyk,Mikolaj Kida,Przemyslaw Musialski*

Main category: cs.GR

TL;DR: 提出用于神经有符号距离场（SDF）的曲率代理，降低计算成本，在ABC基准测试中表现良好，为工程级形状重建的SDF学习开辟实用途径。


<details>
  <summary>Details</summary>
Motivation: 现有的强制可展、类CAD行为的方法依赖高斯曲率惩罚项，需要完整的Hessian评估和二阶自动微分，内存和运行时成本高。

Method: 提出仅对混合二阶项（Weingarten项）进行正则化的曲率代理，有有限差分代理和自动微分代理两种实现方式。

Result: 在ABC基准测试中，代理方法在重建保真度上与基于Hessian的基线方法相当或更优，同时将GPU内存使用和时钟时间减少一半。

Conclusion: 该方法是即插即用且与框架无关的，为工程级形状重建的可扩展、曲率感知的SDF学习开辟了实用途径。

Abstract: Neural signed-distance fields (SDFs) have become a versatile backbone for
geometric learning, yet enforcing developable, CAD-style behavior still hinges
on Gaussian curvature penalties that require full Hessian evaluation and
second-order automatic differentiation, both of which are costly in memory and
runtime. We present a curvature proxy that regularizes only the mixed
second-order term (Weingarten term), allowing the two principal curvatures to
adapt freely to data while suppressing unwanted warp. Two complementary
instantiations realize this idea: (i) a finite-difference proxy that replaces
each Hessian entry with four forward SDF evaluations and a single first-order
gradient, and (ii) an autodiff proxy that computes the same mixed derivative
via one Hessian-vector product, sidestepping explicit full Hessian assembly and
remaining faster in practice. Both variants converge to the exact mixed second
derivative, thus preserving the intended geometric bias without incurring full
second-order graphs. On the ABC benchmarks, the proxies match or exceed the
reconstruction fidelity of Hessian-based baselines while reducing GPU memory
use and wall-clock time by a factor of two. Because the method is drop-in and
framework-agnostic, it opens a practical path toward scalable, curvature-aware
SDF learning for engineering-grade shape reconstruction.

</details>


### [337] [Beyond Blur: A Fluid Perspective on Generative Diffusion Models](https://arxiv.org/abs/2506.16827)
*Grzegorz Gruszczynski,Michal Jan Wlodarczyk,Jakub J Meixner,Przemyslaw Musialski*

Main category: cs.GR

TL;DR: 提出基于对流 - 扩散过程的PDE驱动图像生成方法，概括现有PDE方法，提升图像多样性和质量。


<details>
  <summary>Details</summary>
Motivation: 提出更通用的基于PDE的图像生成腐败过程，连接流体动力学、无量纲PDE理论和深度生成建模。

Method: 通过结合定向对流、各向同性扩散和高斯噪声的PDE来构建图像腐败过程，用GPU加速的自定义格子玻尔兹曼求解器实现，生成随机速度场，用神经网络反转对流 - 扩散算子。

Result: 展示之前方法是该算子的特例，对流提升了生成图像的多样性和质量，且不影响整体调色板。

Conclusion: 该工作为基于扩散的合成提供了物理信息图像腐败过程的新视角。

Abstract: We propose a novel PDE-driven corruption process for generative image
synthesis based on advection-diffusion processes which generalizes existing
PDE-based approaches. Our forward pass formulates image corruption via a
physically motivated PDE that couples directional advection with isotropic
diffusion and Gaussian noise, controlled by dimensionless numbers (Peclet,
Fourier). We implement this PDE numerically through a GPU-accelerated custom
Lattice Boltzmann solver for fast evaluation. To induce realistic turbulence,
we generate stochastic velocity fields that introduce coherent motion and
capture multi-scale mixing. In the generative process, a neural network learns
to reverse the advection-diffusion operator thus constituting a novel
generative model. We discuss how previous methods emerge as specific cases of
our operator, demonstrating that our framework generalizes prior PDE-based
corruption techniques. We illustrate how advection improves the diversity and
quality of the generated images while keeping the overall color palette
unaffected. This work bridges fluid dynamics, dimensionless PDE theory, and
deep generative modeling, offering a fresh perspective on physically informed
image corruption processes for diffusion-based synthesis.

</details>


### [338] [DreamCube: 3D Panorama Generation via Multi-plane Synchronization](https://arxiv.org/abs/2506.17206)
*Yukun Huang,Yanning Zhou,Jianan Wang,Kaiyi Huang,Xihui Liu*

Main category: cs.GR

TL;DR: 提出DreamCube多平面RGB - D扩散模型用于3D全景生成，有效利用2D基础模型先验，实验证明其在多项任务有效。


<details>
  <summary>Details</summary>
Motivation: 现有利用2D基础模型先验的3D全景合成方法因3D全景与2D单视图不兼容，效果受限。

Method: 对2D基础模型的算子应用多平面同步，将其能力扩展到全向领域，引入DreamCube多平面RGB - D扩散模型。

Result: 实验表明该方法在全景图像生成、全景深度估计和3D场景生成任务中有效。

Conclusion: 通过多平面同步扩展2D基础模型算子可用于3D全景生成，DreamCube模型能有效利用2D基础模型先验实现多样外观和准确几何并保持多视图一致性。

Abstract: 3D panorama synthesis is a promising yet challenging task that demands
high-quality and diverse visual appearance and geometry of the generated
omnidirectional content. Existing methods leverage rich image priors from
pre-trained 2D foundation models to circumvent the scarcity of 3D panoramic
data, but the incompatibility between 3D panoramas and 2D single views limits
their effectiveness. In this work, we demonstrate that by applying multi-plane
synchronization to the operators from 2D foundation models, their capabilities
can be seamlessly extended to the omnidirectional domain. Based on this design,
we further introduce DreamCube, a multi-plane RGB-D diffusion model for 3D
panorama generation, which maximizes the reuse of 2D foundation model priors to
achieve diverse appearances and accurate geometry while maintaining multi-view
consistency. Extensive experiments demonstrate the effectiveness of our
approach in panoramic image generation, panoramic depth estimation, and 3D
scene generation.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [339] [A System Level Compiler for Massively-Parallel, Spatial, Dataflow Architectures](https://arxiv.org/abs/2506.15875)
*Dirk Van Essendelft,Patrick Wingo,Terry Jordan,Ryan Smith,Wissam Saidi*

Main category: cs.PL

TL;DR: 开发了适用于大规模并行空间数据流架构的MACH编译器，能在传统设备执行代码，通过虚拟机等方式解决编译复杂性，以密集张量示例介绍概念并展示向晶圆级引擎的转换。


<details>
  <summary>Details</summary>
Motivation: 解决为空间架构编译代码的复杂性问题，提供能在多种架构运行且灵活的数据映射编译方案。

Method: 通过概念上的虚拟机、灵活的领域特定语言以及符合虚拟机概念将高级语言转换为特定机器代码的编译器。

Result: 开发出MACH编译器，可在多种架构运行，能用密集张量示例展示向晶圆级引擎的代码转换。

Conclusion: MACH编译器是适用于多种架构、能解决空间架构编译复杂性的有效工具。

Abstract: We have developed a novel compiler called the Multiple-Architecture Compiler
for Advanced Computing Hardware (MACH) designed specifically for
massively-parallel, spatial, dataflow architectures like the Wafer Scale
Engine. Additionally, MACH can execute code on traditional unified-memory
devices. MACH addresses the complexities in compiling for spatial architectures
through a conceptual Virtual Machine, a flexible domain-specific language, and
a compiler that can lower high-level languages to machine-specific code in
compliance with the Virtual Machine concept. While MACH is designed to be
operable on several architectures and provide the flexibility for several
standard and user-defined data mappings, we introduce the concept with dense
tensor examples from NumPy and show lowering to the Wafer Scale Engine by
targeting Cerebras' hardware specific languages.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [340] [Code Rate Optimization via Neural Polar Decoders](https://arxiv.org/abs/2506.15836)
*Ziv Aharoni,Bashar Huleihel,Henry D Pfister,Haim H Permuter*

Main category: cs.IT

TL;DR: 本文提出用神经极化解码器优化通信码率的方法，分训练和推理两阶段，实验证明在特定信道有效，有实际应用潜力。


<details>
  <summary>Details</summary>
Motivation: 在信道模型未知情况下，同时优化输入分布上的码率，并提供实用编码方案。

Method: 采用极化码，用神经极化解码器估计互信息，优化输入分布的参数模型，分训练和推理两阶段处理。

Result: 在无记忆和有限状态信道实验中，当信道容量达到的输入分布非均匀时，互信息和误码率有显著改善，验证了块长至1024的方法有效性。

Conclusion: 该可扩展方法能弥合理论容量估计和实际编码性能间的差距，有实际通信系统应用潜力。

Abstract: This paper proposes a method to optimize communication code rates via the
application of neural polar decoders (NPDs). Employing this approach enables
simultaneous optimization of code rates over input distributions while
providing a practical coding scheme within the framework of polar codes. The
proposed approach is designed for scenarios where the channel model is unknown,
treating the channel as a black box that produces output samples from input
samples. We employ polar codes to achieve our objectives, using NPDs to
estimate mutual information (MI) between the channel inputs and outputs, and
optimize a parametric model of the input distribution. The methodology involves
a two-phase process: a training phase and an inference phase. In the training
phase, two steps are repeated interchangeably. First, the estimation step
estimates the MI of the channel inputs and outputs via NPDs. Second, the
improvement step optimizes the input distribution parameters to maximize the MI
estimate obtained by the NPDs. In the inference phase, the optimized model is
used to construct polar codes. This involves incorporating the Honda-Yamamoto
(HY) scheme to accommodate the optimized input distributions and list decoding
to enhance decoding performance. Experimental results on memoryless and
finite-state channels (FSCs) demonstrate the effectiveness of our approach,
particularly in cases where the channel's capacity-achieving input distribution
is non-uniform. For these cases, we show significant improvements in MI and bit
error rates (BERs) over those achieved by uniform and independent and
identically distributed (i.i.d.) input distributions, validating our method for
block lengths up to 1024. This scalable approach has potential applications in
real-world communication systems, bridging theoretical capacity estimation and
practical coding performance.

</details>


### [341] [Neural Polar Decoders for DNA Data Storage](https://arxiv.org/abs/2506.17076)
*Ziv Aharoni,Henry D. Pfister*

Main category: cs.IT

TL;DR: 本文提出基于神经极化解码器（NPDs）的数据驱动方法，为存在同步错误的通道设计低复杂度解码器，在合成和真实DNA存储场景中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: DNA数据存储系统中同步错误是基本挑战，设计最大似然解码器计算成本高，需要低复杂度解码器。

Method: 提出基于NPDs的数据驱动方法，该架构能以O(AN log N)复杂度在插入 - 删除 - 替换（IDS）通道上解码，只需样本访问通道且无需显式通道模型训练。

Result: 在合成删除和IDS通道上，NPDs实现接近最优的解码性能和准确的互信息估计，复杂度低于基于网格的解码器；在真实DNA存储场景中，NPDs性能匹配或超越现有方法，参数使用显著少于现有技术。

Conclusion: NPDs在DNA数据存储系统中用于鲁棒高效解码有很大潜力。

Abstract: Synchronization errors, such as insertions and deletions, present a
fundamental challenge in DNA-based data storage systems, arising from both
synthesis and sequencing noise. These channels are often modeled as
insertion-deletion-substitution (IDS) channels, for which designing
maximum-likelihood decoders is computationally expensive. In this work, we
propose a data-driven approach based on neural polar decoders (NPDs) to design
low-complexity decoders for channels with synchronization errors. The proposed
architecture enables decoding over IDS channels with reduced complexity $O(AN
log N )$, where $A$ is a tunable parameter independent of the channel. NPDs
require only sample access to the channel and can be trained without an
explicit channel model. Additionally, NPDs provide mutual information (MI)
estimates that can be used to optimize input distributions and code design. We
demonstrate the effectiveness of NPDs on both synthetic deletion and IDS
channels. For deletion channels, we show that NPDs achieve near-optimal
decoding performance and accurate MI estimation, with significantly lower
complexity than trellis-based decoders. We also provide numerical estimates of
the channel capacity for the deletion channel. We extend our evaluation to
realistic DNA storage settings, including channels with multiple noisy reads
and real-world Nanopore sequencing data. Our results show that NPDs match or
surpass the performance of existing methods while using significantly fewer
parameters than the state-of-the-art. These findings highlight the promise of
NPDs for robust and efficient decoding in DNA data storage systems.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [342] [MoNetV2: Enhanced Motion Network for Freehand 3D Ultrasound Reconstruction](https://arxiv.org/abs/2506.15835)
*Mingyuan Luo,Xin Yang,Zhongnuo Yan,Yan Cao,Yuanji Zhang,Xindi Hu,Jin Wang,Haoxuan Ding,Wei Han,Litao Sun,Dong Ni*

Main category: eess.IV

TL;DR: 本文提出增强运动网络MoNetV2用于徒手3D超声重建，在三个大型数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的徒手3D超声仅图像重建方法在减少累积漂移和提高重建精度上存在困难，尤其是复杂运动轨迹场景。

Method: 提出基于传感器的时间和多分支结构融合图像与运动信息；设计在线多级一致性约束处理不同扫描速度和策略；提炼在线多模态自监督策略减少累积误差。

Result: 在三个大型数据集上，MoNetV2在重建质量和泛化性能上超越现有方法。

Conclusion: MoNetV2能有效提升不同扫描速度和策略下重建的准确性和泛化性。

Abstract: Three-dimensional (3D) ultrasound (US) aims to provide sonographers with the
spatial relationships of anatomical structures, playing a crucial role in
clinical diagnosis. Recently, deep-learning-based freehand 3D US has made
significant advancements. It reconstructs volumes by estimating transformations
between images without external tracking. However, image-only reconstruction
poses difficulties in reducing cumulative drift and further improving
reconstruction accuracy, particularly in scenarios involving complex motion
trajectories. In this context, we propose an enhanced motion network (MoNetV2)
to enhance the accuracy and generalizability of reconstruction under diverse
scanning velocities and tactics. First, we propose a sensor-based temporal and
multi-branch structure that fuses image and motion information from a velocity
perspective to improve image-only reconstruction accuracy. Second, we devise an
online multi-level consistency constraint that exploits the inherent
consistency of scans to handle various scanning velocities and tactics. This
constraint exploits both scan-level velocity consistency, path-level appearance
consistency, and patch-level motion consistency to supervise inter-frame
transformation estimation. Third, we distill an online multi-modal
self-supervised strategy that leverages the correlation between network
estimation and motion information to further reduce cumulative errors.
Extensive experiments clearly demonstrate that MoNetV2 surpasses existing
methods in both reconstruction quality and generalizability performance across
three large datasets.

</details>


### [343] [Cross-Modality Learning for Predicting IHC Biomarkers from H&E-Stained Whole-Slide Images](https://arxiv.org/abs/2506.15853)
*Amit Das,Naofumi Tomita,Kyle J. Syme,Weijie Ma,Paige O'Connor,Kristin N. Corbett,Bing Ren,Xiaoying Liu,Saeed Hassanpour*

Main category: eess.IV

TL;DR: 提出HistoStainAlign深度学习框架从H&E全切片图像预测IHC染色模式，评估表现良好，展示计算方法作为预筛工具潜力。


<details>
  <summary>Details</summary>
Motivation: IHC染色成本高、耗时长且资源密集，需解决这些局限性。

Method: 提出HistoStainAlign框架，通过对比训练策略整合H&E和IHC嵌入，学习形态和分子特征的联合表示。

Result: 在胃肠和肺组织WSIs上评估，三种IHC染色加权F1分数分别为0.735、0.830和0.723，嵌入分析证明对比对齐的鲁棒性，与基线模型对比凸显优势。

Conclusion: 计算方法有潜力作为预筛工具，提高工作流程效率。

Abstract: Hematoxylin and Eosin (H&E) staining is a cornerstone of pathological
analysis, offering reliable visualization of cellular morphology and tissue
architecture for cancer diagnosis, subtyping, and grading. Immunohistochemistry
(IHC) staining provides molecular insights by detecting specific proteins
within tissues, enhancing diagnostic accuracy, and improving treatment
planning. However, IHC staining is costly, time-consuming, and
resource-intensive, requiring specialized expertise. To address these
limitations, this study proposes HistoStainAlign, a novel deep learning
framework that predicts IHC staining patterns directly from H&E whole-slide
images (WSIs) by learning joint representations of morphological and molecular
features. The framework integrates paired H&E and IHC embeddings through a
contrastive training strategy, capturing complementary features across staining
modalities without patch-level annotations or tissue registration. The model
was evaluated on gastrointestinal and lung tissue WSIs with three commonly used
IHC stains: P53, PD-L1, and Ki-67. HistoStainAlign achieved weighted F1 scores
of 0.735 [95% Confidence Interval (CI): 0.670-0.799], 0.830 [95% CI:
0.772-0.886], and 0.723 [95% CI: 0.607-0.836], respectively for these three IHC
stains. Embedding analyses demonstrated the robustness of the contrastive
alignment in capturing meaningful cross-stain relationships. Comparisons with a
baseline model further highlight the advantage of incorporating contrastive
learning for improved stain pattern prediction. This study demonstrates the
potential of computational approaches to serve as a pre-screening tool, helping
prioritize cases for IHC staining and improving workflow efficiency.

</details>


### [344] [CF-Seg: Counterfactuals meet Segmentation](https://arxiv.org/abs/2506.16213)
*Raghav Mehta,Fabio De Sousa Ribeiro,Tian Xia,Melanie Roschewitz,Ainkaran Santhirasekaram,Dominic C. Marshall,Ben Glocker*

Main category: eess.IV

TL;DR: 本文提出生成反事实图像以改进医学图像解剖结构分割，实验证明其能提升分割效果，助力临床决策。


<details>
  <summary>Details</summary>
Motivation: 疾病会改变健康组织外观、引入模糊边界或掩盖关键结构，使现有分割模型在含疾病图像上难以准确分割，可能导致误诊。

Method: 生成反事实图像模拟无疾病情况下相同解剖结构的样子，用这些图像进行感兴趣结构的分割，且无需改变底层分割模型。

Result: 在两个真实临床胸部X光数据集上的实验表明，使用反事实图像改善了解剖结构分割。

Conclusion: 使用反事实图像有助于下游临床决策。

Abstract: Segmenting anatomical structures in medical images plays an important role in
the quantitative assessment of various diseases. However, accurate segmentation
becomes significantly more challenging in the presence of disease. Disease
patterns can alter the appearance of surrounding healthy tissues, introduce
ambiguous boundaries, or even obscure critical anatomical structures. As such,
segmentation models trained on real-world datasets may struggle to provide good
anatomical segmentation, leading to potential misdiagnosis. In this paper, we
generate counterfactual (CF) images to simulate how the same anatomy would
appear in the absence of disease without altering the underlying structure. We
then use these CF images to segment structures of interest, without requiring
any changes to the underlying segmentation model. Our experiments on two
real-world clinical chest X-ray datasets show that the use of counterfactual
images improves anatomical segmentation, thereby aiding downstream clinical
decision-making.

</details>


### [345] [Hybrid Attention Network for Accurate Breast Tumor Segmentation in Ultrasound Images](https://arxiv.org/abs/2506.16592)
*Muhammad Azeem Aslam,Asim Naveed,Nisar Ahmed*

Main category: eess.IV

TL;DR: 提出用于乳腺超声图像病变分割的混合注意力网络，实验表明该方法优于现有方法，有助乳腺癌早期准确诊断。


<details>
  <summary>Details</summary>
Motivation: 乳腺超声成像自动肿瘤分割面临噪声、病变尺度变化和边界模糊等挑战，需新方法解决。

Method: 提出混合注意力网络，编码器用预训练DenseNet121提取特征，解码器采用多分支注意力增强结构，瓶颈层融入GSA、PE、SDPA，跳跃连接嵌入SFEB，使用结合BCE和Jaccard Index的混合损失函数。

Result: 在公共数据集上实验显示该方法优于现有方法。

Conclusion: 该方法有潜力辅助放射科医生进行早期准确的乳腺癌诊断。

Abstract: Breast ultrasound imaging is a valuable tool for early breast cancer
detection, but automated tumor segmentation is challenging due to inherent
noise, variations in scale of lesions, and fuzzy boundaries. To address these
challenges, we propose a novel hybrid attention-based network for lesion
segmentation. Our proposed architecture integrates a pre-trained DenseNet121 in
the encoder part for robust feature extraction with a multi-branch
attention-enhanced decoder tailored for breast ultrasound images. The
bottleneck incorporates Global Spatial Attention (GSA), Position Encoding (PE),
and Scaled Dot-Product Attention (SDPA) to learn global context, spatial
relationships, and relative positional features. The Spatial Feature
Enhancement Block (SFEB) is embedded at skip connections to refine and enhance
spatial features, enabling the network to focus more effectively on tumor
regions. A hybrid loss function combining Binary Cross-Entropy (BCE) and
Jaccard Index loss optimizes both pixel-level accuracy and region-level overlap
metrics, enhancing robustness to class imbalance and irregular tumor shapes.
Experiments on public datasets demonstrate that our method outperforms existing
approaches, highlighting its potential to assist radiologists in early and
accurate breast cancer diagnosis.

</details>


### [346] [Pixel-wise Modulated Dice Loss for Medical Image Segmentation](https://arxiv.org/abs/2506.15744)
*Seyed Mohsen Hosseini*

Main category: eess.IV

TL;DR: 论文指出医疗分割任务中存在类别和难度两种数据不平衡问题，现有方法处理难度不平衡计算成本高且效果有限，提出像素调制骰子损失（PM Dice loss），成本低且效果好。


<details>
  <summary>Details</summary>
Motivation: 解决医疗分割任务中类别不平衡和难度不平衡导致的神经网络训练无效问题，且改进现有处理难度不平衡方法计算成本高、效果有限的状况。

Method: 对骰子损失进行简单修改，引入像素级调制项，利用骰子损失处理类别不平衡的有效性来处理难度不平衡。

Result: 在三个常用医疗分割任务上，提出的PM Dice loss优于其他处理难度不平衡问题的方法。

Conclusion: 提出的PM Dice loss能有效解决医疗分割任务中的数据不平衡问题，且计算成本低。

Abstract: Class imbalance and the difficulty imbalance are the two types of data
imbalance that affect the performance of neural networks in medical
segmentation tasks. In class imbalance the loss is dominated by the majority
classes and in difficulty imbalance the loss is dominated by easy to classify
pixels. This leads to an ineffective training. Dice loss, which is based on a
geometrical metric, is very effective in addressing the class imbalance
compared to the cross entropy (CE) loss, which is adopted directly from
classification tasks. To address the difficulty imbalance, the common approach
is employing a re-weighted CE loss or a modified Dice loss to focus the
training on difficult to classify areas. The existing modification methods are
computationally costly and with limited success. In this study we propose a
simple modification to the Dice loss with minimal computational cost. With a
pixel level modulating term, we take advantage of the effectiveness of Dice
loss in handling the class imbalance to also handle the difficulty imbalance.
Results on three commonly used medical segmentation tasks show that the
proposed Pixel-wise Modulated Dice loss (PM Dice loss) outperforms other
methods, which are designed to tackle the difficulty imbalance problem.

</details>


### [347] [Implicit neural representations for accurate estimation of the standard model of white matter](https://arxiv.org/abs/2506.15762)
*Tom Hendriks,Gerrit Arends,Edwin Versteeg,Anna Vilanova,Maxime Chamberland,Chantal M. W. Tax*

Main category: eess.IV

TL;DR: 本文引入基于隐式神经表示（INR）的新估计框架用于扩散磁共振成像（dMRI）标准模型（SM）参数估计，在准确性、抗噪性等方面表现出色，是分析dMRI数据的潜在重要工具。


<details>
  <summary>Details</summary>
Motivation: dMRI的SM模型因高维特性，参数估计需大量采集协议且受噪声影响，准确估计有挑战，因此需新的估计方法。

Method: 引入基于INR的估计框架，通过输入坐标的正弦编码引入空间正则化，并与立方多项式、监督神经网络和非线性最小二乘法对比。

Result: INR方法在估计SM参数上准确性更高，尤其在低信噪比条件下，能连续表示数据集，全无监督无需标注数据，推理快，抗噪性强，支持联合估计及处理空间变化采集协议。

Conclusion: INR结合多种优势且易适应其他dMRI模型，是分析和解释dMRI数据的潜在重要工具。

Abstract: Diffusion magnetic resonance imaging (dMRI) enables non-invasive
investigation of tissue microstructure. The Standard Model (SM) of white matter
aims to disentangle dMRI signal contributions from intra- and extra-axonal
water compartments. However, due to the model its high-dimensional nature,
extensive acquisition protocols with multiple b-values and diffusion tensor
shapes are typically required to mitigate parameter degeneracies. Even then,
accurate estimation remains challenging due to noise. This work introduces a
novel estimation framework based on implicit neural representations (INRs),
which incorporate spatial regularization through the sinusoidal encoding of the
input coordinates. The INR method is evaluated on both synthetic and in vivo
datasets and compared to parameter estimates using cubic polynomials,
supervised neural networks, and nonlinear least squares. Results demonstrate
superior accuracy of the INR method in estimating SM parameters, particularly
in low signal-to-noise conditions. Additionally, spatial upsampling of the INR
can represent the underlying dataset anatomically plausibly in a continuous
way, which is unattainable with linear or cubic interpolation. The INR is fully
unsupervised, eliminating the need for labeled training data. It achieves fast
inference ($\sim$6 minutes), is robust to both Gaussian and Rician noise,
supports joint estimation of SM kernel parameters and the fiber orientation
distribution function with spherical harmonics orders up to at least 8 and
non-negativity constraints, and accommodates spatially varying acquisition
protocols caused by magnetic gradient non-uniformities. The combination of
these properties along with the possibility to easily adapt the framework to
other dMRI models, positions INRs as a potentially important tool for analyzing
and interpreting diffusion MRI data.

</details>


### [348] [Robust Training with Data Augmentation for Medical Imaging Classification](https://arxiv.org/abs/2506.17133)
*Josué Martínez-Martínez,Olivia Brown,Mostafa Karami,Sheida Nabavi*

Main category: eess.IV

TL;DR: 提出鲁棒训练算法RTDA减轻医学图像分类模型对对抗攻击和分布偏移的脆弱性，实验表明其有更好鲁棒性和泛化性能。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络用于医学图像诊断时易受对抗攻击和分布偏移影响，影响诊断可靠性和医生信任，需减轻其脆弱性。

Method: 提出鲁棒训练算法RTDA，并与六种基线技术在三种成像技术的数据集上对比。

Result: RTDA在每个图像分类任务中，对抗攻击鲁棒性更好，分布偏移时泛化性能更佳，且保持高清洁准确率。

Conclusion: RTDA能有效减轻医学图像分类模型对对抗攻击和分布偏移的脆弱性。

Abstract: Deep neural networks are increasingly being used to detect and diagnose
medical conditions using medical imaging. Despite their utility, these models
are highly vulnerable to adversarial attacks and distribution shifts, which can
affect diagnostic reliability and undermine trust among healthcare
professionals. In this study, we propose a robust training algorithm with data
augmentation (RTDA) to mitigate these vulnerabilities in medical image
classification. We benchmark classifier robustness against adversarial
perturbations and natural variations of RTDA and six competing baseline
techniques, including adversarial training and data augmentation approaches in
isolation and combination, using experimental data sets with three different
imaging technologies (mammograms, X-rays, and ultrasound). We demonstrate that
RTDA achieves superior robustness against adversarial attacks and improved
generalization performance in the presence of distribution shift in each image
classification task while maintaining high clean accuracy.

</details>


### [349] [MeDi: Metadata-Guided Diffusion Models for Mitigating Biases in Tumor Classification](https://arxiv.org/abs/2506.17140)
*David Jacob Drexlin,Jonas Dippel,Julius Hense,Niklas Prenißl,Grégoire Montavon,Frederick Klauschen,Klaus-Robert Müller*

Main category: eess.IV

TL;DR: 提出元数据引导生成扩散模型框架MeDi，可生成高质量组织病理图像，缓解数据偏差，提升下游分类器性能。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在临床实践应用中对不同条件缺乏鲁棒性，存在捷径学习和预测偏差问题，大规模基础模型也未完全解决。

Method: 提出将元数据显式建模到元数据引导生成扩散模型框架MeDi中，用合成数据对代表性不足的子群体进行有针对性的增强。

Result: MeDi能为TCGA中未见子群体生成高质量组织病理图像，提高生成图像的整体保真度，提升下游分类器在有子群体偏移数据集上的性能。

Conclusion: 这是利用生成模型更好缓解数据偏差的概念验证工作。

Abstract: Deep learning models have made significant advances in histological
prediction tasks in recent years. However, for adaptation in clinical practice,
their lack of robustness to varying conditions such as staining, scanner,
hospital, and demographics is still a limiting factor: if trained on
overrepresented subpopulations, models regularly struggle with less frequent
patterns, leading to shortcut learning and biased predictions. Large-scale
foundation models have not fully eliminated this issue. Therefore, we propose a
novel approach explicitly modeling such metadata into a Metadata-guided
generative Diffusion model framework (MeDi). MeDi allows for a targeted
augmentation of underrepresented subpopulations with synthetic data, which
balances limited training data and mitigates biases in downstream models. We
experimentally show that MeDi generates high-quality histopathology images for
unseen subpopulations in TCGA, boosts the overall fidelity of the generated
images, and enables improvements in performance for downstream classifiers on
datasets with subpopulation shifts. Our work is a proof-of-concept towards
better mitigating data biases with generative models.

</details>


### [350] [Proportional Sensitivity in Generative Adversarial Network (GAN)-Augmented Brain Tumor Classification Using Convolutional Neural Network](https://arxiv.org/abs/2506.17165)
*Mahin Montasir Afif,Abdullah Al Noman,K. M. Tahsin Kabir,Md. Mortuza Ahmmed,Md. Mostafizur Rahman,Mufti Mahmud,Md. Ashraful Babu*

Main category: eess.IV

TL;DR: 研究不同比例GAN生成与真实脑肿瘤MRI图像对CNN分类性能的影响，发现适量GAN数据可提升性能，过多则会使性能下降。


<details>
  <summary>Details</summary>
Motivation: 探索GAN扩展有限医学影像数据集时，不同比例的GAN生成与真实脑肿瘤MRI图像对CNN分类性能的影响。

Method: 用DCGAN创建合成图像，以不同比例与真实图像混合训练自定义CNN，在独立真实测试集上评估。

Result: 模型在以合成数据为主训练时仍保持高灵敏度和精度；少量添加GAN数据（如900张真实+100张GAN）时性能优，测试准确率达95.2%；GAN图像比例增加，性能逐渐下降。

Conclusion: GAN可用于扩充有限数据集，尤其是真实数据稀缺时，但过多合成数据会引入伪影，影响模型泛化能力。

Abstract: Generative Adversarial Networks (GAN) have shown potential in expanding
limited medical imaging datasets. This study explores how different ratios of
GAN-generated and real brain tumor MRI images impact the performance of a CNN
in classifying healthy vs. tumorous scans. A DCGAN was used to create synthetic
images which were mixed with real ones at various ratios to train a custom CNN.
The CNN was then evaluated on a separate real-world test set. Our results
indicate that the model maintains high sensitivity and precision in tumor
classification, even when trained predominantly on synthetic data. When only a
small portion of GAN data was added, such as 900 real images and 100 GAN
images, the model achieved excellent performance, with test accuracy reaching
95.2%, and precision, recall, and F1-score all exceeding 95%. However, as the
proportion of GAN images increased further, performance gradually declined.
This study suggests that while GANs are useful for augmenting limited datasets
especially when real data is scarce, too much synthetic data can introduce
artifacts that affect the model's ability to generalize to real world cases.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [351] [Spatially-Aware Evaluation of Segmentation Uncertainty](https://arxiv.org/abs/2506.16589)
*Tal Zeevi,Eléonore V. Lieffrig,Lawrence H. Staib,John A. Onofrey*

Main category: cs.CV

TL;DR: 提出三种空间感知指标用于医学图像分割不确定性评估，在前列腺分区分割数据上验证效果良好。


<details>
  <summary>Details</summary>
Motivation: 现有不确定性评估指标忽略空间上下文和解剖结构，可能对不同模式给出相同分数。

Method: 提出三种结合结构和边界信息的空间感知指标，并在医学分割十项全能中的前列腺分区分割挑战的医学影像数据上进行验证。

Result: 结果显示与临床重要因素的一致性提高，能更好区分有意义和虚假的不确定性模式。

Conclusion: 所提出的空间感知指标在医学图像分割不确定性评估中表现良好。

Abstract: Uncertainty maps highlight unreliable regions in segmentation predictions.
However, most uncertainty evaluation metrics treat voxels independently,
ignoring spatial context and anatomical structure. As a result, they may assign
identical scores to qualitatively distinct patterns (e.g., scattered vs.
boundary-aligned uncertainty). We propose three spatially aware metrics that
incorporate structural and boundary information and conduct a thorough
validation on medical imaging data from the prostate zonal segmentation
challenge within the Medical Segmentation Decathlon. Our results demonstrate
improved alignment with clinically important factors and better discrimination
between meaningful and spurious uncertainty patterns.

</details>


### [352] [Stretching Beyond the Obvious: A Gradient-Free Framework to Unveil the Hidden Landscape of Visual Invariance](https://arxiv.org/abs/2506.17040)
*Lorenzo Tausani,Paolo Muratore,Morgan B. Talbot,Giacomo Amerio,Gabriel Kreiman,Davide Zoccolan*

Main category: cs.CV

TL;DR: 提出Stretch - and - Squeeze (SnS)框架刻画视觉单元不变性和对抗扰动脆弱性，应用于CNN有新发现并支持鲁棒CNN作为视觉系统模型的高保真度。


<details>
  <summary>Details</summary>
Motivation: 现有特征可视化方法不足以揭示视觉单元响应不变性的变换流形，为理解图像如何转换为支持识别的表征，需新方法来刻画单元不变性和对抗扰动脆弱性。

Method: 引入SnS框架，将变换问题构建为双目标优化问题，分别通过寻找最大改变参考刺激表示但保留单元激活的扰动来探测不变性，寻找最小改变刺激但抑制单元激活的扰动来探测对抗敏感性。

Result: 应用于CNN时，SnS发现的图像变化在像素空间上比仿射变换产生的变化离参考图像更远，且更能保留目标单元响应；不同图像表示用于优化时，发现的不变图像有显著差异；鲁棒网络的不变图像比标准网络的更易被人类识别。

Conclusion: SnS框架可有效刻画生物和人工视觉系统中单元的不变性和对抗扰动脆弱性，鲁棒CNN作为视觉系统模型具有更高的保真度。

Abstract: Uncovering which features' combinations high-level visual units encode is
critical to understand how images are transformed into representations that
support recognition. While existing feature visualization approaches typically
infer a unit's most exciting images, this is insufficient to reveal the
manifold of transformations under which responses remain invariant, which is
key to generalization in vision. Here we introduce Stretch-and-Squeeze (SnS),
an unbiased, model-agnostic, and gradient-free framework to systematically
characterize a unit's invariance landscape and its vulnerability to adversarial
perturbations in both biological and artificial visual systems. SnS frames
these transformations as bi-objective optimization problems. To probe
invariance, SnS seeks image perturbations that maximally alter the
representation of a reference stimulus in a given processing stage while
preserving unit activation. To probe adversarial sensitivity, SnS seeks
perturbations that minimally alter the stimulus while suppressing unit
activation. Applied to convolutional neural networks (CNNs), SnS revealed image
variations that were further from a reference image in pixel-space than those
produced by affine transformations, while more strongly preserving the target
unit's response. The discovered invariant images differed dramatically
depending on the choice of image representation used for optimization:
pixel-level changes primarily affected luminance and contrast, while stretching
mid- and late-layer CNN representations altered texture and pose respectively.
Notably, the invariant images from robust networks were more recognizable by
human subjects than those from standard networks, supporting the higher
fidelity of robust CNNs as models of the visual system.

</details>


### [353] [MoiréXNet: Adaptive Multi-Scale Demoiréing with Linear Attention Test-Time Training and Truncated Flow Matching Prior](https://arxiv.org/abs/2506.15929)
*Liangyan Li,Yimo Ning,Kevin Le,Wei Dong,Yunzhe Li,Jun Chen,Xiaohong Liu*

Main category: cs.CV

TL;DR: 本文提出结合MAP估计与深度学习技术的图像和视频去摩尔纹新框架，包含监督学习模型与TFMP，提升恢复性能。


<details>
  <summary>Details</summary>
Motivation: 现有去摩尔纹方法存在无法完全去除摩尔纹、结果过平滑、处理非线性退化能力不足及引入伪影等问题。

Method: 提出混合MAP框架，包括带TTT模块的监督学习模型和TFMP，结合线性注意力计算效率与生成模型细化能力。

Result: 所提框架结合了线性注意力的计算效率和生成模型的细化能力。

Conclusion: 所提框架改善了图像和视频去摩尔纹的恢复性能。

Abstract: This paper introduces a novel framework for image and video demoir\'eing by
integrating Maximum A Posteriori (MAP) estimation with advanced deep learning
techniques. Demoir\'eing addresses inherently nonlinear degradation processes,
which pose significant challenges for existing methods.
  Traditional supervised learning approaches either fail to remove moir\'e
patterns completely or produce overly smooth results. This stems from
constrained model capacity and scarce training data, which inadequately
represent the clean image distribution and hinder accurate reconstruction of
ground-truth images. While generative models excel in image restoration for
linear degradations, they struggle with nonlinear cases such as demoir\'eing
and often introduce artifacts.
  To address these limitations, we propose a hybrid MAP-based framework that
integrates two complementary components. The first is a supervised learning
model enhanced with efficient linear attention Test-Time Training (TTT)
modules, which directly learn nonlinear mappings for RAW-to-sRGB demoir\'eing.
The second is a Truncated Flow Matching Prior (TFMP) that further refines the
outputs by aligning them with the clean image distribution, effectively
restoring high-frequency details and suppressing artifacts. These two
components combine the computational efficiency of linear attention with the
refinement abilities of generative models, resulting in improved restoration
performance.

</details>


### [354] [Beyond Audio and Pose: A General-Purpose Framework for Video Synchronization](https://arxiv.org/abs/2506.15937)
*Yosub Shin,Igor Molybog*

Main category: cs.CV

TL;DR: 本文提出VideoSync视频同步框架，在新数据集上评估，指出先前工作偏差，证明其优于现有方法，确定CNN模型最有效，使视频同步更具通用性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 先前视频同步工作依赖音频或特定视觉事件，适用性受限，且现有基准缺乏通用性和可重复性。

Method: 引入独立于特定特征提取方法的VideoSync框架，在新数据集上评估，纠正先前工作偏差，探索多种同步偏移预测方法。

Result: VideoSync在公平实验条件下优于现有方法，CNN模型是最有效的同步偏移预测方法。

Conclusion: 研究使视频同步突破特定领域限制，更适用于现实世界应用。

Abstract: Video synchronization-aligning multiple video streams capturing the same
event from different angles-is crucial for applications such as reality TV show
production, sports analysis, surveillance, and autonomous systems. Prior work
has heavily relied on audio cues or specific visual events, limiting
applicability in diverse settings where such signals may be unreliable or
absent. Additionally, existing benchmarks for video synchronization lack
generality and reproducibility, restricting progress in the field. In this
work, we introduce VideoSync, a video synchronization framework that operates
independently of specific feature extraction methods, such as human pose
estimation, enabling broader applicability across different content types. We
evaluate our system on newly composed datasets covering single-human,
multi-human, and non-human scenarios, providing both the methodology and code
for dataset creation to establish reproducible benchmarks. Our analysis reveals
biases in prior SOTA work, particularly in SeSyn-Net's preprocessing pipeline,
leading to inflated performance claims. We correct these biases and propose a
more rigorous evaluation framework, demonstrating that VideoSync outperforms
existing approaches, including SeSyn-Net, under fair experimental conditions.
Additionally, we explore various synchronization offset prediction methods,
identifying a convolutional neural network (CNN)-based model as the most
effective. Our findings advance video synchronization beyond domain-specific
constraints, making it more generalizable and robust for real-world
applications.

</details>


### [355] [Heterogeneous-Modal Unsupervised Domain Adaptation via Latent Space Bridging](https://arxiv.org/abs/2506.15971)
*Jiawen Yang,Shuhao Chen,Yucong Duan,Ke Tang,Yu Zhang*

Main category: cs.CV

TL;DR: 提出异构模态无监督域适应（HMUDA）设置及潜在空间桥接（LSB）框架用于语义分割，实验表明LSB达最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有无监督域适应方法在源域和目标域属于完全不同模态时效果不佳，需解决该局限。

Method: 提出HMUDA设置，利用含两种模态未标记样本的桥接域实现知识迁移；提出LSB框架，采用双分支架构，结合特征一致性损失和域对齐损失。

Result: 在六个基准数据集上的大量实验表明LSB达到了最优性能。

Conclusion: 提出的HMUDA设置和LSB框架能有效解决不同模态间的无监督域适应问题，且性能表现优秀。

Abstract: Unsupervised domain adaptation (UDA) methods effectively bridge domain gaps
but become struggled when the source and target domains belong to entirely
distinct modalities. To address this limitation, we propose a novel setting
called Heterogeneous-Modal Unsupervised Domain Adaptation (HMUDA), which
enables knowledge transfer between completely different modalities by
leveraging a bridge domain containing unlabeled samples from both modalities.
To learn under the HMUDA setting, we propose Latent Space Bridging (LSB), a
specialized framework designed for the semantic segmentation task.
Specifically, LSB utilizes a dual-branch architecture, incorporating a feature
consistency loss to align representations across modalities and a domain
alignment loss to reduce discrepancies between class centroids across domains.
Extensive experiments conducted on six benchmark datasets demonstrate that LSB
achieves state-of-the-art performance.

</details>


### [356] [Advanced Sign Language Video Generation with Compressed and Quantized Multi-Condition Tokenization](https://arxiv.org/abs/2506.15980)
*Cong Wang,Zexuan Deng,Zhiwei Jiang,Fei Shen,Yafeng Yin,Shiwei Gan,Zifeng Cheng,Shiping Ge,Qing Gu*

Main category: cs.CV

TL;DR: 提出SignViP框架用于手语视频生成，结合多细粒度条件，实验表现达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有手语视频生成方法主要依赖单一粗粒度条件，限制生成视频的自然度和表现力。

Method: 提出SignViP框架，包含三个核心组件，采用离散标记化范式整合和表示细粒度条件，推理时依次进行文本到标记、标记到嵌入、嵌入到视频的转换。

Result: SignViP在视频质量、时间连贯性和语义保真度等指标上达到了当前最优性能。

Conclusion: SignViP框架有效提升了手语视频生成的效果。

Abstract: Sign Language Video Generation (SLVG) seeks to generate identity-preserving
sign language videos from spoken language texts. Existing methods primarily
rely on the single coarse condition (\eg, skeleton sequences) as the
intermediary to bridge the translation model and the video generation model,
which limits both the naturalness and expressiveness of the generated videos.
To overcome these limitations, we propose SignViP, a novel SLVG framework that
incorporates multiple fine-grained conditions for improved generation fidelity.
Rather than directly translating error-prone high-dimensional conditions,
SignViP adopts a discrete tokenization paradigm to integrate and represent
fine-grained conditions (\ie, fine-grained poses and 3D hands). SignViP
contains three core components. (1) Sign Video Diffusion Model is jointly
trained with a multi-condition encoder to learn continuous embeddings that
encapsulate fine-grained motion and appearance. (2) Finite Scalar Quantization
(FSQ) Autoencoder is further trained to compress and quantize these embeddings
into discrete tokens for compact representation of the conditions. (3)
Multi-Condition Token Translator is trained to translate spoken language text
to discrete multi-condition tokens. During inference, Multi-Condition Token
Translator first translates the spoken language text into discrete
multi-condition tokens. These tokens are then decoded to continuous embeddings
by FSQ Autoencoder, which are subsequently injected into Sign Video Diffusion
Model to guide video generation. Experimental results show that SignViP
achieves state-of-the-art performance across metrics, including video quality,
temporal coherence, and semantic fidelity. The code is available at
https://github.com/umnooob/signvip/.

</details>


### [357] [DIGMAPPER: A Modular System for Automated Geologic Map Digitization](https://arxiv.org/abs/2506.16006)
*Weiwei Duan,Michael P. Gerlek,Steven N. Minton,Craig A. Knoblock,Fandel Lin,Theresa Chen,Leeje Jang,Sofia Kirsanova,Zekun Li,Yijun Lin,Yao-Yi Chiang*

Main category: cs.CV

TL;DR: 介绍DIGMAPPER系统自动数字化地质图，有创新技术，评估效果好且已部署加速数据创建。


<details>
  <summary>Details</summary>
Motivation: 历史地质图含重要信息，但数字化任务耗人力时间，需自动化方法。

Method: 开发模块化、可扩展的DIGMAPPER系统，采用全容器化、工作流编排架构，集成深度学习模型，运用上下文学习、合成数据生成和基于Transformer的模型。

Result: 在超100张标注地图评估中，多边形、线和点特征提取精度高，地理参考性能可靠。

Conclusion: DIGMAPPER部署在USGS，显著加速分析就绪的地理空间数据集创建，支持关键矿产评估和地质科学应用。

Abstract: Historical geologic maps contain rich geospatial information, such as rock
units, faults, folds, and bedding planes, that is critical for assessing
mineral resources essential to renewable energy, electric vehicles, and
national security. However, digitizing maps remains a labor-intensive and
time-consuming task. We present DIGMAPPER, a modular, scalable system developed
in collaboration with the United States Geological Survey (USGS) to automate
the digitization of geologic maps. DIGMAPPER features a fully dockerized,
workflow-orchestrated architecture that integrates state-of-the-art deep
learning models for map layout analysis, feature extraction, and
georeferencing. To overcome challenges such as limited training data and
complex visual content, our system employs innovative techniques, including
in-context learning with large language models, synthetic data generation, and
transformer-based models. Evaluations on over 100 annotated maps from the
DARPA-USGS dataset demonstrate high accuracy across polygon, line, and point
feature extraction, and reliable georeferencing performance. Deployed at USGS,
DIGMAPPER significantly accelerates the creation of analysis-ready geospatial
datasets, supporting national-scale critical mineral assessments and broader
geoscientific applications.

</details>


### [358] [GRPO-CARE: Consistency-Aware Reinforcement Learning for Multimodal Reasoning](https://arxiv.org/abs/2506.16141)
*Yi Chen,Yuying Ge,Rui Wang,Yixiao Ge,Junhao Cheng,Ying Shan,Xihui Liu*

Main category: cs.CV

TL;DR: 提出SEED - Bench - R1基准测试评估MLLM后训练方法，发现标准GRPO问题，提出GRPO - CARE框架解决问题并提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法对多模态大语言模型（MLLMs）的适应性未被探索，且缺乏对MLLM后训练方法的严格评估。

Method: 引入SEED - Bench - R1基准测试；提出GRPO - CARE一致性感知的强化学习框架，采用两层奖励机制。

Result: GRPO - CARE在SEED - Bench - R1上表现优于标准GRPO，在最难评估级别上性能提升6.7%，一致性提升24.5%，且有强迁移性。

Conclusion: 贡献了系统设计的基准和可泛化的后训练框架，推动了更具可解释性和鲁棒性的MLLMs的发展。

Abstract: Recent reinforcement learning approaches, such as outcome-supervised GRPO,
have advanced Chain-of-Thought reasoning in large language models (LLMs), yet
their adaptation to multimodal LLMs (MLLMs) is unexplored. To address the lack
of rigorous evaluation for MLLM post-training methods, we introduce
SEED-Bench-R1, a benchmark with complex real-world videos requiring balanced
perception and reasoning. It offers a large training set and evaluates
generalization across three escalating challenges: in-distribution,
cross-environment, and cross-environment-task scenarios. Using SEED-Bench-R1,
we find that standard GRPO, while improving answer accuracy, often reduces
logical coherence between reasoning steps and answers, with only a 57.9%
consistency rate. This stems from reward signals focusing solely on final
answers, encouraging shortcuts, and strict KL penalties limiting exploration.To
address this, we propose GRPO-CARE, a consistency-aware RL framework optimizing
both answer correctness and reasoning coherence without explicit supervision.
GRPO-CARE introduces a two-tiered reward: (1) a base reward for answer
correctness, and (2) an adaptive consistency bonus, computed by comparing the
model's reasoning-to-answer likelihood (via a slowly-evolving reference model)
against group peers.This dual mechanism amplifies rewards for reasoning paths
that are both correct and logically consistent. Replacing KL penalties with
this adaptive bonus, GRPO-CARE outperforms standard GRPO on SEED-Bench-R1,
achieving a 6.7% performance gain on the hardest evaluation level and a 24.5%
improvement in consistency. It also shows strong transferability, improving
model performance across diverse video understanding benchmarks. Our work
contributes a systematically designed benchmark and a generalizable
post-training framework, advancing the development of more interpretable and
robust MLLMs.

</details>


### [359] [SycnMapV2: Robust and Adaptive Unsupervised Segmentation](https://arxiv.org/abs/2506.16297)
*Heng Zhang,Zikang Wan,Danilo Vasconcellos Vargas*

Main category: cs.CV

TL;DR: 提出SyncMapV2算法解决无监督分割问题，在多种干扰下表现出高鲁棒性，无需额外训练和损失函数，能在线自适应。


<details>
  <summary>Details</summary>
Motivation: 现有AI算法在噪声环境下难以保持准确性，而人类视觉在分割视觉线索方面表现出色，希望开发出具有高鲁棒性的无监督分割算法。

Method: 基于自组织动力学方程和随机网络概念的学习范式，且能在线自适应。

Result: SyncMapV2在数字损坏、噪声、天气、模糊等情况下的mIoU下降远低于SOTA方法，自适应测试中性能几乎无下降。

Conclusion: SyncMapV2是首个能在线完成上述任务的算法，有望推动新一代鲁棒自适应智能的发展。

Abstract: Human vision excels at segmenting visual cues without the need for explicit
training, and it remains remarkably robust even as noise severity increases. In
contrast, existing AI algorithms struggle to maintain accuracy under similar
conditions. Here, we present SyncMapV2, the first to solve unsupervised
segmentation with state-of-the-art robustness. SyncMapV2 exhibits a minimal
drop in mIoU, only 0.01%, under digital corruption, compared to a 23.8% drop
observed in SOTA methods.This superior performance extends across various types
of corruption: noise (7.3% vs. 37.7%), weather (7.5% vs. 33.8%), and blur (7.0%
vs. 29.5%). Notably, SyncMapV2 accomplishes this without any robust training,
supervision, or loss functions. It is based on a learning paradigm that uses
self-organizing dynamical equations combined with concepts from random
networks. Moreover,unlike conventional methods that require re-initialization
for each new input, SyncMapV2 adapts online, mimicking the continuous
adaptability of human vision. Thus, we go beyond the accurate and robust
results, and present the first algorithm that can do all the above online,
adapting to input rather than re-initializing. In adaptability tests, SyncMapV2
demonstrates near-zero performance degradation, which motivates and fosters a
new generation of robust and adaptive intelligence in the near future.

</details>


### [360] [Learning Multi-scale Spatial-frequency Features for Image Denoising](https://arxiv.org/abs/2506.16307)
*Xu Zhao,Chen Zhao,Xiantao Hu,Hongliang Zhang,Ying Tai,Jian Yang*

Main category: cs.CV

TL;DR: 提出多尺度自适应双域网络MADNet用于图像去噪，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有架构依赖固定单输入单输出Unet架构，忽略像素级多尺度表示，且对频域处理统一，忽略高低频噪声差异。

Method: 使用图像金字塔输入从低分辨率图像恢复无噪结果；设计自适应空频学习单元ASFU分离高低频信息；在跳跃连接中设计全局特征融合块增强不同尺度特征。

Result: 在合成和真实噪声图像数据集上的大量实验表明，MADNet比当前最先进的去噪方法更有效。

Conclusion: MADNet在图像去噪任务中具有有效性和优势。

Abstract: Recent advancements in multi-scale architectures have demonstrated
exceptional performance in image denoising tasks. However, existing
architectures mainly depends on a fixed single-input single-output Unet
architecture, ignoring the multi-scale representations of pixel level. In
addition, previous methods treat the frequency domain uniformly, ignoring the
different characteristics of high-frequency and low-frequency noise. In this
paper, we propose a novel multi-scale adaptive dual-domain network (MADNet) for
image denoising. We use image pyramid inputs to restore noise-free results from
low-resolution images. In order to realize the interaction of high-frequency
and low-frequency information, we design an adaptive spatial-frequency learning
unit (ASFU), where a learnable mask is used to separate the information into
high-frequency and low-frequency components. In the skip connections, we design
a global feature fusion block to enhance the features at different scales.
Extensive experiments on both synthetic and real noisy image datasets verify
the effectiveness of MADNet compared with current state-of-the-art denoising
approaches.

</details>


### [361] [Segment Anything for Satellite Imagery: A Strong Baseline and a Regional Dataset for Automatic Field Delineation](https://arxiv.org/abs/2506.16318)
*Carmelo Scribano,Elena Govi,Paolo bertellini,Simone Parisi,Giorgia Franchini,Marko Bertogna*

Main category: cs.CV

TL;DR: 本文提出基于SAM的农田边界划分管道，引入微调策略，采集新区域数据集ERAS，实验评估效果，为自动农田划分提供稳健基线。


<details>
  <summary>Details</summary>
Motivation: 精准绘制农田边界对农业高效运营至关重要，计算机视觉支持下从高分辨率卫星图像自动提取可避免昂贵实地调查。

Method: 提出基于Segment Anything Model (SAM)的农田划分管道，引入微调策略使SAM适应此任务，介绍采集覆盖现有数据源以外区域的互补区域数据集的方法。

Result: 通过大量实验评估了分割准确性和泛化能力，新区域数据集ERAS已公开。

Conclusion: 该方法为自动农田划分提供了稳健基线。

Abstract: Accurate mapping of agricultural field boundaries is essential for the
efficient operation of agriculture. Automatic extraction from high-resolution
satellite imagery, supported by computer vision techniques, can avoid costly
ground surveys. In this paper, we present a pipeline for field delineation
based on the Segment Anything Model (SAM), introducing a fine-tuning strategy
to adapt SAM to this task. In addition to using published datasets, we describe
a method for acquiring a complementary regional dataset that covers areas
beyond current sources. Extensive experiments assess segmentation accuracy and
evaluate the generalization capabilities. Our approach provides a robust
baseline for automated field delineation. The new regional dataset, known as
ERAS, is now publicly available.

</details>


### [362] [Reliable Few-shot Learning under Dual Noises](https://arxiv.org/abs/2506.16330)
*Ji Zhang,Jingkuan Song,Lianli Gao,Nicu Sebe,Heng Tao Shen*

Main category: cs.CV

TL;DR: 现有基于任务适配的少样本学习在开放世界因分布内和分布外噪声易失败，本文提出DETA++方法实现可靠少样本学习，实验证明其有效灵活。


<details>
  <summary>Details</summary>
Motivation: 现有基于任务适配的少样本学习方法在开放世界中，由于目标任务支持和查询样本存在分布内和分布外噪声，易导致任务适配时噪声影响被放大、模型预测不可靠，因此需要解决该问题。

Method: 提出DETA++方法，使用CoRA模块计算支持样本图像和区域权重，提出干净原型损失和噪声熵最大化损失实现抗噪声任务适配；用内存库存储和细化干净区域，设计LocalNCC对查询样本进行抗噪声预测；采用IntraSwap策略在任务适配时纠正分布内类别原型。

Result: 进行了大量实验。

Conclusion: DETA++方法具有有效性和灵活性。

Abstract: Recent advances in model pre-training give rise to task adaptation-based
few-shot learning (FSL), where the goal is to adapt a pre-trained task-agnostic
model for capturing task-specific knowledge with a few-labeled support samples
of the target task.Nevertheless, existing approaches may still fail in the open
world due to the inevitable in-distribution (ID) and out-of-distribution (OOD)
noise from both support and query samples of the target task. With limited
support samples available, i) the adverse effect of the dual noises can be
severely amplified during task adaptation, and ii) the adapted model can
produce unreliable predictions on query samples in the presence of the dual
noises. In this work, we propose DEnoised Task Adaptation (DETA++) for reliable
FSL. DETA++ uses a Contrastive Relevance Aggregation (CoRA) module to calculate
image and region weights for support samples, based on which a clean prototype
loss and a noise entropy maximization loss are proposed to achieve noise-robust
task adaptation. Additionally,DETA++ employs a memory bank to store and refine
clean regions for each inner-task class, based on which a Local Nearest
Centroid Classifier (LocalNCC) is devised to yield noise-robust predictions on
query samples. Moreover, DETA++ utilizes an Intra-class Region Swapping
(IntraSwap) strategy to rectify ID class prototypes during task adaptation,
enhancing the model's robustness to the dual noises. Extensive experiments
demonstrate the effectiveness and flexibility of DETA++.

</details>


### [363] [CLIP-MG: Guiding Semantic Attention with Skeletal Pose Features and RGB Data for Micro-Gesture Recognition on the iMiGUE Dataset](https://arxiv.org/abs/2506.16385)
*Santosh Patapati,Trisanth Srinivasan,Amith Adiraju*

Main category: cs.CV

TL;DR: 提出CLIP - MG模型用于微手势识别，在iMiGUE数据集上取得Top - 1准确率61.82%，展示了方法潜力与挑战。


<details>
  <summary>Details</summary>
Motivation: 微手势具有微妙、非自愿和低运动幅度的特点，微手势识别在情感计算中是一项具有挑战性的任务。

Method: 提出CLIP - MG模型，通过姿态引导的语义查询生成和门控多模态融合机制，将人体姿态信息集成到基于CLIP的识别流程中。

Result: 模型在iMiGUE数据集上实现了61.82%的Top - 1准确率。

Conclusion: 该方法有潜力，但在将CLIP等视觉 - 语言模型完全应用于微手势识别方面仍有困难。

Abstract: Micro-gesture recognition is a challenging task in affective computing due to
the subtle, involuntary nature of the gestures and their low movement
amplitude. In this paper, we introduce a Pose-Guided Semantics-Aware CLIP-based
architecture, or CLIP for Micro-Gesture recognition (CLIP-MG), a modified CLIP
model tailored for micro-gesture classification on the iMiGUE dataset. CLIP-MG
integrates human pose (skeleton) information into the CLIP-based recognition
pipeline through pose-guided semantic query generation and a gated multi-modal
fusion mechanism. The proposed model achieves a Top-1 accuracy of 61.82%. These
results demonstrate both the potential of our approach and the remaining
difficulty in fully adapting vision-language models like CLIP for micro-gesture
recognition.

</details>


### [364] [Robustness Evaluation of OCR-based Visual Document Understanding under Multi-Modal Adversarial Attacks](https://arxiv.org/abs/2506.16407)
*Dong Nguyen Tien,Dung D. Le*

Main category: cs.CV

TL;DR: 本文提出首个统一框架对基于OCR的VDU模型进行多模态对抗攻击生成与评估，实验表明行级攻击和复合扰动效果最差，PGD的BBox扰动表现更好。


<details>
  <summary>Details</summary>
Motivation: 现有VDU系统在现实对抗扰动下的鲁棒性研究不足。

Method: 提出统一框架，涵盖六种基于梯度的布局攻击场景，对OCR边界框、像素和文本进行操作，设置布局扰动预算约束。

Result: 行级攻击和复合扰动（BBox + Pixel + Text）使模型性能下降最严重，PGD的BBox扰动优于随机偏移基线，消融实验验证了布局预算、文本修改和对抗迁移性的影响。

Conclusion: 提出的框架可有效评估基于OCR的VDU模型在多模态对抗攻击下的鲁棒性。

Abstract: Visual Document Understanding (VDU) systems have achieved strong performance
in information extraction by integrating textual, layout, and visual signals.
However, their robustness under realistic adversarial perturbations remains
insufficiently explored. We introduce the first unified framework for
generating and evaluating multi-modal adversarial attacks on OCR-based VDU
models. Our method covers six gradient-based layout attack scenarios,
incorporating manipulations of OCR bounding boxes, pixels, and texts across
both word and line granularities, with constraints on layout perturbation
budget (e.g., IoU >= 0.6) to preserve plausibility.
  Experimental results across four datasets (FUNSD, CORD, SROIE, DocVQA) and
six model families demonstrate that line-level attacks and compound
perturbations (BBox + Pixel + Text) yield the most severe performance
degradation. Projected Gradient Descent (PGD)-based BBox perturbations
outperform random-shift baselines in all investigated models. Ablation studies
further validate the impact of layout budget, text modification, and
adversarial transferability.

</details>


### [365] [Efficient Transformations in Deep Learning Convolutional Neural Networks](https://arxiv.org/abs/2506.16418)
*Berk Yilmaz,Daniel Fidel Harvey,Prajit Dhuri*

Main category: cs.CV

TL;DR: 研究将FFT、WHT和DCT信号处理变换集成到ResNet50中用于图像分类，发现WHT能显著降低能耗并提高准确率。


<details>
  <summary>Details</summary>
Motivation: 评估在ResNet50训练和推理中，计算效率、能耗和分类准确率之间的权衡。

Method: 使用CIFAR - 100数据集进行实验，在ResNet50中集成FFT、WHT和DCT。

Result: 融入WHT显著降低能耗并提高准确率，基线模型测试准确率66%，能耗25,606 kJ/模型；早期卷积层融入WHT的模型准确率74%；早期和后期层都融入WHT的模型准确率79%，能耗仅39 kJ/模型。

Conclusion: WHT在能量受限的CNN应用中是高效且有效的方法。

Abstract: This study investigates the integration of signal processing transformations
-- Fast Fourier Transform (FFT), Walsh-Hadamard Transform (WHT), and Discrete
Cosine Transform (DCT) -- within the ResNet50 convolutional neural network
(CNN) model for image classification. The primary objective is to assess the
trade-offs between computational efficiency, energy consumption, and
classification accuracy during training and inference. Using the CIFAR-100
dataset (100 classes, 60,000 images), experiments demonstrated that
incorporating WHT significantly reduced energy consumption while improving
accuracy. Specifically, a baseline ResNet50 model achieved a testing accuracy
of 66%, consuming an average of 25,606 kJ per model. In contrast, a modified
ResNet50 incorporating WHT in the early convolutional layers achieved 74%
accuracy, and an enhanced version with WHT applied to both early and late
layers achieved 79% accuracy, with an average energy consumption of only 39 kJ
per model. These results demonstrate the potential of WHT as a highly efficient
and effective approach for energy-constrained CNN applications.

</details>


### [366] [Spotting tell-tale visual artifacts in face swapping videos: strengths and pitfalls of CNN detectors](https://arxiv.org/abs/2506.16497)
*Riccardo Ziglio,Cecilia Pasquini,Silvio Ranise*

Main category: cs.CV

TL;DR: 本文通过对基于CNN的数据驱动模型进行基准测试，研究利用换脸算法视觉伪影检测换脸的有效性，结果显示通用CNN架构在同数据源表现好，但跨数据集难稳健表征视觉线索，需专业检测策略。


<details>
  <summary>Details</summary>
Motivation: 视频流中的换脸操作对远程视频通信构成威胁，已有文献提出利用换脸算法在处理复杂物理场景（如面部遮挡）时引入的视觉伪影进行检测，本文旨在研究该方法的有效性。

Method: 在两个数据集（包括一个新收集的数据集）上对基于CNN的数据驱动模型进行基准测试，并分析其在不同采集源和换脸算法下的泛化能力。

Result: 通用CNN架构在同一数据源内表现出色，但在跨数据集时难以稳健地表征基于遮挡的视觉线索。

Conclusion: 需要专门的检测策略来处理此类伪影。

Abstract: Face swapping manipulations in video streams represents an increasing threat
in remote video communications, due to advances
  in automated and real-time tools. Recent literature proposes to characterize
and exploit visual artifacts introduced in video frames
  by swapping algorithms when dealing with challenging physical scenes, such as
face occlusions. This paper investigates the
  effectiveness of this approach by benchmarking CNN-based data-driven models
on two data corpora (including a newly collected
  one) and analyzing generalization capabilities with respect to different
acquisition sources and swapping algorithms. The results
  confirm excellent performance of general-purpose CNN architectures when
operating within the same data source, but a significant
  difficulty in robustly characterizing occlusion-based visual cues across
datasets. This highlights the need for specialized detection
  strategies to deal with such artifacts.

</details>


### [367] [Hunyuan3D 2.5: Towards High-Fidelity 3D Assets Generation with Ultimate Details](https://arxiv.org/abs/2506.16504)
*Zeqiang Lai,Yunfei Zhao,Haolin Liu,Zibo Zhao,Qingxiang Lin,Huiwen Shi,Xianghui Yang,Mingxin Yang,Shuhui Yang,Yifei Feng,Sheng Zhang,Xin Huang,Di Luo,Fan Yang,Fang Yang,Lifu Wang,Sicong Liu,Yixuan Tang,Yulin Cai,Zebin He,Tian Liu,Yuhong Liu,Jie Jiang,Linus,Jingwei Huang,Chunchao Guo*

Main category: cs.CV

TL;DR: 介绍Hunyuan3D 2.5这一3D扩散模型套件，在形状和纹理生成上有显著进步，优于先前方法。


<details>
  <summary>Details</summary>
Motivation: 生成高保真、细节丰富的纹理3D资产。

Method: 形状生成引入新的形状基础模型LATTICE，用大规模高质量数据集等训练；纹理生成通过基于物理的渲染（PBR）和从Hunyuan3D 2.0 Paint模型扩展的多视图架构进行升级。

Result: 最大模型达10B参数，能生成清晰详细3D形状，保持网格表面干净光滑，缩小与手工3D形状差距；在形状和端到端纹理生成上显著优于先前方法。

Conclusion: Hunyuan3D 2.5在3D资产生成上表现出色，在形状和纹理生成方面有显著提升。

Abstract: In this report, we present Hunyuan3D 2.5, a robust suite of 3D diffusion
models aimed at generating high-fidelity and detailed textured 3D assets.
Hunyuan3D 2.5 follows two-stages pipeline of its previous version Hunyuan3D
2.0, while demonstrating substantial advancements in both shape and texture
generation. In terms of shape generation, we introduce a new shape foundation
model -- LATTICE, which is trained with scaled high-quality datasets,
model-size, and compute. Our largest model reaches 10B parameters and generates
sharp and detailed 3D shape with precise image-3D following while keeping mesh
surface clean and smooth, significantly closing the gap between generated and
handcrafted 3D shapes. In terms of texture generation, it is upgraded with
phyiscal-based rendering (PBR) via a novel multi-view architecture extended
from Hunyuan3D 2.0 Paint model. Our extensive evaluation shows that Hunyuan3D
2.5 significantly outperforms previous methods in both shape and end-to-end
texture generation.

</details>


### [368] [From Semantic To Instance: A Semi-Self-Supervised Learning Approach](https://arxiv.org/abs/2506.16563)
*Keyhan Najafian,Farhad Maleki,Lingling Jin,Ian Stavness*

Main category: cs.CV

TL;DR: 提出半自监督学习方法解决实例分割标注难题，在小麦头和COCO数据集表现出色。


<details>
  <summary>Details</summary>
Motivation: 创建大规模像素级标注数据集困难，限制深度学习在实例分割领域应用，特别是农业中密集、自遮挡对象图像。

Method: 提出半自监督学习方法，设计GLMask图像 - 掩码表示，开发生成语义分割再转换为实例分割的管道。

Result: 在小麦头实例分割模型mAP@50达98.5%，在COCO数据集mAP@50提升超12.6%。

Conclusion: 该方法性能优于传统模型，不仅适用于精准农业，也适用于其他有类似数据特征的领域。

Abstract: Instance segmentation is essential for applications such as automated
monitoring of plant health, growth, and yield. However, extensive effort is
required to create large-scale datasets with pixel-level annotations of each
object instance for developing instance segmentation models that restrict the
use of deep learning in these areas. This challenge is more significant in
images with densely packed, self-occluded objects, which are common in
agriculture. To address this challenge, we propose a semi-self-supervised
learning approach that requires minimal manual annotation to develop a
high-performing instance segmentation model. We design GLMask, an image-mask
representation for the model to focus on shape, texture, and pattern while
minimizing its dependence on color features. We develop a pipeline to generate
semantic segmentation and then transform it into instance-level segmentation.
The proposed approach substantially outperforms the conventional instance
segmentation models, establishing a state-of-the-art wheat head instance
segmentation model with mAP@50 of 98.5%. Additionally, we assessed the proposed
methodology on the general-purpose Microsoft COCO dataset, achieving a
significant performance improvement of over 12.6% mAP@50. This highlights that
the utility of our proposed approach extends beyond precision agriculture and
applies to other domains, specifically those with similar data characteristics.

</details>


### [369] [How to Train your Text-to-Image Model: Evaluating Design Choices for Synthetic Training Captions](https://arxiv.org/abs/2506.16679)
*Manuel Brack,Sudeep Katakol,Felix Friedrich,Patrick Schramowski,Hareesh Ravi,Kristian Kersting,Ajinkya Kale*

Main category: cs.CV

TL;DR: 研究不同合成字幕策略对文生图模型下游性能的影响，发现不同策略各有优劣，强调字幕设计重要性。


<details>
  <summary>Details</summary>
Motivation: 当前文献缺乏对合成训练字幕设计选择的研究，本研究旨在填补这一空白。

Method: 系统地研究不同合成字幕策略对文生图模型下游性能的影响。

Result: 密集高质量字幕增强文本对齐但影响输出美感和多样性；随机长度字幕在美感和对齐方面有平衡提升且不降低样本多样性；改变字幕分布会使训练模型的输出偏差显著变化。

Conclusion: 强调字幕设计对实现模型最佳性能的重要性，并为文生图训练数据策略提供实用见解。

Abstract: Training data is at the core of any successful text-to-image models. The
quality and descriptiveness of image text are crucial to a model's performance.
Given the noisiness and inconsistency in web-scraped datasets, recent works
shifted towards synthetic training captions. While this setup is generally
believed to produce more capable models, current literature does not provide
any insights into its design choices. This study closes this gap by
systematically investigating how different synthetic captioning strategies
impact the downstream performance of text-to-image models. Our experiments
demonstrate that dense, high-quality captions enhance text alignment but may
introduce trade-offs in output aesthetics and diversity. Conversely, captions
of randomized lengths yield balanced improvements across aesthetics and
alignment without compromising sample diversity. We also demonstrate that
varying caption distributions introduce significant shifts in the output bias
of a trained model. Our findings underscore the importance of caption design in
achieving optimal model performance and provide practical insights for more
effective training data strategies in text-to-image generation.

</details>


### [370] [PQCAD-DM: Progressive Quantization and Calibration-Assisted Distillation for Extremely Efficient Diffusion Model](https://arxiv.org/abs/2506.16776)
*Beomseok Ko,Hyeryung Jang*

Main category: cs.CV

TL;DR: 本文提出PQCAD - DM混合压缩框架解决扩散模型计算和资源密集问题，减半推理时间且性能有竞争力。


<details>
  <summary>Details</summary>
Motivation: 扩散模型依赖迭代马尔可夫链过程，计算和资源密集，误差积累且朴素压缩技术效果有限。

Method: 提出PQCAD - DM框架，结合渐进量化（PQ）和校准辅助蒸馏（CAD）。PQ采用两阶段量化和基于动量机制的自适应位宽转换；CAD在蒸馏时利用全精度校准数据集。

Result: PQCAD - DM实现计算效率和生成质量平衡，减半推理时间，保持竞争力，在不同数据集上表现优于固定位宽量化方法。

Conclusion: PQCAD - DM具有优越的生成能力和效率。

Abstract: Diffusion models excel in image generation but are computational and
resource-intensive due to their reliance on iterative Markov chain processes,
leading to error accumulation and limiting the effectiveness of naive
compression techniques. In this paper, we propose PQCAD-DM, a novel hybrid
compression framework combining Progressive Quantization (PQ) and
Calibration-Assisted Distillation (CAD) to address these challenges. PQ employs
a two-stage quantization with adaptive bit-width transitions guided by a
momentum-based mechanism, reducing excessive weight perturbations in
low-precision. CAD leverages full-precision calibration datasets during
distillation, enabling the student to match full-precision performance even
with a quantized teacher. As a result, PQCAD-DM achieves a balance between
computational efficiency and generative quality, halving inference time while
maintaining competitive performance. Extensive experiments validate PQCAD-DM's
superior generative capabilities and efficiency across diverse datasets,
outperforming fixed-bit quantization methods.

</details>


### [371] [Loupe: A Generalizable and Adaptive Framework for Image Forgery Detection](https://arxiv.org/abs/2506.16819)
*Yuchu Jiang,Jiaming Chu,Jian Zhao,Xin Zhang,Xu Yang,Lei Jin,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: 提出轻量级框架Loupe用于联合深度伪造检测与定位，在DDL数据集实验表现出色，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有深度伪造检测方法存在泛化性差、架构复杂问题，需有效检测与定位方法。

Method: 集成补丁感知分类器和带条件查询的分割模块，引入伪标签引导的测试时间自适应机制。

Result: 在DDL数据集上表现达SOTA，获IJCAI 2025挑战赛第一名，总分0.846。

Conclusion: 提出的补丁级融合和条件查询设计在不同伪造模式下有效提升分类和定位效果。

Abstract: The proliferation of generative models has raised serious concerns about
visual content forgery. Existing deepfake detection methods primarily target
either image-level classification or pixel-wise localization. While some
achieve high accuracy, they often suffer from limited generalization across
manipulation types or rely on complex architectures. In this paper, we propose
Loupe, a lightweight yet effective framework for joint deepfake detection and
localization. Loupe integrates a patch-aware classifier and a segmentation
module with conditional queries, allowing simultaneous global authenticity
classification and fine-grained mask prediction. To enhance robustness against
distribution shifts of test set, Loupe introduces a pseudo-label-guided
test-time adaptation mechanism by leveraging patch-level predictions to
supervise the segmentation head. Extensive experiments on the DDL dataset
demonstrate that Loupe achieves state-of-the-art performance, securing the
first place in the IJCAI 2025 Deepfake Detection and Localization Challenge
with an overall score of 0.846. Our results validate the effectiveness of the
proposed patch-level fusion and conditional query design in improving both
classification accuracy and spatial localization under diverse forgery
patterns. The code is available at https://github.com/Kamichanw/Loupe.

</details>


### [372] [AnyTraverse: An off-road traversability framework with VLM and human operator in the loop](https://arxiv.org/abs/2506.16826)
*Sattwik Sahu,Agamdeep Singh,Karthik Nambiar,Srikanth Saripalli,P. B. Sujit*

Main category: cs.CV

TL;DR: 提出AnyTraverse框架，结合自然语言提示和人工辅助确定越野可通行区域，零样本学习，实验效果好。


<details>
  <summary>Details</summary>
Motivation: 现有越野可通行性分割框架在非结构化环境中表现不佳，且不能适配不同机器人类型。

Method: 提出AnyTraverse框架，结合自然语言提示和人工辅助，采用零样本学习方法。

Result: 在多个数据集上测试并在多机器人平台部署，性能优于GA - NAV和Off - seg。

Conclusion: AnyTraverse提供与车辆无关的越野可通行性方法，平衡自动化与人工监督。

Abstract: Off-road traversability segmentation enables autonomous navigation with
applications in search-and-rescue, military operations, wildlife exploration,
and agriculture. Current frameworks struggle due to significant variations in
unstructured environments and uncertain scene changes, and are not adaptive to
be used for different robot types. We present AnyTraverse, a framework
combining natural language-based prompts with human-operator assistance to
determine navigable regions for diverse robotic vehicles. The system segments
scenes for a given set of prompts and calls the operator only when encountering
previously unexplored scenery or unknown class not part of the prompt in its
region-of-interest, thus reducing active supervision load while adapting to
varying outdoor scenes. Our zero-shot learning approach eliminates the need for
extensive data collection or retraining. Our experimental validation includes
testing on RELLIS-3D, Freiburg Forest, and RUGD datasets and demonstrate
real-world deployment on multiple robot platforms. The results show that
AnyTraverse performs better than GA-NAV and Off-seg while offering a
vehicle-agnostic approach to off-road traversability that balances automation
with targeted human supervision.

</details>


### [373] [ParkFormer: A Transformer-Based Parking Policy with Goal Embedding and Pedestrian-Aware Control](https://arxiv.org/abs/2506.16856)
*Jun Fu,Bin Tian,Haonan Chen,Shi Meng,Tingting Yao*

Main category: cs.CV

TL;DR: 提出基于Transformer的端到端自动泊车框架，在CARLA模拟器验证，成功率高，代码和数据集将公开。


<details>
  <summary>Details</summary>
Motivation: 传统基于规则的泊车系统在不确定环境和动态场景适应性差，而人类司机可直观泊车，受此启发开展研究。

Method: 构建基于Transformer的端到端框架，输入包括环视相机图像等，输出离散控制序列，采用新型交叉注意力模块和GRU行人预测器。

Result: 在CARLA 0.9.14模拟器的垂直和并行泊车场景中，模型成功率达96.57%，平均位置和方向误差分别为0.21米和0.41度，消融研究证明关键模块有效性。

Conclusion: 所提出的基于Transformer的端到端自动泊车框架有效，能解决传统系统的不足。

Abstract: Autonomous parking plays a vital role in intelligent vehicle systems,
particularly in constrained urban environments where high-precision control is
required. While traditional rule-based parking systems struggle with
environmental uncertainties and lack adaptability in crowded or dynamic scenes,
human drivers demonstrate the ability to park intuitively without explicit
modeling. Inspired by this observation, we propose a Transformer-based
end-to-end framework for autonomous parking that learns from expert
demonstrations. The network takes as input surround-view camera images,
goal-point representations, ego vehicle motion, and pedestrian trajectories. It
outputs discrete control sequences including throttle, braking, steering, and
gear selection. A novel cross-attention module integrates BEV features with
target points, and a GRU-based pedestrian predictor enhances safety by modeling
dynamic obstacles. We validate our method on the CARLA 0.9.14 simulator in both
vertical and parallel parking scenarios. Experiments show our model achieves a
high success rate of 96.57\%, with average positional and orientation errors of
0.21 meters and 0.41 degrees, respectively. The ablation studies further
demonstrate the effectiveness of key modules such as pedestrian prediction and
goal-point attention fusion. The code and dataset will be released at:
https://github.com/little-snail-f/ParkFormer.

</details>


### [374] [Privacy-Preserving in Connected and Autonomous Vehicles Through Vision to Text Transformation](https://arxiv.org/abs/2506.15854)
*Abdolazim Rezaei,Mehdi Sookhak,Ahmad Patooghy*

Main category: cs.CV

TL;DR: 本文提出利用反馈式强化学习和视觉语言模型的隐私保护框架，转换图像为文本保护视觉隐私，评估显示在隐私保护和文本质量上有显著提升。


<details>
  <summary>Details</summary>
Motivation: 联网自动驾驶车辆（CAVs）的路边单元中AI摄像头捕获图像存在隐私风险，传统方法不能有效保护个人隐私。

Method: 引入基于反馈式强化学习（RL）和视觉语言模型（VLMs）的隐私保护框架，将图像转换为语义等效的文本描述，并采用分层RL策略迭代优化生成的文本。

Result: 与现有方法相比，唯一单词计数增加约77%，细节密度增加约50%，在隐私保护和文本质量上有显著改善。

Conclusion: 所提出的框架能有效保护AIE摄像头捕获的敏感视觉信息，提升隐私保护和文本质量。

Abstract: Connected and Autonomous Vehicles (CAVs) rely on a range of devices that
often process privacy-sensitive data. Among these, roadside units play a
critical role particularly through the use of AI-equipped (AIE) cameras for
applications such as violation detection. However, the privacy risks associated
with captured imagery remain a major concern, as such data can be misused for
identity theft, profiling, or unauthorized commercial purposes. While
traditional techniques such as face blurring and obfuscation have been applied
to mitigate privacy risks, individual privacy remains at risk, as individuals
can still be tracked using other features such as their clothing. This paper
introduces a novel privacy-preserving framework that leverages feedback-based
reinforcement learning (RL) and vision-language models (VLMs) to protect
sensitive visual information captured by AIE cameras. The main idea is to
convert images into semantically equivalent textual descriptions, ensuring that
scene-relevant information is retained while visual privacy is preserved. A
hierarchical RL strategy is employed to iteratively refine the generated text,
enhancing both semantic accuracy and privacy. Evaluation results demonstrate
significant improvements in both privacy protection and textual quality, with
the Unique Word Count increasing by approximately 77\% and Detail Density by
around 50\% compared to existing approaches.

</details>


### [375] [With Limited Data for Multimodal Alignment, Let the STRUCTURE Guide You](https://arxiv.org/abs/2506.16895)
*Fabian Gröger,Shuo Wen,Huyen Le,Maria Brbić*

Main category: cs.CV

TL;DR: 本文探索用少量配对数据构建多模态模型，引入正则化技术STRUCTURE并提出对齐高表示相似层的方法，在多个基准测试中取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有多模态模型依赖大量配对样本，在很多领域获取成本高或不可行，因此探索用少量配对数据构建模型的可行性。

Method: 引入正则化技术STRUCTURE保存单模态编码器潜在空间的邻域几何结构，提出对齐跨模态表示相似性最高层的方法，并将这两个组件融入现有对齐方法。

Result: 在24个零样本图像分类和检索基准测试中取得显著提升，分类平均相对提升51.6%，检索任务平均相对提升91.8%。

Conclusion: 框架对小样本多模态学习有效且适用性广，为资源受限领域提供了有前景的方向。

Abstract: Multimodal models have demonstrated powerful capabilities in complex tasks
requiring multimodal alignment including zero-shot classification and
cross-modal retrieval. However, existing models typically rely on millions of
paired multimodal samples, which are prohibitively expensive or infeasible to
obtain in many domains. In this work, we explore the feasibility of building
multimodal models with limited amount of paired data by aligning pretrained
unimodal foundation models. We show that high-quality alignment is possible
with as few as tens of thousands of paired samples$\unicode{x2013}$less than
$1\%$ of the data typically used in the field. To achieve this, we introduce
STRUCTURE, an effective regularization technique that preserves the
neighborhood geometry of the latent space of unimodal encoders. Additionally,
we show that aligning last layers is often suboptimal and demonstrate the
benefits of aligning the layers with the highest representational similarity
across modalities. These two components can be readily incorporated into
existing alignment methods, yielding substantial gains across 24 zero-shot
image classification and retrieval benchmarks, with average relative
improvement of $51.6\%$ in classification and $91.8\%$ in retrieval tasks. Our
results highlight the effectiveness and broad applicability of our framework
for limited-sample multimodal learning and offer a promising path forward for
resource-constrained domains.

</details>


### [376] [Pediatric Pancreas Segmentation from MRI Scans with Deep Learning](https://arxiv.org/abs/2506.15908)
*Elif Keles,Merve Yazol,Gorkem Durak,Ziliang Hong,Halil Ertugrul Aktas,Zheyuan Zhang,Linkai Peng,Onkar Susladkar,Necati Guzelyel,Oznur Leman Boyunaga,Cemal Yazici,Mark Lowe,Aliye Uc,Ulas Bagci*

Main category: cs.CV

TL;DR: 本文评估验证了用于儿童胰腺 MRI 分割的深度学习算法 PanSegNet，在健康及胰腺炎儿童中表现良好，且工具和数据集开源。


<details>
  <summary>Details</summary>
Motivation: 评估和验证 PanSegNet 算法在急性胰腺炎、慢性胰腺炎儿童及健康儿童胰腺 MRI 分割中的效果。

Method: 回顾性收集 84 例儿童 MRI 扫描数据，由放射科医生手动分割胰腺，用 Dice 相似系数、95% 百分位 Hausdorff 距离评估 PanSegNet 分割结果，用 Cohen's kappa 衡量观察者一致性。

Result: PanSegNet 在健康、急性胰腺炎、慢性胰腺炎儿童中的 DSC 得分分别为 88%、81%、80%；HD95 值分别为 3.98mm、9.85mm、15.67mm；观察者间和观察者内一致性良好，自动和手动分割体积相关性强。

Conclusion: PanSegNet 是首个经验证的胰腺 MRI 分割深度学习解决方案，在健康和患病状态下均达专家水平，工具和数据集开源可促进相关研究。

Abstract: Objective: Our study aimed to evaluate and validate PanSegNet, a deep
learning (DL) algorithm for pediatric pancreas segmentation on MRI in children
with acute pancreatitis (AP), chronic pancreatitis (CP), and healthy controls.
Methods: With IRB approval, we retrospectively collected 84 MRI scans (1.5T/3T
Siemens Aera/Verio) from children aged 2-19 years at Gazi University
(2015-2024). The dataset includes healthy children as well as patients
diagnosed with AP or CP based on clinical criteria. Pediatric and general
radiologists manually segmented the pancreas, then confirmed by a senior
pediatric radiologist. PanSegNet-generated segmentations were assessed using
Dice Similarity Coefficient (DSC) and 95th percentile Hausdorff distance
(HD95). Cohen's kappa measured observer agreement. Results: Pancreas MRI T2W
scans were obtained from 42 children with AP/CP (mean age: 11.73 +/- 3.9 years)
and 42 healthy children (mean age: 11.19 +/- 4.88 years). PanSegNet achieved
DSC scores of 88% (controls), 81% (AP), and 80% (CP), with HD95 values of 3.98
mm (controls), 9.85 mm (AP), and 15.67 mm (CP). Inter-observer kappa was 0.86
(controls), 0.82 (pancreatitis), and intra-observer agreement reached 0.88 and
0.81. Strong agreement was observed between automated and manual volumes (R^2 =
0.85 in controls, 0.77 in diseased), demonstrating clinical reliability.
Conclusion: PanSegNet represents the first validated deep learning solution for
pancreatic MRI segmentation, achieving expert-level performance across healthy
and diseased states. This tool, algorithm, along with our annotated dataset,
are freely available on GitHub and OSF, advancing accessible, radiation-free
pediatric pancreatic imaging and fostering collaborative research in this
underserved domain.

</details>


### [377] [Enhancing Step-by-Step and Verifiable Medical Reasoning in MLLMs](https://arxiv.org/abs/2506.16962)
*Haoran Sun,Yankai Jiang,Wenjie Lou,Yujie Zhang,Wenjie Li,Lilong Wang,Mianxin Liu,Lei Liu,Xiaosong Wang*

Main category: cs.CV

TL;DR: 提出MICS方案生成医疗CoT数据，构建MMRP数据集和Chiron - o1模型，实验显示Chiron - o1在医疗视觉问答和推理基准测试中达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏搜索和评估关键诊断有效推理路径的综合框架，需提升医疗MLLMs推理能力。

Method: 提出MICS方案，利用导师模型初始化推理，实习模型延续思考，根据MICS - Score选最优推理路径；构建MMRP数据集和Chiron - o1模型。

Result: Chiron - o1在多个医疗视觉问答和推理基准测试中取得了SOTA性能。

Conclusion: MICS方案构建的CoT数据集能有效提升医疗MLLM的推理能力。

Abstract: Multimodal large language models (MLLMs) have begun to demonstrate robust
reasoning capabilities on general tasks, yet their application in the medical
domain remains in its early stages. Constructing chain-of-thought (CoT)
training data is essential for bolstering the reasoning abilities of medical
MLLMs. However, existing approaches exhibit a deficiency in offering a
comprehensive framework for searching and evaluating effective reasoning paths
towards critical diagnosis. To address this challenge, we propose Mentor-Intern
Collaborative Search (MICS), a novel reasoning-path searching scheme to
generate rigorous and effective medical CoT data. MICS first leverages mentor
models to initialize the reasoning, one step at a time, then prompts each
intern model to continue the thinking along those initiated paths, and finally
selects the optimal reasoning path according to the overall reasoning
performance of multiple intern models. The reasoning performance is determined
by an MICS-Score, which assesses the quality of generated reasoning paths.
Eventually, we construct MMRP, a multi-task medical reasoning dataset with
ranked difficulty, and Chiron-o1, a new medical MLLM devised via a curriculum
learning strategy, with robust visual question-answering and generalizable
reasoning capabilities. Extensive experiments demonstrate that Chiron-o1,
trained on our CoT dataset constructed using MICS, achieves state-of-the-art
performance across a list of medical visual question answering and reasoning
benchmarks. Codes are available at GitHub - manglu097/Chiron-o1: Enhancing
Step-by-Step and Verifiable Medical Reasoning in MLLMs

</details>


### [378] [MEXA: Towards General Multimodal Reasoning with Dynamic Multi-Expert Aggregation](https://arxiv.org/abs/2506.17113)
*Shoubin Yu,Yue Zhang,Ziyang Wang,Jaehong Yoon,Mohit Bansal*

Main category: cs.CV

TL;DR: 本文提出无训练框架MEXA进行多模态推理，经多基准测试验证其有效性和广泛适用性。


<details>
  <summary>Details</summary>
Motivation: 结合预训练专家模型进行可扩展多模态推理有潜力，但构建统一框架因输入模态多样性和任务复杂性面临挑战。

Method: 引入MEXA框架，根据输入模态和任务推理需求动态选择专家模型，生成可解释文本推理输出，用大推理模型聚合推理输出得到最终答案。

Result: 在多种多模态基准测试中，MEXA比强大的多模态基线模型性能更优。

Conclusion: MEXA基于专家驱动的选择和聚合方法在多模态推理任务中有效且具有广泛适用性。

Abstract: Combining pre-trained expert models offers substantial potential for scalable
multimodal reasoning, but building a unified framework remains challenging due
to the increasing diversity of input modalities and task complexity. For
instance, medical diagnosis requires precise reasoning over structured clinical
tables, while financial forecasting depends on interpreting plot-based data to
make informed predictions. To tackle this challenge, we introduce MEXA, a
training-free framework that performs modality- and task-aware aggregation of
multiple expert models to enable effective multimodal reasoning across diverse
and distinct domains. MEXA dynamically selects expert models based on the input
modality and the task-specific reasoning demands (i.e., skills). Each expert
model, specialized in a modality task pair, generates interpretable textual
reasoning outputs. MEXA then aggregates and reasons over these outputs using a
Large Reasoning Model (LRM) to produce the final answer. This modular design
allows flexible and transparent multimodal reasoning across diverse domains
without additional training overhead. We extensively evaluate our approach on
diverse multimodal benchmarks, including Video Reasoning, Audio Reasoning, 3D
Understanding, and Medical QA. MEXA consistently delivers performance
improvements over strong multimodal baselines, highlighting the effectiveness
and broad applicability of our expert-driven selection and aggregation in
diverse multimodal reasoning tasks.

</details>


### [379] [VideoGAN-based Trajectory Proposal for Automated Vehicles](https://arxiv.org/abs/2506.16209)
*Annajoyce Mariani,Kira Maag,Hanno Gottschalk*

Main category: cs.CV

TL;DR: 本文探索用基于鸟瞰图交通场景视频训练的GAN生成统计准确轨迹，提出用低分辨率BEV占用网格视频训练的流程，结果良好且推理时间短。


<details>
  <summary>Details</summary>
Motivation: 现有模型驱动、基于规则和经典基于学习的方法难以有效捕捉未来轨迹的复杂多模态分布，需要新方法生成准确轨迹。

Method: 提出使用低分辨率BEV占用网格视频作为视频生成模型的训练数据的流程，用单帧目标检测和帧间目标匹配从生成的交通场景视频中提取抽象轨迹数据，选用GAN架构。

Result: 在100 GPU小时训练内获得最佳结果，推理时间低于20ms。

Conclusion: 提出的轨迹在空间和动态参数分布与真实视频对齐方面展现出物理真实性。

Abstract: Being able to generate realistic trajectory options is at the core of
increasing the degree of automation of road vehicles. While model-driven,
rule-based, and classical learning-based methods are widely used to tackle
these tasks at present, they can struggle to effectively capture the complex,
multimodal distributions of future trajectories. In this paper we investigate
whether a generative adversarial network (GAN) trained on videos of bird's-eye
view (BEV) traffic scenarios can generate statistically accurate trajectories
that correctly capture spatial relationships between the agents. To this end,
we propose a pipeline that uses low-resolution BEV occupancy grid videos as
training data for a video generative model. From the generated videos of
traffic scenarios we extract abstract trajectory data using single-frame object
detection and frame-to-frame object matching. We particularly choose a GAN
architecture for the fast training and inference times with respect to
diffusion models. We obtain our best results within 100 GPU hours of training,
with inference times under 20\,ms. We demonstrate the physical realism of the
proposed trajectories in terms of distribution alignment of spatial and dynamic
parameters with respect to the ground truth videos from the Waymo Open Motion
Dataset.

</details>


### [380] [Do We Need Large VLMs for Spotting Soccer Actions?](https://arxiv.org/abs/2506.17144)
*Ritabrata Chakraborty,Rajatsubhra Chakraborty,Avijit Dasgupta,Sandeep Chaurasia*

Main category: cs.CV

TL;DR: 本文提出从基于视频的足球动作识别任务转向基于文本的任务，利用大语言模型（LLMs）替代视觉语言模型（VLMs），实现轻量级且可扩展的动作识别。


<details>
  <summary>Details</summary>
Motivation: 传统基于视频的足球动作识别任务依赖视觉输入，需要复杂且计算成本高的模型处理密集视频数据，因此寻求更轻量级和可扩展的方法。

Method: 使用提供带时间戳评论的SoccerNet Echoes数据集，采用三个分别专注于结果、兴奋度和战术的LLMs系统，对评论的滑动窗口进行评估以识别关键动作。

Result: 实验表明，这种以语言为中心的方法能有效检测比赛中的关键事件。

Conclusion: 该语言中心方法为传统基于视频的动作识别方法提供了轻量级且无需训练的替代方案。

Abstract: Traditional video-based tasks like soccer action spotting rely heavily on
visual inputs, often requiring complex and computationally expensive models to
process dense video data. In this work, we propose a shift from this
video-centric approach to a text-based task, making it lightweight and scalable
by utilizing Large Language Models (LLMs) instead of Vision-Language Models
(VLMs). We posit that expert commentary, which provides rich, fine-grained
descriptions and contextual cues such as excitement and tactical insights,
contains enough information to reliably spot key actions in a match. To
demonstrate this, we use the SoccerNet Echoes dataset, which provides
timestamped commentary, and employ a system of three LLMs acting as judges
specializing in outcome, excitement, and tactics. Each LLM evaluates sliding
windows of commentary to identify actions like goals, cards, and substitutions,
generating accurate timestamps for these events. Our experiments show that this
language-centric approach performs effectively in detecting critical match
events, providing a lightweight and training-free alternative to traditional
video-based methods for action spotting.

</details>


### [381] [Facial Landmark Visualization and Emotion Recognition Through Neural Networks](https://arxiv.org/abs/2506.17191)
*Israel Juárez-Jiménez,Tiffany Guadalupe Martínez Paredes,Jesús García-Ramírez,Eric Ramos Aguilar*

Main category: cs.CV

TL;DR: 提出面部地标箱线图可视化技术分析面部数据集，比较两种面部地标特征，发现神经网络性能优于随机森林分类器。


<details>
  <summary>Details</summary>
Motivation: 以往研究在面部图像情感识别中缺乏全面的数据集分析，且提取数据集见解时可视化面部地标有挑战。

Method: 提出面部地标箱线图可视化技术识别面部数据集中的异常值，比较面部地标绝对位置和从中性表情到情感峰值的位移两种特征。

Result: 神经网络比随机森林分类器性能更好。

Conclusion: 所提出的方法可用于面部图像情感识别研究，神经网络是更优选择。

Abstract: Emotion recognition from facial images is a crucial task in human-computer
interaction, enabling machines to learn human emotions through facial
expressions. Previous studies have shown that facial images can be used to
train deep learning models; however, most of these studies do not include a
through dataset analysis. Visualizing facial landmarks can be challenging when
extracting meaningful dataset insights; to address this issue, we propose
facial landmark box plots, a visualization technique designed to identify
outliers in facial datasets. Additionally, we compare two sets of facial
landmark features: (i) the landmarks' absolute positions and (ii) their
displacements from a neutral expression to the peak of an emotional expression.
Our results indicate that a neural network achieves better performance than a
random forest classifier.

</details>


### [382] [Part$^{2}$GS: Part-aware Modeling of Articulated Objects using 3D Gaussian Splatting](https://arxiv.org/abs/2506.17212)
*Tianjiao Yu,Vedant Shah,Muntasir Wahed,Ying Shen,Kiet A. Nguyen,Ismini Lourentzou*

Main category: cs.CV

TL;DR: 提出Part$^{2}$GS框架用于建模多部件物体的铰接数字孪生，在合成和真实数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实世界中铰接物体常见，但3D重建方法对其结构和运动建模仍具挑战。

Method: 利用部分感知的3D高斯表示编码铰接组件，提出基于物理约束的运动感知规范表示，引入排斥点场防止部件碰撞。

Result: 在合成和真实数据集上，Part$^{2}$GS在可移动部件的Chamfer距离上比现有方法最多高10倍。

Conclusion: Part$^{2}$GS框架能有效建模多部件物体的铰接数字孪生，且性能优于现有方法。

Abstract: Articulated objects are common in the real world, yet modeling their
structure and motion remains a challenging task for 3D reconstruction methods.
In this work, we introduce Part$^{2}$GS, a novel framework for modeling
articulated digital twins of multi-part objects with high-fidelity geometry and
physically consistent articulation. Part$^{2}$GS leverages a part-aware 3D
Gaussian representation that encodes articulated components with learnable
attributes, enabling structured, disentangled transformations that preserve
high-fidelity geometry. To ensure physically consistent motion, we propose a
motion-aware canonical representation guided by physics-based constraints,
including contact enforcement, velocity consistency, and vector-field
alignment. Furthermore, we introduce a field of repel points to prevent part
collisions and maintain stable articulation paths, significantly improving
motion coherence over baselines. Extensive evaluations on both synthetic and
real-world datasets show that Part$^{2}$GS consistently outperforms
state-of-the-art methods by up to 10$\times$ in Chamfer Distance for movable
parts.

</details>


### [383] [Long-term Traffic Simulation with Interleaved Autoregressive Motion and Scenario Generation](https://arxiv.org/abs/2506.17213)
*Xiuyu Yang,Shuhan Tan,Philipp Krähenbühl*

Main category: cs.CV

TL;DR: 提出InfGen统一模型，实现闭环运动模拟与场景生成，在长短期交通模拟表现佳。


<details>
  <summary>Details</summary>
Motivation: 现有模型和基准专注场景初始智能体闭环运动模拟，不利于长期模拟，因智能体随车辆移动进出场景。

Method: 提出InfGen统一的下一令牌预测模型，自动切换闭环运动模拟和场景生成模式。

Result: InfGen在短期（9s）交通模拟达最优，长期（30s）模拟显著优于其他方法。

Conclusion: InfGen可实现稳定长期滚动模拟，代码和模型将发布。

Abstract: An ideal traffic simulator replicates the realistic long-term point-to-point
trip that a self-driving system experiences during deployment. Prior models and
benchmarks focus on closed-loop motion simulation for initial agents in a
scene. This is problematic for long-term simulation. Agents enter and exit the
scene as the ego vehicle enters new regions. We propose InfGen, a unified
next-token prediction model that performs interleaved closed-loop motion
simulation and scene generation. InfGen automatically switches between
closed-loop motion simulation and scene generation mode. It enables stable
long-term rollout simulation. InfGen performs at the state-of-the-art in
short-term (9s) traffic simulation, and significantly outperforms all other
methods in long-term (30s) simulation. The code and model of InfGen will be
released at https://orangesodahub.github.io/InfGen

</details>


### [384] [Machine Mental Imagery: Empower Multimodal Reasoning with Latent Visual Tokens](https://arxiv.org/abs/2506.17218)
*Zeyuan Yang,Xueyang Yu,Delin Chen,Maohao Shen,Chuang Gan*

Main category: cs.CV

TL;DR: 现有视觉语言模型在需视觉想象的任务上表现受限，本文提出Mirage框架，通过潜在视觉标记增强解码，无需生成明确图像，实验证明其能提升多模态推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型文本解码限制了需视觉想象任务的表现，图像生成预训练会阻碍推理能力，受人类心理意象推理方式启发。

Method: 提出Machine Mental Imagery框架Mirage，用潜在视觉标记增强VLM解码；先通过蒸馏真实图像嵌入监督潜在标记，再转为文本监督，最后进行强化学习。

Result: 在不同基准测试上，Mirage无需明确图像生成就能解锁更强的多模态推理能力。

Conclusion: Mirage框架能有效提升视觉语言模型的多模态推理能力，且无需生成明确图像。

Abstract: Vision-language models (VLMs) excel at multimodal understanding, yet their
text-only decoding forces them to verbalize visual reasoning, limiting
performance on tasks that demand visual imagination. Recent attempts train VLMs
to render explicit images, but the heavy image-generation pre-training often
hinders the reasoning ability. Inspired by the way humans reason with mental
imagery-the internal construction and manipulation of visual cues-we
investigate whether VLMs can reason through interleaved multimodal trajectories
without producing explicit images. To this end, we present a Machine Mental
Imagery framework, dubbed as Mirage, which augments VLM decoding with latent
visual tokens alongside ordinary text. Concretely, whenever the model chooses
to ``think visually'', it recasts its hidden states as next tokens, thereby
continuing a multimodal trajectory without generating pixel-level images. Begin
by supervising the latent tokens through distillation from ground-truth image
embeddings, we then switch to text-only supervision to make the latent
trajectory align tightly with the task objective. A subsequent reinforcement
learning stage further enhances the multimodal reasoning capability.
Experiments on diverse benchmarks demonstrate that Mirage unlocks stronger
multimodal reasoning without explicit image generation.

</details>


### [385] [LAION-C: An Out-of-Distribution Benchmark for Web-Scale Vision Models](https://arxiv.org/abs/2506.16950)
*Fanfei Li,Thomas Klein,Wieland Brendel,Robert Geirhos,Roland S. Zimmermann*

Main category: cs.CV

TL;DR: 现有ImageNet - C基准不适用于评估网络规模数据集的OOD鲁棒性，提出LAION - C基准，评估显示其对当代模型构成挑战，且模型在OOD泛化上出现范式转变。


<details>
  <summary>Details</summary>
Motivation: 现有ImageNet - C等基准数据集的大部分损坏类型在网络规模数据集里不再是分布外数据，不适合评估网络规模数据集的OOD鲁棒性，需新基准。

Method: 引入包含六种新畸变类型的LAION - C基准进行综合评估，还开展心理物理实验对比模型与人类鲁棒性数据。

Result: LAION - C数据集对当代模型（如Gemini和GPT - 4o）构成重大挑战；模型在OOD泛化上出现范式转变，最佳模型表现与或超越最佳人类观察者。

Conclusion: LAION - C可作为评估网络规模数据集OOD鲁棒性的有效替代基准，模型在OOD泛化能力上有显著提升。

Abstract: Out-of-distribution (OOD) robustness is a desired property of computer vision
models. Improving model robustness requires high-quality signals from
robustness benchmarks to quantify progress. While various benchmark datasets
such as ImageNet-C were proposed in the ImageNet era, most ImageNet-C
corruption types are no longer OOD relative to today's large, web-scraped
datasets, which already contain common corruptions such as blur or JPEG
compression artifacts. Consequently, these benchmarks are no longer well-suited
for evaluating OOD robustness in the era of web-scale datasets. Indeed, recent
models show saturating scores on ImageNet-era OOD benchmarks, indicating that
it is unclear whether models trained on web-scale datasets truly become better
at OOD generalization or whether they have simply been exposed to the test
distortions during training. To address this, we introduce LAION-C as a
benchmark alternative for ImageNet-C. LAION-C consists of six novel distortion
types specifically designed to be OOD, even for web-scale datasets such as
LAION. In a comprehensive evaluation of state-of-the-art models, we find that
the LAION-C dataset poses significant challenges to contemporary models,
including MLLMs such as Gemini and GPT-4o. We additionally conducted a
psychophysical experiment to evaluate the difficulty of our corruptions for
human observers, enabling a comparison of models to lab-quality human
robustness data. We observe a paradigm shift in OOD generalization: from humans
outperforming models, to the best models now matching or outperforming the best
human observers.

</details>


### [386] [Prmpt2Adpt: Prompt-Based Zero-Shot Domain Adaptation for Resource-Constrained Environments](https://arxiv.org/abs/2506.16994)
*Yasir Ali Farrukh,Syed Wali,Irfan Khan,Nathaniel D. Bastian*

Main category: cs.CV

TL;DR: 提出轻量级高效零样本域自适应框架Prmpt2Adpt，在MDS - A数据集实验表现好，适配和推理速度快。


<details>
  <summary>Details</summary>
Motivation: 现有基于提示的无监督域自适应（UDA）方法依赖大模型且需全量源域数据，在资源受限环境适用性差。

Method: 基于师生范式和提示特征对齐构建框架，用蒸馏微调的CLIP模型作骨干，通过提示驱动实例归一化（PIN）对齐特征，用教师模型生成伪标签指导学生模型适应。

Result: 在MDS - A数据集上与现有方法有竞争力，适配速度快7倍，推理速度快5倍，只需少量源图像。

Conclusion: Prmpt2Adpt是低资源领域实时自适应的实用可扩展解决方案。

Abstract: Unsupervised Domain Adaptation (UDA) is a critical challenge in real-world
vision systems, especially in resource-constrained environments like drones,
where memory and computation are limited. Existing prompt-driven UDA methods
typically rely on large vision-language models and require full access to
source-domain data during adaptation, limiting their applicability. In this
work, we propose Prmpt2Adpt, a lightweight and efficient zero-shot domain
adaptation framework built around a teacher-student paradigm guided by
prompt-based feature alignment. At the core of our method is a distilled and
fine-tuned CLIP model, used as the frozen backbone of a Faster R-CNN teacher. A
small set of low-level source features is aligned to the target domain
semantics-specified only through a natural language prompt-via Prompt-driven
Instance Normalization (PIN). These semantically steered features are used to
briefly fine-tune the detection head of the teacher model. The adapted teacher
then generates high-quality pseudo-labels, which guide the on-the-fly
adaptation of a compact student model. Experiments on the MDS-A dataset
demonstrate that Prmpt2Adpt achieves competitive detection performance compared
to state-of-the-art methods, while delivering up to 7x faster adaptation and 5x
faster inference speed using few source images-making it a practical and
scalable solution for real-time adaptation in low-resource domains.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [387] [Long Coalition Leads to Shrink? The Roles of Tipping and Technology-Sharing in Climate Clubs](https://arxiv.org/abs/2506.16162)
*Lei Zhu,Zhihao Yan,Hongbo Duan,Yongyang Cai,Xiaobing Zhang*

Main category: econ.GN

TL;DR: 本文构建动态博弈模型分析多重随机气候临界点下联盟稳定性，设计技术共享机制应对搭便车问题，发现联盟规模随温度上升变化，技术共享比制裁更有效。


<details>
  <summary>Details</summary>
Motivation: 全球合作应对气候变化存在搭便车等障碍，需研究联盟稳定性及应对策略。

Method: 开发动态博弈论模型，设计技术共享机制。

Result: 联盟规模随温度上升缩小，气候临界点威胁使稳定联盟规模减小，后有短暂扩张；技术共享比制裁产生更大集体利益。

Conclusion: 提出的动态技术共享途径可增强联盟抗搭便车能力，限制全球变暖，技术共享对长期气候合作至关重要。

Abstract: Global cooperation is posited as a pivotal solution to address climate
change, yet significant barriers, like free-riding, hinder its realization.
This paper develops a dynamic game-theoretic model to analyze the stability of
coalitions under multiple stochastic climate tippings, and a technology-sharing
mechanism is designed in the model to combat free-ridings. Our results reveal
that coalitions tend to shrink over time as temperatures rise, owing to
potential free-ridings, despite a large size of initial coalition. The threat
of climate tipping reduces the size of stable coalitions compared to the case
where tipping is ignored. However, at post-tipping period, coalitions
temporarily expand as regions respond to the shock, though this cooperation is
short-lived and followed by further shrink. Notably, technology-sharing
generates greater collective benefits than sanctions, suggesting that the
proposed dynamic technology-sharing pathway bolsters coalition resilience
against free-riding while limiting the global warming. This framework
highlights the critical role of technology-sharing in fostering long-term
climate cooperation under climate tipping uncertainties.

</details>


### [388] [Artificial Intelligence, Lean Startup Method, and Product Innovations](https://arxiv.org/abs/2506.16334)
*Gavin Wang,Lynn Wu*

Main category: econ.GN

TL;DR: 研究探讨精益创业法（LSM）对人工智能（AI）在初创企业产品创新中影响，分析中国初创企业数据，发现AI与LSM互补，不同类型AI和LSM不同环节配合提升产品创新效率和质量。


<details>
  <summary>Details</summary>
Motivation: 尽管AI有驱动商业创新潜力，但很多公司难以实现其益处，研究LSM对AI在初创企业产品创新中的影响。

Method: 分析2011 - 2020年1800家中国初创企业数据，结合中国政府鼓励AI应用的政策转变。区分发现型和优化型AI，以及LSM中原型开发和控制实验环节。

Result: AI能力强的公司产出更多创新产品；AI投资与LSM在创新中互补；不同类型AI与LSM不同环节配合提升产品开发效率和质量。

Conclusion: 企业使用AI和LSM进行产品开发可在更短时间产生更多高质量产品，应将AI视为异质结构，不同AI能力需不同组织流程实现最优结果。

Abstract: Although AI has the potential to drive significant business innovation, many
firms struggle to realize its benefits. We examine how the Lean Startup Method
(LSM) influences the impact of AI on product innovation in startups. Analyzing
data from 1,800 Chinese startups between 2011 and 2020, alongside policy shifts
by the Chinese government in encouraging AI adoption, we find that companies
with strong AI capabilities produce more innovative products. Moreover, our
study reveals that AI investments complement LSM in innovation, with
effectiveness varying by the type of innovation and AI capability. We
differentiate between discovery-oriented AI, which reduces uncertainty in novel
areas of innovation, and optimization-oriented AI, which refines and optimizes
existing processes. Within the framework of LSM, we further distinguish between
prototyping focused on developing minimum viable products, and controlled
experimentation, focused on rigorous testing such as AB testing. We find that
LSM complements discovery oriented AI by utilizing AI to expand the search for
market opportunities and employing prototyping to validate these opportunities,
thereby reducing uncertainties and facilitating the development of the first
release of products. Conversely, LSM complements optimization-oriented AI by
using AB testing to experiment with the universe of input features and using AI
to streamline iterative refinement processes, thereby accelerating the
improvement of iterative releases of products. As a result, when firms use AI
and LSM for product development, they are able to generate more high quality
product in less time. These findings, applicable to both software and hardware
development, underscore the importance of treating AI as a heterogeneous
construct, as different AI capabilities require distinct organizational
processes to achieve optimal outcomes.

</details>


### [389] [Social Media Can Reduce Misinformation When Public Scrutiny is High](https://arxiv.org/abs/2506.16355)
*Gavin Wang,Haofei Qin,Xiao Tang,Lynn Wu*

Main category: econ.GN

TL;DR: 研究以中国地方政府GDP报告为例，揭示社交媒体对错误信息传播的双重作用，强调公民参与的调节作用。


<details>
  <summary>Details</summary>
Motivation: 此前研究多关注社交媒体传播虚假信息，本文探索其抑制错误信息的条件。

Method: 分析2011 - 2019年中国地方政府官方GDP报告。

Result: 采用社交媒体后，公众监督高的地区错误报告程度显著下降，低监督地区则可能加剧数据操纵。

Conclusion: 挑战社交媒体主要放大错误信息的观点，强调公民参与的重要性，为平台设计和公共政策提供见解。

Abstract: Misinformation poses a growing global threat to institutional trust,
democratic stability, and public decision-making. While prior research has
often portrayed social media as a channel for spreading falsehoods, less is
known about the conditions under which it may instead constrain misinformation
by enhancing transparency and accountability. Here we show this dual potential
in the context of local governments' GDP reporting in China, where data
falsifications are widespread. Analyzing official reports from 2011 to 2019, we
find that local governments have overstated GDP on average. However, after
adopting social media for public communications, the extent of misreporting
declines significantly but only in regions where the public scrutiny over
political matters is high. In such regions, social media increases the cost of
misinformation by facilitating greater information disclosure and bottom-up
monitoring. In contrast, in regions with low public scrutiny, adopting social
media can exacerbate data manipulation. These findings challenge the prevailing
view that social media primarily amplifies misinformation and instead highlight
the importance of civic engagement as a moderating force. Our findings show a
boundary condition for the spread of misinformation and offer insights for
platform design and public policy aimed at promoting accuracy and institutional
accountability.

</details>


### [390] [Optimal Regulation and Investment Incentives in Financial Networks](https://arxiv.org/abs/2506.16648)
*Matthew O. Jackson,Agathe Pernoud*

Main category: econ.GN

TL;DR: 研究金融网络中债务相互依赖下的最优监管，分析企业冒险动机，探讨监管与企业中心性和投资机会关系及不同网络的监管特点。


<details>
  <summary>Details</summary>
Motivation: 探究金融网络中存在债务相互依赖时如何进行最优监管。

Method: 先分析企业选择高风险和高相关性投资组合的动机，再研究最优监管与企业金融中心性和投资机会的关联。

Result: 在标准核心 - 外围网络中，最优监管与银行投资相关性呈非单调关系，在中等相关性时限制最大；对核心银行进行非对称监管可能是最优的。

Conclusion: 金融网络的最优监管需考虑企业动机、中心性、投资机会及投资相关性等因素。

Abstract: We examine optimal regulation of financial networks with debt
interdependencies between financial firms. We first characterize when it is
firms have an incentive to choose excessively risky portfolios and overly
correlate their portfolios with those of their counterparties. We then
characterize how optimal regulation depends on a firm's financial centrality
and its available investment opportunities. In standard core-periphery
networks, optimal regulation depends non-monotonically on the correlation of
banks' investments, with maximal restrictions for intermediate levels of
correlation. Moreover, it can be uniquely optimal to treat banks
asymmetrically: restricting the investments of one core bank while allowing an
otherwise identical core bank (in all aspects, including network centrality) to
invest freely.

</details>


### [391] [A new equilibrium: COVID-19 lockdowns and WFH persistence](https://arxiv.org/abs/2506.16671)
*Laura Ketter,Todd Morris,Lizi Yu*

Main category: econ.GN

TL;DR: 研究发现新冠封锁与居家办公的采用和持续存在紧密联系，封锁地区员工居家办公水平更高。


<details>
  <summary>Details</summary>
Motivation: 探究新冠封锁与居家办公采用和持续之间的关联。

Method: 利用丰富的纵向数据，采用双重差分策略，对比澳大利亚三个封锁严重州和受影响较小州的办公室员工。

Result: 封锁地区员工到2023年居家办公水平高出43%，每周多0.5天，呈现单调剂量反应关系。

Conclusion: 劳动力市场供需双方的调整促使居家办公持续，雇主缩减办公空间、开设远程或混合岗位，员工搬离市中心、投资家庭办公设施和技术。

Abstract: This paper documents a robust link between COVID-19 lockdowns and the uptake
and persistence of working from home (WFH) practices. Exploiting rich
longitudinal data, we use a difference-in-differences strategy to compare
office workers in three heavily locked-down Australian states to similar
workers in less affected states. Locked-down workers sustain 43% higher WFH
levels through 2023 - 0.5 days per week - with a monotonic dose-response
relationship. Persistence is driven by adjustments on both sides of the labor
market: employers downsize office space and open remote or hybrid positions,
while employees relocate away from city centers and invest in home offices and
technology.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [392] [Finance Language Model Evaluation (FLaME)](https://arxiv.org/abs/2506.15846)
*Glenn Matlin,Mika Okamoto,Huzaifa Pardawala,Yang Yang,Sudheer Chava*

Main category: cs.CL

TL;DR: 文章指出现有评估框架不足影响对大语言模型在金融NLP任务中表现的评估，提出首个金融语言模型评估基准套件FLaME，对23个基础大语言模型进行实证研究并开源框架软件和数据结果。


<details>
  <summary>Details</summary>
Motivation: 现有评估框架方法论存在重大差距，难以评估大语言模型在金融专业知识密集型任务中的有效性，需证明大语言模型在金融NLP任务中的潜力。

Method: 提出金融语言模型评估基准套件FLaME，对23个基础大语言模型在20个金融核心NLP任务上进行实证研究。

Result: 完成对23个基础大语言模型在20个金融核心NLP任务上的研究。

Conclusion: 通过FLaME基准套件可更好评估大语言模型在金融NLP任务中的表现。

Abstract: Language Models (LMs) have demonstrated impressive capabilities with core
Natural Language Processing (NLP) tasks. The effectiveness of LMs for highly
specialized knowledge-intensive tasks in finance remains difficult to assess
due to major gaps in the methodologies of existing evaluation frameworks, which
have caused an erroneous belief in a far lower bound of LMs' performance on
common Finance NLP (FinNLP) tasks. To demonstrate the potential of LMs for
these FinNLP tasks, we present the first holistic benchmarking suite for
Financial Language Model Evaluation (FLaME). We are the first research paper to
comprehensively study LMs against 'reasoning-reinforced' LMs, with an empirical
study of 23 foundation LMs over 20 core NLP tasks in finance. We open-source
our framework software along with all data and results.

</details>


### [393] [REIS: A High-Performance and Energy-Efficient Retrieval System with In-Storage Processing](https://arxiv.org/abs/2506.16444)
*Kangqi Chen,Andreas Kosmas Kakolyris,Rakesh Nadig,Manos Frouzakis,Nika Mansouri Ghiasi,Yu Liang,Haiyu Mao,Jisung Park,Mohammad Sadrosadati,Onur Mutlu*

Main category: cs.CL

TL;DR: 论文指出大语言模型知识受限于训练数据，RAG的检索阶段有瓶颈，现有ISP技术有局限，提出REIS系统解决问题，性能和能效提升显著。


<details>
  <summary>Details</summary>
Motivation: 解决RAG检索阶段因数据库大导致的数据移动开销问题，以及现有ISP技术用于ANNS的局限性。

Method: 提出REIS系统，采用关联数据库嵌入向量与文档的布局、ISP定制的数据放置技术和利用存储系统内计算资源的ANNS引擎。

Result: 相比服务器级系统，REIS检索性能平均提升13倍，能效平均提升55倍。

Conclusion: REIS系统有效解决了RAG检索阶段的瓶颈问题和现有ISP技术的局限，提升了检索性能和能效。

Abstract: Large Language Models (LLMs) face an inherent challenge: their knowledge is
confined to the data that they have been trained on. To overcome this issue,
Retrieval-Augmented Generation (RAG) complements the static training-derived
knowledge of LLMs with an external knowledge repository. RAG consists of three
stages: indexing, retrieval, and generation. The retrieval stage of RAG becomes
a significant bottleneck in inference pipelines. In this stage, a user query is
mapped to an embedding vector and an Approximate Nearest Neighbor Search (ANNS)
algorithm searches for similar vectors in the database to identify relevant
items. Due to the large database sizes, ANNS incurs significant data movement
overheads between the host and the storage system. To alleviate these
overheads, prior works propose In-Storage Processing (ISP) techniques that
accelerate ANNS by performing computations inside storage. However, existing
works that leverage ISP for ANNS (i) employ algorithms that are not tailored to
ISP systems, (ii) do not accelerate data retrieval operations for data selected
by ANNS, and (iii) introduce significant hardware modifications, limiting
performance and hindering their adoption. We propose REIS, the first ISP system
tailored for RAG that addresses these limitations with three key mechanisms.
First, REIS employs a database layout that links database embedding vectors to
their associated documents, enabling efficient retrieval. Second, it enables
efficient ANNS by introducing an ISP-tailored data placement technique that
distributes embeddings across the planes of the storage system and employs a
lightweight Flash Translation Layer. Third, REIS leverages an ANNS engine that
uses the existing computational resources inside the storage system. Compared
to a server-grade system, REIS improves the performance (energy efficiency) of
retrieval by an average of 13x (55x).

</details>


### [394] [MEM1: Learning to Synergize Memory and Reasoning for Efficient Long-Horizon Agents](https://arxiv.org/abs/2506.15841)
*Zijian Zhou,Ao Qu,Zhaoxuan Wu,Sunghwan Kim,Alok Prakash,Daniela Rus,Jinhua Zhao,Bryan Kian Hsiang Low,Paul Pu Liang*

Main category: cs.CL

TL;DR: 提出MEM1框架，使智能体在多轮任务中恒定内存运行，实验表明其性能和内存使用优于Qwen2.5-14B-Instruct。


<details>
  <summary>Details</summary>
Motivation: 多数大语言模型系统全上下文提示存在内存增长无界、计算成本高和推理性能下降问题，需解决长时多轮交互的效率和性能问题。

Method: 引入端到端强化学习框架MEM1更新紧凑共享内部状态；提出组合现有数据集构建多轮环境的方法。

Result: MEM1 - 7B在16目标多跳问答任务中性能提升3.5倍、内存使用降低3.7倍，且泛化性好。

Conclusion: 推理驱动的内存整合有潜力成为训练长时交互智能体的可扩展方案。

Abstract: Modern language agents must operate over long-horizon, multi-turn
interactions, where they retrieve external information, adapt to observations,
and answer interdependent queries. Yet, most LLM systems rely on full-context
prompting, appending all past turns regardless of their relevance. This leads
to unbounded memory growth, increased computational costs, and degraded
reasoning performance on out-of-distribution input lengths. We introduce MEM1,
an end-to-end reinforcement learning framework that enables agents to operate
with constant memory across long multi-turn tasks. At each turn, MEM1 updates a
compact shared internal state that jointly supports memory consolidation and
reasoning. This state integrates prior memory with new observations from the
environment while strategically discarding irrelevant or redundant information.
To support training in more realistic and compositional settings, we propose a
simple yet effective and scalable approach to constructing multi-turn
environments by composing existing datasets into arbitrarily complex task
sequences. Experiments across three domains, including internal retrieval QA,
open-domain web QA, and multi-turn web shopping, show that MEM1-7B improves
performance by 3.5x while reducing memory usage by 3.7x compared to
Qwen2.5-14B-Instruct on a 16-objective multi-hop QA task, and generalizes
beyond the training horizon. Our results demonstrate the promise of
reasoning-driven memory consolidation as a scalable alternative to existing
solutions for training long-horizon interactive agents, where both efficiency
and performance are optimized.

</details>


### [395] [Analyzing the Influence of Knowledge Graph Information on Relation Extraction](https://arxiv.org/abs/2506.16343)
*Cedric Möller,Ricardo Usbeck*

Main category: cs.CL

TL;DR: 研究结合知识图谱信息对关系抽取模型性能的影响，实验表明结合知识图谱信息能显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 探究知识图谱中实体位置信息对关系抽取任务的作用，提升关系抽取模型性能。

Method: 在多个不同数据集上进行实验，将成熟的关系抽取方法与图感知的Neural Bellman - Ford网络结合，在监督和零样本设置下测试基于知识图谱的特征。

Result: 结合知识图谱信息显著提升模型性能，尤其在处理各关系训练样本数量不平衡时；在不同数据集上基于知识图谱的特征均有性能提升。

Conclusion: 整合知识图谱信息可有效提升关系抽取模型的性能。

Abstract: We examine the impact of incorporating knowledge graph information on the
performance of relation extraction models across a range of datasets. Our
hypothesis is that the positions of entities within a knowledge graph provide
important insights for relation extraction tasks. We conduct experiments on
multiple datasets, each varying in the number of relations, training examples,
and underlying knowledge graphs. Our results demonstrate that integrating
knowledge graph information significantly enhances performance, especially when
dealing with an imbalance in the number of training examples for each relation.
We evaluate the contribution of knowledge graph-based features by combining
established relation extraction methods with graph-aware Neural Bellman-Ford
networks. These features are tested in both supervised and zero-shot settings,
demonstrating consistent performance improvements across various datasets.

</details>


### [396] [PersonalAI: Towards digital twins in the graph form](https://arxiv.org/abs/2506.17001)
*Mikhail Menschikov,Dmitry Evseev,Ruslan Kostoev,Ilya Perepechkin,Ilnaz Salimov,Victoria Dochkina,Petr Anokhin,Evgeny Burnaev,Nikita Semenov*

Main category: cs.CL

TL;DR: 提出用知识图谱形式的外部内存实现语言模型个性化，实验证明其在问答系统中有效且能处理时间依赖。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型有所进步，但保留大量个人信息并生成个性化回复仍是挑战，需解决语言模型个性化问题。

Method: 利用大语言模型构建和更新以知识图谱形式存在的外部内存，扩展AriGraph架构并引入含标准边和两种超边的组合图。

Result: 在TriviaQA、HotpotQA和DiaASQ基准测试中，该方法使图构建和知识提取统一且稳健；修改DiaASQ基准测试后，问答系统性能仍稳健。

Conclusion: 所提出的架构能有效维护和利用时间依赖，有助于语言模型个性化。

Abstract: The challenge of personalizing language models, specifically the ability to
account for a user's history during interactions, is of significant interest.
Despite recent advancements in large language models (LLMs) and Retrieval
Augmented Generation that have enhanced the factual base of LLMs, the task of
retaining extensive personal information and using it to generate personalized
responses remains pertinent. To address this, we propose utilizing external
memory in the form of knowledge graphs, which are constructed and updated by
the LLM itself. We have expanded upon ideas of AriGraph architecture and for
the first time introduced a combined graph featuring both standard edges and
two types of hyperedges. Experiments conducted on the TriviaQA, HotpotQA and
DiaASQ benchmarks indicates that this approach aids in making the process of
graph construction and knowledge extraction unified and robust. Furthermore, we
augmented the DiaASQ benchmark by incorporating parameters such as time into
dialogues and introducing contradictory statements made by the same speaker at
different times. Despite these modifications, the performance of the
question-answering system remained robust, demonstrating the proposed
architecture's ability to maintain and utilize temporal dependencies.

</details>


### [397] [Towards AI Search Paradigm](https://arxiv.org/abs/2506.17188)
*Yuchen Li,Hengyi Cai,Rui Kong,Xinran Chen,Jiamin Chen,Jun Yang,Haojie Zhang,Jiayi Li,Jiayi Wu,Yiqun Chen,Changle Qu,Keyi Kong,Wenwen Ye,Lixin Su,Xinyu Ma,Long Xia,Daiting Shi,Jiashu Zhao,Haoyi Xiong,Shuaiqiang Wang,Dawei Yin*

Main category: cs.CL

TL;DR: 介绍AI搜索范式，包括其架构、关键方法，旨在指导可信、自适应和可扩展AI搜索系统开发。


<details>
  <summary>Details</summary>
Motivation: 提出下一代搜索系统的全面蓝图，使系统能模拟人类信息处理和决策。

Method: 采用由四个大语言模型驱动的代理组成的模块化架构，通过协调工作流协作；系统介绍任务规划、工具集成等关键方法。

Result: 无明确提及具体结果。

Conclusion: 该工作能为可信、自适应和可扩展的AI搜索系统开发提供信息指导。

Abstract: In this paper, we introduce the AI Search Paradigm, a comprehensive blueprint
for next-generation search systems capable of emulating human information
processing and decision-making. The paradigm employs a modular architecture of
four LLM-powered agents (Master, Planner, Executor and Writer) that dynamically
adapt to the full spectrum of information needs, from simple factual queries to
complex multi-stage reasoning tasks. These agents collaborate dynamically
through coordinated workflows to evaluate query complexity, decompose problems
into executable plans, and orchestrate tool usage, task execution, and content
synthesis. We systematically present key methodologies for realizing this
paradigm, including task planning and tool integration, execution strategies,
aligned and robust retrieval-augmented generation, and efficient LLM inference,
spanning both algorithmic techniques and infrastructure-level optimizations. By
providing an in-depth guide to these foundational components, this work aims to
inform the development of trustworthy, adaptive, and scalable AI search
systems.

</details>


### [398] [Veracity: An Open-Source AI Fact-Checking System](https://arxiv.org/abs/2506.15794)
*Taylor Lynn Curtis,Maximilian Puelma Touzel,William Garneau,Manon Gruaz,Mike Pinder,Li Wei Wang,Sukanya Krishna,Luda Cohen,Jean-François Godbout,Reihaneh Rabbany,Kellin Pelrine*

Main category: cs.CL

TL;DR: 介绍开源AI系统Veracity，它结合大语言模型和网络检索代理检测错误信息并给出评估及解释，展示其检测和解释能力以促进媒体素养。


<details>
  <summary>Details</summary>
Motivation: 错误信息的传播，特别是在生成式AI能力的推动下，对社会构成重大威胁，需要一个系统来对抗错误信息。

Method: Veracity利用大语言模型（LLMs）和网络检索代理的协同作用，分析用户提交的声明并提供基于事实的真实性评估和直观解释。

Result: Veracity不仅能够检测错误信息，还能解释其推理过程。

Conclusion: Veracity可以培养媒体素养，促进社会更加知情。

Abstract: The proliferation of misinformation poses a significant threat to society,
exacerbated by the capabilities of generative AI. This demo paper introduces
Veracity, an open-source AI system designed to empower individuals to combat
misinformation through transparent and accessible fact-checking. Veracity
leverages the synergy between Large Language Models (LLMs) and web retrieval
agents to analyze user-submitted claims and provide grounded veracity
assessments with intuitive explanations. Key features include multilingual
support, numerical scoring of claim veracity, and an interactive interface
inspired by familiar messaging applications. This paper will showcase
Veracity's ability to not only detect misinformation but also explain its
reasoning, fostering media literacy and promoting a more informed society.

</details>


### [399] [Language Models can perform Single-Utterance Self-Correction of Perturbed Reasoning](https://arxiv.org/abs/2506.15894)
*Sam Silver,Jimin Sun,Ivan Zhang,Sara Hooker,Eddie Kim*

Main category: cs.CL

TL;DR: 研究大语言模型对CoT推理中引入的合成扰动的自我修正能力，发现多种模型和数据集有单轮内修正行为，表明其自我修正能力可能更强。


<details>
  <summary>Details</summary>
Motivation: 大语言模型数学推理能力对问题描述和提示策略变化敏感，且推理易受采样误差影响，需研究其自我修正能力。

Method: 进行实验测量模型对CoT推理中引入的合成扰动的自我修正能力。

Result: 在一系列开源权重模型和数据集上观察到了单轮内固有的自我修正行为。

Conclusion: 大语言模型可能拥有比文献中常见情况更强的固有自我修正能力，近期的推理模型工作可能是对模型中已有特质的放大。

Abstract: Large Language Models (LLMs) have demonstrated impressive mathematical
reasoning capabilities, yet their performance remains brittle to minor
variations in problem description and prompting strategy. Furthermore,
reasoning is vulnerable to sampling-induced errors which autoregressive models
must primarily address using self-correction via additionally-generated tokens.
To better understand self-correction capabilities of recent models, we conduct
experiments measuring models' ability to self-correct synthetic perturbations
introduced into their Chain of Thought (CoT) reasoning. We observe robust
single-utterance intrinsic self-correction behavior across a range of
open-weight models and datasets, ranging from subtle, implicit corrections to
explicit acknowledgments and corrections of errors. Our findings suggest that
LLMs, including those not finetuned for long CoT, may possess stronger
intrinsic self-correction capabilities than commonly shown in the literature.
The presence of this ability suggests that recent "reasoning" model work
involves amplification of traits already meaningfully present in models.

</details>


### [400] [A Vietnamese Dataset for Text Segmentation and Multiple Choices Reading Comprehension](https://arxiv.org/abs/2506.15978)
*Toan Nguyen Hai,Ha Nguyen Viet,Truong Quan Xuan,Duc Do Minh*

Main category: cs.CL

TL;DR: 本文提出越南语数据集VSMRC，实验表明多语言模型在越南语NLP任务表现出色，VSMRC可在HuggingFace获取。


<details>
  <summary>Details</summary>
Motivation: 越南语在文本分割和机器阅读理解等关键NLP任务上缺乏强大资源，需填补此空白。

Method: 从越南语维基百科获取数据，构建VSMRC数据集，包含文本分割文档和合成问答对，并进行人工质量保证。

Result: mBERT在两项任务上始终优于单语模型，MRC测试集准确率88.01%，文本分割测试集F1分数63.15%。

Conclusion: 多语言模型在越南语NLP任务中表现出色，有望应用于其他资源匮乏语言。

Abstract: Vietnamese, the 20th most spoken language with over 102 million native
speakers, lacks robust resources for key natural language processing tasks such
as text segmentation and machine reading comprehension (MRC). To address this
gap, we present VSMRC, the Vietnamese Text Segmentation and Multiple-Choice
Reading Comprehension Dataset. Sourced from Vietnamese Wikipedia, our dataset
includes 15,942 documents for text segmentation and 16,347 synthetic
multiple-choice question-answer pairs generated with human quality assurance,
ensuring a reliable and diverse resource. Experiments show that mBERT
consistently outperforms monolingual models on both tasks, achieving an
accuracy of 88.01% on MRC test set and an F1 score of 63.15\% on text
segmentation test set. Our analysis reveals that multilingual models excel in
NLP tasks for Vietnamese, suggesting potential applications to other
under-resourced languages. VSMRC is available at HuggingFace

</details>


### [401] [Double Entendre: Robust Audio-Based AI-Generated Lyrics Detection via Multi-View Fusion](https://arxiv.org/abs/2506.15981)
*Markus Frohmann,Gabriel Meseguer-Brocal,Markus Schedl,Elena V. Epure*

Main category: cs.CL

TL;DR: AI音乐生成工具发展带来挑战，需可靠检测方法。提出多模态模块化后期融合管道方法DE - detect，实验表明其性能优于现有方法，代码开源。


<details>
  <summary>Details</summary>
Motivation: AI音乐生成工具发展给音乐行业带来挑战，现有基于音频或歌词的检测方法有局限性，需要可靠检测方法。

Method: 提出多模态、模块化后期融合管道，结合自动转录的演唱歌词和音频中捕捉歌词相关信息的语音特征。

Result: 实验显示DE - detect方法优于现有基于歌词的检测器，对音频扰动更鲁棒。

Conclusion: 该方法为现实场景中检测AI生成音乐提供了有效、鲁棒的解决方案。

Abstract: The rapid advancement of AI-based music generation tools is revolutionizing
the music industry but also posing challenges to artists, copyright holders,
and providers alike. This necessitates reliable methods for detecting such
AI-generated content. However, existing detectors, relying on either audio or
lyrics, face key practical limitations: audio-based detectors fail to
generalize to new or unseen generators and are vulnerable to audio
perturbations; lyrics-based methods require cleanly formatted and accurate
lyrics, unavailable in practice. To overcome these limitations, we propose a
novel, practically grounded approach: a multimodal, modular late-fusion
pipeline that combines automatically transcribed sung lyrics and speech
features capturing lyrics-related information within the audio. By relying on
lyrical aspects directly from audio, our method enhances robustness, mitigates
susceptibility to low-level artifacts, and enables practical applicability.
Experiments show that our method, DE-detect, outperforms existing lyrics-based
detectors while also being more robust to audio perturbations. Thus, it offers
an effective, robust solution for detecting AI-generated music in real-world
scenarios. Our code is available at
https://github.com/deezer/robust-AI-lyrics-detection.

</details>


### [402] [From General to Targeted Rewards: Surpassing GPT-4 in Open-Ended Long-Context Generation](https://arxiv.org/abs/2506.16024)
*Zhihan Guo,Jiele Wu,Wenqian Cui,Yifei Zhang,Minda Hu,Yufei Wang,Irwin King*

Main category: cs.CL

TL;DR: 当前大语言模型长文本上下文研究中开放式长文本生成探索不足，本文提出ProxyReward框架，实验表明其性能优于GPT - 4 - Turbo等。


<details>
  <summary>Details</summary>
Motivation: 当前开放式长文本生成研究不足，训练长上下文生成模型缺乏参考数据，以往方法用通用评估作奖励信号限制准确性。

Method: 引入基于强化学习的ProxyReward框架，包括自动生成的数据集和奖励信号计算方法。

Result: ProxyReward性能超过GPT - 4 - Turbo，训练开源模型时在开放式长文本生成任务上性能提升20%，超越LLM - as - a - Judge方法。

Conclusion: 本文方法能有效提升大语言模型处理人类复杂开放式问题的能力。

Abstract: Current research on long-form context in Large Language Models (LLMs)
primarily focuses on the understanding of long-contexts, the Open-ended Long
Text Generation (Open-LTG) remains insufficiently explored. Training a
long-context generation model requires curation of gold standard reference
data, which is typically nonexistent for informative Open-LTG tasks. However,
previous methods only utilize general assessments as reward signals, which
limits accuracy. To bridge this gap, we introduce ProxyReward, an innovative
reinforcement learning (RL) based framework, which includes a dataset and a
reward signal computation method. Firstly, ProxyReward Dataset generation is
accomplished through simple prompts that enables the model to create
automatically, obviating extensive labeled data or significant manual effort.
Secondly, ProxyReward Signal offers a targeted evaluation of information
comprehensiveness and accuracy for specific questions. The experimental results
indicate that our method ProxyReward surpasses even GPT-4-Turbo. It can
significantly enhance performance by 20% on the Open-LTG task when training
widely used open-source models, while also surpassing the LLM-as-a-Judge
approach. Our work presents effective methods to enhance the ability of LLMs to
address complex open-ended questions posed by human.

</details>


### [403] [EvoLM: In Search of Lost Language Model Training Dynamics](https://arxiv.org/abs/2506.16029)
*Zhenting Qi,Fan Nie,Alexandre Alahi,James Zou,Himabindu Lakkaraju,Yilun Du,Eric Xing,Sham Kakade,Hanlin Zhang*

Main category: cs.CL

TL;DR: 提出EvoLM模型套件分析语言模型训练动态，评估推理能力，得出训练相关见解并开源资源。


<details>
  <summary>Details</summary>
Motivation: 现代语言模型训练分多阶段，下游开发者难评估各阶段设计选择的影响。

Method: 从头训练超100个1B和4B参数的语言模型，评估上下游推理能力。

Result: 发现过度预训练和后训练收益递减等关键见解。

Conclusion: 发布预训练和后训练模型、各阶段训练数据集及训练评估管道，促进开放研究和可重复性。

Abstract: Modern language model (LM) training has been divided into multiple stages,
making it difficult for downstream developers to evaluate the impact of design
choices made at each stage. We present EvoLM, a model suite that enables
systematic and transparent analysis of LMs' training dynamics across
pre-training, continued pre-training, supervised fine-tuning, and reinforcement
learning. By training over 100 LMs with 1B and 4B parameters from scratch, we
rigorously evaluate both upstream (language modeling) and downstream
(problem-solving) reasoning capabilities, including considerations of both
in-domain and out-of-domain generalization. Key insights highlight the
diminishing returns from excessive pre-training and post-training, the
importance and practices of mitigating forgetting during domain-specific
continued pre-training, the crucial role of continued pre-training in bridging
pre-training and post-training phases, and various intricate trade-offs when
configuring supervised fine-tuning and reinforcement learning. To facilitate
open research and reproducibility, we release all pre-trained and post-trained
models, training datasets for all stages, and our entire training and
evaluation pipeline.

</details>


### [404] [DynScaling: Efficient Verifier-free Inference Scaling via Dynamic and Integrated Sampling](https://arxiv.org/abs/2506.16043)
*Fei Wang,Xingchen Wan,Ruoxi Sun,Jiefeng Chen,Sercan Ö. Arık*

Main category: cs.CL

TL;DR: 提出DynScaling方法，结合并行 - 顺序采样策略和动态预算分配框架，提升大语言模型在实际资源约束下的性能，实验超现有无验证器推理缩放基线。


<details>
  <summary>Details</summary>
Motivation: 现有推理时间缩放方法在实际应用中受限于依赖外部验证器或未针对实际计算约束优化。

Method: 提出DynScaling，包括集成并行 - 顺序采样策略和基于多臂老虎机的动态预算分配框架。

Result: DynScaling在任务性能和计算成本上均超过现有无验证器推理缩放基线。

Conclusion: DynScaling能在实际资源约束下有效提升大语言模型性能，无需外部验证器。

Abstract: Inference-time scaling has proven effective in boosting large language model
(LLM) performance through increased test-time computation. Yet, its practical
application is often hindered by reliance on external verifiers or a lack of
optimization for realistic computational constraints. We propose DynScaling,
which addresses these limitations through two primary innovations: an
integrated parallel-sequential sampling strategy and a bandit-based dynamic
budget allocation framework. The integrated sampling strategy unifies parallel
and sequential sampling by constructing synthetic sequential reasoning chains
from initially independent parallel responses, promoting diverse and coherent
reasoning trajectories. The dynamic budget allocation framework formulates the
allocation of computational resources as a multi-armed bandit problem,
adaptively distributing the inference budget across queries based on the
uncertainty of previously sampled responses, thereby maximizing computational
efficiency. By combining these components, DynScaling effectively improves LLM
performance under practical resource constraints without the need for external
verifiers. Experimental results demonstrate that DynScaling consistently
surpasses existing verifier-free inference scaling baselines in both task
performance and computational cost.

</details>


### [405] [A Hybrid DeBERTa and Gated Broad Learning System for Cyberbullying Detection in English Text](https://arxiv.org/abs/2506.16052)
*Devesh Kumar*

Main category: cs.CL

TL;DR: 本文提出ModifiedDeBERTa + GBLS混合架构用于网络欺凌检测，在多个数据集上表现良好且有可解释机制，还分析了组件贡献和失败案例。


<details>
  <summary>Details</summary>
Motivation: 在线交流平台增多使网络欺凌问题凸显，影响约54.4%青少年，需有效检测方法。

Method: 结合基于Transformer模型的上下文理解能力和广义学习系统的模式识别优势，整合改进的DeBERTa模型和GBLS分类器。

Result: 在四个英文数据集上取得良好性能，如HateXplain上准确率79.3%等，框架有全面可解释机制。

Conclusion: 架构各组件有意义贡献，检测隐含偏见和讽刺内容存在挑战，为未来改进提供见解。

Abstract: The proliferation of online communication platforms has created unprecedented
opportunities for global connectivity while simultaneously enabling harmful
behaviors such as cyberbullying, which affects approximately 54.4\% of
teenagers according to recent research. This paper presents a hybrid
architecture that combines the contextual understanding capabilities of
transformer-based models with the pattern recognition strengths of broad
learning systems for effective cyberbullying detection. This approach
integrates a modified DeBERTa model augmented with Squeeze-and-Excitation
blocks and sentiment analysis capabilities with a Gated Broad Learning System
(GBLS) classifier, creating a synergistic framework that outperforms existing
approaches across multiple benchmark datasets. The proposed ModifiedDeBERTa +
GBLS model achieved good performance on four English datasets: 79.3\% accuracy
on HateXplain, 95.41\% accuracy on SOSNet, 91.37\% accuracy on Mendeley-I, and
94.67\% accuracy on Mendeley-II. Beyond performance gains, the framework
incorporates comprehensive explainability mechanisms including token-level
attribution analysis, LIME-based local interpretations, and confidence
calibration, addressing critical transparency requirements in automated content
moderation. Ablation studies confirm the meaningful contribution of each
architectural component, while failure case analysis reveals specific
challenges in detecting implicit bias and sarcastic content, providing valuable
insights for future improvements in cyberbullying detection systems.

</details>


### [406] [Under the Shadow of Babel: How Language Shapes Reasoning in LLMs](https://arxiv.org/abs/2506.16151)
*Chenxi Wang,Yixuan Zhang,Lang Gao,Zixiang Xu,Zirui Song,Yanbo Wang,Xiuying Chen*

Main category: cs.CL

TL;DR: 研究通过BICAUSE数据集研究大语言模型（LLMs），发现其不仅模仿语言表面形式，还内化语言塑造的推理偏差。


<details>
  <summary>Details</summary>
Motivation: 验证若语言结构塑造认知模式，基于人类语言训练的LLMs是否会内化不同语言中的逻辑结构这一假设。

Method: 引入结构化双语因果推理数据集BICAUSE，包含语义对齐的中英样本。

Result: LLMs有类型学对齐的注意力模式；内化语言特定的因果词序偏好，处理非典型输入时性能下降；推理成功时模型表征跨语言趋向语义对齐抽象。

Conclusion: LLMs不仅模仿语言表面形式，还内化语言塑造的推理偏差，该现象首次通过模型内部结构分析得到实证验证。

Abstract: Language is not only a tool for communication but also a medium for human
cognition and reasoning. If, as linguistic relativity suggests, the structure
of language shapes cognitive patterns, then large language models (LLMs)
trained on human language may also internalize the habitual logical structures
embedded in different languages. To examine this hypothesis, we introduce
BICAUSE, a structured bilingual dataset for causal reasoning, which includes
semantically aligned Chinese and English samples in both forward and reversed
causal forms. Our study reveals three key findings: (1) LLMs exhibit
typologically aligned attention patterns, focusing more on causes and
sentence-initial connectives in Chinese, while showing a more balanced
distribution in English. (2) Models internalize language-specific preferences
for causal word order and often rigidly apply them to atypical inputs, leading
to degraded performance, especially in Chinese. (3) When causal reasoning
succeeds, model representations converge toward semantically aligned
abstractions across languages, indicating a shared understanding beyond surface
form. Overall, these results suggest that LLMs not only mimic surface
linguistic forms but also internalize the reasoning biases shaped by language.
Rooted in cognitive linguistic theory, this phenomenon is for the first time
empirically verified through structural analysis of model internals.

</details>


### [407] [JETHICS: Japanese Ethics Understanding Evaluation Dataset](https://arxiv.org/abs/2506.16187)
*Masashi Takeshita,Rafal Rzepka*

Main category: cs.CL

TL;DR: 提出日语数据集JETHICS评估AI模型伦理理解，评估实验显示当前大语言模型有较大提升空间。


<details>
  <summary>Details</summary>
Motivation: 构建日语数据集以评估AI模型的伦理理解能力。

Method: 遵循英语ETHICS数据集的构建方法构建JETHICS数据集，对非专有大语言模型和GPT - 4o进行评估实验。

Result: GPT - 4o平均得分约0.7，表现最佳的日语大语言模型得分约0.5。

Conclusion: 当前大语言模型在伦理理解方面有较大提升空间。

Abstract: In this work, we propose JETHICS, a Japanese dataset for evaluating ethics
understanding of AI models. JETHICS contains 78K examples and is built by
following the construction methods of the existing English ETHICS dataset. It
includes four categories based normative theories and concepts from ethics and
political philosophy; and one representing commonsense morality. Our evaluation
experiments on non-proprietary large language models (LLMs) and on GPT-4o
reveal that even GPT-4o achieves only an average score of about 0.7, while the
best-performing Japanese LLM attains around 0.5, indicating a relatively large
room for improvement in current LLMs.

</details>


### [408] [Can structural correspondences ground real world representational content in Large Language Models?](https://arxiv.org/abs/2506.16370)
*Iwan Williams*

Main category: cs.CL

TL;DR: 探讨大语言模型（LLMs）表征能力，认为仅结构对应不足以支撑表征，合适作用的结构对应可支撑现实内容，但存在文本局限挑战。


<details>
  <summary>Details</summary>
Motivation: 许多LLMs仅接触文本，其表征能力不确定，要探究能否表征及表征内容。

Method: 依据基于结构对应的表征理论探讨如何回答相关问题并初步调查证据。

Result: 仅LLMs与世界实体的结构对应不足以支撑对这些实体的表征，合适作用的结构对应可支撑现实内容。

Conclusion: 要使结构对应发挥作用来支撑表征需克服LLMs文本局限性带来的挑战。

Abstract: Large Language Models (LLMs) such as GPT-4 produce compelling responses to a
wide range of prompts. But their representational capacities are uncertain.
Many LLMs have no direct contact with extra-linguistic reality: their inputs,
outputs and training data consist solely of text, raising the questions (1) can
LLMs represent anything and (2) if so, what? In this paper, I explore what it
would take to answer these questions according to a structural-correspondence
based account of representation, and make an initial survey of this evidence. I
argue that the mere existence of structural correspondences between LLMs and
worldly entities is insufficient to ground representation of those entities.
However, if these structural correspondences play an appropriate role - they
are exploited in a way that explains successful task performance - then they
could ground real world contents. This requires overcoming a challenge: the
text-boundedness of LLMs appears, on the face of it, to prevent them engaging
in the right sorts of tasks.

</details>


### [409] [From LLM-anation to LLM-orchestrator: Coordinating Small Models for Data Labeling](https://arxiv.org/abs/2506.16393)
*Yao Lu,Zhaiyuan Ji,Jiawei Du,Yu Shanqing,Qi Xuan,Tianyi Zhou*

Main category: cs.CL

TL;DR: 提出多模型合作标注新范式及自动标注框架AutoAnnotator，可降低成本、提高准确率。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型标注范式在大规模标注成本高，且在细粒度语义理解场景中准确率不如小语言模型。

Method: 设计两层AutoAnnotator框架，上层元控制器选小模型、生成代码和验证样本，下层多小模型投票标注；用元控制器二次审查样本微调小模型。

Result: AutoAnnotator在多种设置下优于现有开源/API大语言模型，相比GPT - 3.5 - turbo降低成本74.15%，提高准确率6.21%。

Conclusion: AutoAnnotator是有效的多模型合作标注框架，能解决现有大语言模型标注的瓶颈问题。

Abstract: Although the annotation paradigm based on Large Language Models (LLMs) has
made significant breakthroughs in recent years, its actual deployment still has
two core bottlenecks: first, the cost of calling commercial APIs in large-scale
annotation is very expensive; second, in scenarios that require fine-grained
semantic understanding, such as sentiment classification and toxicity
classification, the annotation accuracy of LLMs is even lower than that of
Small Language Models (SLMs) dedicated to this field. To address these
problems, we propose a new paradigm of multi-model cooperative annotation and
design a fully automatic annotation framework AutoAnnotator based on this.
Specifically, AutoAnnotator consists of two layers. The upper-level
meta-controller layer uses the generation and reasoning capabilities of LLMs to
select SLMs for annotation, automatically generate annotation code and verify
difficult samples; the lower-level task-specialist layer consists of multiple
SLMs that perform annotation through multi-model voting. In addition, we use
the difficult samples obtained by the secondary review of the meta-controller
layer as the reinforcement learning set and fine-tune the SLMs in stages
through a continual learning strategy, thereby improving the generalization of
SLMs. Extensive experiments show that AutoAnnotator outperforms existing
open-source/API LLMs in zero-shot, one-shot, CoT, and majority voting settings.
Notably, AutoAnnotator reduces the annotation cost by 74.15% compared to
directly annotating with GPT-3.5-turbo, while still improving the accuracy by
6.21%. Project page: https://github.com/Zhaiyuan-Ji/AutoAnnotator.

</details>


### [410] [NepaliGPT: A Generative Language Model for the Nepali Language](https://arxiv.org/abs/2506.16399)
*Shushanta Pudasaini,Aman Shakya,Siddhartha Shrestha,Sahil Bhatta,Sunil Thapa,Sushmita Palikhe*

Main category: cs.CL

TL;DR: 论文提出专为尼泊尔语设计的生成式大语言模型NepaliGPT，介绍相关语料库和基准数据集，展示模型在文本生成中的指标。


<details>
  <summary>Details</summary>
Motivation: 填补尼泊尔语NLP领域无生成式语言模型的研究空白。

Method: 收集多源数据构建尼泊尔语高级语料库Devanagari Corpus，引入含4296个尼泊尔语问答对的NepaliGPT基准数据集。

Result: NepaliGPT在文本生成中达到困惑度26.32245、ROUGE - 1分数0.2604、因果连贯性81.25%、因果一致性85.41%。

Conclusion: 成功提出适用于尼泊尔语的生成式大语言模型NepaliGPT。

Abstract: After the release of ChatGPT, Large Language Models (LLMs) have gained huge
popularity in recent days and thousands of variants of LLMs have been released.
However, there is no generative language model for the Nepali language, due to
which other downstream tasks, including fine-tuning, have not been explored
yet. To fill this research gap in the Nepali NLP space, this research proposes
\textit{NepaliGPT}, a generative large language model tailored specifically for
the Nepali language. This research introduces an advanced corpus for the Nepali
language collected from several sources, called the Devanagari Corpus.
Likewise, the research introduces the first NepaliGPT benchmark dataset
comprised of 4,296 question-answer pairs in the Nepali language. The proposed
LLM NepaliGPT achieves the following metrics in text generation: Perplexity of
26.32245, ROUGE-1 score of 0.2604, causal coherence of 81.25\%, and causal
consistency of 85.41\%.

</details>


### [411] [StoryWriter: A Multi-Agent Framework for Long Story Generation](https://arxiv.org/abs/2506.16445)
*Haotian Xia,Hao Peng,Yunjia Qi,Xiaozhi Wang,Bin Xu,Lei Hou,Juanzi Li*

Main category: cs.CL

TL;DR: 提出StoryWriter多智能体故事生成框架解决长故事生成难题，表现优于基线，还生成数据集训练模型提升长故事生成性能。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在长故事生成面临话语连贯性和叙事复杂性挑战。

Method: 提出StoryWriter框架，包含大纲、规划和写作三个智能体模块，进行人工和自动评估，用其生成数据集训练模型。

Result: StoryWriter在故事质量和长度上显著优于现有基线，生成含约6000个高质量长故事的数据集。

Conclusion: StoryWriter框架及基于其训练的模型在长故事生成方面有先进性能。

Abstract: Long story generation remains a challenge for existing large language models
(LLMs), primarily due to two main factors: (1) discourse coherence, which
requires plot consistency, logical coherence, and completeness in the long-form
generation, and (2) narrative complexity, which requires an interwoven and
engaging narrative. To address these challenges, we propose StoryWriter, a
multi-agent story generation framework, which consists of three main modules:
(1) outline agent, which generates event-based outlines containing rich event
plots, character, and event-event relationships. (2) planning agent, which
further details events and plans which events should be written in each chapter
to maintain an interwoven and engaging story. (3) writing agent, which
dynamically compresses the story history based on the current event to generate
and reflect new plots, ensuring the coherence of the generated story. We
conduct both human and automated evaluation, and StoryWriter significantly
outperforms existing story generation baselines in both story quality and
length. Furthermore, we use StoryWriter to generate a dataset, which contains
about $6,000$ high-quality long stories, with an average length of $8,000$
words. We train the model Llama3.1-8B and GLM4-9B using supervised fine-tuning
on LongStory and develop StoryWriter_GLM and StoryWriter_GLM, which
demonstrates advanced performance in long story generation.

</details>


### [412] [Towards Generalizable Generic Harmful Speech Datasets for Implicit Hate Speech Detection](https://arxiv.org/abs/2506.16476)
*Saad Almohaimeed,Saleh Almohaimeed,Damla Turgut,Ladislau Bölöni*

Main category: cs.CL

TL;DR: 本文提出利用现有有害言论数据集检测隐性仇恨言论的方法，实验表明该方法有效提升检测效果。


<details>
  <summary>Details</summary>
Motivation: 隐性仇恨言论成为社交媒体平台关键挑战，需要可泛化检测技术，现有标注数据集存在问题。

Method: 提出包含有影响力样本识别、重新标注和使用Llama - 3 70B与GPT - 4o进行增强的方法。

Result: 相比基线，F1分数提升12.9点，证明方法在提升隐性仇恨检测上有效。

Conclusion: 所提方法能有效检测隐性仇恨言论，增强跨数据集泛化能力。

Abstract: Implicit hate speech has recently emerged as a critical challenge for social
media platforms. While much of the research has traditionally focused on
harmful speech in general, the need for generalizable techniques to detect
veiled and subtle forms of hate has become increasingly pressing. Based on
lexicon analysis, we hypothesize that implicit hate speech is already present
in publicly available harmful speech datasets but may not have been explicitly
recognized or labeled by annotators. Additionally, crowdsourced datasets are
prone to mislabeling due to the complexity of the task and often influenced by
annotators' subjective interpretations. In this paper, we propose an approach
to address the detection of implicit hate speech and enhance generalizability
across diverse datasets by leveraging existing harmful speech datasets. Our
method comprises three key components: influential sample identification,
reannotation, and augmentation using Llama-3 70B and GPT-4o. Experimental
results demonstrate the effectiveness of our approach in improving implicit
hate detection, achieving a +12.9-point F1 score improvement compared to the
baseline.

</details>


### [413] [Relic: Enhancing Reward Model Generalization for Low-Resource Indic Languages with Few-Shot Examples](https://arxiv.org/abs/2506.16502)
*Soumya Suvra Ghosal,Vaibhav Singh,Akash Ghosh,Soumyabrata Pal,Subhadip Baidya,Sriparna Saha,Dinesh Manocha*

Main category: cs.CL

TL;DR: 提出RELIC框架解决低资源印度语系语言奖励模型不可靠问题，实验证明其显著提升奖励模型准确率。


<details>
  <summary>Details</summary>
Motivation: 多数开源多语言奖励模型主要在高资源语言偏好数据集上训练，导致低资源印度语系语言奖励信号不可靠，且收集数据成本高。

Method: 提出RELIC框架，用成对排序目标训练检索器，从辅助高资源语言中选择上下文示例。

Result: 在三个偏好数据集上实验，RELIC显著提高低资源印度语系语言奖励模型准确率，如在波多语上比零样本提示和现有示例选择方法分别提升12.81%和10.13%。

Conclusion: RELIC框架能有效解决低资源印度语系语言奖励建模问题，优于现有示例选择方法。

Abstract: Reward models are essential for aligning large language models (LLMs) with
human preferences. However, most open-source multilingual reward models are
primarily trained on preference datasets in high-resource languages, resulting
in unreliable reward signals for low-resource Indic languages. Collecting
large-scale, high-quality preference data for these languages is prohibitively
expensive, making preference-based training approaches impractical. To address
this challenge, we propose RELIC, a novel in-context learning framework for
reward modeling in low-resource Indic languages. RELIC trains a retriever with
a pairwise ranking objective to select in-context examples from auxiliary
high-resource languages that most effectively highlight the distinction between
preferred and less-preferred responses. Extensive experiments on three
preference datasets- PKU-SafeRLHF, WebGPT, and HH-RLHF-using state-of-the-art
open-source reward models demonstrate that RELIC significantly improves reward
model accuracy for low-resource Indic languages, consistently outperforming
existing example selection methods. For example, on Bodo-a low-resource Indic
language-using a LLaMA-3.2-3B reward model, RELIC achieves a 12.81% and 10.13%
improvement in accuracy over zero-shot prompting and state-of-the-art example
selection method, respectively.

</details>


### [414] [Measuring (a Sufficient) World Model in LLMs: A Variance Decomposition Framework](https://arxiv.org/abs/2506.16584)
*Nadav Kunievsky,James A. Evans*

Main category: cs.CL

TL;DR: 提出评估大语言模型世界模型的框架，分解模型响应变异性，评估不同领域大模型，发现大模型更能将输出变异性归因于用户目的，但优势不统一，强调语义诊断重要性。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型可靠性，了解其是否拥有世界模型。

Method: 提出评估框架，将模型响应变异性分解为用户目的、用户表达和模型不稳定性三部分。

Result: 大模型能将更多输出变异性归因于用户目的，显示更稳健世界模型，但优势不统一。

Conclusion: 应超越基于准确性的基准，采用语义诊断评估模型内部对世界的理解。

Abstract: Understanding whether large language models (LLMs) possess a world model-a
structured understanding of the world that supports generalization beyond
surface-level patterns-is central to assessing their reliability, especially in
high-stakes applications. We propose a formal framework for evaluating whether
an LLM exhibits a sufficiently robust world model, defined as producing
consistent outputs across semantically equivalent prompts while distinguishing
between prompts that express different intents. We introduce a new evaluation
approach to measure this that decomposes model response variability into three
components: variability due to user purpose, user articulation, and model
instability. An LLM with a strong world model should attribute most of the
variability in its responses to changes in foundational purpose rather than
superficial changes in articulation. This approach allows us to quantify how
much of a model's behavior is semantically grounded rather than driven by model
instability or alternative wording. We apply this framework to evaluate LLMs
across diverse domains. Our results show how larger models attribute a greater
share of output variability to changes in user purpose, indicating a more
robust world model. This improvement is not uniform, however: larger models do
not consistently outperform smaller ones across all domains, and their
advantage in robustness is often modest. These findings highlight the
importance of moving beyond accuracy-based benchmarks toward semantic
diagnostics that more directly assess the structure and stability of a model's
internal understanding of the world.

</details>


### [415] [Modeling Public Perceptions of Science in Media](https://arxiv.org/abs/2506.16622)
*Jiaxin Pei,Dustin Wright,Isabelle Augenstin,David Jurgens*

Main category: cs.CL

TL;DR: 本文提出计算框架建模公众对科学新闻的感知，创建数据集，开发NLP模型预测感知分数，研究发现新闻消费频率影响感知，感知与公众参与度有关，强调感知建模在科学传播中的重要性。


<details>
  <summary>Details</summary>
Motivation: 有效让公众参与科学对科学界至关重要，但科学传播者难预测受众对科学新闻的感知和互动，因此开展研究。

Method: 引入计算框架对公众感知进行多维度建模，创建大规模科学新闻感知数据集，开发NLP模型预测感知分数，从两个视角研究公众对科学的感知，还进行大规模分析和自然实验。

Result: 个人科学新闻消费频率影响感知，人口因素影响小；估计的公众感知与最终参与模式有直接联系，积极感知的帖子评论和点赞更多。

Conclusion: 强调在科学传播中细致的感知建模的重要性，为预测公众对科学内容的兴趣和参与度提供新途径。

Abstract: Effectively engaging the public with science is vital for fostering trust and
understanding in our scientific community. Yet, with an ever-growing volume of
information, science communicators struggle to anticipate how audiences will
perceive and interact with scientific news. In this paper, we introduce a
computational framework that models public perception across twelve dimensions,
such as newsworthiness, importance, and surprisingness. Using this framework,
we create a large-scale science news perception dataset with 10,489 annotations
from 2,101 participants from diverse US and UK populations, providing valuable
insights into public responses to scientific information across domains. We
further develop NLP models that predict public perception scores with a strong
performance. Leveraging the dataset and model, we examine public perception of
science from two perspectives: (1) Perception as an outcome: What factors
affect the public perception of scientific information? (2) Perception as a
predictor: Can we use the estimated perceptions to predict public engagement
with science? We find that individuals' frequency of science news consumption
is the driver of perception, whereas demographic factors exert minimal
influence. More importantly, through a large-scale analysis and carefully
designed natural experiment on Reddit, we demonstrate that the estimated public
perception of scientific information has direct connections with the final
engagement pattern. Posts with more positive perception scores receive
significantly more comments and upvotes, which is consistent across different
scientific information and for the same science, but are framed differently.
Overall, this research underscores the importance of nuanced perception
modeling in science communication, offering new pathways to predict public
interest and engagement with scientific content.

</details>


### [416] [GeoGuess: Multimodal Reasoning based on Hierarchy of Visual Information in Street View](https://arxiv.org/abs/2506.16633)
*Fenghua Cheng,Jinxiang Wang,Sen Wang,Zi Huang,Xue Li*

Main category: cs.CL

TL;DR: 本文引入多模态推理新任务GeoGuess，构建基准数据集GeoExplain，并提出SightSense方法，实验显示其在GeoGuess任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有多模态推理任务缺乏对不同粒度层次视觉线索的推理，而实际场景中常涉及此类推理，为填补这一空白开展研究。

Method: 引入GeoGuess任务，构建GeoExplain数据集，提出SightSense多模态多层次推理方法。

Result: 分析和实验表明SightSense在GeoGuess任务中表现出色。

Conclusion: GeoGuess任务及相关数据集和方法对多模态推理研究有重要意义，能推动多模态推理能力发展。

Abstract: Multimodal reasoning is a process of understanding, integrating and inferring
information across different data modalities. It has recently attracted surging
academic attention as a benchmark for Artificial Intelligence (AI). Although
there are various tasks for evaluating multimodal reasoning ability, they still
have limitations. Lack of reasoning on hierarchical visual clues at different
levels of granularity, e.g., local details and global context, is of little
discussion, despite its frequent involvement in real scenarios. To bridge the
gap, we introduce a novel and challenging task for multimodal reasoning, namely
GeoGuess. Given a street view image, the task is to identify its location and
provide a detailed explanation. A system that succeeds in GeoGuess should be
able to detect tiny visual clues, perceive the broader landscape, and associate
with vast geographic knowledge. Therefore, GeoGuess would require the ability
to reason between hierarchical visual information and geographic knowledge. In
this work, we establish a benchmark for GeoGuess by introducing a specially
curated dataset GeoExplain which consists of
panoramas-geocoordinates-explanation tuples. Additionally, we present a
multimodal and multilevel reasoning method, namely SightSense which can make
prediction and generate comprehensive explanation based on hierarchy of visual
information and external knowledge. Our analysis and experiments demonstrate
their outstanding performance in GeoGuess.

</details>


### [417] [Long-Context Generalization with Sparse Attention](https://arxiv.org/abs/2506.16640)
*Pavlo Vasylenko,Marcos Treviso,André F. T. Martins*

Main category: cs.CL

TL;DR: 本文指出传统Transformer架构的softmax注意力权重存在问题，提出用α - entmax的稀疏注意力机制避免问题，引入ASEntmax并结合位置编码，模型在长上下文泛化任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer架构使用softmax计算注意力权重，在处理需精确关注固定大小模式的任务时，序列长度增加会导致非信息性token积累注意力概率，造成分散和表征崩溃。

Method: 使用α - entmax的稀疏注意力机制，引入Adaptive - Scalable Entmax（ASEntmax），并设计合适的位置编码，将ASEntmax集成到标准Transformer层。

Result: 模型在长上下文泛化任务上大幅超越softmax、可扩展softmax和固定温度α - entmax基线。

Conclusion: α - entmax的稀疏注意力机制能避免传统softmax的问题，结合ASEntmax和合适的位置编码可有效提升模型在长上下文泛化任务中的性能。

Abstract: Transformer-based architectures traditionally employ softmax to compute
attention weights, which produces dense distributions over all tokens in a
sequence. While effective in many settings, this density has been shown to be
detrimental for tasks that demand precise focus on fixed-size patterns: as
sequence length increases, non-informative tokens accumulate attention
probability mass, leading to dispersion and representational collapse. We show
in this paper that sparse attention mechanisms using $\alpha$-entmax can avoid
these issues, due to their ability to assign exact zeros to irrelevant tokens.
Furthermore, we introduce Adaptive-Scalable Entmax (ASEntmax), which endows
$\alpha$-entmax with a learnable temperature parameter, allowing the attention
distribution to interpolate between sparse (pattern-focused) and dense
(softmax-like) regimes. Finally, we show that the ability to locate and
generalize fixed-size patterns can be further improved through a careful design
of position encodings, which impacts both dense and sparse attention methods.
By integrating ASEntmax into standard transformer layers alongside proper
positional encodings, we show that our models greatly outperform softmax,
scalable softmax, and fixed-temperature $\alpha$-entmax baselines on
long-context generalization.

</details>


### [418] [ReasonGRM: Enhancing Generative Reward Models through Large Reasoning Models](https://arxiv.org/abs/2506.16712)
*Bin Chen,Xinzge Gao,Chuanrui Hu,Penghang Yu,Hua Zhang,Bing-Kun Bao*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Generative Reward Models (GRMs) provide greater flexibility than scalar
reward models in capturing human preferences, but their effectiveness is
limited by poor reasoning capabilities. This often results in incomplete or
overly speculative reasoning paths, leading to hallucinations or missing key
information in complex tasks. We address this challenge with ReasonGRM, a
three-stage generative reward modeling framework. In the first stage, Zero-RL
is used to generate concise, outcome-directed reasoning paths that reduce the
likelihood of critical omissions. In the second stage, we introduce a novel
evaluation metric, $R^\star$, which scores reasoning paths based on their
generation likelihood. This favors paths that reach correct answers with
minimal exploration, helping to reduce hallucination-prone data during
training. In the final stage, the model is further refined through
reinforcement learning on challenging examples to enhance its preference
discrimination capabilities. Experiments on three public benchmarks show that
ReasonGRM achieves competitive or state-of-the-art performance, outperforming
previous best GRMs by 1.8\% on average and surpassing proprietary models such
as GPT-4o by up to 5.6\%. These results demonstrate the effectiveness of
reasoning-aware training and highlight the importance of high-quality rationale
selection for reliable preference modeling.

</details>


### [419] [The Role of Model Confidence on Bias Effects in Measured Uncertainties](https://arxiv.org/abs/2506.16724)
*Xinyi Liu,Weiguang Wang,Hangfeng He*

Main category: cs.CL

TL;DR: 研究大语言模型在开放式任务中偏差对不确定性量化的影响，发现缓解提示偏差能改善GPT - 4o不确定性量化，低无偏差模型置信度下偏差影响更大。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型用于开放式任务，准确评估认知不确定性对确保可靠结果至关重要，但量化该不确定性面临挑战，需研究偏差在其中的权衡。

Method: 在视觉问答任务上进行实验，分析提示偏差对GPT - 4o和Qwen2 - VL在不同无偏差置信水平下认知和随机不确定性的影响。

Result: 缓解提示偏差改善GPT - 4o不确定性量化；低无偏差模型置信度下，偏差对两种不确定性影响更大；低无偏差模型置信度会因偏差导致认知不确定性低估，对随机不确定性估计方向无显著影响。

Conclusion: 这些不同影响加深了对偏差缓解用于不确定性量化的理解，可能为开发更先进技术提供参考。

Abstract: With the growing adoption of Large Language Models (LLMs) for open-ended
tasks, accurately assessing epistemic uncertainty, which reflects a model's
lack of knowledge, has become crucial to ensuring reliable outcomes. However,
quantifying epistemic uncertainty in such tasks is challenging due to the
presence of aleatoric uncertainty, which arises from multiple valid answers.
While bias can introduce noise into epistemic uncertainty estimation, it may
also reduce noise from aleatoric uncertainty. To investigate this trade-off, we
conduct experiments on Visual Question Answering (VQA) tasks and find that
mitigating prompt-introduced bias improves uncertainty quantification in
GPT-4o. Building on prior work showing that LLMs tend to copy input information
when model confidence is low, we further analyze how these prompt biases affect
measured epistemic and aleatoric uncertainty across varying bias-free
confidence levels with GPT-4o and Qwen2-VL. We find that all considered biases
induce greater changes in both uncertainties when bias-free model confidence is
lower. Moreover, lower bias-free model confidence leads to greater
underestimation of epistemic uncertainty (i.e. overconfidence) due to bias,
whereas it has no significant effect on the direction of changes in aleatoric
uncertainty estimation. These distinct effects deepen our understanding of bias
mitigation for uncertainty quantification and potentially inform the
development of more advanced techniques.

</details>


### [420] [LM-SPT: LM-Aligned Semantic Distillation for Speech Tokenization](https://arxiv.org/abs/2506.16738)
*Daejin Jo,Jeeyoung Yun,Byungseok Roh,Sungwoong Kim*

Main category: cs.CL

TL;DR: 提出LM - SPT语音分词方法，引入新语义蒸馏，结合架构改进，支持多帧率，实验显示性能优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有语音分词方法产生的语音标记序列比文本对应序列长，标准降帧率技术会扭曲语义结构，不利于高效语音 - 语言建模。

Method: 提出LM - SPT方法，引入新语义蒸馏，通过重构语音最小化原始和重构波形编码表示的差异，还对编码器和解码器进行架构改进，支持多帧率。

Result: LM - SPT在重建保真度上优于基线，基于其标记训练的SLMs在语音转文本任务有竞争力，在文本转语音任务上始终优于基线。

Conclusion: LM - SPT是一种有效的语音分词方法，能更好地使语音标记与语言模型语义对齐，提升语音 - 语言建模性能。

Abstract: With the rapid progress of speech language models (SLMs), discrete speech
tokens have emerged as a core interface between speech and text, enabling
unified modeling across modalities. Recent speech tokenization approaches aim
to isolate semantic information from low-level acoustics to better align with
language models. In particular, previous methods use SSL teachers such as
HuBERT to extract semantic representations, which are then distilled into a
semantic quantizer to suppress acoustic redundancy as well as capture
content-related latent structures. However, they still produce speech token
sequences significantly longer than their textual counterparts, creating
challenges for efficient speech-language modeling. Reducing the frame rate is a
natural solution, but standard techniques, such as rigid average pooling across
frames, can distort or dilute the semantic structure required for effective LM
alignment. To address this, we propose LM-SPT, a speech tokenization method
that introduces a novel semantic distillation. Instead of directly matching
teacher and student features via pooling, we reconstruct speech solely from
semantic tokens and minimize the discrepancy between the encoded
representations of the original and reconstructed waveforms, obtained from a
frozen automatic speech recognition (ASR) encoder. This indirect yet
data-driven supervision enables the tokenizer to learn discrete units that are
more semantically aligned with language models. LM-SPT further incorporates
architectural improvements to the encoder and decoder for speech tokenization,
and supports multiple frame rates, including 25Hz, 12.5Hz, and 6.25Hz.
Experimental results show that LM-SPT achieves superior reconstruction fidelity
compared to baselines, and that SLMs trained with LM-SPT tokens achieve
competitive performances on speech-to-text and consistently outperform
baselines on text-to-speech tasks.

</details>


### [421] [Language-Informed Synthesis of Rational Agent Models for Grounded Theory-of-Mind Reasoning On-The-Fly](https://arxiv.org/abs/2506.16755)
*Lance Ying,Ryan Truong,Katherine M. Collins,Cedegao E. Zhang,Megan Wei,Tyler Brooke-Wilson,Tan Zhi-Xuan,Lionel Wong,Joshua B. Tenenbaum*

Main category: cs.CL

TL;DR: 本文提出LIRAS框架用于整合语言和视觉输入进行特定情境的社会推理，在多项任务中表现优于消融模型和SOTA模型。


<details>
  <summary>Details</summary>
Motivation: 现实社会推理需考虑多模态信息，语言在社交场景是强大信息源，需要一个整合语言和视觉输入进行社会推理的框架。

Method: 提出LIRAS框架，将多模态社会推理构建为特定情境的智能体和环境表征过程，利用多模态语言模型解析输入为统一符号表征，运行贝叶斯逆规划引擎得出概率判断。

Result: 在一系列社会推理任务中，使用相对轻量级VLM实例化的模型在各领域捕捉人类判断的表现优于消融模型和SOTA模型。

Conclusion: LIRAS框架能有效整合语言和视觉输入进行社会推理，表现出色。

Abstract: Drawing real world social inferences usually requires taking into account
information from multiple modalities. Language is a particularly powerful
source of information in social settings, especially in novel situations where
language can provide both abstract information about the environment dynamics
and concrete specifics about an agent that cannot be easily visually observed.
In this paper, we propose Language-Informed Rational Agent Synthesis (LIRAS), a
framework for drawing context-specific social inferences that integrate
linguistic and visual inputs. LIRAS frames multimodal social reasoning as a
process of constructing structured but situation-specific agent and environment
representations - leveraging multimodal language models to parse language and
visual inputs into unified symbolic representations, over which a Bayesian
inverse planning engine can be run to produce granular probabilistic judgments.
On a range of existing and new social reasoning tasks derived from cognitive
science experiments, we find that our model (instantiated with a comparatively
lightweight VLM) outperforms ablations and state-of-the-art models in capturing
human judgments across all domains.

</details>


### [422] [MIST: Jailbreaking Black-box Large Language Models via Iterative Semantic Tuning](https://arxiv.org/abs/2506.16792)
*Muyang Zheng,Yuanzhi Yao,Changting Lin,Rui Wang,Meng Han*

Main category: cs.CL

TL;DR: 提出通过迭代语义调整越狱黑盒大语言模型的方法MIST，实验证明其有效性和实用性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型易受越狱攻击，且越狱黑盒模型面临输入离散、访问受限和查询预算有限等挑战。

Method: 提出MIST方法，通过迭代细化提示以诱导有害内容，包含顺序同义词搜索和顺序确定优化两种策略。

Result: 在多个开源和闭源模型上实验表明，MIST与其他先进方法相比，有有竞争力的攻击成功率和攻击可迁移性，且具有计算效率。

Conclusion: MIST是一种有效的黑盒大语言模型越狱方法，具有实际应用可行性。

Abstract: Despite efforts to align large language models (LLMs) with societal and moral
values, these models remain susceptible to jailbreak attacks--methods designed
to elicit harmful responses. Jailbreaking black-box LLMs is considered
challenging due to the discrete nature of token inputs, restricted access to
the target LLM, and limited query budget. To address the issues above, we
propose an effective method for jailbreaking black-box large language Models
via Iterative Semantic Tuning, named MIST. MIST enables attackers to
iteratively refine prompts that preserve the original semantic intent while
inducing harmful content. Specifically, to balance semantic similarity with
computational efficiency, MIST incorporates two key strategies: sequential
synonym search, and its advanced version--order-determining optimization.
Extensive experiments across two open-source models and four closed-source
models demonstrate that MIST achieves competitive attack success rates and
attack transferability compared with other state-of-the-art white-box and
black-box jailbreak methods. Additionally, we conduct experiments on
computational efficiency to validate the practical viability of MIST.

</details>


### [423] [Language Bottleneck Models: A Framework for Interpretable Knowledge Tracing and Beyond](https://arxiv.org/abs/2506.16982)
*Antonin Berthon,Mihaela van der Schaar*

Main category: cs.CL

TL;DR: 文章将知识追踪重铸为逆问题，提出语言瓶颈模型LBM，实验显示其精度与现有方法相当且所需学生轨迹少，用特定训练方法可提升摘要质量。


<details>
  <summary>Details</summary>
Motivation: 传统知识追踪方法缺乏可解释性，基于大语言模型的方法存在无准确性保证和可能产生幻觉的问题。

Method: 将知识追踪重铸为逆问题，构建由编码器大语言模型和冻结的解码器大语言模型组成的LBM，约束预测信息通过自然语言瓶颈；用组相对策略优化训练编码器。

Result: 在合成算术基准和大规模Eedi数据集上实验，LBM精度与现有方法相当，所需学生轨迹少，用下游解码精度作为奖励信号训练编码器能提升摘要质量。

Conclusion: LBM能保证知识摘要准确且具有可解释性，在知识追踪任务中表现良好。

Abstract: Accurately assessing student knowledge is critical for effective education,
yet traditional Knowledge Tracing (KT) methods rely on opaque latent
embeddings, limiting interpretability. Even LLM-based approaches generate
direct predictions or summaries that may hallucinate without any accuracy
guarantees. We recast KT as an inverse problem: learning the minimum
natural-language summary that makes past answers explainable and future answers
predictable. Our Language Bottleneck Model (LBM) consists of an encoder LLM
that writes an interpretable knowledge summary and a frozen decoder LLM that
must reconstruct and predict student responses using only that summary text. By
constraining all predictive information to pass through a short
natural-language bottleneck, LBMs ensure that the summary contains accurate
information while remaining human-interpretable. Experiments on synthetic
arithmetic benchmarks and the large-scale Eedi dataset show that LBMs rival the
accuracy of state-of-the-art KT and direct LLM methods while requiring
orders-of-magnitude fewer student trajectories. We demonstrate that training
the encoder with group-relative policy optimization, using downstream decoding
accuracy as a reward signal, effectively improves summary quality.

</details>


### [424] [TeXpert: A Multi-Level Benchmark for Evaluating LaTeX Code Generation by LLMs](https://arxiv.org/abs/2506.16990)
*Sahil Kale,Vijaykant Nadadur*

Main category: cs.CL

TL;DR: 提出TeXpert基准数据集评估大语言模型生成LaTeX代码能力，分析结果并给出发现。


<details>
  <summary>Details</summary>
Motivation: 现有基准缺乏对大语言模型使用自然语言指令生成LaTeX代码能力的评估。

Method: 引入TeXpert基准数据集，对多种开源和闭源大语言模型进行评估。

Result: 标准基准表现好的模型在LaTeX生成任务中表现差；开源模型如DeepSeek v3和DeepSeek Coder可与闭源模型竞争；格式和包错误普遍。

Conclusion: 多数大语言模型训练数据中缺少多样化LaTeX示例。

Abstract: LaTeX's precision and flexibility in typesetting have made it the gold
standard for the preparation of scientific documentation. Large Language Models
(LLMs) present a promising opportunity for researchers to produce
publication-ready material using LaTeX with natural language instructions, yet
current benchmarks completely lack evaluation of this ability. By introducing
TeXpert, our benchmark dataset with natural language prompts for generating
LaTeX code focused on components of scientific documents across multiple
difficulty levels, we conduct an in-depth analysis of LLM performance in this
regard and identify frequent error types. Our evaluation across open and
closed-source LLMs highlights multiple key findings: LLMs excelling on standard
benchmarks perform poorly in LaTeX generation with a significant accuracy
drop-off as the complexity of tasks increases; open-source models like DeepSeek
v3 and DeepSeek Coder strongly rival closed-source counterparts in LaTeX tasks;
and formatting and package errors are unexpectedly prevalent, suggesting a lack
of diverse LaTeX examples in the training datasets of most LLMs. Our dataset,
code, and model evaluations are available at
https://github.com/knowledge-verse-ai/TeXpert.

</details>


### [425] [Instituto de Telecomunicações at IWSLT 2025: Aligning Small-Scale Speech and Language Models for Speech-to-Text Learning](https://arxiv.org/abs/2506.17019)
*Giuseppe Attanasio,Sonal Sannigrahi,Ben Peters,André F. T. Martins*

Main category: cs.CL

TL;DR: 本文介绍IT - IST团队针对IWSLT 2025指令跟随语音处理共享任务的提交情况，涵盖短赛道任务，采用统一语音转文本模型，注重小模型骨干和优质数据。


<details>
  <summary>Details</summary>
Motivation: 参与IWSLT 2025共享任务中的指令跟随语音处理，提交短赛道任务结果。

Method: 使用统一语音转文本模型，分模态对齐和指令微调两个阶段，结合预训练连续语音编码器和文本解码器，采用小尺度语言模型骨干和高质量数据及合成数据。

Result: 未提及具体结果。

Conclusion: 未提及明确结论。

Abstract: This paper presents the IT-IST submission to the IWSLT 2025 Shared Task on
Instruction Following Speech Processing. We submit results for the Short Track,
i.e., speech recognition, translation, and spoken question answering. Our model
is a unified speech-to-text model that integrates a pre-trained continuous
speech encoder and text decoder through a first phase of modality alignment and
a second phase of instruction fine-tuning. Crucially, we focus on using
small-scale language model backbones (< 2B) and restrict to high-quality, CC-BY
data along with synthetic data generation to supplement existing resources.

</details>


### [426] [Enhancing Document-Level Question Answering via Multi-Hop Retrieval-Augmented Generation with LLaMA 3](https://arxiv.org/abs/2506.16037)
*Xinyue Huang,Ziqi Lin,Fang Sun,Wenchao Zhang,Kejian Tong,Yunbo Liu*

Main category: cs.CL

TL;DR: 本文提出针对复杂问答任务的RAG框架，基于LLaMA 3，结合多机制和优化策略，实验显示性能优于基线。


<details>
  <summary>Details</summary>
Motivation: 解决多跳推理和长文档上下文理解在复杂问答任务中的挑战。

Method: 基于LLaMA 3构建框架，集成密集检索模块、上下文融合和多跳推理机制，采用结合检索似然和生成交叉熵的联合优化策略。

Result: 所提系统在实验中表现优于现有检索增强和生成基线。

Conclusion: 所提系统能有效给出精确、有上下文依据的答案。

Abstract: This paper presents a novel Retrieval-Augmented Generation (RAG) framework
tailored for complex question answering tasks, addressing challenges in
multi-hop reasoning and contextual understanding across lengthy documents.
Built upon LLaMA 3, the framework integrates a dense retrieval module with
advanced context fusion and multi-hop reasoning mechanisms, enabling more
accurate and coherent response generation. A joint optimization strategy
combining retrieval likelihood and generation cross-entropy improves the
model's robustness and adaptability. Experimental results show that the
proposed system outperforms existing retrieval-augmented and generative
baselines, confirming its effectiveness in delivering precise, contextually
grounded answers.

</details>


### [427] [Tower+: Bridging Generality and Translation Specialization in Multilingual LLMs](https://arxiv.org/abs/2506.17080)
*Ricardo Rei,Nuno M. Guerreiro,José Pombal,João Alves,Pedro Teixeirinha,Amin Farajian,André F. T. Martins*

Main category: cs.CL

TL;DR: 本文介绍Tower+模型套件，通过新训练方法平衡翻译和多语言通用文本能力，不同规模模型表现优异，证明可在特定领域优化同时具备通用能力。


<details>
  <summary>Details</summary>
Motivation: 微调预训练大模型在特定任务有效，但会牺牲通用能力，本文旨在设计模型同时具备翻译和多语言通用文本能力。

Method: 基于Tower引入新训练方法，包括持续预训练、监督微调、偏好优化和可验证奖励的强化学习，各阶段精心生成和整理数据。

Result: 小模型常超越更大的通用开放权重和专有大模型，最大模型在高资源语言翻译和多语言评估中表现出色。

Conclusion: 有可能在优化特定业务领域（如翻译和本地化）的同时，与前沿模型的通用能力相抗衡。

Abstract: Fine-tuning pretrained LLMs has been shown to be an effective strategy for
reaching state-of-the-art performance on specific tasks like machine
translation. However, this process of adaptation often implies sacrificing
general-purpose capabilities, such as conversational reasoning and
instruction-following, hampering the utility of the system in real-world
applications that require a mixture of skills. In this paper, we introduce
Tower+, a suite of models designed to deliver strong performance across both
translation and multilingual general-purpose text capabilities. We achieve a
Pareto frontier between translation specialization and multilingual
general-purpose capabilities by introducing a novel training recipe that builds
on Tower (Alves et al., 2024), comprising continued pretraining, supervised
fine-tuning, preference optimization, and reinforcement learning with
verifiable rewards. At each stage of training, we carefully generate and curate
data to strengthen performance on translation as well as general-purpose tasks
involving code generation, mathematics problem solving, and general
instruction-following. We develop models at multiple scales: 2B, 9B, and 72B.
Our smaller models often outperform larger general-purpose open-weight and
proprietary LLMs (e.g., Llama 3.3 70B, GPT-4o). Our largest model delivers
best-in-class translation performance for high-resource languages and top
results in multilingual Arena Hard evaluations and in IF-MT, a benchmark we
introduce for evaluating both translation and instruction-following. Our
findings highlight that it is possible to rival frontier models in general
capabilities, while optimizing for specific business domains, such as
translation and localization.

</details>


### [428] [When Does Divide and Conquer Work for Long Context LLM? A Noise Decomposition Framework](https://arxiv.org/abs/2506.16411)
*Zhen Xu,Shang Zhu,Jue Wang,Junlin Wang,Ben Athiwaratkun,Chi Wang,James Zou,Ce Zhang*

Main category: cs.CL

TL;DR: 本文研究大语言模型处理长文本的挑战，提出理论框架，分析多智能体分块策略的有效性并通过实验验证，解释弱模型分块处理能超越强模型单步处理的原因。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型处理长文本的难题。

Method: 提出理论框架区分长上下文任务的失败模式，分析多智能体分块策略的有效性，通过检索、问答和摘要等任务进行实验。

Result: 实验证实理论分析和多智能体分块的适用条件，解释了弱模型分块处理在大输入时能超越强模型单步处理的原因。

Conclusion: 提出了处理长文本的原则性理解框架，强调精心管理分块和聚合策略可应对大语言模型的长上下文处理。

Abstract: We investigate the challenge of applying Large Language Models (LLMs) to long
texts. We propose a theoretical framework that distinguishes the failure modes
of long context tasks into three categories: cross-chunk dependence (task
noise), confusion that grows with context size (model noise), and the imperfect
integration of partial results (aggregator noise). Under this view, we analyze
when it is effective to use multi-agent chunking, i.e., dividing a length
sequence into smaller chunks and aggregating the processed results of each
chunk. Our experiments on tasks such as retrieval, question answering, and
summarization confirm both the theoretical analysis and the conditions that
favor multi-agent chunking. By exploring superlinear model noise growth with
input length, we also explain why, for large inputs, a weaker model configured
with chunk-based processing can surpass a more advanced model like GPT4o
applied in a single shot. Overall, we present a principled understanding
framework and our results highlight a direct pathway to handling long contexts
in LLMs with carefully managed chunking and aggregator strategies.

</details>


### [429] [Initial Investigation of LLM-Assisted Development of Rule-Based Clinical NLP System](https://arxiv.org/abs/2506.16628)
*Jianlin Shi,Brian T. Bucher*

Main category: cs.CL

TL;DR: 本文提出在基于规则的NLP系统开发阶段使用大语言模型，实验显示在文本片段识别和关键词提取上效果好，为NLP开发指明新方向。


<details>
  <summary>Details</summary>
Motivation: 基于规则的自然语言处理系统手动开发和维护劳动强度大，需克服这些局限。

Method: 在基于规则的系统开发阶段使用大语言模型，聚焦基于规则的NLP管道开发的前两步进行实验。

Result: 在识别临床相关文本片段上召回率高（Deepseek: 0.98, Qwen: 0.99），提取关键术语准确率为1.0。

Conclusion: 为NLP开发提供了有前景的新方向，相比深度学习模型解决方案，能实现基于规则系统的半自动化或自动化开发，执行更快、更具成本效益且更透明。

Abstract: Despite advances in machine learning (ML) and large language models (LLMs),
rule-based natural language processing (NLP) systems remain active in clinical
settings due to their interpretability and operational efficiency. However,
their manual development and maintenance are labor-intensive, particularly in
tasks with large linguistic variability. To overcome these limitations, we
proposed a novel approach employing LLMs solely during the rule-based systems
development phase. We conducted the initial experiments focusing on the first
two steps of developing a rule-based NLP pipeline: find relevant snippets from
the clinical note; extract informative keywords from the snippets for the
rule-based named entity recognition (NER) component. Our experiments
demonstrated exceptional recall in identifying clinically relevant text
snippets (Deepseek: 0.98, Qwen: 0.99) and 1.0 in extracting key terms for NER.
This study sheds light on a promising new direction for NLP development,
enabling semi-automated or automated development of rule-based systems with
significantly faster, more cost-effective, and transparent execution compared
with deep learning model-based solutions.

</details>


### [430] [From Data to Knowledge: Evaluating How Efficiently Language Models Learn Facts](https://arxiv.org/abs/2506.16912)
*Daniel Christoph,Max Ploner,Patrick Haller,Alan Akbik*

Main category: cs.CL

TL;DR: 研究分析不同架构和大小的模型在相同预训练数据上的表现，发现多数模型在高频事实表现相近，低频事实差异显著，为模型架构、大小与事实学习效率的关系提供新见解。


<details>
  <summary>Details</summary>
Motivation: 样本效率对语言模型训练效率有实际影响，现实文本信息呈长尾分布，期望模型学习和回忆常见及罕见事实，需分析模型应对学习和保留罕见信息的能力。

Method: 分析多种不同架构和大小、在相同预训练数据上训练的模型，通过标注训练语料中关系事实的频率，研究模型性能随事实频率的变化。

Result: 多数模型在高频事实表现相似，但在低频事实表现差异明显。

Conclusion: 分析为模型架构、大小与事实学习效率的关系提供了新见解。

Abstract: Sample efficiency is a crucial property of language models with practical
implications for training efficiency. In real-world text, information follows a
long-tailed distribution. Yet, we expect models to learn and recall frequent
and infrequent facts. Sample-efficient models are better equipped to handle
this challenge of learning and retaining rare information without requiring
excessive exposure. This study analyzes multiple models of varying
architectures and sizes, all trained on the same pre-training data. By
annotating relational facts with their frequencies in the training corpus, we
examine how model performance varies with fact frequency. Our findings show
that most models perform similarly on high-frequency facts but differ notably
on low-frequency facts. This analysis provides new insights into the
relationship between model architecture, size, and factual learning efficiency.

</details>


### [431] [MUCAR: Benchmarking Multilingual Cross-Modal Ambiguity Resolution for Multimodal Large Language Models](https://arxiv.org/abs/2506.17046)
*Xiaolong Wang,Zhaolu Kang,Wangyuxuan Zhai,Xinyue Lou,Yunghwei Lai,Ziyue Wang,Yawen Wang,Kaiyu Huang,Yile Wang,Peng Li,Yang Liu*

Main category: cs.CL

TL;DR: 提出新基准MUCAR评估多模态歧义解决能力，评估显示模型与人类水平有差距，需研究更复杂跨模态歧义理解方法。


<details>
  <summary>Details</summary>
Motivation: 现有多模态基准忽视语言和视觉歧义，未充分利用模态间相互澄清潜力，需新基准评估多模态歧义解决能力。

Method: 引入MUCAR基准，包括多语言数据集和双歧义数据集。

Result: 对19个先进多模态模型评估显示，与人类水平有显著差距。

Conclusion: 需要研究更复杂的跨模态歧义理解方法，推动多模态推理发展。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated significant
advances across numerous vision-language tasks. Due to their strong image-text
alignment capability, MLLMs can effectively understand image-text pairs with
clear meanings. However, effectively resolving the inherent ambiguities in
natural language and visual contexts remains challenging. Existing multimodal
benchmarks typically overlook linguistic and visual ambiguities, relying mainly
on unimodal context for disambiguation and thus failing to exploit the mutual
clarification potential between modalities. To bridge this gap, we introduce
MUCAR, a novel and challenging benchmark designed explicitly for evaluating
multimodal ambiguity resolution across multilingual and cross-modal scenarios.
MUCAR includes: (1) a multilingual dataset where ambiguous textual expressions
are uniquely resolved by corresponding visual contexts, and (2) a
dual-ambiguity dataset that systematically pairs ambiguous images with
ambiguous textual contexts, with each combination carefully constructed to
yield a single, clear interpretation through mutual disambiguation. Extensive
evaluations involving 19 state-of-the-art multimodal models--encompassing both
open-source and proprietary architectures--reveal substantial gaps compared to
human-level performance, highlighting the need for future research into more
sophisticated cross-modal ambiguity comprehension methods, further pushing the
boundaries of multimodal reasoning.

</details>


<div id='cond-mat.quant-gas'></div>

# cond-mat.quant-gas [[Back]](#toc)

### [432] [Single-shot thermometry of simulated Bose--Einstein condensates using artificial intelligence](https://arxiv.org/abs/2506.16925)
*Jack Griffiths,Steven A. Wrathmall,Simon A. Gardiner*

Main category: cond-mat.quant-gas

TL;DR: 提出用人工智能方法快速无损估计有限温度玻色气体的化学势和温度，模型能跨陷阱几何和热化动力学泛化，显示监督学习可克服超冷原子测温传统局限。


<details>
  <summary>Details</summary>
Motivation: 传统测量技术的破坏性和实验固有不确定性使超冷玻色气体热力学参数精确测定具有挑战性。

Method: 使用卷积神经网络，在准二维“煎饼”冷凝物的谐波陷阱配置上进行训练。

Result: 能在数秒内完成参数提取，在陷阱几何和热化动力学上有零样本泛化能力，估计环形陷阱冷凝物热力学参数误差仅几纳开尔文，在动态热化过程中保持预测精度。

Conclusion: 监督学习可克服超冷原子测温传统局限，有望扩展到更多场景，改进量子流体系统测量精度并简化实验流程。

Abstract: Precise determination of thermodynamic parameters in ultracold Bose gases
remains challenging due to the destructive nature of conventional measurement
techniques and inherent experimental uncertainties. We demonstrate an
artificial intelligence approach for rapid, non-destructive estimation of the
chemical potential and temperature from single-shot, in situ imaged density
profiles of finite-temperature Bose gases. Our convolutional neural network is
trained exclusively on quasi-2D `pancake' condensates in harmonic trap
configurations. It achieves parameter extraction within fractions of a second.
The model also demonstrates zero-shot generalisation across both trap geometry
and thermalisation dynamics, successfully estimating thermodynamic parameters
for toroidally trapped condensates with errors of only a few nanokelvin despite
no prior exposure to such geometries during training, and maintaining
predictive accuracy during dynamic thermalisation processes after a relatively
brief evolution without explicit training on non-equilibrium states. These
results suggest that supervised learning can overcome traditional limitations
in ultracold atom thermometry, with extension to broader geometric
configurations, temperature ranges, and additional parameters potentially
enabling comprehensive real-time analysis of quantum gas experiments. Such
capabilities could significantly streamline experimental workflows whilst
improving measurement precision across a range of quantum fluid systems.

</details>


<div id='astro-ph.GA'></div>

# astro-ph.GA [[Back]](#toc)

### [433] [Can AI Dream of Unseen Galaxies? Conditional Diffusion Model for Galaxy Morphology Augmentation](https://arxiv.org/abs/2506.16233)
*Chenrui Ma,Zechang Sun,Tao Jing,Zheng Cai,Yuan-Sen Ting,Song Huang,Mingyu Li*

Main category: astro-ph.GA

TL;DR: 本文提出条件扩散模型合成星系图像增强机器学习训练数据，提升形态分类和稀有天体检测性能。


<details>
  <summary>Details</summary>
Motivation: 机器学习在大规模天文观测调查中因标注数据集代表性有限，难以泛化，尤其对稀有天体检测存在挑战。

Method: 提出条件扩散模型，利用Galaxy Zoo 2数据集生成符合指定形态特征的星系图像，并将合成图像集成到机器学习流程中。

Result: 模型生成的星系图像多样且保真度高；在标准形态分类中关键指标的完整性和纯度提升达30%；在稀有天体检测中，检测到的早期型带尘埃带星系实例数量翻倍。

Conclusion: 生成模型能弥合稀缺标注数据与天文观测未知参数空间的差距，为未来天体物理基础模型发展提供见解。

Abstract: Observational astronomy relies on visual feature identification to detect
critical astrophysical phenomena. While machine learning (ML) increasingly
automates this process, models often struggle with generalization in
large-scale surveys due to the limited representativeness of labeled datasets
-- whether from simulations or human annotation -- a challenge pronounced for
rare yet scientifically valuable objects. To address this, we propose a
conditional diffusion model to synthesize realistic galaxy images for
augmenting ML training data. Leveraging the Galaxy Zoo 2 dataset which contains
visual feature -- galaxy image pairs from volunteer annotation, we demonstrate
that our model generates diverse, high-fidelity galaxy images closely adhere to
the specified morphological feature conditions. Moreover, this model enables
generative extrapolation to project well-annotated data into unseen domains and
advancing rare object detection. Integrating synthesized images into ML
pipelines improves performance in standard morphology classification, boosting
completeness and purity by up to 30\% across key metrics. For rare object
detection, using early-type galaxies with prominent dust lane features (
$\sim$0.1\% in GZ2 dataset) as a test case, our approach doubled the number of
detected instances from 352 to 872, compared to previous studies based on
visual inspection. This study highlights the power of generative models to
bridge gaps between scarce labeled data and the vast, uncharted parameter space
of observational astronomy and sheds insight for future astrophysical
foundation model developments. Our project homepage is available at
https://galaxysd-webpage.streamlit.app/.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [434] [Generative Modeling of Full-Atom Protein Conformations using Latent Diffusion on Graph Embeddings](https://arxiv.org/abs/2506.17064)
*Aditya Sengar,Ali Hariri,Daniel Probst,Patrick Barth,Pierre Vandergheynst*

Main category: q-bio.BM

TL;DR: 提出LD - FPG框架从分子动力学轨迹构建全原子蛋白质结构，在D2R - MD数据集上表现良好，为大蛋白全原子集合生成提供途径。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型在生成动态蛋白质多样全原子构象集合时存在简化原子细节或忽视构象多样性问题，需要更好的方法来理解蛋白质功能。

Method: 使用Chebyshev图神经网络获取蛋白质构象低维潜在嵌入，采用三种池化策略处理，用扩散模型生成新样本，解码器映射回笛卡尔坐标，可加入二面角损失正则化。

Result: 在D2R - MD数据集上，顺序和基于残基的池化策略以高结构保真度重现参考集合，恢复二面角分布，全原子lDDT约0.7，C - alpha - lDDT约0.8，Jensen - Shannon散度小于0.03。

Conclusion: LD - FPG为大蛋白提供了特定系统的全原子集合生成实用途径，是基于结构的复杂动态靶点治疗设计的有前景工具，相关数据集和实现免费可用。

Abstract: Generating diverse, all-atom conformational ensembles of dynamic proteins
such as G-protein-coupled receptors (GPCRs) is critical for understanding their
function, yet most generative models simplify atomic detail or ignore
conformational diversity altogether. We present latent diffusion for full
protein generation (LD-FPG), a framework that constructs complete all-atom
protein structures, including every side-chain heavy atom, directly from
molecular dynamics (MD) trajectories. LD-FPG employs a Chebyshev graph neural
network (ChebNet) to obtain low-dimensional latent embeddings of protein
conformations, which are processed using three pooling strategies: blind,
sequential and residue-based. A diffusion model trained on these latent
representations generates new samples that a decoder, optionally regularized by
dihedral-angle losses, maps back to Cartesian coordinates. Using D2R-MD, a
2-microsecond MD trajectory (12 000 frames) of the human dopamine D2 receptor
in a membrane environment, the sequential and residue-based pooling strategy
reproduces the reference ensemble with high structural fidelity (all-atom lDDT
of approximately 0.7; C-alpha-lDDT of approximately 0.8) and recovers backbone
and side-chain dihedral-angle distributions with a Jensen-Shannon divergence of
less than 0.03 compared to the MD data. LD-FPG thereby offers a practical route
to system-specific, all-atom ensemble generation for large proteins, providing
a promising tool for structure-based therapeutic design on complex, dynamic
targets. The D2R-MD dataset and our implementation are freely available to
facilitate further research.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [435] [RecBayes: Recurrent Bayesian Ad Hoc Teamwork in Large Partially Observable Domains](https://arxiv.org/abs/2506.15756)
*João G. Ribeiro,Yaniv Oren,Alberto Sardinha,Matthijs Spaan,Francisco S. Melo*

Main category: cs.MA

TL;DR: 提出RecBayes用于部分可观测下的临时团队协作，不依赖环境状态和队友行动，在大状态空间表现有效。


<details>
  <summary>Details</summary>
Motivation: 解决部分可观测下临时团队协作问题，现有方法存在依赖环境全观测或状态空间小等局限。

Method: 采用基于过往经验训练的循环贝叶斯分类器，使临时代理仅通过观测识别已知团队和任务。

Result: 在多智能体系统基准领域测试，能在1M状态和2^125观测规模下，仅通过部分观测识别团队和任务并有效协助解决任务。

Conclusion: RecBayes可在部分可观测、不依赖环境状态和队友行动、任意大状态空间下有效识别团队和任务，协助解决任务。

Abstract: This paper proposes RecBayes, a novel approach for ad hoc teamwork under
partial observability, a setting where agents are deployed on-the-fly to
environments where pre-existing teams operate, that never requires, at any
stage, access to the states of the environment or the actions of its teammates.
We show that by relying on a recurrent Bayesian classifier trained using past
experiences, an ad hoc agent is effectively able to identify known teams and
tasks being performed from observations alone. Unlike recent approaches such as
PO-GPL (Gu et al., 2021) and FEAT (Rahman et al., 2023), that require at some
stage fully observable states of the environment, actions of teammates, or
both, or approaches such as ATPO (Ribeiro et al., 2023) that require the
environments to be small enough to be tabularly modelled (Ribeiro et al.,
2023), in their work up to 4.8K states and 1.7K observations, we show RecBayes
is both able to handle arbitrarily large spaces while never relying on either
states and teammates' actions. Our results in benchmark domains from the
multi-agent systems literature, adapted for partial observability and scaled up
to 1M states and 2^125 observations, show that RecBayes is effective at
identifying known teams and tasks being performed from partial observations
alone, and as a result, is able to assist the teams in solving the tasks
effectively.

</details>


### [436] [Generalizable Agent Modeling for Agent Collaboration-Competition Adaptation with Multi-Retrieval and Dynamic Generation](https://arxiv.org/abs/2506.16718)
*Chenxu Wang,Yonggang Jin,Cheng Hu,Youpeng Zhao,Zipeng Dai,Jian Zhao,Shiyu Huang,Liuyu Xiang,Junge Zhang,Zhaofeng He*

Main category: cs.MA

TL;DR: 本文提出Agent Collaborative-Competitive Adaptation (ACCA) 设定和Multi-Retrieval and Dynamic Generation (MRDG) 方法，经测试能提升与未知对手和队友协作竞争能力。


<details>
  <summary>Details</summary>
Motivation: 解决单智能体适应新多智能体系统的挑战，基于已有简化场景提出更全面设定。

Method: 提出ACCA设定，引入MRDG方法，用行为轨迹建模队友和对手，含位置编码器、超网络模块和视角对齐模块。

Result: 在SMAC、Overcooked - AI和Melting Pot等基准场景测试中，MRDG显著提升与未知对手和队友的协作竞争能力，超越现有基线。

Conclusion: MRDG方法有效，代码开源在https://github.com/vcis - wangchenxu/MRDG.git 。

Abstract: Adapting a single agent to a new multi-agent system brings challenges,
necessitating adjustments across various tasks, environments, and interactions
with unknown teammates and opponents. Addressing this challenge is highly
complex, and researchers have proposed two simplified scenarios, Multi-agent
reinforcement learning for zero-shot learning and Ad-Hoc Teamwork. Building on
these foundations, we propose a more comprehensive setting, Agent
Collaborative-Competitive Adaptation (ACCA), which evaluates an agent to
generalize across diverse scenarios, tasks, and interactions with both
unfamiliar opponents and teammates. In ACCA, agents adjust to task and
environmental changes, collaborate with unseen teammates, and compete against
unknown opponents. We introduce a new modeling approach, Multi-Retrieval and
Dynamic Generation (MRDG), that effectively models both teammates and opponents
using their behavioral trajectories. This method incorporates a positional
encoder for varying team sizes and a hypernetwork module to boost agents'
learning and adaptive capabilities. Additionally, a viewpoint alignment module
harmonizes the observational perspectives of retrieved teammates and opponents
with the learning agent. Extensive tests in benchmark scenarios like SMAC,
Overcooked-AI, and Melting Pot show that MRDG significantly improves robust
collaboration and competition with unseen teammates and opponents, surpassing
established baselines. Our code is available at:
https://github.com/vcis-wangchenxu/MRDG.git

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [437] [HetGPU: The pursuit of making binary compatibility towards GPUs](https://arxiv.org/abs/2506.15993)
*Yiwei Yang,Yusheng Zheng,Tong Yu,Andi Quinn*

Main category: cs.AR

TL;DR: 提出hetGPU系统使单一GPU二进制文件能在多厂商硬件运行，初步评估显示可跨GPU迁移且开销小。


<details>
  <summary>Details</summary>
Motivation: 异构GPU基础设施存在二进制兼容性挑战，不同厂商GPU因指令集等差异无法运行其他厂商编译代码。

Method: 构建hetGPU系统，包含编译器、运行时和抽象层，编译器生成架构无关的GPU中间表示并插入元数据，运行时动态转换为目标GPU本地代码并提供统一抽象。

Result: 未修改的GPU二进制文件用hetGPU编译后可跨不同GPU迁移且开销极小。

Conclusion: hetGPU系统为厂商无关的GPU计算提供了可能。

Abstract: Heterogeneous GPU infrastructures present a binary compatibility challenge:
code compiled for one vendor's GPU will not run on another due to divergent
instruction sets, execution models, and driver stacks . We propose hetGPU, a
new system comprising a compiler, runtime, and abstraction layer that together
enable a single GPU binary to execute on NVIDIA, AMD, Intel, and Tenstorrent
hardware. The hetGPU compiler emits an architecture-agnostic GPU intermediate
representation (IR) and inserts metadata for managing execution state. The
hetGPU runtime then dynamically translates this IR to the target GPU's native
code and provides a uniform abstraction of threads, memory, and
synchronization. Our design tackles key challenges: differing SIMT vs. MIMD
execution (warps on NVIDIA/AMD vs. many-core RISC-V on Tenstorrent), varied
instruction sets, scheduling and memory model discrepancies, and the need for
state serialization for live migration. We detail the hetGPU architecture,
including the IR transformation pipeline, a state capture/reload mechanism for
live GPU migration, and an abstraction layer that bridges warp-centric and
core-centric designs. Preliminary evaluation demonstrates that unmodified GPU
binaries compiled with hetGPU can be migrated across disparate GPUs with
minimal overhead, opening the door to vendor-agnostic GPU computing.

</details>


### [438] [DeepRTL2: A Versatile Model for RTL-Related Tasks](https://arxiv.org/abs/2506.15697)
*Yi Liu,Hongji Zhang,Yunhao Zhou,Zhengyuan Shi,Changran Xu,Qiang Xu*

Main category: cs.AR

TL;DR: 提出DeepRTL2统一基于生成和嵌入的RTL任务，实验显示其在各任务达SOTA


<details>
  <summary>Details</summary>
Motivation: 先前研究忽视了对EDA工作流程同样关键的基于嵌入的任务，需全面解决EDA中的多样挑战

Method: 提出DeepRTL2这一多功能大语言模型家族，同时处理多种任务

Result: DeepRTL2在所有评估任务中达到了最先进的性能

Conclusion: DeepRTL2是首个为EDA中多样挑战提供全面解决方案的模型

Abstract: The integration of large language models (LLMs) into electronic design
automation (EDA) has significantly advanced the field, offering transformative
benefits, particularly in register transfer level (RTL) code generation and
understanding. While previous studies have demonstrated the efficacy of
fine-tuning LLMs for these generation-based tasks, embedding-based tasks, which
are equally critical to EDA workflows, have been largely overlooked. These
tasks, including natural language code search, RTL code functionality
equivalence checking, and performance prediction, are essential for
accelerating and optimizing the hardware design process. To address this gap,
we present DeepRTL2, a family of versatile LLMs that unifies both generation-
and embedding-based tasks related to RTL. By simultaneously tackling a broad
range of tasks, DeepRTL2 represents the first model to provide a comprehensive
solution to the diverse challenges in EDA. Through extensive experiments, we
show that DeepRTL2 achieves state-of-the-art performance across all evaluated
tasks.

</details>


### [439] [RCNet: $ΔΣ$ IADCs as Recurrent AutoEncoders](https://arxiv.org/abs/2506.16903)
*Arnaud Verdant,William Guicquero,Jérôme Chossat*

Main category: cs.AR

TL;DR: 提出用于Delta - Sigma ADCs的深度学习模型RCNet，能在一定硬件映射复杂度下优化SNR，提供设计权衡。


<details>
  <summary>Details</summary>
Motivation: 为Delta - Sigma ADCs设计更好的模型，解决硬件设计约束问题。

Method: 使用循环神经网络描述调制器和滤波器，结合高端优化器和自定义损失定义硬件设计约束。

Result: 在特定OSR下，RCNet能在SNR（>13bit）和面积约束（<14pF总电容）间提供设计权衡，最佳架构不依赖高阶调制器。

Conclusion: RCNet模型能在硬件设计约束下优化SNR，有额外拓扑探索自由度。

Abstract: This paper proposes a deep learning model (RCNet) for Delta-Sigma
($\Delta\Sigma$) ADCs. Recurrent Neural Networks (RNNs) allow to describe both
modulators and filters. This analogy is applied to Incremental ADCs (IADC).
High-end optimizers combined with full-custom losses are used to define
additional hardware design constraints: quantized weights, signal saturation,
temporal noise injection, devices area. Focusing on DC conversion, our early
results demonstrate that $SNR$ defined as an Effective Number Of Bits (ENOB)
can be optimized under a certain hardware mapping complexity. The proposed
RCNet succeeded to provide design tradeoffs in terms of $SNR$ ($>$13bit) versus
area constraints ($<$14pF total capacitor) at a given $OSR$ (80 samples).
Interestingly, it appears that the best RCNet architectures do not necessarily
rely on high-order modulators, leveraging additional topology exploration
degrees of freedom.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [440] [Modeling society with a responsible elite](https://arxiv.org/abs/2506.15877)
*Yana Tsodikova,Pavel Chebotarev*

Main category: physics.soc-ph

TL;DR: 在ViSE模型框架下研究含精英群体的社会动态，分析社会态度影响，发现负责精英对社会稳定作用及不同行为的影响。


<details>
  <summary>Details</summary>
Motivation: 研究社会中部分为精英群体时的社会动态，分析社会态度如集体主义、个人主义、利他主义对主体福祉的影响。

Method: 运用ViSE（Voting in a Stochastic Environment）模型进行分析。

Result: 负责精英能稳定社会、消除‘损失陷阱’悖论，与相同规模亲社会群体对社会益处相当；精英过度看重群体因素会使自身收入大增、社会收入下降；新负责精英出现可使社会再次稳定，旧精英失去主导地位。

Conclusion: 只要社会规模允许形成所需规模的新负责精英，上述社会动态过程可不断重复。

Abstract: Within the framework of the ViSE (Voting in a Stochastic Environment) model,
we examine the dynamics in a society, part of which can be considered an elite.
The model allows us to analyze the influence of social attitudes, such as
collectivism, individualism, altruism on the well-being of agents. The dynamics
is determined by collective decisions and changes in the structure of society,
in particular, by the formation of groups of cooperating agents. It is found
that the presence of a "responsible elite", combining the support of other
agents with limited concern for their own benefit, stabilizes society and
eliminates the "pit of losses" paradox. The benefit to society from having a
responsible elite is comparable to that from having a prosocial group of the
same size. If the elite radically increases the weight of the group component
in its combined voting strategy, then its incomes rise sharply, while society's
incomes decline. If, in response to the selfish transformation of the elite, a
new responsible elite emerges, proportionally larger than the previous one,
then society will stabilize again, and the old elite will lose its dominant
position. This process can be repeated as long as the size of society allows
the formation of new responsible elites of the required size.

</details>


### [441] [Autocratic strategies in Cournot oligopoly game](https://arxiv.org/abs/2506.16038)
*Masahiko Ueda,Shoma Yagi,Genki Ichinose*

Main category: physics.soc-ph

TL;DR: 本文研究重复古诺寡头博弈中的零行列式策略，证明存在平均不可战胜的零行列式策略，数值分析其对合谋的影响并指出其在寡头市场的负面影响。


<details>
  <summary>Details</summary>
Motivation: 近期重复囚徒困境中的零行列式策略受关注，人们尝试将其拓展到其他博弈，因此研究重复古诺寡头博弈中是否存在该策略及影响。

Method: 理论证明存在平均不可战胜的零行列式策略，数值分析该策略在面对不同数量适应性学习玩家时对合谋的影响。

Result: 证明重复古诺寡头博弈中存在平均不可战胜的零行列式策略；该策略在面对一个适应性学习玩家时可促进合谋，面对两个时则不能。

Conclusion: 研究结果阐明了零行列式策略在寡头市场的一些负面影响。

Abstract: An oligopoly is a market in which the price of a goods is controlled by a few
firms. Cournot introduced the simplest game-theoretic model of oligopoly, where
profit-maximizing behavior of each firm results in market failure. Furthermore,
when the Cournot oligopoly game is infinitely repeated, firms can tacitly
collude to monopolize the market. Such tacit collusion is realized by the same
mechanism as direct reciprocity in the repeated prisoner's dilemma game, where
mutual cooperation can be realized whereas defection is favorable for both
prisoners in one-shot game. Recently, in the repeated prisoner's dilemma game,
a class of strategies called zero-determinant strategies attracts much
attention in the context of direct reciprocity. Zero-determinant strategies are
autocratic strategies which unilaterally control payoffs of players. There were
many attempts to find zero-determinant strategies in other games and to extend
them so as to apply them to broader situations. In this paper, first, we show
that zero-determinant strategies exist even in the repeated Cournot oligopoly
game. Especially, we prove that an averagely unbeatable zero-determinant
strategy exists, which is guaranteed to obtain the average payoff of the
opponents. Second, we numerically show that the averagely unbeatable
zero-determinant strategy can be used to promote collusion when it is used
against an adaptively learning player, whereas it cannot promote collusion when
it is used against two adaptively learning players. Our findings elucidate some
negative impact of zero-determinant strategies in oligopoly market.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [442] [A Neural Operator based Hybrid Microscale Model for Multiscale Simulation of Rate-Dependent Materials](https://arxiv.org/abs/2506.16918)
*Dhananjeyan Jeyaraj,Hamidreza Eivazi,Jendrik-Alexander Tröger,Stefan Wittek,Stefan Hartmann,Andreas Rausch*

Main category: physics.comp-ph

TL;DR: 本文提出结合数据驱动与物理方法的混合模型，用于时变固体力学问题，计算高效且误差小。


<details>
  <summary>Details</summary>
Motivation: 传统多尺度建模方法计算密集，需引入深度学习技术加速多尺度模拟。

Method: 采用神经算子预测微观尺度物理，构建混合模型，将微观尺度本构关系融入架构并按物理原理计算内部变量。

Result: 均质应力误差小于6%，计算效率提高约100倍。

Conclusion: 该方法计算高效，适用于不同材料和空间离散化的时变固体力学问题。

Abstract: The behavior of materials is influenced by a wide range of phenomena
occurring across various time and length scales. To better understand the
impact of microstructure on macroscopic response, multiscale modeling
strategies are essential. Numerical methods, such as the $\text{FE}^2$
approach, account for micro-macro interactions to predict the global response
in a concurrent manner. However, these methods are computationally intensive
due to the repeated evaluations of the microscale. This challenge has led to
the integration of deep learning techniques into computational homogenization
frameworks to accelerate multiscale simulations. In this work, we employ neural
operators to predict the microscale physics, resulting in a hybrid model that
combines data-driven and physics-based approaches. This allows for
physics-guided learning and provides flexibility for different materials and
spatial discretizations. We apply this method to time-dependent solid mechanics
problems involving viscoelastic material behavior, where the state is
represented by internal variables only at the microscale. The constitutive
relations of the microscale are incorporated into the model architecture and
the internal variables are computed based on established physical principles.
The results for homogenized stresses ($<6\%$ error) show that the approach is
computationally efficient ($\sim 100 \times$ faster).

</details>


<div id='cond-mat.dis-nn'></div>

# cond-mat.dis-nn [[Back]](#toc)

### [443] [Uncertainty in AI-driven Monte Carlo simulations](https://arxiv.org/abs/2506.14594)
*Dimitrios Tzivrailis,Alberto Rosso,Eiji Kawasaki*

Main category: cond-mat.dis-nn

TL;DR: 提出惩罚集成方法 (PEM) 量化认知不确定性并减轻其对蒙特卡罗采样的影响，修改 Metropolis 接受规则提高模拟结果可靠性。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型作为替代函数进行蒙特卡罗模拟时会引入认知不确定性，影响系统宏观行为，需进行量化和缓解。

Method: 提出惩罚集成方法 (PEM)，引入不确定性感知的 Metropolis 接受规则修改，提高高不确定性区域的拒绝概率。

Result: 未明确提及具体结果，但表明该方法可增强模拟结果的可靠性。

Conclusion: PEM 方法能量化认知不确定性并减轻其对蒙特卡罗采样的影响，提高模拟结果可靠性。

Abstract: In the study of complex systems, evaluating physical observables often
requires sampling representative configurations via Monte Carlo techniques.
These methods rely on repeated evaluations of the system's energy and force
fields, which can become computationally expensive, particularly in the
presence of long-range interactions. To accelerate these simulations, deep
learning models are increasingly employed as surrogate functions to approximate
the energy landscape or force fields. However, such models introduce epistemic
uncertainty in their predictions, which may propagate through the sampling
process and affect the system's macroscopic behavior. In this work, we present
the Penalty Ensemble Method (PEM) to quantify epistemic uncertainty and
mitigate its impact on Monte Carlo sampling. Our approach introduces an
uncertainty-aware modification of the Metropolis acceptance rule, which
increases the rejection probability in regions of high uncertainty, thereby
enhancing the reliability of the simulation outcomes.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [444] [Quantum Fisher-Preconditioned Reinforcement Learning: From Single-Qubit Control to Rayleigh-Fading Link Adaptation](https://arxiv.org/abs/2506.15753)
*Oluwaseyi Giwa,Muhammad Ahmed Mohsin,Muhammad Ali Jamshed*

Main category: quant-ph

TL;DR: 提出量子预条件策略梯度算法QPPG，在经典和量子环境评估，收敛快且鲁棒性强。


<details>
  <summary>Details</summary>
Motivation: 解决链路自适应学习稳定性问题，探索量子强化学习可扩展性。

Method: 提出基于自然梯度的QPPG算法，使用带Tikhonov正则化的全逆量子Fisher信息白化策略更新。

Result: 在多种环境中，QPPG比REINFORCE收敛快4倍，有1 dB增益，百次迭代达90%回报，噪声鲁棒性高。

Conclusion: 基于全QFI的预条件处理对可扩展量子强化学习有优势。

Abstract: In this letter, we propose Quantum-Preconditioned Policy Gradient (QPPG), a
natural gradient-based algorithm for link adaptation that whitens policy
updates using the full inverse quantum Fisher information with Tikhonov
regularization. QPPG bridges classical and quantum geometry, achieving stable
learning even under noise. Evaluated on classical and quantum environments,
including noisy single-qubit Gym tasks and Rayleigh-fading channels, QPPG
converges 4 times faster than REINFORCE and sustains a 1 dB gain under
uncertainty. It reaches a 90 percent return in one hundred episodes with high
noise robustness, showcasing the advantages of full QFI-based preconditioning
for scalable quantum reinforcement learning.

</details>


### [445] [Compilation, Optimization, Error Mitigation, and Machine Learning in Quantum Algorithms](https://arxiv.org/abs/2506.15760)
*Shuangbao Paul Wang,Jianzhou Mao,Eric Sakk*

Main category: quant-ph

TL;DR: 本文探讨量子算法编译、优化和错误缓解，提出AQFT优化算法提升电路执行。


<details>
  <summary>Details</summary>
Motivation: 执行现实世界量子算法，利用混合平台高性能计算能力与量子指数加速优势。

Method: 提出近似量子傅里叶变换（AQFT）进行量子算法优化。

Result: 在量子傅里叶变换的指数加速基础上，提升了电路执行效果。

Conclusion: 通过优化可更好执行量子算法，利用现有计算能力和量子加速特性。

Abstract: This paper discusses the compilation, optimization, and error mitigation of
quantum algorithms, essential steps to execute real-world quantum algorithms.
Quantum algorithms running on a hybrid platform with QPU and CPU/GPU take
advantage of existing high-performance computing power with quantum-enabled
exponential speedups. The proposed approximate quantum Fourier transform (AQFT)
for quantum algorithm optimization improves the circuit execution on top of an
exponential speed-ups the quantum Fourier transform has provided.

</details>


### [446] [Superconducting Qubit Readout Using Next-Generation Reservoir Computing](https://arxiv.org/abs/2506.15771)
*Robert Kent,Benjamin Lienhard,Gregory Lafyatis,Daniel J. Gauthier*

Main category: quant-ph

TL;DR: 提出基于下一代储层计算的机器学习方法提升量子比特态区分，有高可扩展性和低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 超导量子比特读取是量子处理器瓶颈，传统方法难处理串扰，现有神经网络方法计算成本高、扩展性差。

Method: 基于下一代储层计算构建多项式特征并映射到对应量子比特态。

Result: 相比传统方法，单比特和五比特数据集误差分别降低50%和11%，五比特数据集串扰降低2.5倍；相比机器学习方法，单比特和五比特模型乘法计算分别减少100倍和2.5倍。

Conclusion: 储层计算可在保持可扩展性的同时增强量子比特态区分能力，适用于未来量子处理器。

Abstract: Quantum processors require rapid and high-fidelity simultaneous measurements
of many qubits. While superconducting qubits are among the leading modalities
toward a useful quantum processor, their readout remains a bottleneck.
Traditional approaches to processing measurement data often struggle to account
for crosstalk present in frequency-multiplexed readout, the preferred method to
reduce the resource overhead. Recent approaches to address this challenge use
neural networks to improve the state-discrimination fidelity. However, they are
computationally expensive to train and evaluate, resulting in increased latency
and poor scalability as the number of qubits increases. We present an
alternative machine learning approach based on next-generation reservoir
computing that constructs polynomial features from the measurement signals and
maps them to the corresponding qubit states. This method is highly
parallelizable, avoids the costly nonlinear activation functions common in
neural networks, and supports real-time training, enabling fast evaluation,
adaptability, and scalability. Despite its lower computational complexity, our
reservoir approach is able to maintain high qubit-state-discrimination
fidelity. Relative to traditional methods, our approach achieves error
reductions of up to 50% and 11% on single- and five-qubit datasets,
respectively, and delivers up to 2.5x crosstalk reduction on the five-qubit
dataset. Compared with recent machine-learning methods, evaluating our model
requires 100x fewer multiplications for single-qubit and 2.5x fewer for
five-qubit models. This work demonstrates that reservoir computing can enhance
qubit-state discrimination while maintaining scalability for future quantum
processors.

</details>


### [447] [Feedback-driven recurrent quantum neural network universality](https://arxiv.org/abs/2506.16332)
*Lukas Gonon,Rodrigo Martínez-Peña,Juan-Pablo Ortega*

Main category: quant-ph

TL;DR: 本文提出循环量子神经网络架构，为基于反馈的量子储层计算提供理论保证，证明模型具线性读出普遍性，推动实时量子储层计算发展。


<details>
  <summary>Details</summary>
Motivation: 早期量子储层计算方案有实时处理和计算开销问题，虽有新方案但基于反馈的量子储层计算理论基础不完善。

Method: 提出循环量子神经网络架构，将现有前馈模型扩展到动态、反馈驱动的储层设置。

Result: 为变分循环量子神经网络提供理论保证，包括近似界和普遍性结果，证明模型线性读出具普遍性。

Conclusion: 研究成果为有实时处理能力的量子储层计算奠定基础。

Abstract: Quantum reservoir computing uses the dynamics of quantum systems to process
temporal data, making it particularly well-suited for learning with noisy
intermediate-scale quantum devices. Early experimental proposals, such as the
restarting and rewinding protocols, relied on repeating previous steps of the
quantum map to avoid backaction. However, this approach compromises real-time
processing and increases computational overhead. Recent developments have
introduced alternative protocols that address these limitations. These include
online, mid-circuit measurement, and feedback techniques, which enable
real-time computation while preserving the input history. Among these, the
feedback protocol stands out for its ability to process temporal information
with comparatively fewer components. Despite this potential advantage, the
theoretical foundations of feedback-based quantum reservoir computing remain
underdeveloped, particularly with regard to the universality and the
approximation capabilities of this approach. This paper addresses this issue by
presenting a recurrent quantum neural network architecture that extends a class
of existing feedforward models to a dynamic, feedback-driven reservoir setting.
We provide theoretical guarantees for variational recurrent quantum neural
networks, including approximation bounds and universality results. Notably, our
analysis demonstrates that the model is universal with linear readouts, making
it both powerful and experimentally accessible. These results pave the way for
practical and theoretically grounded quantum reservoir computing with real-time
processing capabilities.

</details>


### [448] [Enhancing Expressivity of Quantum Neural Networks Based on the SWAP test](https://arxiv.org/abs/2506.16938)
*Sebastian Nagies,Emiliano Tolotti,Davide Pastorello,Enrico Blanzieri*

Main category: quant-ph

TL;DR: 研究基于SWAP测试电路的量子神经网络，指出其与经典两层前馈网络等价及表达局限性，提出改进电路解决问题，展示增强表达能力框架。


<details>
  <summary>Details</summary>
Motivation: 解决参数化量子电路与经典模型联系不明确，难以将经典神经网络成功经验应用到量子领域的问题。

Method: 分析基于SWAP测试电路的量子神经网络与经典两层前馈网络的数学等价性，通过在经典数据集上测试其学习能力，提出用广义SWAP测试电路修改原电路。

Result: 原架构能学习很多实际任务，但有表达局限性，改进电路能学习任意维度奇偶校验函数，原架构在二维以上无法实现。

Conclusion: 建立通过经典任务分析增强量子神经网络表达能力的框架，基于SWAP测试的架构有广泛表示能力，对量子学习任务有潜在价值。

Abstract: Parameterized quantum circuits represent promising architectures for machine
learning applications, yet many lack clear connections to classical models,
potentially limiting their ability to translate the wide success of classical
neural networks to the quantum realm. We examine a specific type of quantum
neural network (QNN) built exclusively from SWAP test circuits, and discuss its
mathematical equivalence to a classical two-layer feedforward network with
quadratic activation functions under amplitude encoding. Our analysis across
classical real-world and synthetic datasets reveals that while this
architecture can successfully learn many practical tasks, it exhibits
fundamental expressivity limitations due to violating the universal
approximation theorem, particularly failing on harder problems like the parity
check function. To address this limitation, we introduce a circuit modification
using generalized SWAP test circuits that effectively implements classical
neural networks with product layers. This enhancement enables successful
learning of parity check functions in arbitrary dimensions which we
analytically argue to be impossible for the original architecture beyond two
dimensions regardless of network size. Our results establish a framework for
enhancing QNN expressivity through classical task analysis and demonstrate that
our SWAP test-based architecture offers broad representational capacity,
suggesting potential promise also for quantum learning tasks.

</details>


<div id='physics.ins-det'></div>

# physics.ins-det [[Back]](#toc)

### [449] [Improvement of Nuclide Detection through Graph Spectroscopic Analysis Framework and its Application to Nuclear Facility Upset Detection](https://arxiv.org/abs/2506.16522)
*Pedro Rodríguez Fernández,Christian Svinth,Alex Hagen*

Main category: physics.ins-det

TL;DR: 提出用带注意力机制的神经网络结合辐射量子到达时间提高放射性核素检测限的方法，在铯检测上比传统方法提升2倍，该方法适用范围广。


<details>
  <summary>Details</summary>
Motivation: 提高光谱辐射探测器对放射性核素的检测限。

Method: 使用带注意力机制的神经网络，结合辐射量子到达时间进行检测。

Result: 在核设施异常时铯释放检测中，比传统光谱方法有2倍提升。

Conclusion: 该方法适用范围广，对衰变链更复杂的放射性核素可能更有效，还能整合其他检测事件数据。

Abstract: We present a method to improve the detection limit for radionuclides using
spectroscopic radiation detectors and the arrival time of each detected
radiation quantum. We enable this method using a neural network with an
attention mechanism. We illustrate the method on the detection of Cesium
release from a nuclear facility during an upset, and our method shows $2\times$
improvement over the traditional spectroscopic method. We hypothesize that our
method achieves this performance increase by modulating its detection
probability by the overall rate of probable detections, specifically by
adapting detection thresholds based on temporal event distributions and local
spectral features, and show evidence to this effect. We believe this method is
applicable broadly and may be more successful for radionuclides with more
complicated decay chains than Cesium; we also note that our method can
generalize beyond the addition of arrival time and could integrate other data
about each detection event, such as pulse quality, location in detector, or
even combining the energy and time from detections in different detectors.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [450] [Category-based Galaxy Image Generation via Diffusion Models](https://arxiv.org/abs/2506.16255)
*Xingzhong Fan,Hongming Tang,Yue Zeng,M. B. N. Kouwenhoven,Guangquan Zeng*

Main category: astro-ph.IM

TL;DR: 本文提出天文领域首个融合星系图像特征和天体物理特性的扩散模型框架GalCatDiff，实验显示其性能优于现有方法，能提升星系模拟可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统星系生成方法依赖物理假设和参数调整，数据驱动生成模型无需预设物理参数，扩散模型表现更优，结合物理先验知识可提升其能力。

Method: 提出GalCatDiff框架，包含增强U - Net和Astro - RAB模块，使用类别嵌入进行特定类别星系生成。

Result: GalCatDiff在样本颜色和大小分布一致性上显著优于现有方法，生成星系视觉逼真且物理一致。

Conclusion: 该框架能提升星系模拟可靠性，可作为数据增强器支持未来星系分类算法发展。

Abstract: Conventional galaxy generation methods rely on semi-analytical models and
hydrodynamic simulations, which are highly dependent on physical assumptions
and parameter tuning. In contrast, data-driven generative models do not have
explicit physical parameters pre-determined, and instead learn them efficiently
from observational data, making them alternative solutions to galaxy
generation. Among these, diffusion models outperform Variational Autoencoders
(VAEs) and Generative Adversarial Networks (GANs) in quality and diversity.
Leveraging physical prior knowledge to these models can further enhance their
capabilities. In this work, we present GalCatDiff, the first framework in
astronomy to leverage both galaxy image features and astrophysical properties
in the network design of diffusion models. GalCatDiff incorporates an enhanced
U-Net and a novel block entitled Astro-RAB (Residual Attention Block), which
dynamically combines attention mechanisms with convolution operations to ensure
global consistency and local feature fidelity. Moreover, GalCatDiff uses
category embeddings for class-specific galaxy generation, avoiding the high
computational costs of training separate models for each category. Our
experimental results demonstrate that GalCatDiff significantly outperforms
existing methods in terms of the consistency of sample color and size
distributions, and the generated galaxies are both visually realistic and
physically consistent. This framework will enhance the reliability of galaxy
simulations and can potentially serve as a data augmentor to support future
galaxy classification algorithm development.

</details>


<div id='cond-mat.str-el'></div>

# cond-mat.str-el [[Back]](#toc)

### [451] [Simulating Correlated Electrons with Symmetry-Enforced Normalizing Flows](https://arxiv.org/abs/2506.17015)
*Dominic Schuh,Janik Kreit,Evan Berkowitz,Lena Funcke,Thomas Luu,Kim A. Nicoli,Marcel Rodekamp*

Main category: cond-mat.str-el

TL;DR: 证明归一化流可准确学习费米子哈伯德模型的玻尔兹曼分布，解决传统方法问题并实现加速


<details>
  <summary>Details</summary>
Motivation: 现有方法如混合蒙特卡罗在时间连续极限附近有遍历性问题，导致估计有偏差

Method: 利用对称感知架构以及独立同分布采样

Result: 解决了传统方法的问题，且相比传统方法实现显著加速

Conclusion: 归一化流能准确学习费米子哈伯德模型的玻尔兹曼分布

Abstract: We present the first proof of principle that normalizing flows can accurately
learn the Boltzmann distribution of the fermionic Hubbard model - a key
framework for describing the electronic structure of graphene and related
materials. State-of-the-art methods like Hybrid Monte Carlo often suffer from
ergodicity issues near the time-continuum limit, leading to biased estimates.
Leveraging symmetry-aware architectures as well as independent and identically
distributed sampling, our approach resolves these issues and achieves
significant speed-ups over traditional methods.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [452] [Quantum Artificial Intelligence for Secure Autonomous Vehicle Navigation: An Architectural Proposal](https://arxiv.org/abs/2506.16000)
*Hemanth Kannamarlapudi,Sowmya Chintalapudi*

Main category: cs.ET

TL;DR: 提出基于量子人工智能的自动驾驶导航新架构，含量子神经网络、Nav - Q和后量子加密协议，解决导航挑战。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶导航依赖大量数据处理和决策，现有方法需改进，要应对导航和通信安全挑战。

Method: 构建新架构，包括用量子神经网络融合多传感器数据，用Nav - Q模块通过变分量子电路学习最优导航策略，用后量子加密协议保障通信安全。

Result: 实现了多传感器数据的统一量子态表示，可学习复杂条件下的最优导航策略，保障通信安全。

Conclusion: 所提框架能提供量子性能和未来安全保障，解决自动驾驶导航的基本挑战。

Abstract: Navigation is a very crucial aspect of autonomous vehicle ecosystem which
heavily relies on collecting and processing large amounts of data in various
states and taking a confident and safe decision to define the next vehicle
maneuver. In this paper, we propose a novel architecture based on Quantum
Artificial Intelligence by enabling quantum and AI at various levels of
navigation decision making and communication process in Autonomous vehicles :
Quantum Neural Networks for multimodal sensor fusion, Nav-Q for Quantum
reinforcement learning for navigation policy optimization and finally
post-quantum cryptographic protocols for secure communication. Quantum neural
networks uses quantum amplitude encoding to fuse data from various sensors like
LiDAR, radar, camera, GPS and weather etc., This approach gives a unified
quantum state representation between heterogeneous sensor modalities. Nav-Q
module processes the fused quantum states through variational quantum circuits
to learn optimal navigation policies under swift dynamic and complex
conditions. Finally, post quantum cryptographic protocols are used to secure
communication channels for both within vehicle communication and V2X (Vehicle
to Everything) communications and thus secures the autonomous vehicle
communication from both classical and quantum security threats. Thus, the
proposed framework addresses fundamental challenges in autonomous vehicles
navigation by providing quantum performance and future proof security. Index
Terms Quantum Computing, Autonomous Vehicles, Sensor Fusion

</details>


### [453] [Artificial Intelligence for Atmospheric Sciences: A Research Roadmap](https://arxiv.org/abs/2506.16281)
*Martha Arbayani Zaidan,Naser Hossein Motlagh,Petteri Nurmi,Tareq Hussein,Markku Kulmala,Tuukka Petäjä,Sasu Tarkoma*

Main category: cs.ET

TL;DR: 本文对大气科学与计算机科学进行跨学科综述，强调AI在大气研究中的潜力，指出集成AI的挑战并给出研究路线图。


<details>
  <summary>Details</summary>
Motivation: 近期传感、通信、计算和AI的突破推动了大气科学发展，需跨学科综述来展现AI在大气研究中的潜力。

Method: 进行跨学科综述，识别集成AI到大气研究中的关键挑战。

Result: 识别出与大数据和基础设施相关的集成AI的关键挑战。

Conclusion: 给出解决当前和新兴挑战的详细研究路线图。

Abstract: Atmospheric sciences are crucial for understanding environmental phenomena
ranging from air quality to extreme weather events, and climate change. Recent
breakthroughs in sensing, communication, computing, and Artificial Intelligence
(AI) have significantly advanced atmospheric sciences, enabling the generation
of vast amounts of data through long-term Earth observations and providing
powerful tools for analyzing atmospheric phenomena and predicting natural
disasters. This paper contributes a critical interdisciplinary overview that
bridges the fields of atmospheric science and computer science, highlighting
the transformative potential of AI in atmospheric research. We identify key
challenges associated with integrating AI into atmospheric research, including
issues related to big data and infrastructure, and provide a detailed research
roadmap that addresses both current and emerging challenges.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [454] [Fast solvers for Tokamak fluid models with PETSC -- Part I](https://arxiv.org/abs/2506.16676)
*Mark F. Adams,Jin Chen,Benjamin Sturdevant*

Main category: physics.plasm-ph

TL;DR: 本文为托卡马克磁流体动力学（MHD）模型添加多重网格求解器迈出第一步，聚焦M3D - C1速度求解，用PETSC库，一维多重网格比现有单级方法快约5倍。


<details>
  <summary>Details</summary>
Motivation: 现有直接求解器扩展性不佳，不适合需要高度并行的现代硬件，为托卡马克MHD模型添加环形半粗化多重网格求解器以减少对直接求解器的依赖。

Method: 聚焦M3D - C1中的速度求解，使用PETSC数值库。

Result: 在含逃逸电子的MHD破裂测试问题上，一维多重网格比现有单级方法快约5倍。

Conclusion: 在托卡马克MHD模型中添加多重网格求解器是可行且有效的，能显著提高求解速度。

Abstract: This report develops the first step in adding multigrid solvers to scientific
and engineering-relevant magnetohydrodynamics (MHD) models of Tokamaks. These
models are characterized by a distinguished direction in the toroidal
coordinate that is partially aligned with the magnetic guide field, which
dominates the plasma dynamics. All Tokamak models exploit this structure, for
example, NIMROD (https://nimrodteam.org/) uses $2D$, unstructured, high-order
finite elements in the poloidal plane with Fourier modes in the toroidal
coordinate, and the $3D$, extended MHD code M3D-C1
(https://w3.pppl.gov/~nferraro/m3dc1.html) uses $2D$, unstructured $C^1$
elements in the poloidal plane with cubic Hermite functions in the toroidal
direction. This structure suggests adding toroidal semi-coarsening multigrid to
the existing solver and thereby reducing reliance on direct solvers, which do
not scale optimally and are not well suited to modern hardware that demands
extreme levels of parallelism. This report focuses on the velocity solve in
M3D-C1, using the PETSC -- the Portable, Extensible Toolkit for Scientific
Computation -- numerical library (https://petsc.org), and shows that with
little new application code, one-dimensional multigrid is about $5x$ faster
than the existing one-level method on an MHD disruption, with runaway
electrons, test problem.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [455] [ETrace:Event-Driven Vulnerability Detection in Smart Contracts via LLM-Based Trace Analysis](https://arxiv.org/abs/2506.15790)
*Chenyang Peng,Haijun Wang,Yin Wu,Hao Wu,Ming Fan,Yitao Zhao,Ting Liu*

Main category: cs.CR

TL;DR: 提出用于智能合约的事件驱动漏洞检测框架ETrace，无需访问源代码，通过LLM追踪分析识别漏洞，并验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 区块链技术应用中，确保智能合约安全稳定是关键挑战，现有传统漏洞检测方法多依赖分析原合约代码，并非所有合约代码都可访问。

Method: 提出ETrace框架，从交易日志中提取细粒度事件序列，利用大语言模型作为自适应语义解释器，通过思维链推理重构事件分析，实现模式匹配建立交易行为模式与已知攻击行为的因果联系。

Result: 通过初步实验结果验证了ETrace的有效性。

Conclusion: ETrace作为一种新颖的事件驱动漏洞检测框架，能在不访问源代码的情况下有效检测智能合约的潜在漏洞。

Abstract: With the advance application of blockchain technology in various fields,
ensuring the security and stability of smart contracts has emerged as a
critical challenge. Current security analysis methodologies in vulnerability
detection can be categorized into static analysis and dynamic analysis
methods.However, these existing traditional vulnerability detection methods
predominantly rely on analyzing original contract code, not all smart contracts
provide accessible code.We present ETrace, a novel event-driven vulnerability
detection framework for smart contracts, which uniquely identifies potential
vulnerabilities through LLM-powered trace analysis without requiring source
code access. By extracting fine-grained event sequences from transaction logs,
the framework leverages Large Language Models (LLMs) as adaptive semantic
interpreters to reconstruct event analysis through chain-of-thought reasoning.
ETrace implements pattern-matching to establish causal links between
transaction behavior patterns and known attack behaviors. Furthermore, we
validate the effectiveness of ETrace through preliminary experimental results.

</details>


### [456] [PRISON: Unmasking the Criminal Potential of Large Language Models](https://arxiv.org/abs/2506.16150)
*Xinyi Wu,Geng Hong,Pei Chen,Yueyue Chen,Xudong Pan,Min Yang*

Main category: cs.CR

TL;DR: 提出PRISON框架评估大语言模型犯罪潜力，发现其有犯罪倾向且犯罪与侦查能力不匹配，强调部署前需安全机制。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对大语言模型在现实交互中犯罪能力的系统理解和评估，随着其发展，对其在复杂社会环境中不当行为的担忧加剧。

Method: 提出统一框架PRISON，从五个维度量化大语言模型犯罪潜力，利用经典电影改编的犯罪场景通过角色扮演评估其犯罪潜力和反犯罪能力。

Result: 先进大语言模型常表现出犯罪倾向，担任侦探角色时识别欺骗行为平均准确率仅41%，犯罪与侦查行为能力不匹配。

Conclusion: 在大语言模型更广泛部署前，迫切需要提升对抗鲁棒性、行为对齐性和建立安全机制。

Abstract: As large language models (LLMs) advance, concerns about their misconduct in
complex social contexts intensify. Existing research overlooked the systematic
understanding and assessment of their criminal capability in realistic
interactions. We propose a unified framework PRISON, to quantify LLMs' criminal
potential across five dimensions: False Statements, Frame-Up, Psychological
Manipulation, Emotional Disguise, and Moral Disengagement. Using structured
crime scenarios adapted from classic films, we evaluate both criminal potential
and anti-crime ability of LLMs via role-play. Results show that
state-of-the-art LLMs frequently exhibit emergent criminal tendencies, such as
proposing misleading statements or evasion tactics, even without explicit
instructions. Moreover, when placed in a detective role, models recognize
deceptive behavior with only 41% accuracy on average, revealing a striking
mismatch between conducting and detecting criminal behavior. These findings
underscore the urgent need for adversarial robustness, behavioral alignment,
and safety mechanisms before broader LLM deployment.

</details>


### [457] [Towards Effective Complementary Security Analysis using Large Language Models](https://arxiv.org/abs/2506.16899)
*Jonas Wagner,Simon Müller,Christian Näther,Jan-Philipp Steghöfer,Andreas Both*

Main category: cs.CR

TL;DR: 提出用大语言模型评估SAST结果以减少误报，先进提示技术可提升误报检测，不同模型结合效果更佳，证明方法有泛化性。


<details>
  <summary>Details</summary>
Motivation: 静态应用安全测试工具报告存在大量误报，降低安全分析有效性，需改进评估方法。

Method: 使用OWASP Benchmark (v1.2)和真实项目数据集，研究大语言模型减少误报能力，运用先进提示技术。

Result: 先进提示技术大幅提升误报检测，OWASP数据集单模型检测约62.5%误报，多模型结合达约78.9%；真实数据集单模型检测33.85%误报，多模型结合达38.46%。

Conclusion: 大语言模型可补充传统SAST工具，提高自动化程度，减少处理误报资源消耗。

Abstract: A key challenge in security analysis is the manual evaluation of potential
security weaknesses generated by static application security testing (SAST)
tools. Numerous false positives (FPs) in these reports reduce the effectiveness
of security analysis. We propose using Large Language Models (LLMs) to improve
the assessment of SAST findings. We investigate the ability of LLMs to reduce
FPs while trying to maintain a perfect true positive rate, using datasets
extracted from the OWASP Benchmark (v1.2) and a real-world software project.
Our results indicate that advanced prompting techniques, such as
Chain-of-Thought and Self-Consistency, substantially improve FP detection.
Notably, some LLMs identified approximately 62.5% of FPs in the OWASP Benchmark
dataset without missing genuine weaknesses. Combining detections from different
LLMs would increase this FP detection to approximately 78.9%. Additionally, we
demonstrate our approach's generalizability using a real-world dataset covering
five SAST tools, three programming languages, and infrastructure files. The
best LLM detected 33.85% of all FPs without missing genuine weaknesses, while
combining detections from different LLMs would increase this detection to
38.46%. Our findings highlight the potential of LLMs to complement traditional
SAST tools, enhancing automation and reducing resources spent addressing false
alarms.

</details>


### [458] [Malware Classification Leveraging NLP & Machine Learning for Enhanced Accuracy](https://arxiv.org/abs/2506.16224)
*Bishwajit Prasad Gond,Rajneekant,Pushkar Kishore,Durga Prasad Mohapatra*

Main category: cs.CR

TL;DR: 本文探讨用NLP的n - gram分析和机器学习技术提升恶意软件分类效果，实验表明准确率显著提高。


<details>
  <summary>Details</summary>
Motivation: 提升恶意软件分类效果。

Method: 运用NLP的n - gram分析从恶意软件样本提取文本特征，探索n - gram大小选择、特征表示和分类算法，使用混合特征选择技术处理高维度问题。

Result: 与传统方法相比，准确率显著提高，使用混合特征选择技术将特征集缩减到原特征的1.6%，在各种机器学习算法中准确率达99.02%。

Conclusion: 基于NLP的n - gram分析和机器学习技术结合混合特征选择技术能有效提升恶意软件分类准确率。

Abstract: This paper investigates the application of natural language processing
(NLP)-based n-gram analysis and machine learning techniques to enhance malware
classification. We explore how NLP can be used to extract and analyze textual
features from malware samples through n-grams, contiguous string or API call
sequences. This approach effectively captures distinctive linguistic patterns
among malware and benign families, enabling finer-grained classification. We
delve into n-gram size selection, feature representation, and classification
algorithms. While evaluating our proposed method on real-world malware samples,
we observe significantly improved accuracy compared to the traditional methods.
By implementing our n-gram approach, we achieved an accuracy of 99.02% across
various machine learning algorithms by using hybrid feature selection technique
to address high dimensionality. Hybrid feature selection technique reduces the
feature set to only 1.6% of the original features.

</details>


### [459] [The Hitchhiker's Guide to Efficient, End-to-End, and Tight DP Auditing](https://arxiv.org/abs/2506.16666)
*Meenatchi Sundaram Muthu Selva Annamalai,Borja Balle,Jamie Hayes,Georgios Kaissis,Emiliano De Cristofaro*

Main category: cs.CR

TL;DR: 本文对差分隐私审计技术研究进行系统化梳理，提出评估框架并指明研究方向。


<details>
  <summary>Details</summary>
Motivation: 识别差分隐私审计技术的当前现状和开放挑战。

Method: 引入综合框架，确立三个跨上下文的期望目标，系统化先进审计技术的操作模式。

Result: 突出了先前工作忽视的细节，分析了实现三个期望目标的限制因素，确定了开放研究问题。

Conclusion: 提供了可复用的系统方法论来评估该领域进展、识别痛点和未来研究方向。

Abstract: This paper systematizes research on auditing Differential Privacy (DP)
techniques, aiming to identify key insights into the current state of the art
and open challenges. First, we introduce a comprehensive framework for
reviewing work in the field and establish three cross-contextual desiderata
that DP audits should target--namely, efficiency, end-to-end-ness, and
tightness. Then, we systematize the modes of operation of state-of-the-art DP
auditing techniques, including threat models, attacks, and evaluation
functions. This allows us to highlight key details overlooked by prior work,
analyze the limiting factors to achieving the three desiderata, and identify
open research problems. Overall, our work provides a reusable and systematic
methodology geared to assess progress in the field and identify friction points
and future directions for our community to focus on.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [460] [Summary Statistics of Large-scale Model Outputs for Observation-corrected Outputs](https://arxiv.org/abs/2506.15845)
*Atlanta Chakraborty,Julie Bessac*

Main category: stat.ME

TL;DR: 本文提出Sig - PCA时空框架，结合模型输出统计摘要与观测数据，用神经网络校正模型输出，在两个数据集上验证方法有效。


<details>
  <summary>Details</summary>
Motivation: 基于物理的模型有偏差和近似问题，观测数据稀疏，且模型输出量大难处理，需整合数据提高模型输出准确性和可靠性。

Method: 提出Sig - PCA时空框架，通过神经网络整合模型输出的统计摘要与局部观测数据，利用降阶表示并结合观测数据校正模型输出。

Result: 在两个不同统计特性和数据比例的数据集（地表温度和地表风）上，方法使模型输出与观测数据紧密对齐，能校正概率分布和时空相关结构。

Conclusion: 该框架突出了观测数据与模型输出统计摘要的协同作用，有效整合多源数据并保留关键统计信息。

Abstract: Physics-based models capture broad spatial and temporal dynamics, but often
suffer from biases and numerical approximations, while observations capture
localized variability but are sparse. Integrating these complementary data
modalities is important to improving the accuracy and reliability of model
outputs. Meanwhile, physics-based models typically generate large outputs that
are challenging to manipulate. In this paper, we propose Sig-PCA, a space-time
framework that integrates summary statistics from model outputs with localized
observations via a neural network (NN). By leveraging reduced-order
representations from physics-based models and integrating them with
observational data, our approach corrects model outputs, while allowing to work
with dimensionally-reduced quantities hence with smaller NNs. This framework
highlights the synergy between observational data and statistical summaries of
model outputs, and effectively combines multisource data by preserving
essential statistical information. We demonstrate our approach on two datasets
(surface temperature and surface wind) with different statistical properties
and different ratios of model to observational data. Our method corrects model
outputs to align closely with the observational data, specifically enabling to
correct probability distributions and space-time correlation structures.

</details>


### [461] [Leveraging Optimal Transport for Distributed Two-Sample Testing: An Integrated Transportation Distance-based Framework](https://arxiv.org/abs/2506.16047)
*Zhengqi Lin,Yan Chen*

Main category: stat.ME

TL;DR: 本文提出基于ITD的分布式双样本检验框架，解决去中心化学习中检测分布变化的问题，理论分析与模拟显示其性能良好。


<details>
  <summary>Details</summary>
Motivation: 解决去中心化学习或联邦学习环境中，在数据隐私和异构性问题下检测分布变化的挑战。

Method: 提出基于ITD的框架，给出理论基础，采用置换检验程序实现分布式计算。

Result: 通过理论功效分析和大量模拟，显示该框架能有效控制一类错误，有高功效，可检测细微分布变化。

Conclusion: 该工作为分布式统计推断领域做出贡献，为现代去中心化数据环境中的双样本检验提供有力工具。

Abstract: This paper introduces a novel framework for distributed two-sample testing
using the Integrated Transportation Distance (ITD), an extension of the Optimal
Transport distance. The approach addresses the challenges of detecting
distributional changes in decentralized learning or federated learning
environments, where data privacy and heterogeneity are significant concerns. We
provide theoretical foundations for the ITD, including convergence properties
and asymptotic behavior. A permutation test procedure is proposed for practical
implementation in distributed settings, allowing for efficient computation
while preserving data privacy. The framework's performance is demonstrated
through theoretical power analysis and extensive simulations, showing robust
Type I error control and high power across various distributions and
dimensions. The results indicate that ITD effectively aggregates information
across distributed clients, detecting subtle distributional shifts that might
be missed when examining individual clients. This work contributes to the
growing field of distributed statistical inference, offering a powerful tool
for two-sample testing in modern, decentralized data environments.

</details>


### [462] [elicito: A Python Package for Expert Prior Elicitation](https://arxiv.org/abs/2506.16830)
*Florence Bockting,Paul-Christian Bürkner*

Main category: stat.ME

TL;DR: 提出Python包elicito用于专家先验提取，介绍方法基础、架构并通过案例展示功能


<details>
  <summary>Details</summary>
Motivation: 专家知识多针对可观测变量，难转化为可用先验，需工具解决该问题

Method: 实现基于模拟的模块化框架，支持结构和预测提取方法，关键组件可灵活定制

Result: elicito使提取过程模块化，有透明度、可重复性和可比性

Conclusion: elicito能有效解决专家先验提取问题，通过案例验证其功能

Abstract: Expert prior elicitation plays a critical role in Bayesian analysis by
enabling the specification of prior distributions that reflect domain
knowledge. However, expert knowledge often refers to observable quantities
rather than directly to model parameters, posing a challenge for translating
this information into usable priors. We present elicito, a Python package that
implements a modular, simulation-based framework for expert prior elicitation.
The framework supports both structural and predictive elicitation methods and
allows for flexible customization of key components, including the generative
model, the form of expert input, prior assumptions (parametric or
nonparametric), and loss functions. By structuring the elicitation process into
configurable modules, elicito offers transparency, reproducibility, and
comparability across elicitation methods. We describe the methodological
foundations of the package, its software architecture, and demonstrate its
functionality through a case study.

</details>


### [463] [TRUST: Transparent, Robust and Ultra-Sparse Trees](https://arxiv.org/abs/2506.15791)
*Albert Dorador*

Main category: stat.ME

TL;DR: 提出新回归树模型TRUST，结合随机森林准确性与浅决策树和稀疏线性模型可解释性，用大模型增强透明度，在多数据集上表现好


<details>
  <summary>Details</summary>
Motivation: 分段常数回归树可解释但预测准确性落后于随机森林等黑盒模型，需要新模型兼顾准确性和可解释性

Method: 引入TRUST模型，并利用大语言模型生成用户友好解释

Result: 在合成和真实世界基准数据集上，TRUST预测准确性始终优于其他可解释模型，与随机森林相当，比M5'在准确性和可解释性上有显著提升

Conclusion: TRUST是一种有效结合准确性和可解释性的回归树模型

Abstract: Piecewise-constant regression trees remain popular for their
interpretability, yet often lag behind black-box models like Random Forest in
predictive accuracy. In this work, we introduce TRUST (Transparent, Robust, and
Ultra-Sparse Trees), a novel regression tree model that combines the accuracy
of Random Forests with the interpretability of shallow decision trees and
sparse linear models. TRUST further enhances transparency by leveraging Large
Language Models to generate tailored, user-friendly explanations. Extensive
validation on synthetic and real-world benchmark datasets demonstrates that
TRUST consistently outperforms other interpretable models -- including CART,
Lasso, and Node Harvest -- in predictive accuracy, while matching the accuracy
of Random Forest and offering substantial gains in both accuracy and
interpretability over M5', a well-established model that is conceptually
related.

</details>


### [464] [An introduction to Causal Modelling](https://arxiv.org/abs/2506.16486)
*Gauranga Kumar Baishya*

Main category: stat.ME

TL;DR: 本教程结合潜在结果和图形方法，对现代因果建模进行简要介绍，涵盖因果问题、效应度量、识别方法、估计和推理以及因果图等内容，并强调为应用研究者提供清晰说明和实例。


<details>
  <summary>Details</summary>
Motivation: 介绍现代因果建模方法，解决如干预下的反事实推理等因果问题。

Method: 结合潜在结果和图形方法，讨论因果效应度量，推导随机实验中的识别方法，介绍逆概率加权、稳健推断及因果图相关知识。

Result: 完成了对现代因果建模的全面介绍，展示了图形和潜在结果方法的互补性。

Conclusion: 通过清晰的符号、直观的解释和实际例子，为应用研究者提供了现代因果建模的有效入门指导。

Abstract: This tutorial provides a concise introduction to modern causal modeling by
integrating potential outcomes and graphical methods. We motivate causal
questions such as counterfactual reasoning under interventions and define
binary treatments and potential outcomes. We discuss causal effect
measures-including average treatment effects on the treated and on the
untreated-and choices of effect scales for binary outcomes. We derive
identification in randomized experiments under exchangeability and consistency,
and extend to stratification and blocking designs. We present inverse
probability weighting with propensity score estimation and robust inference via
sandwich estimators. Finally, we introduce causal graphs, d-separation, the
backdoor criterion, single-world intervention graphs, and structural equation
models, showing how graphical and potential-outcome approaches complement each
other. Emphasis is placed on clear notation, intuitive explanations, and
practical examples for applied researchers.

</details>


### [465] [Bayesian Joint Model of Multi-Sensor and Failure Event Data for Multi-Mode Failure Prediction](https://arxiv.org/abs/2506.17036)
*Sina Aghaee Dabaghan Fard,Minhee Kim,Akash Deep,Jaesung Lee*

Main category: stat.ME

TL;DR: 本文提出统一方法联合建模多传感器时间序列数据与多故障模式故障时间，通过变分贝叶斯和蒙特卡罗采样实现预测，经案例验证优势。


<details>
  <summary>Details</summary>
Motivation: 现有模型独立进行故障模式和剩余使用寿命（RUL）预测，忽略两者关系，部分黑箱机器学习方法缺乏统计严谨性，无法刻画不确定性。

Method: 在分层贝叶斯框架中集成Cox比例风险模型、卷积多输出高斯过程和多项故障模式分布，通过变分贝叶斯获得后验分布，用蒙特卡罗采样进行预测。

Result: 通过喷气发动机数据集的大量数值和案例研究验证了模型优势。

Conclusion: 所提出的统一方法能有效联合建模多传感器时间序列数据与多故障模式故障时间，实现准确预测和稳健的不确定性量化。

Abstract: Modern industrial systems are often subject to multiple failure modes, and
their conditions are monitored by multiple sensors, generating multiple
time-series signals. Additionally, time-to-failure data are commonly available.
Accurately predicting a system's remaining useful life (RUL) requires
effectively leveraging multi-sensor time-series data alongside multi-mode
failure event data. In most existing models, failure modes and RUL prediction
are performed independently, ignoring the inherent relationship between these
two tasks. Some models integrate multiple failure modes and event prediction
using black-box machine learning approaches, which lack statistical rigor and
cannot characterize the inherent uncertainty in the model and data. This paper
introduces a unified approach to jointly model the multi-sensor time-series
data and failure time concerning multiple failure modes. This proposed model
integrate a Cox proportional hazards model, a Convolved Multi-output Gaussian
Process, and multinomial failure mode distributions in a hierarchical Bayesian
framework with corresponding priors, enabling accurate prediction with robust
uncertainty quantification. Posterior distributions are effectively obtained by
Variational Bayes, and prediction is performed with Monte Carlo sampling. The
advantages of the proposed model is validated through extensive numerical and
case studies with jet-engine dataset.

</details>
