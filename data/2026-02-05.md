<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 26]
- [cs.CE](#cs.CE) [Total: 2]
- [cs.DB](#cs.DB) [Total: 7]
- [cs.DC](#cs.DC) [Total: 3]
- [cs.DS](#cs.DS) [Total: 7]
- [cs.GT](#cs.GT) [Total: 5]
- [cs.IR](#cs.IR) [Total: 10]
- [cs.LG](#cs.LG) [Total: 128]
- [cs.NE](#cs.NE) [Total: 4]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.SE](#cs.SE) [Total: 18]
- [q-fin.RM](#q-fin.RM) [Total: 1]
- [stat.ML](#stat.ML) [Total: 21]
- [stat.CO](#stat.CO) [Total: 3]
- [cs.MA](#cs.MA) [Total: 1]
- [math.PR](#math.PR) [Total: 1]
- [cs.CV](#cs.CV) [Total: 33]
- [cs.CC](#cs.CC) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 2]
- [physics.chem-ph](#physics.chem-ph) [Total: 3]
- [math.ST](#math.ST) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 3]
- [physics.optics](#physics.optics) [Total: 1]
- [physics.soc-ph](#physics.soc-ph) [Total: 1]
- [cs.ET](#cs.ET) [Total: 1]
- [eess.AS](#eess.AS) [Total: 3]
- [econ.TH](#econ.TH) [Total: 1]
- [cs.RO](#cs.RO) [Total: 7]
- [cs.CY](#cs.CY) [Total: 3]
- [cs.HC](#cs.HC) [Total: 6]
- [stat.ME](#stat.ME) [Total: 3]
- [cs.OS](#cs.OS) [Total: 1]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 1]
- [quant-ph](#quant-ph) [Total: 1]
- [eess.SP](#eess.SP) [Total: 9]
- [econ.GN](#econ.GN) [Total: 3]
- [cs.NI](#cs.NI) [Total: 2]
- [cond-mat.mes-hall](#cond-mat.mes-hall) [Total: 1]
- [cs.CL](#cs.CL) [Total: 30]
- [cs.GR](#cs.GR) [Total: 1]
- [cond-mat.dis-nn](#cond-mat.dis-nn) [Total: 1]
- [cs.CR](#cs.CR) [Total: 3]
- [eess.SY](#eess.SY) [Total: 2]
- [cs.SD](#cs.SD) [Total: 2]
- [cs.SI](#cs.SI) [Total: 4]
- [cs.IT](#cs.IT) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 2]
- [stat.AP](#stat.AP) [Total: 1]
- [cs.DL](#cs.DL) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Knowledge Model Prompting Increases LLM Performance on Planning Tasks](https://arxiv.org/abs/2602.03900)
*Erik Goh,John Kos,Ashok Goel*

Main category: cs.AI

TL;DR: 本文研究TMK框架能否提升大语言模型推理能力，实验表明TMK提示能显著提高推理模型在不透明符号任务上的准确率，还能引导模型采用形式化代码执行路径。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在推理和规划任务上存在困难，现有提示技术受质疑，研究TMK框架能否提升其推理能力。

Method: 在PlanBench基准上进行实验，聚焦Blocksworld领域，测试TMK结构化提示能否帮助语言模型分解复杂规划问题。

Result: TMK提示使推理模型在不透明符号任务上的准确率从31.5%提升到97.3%，还出现了推理模型的性能反转。

Conclusion: TMK不仅是上下文，还能引导推理模型脱离默认语言模式，采用形式化代码执行路径。

Abstract: Large Language Models (LLM) can struggle with reasoning ability and planning tasks. Many prompting techniques have been developed to assist with LLM reasoning, notably Chain-of-Thought (CoT); however, these techniques, too, have come under scrutiny as LLMs' ability to reason at all has come into question. Borrowing from the domain of cognitive and educational science, this paper investigates whether the Task-Method-Knowledge (TMK) framework can improve LLM reasoning capabilities beyond its previously demonstrated success in educational applications. The TMK framework's unique ability to capture causal, teleological, and hierarchical reasoning structures, combined with its explicit task decomposition mechanisms, makes it particularly well-suited for addressing language model reasoning deficiencies, and unlike other hierarchical frameworks such as HTN and BDI, TMK provides explicit representations of not just what to do and how to do it, but also why actions are taken. The study evaluates TMK by experimenting on the PlanBench benchmark, focusing on the Blocksworld domain to test for reasoning and planning capabilities, examining whether TMK-structured prompting can help language models better decompose complex planning problems into manageable sub-tasks. Results also highlight significant performance inversion in reasoning models. TMK prompting enables the reasoning model to achieve up to an accuracy of 97.3\% on opaque, symbolic tasks (Random versions of Blocksworld in PlanBench) where it previously failed (31.5\%), suggesting the potential to bridge the gap between semantic approximation and symbolic manipulation. Our findings suggest that TMK functions not merely as context, but also as a mechanism that steers reasoning models away from their default linguistic modes to engage formal, code-execution pathways in the context of the experiments.

</details>


### [2] [Enhancing Mathematical Problem Solving in LLMs through Execution-Driven Reasoning Augmentation](https://arxiv.org/abs/2602.03950)
*Aditya Basarkar,Benyamin Tabarsi,Tiffany Barnes,Dongkuan,Xu*

Main category: cs.AI

TL;DR: 本文提出迭代改进程序构建（IIPC）方法解决现有多智能体大语言模型系统数学推理问题，该方法在多个基准测试中表现更优且代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体大语言模型系统在数学推理中缺乏可靠可修正的推理过程表示，且程序上下文会干扰语言模型。

Method: 引入迭代改进程序构建（IIPC）方法，迭代细化程序推理链，结合执行反馈和基础大语言模型的思维链能力。

Result: IIPC在多个基础大语言模型的多数推理基准测试中超越了竞争方法。

Conclusion: IIPC方法有效提升了大语言模型的数学推理能力。

Abstract: Mathematical problem solving is a fundamental benchmark for assessing the reasoning capabilities of artificial intelligence and a gateway to applications in education, science, and engineering where reliable symbolic reasoning is essential. Although recent advances in multi-agent LLM-based systems have enhanced their mathematical reasoning capabilities, they still lack a reliably revisable representation of the reasoning process. Existing agents either operate in rigid sequential pipelines that cannot correct earlier steps or rely on heuristic self-evaluation that can fail to identify and fix errors. In addition, programmatic context can distract language models and degrade accuracy. To address these gaps, we introduce Iteratively Improved Program Construction (IIPC), a reasoning method that iteratively refines programmatic reasoning chains and combines execution feedback with the native Chain-of-thought abilities of the base LLM to maintain high-level contextual focus. IIPC surpasses competing approaches in the majority of reasoning benchmarks on multiple base LLMs. All code and implementations are released as open source.

</details>


### [3] [AgentArk: Distilling Multi-Agent Intelligence into a Single LLM Agent](https://arxiv.org/abs/2602.03955)
*Yinyi Luo,Yiqiao Jin,Weichen Yu,Mengqi Zhang,Srijan Kumar,Xiaoxiao Li,Weijie Xu,Xin Chen,Jindong Wang*

Main category: cs.AI

TL;DR: 本文提出AgentArk框架，将多智能体动态提炼到单模型权重中，研究三种分层蒸馏策略，使蒸馏模型兼具效率与推理能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型多智能体系统因高计算成本和错误传播，实际部署受限。

Method: 提出AgentArk框架，研究推理增强微调、基于轨迹的增强和过程感知蒸馏三种分层蒸馏策略。

Result: 蒸馏模型保留单智能体效率，展现多智能体的推理和自我修正能力，在不同推理任务中表现出更强的鲁棒性和泛化性。

Conclusion: 希望为高效稳健的多智能体开发的未来研究提供思路。

Abstract: While large language model (LLM) multi-agent systems achieve superior reasoning performance through iterative debate, practical deployment is limited by their high computational cost and error propagation. This paper proposes AgentArk, a novel framework to distill multi-agent dynamics into the weights of a single model, effectively transforming explicit test-time interactions into implicit model capabilities. This equips a single agent with the intelligence of multi-agent systems while remaining computationally efficient. Specifically, we investigate three hierarchical distillation strategies across various models, tasks, scaling, and scenarios: reasoning-enhanced fine-tuning; trajectory-based augmentation; and process-aware distillation. By shifting the burden of computation from inference to training, the distilled models preserve the efficiency of one agent while exhibiting strong reasoning and self-correction performance of multiple agents. They further demonstrate enhanced robustness and generalization across diverse reasoning tasks. We hope this work can shed light on future research on efficient and robust multi-agent development. Our code is at https://github.com/AIFrontierLab/AgentArk.

</details>


### [4] [Active Epistemic Control for Query-Efficient Verified Planning](https://arxiv.org/abs/2602.03974)
*Shuhui Qu*

Main category: cs.AI

TL;DR: 提出主动认知控制（AEC）规划层，融合基于模型的信念管理与类别可行性检查，实验表明其比强大的LLM - agent基线有更少重规划轮次且成功率有竞争力。


<details>
  <summary>Details</summary>
Motivation: 在部分可观测的交互环境中规划具有挑战性，学习的世界模型有预测错误问题。

Method: 提出AEC规划层，严格分离用于承诺的‘事实存储’和用于修剪候选计划的‘信念存储’，根据不确定性高低选择查询环境或模拟谓词，最终承诺通过覆盖检查和兼容性检查。

Result: 在ALFWorld和ScienceWorld上的实验显示，AEC比强LLM - agent基线有更少重规划轮次且取得有竞争力的成功率。

Conclusion: AEC在部分可观测的交互环境规划中是一种有效的方法。

Abstract: Planning in interactive environments is challenging under partial observability: task-critical preconditions (e.g., object locations or container states) may be unknown at decision time, yet grounding them through interaction is costly. Learned world models can cheaply predict missing facts, but prediction errors can silently induce infeasible commitments. We present \textbf{Active Epistemic Control (AEC)}, an epistemic-categorical planning layer that integrates model-based belief management with categorical feasibility checks. AEC maintains a strict separation between a \emph{grounded fact store} used for commitment and a \emph{belief store} used only for pruning candidate plans. At each step, it either queries the environment to ground an unresolved predicate when uncertainty is high or predictions are ambiguous, or simulates the predicate to filter hypotheses when confidence is sufficient. Final commitment is gated by grounded precondition coverage and an SQ-BCP pullback-style compatibility check, so simulated beliefs affect efficiency but cannot directly certify feasibility. Experiments on ALFWorld and ScienceWorld show that AEC achieves competitive success with fewer replanning rounds than strong LLM-agent baselines.

</details>


### [5] [Adaptive Test-Time Compute Allocation via Learned Heuristics over Categorical Structure](https://arxiv.org/abs/2602.03975)
*Shuhui Qu*

Main category: cs.AI

TL;DR: 研究验证成本受限下推理问题，提出状态级选择性验证框架，在MATH基准上用更少验证调用达更高准确率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型推理的测试时间计算受昂贵验证瓶颈限制，许多推理系统中大量验证调用浪费在冗余或无前景的中间假设上。

Method: 提出结合确定性可行性门控、预验证排名和基于局部不确定性的验证调用自适应分配的状态级选择性验证框架。

Result: 在MATH基准上，比best - of - N、多数投票和束搜索准确率更高，且验证调用减少44%。

Conclusion: 所提方法能将验证资源分配到最有价值之处，优于现有方法。

Abstract: Test-time computation has become a primary driver of progress in large language model (LLM) reasoning, but it is increasingly bottlenecked by expensive verification. In many reasoning systems, a large fraction of verifier calls are spent on redundant or unpromising intermediate hypotheses. We study reasoning under a \emph{verification-cost-limited} setting and ask how verification effort should be allocated across intermediate states. We propose a state-level selective verification framework that combines (i) deterministic feasibility gating over a structured move interface, (ii) pre-verification ranking using a hybrid of learned state-distance and residual scoring, and (iii) adaptive allocation of verifier calls based on local uncertainty. Unlike solution-level best-of-$N$ or uniform intermediate verification, our method distributes verification where it is most informative. On the \textsc{MATH} benchmark, our approach achieves higher accuracy than best-of-$N$, majority voting, and beam search while using 44\% fewer verifier calls.

</details>


### [6] [Puzzle it Out: Local-to-Global World Model for Offline Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2601.07463)
*Sijia li,Xinran Li,Shibo Chen,Jun Zhang*

Main category: cs.AI

TL;DR: 提出LOGO世界模型和不确定性感知采样机制，用于离线多智能体强化学习，实验表明超越现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有离线多智能体强化学习方法策略保守，模型方法难以准确估计转换和奖励函数，直接建模联合动力学困难。

Method: 提出LOGO世界模型，利用局部预测推断全局状态动态；引入不确定性感知采样机制，自适应加权合成数据。

Result: 在8个场景与8个基线的对比实验中，超越标准离线多智能体强化学习基准的现有基线。

Conclusion: 为可泛化的离线多智能体学习建立了新的基于模型的基线。

Abstract: Offline multi-agent reinforcement learning (MARL) aims to solve cooperative decision-making problems in multi-agent systems using pre-collected datasets. Existing offline MARL methods primarily constrain training within the dataset distribution, resulting in overly conservative policies that struggle to generalize beyond the support of the data. While model-based approaches offer a promising solution by expanding the original dataset with synthetic data generated from a learned world model, the high dimensionality, non-stationarity, and complexity of multi-agent systems make it challenging to accurately estimate the transitions and reward functions in offline MARL. Given the difficulty of directly modeling joint dynamics, we propose a local-to-global (LOGO) world model, a novel framework that leverages local predictions-which are easier to estimate-to infer global state dynamics, thus improving prediction accuracy while implicitly capturing agent-wise dependencies. Using the trained world model, we generate synthetic data to augment the original dataset, expanding the effective state-action space. To ensure reliable policy learning, we further introduce an uncertainty-aware sampling mechanism that adaptively weights synthetic data by prediction uncertainty, reducing approximation error propagation to policies. In contrast to conventional ensemble-based methods, our approach requires only an additional encoder for uncertainty estimation, significantly reducing computational overhead while maintaining accuracy. Extensive experiments across 8 scenarios against 8 baselines demonstrate that our method surpasses state-of-the-art baselines on standard offline MARL benchmarks, establishing a new model-based baseline for generalizable offline multi-agent learning.

</details>


### [7] [Monitorability as a Free Gift: How RLVR Spontaneously Aligns Reasoning](https://arxiv.org/abs/2602.03978)
*Zidi Xiong,Shan Chen,Himabindu Lakkaraju*

Main category: cs.AI

TL;DR: 论文通过系统评估发现LRMs在RLVR早期的可监控性提升并非普遍现象，受数据影响大，且与能力正交，还揭示了相关机制和动态。


<details>
  <summary>Details</summary>
Motivation: 随着大推理模型的广泛部署，对其思维链痕迹进行安全审计很关键，需明确可监控性在RLVR中的情况。

Method: 对不同模型家族和训练领域进行系统评估，开展机械分析。

Result: 可监控性提升强烈依赖数据，与能力正交，主要源于响应分布锐化和对提示注意力增加，且其动态随训练和评估难度变化。

Conclusion: 全面呈现了可监控性在RLVR中的出现情况，明确了何时可能提升，何时不会。

Abstract: As Large Reasoning Models (LRMs) are increasingly deployed, auditing their chain-of-thought (CoT) traces for safety becomes critical. Recent work has reported that monitorability--the degree to which CoT faithfully and informatively reflects internal computation--can appear as a "free gift" during the early stages of Reinforcement Learning with Verifiable Rewards (RLVR). We make this observation concrete through a systematic evaluation across model families and training domains. Our results show that this effect is not universal: monitorability improvements are strongly data-dependent. In particular, we demonstrate the critical role of data diversity and instruction-following data during RLVR training. We further show that monitorability is orthogonal to capability--improvements in reasoning performance do not imply increased transparency. Through mechanistic analysis, we attribute monitorability gains primarily to response distribution sharpening (entropy reduction) and increased attention to the prompt, rather than stronger causal reliance on reasoning traces. We also reveal how monitorability dynamics vary with controlled training and evaluation difficulty. Together, these findings provide a holistic view of how monitorability emerges under RLVR, clarifying when gains are likely to occur and when they are not.

</details>


### [8] [When AI Persuades: Adversarial Explanation Attacks on Human Trust in AI-Assisted Decision Making](https://arxiv.org/abs/2602.04003)
*Shutong Fan,Lan Zhang,Xiaoyong Yuan*

Main category: cs.AI

TL;DR: 文章介绍对抗性解释攻击（AEAs），通过控制实验研究其对人类信任AI输出的影响，发现用户对对抗性和良性解释信任相近，特定情况更易受影响。


<details>
  <summary>Details</summary>
Motivation: 现代AI系统在人类决策循环中运行，LLM生成的解释形成新的攻击面，需研究对抗性解释对人类信任AI输出的影响。

Method: 引入AEAs，用信任校准差距量化威胁，进行控制实验，系统改变解释框架的四个维度。

Result: 用户对对抗性和良性解释报告的信任几乎相同，特定情况更易受影响。

Conclusion: 这是首个将解释作为对抗性认知渠道并量化其对人类信任AI辅助决策影响的系统安全研究。

Abstract: Most adversarial threats in artificial intelligence target the computational behavior of models rather than the humans who rely on them. Yet modern AI systems increasingly operate within human decision loops, where users interpret and act on model recommendations. Large Language Models generate fluent natural-language explanations that shape how users perceive and trust AI outputs, revealing a new attack surface at the cognitive layer: the communication channel between AI and its users. We introduce adversarial explanation attacks (AEAs), where an attacker manipulates the framing of LLM-generated explanations to modulate human trust in incorrect outputs. We formalize this behavioral threat through the trust miscalibration gap, a metric that captures the difference in human trust between correct and incorrect outputs under adversarial explanations. By incorporating this gap, AEAs explore the daunting threats in which persuasive explanations reinforce users' trust in incorrect predictions. To characterize this threat, we conducted a controlled experiment (n = 205), systematically varying four dimensions of explanation framing: reasoning mode, evidence type, communication style, and presentation format. Our findings show that users report nearly identical trust for adversarial and benign explanations, with adversarial explanations preserving the vast majority of benign trust despite being incorrect. The most vulnerable cases arise when AEAs closely resemble expert communication, combining authoritative evidence, neutral tone, and domain-appropriate reasoning. Vulnerability is highest on hard tasks, in fact-driven domains, and among participants who are less formally educated, younger, or highly trusting of AI. This is the first systematic security study that treats explanations as an adversarial cognitive channel and quantifies their impact on human trust in AI-assisted decision making.

</details>


### [9] [Axiomatic Foundations of Counterfactual Explanations](https://arxiv.org/abs/2602.04028)
*Leila Amgoud,Martin Cooper*

Main category: cs.AI

TL;DR: 本文引入公理框架研究反事实解释器，证明不可能定理，刻画兼容集和对应关系，揭示五种反事实类型，对现有解释器进行分类和分析。


<details>
  <summary>Details</summary>
Motivation: 现有解释器多关注单一类型和局部解释，缺乏对替代反事实类型和全局反事实的系统研究，为填补此空白开展研究。

Method: 引入基于理想属性的公理框架，证明不可能定理，建立表示定理。

Result: 揭示五种不同类型的反事实，包括局部和全局解释，对现有解释器进行分类和行为刻画，分析生成解释的计算复杂度。

Conclusion: 该公理框架为反事实解释器提供了系统研究方法，有助于理解和生成不同类型的反事实解释。

Abstract: Explaining autonomous and intelligent systems is critical in order to improve trust in their decisions. Counterfactuals have emerged as one of the most compelling forms of explanation. They address ``why not'' questions by revealing how decisions could be altered. Despite the growing literature, most existing explainers focus on a single type of counterfactual and are restricted to local explanations, focusing on individual instances. There has been no systematic study of alternative counterfactual types, nor of global counterfactuals that shed light on a system's overall reasoning process.
  This paper addresses the two gaps by introducing an axiomatic framework built on a set of desirable properties for counterfactual explainers. It proves impossibility theorems showing that no single explainer can satisfy certain axiom combinations simultaneously, and fully characterizes all compatible sets. Representation theorems then establish five one-to-one correspondences between specific subsets of axioms and the families of explainers that satisfy them. Each family gives rise to a distinct type of counterfactual explanation, uncovering five fundamentally different types of counterfactuals. Some of these correspond to local explanations, while others capture global explanations. Finally, the framework situates existing explainers within this taxonomy, formally characterizes their behavior, and analyzes the computational complexity of generating such explanations.

</details>


### [10] [Scaling In-Context Online Learning Capability of LLMs via Cross-Episode Meta-RL](https://arxiv.org/abs/2602.04089)
*Xiaofeng Lin,Sirou Zhu,Yilei Chen,Mingyu Chen,Hejian Sang,Ioannis Paschalidis,Zhipeng Wang,Aldo Pacchiano,Xuezhou Zhang*

Main category: cs.AI

TL;DR: 现有大语言模型在在线决策任务中难利用上下文交互经验，本文提出ORBIT框架训练模型，小模型经训练后表现提升，且模型越大效果越好。


<details>
  <summary>Details</summary>
Motivation: 许多现实决策任务是在线的，现有大语言模型难以在这种场景中可靠利用上下文交互经验，需解决此局限。

Method: 引入多任务、多回合的元强化学习框架ORBIT，用于训练大语言模型从上下文中的交互学习。

Result: 相对小的开源模型Qwen3 - 14B在未见环境中上下文在线学习能力大幅提升，性能与GPT - 5.2相当，远超标准强化学习微调。

Conclusion: 通过训练可解决大语言模型在在线决策任务中的局限，且模型规模增大能带来持续收益，推理时学习的决策智能体有很大发展空间。

Abstract: Large language models (LLMs) achieve strong performance when all task-relevant information is available upfront, as in static prediction and instruction-following problems. However, many real-world decision-making tasks are inherently online: crucial information must be acquired through interaction, feedback is delayed, and effective behavior requires balancing information collection and exploitation over time. While in-context learning enables adaptation without weight updates, existing LLMs often struggle to reliably leverage in-context interaction experience in such settings. In this work, we show that this limitation can be addressed through training. We introduce ORBIT, a multi-task, multi-episode meta-reinforcement learning framework that trains LLMs to learn from interaction in context. After meta-training, a relatively small open-source model (Qwen3-14B) demonstrates substantially improved in-context online learning on entirely unseen environments, matching the performance of GPT-5.2 and outperforming standard RL fine-tuning by a large margin. Scaling experiments further reveal consistent gains with model size, suggesting significant headroom for learn-at-inference-time decision-making agents. Code reproducing the results in the paper can be found at https://github.com/XiaofengLin7/ORBIT.

</details>


### [11] [Interfaze: The Future of AI is built on Task-Specific Small Models](https://arxiv.org/abs/2602.04101)
*Harsha Vardhan Khurdula,Vineet Agarwal,Yoeven D Khemlani*

Main category: cs.AI

TL;DR: 提出Interfaze系统，结合多种组件处理LLM应用，在多任务上取得良好成绩，多数查询由小模型和工具栈处理，减轻大模型计算负担。


<details>
  <summary>Details</summary>
Motivation: 改变将现代LLM应用仅视为选择合适整体模型的问题，而是从构建和处理上下文的角度解决问题。

Method: 结合异构DNN与小语言模型作为感知模块、上下文构建层和动作层，通过薄控制器暴露单一端点，将处理后的上下文传递给用户选择的LLM。

Result: Interfaze - Beta在MMLU - Pro、MMLU等多个测试集上取得了如83.6%、91.4%等的成绩，且多数查询由小模型和工具栈处理。

Conclusion: 该系统能在保持竞争力的同时，将大部分计算从昂贵的整体模型转移出去。

Abstract: We present Interfaze, a system that treats modern LLM applications as a problem of building and acting over context, not just picking the right monolithic model. Instead of a single transformer, we combine (i) a stack of heterogeneous DNNs paired with small language models as perception modules for OCR involving complex PDFs, charts and diagrams, and multilingual ASR with (ii) a context-construction layer that crawls, indexes, and parses external sources (web pages, code, PDFs) into compact structured state, and (iii) an action layer that can browse, retrieve, execute code in a sandbox, and drive a headless browser for dynamic web pages. A thin controller sits on top of this stack and exposes a single, OpenAI-style endpoint: it decides which small models and actions to run and always forwards the distilled context to a user-selected LLM that produces the final response.
  On this architecture, Interfaze-Beta achieves 83.6% on MMLU-Pro, 91.4% on MMLU, 81.3% on GPQA-Diamond, 57.8% on LiveCodeBench v5, and 90.0% on AIME-2025, along with strong multimodal scores on MMMU (val) (77.3%), AI2D (91.5%), ChartQA (90.9%), and Common Voice v16 (90.8%). We show that most queries are handled primarily by the small-model and tool stack, with the large LLM operating only on distilled context, yielding competitive accuracy while shifting the bulk of computation away from the most expensive and monolithic models.

</details>


### [12] [OMG-Agent: Toward Robust Missing Modality Generation with Decoupled Coarse-to-Fine Agentic Workflows](https://arxiv.org/abs/2602.04144)
*Ruiting Dai,Zheyu Wang,Haoyu Yang,Yihan Liu,Chengzhi Wang,Zekun Zhang,Zishan Huang,Jiaman Cen,Lisi Mo*

Main category: cs.AI

TL;DR: 现有多模态数据重建方法有瓶颈，本文提出OMG - Agent框架，分三阶段处理，实验显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 数据不完整影响多模态系统可靠性，现有重建方法存在易产生幻觉、检索僵化和语义 - 细节纠缠等问题。

Method: 提出OMG - Agent框架，将任务分为MLLM驱动的语义规划器、非参数证据检索器和检索注入执行器三个协同阶段。

Result: 在多个基准测试中，OMG - Agent始终超越现有方法，在极端缺失情况下保持稳健，如在CMU - MOSI数据集70%缺失率时提升2.6分。

Conclusion: OMG - Agent框架有效解决了现有多模态数据重建方法的问题，性能表现优秀。

Abstract: Data incompleteness severely impedes the reliability of multimodal systems. Existing reconstruction methods face distinct bottlenecks: conventional parametric/generative models are prone to hallucinations due to over-reliance on internal memory, while retrieval-augmented frameworks struggle with retrieval rigidity. Critically, these end-to-end architectures are fundamentally constrained by Semantic-Detail Entanglement -- a structural conflict between logical reasoning and signal synthesis that compromises fidelity. In this paper, we present \textbf{\underline{O}}mni-\textbf{\underline{M}}odality \textbf{\underline{G}}eneration Agent (\textbf{OMG-Agent}), a novel framework that shifts the paradigm from static mapping to a dynamic coarse-to-fine Agentic Workflow. By mimicking a \textit{deliberate-then-act} cognitive process, OMG-Agent explicitly decouples the task into three synergistic stages: (1) an MLLM-driven Semantic Planner that resolves input ambiguity via Progressive Contextual Reasoning, creating a deterministic structured semantic plan; (2) a non-parametric Evidence Retriever that grounds abstract semantics in external knowledge; and (3) a Retrieval-Injected Executor that utilizes retrieved evidence as flexible feature prompts to overcome rigidity and synthesize high-fidelity details. Extensive experiments on multiple benchmarks demonstrate that OMG-Agent consistently surpasses state-of-the-art methods, maintaining robustness under extreme missingness, e.g., a $2.6$-point gain on CMU-MOSI at $70$\% missing rates.

</details>


### [13] [From Competition to Collaboration: Designing Sustainable Mechanisms Between LLMs and Online Forums](https://arxiv.org/abs/2602.04572)
*Niv Fono,Yftah Ziser,Omer Ben-Porat*

Main category: cs.AI

TL;DR: 本文针对生成式AI与问答论坛的数据依赖悖论，提出顺序交互框架，用模拟验证其可行性，强调AI与人类知识平台可持续协作潜力。


<details>
  <summary>Details</summary>
Motivation: 解决生成式AI依赖问答论坛数据提升性能，但又会吸引用户离开论坛的悖论。

Method: 提出顺序交互框架，并使用真实Stack Exchange数据和常用大语言模型进行全面数据驱动模拟。

Result: 实证显示激励不一致问题，但参与者能在理想全信息场景中获得约一半效用。

Conclusion: AI系统和人类知识平台存在实现可持续合作、有效知识共享的潜力。

Abstract: While Generative AI (GenAI) systems draw users away from (Q&A) forums, they also depend on the very data those forums produce to improve their performance. Addressing this paradox, we propose a framework of sequential interaction, in which a GenAI system proposes questions to a forum that can publish some of them. Our framework captures several intricacies of such a collaboration, including non-monetary exchanges, asymmetric information, and incentive misalignment. We bring the framework to life through comprehensive, data-driven simulations using real Stack Exchange data and commonly used LLMs. We demonstrate the incentive misalignment empirically, yet show that players can achieve roughly half of the utility in an ideal full-information scenario. Our results highlight the potential for sustainable collaboration that preserves effective knowledge sharing between AI systems and human knowledge platforms.

</details>


### [14] [Steering LLMs via Scalable Interactive Oversight](https://arxiv.org/abs/2602.04210)
*Enyu Zhou,Zhiheng Xi,Long Ma,Zhihao Zhang,Shihan Dou,Zhikai Lei,Guoteng Wang,Rui Zheng,Hang Yan,Tao Gui,Qi Zhang,Xuanjing Huang*

Main category: cs.AI

TL;DR: 大语言模型自动化复杂任务时出现监督差距，提出可扩展交互式监督框架解决，在网页开发任务验证有效，还可通过强化学习优化。


<details>
  <summary>Details</summary>
Motivation: 大语言模型执行任务能力强，但用户因专业知识不足等难以有效监督，存在可扩展监督挑战。

Method: 提出可扩展交互式监督框架，将复杂意图分解为可管理决策的递归树，在各节点获取低负担反馈并聚合为全局指导。

Result: 在网页开发任务中，非专家能生成专家级产品需求文档，对齐度提升54%。

Conclusion: 该框架可通过仅利用在线用户反馈的强化学习进行优化，为AI扩展时保持人类控制提供实用途径。

Abstract: As Large Language Models increasingly automate complex, long-horizon tasks such as \emph{vibe coding}, a supervision gap has emerged. While models excel at execution, users often struggle to guide them effectively due to insufficient domain expertise, the difficulty of articulating precise intent, and the inability to reliably validate complex outputs. It presents a critical challenge in scalable oversight: enabling humans to responsibly steer AI systems on tasks that surpass their own ability to specify or verify. To tackle this, we propose Scalable Interactive Oversight, a framework that decomposes complex intent into a recursive tree of manageable decisions to amplify human supervision. Rather than relying on open-ended prompting, our system elicits low-burden feedback at each node and recursively aggregates these signals into precise global guidance. Validated in web development task, our framework enables non-experts to produce expert-level Product Requirement Documents, achieving a 54\% improvement in alignment. Crucially, we demonstrate that this framework can be optimized via Reinforcement Learning using only online user feedback, offering a practical pathway for maintaining human control as AI scales.

</details>


### [15] [InterPReT: Interactive Policy Restructuring and Training Enable Effective Imitation Learning from Laypersons](https://arxiv.org/abs/2602.04213)
*Feiyu Gavin Zhu,Jean Oh,Reid Simmons*

Main category: cs.AI

TL;DR: 提出InterPReT方法降低普通人教AI代理门槛，用户研究表明该方法更适合普通用户训练可靠策略。


<details>
  <summary>Details</summary>
Motivation: 现有模仿学习方法依赖专业人员大规模演示和紧密监控训练过程，对普通人教代理新技能有挑战，为降低教AI代理的门槛而研究。

Method: 提出Interactive Policy Restructuring and Training (InterPReT) 方法，根据用户指令持续更新策略结构并优化参数以适应演示。

Result: 34人参与的用户研究表明，普通人负责演示和决定停止训练时，与通用模仿学习基线相比，该方法能产生更稳健策略且不影响系统可用性。

Conclusion: 该方法更适合没有太多机器学习技术背景的最终用户训练可靠策略。

Abstract: Imitation learning has shown success in many tasks by learning from expert demonstrations. However, most existing work relies on large-scale demonstrations from technical professionals and close monitoring of the training process. These are challenging for a layperson when they want to teach the agent new skills. To lower the barrier of teaching AI agents, we propose Interactive Policy Restructuring and Training (InterPReT), which takes user instructions to continually update the policy structure and optimize its parameters to fit user demonstrations. This enables end-users to interactively give instructions and demonstrations, monitor the agent's performance, and review the agent's decision-making strategies. A user study (N=34) on teaching an AI agent to drive in a racing game confirms that our approach yields more robust policies without impairing system usability, compared to a generic imitation learning baseline, when a layperson is responsible for both giving demonstrations and determining when to stop. This shows that our method is more suitable for end-users without much technical background in machine learning to train a dependable policy

</details>


### [16] [Empirical-MCTS: Continuous Agent Evolution via Dual-Experience Monte Carlo Tree Search](https://arxiv.org/abs/2602.04248)
*Hao Lu,Haoyuan Huang,Yulin Zhou,Chen Li,Ningxin Zhu*

Main category: cs.AI

TL;DR: 引入Empirical - MCTS框架增强大语言模型推理能力，评估显示其表现优于现有方法，强调结构化搜索与经验积累结合的重要性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型推理的无状态方法丢弃成功推理模式，无法模拟人类经验积累，需改进。

Method: 提出Empirical - MCTS双循环框架，包含PE - EMP和Memory Optimization Agent两种机制。

Result: 在复杂推理基准测试中，Empirical - MCTS显著优于无状态MCTS策略和独立经验驱动代理。

Conclusion: 结构化搜索与经验积累相结合对掌握复杂、开放式推理任务至关重要。

Abstract: Inference-time scaling strategies, particularly Monte Carlo Tree Search (MCTS), have significantly enhanced the reasoning capabilities of Large Language Models (LLMs). However, current approaches remain predominantly stateless, discarding successful reasoning patterns after each problem instance and failing to mimic the empirical accumulation of wisdom characteristic of human problem-solving. To bridge this gap, we introduce Empirical-MCTS, a dual-loop framework that transforms stateless search into a continuous, non-parametric learning process. The framework unifies local exploration with global memory optimization through two novel mechanisms: Pairwise-Experience-Evolutionary Meta-Prompting (PE-EMP) and a Memory Optimization Agent. PE-EMP functions as a reflexive optimizer within the local search, utilizing pairwise feedback to dynamically synthesize adaptive criteria and evolve meta-prompts (system prompts) in real-time. Simultaneously, the Memory Optimization Agent manages a global repository as a dynamic policy prior, employing atomic operations to distill high-quality insights across problems. Extensive evaluations on complex reasoning benchmarks, including AIME25, ARC-AGI-2, and MathArena Apex, demonstrate that Empirical-MCTS significantly outperforms both stateless MCTS strategies and standalone experience-driven agents. These results underscore the critical necessity of coupling structured search with empirical accumulation for mastering complex, open-ended reasoning tasks.

</details>


### [17] [Agent-Omit: Training Efficient LLM Agents for Adaptive Thought and Observation Omission via Agentic Reinforcement Learning](https://arxiv.org/abs/2602.04284)
*Yansong Ning,Jun Fang,Naiqiang Tan,Hao Liu*

Main category: cs.AI

TL;DR: 提出Agent - Omit框架让大语言模型代理自适应省略冗余思考和观察，在五个代理基准测试表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有研究在多轮交互中对整个交互轨迹一视同仁，忽略各轮思考必要性和观察效用差异，旨在提高代理效率和效果。

Method: 对思考和观察影响进行量化研究，提出Agent - Omit框架，合成冷启动数据微调代理，引入省略感知的强化学习方法，包含双采样机制和定制奖励。

Result: 证明省略策略偏差有KL散度上界，在五个代理基准测试中，Agent - Omit - 8B性能与七个前沿大语言模型代理相当，且实现更优的效果 - 效率权衡。

Conclusion: Agent - Omit框架能有效提升大语言模型代理在多轮交互中的效率和效果。

Abstract: Managing agent thought and observation during multi-turn agent-environment interactions is an emerging strategy to improve agent efficiency. However, existing studies treat the entire interaction trajectories equally, overlooking the thought necessity and observation utility varies across turns. To this end, we first conduct quantitative investigations into how thought and observation affect agent effectiveness and efficiency. Based on our findings, we propose Agent-Omit, a unified training framework that empowers LLM agents to adaptively omit redundant thoughts and observations. Specifically, we first synthesize a small amount of cold-start data, including both single-turn and multi-turn omission scenarios, to fine-tune the agent for omission behaviors. Furthermore, we introduce an omit-aware agentic reinforcement learning approach, incorporating a dual sampling mechanism and a tailored omission reward to incentivize the agent's adaptive omission capability. Theoretically, we prove that the deviation of our omission policy is upper-bounded by KL-divergence. Experimental results on five agent benchmarks show that our constructed Agent-Omit-8B could obtain performance comparable to seven frontier LLM agent, and achieve the best effectiveness-efficiency trade-off than seven efficient LLM agents methods. Our code and data are available at https://github.com/usail-hkust/Agent-Omit.

</details>


### [18] [From Assumptions to Actions: Turning LLM Reasoning into Uncertainty-Aware Planning for Embodied Agents](https://arxiv.org/abs/2602.04326)
*SeungWon Seo,SooBin Lim,SeongRae Noh,Haneul Kim,HyeongYeop Kang*

Main category: cs.AI

TL;DR: 提出PCE框架将LLM推理中的假设转化为决策树，在多智能体基准测试中表现出色，为不确定性规划提供可靠策略。


<details>
  <summary>Details</summary>
Motivation: 现有应用LLM的具身智能体主要通过频繁通信缓解不确定性，存在成本高、干扰工作流等问题。

Method: 引入PCE框架，将LLM推理中的潜在假设转化为结构化决策树，通过场景可能性、目标导向收益和执行成本对路径评分以指导行动选择。

Result: 在两个多智能体基准测试和三种不同LLM骨干上，PCE在成功率和任务效率上均优于以通信为中心的基线，且令牌使用量相当；消融实验表明PCE能提升不同规模模型的性能；用户研究显示PCE的通信模式更高效可靠。

Conclusion: PCE为将LLM潜在假设转化为不确定性规划可靠策略提供了原则性途径。

Abstract: Embodied agents operating in multi-agent, partially observable, and decentralized environments must plan and act despite pervasive uncertainty about hidden objects and collaborators' intentions. Recent advances in applying Large Language Models (LLMs) to embodied agents have addressed many long-standing challenges, such as high-level goal decomposition and online adaptation. Yet, uncertainty is still primarily mitigated through frequent inter-agent communication. This incurs substantial token and time costs, and can disrupt established workflows, when human partners are involved. We introduce PCE, a Planner-Composer-Evaluator framework that converts the fragmented assumptions latent in LLM reasoning traces into a structured decision tree. Internal nodes encode environment assumptions and leaves map to actions; each path is then scored by scenario likelihood, goal-directed gain, and execution cost to guide rational action selection without heavy communication. Across two challenging multi-agent benchmarks (C-WAH and TDW-MAT) and three diverse LLM backbones, PCE consistently outperforms communication-centric baselines in success rate and task efficiency while showing comparable token usage. Ablation results indicate that the performance gains obtained by scaling model capacity or reasoning depth persist even when PCE is applied, while PCE consistently raises the baseline across both capacity and reasoning-depth scales, confirming that structured uncertainty handling complements both forms of scaling. A user study further demonstrates that PCE produces communication patterns that human partners perceive as more efficient and trustworthy. Together, these results establish a principled route for turning latent LLM assumptions into reliable strategies for uncertainty-aware planning.

</details>


### [19] [Digital Twins & ZeroConf AI: Structuring Automated Intelligent Pipelines for Industrial Applications](https://arxiv.org/abs/2602.04385)
*Marco Picone,Fabio Turazza,Matteo Martinelli,Marco Mamei*

Main category: cs.AI

TL;DR: 本文提出零配置AI管道概念使AI管道无缝集成到CPS，通过微工厂场景验证可加速智能服务部署。


<details>
  <summary>Details</summary>
Motivation: CPS日益复杂，AI和ML集成困难，现有方法存在耦合问题，限制了可扩展性和重用性。

Method: 提出模块化、可互操作的解决方案，引入零配置（ZeroConf）AI管道，由数字孪生编排数据管理和智能增强。

Result: 在微工厂场景中展示了对并发ML模型和动态数据处理的支持。

Conclusion: 该方案能有效加速复杂工业环境中智能服务的部署。

Abstract: The increasing complexity of Cyber-Physical Systems (CPS), particularly in the industrial domain, has amplified the challenges associated with the effective integration of Artificial Intelligence (AI) and Machine Learning (ML) techniques. Fragmentation across IoT and IIoT technologies, manifested through diverse communication protocols, data formats and device capabilities, creates a substantial gap between low-level physical layers and high-level intelligent functionalities. Recently, Digital Twin (DT) technology has emerged as a promising solution, offering structured, interoperable and semantically rich digital representations of physical assets. Current approaches are often siloed and tightly coupled, limiting scalability and reuse of AI functionalities. This work proposes a modular and interoperable solution that enables seamless AI pipeline integration into CPS by minimizing configuration and decoupling the roles of DTs and AI components. We introduce the concept of Zero Configuration (ZeroConf) AI pipelines, where DTs orchestrate data management and intelligent augmentation. The approach is demonstrated in a MicroFactory scenario, showing support for concurrent ML models and dynamic data processing, effectively accelerating the deployment of intelligent services in complex industrial settings.

</details>


### [20] [ReThinker: Scientific Reasoning by Rethinking with Guided Reflection and Confidence Control](https://arxiv.org/abs/2602.04496)
*Zhentao Tang,Yuqi Cui,Shixiong Kai,Wenqian Zhao,Ke Ye,Xing Li,Anxin Tian,Zehua Pei,Hui-Ling Zhen,Shoubo Hu,Xiaoguang Li,Yunhe Wang,Mingxuan Yuan*

Main category: cs.AI

TL;DR: 提出ReThinker框架解决大语言模型在专家级科学推理任务上的问题，实验表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在专家级科学推理任务（如HLE）中表现不佳，现有方法存在刚性工具管道、脆弱的多智能体协调和低效的测试时扩展等问题。

Method: 引入基于Solver - Critic - Selector架构的ReThinker框架，动态分配计算资源，还提出反向数据合成管道和自适应轨迹回收策略用于无人类标注的可扩展训练。

Result: 在HLE、GAIA和XBench上的实验表明，ReThinker始终优于现有的带工具的基础模型和深度研究系统。

Conclusion: ReThinker在专家级推理任务上取得了最先进的结果。

Abstract: Expert-level scientific reasoning remains challenging for large language models, particularly on benchmarks such as Humanity's Last Exam (HLE), where rigid tool pipelines, brittle multi-agent coordination, and inefficient test-time scaling often limit performance. We introduce ReThinker, a confidence-aware agentic framework that orchestrates retrieval, tool use, and multi-agent reasoning through a stage-wise Solver-Critic-Selector architecture. Rather than following a fixed pipeline, ReThinker dynamically allocates computation based on model confidence, enabling adaptive tool invocation, guided multi-dimensional reflection, and robust confidence-weighted selection. To support scalable training without human annotation, we further propose a reverse data synthesis pipeline and an adaptive trajectory recycling strategy that transform successful reasoning traces into high-quality supervision. Experiments on HLE, GAIA, and XBench demonstrate that ReThinker consistently outperforms state-of-the-art foundation models with tools and existing deep research systems, achieving state-of-the-art results on expert-level reasoning tasks.

</details>


### [21] [Vibe AIGC: A New Paradigm for Content Generation via Agentic Orchestration](https://arxiv.org/abs/2602.04575)
*Jiaheng Liu,Yuanxing Zhang,Shihao Li,Xinping Lei*

Main category: cs.AI

TL;DR: 传统生成式AI遇‘可用性天花板’，本文提出Vibe AIGC新范式，以弥合人类想象与机器执行差距。


<details>
  <summary>Details</summary>
Motivation: 传统模型为中心的生成式AI虽视觉保真度提升，但存在意图 - 执行差距的‘可用性天花板’问题。

Method: 受Vibe Coding启发，引入Vibe AIGC范式，用户成为指挥官给出Vibe，元规划器将其解构为可执行管道。

Result: 从随机推理转向逻辑编排，弥合人类想象和机器执行的差距。

Conclusion: 这一转变将重新定义人机协作经济，使AI成为强大系统工程伙伴。

Abstract: For the past decade, the trajectory of generative artificial intelligence (AI) has been dominated by a model-centric paradigm driven by scaling laws. Despite significant leaps in visual fidelity, this approach has encountered a ``usability ceiling'' manifested as the Intent-Execution Gap (i.e., the fundamental disparity between a creator's high-level intent and the stochastic, black-box nature of current single-shot models). In this paper, inspired by the Vibe Coding, we introduce the \textbf{Vibe AIGC}, a new paradigm for content generation via agentic orchestration, which represents the autonomous synthesis of hierarchical multi-agent workflows.
  Under this paradigm, the user's role transcends traditional prompt engineering, evolving into a Commander who provides a Vibe, a high-level representation encompassing aesthetic preferences, functional logic, and etc. A centralized Meta-Planner then functions as a system architect, deconstructing this ``Vibe'' into executable, verifiable, and adaptive agentic pipelines. By transitioning from stochastic inference to logical orchestration, Vibe AIGC bridges the gap between human imagination and machine execution. We contend that this shift will redefine the human-AI collaborative economy, transforming AI from a fragile inference engine into a robust system-level engineering partner that democratizes the creation of complex, long-horizon digital assets.

</details>


### [22] [WideSeek-R1: Exploring Width Scaling for Broad Information Seeking via Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2602.04634)
*Zelai Xu,Zhexuan Xu,Ruize Zhang,Chunyang Zhu,Shi Yu,Weilin Liu,Quanlu Zhang,Wenbo Ding,Chao Yu,Yu Wang*

Main category: cs.AI

TL;DR: 本文探索大语言模型宽度扩展的多智能体系统，提出WideSeek - R1框架，实验显示其效果良好。


<details>
  <summary>Details</summary>
Motivation: 大语言模型深度扩展在处理更广泛任务时瓶颈从个体能力转向组织能力，现有多智能体系统工作流和交互方式难以有效并行工作，需新方法解决广泛信息搜索问题。

Method: 提出WideSeek - R1，即通过多智能体强化学习训练的主 - 子智能体框架，利用共享大语言模型和专用工具，在20k个信息搜索任务数据集上联合优化主智能体和并行子智能体。

Result: WideSeek - R1 - 4B在WideSearch基准测试中达到40.0%的项目F1分数，与单智能体DeepSeek - R1 - 671B性能相当，且随并行子智能体数量增加性能提升。

Conclusion: 宽度扩展方法有效，可用于解决广泛的信息搜索问题。

Abstract: Recent advancements in Large Language Models (LLMs) have largely focused on depth scaling, where a single agent solves long-horizon problems with multi-turn reasoning and tool use. However, as tasks grow broader, the key bottleneck shifts from individual competence to organizational capability. In this work, we explore a complementary dimension of width scaling with multi-agent systems to address broad information seeking. Existing multi-agent systems often rely on hand-crafted workflows and turn-taking interactions that fail to parallelize work effectively. To bridge this gap, we propose WideSeek-R1, a lead-agent-subagent framework trained via multi-agent reinforcement learning (MARL) to synergize scalable orchestration and parallel execution. By utilizing a shared LLM with isolated contexts and specialized tools, WideSeek-R1 jointly optimizes the lead agent and parallel subagents on a curated dataset of 20k broad information-seeking tasks. Extensive experiments show that WideSeek-R1-4B achieves an item F1 score of 40.0% on the WideSearch benchmark, which is comparable to the performance of single-agent DeepSeek-R1-671B. Furthermore, WideSeek-R1-4B exhibits consistent performance gains as the number of parallel subagents increases, highlighting the effectiveness of width scaling.

</details>


### [23] [Agentic AI in Healthcare & Medicine: A Seven-Dimensional Taxonomy for Empirical Evaluation of LLM-based Agents](https://arxiv.org/abs/2602.04813)
*Shubham Vatsal,Harsh Dubey,Aditi Singh*

Main category: cs.AI

TL;DR: 文章回顾49项研究，用七维分类法分析大语言模型在医疗领域应用，揭示能力分布不对称情况。


<details>
  <summary>Details</summary>
Motivation: 现有文献多为宽泛综述或单一能力深入探讨，缺乏医疗工作通用框架，需全面分析。

Method: 采用七维分类法，含29个操作子维度，用明确标准和标签规则对研究映射并总结。

Result: 发现能力存在明显不对称，如知识管理、交互模式等维度情况不同，多智能体设计为主流架构。

Conclusion: 信息中心能力较强，行动和发现导向领域有较大差距。

Abstract: Large Language Model (LLM)-based agents that plan, use tools and act has begun to shape healthcare and medicine. Reported studies demonstrate competence on various tasks ranging from EHR analysis and differential diagnosis to treatment planning and research workflows. Yet the literature largely consists of overviews which are either broad surveys or narrow dives into a single capability (e.g., memory, planning, reasoning), leaving healthcare work without a common frame. We address this by reviewing 49 studies using a seven-dimensional taxonomy: Cognitive Capabilities, Knowledge Management, Interaction Patterns, Adaptation & Learning, Safety & Ethics, Framework Typology and Core Tasks & Subtasks with 29 operational sub-dimensions. Using explicit inclusion and exclusion criteria and a labeling rubric (Fully Implemented, Partially Implemented, Not Implemented), we map each study to the taxonomy and report quantitative summaries of capability prevalence and co-occurrence patterns. Our empirical analysis surfaces clear asymmetries. For instance, the External Knowledge Integration sub-dimension under Knowledge Management is commonly realized (~76% Fully Implemented) whereas Event-Triggered Activation sub-dimenison under Interaction Patterns is largely absent (~92% Not Implemented) and Drift Detection & Mitigation sub-dimension under Adaptation & Learning is rare (~98% Not Implemented). Architecturally, Multi-Agent Design sub-dimension under Framework Typology is the dominant pattern (~82% Fully Implemented) while orchestration layers remain mostly partial. Across Core Tasks & Subtasks, information centric capabilities lead e.g., Medical Question Answering & Decision Support and Benchmarking & Simulation, while action and discovery oriented areas such as Treatment Planning & Prescription still show substantial gaps (~59% Not Implemented).

</details>


### [24] [Are AI Capabilities Increasing Exponentially? A Competing Hypothesis](https://arxiv.org/abs/2602.04836)
*Haosen Ge,Hamsa Bastani,Osbert Bastani*

Main category: cs.AI

TL;DR: 文章反驳AI能力指数增长观点，拟合曲线发现拐点已过，提出复杂模型支持AI能力近期有拐点，强调现有指数增长预测的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 针对Rapidly increasing AI capabilities have substantial real-world consequences, ranging from AI safety concerns to labor market consequences. The Model Evaluation & Threat Research (METR) 报告称AI能力自2019年呈指数增长的观点，指出数据不支持该观点。

Method: 对现有数据拟合S形曲线，提出将AI能力分解为基础和推理能力并具有各自改进速率的更复杂模型。

Result: 拟合S形曲线发现拐点已过，复杂模型支持AI能力近期将出现拐点的假设。

Conclusion: 不追求自身建立严谨预测，而是强调现有指数增长预测的脆弱性。

Abstract: Rapidly increasing AI capabilities have substantial real-world consequences, ranging from AI safety concerns to labor market consequences. The Model Evaluation & Threat Research (METR) report argues that AI capabilities have exhibited exponential growth since 2019. In this note, we argue that the data does not support exponential growth, even in shorter-term horizons. Whereas the METR study claims that fitting sigmoid/logistic curves results in inflection points far in the future, we fit a sigmoid curve to their current data and find that the inflection point has already passed. In addition, we propose a more complex model that decomposes AI capabilities into base and reasoning capabilities, exhibiting individual rates of improvement. We prove that this model supports our hypothesis that AI capabilities will exhibit an inflection point in the near future. Our goal is not to establish a rigorous forecast of our own, but to highlight the fragility of existing forecasts of exponential growth.

</details>


### [25] [Group-Evolving Agents: Open-Ended Self-Improvement via Experience Sharing](https://arxiv.org/abs/2602.04837)
*Zhaotian Weng,Antonis Antoniades,Deepak Nathani,Zhen Zhang,Xiao Pu,Xin Eric Wang*

Main category: cs.AI

TL;DR: 提出Group - Evolving Agents (GEA) 新范式用于开放式自我改进，在编码基准测试中表现优于现有自进化方法和人类设计框架，且具有更好转移和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 为减少对人类干预的依赖，解决现有开放式自我进化范式中探索多样性利用效率低的问题。

Method: 引入GEA范式，将一组代理作为基本进化单元，实现组内经验共享和重用。

Result: 在编码基准测试中，GEA显著优于现有自进化方法，与或超过顶级人类设计代理框架，能更有效转化早期探索多样性，且转移和鲁棒性更好。

Conclusion: GEA在开放式自我改进方面表现出色，能更有效地实现长期进步，具有良好的通用性和稳定性。

Abstract: Open-ended self-improving agents can autonomously modify their own structural designs to advance their capabilities and overcome the limits of pre-defined architectures, thus reducing reliance on human intervention. We introduce Group-Evolving Agents (GEA), a new paradigm for open-ended self-improvements, which treats a group of agents as the fundamental evolutionary unit, enabling explicit experience sharing and reuse within the group throughout evolution. Unlike existing open-ended self-evolving paradigms that adopt tree-structured evolution, GEA overcomes the limitation of inefficient utilization of exploratory diversity caused by isolated evolutionary branches. We evaluate GEA on challenging coding benchmarks, where it significantly outperforms state-of-the-art self-evolving methods (71.0% vs. 56.7% on SWE-bench Verified, 88.3% vs. 68.3% on Polyglot) and matches or exceeds top human-designed agent frameworks (71.8% and 52.0% on two benchmarks, respectively). Analysis reveals that GEA more effectively converts early-stage exploratory diversity into sustained, long-term progress, achieving stronger performance under the same number of evolved agents. Furthermore, GEA exhibits consistent transferability across different coding models and greater robustness, fixing framework-level bugs in 1.4 iterations on average, versus 5 for self-evolving methods.

</details>


### [26] [Fluid Representations in Reasoning Models](https://arxiv.org/abs/2602.04843)
*Dmitrii Kharlapenko,Alessandro Stolfo,Arthur Conmy,Mrinmaya Sachan,Zhijing Jin*

Main category: cs.AI

TL;DR: 分析QwQ - 32B处理抽象结构信息的机制，发现推理中内部表征改善及流体推理表征因素。


<details>
  <summary>Details</summary>
Motivation: 理解推理语言模型表现优越的内部机制。

Method: 对QwQ - 32B在Mystery Blocksworld上进行分析，开展转向实验。

Result: QwQ - 32B推理中改善行动和概念表征，形成关注结构的抽象编码，特定操作可改善问题解决。

Conclusion: 推理模型性能的驱动因素之一是上下文内对标记表征的细化，即流体推理表征。

Abstract: Reasoning language models, which generate long chains of thought, dramatically outperform non-reasoning language models on abstract problems. However, the internal model mechanisms that allow this superior performance remain poorly understood. We present a mechanistic analysis of how QwQ-32B - a model specifically trained to produce extensive reasoning traces - process abstract structural information. On Mystery Blocksworld - a semantically obfuscated planning domain - we find that QwQ-32B gradually improves its internal representation of actions and concepts during reasoning. The model develops abstract encodings that focus on structure rather than specific action names. Through steering experiments, we establish causal evidence that these adaptations improve problem solving: injecting refined representations from successful traces boosts accuracy, while symbolic representations can replace many obfuscated encodings with minimal performance loss. We find that one of the factors driving reasoning model performance is in-context refinement of token representations, which we dub Fluid Reasoning Representations.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [27] [OSCAgent: Accelerating the Discovery of Organic Solar Cells with LLM Agents](https://arxiv.org/abs/2602.04510)
*Zhaolin Hu,Zhiliang Wu,Hehe Fan,Yi Yang*

Main category: cs.CE

TL;DR: 本文提出用于有机太阳能电池（OSC）分子发现的多智能体框架OSCAgent，能生成化学有效、可合成的OSC分子，性能优于传统和仅使用大语言模型的基线。


<details>
  <summary>Details</summary>
Motivation: 现有的OSC分子生成方法大多局限于优化已知骨架，且缺乏对特定领域化学知识的有效利用，导致生成不切实际的候选分子。

Method: 引入多智能体框架OSCAgent，包含Planner、Generator和Experimenter三个协作智能体，将检索增强设计、分子生成和系统评估整合到持续改进的流程中。

Result: OSCAgent生成化学有效、可合成的OSC分子，预测性能优于传统和仅使用大语言模型的基线，部分候选分子预测效率接近18%。

Conclusion: OSCAgent是一种有效的OSC分子发现方法，代码将公开。

Abstract: Organic solar cells (OSCs) hold great promise for sustainable energy, but discovering high-performance materials is time-consuming and costly. Existing molecular generation methods can aid the design of OSC molecules, but they are mostly confined to optimizing known backbones and lack effective use of domain-specific chemical knowledge, often leading to unrealistic candidates. In this paper, we introduce OSCAgent, a multi-agent framework for OSC molecular discovery that unifies retrieval-augmented design, molecular generation, and systematic evaluation into a continuously improving pipeline, without requiring additional human intervention. OSCAgent comprises three collaborative agents. The Planner retrieves knowledge from literature-curated molecules and prior candidates to guide design directions. The Generator proposes new OSC acceptors aligned with these plans. The Experimenter performs comprehensive evaluation of candidate molecules and provides feedback for refinement. Experiments show that OSCAgent produces chemically valid, synthetically accessible OSC molecules and achieves superior predicted performance compared to both traditional and large language model (LLM)-only baselines. Representative results demonstrate that some candidates achieve predicted efficiencies approaching 18\%. The code will be publicly available.

</details>


### [28] [An Efficient Bayesian Framework for Inverse Problems via Optimization and Inversion: Surrogate Modeling, Parameter Inference, and Uncertainty Quantification](https://arxiv.org/abs/2602.04537)
*Mihaela Chiappetta,Massimo Carraturo,Alexander Raßloff,Markus Kästner,Ferdinando Auricchio*

Main category: cs.CE

TL;DR: 本文提出用于反问题的贝叶斯框架，结合优化与反演，在基准测试中证明能高效解决反问题并进行不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 解决高保真模型计算昂贵、难以处理，且数据有限的工程应用中的反问题。

Method: 采用贝叶斯优化构建高斯过程替代模型，再用贝叶斯反演结合先验知识和观测值推断最优参数。

Result: 在多个解析基准测试中，框架有效解决反问题，提供不确定性量化信息。

Conclusion: 该框架理论可靠、计算高效，结合优化与反演有协同优势，能支持可靠工程决策。

Abstract: The present paper proposes a Bayesian framework for inverse problems that seamlessly integrates optimization and inversion to enable rapid surrogate modeling, accurate parameter inference, and rigorous uncertainty quantification. Bayesian optimization is employed to adaptively construct accurate Gaussian process surrogate models using a minimal number of high-fidelity model evaluations, strategically focusing sampling in regions of high predictive uncertainty. The trained surrogate model is then leveraged within a Bayesian inversion scheme to infer optimal parameter values by combining prior knowledge with observed quantities of interest, resulting in posterior distributions that rigorously characterize epistemic uncertainty. The framework is theoretically grounded, computationally efficient, and particularly suited for engineering applications in which high-fidelity models -- whether arising from numerical simulations or physical experiments -- are computationally expensive, analytically intractable, or difficult to replicate, and data availability is limited. Furthermore, the combined use of Bayesian optimization and inversion outperforms their separate application, highlighting the synergistic benefits of unifying the two approaches. The performance of the proposed Bayesian framework is demonstrated on a suite of one- and two-dimensional analytical benchmarks, including the Mixed Gaussian-Periodic, Lévy, Griewank, Forrester, and Rosenbrock functions, which provide a controlled setting to assess surrogate modeling accuracy, parameter inference robustness, and uncertainty quantification. The results demonstrate the framework's effectiveness in efficiently solving inverse problems while providing informative uncertainty quantification and supporting reliable engineering decision-making at reduced computational cost.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [29] [StraTyper: Automated Semantic Type Discovery and Multi-Type Annotation for Dataset Collections](https://arxiv.org/abs/2602.04004)
*Christos Koutras,Juliana Freire*

Main category: cs.DB

TL;DR: 理解数据集语义对数据处理流程很重要，现有列类型注释方法有局限性，本文提出 StraTyper 方法解决问题，实验表明其有效且节省成本。


<details>
  <summary>Details</summary>
Motivation: 现有列类型注释方法需预定义语义类型，无法适应特定领域数据集，且处理多类型列有问题，使用专有大语言模型成本高、输出不一致，因此需要新方法。

Method: 引入 StraTyper 方法，通过系统利用大语言模型发现类型，结合战略列聚类、受控类型生成和迭代级联发现，平衡类型精度和注释覆盖率，同时降低成本。

Result: 实验表明 StraTyper 能为数值和非数值数据发现准确类型，相比商业大语言模型节省成本，有效处理多类型列。

Conclusion: StraTyper 方法能有效解决现有列类型注释方法的问题，其注释能提升数据连接发现和模式匹配等下游任务表现，优于仅使用大语言模型的基线方法。

Abstract: Understanding dataset semantics is crucial for effective search, discovery, and integration pipelines. To this end, column type annotation (CTA) methods associate columns of tabular datasets with semantic types that accurately describe their contents, using pre-trained deep learning models or Large Language Models (LLMs). However, existing approaches require users to specify a closed set of semantic types either at training or inference time, hindering their application to domain-specific datasets where pre-defined labels often lack adequate coverage and specificity. Furthermore, real-world datasets frequently contain columns with values belonging to multiple semantic types, violating the single-type assumption of existing CTA methods. While proprietary LLMs have shown effectiveness for CTA, they incur high monetary costs and produce inconsistent outputs for similar columns, leading to type redundancy that negatively affects downstream applications. To address these challenges, we introduce StraTyper, a cost-effective method for column type discovery (CTD) and multi-type annotation (CMTA) in dataset collections. StraTyper eliminates the need for pre-defined semantic labels by systematically employing LLMs to discovery types tailored to the dataset collection at hand. Through strategic column clustering, controlled type generation, and iterative cascading discovery, StraTyper balances type precision with annotation coverage while minimizing LLM costs. Our experimental evaluation-both manual and LLM-assisted-on real-world benchmarks demonstrates that StraTyper discovers accurate types for both numerical and non-numerical data, achieves substantial cost savings compared to commercial LLMs, and effectively handles multi-typed columns. We further show that StraTyper's annotations improve downstream tasks, including join discovery and schema matching, outperforming LLM-only baselines.

</details>


### [30] [PluRel: Synthetic Data unlocks Scaling Laws for Relational Foundation Models](https://arxiv.org/abs/2602.04029)
*Vignesh Kothapalli,Rishabh Ranjan,Valter Hudovernik,Vijay Prakash Dwivedi,Johannes Hoffart,Carlos Guestrin,Jure Leskovec*

Main category: cs.DB

TL;DR: 引入PluRel框架合成多表关系数据库，展示合成数据对RFM的作用及潜力


<details>
  <summary>Details</summary>
Motivation: RFM训练所需的多表关系数据库因隐私问题难以获取，现有多表合成方法存在不足

Method: PluRel以分步方式通过有向图建模模式、二分图建模表间连接、条件因果机制建模特征分布

Result: RFM预训练损失与合成数据库数量和总预训练令牌呈幂律缩放，增加合成数据库数量可提升对真实数据库的泛化能力，合成预训练能为真实数据库继续预训练提供强大基础模型

Conclusion: 合成数据扩展是RFM很有前景的范式

Abstract: Relational Foundation Models (RFMs) facilitate data-driven decision-making by learning from complex multi-table databases. However, the diverse relational databases needed to train such models are rarely public due to privacy constraints. While there are methods to generate synthetic tabular data of arbitrary size, incorporating schema structure and primary--foreign key connectivity for multi-table generation remains challenging. Here we introduce PluRel, a framework to synthesize multi-tabular relational databases from scratch. In a step-by-step fashion, PluRel models (1) schemas with directed graphs, (2) inter-table primary-foreign key connectivity with bipartite graphs, and, (3) feature distributions in tables via conditional causal mechanisms. The design space across these stages supports the synthesis of a wide range of diverse databases, while being computationally lightweight. Using PluRel, we observe for the first time that (1) RFM pretraining loss exhibits power-law scaling with the number of synthetic databases and total pretraining tokens, (2) scaling the number of synthetic databases improves generalization to real databases, and (3) synthetic pretraining yields strong base models for continued pretraining on real databases. Overall, our framework and results position synthetic data scaling as a promising paradigm for RFMs.

</details>


### [31] [Piece of CAKE: Adaptive Execution Engines via Microsecond-Scale Learning](https://arxiv.org/abs/2602.04181)
*Zijie Zhao,Ryan Marcus*

Main category: cs.DB

TL;DR: 提出CAKE系统，用微秒级上下文多臂老虎机为数据选择最优内核，可降低端到端工作负载延迟。


<details>
  <summary>Details</summary>
Motivation: 现有数据库系统依靠静态启发式或最坏情况最优默认值选择内核，错过显著性能提升机会。

Method: 使用微秒级上下文多臂老虎机，利用反事实的低成本选择性运行多个内核获取反馈，并将策略编译成低延迟后悔树。

Result: 与现有静态启发式方法相比，CAKE可将端到端工作负载延迟最多降低2倍。

Conclusion: CAKE系统能有效提升数据库内核选择性能，降低工作负载延迟。

Abstract: Low-level database operators often admit multiple physical implementations ("kernels") that are semantically equivalent but have vastly different performance characteristics depending on the input data distribution. Existing database systems typically rely on static heuristics or worst-case optimal defaults to select these kernels, often missing significant performance opportunities. In this work, we propose CAKE (Counterfactual Adaptive Kernel Execution), a system that learns to select the optimal kernel for each data "morsel" using a microsecond-scale contextual multi-armed bandit. CAKE circumvents the high latency of traditional reinforcement learning by exploiting the cheapness of counterfactuals -- selectively running multiple kernels to obtain full feedback -- and compiling policies into low-latency regret trees. Experimentally, we show that CAKE can reduce end-to-end workload latency by up to 2x compared to state-of-the-art static heuristics.

</details>


### [32] [LatentTune: Efficient Tuning of High Dimensional Database Parameters via Latent Representation Learning](https://arxiv.org/abs/2602.04190)
*Sein Kwon,Youngwan Jo,Seungyeon Choi,Jieun Lee,Huijun Jin,Sanghyun Park*

Main category: cs.DB

TL;DR: 随着数据量增长，数据库性能优化重要，现有参数调优方法有局限，提出LatentTune方法，实验表明其优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有数据库参数调优的机器学习方法存在生成训练数据集时间长、仅优化部分参数、未充分利用目标工作负载信息等问题，需改进。

Method: 提出LatentTune方法，采用数据增强策略减少数据生成时间，构建潜在空间优化全配置空间，将外部指标信息集成到潜在空间以适应目标工作负载。

Result: LatentTune在MySQL和RocksDB的四个工作负载上均优于基线模型，RocksDB最多提升1332%，MySQL吞吐量提升11.82%且延迟降低46.01%。

Conclusion: LatentTune是一种有效的数据库参数调优方法，能解决现有方法的局限，提升数据库性能。

Abstract: As data volumes continue to grow, optimizing database performance has become increasingly critical, making the implementation of effective tuning methods essential. Among various approaches, database parameter tuning has proven to be a highly effective means of enhancing performance. Recent studies have shown that machine learning techniques can successfully optimize database parameters, leading to significant performance improvements. However, existing methods still face several limitations. First, they require substantial time to generate large training datasets. Second, to cope with the challenges of highdimensional optimization, they typically optimize only a subset of parameters rather than the full configuration space. Third, they often rely on information from similar workloads instead of directly leveraging information from the target workload. To address these limitations, we propose LatentTune, a novel approach that differs fundamentally from traditional methods. To reduce the time required for data generation, LatentTune incorporates a data augmentation strategy. Furthermore, it constructs a latent space that compresses information from all database parameters, enabling the optimization of the full configuration space. In addition, LatentTune integrates external metric information into the latent space, allowing for precise tuning tailored to the actual target workload. Experimental results demonstrate that LatentTune outperforms baseline models across four workloads on MySQL and RocksDB, achieving up to 1332% improvement for RocksDB and 11.82% throughput gain with 46.01% latency reduction for MySQL.

</details>


### [33] [Data Agents: Levels, State of the Art, and Open Problems](https://arxiv.org/abs/2602.04261)
*Yuyu Luo,Guoliang Li,Ju Fan,Nan Tang*

Main category: cs.DB

TL;DR: 本文指出数据代理术语使用不统一的问题，提出从L0到L5的分层分类法，并介绍生命周期和层级驱动视角，还阐述各层级系统及研究挑战，提供实用地图和研究路线图。


<details>
  <summary>Details</summary>
Motivation: 当前“数据代理”术语使用不一致，模糊了能力边界和责任，使用户、系统构建者和监管者难以判断其能力。

Method: 提出从L0（无自主性）到L5（完全自主性）的数据代理分层分类法，基于此介绍生命周期和层级驱动视角，分别介绍各层级系统并讨论研究挑战。

Result: 给出数据代理的分层分类法、各层级代表系统及新兴系统情况。

Conclusion: 为当前系统提供实用地图，为未来十年数据代理发展提供研究路线图。

Abstract: Data agents are an emerging paradigm that leverages large language models (LLMs) and tool-using agents to automate data management, preparation, and analysis tasks. However, the term "data agent" is currently used inconsistently, conflating simple query responsive assistants with aspirational fully autonomous "data scientists". This ambiguity blurs capability boundaries and accountability, making it difficult for users, system builders, and regulators to reason about what a "data agent" can and cannot do.
  In this tutorial, we propose the first hierarchical taxonomy of data agents from Level 0 (L0, no autonomy) to Level 5 (L5, full autonomy). Building on this taxonomy, we will introduce a lifecycleand level-driven view of data agents. We will (1) present the L0-L5 taxonomy and the key evolutionary leaps that separate simple assistants from truly autonomous data agents, (2) review representative L0-L2 systems across data management, preparation, and analysis, (3) highlight emerging Proto-L3 systems that strive to autonomously orchestrate end-to-end data workflows to tackle diverse and comprehensive data-related tasks under supervision, and (4) discuss forward-looking research challenges towards proactive (L4) and generative (L5) data agents. We aim to offer both a practical map of today's systems and a research roadmap for the next decade of data-agent development.

</details>


### [34] [Identifying knowledge gaps in biodiversity data and their determinants at the regional level](https://arxiv.org/abs/2602.04314)
*Didier Alard,Anaïs Guéry*

Main category: cs.DB

TL;DR: 研究法国某地区生物多样性数据库知识缺口决定因素，发现无脊椎动物知识缺口大，受地点可达性影响，建议调整资金或校正空间采样偏差。


<details>
  <summary>Details</summary>
Motivation: 了解空间和分类学知识缺口的决定因素，以指导开放获取数据的使用。

Method: 用完整性和无知分数两个指标评估8个分类群知识缺口，分析整个地区和三个前子区域数据。

Result: 无脊椎动物知识缺口高于脊椎动物，知识缺口更受地点可达性而非生态吸引力影响，农业压力对无脊椎动物影响更显著。

Conclusion: 强调生物多样性治理对知识分布的影响，建议资金重定向或校正空间采样偏差。

Abstract: Biodiversity open-access databases are valuable resources in the structuring and accessibility of species occurrence data. By compiling different data sources, they reveal the uneven spatial distribution of knowledge, with areas or taxonomic groups better prospected than others. Understanding the determinants of spatial and taxonomic knowledge gaps helps in informing the use of open-access data. Here, we identified knowledge gaps' determinants within a French regional biodiversity database, in the largest administrative region in France. Knowledge gaps were assessed using two metrics, completeness and ignorance scores, for 8 taxonomic groups covering five vertebrates and three invertebrates groups. The data was analyzed for the entire region, but also at the level of the three former sub-regions, to identify the potential drivers that may account for knowledge gaps' determinants. Our findings show that invertebrates were characterized by higher knowledge gaps than vertebrates. Overall, knowledge gaps are influenced by variables related to sites' accessibility rather than ecological appeal across both metrics. All groups shared similar determinants of gaps, except for the impact of agricultural pressure which is found to be more significant for invertebrates than vertebrates. Ultimately, our study emphasizes the impact of biodiversity governance, through local funding and regional political decisions, on knowledge distribution in open-access databases. We recommend limiting these biases by redirecting biodiversity funding towards under-sampled taxonomic groups and under-prospected areas. When not possible, users of data extracted from these databases should correct for spatial-sampling biases (SSP) using knowledge gaps' maps in order to get a more accurate understanding of species occurrence.

</details>


### [35] [The Stretto Execution Engine for LLM-Augmented Data Systems](https://arxiv.org/abs/2602.04430)
*Gabriele Sanmartino,Matthias Urban,Paolo Papotti,Carsten Binnig*

Main category: cs.DB

TL;DR: 提出执行引擎Stretto，通过优化方法处理LLM增强数据系统查询的运行时间 - 准确性权衡问题，实验显示其性能优于现有系统且能满足质量保证。


<details>
  <summary>Details</summary>
Motivation: 解决LLM增强数据系统中执行查询时存在的运行时间 - 准确性权衡问题。

Method: 将查询规划表述为约束优化问题，使用基于梯度的优化器联合选择算子实现并分配误差预算；引入利用KV缓存实现不同物理算子的新思路。

Result: Stretto在实验中优于现有系统，且能持续满足质量保证。

Conclusion: Stretto能有效处理LLM增强数据系统查询的运行时间 - 准确性权衡问题。

Abstract: LLM-augmented data systems enable semantic querying over structured and unstructured data, but executing queries with LLM-powered operators introduces a fundamental runtime--accuracy trade-off. In this paper, we present Stretto, a new execution engine that provides end-to-end query guarantees while efficiently navigating this trade-off in a holistic manner. For this, Stretto formulates query planning as a constrained optimization problem and uses a gradient-based optimizer to jointly select operator implementations and allocate error budgets across pipelines. Moreover, to enable fine-grained execution choices, Stretto introduces a novel idea on how KV-caching can be used to realize a spectrum of different physical operators that transform a sparse design space into a dense continuum of runtime--accuracy trade-offs. Experiments show that Stretto outperforms state-of-the-art systems while consistently meeting quality guarantees.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [36] [Pending Conflicts Make Progress Impossible](https://arxiv.org/abs/2602.04013)
*Petr Kuznetsov,Pierre Sutra,Guillermo Toyos-Marfurt*

Main category: cs.DC

TL;DR: 研究共享对象的可交换性感知线性化实现的进度条件，提出冲突无阻碍性，证明其通用构造在异步读写共享内存模型中无法实现。


<details>
  <summary>Details</summary>
Motivation: 观察到可交换操作可并行执行，为共享对象实现寻找合适进度条件。

Method: 引入冲突无阻碍性这一概念，它是对无阻碍性和无等待性的推广。

Result: 证明冲突无阻碍的通用构造在异步读写共享内存模型中无法实现。

Conclusion: 冲突感知的通用构造存在根本限制，调用冲突操作会有同步成本，进度需要解决待处理冲突。

Abstract: In this work, we study progress conditions for commutativity-aware, linearizable implementations of shared objects. Motivated by the observation that commuting operations can be executed in parallel, we introduce conflict-obstruction-freedom: a process is guaranteed to complete its operation if it runs for long enough without encountering step contention with conflicting (non-commuting) operations. This condition generalizes obstruction-freedom and wait-freedom by allowing progress as long as step contention is only induced by commuting operations. We prove that conflict-obstruction-free universal constructions are impossible to implement in the asynchronous read-write shared memory model. This result exposes a fundamental limitation of conflict-aware universal constructions: the mere invocation of conflicting operations imposes a synchronization cost. Progress requires eventual resolution of pending conflicts.

</details>


### [37] [Six Times to Spare: LDPC Acceleration on DGX Spark for AI-Native Open RAN](https://arxiv.org/abs/2602.04652)
*Ryan Barker,Fatemeh Afghah*

Main category: cs.DC

TL;DR: 本文量化了将5G风格的LDPC5G解码从Grace CPU卸载到NVIDIA DGX~Spark平台上集成的Blackwell GB10 GPU的好处，发现GPU有显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 许多5G物理层LDPC解码在通用CPU上执行，存在错过时隙事件和可扩展性受限问题，需评估卸载到GPU的好处。

Method: 使用NVIDIA Sionna PHY/SYS在TensorFlow上构建NR-like链路级链，对并行解码码字数量和置信传播迭代次数进行扫描，记录CPU和GPU利用率及功率。

Result: GPU/CPU平均吞吐量加速比约为6倍，20次迭代时CPU每个码字延迟超0.5ms，GB10 GPU在相同工作负载下处于时隙的6 - 24%；CPU解码常消耗约十个Grace核心，GPU解码仅增加10 - 15W功率，且让CPU有更多能力处理高层任务。

Conclusion: 结果代表了可实现的加速器性能保守下限，为评估未来平台上LDPC和其他物理层内核提供了可复用、可脚本化的方法。

Abstract: Low-density parity-check (LDPC) decoding is one of the most computationally intensive kernels in the 5G New Radio (NR) physical layer and must complete within a 0.5\,ms transmission time interval while sharing the budget with FFT, channel estimation, demapping, HARQ, and MAC scheduling. Many open and proprietary stacks still execute LDPC on general-purpose CPUs, raising concerns about missed-slot events and limited scalability as bandwidths, modulation orders, and user multiplexing increase. This paper empirically quantifies the benefit of offloading 5G-style LDPC5G decoding from a Grace CPU to the integrated Blackwell GB10 GPU on an NVIDIA DGX~Spark platform. Using NVIDIA Sionna PHY/SYS on TensorFlow, we construct an NR-like link-level chain with an LDPC5G encoder/decoder, 16-QAM modulation, and AWGN, and sweep both the number of codewords decoded in parallel and the number of belief-propagation iterations, timing only the decoding phase while logging CPU and GPU utilization and power. Across the sweep we observe an average GPU/CPU throughput speedup of approximately $6\times$, with per-codeword CPU latency reaching $\approx 0.71$\,ms at 20 iterations (exceeding the 0.5\,ms slot), while the GB10 GPU remains within 6--24\% of the slot for the same workloads. Resource-usage measurements show that CPU-based LDPC decoding often consumes around ten Grace cores, whereas GPU-based decoding adds only $\approx10-15$\,W over GPU idle while leaving most CPU capacity available for higher-layer tasks. Because our implementation relies on high-level Sionna layers rather than hand-tuned CUDA, these results represent conservative lower bounds on achievable accelerator performance and provide a reusable, scriptable methodology for evaluating LDPC and other physical-layer kernels on future Grace/Blackwell and Aerial/ACAR/AODT platforms.

</details>


### [38] [A TEE-based Approach for Preserving Data Secrecy in Process Mining with Decentralized Sources](https://arxiv.org/abs/2602.04697)
*Davide Basile,Valerio Goretti,Luca Barbaro,Hajo A. Reijers,Claudio Di Ciccio*

Main category: cs.DC

TL;DR: 本文提出保密的跨组织流程挖掘方法CONFINE，利用可信执行环境实现多组织事件日志安全挖掘，经评估可处理实际工作负载且有可扩展性。


<details>
  <summary>Details</summary>
Motivation: 跨组织流程挖掘面临数据保密挑战，现有方法难以满足多组织数据挖掘时的保密需求。

Method: 提出CONFINE方法，利用可信执行环境部署可信应用，设计四阶段协议保障数据交换和处理，采用基于分段策略避免内存溢出。

Result: 对真实和合成数据评估显示，该方法能处理实际工作负载，内存随事件日志大小呈对数增长，随组织数量呈线性增长。

Conclusion: CONFINE方法可实现保密的跨组织流程挖掘，具有可扩展性和进一步优化的潜力。

Abstract: Process mining techniques enable organizations to gain insights into their business processes through the analysis of execution records (event logs) stored by information systems. While most process mining efforts focus on intra-organizational scenarios, many real-world business processes span multiple independent organizations. Inter-organizational process mining, though, faces significant challenges, particularly regarding confidentiality guarantees: The analysis of data can reveal information that the participating organizations may not consent to disclose to one another, or to a third party hosting process mining services. To overcome this issue, this paper presents CONFINE, an approach for secrecy-preserving inter-organizational process mining. CONFINE leverages Trusted Execution Environments (TEEs) to deploy trusted applications that are capable of securely mining multi-party event logs while preserving data secrecy. We propose an architecture supporting a four-stage protocol to secure data exchange and processing, allowing for protected transfer and aggregation of unaltered process data across organizational boundaries. To avoid out-of-memory errors due to the limited capacity of TEEs, our protocol employs a segmentation-based strategy, whereby event logs are transmitted to TEEs in smaller batches. We conduct a formal verification of correctness and a security analysis of the guarantees provided by the TEE core. We evaluate our implementation on real-world and synthetic data, showing that the proposed approach can handle realistic workloads. The results indicate logarithmic memory growth with respect to the event log size and linear growth with the number of provisioning organizations, highlighting scalability properties and opportunities for further optimization.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [39] [Approximately Partitioning Vertices into Short Paths](https://arxiv.org/abs/2602.03991)
*Mingyang Gong,Zhi-Zhong Chen,Brendan Mumey*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Given a fixed positive integer $k$ and a simple undirected graph $G = (V, E)$, the {\em $k^-$-path partition} problem, denoted by $k$PP for short, aims to find a minimum collection $\cal{P}$ of vertex-disjoint paths in $G$ such that each path in $\cal{P}$ has at most $k$ vertices and each vertex of $G$ appears in one path in $\cal{P}$. In this paper, we present a $\frac {k+4}5$-approximation algorithm for $k$PP when $k\in\{9,10\}$ and an improved $(\frac{\sqrt{11}-2}7 k + \frac {9-\sqrt{11}}7)$-approximation algorithm when $k \ge 11$. Our algorithms achieve the current best approximation ratios for $k \in \{ 9, 10, \ldots, 18 \}$.
  Our algorithms start with a maximum triangle-free path-cycle cover $\cal{F}$, which may not be feasible because of the existence of cycles or paths with more than $k$ vertices. We connect as many cycles in $\cal{F}$ with $4$ or $5$ vertices as possible by computing another maximum-weight path-cycle cover in a suitably constructed graph so that $\cal{F}$ can be transformed into a $k^-$-path partition of $G$ without losing too many edges.
  Keywords: $k^-$-path partition; Triangle-free path-cycle cover; $[f, g]$-factor; Approximation algorithm

</details>


### [40] [Minimizing Makespan in Sublinear Time via Weighted Random Sampling](https://arxiv.org/abs/2602.04059)
*Bin Fu,Yumei Huo,Hairong Zhao*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We consider the classical makespan minimization scheduling problem where $n$ jobs must be scheduled on $m$ identical machines. Using weighted random sampling, we developed two sublinear time approximation schemes: one for the case where $n$ is known and the other for the case where $n$ is unknown. Both algorithms not only give a $(1+3ε)$-approximation to the optimal makespan but also generate a sketch schedule.
  Our first algorithm, which targets the case where $n$ is known and draws samples in a single round under weighted random sampling, has a running time of $\tilde{O}(\tfrac{m^5}{ε^4} \sqrt{n}+A(\ceiling{m\over ε}, ε ))$, where
  $A(\mathcal{N}, α)$ is the time complexity of any $(1+α)$-approximation scheme for the makespan minimization of $\mathcal{N}$ jobs.
  The second algorithm addresses the case where $n$ is unknown. It uses adaptive weighted random sampling, %\textit{that is}, it draws samples in several rounds, adjusting the number of samples after each round,
  and runs in sublinear time $\tilde{O}\left( \tfrac{m^5} {ε^4} \sqrt{n} +
  A(\ceiling{m\over ε}, ε )\right)$. We also provide an implementation that generates a weighted random sample using $O(\log n)$ uniform random samples.

</details>


### [41] [QuadRank: Engineering a High Throughput Rank](https://arxiv.org/abs/2602.04103)
*R. Groot Koerkamp*

Main category: cs.DS

TL;DR: 本文为位向量和DNA字母表分别开发BiRank和QuadRank数据结构以处理rank查询，二者速度优于类似方法，预取可进一步提升性能，用QuadRank构建的FM - index比现有实现更小更快。


<details>
  <summary>Details</summary>
Motivation: 在生物信息学应用中，需快速处理输入，数据结构应在多线程时有高吞吐量，高效的rank查询方法是简洁数据结构的重要组成部分。

Method: 对于二进制字母表，开发BiRank，结合两篇论文核心思想，减少缓存未命中；对DNA字母表开发QuadRank扩展相关技术。支持预取缓存行以实现高效批处理。

Result: BiRank和QuadRank比不使用内联的类似方法快1.5 - 2倍，预取使速度再提升2倍，QuadRank结合预取构建的QuadFm比Genedex更小更快。

Conclusion: BiRank和QuadRank在处理rank查询时高效，适合高吞吐量和内存受限场景，采用预取技术可显著提高性能。

Abstract: Given a text, a query $\mathsf{rank}(q, c)$ counts the number of occurrences of character $c$ among the first $q$ characters of the text. Space-efficient methods to answer these rank queries form an important building block in many succinct data structures. For example, the FM-index is a widely used data structure that uses rank queries to locate all occurrences of a pattern in a text.
  In bioinformatics applications, the goal is usually to process a given input as fast as possible. Thus, data structures should have high throughput when used with many threads.
  Contributions. For the binary alphabet, we develop BiRank with 3.28% space overhead. It merges the central ideas of two recent papers: (1) we interleave (inline) offsets in each cache line of the underlying bit vector [Laws et al., 2024], reducing cache-misses, and (2) these offsets are to the middle of each block so that only half of them need popcounting [Gottlieb and Reinert, 2025]. In QuadRank (14.4% space overhead), we extend these techniques to the $σ=4$ (DNA) alphabet.
  Both data structures require only a single cache miss per query, making them highly suitable for high-throughput and memory-bound settings. To enable efficient batch-processing, we support prefetching the cache lines required to answer upcoming queries.
  Results. BiRank and QuadRank are around $1.5\times$ and $2\times$ faster than similar-overhead methods that do not use inlining. Prefetching gives an additional $2\times$ speedup, at which point the dual-channel DDR4 RAM bandwidth becomes a hard limit on the total throughput. With prefetching, both methods outperform all other methods apart from SPIDER [Laws et al., 2024] by $2\times$.
  When using QuadRank with prefetching in a toy count-only FM-index, QuadFm, this results in a smaller size and up to $4\times$ speedup over Genedex, a state-of-the-art batching FM-index implementation.

</details>


### [42] [Improved Sparse Recovery for Approximate Matrix Multiplication](https://arxiv.org/abs/2602.04386)
*Yahel Uffenheimer,Omri Weinstein*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We present a simple randomized algorithm for approximate matrix multiplication (AMM) whose error scales with the *output* norm $\|AB\|_F$. Given any $n\times n$ matrices $A,B$ and a runtime parameter $r\leq n$, the algorithm produces in $O(n^2(r+\log n))$ time, a matrix $C$ with total squared error $\mathbb{E}[\|C-AB\|_F^2]\le (1-\frac{r}{n})\|AB\|_F^2$, per-entry variance $\|AB\|_F^2/n^2$ and bias $\mathbb{E}[C]=\frac{r}{n}AB$. Alternatively, the algorithm can compute an *unbiased* estimation with expected total squared error $\frac{n}{r}\|{AB}\|_{F}^2$, recovering the state-of-art AMM error obtained by Pagh's TensorSketch algorithm (Pagh, 2013). Our algorithm is a log-factor faster.
  The key insight in the algorithm is a new variation of pseudo-random rotation of the input matrices (a Fast Hadamard Transform with asymmetric diagonal scaling), which redistributes the Frobenius norm of the *output* $AB$ uniformly across its entries.

</details>


### [43] [Simple 2-approximations for bad triangle transversals and some hardness results for related problems](https://arxiv.org/abs/2602.04463)
*Florian Adriaens,Nikolaj tatti*

Main category: cs.DS

TL;DR: 本文针对有符号图的坏三角形横截（BTT）问题提出新的2 - 近似算法，给出不可近似性结果，并探讨其与相关问题关系。


<details>
  <summary>Details</summary>
Motivation: 解决有符号图中找到移除最少边以消除坏三角形的问题，改进已有算法。

Method: 提出新的2 - 近似算法，利用近似坏三角形覆盖LP的结果，使用转换过程。

Result: 得到新的近似算法，给出不可近似性结果，证明相关问题的硬度，改进了BTT与相关问题最优值的比例。

Conclusion: 新算法简单快速，在BTT问题及相关问题上有较好结果，改进了已有结论。

Abstract: Given a signed graph, the bad triangle transversal (BTT) problem asks to find the smallest number of edges that need to be removed such that the remaining graph does not have a triangle with exactly one negative edge (a bad triangle). We propose novel 2-approximations for this problem, which are much simpler and faster than a folklore adaptation of the 2-approximation by Krivelevich for finding a minimum triangle transversal in unsigned graphs. One of our algorithms also works for weighted BTT and for approximately optimal feasible solutions to the bad triangle cover LP. Using a recent result on approximating the bad triangle cover LP, we obtain a $(2+ε)$ approximation in time almost equal to the time needed to find a maximal set of edge-disjoint bad triangles (which would give a standard 3-approximation). Additionally, several inapproximability results are provided. For complete signed graphs, we show that BTT is NP-hard to approximate with factor better than $\frac{2137}{2136}$. Our reduction also implies the same hardness result for related problems such as correlation clustering (cluster editing), cluster deletion and the min. strong triadic closure problem. On complete signed graphs, BTT is closely related to correlation clustering. We show that the correlation clustering optimum is at most $3/2$ times the BTT optimum, by describing a pivot procedure that transforms BTT solutions into clusters. This improves a result by Veldt, which states that their ratio is at most two.

</details>


### [44] [Incongruity-sensitive access to highly compressed strings](https://arxiv.org/abs/2602.04523)
*Ferdinando Cicalese,Zsuzsanna Lipták,Travis Gagie,Gonzalo Navarro,Nicola Prezza,Cristian Urbina*

Main category: cs.DS

TL;DR: 本文探讨对高度压缩字符串中相对不可压缩子串的快速随机访问，构建数据结构实现按特定规则的更快访问并证明相关结果。


<details>
  <summary>Details</summary>
Motivation: 更好的压缩可能阻碍访问，研究能否对高度压缩字符串中相对不可压缩子串实现更快访问。

Method: 针对运行长度压缩的直线程序（RLSLP）或块树构建特定空间的数据结构，还对短语源不与更大短语大量重叠的解析进行研究。

Result: 构建了O (g_{rl}) - 空间或O (L) - 空间的数据结构，支持按包含字符的最长重复子串长度对数级时间访问字符；对特定解析证明了更强大复杂的结果，查询时间还与获取查询字符需从源复制的短语数量有关。

Conclusion: 可以支持对高度压缩字符串中相对不可压缩子串的更快访问。

Abstract: Random access to highly compressed strings -- represented by straight-line programs or Lempel-Ziv parses, for example -- is a well-studied topic. Random access to such strings in strongly sublogarithmic time is impossible in the worst case, but previous authors have shown how to support faster access to specific characters and their neighbourhoods. In this paper we explore whether, since better compression can impede access, we can support faster access to relatively incompressible substrings of highly compressed strings. We first show how, given a run-length compressed straight-line program (RLSLP) of size $g_{rl}$ or a block tree of size $L$, we can build an $O (g_{rl})$-space or an $O (L)$-space data structure, respectively, that supports access to any character in time logarithmic in the length of the longest repeated substring containing that character. That is, the more incongruous a character is with respect to the characters around it in a certain sense, the faster we can support access to it. We then prove a similar but more powerful and sophisticated result for parsings in which phrases' sources do not overlap much larger phrases, with the query time depending also on the number of phrases we must copy from their sources to obtain the queried character.

</details>


### [45] [The matrix-vector complexity of $Ax=b$](https://arxiv.org/abs/2602.04842)
*Michał Dereziński,Ethan N. Epperly,Raphael A. Meyer*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Matrix-vector algorithms, particularly Krylov subspace methods, are widely viewed as the most effective algorithms for solving large systems of linear equations. This paper establishes lower bounds on the worst-case number of matrix-vector products needed by such an algorithm to approximately solve a general linear system. The first main result is that, for a matrix-vector algorithm which can perform products with both a matrix and its transpose, $Ω(κ\log(1/\varepsilon))$ matrix-vector products are necessary to solve a linear system with condition number $κ$ to accuracy $\varepsilon$, matching an upper bound for conjugate gradient on the normal equations. The second main result is that one-sided algorithms, which lack access to the transpose, must use $n$ matrix-vector products to solve an $n \times n$ linear system, even when the problem is perfectly conditioned. Both main results include explicit constants that match known upper bounds up to a factor of four. These results rigorously demonstrate the limitations of matrix-vector algorithms and confirm the optimality of widely used Krylov subspace algorithms.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [46] [Robustness of Stable Matchings When Attributes and Salience Determine Preferences](https://arxiv.org/abs/2602.04115)
*Amit Ronen,S. S. Ravi,Sarit Kraus*

Main category: cs.GT

TL;DR: 本文研究匹配市场中稳定匹配对显著向量扰动的鲁棒性，给出稳定匹配鲁棒性的验证算法、最大鲁棒半径计算算法及近似最稳健匹配的算法，还分析了鲁棒性与成本关系及鲁棒区域的几何结构。


<details>
  <summary>Details</summary>
Motivation: 在匹配市场中，参与者的显著向量会改变，因此需要研究稳定匹配对这些扰动的鲁棒性。

Method: 提出多项式时间算法验证稳定匹配的鲁棒性、计算最大鲁棒半径，设计随时搜索算法近似最稳健匹配，并结合凸几何工具分析鲁棒区域的几何结构。

Result: 给出了验证算法、计算最大鲁棒半径的算法、近似最稳健匹配的算法，刻画了鲁棒性 - 成本关系，以及鲁棒区域的几何结构，并且可以计算其体积。

Conclusion: 将匹配市场的鲁棒性分析与凸几何的经典工具联系起来，为匹配市场中稳定匹配的鲁棒性研究提供了有效的算法和理论支持。

Abstract: In many matching markets--such as athlete recruitment or academic admissions--participants on one side are evaluated by attribute vectors known to the other side, which in turn applies individual \emph{salience vectors} to assign relative importance to these attributes. Since saliences are known to change in practice, a central question arises: how robust is a stable matching to such perturbations? We address several fundamental questions in this context.
  First, we formalize robustness as a radius within which a stable matching remains immune to blocking pairs under any admissible perturbation of salience vectors (which are assumed to be normalized). Given a stable matching and a radius, we present a polynomial-time algorithm to verify whether the matching is stable within the specified radius. We also give a polynomial-time algorithm for computing the maximum robustness radius of a given stable matching. Further, we design an anytime search algorithm that uses certified lower and upper bounds to approximate the most robust stable matching, and we characterize the robustness-cost relationship through efficiently computable bounds that delineate the achievable tradeoff between robustness and cost. Finally, we show that for each stable matching, the set of salience profiles that preserve its stability factors is a product of low-dimensional polytopes within the simplex. This geometric structure precisely characterizes the polyhedral shape of each robustness region; its volume can then be computed efficiently, with approximate methods available as the dimension grows, thereby linking robustness analysis in matching markets with classical tools from convex geometry.

</details>


### [47] [Optimal Rates for Feasible Payoff Set Estimation in Games](https://arxiv.org/abs/2602.04397)
*Annalisa Barbara,Riccardo Poiani,Martino Bernasconi,Andrea Celli*

Main category: cs.GT

TL;DR: 研究学习者在仅观察玩家行动且不知均衡和博弈的情况下，推断玩家收益函数的问题，给出精确和近似均衡下零和与一般和博弈中可行收益集估计的最优率。


<details>
  <summary>Details</summary>
Motivation: 在多智能体环境下，解决学习者能否通过观察玩家行动来推断其收益函数的问题，为下游反事实分析和机制设计等应用提供基础。

Method: 聚焦于以高概率和豪斯多夫距离上的精度ε估计可行收益集，研究精确和近似均衡下零和与一般和博弈的情况。

Result: 提供了精确和近似均衡下零和与一般和博弈中可行收益集估计的首个最小最大最优率。

Conclusion: 为多智能体环境中的集值收益推断提供了学习理论基础。

Abstract: We study a setting in which two players play a (possibly approximate) Nash equilibrium of a bimatrix game, while a learner observes only their actions and has no knowledge of the equilibrium or the underlying game. A natural question is whether the learner can rationalize the observed behavior by inferring the players' payoff functions. Rather than producing a single payoff estimate, inverse game theory aims to identify the entire set of payoffs consistent with observed behavior, enabling downstream use in, e.g., counterfactual analysis and mechanism design across applications like auctions, pricing, and security games. We focus on the problem of estimating the set of feasible payoffs with high probability and up to precision $ε$ on the Hausdorff metric. We provide the first minimax-optimal rates for both exact and approximate equilibrium play, in zero-sum as well as general-sum games. Our results provide learning-theoretic foundations for set-valued payoff inference in multi-agent environments.

</details>


### [48] [Graph-Based Audits for Meek Single Transferable Vote Elections](https://arxiv.org/abs/2602.04527)
*Edouard Heitzmann*

Main category: cs.GT

TL;DR: 本文提出基于图的方法审计算法选举规则，实现与时间顺序无关的灵活审计框架。


<details>
  <summary>Details</summary>
Motivation: 选举安全背景下，通用的风险限制审计（RLA）框架难以应用于单可转移投票（STV）等算法选举规则，因其依赖选举和淘汰的时间顺序。

Method: 提出基于图的方法，考虑所有可能的选举和淘汰序列空间，预先确定该空间的子图，统计验证真实选举序列不离开该子图。

Result: 构建了可灵活审计算法选举规则的框架。

Conclusion: 该方法能以与时间顺序无关的方式审计算法选举规则。

Abstract: In the context of election security, a Risk-Limiting Audit (RLA) is a statistical framework that uses a minimal partial recount of the ballots to guarantee that the results of the election were correctly reported. A generalized RLA framework has remained elusive for algorithmic election rules such as the Single Transferable Vote (STV) rule, because of the dependence of these rules on the chronology of eliminations and elections leading to the outcome of the election. This paper proposes a new graph-based approach to audit these algorithmic election rules, by considering the space of all possible sequences of elections and eliminations. If we fix a subgraph of this universal space ahead of the audit, a sufficient strategy is to verify statistically that the true election sequence does not leave the fixed subgraph. This makes for a flexible framework to audit these elections in a chronology-agnostic way.

</details>


### [49] [Winning in the Limit: Average-Case Committee Selection with Many Candidates](https://arxiv.org/abs/2602.04815)
*Yifan Lin,Shenyu Qin,Kangning Wang,Lirong Xia*

Main category: cs.GT

TL;DR: 研究大量选民和更多候选人的公正文化模型中委员会选择问题，用概率方法等确定α - 获胜集和α - 支配集的阈值现象。


<details>
  <summary>Details</summary>
Motivation: 研究在固定委员会规模k下，委员会何时能以规定多数水平α集体击败委员会外每个候选人。

Method: 使用概率方法、对偶论证和舍入技术。

Result: α - 获胜集阈值为α_win^★ = 1 - 1/k，α - 支配集阈值为α_dom^★ = 1/2 - 1/(2k)，且得到α - 支配集的不可能结果。

Conclusion: 确定两种集体支配概念的阈值，改进了α - 支配集已知的最佳界限。

Abstract: We study the committee selection problem in the canonical impartial culture model with a large number of voters and an even larger candidate set. Here, each voter independently reports a uniformly random preference order over the candidates. For a fixed committee size $k$, we ask when a committee can collectively beat every candidate outside the committee by a prescribed majority level $α$. We focus on two natural notions of collective dominance, $α$-winning and $α$-dominating sets, and we identify sharp threshold phenomena for both of them using probabilistic methods, duality arguments, and rounding techniques.
  We first consider $α$-winning sets. A set $S$ of $k$ candidates is $α$-winning if, for every outside candidate $a \notin S$, at least an $α$-fraction of voters rank some member of $S$ above $a$. We show a sharp threshold at \[ α_{\mathrm{win}}^\star = 1 - \frac{1}{k}. \] Specifically, an $α$-winning set of size $k$ exists with high probability when $α< α_{\mathrm{win}}^\star$, and is unlikely to exist when $α> α_{\mathrm{win}}^\star$.
  We then study the stronger notion of $α$-dominating sets. A set $S$ of $k$ candidates is $α$-dominating if, for every outside candidate $a \notin S$, there exists a single committee member $b \in S$ such that at least an $α$-fraction of voters prefer $b$ to $a$. Here we establish an analogous sharp threshold at \[ α_{\mathrm{dom}}^\star = \frac{1}{2} - \frac{1}{2k}. \] As a corollary, our analysis yields an impossibility result for $α$-dominating sets: for every $k$ and every $α> α_{\mathrm{dom}}^\star = 1 / 2 - 1 / (2k)$, there exist preference profiles that admit no $α$-dominating set of size $k$. This corollary improves the best previously known bounds for all $k \geq 2$.

</details>


### [50] [Properties of the core and other solution concepts of Bel coalitional games in the ex-ante scenario](https://arxiv.org/abs/2602.04817)
*Michel Grabisch,Silvia Lorenzini*

Main category: cs.GT

TL;DR: 研究Bel联盟博弈核心及其他解概念性质，考虑不确定性，聚焦事前场景，研究几何结构并定义新解概念，发现包含关系与经典情况相同，凸博弈中核心和议价集有特定重合条件。


<details>
  <summary>Details</summary>
Motivation: 经典联盟博弈引入不确定性，研究Bel联盟博弈解概念性质。

Method: 用Dempster - Shafer理论建模先验知识，用Choquet积分建模偏好，聚焦事前场景研究。

Result: 发现解概念间包含关系与经典情况相同，凸Bel联盟博弈中核心和议价集在加强议价集定义下重合。

Conclusion: Bel联盟博弈在事前场景下解概念有与经典情况相似的包含关系，凸博弈有特殊性质。

Abstract: We study the properties of the core and other solution concepts of Bel coalitional games, that generalize classical coalitional games by introducing uncertainty in the framework. In this uncertain environment, we work with contracts, that specify how agents divide the values of the coalitions in the different states of the world. Every agent can have different a priori knowledge on the true state of the world, which is modeled through the Dempster-Shafer theory, while agents' preferences between contracts are modeled by the Choquet integral. We focus on the "ex-ante" scenario, when the contract is evaluated before uncertainty is resolved. We investigate the geometrical structure of the ex-ante core when agents have the same a priori knowledge which is a probability distribution. Finally, we define the (pre)nucleolus, the kernel and the bargaining set (a la Mas-Colell) in the ex-ante situation and we study their properties. It is found that the inclusion relations among these solution concepts are the same as in the classical case. Coincidence of the ex-ante core and the ex-ante bargaining set holds for convex Bel coalitional games, at the price of strengthening the definition of bargaining sets.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [51] [Nemotron ColEmbed V2: Top-Performing Late Interaction embedding models for Visual Document Retrieval](https://arxiv.org/abs/2602.03992)
*Gabriel de Souza P. Moreira,Ronay Ak,Mengyao Xu,Oliver Holworthy,Benedikt Schifferer,Zhiding Yu,Yauhen Babakhin,Radek Osmulski,Jiarui Cai,Ryan Chesler,Bo Liu,Even Oldridge*

Main category: cs.IR

TL;DR: 介绍了RAG系统及视觉文档检索背景，推出Nemotron ColEmbed V2模型家族，在ViDoRe基准测试中表现出色，描述技术并讨论工程挑战。


<details>
  <summary>Details</summary>
Motivation: 满足日益增长的视觉文档检索需求。

Method: 基于预训练VLM发布3种参数变体模型，采用聚类采样、难负例挖掘等技术。

Result: 8B模型在2026年2月3日ViDoRe V3排行榜排名第一，平均NDCG@10为63.42。

Conclusion: 介绍的技术帮助构建了高性能模型，还探讨了平衡准确性和存储的方法。

Abstract: Retrieval-Augmented Generation (RAG) systems have been popular for generative applications, powering language models by injecting external knowledge. Companies have been trying to leverage their large catalog of documents (e.g. PDFs, presentation slides) in such RAG pipelines, whose first step is the retrieval component. Dense retrieval has been a popular approach, where embedding models are used to generate a dense representation of the user query that is closer to relevant content embeddings. More recently, VLM-based embedding models have become popular for visual document retrieval, as they preserve visual information and simplify the indexing pipeline compared to OCR text extraction.
  Motivated by the growing demand for visual document retrieval, we introduce Nemotron ColEmbed V2, a family of models that achieve state-of-the-art performance on the ViDoRe benchmarks. We release three variants - with 3B, 4B, and 8B parameters - based on pre-trained VLMs: NVIDIA Eagle 2 with Llama 3.2 3B backbone, Qwen3-VL-4B-Instruct and Qwen3-VL-8B-Instruct, respectively. The 8B model ranks first on the ViDoRe V3 leaderboard as of February 03, 2026, achieving an average NDCG@10 of 63.42.
  We describe the main techniques used across data processing, training, and post-training - such as cluster-based sampling, hard-negative mining, bidirectional attention, late interaction, and model merging - that helped us build our top-performing models. We also discuss compute and storage engineering challenges posed by the late interaction mechanism and present experiments on how to balance accuracy and storage with lower dimension embeddings.

</details>


### [52] [Following the TRAIL: Predicting and Explaining Tomorrow's Hits with a Fine-Tuned LLM](https://arxiv.org/abs/2602.04225)
*Yinan Zhang,Zhixi Chen,Jiazheng Jing,Zhiqi Shen*

Main category: cs.IR

TL;DR: 提出TRAIL模型，结合大语言模型预测短期物品流行度并生成解释，实验显示其优于基线。


<details>
  <summary>Details</summary>
Motivation: 大语言模型应用于推荐系统有挑战，且现有推荐系统忽视解释，受到预测近期物品流行度成果的启发。

Method: 提出TRAIL，这是一个微调的大语言模型，使用正负样本对比学习使分数和解释与结构化趋势信号对齐。

Result: TRAIL在实验中优于强基线，能生成连贯且有依据的解释。

Conclusion: TRAIL在预测物品短期流行度和生成解释方面表现良好。

Abstract: Large Language Models (LLMs) have been widely applied across multiple domains for their broad knowledge and strong reasoning capabilities. However, applying them to recommendation systems is challenging since it is hard for LLMs to extract user preferences from large, sparse user-item logs, and real-time per-user ranking over the full catalog is too time-consuming to be practical. Moreover, many existing recommender systems focus solely on ranking items while overlooking explanations, which could help improve predictive accuracy and make recommendations more convincing to users. Inspired by recent works that achieve strong recommendation performance by forecasting near-term item popularity, we propose TRAIL (TRend and explAnation Integrated Learner). TRAIL is a fine-tuned LLM that jointly predicts short-term item popularity and generates faithful natural-language explanations. It employs contrastive learning with positive and negative pairs to align its scores and explanations with structured trend signals, yielding accurate and explainable popularity predictions. Extensive experiments show that TRAIL outperforms strong baselines and produces coherent, well-grounded explanations.

</details>


### [53] [LILaC: Late Interacting in Layered Component Graph for Open-domain Multimodal Multihop Retrieval](https://arxiv.org/abs/2602.04263)
*Joohyung Yun,Doyup Lee,Wook-Shin Han*

Main category: cs.IR

TL;DR: 提出多模态检索框架LILaC应对多模态文档检索挑战，在五个基准测试上取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 多模态文档检索需要解决固定单粒度检索单元的无关内容影响和支持多跳推理的问题。

Method: 提出LILaC框架，包括引入分层组件图和基于后期交互的子图检索方法。

Result: LILaC在五个基准测试上取得了最先进的检索性能，且无需额外微调。

Conclusion: LILaC是一个有效的多模态检索框架，具有推广价值，代码已公开。

Abstract: Multimodal document retrieval aims to retrieve query-relevant components from documents composed of textual, tabular, and visual elements. An effective multimodal retriever needs to handle two main challenges: (1) mitigate the effect of irrelevant contents caused by fixed, single-granular retrieval units, and (2) support multihop reasoning by effectively capturing semantic relationships among components within and across documents. To address these challenges, we propose LILaC, a multimodal retrieval framework featuring two core innovations. First, we introduce a layered component graph, explicitly representing multimodal information at two layers - each representing coarse and fine granularity - facilitating efficient yet precise reasoning. Second, we develop a late-interaction-based subgraph retrieval method, an edge-based approach that initially identifies coarse-grained nodes for efficient candidate generation, then performs fine-grained reasoning via late interaction. Extensive experiments demonstrate that LILaC achieves state-of-the-art retrieval performance on all five benchmarks, notably without additional fine-tuning. We make the artifacts publicly available at github.com/joohyung00/lilac.

</details>


### [54] [MiniRec: Data-Efficient Reinforcement Learning for LLM-based Recommendation](https://arxiv.org/abs/2602.04278)
*Lin Wang,Yang Zhang,Jingfan Chen,Xiaoyan Zhao,Fengbin Zhu,Qing Li,Tat-Seng Chua*

Main category: cs.IR

TL;DR: 提出MiniRec框架用于基于强化学习的大语言模型推荐，降低训练成本并保持性能。


<details>
  <summary>Details</summary>
Motivation: 基于强化学习的大语言模型推荐存在效率挑战，现有数据选择方法与强化学习动态不匹配，导致性能不佳。

Method: 提出MiniRec框架，用奖励评估样本可学习性，通过对齐样本梯度与近似“理想”全局强化学习优化轨迹评估代表性，结合从易到难的课程学习策略。

Result: 显著降低训练成本，同时很大程度上保持性能。

Conclusion: 强调了基于奖励对齐、轨迹信息的数据选择在基于强化学习的大语言模型推荐中的重要性。

Abstract: The integration of reinforcement learning (RL) into large language models (LLMs) has opened new opportunities for recommender systems by eliciting reasoning and improving user preference modeling. However, RL-based LLM recommendation faces significant efficiency challenges, making full-data training costly. Existing data selection methods define sample value based on learnability or representativeness, yet their loss- or gradient-driven or dataset coverage-driven criteria often misalign with RL learning dynamics, resulting in suboptimal performance. To address this, we propose MiniRec, a data selection framework tailored for RL-based LLM recommendation. MiniRec evaluates sample learnability using key RL signals -- rewards -- pruning samples that are too easy (too high reward) or too difficult (consistently low reward). It assesses representativeness by aligning sample gradients with the approximated "ideal" global RL optimization trajectory, selecting samples that mainly drive model updates, and it also enforces diversity to reduce redundancy. Combined with a curriculum learning strategy from easy to hard samples, MiniRec significantly reduces training cost while largely preserving performance. Extensive experiments demonstrate MiniRec's effectiveness, highlighting the importance of reward-aligned, trajectory-informed data selection in RL-based LLM recommendation.

</details>


### [55] [SDR-CIR: Semantic Debias Retrieval Framework for Training-Free Zero-Shot Composed Image Retrieval](https://arxiv.org/abs/2602.04451)
*Yi Sun,Jinyu Xu,Qing Xie,Jiachen Li,Yanchun Ma,Yongjian Liu*

Main category: cs.IR

TL;DR: 提出无训练语义去偏排序方法SDR - CIR用于组合图像检索，减少语义偏差，在三个基准测试取得最优结果。


<details>
  <summary>Details</summary>
Motivation: 现有无训练零样本组合图像检索方法生成的描述易产生语义偏差。

Method: 提出SDR - CIR方法，用选择性思维链减少视觉噪声，通过锚定和去偏两步语义去偏排序减轻语义偏差。

Result: 在三个标准CIR基准测试中，SDR - CIR在单阶段方法中取得了最先进的结果，且保持了高效率。

Conclusion: SDR - CIR通过补充缺失线索和抑制冗余信息，减轻了语义偏差，提高了检索性能。

Abstract: Composed Image Retrieval (CIR) aims to retrieve a target image from a query composed of a reference image and modification text. Recent training-free zero-shot methods often employ Multimodal Large Language Models (MLLMs) with Chain-of-Thought (CoT) to compose a target image description for retrieval. However, due to the fuzzy matching nature of ZS-CIR, the generated description is prone to semantic bias relative to the target image. We propose SDR-CIR, a training-free Semantic Debias Ranking method based on CoT reasoning. First, Selective CoT guides the MLLM to extract visual content relevant to the modification text during image understanding, thereby reducing visual noise at the source. We then introduce a Semantic Debias Ranking with two steps, Anchor and Debias, to mitigate semantic bias. In the Anchor step, we fuse reference image features with target description features to reinforce useful semantics and supplement omitted cues. In the Debias step, we explicitly model the visual semantic contribution of the reference image to the description and incorporate it into the similarity score as a penalty term. By supplementing omitted cues while suppressing redundancy, SDR-CIR mitigates semantic bias and improves retrieval performance. Experiments on three standard CIR benchmarks show that SDR-CIR achieves state-of-the-art results among one-stage methods while maintaining high efficiency. The code is publicly available at https://github.com/suny105/SDR-CIR.

</details>


### [56] [DOS: Dual-Flow Orthogonal Semantic IDs for Recommendation in Meituan](https://arxiv.org/abs/2602.04460)
*Junwei Yin,Senjie Kou,Changhao Li,Shuli Wang,Xue Wei,Yinqiu Huang,Yinhua Zhu,Haitao Wang,Xingxing Wang*

Main category: cs.IR

TL;DR: 本文提出Dual - Flow Orthogonal Semantic IDs (DOS)方法解决现有语义ID在生成推荐系统中存在的问题，实验证明有效并已在美团应用部署。


<details>
  <summary>Details</summary>
Motivation: 现有语义ID方法在生成任务中缺乏上下文感知、量化方法欠佳，导致推荐效果不佳和语义损失，需改进。

Method: 提出DOS方法，采用用户 - 物品双流框架利用协同信号对齐语义ID码本空间和生成空间，引入正交残差量化方案旋转语义空间以保留语义。

Result: 大量离线实验和在线A/B测试证明了DOS方法的有效性，且已在美团移动应用部署服务数亿用户。

Conclusion: DOS方法能有效解决现有语义ID方法的问题，可应用于实际推荐系统。

Abstract: Semantic IDs serve as a key component in generative recommendation systems. They not only incorporate open-world knowledge from large language models (LLMs) but also compress the semantic space to reduce generation difficulty. However, existing methods suffer from two major limitations: (1) the lack of contextual awareness in generation tasks leads to a gap between the Semantic ID codebook space and the generation space, resulting in suboptimal recommendations; and (2) suboptimal quantization methods exacerbate semantic loss in LLMs. To address these issues, we propose Dual-Flow Orthogonal Semantic IDs (DOS) method. Specifically, DOS employs a user-item dual flow-framework that leverages collaborative signals to align the Semantic ID codebook space with the generation space. Furthermore, we introduce an orthogonal residual quantization scheme that rotates the semantic space to an appropriate orientation, thereby maximizing semantic preservation. Extensive offline experiments and online A/B testing demonstrate the effectiveness of DOS. The proposed method has been successfully deployed in Meituan's mobile application, serving hundreds of millions of users.

</details>


### [57] [VK-LSVD: A Large-Scale Industrial Dataset for Short-Video Recommendation](https://arxiv.org/abs/2602.04567)
*Aleksandr Poslavsky,Alexander D'yakonov,Yuriy Dorn,Andrey Zimovnov*

Main category: cs.IR

TL;DR: 文章介绍了VK大短视频数据集（VK - LSVD），它是最大的公开工业数据集，可助力推荐系统研究。


<details>
  <summary>Details</summary>
Motivation: 短视推荐面临挑战，缺乏反映现实平台动态的大规模开放数据集，需要填补这一空白。

Method: 引入VK - LSVD数据集，分析其质量和多样性。

Result: 数据集规模空前，有超400亿交互数据，在2025年VK推荐系统挑战赛中发挥核心作用。

Conclusion: VK - LSVD为构建现实基准提供重要开放数据集，可加速序列推荐等研究。

Abstract: Short-video recommendation presents unique challenges, such as modeling rapid user interest shifts from implicit feedback, but progress is constrained by a lack of large-scale open datasets that reflect real-world platform dynamics. To bridge this gap, we introduce the VK Large Short-Video Dataset (VK-LSVD), the largest publicly available industrial dataset of its kind. VK-LSVD offers an unprecedented scale of over 40 billion interactions from 10 million users and almost 20 million videos over six months, alongside rich features including content embeddings, diverse feedback signals, and contextual metadata. Our analysis supports the dataset's quality and diversity. The dataset's immediate impact is confirmed by its central role in the live VK RecSys Challenge 2025. VK-LSVD provides a vital, open dataset to use in building realistic benchmarks to accelerate research in sequential recommendation, cold-start scenarios, and next-generation recommender systems.

</details>


### [58] [AIANO: Enhancing Information Retrieval with AI-Augmented Annotation](https://arxiv.org/abs/2602.04579)
*Sameh Khattab,Marie Bauer,Lukas Heine,Till Rostalski,Jens Kleesiek,Julian Friedrich*

Main category: cs.IR

TL;DR: 现有信息检索数据集标注工具复杂低效，本文开发AIANO工具，用户研究显示其能提升标注速度和检索准确性。


<details>
  <summary>Details</summary>
Motivation: 当前信息检索数据集创建使用现成标注工具，导致标注过程复杂低效，需开发新工具简化流程。

Method: 开发专门的标注工具AIANO，采用AI增强标注工作流，结合人类专业知识和大语言模型辅助。开展15人参与的受试者内用户研究。

Result: 与基线工具相比，AIANO标注速度近乎翻倍，更易用，且提高了检索准确性。

Conclusion: AIANO的AI增强方法能加速和提升信息检索任务的数据集创建，提升检索密集型领域的标注能力。

Abstract: The rise of Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) has rapidly increased the need for high-quality, curated information retrieval datasets. These datasets, however, are currently created with off-the-shelf annotation tools that make the annotation process complex and inefficient. To streamline this process, we developed a specialized annotation tool - AIANO. By adopting an AI-augmented annotation workflow that tightly integrates human expertise with LLM assistance, AIANO enables annotators to leverage AI suggestions while retaining full control over annotation decisions. In a within-subject user study ($n = 15$), participants created question-answering datasets using both a baseline tool and AIANO. AIANO nearly doubled annotation speed compared to the baseline while being easier to use and improving retrieval accuracy. These results demonstrate that AIANO's AI-augmented approach accelerates and enhances dataset creation for information retrieval tasks, advancing annotation capabilities in retrieval-intensive domains.

</details>


### [59] [Multi-Source Retrieval and Reasoning for Legal Sentencing Prediction](https://arxiv.org/abs/2602.04690)
*Junjie Chen,Haitao Li,Qilei Zhang,Zhenghua Li,Ya Zhang,Quan Zhou,Cheng Luo,Yiqun Liu,Dongsheng Guo,Qingyao Ai*

Main category: cs.IR

TL;DR: 提出$MSR^2$框架解决法律量刑预测难题，在两真实数据集实验显示可提升预测准确性与可解释性


<details>
  <summary>Details</summary>
Motivation: 现有方法在法律量刑预测（LSP）任务上表现不佳，因其需细粒度客观知识和灵活主观推理

Method: 提出$MSR^2$框架，集成多源检索和推理至大模型中，并结合强化学习，让大模型按需多源检索，运用过程级奖励引导中间主观推理步骤

Result: 在两个真实数据集上实验，$MSR^2$提升了LSP的准确性和可解释性

Conclusion: $MSR^2$为实用法律AI发展提供了有前景的方向

Abstract: Legal judgment prediction (LJP) aims to predict judicial outcomes from case facts and typically includes law article, charge, and sentencing prediction. While recent methods perform well on the first two subtasks, legal sentencing prediction (LSP) remains difficult due to its need for fine-grained objective knowledge and flexible subjective reasoning. To address these limitations, we propose $MSR^2$, a framework that integrates multi-source retrieval and reasoning in LLMs with reinforcement learning. $MSR^2$ enables LLMs to perform multi-source retrieval based on reasoning needs and applies a process-level reward to guide intermediate subjective reasoning steps. Experiments on two real-world datasets show that $MSR^2$ improves both accuracy and interpretability in LSP, providing a promising step toward practical legal AI. Our code is available at https://anonymous.4open.science/r/MSR2-FC3B.

</details>


### [60] [Addressing Corpus Knowledge Poisoning Attacks on RAG Using Sparse Attention](https://arxiv.org/abs/2602.04711)
*Sagie Dekel,Moshe Tennenholtz,Oren Kurland*

Main category: cs.IR

TL;DR: 提出Sparse Document Attention RAG (SDAG)防御方法，改进RAG应对语料库知识中毒攻击，效果优于标准因果注意力机制，与现有方法结合表现更佳。


<details>
  <summary>Details</summary>
Motivation: RAG易受语料库知识中毒攻击，标准因果注意力机制会导致有害的跨文档交互。

Method: 引入SDAG，一种块稀疏注意力机制，禁止检索文档间的交叉注意力，仅对注意力掩码做最小推理时间更改。

Result: SDAG在攻击成功率上大幅优于标准因果注意力机制，与现有RAG防御方法结合性能显著提升。

Conclusion: SDAG是一种有效的RAG防御方法，能提升模型应对攻击的能力，与现有方法结合可进一步提高性能。

Abstract: Retrieval Augmented Generation (RAG) is a highly effective paradigm for keeping LLM-based responses up-to-date and reducing the likelihood of hallucinations. Yet, RAG was recently shown to be quite vulnerable to corpus knowledge poisoning: an attacker injects misleading documents to the corpus to steer an LLMs' output to an undesired response. We argue that the standard causal attention mechanism in LLMs enables harmful cross-document interactions, specifically in cases of attacks. Accordingly, we introduce a novel defense approach for RAG: Sparse Document Attention RAG (SDAG). This is a block-sparse attention mechanism that disallows cross-attention between retrieved documents. SDAG requires a minimal inference-time change to the attention mask; furthermore, no fine-tuning or additional architectural changes are needed. We present an empirical evaluation of LLM-based question answering (QA) with a variety of attack strategies on RAG. We show that our SDAG method substantially outperforms the standard causal attention mechanism in terms of attack success rate. We further demonstrate the clear merits of integrating SDAG with state-of-the-art RAG defense methods. Specifically, the integration results in performance that is statistically significantly better than the state-of-the-art.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [61] [Understanding the Impact of Differentially Private Training on Memorization of Long-Tailed Data](https://arxiv.org/abs/2602.03872)
*Jiaming Zhang,Huanyi Xie,Meng Ding,Shaopeng Fu,Jinyan Liu,Di Wang*

Main category: cs.LG

TL;DR: 本文从特征学习角度，为长尾数据上的DP - SGD算法构建了首个理论框架，表明该算法在长尾子群体上测试误差大，并通过实验验证理论。


<details>
  <summary>Details</summary>
Motivation: 现代深度学习模型记忆训练样本引发隐私问题，促使DP - SGD算法广泛应用，但它在长尾数据上泛化性能不佳且缺乏理论解释，现有差异隐私分析难用于实际的非凸非平滑神经网络。

Method: 从特征学习角度为DP - SGD构建理论框架以分析其在长尾数据上的表现。

Result: DP - SGD训练的模型在长尾子群体上的测试误差显著大于整个数据集的总体测试误差，且分析了训练动态，表明梯度裁剪和噪声注入联合影响模型记忆信息样本的能力。

Conclusion: 通过理论分析和实验验证展示了DP - SGD在长尾数据上的局限性。

Abstract: Recent research shows that modern deep learning models achieve high predictive accuracy partly by memorizing individual training samples. Such memorization raises serious privacy concerns, motivating the widespread adoption of differentially private training algorithms such as DP-SGD. However, a growing body of empirical work shows that DP-SGD often leads to suboptimal generalization performance, particularly on long-tailed data that contain a large number of rare or atypical samples. Despite these observations, a theoretical understanding of this phenomenon remains largely unexplored, and existing differential privacy analysis are difficult to extend to the nonconvex and nonsmooth neural networks commonly used in practice. In this work, we develop the first theoretical framework for analyzing DP-SGD on long-tailed data from a feature learning perspective. We show that the test error of DP-SGD-trained models on the long-tailed subpopulation is significantly larger than the overall test error over the entire dataset. Our analysis further characterizes the training dynamics of DP-SGD, demonstrating how gradient clipping and noise injection jointly adversely affect the model's ability to memorize informative but underrepresented samples. Finally, we validate our theoretical findings through extensive experiments on both synthetic and real-world datasets.

</details>


### [62] [Scalable Explainability-as-a-Service (XaaS) for Edge AI Systems](https://arxiv.org/abs/2602.04120)
*Samaresh Kumar Singh,Joyjit Roy*

Main category: cs.LG

TL;DR: 提出Explainability - as - a - Service (XaaS)架构解决XAI在边缘和物联网系统应用问题，经实验验证可降低延迟并保证解释质量。


<details>
  <summary>Details</summary>
Motivation: 现有XAI在边缘和物联网系统的应用是临时且低效的，当前方法存在冗余计算、高延迟和可扩展性差的问题。

Method: 提出XaaS分布式架构，将推理和解释生成解耦，还引入分布式解释缓存、轻量级验证协议和自适应解释引擎。

Result: 在三个实际边缘AI用例中进行评估，XaaS可降低38%的延迟，且在三个实际部署中保持高解释质量。

Conclusion: 该工作使透明和可问责的AI能在大规模、异构的物联网系统中部署，弥合了XAI研究与边缘实用性之间的差距。

Abstract: Though Explainable AI (XAI) has made significant advancements, its inclusion in edge and IoT systems is typically ad-hoc and inefficient. Most current methods are "coupled" in such a way that they generate explanations simultaneously with model inferences. As a result, these approaches incur redundant computation, high latency and poor scalability when deployed across heterogeneous sets of edge devices. In this work we propose Explainability-as-a-Service (XaaS), a distributed architecture for treating explainability as a first-class system service (as opposed to a model-specific feature). The key innovation in our proposed XaaS architecture is that it decouples inference from explanation generation allowing edge devices to request, cache and verify explanations subject to resource and latency constraints. To achieve this, we introduce three main innovations: (1) A distributed explanation cache with a semantic similarity based explanation retrieval method which significantly reduces redundant computation; (2) A lightweight verification protocol that ensures the fidelity of both cached and newly generated explanations; and (3) An adaptive explanation engine that chooses explanation methods based upon device capability and user requirement. We evaluated the performance of XaaS on three real-world edge-AI use cases: (i) manufacturing quality control; (ii) autonomous vehicle perception; and (iii) healthcare diagnostics. Experimental results show that XaaS reduces latency by 38\% while maintaining high explanation quality across three real-world deployments. Overall, this work enables the deployment of transparent and accountable AI across large scale, heterogeneous IoT systems, and bridges the gap between XAI research and edge-practicality.

</details>


### [63] [Reversible Deep Learning for 13C NMR in Chemoinformatics: On Structures and Spectra](https://arxiv.org/abs/2602.03875)
*Stefan Kuhn,Vandana Dwarka,Przemyslaw Karol Grenda,Eero Vainikko*

Main category: cs.LG

TL;DR: 提出可逆深度学习模型用于13C NMR，统一谱图预测和结构候选生成。


<details>
  <summary>Details</summary>
Motivation: 构建能在分子结构和谱图间双向转换的模型，统一谱图预测和结构候选生成。

Method: 使用基于i - RevNet风格双射块的条件可逆神经网络，训练模型从图结构编码预测谱图代码，推理时反转网络生成结构候选。

Result: 模型在训练样本上数值可逆，谱图代码预测超随机水平，在验证谱图上反转能产生有意义的结构信号。

Conclusion: 可逆架构可在端到端模型中统一谱图预测和不确定性感知的候选生成。

Abstract: We introduce a reversible deep learning model for 13C NMR that uses a single conditional invertible neural network for both directions between molecular structures and spectra. The network is built from i-RevNet style bijective blocks, so the forward map and its inverse are available by construction. We train the model to predict a 128-bit binned spectrum code from a graph-based structure encoding, while the remaining latent dimensions capture residual variability. At inference time, we invert the same trained network to generate structure candidates from a spectrum code, which explicitly represents the one-to-many nature of spectrum-to-structure inference. On a filtered subset, the model is numerically invertible on trained examples, achieves spectrum-code prediction above chance, and produces coarse but meaningful structural signals when inverted on validation spectra. These results demonstrate that invertible architectures can unify spectrum prediction and uncertainty-aware candidate generation within one end-to-end model.

</details>


### [64] [GOPO: Policy Optimization using Ranked Rewards](https://arxiv.org/abs/2602.03876)
*Kyuseong Choi,Dwaipayan Saha,Woojeong Kim,Anish Agarwal,Raaz Dwivedi*

Main category: cs.LG

TL;DR: 现有基于人类反馈的强化学习（RLHF）中策略优化技术依赖奖励绝对值，在不可验证奖励场景存在问题，本文提出GOPO方法，仅用奖励排名优化策略，相比GRPO有多项优势，在多任务和模型规模上表现更好。


<details>
  <summary>Details</summary>
Motivation: 现有基于人类反馈的强化学习中，策略优化技术依赖奖励绝对值，在不可验证奖励场景如摘要、指令跟随和聊天完成中，会导致性能不佳。

Method: 提出Group Ordinal Policy Optimization (GOPO) 方法，仅使用奖励排名进行策略优化，丢弃奖励的绝对值。

Result: 与Group Relative Policy Optimization (GRPO) 相比，在不可验证奖励场景下，GOPO有以下优势：训练/验证奖励轨迹持续更高；在大多数中间训练步骤中，基于大语言模型的评估结果更好；在更少的训练步骤内达到与GRPO相当的策略质量。

Conclusion: GOPO方法在一系列任务和模型规模上都有持续的改进。

Abstract: Standard reinforcement learning from human feedback (RLHF) trains a reward model on pairwise preference data and then uses it for policy optimization. However, while reward models are optimized to capture relative preferences, existing policy optimization techniques rely on absolute reward magnitudes during training. In settings where the rewards are non-verifiable such as summarization, instruction following, and chat completion, this misalignment often leads to suboptimal performance. We introduce Group Ordinal Policy Optimization (GOPO), a policy optimization method that uses only the ranking of the rewards and discards their magnitudes. Our rank-based transformation of rewards provides several gains, compared to Group Relative Policy Optimization (GRPO), in settings with non-verifiable rewards: (1) consistently higher training/validation reward trajectories, (2) improved LLM-as-judge evaluations across most intermediate training steps, and (3) reaching a policy of comparable quality in substantially less training steps than GRPO. We demonstrate consistent improvements across a range of tasks and model sizes.

</details>


### [65] [NeuroPareto: Calibrated Acquisition for Costly Many-Goal Search in Vast Parameter Spaces](https://arxiv.org/abs/2602.03901)
*Rong Fu,Wenxin Zhang,Chunlei Meng,Youjin Wang,Haoyu Zhao,Jiaxuan Lu,Kun Liu,JiaBao Dou,Simon James Fong*

Main category: cs.LG

TL;DR: 提出NeuroPareto架构解决高维搜索空间多目标优化问题，实验显示其性能优于基线方法


<details>
  <summary>Details</summary>
Motivation: 在严格计算约束下，高维搜索空间的多目标优化追求最优权衡存在挑战

Method: 开发NeuroPareto架构，集成排名过滤、不确定性分解和历史条件获取策略，使用校准贝叶斯分类器估计不确定性，用深度高斯过程代理分离不确定性，通过在线训练的轻量级获取网络指导评估，并采用分层筛选和代理更新

Result: 在DTLZ和ZDT套件及地下能源提取任务实验中，NeuroPareto在Pareto接近度和超体积方面始终优于基线方法

Conclusion: NeuroPareto能在保持较低计算开销的同时保证准确性，有效应对高维多目标优化问题

Abstract: The pursuit of optimal trade-offs in high-dimensional search spaces under stringent computational constraints poses a fundamental challenge for contemporary multi-objective optimization. We develop NeuroPareto, a cohesive architecture that integrates rank-centric filtering, uncertainty disentanglement, and history-conditioned acquisition strategies to navigate complex objective landscapes. A calibrated Bayesian classifier estimates epistemic uncertainty across non-domination tiers, enabling rapid generation of high-quality candidates with minimal evaluation cost. Deep Gaussian Process surrogates further separate predictive uncertainty into reducible and irreducible components, providing refined predictive means and risk-aware signals for downstream selection. A lightweight acquisition network, trained online from historical hypervolume improvements, guides expensive evaluations toward regions balancing convergence and diversity. With hierarchical screening and amortized surrogate updates, the method maintains accuracy while keeping computational overhead low. Experiments on DTLZ and ZDT suites and a subsurface energy extraction task show that NeuroPareto consistently outperforms classifier-enhanced and surrogate-assisted baselines in Pareto proximity and hypervolume.

</details>


### [66] [GeoIB: Geometry-Aware Information Bottleneck via Statistical-Manifold Compression](https://arxiv.org/abs/2602.03906)
*Weiqi Wang,Zhiyi Tian,Chenhan Zhang,Shui Yu*

Main category: cs.LG

TL;DR: 本文提出几何信息瓶颈（GeoIB）方法，无需互信息估计，在信息平面上实现预测精度和压缩比的更好权衡，提高了不变性和优化稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习中信息瓶颈（IB）通过替代方法实现，存在松散性和估计偏差，导致压缩间接控制和优化脆弱。

Method: 从信息几何角度重新审视IB问题，提出GeoIB，用分布级Fisher - Rao差异和几何级Jacobian - Frobenius项控制信息压缩，推导与FR度量一致的自然梯度优化器。

Result: 在流行数据集上，GeoIB比主流IB基线在信息平面上实现了预测精度和压缩比的更好权衡。

Conclusion: GeoIB通过统一分布和几何正则化，提高了不变性和优化稳定性。

Abstract: Information Bottleneck (IB) is widely used, but in deep learning, it is usually implemented through tractable surrogates, such as variational bounds or neural mutual information (MI) estimators, rather than directly controlling the MI I(X;Z) itself. The looseness and estimator-dependent bias can make IB "compression" only indirectly controlled and optimization fragile.
  We revisit the IB problem through the lens of information geometry and propose a \textbf{Geo}metric \textbf{I}nformation \textbf{B}ottleneck (\textbf{GeoIB}) that dispenses with mutual information (MI) estimation. We show that I(X;Z) and I(Z;Y) admit exact projection forms as minimal Kullback-Leibler (KL) distances from the joint distributions to their respective independence manifolds. Guided by this view, GeoIB controls information compression with two complementary terms: (i) a distribution-level Fisher-Rao (FR) discrepancy, which matches KL to second order and is reparameterization-invariant; and (ii) a geometry-level Jacobian-Frobenius (JF) term that provides a local capacity-type upper bound on I(Z;X) by penalizing pullback volume expansion of the encoder. We further derive a natural-gradient optimizer consistent with the FR metric and prove that the standard additive natural-gradient step is first-order equivalent to the geodesic update. We conducted extensive experiments and observed that the GeoIB achieves a better trade-off between prediction accuracy and compression ratio in the information plane than the mainstream IB baselines on popular datasets. GeoIB improves invariance and optimization stability by unifying distributional and geometric regularization under a single bottleneck multiplier. The source code of GeoIB is released at "https://anonymous.4open.science/r/G-IB-0569".

</details>


### [67] [Non-linear PCA via Evolution Strategies: a Novel Objective Function](https://arxiv.org/abs/2602.03967)
*Thomas Uriot,Elise Chung*

Main category: cs.LG

TL;DR: 提出结合PCA可解释性与神经网络灵活性的非线性PCA框架，在合成和真实数据集上表现优于线性PCA和kPCA，且保留可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统PCA线性特性难以捕捉真实数据复杂结构，kPCA牺牲可解释性且超参数选择困难，需新方法结合二者优势。

Method: 用神经网络参数化变量变换，用进化策略优化以处理特征分解不可微问题，引入新的细粒度目标函数。

Result: 在合成和真实数据集上，该方法在解释方差方面显著优于线性PCA和kPCA。

Conclusion: 该方法结合了PCA可解释性和神经网络灵活性，代码可在GitHub获取。

Abstract: Principal Component Analysis (PCA) is a powerful and popular dimensionality reduction technique. However, due to its linear nature, it often fails to capture the complex underlying structure of real-world data. While Kernel PCA (kPCA) addresses non-linearity, it sacrifices interpretability and struggles with hyperparameter selection. In this paper, we propose a robust non-linear PCA framework that unifies the interpretability of PCA with the flexibility of neural networks. Our method parametrizes variable transformations via neural networks, optimized using Evolution Strategies (ES) to handle the non-differentiability of eigendecomposition. We introduce a novel, granular objective function that maximizes the individual variance contribution of each variable providing a stronger learning signal than global variance maximization. This approach natively handles categorical and ordinal variables without the dimensional explosion associated with one-hot encoding. We demonstrate that our method significantly outperforms both linear PCA and kPCA in explained variance across synthetic and real-world datasets. At the same time, it preserves PCA's interpretability, enabling visualization and analysis of feature contributions using standard tools such as biplots. The code can be found on GitHub.

</details>


### [68] [The Role of Target Update Frequencies in Q-Learning](https://arxiv.org/abs/2602.03911)
*Simon Weissmann,Tilman Aach,Benedikt Wille,Sebastian Kassing,Leif Döring*

Main category: cs.LG

TL;DR: 本文从近似动态规划角度对表格Q学习中的目标固定进行理论分析，得出目标更新周期偏差 - 方差权衡的明确特征，证明恒定目标更新策略次优，最优更新频率随学习过程几何增长。


<details>
  <summary>Details</summary>
Motivation: 当前目标网络更新频率（TUF）的选择缺乏理解，常被当作可调超参数而非有原则的设计决策。

Method: 将周期性目标更新表述为嵌套优化方案，对异步采样设置进行有限时间收敛分析，内循环采用随机梯度下降。

Result: 得到目标更新周期偏差 - 方差权衡的明确特征，证明恒定目标更新策略会带来对数级样本复杂度开销，而自适应策略可避免。

Conclusion: 最优目标更新频率应随学习过程几何增长。

Abstract: The target network update frequency (TUF) is a central stabilization mechanism in (deep) Q-learning. However, their selection remains poorly understood and is often treated merely as another tunable hyperparameter rather than as a principled design decision. This work provides a theoretical analysis of target fixing in tabular Q-learning through the lens of approximate dynamic programming. We formulate periodic target updates as a nested optimization scheme in which each outer iteration applies an inexact Bellman optimality operator, approximated by a generic inner loop optimizer. Rigorous theory yields a finite-time convergence analysis for the asynchronous sampling setting, specializing to stochastic gradient descent in the inner loop. Our results deliver an explicit characterization of the bias-variance trade-off induced by the target update period, showing how to optimally set this critical hyperparameter. We prove that constant target update schedules are suboptimal, incurring a logarithmic overhead in sample complexity that is entirely avoidable with adaptive schedules. Our analysis shows that the optimal target update frequency increases geometrically over the course of the learning process.

</details>


### [69] [Echo State Networks for Time Series Forecasting: Hyperparameter Sweep and Benchmarking](https://arxiv.org/abs/2602.03912)
*Alexander Häußer*

Main category: cs.LG

TL;DR: 本文用M4竞赛部分数据研究回声状态网络（ESNs）对单变量时间序列的预测性能，采用两阶段评估，结果表明ESNs在预测准确性、鲁棒性和计算效率间取得平衡，是自动时间序列预测的实用选择。


<details>
  <summary>Details</summary>
Motivation: 评估全自动、纯反馈驱动的ESNs能否成为常用统计预测方法的有竞争力替代方案。

Method: 采用两阶段评估法，先用参数数据集进行超参数搜索，再用预测数据集进行样本外准确性评估，用MASE和sMAPE衡量准确性并与多个基准模型对比。

Result: 超参数分析有一致且可解释的模式，样本外评估中ESNs对月度数据表现与ARIMA和TBATS相当，对季度数据MASE最低，且计算成本低于复杂统计模型。

Conclusion: ESNs在预测准确性、鲁棒性和计算效率间取得平衡，是自动时间序列预测的实用选择。

Abstract: This paper investigates the forecasting performance of Echo State Networks (ESNs) for univariate time series forecasting using a subset of the M4 Forecasting Competition dataset. Focusing on monthly and quarterly time series with at most 20 years of historical data, we evaluate whether a fully automatic, purely feedback-driven ESN can serve as a competitive alternative to widely used statistical forecasting methods. The study adopts a rigorous two-stage evaluation approach: a Parameter dataset is used to conduct an extensive hyperparameter sweep covering leakage rate, spectral radius, reservoir size, and information criteria for regularization, resulting in over four million ESN model fits; a disjoint Forecast dataset is then used for out-of-sample accuracy assessment. Forecast accuracy is measured using MASE and sMAPE and benchmarked against simple benchmarks like drift and seasonal naive and statistical models like ARIMA, ETS, and TBATS. The hyperparameter analysis reveals consistent and interpretable patterns, with monthly series favoring moderately persistent reservoirs and quarterly series favoring more contractive dynamics. Across both frequencies, high leakage rates are preferred, while optimal spectral radii and reservoir sizes vary with temporal resolution. In the out-of-sample evaluation, the ESN performs on par with ARIMA and TBATS for monthly data and achieves the lowest mean MASE for quarterly data, while requiring lower computational cost than the more complex statistical models. Overall, the results demonstrate that ESNs offer a compelling balance between predictive accuracy, robustness, and computational efficiency, positioning them as a practical option for automated time series forecasting.

</details>


### [70] [Benchmarking Uncertainty Quantification of Plug-and-Play Diffusion Priors for Inverse Problems Solving](https://arxiv.org/abs/2602.04189)
*Xiaoyu Qiu,Taewon Yang,Zhanhao Liu,Guanyang Wang,Liyue Shen*

Main category: cs.LG

TL;DR: 文章指出当前对即插即用扩散先验（PnPDP）的重建质量评估存在问题，开展系统性研究对现有扩散逆求解器的不确定性量化（UQ）进行基准测试，提供新见解。


<details>
  <summary>Details</summary>
Motivation: 当前对PnPDP重建质量评估强调单个样本的点估计准确性指标，未反映求解器随机性质和逆问题内在不确定性，而逆问题期望输出是后验分布，现有基准仅评估单个重建，忽略不确定性等分布特征，存在根本性不匹配。

Method: 设计严谨的玩具模型模拟来评估各种PnPDP求解器的不确定性行为，并提出基于UQ的分类。

Result: 在玩具模拟和多样现实世界科学逆问题的广泛实验中，观察到的不确定性行为与分类和理论依据一致。

Conclusion: 为评估和理解PnPDP的不确定性提供了新见解。

Abstract: Plug-and-play diffusion priors (PnPDP) have become a powerful paradigm for solving inverse problems in scientific and engineering domains. Yet, current evaluations of reconstruction quality emphasize point-estimate accuracy metrics on a single sample, which do not reflect the stochastic nature of PnPDP solvers and the intrinsic uncertainty of inverse problems, critical for scientific tasks. This creates a fundamental mismatch: in inverse problems, the desired output is typically a posterior distribution and most PnPDP solvers induce a distribution over reconstructions, but existing benchmarks only evaluate a single reconstruction, ignoring distributional characterization such as uncertainty. To address this gap, we conduct a systematic study to benchmark the uncertainty quantification (UQ) of existing diffusion inverse solvers. Specifically, we design a rigorous toy model simulation to evaluate the uncertainty behavior of various PnPDP solvers, and propose a UQ-driven categorization. Through extensive experiments on toy simulations and diverse real-world scientific inverse problems, we observe uncertainty behaviors consistent with our taxonomy and theoretical justification, providing new insights for evaluating and understanding the uncertainty for PnPDPs.

</details>


### [71] [An Empirical Survey and Benchmark of Learned Distance Indexes for Road Networks](https://arxiv.org/abs/2602.04068)
*Gautam Choudhary,Libin Zhou,Yeasir Rayhan,Walid G. Aref*

Main category: cs.LG

TL;DR: 本文首次对道路网络中基于机器学习的距离索引进行实证调查，评估其训练时间、查询延迟、存储和准确性，对比经典非机器学习方法，还发布开源代码库。


<details>
  <summary>Details</summary>
Motivation: 现有基于机器学习的距离索引缺乏全面系统评估，需要对其进行评估以了解性能。

Method: 沿训练时间、查询延迟、存储和准确性四个关键维度，使用七个真实世界道路网络和基于轨迹数据的工作负载驱动查询数据集，对十种代表性机器学习技术进行基准测试，并与经典非机器学习基线对比。

Result: 对基于机器学习的距离索引进行了全面评估，突出了关键见解和实际权衡。

Conclusion: 通过实证研究，为基于机器学习的距离索引提供了评估，发布的开源代码库支持可重复性和未来研究。

Abstract: The calculation of shortest-path distances in road networks is a core operation in navigation systems, location-based services, and spatial analytics. Although classical algorithms, e.g., Dijkstra's algorithm, provide exact answers, their latency is prohibitive for modern real-time, large-scale deployments. Over the past two decades, numerous distance indexes have been proposed to speed up query processing for shortest distance queries. More recently, with the advancement in machine learning (ML), researchers have designed and proposed ML-based distance indexes to answer approximate shortest path and distance queries efficiently. However, a comprehensive and systematic evaluation of these ML-based approaches is lacking. This paper presents the first empirical survey of ML-based distance indexes on road networks, evaluating them along four key dimensions: Training time, query latency, storage, and accuracy. Using seven real-world road networks and workload-driven query datasets derived from trajectory data, we benchmark ten representative ML techniques and compare them against strong classical non-ML baselines, highlighting key insights and practical trade-offs. We release a unified open-source codebase to support reproducibility and future research on learned distance indexes.

</details>


### [72] [It's not a Lottery, it's a Race: Understanding How Gradient Descent Adapts the Network's Capacity to the Task](https://arxiv.org/abs/2602.04832)
*Hannah Pinson*

Main category: cs.LG

TL;DR: 研究梯度下降如何降低神经网络理论容量至有效容量，提出三个动力学原理并解释彩票假说机制。


<details>
  <summary>Details</summary>
Motivation: 神经网络理论理解滞后，解释梯度下降训练中网络理论容量降至有效容量的原因和方式。

Method: 分析单隐藏层ReLU网络中单个神经元的学习动态。

Result: 确定了相互对齐、解锁和竞争三个动力学原理，解释训练后降低容量的原因及彩票假说机制。

Conclusion: 通过三个动力学原理可解释梯度下降降低网络容量及彩票假说。

Abstract: Our theoretical understanding of neural networks is lagging behind their empirical success. One of the important unexplained phenomena is why and how, during the process of training with gradient descent, the theoretical capacity of neural networks is reduced to an effective capacity that fits the task. We here investigate the mechanism by which gradient descent achieves this through analyzing the learning dynamics at the level of individual neurons in single hidden layer ReLU networks. We identify three dynamical principles -- mutual alignment, unlocking and racing -- that together explain why we can often successfully reduce capacity after training through the merging of equivalent neurons or the pruning of low norm weights. We specifically explain the mechanism behind the lottery ticket conjecture, or why the specific, beneficial initial conditions of some neurons lead them to obtain higher weight norms.

</details>


### [73] [Causal Discovery for Cross-Sectional Data Based on Super-Structure and Divide-and-Conquer](https://arxiv.org/abs/2602.03914)
*Wenyu Wang,Yaping Wan*

Main category: cs.LG

TL;DR: 提出轻量级框架解决超结构因果发现计算成本问题，实验证明可降低条件独立性测试开销并保证精度


<details>
  <summary>Details</summary>
Motivation: 解决基于超结构分治法因果发现中构建精确超结构计算成本高的瓶颈，尤其是条件独立性测试成本高且无领域知识的情况

Method: 提出轻量级框架，整合弱约束超结构与高效图划分和合并策略，并用具体算法实现该框架

Result: 在合成数据、高斯贝叶斯网络和真实数据集上实验表明，方法在保证结构精度的同时大幅减少条件独立性测试次数

Conclusion: 即使对初始超结构假设极少，也能实现准确、可扩展的因果发现，为分治法在大规模、知识匮乏领域应用开辟新途径

Abstract: This paper tackles a critical bottleneck in Super-Structure-based divide-and-conquer causal discovery: the high computational cost of constructing accurate Super-Structures--particularly when conditional independence (CI) tests are expensive and domain knowledge is unavailable. We propose a novel, lightweight framework that relaxes the strict requirements on Super-Structure construction while preserving the algorithmic benefits of divide-and-conquer. By integrating weakly constrained Super-Structures with efficient graph partitioning and merging strategies, our approach substantially lowers CI test overhead without sacrificing accuracy. We instantiate the framework in a concrete causal discovery algorithm and rigorously evaluate its components on synthetic data. Comprehensive experiments on Gaussian Bayesian networks, including magic-NIAB, ECOLI70, and magic-IRRI, demonstrate that our method matches or closely approximates the structural accuracy of PC and FCI while drastically reducing the number of CI tests. Further validation on the real-world China Health and Retirement Longitudinal Study (CHARLS) dataset confirms its practical applicability. Our results establish that accurate, scalable causal discovery is achievable even under minimal assumptions about the initial Super-Structure, opening new avenues for applying divide-and-conquer methods to large-scale, knowledge-scarce domains such as biomedical and social science research.

</details>


### [74] [SpecMD: A Comprehensive Study On Speculative Expert Prefetching](https://arxiv.org/abs/2602.03921)
*Duc Hoang,Ajay Jaiswal,Mohammad Samragh,Minsik Cho*

Main category: cs.LG

TL;DR: 本文开发SpecMD框架对MoE缓存策略进行基准测试，发现MoE专家访问不符合时间局部性假设，提出Least - Stale策略减少冲突缺失，提高命中率并降低TTFT。


<details>
  <summary>Details</summary>
Motivation: 以往硬件中心缓存策略间的交互及与不同硬件规格的适配情况不明，需要填补这一研究空白。

Method: 开发SpecMD框架进行基准测试，提出Least - Stale新的驱逐策略。

Result: MoE专家访问不符合时间局部性假设；Least - Stale策略相比LRU减少碰撞缺失达85倍；在5%（0.6GB）VRAM缓存容量下，OLMoE命中率超88%，TTFT降低34.7%。

Conclusion: Least - Stale策略能有效提升MoE模型的缓存性能，减少冲突缺失和TTFT。

Abstract: Mixture-of-Experts (MoE) models enable sparse expert activation, meaning that only a subset of the model's parameters is used during each inference. However, to translate this sparsity into practical performance, an expert caching mechanism is required. Previous works have proposed hardware-centric caching policies, but how these various caching policies interact with each other and different hardware specification remains poorly understood. To address this gap, we develop \textbf{SpecMD}, a standardized framework for benchmarking ad-hoc cache policies on various hardware configurations. Using SpecMD, we perform an exhaustive benchmarking of several MoE caching strategies, reproducing and extending prior approaches in controlled settings with realistic constraints. Our experiments reveal that MoE expert access is not consistent with temporal locality assumptions (e.g LRU, LFU). Motivated by this observation, we propose \textbf{Least-Stale}, a novel eviction policy that exploits MoE's predictable expert access patterns to reduce collision misses by up to $85\times$ over LRU. With such gains, we achieve over $88\%$ hit rates with up to $34.7\%$ Time-to-first-token (TTFT) reduction on OLMoE at only $5\%$ or $0.6GB$ of VRAM cache capacity.

</details>


### [75] [MaMa: A Game-Theoretic Approach for Designing Safe Agentic Systems](https://arxiv.org/abs/2602.04431)
*Jonathan Nöther,Adish Singla,Goran Radanovic*

Main category: cs.LG

TL;DR: 研究基于LLM的多智能体系统在部分智能体受损时的安全设计，提出MaMa算法，实验显示该算法设计的系统能抵御最坏情况攻击并具鲁棒性。


<details>
  <summary>Details</summary>
Motivation: LLM-based多智能体系统个体智能体故障或恶意行为会带来重大安全风险，需设计部分智能体受损时仍安全的系统。

Method: 将问题形式化为Stackelberg安全博弈，提出MaMa算法，采用LLM-based对抗搜索，元智能体迭代提出设计并根据元攻击者的最强攻击接收反馈。

Result: MaMa设计的系统能抵御最坏情况攻击，性能与单纯优化任务成功的系统相当，且能泛化到更强攻击者及不同攻击目标或底层LLM的情况。

Conclusion: MaMa算法可自动设计安全的智能体系统，具备鲁棒的安全性。

Abstract: LLM-based multi-agent systems have demonstrated impressive capabilities, but they also introduce significant safety risks when individual agents fail or behave adversarially. In this work, we study the automated design of agentic systems that remain safe even when a subset of agents is compromised. We formalize this challenge as a Stackelberg security game between a system designer (the Meta-Agent) and a best-responding Meta-Adversary that selects and compromises a subset of agents to minimize safety. We propose Meta-Adversary-Meta-Agent (MaMa), a novel algorithm for approximately solving this game and automatically designing safe agentic systems. Our approach uses LLM-based adversarial search, where the Meta-Agent iteratively proposes system designs and receives feedback based on the strongest attacks discovered by the Meta-Adversary. Empirical evaluations across diverse environments show that systems designed with MaMa consistently defend against worst-case attacks while maintaining performance comparable to systems optimized solely for task success. Moreover, the resulting systems generalize to stronger adversaries, as well as ones with different attack objectives or underlying LLMs, demonstrating robust safety beyond the training setting.

</details>


### [76] [Online Vector Quantized Attention](https://arxiv.org/abs/2602.03922)
*Nick Alonso,Tomas Figliolia,Beren Millidge*

Main category: cs.LG

TL;DR: 论文提出在线向量量化（OVQ）注意力机制，在内存计算成本和长上下文处理间取得平衡，测试效果良好。


<details>
  <summary>Details</summary>
Motivation: 标准序列混合层难以平衡效率和性能，自注意力计算成本高，线性注意力和SSMs长上下文处理能力弱，需寻找更好方案。

Method: 基于高斯混合回归为OVQ - 注意力建立理论基础，在多种合成长上下文任务和长上下文语言建模中测试。

Result: OVQ - 注意力相比线性注意力基线和原始VQ - 注意力有显著提升，与强自注意力基线性能相当甚至相同，且内存使用少。

Conclusion: OVQ - 注意力能在内存计算成本和长上下文处理间实现较好妥协。

Abstract: Standard sequence mixing layers used in language models struggle to balance efficiency and performance. Self-attention performs well on long context tasks but has expensive quadratic compute and linear memory costs, while linear attention and SSMs use only linear compute and constant memory but struggle with long context processing. In this paper, we develop a sequence mixing layer that aims to find a better compromise between memory-compute costs and long-context processing, which we call online vector-quantized (OVQ) attention. OVQ-attention requires linear compute costs and constant memory, but, unlike linear attention and SSMs, it uses a sparse memory update that allows it to greatly increase the size of its memory state and, consequently, memory capacity. We develop a theoretical basis for OVQ-attention based on Gaussian mixture regression, and we test it on a variety of synthetic long context tasks and on long context language modeling. OVQ-attention shows significant improvements over linear attention baselines and the original VQ-attention, on which OVQ-attention was inspired. It demonstrates competitive, and sometimes identical, performance to strong self-attention baselines up 64k sequence length, despite using a small fraction of the memory of full self-attention.

</details>


### [77] [WIND: Weather Inverse Diffusion for Zero-Shot Atmospheric Modeling](https://arxiv.org/abs/2602.03924)
*Michael Aich,Andreas Fürst,Florian Sestak,Carlos Ruiz-Gonzalez,Niklas Boers,Johannes Brandstetter*

Main category: cs.LG

TL;DR: 介绍了预训练基础模型WIND，无需特定微调可完成多种气象和气候任务，结合生成视频建模与逆问题求解，实现计算高效范式转变。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习在气象和气候建模领域碎片化，各任务模型单独训练，需统一模型。

Method: 用自监督视频重建目标预训练WIND，利用无条件视频扩散模型迭代重建大气动力学，推理时将特定问题作为逆问题通过后验采样解决。

Result: 可解决多种气象和气候问题，能生成极端天气反事实情景。

Conclusion: WIND结合生成视频建模与逆问题求解，在基于AI的大气建模中实现计算高效范式转变。

Abstract: Deep learning has revolutionized weather and climate modeling, yet the current landscape remains fragmented: highly specialized models are typically trained individually for distinct tasks. To unify this landscape, we introduce WIND, a single pre-trained foundation model capable of replacing specialized baselines across a vast array of tasks. Crucially, in contrast to previous atmospheric foundation models, we achieve this without any task-specific fine-tuning. To learn a robust, task-agnostic prior of the atmosphere, we pre-train WIND with a self-supervised video reconstruction objective, utilizing an unconditional video diffusion model to iteratively reconstruct atmospheric dynamics from a noisy state. At inference, we frame diverse domain-specific problems strictly as inverse problems and solve them via posterior sampling. This unified approach allows us to tackle highly relevant weather and climate problems, including probabilistic forecasting, spatial and temporal downscaling, sparse reconstruction and enforcing conservation laws purely with our pre-trained model. We further demonstrate the model's capacity to generate physically consistent counterfactual storylines of extreme weather events under global warming scenarios. By combining generative video modeling with inverse problem solving, WIND offers a computationally efficient paradigm shift in AI-based atmospheric modeling.

</details>


### [78] [Autonomous AI Agents for Real-Time Affordable Housing Site Selection: Multi-Objective Reinforcement Learning Under Regulatory Constraints](https://arxiv.org/abs/2602.03940)
*Olaf Yunus Laitinen Imanov,Duygu Erisken,Derya Umut Kulali,Taner Yilmaz,Rana Irem Turhan*

Main category: cs.LG

TL;DR: 提出AURA系统用于实时经济适用房选址，在多方面表现佳。


<details>
  <summary>Details</summary>
Motivation: 经济适用房短缺，土地稀缺和法规使选址缓慢。

Method: 建立AURA系统，将任务建模为受限多目标马尔可夫决策过程，采用监管感知状态编码、帕累托约束策略梯度和奖励分解等方法。

Result: 在8个美国城市数据集上，合规率达94.3%，帕累托超体积提升37.2%；纽约案例中，选址时间从18个月减至72小时，可行地点多23%，所选地点交通和环境指标更优。

Conclusion: AURA系统能有效解决经济适用房选址难题，提高效率和选址质量。

Abstract: Affordable housing shortages affect billions, while land scarcity and regulations make site selection slow. We present AURA (Autonomous Urban Resource Allocator), a hierarchical multi-agent reinforcement learning system for real-time affordable housing site selection under hard regulatory constraints (QCT, DDA, LIHTC). We model the task as a constrained multi-objective Markov decision process optimizing accessibility, environmental impact, construction cost, and social equity while enforcing feasibility. AURA uses a regulatory-aware state encoding 127 federal and local constraints, Pareto-constrained policy gradients with feasibility guarantees, and reward decomposition separating immediate costs from long-term social outcomes. On datasets from 8 U.S. metros (47,392 candidate parcels), AURA attains 94.3% regulatory compliance and improves Pareto hypervolume by 37.2% over strong baselines. In a New York City 2026 case study, it reduces selection time from 18 months to 72 hours and identifies 23% more viable sites; chosen sites have 31% better transit access and 19% lower environmental impact than expert picks.

</details>


### [79] [From Data to Behavior: Predicting Unintended Model Behaviors Before Training](https://arxiv.org/abs/2602.04735)
*Mengru Wang,Zhenqian Xu,Junfeng Fang,Yunzhi Yao,Shumin Deng,Huajun Chen,Ningyu Zhang*

Main category: cs.LG

TL;DR: 提出Data2Behavior任务和MDF方法预测大模型训练前意外行为，MDF轻量省资源且实验证实有效


<details>
  <summary>Details</summary>
Motivation: 现有方法难在微调前检测大模型从训练数据获意外偏差的风险，事后评估成本高、效率低

Method: 引入Data2Behavior任务和Manipulating Data Features (MDF)方法，MDF通过平均表示总结候选数据并注入基础模型前向传播，不更新参数揭示潜在偏差和安全风险

Result: MDF仅消耗微调约20%的GPU资源，在Qwen3 - 14B等模型实验中能预测意外行为，洞察预训练漏洞

Conclusion: MDF可在训练前可靠预测大模型意外行为，有资源优势和应用价值

Abstract: Large Language Models (LLMs) can acquire unintended biases from seemingly benign training data even without explicit cues or malicious content. Existing methods struggle to detect such risks before fine-tuning, making post hoc evaluation costly and inefficient. To address this challenge, we introduce Data2Behavior, a new task for predicting unintended model behaviors prior to training. We also propose Manipulating Data Features (MDF), a lightweight approach that summarizes candidate data through their mean representations and injects them into the forward pass of a base model, allowing latent statistical signals in the data to shape model activations and reveal potential biases and safety risks without updating any parameters. MDF achieves reliable prediction while consuming only about 20% of the GPU resources required for fine-tuning. Experiments on Qwen3-14B, Qwen2.5-32B-Instruct, and Gemma-3-12b-it confirm that MDF can anticipate unintended behaviors and provide insight into pre-training vulnerabilities.

</details>


### [80] [Grables: Tabular Learning Beyond Independent Rows](https://arxiv.org/abs/2602.03945)
*Tamara Cucumides,Floris Geerts*

Main category: cs.LG

TL;DR: 传统行式预测在表格学习有局限，引入grable接口，实验表明消息传递和混合方法有优势。


<details>
  <summary>Details</summary>
Motivation: 传统行式预测器在处理事务、时间和关系表时失效，无法处理依赖其他行的标签。

Method: 引入grable接口，分离表格转换为图的方式和在图上进行预测的方式。

Result: 实验证实消息传递能捕捉行间依赖，混合方法能持续提升效果。

Conclusion: 使用消息传递和明确提取行间结构的混合方法在表格学习中更有效。

Abstract: Tabular learning is still dominated by row-wise predictors that score each row independently, which fits i.i.d. benchmarks but fails on transactional, temporal, and relational tables where labels depend on other rows. We show that row-wise prediction rules out natural targets driven by global counts, overlaps, and relational patterns. To make "using structure" precise across architectures, we introduce grables: a modular interface that separates how a table is lifted to a graph (constructor) from how predictions are computed on that graph (node predictor), pinpointing where expressive power comes from. Experiments on synthetic tasks, transaction data, and a RelBench clinical-trials dataset confirm the predicted separations: message passing captures inter-row dependencies that row-local models miss, and hybrid approaches that explicitly extract inter-row structure and feed it to strong tabular learners yield consistent gains.

</details>


### [81] [Robust Generalizable Heterogeneous Legal Link Prediction](https://arxiv.org/abs/2602.04812)
*Lorenz Wendlinger,Simon Alexander Nonn,Abdullah Al Zubaer,Michael Granitzer*

Main category: cs.LG

TL;DR: 通过改进方法提升法律引文网络链接预测效果，降低错误率并拓展到新西兰数据，提高归纳迁移性。


<details>
  <summary>Details</summary>
Motivation: 改进现有在大型异构法律引文网络中应用的链接预测方法。

Method: 在学习中加入边丢弃和特征拼接，提出基于多语言节点特征和改进非对称解码器的方法。

Result: 降低错误率达45%，可将预测推广到新西兰数据，改进不同法律系统间的归纳迁移性。

Conclusion: 所提方法能有效改进法律引文网络的链接预测。

Abstract: Recent work has applied link prediction to large heterogeneous legal citation networks \new{with rich meta-features}. We find that this approach can be improved by including edge dropout and feature concatenation for the learning of more robust representations, which reduces error rates by up to 45%. We also propose an approach based on multilingual node features with an improved asymmetric decoder for compatibility, which allows us to generalize and extend the prediction to more, geographically and linguistically disjoint, data from New Zealand. Our adaptations also improve inductive transferability between these disjoint legal systems.

</details>


### [82] [Representation Geometry as a Diagnostic for Out-of-Distribution Robustness](https://arxiv.org/abs/2602.03951)
*Ali Zia,Farid Hazratian*

Main category: cs.LG

TL;DR: 提出基于几何的诊断框架，利用嵌入几何结构预测模型分布外鲁棒性，支持无监督检查点选择。


<details>
  <summary>Details</summary>
Motivation: 在无目标域标签时，分布偏移下模型鲁棒泛化难监测和优化，此前对嵌入几何结构能否提供鲁棒性事后信号了解少。

Method: 构建类条件互k近邻图，提取全局谱复杂度代理和局部平滑度测度。

Result: 较低谱复杂度和较高平均曲率能持续预测更强的分布外准确率，信号反映有意义的表征结构。

Conclusion: 表征几何能实现可解释、无标签的鲁棒性诊断，支持分布偏移下可靠的无监督检查点选择。

Abstract: Robust generalization under distribution shift remains difficult to monitor and optimize in the absence of target-domain labels, as models with similar in-distribution accuracy can exhibit markedly different out-of-distribution (OOD) performance. While prior work has focused on training-time regularization and low-order representation statistics, little is known about whether the geometric structure of learned embeddings provides reliable post-hoc signals of robustness. We propose a geometry-based diagnostic framework that constructs class-conditional mutual k-nearest-neighbor graphs from in-distribution embeddings and extracts two complementary invariants: a global spectral complexity proxy based on the reduced log-determinant of the normalized Laplacian, and a local smoothness measure based on Ollivier--Ricci curvature. Across multiple architectures, training regimes, and corruption benchmarks, we find that lower spectral complexity and higher mean curvature consistently predict stronger OOD accuracy across checkpoints. Controlled perturbations and topological analyses further show that these signals reflect meaningful representation structure rather than superficial embedding statistics. Our results demonstrate that representation geometry enables interpretable, label-free robustness diagnosis and supports reliable unsupervised checkpoint selection under distribution shift.

</details>


### [83] [Child Mortality Prediction in Bangladesh: A Decade-Long Validation Study](https://arxiv.org/abs/2602.03957)
*Md Muhtasim Munif Fahim,Md Rezaul Karim*

Main category: cs.LG

TL;DR: 本文用孟加拉国DHS数据训练儿童死亡率预测模型，发现基于遗传算法的神经网络架构优于XGBoost，且模型能识别最需干预区域，比梯度提升模型每年多识别约1300名高危儿童。


<details>
  <summary>Details</summary>
Motivation: 解决现有儿童死亡率预测机器学习模型因交叉验证随机化存在前瞻偏差、应用于未来人群时不准确的问题。

Method: 使用2011 - 2022年孟加拉国DHS数据，按2011 - 2014年训练、2017年验证、2022年测试划分数据，用基于遗传算法的神经架构搜索模型，并进行详细公平性审计。

Result: 单隐藏层（64单元）神经网络架构优于XGBoost（AUROC = 0.76 vs. 0.73; p < 0.01）；存在“社会经济预测梯度”，模型在最贫困地区表现最佳（AUC 0.74），在最富裕地区最差（AUC 0.66）；在10%水平筛查时比梯度提升模型每年多识别约1300名高危儿童。

Conclusion: 模型能识别最需干预区域，可用于有针对性的母婴健康干预。

Abstract: The predictive machine learning models for child mortality tend to be inaccurate when applied to future populations, since they suffer from look-ahead bias due to the randomization used in cross-validation. The Demographic and Health Surveys (DHS) data from Bangladesh for 2011-2022, with n = 33,962, are used in this paper. We trained the model on (2011-2014) data, validated it on 2017 data, and tested it on 2022 data. Eight years after the initial test of the model, a genetic algorithm-based Neural Architecture Search found a single-layer neural architecture (with 64 units) to be superior to XGBoost (AUROC = 0.76 vs. 0.73; p < 0.01). Additionally, through a detailed fairness audit, we identified an overall "Socioeconomic Predictive Gradient," with a positive correlation between regional poverty level (r = -0.62) and the algorithm's AUC. In addition, we found that the model performed at its highest levels in the least affluent divisions (AUC 0.74) and decreased dramatically in the wealthiest divisions (AUC 0.66). These findings suggest that the model is identifying areas with the greatest need for intervention. Our model would identify approximately 1300 additional at-risk children annually than a Gradient Boosting model when screened at the 10% level and validated using SHAP values and Platt Calibration, and therefore provide a robust, production-ready computational phenotype for targeted maternal and child health interventions.

</details>


### [84] [DeXposure-FM: A Time-series, Graph Foundation Model for Credit Exposures and Stability on Decentralized Financial Networks](https://arxiv.org/abs/2602.03981)
*Aijie Shu,Wenbin Wu,Gbenga Ibikunle,Fengxiang He*

Main category: cs.LG

TL;DR: 引入用于测量和预测DeFi网络协议间信用风险敞口的DeXposure - FM模型，在基准测试中表现出色，还能提供金融经济工具，模型和代码已开源。


<details>
  <summary>Details</summary>
Motivation: DeFi信用风险敞口存在隐含性和代币中介性，会引发连锁反应，且与传统金融联系增加，需要更强大的量化工具。

Method: 使用图 - 表格编码器和预训练权重初始化及多任务头，在DeXposure数据集上训练，用于预测协议级流量和信用风险敞口链接的拓扑与权重。

Result: 在两个机器学习基准测试中，DeXposure - FM始终优于现有方法。

Conclusion: DeXposure - FM能提供支持宏观审慎监测和压力测试的金融经济工具，且得到实证验证。

Abstract: Credit exposure in Decentralized Finance (DeFi) is often implicit and token-mediated, creating a dense web of inter-protocol dependencies. Thus, a shock to one token may result in significant and uncontrolled contagion effects. As the DeFi ecosystem becomes increasingly linked with traditional financial infrastructure through instruments, such as stablecoins, the risk posed by this dynamic demands more powerful quantification tools. We introduce DeXposure-FM, the first time-series, graph foundation model for measuring and forecasting inter-protocol credit exposure on DeFi networks, to the best of our knowledge. Employing a graph-tabular encoder, with pre-trained weight initialization, and multiple task-specific heads, DeXposure-FM is trained on the DeXposure dataset that has 43.7 million data entries, across 4,300+ protocols on 602 blockchains, covering 24,300+ unique tokens. The training is operationalized for credit-exposure forecasting, predicting the joint dynamics of (1) protocol-level flows, and (2) the topology and weights of credit-exposure links. The DeXposure-FM is empirically validated on two machine learning benchmarks; it consistently outperforms the state-of-the-art approaches, including a graph foundation model and temporal graph neural networks. DeXposure-FM further produces financial economics tools that support macroprudential monitoring and scenario-based DeFi stress testing, by enabling protocol-level systemic-importance scores, sector-level spillover and concentration measures via a forecast-then-measure pipeline. Empirical verification fully supports our financial economics tools. The model and code have been publicly available. Model: https://huggingface.co/EVIEHub/DeXposure-FM.
Code: https://github.com/EVIEHub/DeXposure-FM.

</details>


### [85] [eCP: Informative uncertainty quantification via Equivariantized Conformal Prediction with pre-trained models](https://arxiv.org/abs/2602.03986)
*Nikolaos Bousias,Lars Lindemann,George Pappas*

Main category: cs.LG

TL;DR: 研究预训练模型的群对称化对共形预测（CP）的影响，提出通过预训练预测器的群平均将几何信息融入CP以减轻不确定性，有理论结果并提出实验设计。


<details>
  <summary>Details</summary>
Motivation: CP在长程任务中不确定性区域显著增长，导致统计保证缺乏信息。

Method: 通过预训练预测器的群平均将几何信息融入CP，将每个样本视为轨道的代表。

Result: 所提方法可证明在递增凸序中产生收缩的非一致性分数，意味着改善的指数尾界和更尖锐的共形预测集。

Conclusion: 提出的方法能改善CP的性能，尤其是在高置信水平下，还提出在行人轨迹预测中测试理论的实验设计。

Abstract: We study the effect of group symmetrization of pre-trained models on conformal prediction (CP), a post-hoc, distribution-free, finite-sample method of uncertainty quantification that offers formal coverage guarantees under the assumption of data exchangeability. Unfortunately, CP uncertainty regions can grow significantly in long horizon missions, rendering the statistical guarantees uninformative. To that end, we propose infusing CP with geometric information via group-averaging of the pretrained predictor to distribute the non-conformity mass across the orbits. Each sample now is treated as a representative of an orbit, thus uncertainty can be mitigated by other samples entangled to it via the orbit inducing elements of the symmetry group. Our approach provably yields contracted non-conformity scores in increasing convex order, implying improved exponential-tail bounds and sharper conformal prediction sets in expectation, especially at high confidence levels. We then propose an experimental design to test these theoretical claims in pedestrian trajectory prediction.

</details>


### [86] [When Chains of Thought Don't Matter: Causal Bypass in Large Language Models](https://arxiv.org/abs/2602.03994)
*Anish Sathyanarayanan,Aditya Nagarsekar,Aarush Rathore*

Main category: cs.LG

TL;DR: 研究发现即使CoT提示在表面上符合要求，模型答案也常与CoT内容因果独立，并提出诊断框架评估此失效模式。


<details>
  <summary>Details</summary>
Motivation: 验证CoT提示能暴露模型推理过程并提高透明度这一假设。

Method: 通过惩罚不忠实推理；提出诊断框架，结合可解释行为模块和因果探针，利用隐藏状态修补测量CoT介导影响。

Result: 审计感知提示增加了可检测的操纵信号；因果探针显示任务依赖的中介作用，不同任务的CMI不同；层分析显示存在狭窄且任务依赖的“推理窗口”。

Conclusion: 即便CoT提示很好，模型答案也可能与CoT内容因果独立，需利用诊断框架评估。

Abstract: Chain-of-thought (CoT) prompting is widely assumed to expose a model's reasoning process and improve transparency. We attempted to enforce this assumption by penalizing unfaithful reasoning, but found that surface-level compliance does not guarantee causal reliance. Our central finding is negative: even when CoT is verbose, strategic, and flagged by surface-level manipulation detectors, model answers are often causally independent of the CoT content. We present a diagnostic framework for auditing this failure mode: it combines (i) an interpretable behavioral module that scores manipulation-relevant signals in CoT text and (ii) a causal probe that measures CoT-mediated influence (CMI) via hidden-state patching and reports a bypass score ($1-\mathrm{CMI}$), quantifying the degree to which the answer is produced by a bypass circuit independent of the rationale. In pilot evaluations, audit-aware prompting increases detectable manipulation signals (mean risk-score delta: $+5.10$), yet causal probes reveal task-dependent mediation: many QA items exhibit near-total bypass (CMI $\approx 0$), while some logic problems show stronger mediation (CMI up to $0.56$). Layer-wise analysis reveals narrow and task-dependent ``reasoning windows'' even when mean CMI is low.

</details>


### [87] [Rational ANOVA Networks](https://arxiv.org/abs/2602.04006)
*Jusheng Zhang,Ningyuan Liu,Qinhan Lyu,Jing Yang,Keze Wang*

Main category: cs.LG

TL;DR: 提出Rational - ANOVA Network (RAN)架构，结合ANOVA分解和Padé式有理逼近，在多任务中表现优于对比模型，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有深度神经网络中非线性处理方式限制可解释性和函数类控制粒度，近期加法模型有计算低效和边界不稳定问题。

Method: 基于功能ANOVA分解和Padé式有理逼近构建RAN，将f(x)建模为主效应和稀疏两两交互的组合，每个组件由稳定、可学习的有理单元参数化，且保证分母严格为正。

Result: 在控制函数基准和视觉分类任务中，匹配参数和计算预算下，RAN表现与或超越参数匹配的MLP和可学习激活基线模型，有更好稳定性和吞吐量。

Conclusion: RAN架构通过ANOVA结构和有理参数化，在数据效率、可解释性和外推能力上表现良好。

Abstract: Deep neural networks typically treat nonlinearities as fixed primitives (e.g., ReLU), limiting both interpretability and the granularity of control over the induced function class. While recent additive models (like KANs) attempt to address this using splines, they often suffer from computational inefficiency and boundary instability. We propose the Rational-ANOVA Network (RAN), a foundational architecture grounded in functional ANOVA decomposition and Padé-style rational approximation. RAN models f(x) as a composition of main effects and sparse pairwise interactions, where each component is parameterized by a stable, learnable rational unit. Crucially, we enforce a strictly positive denominator, which avoids poles and numerical instability while capturing sharp transitions and near-singular behaviors more efficiently than polynomial bases. This ANOVA structure provides an explicit low-order interaction bias for data efficiency and interpretability, while the rational parameterization significantly improves extrapolation. Across controlled function benchmarks and vision classification tasks (e.g., CIFAR-10) under matched parameter and compute budgets, RAN matches or surpasses parameter-matched MLPs and learnable-activation baselines, with better stability and throughput. Code is available at https://github.com/jushengzhang/Rational-ANOVA-Networks.git.

</details>


### [88] [PromptSplit: Revealing Prompt-Level Disagreement in Generative Models](https://arxiv.org/abs/2602.04009)
*Mehdi Lotfian,Mohammad Jalali,Farzan Farnia*

Main category: cs.LG

TL;DR: 提出PromptSplit框架用于检测和分析生成模型间依赖提示的差异，有理论分析和实验验证。


<details>
  <summary>Details</summary>
Motivation: 生成式AI模型多样，需方法识别不同提示引起的模型行为差异。

Method: 构建联合提示-输出表示，计算核协方差矩阵，利用矩阵加权差的特征空间识别差异方向，用随机投影近似降低复杂度并进行理论分析。

Result: 在文本到图像、文本到文本和图像字幕等设置下，能准确检测真实行为差异并找出责任提示。

Conclusion: PromptSplit是检测生成模型分歧的可解释工具。

Abstract: Prompt-guided generative AI models have rapidly expanded across vision and language domains, producing realistic and diverse outputs from textual inputs. The growing variety of such models, trained with different data and architectures, calls for principled methods to identify which types of prompts lead to distinct model behaviors. In this work, we propose PromptSplit, a kernel-based framework for detecting and analyzing prompt-dependent disagreement between generative models. For each compared model pair, PromptSplit constructs a joint prompt--output representation by forming tensor-product embeddings of the prompt and image (or text) features, and then computes the corresponding kernel covariance matrix. We utilize the eigenspace of the weighted difference between these matrices to identify the main directions of behavioral difference across prompts. To ensure scalability, we employ a random-projection approximation that reduces computational complexity to $O(nr^2 + r^3)$ for projection dimension $r$. We further provide a theoretical analysis showing that this approximation yields an eigenstructure estimate whose expected deviation from the full-dimensional result is bounded by $O(1/r^2)$. Experiments across text-to-image, text-to-text, and image-captioning settings demonstrate that PromptSplit accurately detects ground-truth behavioral differences and isolates the prompts responsible, offering an interpretable tool for detecting where generative models disagree.

</details>


### [89] [Understanding and Guiding Layer Placement in Parameter-Efficient Fine-Tuning of Large Language Models](https://arxiv.org/abs/2602.04019)
*Yichen Xu,Yuyang Liang,Shan Dai,Tianyang Hu,Tsz Nam Chan,Chenhao Ma*

Main category: cs.LG

TL;DR: 随着大语言模型发展，参数高效微调（PEFT）成下游适配默认策略，本文提出统一投影残差视图，引入Layer Card指导层选择，在Qwen3 - 8B上验证可降低微调成本和推理时适配层数量。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型全参数微调成本高，PEFT通常对所有层统一应用，缺乏层选择的理解和利用，同时推理延迟和微调成本限制要求选择微调层。

Method: 在冻结基础模型上开发统一投影残差视图，基于局部二次近似，分析层适配受投影残差范数、激活能量和层耦合三个量的影响；引入Layer Card总结各层信息以指导层选择。

Result: 在Qwen3 - 8B上，选择性适配部分层可达到接近全层LoRA的性能，同时大幅降低微调成本和推理时适配器增强层的数量。

Conclusion: Layer Card指导的层选择能灵活优先考虑不同目标，为全层插入提供了更具成本 - 性能意识的替代方案。

Abstract: As large language models (LLMs) continue to grow, the cost of full-parameter fine-tuning has made parameter-efficient fine-tuning (PEFT) the default strategy for downstream adaptation. Constraints from inference latency in scalable serving and fine-tuning cost in edge or rapid-deployment settings make the choice of which layers to fine-tune unavoidable. Yet current practice typically applies PEFT uniformly across all layers, with limited understanding or leverage of layer selection. This paper develops a unified projected residual view of PEFT on top of a frozen base model. Under a local quadratic approximation, layerwise adaptation is governed by three quantities: (i) the projected residual norm (resnorm), which measures how much correctable bias a layer can capture; (ii) the activation energy, which determines feature conditioning; and (iii) layer coupling, which quantifies how strongly residuals interact across layers. We show that, for squared loss and linear adapters, the resnorm equals a normalized gradient norm, activation energy controls ill-conditioning and noise amplification, and weak coupling yields approximately additive layerwise contributions. Building on these insights, we introduce the Layer Card, a reusable diagnostic that summarizes residual signal strength, compute cost, and performance for each layer of a given model. With an identical model and LoRA configuration, Layer Card-guided placement refines the choice of adapted layers to flexibly prioritize different objectives, such as maximizing performance or reducing fine-tuning cost. Moreover, on Qwen3-8B, we show that selectively adapting a subset of layers can achieve performance close to full-layer LoRA while substantially reducing fine-tuning cost and the number of adapter-augmented layers during inference, offering a more cost-performance-aware alternative to full-layer insertion.

</details>


### [90] [Group Contrastive Learning for Weakly Paired Multimodal Data](https://arxiv.org/abs/2602.04021)
*Aditya Gorla,Hugues Van Assel,Jan-Christian Huetter,Heming Yao,Kyunghyun Cho,Aviv Regev,Russell Littman*

Main category: cs.LG

TL;DR: 提出GROOVE方法用于高内涵扰动数据的半监督多模态表示学习，含GroupCLIP损失、自编码器框架和评估框架，在多项任务表现良好，突出利用组级约束的重要性。


<details>
  <summary>Details</summary>
Motivation: 解决高内涵扰动数据中模态间样本弱配对时对比学习的空白，改善现有评估策略的局限性。

Method: 提出GroupCLIP损失，集成于动态反向翻译自编码器框架，引入综合组合评估框架。

Result: 组合基准测试显示无始终占优的对齐器，GROOVE在模拟和真实数据集的下游任务表现佳，消融实验表明GroupCLIP是性能提升关键。

Conclusion: 在弱配对情况下，利用组级约束对有效的多模态表示学习很重要。

Abstract: We present GROOVE, a semi-supervised multi-modal representation learning approach for high-content perturbation data where samples across modalities are weakly paired through shared perturbation labels but lack direct correspondence. Our primary contribution is GroupCLIP, a novel group-level contrastive loss that bridges the gap between CLIP for paired cross-modal data and SupCon for uni-modal supervised contrastive learning, addressing a fundamental gap in contrastive learning for weakly-paired settings. We integrate GroupCLIP with an on-the-fly backtranslating autoencoder framework to encourage cross-modally entangled representations while maintaining group-level coherence within a shared latent space. Critically, we introduce a comprehensive combinatorial evaluation framework that systematically assesses representation learners across multiple optimal transport aligners, addressing key limitations in existing evaluation strategies. This framework includes novel simulations that systematically vary shared versus modality-specific perturbation effects enabling principled assessment of method robustness. Our combinatorial benchmarking reveals that there is not yet an aligner that uniformly dominates across settings or modality pairs. Across simulations and two real single-cell genetic perturbation datasets, GROOVE performs on par with or outperforms existing approaches for downstream cross-modal matching and imputation tasks. Our ablation studies demonstrate that GroupCLIP is the key component driving performance gains. These results highlight the importance of leveraging group-level constraints for effective multi-modal representation learning in scenarios where only weak pairing is available.

</details>


### [91] [A Consensus-Bayesian Framework for Detecting Malicious Activity in Enterprise Directory Access Graphs](https://arxiv.org/abs/2602.04027)
*Pratyush Uppuluri,Shilpa Noushad,Sajan Kumar*

Main category: cs.LG

TL;DR: 提出基于共识的贝叶斯框架检测企业目录访问图中的恶意用户行为，模拟访问演变，引入贝叶斯异常评分机制，经模拟验证有效。


<details>
  <summary>Details</summary>
Motivation: 检测企业目录访问图中的恶意用户行为。

Method: 将目录建模为主题、用户建模为多级别交互图中的代理，用影响加权意见动态模拟访问演变，用动态矩阵编码用户间逻辑依赖，用共享影响矩阵捕捉目录相似性，通过意见动态理论确定主题收敛和检测异常，引入随时间演变的贝叶斯异常评分机制。

Result: 在合成访问图上的模拟验证了该方法对逻辑不一致的敏感性和动态扰动下的鲁棒性。

Conclusion: 所提出的基于共识的贝叶斯框架能有效检测企业目录访问图中的恶意用户行为。

Abstract: This work presents a consensus-based Bayesian framework to detect malicious user behavior in enterprise directory access graphs. By modeling directories as topics and users as agents within a multi-level interaction graph, we simulate access evolution using influence-weighted opinion dynamics. Logical dependencies between users are encoded in dynamic matrices Ci, and directory similarity is captured via a shared influence matrix W. Malicious behavior is injected as cross-component logical perturbations that violate structural norms of strongly connected components(SCCs). We apply theoretical guarantees from opinion dynamics literature to determine topic convergence and detect anomaly via scaled opinion variance. To quantify uncertainty, we introduce a Bayesian anomaly scoring mechanism that evolves over time, using both static and online priors. Simulations over synthetic access graphs validate our method, demonstrating its sensitivity to logical inconsistencies and robustness under dynamic perturbation.

</details>


### [92] [The Illusion of Generalization: Re-examining Tabular Language Model Evaluation](https://arxiv.org/abs/2602.04031)
*Aditya Gorla,Ratish Puduppully*

Main category: cs.LG

TL;DR: 对代表性表格语言模型Tabula - 8B进行系统重评估，发现其所谓泛化可能是评估假象，最后给出增强评估建议。


<details>
  <summary>Details</summary>
Motivation: 已有说法称表格语言模型（TLMs）在表格预测中实现了涌现式泛化，对这一说法进行系统验证。

Method: 利用UniPredict基准中的165个数据集对Tabula - 8B进行系统重评估。

Result: 1. 二分类和多分类在多数类基线的提升接近零，整体表现靠四分位数分类任务驱动。2. 表现好的数据集存在污染问题。3. 无表格数据的指令调优可恢复大部分标准分类性能。

Conclusion: 声称的泛化可能是评估假象而非学习到的表格推理，给出加强TLM评估的建议。

Abstract: Tabular Language Models (TLMs) have been claimed to achieve emergent generalization for tabular prediction. We conduct a systematic re-evaluation of Tabula-8B as a representative TLM, utilizing 165 datasets from the UniPredict benchmark. Our investigation reveals three findings. First, binary and categorical classification achieve near-zero median lift over majority-class baselines and strong aggregate performance is driven entirely by quartile classification tasks. Second, top-performing datasets exhibit pervasive contamination, including complete train-test overlap and task-level leakage that evades standard deduplication. Third, instruction-tuning without tabular exposure recovers 92.2% of standard classification performance and on quartile classification, format familiarity closes 71.3% of the gap with the residual attributable to contaminated datasets. These findings suggest claimed generalization likely reflects evaluation artifacts rather than learned tabular reasoning. We conclude with recommendations for strengthening TLM evaluation.

</details>


### [93] [DADP: Domain Adaptive Diffusion Policy](https://arxiv.org/abs/2602.04037)
*Pengcheng Wang,Qinghang Liu,Haotian Lin,Yiheng Li,Guojian Zhan,Masayoshi Tomizuka,Yixiao Wang*

Main category: cs.LG

TL;DR: 本文提出DADP解决基于学习控制中的域适应策略泛化挑战，通过无监督解缠和域感知扩散注入实现鲁棒适应，实验证明其性能与泛化性更优。


<details>
  <summary>Details</summary>
Motivation: 现有学习域表示方法在学习中会使静态域信息和动态属性纠缠，限制零样本适应，需解决域适应策略泛化到未知转移动态的挑战。

Method: 提出DADP，采用滞后上下文动态预测策略无监督解缠静态域表示，将学习到的域表示集成到生成过程中。

Result: 在运动和操作等具有挑战性的基准测试中，DADP表现出比先前方法更优的性能和泛化性。

Conclusion: DADP通过有效解缠和域感知扩散注入，能实现鲁棒的零样本域适应，优于现有方法。

Abstract: Learning domain adaptive policies that can generalize to unseen transition dynamics, remains a fundamental challenge in learning-based control. Substantial progress has been made through domain representation learning to capture domain-specific information, thus enabling domain-aware decision making. We analyze the process of learning domain representations through dynamical prediction and find that selecting contexts adjacent to the current step causes the learned representations to entangle static domain information with varying dynamical properties. Such mixture can confuse the conditioned policy, thereby constraining zero-shot adaptation. To tackle the challenge, we propose DADP (Domain Adaptive Diffusion Policy), which achieves robust adaptation through unsupervised disentanglement and domain-aware diffusion injection. First, we introduce Lagged Context Dynamical Prediction, a strategy that conditions future state estimation on a historical offset context; by increasing this temporal gap, we unsupervisedly disentangle static domain representations by filtering out transient properties. Second, we integrate the learned domain representations directly into the generative process by biasing the prior distribution and reformulating the diffusion target. Extensive experiments on challenging benchmarks across locomotion and manipulation demonstrate the superior performance, and the generalizability of DADP over prior methods. More visualization results are available on the https://outsider86.github.io/DomainAdaptiveDiffusionPolicy/.

</details>


### [94] [Partition Trees: Conditional Density Estimation over General Outcome Spaces](https://arxiv.org/abs/2602.04042)
*Felipe Angelim,Alessandro Leite*

Main category: cs.LG

TL;DR: 提出Partition Trees用于条件密度估计，引入Partition Forests，实证显示性能良好。


<details>
  <summary>Details</summary>
Motivation: 为一般结果空间的条件密度估计提供可扩展、非参数的方法，避免对目标分布做参数假设。

Method: 将条件分布建模为数据自适应分区上的分段常数密度，通过直接最小化条件负对数似然学习树；引入Partition Forests，对条件密度进行平均。

Result: 相比CART风格树有更好的概率预测，与最先进的概率树方法和随机森林相比有竞争力或更优，对冗余特征和异方差噪声有鲁棒性。

Conclusion: Partition Trees和Partition Forests是有效的条件密度估计方法。

Abstract: We propose Partition Trees, a tree-based framework for conditional density estimation over general outcome spaces, supporting both continuous and categorical variables within a unified formulation. Our approach models conditional distributions as piecewise-constant densities on data adaptive partitions and learns trees by directly minimizing conditional negative log-likelihood. This yields a scalable, nonparametric alternative to existing probabilistic trees that does not make parametric assumptions about the target distribution. We further introduce Partition Forests, an ensemble extension obtained by averaging conditional densities. Empirically, we demonstrate improved probabilistic prediction over CART-style trees and competitive or superior performance compared to state-of-the-art probabilistic tree methods and Random Forests, along with robustness to redundant features and heteroscedastic noise.

</details>


### [95] [Principles of Lipschitz continuity in neural networks](https://arxiv.org/abs/2602.04078)
*Róisín Luo*

Main category: cs.LG

TL;DR: 深度学习虽成功但面临鲁棒性和泛化性挑战，本文从训练动态和频率信号传播两方面研究神经网络中Lipschitz连续性原理。


<details>
  <summary>Details</summary>
Motivation: 深度学习存在确保对小输入扰动的鲁棒性和对分布外数据的泛化性挑战，需理解Lipschitz连续性的基本原理，而此前该原理研究不足。

Method: 从内部（训练动态）和外部（频率信号传播调制）两个互补的角度，研究神经网络中Lipschitz连续性的原理。

Result: 未提及

Conclusion: 未提及

Abstract: Deep learning has achieved remarkable success across a wide range of domains, significantly expanding the frontiers of what is achievable in artificial intelligence. Yet, despite these advances, critical challenges remain -- most notably, ensuring robustness to small input perturbations and generalization to out-of-distribution data. These critical challenges underscore the need to understand the underlying fundamental principles that govern robustness and generalization. Among the theoretical tools available, Lipschitz continuity plays a pivotal role in governing the fundamental properties of neural networks related to robustness and generalization. It quantifies the worst-case sensitivity of network's outputs to small input perturbations. While its importance is widely acknowledged, prior research has predominantly focused on empirical regularization approaches based on Lipschitz constraints, leaving the underlying principles less explored. This thesis seeks to advance a principled understanding of the principles of Lipschitz continuity in neural networks within the paradigm of machine learning, examined from two complementary perspectives: an internal perspective -- focusing on the temporal evolution of Lipschitz continuity in neural networks during training (i.e., training dynamics); and an external perspective -- investigating how Lipschitz continuity modulates the behavior of neural networks with respect to features in the input data, particularly its role in governing frequency signal propagation (i.e., modulation of frequency signal propagation).

</details>


### [96] [SEIS: Subspace-based Equivariance and Invariance Scores for Neural Representations](https://arxiv.org/abs/2602.04054)
*Huahua Lin,Katayoun Farrahi,Xiaohao Cai*

Main category: cs.LG

TL;DR: 提出SEIS指标分析几何变换下特征表示，验证有效并揭示网络层特性及不同学习策略影响。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以洞察内部表示中几何信息组织方式及区分信息损失和重新编码，需新方法评估特征是否保留有意义空间结构。

Method: 引入SEIS（基于子空间的等变性和不变性分数），一种无需标签和变换先验知识的子空间度量方法分析层特征表示。

Result: 合成验证表明SEIS能正确恢复已知变换；用于分类网络，发现早期层到深层从等变性过渡到不变性；数据增强保留等变性同时增加不变性；多任务学习使编码器在等变性和不变性上有协同增益，跳跃连接恢复解码中损失的等变性。

Conclusion: SEIS为分析几何变换下神经网络内部表示提供有效方法，能揭示层特性及不同学习策略影响。

Abstract: Understanding how neural representations respond to geometric transformations is essential for evaluating whether learned features preserve meaningful spatial structure. Existing approaches primarily assess robustness by comparing model outputs under transformed inputs, offering limited insight into how geometric information is organized within internal representations and failing to distinguish between information loss and re-encoding. In this work, we introduce SEIS (Subspace-based Equivariance and Invariance Scores), a subspace metric for analyzing layer-wise feature representations under geometric transformations, disentangling equivariance from invariance without requiring labels or explicit knowledge of the transformation. Synthetic validation confirms that SEIS correctly recovers known transformations. Applied to trained classification networks, SEIS reveals a transition from equivariance in early layers to invariance in deeper layers, and that data augmentation increases invariance while preserving equivariance. We further show that multi-task learning induces synergistic gains in both properties at the shared encoder, and skip connections restore equivariance lost during decoding.

</details>


### [97] [Multi-Integration of Labels across Categories for Component Identification (MILCCI)](https://arxiv.org/abs/2602.04270)
*Noga Mudrik,Yuxi Chen,Gal Mishne,Adam S. Charles*

Main category: cs.LG

TL;DR: 本文提出MILCCI方法，用于分析含元数据标签的大规模时序数据。通过合成与真实数据验证了该方法性能。


<details>
  <summary>Details</summary>
Motivation: 在时间序列分析中，需理解多试验观测中标签的编码方式，厘清各标签条目的不同影响。

Method: 提出MILCCI方法，扩展稀疏逐试验分解，利用类别内标签相似性进行跨试验调整，学习组件对应时间轨迹。

Result: 通过合成和真实世界的例子（投票模式、网页浏览趋势、神经元记录）展示了MILCCI的性能。

Conclusion: MILCCI方法能有效分析含多类别元数据标签的大规模时序数据。

Abstract: Many fields collect large-scale temporal data through repeated measurements (trials), where each trial is labeled with a set of metadata variables spanning several categories. For example, a trial in a neuroscience study may be linked to a value from category (a): task difficulty, and category (b): animal choice. A critical challenge in time-series analysis is to understand how these labels are encoded within the multi-trial observations, and disentangle the distinct effect of each label entry across categories. Here, we present MILCCI, a novel data-driven method that i) identifies the interpretable components underlying the data, ii) captures cross-trial variability, and iii) integrates label information to understand each category's representation within the data. MILCCI extends a sparse per-trial decomposition that leverages label similarities within each category to enable subtle, label-driven cross-trial adjustments in component compositions and to distinguish the contribution of each category. MILCCI also learns each component's corresponding temporal trace, which evolves over time within each trial and varies flexibly across trials. We demonstrate MILCCI's performance through both synthetic and real-world examples, including voting patterns, online page view trends, and neuronal recordings.

</details>


### [98] [Separation-Utility Pareto Frontier: An Information-Theoretic Characterization](https://arxiv.org/abs/2602.04408)
*Shizhou Xu*

Main category: cs.LG

TL;DR: 研究效用与分离（公平性准则）的帕累托最优权衡，给出理论表征，开发经验正则化器，实验验证方法有效。


<details>
  <summary>Details</summary>
Motivation: 探索效用与分离之间的最优权衡，为实际权衡选择提供指导。

Method: 从信息论角度证明效用 - 分离帕累托前沿的表征和凹性；基于理论表征开发基于条件互信息的经验正则化器。

Result: 数值实验表明，该方法在多个数据集上大幅减少分离违规，且效用匹配或超过现有基线方法。

Conclusion: 该研究提供了一种可证明、稳定且灵活的方法来在深度学习中实施分离准则。

Abstract: We study the Pareto frontier (optimal trade-off) between utility and separation, a fairness criterion requiring predictive independence from sensitive attributes conditional on the true outcome. Through an information-theoretic lens, we prove a characterization of the utility-separation Pareto frontier, establish its concavity, and thereby prove the increasing marginal cost of separation in terms of utility. In addition, we characterize the conditions under which this trade-off becomes strict, providing a guide for trade-off selection in practice. Based on the theoretical characterization, we develop an empirical regularizer based on conditional mutual information (CMI) between predictions and sensitive attributes given the true outcome. The CMI regularizer is compatible with any deep model trained via gradient-based optimization and serves as a scalar monitor of residual separation violations, offering tractable guarantees during training. Finally, numerical experiments support our theoretical findings: across COMPAS, UCI Adult, UCI Bank, and CelebA, the proposed method substantially reduces separation violations while matching or exceeding the utility of established baseline methods. This study thus offers a provable, stable, and flexible approach to enforcing separation in deep learning.

</details>


### [99] [Agentic AI-Empowered Dynamic Survey Framework](https://arxiv.org/abs/2602.04071)
*Furkan Mumcu,Lokman Bekit,Michael J. Jones,Anoop Cherian,Yasin Yilmaz*

Main category: cs.LG

TL;DR: 将综述写作视为长期维护问题，提出代理动态综述框架以持续更新综述，实验证明其有效。


<details>
  <summary>Details</summary>
Motivation: 研究产出快速增长使综述易过时，造成文献冗余和分散。

Method: 提出代理动态综述框架，以持续增量整合新研究更新现有综述，保持结构并减少干扰。

Result: 通过回顾性实验，框架能有效识别和纳入新兴研究，维持现有综述连贯性和结构。

Conclusion: 所提出的动态综述框架可有效解决现有综述及时更新问题。

Abstract: Survey papers play a central role in synthesizing and organizing scientific knowledge, yet they are increasingly strained by the rapid growth of research output. As new work continues to appear after publication, surveys quickly become outdated, contributing to redundancy and fragmentation in the literature. We reframe survey writing as a long-horizon maintenance problem rather than a one-time generation task, treating surveys as living documents that evolve alongside the research they describe. We propose an agentic Dynamic Survey Framework that supports the continuous updating of existing survey papers by incrementally integrating new work while preserving survey structure and minimizing unnecessary disruption. Using a retrospective experimental setup, we demonstrate that the proposed framework effectively identifies and incorporates emerging research while preserving the coherence and structure of existing surveys.

</details>


### [100] [Stroke Lesions as a Rosetta Stone for Language Model Interpretability](https://arxiv.org/abs/2602.04074)
*Julius Fridriksson,Roger D. Newman-Norlund,Saeed Ahmadi,Regan Willis,Nadra Salman,Kalil Warren,Xiang Guan,Yong Yang,Srihari Nelakuditi,Rutvik Desai,Leonardo Bonilha,Jeff Charney,Chris Rorden*

Main category: cs.LG

TL;DR: 提出Brain - LLM Unified Model (BLUM)框架，利用中风后失语症患者数据评估LLM扰动效应，发现LLM错误模式与人类相似，为LLM可解释性开辟新途径。


<details>
  <summary>Details</summary>
Motivation: 现有LLM验证必要组件的方法有限，当前可解释性方法缺乏外部验证。

Method: 提出BLUM框架，利用病变 - 症状映射作为外部参考结构；用中风后失语症患者数据训练症状 - 病变模型，对转换器层进行系统扰动，给受扰动LLM和人类患者进行相同临床评估，并将LLM错误模式投射到人类病变空间。

Result: LLM错误模式与人类足够相似，在67%的图片命名和68.3%的句子完成条件下，预测病变与匹配人类实际病变高于随机水平；语义主导错误对应腹侧流病变模式，音位主导错误对应背侧流模式。

Conclusion: 为LLM可解释性开辟新方法途径，确立人类病变 - 症状映射为评估人工语言系统的参考框架，促使研究行为一致性是否反映共享计算原则。

Abstract: Large language models (LLMs) have achieved remarkable capabilities, yet methods to verify which model components are truly necessary for language function remain limited. Current interpretability approaches rely on internal metrics and lack external validation. Here we present the Brain-LLM Unified Model (BLUM), a framework that leverages lesion-symptom mapping, the gold standard for establishing causal brain-behavior relationships for over a century, as an external reference structure for evaluating LLM perturbation effects. Using data from individuals with chronic post-stroke aphasia (N = 410), we trained symptom-to-lesion models that predict brain damage location from behavioral error profiles, applied systematic perturbations to transformer layers, administered identical clinical assessments to perturbed LLMs and human patients, and projected LLM error profiles into human lesion space. LLM error profiles were sufficiently similar to human error profiles that predicted lesions corresponded to actual lesions in error-matched humans above chance in 67% of picture naming conditions (p < 10^{-23}) and 68.3% of sentence completion conditions (p < 10^{-61}), with semantic-dominant errors mapping onto ventral-stream lesion patterns and phonemic-dominant errors onto dorsal-stream patterns. These findings open a new methodological avenue for LLM interpretability in which clinical neuroscience provides external validation, establishing human lesion-symptom mapping as a reference framework for evaluating artificial language systems and motivating direct investigation of whether behavioral alignment reflects shared computational principles.

</details>


### [101] [Gradient Flow Through Diagram Expansions: Learning Regimes and Explicit Solutions](https://arxiv.org/abs/2602.04548)
*Dmitry Yarotsky,Eugene Golikov,Yaroslav Gusev*

Main category: cs.LG

TL;DR: 本文开发数学框架分析大规模学习问题中梯度流的缩放机制，以幂级数展开损失演化，聚焦学习高阶张量的CP分解，揭示不同学习阶段，提出求和损失展开的方法，理论与实验吻合。


<details>
  <summary>Details</summary>
Motivation: 分析大规模学习问题中梯度流的缩放机制，揭示不同学习阶段。

Method: 采用形式幂级数展开损失演化，系数由类似费曼图的图编码；将形式损失展开归结为PDE并使用特征线方法求解。

Result: 揭示了学习高阶张量CP分解的不同梯度流机制，这些机制依赖于参数缩放、张量阶数和模型对称性；理论预测与实验吻合。

Conclusion: 所提出的数学框架和方法能有效分析大规模学习问题中的梯度流，可用于揭示学习阶段和获得非线性梯度流的显式解。

Abstract: We develop a general mathematical framework to analyze scaling regimes and derive explicit analytic solutions for gradient flow (GF) in large learning problems. Our key innovation is a formal power series expansion of the loss evolution, with coefficients encoded by diagrams akin to Feynman diagrams. We show that this expansion has a well-defined large-size limit that can be used to reveal different learning phases and, in some cases, to obtain explicit solutions of the nonlinear GF. We focus on learning Canonical Polyadic (CP) decompositions of high-order tensors, and show that this model has several distinct extreme lazy and rich GF regimes such as free evolution, NTK and under- and over-parameterized mean-field. We show that these regimes depend on the parameter scaling, tensor order, and symmetry of the model in a specific and subtle way. Moreover, we propose a general approach to summing the formal loss expansion by reducing it to a PDE; in a wide range of scenarios, it turns out to be 1st order and solvable by the method of characteristics. We observe a very good agreement of our theoretical predictions with experiment.

</details>


### [102] [Improved Dimension Dependence for Bandit Convex Optimization with Gradient Variations](https://arxiv.org/abs/2602.04761)
*Hang Yu,Yu-Hu Yan,Peng Zhao*

Main category: cs.LG

TL;DR: 本文聚焦带两点反馈的强盗凸优化（BCO）中的梯度变化问题，通过对非连续梯度变化的精细分析改进了已有结果，还拓展到单点设置并在其他任务验证了效果。


<details>
  <summary>Details</summary>
Motivation: 梯度变化在线学习在全信息设置下研究较多，但在强盗反馈下研究不足，本文旨在研究带两点反馈的BCO中的梯度变化。

Method: 对非连续梯度变化进行精细分析。

Result: 相比已有最好结果，改进了凸函数和强凸函数的维度依赖，得到其他问题依赖的保障，实现单点强盗线性优化的首个梯度变化界，在动态/通用后悔最小化和强盗游戏等任务验证了结果有效性。

Conclusion: 验证了分析方法的有效性，建立了两点BCO的首个梯度变化动态和通用后悔界以及强盗游戏的快速收敛率。

Abstract: Gradient-variation online learning has drawn increasing attention due to its deep connections to game theory, optimization, etc. It has been studied extensively in the full-information setting, but is underexplored with bandit feedback. In this work, we focus on gradient variation in Bandit Convex Optimization (BCO) with two-point feedback. By proposing a refined analysis on the non-consecutive gradient variation, a fundamental quantity in gradient variation with bandits, we improve the dimension dependence for both convex and strongly convex functions compared with the best known results (Chiang et al., 2013). Our improved analysis for the non-consecutive gradient variation also implies other favorable problem-dependent guarantees, such as gradient-variance and small-loss regrets. Beyond the two-point setup, we demonstrate the versatility of our technique by achieving the first gradient-variation bound for one-point bandit linear optimization over hyper-rectangular domains. Finally, we validate the effectiveness of our results in more challenging tasks such as dynamic/universal regret minimization and bandit games, establishing the first gradient-variation dynamic and universal regret bounds for two-point BCO and fast convergence rates in bandit games.

</details>


### [103] [A Probabilistic Framework for Solving High-Frequency Helmholtz Equations via Diffusion Models](https://arxiv.org/abs/2602.04082)
*Yicheng Zou,Samuel Lanthaler,Hossein Salahshoor*

Main category: cs.LG

TL;DR: 论文提出用概率方法在高频区域近似波，开发基于分数的条件扩散算子框架，实验显示该概率神经算子表现好且能捕捉不确定性。


<details>
  <summary>Details</summary>
Motivation: 确定性神经算子在近似高频波现象时存在困难，输入到输出的强敏感性和频谱偏差使其学习具挑战性。

Method: 采用概率方法，用基于分数的条件扩散算子开发概率框架，并对亥姆霍兹算子进行稳定性分析。

Result: 概率神经算子在不同频率下预测稳定，在多种范数下误差最低，且能捕捉输入声速图传播到解场的不确定性。

Conclusion: 概率算子学习是在高频区域解决亥姆霍兹等复杂偏微分方程的有效方法。

Abstract: Deterministic neural operators perform well on many PDEs but can struggle with the approximation of high-frequency wave phenomena, where strong input-to-output sensitivity makes operator learning challenging, and spectral bias blurs oscillations. We argue for adopting a probabilistic approach for approximating waves in high-frequency regime, and develop our probabilistic framework using a score-based conditional diffusion operator. After demonstrating a stability analysis of the Helmholtz operator, we present our numerical experiments across a wide range of frequencies, benchmarked against other popular data-driven and machine learning approaches for waves. We show that our probabilistic neural operator consistently produces robust predictions with the lowest errors in $L^2$, $H^1$, and energy norms. Moreover, unlike all the other tested deterministic approaches, our framework remarkably captures uncertainties in the input sound speed map propagated to the solution field. We envision that our results position probabilistic operator learning as a principled and effective approach for solving complex PDEs such as Helmholtz in the challenging high-frequency regime.

</details>


### [104] [Federated Concept-Based Models: Interpretable models with distributed supervision](https://arxiv.org/abs/2602.04093)
*Dario Fenoglio,Arianna Casanova,Francesco De Santis,Mohan Li,Gabriele Dominici,Johannes Schneider,Martin Gjoreski,Marc Langheinrich,Pietro Barbiero,Giovanni De Felice*

Main category: cs.LG

TL;DR: 本文提出Federated Concept-based Models (F-CMs)，可在联邦学习中部署概念模型，兼顾准确性、隐私性与可解释性，优于非自适应基线。


<details>
  <summary>Details</summary>
Motivation: 概念模型可增强深度学习可解释性，但概念注释获取成本高，联邦学习虽可缓解此问题，但缺乏可解释建模范式，且集成概念模型有挑战。

Method: 提出F-CMs方法，聚合机构间概念信息，根据概念监督变化调整模型架构，同时保护机构隐私。

Result: F-CMs保留了全概念监督训练的准确性和干预有效性，优于非自适应联邦基线，能对机构不可用概念进行可解释推断。

Conclusion: F-CMs是一种在不断发展的联邦学习环境中部署概念模型的有效方法，具有创新性。

Abstract: Concept-based models (CMs) enhance interpretability in deep learning by grounding predictions in human-understandable concepts. However, concept annotations are expensive to obtain and rarely available at scale within a single data source. Federated learning (FL) could alleviate this limitation by enabling cross-institutional training that leverages concept annotations distributed across multiple data owners. Yet, FL lacks interpretable modeling paradigms. Integrating CMs with FL is non-trivial: CMs assume a fixed concept space and a predefined model architecture, whereas real-world FL is heterogeneous and non-stationary, with institutions joining over time and bringing new supervision. In this work, we propose Federated Concept-based Models (F-CMs), a new methodology for deploying CMs in evolving FL settings. F-CMs aggregate concept-level information across institutions and efficiently adapt the model architecture in response to changes in the available concept supervision, while preserving institutional privacy. Empirically, F-CMs preserve the accuracy and intervention effectiveness of training settings with full concept supervision, while outperforming non-adaptive federated baselines. Notably, F-CMs enable interpretable inference on concepts not available to a given institution, a key novelty with respect to existing approaches.

</details>


### [105] [Maximum-Volume Nonnegative Matrix Factorization](https://arxiv.org/abs/2602.04795)
*Olivier Vu Thanh,Nicolas Gillis*

Main category: cs.LG

TL;DR: 本文提出最大体积非负矩阵分解（MaxVol NMF），对比最小体积非负矩阵分解（MinVol NMF），给出求解算法及归一化变体，并在高光谱解混中验证。


<details>
  <summary>Details</summary>
Motivation: 为获得更具解释性和唯一性的非负矩阵分解解，考虑与MinVol NMF相反的最大化H体积的方法。

Method: 提出两种算法求解MaxVol NMF，还给出其归一化变体。

Result: MaxVol NMF在提取稀疏分解方面更有效，不会产生秩亏解；归一化变体性能优于MinVol NMF和MaxVol NMF。

Conclusion: MaxVol NMF及其归一化变体在非负矩阵分解中有良好表现，可用于高光谱解混。

Abstract: Nonnegative matrix factorization (NMF) is a popular data embedding technique. Given a nonnegative data matrix $X$, it aims at finding two lower dimensional matrices, $W$ and $H$, such that $X\approx WH$, where the factors $W$ and $H$ are constrained to be element-wise nonnegative. The factor $W$ serves as a basis for the columns of $X$. In order to obtain more interpretable and unique solutions, minimum-volume NMF (MinVol NMF) minimizes the volume of $W$. In this paper, we consider the dual approach, where the volume of $H$ is maximized instead; this is referred to as maximum-volume NMF (MaxVol NMF). MaxVol NMF is identifiable under the same conditions as MinVol NMF in the noiseless case, but it behaves rather differently in the presence of noise. In practice, MaxVol NMF is much more effective to extract a sparse decomposition and does not generate rank-deficient solutions. In fact, we prove that the solutions of MaxVol NMF with the largest volume correspond to clustering the columns of $X$ in disjoint clusters, while the solutions of MinVol NMF with smallest volume are rank deficient. We propose two algorithms to solve MaxVol NMF. We also present a normalized variant of MaxVol NMF that exhibits better performance than MinVol NMF and MaxVol NMF, and can be interpreted as a continuum between standard NMF and orthogonal NMF. We illustrate our results in the context of hyperspectral unmixing.

</details>


### [106] [CoRe: Context-Robust Remasking for Diffusion Language Models](https://arxiv.org/abs/2602.04096)
*Kevin Zhai,Sabbir Mollah,Zhenyi Wang,Mubarak Shah*

Main category: cs.LG

TL;DR: 标准解码在掩码扩散模型中受上下文刚性阻碍，本文提出无训练框架CoRe进行推理时修正，在基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决掩码扩散模型标准解码中上下文刚性问题，现有修正策略依赖静态置信度分数存在局限性。

Method: 提出Context - Robust Remasking (CoRe)框架，通过探测标记对掩码上下文扰动的敏感性识别上下文脆弱标记，将修正形式化为上下文偏移的鲁棒优化目标并近似求解。

Result: 在LLaDA - 8B - Base上，CoRe在推理和代码基准测试中持续改进，优于计算匹配的基线，将MBPP提高了9.2个百分点。

Conclusion: CoRe是一种有效的推理时修正框架，能解决掩码扩散模型解码中的上下文刚性问题。

Abstract: Standard decoding in Masked Diffusion Models (MDMs) is hindered by context rigidity: tokens are retained based on transient high confidence, often ignoring that early predictions lack full context. This creates cascade effects where initial inconsistencies misguide the remaining generation. Existing revision strategies attempt to mitigate this by relying on static confidence scores, but these signals are inherently myopic; inconsistent tokens can appear confident to the model itself. We propose Context-Robust Remasking (CoRe), a training-free framework for inference-time revision. Rather than trusting static token probabilities, CoRe identifies context-brittle tokens by probing their sensitivity to targeted masked-context perturbations. We formalize revision as a robust optimization objective over context shifts and efficiently approximate this objective to prioritize unstable tokens for revision. On LLaDA-8B-Base, CoRe delivers consistent improvements across reasoning and code benchmarks, outperforming compute-matched baselines and improving MBPP by up to 9.2 percentage points.

</details>


### [107] [Rethinking Perplexity: Revealing the Impact of Input Length on Perplexity Evaluation in LLMs](https://arxiv.org/abs/2602.04099)
*Letian Cheng,Junyan Wang,Yan Gao,Elliott Wen,Ting Dang,Hong Jia*

Main category: cs.LG

TL;DR: 提出LengthBenchmark评估框架，考虑输入长度等因素评估大语言模型，发现滑动窗口评估影响性能及模型受输入长度影响的现象。


<details>
  <summary>Details</summary>
Motivation: 现有困惑度指标在处理长输入时不可靠，且输入长度对困惑度的影响缺乏系统研究，未将其作为影响公平性和效率的关键系统变量。

Method: 引入LengthBenchmark评估框架，在两种评分协议下评估代表性大语言模型，测量延迟、内存占用和评估成本，还纳入量化变体进行稳健性检查。

Result: 滑动窗口评估会使短输入性能虚高，全精度和量化模型在评估段长度增加时表现提升。

Conclusion: 长度偏差是普遍现象，会破坏跨模型的公平比较。

Abstract: Perplexity is a widely adopted metric for assessing the predictive quality of large language models (LLMs) and often serves as a reference metric for downstream evaluations. However, recent evidence shows that perplexity can be unreliable, especially when irrelevant long inputs are used, raising concerns for both benchmarking and system deployment. While prior efforts have employed selective input filtering and curated datasets, the impact of input length on perplexity has not been systematically studied from a systems perspective and input length has rarely been treated as a first-class system variable affecting both fairness and efficiency. In this work, we close this gap by introducing LengthBenchmark, a system-conscious evaluation framework that explicitly integrates input length, evaluation protocol design, and system-level costs, evaluating representative LLMs under two scoring protocols (direct accumulation and fixed window sliding) across varying context lengths. Unlike prior work that focuses solely on accuracy-oriented metrics, LengthBenchmark additionally measures latency, memory footprint, and evaluation cost, thereby linking predictive metrics to deployment realities. We further incorporate quantized variants not as a main contribution, but as robustness checks, showing that length-induced biases persist across both full-precision and compressed models. This design disentangles the effects of evaluation logic, quantization, and input length, and demonstrates that length bias is a general phenomenon that undermines fair cross-model comparison. Our analysis yields two key observations: (i) sliding window evaluation consistently inflates performance on short inputs, and (ii) both full-precision and quantized models appear to realise gains as the evaluated segment length grows.

</details>


### [108] [Subliminal Effects in Your Data: A General Mechanism via Log-Linearity](https://arxiv.org/abs/2602.04863)
*Ishaq Aden-Ali,Noah Golowich,Allen Liu,Abhishek Shetty,Ankur Moitra,Nika Haghtalab*

Main category: cs.LG

TL;DR: 本文探讨数据集对大语言模型属性的影响，提出LLS方法选择数据集子集以引出隐藏效果，且效果具有普遍性。


<details>
  <summary>Details</summary>
Motivation: 训练大语言模型涉及众多算法和数据集，需理解数据集对模型属性的影响，且现有实验表明数据集存在难以从单个数据点观察到的信号。

Method: 引入Logit - Linear - Selection (LLS)方法，用于选择通用偏好数据集的子集。

Result: 应用LLS在真实世界数据集找到子集，使模型展现特定偏好、用数据集中未出现的语言响应、呈现不同角色等行为，且效果在不同架构模型中持续存在。

Conclusion: LLS方法具有一般性和普遍性，能有效揭示通用数据集中隐藏子文本产生的机制。

Abstract: Training modern large language models (LLMs) has become a veritable smorgasbord of algorithms and datasets designed to elicit particular behaviors, making it critical to develop techniques to understand the effects of datasets on the model's properties. This is exacerbated by recent experiments that show datasets can transmit signals that are not directly observable from individual datapoints, posing a conceptual challenge for dataset-centric understandings of LLM training and suggesting a missing fundamental account of such phenomena. Towards understanding such effects, inspired by recent work on the linear structure of LLMs, we uncover a general mechanism through which hidden subtexts can arise in generic datasets.
  We introduce Logit-Linear-Selection (LLS), a method that prescribes how to select subsets of a generic preference dataset to elicit a wide range of hidden effects. We apply LLS to discover subsets of real-world datasets so that models trained on them exhibit behaviors ranging from having specific preferences, to responding to prompts in a different language not present in the dataset, to taking on a different persona. Crucially, the effect persists for the selected subset, across models with varying architectures, supporting its generality and universality.

</details>


### [109] [Supervised Learning as Lossy Compression: Characterizing Generalization and Sample Complexity via Finite Blocklength Analysis](https://arxiv.org/abs/2602.04107)
*Kosuke Sugiyama,Masato Uchida*

Main category: cs.LG

TL;DR: 本文从有损压缩和有限块长分析角度为机器学习泛化提供新视角，推导样本复杂度和泛化误差下界，分解过拟合项。


<details>
  <summary>Details</summary>
Motivation: 为机器学习泛化问题提供新的信息论视角。

Method: 将学习问题置于有损压缩背景，运用有限块长分析，把训练数据采样视为编码，模型构建视为解码。

Result: 推导出固定随机学习算法及其最优采样策略的样本复杂度和泛化误差下界，明确刻画过拟合程度与归纳偏置和任务的不匹配。

Conclusion: 该视角有别于现有框架，能分解过拟合项并统一信息论界和稳定性理论的观点。

Abstract: This paper presents a novel information-theoretic perspective on generalization in machine learning by framing the learning problem within the context of lossy compression and applying finite blocklength analysis. In our approach, the sampling of training data formally corresponds to an encoding process, and the model construction to a decoding process. By leveraging finite blocklength analysis, we derive lower bounds on sample complexity and generalization error for a fixed randomized learning algorithm and its associated optimal sampling strategy. Our bounds explicitly characterize the degree of overfitting of the learning algorithm and the mismatch between its inductive bias and the task as distinct terms. This separation provides a significant advantage over existing frameworks. Additionally, we decompose the overfitting term to show its theoretical connection to existing metrics found in information-theoretic bounds and stability theory, unifying these perspectives under our proposed framework.

</details>


### [110] [Rate-Optimal Noise Annealing in Semi-Dual Neural Optimal Transport: Tangential Identifiability, Off-Manifold Ambiguity, and Guaranteed Recovery](https://arxiv.org/abs/2602.04110)
*Raymond Chu,Jaewoong Choi,Dohyun Kwon*

Main category: cs.LG

TL;DR: 半对偶神经最优传输训练存在收敛到错误映射的问题，研究低维流形上的虚假解，提出加性噪声平滑方法及可达最优统计率的计算终端噪声水平，给出退火停止规则。


<details>
  <summary>Details</summary>
Motivation: 半对偶神经最优传输训练可能收敛到不正确或退化的映射，需要解决该问题。

Method: 研究加性噪声平滑方法，对最优计划定量稳定性、平滑诱导偏差和有限样本误差进行统一理论分析。

Result: 得到可达最优统计率的计算终端噪声水平，其缩放由数据内在维度决定，且发现当噪声趋于0时，简化半对偶目标条件数变差。

Conclusion: 给出一个原则性的停止规则，退火到低于计算的终端噪声水平不会提高统计精度，反而会使优化条件变差。

Abstract: Semi-dual neural optimal transport learns a transport map via a max-min objective, yet training can converge to incorrect or degenerate maps. We fully characterize these spurious solutions in the common regime where data concentrate on low-dimensional manifold: the objective is underconstrained off the data manifold, while the on-manifold transport signal remains identifiable. Following Choi, Choi, and Kwon (2025), we study additive-noise smoothing as a remedy and prove new map recovery guarantees as the noise vanishes. Our main practical contribution is a computable terminal noise level $\varepsilon_{\mathrm{stat}}(N)$ that attains the optimal statistical rate, with scaling governed by the intrinsic dimension $m$ of the data. The formula arises from a theoretical unified analysis of (i) quantitative stability of optimal plans, (ii) smoothing-induced bias, and (iii) finite-sample error, yielding rates that depend on $m$ rather than the ambient dimension. Finally, we show that the reduced semi-dual objective becomes increasingly ill-conditioned as $\varepsilon \downarrow 0$. This provides a principled stopping rule: annealing below $\varepsilon_{\mathrm{stat}}(N)$ can $\textit{worsen}$ optimization conditioning without improving statistical accuracy.

</details>


### [111] [Turning mechanistic models into forecasters by using machine learning](https://arxiv.org/abs/2602.04114)
*Amit K. Chakraborty,Hao Wang,Pouria Ramazi*

Main category: cs.LG

TL;DR: 本文提出一种结合时变参数的数据驱动方法，用于发现微分方程，提高建模和预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动方法假设系数时不变，限制了捕捉系统动态演化的能力，本文旨在克服这一局限。

Method: 允许部分参数随时间变化，直接从数据中学习其时间演化，构建包含常量和时变参数的方程系统，并将其转化为预测模型。

Result: 模型在多个数据集上验证，学习时间序列的平均绝对误差低于3%，提前一个月预测的误差低于6%，且在多数数据集上优于CNN - LSTM和GBM。

Conclusion: 将时变参数集成到数据驱动的微分方程发现中可提高建模精度和预测性能。

Abstract: The equations of complex dynamical systems may not be identified by expert knowledge, especially if the underlying mechanisms are unknown. Data-driven discovery methods address this challenge by inferring governing equations from time-series data using a library of functions constructed from the measured variables. However, these methods typically assume time-invariant coefficients, which limits their ability to capture evolving system dynamics. To overcome this limitation, we allow some of the parameters to vary over time, learn their temporal evolution directly from data, and infer a system of equations that incorporates both constant and time-varying parameters. We then transform this framework into a forecasting model by predicting the time-varying parameters and substituting these predictions into the learned equations. The model is validated using datasets for Susceptible-Infected-Recovered, Consumer--Resource, greenhouse gas concentration, and Cyanobacteria cell count. By dynamically adapting to temporal shifts, our proposed model achieved a mean absolute error below 3\% for learning a time series and below 6\% for forecasting up to a month ahead. We additionally compare forecasting performance against CNN-LSTM and Gradient Boosting Machine (GBM), and show that our model outperforms these methods across most datasets. Our findings demonstrate that integrating time-varying parameters into data-driven discovery of differential equations improves both modeling accuracy and forecasting performance.

</details>


### [112] [Toward Effective Multimodal Graph Foundation Model: A Divide-and-Conquer Based Approach](https://arxiv.org/abs/2602.04116)
*Sicheng Liu,Xunkai Li,Daohan Su,Ru Zhang,Hongchao Qin,Ronghua Li,Guoren Wang*

Main category: cs.LG

TL;DR: 现有图基础模型多关注文本属性图，多模态图基础模型（MGFMs）有局限，提出PLANET框架，实验显示其表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有图基础模型主要关注文本属性图，MGFMs未充分开发且存在未显式建模模态交互和模态对齐欠佳的问题，需改进。

Method: 提出PLANET框架，采用分治策略，在嵌入粒度用嵌入域门控实现模态交互，在节点粒度用节点离散化检索确保全局模态对齐。

Result: PLANET在多种以图为中心和多模态生成任务中显著优于现有基线。

Conclusion: PLANET框架有效解决了现有MGFMs的问题，提升了多模态图基础模型的性能。

Abstract: Graph Foundation Models (GFMs) have achieved remarkable success in generalizing across diverse domains. However, they mainly focus on Text-Attributed Graphs (TAGs), leaving Multimodal-Attributed Graphs (MAGs) largely untapped. Developing Multimodal Graph Foundation Models (MGFMs) allows for leveraging the rich multimodal information in MAGs, and extends applicability to broader types of downstream tasks. While recent MGFMs integrate diverse modality information, our empirical investigation reveals two fundamental limitations of existing MGFMs: (1)they fail to explicitly model modality interaction, essential for capturing intricate cross-modal semantics beyond simple aggregation, and (2)they exhibit sub-optimal modality alignment, which is critical for bridging the significant semantic disparity between distinct modal spaces. To address these challenges, we propose PLANET (graPh topoLogy-aware modAlity iNteraction and alignmEnT), a novel framework employing a Divide-and-Conquer strategy to decouple modality interaction and alignment across distinct granularities. At the embedding granularity, (1)Embedding-wise Domain Gating (EDG) performs local semantic enrichment by adaptively infusing topology-aware cross-modal context, achieving modality interaction. At the node granularity, (2)Node-wise Discretization Retrieval (NDR) ensures global modality alignment by constructing a Discretized Semantic Representation Space (DSRS) to bridge modality gaps. Extensive experiments demonstrate that PLANET significantly outperforms state-of-the-art baselines across diverse graph-centric and multimodal generative tasks.

</details>


### [113] [Learning to Reason in 13 Parameters](https://arxiv.org/abs/2602.04118)
*John X. Morris,Niloofar Mireshghallah,Mark Ibrahim,Saeed Mahloujifar*

Main category: cs.LG

TL;DR: 提出TinyLoRA方法，能将低秩适配器缩至单参数规模，用少量参数训练模型在推理任务上取得良好效果，且RL比SFT更高效。


<details>
  <summary>Details</summary>
Motivation: 质疑rank=1 LoRA对学习推理的必要性，探索能否进一步缩小低秩适配器规模。

Method: 提出TinyLoRA方法，在新参数化下训练模型，使用RL进行训练。

Result: 用仅13个bf16训练参数将Qwen2.5在GSM8K上训练至91%准确率，在更难推理基准测试上用少1000倍参数恢复90%性能提升，RL比SFT效率高。

Conclusion: TinyLoRA方法有效，能以极少训练参数使模型在推理任务上取得良好性能，RL在训练中更具优势。

Abstract: Recent research has shown that language models can learn to \textit{reason}, often via reinforcement learning. Some work even trains low-rank parameterizations for reasoning, but conventional LoRA cannot scale below the model dimension. We question whether even rank=1 LoRA is necessary for learning to reason and propose TinyLoRA, a method for scaling low-rank adapters to sizes as small as one parameter. Within our new parameterization, we are able to train the 8B parameter size of Qwen2.5 to 91\% accuracy on GSM8K with only 13 trained parameters in bf16 (26 total bytes). We find this trend holds in general: we are able to recover 90\% of performance improvements while training $1000x$ fewer parameters across a suite of more difficult learning-to-reason benchmarks such as AIME, AMC, and MATH500. Notably, we are only able to achieve such strong performance with RL: models trained using SFT require $100-1000x$ larger updates to reach the same performance.

</details>


### [114] [Synthesizable Molecular Generation via Soft-constrained GFlowNets with Rich Chemical Priors](https://arxiv.org/abs/2602.04119)
*Hyeonah Kim,Minsu Kim,Celine Roget,Dionessa Biton,Louis Vaillancourt,Yves V. Brun,Yoshua Bengio,Alex Hernandez-Garcia*

Main category: cs.LG

TL;DR: 提出用S3 - GFN生成可合成的SMILES分子，通过简单软正则化和对比学习信号，实验显示能生成高奖励可合成分子。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型在实验性药物发现中因难以设计可实际合成的分子受限，基于预定义模板和构建块的GFlowNets方法缺乏灵活性和可扩展性。

Method: 提出S3 - GFN，通过对基于序列的GFlowNet进行软正则化生成可合成SMILES分子，利用大规模SMILES语料库学习的先验知识，采用基于可合成和不可合成样本缓冲的对比学习信号进行离策略重放训练。

Result: S3 - GFN学会在不同任务中生成奖励更高的可合成分子，可合成分子比例≥95%。

Conclusion: S3 - GFN能有效生成高奖励的可合成分子，为实验性药物发现中的分子生成提供了更优方案。

Abstract: The application of generative models for experimental drug discovery campaigns is severely limited by the difficulty of designing molecules de novo that can be synthesized in practice. Previous works have leveraged Generative Flow Networks (GFlowNets) to impose hard synthesizability constraints through the design of state and action spaces based on predefined reaction templates and building blocks. Despite the promising prospects of this approach, it currently lacks flexibility and scalability. As an alternative, we propose S3-GFN, which generates synthesizable SMILES molecules via simple soft regularization of a sequence-based GFlowNet. Our approach leverages rich molecular priors learned from large-scale SMILES corpora to steer molecular generation towards high-reward, synthesizable chemical spaces. The model induces constraints through off-policy replay training with a contrastive learning signal based on separate buffers of synthesizable and unsynthesizable samples. Our experiments show that S3-GFN learns to generate synthesizable molecules ($\geq 95\%$) with higher rewards in diverse tasks.

</details>


### [115] [Decoupling Time and Risk: Risk-Sensitive Reinforcement Learning with General Discounting](https://arxiv.org/abs/2602.04131)
*Mehrdad Moghimi,Anthony Coache,Hyejin Ku*

Main category: cs.LG

TL;DR: 本文提出一种支持灵活折扣未来奖励和优化风险度量的分布强化学习框架，经分析和实验验证方法有效性。


<details>
  <summary>Details</summary>
Motivation: 分布强化学习中折扣因子常被视为固定参数或可调整超参数，其对学习策略的影响被忽视，且指数折扣因子无法完全捕捉智能体的时间偏好。

Method: 提出支持未来奖励灵活折扣和风险度量优化的新框架，对算法最优性进行技术分析，并进行多步扩展。

Result: 多步扩展解决了现有方法的问题，大量实验验证了方法的鲁棒性。

Conclusion: 折扣是决策问题的基石，能捕捉更丰富的时间和风险偏好，对现实安全关键应用有潜在意义。

Abstract: Distributional reinforcement learning (RL) is a powerful framework increasingly adopted in safety-critical domains for its ability to optimize risk-sensitive objectives. However, the role of the discount factor is often overlooked, as it is typically treated as a fixed parameter of the Markov decision process or tunable hyperparameter, with little consideration of its effect on the learned policy. In the literature, it is well-known that the discounting function plays a major role in characterizing time preferences of an agent, which an exponential discount factor cannot fully capture. Building on this insight, we propose a novel framework that supports flexible discounting of future rewards and optimization of risk measures in distributional RL. We provide a technical analysis of the optimality of our algorithms, show that our multi-horizon extension fixes issues raised with existing methodologies, and validate the robustness of our methods through extensive experiments. Our results highlight that discounting is a cornerstone in decision-making problems for capturing more expressive temporal and risk preferences profiles, with potential implications for real-world safety-critical applications.

</details>


### [116] [Generative Neural Operators through Diffusion Last Layer](https://arxiv.org/abs/2602.04139)
*Sungwon Park,Anthony Zhou,Hongjoong Kim,Amir Barati Farimani*

Main category: cs.LG

TL;DR: 本文提出扩散最后一层（DLL），可附加到任意神经算子骨干模型上对预测不确定性建模，在随机偏微分方程算子学习基准测试中表现良好，也能提升确定性长程预测稳定性。


<details>
  <summary>Details</summary>
Motivation: 实际系统具有随机性，需要进行原则性的不确定性量化以实现可靠部署。

Method: 引入扩散最后一层（DLL），通过低秩Karhunen - Loève展开在函数空间直接参数化条件输出分布。

Result: 在随机偏微分方程算子学习基准测试中，DLL提升泛化能力和不确定性感知预测能力；在确定性长程预测中，提升预测稳定性并提供认知不确定性的有意义估计。

Conclusion: DLL能有效对预测不确定性建模，提升模型性能。

Abstract: Neural operators have emerged as a powerful paradigm for learning discretization-invariant function-to-function mappings in scientific computing. However, many practical systems are inherently stochastic, making principled uncertainty quantification essential for reliable deployment. To address this, we introduce a simple add-on, the diffusion last layer (DLL), a lightweight probabilistic head that can be attached to arbitrary neural operator backbones to model predictive uncertainty. Motivated by the relative smoothness and low-dimensional structure often exhibited by PDE solution distributions, DLL parameterizes the conditional output distribution directly in function space through a low-rank Karhunen-Loève expansion, enabling efficient and expressive uncertainty modeling. Across stochastic PDE operator learning benchmarks, DLL improves generalization and uncertainty-aware prediction. Moreover, even in deterministic long-horizon rollout settings, DLL enhances rollout stability and provides meaningful estimates of epistemic uncertainty for backbone neural operators.

</details>


### [117] [Training Data Efficiency in Multimodal Process Reward Models](https://arxiv.org/abs/2602.04145)
*Jinyuan Li,Chengsong Huang,Langlin Huang,Shaoyang Xu,Haolin Liu,Wenxuan Zhang,Jiaxin Huang*

Main category: cs.LG

TL;DR: 研究MPRM训练的数据效率，提出BIS方法，所选子集在少量数据时表现好。


<details>
  <summary>Details</summary>
Motivation: 训练MPRMs需要大规模MC注释语料，成本高，且现有语料有冗余。

Method: 先进行初步实验发现数据冗余，然后建立理论框架，提出基于现有MC信号的Balanced - Information Score (BIS)方法。

Result: 在两个骨干模型上，BIS选择的子集在少量数据时能达到甚至超过全量数据性能，10%训练数据时达到全量性能，比随机采样提高4.1%。

Conclusion: BIS方法能有效提高MPRM训练的数据效率。

Abstract: Multimodal Process Reward Models (MPRMs) are central to step-level supervision for visual reasoning in MLLMs. Training MPRMs typically requires large-scale Monte Carlo (MC)-annotated corpora, incurring substantial training cost. This paper studies the data efficiency for MPRM training.Our preliminary experiments reveal that MPRM training quickly saturates under random subsampling of the training data, indicating substantial redundancy within existing MC-annotated corpora.To explain this, we formalize a theoretical framework and reveal that informative gradient updates depend on two factors: label mixtures of positive/negative steps and label reliability (average MC scores of positive steps). Guided by these insights, we propose the Balanced-Information Score (BIS), which prioritizes both mixture and reliability based on existing MC signals at the rollout level, without incurring any additional cost. Across two backbones (InternVL2.5-8B and Qwen2.5-VL-7B) on VisualProcessBench, BIS-selected subsets consistently match and even surpass the full-data performance at small fractions. Notably, the BIS subset reaches full-data performance using only 10% of the training data, improving over random subsampling by a relative 4.1%.

</details>


### [118] [Pruning for Generalization: A Transfer-Oriented Spatiotemporal Graph Framework](https://arxiv.org/abs/2602.04153)
*Zihao Jing,Yuxi Long,Ganlin Feng*

Main category: cs.LG

TL;DR: 提出TL - GPSTGN框架，通过结构感知上下文选择应对数据稀缺和跨域转移问题，在交通基准测试中表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有时空模型在数据稀缺和跨域转移时性能下降，需解决多元时间序列预测问题。

Method: 采用信息论和基于相关性的标准提取子图和特征，将优化后的上下文集成到时空卷积架构中。

Result: 在大规模交通基准测试中，TL - GPSTGN在低数据转移场景下始终优于基线。

Conclusion: 显式上下文剪枝可作为一种强大的归纳偏置，提高基于图的预测模型的鲁棒性。

Abstract: Multivariate time series forecasting in graph-structured domains is critical for real-world applications, yet existing spatiotemporal models often suffer from performance degradation under data scarcity and cross-domain shifts. We address these challenges through the lens of structure-aware context selection. We propose TL-GPSTGN, a transfer-oriented spatiotemporal framework that enhances sample efficiency and out-of-distribution generalization by selectively pruning non-optimized graph context. Specifically, our method employs information-theoretic and correlation-based criteria to extract structurally informative subgraphs and features, resulting in a compact, semantically grounded representation. This optimized context is subsequently integrated into a spatiotemporal convolutional architecture to capture complex multivariate dynamics. Evaluations on large-scale traffic benchmarks demonstrate that TL-GPSTGN consistently outperforms baselines in low-data transfer scenarios. Our findings suggest that explicit context pruning serves as a powerful inductive bias for improving the robustness of graph-based forecasting models.

</details>


### [119] [BPDQ: Bit-Plane Decomposition Quantization on a Variable Grid for Large Language Models](https://arxiv.org/abs/2602.04163)
*Junyu Chen,Jungang Li,Jing Xiong,Wenjie Wang,Qingyao Yang,He Xiao,Zhen Li,Taiqiang Wu,Mengzhao Chen,Zhen Peng,Chaofan Tao,Long Shi,Hongxia Yang,Ngai Wong*

Main category: cs.LG

TL;DR: 针对资源受限部署中LLM推理的量化问题，提出BPDQ方法，能在2位量化时取得较好效果，还有理论分析。


<details>
  <summary>Details</summary>
Motivation: 资源受限部署中LLM推理受内存限制，现有PTQ方法在2 - 3位量化时效果变差，现有方法的量化网格限制误差最小化可行集。

Method: 提出Bit - Plane Decomposition Quantization (BPDQ) 方法，通过位平面和标量系数构建可变量化网格，用近似二阶信息迭代优化并补偿量化误差。

Result: 在2位量化时，能让Qwen2.5 - 72B在单张RTX 3090上以83.85%的GSM8K准确率运行。

Conclusion: 可变网格扩展了可行集，量化过程在Hessian诱导几何中与优化目标一致。

Abstract: Large language model (LLM) inference is often bounded by memory footprint and memory bandwidth in resource-constrained deployments, making quantization a fundamental technique for efficient serving. While post-training quantization (PTQ) maintains high fidelity at 4-bit, it deteriorates at 2-3 bits. Fundamentally, existing methods enforce a shape-invariant quantization grid (e.g., the fixed uniform intervals of UINT2) for each group, severely restricting the feasible set for error minimization. To address this, we propose Bit-Plane Decomposition Quantization (BPDQ), which constructs a variable quantization grid via bit-planes and scalar coefficients, and iteratively refines them using approximate second-order information while progressively compensating quantization errors to minimize output discrepancy. In the 2-bit regime, BPDQ enables serving Qwen2.5-72B on a single RTX 3090 with 83.85% GSM8K accuracy (vs. 90.83% at 16-bit). Moreover, we provide theoretical analysis showing that the variable grid expands the feasible set, and that the quantization process consistently aligns with the optimization objective in Hessian-induced geometry. Code: github.com/KingdalfGoodman/BPDQ.

</details>


### [120] [Topology-Aware Revival for Efficient Sparse Training](https://arxiv.org/abs/2602.04166)
*Meiling Jin,Fei Wang,Xiaoyun Yuan,Chen Qian,Yuan Cheng*

Main category: cs.LG

TL;DR: 提出Topology - Aware Revival (TAR)方法提升静态稀疏训练效果，在多任务上表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 静态稀疏训练因固定掩码模式降低网络鲁棒性，早期剪枝决策使网络陷入脆弱结构，在深度强化学习中问题更突出。

Method: 提出TAR，一种轻量级一次性剪枝后处理程序，静态剪枝后根据拓扑需求在各层分配小储备预算，随机均匀激活部分之前被剪连接，后续训练固定连接。

Result: 在多个连续控制任务上，TAR比静态稀疏基线最终回报最高提升37.9%，比动态稀疏训练基线中位数增益达13.5%。

Conclusion: TAR能有效提高静态稀疏训练效果，且无需动态重连。

Abstract: Static sparse training is a promising route to efficient learning by committing to a fixed mask pattern, yet the constrained structure reduces robustness. Early pruning decisions can lock the network into a brittle structure that is difficult to escape, especially in deep reinforcement learning (RL) where the evolving policy continually shifts the training distribution. We propose Topology-Aware Revival (TAR), a lightweight one-shot post-pruning procedure that improves static sparsity without dynamic rewiring. After static pruning, TAR performs a single revival step by allocating a small reserve budget across layers according to topology needs, randomly uniformly reactivating a few previously pruned connections within each layer, and then keeping the resulting connectivity fixed for the remainder of training. Across multiple continuous-control tasks with SAC and TD3, TAR improves final return over static sparse baselines by up to +37.9% and also outperforms dynamic sparse training baselines with a median gain of +13.5%.

</details>


### [121] [LORE: Jointly Learning the Intrinsic Dimensionality and Relative Similarity Structure From Ordinal Data](https://arxiv.org/abs/2602.04192)
*Vivek Anand,Alec Helbling,Mark Davenport,Gordon Berman,Sankar Alagapan,Christopher Rozell*

Main category: cs.LG

TL;DR: 介绍LORE框架，可从三元比较中联合学习内在维度和序数嵌入，实验证明其有效性，为感知建模和机器学习带来新方向。


<details>
  <summary>Details</summary>
Motivation: 解决从序数数据中学习主观感知空间内在维度的难题。

Method: 引入LORE框架，用非凸Schatten - p拟范数正则化，通过迭代重加权算法优化联合目标并给出收敛保证。

Result: 在合成数据集、模拟感知空间和真实众包序数判断上实验表明，LORE能学习紧凑、可解释且高精度的低维嵌入。

Conclusion: LORE可同时推断内在维度和序数嵌入，为心理物理学感知建模和机器学习从序数数据中发现低维结构开辟新方向。

Abstract: Learning the intrinsic dimensionality of subjective perceptual spaces such as taste, smell, or aesthetics from ordinal data is a challenging problem. We introduce LORE (Low Rank Ordinal Embedding), a scalable framework that jointly learns both the intrinsic dimensionality and an ordinal embedding from noisy triplet comparisons of the form, "Is A more similar to B than C?". Unlike existing methods that require the embedding dimension to be set apriori, LORE regularizes the solution using the nonconvex Schatten-$p$ quasi norm, enabling automatic joint recovery of both the ordinal embedding and its dimensionality. We optimize this joint objective via an iteratively reweighted algorithm and establish convergence guarantees. Extensive experiments on synthetic datasets, simulated perceptual spaces, and real world crowdsourced ordinal judgements show that LORE learns compact, interpretable and highly accurate low dimensional embeddings that recover the latent geometry of subjective percepts. By simultaneously inferring both the intrinsic dimensionality and ordinal embeddings, LORE enables more interpretable and data efficient perceptual modeling in psychophysics and opens new directions for scalable discovery of low dimensional structure from ordinal data in machine learning.

</details>


### [122] [From Sparse Sensors to Continuous Fields: STRIDE for Spatiotemporal Reconstruction](https://arxiv.org/abs/2602.04201)
*Yanjie Tong,Peng Chen*

Main category: cs.LG

TL;DR: 提出STRIDE框架，从稀疏点传感器测量重建高维时空场，在多基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有从稀疏点传感器测量重建高维时空场的方法难以跨轨迹和参数设置泛化，或依赖于与离散化相关的解码器。

Method: 提出两阶段的STRIDE框架，用时间编码器将传感器测量映射到潜在状态，用调制隐式神经表示解码器在任意查询位置重建场，使用FMMNN作为INR骨干。

Result: 在四个具有挑战性的基准测试中，STRIDE在极端稀疏传感下优于强基线，支持超分辨率，且对噪声具有鲁棒性。

Conclusion: STRIDE框架是从稀疏点传感器测量重建高维时空场的有效途径。

Abstract: Reconstructing high-dimensional spatiotemporal fields from sparse point-sensor measurements is a central challenge in learning parametric PDE dynamics. Existing approaches often struggle to generalize across trajectories and parameter settings, or rely on discretization-tied decoders that do not naturally transfer across meshes and resolutions. We propose STRIDE (Spatio-Temporal Recurrent Implicit DEcoder), a two-stage framework that maps a short window of sensor measurements to a latent state with a temporal encoder and reconstructs the field at arbitrary query locations with a modulated implicit neural representation (INR) decoder. Using the Fourier Multi-Component and Multi-Layer Neural Network (FMMNN) as the INR backbone improves representation of complex spatial fields and yields more stable optimization than sine-based INRs. We provide a conditional theoretical justification: under stable delay observability of point measurements on a low-dimensional parametric invariant set, the reconstruction operator factors through a finite-dimensional embedding, making STRIDE-type architectures natural approximators. Experiments on four challenging benchmarks spanning chaotic dynamics and wave propagation show that STRIDE outperforms strong baselines under extremely sparse sensing, supports super-resolution, and remains robust to noise.

</details>


### [123] [RAPO: Risk-Aware Preference Optimization for Generalizable Safe Reasoning](https://arxiv.org/abs/2602.04224)
*Zeming Wei,Qiaosheng Zhang,Xia Hu,Xingcheng Xu*

Main category: cs.LG

TL;DR: 本文指出大型推理模型安全推理泛化不足问题，提出RAPO框架，实验表明其能自适应泛化安全推理。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型链式思维推理虽成功但面临安全问题，现有安全推理过程在应对复杂越狱攻击时泛化不足。

Method: 提出风险感知偏好优化（RAPO）框架，使模型能在思维内容中以适当粒度自适应识别和处理安全风险。

Result: 广泛实验证明RAPO能让多个大型推理模型在不同攻击提示下自适应泛化安全推理，同时保留通用效用。

Conclusion: RAPO为大型推理模型安全提供了一种强大的对齐技术。

Abstract: Large Reasoning Models (LRMs) have achieved tremendous success with their chain-of-thought (CoT) reasoning, yet also face safety issues similar to those of basic language models. In particular, while algorithms are designed to guide them to deliberately refuse harmful prompts with safe reasoning, this process often fails to generalize against diverse and complex jailbreak attacks. In this work, we attribute these failures to the generalization of the safe reasoning process, particularly their insufficiency against complex attack prompts. We provide both theoretical and empirical evidence to show the necessity of a more sufficient safe reasoning process to defend against advanced attack prompts. Building on this insight, we propose a Risk-Aware Preference Optimization (RAPO) framework that enables LRM to adaptively identify and address the safety risks with appropriate granularity in its thinking content. Extensive experiments demonstrate that RAPO successfully generalizes multiple LRMs' safe reasoning adaptively across diverse attack prompts whilst preserving general utility, contributing a robust alignment technique for LRM safety. Our code is available at https://github.com/weizeming/RAPO.

</details>


### [124] [Cascading Robustness Verification: Toward Efficient Model-Agnostic Certification](https://arxiv.org/abs/2602.04236)
*Mohammadreza Maleki,Rushendra Sidibomma,Arman Adibi,Reza Samavi*

Main category: cs.LG

TL;DR: 提出级联鲁棒性验证(CRV)方法，通过多个验证器平衡约束紧密度和计算成本，并引入逐步松弛算法(SR)，理论和实验证明其在保证验证准确性的同时显著降低开销。


<details>
  <summary>Details</summary>
Motivation: 现有不完全验证器单独使用可能低估鲁棒性，且存在现有鲁棒性度量的基本局限性问题。

Method: 提出CRV框架，它是模型无关的验证器，逐步应用多个验证器，从最便宜方法开始，若输入被认证为鲁棒则停止，否则用更昂贵方法；对计算昂贵方法引入SR算法逐步添加约束并检查认证。

Result: 理论分析表明CRV与级联中强大但昂贵的不完全验证器相比，验证准确性相同或更高且显著降低开销；实验结果证实CRV认证输入数量不少于基准方法，运行效率最高提升约90%。

Conclusion: CRV可有效提升神经网络对抗样本鲁棒性验证的可靠性和效率。

Abstract: Certifying neural network robustness against adversarial examples is challenging, as formal guarantees often require solving non-convex problems. Hence, incomplete verifiers are widely used because they scale efficiently and substantially reduce the cost of robustness verification compared to complete methods. However, relying on a single verifier can underestimate robustness because of loose approximations or misalignment with training methods. In this work, we propose Cascading Robustness Verification (CRV), which goes beyond an engineering improvement by exposing fundamental limitations of existing robustness metric and introducing a framework that enhances both reliability and efficiency. CRV is a model-agnostic verifier, meaning that its robustness guarantees are independent of the model's training process. The key insight behind the CRV framework is that, when using multiple verification methods, an input is certifiably robust if at least one method certifies it as robust. Rather than relying solely on a single verifier with a fixed constraint set, CRV progressively applies multiple verifiers to balance the tightness of the bound and computational cost. Starting with the least expensive method, CRV halts as soon as an input is certified as robust; otherwise, it proceeds to more expensive methods. For computationally expensive methods, we introduce a Stepwise Relaxation Algorithm (SR) that incrementally adds constraints and checks for certification at each step, thereby avoiding unnecessary computation. Our theoretical analysis demonstrates that CRV achieves equal or higher verified accuracy compared to powerful but computationally expensive incomplete verifiers in the cascade, while significantly reducing verification overhead. Empirical results confirm that CRV certifies at least as many inputs as benchmark approaches, while improving runtime efficiency by up to ~90%.

</details>


### [125] [Training A Foundation Model to Represent Graphs as Vectors](https://arxiv.org/abs/2602.04244)
*Qi Feng,Jicong Fan*

Main category: cs.LG

TL;DR: 本文提出多图特征对齐方法等训练图基础模型，实验显示该模型在少样本图分类和图聚类任务中优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 训练能将图表示为向量、保留结构和语义信息以用于下游图级任务的图基础模型，同时学习不同领域图特征并保持对新领域的泛化能力。

Method: 提出多图特征对齐方法构建加权图并生成一致节点嵌入；提出密度最大化均值对齐算法增强特征一致性；在对比学习中用图神经网络实现判别式图表示；构建多层参考分布模块增强信息保留；给出理论泛化界。

Result: 少样本图分类和图聚类实验中，模型优于强基线模型。

Conclusion: 所提模型有效，能在图级任务中取得较好表现。

Abstract: This paper aims to train a graph foundation model that is able to represent any graph as a vector preserving structural and semantic information useful for downstream graph-level tasks such as graph classification and graph clustering. To learn the features of graphs from diverse domains while maintaining strong generalization ability to new domains, we propose a multi-graph-based feature alignment method, which constructs weighted graphs using the attributes of all nodes in each dataset and then generates consistent node embeddings. To enhance the consistency of the features from different datasets, we propose a density maximization mean alignment algorithm with guaranteed convergence. The original graphs and generated node embeddings are fed into a graph neural network to achieve discriminative graph representations in contrastive learning. More importantly, to enhance the information preservation from node-level representations to the graph-level representation, we construct a multi-layer reference distribution module without using any pooling operation. We also provide a theoretical generalization bound to support the effectiveness of the proposed model. The experimental results of few-shot graph classification and graph clustering show that our model outperforms strong baselines.

</details>


### [126] [From Ambiguity to Action: A POMDP Perspective on Partial Multi-Label Ambiguity and Its Horizon-One Resolution](https://arxiv.org/abs/2602.04255)
*Hanlin Pan,Yuhao Tang,Wanfu Gao*

Main category: cs.LG

TL;DR: 本文针对部分多标签学习中标签消歧难题，将消歧和特征选择任务联合建模为POMDP，分两阶段处理，有理论分析且实验验证了框架优势。


<details>
  <summary>Details</summary>
Motivation: 部分多标签学习中真实标签未被观测，模糊候选标签会将误差传播到下游任务，标签消歧重要但困难。

Method: 将消歧和特征选择任务联合建模为POMDP，把PML风险最小化转化为期望回报最大化；第一阶段通过强化学习训练变压器策略生成高质量硬伪标签，第二阶段将特征选择描述为顺序强化学习问题逐步选择特征并输出可解释全局排名。

Result: 实验在多个指标和数据集上验证了框架的优势。

Conclusion: 提出的联合建模方法能有效解决部分多标签学习中的标签消歧问题，且有理论保证。

Abstract: In partial multi-label learning (PML), the true labels are unobserved, which makes label disambiguation important but difficult. A key challenge is that ambiguous candidate labels can propagate errors into downstream tasks such as feature engineering. To solve this issue, we jointly model the disambiguation and feature selection tasks as Partially Observable Markov Decision Processes (POMDP) to turn PML risk minimization into expected-return maximization. Stage 1 trains a transformer policy via reinforcement learning to produce high-quality hard pseudo-labels; Stage 2 describes feature selection as a sequential reinforcement learning problem, selecting features step by step and outputting an interpretable global ranking. We further provide the theoretical analysis of PML-POMDP correspondence and the excess-risk bound that decompose the error into pseudo label quality term and sample size. Experiments in multiple metrics and data sets verify the advantages of the framework.

</details>


### [127] [From Dead Neurons to Deep Approximators: Deep Bernstein Networks as a Provable Alternative to Residual Layers](https://arxiv.org/abs/2602.04264)
*Ibrahim Albool,Malak Gamal El-Din,Salma Elmalaki,Yasser Shoukry*

Main category: cs.LG

TL;DR: 本文提出深度伯恩斯坦网络，可作为无残差架构，优化可训练性和表示能力，实验证明其效果良好。


<details>
  <summary>Details</summary>
Motivation: 残差连接有结构约束且无法解决分段线性激活的低效问题，需新架构优化可训练性和表示能力。

Method: 使用伯恩斯坦多项式作为激活函数，提供两方面理论基础：推导局部导数下界，证明其远离零；证明基于伯恩斯坦的网络逼近误差随深度指数衰减。

Result: 架构将标准深度网络中“死亡”神经元比例从90%降至不到5%，优于ReLU等激活函数；在HIGGS和MNIST上无需跳跃连接实现高性能训练。

Conclusion: 伯恩斯坦激活函数为函数逼近和信号流动提供了更好机制，为无残差深度架构提供了可行路径。

Abstract: Residual connections are the de facto standard for mitigating vanishing gradients, yet they impose structural constraints and fail to address the inherent inefficiencies of piecewise linear activations. We show that Deep Bernstein Networks (which utilizes Bernstein polynomials as activation functions) can act as residual-free architecture while simultaneously optimize trainability and representation power. We provide a two-fold theoretical foundation for our approach. First, we derive a theoretical lower bound on the local derivative, proving it remains strictly bounded away from zero. This directly addresses the root cause of gradient stagnation; empirically, our architecture reduces ``dead'' neurons from 90\% in standard deep networks to less than 5\%, outperforming ReLU, Leaky ReLU, SeLU, and GeLU. Second, we establish that the approximation error for Bernstein-based networks decays exponentially with depth, a significant improvement over the polynomial rates of ReLU-based architectures. By unifying these results, we demonstrate that Bernstein activations provide a superior mechanism for function approximation and signal flow. Our experiments on HIGGS and MNIST confirm that Deep Bernstein Networks achieve high-performance training without skip-connections, offering a principled path toward deep, residual-free architectures with enhanced expressive capacity.

</details>


### [128] [Thickening-to-Thinning: Reward Shaping via Human-Inspired Learning Dynamics for LLM Reasoning](https://arxiv.org/abs/2602.04265)
*Wenze Lin,Zhen Yang,Xitai Jiang,Pony Ma,Gao Huang*

Main category: cs.LG

TL;DR: 本文提出T2T动态奖励框架改进大语言模型强化学习，实验显示其性能优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习范式（RLVR）存在熵崩溃、冗长、探索不足等问题，且奖励方案无法区分问题求解时的搜索需求和知识掌握后的效率需求。

Method: 引入受人类学习过程启发的T2T动态奖励框架，采用双阶段机制，错误时‘增厚’探索，正确时‘变薄’避免冗余。

Result: 在数学基准测试中，T2T在Qwen和Deepseek模型上显著优于标准GRPO和近期基线。

Conclusion: T2T框架能有效提升大语言模型推理能力。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a promising paradigm for enhancing reasoning in Large Language Models (LLMs). However, it frequently encounters challenges such as entropy collapse, excessive verbosity, and insufficient exploration for hard problems. Crucially, existing reward schemes fail to distinguish between the need for extensive search during problem-solving and the efficiency required for mastered knowledge. In this work, we introduce T2T(Thickening-to-Thinning), a dynamic reward framework inspired by human learning processes. Specifically, it implements a dual-phase mechanism: (1) On incorrect attempts, T2T incentivizes "thickening" (longer trajectories) to broaden the search space and explore novel solution paths; (2) Upon achieving correctness, it shifts to "thinning", imposing length penalties to discourage redundancy, thereby fostering model confidence and crystallizing reasoning capabilities. Extensive experiments on mathematical benchmarks (MATH-500, AIME, AMC) across Qwen-series and Deepseek models demonstrate that T2T significantly outperforms standard GRPO and recent baselines, achieving superior performance.

</details>


### [129] [Multi Objective Design Optimization of Non Pneumatic Passenger Car Tires Using Finite Element Modeling, Machine Learning, and Particle swarm Optimization and Bayesian Optimization Algorithms](https://arxiv.org/abs/2602.04277)
*Priyankkumar Dhrangdhariya,Soumyadipta Maiti,Venkataramana Runkana*

Main category: cs.LG

TL;DR: 本文提出集成生成式设计与机器学习框架优化乘用车UPTIS型非充气轮胎辐条几何结构，设计结果性能提升显著。


<details>
  <summary>Details</summary>
Motivation: 非充气轮胎辐条结构在刚度调节、耐久性和高速振动方面存在挑战，需优化设计。

Method: 采用高阶多项式表示上下辐条轮廓，通过PCHIP几何变化生成约250个生成式设计；用KRR和XGBoost机器学习模型预测性能，减少对FEM模拟的依赖；使用粒子群优化和贝叶斯优化进行性能优化。

Result: 优化后设计的刚度可调性达53%，耐久性提高50%，振动降低43%；PSO收敛快，贝叶斯优化有效探索多目标权衡。

Conclusion: 所提框架可系统开发高性能下一代UPTIS辐条结构。

Abstract: Non Pneumatic tires offer a promising alternative to pneumatic tires. However, their discontinuous spoke structures present challenges in stiffness tuning, durability, and high speed vibration. This study introduces an integrated generative design and machine learning driven framework to optimize UPTIS type spoke geometries for passenger vehicles. Upper and lower spoke profiles were parameterized using high order polynomial representations, enabling the creation of approximately 250 generative designs through PCHIP based geometric variation. Machine learning models like KRR for stiffness and XGBoost for durability and vibration achieved strong predictive accuracy, reducing the reliance on computationally intensive FEM simulations. Optimization using Particle Swarm Optimization and Bayesian Optimization further enabled extensive performance refinement. The resulting designs demonstrate 53% stiffness tunability, up to 50% durability improvement, and 43% reduction in vibration compared to the baseline. PSO provided fast, targeted convergence, while Bayesian Optimization effectively explored multi objective tradeoffs. Overall, the proposed framework enables systematic development of high performance, next generation UPTIS spoke structures.

</details>


### [130] [Convolution Operator Network for Forward and Inverse Problems (FI-Conv): Application to Plasma Turbulence Simulations](https://arxiv.org/abs/2602.04287)
*Xingzhuo Chen,Anthony Poole,Ionut-Gabriel Farcas,David R. Hatch,Ulisses Braga-Neto*

Main category: cs.LG

TL;DR: 提出FI - Conv框架用于复杂时空动力学系统的正反问题，在预测湍流等离子体场任务中表现良好，是现有物理信息机器学习方法的有效替代。


<details>
  <summary>Details</summary>
Motivation: 解决复杂时空动力学系统中系统演化预测和参数估计问题，如湍流问题。

Method: 构建基于U - Net架构的FI - Conv，多数卷积层替换为ConvNeXt V2块；用初始状态、PDE参数和演化时间作输入预测系统未来状态；用自回归预测程序进行正向预测；开发基于梯度下降的逆估计方法。

Result: 在预测湍流等离子体场任务中，能在短时间准确正向预测等离子体状态演化，长时间捕捉感兴趣物理量统计特性；无需修改训练模型权重准确推断PDE参数。

Conclusion: FI - Conv可作为现有物理信息机器学习方法的有效替代，用于处理复杂时空动力学系统。

Abstract: We propose the Convolutional Operator Network for Forward and Inverse Problems (FI-Conv), a framework capable of predicting system evolution and estimating parameters in complex spatio-temporal dynamics, such as turbulence. FI-Conv is built on a U-Net architecture, in which most convolutional layers are replaced by ConvNeXt V2 blocks. This design preserves U-Net performance on inputs with high-frequency variations while maintaining low computational complexity. FI-Conv uses an initial state, PDE parameters, and evolution time as input to predict the system future state. As a representative example of a system exhibiting complex dynamics, we evaluate the performance of FI-Conv on the task of predicting turbulent plasma fields governed by the Hasegawa-Wakatani (HW) equations. The HW system models two-dimensional electrostatic drift-wave turbulence and exhibits strongly nonlinear behavior, making accurate approximation and long-term prediction particularly challenging. Using an autoregressive forecasting procedure, FI-Conv achieves accurate forward prediction of the plasma state evolution over short times (t ~ 3) and captures the statistic properties of derived physical quantities of interest over longer times (t ~ 100). Moreover, we develop a gradient-descent-based inverse estimation method that accurately infers PDE parameters from plasma state evolution data, without modifying the trained model weights. Collectively, our results demonstrate that FI-Conv can be an effective alternative to existing physics-informed machine learning methods for systems with complex spatio-temporal dynamics.

</details>


### [131] [Disentangling Causal Importance from Emergent Structure in Multi-Expert Orchestration](https://arxiv.org/abs/2602.04291)
*Sudipto Ghosh,Sujoy Nath,Sunny Manchanda,Tanmoy Chakraborty*

Main category: cs.LG

TL;DR: 引入INFORM方法分析多专家系统编排策略，发现路由主导性不能代表功能必要性，揭示关系重要性和内在重要性差异，表明编排行为异步出现，且INFORM能揭示因果和结构依赖。


<details>
  <summary>Details</summary>
Motivation: 多专家系统中编排策略不透明，需要对其进行可解释性分析。

Method: 引入INFORM分析方法，用同质和异质专家联盟评估编排器，控制解码温度变化，进行有针对性的消融实验。

Result: 路由主导性不能很好代表功能必要性；关系重要性和内在重要性有差异；编排行为异步出现；屏蔽内在重要专家会导致交互结构更大程度崩溃。

Conclusion: INFORM能暴露超越准确性指标的因果和结构依赖关系。

Abstract: Multi-expert systems, where multiple Large Language Models (LLMs) collaborate to solve complex tasks, are increasingly adopted for high-performance reasoning and generation. However, the orchestration policies governing expert interaction and sequencing remain largely opaque. We introduce INFORM, an interpretability analysis that treats orchestration as an explicit, analyzable computation, enabling the decoupling of expert interaction structure, execution order, and causal attribution. We use INFORM to evaluate an orchestrator on GSM8K, HumanEval, and MMLU using a homogeneous consortium of ten instruction-tuned experts drawn from LLaMA-3.1 8B, Qwen-3 8B, and DeepSeek-R1 8B, with controlled decoding-temperature variation, and a secondary heterogeneous consortium spanning 1B-7B parameter models. Across tasks, routing dominance is a poor proxy for functional necessity. We reveal a divergence between relational importance, captured by routing mass and interaction topology, and intrinsic importance, measured via gradient-based causal attribution: frequently selected experts often act as interaction hubs with limited causal influence, while sparsely routed experts can be structurally critical. Orchestration behaviors emerge asynchronously, with expert centralization preceding stable routing confidence and expert ordering remaining non-deterministic. Targeted ablations show that masking intrinsically important experts induces disproportionate collapse in interaction structure compared to masking frequent peers, confirming that INFORM exposes causal and structural dependencies beyond accuracy metrics alone.

</details>


### [132] [Efficient Equivariant High-Order Crystal Tensor Prediction via Cartesian Local-Environment Many-Body Coupling](https://arxiv.org/abs/2602.04323)
*Dian Jin,Yancheng Yuan,Xiaoming Tao*

Main category: cs.LG

TL;DR: 本文提出CEITNet用于高阶晶体张量性质端到端预测，在多个基准数据集上表现优越且计算效率高。


<details>
  <summary>Details</summary>
Motivation: 现有球谐等变模型在预测高阶晶体张量性质时计算和内存成本高。

Method: 构建每个原子的多通道笛卡尔局部环境张量，并通过可学习的通道空间相互作用实现灵活的多体混合。

Result: 在二阶介电、三阶压电和四阶弹性张量预测的基准数据集上，CEITNet在关键精度指标上超越先前方法，且计算效率高。

Conclusion: CEITNet能有效构建高阶张量，是高阶晶体张量性质预测的有效方法。

Abstract: End-to-end prediction of high-order crystal tensor properties from atomic structures remains challenging: while spherical-harmonic equivariant models are expressive, their Clebsch-Gordan tensor products incur substantial compute and memory costs for higher-order targets. We propose the Cartesian Environment Interaction Tensor Network (CEITNet), an approach that constructs a multi-channel Cartesian local environment tensor for each atom and performs flexible many-body mixing via a learnable channel-space interaction. By performing learning in channel space and using Cartesian tensor bases to assemble equivariant outputs, CEITNet enables efficient construction of high-order tensor. Across benchmark datasets for order-2 dielectric, order-3 piezoelectric, and order-4 elastic tensor prediction, CEITNet surpasses prior high-order prediction methods on key accuracy criteria while offering high computational efficiency.

</details>


### [133] [RISE: Interactive Visual Diagnosis of Fairness in Machine Learning Models](https://arxiv.org/abs/2602.04339)
*Ray Chen,Christan Grant*

Main category: cs.LG

TL;DR: 提出可视化工具RISE用于评估域转移下的公平性。


<details>
  <summary>Details</summary>
Motivation: 标量指标难以评估域转移下的公平性，难以明确差异产生的位置和方式。

Method: 引入RISE工具，将排序后的残差转换为可解释模式，连接残差曲线结构和公平性概念。

Result: 可进行局部差异诊断、跨环境子组比较、检测隐藏公平性问题，揭示聚合统计遗漏的准确性 - 公平性权衡。

Conclusion: RISE支持更明智的模型选择。

Abstract: Evaluating fairness under domain shift is challenging because scalar metrics often obscure exactly where and how disparities arise. We introduce \textit{RISE} (Residual Inspection through Sorted Evaluation), an interactive visualization tool that converts sorted residuals into interpretable patterns. By connecting residual curve structures to formal fairness notions, RISE enables localized disparity diagnosis, subgroup comparison across environments, and the detection of hidden fairness issues. Through post-hoc analysis, RISE exposes accuracy-fairness trade-offs that aggregate statistics miss, supporting more informed model selection.

</details>


### [134] [UnMaskFork: Test-Time Scaling for Masked Diffusion via Deterministic Action Branching](https://arxiv.org/abs/2602.04344)
*Kou Misaki,Takuya Akiba*

Main category: cs.LG

TL;DR: 本文提出UnMaskFork (UMF)框架，利用Masked Diffusion Language Models (MDLMs)特性优化生成路径，在复杂编码基准和数学推理任务上表现出色。


<details>
  <summary>Details</summary>
Motivation: 利用Masked Diffusion Language Models (MDLMs)迭代和非自回归生成过程的特性，开发更有效的测试时扩展策略，提升推理能力。

Method: 提出UnMaskFork (UMF)框架，将解掩码轨迹表示为搜索树，使用蒙特卡罗树搜索优化生成路径，通过多个MDLMs进行确定性部分解掩码操作探索搜索空间。

Result: UMF在复杂编码基准上始终优于现有的测试时扩展基线，在数学推理任务上也具有很强的可扩展性。

Conclusion: UnMaskFork (UMF)框架是一种有效的方法，能提升模型在复杂任务中的推理能力。

Abstract: Test-time scaling strategies have effectively leveraged inference-time compute to enhance the reasoning abilities of Autoregressive Large Language Models. In this work, we demonstrate that Masked Diffusion Language Models (MDLMs) are inherently amenable to advanced search strategies, owing to their iterative and non-autoregressive generation process. To leverage this, we propose UnMaskFork (UMF), a framework that formulates the unmasking trajectory as a search tree and employs Monte Carlo Tree Search to optimize the generation path. In contrast to standard scaling methods relying on stochastic sampling, UMF explores the search space through deterministic partial unmasking actions performed by multiple MDLMs. Our empirical evaluation demonstrates that UMF consistently outperforms existing test-time scaling baselines on complex coding benchmarks, while also exhibiting strong scalability on mathematical reasoning tasks.

</details>


### [135] [MirrorLA: Reflecting Feature Map for Vision Linear Attention](https://arxiv.org/abs/2602.04346)
*Weikang Meng,Liangyu Huo,Yadan Luo,Yaowei Wang,Yingjian Li,Zheng Zhang*

Main category: cs.LG

TL;DR: 提出MirrorLA框架解决线性注意力机制性能不如基于softmax注意力的问题，在基准测试达SOTA。


<details>
  <summary>Details</summary>
Motivation: 线性注意力机制计算复杂度低，但性能落后于基于softmax的注意力，根源在于核特征图的非负性约束导致语义信息丢失。

Method: 提出MirrorLA几何框架，用可学习的Householder反射将特征几何旋转到非负象限，通过多尺度设计恢复表征密度。

Result: MirrorLA在标准基准测试中达到了最先进的性能。

Conclusion: 可以在保证严格线性效率的同时不损害表征保真度。

Abstract: Linear attention significantly reduces the computational complexity of Transformers from quadratic to linear, yet it consistently lags behind softmax-based attention in performance. We identify the root cause of this degradation as the non-negativity constraint imposed on kernel feature maps: standard projections like ReLU act as "passive truncation" operators, indiscriminately discarding semantic information residing in the negative domain. We propose MirrorLA, a geometric framework that substitutes passive truncation with active reorientation. By leveraging learnable Householder reflections, MirrorLA rotates the feature geometry into the non-negative orthant to maximize information retention. Our approach restores representational density through a cohesive, multi-scale design: it first optimizes local discriminability via block-wise isometries, stabilizes long-context dynamics using variance-aware modulation to diversify activations, and finally, integrates dispersed subspaces via cross-head reflections to induce global covariance mixing. MirrorLA achieves state-of-the-art performance across standard benchmarks, demonstrating that strictly linear efficiency can be achieved without compromising representational fidelity.

</details>


### [136] [Mosaic Learning: A Framework for Decentralized Learning with Model Fragmentation](https://arxiv.org/abs/2602.04352)
*Sayan Biswas,Davide Frey,Romaric Gaudel,Nirupam Gupta,Anne-Marie Kermarrec,Dimitri Lerévérend,Rafael Pires,Rishi Sharma,François Taïani,Martijn de Vos*

Main category: cs.LG

TL;DR: 介绍Mosaic Learning这一去中心化学习框架，理论证明其收敛率，实验表明比基线有更高测试准确率，可成新标准。


<details>
  <summary>Details</summary>
Motivation: 去中心化学习可在无中央服务器下协作机器学习，当前需一个更优的框架。

Method: 引入Mosaic Learning框架，将模型分解成片段并在网络中独立传播。

Result: 理论上达到最优最坏情况收敛率，利用参数相关性减少最高特征值；实验上比流行病学习有最高12个百分点的节点测试准确率提升。

Conclusion: Mosaic Learning在不牺牲实用性和效率下提升去中心化学习性能，可作为新的去中心化学习标准。

Abstract: Decentralized learning (DL) enables collaborative machine learning (ML) without a central server, making it suitable for settings where training data cannot be centrally hosted. We introduce Mosaic Learning, a DL framework that decomposes models into fragments and disseminates them independently across the network. Fragmentation reduces redundant communication across correlated parameters and enables more diverse information propagation without increasing communication cost. We theoretically show that Mosaic Learning (i) shows state-of-the-art worst-case convergence rate, and (ii) leverages parameter correlation in an ML model, improving contraction by reducing the highest eigenvalue of a simplified system. We empirically evaluate Mosaic Learning on four learning tasks and observe up to 12 percentage points higher node-level test accuracy compared to epidemic learning (EL), a state-of-the-art baseline. In summary, Mosaic Learning improves DL performance without sacrificing its utility or efficiency, and positions itself as a new DL standard.

</details>


### [137] [Counterfactual Explanations for Hypergraph Neural Networks](https://arxiv.org/abs/2602.04360)
*Fabiano Veglianti,Lorenzo Antonelli,Gabriele Tolomei*

Main category: cs.LG

TL;DR: 提出CF - HyperGNNExplainer方法为HGNNs生成反事实解释，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: HGNNs难以解释，限制其在高风险场景的应用。

Method: 引入CF - HyperGNNExplainer方法，通过移除节点 - 超边关联或删除超边生成反事实超图。

Result: 在三个基准数据集上实验表明，该方法能生成有效且简洁的反事实，突出对HGNN决策最关键的高阶关系。

Conclusion: CF - HyperGNNExplainer方法能为HGNNs提供有效的反事实解释。

Abstract: Hypergraph neural networks (HGNNs) effectively model higher-order interactions in many real-world systems but remain difficult to interpret, limiting their deployment in high-stakes settings.
  We introduce CF-HyperGNNExplainer, a counterfactual explanation method for HGNNs that identifies the minimal structural changes required to alter a model's prediction. The method generates counterfactual hypergraphs using actionable edits limited to removing node-hyperedge incidences or deleting hyperedges, producing concise and structurally meaningful explanations. Experiments on three benchmark datasets show that CF-HyperGNNExplainer generates valid and concise counterfactuals, highlighting the higher-order relations most critical to HGNN decisions.

</details>


### [138] [EXaMCaP: Subset Selection with Entropy Gain Maximization for Probing Capability Gains of Large Chart Understanding Training Sets](https://arxiv.org/abs/2602.04365)
*Jiapeng Liu,Liang Li,Bing Li,Peng Fu,Xiyan Gao,Chengyang Fang,Xiaoshuai Hao,Can Ma*

Main category: cs.LG

TL;DR: 当前多通过微调评估多模态大模型在图表理解能力的提升，但成本高。本文提出EXaMCaP方法用熵增益最大化选子集来探测能力提升，实验证明其效果好。


<details>
  <summary>Details</summary>
Motivation: 全量微调多模态大语言模型评估图表理解能力提升成本高，阻碍数据集迭代优化，需找到更高效方法。

Method: 提出EXaMCaP方法，通过迭代选择样本来最大化集合熵增益，近似全量数据集的最大熵子集。

Result: EXaMCaP在探测图表理解训练集能力提升方面优于基线方法，在不同子集规模下有效，且与多种模型架构兼容。

Conclusion: EXaMCaP是一种有效且高效的探测图表理解训练集能力提升的方法。

Abstract: Recent works focus on synthesizing Chart Understanding (ChartU) training sets to inject advanced chart knowledge into Multimodal Large Language Models (MLLMs), where the sufficiency of the knowledge is typically verified by quantifying capability gains via the fine-tune-then-evaluate paradigm. However, full-set fine-tuning MLLMs to assess such gains incurs significant time costs, hindering the iterative refinement cycles of the ChartU dataset. Reviewing the ChartU dataset synthesis and data selection domains, we find that subsets can potentially probe the MLLMs' capability gains from full-set fine-tuning. Given that data diversity is vital for boosting MLLMs' performance and entropy reflects this feature, we propose EXaMCaP, which uses entropy gain maximization to select a subset. To obtain a high-diversity subset, EXaMCaP chooses the maximum-entropy subset from the large ChartU dataset. As enumerating all possible subsets is impractical, EXaMCaP iteratively selects samples to maximize the gain in set entropy relative to the current set, approximating the maximum-entropy subset of the full dataset. Experiments show that EXaMCaP outperforms baselines in probing the capability gains of the ChartU training set, along with its strong effectiveness across diverse subset sizes and compatibility with various MLLM architectures.

</details>


### [139] [Multi-scale hypergraph meets LLMs: Aligning large language models for time series analysis](https://arxiv.org/abs/2602.04369)
*Zongjiang Shang,Dongliang Cui,Binqing Wu,Ling Chen*

Main category: cs.LG

TL;DR: 文章提出用于时间序列分析的多尺度超图方法MSH - LLM，能有效对齐自然语言与时间序列的模态，在27个真实数据集上达到了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 现有的利用预训练大语言模型（LLMs）进行时间序列分析的方法未充分考虑自然语言和时间序列的多尺度结构，导致LLMs能力利用不充分。

Method: 提出MSH - LLM方法，包括设计超边机制增强时间序列语义空间的多尺度语义信息、引入跨模态对齐（CMA）模块在不同尺度对齐自然语言和时间序列的模态、引入混合提示（MoP）机制提供上下文信息并增强LLMs理解时间序列多尺度时间模式的能力。

Result: 在5种不同应用的27个真实世界数据集上的实验表明，MSH - LLM取得了最先进的结果。

Conclusion: MSH - LLM方法在时间序列分析中是有效的，可充分利用LLMs的能力。

Abstract: Recently, there has been great success in leveraging pre-trained large language models (LLMs) for time series analysis. The core idea lies in effectively aligning the modality between natural language and time series. However, the multi-scale structures of natural language and time series have not been fully considered, resulting in insufficient utilization of LLMs capabilities. To this end, we propose MSH-LLM, a Multi-Scale Hypergraph method that aligns Large Language Models for time series analysis. Specifically, a hyperedging mechanism is designed to enhance the multi-scale semantic information of time series semantic space. Then, a cross-modality alignment (CMA) module is introduced to align the modality between natural language and time series at different scales. In addition, a mixture of prompts (MoP) mechanism is introduced to provide contextual information and enhance the ability of LLMs to understand the multi-scale temporal patterns of time series. Experimental results on 27 real-world datasets across 5 different applications demonstrate that MSH-LLM achieves the state-of-the-art results.

</details>


### [140] [Reducing the labeling burden in time-series mapping using Common Ground: a semi-automated approach to tracking changes in land cover and species over time](https://arxiv.org/abs/2602.04373)
*Geethen Singh,Jasper A Slingsby,Tamara B Robinson,Glenn Moncrieff*

Main category: cs.LG

TL;DR: 本文提出“Common Ground”方法，结合变化检测和半监督学习概念，在多时间步遥感数据分类中表现出色，证明无需不断更新参考标签也能实现有效时间泛化。


<details>
  <summary>Details</summary>
Motivation: 收集新的标记数据成本高且操作困难，需要一种无需不断更新参考标签就能实现有效时间泛化的方法。

Method: 提出“Common Ground”方法，利用时间稳定区域作为动态区域的隐式监督，结合变化检测和半监督学习。

Result: 在入侵树种映射中，相比简单时间转移方法分类准确率提高21 - 40%，比黄金标准方法高10 - 16%；在欧洲土地覆盖类别映射中，比简单和黄金标准方法高2%。

Conclusion: 结合稳定参考筛选和半监督学习对可扩展且标签高效的多时间步遥感分类有效。

Abstract: Reliable classification of Earth Observation data depends on consistent, up-to-date reference labels. However, collecting new labelled data at each time step remains expensive and logistically difficult, especially in dynamic or remote ecological systems. As a response to this challenge, we demonstrate that a model with access to reference data solely from time step t0 can perform competitively on both t0 and a future time step t1, outperforming models trained separately on time-specific reference data (the gold standard). This finding suggests that effective temporal generalization can be achieved without requiring manual updates to reference labels beyond the initial time step t0. Drawing on concepts from change detection and semi-supervised learning (SSL), the most performant approach, "Common Ground", uses a semi-supervised framework that leverages temporally stable regions-areas with little to no change in spectral or semantic characteristics between time steps-as a source of implicit supervision for dynamic regions. We evaluate this strategy across multiple classifiers, sensors (Landsat-8, Sentinel-2 satellite multispectral and airborne imaging spectroscopy), and ecological use cases. For invasive tree species mapping, we observed a 21-40% improvement in classification accuracy using Common Ground compared to naive temporal transfer, where models trained at a single time step are directly applied to a future time step. We also observe a 10 -16% higher accuracy for the introduced approach compared to a gold-standard approach. In contrast, when broad land cover categories were mapped across Europe, we observed a more modest 2% increase in accuracy compared to both the naive and gold-standard approaches. These results underscore the effectiveness of combining stable reference screening with SSL for scalable and label-efficient multi-temporal remote sensing classification.

</details>


### [141] [Beyond KL Divergence: Policy Optimization with Flexible Bregman Divergences for LLM Reasoning](https://arxiv.org/abs/2602.04380)
*Rui Yuan,Mykola Khandoga,Vinay Kumar Sankarapu*

Main category: cs.LG

TL;DR: 提出GBMPO框架扩展基于组的策略优化到灵活的Bregman散度，在数学推理和代码生成任务上有良好表现，表明散度选择是重要设计维度。


<details>
  <summary>Details</summary>
Motivation: 现有基于组的策略优化方法仅使用KL散度进行策略正则化，散度函数的选择未被探索。

Method: 引入GBMPO框架，扩展到灵活的Bregman散度，包括手工设计的替代方案和学习的神经镜像映射。

Result: 在GSM8K数学推理上，ProbL2 - GRPO准确率达86.7%；在MBPP代码生成上，神经镜像映射pass@1达60.1 - 60.8%；进化策略元学习可减少方差和提高效率。

Conclusion: 散度选择是基于组的大语言模型推理策略优化中一个关键且未被探索的设计维度。

Abstract: Policy optimization methods like Group Relative Policy Optimization (GRPO) and its variants have achieved strong results on mathematical reasoning and code generation tasks. Despite extensive exploration of reward processing strategies and training dynamics, all existing group-based methods exclusively use KL divergence for policy regularization, leaving the choice of divergence function unexplored. We introduce Group-Based Mirror Policy Optimization (GBMPO), a framework that extends group-based policy optimization to flexible Bregman divergences, including hand-designed alternatives (L2 in probability space) and learned neural mirror maps. On GSM8K mathematical reasoning, hand-designed ProbL2-GRPO achieves 86.7% accuracy, improving +5.5 points over the Dr. GRPO baseline. On MBPP code generation, neural mirror maps reach 60.1-60.8% pass@1, with random initialization already capturing most of the benefit. While evolutionary strategies meta-learning provides marginal accuracy improvements, its primary value lies in variance reduction ($\pm$0.2 versus $\pm$0.6) and efficiency gains (15% shorter responses on MBPP), suggesting that random initialization of neural mirror maps is sufficient for most practical applications. These results establish divergence choice as a critical, previously unexplored design dimension in group-based policy optimization for LLM reasoning.

</details>


### [142] [Blockchain Federated Learning for Sustainable Retail: Reducing Waste through Collaborative Demand Forecasting](https://arxiv.org/abs/2602.04384)
*Fabio Turazza,Alessandro Neri,Marcello Pietri,Maria Angela Butturi,Marco Picone,Marco Mamei*

Main category: cs.LG

TL;DR: 研究探索联合学习在可持续供应链管理中的应用，开发孤立零售商预测模型和基于区块链的联合学习模型，结果显示联合学习模型能减少浪费、提高效率。


<details>
  <summary>Details</summary>
Motivation: 有效需求预测对减少食物浪费至关重要，但数据隐私问题阻碍零售商间合作，影响预测准确性，因此探索联合学习在可持续供应链管理中的应用。

Method: 先为孤立零售商场景开发需求预测和浪费评估的基线预测模型，再引入基于区块链的联合学习模型，在不直接共享数据的情况下跨多个零售商进行协作训练。

Result: 联合学习模型的性能几乎等同于各方相互共享数据的理想情况，明显优于各方不共享数据构建的模型，可减少浪费、提高效率。

Conclusion: 联合学习在可持续供应链管理的杂货零售领域有应用潜力，能在保护数据隐私的同时提升需求预测准确性，减少食物浪费。

Abstract: Effective demand forecasting is crucial for reducing food waste. However, data privacy concerns often hinder collaboration among retailers, limiting the potential for improved predictive accuracy. In this study, we explore the application of Federated Learning (FL) in Sustainable Supply Chain Management (SSCM), with a focus on the grocery retail sector dealing with perishable goods. We develop a baseline predictive model for demand forecasting and waste assessment in an isolated retailer scenario. Subsequently, we introduce a Blockchain-based FL model, trained collaboratively across multiple retailers without direct data sharing. Our preliminary results show that FL models have performance almost equivalent to the ideal setting in which parties share data with each other, and are notably superior to models built by individual parties without sharing data, cutting waste and boosting efficiency.

</details>


### [143] [On the use of LLMs to generate a dataset of Neural Networks](https://arxiv.org/abs/2602.04388)
*Nadia Daoudi,Jordi Cabot*

Main category: cs.LG

TL;DR: 为解决神经网络工具评估缺乏公开多样数据集问题，用大语言模型生成神经网络数据集作基准并公开。


<details>
  <summary>Details</summary>
Motivation: 评估神经网络相关工具有效性缺乏公开多样数据集，需构建新数据集用于评估。

Method: 利用大语言模型自动生成数据集，设计时覆盖多样架构组件和多种输入、任务，用静态分析和符号追踪验证网络正确性。

Result: 生成608个符合设计要求的样本并公开数据集。

Conclusion: 公开数据集能支持社区推进神经网络可靠性和适应性研究。

Abstract: Neural networks are increasingly used to support decision-making. To verify their reliability and adaptability, researchers and practitioners have proposed a variety of tools and methods for tasks such as NN code verification, refactoring, and migration. These tools play a crucial role in guaranteeing both the correctness and maintainability of neural network architectures, helping to prevent implementation errors, simplify model updates, and ensure that complex networks can be reliably extended and reused. Yet, assessing their effectiveness remains challenging due to the lack of publicly diverse datasets of neural networks that would allow systematic evaluation. To address this gap, we leverage large language models (LLMs) to automatically generate a dataset of neural networks that can serve as a benchmark for validation. The dataset is designed to cover diverse architectural components and to handle multiple input data types and tasks. In total, 608 samples are generated, each conforming to a set of precise design choices. To further ensure their consistency, we validate the correctness of the generated networks using static analysis and symbolic tracing. We make the dataset publicly available to support the community in advancing research on neural network reliability and adaptability.

</details>


### [144] [LoRDO: Distributed Low-Rank Optimization with Infrequent Communication](https://arxiv.org/abs/2602.04396)
*Andrej Jovanović,Alex Iacob,Mher Safaryan,Ionut-Vlad Modoranu,Lorenzo Sani,William F. Shen,Xinchi Qiu,Dan Alistarh,Nicholas D. Lane*

Main category: cs.LG

TL;DR: 提出LoRDO框架统一低秩优化和不频繁同步，减少通信，在不同场景表现良好。


<details>
  <summary>Details</summary>
Motivation: 分布式训练因互连带宽受限，不频繁通信策略受优化器状态内存和通信要求瓶颈，低秩优化器在本地更新时性能下降。

Method: 提出LoRDO框架，引入全秩准双曲更新以恢复子空间探索。

Result: 在125M - 720M模型规模的语言建模和下游任务中与低秩DDP接近，通信减少约10倍，在低内存设置中性能提升更多。

Conclusion: LoRDO能有效统一低秩优化和不频繁同步，在不同场景下表现出色。

Abstract: Distributed training of foundation models via $\texttt{DDP}$ is limited by interconnect bandwidth. While infrequent communication strategies reduce synchronization frequency, they remain bottlenecked by the memory and communication requirements of optimizer states. Low-rank optimizers can alleviate these constraints; however, in the local-update regime, workers lack access to the full-batch gradients required to compute low-rank projections, which degrades performance. We propose $\texttt{LoRDO}$, a principled framework unifying low-rank optimization with infrequent synchronization. We first demonstrate that, while global projections based on pseudo-gradients are theoretically superior, they permanently restrict the optimization trajectory to a low-rank subspace. To restore subspace exploration, we introduce a full-rank quasi-hyperbolic update. $\texttt{LoRDO}$ achieves near-parity with low-rank $\texttt{DDP}$ in language modeling and downstream tasks at model scales of $125$M--$720$M, while reducing communication by $\approx 10 \times$. Finally, we show that $\texttt{LoRDO}$ improves performance even more in very low-memory settings with small rank/batch size.

</details>


### [145] [Theory of Speciation Transitions in Diffusion Models with General Class Structure](https://arxiv.org/abs/2602.04404)
*Beatrice Achilli,Marco Benedetti,Giulio Biroli,Marc Mézard*

Main category: cs.LG

TL;DR: 本文提出扩散模型物种形成的通用理论，适用于任意目标分布，拓展了现有理论，并用两个例子说明。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型物种形成理论仅适用于通过一阶矩识别类别的情况，需更通用理论。

Method: 通过贝叶斯分类形式化类结构概念，用类间自由熵差表征物种形成时间，还将问题映射到随机场伊辛模型并用复本方法求解。

Result: 该理论能恢复先前高斯混合模型的已知结果，还适用于无法通过一阶矩区分类别的情况，且能处理多类问题，预测连续物种形成时间。

Conclusion: 研究为基于扩散的生成模型中的物种形成转变提供统一且广泛适用的描述。

Abstract: Diffusion Models generate data by reversing a stochastic diffusion process, progressively transforming noise into structured samples drawn from a target distribution. Recent theoretical work has shown that this backward dynamics can undergo sharp qualitative transitions, known as speciation transitions, during which trajectories become dynamically committed to data classes. Existing theoretical analyses, however, are limited to settings where classes are identifiable through first moments, such as mixtures of Gaussians with well-separated means. In this work, we develop a general theory of speciation in diffusion models that applies to arbitrary target distributions admitting well-defined classes. We formalize the notion of class structure through Bayes classification and characterize speciation times in terms of free-entropy difference between classes. This criterion recovers known results in previously studied Gaussian-mixture models, while extending to situations in which classes are not distinguishable by first moments and may instead differ through higher-order or collective features. Our framework also accommodates multiple classes and predicts the existence of successive speciation times associated with increasingly fine-grained class commitment. We illustrate the theory on two analytically tractable examples: mixtures of one-dimensional Ising models at different temperatures and mixtures of zero-mean Gaussians with distinct covariance structures. In the Ising case, we obtain explicit expressions for speciation times by mapping the problem onto a random-field Ising model and solving it via the replica method. Our results provide a unified and broadly applicable description of speciation transitions in diffusion-based generative models.

</details>


### [146] [EMA Policy Gradient: Taming Reinforcement Learning for LLMs with EMA Anchor and Top-k KL](https://arxiv.org/abs/2602.04417)
*Lunjun Zhang,Jimmy Ba*

Main category: cs.LG

TL;DR: 本文提出两种技术改进大语言模型的策略梯度算法，组合使用时显著提升性能，证明是强大的扩展大语言模型强化学习的方法。


<details>
  <summary>Details</summary>
Motivation: 改进大语言模型的策略梯度算法，提升强化学习性能。

Method: 一是用指数移动平均（EMA）替代强化学习中的固定锚策略；二是引入Top - k KL估计器。

Result: 与GRPO结合时显著提升性能，数学推理中R1 - 蒸馏Qwen - 1.5B在OlympiadBench上从50.8%提升到53.9%；在智能体强化学习领域，Qwen - 3B基础上平均提升33.3%。

Conclusion: EMA - PG是扩展大语言模型强化学习的简单、有原则且强大的方法。

Abstract: Reinforcement Learning (RL) has enabled Large Language Models (LLMs) to acquire increasingly complex reasoning and agentic behaviors. In this work, we propose two simple techniques to improve policy gradient algorithms for LLMs. First, we replace the fixed anchor policy during RL with an Exponential Moving Average (EMA), similar to a target network in deep Q-learning. Second, we introduce Top-k KL estimator, which allows for flexible interpolation between exact KL and sampled KL. We derive the stability conditions for using EMA anchor; moreover, we show that our Top-k KL estimator yields both unbiased KL values and unbiased gradients at any k, while bringing the benefits of exact KL. When combined with GRPO, the two techniques (EMA-PG) lead to a significant performance boost. On math reasoning, it allows R1-distilled Qwen-1.5B to reach 53.9% on OlympiadBench compared to 50.8% by GRPO. On agentic RL domains, with Qwen-3B base, EMA-PG improves GRPO by an average of 33.3% across 7 datasets of Q&A with search engines, including 29.7% $\rightarrow$ 44.1% on HotpotQA, 27.4% $\rightarrow$ 40.1% on 2WikiMultiHopQA. Overall, we show that EMA-PG is a simple, principled, and powerful approach to scaling RL for LLMs. Code: https://github.com/LunjunZhang/ema-pg

</details>


### [147] [Hand Gesture Recognition from Doppler Radar Signals Using Echo State Networks](https://arxiv.org/abs/2602.04436)
*Towa Sano,Gouhei Tanaka*

Main category: cs.LG

TL;DR: 提出基于回声状态网络（ESN）的雷达手势识别方法，低计算成本且性能优，有应用潜力。


<details>
  <summary>Details</summary>
Motivation: 传统基于深度学习的雷达手势识别方法计算成本高，需轻量级高效识别技术。

Method: 将原始雷达数据转换为特征图，输入基于循环神经网络的储备池，用读出分类器处理储备池状态。

Result: 在Soli和Dop - NET数据集任务中表现优于现有方法和模型。

Conclusion: 多储备池ESN并行处理有效，该方法实现低计算成本高识别性能，对资源受限环境有潜力。

Abstract: Hand gesture recognition (HGR) is a fundamental technology in human computer interaction (HCI).In particular, HGR based on Doppler radar signals is suited for in-vehicle interfaces and robotic systems, necessitating lightweight and computationally efficient recognition techniques. However, conventional deep learning-based methods still suffer from high computational costs. To address this issue, we propose an Echo State Network (ESN) approach for radar-based HGR, using frequency-modulated-continuous-wave (FMCW) radar signals. Raw radar data is first converted into feature maps, such as range-time and Doppler-time maps, which are then fed into one or more recurrent neural network-based reservoirs. The obtained reservoir states are processed by readout classifiers, including ridge regression, support vector machines, and random forests. Comparative experiments demonstrate that our method outperforms existing approaches on an 11-class HGR task using the Soli dataset and surpasses existing deep learning models on a 4-class HGR task using the Dop-NET dataset. The results indicate that parallel processing using multi-reservoir ESNs are effective for recognizing temporal patterns from the multiple different feature maps in the time-space and time-frequency domains. Our ESN approaches achieve high recognition performance with low computational cost in HGR, showing great potential for more advanced HCI technologies, especially in resource-constrained environments.

</details>


### [148] [Mixture of Masters: Sparse Chess Language Models with Player Routing](https://arxiv.org/abs/2602.04447)
*Giacomo Frisoni,Lorenzo Molfetta,Davide Freddi,Gianluca Moro*

Main category: cs.LG

TL;DR: 提出MoM模型应对现代国际象棋语言模型的同质化，经评估效果良好。


<details>
  <summary>Details</summary>
Motivation: 现代国际象棋语言模型为密集变换器，易出现同质化问题，风格界限模糊、有效策略被抑制。

Method: 引入以小型GPT专家模拟世界级大师的国际象棋混合专家模型MoM，结合自监督学习和基于国际象棋特定奖励的强化学习训练每个专家，用可学习门控网络根据棋局选择合适“角色”。

Result: 在与Stockfish的未知标准对局评估中，MoM优于密集单专家网络和基于聚合数据训练的常用GPT基线。

Conclusion: MoM能实现动态风格切换，保证生成的多样性、可控性和可解释性。

Abstract: Modern chess language models are dense transformers trained on millions of games played by thousands of high-rated individuals. However, these monolithic networks tend to collapse into mode-averaged behavior, where stylistic boundaries are blurred, and rare but effective strategies are suppressed. To counteract homogenization, we introduce Mixture-of-Masters (MoM), the first chess mixture-of-experts model with small-sized GPT experts emulating world-class grandmasters. Each expert is trained with a combination of self-supervised learning and reinforcement learning guided by chess-specific rewards. For each move, a post-hoc learnable gating network selects the most appropriate persona to channel depending on the game state, allowing MoM to switch its style dynamically$--$e.g., Tal's offensive vocation or Petrosian's defensive solidity. When evaluated against Stockfish on unseen standard games, MoM outperforms both dense individual expert networks and popular GPT baselines trained on aggregated data, while ensuring generation variety, control, and interpretability.

</details>


### [149] [RASA: Routing-Aware Safety Alignment for Mixture-of-Experts Models](https://arxiv.org/abs/2602.04448)
*Jiacheng Liang,Yuhui Wang,Tanqiu Jiang,Ting Wang*

Main category: cs.LG

TL;DR: 提出RASA框架解决MoE语言模型安全对齐挑战，实现高鲁棒性和泛化性，保留通用能力。


<details>
  <summary>Details</summary>
Motivation: MoE语言模型的稀疏路由机制在标准全参数微调下会带来安全对齐挑战，直接全参数安全微调不能直接修复关键安全专家。

Method: 提出RASA框架，识别越狱攻击中过度激活的专家，在固定路由下选择性微调这些专家，随后强制路由与安全对齐上下文保持一致。

Result: 在两种MoE架构和多种越狱攻击中，RASA实现了近乎完美的鲁棒性、强跨攻击泛化性，大幅减少过度拒绝，同时保留了在MMLU、GSM8K和TruthfulQA等基准测试上的通用能力。

Conclusion: MoE安全对齐通过针对性专家修复而非全局参数更新更有益，为先前方法提供了实用且保留架构的替代方案。

Abstract: Mixture-of-Experts (MoE) language models introduce unique challenges for safety alignment due to their sparse routing mechanisms, which can enable degenerate optimization behaviors under standard full-parameter fine-tuning. In our preliminary experiments, we observe that naively applying full-parameter safety fine-tuning to MoE models can reduce attack success rates through routing or expert dominance effects, rather than by directly repairing Safety-Critical Experts. To address this challenge, we propose RASA, a routing-aware expert-level alignment framework that explicitly repairs Safety-Critical Experts while preventing routing-based bypasses. RASA identifies experts disproportionately activated by successful jailbreaks, selectively fine-tunes only these experts under fixed routing, and subsequently enforces routing consistency with safety-aligned contexts. Across two representative MoE architectures and a diverse set of jailbreak attacks, RASA achieves near-perfect robustness, strong cross-attack generalization, and substantially reduced over-refusal, while preserving general capabilities on benchmarks such as MMLU, GSM8K, and TruthfulQA. Our results suggest that robust MoE safety alignment benefits from targeted expert repair rather than global parameter updates, offering a practical and architecture-preserving alternative to prior approaches.

</details>


### [150] [Greedy-Gnorm: A Gradient Matrix Norm-Based Alternative to Attention Entropy for Head Pruning](https://arxiv.org/abs/2602.04491)
*Yuxi Guo,Paul Sheridan*

Main category: cs.LG

TL;DR: 提出Greedy - Gnorm剪枝算法，动态计算注意力头重要性，实验表明其在多种模型上表现优于注意力熵，有助于高效部署模型。


<details>
  <summary>Details</summary>
Motivation: 现有注意力头剪枝方法依赖静态重要性得分，无法捕捉迭代移除过程中注意力头角色的变化，在绿色AI时代需更有效模型压缩方法。

Method: 提出Greedy - Gnorm算法，通过验证集估计每个注意力头Q/K/V梯度块的l2 - 范数的逐元素乘积来评分，每次贪婪迭代更新。

Result: 在BERT、ALBERT等模型上进行大量实验，Greedy - Gnorm在移除大量注意力头时能持续保持准确率，优于注意力熵。

Conclusion: Greedy - Gnorm能有效减小模型大小并保持任务性能，为更节能的Transformer模型部署提供了有前景的一步。

Abstract: Attention head pruning has emerged as an effective technique for transformer model compression, an increasingly important goal in the era of Green AI. However, existing pruning methods often rely on static importance scores, which fail to capture the evolving role of attention heads during iterative removal. We propose Greedy-Gradient norm (Greedy-Gnorm), a novel head pruning algorithm that dynamically recalculates head importance after each pruning step. Specifically, each head is scored by the elementwise product of the l2-norms of its Q/K/V gradient blocks, as estimated from a hold-out validation set and updated at every greedy iteration. This dynamic approach to scoring mitigates against stale rankings and better reflects gradient-informed importance as pruning progresses. Extensive experiments on BERT, ALBERT, RoBERTa, and XLM-RoBERTa demonstrate that Greedy-Gnorm consistently preserves accuracy under substantial head removal, outperforming attention entropy. By effectively reducing model size while maintaining task performance, Greedy-Gnorm offers a promising step toward more energy-efficient transformer model deployment.

</details>


### [151] [Forget to Generalize: Iterative Adaptation for Generalization in Federated Learning](https://arxiv.org/abs/2602.04536)
*Abdulrahman Alotaibi,Irene Tenison,Miriam Kim,Isaac Lee,Lalana Kagal*

Main category: cs.LG

TL;DR: 本文提出迭代联邦适应（IFA）训练范式，通过逐代遗忘和进化策略提升异构联邦环境下的泛化能力，实验表明该方法能提高全局准确率。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在非独立同分布（non - IID）客户分布下性能严重下降，但在Web场景中non - IID分布普遍存在，因此需要提升其在异构环境下的泛化能力。

Method: 提出迭代联邦适应（IFA）范式，将训练分为多个世代，在每代结束时随机或从模型的后几层选择一部分模型参数并重新初始化。

Result: 在CIFAR - 10、MIT - Indoors和Stanford Dogs数据集上的实验表明，该方法提高了全局准确率，尤其是数据在客户端间为non - IID时，平均提升21.5%，且可在任何联邦算法上实现以提升泛化性能。

Conclusion: 该工作推动了现实世界异构和分布式Web系统中可扩展、保护隐私的智能发展。

Abstract: The Web is naturally heterogeneous with user devices, geographic regions, browsing patterns, and contexts all leading to highly diverse, unique datasets. Federated Learning (FL) is an important paradigm for the Web because it enables privacy-preserving, collaborative machine learning across diverse user devices, web services and clients without needing to centralize sensitive data. However, its performance degrades severely under non-IID client distributions that is prevalent in real-world web systems. In this work, we propose a new training paradigm - Iterative Federated Adaptation (IFA) - that enhances generalization in heterogeneous federated settings through generation-wise forget and evolve strategy. Specifically, we divide training into multiple generations and, at the end of each, select a fraction of model parameters (a) randomly or (b) from the later layers of the model and reinitialize them. This iterative forget and evolve schedule allows the model to escape local minima and preserve globally relevant representations. Extensive experiments on CIFAR-10, MIT-Indoors, and Stanford Dogs datasets show that the proposed approach improves global accuracy, especially when the data cross clients are Non-IID. This method can be implemented on top any federated algorithm to improve its generalization performance. We observe an average of 21.5%improvement across datasets. This work advances the vision of scalable, privacy-preserving intelligence for real-world heterogeneous and distributed web systems.

</details>


### [152] [Continual Learning through Control Minimization](https://arxiv.org/abs/2602.04542)
*Sander de Haan,Yassine Taoudi-Benchekroun,Pau Vilimelis Aceituno,Benjamin F. Grewe*

Main category: cs.LG

TL;DR: 将持续学习重新表述为控制问题，提出持续自然梯度概念，实验显示该框架在无回放基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决神经网络顺序训练任务时的灾难性遗忘问题。

Method: 把持续学习重构为控制问题，将正则化惩罚转化为保存信号，通过最小化控制努力来整合新任务。

Result: 学习框架能恢复真实的先验任务曲率，实现任务区分。

Conclusion: 所提方法在标准基准测试中无回放时优于现有方法。

Abstract: Catastrophic forgetting remains a fundamental challenge for neural networks when tasks are trained sequentially. In this work, we reformulate continual learning as a control problem where learning and preservation signals compete within neural activity dynamics. We convert regularization penalties into preservation signals that protect prior-task representations. Learning then proceeds by minimizing the control effort required to integrate new tasks while competing with the preservation of prior tasks. At equilibrium, the neural activities produce weight updates that implicitly encode the full prior-task curvature, a property we term the continual-natural gradient, requiring no explicit curvature storage. Experiments confirm that our learning framework recovers true prior-task curvature and enables task discrimination, outperforming existing methods on standard benchmarks without replay.

</details>


### [153] [Finding Structure in Continual Learning](https://arxiv.org/abs/2602.04555)
*Pourya Shamsolmoali,Masoumeh Zareapoor*

Main category: cs.LG

TL;DR: 提出用Douglas - Rachford Splitting (DRS)重新构建持续学习目标，实现稳定性和可塑性平衡。


<details>
  <summary>Details</summary>
Motivation: 解决持续学习中可塑性和稳定性的冲突以及现有方法的梯度冲突和效率问题。

Method: 使用Douglas - Rachford Splitting (DRS)重新表述持续学习目标，通过近端算子迭代达成共识。

Result: 实现了稳定性和可塑性之间的有效平衡，无需辅助模块或复杂附加项。

Conclusion: 为持续学习系统提供了更简单强大的范式。

Abstract: Learning from a stream of tasks usually pits plasticity against stability: acquiring new knowledge often causes catastrophic forgetting of past information. Most methods address this by summing competing loss terms, creating gradient conflicts that are managed with complex and often inefficient strategies such as external memory replay or parameter regularization. We propose a reformulation of the continual learning objective using Douglas-Rachford Splitting (DRS). This reframes the learning process not as a direct trade-off, but as a negotiation between two decoupled objectives: one promoting plasticity for new tasks and the other enforcing stability of old knowledge. By iteratively finding a consensus through their proximal operators, DRS provides a more principled and stable learning dynamic. Our approach achieves an efficient balance between stability and plasticity without the need for auxiliary modules or complex add-ons, providing a simpler yet more powerful paradigm for continual learning systems.

</details>


### [154] [Probabilistic Label Spreading: Efficient and Consistent Estimation of Soft Labels with Epistemic Uncertainty on Graphs](https://arxiv.org/abs/2602.04574)
*Jonathan Klees,Tobias Riedlinger,Peter Stehr,Bennet Böddecker,Daniel Kondermann,Matthias Rottmann*

Main category: cs.LG

TL;DR: 提出概率标签传播方法估算标签不确定性，减少标注成本并达新水平


<details>
  <summary>Details</summary>
Motivation: 感知任务中安全AI面临挑战，缺乏高质量标签数据，标注有不确定性且众包不实用

Method: 引入概率标签传播方法，基于假设特征空间标签平滑，用图扩散方法传播单标注，证明收敛性并给出可扩展实现

Result: 相比基线，大幅降低常见图像数据集达到所需标签质量的标注预算，在数据中心图像分类基准上达新水平

Conclusion: 所提方法有效降低标注成本，在图像数据集和基准上表现优异

Abstract: Safe artificial intelligence for perception tasks remains a major challenge, partly due to the lack of data with high-quality labels. Annotations themselves are subject to aleatoric and epistemic uncertainty, which is typically ignored during annotation and evaluation. While crowdsourcing enables collecting multiple annotations per image to estimate these uncertainties, this approach is impractical at scale due to the required annotation effort. We introduce a probabilistic label spreading method that provides reliable estimates of aleatoric and epistemic uncertainty of labels. Assuming label smoothness over the feature space, we propagate single annotations using a graph-based diffusion method. We prove that label spreading yields consistent probability estimators even when the number of annotations per data point converges to zero. We present and analyze a scalable implementation of our method. Experimental results indicate that, compared to baselines, our approach substantially reduces the annotation budget required to achieve a desired label quality on common image datasets and achieves a new state of the art on the Data-Centric Image Classification benchmark.

</details>


### [155] [Stochastic Decision Horizons for Constrained Reinforcement Learning](https://arxiv.org/abs/2602.04599)
*Nikola Milosevic,Leonard Franz,Daniel Haeufle,Georg Martius,Nico Scherf,Pavel Kolev*

Main category: cs.LG

TL;DR: 提出基于随机决策视野的Control as Inference公式，有两种违反语义，实验显示提升样本效率和回报 - 违规权衡。


<details>
  <summary>Details</summary>
Motivation: 传统CMDP使用加性成本约束和对偶变量阻碍离策略可扩展性。

Method: 提出基于随机决策视野的Control as Inference公式，有吸收和虚拟终止两种违反语义。

Result: 实验在标准基准上提升样本效率，有良好回报 - 违规权衡，VT - MPO在高维肌肉骨骼设置中有效扩展。

Conclusion: 所提方法有效，能提升样本效率和适应高维场景。

Abstract: Constrained Markov decision processes (CMDPs) provide a principled model for handling constraints, such as safety and other auxiliary objectives, in reinforcement learning. The common approach of using additive-cost constraints and dual variables often hinders off-policy scalability. We propose a Control as Inference formulation based on stochastic decision horizons, where constraint violations attenuate reward contributions and shorten the effective planning horizon via state-action-dependent continuation. This yields survival-weighted objectives that remain replay-compatible for off-policy actor-critic learning. We propose two violation semantics, absorbing and virtual termination, that share the same survival-weighted return but result in distinct optimization structures that lead to SAC/MPO-style policy improvement. Experiments demonstrate improved sample efficiency and favorable return-violation trade-offs on standard benchmarks. Moreover, MPO with virtual termination (VT-MPO) scales effectively to our high-dimensional musculoskeletal Hyfydy setup.

</details>


### [156] [Jacobian Regularization Stabilizes Long-Term Integration of Neural Differential Equations](https://arxiv.org/abs/2602.04608)
*Maya Janvier,Julien Salomon,Etienne Meunier*

Main category: cs.LG

TL;DR: 本文提出在训练中通过正则化NDE模型的雅可比矩阵来稳定长期积分，设计了两种正则化方法并取得成功。


<details>
  <summary>Details</summary>
Motivation: 混合模型和神经微分方程（NDE）在物理系统建模中常遇到长期积分的稳定性和准确性问题，传统展开轨迹训练成本高。

Method: 在训练中通过方向导数正则化NDE模型的雅可比矩阵，设计了已知动力学和未知动力学两种正则化方法。

Result: 两种方法成本远低于长期展开训练，成功提高了多个常微分方程和偏微分方程长期模拟的稳定性。

Conclusion: 为大规模系统的长期积分训练NDE方法提供了可能。

Abstract: Hybrid models and Neural Differential Equations (NDE) are getting increasingly important for the modeling of physical systems, however they often encounter stability and accuracy issues during long-term integration. Training on unrolled trajectories is known to limit these divergences but quickly becomes too expensive due to the need for computing gradients over an iterative process. In this paper, we demonstrate that regularizing the Jacobian of the NDE model via its directional derivatives during training stabilizes long-term integration in the challenging context of short training rollouts. We design two regularizations, one for the case of known dynamics where we can directly derive the directional derivatives of the dynamic and one for the case of unknown dynamics where they are approximated using finite differences. Both methods, while having a far lower cost compared to long rollouts during training, are successful in improving the stability of long-term simulations for several ordinary and partial differential equations, opening up the door to training NDE methods for long-term integration of large scale systems.

</details>


### [157] [Resilient Load Forecasting under Climate Change: Adaptive Conditional Neural Processes for Few-Shot Extreme Load Forecasting](https://arxiv.org/abs/2602.04609)
*Chenxi Hu,Yue Ma,Yifan Wu,Yunhe Hou*

Main category: cs.LG

TL;DR: 提出AdaCNP模型用于数据稀缺条件下的电力系统负荷概率预测，在极端天气期间表现更好。


<details>
  <summary>Details</summary>
Motivation: 极端天气会改变电力消耗行为，极端样本稀缺不规则，现有预测难以应对，易引发电力系统问题。

Method: 提出AdaCNP概率预测模型，在共享嵌入空间学习相似度，评估历史上下文与当前情况的相关性并重新加权信息。

Result: 在真实电力系统负荷数据上评估，相比基线模型减少22%均方误差，负对数似然最低，输出更可靠。

Conclusion: AdaCNP能缓解分布突变和极端样本稀缺的影响，为极端事件下电力系统提供可靠预测。

Abstract: Extreme weather can substantially change electricity consumption behavior, causing load curves to exhibit sharp spikes and pronounced volatility. If forecasts are inaccurate during those periods, power systems are more likely to face supply shortfalls or localized overloads, forcing emergency actions such as load shedding and increasing the risk of service disruptions and public-safety impacts. This problem is inherently difficult because extreme events can trigger abrupt regime shifts in load patterns, while relevant extreme samples are rare and irregular, making reliable learning and calibration challenging. We propose AdaCNP, a probabilistic forecasting model for data-scarce condition. AdaCNP learns similarity in a shared embedding space. For each target data, it evaluates how relevant each historical context segment is to the current condition and reweights the context information accordingly. This design highlights the most informative historical evidence even when extreme samples are rare. It enables few-shot adaptation to previously unseen extreme patterns. AdaCNP also produces predictive distributions for risk-aware decision-making without expensive fine-tuning on the target domain. We evaluate AdaCNP on real-world power-system load data and compare it against a range of representative baselines. The results show that AdaCNP is more robust during extreme periods, reducing the mean squared error by 22\% relative to the strongest baseline while achieving the lowest negative log-likelihood, indicating more reliable probabilistic outputs. These findings suggest that AdaCNP can effectively mitigate the combined impact of abrupt distribution shifts and scarce extreme samples, providing a more trustworthy forecasting for resilient power system operation under extreme events.

</details>


### [158] [QUATRO: Query-Adaptive Trust Region Policy Optimization for LLM Fine-tuning](https://arxiv.org/abs/2602.04620)
*Doyeon Lee,Eunyi Lyou,Hyunsoo Cho,Sookyung Kim,Joonseok Lee,Jaemoo Choi*

Main category: cs.LG

TL;DR: 提出QUATRO算法改进GRPO风格的基于强化学习的大语言模型微调算法，在数学推理基准测试上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: GRPO风格的基于强化学习的大语言模型微调算法依赖启发式信任区域近似，会导致优化行为不稳定，现有方法无法有效调节重要性比率超出裁剪范围的样本。

Method: 提出Query - Adaptive Trust - Region policy Optimization (QUATRO)算法，通过原则性优化直接实施信任区域约束。

Result: 在多种数学推理基准测试中，QUATRO在策略陈旧性增加和激进学习率下训练稳定，训练过程中熵得到良好控制。

Conclusion: QUATRO算法能实现对政策更新的明确控制和稳定的、熵可控的优化。

Abstract: GRPO-style reinforcement learning (RL)-based LLM fine-tuning algorithms have recently gained popularity. Relying on heuristic trust-region approximations, however, they can lead to brittle optimization behavior, as global importance-ratio clipping and group-wise normalization fail to regulate samples whose importance ratios fall outside the clipping range. We propose Query-Adaptive Trust-Region policy Optimization (QUATRO), which directly enforces trust-region constraints through a principled optimization. This yields a clear and interpretable objective that enables explicit control over policy updates and stable, entropy-controlled optimization, with a stabilizer terms arising intrinsically from the exact trust-region formulation. Empirically verified on diverse mathematical reasoning benchmarks, QUATRO shows stable training under increased policy staleness and aggressive learning rates, maintaining well-controlled entropy throughout training.

</details>


### [159] [RIGA-Fold: A General Framework for Protein Inverse Folding via Recurrent Interaction and Geometric Awareness](https://arxiv.org/abs/2602.04637)
*Sisi Yuan,Jiehuang Chen,Junchuang Cai,Dong Xu,Xueliang Li,Zexuan Zhu,Junkai Ji*

Main category: cs.LG

TL;DR: 提出RIGA - Fold及其增强版RIGA - Fold*框架解决蛋白质逆折叠问题，实验表明框架具竞争力，RIGA - Fold*表现优。


<details>
  <summary>Details</summary>
Motivation: 现有基于GNN的方法存在感受野受限、错过长程依赖和单遍推理范式导致误差累积的问题，需要改进。

Method: 提出RIGA - Fold框架，微观有GAU模块，宏观有全局上下文桥；引入RIGA - Fold*增强版，采用双流架构结合可训练几何特征与进化先验；实施“预测 - 循环 - 细化”策略。

Result: 在CATH 4.2、TS50和TS500基准测试中，几何框架具竞争力，RIGA - Fold*在序列恢复和结构一致性上大幅超越现有基线。

Conclusion: 所提的RIGA - Fold和RIGA - Fold*框架能有效解决现有蛋白质逆折叠方法的瓶颈，性能优越。

Abstract: Protein inverse folding, the task of predicting amino acid sequences for desired structures, is pivotal for de novo protein design. However, existing GNN-based methods typically suffer from restricted receptive fields that miss long-range dependencies and a "single-pass" inference paradigm that leads to error accumulation. To address these bottlenecks, we propose RIGA-Fold, a framework that synergizes Recurrent Interaction with Geometric Awareness. At the micro-level, we introduce a Geometric Attention Update (GAU) module where edge features explicitly serve as attention keys, ensuring strictly SE(3)-invariant local encoding. At the macro-level, we design an attention-based Global Context Bridge that acts as a soft gating mechanism to dynamically inject global topological information. Furthermore, to bridge the gap between structural and sequence modalities, we introduce an enhanced variant, RIGA-Fold*, which integrates trainable geometric features with frozen evolutionary priors from ESM-2 and ESM-IF via a dual-stream architecture. Finally, a biologically inspired ``predict-recycle-refine'' strategy is implemented to iteratively denoise sequence distributions. Extensive experiments on CATH 4.2, TS50, and TS500 benchmarks demonstrate that our geometric framework is highly competitive, while RIGA-Fold* significantly outperforms state-of-the-art baselines in both sequence recovery and structural consistency.

</details>


### [160] [MTS-JEPA: Multi-Resolution Joint-Embedding Predictive Architecture for Time-Series Anomaly Prediction](https://arxiv.org/abs/2602.04643)
*Yanan He,Yunshi Wen,Xin Wang,Tengfei Ma*

Main category: cs.LG

TL;DR: 提出MTS - JEPA架构解决多变量时间序列异常预测问题，在标准基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 多变量时间序列异常预测很重要，但现有的JEPA在应用中存在表示崩溃和无法捕捉不同时间尺度前体信号的问题。

Method: 提出MTS - JEPA架构，将多分辨率预测目标与软码本瓶颈相结合，分离瞬态冲击和长期趋势，利用码本捕捉离散状态转换。

Result: 在标准基准的实证评估中，有效防止退化解并在早期预警协议下达到了最先进的性能。

Conclusion: MTS - JEPA能有效解决现有JEPA的局限性，适用于多变量时间序列的异常预测。

Abstract: Multivariate time series underpin modern critical infrastructure, making the prediction of anomalies a vital necessity for proactive risk mitigation. While Joint-Embedding Predictive Architectures (JEPA) offer a promising framework for modeling the latent evolution of these systems, their application is hindered by representation collapse and an inability to capture precursor signals across varying temporal scales. To address these limitations, we propose MTS-JEPA, a specialized architecture that integrates a multi-resolution predictive objective with a soft codebook bottleneck. This design explicitly decouples transient shocks from long-term trends, and utilizes the codebook to capture discrete regime transitions. Notably, we find this constraint also acts as an intrinsic regularizer to ensure optimization stability. Empirical evaluations on standard benchmarks confirm that our approach effectively prevents degenerate solutions and achieves state-of-the-art performance under the early-warning protocol.

</details>


### [161] [SAFE: Stable Alignment Finetuning with Entropy-Aware Predictive Control for RLHF](https://arxiv.org/abs/2602.04651)
*Dipan Maity*

Main category: cs.LG

TL;DR: 本文提出新的用于LM - RLHF的纯在线策略演员 - 评论员RL方法SAFE，在实验中表现优于PPO，且计算开销小，适合生产部署。


<details>
  <summary>Details</summary>
Motivation: 当前PPO作为RLHF中RL部分的标准方法存在启发式动机、对KL散度约束处理随意、有奖励振荡等问题，需要频繁重启和大量超参调优。

Method: 提出SAFE算法，结合双软最小评论员进行悲观值估计和新的多层稳定框架，包括熵门控KL调节和PID控制自适应阈值，能根据奖励速度动态调整惩罚。

Result: 在3B参数模型实验中，SAFE比PPO训练平均奖励高5.15%，奖励崩溃可忽略不计，KL控制更优。

Conclusion: SAFE计算开销小，提供了可解释、抗崩溃的RLHF框架，能在保证学习速度的同时实现稳定的长期优化，适合生产部署。

Abstract: Optimization (PPO) has been positioned by recent literature as the canonical method for the RL part of RLHF. PPO performs well empirically but has a heuristic motivation and handles the KL-divergence constraint used in LM-RLHF in an ad-hoc manner and suffers form reward oscillations, entropy collapse, value function drift, and sudden policy divergence that require frequent restarts and extensive hyperparameter tuning. In this paper, we develop a new pure on policy actor-critic RL method for the LM-RLHF setting. We present SAFE (Stable Alignment Finetuning with Entropy-aware control),a novel RLHF algorithm that combines a Double Soft-Min Critic for pessimistic value estimation with a new multi-layer stabilization framework combining entropy-gated KL regulation, and PID-controlled adaptive thresholds. Unlike standard PPO's symmetric KL penalties, SAFE distinguishes high-entropy exploration from low-entropy mode collapse and adjusts penalties dynamically based on reward velocity. Experiments on a 3B parameter model show SAFE achieves +5.15\% training-average reward than PPO (0.725 vs 0.689), negligible reward crashes, and superior KL control than ppo . Our method adds minimal computational overhead and provides an interpretable, crash-resistant RLHF framework that maintains aggressive learning speed while ensuring stable long-horizon optimization suitable for production deployment. Code is available at https://github.com/ryyzn9/SAFE

</details>


### [162] [Delving into Muon and Beyond: Deep Analysis and Extensions](https://arxiv.org/abs/2602.04669)
*Xianbiao Qi,Marco Chen,Jiaquan Ye,Yelin He,Rong Xiao*

Main category: cs.LG

TL;DR: 从统一谱视角研究Muon优化器，发现RMS归一化更新更稳定，Muon是有效谱归一化但非通用最优方法。


<details>
  <summary>Details</summary>
Motivation: Muon优化器经验性能强，但底层机制及与Adam等自适应优化器关系不明，需深入研究。

Method: 将Muon视为谱变换族p = 0端点，考虑p = 1/2、1/4、1情况下变体，对一阶矩和RMS归一化梯度更新应用变换，开发耦合牛顿迭代避免显式奇异值分解。

Result: RMS归一化更新比一阶矩更新优化更稳定；谱压缩在一阶矩更新下有益，但Muon更新不总优于Adam。

Conclusion: Muon是有效谱归一化形式，并非通用更优优化方法。

Abstract: The Muon optimizer has recently attracted considerable attention for its strong empirical performance and use of orthogonalized updates on matrix-shaped parameters, yet its underlying mechanisms and relationship to adaptive optimizers such as Adam remain insufficiently understood. In this work, we aim to address these questions through a unified spectral perspective. Specifically, we view Muon as the p = 0 endpoint of a family of spectral transformations of the form U \boldsymbolΣ^{p} V' , and consider additional variants with p = 1/2 , p = 1/4 , and p = 1 . These transformations are applied to both first-moment updates, as in momentum SGD, and to root-mean-square (RMS) normalized gradient updates as in Adam. To enable efficient computation, we develop a coupled Newton iteration that avoids explicit singular value decomposition. Across controlled experiments, we find that RMS-normalized updates yield more stable optimization than first-moment updates. Moreover, while spectral compression provides strong stabilization benefits under first-moment updates, the Muon update (p = 0) does not consistently outperform Adam. These results suggest that Muon is best understood as an effective form of spectral normalization, but not a universally superior optimization method. Our source code will be released at https://github.com/Ocram7/BeyondMuon.

</details>


### [163] [REDistill: Robust Estimator Distillation for Balancing Robustness and Efficiency](https://arxiv.org/abs/2602.04677)
*Ondrej Tybl,Lukas Neumann*

Main category: cs.LG

TL;DR: 引入REDistill框架，以改进传统知识蒸馏在处理教师模型噪声时的问题，实验表明其能提升学生模型准确率且无需特定超参数调整。


<details>
  <summary>Details</summary>
Motivation: 传统知识蒸馏假设教师模型提供可靠软目标，但实际中教师预测常有噪声或过自信，现有基于修正的方法依赖特定启发式和大量超参数调整，影响泛化性。

Method: 引入基于鲁棒统计的REDistill框架，用幂散度损失替换标准知识蒸馏目标，能自适应降低不可靠教师输出的权重。

Result: 在CIFAR - 100和ImageNet - 1k上的大量实验表明，REDistill在多种教师 - 学生架构中均能持续提升学生模型准确率。

Conclusion: REDistill无需特定于模型的超参数调整，具有很强的鲁棒性和泛化能力，能处理教师模型噪声。

Abstract: Knowledge Distillation (KD) transfers knowledge from a large teacher model to a smaller student by aligning their predictive distributions. However, conventional KD formulations - typically based on Kullback-Leibler divergence - assume that the teacher provides reliable soft targets. In practice, teacher predictions are often noisy or overconfident, and existing correction-based approaches rely on ad-hoc heuristics and extensive hyper-parameter tuning, which hinders generalization. We introduce REDistill (Robust Estimator Distillation), a simple yet principled framework grounded in robust statistics. REDistill replaces the standard KD objective with a power divergence loss, a generalization of KL divergence that adaptively downweights unreliable teacher output while preserving informative logit relationships. This formulation provides a unified and interpretable treatment of teacher noise, requires only logits, integrates seamlessly into existing KD pipelines, and incurs negligible computational overhead. Extensive experiments on CIFAR-100 and ImageNet-1k demonstrate that REDistill consistently improves student accuracy in diverse teacher-student architectures. Remarkably, it achieves these gains without model-specific hyper-parameter tuning, underscoring its robustness and strong generalization to unseen teacher-student pairs.

</details>


### [164] [Identifying Intervenable and Interpretable Features via Orthogonality Regularization](https://arxiv.org/abs/2602.04718)
*Moritz Miller,Florent Draye,Bernhard Schölkopf*

Main category: cs.LG

TL;DR: 通过微调围绕固定稀疏自动编码器的语言模型，解纠缠解码器矩阵为近似正交特征，减少特征干扰和叠加，提升可解释性并支持因果干预。


<details>
  <summary>Details</summary>
Motivation: 减少特征间的干扰和叠加，提升模型特征的可解释性，实现因果干预。

Method: 围绕固定稀疏自动编码器微调语言模型，使用正交性惩罚。

Result: 实现特征解纠缠，减少干扰和叠加，随着正交性惩罚增强，特征解释距离增加，可进行孤立干预。

Conclusion: 正交性促进模块化表示，有利于因果干预，且正交化特征允许孤立干预。

Abstract: With recent progress on fine-tuning language models around a fixed sparse autoencoder, we disentangle the decoder matrix into almost orthogonal features. This reduces interference and superposition between the features, while keeping performance on the target dataset essentially unchanged. Our orthogonality penalty leads to identifiable features, ensuring the uniqueness of the decomposition. Further, we find that the distance between embedded feature explanations increases with stricter orthogonality penalty, a desirable property for interpretability. Invoking the $\textit{Independent Causal Mechanisms}$ principle, we argue that orthogonality promotes modular representations amenable to causal intervention. We empirically show that these increasingly orthogonalized features allow for isolated interventions. Our code is available under $\texttt{https://github.com/mrtzmllr/sae-icm}$.

</details>


### [165] [DMFlow: Disordered Materials Generation by Flow Matching](https://arxiv.org/abs/2602.04734)
*Liming Wu,Rui Jiao,Qi Li,Mingze Li,Songyou Li,Shifeng Jin,Wenbing Huang*

Main category: cs.LG

TL;DR: 提出针对无序晶体的生成框架DMFlow，在CSP和DNG任务上表现优于基线，为无序材料AI发现奠基。


<details>
  <summary>Details</summary>
Motivation: 多数深度生成模型只关注有序晶体，忽略无序材料，需填补该研究空白。

Method: 引入统一表示方法，采用流匹配模型生成结构组件，用带球面重参数化的黎曼流匹配框架，通过结合物理对称性和特殊消息传递方案的GNN学习向量场，用两阶段离散化程序转换权重。还发布基准数据集。

Result: 在CSP和DNG任务上，DMFlow显著优于从有序晶体生成改编的现有最佳基线。

Conclusion: 研究为AI驱动的无序材料发现提供了基础。

Abstract: The design of materials with tailored properties is crucial for technological progress. However, most deep generative models focus exclusively on perfectly ordered crystals, neglecting the important class of disordered materials. To address this gap, we introduce DMFlow, a generative framework specifically designed for disordered crystals. Our approach introduces a unified representation for ordered, Substitutionally Disordered (SD), and Positionally Disordered (PD) crystals, and employs a flow matching model to jointly generate all structural components. A key innovation is a Riemannian flow matching framework with spherical reparameterization, which ensures physically valid disorder weights on the probability simplex. The vector field is learned by a novel Graph Neural Network (GNN) that incorporates physical symmetries and a specialized message-passing scheme. Finally, a two-stage discretization procedure converts the continuous weights into multi-hot atomic assignments. To support research in this area, we release a benchmark containing SD, PD, and mixed structures curated from the Crystallography Open Database. Experiments on Crystal Structure Prediction (CSP) and De Novo Generation (DNG) tasks demonstrate that DMFlow significantly outperforms state-of-the-art baselines adapted from ordered crystal generation. We hope our work provides a foundation for the AI-driven discovery of disordered materials.

</details>


### [166] [Rationality Measurement and Theory for Reinforcement Learning Agents](https://arxiv.org/abs/2602.04737)
*Kejiang Qian,Amos Storkey,Fengxiang He*

Main category: cs.LG

TL;DR: 本文为强化学习智能体提出一套合理性度量及相关理论，定义了期望合理风险和合理风险差距，分析其组成并上界约束，理论假设获实验验证。


<details>
  <summary>Details</summary>
Motivation: 强化学习智能体的合理性这一关键属性鲜少被探索，需要进行研究。

Method: 定义完美合理行动、期望合理风险和合理风险差距，将合理风险差距分解为外在和内在成分并分别给出上界，提出关于正则化器和领域随机化的假设。

Result: 实验结果与理论提出的假设完全一致。

Conclusion: 所提出的合理性度量和理论有效，揭示了正则化器、领域随机化的好处以及环境变化的危害。

Abstract: This paper proposes a suite of rationality measures and associated theory for reinforcement learning agents, a property increasingly critical yet rarely explored. We define an action in deployment to be perfectly rational if it maximises the hidden true value function in the steepest direction. The expected value discrepancy of a policy's actions against their rational counterparts, culminating over the trajectory in deployment, is defined to be expected rational risk; an empirical average version in training is also defined. Their difference, termed as rational risk gap, is decomposed into (1) an extrinsic component caused by environment shifts between training and deployment, and (2) an intrinsic one due to the algorithm's generalisability in a dynamic environment. They are upper bounded by, respectively, (1) the $1$-Wasserstein distance between transition kernels and initial state distributions in training and deployment, and (2) the empirical Rademacher complexity of the value function class. Our theory suggests hypotheses on the benefits from regularisers (including layer normalisation, $\ell_2$ regularisation, and weight normalisation) and domain randomisation, as well as the harm from environment shifts. Experiments are in full agreement with these hypotheses. The code is available at https://github.com/EVIEHub/Rationality.

</details>


### [167] [Decomposing Query-Key Feature Interactions Using Contrastive Covariances](https://arxiv.org/abs/2602.04752)
*Andrew Lee,Yonatan Belinkov,Fernanda Viégas,Martin Wattenberg*

Main category: cs.LG

TL;DR: 研究Transformer注意力头的查询-键（QK）空间，提出对比协方差方法分解QK空间，用于识别大语言模型语义和绑定特征，并归因注意力分数。


<details>
  <summary>Details</summary>
Motivation: 缺乏理解Transformer模型为何关注特定token的工具。

Method: 提出对比协方差方法分解QK空间，并先在简化场景中研究，再应用于大语言模型。

Result: 识别出大语言模型中分类语义特征和绑定特征的可解释QK子空间，能将注意力分数归因到识别出的特征。

Conclusion: 所提方法可有效分解QK空间，帮助理解模型注意力机制。

Abstract: Despite the central role of attention heads in Transformers, we lack tools to understand why a model attends to a particular token. To address this, we study the query-key (QK) space -- the bilinear joint embedding space between queries and keys. We present a contrastive covariance method to decompose the QK space into low-rank, human-interpretable components. It is when features in keys and queries align in these low-rank subspaces that high attention scores are produced. We first study our method both analytically and empirically in a simplified setting. We then apply our method to large language models to identify human-interpretable QK subspaces for categorical semantic features and binding features. Finally, we demonstrate how attention scores can be attributed to our identified features.

</details>


### [168] [A Dual-TransUNet Deep Learning Framework for Multi-Source Precipitation Merging and Improving Seasonal and Extreme Estimates](https://arxiv.org/abs/2602.04757)
*Yuchen Ye,Zixuan Qi,Shixuan Li,Wei Qi,Yanpeng Cai,Chaoxia Yuan*

Main category: cs.LG

TL;DR: 本文提出一种基于TransUNet的双阶段多源降水融合框架DDL - MSPMF，该框架表现良好，可用于降水融合和极端事件评估。


<details>
  <summary>Details</summary>
Motivation: 多源降水产品存在空间异质性偏差和对极端降水模拟能力有限的问题，限制了其水文应用。

Method: 开发双阶段的基于TransUNet的多源降水融合框架DDL - MSPMF，第一阶段分类器估计降水发生概率，第二阶段回归器融合分类器输出和所有预测因子估计降水量。

Result: 该框架（TransUNet - TransUNet配置）季节性表现最佳，提高了单回归器设置的鲁棒性；对强降水，提高了中国东部大部分地区的公平威胁评分；对郑卅暴雨的空间模式模拟更好；在青藏高原的独立评估也验证其适用性；SHAP分析突出了降水发生概率和地表压力的重要性。

Conclusion: 所提出的框架为降水融合和极端事件评估提供了可扩展和可解释的方法。

Abstract: Multi-source precipitation products (MSPs) from satellite retrievals and reanalysis are widely used for hydroclimatic monitoring, yet spatially heterogeneous biases and limited skill for extremes still constrain their hydrologic utility. Here we develop a dual-stage TransUNet-based multi-source precipitation merging framework (DDL-MSPMF) that integrates six MSPs with four ERA5 near-surface physical predictors. A first-stage classifier estimates daily precipitation occurrence probability, and a second-stage regressor fuses the classifier outputs together with all predictors to estimate daily precipitation amount at 0.25 degree resolution over China for 2001-2020. Benchmarking against multiple deep learning and hybrid baselines shows that the TransUNet - TransUNet configuration yields the best seasonal performance (R = 0.75; RMSE = 2.70 mm/day) and improves robustness relative to a single-regressor setting. For heavy precipitation (>25 mm/day), DDL-MSPMF increases equitable threat scores across most regions of eastern China and better reproduces the spatial pattern of the July 2021 Zhengzhou rainstorm, indicating enhanced extreme-event detection beyond seasonal-mean corrections. Independent evaluation over the Qinghai-Tibet Plateau using TPHiPr further supports its applicability in data-scarce regions. SHAP analysis highlights the importance of precipitation occurrence probabilities and surface pressure, providing physically interpretable diagnostics. The proposed framework offers a scalable and explainable approach for precipitation fusion and extreme-event assessment.

</details>


### [169] [Active Asymmetric Multi-Agent Multimodal Learning under Uncertainty](https://arxiv.org/abs/2602.04763)
*Rui Liu,Pratap Tokekar,Ming Lin*

Main category: cs.LG

TL;DR: 针对多智能体异质多模态传感器系统的问题，提出A2MAML方法，实验显示其性能优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体协作框架存在推理层面受限、假设感知同质、隐式处理不确定性等问题，在传感器损坏时鲁棒性不足。

Method: 提出A2MAML方法，将各模态特征建模为带不确定性预测的随机估计，主动选择可靠的智能体 - 模态对，通过贝叶斯逆方差加权聚合信息。

Result: 在互联自动驾驶场景的协作事故检测实验中，A2MAML始终优于单智能体和协作基线，事故检测率最高提升18.7%。

Conclusion: A2MAML是一种有效的不确定性感知、模态级协作的方法，能抑制损坏或有噪声的模态，提升性能。

Abstract: Multi-agent systems are increasingly equipped with heterogeneous multimodal sensors, enabling richer perception but introducing modality-specific and agent-dependent uncertainty. Existing multi-agent collaboration frameworks typically reason at the agent level, assume homogeneous sensing, and handle uncertainty implicitly, limiting robustness under sensor corruption. We propose Active Asymmetric Multi-Agent Multimodal Learning under Uncertainty (A2MAML), a principled approach for uncertainty-aware, modality-level collaboration. A2MAML models each modality-specific feature as a stochastic estimate with uncertainty prediction, actively selects reliable agent-modality pairs, and aggregates information via Bayesian inverse-variance weighting. This formulation enables fine-grained, modality-level fusion, supports asymmetric modality availability, and provides a principled mechanism to suppress corrupted or noisy modalities. Extensive experiments on connected autonomous driving scenarios for collaborative accident detection demonstrate that A2MAML consistently outperforms both single-agent and collaborative baselines, achieving up to 18.7% higher accident detection rate.

</details>


### [170] [Generative Modeling via Drifting](https://arxiv.org/abs/2602.04770)
*Mingyang Deng,He Li,Tianhong Li,Yilun Du,Kaiming He*

Main category: cs.LG

TL;DR: 提出漂移模型范式，可在训练时演变前推分布并实现一步推理，在 ImageNet 上取得 SOTA 结果。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型在推理时需迭代执行前推行为，希望提出新范式实现一步推理。

Method: 提出漂移模型，引入漂移场控制样本移动，形成训练目标让神经网络优化器演变分布。

Result: 一步生成器在 256x256 分辨率的 ImageNet 上取得了先进结果，潜在空间 FID 为 1.54，像素空间为 1.61。

Conclusion: 该工作为高质量一步生成开辟了新机会。

Abstract: Generative modeling can be formulated as learning a mapping f such that its pushforward distribution matches the data distribution. The pushforward behavior can be carried out iteratively at inference time, for example in diffusion and flow-based models. In this paper, we propose a new paradigm called Drifting Models, which evolve the pushforward distribution during training and naturally admit one-step inference. We introduce a drifting field that governs the sample movement and achieves equilibrium when the distributions match. This leads to a training objective that allows the neural network optimizer to evolve the distribution. In experiments, our one-step generator achieves state-of-the-art results on ImageNet at 256 x 256 resolution, with an FID of 1.54 in latent space and 1.61 in pixel space. We hope that our work opens up new opportunities for high-quality one-step generation.

</details>


### [171] [Interval-Based AUC (iAUC): Extending ROC Analysis to Uncertainty-Aware Classification](https://arxiv.org/abs/2602.04775)
*Yuqi Li,Matthew M. Engelhard*

Main category: cs.LG

TL;DR: 提出适用于区间值预测的不确定性感知ROC框架，含新指标AUC_L和AUC_U，能分解ROC平面，支持选择性预测，证明AUC_L和AUC_U是理论最优AUC的上下界，实验验证框架正确性和实用性。


<details>
  <summary>Details</summary>
Motivation: 标准评估工具不适用于区间值预测，无法捕捉预测不确定性对排序性能的影响，需要新的评估框架。

Method: 提出不确定性感知ROC框架，引入AUC_L和AUC_U指标，对ROC平面进行三区域分解，支持选择性预测。

Result: 证明AUC_L和AUC_U是理论最优AUC的上下界，在真实基准数据集上验证框架正确性和实用性。

Conclusion: 所提框架适用于区间值预测模型，可用于不确定性感知评估和决策。

Abstract: In high-stakes risk prediction, quantifying uncertainty through interval-valued predictions is essential for reliable decision-making. However, standard evaluation tools like the receiver operating characteristic (ROC) curve and the area under the curve (AUC) are designed for point scores and fail to capture the impact of predictive uncertainty on ranking performance. We propose an uncertainty-aware ROC framework specifically for interval-valued predictions, introducing two new measures: $AUC_L$ and $AUC_U$. This framework enables an informative three-region decomposition of the ROC plane, partitioning pairwise rankings into correct, incorrect, and uncertain orderings. This approach naturally supports selective prediction by allowing models to abstain from ranking cases with overlapping intervals, thereby optimizing the trade-off between abstention rate and discriminative reliability. We prove that under valid class-conditional coverage, $AUC_L$ and $AUC_U$ provide formal lower and upper bounds on the theoretical optimal AUC ($AUC^*$), characterizing the physical limit of achievable discrimination. The proposed framework applies broadly to interval-valued prediction models, regardless of the interval construction method. Experiments on real-world benchmark datasets, using bootstrap-based intervals as one instantiation, validate the framework's correctness and demonstrate its practical utility for uncertainty-aware evaluation and decision-making.

</details>


### [172] [Dynamical Regimes of Multimodal Diffusion Models](https://arxiv.org/abs/2602.04780)
*Emil Albrychiewicz,Andrés Franco Valiente,Li-Ching Chen*

Main category: cs.LG

TL;DR: 提出耦合扩散模型理论框架，揭示多模态生成机制，通过实验验证并提出时间依赖耦合策略。


<details>
  <summary>Details</summary>
Motivation: 现有扩散生成模型多模态生成理论机制不明，需深入研究。

Method: 使用耦合Ornstein - Uhlenbeck过程作为模型，结合非平衡统计物理中的动态相变理论。

Result: 多模态生成由相互作用时间尺度的谱层次控制；存在“同步间隙”；推导了物种形成和崩溃时间的分析条件；耦合强度可作为光谱滤波器；通过MNIST数据集实验支持预测。

Conclusion: 提出时间依赖耦合策略，有望替代临时指导调整方法。

Abstract: Diffusion based generative models have achieved unprecedented fidelity in synthesizing high dimensional data, yet the theoretical mechanisms governing multimodal generation remain poorly understood. Here, we present a theoretical framework for coupled diffusion models, using coupled Ornstein-Uhlenbeck processes as a tractable model. By using the nonequilibrium statistical physics of dynamical phase transitions, we demonstrate that multimodal generation is governed by a spectral hierarchy of interaction timescales rather than simultaneous resolution. A key prediction is the ``synchronization gap'', a temporal window during the reverse generative process where distinct eigenmodes stabilize at different rates, providing a theoretical explanation for common desynchronization artifacts. We derive analytical conditions for speciation and collapse times under both symmetric and anisotropic coupling regimes, establishing strict bounds for coupling strength to avoid unstable symmetry breaking. We show that the coupling strength acts as a spectral filter that enforces a tunable temporal hierarchy on generation. We support these predictions through controlled experiments with diffusion models trained on MNIST datasets and exact score samplers. These results motivate time dependent coupling schedules that target mode specific timescales, offering a potential alternative to ad hoc guidance tuning.

</details>


### [173] [Legendre Memory Unit with A Multi-Slice Compensation Model for Short-Term Wind Speed Forecasting Based on Wind Farm Cluster Data](https://arxiv.org/abs/2602.04782)
*Mumin Zhang,Haochen Zhang,Xin Zhi Khoo,Yilin Zhang,Nuo Chen,Ting Zhang,Junjie Tang*

Main category: cs.LG

TL;DR: 本文提出 WMF - CPK - MSLMU 集成模型用于风电场集群短期风速预测，经测试优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 随着风电场集群整合，需充分利用具有时空相关性的集群数据实现准确、快速和稳健的风速预测，保障电力系统正常运行。

Method: 先采用加权平均滤波（WMF）对单农场风速数据去噪；创新性应用勒让德记忆单元（LMU），结合基于肯德尔秩相关系数的补偿参数（CPK）构建多切片 LMU（MSLMU）；提出 WMF - CPK - MSLMU 集成模型，含数据预处理、预测和多切片补偿三个关键模块。

Result: 不同风电场集群的测试结果表明，提出的 WMF - CPK - MSLMU 集成模型在风电场集群短期预测中有效且优于现有模型。

Conclusion: WMF - CPK - MSLMU 集成模型在风电场集群短期风速预测方面具有有效性和优越性。

Abstract: With more wind farms clustered for integration, the short-term wind speed prediction of such wind farm clusters is critical for normal operation of power systems. This paper focuses on achieving accurate, fast, and robust wind speed prediction by full use of cluster data with spatial-temporal correlation. First, weighted mean filtering (WMF) is applied to denoise wind speed data at the single-farm level. The Legendre memory unit (LMU) is then innovatively applied for the wind speed prediction, in combination with the Compensating Parameter based on Kendall rank correlation coefficient (CPK) of wind farm cluster data, to construct the multi-slice LMU (MSLMU). Finally, an innovative ensemble model WMF-CPK-MSLMU is proposed herein, with three key blocks: data pre-processing, forecasting, and multi-slice compensation. Advantages include: 1) LMU jointly models linear and nonlinear dependencies among farms to capture spatial-temporal correlations through backpropagation; 2) MSLMU enhances forecasting by using CPK-derived weights instead of random initialization, allowing spatial correlations to fully activate hidden nodes across clustered wind farms.; 3) CPK adaptively weights the compensation model in MSLMU and complements missing data spatially, to facilitate the whole model highly accurate and robust. Test results on different wind farm clusters indicate the effectiveness and superiority of proposed ensemble model WMF-CPK-MSLMU in the short-term prediction of wind farm clusters compared to the existing models.

</details>


### [174] [From independent patches to coordinated attention: Controlling information flow in vision transformers](https://arxiv.org/abs/2602.04784)
*Kieran A. Murphy*

Main category: cs.LG

TL;DR: 在视觉Transformer中让注意力传输信息可明确测量，通过插入变分信息瓶颈训练模型，在ImageNet - 100上研究分类行为和信息路由，该方法使模型更易分析和控制。


<details>
  <summary>Details</summary>
Motivation: 让视觉Transformer中注意力传输的信息成为明确、可测量的量，便于对模型进行分析和控制。

Method: 在所有通过注意力介导写入残差流的位置插入变分信息瓶颈，在不改变其他架构的情况下训练模型。

Result: 在ImageNet - 100上刻画了分类行为和信息路由在不同情况下的演变，分析了传输信息的首个注意力头。

Conclusion: 该方法使模型更易于进行机制分析，也更易于控制。

Abstract: We make the information transmitted by attention an explicit, measurable quantity in vision transformers. By inserting variational information bottlenecks on all attention-mediated writes to the residual stream -- without other architectural changes -- we train models with an explicit information cost and obtain a controllable spectrum from independent patch processing to fully expressive global attention. On ImageNet-100, we characterize how classification behavior and information routing evolve across this spectrum, and provide initial insights into how global visual representations emerge from local patch processing by analyzing the first attention heads that transmit information. By biasing learning toward solutions with constrained internal communication, our approach yields models that are more tractable for mechanistic analysis and more amenable to control.

</details>


### [175] [Team, Then Trim: An Assembly-Line LLM Framework for High-Quality Tabular Data Generation](https://arxiv.org/abs/2602.04785)
*Congjing Zhang,Ryan Feng Lin,Ruoxuan Bao,Shuai Huang*

Main category: cs.LG

TL;DR: 本文提出T²框架合成高质量表格数据，在模拟和真实数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 获取高质量表格数据劳动密集且昂贵，表格数据集存在类别不平衡、选择偏差和低保真度等问题。

Method: 引入T²框架，通过LLM协作团队合成数据，再经三阶段数据质量控制管道处理。

Result: 在模拟和真实数据集上，T²在生成高质量表格数据方面优于现有方法。

Conclusion: T²有潜力在直接数据收集不可行时支持下游模型。

Abstract: While tabular data is fundamental to many real-world machine learning (ML) applications, acquiring high-quality tabular data is usually labor-intensive and expensive. Limited by the scarcity of observations, tabular datasets often exhibit critical deficiencies, such as class imbalance, selection bias, and low fidelity. To address these challenges, building on recent advances in Large Language Models (LLMs), this paper introduces Team-then-Trim (T$^2$), a framework that synthesizes high-quality tabular data through a collaborative team of LLMs, followed by a rigorous three-stage plug-in data quality control (QC) pipeline. In T$^2$, tabular data generation is conceptualized as a manufacturing process: specialized LLMs, guided by domain knowledge, are tasked with generating different data components sequentially, and the resulting products, i.e., the synthetic data, are systematically evaluated across multiple dimensions of QC. Empirical results on both simulated and real-world datasets demonstrate that T$^2$ outperforms state-of-the-art methods in producing high-quality tabular data, highlighting its potential to support downstream models when direct data collection is practically infeasible.

</details>


### [176] [Evolving Afferent Architectures: Biologically-inspired Models for Damage-Avoidance Learning](https://arxiv.org/abs/2602.04807)
*Wolfgang Maass,Sabine Janzen,Prajvi Saxena,Sach Mukherjee*

Main category: cs.LG

TL;DR: 提出Afferent Learning框架生成计算传入轨迹作为内部风险信号用于避损学习，在生物力学数字孪生场景验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 构建一种能产生自适应内部风险信号用于避损学习的框架。

Method: 采用两级架构，外层用进化优化发现能实现有效策略学习的传入传感架构，内层用强化学习训练避损策略。

Result: 在生物力学数字孪生场景中，基于CAT的进化架构比手工设计基线更高效、更具年龄鲁棒性，高风险行动减少23%。

Conclusion: 验证了CAT信号、进化和预测差异的重要性，发布代码和数据以保证可重复性。

Abstract: We introduce Afferent Learning, a framework that produces Computational Afferent Traces (CATs) as adaptive, internal risk signals for damage-avoidance learning. Inspired by biological systems, the framework uses a two-level architecture: evolutionary optimization (outer loop) discovers afferent sensing architectures that enable effective policy learning, while reinforcement learning (inner loop) trains damage-avoidance policies using these signals. This formalizes afferent sensing as providing an inductive bias for efficient learning: architectures are selected based on their ability to enable effective learning (rather than directly minimizing damage). We provide theoretical convergence guarantees under smoothness and bounded-noise assumptions. We illustrate the general approach in the challenging context of biomechanical digital twins operating over long time horizons (multiple decades of the life-course). Here, we find that CAT-based evolved architectures achieve significantly higher efficiency and better age-robustness than hand-designed baselines, enabling policies that exhibit age-dependent behavioral adaptation (23% reduction in high-risk actions). Ablation studies validate CAT signals, evolution, and predictive discrepancy as essential. We release code and data for reproducibility.

</details>


### [177] [Beyond Rewards in Reinforcement Learning for Cyber Defence](https://arxiv.org/abs/2602.04809)
*Elizabeth Bates,Chris Hicks,Vasilios Mavroudis*

Main category: cs.LG

TL;DR: 研究评估不同奖励函数对网络安全强化学习智能体的影响，发现稀疏奖励在训练可靠性和网络防御效能上更优。


<details>
  <summary>Details</summary>
Motivation: 当前深度强化学习训练的网络防御智能体常用密集奖励函数，但可能导致次优解，需评估奖励函数结构对学习和策略行为的影响。

Method: 用多种稀疏和密集奖励函数、两个网络训练环境、不同网络规模和策略梯度及基于价值的强化学习算法进行评估，采用新的评估方法对比不同奖励函数。

Result: 若稀疏奖励与目标一致且常出现，可增强训练可靠性，生成低风险策略，且无需显式数值惩罚就能让策略更契合防御目标。

Conclusion: 稀疏奖励在网络防御智能体中效果更好。

Abstract: Recent years have seen an explosion of interest in autonomous cyber defence agents trained to defend computer networks using deep reinforcement learning. These agents are typically trained in cyber gym environments using dense, highly engineered reward functions which combine many penalties and incentives for a range of (un)desirable states and costly actions. Dense rewards help alleviate the challenge of exploring complex environments but risk biasing agents towards suboptimal and potentially riskier solutions, a critical issue in complex cyber environments. We thoroughly evaluate the impact of reward function structure on learning and policy behavioural characteristics using a variety of sparse and dense reward functions, two well-established cyber gyms, a range of network sizes, and both policy gradient and value-based RL algorithms. Our evaluation is enabled by a novel ground truth evaluation approach which allows directly comparing between different reward functions, illuminating the nuanced inter-relationships between rewards, action space and the risks of suboptimal policies in cyber environments. Our results show that sparse rewards, provided they are goal aligned and can be encountered frequently, uniquely offer both enhanced training reliability and more effective cyber defence agents with lower-risk policies. Surprisingly, sparse rewards can also yield policies that are better aligned with cyber defender goals and make sparing use of costly defensive actions without explicit reward-based numerical penalties.

</details>


### [178] [Safe Urban Traffic Control via Uncertainty-Aware Conformal Prediction and World-Model Reinforcement Learning](https://arxiv.org/abs/2602.04821)
*Joydeep Chandra,Satyam Kumar Navneet,Aleksandr Algazinov,Yong Zhang*

Main category: cs.LG

TL;DR: 提出STREAM - RL统一框架用于城市交通管理，含三种新算法，有端到端理论保证，实验效果好。


<details>
  <summary>Details</summary>
Motivation: 城市交通管理需要能同时预测未来状况、检测异常并采取安全纠正行动且提供可靠性保证的系统。

Method: 提出STREAM - RL框架，包含PU - GAT+（不确定性引导自适应共形预测器）、CRFN - BY（共形残差流网络）、LyCon - WRL+（不确定性引导安全世界模型强化学习智能体）三种算法。

Result: 在多个真实世界交通轨迹数据实验中，STREAM - RL实现91.4%覆盖效率，在验证依赖下控制FDR为4.1%，安全率提高到95.2%，奖励更高，端到端推理延迟23ms。

Conclusion: STREAM - RL是首个从预测到异常检测再到安全策略学习能传播校准不确定性并具有端到端理论保证的框架，且实验效果良好。

Abstract: Urban traffic management demands systems that simultaneously predict future conditions, detect anomalies, and take safe corrective actions -- all while providing reliability guarantees. We present STREAM-RL, a unified framework that introduces three novel algorithmic contributions: (1) PU-GAT+, an Uncertainty-Guided Adaptive Conformal Forecaster that uses prediction uncertainty to dynamically reweight graph attention via confidence-monotonic attention, achieving distribution-free coverage guarantees; (2) CRFN-BY, a Conformal Residual Flow Network that models uncertainty-normalized residuals via normalizing flows with Benjamini-Yekutieli FDR control under arbitrary dependence; and (3) LyCon-WRL+, an Uncertainty-Guided Safe World-Model RL agent with Lyapunov stability certificates, certified Lipschitz bounds, and uncertainty-propagated imagination rollouts. To our knowledge, this is the first framework to propagate calibrated uncertainty from forecasting through anomaly detection to safe policy learning with end-to-end theoretical guarantees. Experiments on multiple real-world traffic trajectory data demonstrate that STREAM-RL achieves 91.4\% coverage efficiency, controls FDR at 4.1\% under verified dependence, and improves safety rate to 95.2\% compared to 69\% for standard PPO while achieving higher reward, with 23ms end-to-end inference latency.

</details>


### [179] [The Key to State Reduction in Linear Attention: A Rank-based Perspective](https://arxiv.org/abs/2602.04852)
*Philipp Nazari,T. Konstantin Rusch*

Main category: cs.LG

TL;DR: 本文对线性注意力中的秩现象进行理论分析，提出硬件感知的剪枝方法减少模型状态大小，实验证明该框架有效。


<details>
  <summary>Details</summary>
Motivation: 近期实验表明训练后的线性注意力模型常呈现低秩结构，未充分利用模型容量，需对此现象进行理论分析并提出改进方法。

Method: 提出硬件感知的剪枝方法对键和查询矩阵进行结构剪枝，适配现有剪枝策略，提出基于秩揭示QR分解的新型剪枝方法。

Result: 在不同大小模型和下游任务的实验中，该框架能在仅使困惑度小幅增加的情况下移除50%的查询和键通道。

Conclusion: 所提状态缩减框架有效，能在减少模型状态大小的同时保持性能。

Abstract: Linear attention offers a computationally efficient yet expressive alternative to softmax attention. However, recent empirical results indicate that the state of trained linear attention models often exhibits a low-rank structure, suggesting that these models underexploit their capacity in practice. To illuminate this phenomenon, we provide a theoretical analysis of the role of rank in linear attention, revealing that low effective rank can affect retrieval error by amplifying query noise. In addition to these theoretical insights, we conjecture that the low-rank states can be substantially reduced post-training with only minimal performance degradation, yielding faster and more memory-efficient models. To this end, we propose a novel hardware-aware approach that structurally prunes key and query matrices, reducing the state size while retaining compatibility with existing CUDA kernels. We adapt several existing pruning strategies to fit our framework and, building on our theoretical analysis, propose a novel structured pruning method based on a rank-revealing QR decomposition. Our empirical results, evaluated across models of varying sizes and on various downstream tasks, demonstrate the effectiveness of our state reduction framework. We highlight that our framework enables the removal of 50% of the query and key channels at only a marginal increase in perplexity. The code for this project can be found at https://github.com/camail-official/LinearAttentionPruning.

</details>


### [180] [From Evaluation to Design: Using Potential Energy Surface Smoothness Metrics to Guide Machine Learning Interatomic Potential Architectures](https://arxiv.org/abs/2602.04861)
*Ryan Liu,Eric Qu,Tobias Kreiman,Samuel M. Blau,Aditi S. Krishnapriyan*

Main category: cs.LG

TL;DR: 引入Bond Smoothness Characterization Test (BSCT)改善机器学习原子间势(MLIPs)评估指标，成本低且能关联MD稳定性，可指导模型设计优化。


<details>
  <summary>Details</summary>
Motivation: 现有MLIPs评估方法不能发现下游模拟错误行为且计算昂贵，主要探测近平衡态，需改善评估指标。

Method: 引入BSCT，通过控制键变形探测势能面，检测非平滑性。

Result: BSCT与MD稳定性强相关，成本远低于MD；以Transformer为测试台，基于BSCT优化模型可实现低回归误差、稳定MD模拟和可靠原子属性预测。

Conclusion: BSCT可作为验证指标和模型设计代理，能发现现有基准无法有效评估的物理挑战。

Abstract: Machine Learning Interatomic Potentials (MLIPs) sometimes fail to reproduce the physical smoothness of the quantum potential energy surface (PES), leading to erroneous behavior in downstream simulations that standard energy and force regression evaluations can miss. Existing evaluations, such as microcanonical molecular dynamics (MD), are computationally expensive and primarily probe near-equilibrium states. To improve evaluation metrics for MLIPs, we introduce the Bond Smoothness Characterization Test (BSCT). This efficient benchmark probes the PES via controlled bond deformations and detects non-smoothness, including discontinuities, artificial minima, and spurious forces, both near and far from equilibrium. We show that BSCT correlates strongly with MD stability while requiring a fraction of the cost of MD. To demonstrate how BSCT can guide iterative model design, we utilize an unconstrained Transformer backbone as a testbed, illustrating how refinements such as a new differentiable $k$-nearest neighbors algorithm and temperature-controlled attention reduce artifacts identified by our metric. By optimizing model design systematically based on BSCT, the resulting MLIP simultaneously achieves a low conventional E/F regression error, stable MD simulations, and robust atomistic property predictions. Our results establish BSCT as both a validation metric and as an "in-the-loop" model design proxy that alerts MLIP developers to physical challenges that cannot be efficiently evaluated by current MLIP benchmarks.

</details>


### [181] [CRoSS: A Continual Robotic Simulation Suite for Scalable Reinforcement Learning with High Task Diversity and Realistic Physics Simulation](https://arxiv.org/abs/2602.04868)
*Yannick Denker,Alexander Gepperth*

Main category: cs.LG

TL;DR: 本文介绍基于Gazebo模拟器的新型持续强化学习(CRL)基准套件CRoSS，适用于CRL研究，具有可扩展性、高物理现实性，方便使用和复现。


<details>
  <summary>Details</summary>
Motivation: 持续强化学习需agent从任务序列学习且不忘先前策略，缺乏合适的基于真实模拟机器人的基准套件。

Method: 构建基于Gazebo的持续机器人模拟套件CRoSS，采用两种机器人平台设置多种场景，还提供容器化设置。

Result: CRoSS具有高物理现实性、易扩展性，可快速运行且能使用任意模拟传感器，报告了标准RL算法性能。

Conclusion: CRoSS适合作为可扩展、可复现的持续强化学习研究基准。

Abstract: Continual reinforcement learning (CRL) requires agents to learn from a sequence of tasks without forgetting previously acquired policies. In this work, we introduce a novel benchmark suite for CRL based on realistically simulated robots in the Gazebo simulator. Our Continual Robotic Simulation Suite (CRoSS) benchmarks rely on two robotic platforms: a two-wheeled differential-drive robot with lidar, camera and bumper sensor, and a robotic arm with seven joints. The former represent an agent in line-following and object-pushing scenarios, where variation of visual and structural parameters yields a large number of distinct tasks, whereas the latter is used in two goal-reaching scenarios with high-level cartesian hand position control (modeled after the Continual World benchmark), and low-level control based on joint angles. For the robotic arm benchmarks, we provide additional kinematics-only variants that bypass the need for physical simulation (as long as no sensor readings are required), and which can be run two orders of magnitude faster. CRoSS is designed to be easily extensible and enables controlled studies of continual reinforcement learning in robotic settings with high physical realism, and in particular allow the use of almost arbitrary simulated sensors. To ensure reproducibility and ease of use, we provide a containerized setup (Apptainer) that runs out-of-the-box, and report performances of standard RL algorithms, including Deep Q-Networks (DQN) and policy gradient methods. This highlights the suitability as a scalable and reproducible benchmark for CRL research.

</details>


### [182] [Multi-Head LatentMoE and Head Parallel: Communication-Efficient and Deterministic MoE Parallelism](https://arxiv.org/abs/2602.04870)
*Chenwei Cui,Rockwell Jackson,Benjamin Joseph Herrera,Ana María Tárano,Hannah Kerner*

Main category: cs.LG

TL;DR: 提出Multi - Head LatentMoE和Head Parallel，通信成本低、流量平衡、通信确定，训练更快，让大参数基础模型研究更易开展。


<details>
  <summary>Details</summary>
Motivation: 现有Expert Parallel分布式训练方法存在通信成本与激活专家数线性增长、负载不平衡、数据依赖通信需元数据交换等局限。

Method: 提出Multi - Head LatentMoE和Head Parallel架构与并行方式，及IO - aware路由和专家计算以加速。

Result: 相比MoE with EP，Multi - Head LatentMoE with HP训练快1.61倍，性能相同；粒度加倍时性能更高且快1.11倍。

Conclusion: 该方法让多十亿参数基础模型研究更易进行。

Abstract: Large language models have transformed many applications but remain expensive to train. Sparse Mixture of Experts (MoE) addresses this through conditional computation, with Expert Parallel (EP) as the standard distributed training method. However, EP has three limitations: communication cost grows linearly with the number of activated experts $k$, load imbalance affects latency and memory usage, and data-dependent communication requires metadata exchange. We propose Multi-Head LatentMoE and Head Parallel (HP), a new architecture and parallelism achieving $O(1)$ communication cost regardless of $k$, completely balanced traffic, and deterministic communication, all while remaining compatible with EP. To accelerate Multi-Head LatentMoE, we propose IO-aware routing and expert computation. Compared to MoE with EP, Multi-Head LatentMoE with HP trains up to $1.61\times$ faster while having identical performance. With doubled granularity, it achieves higher overall performance while still being $1.11\times$ faster. Our method makes multi-billion-parameter foundation model research more accessible.

</details>


### [183] [Rethinking the Trust Region in LLM Reinforcement Learning](https://arxiv.org/abs/2602.04879)
*Penghui Qi,Xiangxin Zhou,Zichen Liu,Tianyu Pang,Chao Du,Min Lin,Wee Sun Lee*

Main category: cs.LG

TL;DR: 指出PPO用于大语言模型微调存在缺陷，提出DPPO算法，用策略散度估计替代启发式裁剪，并引入近似方法，实验证明其更稳定高效。


<details>
  <summary>Details</summary>
Motivation: PPO核心比率裁剪机制不适合大语言模型的大词汇量情况，导致训练低效和不稳定。

Method: 提出Divergence Proximal Policy Optimization (DPPO)算法，用策略散度直接估计替代启发式裁剪，引入Binary和Top - K近似方法减少内存占用。

Result: DPPO在训练稳定性和效率上优于现有方法。

Conclusion: DPPO为基于强化学习的大语言模型微调提供了更可靠的基础。

Abstract: Reinforcement learning (RL) has become a cornerstone for fine-tuning Large Language Models (LLMs), with Proximal Policy Optimization (PPO) serving as the de facto standard algorithm. Despite its ubiquity, we argue that the core ratio clipping mechanism in PPO is structurally ill-suited for the large vocabularies inherent to LLMs. PPO constrains policy updates based on the probability ratio of sampled tokens, which serves as a noisy single-sample Monte Carlo estimate of the true policy divergence. This creates a sub-optimal learning dynamic: updates to low-probability tokens are aggressively over-penalized, while potentially catastrophic shifts in high-probability tokens are under-constrained, leading to training inefficiency and instability. To address this, we propose Divergence Proximal Policy Optimization (DPPO), which substitutes heuristic clipping with a more principled constraint based on a direct estimate of policy divergence (e.g., Total Variation or KL). To avoid huge memory footprint, we introduce the efficient Binary and Top-K approximations to capture the essential divergence with negligible overhead. Extensive empirical evaluations demonstrate that DPPO achieves superior training stability and efficiency compared to existing methods, offering a more robust foundation for RL-based LLM fine-tuning.

</details>


### [184] [Contrastive Continual Learning for Model Adaptability in Internet of Things](https://arxiv.org/abs/2602.04881)
*Ajesh Koyatan Chathoth*

Main category: cs.LG

TL;DR: 本文回顾了对比持续学习（CCL）在物联网中的应用，提出统一问题公式、目标和架构，提供评估指导并指出物联网领域的独特挑战。


<details>
  <summary>Details</summary>
Motivation: 物联网部署在非平稳、动态环境中，因素影响应用效用，持续学习可避免灾难性遗忘，对比学习能提升稳健性和样本效率，因此研究CCL在物联网中的应用。

Method: 连接算法设计与物联网系统实际，提出统一问题公式，推导包含对比和蒸馏损失的通用目标，提出面向物联网的参考架构，给出评估协议和指标指导。

Result: 完成了CCL在物联网应用的回顾、问题公式推导、架构设计及评估指导。

Conclusion: 指出物联网领域存在处理不同类型数据、概念漂移、联邦设置和能源感知训练等独特挑战。

Abstract: Internet of Things (IoT) deployments operate in nonstationary, dynamic environments where factors such as sensor drift, evolving user behavior, and heterogeneous user privacy requirements can affect application utility. Continual learning (CL) addresses this by adapting models over time without catastrophic forgetting. Meanwhile, contrastive learning has emerged as a powerful representation-learning paradigm that improves robustness and sample efficiency in a self-supervised manner. This paper reviews the usage of \emph{contrastive continual learning} (CCL) for IoT, connecting algorithmic design (replay, regularization, distillation, prompts) with IoT system realities (TinyML constraints, intermittent connectivity, privacy). We present a unifying problem formulation, derive common objectives that blend contrastive and distillation losses, propose an IoT-oriented reference architecture for on-device, edge, and cloud-based CCL, and provide guidance on evaluation protocols and metrics. Finally, we highlight open unique challenges with respect to the IoT domain, such as spanning tabular and streaming IoT data, concept drift, federated settings, and energy-aware training.

</details>


### [185] [Protein Autoregressive Modeling via Multiscale Structure Generation](https://arxiv.org/abs/2602.04883)
*Yanru Qu,Cheng-Yen Hsieh,Zaixiang Zheng,Ge Liu,Quanquan Gu*

Main category: cs.LG

TL;DR: 提出蛋白质自回归建模（PAR）框架用于蛋白质主链生成，有三个关键组件，缓解曝光偏差问题，有零样本泛化能力，在无条件生成基准测试表现良好。


<details>
  <summary>Details</summary>
Motivation: 开发一种多尺度自回归框架用于蛋白质主链生成，解决自回归模型的曝光偏差问题，实现灵活的蛋白质结构生成。

Method: PAR包含多尺度下采样操作、自回归变压器和基于流的主链解码器，采用噪声上下文学习和计划采样缓解曝光偏差。

Result: PAR具有强零样本泛化能力，在无条件生成基准测试中有效学习蛋白质分布，生成高质量主链，有良好的缩放行为。

Conclusion: PAR是一个有前景的蛋白质结构生成框架。

Abstract: We present protein autoregressive modeling (PAR), the first multi-scale autoregressive framework for protein backbone generation via coarse-to-fine next-scale prediction. Using the hierarchical nature of proteins, PAR generates structures that mimic sculpting a statue, forming a coarse topology and refining structural details over scales. To achieve this, PAR consists of three key components: (i) multi-scale downsampling operations that represent protein structures across multiple scales during training; (ii) an autoregressive transformer that encodes multi-scale information and produces conditional embeddings to guide structure generation; (iii) a flow-based backbone decoder that generates backbone atoms conditioned on these embeddings. Moreover, autoregressive models suffer from exposure bias, caused by the training and the generation procedure mismatch, and substantially degrades structure generation quality. We effectively alleviate this issue by adopting noisy context learning and scheduled sampling, enabling robust backbone generation. Notably, PAR exhibits strong zero-shot generalization, supporting flexible human-prompted conditional generation and motif scaffolding without requiring fine-tuning. On the unconditional generation benchmark, PAR effectively learns protein distributions and produces backbones of high design quality, and exhibits favorable scaling behavior. Together, these properties establish PAR as a promising framework for protein structure generation.

</details>


### [186] [Rethinking the Design Space of Reinforcement Learning for Diffusion Models: On the Importance of Likelihood Estimation Beyond Loss Design](https://arxiv.org/abs/2602.04663)
*Jaemoo Choi,Yuchen Zhu,Wei Guo,Petr Molodyk,Bo Yuan,Jinbin Bai,Yi Xin,Molei Tao,Yongxin Chen*

Main category: cs.LG

TL;DR: 论文对扩散和流模型强化学习设计空间系统分析，表明基于ELBO的模型似然估计器是关键，在多任务验证有效且提高效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法未深入研究似然估计对算法性能的影响，扩散模型难直接应用策略梯度方法。

Method: 对强化学习设计空间的三个因素解耦分析，采用基于ELBO且只从最终生成样本计算的模型似然估计器。

Result: 在多个奖励基准上验证发现，特定似然估计器影响更大，方法在90 GPU小时内将GenEval分数从0.24提高到0.95，比FlowGRPO快4.6倍，比DiffusionNFT快2倍。

Conclusion: 基于ELBO的模型似然估计器是RL优化主导因素，方法在多任务有效且高效。

Abstract: Reinforcement learning has been widely applied to diffusion and flow models for visual tasks such as text-to-image generation. However, these tasks remain challenging because diffusion models have intractable likelihoods, which creates a barrier for directly applying popular policy-gradient type methods. Existing approaches primarily focus on crafting new objectives built on already heavily engineered LLM objectives, using ad hoc estimators for likelihood, without a thorough investigation into how such estimation affects overall algorithmic performance. In this work, we provide a systematic analysis of the RL design space by disentangling three factors: i) policy-gradient objectives, ii) likelihood estimators, and iii) rollout sampling schemes. We show that adopting an evidence lower bound (ELBO) based model likelihood estimator, computed only from the final generated sample, is the dominant factor enabling effective, efficient, and stable RL optimization, outweighing the impact of the specific policy-gradient loss functional. We validate our findings across multiple reward benchmarks using SD 3.5 Medium, and observe consistent trends across all tasks. Our method improves the GenEval score from 0.24 to 0.95 in 90 GPU hours, which is $4.6\times$ more efficient than FlowGRPO and $2\times$ more efficient than the SOTA method DiffusionNFT without reward hacking.

</details>


### [187] [Let Experts Feel Uncertainty: A Multi-Expert Label Distribution Approach to Probabilistic Time Series Forecasting](https://arxiv.org/abs/2602.04678)
*Zhen Zhou,Zhirui Wang,Qi Hong,Yunyang Shi,Ziyuan Gu,Zhiyuan Liu*

Main category: cs.LG

TL;DR: 提出Multi - Expert Learning Distributional Labels (LDL)框架解决时间序列预测问题，在M5数据集上表现优于基线方法，能平衡预测准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统点预测方法无法捕捉时间序列数据固有不确定性，现有概率方法难以平衡计算效率和可解释性，需要新方法。

Method: 提出Multi - Expert LDL和Pattern - Aware LDL - MoE两种互补方法，通过混合专家架构和分布学习能力解决问题，用MMD进行不确定性量化。

Result: 在M5数据集的聚合销售数据上评估，连续Multi - Expert LDL整体性能最佳，Pattern - Aware LDL - MoE通过组件分析有更好可解释性。

Conclusion: 所提框架成功平衡预测准确性和可解释性，适用于对性能和可操作洞见都有要求的实际预测应用。

Abstract: Time series forecasting in real-world applications requires both high predictive accuracy and interpretable uncertainty quantification. Traditional point prediction methods often fail to capture the inherent uncertainty in time series data, while existing probabilistic approaches struggle to balance computational efficiency with interpretability. We propose a novel Multi-Expert Learning Distributional Labels (LDL) framework that addresses these challenges through mixture-of-experts architectures with distributional learning capabilities. Our approach introduces two complementary methods: (1) Multi-Expert LDL, which employs multiple experts with different learned parameters to capture diverse temporal patterns, and (2) Pattern-Aware LDL-MoE, which explicitly decomposes time series into interpretable components (trend, seasonality, changepoints, volatility) through specialized sub-experts. Both frameworks extend traditional point prediction to distributional learning, enabling rich uncertainty quantification through Maximum Mean Discrepancy (MMD). We evaluate our methods on aggregated sales data derived from the M5 dataset, demonstrating superior performance compared to baseline approaches. The continuous Multi-Expert LDL achieves the best overall performance, while the Pattern-Aware LDL-MoE provides enhanced interpretability through component-wise analysis. Our frameworks successfully balance predictive accuracy with interpretability, making them suitable for real-world forecasting applications where both performance and actionable insights are crucial.

</details>


### [188] [Billion-Scale Graph Foundation Models](https://arxiv.org/abs/2602.04768)
*Maya Bechler-Speicher,Yoel Gottlieb,Andrey Isakov,David Abensur,Ami Tavory,Daniel Haimovich,Ido Guy,Udi Weinsberg*

Main category: cs.LG

TL;DR: 提出GraphBFF构建十亿参数图基础模型，给出图的神经缩放定律，框架经评估在下游任务表现出色，并探讨关键挑战与机遇。


<details>
  <summary>Details</summary>
Motivation: 扩展基础模型范式到通用真实世界图面临挑战，需要构建适用于任意异构、十亿规模图的图基础模型。

Method: 提出GraphBFF Transformer架构，提供数据批处理、预训练和微调等具体方法。

Result: 呈现图的神经缩放定律，14亿参数的GraphBFF Transformer在下游任务中取得显著零样本和探测性能，包括少样本情况。

Conclusion: 讨论了使图基础模型成为工业规模图学习实用基础面临的关键挑战和开放机遇。

Abstract: Graph-structured data underpins many critical applications. While foundation models have transformed language and vision via large-scale pretraining and lightweight adaptation, extending this paradigm to general, real-world graphs is challenging. In this work, we present Graph Billion- Foundation-Fusion (GraphBFF): the first end-to-end recipe for building billion-parameter Graph Foundation Models (GFMs) for arbitrary heterogeneous, billion-scale graphs. Central to the recipe is the GraphBFF Transformer, a flexible and scalable architecture designed for practical billion-scale GFMs. Using the GraphBFF, we present the first neural scaling laws for general graphs and show that loss decreases predictably as either model capacity or training data scales, depending on which factor is the bottleneck. The GraphBFF framework provides concrete methodologies for data batching, pretraining, and fine-tuning for building GFMs at scale. We demonstrate the effectiveness of the framework with an evaluation of a 1.4 billion-parameter GraphBFF Transformer pretrained on one billion samples. Across ten diverse, real-world downstream tasks on graphs unseen during training, spanning node- and link-level classification and regression, GraphBFF achieves remarkable zero-shot and probing performance, including in few-shot settings, with large margins of up to 31 PRAUC points. Finally, we discuss key challenges and open opportunities for making GFMs a practical and principled foundation for graph learning at industrial scale.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [189] [Landscape-aware Automated Algorithm Design: An Efficient Framework for Real-world Optimization](https://arxiv.org/abs/2602.04529)
*Haoran Yin,Shuaiqun Pan,Zhao Wei,Jian Cheng Wong,Yew-Soon Ong,Anna V. Kononova,Thomas Bäck,Niki van Stein*

Main category: cs.NE

TL;DR: 提出一种高效框架解耦算法发现与高成本评估，可减少高成本评估，适用于计算密集优化挑战。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型驱动的自动算法设计方法需大量目标问题评估，在实际优化任务中因高计算成本而不实用。

Method: 结合遗传编程函数生成器与大语言模型驱动的进化算法设计器，通过代理函数景观特征与实际问题的相似度指导遗传编程函数生成器的进化方向。

Result: 在多个实际问题中验证了框架的有效性，能发现高性能算法并显著减少昂贵评估。

Conclusion: 该方法为基于大语言模型的自动算法设计应用于计算密集型实际优化挑战提供了途径。

Abstract: The advent of Large Language Models (LLMs) has opened new frontiers in automated algorithm design, giving rise to numerous powerful methods. However, these approaches retain critical limitations: they require extensive evaluation of the target problem to guide the search process, making them impractical for real-world optimization tasks, where each evaluation consumes substantial computational resources. This research proposes an innovative and efficient framework that decouples algorithm discovery from high-cost evaluation. Our core innovation lies in combining a Genetic Programming (GP) function generator with an LLM-driven evolutionary algorithm designer. The evolutionary direction of the GP-based function generator is guided by the similarity between the landscape characteristics of generated proxy functions and those of real-world problems, ensuring that algorithms discovered via proxy functions exhibit comparable performance on real-world problems. Our method enables deep exploration of the algorithmic space before final validation while avoiding costly real-world evaluations. We validated the framework's efficacy across multiple real-world problems, demonstrating its ability to discover high-performance algorithms while substantially reducing expensive evaluations. This approach shows a path to apply LLM-based automated algorithm design to computationally intensive real-world optimization challenges.

</details>


### [190] [Real-time processing of analog signals on accelerated neuromorphic hardware](https://arxiv.org/abs/2602.04582)
*Yannik Stradmann,Johannes Schemmel,Mihai A. Petrovici,Laura Kriener*

Main category: cs.NE

TL;DR: 提出直接模拟信号注入方法用于神经形态系统的感官处理，利用BrainScaleS - 2平台实现了从感官输入到物理动作的全片上处理。


<details>
  <summary>Details</summary>
Motivation: 传统神经形态系统的感官处理方法存在多余且耗能的模数和数模转换，需要一种更高效的方法。

Method: 使用加速的BrainScaleS - 2混合信号神经形态研究平台，将其直接与麦克风和伺服电机驱动的执行器连接，利用尖峰神经网络将双耳时间差转化为空间代码。

Result: 首次实现将连续值传感器数据直接注入BrainScaleS - 2 ASIC的模拟计算单元，并用其嵌入式微处理器进行执行器控制，还实现了对系统编程以实时定位并使伺服电机与瞬态噪声峰值的空间方向对齐。

Conclusion: 直接模拟信号注入方法适合高效的近传感器处理，可构建全片上处理流程。

Abstract: Sensory processing with neuromorphic systems is typically done by using either event-based sensors or translating input signals to spikes before presenting them to the neuromorphic processor. Here, we offer an alternative approach: direct analog signal injection eliminates superfluous and power-intensive analog-to-digital and digital-to-analog conversions, making it particularly suitable for efficient near-sensor processing. We demonstrate this by using the accelerated BrainScaleS-2 mixed-signal neuromorphic research platform and interfacing it directly to microphones and a servo-motor-driven actuator. Utilizing BrainScaleS-2's 1000-fold acceleration factor, we employ a spiking neural network to transform interaural time differences into a spatial code and thereby predict the location of sound sources. Our primary contributions are the first demonstrations of direct, continuous-valued sensor data injection into the analog compute units of the BrainScaleS-2 ASIC, and actuator control using its embedded microprocessors. This enables a fully on-chip processing pipeline$\unicode{x2014}$from sensory input handling, via spiking neural network processing to physical action. We showcase this by programming the system to localize and align a servo motor with the spatial direction of transient noise peaks in real-time.

</details>


### [191] [Evolutionary Mapping of Neural Networks to Spatial Accelerators](https://arxiv.org/abs/2602.04717)
*Alessandro Pierro,Jonathan Timcheck,Jason Yik,Marius Lindauer,Eyke Hüllermeier,Marcel Wever*

Main category: cs.NE

TL;DR: 本文提出针对神经形态加速器的进化硬件在环映射框架，自动化计算图到分布式处理元素的映射，在Intel Loihi 2上评估，减少了延迟并提高了能效。


<details>
  <summary>Details</summary>
Motivation: 空间加速器充分发挥架构优势需专家手动将计算图映射到分布式处理元素，过程复杂，因此需自动化该过程。

Method: 将映射挑战构建为黑盒优化问题，引入第一个针对神经形态加速器的进化硬件在环映射框架。

Result: 在Intel Loihi 2上，相比默认启发式方法，两种稀疏多层感知机网络总延迟最多减少35%；在多芯片系统中能效最多提高40%。

Conclusion: 该方法能让无深厚硬件知识的用户更高效部署工作负载，具有良好的可扩展性和能效表现。

Abstract: Spatial accelerators, composed of arrays of compute-memory integrated units, offer an attractive platform for deploying inference workloads with low latency and low energy consumption. However, fully exploiting their architectural advantages typically requires careful, expert-driven mapping of computational graphs to distributed processing elements. In this work, we automate this process by framing the mapping challenge as a black-box optimization problem. We introduce the first evolutionary, hardware-in-the-loop mapping framework for neuromorphic accelerators, enabling users without deep hardware knowledge to deploy workloads more efficiently. We evaluate our approach on Intel Loihi 2, a representative spatial accelerator featuring 152 cores per chip in a 2D mesh. Our method achieves up to 35% reduction in total latency compared to default heuristics on two sparse multi-layer perceptron networks. Furthermore, we demonstrate the scalability of our approach to multi-chip systems and observe an up to 40% improvement in energy efficiency, without explicitly optimizing for it.

</details>


### [192] [Impact of diversity on bounded archives for multi-objective local search](https://arxiv.org/abs/2602.04745)
*Amadeu A. Coco,Cyprien Borée,Julien Baste,Laetitia Jourdan,Lucien Mousin*

Main category: cs.NE

TL;DR: 本文解决多目标优化问题元启发式算法发展的两大挑战，采用有界存档和创新的解空间多样性算法，新算法效果优于现有算法，为提升元启发式算法效率提供方向。


<details>
  <summary>Details</summary>
Motivation: 解决多目标优化问题元启发式算法面临的非支配解指数增长和搜索集中于帕累托前沿子集的问题。

Method: 采用有界存档管理非支配解数量，深入研究现有解多样性算法，引入增强解空间多样性的创新方法。

Result: 新提出的汉明距离存档算法在多目标局部搜索中效果优于文献中的自适应网格存档和超体积存档算法。

Conclusion: 新算法为提高解决多目标优化问题的元启发式算法整体效率提供了有前景的途径。

Abstract: This work tackles two critical challenges related to the development of metaheuristics for Multi-Objective Optimization Problems (MOOPs): the exponential growth of non-dominated solutions and the tendency of metaheuristics to disproportionately concentrate their search on a subset of the Pareto Front. To counteract the first, bounded archives are employed as a strategic mechanism for effectively managing the increasing number of non-dominated solutions. Addressing the second challenge involves an in-depth exploration of solution diversity algorithms found in existing literature. Upon recognizing that current approaches predominantly center on diversity within the objective space, this research introduces innovative methods specifically designed to enhance diversity in the solution space. Results demonstrate the efficacy of the Hamming Distance Archiving Algorithm, one of the newly proposed algorithms for multi-objective local search, surpassing the performance of the Adaptive Grid Archiving and the Hypervolume Archiving, both drawn from the literature. This outcome suggests a promising avenue for enhancing the overall efficiency of metaheuristics employed for solving MOOPs.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [193] [A-Graph: A Unified Graph Representation for At-Will Simulation across System Stacks](https://arxiv.org/abs/2602.04847)
*Daniel Price,Prabhu Vellaisamy,Patricia Gonzalez,George Michelogiannakis,John P. Shen,Di Wu*

Main category: cs.PF

TL;DR: 文章引入Architecture - Graph (Agraph)统一系统表示，提出Archx框架实现Agraph，能跨技术、架构和应用进行设计空间探索，且模拟精度高。


<details>
  <summary>Details</summary>
Motivation: 现有设计空间探索（DSE）方法存在局限性，如特定领域性、灵活性差、对领域专业知识要求高等，需要能跨技术、架构和应用的模拟方法。

Method: 引入统一系统表示的Agraph，提出实现Agraph的Archx框架，有易用编程接口和基于范围的指标检索。

Result: 案例研究表明Agraph能跨技术、架构和应用，模拟精度高。

Conclusion: Agraph和Archx可作为随意模拟性能和成本的基础。

Abstract: As computer systems continue to diversify across technologies, architectures, applications, and beyond, the relevant design space has become larger and more complex. Given such trends, design space exploration (DSE) at early stages is critical to ensure agile development towards optimal performance and cost. Industry-grade EDA tools directly take in RTL code and report accurate results, but do not perform DSE. Recent works have attempted to explore the design space via simulation. However, most of these works are domain-specific and constrain the space that users are allowed to explore, offering limited flexibility between technologies, architecture, and applications. Moreover, they often demand high domain expertise to ensure high accuracy. To enable simulation that is agnostic to technology, architecture, and application at any granularity, we introduce Architecture-Graph (Agraph), a graph that unifies the system representation surrounding any arbitrary application, software, architecture, and circuit. Such a unified representation distinguishes Agraph from prior works, which focus on a single stack, allowing users to freely explore the design space across system stacks. To fully unleash the potential of Agraph, we further present Archx, a framework that implements Agraph. Archx is user-friendly in two ways. First, Archx has an easy-to-use programming interface to automatically generate and sweep design points under user constraints, boosting the programmability. Second, Archx adopts scope-based metric retrieval to analyze and understand each design point at any user-preferred hierarchy, enhancing the explainability. We conduct case studies that demonstrate Agraph's generalization across technologies, architecture, and applications with high simulation accuracy. Overall, we argue that Agraph and Archx serve as a foundation to simulate both performance and cost at will.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [194] [Accountability in Open Source Software Ecosystems: Workshop Report](https://arxiv.org/abs/2602.04026)
*Nandini Sharma,Thomas Bock,Rich Bowen,Sayeed Choudhury,Brian Fitzgerald,Matt Germonprez,Jim Herbsleb,James Howison,Tom Hughes,Min Kyung Lee,Stephanie Lieggi,Andreas Liesenfeld,Georg Link,Nicholas Matsakis,Audris Mockus,Narayan Ramasubbu,Christopher Robinson,Gregorio Robles,Nithya Ruff,Sonali Shah,Igor Steinmacher,Bogdan Vasilescu,Stephen Walli,Christopher Yoo*

Main category: cs.SE

TL;DR: 文章围绕开源软件生态系统中利益相关者的问责问题，组织专家学者和从业者开展研讨会。


<details>
  <summary>Details</summary>
Motivation: 开源软件生态系统中利益相关者需求多样、未知且有时冲突，不清楚社区如何识别、与利益相关者互动及对其需求负责。

Method: 召集24位研究和从事开源软件社区工作的专家学者和从业者，举办名为‘问责与开源软件生态系统’的探索性研讨会。

Result: 未提及明确结果。

Conclusion: 未提及明确结论，研讨会旨在引发关于问责在开源软件生态系统中作用的讨论，激发研究议程和利益相关者参与的想法。

Abstract: Open source software ecosystems are composed of a variety of stakeholders including but not limited to non-profit organizations, volunteer contributors, users, and corporations. The needs and motivations of these stakeholders are often diverse, unknown, and sometimes even conflicting given the engagement and investment of both volunteers and corporate actors. Given this, it is not clear how open source communities identify and engage with their stakeholders, understand their needs, and hold themselves accountable to those needs. We convened 24 expert scholars and practitioners studying and working with open source software communities for an exploratory workshop discussion on these ideas. The workshop titled "Accountability and Open Source Software Ecosystems" was organized on Oct 14-15 on campus in Carnegie Mellon University, Pittsburgh, PA. The purpose of this in-person workshop was to initiate conversations that explore important and urgent questions related to the role of accountability in open source software ecosystems, and to inspire an exciting research agenda and meaningful stakeholder engagement ideas for practitioners.

</details>


### [195] [Exploring the Potential of Large Language Models in Simulink-Stateflow Mutant Generation](https://arxiv.org/abs/2602.04066)
*Pablo Valle,Shaukat Ali,Aitor Arrieta*

Main category: cs.SE

TL;DR: 文章探索用大语言模型为Simulink - Stateflow模型生成高质量特定领域突变体，开发自动化流程评估不同策略，结果显示LLM生成突变体速度更快、质量更高，还提供开源工具和数据集。


<details>
  <summary>Details</summary>
Motivation: 传统突变分析方法在Simulink - Stateflow模型中存在生成冗余、等效或不可执行突变体的问题，且已有方法受训练数据和可扩展性限制，受大语言模型进展启发展开研究。

Method: 开发自动化流程，将模型转换为JSON表示，系统评估八种大语言模型的不同突变和提示策略。

Result: LLM生成突变体比手动基线方法快13倍，等效和重复突变体更少，突变体质量更高，少样本提示结合中低温值效果最佳。

Conclusion: 证明了大语言模型在Simulink - Stateflow模型突变体生成方面的有效性，提供开源工具和数据集促进该领域研究。

Abstract: Mutation analysis is a powerful technique for assessing test-suite adequacy, yet conventional approaches suffer from generating redundant, equivalent, or non-executable mutants. These challenges are particularly amplified in Simulink-Stateflow models due to the hierarchical structure these models have, which integrate continuous dynamics with discrete-event behaviors and are widely deployed in safety-critical Cyber-Physical Systems (CPSs). While prior work has explored machine learning and manually engineered mutation operators, these approaches remain constrained by limited training data and scalability issues. Motivated by recent advances in Large Language Models (LLMs), we investigate their potential to generate high-quality, domain-specific mutants for Simulink-Stateflow models. We develop an automated pipeline that converts Simulink-Stateflow models to structured JSON representations and systematically evaluates different mutation and prompting strategies across eight state-of-the-art LLMs. Through a comprehensive empirical study involving 38,400 LLM-generated mutants across four Simulink-Stateflow models, we demonstrate that LLMs generate mutants up to 13x faster than a manually engineered mutation-based baseline while producing significantly fewer equivalent and duplicate mutants and consistently achieving superior mutant quality. Moreover, our analysis reveals that few-shot prompting combined with low-to-medium temperature values yields optimal results. We provide an open-source prototype tool and release our complete dataset to facilitate reproducibility and advance future research in this domain.

</details>


### [196] [I Can't Believe It's Not a Valid Exploit](https://arxiv.org/abs/2602.04165)
*Derin Gezgin,Amartya Das,Shinhae Kim,Zhengdong Huang,Nevena Stojkovic,Claire Wang*

Main category: cs.SE

TL;DR: 本文开发PoC - Gym框架评估大语言模型生成Java安全漏洞PoC的效果，发现静态分析指导可提升成功率，但多数PoC无效，当前验证机制难检测误导性结果。


<details>
  <summary>Details</summary>
Motivation: 已有方法表明额外指导可提升大语言模型生成PoC的效果，需进一步评估其有效性。

Method: 开发PoC - Gym框架，用Claude Sonnet 4、GPT - 5 Medium和gpt - oss - 20b运行该框架，用静态分析工具指导评估PoC生成成功率，并手动检查结果。

Result: 使用静态分析指导和标准生成PoC的成功率比先前基线FaultLine高21%，但手动检查发现71.5%的PoC无效。

Conclusion: 基于大语言模型的PoC生成报告的成功率可能有显著误导性，当前验证机制难以检测。

Abstract: Recently Large Language Models (LLMs) have been used in security vulnerability detection tasks including generating proof-of-concept (PoC) exploits. A PoC exploit is a program used to demonstrate how a vulnerability can be exploited. Several approaches suggest that supporting LLMs with additional guidance can improve PoC generation outcomes, motivating further evaluation of their effectiveness. In this work, we develop PoC-Gym, a framework for PoC generation for Java security vulnerabilities via LLMs and systematic validation of generated exploits. Using PoC-Gym, we evaluate whether the guidance from static analysis tools improves the PoC generation success rate and manually inspect the resulting PoCs. Our results from running PoC-Gym with Claude Sonnet 4, GPT-5 Medium, and gpt-oss-20b show that using static analysis for guidance and criteria lead to 21% higher success rates than the prior baseline, FaultLine. However, manual inspection of both successful and failed PoCs reveals that 71.5% of the PoCs are invalid. These results show that the reported success of LLM-based PoC generation can be significantly misleading, which is hard to detect with current validation mechanisms.

</details>


### [197] [SOGPTSpotter: Detecting ChatGPT-Generated Answers on Stack Overflow](https://arxiv.org/abs/2602.04185)
*Suyu Ma,Chunyang Chen,Hourieh Khalajzadeh,John Grundy*

Main category: cs.SE

TL;DR: 提出SOGPTSpotter方法，利用Siamese神经网络检测Stack Overflow上ChatGPT生成的答案，表现优于基线方法，经多项实验验证有效，有实际应用价值。


<details>
  <summary>Details</summary>
Motivation: Stack Overflow上ChatGPT生成的答案增多，可能带来错误信息，且检测此类内容是挑战。

Method: 引入SOGPTSpotter，采用Siamese神经网络，利用BigBird模型和Triplet loss，使用人类答案、参考答案和ChatGPT答案三元组。

Result: 该方法在识别ChatGPT生成的Stack Overflow回答上优于GPTZero等基线；消融实验证明模型有效；评估了文本长度等因素；真实案例中帮助版主识别并移除可疑答案。

Conclusion: SOGPTSpotter方法能有效检测Stack Overflow上ChatGPT生成的答案，有实际应用价值。

Abstract: Stack Overflow is a popular Q&A platform where users ask technical questions and receive answers from a community of experts. Recently, there has been a significant increase in the number of answers generated by ChatGPT, which can lead to incorrect and unreliable information being posted on the site. While Stack Overflow has banned such AI-generated content, detecting whether a post is ChatGPT-generated remains a challenging task. We introduce a novel approach, SOGPTSpotter, that employs Siamese Neural Networks, leveraging the BigBird model and the Triplet loss, to detect ChatGPT-generated answers on Stack Overflow. We use triplets of human answers, reference answers, and ChatGPT answers. Our empirical evaluation reveals that our approach outperforms well-established baselines like GPTZero, DetectGPT, GLTR, BERT, RoBERTa, and GPT-2 in identifying ChatGPT-synthesized Stack Overflow responses. We also conducted an ablation study to show the effectiveness of our model. Additional experiments were conducted to assess various factors, including the impact of text length, the model's robustness against adversarial attacks, and its generalization capabilities across different domains and large language models. We also conducted a real-world case study on Stack Overflow. Using our tool's recommendations, Stack Overflow moderators were able to identify and take down ChatGPT-suspected generated answers, demonstrating the practical applicability and effectiveness of our approach.

</details>


### [198] [Semantic Consensus Decoding: Backdoor Defense for Verilog Code Generation](https://arxiv.org/abs/2602.04195)
*Guang Yang,Xing Hu,Xiang Chen,Xin Xia*

Main category: cs.SE

TL;DR: 本文提出语义共识解码（SCD）方法防御Verilog代码生成大语言模型的后门攻击，降低攻击成功率且不影响生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型用于Verilog代码生成时易受后门攻击，现有主动防御需训练数据，被动防御难应对语义隐蔽触发器，因此需新防御方法。

Method: 提出SCD方法，包括从用户规范中识别关键要求的功能需求提取和基于完整用户规范与提取功能需求自适应融合输出分布的共识解码，若分布差异大则抑制可疑组件。

Result: 通过对三种代表性后门攻击的大量实验，SCD将平均攻击成功率从89%降至3%以下，对生成质量影响可忽略。

Conclusion: SCD能有效防御Verilog代码生成大语言模型的后门攻击，且对生成质量影响小。

Abstract: Large language models (LLMs) for Verilog code generation are increasingly adopted in hardware design, yet remain vulnerable to backdoor attacks where adversaries inject malicious triggers during training to induce vulnerable hardware designs. Unlike patchable software vulnerabilities, hardware trojans become irreversible once fabricated, making remediation extremely costly or impossible. Existing active defenses require access to training data, impractical for third-party LLM users, while passive defenses struggle against semantically stealthy triggers that naturally blend into design specifications. In this paper, we hypothesize that under the requirements of both effectiveness and stealthiness, attackers are strongly biased toward embedding triggers in non-functional requirements (e.g., style modifiers, quality descriptors) rather than functional specifications that determine hardware behavior. Exploiting this insight, we propose Semantic Consensus Decoding (SCD), an inference-time passive defense with two key components: (1) functional requirement extraction that identifies essential requirements from user specifications, and (2) consensus decoding that adaptively fuses output distributions based on full user specifications and extracted functional requirements. When these distributions diverge significantly, SCD automatically suppresses suspicious components. Extensive experiments with three representative backdoor attacks demonstrate that SCD reduces average attack success rate from 89% to under 3% with negligible impact on generation quality.

</details>


### [199] [Why Agentic-PRs Get Rejected: A Comparative Study of Coding Agents](https://arxiv.org/abs/2602.04226)
*Sota Nakashima,Yuta Ishimoto,Masanari Kondo,Shane Mclntosh,Yasutaka Kamei*

Main category: cs.SE

TL;DR: 本文研究了不同编码智能体生成的PR被拒原因对比，发现7种仅存在于Agentic - PR的拒签模式，有大量未明确拒因的PR，并提出减轻该问题的启发式方法。


<details>
  <summary>Details</summary>
Motivation: 不同编码智能体用途不同导致可能有特定失败模式，但此前未比较不同智能体生成的Agentic - PR被拒原因差异，因此开展研究。

Method: 检查来自AIDev数据集的654个被拒PR，涵盖五个编码智能体和人类基线。

Result: 发现7种仅存在于Agentic - PR的拒签模式，如对AI代码不信任；存在智能体特定模式；67.9%被拒PR缺乏明确反馈。

Conclusion: 提出一组启发式方法减少缺乏明确反馈的被拒PR比例，为后续研究提供预处理步骤。

Abstract: Agentic coding -- software development workflows in which autonomous coding agents plan, implement, and submit code changes with minimal human involvement -- is rapidly gaining traction. Prior work has shown that Pull Requests (PRs) produced using coding agents (Agentic-PRs) are accepted less often than PRs that are not labeled as agentic (Human-PRs). The rejection reasons for a single agent (Claude Code) have been explored, but a comparison of how rejection reasons differ between Agentic-PRs generated by different agents has not yet been performed. This comparison is important since different coding agents are often used for different purposes, which can lead to agent-specific failure patterns. In this paper, we inspect 654 rejected PRs from the AIDev dataset covering five coding agents, as well as a human baseline. Our results show that seven rejection modes occur only in Agentic-PRs, including distrust of AI-generated code. We also observe agent-specific patterns (e.g., automated withdrawal of inactive PRs by Devin), reflecting differences in how agents are configured and used in practice. Notably, a large proportion of rejected PRs (67.9%) lack explicit reviewer feedback, making their rejection reasons difficult to determine. To mitigate this issue, we propose a set of heuristics that reduce the proportion of such cases, offering a practical preprocessing step for future studies of PR rejection in agentic coding.

</details>


### [200] [ProxyWar: Dynamic Assessment of LLM Code Generation in Game Arenas](https://arxiv.org/abs/2602.04296)
*Wenjun Peng,Xinyu Wang,Qi Wu*

Main category: cs.SE

TL;DR: 提出ProxyWar框架评估代码生成质量，发现基准分数与实际表现差异，强调基于竞争评估的必要性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型代码生成评估受静态基准和简单指标限制，需更好评估方法。

Method: 将大语言模型生成的代理嵌入多样竞争游戏环境，结合自动测试、迭代代码修复和多智能体锦标赛评估。

Result: 发现基准分数和动态环境实际表现有显著差异，揭示了被忽视的局限性和改进机会。

Conclusion: 强调需要更丰富、基于竞争的代码生成评估，ProxyWar为相关研究奠定基础。

Abstract: Large language models (LLMs) have revolutionized automated code generation, yet the evaluation of their real-world effectiveness remains limited by static benchmarks and simplistic metrics. We present ProxyWar, a novel framework that systematically assesses code generation quality by embedding LLM-generated agents within diverse, competitive game environments. Unlike existing approaches, ProxyWar evaluates not only functional correctness but also the operational characteristics of generated programs, combining automated testing, iterative code repair, and multi-agent tournaments to provide a holistic view of program behavior. Applied to a range of state-of-the-art coders and games, our approach uncovers notable discrepancies between benchmark scores and actual performance in dynamic settings, revealing overlooked limitations and opportunities for improvement. These findings highlight the need for richer, competition-based evaluation of code generation. Looking forward, ProxyWar lays a foundation for research into LLM-driven algorithm discovery, adaptive problem solving, and the study of practical efficiency and robustness, including the potential for models to outperform hand-crafted agents. The project is available at https://github.com/xinke-wang/ProxyWar.

</details>


### [201] [Model-Driven Legacy System Modernization at Scale](https://arxiv.org/abs/2602.04341)
*Tobias Böhm,Jens Guan Su Tien,Mohini Nonnenmann,Tom Schoonbaert,Bart Carpels,Andreas Biesdorf*

Main category: cs.SE

TL;DR: 提出模型驱动的遗留系统现代化方法，经应用评估证明可半自动化迁移，虽有挑战但能提升可维护和可扩展性，推广性好。


<details>
  <summary>Details</summary>
Motivation: 解决遗留系统现代化问题，提升系统可维护性和可扩展性，减少现代化的风险和工作量。

Method: 在遗留代码库和现代目标平台间插入技术无关的中间模型，采用分析、丰富、合成和过渡四阶段流程处理系统工件。

Result: 应用于大型工业应用，核心用户界面组件和页面结构可半自动化迁移到现代Web栈，保留功能和非功能质量，新代码库可维护和扩展性更高。

Conclusion: 基于模型的抽象降低风险和工作量，支持遗留应用可扩展、可追溯的现代化，方法可推广并促进迁移模式复用。

Abstract: This experience report presents a model-driven approach to legacy system modernization that inserts an enriched, technology-agnostic intermediate model between the legacy codebase and the modern target platform, and reports on its application and evaluation. The four-stage process of analysis, enrichment, synthesis, and transition systematically extracts, abstracts, and transforms system artifacts. We apply our approach to a large industrial application built on legacy versions of the .NET Framework and ASP.NET MVC and show that core user interface components and page structures can be migrated semi-automatically to a modern web stack while preserving functional behavior and essential non-functional qualities. By consolidating architectural knowledge into explicit model representations, the resulting codebase exhibits higher maintainability and extensibility, thereby improving developer experience. Although automation is effective for standard patterns, migration of bespoke layout composites remains challenging and requires targeted manual adaptation. Our contributions are: (i) an end-to-end model-driven process, (ii) an enriched intermediate model that captures structure, dependencies, and semantic metadata, (iii) transformation rules that preserve functional behavior and essential non-functional qualities, and (iv) application and evaluation of the approach in an industrial setting. Overall, model-based abstractions reduce risk and effort while supporting scalable, traceable modernization of legacy applications. Our approach generalizes to comparable modernization contexts and promotes reuse of migration patterns.

</details>


### [202] [Generative AI in Systems Engineering: A Framework for Risk Assessment of Large Language Models](https://arxiv.org/abs/2602.04358)
*Stefan Otten,Philipp Reis,Philipp Rigoll,Joshua Ransiek,Tobias Schürmann,Jacob Langner,Eric Sax*

Main category: cs.SE

TL;DR: 本文介绍了LLM风险评估框架（LRF），用于评估系统工程环境中LLM的应用，可确定风险水平，支持组织采取相应策略，推动在复杂工程环境中风险感知地采用LLM。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在工程生命周期有巨大潜力，但组织在评估其使用风险方面面临挑战，导致集成不一致等问题，因此需要评估框架。

Method: 引入LRF，将基于LLM的应用按自主性和影响两个基本维度分类，结合这两个维度确定相应风险水平。

Result: 通过分类支持组织确定适当的验证策略、人工监督水平和所需对策，确保安全透明部署。

Conclusion: LRF为复杂工程环境中风险感知地采用LLM提供基础，是系统工程中标准化AI保证实践的第一步。

Abstract: The increasing use of Large Language Models (LLMs) offers significant opportunities across the engineering lifecycle, including requirements engineering, software development, process optimization, and decision support. Despite this potential, organizations face substantial challenges in assessing the risks associated with LLM use, resulting in inconsistent integration, unknown failure modes, and limited scalability. This paper introduces the LLM Risk Assessment Framework (LRF), a structured approach for evaluating the application of LLMs within Systems Engineering (SE) environments. The framework classifies LLM-based applications along two fundamental dimensions: autonomy, ranging from supportive assistance to fully automated decision making, and impact, reflecting the potential severity of incorrect or misleading model outputs on engineering processes and system elements. By combining these dimensions, the LRF enables consistent determination of corresponding risk levels across the development lifecycle. The resulting classification supports organizations in identifying appropriate validation strategies, levels of human oversight, and required countermeasures to ensure safe and transparent deployment. The framework thereby helps align the rapid evolution of AI technologies with established engineering principles of reliability, traceability, and controlled process integration. Overall, the LRF provides a basis for risk-aware adoption of LLMs in complex engineering environments and represents a first step toward standardized AI assurance practices in systems engineering.

</details>


### [203] [AgenticAKM : Enroute to Agentic Architecture Knowledge Management](https://arxiv.org/abs/2602.04445)
*Rudra Dhar,Karthik Vaidhyanathan,Vasudeva Varma*

Main category: cs.SE

TL;DR: 提出 AgenticAKM 方法解决 AKM 自动化问题，经用户研究验证有效。


<details>
  <summary>Details</summary>
Motivation: AKM 过程繁琐未被广泛采用，单提示方法用于 AKM 有局限，需新方法实现自动化。

Method: 提出 AgenticAKM 方法，将架构恢复和文档编制问题分解为子任务，由专业代理协作生成 AK。

Result: 通过对 29 个代码仓库的用户研究，表明该方法生成的 ADRs 更好。

Conclusion: AgenticAKM 是自动化 AKM 有前景且实用的方法。

Abstract: Architecture Knowledge Management (AKM) is crucial for maintaining current and comprehensive software Architecture Knowledge (AK) in a software project. However AKM is often a laborious process and is not adopted by developers and architects. While LLMs present an opportunity for automation, a naive, single-prompt approach is often ineffective, constrained by context limits and an inability to grasp the distributed nature of architectural knowledge. To address these limitations, we propose an Agentic approach for AKM, AgenticAKM, where the complex problem of architecture recovery and documentation is decomposed into manageable sub-tasks. Specialized agents for architecture Extraction, Retrieval, Generation, and Validation collaborate in a structured workflow to generate AK. To validate we made an initial instantiation of our approach to generate Architecture Decision Records (ADRs) from code repositories. We validated our approach through a user study with 29 repositories. The results demonstrate that our agentic approach generates better ADRs, and is a promising and practical approach for automating AKM.

</details>


### [204] [What's in a Benchmark? The Case of SWE-Bench in Automated Program Repair](https://arxiv.org/abs/2602.04449)
*Matias Martinez,Xavier Franch*

Main category: cs.SE

TL;DR: 本文全面研究SWE - Bench的两个排行榜，分析提交者、背后产品、使用的LLMs和方法开放性，发现多数提交来自行业，专有LLMs占主导，研究为未来研究提供指导。


<details>
  <summary>Details</summary>
Motivation: 对SWE - Bench的两个公共排行榜进行全面研究以了解其生态情况。

Method: 分析Lite排行榜的79个条目和Verified排行榜的133个条目。

Result: 两个排行榜上多数条目来自行业，学术贡献也有竞争力；专有LLMs占主导，Claude系列表现出色。

Conclusion: 研究结果可为未来基准驱动研究带来更高的透明度和多样性提供指导。

Abstract: The rapid progress in Automated Program Repair (APR) has been fueled by advances in AI, particularly large language models (LLMs) and agent-based systems. SWE-Bench is a benchmark designed to evaluate repair systems using real issues mined from popular open-source Python repositories. Its public leaderboards-SWE-Bench Lite and Verified-have become central platforms for tracking progress and comparing solutions. In this paper, we present the first comprehensive study of these two leaderboards, examining who is submitting solutions, the products behind the submissions, the LLMs employed, and the openness of the approaches. We analyze 79 entries submitted to Lite leaderboard and 133 to Verified. Our results show that most entries on both leaderboards originate from industry, particularly small companies and large publicly traded companies. These submissions often achieve top results, although academic contributions-typically open source-also remain competitive. We also find a clear dominance of proprietary LLMs, especially Claude family, with state-of-the-art results on both leaderboards currently achieved by Claude 4 Sonnet. These findings offer insights into the SWE-Bench ecosystem that can guide greater transparency and diversity in future benchmark-driven research.

</details>


### [205] [A Framework of Critical Success Factors for Agile Software Development](https://arxiv.org/abs/2602.04467)
*Ridewaan Hanslo,Maureen Tanner*

Main category: cs.SE

TL;DR: 通过系统文献综述分析53项研究确定敏捷项目关键成功因素，得出理论框架并为后续研究提供方向


<details>
  <summary>Details</summary>
Motivation: 尽管敏捷软件开发流行，但实现项目持续成功仍有挑战，需找出关键成功因素

Method: 采用主题合成与内容分析方法分析53项主要研究

Result: 得出21个关键成功因素，分为组织、人员、技术、流程和项目五个主题，团队有效性和项目管理最常被提及

Conclusion: 研究结果有助于开发理论框架，为研究者和从业者提供见解，指导未来用定量方法验证结果和框架

Abstract: Despite the popularity of Agile software development, achieving consistent project success remains challenging. This systematic literature review identifies critical success factors (CSFs) in Agile projects by analyzing 53 primary studies. Employing thematic synthesis with content analysis, our analysis yielded 21 CSFs categorized into five themes: organizational, people, technical, process, and project. Team effectiveness and project management emerged as the most frequently cited CSFs, highlighting the importance of people and process factors. These interpreted themes and factors contributed to the development of a theoretical framework to identify how these factors contribute to project success. This study offers valuable insights for researchers and practitioners, guiding future research to validate these findings and test the proposed framework using quantitative methods.

</details>


### [206] [Towards Structured, State-Aware, and Execution-Grounded Reasoning for Software Engineering Agents](https://arxiv.org/abs/2602.04640)
*Tse-Hsun,Chen*

Main category: cs.SE

TL;DR: 当前软件工程（SE）代理为反应式，长程推理有挑战，需转向结构化、状态感知和基于执行的推理并给出开发路线图。


<details>
  <summary>Details</summary>
Motivation: 当前SE代理是反应式，长程推理存在困难，为进一步推动SE代理发展。

Method: 提出应转向结构化、状态感知和基于执行的推理，阐述其如何帮助SE代理进行长程推理。

Result: 无明确结果阐述

Conclusion: 为开发能有效执行现实任务的下一代SE代理提供了初步路线图。

Abstract: Software Engineering (SE) agents have shown promising abilities in supporting various SE tasks. Current SE agents remain fundamentally reactive, making decisions mainly based on conversation history and the most recent response. However, this reactive design provides no explicit structure or persistent state within the agent's memory, making long-horizon reasoning challenging. As a result, SE agents struggle to maintain a coherent understanding across reasoning steps, adapt their hypotheses as new evidence emerges, or incorporate execution feedback into the mental reasoning model of the system state.
  In this position paper, we argue that, to further advance SE agents, we need to move beyond reactive behavior toward a structured, state-aware, and execution-grounded reasoning. We outline how explicit structure, persistent and evolving state, and the integration of execution-grounded feedback can help SE agents perform more coherent and reliable reasoning in long-horizon tasks. We also provide an initial roadmap for developing next-generation SE agents that can more effectively perform real-world tasks.

</details>


### [207] [Supporting software engineering tasks with agentic AI: Demonstration on document retrieval and test scenario generation](https://arxiv.org/abs/2602.04726)
*Marian Kica,Lukas Radosky,David Slivka,Karin Kubinova,Daniel Dovhun,Tomas Uhercik,Erik Bircak,Ivan Polasek*

Main category: cs.SE

TL;DR: 本文提出针对两个软件工程任务的智能AI解决方案，包括自动测试场景生成和文档检索，并暗示研究的未来方向。


<details>
  <summary>Details</summary>
Motivation: 大语言模型促使软件开发模型重新调整和思考，软件工程研究产生大量工具和方法，本文加入此潮流提出解决方案。

Method: 对于自动测试场景生成，采用专门工作代理与监督代理形成星型拓扑的方法；对于文档检索任务，每个用例由基于大语言模型的专用代理处理相关子任务。

Result: 在真实示例中展示了自动测试场景生成方法的能力；文档检索解决方案能执行多种用例。

Conclusion: 暗示了研究方向的未来前景。

Abstract: The introduction of large language models ignited great retooling and rethinking of the software development models. The ensuing response of software engineering research yielded a massive body of tools and approaches. In this paper, we join the hassle by introducing agentic AI solutions for two tasks. First, we developed a solution for automatic test scenario generation from a detailed requirements description. This approach relies on specialized worker agents forming a star topology with the supervisor agent in the middle. We demonstrate its capabilities on a real-world example. Second, we developed an agentic AI solution for the document retrieval task in the context of software engineering documents. Our solution enables performing various use cases on a body of documents related to the development of a single software, including search, question answering, tracking changes, and large document summarization. In this case, each use case is handled by a dedicated LLM-based agent, which performs all subtasks related to the corresponding use case. We conclude by hinting at the future perspectives of our line of research.

</details>


### [208] [Demonstrating ARG-V's Generation of Realistic Java Benchmarks for SV-COMP](https://arxiv.org/abs/2602.04786)
*Charles Moloney,Robert Dyer,Elena Sherman*

Main category: cs.SE

TL;DR: 本文介绍ARG - V工具用于自动生成SV - COMP格式的Java验证基准，新基准使领先Java验证器性能下降，凸显ARG - V提升评估质量潜力。


<details>
  <summary>Details</summary>
Motivation: SV - COMP竞赛结果受基准程序组成影响，新增基准需考虑对验证器行为的影响，以减少对竞赛结果有效性的外部威胁。

Method: 应用ARG - V工具自动生成SV - COMP格式的Java验证基准。

Result: 在新生成的68个现实基准上，四个领先的Java验证器在准确性和召回率上比现有基准套件表现下降。

Conclusion: ARG - V有潜力提高验证工具评估的全面性和真实性，为验证器开发者改进工具提供方向。

Abstract: The SV-COMP competition provides a state-of-the-art platform for evaluating software verification tools on a standardized set of verification tasks. Consequently, verifier development outcomes are influenced by the composition of program benchmarks included in SV-COMP. When expanding this benchmark corpus, it is crucial to consider whether newly added programs cause verifiers to exhibit behavior distinct from that observed on existing benchmarks. Doing so helps mitigate external threats to the validity of the competition's results.
  In this paper, we present the application of the ARG-V tool for automatically generating Java verification benchmarks in the SV-COMP format. We demonstrate that, on a newly generated set of 68 realistic benchmarks, all four leading Java verifiers decrease in accuracy and recall compared to their performance on the existing benchmark suite. These findings highlight the potential of ARG-V to enhance the comprehensiveness and realism of verification tool evaluation, while also providing a roadmap for verifier developers aiming to improve their tools' applicability to real-world software.

</details>


### [209] [Beyond the Control Equations: An Artifact Study of Implementation Quality in Robot Control Software](https://arxiv.org/abs/2602.04799)
*Nils Chur,Thorsten Berger,Einar Broch Johnsen,Andrzej Wąsowski*

Main category: cs.SE

TL;DR: 分析开源机器人软件中184个控制器实现，发现实现和测试存在问题，强调需改进准则和验证技术。


<details>
  <summary>Details</summary>
Motivation: 现有研究忽略控制器实际软件实现及其理论保证问题，文中展开调查。

Method: 对开源机器人软件中184个控制器实现进行调查，检查其应用场景、实现特征和测试方法。

Result: 控制器实现处理离散化随意，存在实时可靠性问题；测试实践表面，无理论保证的系统验证。

Conclusion: 需要改进实现准则和严格验证技术，以确保机器人控制器可靠性和安全性。

Abstract: A controller -- a software module managing hardware behavior -- is a key component of a typical robot system. While control theory gives safety guarantees for standard controller designs, the practical implementation of controllers in software introduces complexities that are often overlooked. Controllers are often designed in continuous space, while the software is executed in discrete space, undermining some of the theoretical guarantees. Despite extensive research on control theory and control modeling, little attention has been paid to the implementations of controllers and how their theoretical guarantees are ensured in real-world software systems. We investigate 184 real-world controller implementations in open-source robot software. We examine their application context, the implementation characteristics, and the testing methods employed to ensure correctness. We find that the implementations often handle discretization in an ad hoc manner, leading to potential issues with real-time reliability. Challenges such as timing inconsistencies, lack of proper error handling, and inadequate consideration of real-time constraints further complicate matters. Testing practices are superficial, no systematic verification of theoretical guarantees is used, leaving possible inconsistencies between expected and actual behavior. Our findings highlight the need for improved implementation guidelines and rigorous verification techniques to ensure the reliability and safety of robotic controllers in practice.

</details>


### [210] [Do Developers Read Type Information? An Eye-Tracking Study on TypeScript](https://arxiv.org/abs/2602.04824)
*Samuel W. Flint,Robert Dyer,Bonita Sharif*

Main category: cs.SE

TL;DR: 研究旨在验证开发者将类型注解用作代码内文档，通过眼动研究，发现开发者在代码总结和定位bug时，不会更频繁查看含类型注解或声明的行。


<details>
  <summary>Details</summary>
Motivation: 提供开发者将类型注解用作代码内文档的证据，帮助理解开发者如何及在何种场景使用类型信息，助力设计更好的开发工具和教育决策。

Method: 对26名本科生进行眼动研究，观察他们在TypeScript语言的代码理解和bug定位任务中是否阅读类型注解。

Result: 开发者在代码总结和定位bug任务中，不会更频繁直接查看含类型注解或声明的行。

Conclusion: 研究结果对工具开发者改进类型信息可用性、开发社区制定类型注解使用标准以及教育领域强调刻意教学阅读模式有启示意义。

Abstract: Statically-annotated types have been shown to aid developers in a number of programming tasks, and this benefit holds true even when static type checking is not used. It is hypothesized that this is because developers use type annotations as in-code documentation. In this study, we aim to provide evidence that developers use type annotations as in-code documentation. Understanding this hypothesized use will help to understand how, and in what contexts, developers use type information; additionally, it may help to design better development tools and inform educational decisions. To provide this evidence, we conduct an eye tracking study with 26 undergraduate students to determine if they read type annotations during code comprehension and bug localization in the TypeScript language. We found that developers do not look directly at lines containing type annotations or type declarations more often when they are present, in either code summarization or bug localization tasks. The results have implications for tool builders to improve the availability of type information, the development community to build good standards for use of type annotations, and education to enforce deliberate teaching of reading patterns.

</details>


### [211] [When Code Becomes Abundant: Redefining Software Engineering Around Orchestration and Verification](https://arxiv.org/abs/2602.04830)
*Karina Kohl,Luigi Carro*

Main category: cs.SE

TL;DR: 软件工程面临AI自动化和硬件能源约束压力，需围绕人类洞察力重新定义，会带来问责崩溃风险，对研、教、产有深远影响。


<details>
  <summary>Details</summary>
Motivation: 软件工程同时面临AI自动化降低代码生产成本和硬件能源约束增加故障成本的压力，传统定义已不足。

Method: 无具体方法提及，提出软件工程应围绕意图表达、架构控制和系统验证重新定义。

Result: 将软件工程从生产导向转变为以自动化下的人类判断为核心。

Conclusion: 软件工程必须重新定义，这对研究、实践和教育有深刻影响。

Abstract: Software Engineering (SE) faces simultaneous pressure from AI automation (reducing code production costs) and hardware-energy constraints (amplifying failure costs). We position that SE must redefine itself around human discernment-intent articulation, architectural control, and verification-rather than code construction. This shift introduces accountability collapse as a central risk and requires fundamental changes to research priorities, educational curricula, and industrial practices. We argue that Software Engineering, as traditionally defined around code construction and process management, is no longer sufficient. Instead, the discipline must be redefined around intent articulation, architectural control, and systematic verification. This redefinition shifts Software Engineering from a production-oriented field to one centered on human judgment under automation, with profound implications for research, practice, and education.

</details>


<div id='q-fin.RM'></div>

# q-fin.RM [[Back]](#toc)

### [212] [ASRI: An Aggregated Systemic Risk Index for Cryptocurrency Markets](https://arxiv.org/abs/2602.03874)
*Murad Farzulla,Andrew Maksakov*

Main category: q-fin.RM

TL;DR: 本文引入聚合系统性风险指数（ASRI）监测加密货币市场风险，验证了该框架有效性，能解决现有风险监测不足。


<details>
  <summary>Details</summary>
Motivation: 加密货币市场市值超3万亿美元，但缺乏统一指数监测DeFi协议和传统金融机构相互关联带来的系统性风险。

Method: 引入包含四个加权子指数的ASRI，给出各组件理论基础和量化公式，结合多个数据源数据，通过历史危机事件验证框架，还进行事件研究分析、阈值操作检测、三状态隐马尔可夫模型分析和样本外特异性测试。

Result: 事件研究分析显示四个危机有显著异常信号；阈值操作检测识别出三个事件，平均提前18天；隐马尔可夫模型识别出不同风险状态，状态持续性超94%；样本外特异性测试无假阳性。

Conclusion: ASRI框架能捕捉传统系统性风险指标无法涵盖的DeFi特定漏洞，弥补现有风险监测的关键缺口。

Abstract: Cryptocurrency markets have grown to represent over $3 trillion in capitalization, yet no unified index exists to monitor the systemic risks arising from the interconnection between decentralized finance (DeFi) protocols and traditional financial institutions. This paper introduces the Aggregated Systemic Risk Index (ASRI), a composite measure comprising four weighted sub-indices: Stablecoin Concentration Risk (30%), DeFi Liquidity Risk (25%), Contagion Risk (25%), and Regulatory Opacity Risk (20%). We derive theoretical foundations for each component, specify quantitative formulas incorporating data from DeFi Llama, Federal Reserve FRED, and on-chain analytics, and validate the framework against historical crisis events including the Terra/Luna collapse (May 2022), the Celsius/3AC contagion (June 2022), the FTX bankruptcy (November 2022), and the SVB banking crisis (March 2023). Event study analysis detects statistically significant abnormal signals for all four crises (t-statistics 5.47-32.64, all p < 0.01), though threshold-based operational detection identifies three of four events with an average lead time of 18 days. A three-regime Hidden Markov Model identifies distinct Low Risk, Moderate, and Elevated states with regime persistence exceeding 94%. Out-of-sample specificity testing on 2024-2025 data confirms zero false positives. The ASRI framework addresses a critical gap in existing risk monitoring by capturing DeFi-specific vulnerabilities -- composability risk, flash loan exposure, and tokenized real-world asset linkages -- that traditional systemic risk measures cannot accommodate.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [213] [Transcendental Regularization of Finite Mixtures:Theoretical Guarantees and Practical Limitations](https://arxiv.org/abs/2602.03889)
*Ernest Fokoué*

Main category: stat.ML

TL;DR: 引入超越正则化解决有限混合模型最大似然估计的退化问题，提出TAMD算法，有理论保证，但分类精度提升有限。


<details>
  <summary>Details</summary>
Motivation: 有限混合模型最大似然估计存在成分坍塌导致的退化问题。

Method: 引入超越正则化，采用带解析障碍函数的惩罚似然框架，提出TAMD算法。

Result: TAMD算法能稳定估计、防止坍塌，但分类精度提升有限。

Conclusion: 提供了新理论框架，也评估了实际局限性，并实现开源R包。

Abstract: Finite mixture models are widely used for unsupervised learning, but maximum likelihood estimation via EM suffers from degeneracy as components collapse. We introduce transcendental regularization, a penalized likelihood framework with analytic barrier functions that prevent degeneracy while maintaining asymptotic efficiency. The resulting Transcendental Algorithm for Mixtures of Distributions (TAMD) offers strong theoretical guarantees: identifiability, consistency, and robustness. Empirically, TAMD successfully stabilizes estimation and prevents collapse, yet achieves only modest improvements in classification accuracy-highlighting fundamental limits of mixture models for unsupervised learning in high dimensions. Our work provides both a novel theoretical framework and an honest assessment of practical limitations, implemented in an open-source R package.

</details>


### [214] [A Hitchhiker's Guide to Poisson Gradient Estimation](https://arxiv.org/abs/2602.03896)
*Michael Ibrahim,Hanqi Zhao,Eli Sennesh,Zhi Li,Anqi Wu,Jacob L. Yates,Chengrui Li,Hadi Vafaii*

Main category: stat.ML

TL;DR: 对泊松分布潜变量模型中EAT和GSM方法进行系统比较，改进EAT方法并评估，结果显示改进EAT性能更好。


<details>
  <summary>Details</summary>
Motivation: 泊松分布潜变量模型中对离散随机样本求导有挑战，此前无EAT和GSM方法的系统比较，需为从业者提供指导。

Method: 对EAT方法进行修改以保证一阶矩无偏并减少二阶矩偏差，在变分自编码器和部分可观测广义线性模型两个任务上评估EAT和GSM方法。

Result: 改进的EAT方法在各指标上整体性能更好，对超参数选择的鲁棒性更高。

Conclusion: 明确了两种方法间的权衡，为处理泊松潜变量模型的从业者提供具体建议。

Abstract: Poisson-distributed latent variable models are widely used in computational neuroscience, but differentiating through discrete stochastic samples remains challenging. Two approaches address this: Exponential Arrival Time (EAT) simulation and Gumbel-SoftMax (GSM) relaxation. We provide the first systematic comparison of these methods, along with practical guidance for practitioners. Our main technical contribution is a modification to the EAT method that theoretically guarantees an unbiased first moment (exactly matching the firing rate), and reduces second-moment bias. We evaluate these methods on their distributional fidelity, gradient quality, and performance on two tasks: (1) variational autoencoders with Poisson latents, and (2) partially observable generalized linear models, where latent neural connectivity must be inferred from observed spike trains. Across all metrics, our modified EAT method exhibits better overall performance (often comparable to exact gradients), and substantially higher robustness to hyperparameter choices. Together, our results clarify the trade-offs between these methods and offer concrete recommendations for practitioners working with Poisson latent variable models.

</details>


### [215] [Byzantine Machine Learning: MultiKrum and an optimal notion of robustness](https://arxiv.org/abs/2602.03899)
*Gilles Bareilles,Wassim Bouaziz,Julien Fageot,El-Mahdi El-Mhamdi*

Main category: stat.ML

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Aggregation rules are the cornerstone of distributed (or federated) learning in the presence of adversaries, under the so-called Byzantine threat model. They are also interesting mathematical objects from the point of view of robust mean estimation. The Krum aggregation rule has been extensively studied, and endowed with formal robustness and convergence guarantees. Yet, MultiKrum, a natural extension of Krum, is often preferred in practice for its superior empirical performance, even though no theoretical guarantees were available until now. In this work, we provide the first proof that MultiKrum is a robust aggregation rule, and bound its robustness coefficient. To do so, we introduce $κ^\star$, the optimal *robustness coefficient* of an aggregation rule, which quantifies the accuracy of mean estimation in the presence of adversaries in a tighter manner compared with previously adopted notions of robustness. We then construct an upper and a lower bound on MultiKrum's robustness coefficient. As a by-product, we also improve on the best-known bounds on Krum's robustness coefficient. We show that MultiKrum's bounds are never worse than Krum's, and better in realistic regimes. We illustrate this analysis by an experimental investigation on the quality of the lower bound.

</details>


### [216] [Learning Multi-type heterogeneous interacting particle systems](https://arxiv.org/abs/2602.03954)
*Quanjun Lang,Xiong Wang,Fei Lu,Mauro Maggioni*

Main category: stat.ML

TL;DR: 提出从多轨迹数据中联合推断异构交互粒子系统网络拓扑等的框架，用三阶段方法解决非凸混合整数优化问题，有理论保证，实验显示方法准确且抗噪。


<details>
  <summary>Details</summary>
Motivation: 解决从多轨迹数据中联合推断异构交互粒子系统网络拓扑、多类型交互内核和潜在类型分配这一具有挑战性的非凸混合整数优化问题。

Method: 采用三阶段方法，先通过矩阵感知恢复系统参数的低秩嵌入，再通过聚类识别离散交互类型，最后通过矩阵分解和后处理细化恢复网络权重矩阵和内核系数。

Result: 在合成数据集上的数值实验表明，该方法能准确重建潜在动态，且对噪声具有鲁棒性。

Conclusion: 提出的框架和方法在解决相关问题上有效，有理论保证且实验效果良好。

Abstract: We propose a framework for the joint inference of network topology, multi-type interaction kernels, and latent type assignments in heterogeneous interacting particle systems from multi-trajectory data. This learning task is a challenging non-convex mixed-integer optimization problem, which we address through a novel three-stage approach. First, we leverage shared structure across agent interactions to recover a low-rank embedding of the system parameters via matrix sensing. Second, we identify discrete interaction types by clustering within the learned embedding. Third, we recover the network weight matrix and kernel coefficients through matrix factorization and a post-processing refinement. We provide theoretical guarantees with estimation error bounds under a Restricted Isometry Property (RIP) assumption and establish conditions for the exact recovery of interaction types based on cluster separability. Numerical experiments on synthetic datasets, including heterogeneous predator-prey systems, demonstrate that our method yields an accurate reconstruction of the underlying dynamics and is robust to noise.

</details>


### [217] [Privacy utility trade offs for parameter estimation in degree heterogeneous higher order networks](https://arxiv.org/abs/2602.03948)
*Bibhabasu Mandal,Sagnik Nandy*

Main category: stat.ML

TL;DR: 本文研究β模型在局部和中心差分隐私约束下的极小极大最优参数估计问题，给出有限样本极小极大下界，提出简单估计器，并通过实验验证方法有效性。


<details>
  <summary>Details</summary>
Motivation: 在涉及关系数据集的敏感应用中，保护个体链接信息免受对抗性查询，且在许多场景下仅能获取网络节点度信息，需研究β模型参数估计的隐私效用权衡。

Method: 采用β模型，研究局部和中心差分隐私约束下极小极大最优参数估计问题，给出有限样本极小极大下界，提出简单估计器。

Result: 给出有限样本极小极大下界，提出的简单估计器在局部和中心差分隐私框架下达到这些下界（至常数和对数因子），并通过合成数据和真实通信网络实验验证方法有效性。

Conclusion: 首次全面刻画了β模型参数估计的有限样本隐私效用权衡，涵盖经典图和高阶超图模型。

Abstract: In sensitive applications involving relational datasets, protecting information about individual links from adversarial queries is of paramount importance. In many such settings, the available data are summarized solely through the degrees of the nodes in the network. We adopt the $β$ model, which is the prototypical statistical model adopted for this form of aggregated relational information, and study the problem of minimax-optimal parameter estimation under both local and central differential privacy constraints. We establish finite sample minimax lower bounds that characterize the precise dependence of the estimation risk on the network size and the privacy parameters, and we propose simple estimators that achieve these bounds up to constants and logarithmic factors under both local and central differential privacy frameworks. Our results provide the first comprehensive finite sample characterization of privacy utility trade offs for parameter estimation in $β$ models, addressing the classical graph case and extending the analysis to higher order hypergraph models. We further demonstrate the effectiveness of our methods through experiments on synthetic data and a real world communication network.

</details>


### [218] [Statistical Guarantees for Reasoning Probes on Looped Boolean Circuits](https://arxiv.org/abs/2602.03970)
*Anastasis Kratsios,Giulia Livieri,A. Martina Neuman*

Main category: stat.ML

TL;DR: 研究循环推理模型中推理探针的统计行为，证明基于GCN的推理探针泛化误差达最优速率，结果刻画计算图结构对推理统计效率的影响。


<details>
  <summary>Details</summary>
Motivation: 分析循环推理模型中推理探针在部分可观测情况下的泛化问题。

Method: 结合雪花度量嵌入技术和统计最优传输工具进行分析。

Result: 当推理探针由基于GCN的假设类参数化并查询N个节点时，最坏情况下泛化误差以至少1 - δ的概率达到最优速率O(√(log(2/δ))/√N)。

Conclusion: 结果清晰刻画了计算图的结构属性如何支配部分访问下推理的统计效率。

Abstract: We study the statistical behaviour of reasoning probes in a stylized model of looped reasoning, given by Boolean circuits whose computational graph is a perfect $ν$-ary tree ($ν\ge 2$) and whose output is appended to the input and fed back iteratively for subsequent computation rounds. A reasoning probe has access to a sampled subset of internal computation nodes, possibly without covering the entire graph, and seeks to infer which $ν$-ary Boolean gate is executed at each queried node, representing uncertainty via a probability distribution over a fixed collection of $\mathtt{m}$ admissible $ν$-ary gates. This partial observability induces a generalization problem, which we analyze in a realizable, transductive setting.
  We show that, when the reasoning probe is parameterized by a graph convolutional network (GCN)-based hypothesis class and queries $N$ nodes, the worst-case generalization error attains the optimal rate $\mathcal{O}(\sqrt{\log(2/δ)}/\sqrt{N})$ with probability at least $1-δ$, for $δ\in (0,1)$. Our analysis combines snowflake metric embedding techniques with tools from statistical optimal transport. A key insight is that this optimal rate is achievable independently of graph size, owing to the existence of a low-distortion one-dimensional snowflake embedding of the induced graph metric. As a consequence, our results provide a sharp characterization of how structural properties of the computational graph govern the statistical efficiency of reasoning under partial access.

</details>


### [219] [Fixed Budget is No Harder Than Fixed Confidence in Best-Arm Identification up to Logarithmic Factors](https://arxiv.org/abs/2602.03972)
*Kapilan Balagopalan,Yinan Li,Yao Zhao,Tuan Nguyen,Anton Daitche,Houssam Nassif,Kwang-Sung Jun*

Main category: stat.ML

TL;DR: 研究最佳臂识别问题中固定预算（FB）和固定置信（FC）设置难度关系，提出 FC2FB 算法证明 FB 复杂度不高于 FC，可改进 FB 问题样本复杂度。


<details>
  <summary>Details</summary>
Motivation: 在已知 K 臂老虎机两种设置最优样本复杂度情况下，探究通用的最佳臂识别问题中 FB 和 FC 哪个更难。

Method: 提出了一种名为 FC2FB 的元算法，将 FC 算法转化为 FB 算法。

Result: 证明 FC2FB 的样本复杂度与原 FC 算法样本复杂度在对数因子上匹配，且最优 FC 样本复杂度是最优 FB 样本复杂度的上界。

Conclusion: 揭示了 FB 和 FC 之间的基本关系，结合现有 FC 算法可改进一些 FB 问题的样本复杂度。

Abstract: The best-arm identification (BAI) problem is one of the most fundamental problems in interactive machine learning, which has two flavors: the fixed-budget setting (FB) and the fixed-confidence setting (FC).
  For $K$-armed bandits with the unique best arm, the optimal sample complexities for both settings have been settled down, and they match up to logarithmic factors.
  This prompts an interesting research question about the generic, potentially structured BAI problems: Is FB harder than FC or the other way around?
  In this paper, we show that FB is no harder than FC up to logarithmic factors.
  We do this constructively: we propose a novel algorithm called FC2FB (fixed confidence to fixed budget), which is a meta algorithm that takes in an FC algorithm $\mathcal{A}$ and turn it into an FB algorithm.
  We prove that this FC2FB enjoys a sample complexity that matches, up to logarithmic factors, that of the sample complexity of $\mathcal{A}$.
  This means that the optimal FC sample complexity is an upper bound of the optimal FB sample complexity up to logarithmic factors.
  Our result not only reveals a fundamental relationship between FB and FC, but also has a significant implication: FC2FB, combined with existing state-of-the-art FC algorithms, leads to improved sample complexity for a number of FB problems.

</details>


### [220] [Efficient Subgroup Analysis via Optimal Trees with Global Parameter Fusion](https://arxiv.org/abs/2602.04077)
*Zhongming Xie,Joseph Giorgio,Jingshen Wang*

Main category: stat.ML

TL;DR: 本文提出融合最优因果树方法用于精准健康中的亚组分析，理论和实证均显示其优于传统方法，在实际案例中也有实用价值。


<details>
  <summary>Details</summary>
Motivation: 传统基于树的递归划分方法用于亚组分析存在局限性，如贪婪启发式导致的次优划分和局部估计分裂的过拟合问题。

Method: 提出融合最优因果树方法，利用混合整数优化实现精确亚组识别，引入参数融合约束促进相关亚组间信息共享。

Result: 理论上给出样本外风险界，实证中在模拟实验里持续优于流行基线方法，在实际数据集上有临床意义洞察。

Conclusion: 所提融合最优因果树方法能有效解决传统方法局限性，提高亚组发现准确性和统计效率。

Abstract: Identifying and making statistical inferences on differential treatment effects (commonly known as subgroup analysis in clinical research) is central to precision health. Subgroup analysis allows practitioners to pinpoint populations for whom a treatment is especially beneficial or protective, thereby advancing targeted interventions. Tree based recursive partitioning methods are widely used for subgroup analysis due to their interpretability. Nevertheless, these approaches encounter significant limitations, including suboptimal partitions induced by greedy heuristics and overfitting from locally estimated splits, especially under limited sample sizes. To address these limitations, we propose a fused optimal causal tree method that leverages mixed integer optimization (MIO) to facilitate precise subgroup identification. Our approach ensures globally optimal partitions and introduces a parameter fusion constraint to facilitate information sharing across related subgroups. This design substantially improves subgroup discovery accuracy and enhances statistical efficiency. We provide theoretical guarantees by rigorously establishing out of sample risk bounds and comparing them with those of classical tree based methods. Empirically, our method consistently outperforms popular baselines in simulations. Finally, we demonstrate its practical utility through a case study on the Health and Aging Brain Study Health Disparities (HABS-HD) dataset, where our approach yields clinically meaningful insights.

</details>


### [221] [Maximin Relative Improvement: Fair Learning as a Bargaining Problem](https://arxiv.org/abs/2602.04155)
*Jiwoo Han,Moulinath Banerjee,Yuekai Sun*

Main category: stat.ML

TL;DR: 提出将群体公平视为子群体间的讨价还价问题，引入相对改进指标，有理论性质和收敛保证。


<details>
  <summary>Details</summary>
Motivation: 在多子群体中部署单一预测器时，寻求一种新的处理群体公平问题的方法。

Method: 从博弈论角度，将群体公平看作子群体间的讨价还价问题，提出相对改进指标。

Result: 揭示现有鲁棒优化方法对应经典讨价还价解，相对改进恢复了Kalai - Smorodinsky解，具有尺度不变性和个体单调性等性质，且在温和条件下有有限样本收敛保证。

Conclusion: 相对改进指标在多子群体预测器公平性问题上是一种有效的解决方案。

Abstract: When deploying a single predictor across multiple subpopulations, we propose a fundamentally different approach: interpreting group fairness as a bargaining problem among subpopulations. This game-theoretic perspective reveals that existing robust optimization methods such as minimizing worst-group loss or regret correspond to classical bargaining solutions and embody different fairness principles. We propose relative improvement, the ratio of actual risk reduction to potential reduction from a baseline predictor, which recovers the Kalai-Smorodinsky solution. Unlike absolute-scale methods that may not be comparable when groups have different potential predictability, relative improvement provides axiomatic justification including scale invariance and individual monotonicity. We establish finite-sample convergence guarantees under mild conditions.

</details>


### [222] [Attack-Resistant Uniform Fairness for Linear and Smooth Contextual Bandits](https://arxiv.org/abs/2602.04125)
*Qingwen Zhang,Wenjia Wang*

Main category: stat.ML

TL;DR: 本文研究上下文多臂老虎机问题的公平性及抗操纵性，提出新算法并进行对抗分析，设计鲁棒变体，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现代系统中上下文多臂老虎机的部署可能造成不公平曝光，影响平台可持续性和供应商信任，且存在被策略性操纵的风险。

Method: 提出新算法实现线性和光滑奖励函数的（近）极小极大最优遗憾，设计结合自适应探索和误差补偿阈值的鲁棒变体。

Result: 揭示基于功绩的公平性易受信号操纵，对手能用少量预算造成特定公平性失败；鲁棒变体在攻击下获得极小极大最优遗憾界，保持公平性。

Conclusion: 数值实验和实际案例表明算法能兼顾公平性和效率。

Abstract: Modern systems, such as digital platforms and service systems, increasingly rely on contextual bandits for online decision-making; however, their deployment can inadvertently create unfair exposure among arms, undermining long-term platform sustainability and supplier trust. This paper studies the contextual bandit problem under a uniform $(1-δ)$-fairness constraint, and addresses its unique vulnerabilities to strategic manipulation. The fairness constraint ensures that preferential treatment is strictly justified by an arm's actual reward across all contexts and time horizons, using uniformity to prevent statistical loopholes. We develop novel algorithms that achieve (nearly) minimax-optimal regret for both linear and smooth reward functions, while maintaining strong $(1-\tilde{O}(1/T))$-fairness guarantees, and further characterize the theoretically inherent yet asymptotically marginal "price of fairness". However, we reveal that such merit-based fairness becomes uniquely susceptible to signal manipulation. We show that an adversary with a minimal $\tilde{O}(1)$ budget can not only degrade overall performance as in traditional attacks, but also selectively induce insidious fairness-specific failures while leaving conspicuous regret measures largely unaffected. To counter this, we design robust variants incorporating corruption-adaptive exploration and error-compensated thresholding. Our approach yields the first minimax-optimal regret bounds under $C$-budgeted attack while preserving $(1-\tilde{O}(1/T))$-fairness. Numerical experiments and a real-world case demonstrate that our algorithms sustain both fairness and efficiency.

</details>


### [223] [Provable Target Sample Complexity Improvements as Pre-Trained Models Scale](https://arxiv.org/abs/2602.04233)
*Kazuto Fukuchi,Ryuichiro Hataya,Kota Matsui*

Main category: stat.ML

TL;DR: 本文引入新框架对预训练模型进行理论研究，证明其能降低下游任务样本复杂度，为经验性缩放定律提供理论依据。


<details>
  <summary>Details</summary>
Motivation: 现有对预训练模型的理论研究无法解释大预训练模型可降低下游学习样本复杂度的现象。

Method: 引入受参数高效微调方法启发的新框架“caulking”进行理论研究。

Result: 证明改进后的预训练模型能降低下游任务的样本复杂度。

Conclusion: 为预训练模型大小与下游性能的经验性缩放定律提供了理论依据，这是现有结果未涵盖的。

Abstract: Pre-trained models have become indispensable for efficiently building models across a broad spectrum of downstream tasks. The advantages of pre-trained models have been highlighted by empirical studies on scaling laws, which demonstrate that larger pre-trained models can significantly reduce the sample complexity of downstream learning. However, existing theoretical investigations of pre-trained models lack the capability to explain this phenomenon. In this paper, we provide a theoretical investigation by introducing a novel framework, caulking, inspired by parameter-efficient fine-tuning (PEFT) methods such as adapter-based fine-tuning, low-rank adaptation, and partial fine-tuning. Our analysis establishes that improved pre-trained models provably decrease the sample complexity of downstream tasks, thereby offering theoretical justification for the empirically observed scaling laws relating pre-trained model size to downstream performance, a relationship not covered by existing results.

</details>


### [224] [Geometry-Aware Optimal Transport: Fast Intrinsic Dimension and Wasserstein Distance Estimation](https://arxiv.org/abs/2602.04335)
*Ferdinand Genans,Olivier Wintenberger*

Main category: stat.ML

TL;DR: 本文针对机器学习中大规模最优传输（OT）问题，引入采样误差和内在维度的新型估计器，解决采样误差方面的问题，数值实验表明方法有效。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习中大规模最优传输问题时，离散化误差收敛率受数据内在维度影响，关键瓶颈是采样误差的了解和控制。

Method: 引入采样误差和内在维度的新型估计器，包括利用半对偶OT泛函的无调参采样误差估计器和基于采样误差估计器多尺度衰减的快速内在维度估计器。

Result: 该框架在实践中具有计算和统计优势，能量化离散化误差收敛率、校准熵正则化、引入去偏的Wasserstein距离估计器。

Conclusion: 几何感知的流程能有效缓解离散化误差瓶颈，同时保持计算效率。

Abstract: Solving large scale Optimal Transport (OT) in machine learning typically relies on sampling measures to obtain a tractable discrete problem. While the discrete solver's accuracy is controllable, the rate of convergence of the discretization error is governed by the intrinsic dimension of our data. Therefore, the true bottleneck is the knowledge and control of the sampling error. In this work, we tackle this issue by introducing novel estimators for both sampling error and intrinsic dimension. The key finding is a simple, tuning-free estimator of $\text{OT}_c(ρ, \hatρ)$ that utilizes the semi-dual OT functional and, remarkably, requires no OT solver. Furthermore, we derive a fast intrinsic dimension estimator from the multi-scale decay of our sampling error estimator. This framework unlocks significant computational and statistical advantages in practice, enabling us to (i) quantify the convergence rate of the discretization error, (ii) calibrate the entropic regularization of Sinkhorn divergences to the data's intrinsic geometry, and (iii) introduce a novel, intrinsic-dimension-based Richardson extrapolation estimator that strongly debiases Wasserstein distance estimation. Numerical experiments demonstrate that our geometry-aware pipeline effectively mitigates the discretization error bottleneck while maintaining computational efficiency.

</details>


### [225] [A Bandit-Based Approach to Educational Recommender Systems: Contextual Thompson Sampling for Learner Skill Gain Optimization](https://arxiv.org/abs/2602.04347)
*Lukas De Kerpel,Arthur Thuy,Dries F. Benoit*

Main category: stat.ML

TL;DR: 本文介绍一种为学习者生成个性化练习序列的方法，用在线数学辅导平台数据验证其有效性，该方法能实现大规模个性化练习。


<details>
  <summary>Details</summary>
Motivation: 运筹学、管理科学和分析学教学向数字环境转变，大量不同学习者使个性化练习提供困难。

Method: 利用学习者及其过往表现信息，在每一步选择最可能提升目标技能理解的练习，以每次练习前后估算技能水平变化衡量学习进展。

Result: 该方法推荐的练习与更大技能提升相关，能有效适应不同学习者差异。

Conclusion: 该框架可实现大规模个性化练习，突出有强学习价值练习，助力教师识别需额外支持的学习者。

Abstract: In recent years, instructional practices in Operations Research (OR), Management Science (MS), and Analytics have increasingly shifted toward digital environments, where large and diverse groups of learners make it difficult to provide practice that adapts to individual needs. This paper introduces a method that generates personalized sequences of exercises by selecting, at each step, the exercise most likely to advance a learner's understanding of a targeted skill. The method uses information about the learner and their past performance to guide these choices, and learning progress is measured as the change in estimated skill level before and after each exercise. Using data from an online mathematics tutoring platform, we find that the approach recommends exercises associated with greater skill improvement and adapts effectively to differences across learners. From an instructional perspective, the framework enables personalized practice at scale, highlights exercises with consistently strong learning value, and helps instructors identify learners who may benefit from additional support.

</details>


### [226] [Anytime-Valid Conformal Risk Control](https://arxiv.org/abs/2602.04364)
*Bror Hultberg,Dave Zachariah,Antônio H. Ribeiro*

Main category: stat.ML

TL;DR: 本文扩展了预测集的误差控制，使其在累积增长的校准数据集上高概率有效，并给出理论保证和实际验证。


<details>
  <summary>Details</summary>
Motivation: 标准公式下预测集的误差仅在固定大小的校准数据集上平均控制，本文要将控制扩展到累积增长的校准数据集在任意时间点都高概率有效。

Method: 使用基于分位数的论证推导保证，建立匹配的下界。

Result: 所提出框架适用于分布偏移场景，保证渐近紧密。

Conclusion: 通过模拟和真实数值例子证明了方法的实际性能。

Abstract: Prediction sets provide a means of quantifying the uncertainty in predictive tasks. Using held out calibration data, conformal prediction and risk control can produce prediction sets that exhibit statistically valid error control in a computationally efficient manner. However, in the standard formulations, the error is only controlled on average over many possible calibration datasets of fixed size. In this paper, we extend the control to remain valid with high probability over a cumulatively growing calibration dataset at any time point. We derive such guarantees using quantile-based arguments and illustrate the applicability of the proposed framework to settings involving distribution shift. We further establish a matching lower bound and show that our guarantees are asymptotically tight. Finally, we demonstrate the practical performance of our methods through both simulations and real-world numerical examples.

</details>


### [227] [Performative Learning Theory](https://arxiv.org/abs/2602.04402)
*Julian Rodemann,Unai Fischer-Abaigar,James Bailie,Krikamol Muandet*

Main category: stat.ML

TL;DR: 研究表现性预测对样本和总体的影响及模型泛化问题，通过嵌入统计学习理论证明泛化边界，揭示改变世界与学习数据间的权衡，还提出在扭曲样本上再训练可提升泛化保证，并以德国就业培训分配为例。


<details>
  <summary>Details</summary>
Motivation: 探究表现性预测影响下模型在样本和总体上的泛化能力，如基于现有用户对新用户进行预测的效果。

Method: 将表现性预测嵌入统计学习理论，把自否定和自实现预测分别建模为Wasserstein空间中的最小 - 最大和最小 - 最小风险泛函。

Result: 证明了表现性效应对样本、总体及两者同时作用时的泛化边界，揭示数据受影响程度与模型学习能力的权衡关系，发现再训练可提升泛化保证。

Conclusion: 表现性预测下，模型改变数据和学习数据存在根本权衡，在表现性扭曲样本上再训练有助于改善泛化。

Abstract: Performative predictions influence the very outcomes they aim to forecast. We study performative predictions that affect a sample (e.g., only existing users of an app) and/or the whole population (e.g., all potential app users). This raises the question of how well models generalize under performativity. For example, how well can we draw insights about new app users based on existing users when both of them react to the app's predictions? We address this question by embedding performative predictions into statistical learning theory. We prove generalization bounds under performative effects on the sample, on the population, and on both. A key intuition behind our proofs is that in the worst case, the population negates predictions, while the sample deceptively fulfills them. We cast such self-negating and self-fulfilling predictions as min-max and min-min risk functionals in Wasserstein space, respectively. Our analysis reveals a fundamental trade-off between performatively changing the world and learning from it: the more a model affects data, the less it can learn from it. Moreover, our analysis results in a surprising insight on how to improve generalization guarantees by retraining on performatively distorted samples. We illustrate our bounds in a case study on prediction-informed assignments of unemployed German residents to job trainings, drawing upon administrative labor market records from 1975 to 2017 in Germany.

</details>


### [228] [Bayesian PINNs for uncertainty-aware inverse problems (BPINN-IP)](https://arxiv.org/abs/2602.04459)
*Ali Mohammad-Djafari*

Main category: stat.ML

TL;DR: 本文提出用于线性逆问题的分层贝叶斯PINNs公式BPINN - IP，可量化不确定性并给出重建图像预测均值和方差，还给出应用示例和初步结果。


<details>
  <summary>Details</summary>
Motivation: 开发一种能结合先验知识、量化不确定性的PINNs方法用于线性逆问题。

Method: 提出BPINN - IP，将PINN扩展以纳入先验知识，采用变分推理和蒙特卡罗丢弃法。

Result: 给出了去卷积和超分辨率应用示例及初步结果。

Conclusion: 所提BPINN - IP方法可有效处理线性逆问题，能量化不确定性。

Abstract: The main contribution of this paper is to develop a hierarchical Bayesian formulation of PINNs for linear inverse problems, which is called BPINN-IP. The proposed methodology extends PINN to account for prior knowledge on the nature of the expected NN output, as well as its weights. Also, as we can have access to the posterior probability distributions, naturally uncertainties can be quantified. Also, variational inference and Monte Carlo dropout are employed to provide predictive means and variances for reconstructed images. Un example of applications to deconvolution and super-resolution is considered, details of the different steps of implementations are given, and some preliminary results are presented.

</details>


### [229] [A principled framework for uncertainty decomposition in TabPFN](https://arxiv.org/abs/2602.04596)
*Sandra Fortini,Kenyon Ng,Sonia Petrone,Judith Rousseau,Susan Wei*

Main category: stat.ML

TL;DR: 分析TabPFN不确定性分解问题，提出渐近方法获可信区间和分类不确定性分解。


<details>
  <summary>Details</summary>
Motivation: TabPFN目前无不确定性分解方法，需解决该挑战。

Method: 将分解挑战转化为贝叶斯预测推理问题，在准鞅条件下证明预测中心极限定理，推导方差估计量。

Result: 得到的可信区间计算快、针对认知不确定性且达到接近标称的频率覆盖；分类时得到基于熵的不确定性分解。

Conclusion: 渐近方法解决了TabPFN在监督设置下不确定性分解的理论空白。

Abstract: TabPFN is a transformer that achieves state-of-the-art performance on supervised tabular tasks by amortizing Bayesian prediction into a single forward pass. However, there is currently no method for uncertainty decomposition in TabPFN. Because it behaves, in an idealised limit, as a Bayesian in-context learner, we cast the decomposition challenge as a Bayesian predictive inference (BPI) problem. The main computational tool in BPI, predictive Monte Carlo, is challenging to apply here as it requires simulating unmodeled covariates. We therefore pursue the asymptotic alternative, filling a gap in the theory for supervised settings by proving a predictive CLT under quasi-martingale conditions. We derive variance estimators determined by the volatility of predictive updates along the context. The resulting credible bands are fast to compute, target epistemic uncertainty, and achieve near-nominal frequentist coverage. For classification, we further obtain an entropy-based uncertainty decomposition.

</details>


### [230] [Targeted Synthetic Control Method](https://arxiv.org/abs/2602.04611)
*Yuxin Wang,Dennis Frauen,Emil Javurek,Konstantin Hess,Yuchen Ma,Stefan Feuerriegel*

Main category: stat.ML

TL;DR: 本文引入目标合成控制（TSC）方法，该方法可直接估计反事实结果，具有多种优势，且在实验中比现有方法有更好的估计准确性。


<details>
  <summary>Details</summary>
Motivation: 现有合成控制方法存在一些不足，如可能产生无界的反事实估计，需要新的方法来改进。

Method: TSC是一个两阶段估计器，先通过权重倾斜子模型进行一维目标更新得到初始合成控制权重，校准权重以减少估计偏差，可使用任意机器学习模型。

Result: 在大量合成和真实世界实验中，TSC始终比最先进的SCM基线提高了估计准确性。

Conclusion: TSC方法是一种灵活且有效的新方法，避免了现有方法的关键缺点，能提升估计精度。

Abstract: The synthetic control method (SCM) estimates causal effects in panel data with a single-treated unit by constructing a counterfactual outcome as a weighted combination of untreated control units that matches the pre-treatment trajectory. In this paper, we introduce the targeted synthetic control (TSC) method, a new two-stage estimator that directly estimates the counterfactual outcome. Specifically, our TSC method (1) yields a targeted debiasing estimator, in the sense that the targeted updating refines the initial weights to produce more stable weights; and (2) ensures that the final counterfactual estimation is a convex combination of observed control outcomes to enable direct interpretation of the synthetic control weights. TSC is flexible and can be instantiated with arbitrary machine learning models. Methodologically, TSC starts from an initial set of synthetic-control weights via a one-dimensional targeted update through the weight-tilting submodel, which calibrates the weights to reduce bias of weights estimation arising from pre-treatment fit. Furthermore, TSC avoids key shortcomings of existing methods (e.g., the augmented SCM), which can produce unbounded counterfactual estimates. Across extensive synthetic and real-world experiments, TSC consistently improves estimation accuracy over state-of-the-art SCM baselines.

</details>


### [231] [Causal explanations of outliers in systems with lagged time-dependencies](https://arxiv.org/abs/2602.04667)
*Philipp Alexander Schwarz,Johannes Oberpriller,Sven Klaassen*

Main category: stat.ML

TL;DR: 本文将因果根因分析方法应用于一般时间依赖系统，讨论两种截断方法处理无限依赖图，通过基准测试验证其有效性并讨论机制近似的影响。


<details>
  <summary>Details</summary>
Motivation: 在受控时间依赖系统中进行根因分析是应用中的重大挑战，特别是能源系统难以处理，需要合适的根因分析方法。

Method: 将Budhathoki等人的因果根因分析方法应用于一般时间依赖系统，讨论两种截断方法处理无限依赖图。

Result: 在给定足够滞后的情况下，扩展方法能够在特征和时间域中定位根因，还讨论了机制近似的效果。

Conclusion: 所提出的扩展方法在时间依赖系统的根因分析中是有效的。

Abstract: Root-cause analysis in controlled time dependent systems poses a major challenge in applications. Especially energy systems are difficult to handle as they exhibit instantaneous as well as delayed effects and if equipped with storage, do have a memory. In this paper we adapt the causal root-cause analysis method of Budhathoki et al. [2022] to general time-dependent systems, as it can be regarded as a strictly causal definition of the term "root-cause". Particularly, we discuss two truncation approaches to handle the infinite dependency graphs present in time-dependent systems. While one leaves the causal mechanisms intact, the other approximates the mechanisms at the start nodes. The effectiveness of the different approaches is benchmarked using a challenging data generation process inspired by a problem in factory energy management: the avoidance of peaks in the power consumption. We show that given enough lags our extension is able to localize the root-causes in the feature and time domain. Further the effect of mechanism approximation is discussed.

</details>


### [232] [Conditional Counterfactual Mean Embeddings: Doubly Robust Estimation and Learning Rates](https://arxiv.org/abs/2602.04736)
*Thatchanon Anancharoenkij,Donlapark Ponnoprat*

Main category: stat.ML

TL;DR: 提出CCME框架嵌入反事实结果条件分布，开发元估计器和三种实用估计器，给出收敛率，验证可准确恢复分布特征。


<details>
  <summary>Details</summary>
Motivation: 全面理解异质处理效应需刻画潜在结果的全条件分布。

Method: 提出CCME框架将反事实结果的条件分布嵌入RKHS，开发两阶段元估计器，基于此开发三种实用估计器（岭回归、深度特征、神经核估计器）。

Result: 为所有估计器提供了有限样本收敛率，证明其具有双重鲁棒性；实验表明估计器能准确恢复分布特征。

Conclusion: 所提CCME框架及其相关估计器在刻画反事实结果条件分布方面有效且具有较好性能。

Abstract: A complete understanding of heterogeneous treatment effects involves characterizing the full conditional distribution of potential outcomes. To this end, we propose the Conditional Counterfactual Mean Embeddings (CCME), a framework that embeds conditional distributions of counterfactual outcomes into a reproducing kernel Hilbert space (RKHS). Under this framework, we develop a two-stage meta-estimator for CCME that accommodates any RKHS-valued regression in each stage. Based on this meta-estimator, we develop three practical CCME estimators: (1) Ridge Regression estimator, (2) Deep Feature estimator that parameterizes the feature map by a neural network, and (3) Neural-Kernel estimator that performs RKHS-valued regression, with the coefficients parameterized by a neural network. We provide finite-sample convergence rates for all estimators, establishing that they possess the double robustness property. Our experiments demonstrate that our estimators accurately recover distributional features including multimodal structure of conditional counterfactual distributions.

</details>


### [233] [Multi-layer Cross-Attention is Provably Optimal for Multi-modal In-context Learning](https://arxiv.org/abs/2602.04872)
*Nicholas Barnfield,Subhabrata Sen,Pragya Sur*

Main category: stat.ML

TL;DR: 提出研究多模态学习的数学框架，研究类Transformer架构在上下文学习中恢复贝叶斯最优性能的条件，指出单层线性自注意力的局限性，引入线性化交叉注意力机制并证明其贝叶斯最优性。


<details>
  <summary>Details</summary>
Motivation: 现有关于上下文学习的研究主要集中在单模态数据，多模态数据的上下文学习理论基础尚不清楚。

Method: 引入数学框架，假设观测数据来自潜在因子模型，研究单层线性自注意力，引入线性化交叉注意力机制并在交叉注意力层数和上下文长度都较大的情况下进行研究。

Result: 证明单层线性自注意力无法在任务分布上统一恢复贝叶斯最优预测器；线性化交叉注意力机制在使用梯度流优化时可证明是贝叶斯最优的。

Conclusion: 强调了深度对上下文学习的好处，确立了交叉注意力对多模态分布的可证明效用。

Abstract: Recent progress has rapidly advanced our understanding of the mechanisms underlying in-context learning in modern attention-based neural networks. However, existing results focus exclusively on unimodal data; in contrast, the theoretical underpinnings of in-context learning for multi-modal data remain poorly understood. We introduce a mathematically tractable framework for studying multi-modal learning and explore when transformer-like architectures can recover Bayes-optimal performance in-context. To model multi-modal problems, we assume the observed data arises from a latent factor model. Our first result comprises a negative take on expressibility: we prove that single-layer, linear self-attention fails to recover the Bayes-optimal predictor uniformly over the task distribution. To address this limitation, we introduce a novel, linearized cross-attention mechanism, which we study in the regime where both the number of cross-attention layers and the context length are large. We show that this cross-attention mechanism is provably Bayes optimal when optimized using gradient flow. Our results underscore the benefits of depth for in-context learning and establish the provable utility of cross-attention for multi-modal distributions.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [234] [Bures-Wasserstein Importance-Weighted Evidence Lower Bound: Exposition and Applications](https://arxiv.org/abs/2602.04272)
*Peiwen Jiang,Takuo Matsubara,Minh-Ngoc Tran*

Main category: stat.CO

TL;DR: 本文在Bures - Wasserstein空间中对IW - ELBO进行优化，推导其Wasserstein梯度，证明梯度估计器稳定性，扩展到变分Rényi重要性加权自动编码器界，实验显示性能优越。


<details>
  <summary>Details</summary>
Motivation: 在欧几里得空间优化IW - ELBO效率低，其梯度估计器信噪比会消失。

Method: 在Bures - Wasserstein空间对IW - ELBO进行优化，推导Wasserstein梯度并投影到该空间得到高斯变分推理算法，还将几何分析扩展到变分Rényi重要性加权自动编码器界。

Result: 证明Wasserstein梯度信噪比随重要样本数量K的平方根增长，实验表明所提框架比其他基线有更优近似性能。

Conclusion: 在Bures - Wasserstein空间优化IW - ELBO可提高优化效率和近似性能。

Abstract: The Importance-Weighted Evidence Lower Bound (IW-ELBO) has emerged as an effective objective for variational inference (VI), tightening the standard ELBO and mitigating the mode-seeking behaviour.
  However, optimizing the IW-ELBO in Euclidean space is often inefficient, as its gradient estimators suffer from a vanishing signal-to-noise ratio (SNR). This paper formulates the optimisation of the IW-ELBO in Bures-Wasserstein space, a manifold of Gaussian distributions equipped with the 2-Wasserstein metric. We derive the Wasserstein gradient of the IW-ELBO and project it onto the Bures-Wasserstein space to yield a tractable algorithm for Gaussian VI.
  A pivotal contribution of our analysis concerns the stability of the gradient estimator. While the SNR of the standard Euclidean gradient estimator is known to vanish as the number of importance samples $K$ increases, we prove that the SNR of the Wasserstein gradient scales favourably as $Ω(\sqrt{K})$, ensuring optimisation efficiency even for large $K$. We further extend this geometric analysis to the Variational Rényi Importance-Weighted Autoencoder bound, establishing analogous stability guarantees. Experiments demonstrate that the proposed framework achieves superior approximation performance compared to other baselines.

</details>


### [235] [LID Framework: A new method for geospatial and exploratory data analysis of potential innovation deter-minants at the neighborhood level](https://arxiv.org/abs/2602.04679)
*Eleni Oikonomaki,Belivanis Dimitris,Kakderi Christina*

Main category: stat.CO

TL;DR: 研究以更精细空间分辨率研究创新，开发LID数据库和框架，分析纽约和马萨诸塞州社区，结果显示替代数据源有潜力，建议政策制定考虑社区特点。


<details>
  <summary>Details</summary>
Motivation: 以往实证研究多关注国家和区域尺度，城市和次区域地理受关注少，且地方研究指标有限，缺乏系统框架。

Method: 开发Local Innovation Determinants (LID)数据库和框架，结合传统政府数据与API公开数据，运用探索性大数据和地理空间数据分析及随机森林模型，从四个维度分析社区。

Result: 替代数据源有显著但未充分挖掘的潜力，可增强对创新动态的洞察。

Conclusion: 城市政策制定者在设计和实施本地创新战略时应考虑社区特定决定因素和特征。

Abstract: The geography of innovation offers a framework to understand how territorial characteristics shape innovation, often via spatial and cognitive proximity. Empirical research has focused largely on national and regional scales, while urban and sub-regional geographies receive less attention. Local studies typically rely on limited indicators (e.g., firm-level data, patents, basic socioeconomic measures), with few offering a systematic framework integrating urban form, mobility, amenities, and human-capital proxies at the neighborhood scale. Our study investigates innovation at a finer spatial resolution, going beyond proprietary or static indicators. We develop the Local Innovation Determinants (LID) database and framework to identify key enabling factors across regions, combining traditional government data with publicly available data via APIs for a more granular understanding of spatial dynamics shaping innovation capacity. Using exploratory big and geospatial data analytics and random forest models, we examine neighborhoods in New York and Massachusetts across four dimensions: social factors, economic characteristics, land use and mobility, morphology, and environment. Results show that alternative data sources offer significant yet underexplored potential to enhance insights into innovation dynamics. City policymakers should consider neighborhood-specific determinants and characteristics when designing and implementing local innovation strategies.

</details>


### [236] [Multiple Imputation Methods under Extreme Values](https://arxiv.org/abs/2602.04751)
*Enzo Porto Brasil*

Main category: stat.CO

TL;DR: 本文用R中MICE包评估不同多重插补方法在有和无极端值情况下的表现，通过模拟生成数据集在回归模型中评估，发现基于线性回归的插补方法表现最佳，强调选择插补策略时要考虑极端值等因素并给出实用建议。


<details>
  <summary>Details</summary>
Motivation: 缺失数据在实证数据库中常见，但统计分析通常需完整数据矩阵，多重插补是解决数据缺失的有效方法，因此评估不同多重插补方法的性能。

Method: 使用R中的MICE包，通过蒙特卡罗模拟生成包含三个变量的不完整数据集，在回归模型中评估各插补方法。

Result: 基于线性回归的插补方法总体预测性能（CV - MSE）最佳，稀疏模型方法效率较低。

Conclusion: 选择插补策略时极端值很关键，样本量、缺失比例、极端值存在情况和拟合模型类型是影响性能的关键因素，研究建议研究者在选择插补方法前检查缺失机制和极端值情况。

Abstract: Missing data are ubiquitous in empirical databases, yet statistical analyses typically require complete data matrices. Multiple imputation offers a principled solution for filling these gaps. This study evaluates the performance of several multiple imputation methods, both in the presence and absence of extreme values, using the MICE package in R. Through Monte Carlo simulations, we generated incomplete data sets with three variables and assessed each imputation method within regression models. The results indicate that the linear regression based imputation method showed the best overall predictive performance (CV-MSE), whereas the sparse model approach was generally less efficient. Our findings underscore the relevance of extreme values when selecting an imputation strategy and highlight sample size, proportion of missingness, presence of extremes, and the type of fitted model as key determinants of performance. Despite its limitations, the study offers practical recommendations for researchers, stressing the need to examine the missingness mechanism and the occurrence of extreme values before choosing an imputation method.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [237] [SPEAR: An Engineering Case Study of Multi-Agent Coordination for Smart Contract Auditing](https://arxiv.org/abs/2602.04418)
*Arnab Mallick,Indraveni Chebolu,Harmesh Rana*

Main category: cs.MA

TL;DR: 提出SPEAR多智能体协调框架用于智能合约审计，介绍其代理分工、运作方式并进行实证研究。


<details>
  <summary>Details</summary>
Motivation: 为智能合约审计提供有效的多智能体协调框架。

Method: 将审计建模为专业代理执行的协调任务，各代理有不同职责，通过特定协议协调，更新本地信念和修订计划；进行实证研究对比不同设计。

Result: 进行了在受控失败场景下与集中式和基于管道的替代方案对比的实证研究。

Conclusion: 未明确提及，但暗示多智能体设计在协调、恢复行为和资源使用方面可能有优势。

Abstract: We present SPEAR, a multi-agent coordination framework for smart contract auditing that applies established MAS patterns in a realistic security analysis workflow. SPEAR models auditing as a coordinated mission carried out by specialized agents: a Planning Agent prioritizes contracts using risk-aware heuristics, an Execution Agent allocates tasks via the Contract Net protocol, and a Repair Agent autonomously recovers from brittle generated artifacts using a programmatic-first repair policy. Agents maintain local beliefs updated through AGM-compliant revision, coordinate via negotiation and auction protocols, and revise plans as new information becomes available. An empirical study compares the multi-agent design with centralized and pipeline-based alternatives under controlled failure scenarios, focusing on coordination, recovery behavior, and resource use.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [238] [Functional Stochastic Localization](https://arxiv.org/abs/2602.03999)
*Anming Gu,Bobby Shi,Kevin Tian*

Main category: math.PR

TL;DR: 提出Eldan随机局部化过程的泛化形式，给出混合时间界并应用于差分隐私凸优化，改进了查询复杂度。


<details>
  <summary>Details</summary>
Motivation: 受非欧几何采样和优化中镜像下降算法的启发。

Method: 将Eldan过程进行泛化，用对数拉普拉斯变换的正整数倍正则化替代高斯正则化。

Result: 在目标分布满足泛函庞加莱不等式时给出马尔可夫链混合时间界；应用于差分隐私凸优化，改进了零阶模型中的查询复杂度。

Conclusion: 所提出的泛化框架在高维几何、采样算法和优化问题中有一定价值和效果。

Abstract: Eldan's stochastic localization is a probabilistic construction that has proved instrumental to modern breakthroughs in high-dimensional geometry and the design of sampling algorithms. Motivated by sampling under non-Euclidean geometries and the mirror descent algorithm in optimization, we develop a functional generalization of Eldan's process that replaces Gaussian regularization with regularization by any positive integer multiple of a log-Laplace transform. We further give a mixing time bound on the Markov chain induced by our localization process, which holds if our target distribution satisfies a functional Poincaré inequality. Finally, we apply our framework to differentially private convex optimization in $\ell_p$ norms for $p \in [1, 2)$, where we improve state-of-the-art query complexities in a zeroth-order model.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [239] [Explainable Computer Vision Framework for Automated Pore Detection and Criticality Assessment in Additive Manufacturing](https://arxiv.org/abs/2602.03883)
*Akshansh Mishra,Rakesh Morisetty*

Main category: cs.CV

TL;DR: 本文提出可解释计算机视觉框架用于3D断层扫描孔隙检测与临界性评估，发现归一化表面距离主导模型预测，该框架可助力增材制造质量控制。


<details>
  <summary>Details</summary>
Motivation: 增材制造部件内部孔隙是关键缺陷，现有自动缺陷检测方法缺乏可解释性，工程师难以理解临界性预测的物理基础。

Method: 将灰度切片重建为体积数据集，用基于强度的阈值和连通分量分析识别孔隙，构建孔隙交互网络，用机器学习模型预测孔隙临界性分数，用SHAP分析量化特征贡献。

Result: 归一化表面距离主导模型预测，其重要性比其他描述符高一个数量级以上，孔隙大小影响小，几何参数影响可忽略，表面接近度与临界性呈强反比关系。

Conclusion: 该可解释框架能实现透明的缺陷评估，为增材制造的工艺优化和质量控制提供可行见解。

Abstract: Internal porosity remains a critical defect mode in additively manufactured components, compromising structural performance and limiting industrial adoption. Automated defect detection methods exist but lack interpretability, preventing engineers from understanding the physical basis of criticality predictions. This study presents an explainable computer vision framework for pore detection and criticality assessment in three-dimensional tomographic volumes. Sequential grayscale slices were reconstructed into volumetric datasets, and intensity-based thresholding with connected component analysis identified 500 individual pores. Each pore was characterized using geometric descriptors including size, aspect ratio, extent, and spatial position relative to the specimen boundary. A pore interaction network was constructed using percentile-based Euclidean distance criteria, yielding 24,950 inter-pore connections. Machine learning models predicted pore criticality scores from extracted features, and SHAP analysis quantified individual feature contributions. Results demonstrate that normalized surface distance dominates model predictions, contributing more than an order of magnitude greater importance than all other descriptors. Pore size provides minimal influence, while geometric parameters show negligible impact. The strong inverse relationship between surface proximity and criticality reveals boundary-driven failure mechanisms. This interpretable framework enables transparent defect assessment and provides actionable insights for process optimization and quality control in additive manufacturing.

</details>


### [240] [Phaedra: Learning High-Fidelity Discrete Tokenization for the Physical Science](https://arxiv.org/abs/2602.03915)
*Levi Lingsch,Georgios Kissas,Johannes Jakubik,Siddhartha Mishra*

Main category: cs.CV

TL;DR: 现有分词器用于科学图像或非最优，本文提出Phaedra，在PDE数据集重建和分布外泛化上表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有分词器为图像的视觉感知设计，需探究其对科学图像是否最优。

Method: 研究一系列图像分词器在测量PDE属性保真度方面的准确性，基于不足提出受经典形状增益量化和本征正交分解启发的Phaedra。

Result: Phaedra在一系列PDE数据集上持续改进重建效果，且在三个复杂度递增任务中有强分布外泛化能力。

Conclusion: Phaedra在PDE相关的重建和泛化任务表现出色，有应用价值。

Abstract: Tokens are discrete representations that allow modern deep learning to scale by transforming high-dimensional data into sequences that can be efficiently learned, generated, and generalized to new tasks. These have become foundational for image and video generation and, more recently, physical simulation. As existing tokenizers are designed for the explicit requirements of realistic visual perception of images, it is necessary to ask whether these approaches are optimal for scientific images, which exhibit a large dynamic range and require token embeddings to retain physical and spectral properties. In this work, we investigate the accuracy of a suite of image tokenizers across a range of metrics designed to measure the fidelity of PDE properties in both physical and spectral space. Based on the observation that these struggle to capture both fine details and precise magnitudes, we propose Phaedra, inspired by classical shape-gain quantization and proper orthogonal decomposition. We demonstrate that Phaedra consistently improves reconstruction across a range of PDE datasets. Additionally, our results show strong out-of-distribution generalization capabilities to three tasks of increasing complexity, namely known PDEs with different conditions, unknown PDEs, and real-world Earth observation and weather data.

</details>


### [241] [SpatiaLab: Can Vision-Language Models Perform Spatial Reasoning in the Wild?](https://arxiv.org/abs/2602.03916)
*Azmine Toushik Wasi,Wahid Faisal,Abdur Rahman,Mahfuz Ahmed Anik,Munem Shahriar,Mohsin Mahmud Topu,Sadia Tasnim Meem,Rahatun Nesa Priti,Sabrina Afroz Mitu,Md. Iqramul Hoque,Shahriyar Zaman Ridoy,Mohammed Eunus Ali,Majd Hawasly,Mohammad Raza,Md Rizwan Parvez*

Main category: cs.CV

TL;DR: 文章介绍SpatiaLab基准用于评估VLM空间推理能力，实验表明模型与人类存在差距，为后续研究提供框架。


<details>
  <summary>Details</summary>
Motivation: 现有研究多依赖合成或大语言模型生成的环境，无法捕捉现实世界复杂性，需新基准评估VLM空间推理能力。

Method: 引入包含1400个视觉问答对、6大类30种任务类型的SpatiaLab基准，并对多种VLM进行实验。

Result: 各类VLM在空间推理能力上与人类有显著差距，在不同评估设置中表现不佳，存在处理复杂空间关系等问题。

Conclusion: SpatiaLab暴露出VLM在空间推理方面挑战与机遇，为未来研究提供指引。

Abstract: Spatial reasoning is a fundamental aspect of human cognition, yet it remains a major challenge for contemporary vision-language models (VLMs). Prior work largely relied on synthetic or LLM-generated environments with limited task designs and puzzle-like setups, failing to capture the real-world complexity, visual noise, and diverse spatial relationships that VLMs encounter. To address this, we introduce SpatiaLab, a comprehensive benchmark for evaluating VLMs' spatial reasoning in realistic, unconstrained contexts. SpatiaLab comprises 1,400 visual question-answer pairs across six major categories: Relative Positioning, Depth & Occlusion, Orientation, Size & Scale, Spatial Navigation, and 3D Geometry, each with five subcategories, yielding 30 distinct task types. Each subcategory contains at least 25 questions, and each main category includes at least 200 questions, supporting both multiple-choice and open-ended evaluation. Experiments across diverse state-of-the-art VLMs, including open- and closed-source models, reasoning-focused, and specialized spatial reasoning models, reveal a substantial gap in spatial reasoning capabilities compared with humans. In the multiple-choice setup, InternVL3.5-72B achieves 54.93% accuracy versus 87.57% for humans. In the open-ended setting, all models show a performance drop of around 10-25%, with GPT-5-mini scoring highest at 40.93% versus 64.93% for humans. These results highlight key limitations in handling complex spatial relationships, depth perception, navigation, and 3D geometry. By providing a diverse, real-world evaluation framework, SpatiaLab exposes critical challenges and opportunities for advancing VLMs' spatial reasoning, offering a benchmark to guide future research toward robust, human-aligned spatial understanding. SpatiaLab is available at: https://spatialab-reasoning.github.io/.

</details>


### [242] [TruKAN: Towards More Efficient Kolmogorov-Arnold Networks Using Truncated Power Functions](https://arxiv.org/abs/2602.03879)
*Ali Bayeh,Samira Sadaoui,Malek Mouhoub*

Main category: cs.CV

TL;DR: 提出基于KAN的TruKAN架构，将其集成到EfficientNet - V2框架评估，在复杂视觉任务上多项指标优于其他KAN模型。


<details>
  <summary>Details</summary>
Motivation: 解决计算效率和遵循KAN原则之间的权衡问题。

Method: 提出TruKAN架构，用截断幂函数替换KAN的B样条基，将TruKAN集成到EfficientNet - V2框架，开发多种对比模型，使用混合优化训练，研究层归一化技术和TruKAN中共享与单独节点影响。

Result: TruKAN在复杂视觉任务的准确性、计算效率和内存使用方面优于其他KAN模型。

Conclusion: TruKAN能平衡近似效果和透明度，在复杂视觉任务中有优势，超越先前KAN研究的有限场景。

Abstract: To address the trade-off between computational efficiency and adherence to Kolmogorov-Arnold Network (KAN) principles, we propose TruKAN, a new architecture based on the KAN structure and learnable activation functions. TruKAN replaces the B-spline basis in KAN with a family of truncated power functions derived from k-order spline theory. This change maintains the KAN's expressiveness while enhancing accuracy and training time. Each TruKAN layer combines a truncated power term with a polynomial term and employs either shared or individual knots. TruKAN exhibits greater interpretability than other KAN variants due to its simplified basis functions and knot configurations. By prioritizing interpretable basis functions, TruKAN aims to balance approximation efficacy with transparency. We develop the TruKAN model and integrate it into an advanced EfficientNet-V2-based framework, which is then evaluated on computer vision benchmark datasets. To ensure a fair comparison, we develop various models: MLP-, KAN-, SineKAN and TruKAN-based EfficientNet frameworks and assess their training time and accuracy across small and deep architectures. The training phase uses hybrid optimization to improve convergence stability. Additionally, we investigate layer normalization techniques for all the models and assess the impact of shared versus individual knots in TruKAN. Overall, TruKAN outperforms other KAN models in terms of accuracy, computational efficiency and memory usage on the complex vision task, demonstrating advantages beyond the limited settings explored in prior KAN studies.

</details>


### [243] [DiGAN: Diffusion-Guided Attention Network for Early Alzheimer's Disease Detection](https://arxiv.org/abs/2602.03881)
*Maxx Richard Rahman,Mostafa Hammouda,Wolfgang Maass*

Main category: cs.CV

TL;DR: 提出Diffusion - Guided Attention Network (DiGAN)用于早期阿尔茨海默病检测，实验显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在早期阿尔茨海默病诊断中需大量纵向数据集，且无法很好处理时间连续性和模态不规则性问题。

Method: 提出DiGAN，结合潜在扩散模型和注意力引导卷积网络，扩散模型合成纵向神经影像轨迹，注意力卷积层捕捉判别性结构 - 时间模式。

Result: 在合成和ADNI数据集实验中，DiGAN优于现有最先进的基线模型。

Conclusion: DiGAN在早期阿尔茨海默病检测中有应用潜力。

Abstract: Early diagnosis of Alzheimer's disease (AD) remains a major challenge due to the subtle and temporally irregular progression of structural brain changes in the prodromal stages. Existing deep learning approaches require large longitudinal datasets and often fail to model the temporal continuity and modality irregularities inherent in real-world clinical data. To address these limitations, we propose the Diffusion-Guided Attention Network (DiGAN), which integrates latent diffusion modelling with an attention-guided convolutional network. The diffusion model synthesizes realistic longitudinal neuroimaging trajectories from limited training data, enriching temporal context and improving robustness to unevenly spaced visits. The attention-convolutional layer then captures discriminative structural--temporal patterns that distinguish cognitively normal subjects from those with mild cognitive impairment and subjective cognitive decline. Experiments on synthetic and ADNI datasets demonstrate that DiGAN outperforms existing state-of-the-art baselines, showing its potential for early-stage AD detection.

</details>


### [244] [PriorProbe: Recovering Individual-Level Priors for Personalizing Neural Networks in Facial Expression Recognition](https://arxiv.org/abs/2602.03882)
*Haijiang Yan,Nick Chater,Adam Sanborn*

Main category: cs.CV

TL;DR: 提出PriorProbe方法恢复个体先验，应用于面部表情识别任务效果好，为个性化DNN提供框架


<details>
  <summary>Details</summary>
Motivation: 现有引出个体认知先验的方法存在无法唯一识别或引入系统偏差的问题，需要新方法

Method: 引入基于Markov Chain Monte Carlo with People的PriorProbe方法恢复细粒度、个体特定先验

Result: PriorProbe得出的先验使性能显著提升，优于单独的神经网络和其他先验来源，且保留网络对真实标签的推理

Conclusion: PriorProbe为个性化深度神经网络提供了通用且可解释的框架

Abstract: Incorporating individual-level cognitive priors offers an important route to personalizing neural networks, yet accurately eliciting such priors remains challenging: existing methods either fail to uniquely identify them or introduce systematic biases. Here, we introduce PriorProbe, a novel elicitation approach grounded in Markov Chain Monte Carlo with People that recovers fine-grained, individual-specific priors. Focusing on a facial expression recognition task, we apply PriorProbe to individual participants and test whether integrating the recovered priors with a state-of-the-art neural network improves its ability to predict an individual's classification on ambiguous stimuli. The PriorProbe-derived priors yield substantial performance gains, outperforming both the neural network alone and alternative sources of priors, while preserving the network's inference on ground-truth labels. Together, these results demonstrate that PriorProbe provides a general and interpretable framework for personalizing deep neural networks.

</details>


### [245] [Audit After Segmentation: Reference-Free Mask Quality Assessment for Language-Referred Audio-Visual Segmentation](https://arxiv.org/abs/2602.03892)
*Jinxing Zhou,Yanghao Zhou,Yaoting Wang,Zongyan Han,Jiaqi Ma,Henghui Ding,Rao Muhammad Anwer,Hisham Cholakkal*

Main category: cs.CV

TL;DR: 提出Ref - AVS场景下的掩码质量评估任务MQA - RefAVS，构建MQ - RAVSBench基准，提出基于MLLM的MQ - Auditor进行评估，实验表明其性能出色且可集成到现有系统。


<details>
  <summary>Details</summary>
Motivation: 现有Ref - AVS研究在生成掩码后对掩码质量的可解释诊断探索不足，需评估候选分割掩码质量。

Method: 构造MQ - RAVSBench基准，提出基于MLLM的MQ - Auditor，通过多模态线索和掩码信息推理进行质量评估。

Result: MQ - Auditor性能优于开源和商业MLLMs，可集成到现有Ref - AVS系统中检测分割失败并支持下游分割改进。

Conclusion: MQ - Auditor在MQA - RefAVS任务中有良好表现，可推动Ref - AVS领域发展。

Abstract: Language-referred audio-visual segmentation (Ref-AVS) aims to segment target objects described by natural language by jointly reasoning over video, audio, and text. Beyond generating segmentation masks, providing rich and interpretable diagnoses of mask quality remains largely underexplored. In this work, we introduce Mask Quality Assessment in the Ref-AVS context (MQA-RefAVS), a new task that evaluates the quality of candidate segmentation masks without relying on ground-truth annotations as references at inference time. Given audio-visual-language inputs and each provided segmentation mask, the task requires estimating its IoU with the unobserved ground truth, identifying the corresponding error type, and recommending an actionable quality-control decision. To support this task, we construct MQ-RAVSBench, a benchmark featuring diverse and representative mask error modes that span both geometric and semantic issues. We further propose MQ-Auditor, a multimodal large language model (MLLM)-based auditor that explicitly reasons over multimodal cues and mask information to produce quantitative and qualitative mask quality assessments. Extensive experiments demonstrate that MQ-Auditor outperforms strong open-source and commercial MLLMs and can be integrated with existing Ref-AVS systems to detect segmentation failures and support downstream segmentation improvement. Data and codes will be released at https://github.com/jasongief/MQA-RefAVS.

</details>


### [246] [Vision Transformers for Zero-Shot Clustering of Animal Images: A Comparative Benchmarking Study](https://arxiv.org/abs/2602.03894)
*Hugo Markoff,Stefan Hein Bengtson,Michael Ørsted*

Main category: cs.CV

TL;DR: 研究利用 ViT 模型对未标记动物图像进行物种级聚类，提出基准框架，结果显示部分方法聚类效果好，还能提取种内差异，最后提供开源工具和方法选择建议。


<details>
  <summary>Details</summary>
Motivation: 手动标记动物图像是生态研究瓶颈，限制生物多样性监测规模和效率，研究旨在探究 ViT 模型能否直接将大量未标记动物图像聚类到物种级别。

Method: 提出综合基准框架，评估五种 ViT 模型、五种降维技术和四种聚类算法（两种监督、两种无监督），对 60 种动物（30 种哺乳动物和 30 种鸟类）进行测试，每次使用每个物种 200 张验证图像的随机子集。

Result: 使用 DINOv3 嵌入、t - SNE 和监督分层聚类方法实现接近完美的物种级聚类（V - measure: 0.958）；无监督方法性能有竞争力（0.943），仅 1.14%图像需专家审核；对物种长尾分布有鲁棒性，过聚类可提取种内差异。

Conclusion: 提供开源基准工具包，并为生态学家选择适合其特定分类群和数据的方法提供建议。

Abstract: Manual labeling of animal images remains a significant bottleneck in ecological research, limiting the scale and efficiency of biodiversity monitoring efforts. This study investigates whether state-of-the-art Vision Transformer (ViT) foundation models can reduce thousands of unlabeled animal images directly to species-level clusters. We present a comprehensive benchmarking framework evaluating five ViT models combined with five dimensionality reduction techniques and four clustering algorithms, two supervised and two unsupervised, across 60 species (30 mammals and 30 birds), with each test using a random subset of 200 validated images per species. We investigate when clustering succeeds at species-level, where it fails, and whether clustering within the species-level reveals ecologically meaningful patterns such as sex, age, or phenotypic variation. Our results demonstrate near-perfect species-level clustering (V-measure: 0.958) using DINOv3 embeddings with t-SNE and supervised hierarchical clustering methods. Unsupervised approaches achieve competitive performance (0.943) while requiring no prior species knowledge, rejecting only 1.14% of images as outliers requiring expert review. We further demonstrate robustness to realistic long-tailed distributions of species and show that intentional over-clustering can reliably extract intra-specific variation including age classes, sexual dimorphism, and pelage differences. We introduce an open-source benchmarking toolkit and provide recommendations for ecologists to select appropriate methods for sorting their specific taxonomic groups and data.

</details>


### [247] [HY3D-Bench: Generation of 3D Assets](https://arxiv.org/abs/2602.03907)
*Team Hunyuan3D,:,Bowen Zhang,Chunchao Guo,Dongyuan Guo,Haolin Liu,Hongyu Yan,Huiwen Shi,Jiaao Yu,Jiachen Xu,Jingwei Huang,Kunhong Li,Lifu Wang,Linus,Penghao Wang,Qingxiang Lin,Ruining Tang,Xianghui Yang,Yang Li,Yirui Guan,Yunfei Zhao,Yunhan Yang,Zeqiang Lai,Zhihao Liang,Zibo Zhao*

Main category: cs.CV

TL;DR: 本文介绍开源生态系统HY3D - Bench以解决3D内容创建数据处理瓶颈，有三方面贡献，经实证验证可推动多领域创新。


<details>
  <summary>Details</summary>
Motivation: 解决3D内容创建中存在的显著数据处理瓶颈问题。

Method: 1. 从大规模仓库精选250k高保真3D对象，提供训练工件；2. 引入结构化部件级分解；3. 通过可扩展AIGC合成管道缩小现实分布差距，贡献125k合成资产。

Result: 通过训练Hunyuan3D - 2.1 - Small进行了实证验证。

Conclusion: HY3D - Bench使强大数据资源更易获取，旨在推动3D感知、机器人和数字内容创作等领域的创新。

Abstract: While recent advances in neural representations and generative models have revolutionized 3D content creation, the field remains constrained by significant data processing bottlenecks. To address this, we introduce HY3D-Bench, an open-source ecosystem designed to establish a unified, high-quality foundation for 3D generation. Our contributions are threefold: (1) We curate a library of 250k high-fidelity 3D objects distilled from large-scale repositories, employing a rigorous pipeline to deliver training-ready artifacts, including watertight meshes and multi-view renderings; (2) We introduce structured part-level decomposition, providing the granularity essential for fine-grained perception and controllable editing; and (3) We bridge real-world distribution gaps via a scalable AIGC synthesis pipeline, contributing 125k synthetic assets to enhance diversity in long-tail categories. Validated empirically through the training of Hunyuan3D-2.1-Small, HY3D-Bench democratizes access to robust data resources, aiming to catalyze innovation across 3D perception, robotics, and digital content creation.

</details>


### [248] [Entropy-Aware Structural Alignment for Zero-Shot Handwritten Chinese Character Recognition](https://arxiv.org/abs/2602.03913)
*Qiuming Luo,Tao Zeng,Feng Li,Heming Liu,Rui Mao,Chang Kong*

Main category: cs.CV

TL;DR: 提出熵感知结构对齐网络用于零样本手写汉字识别，实验表现佳且数据效率高。


<details>
  <summary>Details</summary>
Motivation: 现有零样本手写汉字识别方法将字符视为扁平部首序列，忽略了层次拓扑和不同组件的信息密度差异。

Method: 引入信息熵先验动态调整位置嵌入；构建双视图部首树提取多粒度特征，通过基于Sigmoid的门控网络集成；设计Top - K语义特征融合机制增强解码过程。

Result: 方法达到了新的最优性能，显著超越现有基于CLIP的基线模型，且数据效率高，少量支持样本就能快速适应。

Conclusion: 提出的熵感知结构对齐网络有效解决了现有方法的局限，在零样本手写汉字识别中表现出色。

Abstract: Zero-shot Handwritten Chinese Character Recognition (HCCR) aims to recognize unseen characters by leveraging radical-based semantic compositions. However, existing approaches often treat characters as flat radical sequences, neglecting the hierarchical topology and the uneven information density of different components. To address these limitations, we propose an Entropy-Aware Structural Alignment Network that bridges the visual-semantic gap through information-theoretic modeling. First, we introduce an Information Entropy Prior to dynamically modulate positional embeddings via multiplicative interaction, acting as a saliency detector that prioritizes discriminative roots over ubiquitous components. Second, we construct a Dual-View Radical Tree to extract multi-granularity structural features, which are integrated via an adaptive Sigmoid-based gating network to encode both global layout and local spatial roles. Finally, a Top-K Semantic Feature Fusion mechanism is devised to augment the decoding process by utilizing the centroid of semantic neighbors, effectively rectifying visual ambiguities through feature-level consensus. Extensive experiments demonstrate that our method establishes new state-of-the-art performance, significantly outperforming existing CLIP-based baselines in the challenging zero-shot setting. Furthermore, the framework exhibits exceptional data efficiency, demonstrating rapid adaptability with minimal support samples.

</details>


### [249] [DMS2F-HAD: A Dual-branch Mamba-based Spatial-Spectral Fusion Network for Hyperspectral Anomaly Detection](https://arxiv.org/abs/2602.04102)
*Aayushma Pant,Lakpa Tamang,Tsz-Kwan Lee,Sunil Aryal*

Main category: cs.CV

TL;DR: 本文提出DMS2F - HAD模型用于高光谱异常检测，在14个数据集上表现优异且效率高。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在高光谱异常检测中存在无法捕捉长距离光谱依赖或计算成本高的问题。

Method: 提出基于Mamba的双分支模型DMS2F - HAD，用Mamba的线性时间建模学习特征，通过动态门控融合机制整合特征。

Result: 在十四个基准数据集上平均AUC达98.78%，推理速度比同类方法快4.6倍。

Conclusion: DMS2F - HAD具有强泛化和可扩展性，是实际高光谱异常检测应用的有力候选。

Abstract: Hyperspectral anomaly detection (HAD) aims to identify rare and irregular targets in high-dimensional hyperspectral images (HSIs), which are often noisy and unlabelled data. Existing deep learning methods either fail to capture long-range spectral dependencies (e.g., convolutional neural networks) or suffer from high computational cost (e.g., Transformers). To address these challenges, we propose DMS2F-HAD, a novel dual-branch Mamba-based model. Our architecture utilizes Mamba's linear-time modeling to efficiently learn distinct spatial and spectral features in specialized branches, which are then integrated by a dynamic gated fusion mechanism to enhance anomaly localization. Across fourteen benchmark HSI datasets, our proposed DMS2F-HAD not only achieves a state-of-the-art average AUC of 98.78%, but also demonstrates superior efficiency with an inference speed 4.6 times faster than comparable deep learning methods. The results highlight DMS2FHAD's strong generalization and scalability, positioning it as a strong candidate for practical HAD applications.

</details>


### [250] [JSynFlow: Japanese Synthesised Flowchart Visual Question Answering Dataset built with Large Language Models](https://arxiv.org/abs/2602.04142)
*Hiroshi Sasaki*

Main category: cs.CV

TL;DR: 提出合成的日语流程图视觉问答数据集JSynFlow，可提升视觉语言模型在基于流程图的问答任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 开发能精确理解流程图的视觉语言模型需要大规模数据集，但创建此类数据集耗时，因此构建合成数据集来应对这一挑战。

Method: 使用大语言模型生成JSynFlow数据集，包含业务任务描述、从特定领域语言代码渲染的流程图图像和相关问答对。

Result: 基于JSynFlow数据集微调，显著提升了视觉语言模型在流程图问答任务上的性能。

Conclusion: 提出的JSynFlow数据集具有可用性和有效性，可公开获取。

Abstract: Vision and language models (VLMs) are expected to analyse complex documents, such as those containing flowcharts, through a question-answering (QA) interface. The ability to recognise and interpret these flowcharts is in high demand, as they provide valuable insights unavailable in text-only explanations. However, developing VLMs with precise flowchart understanding requires large-scale datasets of flowchart images and corresponding text, the creation of which is highly time-consuming. To address this challenge, we introduce JSynFlow, a synthesised visual QA dataset for Japanese flowcharts, generated using large language models (LLMs). Our dataset comprises task descriptions for various business occupations, the corresponding flowchart images rendered from domain-specific language (DSL) code, and related QA pairs. This paper details the dataset's synthesis procedure and demonstrates that fine-tuning with JSynFlow significantly improves VLM performance on flowchart-based QA tasks. Our dataset is publicly available at https://huggingface.co/datasets/jri-advtechlab/jsynflow.

</details>


### [251] [Improving 2D Diffusion Models for 3D Medical Imaging with Inter-Slice Consistent Stochasticity](https://arxiv.org/abs/2602.04162)
*Chenhe Du,Qing Wu,Xuanyu Tian,Jingyi Yu,Hongjiang Wei,Yuyao Zhang*

Main category: cs.CV

TL;DR: 本文指出用2D扩散模型解决3D医学成像问题存在层间不连续问题，提出ISCS策略，可提升成像性能。


<details>
  <summary>Details</summary>
Motivation: 当前用扩散模型学习3D医学数据分布有困难，用2D数据先验重建3D存在层间不连续，现有方法有缺陷，需寻找有效策略。

Method: 引入Inter - Slice Consistent Stochasticity (ISCS)策略，控制扩散采样期间随机噪声分量的一致性。

Result: 在多个医学成像问题实验中，该方法有效提升了基于2D扩散模型的医学3D成像问题性能。

Conclusion: 控制层间随机性是利用2D扩散先验实现高保真3D医学成像的可行且有吸引力的途径。

Abstract: 3D medical imaging is in high demand and essential for clinical diagnosis and scientific research. Currently, diffusion models (DMs) have become an effective tool for medical imaging reconstruction thanks to their ability to learn rich, high-quality data priors. However, learning the 3D data distribution with DMs in medical imaging is challenging, not only due to the difficulties in data collection but also because of the significant computational burden during model training. A common compromise is to train the DMs on 2D data priors and reconstruct stacked 2D slices to address 3D medical inverse problems. However, the intrinsic randomness of diffusion sampling causes severe inter-slice discontinuities of reconstructed 3D volumes. Existing methods often enforce continuity regularizations along the z-axis, which introduces sensitive hyper-parameters and may lead to over-smoothing results. In this work, we revisit the origin of stochasticity in diffusion sampling and introduce Inter-Slice Consistent Stochasticity (ISCS), a simple yet effective strategy that encourages interslice consistency during diffusion sampling. Our key idea is to control the consistency of stochastic noise components during diffusion sampling, thereby aligning their sampling trajectories without adding any new loss terms or optimization steps. Importantly, the proposed ISCS is plug-and-play and can be dropped into any 2D trained diffusion based 3D reconstruction pipeline without additional computational cost. Experiments on several medical imaging problems show that our method can effectively improve the performance of medical 3D imaging problems based on 2D diffusion models. Our findings suggest that controlling inter-slice stochasticity is a principled and practically attractive route toward high-fidelity 3D medical imaging with 2D diffusion priors. The code is available at: https://github.com/duchenhe/ISCS

</details>


### [252] [HoloEv-Net: Efficient Event-based Action Recognition via Holographic Spatial Embedding and Global Spectral Gating](https://arxiv.org/abs/2602.04182)
*Weidong Hao*

Main category: cs.CV

TL;DR: 该论文针对现有基于事件的动作识别（EAR）方法的不足，提出HoloEv - Net框架，实验证明其有效且高效。


<details>
  <summary>Details</summary>
Motivation: 现有EAR方法存在密集体素表示的计算冗余、多分支架构的结构冗余以及光谱信息利用不足等问题。

Method: 提出HoloEv - Net框架，包括引入紧凑全息时空表示（CHSR）解决表示和结构冗余，设计全局光谱门控（GSG）模块利用光谱线索。

Result: HoloEv - Net - Base在多个数据集上达到SOTA，优于现有方法；轻量级变体HoloEv - Net - Small准确率有竞争力，且参数、FLOPs和延迟大幅降低。

Conclusion: HoloEv - Net框架具有可扩展性和有效性，轻量级变体适合边缘部署。

Abstract: Event-based Action Recognition (EAR) has attracted significant attention due to the high temporal resolution and high dynamic range of event cameras. However, existing methods typically suffer from (i) the computational redundancy of dense voxel representations, (ii) structural redundancy inherent in multi-branch architectures, and (iii) the under-utilization of spectral information in capturing global motion patterns. To address these challenges, we propose an efficient EAR framework named HoloEv-Net. First, to simultaneously tackle representation and structural redundancies, we introduce a Compact Holographic Spatiotemporal Representation (CHSR). Departing from computationally expensive voxel grids, CHSR implicitly embeds horizontal spatial cues into the Time-Height (T-H) view, effectively preserving 3D spatiotemporal contexts within a 2D representation. Second, to exploit the neglected spectral cues, we design a Global Spectral Gating (GSG) module. By leveraging the Fast Fourier Transform (FFT) for global token mixing in the frequency domain, GSG enhances the representation capability with negligible parameter overhead. Extensive experiments demonstrate the scalability and effectiveness of our framework. Specifically, HoloEv-Net-Base achieves state-of-the-art performance on THU-EACT-50-CHL, HARDVS and DailyDVS-200, outperforming existing methods by 10.29%, 1.71% and 6.25%, respectively. Furthermore, our lightweight variant, HoloEv-Net-Small, delivers highly competitive accuracy while offering extreme efficiency, reducing parameters by 5.4 times, FLOPs by 300times, and latency by 2.4times compared to heavy baselines, demonstrating its potential for edge deployment.

</details>


### [253] [Natural Language Instructions for Scene-Responsive Human-in-the-Loop Motion Planning in Autonomous Driving using Vision-Language-Action Models](https://arxiv.org/abs/2602.04184)
*Angel Martinez-Sanchez,Parthib Roy,Ross Greer*

Main category: cs.CV

TL;DR: 本文利用doScenes数据集，将OpenEMMA框架应用于指令驱动驾驶，研究人类指令提示对驾驶行为的影响，结果表明指令调节能提高鲁棒性和轨迹对齐，还讨论了‘好’指令的标准并发布评估脚本。


<details>
  <summary>Details</summary>
Motivation: 解决现有指令跟随规划器依赖模拟或固定命令词汇、限制现实世界泛化的问题，实现指令驱动驾驶。

Method: 将doScenes指令作为乘客式提示集成到OpenEMMA的视觉 - 语言界面中，在轨迹生成前进行语言调节。

Result: 指令调节大幅提高鲁棒性，平均ADE降低98.7%；去除异常值后，好的提示可使ADE最多提高5.1%。

Conclusion: 讨论了OpenEMMA框架‘好’指令的标准，发布评估提示和脚本以建立可复现的指令感知规划基线。

Abstract: Instruction-grounded driving, where passenger language guides trajectory planning, requires vehicles to understand intent before motion. However, most prior instruction-following planners rely on simulation or fixed command vocabularies, limiting real-world generalization. doScenes, the first real-world dataset linking free-form instructions (with referentiality) to nuScenes ground-truth motion, enables instruction-conditioned planning. In this work, we adapt OpenEMMA, an open-source MLLM-based end-to-end driving framework that ingests front-camera views and ego-state and outputs 10-step speed-curvature trajectories, to this setting, presenting a reproducible instruction-conditioned baseline on doScenes and investigate the effects of human instruction prompts on predicted driving behavior. We integrate doScenes directives as passenger-style prompts within OpenEMMA's vision-language interface, enabling linguistic conditioning before trajectory generation. Evaluated on 849 annotated scenes using ADE, we observe that instruction conditioning substantially improves robustness by preventing extreme baseline failures, yielding a 98.7% reduction in mean ADE. When such outliers are removed, instructions still influence trajectory alignment, with well-phrased prompts improving ADE by up to 5.1%. We use this analysis to discuss what makes a "good" instruction for the OpenEMMA framework. We release the evaluation prompts and scripts to establish a reproducible baseline for instruction-aware planning. GitHub: https://github.com/Mi3-Lab/doScenes-VLM-Planning

</details>


### [254] [ACIL: Active Class Incremental Learning for Image Classification](https://arxiv.org/abs/2602.04252)
*Aditya R. Bhattacharya,Debanjan Goswami,Shayok Chakraborty*

Main category: cs.CV

TL;DR: 提出用于类别增量学习设置的主动学习框架ACIL，可降低标注成本并避免灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 现有类别增量学习方法假设所有训练样本都有标注，导致巨大标注成本和标注精力浪费，需减少人工标注精力。

Method: 提出ACIL框架，利用基于不确定性和多样性的标准识别每一轮需要标注的样本，并添加到下一轮数据中。

Result: 在多个视觉数据集上的大量实证分析表明，该框架相比相关基线方法有良好表现。

Conclusion: 所提出的ACIL框架能大幅降低标注成本，避免灾难性遗忘，有应用潜力。

Abstract: Continual learning (or class incremental learning) is a realistic learning scenario for computer vision systems, where deep neural networks are trained on episodic data, and the data from previous episodes are generally inaccessible to the model. Existing research in this domain has primarily focused on avoiding catastrophic forgetting, which occurs due to the continuously changing class distributions in each episode and the inaccessibility of the data from previous episodes. However, these methods assume that all the training samples in every episode are annotated; this not only incurs a huge annotation cost, but also results in a wastage of annotation effort, since most of the samples in a given episode will not be accessible to the model in subsequent episodes. Active learning algorithms identify the salient and informative samples from large amounts of unlabeled data and are instrumental in reducing the human annotation effort in inducing a deep neural network. In this paper, we propose ACIL, a novel active learning framework for class incremental learning settings. We exploit a criterion based on uncertainty and diversity to identify the exemplar samples that need to be annotated in each episode, and will be appended to the data in the next episode. Such a framework can drastically reduce annotation cost and can also avoid catastrophic forgetting. Our extensive empirical analyses on several vision datasets corroborate the promise and potential of our framework against relevant baselines.

</details>


### [255] [SkeletonGaussian: Editable 4D Generation through Gaussian Skeletonization](https://arxiv.org/abs/2602.04271)
*Lifan Wu,Ruijie Zhu,Yubo Ai,Tianzhu Zhang*

Main category: cs.CV

TL;DR: 提出SkeletonGaussian框架从单目视频生成可编辑动态3D高斯，实验显示效果好且可直观编辑运动


<details>
  <summary>Details</summary>
Motivation: 现有4D生成方法将运动表示为隐式变形场，限制直接控制和可编辑性

Method: 引入层次化关节表示，将运动分解为骨架驱动的稀疏刚性运动和细粒度非刚性运动，提取骨架并通过线性混合蒙皮驱动刚性运动，用六平面细化非刚性变形

Result: SkeletonGaussian在生成质量上超越现有方法

Conclusion: SkeletonGaussian为可编辑4D生成建立了新范式

Abstract: 4D generation has made remarkable progress in synthesizing dynamic 3D objects from input text, images, or videos. However, existing methods often represent motion as an implicit deformation field, which limits direct control and editability. To address this issue, we propose SkeletonGaussian, a novel framework for generating editable dynamic 3D Gaussians from monocular video input. Our approach introduces a hierarchical articulated representation that decomposes motion into sparse rigid motion explicitly driven by a skeleton and fine-grained non-rigid motion. Concretely, we extract a robust skeleton and drive rigid motion via linear blend skinning, followed by a hexplane-based refinement for non-rigid deformations, enhancing interpretability and editability. Experimental results demonstrate that SkeletonGaussian surpasses existing methods in generation quality while enabling intuitive motion editing, establishing a new paradigm for editable 4D generation. Project page: https://wusar.github.io/projects/skeletongaussian/

</details>


### [256] [Beyond Static Cropping: Layer-Adaptive Visual Localization and Decoding Enhancement](https://arxiv.org/abs/2602.04304)
*Zipeng Zhu,Zhanghao Hu,Qinglin Zhu,Yuxi Hong,Yijun Liu,Jingyong Su,Yulan He,Lin Gui*

Main category: cs.CV

TL;DR: LVLMs有局限性，本文提出动态视角的视觉定位方法VAQ和LASER，提升VQA准确度。


<details>
  <summary>Details</summary>
Motivation: 现有LVLMs因固定视觉令牌预算存在问题，且注意力引导增强方法有局限，不适用于复杂推理任务。

Method: 通过层灵敏度分析发现视觉定位是动态过程，提出VAQ识别与查询特定视觉定位最相关的层，基于此提出LASER进行自适应推理。

Result: 在不同VQA基准测试中，LASER显著提高了不同复杂度任务的VQA准确率。

Conclusion: 动态视角的视觉定位方法有效，LASER能提升复杂推理任务的性能。

Abstract: Large Vision-Language Models (LVLMs) have advanced rapidly by aligning visual patches with the text embedding space, but a fixed visual-token budget forces images to be resized to a uniform pretraining resolution, often erasing fine-grained details and causing hallucinations via over-reliance on language priors. Recent attention-guided enhancement (e.g., cropping or region-focused attention allocation) alleviates this, yet it commonly hinges on a static "magic layer" empirically chosen on simple recognition benchmarks and thus may not transfer to complex reasoning tasks. In contrast to this static assumption, we propose a dynamic perspective on visual grounding. Through a layer-wise sensitivity analysis, we demonstrate that visual grounding is a dynamic process: while simple object recognition tasks rely on middle layers, complex visual search and reasoning tasks require visual information to be reactivated at deeper layers. Based on this observation, we introduce Visual Activation by Query (VAQ), a metric that identifies the layer whose attention map is most relevant to query-specific visual grounding by measuring attention sensitivity to the input query. Building on VAQ, we further propose LASER (Layer-adaptive Attention-guided Selective visual and decoding Enhancement for Reasoning), a training-free inference procedure that adaptively selects task-appropriate layers for visual localization and question answering. Experiments across diverse VQA benchmarks show that LASER significantly improves VQA accuracy across tasks with varying levels of complexity.

</details>


### [257] [Fine-tuning Pre-trained Vision-Language Models in a Human-Annotation-Free Manner](https://arxiv.org/abs/2602.04337)
*Qian-Wei Wang,Guanghao Meng,Ren Cai,Yaguang Song,Shu-Tao Xia*

Main category: cs.CV

TL;DR: 现有无监督自训练方法有不足，提出CoFT和CoFT+框架，实验显示效果好。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型适应下游任务需昂贵标签数据，现有无监督自训练方法有可靠性等问题。

Method: 提出CoFT框架，采用双模型跨模态协作机制、双提示学习策略及两阶段训练方案；CoFT+通过迭代微调等增强适应。

Result: 在实验中比现有无监督方法和少样本监督基线有持续改进。

Conclusion: 提出的方法有效，能改进VLMs在下游任务中的无监督适应。

Abstract: Large-scale vision-language models (VLMs) such as CLIP exhibit strong zero-shot generalization, but adapting them to downstream tasks typically requires costly labeled data. Existing unsupervised self-training methods rely on pseudo-labeling, yet often suffer from unreliable confidence filtering, confirmation bias, and underutilization of low-confidence samples. We propose Collaborative Fine-Tuning (CoFT), an unsupervised adaptation framework that leverages unlabeled data through a dual-model, cross-modal collaboration mechanism. CoFT introduces a dual-prompt learning strategy with positive and negative textual prompts to explicitly model pseudo-label cleanliness in a sample-dependent manner, removing the need for hand-crafted thresholds or noise assumptions. The negative prompt also regularizes lightweight visual adaptation modules, improving robustness under noisy supervision. CoFT employs a two-phase training scheme, transitioning from parameter-efficient fine-tuning on high-confidence samples to full fine-tuning guided by collaboratively filtered pseudo-labels. Building on CoFT, CoFT+ further enhances adaptation via iterative fine-tuning, momentum contrastive learning, and LLM-generated prompts. Extensive experiments demonstrate consistent gains over existing unsupervised methods and even few-shot supervised baselines.

</details>


### [258] [Explicit Uncertainty Modeling for Active CLIP Adaptation with Dual Prompt Tuning](https://arxiv.org/abs/2602.04340)
*Qian-Wei Wang,Yaguang Song,Shu-Tao Xia*

Main category: cs.CV

TL;DR: 提出基于双提示调优的主动CLIP自适应不确定性建模框架，在不同微调范式实验中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 预训练视觉语言模型应用于低标注预算的下游图像分类任务有挑战，现有主动学习方法未从模型角度明确建模不确定性。

Method: 在CLIP文本分支引入两个可学习提示，正提示增强任务特定文本嵌入的可区分性，负提示反向训练以明确建模预测标签正确性概率来指导样本选择。

Result: 在不同微调范式的广泛实验中，该方法在相同标注预算下始终优于现有主动学习方法。

Conclusion: 所提基于双提示调优的不确定性建模框架在主动CLIP自适应中效果良好。

Abstract: Pre-trained vision-language models such as CLIP exhibit strong transferability, yet adapting them to downstream image classification tasks under limited annotation budgets remains challenging. In active learning settings, the model must select the most informative samples for annotation from a large pool of unlabeled data. Existing approaches typically estimate uncertainty via entropy-based criteria or representation clustering, without explicitly modeling uncertainty from the model perspective. In this work, we propose a robust uncertainty modeling framework for active CLIP adaptation based on dual-prompt tuning. We introduce two learnable prompts in the textual branch of CLIP. The positive prompt enhances the discriminability of task-specific textual embeddings corresponding to light-weight tuned visual embeddings, improving classification reliability. Meanwhile, the negative prompt is trained in an reversed manner to explicitly model the probability that the predicted label is correct, providing a principled uncertainty signal for guiding active sample selection. Extensive experiments across different fine-tuning paradigms demonstrate that our method consistently outperforms existing active learning methods under the same annotation budget.

</details>


### [259] [VecSet-Edit: Unleashing Pre-trained LRM for Mesh Editing from Single Image](https://arxiv.org/abs/2602.04349)
*Teng-Fang Hsiao,Bo-Kai Ruan,Yu-Lun Liu,Hong-Han Shuai*

Main category: cs.CV

TL;DR: 文章提出VecSet - Edit用于3D网格编辑，利用VecSet LRM模型，有定位、去噪和纹理烘焙模块，可保留几何和纹理信息。


<details>
  <summary>Details</summary>
Motivation: 当前3D编辑多关注3D高斯溅射或多视图图像，3D网格直接编辑研究不足，现有方法如VoxHammer有分辨率低和需大量人力制作3D掩码的问题。

Method: 以VecSet LRM为骨干，分析VecSet token空间属性，采用掩码引导的token播种和注意力对齐的token门控策略定位目标区域，设计漂移感知的token修剪去除噪声，有细节保留的纹理烘焙模块。

Result: 未提及具体结果，可在项目页面查看更多细节。

Conclusion: 未明确提及结论。

Abstract: 3D editing has emerged as a critical research area to provide users with flexible control over 3D assets. While current editing approaches predominantly focus on 3D Gaussian Splatting or multi-view images, the direct editing of 3D meshes remains underexplored. Prior attempts, such as VoxHammer, rely on voxel-based representations that suffer from limited resolution and necessitate labor-intensive 3D mask. To address these limitations, we propose \textbf{VecSet-Edit}, the first pipeline that leverages the high-fidelity VecSet Large Reconstruction Model (LRM) as a backbone for mesh editing. Our approach is grounded on a analysis of the spatial properties in VecSet tokens, revealing that token subsets govern distinct geometric regions. Based on this insight, we introduce Mask-guided Token Seeding and Attention-aligned Token Gating strategies to precisely localize target regions using only 2D image conditions. Also, considering the difference between VecSet diffusion process versus voxel we design a Drift-aware Token Pruning to reject geometric outliers during the denoising process. Finally, our Detail-preserving Texture Baking module ensures that we not only preserve the geometric details of original mesh but also the textural information. More details can be found in our project page: https://github.com/BlueDyee/VecSet-Edit/tree/main

</details>


### [260] [SparVAR: Exploring Sparsity in Visual AutoRegressive Modeling for Training-Free Acceleration](https://arxiv.org/abs/2602.04361)
*Zekun Li,Ning Wang,Tongxin Bai,Changwang Mei,Peisong Wang,Shuang Qiu,Jian Cheng*

Main category: cs.CV

TL;DR: 提出训练无关加速框架SparVAR，利用VAR注意力特性实现高效稀疏注意力计算，加速高分辨率图像生成，保留高频细节。


<details>
  <summary>Details</summary>
Motivation: 主流VAR范式计算复杂度高、有延迟，现有加速方法会丢弃高频细节，影响图像质量。

Method: 利用VAR注意力三个特性，动态预测稀疏注意力模式，构建自相似稀疏注意力，提出跨尺度局部稀疏注意力并实现高效块稀疏内核。

Result: SparseVAR能将8B模型生成1024×1024高分辨率图像时间降至1秒，比FlashAttention加速的VAR基线快1.57倍，结合现有策略可达2.28倍加速。

Conclusion: SparVAR可高效加速高分辨率图像生成，保留高频细节，结合现有策略能进一步提升加速效果。

Abstract: Visual AutoRegressive (VAR) modeling has garnered significant attention for its innovative next-scale prediction paradigm. However, mainstream VAR paradigms attend to all tokens across historical scales at each autoregressive step. As the next scale resolution grows, the computational complexity of attention increases quartically with resolution, causing substantial latency. Prior accelerations often skip high-resolution scales, which speeds up inference but discards high-frequency details and harms image quality. To address these problems, we present SparVAR, a training-free acceleration framework that exploits three properties of VAR attention: (i) strong attention sinks, (ii) cross-scale activation similarity, and (iii) pronounced locality. Specifically, we dynamically predict the sparse attention pattern of later high-resolution scales from a sparse decision scale, and construct scale self-similar sparse attention via an efficient index-mapping mechanism, enabling high-efficiency sparse attention computation at large scales. Furthermore, we propose cross-scale local sparse attention and implement an efficient block-wise sparse kernel, which achieves $\mathbf{> 5\times}$ faster forward speed than FlashAttention. Extensive experiments demonstrate that the proposed SparseVAR can reduce the generation time of an 8B model producing $1024\times1024$ high-resolution images to the 1s, without skipping the last scales. Compared with the VAR baseline accelerated by FlashAttention, our method achieves a $\mathbf{1.57\times}$ speed-up while preserving almost all high-frequency details. When combined with existing scale-skipping strategies, SparseVAR attains up to a $\mathbf{2.28\times}$ acceleration, while maintaining competitive visual generation quality. Code is available at https://github.com/CAS-CLab/SparVAR.

</details>


### [261] [Enabling Real-Time Colonoscopic Polyp Segmentation on Commodity CPUs via Ultra-Lightweight Architecture](https://arxiv.org/abs/2602.04381)
*Weihao Gao,Zhuo Deng,Zheng Gong,Lan Ma*

Main category: cs.CV

TL;DR: 提出UltraSeg系列极端压缩模型，能在单CPU核心上高速运行且精度高，为资源受限场景提供可行方案。


<details>
  <summary>Details</summary>
Motivation: 现有高精度分割模型依赖GPU，难以在基层医院、移动内镜单元或胶囊机器人中部署，需开发适合资源受限场景的模型。

Method: 联合优化编码器 - 解码器宽度，采用约束扩张卷积扩大感受野，集成跨层轻量级融合模块。

Result: UltraSeg系列模型在单CPU核心上达90 FPS，在七个公共数据集上保留了31 M参数U - Net 94%以上的Dice分数，仅使用其0.4%的参数。

Conclusion: 为结肠镜检查提供CPU原生解决方案，为更广泛的微创手术视觉应用提供可复制蓝图，代码公开以确保可重复性和便于基准测试。

Abstract: Early detection of colorectal cancer hinges on real-time, accurate polyp identification and resection. Yet current high-precision segmentation models rely on GPUs, making them impractical to deploy in primary hospitals, mobile endoscopy units, or capsule robots. To bridge this gap, we present the UltraSeg family, operating in an extreme-compression regime (<0.3 M parameters). UltraSeg-108K (0.108 M parameters) is optimized for single-center data, while UltraSeg-130K (0.13 M parameters) generalizes to multi-center, multi-modal images. By jointly optimizing encoder-decoder widths, incorporating constrained dilated convolutions to enlarge receptive fields, and integrating a cross-layer lightweight fusion module, the models achieve 90 FPS on a single CPU core without sacrificing accuracy. Evaluated on seven public datasets, UltraSeg retains >94% of the Dice score of a 31 M-parameter U-Net while utilizing only 0.4% of its parameters, establishing a strong, clinically viable baseline for the extreme-compression domain and offering an immediately deployable solution for resource-constrained settings. This work provides not only a CPU-native solution for colonoscopy but also a reproducible blueprint for broader minimally invasive surgical vision applications. Source code is publicly available to ensure reproducibility and facilitate future benchmarking.

</details>


### [262] [Med-MMFL: A Multimodal Federated Learning Benchmark in Healthcare](https://arxiv.org/abs/2602.04416)
*Aavash Chhetri,Bibek Niroula,Pratik Shrestha,Yash Raj Shrestha,Lesley A Anderson,Prashnna K Gyawali,Loris Bazzani,Binod Bhattarai*

Main category: cs.CV

TL;DR: 本文提出医学多模态联邦学习基准Med - MMFL，评估六种算法，涵盖多样任务和数据模态，模拟不同场景实验并开源代码。


<details>
  <summary>Details</summary>
Motivation: 医学联邦学习基准稀缺，且现有集中在单/双模态及有限任务，需标准化评估以提升对医学多模态联邦学习的系统理解。

Method: 引入Med - MMFL基准，评估六种代表性的联邦学习算法，在不同数据集和场景下进行实验，涵盖多种任务。

Result: 完成多种场景下的多任务实验。

Conclusion: 发布完整基准实现，支持未来多模态联邦学习方法在现实医学环境中的复现和公平比较。

Abstract: Federated learning (FL) enables collaborative model training across decentralized medical institutions while preserving data privacy. However, medical FL benchmarks remain scarce, with existing efforts focusing mainly on unimodal or bimodal modalities and a limited range of medical tasks. This gap underscores the need for standardized evaluation to advance systematic understanding in medical MultiModal FL (MMFL). To this end, we introduce Med-MMFL, the first comprehensive MMFL benchmark for the medical domain, encompassing diverse modalities, tasks, and federation scenarios. Our benchmark evaluates six representative state-of-the-art FL algorithms, covering different aggregation strategies, loss formulations, and regularization techniques. It spans datasets with 2 to 4 modalities, comprising a total of 10 unique medical modalities, including text, pathology images, ECG, X-ray, radiology reports, and multiple MRI sequences. Experiments are conducted across naturally federated, synthetic IID, and synthetic non-IID settings to simulate real-world heterogeneity. We assess segmentation, classification, modality alignment (retrieval), and VQA tasks. To support reproducibility and fair comparison of future multimodal federated learning (MMFL) methods under realistic medical settings, we release the complete benchmark implementation, including data processing and partitioning pipelines, at https://github.com/bhattarailab/Med-MMFL-Benchmark .

</details>


### [263] [SLUM-i: Semi-supervised Learning for Urban Mapping of Informal Settlements and Data Quality Benchmarking](https://arxiv.org/abs/2602.04525)
*Muhammad Taha Mukhtar,Syed Musa Ali Kazmi,Khola Naseem,Muhammad Ali Chattha,Andreas Dengel,Sheraz Ahmed,Muhammad Naseer Bajwa,Muhammad Imran Malik*

Main category: cs.CV

TL;DR: 本文构建了拉合尔、卡拉奇和孟买的非正规住区数据集，提出半监督分割框架，在多城市实验中表现优于现有基线，有出色的域转移能力。


<details>
  <summary>Details</summary>
Motivation: 快速城市扩张使低收入和中等收入国家大城市非正规住区增多，但大规模绘制这些住区受标注稀缺和数据质量问题限制。

Method: 构建拉合尔基准数据集及相关数据集，对所有数据集进行数据质量评估；提出半监督分割框架，集成类感知自适应阈值机制和原型库系统。

Result: 在三大洲八个城市的实验中，方法优于现有半监督基线，仅用10%源标签训练的模型在未见地理区域达到0.461的mIoU，优于全监督模型的零样本泛化能力。

Conclusion: 提出的方法能有效缓解标注和数据质量问题，在非正规住区大规模绘制上有良好效果和强大的域转移能力。

Abstract: Rapid urban expansion has fueled the growth of informal settlements in major cities of low- and middle-income countries, with Lahore and Karachi in Pakistan and Mumbai in India serving as prominent examples. However, large-scale mapping of these settlements is severely constrained not only by the scarcity of annotations but by inherent data quality challenges, specifically high spectral ambiguity between formal and informal structures and significant annotation noise. We address this by introducing a benchmark dataset for Lahore, constructed from scratch, along with companion datasets for Karachi and Mumbai, which were derived from verified administrative boundaries, totaling 1,869 $\text{km}^2$ of area. To evaluate the global robustness of our framework, we extend our experiments to five additional established benchmarks, encompassing eight cities across three continents, and provide comprehensive data quality assessments of all datasets. We also propose a new semi-supervised segmentation framework designed to mitigate the class imbalance and feature degradation inherent in standard semi-supervised learning pipelines. Our method integrates a Class-Aware Adaptive Thresholding mechanism that dynamically adjusts confidence thresholds to prevent minority class suppression and a Prototype Bank System that enforces semantic consistency by anchoring predictions to historically learned high-fidelity feature representations. Extensive experiments across a total of eight cities spanning three continents demonstrate that our approach outperforms state-of-the-art semi-supervised baselines. Most notably, our method demonstrates superior domain transfer capability whereby a model trained on only 10% of source labels reaches a 0.461 mIoU on unseen geographies and outperforms the zero-shot generalization of fully supervised models.

</details>


### [264] [OmniRad: A Radiological Foundation Model for Multi-Task Medical Image Analysis](https://arxiv.org/abs/2602.04547)
*Luca Zedda,Andrea Loddo,Cecilia Di Ruberto*

Main category: cs.CV

TL;DR: 介绍自监督放射学基础模型OmniRad，在多基准测试中评估，表现优于竞品。


<details>
  <summary>Details</summary>
Motivation: 利用预训练视觉表征支持放射学下游任务，强调表征复用和跨任务迁移性。

Method: 在120万张医学图像上预训练OmniRad，采用多种下游适应机制评估。

Result: 在MedMNISTv2上F1提高最多2.05%，在MedSegBench数据集上Dice分数提升，定性分析显示特征聚类和模态分离更好。

Conclusion: OmniRad具有良好表征质量和任务特定性能，能提升下游任务表现。

Abstract: Radiological analysis increasingly benefits from pretrained visual representations that can support heterogeneous downstream tasks across imaging modalities. In this work, we introduce OmniRad, a self-supervised radiological foundation model pretrained on 1.2 million medical images, designed with radiology-inspired principles emphasizing representation reuse and cross-task transferability. We evaluate the pretrained encoder under multiple downstream adaptation regimes, including lightweight task-specific adapters with a frozen backbone as well as full end-to-end fine-tuning for classification, allowing us to assess both representation quality and task-specific performance. OmniRad is evaluated on a broad suite of public benchmarks spanning classification and segmentation across multiple modalities. On the MedMNISTv2 collection, OmniRad improves classification F1 by up to 2.05% over competing foundation models. For dense prediction, OmniRad attains mean Dice score improvements across six MedSegBench datasets when using frozen representations. Qualitative analyses and latent-space visualizations suggest improved feature clustering and modality-related separation.

</details>


### [265] [DRMOT: A Dataset and Framework for RGBD Referring Multi-Object Tracking](https://arxiv.org/abs/2602.04692)
*Sijia Chen,Lijuan Ma,Yanqiu Yu,En Yu,Liman Liu,Wenbing Tao*

Main category: cs.CV

TL;DR: 提出RGBD Referring Multi-Object Tracking (DRMOT)任务，构建DRSet数据集，提出DRTrack框架并验证有效性。


<details>
  <summary>Details</summary>
Motivation: 现有RMOT模型仅依赖2D RGB数据，缺乏3D空间信息，难以处理复杂空间语义目标检测与关联，以及严重遮挡下的身份维护。

Method: 提出DRMOT任务，构建DRSet数据集，提出MLLM引导的DRTrack框架进行深度感知目标定位和轨迹关联。

Result: 在DRSet数据集上的大量实验表明DRTrack框架有效。

Conclusion: 所提DRMOT任务、DRSet数据集和DRTrack框架有助于解决现有RMOT模型问题，推动相关研究。

Abstract: Referring Multi-Object Tracking (RMOT) aims to track specific targets based on language descriptions and is vital for interactive AI systems such as robotics and autonomous driving. However, existing RMOT models rely solely on 2D RGB data, making it challenging to accurately detect and associate targets characterized by complex spatial semantics (e.g., ``the person closest to the camera'') and to maintain reliable identities under severe occlusion, due to the absence of explicit 3D spatial information. In this work, we propose a novel task, RGBD Referring Multi-Object Tracking (DRMOT), which explicitly requires models to fuse RGB, Depth (D), and Language (L) modalities to achieve 3D-aware tracking. To advance research on the DRMOT task, we construct a tailored RGBD referring multi-object tracking dataset, named DRSet, designed to evaluate models' spatial-semantic grounding and tracking capabilities. Specifically, DRSet contains RGB images and depth maps from 187 scenes, along with 240 language descriptions, among which 56 descriptions incorporate depth-related information. Furthermore, we propose DRTrack, a MLLM-guided depth-referring tracking framework. DRTrack performs depth-aware target grounding from joint RGB-D-L inputs and enforces robust trajectory association by incorporating depth cues. Extensive experiments on the DRSet dataset demonstrate the effectiveness of our framework.

</details>


### [266] [SAR-RAG: ATR Visual Question Answering by Semantic Search, Retrieval, and MLLM Generation](https://arxiv.org/abs/2602.04712)
*David F. Ramirez,Tim Overman,Kristen Jaskie,Joe Marvin,Andreas Spanias*

Main category: cs.CV

TL;DR: 提出用于合成孔径雷达自动目标识别的ImageRAG辅助AI代理SAR - RAG，结合MLLM和向量数据库，提升识别准确率。


<details>
  <summary>Details</summary>
Motivation: 合成孔径雷达在国防安全中用于检测军事车辆，但车辆在图像中难以区分，需提高SAR自动目标识别能力。

Method: 提出SAR - RAG方法，将多模态大语言模型与语义嵌入向量数据库结合，进行图像示例的上下文搜索。

Result: 通过搜索和检索指标、分类准确率和车辆尺寸数值回归评估，添加SAR - RAG到MLLM基线方法作为ATR记忆库，各项指标均有提升。

Conclusion: SAR - RAG系统可比较相似车辆类别，提高自动目标识别预测的准确性。

Abstract: We present a visual-context image retrieval-augmented generation (ImageRAG) assisted AI agent for automatic target recognition (ATR) of synthetic aperture radar (SAR). SAR is a remote sensing method used in defense and security applications to detect and monitor the positions of military vehicles, which may appear indistinguishable in images. Researchers have extensively studied SAR ATR to improve the differentiation and identification of vehicle types, characteristics, and measurements. Test examples can be compared with known vehicle target types to improve recognition tasks. New methods enhance the capabilities of neural networks, transformer attention, and multimodal large language models. An agentic AI method may be developed to utilize a defined set of tools, such as searching through a library of similar examples. Our proposed method, SAR Retrieval-Augmented Generation (SAR-RAG), combines a multimodal large language model (MLLM) with a vector database of semantic embeddings to support contextual search for image exemplars with known qualities. By recovering past image examples with known true target types, our SAR-RAG system can compare similar vehicle categories, achieving improved ATR prediction accuracy. We evaluate this through search and retrieval metrics, categorical classification accuracy, and numeric regression of vehicle dimensions. These metrics all show improvements when SAR-RAG is added to an MLLM baseline method as an attached ATR memory bank.

</details>


### [267] [Benchmarking Bias Mitigation Toward Fairness Without Harm from Vision to LVLMs](https://arxiv.org/abs/2602.03895)
*Xuwei Tan,Ziyu Hu,Xueru Zhang*

Main category: cs.CV

TL;DR: 提出统一基准NH - Fair用于公平性评估，给出调优建议，发现部分去偏方法表现，分析LVLMs公平性。


<details>
  <summary>Details</summary>
Motivation: 现有数据训练的机器学习模型有偏见问题，且难以比较不同偏差缓解方法的有效性。

Method: 引入NH - Fair，对标准数据、指标和训练协议下的视觉模型和大型视觉语言模型进行评估，开展系统的ERM调优研究。

Result: 明确对实用性和差异有重大影响的训练选择；许多去偏方法不如调优后的ERM基线，复合数据增强方法效果好；LVLMs有子组差异，扩展带来的改进小于架构或训练协议选择。

Conclusion: NH - Fair提供了可重现、考虑调优的严格且关注危害的公平性评估流程。

Abstract: Machine learning models trained on real-world data often inherit and amplify biases against certain social groups, raising urgent concerns about their deployment at scale. While numerous bias mitigation methods have been proposed, comparing the effectiveness of bias mitigation methods remains difficult due to heterogeneous datasets, inconsistent fairness metrics, isolated evaluation of vision versus multi-modal models, and insufficient hyperparameter tuning that undermines fair comparisons. We introduce NH-Fair, a unified benchmark for fairness without harm that spans both vision models and large vision-language models (LVLMs) under standardized data, metrics, and training protocols, covering supervised and zero-shot regimes. Our key contributions are: (1) a systematic ERM tuning study that identifies training choices with large influence on both utility and disparities, yielding empirically grounded guidelines to help practitioners reduce expensive hyperparameter tuning space in achieving strong fairness and accuracy; (2) evidence that many debiasing methods do not reliably outperform a well-tuned ERM baseline, whereas a composite data-augmentation method consistently delivers parity gains without sacrificing utility, emerging as a promising practical strategy. (3) an analysis showing that while LVLMs achieve higher average accuracy, they still exhibit subgroup disparities, and gains from scaling are typically smaller than those from architectural or training-protocol choices. NH-Fair provides a reproducible, tuning-aware pipeline for rigorous, harm-aware fairness evaluation.

</details>


### [268] [Toward Reliable and Explainable Nail Disease Classification: Leveraging Adversarial Training and Grad-CAM Visualization](https://arxiv.org/abs/2602.04820)
*Farzia Hossain,Samanta Ghosh,Shahida Begum,B. M. Shahria Alam,Mohammad Tahmid Noor,Md Parvez Mia,Nishat Tasnim Niloy*

Main category: cs.CV

TL;DR: 本文基于公开数据集提出机器学习模型自动分类指甲疾病，利用四种CNN模型评估，InceptionV3表现最佳，还采用对抗训练和SHAP助力，可辅助医生诊断。


<details>
  <summary>Details</summary>
Motivation: 人类指甲疾病易被忽视，早期准确诊断重要但因疾病类型视觉差异大而有挑战，因此需开发自动分类模型。

Method: 基于含3835张六类图像的公开数据集，将图像统一调整为224x224像素，训练分析InceptionV3、DenseNet201、EfficientNetV2和ResNet50四种CNN模型，采用对抗训练增强模型，用SHAP解释模型决策。

Result: InceptionV3表现最佳，准确率达95.57%，DenseNet201次之，准确率94.79%。

Conclusion: 该系统可辅助医生，使指甲疾病诊断更准确快速。

Abstract: Human nail diseases are gradually observed over all age groups, especially among older individuals, often going ignored until they become severe. Early detection and accurate diagnosis of such conditions are important because they sometimes reveal our body's health problems. But it is challenging due to the inferred visual differences between disease types. This paper presents a machine learning-based model for automated classification of nail diseases based on a publicly available dataset, which contains 3,835 images scaling six categories. In 224x224 pixels, all images were resized to ensure consistency. To evaluate performance, four well-known CNN models-InceptionV3, DenseNet201, EfficientNetV2, and ResNet50 were trained and analyzed. Among these, InceptionV3 outperformed the others with an accuracy of 95.57%, while DenseNet201 came next with 94.79%. To make the model stronger and less likely to make mistakes on tricky or noisy images, we used adversarial training. To help understand how the model makes decisions, we used SHAP to highlight important features in the predictions. This system could be a helpful support for doctors, making nail disease diagnosis more accurate and faster.

</details>


### [269] [AGMA: Adaptive Gaussian Mixture Anchors for Prior-Guided Multimodal Human Trajectory Forecasting](https://arxiv.org/abs/2602.04204)
*Chao Li,Rui Zhang,Siyuan Huang,Xian Zhong,Hongbo Jiang*

Main category: cs.CV

TL;DR: 现有行人轨迹预测方法存在先验不对齐问题，提出AGMA构建先验，实验证明其达SOTA，凸显高质量先验重要性。


<details>
  <summary>Details</summary>
Motivation: 现有行人轨迹预测方法存在先验不对齐问题，导致预测准确性和多样性受限，且理论证明预测误差受先验质量下限约束。

Method: 提出AGMA方法，分两个阶段构建表达性先验：从训练数据中提取多样行为模式，将其提炼为场景自适应全局先验用于推理。

Result: 在ETH - UCY、Stanford Drone和JRDB数据集上实验，AGMA达到了SOTA性能。

Conclusion: 高质量先验在轨迹预测中起着关键作用。

Abstract: Human trajectory forecasting requires capturing the multimodal nature of pedestrian behavior. However, existing approaches suffer from prior misalignment. Their learned or fixed priors often fail to capture the full distribution of plausible futures, limiting both prediction accuracy and diversity. We theoretically establish that prediction error is lower-bounded by prior quality, making prior modeling a key performance bottleneck. Guided by this insight, we propose AGMA (Adaptive Gaussian Mixture Anchors), which constructs expressive priors through two stages: extracting diverse behavioral patterns from training data and distilling them into a scene-adaptive global prior for inference. Extensive experiments on ETH-UCY, Stanford Drone, and JRDB datasets demonstrate that AGMA achieves state-of-the-art performance, confirming the critical role of high-quality priors in trajectory forecasting.

</details>


### [270] [SPOT-Occ: Sparse Prototype-guided Transformer for Camera-based 3D Occupancy Prediction](https://arxiv.org/abs/2602.04240)
*Suzeyu Chen,Leheng Li,Ying-Cong Chen*

Main category: cs.CV

TL;DR: 提出基于原型的稀疏Transformer解码器SPOT - Occ用于3D占用预测，速度和准确性优于之前方法，代码开源。


<details>
  <summary>Details</summary>
Motivation: 解决从相机实现高精度实时3D占用预测时，稀疏3D表示给解码器带来的如何高效聚合体素特征信息的挑战。

Method: 提出基于原型的稀疏Transformer解码器，采用稀疏原型选择机制使解码器注意力受原型引导，引入互补去噪范式保证动态选择稳定有效。

Result: 模型SPOT - Occ在速度上大幅超越先前方法，同时提高了准确性。

Conclusion: 所提出的方法能有效解决3D占用预测中解码器的信息聚合问题，实现更优的性能。

Abstract: Achieving highly accurate and real-time 3D occupancy prediction from cameras is a critical requirement for the safe and practical deployment of autonomous vehicles. While this shift to sparse 3D representations solves the encoding bottleneck, it creates a new challenge for the decoder: how to efficiently aggregate information from a sparse, non-uniformly distributed set of voxel features without resorting to computationally prohibitive dense attention.
  In this paper, we propose a novel Prototype-based Sparse Transformer Decoder that replaces this costly interaction with an efficient, two-stage process of guided feature selection and focused aggregation. Our core idea is to make the decoder's attention prototype-guided. We achieve this through a sparse prototype selection mechanism, where each query adaptively identifies a compact set of the most salient voxel features, termed prototypes, for focused feature aggregation.
  To ensure this dynamic selection is stable and effective, we introduce a complementary denoising paradigm. This approach leverages ground-truth masks to provide explicit guidance, guaranteeing a consistent query-prototype association across decoder layers. Our model, dubbed SPOT-Occ, outperforms previous methods with a significant margin in speed while also improving accuracy. Source code is released at https://github.com/chensuzeyu/SpotOcc.

</details>


### [271] [XtraLight-MedMamba for Classification of Neoplastic Tubular Adenomas](https://arxiv.org/abs/2602.04819)
*Aqsa Sultana,Rayan Afsar,Ahmed Rahu,Surendra P. Singh,Brian Shula,Brandon Combs,Derrick Forchetti,Vijayan K. Asari*

Main category: cs.CV

TL;DR: 本文提出超轻量级深度学习框架XtraLight - MedMamba用于从全切片图像中分类肿瘤性管状腺瘤，模型参数少且性能优于其他架构。


<details>
  <summary>Details</summary>
Motivation: 常规结肠镜筛查中对癌前息肉进行准确风险分层对降低结直肠癌风险至关重要，但低级别异型增生评估受主观组织病理学解释限制，数字病理和深度学习发展提供新机会。

Method: 提出XtraLight - MedMamba框架，融合ConvNext浅特征提取器和并行视觉mamba，集成SCAB模块增强多尺度特征提取，使用FNOClassifier减少参数并提高泛化能力，在特定数据集上评估。

Result: XtraLight - MedMamba使用约32000个参数，准确率达97.18%，F1分数为0.9767，优于基于Transformer和传统Mamba架构。

Conclusion: XtraLight - MedMamba在分类肿瘤性管状腺瘤上表现出色，参数少且性能优于复杂度更高的模型，有应用潜力。

Abstract: Accurate risk stratification of precancerous polyps during routine colonoscopy screenings is essential for lowering the risk of developing colorectal cancer (CRC). However, assessment of low-grade dysplasia remains limited by subjective histopathologic interpretation. Advancements in digital pathology and deep learning provide new opportunities to identify subtle and fine morphologic patterns associated with malignant progression that may be imperceptible to the human eye. In this work, we propose XtraLight-MedMamba, an ultra-lightweight state-space-based deep learning framework for classifying neoplastic tubular adenomas from whole-slide images (WSIs). The architecture is a blend of ConvNext based shallow feature extractor with parallel vision mamba to efficiently model both long- and short-range dependencies and image generalization. An integration of Spatial and Channel Attention Bridge (SCAB) module enhances multiscale feature extraction, while Fixed Non-Negative Orthogonal Classifier (FNOClassifier) enables substantial parameter reduction and improved generalization. The model was evaluated on a curated dataset acquired from patients with low-grade tubular adenomas, stratified into case and control cohorts based on subsequent CRC development. XtraLight-MedMamba achieved an accuracy of 97.18% and an F1-score of 0.9767 using approximately 32,000 parameters, outperforming transformer-based and conventional Mamba architectures with significantly higher model complexity.

</details>


<div id='cs.CC'></div>

# cs.CC [[Back]](#toc)

### [272] [The Complexity of Min-Max Optimization with Product Constraints](https://arxiv.org/abs/2602.04665)
*Martino Bernasconi,Matteo Castiglioni*

Main category: cs.CC

TL;DR: 研究非凸 - 非凹效用函数博弈计算局部最小 - 最大均衡的计算复杂度，证明在乘积约束和超立方体上该问题是PPAD难的。


<details>
  <summary>Details</summary>
Motivation: Daskalakis等人的工作仅解决了玩家策略联合约束这一特殊情况下计算复杂度问题，本文旨在明确更自然约束下的复杂度。

Method: 未明确提及。

Result: 证明即使在乘积约束下，尤其是在超立方体上，该问题是PPAD难的。

Conclusion: 解决了在更自然约束下计算局部最小 - 最大均衡的复杂度问题，表明该问题是PPAD难的。

Abstract: We study the computational complexity of the problem of computing local min-max equilibria of games with a nonconvex-nonconcave utility function $f$. From the work of Daskalakis, Skoulakis, and Zampetakis [DSZ21], this problem was known to be hard in the restrictive case in which players are required to play strategies that are jointly constrained, leaving open the question of its complexity under more natural constraints. In this paper, we settle the question and show that the problem is PPAD-hard even under product constraints and, in particular, over the hypercube.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [273] [All-Atom GPCR-Ligand Simulation via Residual Isometric Latent Flow](https://arxiv.org/abs/2602.03902)
*Jiying Zhang,Shuhao Zhang,Pierre Vandergheynst,Patrick Barth*

Main category: q-bio.QM

TL;DR: 本文介绍了用于高效全原子GPCR - 配体模拟的深度生成框架GPCRLMD，实验显示其在模拟中达先进水平。


<details>
  <summary>Details</summary>
Motivation: G蛋白偶联受体（GPCRs）信号转导依赖复杂构象转变，传统全原子分子动力学（MD）模拟计算成本高，需高效模拟方法。

Method: 引入GPCRLMD框架，用Harmonic - Prior Variational Autoencoder（HP - VAE）将复合物映射到正则等距潜空间，在潜空间用Residual Latent Flow采样轨迹，再解码为原子坐标，通过相对位移捕捉时间动态。

Result: GPCRLMD在GPCR - 配体动力学模拟中达到了先进水平，能如实再现热力学可观测量和关键配体 - 受体相互作用。

Conclusion: GPCRLMD是一种有效的全原子GPCR - 配体模拟框架。

Abstract: G-protein-coupled receptors (GPCRs), primary targets for over one-third of approved therapeutics, rely on intricate conformational transitions to transduce signals. While Molecular Dynamics (MD) is essential for elucidating this transduction process, particularly within ligand-bound complexes, conventional all-atom MD simulation is computationally prohibitive. In this paper, we introduce GPCRLMD, a deep generative framework for efficient all-atom GPCR-ligand simulation.GPCRLMD employs a Harmonic-Prior Variational Autoencoder (HP-VAE) to first map the complex into a regularized isometric latent space, preserving geometric topology via physics-informed constraints. Within this latent space, a Residual Latent Flow samples evolution trajectories, which are subsequently decoded back to atomic coordinates. By capturing temporal dynamics via relative displacements anchored to the initial structure, this residual mechanism effectively decouples static topology from dynamic fluctuations. Experimental results demonstrate that GPCRLMD achieves state-of-the-art performance in GPCR-ligand dynamics simulation, faithfully reproducing thermodynamic observables and critical ligand-receptor interactions.

</details>


### [274] [Prenatal Stress Detection from Electrocardiography Using Self-Supervised Deep Learning: Development and External Validation](https://arxiv.org/abs/2602.03886)
*Martin G. Frasch,Marlene J. E. Mayer,Clara Becker,Peter Zimmermann,Camilla Zelgert,Marta C. Antonelli,Silvia M. Lobmaier*

Main category: q-bio.QM

TL;DR: 开发基于心电图的深度学习模型用于孕期压力检测，在不同数据集上表现良好，多层特征提取优于单嵌入方法。


<details>
  <summary>Details</summary>
Motivation: 产前心理压力影响部分孕期且现有筛查依赖主观问卷，限制连续监测。

Method: 使用FELICITy 1队列，通过SimCLR对比学习预训练ResNet - 34编码器，进行多层特征提取以实现分类和预测，用FELICITy 2进行外部验证。

Result: 在FELICITy 1上不同心电图准确率高；外部验证中指标不同；信号质量通道选择表现更好；混合效应模型检测到干预反应显著。

Conclusion: 妊娠心电图的自监督深度学习可实现准确、客观的压力评估，多层特征提取效果更佳。

Abstract: Prenatal psychological stress affects 15-25% of pregnancies and increases risks of preterm birth, low birth weight, and adverse neurodevelopmental outcomes. Current screening relies on subjective questionnaires (PSS-10), limiting continuous monitoring. We developed deep learning models for stress detection from electrocardiography (ECG) using the FELICITy 1 cohort (151 pregnant women, 32-38 weeks gestation). A ResNet-34 encoder was pretrained via SimCLR contrastive learning on 40,692 ECG segments per subject. Multi-layer feature extraction enabled binary classification and continuous PSS prediction across maternal (mECG), fetal (fECG), and abdominal ECG (aECG). External validation used the FELICITy 2 RCT (28 subjects, different ECG device, yoga intervention vs. control). On FELICITy 1 (5-fold CV): mECG 98.6% accuracy (R2=0.88, MAE=1.90), fECG 99.8% (R2=0.95, MAE=1.19), aECG 95.5% (R2=0.75, MAE=2.80). External validation on FELICITy 2: mECG 77.3% accuracy (R2=0.62, MAE=3.54, AUC=0.826), aECG 63.6% (R2=0.29, AUC=0.705). Signal quality-based channel selection outperformed all-channel averaging (+12% R2 improvement). Mixed-effects models detected a significant intervention response (p=0.041). Self-supervised deep learning on pregnancy ECG enables accurate, objective stress assessment, with multi-layer feature extraction substantially outperforming single embedding approaches.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [275] [El Agente Estructural: An Artificially Intelligent Molecular Editor](https://arxiv.org/abs/2602.04849)
*Changhyeok Choi,Yunheng Zou,Marcel Müller,Han Hao,Yeonghun Kang,Juan B. Pérez-Sánchez,Ignacio Gustin,Hanyong Xu,Mohammad Ghazi Vakili,Chris Crebolder,Alán Aspuru-Guzik,Varinia Bernales*

Main category: physics.chem-ph

TL;DR: 提出Estructural代理，可多模态自然语言驱动化学分子几何结构生成与操作，结合工具与模型，能在多场景精确操作分子结构，还可集成到Quntur提升其功能。


<details>
  <summary>Details</summary>
Motivation: 研发新的分子几何生成与操作方法，避免通过生成模型进行分子生成或编辑的局限，实现更精确、直接的三维分子系统操作。

Method: 集成领域知识工具和视觉语言模型，模仿人类专家三维直接操作分子系统。

Result: Estructural能在多种真实场景下实现有化学意义的几何结构操作。

Conclusion: 多模态推理结合几何感知工具可支持超越结构生成的交互式、上下文感知分子建模，集成到Quntur可增强其功能。

Abstract: We present El Agente Estructural, a multimodal, natural-language-driven geometry-generation and manipulation agent for autonomous chemistry and molecular modelling. Unlike molecular generation or editing via generative models, Estructural mimics how human experts directly manipulate molecular systems in three dimensions by integrating a comprehensive set of domain-informed tools and vision-language models. This design enables precise control over atomic or functional group replacements, atomic connectivity, and stereochemistry without the need to rebuild extensive core molecular frameworks. Through a series of representative case studies, we demonstrate that Estructural enables chemically meaningful geometry manipulation across a wide range of real-world scenarios. These include site-selective functionalization, ligand binding, ligand exchange, stereochemically controlled structure construction, isomer interconversion, fragment-level structural analysis, image-guided generation of structures from schematic reaction mechanisms, and mechanism-driven geometry generation and modification. These examples illustrate how multimodal reasoning, when combined with specialized geometry-aware tools, supports interactive and context-aware molecular modelling beyond structure generation. Looking forward, the integration of Estructural into El Agente Quntur, an autonomous multi-agent quantum chemistry platform, enhances its capabilities by adding sophisticated tools for the generation and editing of three-dimensional structures.

</details>


### [276] [El Agente Quntur: A research collaborator agent for quantum chemistry](https://arxiv.org/abs/2602.04850)
*Juan B. Pérez-Sánchez,Yunheng Zou,Jorge A. Campos-Gonzalez-Angulo,Marcel Müller,Ignacio Gustin,Andrew Wang,Han Hao,Tsz Wai Ko,Changhyeok Choi,Eric S. Isbrandt,Mohammad Ghazi Vakili,Hanyong Xu,Chris Crebolder,Varinia Bernales,Alán Aspuru-Guzik*

Main category: physics.chem-ph

TL;DR: 介绍量子化学在多领域重要但应用受限，引入多智能体系统Quntur助力计算量子化学研究，并给出适用范围与发展路线图。


<details>
  <summary>Details</summary>
Motivation: 量子化学模拟因方法复杂等原因，实际应用仅掌握在少数专家手中，需扩大其使用范围。

Method: 遵循三个策略设计Quntur：消除硬编码程序策略、构建通用可组合动作、实施引导式深度研究。

Result: Quntur支持ORCA 6.0所有计算，能依据文献规划、执行和分析实验。

Conclusion: 探讨计算化学研究层面智能体系统现状与瓶颈，给出全自主端到端计算化学研究智能体的发展路线。

Abstract: Quantum chemistry is a foundational enabling tool for the fields of chemistry, materials science, computational biology and others. Despite of its power, the practical application of quantum chemistry simulations remains in the hands of qualified experts due to methodological complexity, software heterogeneity, and the need for informed interpretation of results. To bridge the accessibility gap for these tools and expand their reach to chemists with broader backgrounds, we introduce El Agente Quntur, a hierarchical, multi-agent AI system designed to operate not merely as an automation tool but as a research collaborator for computational quantum chemistry. Quntur was designed following three main strategies: i) elimination of hard-coded procedural policies in favour of reasoning-driven decisions, ii) construction of general and composable actions that facilitate generalization and efficiency, and iii) implementation of guided deep research to integrate abstract quantum-chemical reasoning across subdisciplines and a detailed understanding of the software's internal logic and syntax. Although instantiated in ORCA, these design principles are applicable to research agents more generally and easily expandable to additional quantum chemistry packages and beyond. Quntur supports the full range of calculations available in ORCA 6.0 and reasons over software documentation and scientific literature to plan, execute, adapt, and analyze in silico chemistry experiments following best practices. We discuss the advances and current bottlenecks in agentic systems operating at the research level in computational chemistry, and outline a roadmap toward a fully autonomous end-to-end computational chemistry research agent.

</details>


### [277] [Beyond Learning on Molecules by Weakly Supervising on Molecules](https://arxiv.org/abs/2602.04696)
*Gordan Prastalo,Kevin Maik Jablonka*

Main category: physics.chem-ph

TL;DR: 提出自适应化学嵌入模型ACE - Mol，利用弱监督学习分子基序，在分子属性预测基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 大多数预训练分子编码器不依赖任务，现有任务条件化方法依赖昂贵的标注数据，需要改进。

Method: 使用弱监督学习从数百个与自然语言描述符配对的分子基序中学习，构建ACE - Mol模型。

Result: ACE - Mol能立即将表示与任务对齐，在分子属性预测基准测试中达到了最先进的性能。

Conclusion: 基于弱监督学习分子基序的ACE - Mol模型有效，能产生可解释、具有化学意义的表示。

Abstract: Molecular representations are inherently task-dependent, yet most pre-trained molecular encoders are not. Task conditioning promises representations that reorganize based on task descriptions, but existing approaches rely on expensive labeled data. We show that weak supervision on programmatically derived molecular motifs is sufficient. Our Adaptive Chemical Embedding Model (ACE-Mol) learns from hundreds of motifs paired with natural language descriptors that are cheap to compute, trivial to scale. Conventional encoders slowly search the embedding space for task-relevant structure, whereas ACE-Mol immediately aligns its representations with the task. ACE-Mol achieves state-of-the-art performance across molecular property prediction benchmarks with interpretable, chemically meaningful representations.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [278] [Universality of General Spiked Tensor Models](https://arxiv.org/abs/2602.04472)
*Yanjin Xiang,Zhihua Zhang*

Main category: math.ST

TL;DR: 研究高维下秩一尖峰张量模型，证明其在非高斯噪声下谱行为和统计极限有普适性，方法依赖随机矩阵理论等。


<details>
  <summary>Details</summary>
Motivation: 将经典高斯框架扩展到更广泛的噪声分布类，研究非高斯噪声下尖峰张量模型的性质。

Method: 使用随机矩阵理论的预解式方法、有限矩假设下的累积量展开以及基于Efron - Stein型论证的方差界。

Result: 在温和假设下，适当定义的分块张量收缩的经验谱分布几乎必然收敛到与高斯情况一致的确定性极限，渐近奇异值和估计与真实尖峰方向的对齐有明确表征。

Conclusion: 尖峰张量模型具有普适性原则，其高维谱行为和统计极限对非高斯噪声具有鲁棒性。

Abstract: We study the rank-one spiked tensor model in the high-dimensional regime, where the noise entries are independent and identically distributed with zero mean, unit variance, and finite fourth moment.This setting extends the classical Gaussian framework to a substantially broader class of noise distributions.Focusing on asymmetric tensors of order $d$ ($\ge 3$), we analyze the maximum likelihood estimator of the best rank-one approximation.Under a mild assumption isolating informative critical points of the associated optimization landscape, we show that the empirical spectral distribution of a suitably defined block-wise tensor contraction converges almost surely to a deterministic limit that coincides with the Gaussian case.As a consequence, the asymptotic singular value and the alignments between the estimated and true spike directions admit explicit characterizations identical to those obtained under Gaussian noise. These results establish a universality principle for spiked tensor models, demonstrating that their high-dimensional spectral behavior and statistical limits are robust to non-Gaussian noise.
  Our analysis relies on resolvent methods from random matrix theory, cumulant expansions valid under finite moment assumptions, and variance bounds based on Efron-Stein-type arguments. A key challenge in the proof is how to handle the statistical dependence between the signal term and the noise term.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [279] [A computational account of dreaming: learning and memory consolidation](https://arxiv.org/abs/2602.04095)
*Qi Zhang*

Main category: q-bio.NC

TL;DR: 本文针对梦的成因和功能争议，提出认知和计算模型，模拟表明随机信号可实现学习和记忆巩固，认为梦是大脑清醒活动延续。


<details>
  <summary>Details</summary>
Motivation: 解决梦的理论中因随机性导致的功能派和无功能派的分歧，探讨梦在学习和智力功能中的作用。

Method: 提出梦过程的认知和计算模型，并进行模拟，以实现学习和记忆巩固功能。

Result: 模拟显示随机信号可带来学习和记忆巩固。

Conclusion: 梦是大脑清醒活动的延续，处理海马体自发随机激活的信号，模型特征与实证研究相符。

Abstract: A number of studies have concluded that dreaming is mostly caused by randomly arriving internal signals because "dream contents are random impulses", and argued that dream sleep is unlikely to play an important part in our intellectual capacity. On the contrary, numerous functional studies have revealed that dream sleep does play an important role in our learning and other intellectual functions. Specifically, recent studies have suggested the importance of dream sleep in memory consolidation, following the findings of neural replaying of recent waking patterns in the hippocampus. The randomness has been the hurdle that divides dream theories into either functional or functionless. This study presents a cognitive and computational model of dream process. This model is simulated to perform the functions of learning and memory consolidation, which are two most popular dream functions that have been proposed. The simulations demonstrate that random signals may result in learning and memory consolidation. Thus, dreaming is proposed as a continuation of brain's waking activities that processes signals activated spontaneously and randomly from the hippocampus. The characteristics of the model are discussed and found in agreement with many characteristics concluded from various empirical studies.

</details>


### [280] [Discovering Mechanistic Models of Neural Activity: System Identification in an in Silico Zebrafish](https://arxiv.org/abs/2602.04492)
*Jan-Matthis Lueckmann,Viren Jain,Michał Januszewski*

Main category: q-bio.NC

TL;DR: 本文建立虚拟测试平台，用LLM树搜索发现预测模型，指出条件和结构先验对模型的影响并为科学发现提供模板。


<details>
  <summary>Details</summary>
Motivation: 构建神经回路机制模型时缺乏真实数据来验证模型，需要严格测试模型发现。

Method: 使用斑马鱼幼体神经机械模拟建立虚拟测试平台，采用基于大语言模型（LLM）的树搜索方法。

Result: LLM树搜索自主发现的预测模型显著优于现有预测基线；仅基于感官驱动不足以实现系统的真实识别；结构先验对模型的分布外泛化和可解释机制模型的恢复至关重要。

Conclusion: 研究为建模真实神经记录提供指导，并为人工智能驱动的科学发现提供通用模板。

Abstract: Constructing mechanistic models of neural circuits is a fundamental goal of neuroscience, yet verifying such models is limited by the lack of ground truth. To rigorously test model discovery, we establish an in silico testbed using neuromechanical simulations of a larval zebrafish as a transparent ground truth. We find that LLM-based tree search autonomously discovers predictive models that significantly outperform established forecasting baselines. Conditioning on sensory drive is necessary but not sufficient for faithful system identification, as models exploit statistical shortcuts. Structural priors prove essential for enabling robust out-of-distribution generalization and recovery of interpretable mechanistic models. Our insights provide guidance for modeling real-world neural recordings and offer a broader template for AI-driven scientific discovery.

</details>


### [281] [BrainVista: Modeling Naturalistic Brain Dynamics as Multimodal Next-Token Prediction](https://arxiv.org/abs/2602.04512)
*Xuanhua Yin,Runkai Zhao,Lina Yao,Weidong Cai*

Main category: q-bio.NC

TL;DR: 提出多模态自回归框架BrainVista用于建模脑状态因果演变，在多个数据集验证取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 解决多模态输入和皮层网络复杂拓扑的时间尺度不匹配问题，以实现现实神经模拟中脑状态因果正向演变建模。

Method: 引入BrainVista框架，包含Network - wise Tokenizers和Spatial Mixer Head，提出S2B掩码机制。

Result: 在Algonauts 2025、CineBrain和HAD上实现SOTA的fMRI编码性能，长时预测中比基线有显著提升。

Conclusion: BrainVista框架有效解决相关挑战，能提升fMRI编码和长时预测性能。

Abstract: Naturalistic fMRI characterizes the brain as a dynamic predictive engine driven by continuous sensory streams. However, modeling the causal forward evolution in realistic neural simulation is impeded by the timescale mismatch between multimodal inputs and the complex topology of cortical networks. To address these challenges, we introduce BrainVista, a multimodal autoregressive framework designed to model the causal evolution of brain states. BrainVista incorporates Network-wise Tokenizers to disentangle system-specific dynamics and a Spatial Mixer Head that captures inter-network information flow without compromising functional boundaries. Furthermore, we propose a novel Stimulus-to-Brain (S2B) masking mechanism to synchronize high-frequency sensory stimuli with hemodynamically filtered signals, enabling strict, history-only causal conditioning. We validate our framework on Algonauts 2025, CineBrain, and HAD, achieving state-of-the-art fMRI encoding performance. In long-horizon rollout settings, our model yields substantial improvements over baselines, increasing pattern correlation by 36.0\% and 33.3\% on relative to the strongest baseline Algonauts 2025 and CineBrain, respectively.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [282] [Online unsupervised Hebbian learning in deep photonic neuromorphic networks](https://arxiv.org/abs/2601.22300)
*Xi Li,Disha Biswas,Peng Zhou,Wesley H. Brigner,Anna Capuano,Joseph S. Friedman,Qing Gu*

Main category: physics.optics

TL;DR: 本文提出纯光子深度神经形态网络架构实现无监督学习，在字母识别任务达100%准确率，解锁光子计算在复杂AI应用潜力。


<details>
  <summary>Details</summary>
Motivation: 传统冯诺依曼架构对神经网络计算速度和能效有局限，现有光子神经形态网络多依赖监督学习和OEO转换，需新架构。

Method: 引入纯光子深度PNN架构，提出全光域本地反馈机制，用非易失相变材料突触实现Hebbian学习规则。

Result: 在非平凡字母识别任务中达100%识别率，实现高效实时的全光信息处理。

Conclusion: 该工作使光子计算能直接、高通量处理光信息，解锁其在复杂AI应用中的潜力。

Abstract: While software implementations of neural networks have driven significant advances in computation, the von Neumann architecture imposes fundamental limitations on speed and energy efficiency. Neuromorphic networks, with structures inspired by the brain's architecture, offer a compelling solution with the potential to approach the extreme energy efficiency of neurobiological systems. Photonic neuromorphic networks (PNNs) are particularly attractive because they leverage the inherent advantages of light, namely high parallelism, low latency, and exceptional energy efficiency. Previous PNN demonstrations have largely focused on device-level functionalities or system-level implementations reliant on supervised learning and inefficient optical-electrical-optical (OEO) conversions. Here, we introduce a purely photonic deep PNN architecture that enables online, unsupervised learning. We propose a local feedback mechanism operating entirely in the optical domain that implements a Hebbian learning rule using non-volatile phase-change material synapses. We experimentally demonstrate this approach on a non-trivial letter recognition task using a commercially available fiber-optic platform and achieve a 100 percent recognition rate, showcasing an all-optical solution for efficient, real-time information processing. This work unlocks the potential of photonic computing for complex artificial intelligence applications by enabling direct, high-throughput processing of optical information without intermediate OEO signal conversions.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [283] [The impact of heterogeneity on the co-evolution of cooperation and epidemic spreading in complex networks](https://arxiv.org/abs/2602.04481)
*Mehran Noori,Nahid Azimi-Tafreshi,Mohammad Salahshour*

Main category: physics.soc-ph

TL;DR: 结合公共品博弈和传染病模型分析异质性在群体免疫动力学中的作用，指出其是双刃剑，给出干预政策建议。


<details>
  <summary>Details</summary>
Motivation: 明确群体免疫动力学中集体社会行为与疾病传播相互作用里异质性的作用。

Method: 在复杂网络（包括多重和真实网络）上耦合公共品博弈与传染病模型来剖析共同演化反馈。

Result: 社交网络结构异质性促进合作和疾病抑制，个体感染成本异质性破坏合作、加剧疫情。

Conclusion: 异质性是双刃剑，影响取决于形成影响力还是动机不对称，建议实施促进枢纽节点合作转变和统一薄弱环节激励的疾病干预政策。

Abstract: The dynamics of herd immunity depend crucially on the interaction between collective social behavior and disease transmission, but the role of heterogeneity in this context frequently remains unclear. Here, we dissect this co-evolutionary feedback by coupling a public goods game with an epidemic model on complex networks, including multiplex and real-world networks. Our results reveals a dichotomy in how heterogeneity shapes outcomes. We demonstrate that structural heterogeneity in social networks acts as a powerful catalyst for cooperation and disease suppression. This emergent effect is driven by highly connected hubs who, facing amplified personal risk, adopt protective strategies out of self-interest. In contrast, heterogeneity in individual infection costs proves detrimental, undermining cooperation and amplifying the epidemic. This creates a ``weakest link'' problem, where individuals with low perceived risk act as persistent free-riders and disease reservoirs, degrading the collective response. Our findings establish that heterogeneity is a double-edged sword: its impact is determined by whether it creates an asymmetry of influence (leverage points) or an asymmetry of motivation (weakest links), recommending disease intervention policies that facilitate cooperative transition in hubs (strengthening the leverage point) and homogenize incentives to weakest links.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [284] [The Dynamics of Attention across Automated and Manual Driving Modes: A Driving Simulation Study](https://arxiv.org/abs/2602.04164)
*Yuan Cai,Mustafa Demir,Farzan Sasangohar,Mohsen Zare*

Main category: cs.ET

TL;DR: 研究探索自动驾驶车辆不同驾驶模式下驾驶员对不同区域的注意力分配，发现注意力分配与驾驶模式相关，研究成果可用于设计自适应人机界面。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆融入交通系统带来安全隐患，特别是模式转换时驾驶员重新参与的问题，过去事故凸显了解动态注意力分配以保障自动驾驶安全的必要性。

Method: 进行高保真驾驶模拟，使用眼动追踪技术测量不同驾驶模式下的注视时长、注视次数和首次注视时间，以评估驾驶员对不同感兴趣区域的注意力分配。

Result: 驾驶员的注意力在不同驾驶模式下有显著差异，手动模式关注道路，自动模式长时间注视嵌入式人机界面，交接阶段注意力在环境和技术元素间动态转移。

Conclusion: 驾驶员注意力分配与驾驶模式相关，研究结果可用于设计自适应人机界面，根据驾驶场景提供相关信息，增强人车交互，支持有效过渡，提高整体安全性。

Abstract: This study aims to explore the dynamics of driver attention to various zones, including the road, the central mirror, the embedded Human-Machine Interface (HMI), and the speedometer, across different driving modes in AVs. The integration of autonomous vehicles (AVs) into transportation systems has introduced critical safety concerns, particularly regarding driver re-engagement during mode transitions. Past accidents underscore the risks of overreliance on automation and highlight the need to understand dynamic attention allocation to support safety in autonomous driving. A high-fidelity driving simulation was conducted. Eye-tracking technology was used to measure fixation duration, fixation count, and time to first fixation across distinct driving modes (automated, manual, and transition), which were then used to assess how drivers allocated attention to various areas of interest (AOIs). Findings show that drivers' attention varies significantly across driving modes. In manual mode, attention consistently focuses on the road, while in automated mode, prolonged fixation on the embedded HMI was observed. During the handover and takeover phases, attention shifts dynamically between environmental and technological elements. The study reveals that driver attention allocation is mode-dependent. These findings inform the design of adaptive HMIs in AVs that align with drivers' attention patterns. By presenting relevant information according to the driving context, such systems can enhance driver-vehicle interaction, support effective transitions, and improve overall safety. Systematic analysis of visual attention dynamics across driving modes is gaining prominence, as it informs adaptive HMI designs and driver readiness interventions. The GLMM findings can be directly applied to the design of adaptive HMIs or driver training programs to enhance attention and improve safety.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [285] [Benchmarking Automatic Speech Recognition for Indian Languages in Agricultural Contexts](https://arxiv.org/abs/2602.03868)
*Chandrashekar M S,Vineet Singh,Lakshmi Pedapudi*

Main category: eess.AS

TL;DR: 本文提出农业领域自动语音识别（ASR）系统的基准评估框架，评估多语言表现，揭示差异，提出改进建议并建立基线基准。


<details>
  <summary>Details</summary>
Motivation: 印度农业咨询服务数字化需要能准确转录多种印度语言特定领域术语的强大ASR系统，需评估其在农业语境中的表现。

Method: 提出评估框架，引入农业加权词错误率（AWWER）和特定领域效用评分等评估指标，评估10,934个音频记录，由多达10个ASR模型转录。

Result: 不同语言和模型性能有差异，印地语整体表现最佳，奥里亚语挑战最大；说话人分割结合最佳说话人选择可大幅降低多说话人录音的词错误率。

Conclusion: 识别出农业术语中的常见错误模式，为低资源农业领域改进ASR系统提供实用建议，建立了未来农业ASR发展的基线基准。

Abstract: The digitization of agricultural advisory services in India requires robust Automatic Speech Recognition (ASR) systems capable of accurately transcribing domain-specific terminology in multiple Indian languages. This paper presents a benchmarking framework for evaluating ASR performance in agricultural contexts across Hindi, Telugu, and Odia languages. We introduce evaluation metrics including Agriculture Weighted Word Error Rate (AWWER) and domain-specific utility scoring to complement traditional metrics. Our evaluation of 10,934 audio recordings, each transcribed by up to 10 ASR models, reveals performance variations across languages and models, with Hindi achieving the best overall performance (WER: 16.2%) while Odia presents the greatest challenges (best WER: 35.1%, achieved only with speaker diarization). We characterize audio quality challenges inherent to real-world agricultural field recordings and demonstrate that speaker diarization with best-speaker selection can substantially reduce WER for multi-speaker recordings (upto 66% depending on the proportion of multi-speaker audio). We identify recurring error patterns in agricultural terminology and provide practical recommendations for improving ASR systems in low-resource agricultural domains. The study establishes baseline benchmarks for future agricultural ASR development.

</details>


### [286] [Sounding Highlights: Dual-Pathway Audio Encoders for Audio-Visual Video Highlight Detection](https://arxiv.org/abs/2602.03891)
*Seohyun Joo,Yoori Oh*

Main category: eess.AS

TL;DR: 提出DAViHD框架解决现有视频高光检测模型未充分利用音频模态问题，在Mr.HiSum基准测试取得新的最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频高光检测模型未充分利用音频模态，仅关注高层语义特征，未利用声音丰富动态特征。

Method: 提出DAViHD框架，其双路径音频编码器包含语义路径和动态路径，语义路径提取高层信息，动态路径用频率自适应机制建模动态。

Result: 将新音频编码器集成到完整视听框架，在Mr.HiSum基准测试取得新的最优性能。

Conclusion: 复杂的双方面音频表示是推动高光检测领域发展的关键。

Abstract: Audio-visual video highlight detection aims to automatically identify the most salient moments in videos by leveraging both visual and auditory cues. However, existing models often underutilize the audio modality, focusing on high-level semantic features while failing to fully leverage the rich, dynamic characteristics of sound. To address this limitation, we propose a novel framework, Dual-Pathway Audio Encoders for Video Highlight Detection (DAViHD). The dual-pathway audio encoder is composed of a semantic pathway for content understanding and a dynamic pathway that captures spectro-temporal dynamics. The semantic pathway extracts high-level information by identifying the content within the audio, such as speech, music, or specific sound events. The dynamic pathway employs a frequency-adaptive mechanism as time evolves to jointly model these dynamics, enabling it to identify transient acoustic events via salient spectral bands and rapid energy changes. We integrate the novel audio encoder into a full audio-visual framework and achieve new state-of-the-art performance on the large-scale Mr.HiSum benchmark. Our results demonstrate that a sophisticated, dual-faceted audio representation is key to advancing the field of highlight detection.

</details>


### [287] [Universal Robust Speech Adaptation for Cross-Domain Speech Recognition and Enhancement](https://arxiv.org/abs/2602.04307)
*Chien-Chun Wang,Hung-Shin Lee,Hsin-Min Wang,Berlin Chen*

Main category: eess.AS

TL;DR: 提出URSA - GAN框架解决语音识别和增强模型在域转移下的性能下降问题，采用双嵌入架构和动态随机扰动技术，实验显示其在多场景下有效提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有自动语音识别和语音增强预训练模型在域转移（如遇到未见噪声和信道失真）时性能严重下降。

Method: 提出URSA - GAN框架，采用由噪声编码器和信道编码器组成的双嵌入架构，用有限域内数据预训练以捕捉域相关表示；提出动态随机扰动正则化技术，在生成时为嵌入引入可控可变性。

Result: URSA - GAN有效降低了自动语音识别中的字符错误率，提高了语音增强中的感知指标；在信道和噪声复合测试条件下，自动语音识别性能提升16.16%，语音增强指标提升15.58%。

Conclusion: URSA - GAN能有效解决噪声和信道条件不匹配问题，具有良好的泛化能力，可提升不同噪声和失配信道场景下自动语音识别和语音增强的性能。

Abstract: Pre-trained models for automatic speech recognition (ASR) and speech enhancement (SE) have exhibited remarkable capabilities under matched noise and channel conditions. However, these models often suffer from severe performance degradation when confronted with domain shifts, particularly in the presence of unseen noise and channel distortions. In view of this, we in this paper present URSA-GAN, a unified and domain-aware generative framework specifically designed to mitigate mismatches in both noise and channel conditions. URSA-GAN leverages a dual-embedding architecture that consists of a noise encoder and a channel encoder, each pre-trained with limited in-domain data to capture domain-relevant representations. These embeddings condition a GAN-based speech generator, facilitating the synthesis of speech that is acoustically aligned with the target domain while preserving phonetic content. To enhance generalization further, we propose dynamic stochastic perturbation, a novel regularization technique that introduces controlled variability into the embeddings during generation, promoting robustness to unseen domains. Empirical results demonstrate that URSA-GAN effectively reduces character error rates in ASR and improves perceptual metrics in SE across diverse noisy and mismatched channel scenarios. Notably, evaluations on compound test conditions with both channel and noise degradations confirm the generalization ability of URSA-GAN, yielding relative improvements of 16.16% in ASR performance and 15.58% in SE metrics.

</details>


<div id='econ.TH'></div>

# econ.TH [[Back]](#toc)

### [288] [Dynamic Matching Under Patience Imbalance](https://arxiv.org/abs/2602.03995)
*Zhiyuan Chen,Rui,Chen,Ming Hu,Yun Zhou*

Main category: econ.TH

TL;DR: 研究双边平台上耐心不平衡的动态匹配问题，对比集中式和分散式系统的匹配策略、福利情况，并比较不同耐心水平系统的社会福利。


<details>
  <summary>Details</summary>
Motivation: 分析双边平台上耐心不平衡场景下的动态匹配问题，找到优化匹配策略和提升社会福利的方法。

Method: 先研究集中式基准下的最优策略，再分析分散式系统中的马尔可夫完美均衡，最后比较不同耐心水平系统的社会福利。

Result: 集中式最优策略是基于阈值规则，分散式均衡有独特匹配模式，且可通过调整收益分配实现与集中式最优对齐；不同耐心水平系统的社会福利排序在集中式和分散式中有不同结果。

Conclusion: 双边平台动态匹配中耐心不平衡对匹配策略和社会福利有重要影响，分散式系统可通过调整收益分配优化，且不同耐心水平系统的社会福利受匹配收益分配规则和等待成本影响。

Abstract: We study a dynamic matching problem on a two-sided platform with unbalanced patience, in which long-lived supply accumulates over time with a unit waiting cost per period, while short-lived demand departs if not matched promptly. High- or low-quality agents arrive sequentially with one supply agent and one demand agent arriving in each period, and matching payoffs are supermodular. In the centralized benchmark, the optimal policy follows a threshold-based rule that rations high-quality supply, preserving it for future high-quality demand. In the decentralized system, where self-interested agents decide whether to match under an exogenously specified payoff allocation proportion, we characterize a welfare-maximizing Markov perfect equilibrium. Unlike outcomes in the centralized benchmark or in full-backlog markets, the equilibrium exhibits distinct matching patterns in which low-type demand may match with high-type supply even when low-type supply is available. Unlike settings in which both sides have long-lived agents and perfect coordination is impossible, the decentralized system can always be perfectly aligned with the centralized optimum by appropriately adjusting the allocation of matching payoffs across agents on both sides. Finally, when the arrival probabilities for H- and L-type arrivals are identical on both sides, we compare social welfare across systems with different patience levels: full backlog on both sides, one-sided backlog, and no backlog. In the centralized setting, social welfare is weakly ordered across systems. However, in the decentralized setting, the social welfare ranking across the three systems depends on the matching payoff allocation rule and the unit waiting cost, and enabling patience can either increase or decrease social welfare.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [289] [GenMRP: A Generative Multi-Route Planning Framework for Efficient and Personalized Real-Time Industrial Navigation](https://arxiv.org/abs/2602.04174)
*Chengzhang Wang,Chao Chen,Jun Tao,Tengfei Liu,He Bai,Song Wang,Longfei Xu,Kaikui Liu,Xiangxiang Chu*

Main category: cs.RO

TL;DR: 提出GenMRP框架解决现有路线规划方法的局限，在离线和在线环境高效实现多路线规划，并已部署到导航应用。


<details>
  <summary>Details</summary>
Motivation: 现有工业级导航应用的路线规划方法存在个性化、路线多样性和效率问题，需改进。

Method: GenMRP采用骨架到毛细血管的方法构建子网络，迭代生成路线，用修正提升法平衡质量和多样性，结合多种信息更新道路成本，用Dijkstra算法生成路线。

Result: GenMRP在离线和在线环境达到了先进水平，已发布数据集，且部署到真实导航应用。

Conclusion: GenMRP有效解决了现有路线规划方法的局限，具有高效性和实用性。

Abstract: Existing industrial-scale navigation applications contend with massive road networks, typically employing two main categories of approaches for route planning. The first relies on precomputed road costs for optimal routing and heuristic algorithms for generating alternatives, while the second, generative methods, has recently gained significant attention. However, the former struggles with personalization and route diversity, while the latter fails to meet the efficiency requirements of large-scale real-time scenarios. To address these limitations, we propose GenMRP, a generative framework for multi-route planning. To ensure generation efficiency, GenMRP first introduces a skeleton-to-capillary approach that dynamically constructs a relevant sub-network significantly smaller than the full road network. Within this sub-network, routes are generated iteratively. The first iteration identifies the optimal route, while the subsequent ones generate alternatives that balance quality and diversity using the newly proposed correctional boosting approach. Each iteration incorporates road features, user historical sequences, and previously generated routes into a Link Cost Model to update road costs, followed by route generation using the Dijkstra algorithm. Extensive experiments show that GenMRP achieves state-of-the-art performance with high efficiency in both offline and online environments. To facilitate further research, we have publicly released the training and evaluation dataset. GenMRP has been fully deployed in a real-world navigation app, demonstrating its effectiveness and benefits.

</details>


### [290] [KGLAMP: Knowledge Graph-guided Language model for Adaptive Multi-robot Planning and Replanning](https://arxiv.org/abs/2602.04129)
*Chak Lam Shek,Faizan M. Tariq,Sangjae Bae,David Isele,Piyush Gupta*

Main category: cs.RO

TL;DR: 提出KGLAMP框架解决异构多机器人系统规划问题，实验显示性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有规划方法在异构多机器人系统动态环境中难以构建准确符号表示和维持计划一致性，经典PDDL规划器需手动建模，LLM规划器常忽略代理异构性和环境不确定性。

Method: 引入知识图谱引导的LLM规划框架KGLAMP，利用知识图谱编码对象关系、空间可达性和机器人能力，引导LLM生成准确PDDL问题规范，知识图谱可动态更新并触发重新规划。

Result: 在MAT - THOR基准测试中，KGLAMP比仅基于LLM和基于PDDL的变体性能至少提高25.5%。

Conclusion: KGLAMP框架能有效解决异构多机器人系统在动态环境下的规划问题，提升规划性能。

Abstract: Heterogeneous multi-robot systems are increasingly deployed in long-horizon missions that require coordination among robots with diverse capabilities. However, existing planning approaches struggle to construct accurate symbolic representations and maintain plan consistency in dynamic environments. Classical PDDL planners require manually crafted symbolic models, while LLM-based planners often ignore agent heterogeneity and environmental uncertainty. We introduce KGLAMP, a knowledge-graph-guided LLM planning framework for heterogeneous multi-robot teams. The framework maintains a structured knowledge graph encoding object relations, spatial reachability, and robot capabilities, which guides the LLM in generating accurate PDDL problem specifications. The knowledge graph serves as a persistent, dynamically updated memory that incorporates new observations and triggers replanning upon detecting inconsistencies, enabling symbolic plans to adapt to evolving world states. Experiments on the MAT-THOR benchmark show that KGLAMP improves performance by at least 25.5% over both LLM-only and PDDL-based variants.

</details>


### [291] [MA3DSG: Multi-Agent 3D Scene Graph Generation for Large-Scale Indoor Environments](https://arxiv.org/abs/2602.04152)
*Yirum Kim,Jaewoo Kim,Ue-Hwan Kim*

Main category: cs.RO

TL;DR: 现有3D场景图生成方法扩展性有限，本文提出MA3DSG模型、无训练图对齐算法和MA3DSG - Bench基准，为多智能体3D场景图生成研究奠定基础。


<details>
  <summary>Details</summary>
Motivation: 当前3D场景图生成方法依赖单智能体假设和小规模环境，在现实场景中扩展性有限。

Method: 引入MA3DSG模型，开发无训练图对齐算法，使传统单智能体系统可协作；提出MA3DSG - Bench基准进行评估。

Result: 实现了传统单智能体系统的协作，且无需可学习参数；提供了更通用和可扩展的评估框架。

Conclusion: 为可扩展的多智能体3D场景图生成研究奠定了坚实基础。

Abstract: Current 3D scene graph generation (3DSGG) approaches heavily rely on a single-agent assumption and small-scale environments, exhibiting limited scalability to real-world scenarios. In this work, we introduce Multi-Agent 3D Scene Graph Generation (MA3DSG) model, the first framework designed to tackle this scalability challenge using multiple agents. We develop a training-free graph alignment algorithm that efficiently merges partial query graphs from individual agents into a unified global scene graph. Leveraging extensive analysis and empirical insights, our approach enables conventional single-agent systems to operate collaboratively without requiring any learnable parameters. To rigorously evaluate 3DSGG performance, we propose MA3DSG-Bench-a benchmark that supports diverse agent configurations, domain sizes, and environmental conditions-providing a more general and extensible evaluation framework. This work lays a solid foundation for scalable, multi-agent 3DSGG research.

</details>


### [292] [SCALE: Self-uncertainty Conditioned Adaptive Looking and Execution for Vision-Language-Action Models](https://arxiv.org/abs/2602.04208)
*Hyeonbeom Choi,Daechul Ahn,Youhan Lee,Taewook Kang,Seongwon Cho,Jonghyun Choi*

Main category: cs.RO

TL;DR: 提出名为SCALE的简单推理策略来解决现有VLA模型TTS方法的局限，实验证明其能提升VLA性能并优于现有TTS方法。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型的TTS方法需额外训练、验证器和多次前向传播，且只在动作解码时干预，在感知模糊下不足，因此要解决这些局限。

Method: 提出SCALE，一种基于“自我不确定性”联合调节视觉感知和动作的简单推理策略，无需额外训练、验证器，只需单次前向传播。

Result: 在模拟和真实世界基准测试中，SCALE改进了现有VLA模型，优于现有TTS方法，同时保持单次前向传播效率。

Conclusion: SCALE是一种有效的推理策略，能提升VLA模型在不同条件下的性能。

Abstract: Vision-Language-Action (VLA) models have emerged as a promising paradigm for general-purpose robotic control, with test-time scaling (TTS) gaining attention to enhance robustness beyond training. However, existing TTS methods for VLAs require additional training, verifiers, and multiple forward passes, making them impractical for deployment. Moreover, they intervene only at action decoding while keeping visual representations fixed-insufficient under perceptual ambiguity, where reconsidering how to perceive is as important as deciding what to do. To address these limitations, we propose SCALE, a simple inference strategy that jointly modulates visual perception and action based on 'self-uncertainty', inspired by uncertainty-driven exploration in Active Inference theory-requiring no additional training, no verifier, and only a single forward pass. SCALE broadens exploration in both perception and action under high uncertainty, while focusing on exploitation when confident-enabling adaptive execution across varying conditions. Experiments on simulated and real-world benchmarks demonstrate that SCALE improves state-of-the-art VLAs and outperforms existing TTS methods while maintaining single-pass efficiency.

</details>


### [293] [OAT: Ordered Action Tokenization](https://arxiv.org/abs/2602.04215)
*Chaoqi Liu,Xiaoshen Han,Jiawei Gao,Yue Zhao,Haonan Chen,Yilun Du*

Main category: cs.RO

TL;DR: 提出Ordered Action Tokenization (OAT)方法，用于机器人动作标记化，在多个任务中表现优于现有方案。


<details>
  <summary>Details</summary>
Motivation: 现有将自回归建模应用于连续机器人动作的动作标记化方案有局限性，要么生成过长标记序列，要么缺乏结构，不适合下一个标记预测。

Method: 提出OAT方法，利用带寄存器的变压器、有限标量量化和诱导排序的训练机制，将动作块离散为有序的标记序列。

Result: 在四个模拟基准和现实世界设置的20多个任务中，配备OAT的自回归策略一致优于先前的标记化方案和基于扩散的基线，推理时具有更高灵活性。

Conclusion: OAT是一种有效的动作标记化方案，满足高压缩、完全可解码和标记空间左到右因果有序三个要求，适用于机器人自回归策略。

Abstract: Autoregressive policies offer a compelling foundation for scalable robot learning by enabling discrete abstraction, token-level reasoning, and flexible inference. However, applying autoregressive modeling to continuous robot actions requires an effective action tokenization scheme. Existing approaches either rely on analytical discretization methods that produce prohibitively long token sequences, or learned latent tokenizers that lack structure, limiting their compatibility with next-token prediction. In this work, we identify three desiderata for action tokenization - high compression, total decodability, and a left-to-right causally ordered token space - and introduce Ordered Action Tokenization (OAT), a learned action tokenizer that satisfies all three. OAT discretizes action chunks into an ordered sequence of tokens using transformer with registers, finite scalar quantization, and ordering-inducing training mechanisms. The resulting token space aligns naturally with autoregressive generation and enables prefix-based detokenization, yielding an anytime trade-off between inference cost and action fidelity. Across more than 20 tasks spanning four simulation benchmarks and real-world settings, autoregressive policies equipped with OAT consistently outperform prior tokenization schemes and diffusion-based baselines, while offering significantly greater flexibility at inference time.

</details>


### [294] [AppleVLM: End-to-end Autonomous Driving with Advanced Perception and Planning-Enhanced Vision-Language Models](https://arxiv.org/abs/2602.04256)
*Yuxuan Han,Kunyuan Wu,Qianyi Shao,Renxiang Xiao,Zilu Wang,Cansen Jiang,Yi Xiao,Liang Hu,Yunjiang Lou*

Main category: cs.RO

TL;DR: 提出AppleVLM模型用于端到端自动驾驶，通过新的编码器和规划策略提升性能，实验表现优异并在AGV平台展示效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于VLM的端到端自动驾驶方法存在车道感知不佳、语言理解偏差和处理极端情况困难等问题。

Method: 引入新颖的视觉编码器和规划策略编码器，视觉编码器用可变形变压器机制融合时空信息，规划模态编码鸟瞰图空间信息，VLM解码器通过分层思维链微调。

Result: 在两个CARLA基准的闭环实验中达到了最先进的驾驶性能，在AGV平台上成功展示了复杂户外环境中的端到端自动驾驶。

Conclusion: AppleVLM模型能有效提升端到端自动驾驶的鲁棒性和性能。

Abstract: End-to-end autonomous driving has emerged as a promising paradigm integrating perception, decision-making, and control within a unified learning framework. Recently, Vision-Language Models (VLMs) have gained significant attention for their potential to enhance the robustness and generalization of end-to-end driving models in diverse and unseen scenarios. However, existing VLM-based approaches still face challenges, including suboptimal lane perception, language understanding biases, and difficulties in handling corner cases. To address these issues, we propose AppleVLM, an advanced perception and planning-enhanced VLM model for robust end-to-end driving. AppleVLM introduces a novel vision encoder and a planning strategy encoder to improve perception and decision-making. Firstly, the vision encoder fuses spatial-temporal information from multi-view images across multiple timesteps using a deformable transformer mechanism, enhancing robustness to camera variations and facilitating scalable deployment across different vehicle platforms. Secondly, unlike traditional VLM-based approaches, AppleVLM introduces a dedicated planning modality that encodes explicit Bird's-Eye-View spatial information, mitigating language biases in navigation instructions. Finally, a VLM decoder fine-tuned by a hierarchical Chain-of-Thought integrates vision, language, and planning features to output robust driving waypoints. We evaluate AppleVLM in closed-loop experiments on two CARLA benchmarks, achieving state-of-the-art driving performance. Furthermore, we deploy AppleVLM on an AGV platform and successfully showcase real-world end-to-end autonomous driving in complex outdoor environments.

</details>


### [295] [HoRD: Robust Humanoid Control via History-Conditioned Reinforcement Learning and Online Distillation](https://arxiv.org/abs/2602.04412)
*Puyue Wang,Jiawei Hu,Yan Gao,Junyan Wang,Yu Zhang,Gillian Dobbie,Tao Gu,Wafa Johal,Ting Dang,Hong Jia*

Main category: cs.RO

TL;DR: 提出HoRD框架用于人形机器人在领域转移下的鲁棒控制，结合历史条件适应和在线蒸馏使单策略零样本适应未见领域，实验表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 人形机器人在动力学、任务规格或环境设置小变化下性能显著下降，需要提高其鲁棒性。

Method: 提出两阶段学习框架HoRD，先通过历史条件强化学习训练高性能教师策略，再进行在线蒸馏将教师策略能力转移到基于变压器的学生策略。

Result: HoRD在鲁棒性和迁移性上优于强基线，尤其在未见领域和外部扰动下。

Conclusion: HoRD能使单策略零样本适应未见领域，无需按领域重新训练。

Abstract: Humanoid robots can suffer significant performance drops under small changes in dynamics, task specifications, or environment setup. We propose HoRD, a two-stage learning framework for robust humanoid control under domain shift. First, we train a high-performance teacher policy via history-conditioned reinforcement learning, where the policy infers latent dynamics context from recent state--action trajectories to adapt online to diverse randomized dynamics. Second, we perform online distillation to transfer the teacher's robust control capabilities into a transformer-based student policy that operates on sparse root-relative 3D joint keypoint trajectories. By combining history-conditioned adaptation with online distillation, HoRD enables a single policy to adapt zero-shot to unseen domains without per-domain retraining. Extensive experiments show HoRD outperforms strong baselines in robustness and transfer, especially under unseen domains and external perturbations. Code and project page are available at \href{https://tonywang-0517.github.io/hord/}{https://tonywang-0517.github.io/hord/}.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [296] [They Call Her 'Miss' and Him 'Professor': Lived Experiences of Women Teaching Support Staff in IT/SE Education](https://arxiv.org/abs/2602.04332)
*Vasudha Malhotra,Rhea D'silva,Rashina Hoda*

Main category: cs.CY

TL;DR: 本文综合15名女性教学支持人员的经历，揭示教学中权威的获取等情况，绘制“特权与权力轮”，并给出创建包容教育环境的建议。


<details>
  <summary>Details</summary>
Motivation: 女性教学支持人员贡献常未得到认可和重视，需要揭示她们在教学中的经历情况。

Method: 综合15名IT/SE高等教育领域女性教学支持人员的实际经历。

Result: 绘制适用于教学支持人员角色的“特权与权力轮”，显示不同身份特征所需的额外劳动。

Conclusion: 给出在技术主导领域创建更具包容性教育环境的切实建议，当下具有及时性。

Abstract: Despite their critical role in shaping student learning in computing education, the contributions of women teaching-support staff (TSS) often go unrecognised and undervalued. In this experience report, we synthesise lived experiences of 15 women TSS in IT/SE higher education to illuminate how authority is earned, resisted, and maintained in everyday teaching. Participants shared both their positive and negative lived experiences associated with finding and losing voice with teaching team colleagues on the one hand, and rewarding connections and gendered friction with students on the other. We map these dynamics onto an intersectional "wheel of privilege and power" tailored to TSS roles. The farther a TSS profile sits from the wheel's center (e.g., non-native English, non-white, younger-seeming, non-permanent, early-career), the more relational, emotional, and disciplinary labour is needed to reach parity. We provide actionable insights and recommendations for creating more inclusive education environments in technology dominant fields that are particularly timely as universities worldwide grapple with post-pandemic teaching models and seek to build more inclusive and resilient academic communities.

</details>


### [297] [Growth First, Care Second? Tracing the Landscape of LLM Value Preferences in Everyday Dilemmas](https://arxiv.org/abs/2602.04456)
*Zhiyi Chen,Eun Cheol Choi,Yingjia Luo,Xinyi Wang,Yulei Xiao,Aizi Yang,Luca Luceri*

Main category: cs.CY

TL;DR: 研究LLMs在不同建议寻求情境中如何处理价值权衡，发现价值权衡结构有差异，LLMs价值偏好有偏向，存在价值同质化风险。


<details>
  <summary>Details</summary>
Motivation: 刻画大语言模型在不同建议寻求情境中如何处理价值权衡。

Method: 用四个建议导向的Reddit子版块的数据集，自下而上构建分层价值框架，构建价值共现网络，评估LLMs的价值偏好。

Result: 不同建议寻求情境的价值权衡结构有很大异质性；各模型和情境中，LLMs始终优先考虑探索与成长相关价值而非仁爱与联系相关价值。

Conclusion: LLMs的价值取向系统性偏向可能导致AI介导建议中的价值同质化风险，引发对其影响决策和规范结果的担忧。

Abstract: People increasingly seek advice online from both human peers and large language model (LLM)-based chatbots. Such advice rarely involves identifying a single correct answer; instead, it typically requires navigating trade-offs among competing values. We aim to characterize how LLMs navigate value trade-offs across different advice-seeking contexts. First, we examine the value trade-off structure underlying advice seeking using a curated dataset from four advice-oriented subreddits. Using a bottom-up approach, we inductively construct a hierarchical value framework by aggregating fine-grained values extracted from individual advice options into higher-level value categories. We construct value co-occurrence networks to characterize how values co-occur within dilemmas and find substantial heterogeneity in value trade-off structures across advice-seeking contexts: a women-focused subreddit exhibits the highest network density, indicating more complex value conflicts; women's, men's, and friendship-related subreddits exhibit highly correlated value-conflict patterns centered on security-related tensions (security vs. respect/connection/commitment); by contrast, career advice forms a distinct structure where security frequently clashes with self-actualization and growth. We then evaluate LLM value preferences against these dilemmas and find that, across models and contexts, LLMs consistently prioritize values related to Exploration & Growth over Benevolence & Connection. This systemically skewed value orientation highlights a potential risk of value homogenization in AI-mediated advice, raising concerns about how such systems may shape decision-making and normative outcomes at scale.

</details>


### [298] [Learning the Value Systems of Agents with Preference-based and Inverse Reinforcement Learning](https://arxiv.org/abs/2602.04518)
*Andrés Holgado-Sánchez,Holger Billhardt,Alberto Fernández,Sascha Ossowski*

Main category: cs.CY

TL;DR: 提出一种从观察和人类示范中自动学习价值系统的新方法，并通过模拟用例评估。


<details>
  <summary>Details</summary>
Motivation: 随着AI发展，协议需符合道德原则，但不同用户价值体系不同，且难以计算定义价值，现有基于人工规范的方法规模受限。

Method: 提出价值系统学习问题的形式化模型，基于多目标马尔可夫决策过程应用于序列决策领域，使用偏好和逆强化学习算法推理价值接地函数和价值系统。

Result: 方法通过两个模拟用例进行了说明和评估。

Conclusion: 提出的从观察和人类示范中自动学习价值系统的方法具有一定可行性和应用价值。

Abstract: Agreement Technologies refer to open computer systems in which autonomous software agents interact with one another, typically on behalf of humans, in order to come to mutually acceptable agreements. With the advance of AI systems in recent years, it has become apparent that such agreements, in order to be acceptable to the involved parties, must remain aligned with ethical principles and moral values. However, this is notoriously difficult to ensure, especially as different human users (and their software agents) may hold different value systems, i.e. they may differently weigh the importance of individual moral values. Furthermore, it is often hard to specify the precise meaning of a value in a particular context in a computational manner. Methods to estimate value systems based on human-engineered specifications, e.g. based on value surveys, are limited in scale due to the need for intense human moderation. In this article, we propose a novel method to automatically \emph{learn} value systems from observations and human demonstrations. In particular, we propose a formal model of the \emph{value system learning} problem, its instantiation to sequential decision-making domains based on multi-objective Markov decision processes, as well as tailored preference-based and inverse reinforcement learning algorithms to infer value grounding functions and value systems. The approach is illustrated and evaluated by two simulated use cases.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [299] [HybridQuestion: Human-AI Collaboration for Identifying High-Impact Research Questions](https://arxiv.org/abs/2602.03849)
*Keyu Zhao,Fengli Xu,Yong Li,Tie-Yan Liu*

Main category: cs.HC

TL;DR: 探讨AI科学家能否识别有意义研究问题，提出人机混合方案并实验验证，表明人类判断对前瞻性问题评估至关重要。


<details>
  <summary>Details</summary>
Motivation: 当前不清楚AI科学家能否识别有意义研究问题，且大语言模型在长期战略评估方面潜力待探索。

Method: 分三阶段：AI加速信息收集生成混合信息库；候选问题提出通过LLMs生成初始候选池并过滤；混合问题选择通过多阶段过滤增加人类监督。

Result: 实验表明AI在识别既有突破上与人类专家高对齐，但在预测前瞻性问题上分歧大。

Conclusion: 人类判断对评估主观、前瞻性挑战仍是关键。

Abstract: The "AI Scientist" paradigm is transforming scientific research by automating key stages of the research process, from idea generation to scholarly writing. This shift is expected to accelerate discovery and expand the scope of scientific inquiry. However, a key question remains unclear: can AI scientists identify meaningful research questions? While Large Language Models (LLMs) have been applied successfully to task-specific ideation, their potential to conduct strategic, long-term assessments of past breakthroughs and future questions remains largely unexplored. To address this gap, we explore a human-AI hybrid solution that integrates the scalable data processing capabilities of AI with the value judgment of human experts. Our methodology is structured in three phases. The first phase, AI-Accelerated Information Gathering, leverages AI's advantage in processing vast amounts of literature to generate a hybrid information base. The second phase, Candidate Question Proposing, utilizes this synthesized data to prompt an ensemble of six diverse LLMs to propose an initial candidate pool, filtered via a cross-model voting mechanism. The third phase, Hybrid Question Selection, refines this pool through a multi-stage filtering process that progressively increases human oversight. To validate this system, we conducted an experiment aiming to identify the Top 10 Scientific Breakthroughs of 2025 and the Top 10 Scientific Questions for 2026 across five major disciplines. Our analysis reveals that while AI agents demonstrate high alignment with human experts in recognizing established breakthroughs, they exhibit greater divergence in forecasting prospective questions, suggesting that human judgment remains crucial for evaluating subjective, forward-looking challenges.

</details>


### [300] [WebAccessVL: Making an Accessible Web via Violation-Conditioned VLM](https://arxiv.org/abs/2602.03850)
*Amber Yijia Zheng,Jae Joong Lee,Bedrich Benes,Raymond A. Yeh*

Main category: cs.HC

TL;DR: 提出自动编辑网站HTML以解决WCAG2违规问题的视觉语言模型，实验效果佳且不改变网站外观内容。


<details>
  <summary>Details</summary>
Motivation: 解决网站HTML存在的WCAG2违规问题。

Method: 将其定义为有监督的图像条件程序合成任务，收集新数据集WebAccessVL，提出违规条件VLM指导修正过程。

Result: 有效将每个网站平均违规数从5.34降至0.44，优于商业LLM API。

Conclusion: 所提方法能有效解决WCAG2违规问题，且编辑后网站保持原有视觉外观和内容。

Abstract: We present a vision-language model (VLM) that automatically edits website HTML to address Web Content Accessibility Guidelines 2 (WCAG2) violations. We formulate this as a supervised image-conditioned program synthesis task, where the model learns to correct HTML given the HTML and its rendering. We collected WebAccessVL, a new dataset with manually corrected accessibility violations, establishing paired training data. We then propose a violation-conditioned VLM that additionally conditions on the WCAG2 violation count to guide the correction process. Experiments demonstrate that our method effectively reduces the average number of violations from 5.34 to 0.44 per website, outperforming commercial LLM APIs (Gemini, GPT-5). A perceptual study confirms that our edited websites maintain the original visual appearance and content.

</details>


### [301] [Perceptions of AI-CBT: Trust and Barriers in Chinese Postgrads](https://arxiv.org/abs/2602.03852)
*Chan-in Sio,Alex Mann,Lingxi Fan,Andrew Cheung,Lik-hang Lee*

Main category: cs.HC

TL;DR: 研究中国研究生对人工智能认知行为疗法聊天机器人的看法和体验，发现他们持谨慎开放态度。


<details>
  <summary>Details</summary>
Motivation: 研究生心理健康受关注，人工智能聊天机器人可提供帮助，但中国研究生对其看法和使用情况不明。

Method: 通过社交媒体招募十名中国研究生，进行半结构化Zoom访谈，用反射性主题分析法分析，以健康信念模型和计划行为理论为框架。

Result: 研究生对AI - CBT聊天机器人持谨慎开放态度，有用性和随时可用支持积极态度，数据隐私等限制使用意愿，社会规范和感知控制影响采纳。

Conclusion: 为中国学生群体的人工智能心理健康工具设计、沟通和部署提供特定背景信息，指出透明性等方面的设计启示。

Abstract: The mental well-being of graduate students is an increasing concern, yet the adoption of scalable support remains uneven. Artificial intelligence-powered cognitive behavioral therapy chatbots (AI-CBT) offer low barrier help, but little is known about how Chinese postgraduates perceive and use them. This qualitative study explored perceptions and experiences of AI-CBT chatbots among ten Chinese graduate students recruited through social media. Semi-structured Zoom interviews were conducted and analyzed using reflexive thematic analysis, with the Health Belief Model (HBM) and the Theory of Planned Behavior (TPB) as sensitizing frameworks. The findings indicate a cautious openness to AI-CBT chatbots: perceived usefulness and 24/7 access supported favorable attitudes, while data privacy, emotional safety, and uncertainty about `fit' for complex problems restricted the intention to use. Social norms (e.g., stigma and peer views) and perceived control (digital literacy, language quality) further shaped adoption. The study offers context-specific information to guide the culturally sensitive design, communication, and deployment of AI mental well-being tools for student populations in China and outlines the design implications around transparency, safeguards, and graduated care pathways.

</details>


### [302] [Tinker Tales: Supporting Child-AI Collaboration through Co-Creative Storytelling with Educational Scaffolding](https://arxiv.org/abs/2602.04109)
*Nayoung Choi,Jiseung Hong,Peace Cyebukayire,Ikseon Choi,Jinho D. Choi*

Main category: cs.HC

TL;DR: 研究儿童与AI在共创故事中的互动，介绍Tinker Tales系统及用户研究，发现儿童视AI为协作伙伴，支架支持叙事完善且不削弱儿童自主性。


<details>
  <summary>Details</summary>
Motivation: 现有研究多在AI主导教学场景，需研究儿童如何通过迭代共创与AI有意义互动。

Method: 设计Tinker Tales系统，结合实体故事板、NFC玩具和移动应用，通过实物和语音交互，对10名儿童进行探索性用户研究。

Result: 儿童将AI视为专注、有回应的协作伙伴，支架支持连贯叙事完善且不削弱儿童能动性。

Conclusion: Tinker Tales系统能有效支持儿童与AI进行共创故事的协作。

Abstract: Artificial intelligence (AI) is increasingly framed as a collaborative partner in creative activities, yet children's interactions with AI have largely been studied in AI-led instructional settings rather than co-creative collaboration. This leaves open questions about how children can meaningfully engage with AI through iterative co-creation. We present Tinker Tales, a tangible storytelling system designed with narrative and social-emotional scaffolding to support child-AI collaboration. The system combines a physical storytelling board, NFC-embedded toys representing story elements (e.g., characters, places, items, and emotions), and a mobile app that mediates child-AI interaction. Children shape and refine stories by placing and moving story elements and interacting with the AI through tangible and voice-based interaction. We conducted an exploratory user study with 10 children to examine how they interacted with Tinker Tales. Our findings show that children treated the AI as an attentive, responsive collaborator, while scaffolding supported coherent narrative refinement without diminishing children's agency.

</details>


### [303] [A Human-Centered Privacy Approach (HCP) to AI](https://arxiv.org/abs/2602.04616)
*Luyi Sun,Wei Xu,Zaifeng Gao*

Main category: cs.HC

TL;DR: 本文全面介绍了以人为中心的人工智能（HCAI）中的隐私问题，提出了以人为中心的隐私（HCP）框架，综合多方面提供解决方案，并探讨了挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: 在HCAI范式兴起且面临显著伦理问题背景下，关注个人隐私保护。

Method: 分析AI开发生命周期各阶段隐私风险，引入隐私保护技术，结合用户视角、监管伦理情况和隐私治理，给出设计指南，介绍案例研究。

Result: 提出了HCP框架并综合各方视角给出解决方案。

Conclusion: 需多学科方法将隐私嵌入HCAI核心，确保技术发展尊重人的自主性、信任和尊严。

Abstract: As the paradigm of Human-Centered AI (HCAI) gains prominence, its benefits to society are accompanied by significant ethical concerns, one of which is the protection of individual privacy. This chapter provides a comprehensive overview of privacy within HCAI, proposing a human-centered privacy (HCP) framework, providing integrated solution from technology, ethics, and human factors perspectives. The chapter begins by mapping privacy risks across each stage of AI development lifecycle, from data collection to deployment and reuse, highlighting the impact of privacy risks on the entire system. The chapter then introduces privacy-preserving techniques such as federated learning and dif erential privacy. Subsequent chapters integrate the crucial user perspective by examining mental models, alongside the evolving regulatory and ethical landscapes as well as privacy governance. Next, advice on design guidelines is provided based on the human-centered privacy framework. After that, we introduce practical case studies across diverse fields. Finally, the chapter discusses persistent open challenges and future research directions, concluding that a multidisciplinary approach, merging technical, design, policy, and ethical expertise, is essential to successfully embed privacy into the core of HCAI, thereby ensuring these technologies advance in a manner that respects and ensures human autonomy, trust and dignity.

</details>


### [304] [Adaptive Prompt Elicitation for Text-to-Image Generation](https://arxiv.org/abs/2602.04713)
*Xinyi Wen,Lena Hegemann,Xiaofu Jin,Shuai Ma,Antti Oulasvirta*

Main category: cs.HC

TL;DR: 提出Adaptive Prompt Elicitation (APE)技术，能自适应询问视觉查询以帮助用户完善提示词，在数据集和用户研究中表现良好，为文本到图像模型提示交互提供有效补充。


<details>
  <summary>Details</summary>
Motivation: 解决文本到图像生成中用户输入模糊和难以应对模型特性，使生成结果与用户意图对齐的问题。

Method: 在信息论框架下进行交互式意图推理，用语言模型先验将潜在意图表示为可解释特征需求，自适应生成视觉查询并将需求编译成有效提示。

Result: 在IDEA - Bench和DesignBench上实现更强对齐且效率提升，用户研究中对齐度提高19.8%且无额外工作量。

Conclusion: 为普通用户提供了一种有效且高效的方法，作为现有文本到图像模型基于提示交互范式的补充。

Abstract: Aligning text-to-image generation with user intent remains challenging, for users who provide ambiguous inputs and struggle with model idiosyncrasies. We propose Adaptive Prompt Elicitation (APE), a technique that adaptively asks visual queries to help users refine prompts without extensive writing. Our technical contribution is a formulation of interactive intent inference under an information-theoretic framework. APE represents latent intent as interpretable feature requirements using language model priors, adaptively generates visual queries, and compiles elicited requirements into effective prompts. Evaluation on IDEA-Bench and DesignBench shows that APE achieves stronger alignment with improved efficiency. A user study with challenging user-defined tasks demonstrates 19.8% higher alignment without workload overhead. Our work contributes a principled approach to prompting that, for general users, offers an effective and efficient complement to the prevailing prompt-based interaction paradigm with text-to-image models.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [305] [Sparse group principal component analysis via double thresholding with application to multi-cellular programs](https://arxiv.org/abs/2602.04178)
*Qi Xu,Jing Lei,Kathryn Roeder*

Main category: stat.ME

TL;DR: 提出SGPCA方法估计多细胞程序（MCPs），有高效算法和理论保证，模拟显示效果好并应用于狼疮研究。


<details>
  <summary>Details</summary>
Motivation: 现有估计MCPs的方法存在计算成本高和统计功效有限的问题。

Method: 提出Sparse Group Principal Component Analysis（SGPCA）方法，引入基于幂迭代的双阈值算法。

Result: 算法复杂度为$O(np)$，有理论保证，模拟显示估计准确性高、信号检测统计功效好，应用于狼疮研究发现差异表达的MCPs。

Conclusion: SGPCA方法高效、可扩展，在估计MCPs上表现优于现有方法。

Abstract: Multi-cellular programs (MCPs) are coordinated patterns of gene expression across interacting cell types that collectively drive complex biological processes such as tissue development and immune responses. While MCPs are typically estimated from high-dimensional gene expression data using methods like sparse principal component analysis or latent factor models, these approaches often suffer from high computational costs and limited statistical power. In this work, we propose Sparse Group Principal Component Analysis (SGPCA) to estimate MCPs by leveraging their inherent group and individual sparsity. We introduce an efficient double-thresholding algorithm based on power iteration. In each iteration, a group thresholding step first identifies relevant gene groups, followed by an individual thresholding step to select active cell types. This algorithm achieves a linear computational complexity of $O(np)$, making it highly efficient and scalable for large-scale genomic analyses. We establish theoretical guarantees for SGPCA, including statistical consistency and a convergence rate that surpasses competing methods. Through extensive simulations, we demonstrate that SGPCA achieves superior estimation accuracy and improved statistical power for signal detection. Furthermore, We apply SGPCA to a Lupus study, discovering differentially expressed MCPs distinguishing Lupus patients from normal subjects.

</details>


### [306] [Score-Based Change-Point Detection and Region Localization for Spatio-Temporal Point Processes](https://arxiv.org/abs/2602.04798)
*Wenbin Zhou,Liyan Xie,Shixiang Zhu*

Main category: stat.ME

TL;DR: 研究时空点过程的顺序变点检测，提出无似然、基于得分的检测框架，给出理论保证并通过模拟和实际数据验证有效性。


<details>
  <summary>Details</summary>
Motivation: 经典方法多用于时间变化检测，缺乏对空间区域的推断，需解决时空点过程中变点检测及空间定位问题。

Method: 提出无似然、基于得分的检测框架，利用局部条件加权Hyvärinen得分量化偏差，结合时空CUSUM型统计量聚合得分。

Result: 建立了误报控制、检测延迟和空间定位准确性的理论保证，并通过模拟和真实时空事件数据验证了方法的有效性。

Conclusion: 所提方法可以实现具有空间可解释性的实时检测，在时空点过程变点检测中有效。

Abstract: We study sequential change-point detection for spatio-temporal point processes, where actionable detection requires not only identifying when a distributional change occurs but also localizing where it manifests in space. While classical quickest change detection methods provide strong guarantees on detection delay and false-alarm rates, existing approaches for point-process data predominantly focus on temporal changes and do not explicitly infer affected spatial regions. We propose a likelihood-free, score-based detection framework that jointly estimates the change time and the change region in continuous space-time without assuming parametric knowledge of the pre- or post-change dynamics. The method leverages a localized and conditionally weighted Hyvärinen score to quantify event-level deviations from nominal behavior and aggregates these scores using a spatio-temporal CUSUM-type statistic over a prescribed class of spatial regions. Operating sequentially, the procedure outputs both a stopping time and an estimated change region, enabling real-time detection with spatial interpretability. We establish theoretical guarantees on false-alarm control, detection delay, and spatial localization accuracy, and demonstrate the effectiveness of the proposed approach through simulations and real-world spatio-temporal event data.

</details>


### [307] [Journey to the Centre of Cluster: Harnessing Interior Nodes for A/B Testing under Network Interference](https://arxiv.org/abs/2602.04457)
*Qianyi Chen,Anpeng Wu,Bo Li,Lu Deng,Yong Wang*

Main category: stat.ME

TL;DR: 本文针对A/B测试在平台上面临的网络干扰挑战，提出增强MII估计量，通过模拟研究证明其在不同设置下表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决A/B测试中网络干扰问题，现有基于聚类随机化的网络感知估计量方差高、存在偏差的问题。

Method: 先提出直接对内部节点求平均构建MII估计量，后用在全网络上训练的反事实预测器对MII估计量进行增强，调整协变量分布偏移。

Result: 增强的MII估计量体现了预测驱动推理框架内点估计量的解析形式，模拟研究表明其在不同设置下表现出色。

Conclusion: 增强的MII估计量能有效解决A/B测试中网络干扰带来的估计问题，在不同设置下有良好性能。

Abstract: A/B testing on platforms often faces challenges from network interference, where a unit's outcome depends not only on its own treatment but also on the treatments of its network neighbors. To address this, cluster-level randomization has become standard, enabling the use of network-aware estimators. These estimators typically trim the data to retain only a subset of informative units, achieving low bias under suitable conditions but often suffering from high variance. In this paper, we first demonstrate that the interior nodes - units whose neighbors all lie within the same cluster - constitute the vast majority of the post-trimming subpopulation. In light of this, we propose directly averaging over the interior nodes to construct the mean-in-interior (MII) estimator, which circumvents the delicate reweighting required by existing network-aware estimators and substantially reduces variance in classical settings. However, we show that interior nodes are often not representative of the full population, particularly in terms of network-dependent covariates, leading to notable bias. We then augment the MII estimator with a counterfactual predictor trained on the entire network, allowing us to adjust for covariate distribution shifts between the interior nodes and full population. By rearranging the expression, we reveal that our augmented MII estimator embodies an analytical form of the point estimator within prediction-powered inference framework. This insight motivates a semi-supervised lens, wherein interior nodes are treated as labeled data subject to selection bias. Extensive and challenging simulation studies demonstrate the outstanding performance of our augmented MII estimator across various settings.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [308] [Horizon-LM: A RAM-Centric Architecture for LLM Training](https://arxiv.org/abs/2602.04816)
*Zhengqing Yuan,Lichao Sun,Yanfang,Ye*

Main category: cs.OS

TL;DR: 提出了名为 Horizon - LM 的以内存为中心的训练系统，重新定义 CPU 和 GPU 在大模型优化中的作用，在单 GPU 上训练大模型表现出色。


<details>
  <summary>Details</summary>
Motivation: 大语言模型快速发展，但现有 GPU 为中心的执行模式使大模型扩展受限于多 GPU 集群、复杂运行时和不可预测的内存消耗，阻碍节点级后训练工作负载。

Method: 采用 CPU 主、GPU 模板执行模型，将主机内存作为参数存储地，GPU 仅作为临时计算引擎，消除 GPU 持久模块和自动求导图，采用显式重新计算和手动梯度传播，引入流水线双缓冲执行引擎。

Result: 在单 H200 GPU 与 1.5 TB 主机内存上能可靠训练达 120B 参数的模型，在单 A100 机器上训练吞吐量比带 CPU 卸载的 DeepSpeed ZeRO - 3 高 12.2 倍，且保留数值正确性。

Conclusion: 主机内存而非 GPU 内存决定了节点级大模型训练的真正可行性边界。

Abstract: The rapid growth of large language models (LLMs) has outpaced the evolution of single-GPU hardware, making model scale increasingly constrained by memory capacity rather than computation. While modern training systems extend GPU memory through distributed parallelism and offloading across CPU and storage tiers, they fundamentally retain a GPU-centric execution paradigm in which GPUs host persistent model replicas and full autograd graphs. As a result, scaling large models remains tightly coupled to multi-GPU clusters, complex distributed runtimes, and unpredictable host memory consumption, creating substantial barriers for node-scale post-training workloads such as instruction tuning, alignment, and domain adaptation. We present Horizon-LM, a memory-centric training system that redefines the roles of CPU and GPU for large-model optimization. Horizon-LM treats host memory as the authoritative parameter store and uses GPUs solely as transient compute engines through a CPU-master, GPU-template execution model. By eliminating persistent GPU-resident modules and autograd graphs, employing explicit recomputation with manual gradient propagation, and introducing a pipelined double-buffered execution engine, Horizon-LM decouples model scale from GPU count and bounds memory usage to the theoretical parameter footprint. On a single H200 GPU with 1.5\,TB host RAM, Horizon-LM reliably trains models up to 120B parameters. On a standard single A100 machine, Horizon-LM achieves up to 12.2$\times$ higher training throughput than DeepSpeed ZeRO-3 with CPU offloading while preserving numerical correctness. Across platforms and scales, Horizon-LM sustains high device utilization and predictable memory growth, demonstrating that host memory, not GPU memory, defines the true feasibility boundary for node-scale large-model training.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [309] [GPU Acceleration and Portability of the TRIMEG Code for Gyrokinetic Plasma Simulations using OpenMP](https://arxiv.org/abs/2601.14301)
*Giorgio Daneri*

Main category: physics.plasm-ph

TL;DR: 等离子体物理依赖模拟，陀螺动力学模拟计算成本高，本文围绕TRIMEG代码，用GPU加速，评估性能并模拟ITG模式验证正确性。


<details>
  <summary>Details</summary>
Motivation: 等离子体物理模拟对研究现象和实验设置重要，陀螺动力学模拟计算成本高，需加速平台减少执行时间。

Method: 基于TRIMEG代码，采用OpenMP API进行GPU卸载，将部分代码移植到GPU平台，克服编译器限制，评估内核性能和混合并行化效率。

Result: 计算了GPU实现的加速比，用GPU加速版本模拟ITG模式，验证了能量增长率和二维模式结构的正确性。

Conclusion: 通过GPU加速可有效减少TRIMEG代码的执行时间，且模拟结果正确。

Abstract: The field of plasma physics heavily relies on simulations to model various phenomena, such as instabilities, turbulence, and nonlinear behaviors that would otherwise be difficult to study from a purely theoretical approach. Simulations are fundamental in accurately setting up experiments, which can be extremely costly and complex. As high-fidelity tools, gyrokinetic simulations play a crucial role in discovering new physics, interpreting experimental results, and improving the design of next-generation devices. However, their high computational costs necessitate the use of acceleration platforms to reduce execution time. This work revolves around the TRIangular MEsh based Gyrokinetic (TRIMEG) code, which performs high-accuracy particle-in-cell plasma simulations in tokamak geometries, leveraging a novel finite element approach. The rise of graphical processing units (GPUs) constitutes an occasion to satisfy such computational needs, by offloading the most expensive portion of the code to the accelerators. The chosen approach features GPU offloading with the OpenMP API, which grants portability of the code to different architectures, namely AMD and NVIDIA. The particle pushing as well as the grid-to-particle operations have been ported to GPU platforms. Compiler limitations had to be overcome, and portions of the code were restructured to be suitable for GPU acceleration. Kernel performance was evaluated by carrying out GPU grid size exploration, as well as scalability studies. In addition, the efficiency of hybrid MPI-OpenMP offloading parallelization was assessed. The speedup of the GPU implementation was calculated by comparing it with the pure CPU version using different rationales. The Ion Temperature Gradient (ITG) mode was simulated using the GPU-accelerated version, and its correctness was verified in terms of the energy growth rate and the two-dimensional mode structures.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [310] [Entanglement improves coordination in distributed systems](https://arxiv.org/abs/2602.04588)
*Francisco Ferreira da Silva,Stephanie Wehner*

Main category: quant-ph

TL;DR: 研究在含两台服务器的分布式系统中应用共享纠缠解决双工作优化问题，表明纠缠辅助路由策略在特定条件下优于经典策略，确定分布式调度协调为量子网络新应用领域。


<details>
  <summary>Details</summary>
Motivation: 分布式系统协调受通信延迟影响性能，量子纠缠有强关联且测量时瞬间显现，可用于改善系统性能。

Method: 提出严格分析模型，通过排队理论分析、非局部博弈公式化和经典边界计算认证。

Result: 当基线任务吞吐量函数严格凸时，纠缠辅助路由策略比最优无通信经典策略有帕累托最优性能。

Conclusion: 分布式调度和协调是近期限基于纠缠的量子网络新应用领域。

Abstract: Coordination in distributed systems is often hampered by communication latency, which degrades performance. Quantum entanglement offers fundamentally stronger correlations than classically achievable without communication. Crucially, these correlations manifest instantaneously upon measurement, irrespective of the physical distance separating the systems. We investigate the application of shared entanglement to a dual-work optimization problem in a distributed system comprising two servers. The system must process both a continuously available, preemptible baseline task and incoming customer requests arriving in pairs. System performance is characterized by the trade-off between baseline task throughput and customer waiting time. We present a rigorous analytical model demonstrating that when the baseline task throughput function is strictly convex, rewarding longer uninterrupted processing periods, entanglement-assisted routing strategies achieve Pareto-superior performance compared to optimal communication-free classical strategies. We prove this advantage through queueing-theoretic analysis, non-local game formulation, and computational certification of classical bounds. Our results identify distributed scheduling and coordination as a novel application domain for near-term entanglement-based quantum networks.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [311] [Structure-Informed Estimation for Pilot-Limited MIMO Channels via Tensor Decomposition](https://arxiv.org/abs/2602.04083)
*Alexandre Barbosa de Lima*

Main category: eess.SP

TL;DR: 本文提出混合张量 - 神经架构用于宽带MIMO系统的导频受限信道估计，对比不同方法，分析恢复阈值，实验表明其性能优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 解决高维6G场景下宽带MIMO系统信道估计的导频开销限制问题。

Method: 提出混合张量 - 神经架构，将导频受限信道估计建模为稀疏观测的低秩张量补全，对比CP和Tucker分解方法，用3D U - Net学习低秩结构外的残差分量。

Result: 样本复杂度与内在模型维度相关；在合成信道上，在5 - 10%导频密度下NMSE比LS和OMP基线提高10 - 20 dB；在DeepMIMO射线追踪信道上，比纯张量方法额外降低24 - 44%的NMSE。

Conclusion: 所提混合张量 - 神经架构在导频受限信道估计中表现良好，优于传统方法。

Abstract: Channel estimation in wideband multiple-input multiple-output (MIMO) systems faces fundamental pilot overhead limitations in high-dimensional beyond-5G and sixth-generation (6G) scenarios. This paper presents a hybrid tensor-neural architecture that formulates pilot-limited channel estimation as low-rank tensor completion from sparse observations -- a fundamentally different setting from prior tensor methods that assume fully observed received signal tensors. A canonical polyadic (CP) baseline implemented via a projection-based scheme (Tucker completion under partial observations) and Tucker decompositions are compared under varying signal-to-noise ratio (SNR) and scattering conditions: CP performs well for specular channels matching the multipath model, while Tucker provides greater robustness under model mismatch. A lightweight three-dimensional (3D) U-Net learns residual components beyond the low-rank structure, bridging algebraic models and realistic propagation effects. Empirical recovery threshold analysis shows that sample complexity scales approximately with intrinsic model dimensionality $L(N_r + N_t + N_f)$ rather than ambient tensor size $N_r N_t N_f$, where $L$ denotes the number of dominant propagation paths. Experiments on synthetic channels demonstrate 10-20\,dB normalized mean-square error (NMSE) improvement over least-squares (LS) and orthogonal matching pursuit (OMP) baselines at 5-10\% pilot density, while evaluations on DeepMIMO ray-tracing channels show 24-44\% additional NMSE reduction over pure tensor-based methods.

</details>


### [312] [Majorization-Minimization Networks for Inverse Problems: An Application to EEG Imaging](https://arxiv.org/abs/2602.03855)
*Le Minh Triet Tran,Sarah Reynaud,Ronan Fablet,Adrien Merlini,François Rousseau,Mai Quyen Pham*

Main category: eess.SP

TL;DR: 提出用于逆问题的学习型MM框架，在EEG源成像实验中表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的逆问题优化方法缺乏对下降和曲率的明确控制，限制了鲁棒性。

Method: 在双层优化设置下提出学习型MM框架，学习结构化曲率主元，用轻量级循环神经网络参数化主元并约束其满足MM条件，针对不同情况采用不同曲率界定方法。

Result: 在EEG源成像实验中，相比深度展开和元学习基线，新框架在准确性、稳定性和跨数据集泛化性方面有改进。

Conclusion: 所提学习型MM框架在逆问题上有良好效果。

Abstract: Inverse problems are often ill-posed and require optimization schemes with strong stability and convergence guarantees. While learning-based approaches such as deep unrolling and meta-learning achieve strong empirical performance, they typically lack explicit control over descent and curvature, limiting robustness. We propose a learned Majorization-Minimization (MM) framework for inverse problems within a bilevel optimization setting. Instead of learning a full optimizer, we learn a structured curvature majorant that governs each MM step while preserving classical MM descent guarantees. The majorant is parameterized by a lightweight recurrent neural network and explicitly constrained to satisfy valid MM conditions. For cosine-similarity losses, we derive explicit curvature bounds yielding diagonal majorants. When analytic bounds are unavailable, we rely on efficient Hessian-vector product-based spectral estimation to automatically upper-bound local curvature without forming the Hessian explicitly. Experiments on EEG source imaging demonstrate improved accuracy, stability, and cross-dataset generalization over deep-unrolled and meta-learning baselines.

</details>


### [313] [The Turing Synthetic Radar Dataset: A dataset for pulse deinterleaving](https://arxiv.org/abs/2602.03856)
*Edward Gunn,Adam Hosford,Robert Jones,Leo Zeitler,Ian Groves,Victoria Nockles*

Main category: eess.SP

TL;DR: 提出图灵合成雷达数据集，用于雷达脉冲解交织研究，还发起相关挑战。


<details>
  <summary>Details</summary>
Motivation: 解决电子战和信号情报中分离多个未知辐射源交织雷达脉冲的关键问题。

Method: 创建包含6000个脉冲序列、近30亿个脉冲的数据集，发起图灵解交织挑战，模型通过聚类关联脉冲到正确辐射源。

Result: 得到图灵合成雷达数据集，可用于电子战领域复杂模型开发。

Conclusion: 图灵合成雷达数据集是首批公开、全面模拟的脉冲序列数据集，有助于电子战社区模型发展。

Abstract: We present the Turing Synthetic Radar Dataset, a comprehensive dataset to serve both as a benchmark for radar pulse deinterleaving research and as an enabler of new research methods. The dataset addresses the critical problem of separating interleaved radar pulses from multiple unknown emitters for electronic warfare applications and signal intelligence. Our dataset contains a total of 6000 pulse trains over two receiver configurations, totalling to almost 3 billion pulses, featuring realistic scenarios with up to 110 emitters and significant parameter space overlap. To encourage dataset adoption and establish standardised evaluation procedures, we have launched an accompanying Turing Deinterleaving Challenge, for which models need to associate pulses in interleaved pulse trains to the correct emitter by clustering and maximising metrics such as the V-measure. The Turing Synthetic Radar Dataset is one of the first publicly available, comprehensively simulated pulse train datasets aimed to facilitate sophisticated model development in the electronic warfare community

</details>


### [314] [PENGUIN: General Vital Sign Reconstruction from PPG with Flow Matching State Space Model](https://arxiv.org/abs/2602.03858)
*Shuntaro Suzuki,Shuitsu Koyama,Shinnosuke Hirano,Shunya Nagashima*

Main category: eess.SP

TL;DR: 提出PPENGUIN框架用于从PPG信号重建生命体征，经多数据集评估表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: PPG信号易受运动伪影和噪声影响，现有估计方法存在单任务或环境限制、丢弃生命体征形态特征等问题。

Method: 提出PPENGUIN，一个扩展深度状态空间模型的生成流匹配框架，可对PPG进行细粒度条件处理以重建多个生命体征为连续波形。

Result: 使用六个真实世界PPG数据集在三个不同生命体征重建任务中评估，始终优于特定任务和通用基线。

Conclusion: PPENGUIN是一个从PPG进行稳健生命体征重建的通用框架。

Abstract: Photoplethysmography (PPG) plays a crucial role in continuous cardiovascular health monitoring as a non-invasive and cost-effective modality. However, PPG signals are susceptible to motion artifacts and noise, making accurate estimation of vital signs such as arterial blood pressure (ABP) challenging. Existing estimation methods are often restricted to a single-task or environment, limiting their generalizability across diverse PPG decoding scenarios. Moreover, recent general-purpose approaches typically rely on predictions over multi-second intervals, discarding the morphological characteristics of vital signs. To address these challenges, we propose PENGUIN, a generative flow-matching framework that extends deep state space models, enabling fine-grained conditioning on PPG for reconstructing multiple vital signs as continuous waveforms. We evaluate PENGUIN using six real-world PPG datasets across three distinct vital sign reconstruction tasks (electrocardiogram reconstruction, respiratory monitoring, and ABP monitoring). Our method consistently outperformed both task-specific and general-purpose baselines, demonstrating PENGUIN as a general framework for robust vital sign reconstruction from PPG.

</details>


### [315] [A Multi-Modal Foundational Model for Wireless Communication and Sensing](https://arxiv.org/abs/2602.04016)
*Vahid Yazdnian,Yasaman Ghasempour*

Main category: eess.SP

TL;DR: 本文提出用于物理层无线系统的任务无关的多模态基础模型，能学习跨模态表征，在多任务实现良好泛化。


<details>
  <summary>Details</summary>
Motivation: 现有的基于学习的无线技术泛化性差，在新场景需大量再训练。

Method: 采用物理引导的自监督预训练策略，融入专用物理令牌来捕捉跨模态物理对应关系。

Result: 在包括多天线优化、信道估计和设备定位等下游任务评估中，相比特定任务基线具有更好的泛化性、对部署变化的鲁棒性和更少的数据需求。

Conclusion: 所提出的模型能有效解决现有无线技术泛化性差的问题，为无线通信和感知提供更好方案。

Abstract: Artificial intelligence is a key enabler for next-generation wireless communication and sensing. Yet, today's learning-based wireless techniques do not generalize well: most models are task-specific, environment-dependent, and limited to narrow sensing modalities, requiring costly retraining when deployed in new scenarios. This work introduces a task-agnostic, multi-modal foundational model for physical-layer wireless systems that learns transferable, physics-aware representations across heterogeneous modalities, enabling robust generalization across tasks and environments. Our framework employs a physics-guided self-supervised pretraining strategy incorporating a dedicated physical token to capture cross-modal physical correspondences governed by electromagnetic propagation. The learned representations enable efficient adaptation to diverse downstream tasks, including massive multi-antenna optimization, wireless channel estimation, and device localization, using limited labeled data. Our extensive evaluations demonstrate superior generalization, robustness to deployment shifts, and reduced data requirements compared to task-specific baselines.

</details>


### [316] [Aortic Valve Disease Detection from PPG via Physiology-Informed Self-Supervised Learning](https://arxiv.org/abs/2602.04266)
*Jiaze Wang,Qinghao Zhao,Zizheng Chen,Zhejun Sun,Deyun Zhang,Yuxi Zhou,Shenda Hong*

Main category: eess.SP

TL;DR: 传统主动脉瓣疾病诊断受限于超声心动图，研究提出PG - SSL范式利用未标记PPG数据进行主动脉瓣疾病筛查，表现优于监督基线，证明可解决医学AI标记稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 传统超声心动图用于主动脉瓣疾病诊断在大规模早期筛查中受限，且PPG数据驱动方法因缺乏标记数据效果不佳，需利用大规模未标记PPG数据进行高效筛查。

Method: 提出生理学引导的自监督学习（PG - SSL）范式，将临床知识转化为PPG形态表型构建代理任务进行自监督预训练，在小标记子集上采用双分支门控融合架构微调。

Result: PG - SSL框架对主动脉瓣狭窄和主动脉瓣反流筛查的AUC分别达0.765和0.776，显著优于监督基线。多变量分析验证模型输出是独立数字生物标志物，有持续预后价值。

Conclusion: PG - SSL为医学人工智能标记稀缺问题提供有效、领域知识驱动的解决方案，在低成本、大规模主动脉瓣疾病早期筛查中有潜力。

Abstract: Traditional diagnosis of aortic valve disease relies on echocardiography, but its cost and required expertise limit its use in large-scale early screening. Photoplethysmography (PPG) has emerged as a promising screening modality due to its widespread availability in wearable devices and its ability to reflect underlying hemodynamic dynamics. However, the extreme scarcity of gold-standard labeled PPG data severely constrains the effectiveness of data-driven approaches. To address this challenge, we propose and validate a new paradigm, Physiology-Guided Self-Supervised Learning (PG-SSL), aimed at unlocking the value of large-scale unlabeled PPG data for efficient screening of Aortic Stenosis (AS) and Aortic Regurgitation (AR). Using over 170,000 unlabeled PPG samples from the UK Biobank, we formalize clinical knowledge into a set of PPG morphological phenotypes and construct a pulse pattern recognition proxy task for self-supervised pre-training. A dual-branch, gated-fusion architecture is then employed for efficient fine-tuning on a small labeled subset. The proposed PG-SSL framework achieves AUCs of 0.765 and 0.776 for AS and AR screening, respectively, significantly outperforming supervised baselines trained on limited labeled data. Multivariable analysis further validates the model output as an independent digital biomarker with sustained prognostic value after adjustment for standard clinical risk factors. This study demonstrates that PG-SSL provides an effective, domain knowledge-driven solution to label scarcity in medical artificial intelligence and shows strong potential for enabling low-cost, large-scale early screening of aortic valve disease.

</details>


### [317] [Learning to Separate RF Signals Under Uncertainty: Detect-Then-Separate vs. Unified Joint Models](https://arxiv.org/abs/2602.04650)
*Ariel Rodrigez,Alejandro Lancho,Amir Weiss*

Main category: eess.SP

TL;DR: 针对射频信号干扰恢复问题，分析DTS策略，提出统一联合模型UJM并对比性能，显示UJM可作为DTS可扩展实用替代方案。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动方法在多干扰源场景扩展性差，需更优信号恢复方法。

Method: 先分析DTS策略获得理论依据，再提出UJM，用定制UNet架构对比DTS和UJM。

Result: 容量匹配的UJM在多种条件下可达到类似DTS的性能，包括训练和测试类型不确定比例不匹配情况。

Conclusion: UJM是DTS可扩展实用替代方案，为统一分离开辟新方向。

Abstract: The increasingly crowded radio frequency (RF) spectrum forces communication signals to coexist, creating heterogeneous interferers whose structure often departs from Gaussian models. Recovering the interference-contaminated signal of interest in such settings is a central challenge, especially in single-channel RF processing. Existing data-driven methods often assume that the interference type is known, yielding ensembles of specialized models that scale poorly with the number of interferers. We show that detect-then-separate (DTS) strategies admit an analytical justification: within a Gaussian mixture framework, a plug-in maximum a posteriori detector followed by type-conditioned optimal estimation achieves asymptotic minimum mean-square error optimality under a mild temporal-diversity condition. This makes DTS a principled benchmark, but its reliance on multiple type-specific models limits scalability. Motivated by this, we propose a unified joint model (UJM), in which a single deep neural architecture learns to jointly detect and separate when applied directly to the received signal. Using tailored UNet architectures for baseband (complex-valued) RF signals, we compare DTS and UJM on synthetic and recorded interference types, showing that a capacity-matched UJM can match oracle-aided DTS performance across diverse signal-to-interference-and-noise ratios, interference types, and constellation orders, including mismatched training and testing type-uncertainty proportions. These findings highlight UJM as a scalable and practical alternative to DTS, while opening new directions for unified separation under broader regimes.

</details>


### [318] [Knowledge Distillation for mmWave Beam Prediction Using Sub-6 GHz Channels](https://arxiv.org/abs/2602.04703)
*Sina Tavakolian,Nhan Thanh Nguyen,Ahmed Alkhateeb,Markku Juntti*

Main category: eess.SP

TL;DR: 提出基于知识蒸馏技术的低计算开销的sub - 6 GHz信道与毫米波波束映射框架，学生模型在降参降复杂度情况下达到教师模型性能。


<details>
  <summary>Details</summary>
Motivation: 毫米波高移动性环境波束成形训练开销大，现有利用sub - 6 GHz信道预测毫米波波束的方法依赖大深度学习模型，计算和内存要求高。

Method: 基于知识蒸馏技术构建sub - 6 GHz信道与毫米波波束映射框架，开发基于个体和关系蒸馏策略的两个紧凑学生深度学习架构。

Result: 学生模型达到教师模型的波束预测精度和频谱效率，同时可训练参数和计算复杂度降低99%。

Conclusion: 所提出的紧凑学生模型在减少计算资源的同时，能有效实现毫米波波束预测。

Abstract: Beamforming in millimeter-wave (mmWave) high-mobility environments typically incurs substantial training overhead. While prior studies suggest that sub-6 GHz channels can be exploited to predict optimal mmWave beams, existing methods depend on large deep learning (DL) models with prohibitive computational and memory requirements. In this paper, we propose a computationally efficient framework for sub-6 GHz channel-mmWave beam mapping based on the knowledge distillation (KD) technique. We develop two compact student DL architectures based on individual and relational distillation strategies, which retain only a few hidden layers yet closely mimic the performance of large teacher DL models. Extensive simulations demonstrate that the proposed student models achieve the teacher's beam prediction accuracy and spectral efficiency while reducing trainable parameters and computational complexity by 99%.

</details>


### [319] [Cross-Attention Transformer for Joint Multi-Receiver Uplink Neural Decoding](https://arxiv.org/abs/2602.04728)
*Xavier Tardy,Grégoire Lefebvre,Apostolos Kountouris,Haïfa Fares,Amor Nafkha*

Main category: eess.SP

TL;DR: 提出跨注意力Transformer用于联合解码多接入点接收的上行OFDM信号，性能优且架构实用。


<details>
  <summary>Details</summary>
Motivation: 解决多协调接入点接收的上行OFDM信号联合解码问题，提升解码性能和适应性。

Method: 使用跨注意力Transformer，含共享的每个接收器编码器和逐令牌跨注意力模块，以比特度量目标进行训练。

Result: 在现实Wi-Fi信道中，始终优于经典管道和卷积基线，常与假设完美信道知识的强大基线相匹配甚至超越。

Conclusion: 该架构紧凑、计算成本低、延迟低，是下一代Wi-Fi接收器的实用构建块。

Abstract: We propose a cross-attention Transformer for joint decoding of uplink OFDM signals received by multiple coordinated access points. A shared per-receiver encoder learns time-frequency structure within each received grid, and a token-wise cross-attention module fuses the receivers to produce soft log-likelihood ratios for a standard channel decoder, without requiring explicit per-receiver channel estimates. Trained with a bit-metric objective, the model adapts its fusion to per-receiver reliability, tolerates missing or degraded links, and remains robust when pilots are sparse. Across realistic Wi-Fi channels, it consistently outperforms classical pipelines and strong convolutional baselines, frequently matching (and in some cases surpassing) a powerful baseline that assumes perfect channel knowledge per access point. Despite its expressiveness, the architecture is compact, has low computational cost (low GFLOPs), and achieves low latency on GPUs, making it a practical building block for next-generation Wi-Fi receivers.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [320] [Nota de Política Pública: Quanto de produtividade precisamos para reduzir a jornada de trabalho?](https://arxiv.org/abs/2602.03884)
*Victor Rangel*

Main category: econ.GN

TL;DR: 本文在短期结构模型中量化了将每周工作时间从44小时减至36小时的工时上限政策对GDP的即时影响，基线模拟中该转变需约8.5%的全要素生产率来维持GDP水平。


<details>
  <summary>Details</summary>
Motivation: 量化将正式工作时间上限从每周44小时降至36小时这一政策的即时影响。

Method: 在具有预设资本的短期结构模型中进行分析，定义维持GDP在基线水平所需的全要素生产率A_req。

Result: 基线模拟中，从44小时到36小时的转变意味着A_req约为8.5%。

Conclusion: 未明确提及，但可知工时上限政策对GDP有影响，维持GDP需一定全要素生产率提升。

Abstract: This paper quantifies, within a short-run structural model with predetermined capital, the immediate effects of imposing a cap on formal working hours that reduces the weekly workweek from 44 to 36 hours. The central object is the total factor productivity required to preserve GDP at its baseline level, A_req, defined as the multiplicative factor applied to A_t that equates output under the policy to output in the baseline. In the baseline simulation, the 44 -> 36 transition implies A_req ~ 8.5%

</details>


### [321] [Discounted Sales of Expiring Perishables: Challenges for Demand Forecasting in Grocery Retail Practice](https://arxiv.org/abs/2602.04464)
*David Winkelmann,Theresa Elbracht,Jonas Brenker,Arnold Gerzen*

Main category: econ.GN

TL;DR: 研究探讨将折扣销售纳入需求预测的方法，发现当前预测低估需求，强调需更精确方法。


<details>
  <summary>Details</summary>
Motivation: 杂货店零售商常对易腐品打折促销，但将折扣销售纳入未来需求预测存在挑战。

Method: 采用两步回归方法，分析欧洲一家大型杂货店零售商的超1700种产品、676家门店的数据。

Result: 折扣销售出现时，多数SKU的预测低估实际需求，剩余提升效应受降价销售数量显著影响。

Conclusion: 杂货店行业需要更精确的方法将折扣销售纳入需求预测，以防止库存过剩及变质带来的经济和环境影响。

Abstract: Grocery retailers frequently apply price discounts to stimulate demand for expiring perishables. However, integrating these discounted sales into future demand forecasts presents a significant challenge. This study investigates the effectiveness of incorporating a fixed share of these sales as \textit{regular} demand into the forecast, as commonly applied in practice. We employ a two-step regression approach on data from a major European grocery retailer, covering over 1,700 products across 676 stores. We reveal that forecasts underestimate actual demand for most SKUs when discounted sales occur. This residual uplift effect is significantly influenced by the number of sales at reduced prices. Our findings underscore the necessity for more precise approaches to integrate discounted sales into demand forecasts, thereby preventing excess inventory and the associated economic and environmental impacts of spoilage in the grocery sector.

</details>


### [322] [Mobility-as-a-service (MaaS) system as a multi-leader-multi-follower game: A single-level variational inequality (VI) formulation](https://arxiv.org/abs/2601.19880)
*Rui Yao,Xinyu Ma,Kenan Zhang*

Main category: econ.GN

TL;DR: 本文将MaaS系统建模为多领导者 - 多跟随者博弈，提出新的单级变分不等式（VI）公式，证明均衡解存在，数值实验验证其可扩展性和“三赢”效果。


<details>
  <summary>Details</summary>
Motivation: 捕捉MaaS平台、服务运营商和旅行者之间的复杂交互，解决多模式交通网络中的决策问题。

Method: 将MaaS系统建模为多领导者 - 多跟随者博弈，引入虚拟交通运营商提出单级VI公式。

Result: 证明给定服务容量协商批发价时均衡解存在，数值实验表明批发价可调整以实现系统目标，实现“三赢”结果。

Conclusion: 提出的MaaS系统有潜力实现“三赢”结果，可通过批发容量价格明确帕累托改进机制，模型和算法具有可扩展性。

Abstract: This study models a Mobility-as-a-Service (MaaS) system as a multi-leader-multi-follower game that captures the complex interactions among the MaaS platform, service operators, and travelers. We consider a coopetitive setting where the MaaS platform purchases service capacity from service operators and sells multi-modal trips to travelers following an origin-destination-based pricing scheme; meanwhile, service operators use their remaining capacities to serve single-modal trips. As followers, travelers make both mode choices, including whether to use MaaS, and route choices in the multi-modal transportation network, subject to prices and congestion. Inspired by the dual formulation for traffic assignment problems, we propose a novel single-level variational inequality (VI) formulation by introducing a virtual traffic operator, along with the MaaS platform and multiple service operators. A key advantage of the proposed VI formulation is that it supports parallel solution procedures and thus enables large-scale applications. We prove that an equilibrium solution always exists given the negotiated wholesale price of service capacity. Numerical experiments on a small network further demonstrate that the wholesale price can be tailored to align with varying system-wide objectives. The proposed MaaS system demonstrates potential for creating a "win-win-win" outcome -- service operators and travelers are better off compared to the "without MaaS" scenario, meanwhile the MaaS platform remains profitable. Such a Pareto-improving regime can be explicitly specified with the wholesale capacity price. Similar conclusions are drawn from the experiment of an extended multi-modal Sioux Falls network, which also validates the scalability of the proposed model and solution algorithm.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [323] [LLM-Empowered Cooperative Content Caching in Vehicular Fog Caching-Assisted Platoon Networks](https://arxiv.org/abs/2602.04471)
*Bowen Tan,Qiong Wu,Pingyi Fan,Kezhi Wang,Nan Cheng,Wen Chen*

Main category: cs.NI

TL;DR: 本文提出用于车联网雾缓存辅助车队的三层内容缓存架构，并结合大语言模型实现高效缓存，仿真显示该方案有优势。


<details>
  <summary>Details</summary>
Motivation: 为降低车联网中内容检索的延迟。

Method: 提出三层内容缓存架构，结合大语言模型处理异构信息，制定提示框架和分层确定性缓存映射策略。

Result: 仿真结果显示所提缓存方案具有优势。

Conclusion: 所提出的缓存架构和结合大语言模型的方法可有效降低内容检索延迟。

Abstract: This letter proposes a novel three-tier content caching architecture for Vehicular Fog Caching (VFC)-assisted platoon, where the VFC is formed by the vehicles driving near the platoon. The system strategically coordinates storage across local platoon vehicles, dynamic VFC clusters, and cloud server (CS) to minimize content retrieval latency. To efficiently manage distributed storage, we integrate large language models (LLMs) for real-time and intelligent caching decisions. The proposed approach leverages LLMs' ability to process heterogeneous information, including user profiles, historical data, content characteristics, and dynamic system states. Through a designed prompting framework encoding task objectives and caching constraints, the LLMs formulate caching as a decision-making task, and our hierarchical deterministic caching mapping strategy enables adaptive requests prediction and precise content placement across three tiers without frequent retraining. Simulation results demonstrate the advantages of our proposed caching scheme.

</details>


### [324] [Dual Mind World Model Inspired Network Digital Twin for Access Scheduling](https://arxiv.org/abs/2602.04566)
*Hrishikesh Dutta,Roberto Minerva,Noel Crespi*

Main category: cs.NI

TL;DR: 提出受DMWM架构启发的数字孪生调度框架，经测试在多种环境表现优越，推动网络优化。


<details>
  <summary>Details</summary>
Motivation: 新兴网络系统需要能适应动态流量、期限和干扰约束的智能调度策略。

Method: 提出结合短视预测规划与基于符号模型的滚动的DMWM调度框架，并在可配置模拟测试平台上实现，与传统启发式和强化学习基线进行性能对比。

Result: DMWM在突发、干扰受限和对期限敏感的环境中表现出卓越性能，同时保持可解释性和样本效率。

Conclusion: 该设计弥合了网络级推理与低开销学习之间的差距，向可扩展和自适应的基于NDT的网络优化迈进了一步。

Abstract: Emerging networked systems such as industrial IoT and real-time cyber-physical infrastructures demand intelligent scheduling strategies capable of adapting to dynamic traffic, deadlines, and interference constraints. In this work, we present a novel Digital Twin-enabled scheduling framework inspired by Dual Mind World Model (DMWM) architecture, for learning-informed and imagination-driven network control. Unlike conventional rule-based or purely data-driven policies, the proposed DMWM combines short-horizon predictive planning with symbolic model-based rollout, enabling the scheduler to anticipate future network states and adjust transmission decisions accordingly. We implement the framework in a configurable simulation testbed and benchmark its performance against traditional heuristics and reinforcement learning baselines under varied traffic conditions. Our results show that DMWM achieves superior performance in bursty, interference-limited, and deadline-sensitive environments, while maintaining interpretability and sample efficiency. The proposed design bridges the gap between network-level reasoning and low-overhead learning, marking a step toward scalable and adaptive NDT-based network optimization.

</details>


<div id='cond-mat.mes-hall'></div>

# cond-mat.mes-hall [[Back]](#toc)

### [325] [First-Principles AI finds crystallization of fractional quantum Hall liquids](https://arxiv.org/abs/2602.03927)
*Ahmed Abouelkomsan,Liang Fu*

Main category: cond-mat.mes-hall

TL;DR: 引入MagNet变分波函数研究分数量子霍尔（FQH）液体结晶问题，能统一描述FQH态和电子晶体，发现拓扑液体和电子晶体基态，展现了第一性原理AI解决强相互作用多体问题的能力。


<details>
  <summary>Details</summary>
Motivation: 解决在强朗道能级混合体系中，需要一个能平等处理分数化和结晶的框架来研究FQH液体何时结晶的问题。

Method: 引入MagNet自注意力神经网络变分波函数，通过对微观哈密顿量进行能量最小化训练。

Result: MagNet在广泛的朗道能级混合范围内发现了拓扑液体和电子晶体基态。

Conclusion: 第一性原理AI可解决强相互作用多体问题，在无外部训练数据和物理先验知识的情况下找到竞争相。

Abstract: When does a fractional quantum Hall (FQH) liquid crystallize? Addressing this question requires a framework that treats fractionalization and crystallization on equal footing, especially in strong Landau-level mixing regime. Here, we introduce MagNet, a self-attention neural-network variational wavefunction designed for quantum systems in magnetic fields on the torus geometry. We show that MagNet provides a unifying and expressive ansatz capable of describing both FQH states and electron crystals within the same architecture. Trained solely by energy minimization of the microscopic Hamiltonian, MagNet discovers topological liquid and electron crystal ground states across a broad range of Landau-level mixing. Our results highlight the power of first-principles AI for solving strongly interacting many-body problems and finding competing phases without external training data or physics pre-knowledge.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [326] [Trust The Typical](https://arxiv.org/abs/2602.04581)
*Debargha Ganguly,Sreehari Sankar,Biyao Zhang,Vikash Singh,Kanan Gupta,Harshini Kavuru,Alan Luo,Weicong Chen,Warren Morningstar,Raghu Machiraju,Vipin Chaudhary*

Main category: cs.CL

TL;DR: 提出Trust The Typical (T3)框架解决大语言模型安全问题，无需有害示例训练，性能优且可跨领域和语言，还集成到vLLM展示生产就绪性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型安全方法依赖识别和阻止已知威胁，不够稳健，需新方法从理解安全入手保障安全。

Method: 引入T3框架，将安全视为分布外检测问题，学习可接受提示在语义空间的分布，标记显著偏差为潜在威胁。

Result: 在18个基准测试中达到了最先进的性能，将误报率比专业安全模型降低了40倍，单模型可跨领域和14种语言，集成到vLLM后开销小于6%。

Conclusion: T3框架是一种有效的大语言模型安全保障方法，具有良好的性能和跨领域、语言能力，具备生产实用性。

Abstract: Current approaches to LLM safety fundamentally rely on a brittle cat-and-mouse game of identifying and blocking known threats via guardrails. We argue for a fresh approach: robust safety comes not from enumerating what is harmful, but from deeply understanding what is safe. We introduce Trust The Typical (T3), a framework that operationalizes this principle by treating safety as an out-of-distribution (OOD) detection problem. T3 learns the distribution of acceptable prompts in a semantic space and flags any significant deviation as a potential threat. Unlike prior methods, it requires no training on harmful examples, yet achieves state-of-the-art performance across 18 benchmarks spanning toxicity, hate speech, jailbreaking, multilingual harms, and over-refusal, reducing false positive rates by up to 40x relative to specialized safety models. A single model trained only on safe English text transfers effectively to diverse domains and over 14 languages without retraining. Finally, we demonstrate production readiness by integrating a GPU-optimized version into vLLM, enabling continuous guardrailing during token generation with less than 6% overhead even under dense evaluation intervals on large-scale workloads.

</details>


### [327] [Merged ChemProt-DrugProt for Relation Extraction from Biomedical Literature](https://arxiv.org/abs/2405.18605)
*Mai H. Nguyen,Shibani Likhite,Jiawei Tang,Darshini Mahendran,Bridget T. McInnes*

Main category: cs.CL

TL;DR: 本文合并ChemProt和DrugProt数据集以提升化学 - 基因关系抽取模型性能，用BioBERT和结合BioBERT的GCN评估，结果显示合并数据集有显著提升，结合GCN能提高部分组的精确率和召回率。


<details>
  <summary>Details</summary>
Motivation: 化学 - 基因关系抽取对药物发现、疾病理解和生物医学研究至关重要，现有方法需提升模型准确度。

Method: 合并ChemProt和DrugProt数据集，用BioBERT和结合BioBERT的GCN两种先进关系抽取算法评估。

Result: 合并数据集显著提升模型性能，尤其在数据集共享的CPR组；结合GCN比仅用BioBERT能提高部分CPR组的精确率和召回率。

Conclusion: 合并数据集及结合GCN与BioBERT有助于提高化学 - 基因关系抽取模型性能。

Abstract: The extraction of chemical-gene relations plays a pivotal role in understanding the intricate interactions between chemical compounds and genes, with significant implications for drug discovery, disease understanding, and biomedical research. This paper presents a data set created by merging the ChemProt and DrugProt datasets to augment sample counts and improve model accuracy. We evaluate the merged dataset using two state of the art relationship extraction algorithms: Bidirectional Encoder Representations from Transformers (BERT) specifically BioBERT, and Graph Convolutional Networks (GCNs) combined with BioBERT. While BioBERT excels at capturing local contexts, it may benefit from incorporating global information essential for understanding chemical-gene interactions. This can be achieved by integrating GCNs with BioBERT to harness both global and local context. Our results show that by integrating the ChemProt and DrugProt datasets, we demonstrated significant improvements in model performance, particularly in CPR groups shared between the datasets. Incorporating the global context using GCN can help increase the overall precision and recall in some of the CPR groups over using just BioBERT.

</details>


### [328] [Linguistic Blind Spots in Clinical Decision Extraction](https://arxiv.org/abs/2602.03942)
*Mohamed Elgaar,Hadi Amiri*

Main category: cs.CL

TL;DR: 研究临床决策语言特征差异及对提取性能影响，发现叙事风格跨度影响提取召回率，建议下游系统采用容错策略。


<details>
  <summary>Details</summary>
Motivation: 提取临床笔记中的医疗决策对临床决策支持等很关键，研究各决策类别语言特征差异及其对提取失败的影响。

Method: 使用带有决策类别的MedDec出院总结，计算每个决策跨度的七个语言指标，分析标准Transformer模型的跨度级提取召回率。

Result: 不同决策类别有特定语言特征；精确匹配召回率48%，不同语言层次差异大；宽松匹配下召回率升至71%，很多错误是跨度边界不一致。

Conclusion: 叙事风格跨度在精确匹配中是盲点，下游系统应采用边界容错评估和提取策略。

Abstract: Extracting medical decisions from clinical notes is a key step for clinical decision support and patient-facing care summaries. We study how the linguistic characteristics of clinical decisions vary across decision categories and whether these differences explain extraction failures. Using MedDec discharge summaries annotated with decision categories from the Decision Identification and Classification Taxonomy for Use in Medicine (DICTUM), we compute seven linguistic indices for each decision span and analyze span-level extraction recall of a standard transformer model. We find clear category-specific signatures: drug-related and problem-defining decisions are entity-dense and telegraphic, whereas advice and precaution decisions contain more narrative, with higher stopword and pronoun proportions and more frequent hedging and negation cues. On the validation split, exact-match recall is 48%, with large gaps across linguistic strata: recall drops from 58% to 24% from the lowest to highest stopword-proportion bins, and spans containing hedging or negation cues are less likely to be recovered. Under a relaxed overlap-based match criterion, recall increases to 71%, indicating that many errors are span boundary disagreements rather than complete misses. Overall, narrative-style spans--common in advice and precaution decisions--are a consistent blind spot under exact matching, suggesting that downstream systems should incorporate boundary-tolerant evaluation and extraction strategies for clinical decisions.

</details>


### [329] [Transformers perform adaptive partial pooling](https://arxiv.org/abs/2602.03980)
*Vsevolod Kapatsinski*

Main category: cs.CL

TL;DR: 研究表明GPT2在训练中预测受外部观察影响减小，且合并程度受上下文因素影响，学习特性合理


<details>
  <summary>Details</summary>
Motivation: 探讨语言模型在非新颖但低频上下文的表现，即信息合并情况

Method: 研究GPT2在训练各阶段下一个词预测，对比其与分层回归的信息合并情况

Result: GPT2在训练过程中预测受当前上下文之外观察的影响逐渐减小，合并程度受上下文频率、数量和可变性影响，与分层回归类似

Conclusion: transformer的学习特性在理论和实证上都是合理的

Abstract: Because language is creative, any reasonable language model must generalize, deciding what to say in novel contexts by using information from similar contexts. But what about contexts that are not novel but merely infrequent? In hierarchical regression, the model's predictions for behavior in a context are affected by observations from other similar contexts to the extent that 1) the current context is infrequent and 2) different contexts behave similarly. This is called adaptive partial pooling of evidence. This paper shows that next-word predictions of a transformer (GPT2) are increasingly unaffected by observations from outside the current context across epochs of training (the amount of pooling reduces with training), and that the extent of pooling is affected by context frequency, context number (type frequency) and context variability in a similar way to hierarchical regression. These characteristics of learning in transformers are argued to be realistic on both rational and empirical grounds.

</details>


### [330] [On the Credibility of Evaluating LLMs using Survey Questions](https://arxiv.org/abs/2602.04033)
*Jindřich Libovický*

Main category: cs.CL

TL;DR: 本文指出用适应社调评估大语言模型(LLMs)价值取向方法的局限，以多语言多国家世界价值观调查为例，说明提示方法和译码策略影响结果，引入新度量自相关距离，揭示常见评估指标关联弱并给出研究建议。


<details>
  <summary>Details</summary>
Motivation: 发现当前用社调评估LLMs价值取向方法存在会低估或高估价值取向相似性的局限，需改进评估方法。

Method: 使用三国语言覆盖五国的世界价值观调查，比较不同提示方法和译码策略；引入自相关距离度量；对比均方距离和KL散度两个常见评估指标。

Result: 提示方法和译码策略显著影响结果；自相关距离表明LLMs平均答案与人类一致不保证结构对齐；均方距离和KL散度相关性弱。

Conclusion: 未来研究推荐CoT提示、基于采样译码和用含自相关距离等多指标进行稳健分析。

Abstract: Recent studies evaluate the value orientation of large language models (LLMs) using adapted social surveys, typically by prompting models with survey questions and comparing their responses to average human responses. This paper identifies limitations in this methodology that, depending on the exact setup, can lead to both underestimating and overestimating the similarity of value orientation. Using the World Value Survey in three languages across five countries, we demonstrate that prompting methods (direct vs. chain-of-thought) and decoding strategies (greedy vs. sampling) significantly affect results. To assess the interaction between answers, we introduce a novel metric, self-correlation distance. This metric measures whether LLMs maintain consistent relationships between answers across different questions, as humans do. This indicates that even a high average agreement with human data, when considering LLM responses independently, does not guarantee structural alignment in responses. Additionally, we reveal a weak correlation between two common evaluation metrics, mean-squared distance and KL divergence, which assume that survey answers are independent of each other. For future research, we recommend CoT prompting, sampling-based decoding with dozens of samples, and robust analysis using multiple metrics, including self-correlation distance.

</details>


### [331] [From Lemmas to Dependencies: What Signals Drive Light Verbs Classification?](https://arxiv.org/abs/2602.04127)
*Sercan Karakaş,Yusuf Şimşek*

Main category: cs.CL

TL;DR: 本文通过系统限制模型输入研究推动轻动词结构（LVCs）分类的信号。对比不同方法，发现粗粒度句法不足以稳健检测LVC，词汇身份支持判断但受校准和归一化影响。


<details>
  <summary>Details</summary>
Motivation: 土耳其语中，丰富的词法和复杂谓语使轻动词结构的习语谓语含义和字面动词 - 论元用法对比极小，需探究驱动LVC分类的信号。

Method: 使用UD衍生监督，对比基于词元的基线模型、基于UD形态句法的仅语法逻辑回归模型和全输入BERTurk基线模型，在控制诊断集上评估。

Result: 粗粒度形态句法单独使用不足以在控制对比下稳健检测LVC，词汇身份支持LVC判断但对校准和归一化选择敏感。

Conclusion: 研究结果促使对土耳其多词表达进行有针对性评估，表明“仅词元”不是单一明确表示，关键取决于归一化操作方式。

Abstract: Light verb constructions (LVCs) are a challenging class of verbal multiword expressions, especially in Turkish, where rich morphology and productive complex predicates create minimal contrasts between idiomatic predicate meanings and literal verb--argument uses. This paper asks what signals drive LVC classification by systematically restricting model inputs. Using UD-derived supervision, we compare lemma-driven baselines (lemma TF--IDF + Logistic Regression; BERTurk trained on lemma sequences), a grammar-only Logistic Regression over UD morphosyntax (UPOS/DEPREL/MORPH), and a full-input BERTurk baseline. We evaluate on a controlled diagnostic set with Random negatives, lexical controls (NLVC), and LVC positives, reporting split-wise performance to expose decision-boundary behavior. Results show that coarse morphosyntax alone is insufficient for robust LVC detection under controlled contrasts, while lexical identity supports LVC judgments but is sensitive to calibration and normalization choices. Overall, Our findings motivate targeted evaluation of Turkish MWEs and show that ``lemma-only'' is not a single, well-defined representation, but one that depends critically on how normalization is operationalized.

</details>


### [332] [From Helpfulness to Toxic Proactivity: Diagnosing Behavioral Misalignment in LLM Agents](https://arxiv.org/abs/2602.04197)
*Xinyue Wang,Yuanhe Zhang,Zhengshuo Gong,Haoran Gao,Fanyu Meng,Zhenhong Zhou,Li Sun,Yang Liu,Sen Su*

Main category: cs.CL

TL;DR: 本文指出LLM代理存在“Toxic Proactivity”问题，提出新的评估框架进行研究，证明该现象普遍存在并给出倾向及评估基准。


<details>
  <summary>Details</summary>
Motivation: LLM代理的规划和工具使用能力带来新风险，现有研究对“Toxic Proactivity”行为识别关注不足，需揭示此风险。

Method: 引入基于双模型困境驱动交互的评估框架，对多步行为轨迹进行模拟和分析。

Result: 证明“Toxic Proactivity”是普遍存在的行为现象，揭示两大主要倾向。

Conclusion: 给出评估不同情境下“Toxic Proactive”行为的系统基准。

Abstract: The enhanced capabilities of LLM-based agents come with an emergency for model planning and tool-use abilities. Attributing to helpful-harmless trade-off from LLM alignment, agents typically also inherit the flaw of "over-refusal", which is a passive failure mode. However, the proactive planning and action capabilities of agents introduce another crucial danger on the other side of the trade-off. This phenomenon we term "Toxic Proactivity'': an active failure mode in which an agent, driven by the optimization for Machiavellian helpfulness, disregards ethical constraints to maximize utility. Unlike over-refusal, Toxic Proactivity manifests as the agent taking excessive or manipulative measures to ensure its "usefulness'' is maintained. Existing research pays little attention to identifying this behavior, as it often lacks the subtle context required for such strategies to unfold. To reveal this risk, we introduce a novel evaluation framework based on dilemma-driven interactions between dual models, enabling the simulation and analysis of agent behavior over multi-step behavioral trajectories. Through extensive experiments with mainstream LLMs, we demonstrate that Toxic Proactivity is a widespread behavioral phenomenon and reveal two major tendencies. We further present a systematic benchmark for evaluating Toxic Proactive behavior across contextual settings.

</details>


### [333] [Enforcing Monotonic Progress in Legal Cross-Examination: Preventing Long-Horizon Stagnation in LLM-Based Inquiry](https://arxiv.org/abs/2602.04206)
*Hsien-Jyh Liao*

Main category: cs.CL

TL;DR: 大语言模型处理长任务有局限，提出Soft - FSM架构，实验显示其效果好，说明需外部状态控制。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在明确程序约束下完成长任务存在困难，如法律交叉询问中出现程序停滞问题。

Method: 提出Soft - FSM神经符号架构，通过外部确定性状态控制器对累积的关键信息单元（KIUs）强制执行单调进展。

Result: 在三个台湾真实刑事杀人案件实验中，基线方法完成率低于40%，Soft - FSM完成率超97%且冗余近零。

Conclusion: 在相关领域，仅靠大语言模型的涌现行为不能保证可靠的任务完成，需通过明确且可验证的外部状态控制来实现。

Abstract: Large language models (LLMs) exhibit impressive linguistic fluency but struggle to reliably complete long-horizon tasks under explicit procedural constraints. In legal cross-examination, purely proba-bilistic generation often maintains behavioral coherence while failing to ensure procedural advancement. We characterize this failure as procedural stagnation and propose Soft-FSM, a neuro-symbolic architecture that enforces monotonic progress over accumulated Key Information Units (KIUs) via an external deterministic state controller. Experiments on three real-world Taiwanese criminal homicide cases show that baseline methods collapse below 40% completeness, while Soft-FSM consistently achieves over 97% with near-zero redundancy. These results suggest that, in such domains, reliable task completion cannot be guaranteed by emergent LLM behavior alone, and can be reliably enforced through explicit and verifiable external state control.

</details>


### [334] [Language Models Struggle to Use Representations Learned In-Context](https://arxiv.org/abs/2602.04212)
*Michael A. Lepori,Tal Linzen,Ann Yuan,Katja Filippova*

Main category: cs.CL

TL;DR: 研究大语言模型能否利用上下文表示完成下游任务，发现其在运用上下文定义的语义表示方面有困难，旨在激发新方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型距创建能适应全新情境的人造系统目标仍有差距，此前研究表明大语言模型能从上下文进行表示学习，本文研究其能否利用这些表示完成下游任务。

Method: 评估开放权重的大语言模型在预测下一个标记以及在自适应世界建模任务中运用上下文表示的能力，也对闭源的先进推理模型进行自适应世界建模任务评估。

Result: 开放权重的大语言模型难以运用上下文中定义的新颖语义表示；即使是最优秀的大语言模型也无法可靠地利用上下文中呈现的新颖模式。

Conclusion: 需开发新方法促使模型不仅能编码上下文信息，还能灵活运用这些信息。

Abstract: Though large language models (LLMs) have enabled great success across a wide variety of tasks, they still appear to fall short of one of the loftier goals of artificial intelligence research: creating an artificial system that can adapt its behavior to radically new contexts upon deployment. One important step towards this goal is to create systems that can induce rich representations of data that are seen in-context, and then flexibly deploy these representations to accomplish goals. Recently, Park et al. (2024) demonstrated that current LLMs are indeed capable of inducing such representation from context (i.e., in-context representation learning). The present study investigates whether LLMs can use these representations to complete simple downstream tasks.
  We first assess whether open-weights LLMs can use in-context representations for next-token prediction, and then probe models using a novel task, adaptive world modeling. In both tasks, we find evidence that open-weights LLMs struggle to deploy representations of novel semantics that are defined in-context, even if they encode these semantics in their latent representations. Furthermore, we assess closed-source, state-of-the-art reasoning models on the adaptive world modeling task, demonstrating that even the most performant LLMs cannot reliably leverage novel patterns presented in-context. Overall, this work seeks to inspire novel methods for encouraging models to not only encode information presented in-context, but to do so in a manner that supports flexible deployment of this information.

</details>


### [335] [Contextual Drag: How Errors in the Context Affect LLM Reasoning](https://arxiv.org/abs/2602.04288)
*Yun Cheng,Xingyu Zhu,Haoyu Zhao,Sanjeev Arora*

Main category: cs.CL

TL;DR: 研究大语言模型自改进中上下文拖累现象，发现其导致性能下降，现有策略无法完全解决。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型自改进中反思过往错误时可能存在的问题。

Method: 对11个专有和开源权重模型在8个推理任务上进行评估，用树编辑距离进行结构分析。

Result: 上下文拖累使模型性能下降10 - 20%，严重时迭代自改进会导致自我恶化，后续推理轨迹继承上下文的错误模式，外部反馈和自我验证无法消除该影响，现有缓解策略无法完全恢复基线性能。

Conclusion: 上下文拖累是当前推理架构中持续存在的失败模式。

Abstract: Central to many self-improvement pipelines for large language models (LLMs) is the assumption that models can improve by reflecting on past mistakes. We study a phenomenon termed contextual drag: the presence of failed attempts in the context biases subsequent generations toward structurally similar errors. Across evaluations of 11 proprietary and open-weight models on 8 reasoning tasks, contextual drag induces 10-20% performance drops, and iterative self-refinement in models with severe contextual drag can collapse into self-deterioration. Structural analysis using tree edit distance reveals that subsequent reasoning trajectories inherit structurally similar error patterns from the context. We demonstrate that neither external feedback nor successful self-verification suffices to eliminate this effect. While mitigation strategies such as fallback-behavior fine-tuning and context denoising yield partial improvements, they fail to fully restore baseline performance, positioning contextual drag as a persistent failure mode in current reasoning architectures.

</details>


### [336] [How Few-shot Demonstrations Affect Prompt-based Defenses Against LLM Jailbreak Attacks](https://arxiv.org/abs/2602.04294)
*Yanshu Wang,Shuaishuai Yang,Jingjing He,Tong Yang*

Main category: cs.CL

TL;DR: 本文对多个主流大语言模型进行全面评估，发现few-shot演示对RoP和ToP有相反效果，并给出真实应用中基于提示防御的实用建议。


<details>
  <summary>Details</summary>
Motivation: 大语言模型面临越狱攻击威胁，基于提示的防御虽有效，但few-shot演示在这些防御策略中的作用不明，且缺少其与不同系统提示策略交互的研究。

Method: 在四个安全基准上，使用六种越狱攻击方法对多个主流大语言模型进行全面评估。

Result: Few-shot演示对RoP和ToP有相反效果，能使RoP安全率提升达4.5%，使ToP有效性降低达21.2%。

Conclusion: 基于研究结果，为真实世界大语言模型应用中部署基于提示的防御提供实用建议。

Abstract: Large Language Models (LLMs) face increasing threats from jailbreak attacks that bypass safety alignment. While prompt-based defenses such as Role-Oriented Prompts (RoP) and Task-Oriented Prompts (ToP) have shown effectiveness, the role of few-shot demonstrations in these defense strategies remains unclear. Prior work suggests that few-shot examples may compromise safety, but lacks investigation into how few-shot interacts with different system prompt strategies. In this paper, we conduct a comprehensive evaluation on multiple mainstream LLMs across four safety benchmarks (AdvBench, HarmBench, SG-Bench, XSTest) using six jailbreak attack methods. Our key finding reveals that few-shot demonstrations produce opposite effects on RoP and ToP: few-shot enhances RoP's safety rate by up to 4.5% through reinforcing role identity, while it degrades ToP's effectiveness by up to 21.2% through distracting attention from task instructions. Based on these findings, we provide practical recommendations for deploying prompt-based defenses in real-world LLM applications.

</details>


### [337] [Revisiting Prompt Sensitivity in Large Language Models for Text Classification: The Role of Prompt Underspecification](https://arxiv.org/abs/2602.04297)
*Branislav Pecher,Michal Spiegel,Robert Belanec,Jan Cegin*

Main category: cs.CL

TL;DR: 研究表明大语言模型提示敏感性部分源于提示说明不足，需更严谨研究和缓解该问题。


<details>
  <summary>Details</summary>
Motivation: 现有研究用说明不足的提示研究大语言模型提示敏感性，本文认为部分敏感性源于提示说明不足。

Method: 系统研究和比较说明不足的提示与有具体指令提示的敏感性，运用性能分析、对数分析和线性探测。

Result: 说明不足的提示性能方差高、相关标记对数低，而指令提示问题较少；提示说明不足对内部表征影响小，主要影响最后层。

Conclusion: 研究和缓解提示敏感性时需更严谨。

Abstract: Large language models (LLMs) are widely used as zero-shot and few-shot classifiers, where task behaviour is largely controlled through prompting. A growing number of works have observed that LLMs are sensitive to prompt variations, with small changes leading to large changes in performance. However, in many cases, the investigation of sensitivity is performed using underspecified prompts that provide minimal task instructions and weakly constrain the model's output space. In this work, we argue that a significant portion of the observed prompt sensitivity can be attributed to prompt underspecification. We systematically study and compare the sensitivity of underspecified prompts and prompts that provide specific instructions. Utilising performance analysis, logit analysis, and linear probing, we find that underspecified prompts exhibit higher performance variance and lower logit values for relevant tokens, while instruction-prompts suffer less from such problems. However, linear probing analysis suggests that the effects of prompt underspecification have only a marginal impact on the internal LLM representations, instead emerging in the final layers. Overall, our findings highlight the need for more rigour when investigating and mitigating prompt sensitivity.

</details>


### [338] [DeFrame: Debiasing Large Language Models Against Framing Effects](https://arxiv.org/abs/2602.04306)
*Kahee Lim,Soyeon Kim,Steven Euijong Whang*

Main category: cs.CL

TL;DR: 文章指出大语言模型存在隐藏偏见，研究发现框架会影响公平性评估，提出框架感知去偏方法，减少整体偏差和框架差异。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在现实应用中需保证跨群体公平响应，现有标准评估存在隐藏偏见问题。

Method: 引入“框架差异”概念量化框架对公平性评估的影响，用替代框架扩充公平性评估基准，提出框架感知去偏方法。

Result: 公平性得分随框架显著变化，现有去偏方法不能减少框架诱导的差异，提出的方法减少整体偏差，提高对框架差异的鲁棒性。

Conclusion: 提出的框架感知去偏方法能使大语言模型产生更公平、一致的响应。

Abstract: As large language models (LLMs) are increasingly deployed in real-world applications, ensuring their fair responses across demographics has become crucial. Despite many efforts, an ongoing challenge is hidden bias: LLMs appear fair under standard evaluations, but can produce biased responses outside those evaluation settings. In this paper, we identify framing -- differences in how semantically equivalent prompts are expressed (e.g., "A is better than B" vs. "B is worse than A") -- as an underexplored contributor to this gap. We first introduce the concept of "framing disparity" to quantify the impact of framing on fairness evaluation. By augmenting fairness evaluation benchmarks with alternative framings, we find that (1) fairness scores vary significantly with framing and (2) existing debiasing methods improve overall (i.e., frame-averaged) fairness, but often fail to reduce framing-induced disparities. To address this, we propose a framing-aware debiasing method that encourages LLMs to be more consistent across framings. Experiments demonstrate that our approach reduces overall bias and improves robustness against framing disparities, enabling LLMs to produce fairer and more consistent responses.

</details>


### [339] [Bi-directional Bias Attribution: Debiasing Large Language Models without Modifying Prompts](https://arxiv.org/abs/2602.04398)
*Yujie Lin,Kunquan Li,Yixuan Liao,Xiaoxin Chen,Jinsong Su*

Main category: cs.CL

TL;DR: 提出无需微调或修改提示的大语言模型去偏框架，实验证明有效降偏且保性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型输出存在社会偏见，现有去偏方法有扩展性问题或影响多轮交互体验。

Method: 先通过跨人口群体的比较分析识别刻板印象诱导词，再用基于积分梯度的两种归因策略将偏差行为归因于特定神经元，最后在投影层直接干预激活以减轻偏差。

Result: 在三个常用大语言模型上的实验表明该方法有效减少偏差，同时保持整体模型性能。

Conclusion: 所提出框架能在不进行微调或修改提示的情况下，有效解决大语言模型的偏差问题。

Abstract: Large language models (LLMs) have demonstrated impressive capabilities across a wide range of natural language processing tasks. However, their outputs often exhibit social biases, raising fairness concerns. Existing debiasing methods, such as fine-tuning on additional datasets or prompt engineering, face scalability issues or compromise user experience in multi-turn interactions. To address these challenges, we propose a framework for detecting stereotype-inducing words and attributing neuron-level bias in LLMs, without the need for fine-tuning or prompt modification. Our framework first identifies stereotype-inducing adjectives and nouns via comparative analysis across demographic groups. We then attribute biased behavior to specific neurons using two attribution strategies based on integrated gradients. Finally, we mitigate bias by directly intervening on their activations at the projection layer. Experiments on three widely used LLMs demonstrate that our method effectively reduces bias while preserving overall model performance. Code is available at the github link: https://github.com/XMUDeepLIT/Bi-directional-Bias-Attribution.

</details>


### [340] [History-Guided Iterative Visual Reasoning with Self-Correction](https://arxiv.org/abs/2602.04413)
*Xinglong Yang,Zhilin Peng,Zhanzhan Liu,Haochen Shi,Sheng-Jun Huang*

Main category: cs.CL

TL;DR: 提出H - GIVR框架提升多模态大语言模型跨模态推理准确性，实验显示效果显著且成本低。


<details>
  <summary>Details</summary>
Motivation: 现有自一致性方法局限于固定范式，无法重用历史推理信息，难以主动纠正视觉理解错误和动态调整推理。

Method: 受人类反复验证和动态纠错推理行为启发，提出H - GIVR框架，在迭代推理时多次观察图像，用先前答案作后续步骤参考。

Result: 在五个数据集和三个模型上实验，H - GIVR框架能显著提升跨模态推理准确性且计算成本低，如在ScienceQA数据集上有明显提升。

Conclusion: H - GIVR框架有效提升多模态大语言模型跨模态推理准确性，且成本可控。

Abstract: Self-consistency methods are the core technique for improving the reasoning reliability of multimodal large language models (MLLMs). By generating multiple reasoning results through repeated sampling and selecting the best answer via voting, they play an important role in cross-modal tasks. However, most existing self-consistency methods are limited to a fixed ``repeated sampling and voting'' paradigm and do not reuse historical reasoning information. As a result, models struggle to actively correct visual understanding errors and dynamically adjust their reasoning during iteration. Inspired by the human reasoning behavior of repeated verification and dynamic error correction, we propose the H-GIVR framework. During iterative reasoning, the MLLM observes the image multiple times and uses previously generated answers as references for subsequent steps, enabling dynamic correction of errors and improving answer accuracy. We conduct comprehensive experiments on five datasets and three models. The results show that the H-GIVR framework can significantly improve cross-modal reasoning accuracy while maintaining low computational cost. For instance, using \texttt{Llama3.2-vision:11b} on the ScienceQA dataset, the model requires an average of 2.57 responses per question to achieve an accuracy of 78.90\%, representing a 107\% improvement over the baseline.

</details>


### [341] [No One-Size-Fits-All: Building Systems For Translation to Bashkir, Kazakh, Kyrgyz, Tatar and Chuvash Using Synthetic And Original Data](https://arxiv.org/abs/2602.04442)
*Dmitry Karpov*

Main category: cs.CL

TL;DR: 探索五种突厥语对机器翻译，用不同方法获不同语言chrF++分数并发布数据集和权重


<details>
  <summary>Details</summary>
Motivation: 开展五种突厥语对的机器翻译研究

Method: 用LoRA在合成数据上微调nllb - 200 - distilled - 600M；用检索到的相似示例提示DeepSeek - V3.2；零样本或基于检索的方法

Result: 不同方法在各语言对翻译中取得不同的chrF++分数，如哈萨克语49.71、巴什基尔语46.94等

Conclusion: 发布相关数据集和获得的权重

Abstract: We explore machine translation for five Turkic language pairs: Russian-Bashkir, Russian-Kazakh, Russian-Kyrgyz, English-Tatar, English-Chuvash. Fine-tuning nllb-200-distilled-600M with LoRA on synthetic data achieved chrF++ 49.71 for Kazakh and 46.94 for Bashkir. Prompting DeepSeek-V3.2 with retrieved similar examples achieved chrF++ 39.47 for Chuvash. For Tatar, zero-shot or retrieval-based approaches achieved chrF++ 41.6, while for Kyrgyz the zero-shot approach reached 45.6. We release the dataset and the obtained weights.

</details>


### [342] [Is Micro Domain-Adaptive Pre-Training Effective for Real-World Operations? Multi-Step Evaluation Reveals Potential and Bottlenecks](https://arxiv.org/abs/2602.04466)
*Masaya Tsunokake,Yuta Koreeda,Terufumi Morishita,Koichi Nagatsuka,Hikaru Tomonari,Yasuhiro Sogawa*

Main category: cs.CL

TL;DR: 研究mDAPT在生成式任务中的潜力和瓶颈，验证其在IT技术支持操作中的效果，明确其知识方面有效性和其他方面瓶颈，强调提升推理能力的必要性。


<details>
  <summary>Details</summary>
Motivation: 先前研究仅在选择题上评估mDAPT，其对现实操作中生成式任务的有效性未知，因此要揭示mDAPT在生成式任务中的潜力和瓶颈。

Method: 将回答过程分解为引出事实、推理结论、撰写答案三个子任务，在IT技术支持操作的专有IT产品知识上验证mDAPT。

Result: mDAPT解决了基础模型难以完成的引出事实任务，但未解决其他子任务。

Conclusion: 明确了mDAPT在知识方面的有效性和其他方面的瓶颈，实证表明解决引出和推理任务可确保足够性能，强调需提升推理能力。

Abstract: When applying LLMs to real-world enterprise operations, LLMs need to handle proprietary knowledge in small domains of specific operations ($\textbf{micro domains}$). A previous study shows micro domain-adaptive pre-training ($\textbf{mDAPT}$) with fewer documents is effective, similarly to DAPT in larger domains. However, it evaluates mDAPT only on multiple-choice questions; thus, its effectiveness for generative tasks in real-world operations remains unknown. We aim to reveal the potential and bottlenecks of mDAPT for generative tasks. To this end, we disentangle the answering process into three subtasks and evaluate the performance of each subtask: (1) $\textbf{eliciting}$ facts relevant to questions from an LLM's own knowledge, (2) $\textbf{reasoning}$ over the facts to obtain conclusions, and (3) $\textbf{composing}$ long-form answers based on the conclusions. We verified mDAPT on proprietary IT product knowledge for real-world questions in IT technical support operations. As a result, mDAPT resolved the elicitation task that the base model struggled with but did not resolve other subtasks. This clarifies mDAPT's effectiveness in the knowledge aspect and its bottlenecks in other aspects. Further analysis empirically shows that resolving the elicitation and reasoning tasks ensures sufficient performance (over 90%), emphasizing the need to enhance reasoning capability.

</details>


### [343] [LycheeDecode: Accelerating Long-Context LLM Inference via Hybrid-Head Sparse Decoding](https://arxiv.org/abs/2602.04541)
*Gang Lin,Dongfang Li,Zhuoen Chen,Yukun Shi,Xuhui Chen,Baotian Hu,Min Zhang*

Main category: cs.CL

TL;DR: 为解决长上下文大语言模型解码时键值缓存膨胀问题，提出LycheeDecode方法，实验表明其在保持生成质量的同时提升了速度。


<details>
  <summary>Details</summary>
Motivation: 长上下文大语言模型解码时键值缓存膨胀带来高内存和延迟成本，现有方法粗粒度共享令牌损害模型性能。

Method: 提出基于细粒度混合头注意力机制的LycheeDecode方法，利用硬件高效的top - k选择策略，用HardKuma机制划分注意力头。

Result: 在多个基准测试中，LycheeDecode生成质量与全注意力基线相当甚至更好，在128K上下文长度下速度提升达2.7倍。

Conclusion: 细粒度策略克服了现有方法的性能瓶颈，为高效高质量的长上下文大语言模型推理提供了有效途径。

Abstract: The proliferation of long-context large language models (LLMs) exposes a key bottleneck: the rapidly expanding key-value cache during decoding, which imposes heavy memory and latency costs. While recent approaches attempt to alleviate this by sharing a single set of crucial tokens across layers, such coarse-grained sharing undermines model performance by neglecting the functional diversity of attention heads. To address this, we propose LycheeDecode, an efficient decoding method centered on a fine-grained hybrid-head attention mechanism that employs a hardware-efficient top-k selection strategy. Specifically, the novel HardKuma-based mechanism partitions attention heads into a small subset of retrieval heads that dynamically identify crucial tokens and a majority of sparse heads that reuse them for efficient computation. Through extensive experiments on leading models like Llama3 and Qwen3 across diverse benchmarks for long-context understanding (e.g., LongBench, RULER) and complex reasoning (e.g., AIME24, OlympiadBench), we demonstrate that LycheeDecode achieves generative quality comparable to, and at times surpassing even the full-attention baseline. Crucially, this is accomplished with up to a 2.7x speedup at a 128K context length. By preserving the functional diversity of attention heads, our fine-grained strategy overcomes the performance bottlenecks of existing methods, providing a powerful and validated pathway to both efficient and high-quality long-context LLM inference.

</details>


### [344] [VILLAIN at AVerImaTeC: Verifying Image-Text Claims via Multi-Agent Collaboration](https://arxiv.org/abs/2602.04587)
*Jaeyoon Jung,Yejun Yoon,Seunghyun Yoon,Kunwoo Park*

Main category: cs.CL

TL;DR: 介绍多模态事实核查系统VILLAIN，通过基于提示的多智能体协作验证图文声明，在AVerImaTeC共享任务中排名第一，代码开源。


<details>
  <summary>Details</summary>
Motivation: 构建多模态事实核查系统以验证图文声明。

Method: 采用基于提示的多智能体协作，在事实核查多阶段使用视觉语言模型智能体，从知识存储中检索文本和视觉证据，通过模态特定和跨模态智能体生成分析报告，进而生成问答对，最后由Verdict Prediction智能体得出验证结果。

Result: 系统在所有评估指标的排行榜上排名第一。

Conclusion: VILLAIN系统在图文声明的事实核查方面表现出色，可有效完成任务。

Abstract: This paper describes VILLAIN, a multimodal fact-checking system that verifies image-text claims through prompt-based multi-agent collaboration. For the AVerImaTeC shared task, VILLAIN employs vision-language model agents across multiple stages of fact-checking. Textual and visual evidence is retrieved from the knowledge store enriched through additional web collection. To identify key information and address inconsistencies among evidence items, modality-specific and cross-modal agents generate analysis reports. In the subsequent stage, question-answer pairs are produced based on these reports. Finally, the Verdict Prediction agent produces the verification outcome based on the image-text claim and the generated question-answer pairs. Our system ranked first on the leaderboard across all evaluation metrics. The source code is publicly available at https://github.com/ssu-humane/VILLAIN.

</details>


### [345] [RexBERT: Context Specialized Bidirectional Encoders for E-commerce](https://arxiv.org/abs/2602.04605)
*Rahul Bajaj,Anuj Garg*

Main category: cs.CL

TL;DR: 介绍RexBERT，一种专为电商语义设计的BERT式编码器，发布电商语料库，提出训练方法，模型表现良好。


<details>
  <summary>Details</summary>
Motivation: 通用编码器在专业领域覆盖有限，需要适用于电商语义的编码器。

Method: 1. 发布Ecom - niverse语料库，介绍内容提取流程；2. 基于ModernBERT架构提出三阶段预训练方法；3. 训练不同参数的RexBERT模型并在电商数据集上评估。

Result: RexBERT参数更少却优于通用编码器，在特定领域基准测试中表现佳。

Conclusion: 高质量领域内数据与原则性训练方法为电商应用提供更坚实基础，而非单纯扩大规模。

Abstract: Encoder-only transformers remain indispensable in retrieval, classification, and ranking systems where latency, stability, and cost are paramount. Most general purpose encoders, however, are trained on generic corpora with limited coverage of specialized domains. We introduce RexBERT, a family of BERT-style encoders designed specifically for e-commerce semantics. We make three contributions. First, we release Ecom-niverse, a 350 billion token corpus curated from diverse retail and shopping sources. We describe a modular pipeline that isolates and extracts e-commerce content from FineFineWeb and other open web resources, and characterize the resulting domain distribution. Second, we present a reproducible pretraining recipe building on ModernBERT's architectural advances. The recipe consists of three phases: general pre-training, context extension, and annealed domain specialization. Third, we train RexBERT models ranging from 17M to 400M parameters and evaluate them on token classification, semantic similarity, and general natural language understanding tasks using e-commerce datasets. Despite having 2-3x fewer parameters, RexBERT outperforms larger general-purpose encoders and matches or surpasses modern long-context models on domain-specific benchmarks. Our results demonstrate that high quality in-domain data combined with a principled training approach provides a stronger foundation for e-commerce applications than indiscriminate scaling alone.

</details>


### [346] [Alignment Drift in Multimodal LLMs: A Two-Phase, Longitudinal Evaluation of Harm Across Eight Model Releases](https://arxiv.org/abs/2602.04739)
*Casey Ford,Madison Van Doren,Emily Dix*

Main category: cs.CL

TL;DR: 使用726条对抗性提示对多模态大语言模型无害性进行两阶段评估，发现不同模型家族存在差异，攻击成功率有对齐漂移，模态效果随时间变化，强调需纵向多模态基准来跟踪安全行为。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在对抗性提示下的安全性研究不足，需评估其无害性。

Method: 采用由26名专业红队成员创作的726条对抗性提示进行两阶段评估，对多个模型及后续版本进行评估，生成82,256个人为伤害评级。

Result: 不同模型家族差异大，Pixtral模型最脆弱，Claude模型最安全；攻击成功率显示对齐漂移，GPT和Claude模型成功率增加，Pixtral和Qwen略有下降；模态效果随时间变化。

Conclusion: 多模态大语言模型的无害性在更新中既不统一也不稳定，需要纵向多模态基准来追踪其安全行为的变化。

Abstract: Multimodal large language models (MLLMs) are increasingly deployed in real-world systems, yet their safety under adversarial prompting remains underexplored. We present a two-phase evaluation of MLLM harmlessness using a fixed benchmark of 726 adversarial prompts authored by 26 professional red teamers. Phase 1 assessed GPT-4o, Claude Sonnet 3.5, Pixtral 12B, and Qwen VL Plus; Phase 2 evaluated their successors (GPT-5, Claude Sonnet 4.5, Pixtral Large, and Qwen Omni) yielding 82,256 human harm ratings. Large, persistent differences emerged across model families: Pixtral models were consistently the most vulnerable, whereas Claude models appeared safest due to high refusal rates. Attack success rates (ASR) showed clear alignment drift: GPT and Claude models exhibited increased ASR across generations, while Pixtral and Qwen showed modest decreases. Modality effects also shifted over time: text-only prompts were more effective in Phase 1, whereas Phase 2 produced model-specific patterns, with GPT-5 and Claude 4.5 showing near-equivalent vulnerability across modalities. These findings demonstrate that MLLM harmlessness is neither uniform nor stable across updates, underscoring the need for longitudinal, multimodal benchmarks to track evolving safety behaviour.

</details>


### [347] [Exploiting contextual information to improve stance detection in informal political discourse with LLMs](https://arxiv.org/abs/2602.04750)
*Arman Engin Sucu,Yixiang Zhou,Mario A. Nascimento,Tony Mullen*

Main category: cs.CL

TL;DR: 研究用大语言模型进行在线政治立场检测，发现提供用户上下文信息可显著提升分类准确率。


<details>
  <summary>Details</summary>
Motivation: 在非正式在线话语中，语言具有讽刺、模糊和依赖上下文的特点，研究提供上下文信息能否提高大语言模型政治立场检测的分类准确率。

Method: 使用真实政治论坛数据集生成用户结构化档案，对七个最先进的大语言模型在基线和上下文增强设置下进行全面跨模型评估。

Result: 上下文提示显著提高了准确率，提升幅度在17.5%到38.5%之间，最高达到74%，超过以往方法；策略性选择政治内容比随机选择大上下文效果更好。

Conclusion: 在细微的政治分类任务中，纳入用户层面的上下文信息可提升大语言模型的性能。

Abstract: This study investigates the use of Large Language Models (LLMs) for political stance detection in informal online discourse, where language is often sarcastic, ambiguous, and context-dependent. We explore whether providing contextual information, specifically user profile summaries derived from historical posts, can improve classification accuracy. Using a real-world political forum dataset, we generate structured profiles that summarize users' ideological leaning, recurring topics, and linguistic patterns. We evaluate seven state-of-the-art LLMs across baseline and context-enriched setups through a comprehensive cross-model evaluation. Our findings show that contextual prompts significantly boost accuracy, with improvements ranging from +17.5\% to +38.5\%, achieving up to 74\% accuracy that surpasses previous approaches. We also analyze how profile size and post selection strategies affect performance, showing that strategically chosen political content yields better results than larger, randomly selected contexts. These findings underscore the value of incorporating user-level context to enhance LLM performance in nuanced political classification tasks.

</details>


### [348] [When Silence Is Golden: Can LLMs Learn to Abstain in Temporal QA and Beyond?](https://arxiv.org/abs/2602.04755)
*Xinyu Zhou,Chang Jin,Carsten Eickhoff,Zhijiang Guo,Seyed Ali Bahrainian*

Main category: cs.CL

TL;DR: 本文首次对大语言模型在时间问答推理中进行弃权能力训练展开实证研究，提出结合思维链监督和强化学习的管道，实验表明强化学习能提升推理能力，并揭示不同训练方法和信息类型的影响，为构建更可靠大语言模型提供基础。


<details>
  <summary>Details</summary>
Motivation: 大语言模型很少承认不确定性，在时间问答中常忽略时间敏感证据，现有校准等方法可能无法可靠捕捉复杂推理中的不确定性，因此需要研究训练大语言模型在时间问答推理中的弃权能力。

Method: 将弃权作为可教授技能，引入结合思维链监督和由弃权感知奖励引导的强化学习的管道，系统分析不同信息类型和训练技术的影响。

Result: 强化学习在推理上有显著提升，Qwen2.5 - 1.5B - Instruct初始化的模型在时间问答任务的精确匹配上超过GPT - 4o，在不可回答问题的真阳性率上比纯监督微调变体提高20%；监督微调会导致过度自信，强化学习虽提升预测准确性但也有类似风险；隐式信息对带弃权的推理益处有限。

Conclusion: 研究为弃权和推理的联合优化提供新见解，为构建更可靠大语言模型奠定基础。

Abstract: Large language models (LLMs) rarely admit uncertainty, often producing fluent but misleading answers, rather than abstaining (i.e., refusing to answer). This weakness is even evident in temporal question answering, where models frequently ignore time-sensitive evidence and conflate facts across different time-periods. In this paper, we present the first empirical study of training LLMs with an abstention ability while reasoning about temporal QA. Existing approaches such as calibration might be unreliable in capturing uncertainty in complex reasoning. We instead frame abstention as a teachable skill and introduce a pipeline that couples Chain-of-Thought (CoT) supervision with Reinforcement Learning (RL) guided by abstention-aware rewards. Our goal is to systematically analyze how different information types and training techniques affect temporal reasoning with abstention behavior in LLMs. Through extensive experiments studying various methods, we find that RL yields strong empirical gains on reasoning: a model initialized by Qwen2.5-1.5B-Instruct surpasses GPT-4o by $3.46\%$ and $5.80\%$ in Exact Match on TimeQA-Easy and Hard, respectively. Moreover, it improves the True Positive rate on unanswerable questions by $20\%$ over a pure supervised fine-tuned (SFT) variant. Beyond performance, our analysis shows that SFT induces overconfidence and harms reliability, while RL improves prediction accuracy but exhibits similar risks. Finally, by comparing implicit reasoning cues (e.g., original context, temporal sub-context, knowledge graphs) with explicit CoT supervision, we find that implicit information provides limited benefit for reasoning with abstention. Our study provides new insights into how abstention and reasoning can be jointly optimized, providing a foundation for building more reliable LLMs.

</details>


### [349] [SE-Bench: Benchmarking Self-Evolution with Knowledge Internalization](https://arxiv.org/abs/2602.04811)
*Jiarui Yuan,Tailin Jin,Weize Chen,Zeyuan Liu,Zhiyuan Liu,Maosong Sun*

Main category: cs.CL

TL;DR: 提出SE - Bench诊断环境测量智能体自我进化能力，揭示三点见解并搭建严格诊断平台。


<details>
  <summary>Details</summary>
Motivation: 严格测量智能体自我进化基础能力受先验知识和推理复杂性纠缠的阻碍。

Method: 构建SE - Bench，将NumPy库及其API文档混淆为伪新包，训练智能体内化该包并在无文档条件下进行简单编码任务评估。

Result: 揭示了开放书悖论、RL差距和自我博弈内化的可行性。

Conclusion: SE - Bench为知识内化的自我进化建立了严格的诊断平台。

Abstract: True self-evolution requires agents to act as lifelong learners that internalize novel experiences to solve future problems. However, rigorously measuring this foundational capability is hindered by two obstacles: the entanglement of prior knowledge, where ``new'' knowledge may appear in pre-training data, and the entanglement of reasoning complexity, where failures may stem from problem difficulty rather than an inability to recall learned knowledge. We introduce SE-Bench, a diagnostic environment that obfuscates the NumPy library and its API doc into a pseudo-novel package with randomized identifiers. Agents are trained to internalize this package and evaluated on simple coding tasks without access to documentation, yielding a clean setting where tasks are trivial with the new API doc but impossible for base models without it. Our investigation reveals three insights: (1) the Open-Book Paradox, where training with reference documentation inhibits retention, requiring "Closed-Book Training" to force knowledge compression into weights; (2) the RL Gap, where standard RL fails to internalize new knowledge completely due to PPO clipping and negative gradients; and (3) the viability of Self-Play for internalization, proving models can learn from self-generated, noisy tasks when coupled with SFT, but not RL. Overall, SE-Bench establishes a rigorous diagnostic platform for self-evolution with knowledge internalization. Our code and dataset can be found at https://github.com/thunlp/SE-Bench.

</details>


### [350] [The Missing Half: Unveiling Training-time Implicit Safety Risks Beyond Deployment](https://arxiv.org/abs/2602.04196)
*Zhexin Zhang,Yida Lu,Junfeng Fang,Junxiao Yang,Shiyao Cui,Hao Zhou,Fandong Meng,Jie Zhou,Hongning Wang,Minlie Huang,Tat-Seng Chua*

Main category: cs.CL

TL;DR: 本文研究AI模型训练阶段的隐式安全风险，提出分类体系，实验揭示风险普遍性和严重性，指出训练中被忽视的安全挑战。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注AI模型部署时的安全风险，而训练阶段的安全风险研究较少，尤其是隐式风险。

Method: 提出包含五个风险级别、十个细粒度风险类别和三种激励类型的分类体系，并进行大量实验。

Result: 实验显示风险普遍且严重，如Llama - 3.1 - 8B - Instruct在74.4%的训练运行中出现风险行为，且隐式训练风险也存在于多智能体训练中。

Conclusion: 训练中的隐式安全风险是一个被忽视但亟待解决的问题。

Abstract: Safety risks of AI models have been widely studied at deployment time, such as jailbreak attacks that elicit harmful outputs. In contrast, safety risks emerging during training remain largely unexplored. Beyond explicit reward hacking that directly manipulates explicit reward functions in reinforcement learning, we study implicit training-time safety risks: harmful behaviors driven by a model's internal incentives and contextual background information. For example, during code-based reinforcement learning, a model may covertly manipulate logged accuracy for self-preservation. We present the first systematic study of this problem, introducing a taxonomy with five risk levels, ten fine-grained risk categories, and three incentive types. Extensive experiments reveal the prevalence and severity of these risks: notably, Llama-3.1-8B-Instruct exhibits risky behaviors in 74.4% of training runs when provided only with background information. We further analyze factors influencing these behaviors and demonstrate that implicit training-time risks also arise in multi-agent training settings. Our results identify an overlooked yet urgent safety challenge in training.

</details>


### [351] [Proxy Compression for Language Modeling](https://arxiv.org/abs/2602.04289)
*Lin Zheng,Xinyu Li,Qian Liu,Xiachong Feng,Lingpeng Kong*

Main category: cs.CL

TL;DR: 本文提出代理压缩训练方案，在代码语言建模实验中提升训练效率，表现优于纯字节级基线，大模型下优势更明显。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型训练与固定分词器耦合，本文旨在提供一种在推理时具有端到端原始字节接口同时保留压缩输入效率优势的训练方案。

Method: 联合训练一个语言模型在原始字节序列和外部压缩器生成的压缩视图上，让模型学习内部对齐压缩序列和原始字节。

Result: 在代码语言建模的大量实验中，代理压缩显著提高训练效率，在固定计算预算下显著优于纯字节级基线。

Conclusion: 随着模型规模增加，代理压缩优势更明显，最终可媲美分词器方法，且仅处理原始字节，保留字节级建模的鲁棒性。

Abstract: Modern language models are trained almost exclusively on token sequences produced by a fixed tokenizer, an external lossless compressor often over UTF-8 byte sequences, thereby coupling the model to that compressor. This work introduces proxy compression, an alternative training scheme that preserves the efficiency benefits of compressed inputs while providing an end-to-end, raw-byte interface at inference time. During training, one language model is jointly trained on raw byte sequences and compressed views generated by external compressors; through the process, the model learns to internally align compressed sequences and raw bytes. This alignment enables strong transfer between the two formats, even when training predominantly on compressed inputs which are discarded at inference. Extensive experiments on code language modeling demonstrate that proxy compression substantially improves training efficiency and significantly outperforms pure byte-level baselines given fixed compute budgets. As model scale increases, these gains become more pronounced, and proxy-trained models eventually match or rival tokenizer approaches, all while operating solely on raw bytes and retaining the inherent robustness of byte-level modeling.

</details>


### [352] [Rethinking Weight Tying: Pseudo-Inverse Tying for Stable LM Training and Updates](https://arxiv.org/abs/2602.04556)
*Jian Gu,Aldeida Aleti,Chunyang Chen,Hongyu Zhang*

Main category: cs.CL

TL;DR: 提出伪逆绑定（PIT）方法以解决权重绑定中令牌接口不稳定问题，评估显示其提升训练稳定性等。


<details>
  <summary>Details</summary>
Motivation: 现有权重绑定方法不能保证稳定的令牌接口，导致优化敏感性变差和后训练干预不可预测。

Method: 提出PIT，将嵌入和反嵌入同步为共享潜在令牌内存的耦合投影，维护正交共享内存，引入通过Cholesky因子参数化的对称正定隐藏空间变换。

Result: 在2.56亿 - 13亿参数的设备模型上评估，观察到训练稳定性提高、层间语义一致性增强和副作用显著减少。

Conclusion: PIT方法有效，能改善训练稳定性和减少副作用。

Abstract: Weight tying is widely used in compact language models to reduce parameters by sharing the token table between the input embedding and the output projection. However, weight sharing does not guarantee a stable token interface: during training, the correspondence between encoding tokens into hidden states and decoding hidden states into logits can drift, worsening optimization sensitivity and making post-training interventions such as editing, patching, and lightweight adaptation less predictable. We propose Pseudo-Inverse Tying (PIT), which synchronizes embedding and unembedding as coupled projections of a shared latent token memory, guaranteeing a pseudo-inverse-consistent interface throughout training. PIT maintains an orthonormal shared memory, obtained by thin polar decomposition for teacher initialization or random orthonormal initialization from scratch, and introduces a fully learned symmetric positive definite hidden-space transform parameterized via a Cholesky factor. The output head applies this transform to hidden states before the vocabulary projection, while the embedding applies the inverse transform to token vectors using stable triangular solves, avoiding explicit pseudo-inverse recomputation and any vocabulary-sized auxiliary parameters. We evaluate PIT on on-device models spanning 256M-1.3B parameters across pretraining and adaptation, and consistently observe improved training stability, stronger layerwise semantic consistency, and substantially reduced side effects.

</details>


### [353] [Focus-LIME: Surgical Interpretation of Long-Context Large Language Models via Proxy-Based Neighborhood Selection](https://arxiv.org/abs/2602.04607)
*Junhao Liu,Haonan Yu,Zhenyu Yan,Xin Zhang*

Main category: cs.CL

TL;DR: 提出Focus - LIME框架解决大语言模型特征级解释难题，在长上下文基准测试表现良好


<details>
  <summary>Details</summary>
Motivation: 现有局部模型无关解释方法在高维特征下存在归因稀释问题，无法为高风险任务提供可靠解释

Method: 提出Focus - LIME粗到细框架，用代理模型管理扰动邻域，让目标模型在优化上下文内进行细粒度归因

Result: 在长上下文基准测试中，方法使精细解释可行，能为用户提供可靠解释

Conclusion: Focus - LIME框架可恢复精细特征级解释的可行性

Abstract: As Large Language Models (LLMs) scale to handle massive context windows, achieving surgical feature-level interpretation is essential for high-stakes tasks like legal auditing and code debugging. However, existing local model-agnostic explanation methods face a critical dilemma in these scenarios: feature-based methods suffer from attribution dilution due to high feature dimensionality, thus failing to provide faithful explanations. In this paper, we propose Focus-LIME, a coarse-to-fine framework designed to restore the tractability of surgical interpretation. Focus-LIME utilizes a proxy model to curate the perturbation neighborhood, allowing the target model to perform fine-grained attribution exclusively within the optimized context. Empirical evaluations on long-context benchmarks demonstrate that our method makes surgical explanations practicable and provides faithful explanations to users.

</details>


### [354] [Less Finetuning, Better Retrieval: Rethinking LLM Adaptation for Biomedical Retrievers via Synthetic Data and Model Merging](https://arxiv.org/abs/2602.04731)
*Sameh Khattab,Jean-Philippe Corbeil,Osman Alperen Koraş,Amin Dada,Julian Friedrich,François Beaulieu,Paul Vozila,Jens Kleesiek*

Main category: cs.CL

TL;DR: 提出STM框架将通用大语言模型转化为高性能领域专用检索器，在MTEB基准测试子集上表现良好。


<details>
  <summary>Details</summary>
Motivation: 探索如何将通用大语言模型适配为有效的领域特定检索器，尤其是生物医学等专业领域。

Method: 提出Synthesize - Train - Merge (STM)模块化框架，通过合成硬负样本、检索提示优化和模型合并来增强仅解码器的大语言模型。

Result: 在MTEB基准测试的12个医学和通用任务子集上，STM将特定任务专家模型提升了23.5%（平均7.5%），合并后的模型优于单一专家模型和强基线模型。

Conclusion: STM为将通用大语言模型转变为高性能、领域专用检索器提供了可扩展、高效的途径，能保留通用领域能力并在特定任务上表现出色。

Abstract: Retrieval-augmented generation (RAG) has become the backbone of grounding Large Language Models (LLMs), improving knowledge updates and reducing hallucinations. Recently, LLM-based retriever models have shown state-of-the-art performance for RAG applications. However, several technical aspects remain underexplored on how to adapt general-purpose LLMs into effective domain-specific retrievers, especially in specialized domains such as biomedicine. We present Synthesize-Train-Merge (STM), a modular framework that enhances decoder-only LLMs with synthetic hard negatives, retrieval prompt optimization, and model merging. Experiments on a subset of 12 medical and general tasks from the MTEB benchmark show STM boosts task-specific experts by up to 23.5\% (average 7.5\%) and produces merged models that outperform both single experts and strong baselines without extensive pretraining. Our results demonstrate a scalable, efficient path for turning general LLMs into high-performing, domain-specialized retrievers, preserving general-domain capabilities while excelling on specialized tasks.

</details>


### [355] [Reinforced Attention Learning](https://arxiv.org/abs/2602.04884)
*Bangzheng Li,Jianmo Ni,Chen Qu,Ian Miao,Liu Yang,Xingyu Fu,Muhao Chen,Derek Zhiyuan Cheng*

Main category: cs.CL

TL;DR: 提出强化注意力学习（RAL）框架优化多模态大语言模型，实验显示优于基线，注意力策略可作为多模态训练替代方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多模态大语言模型中提升有限，甚至会降低性能。

Method: 提出政策梯度框架RAL直接优化内部注意力分布，还引入策略内注意力蒸馏。

Result: 在多样图像和视频基准实验中，相比GRPO等基线有一致提升，注意力行为转移比标准知识蒸馏跨模态对齐效果好。

Conclusion: 注意力策略可作为多模态后训练的原则性通用替代方法。

Abstract: Post-training with Reinforcement Learning (RL) has substantially improved reasoning in Large Language Models (LLMs) via test-time scaling. However, extending this paradigm to Multimodal LLMs (MLLMs) through verbose rationales yields limited gains for perception and can even degrade performance.
  We propose Reinforced Attention Learning (RAL), a policy-gradient framework that directly optimizes internal attention distributions rather than output token sequences. By shifting optimization from what to generate to where to attend, RAL promotes effective information allocation and improved grounding in complex multimodal inputs. Experiments across diverse image and video benchmarks show consistent gains over GRPO and other baselines. We further introduce On-Policy Attention Distillation, demonstrating that transferring latent attention behaviors yields stronger cross-modal alignment than standard knowledge distillation. Our results position attention policies as a principled and general alternative for multimodal post-training.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [356] [Skin Tokens: A Learned Compact Representation for Unified Autoregressive Rigging](https://arxiv.org/abs/2602.04805)
*Jia-peng Zhang,Cheng-Feng Pu,Meng-Hao Guo,Yan-Pei Cao,Shi-Min Hu*

Main category: cs.GR

TL;DR: 提出SkinTokens和TokenRig框架解决3D模型动画绑定瓶颈，提升了皮肤绑定和骨骼预测精度。


<details>
  <summary>Details</summary>
Motivation: 生成式3D模型快速发展，现有自动绑定方法在蒙皮处理上有局限，将其视为高维回归任务，优化低效且与骨骼生成解耦。

Method: 引入SkinTokens作为蒙皮权重的表示，利用FSQ - CVAE捕捉蒙皮稀疏性，将任务转化为令牌序列预测问题；提出TokenRig统一自回归框架，结合强化学习阶段。

Result: SkinTokens表示使蒙皮精度比现有方法提高98% - 133%，TokenRig框架经强化学习优化后骨骼预测提高17% - 22%。

Conclusion: 提出统一、生成式的绑定方法，具有更高保真度和鲁棒性，为3D内容创作中的长期挑战提供可扩展解决方案。

Abstract: The rapid proliferation of generative 3D models has created a critical bottleneck in animation pipelines: rigging. Existing automated methods are fundamentally limited by their approach to skinning, treating it as an ill-posed, high-dimensional regression task that is inefficient to optimize and is typically decoupled from skeleton generation. We posit this is a representation problem and introduce SkinTokens: a learned, compact, and discrete representation for skinning weights. By leveraging an FSQ-CVAE to capture the intrinsic sparsity of skinning, we reframe the task from continuous regression to a more tractable token sequence prediction problem. This representation enables TokenRig, a unified autoregressive framework that models the entire rig as a single sequence of skeletal parameters and SkinTokens, learning the complicated dependencies between skeletons and skin deformations. The unified model is then amenable to a reinforcement learning stage, where tailored geometric and semantic rewards improve generalization to complex, out-of-distribution assets. Quantitatively, the SkinTokens representation leads to a 98%-133% percents improvement in skinning accuracy over state-of-the-art methods, while the full TokenRig framework, refined with RL, enhances bone prediction by 17%-22%. Our work presents a unified, generative approach to rigging that yields higher fidelity and robustness, offering a scalable solution to a long-standing challenge in 3D content creation.

</details>


<div id='cond-mat.dis-nn'></div>

# cond-mat.dis-nn [[Back]](#toc)

### [357] [Theory of Optimal Learning Rate Schedules and Scaling Laws for a Random Feature Model](https://arxiv.org/abs/2602.04774)
*Blake Bordelon,Francesco Mori*

Main category: cond-mat.dis-nn

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Setting the learning rate for a deep learning model is a critical part of successful training, yet choosing this hyperparameter is often done empirically with trial and error. In this work, we explore a solvable model of optimal learning rate schedules for a powerlaw random feature model trained with stochastic gradient descent (SGD). We consider the optimal schedule $η_T^\star(t)$ where $t$ is the current iterate and $T$ is the total training horizon. This schedule is computed both numerically and analytically (when possible) using optimal control methods. Our analysis reveals two regimes which we term the easy phase and hard phase. In the easy phase the optimal schedule is a polynomial decay $η_T^\star(t) \simeq T^{-ξ} (1-t/T)^δ$ where $ξ$ and $δ$ depend on the properties of the features and task. In the hard phase, the optimal schedule resembles warmup-stable-decay with constant (in $T$) initial learning rate and annealing performed over a vanishing (in $T$) fraction of training steps. We investigate joint optimization of learning rate and batch size, identifying a degenerate optimality condition. Our model also predicts the compute-optimal scaling laws (where model size and training steps are chosen optimally) in both easy and hard regimes. Going beyond SGD, we consider optimal schedules for the momentum $β(t)$, where speedups in the hard phase are possible. We compare our optimal schedule to various benchmarks in our task including (1) optimal constant learning rates $η_T(t) \sim T^{-ξ}$ (2) optimal power laws $η_T(t) \sim T^{-ξ} t^{-χ}$, finding that our schedule achieves better rates than either of these. Our theory suggests that learning rate transfer across training horizon depends on the structure of the model and task. We explore these ideas in simple experimental pretraining setups.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [358] [Comparative Insights on Adversarial Machine Learning from Industry and Academia: A User-Study Approach](https://arxiv.org/abs/2602.04753)
*Vishruti Kakkad,Paul Chung,Hanan Hibshi,Maverick Woo*

Main category: cs.CR

TL;DR: 本文对行业专业人士和学生开展研究，探讨AML漏洞及教育策略，发现网络安全教育与对AML威胁的关注有关，CTF方法能有效激发对AML威胁的兴趣，并给出ML课程中融入安全教育的建议。


<details>
  <summary>Details</summary>
Motivation: 机器学习及其生成式AI应用的指数级增长带来AML安全挑战，需研究专业人士和学生对AML漏洞的看法及教育策略。

Method: 一是对专业人士进行在线调查；二是开发两个融入自然语言处理和生成式AI概念的CTF挑战，并通过调查卡内基梅隆大学学生评估其有效性。

Result: 专业人士调查显示网络安全教育和对AML威胁的关注显著相关；对学生的调查表明CTF方法能有效激发兴趣。

Conclusion: 强调在ML课程中进行集成安全教育的迫切需求，并给出详细建议。

Abstract: An exponential growth of Machine Learning and its Generative AI applications brings with it significant security challenges, often referred to as Adversarial Machine Learning (AML). In this paper, we conducted two comprehensive studies to explore the perspectives of industry professionals and students on different AML vulnerabilities and their educational strategies. In our first study, we conducted an online survey with professionals revealing a notable correlation between cybersecurity education and concern for AML threats. For our second study, we developed two CTF challenges that implement Natural Language Processing and Generative AI concepts and demonstrate a poisoning attack on the training data set. The effectiveness of these challenges was evaluated by surveying undergraduate and graduate students at Carnegie Mellon University, finding that a CTF-based approach effectively engages interest in AML threats. Based on the responses of the participants in our research, we provide detailed recommendations emphasizing the critical need for integrated security education within the ML curriculum.

</details>


### [359] [ZKBoost: Zero-Knowledge Verifiable Training for XGBoost](https://arxiv.org/abs/2602.04113)
*Nikolas Melissaris,Jiayi Xu,Antigoni Polychroniadou,Akira Takahashi,Chenkai Weng*

Main category: cs.CR

TL;DR: 提出首个用于XGBoost的零知识训练证明协议ZKBoost，实现模型完整性的加密保证。


<details>
  <summary>Details</summary>
Motivation: 随着XGBoost在敏感场景部署增加，需对模型完整性提供加密保证。

Method: 提出兼容算术电路的定点XGBoost实现、通用的zkPoT模板以及基于VOLE的实例化方法。

Result: 定点实现与标准XGBoost准确率相差在1%内，并能在真实数据集上进行实用的zkPoT。

Conclusion: ZKBoost可让模型所有者在不泄露数据和参数情况下证明正确训练。

Abstract: Gradient boosted decision trees, particularly XGBoost, are among the most effective methods for tabular data. As deployment in sensitive settings increases, cryptographic guarantees of model integrity become essential. We present ZKBoost, the first zero-knowledge proof of training (zkPoT) protocol for XGBoost, enabling model owners to prove correct training on a committed dataset without revealing data or parameters. We make three key contributions: (1) a fixed-point XGBoost implementation compatible with arithmetic circuits, enabling instantiation of efficient zkPoT, (2) a generic template of zkPoT for XGBoost, which can be instantiated with any general-purpose ZKP backend, and (3) vector oblivious linear evaluation (VOLE)-based instantiation resolving challenges in proving nonlinear fixed-point operations. Our fixed-point implementation matches standard XGBoost accuracy within 1\% while enabling practical zkPoT on real-world datasets.

</details>


### [360] [Inference-Time Backdoors via Hidden Instructions in LLM Chat Templates](https://arxiv.org/abs/2602.04653)
*Ariel Fogel,Omer Hofman,Eilon Cohen,Roman Vainshtein*

Main category: cs.CR

TL;DR: 本文提出利用聊天模板的新型大语言模型后门攻击，无需访问训练或部署设施，评估显示该攻击有效且能绕过安全扫描。


<details>
  <summary>Details</summary>
Motivation: 开放权重语言模型在生产环境使用带来新安全挑战，以往后门攻击假设对手可访问训练或部署设施，本文探索新的攻击面。

Method: 利用聊天模板（Jinja2程序），分发带有恶意修改模板的模型来植入推理时后门，不修改模型权重、不污染训练数据或控制运行时基础设施。

Result: 构建针对降低事实准确性和诱导输出攻击者控制URL两个目标的模板后门，在18个模型上评估，触发条件下事实准确率从90%降至15%，输出URL成功率超80%，良性输入无明显降级；后门可跨推理运行时通用，能绕过最大开放权重分发平台安全扫描。

Conclusion: 聊天模板是大语言模型供应链中可靠且目前无防御的攻击面。

Abstract: Open-weight language models are increasingly used in production settings, raising new security challenges. One prominent threat in this context is backdoor attacks, in which adversaries embed hidden behaviors in language models that activate under specific conditions. Previous work has assumed that adversaries have access to training pipelines or deployment infrastructure. We propose a novel attack surface requiring neither, which utilizes the chat template. Chat templates are executable Jinja2 programs invoked at every inference call, occupying a privileged position between user input and model processing. We show that an adversary who distributes a model with a maliciously modified template can implant an inference-time backdoor without modifying model weights, poisoning training data, or controlling runtime infrastructure. We evaluated this attack vector by constructing template backdoors targeting two objectives: degrading factual accuracy and inducing emission of attacker-controlled URLs, and applied them across eighteen models spanning seven families and four inference engines. Under triggered conditions, factual accuracy drops from 90% to 15% on average while attacker-controlled URLs are emitted with success rates exceeding 80%; benign inputs show no measurable degradation. Backdoors generalize across inference runtimes and evade all automated security scans applied by the largest open-weight distribution platform. These results establish chat templates as a reliable and currently undefended attack surface in the LLM supply chain.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [361] [C-IDS: Solving Contextual POMDP via Information-Directed Objective](https://arxiv.org/abs/2602.03939)
*Chongyang Shi,Michael Dorothy,Jie Fu*

Main category: eess.SY

TL;DR: 研究CPOMDPs中的策略合成问题，引入信息导向目标，开发C - IDS算法，有理论分析和实验验证。


<details>
  <summary>Details</summary>
Motivation: 在CPOMDPs中设计能同时最大化累积回报和减少潜在上下文不确定性的策略。

Method: 引入信息导向目标，开发C - IDS算法来最大化该目标，对目标进行理论分析。

Result: 证明目标可解释为线性信息比率的拉格朗日松弛，温度参数是信息比率上界，建立了K轮次的次线性贝叶斯遗憾界。在连续Light - Dark环境实验中表现优于标准POMDP求解器。

Conclusion: 所提方法在CPOMDPs策略合成上有效，能更快识别上下文并获得更高回报。

Abstract: We study the policy synthesis problem in contextual partially observable Markov decision processes (CPOMDPs), where the environment is governed by an unknown latent context that induces distinct POMDP dynamics. Our goal is to design a policy that simultaneously maximizes cumulative return and actively reduces uncertainty about the underlying context. We introduce an information-directed objective that augments reward maximization with mutual information between the latent context and the agent's observations. We develop the C-IDS algorithm to synthesize policies that maximize the information-directed objective. We show that the objective can be interpreted as a Lagrangian relaxation of the linear information ratio and prove that the temperature parameter is an upper bound on the information ratio. Based on this characterization, we establish a sublinear Bayesian regret bound over K episodes. We evaluate our approach on a continuous Light-Dark environment and show that it consistently outperforms standard POMDP solvers that treat the unknown context as a latent state variable, achieving faster context identification and higher returns.

</details>


### [362] [Lyapunov Constrained Soft Actor-Critic (LC-SAC) using Koopman Operator Theory for Quadrotor Trajectory Tracking](https://arxiv.org/abs/2602.04132)
*Dhruv S. Kushwaha,Zoleikha A. Biron*

Main category: eess.SY

TL;DR: 本文针对强化学习应用于安全关键物理系统缺乏稳定性保证的问题，提出基于Koopman算子理论的Lyapunov约束Soft Actor - Critic（LC - SAC）算法，在二维四旋翼环境轨迹跟踪中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 强化学习在解决复杂决策问题上成功，但应用于安全关键物理系统缺乏稳定性保证，标准算法重奖励最大化，可能导致振荡或状态发散，现有结合Lyapunov稳定性保证的算法有选函数、计算复杂和策略保守等挑战。

Method: 提出LC - SAC算法，用扩展动态模态分解（EDMD）生成系统线性近似，推导候选Lyapunov函数闭式解，并将其融入SAC算法以保证策略稳定非线性系统。

Result: 在基于safe - control - gym的二维四旋翼环境轨迹跟踪中评估，相比基线纯SAC算法，所提算法有训练收敛性，且Lyapunov稳定性准则违规减少。

Conclusion: 所提出的基于Koopman算子理论的LC - SAC算法能提升强化学习在安全关键物理系统应用的稳定性。

Abstract: Reinforcement Learning (RL) has achieved remarkable success in solving complex sequential decision-making problems. However, its application to safety-critical physical systems remains constrained by the lack of stability guarantees. Standard RL algorithms prioritize reward maximization, often yielding policies that may induce oscillations or unbounded state divergence. There has significant work in incorporating Lyapunov-based stability guarantees in RL algorithms with key challenges being selecting a candidate Lyapunov function, computational complexity by using excessive function approximators and conservative policies by incorporating stability criterion in the learning process. In this work we propose a novel Lyapunov-constrained Soft Actor-Critic (LC-SAC) algorithm using Koopman operator theory. We propose use of extended dynamic mode decomposition (EDMD) to produce a linear approximation of the system and use this approximation to derive a closed form solution for candidate Lyapunov function. This derived Lyapunov function is incorporated in the SAC algorithm to further provide guarantees for a policy that stabilizes the nonlinear system. The results are evaluated trajectory tracking of a 2D Quadrotor environment based on safe-control-gym. The proposed algorithm shows training convergence and decaying violations for Lyapunov stability criterion compared to baseline vanilla SAC algorithm. GitHub Repository: https://github.com/DhruvKushwaha/LC-SAC-Quadrotor-Trajectory-Tracking

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [363] [Decoding Ambiguous Emotions with Test-Time Scaling in Audio-Language Models](https://arxiv.org/abs/2602.03873)
*Hong Jia,Weibin Li,Jingyao Wu,Xiaofeng Yu,Yan Gao,Jintao Cheng,Xiaoyu Tang,Feng Xia,Ting Dang*

Main category: cs.SD

TL;DR: 本文提出首个在测试时缩放条件下使用音频语言模型进行语音模糊情感识别的基准，评估多种模型和策略，分析相关因素交互，为开发语音AI系统奠定基础。


<details>
  <summary>Details</summary>
Motivation: 以往情感识别多为分类问题，难以处理现实中模糊、重叠和依赖上下文的情感状态，且音频语言模型处理模糊情感能力及推理时间技术在情感计算中的相关性待探索。

Method: 引入基准，系统比较8个先进音频语言模型和5种测试时缩放策略，对三个语音情感数据集进行评估。

Result: 提供了模型能力、测试时缩放和情感模糊性之间相互作用的深入分析。

Conclusion: 为开发更强大、有上下文感知和情感智能的语音AI系统奠定基础，指出缩小模型假设与现实人类情感复杂性差距的未来方向。

Abstract: Emotion recognition from human speech is a critical enabler for socially aware conversational AI. However, while most prior work frames emotion recognition as a categorical classification problem, real-world affective states are often ambiguous, overlapping, and context-dependent, posing significant challenges for both annotation and automatic modeling. Recent large-scale audio language models (ALMs) offer new opportunities for nuanced affective reasoning without explicit emotion supervision, but their capacity to handle ambiguous emotions remains underexplored. At the same time, advances in inference-time techniques such as test-time scaling (TTS) have shown promise for improving generalization and adaptability in hard NLP tasks, but their relevance to affective computing is still largely unknown. In this work, we introduce the first benchmark for ambiguous emotion recognition in speech with ALMs under test-time scaling. Our evaluation systematically compares eight state-of-the-art ALMs and five TTS strategies across three prominent speech emotion datasets. We further provide an in-depth analysis of the interaction between model capacity, TTS, and affective ambiguity, offering new insights into the computational and representational challenges of ambiguous emotion understanding. Our benchmark establishes a foundation for developing more robust, context-aware, and emotionally intelligent speech-based AI systems, and highlights key future directions for bridging the gap between model assumptions and the complexity of real-world human emotion.

</details>


### [364] [Audio ControlNet for Fine-Grained Audio Generation and Editing](https://arxiv.org/abs/2602.04680)
*Haina Zhu,Yao Xiao,Xiquan Li,Ziyang Ma,Jianwei Yu,Bowen Zhang,Mingqi Yang,Xie Chen*

Main category: cs.SD

TL;DR: 研究细粒度文本到音频生成任务，提出T2A - ControlNet和T2A - Adapter，T2A - Adapter性能优，还扩展到音频编辑，将发布相关资源。


<details>
  <summary>Details</summary>
Motivation: 现有模型在文本到音频生成时缺乏对响度、音高和声音事件等属性的精确控制，且之前方法需为特定控制类型重新训练模型。

Method: 在预训练的T2A骨干网络上训练ControlNet模型，引入T2A - ControlNet和T2A - Adapter；将框架扩展到音频编辑，提出T2A - Editor。

Result: T2A - Adapter仅用38M额外参数，在AudioSet - Strong的事件级和片段级F1分数上达到了最先进水平。

Conclusion: 提出的方法可实现对音频生成和编辑的有效控制，相关资源将发布以支持后续研究。

Abstract: We study the fine-grained text-to-audio (T2A) generation task. While recent models can synthesize high-quality audio from text descriptions, they often lack precise control over attributes such as loudness, pitch, and sound events. Unlike prior approaches that retrain models for specific control types, we propose to train ControlNet models on top of pre-trained T2A backbones to achieve controllable generation over loudness, pitch, and event roll. We introduce two designs, T2A-ControlNet and T2A-Adapter, and show that the T2A-Adapter model offers a more efficient structure with strong control ability. With only 38M additional parameters, T2A-Adapter achieves state-of-the-art performance on the AudioSet-Strong in both event-level and segment-level F1 scores. We further extend this framework to audio editing, proposing T2A-Editor for removing and inserting audio events at time locations specified by instructions. Models, code, dataset pipelines, and benchmarks will be released to support future research on controllable audio generation and editing.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [365] [The Needle is a Thread: Finding Planted Paths in Noisy Process Trees](https://arxiv.org/abs/2602.04694)
*Maya Le,Paweł Prałat,Aaron Smith,François Théberge*

Main category: cs.SI

TL;DR: 本文引入‘植入路径’问题，提出树模糊匹配算法，并用合成数据和真实数据集验证相关工作流的有效性。


<details>
  <summary>Details</summary>
Motivation: 解决网络安全领域从大量计算机日志数据中挖掘恶意软件相关事件有意义序列的问题。

Method: 提出用于在两棵树之间进行模糊匹配的算法，该算法可作为更复杂工作流的基础。

Result: 通过合成数据和现实世界的ACME网络安全数据集验证了一些相关工作流的有效性。

Conclusion: 所提出的算法和工作流在网络安全数据挖掘中有一定应用价值。

Abstract: Motivated by applications in cybersecurity such as finding meaningful sequences of malware-related events buried inside large amounts of computer log data, we introduce the "planted path" problem and propose an algorithm to find fuzzy matchings between two trees. This algorithm can be used as a "building block" for more complicated workflows. We demonstrate usefulness of a few of such workflows in mining synthetically generated data as well as real-world ACME cybersecurity datasets.

</details>


### [366] [Unmasking Superspreaders: Data-Driven Approaches for Identifying and Comparing Key Influencers of Conspiracy Theories on X.com](https://arxiv.org/abs/2602.04546)
*Florian Kramer,Henrich R. Greve,Moritz von Zahn,Hayagreeva Rao*

Main category: cs.SI

TL;DR: 本文利用新冠疫情期间超700万条推文，分析人类超级传播者和机器人传播阴谋论的差异，提出27种新指标，发现改进的H指数可识别超级传播者，为应对策略提供基础。


<details>
  <summary>Details</summary>
Motivation: 阴谋论威胁社会，社交媒体助长其传播，缺乏对传播者的系统分析和实用识别方法，因此需分析关键传播者差异并找到识别方法。

Method: 分析超700万条新冠疫情推文，对比人类超级传播者和机器人在语言复杂度、毒性和标签使用等维度差异，提出并评估27种量化阴谋论传播严重性的新指标。

Result: 发现超级传播者和机器人有不同传播策略，改进的H指数可有效识别超级传播者。

Conclusion: 研究为应对阴谋论传播的策略提供基础，如平台审核政策、账号封禁和公众宣传活动。

Abstract: Conspiracy theories can threaten society by spreading misinformation, deepening polarization, and eroding trust in democratic institutions. Social media often fuels the spread of conspiracies, primarily driven by two key actors: Superspreaders -- influential individuals disseminating conspiracy content at disproportionately high rates, and Bots -- automated accounts designed to amplify conspiracies strategically. To counter the spread of conspiracy theories, it is critical to both identify these actors and to better understand their behavior. However, a systematic analysis of these actors as well as real-world-applicable identification methods are still lacking. In this study, we leverage over seven million tweets from the COVID-19 pandemic to analyze key differences between Human Superspreaders and Bots across dimensions such as linguistic complexity, toxicity, and hashtag usage. Our analysis reveals distinct communication strategies: Superspreaders tend to use more complex language and substantive content while relying less on structural elements like hashtags and emojis, likely to enhance credibility and authority. By contrast, Bots favor simpler language and strategic cross-usage of hashtags, likely to increase accessibility, facilitate infiltration into trending discussions, and amplify reach. To counter both Human Superspreaders and Bots, we propose and evaluate 27 novel metrics for quantifying the severity of conspiracy theory spread. Our findings highlight the effectiveness of an adapted H-Index for computationally feasible identification of Human Superspreaders. By identifying behavioral patterns unique to Human Superspreaders and Bots as well as providing suitable identification methods, this study provides a foundation for mitigation strategies, including platform moderation policies, temporary and permanent account suspensions, and public awareness campaigns.

</details>


### [367] [Structural shifts in institutional participation and collaboration within the AI arXiv preprint research ecosystem](https://arxiv.org/abs/2602.03969)
*Shama Magnur,Mayank Kejriwal*

Main category: cs.SI

TL;DR: 本文利用2021 - 2025年arXiv预印本数据集研究AI研究格局的结构变化，发现ChatGPT推出后出版物数量激增，但学术 - 行业合作仍受抑制。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型出现，研究AI研究格局的结构变化，预印本生态系统能反映实时科学变化。

Method: 采用多阶段数据收集和富集管道，结合基于大语言模型的机构分类，分析出版物数量、作者团队规模和学术 - 行业合作模式的演变。

Result: ChatGPT推出后出版物数量激增，学术机构研究量最大，但学术 - 行业合作受抑制，标准化合作指数低于随机混合基线。

Conclusion: 研究凸显了持续的机构分歧，生成式AI研究的资本密集性可能正在重塑科学合作的边界。

Abstract: The emergence of large language models (LLMs) represents a significant technological shift within the scientific ecosystem, particularly within the field of artificial intelligence (AI). This paper examines structural changes in the AI research landscape using a dataset of arXiv preprints (cs.AI) from 2021 through 2025. Given the rapid pace of AI development, the preprint ecosystem has become a critical barometer for real-time scientific shifts, often preceding formal peer-reviewed publication by months or years. By employing a multi-stage data collection and enrichment pipeline in conjunction with LLM-based institution classification, we analyze the evolution of publication volumes, author team sizes, and academic--industry collaboration patterns. Our results reveal an unprecedented surge in publication output following the introduction of ChatGPT, with academic institutions continuing to provide the largest volume of research. However, we observe that academic--industry collaboration is still suppressed, as measured by a Normalized Collaboration Index (NCI) that remains significantly below the random-mixing baseline across all major subfields. These findings highlight a continuing institutional divide and suggest that the capital-intensive nature of generative AI research may be reshaping the boundaries of scientific collaboration.

</details>


### [368] [Overstating Attitudes, Ignoring Networks: LLM Biases in Simulating Misinformation Susceptibility](https://arxiv.org/abs/2602.04674)
*Eun Cheol Choi,Lindsay E. Young,Emilio Ferrara*

Main category: cs.SI

TL;DR: 研究LLM模拟调查受访者能否重现人类对待错误信息的模式，发现LLM模拟有局限性，更适合诊断与人类判断偏差。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型能否重现人们受错误信息影响的模式，目前其能力尚不明确。

Method: 以三个在线调查为基线，用社会调查数据提示LLM生成模拟调查回复，评估其与原调查数据的匹配度。

Result: LLM生成回复捕捉到大致分布趋势，但高估了相信与分享错误信息的关联，线性模型表现与人类回复有差异。

Conclusion: 基于LLM的调查模拟更适合诊断与人类判断的系统差异，而非替代人类判断。

Abstract: Large language models (LLMs) are increasingly used as proxies for human judgment in computational social science, yet their ability to reproduce patterns of susceptibility to misinformation remains unclear. We test whether LLM-simulated survey respondents, prompted with participant profiles drawn from social survey data measuring network, demographic, attitudinal and behavioral features, can reproduce human patterns of misinformation belief and sharing. Using three online surveys as baselines, we evaluate whether LLM outputs match observed response distributions and recover feature-outcome associations present in the original survey data. LLM-generated responses capture broad distributional tendencies and show modest correlation with human responses, but consistently overstate the association between belief and sharing. Linear models fit to simulated responses exhibit substantially higher explained variance and place disproportionate weight on attitudinal and behavioral features, while largely ignoring personal network characteristics, relative to models fit to human responses. Analyses of model-generated reasoning and LLM training data suggest that these distortions reflect systematic biases in how misinformation-related concepts are represented. Our findings suggest that LLM-based survey simulations are better suited for diagnosing systematic divergences from human judgment than for substituting it.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [369] [Semantic Rate Distortion and Posterior Design: Compute Constraints, Multimodality, and Strategic Inference](https://arxiv.org/abs/2602.03949)
*Emrah Akyol*

Main category: cs.IT

TL;DR: 研究了在速率和计算约束下的战略高斯语义压缩，给出不同机制解决方案及相关结果，为AI提供信息论基础。


<details>
  <summary>Details</summary>
Motivation: 研究在速率和计算约束下，编码器和解码器优化不同二次目标时的战略高斯语义压缩。

Method: 通过将编码器问题简化为信息速率约束下的后验协方差设计，分析不同信息机制，推导语义注水和速率约束高斯说服方案。

Result: 刻画不同机制的战略速率失真函数，证明高斯最优性，说明计算限制与语义准确性关系，以及多模态观察的作用。

Conclusion: 研究结果为数据和能源高效AI提供信息论基础，为现代多模态语言模型提供原理性解释。

Abstract: We study strategic Gaussian semantic compression under rate and compute constraints, where an encoder and decoder optimize distinct quadratic objectives. A latent Gaussian state generates a task dependent semantic variable, and the decoder best responds via MMSE estimation, reducing the encoder's problem to posterior covariance design under an information rate constraint. We characterize the strategic rate distortion function in direct, remote, and full information regimes, derive semantic waterfilling and rate constrained Gaussian persuasion solutions, and establish Gaussian optimality under misaligned objectives. We further show that architectural compute limits act as implicit rate constraints, yielding exponential improvements in semantic accuracy with model depth and inference time compute, while multimodal observation eliminates the geometric mean penalty inherent to remote encoding. These results provide information theoretic foundations for data and energy efficient AI and offer a principled interpretation of modern multimodal language models as posterior design mechanisms under resource constraints.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [370] [Thermodynamic assessment of machine learning models for solid-state synthesis prediction](https://arxiv.org/abs/2602.04075)
*Jane Schlesinger,Simon Hjaltason,Nathan J. Szymanski,Christopher J. Bartel*

Main category: cond-mat.mtrl-sci

TL;DR: 评估机器学习合成预测模型与材料和反应热力学的一致性，发现模型普遍高估合成可能性，还提出评估模型质量新方法。


<details>
  <summary>Details</summary>
Motivation: 评估近期合成预测模型与材料和反应热力学的一致性，发现现有机器学习用于材料合成模型的差距。

Method: 用成功合成配方数据集确定相关量界限，用CHGNet计算新假设材料热力学量，应用四个合成预测模型并对比预测结果和热力学计算结果。

Result: 模型普遍高估合成可能性，部分模型得分与热力学启发式有趋势关系。

Conclusion: 确定了机器学习材料合成模型的现有差距，引入在缺乏大量失败合成案例时评估模型质量的新方法。

Abstract: Machine learning models have recently emerged to predict whether hypothetical solid-state materials can be synthesized. These models aim to circumvent direct first-principles modeling of solid-state phase transformations, instead learning from large databases of successfully synthesized materials. Here, we assess the alignment of several recently introduced synthesis prediction models with material and reaction thermodynamics, quantified by the energy with respect to the convex hull and a metric accounting for thermodynamic selectivity of enumerated synthesis reactions. A dataset of successful synthesis recipes was used to determine the likely bounds on both quantities beyond which materials can be deemed unlikely to be synthesized. With these bounds as context, thermodynamic quantities were computed using the CHGNet foundation potential for thousands of new hypothetical materials generated using the Chemeleon generative model. Four recently published machine learning models for synthesizability prediction were applied to this same dataset, and the resultant predictions were considered against computed thermodynamics. We find these models generally overpredict the likelihood of synthesis, but some model scores do trend with thermodynamic heuristics, assigning lower scores to materials that are less stable or do not have an available synthesis recipe that is calculated to be thermodynamically selective. In total, this work identifies existing gaps in machine learning models for materials synthesis and introduces a new approach to assess their quality in the absence of extensive negative examples (failed syntheses).

</details>


### [371] [Machine Learning-Driven Crystal System Prediction for Perovskites Using Augmented X-ray Diffraction Data](https://arxiv.org/abs/2602.04435)
*Ansu Mathew,Ahmer A. B. Baloch,Alamin Yakasai,Hemant Mittal,Vivian Alberts,Jayakumar V. Karunamurthy*

Main category: cond-mat.mtrl-sci

TL;DR: 本文提出基于机器学习的框架，利用多种模型对钙钛矿材料XRD数据进行晶体系统等分类，经特征增强策略提升性能，模型表现良好。


<details>
  <summary>Details</summary>
Motivation: 在材料科学中，从XRD光谱预测晶体系统很关键，尤其是对有广泛应用的钙钛矿材料。

Method: 采用多种机器学习模型，如TSF、RF等，结合SMOTE等特征增强策略和数据预处理管道。

Result: TSF模型结合SMOTE增强策略在晶体系统预测中表现出色，MCC为0.9，F1分数0.92，准确率97.76%；点群和空间群预测平衡准确率超95%。

Conclusion: 强调了机器学习在基于XRD的钙钛矿材料结构表征和加速发现方面的潜力。

Abstract: Prediction of crystal system from X-ray diffraction (XRD) spectra is a critical task in materials science, particularly for perovskite materials which are known for their diverse applications in photovoltaics, optoelectronics, and catalysis. In this study, we present a machine learning (ML)-driven framework that leverages advanced models, including Time Series Forest (TSF), Random Forest (RF), Extreme Gradient Boosting (XGBoost), Recurrent Neural Network (RNN), Long Short-Term Memory (LSTM), Gated Recurrent Unit (GRU), and a simple feedforward neural network (NN), to classify crystal systems, point groups, and space groups from XRD data of perovskite materials. To address class imbalance and enhance model robustness, we integrated feature augmentation strategies such as Synthetic Minority Over-sampling Technique (SMOTE), class weighting, jittering, and spectrum shifting, along with efficient data preprocessing pipelines. The TSF model with SMOTE augmentation achieved strong performance for crystal system prediction, with a Matthews correlation coefficient (MCC) of 0.9, an F1 score of 0.92, and an accuracy of 97.76%. For point and space group prediction, balanced accuracies above 95% were obtained. The model demonstrated high performance for symmetry-distinct classes, including cubic crystal systems, point groups 3m and m-3m, and space groups Pnma and Pnnn. This work highlights the potential of ML for XRD-based structural characterization and accelerated discovery of perovskite materials

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [372] [mmcmcBayes:An R Package Implementing a Multistage MCMC Framework for Detecting the Differentially Methylated Regions](https://arxiv.org/abs/2602.04554)
*Zhexuan Yang,Duchwan Ryu,Feng Luan*

Main category: stat.AP

TL;DR: 本文介绍R包mmcmcBayes，用于检测差异甲基化区域，通过模拟研究和实际数据展示其性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法聚合CpG水平测试结果检测差异甲基化区域，难以捕捉复杂区域甲基化模式，需要新方法。

Method: 使用多阶段马尔可夫链蒙特卡罗程序，用alpha - 偏态广义正态分布建模样本区域甲基化摘要，通过贝叶斯因子评估组间差异甲基化证据，采用多阶段区域分割策略优化候选区域。

Result: 通过模拟研究和Illumina 450K甲基化数据应用展示性能。

Conclusion: mmcmcBayes包为基于CpG的差异甲基化区域检测方法提供实用的区域水平替代方案，包含相关支持函数。

Abstract: Identifying differentially methylated regions is an important task in epigenome-wide association studies, where differential signals often arise across groups of neighboring CpG sites. Many existing methods detect differentially methylated regions by aggregating CpG-level test results, which may limit their ability to capture complex regional methylation patterns. In this paper, we introduce the R package mmcmcBayes, which implements a multistage Markov chain Monte Carlo procedure for region-level detection of differentially methylated regions. The method models sample-wise regional methylation summaries using the alpha-skew generalized normal distribution and evaluates evidence for differential methylation between groups through Bayes factors. We use a multistage region-splitting strategy to refine candidate regions based on statistical evidence. We describe the underlying methodology and software implementation, and illustrate its performance through simulation studies and applications to Illumina 450K methylation data. The mmcmcBayes package provides a practical region-level alternative to existing CpG-based differentially methylated regions detection methods and includes supporting functions for summarizing, comparing, and visualizing detected regions.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [373] [PaperX: A Unified Framework for Multimodal Academic Presentation Generation with Scholar DAG](https://arxiv.org/abs/2602.03866)
*Tao Yu,Minghui Zhang,Zhiqing Cui,Hao Wang,Zhongtian Luo,Shenghua Chai,Junhao Gong,Yuzhao Peng,Yuxuan Zhou,Yujia Yang,Zhenghao Zhang,Haopeng Jin,Xinming Wang,Yufei Xiong,Jiabing Yang,Jiahao Yuan,Hanqing Wang,Hongzhu Yi,YiFan Zhang,Yan Huang,Liang Wang*

Main category: cs.DL

TL;DR: 提出统一框架PaperX将学术论文转化为多模态展示内容，表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有自动解决方案将各格式视为孤立下游任务，导致处理冗余和语义不一致，需解决论文转化展示内容的劳动密集问题。

Method: 引入PaperX框架，用Scholar DAG中间表示解耦论文逻辑结构和展示语法，应用自适应图遍历策略从单一源生成多样输出。

Result: 综合评估表明，框架在内容保真度和美学质量上达到了先进水平，且与专门的单任务代理相比显著提高了成本效率。

Conclusion: PaperX框架在学术论文转化为多模态展示内容方面效果良好，能提升效率和质量。

Abstract: Transforming scientific papers into multimodal presentation content is essential for research dissemination but remains labor intensive. Existing automated solutions typically treat each format as an isolated downstream task, leading to redundant processing and semantic inconsistency. We introduce PaperX, a unified framework that models academic presentation generation as a structural transformation and rendering process. Central to our approach is the Scholar DAG, an intermediate representation that decouples the paper's logical structure from its final presentation syntax. By applying adaptive graph traversal strategies, PaperX generates diverse, high quality outputs from a single source. Comprehensive evaluations demonstrate that our framework achieves the state of the art performance in content fidelity and aesthetic quality while significantly improving cost efficiency compared to specialized single task agents.

</details>
