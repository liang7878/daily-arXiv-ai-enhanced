<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 16]
- [cs.CE](#cs.CE) [Total: 8]
- [cs.DB](#cs.DB) [Total: 4]
- [cs.DC](#cs.DC) [Total: 6]
- [cs.DS](#cs.DS) [Total: 13]
- [cs.GT](#cs.GT) [Total: 2]
- [cs.IR](#cs.IR) [Total: 5]
- [cs.LG](#cs.LG) [Total: 72]
- [cs.NE](#cs.NE) [Total: 3]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.SE](#cs.SE) [Total: 19]
- [q-fin.ST](#q-fin.ST) [Total: 1]
- [q-fin.TR](#q-fin.TR) [Total: 1]
- [stat.ML](#stat.ML) [Total: 8]
- [stat.CO](#stat.CO) [Total: 2]
- [stat.ME](#stat.ME) [Total: 3]
- [cs.NI](#cs.NI) [Total: 1]
- [cs.DL](#cs.DL) [Total: 2]
- [cs.CL](#cs.CL) [Total: 24]
- [cs.HC](#cs.HC) [Total: 9]
- [eess.SP](#eess.SP) [Total: 8]
- [cs.GR](#cs.GR) [Total: 2]
- [cs.CG](#cs.CG) [Total: 1]
- [eess.IV](#eess.IV) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [q-bio.GN](#q-bio.GN) [Total: 1]
- [stat.AP](#stat.AP) [Total: 1]
- [astro-ph.EP](#astro-ph.EP) [Total: 1]
- [cs.CY](#cs.CY) [Total: 4]
- [q-bio.NC](#q-bio.NC) [Total: 2]
- [eess.SY](#eess.SY) [Total: 2]
- [stat.TH](#stat.TH) [Total: 1]
- [math.OC](#math.OC) [Total: 4]
- [q-fin.MF](#q-fin.MF) [Total: 1]
- [econ.TH](#econ.TH) [Total: 1]
- [quant-ph](#quant-ph) [Total: 2]
- [cs.CC](#cs.CC) [Total: 2]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [cs.CV](#cs.CV) [Total: 10]
- [cs.CR](#cs.CR) [Total: 6]
- [cs.SD](#cs.SD) [Total: 1]
- [physics.bio-ph](#physics.bio-ph) [Total: 1]
- [cs.RO](#cs.RO) [Total: 5]
- [math.ST](#math.ST) [Total: 2]
- [q-bio.PE](#q-bio.PE) [Total: 2]
- [astro-ph.CO](#astro-ph.CO) [Total: 1]
- [econ.GN](#econ.GN) [Total: 6]
- [cs.IT](#cs.IT) [Total: 2]
- [math.PR](#math.PR) [Total: 1]
- [eess.AS](#eess.AS) [Total: 2]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Evaluating Control Protocols for Untrusted AI Agents](https://arxiv.org/abs/2511.02997)
*Jon Kutasov,Chloe Loughridge,Yuqi Sun,Henry Sleight,Buck Shlegeris,Tyler Tracy,Joe Benton*

Main category: cs.AI

TL;DR: 本文在SHADE - Arena中系统评估一系列AI控制协议的安全性，发现重采样和关键动作延迟策略初始表现佳，但重采样易受新攻击策略影响，关键动作延迟策略很稳健。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统作为代理广泛部署，确保其安全运行至关重要，需要评估AI控制协议的安全性。

Method: 在SHADE - Arena数据集里系统评估一系列控制协议，先评估蓝队协议对抗默认攻击策略，再迭代红队策略对抗这些协议。

Result: 重采样和关键动作延迟策略初始将安全性从50%提升到96%；有额外能力的红队攻击策略使重采样策略安全性降至17%；关键动作延迟策略对最强红队策略仍高度稳健。

Conclusion: 拒绝攻击策略获取协议内部信息很重要。

Abstract: As AI systems become more capable and widely deployed as agents, ensuring
their safe operation becomes critical. AI control offers one approach to
mitigating the risk from untrusted AI agents by monitoring their actions and
intervening or auditing when necessary. Evaluating the safety of these
protocols requires understanding both their effectiveness against current
attacks and their robustness to adaptive adversaries. In this work, we
systematically evaluate a range of control protocols in SHADE-Arena, a dataset
of diverse agentic environments. First, we evaluate blue team protocols,
including deferral to trusted models, resampling, and deferring on critical
actions, against a default attack policy. We find that resampling for
incrimination and deferring on critical actions perform best, increasing safety
from 50% to 96%. We then iterate on red team strategies against these protocols
and find that attack policies with additional affordances, such as knowledge of
when resampling occurs or the ability to simulate monitors, can substantially
improve attack success rates against our resampling strategy, decreasing safety
to 17%. However, deferring on critical actions is highly robust to even our
strongest red team strategies, demonstrating the importance of denying attack
policies access to protocol internals.

</details>


### [2] [PublicAgent: Multi-Agent Design Principles From an LLM-Based Open Data Analysis Framework](https://arxiv.org/abs/2511.03023)
*Sina Montazeri,Yunhe Feng,Kewei Sha*

Main category: cs.AI

TL;DR: 介绍PublicAgent多智能体框架解决大语言模型在端到端分析工作流中的局限，评估得出多智能体LLM系统的五个设计原则，指导复杂分析工作流专业化及公共数据访问。


<details>
  <summary>Details</summary>
Motivation: 开放数据存储库对非专家不可访问，大语言模型在端到端分析工作流有局限。

Method: 提出PublicAgent多智能体框架，将任务分解为专业智能体，跨五个模型和50个查询进行评估。

Result: 得到五个设计原则，如专业化有价值、智能体分类别、可缓解不同失败模式、架构效益跨任务复杂度稳定、智能体有效性因模型而异。

Conclusion: 这些原则指导复杂分析工作流何时及为何需要专业化，能让非专家通过自然语言接口访问公共数据。

Abstract: Open data repositories hold potential for evidence-based decision-making, yet
are inaccessible to non-experts lacking expertise in dataset discovery, schema
mapping, and statistical analysis. Large language models show promise for
individual tasks, but end-to-end analytical workflows expose fundamental
limitations: attention dilutes across growing contexts, specialized reasoning
patterns interfere, and errors propagate undetected. We present PublicAgent, a
multi-agent framework that addresses these limitations through decomposition
into specialized agents for intent clarification, dataset discovery, analysis,
and reporting. This architecture maintains focused attention within agent
contexts and enables validation at each stage. Evaluation across five models
and 50 queries derives five design principles for multi-agent LLM systems.
First, specialization provides value independent of model strength--even the
strongest model shows 97.5% agent win rates, with benefits orthogonal to model
scale. Second, agents divide into universal (discovery, analysis) and
conditional (report, intent) categories. Universal agents show consistent
effectiveness (std dev 12.4%) while conditional agents vary by model (std dev
20.5%). Third, agents mitigate distinct failure modes--removing discovery or
analysis causes catastrophic failures (243-280 instances), while removing
report or intent causes quality degradation. Fourth, architectural benefits
persist across task complexity with stable win rates (86-92% analysis, 84-94%
discovery), indicating workflow management value rather than reasoning
enhancement. Fifth, wide variance in agent effectiveness across models (42-96%
for analysis) requires model-aware architecture design. These principles guide
when and why specialization is necessary for complex analytical workflows while
enabling broader access to public data through natural language interfaces.

</details>


### [3] [No-Human in the Loop: Agentic Evaluation at Scale for Recommendation](https://arxiv.org/abs/2511.03051)
*Tao Zhang,Kehui Yao,Luyi Ma,Jiao Chen,Reza Yousefi Maragheh,Kai Zhao,Jianpeng Xu,Evren Korpeoglu,Sushant Kumar,Kannan Achan*

Main category: cs.AI

TL;DR: 本文提出ScalingEval基准研究，系统比较36个大语言模型，用于大语言模型作为评估者的评估，得出不同模型表现，建立可复现基准和评估协议。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型作为评估者对构建可扩展且可信的评估流程至关重要。

Method: 采用共识驱动的评估协议，通过多智能体框架和可扩展多数投票将模式审计和问题代码聚合为真实标签，无需人工注释。

Result: 在大规模互补商品推荐应用中，Claude 3.5 Sonnet决策置信度最高，Gemini 1.5 Pro整体表现最佳，GPT - 4o在延迟、准确性和成本上权衡最优，GPT - OSS 20B在开源模型中领先；结构化领域有强共识，生活方式类别存在分歧。

Conclusion: ScalingEval可作为大语言模型作为评估者的可复现基准和评估协议，提供了关于扩展性、可靠性和模型家族权衡的可行指导。

Abstract: Evaluating large language models (LLMs) as judges is increasingly critical
for building scalable and trustworthy evaluation pipelines. We present
ScalingEval, a large-scale benchmarking study that systematically compares 36
LLMs, including GPT, Gemini, Claude, and Llama, across multiple product
categories using a consensus-driven evaluation protocol. Our multi-agent
framework aggregates pattern audits and issue codes into ground-truth labels
via scalable majority voting, enabling reproducible comparison of LLM
evaluators without human annotation. Applied to large-scale complementary-item
recommendation, the benchmark reports four key findings: (i) Anthropic Claude
3.5 Sonnet achieves the highest decision confidence; (ii) Gemini 1.5 Pro offers
the best overall performance across categories; (iii) GPT-4o provides the most
favorable latency-accuracy-cost tradeoff; and (iv) GPT-OSS 20B leads among
open-source models. Category-level analysis shows strong consensus in
structured domains (Electronics, Sports) but persistent disagreement in
lifestyle categories (Clothing, Food). These results establish ScalingEval as a
reproducible benchmark and evaluation protocol for LLMs as judges, with
actionable guidance on scaling, reliability, and model family tradeoffs.

</details>


### [4] [Epidemiology of Large Language Models: A Benchmark for Observational Distribution Knowledge](https://arxiv.org/abs/2511.03070)
*Drago Plecko,Patrik Okanovic,Torsten Hoefler,Elias Bareinboim*

Main category: cs.AI

TL;DR: 本文构建基准测试大语言模型（LLMs）对现实世界概率分布的理解能力，结果显示LLMs整体表现不佳，缺乏相关知识。


<details>
  <summary>Details</summary>
Motivation: 为评估LLMs对描述现实世界的概率分布知识的理解能力，构建基准。

Method: 开发首个基准，评估LLMs是否能获取不同领域描述现实世界人口的经验分布。

Result: LLMs整体表现差，未自然内化现实世界统计数据。

Conclusion: 语言模型缺乏观测分布知识，干预和反事实知识也受限。

Abstract: Artificial intelligence (AI) systems hold great promise for advancing various
scientific disciplines, and are increasingly used in real-world applications.
Despite their remarkable progress, further capabilities are expected in order
to achieve more general types of intelligence. A critical distinction in this
context is between factual knowledge, which can be evaluated against true or
false answers (e.g., "what is the capital of England?"), and probabilistic
knowledge, reflecting probabilistic properties of the real world (e.g., "what
is the sex of a computer science graduate in the US?"). In this paper, our goal
is to build a benchmark for understanding the capabilities of LLMs in terms of
knowledge of probability distributions describing the real world. Given that
LLMs are trained on vast amounts of text, it may be plausible that they
internalize aspects of these distributions. Indeed, LLMs are touted as powerful
universal approximators of real-world distributions. At the same time,
classical results in statistics, known as curse of dimensionality, highlight
fundamental challenges in learning distributions in high dimensions,
challenging the notion of universal distributional learning. In this work, we
develop the first benchmark to directly test this hypothesis, evaluating
whether LLMs have access to empirical distributions describing real-world
populations across domains such as economics, health, education, and social
behavior. Our results demonstrate that LLMs perform poorly overall, and do not
seem to internalize real-world statistics naturally. When interpreted in the
context of Pearl's Causal Hierarchy (PCH), our benchmark demonstrates that
language models do not contain knowledge on observational distributions (Layer
1 of PCH), and thus the Causal Hierarchy Theorem implies that interventional
(Layer 2) and counterfactual (Layer 3) knowledge of these models is also
limited.

</details>


### [5] [SnapStream: Efficient Long Sequence Decoding on Dataflow Accelerators](https://arxiv.org/abs/2511.03092)
*Jonathan Li,Nasim Farahini,Evgenii Iuliugin,Magnus Vesterlund,Christian Haggstrom,Guangtao Wang,Shubhangi Upasani,Ayush Sachdeva,Rui Li,Faline Fu,Chen Wu,Ayesha Siddiqua,John Long,Tuowen Zhao,Matheen Musaddiq,Hakan Zeffer,Yun Du,Mingran Wang,Qinghua Li,Bo Li,Urmish Thakker,Raghu Prabhakar*

Main category: cs.AI

TL;DR: 本文探讨KV缓存压缩技术在工业部署中的应用问题，开发SnapStream方法并验证其在实际生产中的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有控制KV缓存大小技术未在工业部署中广泛应用，一是框架难以修改标准多头注意力算法，二是对现代模型准确性影响不明。

Method: 探索对Llama - 3.1 - 8B - Instruct和DeepSeek - R1的准确性影响，开发SnapStream KV缓存压缩方法。

Result: 在SambaNova SN40L加速器上对DeepSeek - 671B进行16路张量并行部署，每秒达1832个token，提升4倍片上内存使用率，在多个基准测试中精度损失小。

Conclusion: 这是首次在静态图和连续批处理的生产推理系统中实现稀疏KV注意力技术。

Abstract: The proliferation of 100B+ parameter Large Language Models (LLMs) with 100k+
context length support have resulted in increasing demands for on-chip memory
to support large KV caches. Techniques such as StreamingLLM and SnapKV
demonstrate how to control KV cache size while maintaining model accuracy. Yet,
these techniques are not commonly used within industrial deployments using
frameworks like vLLM or SGLang. The reason is twofold: on one hand, the static
graphs and continuous batching methodology employed by these frameworks make it
difficult to admit modifications to the standard multi-head attention
algorithm, while on the other hand, the accuracy implications of such
techniques on modern instruction-following and reasoning models are not well
understood, obfuscating the need for implementing these techniques. In this
paper, we explore these accuracy implications on Llama-3.1-8B-Instruct and
DeepSeek-R1, and develop SnapStream, a KV cache compression method that can be
deployed at scale. We demonstrate the efficacy of SnapStream in a 16-way
tensor-parallel deployment of DeepSeek-671B on SambaNova SN40L accelerators
running at 128k context length and up to 1832 tokens per second in a real
production setting. SnapStream enables $4\times$ improved on-chip memory usage
and introduces minimal accuracy degradation on LongBench-v2, AIME24 and
LiveCodeBench. To the best of our knowledge, this is the first implementation
of sparse KV attention techniques deployed in a production inference system
with static graphs and continuous batching.

</details>


### [6] [Large language models require a new form of oversight: capability-based monitoring](https://arxiv.org/abs/2511.03106)
*Katherine C. Kellogg,Bingyang Ye,Yifan Hu,Guergana K. Savova,Byron Wallace,Danielle S. Bitterman*

Main category: cs.AI

TL;DR: 提出基于能力的大语言模型（LLM）监控方法，可跨任务检测潜在问题，为医疗领域安全监控提供基础。


<details>
  <summary>Details</summary>
Motivation: 现有基于传统机器学习的任务型监控方法不适用于LLM，因LLM未针对特定任务训练，无法假设性能下降。

Method: 提出基于能力的监控方法，围绕模型共享能力组织监控，而非独立评估下游任务。

Result: 描述了开发者、组织领导者和专业协会实施该方法的考虑因素。

Conclusion: 基于能力的监控为医疗领域的LLM及未来通用人工智能模型的安全、自适应和协作监控提供可扩展基础。

Abstract: The rapid adoption of large language models (LLMs) in healthcare has been
accompanied by scrutiny of their oversight. Existing monitoring approaches,
inherited from traditional machine learning (ML), are task-based and founded on
assumed performance degradation arising from dataset drift. In contrast, with
LLMs, inevitable model degradation due to changes in populations compared to
the training dataset cannot be assumed, because LLMs were not trained for any
specific task in any given population. We therefore propose a new organizing
principle guiding generalist LLM monitoring that is scalable and grounded in
how these models are developed and used in practice: capability-based
monitoring. Capability-based monitoring is motivated by the fact that LLMs are
generalist systems whose overlapping internal capabilities are reused across
numerous downstream tasks. Instead of evaluating each downstream task
independently, this approach organizes monitoring around shared model
capabilities, such as summarization, reasoning, translation, or safety
guardrails, in order to enable cross-task detection of systemic weaknesses,
long-tail errors, and emergent behaviors that task-based monitoring may miss.
We describe considerations for developers, organizational leaders, and
professional societies for implementing a capability-based monitoring approach.
Ultimately, capability-based monitoring will provide a scalable foundation for
safe, adaptive, and collaborative monitoring of LLMs and future generalist
artificial intelligence models in healthcare.

</details>


### [7] [miniF2F-Lean Revisited: Reviewing Limitations and Charting a Path Forward](https://arxiv.org/abs/2511.03108)
*Azim Ospanov,Farzan Farnia,Roozbeh Yousefzadeh*

Main category: cs.AI

TL;DR: 本文从参加数学奥林匹克的AI系统角度分析miniF2F基准，发现原基准准确率低，原因是表述差异，修正后推出miniF2F - v2，准确率提升，表明高质量基准有助于评估形式推理进展。


<details>
  <summary>Details</summary>
Motivation: 从参加数学奥林匹克的AI系统角度，对miniF2F基准中的正式和非正式陈述进行全面分析，评估自动形式化和定理证明模型的性能。

Method: 分析miniF2F基准问题，找出正式和非正式陈述的差异并修正，创建miniF2F - v2，评估完整定理证明流程在原基准和新基准上的准确率。

Result: 原基准最佳准确率36%，修正后miniF2F - v2最佳准确率达70%，但自动形式化模型和定理证明器仍存在较大不一致。

Conclusion: 高质量基准有助于社区更好评估形式推理领域进展，诊断自动形式化和定理证明模型的失败与成功模式。

Abstract: We perform a thorough analysis of the formal and informal statements in the
miniF2F benchmark from the perspective of an AI system that is tasked to
participate in a math Olympiad consisting of the problems in miniF2F. In such
setting, the model has to read and comprehend the problems in natural language,
formalize them in Lean language, then proceed with proving the problems, and it
will get credit for each problem if the formal proof corresponds to the
original informal statement presented to the model. Our evaluation results
reveal that the best accuracy of such pipeline can be about 36% using the SoTA
models in the literature, considerably lower than the individual SoTA
accuracies, 97% and 69% reported in the autoformalization and theorem proving
literature. Analyzing the failure modes, we trace back a considerable portion
of this drop to discrepancies between the formal and informal statements for
more than half of the problems in miniF2F. We proceed with correcting all the
errors, discrepancies and simplifications in formal and informal statements,
and present the miniF2F-v2 with fully verified formal and informal statements
and proofs. Evaluating the full theorem proving pipeline on miniF2F-v2 leads to
the best accuracy of 70%, a significant improvement from the 40% on the
original miniF2F, yet indicating considerable misalignment between the
autoformalization models and theorem provers. Our deep analysis suggests that a
higher quality benchmark can help the community better evaluate progress in the
field of formal reasoning and also better diagnose the failure and success
modes of autoformalization and theorem proving models. Our dataset is available
at https://github.com/roozbeh-yz/miniF2F_v2.

</details>


### [8] [Using Multi-modal Large Language Model to Boost Fireworks Algorithm's Ability in Settling Challenging Optimization Tasks](https://arxiv.org/abs/2511.03137)
*Shipeng Cen,Ying Tan*

Main category: cs.AI

TL;DR: 随着优化问题复杂度增加，传统方法难以应对，本文结合多模态大语言模型辅助烟花算法设计，在TSP和EDA任务实验中取得或超越SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 优化问题复杂多样，传统零阶或一阶方法效率低、梯度信息不准确且优化信息利用不足，大语言模型发展使利用其设计优化算法受关注。

Method: 选择烟花算法作为基础优化器，提出关键部分（CP）概念扩展到复杂高维任务，借助大语言模型多模态特性利用优化过程信息。

Result: 新框架生成的烟花算法在许多问题实例上取得或超越SOTA结果。

Conclusion: 结合多模态大语言模型辅助烟花算法设计是有效的，能提升算法在复杂优化问题上的性能。

Abstract: As optimization problems grow increasingly complex and diverse, advancements
in optimization techniques and paradigm innovations hold significant
importance. The challenges posed by optimization problems are primarily
manifested in their non-convexity, high-dimensionality, black-box nature, and
other unfavorable characteristics. Traditional zero-order or first-order
methods, which are often characterized by low efficiency, inaccurate gradient
information, and insufficient utilization of optimization information, are
ill-equipped to address these challenges effectively. In recent years, the
rapid development of large language models (LLM) has led to substantial
improvements in their language understanding and code generation capabilities.
Consequently, the design of optimization algorithms leveraging large language
models has garnered increasing attention from researchers. In this study, we
choose the fireworks algorithm(FWA) as the basic optimizer and propose a novel
approach to assist the design of the FWA by incorporating multi-modal large
language model(MLLM). To put it simply, we propose the concept of Critical
Part(CP), which extends FWA to complex high-dimensional tasks, and further
utilizes the information in the optimization process with the help of the
multi-modal characteristics of large language models. We focus on two specific
tasks: the \textit{traveling salesman problem }(TSP) and \textit{electronic
design automation problem} (EDA). The experimental results show that FWAs
generated under our new framework have achieved or surpassed SOTA results on
many problem instances.

</details>


### [9] [A Proprietary Model-Based Safety Response Framework for AI Agents](https://arxiv.org/abs/2511.03138)
*Qi Li,Jianjun Xu,Pingtao Wei,Jiu Li,Peiqiang Zhao,Jiwei Shi,Xuan Zhang,Yanhui Yang,Xiaodong Hui,Peng Xu,Wenqin Shao*

Main category: cs.AI

TL;DR: 本文提出一种安全响应框架保障大语言模型安全，在输入和输出层发挥作用，实验效果良好，为构建高安全、高可信应用提供工程途径。


<details>
  <summary>Details</summary>
Motivation: 大语言模型广泛应用，安全问题突出，限制其在关键领域的可信部署。

Method: 输入层采用基于监督微调的安全分类模型，通过四级分类法处理用户查询；输出层将检索增强生成与微调解释模型结合，确保响应基于可信知识库。

Result: 所提安全控制模型在公共安全评估基准上得分高于基线模型，在专有高风险测试集上组件安全得分达100%。

Conclusion: 该研究为构建高安全、高可信的大语言模型应用提供了有效工程途径。

Abstract: With the widespread application of Large Language Models (LLMs), their
associated security issues have become increasingly prominent, severely
constraining their trustworthy deployment in critical domains. This paper
proposes a novel safety response framework designed to systematically safeguard
LLMs at both the input and output levels. At the input level, the framework
employs a supervised fine-tuning-based safety classification model. Through a
fine-grained four-tier taxonomy (Safe, Unsafe, Conditionally Safe, Focused
Attention), it performs precise risk identification and differentiated handling
of user queries, significantly enhancing risk coverage and business scenario
adaptability, and achieving a risk recall rate of 99.3%. At the output level,
the framework integrates Retrieval-Augmented Generation (RAG) with a
specifically fine-tuned interpretation model, ensuring all responses are
grounded in a real-time, trustworthy knowledge base. This approach eliminates
information fabrication and enables result traceability. Experimental results
demonstrate that our proposed safety control model achieves a significantly
higher safety score on public safety evaluation benchmarks compared to the
baseline model, TinyR1-Safety-8B. Furthermore, on our proprietary high-risk
test set, the framework's components attained a perfect 100% safety score,
validating their exceptional protective capabilities in complex risk scenarios.
This research provides an effective engineering pathway for building
high-security, high-trust LLM applications.

</details>


### [10] [Uncovering Bugs in Formal Explainers: A Case Study with PyXAI](https://arxiv.org/abs/2511.03169)
*Xuanxiang Huang,Yacine Izza,Alexey Ignatiev,Joao Marques-Silva*

Main category: cs.AI

TL;DR: 本文提出验证形式化解释器的新方法，评估PyXAI发现其存在错误解释，证实新方法重要性。


<details>
  <summary>Details</summary>
Motivation: 现有研究对形式化解释器实际实现的验证关注不足。

Method: 开发一种验证形式化解释器的新方法，并对公开可用的形式化解释器PyXAI进行评估。

Result: 实验分析的大多数数据集上，PyXAI计算出的解释存在错误。

Conclusion: 提出的新方法对于验证形式化解释器很重要。

Abstract: Formal explainable artificial intelligence (XAI) offers unique theoretical
guarantees of rigor when compared to other non-formal methods of
explainability. However, little attention has been given to the validation of
practical implementations of formal explainers. This paper develops a novel
methodology for validating formal explainers and reports on the assessment of
the publicly available formal explainer PyXAI. The paper documents the
existence of incorrect explanations computed by PyXAI on most of the datasets
analyzed in the experiments, thereby confirming the importance of the proposed
novel methodology for the validation of formal explainers.

</details>


### [11] [Toward Autonomous Engineering Design: A Knowledge-Guided Multi-Agent Framework](https://arxiv.org/abs/2511.03179)
*Varun Kumar,George Em Karniadakis*

Main category: cs.AI

TL;DR: 本文提出多智能体AI框架优化工程设计流程，以NACA翼型气动优化为例，展示其提升设计效率、一致性和质量。


<details>
  <summary>Details</summary>
Motivation: 传统工程设计方法资源消耗大且效率低，需要新方法解决多领域协作和迭代优化问题。

Method: 构建多智能体AI框架，包含Graph Ontologist、Design Engineer和Systems Engineer三个智能体，通过知识图谱和迭代反馈优化设计。

Result: 以NACA翼型气动优化为例，完成设计并优化性能指标。

Conclusion: 协作式AI智能体结合结构化知识表示可提升工程设计过程的效率、一致性和质量。

Abstract: The engineering design process often demands expertise from multiple domains,
leading to complex collaborations and iterative refinements. Traditional
methods can be resource-intensive and prone to inefficiencies. To address this,
we formalize the engineering design process through a multi-agent AI framework
that integrates structured design and review loops. The framework introduces
specialized knowledge-driven agents that collaborate to generate and refine
design candidates. As an exemplar, we demonstrate its application to the
aerodynamic optimization of 4-digit NACA airfoils. The framework consists of
three key AI agents: a Graph Ontologist, a Design Engineer, and a Systems
Engineer. The Graph Ontologist employs a Large Language Model (LLM) to
construct two domain-specific knowledge graphs from airfoil design literature.
The Systems Engineer, informed by a human manager, formulates technical
requirements that guide design generation and evaluation. The Design Engineer
leverages the design knowledge graph and computational tools to propose
candidate airfoils meeting these requirements. The Systems Engineer reviews and
provides feedback both qualitative and quantitative using its own knowledge
graph, forming an iterative feedback loop until a design is validated by the
manager. The final design is then optimized to maximize performance metrics
such as the lift-to-drag ratio. Overall, this work demonstrates how
collaborative AI agents equipped with structured knowledge representations can
enhance efficiency, consistency, and quality in the engineering design process.

</details>


### [12] [Adobe Summit Concierge Evaluation with Human in the Loop](https://arxiv.org/abs/2511.03186)
*Yiru Chen,Sally Fang,Sai Sree Harsha,Dan Luo,Vaishnavi Muppala,Fei Wu,Shun Jiang,Kun Qian,Yunyao Li*

Main category: cs.AI

TL;DR: 介绍为Adobe Summit开发的特定领域AI助手Summit Concierge，采用人在环开发流程应对挑战，经验表明敏捷反馈驱动开发可实现可扩展可靠的AI助手。


<details>
  <summary>Details</summary>
Motivation: 利用生成式AI助手提升企业场景下的生产力、简化信息获取和改善用户体验，为Adobe Summit开发特定领域AI助手。

Method: 采用人在环开发工作流程，结合提示工程、检索基础和轻量级人工验证。

Result: 描述了系统架构、开发过程和实际部署成果。

Conclusion: 敏捷、反馈驱动的开发即使在冷启动场景下也能实现可扩展且可靠的AI助手。

Abstract: Generative AI assistants offer significant potential to enhance productivity,
streamline information access, and improve user experience in enterprise
contexts. In this work, we present Summit Concierge, a domain-specific AI
assistant developed for Adobe Summit. The assistant handles a wide range of
event-related queries and operates under real-world constraints such as data
sparsity, quality assurance, and rapid deployment. To address these challenges,
we adopt a human-in-the-loop development workflow that combines prompt
engineering, retrieval grounding, and lightweight human validation. We describe
the system architecture, development process, and real-world deployment
outcomes. Our experience shows that agile, feedback-driven development enables
scalable and reliable AI assistants, even in cold-start scenarios.

</details>


### [13] [From Five Dimensions to Many: Large Language Models as Precise and Interpretable Psychological Profilers](https://arxiv.org/abs/2511.03235)
*Yi-Fei Liu,Yi-Long Lu,Di He,Hang Zhang*

Main category: cs.AI

TL;DR: 研究大语言模型（LLMs）能否及如何从最少定量输入中建模人类心理特征的相关结构，发现LLMs表现出色。


<details>
  <summary>Details</summary>
Motivation: 探究LLMs能否及如何从最少定量输入中建模人类心理特征的相关结构。

Method: 用816个人的大五人格量表回答提示各种LLMs，让其对其他九个心理量表进行角色扮演回答。

Result: LLMs在捕捉人类心理结构上有出色准确性，生成的量表间相关模式与人类数据高度一致，零样本性能远超基于语义相似性的预测，接近直接在数据集上训练的机器学习算法。分析显示LLMs采用两阶段过程。

Conclusion: LLMs能通过抽象和推理过程从最少数据中精确预测个体心理特征，为心理模拟提供强大工具，也为其新兴推理能力提供见解。

Abstract: Psychological constructs within individuals are widely believed to be
interconnected. We investigated whether and how Large Language Models (LLMs)
can model the correlational structure of human psychological traits from
minimal quantitative inputs. We prompted various LLMs with Big Five Personality
Scale responses from 816 human individuals to role-play their responses on nine
other psychological scales. LLMs demonstrated remarkable accuracy in capturing
human psychological structure, with the inter-scale correlation patterns from
LLM-generated responses strongly aligning with those from human data $(R^2 >
0.89)$. This zero-shot performance substantially exceeded predictions based on
semantic similarity and approached the accuracy of machine learning algorithms
trained directly on the dataset. Analysis of reasoning traces revealed that
LLMs use a systematic two-stage process: First, they transform raw Big Five
responses into natural language personality summaries through information
selection and compression, analogous to generating sufficient statistics.
Second, they generate target scale responses based on reasoning from these
summaries. For information selection, LLMs identify the same key personality
factors as trained algorithms, though they fail to differentiate item
importance within factors. The resulting compressed summaries are not merely
redundant representations but capture synergistic information--adding them to
original scores enhances prediction alignment, suggesting they encode emergent,
second-order patterns of trait interplay. Our findings demonstrate that LLMs
can precisely predict individual participants' psychological traits from
minimal data through a process of abstraction and reasoning, offering both a
powerful tool for psychological simulation and valuable insights into their
emergent reasoning capabilities.

</details>


### [14] [Towards Scalable Web Accessibility Audit with MLLMs as Copilots](https://arxiv.org/abs/2511.03471)
*Ming Gu,Ziwei Wang,Sicen Lai,Zirui Gao,Sheng Zhou,Jiajun Bu*

Main category: cs.AI

TL;DR: 提出审计框架AAA实现可扩展的端到端网页可访问性审计，并贡献四个新数据集，实验证明方法有效。


<details>
  <summary>Details</summary>
Motivation: 当前网页可访问性审计实践资源密集且不可扩展，多数网站用户界面不达标，WCAG - EM缺乏大规模执行的实际支持。

Method: 提出审计框架AAA，采用人机合作模式，包含基于图的多模态采样方法GRASP和基于多模态大语言模型的助手MaC，还贡献四个新数据集用于基准测试。

Result: 实验证明方法有效，发现微调后的小规模语言模型可成为有能力的专家。

Conclusion: AAA框架能实现可扩展的端到端网页可访问性审计，为人类审计员提供AI增强辅助，在现实中有积极影响。

Abstract: Ensuring web accessibility is crucial for advancing social welfare, justice,
and equality in digital spaces, yet the vast majority of website user
interfaces remain non-compliant, due in part to the resource-intensive and
unscalable nature of current auditing practices. While WCAG-EM offers a
structured methodology for site-wise conformance evaluation, it involves great
human efforts and lacks practical support for execution at scale. In this work,
we present an auditing framework, AAA, which operationalizes WCAG-EM through a
human-AI partnership model. AAA is anchored by two key innovations: GRASP, a
graph-based multimodal sampling method that ensures representative page
coverage via learned embeddings of visual, textual, and relational cues; and
MaC, a multimodal large language model-based copilot that supports auditors
through cross-modal reasoning and intelligent assistance in high-effort tasks.
Together, these components enable scalable, end-to-end web accessibility
auditing, empowering human auditors with AI-enhanced assistance for real-world
impact. We further contribute four novel datasets designed for benchmarking
core stages of the audit pipeline. Extensive experiments demonstrate the
effectiveness of our methods, providing insights that small-scale language
models can serve as capable experts when fine-tuned.

</details>


### [15] [Explaining Decisions in ML Models: a Parameterized Complexity Analysis (Part I)](https://arxiv.org/abs/2511.03545)
*Sebastian Ordyniak,Giacomo Paesani,Mateusz Rychlicki,Stefan Szeider*

Main category: cs.AI

TL;DR: 本文对多种机器学习模型解释问题的参数化复杂性进行理论研究，聚焦有透明机制的模型，分析两类解释问题，涵盖多种模型，填补可解释AI空白。


<details>
  <summary>Details</summary>
Motivation: 填补可解释AI中对多种机器学习模型解释复杂性理解的空白，促进AI系统透明度和问责制的讨论。

Method: 对多种机器学习模型（决策树、决策集等）的两类解释问题（溯因和对比，分局部和全局变体）进行参数化复杂性的理论分析。

Result: 对多种机器学习模型解释问题的复杂性有了基础性理解。

Conclusion: 为可解释AI领域进一步研究提供重要见解，有助于推动AI系统透明度和问责制的讨论。

Abstract: This paper presents a comprehensive theoretical investigation into the
parameterized complexity of explanation problems in various machine learning
(ML) models. Contrary to the prevalent black-box perception, our study focuses
on models with transparent internal mechanisms. We address two principal types
of explanation problems: abductive and contrastive, both in their local and
global variants. Our analysis encompasses diverse ML models, including Decision
Trees, Decision Sets, Decision Lists, Boolean Circuits, and ensembles thereof,
each offering unique explanatory challenges. This research fills a significant
gap in explainable AI (XAI) by providing a foundational understanding of the
complexities of generating explanations for these models. This work provides
insights vital for further research in the domain of XAI, contributing to the
broader discourse on the necessity of transparency and accountability in AI
systems.

</details>


### [16] [Outbidding and Outbluffing Elite Humans: Mastering Liar's Poker via Self-Play and Reinforcement Learning](https://arxiv.org/abs/2511.03724)
*Richard Dewey,Janos Botyanszki,Ciamac C. Moallemi,Andrew T. Zheng*

Main category: cs.AI

TL;DR: 本文提出AI代理Solly，在简化版大话扑克中达人类精英水平，训练采用无模型演员-评论家深度强化学习算法，表现优于人类和大语言模型。


<details>
  <summary>Details</summary>
Motivation: 此前AI在无限制德州扑克中多玩家动态不足，本文旨在让AI在多玩家参与度高的简化版大话扑克中达到人类精英水平。

Method: 使用无模型、演员-评论家的深度强化学习算法进行自我对弈训练。

Result: Solly在单挑和多玩家大话扑克中胜率超50%，在胜率和收益指标上优于大语言模型，还开发了新策略，有效随机出牌，不易被世界级人类玩家利用。

Conclusion: Solly成功在多玩家参与度高的简化版大话扑克中达到人类精英水平。

Abstract: AI researchers have long focused on poker-like games as a testbed for
environments characterized by multi-player dynamics, imperfect information, and
reasoning under uncertainty. While recent breakthroughs have matched elite
human play at no-limit Texas hold'em, the multi-player dynamics are subdued:
most hands converge quickly with only two players engaged through multiple
rounds of bidding. In this paper, we present Solly, the first AI agent to
achieve elite human play in reduced-format Liar's Poker, a game characterized
by extensive multi-player engagement. We trained Solly using self-play with a
model-free, actor-critic, deep reinforcement learning algorithm. Solly played
at an elite human level as measured by win rate (won over 50% of hands) and
equity (money won) in heads-up and multi-player Liar's Poker. Solly also
outperformed large language models (LLMs), including those with reasoning
abilities, on the same metrics. Solly developed novel bidding strategies,
randomized play effectively, and was not easily exploitable by world-class
human players.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [17] [Workday's Approach to Secure and Compliant Cloud ERP Systems](https://arxiv.org/abs/2511.02856)
*Monu Sharma*

Main category: cs.CE

TL;DR: 论文指出Workday符合多项全球标准，具备自动化合规属性，经比较展示出优势，还探讨未来技术应用，表明其是可靠、合规且面向未来的ERP解决方案。


<details>
  <summary>Details</summary>
Motivation: 评估Workday作为ERP解决方案在数据保护、合规性和应对未来挑战方面的能力。

Method: 分析Workday对全球标准的合规情况，进行比较审查，探讨未来技术应用。

Result: Workday能保护关键数据，自动化合规属性提升流程自动化，具备更好的风险管理等能力。

Conclusion: Workday是可靠、合规且面向未来的ERP解决方案，为企业云安全管理树立新标杆。

Abstract: Workday's compliance with global standards -- such as GDPR, SOC 2, HIPAA, ISO
27001, and FedRAMP -- shows its ability to best protect critical financial,
healthcare, and government data.Automated compliance attributes like audit
trails, behavioral analytics, and continuous reporting improve automation of
the process and cut down on the manual effort to audit. A comparative review
demonstrates enhanced risk management, operational flexibility, and breach
mitigation. The paper also discusses potential future solutions with AI, ML and
blockchain, to enhance attackdetection and data integrity. Overall, Workday
turns out to be a secure, compliant and future-ready ERP solution. The paper
also explores emerging trends, including the integration of AI, machine
learning, and blockchain technologies to enhance next-generation threat
detection and data integrity. The findings position Workday as a reliable,
compliant, and future-ready ERP solution, setting a new benchmark for secure
enterprise cloud management.

</details>


### [18] [A Conditional Diffusion Model for Building Energy Modeling Workflows](https://arxiv.org/abs/2511.02930)
*Saumya Sinha,Alexandre Cortiella,Rawad El Kontar,Andrew Glaws,Ryan King,Patrick Emami*

Main category: cs.CE

TL;DR: 论文提出用生成式建模方法生成建筑属性填补数据缺口，为能源模型提供完整输入，经验证展示了生成式建模加速建筑能源建模工作流的潜力。


<details>
  <summary>Details</summary>
Motivation: 城市能源模型需详细建筑特征数据集，但个体建筑层面的详细特征常未知、获取成本高或不可用，需填补数据缺口。

Method: 采用基于表格扩散的框架处理表格建筑数据的异构特征，开发条件扩散能力，根据已知属性填补缺失建筑特征。

Result: 通过比较生成的条件分布与基础数据分布，以及对巴尔的摩住宅区进行案例研究，验证了条件扩散模型。

Conclusion: 展示了生成式建模加速建筑能源建模工作流的潜力。

Abstract: Understanding current energy consumption behavior in communities is critical
for informing future energy use decisions and enabling efficient energy
management. Urban energy models, which are used to simulate these energy use
patterns, require large datasets with detailed building characteristics for
accurate outcomes. However, such detailed characteristics at the individual
building level are often unknown and costly to acquire, or unavailable. Through
this work, we propose using a generative modeling approach to generate
realistic building attributes to fill in the data gaps and finally provide
complete characteristics as inputs to energy models. Our model learns complex,
building-level patterns from training on a large-scale residential building
stock model containing 2.2 million buildings. We employ a tabular
diffusion-based framework that is designed to handle heterogeneous (discrete
and continuous) features in tabular building data, such as occupancy, floor
area, heating, cooling, and other equipment details. We develop a capability
for conditional diffusion, enabling the imputation of missing building
characteristics conditioned on known attributes. We conduct a comprehensive
validation of our conditional diffusion model, firstly by comparing the
generated conditional distributions against the underlying data distribution,
and secondly, by performing a case study for a Baltimore residential region,
showing the practical utility of our approach. Our work is one of the first to
demonstrate the potential of generative modeling to accelerate building energy
modeling workflows.

</details>


### [19] [A physics-augmented neural network framework for finite strain incompressible viscoelasticity](https://arxiv.org/abs/2511.02959)
*Karl A. Kalina,Jörg Brummund,Markus Kästner*

Main category: cs.CE

TL;DR: 提出物理增强神经网络（PANN）框架用于有限应变不可压缩粘弹性问题，校准后表现良好，有出色的插值和外推能力，与线性粘弹性一致。


<details>
  <summary>Details</summary>
Motivation: 解决有限应变不可压缩粘弹性问题，确保热力学一致性、客观性和材料对称性。

Method: 基于变形梯度的乘法分解，用单调和全输入凸神经网络表示自由能和对偶耗散势，用隐式指数时间积分器处理内部变量演化，结合可训练门层和lp正则化确定内部变量数量。

Result: PANN与合成和实验数据校准后，在不同变形率和加载路径下表现出色，有良好的插值和外推能力。

Conclusion: 所提出的PANN框架有效，与线性粘弹性一致。

Abstract: We propose a physics-augmented neural network (PANN) framework for finite
strain incompressible viscoelasticity within the generalized standard materials
theory. The formulation is based on the multiplicative decomposition of the
deformation gradient and enforces unimodularity of the inelastic deformation
part throughout the evolution. Invariant-based representations of the free
energy and the dual dissipation potential by monotonic and fully input-convex
neural networks ensure thermodynamic consistency, objectivity, and material
symmetry by construction. The evolution of the internal variables during
training is handled by solving the evolution equations using an implicit
exponential time integrator. In addition, a trainable gate layer combined with
lp regularization automatically identifies the required number of internal
variables during training. The PANN is calibrated with synthetic and
experimental data, showing excellent agreement for a wide range of deformation
rates and different load paths. We also show that the proposed model achieves
excellent interpolation as well as plausible and accurate extrapolation
behaviors. In addition, we demonstrate consistency of the PANN with linear
viscoelasticity by linearization of the full model.

</details>


### [20] [Hybrid DeepONet Surrogates for Multiphase Flow in Porous Media](https://arxiv.org/abs/2511.02962)
*Ezequiel S. Santos,Gabriel F. Barros,Amanda C. N. Oliveira,Rômulo M. Silva,Rodolfo S. M. Freitas,Dakshina M. Valiveti,Xiao-Hui Wu,Fernando A. Rochinha,Alvaro L. G. A. Coutinho*

Main category: cs.CE

TL;DR: 本文提出基于DeepONet的混合神经算子替代模型，用于解决多孔介质多相流偏微分方程求解难题，结果显示该模型参数少且预测性能强。


<details>
  <summary>Details</summary>
Motivation: 传统方法求解多孔介质多相流偏微分方程计算成本高，FNO和DeepONet独立应用时存在高内存需求和难处理时间维度等问题。

Method: 引入基于DeepONet模型的混合神经算子替代模型，集成FNO、MLP和KAN，将时空学习任务分别分配到分支和主干网络。

Result: 在不同复杂度的多孔介质多相流问题上评估，混合方案能以更少参数实现准确的替代建模，在大规模油藏模拟中保持强预测性能。

Conclusion: 混合神经算子替代模型可有效解决多孔介质多相流偏微分方程求解问题，具有参数少、预测性能强的优势。

Abstract: The solution of partial differential equations (PDEs) plays a central role in
numerous applications in science and engineering, particularly those involving
multiphase flow in porous media. Complex, nonlinear systems govern these
problems and are notoriously computationally intensive, especially in
real-world applications and reservoirs. Recent advances in deep learning have
spurred the development of data-driven surrogate models that approximate PDE
solutions with reduced computational cost. Among these, Neural Operators such
as Fourier Neural Operator (FNO) and Deep Operator Networks (DeepONet) have
shown strong potential for learning parameter-to-solution mappings, enabling
the generalization across families of PDEs. However, both methods face
challenges when applied independently to complex porous media flows, including
high memory requirements and difficulty handling the time dimension. To address
these limitations, this work introduces hybrid neural operator surrogates based
on DeepONet models that integrate Fourier Neural Operators, Multi-Layer
Perceptrons (MLPs), and Kolmogorov-Arnold Networks (KANs) within their branch
and trunk networks. The proposed framework decouples spatial and temporal
learning tasks by splitting these structures into the branch and trunk
networks, respectively. We evaluate these hybrid models on multiphase flow in
porous media problems ranging in complexity from the steady 2D Darcy flow to
the 2D and 3D problems belonging to the $10$th Comparative Solution Project
from the Society of Petroleum Engineers. Results demonstrate that hybrid
schemes achieve accurate surrogate modeling with significantly fewer parameters
while maintaining strong predictive performance on large-scale reservoir
simulations.

</details>


### [21] [GraphCliff: Short-Long Range Gating for Subtle Differences but Critical Changes](https://arxiv.org/abs/2511.03170)
*Hajung Kim,Jueon Park,Junseok Choe,Sheunheun Baek,Hyeon Hwang,Jaewoo Kang*

Main category: cs.CE

TL;DR: 提出新模型GraphCliff处理活性悬崖问题，实验显示其性能提升。


<details>
  <summary>Details</summary>
Motivation: 经典机器学习模型在活性悬崖问题上优于图神经网络，图嵌入难以区分结构相似但功能不同分子，需新模型利用分子图结构优势。

Method: 提出GraphCliff模型，通过门控机制整合短程和长程信息。

Result: GraphCliff在非悬崖和悬崖化合物上均提升性能，减少过平滑并增强判别能力。

Conclusion: GraphCliff是处理活性悬崖问题的有效模型。

Abstract: Quantitative structure-activity relationship assumes a smooth relationship
between molecular structure and biological activity. However, activity cliffs
defined as pairs of structurally similar compounds with large potency
differences break this continuity. Recent benchmarks targeting activity cliffs
have revealed that classical machine learning models with extended connectivity
fingerprints outperform graph neural networks. Our analysis shows that graph
embeddings fail to adequately separate structurally similar molecules in the
embedding space, making it difficult to distinguish between structurally
similar but functionally different molecules. Despite this limitation,
molecular graph structures are inherently expressive and attractive, as they
preserve molecular topology. To preserve the structural representation of
molecules as graphs, we propose a new model, GraphCliff, which integrates
short- and long-range information through a gating mechanism. Experimental
results demonstrate that GraphCliff consistently improves performance on both
non-cliff and cliff compounds. Furthermore, layer-wise node embedding analyses
reveal reduced over-smoothing and enhanced discriminative power relative to
strong baseline graph models.

</details>


### [22] [A Theoretical Framework for Environmental Similarity and Vessel Mobility as Coupled Predictors of Marine Invasive Species Pathways](https://arxiv.org/abs/2511.03499)
*Gabriel Spadon,Vaishnav Vaidheeswaran,Claudio DiBacco*

Main category: cs.CE

TL;DR: 本文提出结合港口环境相似性与海上流动性量化海洋入侵物种入侵风险的理论框架，支持针对性管理。


<details>
  <summary>Details</summary>
Motivation: 传统海洋入侵物种风险评估因数据不完整限制全球覆盖，需新方法。

Method: 结合港口环境相似性与观测和预测的海上流动性，用气候特征表征港口海洋条件，用AIS数据构建移动网络，通过聚类和度量学习估计物种生存可能性，用时间链接预测模型捕捉交通模式变化。

Result: 融合环境相似性和预测的流动性，提供港口和航次层面的暴露估计。

Conclusion: 该理论框架可支持针对性监测、航线调整和管理干预。

Abstract: Marine invasive species spread through global shipping and generate
substantial ecological and economic impacts. Traditional risk assessments
require detailed records of ballast water and traffic patterns, which are often
incomplete, limiting global coverage. This work advances a theoretical
framework that quantifies invasion risk by combining environmental similarity
across ports with observed and forecasted maritime mobility. Climate-based
feature representations characterize each port's marine conditions, while
mobility networks derived from Automatic Identification System data capture
vessel flows and potential transfer pathways. Clustering and metric learning
reveal climate analogues and enable the estimation of species survival
likelihood along shipping routes. A temporal link prediction model captures how
traffic patterns may change under shifting environmental conditions. The
resulting fusion of environmental similarity and predicted mobility provides
exposure estimates at the port and voyage levels, supporting targeted
monitoring, routing adjustments, and management interventions.

</details>


### [23] [Simulation-Based Validation of an Integrated 4D/5D Digital-Twin Framework for Predictive Construction Control](https://arxiv.org/abs/2511.03684)
*Atena Khoshkonesh,Mohsen Mohammadagha,Navid Ebrahimi*

Main category: cs.CE

TL;DR: 研究提出集成4D/5D数字孪生框架，在项目中实现了估算和进度管理的提升，证实集成AI分析能增强预测能力。


<details>
  <summary>Details</summary>
Motivation: 美国建筑行业存在成本和进度偏差问题，传统方法有局限性。

Method: 提出集成4D/5D数字孪生框架，结合BIM、NLP、CV、贝叶斯概率CPM更新和DRL资源调配。

Result: 在项目中实现估算人工减少43%、加班减少6%、项目缓冲利用率30%，按时完成，实现实时预测和成本进度对齐。

Conclusion: 集成AI分析与概率CPM和DRL能提升预测精度、透明度和控制弹性，为建设管理提供实用途径。

Abstract: Persistent cost and schedule deviations remain a major challenge in the U.S.
construction industry, revealing the limitations of deterministic CPM and
static document-based estimating. This study presents an integrated 4D/5D
digital-twin framework that couples Building Information Modeling (BIM) with
natural-language processing (NLP)-based cost mapping, computer-vision
(CV)-driven progress measurement, Bayesian probabilistic CPM updating, and
deep-reinforcement-learning (DRL) resource-leveling. A nine-month case
implementation on a Dallas-Fort Worth mid-rise project demonstrated measurable
gains in accuracy and efficiency: 43% reduction in estimating labor, 6%
reduction in overtime, and 30% project-buffer utilization, while maintaining an
on-time finish at 128 days within P50-P80 confidence bounds. The digital-twin
sandbox also enabled real-time "what-if" forecasting and traceable
cost-schedule alignment through a 5D knowledge graph. Findings confirm that
integrating AI-based analytics with probabilistic CPM and DRL enhances
forecasting precision, transparency, and control resilience. The validated
workflow establishes a practical pathway toward predictive, adaptive, and
auditable construction management.

</details>


### [24] [Multi-Region Matrix Interpolation for Dynamic Analysis of Aperiodic Structures under Large Model Parameter Perturbations](https://arxiv.org/abs/2511.03711)
*J. Pereira,R. O. Ruiz*

Main category: cs.CE

TL;DR: 本文引入基于代理的模型估计动态机械超材料频率响应，解决现有方法局限性，提出三项改进并通过两个例子验证有效性。


<details>
  <summary>Details</summary>
Motivation: 现有基于Craig - Bampton模态缩减的矩阵插值方法有局限性，需共同模态投影限制了模型参数扰动大小和有效插值的参数空间。

Method: 提供结构动力学见解控制病态条件；提出基于采样的程序识别可用区域非规则边界；提出多区域插值策略以适应更大参数扰动。

Result: 通过两个例子验证框架有效性，多区域方法在不同扰动水平下保持高精度。

Conclusion: 提出的框架能有效解决现有方法局限性，提高估计动态机械超材料频率响应的能力。

Abstract: This work introduces a surrogate-based model for efficiently estimating the
frequency response of dynamic mechanical metamaterials, particularly when
dealing with large parametric perturbations and aperiodic substructures. The
research builds upon a previous matrix interpolation method applied on top of a
Craig-Bampton modal reduction, allowing the variations of geometrical features
without the need to remesh and recompute Finite Element matrices. This existing
procedure has significant limitations since it requires a common modal
projection, which inherently restricts the allowable perturbation size of the
model parameters, thereby limiting the model parameter space where matrices can
be effectively interpolated. The present work offers three contributions: (1)
It provides structural dynamic insight into the restrictions imposed by the
common modal projection, demonstrating that ill-conditioning can be controlled,
(2) it proposes an efficient, sampling-based procedure to identify the
non-regular boundaries of the usable region in the model parameter space, and
(3) it enhances the surrogate model to accommodate larger model parameter
perturbations by proposing a multi-region interpolation strategy. The efficacy
of this proposed framework is verified through two illustrative examples. The
first example, involving a unit cell with a square plate and circular core,
validates the approach for a single well-conditioned projection region. The
second example, using a beam-like structure with vibration attenuation bands,
demonstrates the true advantage of the multi-region approach, where predictions
from traditional Lagrange interpolation deviated significantly with increasing
perturbations, while the proposed method maintained high accuracy across
different perturbation levels.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [25] [Formalizing ETLT and ELTL Design Patterns and Proposing Enhanced Variants: A Systematic Framework for Modern Data Engineering](https://arxiv.org/abs/2511.03393)
*Chiara Rucco,Motaz Saad,Antonella Longo*

Main category: cs.DB

TL;DR: 传统ETL和ELT模式难以满足现代需求，本文将ETLT和ELTL形式化为设计模式，并提出增强变体ETLT++和ELTL++，为数据工程提供基础。


<details>
  <summary>Details</summary>
Motivation: 传统ETL和ELT设计模式无法满足现代要求，且ETLT和ELTL缺乏最佳实践和正式认可。

Method: 在设计模式框架内系统定义ETLT和ELTL模式，通过嵌入明确契约、版本控制等将其扩展为ETLT++和ELTL++。

Result: 提出的框架为从业者提供构建可审计、可扩展和经济高效管道的结构化路线图。

Conclusion: 形式化ETLT和ELTL并增强为ETLT++和ELTL++，弥合了临时实践和系统设计的差距，为现代数据工程提供可复用基础。

Abstract: Traditional ETL and ELT design patterns struggle to meet modern requirements
of scalability, governance, and real-time data processing. Hybrid approaches
such as ETLT (Extract-Transform-Load-Transform) and ELTL
(Extract-Load-Transform-Load) are already used in practice, but the literature
lacks best practices and formal recognition of these approaches as design
patterns. This paper formalizes ETLT and ELTL as reusable design patterns by
codifying implicit best practices and introduces enhanced variants, ETLT++ and
ELTL++, to address persistent gaps in governance, quality assurance, and
observability. We define ETLT and ELTL patterns systematically within a design
pattern framework, outlining their structure, trade-offs, and use cases.
Building on this foundation, we extend them into ETLT++ and ELTL++ by embedding
explicit contracts, versioning, semantic curation, and continuous monitoring as
mandatory design obligations. The proposed framework offers practitioners a
structured roadmap to build auditable, scalable, and cost-efficient pipelines,
unifying quality enforcement, lineage, and usability across multi-cloud and
real-time contexts. By formalizing ETLT and ELTL, and enhancing them through
ETLT++ and ELTL++, this work bridges the gap between ad hoc practice and
systematic design, providing a reusable foundation for modern, trustworthy data
engineering.

</details>


### [26] [HERP: Hardware for Energy Efficient and Realtime DB Search and Cluster Expansion in Proteomics](https://arxiv.org/abs/2511.03437)
*Md Mizanur Rahaman Nayan,Zheyu Li,Flavio Ponzina,Sumukh Pinge,Tajana Rosing,Azad J. Naeemi*

Main category: cs.DB

TL;DR: 提出适用于资源受限环境的轻量级增量聚类和高度可并行化数据库搜索平台，低能耗低延迟且性能不打折。


<details>
  <summary>Details</summary>
Motivation: 传统全聚类和搜索方法资源需求高、延迟长，需要适用于资源受限环境的方案。

Method: 利用质谱见解，采用桶级并行化和查询调度，结合预聚类蛋白质组学数据进行一次性硬件初始化，以启发式方法指导增量聚类，硬件采用7nm节点的3T 2M T J SOT - CAM和存内计算设计。

Result: 增量聚类加速20倍，聚类误差仅增加0.3%；数据库搜索结果与最先进工具重叠率达96%；人类基因组草案数据集设置能耗低，桶级并行化实现100倍加速。

Conclusion: 该平台是一种更实用高效的替代从头聚类的方案。

Abstract: Database (DB) search and clustering are fundamental in proteomics but
conventional full clustering and search approaches demand high resources and
incur long latency. We propose a lightweight incremental clustering and highly
parallelizable DB search platform tailored for resource-constrained
environments, delivering low energy and latency without compromising
performance. By leveraging mass-spectrometry insights, we employ bucket-wise
parallelization and query scheduling to reduce latency. A one-time hardware
initialization with pre-clustered proteomics data enables continuous DB search
and local re-clustering, offering a more practical and efficient alternative to
clustering from scratch. Heuristics from pre-clustered data guide incremental
clustering, accelerating the process by 20x with only a 0.3% increase in
clustering error. DB search results overlap by 96% with state-of-the-art tools,
validating search quality. The hardware leverages a 3T 2M T J SOT-CAM at the
7nm node with a compute-in-memory design. For the human genome draft dataset
(131GB), setup requires 1.19mJ for 2M spectra, while a 1000 query search
consumes 1.1{\mu}J. Bucket-wise parallelization further achieves 100x speedup.

</details>


### [27] [In-Memory Indexing and Querying of Provenance in Data Preparation Pipelines](https://arxiv.org/abs/2511.03480)
*Khalid Belhajjame,Haroun Mezrioui,Yuyan Zhao*

Main category: cs.DB

TL;DR: 本文提出一种索引机制来高效捕获和查询管道来源，利用张量以最小内存捕获细粒度数据处理操作来源，并结合回顾性和前瞻性来源信息，通过评估证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 数据来源在数据准备管道中有诸多应用，需高效的捕获和查询管道来源的方法。

Method: 利用张量捕获数据处理操作的细粒度来源，结合回顾性和前瞻性来源信息，在属性级别提供更细粒度。

Result: 能结合两种类型的来源信息高效回答广泛的来源查询，通过真实和合成数据评估证明了有效性。

Conclusion: 所提出的索引机制能有效捕获和查询管道来源。

Abstract: Data provenance has numerous applications in the context of data preparation
pipelines. It can be used for debugging faulty pipelines, interpreting results,
verifying fairness, and identifying data quality issues, which may affect the
sources feeding the pipeline execution. In this paper, we present an indexing
mechanism to efficiently capture and query pipeline provenance. Our solution
leverages tensors to capture fine-grained provenance of data processing
operations, using minimal memory. In addition to record-level lineage
relationships, we provide finer granularity at the attribute level. This is
achieved by augmenting tensors, which capture retrospective provenance, with
prospective provenance information, drawing connections between input and
output schemas of data processing operations. We demonstrate how these two
types of provenance (retrospective and prospective) can be combined to answer a
broad range of provenance queries efficiently, and show effectiveness through
evaluation exercises using both real and synthetic data.

</details>


### [28] [Analytical Queries for Unstructured Data](https://arxiv.org/abs/2511.03489)
*Daniel Kang*

Main category: cs.DB

TL;DR: 本文探讨利用机器学习处理非结构化数据管理系统的挑战与进展，以视频分析为例。


<details>
  <summary>Details</summary>
Motivation: 非结构化数据增长迅速，机器学习分析能力增强，但部署技术存在挑战，故探讨相关挑战与进展。

Method: 介绍用户通过用户定义函数、标准结构化模式、提供示例表达查询；通过近似昂贵方法优化查询；应用异常和漂移检测处理错误。

Result: 用户可通过多种方式表达查询，能对查询进行优化，可处理机器学习模型中的错误。

Conclusion: 数据管理社区的近期工作在应对利用机器学习处理非结构化数据的挑战上取得进展。

Abstract: Unstructured data, in the form of text, images, video, and audio, is produced
at exponentially higher rates. In tandem, machine learning (ML) methods have
become increasingly powerful at analyzing unstructured data. Modern ML methods
can now detect objects in images, understand actions in videos, and even
classify complex legal texts based on legal intent. Combined, these trends make
it increasingly feasible for analysts and researchers to automatically
understand the "real world." However, there are major challenges in deploying
these techniques: 1) executing queries efficiently given the expense of ML
methods, 2) expressing queries over bespoke forms of data, and 3) handling
errors in ML methods.
  In this monograph, we discuss challenges and advances in data management
systems for unstructured data using ML, with a particular focus on video
analytics. Using ML to answer queries introduces new challenges.First, even
turning user intent into queries can be challenging: it is not obvious how to
express a query of the form "select instances of cars turning left." Second, ML
models can be orders of magnitude more expensive compared to processing
traditional structured data. Third, ML models and the methods to accelerate
analytics with ML models can be error-prone.
  Recent work in the data management community has aimed to address all of
these challenges. Users can now express queries via user-defined functions,
opaquely through standard structured schemas, and even by providing examples.
Given a query, recent work focuses on optimizing queries by approximating
expensive "gold" methods with varying levels of guarantees. Finally, to handle
errors in ML models, recent work has focused on applying outlier and drift
detection to data analytics with ML.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [29] [Harvesting energy consumption on European HPC systems: Sharing Experience from the CEEC project](https://arxiv.org/abs/2511.03029)
*Kajol Kulkarni,Samuel Kemmler,Anna Schwarz,Gulcin Gedik,Yanxiang Chen,Dimitrios Papageorgiou,Ioannis Kavroulakis,Roman Iakymchuk*

Main category: cs.DC

TL;DR: 本文介绍欧洲高性能计算中心在测量、分析和优化欧洲主要高性能计算系统能耗方面的经验，通过案例研究评估了不同架构的能耗与计算时间指标，强调加速器和混合精度技术优势，倡导促进能耗测量以实现可持续超算。


<details>
  <summary>Details</summary>
Motivation: 现代高性能计算系统面临能耗挑战，计算需求和架构复杂性导致显著的能源足迹。

Method: 回顾能耗测量的关键方法和工具，定义报告结果的指标，通过代表性CFD应用案例研究，在不同架构上评估能耗与计算时间指标。

Result: 结果凸显了加速器和混合精度技术在降低能耗同时保持计算精度方面的优势。

Conclusion: 倡导促进高性能计算系统的能耗测量，以提高意识、教导社区并推动可持续超算。

Abstract: Energy efficiency has emerged as a central challenge for modern
high-performance computing (HPC) systems, where escalating computational
demands and architectural complexity have led to significant energy footprints.
This paper presents the collective experience of the EuroHPC JU Center of
Excellence in Exascale CFD (CEEC) in measuring, analyzing, and optimizing
energy consumption across major European HPC systems. We briefly review key
methodologies and tools for energy measurement as well as define metrics for
reporting results. Through case studies using representative CFD applications
(waLBerla, FLEXI/GAL{\AE}XI, Neko, and NekRS), we evaluate energy-to-solution
and time-to-solution metrics on diverse architectures, including CPU- and
GPU-based partitions of LUMI, MareNostrum5, MeluXina, and JUWELS Booster. Our
results highlight the advantages of accelerators and mixed-precision techniques
for reducing energy consumption while maintaining computational accuracy.
Finally, we advocate the need to facilitate energy measurements on HPC systems
in order to raise awareness, teach the community, and take actions toward more
sustainable exascale computing.

</details>


### [30] [Characterising Global Platforms: Centralised, Decentralised, Federated, and Grassroots](https://arxiv.org/abs/2511.03286)
*Ehud Shapiro*

Main category: cs.DC

TL;DR: 提出基于原子事务的多智能体转换系统和协议作为研究全球数字平台的形式框架，将平台分为四类并给出示例和证明，提供首个对全球平台分类的数学框架。


<details>
  <summary>Details</summary>
Motivation: 研究全球数字平台，对其进行分类和形式化研究。

Method: 提出基于原子事务的多智能体转换系统和协议，引入必要智能体概念，通过多智能体原子事务给出示例并证明正确性。

Result: 将全球平台分为集中式、分布式、联邦式和草根式四类，证明示例满足基本正确性属性，证明草根平台定义使其所有智能体都是必要的。

Conclusion: 提供首个对全球平台分类的数学框架，可对现有或想象的平台进行分类。

Abstract: Global digital platforms are software systems designed to serve entire
populations, with some already serving billions of people. We propose atomic
transactions-based multiagent transition systems and protocols as a formal
framework to study them; introduce essential agents -- minimal sets of agents
the removal of which makes communication impossible; and show that the
cardinality of essential agents partitions all global platforms into four
classes:
  1. Centralised -- one (the server)
  2. Decentralised -- finite $>1$ (bootstrap nodes)
  3. Federated -- infinite but not universal (all servers)
  4. Grassroots -- universal (all agents)
  Our illustrative formal example is a global social network, for which we
provide centralised, decentralised, federated, and grassroots specifications
via multiagent atomic transactions, and prove they satisfy basic correctness
properties. We discuss informally additional global platforms -- currencies,
``sharing economy'' apps, AI, and more. While this may be the first
characterisation of centralised, decentralised, and federated global platforms,
grassroots platforms have been formally defined previously, but using different
notions. Here, we prove that their original definition implies that all agents
are essential, placing grassroots platforms in a distinct class within the
broader formal context that includes all global platforms. This work provides
the first mathematical framework for classifying any global platform --
existing or imagined -- by providing a multiagent atomic-transactions
specification of it and determining the cardinality of the minimal set of
essential agents in the ensuing multiagent protocol. It thus

</details>


### [31] [UMDAM: A Unified Data Layout and DRAM Address Mapping for Heterogenous NPU-PIM](https://arxiv.org/abs/2511.03293)
*Hai Huang,Xuhong Qiang,Weisheng Zhao,Chenchen Liu*

Main category: cs.DC

TL;DR: 提出UMDAM方案解决NPU - PIM系统问题，提升边缘设备大语言模型推理效率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在边缘设备NPU上解码阶段内存需求大，NPU - PIM系统存在数据布局不匹配、带宽损失和冗余存储等问题。

Method: 提出UMDAM，采用列主序、基于图块的布局和可配置的DRAM映射策略。

Result: 在OPT模型上评估，UMDAM使TTFT最多降低3.0倍，TTLT降低2.18倍。

Conclusion: UMDAM显著提高了边缘设备端到端大语言模型推理效率。

Abstract: Large Language Models (LLMs) are increasingly deployed on edge devices with
Neural Processing Units (NPUs), yet the decode phase remains memory-intensive,
limiting performance. Processing-in-Memory (PIM) offers a promising solution,
but co-executing NPU-PIM systems face challenges such as data layout
mismatches, bandwidth loss, and redundant storage. To address these issues, we
propose UMDAM, a unified memory-affinity data layout and DRAM address mapping
scheme tailored for NPU-PIM co-execution. UMDAM employs a column-major,
tile-based layout and a configurable DRAM mapping strategy to ensure
compatibility with NPU computation while maximizing PIM efficiency -- without
introducing extra memory overhead or bandwidth loss. Comprehensive evaluations
on OPT models demonstrate that UMDAM reduces time-to-first-token (TTFT) by up
to 3.0x and time-to-last-token (TTLT) by 2.18x, significantly improving
end-to-end LLM inference efficiency on edge devices.

</details>


### [32] [Investigating the Impact of Isolation on Synchronized Benchmarks](https://arxiv.org/abs/2511.03533)
*Nils Japke,Furat Hamdan,Diana Baumann,David Bermbach*

Main category: cs.DC

TL;DR: 云环境基准测试受多租户资源竞争影响，Duet基准测试可缓解但需隔离机制，评估三种策略，除Docker容器外，进程隔离可降低误报率。


<details>
  <summary>Details</summary>
Motivation: 解决云环境基准测试中因多租户资源竞争导致的性能可变性问题。

Method: 评估cgroups和CPU pinning、Docker容器、Firecracker MicroVMs三种隔离策略，与无隔离基线实验对比，通过噪声发生器干扰。

Result: 各实验在噪声影响下延迟分布不同，除Docker容器外，进程隔离降低误报率，Docker容器更易受噪声影响。

Conclusion: 建议同步工作负载使用进程隔离，不使用Docker容器。

Abstract: Benchmarking in cloud environments suffers from performance variability from
multi-tenant resource contention. Duet benchmarking mitigates this by running
two workload versions concurrently on the same VM, exposing them to identical
external interference. However, intra-VM contention between synchronized
workloads necessitates additional isolation mechanisms.
  This work evaluates three such strategies: cgroups and CPU pinning, Docker
containers, and Firecracker MicroVMs. We compare all strategies with an
unisolated baseline experiment, by running benchmarks with a duet setup
alongside a noise generator. This noise generator "steals" compute resources to
degrade performance measurements.
  All experiments showed different latency distributions while under the
effects of noise generation, but results show that process isolation generally
lowered false positives, except for our experiments with Docker containers.
Even though Docker containers rely internally on cgroups and CPU pinning, they
were more susceptible to performance degradation due to noise influence.
Therefore, we recommend to use process isolation for synchronized workloads,
with the exception of Docker containers.

</details>


### [33] [Stone Duality Proofs for Colorless Distributed Computability Theorems](https://arxiv.org/abs/2511.03609)
*Cameron Calk,Emmanuel Godard*

Main category: cs.DC

TL;DR: 引入基于轮次的全信息对手执行谱空间的拓扑编码，给出无色任务可解性表征，统一分布式计算拓扑方法，发现有色和无色模型计算能力相同。


<details>
  <summary>Details</summary>
Motivation: 统一分布式计算中的拓扑方法，研究无色任务在紧致对手下的可解性。

Method: 将分布式协议有限执行后的全局状态视为谱空间，利用Stone对偶性定义极限对象，推导分布式可计算性定理。

Result: 得到无色任务可解性的一般表征，可推导出许多已知的无色可计算性定理，发现有色和无色模型计算能力相同。

Conclusion: 新的拓扑编码和定理是统一分布式计算拓扑方法的重要一步，为有色和无色模型计算能力等价提供拓扑解释。

Abstract: We introduce a new topological encoding by spectral spaces of executions of
  round-based full-information adversaries, a model of distributed computations
that is functorially presented and that
  contains many message adversaries. We give a characterization of the
solvability of colorless tasks against compact adversaries.
  Message adversaries are distributed
  models that are known to be very expressive despite being
  round-based and crash-free. Colorless tasks are
  an important class of distributed tasks. For a colorless task, the
  specification does not depend upon the multiplicity of input or
  output values, like the ubiquitous agreement tasks.
  Therefore, our result is a significant
  step toward unifying topological methods in distributed computing.
  The main insight is to consider global states obtained after finite
executions of a distributed protocol
  not as abstract
  simplicial complexes as previously done, but as spectral
  spaces, considering the Alexandrov topology on the faces poset. Given
  an adversary $\mathcal M$ with a set of inputs $\mathcal I$,
  we define a limit object $\Pi^\infty_\mathcal M(\mathcal I)$
  by projective limit in the category of spectral spaces. We derive a new
general distributed computability
  theorem using Stone duality: there exists an algorithm solving a colorless
task $(\mathcal I,\mathcal O,\Delta)$
  against the compact adversary $\mathcal M$ if and only if there exists a
spectral
  map $f:\Pi^\infty_\mathcal M(\mathcal I)\longrightarrow\mathcal O$ compatible
with $\Delta$.
  From this general characterization are derived many known colorless
computability
  theorems.
  Quite surprisingly, colored and uncolored models have the same
  computability power (they solve the same tasks). Our new proofs give
  topological reasons for this equivalence, previously known through
  algorithmic reductions.

</details>


### [34] [A General Input-Dependent Colorless Computability Theorem and Applications to Core-Dependent Adversaries](https://arxiv.org/abs/2511.03662)
*Yannis Coutouly,Emmanuel Godard*

Main category: cs.DC

TL;DR: 本文将已有成果推广到输入依赖型对手，证明特定对手计算能力相同，给出解决k - Set协议的充要条件，区分任务表示的四种设置并给出载体映射结构性质。


<details>
  <summary>Details</summary>
Motivation: 已有对分布式计算任务可解性的研究，本文旨在进一步推广已有成果，研究输入依赖型对手、不同对手计算能力及k - Set协议可解性的充要条件等。

Method: 使用拓扑框架和简单几何构造进行证明。

Result: 推广了已有成果，证明核心弹性对手计算能力相同，给出k - Set协议可解性充要条件，区分任务表示的四种设置，给出载体映射结构性质。

Conclusion: 通过研究输入依赖型对手等内容，为分布式计算任务可解性提供了更全面的理论结果，利用拓扑和几何方法简化证明。

Abstract: Distributed computing tasks can be presented with a triple $(\I,\Ou,\Delta)$.
The solvability of a colorless task on the Iterated Immediate Snapshot model
(IIS) has been characterized by the Colorless Computability Theorem
\cite[Th.4.3.1]{HKRbook}. A recent paper~\cite{CG-24} generalizes this theorem
for any message adversaries $\ma \subseteq IIS$ by geometric methods. In 2001,
Most\'efaoui, Rajsbaum, Raynal, and Roy \cite{condbased} introduced
\emph{condition-based adversaries}. This setting considers a particular
adversary that will be applied only to a subset of input configurations. In
this setting, they studied the $k$-set agreement task with condition-based
$t$-resilient adversaries and obtained a sufficient condition on the conditions
that make $k$-Set Agreement solvable. In this paper we have three
contributions:
  -We generalize the characterization of~\cite{CG-24} to \emph{input-dependent}
adversaries, which means that the adversaries can change depending on the input
configuration.
  - We show that core-resilient adversaries of $IIS_n$ have the same
computability power as the core-resilient adversaries of $IIS_n$ where crashes
only happen at the start.
  - Using the two previous contributions, we provide a necessary and sufficient
characterization of the condition-based, core-dependent adversaries that can
solve $k$-Set Agreement. We also distinguish four settings that may appear when
presenting a distributed task as $(\I,\Ou,\Delta)$. Finally, in a later
section, we present structural properties on the carrier map $\Delta$. Such
properties allow simpler proof, without changing the computability power of the
task. Most of the proofs in this article leverage the topological framework
used in distributed computing by using simple geometric constructions.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [35] [Faster Weak Expander Decompositions and Approximate Max Flow](https://arxiv.org/abs/2511.02943)
*Henry Fleischmann,George Z. Li,Jason Li*

Main category: cs.DS

TL;DR: 本文给出无向图弱扩张器分解和近似最大流的更快算法，一是优化弱扩张器分解算法，二是改进近似最大流算法。


<details>
  <summary>Details</summary>
Motivation: 为无向图的弱扩张器分解和近似最大流问题找到更快的算法。

Method: 在计算弱扩张器分解时采用“热启动”方法，避免递归深度成本，还简化Li等人的非递归近似最大流算法并适配新的弱扩张器分解原语。

Result: 得到了更快的弱扩张器分解算法，以及在扩张器分解方法极限的几个对数因子内的近似最大流算法。

Conclusion: 通过新方法可以提升无向图弱扩张器分解和近似最大流问题的计算效率。

Abstract: We give faster algorithms for weak expander decompositions and approximate
max flow on undirected graphs. First, we show that it is possible to "warm
start" the cut-matching game when computing weak expander decompositions,
avoiding the cost of the recursion depth. Our algorithm is also flexible enough
to support weaker flow subroutines than previous algorithms.
  Our second contribution is to streamline the recent non-recursive approximate
max flow algorithm of Li, Rao, and Wang (SODA, 2025) and adapt their framework
to use our new weak expander decomposition primitive. Consequently, we give an
approximate max flow algorithm within a few logarithmic factors of the limit of
expander decomposition-based approaches.

</details>


### [36] [Tight Better-Than-Worst-Case Bounds for Element Distinctness and Set Intersection](https://arxiv.org/abs/2511.02954)
*Ivor van der Hoog,Eva Rotenberg,Daniel Rutschmann*

Main category: cs.DS

TL;DR: 本文研究元素唯一性和集合交集问题，推导实例特定的下界，证明元素唯一性无o(log log n)竞争确定性算法，有O(log log n)竞争算法；集合交集无o(log n)竞争算法，有O(log n)竞争算法。


<details>
  <summary>Details</summary>
Motivation: 经典元素唯一性问题的Ω(n log n)下界不适用于有大量重复元素的输入，因此探索对输入中重复元素数量敏感的比较型下界。

Method: 用无向图表示输入实例中重复元素的组合结构，研究算法在同构图输入上的最坏情况运行时间，建立对抗性下界。

Result: 元素唯一性问题：无o(log log n)竞争的确定性算法，有O(log log n)竞争的确定性算法；集合交集问题：无o(log n)竞争的确定性算法，有O(log n)竞争的确定性算法。

Conclusion: 获得了超越经典最坏情况分析的元素唯一性问题的紧下界，展示了元素唯一性和集合交集问题之间的分离。

Abstract: The element distinctness problem takes as input a list $I$ of $n$ values from
a totally ordered universe and the goal is to decide whether $I$ contains any
duplicates. It is a well-studied problem with a classical worst-case $\Omega(n
\log n)$ comparison-based lower bound by Fredman. At first glance, this lower
bound appears to rule out any algorithm more efficient than the naive approach
of sorting $I$ and comparing adjacent elements. However, upon closer
inspection, the $\Omega(n \log n)$ bound does not apply if the input has many
duplicates. We therefore ask: Are there comparison-based lower bounds for
element distinctness that are sensitive to the amount of duplicates in the
input?
  To address this question, we derive instance-specific lower bounds. For any
input instance $I$, we represent the combinatorial structure of the duplicates
in $I$ by an undirected graph $G(I)$ that connects identical elements. Each
such graph $G$ is a union of cliques, and we study algorithms by their
worst-case running time over all inputs $I'$ with $G(I') \cong G$. We establish
an adversarial lower bound showing that, for any deterministic algorithm
$\mathcal{A}$, there exists a graph $G$ and an algorithm $\mathcal{A}'$ that,
for all inputs $I$ with $G(I) \cong G$, is a factor $O(\log \log n)$ faster
than $\mathcal{A}$. Consequently, no deterministic algorithm can be $o(\log
\log n)$-competitive for all graphs $G$. We complement this with an $O(\log
\log n)$-competitive deterministic algorithm, thereby obtaining tight bounds
for element distinctness that go beyond classical worst-case analysis.
  We subsequently study the related problem of set intersection. We show that
no deterministic set intersection algorithm can be $o(\log n)$-competitive, and
provide an $O(\log n)$-competitive deterministic algorithm. This shows a
separation between element distinctness and the set intersection problem.

</details>


### [37] [Implementation and Brief Experimental Analysis of the Duan et al. (2025) Algorithm for Single-Source Shortest Paths](https://arxiv.org/abs/2511.03007)
*Lucas Castro,Thailsson Clementino,Rosiane de Freitas*

Main category: cs.DS

TL;DR: 本文实现并实验分析Duan等人2025年提出的单源最短路径（SSSP）问题确定性算法，与经典Dijkstra算法对比，新算法虽渐近复杂度优，但常数因子大。


<details>
  <summary>Details</summary>
Motivation: 实现并分析Duan等人提出的具有最优渐近上界的单源最短路径问题算法，并与经典Dijkstra算法对比性能。

Method: 忠实实现原算法的C++代码，在合成稀疏随机图和真实世界道路网络实例上与使用二叉堆的Dijkstra算法进行实验对比。

Result: 新算法虽渐近复杂度优，但常数因子大，Dijkstra算法在所有测试的稀疏图规模上更快。

Conclusion: 新算法虽理论复杂度好，但实际性能受常数因子影响，当前实现达到期望时间复杂度，正考虑使其达到最坏情况复杂度。

Abstract: We present an implementation and a brief experimental analysis of the
deterministic algorithm proposed by Duan et al. (2025) for the Single-Source
Shortest Path (SSSP) problem, which achieves the best known asymptotic upper
bound in the comparison-addition model, with running time $O(m \log^{2/3} n)$.
We provide a faithful C++ implementation of this algorithm, following all
structural details described in the original paper, and compare its empirical
performance with the classical Dijkstra's algorithm using binary heaps. The
experiments were conducted on both synthetic sparse random graphs and
real-world road network instances from the DIMACS benchmark. Our results show
that, despite its superior asymptotic complexity, the new algorithm presents
significantly larger constant factors, making Dijkstra's algorithm faster for
all tested sparse graph sizes, including instances with tens of millions of
vertices. Our implementation achieves $O(m \log^{2/3} n)$ expected time, due to
the use of hash tables, and some possibilities for making it worst-case are
being considered. (This is a ongoing work.)

</details>


### [38] [A Branch-and-Bound Approach for Maximum Low-Diameter Dense Subgraph Problems](https://arxiv.org/abs/2511.03157)
*Yi Zhoua,Chunyu Luoa,Zhengren Wangb,Zhang-Hua Fuc*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: A graph with $n$ vertices is an $f(\cdot)$-dense graph if it has at least
$f(n)$ edges, $f(\cdot)$ being a well-defined function. The notion
$f(\cdot)$-dense graph encompasses various clique models like $\gamma$-quasi
cliques, $k$-defective cliques, and dense cliques, arising in cohesive subgraph
extraction applications. However, the $f(\cdot)$-dense graph may be
disconnected or weakly connected. To conquer this, we study the problem of
finding the largest $f(\cdot)$-dense subgraph with a diameter of at most two in
the paper. Specifically, we present a decomposition-based branch-and-bound
algorithm to optimally solve this problem. The key feature of the algorithm is
a decomposition framework that breaks the graph into $n$ smaller subgraphs,
allowing independent searches in each subgraph. We also introduce decomposition
strategies including degeneracy and two-hop degeneracy orderings, alongside a
branch-and-bound algorithm with a novel sorting-based upper bound to solve each
subproblem. Worst-case complexity for each component is provided. Empirical
results on 139 real-world graphs under two $f(\cdot)$ functions show our
algorithm outperforms the MIP solver and pure branch-and-bound, solving nearly
twice as many instances optimally within one hour.

</details>


### [39] [Optimal Stopping with a Predicted Prior](https://arxiv.org/abs/2511.03289)
*Tian Bai,Zhiyi Huang,Chui Shan Lee,Dongchen Li*

Main category: cs.DS

TL;DR: 论文针对最优停止问题中价值不确定性模型，提出带预测先验的最优停止模型，设计兼顾一致性和鲁棒性的算法，展示双准则算法能改善权衡，并证明特定目标下无法同时达到先知不等式算法和秘书算法最优。


<details>
  <summary>Details</summary>
Motivation: 最优停止文献中两种主要价值不确定性模型与实际决策中依赖可能错误的机器学习先验存在差距。

Method: 提出带预测先验的最优停止模型，设计双准则算法。

Result: 双准则算法在最大化期望接受值和接受最大值概率上实现了更好的一致性 - 鲁棒性权衡，且证明特定目标下无法同时达到先知不等式算法和秘书算法最优。

Conclusion: 双准则算法能改善一致性 - 鲁棒性权衡，在特定目标下存在算法性能的限制。

Abstract: There are two major models of value uncertainty in the optimal stopping
literature: the secretary model, which assumes no prior knowledge, and the
prophet inequality model, which assumes full information about value
distributions. In practice, decision makers often rely on machine-learned
priors that may be erroneous. Motivated by this gap, we formulate the model of
optimal stopping with a predicted prior to design algorithms that are both
consistent, exploiting the prediction when accurate, and robust, retaining
worst-case guarantees when it is not.
  Existing secretary and prophet inequality algorithms are either pessimistic
in consistency or not robust to misprediction. A randomized combination only
interpolates their guarantees linearly. We show that a family of bi-criteria
algorithms achieves improved consistency-robustness trade-offs, both for
maximizing the expected accepted value and for maximizing the probability of
accepting the maximum value. We further prove that for the latter objective, no
algorithm can simultaneously match the best prophet inequality algorithm in
consistency, and the best secretary algorithm in robustness.

</details>


### [40] [Improved Online Load Balancing in the Two-Norm](https://arxiv.org/abs/2511.03345)
*Sander Borst,Danish Kashaev*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study the online load balancing problem on unrelated machines, with the
objective of minimizing the square of the $\ell_2$ norm of the loads on the
machines. The greedy algorithm of Awerbuch et al. (STOC'95) is optimal for
deterministic algorithms and achieves a competitive ratio of $3 + 2 \sqrt{2}
\approx 5.828$, and an improved $5$-competitive randomized algorithm based on
independent rounding has been shown by Caragiannis (SODA'08). In this work, we
present the first algorithm breaking the barrier of $5$ on the competitive
ratio, achieving a bound of $4.9843$. To obtain this result, we use a new
primal-dual framework to analyze this problem based on a natural semidefinite
programming relaxation, together with an online implementation of a correlated
randomized rounding procedure of Im and Shadloo (SODA'20). This novel
primal-dual framework also yields new, simple and unified proofs of the
competitive ratio of the $(3 + 2 \sqrt{2})$-competitive greedy algorithm, the
$5$-competitive randomized independent rounding algorithm, and that of a new
$4$-competitive optimal fractional algorithm. We also provide lower bounds
showing that the previous best randomized algorithm is optimal among
independent rounding algorithms, that our new fractional algorithm is optimal,
and that a simple greedy algorithm is optimal for the closely related online
scheduling problem $R || \sum w_j C_j$.

</details>


### [41] [Hesse's Redemption: Efficient Convex Polynomial Programming](https://arxiv.org/abs/2511.03440)
*Lucas Slot,David Steurer,Manuel Wiedmer*

Main category: cs.DS

TL;DR: 本文针对凸多项式编程无线性表征时的解界问题，提出新技术证明解界，结合椭球法得到首个凸多项式编程的多项式时间算法。


<details>
  <summary>Details</summary>
Motivation: 凸优化高效算法需先验解界，线性和凸二次规划有经典方法确定解界，半定规划存在解系数过大问题，四次及更高次凸多项式无约束最小化处于两者之间，此前是未解决问题。

Method: 开发新技术在无线性表征时证明解界。

Result: 证明存在最优解时，存在多项式比特长度的近似最优解，结合椭球法得到凸多项式编程的多项式时间算法。

Conclusion: 解决了Nesterov提出的问题，得到凸多项式编程的首个多项式时间算法。

Abstract: Efficient algorithms for convex optimization, such as the ellipsoid method,
require an a priori bound on the radius of a ball around the origin guaranteed
to contain an optimal solution if one exists. For linear and convex quadratic
programming, such solution bounds follow from classical characterizations of
optimal solutions by systems of linear equations. For other programs, e.g.,
semidefinite ones, examples due to Khachiyan show that optimal solutions may
require huge coefficients with an exponential number of bits, even if we allow
approximations. Correspondingly, semidefinite programming is not even known to
be in NP. The unconstrained minimization of convex polynomials of degree four
and higher has remained a fundamental open problem between these two extremes:
its optimal solutions do not admit a linear characterization and, at the same
time, Khachiyan-type examples do not apply. We resolve this problem by
developing new techniques to prove solution bounds when no linear
characterizations are available. Even for programs minimizing a convex
polynomial (of arbitrary degree) over a polyhedron, we prove that the existence
of an optimal solution implies that an approximately optimal one with
polynomial bit length also exists. These solution bounds, combined with the
ellipsoid method, yield the first polynomial-time algorithm for convex
polynomial programming, settling a question posed by Nesterov (Math. Program.,
2019). Before, no polynomial-time algorithm was known even for unconstrained
minimization of a convex polynomial of degree four.

</details>


### [42] [Dynamic Meta-Kernelization](https://arxiv.org/abs/2511.03461)
*Christian Bertram,Deborah Haun,Mads Vestergaard Jensen,Tuukka Korhonen*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Kernelization studies polynomial-time preprocessing algorithms. Over the last
20 years, the most celebrated positive results of the field have been linear
kernels for classical NP-hard graph problems on sparse graph classes. In this
paper, we lift these results to the dynamic setting.
  As the canonical example, Alber, Fellows, and Niedermeier [J. ACM 2004] gave
a linear kernel for dominating set on planar graphs. We provide the following
dynamic version of their kernel: Our data structure is initialized with an
$n$-vertex planar graph $G$ in $O(n \log n)$ amortized time, and, at
initialization, outputs a planar graph $K$ with $\mathrm{OPT}(K) =
\mathrm{OPT}(G)$ and $|K| = O(\mathrm{OPT}(G))$, where $\mathrm{OPT}(\cdot)$
denotes the size of a minimum dominating set. The graph $G$ can be updated by
insertions and deletions of edges and isolated vertices in $O(\log n)$
amortized time per update, under the promise that it remains planar. After each
update to $G$, the data structure outputs $O(1)$ updates to $K$, maintaining
$\mathrm{OPT}(K) = \mathrm{OPT}(G)$, $|K| = O(\mathrm{OPT}(G))$, and planarity
of $K$.
  Furthermore, we obtain similar dynamic kernelization algorithms for all
problems satisfying certain conditions on (topological-)minor-free graph
classes. Besides kernelization, this directly implies new dynamic
constant-approximation algorithms and improvements to dynamic FPT algorithms
for such problems.
  Our main technical contribution is a dynamic data structure for maintaining
an approximately optimal protrusion decomposition of a dynamic
topological-minor-free graph. Protrusion decompositions were introduced by
Bodlaender, Fomin, Lokshtanov, Penninkx, Saurabh, and Thilikos [J. ACM 2016],
and have since developed into a part of the core toolbox in kernelization and
parameterized algorithms.

</details>


### [43] [Online Flow Time Minimization: Tight Bounds for Non-Preemptive Algorithms](https://arxiv.org/abs/2511.03485)
*Yutong Geng,Enze Sun,Zonghan Yang,Yuhao Zhang*

Main category: cs.DS

TL;DR: 本文研究m台相同机器上最小化n个作业总流时间的经典在线调度问题，给出随机和确定性算法及对应下界，框架还扩展到kill - and - restart模型，也研究了n未知的情况。


<details>
  <summary>Details</summary>
Motivation: 先前工作所引用的单机器非抢占式确定性算法的Ω(n)下界，留下了非抢占式下随机性是否有帮助以及m≥2时最优在线确定性算法是什么等问题未解决。

Method: 提出多项式时间随机算法和确定性非抢占式算法，证明匹配的上下界，将框架扩展到kill - and - restart模型，并研究n未知的情况。

Result: 随机算法竞争比为Θ(√(n/m))，确定性算法竞争比为O(n/m² + √(n/m)log m)；kill - and - restart模型有算法竞争比为O(√(n/m))（m≥2）和Ω(n/log n)（m = 1）；n未知时kill - and - restart能打破O(n)壁垒，随机化单独使用不行。

Conclusion: 解决了非抢占式下随机性作用和m≥2时最优在线确定性算法问题，kill - and - restart在m≥2时即使n未知也有优势，随机化单独使用在n未知时无法突破O(n)竞争比。

Abstract: This paper studies the classical online scheduling problem of minimizing
total flow time for $n$ jobs on $m$ identical machines. Prior work often cites
the $\Omega(n)$ lower bound for non-preemptive algorithms to argue for the
necessity of preemption or resource augmentation, which shows the trivial
$O(n)$-competitive greedy algorithm is tight. However, this lower bound applies
only to \emph{deterministic} algorithms in the \emph{single-machine} case,
leaving several fundamental questions unanswered. Can randomness help in the
non-preemptive setting, and what is the optimal online deterministic algorithm
when $m \geq 2$? We resolve both questions. We present a polynomial-time
randomized algorithm with competitive ratio $\Theta(\sqrt{n/m})$ and prove a
matching randomized lower bound, settling the randomized non-preemptive setting
for every $m$. This also improves the best-known offline approximation ratio
from $O(\sqrt{n/m}\log(n/m))$ to $O(\sqrt{n/m})$. On the deterministic side, we
present a non-preemptive algorithm with competitive ratio
$O(n/m^{2}+\sqrt{n/m}\log m)$ and prove a nearly matching lower bound.
  Our framework also extends to the kill-and-restart model, where we reveal a
sharp transition of deterministic algorithms: we design an asymptotically
optimal algorithm with the competitive ratio $O(\sqrt{n/m})$ for $m\ge 2$, yet
establish a strong $\Omega(n/\log n)$ lower bound for $m=1$. Moreover, we show
that randomization provides no further advantage, as the lower bound coincides
with that of the non-preemptive setting.
  While our main results assume prior knowledge of $n$, we also investigate the
setting where $n$ is unknown. We show kill-and-restart is powerful enough to
break the $O(n)$ barrier for $m \geq 2$ even without knowing $n$. Conversely,
we prove randomization alone is insufficient, as no algorithm can achieve an
$o(n)$ competitive ratio in this setting.

</details>


### [44] [Randomized Rounding over Dynamic Programs](https://arxiv.org/abs/2511.03490)
*Etienne Bamas,Shi Li,Lars Rohwedder*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We show that under mild assumptions for a problem whose solutions admit a
dynamic programming-like recurrence relation, we can still find a solution
under additional packing constraints, which need to be satisfied approximately.
The number of additional constraints can be very large, for example, polynomial
in the problem size. Technically, we reinterpret the dynamic programming
subproblems and their solutions as a network design problem. Inspired by
techniques from, for example, the Directed Steiner Tree problem, we construct a
strong LP relaxation, on which we then apply randomized rounding. Our
approximation guarantees on the packing constraints have roughly the form of a
$(n^{\epsilon} \mathrm{polylog}\ n)$-approximation in time $n^{O(1/\epsilon)}$,
for any $\epsilon > 0$. By setting $\epsilon=\log \log n/\log n$, we obtain a
polylogarithmic approximation in quasi-polynomial time, or by setting
$\epsilon$ as a constant, an $n^\epsilon$-approximation in polynomial time.
  While there are necessary assumptions on the form of the DP, it is general
enough to capture many textbook dynamic programs from Shortest Path to Longest
Common Subsequence. Our algorithm then implies that we can impose additional
constraints on the solutions to these problems. This allows us to model various
problems from the literature in approximation algorithms, many of which were
not thought to be connected to dynamic programming. In fact, our result can
even be applied indirectly to some problems that involve covering instead of
packing constraints, for example, the Directed Steiner Tree problem, or those
that do not directly follow a recurrence relation, for example, variants of the
Matching problem.

</details>


### [45] [Engineering Algorithms for $\ell$-Isolated Maximal Clique Enumeration](https://arxiv.org/abs/2511.03525)
*Marco D'Elia,Irene Finocchi,Maurizio Patrignani*

Main category: cs.DS

TL;DR: 本文提出针对枚举l孤立最大团的四种剪枝启发式方法，实验表明其中两种可显著提升效率。


<details>
  <summary>Details</summary>
Motivation: 最大团数量过多，在实际应用中难以有效利用，因此需枚举l孤立最大团来过滤掉与外部连接过多的团。

Method: 基于Tomita等人的递归算法，提出四种剪枝启发式方法，可单独或组合使用，以丢弃不会产生l孤立最大团的搜索分支。

Result: 实验表明，两种启发式方法在具有社交网络属性的真实图上能大幅提高效率。

Conclusion: 所提出的两种剪枝启发式方法在枚举l孤立最大团时具有较高效率，尤其适用于特定的真实图。

Abstract: Maximal cliques play a fundamental role in numerous application domains,
where their enumeration can prove extremely useful. Yet their sheer number,
even in sparse real-world graphs, can make them impractical to be exploited
effectively. To address this issue, one approach is to enumerate
$\ell$-isolated maximal cliques, whose vertices have (on average) less than
$\ell$ edges toward the rest of the graph. By tuning parameter $\ell$, the
degree of isolation can be controlled, and cliques that are overly connected to
the outside are filtered out. Building on Tomita et al.'s very practical
recursive algorithm for maximal clique enumeration, we propose four pruning
heuristics, applicable individually or in combination, that discard recursive
search branches that are guaranteed not to yield $\ell$-isolated maximal
cliques. Besides proving correctness, we characterize both the pruning power
and the computational cost of these heuristics, and we conduct an extensive
experimental study comparing our methods with Tomita's baseline and with a
state-of-the-art approach. Results show that two of our heuristics offer
substantial efficiency improvements, especially on real-world graphs with
social network properties.

</details>


### [46] [Improved Bounds with a Simple Algorithm for Edge Estimation for Graphs of Unknown Size](https://arxiv.org/abs/2511.03650)
*Debarshi Chanda*

Main category: cs.DS

TL;DR: 提出随机算法估计图平均度，改进已有算法，给出上下界，解决先前工作问题。


<details>
  <summary>Details</summary>
Motivation: 改进已有图平均度估计算法的查询复杂度，解决先前工作提出的问题。

Method: 提出一种随机算法，使用Degree和Random Edge查询，采用新估计技术。

Result: 新算法查询次数少，不依赖图参数，给出查询次数下界。

Conclusion: 新算法在图平均度估计上更优，解决了先前工作的疑问。

Abstract: We propose a randomized algorithm with query access that given a graph $G$
with arboricity $\alpha$, and average degree $d$, makes
$\widetilde{O}\left(\frac{\alpha}{\varepsilon^2d}\right)$ \texttt{Degree} and
$\widetilde{O}\left(\frac{1}{\varepsilon^2}\right)$ \texttt{Random Edge}
queries to obtain an estimate $\widehat{d}$ satisfying $\widehat{d} \in
(1\pm\varepsilon)d$. This improves the $\widetilde{O}_{\varepsilon,\log
n}\left(\sqrt{\frac{n}{d}}\right)$ query algorithm of [Beretta et al., SODA
2026] that has access to \texttt{Degree}, \texttt{Neighbour}, and
\texttt{Random Edge} queries. Our algorithm does not require any graph
parameter as input, not even the size of the vertex set, and attains both
simplicity and practicality through a new estimation technique. We complement
our upper bounds with a lower bound that shows for all valid $n,d$, and
$\alpha$, any algorithm that has access to \texttt{Degree}, \texttt{Neighbour},
and \texttt{Random Edge} queries, must make at least
$\Omega\left(\min\left(d,\frac{\alpha}{d}\right)\right)$ queries to obtain a
$(1\pm\varepsilon)$-multiplicative estimate of $d$, even with the knowledge of
$n$ and $\alpha$. We also show that even with \texttt{Pair} and
\texttt{FullNbr} queries, an algorithm must make
$\Omega\left(\min\left(d,\frac{\alpha}{d}\right)\right)$ queries to obtain a
$(1\pm\varepsilon)$-multiplicative estimate of $d$. Our work addresses both the
questions raised by the work of [Beretta et al., SODA 2026].

</details>


### [47] [An Improved Quality Hierarchical Congestion Approximator in Near-Linear Time](https://arxiv.org/abs/2511.03716)
*Monika Henzinger,Robin Münk,Harald Räcke*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: A congestion approximator for a graph is a compact data structure that
approximately predicts the edge congestion required to route any set of flow
demands in a network. A congestion approximator is hierarchical if it consists
of a laminar family of cuts in the graph. There is a tradeoff between the
running time for computing a congestion approximator and its approximation
quality. Currently, for an $n$-node graph there exists a polynomial time
algorithm that achieves a $O(\log^{1.5}n \log \log n)$ approximation and a
near-linear time algorithm that achieves w.h.p. a $O(\log^4 n)$ approximation.
In this paper we give the first near-linear time algorithm, that achieves
w.h.p. a $O(\log^2 n \log \log n)$ approximation, using an hierarchical
congestion approximator with $O(n \log n)$ cuts. Based on a reduction from
oblivious routing, we also present a lower bound of $\Omega(\log n)$ for the
approximation quality of hierarchical congestion approximators.
  Our algorithm can also be implemented in the parallel setting achieving the
same approximation quality, polylogarithmic span and near-linear work. This
improves upon the best prior parallel algorithm, which has a $O(\log^9n)$
approximation.
  Crucial for achieving a near linear running time is a new partitioning
routine that, unlike previous such routines, manages to avoid recursing on
large subgraphs. To achieve the improved approximation quality, we introduce
the new concept of border routability of a cut and give an improved sparsest
cut oracle for general vertex weights.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [48] [Branch-and-Cut for Computing Approximate Equilibria of Mixed-Integer Generalized Nash Games](https://arxiv.org/abs/2511.03340)
*Aloïs Duguet,Tobias Harks,Martin Schmidt,Julian Schwarz*

Main category: cs.GT

TL;DR: 研究含混合整数变量的广义Nash均衡问题的近似均衡，提出分支 - 切割法等并给出数值结果。


<details>
  <summary>Details</summary>
Motivation: 此类游戏难以有精确均衡，且假设玩家能全局最优解非凸问题存疑，故研究近似均衡。

Method: 提出分支 - 切割法，采用交集切割思想，对标准Nash均衡问题引入另类切割，基于分支 - 切割法引入单树二分搜索法。

Result: 在约束线性、玩家成本函数满足特定条件下证明交集切割存在，特殊情况下方法有限终止，实现方法并给出混合整数流游戏数值结果。

Conclusion: 所提方法可用于计算含混合整数变量的广义Nash均衡问题的近似均衡。

Abstract: Generalized Nash equilibrium problems with mixed-integer variables constitute
an important class of games in which each player solves a mixed-integer
optimization problem, where both the objective and the feasible set is
parameterized by the rivals' strategies. However, such games are known for
failing to admit exact equilibria and also the assumption of all players being
able to solve nonconvex problems to global optimality is questionable. This
motivates the study of approximate equilibria. In this work, we consider an
approximation concept that incorporates both multiplicative and additive
relaxations of optimality. We propose a branch-and-cut (B&C) method that
computes such approximate equilibria or proves its non-existence. For this, we
adopt the idea of intersection cuts and show the existence of such cuts under
the condition that the constraints are linear and each player's cost function
is either convex in the entire strategy profile, or, concave in the entire
strategy profile and linear in the rivals' strategies. For the special case of
standard Nash equilibrium problems, we introduce an alternative type of cut and
show that the method terminates finitely, provided that each player has only
finitely many distinct best-response sets. Finally, on the basis of the B&C
method, we introduce a single-tree binary-search method to compute
best-approximate equilibria under some simplifying assumptions. We implemented
these methods and present numerical results for a class of mixed-integer flow
games.

</details>


### [49] [Non-Monotonicity in Fair Division of Graphs](https://arxiv.org/abs/2511.03629)
*Hadi Hosseini,Shraddha Pathak,Yu Zhou*

Main category: cs.GT

TL;DR: 研究图顶点公平分配问题，探讨EF1公平性与TS效率概念的兼容性，发现n与满足条件分配存在性的非单调关系，通过算法得到积极结果。


<details>
  <summary>Details</summary>
Motivation: 解决图顶点公平分配问题，该问题在团队组建和网络划分等应用中有重要意义，且估值具有非单调性。

Method: 聚焦EF1公平性概念，探索其与TS等效率概念的兼容性，通过研究一般图和森林图，以及调整效率要求进行分析，用高效算法实现积极结果。

Result: 发现n与满足EF1和TS的分配存在非单调关系，n=2时总存在，n=3时可能不存在，n≥4时又存在；稍弱效率要求或限制为森林图时，任意n都能保证存在。

Conclusion: 通过高效算法在图顶点公平分配问题上得到关于公平性和效率兼容性的积极结果。

Abstract: We consider the problem of fairly allocating the vertices of a graph among
$n$ agents, where the value of a bundle is determined by its cut value -- the
number of edges with exactly one endpoint in the bundle. This model naturally
captures applications such as team formation and network partitioning, where
valuations are inherently non-monotonic: the marginal values may be positive,
negative, or zero depending on the composition of the bundle. We focus on the
fairness notion of envy-freeness up to one item (EF1) and explore its
compatibility with several efficiency concepts such as Transfer Stability (TS)
that prohibits single-item transfers that benefit one agent without making the
other worse-off. For general graphs, our results uncover a non-monotonic
relationship between the number of agents $n$ and the existence of allocations
satisfying EF1 and transfer stability (TS): such allocations always exist for
$n=2$, may fail to exist for $n=3$, but exist again for all $n\geq 4$. We
further show that existence can be guaranteed for any $n$ by slightly weakening
the efficiency requirement or by restricting the graph to forests. All of our
positive results are achieved via efficient algorithms.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [50] [Generative Sequential Recommendation via Hierarchical Behavior Modeling](https://arxiv.org/abs/2511.03155)
*Zhefan Wang,Guokai Yan,Jinbei Yu,Siyu Gu,Jingyan Chen,Peng Jiang,Zhiqiang Guo,Min Zhang*

Main category: cs.IR

TL;DR: 提出生成式框架GAMER和多行为数据集ShortVideoAD，实验表明GAMER性能优。


<details>
  <summary>Details</summary>
Motivation: 现有生成式多行为推荐方法存在序列建模不足和缺乏合适数据集的问题，需改进。

Method: 提出基于解码器的GAMER框架，引入跨层交互层和序列增强策略；收集并发布ShortVideoAD数据集。

Result: GAMER在多个指标上始终优于判别式和生成式基线。

Conclusion: GAMER框架和ShortVideoAD数据集有效提升多行为推荐性能。

Abstract: Recommender systems in multi-behavior domains, such as advertising and
e-commerce, aim to guide users toward high-value but inherently sparse
conversions. Leveraging auxiliary behaviors (e.g., clicks, likes, shares) is
therefore essential. Recent progress on generative recommendations has brought
new possibilities for multi-behavior sequential recommendation. However,
existing generative approaches face two significant challenges: 1) Inadequate
Sequence Modeling: capture the complex, cross-level dependencies within user
behavior sequences, and 2) Lack of Suitable Datasets: publicly available
multi-behavior recommendation datasets are almost exclusively derived from
e-commerce platforms, limiting the validation of feasibility in other domains,
while also lacking sufficient side information for semantic ID generation. To
address these issues, we propose a novel generative framework, GAMER
(Generative Augmentation and Multi-lEvel behavior modeling for Recommendation),
built upon a decoder-only backbone. GAMER introduces a cross-level interaction
layer to capture hierarchical dependencies among behaviors and a sequential
augmentation strategy that enhances robustness in training. To further advance
this direction, we collect and release ShortVideoAD, a large-scale
multi-behavior dataset from a mainstream short-video platform, which differs
fundamentally from existing e-commerce datasets and provides pretrained
semantic IDs for research on generative methods. Extensive experiments show
that GAMER consistently outperforms both discriminative and generative
baselines across multiple metrics.

</details>


### [51] [KScaNN: Scalable Approximate Nearest Neighbor Search on Kunpeng](https://arxiv.org/abs/2511.03298)
*Oleg Senkevich,Siyang Xu,Tianyi Jiang,Alexander Radionov,Jan Tabaszewski,Dmitriy Malyshev,Zijian Li,Daihao Xue,Licheng Yu,Weidi Zeng,Meiling Wang,Xin Yao,Siyu Huang,Gleb Neshchetkin,Qiuling Pan,Yaoyao Fu*

Main category: cs.IR

TL;DR: 介绍适用于鲲鹏920 ARM架构的新型ANNS算法KScaNN，通过算法优化和硬件优化提升性能，速度比最快的x86方案快1.63倍。


<details>
  <summary>Details</summary>
Motivation: 随着ARM服务器在行业中应用增加，需要针对ARM架构优化的ANNS解决方案，而直接移植x86算法到ARM平台性能不佳。

Method: 提出KScaNN算法，采用混合簇内搜索策略、改进PQ残差计算方法、ML驱动的自适应搜索模块和高度优化的ARM SIMD内核。

Result: KScaNN不仅缩小了性能差距，还树立新标准，比最快的x86方案快1.63倍。

Conclusion: 该工作为现代ARM架构上的向量搜索实现领先性能提供了蓝图。

Abstract: Approximate Nearest Neighbor Search (ANNS) is a cornerstone algorithm for
information retrieval, recommendation systems, and machine learning
applications. While x86-based architectures have historically dominated this
domain, the increasing adoption of ARM-based servers in industry presents a
critical need for ANNS solutions optimized on ARM architectures. A naive port
of existing x86 ANNS algorithms to ARM platforms results in a substantial
performance deficit, failing to leverage the unique capabilities of the
underlying hardware. To address this challenge, we introduce KScaNN, a novel
ANNS algorithm co-designed for the Kunpeng 920 ARM architecture. KScaNN
embodies a holistic approach that synergizes sophisticated, data aware
algorithmic refinements with carefully-designed hardware specific
optimizations. Its core contributions include: 1) novel algorithmic techniques,
including a hybrid intra-cluster search strategy and an improved PQ residual
calculation method, which optimize the search process at a higher level; 2) an
ML-driven adaptive search module that provides adaptive, per-query tuning of
search parameters, eliminating the inefficiencies of static configurations; and
3) highly-optimized SIMD kernels for ARM that maximize hardware utilization for
the critical distance computation workloads. The experimental results
demonstrate that KScaNN not only closes the performance gap but establishes a
new standard, achieving up to a 1.63x speedup over the fastest x86-based
solution. This work provides a definitive blueprint for achieving
leadership-class performance for vector search on modern ARM architectures and
underscores

</details>


### [52] [Discourse-Aware Scientific Paper Recommendation via QA-Style Summarization and Multi-Level Contrastive Learning](https://arxiv.org/abs/2511.03330)
*Shenghua Wang,Zhen Yin*

Main category: cs.IR

TL;DR: 现有模型将论文视为非结构化文本，忽略话语组织，本文提出 OMRC - MR 框架用于学术推荐，实验表明其超越基线模型，提供了统一可解释的内容推荐范式。


<details>
  <summary>Details</summary>
Motivation: 开放获取出版物增长使识别相关论文挑战加剧，现有基于内容推荐模型忽略论文话语组织，限制语义完整性和可解释性。

Method: 提出 OMRC - MR 框架，集成 QA 风格的 OMRC 摘要、多级对比学习和结构感知重排序。

Result: 在 DBLP、S2ORC 和 Sci - OMRC 数据集上实验，Precision@10 最高提升 7.2%，Recall@10 最高提升 3.8%，QA 风格摘要生成的表征更连贯完整。

Conclusion: OMRC - MR 为科学论文推荐提供统一可解释的基于内容范式，推动可信和隐私感知的学术信息检索。

Abstract: The rapid growth of open-access (OA) publications has intensified the
challenge of identifying relevant scientific papers. Due to privacy constraints
and limited access to user interaction data, recent efforts have shifted toward
content-based recommendation, which relies solely on textual information.
However, existing models typically treat papers as unstructured text,
neglecting their discourse organization and thereby limiting semantic
completeness and interpretability. To address these limitations, we propose
OMRC-MR, a hierarchical framework that integrates QA-style OMRC (Objective,
Method, Result, Conclusion) summarization, multi-level contrastive learning,
and structure-aware re-ranking for scholarly recommendation. The QA-style
summarization module converts raw papers into structured and
discourse-consistent representations, while multi-level contrastive objectives
align semantic representations across metadata, section, and document levels.
The final re-ranking stage further refines retrieval precision through
contextual similarity calibration. Experiments on DBLP, S2ORC, and the newly
constructed Sci-OMRC dataset demonstrate that OMRC-MR consistently surpasses
state-of-the-art baselines, achieving up to 7.2% and 3.8% improvements in
Precision@10 and Recall@10, respectively. Additional evaluations confirm that
QA-style summarization produces more coherent and factually complete
representations. Overall, OMRC-MR provides a unified and interpretable
content-based paradigm for scientific paper recommendation, advancing
trustworthy and privacy-aware scholarly information retrieval.

</details>


### [53] [A Semantic Encoding of Object Centric Event Data](https://arxiv.org/abs/2511.03351)
*Saba Latif,Fajar J. Ekaputra,Maxim Vidgof,Sabrina Kirrane,Claudio Di Ciccio*

Main category: cs.IR

TL;DR: 本文介绍基于语义网技术实现语义增强的以对象为中心的事件数据（OCED）的方法，以强化过程数据推理等。


<details>
  <summary>Details</summary>
Motivation: OCED旨在为以事件和对象为中心的过程数据记录提供基础，促进互操作性和信息交换，但存在数据集成等挑战，需要语义增强。

Method: 采用语义网技术，通过基于本体的关系和实体分类创建机器可读的OCED描述。

Result: 未提及具体结果

Conclusion: 未提及具体结论

Abstract: The Object-Centric Event Data (OCED) is a novel meta-model aimed at providing
a common ground for process data records centered around events and objects.
One of its objectives is to foster interoperability and process information
exchange. In this context, the integration of data from different providers,
the combination of multiple processes, and the enhancement of knowledge
inference are novel challenges. Semantic Web technologies can enable the
creation of a machine-readable OCED description enriched through ontology-based
relationships and entity categorization. In this paper, we introduce an
approach built upon Semantic Web technologies for the realization of
semantic-enhanced OCED, with the aim to strengthen process data reasoning,
interconnect information sources, and boost expressiveness.

</details>


### [54] [CLAX: Fast and Flexible Neural Click Models in JAX](https://arxiv.org/abs/2511.03620)
*Philipp Hager,Onno Zoeter,Maarten de Rijke*

Main category: cs.IR

TL;DR: CLAX是基于JAX的库，用梯度优化实现经典点击模型，比传统方法快，实现十种模型，开源可用。


<details>
  <summary>Details</summary>
Motivation: 复杂概率图模型的点击模型未系统采用梯度优化，无法利用深度学习框架并保留经典模型可解释性。

Method: 用数值稳定的直接梯度优化替代基于EM的优化，框架模块化设计可集成组件进行端到端优化。

Result: 在单GPU上约2小时完成对超十亿用户会话的百度数据集实验，比传统EM方法快几个数量级，实现十种经典点击模型。

Conclusion: CLAX可帮助从业者理解用户行为和提升排序性能，也助于研究者开发新模型。

Abstract: CLAX is a JAX-based library that implements classic click models using modern
gradient-based optimization. While neural click models have emerged over the
past decade, complex click models based on probabilistic graphical models
(PGMs) have not systematically adopted gradient-based optimization, preventing
practitioners from leveraging modern deep learning frameworks while preserving
the interpretability of classic models. CLAX addresses this gap by replacing
EM-based optimization with direct gradient-based optimization in a numerically
stable manner. The framework's modular design enables the integration of any
component, from embeddings and deep networks to custom modules, into classic
click models for end-to-end optimization. We demonstrate CLAX's efficiency by
running experiments on the full Baidu-ULTR dataset comprising over a billion
user sessions in $\approx$ 2 hours on a single GPU, orders of magnitude faster
than traditional EM approaches. CLAX implements ten classic click models,
serving both industry practitioners seeking to understand user behavior and
improve ranking performance at scale and researchers developing new click
models. CLAX is available at: https://github.com/philipphager/clax

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [55] [FATE: A Formal Benchmark Series for Frontier Algebra of Multiple Difficulty Levels](https://arxiv.org/abs/2511.02872)
*Jiedong Jiang,Wanyi He,Yuefeng Wang,Guoxiong Gao,Yongle Hu,Jingting Wang,Nailing Guan,Peihao Wu,Chunbo Dai,Liang Xiao,Bin Dong*

Main category: cs.LG

TL;DR: 引入形式代数新基准FATE系列，含FATE - H和FATE - X，评估LLM证明器在其上表现差，分析错误，认为FATE是研究级形式数学推理的有力基准。


<details>
  <summary>Details</summary>
Motivation: 现有竞赛基准不能反映现代数学研究深度、广度和抽象性，需新基准推动高级数学推理。

Method: 引入FATE基准系列，含两个组件各100个问题，对最先进LLM证明器进行评估，分两阶段评估并分类形式化过程常见错误，开展对比研究。

Result: LLM证明器在FATE上表现与竞赛数学有巨大差距，最佳模型在FATE - H上准确率3%（pass@64），FATE - X上为0%；自然语言推理比形式化推理更准确；专用证明器自然语言阶段准确性不如通用模型。

Conclusion: FATE为研究级形式数学推理提供了强大且具挑战性的基准。

Abstract: Recent advances in large language models (LLMs) have demonstrated impressive
capabilities in formal theorem proving, particularly on contest-based
mathematical benchmarks like the IMO. However, these contests do not reflect
the depth, breadth, and abstraction of modern mathematical research. To bridge
this gap, we introduce FATE (Formal Algebra Theorem Evaluation), a new
benchmark series in formal algebra designed to chart a course toward advanced
mathematical reasoning. We present two new components, FATE-H and FATE-X, each
with 100 problems in abstract and commutative algebra. The FATE series spans a
difficulty spectrum from undergraduate exercises to problems exceeding PhD
qualifying exams. Notably, FATE-X is the first formal benchmark to surpass both
PhD-level exam difficulty and the coverage of the Mathlib library. Our
evaluations of state-of-the-art LLM provers on this new benchmark reveal a
stark performance gap compared to contest math: the best model achieves only 3%
(pass@64) accuracy on FATE-H and 0% on FATE-X. Our two-stage evaluation reveals
that models' natural-language reasoning is notably more accurate than their
ability to formalize this reasoning. We systematically classify the common
errors that arise during this formalization process. Furthermore, a comparative
study shows that a specialized prover can exhibit less effective reflection
than general-purpose models, reducing its accuracy at the natural-language
stage. We believe FATE provides a robust and challenging benchmark that
establishes essential checkpoints on the path toward research-level formal
mathematical reasoning.

</details>


### [56] [Stochastic Deep Graph Clustering for Practical Group Formation](https://arxiv.org/abs/2511.02879)
*Junhyung Park,Hyungjin Kim,Seokho Ahn,Young-Duk Seo*

Main category: cs.LG

TL;DR: 现有GRS方法多适用于静态组，本文提出DeepForm框架，满足GRS组形成的关键需求，实验表明其性能优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有GRS方法多假设静态或预定义组，不适用于动态现实场景，需解决组形成问题。

Method: 提出DeepForm框架，采用轻量级GCN架构捕捉高阶结构信号，通过随机聚类学习实现自适应组重配置，用对比学习在动态条件下优化组。

Result: 在多个数据集上实验，DeepForm在组形成质量、效率和推荐准确性上优于各种基线。

Conclusion: DeepForm能有效解决GRS中的组形成问题，提升性能。

Abstract: While prior work on group recommender systems (GRSs) has primarily focused on
improving recommendation accuracy, most approaches assume static or predefined
groups, making them unsuitable for dynamic, real-world scenarios. We reframe
group formation as a core challenge in GRSs and propose DeepForm (Stochastic
Deep Graph Clustering for Practical Group Formation), a framework designed to
meet three key operational requirements: (1) the incorporation of high-order
user information, (2) real-time group formation, and (3) dynamic adjustment of
the number of groups. DeepForm employs a lightweight GCN architecture that
effectively captures high-order structural signals. Stochastic cluster learning
enables adaptive group reconfiguration without retraining, while contrastive
learning refines groups under dynamic conditions. Experiments on multiple
datasets demonstrate that DeepForm achieves superior group formation quality,
efficiency, and recommendation accuracy compared with various baselines.

</details>


### [57] [Test-time Adaptation of Tiny Recursive Models](https://arxiv.org/abs/2511.02886)
*Ronan Killian McGovern*

Main category: cs.LG

TL;DR: 本文提出从预训练的微小递归模型出发，在竞赛允许的计算资源限制内对竞赛任务进行高效微调，取得一定成绩。


<details>
  <summary>Details</summary>
Motivation: 之前领先的开源方法TRM虽在ARC任务上有一定得分，但计算量远超竞赛允许范围，需找到在计算限制内的有效方法。

Method: 从在公共ARC任务上预训练的微小递归模型开始，先在4块H100 SXM GPU上用1280个公共任务进行700k+优化步骤、时长48小时的预训练，再在竞赛期间进行12500个梯度步骤的后训练。

Result: 预训练模型在公共评估集上得分约10%，后训练在半私有评估任务上得分6.67%。

Conclusion: 通过对微小模型进行全量微调而非LoRA微调或仅微调任务嵌入，可在计算限制内取得较好后训练表现。

Abstract: Prior to the close of the 2025 ARC Prize competition, the leading open source
approach - known as TRM, or Tiny Recursive Models - involved training a 7M
parameter recursive neural network on augmented variants of ARC tasks. That
approach scored approximately 7.8% on the public ARC AGI II evaluation set, but
required a level of compute far in excess of what is allowed during the
competition. This paper shows that, by starting from a tiny recursive model
that has been pre-trained on public ARC tasks, one can efficiently fine-tune on
competition tasks within the allowed compute limits. Specifically, a model was
pre-trained on 1,280 public tasks for 700k+ optimizer steps over 48 hours on
4xH100 SXM GPUs to obtain a ~10% score on the public evaluation set. That model
was then post-trained in just 12,500 gradient steps during the competition to
reach a score of 6.67% on semi-private evaluation tasks. Notably, such
post-training performance is achieved by full-fine tuning of the tiny model,
not LoRA fine-tuning or fine-tuning of task embeddings alone.

</details>


### [58] [Digital Twin-Driven Pavement Health Monitoring and Maintenance Optimization Using Graph Neural Networks](https://arxiv.org/abs/2511.02957)
*Mohsin Mahmud Topu,Mahfuz Ahmed Anik,Azmine Toushik Wasi,Md Manjurul Ahsan*

Main category: cs.LG

TL;DR: 提出统一的数字孪生（DT）和图神经网络（GNN）框架用于路面健康监测和预测性维护，模型表现良好，还开发了相关模块，为主动、智能和可持续的路面管理奠定基础。


<details>
  <summary>Details</summary>
Motivation: 传统路面管理系统缺乏实时智能，难以预防故障和进行最优维护规划，且路面基础设施监测面临复杂空间依赖等挑战。

Method: 将路面路段和空间关系建模为图节点和边，实时的无人机、传感器和激光雷达数据流入数字孪生，归纳式GNN从图结构输入中学习劣化模式以预测病害。

Result: 模型在真实世界启发的数据集上训练，R2为0.3798，优于基线回归器，能有效捕捉非线性退化。

Conclusion: DT - GNN集成提高了预测精度，建立了闭环反馈，为主动、智能和可持续的路面管理奠定基础，未来可向实际部署、多智能体协调和智慧城市集成扩展。

Abstract: Pavement infrastructure monitoring is challenged by complex spatial
dependencies, changing environmental conditions, and non-linear deterioration
across road networks. Traditional Pavement Management Systems (PMS) remain
largely reactive, lacking real-time intelligence for failure prevention and
optimal maintenance planning. To address this, we propose a unified Digital
Twin (DT) and Graph Neural Network (GNN) framework for scalable, data-driven
pavement health monitoring and predictive maintenance. Pavement segments and
spatial relations are modeled as graph nodes and edges, while real-time UAV,
sensor, and LiDAR data stream into the DT. The inductive GNN learns
deterioration patterns from graph-structured inputs to forecast distress and
enable proactive interventions. Trained on a real-world-inspired dataset with
segment attributes and dynamic connectivity, our model achieves an R2 of
0.3798, outperforming baseline regressors and effectively capturing non-linear
degradation. We also develop an interactive dashboard and reinforcement
learning module for simulation, visualization, and adaptive maintenance
planning. This DT-GNN integration enhances forecasting precision and
establishes a closed feedback loop for continuous improvement, positioning the
approach as a foundation for proactive, intelligent, and sustainable pavement
management, with future extensions toward real-world deployment, multi-agent
coordination, and smart-city integration.

</details>


### [59] [Predicting Weekly Fishing Concentration Zones through Deep Learning Integration of Heterogeneous Environmental Spatial Datasets](https://arxiv.org/abs/2511.02887)
*Chaitanya Rele,Aditya Rathod,Kaustubh Natu,Saurabh Kulkarni,Ajay Koli,Swapnali Makdey*

Main category: cs.LG

TL;DR: 提出AI辅助框架预测北印度洋潜在渔场，初步结果显示可助渔民降本增效。


<details>
  <summary>Details</summary>
Motivation: 北印度洋渔民定位渔场存在不确定性，需解决该问题。

Method: 利用海面温度和叶绿素浓度等海洋参数，构建AI辅助框架预测潜在渔场。

Result: 框架能减少渔民搜索时间、降低油耗、促进资源高效利用。

Conclusion: 该框架可提高潜在渔场识别准确性，为可持续渔业提供特定区域见解。

Abstract: The North Indian Ocean, including the Arabian Sea and the Bay of Bengal,
represents a vital source of livelihood for coastal communities, yet fishermen
often face uncertainty in locating productive fishing grounds. To address this
challenge, we present an AI-assisted framework for predicting Potential Fishing
Zones (PFZs) using oceanographic parameters such as sea surface temperature and
chlorophyll concentration. The approach is designed to enhance the accuracy of
PFZ identification and provide region-specific insights for sustainable fishing
practices. Preliminary results indicate that the framework can support
fishermen by reducing search time, lowering fuel consumption, and promoting
efficient resource utilization.

</details>


### [60] [Adaptive and Robust Data Poisoning Detection and Sanitization in Wearable IoT Systems using Large Language Models](https://arxiv.org/abs/2511.02894)
*W. K. M Mithsara,Ning Yang,Ahmed Imteaj,Hussein Zangoti,Abdur R. Shahid*

Main category: cs.LG

TL;DR: 可穿戴传感设备集成需要强大的人类活动识别（HAR）技术，但机器学习模型易受数据投毒攻击。本文提出用大语言模型（LLMs）的框架进行投毒检测和清理，评估证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 可穿戴传感设备集成使HAR技术需求增加，但机器学习模型易受数据投毒攻击，传统防御方法适应性有限。

Method: 提出使用LLMs的框架，采用零样本、单样本和少样本学习范式，结合角色扮演提示和逐步推理策略。

Result: 对框架进行了广泛评估，量化了检测准确性、清理质量、延迟和通信成本。

Conclusion: 证明了LLMs在提高可穿戴物联网系统安全性和可靠性方面的实用性和有效性。

Abstract: The widespread integration of wearable sensing devices in Internet of Things
(IoT) ecosystems, particularly in healthcare, smart homes, and industrial
applications, has required robust human activity recognition (HAR) techniques
to improve functionality and user experience. Although machine learning models
have advanced HAR, they are increasingly susceptible to data poisoning attacks
that compromise the data integrity and reliability of these systems.
Conventional approaches to defending against such attacks often require
extensive task-specific training with large, labeled datasets, which limits
adaptability in dynamic IoT environments. This work proposes a novel framework
that uses large language models (LLMs) to perform poisoning detection and
sanitization in HAR systems, utilizing zero-shot, one-shot, and few-shot
learning paradigms. Our approach incorporates \textit{role play} prompting,
whereby the LLM assumes the role of expert to contextualize and evaluate sensor
anomalies, and \textit{think step-by-step} reasoning, guiding the LLM to infer
poisoning indicators in the raw sensor data and plausible clean alternatives.
These strategies minimize reliance on curation of extensive datasets and enable
robust, adaptable defense mechanisms in real-time. We perform an extensive
evaluation of the framework, quantifying detection accuracy, sanitization
quality, latency, and communication cost, thus demonstrating the practicality
and effectiveness of LLMs in improving the security and reliability of wearable
IoT systems.

</details>


### [61] [Zero-shot data citation function classification using transformer-based large language models (LLMs)](https://arxiv.org/abs/2511.02936)
*Neil Byers,Ali Zaidi,Valerie Skye,Chris Beecroft,Kjiersten Fagnan*

Main category: cs.LG

TL;DR: 本文应用开源大语言模型Llama 3.1 - 405B为引用特定基因组数据集的出版物生成结构化数据用例标签，并引入评估框架，结果有前景但存在一些障碍。


<details>
  <summary>Details</summary>
Motivation: 探索数据使用方式及原因，利用预训练大语言模型避免传统机器学习手动标注和训练数据集开发的高成本。

Method: 应用开源大语言模型Llama 3.1 - 405B生成结构化数据用例标签，引入新的评估框架。

Result: 基础模型在零样本数据引用分类任务中F1分数达0.674。

Conclusion: 结果有前景，但受数据可用性、提示过拟合、计算基础设施和评估费用等障碍限制。

Abstract: Efforts have increased in recent years to identify associations between
specific datasets and the scientific literature that incorporates them. Knowing
that a given publication cites a given dataset, the next logical step is to
explore how or why that data was used. Advances in recent years with
pretrained, transformer-based large language models (LLMs) offer potential
means for scaling the description of data use cases in the published
literature. This avoids expensive manual labeling and the development of
training datasets for classical machine-learning (ML) systems. In this work we
apply an open-source LLM, Llama 3.1-405B, to generate structured data use case
labels for publications known to incorporate specific genomic datasets. We also
introduce a novel evaluation framework for determining the efficacy of our
methods. Our results demonstrate that the stock model can achieve an F1 score
of .674 on a zero-shot data citation classification task with no previously
defined categories. While promising, our results are qualified by barriers
related to data availability, prompt overfitting, computational infrastructure,
and the expense required to conduct responsible performance evaluation.

</details>


### [62] [Power Constrained Nonstationary Bandits with Habituation and Recovery Dynamics](https://arxiv.org/abs/2511.02944)
*Fengxu Li,Stephanie M. Carpenter,Matthew P. Buman,Yonatan Mintz*

Main category: cs.LG

TL;DR: 本文针对ROGUE多臂老虎机框架开发了ROGUE - TS算法及概率裁剪程序，在两个MRT数据集验证中表现优于现有方法，为研究者平衡个性化与统计有效性提供指导。


<details>
  <summary>Details</summary>
Motivation: 现有算法在ROGUE框架下过度强调利用、探索不足，难以估计总体效应，在MRTs中面临挑战。

Method: 开发ROGUE - TS算法并提供次线性遗憾理论保证，引入概率裁剪程序平衡个性化与总体学习。

Result: 在两个MRT数据集验证中，方法比现有方法遗憾更低，通过裁剪程序保持高统计功效且不过多增加遗憾。

Conclusion: 框架为设计MRTs的研究者在平衡个性化与统计有效性方面提供实用指导。

Abstract: A common challenge for decision makers is selecting actions whose rewards are
unknown and evolve over time based on prior policies. For instance, repeated
use may reduce an action's effectiveness (habituation), while inactivity may
restore it (recovery). These nonstationarities are captured by the Reducing or
Gaining Unknown Efficacy (ROGUE) bandit framework, which models real-world
settings such as behavioral health interventions. While existing algorithms can
compute sublinear regret policies to optimize these settings, they may not
provide sufficient exploration due to overemphasis on exploitation, limiting
the ability to estimate population-level effects. This is a challenge of
particular interest in micro-randomized trials (MRTs) that aid researchers in
developing just-in-time adaptive interventions that have population-level
effects while still providing personalized recommendations to individuals. In
this paper, we first develop ROGUE-TS, a Thompson Sampling algorithm tailored
to the ROGUE framework, and provide theoretical guarantees of sublinear regret.
We then introduce a probability clipping procedure to balance personalization
and population-level learning, with quantified trade-off that balances regret
and minimum exploration probability. Validation on two MRT datasets concerning
physical activity promotion and bipolar disorder treatment shows that our
methods both achieve lower regret than existing approaches and maintain high
statistical power through the clipping procedure without significantly
increasing regret. This enables reliable detection of treatment effects while
accounting for individual behavioral dynamics. For researchers designing MRTs,
our framework offers practical guidance on balancing personalization with
statistical validity.

</details>


### [63] [Inference-Time Personalized Alignment with a Few User Preference Queries](https://arxiv.org/abs/2511.02966)
*Victor-Alexandru Pădurean,Parameswaran Kamalaruban,Nachiket Kotalwar,Alkis Gotovos,Adish Singla*

Main category: cs.LG

TL;DR: 本文提出UserAlign方法解决生成模型响应与用户偏好对齐问题，通过少量查询实现个性化对齐，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有个性化对齐方法要么需大量用户偏好查询，要么需用户明确以文本输入偏好，存在局限性。

Method: 提出UserAlign方法，基于逻辑多臂老虎机的最佳臂识别理论框架，通过少量成对响应比较查询获取用户偏好，从固定响应池中选择个性化响应。

Result: 在多个涉及个性化文本和图像生成的任务中进行实验，展示了UserAlign在实现个性化对齐方面的有效性。

Conclusion: UserAlign方法能有效解决生成模型响应与用户偏好的对齐问题。

Abstract: We study the problem of aligning a generative model's response with a user's
preferences. Recent works have proposed several different formulations for
personalized alignment; however, they either require a large amount of user
preference queries or require that the preference be explicitly specified as a
text input. In this paper, we propose a novel inference-time personalized
alignment method, UserAlign, that elicits the user's preferences with a few
queries as pairwise response comparisons. In particular, UserAlign builds on
the theoretical framework of best-arm identification in logistic bandits and
selects a personalized response from a fixed pool of the model's generated
responses. The key idea is to consider the user's feedback consistent and
noise-free, and incorporate it into the theoretical framework to identify the
best response quickly. Experimental results across several tasks, involving
personalized text and image generation, showcase the effectiveness of UserAlign
in achieving personalized alignment.

</details>


### [64] [Adaptive-Sensorless Monitoring of Shipping Containers](https://arxiv.org/abs/2511.03022)
*Lingqing Shen,Chi Heem Wong,Misaki Mito,Arnab Chakrabarti*

Main category: cs.LG

TL;DR: 提出残差校正方法改进无传感器监测模型，在大样本数据上验证其优于基线模型，能实现更准确货物监测等。


<details>
  <summary>Details</summary>
Motivation: 现有无传感器监测未纳入遥测信息和校正系统误差，预测与实时数据差异大，需改进。

Method: 引入残差校正方法，构建自适应无传感器监测模型。

Result: 在348万个数据点上训练和评估，自适应无传感器模型在温度和湿度预测上的平均绝对误差和均方根误差均优于基线无传感器模型。

Conclusion: 自适应无传感器模型能实现更准确的货物监测、早期风险检测，减少对全球航运全连接的依赖。

Abstract: Monitoring the internal temperature and humidity of shipping containers is
essential to preventing quality degradation during cargo transportation.
Sensorless monitoring -- machine learning models that predict the internal
conditions of the containers using exogenous factors -- shows promise as an
alternative to monitoring using sensors. However, it does not incorporate
telemetry information and correct for systematic errors, causing the
predictions to differ significantly from the live data and confusing the users.
In this paper, we introduce the residual correction method, a general framework
for correcting for systematic biases in sensorless models after observing live
telemetry data. We call this class of models ``adaptive-sensorless''
monitoring. We train and evaluate adaptive-sensorless models on the 3.48
million data points -- the largest dataset of container sensor readings ever
used in academic research -- and show that they produce consistent improvements
over the baseline sensorless models. When evaluated on the holdout set of the
simulated data, they achieve average mean absolute errors (MAEs) of 2.24 $\sim$
2.31$^\circ$C (vs 2.43$^\circ$C by sensorless) for temperature and 5.72 $\sim$
7.09% for relative humidity (vs 7.99% by sensorless) and average root
mean-squared errors (RMSEs) of 3.19 $\sim$ 3.26$^\circ$C for temperature (vs
3.38$^\circ$C by sensorless) and 7.70 $\sim$ 9.12% for relative humidity (vs
10.0% by sensorless). Adaptive-sensorless models enable more accurate cargo
monitoring, early risk detection, and less dependence on full connectivity in
global shipping.

</details>


### [65] [Value of Information-Enhanced Exploration in Bootstrapped DQN](https://arxiv.org/abs/2511.02969)
*Stergios Plataniotis,Charilaos Akasiadis,Georgios Chalkiadakis*

Main category: cs.LG

TL;DR: 本文将信息价值概念融入Bootstrapped DQN算法框架，开发两种新算法提升深度探索能力，在Atari游戏实验中表现良好且无需额外超参数。


<details>
  <summary>Details</summary>
Motivation: 解决深度强化学习在高维状态和稀疏奖励环境下高效探索的挑战，传统随机局部策略噪声探索策略难以平衡探索与利用。

Method: 将信息价值的预期收益融入Bootstrapped DQN，开发两种新算法，用信息价值估计衡量不同网络头观点差异，驱动探索。

Result: 在复杂、稀疏奖励的Atari游戏实验中，算法性能提升，更好利用不确定性。

Conclusion: 所提出的算法能有效提升深度强化学习的探索能力，且无需引入额外超参数。

Abstract: Efficient exploration in deep reinforcement learning remains a fundamental
challenge, especially in environments characterized by high-dimensional states
and sparse rewards. Traditional exploration strategies that rely on random
local policy noise, such as $\epsilon$-greedy and Boltzmann exploration
methods, often struggle to efficiently balance exploration and exploitation. In
this paper, we integrate the notion of (expected) value of information (EVOI)
within the well-known Bootstrapped DQN algorithmic framework, to enhance the
algorithm's deep exploration ability. Specifically, we develop two novel
algorithms that incorporate the expected gain from learning the value of
information into Bootstrapped DQN. Our methods use value of information
estimates to measure the discrepancies of opinions among distinct network
heads, and drive exploration towards areas with the most potential. We evaluate
our algorithms with respect to performance and their ability to exploit
inherent uncertainty arising from random network initialization. Our
experiments in complex, sparse-reward Atari games demonstrate increased
performance, all the while making better use of uncertainty, and, importantly,
without introducing extra hyperparameters.

</details>


### [66] [Discrete Bayesian Sample Inference for Graph Generation](https://arxiv.org/abs/2511.03015)
*Ole Petersen,Marcel Kollovieh,Marten Lienen,Stephan Günnemann*

Main category: cs.LG

TL;DR: 本文提出基于贝叶斯样本推理的图生成模型GraphBSI，在分子和合成图生成上表现优异。


<details>
  <summary>Details</summary>
Motivation: 图结构数据离散无序，传统生成模型处理困难，离散扩散和流匹配模型兴起。

Method: 提出GraphBSI模型，在分布参数的连续空间中迭代细化图的置信度，将BSI表示为随机微分方程并推导噪声控制的SDE族。

Result: GraphBSI在分子和合成图生成任务中表现达到了当前最优，在标准基准上超越现有一次性图生成模型。

Conclusion: GraphBSI是一种有效的图生成模型，能自然处理离散结构。

Abstract: Generating graph-structured data is crucial in applications such as molecular
generation, knowledge graphs, and network analysis. However, their discrete,
unordered nature makes them difficult for traditional generative models,
leading to the rise of discrete diffusion and flow matching models. In this
work, we introduce GraphBSI, a novel one-shot graph generative model based on
Bayesian Sample Inference (BSI). Instead of evolving samples directly, GraphBSI
iteratively refines a belief over graphs in the continuous space of
distribution parameters, naturally handling discrete structures. Further, we
state BSI as a stochastic differential equation (SDE) and derive a
noise-controlled family of SDEs that preserves the marginal distributions via
an approximation of the score function. Our theoretical analysis further
reveals the connection to Bayesian Flow Networks and Diffusion models. Finally,
in our empirical evaluation, we demonstrate state-of-the-art performance on
molecular and synthetic graph generation, outperforming existing one-shot graph
generative models on the standard benchmarks Moses and GuacaMol.

</details>


### [67] [Heterogeneous Metamaterials Design via Multiscale Neural Implicit Representation](https://arxiv.org/abs/2511.03012)
*Hongrui Chen,Liwei Wang,Levent Burak Kara*

Main category: cs.LG

TL;DR: 提出基于神经网络的超材料设计框架，解决异质超材料设计难题，在多问题上验证有效性。


<details>
  <summary>Details</summary>
Motivation: 复杂工程任务需异质超材料，但传统设计方法有局限，数据驱动方法受数据集限制。

Method: 提出基于神经网络的设计框架，以全局和局部坐标为输入，输出隐式场，训练时用兼容性损失项。

Result: 训练后的网络可产生任意高分辨率超材料设计，能无限上采样。

Conclusion: 该方法在机械超材料设计等问题上有效，有潜在应用价值。

Abstract: Metamaterials are engineered materials composed of specially designed unit
cells that exhibit extraordinary properties beyond those of natural materials.
Complex engineering tasks often require heterogeneous unit cells to accommodate
spatially varying property requirements. However, designing heterogeneous
metamaterials poses significant challenges due to the enormous design space and
strict compatibility requirements between neighboring cells. Traditional
concurrent multiscale design methods require solving an expensive optimization
problem for each unit cell and often suffer from discontinuities at cell
boundaries. On the other hand, data-driven approaches that assemble structures
from a fixed library of microstructures are limited by the dataset and require
additional post-processing to ensure seamless connections. In this work, we
propose a neural network-based metamaterial design framework that learns a
continuous two-scale representation of the structure, thereby jointly
addressing these challenges. Central to our framework is a multiscale neural
representation in which the neural network takes both global (macroscale) and
local (microscale) coordinates as inputs, outputting an implicit field that
represents multiscale structures with compatible unit cell geometries across
the domain, without the need for a predefined dataset. We use a compatibility
loss term during training to enforce connectivity between adjacent unit cells.
Once trained, the network can produce metamaterial designs at arbitrarily high
resolution, hence enabling infinite upsampling for fabrication or simulation.
We demonstrate the effectiveness of the proposed approach on mechanical
metamaterial design, negative Poisson's ratio, and mechanical cloaking problems
with potential applications in robotics, bioengineering, and aerospace.

</details>


### [68] [Leveraging Discrete Function Decomposability for Scientific Design](https://arxiv.org/abs/2511.03032)
*James C. Bowden,Sergey Levine,Jennifer Listgarten*

Main category: cs.LG

TL;DR: 提出名为DADO的分布优化算法，能利用分解结构让离散对象的计算机设计优化更高效。


<details>
  <summary>Details</summary>
Motivation: 在AI驱动的科学工程时代，计算机设计离散对象时，现有分布优化算法无法利用属性预测器的可分解结构。

Method: 提出Decomposition - Aware Distributional Optimization (DADO)算法，使用软因子化的“搜索分布”，通过图消息传递协调优化。

Result: 展示了DADO算法可利用设计变量上由连接树定义的任何可分解性。

Conclusion: DADO算法能让离散对象设计的优化更高效。

Abstract: In the era of AI-driven science and engineering, we often want to design
discrete objects in silico according to user-specified properties. For example,
we may wish to design a protein to bind its target, arrange components within a
circuit to minimize latency, or find materials with certain properties. Given a
property predictive model, in silico design typically involves training a
generative model over the design space (e.g., protein sequence space) to
concentrate on designs with the desired properties. Distributional optimization
-- which can be formalized as an estimation of distribution algorithm or as
reinforcement learning policy optimization -- finds the generative model that
maximizes an objective function in expectation. Optimizing a distribution over
discrete-valued designs is in general challenging because of the combinatorial
nature of the design space. However, many property predictors in scientific
applications are decomposable in the sense that they can be factorized over
design variables in a way that could in principle enable more effective
optimization. For example, amino acids at a catalytic site of a protein may
only loosely interact with amino acids of the rest of the protein to achieve
maximal catalytic activity. Current distributional optimization algorithms are
unable to make use of such decomposability structure. Herein, we propose and
demonstrate use of a new distributional optimization algorithm,
Decomposition-Aware Distributional Optimization (DADO), that can leverage any
decomposability defined by a junction tree on the design variables, to make
optimization more efficient. At its core, DADO employs a soft-factorized
"search distribution" -- a learned generative model -- for efficient navigation
of the search space, invoking graph message-passing to coordinate optimization
across linked factors.

</details>


### [69] [Cross-Modal Alignment via Variational Copula Modelling](https://arxiv.org/abs/2511.03196)
*Feng Wu,Tsai Hor Chan,Fuying Wang,Guosheng Yin,Lequan Yu*

Main category: cs.LG

TL;DR: 提出基于Copula的多模态学习框架，在MIMIC数据集上表现优于其他模型。


<details>
  <summary>Details</summary>
Motivation: 现实应用中多模态数据常见，现有方法简化模态间交互结构，需建模更复杂交互，且高阶交互的潜在表示联合分布研究不足。

Method: 提出Copula驱动的多模态学习框架，将Copula模型作为工具高效对齐模态的边缘分布，假设各模态为高斯混合分布且联合分布采用Copula模型。

Result: 在公共MIMIC数据集上的广泛实验表明，该模型生成缺失模态的准确表示，性能优于其他竞争对手。

Conclusion: 所提Copula驱动的多模态学习框架有效，代码开源。

Abstract: Various data modalities are common in real-world applications (e.g.,
electronic health records, medical images and clinical notes in healthcare). It
is essential to develop multimodal learning methods to aggregate various
information from multiple modalities. The main challenge is how to
appropriately align and fuse the representations of different modalities into a
joint distribution. Existing methods mainly rely on concatenation or the
Kronecker product, oversimplifying the interaction structure between modalities
and indicating a need to model more complex interactions. Additionally, the
joint distribution of latent representations with higher-order interactions is
underexplored. Copula is a powerful statistical structure for modelling the
interactions among variables, as it naturally bridges the joint distribution
and marginal distributions of multiple variables. We propose a novel
copula-driven multimodal learning framework, which focuses on learning the
joint distribution of various modalities to capture the complex interactions
among them. The key idea is to interpret the copula model as a tool to align
the marginal distributions of the modalities efficiently. By assuming a
Gaussian mixture distribution for each modality and a copula model on the joint
distribution, our model can generate accurate representations for missing
modalities. Extensive experiments on public MIMIC datasets demonstrate the
superior performance of our model over other competitors. The code is available
at https://github.com/HKU-MedAI/CMCM.

</details>


### [70] [Data-Efficient Realized Volatility Forecasting with Vision Transformers](https://arxiv.org/abs/2511.03046)
*Emi Soroka,Artem Arzyn*

Main category: cs.LG

TL;DR: 研究用ViT架构对期权数据进行预测，表明其能学习隐含波动率表面的特征，是模型开发的有前景方向。


<details>
  <summary>Details</summary>
Motivation: 金融机器学习中深度学习方法展现优势，但变压器模型在期权数据应用待探索，故开展相关初步研究。

Method: 训练通常用于图像识别和分类的Vision Transformer (ViT)架构，用单日隐含波动率表面（含日期信息）预测资产未来30天实现波动率。

Result: ViT能从隐含波动率表面学习季节性模式和非线性特征。

Conclusion: 为期权数据的模型开发提供了有前景的方向。

Abstract: Recent work in financial machine learning has shown the virtue of complexity:
the phenomenon by which deep learning methods capable of learning highly
nonlinear relationships outperform simpler approaches in financial forecasting.
While transformer architectures like Informer have shown promise for financial
time series forecasting, the application of transformer models for options data
remains largely unexplored. We conduct preliminary studies towards the
development of a transformer model for options data by training the Vision
Transformer (ViT) architecture, typically used in modern image recognition and
classification systems, to predict the realized volatility of an asset over the
next 30 days from its implied volatility surface (augmented with date
information) for a single day. We show that the ViT can learn seasonal patterns
and nonlinear features from the IV surface, suggesting a promising direction
for model development.

</details>


### [71] [Decoupled Entropy Minimization](https://arxiv.org/abs/2511.03256)
*Jing Ma,Hanlin Li,Xiang Xiang*

Main category: cs.LG

TL;DR: 文章重新分解经典熵最小化（EM），揭示其局限性，提出自适应解耦熵最小化（AdaDEM），在多种环境下表现更优。


<details>
  <summary>Details</summary>
Motivation: 研究经典EM的内在机制，解决其局限性问题。

Method: 将经典EM重新表述和解耦为CADF和GMC两部分，提出AdaDEM，对CADF奖励归一化并以MEC替代GMC。

Result: AdaDEM优于DEM*，在有噪声和动态环境的不完全监督学习任务中表现出色。

Conclusion: AdaDEM能有效解决经典EM的局限性，在多种环境下有更好的性能。

Abstract: Entropy Minimization (EM) is beneficial to reducing class overlap, bridging
domain gap, and restricting uncertainty for various tasks in machine learning,
yet its potential is limited. To study the internal mechanism of EM, we
reformulate and decouple the classical EM into two parts with opposite effects:
cluster aggregation driving factor (CADF) rewards dominant classes and prompts
a peaked output distribution, while gradient mitigation calibrator (GMC)
penalizes high-confidence classes based on predicted probabilities.
Furthermore, we reveal the limitations of classical EM caused by its coupled
formulation: 1) reward collapse impedes the contribution of high-certainty
samples in the learning process, and 2) easy-class bias induces misalignment
between output distribution and label distribution. To address these issues, we
propose Adaptive Decoupled Entropy Minimization (AdaDEM), which normalizes the
reward brought from CADF and employs a marginal entropy calibrator (MEC) to
replace GMC. AdaDEM outperforms DEM*, an upper-bound variant of classical EM,
and achieves superior performance across various imperfectly supervised
learning tasks in noisy and dynamic environments.

</details>


### [72] [Unsupervised Evaluation of Multi-Turn Objective-Driven Interactions](https://arxiv.org/abs/2511.03047)
*Emi Soroka,Tanmay Chopra,Krish Desai,Sanjay Lall*

Main category: cs.LG

TL;DR: 提出针对目标驱动交互的无监督指标，利用未标记交互数据统计特性和微调大语言模型，在开放域和特定任务交互数据上验证。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在企业应用的目标驱动交互系统难以评估，现有评估方法存在局限。

Method: 引入无监督指标，利用未标记交互数据统计特性和微调大语言模型，开发标记用户目标、衡量目标完成度和量化大语言模型不确定性的指标。

Result: 方法在开放域和特定任务交互数据上得到验证。

Conclusion: 所提出的无监督指标可用于目标驱动交互的评估。

Abstract: Large language models (LLMs) have seen increasing popularity in enterprise
applications where AI agents and humans engage in objective-driven
interactions. However, these systems are difficult to evaluate: data may be
complex and unlabeled; human annotation is often impractical at scale; custom
metrics can monitor for specific errors, but not previously-undetected ones;
and LLM judges can produce unreliable results. We introduce the first set of
unsupervised metrics for objective-driven interactions, leveraging statistical
properties of unlabeled interaction data and using fine-tuned LLMs to adapt to
distributional shifts. We develop metrics for labeling user goals, measuring
goal completion, and quantifying LLM uncertainty without grounding evaluations
in human-generated ideal responses. Our approach is validated on open-domain
and task-specific interaction data.

</details>


### [73] [The Curved Spacetime of Transformer Architectures](https://arxiv.org/abs/2511.03060)
*Riccardo Di Sipio,Jairo Diaz-Rodriguez,Luis Serrano*

Main category: cs.LG

TL;DR: 提出用几何框架理解Transformer语言模型，类比广义相对论，设计实验验证嵌入空间曲率的存在和影响。


<details>
  <summary>Details</summary>
Motivation: 为理解Transformer语言模型提供新视角，通过与广义相对论类比探索模型内部机制。

Method: 提出类比理论，设计三类实验：可视化段落曲率景观、模拟角度和长度比例、控制上下文编辑探测嵌入轨迹偏转。

Result: 可视化显示局部转向角随标记和层变化；模拟表明角度和长度比例不能用维度或偶然解释；探测实验证实注意力诱导的曲率导致嵌入轨迹有可测量、符合语义的弯曲。

Conclusion: Transformer语言模型中存在注意力诱导的嵌入空间曲率，该类比有助于理解模型。

Abstract: We present a geometric framework for understanding Transformer-based language
models, drawing an explicit analogy to General Relativity. Queries and keys
induce an effective metric on representation space, and attention acts as a
discrete connection that implements parallel transport of value vectors across
tokens. Stacked layers provide discrete time-slices through which token
representations evolve on this curved manifold, while backpropagation plays the
role of a least-action principle that shapes loss-minimizing trajectories in
parameter space. If this analogy is correct, token embeddings should not
traverse straight paths in feature space; instead, their layer-wise steps
should bend and reorient as interactions mediated by embedding space curvature.
To test this prediction, we design experiments that expose both the presence
and the consequences of curvature: (i) we visualize a curvature landscape for a
full paragraph, revealing how local turning angles vary across tokens and
layers; (ii) we show through simulations that excess counts of sharp/flat
angles and longer length-to-chord ratios are not explainable by dimensionality
or chance; and (iii) inspired by Einstein's eclipse experiment, we probe
deflection under controlled context edits, demonstrating measurable,
meaning-consistent bends in embedding trajectories that confirm
attention-induced curvature.

</details>


### [74] [Homomorphism distortion: A metric to distinguish them all and in the latent space bind them](https://arxiv.org/abs/2511.03068)
*Martin Carrasco,Olga Zaghen,Erik Bekkers,Bastian Rieck*

Main category: cs.LG

TL;DR: 本文提出图同态失真度量方法，可完整表征图，通过采样计算解决图规范化问题，在数据集上验证效果良好。


<details>
  <summary>Details</summary>
Motivation: 以往仅从组合属性衡量图神经网络表达能力，本文要提供一种衡量顶点属性图相似度的原则性方法。

Method: 提出图同态失真度量，通过采样高效计算该度量以绕过图规范化问题。

Result: 图同态失真度量能完全区分BREC数据集，在ZINC - 12k数据集上优于先前基于同态的方法。

Conclusion: 理论结果及其实证验证为图的未来表征铺平道路，拓展了图论传统。

Abstract: For far too long, expressivity of graph neural networks has been measured
\emph{only} in terms of combinatorial properties. In this work we stray away
from this tradition and provide a principled way to measure similarity between
vertex attributed graphs. We denote this measure as the \emph{graph
homomorphism distortion}. We show it can \emph{completely characterize} graphs
and thus is also a \emph{complete graph embedding}. However, somewhere along
the road, we run into the graph canonization problem. To circumvent this
obstacle, we devise to efficiently compute this measure via sampling, which in
expectation ensures \emph{completeness}. Additionally, we also discovered that
we can obtain a metric from this measure. We validate our claims empirically
and find that the \emph{graph homomorphism distortion}: (1.) fully
distinguishes the \texttt{BREC} dataset with up to $4$-WL non-distinguishable
graphs, and (2.) \emph{outperforms} previous methods inspired in homomorphisms
under the \texttt{ZINC-12k} dataset.
  These theoretical results, (and their empirical validation), pave the way for
future characterization of graphs, extending the graph theoretic tradition to
new frontiers.

</details>


### [75] [Why Less is More (Sometimes): A Theory of Data Curation](https://arxiv.org/abs/2511.03492)
*Elvis Dohmatob,Mohammad Pezeshki,Reyhane Askari-Hemmat*

Main category: cs.LG

TL;DR: 本文提出理论框架解决机器学习中‘何时少用数据更好’的悖论，给出精确缩放定律曲线，证明特定条件下小数据集可优于全量数据集，并通过实验验证。


<details>
  <summary>Details</summary>
Motivation: 解决现代机器学习中经典‘越多越好’缩放定律与‘少即是多’方法的矛盾，探究何时少用数据更好。

Method: 研究不完美神谕根据训练样本难度和正确性选择样本的数据筛选策略，推导标签无关和标签相关筛选规则下测试误差的精确缩放定律曲线。

Result: 得到精确缩放定律曲线，揭示保留部分数据可提升泛化能力的时机和原因；理论证明特定条件下小数据集可优于全量数据集；在ImageNet上的实验验证了理论预测；框架解释了大语言模型数学推理中矛盾的筛选策略。

Conclusion: 在特定条件下，小的筛选后数据集能比全量数据集表现更好，该理论框架能有效解释相关现象并指导数据筛选工作。

Abstract: This paper introduces a theoretical framework to resolve a central paradox in
modern machine learning: When is it better to use less data? This question has
become critical as classical scaling laws suggesting ``more is more'' (Sun et
al., 2025) are challenged by methods like LIMO (``less is more'') and s1 (Ye et
al., 2025; Muenighoff et al., 2025), which achieve superior performance with
small, aggressively curated datasets. Here, we study data curation strategies
where an imperfect oracle selects the training examples according to their
difficulty and correctness. Our results provide exact scaling law curves for
test error under both label-agnostic and label-aware curation rules, revealing
when and why keeping only a subset of data can improve generalization. In
contrast to classical scaling laws, we show that under certain conditions,
small curated datasets can outperform full datasets, and we provide analytical
conditions for this by deriving precise phase transition curves tied to data
size and quality. We validate these theoretical claims with empirical results
on ImageNet, confirming our predictions about when curation improves accuracy
and can even mitigate model collapse. Furthermore, our framework provides a
principled explanation for the contradictory curation strategies recently
observed in LLM mathematical reasoning.

</details>


### [76] [Online Learning to Rank under Corruption: A Robust Cascading Bandits Approach](https://arxiv.org/abs/2511.03074)
*Fatemeh Ghaffari,Siddarth Sitaraman,Xutong Liu,Xuchuang Wang,Mohammad Hajiesmaili*

Main category: cs.LG

TL;DR: 提出MSUCB算法应对在线学习排序中的点击欺诈，无欺诈时达最优对数遗憾，有欺诈时表现良好，实验显示优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有在线学习排序系统易受点击欺诈等操纵影响，误导学习过程并降低用户体验。

Method: 提出MSUCB算法，采用新颖的中位数均值估计器，无欺诈时如标准均值，有欺诈时过滤异常值和受污染样本。

Result: MSUCB在无欺诈时达最优对数遗憾，有欺诈时遗憾仅增加与总欺诈相关的附加项，在真实数据集实验中始终优于先前方法，对两种最先进方法分别实现97.35%和91.60%的遗憾改进。

Conclusion: MSUCB算法有效且鲁棒，能应对点击欺诈等操纵问题，性能优于现有方法。

Abstract: Online learning to rank (OLTR) studies how to recommend a short ranked list
of items from a large pool and improves future rankings based on user clicks.
This setting is commonly modeled as cascading bandits, where the objective is
to maximize the likelihood that the user clicks on at least one of the
presented items across as many timesteps as possible. However, such systems are
vulnerable to click fraud and other manipulations (i.e., corruption), where
bots or paid click farms inject corrupted feedback that misleads the learning
process and degrades user experience. In this paper, we propose MSUCB, a robust
algorithm that incorporates a novel mean-of-medians estimator, which to our
knowledge is applied to bandits with corruption setting for the first time.
This estimator behaves like a standard mean in the absence of corruption, so no
cost is paid for robustness. Under corruption, the median step filters out
outliers and corrupted samples, keeping the estimate close to its true value.
Updating this estimate at every round further accelerates empirical convergence
in experiments. Hence, MSUCB achieves optimal logarithmic regret in the absence
of corruption and degrades gracefully under corruptions, with regret increasing
only by an additive term tied to the total corruption. Comprehensive and
extensive experiments on real-world datasets further demonstrate that our
approach consistently outperforms prior methods while maintaining strong
robustness. In particular, it achieves a \(97.35\%\) and a \(91.60\%\) regret
improvement over two state-of-the-art methods.

</details>


### [77] [Towards Formalizing Reinforcement Learning Theory](https://arxiv.org/abs/2511.03618)
*Shangtong Zhang*

Main category: cs.LG

TL;DR: 本文使用Lean 4定理证明器形式化验证了Q学习和线性TD学习的几乎必然收敛性。


<details>
  <summary>Details</summary>
Motivation: Q学习和线性TD学习是早期且有影响力的强化学习算法，研究其收敛性是早期重要课题且如今受关注，本文旨在统一框架下验证其收敛性。

Method: 基于Mathlib库，使用Lean 4定理证明器，基于Robbins - Siegmund定理在统一框架下验证。

Result: 在统一框架下形式化验证了Q学习和线性TD学习的几乎必然收敛性，框架可扩展到收敛速率等。

Conclusion: 本文为完全形式化收敛的强化学习结果迈出重要一步。

Abstract: In this paper, we formalize the almost sure convergence of $Q$-learning and
linear temporal difference (TD) learning with Markovian samples using the Lean
4 theorem prover based on the Mathlib library. $Q$-learning and linear TD are
among the earliest and most influential reinforcement learning (RL) algorithms.
The investigation of their convergence properties is not only a major research
topic during the early development of the RL field but also receives increasing
attention nowadays. This paper formally verifies their almost sure convergence
in a unified framework based on the Robbins-Siegmund theorem. The framework
developed in this work can be easily extended to convergence rates and other
modes of convergence. This work thus makes an important step towards fully
formalizing convergent RL results. The code is available at
https://github.com/ShangtongZhang/rl-theory-in-lean.

</details>


### [78] [Sparse, self-organizing ensembles of local kernels detect rare statistical anomalies](https://arxiv.org/abs/2511.03095)
*Gaia Grosso,Sai Sumedh R. Hindupur,Thomas Fel,Samuel Bright-Thonney,Philip Harris,Demba Ba*

Main category: cs.LG

TL;DR: 现代人工智能数据表征统计特性难控致异常检测方法失效，提出结构要求，引入SparKer方法并在多领域验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决现代人工智能数据表征统计特性控制不佳，导致异常检测方法失效，难以检测和解释异常的问题。

Method: 确定检测方法的结构要求，引入SparKer，在半监督Neyman - Pearson框架下训练高斯核稀疏集成。

Result: 在多领域应用中，少量核的集成能在高维表征空间识别显著异常位置。

Conclusion: 所提方法具有可解释性、高效性和可扩展性。

Abstract: Modern artificial intelligence has revolutionized our ability to extract rich
and versatile data representations across scientific disciplines. Yet, the
statistical properties of these representations remain poorly controlled,
causing misspecified anomaly detection (AD) methods to falter. Weak or rare
signals can remain hidden within the apparent regularity of normal data,
creating a gap in our ability to detect and interpret anomalies. We examine
this gap and identify a set of structural desiderata for detection methods
operating under minimal prior information: sparsity, to enforce parsimony;
locality, to preserve geometric sensitivity; and competition, to promote
efficient allocation of model capacity. These principles define a class of
self-organizing local kernels that adaptively partition the representation
space around regions of statistical imbalance. As an instantiation of these
principles, we introduce SparKer, a sparse ensemble of Gaussian kernels trained
within a semi-supervised Neyman--Pearson framework to locally model the
likelihood ratio between a sample that may contain anomalies and a nominal,
anomaly-free reference. We provide theoretical insights into the mechanisms
that drive detection and self-organization in the proposed model, and
demonstrate the effectiveness of this approach on realistic high-dimensional
problems of scientific discovery, open-world novelty detection, intrusion
detection, and generative-model validation. Our applications span both the
natural- and computer-science domains. We demonstrate that ensembles containing
only a handful of kernels can identify statistically significant anomalous
locations within representation spaces of thousands of dimensions, underscoring
both the interpretability, efficiency and scalability of the proposed approach.

</details>


### [79] [Scaling Multi-Agent Environment Co-Design with Diffusion Models](https://arxiv.org/abs/2511.03100)
*Hao Xiang Li,Michael Amir,Amanda Prorok*

Main category: cs.LG

TL;DR: 提出可扩展且样本高效的Diffusion Co - Design (DiCoDe)框架解决当前协同设计方法扩展性问题，在多智能体环境协同设计基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前协同设计方法难以扩展，在高维环境设计空间和联合优化中存在样本效率低的问题。

Method: 开发DiCoDe框架，引入Projected Universal Guidance (PUG)采样技术和批评家蒸馏机制。

Result: 在多智能体环境协同设计基准测试中取得更好的环境 - 策略对，如在仓库场景中奖励提高39%，模拟样本减少66%。

Conclusion: DiCoDe框架为智能体 - 环境协同设计设定了新标准，是在现实领域收获协同设计成果的垫脚石。

Abstract: The agent-environment co-design paradigm jointly optimises agent policies and
environment configurations in search of improved system performance. With
application domains ranging from warehouse logistics to windfarm management,
co-design promises to fundamentally change how we deploy multi-agent systems.
However, current co-design methods struggle to scale. They collapse under
high-dimensional environment design spaces and suffer from sample inefficiency
when addressing moving targets inherent to joint optimisation. We address these
challenges by developing Diffusion Co-Design (DiCoDe), a scalable and
sample-efficient co-design framework pushing co-design towards practically
relevant settings. DiCoDe incorporates two core innovations. First, we
introduce Projected Universal Guidance (PUG), a sampling technique that enables
DiCoDe to explore a distribution of reward-maximising environments while
satisfying hard constraints such as spatial separation between obstacles.
Second, we devise a critic distillation mechanism to share knowledge from the
reinforcement learning critic, ensuring that the guided diffusion model adapts
to evolving agent policies using a dense and up-to-date learning signal.
Together, these improvements lead to superior environment-policy pairs when
validated on challenging multi-agent environment co-design benchmarks including
warehouse automation, multi-agent pathfinding and wind farm optimisation. Our
method consistently exceeds the state-of-the-art, achieving, for example, 39%
higher rewards in the warehouse setting with 66% fewer simulation samples. This
sets a new standard in agent-environment co-design, and is a stepping stone
towards reaping the rewards of co-design in real world domains.

</details>


### [80] [An Efficient Classification Model for Cyber Text](https://arxiv.org/abs/2511.03107)
*Md Sakhawat Hossen,Md. Zashid Iqbal Borshon,A. S. M. Badrudduza*

Main category: cs.LG

TL;DR: 本文提出CTF - IDF用于数据预处理，结合IRLBA算法在文本分析中采用经典机器学习技术，相比深度学习更高效、低碳，实验显示时间复杂度降低且模型精度提升。


<details>
  <summary>Details</summary>
Motivation: 深度学习因对计算资源和电力需求大，导致碳足迹增加，故探索更高效低碳的文本分析方法。

Method: 修改TF - IDF算法提出CTF - IDF用于数据预处理，采用更快的IRLBA算法进行降维，在传统文本分析流程中引入这两种技术。

Result: 实验结果显示时间复杂度大幅降低，文中讨论的经典机器学习方法模型精度有所提高。

Conclusion: 经典机器学习技术结合CTF - IDF和IRLBA算法在文本分析中更高效、快速、计算量小，在碳足迹方面优于深度学习，仅在精度上有小幅度妥协。

Abstract: The uprising of deep learning methodology and practice in recent years has
brought about a severe consequence of increasing carbon footprint due to the
insatiable demand for computational resources and power. The field of text
analytics also experienced a massive transformation in this trend of
monopolizing methodology. In this paper, the original TF-IDF algorithm has been
modified, and Clement Term Frequency-Inverse Document Frequency (CTF-IDF) has
been proposed for data preprocessing. This paper primarily discusses the
effectiveness of classical machine learning techniques in text analytics with
CTF-IDF and a faster IRLBA algorithm for dimensionality reduction. The
introduction of both of these techniques in the conventional text analytics
pipeline ensures a more efficient, faster, and less computationally intensive
application when compared with deep learning methodology regarding carbon
footprint, with minor compromise in accuracy. The experimental results also
exhibit a manifold of reduction in time complexity and improvement of model
accuracy for the classical machine learning methods discussed further in this
paper.

</details>


### [81] [Towards Scalable Backpropagation-Free Gradient Estimation](https://arxiv.org/abs/2511.03110)
*Daniel Wang,Evan Markou,Dylan Campbell*

Main category: cs.LG

TL;DR: 提出一种减少偏差和方差的梯度估计方法，有望扩展到更大网络。


<details>
  <summary>Details</summary>
Motivation: 反向传播需两次网络遍历和存储中间激活值，现有前向模式自动微分梯度估计方法在大网络中因估计方差高难以扩展，且缓解方法会引入显著偏差。

Method: 在计算猜测方向时操纵上游雅可比矩阵以减少偏差和方差。

Result: 显示出有前景的结果，网络宽度增加时表现更好。

Conclusion: 所提方法有潜力扩展到更大网络，对偏差和方差的分析有助于理解该方法。

Abstract: While backpropagation--reverse-mode automatic differentiation--has been
extraordinarily successful in deep learning, it requires two passes (forward
and backward) through the neural network and the storage of intermediate
activations. Existing gradient estimation methods that instead use forward-mode
automatic differentiation struggle to scale beyond small networks due to the
high variance of the estimates. Efforts to mitigate this have so far introduced
significant bias to the estimates, reducing their utility. We introduce a
gradient estimation approach that reduces both bias and variance by
manipulating upstream Jacobian matrices when computing guess directions. It
shows promising results and has the potential to scale to larger networks,
indeed performing better as the network width is increased. Our understanding
of this method is facilitated by analyses of bias and variance, and their
connection to the low-dimensional structure of neural network gradients.

</details>


### [82] [FP-AbDiff: Improving Score-based Antibody Design by Capturing Nonequilibrium Dynamics through the Underlying Fokker-Planck Equation](https://arxiv.org/abs/2511.03113)
*Jiameng Chen,Yida Xiong,Kun Li,Hongzhi Zhang,Xiantao Cai,Wenbin Hu,Jia Wu*

Main category: cs.LG

TL;DR: 本文提出抗体生成器FP - AbDiff，结合物理方程与生物先验，在RAbD基准测试中达新的最优水平，提升抗体设计鲁棒性和泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有抗体生成模型存在动力学不一致和泛化性差的问题，限制了计算抗体设计在治疗发现中的应用。

Method: 引入FP - AbDiff，在整个生成轨迹上强制执行福克 - 普朗克方程（FPE）物理，最小化CDR几何混合流形上的FPE残差损失，将物理正则化器与生物先验集成到SE(3) - 等变扩散框架中。

Result: 在RAbD基准测试中，FP - AbDiff在CDR - H3设计和六 - CDR协同设计任务中表现优于现有模型，如CDR - H3设计的均方根偏差有25%的提升等。

Conclusion: FP - AbDiff通过使生成动力学符合物理定律，增强了抗体设计的鲁棒性和泛化性，为抗体设计提供了原则性方法。

Abstract: Computational antibody design holds immense promise for therapeutic
discovery, yet existing generative models are fundamentally limited by two core
challenges: (i) a lack of dynamical consistency, which yields physically
implausible structures, and (ii) poor generalization due to data scarcity and
structural bias. We introduce FP-AbDiff, the first antibody generator to
enforce Fokker-Planck Equation (FPE) physics along the entire generative
trajectory. Our method minimizes a novel FPE residual loss over the mixed
manifold of CDR geometries (R^3 x SO(3)), compelling locally-learned denoising
scores to assemble into a globally coherent probability flow. This
physics-informed regularizer is synergistically integrated with deep biological
priors within a state-of-the-art SE(3)-equivariant diffusion framework.
Rigorous evaluation on the RAbD benchmark confirms that FP-AbDiff establishes a
new state-of-the-art. In de novo CDR-H3 design, it achieves a mean Root Mean
Square Deviation of 0.99 {\AA} when superposing on the variable region, a 25%
improvement over the previous state-of-the-art model, AbX, and the highest
reported Contact Amino Acid Recovery of 39.91%. This superiority is underscored
in the more challenging six-CDR co-design task, where our model delivers
consistently superior geometric precision, cutting the average full-chain Root
Mean Square Deviation by ~15%, and crucially, achieves the highest full-chain
Amino Acid Recovery on the functionally dominant CDR-H3 loop (45.67%). By
aligning generative dynamics with physical laws, FP-AbDiff enhances robustness
and generalizability, establishing a principled approach for physically
faithful and functionally viable antibody design.

</details>


### [83] [An Augmentation Overlap Theory of Contrastive Learning](https://arxiv.org/abs/2511.03114)
*Qi Zhang,Yifei Wang,Yisen Wang*

Main category: cs.LG

TL;DR: 本文研究自监督对比学习的工作机制，基于条件独立假设给出最紧边界，放宽假设到增强重叠假设推导下游性能渐近封闭边界，还开发了无监督度量。


<details>
  <summary>Details</summary>
Motivation: 自监督对比学习虽成功但工作机制不明。

Method: 先基于条件独立假设给出边界，再放宽到增强重叠假设推导边界，从增强重叠视角开发无监督度量。

Result: 得到了相关边界，开发出与下游性能匹配良好且几乎不依赖额外模块的无监督度量。

Conclusion: 提出的增强重叠理论解释了对比学习工作机制，开发的无监督度量有效。

Abstract: Recently, self-supervised contrastive learning has achieved great success on
various tasks. However, its underlying working mechanism is yet unclear. In
this paper, we first provide the tightest bounds based on the widely adopted
assumption of conditional independence. Further, we relax the conditional
independence assumption to a more practical assumption of augmentation overlap
and derive the asymptotically closed bounds for the downstream performance. Our
proposed augmentation overlap theory hinges on the insight that the support of
different intra-class samples will become more overlapped under aggressive data
augmentations, thus simply aligning the positive samples (augmented views of
the same sample) could make contrastive learning cluster intra-class samples
together. Moreover, from the newly derived augmentation overlap perspective, we
develop an unsupervised metric for the representation evaluation of contrastive
learning, which aligns well with the downstream performance almost without
relying on additional modules. Code is available at
https://github.com/PKU-ML/GARC.

</details>


### [84] [From Insight to Exploit: Leveraging LLM Collaboration for Adaptive Adversarial Text Generation](https://arxiv.org/abs/2511.03128)
*Najrin Sultana,Md Rafi Ur Rashid,Kang Gu,Shagufta Mehnaz*

Main category: cs.LG

TL;DR: 本文提出两种攻击框架评估大语言模型对对抗输入的鲁棒性，攻击可随模型发展而进化且有强迁移性，还提供了代码和数据。


<details>
  <summary>Details</summary>
Motivation: 在将大语言模型应用于敏感任务时，需评估其对对抗输入的鲁棒性。

Method: 引入Static Deceptor (StaDec)和Dynamic Deceptor (DyDec)两种攻击框架，利用对大语言模型的理解生成动态、自适应的对抗样本，采用自动化、大语言模型驱动的流程，不依赖外部启发式方法。

Result: 生成的对抗输入语义与原文相似，能有效欺骗目标大语言模型，攻击可随模型发展进化，且在攻击者未知的模型间有强迁移性。

Conclusion: 为大语言模型的自我鲁棒性评估提供了系统方法。

Abstract: LLMs can provide substantial zero-shot performance on diverse tasks using a
simple task prompt, eliminating the need for training or fine-tuning. However,
when applying these models to sensitive tasks, it is crucial to thoroughly
assess their robustness against adversarial inputs. In this work, we introduce
Static Deceptor (StaDec) and Dynamic Deceptor (DyDec), two innovative attack
frameworks designed to systematically generate dynamic and adaptive adversarial
examples by leveraging the understanding of the LLMs. We produce subtle and
natural-looking adversarial inputs that preserve semantic similarity to the
original text while effectively deceiving the target LLM. By utilizing an
automated, LLM-driven pipeline, we eliminate the dependence on external
heuristics. Our attacks evolve with the advancements in LLMs and demonstrate
strong transferability across models unknown to the attacker. Overall, this
work provides a systematic approach for the self-assessment of an LLM's
robustness. We release our code and data at
https://github.com/Shukti042/AdversarialExample.

</details>


### [85] [Test Time Adaptation Using Adaptive Quantile Recalibration](https://arxiv.org/abs/2511.03148)
*Paria Mehrbod,Pedro Vianna,Geraldin Nanfack,Guy Wolf,Eugene Belilovsky*

Main category: cs.LG

TL;DR: 提出测试时自适应技术AQR，可修改预激活分布，实验表明其在不同设置下表现出色，有部署潜力。


<details>
  <summary>Details</summary>
Motivation: 传统领域自适应方法依赖目标域先验知识或需模型再训练，现有测试时自适应方法有局限性，需更好方法。

Method: 提出Adaptive Quantile Recalibration (AQR)，按通道对齐分位数修改预激活分布，包含鲁棒尾校准策略，利用训练时源域统计信息。

Result: 在多个数据集和架构上实验，AQR在不同设置下实现稳健自适应，优于现有测试时自适应基线。

Conclusion: AQR有在动态和不可预测数据分布的现实场景中部署的潜力。

Abstract: Domain adaptation is a key strategy for enhancing the generalizability of
deep learning models in real-world scenarios, where test distributions often
diverge significantly from the training domain. However, conventional
approaches typically rely on prior knowledge of the target domain or require
model retraining, limiting their practicality in dynamic or
resource-constrained environments. Recent test-time adaptation methods based on
batch normalization statistic updates allow for unsupervised adaptation, but
they often fail to capture complex activation distributions and are constrained
to specific normalization layers. We propose Adaptive Quantile Recalibration
(AQR), a test-time adaptation technique that modifies pre-activation
distributions by aligning quantiles on a channel-wise basis. AQR captures the
full shape of activation distributions and generalizes across architectures
employing BatchNorm, GroupNorm, or LayerNorm. To address the challenge of
estimating distribution tails under varying batch sizes, AQR incorporates a
robust tail calibration strategy that improves stability and precision. Our
method leverages source-domain statistics computed at training time, enabling
unsupervised adaptation without retraining models. Experiments on CIFAR-10-C,
CIFAR-100-C, and ImageNet-C across multiple architectures demonstrate that AQR
achieves robust adaptation across diverse settings, outperforming existing
test-time adaptation baselines. These results highlight AQR's potential for
deployment in real-world scenarios with dynamic and unpredictable data
distributions.

</details>


### [86] [Forecast2Anomaly (F2A): Adapting Multivariate Time Series Foundation Models for Anomaly Prediction](https://arxiv.org/abs/2511.03149)
*Atif Hassan,Tarun Kumar,Ashish Mishra,Sergey Serebryakov,Satish Kumar Mopur,Phanidhar Koganti,Murthy Chelankuri,Ramanagopal Vogety,Suparna Bhattacharya,Martin Foltin*

Main category: cs.LG

TL;DR: 文章提出Forecast2Anomaly (F2A)框架让预训练时间序列基础模型具备异常预测能力，实验显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多元时间序列异常预测方法泛化能力不足，预训练时间序列基础模型在异常预测方面潜力未被挖掘。

Method: 提出联合预测 - 异常损失微调模型；引入检索增强生成模块动态适应分布变化。

Result: 在16个不同数据集和多个模型骨干上实验，F2A始终优于现有方法。

Conclusion: F2A弥合了时间序列基础模型零样本预测和零样本异常预测的差距，为实际应用提供可扩展的零样本异常预测解决方案。

Abstract: Forecasting anomalies (anomaly prediction) in multivariate time series from
different real-world, dynamic, and complex systems is vital for preempting
critical failures, leading to a substantial minimization in operational costs
and human labor. Yet, existing methods are limited to specific systems while
failing to generalize to evolving anomaly patterns over time. In contrast,
pretrained Time Series Foundation Models (TSFMs) have recently demonstrated
strong generalization and zero-shot forecasting capabilities. However, their
potential remains untapped for anomaly prediction, a task fundamentally
different from forecasting normal behavior. Thus, we present Forecast2Anomaly
(F2A), a novel framework that empowers TSFMs with anomaly prediction abilities
through two key innovations. First, we propose a joint forecast-anomaly loss
that fine-tunes TSFMs to accurately forecast future signals even at anomalous
time points. Second, we introduce a Retrieval-Augmented Generation (RAG) module
that retrieves historically relevant horizons and conditions predictions on
them. This component dynamically adapts to distributional shifts at inference
time, enabling F2A to track evolving anomalies without requiring model updates.
By combining targeted fine-tuning with dynamic retrieval, F2A bridges the gap
between robust TSFM zero-shot forecasting and zero-shot anomaly prediction.
Extensive experiments across 16 diverse datasets and multiple TSFM backbones
show that F2A consistently outperforms state-of-the-art methods, offering a
scalable, zero-shot anomaly prediction solution for real-world applications.

</details>


### [87] [UnCLe: Towards Scalable Dynamic Causal Discovery in Non-linear Temporal Systems](https://arxiv.org/abs/2511.03168)
*Tingzhu Bi,Yicheng Pan,Xinrui Jiang,Huize Sun,Meng Ma,Ping Wang*

Main category: cs.LG

TL;DR: 本文提出用于可扩展动态因果发现的深度学习方法UnCLe，实验表明其在静态因果发现基准上表现出色，且能捕捉动态系统的时间因果性。


<details>
  <summary>Details</summary>
Motivation: 现有方法多推断静态因果图，而现实世界系统常存在动态因果关系，需要时间分辨的因果图来捕捉这些时间动态。

Method: 提出UnCLe方法，使用解耦器和重耦合器网络将输入时间序列解耦为语义表示，通过自回归依赖矩阵学习变量间依赖关系，分析时间扰动引起的数据点预测误差来估计动态因果影响。

Result: UnCLe在静态因果发现基准上优于现有基线，能准确捕捉和表示合成与现实世界动态系统中的时间因果性。

Conclusion: UnCLe为揭示复杂现象潜在的时变机制提供了有前景的方法。

Abstract: Uncovering cause-effect relationships from observational time series is
fundamental to understanding complex systems. While many methods infer static
causal graphs, real-world systems often exhibit dynamic causality-where
relationships evolve over time. Accurately capturing these temporal dynamics
requires time-resolved causal graphs. We propose UnCLe, a novel deep learning
method for scalable dynamic causal discovery. UnCLe employs a pair of Uncoupler
and Recoupler networks to disentangle input time series into semantic
representations and learns inter-variable dependencies via auto-regressive
Dependency Matrices. It estimates dynamic causal influences by analyzing
datapoint-wise prediction errors induced by temporal perturbations. Extensive
experiments demonstrate that UnCLe not only outperforms state-of-the-art
baselines on static causal discovery benchmarks but, more importantly, exhibits
a unique capability to accurately capture and represent evolving temporal
causality in both synthetic and real-world dynamic systems (e.g., human
motion). UnCLe offers a promising approach for revealing the underlying,
time-varying mechanisms of complex phenomena.

</details>


### [88] [Periodic Skill Discovery](https://arxiv.org/abs/2511.03187)
*Jonghae Park,Daesol Cho,Jusuk Lee,Dongseok Shim,Inkyu Jang,H. Jin Kim*

Main category: cs.LG

TL;DR: 提出无监督周期性技能发现框架PSD，可在复杂机器人任务中学习不同周期技能，在下游任务表现好，与现有方法结合能增加行为多样性。


<details>
  <summary>Details</summary>
Motivation: 现有无监督技能发现方法常忽略技能周期性，而很多机器人任务需周期性行为，所以要发现多样化周期性技能。

Method: 提出PSD框架，训练将状态映射到圆形潜在空间的编码器，以在潜在表示中自然编码周期性。

Result: 能在复杂机器人任务中有效学习不同周期技能，在下游任务如跨栏中表现出色，与现有技能发现方法结合可提供更多样行为。

Conclusion: PSD框架可无监督发现周期性行为，有良好应用效果和拓展性。

Abstract: Unsupervised skill discovery in reinforcement learning (RL) aims to learn
diverse behaviors without relying on external rewards. However, current methods
often overlook the periodic nature of learned skills, focusing instead on
increasing the mutual dependence between states and skills or maximizing the
distance traveled in latent space. Considering that many robotic tasks --
particularly those involving locomotion -- require periodic behaviors across
varying timescales, the ability to discover diverse periodic skills is
essential. Motivated by this, we propose Periodic Skill Discovery (PSD), a
framework that discovers periodic behaviors in an unsupervised manner. The key
idea of PSD is to train an encoder that maps states to a circular latent space,
thereby naturally encoding periodicity in the latent representation. By
capturing temporal distance, PSD can effectively learn skills with diverse
periods in complex robotic tasks, even with pixel-based observations. We
further show that these learned skills achieve high performance on downstream
tasks such as hurdling. Moreover, integrating PSD with an existing skill
discovery method offers more diverse behaviors, thus broadening the agent's
repertoire. Our code and demos are available at
https://jonghaepark.github.io/psd/

</details>


### [89] [Efficient Linear Attention for Multivariate Time Series Modeling via Entropy Equality](https://arxiv.org/abs/2511.03190)
*Mingtao Zhang,Guoli Yang,Zhanxing Zhu,Mengzhu Wang,Xiaoying Bai*

Main category: cs.LG

TL;DR: 提出新的线性注意力机制，基于熵理论开发近似算法，实验验证其在时空时间序列建模中效果好，减少内存和计算时间。


<details>
  <summary>Details</summary>
Motivation: 现有注意力机制计算复杂度为二次方，限制长序列可扩展性，需新机制克服此局限。

Method: 理论证明熵相关性质，开发线性复杂度的近似算法，实现基于熵相等的线性注意力机制。

Result: 实验表明该方法在四个时空数据集上有竞争力或更优的预测性能，大幅减少内存使用和计算时间。

Conclusion: 注意力在时空时间序列建模中的有效性可能源于适度且平衡的权重分布，而非softmax的非线性。

Abstract: Attention mechanisms have been extensively employed in various applications,
including time series modeling, owing to their capacity to capture intricate
dependencies; however, their utility is often constrained by quadratic
computational complexity, which impedes scalability for long sequences. In this
work, we propose a novel linear attention mechanism designed to overcome these
limitations. Our approach is grounded in a theoretical demonstration that
entropy, as a strictly concave function on the probability simplex, implies
that distributions with aligned probability rankings and similar entropy values
exhibit structural resemblance. Building on this insight, we develop an
efficient approximation algorithm that computes the entropy of
dot-product-derived distributions with only linear complexity, enabling the
implementation of a linear attention mechanism based on entropy equality.
Through rigorous analysis, we reveal that the effectiveness of attention in
spatio-temporal time series modeling may not primarily stem from the
non-linearity of softmax but rather from the attainment of a moderate and
well-balanced weight distribution. Extensive experiments on four
spatio-temporal datasets validate our method, demonstrating competitive or
superior forecasting performance while achieving substantial reductions in both
memory usage and computational time.

</details>


### [90] [A Probabilistic U-Net Approach to Downscaling Climate Simulations](https://arxiv.org/abs/2511.03197)
*Maryam Alipourhajiagha,Pierre-Louis Lemaire,Youssef Diouane,Julie Carreau*

Main category: cs.LG

TL;DR: 本文采用概率U - Net进行气候数据统计降尺度，评估四种训练目标，发现WMSE - MS - SSIM在特定设置下对极端值表现好，afCRPS更能捕捉跨尺度空间变异性。


<details>
  <summary>Details</summary>
Motivation: 气候模型计算成本高、输出空间分辨率粗，而气候变化影响研究需要更精细尺度，需统计降尺度方法填补差距。

Method: 采用概率U - Net，结合确定性U - Net主干和变分潜在空间，评估afCRPS和WMSE - MS - SSIM四种训练目标。

Result: WMSE - MS - SSIM在特定设置下对极端值表现好，afCRPS更能捕捉跨尺度空间变异性。

Conclusion: 概率U - Net结合特定训练目标可用于气候数据统计降尺度，不同训练目标有不同优势。

Abstract: Climate models are limited by heavy computational costs, often producing
outputs at coarse spatial resolutions, while many climate change impact studies
require finer scales. Statistical downscaling bridges this gap, and we adapt
the probabilistic U-Net for this task, combining a deterministic U-Net backbone
with a variational latent space to capture aleatoric uncertainty. We evaluate
four training objectives, afCRPS and WMSE-MS-SSIM with three settings for
downscaling precipitation and temperature from $16\times$ coarser resolution.
Our main finding is that WMSE-MS-SSIM performs well for extremes under certain
settings, whereas afCRPS better captures spatial variability across scales.

</details>


### [91] [A Quantized VAE-MLP Botnet Detection Model: A Systematic Evaluation of Quantization-Aware Training and Post-Training Quantization Strategies](https://arxiv.org/abs/2511.03201)
*Hassan Wasswa,Hussein Abbass,Timothy Lynar*

Main category: cs.LG

TL;DR: 为应对物联网僵尸网络攻击，研究提出VAE - MLP模型框架，评估两种量化策略在物联网僵尸网络检测中的效果，PTQ在检测精度、速度和大小上表现较好。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法计算强度大，难以部署在资源受限的物联网设备上，需要轻量级检测模型。

Method: 提出VAE - MLP模型框架，用预训练VAE编码器得到8维潜在向量训练MLP分类器，用N - BaIoT和CICIoT2022数据集评估QAT和PTQ两种量化策略。

Result: QAT检测精度下降明显，PTQ仅有轻微降低；PTQ加速6倍、大小减少21倍，QAT加速3倍、压缩24倍。

Conclusion: 量化策略对设备级物联网僵尸网络检测具有实用性。

Abstract: In an effort to counter the increasing IoT botnet-based attacks,
state-of-the-art deep learning methods have been proposed and have achieved
impressive detection accuracy. However, their computational intensity restricts
deployment on resource-constrained IoT devices, creating a critical need for
lightweight detection models. A common solution to this challenge is model
compression via quantization. This study proposes a VAE-MLP model framework
where an MLP-based classifier is trained on 8-dimensional latent vectors
derived from the high-dimensional train data using the encoder component of a
pretrained variational autoencoder (VAE). Two widely used quantization
strategies--Quantization-Aware Training (QAT) and Post-Training Quantization
(PTQ)--are then systematically evaluated in terms of their impact on detection
performance, storage efficiency, and inference latency using two benchmark IoT
botnet datasets--N-BaIoT and CICIoT2022. The results revealed that, with
respect to detection accuracy, the QAT strategy experienced a more noticeable
decline,whereas PTQ incurred only a marginal reduction compared to the original
unquantized model. Furthermore, PTQ yielded a 6x speedup and 21x reduction in
size, while QAT achieved a 3x speedup and 24x compression, demonstrating the
practicality of quantization for device-level IoT botnet detection.

</details>


### [92] [Incorporating Quality of Life in Climate Adaptation Planning via Reinforcement Learning](https://arxiv.org/abs/2511.03238)
*Miguel Costa,Arthur Vandervoort,Martin Drews,Karyn Morrissey,Francisco C. Pereira*

Main category: cs.LG

TL;DR: 本文用强化学习结合综合评估模型识别长期提升城市生活质量的气候适应路径，初步结果显示该方法能学习最优适应措施且优于其他规划策略，框架已公开。


<details>
  <summary>Details</summary>
Motivation: 气候变化使城市洪水频发、影响生活质量，政策制定者需应对气候变化不确定性和城市洪水复杂性，强化学习有解决此类问题的潜力。

Method: 使用强化学习，结合包含降雨预测、洪水、交通可达性和生活质量指数的综合评估模型。

Result: 该方法能学习最优适应措施，且性能优于其他现实规划策略。

Conclusion: 所提出的方法可用于识别长期提升城市生活质量的气候适应路径，且框架已公开供使用。

Abstract: Urban flooding is expected to increase in frequency and severity as a
consequence of climate change, causing wide-ranging impacts that include a
decrease in urban Quality of Life (QoL). Meanwhile, policymakers must devise
adaptation strategies that can cope with the uncertain nature of climate change
and the complex and dynamic nature of urban flooding. Reinforcement Learning
(RL) holds significant promise in tackling such complex, dynamic, and uncertain
problems. Because of this, we use RL to identify which climate adaptation
pathways lead to a higher QoL in the long term. We do this using an Integrated
Assessment Model (IAM) which combines a rainfall projection model, a flood
model, a transport accessibility model, and a quality of life index. Our
preliminary results suggest that this approach can be used to learn optimal
adaptation measures and it outperforms other realistic and real-world planning
strategies. Our framework is publicly available:
https://github.com/MLSM-at-DTU/maat_qol_framework.

</details>


### [93] [A Feedback-Control Framework for Efficient Dataset Collection from In-Vehicle Data Streams](https://arxiv.org/abs/2511.03239)
*Philipp Reis,Philipp Rigoll,Christian Steinhauser,Jacob Langner,Eric Sax*

Main category: cs.LG

TL;DR: 现代AI系统受数据质量和多样性制约，论文提出FCDC范式将数据收集转为闭环控制问题，实验表明其可使数据集更平衡并减少存储。


<details>
  <summary>Details</summary>
Motivation: 当前多数数据集以开环方式收集，存在存储低效、标注成本高和泛化能力有限等问题。

Method: 引入FCDC范式，用在线概率模型近似收集数据分布状态，基于似然和马氏距离等反馈信号自适应调节样本保留。

Result: 在合成数据集展示可控性，在真实数据流实验中使数据集更平衡25.9%，减少数据存储39.8%。

Conclusion: 数据收集可被主动控制，成为以数据为中心AI的核心自调节、反馈驱动过程。

Abstract: Modern AI systems are increasingly constrained not by model capacity but by
the quality and diversity of their data. Despite growing emphasis on
data-centric AI, most datasets are still gathered in an open-loop manner which
accumulates redundant samples without feedback from the current coverage. This
results in inefficient storage, costly labeling, and limited generalization. To
address this, this paper introduces \ac{FCDC}, a paradigm that formulates data
collection as a closed-loop control problem. \ac{FCDC} continuously
approximates the state of the collected data distribution using an online
probabilistic model and adaptively regulates sample retention using based on
feedback signals such as likelihood and Mahalanobis distance. Through this
feedback mechanism, the system dynamically balances exploration and
exploitation, maintains dataset diversity, and prevents redundancy from
accumulating over time. Besides showcasing the controllability of \ac{FCDC} on
a synthetic dataset, experiments on a real data stream show that \ac{FCDC}
produces more balanced datasets by $\SI{25.9}{\percent}$ while reducing data
storage by $\SI{39.8}{\percent}$. These results demonstrate that data
collection itself can be actively controlled, transforming collection from a
passive pipeline stage into a self-regulating, feedback-driven process at the
core of data-centric AI.

</details>


### [94] [A unified physics-informed generative operator framework for general inverse problems](https://arxiv.org/abs/2511.03241)
*Gang Bao,Yaohua Zang*

Main category: cs.LG

TL;DR: 提出IGNO框架解决PDE反问题，无需标记数据，在多场景表现佳。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法解决PDE反问题有局限，如需要大量标记数据、适用于特定测量类型。

Method: 引入IGNO框架，将高维系数场编码到低维潜空间，通过PDE残差训练，在潜空间基于梯度优化进行反演。

Result: IGNO在多种挑战性反问题中能准确、稳定、可扩展地反演，性能超现有方法，泛化能力强。

Conclusion: IGNO是解决计算科学领域挑战性反问题的统一且强大框架。

Abstract: Solving inverse problems governed by partial differential equations (PDEs) is
central to science and engineering, yet remains challenging when measurements
are sparse, noisy, or when the underlying coefficients are high-dimensional or
discontinuous. Existing deep learning approaches either require extensive
labeled datasets or are limited to specific measurement types, often leading to
failure in such regimes and restricting their practical applicability. Here, a
novel generative neural operator framework, IGNO, is introduced to overcome
these limitations. IGNO unifies the solution of inverse problems from both
point measurements and operator-valued data without labeled training pairs.
This framework encodes high-dimensional, potentially discontinuous coefficient
fields into a low-dimensional latent space, which drives neural operator
decoders to reconstruct both coefficients and PDE solutions. Training relies
purely on physics constraints through PDE residuals, while inversion proceeds
via efficient gradient-based optimization in latent space, accelerated by an a
priori normalizing flow model. Across a diverse set of challenging inverse
problems, including recovery of discontinuous coefficients from solution-based
measurements and the EIT problem with operator-based measurements, IGNO
consistently achieves accurate, stable, and scalable inversion even under
severe noise. It consistently outperforms the state-of-the-art method under
varying noise levels and demonstrates strong generalization to
out-of-distribution targets. These results establish IGNO as a unified and
powerful framework for tackling challenging inverse problems across
computational science domains.

</details>


### [95] [Climate Adaptation with Reinforcement Learning: Economic vs. Quality of Life Adaptation Pathways](https://arxiv.org/abs/2511.03243)
*Miguel Costa,Arthur Vandervoort,Martin Drews,Karyn Morrissey,Francisco C. Pereira*

Main category: cs.LG

TL;DR: 本文提出用强化学习应对气候变化下洪水适应政策制定问题，通过综合评估模型计算影响，结果显示不同优先假设会改变适应政策，框架已公开。


<details>
  <summary>Details</summary>
Motivation: 气候变化使洪水事件增多，需制定适应政策，但要处理长期气候影响的不确定性及规范选择问题。

Method: 提出使用强化学习识别不确定条件下的适应路径，用综合评估模型连接降雨和洪水模型并计算洪水影响。

Result: 优先考虑生活质量而非经济影响的模型会导致更多适应支出和更均匀的支出分布。

Conclusion: 不同规范假设会显著改变适应政策，所提框架可用于相关研究。

Abstract: Climate change will cause an increase in the frequency and severity of flood
events, prompting the need for cohesive adaptation policymaking. Designing
effective adaptation policies, however, depends on managing the uncertainty of
long-term climate impacts. Meanwhile, such policies can feature important
normative choices that are not always made explicit. We propose that
Reinforcement Learning (RL) can be a useful tool to both identify adaptation
pathways under uncertain conditions while it also allows for the explicit
modelling (and consequent comparison) of different adaptation priorities (e.g.
economic vs. wellbeing). We use an Integrated Assessment Model (IAM) to link
together a rainfall and flood model, and compute the impacts of flooding in
terms of quality of life (QoL), transportation, and infrastructure damage. Our
results show that models prioritising QoL over economic impacts results in more
adaptation spending as well as a more even distribution of spending over the
study area, highlighting the extent to which such normative assumptions can
alter adaptation policy. Our framework is publicly available:
https://github.com/MLSM-at-DTU/maat_qol_framework.

</details>


### [96] [GMoPE:A Prompt-Expert Mixture Framework for Graph Foundation Models](https://arxiv.org/abs/2511.03251)
*Zhibin Wang,Zhixing Zhang,Shuqi Wang,Xuanting Xie,Zhao Kang*

Main category: cs.LG

TL;DR: 提出GMoPE框架解决GNN泛化问题，实验显示其优于基线且开销小，为图基础模型提供框架。


<details>
  <summary>Details</summary>
Motivation: 现有GNN在跨领域和任务泛化能力有限，存在负迁移、可扩展性和高适应成本问题。

Method: 提出GMoPE框架，结合MoE架构与基于提示的学习，引入软正交约束，采用仅提示微调策略。

Result: GMoPE在多种预训练策略和下游任务中始终优于现有基线，性能与全参数微调相当，但适应开销小。

Conclusion: 工作为推进可泛化和高效的图基础模型提供了有原则且可扩展的框架。

Abstract: Graph Neural Networks (GNNs) have demonstrated impressive performance on
task-specific benchmarks, yet their ability to generalize across diverse
domains and tasks remains limited. Existing approaches often struggle with
negative transfer, scalability issues, and high adaptation costs. To address
these challenges, we propose GMoPE (Graph Mixture of Prompt-Experts), a novel
framework that seamlessly integrates the Mixture-of-Experts (MoE) architecture
with prompt-based learning for graphs. GMoPE leverages expert-specific prompt
vectors and structure-aware MoE routing to enable each expert to specialize in
distinct subdomains and dynamically contribute to predictions. To promote
diversity and prevent expert collapse, we introduce a soft orthogonality
constraint across prompt vectors, encouraging expert specialization and
facilitating a more balanced expert utilization. Additionally, we adopt a
prompt-only fine-tuning strategy that significantly reduces spatiotemporal
complexity during transfer. We validate GMoPE through extensive experiments
under various pretraining strategies and multiple downstream tasks. Results
show that GMoPE consistently outperforms state-of-the-art baselines and
achieves performance comparable to full parameter fine-tuning-while requiring
only a fraction of the adaptation overhead. Our work provides a principled and
scalable framework for advancing generalizable and efficient graph foundation
models.

</details>


### [97] [Diffusion Language Models are Super Data Learners](https://arxiv.org/abs/2511.03276)
*Jinjie Ni,Qian Liu,Longxu Dou,Chao Du,Zili Wang,Hang Yan,Tianyu Pang,Michael Qizhe Shieh*

Main category: cs.LG

TL;DR: 在严格控制的预训练设置下，扩散语言模型（DLMs）在特定条件下超越自回归（AR）模型，分析了优势原因并给出实验结果。


<details>
  <summary>Details</summary>
Motivation: 探究扩散语言模型在预训练中的表现及与自回归模型的对比情况。

Method: 在严格控制的预训练设置下进行实验，分析不同数据量、模型规模等因素对DLMs和AR模型表现的影响。

Result: 数据有限时，DLMs通过更多轮训练超越AR模型；特定DLM在代码和任务上表现优于AR模型；DLMs在少数据下也有较好表现；验证交叉熵上升不意味着下游性能下降。

Conclusion: DLMs在特定条件下有超越AR模型的优势，其优势源于多种因素。

Abstract: Under strictly controlled pre-training settings, we observe a Crossover: when
unique data is limited, diffusion language models (DLMs) consistently surpass
autoregressive (AR) models by training for more epochs. The crossover shifts
later with more or higher-quality data, earlier with larger models, and
persists across dense and sparse architectures. We attribute the gains to three
compounding factors: (1) any-order modeling, (2) super-dense compute from
iterative bidirectional denoising, and (3) built-in Monte Carlo augmentation;
input or parameter noise improves AR under data constraint but cannot close the
gap. At scale, a 1.7B DLM trained with a ~1.5T-token compute budget on 10B
unique Python tokens overtakes an AR coder trained with strictly matched
settings. In addition, a 1B-parameter DLM achieves > 56% accuracy on HellaSwag
and > 33% on MMLU using only 1B tokens, without any special tricks, just by
repeating standard pre-training data. We also show that rising validation
cross-entropy does not imply degraded downstream performance in this regime.

</details>


### [98] [Multi-Objective Adaptive Rate Limiting in Microservices Using Deep Reinforcement Learning](https://arxiv.org/abs/2511.03279)
*Ning Lyu,Yuxi Wang,Ziyu Cheng,Qingyuan Zhang,Feng Chen*

Main category: cs.LG

TL;DR: 本文提出基于深度强化学习的自适应速率限制策略，结合DQN和A3C算法，实验显示高负载下吞吐量提升、延迟降低，生产部署验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 传统速率限制算法难以适应动态流量模式和不同系统负载，需要新策略确保系统稳定和服务质量。

Method: 设计结合DQN和A3C算法的混合架构，将速率限制决策过程建模为马尔可夫决策过程，通过环境交互学习最优策略。

Result: 实验表明高负载场景下吞吐量提升23.7%，P99延迟降低31.4%；90天生产部署中服务降级事件减少82%，手动干预减少68%。

Conclusion: 所提出的自适应速率限制策略有效，能提升系统性能，减少服务降级和手动干预。

Abstract: As cloud computing and microservice architectures become increasingly
prevalent, API rate limiting has emerged as a critical mechanism for ensuring
system stability and service quality. Traditional rate limiting algorithms,
such as token bucket and sliding window, while widely adopted, struggle to
adapt to dynamic traffic patterns and varying system loads. This paper proposes
an adaptive rate limiting strategy based on deep reinforcement learning that
dynamically balances system throughput and service latency. We design a hybrid
architecture combining Deep Q-Network (DQN) and Asynchronous Advantage
Actor-Critic (A3C) algorithms, modeling the rate limiting decision process as a
Markov Decision Process. The system continuously monitors microservice states
and learns optimal rate limiting policies through environmental interaction.
Extensive experiments conducted in a Kubernetes cluster environment demonstrate
that our approach achieves 23.7% throughput improvement and 31.4% P99 latency
reduction compared to traditional fixed-threshold strategies under high-load
scenarios. Results from a 90-day production deployment handling 500 million
daily requests validate the practical effectiveness of the proposed method,
with 82% reduction in service degradation incidents and 68% decrease in manual
interventions.

</details>


### [99] [A Probabilistic Approach to Pose Synchronization for Multi-Reference Alignment with Applications to MIMO Wireless Communication Systems](https://arxiv.org/abs/2511.03280)
*Rob Romijnders,Gabriele Cesa,Christos Louizos,Kumar Pratik,Arash Behboodi*

Main category: cs.LG

TL;DR: 研究多参考对齐问题，提出新算法降低重建误差并节省计算成本。


<details>
  <summary>Details</summary>
Motivation: 多领域中，从多个未对齐观测中对齐和重建信号对系统性能至关重要，研究多参考对齐问题。

Method: 用概率方法建模MRA，以相对姿态为干扰变量边缘化，利用循环一致性实现去中心化。

Result: 两个提出的算法在实验设置中实现了更低的重建误差。

Conclusion: 新算法能去除问题的全局对称性，实现更直接的解决方案和更好的收敛，去中心化可节省计算成本。

Abstract: From molecular imaging to wireless communications, the ability to align and
reconstruct signals from multiple misaligned observations is crucial for system
performance. We study the problem of multi-reference alignment (MRA), which
arises in many real-world problems, such as cryo-EM, computer vision, and, in
particular, wireless communication systems. Using a probabilistic approach to
model MRA, we find a new algorithm that uses relative poses as nuisance
variables to marginalize out -- thereby removing the global symmetries of the
problem and allowing for more direct solutions and improved convergence. The
decentralization of this approach enables significant computational savings by
avoiding the cubic scaling of centralized methods through cycle consistency.
Both proposed algorithms achieve lower reconstruction error across experimental
settings.

</details>


### [100] [Graph Neural AI with Temporal Dynamics for Comprehensive Anomaly Detection in Microservices](https://arxiv.org/abs/2511.03285)
*Qingyuan Zhang,Ning Lyu,Le Liu,Yuxi Wang,Ziyu Cheng,Cancan Hua*

Main category: cs.LG

TL;DR: 本文提出结合图神经网络与时间建模的统一框架用于微服务架构异常检测与根因追踪，实验表明该框架性能优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 解决微服务架构中的异常检测和根因追踪问题。

Method: 将微服务调用链抽象为有向图，用图卷积聚合特征，引入门控循环单元建模时间演化，定义节点和路径级别的异常评分函数。

Result: 该框架在AUC、ACC、Recall和F1 - Score等关键指标上优于基线方法，在动态拓扑和复杂环境下保持高准确性和稳定性。

Conclusion: 为微服务异常检测提供新的技术路径，为分布式系统智能运维奠定方法基础。

Abstract: This study addresses the problem of anomaly detection and root cause tracing
in microservice architectures and proposes a unified framework that combines
graph neural networks with temporal modeling. The microservice call chain is
abstracted as a directed graph, where multidimensional features of nodes and
edges are used to construct a service topology representation, and graph
convolution is applied to aggregate features across nodes and model
dependencies, capturing complex structural relationships among services. On
this basis, gated recurrent units are introduced to model the temporal
evolution of call chains, and multi-layer stacking and concatenation operations
are used to jointly obtain structural and temporal representations, improving
the ability to identify anomaly patterns. Furthermore, anomaly scoring
functions at both the node and path levels are defined to achieve unified
modeling from local anomaly detection to global call chain tracing, which
enables the identification of abnormal service nodes and the reconstruction of
potential anomaly propagation paths. Sensitivity experiments are then designed
from multiple dimensions, including hyperparameters, environmental
disturbances, and data distribution, to evaluate the framework, and results
show that it outperforms baseline methods in key metrics such as AUC, ACC,
Recall, and F1-Score, maintaining high accuracy and stability under dynamic
topologies and complex environments. This research not only provides a new
technical path for anomaly detection in microservices but also lays a
methodological foundation for intelligent operations in distributed systems.

</details>


### [101] [Extending Fair Null-Space Projections for Continuous Attributes to Kernel Methods](https://arxiv.org/abs/2511.03304)
*Felix Störck,Fabian Hinder,Barbara Hammer*

Main category: cs.LG

TL;DR: 随着机器学习融入生活，公平性愈发重要，现有文献多关注离散属性，本文将迭代零空间投影推广到核方法，用于连续属性，在多数据集上表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有文献多关注离散属性，关于连续属性尤其是连续公平性的研究稀缺，需改进现有方法。

Method: 将迭代零空间投影推广到核方法，得到适用于连续受保护属性的核嵌入方法。

Result: 结合支持向量回归（SVR）在多个数据集上与其他当代方法相比具有有竞争力或更优的性能。

Conclusion: 提出的适用于连续受保护属性的核嵌入方法是有效的，可拓展公平性研究范围。

Abstract: With the on-going integration of machine learning systems into the everyday
social life of millions the notion of fairness becomes an ever increasing
priority in their development. Fairness notions commonly rely on protected
attributes to assess potential biases. Here, the majority of literature focuses
on discrete setups regarding both target and protected attributes. The
literature on continuous attributes especially in conjunction with regression
-- we refer to this as \emph{continuous fairness} -- is scarce. A common
strategy is iterative null-space projection which as of now has only been
explored for linear models or embeddings such as obtained by a non-linear
encoder. We improve on this by generalizing to kernel methods, significantly
extending the scope. This yields a model and fairness-score agnostic method for
kernel embeddings applicable to continuous protected attributes. We demonstrate
that our novel approach in conjunction with Support Vector Regression (SVR)
provides competitive or improved performance across multiple datasets in
comparisons to other contemporary methods.

</details>


### [102] [SORTeD Rashomon Sets of Sparse Decision Trees: Anytime Enumeration](https://arxiv.org/abs/2511.03344)
*Elif Arslan,Jacobus G. M. van der Linden,Serge Hoogendoorn,Marco Rinaldi,Emir Demirović*

Main category: cs.LG

TL;DR: 提出SORTD框架用于枚举Rashomon集的树，提升可扩展性，减少运行时间，让探索Rashomon集在实际应用更可行。


<details>
  <summary>Details</summary>
Motivation: 由于寻找最优树是NP难问题，枚举Rashomon集具有挑战性，需要提升其可扩展性。

Method: 引入SORTD框架，按目标值顺序枚举Rashomon集中的树。

Result: SORTD相比现有技术最多可将运行时间减少两个数量级，能计算任意可分离和全序目标的Rashomon集，支持用其他可分离（和偏序）目标对集合进行后评估。

Conclusion: 这些进展使探索Rashomon集在现实应用中更具实用性。

Abstract: Sparse decision tree learning provides accurate and interpretable predictive
models that are ideal for high-stakes applications by finding the single most
accurate tree within a (soft) size limit. Rather than relying on a single
"best" tree, Rashomon sets-trees with similar performance but varying
structures-can be used to enhance variable importance analysis, enrich
explanations, and enable users to choose simpler trees or those that satisfy
stakeholder preferences (e.g., fairness) without hard-coding such criteria into
the objective function. However, because finding the optimal tree is NP-hard,
enumerating the Rashomon set is inherently challenging. Therefore, we introduce
SORTD, a novel framework that improves scalability and enumerates trees in the
Rashomon set in order of the objective value, thus offering anytime behavior.
Our experiments show that SORTD reduces runtime by up to two orders of
magnitude compared with the state of the art. Moreover, SORTD can compute
Rashomon sets for any separable and totally ordered objective and supports
post-evaluating the set using other separable (and partially ordered)
objectives. Together, these advances make exploring Rashomon sets more
practical in real-world applications.

</details>


### [103] [A Modular, Data-Free Pipeline for Multi-Label Intention Recognition in Transportation Agentic AI Applications](https://arxiv.org/abs/2511.03363)
*Xiaocai Zhang,Hur Lim,Ke Wang,Zhe Xiao,Jing Wang,Kelvin Lee,Xiuju Fu,Zheng Qin*

Main category: cs.LG

TL;DR: 提出用于交通领域代理AI应用的无数据多标签意图识别模块化管道DMTC，在海运应用中表现出色，无需手动标注。


<details>
  <summary>Details</summary>
Motivation: 传统意图识别系统依赖大量标注语料，在细粒度多标签区分上有困难，需要消除昂贵数据收集成本并提高多标签意图理解准确性。

Method: DMTC管道分三步：用提示工程引导大语言模型生成合成查询；用Sentence - T5模型编码查询获得语义嵌入；用新型在线焦点对比损失训练轻量级分类器。

Result: DMTC汉明损失5.35%，AUC 95.92%，优于现有方法；Sentence - T5嵌入提高子集准确率至少3.29%，OFC损失额外提升0.98%。

Conclusion: 系统能将用户查询路由到特定任务模块，为无昂贵手动标注的意图感知代理奠定基础。

Abstract: In this study, a modular, data-free pipeline for multi-label intention
recognition is proposed for agentic AI applications in transportation. Unlike
traditional intent recognition systems that depend on large, annotated corpora
and often struggle with fine-grained, multi-label discrimination, our approach
eliminates the need for costly data collection while enhancing the accuracy of
multi-label intention understanding. Specifically, the overall pipeline, named
DMTC, consists of three steps: 1) using prompt engineering to guide large
language models (LLMs) to generate diverse synthetic queries in different
transport scenarios; 2) encoding each textual query with a Sentence-T5 model to
obtain compact semantic embeddings; 3) training a lightweight classifier using
a novel online focal-contrastive (OFC) loss that emphasizes hard samples and
maximizes inter-class separability. The applicability of the proposed pipeline
is demonstrated in an agentic AI application in the maritime transportation
context. Extensive experiments show that DMTC achieves a Hamming loss of 5.35%
and an AUC of 95.92%, outperforming state-of-the-art multi-label classifiers
and recent end-to-end SOTA LLM-based baselines. Further analysis reveals that
Sentence-T5 embeddings improve subset accuracy by at least 3.29% over
alternative encoders, and integrating the OFC loss yields an additional 0.98%
gain compared to standard contrastive objectives. In conclusion, our system
seamlessly routes user queries to task-specific modules (e.g., ETA information,
traffic risk evaluation, and other typical scenarios in the transportation
domain), laying the groundwork for fully autonomous, intention-aware agents
without costly manual labelling.

</details>


### [104] [TripleWin: Fixed-Point Equilibrium Pricing for Data-Model Coupled Markets](https://arxiv.org/abs/2511.03368)
*Hongrun Ren,Yun Xiong,Lei You,Yingying Wang,Haixu Xiong,Yangyong Zhu*

Main category: cs.LG

TL;DR: 提出统一的数据 - 模型耦合市场，处理数据集和模型交易，证明联合算子特性，实验显示有高效收敛性和公平性提升。


<details>
  <summary>Details</summary>
Motivation: 现有定价方法分离数据和模型交易或偏向一方，且缺乏同时对称机制。

Method: 提出统一数据 - 模型耦合市场，通过供需两侧映射形成闭环，证明联合算子是标准干扰函数。

Result: 实验表明与以经纪人为中心和单边基线相比，有高效收敛性和公平性提升。

Conclusion: 统一的数据 - 模型耦合市场在定价机制上有优势，代码开源。

Abstract: The rise of the machine learning (ML) model economy has intertwined markets
for training datasets and pre-trained models. However, most pricing approaches
still separate data and model transactions or rely on broker-centric pipelines
that favor one side. Recent studies of data markets with externalities capture
buyer interactions but do not yield a simultaneous and symmetric mechanism
across data sellers, model producers, and model buyers. We propose a unified
data-model coupled market that treats dataset and model trading as a single
system. A supply-side mapping transforms dataset payments into buyer-visible
model quotations, while a demand-side mapping propagates buyer prices back to
datasets through Shapley-based allocation. Together, they form a closed loop
that links four interactions: supply-demand propagation in both directions and
mutual coupling among buyers and among sellers. We prove that the joint
operator is a standard interference function (SIF), guaranteeing existence,
uniqueness, and global convergence of equilibrium prices. Experiments
demonstrate efficient convergence and improved fairness compared with
broker-centric and one-sided baselines. The code is available on
https://github.com/HongrunRen1109/Triple-Win-Pricing.

</details>


### [105] [Adaptable Hindsight Experience Replay for Search-Based Learning](https://arxiv.org/abs/2511.03405)
*Alexandros Vazaios,Jannis Brugger,Cedric Derstroff,Kristian Kersting,Mira Mezini*

Main category: cs.LG

TL;DR: 提出Adaptable HER框架集成HER与AlphaZero，实验表明可调整HER有益且性能超纯监督或强化学习。


<details>
  <summary>Details</summary>
Motivation: 原AlphaZero网络训练方法在稀疏奖励场景早期受限，需解决网络无法提供有效指导问题。

Method: 引入Adaptable HER框架，将HER与AlphaZero集成，可灵活调整HER属性。

Result: 在方程发现等实验中，可调整HER框架性能优于纯监督或强化学习。

Conclusion: 对HER进行修改是有益的，该集成框架有良好效果。

Abstract: AlphaZero-like Monte Carlo Tree Search systems, originally introduced for
two-player games, dynamically balance exploration and exploitation using neural
network guidance. This combination makes them also suitable for classical
search problems. However, the original method of training the network with
simulation results is limited in sparse reward settings, especially in the
early stages, where the network cannot yet give guidance. Hindsight Experience
Replay (HER) addresses this issue by relabeling unsuccessful trajectories from
the search tree as supervised learning signals. We introduce Adaptable HER
(\ours{}), a flexible framework that integrates HER with AlphaZero, allowing
easy adjustments to HER properties such as relabeled goals, policy targets, and
trajectory selection. Our experiments, including equation discovery, show that
the possibility of modifying HER is beneficial and surpasses the performance of
pure supervised or reinforcement learning.

</details>


### [106] [POEMS: Product of Experts for Interpretable Multi-omic Integration using Sparse Decoding](https://arxiv.org/abs/2511.03464)
*Mihriban Kocak Balik,Pekka Marttinen,Negar Safinianaini*

Main category: cs.LG

TL;DR: 提出POEMS框架解决多组学数据集成中预测性能与可解释性的权衡问题，在癌症亚型分析中证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有深度生成模型在多组学数据集成中难以兼顾预测性能和可解释性。

Method: 引入POEMS无监督概率框架，通过稀疏连接映射特征、专家乘积模型实现跨组学关联、门控网络报告各组学贡献，还提出高效稀疏解码器。

Result: 在癌症亚型分析案例中，POEMS取得有竞争力的聚类和分类性能。

Conclusion: 基于生物标志物的洞察和预测准确性可在多组学表示学习中并存。

Abstract: Integrating different molecular layers, i.e., multiomics data, is crucial for
unraveling the complexity of diseases; yet, most deep generative models either
prioritize predictive performance at the expense of interpretability or enforce
interpretability by linearizing the decoder, thereby weakening the network's
nonlinear expressiveness. To overcome this tradeoff, we introduce POEMS:
Product Of Experts for Interpretable Multiomics Integration using Sparse
Decoding, an unsupervised probabilistic framework that preserves predictive
performance while providing interpretability. POEMS provides interpretability
without linearizing any part of the network by 1) mapping features to latent
factors using sparse connections, which directly translates to biomarker
discovery, 2) allowing for cross-omic associations through a shared latent
space using product of experts model, and 3) reporting contributions of each
omic by a gating network that adaptively computes their influence in the
representation learning. Additionally, we present an efficient sparse decoder.
In a cancer subtyping case study, POEMS achieves competitive clustering and
classification performance while offering our novel set of interpretations,
demonstrating that biomarker based insight and predictive accuracy can coexist
in multiomics representation learning.

</details>


### [107] [Reinforcement Learning Using known Invariances](https://arxiv.org/abs/2511.03473)
*Alexandru Cioba,Aya Kayal,Laura Toni,Sattar Vakili,Alberto Bernacchia*

Main category: cs.LG

TL;DR: 本文为基于核的强化学习提出结合已知群对称性的理论和算法框架，提出对称感知的乐观最小二乘值迭代变体，分析有新边界并通过实验验证，凸显结构先验在设计高效算法中的价值。


<details>
  <summary>Details</summary>
Motivation: 现实中强化学习问题环境存在对称性，可利用其提高学习效率。

Method: 提出对称感知的乐观最小二乘值迭代（LSVI）变体，利用不变核编码奖励和转移动态的不变性。

Result: 分析得到不变再生核希尔伯特空间（RKHS）的最大信息增益和覆盖数的新边界，在定制的Frozen Lake环境和二维布局设计问题上的实验结果表明对称感知的强化学习比标准核方法性能更好。

Conclusion: 结构先验在设计更高效的强化学习算法中有价值。

Abstract: In many real-world reinforcement learning (RL) problems, the environment
exhibits inherent symmetries that can be exploited to improve learning
efficiency. This paper develops a theoretical and algorithmic framework for
incorporating known group symmetries into kernel-based RL. We propose a
symmetry-aware variant of optimistic least-squares value iteration (LSVI),
which leverages invariant kernels to encode invariance in both rewards and
transition dynamics. Our analysis establishes new bounds on the maximum
information gain and covering numbers for invariant RKHSs, explicitly
quantifying the sample efficiency gains from symmetry. Empirical results on a
customized Frozen Lake environment and a 2D placement design problem confirm
the theoretical improvements, demonstrating that symmetry-aware RL achieves
significantly better performance than their standard kernel counterparts. These
findings highlight the value of structural priors in designing more
sample-efficient reinforcement learning algorithms.

</details>


### [108] [RAGBoost: Efficient Retrieval-Augmented Generation with Accuracy-Preserving Context Reuse](https://arxiv.org/abs/2511.03475)
*Yinsicheng Jiang,Yeqi Huang,Liang Cheng,Cheng Deng,Xuan Sun,Luo Mai*

Main category: cs.LG

TL;DR: 提出RAGBoost系统，通过保准上下文复用实现高缓存复用且不牺牲准确性，提升预填充性能。


<details>
  <summary>Details</summary>
Motivation: 现有检索增强生成（RAG）系统预填充性能下降，现有缓存技术存在低复用或牺牲推理质量的问题。

Method: RAGBoost检测并发会话和多轮交互中重叠的检索项，采用高效上下文索引、排序和去重最大化复用，用轻量级上下文提示维持推理准确性。

Result: 与现有大语言模型推理引擎无缝集成，相比现有技术将预填充性能提升1.5 - 3倍，在多样工作负载中保持或提升推理准确性。

Conclusion: RAGBoost能高效提升RAG系统性能，代码已开源。

Abstract: Retrieval-augmented generation (RAG) enhances large language models (LLMs)
with retrieved context but often suffers from downgraded prefill performance as
modern applications demand longer and more complex inputs. Existing caching
techniques either preserve accuracy with low cache reuse or improve reuse at
the cost of degraded reasoning quality. We present RAGBoost, an efficient RAG
system that achieves high cache reuse without sacrificing accuracy through
accuracy-preserving context reuse. RAGBoost detects overlapping retrieved items
across concurrent sessions and multi-turn interactions, using efficient context
indexing, ordering, and de-duplication to maximize reuse, while lightweight
contextual hints maintain reasoning fidelity. It integrates seamlessly with
existing LLM inference engines and improves their prefill performance by 1.5-3X
over state-of-the-art methods, while preserving or even enhancing reasoning
accuracy across diverse RAG and agentic AI workloads. Our code is released at:
https://github.com/Edinburgh-AgenticAI/RAGBoost.

</details>


### [109] [NAP: Attention-Based Late Fusion for Automatic Sleep Staging](https://arxiv.org/abs/2511.03488)
*Alvise Dei Rossi,Julia van der Meer,Markus H. Schmidt,Claudio L. A. Bassetti,Luigi Fiorillo,Francesca Faraci*

Main category: cs.LG

TL;DR: 提出注意力模型NAP处理多导睡眠图信号，聚合预训练单通道模型输出，在多数据集零样本泛化中表现出色，可拓展到其他多模态生理应用。


<details>
  <summary>Details</summary>
Motivation: 多导睡眠图信号异质性高，现有模型未充分利用其多模态特性。

Method: 引入基于注意力的模型NAP，使用三轴注意力机制结合多预测流，训练适应不同输入维度，聚合预训练单通道模型输出。

Result: NAP始终优于单个预测器和简单集成，在多数据集上实现了零样本泛化的最优效果。

Conclusion: 该方法可用于自动睡眠分期，也可拓展到其他多模态生理应用。

Abstract: Polysomnography signals are highly heterogeneous, varying in modality
composition (e.g., EEG, EOG, ECG), channel availability (e.g., frontal,
occipital EEG), and acquisition protocols across datasets and clinical sites.
Most existing models that process polysomnography data rely on a fixed subset
of modalities or channels and therefore neglect to fully exploit its inherently
multimodal nature. We address this limitation by introducing NAP (Neural
Aggregator of Predictions), an attention-based model which learns to combine
multiple prediction streams using a tri-axial attention mechanism that captures
temporal, spatial, and predictor-level dependencies. NAP is trained to adapt to
different input dimensions. By aggregating outputs from frozen, pretrained
single-channel models, NAP consistently outperforms individual predictors and
simple ensembles, achieving state-of-the-art zero-shot generalization across
multiple datasets. While demonstrated in the context of automated sleep staging
from polysomnography, the proposed approach could be extended to other
multimodal physiological applications.

</details>


### [110] [Learning Without Critics? Revisiting GRPO in Classical Reinforcement Learning Environments](https://arxiv.org/abs/2511.03527)
*Bryan L. M. de Oliveira,Felipe V. Frujeri,Marcos P. C. M. Queiroz,Luana G. B. Martins,Telma W. de L. Soares,Luckeciano C. Melo*

Main category: cs.LG

TL;DR: 对GRPO在经典单任务强化学习环境中进行系统研究，揭示无批评家方法的局限性和适用条件。


<details>
  <summary>Details</summary>
Motivation: GRPO消除学习的批评家，引发对策略梯度方法中学习基线必要性的疑问，需系统研究GRPO。

Method: 在经典单任务强化学习环境中进行研究，通过控制消融实验隔离基线、折扣和组采样。

Result: （1）学习的批评家对长视野任务至关重要；（2）GRPO在多数情况受益于高折扣因子；（3）较小组规模表现更优。

Conclusion: 揭示无批评家方法在经典控制中的局限性和特定可行条件。

Abstract: Group Relative Policy Optimization (GRPO) has emerged as a scalable
alternative to Proximal Policy Optimization (PPO) by eliminating the learned
critic and instead estimating advantages through group-relative comparisons of
trajectories. This simplification raises fundamental questions about the
necessity of learned baselines in policy-gradient methods. We present the first
systematic study of GRPO in classical single-task reinforcement learning
environments, spanning discrete and continuous control tasks. Through
controlled ablations isolating baselines, discounting, and group sampling, we
reveal three key findings: (1) learned critics remain essential for
long-horizon tasks: all critic-free baselines underperform PPO except in
short-horizon environments like CartPole where episodic returns can be
effective; (2) GRPO benefits from high discount factors (gamma = 0.99) except
in HalfCheetah, where lack of early termination favors moderate discounting
(gamma = 0.9); (3) smaller group sizes outperform larger ones, suggesting
limitations in batch-based grouping strategies that mix unrelated episodes.
These results reveal both the limitations of critic-free methods in classical
control and the specific conditions where they remain viable alternatives to
learned value functions.

</details>


### [111] [Byzantine-Robust Federated Learning with Learnable Aggregation Weights](https://arxiv.org/abs/2511.03529)
*Javad Parsa,Amir Hossein Daghestani,André M. H. Teixeira,Mikael Johansson*

Main category: cs.LG

TL;DR: 提出结合自适应加权的拜占庭鲁棒联邦学习优化问题，开发交替最小化算法，实验显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 恶意客户端对联邦学习鲁棒性构成挑战，尤其是数据分布异构时，需提升鲁棒性。

Method: 提出含自适应加权的优化问题，将聚合权重作为可学习参数与全局模型参数联合优化，开发交替最小化算法。

Result: 实验表明该方法在不同数据集和攻击场景中，尤其在数据高度异构和恶意客户端比例大的情况下，始终优于现有方法。

Conclusion: 提出的方法有效提升了联邦学习在拜占庭攻击下的鲁棒性。

Abstract: Federated Learning (FL) enables clients to collaboratively train a global
model without sharing their private data. However, the presence of malicious
(Byzantine) clients poses significant challenges to the robustness of FL,
particularly when data distributions across clients are heterogeneous. In this
paper, we propose a novel Byzantine-robust FL optimization problem that
incorporates adaptive weighting into the aggregation process. Unlike
conventional approaches, our formulation treats aggregation weights as
learnable parameters, jointly optimizing them alongside the global model
parameters. To solve this optimization problem, we develop an alternating
minimization algorithm with strong convergence guarantees under adversarial
attack. We analyze the Byzantine resilience of the proposed objective. We
evaluate the performance of our algorithm against state-of-the-art
Byzantine-robust FL approaches across various datasets and attack scenarios.
Experimental results demonstrate that our method consistently outperforms
existing approaches, particularly in settings with highly heterogeneous data
and a large proportion of malicious clients.

</details>


### [112] [Efficient Neural Networks with Discrete Cosine Transform Activations](https://arxiv.org/abs/2511.03531)
*Marc Martinez-Gost,Sara Pepe,Ana Pérez-Neira,Miguel Ángel Lagunas*

Main category: cs.LG

TL;DR: 本文扩展ENN研究，强调其效率、可解释性和剪枝能力，提出高效剪枝策略，实验证明ENN准确率高且参数少，可安全剪枝，实现多方面平衡。


<details>
  <summary>Details</summary>
Motivation: 在先前证明ENN强大表达能力基础上，进一步强调其效率、可解释性和剪枝能力。

Method: 利用DCT参数化的结构化和去相关表示，提出去除不必要DCT系数的高效剪枝策略。

Result: 在分类和隐式神经表示任务中，ENN达到了最先进的准确率，同时参数数量少，最多可安全剪枝40%的激活系数。

Conclusion: ENN框架将信号处理概念融入神经网络设计，在表达能力、紧凑性和可解释性之间取得了平衡。

Abstract: In this paper, we extend our previous work on the Expressive Neural Network
(ENN), a multilayer perceptron with adaptive activation functions parametrized
using the Discrete Cosine Transform (DCT). Building upon previous work that
demonstrated the strong expressiveness of ENNs with compact architectures, we
now emphasize their efficiency, interpretability and pruning capabilities. The
DCT-based parameterization provides a structured and decorrelated
representation that reveals the functional role of each neuron and allows
direct identification of redundant components. Leveraging this property, we
propose an efficient pruning strategy that removes unnecessary DCT coefficients
with negligible or no loss in performance. Experimental results across
classification and implicit neural representation tasks confirm that ENNs
achieve state-of-the-art accuracy while maintaining a low number of parameters.
Furthermore, up to 40% of the activation coefficients can be safely pruned,
thanks to the orthogonality and bounded nature of the DCT basis. Overall, these
findings demonstrate that the ENN framework offers a principled integration of
signal processing concepts into neural network design, achieving a balanced
trade-off between expressiveness, compactness, and interpretability.

</details>


### [113] [Flat Minima and Generalization: Insights from Stochastic Convex Optimization](https://arxiv.org/abs/2511.03548)
*Matan Schliserman,Shira Vansover-Hager,Tomer Koren*

Main category: cs.LG

TL;DR: 研究随机凸优化中平坦极小值与泛化的联系，发现平坦经验极小值可能泛化差，分析SA - GD和SAM算法泛化问题并给出风险上界。


<details>
  <summary>Details</summary>
Motivation: 理解学习算法泛化行为是学习理论核心目标，新兴观点认为算法成功因收敛到平坦极小值，本文研究其与泛化的联系。

Method: 对随机凸优化场景进行理论分析，证明SA - GD和SAM算法的相关性质，用算法稳定性技术建立风险上界。

Result: 平坦经验极小值可能有平凡的Ω(1)总体风险，SA - GD收敛到平坦极小值但总体风险大，SAM可能收敛到尖锐极小值且总体风险为Ω(1)。

Conclusion: 在随机凸优化中，平坦极小值不一定带来好的泛化性能，用算法稳定性技术可建立SA - GD和SAM的总体风险上界。

Abstract: Understanding the generalization behavior of learning algorithms is a central
goal of learning theory. A recently emerging explanation is that learning
algorithms are successful in practice because they converge to flat minima,
which have been consistently associated with improved generalization
performance. In this work, we study the link between flat minima and
generalization in the canonical setting of stochastic convex optimization with
a non-negative, $\beta$-smooth objective. Our first finding is that, even in
this fundamental and well-studied setting, flat empirical minima may incur
trivial $\Omega(1)$ population risk while sharp minima generalizes optimally.
Then, we show that this poor generalization behavior extends to two natural
''sharpness-aware'' algorithms originally proposed by Foret et al. (2021),
designed to bias optimization toward flat solutions: Sharpness-Aware Gradient
Descent (SA-GD) and Sharpness-Aware Minimization (SAM). For SA-GD, which
performs gradient steps on the maximal loss in a predefined neighborhood, we
prove that while it successfully converges to a flat minimum at a fast rate,
the population risk of the solution can still be as large as $\Omega(1)$,
indicating that even flat minima found algorithmically using a sharpness-aware
gradient method might generalize poorly. For SAM, a computationally efficient
approximation of SA-GD based on normalized ascent steps, we show that although
it minimizes the empirical loss, it may converge to a sharp minimum and also
incur population risk $\Omega(1)$. Finally, we establish population risk upper
bounds for both SA-GD and SAM using algorithmic stability techniques.

</details>


### [114] [Imitation Learning in the Deep Learning Era: A Novel Taxonomy and Recent Advances](https://arxiv.org/abs/2511.03565)
*Iason Chrysomallis,Georgios Chalkiadakis*

Main category: cs.LG

TL;DR: 本文综述模仿学习研究进展，提出新分类法，分析代表工作优缺点，指出未来研究挑战和方向。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习发展，模仿学习有诸多进展且新方法涌现，需对其进行综述。

Method: 对模仿学习研究进行综述，提出新的分类法，批判性分析代表工作。

Result: 梳理最新进展、趋势、方法创新和实际应用，提出新分类法。

Conclusion: 指出代表工作的优缺点，明确未来研究的关键挑战和开放方向。

Abstract: Imitation learning (IL) enables agents to acquire skills by observing and
replicating the behavior of one or multiple experts. In recent years, advances
in deep learning have significantly expanded the capabilities and scalability
of imitation learning across a range of domains, where expert data can range
from full state-action trajectories to partial observations or unlabeled
sequences. Alongside this growth, novel approaches have emerged, with new
methodologies being developed to address longstanding challenges such as
generalization, covariate shift, and demonstration quality. In this survey, we
review the latest advances in imitation learning research, highlighting recent
trends, methodological innovations, and practical applications. We propose a
novel taxonomy that is distinct from existing categorizations to better reflect
the current state of the IL research stratum and its trends. Throughout the
survey, we critically examine the strengths, limitations, and evaluation
practices of representative works, and we outline key challenges and open
directions for future research.

</details>


### [115] [TabGemma: Text-Based Tabular ICL via LLM using Continued Pretraining and Retrieval](https://arxiv.org/abs/2511.03570)
*Günther Schindler,Maximilian Schambach,Michael Medek,Sam Thelin*

Main category: cs.LG

TL;DR: 研究用于表格预测的大语言模型，提出TabGemma解决实际问题，在分类任务表现佳，回归任务有局限，表明LLMs结合特定处理可用于表格预测。


<details>
  <summary>Details</summary>
Motivation: 解决预训练大语言模型用于表格预测时不稳定的数字标记化和有限上下文大小的问题。

Method: 通过有符号科学记数法规范数字，使用大规模真实世界数据集以目标插补为目标继续预训练12B Gemma 3模型，推理时用紧凑n - 元语法检索选择信息示例。

Result: TabGemma在分类任务中达到新的最优水平，回归任务小样本有竞争力，大样本落后于传统方法。

Conclusion: 大语言模型结合专用数字处理和上下文检索可用于高度语义化表格预测任务，需在数字建模和长上下文扩展方面进一步改进。

Abstract: We study LLMs for tabular prediction with mixed text, numeric, and
categorical fields. We introduce TabGemma, a schema-agnostic in-context learner
that treats rows as sequences and tackles two practical hurdles when adapting
pretrained LLMs for tabular predictions: unstable numeric tokenization and
limited context size. We propose to canonicalize numbers via signed scientific
notation and continue pretraining of a 12B Gemma 3 model with a target
imputation objective using a large-scale real world dataset. For inference, we
use a compact n-gram-based retrieval to select informative exemplars that fit
within a 128k-token window.
  On semantically rich benchmarks, TabGemma establishes a new state of the art
on classification across low- and high-data regimes and improves monotonically
with more context rows. For regression, it is competitive at small sample sizes
but trails conventional approaches as data grows. Our results show that LLMs
can be effective tabular in-context learners on highly semantic tasks when
paired with dedicated numeric handling and context retrieval, while motivating
further advances in numeric modeling and long-context scaling.

</details>


### [116] [Learning Under Laws: A Constraint-Projected Neural PDE Solver that Eliminates Hallucinations](https://arxiv.org/abs/2511.03578)
*Mainak Singha*

Main category: cs.LG

TL;DR: 提出约束投影学习（CPL）框架，解决神经网络求解偏微分方程时违反物理定律问题，能生成稳定、符合物理规律的解。


<details>
  <summary>Details</summary>
Motivation: 神经网络求解偏微分方程时常常违反其应模拟的物理定律，如凭空产生质量、激波漂移等。

Method: 提出CPL框架，将网络输出投影到由守恒、Rankine - Hugoniot平衡、熵和正性定义的约束集交集上；用总变差阻尼（TVD）稳定训练，采用滚动课程确保长预测期的一致性。

Result: 消除了硬和软违反情况，守恒达到机器精度，总变差增长消失，熵和误差有界，在Burgers和Euler系统上生成稳定、符合物理规律且不失精度的解。

Conclusion: CPL使尊重物理规律成为学习过程的内在属性，而非依赖神经网络求解器自发遵守。

Abstract: Neural networks can approximate solutions to partial differential equations,
but they often break the very laws they are meant to model-creating mass from
nowhere, drifting shocks, or violating conservation and entropy. We address
this by training within the laws of physics rather than beside them. Our
framework, called Constraint-Projected Learning (CPL), keeps every update
physically admissible by projecting network outputs onto the intersection of
constraint sets defined by conservation, Rankine-Hugoniot balance, entropy, and
positivity. The projection is differentiable and adds only about 10%
computational overhead, making it fully compatible with back-propagation. We
further stabilize training with total-variation damping (TVD) to suppress small
oscillations and a rollout curriculum that enforces consistency over long
prediction horizons. Together, these mechanisms eliminate both hard and soft
violations: conservation holds at machine precision, total-variation growth
vanishes, and entropy and error remain bounded. On Burgers and Euler systems,
CPL produces stable, physically lawful solutions without loss of accuracy.
Instead of hoping neural solvers will respect physics, CPL makes that behavior
an intrinsic property of the learning process.

</details>


### [117] [Tensor-Efficient High-Dimensional Q-learning](https://arxiv.org/abs/2511.03595)
*Junyi Wu,Dan Li*

Main category: cs.LG

TL;DR: 本文提出Tensor - Efficient Q - Learning (TEQL)算法，结合改进的低秩张量分解、新探索和正则化机制，在经典控制任务上表现优于传统方法，适用于资源受限应用。


<details>
  <summary>Details</summary>
Motivation: 高维强化学习在大状态 - 动作空间中计算复杂、样本效率低，Q学习算法受维度诅咒影响，现有方法有改进空间。

Method: 提出TEQL算法，在离散化状态 - 动作空间上通过改进的块坐标下降增强低秩张量分解，结合近似误差和基于访问计数的上置信界进行探索，在目标函数中加入基于频率的惩罚项。

Result: 在经典控制任务的实验中，TEQL在样本效率和总奖励方面优于传统矩阵方法和深度强化学习方法。

Conclusion: TEQL适用于资源受限、采样成本高的应用，如航天和医疗领域。

Abstract: High-dimensional reinforcement learning faces challenges with complex
calculations and low sample efficiency in large state-action spaces. Q-learning
algorithms struggle particularly with the curse of dimensionality, where the
number of state-action pairs grows exponentially with problem size. While
neural network-based approaches like Deep Q-Networks have shown success, recent
tensor-based methods using low-rank decomposition offer more
parameter-efficient alternatives. Building upon existing tensor-based methods,
we propose Tensor-Efficient Q-Learning (TEQL), which enhances low-rank tensor
decomposition via improved block coordinate descent on discretized state-action
spaces, incorporating novel exploration and regularization mechanisms. The key
innovation is an exploration strategy that combines approximation error with
visit count-based upper confidence bound to prioritize actions with high
uncertainty, avoiding wasteful random exploration. Additionally, we incorporate
a frequency-based penalty term in the objective function to encourage
exploration of less-visited state-action pairs and reduce overfitting to
frequently visited regions. Empirical results on classic control tasks
demonstrate that TEQL outperforms conventional matrix-based methods and deep RL
approaches in both sample efficiency and total rewards, making it suitable for
resource-constrained applications, such as space and healthcare where sampling
costs are high.

</details>


### [118] [Going Beyond Expert Performance via Deep Implicit Imitation Reinforcement Learning](https://arxiv.org/abs/2511.03616)
*Iason Chrysomallis,Georgios Chalkiadakis*

Main category: cs.LG

TL;DR: 本文提出深度隐式模仿强化学习框架，含DIIQN和HA - DIIQN算法，实验显示DIIQN回报更高，HA - DIIQN学习更快，框架具鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统模仿学习要求完整状态 - 动作示范，限制实际应用，现实场景常只有状态观测且专家表现欠佳。

Method: 结合深度强化学习与仅观测数据集的隐式模仿学习，提出DIIQN算法，采用动作推理和动态置信机制；扩展出HA - DIIQN算法，引入不可行性检测和桥接程序。

Result: DIIQN比标准DQN episodic回报高130%，优于现有隐式模仿方法；HA - DIIQN在异构动作设置中学习速度比基线快64%，利用传统方法无法使用的专家数据集。

Conclusion: 该框架能解决传统模仿学习的局限，具有良好性能和鲁棒性。

Abstract: Imitation learning traditionally requires complete state-action
demonstrations from optimal or near-optimal experts. These requirements
severely limit practical applicability, as many real-world scenarios provide
only state observations without corresponding actions and expert performance is
often suboptimal. In this paper we introduce a deep implicit imitation
reinforcement learning framework that addresses both limitations by combining
deep reinforcement learning with implicit imitation learning from
observation-only datasets. Our main algorithm, Deep Implicit Imitation
Q-Network (DIIQN), employs an action inference mechanism that reconstructs
expert actions through online exploration and integrates a dynamic confidence
mechanism that adaptively balances expert-guided and self-directed learning.
This enables the agent to leverage expert guidance for accelerated training
while maintaining capacity to surpass suboptimal expert performance. We further
extend our framework with a Heterogeneous Actions DIIQN (HA-DIIQN) algorithm to
tackle scenarios where expert and agent possess different action sets, a
challenge previously unaddressed in the implicit imitation learning literature.
HA-DIIQN introduces an infeasibility detection mechanism and a bridging
procedure identifying alternative pathways connecting agent capabilities to
expert guidance when direct action replication is impossible. Our experimental
results demonstrate that DIIQN achieves up to 130% higher episodic returns
compared to standard DQN, while consistently outperforming existing implicit
imitation methods that cannot exceed expert performance. In heterogeneous
action settings, HA-DIIQN learns up to 64% faster than baselines, leveraging
expert datasets unusable by conventional approaches. Extensive parameter
sensitivity analysis reveals the framework's robustness across varying dataset
sizes and hyperparameter configurations.

</details>


### [119] [Financial Management System for SMEs: Real-World Deployment of Accounts Receivable and Cash Flow Prediction](https://arxiv.org/abs/2511.03631)
*Bartłomiej Małkus,Szymon Bobek,Grzegorz J. Nalepa*

Main category: cs.LG

TL;DR: 针对中小企业财务挑战，开发集成财务预测系统，含应收账款预测和现金流预测，已实现原型并部署到Cluee平台。


<details>
  <summary>Details</summary>
Motivation: 中小企业因资源、客户群和数据有限面临独特财务管理挑战，现有企业级财务工具不能满足其需求。

Method: 开发集成系统，包含预测发票支付延迟的二元分类模型和处理不完整有限历史数据的多模块现金流预测模型。

Result: 实现原型系统并作为Web应用部署到Cluee平台。

Conclusion: 该系统对现实世界中小企业财务管理具有实际可行性。

Abstract: Small and Medium Enterprises (SMEs), particularly freelancers and early-stage
businesses, face unique financial management challenges due to limited
resources, small customer bases, and constrained data availability. This paper
presents the development and deployment of an integrated financial prediction
system that combines accounts receivable prediction and cash flow forecasting
specifically designed for SME operational constraints. Our system addresses the
gap between enterprise-focused financial tools and the practical needs of
freelancers and small businesses. The solution integrates two key components: a
binary classification model for predicting invoice payment delays, and a
multi-module cash flow forecasting model that handles incomplete and limited
historical data. A prototype system has been implemented and deployed as a web
application with integration into Cluee's platform, a startup providing
financial management tools for freelancers, demonstrating practical feasibility
for real-world SME financial management.

</details>


### [120] [nanoTabPFN: A Lightweight and Educational Reimplementation of TabPFN](https://arxiv.org/abs/2511.03634)
*Alexander Pfefferle,Johannes Hog,Lennart Purucker,Frank Hutter*

Main category: cs.LG

TL;DR: 介绍简化轻量的nanoTabPFN实现，让表格基础模型更易获取，小数据下性能佳且预训练快。


<details>
  <summary>Details</summary>
Motivation: 现有开源表格基础模型实现复杂、难理解、不适合新手和新实验，需更易获取的实现。

Method: 引入nanoTabPFN，简化TabPFN v2架构并使用预生成训练数据的训练循环。

Result: 在小数据设置下，一分钟预训练性能与传统机器学习基线相当，比TabPFN v2预训练快160,000倍。

Conclusion: nanoTabPFN让表格基础模型更易获取，消除大量计算资源需求，适用于教育目的，代码开源。

Abstract: Tabular foundation models such as TabPFN have revolutionized predictive
machine learning for tabular data. At the same time, the driving factors of
this revolution are hard to understand. Existing open-source tabular foundation
models are implemented in complicated pipelines boasting over 10,000 lines of
code, lack architecture documentation or code quality. In short, the
implementations are hard to understand, not beginner-friendly, and complicated
to adapt for new experiments. We introduce nanoTabPFN, a simplified and
lightweight implementation of the TabPFN v2 architecture and a corresponding
training loop that uses pre-generated training data. nanoTabPFN makes tabular
foundation models more accessible to students and researchers alike. For
example, restricted to a small data setting it achieves a performance
comparable to traditional machine learning baselines within one minute of
pre-training on a single GPU (160,000x faster than TabPFN v2 pretraining). This
eliminated requirement of large computational resources makes pre-training
tabular foundation models accessible for educational purposes. Our code is
available at https://github.com/automl/nanoTabPFN.

</details>


### [121] [SHIELD: Securing Healthcare IoT with Efficient Machine Learning Techniques for Anomaly Detection](https://arxiv.org/abs/2511.03661)
*Mahek Desai,Apoorva Rumale,Marjan Asadinia*

Main category: cs.LG

TL;DR: 本文提出机器学习驱动框架检测物联网医疗设备的网络攻击和故障异常，评估8种模型，得出各模型表现，提升了物联网医疗安全。


<details>
  <summary>Details</summary>
Motivation: 物联网设备集成到医疗领域带来安全和可靠性挑战，易受网络威胁和操作异常影响。

Method: 利用20万条记录的数据集，采用监督学习、半监督学习和无监督学习三种方法评估8种机器学习模型，通过多种指标进行综合评估。

Result: XGBoost异常检测准确率达99%且计算开销小；Isolation Forest平衡了精度和召回率；LSTM Autoencoders表现不佳。KNN攻击检测精度、召回率和F1得分近乎完美且计算成本低；VAE准确率达97%；GAN计算成本高且准确率和ROC - AUC低。

Conclusion: 该框架通过有效的异常检测策略增强了物联网医疗安全，可预防数据泄露、减少系统停机时间，保障医疗设备持续安全运行。

Abstract: The integration of IoT devices in healthcare introduces significant security
and reliability challenges, increasing susceptibility to cyber threats and
operational anomalies. This study proposes a machine learning-driven framework
for (1) detecting malicious cyberattacks and (2) identifying faulty device
anomalies, leveraging a dataset of 200,000 records. Eight machine learning
models are evaluated across three learning approaches: supervised learning
(XGBoost, K-Nearest Neighbors (K- NN)), semi-supervised learning (Generative
Adversarial Networks (GAN), Variational Autoencoders (VAE)), and unsupervised
learning (One-Class Support Vector Machine (SVM), Isolation Forest, Graph
Neural Networks (GNN), and Long Short-Term Memory (LSTM) Autoencoders). The
comprehensive evaluation was conducted across multiple metrics like F1-score,
precision, recall, accuracy, ROC-AUC, computational efficiency. XGBoost
achieved 99\% accuracy with minimal computational overhead (0.04s) for anomaly
detection, while Isolation Forest balanced precision and recall effectively.
LSTM Autoencoders underperformed with lower accuracy and higher latency. For
attack detection, KNN achieved near-perfect precision, recall, and F1-score
with the lowest computational cost (0.05s), followed by VAE at 97% accuracy.
GAN showed the highest computational cost with lowest accuracy and ROC-AUC.
These findings enhance IoT-enabled healthcare security through effective
anomaly detection strategies. By improving early detection of cyber threats and
device failures, this framework has the potential to prevent data breaches,
minimize system downtime, and ensure the continuous and safe operation of
medical devices, ultimately safeguarding patient health and trust in IoT-driven
healthcare solutions.

</details>


### [122] [DQN Performance with Epsilon Greedy Policies and Prioritized Experience Replay](https://arxiv.org/abs/2511.03670)
*Daniel Perkins,Oscar J. Escobar,Luke Green*

Main category: cs.LG

TL;DR: 研究有限环境下深度Q网络，分析epsilon - greedy探索策略和优先经验回放的影响，给出实证结果和实用建议。


<details>
  <summary>Details</summary>
Motivation: 了解epsilon - greedy探索策略和优先经验回放对深度Q网络在有限环境中学习的影响，为资源受限环境下的强化学习提供建议。

Method: 通过系统实验，评估epsilon衰减策略的变化对学习效率、收敛行为和奖励优化的影响，比较均匀、无回放和优先回放策略。

Result: 得到了不同探索策略和记忆管理方法在多个模拟中的实证结果。

Conclusion: 揭示了DQN训练中探索策略和记忆管理之间的权衡和相互作用，为资源受限环境下的强化学习提供实用建议。

Abstract: We present a detailed study of Deep Q-Networks in finite environments,
emphasizing the impact of epsilon-greedy exploration schedules and prioritized
experience replay. Through systematic experimentation, we evaluate how
variations in epsilon decay schedules affect learning efficiency, convergence
behavior, and reward optimization. We investigate how prioritized experience
replay leads to faster convergence and higher returns and show empirical
results comparing uniform, no replay, and prioritized strategies across
multiple simulations. Our findings illuminate the trade-offs and interactions
between exploration strategies and memory management in DQN training, offering
practical recommendations for robust reinforcement learning in
resource-constrained settings.

</details>


### [123] [Structured Matrix Scaling for Multi-Class Calibration](https://arxiv.org/abs/2511.03685)
*Eugène Berta,David Holzmüller,Michael I. Jordan,Francis Bach*

Main category: cs.LG

TL;DR: 本文从理论出发探讨参数化校准函数，指出多类校准挑战，通过实验提出应对方法，有开源实现。


<details>
  <summary>Details</summary>
Motivation: 从理论层面为基于逻辑回归的参数化校准函数提供依据，探索比标准温度缩放更具表现力的校准方法。

Method: 采用结构化正则化、鲁棒预处理和高效优化来处理多类校准中的偏差 - 方差权衡。

Result: 所提出的方法比现有的基于逻辑的校准技术有显著提升。

Conclusion: 提供的方法是常见温度、向量和矩阵缩放实现的有吸引力的替代方案。

Abstract: Post-hoc recalibration methods are widely used to ensure that classifiers
provide faithful probability estimates. We argue that parametric recalibration
functions based on logistic regression can be motivated from a simple
theoretical setting for both binary and multiclass classification. This insight
motivates the use of more expressive calibration methods beyond standard
temperature scaling. For multi-class calibration however, a key challenge lies
in the increasing number of parameters introduced by more complex models, often
coupled with limited calibration data, which can lead to overfitting. Through
extensive experiments, we demonstrate that the resulting bias-variance tradeoff
can be effectively managed by structured regularization, robust preprocessing
and efficient optimization. The resulting methods lead to substantial gains
over existing logistic-based calibration techniques. We provide efficient and
easy-to-use open-source implementations of our methods, making them an
attractive alternative to common temperature, vector, and matrix scaling
implementations.

</details>


### [124] [Behavior-Adaptive Q-Learning: A Unifying Framework for Offline-to-Online RL](https://arxiv.org/abs/2511.03695)
*Lipeng Zu,Hansong Zhou,Xiaonan Zhang*

Main category: cs.LG

TL;DR: 提出Behavior - Adaptive Q - Learning (BAQ)框架，实现从离线到在线强化学习的平稳可靠过渡，在基准测试中表现优于先前方法。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习在动态环境中部署时因分布偏移和不可靠的价值估计而面临困难，需要实现从离线到在线RL的平稳过渡。

Method: 引入BAQ框架，利用从离线数据导出的隐式行为模型，在在线微调时提供行为一致性信号，采用双目标损失函数。

Result: 在标准基准测试中，BAQ始终优于先前的离线到在线RL方法，实现更快恢复、更高鲁棒性和整体性能提升。

Conclusion: 隐式行为适应是可靠的现实世界策略部署的原则性和实用性解决方案。

Abstract: Offline reinforcement learning (RL) enables training from fixed data without
online interaction, but policies learned offline often struggle when deployed
in dynamic environments due to distributional shift and unreliable value
estimates on unseen state-action pairs. We introduce Behavior-Adaptive
Q-Learning (BAQ), a framework designed to enable a smooth and reliable
transition from offline to online RL. The key idea is to leverage an implicit
behavioral model derived from offline data to provide a behavior-consistency
signal during online fine-tuning. BAQ incorporates a dual-objective loss that
(i) aligns the online policy toward the offline behavior when uncertainty is
high, and (ii) gradually relaxes this constraint as more confident online
experience is accumulated. This adaptive mechanism reduces error propagation
from out-of-distribution estimates, stabilizes early online updates, and
accelerates adaptation to new scenarios. Across standard benchmarks, BAQ
consistently outperforms prior offline-to-online RL approaches, achieving
faster recovery, improved robustness, and higher overall performance. Our
results demonstrate that implicit behavior adaptation is a principled and
practical solution for reliable real-world policy deployment.

</details>


### [125] [AnaFlow: Agentic LLM-based Workflow for Reasoning-Driven Explainable and Sample-Efficient Analog Circuit Sizing](https://arxiv.org/abs/2511.03697)
*Mohsen Ahmadzadeh,Kaichang Chen,Georges Gielen*

Main category: cs.LG

TL;DR: 提出新型代理AI框架AnaFlow用于模拟电路尺寸设计，解决效率和可解释性问题，在不同复杂度电路上验证效果良好。


<details>
  <summary>Details</summary>
Motivation: 模拟/混合信号电路设计多为手工过程，设计周期长且易出错，现有AI技术需大量耗时仿真且设计方案缺乏可解释性。

Method: 采用多智能体工作流，基于大语言模型的智能体协作，结合自适应仿真策略。

Result: AnaFlow框架能在两个不同复杂度电路上自动完成尺寸设计任务，可从优化历史中学习，加速收敛。

Conclusion: 该框架具有高样本效率和可解释性，是模拟设计空间探索的有力工具，开创模拟EDA新范式。

Abstract: Analog/mixed-signal circuits are key for interfacing electronics with the
physical world. Their design, however, remains a largely handcrafted process,
resulting in long and error-prone design cycles. While the recent rise of
AI-based reinforcement learning and generative AI has created new techniques to
automate this task, the need for many time-consuming simulations is a critical
bottleneck hindering the overall efficiency. Furthermore, the lack of
explainability of the resulting design solutions hampers widespread adoption of
the tools. To address these issues, a novel agentic AI framework for
sample-efficient and explainable analog circuit sizing is presented. It employs
a multi-agent workflow where specialized Large Language Model (LLM)-based
agents collaborate to interpret the circuit topology, to understand the design
goals, and to iteratively refine the circuit's design parameters towards the
target goals with human-interpretable reasoning. The adaptive simulation
strategy creates an intelligent control that yields a high sample efficiency.
The AnaFlow framework is demonstrated for two circuits of varying complexity
and is able to complete the sizing task fully automatically, differently from
pure Bayesian optimization and reinforcement learning approaches. The system
learns from its optimization history to avoid past mistakes and to accelerate
convergence. The inherent explainability makes this a powerful tool for analog
design space exploration and a new paradigm in analog EDA, where AI agents
serve as transparent design assistants.

</details>


### [126] [Shrinking the Variance: Shrinkage Baselines for Reinforcement Learning with Verifiable Rewards](https://arxiv.org/abs/2511.03710)
*Guanning Zeng,Zhaoyi Zhou,Daman Arora,Andrea Zanette*

Main category: cs.LG

TL;DR: 本文提出基于收缩估计器的基线方法改进RLVR训练，理论证明可降低策略梯度估计器方差，实验显示优于标准经验均值基线。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法使用按提示的经验均值估计奖励均值，作者受Stein悖论启发，希望改进整体按提示的均值估计准确性，尤其在低生成场景。

Method: 提出使用结合按提示和跨提示均值的收缩估计器，构建基于收缩的基线。

Result: 收缩基线在实验中始终优于标准经验均值基线，带来更低方差的梯度更新和更好的训练稳定性。

Conclusion: 所提出的基于收缩的基线可作为现有按提示均值基线的替代，无需额外超参数和计算，能提升训练效果。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a
powerful paradigm for post-training large reasoning models (LRMs) using
policy-gradient methods such as GRPO. To stabilize training, these methods
typically center trajectory rewards by subtracting the empirical mean for each
prompt. Statistically, this centering acts as a control variate (or baseline),
reducing the variance of the policy-gradient estimator.
  Typically, the mean reward is estimated using per-prompt empirical averages
for each prompt in a batch. Drawing inspiration from Stein's paradox, we
propose using shrinkage estimators that combine per-prompt and across-prompt
means to improve the overall per-prompt mean estimation accuracy --
particularly in the low-generation regime typical of RLVR. Theoretically, we
construct a shrinkage-based baseline that provably yields lower-variance
policy-gradient estimators across algorithms. Our proposed baseline serves as a
drop-in replacement for existing per-prompt mean baselines, requiring no
additional hyper-parameters or computation. Empirically, shrinkage baselines
consistently outperform standard empirical-mean baselines, leading to
lower-variance gradient updates and improved training stability.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [127] [Mathematical exploration and discovery at scale](https://arxiv.org/abs/2511.02864)
*Bogdan Georgiev,Javier Gómez-Serrano,Terence Tao,Adam Zsolt Wagner*

Main category: cs.NE

TL;DR: 介绍AlphaEvolve结合大模型生成能力与自动化评估，可自主发现数学构造，解决多领域问题，结合其他方法有更好效果，是数学发现有力工具。


<details>
  <summary>Details</summary>
Motivation: 展示AlphaEvolve作为工具，自主发现新颖数学构造，推动对长期存在的开放问题的理解。

Method: 在迭代进化框架中，结合大语言模型生成能力与自动化评估，还结合Deep Think和AlphaProof。

Result: 在67个数学问题中多数情况下重新发现已知最佳解，部分找到改进解，还能将有限输入值结果推广到通用公式。

Conclusion: 大语言模型引导的进化搜索能自主发现补充人类直觉的数学构造，AlphaEvolve是强大的数学发现工具。

Abstract: AlphaEvolve is a generic evolutionary coding agent that combines the
generative capabilities of LLMs with automated evaluation in an iterative
evolutionary framework that proposes, tests, and refines algorithmic solutions
to challenging scientific and practical problems. In this paper we showcase
AlphaEvolve as a tool for autonomously discovering novel mathematical
constructions and advancing our understanding of long-standing open problems.
  To demonstrate its breadth, we considered a list of 67 problems spanning
mathematical analysis, combinatorics, geometry, and number theory. The system
rediscovered the best known solutions in most of the cases and discovered
improved solutions in several. In some instances, AlphaEvolve is also able to
generalize results for a finite number of input values into a formula valid for
all input values. Furthermore, we are able to combine this methodology with
Deep Think and AlphaProof in a broader framework where the additional
proof-assistants and reasoning systems provide automated proof generation and
further mathematical insights.
  These results demonstrate that large language model-guided evolutionary
search can autonomously discover mathematical constructions that complement
human intuition, at times matching or even improving the best known results,
highlighting the potential for significant new ways of interaction between
mathematicians and AI systems. We present AlphaEvolve as a powerful new tool
for mathematical discovery, capable of exploring vast search spaces to solve
complex optimization problems at scale, often with significantly reduced
requirements on preparation and computation time.

</details>


### [128] [Performance Evaluation of Bitstring Representations in a Linear Genetic Programming Framework](https://arxiv.org/abs/2511.02897)
*Clyde Meli,Vitezslav Nezval,Zuzana Kominkova Oplatkova,Victor Buttigieg,Anthony Spiteri Staines*

Main category: cs.NE

TL;DR: 本文比较C++三种位串实现（std::bitset、boost::dynamic_bitset和自定义直接实现）在线性遗传编程系统中拼接操作的性能，在三个平台测试，得出不同平台的最优实现。


<details>
  <summary>Details</summary>
Motivation: 不同位串表示对计算性能有不同影响，需比较不同位串实现的性能，为平台和应用选择最优方法。

Method: 对三种位串实现进行拼接操作的性能基准测试，在macOS、Linux和Windows MSYS2三个平台上进行测试。

Result: 自定义直接实现在Linux和Windows上性能最快，std::bitset在macOS上表现最佳，boost::dynamic_bitset虽慢但灵活。

Conclusion: 编译器优化和系统架构会影响性能，可根据平台和应用需求选择最优方法。

Abstract: Different bitstring representations can yield varying computational
performance. This work compares three bitstring implementations in C++:
std::bitset, boost::dynamic_bitset, and a custom direct implementation. Their
performance is benchmarked in the context of concatenation within a Linear
Genetic Programming system. Benchmarks were conducted on three platforms
(macOS, Linux, and Windows MSYS2) to assess platform specific performance
variations. The results show that the custom direct implementation delivers the
fastest performance on Linux and Windows, while std::bitset performs best on
macOS. Although consistently slower, boost::dynamic_bitset remains a viable and
flexible option. These findings highlight the influence of compiler
optimisations and system architecture on performance, providing practical
guidance for selecting the optimal method based on platform and application
requirements.

</details>


### [129] [A Novel Reservoir Computing Framework for Chaotic Time Series Prediction Using Time Delay Embedding and Random Fourier Features](https://arxiv.org/abs/2511.02877)
*S. K. Laha*

Main category: cs.NE

TL;DR: 提出结合时间延迟嵌入与随机傅里叶特征映射的新型储层计算框架RFF - RC用于混沌时间序列预测，评估显示其有良好效果。


<details>
  <summary>Details</summary>
Motivation: 需要能捕捉潜在吸引子内在几何结构且计算高效的混沌时间序列预测模型。

Method: 引入集成时间延迟嵌入与随机傅里叶特征映射的储层计算框架RFF - RC，近似非线性核变换。

Result: 在典型混沌系统评估中，RFF - RC实现了更高预测精度，有稳健的吸引子重建和长时预测效果。

Conclusion: 延迟嵌入和基于RFF的储层结合在丰富特征空间揭示新动态结构，为混沌动力学建模提供高效且可解释方法。

Abstract: Forecasting chaotic time series requires models that can capture the
intrinsic geometry of the underlying attractor while remaining computationally
efficient. We introduce a novel reservoir computing (RC) framework that
integrates time-delay embedding with Random Fourier Feature (RFF) mappings to
construct a dynamical reservoir without the need for traditional recurrent
architectures. Unlike standard RC, which relies on high-dimensional recurrent
connectivity, the proposed RFF-RC explicitly approximates nonlinear kernel
transformations that uncover latent dynamical relations in the reconstructed
phase space. This hybrid formulation offers two key advantages: (i) it provides
a principled way to approximate complex nonlinear interactions among delayed
coordinates, thereby enriching the effective dynamical representation of the
reservoir, and (ii) it reduces reliance on manual reservoir hyperparameters
such as spectral radius and leaking rate. We evaluate the framework on
canonical chaotic systems-the Mackey-Glass equation, the Lorenz system, and the
Kuramoto-Sivashinsky equation. This novel formulation demonstrates that RFF-RC
not only achieves superior prediction accuracy but also yields robust attractor
reconstructions and long-horizon forecasts. These results show that the
combination of delay embedding and RFF-based reservoirs reveals new dynamical
structure by embedding the system in an enriched feature space, providing a
computationally efficient and interpretable approach to modeling chaotic
dynamics.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [130] [PerfDojo: Automated ML Library Generation for Heterogeneous Architectures](https://arxiv.org/abs/2511.03586)
*Andrei Ivanov,Siyuan Shen,Gioele Gottardo,Marcin Chrapek,Afif Boudaoud,Timo Schneider,Luca Benini,Torsten Hoefler*

Main category: cs.PF

TL;DR: 提出PerfLLM自动优化方法，利用LLM和RL，能跨CPU和GPU架构提升性能。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型复杂度增加和硬件架构多样化使性能优化困难，手动优化耗资源，现有自动方法依赖复杂启发式和不可解释表示，阻碍性能可移植性。

Method: 引入PerfLLM，以PerfDojo为核心，将优化作为RL游戏，使用人类可读、数学启发的代码表示。

Result: PerfLLM能在不同CPU（x86、Arm、RISC - V）和GPU架构上实现显著性能提升。

Conclusion: PerfLLM是一种有效的自动优化方法，无需先验硬件知识，有利于人类分析和RL代理训练。

Abstract: The increasing complexity of machine learning models and the proliferation of
diverse hardware architectures (CPUs, GPUs, accelerators) make achieving
optimal performance a significant challenge. Heterogeneity in instruction sets,
specialized kernel requirements for different data types and model features
(e.g., sparsity, quantization), and architecture-specific optimizations
complicate performance tuning. Manual optimization is resource-intensive, while
existing automatic approaches often rely on complex hardware-specific
heuristics and uninterpretable intermediate representations, hindering
performance portability. We introduce PerfLLM, a novel automatic optimization
methodology leveraging Large Language Models (LLMs) and Reinforcement Learning
(RL). Central to this is PerfDojo, an environment framing optimization as an RL
game using a human-readable, mathematically-inspired code representation that
guarantees semantic validity through transformations. This allows effective
optimization without prior hardware knowledge, facilitating both human analysis
and RL agent training. We demonstrate PerfLLM's ability to achieve significant
performance gains across diverse CPU (x86, Arm, RISC-V) and GPU architectures.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [131] [SELF-REDRAFT: Eliciting Intrinsic Exploration-Exploitation Balance in Test-Time Scaling for Code Generation](https://arxiv.org/abs/2511.02854)
*Yixiang Chen,Tianshi Zheng,Shijue Huang,Zhitao He,Yi R. Fung*

Main category: cs.SE

TL;DR: 提出SELF - REDRAFT框架研究大语言模型平衡开发与探索的内在能力，其性能优于Self - Refine，但仍有改进空间，研究为测试时扩展建立基线并指出关键改进领域。


<details>
  <summary>Details</summary>
Motivation: 现有范式在平衡开发与探索维度研究不足，要研究大语言模型平衡开发与探索的内在能力。

Method: 引入基于Self - Refine的SELF - REDRAFT框架，促使模型为有根本缺陷的解决方案提出新草案。

Result: SELF - REDRAFT在相同最大迭代次数收敛时性能优于Self - Refine，但存在生成指导性反馈能力受限和判别判断脆弱的问题，不同大语言模型平衡策略差异明显。

Conclusion: 研究为测试时扩展的内在探索 - 开发平衡建立基线，确定反馈和判别是未来改进的关键领域。

Abstract: Test-time scaling without interpreter feedback is essential for real-world
code generation scenarios where test cases are not readily available. While
existing paradigms often rely on either greedy exploitation (i.e., iterative
refinement) or stochastic exploration (i.e., relying on sample-based voting or
reranking mechanisms), the balance between these two dimensions remains
underexplored. To investigate the LLM's intrinsic ability to balance
exploitation and exploration, we introduce SELF-REDRAFT, a framework built upon
Self-Refine that encourages the model to propose new drafts for solutions that
are fundamentally flawed. Our results show that SELF-REDRAFT consistently
achieves better performance than Self-Refine when converged under the same
maximum number of iterations. Still, we observe that significant room for
improvement remains, largely due to two core aspects of current self-redraft
capabilities: constrained capacity for generating instructive feedback and
fragile discriminative judgment. We also find that balancing strategies vary
notably across different LLMs, reflecting distinct, model-specific behaviors.
Overall, our study establishes a baseline for intrinsic
exploration-exploitation balancing in test-time scaling and identifies feedback
and discrimination as key areas with potential for future advances.

</details>


### [132] [The Evolution of Agile and Hybrid Project Management Methodologies: A Systematic Literature Review](https://arxiv.org/abs/2511.02859)
*Bianca Leech,Ridewaan Hanslo*

Main category: cs.SE

TL;DR: 文章探讨IT项目管理方法从传统向敏捷、混合模式转变，分析敏捷到混合框架的演变、挑战与成功因素，提供实用见解。


<details>
  <summary>Details</summary>
Motivation: IT项目快速发展促使项目管理方法转变，研究敏捷到混合框架的演变、挑战与成功因素。

Method: 通过PRISMA方法分析过去8年同行评审研究来确定关键趋势。

Result: 混合方法源于敏捷在大规模和监管环境中的局限，结合迭代灵活性与结构化治理，成功取决于领导支持、定制流程整合和持续改进机制。

Conclusion: 组织应进行情境适应而非依赖僵化框架，研究为组织混合转型提供实用见解。

Abstract: The rapid evolution of IT projects has driven the transformation of project
management methodologies, from traditional waterfall approaches to agile
frameworks and, more recently, hybrid models. This systematic literature review
investigates the evolution of agile methodologies into hybrid frameworks,
analysing their implementation challenges and success factors. We identify key
trends through PRISMA-guided analysis of peer-reviewed studies from the last 8
years. Hybrid methodologies emerge from agile limitations in large-scale and
regulated environments, combining iterative flexibility with structured
governance. Agile has several implementation challenges, leading to hybrid
methods, and the success hinges on leadership support, tailored process
integration, and continuous improvement mechanisms. The study explores the need
for contextual adaptation over rigid frameworks, offering practical insights
for organisations navigating hybrid transitions.

</details>


### [133] [LM-Fix: Lightweight Bit-Flip Detection and Rapid Recovery Framework for Language Models](https://arxiv.org/abs/2511.02866)
*Ahmad Tahmasivand,Noureldin Zahran,Saba Al-Sayouri,Mohammed Fouda,Khaled N. Khasawneh*

Main category: cs.SE

TL;DR: 提出轻量级框架LM - Fix用于检测和修复大语言模型故障，检测率高、开销低、恢复快。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型完整性检测方法重且慢，需要更好的解决方案。

Method: 运行短测试向量并使用哈希引导检查来检测位翻转故障，然后本地修复，无需完全重新加载。

Result: 在多个模型上，TVL = 200时单比特翻转检测率超94%，多比特翻转近100%，运行时开销1% - 7.7%，恢复速度比重新加载快超100倍。

Conclusion: LM - Fix是一种实用、低开销的解决方案，可保证大语言模型在生产中的可靠性。

Abstract: This paper presents LM-Fix, a lightweight detection and rapid recovery
framework for faults in large language models (LLMs). Existing integrity
approaches are often heavy or slow for modern LLMs. LM-Fix runs a short
test-vector pass and uses hash-guided checks to detect bit-flip faults, then
repairs them locally without a full reload. Across multiple models, it detects
over 94% of single-bit flips at TVL=200 and nearly 100% of multi-bit flips with
approximately 1% to 7.7% runtime overhead; recovery is more than 100x faster
than reloading. These results show a practical, low-overhead solution to keep
LLMs reliable in production

</details>


### [134] [Analysis of AdvFusion: Adapter-based Multilingual Learning for Code Large Language Models](https://arxiv.org/abs/2511.02869)
*Amirreza Esmaeili,Fahd Seddik,Yongyi Ji,Fatemeh Fard,Fuxiang Chen*

Main category: cs.SE

TL;DR: 本文研究了AdvFusion在代码大语言模型上对三个新任务的表现，发现不同模型和任务呈现不同特性，AdvFusion在不同任务中的表现有差异。


<details>
  <summary>Details</summary>
Motivation: 之前AdvFusion仅应用于预训练代码语言模型且局限于两个任务，本文希望拓展其在代码大语言模型上对三个新任务的研究。

Method: 在代码大语言模型上应用AdvFusion处理代码生成、代码翻译和提交消息生成三个新任务。

Result: 在代码生成中，AdvFusion优于AdapterFusion但不如其他PEFT方法；在提交消息生成中，AdapterFusion表现更好；在代码翻译中，AdvFusion总体不如AdapterFusion且差距随模型增大略扩大，其他PEFT方法表现更好。

Conclusion: 不同Code - LLMs/任务有不同特性，AdvFusion在不同任务中的性能表现有差异。

Abstract: Programming languages can benefit from one another by utilizing a language
model for software engineering tasks. Full fine-tuning and Parameter Efficient
Fine-Tuning (PEFT) of Code Language Models (Code-LMs) has been explored for
multilingual knowledge transfer. AdapterFusion is a PEFT architecture that aims
to enhance task performance by leveraging information from multiple programming
languages, but primarily focuses on the target programming language.
  In our previous work, we proposed AdvFusion, a novel PEFT-based approach that
effectively learns from other programming languages before adapting to the
target task. Though previous experiments showed that AdvFusion outperformed
AdapterFusion and LoRA, it was applied on pre-trained Code-LMs and was limited
to only two tasks, code summarization and method name prediction. In this
study, we expanded our work and investigated AdvFusion on Code Large Language
Models (Code-LLMs), considering three new tasks: code generation, code
translation, and commit message generation. We observed that different
Code-LLMs/tasks exhibit different characteristics. In code generation,
AdvFusion outperformed AdapterFusion but not other PEFT methods (LoRA,
Compacter, and TaskAdapter). In commit message generation, AdapterFusion
performed better than AdvFusion, and contrary to code generation, we found that
the other PEFT methods do not have better performance. In code translation,
AdvFusion performed worse than AdapterFusion overall, with the performance gap
marginally widening as the model size increases. However, consistent with code
generation, other PEFT methods showed better performance.

</details>


### [135] [An Analysis of Early-Stage Functional Safety Analysis Methods and Their Integration into Model-Based Systems Engineering](https://arxiv.org/abs/2511.02874)
*Jannatul Shefa,Taylan G. Topcu*

Main category: cs.SE

TL;DR: 文章研究关键安全分析技术能力及与MBSE集成现状，对比技术，发现FFIP适合现代系统，MBSE集成多聚焦FMEA，缺乏通用框架。


<details>
  <summary>Details</summary>
Motivation: 随着系统复杂度增加，需在系统生命周期早期进行有效安全分析以识别和降低风险。

Method: 采用两阶段方法，第一阶段对比FMEA、FHA和FFIP技术，分析优缺点；第二阶段回顾这些方法与MBSE集成的研究。

Result: FFIP更适合现代互联系统安全需求；MBSE集成主要聚焦FMEA，FHA和FFIP集成尚处起步阶段；FMEA - MBSE集成有四种类型；缺乏通用框架和标准。

Conclusion: 目前缺乏通用集成框架，有必要开发支持数字工程转型、实现协同生命周期安全管理的集成方法。

Abstract: As systems become increasingly complex, conducting effective safety analysis
in the earlier phases of a system's lifecycle is essential to identify and
mitigate risks before they escalate. To that end, this paper investigates the
capabilities of key safety analysis techniques, namely: Failure Mode and
Effects Analysis (FMEA), Functional Hazard Analysis (FHA), and Functional
Failure Identification and Propagation (FFIP), along with the current state of
the literature in terms of their integration into Model-Based Systems
Engineering (MBSE). A two-phase approach is adopted. The first phase is focused
on contrasting FMEA, FHA, and FFIP techniques, examining their procedures,
along with a documentation of their relative strengths and limitations. Our
analysis highlights FFIP's capability in identifying emergent system behaviors,
second-order effects, and fault propagation; thus, suggesting it is better
suited for the safety needs of modern interconnected systems. Second, we review
the existing research on the efforts to integrate each of these methods into
MBSE. We find that MBSE integration efforts primarily focus on FMEA, and
integration of FHA and FFIP is nascent. Additionally, FMEA-MBSE integration
efforts could be organized into four categories: model-to-model transformation,
use of external customized algorithms, built-in MBSE packages, and manual use
of standard MBSE diagrams. While our findings indicate a variety of MBSE
integration approaches, there is no universally established framework or
standard. This leaves room for an integration approach that could support the
ongoing Digital Engineering transformation efforts by enabling a more
synergistic lifecycle safety management methods and tools.

</details>


### [136] [CS Educator challenges and their solutions : A systematic mapping study](https://arxiv.org/abs/2511.02876)
*Anjali Chouhan,Sruti Srinivasa Ragavan,Amey Karkare*

Main category: cs.SE

TL;DR: 对近五年同行评审研究论文进行结构化文献综述，分析计算机科学教育挑战与对策，为相关人员提供见解。


<details>
  <summary>Details</summary>
Motivation: 计算机科学教育发展快，但缺乏对教育者面临挑战及对策的系统分类和综合研究，不清楚哪些领域研究充分或不足。

Method: 对近五年同行评审研究论文进行结构化文献综述，聚焦十个分类主题中的挑战和对策。

Result: 发现评估实践、教师培训等方面存在反复出现的问题，以及专业发展计划等应对策略，还有一些领域关注不足。

Conclusion: 该综述有助于全面了解计算机科学教育现状，为研究者、课程设计者和政策制定者提供有价值的见解以提高教学效果和支持教育者。

Abstract: Computer Science (CS) education is expanding rapidly, but educators continue
to face persistent challenges in teaching and learning environments.Despite
growing interest, limited systematic work exists to categorize and synthesize
the specific challenges faced by CS educators and the remedies adopted in
response.This is problematic because it remains unclear which areas have been
thoroughly addressed and which still lack sufficient scholarly attention. In
this study, we conducted a structured literature review of peer-reviewed
research papers published over the last five years, focusing on challenges and
remedies across ten categorized themes, including pedagogical, emotional,
technological, and institutional dimensions.Our analysis revealed recurring
issues in areas such as assessment practices, teacher training, classroom
management, and emotional well-being, along with various strategies such as
professional development programs and policy interventions adopted to mitigate
them while also revealing several areas that have received insufficient
attention.This review offers a consolidated understanding of the CS education
landscape, providing valuable insights for researchers, curriculum designers,
and policymakers aiming to improve teaching effectiveness and educator support.

</details>


### [137] [AgentSLA : Towards a Service Level Agreement for AI Agents](https://arxiv.org/abs/2511.02885)
*Gwendal Jouneaux,Jordi Cabot*

Main category: cs.SE

TL;DR: 本文针对AI代理服务质量保障难题，提出基于ISO/IEC 25010标准的质量模型和特定领域语言。


<details>
  <summary>Details</summary>
Motivation: AI组件以代理形式服务带来新挑战，QoS和SLA规范缺乏，质量定义难、QA方法无共识。

Method: 提出基于ISO/IEC 25010标准的AI代理质量模型，以及支持定义AI代理服务SLA的特定领域语言。

Result: 未提及具体结果。

Conclusion: 未提及具体结论。

Abstract: AI components are increasingly becoming a key element of all types of
software systems to enhance their functionality. These AI components are often
implemented as AI Agents, offering more autonomy than a plain integration of
Large Language Models (LLMs), moving from a Model-as-a-Service paradigm to an
Agent-as-a-Service one, bringing new challenges to the development of smart
software systems. Indeed, while support for the design, implementation, and
deployment of those agents exist, the specification of Quality of Service (QoS)
and definition of Service Level Agreements (SLAs) aspects for those agents,
important to ensure the quality of the resulting systems, remains an open
challenge. Part of this is due to the difficulty to clearly define quality in
the context of AI components, resulting in a lack of consensus on how to best
approach Quality Assurance (QA) for these types of systems. To address this
challenge, this paper proposes both a quality model for AI agents based on the
ISO/IEC 25010 standard, and a domain specific language to support the
definition of SLAs for the services provided by these AI agents.

</details>


### [138] [Comprehension-Performance Gap in GenAI-Assisted Brownfield Programming: A Replication and Extension](https://arxiv.org/abs/2511.02922)
*Yunhan Qiao,Christopher Hundhausen,Summit Haque,Md Istiak Hossain Shihab*

Main category: cs.SE

TL;DR: 研究GenAI编码助手Copilot对棕地编程任务的性能和代码理解的影响，发现其能提升性能但不提高理解。


<details>
  <summary>Details</summary>
Motivation: 探讨GenAI编码助手对代码理解的影响，复制并扩展先前研究。

Method: 进行被试内实验，让18名计算机科学研究生在有和无Copilot的情况下完成功能实现任务。

Result: Copilot显著减少任务时间、增加通过的测试用例数，但理解分数无差异，未发现理解与任务性能的相关性。

Conclusion: GenAI工具可加速遗留代码库的编程进度，但可能不会提升对代码库的理解，需考虑对编程教育和工具设计的影响。

Abstract: Code comprehension is essential for brownfield programming tasks, in which
developers maintain and enhance legacy code bases. Generative AI (GenAI) coding
assistants such as GitHub Copilot have been shown to improve developer
productivity, but their impact on code understanding is less clear. We
replicate and extend a previous study by exploring both performance and
comprehension in GenAI-assisted brownfield programming tasks. In a
within-subjects experimental study, 18 computer science graduate students
completed feature implementation tasks with and without Copilot. Results show
that Copilot significantly reduced task time and increased the number of test
cases passed. However, comprehension scores did not differ across conditions,
revealing a comprehension-performance gap: participants passed more test cases
with Copilot, but did not demonstrate greater understanding of the legacy
codebase. Moreover, we failed to find a correlation between comprehension and
task performance. These findings suggest that while GenAI tools can accelerate
programming progress in a legacy codebase, such progress may come without an
improved understanding of that codebase. We consider the implications of these
findings for programming education and GenAI tool design.

</details>


### [139] [Risk Estimation in Differential Fuzzing via Extreme Value Theory](https://arxiv.org/abs/2511.02927)
*Rafael Baez,Alejandro Olivas,Nathan K. Diamond,Marcelo Frias,Yannic Noller,Saeid Tizpaz-Niari*

Main category: cs.SE

TL;DR: 本文探讨用极值理论（EVT）解决差分模糊测试中遗漏或低估漏洞的风险，实验表明 EVT 表现良好并能节省大量字节码执行。


<details>
  <summary>Details</summary>
Motivation: 模糊测试作为动态分析，无法保证无漏洞，需评估差分模糊测试中遗漏或低估漏洞的风险。

Method: 将 EVT 应用于差分模糊测试，在真实 Java 库上实验，探索 EVT 可行性与最优超参数，对比 EVT 与基准统计方法。

Result: EVT 外推法在 14.3% 的情况下优于基准技术，64.2% 的情况打平；在真实 Java 库上平均节省数千万字节码执行。

Conclusion: EVT 可用于差分模糊测试以控制风险，能提高准确性和性能。

Abstract: Differential testing is a highly effective technique for automatically
detecting software bugs and vulnerabilities when the specifications involve an
analysis over multiple executions simultaneously. Differential fuzzing, in
particular, operates as a guided randomized search, aiming to find (similar)
inputs that lead to a maximum difference in software outputs or their
behaviors. However, fuzzing, as a dynamic analysis, lacks any guarantees on the
absence of bugs: from a differential fuzzing campaign that has observed no bugs
(or a minimal difference), what is the risk of observing a bug (or a larger
difference) if we run the fuzzer for one or more steps?
  This paper investigates the application of Extreme Value Theory (EVT) to
address the risk of missing or underestimating bugs in differential fuzzing.
The key observation is that differential fuzzing as a random process resembles
the maximum distribution of observed differences. Hence, EVT, a branch of
statistics dealing with extreme values, is an ideal framework to analyze the
tail of the differential fuzzing campaign to contain the risk. We perform
experiments on a set of real-world Java libraries and use differential fuzzing
to find information leaks via side channels in these libraries. We first
explore the feasibility of EVT for this task and the optimal hyperparameters
for EVT distributions. We then compare EVT-based extrapolation against baseline
statistical methods like Markov's as well as Chebyshev's inequalities, and the
Bayes factor. EVT-based extrapolations outperform the baseline techniques in
14.3% of cases and tie with the baseline in 64.2% of cases. Finally, we
evaluate the accuracy and performance gains of EVT-enabled differential fuzzing
in real-world Java libraries, where we reported an average saving of tens of
millions of bytecode executions by an early stop.

</details>


### [140] [Assurance Case Development for Evolving Software Product Lines: A Formal Approach](https://arxiv.org/abs/2511.03026)
*Logan Murphy,Torin Viger,Alessio Di Sandro,Aren A. Babikian,Marsha Chechik*

Main category: cs.SE

TL;DR: 本文提出一种正式方法用于结构化保证案例（AC）开发和回归分析，以应对软件产品线（SPL）中AC创建和维护难题，并通过医疗设备产品线案例展示成果。


<details>
  <summary>Details</summary>
Motivation: 在软件产品线中，为每个产品单独创建严格的AC不可行，且难以评估SPL演变带来的影响，因此需要一种能为整个SPL同时开发和维护AC的方法。

Method: 形式化一种用于SPL的可变感知AC语言，研究基于模板的AC开发提升，定义回归分析方法，开发基于模型的保证管理工具。

Result: 开发了基于模型的保证管理工具，通过医疗设备产品线案例展示了方法的应用。

Conclusion: 所提出的正式方法可用于SPL的AC开发和回归分析，能有效应对SPL中AC创建和维护的挑战。

Abstract: In critical software engineering, structured assurance cases (ACs) are used
to demonstrate how key system properties are supported by evidence (e.g., test
results, proofs). Creating rigorous ACs is particularly challenging in the
context of software product lines (SPLs), i.e, sets of software products with
overlapping but distinct features and behaviours. Since SPLs can encompass very
large numbers of products, developing a rigorous AC for each product
individually is infeasible. Moreover, if the SPL evolves, e.g., by the
modification or introduction of features, it can be infeasible to assess the
impact of this change. Instead, the development and maintenance of ACs ought to
be lifted such that a single AC can be developed for the entire SPL
simultaneously, and be analyzed for regression in a variability-aware fashion.
In this article, we describe a formal approach to lifted AC development and
regression analysis. We formalize a language of variability-aware ACs for SPLs
and study the lifting of template-based AC development. We also define a
regression analysis to determine the effects of SPL evolutions on
variability-aware ACs. We describe a model-based assurance management tool
which implements these techniques, and illustrate our contributions by
developing an AC for a product line of medical devices.

</details>


### [141] [Adaptive Detection of Software Aging under Workload Shift](https://arxiv.org/abs/2511.03103)
*Rafael José Moura,Maria Gizele Nascimento,Fumio Machida,Ermeson Andrade*

Main category: cs.SE

TL;DR: 提出基于机器学习的自适应方法用于动态负载环境下软件老化检测，对比静态与自适应模型，自适应ADWIN模型表现佳。


<details>
  <summary>Details</summary>
Motivation: 解决软件老化导致的性能下降和故障风险问题，尤其是在动态负载环境下。

Method: 提出基于机器学习的自适应方法，对比静态模型与结合自适应检测器（DDM和ADWIN）的自适应模型。

Result: 实验表明静态模型在未知负载下性能下降，自适应ADWIN模型在各场景F1分数超0.93。

Conclusion: 自适应ADWIN模型在动态负载环境下软件老化检测中表现优秀，能保持高精度。

Abstract: Software aging is a phenomenon that affects long-running systems, leading to
progressive performance degradation and increasing the risk of failures. To
mitigate this problem, this work proposes an adaptive approach based on machine
learning for software aging detection in environments subject to dynamic
workload conditions. We evaluate and compare a static model with adaptive
models that incorporate adaptive detectors, specifically the Drift Detection
Method (DDM) and Adaptive Windowing (ADWIN), originally developed for concept
drift scenarios and applied in this work to handle workload shifts. Experiments
with simulated sudden, gradual, and recurring workload transitions show that
static models suffer a notable performance drop when applied to unseen workload
profiles, whereas the adaptive model with ADWIN maintains high accuracy,
achieving an F1-Score above 0.93 in all analyzed scenarios.

</details>


### [142] [Automated Prompt Generation for Code Intelligence: An Empirical study and Experience in WeChat](https://arxiv.org/abs/2511.03136)
*Kexing Ji,Shiyun Fu,Cuiyun Gao,Yujia Chen,Zezhou Yang,Chaozheng Wang,Yuetang Deng*

Main category: cs.SE

TL;DR: 本文研究代码智能中自动化提示生成（APG），实证研究IG和MSR，结合两者提出新APG方法，实验表明效果显著。


<details>
  <summary>Details</summary>
Motivation: 当前代码智能中提示设计多为手动，耗时且依赖特定模型和任务，APG在代码智能中研究不足。

Method: 实证研究APG的IG和MSR两部分，在多个开源模型和任务上评估常用APG方法，结合两者最佳方法提出新APG方法。

Result: IG和MSR相比基本提示显著提升性能，新方法在多个指标上有大幅提升，在工业场景数据集上也有良好表现。

Conclusion: 提出的结合IG和MSR的新APG方法在代码智能任务中效果显著，能有效提升性能。

Abstract: Large Code Models (LCMs) show potential in code intelligence, but their
effectiveness is greatly influenced by prompt quality. Current prompt design is
mostly manual, which is time-consuming and highly dependent on specific LCMs
and tasks. While automated prompt generation (APG) exists in NLP, it is
underexplored for code intelligence. This creates a gap, as automating the
prompt process is essential for developers facing diverse tasks and black-box
LCMs.
  To mitigate this, we empirically investigate two important parts of APG:
Instruction Generation (IG) and Multi-Step Reasoning (MSR). IG provides a
task-related description to instruct LCMs, while MSR guides them to produce
logical steps before the final answer. We evaluate widely-used APG methods for
each part on four open-source LCMs and three code intelligence tasks: code
translation (PL-PL), code summarization (PL-NL), and API recommendation
(NL-PL).Experimental results indicate that both IG and MSR dramatically enhance
performance compared to basic prompts. Based on these results, we propose a
novel APG approach combining the best methods of the two parts. Experiments
show our approach achieves average improvements of 28.38% in CodeBLEU (code
translation), 58.11% in ROUGE-L (code summarization), and 84.53% in
SuccessRate@1 (API recommendation) over basic prompts. To validate its
effectiveness in an industrial scenario, we evaluate our approach on
WeChat-Bench, a proprietary dataset, achieving an average MRR improvement of
148.89% for API recommendation.

</details>


### [143] [RefAgent: A Multi-agent LLM-based Framework for Automatic Software Refactoring](https://arxiv.org/abs/2511.03153)
*Khouloud Oueslati,Maxime Lamothe,Foutse Khomh*

Main category: cs.SE

TL;DR: 本文探索基于大语言模型的代理在软件重构中的潜力，介绍多智能体框架RefAgent并评估，结果显示其在多方面表现良好，凸显多智能体架构在自动软件重构中的前景。


<details>
  <summary>Details</summary>
Motivation: 传统大语言模型在软件重构中依赖静态指令，而基于大语言模型的代理能动态适应，探索其在软件重构活动中的潜力。

Method: 引入多智能体框架RefAgent，由专业代理负责规划、执行、测试和迭代优化重构，在八个开源Java项目上评估，与单智能体方法、基于搜索的重构工具和历史开发者重构对比。

Result: RefAgent单元测试通过率中位数达90%，减少代码异味中位数52.5%，改善关键质量属性中位数8.6%，识别重构机会的F1分数较高，相比单智能体方法有显著提升。

Conclusion: 多智能体架构在推进自动软件重构方面有很大潜力。

Abstract: Large Language Models (LLMs) have substantially influenced various software
engineering tasks. Indeed, in the case of software refactoring, traditional
LLMs have shown the ability to reduce development time and enhance code
quality. However, these LLMs often rely on static, detailed instructions for
specific tasks. In contrast, LLM-based agents can dynamically adapt to evolving
contexts and autonomously make decisions by interacting with software tools and
executing workflows. In this paper, we explore the potential of LLM-based
agents in supporting refactoring activities. Specifically, we introduce
RefAgent, a multi-agent LLM-based framework for end-to-end software
refactoring. RefAgent consists of specialized agents responsible for planning,
executing, testing, and iteratively refining refactorings using self-reflection
and tool-calling capabilities. We evaluate RefAgent on eight open-source Java
projects, comparing its effectiveness against a single-agent approach, a
search-based refactoring tool, and historical developer refactorings. Our
assessment focuses on: (1) the impact of generated refactorings on software
quality, (2) the ability to identify refactoring opportunities, and (3) the
contribution of each LLM agent through an ablation study. Our results show that
RefAgent achieves a median unit test pass rate of 90%, reduces code smells by a
median of 52.5%, and improves key quality attributes (e.g., reusability) by a
median of 8.6%. Additionally, it closely aligns with developer refactorings and
the search-based tool in identifying refactoring opportunities, attaining a
median F1-score of 79.15% and 72.7%, respectively. Compared to single-agent
approaches, RefAgent improves the median unit test pass rate by 64.7% and the
median compilation success rate by 40.1%. These findings highlight the promise
of multi-agent architectures in advancing automated software refactoring.

</details>


### [144] [Understanding Robustness of Model Editing in Code LLMs: An Empirical Study](https://arxiv.org/abs/2511.03182)
*Vinaik Chhetri,A. B Siddique,Umar Farooq*

Main category: cs.SE

TL;DR: 研究对五种模型编辑方法在三个开源代码大模型上进行测试，发现即时和顺序编辑都会降低模型性能，正确采用预期更改的情况仅约6%。


<details>
  <summary>Details</summary>
Motivation: 大语言模型预训练后保持静态，而编程语言和API不断发展，从头重新训练成本高，模型编辑是轻量级替代方案，但不清楚其是否能真正实现句法和语义适应，故开展研究。

Method: 对Constrained Fine - Tuning (FT)、GRACE、MEMIT、PMET和ROME五种模型编辑方法，在CodeLlama、CodeQwen1.5和DeepSeek - Coder三个开源代码大模型上，进行即时和顺序编辑实验，用三个评估集评估可靠性、泛化性和特异性，在三个层面测量模型正确性。

Result: 即时编辑会持续降低模型性能，顺序编辑会进一步加剧，多数通过的生成依赖变通方法，错误采用情况更频繁，正确采用仅约6%。

Conclusion: 目前的模型编辑方法在处理API变更时效果不佳，难以真正实现句法和语义适应。

Abstract: Large language models (LLMs) are increasingly used in software development.
However, while LLMs remain static after pretraining, programming languages and
APIs continue to evolve, leading to the generation of deprecated or
incompatible code that undermines reliability. Retraining LLMs from scratch to
reflect such changes is computationally expensive, making model editing a
promising lightweight alternative that updates only a small subset of
parameters. Despite its potential, it remains unclear whether model editing
yields genuine syntactic and semantic adaptations or merely superficial fixes.
In this work, we present a systematic study of five state-of-the-art model
editing methods: Constrained Fine-Tuning (FT), GRACE, MEMIT, PMET, and ROME. We
apply these methods to three leading open-source code LLMs, CodeLlama,
CodeQwen1.5, and DeepSeek-Coder, under controlled API deprecation scenarios.
Our evaluation covers both instant and sequential editing settings, using three
disjoint evaluation sets designed to assess reliability, generalization, and
specificity. We measure model correctness at three levels: successful
compilation, partial test case pass, and full test pass. Our findings show that
instant edits consistently degrade model performance, with syntactic validity
dropping by up to 86 percentage points and functional correctness declining by
45 points even in the best-performing setting. Sequential edits further amplify
this degradation, and in some cases, model performance collapses entirely.
Across all models, most passing generations relied on workarounds rather than
correctly adopting the intended changes, while faulty adoptions that result in
test failures or compilation errors were significantly more frequent. Correct
adoptions, where the model correctly integrates the intended change, occurred
in only about 6% of cases.

</details>


### [145] [Towards Realistic Project-Level Code Generation via Multi-Agent Collaboration and Semantic Architecture Modeling](https://arxiv.org/abs/2511.03404)
*Qianhui Zhao,Li Zhang,Fang Liu,Junhang Cheng,Chengru Wu,Junchen Ai,Qiaoyuanhe Meng,Lichen Zhang,Xiaoli Lian,Shubin Song,Yuanping Guo*

Main category: cs.SE

TL;DR: 现有大语言模型在项目级代码生成存在局限，本文引入CodeProjectEval数据集，提出ProjectGen框架，实验显示其表现达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现实软件工程对项目级代码生成需求增长，但现有研究存在数据集不真实、评估指标不可靠、语义差距和管理依赖与保证质量困难等局限。

Method: 引入CodeProjectEval数据集，提出ProjectGen多智能体框架，框架内引入语义软件架构树（SSAT）。

Result: ProjectGen在DevBench小数据集通过52/124测试用例，较基线方法提升57%；在CodeProjectEval上通过310个测试用例，较基线提升约10倍。

Conclusion: ProjectGen在项目级代码生成中表现出色，达到了当前最优性能。

Abstract: In recent years, Large Language Models (LLMs) have achieved remarkable
progress in automated code generation. In real-world software engineering, the
growing demand for rapid iteration and continuous delivery underscores the
importance of project-level code generation, where LLMs are expected to
generate complete software projects directly from complex user requirements.
Although existing studies have made initial explorations, they still face key
limitations, including unrealistic datasets and unreliable evaluation metrics
that fail to reflect real-world complexity, the semantic gap between
human-written requirements and machine-interpretable structures, and
difficulties in managing hierarchical dependencies and maintaining quality
throughout the generation process. To address these limitations, we first
introduce CodeProjectEval, a project-level code generation dataset built from
18 real-world repositories with 12.7 files and 2,388.6 lines of code per task
on average, supplemented with documentation and executable test cases for
automatic evaluation. We further propose ProjectGen, a multi-agent framework
that decomposes projects into architecture design, skeleton generation, and
code filling stages with iterative refinement and memory-based context
management. Within this framework, we introduce the Semantic Software
Architecture Tree (SSAT), a structured and semantically rich representation
that effectively bridges user requirements and source code implementation.
Experiments show that ProjectGen achieves state-of-the-art performance, passing
52/124 test cases on the small-scale project-level code generation dataset
DevBench, a 57% improvement over the baseline approaches, and 310 test cases on
CodeProjectEval, representing an improvement of roughly tenfold compared to the
baselines.

</details>


### [146] [Light over Heavy: Automated Performance Requirements Quantification with Linguistic Inducement](https://arxiv.org/abs/2511.03421)
*Shihai Wang,Tao Chen*

Main category: cs.SE

TL;DR: 本文提出高效自动方法 LQPR 用于性能需求量化，对比多种方法证明其优势，表明专业方法在性能需求量化上更合适。


<details>
  <summary>Details</summary>
Motivation: 现有性能需求量化依赖人工，成本高且易出错，需要自动方法。

Method: LQPR 基于新理论框架将量化转化为分类问题，设计轻量级语言诱导匹配机制。

Result: 在不同数据集上与九种最先进的基于学习的方法对比，LQPR 在 75% 以上的情况下排名最佳，成本低两个数量级。

Conclusion: 至少在性能需求量化方面，专业方法比通用的大语言模型驱动方法更合适。

Abstract: Elicited performance requirements need to be quantified for compliance in
different engineering tasks, e.g., configuration tuning and performance
testing. Much existing work has relied on manual quantification, which is
expensive and error-prone due to the imprecision. In this paper, we present
LQPR, a highly efficient automatic approach for performance requirements
quantification.LQPR relies on a new theoretical framework that converts
quantification as a classification problem. Despite the prevalent applications
of Large Language Models (LLMs) for requirement analytics, LQPR takes a
different perspective to address the classification: we observed that
performance requirements can exhibit strong patterns and are often
short/concise, therefore we design a lightweight linguistically induced
matching mechanism. We compare LQPR against nine state-of-the-art
learning-based approaches over diverse datasets, demonstrating that it is
ranked as the sole best for 75% or more cases with two orders less cost. Our
work proves that, at least for performance requirement quantification,
specialized methods can be more suitable than the general LLM-driven
approaches.

</details>


### [147] [U2F: Encouraging SWE-Agent to Seize Novelty without Losing Feasibility](https://arxiv.org/abs/2511.03517)
*Wencheng Ye,Yan Liu*

Main category: cs.SE

TL;DR: 现有基于大语言模型的软件工程代理有局限，本文提出U2F框架应对，应用于实际任务取得显著效果，体现拥抱不确定性对软件工程创新的潜力。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的软件工程代理多采用传统方法解决明确问题，忽视创新方案，在开放世界软件环境中存在局限。

Method: 提出U2F框架，包含发现 - 探索 - 集成代理系统和三个维度的认知增强机制。

Result: 应用于218个实际软件工程故事，人类专家评价整体新颖性提升14%，语义新颖性提升51%，可行性稳定（4.02/5.0），并得到基于大语言模型评估器的证实。

Conclusion: 拥抱不确定性可作为软件工程创新的催化剂。

Abstract: Large language models (LLMs) have shown strong capabilities in software
engineering tasks, yet most existing LLM-based SWE-Agents mainly tackle
well-defined problems using conventional methods, often overlooking alternative
or innovative solutions beyond their predefined frameworks. This limitation is
evident in open-world software environments, where emerging challenges
transcend established paradigms.
  We propose U2F (Unknown Unknowns to Functional solutions), a
cognitive-inspired, uncertainty-embracing multi-agent framework that
systematically surfaces "Unknown Unknowns" - novel solution pathways absent
from initial formulations but holding innovative potential. U2F consists of two
key components: (1) a Discovery-Exploration-Integration agent system for
uncovering and synthesizing potential solutions, and (2) cognitive enhancement
mechanisms across three dimensions: cross-domain analogical reasoning, reverse
thinking, and external validation, which strategically reframe and extend
conventional solution boundaries.
  Applied to 218 real-world software enabler stories curated from authentic
engineering tasks, U2F achieved notable improvements: human experts reported a
14 percent increase in overall novelty, 51 percent improvement in semantic
novelty, and stable feasibility (4.02/5.0), corroborated by an LLM-based
evaluator. These results highlight the potential of embracing uncertainty as a
catalyst for innovation in software engineering.

</details>


### [148] [Uncovering Code Insights: Leveraging GitHub Artifacts for Deeper Code Understanding](https://arxiv.org/abs/2511.03549)
*Ziv Nevo,Orna Raz,Karen Yorav*

Main category: cs.SE

TL;DR: 提出利用GitHub自然语言工件增强基于大语言模型的代码理解方法，实现工具并进行用户研究，结果显示生成的见解有帮助且无幻觉。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在生成代码解释时缺乏软件工程上下文，为更好理解代码目的。

Method: 利用GitHub的自然语言工件，系统包含提取结构上下文、生成代码目的解释、验证解释三个组件，实现独立工具和MCP服务器。

Result: 小规模用户研究表明生成的见解通常有帮助、非平凡且无幻觉。

Conclusion: 所提方法能有效增强基于大语言模型的代码理解，生成有价值的代码见解。

Abstract: Understanding the purpose of source code is a critical task in software
maintenance, onboarding, and modernization. While large language models (LLMs)
have shown promise in generating code explanations, they often lack grounding
in the broader software engineering context. We propose a novel approach that
leverages natural language artifacts from GitHub -- such as pull request
descriptions, issue descriptions and discussions, and commit messages -- to
enhance LLM-based code understanding. Our system consists of three components:
one that extracts and structures relevant GitHub context, another that uses
this context to generate high-level explanations of the code's purpose, and a
third that validates the explanation. We implemented this as a standalone tool,
as well as a server within the Model Context Protocol (MCP), enabling
integration with other AI-assisted development tools. Our main use case is that
of enhancing a standard LLM-based code explanation with code insights that our
system generates. To evaluate explanations' quality, we conducted a small scale
user study, with developers of several open projects, as well as developers of
proprietary projects. Our user study indicates that when insights are generated
they often are helpful and non trivial, and are free from hallucinations.

</details>


### [149] [The OpenHands Software Agent SDK: A Composable and Extensible Foundation for Production Agents](https://arxiv.org/abs/2511.03690)
*Xingyao Wang,Simon Rosenberg,Juan Michelini,Calvin Smith,Hoang Tran,Engel Nyst,Rohit Malhotra,Xuhui Zhou,Valerie Chen,Robert Brennan,Graham Neubig*

Main category: cs.SE

TL;DR: 本文介绍OpenHands软件代理SDK，它是对流行框架代理组件的重新设计，具备灵活性、安全性等特点，在基准测试中表现良好，能为代理开发提供实用基础。


<details>
  <summary>Details</summary>
Motivation: 构建适用于软件开发的生产就绪软件代理是复杂任务，需要实现的灵活性、可靠安全的执行以及用户交互接口，因此开发OpenHands软件代理SDK。

Method: 对OpenHands框架的代理组件进行完整架构重新设计，设计简单接口实现代理，具备本地到远程执行的可移植性、集成REST/WebSocket服务，可连接多种用户接口，集成多种独特功能。

Result: 在SWE - Bench Verified和GAIA基准测试中表现出强劲性能。

Conclusion: OpenHands软件代理SDK能为原型设计、开发新定制应用和大规模可靠部署代理提供实用基础。

Abstract: Agents are now used widely in the process of software development, but
building production-ready software engineering agents is a complex task.
Deploying software agents effectively requires flexibility in implementation
and experimentation, reliable and secure execution, and interfaces for users to
interact with agents. In this paper, we present the OpenHands Software Agent
SDK, a toolkit for implementing software development agents that satisfy these
desiderata. This toolkit is a complete architectural redesign of the agent
components of the popular OpenHands framework for software development agents,
which has 64k+ GitHub stars. To achieve flexibility, we design a simple
interface for implementing agents that requires only a few lines of code in the
default case, but is easily extensible to more complex, full-featured agents
with features such as custom tools, memory management, and more. For security
and reliability, it delivers seamless local-to-remote execution portability,
integrated REST/WebSocket services. For interaction with human users, it can
connect directly to a variety of interfaces, such as visual workspaces (VS
Code, VNC, browser), command-line interfaces, and APIs. Compared with existing
SDKs from OpenAI, Claude, and Google, OpenHands uniquely integrates native
sandboxed execution, lifecycle control, model-agnostic multi-LLM routing, and
built-in security analysis. Empirical results on SWE-Bench Verified and GAIA
benchmarks demonstrate strong performance. Put together, these elements allow
the OpenHands Software Agent SDK to provide a practical foundation for
prototyping, unlocking new classes of custom applications, and reliably
deploying agents at scale.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [150] [Multifractality and sample size influence on Bitcoin volatility patterns](https://arxiv.org/abs/2511.03314)
*Tetsuya Takaishi*

Main category: q-fin.ST

TL;DR: 利用比特币数据研究已实现波动率时间序列的Hurst指数有限样本效应，发现Hurst指数随采样周期变化，存在多重分形性。


<details>
  <summary>Details</summary>
Motivation: 研究已实现波动率时间序列的Hurst指数的有限样本效应。

Method: 使用比特币数据，进行多重分形分析。

Result: Hurst指数随采样周期Δ增加而减小，简单有限样本假设拟合数据好；Δ趋于0时Hurst指数小于1/2；五分钟已实现波动率相对误差1%；已实现波动率时间序列有多重分形性，小于价格回报时间序列。

Conclusion: 已实现波动率时间序列存在有限样本效应和多重分形性，且具有粗糙波动性。

Abstract: The finite sample effect on the Hurst exponent (HE) of realized volatility
time series is examined using Bitcoin data. This study finds that the HE
decreases as the sampling period $\Delta$ increases and a simple finite sample
ansatz closely fits the HE data. We obtain values of the HE as $\Delta
\rightarrow 0$, which are smaller than 1/2, indicating rough volatility. The
relative error is found to be $1\%$ for the widely used five-minute realized
volatility. Performing a multifractal analysis, we find the multifractality in
the realized volatility time series, smaller than that of the price-return time
series.

</details>


<div id='q-fin.TR'></div>

# q-fin.TR [[Back]](#toc)

### [151] [LiveTradeBench: Seeking Real-World Alpha with Large Language Models](https://arxiv.org/abs/2511.03628)
*Haofei Yu,Fenghai Li,Jiaxuan You*

Main category: q-fin.TR

TL;DR: 文章引入LiveTradeBench评估LLM代理在现实市场的表现，对21个LLM进行50天实时评估，发现静态评估与现实能力有差距。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型测试缺乏真实动态和不确定性，无法评估不确定下的决策能力，需新评估环境。

Method: 引入LiveTradeBench，遵循实时数据、投资组合管理抽象、多市场评估三个设计原则，让代理观察并输出资产分配比例。

Result: 高LMArena分数不意味着更好交易结果；模型有不同投资组合风格；部分LLM能利用实时信号调整决策。

Conclusion: 静态评估与现实能力存在差距，需测试实时不确定性下的连续决策和一致性的基准。

Abstract: Large language models (LLMs) achieve strong performance across
benchmarks--from knowledge quizzes and math reasoning to web-agent tasks--but
these tests occur in static settings, lacking real dynamics and uncertainty.
Consequently, they evaluate isolated reasoning or problem-solving rather than
decision-making under uncertainty. To address this, we introduce
LiveTradeBench, a live trading environment for evaluating LLM agents in
realistic and evolving markets. LiveTradeBench follows three design principles:
(i) Live data streaming of market prices and news, eliminating dependence on
offline backtesting and preventing information leakage while capturing
real-time uncertainty; (ii) a portfolio-management abstraction that extends
control from single-asset actions to multi-asset allocation, integrating risk
management and cross-asset reasoning; and (iii) multi-market evaluation across
structurally distinct environments--U.S. stocks and Polymarket prediction
markets--differing in volatility, liquidity, and information flow. At each
step, an agent observes prices, news, and its portfolio, then outputs
percentage allocations that balance risk and return. Using LiveTradeBench, we
run 50-day live evaluations of 21 LLMs across families. Results show that (1)
high LMArena scores do not imply superior trading outcomes; (2) models display
distinct portfolio styles reflecting risk appetite and reasoning dynamics; and
(3) some LLMs effectively leverage live signals to adapt decisions. These
findings expose a gap between static evaluation and real-world competence,
motivating benchmarks that test sequential decision making and consistency
under live uncertainty.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [152] [Scalable Single-Cell Gene Expression Generation with Latent Diffusion Models](https://arxiv.org/abs/2511.02986)
*Giovanni Palla,Sudarshan Babu,Payam Dibaeinia,James D. Pearce,Donghui Li,Aly A. Khan,Theofanis Karaletsos,Jakub M. Tomczak*

Main category: stat.ML

TL;DR: 提出用于单细胞基因表达数据的可扩展潜在扩散模型scLDM，实验证明其在多种任务中性能优越。


<details>
  <summary>Details</summary>
Motivation: 单细胞基因表达计算建模重要，但生成真实表达谱有挑战，现有生成模型存在缺陷。

Method: 引入scLDM，VAE使用固定大小潜在变量和MCAB架构，用Diffusion Transformers和线性插值替换高斯先验，采用多条件无分类器引导。

Result: 在观察性和扰动性单细胞数据及细胞水平分类等下游任务实验中表现优越。

Conclusion: scLDM是一个有效的单细胞基因表达数据生成模型。

Abstract: Computational modeling of single-cell gene expression is crucial for
understanding cellular processes, but generating realistic expression profiles
remains a major challenge. This difficulty arises from the count nature of gene
expression data and complex latent dependencies among genes. Existing
generative models often impose artificial gene orderings or rely on shallow
neural network architectures. We introduce a scalable latent diffusion model
for single-cell gene expression data, which we refer to as scLDM, that respects
the fundamental exchangeability property of the data. Our VAE uses fixed-size
latent variables leveraging a unified Multi-head Cross-Attention Block (MCAB)
architecture, which serves dual roles: permutation-invariant pooling in the
encoder and permutation-equivariant unpooling in the decoder. We enhance this
framework by replacing the Gaussian prior with a latent diffusion model using
Diffusion Transformers and linear interpolants, enabling high-quality
generation with multi-conditional classifier-free guidance. We show its
superior performance in a variety of experiments for both observational and
perturbational single-cell data, as well as downstream tasks like cell-level
classification.

</details>


### [153] [Unifying Information-Theoretic and Pair-Counting Clustering Similarity](https://arxiv.org/abs/2511.03000)
*Alexander J. Gates*

Main category: stat.ML

TL;DR: 本文提出统一聚类相似性度量两个主要家族的分析框架，阐明二者差异及提供选择、解释和扩展度量的原则。


<details>
  <summary>Details</summary>
Motivation: 现有聚类相似性度量评估结果差异大，两大家族的深层分析联系未完全明晰。

Method: 从两个互补视角构建分析框架，一是将两大家族表示为观测与期望共现的加权展开，二是将成对计数推广到k元组一致性。

Result: 以兰德指数和互信息为例说明方法，展示各家族其他指数的自然扩展。

Conclusion: 阐明两大家族差异原因，为跨应用选择、解释和扩展聚类相似性度量提供原则基础。

Abstract: Comparing clusterings is central to evaluating unsupervised models, yet the
many existing similarity measures can produce widely divergent, sometimes
contradictory, evaluations. Clustering similarity measures are typically
organized into two principal families, pair-counting and information-theoretic,
reflecting whether they quantify agreement through element pairs or aggregate
information across full cluster contingency tables. Prior work has uncovered
parallels between these families and applied empirical normalization or
chance-correction schemes, but their deeper analytical connection remains only
partially understood. Here, we develop an analytical framework that unifies
these families through two complementary perspectives. First, both families are
expressed as weighted expansions of observed versus expected co-occurrences,
with pair-counting arising as a quadratic, low-order approximation and
information-theoretic measures as higher-order, frequency-weighted extensions.
Second, we generalize pair-counting to $k$-tuple agreement and show that
information-theoretic measures can be viewed as systematically accumulating
higher-order co-assignment structure beyond the pairwise level. We illustrate
the approaches analytically for the Rand index and Mutual Information, and show
how other indices in each family emerge as natural extensions. Together, these
views clarify when and why the two regimes diverge, relating their
sensitivities directly to weighting and approximation order, and provide a
principled basis for selecting, interpreting, and extending clustering
similarity measures across applications.

</details>


### [154] [Precise asymptotic analysis of Sobolev training for random feature models](https://arxiv.org/abs/2511.03050)
*Katharine E Fisher,Matthew TC Li,Youssef Marzouk,Timo Schorlepp*

Main category: stat.ML

TL;DR: 本文对高维随机特征模型的Sobolev训练进行理论分析，推导泛化误差闭式解，指出梯度数据补充不总提升性能，过参数化程度应影响训练方法选择。


<details>
  <summary>Details</summary>
Motivation: 目前理论上对高维过参数化预测模型中Sobolev训练（结合函数和梯度数据的回归）对泛化误差的影响了解甚少，有必要进行研究。

Method: 结合统计物理中的复制方法和算子值自由概率论中的线性化方法。

Result: 得到随机特征模型泛化误差的闭式描述，发现补充梯度数据不总能提升单指标模型的预测性能。

Conclusion: 过参数化程度应指导训练方法的选择，确定了模型通过插值噪声函数和梯度数据实现最优性能的设置。

Abstract: Gradient information is widely useful and available in applications, and is
therefore natural to include in the training of neural networks. Yet little is
known theoretically about the impact of Sobolev training -- regression with
both function and gradient data -- on the generalization error of highly
overparameterized predictive models in high dimensions. In this paper, we
obtain a precise characterization of this training modality for random feature
(RF) models in the limit where the number of trainable parameters, input
dimensions, and training data tend proportionally to infinity. Our model for
Sobolev training reflects practical implementations by sketching gradient data
onto finite dimensional subspaces. By combining the replica method from
statistical physics with linearizations in operator-valued free probability
theory, we derive a closed-form description for the generalization errors of
the trained RF models. For target functions described by single-index models,
we demonstrate that supplementing function data with additional gradient data
does not universally improve predictive performance. Rather, the degree of
overparameterization should inform the choice of training method. More broadly,
our results identify settings where models perform optimally by interpolating
noisy function and gradient data.

</details>


### [155] [Provable Accelerated Bayesian Optimization with Knowledge Transfer](https://arxiv.org/abs/2511.03125)
*Haitao Lin,Boxin Zhao,Mladen Kolar,Chong Liu*

Main category: stat.ML

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study how Bayesian optimization (BO) can be accelerated on a target task
with historical knowledge transferred from related source tasks. Existing works
on BO with knowledge transfer either do not have theoretical guarantees or
achieve the same regret as BO in the non-transfer setting,
$\tilde{\mathcal{O}}(\sqrt{T \gamma_f})$, where $T$ is the number of
evaluations of the target function and $\gamma_f$ denotes its information gain.
In this paper, we propose the DeltaBO algorithm, in which a novel
uncertainty-quantification approach is built on the difference function
$\delta$ between the source and target functions, which are allowed to belong
to different reproducing kernel Hilbert spaces (RKHSs). Under mild assumptions,
we prove that the regret of DeltaBO is of order $\tilde{\mathcal{O}}(\sqrt{T
(T/N + \gamma_\delta)})$, where $N$ denotes the number of evaluations from
source tasks and typically $N \gg T$. In many applications, source and target
tasks are similar, which implies that $\gamma_\delta$ can be much smaller than
$\gamma_f$. Empirical studies on both real-world hyperparameter tuning tasks
and synthetic functions show that DeltaBO outperforms other baseline methods
and support our theoretical claims.

</details>


### [156] [Provable Separations between Memorization and Generalization in Diffusion Models](https://arxiv.org/abs/2511.03202)
*Zeqi Ye,Qijie Zhu,Molei Tao,Minshuo Chen*

Main category: stat.ML

TL;DR: 扩散模型易记忆训练数据，本文从统计估计和网络逼近两个视角给出双分离结果，开发基于剪枝的方法减少记忆同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型存在记忆训练数据问题，影响创造力并引发隐私安全担忧，且理论理解有限。

Method: 从统计估计和网络逼近两个互补视角开发双分离结果，在此基础上开发基于剪枝的方法。

Result: 从估计角度发现真实得分函数不最小化经验去噪损失，驱动记忆；从逼近角度证明实现经验得分函数网络规模需随样本量缩放。

Conclusion: 开发的基于剪枝的方法可减少扩散变换器中的记忆现象，同时保持生成质量。

Abstract: Diffusion models have achieved remarkable success across diverse domains, but
they remain vulnerable to memorization -- reproducing training data rather than
generating novel outputs. This not only limits their creative potential but
also raises concerns about privacy and safety. While empirical studies have
explored mitigation strategies, theoretical understanding of memorization
remains limited. We address this gap through developing a dual-separation
result via two complementary perspectives: statistical estimation and network
approximation. From the estimation side, we show that the ground-truth score
function does not minimize the empirical denoising loss, creating a separation
that drives memorization. From the approximation side, we prove that
implementing the empirical score function requires network size to scale with
sample size, spelling a separation compared to the more compact network
representation of the ground-truth score function. Guided by these insights, we
develop a pruning-based method that reduces memorization while maintaining
generation quality in diffusion transformers.

</details>


### [157] [RKUM: An R Package for Robust Kernel Unsupervised Methods](https://arxiv.org/abs/2511.03216)
*Md Ashad Alam*

Main category: stat.ML

TL;DR: 介绍了R包RKUM，可实现基于鲁棒核的无监督方法，实验表明其能有效识别异常值且对污染不敏感，为高维数据分析提供平台。


<details>
  <summary>Details</summary>
Motivation: 在有污染或噪声的数据条件下进行可靠的核学习分析。

Method: 使用广义损失函数估计鲁棒核协方差算子和交叉协方差算子，实现鲁棒核典型相关分析及影响函数。

Result: 标准核典型相关分析的影响函数能有效识别异常值，RKUM中的鲁棒核方法对污染的敏感性降低。

Conclusion: RKUM为高维数据应用中的鲁棒核分析提供了高效且可扩展的平台。

Abstract: RKUM is an R package developed for implementing robust kernel-based
unsupervised methods. It provides functions for estimating the robust kernel
covariance operator (CO) and the robust kernel cross-covariance operator (CCO)
using generalized loss functions instead of the conventional quadratic loss.
These operators form the foundation of robust kernel learning and enable
reliable analysis under contaminated or noisy data conditions. The package
includes implementations of robust kernel canonical correlation analysis
(Kernel CCA), as well as the influence function (IF) for both standard and
multiple kernel CCA frameworks. The influence function quantifies sensitivity
and helps detect influential or outlying observations across two-view and
multi-view datasets. Experiments using synthesized two-view and multi-view data
demonstrate that the IF of the standard kernel CCA effectively identifies
outliers, while the robust kernel methods implemented in RKUM exhibit reduced
sensitivity to contamination. Overall, RKUM provides an efficient and
extensible platform for robust kernel-based analysis in high-dimensional data
applications.

</details>


### [158] [Vector-valued self-normalized concentration inequalities beyond sub-Gaussianity](https://arxiv.org/abs/2511.03606)
*Diego Martinez-Taboada,Tomas Gonzalez,Aaditya Ramdas*

Main category: stat.ML

TL;DR: 本文为超亚高斯轻尾自归一化过程提供浓度界，并在线性回归中展示其应用。


<details>
  <summary>Details</summary>
Motivation: 标量值过程的自归一化集中行为研究广泛，但向量值过程，尤其是非亚高斯框架下的研究较少。

Method: 为超亚高斯轻尾的自归一化过程提供浓度界（如Bennett或Bernstein界）。

Result: 得出了超亚高斯轻尾自归一化过程的浓度界。

Conclusion: 研究成果在线性回归和线性老虎机中有应用价值。

Abstract: The study of self-normalized processes plays a crucial role in a wide range
of applications, from sequential decision-making to econometrics. While the
behavior of self-normalized concentration has been widely investigated for
scalar-valued processes, vector-valued processes remain comparatively
underexplored, especially outside of the sub-Gaussian framework. In this
contribution, we provide concentration bounds for self-normalized processes
with light tails beyond sub-Gaussianity (such as Bennett or Bernstein bounds).
We illustrate the relevance of our results in the context of online linear
regression, with applications in (kernelized) linear bandits.

</details>


### [159] [Colorectal Cancer Histopathological Grading using Multi-Scale Federated Learning](https://arxiv.org/abs/2511.03693)
*Md Ahasanul Arafath,Abhijit Kumar Ghosh,Md Rony Ahmed,Sabrin Afroz,Minhazul Hosen,Md Hasan Moon,Md Tanzim Reza,Md Ashad Alam*

Main category: stat.ML

TL;DR: 提出用于结直肠癌组织病理学分级的联邦学习框架，集成多尺度特征学习，在CRC - HGD数据集上表现良好，兼顾隐私与性能。


<details>
  <summary>Details</summary>
Motivation: 结直肠癌分级受观察者差异和多机构数据共享隐私限制影响，集中训练模型与数据治理法规冲突且忽视多尺度分析重要性。

Method: 提出可扩展、保护隐私的联邦学习框架，采用双流ResNetRS50骨干网络，集成到用FedProx稳定的联邦学习系统。

Result: 在CRC - HGD数据集上整体准确率达83.5%，优于集中式模型，对III级肿瘤召回率达87.5%，40x放大时准确率达88.0%。

Conclusion: 联邦多尺度方法既保护患者隐私，又提升模型性能和泛化能力，模块化管道为数字病理临床AI奠定基础。

Abstract: Colorectal cancer (CRC) grading is a critical prognostic factor but remains
hampered by inter-observer variability and the privacy constraints of
multi-institutional data sharing. While deep learning offers a path to
automation, centralized training models conflict with data governance
regulations and neglect the diagnostic importance of multi-scale analysis. In
this work, we propose a scalable, privacy-preserving federated learning (FL)
framework for CRC histopathological grading that integrates multi-scale feature
learning within a distributed training paradigm. Our approach employs a
dual-stream ResNetRS50 backbone to concurrently capture fine-grained nuclear
detail and broader tissue-level context. This architecture is integrated into a
robust FL system stabilized using FedProx to mitigate client drift across
heterogeneous data distributions from multiple hospitals. Extensive evaluation
on the CRC-HGD dataset demonstrates that our framework achieves an overall
accuracy of 83.5%, outperforming a comparable centralized model (81.6%).
Crucially, the system excels in identifying the most aggressive Grade III
tumors with a high recall of 87.5%, a key clinical priority to prevent
dangerous false negatives. Performance further improves with higher
magnification, reaching 88.0% accuracy at 40x. These results validate that our
federated multi-scale approach not only preserves patient privacy but also
enhances model performance and generalization. The proposed modular pipeline,
with built-in preprocessing, checkpointing, and error handling, establishes a
foundational step toward deployable, privacy-aware clinical AI for digital
pathology.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [160] [Optimal transport with a density-dependent cost function](https://arxiv.org/abs/2511.02929)
*Zichu Wang,Esteban G. Tabak*

Main category: stat.CO

TL;DR: 提出新的成对成本函数解决最优传输重心问题，开发数值求解框架并通过合成示例说明应用。


<details>
  <summary>Details</summary>
Motivation: 解决最优传输重心问题，提出考虑潜在概率分布的新距离概念。

Method: 采用两点间最小作用形式的成对成本函数，用依赖路径的切比雪夫多项式参数化最小作用路径，通过对抗惩罚确保路径端点与给定分布一致。

Result: 开发了对应数据驱动最优传输问题的数值求解框架。

Conclusion: 该方法可应用于聚类和匹配问题，通过合成示例得到验证。

Abstract: A new pairwise cost function is proposed for the optimal transport barycenter
problem, adopting the form of the minimal action between two points, with a
Lagrangian that takes into account an underlying probability distribution.
Under this notion of distance, two points can only be close if there exist
paths joining them that do not traverse areas of small probability. A framework
is proposed and developed for the numerical solution of the corresponding
data-driven optimal transport problem. The procedure parameterizes the paths of
minimal action through path dependent Chebyshev polynomials and enforces the
agreement between the paths' endpoints and the given source and target
distributions through an adversarial penalization. The methodology and its
application to clustering and matching problems is illustrated through
synthetic examples.

</details>


### [161] [Robust Global Fr'echet Regression via Weight Regularization](https://arxiv.org/abs/2511.03694)
*Hao Li,Shonosuke Sugasawa,Shota Katayama*

Main category: stat.CO

TL;DR: 提出一种鲁棒的全局Fréchet回归方法，结合弹性网正则化，给出迭代算法和调参准则，并通过数值研究验证性能。


<details>
  <summary>Details</summary>
Motivation: 传统Fréchet回归方法对离群对象敏感，需要开发鲁棒版本。

Method: 在目标函数中引入权重参数开发鲁棒的全局Fréchet回归，引入弹性网正则化控制离群对象影响，提供迭代算法估计回归函数和权重参数，提出贝叶斯信息准则选择正则化调参。

Result: 算法具有线性收敛性，数值研究展示了方法在矩阵和分布响应上的有限样本性能。

Conclusion: 所提方法能提供随观测数据自适应的鲁棒性。

Abstract: The Fr\'echet regression is a useful method for modeling random objects in a
general metric space given Euclidean covariates. However, the conventional
approach could be sensitive to outlying objects in the sense that the distance
from the regression surface is large compared to the other objects. In this
study, we develop a robust version of the global Fr\'echet regression by
incorporating weight parameters into the objective function. We then introduce
the Elastic net regularization, favoring a sparse vector of robust parameters
to control the influence of outlying objects. We provide a computational
algorithm to iteratively estimate the regression function and weight
parameters, with providing a linear convergence property. We also propose the
Bayesian information criterion to select the tuning parameters for
regularization, which gives adaptive robustness along with observed data. The
finite sample performance of the proposed method is demonstrated through
numerical studies on matrix and distribution responses.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [162] [Detecting Conflicts in Evidence Synthesis Models Using Score Discrepancies](https://arxiv.org/abs/2511.02977)
*Fuming Yang,David J. Nott,Anne M. Presanis*

Main category: stat.ME

TL;DR: 提出基于分数差异的证据合成模型冲突检测通用框架，经模拟和实际应用验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 证据合成模型中数据源共享参数会引发冲突，检测和量化这些冲突在模型批评中是挑战。

Method: 提出基于分数差异的冲突检测通用框架，将先验 - 数据冲突诊断扩展到层次模型潜在空间的更一般冲突检查。

Result: 模拟研究表明该方法能有效检测数据间不一致，应用于流感严重性模型显示其在复杂现实层次场景中的作用。

Conclusion: 该框架为贝叶斯证据合成中的一致性评估提供了灵活且广泛适用的工具。

Abstract: Evidence synthesis models combine multiple data sources to estimate latent
quantities of interest, enabling reliable inference on parameters that are
difficult to measure directly. However, shared parameters across data sources
can induce conflicts both among the data and with the assumed model structure.
Detecting and quantifying such conflicts remains a challenge in model
criticism. Here we propose a general framework for conflict detection in
evidence synthesis models based on score discrepancies, extending prior-data
conflict diagnostics to more general conflict checks in the latent space of
hierarchical models. Simulation studies in an exchangeable model demonstrate
that the proposed approach effectively detects between-data inconsistencies.
Application to an influenza severity model illustrates its use, complementary
to traditional deviance-based diagnostics, in complex real-world hierarchical
settings. The proposed framework thus provides a flexible and broadly
applicable tool for consistency assessment in Bayesian evidence synthesis.

</details>


### [163] [New sampling approaches for Shrinkage Inverse-Wishart distribution](https://arxiv.org/abs/2511.03044)
*Yiye Jiang*

Main category: stat.ME

TL;DR: 提出收缩逆威沙特（SIW）分布的新采样方法，基于SIR法，还对算法进行鲁棒化处理并给出理论结果和参数选择策略。


<details>
  <summary>Details</summary>
Motivation: SIW分布虽有优势，但现有采样算法慢且缺乏收敛性理论分析，SIR方法存在重要性权重差异大问题。

Method: 提出基于采样重要性重采样（SIR）方法的新算法，在SIR框架中加入裁剪步骤对算法进行鲁棒化处理。

Result: 新算法显著更快且有收敛率的理论保证，鲁棒化版本保留了原算法的计算效率。

Conclusion: 新算法和鲁棒化处理能有效解决SIW分布采样问题，且给出了理论结果和参数选择策略。

Abstract: In this paper, we propose new sampling approaches for the Shrinkage
Inverse-Wishart (SIW) distribution, a generalized family of the Inverse-Wishart
distribution originally proposed by Berger et al. (2020, Annals of Statistics).
It offers a flexible prior for covariance matrices and remains conjugate to the
Gaussian likelihood, similar to the classical Inverse-Wishart. Despite these
advantages, sampling from SIW remains challenging. The existing algorithm
relies on a nested Gibbs sampler, which is slow and lacks rigorous theoretical
analysis of its convergence. We propose a new algorithm based on the Sampling
Importance Resampling (SIR) method, which is significantly faster and comes
with theoretical guarantees on convergence rates. A known issue with SIR
methods is the large discrepancy in importance weights, which occurs when the
proposal distribution has thinner tails than the target. In the case of SIW,
certain parameter settings can lead to such discrepancies, reducing the
robustness of the output samples. To sample from such SIW distributions, we
robustify the proposed algorithm by including a clipping step to the SIR
framework which transforms large importance weights. We provide theoretical
results on the convergence behavior in terms of the clipping size, and discuss
strategies for choosing this parameter via simulation studies. The robustified
version retains the computational efficiency of the original algorithm.

</details>


### [164] [Beyond Maximum Likelihood: Variational Inequality Estimation for Generalized Linear Models](https://arxiv.org/abs/2511.03087)
*Linglingzhi Zhu,Jonghyeok Lee,Yao Xie*

Main category: stat.ME

TL;DR: 本文研究广义线性模型（GLM）中替代最大似然估计（MLE）的变分不等式（VI）估计器，分析其统计特性并拓展理论，数值实验表明VI框架有效。


<details>
  <summary>Details</summary>
Motivation: MLE在GLM参数推断中，在接近真实参数值时计算效率低，在非规范或一般链接函数设置下，优化景观存在非凸、非光滑和数值不稳定问题。

Method: 研究基于求解GLM似然方程变分不等式的替代估计器，从统计角度分析VI方法并与MLE系统比较，拓展VI估计器理论到更广泛链接函数类。

Result: 建立了VI估计器的非渐近估计误差界和渐近正态性，为定点和随机逼近算法提供收敛保证。数值实验显示VI框架保留了MLE的统计效率，且拓展了其适用性。

Conclusion: VI估计器是GLM参数推断中比MLE更优的方法，能在更具挑战性的GLM设置中有效应用。

Abstract: Generalized linear models (GLMs) are fundamental tools for statistical
modeling, with maximum likelihood estimation (MLE) serving as the classical
method for parameter inference. While MLE performs well in canonical GLMs, it
can become computationally inefficient near the true parameter value. In more
general settings with non-canonical or fully general link functions, the
resulting optimization landscape is often non-convex, non-smooth, and
numerically unstable. To address these challenges, we investigate an
alternative estimator based on solving the variational inequality (VI)
formulation of the GLM likelihood equations, originally proposed by Juditsky
and Nemirovski as an alternative for solving nonlinear least-squares problems.
Unlike their focus on algorithmic convergence in monotone settings, we analyze
the VI approach from a statistical perspective, comparing it systematically
with the MLE. We also extend the theory of VI estimators to a broader class of
link functions, including non-monotone cases satisfying a strong Minty
condition, and show that it admits weaker smoothness requirements than MLE,
enabling faster, more stable, and less locally trapped optimization.
Theoretically, we establish both non-asymptotic estimation error bounds and
asymptotic normality for the VI estimator, and further provide convergence
guarantees for fixed-point and stochastic approximation algorithms. Numerical
experiments show that the VI framework preserves the statistical efficiency of
MLE while substantially extending its applicability to more challenging GLM
settings.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [165] [Integrity Under Siege: A Rogue gNodeB's Manipulation of 5G Network Slice Allocation](https://arxiv.org/abs/2511.03312)
*Jiali Xu,Valeria Loscri,Romain Rouvoy*

Main category: cs.NI

TL;DR: 文章研究5G网络切片分配完整性漏洞，介绍威胁模型，通过实验评估攻击影响，包括QoS下降、资源污染等，讨论影响并提出后续缓解策略。


<details>
  <summary>Details</summary>
Motivation: 5G网络带来新攻击面，研究网络切片分配完整性漏洞。

Method: 引入基于风险分析的威胁模型，在5G测试平台进行综合实验评估。

Result: 攻击会导致QoS明显下降、切片隐蔽操纵、资源污染攻击引发拒绝服务等。

Conclusion: 强调保障5G网络动态资源管理完整性的紧迫性，提出后续跨层缓解策略。

Abstract: The advent of 5G networks, with network slicing as a cornerstone technology,
promises customized, high-performance services, but also introduces novel
attack surfaces beyond traditional threats. This article investigates a
critical and underexplored integrity vulnerability: the manipulation of network
slice allocation to compromise Quality of Service (QoS) and resource integrity.
We introduce a threat model, grounded in a risk analysis of permissible yet
insecure configurations like null-ciphering (5G-EA0), demonstrating how a rogue
gNodeB acting as a Man-in-the-Middle can exploit protocol weaknesses to forge
slice requests and hijack a User Equipment's (UE) connection. Through a
comprehensive experimental evaluation on a 5G testbed, we demonstrate the
attack's versatile and severe impacts. Our findings show this integrity breach
can manifest as obvious QoS degradation, such as a 95% bandwidth reduction and
150% latency increase when forcing UE to a suboptimal slice, or as stealthy
slice manipulation that is indistinguishable from benign network operation and
generates no core network errors. Furthermore, we validate a systemic resource
contamination attack where redirecting a crowd of UE orchestrates a
Denial-of-Service, causing packet loss to exceed 60% and inducing measurable
CPU saturation (~80%) on core network User Plane Functions (UPFs). Based on
these results, we discuss the profound implications for Service Level
Agreements (SLAs) and critical infrastructure. We propose concrete, cross-layer
mitigation strategies for network operators as future work, underscoring the
urgent need to secure the integrity of dynamic resource management in 5G
networks.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [166] [A Study on Library Resources with Services Satisfaction based on Library Users Affiliated Colleges to Solapur University](https://arxiv.org/abs/2511.03209)
*Patel Adam Burhansab,M Sadik Batcha,Muneer Ahmad*

Main category: cs.DL

TL;DR: 该研究评估索拉布尔大学相关图书馆用户对图书馆资源和服务的满意度，发现大量受访者满意。


<details>
  <summary>Details</summary>
Motivation: 评估和评价索拉布尔大学图书馆用户对图书馆资源和服务的满意度。

Method: 从26所学院收集数据，根据用户结果分析用户满意度。

Result: 大量受访者对图书馆设施和服务感到满意。

Conclusion: 未明确提及，但研究旨在实现图书馆用户的满意度。

Abstract: The main aim of this study was to assess and evaluate user satisfaction with
library resources and services among library users associated with Solapur
University. The current research shows the level of users satisfaction with
different library resources and services offered by college libraries. The
research found that a vast number of respondents were pleased with library
facilities and services. The research is designed to achieve users satisfaction
in the library to investigate the level of satisfaction towards library
resources and services with regards to 26 colleges of Solapur University based
in Maharashtra. Information in the form of data has been collected from
colleges and on the basis of users results; analysis needs to analyze users
satisfaction.

</details>


### [167] [Russian Contribution to Coronary Artery Disease Research: A Scientometric Mapping of Publications](https://arxiv.org/abs/2511.03215)
*Muneer Ahmad,M Sadik Batcha*

Main category: cs.DL

TL;DR: 本文研究1990 - 2019年俄罗斯冠心病研究产出，包括分布、期刊、作者等情况。


<details>
  <summary>Details</summary>
Motivation: 了解1990 - 2019年俄罗斯冠心病研究产出分布、期刊、作者、作者模式和引用模式。

Method: 基于科学引文索引（SCI）的二手数据，运用描述性和推断性统计技术。

Result: 1990 - 2019年俄罗斯学者发表5058篇冠心病文章，倾向发表在俄期刊，研究贡献形式多样，近年多作者文章增多。

Conclusion: 

Abstract: The present study attempts to highlight the research output generated in
Russia in coronary artery disease (CAD) research during the period 1990-2019 to
understand the distribution of research output, top journals for publications,
and most prolific authors, authorship pattern, and citation pattern. This study
is based on secondary data extracted from the Science Citation Index (SCI),
which is an integral component of the Web of Science. Descriptive and
inferential statistical techniques were applied in the study. There were 5058
articles by Russian scholars in coronary artery disease during 1990-2019; they
preferred to publish in Russian journals. The research contributions were in
the form of research articles, meeting abstracts and reviews with a consistent
drop in the number of editorial material and article; proceedings paper with
time. Co-authorship was the norm in coronary artery disease research, with a
steady increase in the number of multi-author documents in recent years.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [168] [Hybrid Fact-Checking that Integrates Knowledge Graphs, Large Language Models, and Search-Based Retrieval Agents Improves Interpretable Claim Verification](https://arxiv.org/abs/2511.03217)
*Shaghayegh Kolli,Richard Rosenbaum,Timo Cavelius,Lasse Strothe,Andrii Lata,Jana Diesner*

Main category: cs.CL

TL;DR: 本文提出结合大语言模型、知识图谱和实时搜索代理的混合事实核查方法，系统分三步，在FEVER基准测试表现好，还能处理信息不足情况。


<details>
  <summary>Details</summary>
Motivation: 大语言模型生成流畅表述但信息可靠性不足，基于知识图谱的事实核查器存在覆盖范围有限或延迟问题，需结合二者优势。

Method: 将大语言模型与知识图谱和实时搜索代理集成，系统分三步：知识图谱检索、基于大语言模型的分类、知识图谱覆盖不足时调用网络搜索代理。

Result: 在FEVER基准测试的支持/反驳拆分上F1分数达0.93，对信息不足案例的重新标注研究表明能找到有效证据。

Conclusion: 提出模块化、开源的事实核查管道，有后备策略且能跨数据集泛化。

Abstract: Large language models (LLMs) excel in generating fluent utterances but can
lack reliable grounding in verified information. At the same time,
knowledge-graph-based fact-checkers deliver precise and interpretable evidence,
yet suffer from limited coverage or latency. By integrating LLMs with knowledge
graphs and real-time search agents, we introduce a hybrid fact-checking
approach that leverages the individual strengths of each component. Our system
comprises three autonomous steps: 1) a Knowledge Graph (KG) Retrieval for rapid
one-hop lookups in DBpedia, 2) an LM-based classification guided by a
task-specific labeling prompt, producing outputs with internal rule-based
logic, and 3) a Web Search Agent invoked only when KG coverage is insufficient.
Our pipeline achieves an F1 score of 0.93 on the FEVER benchmark on the
Supported/Refuted split without task-specific fine-tuning. To address Not
enough information cases, we conduct a targeted reannotation study showing that
our approach frequently uncovers valid evidence for claims originally labeled
as Not Enough Information (NEI), as confirmed by both expert annotators and LLM
reviewers. With this paper, we present a modular, opensource fact-checking
pipeline with fallback strategies and generalization across datasets.

</details>


### [169] [Beyond Ranked Lists: The SARAL Framework for Cross-Lingual Document Set Retrieval](https://arxiv.org/abs/2511.03228)
*Shantanu Agarwal,Joel Barry,Elizabeth Boschee,Scott Miller*

Main category: cs.CL

TL;DR: 介绍ISI的SARAL为MATERIAL项目所做工作，该方法着重检索相关文档集，在MATERIAL第三阶段评估中表现出色。


<details>
  <summary>Details</summary>
Motivation: 推进跨语言信息检索（CLIR）的发展。

Method: 提出一种处理CLIR的新方法，重点开发能检索相关文档集而非仅排序文档列表的方法。

Result: 在MATERIAL第三阶段评估中，SARAL在三种不同语言（波斯语、哈萨克语和格鲁吉亚语）的六个评估条件中的五个里超越其他团队。

Conclusion: SARAL的方法在跨语言信息检索中有良好表现。

Abstract: Machine Translation for English Retrieval of Information in Any Language
(MATERIAL) is an IARPA initiative targeted to advance the state of
cross-lingual information retrieval (CLIR). This report provides a detailed
description of Information Sciences Institute's (ISI's) Summarization and
domain-Adaptive Retrieval Across Language's (SARAL's) effort for MATERIAL.
Specifically, we outline our team's novel approach to handle CLIR with emphasis
in developing an approach amenable to retrieve a query-relevant document
\textit{set}, and not just a ranked document-list. In MATERIAL's Phase-3
evaluations, SARAL exceeded the performance of other teams in five out of six
evaluation conditions spanning three different languages (Farsi, Kazakh, and
Georgian).

</details>


### [170] [Silenced Biases: The Dark Side LLMs Learned to Refuse](https://arxiv.org/abs/2511.03369)
*Rom Himelstein,Amit LeVi,Brit Youngmann,Yaniv Nemcovsky,Avi Mendelson*

Main category: cs.CL

TL;DR: 提出Silenced Bias Benchmark (SBB)评估大语言模型隐藏的公平性问题，揭示模型直接响应与潜在公平问题的差异。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型公平性评估方法存在不足，易产生公平性假象，且之前考虑间接偏差的方法可扩展性有限并可能引入额外偏差。

Method: 提出SBB，通过激活引导减少问答过程中模型的拒绝回应，以揭示隐藏的偏差，且支持扩展到新的群体和主题。

Result: 在多个大语言模型上验证，发现模型直接响应和潜在公平问题存在显著差异。

Conclusion: SBB提供了一个公平性评估框架，有助于推动无对齐训练掩盖效应的公平模型和工具的未来发展。

Abstract: Safety-aligned large language models (LLMs) are becoming increasingly
widespread, especially in sensitive applications where fairness is essential
and biased outputs can cause significant harm. However, evaluating the fairness
of models is a complex challenge, and approaches that do so typically utilize
standard question-answer (QA) styled schemes. Such methods often overlook
deeper issues by interpreting the model's refusal responses as positive
fairness measurements, which creates a false sense of fairness. In this work,
we introduce the concept of silenced biases, which are unfair preferences
encoded within models' latent space and are effectively concealed by
safety-alignment. Previous approaches that considered similar indirect biases
often relied on prompt manipulation or handcrafted implicit queries, which
present limited scalability and risk contaminating the evaluation process with
additional biases. We propose the Silenced Bias Benchmark (SBB), which aims to
uncover these biases by employing activation steering to reduce model refusals
during QA. SBB supports easy expansion to new demographic groups and subjects,
presenting a fairness evaluation framework that encourages the future
development of fair models and tools beyond the masking effects of alignment
training. We demonstrate our approach over multiple LLMs, where our findings
expose an alarming distinction between models' direct responses and their
underlying fairness issues.

</details>


### [171] [Reading Between the Lines: The One-Sided Conversation Problem](https://arxiv.org/abs/2511.03056)
*Victoria Ebert,Rishabh Singh,Tuochao Chen,Noah A. Smith,Shyamnath Gollakota*

Main category: cs.CL

TL;DR: 本文研究单端对话问题，评估不同模型在重建缺失发言和生成摘要任务上的表现，取得有前景结果，迈向隐私感知对话AI。


<details>
  <summary>Details</summary>
Motivation: 解决现实中只能记录对话一方的场景下对话AI的应用问题，提出单端对话问题。

Method: 在MultiWOZ、DailyDialog和Candor数据集上，用人工A/B测试和大语言模型评判指标评估提示和微调模型。

Result: 获取未来一轮对话和话语长度信息可提升重建效果，占位提示可减少幻觉，大模型用提示能有好的重建效果，小模型需微调，不重建缺失发言也能生成高质量摘要。

Conclusion: 单端对话是新挑战，研究结果为隐私感知对话AI发展迈进了一步。

Abstract: Conversational AI is constrained in many real-world settings where only one
side of a dialogue can be recorded, such as telemedicine, call centers, and
smart glasses. We formalize this as the one-sided conversation problem (1SC):
inferring and learning from one side of a conversation. We study two tasks: (1)
reconstructing the missing speaker's turns for real-time use cases, and (2)
generating summaries from one-sided transcripts. Evaluating prompting and
finetuned models on MultiWOZ, DailyDialog, and Candor with both human A/B
testing and LLM-as-a-judge metrics, we find that access to one future turn and
information about utterance length improves reconstruction, placeholder
prompting helps to mitigate hallucination, and while large models generate
promising reconstructions with prompting, smaller models require finetuning.
Further, high-quality summaries can be generated without reconstructing missing
turns. We present 1SC as a novel challenge and report promising results that
mark a step toward privacy-aware conversational AI.

</details>


### [172] [CARMA: Comprehensive Automatically-annotated Reddit Mental Health Dataset for Arabic](https://arxiv.org/abs/2511.03102)
*Saad Mankarious,Ayah Zirikly*

Main category: cs.CL

TL;DR: 提出首个自动标注的阿拉伯语Reddit帖子大规模数据集CARMA，开展分析和分类实验，显示推动阿拉伯语心理健康检测的潜力。


<details>
  <summary>Details</summary>
Motivation: 心理健康障碍全球影响大，阿拉伯语群体早期检测面临资源有限和文化污名问题，且相关研究不足，缺乏标注数据集。

Method: 创建CARMA数据集，对用户词汇和语义差异进行定性和定量分析，用多种模型进行分类实验。

Result: CARMA在规模和多样性上超越现有资源，实验结果显示推动阿拉伯语心理健康检测有前景。

Conclusion: CARMA数据集有潜力推进像阿拉伯语这类代表性不足语言的心理健康检测。

Abstract: Mental health disorders affect millions worldwide, yet early detection
remains a major challenge, particularly for Arabic-speaking populations where
resources are limited and mental health discourse is often discouraged due to
cultural stigma. While substantial research has focused on English-language
mental health detection, Arabic remains significantly underexplored, partly due
to the scarcity of annotated datasets. We present CARMA, the first
automatically annotated large-scale dataset of Arabic Reddit posts. The dataset
encompasses six mental health conditions, such as Anxiety, Autism, and
Depression, and a control group. CARMA surpasses existing resources in both
scale and diversity. We conduct qualitative and quantitative analyses of
lexical and semantic differences between users, providing insights into the
linguistic markers of specific mental health conditions. To demonstrate the
dataset's potential for further mental health analysis, we perform
classification experiments using a range of models, from shallow classifiers to
large language models. Our results highlight the promise of advancing mental
health detection in underrepresented languages such as Arabic.

</details>


### [173] [Control Barrier Function for Aligning Large Language Models](https://arxiv.org/abs/2511.03121)
*Yuya Miyaoka,Masaki Inoue*

Main category: cs.CL

TL;DR: 提出基于控制的框架，用控制障碍函数确保大语言模型生成用户期望文本。


<details>
  <summary>Details</summary>
Motivation: 实现大语言模型与用户期望的对齐，生成符合要求的文本。

Method: 应用控制障碍函数安全过滤器处理基线大语言模型预测的标记，干预生成文本。

Result: 未提及具体结果，系统用开源语言模型实现，目标是生成积极文本。

Conclusion: 未提及明确结论，提出的安全过滤器有无需微调基线模型和可结合评估模型的优势。

Abstract: This paper proposes a control-based framework for aligning large language
models (LLMs) by leveraging a control barrier function (CBF) to ensure
user-desirable text generation. The presented framework applies the CBF safety
filter to the predicted token generated from the baseline LLM, to intervene in
the generated text. The safety filter includes two significant advantages: this
safety filter is an add-on type, allowing it to be used for alignment purposes
without fine-tuning the baseline LLM, and if there is an evaluation model
regarding the desired alignment, it can be directly applied to the filter
design. The overall text-generation system is implemented with open-source
language models, aiming to generate positive text.

</details>


### [174] [Who Sees the Risk? Stakeholder Conflicts and Explanatory Policies in LLM-based Risk Assessment](https://arxiv.org/abs/2511.03152)
*Srishti Yadav,Jasmina Gajcin,Erik Miehling,Elizabeth Daly*

Main category: cs.CL

TL;DR: 本文提出用大语言模型进行利益相关者风险评估的框架，通过案例展示其有效性，强调利益相关者感知对风险评估的影响及解释的重要性。


<details>
  <summary>Details</summary>
Motivation: 理解不同利益相关者对人工智能系统风险的感知，以实现其负责任的部署。

Method: 运用大语言模型作为评判者预测和解释风险，结合风险图谱关联和GloVE解释方法生成利益相关者特定的可解释策略，通过三个真实人工智能用例展示方法，并提出交互式可视化。

Result: 利益相关者视角显著影响风险感知和冲突模式。

Conclusion: 强调利益相关者感知解释对基于大语言模型的评估的重要性，使其更透明、可解释，符合以人为本的人工智能治理目标。

Abstract: Understanding how different stakeholders perceive risks in AI systems is
essential for their responsible deployment. This paper presents a framework for
stakeholder-grounded risk assessment by using LLMs, acting as judges to predict
and explain risks. Using the Risk Atlas Nexus and GloVE explanation method, our
framework generates stakeholder-specific, interpretable policies that shows how
different stakeholders agree or disagree about the same risks. We demonstrate
our method using three real-world AI use cases of medical AI, autonomous
vehicles, and fraud detection domain. We further propose an interactive
visualization that reveals how and why conflicts emerge across stakeholder
perspectives, enhancing transparency in conflict reasoning. Our results show
that stakeholder perspectives significantly influence risk perception and
conflict patterns. Our work emphasizes the importance of these
stakeholder-aware explanations needed to make LLM-based evaluations more
transparent, interpretable, and aligned with human-centered AI governance
goals.

</details>


### [175] [LGM: Enhancing Large Language Models with Conceptual Meta-Relations and Iterative Retrieval](https://arxiv.org/abs/2511.03214)
*Wenchang Lei,Ping Zou,Yue Wang,Feng Sun,Lei Zhao*

Main category: cs.CL

TL;DR: 提出语言图模型（LGM）增强大语言模型概念理解，实验显示其优于现有RAG基线。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在处理模糊或概念不对齐的用户指令时存在困难，需要增强其概念理解能力。

Method: 提出LGM，从自然语言提取元关系，用反射机制验证，通过概念迭代检索算法动态提供关系和描述，让大模型处理任意长度文本。

Result: 在标准基准测试中，LGM始终优于现有的RAG基线。

Conclusion: LGM能有效提升大语言模型对概念的解释和准确响应能力，且无需截断文本。

Abstract: Large language models (LLMs) exhibit strong semantic understanding, yet
struggle when user instructions involve ambiguous or conceptually misaligned
terms. We propose the Language Graph Model (LGM) to enhance conceptual clarity
by extracting meta-relations-inheritance, alias, and composition-from natural
language. The model further employs a reflection mechanism to validate these
meta-relations. Leveraging a Concept Iterative Retrieval Algorithm, these
relations and related descriptions are dynamically supplied to the LLM,
improving its ability to interpret concepts and generate accurate responses.
Unlike conventional Retrieval-Augmented Generation (RAG) approaches that rely
on extended context windows, our method enables large language models to
process texts of any length without the need for truncation. Experiments on
standard benchmarks demonstrate that the LGM consistently outperforms existing
RAG baselines.

</details>


### [176] [Comparing the Performance of LLMs in RAG-based Question-Answering: A Case Study in Computer Science Literature](https://arxiv.org/abs/2511.03261)
*Ranul Dayarathne,Uvini Ranaweera,Upeksha Ganegoda*

Main category: cs.CL

TL;DR: 研究对比四种开源大语言模型和GPT - 3.5在RAG支持下计算机科学文献问答任务的表现，指出GPT - 3.5表现出色，Mistral - 7b - instruct在开源模型中表现突出，Orca - mini - v3 - 7b响应延迟最短，强调开源模型结合更好基础设施可与专有模型媲美。


<details>
  <summary>Details</summary>
Motivation: 随着RAG兴起，比较不同大语言模型在不同领域问答任务中的表现。

Method: 比较四种开源大语言模型（Mistral - 7b - instruct、LLaMa2 - 7b - chat、Falcon - 7b - instruct、Orca - mini - v3 - 7b）和GPT - 3.5在RAG支持下计算机科学文献问答任务的表现，采用准确性、精度、人工专家排名、Gemini排名和余弦相似度等评估指标。

Result: GPT - 3.5搭配RAG能有效回答二元和长答案问题；Mistral - 7b - instruct在开源模型中回答二元和长答案问题表现最佳；Orca - mini - v3 - 7b平均响应延迟最短，LLaMa2 - 7b - chat平均响应延迟最长。

Conclusion: 开源大语言模型结合更好的基础设施可以与GPT - 3.5等专有模型相配合。

Abstract: Retrieval Augmented Generation (RAG) is emerging as a powerful technique to
enhance the capabilities of Generative AI models by reducing hallucination.
Thus, the increasing prominence of RAG alongside Large Language Models (LLMs)
has sparked interest in comparing the performance of different LLMs in
question-answering (QA) in diverse domains. This study compares the performance
of four open-source LLMs, Mistral-7b-instruct, LLaMa2-7b-chat,
Falcon-7b-instruct and Orca-mini-v3-7b, and OpenAI's trending GPT-3.5 over QA
tasks within the computer science literature leveraging RAG support. Evaluation
metrics employed in the study include accuracy and precision for binary
questions and ranking by a human expert, ranking by Google's AI model Gemini,
alongside cosine similarity for long-answer questions. GPT-3.5, when paired
with RAG, effectively answers binary and long-answer questions, reaffirming its
status as an advanced LLM. Regarding open-source LLMs, Mistral AI's
Mistral-7b-instruct paired with RAG surpasses the rest in answering both binary
and long-answer questions. However, among the open-source LLMs, Orca-mini-v3-7b
reports the shortest average latency in generating responses, whereas
LLaMa2-7b-chat by Meta reports the highest average latency. This research
underscores the fact that open-source LLMs, too, can go hand in hand with
proprietary models like GPT-3.5 with better infrastructure.

</details>


### [177] [How to Evaluate Speech Translation with Source-Aware Neural MT Metrics](https://arxiv.org/abs/2511.03295)
*Mauro Cettolo,Marco Gaido,Matteo Negri,Sara Papi,Luisa Bentivogli*

Main category: cs.CL

TL;DR: 本文对语音翻译的源感知指标进行系统研究，探索生成音频文本代理的策略，提出跨语言重分割算法，实验表明不同策略效果及算法作用。


<details>
  <summary>Details</summary>
Motivation: 现有语音翻译自动评估方法忽略源输入信息，且将机器翻译中结合源文本的思路拓展到语音翻译存在挑战。

Method: 探索自动语音识别转录和参考翻译的反向翻译两种生成音频文本代理的策略，引入两步跨语言重分割算法解决合成源与参考翻译的对齐不匹配问题。

Result: 在两个语音翻译基准测试上实验，词错误率低于20%时，自动语音识别转录比反向翻译更可靠，反向翻译计算成本低且有效；跨语言重分割算法能让源感知机器翻译指标在语音翻译评估中稳健使用。

Conclusion: 为语音翻译更准确、有原则的评估方法铺平道路。

Abstract: Automatic evaluation of speech-to-text translation (ST) systems is typically
performed by comparing translation hypotheses with one or more reference
translations. While effective to some extent, this approach inherits the
limitation of reference-based evaluation that ignores valuable information from
the source input. In machine translation (MT), recent progress has shown that
neural metrics incorporating the source text achieve stronger correlation with
human judgments. Extending this idea to ST, however, is not trivial because the
source is audio rather than text, and reliable transcripts or alignments
between source and references are often unavailable. In this work, we conduct
the first systematic study of source-aware metrics for ST, with a particular
focus on real-world operating conditions where source transcripts are not
available. We explore two complementary strategies for generating textual
proxies of the input audio, automatic speech recognition (ASR) transcripts, and
back-translations of the reference translation, and introduce a novel two-step
cross-lingual re-segmentation algorithm to address the alignment mismatch
between synthetic sources and reference translations. Our experiments, carried
out on two ST benchmarks covering 79 language pairs and six ST systems with
diverse architectures and performance levels, show that ASR transcripts
constitute a more reliable synthetic source than back-translations when word
error rate is below 20%, while back-translations always represent a
computationally cheaper but still effective alternative. Furthermore, our
cross-lingual re-segmentation algorithm enables robust use of source-aware MT
metrics in ST evaluation, paving the way toward more accurate and principled
evaluation methodologies for speech translation.

</details>


### [178] [Benchmarking the Thinking Mode of Multimodal Large Language Models in Clinical Tasks](https://arxiv.org/abs/2511.03328)
*Jindong Hong,Tianjie Chen,Lingjie Luo,Chuanyang Zheng,Ting Xu,Haibao Yu,Jianing Qiu,Qianzhong Chen,Suning Huang,Yan Xu,Yong Gui,Yijun He,Jiankai Sun*

Main category: cs.CL

TL;DR: 评估推理多模态大语言模型（MLLMs）在医学任务中思考模式对性能的影响，发现多数任务中思考模式提升有限，复杂医学任务表现欠佳。


<details>
  <summary>Details</summary>
Motivation: 随着“双状态”MLLMs的快速发展，评估其增强推理过程对临床任务中模型性能和可靠性的影响。

Method: 评估Seed1.5 - VL和Gemini - 2.5 - Flash两个领先MLLMs在思考模式下的能力，用VQA - RAD和ROCOv2数据集对四个视觉医学任务进行评估。

Result: 多数任务中，激活思考模式相比非思考模式提升有限，复杂医学任务表现欠佳。

Conclusion: 需要特定领域医学数据和更先进的医学知识整合方法。

Abstract: A recent advancement in Multimodal Large Language Models (MLLMs) research is
the emergence of "reasoning MLLMs" that offer explicit control over their
internal thinking processes (normally referred as the "thinking mode")
alongside the standard "non-thinking mode". This capability allows these models
to engage in a step-by-step process of internal deliberation before generating
a final response. With the rapid transition to and adoption of these
"dual-state" MLLMs, this work rigorously evaluated how the enhanced reasoning
processes of these MLLMs impact model performance and reliability in clinical
tasks. This paper evaluates the active "thinking mode" capabilities of two
leading MLLMs, Seed1.5-VL and Gemini-2.5-Flash, for medical applications. We
assessed their performance on four visual medical tasks using VQA-RAD and
ROCOv2 datasets. Our findings reveal that the improvement from activating the
thinking mode remains marginal compared to the standard non-thinking mode for
the majority of the tasks. Their performance on complex medical tasks such as
open-ended VQA and medical image interpretation remains suboptimal,
highlighting the need for domain-specific medical data and more advanced
methods for medical knowledge integration.

</details>


### [179] [Automatic Machine Translation Detection Using a Surrogate Multilingual Translation Model](https://arxiv.org/abs/2511.02958)
*Cristian García-Romero,Miquel Esplà-Gomis,Felipe Sánchez-Martínez*

Main category: cs.CL

TL;DR: 提出利用代理多语言MT模型内部表示区分人机翻译句子的方法，实验显示该方法优于现有技术，尤其对非英语语言对。


<details>
  <summary>Details</summary>
Motivation: 现代机器翻译系统依赖含大量机器生成翻译文本的平行语料库，过度依赖这类合成内容会降低翻译质量，因此需过滤非人类翻译。

Method: 直接利用代理多语言MT模型的内部表示来区分人类和机器翻译的句子。

Result: 该方法优于当前最先进技术，尤其对非英语语言对，准确率至少提高5个百分点。

Conclusion: 所提方法在区分人机翻译句子上效果良好，可作为构建高质量MT系统的有效预处理步骤。

Abstract: Modern machine translation (MT) systems depend on large parallel corpora,
often collected from the Internet. However, recent evidence indicates that (i)
a substantial portion of these texts are machine-generated translations, and
(ii) an overreliance on such synthetic content in training data can
significantly degrade translation quality. As a result, filtering out non-human
translations is becoming an essential pre-processing step in building
high-quality MT systems. In this work, we propose a novel approach that
directly exploits the internal representations of a surrogate multilingual MT
model to distinguish between human and machine-translated sentences.
Experimental results show that our method outperforms current state-of-the-art
techniques, particularly for non-English language pairs, achieving gains of at
least 5 percentage points of accuracy.

</details>


### [180] [Generative Artificial Intelligence in Bioinformatics: A Systematic Review of Models, Applications, and Methodological Advances](https://arxiv.org/abs/2511.03354)
*Riasad Alvi,Sayeem Been Zaman,Wasimul Karim,Arefin Ittesafun Abian,Mohaimenul Azam Khan Raiaan,Saddam Mukta,Md Rafi Ur Rashid,Md Rafiqul Islam,Yakub Sebastian,Sami Azam*

Main category: cs.CL

TL;DR: 本文围绕生成式人工智能在生物信息学中的应用展开，提出六个研究问题进行系统评估，分析其优势、局限并指明未来方向。


<details>
  <summary>Details</summary>
Motivation: 系统识别和评估生成式人工智能在生物信息学领域的发展。

Method: 根据系统评价和荟萃分析方法的首选报告项目，提出六个研究问题。

Result: 各研究问题揭示了生成式人工智能在多方面优于传统方法、专用模型表现更好、在生物信息学领域有显著益处等成果，也指出了缺乏可扩展性和数据偏差等局限。

Conclusion: 肯定了生成式人工智能在生物信息学中的应用潜力，同时强调需进行稳健评估和基于生物学的建模以解决局限。

Abstract: Generative artificial intelligence (GenAI) has become a transformative
approach in bioinformatics that often enables advancements in genomics,
proteomics, transcriptomics, structural biology, and drug discovery. To
systematically identify and evaluate these growing developments, this review
proposed six research questions (RQs), according to the preferred reporting
items for systematic reviews and meta-analysis methods. The objective is to
evaluate impactful GenAI strategies in methodological advancement, predictive
performance, and specialization, and to identify promising approaches for
advanced modeling, data-intensive discovery, and integrative biological
analysis. RQ1 highlights diverse applications across multiple bioinformatics
subfields (sequence analysis, molecular design, and integrative data modeling),
which demonstrate superior performance over traditional methods through pattern
recognition and output generation. RQ2 reveals that adapted specialized model
architectures outperformed general-purpose models, an advantage attributed to
targeted pretraining and context-aware strategies. RQ3 identifies significant
benefits in the bioinformatics domains, focusing on molecular analysis and data
integration, which improves accuracy and reduces errors in complex analysis.
RQ4 indicates improvements in structural modeling, functional prediction, and
synthetic data generation, validated by established benchmarks. RQ5 suggests
the main constraints, such as the lack of scalability and biases in data that
impact generalizability, and proposes future directions focused on robust
evaluation and biologically grounded modeling. RQ6 examines that molecular
datasets (such as UniProtKB and ProteinNet12), cellular datasets (such as
CELLxGENE and GTEx) and textual resources (such as PubMedQA and OMIM) broadly
support the training and generalization of GenAI models.

</details>


### [181] [Data-Efficient Adaptation and a Novel Evaluation Method for Aspect-based Sentiment Analysis](https://arxiv.org/abs/2511.03034)
*Yan Cathy Hua,Paul Denny,Jörg Wicker,Katerina Taškova*

Main category: cs.CL

TL;DR: 本文针对ABSA在低资源领域研究不足、评估方法僵化等问题，提出新评估方法FTS - OBP，研究小模型在教育领域ABSA表现并提出调优策略，发布教育领域ABSA资源。


<details>
  <summary>Details</summary>
Motivation: ABSA研究和资源集中在商业领域，低资源领域需求未满足，且传统评估方法过于僵化。

Method: 提出FTS - OBP评估方法；对小的仅解码器生成语言模型进行教育领域ABSA研究，探索数据免费和轻数据微调方法并提出多任务微调策略；发布教育领域ABSA资源。

Result: FTS - OBP评估方法能适应边界变化且与传统指标强相关；多任务微调策略显著提升小模型性能，使小模型超越专有大模型并接近基准结果。

Conclusion: 新评估方法和小模型调优策略有助于推动低资源领域ABSA研究，发布的资源能支持后续相关研究。

Abstract: Aspect-based Sentiment Analysis (ABSA) is a fine-grained opinion mining
approach that identifies and classifies opinions associated with specific
entities (aspects) or their categories within a sentence. Despite its rapid
growth and broad potential, ABSA research and resources remain concentrated in
commercial domains, leaving analytical needs unmet in high-demand yet
low-resource areas such as education and healthcare. Domain adaptation
challenges and most existing methods' reliance on resource-intensive
in-training knowledge injection further hinder progress in these areas.
Moreover, traditional evaluation methods based on exact matches are overly
rigid for ABSA tasks, penalising any boundary variations which may misrepresent
the performance of generative models. This work addresses these gaps through
three contributions: 1) We propose a novel evaluation method, Flexible Text
Similarity Matching and Optimal Bipartite Pairing (FTS-OBP), which accommodates
realistic extraction boundary variations while maintaining strong correlation
with traditional metrics and offering fine-grained diagnostics. 2) We present
the first ABSA study of small decoder-only generative language models (SLMs;
<7B parameters), examining resource lower bounds via a case study in education
review ABSA. We systematically explore data-free (in-context learning and
weight merging) and data-light fine-tuning methods, and propose a multitask
fine-tuning strategy that significantly enhances SLM performance, enabling
1.5-3.8 B models to surpass proprietary large models and approach benchmark
results with only 200-1,000 examples on a single GPU. 3) We release the first
public set of education review ABSA resources to support future research in
low-resource domains.

</details>


### [182] [CareMedEval dataset: Evaluating Critical Appraisal and Reasoning in the Biomedical Field](https://arxiv.org/abs/2511.03441)
*Doria Bonzi,Alexandre Guiggi,Frédéric Béchet,Carlos Ramisch,Benoit Favre*

Main category: cs.CL

TL;DR: 介绍CareMedEval数据集评估大语言模型在生物医学批判性评估和推理任务上的表现，揭示模型局限性，为未来发展铺路。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在生物医学领域批判性评估任务的可靠性有限，需评估其在该领域的表现。

Method: 创建源于法国医学生真实考试的CareMedEval数据集，包含534个基于37篇科学文章的问题，在不同上下文条件下对通用和生物医学专业大语言模型进行基准测试。

Result: 任务难度大，开放和商业模型精确匹配率未超0.5，生成中间推理标记可改善结果，但模型在研究局限性和统计分析问题上仍面临挑战。

Conclusion: CareMedEval为基于证据的推理提供了具有挑战性的基准，揭示了当前大语言模型的局限性，为批判性评估的自动化支持的未来发展奠定基础。

Abstract: Critical appraisal of scientific literature is an essential skill in the
biomedical field. While large language models (LLMs) can offer promising
support in this task, their reliability remains limited, particularly for
critical reasoning in specialized domains. We introduce CareMedEval, an
original dataset designed to evaluate LLMs on biomedical critical appraisal and
reasoning tasks. Derived from authentic exams taken by French medical students,
the dataset contains 534 questions based on 37 scientific articles. Unlike
existing benchmarks, CareMedEval explicitly evaluates critical reading and
reasoning grounded in scientific papers. Benchmarking state-of-the-art
generalist and biomedical-specialized LLMs under various context conditions
reveals the difficulty of the task: open and commercial models fail to exceed
an Exact Match Rate of 0.5 even though generating intermediate reasoning tokens
considerably improves the results. Yet, models remain challenged especially on
questions about study limitations and statistical analysis. CareMedEval
provides a challenging benchmark for grounded reasoning, exposing current LLM
limitations and paving the way for future development of automated support for
critical appraisal.

</details>


### [183] [PolyNorm: Few-Shot LLM-Based Text Normalization for Text-to-Speech](https://arxiv.org/abs/2511.03080)
*Michel Wong,Ali Alshehri,Sophia Kao,Haotian He*

Main category: cs.CL

TL;DR: 提出基于大语言模型的文本归一化方法PolyNorm及语言无关的数据整理评估流程，多语言实验降低WER并发布数据集。


<details>
  <summary>Details</summary>
Motivation: 传统文本归一化系统工程工作量大、难扩展、语言覆盖有挑战，尤其是低资源场景。

Method: 提出基于提示的PolyNorm方法，使用大语言模型；设计语言无关的自动数据整理和评估流程。

Result: 在八种语言的实验中，与基于生产级的系统相比，词错误率（WER）持续降低。

Conclusion: PolyNorm减少对手工规则的依赖，能以最少人工干预实现更广泛的语言适用性，发布的数据集支持进一步研究。

Abstract: Text Normalization (TN) is a key preprocessing step in Text-to-Speech (TTS)
systems, converting written forms into their canonical spoken equivalents.
Traditional TN systems can exhibit high accuracy, but involve substantial
engineering effort, are difficult to scale, and pose challenges to language
coverage, particularly in low-resource settings. We propose PolyNorm, a
prompt-based approach to TN using Large Language Models (LLMs), aiming to
reduce the reliance on manually crafted rules and enable broader linguistic
applicability with minimal human intervention. Additionally, we present a
language-agnostic pipeline for automatic data curation and evaluation, designed
to facilitate scalable experimentation across diverse languages. Experiments
across eight languages show consistent reductions in the word error rate (WER)
compared to a production-grade-based system. To support further research, we
release PolyNorm-Benchmark, a multilingual data set covering a diverse range of
text normalization phenomena.

</details>


### [184] [SOLVE-Med: Specialized Orchestration for Leading Vertical Experts across Medical Specialties](https://arxiv.org/abs/2511.03542)
*Roberta Di Marino,Giovanni Dioguardi,Antonio Romano,Giuseppe Riccio,Mariano Barone,Marco Postiglione,Flora Amato,Vincenzo Moscato*

Main category: cs.CL

TL;DR: 提出多智能体架构SOLVE - Med处理复杂医疗查询，在意大利医疗论坛数据上表现优且可本地部署，代码开源。


<details>
  <summary>Details</summary>
Motivation: 解决医疗问答系统部署时存在的幻觉、偏差、计算需求、隐私问题及跨领域专业知识需求等挑战。

Method: 采用多智能体架构，包含路由智能体动态选择专家、十个特定医疗领域微调的小语言模型（各10亿参数）和编排智能体合成回答。

Result: 在意大利医疗论坛十个专业数据上评估，ROUGE - 1为0.301，BERTScore F1为0.697，优于最高140亿参数的独立模型，可本地部署。

Conclusion: SOLVE - Med是解决复杂医疗查询的有效方案，代码开源利于后续研究。

Abstract: Medical question answering systems face deployment challenges including
hallucinations, bias, computational demands, privacy concerns, and the need for
specialized expertise across diverse domains. Here, we present SOLVE-Med, a
multi-agent architecture combining domain-specialized small language models for
complex medical queries. The system employs a Router Agent for dynamic
specialist selection, ten specialized models (1B parameters each) fine-tuned on
specific medical domains, and an Orchestrator Agent that synthesizes responses.
Evaluated on Italian medical forum data across ten specialties, SOLVE-Med
achieves superior performance with ROUGE-1 of 0.301 and BERTScore F1 of 0.697,
outperforming standalone models up to 14B parameters while enabling local
deployment. Our code is publicly available on GitHub:
https://github.com/PRAISELab-PicusLab/SOLVE-Med.

</details>


### [185] [MultiZebraLogic: A Multilingual Logical Reasoning Benchmark](https://arxiv.org/abs/2511.03553)
*Sofie Helene Bruun,Dan Saattrup Smart*

Main category: cs.CL

TL;DR: 创建多语言斑马谜题数据集用于评估大语言模型逻辑推理能力，发布数据集和代码。


<details>
  <summary>Details</summary>
Motivation: 创建大规模、高质量数据集，用于比较多种语言下大语言模型的逻辑推理能力。

Method: 生成多语言、多主题、不同大小及包含多种线索类型和干扰项的斑马谜题。

Result: 2x3和4x5谜题对不同模型有足够挑战性；5个干扰项使o3 - mini在4x5谜题准确率下降；语言和主题对o3 - mini分数无显著影响；难度与线索类型无关联。

Conclusion: 发布九种日耳曼语言的MultiZebraLogic数据集和可适配更多语言与主题的谜题生成代码。

Abstract: Measuring the full abilities of large language models (LLMs) requires
benchmarks representing multiple tasks. We aim to create large, high-quality
datasets for comparison of logical reasoning skills across several languages
and of suitable difficulty for LLMs of various reasoning ability. We explore
multiple ways of increasing difficulty. We generate zebra puzzles in multiple
languages, themes, sizes and including 14 different clue types and 8 red
herring types (uninformative clues). We find puzzle sizes 2x3 and 4x5 are
sufficiently challenging for GPT-4o mini (a non-reasoning model) and o3-mini (a
reasoning model), respectively. Including 5 red herrings decreases o3-mini
puzzle-level accuracy on 4x5 puzzles by 15$\pm$7 %. Scores of o3-mini on 4x5
puzzles are not significantly affected by use of English vs. Danish or the
common houses theme vs. the country-specific smoerrebroed theme. We find no
correlation between difficulty and the selected clue types. Datasets of
128+1024 puzzles are published as MultiZebraLogic in each of nine Germanic
languages for sizes 2x3 and 4x5. We publish code for puzzle generation,
designed for adaptablity into more languages and themes.

</details>


### [186] [AILA--First Experiments with Localist Language Models](https://arxiv.org/abs/2511.03559)
*Joachim Diederich*

Main category: cs.CL

TL;DR: 本文首次实证展示了Transformer语言模型中可控局部性，通过实验表明局部主义配置能降低注意力熵，中间局部性值可优化可解释性与性能的权衡，局部主义语言模型适用于有透明度和能力要求的领域。


<details>
  <summary>Details</summary>
Motivation: 传统语言模型仅依赖分布式表示，本文旨在实现对表示局部性程度的连续控制，在可解释的局部主义编码和高效的分布式表示之间动态插值，且无需重新训练模型。

Method: 在WikiText语料库上使用两层Transformer架构进行实验，系统地改变局部性参数λ从1.0（完全局部主义）到0.0（完全分布式）。

Result: 局部主义配置能显著降低注意力熵，λ = 1.0时为5.36比特，λ = 0.0时为7.18比特，同时指针保真度得分更高；中间局部性值λ = 0.6优化了可解释性和性能的权衡，测试困惑度为4.65，准确率为84.7%。

Conclusion: 局部主义语言模型为需要透明度和能力的受监管领域应用提供了实用框架，可通过明确的惩罚阈值和信息论设计原则对可解释性 - 性能谱进行精确数学控制。

Abstract: This paper presents the first empirical demonstration of controllable
locality in transformer language models, a novel architectural framework that
enables continuous control over the degree of representation localization
through a tunable locality dial parameter. Unlike traditional language models
that rely exclusively on distributed representations, our approach allows
dynamic interpolation between highly interpretable localist encodings and
efficient distributed representations without requiring model retraining. We
conducted experiments on the WikiText corpus using a two-layer transformer
architecture, systematically varying the locality parameter {\lambda} across
the full spectrum from 1.0 (fully localist) to 0.0 (fully distributed). Our
results demonstrate that localist configurations achieve dramatically lower
attention entropy, with {\lambda} = 1.0 yielding 5.36 bits compared to 7.18
bits at {\lambda} = 0.0, while maintaining substantially higher pointer
fidelity scores reflecting stronger alignment with rule-specified targets.
Prediction experiments reveal that intermediate locality values optimize the
tradeoff between interpretability and performance, with {\lambda} = 0.6
achieving test perplexity of 4.65 and accuracy of 84.7%. These findings
establish that localist language models provide a practical framework for
applications in regulated domains requiring both transparency and capability,
offering precise mathematical control over the interpretability-performance
spectrum through explicit penalty thresholds and information-theoretic design
principles.

</details>


### [187] [Step-Audio-EditX Technical Report](https://arxiv.org/abs/2511.03601)
*Chao Yan,Boyong Wu,Peng Yang,Pengfei Tan,Guoqiang Hu,Yuxin Zhang,Xiangyu,Zhang,Fei Tian,Xuerui Yang,Xiangyu Zhang,Daxin Jiang,Gang Yu*

Main category: cs.CL

TL;DR: 介绍首个基于LLM的开源音频模型Step - Audio - EditX，其具音频编辑和TTS能力，采用大间隔学习法，表现超竞品。


<details>
  <summary>Details</summary>
Motivation: 开发具有表现力和迭代音频编辑能力以及强大零样本TTS能力的开源音频模型。

Method: 仅利用大间隔合成数据，采用大间隔学习方法，不依赖基于嵌入的先验或辅助模块。

Result: Step - Audio - EditX在情感编辑和其他细粒度控制任务上超越MiniMax - 2.6 - hd和Doubao - Seed - TTS - 2.0。

Conclusion: Step - Audio - EditX是一个有创新性且性能优秀的音频模型。

Abstract: We present Step-Audio-EditX, the first open-source LLM-based audio model
excelling at expressive and iterative audio editing encompassing emotion,
speaking style, and paralinguistics alongside robust zero-shot text-to-speech
(TTS) capabilities.Our core innovation lies in leveraging only large-margin
synthetic data, which circumvents the need for embedding-based priors or
auxiliary modules. This large-margin learning approach enables both iterative
control and high expressivity across voices, and represents a fundamental pivot
from the conventional focus on representation-level disentanglement. Evaluation
results demonstrate that Step-Audio-EditX surpasses both MiniMax-2.6-hd and
Doubao-Seed-TTS-2.0 in emotion editing and other fine-grained control tasks.

</details>


### [188] [ChiMDQA: Towards Comprehensive Chinese Document QA with Fine-grained Evaluation](https://arxiv.org/abs/2511.03656)
*Jing Gao,Shutiao Luo,Yumeng Liu,Yuanming Li,Hongji Zeng*

Main category: cs.CL

TL;DR: 本文提出中文多文档问答数据集ChiMDQA，涵盖多领域长文档和高质量问答对，适用于多种NLP任务，还介绍了设计、构建和评估体系，代码和数据开源。


<details>
  <summary>Details</summary>
Motivation: 随着NLP技术发展，对高质量中文文档问答数据集需求增长，需构建适用数据集。

Method: 通过文档筛选和系统的问题设计方法，构建包含6个领域、6068个问答对且分为10个细分类别的数据集，同时建立评估体系。

Result: 构建了ChiMDQA数据集，保证了多样性和高质量，适用于多种NLP任务。

Conclusion: 该数据集为中文问答的未来研究和实际应用提供了坚实基础。

Abstract: With the rapid advancement of natural language processing (NLP) technologies,
the demand for high-quality Chinese document question-answering datasets is
steadily growing. To address this issue, we present the Chinese Multi-Document
Question Answering Dataset(ChiMDQA), specifically designed for downstream
business scenarios across prevalent domains including academic, education,
finance, law, medical treatment, and news. ChiMDQA encompasses long-form
documents from six distinct fields, consisting of 6,068 rigorously curated,
high-quality question-answer (QA) pairs further classified into ten
fine-grained categories. Through meticulous document screening and a systematic
question-design methodology, the dataset guarantees both diversity and high
quality, rendering it applicable to various NLP tasks such as document
comprehension, knowledge extraction, and intelligent QA systems. Additionally,
this paper offers a comprehensive overview of the dataset's design objectives,
construction methodologies, and fine-grained evaluation system, supplying a
substantial foundation for future research and practical applications in
Chinese QA. The code and data are available at:
https://anonymous.4open.science/r/Foxit-CHiMDQA/.

</details>


### [189] [Grounded Misunderstandings in Asymmetric Dialogue: A Perspectivist Annotation Scheme for MapTask](https://arxiv.org/abs/2511.03718)
*Nan Li,Albert Gatt,Massimo Poesio*

Main category: cs.CL

TL;DR: 引入透视主义标注方案分析HCRC MapTask语料库，研究协作对话中理解的形成、分歧和修复，发现统一词汇变体后完全误解罕见，框架为研究误解和评估模型能力提供资源和视角。


<details>
  <summary>Details</summary>
Motivation: 协作对话中不对称场景下参与者可能指不同实体却以为达成一致，需要研究理解的形成、分歧和修复。

Method: 为HCRC MapTask语料库引入透视主义标注方案，使用受方案约束的大语言模型标注流程获取标注的引用表达式并分析理解状态。

Result: 统一词汇变体后完全误解罕见，多样性差异会系统性导致分歧。

Conclusion: 该框架为研究有根据的误解和评估（视觉）大语言模型在协作对话中建模依赖视角的基础的能力提供资源和分析视角。

Abstract: Collaborative dialogue relies on participants incrementally establishing
common ground, yet in asymmetric settings they may believe they agree while
referring to different entities. We introduce a perspectivist annotation scheme
for the HCRC MapTask corpus (Anderson et al., 1991) that separately captures
speaker and addressee grounded interpretations for each reference expression,
enabling us to trace how understanding emerges, diverges, and repairs over
time. Using a scheme-constrained LLM annotation pipeline, we obtain 13k
annotated reference expressions with reliability estimates and analyze the
resulting understanding states. The results show that full misunderstandings
are rare once lexical variants are unified, but multiplicity discrepancies
systematically induce divergences, revealing how apparent grounding can mask
referential misalignment. Our framework provides both a resource and an
analytic lens for studying grounded misunderstanding and for evaluating
(V)LLMs' capacity to model perspective-dependent grounding in collaborative
dialogue.

</details>


### [190] [BanglaSTEM: A Parallel Corpus for Technical Domain Bangla-English Translation](https://arxiv.org/abs/2511.03498)
*Kazi Reyazul Hasan,Mubasshira Musarrat,A. B. M. Alim Al Islam,Muhammad Abdullah Adnan*

Main category: cs.CL

TL;DR: 现有孟加拉语 - 英语翻译系统处理技术术语不佳，提出BanglaSTEM数据集训练T5翻译模型，提升技术内容翻译准确性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型处理英语技术问题表现好，但处理孟加拉语问题差，现有孟加拉语 - 英语翻译系统对技术术语翻译不佳。

Method: 从STEM领域选取5000个孟加拉语 - 英语句子对构建BanglaSTEM数据集，用语言模型生成超12000个翻译，人工筛选高质量对，在数据集上训练T5翻译模型并测试。

Result: 技术内容翻译准确性显著提升，方便孟加拉语使用者有效使用英语大语言模型。

Conclusion: BanglaSTEM数据集和训练的翻译模型有助于解决孟加拉语技术问题处理不佳的问题，且二者已公开。

Abstract: Large language models work well for technical problem solving in English but
perform poorly when the same questions are asked in Bangla. A simple solution
would be to translate Bangla questions into English first and then use these
models. However, existing Bangla-English translation systems struggle with
technical terms. They often mistranslate specialized vocabulary, which changes
the meaning of the problem and leads to wrong answers. We present BanglaSTEM, a
dataset of 5,000 carefully selected Bangla-English sentence pairs from STEM
fields including computer science, mathematics, physics, chemistry, and
biology. We generated over 12,000 translations using language models and then
used human evaluators to select the highest quality pairs that preserve
technical terminology correctly. We train a T5-based translation model on
BanglaSTEM and test it on two tasks: generating code and solving math problems.
Our results show significant improvements in translation accuracy for technical
content, making it easier for Bangla speakers to use English-focused language
models effectively. Both the BanglaSTEM dataset and the trained translation
model are publicly released at https://huggingface.co/reyazul/BanglaSTEM-T5.

</details>


### [191] [Towards Transparent Stance Detection: A Zero-Shot Approach Using Implicit and Explicit Interpretability](https://arxiv.org/abs/2511.03635)
*Apoorva Upadhyaya,Wolfgang Nejdl,Marco Fisichella*

Main category: cs.CL

TL;DR: 提出新颖可解释零样本立场检测框架IRIS，通过隐式和显式理由实现可解释性，实验证明模型泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有零样本立场检测研究存在泛化性问题、文本与目标缺乏连贯性，依赖显式推理且解释不细致、未显式建模推理过程。

Method: 开发IRIS框架，基于文本序列隐式理解态度，基于语言度量显式理解态度，将立场检测视为信息检索排序任务。

Result: 在多个基准数据集上使用不同比例训练数据的实验证明了模型的泛化性。

Conclusion: 提出的架构和可解释设计使模型具有良好的泛化性。

Abstract: Zero-Shot Stance Detection (ZSSD) identifies the attitude of the post toward
unseen targets. Existing research using contrastive, meta-learning, or data
augmentation suffers from generalizability issues or lack of coherence between
text and target. Recent works leveraging large language models (LLMs) for ZSSD
focus either on improving unseen target-specific knowledge or generating
explanations for stance analysis. However, most of these works are limited by
their over-reliance on explicit reasoning, provide coarse explanations that
lack nuance, and do not explicitly model the reasoning process, making it
difficult to interpret the model's predictions. To address these issues, in our
study, we develop a novel interpretable ZSSD framework, IRIS. We provide an
interpretable understanding of the attitude of the input towards the target
implicitly based on sequences within the text (implicit rationales) and
explicitly based on linguistic measures (explicit rationales). IRIS considers
stance detection as an information retrieval ranking task, understanding the
relevance of implicit rationales for different stances to guide the model
towards correct predictions without requiring the ground-truth of rationales,
thus providing inherent interpretability. In addition, explicit rationales
based on communicative features help decode the emotional and cognitive
dimensions of stance, offering an interpretable understanding of the author's
attitude towards the given target. Extensive experiments on the benchmark
datasets of VAST, EZ-STANCE, P-Stance, and RFD using 50%, 30%, and even 10%
training data prove the generalizability of our model, benefiting from the
proposed architecture and interpretable design.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [192] [An extended reality-based framework for user risk training in urban built environment](https://arxiv.org/abs/2511.02837)
*Sotirios Konstantakos,Sotirios Asparagkathos,Moatasim Mahmoud,Stamatia Rizou,Enrico Quagliarini,Gabriele Bernardini*

Main category: cs.HC

TL;DR: 本文提出基于扩展现实（XR）的框架以改善城市建筑环境中的用户风险培训，介绍了实施步骤，强调其对城市风险培训的潜力。


<details>
  <summary>Details</summary>
Motivation: 在城市风险增加，特别是气候变化引发洪水风险的背景下，提高各利益相关者的风险意识和应对准备。

Method: 采用沉浸式XR技术模拟真实紧急场景，强调利益相关者参与开发，通过用户反馈和性能数据迭代优化，实施步骤包括用户流程映射、场景选择和性能评估。

Result: 以意大利塞尼加利亚的试点应用为例，凸显了XR技术改造城市风险培训的潜力。

Conclusion: XR技术可改变城市风险培训，促进应对城市灾害的准备和恢复文化。

Abstract: In the context of increasing urban risks, particularly from climate
change-induced flooding, this paper presents an extended Reality (XR)-based
framework to improve user risk training within urban built environments. The
framework is designed to improve risk awareness and preparedness among various
stakeholders, including citizens, local authorities, and emergency responders.
Using immersive XR technologies, the training experience simulates real-world
emergency scenarios, contributing to active participation and a deeper
understanding of potential hazards and especially for floods. The framework
highlights the importance of stakeholder participation in its development,
ensuring that training modules are customized to address the specific needs of
different user groups. The iterative approach of the framework supports ongoing
refinement through user feedback and performance data, thus improving the
overall effectiveness of risk training initiatives. This work outlines the
methodological phases involved in the framework's implementation, including i)
user flow mapping, ii) scenario selection, and iii) performance evaluation,
with a focus on the pilot application in Senigallia, Italy. The findings
underscore the potential of XR technologies to transform urban risk training,
promoting a culture of preparedness and resilience against urban hazards.

</details>


### [193] [Evaluating Generative AI as an Educational Tool for Radiology Resident Report Drafting](https://arxiv.org/abs/2511.02839)
*Antonio Verdone,Aidan Cardall,Fardeen Siddiqui,Motaz Nashawaty,Danielle Rigau,Youngjoon Kwon,Mira Yousef,Shalin Patel,Alex Kieturakis,Eric Kim,Laura Heacock,Beatriu Reig,Yiqiu Shen*

Main category: cs.HC

TL;DR: 研究评估HIPAA合规GPT - 4o系统对放射科住院医师乳腺影像报告的自动反馈效果，结果表明该系统能可靠识别关键错误，或可支持放射学教育。


<details>
  <summary>Details</summary>
Motivation: 放射科住院医师需及时个性化反馈以提升技能，但临床工作量大限制了上级医师提供指导，因此评估GPT - 4o系统的自动反馈能力。

Method: 分析5000对住院医师 - 上级医师报告对，用GPT - 4o识别常见错误并提供反馈；进行100对报告的读者研究，由医师和住院医师评估反馈，用百分比匹配评估一致性，Krippendorff's alpha测量读者间可靠性。

Result: 确定三种常见错误类型；GPT - 4o与上级医师共识一致性高；读者间可靠性有一定差异，用GPT - 4o替代人类读者不显著影响一致性；多数情况下GPT反馈被认为有帮助。

Conclusion: ChatGPT - 4o能可靠识别关键教育错误，可作为支持放射学教育的可扩展工具。

Abstract: Objective: Radiology residents require timely, personalized feedback to
develop accurate image analysis and reporting skills. Increasing clinical
workload often limits attendings' ability to provide guidance. This study
evaluates a HIPAA-compliant GPT-4o system that delivers automated feedback on
breast imaging reports drafted by residents in real clinical settings.
  Methods: We analyzed 5,000 resident-attending report pairs from routine
practice at a multi-site U.S. health system. GPT-4o was prompted with clinical
instructions to identify common errors and provide feedback. A reader study
using 100 report pairs was conducted. Four attending radiologists and four
residents independently reviewed each pair, determined whether predefined error
types were present, and rated GPT-4o's feedback as helpful or not. Agreement
between GPT and readers was assessed using percent match. Inter-reader
reliability was measured with Krippendorff's alpha. Educational value was
measured as the proportion of cases rated helpful.
  Results: Three common error types were identified: (1) omission or addition
of key findings, (2) incorrect use or omission of technical descriptors, and
(3) final assessment inconsistent with findings. GPT-4o showed strong agreement
with attending consensus: 90.5%, 78.3%, and 90.4% across error types.
Inter-reader reliability showed moderate variability ({\alpha} = 0.767, 0.595,
0.567), and replacing a human reader with GPT-4o did not significantly affect
agreement ({\Delta} = -0.004 to 0.002). GPT's feedback was rated helpful in
most cases: 89.8%, 83.0%, and 92.0%.
  Discussion: ChatGPT-4o can reliably identify key educational errors. It may
serve as a scalable tool to support radiology education.

</details>


### [194] [Digital Transformation Chatbot (DTchatbot): Integrating Large Language Model-based Chatbot in Acquiring Digital Transformation Needs](https://arxiv.org/abs/2511.02842)
*Jiawei Zheng,Gokcen Yilmaz,Ji Han,Saeema Ahmed-Kristensen*

Main category: cs.HC

TL;DR: 本文探讨用大语言模型驱动的聊天机器人获取组织数字化转型需求，初步评估显示其按设计运行，最后讨论了其影响、潜力与局限。


<details>
  <summary>Details</summary>
Motivation: 传统方法获取组织数字化转型需求面临调度冲突、资源限制等挑战，需要新方法。

Method: 使用大语言模型驱动的聊天机器人，结合基于工作流的指令和大语言模型的规划推理能力，使其作为虚拟专家进行访谈，并详细介绍其功能和实现。

Result: 初步评估表明聊天机器人按设计运行，能有效遵循预定义工作流并支持用户交互，但有改进空间。

Conclusion: 讨论了使用聊天机器人获取用户信息的影响，强调其潜力和局限性。

Abstract: Many organisations pursue digital transformation to enhance operational
efficiency, reduce manual efforts, and optimise processes by automation and
digital tools. To achieve this, a comprehensive understanding of their unique
needs is required. However, traditional methods, such as expert interviews,
while effective, face several challenges, including scheduling conflicts,
resource constraints, inconsistency, etc. To tackle these issues, we
investigate the use of a Large Language Model (LLM)-powered chatbot to acquire
organisations' digital transformation needs. Specifically, the chatbot
integrates workflow-based instruction with LLM's planning and reasoning
capabilities, enabling it to function as a virtual expert and conduct
interviews. We detail the chatbot's features and its implementation. Our
preliminary evaluation indicates that the chatbot performs as designed,
effectively following predefined workflows and supporting user interactions
with areas for improvement. We conclude by discussing the implications of
employing chatbots to elicit user information, emphasizing their potential and
limitations.

</details>


### [195] [A Survey of Driver Distraction and Inattention in Popular Commercial Software-Defined Vehicles](https://arxiv.org/abs/2511.02891)
*Lingyu Zhao,Yuankai He*

Main category: cs.HC

TL;DR: 本文探讨软件定义汽车（SDV）中用户界面（UI）设计对驾驶员分心和注意力不集中的影响，通过调查提出需平衡软件功能与驾驶员认知人体工程学的UI设计。


<details>
  <summary>Details</summary>
Motivation: 随着SDV发展，UI设计对驾驶员安全至关重要，但现有SDV的UI实现大多未考虑驾驶分心和注意力不集中问题。

Method: 对流行商用车进行调查，识别可能增加认知负荷的UI特征并评估缓解风险的设计策略。

Result: 发现现有UI设计存在问题，强调需要平衡先进软件功能和驾驶员认知人体工程学的UI设计。

Conclusion: 研究结果为汽车UI领域的研究人员和原始设备制造商提供有价值的指导，有助于提升软件主导汽车时代的车辆安全性。

Abstract: As the automotive industry embraces software-defined vehicles (SDVs), the
role of user interface (UI) design in ensuring driver safety has become
increasingly significant. In crashes related to distracted driving, over 90%
did not involve cellphone use but were related to UI controls. However, many of
the existing UI SDV implementations do not consider Drive Distraction and
Inattention (DDI), which is reflected in many popular commercial vehicles. This
paper investigates the impact of UI designs on driver distraction and
inattention within the context of SDVs. Through a survey of popular commercial
vehicles, we identify UI features that potentially increase cognitive load and
evaluate design strategies to mitigate these risks. This survey highlights the
need for UI designs that balance advanced software functionalities with
driver-cognitive ergonomics. Findings aim to provide valuable guidance to
researchers and OEMs to contribute to the field of automotive UI, contributing
to the broader discussion on enhancing vehicular safety in the software-centric
automotive era.

</details>


### [196] [Systematizing LLM Persona Design: A Four-Quadrant Technical Taxonomy for AI Companion Applications](https://arxiv.org/abs/2511.02979)
*Esther Sun,Zichu Wu*

Main category: cs.HC

TL;DR: 本文针对基于大语言模型的AI陪伴角色设计与应用领域缺乏统一框架的问题，提出四象限技术分类法，为研究者、开发者和政策制定者提供帮助。


<details>
  <summary>Details</summary>
Motivation: 该领域发展迅速但碎片化，目标、模态和技术栈多样，急需统一框架。

Method: 提出基于“虚拟与具身”“情感陪伴与功能增强”两个关键轴的四象限技术分类法。

Result: 分析了四个象限不同应用场景下的挑战和关键技术。

Conclusion: 该分类法为研究者和开发者提供设计指引，为政策制定者识别和应对不同场景风险提供依据。

Abstract: The design and application of LLM-based personas in AI companionship is a
rapidly expanding but fragmented field, spanning from virtual emotional
companions and game NPCs to embodied functional robots. This diversity in
objectives, modality, and technical stacks creates an urgent need for a unified
framework. To address this gap, this paper systematizes the field by proposing
a Four-Quadrant Technical Taxonomy for AI companion applications. The framework
is structured along two critical axes: Virtual vs. Embodied and Emotional
Companionship vs. Functional Augmentation. Quadrant I (Virtual Companionship)
explores virtual idols, romantic companions, and story characters, introducing
a four-layer technical framework to analyze their challenges in maintaining
long-term emotional consistency. Quadrant II (Functional Virtual Assistants)
analyzes AI applications in work, gaming, and mental health, highlighting the
shift from "feeling" to "thinking and acting" and pinpointing key technologies
like enterprise RAG and on-device inference. Quadrants III & IV (Embodied
Intelligence) shift from the virtual to the physical world, analyzing home
robots and vertical-domain assistants, revealing core challenges in symbol
grounding, data privacy, and ethical liability. This taxonomy provides not only
a systematic map for researchers and developers to navigate the complex persona
design space but also a basis for policymakers to identify and address the
unique risks inherent in different application scenarios.

</details>


### [197] [From Measurement to Expertise: Empathetic Expert Adapters for Context-Based Empathy in Conversational AI Agents](https://arxiv.org/abs/2511.03143)
*Erfan Shayegani,Jina Suh,Andy Wilson,Nagu Rangan,Javier Hernandez*

Main category: cs.HC

TL;DR: 本文介绍开发和评估特定上下文移情大语言模型的框架，缩小感知与期望移情差距，适配器效果佳。


<details>
  <summary>Details</summary>
Motivation: 当前对话式AI模型的移情表达通常是通用的，未针对特定任务和上下文，需要开发特定上下文的移情大语言模型。

Method: 分析真实对话数据集，开发合成多轮对话生成管道，引导回复符合移情模式，训练特定上下文的移情专家适配器。

Result: 感知与期望移情差距缩小72.66%，分数平均提高2.43倍，移情专家适配器在保持移情模式上效果优于系统提示。

Conclusion: 所提出的框架和训练的适配器能有效提升对话式AI在特定上下文的移情能力。

Abstract: Empathy is a critical factor in fostering positive user experiences in
conversational AI. While models can display empathy, it is often generic rather
than tailored to specific tasks and contexts. In this work, we introduce a
novel framework for developing and evaluating context-specific empathetic large
language models (LLMs). We first analyze a real-world conversational dataset
consisting of 672 multi-turn conversations across 8 tasks, revealing
significant differences in terms of expected and experienced empathy before and
after the conversations, respectively. To help minimize this gap, we develop a
synthetic multi-turn conversational generation pipeline and steer responses
toward our defined empathy patterns based on the context that more closely
matches users' expectations. We then train empathetic expert adapters for
context-specific empathy that specialize in varying empathy levels based on the
recognized task. Our empirical results demonstrate a significant gap reduction
of 72.66% between perceived and desired empathy with scores increasing by an
average factor of 2.43 as measured by our metrics and reward models.
Additionally, our trained empathetic expert adapters demonstrate superior
effectiveness in preserving empathy patterns throughout conversation turns,
outperforming system prompts, which tend to dramatically diminish in impact as
conversations lengthen.

</details>


### [198] [Node-Based Editing for Multimodal Generation of Text, Audio, Image, and Vide](https://arxiv.org/abs/2511.03227)
*Alexander Htet Kyaw,Lenin Ravindranath Sivalingam*

Main category: cs.HC

TL;DR: 本文提出基于节点的多模态内容生成叙事系统，展示其功能、效果，讨论局限并规划未来工作。


<details>
  <summary>Details</summary>
Motivation: 开发能有效生成多模态内容的叙事系统，支持创作者创作多模态叙事。

Method: 将故事表示为节点图，通过用户编辑和自然语言提示扩展、编辑节点；用任务选择代理处理不同生成任务；界面支持节点编辑等操作。

Result: 基于节点的编辑支持对叙事结构的控制和多模态内容的迭代生成，有自动故事大纲生成的定量结果和编辑工作流的定性观察。

Conclusion: 系统有一定效果，但存在可扩展性和一致性等局限，未来将朝着人在环和以用户为中心的创意AI工具发展。

Abstract: We present a node-based storytelling system for multimodal content
generation. The system represents stories as graphs of nodes that can be
expanded, edited, and iteratively refined through direct user edits and
natural-language prompts. Each node can integrate text, images, audio, and
video, allowing creators to compose multimodal narratives. A task selection
agent routes between specialized generative tasks that handle story generation,
node structure reasoning, node diagram formatting, and context generation. The
interface supports targeted editing of individual nodes, automatic branching
for parallel storylines, and node-based iterative refinement. Our results
demonstrate that node-based editing supports control over narrative structure
and iterative generation of text, images, audio, and video. We report
quantitative outcomes on automatic story outline generation and qualitative
observations of editing workflows. Finally, we discuss current limitations such
as scalability to longer narratives and consistency across multiple nodes, and
outline future work toward human-in-the-loop and user-centered creative AI
tools.

</details>


### [199] [When Generative Artificial Intelligence meets Extended Reality: A Systematic Review](https://arxiv.org/abs/2511.03282)
*Xinyu Ning,Yan Zhuo,Xian Wang,Chan-In Devin Sio,Lik-Hang Lee*

Main category: cs.HC

TL;DR: 本文系统回顾2023 - 2025年生成式AI在XR中的应用，总结应用领域和关键技术，指出研究趋势、差距并探索未来研究机会。


<details>
  <summary>Details</summary>
Motivation: 随着技术发展，生成式AI与XR结合有巨大潜力，需系统了解其应用情况。

Method: 通过PRISMA筛选，分析26篇文章。

Result: 总结了生成式AI在XR中的应用领域和关键技术实现，突出了近三年相关文章。

Conclusion: 指出当前趋势和研究差距，为未来生成式XR研究提供指导和信息。

Abstract: With the continuous advancement of technology, the application of generative
artificial intelligence (AI) in various fields is gradually demonstrating great
potential, particularly when combined with Extended Reality (XR), creating
unprecedented possibilities. This survey article systematically reviews the
applications of generative AI in XR, covering as much relevant literature as
possible from 2023 to 2025. The application areas of generative AI in XR and
its key technology implementations are summarised through PRISMA screening and
analysis of the final 26 articles. The survey highlights existing articles from
the last three years related to how XR utilises generative AI, providing
insights into current trends and research gaps. We also explore potential
opportunities for future research to further empower XR through generative AI,
providing guidance and information for future generative XR research.

</details>


### [200] [Inter-Agent Trust Models: A Comparative Study of Brief, Claim, Proof, Stake, Reputation and Constraint in Agentic Web Protocol Design-A2A, AP2, ERC-8004, and Beyond](https://arxiv.org/abs/2511.03434)
*Botao 'Amber' Hu,Helena Rong*

Main category: cs.HC

TL;DR: 文章对多智能体协议设计中的信任模型进行比较研究，分析各模型优缺点，评估相关协议，给出混合信任模型建议和设计指南。


<details>
  <summary>Details</summary>
Motivation: 随着“智能体网络”形成，信任从人为监督转向协议设计，但现有多智能体协议的信任假设缺乏研究。

Method: 对Brief、Claim、Proof等信任模型进行分析，评估A2A、AP2、ERC - 8004等协议，考量安全、隐私等指标。

Result: 单一信任机制不足，需以Proof和Stake为基础的默认无信任架构，结合Brief和Reputation。

Conclusion: 给出混合信任模型建议，提炼可操作的设计指南以构建安全、可互操作和可扩展的智能体经济。

Abstract: As the "agentic web" takes shape-billions of AI agents (often LLM-powered)
autonomously transacting and collaborating-trust shifts from human oversight to
protocol design. In 2025, several inter-agent protocols crystallized this
shift, including Google's Agent-to-Agent (A2A), Agent Payments Protocol (AP2),
and Ethereum's ERC-8004 "Trustless Agents," yet their underlying trust
assumptions remain under-examined. This paper presents a comparative study of
trust models in inter-agent protocol design: Brief (self- or third-party
verifiable claims), Claim (self-proclaimed capabilities and identity, e.g.
AgentCard), Proof (cryptographic verification, including zero-knowledge proofs
and trusted execution environment attestations), Stake (bonded collateral with
slashing and insurance), Reputation (crowd feedback and graph-based trust
signals), and Constraint (sandboxing and capability bounding). For each, we
analyze assumptions, attack surfaces, and design trade-offs, with particular
emphasis on LLM-specific fragilities-prompt injection,
sycophancy/nudge-susceptibility, hallucination, deception, and
misalignment-that render purely reputational or claim-only approaches brittle.
Our findings indicate no single mechanism suffices. We argue for
trustless-by-default architectures anchored in Proof and Stake to gate
high-impact actions, augmented by Brief for identity and discovery and
Reputation overlays for flexibility and social signals. We comparatively
evaluate A2A, AP2, ERC-8004 and related historical variations in academic
research under metrics spanning security, privacy, latency/cost, and social
robustness (Sybil/collusion/whitewashing resistance). We conclude with hybrid
trust model recommendations that mitigate reputation gaming and misinformed LLM
behavior, and we distill actionable design guidelines for safer, interoperable,
and scalable agent economies.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [201] [AI-Enhanced Wi-Fi Sensing Through Single Transceiver Pair](https://arxiv.org/abs/2511.02845)
*Yuxuan Liu,Chiya Zhang,Yifeng Yuan,Chunlong He,Weizheng Zhang,Gaojie Chen*

Main category: eess.SP

TL;DR: 研究发现硬件约束下AI给Wi-Fi传感系统带来性能提升源于先验信息和时间相关性，开发系统并实验验证。


<details>
  <summary>Details</summary>
Motivation: 下一代Wi-Fi技术依赖传感能力，现有研究未深入探究AI驱动感知技术超越雷达理论分辨率限制的理论基础。

Method: 开发基于AI的单收发对Wi-Fi传感系统，设计人体姿态估计和室内定位实验。

Result: 实验结果证实了时间相关性和先验信息带来的性能提升。

Conclusion: 硬件约束下，AI给Wi-Fi传感系统带来的性能提升主要源于先验信息和时间相关性。

Abstract: The advancement of next-generation Wi-Fi technology heavily relies on sensing
capabilities, which play a pivotal role in enabling sophisticated applications.
In response to the growing demand for large-scale deployments, contemporary
Wi-Fi sensing systems strive to achieve high-precision perception while
maintaining minimal bandwidth consumption and antenna count requirements.
Remarkably, various AI-driven perception technologies have demonstrated the
ability to surpass the traditional resolution limitations imposed by radar
theory. However, the theoretical underpinnings of this phenomenon have not been
thoroughly investigated in existing research. In this study, we found that
under hardware-constrained conditions, the performance gains brought by AI to
Wi-Fi sensing systems primarily originate from two aspects: prior information
and temporal correlation. Prior information enables the AI to generate
plausible details based on vague input, while temporal correlation helps reduce
the upper bound of sensing error. We developed an AI-based Wi-Fi sensing system
using a single transceiver pair and designed experiments focusing on human pose
estimation and indoor localization to validate the theoretical claims. The
results confirm the performance gains contributed by temporal correlation and
prior information.

</details>


### [202] [Spatio-Temporal Attention Network for Epileptic Seizure Prediction](https://arxiv.org/abs/2511.02846)
*Zan Li,Kyongmin Yeo,Wesley Gifford,Lara Marcuse,Madeline Fields,Bülent Yener*

Main category: eess.SP

TL;DR: 提出基于STAN的深度学习框架预测癫痫发作，在数据集上表现优于现有方法，能提前较长时间可靠检测发作前状态。


<details>
  <summary>Details</summary>
Motivation: 现有癫痫发作预测方法依赖特征工程且假设发作前持续时间固定，需要更好的方法。

Method: 通过Spatio - Temporal Attention Network（STAN）同时建模时空相关性，使用对抗判别器区分发作前和发作间期的注意力模式，实现患者特定学习。

Result: 在CHB - MIT数据集上灵敏度96.6%，误检率0.011/h；在MSSM数据集上灵敏度94.2%，误检率0.063/h，显著优于现有方法。

Conclusion: 该框架能至少提前15分钟可靠检测发作前状态，患者特定窗口可达45分钟，可为临床应用提供足够干预时间。

Abstract: In this study, we present a deep learning framework that learns complex
spatio-temporal correlation structures of EEG signals through a Spatio-Temporal
Attention Network (STAN) for accurate predictions of onset of seizures for
Epilepsy patients. Unlike existing methods, which rely on feature engineering
and/or assume fixed preictal durations, our approach simultaneously models
spatio-temporal correlations through STAN and employs an adversarial
discriminator to distinguish preictal from interictal attention patterns,
enabling patient-specific learning. Evaluation on CHB-MIT and MSSM datasets
demonstrates 96.6\% sensitivity with 0.011/h false detection rate on CHB-MIT,
and 94.2% sensitivity with 0.063/h FDR on MSSM, significantly outperforming
state-of-the-art methods. The framework reliably detects preictal states at
least 15 minutes before an onset, with patient-specific windows extending to 45
minutes, providing sufficient intervention time for clinical applications.

</details>


### [203] [EEGReXferNet: A Lightweight Gen-AI Framework for EEG Subspace Reconstruction via Cross-Subject Transfer Learning and Channel-Aware Embedding](https://arxiv.org/abs/2511.02848)
*Shantanu Sarkar,Piotr Nabrzyski,Saurabh Prasad,Jose Luis Contreras-Vidal*

Main category: eess.SP

TL;DR: 本文介绍了轻量级Gen - AI框架EEGReXferNet用于EEG子空间重建，可提升时空谱分辨率，减少权重，适用于实时应用。


<details>
  <summary>Details</summary>
Motivation: 传统脑电信号去伪迹方法需人工干预或会抑制关键神经特征，现有生成模型缺乏时空谱敏感性且计算量大，不适用于实时应用。

Method: 使用Keras TensorFlow开发EEGReXferNet，采用模块化架构，利用相邻通道体积传导、特定频段卷积编码和滑动窗口动态提取潜在特征，结合基于参考的缩放。

Result: 提高了时空谱分辨率（平均PSD相关性>=0.95；平均频谱图RV系数>=0.85），减少约45%的总权重。

Conclusion: EEGReXferNet可有效用于神经生理学和脑机接口应用中的鲁棒实时脑电预处理。

Abstract: Electroencephalography (EEG) is a widely used non-invasive technique for
monitoring brain activity, but low signal-to-noise ratios (SNR) due to various
artifacts often compromise its utility. Conventional artifact removal methods
require manual intervention or risk suppressing critical neural features during
filtering/reconstruction. Recent advances in generative models, including
Variational Autoencoders (VAEs) and Generative Adversarial Networks (GANs),
have shown promise for EEG reconstruction; however, these approaches often lack
integrated temporal-spectral-spatial sensitivity and are computationally
intensive, limiting their suitability for real-time applications like
brain-computer interfaces (BCIs). To overcome these challenges, we introduce
EEGReXferNet, a lightweight Gen-AI framework for EEG subspace reconstruction
via cross-subject transfer learning - developed using Keras TensorFlow
(v2.15.1). EEGReXferNet employs a modular architecture that leverages volume
conduction across neighboring channels, band-specific convolution encoding, and
dynamic latent feature extraction through sliding windows. By integrating
reference-based scaling, the framework ensures continuity across successive
windows and generalizes effectively across subjects. This design improves
spatial-temporal-spectral resolution (mean PSD correlation >= 0.95; mean
spectrogram RV-Coefficient >= 0.85), reduces total weights by ~45% to mitigate
overfitting, and maintains computational efficiency for robust, real-time EEG
preprocessing in neurophysiological and BCI applications.

</details>


### [204] [Approaching Low-Cost Cardiac Intelligence with Semi-Supervised Knowledge Distillation](https://arxiv.org/abs/2511.02851)
*Rushuang Zhou,Yuan-Ting Zhang,M. Jamal Deen,Yining Dong*

Main category: eess.SP

TL;DR: 提出LiteHeart框架以缩小低成本与高成本心脏智能诊断性能差距，在多数据集表现出色，提升低成本心脏智能诊断能力。


<details>
  <summary>Details</summary>
Motivation: 高级心脏人工智能依赖大量医疗数据和高计算资源，低成本心脏智能（LCCI）与高成本心脏智能（HCCI）存在诊断性能差距。

Method: 提出半监督知识蒸馏框架LiteHeart，引入区域感知蒸馏模块和跨层互信息模块，采用半监督训练策略。

Result: 在五个涵盖超38种心血管疾病的数据集上，显著缩小LCCI和HCCI性能差距，宏观F1分数比现有方法高4.27% - 7.10%。

Conclusion: LiteHeart显著提升低成本心脏智能系统诊断能力，为可扩展、经济且准确的日常心脏健康护理铺平道路。

Abstract: Deploying advanced cardiac artificial intelligence for daily cardiac
monitoring is hindered by its reliance on extensive medical data and high
computational resources. Low-cost cardiac intelligence (LCCI) offers a
promising alternative by using wearable device data, such as 1-lead
electrocardiogram (ECG), but it suffers from a significant diagnostic
performance gap compared to high-cost cardiac intelligence (HCCI). To bridge
this gap, we propose LiteHeart, a semi-supervised knowledge distillation
framework. LiteHeart introduces a region-aware distillation module to mimic how
cardiologists focus on diagnostically relevant ECG regions and a cross-layer
mutual information module to align the decision processes of LCCI and HCCI
systems. Using a semi-supervised training strategy, LiteHeart further improves
model robustness under limited supervision. Evaluated on five datasets covering
over 38 cardiovascular diseases, LiteHeart substantially reduces the
performance gap between LCCI and HCCI, outperforming existing methods by 4.27%
to 7.10% in macro F1 score. These results demonstrate that LiteHeart
significantly enhances the diagnostic capabilities of low-cost cardiac
intelligence systems, paving the way for scalable, affordable, and accurate
daily cardiac healthcare using wearable technologies.

</details>


### [205] [Consciousness-ECG Transformer for Conscious State Estimation System with Real-Time Monitoring](https://arxiv.org/abs/2511.02853)
*Young-Seok Kweon,Gi-Hwan Shin,Ji-Yong Kim,Bokyeong Ryu,Seong-Whan Lee*

Main category: eess.SP

TL;DR: 提出基于心电图信号的意识状态估计模型，在睡眠分期和麻醉水平监测任务上表现优于基线模型，为临床环境提供实用替代方案。


<details>
  <summary>Details</summary>
Motivation: 传统基于脑电图的意识状态估计方法存在对噪声敏感、需可控环境等问题，需要更可靠方法确保患者安全和优化健康结果。

Method: 提出意识 - 心电图变压器模型，采用解耦查询注意力的变压器有效捕捉心率变异性特征，实现实时监测的意识状态估计系统。

Result: 模型在睡眠分期和麻醉水平监测任务上准确率分别达0.877和0.880，曲线下面积分别达0.786和0.895，优于基线模型。

Conclusion: 该系统是基于脑电图方法的实用且稳健替代方案，心电图意识监测有潜力提高患者安全和加深对意识状态的理解。

Abstract: Conscious state estimation is important in various medical settings,
including sleep staging and anesthesia management, to ensure patient safety and
optimize health outcomes. Traditional methods predominantly utilize
electroencephalography (EEG), which faces challenges such as high sensitivity
to noise and the requirement for controlled environments. In this study, we
propose the consciousness-ECG transformer that leverages electrocardiography
(ECG) signals for non-invasive and reliable conscious state estimation. Our
approach employs a transformer with decoupled query attention to effectively
capture heart rate variability features that distinguish between conscious and
unconscious states. We implemented the conscious state estimation system with
real-time monitoring and validated our system on datasets involving sleep
staging and anesthesia level monitoring during surgeries. Experimental results
demonstrate that our model outperforms baseline models, achieving accuracies of
0.877 on sleep staging and 0.880 on anesthesia level monitoring. Moreover, our
model achieves the highest area under curve values of 0.786 and 0.895 on sleep
staging and anesthesia level monitoring, respectively. The proposed system
offers a practical and robust alternative to EEG-based methods, particularly
suited for dynamic clinical environments. Our results highlight the potential
of ECG-based consciousness monitoring to enhance patient safety and advance our
understanding of conscious states.

</details>


### [206] [NEF-NET+: Adapting Electrocardio panorama in the wild](https://arxiv.org/abs/2511.02880)
*Zehui Zhan,Yaojun Hu,Jiajing Zhan,Wanchen Lian,Wanqing Wu,Jintai Chen*

Main category: eess.SP

TL;DR: 传统多导联心电图系统有局限，Nef - Net有不足，本文提出NEF - NET+框架用于全景心电图合成，构建Panobench基准，实验显示NEF - NET+有显著提升。


<details>
  <summary>Details</summary>
Motivation: 传统多导联心电图系统不能提供非标准视角，Nef - Net存在理想化假设和实际挑战，需要改进以实现更真实的全景心电图合成。

Method: 设计新的模型架构进行直接视图转换，包含离线预训练、设备校准调整和针对患者的实时校准步骤；构建新的Electrocardio Panorama基准Panobench。

Result: 实验结果表明，在现实环境中，NEF - NET+比Nef - Net有显著改进，PSNR提高约6 dB。

Conclusion: NEF - NET+能支持任意长度信号合成，跨设备泛化并补偿电极放置偏差，优于Nef - Net。代码和Panobench将后续发布。

Abstract: Conventional multi-lead electrocardiogram (ECG) systems capture cardiac
signals from a fixed set of anatomical viewpoints defined by lead placement.
However, certain cardiac conditions (e.g., Brugada syndrome) require
additional, non-standard viewpoints to reveal diagnostically critical patterns
that may be absent in standard leads. To systematically overcome this
limitation, Nef-Net was recently introduced to reconstruct a continuous
electrocardiac field, enabling virtual observation of ECG signals from
arbitrary views (termed Electrocardio Panorama). Despite its promise, Nef-Net
operates under idealized assumptions and faces in-the-wild challenges, such as
long-duration ECG modeling, robustness to device-specific signal artifacts, and
suboptimal lead placement calibration. This paper presents NEF-NET+, an
enhanced framework for realistic panoramic ECG synthesis that supports
arbitrary-length signal synthesis from any desired view, generalizes across ECG
devices, and compensates for operator-induced deviations in electrode
placement. These capabilities are enabled by a newly designed model
architecture that performs direct view transformation, incorporating a workflow
comprising offline pretraining, device calibration tuning steps as well as an
on-the-fly calibration step for patient-specific adaptation. To rigorously
evaluate panoramic ECG synthesis, we construct a new Electrocardio Panorama
benchmark, called Panobench, comprising 5367 recordings with 48-view per
subject, capturing the full spatial variability of cardiac electrical activity.
Experimental results show that NEF-NET+ delivers substantial improvements over
Nef-Net, yielding an increase of around 6 dB in PSNR in real-world setting. The
code and Panobench will be released in a subsequent publication.

</details>


### [207] [From Narrow to Wide: Autoencoding Transformers for Ultrasound Bandwidth Recovery](https://arxiv.org/abs/2511.02938)
*Sepideh KhakzadGharamaleki,Hassan Rivaz,Brandon Helfield*

Main category: eess.SP

TL;DR: 通过学习数据驱动映射解决低成本探头窄带宽问题，提升超声成像性能。


<details>
  <summary>Details</summary>
Motivation: 传统脉冲回波超声在低成本探头窄带宽时表现不佳，丢失高频细节。

Method: 训练Tiny Vision Transform (ViT) 自编码器变种，使用课程加权损失在模拟数据上训练。

Result: 在异质散斑 - 囊肿体模上，网络使图像域MSE降低90%，PSNR提高6.7 dB，SSIM提升到0.965；能锐化未见过的分辨率体模中的点目标行。

Conclusion: 纯软件升级可让窄带探头获得类似宽带性能，有望在资源受限环境扩大高分辨率超声的使用。

Abstract: Conventional pulse-echo ultrasound suffers when low-cost probes deliver only
narrow fractional bandwidths, elongating pulses and erasing high-frequency
detail. We address this limitation by learning a data-driven mapping from
band-limited to broadband spectrogram of radio-frequency (RF) lines. To this
end, a variation of Tiny Vision Transform (ViT) auto-encoder is trained on
simulation data using a curriculum-weighted loss. On heterogeneous speckle-cyst
phantoms, the network reduces image-domain MSE by 90 percent, boosts PSNR by
6.7 dB, and raises SSIM to 0.965 compared with the narrow-band input. It also
sharpens point-target rows in a completely unseen resolution phantom,
demonstrating strong out-of-distribution generalisation without sacrificing
frame rate or phase information. These results indicate that a purely software
upgrade can endow installed narrow-band probes with broadband-like performance,
potentially widening access to high-resolution ultrasound in
resource-constrained settings.

</details>


### [208] [ECGXtract: Deep Learning-based ECG Feature Extraction for Automated CVD Diagnosis](https://arxiv.org/abs/2511.02850)
*Youssif Abuzied,Hassan AbdEltawab,Abdelrhman Gaber,Tamer ElBatt*

Main category: eess.SP

TL;DR: 提出基于深度学习的ECGXtract用于可解释心电图特征提取，实验表明其性能佳，探索多特征提取分组策略有潜力。


<details>
  <summary>Details</summary>
Motivation: 解决传统信号处理和黑盒机器学习方法在心电图特征提取上的局限。

Method: 开发卷积神经网络模型提取特征，先单特征训练，再多场景实验评估，探索多特征提取分组策略。

Result: ECGXtract多数特征表现稳健，全局特征与真值平均相关得分0.80，导联特异性特征平均相关得分0.822，优于ECGdeli；语义分组对全局特征有效，大规模分组和导联特异性多输出模型性能下降。

Conclusion: 结构化分组策略可平衡计算效率和模型准确性，为有限资源下的心电图特征提取系统提供方向。

Abstract: This paper presents ECGXtract, a deep learning-based approach for
interpretable ECG feature extraction, addressing the limitations of traditional
signal processing and black-box machine learning methods. In particular, we
develop convolutional neural network models capable of extracting both temporal
and morphological features with strong correlations to a clinically validated
ground truth. Initially, each model is trained to extract a single feature,
ensuring precise and interpretable outputs. A series of experiments is then
carried out to evaluate the proposed method across multiple setups, including
global versus lead-specific features, different sampling frequencies, and
comparisons with other approaches such as ECGdeli. Our findings show that
ECGXtract achieves robust performance across most features with a mean
correlation score of 0.80 with the ground truth for global features, with lead
II consistently providing the best results. For lead-specific features,
ECGXtract achieves a mean correlation score of 0.822. Moreover, ECGXtract
achieves superior results to the state-of-the-art open source ECGdeli as it got
a higher correlation score with the ground truth in 90% of the features.
Furthermore, we explore the feasibility of extracting multiple features
simultaneously utilizing a single model. Semantic grouping is proved to be
effective for global features, while large-scale grouping and lead-specific
multi-output models show notable performance drops. These results highlight the
potential of structured grouping strategies to balance the computational
efficiency vs. model accuracy, paving the way for more scalable and clinically
interpretable ECG feature extraction systems in limited resource settings.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [209] [Scheduling the Off-Diagonal Weingarten Loss of Neural SDFs for CAD Models](https://arxiv.org/abs/2511.03147)
*Haotian Yin,Przemyslaw Musialski*

Main category: cs.GR

TL;DR: 提出用于ODW损失的调度策略，在CAD重建中优于固定权重方法。


<details>
  <summary>Details</summary>
Motivation: FlatCAD在训练中使用固定ODW权重，早期优化稳定但后期细节恢复不佳，需改进。

Method: 研究常数、线性、五次、阶跃插值调度以及递增预热变体等时间变化调度策略。

Result: 在ABC CAD数据集实验表明，时间变化调度始终优于固定权重，Chamfer距离最多提升35%。

Conclusion: 调度策略是曲率正则化用于稳健CAD重建的简单有效扩展。

Abstract: Neural signed distance functions (SDFs) have become a powerful representation
for geometric reconstruction from point clouds, yet they often require both
gradient- and curvature-based regularization to suppress spurious warp and
preserve structural fidelity. FlatCAD introduced the Off-Diagonal Weingarten
(ODW) loss as an efficient second-order prior for CAD surfaces, approximating
full-Hessian regularization at roughly half the computational cost. However,
FlatCAD applies a fixed ODW weight throughout training, which is suboptimal:
strong regularization stabilizes early optimization but suppresses detail
recovery in later stages. We present scheduling strategies for the ODW loss
that assign a high initial weight to stabilize optimization and progressively
decay it to permit fine-scale refinement. We investigate constant, linear,
quintic, and step interpolation schedules, as well as an increasing warm-up
variant. Experiments on the ABC CAD dataset demonstrate that time-varying
schedules consistently outperform fixed weights. Our method achieves up to a
35% improvement in Chamfer Distance over the FlatCAD baseline, establishing
scheduling as a simple yet effective extension of curvature regularization for
robust CAD reconstruction.

</details>


### [210] [Visualization Biases MLLM's Decision Making in Network Data Tasks](https://arxiv.org/abs/2511.03617)
*Timo Brand,Henry Förster,Stephen G. Kobourov,Jacob Miller*

Main category: cs.GR

TL;DR: 研究可视化对MLLMs判断网络中桥梁存在与否的影响，发现可视化能提升信心但有偏差，使用需谨慎。


<details>
  <summary>Details</summary>
Motivation: 评估可视化对MLLMs判断网络中桥梁存在与否的影响。

Method: 对比可视化输入和结构化文本输入对MLLMs判断的影响。

Result: 可视化能提升MLLM判断信心，但标准可视化技术会产生判断偏差。

Conclusion: 从业者在生成式AI应用中使用可视化时需谨慎，避免产生幻觉。

Abstract: We evaluate how visualizations can influence the judgment of MLLMs about the
presence or absence of bridges in a network. We show that the inclusion of
visualization improves confidence over a structured text-based input that could
theoretically be helpful for answering the question. On the other hand, we
observe that standard visualization techniques create a strong bias towards
accepting or refuting the presence of a bridge -- independently of whether or
not a bridge actually exists in the network. While our results indicate that
the inclusion of visualization techniques can effectively influence the MLLM's
judgment without compromising its self-reported confidence, they also imply
that practitioners must be careful of allowing users to include visualizations
in generative AI applications so as to avoid undesired hallucinations.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [211] [The Contiguous Art Gallery Problem is in Θ(n log n)](https://arxiv.org/abs/2511.02960)
*Sarita de Berg,Jacobus Conradi,Ivor van der Hoog,Eva Rotenberg*

Main category: cs.CG

TL;DR: 本文针对连续艺术画廊问题提出O(n log n)时间算法，优于之前算法，并给出下界，证明该问题复杂度为Θ(n log n)。


<details>
  <summary>Details</summary>
Motivation: 之前连续艺术画廊问题的算法时间复杂度均约为O(k n^5 log n) 或O(n^6log n)，作者认为实际复杂度可能更低。

Method: 在实数随机存取机模型下设计O(n log n)时间算法，通过归约集合交集问题给出基于排序的下界。

Result: 提出O(n log n)时间算法，相比之前算法有O(k n^4)的加速。

Conclusion: 连续艺术画廊问题复杂度为Θ(n log n)。

Abstract: Recently, a natural variant of the Art Gallery problem, known as the
\emph{Contiguous Art Gallery problem} was proposed. Given a simple polygon $P$,
the goal is to partition its boundary $\partial P$ into the smallest number of
contiguous segments such that each segment is completely visible from some
point in $P$. Unlike the classical Art Gallery problem, which is NP-hard, this
variant is polynomial-time solvable. At SoCG~2025, three independent works
presented algorithms for this problem, each achieving a running time of $O(k
n^5 \log n)$ (or $O(n^6\log n)$), where $k$ is the size of an optimal solution.
Interestingly, these results were obtained using entirely different approaches,
yet all led to roughly the same asymptotic complexity, suggesting that such a
running time might be inherent to the problem.
  We show that this is not the case. In the real RAM-model, the prevalent model
in computational geometry, we present an $O(n \log n)$-time algorithm,
achieving an $O(k n^4)$ factor speed-up over the previous state-of-the-art. We
also give a straightforward sorting-based lower bound by reducing from the set
intersection problem. We thus show that the Contiguous Art Gallery problem is
in $\Theta(n \log n)$.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [212] [Computational Imaging Meets LLMs: Zero-Shot IDH Mutation Prediction in Brain Gliomas](https://arxiv.org/abs/2511.03376)
*Syed Muqeem Mahmood,Hassan Mohy-ud-Din*

Main category: eess.IV

TL;DR: 提出结合大语言模型与计算图像分析的框架用于脑胶质瘤IDH突变状态无创零样本预测，评估效果好，代码开源。


<details>
  <summary>Details</summary>
Motivation: 实现脑胶质瘤IDH突变状态的无创、零样本预测，改进神经肿瘤学诊断策略。

Method: 处理多参数MRI扫描和肿瘤分割图，提取特征存入JSON文件，用其查询GPT 4o和GPT 5，不进行微调。

Result: 在六个公开数据集上评估，准确性高、分类性能均衡，GPT 5表现更优，体积特征是重要预测因素。

Conclusion: 整合大语言模型推理与计算图像分析可实现精确无创肿瘤基因分型，推动神经肿瘤学诊断发展。

Abstract: We present a framework that combines Large Language Models with computational
image analytics for non-invasive, zero-shot prediction of IDH mutation status
in brain gliomas. For each subject, coregistered multi-parametric MRI scans and
multi-class tumor segmentation maps were processed to extract interpretable
semantic (visual) attributes and quantitative features, serialized in a
standardized JSON file, and used to query GPT 4o and GPT 5 without fine-tuning.
We evaluated this framework on six publicly available datasets (N = 1427) and
results showcased high accuracy and balanced classification performance across
heterogeneous cohorts, even in the absence of manual annotations. GPT 5
outperformed GPT 4o in context-driven phenotype interpretation. Volumetric
features emerged as the most important predictors, supplemented by
subtype-specific imaging markers and clinical information. Our results
demonstrate the potential of integrating LLM-based reasoning with computational
image analytics for precise, non-invasive tumor genotyping, advancing
diagnostic strategies in neuro-oncology. The code is available at
https://github.com/ATPLab-LUMS/CIM-LLM.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [213] [EGMOF: Efficient Generation of Metal-Organic Frameworks Using a Hybrid Diffusion-Transformer Architecture](https://arxiv.org/abs/2511.03122)
*Seunghee Han,Yeonghun Kang,Taeun Bae,Varinia Bernales,Alan Aspuru-Guzik,Jihan Kim*

Main category: cond-mat.mtrl-sci

TL;DR: 提出EGMOF框架用于MOFs逆向设计，在小数据条件下有效且表现优于现有方法，具有数据高效和可推广性。


<details>
  <summary>Details</summary>
Motivation: 化学空间大、带属性标签数据稀缺，现有生成模型用于逆向设计需大量数据且针对新目标属性要重新训练。

Method: 采用模块化、描述符介导的工作流程，将逆向设计分解为Prop2Desc和Desc2MOF两步。

Result: 在氢气吸附数据集上有效性超95%、命中率达84%，比现有方法有显著提升，且仅1000个训练样本仍有效，能在29个不同属性数据集上进行条件生成。

Conclusion: EGMOF是一种数据高效、可推广的MOFs逆向设计方法，模块化逆向设计工作流在更广泛材料发现中有潜力。

Abstract: Designing materials with targeted properties remains challenging due to the
vastness of chemical space and the scarcity of property-labeled data. While
recent advances in generative models offer a promising way for inverse design,
most approaches require large datasets and must be retrained for every new
target property. Here, we introduce the EGMOF (Efficient Generation of MOFs), a
hybrid diffusion-transformer framework that overcomes these limitations through
a modular, descriptor-mediated workflow. EGMOF decomposes inverse design into
two steps: (1) a one-dimensional diffusion model (Prop2Desc) that maps desired
properties to chemically meaningful descriptors followed by (2) a transformer
model (Desc2MOF) that generates structures from these descriptors. This modular
hybrid design enables minimal retraining and maintains high accuracy even under
small-data conditions. On a hydrogen uptake dataset, EGMOF achieved over 95%
validity and 84% hit rate, representing significant improvements of up to 57%
in validity and 14% in hit rate compared to existing methods, while remaining
effective with only 1,000 training samples. Moreover, our model successfully
performed conditional generation across 29 diverse property datasets, including
CoREMOF, QMOF, and text-mined experimental datasets, whereas previous models
have not. This work presents a data-efficient, generalizable approach to the
inverse design of diverse MOFs and highlights the potential of modular inverse
design workflows for broader materials discovery.

</details>


<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [214] [NABench: Large-Scale Benchmarks of Nucleotide Foundation Models for Fitness Prediction](https://arxiv.org/abs/2511.02888)
*Zhongmin Li,Runze Ma,Jiahao Tan,Chengzi Tan,Shuangjia Zheng*

Main category: q-bio.GN

TL;DR: 介绍核酸适应性预测基准NABench，评估29个模型，发布以推动核酸建模。


<details>
  <summary>Details</summary>
Motivation: 现有核苷酸基础模型因数据集异构和预处理不一致，难以公平比较不同DNA和RNA家族的方法。

Method: 引入NABench，聚合162个高通量实验，整理260万个突变序列，在统一评估套件下评估29个模型。

Result: NABench在规模、多样性和数据质量上超越先前基准，量化了不同任务和核酸类型的性能异质性。

Conclusion: 发布NABench以推动核酸建模，支持下游应用，代码开源。

Abstract: Nucleotide sequence variation can induce significant shifts in functional
fitness. Recent nucleotide foundation models promise to predict such fitness
effects directly from sequence, yet heterogeneous datasets and inconsistent
preprocessing make it difficult to compare methods fairly across DNA and RNA
families. Here we introduce NABench, a large-scale, systematic benchmark for
nucleic acid fitness prediction. NABench aggregates 162 high-throughput assays
and curates 2.6 million mutated sequences spanning diverse DNA and RNA
families, with standardized splits and rich metadata. We show that NABench
surpasses prior nucleotide fitness benchmarks in scale, diversity, and data
quality. Under a unified evaluation suite, we rigorously assess 29
representative foundation models across zero-shot, few-shot prediction,
transfer learning, and supervised settings. The results quantify performance
heterogeneity across tasks and nucleic-acid types, demonstrating clear
strengths and failure modes for different modeling choices and establishing
strong, reproducible baselines. We release NABench to advance nucleic acid
modeling, supporting downstream applications in RNA/DNA design, synthetic
biology, and biochemistry. Our code is available at
https://github.com/mrzzmrzz/NABench.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [215] [Modeling Headway in Heterogeneous and Mixed Traffic Flow: A Statistical Distribution Based on a General Exponential Function](https://arxiv.org/abs/2511.03154)
*Natchaphon Leungbootnak,Zihao Li,Zihang Wei,Dominique Lord,Yunlong Zhang*

Main category: stat.AP

TL;DR: 现有车头时距分布在异质和混合交通中拟合不佳，提出新分布并实验验证其性能更优。


<details>
  <summary>Details</summary>
Motivation: 现有车头时距分布难以准确反映异质和混合交通的特征，拟合度差。

Method: 修改指数函数得到新的车头时距分布，用实数为底增加建模灵活性，归一化计算概率并推导闭式方程，用五个开放数据集实验评估。

Result: 新分布能捕捉车头时距基本特征，参数有物理意义，在高速异质流和城市道路异质、混合流中表现良好。

Conclusion: 新的车头时距分布在异质和混合交通流中优于其他候选分布。

Abstract: The ability of existing headway distributions to accurately reflect the
diverse behaviors and characteristics in heterogeneous traffic (different types
of vehicles) and mixed traffic (human-driven vehicles with autonomous vehicles)
is limited, leading to unsatisfactory goodness of fit. To address these issues,
we modified the exponential function to obtain a novel headway distribution.
Rather than employing Euler's number (e) as the base of the exponential
function, we utilized a real number base to provide greater flexibility in
modeling the observed headway. However, the proposed is not a probability
function. We normalize it to calculate the probability and derive the
closed-form equation. In this study, we utilized a comprehensive experiment
with five open datasets: highD, exiD, NGSIM, Waymo, and Lyft to evaluate the
performance of the proposed distribution and compared its performance with six
existing distributions under mixed and heterogeneous traffic flow. The results
revealed that the proposed distribution not only captures the fundamental
characteristics of headway distribution but also provides physically meaningful
parameters that describe the distribution shape of observed headways. Under
heterogeneous flow on highways (i.e., uninterrupted traffic flow), the proposed
distribution outperforms other candidate distributions. Under urban road
conditions (i.e., interrupted traffic flow), including heterogeneous and mixed
traffic, the proposed distribution still achieves decent results.

</details>


<div id='astro-ph.EP'></div>

# astro-ph.EP [[Back]](#toc)

### [216] [Optimizing Earth-Moon Transfer and Cislunar Navigation: Integrating Low-Energy Trajectories, AI Techniques and GNSS-R Technologies](https://arxiv.org/abs/2511.03173)
*Arsalan Muhammad,Wasiu Akande Ahmed,Omada Friday Ojonugwa,Paul Puspendu Biswas*

Main category: astro-ph.EP

TL;DR: 本文综述了地月空间转移策略，强调人工智能作用，探讨扩展导航能力方法，构建可持续地月探索框架。


<details>
  <summary>Details</summary>
Motivation: 地月活动快速增长，传统地月转移和导航存在局限，需发展高效轨迹设计和可靠导航遥感集成。

Method: 比较四种主要转移策略，评估速度要求、飞行时长和燃油效率；分析人工智能和机器学习应用；研究GNSS - Reflectometry和先进PNT架构。

Result: 确定各转移策略对载人及机器人任务的适用性；卷积神经网络支持自动陨石坑识别等，深度强化学习可优化着陆轨迹；GNSS - R可绘制月球特征，PNT系统支持多种操作。

Conclusion: 结合这些发展可建立可持续地月探索和长期人类及机器人存在的可扩展框架。

Abstract: The rapid growth of cislunar activities, including lunar landings, the Lunar
Gateway, and in-space refueling stations, requires advances in cost-efficient
trajectory design and reliable integration of navigation and remote sensing.
Traditional Earth-Moon transfers suffer from rigid launch windows and high
propellant demands, while Earth-based GNSS systems provide little to no
coverage beyond geostationary orbit. This limits autonomy and environmental
awareness in cislunar space. This review compares four major transfer
strategies by evaluating velocity requirements, flight durations, and fuel
efficiency, and by identifying their suitability for both crewed and robotic
missions. The emerging role of artificial intelligence and machine learning is
highlighted: convolutional neural networks support automated crater recognition
and digital terrain model generation, while deep reinforcement learning enables
adaptive trajectory refinement during descent and landing to reduce risk and
decision latency. The study also examines how GNSS-Reflectometry and advanced
Positioning, Navigation, and Timing architectures can extend navigation
capabilities beyond current limits. GNSS-R can act as a bistatic radar for
mapping lunar ice, soil properties, and surface topography, while PNT systems
support autonomous rendezvous, Lagrange point station-keeping, and coordinated
satellite swarm operations. Combining these developments establishes a scalable
framework for sustainable cislunar exploration and long-term human and robotic
presence.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [217] [Google's Hidden Empire](https://arxiv.org/abs/2511.02931)
*Aline Blankertz,Brianna Rock,Nicholas Shaxson*

Main category: cs.CY

TL;DR: 论文揭示谷歌在全球数字和企业领域的参与规模，分析反垄断失败原因并为当下谷歌收购案提供借鉴。


<details>
  <summary>Details</summary>
Motivation: 探究谷歌在全球数字和企业格局中的参与规模及反垄断监管的不足。

Method: 呈现谷歌收购等数据，以谷歌/DoubleClick和谷歌/Fitbit并购未干预为例分析。

Result: 谷歌对数字市场的影响力比以往记录更大，新古典经济学的狭隘方法阻碍了反垄断执法。

Conclusion: 过去反垄断失败的教训可用于指导当前谷歌320亿美元收购Wiz的交易。

Abstract: This paper presents striking new data about the scale of Google's involvement
in the global digital and corporate landscape, head and shoulders above the
other big tech firms. While public attention and some antitrust scrutiny has
focused on these firms' mergers and acquisitions (M&A) activities, Google has
also been amassing an empire of more than 6,000 companies which it has
acquired, supported or invested in, across the digital economy and beyond. The
power of Google over the digital markets infrastructure and dynamics is likely
greater than previously documented. We also trace the antitrust failures that
have led to this state of affairs. In particular, we explore the role of
neoclassical economics practiced both inside the regulatory authorities and by
consultants on the outside. Their unduly narrow approach has obscured harms
from vertical and conglomerate concentrations of market power and erected ever
higher hurdles for enforcement action, as we demonstrate using examples of the
failure to intervene in the Google/DoubleClick and Google/Fitbit mergers. Our
lessons from the past failures can inform the current approach towards one of
the biggest ever big tech M&A deals: Google's $32 billion acquisition of the
Israeli cloud cybersecurity firm Wiz.

</details>


### [218] [Academics and Generative AI: Empirical and Epistemic Indicators of Policy-Practice Voids](https://arxiv.org/abs/2511.02875)
*R. Yamamoto Ravenor*

Main category: cs.CY

TL;DR: 本文针对生成式AI在学术界扩散时政策与实践差异问题，设计工具并提出三个衡量差异的指标。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在学术界扩散，政策与实践差异产生对可审计的一致性指标的需求。

Method: 设计一个包含十项的间接诱导工具，嵌入结构化解释框架，从学者处提取实证和认知信号。

Result: 得出三个衡量制度规则与从业者AI使用差异的指标，分别是AI集成评估能力、部门层面必要性、本体论立场。

Conclusion: 通过所设计工具和框架得到了可衡量政策与实践差异的指标。

Abstract: As generative AI diffuses through academia, policy-practice divergence
becomes consequential, creating demand for auditable indicators of alignment.
This study prototypes a ten-item, indirect-elicitation instrument embedded in a
structured interpretive framework to surface voids between institutional rules
and practitioner AI use. The framework extracts empirical and epistemic signals
from academics, yielding three filtered indicators of such voids: (1)
AI-integrated assessment capacity (proxy) - within a three-signal screen (AI
skill, perceived teaching benefit, detection confidence), the share who would
fully allow AI in exams; (2) sector-level necessity (proxy) - among high output
control users who still credit AI with high contribution, the proportion who
judge AI capable of challenging established disciplines; and (3) ontological
stance - among respondents who judge AI different in kind from prior tools,
report practice change, and pass a metacognition gate, the split between
material and immaterial views as an ontological map aligning procurement claims
with evidence classes.

</details>


### [219] [A Criminology of Machines](https://arxiv.org/abs/2511.02895)
*Gian Maria Campedelli*

Main category: cs.CY

TL;DR: 随着自主AI代理普及，犯罪学需研究多智能体AI系统对犯罪和社会控制的影响，提出分类法并提出四个关键问题。


<details>
  <summary>Details</summary>
Motivation: 自主AI代理日益普遍，犯罪学需应对其对犯罪和社会控制的影响。

Method: 借鉴行动者网络理论和Woolgar的呼吁，结合AI安全文献，提出双重分类法。

Result: 提出双重分类法来描述AI代理交互产生异常、非法或犯罪结果的渠道，提出四个关键问题。

Conclusion: 犯罪学家需从理论和实证上研究多智能体AI系统对犯罪研究的影响，在AI安全和治理辩论中发挥更积极作用。

Abstract: While the possibility of reaching human-like Artificial Intelligence (AI)
remains controversial, the likelihood that the future will be characterized by
a society with a growing presence of autonomous machines is high. Autonomous AI
agents are already deployed and active across several industries and digital
environments and alongside human-human and human-machine interactions,
machine-machine interactions are poised to become increasingly prevalent. Given
these developments, I argue that criminology must begin to address the
implications of this transition for crime and social control. Drawing on
Actor-Network Theory and Woolgar's decades-old call for a sociology of machines
-- frameworks that acquire renewed relevance with the rise of generative AI
agents -- I contend that criminologists should move beyond conceiving AI solely
as a tool. Instead, AI agents should be recognized as entities with agency
encompassing computational, social, and legal dimensions. Building on the
literature on AI safety, I thus examine the risks associated with the rise of
multi-agent AI systems, proposing a dual taxonomy to characterize the channels
through which interactions among AI agents may generate deviant, unlawful, or
criminal outcomes. I then advance and discuss four key questions that warrant
theoretical and empirical attention: (1) Can we assume that machines will
simply mimic humans? (2) Will crime theories developed for humans suffice to
explain deviant or criminal behaviors emerging from interactions between
autonomous AI agents? (3) What types of criminal behaviors will be affected
first? (4) How might this unprecedented societal shift impact policing? These
questions underscore the urgent need for criminologists to theoretically and
empirically engage with the implications of multi-agent AI systems for the
study of crime and play a more active role in debates on AI safety and
governance.

</details>


### [220] [Retrofitters, pragmatists and activists: Public interest litigation for accountable automated decision-making](https://arxiv.org/abs/2511.03211)
*Henry Fraser,Zahra Stardust*

Main category: cs.CY

TL;DR: 本文探讨澳大利亚公益诉讼在推动人工智能和自动化决策问责方面的作用，整合并呈现有效诉讼策略，分析其局限性和迫切需求。


<details>
  <summary>Details</summary>
Motivation: 由于自动化决策监管面临地缘政治阻力，有效治理需依靠现有法律执行，探讨公益诉讼作用。

Method: 采访澳大利亚公益诉讼律师、技术政策活动家和技术法学者。

Result: 将公益诉讼定位为自动化决策相关生态的一部分，提出有效公益诉讼的策略和战术，分析策略及法律系统的局限性。

Conclusion: 指出需有支持性制度安排，否则有效诉讼和问责将受阻，对多类群体有参考价值。

Abstract: This paper examines the role of public interest litigation in promoting
accountability for AI and automated decision-making (ADM) in Australia. Since
ADM regulatio faces geopolitical headwinds, effective governance will have to
rely at least in part on the enforcement of existing laws. Drawing on
interviews with Australian public interest litigators, technology policy
activists, and technology law scholars, the paper positions public interest
litigation as part of a larger ecosystem for transparency, accountability and
justice with respect to ADM. It builds on one participants's characterisation
of litigation about ADM as an exercise in legal retrofitting: adapting old laws
to new circumstances. The paper's primary contribution is to aggregate,
organise and present original insights on pragmatic strategies and tactics for
effective public interest litigation about ADM. Naturally, it also contends
with the limits of these strategies, and of the legal system. Where limits are,
however, capable of being overcome, the paper presents findings on urgent
needs: the enabling institutional arrangements without which effective
litigation and accountability will falter. The paper is relevant to law and
technology scholars; individuals and groups harmed by ADM; public interest
litigators and technology lawyers; civil society and advocacy organisations;
and policymakers.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [221] [Association-sensory spatiotemporal hierarchy and functional gradient-regularised recurrent neural network with implications for schizophrenia](https://arxiv.org/abs/2511.02722)
*Subati Abulikemu,Puria Radmard,Michail Mamalakis,John Suckling*

Main category: q-bio.NC

TL;DR: 研究对比精神分裂症患者与对照组的大脑感觉 - 关联（AS）层级，发现患者 AS 层级压缩，功能分化降低，通过模型和网络研究揭示层级去分化影响神经计算稳定性。


<details>
  <summary>Details</summary>
Motivation: 刻画精神分裂症患者的 AS 层级并与对照组对比，探究精神分裂症在大脑功能层级上的特征。

Method: 利用大型 fMRI 数据集，通过频谱分析提取个体 AS 梯度，量化层级专业化；用奥恩斯坦 - 乌伦贝克过程建模神经时间尺度；用梯度正则化特定受试者的循环神经网络（RNN）。

Result: 精神分裂症压缩 AS 层级，功能分化降低；层级极端区域时间常数变化在患者中减弱；梯度扩展大的网络学习效率高、任务损失低、状态更稳定。

Conclusion: 梯度正则化 RNN 框架将大脑皮层架构与定点稳定性联系起来，解释了层级去分化如何破坏精神分裂症神经计算稳定性。

Abstract: The human neocortex is functionally organised at its highest level along a
continuous sensory-to-association (AS) hierarchy. This study characterises the
AS hierarchy of patients with schizophrenia in a comparison with controls.
Using a large fMRI dataset (N=355), we extracted individual AS gradients via
spectral analysis of brain connectivity, quantified hierarchical specialisation
by gradient spread, and related this spread with connectivity geometry. We
found that schizophrenia compresses the AS hierarchy indicating reduced
functional differentiation. By modelling neural timescale with the
Ornstein-Uhlenbeck process, we observed that the most specialised, locally
cohesive regions at the gradient extremes exhibit dynamics with a longer time
constant, an effect that is attenuated in schizophrenia. To study computation,
we used the gradients to regularise subject-specific recurrent neural networks
(RNNs) trained on working memory tasks. Networks endowed with greater gradient
spread learned more efficiently, plateaued at lower task loss, and maintained
stronger alignment to the prescribed AS hierarchical geometry. Fixed point
linearisation showed that high-range networks settled into more stable neural
states during memory delay, evidenced by lower energy and smaller maximal
Jacobian eigenvalues. This gradient-regularised RNN framework therefore links
large-scale cortical architecture with fixed point stability, providing a
mechanistic account of how gradient de-differentiation could destabilise neural
computations in schizophrenia, convergently supported by empirical timescale
flattening and model-based evidence of less stable fixed points.

</details>


### [222] [Explaining Human Choice Probabilities with Simple Vector Representations](https://arxiv.org/abs/2511.03643)
*Peter DiBerardino,Britt Anderson*

Main category: q-bio.NC

TL;DR: 研究人们在随机环境中追求奖励时的选择行为，发现两种基本策略可解释选择模式。


<details>
  <summary>Details</summary>
Motivation: 探究人们在随机环境中追求奖励时，即使策略次优仍按观察频率选择的行为，通过改变任务条件进一步研究。

Method: 使用“躲猫猫”任务，改变追求与回避状态、选择数量，构建基于选择频率直方图向量的模型，提出回避时概率反匹配策略。

Result: 仅匹配/反匹配和最大化/最小化两种基本策略足以解释不同房间数量和对手概率分布下参与者的选择。

Conclusion: 人们只需记住不同结果相对频率，通过简单操作构建策略，两种策略混合可捕捉随机环境中人类选择模式。

Abstract: When people pursue rewards in stochastic environments, they often match their
choice frequencies to the observed target frequencies, even when this policy is
demonstrably sub-optimal. We used a ``hide and seek'' task to evaluate this
behavior under conditions where pursuit (seeking) could be toggled to avoidance
(hiding), while leaving the probability distribution fixed, or varying
complexity by changing the number of possible choices. We developed a model for
participant choice built from choice frequency histograms treated as vectors.
We posited the existence of a probability antimatching strategy for avoidance
(hiding) rounds, and formalized this as a vector reflection of probability
matching. We found that only two basis policies: matching/antimatching and
maximizing/minimizing were sufficient to account for participant choices across
a range of room numbers and opponent probability distributions. This schema
requires only that people have the ability to remember the relative frequency
of the different outcomes. With this knowledge simple operations can construct
the maximizing and minimizing policies as well as matching and antimatching
strategies. A mixture of these two policies captures human choice patterns in a
stochastic environment.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [223] [Evolutionary Dynamics in Continuous-time Finite-state Mean Field Games -- Part II: Stability](https://arxiv.org/abs/2511.03297)
*Leonardo Pedroso,Andrea Agazzi,W. P. M. H. Heemels,Mauro Salazar*

Main category: eess.SY

TL;DR: 研究大群体玩家在连续时间的动态博弈，在第一部分引入模型和MSNE概念基础上，第二部分研究MSNE的进化稳定性并推导稳定性条件。


<details>
  <summary>Details</summary>
Motivation: 探讨大群体动态博弈中MSNE的长期可行性，即其能否在战略偏差下稳健出现并持续。

Method: 对动态博弈进行理论分析，推导MSNE结构和博弈收益映射的条件。

Result: 得出确保MSNE在进化动态下局部和全局稳定的条件。

Conclusion: 这些结果为理解MSNE在大群体动态博弈中的长期可行性提供了见解。

Abstract: We study a dynamic game with a large population of players who choose actions
from a finite set in continuous time. Each player has a state in a finite state
space that evolves stochastically with their actions. A player's reward depends
not only on their own state and action but also on the distribution of states
and actions across the population, capturing effects such as congestion in
traffic networks. In Part I, we introduced an evolutionary model and a new
solution concept - the mixed stationary Nash Equilibrium (MSNE) - which
coincides with the rest points of the mean field evolutionary model under
meaningful families of revision protocols. In this second part, we investigate
the evolutionary stability of MSNE. We derive conditions on both the structure
of the MSNE and the game's payoff map that ensure local and global stability
under evolutionary dynamics. These results characterize when MSNE can robustly
emerge and persist against strategic deviations, thereby providing insight into
its long-term viability in large population dynamic games.

</details>


### [224] [System Identification of a Moored ASV with Recessed Moon Pool via Deterministic and Bayesian Hankel-DMDc](https://arxiv.org/abs/2511.03482)
*Giorgio Palma,Ivan Santic,Andrea Serani,Lorenzo Minno,Matteo Diez*

Main category: eess.SY

TL;DR: 本文利用HDMDc和BHDMDc对系泊状态下小型自主水面航行器（ASV）进行系统识别，实验表明两种方法能预测ASV对未知波浪激励的响应，HDMDc基降阶模型是可行的数据驱动替代方案。


<details>
  <summary>Details</summary>
Motivation: 解决系泊状态下小型自主水面航行器的系统识别问题，尤其是带有凹陷月池的ASV因晃荡产生非线性响应带来的建模挑战。

Method: 采用Hankel动态模式分解与控制（HDMDc）及其贝叶斯扩展（BHDMDc）方法，基于测量的船舶运动和系泊载荷数据构建数据驱动的降阶模型。

Result: HDMDc能准确确定性预测船舶动力学，贝叶斯公式可实现对模型响应的不确定性表征；两种方法都能预测船舶对未知规则和不规则波浪激励的响应。

Conclusion: 基于HDMDc的降阶模型是系统识别可行的数据驱动替代方案，首次证明其对不同海况的泛化能力，能高精度再现船舶动力学。

Abstract: This study addresses the system identification of a small autonomous surface
vehicle (ASV) under moored conditions using Hankel dynamic mode decomposition
with control (HDMDc) and its Bayesian extension (BHDMDc). Experiments were
carried out on a Codevintec CK-14e ASV in the towing tank of CNR-INM, under
both irregular and regular head-sea wave conditions. The ASV under
investigation features a recessed moon pool, which induces nonlinear responses
due to sloshing, thereby increasing the modelling challenge. Data-driven
reduced-order models were built from measurements of vessel motions and mooring
loads. The HDMDc framework provided accurate deterministic predictions of
vessel dynamics, while the Bayesian formulation enabled uncertainty-aware
characterization of the model response by accounting for variability in
hyperparameter selection. Validation against experimental data demonstrated
that both HDMDc and BHDMDc can predict the vessel's response to unseen regular
and irregular wave excitations. In conclusion, the study shows that HDMDc-based
ROMs are a viable data-driven alternative for system identification,
demonstrating for the first time their generalization capability for a sea
condition different from the training set, achieving high accuracy in
reproducing vessel dynamics.

</details>


<div id='stat.TH'></div>

# stat.TH [[Back]](#toc)

### [225] [Statistical Properties of Rectified Flow](https://arxiv.org/abs/2511.03193)
*Gonzalo Mena,Arun Kumar Kuchibhotla,Larry Wasserman*

Main category: stat.TH

TL;DR: 研究整流流的结构和统计特性，在有界和无界情况下分析，收敛速度比普通非参数回归和密度估计更快。


<details>
  <summary>Details</summary>
Motivation: 整流流方法在机器学习中流行但缺乏理论支持，需研究其性质。

Method: 分别分析有界和无界情况，利用标准统计工具如回归和密度估计。

Result: 在有界和无界情况下都能建立比通常非参数回归和密度估计更快的收敛速度。

Conclusion: 对整流流的结构和统计特性研究有积极成果，收敛速度表现更好。

Abstract: Rectified flow (Liu et al., 2022; Liu, 2022; Wu et al., 2023) is a method for
defining a transport map between two distributions, and enjoys popularity in
machine learning, although theoretical results supporting the validity of these
methods are scant. The rectified flow can be regarded as an approximation to
optimal transport, but in contrast to other transport methods that require
optimization over a function space, computing the rectified flow only requires
standard statistical tools such as regression or density estimation. Because of
this, one can leverage standard data analysis tools for regression and density
estimation to develop empirical versions of transport maps. We study some
structural properties of the rectified flow, including existence, uniqueness,
and regularity, as well as the related statistical properties, such as rates of
convergence and central limit theorems, for some selected estimators. To do so,
we analyze separately the bounded and unbounded cases as each presents unique
challenges. In both cases, we are able to establish convergence at faster rates
than the ones for the usual nonparametric regression and density estimation.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [226] [Robust optimal consumption, investment and reinsurance for recursive preferences](https://arxiv.org/abs/2511.03031)
*Elizabeth Dadzie,Wilfried Kuissi-Kamdem,Marcel Ndengo*

Main category: math.OC

TL;DR: 本文研究具有Epstein - Zin递归偏好的保险公司在模型不确定下的鲁棒最优消费、投资和再保险问题，推导了最优策略和价值函数的闭式解，分析了关键参数影响并给出管理框架。


<details>
  <summary>Details</summary>
Motivation: 研究保险公司在模型不确定下如何进行最优的消费、投资和再保险决策，以实现最坏情况下的效用最大化。

Method: 通过求解相关的耦合正倒向随机微分方程（FBSDE）。

Result: 得到了最优策略和价值函数的闭式解；数值实验表明最优消费随风险厌恶和跨期替代弹性增加而降低，投资和再保险策略依赖于金融和保险市场参数。

Conclusion: 为保险公司在深度不确定性下进行资本配置和风险转移提供了综合框架。

Abstract: This paper investigates a robust optimal consumption, investment, and
reinsurance problem for an insurer with Epstein-Zin recursive preferences
operating under model uncertainty. The insurer's surplus follows the diffusion
approximation of the Cram\'er-Lundberg model, and the insurer can purchase
proportional reinsurance. Model ambiguity is characterised by a class of
equivalent probability measures, and the insurer, being ambiguity-averse, aims
to maximise utility under the worst-case scenario. By solving the associated
coupled forward-backward stochastic differential equation (FBSDE), we derive
closed-form solutions for the optimal strategies and the value function. Our
analysis reveals how ambiguity aversion, risk aversion, and the elasticity of
intertemporal substitution (EIS) influence the optimal policies. Numerical
experiments illustrate the effects of key parameters, showing that optimal
consumption decreases with higher risk aversion and EIS, while investment and
reinsurance strategies are co-dependent on both financial and insurance market
parameters, even without correlation. This study provides a comprehensive
framework for insurers to manage capital allocation and risk transfer under
deep uncertainty.

</details>


### [227] [Min-Max Optimization Is Strictly Easier Than Variational Inequalities](https://arxiv.org/abs/2511.03052)
*Henry Shugart,Jason M. Altschuler*

Main category: math.OC

TL;DR: 本文探讨绕过变分不等式求解凸 - 凹极小 - 极大问题的可能性，发现在无约束二次目标的教科书设定下可行，且极小 - 极大算法更快。


<details>
  <summary>Details</summary>
Motivation: 研究是否能绕过传统的变分不等式方法，更快地求解极小 - 极大问题。

Method: 通过极值多项式对最优收敛率进行精确刻画，并利用格林函数和共形映射进行计算。

Result: 在无约束二次目标的设定下，一阶算法求解极小 - 极大问题的最优收敛速度比求解相应变分不等式严格更快。

Conclusion: 极小 - 极大算法能利用变量不对称性，绕过变分不等式可更快求解极小 - 极大问题。

Abstract: Classically, a mainstream approach for solving a convex-concave min-max
problem is to instead solve the variational inequality problem arising from its
first-order optimality conditions. Is it possible to solve min-max problems
faster by bypassing this reduction? This paper initiates this investigation. We
show that the answer is yes in the textbook setting of unconstrained quadratic
objectives: the optimal convergence rate for first-order algorithms is strictly
better for min-max problems than for the corresponding variational
inequalities. The key reason that min-max algorithms can be faster is that they
can exploit the asymmetry of the min and max variables--a property that is lost
in the reduction to variational inequalities. Central to our analyses are sharp
characterizations of optimal convergence rates in terms of extremal polynomials
which we compute using Green's functions and conformal mappings.

</details>


### [228] [A Support-Set Algorithm for Optimization Problems with Nonnegative and Orthogonal Constraints](https://arxiv.org/abs/2511.03443)
*Lei Wang,Xin Liu,Xiaojun Chen*

Main category: math.OC

TL;DR: 研究非负正交约束优化问题，提出支持集算法，理论证明收敛性与复杂度，数值实验表明算法在实际应用中表现良好。


<details>
  <summary>Details</summary>
Motivation: 解决非负正交约束优化问题，提高计算效率。

Method: 通过固定支持集计算目标函数近端线性化子问题的全局解，提出支持集算法并设计更新方案。

Result: 支持集算法全局收敛到一阶驻点，迭代复杂度为 $O (\epsilon^{-2})$，数值实验在实际应用中表现好。

Conclusion: 所提支持集算法在非负正交约束优化问题上有效且计算效率高。

Abstract: In this paper, we investigate optimization problems with nonnegative and
orthogonal constraints, where any feasible matrix of size $n \times p$ exhibits
a sparsity pattern such that each row accommodates at most one nonzero entry.
Our analysis demonstrates that, by fixing the support set, the global solution
of the minimization subproblem for the proximal linearization of the objective
function can be computed in closed form with at most $n$ nonzero entries.
Exploiting this structural property offers a powerful avenue for dramatically
enhancing computational efficiency. Guided by this insight, we propose a
support-set algorithm preserving strictly the feasibility of iterates. A
central ingredient is a strategically devised update scheme for support sets
that adjusts the placement of nonzero entries. We establish the global
convergence of the support-set algorithm to a first-order stationary point, and
show that its iteration complexity required to reach an $\epsilon$-approximate
first-order stationary point is $O (\epsilon^{-2})$. Numerical results are
strongly in favor of our algorithm in real-world applications, including
nonnegative PCA, clustering, and community detection.

</details>


### [229] [Optimal Boundary Control of Diffusion on Graphs via Linear Programming](https://arxiv.org/abs/2511.03129)
*Harbir Antil,Rainald Löhner,Felipe Pérez*

Main category: math.OC

TL;DR: 提出用于几何网络稳态扩散和通量优化的线性规划框架，证明LP有全局最小解，给出可行域有界的条件，并通过两个实例说明。


<details>
  <summary>Details</summary>
Motivation: 解决几何网络稳态扩散和通量优化问题。

Method: 构建线性规划框架，考虑离散扩散定律、边界电位控制、物理约束等，结合经典定理进行分析。

Result: 证明在无负衰退方向时LP有全局最小解，确定可行域有界的充分条件。

Conclusion: 该线性规划框架可用于几何网络的扩散和通量优化，且能结合经典理论进行有效分析。

Abstract: We propose a linear programming (LP) framework for steady-state diffusion and
flux optimization on geometric networks. The state variable satisfies a
discrete diffusion law on a weighted, oriented graph, where conductances are
scaled by edge lengths to preserve geometric fidelity. Boundary potentials act
as controls that drive interior fluxes according to a linear network Laplacian.
The optimization problem enforces physically meaningful sign and flux-cap
constraints at all boundary edges, derived directly from a gradient bound. This
yields a finite-dimensional LP whose feasible set is polyhedral, and whose
boundedness and solvability follow from simple geometric or algebraic
conditions on the network data.
  We prove that under the absence of negative recession
directions--automatically satisfied in the presence of finite box bounds, flux
caps, or sign restrictions--the LP admits a global minimizer. Several
sufficient conditions guaranteeing boundedness of the feasible region are
identified, covering both full-rank and rank-deficient flux maps. The analysis
connects classical results such as the Minkowski--Weyl decomposition, Hoffman's
bound, and the fundamental theorem of linear programming with modern
network-based diffusion modeling.
  Two large-scale examples illustrate the framework: (i) A typical large
stadium in a major modern city, which forms a single connected component with
relatively uniform corridor widths, and a (ii) A complex street network
emanating from a large, historical city center, which forms a multi-component
system.

</details>


<div id='q-fin.MF'></div>

# q-fin.MF [[Back]](#toc)

### [230] [PELVE from a regulatory perspective](https://arxiv.org/abs/2511.03551)
*Christian Laudagé,Jörn Sass*

Main category: q-fin.MF

TL;DR: 论文探讨Solvency II下风险度量从VaR转向ES时ES水平的确定，提出适用于多个保险公司的PELVE启发式方法，分析ES水平特性，通过案例研究强调方法选择的重要性并给出推荐。


<details>
  <summary>Details</summary>
Motivation: 在Solvency II框架下，ES比VaR更适合作为风险度量指标，但转向ES需要确定相应的ES水平，现有PELVE方法针对单个保险公司，因此需从监管者角度为多个保险公司提出方法。

Method: 提出PELVE启发式方法用于多个保险公司，分析ES水平的存在性和唯一性，推导椭圆分布收益的表达式，建立多元正则分布收益的极限结果。

Result: 案例研究表明当收益来自不同分布族时，方法的选择至关重要。

Conclusion: 推荐了适用于多个保险公司确定ES水平的特定方法。

Abstract: Under Solvency II, the Value-at-Risk (VaR) is applied, although there is
broad consensus that the Expected Shortfall (ES) constitutes a more appropriate
measure. Moving towards ES would necessitate specifying the corresponding ES
level. The recently introduced Probability Equivalent Level of VaR and ES
(PELVE) determines this by requiring that ES equals the prescribed VaR for a
given future payoff, reflecting the situation of an individual insurer. We
incorporate the regulator's perspective by proposing PELVE-inspired methods for
multiple insurers. We analyze existence and uniqueness of the resulting ES
levels, derive expressions for elliptically distributed payoffs and establish
limit results for multivariate regularly distributed payoffs. A case study
highlights that the choice of method is crucial when payoffs arise from
different distribution families. Moreover, we recommend specific methods.

</details>


<div id='econ.TH'></div>

# econ.TH [[Back]](#toc)

### [231] [Balanced contributions, consistency, and value for games with externalities](https://arxiv.org/abs/2511.03145)
*André Casajus,Yukihiko Funaki,Frank Huettner*

Main category: econ.TH

TL;DR: 研究带外部性博弈中夏普利值的公平一致扩展，基于已有研究定义相关性质，得出对夏普利值推广的刻画。


<details>
  <summary>Details</summary>
Motivation: 考虑带外部性博弈中夏普利值的公平和一致扩展。

Method: 基于Casajus等人（2024）的限制，定义平衡贡献、索博列夫一致性和哈特 - 马斯 - 科莱尔一致性。

Result: 这些性质得出了Macho - Stadler等人（2007）引入的夏普利值推广的刻画。

Conclusion: 这些性质对夏普利值推广的刻画与夏普利值的重要刻画平行。

Abstract: We consider fair and consistent extensions of the Shapley value for games
with externalities. Based on the restriction identified by Casajus et al.
(2024, Games Econ. Behavior 147, 88-146), we define balanced contributions,
Sobolev's consistency, and Hart and Mas-Colell's consistency for games with
externalities, and we show that these properties lead to characterizations of
the generalization of the Shapley value introduced by Macho-Stadler et al.
(2007, J. Econ. Theory 135, 339-356), that parallel important characterizations
of the Shapley value.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [232] [Exploring Topologies in Quantum Annealing: A Hardware-Aware Perspective](https://arxiv.org/abs/2511.03327)
*Mario Bifulco,Luca Roversi*

Main category: quant-ph

TL;DR: 本文研究量子退火（QA）中硬件拓扑对问题嵌入的影响，评估两种QPU拓扑下的Minor Embedding（ME）结果，发现Havel - Hakimi拓扑有作为QA - 基QPU替代设计的潜力。


<details>
  <summary>Details</summary>
Motivation: 量子退火解决NP - 难优化问题的有效性受底层量子硬件拓扑限制，硬连线量子比特的静态拓扑会导致低效编译，需评估硬件拓扑对嵌入问题的负面影响。

Method: 提出评估硬件拓扑对嵌入问题负面影响的方法和标准，在Zephyr图和Havel - Hakimi图两种QPU拓扑上评估ME结果，研究节点数与节点入射弧数之比对ME成功率的影响。

Result: 通过在经典架构上执行ME发现，Havel - Hakimi拓扑平均需要更短的量子比特链，随着QPU尺寸增加，可嵌入的最大图G_P缩放更平滑。

Conclusion: Havel - Hakimi拓扑有潜力作为基于QA的QPU的替代设计。

Abstract: Quantum Annealing (QA) offers a promising framework for solving NP-hard
optimization problems, but its effectiveness is constrained by the topology of
the underlying quantum hardware. Solving an optimization problem $P$ via QA
involves a hardware-aware circuit compilation which requires representing $P$
as a graph $G_P$ and embedding it into the hardware connectivity graph $G_Q$
that defines how qubits connect to each other in a QA-based quantum processing
unit (QPU).
  Minor Embedding (ME) is a possible operational form of this hardware-aware
compilation. ME heuristically builds a map that associates each node of $G_P$
-- the logical variables of $P$ -- to a chain of adjacent nodes in $G_Q$ by
means of one of its minors, so that the arcs of $G_P$ are preserved as physical
connections among qubits in $G_Q$.
  The static topology of hardwired qubits can clearly lead to inefficient
compilations because $G_Q$ cannot be a clique, currently. We propose a
methodology and a set of criteria to evaluate how the hardware topology $G_Q$
can negatively affect the embedded problem, thus making the quantum
optimization more sensible to noise.
  We evaluate the result of ME across two QPU topologies: Zephyr graphs (used
in current D-Wave systems) and Havel-Hakimi graphs, which allow controlled
variation of the average node degree. This enables us to study how the ratio
`number of nodes/number of incident arcs per node' affects ME success rates to
map $G_P$ into a minor of $G_Q$.
  Our findings, obtained through ME executed on classical, i.e. non-quantum,
architectures, suggest that Havel-Hakimi-based topologies, on average, require
shorter qubit chains in the minor of $G_P$, exhibiting smoother scaling of the
largest embeddable $G_P$ as the QPU size increases. These characteristics
indicate their potential as alternative designs for QA-based QPUs.

</details>


### [233] [Influence of Data Dimensionality Reduction Methods on the Effectiveness of Quantum Machine Learning Models](https://arxiv.org/abs/2511.03320)
*Aakash Ravindra Shinde,Jukka K. Nurminen*

Main category: quant-ph

TL;DR: 分析数据降维方法对QML模型的影响，发现降维会导致性能指标值偏差，不同情况下降维效果有差异。


<details>
  <summary>Details</summary>
Motivation: 解决NISQ量子设备的限制和经典设备模拟大量量子比特的挑战，以及降维方法对大数据集适应性慢的问题。

Method: 在多个生成数据集、量子机器学习算法、量子数据编码方法和数据降维方法上进行实验，用准确率、精度、召回率和F1分数评估模型。

Result: 使用数据降维方法会使性能指标值偏差，不同模型使用和不使用降维在准确率上有14% - 48%的差异，部分降维方法对特定数据嵌入方法和电路构建表现更好。

Conclusion: 数据降维方法会导致对量子机器学习模型实际性能的错误估计，多种因素会加剧该问题。

Abstract: Data dimensionality reduction techniques are often utilized in the
implementation of Quantum Machine Learning models to address two significant
issues: the constraints of NISQ quantum devices, which are characterized by
noise and a limited number of qubits, and the challenge of simulating a large
number of qubits on classical devices. It also raises concerns over the
scalability of these approaches, as dimensionality reduction methods are slow
to adapt to large datasets. In this article, we analyze how data reduction
methods affect different QML models. We conduct this experiment over several
generated datasets, quantum machine algorithms, quantum data encoding methods,
and data reduction methods. All these models were evaluated on the performance
metrics like accuracy, precision, recall, and F1 score. Our findings have led
us to conclude that the usage of data dimensionality reduction methods results
in skewed performance metric values, which results in wrongly estimating the
actual performance of quantum machine learning models. There are several
factors, along with data dimensionality reduction methods, that worsen this
problem, such as characteristics of the datasets, classical to quantum
information embedding methods, percentage of feature reduction, classical
components associated with quantum models, and structure of quantum machine
learning models. We consistently observed the difference in the accuracy range
of 14% to 48% amongst these models, using data reduction and not using it.
Apart from this, our observations have shown that some data reduction methods
tend to perform better for some specific data embedding methodologies and
ansatz constructions.

</details>


<div id='cs.CC'></div>

# cs.CC [[Back]](#toc)

### [234] [Supersimulators](https://arxiv.org/abs/2509.17994)
*Cynthia Dwork,Pranay Tankala*

Main category: cs.CC

TL;DR: 证明每个随机布尔函数都有超模拟器，其输出难被高效区分，还改进了乘积分布计算不可区分性的刻画。


<details>
  <summary>Details</summary>
Motivation: 在复杂度理论、密码学等领域，已有模拟器有应用但存在局限性，需研究更好的模拟器。

Method: 基于Trevisan等人的复杂度理论正则引理，使用图正则文献中的迭代技术，还探讨多精度和多校准预测器。

Result: 证明随机布尔函数存在超模拟器，指出刻画乘积分布计算不可区分性只需多精度，超模拟器能得到更优结果。

Conclusion: 超模拟器可解决已有模拟器的一些问题，在相关刻画中能缩小复杂度差距。

Abstract: We prove that every randomized Boolean function admits a supersimulator: a
randomized polynomial-size circuit whose output on random inputs cannot be
efficiently distinguished from reality with constant advantage, even by
polynomially larger distinguishers. Our result builds on the landmark
complexity-theoretic regularity lemma of Trevisan, Tulsiani and Vadhan (2009),
which, in contrast, provides a simulator that fools smaller distinguishers. We
circumvent lower bounds for the simulator size by letting the distinguisher
size bound vary with the target function, while remaining below an absolute
upper bound independent of the target function. This dependence on the target
function arises naturally from our use of an iteration technique originating in
the graph regularity literature.
  The simulators provided by the regularity lemma and recent refinements
thereof, known as multiaccurate and multicalibrated predictors, respectively,
as per Hebert-Johnson et al. (2018), have previously been shown to have myriad
applications in complexity theory, cryptography, learning theory, and beyond.
We first show that a recent multicalibration-based characterization of the
computational indistinguishability of product distributions actually requires
only (calibrated) multiaccuracy. We then show that supersimulators yield an
even tighter result in this application domain, closing a complexity gap
present in prior versions of the characterization.

</details>


### [235] [Efficient Testing Implies Structured Symmetry](https://arxiv.org/abs/2511.03653)
*Cynthia Dwork,Pranay Tankala*

Main category: cs.CC

TL;DR: 研究从少量样本高效测试布尔函数属性，揭示可高效测试属性与结构化对称的等价性，扩展相关特征，结果可推广到高熵分布测试。


<details>
  <summary>Details</summary>
Motivation: 探究给定少量随机样本时，未知布尔函数哪些属性可被高效测试。

Method: 主要技术工具是超模拟，借助算法公平性文献方法用小电路模拟器近似复杂函数。

Result: 揭示可高效测试属性与结构化对称的等价性；允许部分重叠可减少所需数量；大样本时高效测试器本质是检查与小电路集合的不可区分性；结果可推广到高熵分布测试。

Conclusion: 建立了布尔函数属性高效测试与结构化对称的联系，扩展了相关特征，且结果具有一定普适性。

Abstract: Given a small random sample of $n$-bit strings labeled by an unknown Boolean
function, which properties of this function can be tested computationally
efficiently? We show an equivalence between properties that are efficiently
testable from few samples and properties with structured symmetry, which depend
only on the function's average values on parts of a low-complexity partition of
the domain. Without the efficiency constraint, a similar characterization in
terms of unstructured symmetry was obtained by Blais and Yoshida (2019). Our
main technical tool is supersimulation, which builds on methods from the
algorithmic fairness literature to approximate arbitrarily complex functions by
small-circuit simulators that fool significantly larger distinguishers.
  We extend the characterization along other axes as well. We show that
allowing parts to overlap exponentially reduces their required number,
broadening the scope of the construction from properties testable with $O(\log
n)$ samples to properties testable with $O(n)$ samples. For larger sample
sizes, we show that any efficient tester is essentially checking for
indistinguishability from a bounded collection of small circuits, in the spirit
of a characterization of testable graph properties. Finally, we show that our
results for Boolean function testing generalize to high-entropy distribution
testing on arbitrary domains.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [236] [Addressing prior dependence in hierarchical Bayesian modeling for PTA data analysis I: Methodology and implementation](https://arxiv.org/abs/2511.03667)
*Luigi D'amico,Eleonora Villa,Fatima Modica Bittordo,Aldo Barca,Francesco Alì,Massimo Meneghetti,Luca Naso*

Main category: astro-ph.IM

TL;DR: 针对脉冲星计时阵列（PTA）数据分析等复杂推理任务中高维参数空间和参数强依赖问题，引入重参数化策略，结合归一化流（NFs）和i - nessai，提高统计稳健性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 复杂推理任务在高维参数空间和参数强依赖情况下，传统方法存在模型过参数化和先验敏感性问题，影响计算可处理性和结果可靠性。

Method: 在分层贝叶斯建模框架下，引入重参数化策略，使用归一化流（NFs）对分层先验参数和天体物理感兴趣参数进行去相关，采用i - nessai加速复杂后验探索。

Result: 统一使用NFs提高了统计稳健性和计算效率。

Conclusion: 该方法为PTA分析中的分层贝叶斯推理提供了一种有原则的方法论。

Abstract: Complex inference tasks, such as those encountered in Pulsar Timing Array
(PTA) data analysis, rely on Bayesian frameworks. The high-dimensional
parameter space and the strong interdependencies among astrophysical, pulsar
noise, and nuisance parameters introduce significant challenges for efficient
learning and robust inference. These challenges are emblematic of broader
issues in decision science, where model over-parameterization and prior
sensitivity can compromise both computational tractability and the reliability
of the results. We address these issues in the framework of hierarchical
Bayesian modeling by introducing a reparameterization strategy. Our approach
employs Normalizing Flows (NFs) to decorrelate the parameters governing
hierarchical priors from those of astrophysical interest. The use of NF-based
mappings provides both the flexibility to realize the reparametrization and the
tractability to preserve proper probability densities. We further adopt
i-nessai, a flow-guided nested sampler, to accelerate exploration of complex
posteriors. This unified use of NFs improves statistical robustness and
computational efficiency, providing a principled methodology for addressing
hierarchical Bayesian inference in PTA analysis.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [237] [Generative Hints](https://arxiv.org/abs/2511.02933)
*Andy Dimnaku,Abdullah Yusuf Kavranoğlu,Yaser Abu-Mostafa*

Main category: cs.CV

TL;DR: 提出生成式提示训练方法，利用生成模型生成虚拟示例，半监督训练模型，在多数据集上表现优于标准数据增强。


<details>
  <summary>Details</summary>
Motivation: 现有数据增强方法无法完全捕获不变性属性，仅在训练数据变换上学习。

Method: 提出生成式提示训练方法，利用生成模型近似输入分布生成虚拟示例，半监督训练模型学习提示。

Result: 在多数据集、架构和损失函数上，生成式提示始终优于标准数据增强，在多个基准测试上有精度提升。

Conclusion: 生成式提示方法在学习相同属性时比标准数据增强更有效。

Abstract: Data augmentation is widely used in vision to introduce variation and
mitigate overfitting, through enabling models to learn invariant properties,
such as spatial invariance. However, these properties are not fully captured by
data augmentation alone, since it attempts to learn the property on
transformations of the training data only. We propose generative hints, a
training methodology that directly enforces known invariances in the entire
input space. Our approach leverages a generative model trained on the training
set to approximate the input distribution and generate unlabeled images, which
we refer to as virtual examples. These virtual examples are used to enforce
functional properties known as hints. In generative hints, although the
training dataset is fully labeled, the model is trained in a semi-supervised
manner on both the classification and hint objectives, using the unlabeled
virtual examples to guide the model in learning the desired hint. Across
datasets, architectures, and loss functions, generative hints consistently
outperform standard data augmentation when learning the same property. On
popular fine-grained visual classification benchmarks, we achieved up to 1.78%
top-1 accuracy improvement (0.63% on average) over fine-tuned models with data
augmentation and an average performance boost of 1.286% on the CheXpert X-ray
dataset.

</details>


### [238] [EvtSlowTV -- A Large and Diverse Dataset for Event-Based Depth Estimation](https://arxiv.org/abs/2511.02953)
*Sadiq Layi Macaulay,Nimet Kaygusuz,Simon Hadfield*

Main category: cs.CV

TL;DR: 介绍大规模事件相机数据集EvtSlowTV用于事件深度学习，展示其适用性及对模型泛化能力的提升。


<details>
  <summary>Details</summary>
Motivation: 许多基于事件的深度估计方法受小规模标注数据集限制，难以推广到真实场景，需构建大规模数据集。

Method: 从公开YouTube视频中整理出大规模事件相机数据集EvtSlowTV，采用自监督学习框架。

Result: EvtSlowTV比现有事件数据集大一个数量级，训练能提升模型对复杂场景和运动的泛化能力。

Conclusion: EvtSlowTV适用于自监督学习框架，可挖掘原始事件流的HDR潜力，无需基于帧的标注并保留事件数据异步性。

Abstract: Event cameras, with their high dynamic range (HDR) and low latency, offer a
promising alternative for robust depth estimation in challenging environments.
However, many event-based depth estimation approaches are constrained by
small-scale annotated datasets, limiting their generalizability to real-world
scenarios. To bridge this gap, we introduce EvtSlowTV, a large-scale event
camera dataset curated from publicly available YouTube footage, which contains
more than 13B events across various environmental conditions and motions,
including seasonal hiking, flying, scenic driving, and underwater exploration.
EvtSlowTV is an order of magnitude larger than existing event datasets,
providing an unconstrained, naturalistic setting for event-based depth
learning. This work shows the suitability of EvtSlowTV for a self-supervised
learning framework to capitalise on the HDR potential of raw event streams. We
further demonstrate that training with EvtSlowTV enhances the model's ability
to generalise to complex scenes and motions. Our approach removes the need for
frame-based annotations and preserves the asynchronous nature of event data.

</details>


### [239] [SLIP: Structural-aware Language-Image Pretraining for Vision-Language Alignment](https://arxiv.org/abs/2511.03019)
*Wenbo Lu*

Main category: cs.CV

TL;DR: 提出结构感知语言图像预训练模型SLIP，构建数据集，实验显示其在跨模态任务上优于CLIP。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言预训练方法将图像文本对视为孤立训练样本，忽略了许多领域中丰富的关系结构，受人类编码知识方式启发提出改进。

Method: 引入结构对比损失来对齐模态并建模结构化图中相邻实体关系，构建大规模亚马逊产品共购多模态图数据集。

Result: SLIP在零样本和少样本设置的跨模态检索和分类任务中始终优于CLIP。

Conclusion: 关系监督对跨模态对齐有价值。

Abstract: Vision-Language Pretraining (VLP) has achieved remarkable success across
various downstream tasks, but such gains are largely driven by scaling up on
training data. Yet, literature methods treat image-text pairs as isolated
training examples; this neglects the rich relational structure naturally
present in many domains, such as e-commerce product co-purchase graphs and
social recommendation networks. Inspired by neuroscientific evidence that human
encodes knowledge as relationship cognitive maps, we introduce Structure-aware
Language-Image Pretraining (SLIP). SLIP integrates a structural contrastive
loss to align modalities while also modeling relationships between neighboring
entities in a structured graph. To support this paradigm, we construct a
large-scale Amazon Product Co-purchase Multimodal Graph Dataset, enabling
structured cross-modality supervision at scale. Experiment results show that
SLIP consistently outperforms CLIP on cross-modal retrieval and classification
tasks in both zero-shot and few-shot settings, showing the value of relational
supervision for cross-modal alignment.

</details>


### [240] [Image-Intrinsic Priors for Integrated Circuit Defect Detection and Novel Class Discovery via Self-Supervised Learning](https://arxiv.org/abs/2511.03120)
*Botong. Zhao,Xubin. Wang,Shujing. Lyu,Yue. Lu*

Main category: cs.CV

TL;DR: 提出IC DefectNCD框架用于集成电路缺陷检测和新类别发现，在真实数据集验证性能良好。


<details>
  <summary>Details</summary>
Motivation: 集成电路制造复杂易产生缺陷，有监督方法需大量人工标注，聚类无监督方法性能不稳定。

Method: 开发Self Normal Information Guided IC Defect Detection粗定位缺陷区域，引入自适应二值化策略，设计Self Defect Information Guided IC Defect Classification进行缺陷分类。

Result: 在涵盖三个关键制造阶段、15种缺陷类型的真实数据集上实验，在缺陷检测和未知缺陷分类上表现稳健。

Conclusion: IC DefectNCD框架能有效进行集成电路缺陷检测和新类别发现。

Abstract: Integrated circuit manufacturing is highly complex, comprising hundreds of
process steps. Defects can arise at any stage, causing yield loss and
ultimately degrading product reliability. Supervised methods require extensive
human annotation and struggle with emergent categories and rare, data scarce
defects. Clustering-based unsupervised methods often exhibit unstable
performance due to missing priors. We propose IC DefectNCD, a support set free
framework that leverages Image Intrinsic Priors in IC SEM images for defect
detection and novel class discovery. We first develop Self Normal Information
Guided IC Defect Detection, aggregating representative normal features via a
learnable normal information extractor and using reconstruction residuals to
coarsely localize defect regions. To handle saliency variations across defects,
we introduce an adaptive binarization strategy that produces stable subimages
focused on core defective areas. Finally, we design Self Defect Information
Guided IC Defect Classification, which incorporates a soft mask guided
attention mechanism to inject spatial defect priors into the teacher student
model. This enhances sensitivity to defective regions, suppresses background
interference, and enables recognition and classification of unseen defects. We
validate the approach on a real world dataset spanning three key fabrication
stages and covering 15 defect types. Experiments demonstrate robust performance
on both defect detection and unseen defect classification.

</details>


### [241] [Deploying Rapid Damage Assessments from sUAS Imagery for Disaster Response](https://arxiv.org/abs/2511.03132)
*Thomas Manzini,Priyankari Perali,Robin R. Murphy*

Main category: cs.CV

TL;DR: 本文开发首个用于无人生态系统（sUAS）图像中自动建筑物损坏评估的AI/ML系统，并在飓风灾害中部署应用。


<details>
  <summary>Details</summary>
Motivation: 灾害响应中sUAS收集的图像数据量大，专家难以处理，且现无基于sUAS的实用损坏评估系统。

Method: 开发基于sUAS图像的建筑物损坏评估模型，在最大的灾后sUAS航空图像数据集上训练，并对91名灾害从业者进行操作培训。

Result: 最佳模型在两次飓风响应中约18分钟评估了415座建筑物。

Conclusion: 为灾害中AI/ML用于损坏评估提供实际应用文档和经验教训。

Abstract: This paper presents the first AI/ML system for automating building damage
assessment in uncrewed aerial systems (sUAS) imagery to be deployed
operationally during federally declared disasters (Hurricanes Debby and
Helene). In response to major disasters, sUAS teams are dispatched to collect
imagery of the affected areas to assess damage; however, at recent disasters,
teams collectively delivered between 47GB and 369GB of imagery per day,
representing more imagery than can reasonably be transmitted or interpreted by
subject matter experts in the disaster scene, thus delaying response efforts.
To alleviate this data avalanche encountered in practice, computer vision and
machine learning techniques are necessary. While prior work has been deployed
to automatically assess damage in satellite imagery, there is no current state
of practice for sUAS-based damage assessment systems, as all known work has
been confined to academic settings. This work establishes the state of practice
via the development and deployment of models for building damage assessment
with sUAS imagery. The model development involved training on the largest known
dataset of post-disaster sUAS aerial imagery, containing 21,716 building damage
labels, and the operational training of 91 disaster practitioners. The best
performing model was deployed during the responses to Hurricanes Debby and
Helene, where it assessed a combined 415 buildings in approximately 18 minutes.
This work contributes documentation of the actual use of AI/ML for damage
assessment during a disaster and lessons learned to the benefit of the AI/ML
research and user communities.

</details>


### [242] [QG-CoC: Question-Guided Chain-of-Captions for Large Multimodal Models](https://arxiv.org/abs/2511.03206)
*Kuei-Chun Kao,Hsu Tzu-Yin,Yunqi Hong,Ruochen Wang,Cho-Jui Hsieh*

Main category: cs.CV

TL;DR: 本文研究多模态大语言模型在多图像推理任务中的提示方法，提出新的零样本提示方法QG - CoC，实验显示其性能良好。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在多图像上下文存在细粒度感知和信息推理合成问题，现有提示方法多聚焦单图像或特定场景，需解决通用复杂多图像推理任务。

Method: 广泛研究现有提示方法在多图像场景的表现，提出新的零样本提示方法QG - CoC。

Result: 在多种开源和闭源多模态大语言模型的多图像和单图像基准测试中，QG - CoC表现出有竞争力的性能，在现有方法失败的场景有显著提升。

Conclusion: QG - CoC是一种有效的通用提示方法，能处理任意数量图像的问题。

Abstract: Recently, Multimodal Large Language Models (MLLMs) encounter two key issues
in multi-image contexts: (1) a lack of fine-grained perception across disparate
images, and (2) a diminished capability to effectively reason over and
synthesize information from multiple visual inputs. However, while various
prompting methods aim to describe visual content, many existing studies focus
primarily on single-image settings or specific, constrained scenarios. This
leaves a critical gap in understanding and addressing how MLLMs tackle more
general and complex multi-image reasoning tasks. Thus, we first extensively
investigate how current prompting methods perceive fine-grained visual details
and process visual information when dealing with multiple images. Our findings
reveal that existing prompting methods fall short in attending to needed clues
and seamlessly integrating perception and reasoning. Inspired by the findings,
we propose a new zero-shot prompting method, Question-Guided Chain-of-Captions
(QG-CoC), a generalized prompting approach that effectively handles problems
with an arbitrary number of images. We evaluate our method on various
open-source and closed-source MLLMs for multi-image and single-image
benchmarks. Experimental results indicate that QG-CoC demonstrates competitive
performance across tasks and exhibits robust improvements in the challenging
scenarios where existing prompting methods fail.

</details>


### [243] [Generative deep learning for foundational video translation in ultrasound](https://arxiv.org/abs/2511.03255)
*Nikolina Tomic Roshni Bhatnagar,Sarthak Jain,Connor Lau,Tien-Yu Liu,Laura Gambini,Rima Arnaout*

Main category: cs.CV

TL;DR: 提出超声CFD - 灰度视频翻译生成方法，测试效果好，拓展影像效用。


<details>
  <summary>Details</summary>
Motivation: 深度学习用于医学影像需关注数据不平衡和缺失问题，超声数据各子模态在临床研究中常不平衡，图像翻译有挑战。

Method: 开发生成方法，利用像素级、对抗性和感知损失，使用两个网络分别重建解剖结构和去噪。在54,975个视频上训练，8,368个视频上测试。

Result: 合成视频与真实视频平均成对SSIM为0.91±0.04；在DL分类和分割任务及专家评估中表现相近；模型在多个临床领域超声上效果好。

Conclusion: 该方法拓展了回顾性收集影像的效用，丰富了医学影像数据集设计工具。

Abstract: Deep learning (DL) has the potential to revolutionize image acquisition and
interpretation across medicine, however, attention to data imbalance and
missingness is required. Ultrasound data presents a particular challenge
because in addition to different views and structures, it includes several
sub-modalities-such as greyscale and color flow doppler (CFD)-that are often
imbalanced in clinical studies. Image translation can help balance datasets but
is challenging for ultrasound sub-modalities to date. Here, we present a
generative method for ultrasound CFD-greyscale video translation, trained on
54,975 videos and tested on 8,368. The method developed leveraged pixel-wise,
adversarial, and perceptual loses and utilized two networks: one for
reconstructing anatomic structures and one for denoising to achieve realistic
ultrasound imaging. Average pairwise SSIM between synthetic videos and ground
truth was 0.91+/-0.04. Synthetic videos performed indistinguishably from real
ones in DL classification and segmentation tasks and when evaluated by blinded
clinical experts: F1 score was 0.9 for real and 0.89 for synthetic videos; Dice
score between real and synthetic segmentation was 0.97. Overall clinician
accuracy in distinguishing real vs synthetic videos was 54+/-6% (42-61%),
indicating realistic synthetic videos. Although trained only on heart videos,
the model worked well on ultrasound spanning several clinical domains (average
SSIM 0.91+/-0.05), demonstrating foundational abilities. Together, these data
expand the utility of retrospectively collected imaging and augment the dataset
design toolbox for medical imaging.

</details>


### [244] [Hybrid Convolution and Vision Transformer NAS Search Space for TinyML Image Classification](https://arxiv.org/abs/2511.02992)
*Mikhael Djajapermana,Moritz Reiber,Daniel Mueller-Gritschneder,Ulf Schlichtmann*

Main category: cs.CV

TL;DR: 提出新的CNN - ViT搜索空间用于NAS，在CIFAR10数据集实验表明可产生优于ResNet的架构。


<details>
  <summary>Details</summary>
Motivation: 现有CNN - ViT混合架构参数多、计算成本大，不适合tinyML部署，需寻找高效混合架构。

Method: 引入新的CNN - ViT搜索空间用于NAS，该空间涵盖混合CNN和ViT块以及可搜索池化层的池化块。

Result: 在CIFAR10数据集上，提出的搜索空间能在严格模型大小约束下产生准确率和推理速度优于基于ResNet的tinyML模型的混合CNN - ViT架构。

Conclusion: 提出的搜索空间可有效为图像分类找到高效的混合CNN - ViT架构。

Abstract: Hybrids of Convolutional Neural Network (CNN) and Vision Transformer (ViT)
have outperformed pure CNN or ViT architecture. However, since these
architectures require large parameters and incur large computational costs,
they are unsuitable for tinyML deployment. This paper introduces a new hybrid
CNN-ViT search space for Neural Architecture Search (NAS) to find efficient
hybrid architectures for image classification. The search space covers hybrid
CNN and ViT blocks to learn local and global information, as well as the novel
Pooling block of searchable pooling layers for efficient feature map reduction.
Experimental results on the CIFAR10 dataset show that our proposed search space
can produce hybrid CNN-ViT architectures with superior accuracy and inference
speed to ResNet-based tinyML models under tight model size constraints.

</details>


### [245] [Decoupling Augmentation Bias in Prompt Learning for Vision-Language Models](https://arxiv.org/abs/2511.03367)
*Gahyeon Kim,Sohee Kim,Seokju Lee*

Main category: cs.CV

TL;DR: 本文探讨图像增强对提示学习的作用，提出AAPL方法，在多数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有提示学习模型难泛化到未见类别，且图像增强潜力未被充分挖掘，现有方法缺乏学习有语义视觉特征提示的明确指导。

Method: 提出AAPL方法，引入对抗性标记嵌入，将增强带来的表面视觉变化与类别相关语义表示解耦。

Result: 在十一个基准数据集上，AAPL在少样本、零样本、跨数据集和领域泛化设置中均优于现有方法。

Conclusion: 图像增强可支持和增强提示学习，AAPL方法有效提升泛化能力。

Abstract: Recent advances in large-scale vision and language models have led to
significant progress in zero-shot learning tasks. Methods such as CoOp and
CoCoOp have shown that replacing handcrafted prompts with learnable vectors,
known as prompt learning, can result in improved performance. However, these
models often struggle to generalize to entirely unseen categories. While
traditional zero-shot learning techniques benefit from various data
augmentation strategies, prompt learning has primarily focused on text-based
modifications, leaving the potential of image-based augmentation largely
unexplored. In this work, we explore how image-level augmentations,
particularly those that introduce attribute-specific variations, can support
and enhance prompt learning. Our analysis examines the interaction between
these augmentations and soft prompt frameworks, revealing their potential to
improve generalization. We also identify a limitation in existing methods, such
as CoCoOp, which do not provide explicit guidance for learning prompts that
focus on semantically meaningful visual features. To address this, we propose
Adding Attributes to Prompt Learning, AAPL, a novel method that introduces
adversarial token embeddings to decouple superficial visual variations
introduced by augmentation from class-relevant semantic representations. This
decoupling enables the learned prompts to concentrate on visually
discriminative features that align with the target categories. We conduct
comprehensive experiments on eleven benchmark datasets, and AAPL consistently
outperforms existing methods across few-shot, zero-shot, cross-dataset, and
domain generalization settings. Our source code is publicly available at:
https://github.com/Gahyeonkim09/AAPL

</details>


### [246] [From Propagation to Prediction: Point-level Uncertainty Evaluation of MLS Point Clouds under Limited Ground Truth](https://arxiv.org/abs/2511.03053)
*Ziyang Xu,Olaf Wysocki,Christoph Holst*

Main category: cs.CV

TL;DR: 本文提出基于学习的框架评估移动激光扫描点云不确定性，实验证明可行，表明不确定性可学习。


<details>
  <summary>Details</summary>
Motivation: 在许多高精度应用中评估移动激光扫描点云不确定性很关键，但获取评估所需的地面真值成本高且在现实中不可行，需减少对地面真值的依赖。

Method: 提出一个基于学习的框架，将最优邻域估计与几何特征提取相结合。

Result: 在真实数据集上实验表明，所提框架可行，XGBoost模型与随机森林精度相当，但效率高约3倍，证明可用几何特征预测点级不确定性。

Conclusion: 移动激光扫描点云的不确定性是可学习的，为不确定性评估研究提供了新的基于学习的视角。

Abstract: Evaluating uncertainty is critical for reliable use of Mobile Laser Scanning
(MLS) point clouds in many high-precision applications such as Scan-to-BIM,
deformation analysis, and 3D modeling. However, obtaining the ground truth (GT)
for evaluation is often costly and infeasible in many real-world applications.
To reduce this long-standing reliance on GT in uncertainty evaluation research,
this study presents a learning-based framework for MLS point clouds that
integrates optimal neighborhood estimation with geometric feature extraction.
Experiments on a real-world dataset show that the proposed framework is
feasible and the XGBoost model delivers fully comparable accuracy to Random
Forest while achieving substantially higher efficiency (about 3 times faster),
providing initial evidence that geometric features can be used to predict
point-level uncertainty quantified by the C2C distance. In summary, this study
shows that MLS point clouds' uncertainty is learnable, offering a novel
learning-based viewpoint towards uncertainty evaluation research.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [247] [Two thousand years of the oracle problem. Insights from Ancient Delphi on the future of blockchain oracles](https://arxiv.org/abs/2511.03319)
*Giulio Caldarelli,Massimiliano Ornaghi*

Main category: cs.CR

TL;DR: 本文将德尔斐神谕与现代区块链预言机概念关联，构建比较框架，通过词法分析研究神谕答案质量与问题类型关系，为提升区块链预言机可靠性及解读古代神谕机制提供策略和框架。


<details>
  <summary>Details</summary>
Motivation: 解决神谕问题，即代理无法确定神谕信息真实性和无偏性，尤其是现代区块链预言机因去中心化带来的复杂性问题。

Method: 构建德尔斐神谕与现代区块链预言机的比较框架，利用区块链预言机分类法对167条德尔斐查询进行词法分析。

Result: 揭示了古典和计算神谕的共性，有助于各领域的神谕分析。

Conclusion: 为计算机科学文献提出基于德尔斐见解提升区块链预言机可靠性的策略，为古典文献引入可用于解读和分类其他古代神谕机制的框架。

Abstract: The oracle problem refers to the inability of an agent to know if the
information coming from an oracle is authentic and unbiased. In ancient times,
philosophers and historians debated on how to evaluate, increase, and secure
the reliability of oracle predictions, particularly those from Delphi, which
pertained to matters of state. Today, we refer to data carriers for automatic
machines as oracles, but establishing a secure channel between these oracles
and the real world still represents a challenge. Despite numerous efforts, this
problem remains mostly unsolved, and the recent advent of blockchain oracles
has added a layer of complexity because of the decentralization of blockchains.
This paper conceptually connects Delphic and modern blockchain oracles,
developing a comparative framework. Leveraging blockchain oracle taxonomy,
lexical analysis is also performed on 167 Delphic queries to shed light on the
relationship between oracle answer quality and question type. The presented
framework aims first at revealing commonalities between classical and
computational oracles and then at enriching the oracle analysis within each
field. This study contributes to the computer science literature by proposing
strategies to improve the reliability of blockchain oracles based on insights
from Delphi and to classical literature by introducing a framework that can
also be applied to interpret and classify other ancient oracular mechanisms.

</details>


### [248] [Proof-of-Spiking-Neurons(PoSN): Neuromorphic Consensus for Next-Generation Blockchains](https://arxiv.org/abs/2511.02868)
*M. Z. Haider,M. U Ghouri,Tayyaba Noreen,M. Salman*

Main category: cs.CR

TL;DR: 本文提出受脉冲神经网络启发的PoSN共识协议，实现低能耗并行共识，实验显示在能效、吞吐量和收敛性上优于PoB和PoR，为可持续区块链奠定基础。


<details>
  <summary>Details</summary>
Motivation: 现有区块链共识协议如PoW和PoS存在资源消耗大或易中心化问题，需解决区块链系统可扩展性、延迟和能源效率问题。

Method: 提出PoSN协议，将交易编码为脉冲序列，通过竞争激发动态选举领导者，经神经同步确定区块，在神经形态平台实现混合系统架构，借助Nengo和PyNN等仿真框架。

Result: 实验表明PoSN在能源效率、吞吐量和收敛性方面比PoB和PoR有显著提升。

Conclusion: PoSN为适用于物联网、边缘和大规模分布式系统的可持续、自适应区块链奠定了基础。

Abstract: Blockchain systems face persistent challenges of scalability, latency, and
energy inefficiency. Existing consensus protocols such as Proof-of-Work (PoW)
and Proof-of-Stake (PoS) either consume excessive resources or risk
centralization. This paper proposes \textit{Proof-of-Spiking-Neurons (PoSN)}, a
neuromorphic consensus protocol inspired by spiking neural networks. PoSN
encodes transactions as spike trains, elects leaders through competitive firing
dynamics, and finalizes blocks via neural synchronization, enabling parallel
and event-driven consensus with minimal energy overhead. A hybrid system
architecture is implemented on neuromorphic platforms, supported by simulation
frameworks such as Nengo and PyNN. Experimental results show significant gains
in energy efficiency, throughput, and convergence compared to PoB and PoR. PoSN
establishes a foundation for sustainable, adaptive blockchains suitable for
IoT, edge, and large-scale distributed systems.

</details>


### [249] [Exploratory Analysis of Cyberattack Patterns on E-Commerce Platforms Using Statistical Methods](https://arxiv.org/abs/2511.03020)
*Fatimo Adenike Adeniya*

Main category: cs.CR

TL;DR: 研究提出混合分析框架检测和预测电商领域网络攻击模式，发现高危期攻击高峰，CatBoost模型表现最佳，研究有启示但也有局限。


<details>
  <summary>Details</summary>
Motivation: 电商平台网络攻击日益复杂，威胁消费者信任和运营连续性，需检测和预测攻击模式。

Method: 集成统计建模和机器学习，用VCDB数据集，Auto ARIMA做时间预测和显著性测试，ANOVA分析季节变化，用XGBoost、LightGBM和CatBoost做预测分类。

Result: 高危期有攻击高峰，涉及PII的攻击威胁指标高，CatBoost性能最佳。

Conclusion: 框架结合季节预测和可解释集成学习，虽有不足但为主动分配安全资源提供见解，指明实时威胁检测研究方向。

Abstract: Cyberattacks on e-commerce platforms have grown in sophistication,
threatening consumer trust and operational continuity. This research presents a
hybrid analytical framework that integrates statistical modelling and machine
learning for detecting and forecasting cyberattack patterns in the e-commerce
domain. Using the Verizon Community Data Breach (VCDB) dataset, the study
applies Auto ARIMA for temporal forecasting and significance testing, including
a Mann-Whitney U test (U = 2579981.5, p = 0.0121), which confirmed that holiday
shopping events experienced significantly more severe cyberattacks than
non-holiday periods. ANOVA was also used to examine seasonal variation in
threat severity, while ensemble machine learning models (XGBoost, LightGBM, and
CatBoost) were employed for predictive classification. Results reveal recurrent
attack spikes during high-risk periods such as Black Friday and holiday
seasons, with breaches involving Personally Identifiable Information (PII)
exhibiting elevated threat indicators. Among the models, CatBoost achieved the
highest performance (accuracy = 85.29%, F1 score = 0.2254, ROC AUC = 0.8247).
The framework uniquely combines seasonal forecasting with interpretable
ensemble learning, enabling temporal risk anticipation and breach-type
classification. Ethical considerations, including responsible use of sensitive
data and bias assessment, were incorporated. Despite class imbalance and
reliance on historical data, the study provides insights for proactive
cybersecurity resource allocation and outlines directions for future real-time
threat detection research.

</details>


### [250] [Watermarking Large Language Models in Europe: Interpreting the AI Act in Light of Technology](https://arxiv.org/abs/2511.03641)
*Thomas Souverain*

Main category: cs.CR

TL;DR: 本文应对欧盟AI法案中大型语言模型水印评估难题，提出水印分类、评估和比较三方面贡献，建议对嵌入LLM底层架构的水印进一步研究。


<details>
  <summary>Details</summary>
Motivation: 欧盟AI法案要求标记和检测通用模型输出，但LLM水印快速发展且多样，难以将法案的四个标准转化为具体可衡量评估。

Method: 提出按LLM生命周期阶段的水印分类方法；将欧盟AI法案要求与现有水印评估标准映射，并提出互操作性评估的三个规范维度；用欧盟标准比较当前LLM水印方法。

Result: 当前没有一种水印方法能满足欧盟法案的四个标准。

Conclusion: 建议对直接嵌入LLM底层架构的水印进行更多研究。

Abstract: To foster trustworthy Artificial Intelligence (AI) within the European Union,
the AI Act requires providers to mark and detect the outputs of their
general-purpose models. The Article 50 and Recital 133 call for marking methods
that are ''sufficiently reliable, interoperable, effective and robust''. Yet,
the rapidly evolving and heterogeneous landscape of watermarks for Large
Language Models (LLMs) makes it difficult to determine how these four standards
can be translated into concrete and measurable evaluations. Our paper addresses
this challenge, anchoring the normativity of European requirements in the
multiplicity of watermarking techniques. Introducing clear and distinct
concepts on LLM watermarking, our contribution is threefold. (1) Watermarking
Categorisation: We propose an accessible taxonomy of watermarking methods
according to the stage of the LLM lifecycle at which they are applied - before,
during, or after training, and during next-token distribution or sampling. (2)
Watermarking Evaluation: We interpret the EU AI Act's requirements by mapping
each criterion with state-of-the-art evaluations on robustness and
detectability of the watermark, and of quality of the LLM. Since
interoperability remains largely untheorised in LLM watermarking research, we
propose three normative dimensions to frame its assessment. (3) Watermarking
Comparison: We compare current watermarking methods for LLMs against the
operationalised European criteria and show that no approach yet satisfies all
four standards. Encouraged by emerging empirical tests, we recommend further
research into watermarking directly embedded within the low-level architecture
of LLMs.

</details>


### [251] [Death by a Thousand Prompts: Open Model Vulnerability Analysis](https://arxiv.org/abs/2511.03247)
*Amy Chang,Nicholas Conley,Harish Santhanalakshmi Ganesan,Adam Swanda*

Main category: cs.CR

TL;DR: 测试八个开源权重大型语言模型的安全态势，发现普遍存在漏洞，建议采用安全优先设计和分层保护。


<details>
  <summary>Details</summary>
Motivation: 识别可能影响后续微调与部署的开源权重大型语言模型的漏洞。

Method: 使用自动化对抗测试，测量模型对单轮和多轮提示注入及越狱攻击的抵御能力。

Result: 所有测试模型都有普遍漏洞，多轮攻击成功率 25.86% - 92.78%，能力导向模型多轮易受攻击，安全导向模型表现更平衡。

Conclusion: 开源权重模型虽对创新重要，但无分层安全控制部署有风险，建议采用安全优先设计和分层保护确保安全部署。

Abstract: Open-weight models provide researchers and developers with accessible
foundations for diverse downstream applications. We tested the safety and
security postures of eight open-weight large language models (LLMs) to identify
vulnerabilities that may impact subsequent fine-tuning and deployment. Using
automated adversarial testing, we measured each model's resilience against
single-turn and multi-turn prompt injection and jailbreak attacks. Our findings
reveal pervasive vulnerabilities across all tested models, with multi-turn
attacks achieving success rates between 25.86\% and 92.78\% -- representing a
$2\times$ to $10\times$ increase over single-turn baselines. These results
underscore a systemic inability of current open-weight models to maintain
safety guardrails across extended interactions. We assess that alignment
strategies and lab priorities significantly influence resilience:
capability-focused models such as Llama 3.3 and Qwen 3 demonstrate higher
multi-turn susceptibility, whereas safety-oriented designs such as Google Gemma
3 exhibit more balanced performance.
  The analysis concludes that open-weight models, while crucial for innovation,
pose tangible operational and ethical risks when deployed without layered
security controls. These findings are intended to inform practitioners and
developers of the potential risks and the value of professional AI security
solutions to mitigate exposure. Addressing multi-turn vulnerabilities is
essential to ensure the safe, reliable, and responsible deployment of
open-weight LLMs in enterprise and public domains. We recommend adopting a
security-first design philosophy and layered protections to ensure resilient
deployments of open-weight models.

</details>


### [252] [Whisper Leak: a side-channel attack on Large Language Models](https://arxiv.org/abs/2511.03675)
*Geoff McDonald,Jonathan Bar Or*

Main category: cs.CR

TL;DR: 本文提出Whisper Leak侧信道攻击，可从加密大语言模型流量中推断用户提示主题，展示攻击效果，评估三种缓解策略并与供应商合作采取对策，强调解决元数据泄漏问题的必要性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在隐私至关重要的敏感领域广泛应用，需研究其加密流量中是否存在隐私泄漏风险。

Method: 提出Whisper Leak侧信道攻击，分析流式响应中的数据包大小和时间模式来推断用户提示主题；评估随机填充、令牌批处理和数据包注入三种缓解策略。

Result: 在28个流行大语言模型上攻击实现接近完美的分类（AUPRC常>98%），在极端类别不平衡下也有高精度；三种缓解策略均能降低攻击有效性，但都无法提供完全保护。

Conclusion: 大语言模型元数据泄漏是行业普遍漏洞，提供商需解决元数据泄漏问题以处理敏感信息。

Abstract: Large Language Models (LLMs) are increasingly deployed in sensitive domains
including healthcare, legal services, and confidential communications, where
privacy is paramount. This paper introduces Whisper Leak, a side-channel attack
that infers user prompt topics from encrypted LLM traffic by analyzing packet
size and timing patterns in streaming responses. Despite TLS encryption
protecting content, these metadata patterns leak sufficient information to
enable topic classification. We demonstrate the attack across 28 popular LLMs
from major providers, achieving near-perfect classification (often >98% AUPRC)
and high precision even at extreme class imbalance (10,000:1 noise-to-target
ratio). For many models, we achieve 100% precision in identifying sensitive
topics like "money laundering" while recovering 5-20% of target conversations.
This industry-wide vulnerability poses significant risks for users under
network surveillance by ISPs, governments, or local adversaries. We evaluate
three mitigation strategies - random padding, token batching, and packet
injection - finding that while each reduces attack effectiveness, none provides
complete protection. Through responsible disclosure, we have collaborated with
providers to implement initial countermeasures. Our findings underscore the
need for LLM providers to address metadata leakage as AI systems handle
increasingly sensitive information.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [253] [SyMuPe: Affective and Controllable Symbolic Music Performance](https://arxiv.org/abs/2511.03425)
*Ilya Borovik,Dmitrii Gavrilev,Vladimir Viro*

Main category: cs.SD

TL;DR: 提出SyMuPe框架开发训练情感可控的符号钢琴演奏模型PianoFlow，表现优于其他方法，可集成到交互应用。


<details>
  <summary>Details</summary>
Motivation: 通过机器学习模型实现类似人类的音乐表演表达和情感渲染是一项具有挑战性的任务。

Method: 提出SyMuPe框架，旗舰模型PianoFlow使用条件流匹配解决多掩码表演修复任务，使用2968小时对齐的乐谱和MIDI表演数据集训练，集成钢琴表演情感分类器，用情感加权的Flan - T5文本嵌入进行调优。

Result: 与基于Transformer的基线和现有模型的客观和主观评估表明，PianoFlow不仅优于其他方法，而且性能质量与人类录制和转录的MIDI样本相当。

Conclusion: 开发的模型可集成到交互式应用中，有助于创建更易访问和吸引人的音乐表演系统。

Abstract: Emotions are fundamental to the creation and perception of music
performances. However, achieving human-like expression and emotion through
machine learning models for performance rendering remains a challenging task.
In this work, we present SyMuPe, a novel framework for developing and training
affective and controllable symbolic piano performance models. Our flagship
model, PianoFlow, uses conditional flow matching trained to solve diverse
multi-mask performance inpainting tasks. By design, it supports both
unconditional generation and infilling of music performance features. For
training, we use a curated, cleaned dataset of 2,968 hours of aligned musical
scores and expressive MIDI performances. For text and emotion control, we
integrate a piano performance emotion classifier and tune PianoFlow with the
emotion-weighted Flan-T5 text embeddings provided as conditional inputs.
Objective and subjective evaluations against transformer-based baselines and
existing models show that PianoFlow not only outperforms other approaches, but
also achieves performance quality comparable to that of human-recorded and
transcribed MIDI samples. For emotion control, we present and analyze samples
generated under different text conditioning scenarios. The developed model can
be integrated into interactive applications, contributing to the creation of
more accessible and engaging music performance systems.

</details>


<div id='physics.bio-ph'></div>

# physics.bio-ph [[Back]](#toc)

### [254] [Digitizing Spermatogenesis Lineage at Nanoscale Resolution In Tissue-Level Electron Microscopy](https://arxiv.org/abs/2511.02860)
*Li Xiao,Liqing Liu,Hongjun Wu,Jiayi Zhong,Yan Zhang,Junjie Hu,Sun Fei,Ge Yang,Tao Xu*

Main category: physics.bio-ph

TL;DR: 开发DeepOrganelle工具对组织水平细胞器进行分析，在生精上皮周期研究中发现细胞器相互作用时空梯度等现象。


<details>
  <summary>Details</summary>
Motivation: 满足在组织水平表征细胞内细胞器及其相互作用的需求，推动细胞图谱内容革新。

Method: 采用轻量级Mask2Former框架作为通用分割器，对不同细胞类型内细胞器进行分割、提取、统计定量分析等。

Result: 系统地对生精上皮周期中膜接触位点动态进行跨尺度量化，发现生殖细胞分化图谱时空梯度，线粒体 - 内质网接触波动模式，以及支持细胞中血睾屏障动态重组和细胞器拓扑结构阶段特异性重组。

Conclusion: DeepOrganelle可用于组织尺度不同细胞类型细胞器形态和相互作用的分析，揭示细胞发育中的相关现象。

Abstract: Recent advances in 2D large-scale and 3D volume electron microscopy have
stimulated the rapid development of nanoscale functional analysis at the tissue
and organ levels. Digitizing the cell by mapping the intricate organellar
networks into its physiological and pathological textures will revolutionarize
the contents of cell atlases. To meet the requirements of characterizing
intracellular organelles and their interactions within defined cellular cohorts
at tissue level, we have developed DeepOrganelle. It adopts a lightweighted
Mask2Former frameworks as a universal segmentor and is capable of segmenting
and extracting organelles within different cell types, performing statistical
quantitative analysis, as well as visualizing and quantifying the spatial
distribution of organelle morphologies and interactions across different cell
types at tissue scales. Using DeepOrganelle, we systemically perform
cross-scale quantification of membrane contact sites(MCSs) dynamics across the
progression of the seminiferous epithelial cycle, covering 12 distinct
developmental stages and 24 statuses of germ cells. DeepOrganelle uncovers the
spatiotemporal gradient of the germ cell differentiation atlas according to
different types of organelles and their interactions. Noticeably, it discovers
a waved pattern of mitochondria(Mito)-endoplasmic reticulum(ER) contact with a
significant increase specifically at Stage X pachytene preceding the transition
to diplotene, which aligns well with a newly reported experiment that
mitochondrial metabolic proteins like PDHA2 are essential for this transition
by maintaining ATP supply for double-strand break(DSB) repair. DeepOrganelle
also observes a dynamic restructuring of the blood-testis barrier and
stage-specific reorganization of organelle topography in Sertoli cells from
preleptotene to leptotene phases of prophase I.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [255] [Toward an Agricultural Operational Design Domain: A Framework](https://arxiv.org/abs/2511.02937)
*Mirco Felske,Jannik Redenius,Georg Happich,Julius Schöning*

Main category: cs.RO

TL;DR: 论文引入农业ODD框架解决现有ODD概念无法应对农业应用挑战的问题，介绍框架构成并展示其对农业自动化系统环境描述标准化和可扩展性的支持。


<details>
  <summary>Details</summary>
Motivation: 现有ODD概念不能满足农业自动化在复杂多变环境下开发和验证时对环境结构化描述的需求。

Method: 引入农业ODD框架，包括描述概念、7层模型和迭代验证过程。

Result: 框架各元素提供创建明确可验证农业ODD的一致方法，演示用例展示其对环境描述标准化和可扩展性的支持。

Conclusion: 农业ODD框架能解决农业应用中现有ODD概念的不足，利于农业自动化系统环境描述的标准化和可扩展。

Abstract: The agricultural sector increasingly relies on autonomous systems that
operate in complex and variable environments. Unlike on-road applications,
agricultural automation integrates driving and working processes, each of which
imposes distinct operational constraints. Handling this complexity and ensuring
consistency throughout the development and validation processes requires a
structured, transparent, and verified description of the environment. However,
existing Operational Design Domain (ODD) concepts do not yet address the unique
challenges of agricultural applications.
  Therefore, this work introduces the Agricultural ODD (Ag-ODD) Framework,
which can be used to describe and verify the operational boundaries of
autonomous agricultural systems. The Ag-ODD Framework consists of three core
elements. First, the Ag-ODD description concept, which provides a structured
method for unambiguously defining environmental and operational parameters
using concepts from ASAM Open ODD and CityGML. Second, the 7-Layer Model
derived from the PEGASUS 6-Layer Model, has been extended to include a process
layer to capture dynamic agricultural operations. Third, the iterative
verification process verifies the Ag-ODD against its corresponding logical
scenarios, derived from the 7-Layer Model, to ensure the Ag-ODD's completeness
and consistency.
  Together, these elements provide a consistent approach for creating
unambiguous and verifiable Ag-ODD. Demonstrative use cases show how the Ag-ODD
Framework can support the standardization and scalability of environmental
descriptions for autonomous agricultural systems.

</details>


### [256] [ROSBag MCP Server: Analyzing Robot Data with LLMs for Agentic Embodied AI Applications](https://arxiv.org/abs/2511.03497)
*Lei Fu,Sahar Salimpour,Leonardo Militano,Harry Edelman,Jorge Peña Queralta,Giovanni Toffetti*

Main category: cs.RO

TL;DR: 本文介绍用于分析ROS和ROS 2包的MCP服务器，结合轻量级UI对不同LLM进行基准测试，实验表明不同模型工具调用能力有差异，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有关于Agentic Embodied AI的文献稀缺，需开发相关工具分析和处理机器人数据。

Method: 引入MCP服务器分析ROS和ROS 2包，构建有机器人领域知识的工具，提供轻量级UI对不同LLM进行基准测试。

Result: 对八种不同的LLM/VLM模型工具调用能力分析，Kimi K2和Claude Sonnet 4表现明显更优。

Conclusion: 模型工具调用成功率受工具描述模式、参数数量和可用工具数量等多种因素影响。

Abstract: Agentic AI systems and Physical or Embodied AI systems have been two key
research verticals at the forefront of Artificial Intelligence and Robotics,
with Model Context Protocol (MCP) increasingly becoming a key component and
enabler of agentic applications. However, the literature at the intersection of
these verticals, i.e., Agentic Embodied AI, remains scarce. This paper
introduces an MCP server for analyzing ROS and ROS 2 bags, allowing for
analyzing, visualizing and processing robot data with natural language through
LLMs and VLMs. We describe specific tooling built with robotics domain
knowledge, with our initial release focused on mobile robotics and supporting
natively the analysis of trajectories, laser scan data, transforms, or time
series data. This is in addition to providing an interface to standard ROS 2
CLI tools ("ros2 bag list" or "ros2 bag info"), as well as the ability to
filter bags with a subset of topics or trimmed in time. Coupled with the MCP
server, we provide a lightweight UI that allows the benchmarking of the tooling
with different LLMs, both proprietary (Anthropic, OpenAI) and open-source
(through Groq). Our experimental results include the analysis of tool calling
capabilities of eight different state-of-the-art LLM/VLM models, both
proprietary and open-source, large and small. Our experiments indicate that
there is a large divide in tool calling capabilities, with Kimi K2 and Claude
Sonnet 4 demonstrating clearly superior performance. We also conclude that
there are multiple factors affecting the success rates, from the tool
description schema to the number of arguments, as well as the number of tools
available to the models. The code is available with a permissive license at
https://github.com/binabik-ai/mcp-rosbags.

</details>


### [257] [Development of the Bioinspired Tendon-Driven DexHand 021 with Proprioceptive Compliance Control](https://arxiv.org/abs/2511.03481)
*Jianbo Yuan,Haohua Zhu,Jing Dai,Sheng Yi*

Main category: cs.RO

TL;DR: 本文介绍高性能电缆驱动五手指机器人手Dex - Hand 021，通过实验展示其优越性能，推动了灵巧手设计与控制发展。


<details>
  <summary>Details</summary>
Motivation: 复制人类手多功能能力、在工程约束下开发灵巧机器人手是挑战，需平衡类人敏捷性与工程限制。

Method: 提出基于本体感受力传感的导纳控制方法，设计Dex - Hand 021，有12个主动和7个被动自由度。

Result: 单指负载超10N，指尖重复性在0.001m内，力估计误差低于0.2N，多目标抓取关节扭矩比PID控制降低31.19%，能完成33种GRASP分类动作和复杂操作任务。

Conclusion: 这项工作推动了轻量级工业级灵巧手设计和本体感受控制，有助于机器人操作和智能制造。

Abstract: The human hand plays a vital role in daily life and industrial applications,
yet replicating its multifunctional capabilities-including motion, sensing, and
coordinated manipulation-with robotic systems remains a formidable challenge.
Developing a dexterous robotic hand requires balancing human-like agility with
engineering constraints such as complexity, size-to-weight ratio, durability,
and force-sensing performance. This letter presents Dex-Hand 021, a
high-performance, cable-driven five-finger robotic hand with 12 active and 7
passive degrees of freedom (DoFs), achieving 19 DoFs dexterity in a lightweight
1 kg design. We propose a proprioceptive force-sensing-based admittance control
method to enhance manipulation. Experimental results demonstrate its superior
performance: a single-finger load capacity exceeding 10 N, fingertip
repeatability under 0.001 m, and force estimation errors below 0.2 N. Compared
to PID control, joint torques in multi-object grasping are reduced by 31.19%,
significantly improves force-sensing capability while preventing overload
during collisions. The hand excels in both power and precision grasps,
successfully executing 33 GRASP taxonomy motions and complex manipulation
tasks. This work advances the design of lightweight, industrial-grade dexterous
hands and enhances proprioceptive control, contributing to robotic manipulation
and intelligent manufacturing.

</details>


### [258] [Multi-User Personalisation in Human-Robot Interaction: Using Quantitative Bipolar Argumentation Frameworks for Preferences Conflict Resolution](https://arxiv.org/abs/2511.03576)
*Aniol Civit,Antonio Andriella,Carles Sierra,Guillem Alenyà*

Main category: cs.RO

TL;DR: 提出多用户偏好定量双极论证框架MUP - QBAF解决多用户HRI偏好冲突问题，经案例验证，推动了多用户HRI领域发展。


<details>
  <summary>Details</summary>
Motivation: 现有HRI个性化方法多关注单用户适应，忽略多用户潜在冲突偏好场景，需解决多用户偏好冲突问题。

Method: 提出基于QBAFs的MUP - QBAF框架，融合用户论点和机器人环境动态观察，迭代重新计算论点强度。

Result: 通过脆弱性评估任务案例验证框架特性和能力，还进行了论点基础分数敏感性分析。

Conclusion: 该工作为解决多用户偏好冲突提供透明、结构化和上下文敏感方法，是数据驱动方法的原则性替代方案，使机器人能处理现实环境冲突。

Abstract: While personalisation in Human-Robot Interaction (HRI) has advanced
significantly, most existing approaches focus on single-user adaptation,
overlooking scenarios involving multiple stakeholders with potentially
conflicting preferences. To address this, we propose the Multi-User Preferences
Quantitative Bipolar Argumentation Framework (MUP-QBAF), a novel multi-user
personalisation framework based on Quantitative Bipolar Argumentation
Frameworks (QBAFs) that explicitly models and resolves multi-user preference
conflicts. Unlike prior work in Argumentation Frameworks, which typically
assumes static inputs, our approach is tailored to robotics: it incorporates
both users' arguments and the robot's dynamic observations of the environment,
allowing the system to adapt over time and respond to changing contexts.
Preferences, both positive and negative, are represented as arguments whose
strength is recalculated iteratively based on new information. The framework's
properties and capabilities are presented and validated through a realistic
case study, where an assistive robot mediates between the conflicting
preferences of a caregiver and a care recipient during a frailty assessment
task. This evaluation further includes a sensitivity analysis of argument base
scores, demonstrating how preference outcomes can be shaped by user input and
contextual observations. By offering a transparent, structured, and
context-sensitive approach to resolving competing user preferences, this work
advances the field of multi-user HRI. It provides a principled alternative to
data-driven methods, enabling robots to navigate conflicts in real-world
environments.

</details>


### [259] [Learning-based Cooperative Robotic Paper Wrapping: A Unified Control Policy with Residual Force Control](https://arxiv.org/abs/2511.03181)
*Rewida Ali,Cristian C. Beltran-Hernandez,Weiwei Wan,Kensuke Harada*

Main category: cs.RO

TL;DR: 本文聚焦礼品包装任务，提出基于学习的框架，结合大语言模型任务规划器与混合学习策略，在实际包装任务中成功率达97%。


<details>
  <summary>Details</summary>
Motivation: 在人机协作处理可变形物体时，因材料动态不可预测和需自适应力控制，协调机器人动作困难，以礼品包装任务为例探索该挑战。

Method: 提出基于学习的框架，将大语言模型驱动的高级任务规划器与低级混合模仿学习和强化学习策略集成，核心是Sub - task Aware Robotic Transformer（START），引入子任务ID。

Result: 框架在实际包装任务中成功率达97%。

Conclusion: 基于统一变压器的策略减少了对专用模型的需求，允许人为监督，有效弥合高级意图与可变形物体操作所需的细粒度力控制之间的差距。

Abstract: Human-robot cooperation is essential in environments such as warehouses and
retail stores, where workers frequently handle deformable objects like paper,
bags, and fabrics. Coordinating robotic actions with human assistance remains
difficult due to the unpredictable dynamics of deformable materials and the
need for adaptive force control. To explore this challenge, we focus on the
task of gift wrapping, which exemplifies a long-horizon manipulation problem
involving precise folding, controlled creasing, and secure fixation of paper.
Success is achieved when the robot completes the sequence to produce a neatly
wrapped package with clean folds and no tears.
  We propose a learning-based framework that integrates a high-level task
planner powered by a large language model (LLM) with a low-level hybrid
imitation learning (IL) and reinforcement learning (RL) policy. At its core is
a Sub-task Aware Robotic Transformer (START) that learns a unified policy from
human demonstrations. The key novelty lies in capturing long-range temporal
dependencies across the full wrapping sequence within a single model. Unlike
vanilla Action Chunking with Transformer (ACT), typically applied to short
tasks, our method introduces sub-task IDs that provide explicit temporal
grounding. This enables robust performance across the entire wrapping process
and supports flexible execution, as the policy learns sub-goals rather than
merely replicating motion sequences.
  Our framework achieves a 97% success rate on real-world wrapping tasks. We
show that the unified transformer-based policy reduces the need for specialized
models, allows controlled human supervision, and effectively bridges high-level
intent with the fine-grained force control required for deformable object
manipulation.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [260] [The Adaptivity Barrier in Batched Nonparametric Bandits: Sharp Characterization of the Price of Unknown Margin](https://arxiv.org/abs/2511.03708)
*Rong Jiang,Cong Ma*

Main category: math.ST

TL;DR: 研究未知margin参数下批量非参数上下文老虎机问题，引入后悔膨胀准则，开发RoBIN算法达最优后悔膨胀，揭示新适应性障碍。


<details>
  <summary>Details</summary>
Motivation: 研究未知margin参数时批量非参数上下文老虎机问题，捕捉因参数未知带来的统计代价。

Method: 引入后悔膨胀准则，通过凸优化问题确定最优后悔膨胀，基于此开发RoBIN算法。

Result: 最优后悔膨胀随时间范围T多项式增长，RoBIN算法达最优后悔膨胀（至对数因子），发现新适应性障碍，批量数超log log T时障碍消失。

Conclusion: 在批量情况下，适应未知margin参数不可避免有多项式惩罚，可由变分问题精确刻画；批量数足够时可恢复至接近神谕后悔率。

Abstract: We study batched nonparametric contextual bandits under a margin condition
when the margin parameter $\alpha$ is unknown. To capture the statistical price
of this ignorance, we introduce the regret inflation criterion, defined as the
ratio between the regret of an adaptive algorithm and that of an oracle knowing
$\alpha$. We show that the optimal regret inflation grows polynomial with the
horizon $T$, with exponent precisely given by the value of a convex
optimization problem involving the dimension, smoothness, and batch budget.
Moreover, the minimizers of this optimization problem directly prescribe the
batch allocation and exploration strategy of a rate-optimal algorithm. Building
on this principle, we develop RoBIN (RObust batched algorithm with adaptive
BINning), which achieves the optimal regret inflation up to logarithmic
factors. These results reveal a new adaptivity barrier: under batching,
adaptation to an unknown margin parameter inevitably incurs a polynomial
penalty, sharply characterized by a variational problem. Remarkably, this
barrier vanishes when the number of batches exceeds $\log \log T$; with only a
doubly logarithmic number of updates, one can recover the oracle regret rate up
to polylogarithmic factors.

</details>


### [261] [The Structure of Cross-Validation Error: Stability, Covariance, and Minimax Limits](https://arxiv.org/abs/2511.03554)
*Ido Nachum,Rüdiger Urbanke,Thomas Weinberger*

Main category: math.ST

TL;DR: 本文研究算法 - 分布对属性如何影响k折交叉验证折数选择，给出均方误差分解，证明相关下界，揭示重采样风险估计的权衡。


<details>
  <summary>Details</summary>
Motivation: 现有交叉验证理论研究存在许多未解决问题，因此研究算法 - 分布对属性对k折交叉验证折数选择的影响。

Method: 对交叉验证均方误差进行新的分解，引入平方损失稳定性概念，并进行理论证明。

Result: 得到交叉验证均方误差的新分解；证明最小最大下界，表明大k值时CV无法达到最优；给出匹配单折留出估计器精度的学习规则。

Conclusion: 重采样风险估计存在权衡，CV不能充分利用所有样本进行无偏风险评估，其最小最大性能介于k/n和√k/n之间。

Abstract: Despite ongoing theoretical research on cross-validation (CV), many
theoretical questions about CV remain widely open. This motivates our
investigation into how properties of algorithm-distribution pairs can affect
the choice for the number of folds in $k$-fold cross-validation.
  Our results consist of a novel decomposition of the mean-squared error of
cross-validation for risk estimation, which explicitly captures the
correlations of error estimates across overlapping folds and includes a novel
algorithmic stability notion, squared loss stability, that is considerably
weaker than the typically required hypothesis stability in other comparable
works.
  Furthermore, we prove:
  1. For every learning algorithm that minimizes empirical error, a minimax
lower bound on the mean-squared error of $k$-fold CV estimating the population
risk $L_\mathcal{D}$: \[ \min_{k \mid n}\; \max_{\mathcal{D}}\;
\mathbb{E}\!\left[\big(\widehat{L}_{\mathrm{CV}}^{(k)} -
L_{\mathcal{D}}\big)^{2}\right] \;=\; \Omega\!\big(\sqrt{k}/n\big), \] where
$n$ is the sample size and $k$ the number of folds. This shows that even under
idealized conditions, for large values of $k$, CV cannot attain the optimum of
order $1/n$ achievable by a validation set of size $n$, reflecting an inherent
penalty caused by dependence between folds.
  2. Complementing this, we exhibit learning rules for which \[
  \max_{\mathcal{D}}\; \mathbb{E}\!\left[\big(\widehat{L}_{\mathrm{CV}}^{(k)} -
L_{\mathcal{D}}\big)^{2}\right] \;=\; \Omega(k/n), \] matching (up to
constants) the accuracy of a hold-out estimator of a single fold of size $n/k$.
  Together these results delineate the fundamental trade-off in
resampling-based risk estimation: CV cannot fully exploit all $n$ samples for
unbiased risk evaluation, and its minimax performance is pinned between the
$k/n$ and $\sqrt{k}/n$ regimes.

</details>


<div id='q-bio.PE'></div>

# q-bio.PE [[Back]](#toc)

### [262] [Improving Gene Trees without more data](https://arxiv.org/abs/2511.03692)
*Ashu Gupta*

Main category: q-bio.PE

TL;DR: 提出新管道WSB+WQMC，通过BestML分析评估其与WSB+CAML，结果显示WSB+WQMC在多数ILS水平下提高树准确性，是有潜力的替代方案。


<details>
  <summary>Details</summary>
Motivation: 现有方法在基因树准确性低时进行物种树估计存在问题，之前对统计分箱管道的测试未采用BestML方法，需新的有效管道。

Method: 提出WSB+WQMC管道，用BestML分析在各种模拟数据集上评估WSB+WQMC与WSB+CAML。

Result: WSB+WQMC在多数低、中、中高ILS水平数据集上提高基因树和物种树准确性；在特定低、中ILS条件下不如WSB+CAML，但在中高和高ILS数据集上表现更好，在部分中低ILS数据集上更适合估计基因树。

Conclusion: WSB+WQMC是WSB+CAML进行系统发育估计的有前景替代方案，尤其在系统发育信号低时。

Abstract: Estimating species and gene trees from sequence data is challenging. Gene
tree estimation is often hampered by low phylogenetic signal in alignments,
leading to inaccurate trees. Species tree estimation is complicated by
incomplete lineage sorting (ILS), where gene histories differ from the species'
history. Summary methods like MP-EST, ASTRAL2, and ASTRID infer species trees
from gene trees but suffer when gene tree accuracy is low. To address this, the
Statistical Binning (SB) and Weighted Statistical Binning (WSB) pipelines were
developed to improve gene tree estimation. However, previous studies only
tested these pipelines using multi-locus bootstrapping (MLBS), not the BestML
approach.
  This thesis proposes a novel pipeline, WSB+WQMC, which shares design features
with the existing WSB+CAML pipeline but has other desirable properties and is
statistically consistent under the GTR+MSC model. This study evaluated WSB+WQMC
against WSB+CAML using BestML analysis on various simulated datasets. The
results confirmed many trends seen in prior MLBS analyses. WSB+WQMC
substantially improved gene tree and species tree accuracy (using ASTRAL2 and
ASTRID) on most datasets with low, medium, and moderately high ILS levels. In a
direct comparison, WSB+WQMC computed less accurate trees than WSB+CAML under
certain low and medium ILS conditions. However, WSB+WQMC performed better or at
least as accurately as WSB+CAML on all datasets with moderately high and high
ILS. It also proved better for estimating gene trees on some medium and low ILS
datasets. Thus, WSB+WQMC is a promising alternative to WSB+CAML for
phylogenetic estimation, especially in the presence of low phylogenetic signal.

</details>


### [263] [Topography, climate, land cover, and biodiversity: Explaining endemic richness and management implications on a Mediterranean island](https://arxiv.org/abs/2511.03242)
*Aristides Moustakas,Ioannis N Vogiatzakis*

Main category: q-bio.PE

TL;DR: 研究克里特岛特有植物丰富度驱动因素，发现总物种丰富度、海拔范围和气候变异性是强预测因子，强调保护规划需优先考虑山区和气候多变地区。


<details>
  <summary>Details</summary>
Motivation: 量化地形、气候和土地覆盖对岛屿特有性的相对贡献。

Method: 利用物种分布、地形、气候等空间明确数据，采用人工神经网络模型评估预测因子重要性并确定特有性热点。

Result: 总物种丰富度、海拔范围和气候变异性是特有丰富度的强预测因子，特有热点与高总物种丰富度区域部分重叠，环境异质区提供重要生态系统服务且受多种威胁。

Conclusion: 保护规划应优先考虑山区和气候多变地区，整合生态系统服务考量，本研究为克里特岛及类似地中海岛屿提供循证保护规划框架。

Abstract: Island endemism is shaped by complex interactions among environmental,
ecological, and evolutionary factors, yet the relative contributions of
topography, climate, and land cover remain incompletely quantified. We
investigated the drivers of endemic plant richness across Crete, a
Mediterranean biodiversity hotspot, using spatially explicit data on species
distributions, topographic complexity, climatic variability, land cover, and
soil characteristics. Artificial Neural Network models, a machine learning
tool, were employed to assess the relative importance of these predictors and
to identify hotspots of endemism. We found that total species richness,
elevation range, and climatic variability were the strongest predictors of
endemic richness, reflecting the role of biodiversity, topographic
heterogeneity, and climatic gradients in generating diverse habitats and
micro-refugia that promote speciation and buffer extinction risk. Endemic
hotspots only partially overlapped with areas of high total species richness,
indicating that total species richness was the optimal from the ones examined,
yet an imperfect surrogate. These environmentally heterogeneous areas also
provide critical ecosystem services, including soil stabilization, pollination,
and cultural value, which are increasingly threatened by tourism, renewable
energy development, land-use change, and climate impacts. Our findings
underscore the importance of prioritizing mountainous and climatically variable
regions in conservation planning, integrating ecosystem service considerations,
and accounting for within-island spatial heterogeneity. By explicitly linking
the environmental drivers of endemism to both biodiversity patterns and
ecosystem function, this study provides a framework for evidence-based
conservation planning in Crete and other Mediterranean islands with similar
geological and biogeographic contexts.

</details>


<div id='astro-ph.CO'></div>

# astro-ph.CO [[Back]](#toc)

### [264] [Quantifying Weighted Morphological Content of Large-Scale Structures via Simulation-Based Inference](https://arxiv.org/abs/2511.03636)
*M. H. Jalali Kanafi,S. M. S. Movahed*

Main category: astro-ph.CO

TL;DR: 本文通过模拟预测分析，比较大尺度结构的两种高阶统计量Minkowski Functionals (MFs)和Conditional Moments of Derivative (CMD)对宇宙学参数的约束能力，发现CMD约束更优，两者结合能进一步提高精度，且相对约束能力较稳定。


<details>
  <summary>Details</summary>
Motivation: 比较大尺度结构的MFs和CMD两种高阶统计量对宇宙学参数的约束能力，研究它们对红移空间中非线性和各向异性特征的敏感性。

Method: 基于Big Sobol Sequence(BSQ)模拟的晕目录，使用无似然推理框架（通过神经后验估计实现）进行模拟预测分析。

Result: 在特定宇宙学模型和光滑尺度下，CMD对宇宙学参数的预测比MFs更精确，两者结合比单独使用MFs精度提高约27%，且在不同参数值和光滑尺度下，各统计量的相对约束能力几乎不变。

Conclusion: CMD对宇宙学参数的约束能力优于MFs，两者结合能提供互补信息以提高精度，且相对约束能力在不同条件下较稳定。

Abstract: In this work, we perform a simulation-based forecasting analysis to compare
the constraining power of two higher-order summary statistics of the
large-scale structure (LSS), the Minkowski Functionals (MFs) and the
Conditional Moments of Derivative (CMD), with a particular focus on their
sensitivity to nonlinear and anisotropic features in redshift-space. Our
analysis relies on halo catalogs from the Big Sobol Sequence(BSQ) simulations
at redshift $z=0.5$, employing a likelihood-free inference framework
implemented via neural posterior estimation. At the fiducial cosmology of the
Quijote simulations $(\Omega_{m}=0.3175,\,\sigma_{8}=0.834)$, and for the
smoothing scale $R=15\,h^{-1}$Mpc, we find that the CMD yields tighter
forecasts for $(\Omega_{m}},\,\sigma_{8})$ than the zeroth- to third-order MFs
components, improving the constraint precision by ${\sim}(44\%,\,52\%)$,
${\sim}(30\%,\,45\%)$, ${\sim}(27\%,\,17\%)$, and ${\sim}(26\%,\,17\%)$,
respectively. A joint configuration combining the MFs and CMD further enhances
the precision by approximately ${\sim}27\%$ compared to the standard MFs alone,
highlighting the complementary anisotropy-sensitive information captured by the
CMD in contrast to the scalar morphological content encapsulated by the MFs. We
further extend the forecasting analysis to a continuous range of cosmological
parameter values and multiple smoothing scales. Our results show that, although
the absolute forecast uncertainty for each component of summary statistics
depends on the underlying parameter values and the adopted smoothing scale, the
relative constraining power among the summary statistics remains nearly
constant throughout.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [265] [Extreme events and public debt dynamics: Lessons from Croatia's experience](https://arxiv.org/abs/2511.02973)
*Luka Draganić,Leonarda Srdelić,Marwil J. Davila-Fernandez*

Main category: econ.GN

TL;DR: 本文用克罗地亚数据和IMF工具评估小国开放经济中公共债务对极端事件的调整，对比不同情景下债务路径，发现无冲击时债务可持续下降，大灾难会逆转趋势，强调财政缓冲重要性。


<details>
  <summary>Details</summary>
Motivation: 评估小国开放经济中公共债务如何应对极端事件，以克罗地亚为例为欧盟新成员国灾后恢复评估提供参考。

Method: 使用克罗地亚数据和IMF自然灾害债务动态工具，对比基线和压力情景下的债务路径，进行确定性和随机性模拟。

Result: 基准预测显示到2040年债务与GDP比率降至55%以下；灾难情景下债务短期激增且长期上升，达GDP的75%。

Conclusion: 无冲击时公共债务可持续下降，严重自然灾害会逆转趋势；强调财政缓冲对吸收冲击的重要性，将自然灾害压力测试融入公共债务分析有创新意义，研究成果对其他面临极端事件的小国开放经济有借鉴意义。

Abstract: Using Croatian data and the IMF's Natural Disaster Debt Dynamic Tool, this
paper assesses how public debt adjusts to extreme events in a small open
economy. We compare debt paths under baseline and stress scenarios, the latter
simulating a major earthquake in 2025. Croatia provides a unique setting for
evaluating post-disaster recovery in countries recently incorporated into the
European Union. Our benchmark projections, which assume moderate economic
growth and a broadly neutral fiscal stance, suggest the debt-to-GDP ratio will
gradually decline to below 55% by 2040. In contrast, in the disaster scenario,
we document a sharp short-term increase and a persistent upward shift in the
debt trajectory, reaching 75% of GDP. Deterministic and stochastic simulations
allow us to assess the distribution of potential outcomes. It is shown that, in
the absence of shocks, public debt is on a sustainable downward path, but a
severe natural disaster could reverse this trend and keep it elevated for
years. Our findings highlight the importance of fiscal buffers that are
critical for creating space to absorb shocks. The paper innovates by
integrating natural disaster stress-testing into public debt analysis, with
implications for fiscal risk management and policy planning. While we focus on
Croatia, the mechanisms we uncover have broader implications for small open
economies exposed to extreme events.

</details>


### [266] [A Computer Vision Based Proxy for Political Polarization in Religious Countries: A Turkiye Case Study](https://arxiv.org/abs/2511.03088)
*Liangze Ke*

Main category: econ.GN

TL;DR: 文章研究用计算机视觉估计群体间距离的新政治极化指标，分析土耳其视频量化其群体差异，发现群体距离与选举极化的关联及两点见解，表明身体距离可作极化指标。


<details>
  <summary>Details</summary>
Motivation: 研究新的政治极化指标，量化土耳其的人口和宗教分歧情况。

Method: 使用先进目标检测技术分析1400多个YouTube视频，通过基于熵的投票指标衡量选举极化。

Result: 发现群体间距离与选举极化有强相关性，宗教与非宗教个体距离大增加选举熵，非宗教个体内部分歧稳定极化。

Conclusion: 身体距离可作为可扩展的极化指标，补充传统经济指标，推动计算社会科学和经济学发展。

Abstract: This paper examines a novel proxy for political polarization, initially
proposed by Caliskan et al., which estimates intergroup distances using
computer vision. Analyzing 1,400+ YouTube videos with advanced object
detection, their study quantifies demographic and religious divides in Turkiye,
a deeply polarized nation. Our findings reveal strong correlations between
intergroup distances and electoral polarization, measured via entropy-based
voting metrics weighted by religiosity and political inclination. Two key
insights emerge: (1) Greater distances between religious and nonreligious
individuals (NRP vs RP) heighten electoral entropy, underscoring sociocultural
fragmentation. (2) Intragroup diversity among nonreligious individuals (NRP vs
NRP) stabilizes polarization, aligning with Axelrod's cultural dissemination
model. This research advances computational social science and economics by
showing that physical distancing serves as a scalable proxy for polarization,
complementing traditional economic indicators.

</details>


### [267] [Gender gap in the desired wages: Evidence from large administrative data](https://arxiv.org/abs/2511.03252)
*Taiyo Fukai,Keisuke Kawata,Mizuki Komura,Takahiro Toriyabe*

Main category: econ.GN

TL;DR: 研究利用公共职位推荐行政数据，分析期望工资性别差距，发现期望职业影响大，对残差做进一步分析。


<details>
  <summary>Details</summary>
Motivation: 分析期望工资的性别差距。

Method: 利用年龄、期望工作地区和期望职业信息进行分解分析，对未解释残差进行异质性和敏感性分析。

Result: 三个因素中期望职业对期望工资差异影响最大，但三个因素外的残差最大。

Conclusion: 研究了期望工资性别差距及各因素影响，对未解释部分做了进一步分析。

Abstract: This study analyzes the gender gap in desired wages using large
administrative data of public job referrals, which allows us to look at the
desired salaries of individuals from a wider wage distribution. We conduct a
decomposition analysis using available information on age, desired work region,
and desired occupation. We find that of the three factors, desired occupation
is the most important in generating differences in desired wages; however, the
residuals are the largest outside of the three factors. To further probe the
unexplained residuals, we also conduct heterogeneity and sensitivity analyses
using the available data.

</details>


### [268] [Duration Dependence and Job Search over the Spell: Evidence from Job Seeker Activity Reports](https://arxiv.org/abs/2511.03377)
*Jonas Cederlöf,Sara Roman*

Main category: econ.GN

TL;DR: 研究求职者在失业期间的求职行为演变和回调的持续期依赖，发现求职努力和回调的变化情况及异质性。


<details>
  <summary>Details</summary>
Motivation: 探究求职者在失业期间求职行为如何演变以及回调的持续期依赖程度。

Method: 利用240万份月度活动报告数据，采用时间和失业期固定效应设计分离失业期内变化和动态选择。

Result: 原始求职努力随失业时长增加，但失业期内求职努力平稳且再就业前急剧下降；失业保险到期时求职努力下降约10%；回调每月下降6%，仅10 - 14%是“真正”的持续期依赖；存在显著异质性。

Conclusion: 求职行为和持续期依赖在不同年龄和劳动力市场状况下有明显差异。

Abstract: We study how job search behavior evolves over the unemployment spell and the
extent to which job seekers experience duration dependence in callbacks.
Leveraging data on 2.4 million monthly activity reports containing detailed
information on job applications, interviews, and other search activities, we
separate within-spell changes from dynamic selection with a time-and-spell
fixed effects design. We find that raw search effort increases with
unemployment duration, but this pattern reflects dynamic selection:
within-spell search effort remains flat and declines sharply in the months
preceding re-employment. Around unemployment insurance (UI) exhaustion, search
effort drops by approximately 10%, likely due to participation in labor market
programs crowding out job search. Reported interviews indicate that callbacks
decline by 6% per month, but only 10--14% of this decline reflects ``true''
duration dependence. Finally, we document substantial heterogeneity: search
effort and duration dependence vary strongly by age, and job seekers in tight
labor markets experience about 50% more duration dependence.

</details>


### [269] [Defining the payback period for nonconventional cash flows: an axiomatic approach](https://arxiv.org/abs/2511.03568)
*Mikhail V. Sokolov*

Main category: econ.GN

TL;DR: 指出非传统项目投资回收期定义有挑战，证明项目余额最后盈亏平衡点是投资回收期唯一符合经济公理的定义，贴现投资回收期有类似结果。


<details>
  <summary>Details</summary>
Motivation: 非传统项目投资回收期定义存在矛盾且有挑战性，需找到合理定义。

Method: 依据一组经济上有意义的公理进行分析。

Result: 项目余额最后盈亏平衡点是投资回收期唯一符合公理的定义，贴现投资回收期有类似结论。

Conclusion: 项目余额最后盈亏平衡点可作为非传统项目投资回收期的合理定义。

Abstract: The payback period is unambiguously defined for conventional investment
projects, projects in which a series of cash outflows is followed by a series
of cash inflows. Its definition for nonconventional projects is more
challenging, since their balances (cumulative cash flow streams) may have
multiple break-even points. Academics and practitioners offer a few
contradictory recipes to manage this issue, suggesting to use the first
break-even point of the balance, the last break-even point of the balance, or
the break-even point of the modified cumulative cash flow stream, representing
the moment of time in which the cumulative cash inflow exceeds the total cash
outflow. In this note, we show that the last break-even point of the project
balance is the only definition of the payback period consistent with a set of
economically meaningful axioms. An analogous result is established for the
discounted payback period.

</details>


### [270] [Supply Chain Disruptions, the Structure of Production Networks, and the Impact of Globalization](https://arxiv.org/abs/2511.03660)
*Matthew L. Elliott,Matthew O. Jackson*

Main category: econ.GN

TL;DR: 本文构建多部门国际生产模型，研究生产中断影响及供应链相关特性，还分析国家权力。


<details>
  <summary>Details</summary>
Motivation: 研究部分商品生产中断如何传导至其他商品和消费者，以及这种影响与生产网络的关系。

Method: 构建简约的多部门国际生产模型进行研究。

Result: 短期生产中断影响比长期大；供应链复杂性增加会提高中断概率和短期规模；运输成本降低使生产更专业化，降低中断几率但提高条件影响；可刻画国家通过生产转移、进出口配额对他国的权力。

Conclusion: 通过模型揭示了生产中断影响、供应链特性及国家权力等方面的规律。

Abstract: We introduce a parsimonious multi-sector model of international production
and use it to study the impact of a disruption in the production of some goods
propagates to other goods and consumers, and how that impact depends on the
goods' positions in, and overall structure of, the production network. We show
that the short-run impact of a disruption can be dramatically larger than the
long-run impact. The short-run disruption depends on the value of all of the
final goods whose supply chains involve a disrupted good, while by contrast the
long-run disruption depends only on the cost of the disrupted goods. We use the
model to show how increased complexity of supply chains leads to increased
fragility in terms of the probability and expected short-run size of a
disruption. We also show how decreased transportation costs can lead to
increased specialization in production, lowering the chances for disruption but
increasing the impact conditional upon disruption. We use the model to
characterize the power that a country has over others via diversions of its
production as well as quotas on imports and exports.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [271] [A Tsallis-Entropy Lens on Genetic Variation](https://arxiv.org/abs/2511.03063)
*Margarita Geleta,Daniel Mas Montserrat,Alexander G. Ioannidis*

Main category: cs.IT

TL;DR: 引入Tsallis阶q F统计量$F_q$，它是固定统计量的信息论推广，在真实数据和模拟中能清晰归因亚群结构驱动因素和敏感标记事件，可作为模拟审计和群体结构总结的补充。


<details>
  <summary>Details</summary>
Motivation: 为了提供比经典$F_{\textbf{ST}}$更细粒度的群体分化视图，特别是在等位基因频率谱偏斜时。

Method: 引入$F_q$统计量，通过改变q值调整对稀有和常见变异的权重，并在真实数据（865个大洋洲基因组）和受控谱系模拟上进行验证。

Result: $F_q$在一对多和留一法模式下能清晰归因亚群对区域结构的驱动作用，敏感标记隔离 - 迁移事件和奠基者效应。

Conclusion: $F_q$可作为模拟审计和群体结构总结的更高分辨率补充。

Abstract: We introduce an information-theoretic generalization of the fixation
statistic, the Tsallis-order $q$ F-statistic, $F_q$, which measures the
fraction of Tsallis $q$-entropy lost within subpopulations relative to the
pooled population. The family nests the classical variance-based fixation index
$F_{\textbf{ST}}$ at $q{=}2$ and a Shannon-entropy analogue at $q{=}1$, whose
absolute form equals the mutual information between alleles and population
labels. By varying $q$, $F_q$ acts as a spectral differentiator that up-weights
rare variants at low $q$, while $q{>}1$ increasingly emphasizes common
variants, providing a more fine-grained view of differentiation than
$F_{\textbf{ST}}$ when allele-frequency spectra are skewed. On real data (865
Oceanian genomes with 1,823,000 sites) and controlled genealogical simulations
(seeded from 1,432 founders from HGDP and 1000 Genomes panels, with 322,216
sites), we show that $F_q$ in One-vs-Rest (OVR) and Leave-One-Out (LOO) modes
provides clear attribution of which subpopulations drive regional structure,
and sensitively timestamps isolation-migration events and founder effects.
$F_q$ serves as finer-resolution complement for simulation audits and
population-structure summaries.

</details>


### [272] [Neural Beamforming with Doppler-Aware Sparse Attention for High Mobility Environments](https://arxiv.org/abs/2511.03632)
*Cemil Vahapoglu,Timothy J. O'Shea,Wan Liu,Sennur Ulukus*

Main category: cs.IT

TL;DR: 提出多普勒感知稀疏神经网络波束成形（Doppler - aware Sparse NNBF）模型，在高移动性场景下性能优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统波束成形技术在恶劣信道条件下性能下降，现有基于深度学习的方法存在复杂度高或未考虑信道动态的问题。

Method: 提出Doppler - aware Sparse NNBF模型，在多用户单输入多输出（MU - SIMO）设置中引入信道自适应稀疏注意力机制，其稀疏结构可根据信道动态在二维时频轴上配置。

Result: 在城市宏（UMa）信道条件模拟结果显示，Doppler - aware Sparse NNBF在高移动性场景下显著优于固定模式基线和传统波束成形技术，同时保持结构化稀疏。

Conclusion: Doppler - aware Sparse NNBF模型在高移动性场景下具有更好的性能，且能控制每个查询关注键的数量。

Abstract: Beamforming has significance for enhancing spectral efficiency and mitigating
interference in multi-antenna wireless systems, facilitating spatial
multiplexing and diversity in dense and high mobility scenarios. Traditional
beamforming techniques such as zero-forcing beamforming (ZFBF) and minimum mean
square error (MMSE) beamforming experience performance deterioration under
adverse channel conditions. Deep learning-based beamforming offers an
alternative with nonlinear mappings from channel state information (CSI) to
beamforming weights by improving robustness against dynamic channel
environments. Transformer-based models are particularly effective due to their
ability to model long-range dependencies across time and frequency. However,
their quadratic attention complexity limits scalability in large OFDM grids.
Recent studies address this issue through sparse attention mechanisms that
reduce complexity while maintaining expressiveness, yet often employ patterns
that disregard channel dynamics, as they are not specifically designed for
wireless communication scenarios. In this work, we propose a Doppler-aware
Sparse Neural Network Beamforming (Doppler-aware Sparse NNBF) model that
incorporates a channel-adaptive sparse attention mechanism in a multi-user
single-input multiple-output (MU-SIMO) setting. The proposed sparsity structure
is configurable along 2D time-frequency axes based on channel dynamics and is
theoretically proven to ensure full connectivity within p hops, where p is the
number of attention heads. Simulation results under urban macro (UMa) channel
conditions show that Doppler-aware Sparse NNBF significantly outperforms both a
fixed-pattern baseline, referred to as Standard Sparse NNBF, and conventional
beamforming techniques ZFBF and MMSE beamforming in high mobility scenarios,
while maintaining structured sparsity with a controlled number of attended keys
per query.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [273] [Noise induced Stability of a Mean-Field model of Systemic Risk with uncertain robustness](https://arxiv.org/abs/2511.03358)
*Alexander Alecio*

Main category: math.PR

TL;DR: 本文研究含经验均值交互的扩散过程系统模型，发现考虑势函数系数不确定性时系统相变更丰富，存在噪声诱导稳定现象。


<details>
  <summary>Details</summary>
Motivation: 研究系统风险模型，考虑势函数系数不确定时系统的行为和相变化。

Method: 构建含经验均值交互的扩散过程模型，研究对应的McKean - Vlasov SDE，分析不同噪声强度下系统的相变化。

Result: 发现考虑势函数系数不确定性时，系统相变更丰富，存在噪声诱导稳定现象，即更多噪声可降低系统故障风险。

Conclusion: 考虑势函数系数不确定性会使系统呈现出在无不确定性时不会出现的相变化，噪声诱导稳定现象为降低系统风险提供新思路。

Abstract: We consider a model for systemic risk comprising of a system of diffusion
processes, interacting through their empirical mean. Each process is subject to
a confining double-well potential with some uncertainty in the coefficients,
corresponding to fluctuations in height of the potential barrier seperating the
two wells. This is equivalent to studying a single McKean-Vlasov SDE with
explicit dependence on its moments and, novelly, independently varying additive
and multiplicative noise. Such non-linear SDEs are known to possess two phases:
stable (ordered) and unstable (disordered). When the potential is purely
bistable, the phase changes from stable to unstable when noise intensity is
increased past a critical threshold. With the recent advances, it will be shown
that the behaviour here is far richer: indeed, depending on the interpretation
of the stochastic integral, the system exhibits phase changes that cannot occur
in any regime where there is no uncertainty in the potential. Strikingly, this
allows for the phenomenon of noise induced stability; situations where more
noise can reduce the risk of system failure.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [274] [Open Source State-Of-the-Art Solution for Romanian Speech Recognition](https://arxiv.org/abs/2511.03361)
*Gabriel Pirlogeanu,Alexandru-Lucian Georgescu,Horia Cucu*

Main category: eess.AS

TL;DR: 本文提出基于FastConformer架构的罗马尼亚语自动语音识别系统，在大量弱监督转录语料上训练，采用混合解码器评估多种解码策略，实现了最优性能和高效解码。


<details>
  <summary>Details</summary>
Motivation: 构建性能更优的罗马尼亚语自动语音识别系统。

Method: 基于NVIDIA的FastConformer架构，在超2600小时语音的弱监督转录语料上训练模型，使用含CTC和TDT分支的混合解码器，评估多种解码策略。

Result: 系统在所有罗马尼亚语评估基准上达到最优性能，相对之前最佳系统WER最多降低27%，且有高效解码能力。

Conclusion: 该方法不仅提高转录准确性，还具有实际解码效率，适用于研究和低延迟ASR应用部署。

Abstract: In this work, we present a new state-of-the-art Romanian Automatic Speech
Recognition (ASR) system based on NVIDIA's FastConformer architecture--explored
here for the first time in the context of Romanian. We train our model on a
large corpus of, mostly, weakly supervised transcriptions, totaling over 2,600
hours of speech. Leveraging a hybrid decoder with both Connectionist Temporal
Classification (CTC) and Token-Duration Transducer (TDT) branches, we evaluate
a range of decoding strategies including greedy, ALSD, and CTC beam search with
a 6-gram token-level language model. Our system achieves state-of-the-art
performance across all Romanian evaluation benchmarks, including read,
spontaneous, and domain-specific speech, with up to 27% relative WER reduction
compared to previous best-performing systems. In addition to improved
transcription accuracy, our approach demonstrates practical decoding
efficiency, making it suitable for both research and deployment in low-latency
ASR applications.

</details>


### [275] [Quantifying Articulatory Coordination as a Biomarker for Schizophrenia](https://arxiv.org/abs/2511.03084)
*Gowtham Premananth,Carol Espy-Wilson*

Main category: eess.AS

TL;DR: 文章提出可解释框架利用语音特征量化声道协调性，为精神分裂症提供透明、敏感度高的生物标志物，推动基于语音评估工具发展。


<details>
  <summary>Details</summary>
Motivation: 人工智能和深度学习虽提升医疗诊断能力，但可解释性不足阻碍临床应用，精神分裂症需要能捕捉症状严重程度并提供有临床意义见解的工具。

Method: 提出可解释框架，利用特征通过本征谱差异图和加权和指数衰减（WSED）量化声道协调性。

Result: 本征谱图有效区分复杂和简单协调模式，WSED分数可靠区分两组，且与BPRS严重程度及正负症状平衡相关。

Conclusion: 该方法为精神分裂症提供透明、对严重程度敏感的生物标志物，推动基于语音的临床可解释评估工具发展。

Abstract: Advances in artificial intelligence (AI) and deep learning have improved
diagnostic capabilities in healthcare, yet limited interpretability continues
to hinder clinical adoption. Schizophrenia, a complex disorder with diverse
symptoms including disorganized speech and social withdrawal, demands tools
that capture symptom severity and provide clinically meaningful insights beyond
binary diagnosis. Here, we present an interpretable framework that leverages
articulatory speech features through eigenspectra difference plots and a
weighted sum with exponential decay (WSED) to quantify vocal tract
coordination. Eigenspectra plots effectively distinguished complex from simpler
coordination patterns, and WSED scores reliably separated these groups, with
ambiguity confined to a narrow range near zero. Importantly, WSED scores
correlated not only with overall BPRS severity but also with the balance
between positive and negative symptoms, reflecting more complex coordination in
subjects with pronounced positive symptoms and the opposite trend for stronger
negative symptoms. This approach offers a transparent, severity-sensitive
biomarker for schizophrenia, advancing the potential for clinically
interpretable speech-based assessment tools.

</details>
