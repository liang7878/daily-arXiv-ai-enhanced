<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 69]
- [cs.CE](#cs.CE) [Total: 2]
- [cs.DB](#cs.DB) [Total: 4]
- [cs.DC](#cs.DC) [Total: 8]
- [cs.DS](#cs.DS) [Total: 4]
- [cs.GT](#cs.GT) [Total: 6]
- [cs.IR](#cs.IR) [Total: 16]
- [cs.LG](#cs.LG) [Total: 252]
- [cs.NE](#cs.NE) [Total: 3]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.SE](#cs.SE) [Total: 28]
- [q-fin.CP](#q-fin.CP) [Total: 1]
- [q-fin.PM](#q-fin.PM) [Total: 1]
- [stat.ML](#stat.ML) [Total: 18]
- [stat.CO](#stat.CO) [Total: 4]
- [cs.PL](#cs.PL) [Total: 1]
- [cs.SI](#cs.SI) [Total: 8]
- [cs.HC](#cs.HC) [Total: 6]
- [cs.LO](#cs.LO) [Total: 1]
- [cs.CL](#cs.CL) [Total: 32]
- [math.NA](#math.NA) [Total: 1]
- [math.ST](#math.ST) [Total: 2]
- [cs.CY](#cs.CY) [Total: 7]
- [eess.SP](#eess.SP) [Total: 3]
- [cs.MA](#cs.MA) [Total: 5]
- [physics.soc-ph](#physics.soc-ph) [Total: 2]
- [stat.AP](#stat.AP) [Total: 1]
- [stat.ME](#stat.ME) [Total: 4]
- [cond-mat.dis-nn](#cond-mat.dis-nn) [Total: 1]
- [eess.AS](#eess.AS) [Total: 2]
- [eess.SY](#eess.SY) [Total: 1]
- [cs.NI](#cs.NI) [Total: 2]
- [cs.SC](#cs.SC) [Total: 1]
- [quant-ph](#quant-ph) [Total: 3]
- [cs.MM](#cs.MM) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cs.CV](#cs.CV) [Total: 31]
- [econ.GN](#econ.GN) [Total: 5]
- [cs.OS](#cs.OS) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [cs.CR](#cs.CR) [Total: 14]
- [cs.IT](#cs.IT) [Total: 2]
- [cs.DM](#cs.DM) [Total: 1]
- [cs.SD](#cs.SD) [Total: 8]
- [cs.RO](#cs.RO) [Total: 10]
- [econ.EM](#econ.EM) [Total: 2]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [CreditAudit: 2D Auditing for LLM Evaluation and Selection](https://arxiv.org/abs/2602.02515)
*Yiliang Song,Hongjun An,Jiangong Xiao,Haofei Zhao,Jiawei Shao,Xuelong Li*

Main category: cs.AI

TL;DR: 现有基准测试分数与用户日常体验不符，提出CreditAudit框架评估模型，实验表明稳定性风险影响模型选择，该框架支持分层部署和更客观评估。


<details>
  <summary>Details</summary>
Motivation: 当前公共基准测试分数与用户日常体验不匹配，小的协议变化会导致重大失败，让从业者不确定部署哪个模型。

Method: 提出CreditAudit框架，在多个基准测试下使用语义对齐且非对抗性的系统提示模板评估模型，报告平均性能和稳定性风险信号，通过跨模型分位数将波动性映射为可解释的信用等级。

Result: 在GPQA、TruthfulQA和MMLU Pro上的实验表明，平均能力相似的模型波动差异大，稳定性风险会改变模型选择决策。

Conclusion: CreditAudit框架提供基于二维和等级的语言用于特定场景选择，支持分层部署和更合理的测试监控资源分配，实现更客观可信的现实模型评估。

Abstract: Leaderboard scores on public benchmarks have been steadily rising and converging, with many frontier language models now separated by only marginal differences. However, these scores often fail to match users' day to day experience, because system prompts, output protocols, and interaction modes evolve under routine iteration, and in agentic multi step pipelines small protocol shifts can trigger disproportionate failures, leaving practitioners uncertain about which model to deploy. We propose CreditAudit, a deployment oriented credit audit framework that evaluates models under a family of semantically aligned and non adversarial system prompt templates across multiple benchmarks, reporting mean ability as average performance across scenarios and scenario induced fluctuation sigma as a stability risk signal, and further mapping volatility into interpretable credit grades from AAA to BBB via cross model quantiles with diagnostics that mitigate template difficulty drift. Controlled experiments on GPQA, TruthfulQA, and MMLU Pro show that models with similar mean ability can exhibit substantially different fluctuation, and stability risk can overturn prioritization decisions in agentic or high failure cost regimes. By providing a 2D and grade based language for regime specific selection, CreditAudit supports tiered deployment and more disciplined allocation of testing and monitoring effort, enabling more objective and trustworthy model evaluation for real world use.

</details>


### [2] [Experience-Driven Multi-Agent Systems Are Training-free Context-aware Earth Observers](https://arxiv.org/abs/2602.02559)
*Pengyu Dai,Weihao Xuan,Junjue Wang,Hongruixuan Chen,Jian Song,Yafei Ou,Naoto Yokoya*

Main category: cs.AI

TL;DR: 引入GeoEvolver多智能体系统，让大语言模型智能体无参数更新下获取EO专业知识，实验证明提升任务成功率。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型智能体在专业、工具密集型领域（如地球观测任务）存在困难，缺乏从交互中学习细粒度工具专业知识的机制。

Method: 引入GeoEvolver系统，通过检索增强多智能体编排器将查询分解为子目标，在子目标层探索工具参数配置，将成功模式和失败归因提炼到内存库为未来查询提供示范。

Result: 在三个工具集成的地球观测基准测试中，GeoEvolver持续提高端到端任务成功率，多个LLM主干平均增益12%。

Conclusion: 地球观测专业知识可从与环境的高效、细粒度交互中逐步产生。

Abstract: Recent advances have enabled large language model (LLM) agents to solve complex tasks by orchestrating external tools. However, these agents often struggle in specialized, tool-intensive domains that demand long-horizon execution, tight coordination across modalities, and strict adherence to implicit tool constraints. Earth Observation (EO) tasks exemplify this challenge due to the multi-modal and multi-temporal data inputs, as well as the requirements of geo-knowledge constraints (spectrum library, spatial reasoning, etc): many high-level plans can be derailed by subtle execution errors that propagate through a pipeline and invalidate final results. A core difficulty is that existing agents lack a mechanism to learn fine-grained, tool-level expertise from interaction. Without such expertise, they cannot reliably configure tool parameters or recover from mid-execution failures, limiting their effectiveness in complex EO workflows. To address this, we introduce \textbf{GeoEvolver}, a self-evolving multi-agent system~(MAS) that enables LLM agents to acquire EO expertise through structured interaction without any parameter updates. GeoEvolver decomposes each query into independent sub-goals via a retrieval-augmented multi-agent orchestrator, then explores diverse tool-parameter configurations at the sub-goal level. Successful patterns and root-cause attribution from failures are then distilled in an evolving memory bank that provides in-context demonstrations for future queries. Experiments on three tool-integrated EO benchmarks show that GeoEvolver consistently improves end-to-end task success, with an average gain of 12\% across multiple LLM backbones, demonstrating that EO expertise can emerge progressively from efficient, fine-grained interactions with the environment.

</details>


### [3] [Uncertainty and Fairness Awareness in LLM-Based Recommendation Systems](https://arxiv.org/abs/2602.02582)
*Chandan Kumar Sah,Xiaoli Lian,Li Zhang,Tony Xu,Syed Shazaib Shah*

Main category: cs.AI

TL;DR: 本文研究不确定性和公平性评估对大语言模型生成推荐的影响，引入评估指标和数据集，发现模型存在不公平性，提出新评估方法和公平性基准。


<details>
  <summary>Details</summary>
Motivation: 大语言模型零样本推荐存在预测不确定性和嵌入偏差，威胁可靠性和公平性，需研究评估影响。

Method: 引入评估指标和数据集，通过案例研究量化不确定性，将人格感知公平性融入评估流程。

Result: 发现Google DeepMind的Gemini 1.5 Flash对特定敏感属性存在系统性不公平，差距分别为SNSR 0.1363和SNSV 0.0507，且在提示扰动下仍存在；揭示人格关联偏差模式和个性化与群体公平性权衡。

Conclusion: 提出的方法为更安全、可解释的推荐大语言模型奠定基础，激励未来多模型基准和自适应校准研究。

Abstract: Large language models (LLMs) enable powerful zero-shot recommendations by leveraging broad contextual knowledge, yet predictive uncertainty and embedded biases threaten reliability and fairness. This paper studies how uncertainty and fairness evaluations affect the accuracy, consistency, and trustworthiness of LLM-generated recommendations. We introduce a benchmark of curated metrics and a dataset annotated for eight demographic attributes (31 categorical values) across two domains: movies and music. Through in-depth case studies, we quantify predictive uncertainty (via entropy) and demonstrate that Google DeepMind's Gemini 1.5 Flash exhibits systematic unfairness for certain sensitive attributes; measured similarity-based gaps are SNSR at 0.1363 and SNSV at 0.0507. These disparities persist under prompt perturbations such as typographical errors and multilingual inputs. We further integrate personality-aware fairness into the RecLLM evaluation pipeline to reveal personality-linked bias patterns and expose trade-offs between personalization and group fairness. We propose a novel uncertainty-aware evaluation methodology for RecLLMs, present empirical insights from deep uncertainty case studies, and introduce a personality profile-informed fairness benchmark that advances explainability and equity in LLM recommendations. Together, these contributions establish a foundation for safer, more interpretable RecLLMs and motivate future work on multi-model benchmarks and adaptive calibration for trustworthy deployment.

</details>


### [4] [PeerRank: Autonomous LLM Evaluation Through Web-Grounded, Bias-Controlled Peer Review](https://arxiv.org/abs/2602.02589)
*Yanki Margalit,Erni Avram,Ran Taig,Oded Margalit,Nurit Cohen-Inger*

Main category: cs.AI

TL;DR: 提出PeerRank自主评估框架，对大语言模型进行评估，结果稳定且能揭示偏见，可拓展开放世界LLM评估。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型评估方法扩展性差、易过时且与开放世界部署不匹配。

Method: 引入PeerRank框架，让模型自主生成评估任务、带网络信息回答、评判同行回答并汇总评估。

Result: 在12个模型和420个问题的研究中产生稳定且有区分度的排名，揭示偏见，在TruthfulQA和GSM8K上验证与客观准确性相关。

Conclusion: 基于选择性网络信息回答的无偏同行评估可拓展开放世界LLM评估。

Abstract: Evaluating large language models typically relies on human-authored benchmarks, reference answers, and human or single-model judgments, approaches that scale poorly, become quickly outdated, and mismatch open-world deployments that depend on web retrieval and synthesis. We introduce PeerRank, a fully autonomous end-to-end evaluation framework in which models generate evaluation tasks, answer them with category-scoped live web grounding, judge peer responses and aggregate dense peer assessments into relative performance estimates, without human supervision or gold references. PeerRank treats evaluation as a multi-agent process where each model participates symmetrically as task designer, respondent, and evaluator, while removing biased judgments. In a large-scale study over 12 commercially available models and 420 autonomously generated questions, PeerRank produces stable, discriminative rankings and reveals measurable identity and presentation biases. Rankings are robust, and mean peer scores agree with Elo. We further validate PeerRank on TruthfulQA and GSM8K, where peer scores correlate with objective accuracy. Together, these results suggest that bias-aware peer evaluation with selective web-grounded answering can scale open-world LLM assessment beyond static and human curated benchmarks.

</details>


### [5] [A Positive Case for Faithfulness: LLM Self-Explanations Help Predict Model Behavior](https://arxiv.org/abs/2602.02639)
*Harry Mayne,Justin Singh Kang,Dewi Gould,Kannan Ramchandran,Adam Mahdi,Noah Y. Siegel*

Main category: cs.AI

TL;DR: 本文引入NSG指标评估大语言模型自解释的忠实度，发现自解释能提升模型行为预测，虽有缺陷但有积极作用。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型自解释忠实度指标有局限，忽略解释的预测价值，需新指标。

Method: 引入NSG指标，对18个前沿模型在7000个反事实样本上评估。

Result: 自解释显著提升模型行为预测（11 - 37% NSG），比外部模型解释更具预测性，5 - 15%自解释有严重误导性。

Conclusion: 尽管有不足，自解释编码的信息有助于预测模型行为。

Abstract: LLM self-explanations are often presented as a promising tool for AI oversight, yet their faithfulness to the model's true reasoning process is poorly understood. Existing faithfulness metrics have critical limitations, typically relying on identifying unfaithfulness via adversarial prompting or detecting reasoning errors. These methods overlook the predictive value of explanations. We introduce Normalized Simulatability Gain (NSG), a general and scalable metric based on the idea that a faithful explanation should allow an observer to learn a model's decision-making criteria, and thus better predict its behavior on related inputs. We evaluate 18 frontier proprietary and open-weight models, e.g., Gemini 3, GPT-5.2, and Claude 4.5, on 7,000 counterfactuals from popular datasets covering health, business, and ethics. We find self-explanations substantially improve prediction of model behavior (11-37% NSG). Self-explanations also provide more predictive information than explanations generated by external models, even when those models are stronger. This implies an advantage from self-knowledge that external explanation methods cannot replicate. Our approach also reveals that, across models, 5-15% of self-explanations are egregiously misleading. Despite their imperfections, we show a positive case for self-explanations: they encode information that helps predict model behavior.

</details>


### [6] [MARS: Modular Agent with Reflective Search for Automated AI Research](https://arxiv.org/abs/2602.02660)
*Jiefeng Chen,Bhavana Dalvi Mishra,Jaehyun Nam,Rui Meng,Tomas Pfister,Jinsung Yoon*

Main category: cs.AI

TL;DR: 介绍MARS框架用于自主AI研究，有预算规划、模块化构建和比较反思记忆三大支柱，在MLE - Bench上表现佳


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的智能体在自动化AI研究中存在不足，常生成忽视执行成本和因果因素的一体化脚本

Method: 采用成本约束的蒙特卡罗树搜索进行预算感知规划，通过“设计 - 分解 - 实现”管道进行模块化构建，使用比较反思记忆分析解的差异

Result: 在MLE - Bench开源框架中达最优性能，与全球排行榜顶尖方法有竞争力，63%利用的经验源于跨分支转移

Conclusion: MARS框架能有效平衡性能与执行成本，在自主AI研究中可有效泛化见解

Abstract: Automating AI research differs from general software engineering due to computationally expensive evaluation (e.g., model training) and opaque performance attribution. Current LLM-based agents struggle here, often generating monolithic scripts that ignore execution costs and causal factors. We introduce MARS (Modular Agent with Reflective Search), a framework optimized for autonomous AI research. MARS relies on three pillars: (1) Budget-Aware Planning via cost-constrained Monte Carlo Tree Search (MCTS) to explicitly balance performance with execution expense; (2) Modular Construction, employing a "Design-Decompose-Implement" pipeline to manage complex research repositories; and (3) Comparative Reflective Memory, which addresses credit assignment by analyzing solution differences to distill high-signal insights. MARS achieves state-of-the-art performance among open-source frameworks on MLE-Bench under comparable settings, maintaining competitiveness with the global leaderboard's top methods. Furthermore, the system exhibits qualitative "Aha!" moments, where 63% of all utilized lessons originate from cross-branch transfer, demonstrating that the agent effectively generalizes insights across search paths.

</details>


### [7] [ATLAS : Adaptive Self-Evolutionary Research Agent with Task-Distributed Multi-LLM Supporters](https://arxiv.org/abs/2602.02709)
*Ujin Jeon,Jiyong Kwon,Madison Ann Sullivan,Caleb Eunho Lee,Guang Lin*

Main category: cs.AI

TL;DR: 提出ATLAS框架解决多LLM代理系统长时任务难题，实验表明其性能优于单代理基线


<details>
  <summary>Details</summary>
Motivation: 现有多LLM代理系统在长时任务中存在问题，如微调后求解器固定或依赖静态偏好优化循环

Method: 提出ATLAS任务分布式框架，核心算法EvoDPO自适应更新参考策略，并进行理论后悔分析

Result: 在非平稳线性上下文老虎机和科学机器学习损失重加权实验中，ATLAS提高了稳定性和性能

Conclusion: ATLAS在性能和稳定性上优于静态单代理基线

Abstract: Recent multi-LLM agent systems perform well in prompt optimization and automated problem-solving, but many either keep the solver frozen after fine-tuning or rely on a static preference-optimization loop, which becomes intractable for long-horizon tasks. We propose ATLAS (Adaptive Task-distributed Learning for Agentic Self-evolution), a task-distributed framework that iteratively develops a lightweight research agent while delegating complementary roles to specialized supporter agents for exploration, hyperparameter tuning, and reference policy management. Our core algorithm, Evolving Direct Preference Optimization (EvoDPO), adaptively updates the phase-indexed reference policy. We provide a theoretical regret analysis for a preference-based contextual bandit under concept drift. In addition, experiments were conducted on non-stationary linear contextual bandits and scientific machine learning (SciML) loss reweighting for the 1D Burgers' equation. Both results show that ATLAS improves stability and performance over a static single-agent baseline.

</details>


### [8] [Dynamic Mix Precision Routing for Efficient Multi-step LLM Interaction](https://arxiv.org/abs/2602.02711)
*Yuanzhe Li,Jianing Deng,Jingtong Hu,Tianlong Chen,Song Wang,Huanrui Yang*

Main category: cs.AI

TL;DR: 本文探索在长程决策中使用低精度量化大语言模型，提出动态混合精度路由框架，在ALFWorld实验中取得精度 - 成本权衡的改进。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在长程决策任务中多步交互推理虽表现好，但使用大模型成本高，需解决成本与性能的平衡问题。

Method: 提出动态混合精度路由框架，在每个决策步骤自适应选择高精度和低精度大语言模型；通过两阶段管道训练路由器，先基于KL散度监督学习识别精度敏感步骤，再用GRPO进一步提高任务成功率。

Result: 在ALFWorld上的实验表明，该方法在精度 - 成本权衡上比单精度基线和启发式路由方法有很大改进。

Conclusion: 动态混合精度路由框架能有效解决长程决策中使用大语言模型的成本问题，实现更好的精度 - 成本权衡。

Abstract: Large language models (LLM) achieve strong performance in long-horizon decision-making tasks through multi-step interaction and reasoning at test time. While practitioners commonly believe a higher task success rate necessitates the use of a larger and stronger LLM model, multi-step interaction with a large LLM incurs prohibitive inference cost. To address this problem, we explore the use of low-precision quantized LLM in the long-horizon decision-making process. Based on the observation of diverse sensitivities among interaction steps, we propose a dynamic mix-precision routing framework that adaptively selects between high-precision and low-precision LLMs at each decision step. The router is trained via a two-stage pipeline, consisting of KL-divergence-based supervised learning that identifies precision-sensitive steps, followed by Group-Relative Policy Optimization (GRPO) to further improve task success rates. Experiments on ALFWorld demonstrate that our approach achieves a great improvement on accuracy-cost trade-off over single-precision baselines and heuristic routing methods.

</details>


### [9] [Scaling-Aware Adapter for Structure-Grounded LLM Reasoning](https://arxiv.org/abs/2602.02780)
*Zihao Jing,Qiuhao Zeng,Ruiyi Fang,Yan Yi Li,Yan Sun,Boyu Wang,Pingzhao Hu*

Main category: cs.AI

TL;DR: 提出统一全原子大语言模型Cuttlefish，在多原子基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在生物分子结构推理中存在模态特定、忽略几何基础、模态融合瓶颈等问题，阻碍通用全原子推理。

Method: 采用Scaling - Aware Patching利用指令条件门控机制生成可变大小补丁，Geometry Grounding Adapter通过交叉注意力细化自适应令牌并注入LLM。

Result: 在不同全原子基准测试中，Cuttlefish在异构结构推理中表现优越。

Conclusion: Cuttlefish能有效解决现有模型问题，实现更好的全原子推理。

Abstract: Large language models (LLMs) are enabling reasoning over biomolecular structures, yet existing methods remain modality-specific and typically compress structural inputs through sequence-based tokenization or fixed-length query connectors. Such architectures either omit the geometric groundings requisite for mitigating structural hallucinations or impose inflexible modality fusion bottlenecks that concurrently over-compress and suboptimally allocate structural tokens, thereby impeding the realization of generalized all-atom reasoning. We introduce Cuttlefish, a unified all-atom LLM that grounds language reasoning in geometric cues while scaling modality tokens with structural complexity. First, Scaling-Aware Patching leverages an instruction-conditioned gating mechanism to generate variable-size patches over structural graphs, adaptively scaling the query token budget with structural complexity to mitigate fixed-length connector bottlenecks. Second, Geometry Grounding Adapter refines these adaptive tokens via cross-attention to modality embeddings and injects the resulting modality tokens into the LLM, exposing explicit geometric cues to reduce structural hallucination. Experiments across diverse all-atom benchmarks demonstrate that Cuttlefish achieves superior performance in heterogeneous structure-grounded reasoning. Code is available at the project repository.

</details>


### [10] [Chain of Simulation: A Dual-Mode Reasoning Framework for Large Language Models with Dynamic Problem Routing](https://arxiv.org/abs/2602.02842)
*Saeid Sheikhi*

Main category: cs.AI

TL;DR: 提出Chain of Simulation (CoS)框架，在多个基准测试中表现优异，证明问题特定模式选择重要性，且无需额外训练，具准确性和效率优势。


<details>
  <summary>Details</summary>
Motivation: 现有的统一提示方法存在局限性，需要更好的推理框架提升大语言模型推理能力。

Method: 采用三种不同推理模式（数学问题的计算流与自一致性、空间推理的符号状态跟踪与JSON表示、多跳推理的混合事实提取），并提供模式选择、状态跟踪和答案提取的详细算法。

Result: 在GSM8K、StrategyQA和bAbI基准测试中，相比最强基线有提升，如GSM8K达71.5%准确率（提升1.0%）等；正确应用计算模式到数学问题准确率达81.2%，错误路由准确率为0%；与Self - Consistency相比，以54%更低计算成本实现相近性能。

Conclusion: CoS是一种无需额外训练就能有效提升大语言模型推理能力的方法，在准确性和效率间有较好权衡。

Abstract: We present Chain of Simulation (CoS), a novel dual-mode reasoning framework that dynamically routes problems to specialized reasoning strategies in Large Language Models (LLMs). Unlike existing uniform prompting approaches, CoS employs three distinct reasoning modes: (1) computational flow with self-consistency for mathematical problems, (2) symbolic state tracking with JSON representations for spatial reasoning, and (3) hybrid fact-extraction for multi-hop inference. Through comprehensive evaluation on GSM8K, StrategyQA, and bAbI benchmarks using four state-of-the-art models (Gemma-3 27B, LLaMA-3.1 8B, Mistral 7B, and Qwen-2.5 14B), we demonstrate that CoS achieves 71.5% accuracy on GSM8K (1.0% absolute improvement), 90.0% on StrategyQA (2.5% improvement), and 19.0% on bAbI (65.2% relative improvement) compared to the strongest baselines. The analysis reveals that problem-specific mode selection is crucial, with computational mode achieving 81.2% accuracy when correctly applied to mathematical problems, while misrouting leads to 0% accuracy. We provide detailed algorithms for mode selection, state tracking, and answer extraction, establishing CoS as an effective approach for improving LLM reasoning without additional training. The framework provides superior trade-offs between accuracy and efficiency compared to Self-Consistency, achieving comparable performance at 54% lower computational cost.

</details>


### [11] [AutoSizer: Automatic Sizing of Analog and Mixed-Signal Circuits via Large Language Model (LLM) Agents](https://arxiv.org/abs/2602.02849)
*Xi Yu,Dmitrii Torbunov,Soumyajit Mandal,Yihui Ren*

Main category: cs.AI

TL;DR: 现有模拟和混合信号（AMS）集成电路晶体管尺寸确定方法效率低、鲁棒性差，本文提出AutoSizer框架和AMS - SizingBench基准，实验表现优于传统方法和现有基于大模型的代理。


<details>
  <summary>Details</summary>
Motivation: AMS集成电路设计依赖专家知识，晶体管尺寸确定是瓶颈，现有EDA方法效率低、鲁棒性差，大语言模型不适合精确数值优化。

Method: 提出AutoSizer框架，采用双循环优化框架；引入AMS - SizingBench基准评估优化策略。

Result: AutoSizer实验中在不同电路难度下实现了更高的解决方案质量、更快的收敛速度和更高的成功率。

Conclusion: AutoSizer优于传统优化方法和现有基于大模型的代理。

Abstract: The design of Analog and Mixed-Signal (AMS) integrated circuits remains heavily reliant on expert knowledge, with transistor sizing a major bottleneck due to nonlinear behavior, high-dimensional design spaces, and strict performance constraints. Existing Electronic Design Automation (EDA) methods typically frame sizing as static black-box optimization, resulting in inefficient and less robust solutions. Although Large Language Models (LLMs) exhibit strong reasoning abilities, they are not suited for precise numerical optimization in AMS sizing. To address this gap, we propose AutoSizer, a reflective LLM-driven meta-optimization framework that unifies circuit understanding, adaptive search-space construction, and optimization orchestration in a closed loop. It employs a two-loop optimization framework, with an inner loop for circuit sizing and an outer loop that analyzes optimization dynamics and constraints to iteratively refine the search space from simulation feedback. We further introduce AMS-SizingBench, an open benchmark comprising 24 diverse AMS circuits in SKY130 CMOS technology, designed to evaluate adaptive optimization policies under realistic simulator-based constraints. AutoSizer experimentally achieves higher solution quality, faster convergence, and higher success rate across varying circuit difficulties, outperforming both traditional optimization methods and existing LLM-based agents.

</details>


### [12] [STEER: Inference-Time Risk Control via Constrained Quality-Diversity Search](https://arxiv.org/abs/2602.02862)
*Eric Yang,Jong Ha Lee,Jonathan Amar,Elissa Ye,Yugang Jia*

Main category: cs.AI

TL;DR: 论文指出大语言模型存在模式崩溃问题，提出无训练框架STEER重新引入可调控制，在临床分诊基准测试中取得较好结果，是一种安全的风险控制范式。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在训练平均正确性时存在模式崩溃问题，在序数决策场景中标准对齐会消除基于上下文约束权衡特异性和敏感性的能力。

Method: 提出STEER框架，通过离线的受限质量 - 多样性搜索构建自然语言角色群体，在推理时通过一个可解释控制参数将用户指定风险百分位数映射到选定角色。

Result: 在两个临床分诊基准上，比基于温度的采样和静态角色集合有更广泛的行为覆盖；与代表性的训练后方法相比，在明确紧急情况上保持更高准确性，在模糊决策上有相当的控制能力。

Conclusion: STEER是一种能在不损害领域能力的情况下引导行为的安全的风险控制范式。

Abstract: Large Language Models (LLMs) trained for average correctness often exhibit mode collapse, producing narrow decision behaviors on tasks where multiple responses may be reasonable. This limitation is particularly problematic in ordinal decision settings such as clinical triage, where standard alignment removes the ability to trade off specificity and sensitivity (the ROC operating point) based on contextual constraints. We propose STEER (Steerable Tuning via Evolutionary Ensemble Refinement), a training-free framework that reintroduces this tunable control. STEER constructs a population of natural-language personas through an offline, constrained quality-diversity search that promotes behavioral coverage while enforcing minimum safety, reasoning, and stability thresholds. At inference time, STEER exposes a single, interpretable control parameter that maps a user-specified risk percentile to a selected persona, yielding a monotonic adjustment of decision conservativeness. On two clinical triage benchmarks, STEER achieves broader behavioral coverage compared to temperature-based sampling and static persona ensembles. Compared to a representative post-training method, STEER maintains substantially higher accuracy on unambiguous urgent cases while providing comparable control over ambiguous decisions. These results demonstrate STEER as a safety-preserving paradigm for risk control, capable of steering behavior without compromising domain competence.

</details>


### [13] ["I May Not Have Articulated Myself Clearly": Diagnosing Dynamic Instability in LLM Reasoning at Inference Time](https://arxiv.org/abs/2602.02863)
*Jinkun Chen,Fengxiang Cheng,Sijia Han,Vlado Keselj*

Main category: cs.AI

TL;DR: 研究能否通过标准API的推理时可观测指标（令牌对数概率）检测大语言模型推理过程中的故障，定义不稳定性信号预测推理失败，发现不稳定的时机影响结果。


<details>
  <summary>Details</summary>
Motivation: 大语言模型推理失败常表现为过程性故障，现有方法多在生成结束时测量，研究能否不经过训练或微调，通过标准API的推理时可观测指标检测这类故障。

Method: 定义结合连续步骤分布偏移和不确定性的简单不稳定性信号，用峰值不稳定强度总结每个推理轨迹。

Result: 不稳定强度能以高于随机水平的AUC预测错误答案，不同模型大小下桶级准确率随不稳定强度单调下降；早期不稳定可能随后稳定并得到正确答案，晚期不稳定更易导致失败。

Conclusion: 该方法与模型无关、无需训练且可复现，可作为诊断工具而非纠正或控制机制。

Abstract: Reasoning failures in large language models (LLMs) are typically measured only at the end of a generation, yet many failures manifest as a process-level breakdown: the model "loses the thread" mid-reasoning. We study whether such breakdowns are detectable from inference-time observables available in standard APIs (token log probabilities), without any training or fine-tuning. We define a simple instability signal that combines consecutive-step distributional shift (JSD) and uncertainty (entropy), summarize each trace by its peak instability strength, and show that this signal reliably predicts failure. Across GSM8K and HotpotQA, instability strength predicts wrong answers with above-chance AUC and yields monotonic bucket-level accuracy decline at scale across model sizes. Crucially, we show that instability is not uniformly harmful: early instability can reflect subsequent stabilization and a correct final answer (\emph{corrective instability}), whereas late instability is more often followed by failure (\emph{destructive instability}), even at comparable peak magnitudes, indicating that recoverability depends not only on how strongly the distribution changes but also on when such changes occur relative to the remaining decoding horizon. The method is model-agnostic, training-free, and reproducible, and is presented as a diagnostic lens rather than a corrective or control mechanism.

</details>


### [14] [Aligning Language Model Benchmarks with Pairwise Preferences](https://arxiv.org/abs/2602.02898)
*Marco Gutierrez,Xinyi Leng,Hannah Cyberey,Jonathan Richard Schwarz,Ahmed Alaa,Thomas Hartvigsen*

Main category: cs.AI

TL;DR: 提出基准对齐概念和BenchAlign方法，使基准能按人类偏好对模型排序，为模型开发提供见解。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型基准难以预测实际效用，需缩小基准与实际效用的差距。

Method: 引入基准对齐概念，提出BenchAlign方法，利用模型问题级性能和部署中收集的模型排名对基准问题学习偏好对齐权重。

Result: 对齐后的基准能准确按人类偏好对不同大小的未见过模型排序，且具有可解释性。

Conclusion: 研究为基准与人类实际偏好对齐的局限性提供见解，有助于加速模型向实际效用发展。

Abstract: Language model benchmarks are pervasive and computationally-efficient proxies for real-world performance. However, many recent works find that benchmarks often fail to predict real utility. Towards bridging this gap, we introduce benchmark alignment, where we use limited amounts of information about model performance to automatically update offline benchmarks, aiming to produce new static benchmarks that predict model pairwise preferences in given test settings. We then propose BenchAlign, the first solution to this problem, which learns preference-aligned weight- ings for benchmark questions using the question-level performance of language models alongside ranked pairs of models that could be collected during deployment, producing new benchmarks that rank previously unseen models according to these preferences. Our experiments show that our aligned benchmarks can accurately rank unseen models according to models of human preferences, even across different sizes, while remaining interpretable. Overall, our work provides insights into the limits of aligning benchmarks with practical human preferences, which stands to accelerate model development towards real utility.

</details>


### [15] [Minimal Computational Preconditions for Subjective Perspective in Artificial Agents](https://arxiv.org/abs/2602.02902)
*Hongju Pae*

Main category: cs.AI

TL;DR: 研究通过最小的、受现象学启发的内部结构将人工代理的主观视角操作化，发现潜在结构的滞后现象可作为机器系统中类似视角主观性的可测量标志。


<details>
  <summary>Details</summary>
Motivation: 将人工代理的主观视角进行操作化。

Method: 将视角实现为缓慢演变的全局潜在状态，该状态调节快速策略动态且不直接针对行为后果进行优化。

Result: 在无奖励且有制度转变的环境中，潜在结构呈现方向依赖的滞后现象，而策略层面行为相对更具反应性。

Conclusion: 这种滞后现象构成了机器系统中类似视角主观性的可测量标志。

Abstract: This study operationalizes subjective perspective in artificial agents by grounding it in a minimal, phenomenologically motivated internal structure. The perspective is implemented as a slowly evolving global latent state that modulates fast policy dynamics without being directly optimized for behavioral consequences. In a reward-free environment with regime shifts, this latent structure exhibits direction-dependent hysteresis, while policy-level behavior remains comparatively reactive. I argue that such hysteresis constitutes a measurable signature of perspective-like subjectivity in machine systems.

</details>


### [16] [FIRE-Bench: Evaluating Agents on the Rediscovery of Scientific Insights](https://arxiv.org/abs/2602.02905)
*Zhen Wang,Fan Bai,Zhongyan Luo,Jinyan Su,Kaiser Sun,Xinle Yu,Jieyuan Liu,Kun Zhou,Claire Cardie,Mark Dredze,Eric P. Xing,Zhiting Hu*

Main category: cs.AI

TL;DR: 介绍FIRE - Bench基准测试评估大语言模型驱动的自主智能体的科学发现能力，结果显示当前智能体系统进行全周期科研仍具挑战。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试在评估大语言模型驱动的自主智能体可验证发现能力方面存在不足，需新基准测试。

Method: 引入FIRE - Bench基准测试，让智能体重新发现近期高影响力机器学习研究中的既定发现，并使用前沿大语言模型骨干的智能体进行评估。

Result: 当前智能体系统进行全周期科研仍具挑战，最强智能体重新发现成功率有限，运行差异大，在实验设计、执行和基于证据的推理方面存在反复出现的失败模式。

Conclusion: FIRE - Bench为衡量可靠的智能体驱动的科学发现进展提供了严格且具诊断性的框架。

Abstract: Autonomous agents powered by large language models (LLMs) promise to accelerate scientific discovery end-to-end, but rigorously evaluating their capacity for verifiable discovery remains a central challenge. Existing benchmarks face a trade-off: they either heavily rely on LLM-as-judge evaluations of automatically generated research outputs or optimize convenient yet isolated performance metrics that provide coarse proxies for scientific insight. To address this gap, we introduce FIRE-Bench (Full-cycle Insight Rediscovery Evaluation), a benchmark that evaluates agents through the rediscovery of established findings from recent, high-impact machine learning research. Agents are given only a high-level research question extracted from a published, verified study and must autonomously explore ideas, design experiments, implement code, execute their plans, and derive conclusions supported by empirical evidence. We evaluate a range of state-of-the-art agents with frontier LLMs backbones like gpt-5 on FIRE-Bench. Our results show that full-cycle scientific research remains challenging for current agent systems: even the strongest agents achieve limited rediscovery success (<50 F1), exhibit high variance across runs, and display recurring failure modes in experimental design, execution, and evidence-based reasoning. FIRE-Bench provides a rigorous and diagnostic framework for measuring progress toward reliable agent-driven scientific discovery.

</details>


### [17] [Reasoning about Reasoning: BAPO Bounds on Chain-of-Thought Token Complexity in LLMs](https://arxiv.org/abs/2602.02909)
*Kiran Tomlinson,Tobias Schnabel,Adith Swaminathan,Jennifer Neville*

Main category: cs.AI

TL;DR: 研究解决了推理时CoT推理所需推理令牌数量与输入大小的理论问题，给出了三个任务的上下界，并通过实验验证。


<details>
  <summary>Details</summary>
Motivation: 推理时CoT推理虽提升性能但有延迟和计算成本，解决推理所需token数量随输入大小增长的理论问题。

Method: 扩展BAPO模型，证明三个典型BAPO - 困难任务的CoT tokens下界，通过显式构造给出上界，并进行前沿推理模型实验。

Result: 三个任务在输入大小为n时需Ω(n)推理tokens，实验显示推理token近似线性缩放，小推理预算下模型失败。

Conclusion: 研究确定了CoT推理时计算的基本瓶颈，为分析最优推理长度提供了原则性工具。

Abstract: Inference-time scaling via chain-of-thought (CoT) reasoning is a major driver of state-of-the-art LLM performance, but it comes with substantial latency and compute costs. We address a fundamental theoretical question: how many reasoning tokens are required to solve a problem as input size grows? By extending the bounded attention prefix oracle (BAPO) model--an abstraction of LLMs that quantifies the information flow required to solve a task--we prove lower bounds on the CoT tokens required for three canonical BAPO-hard tasks: binary majority, triplet matching, and graph reachability. We show that each requires $Ω(n)$ reasoning tokens when the input size is $n$. We complement these results with matching or near-matching upper bounds via explicit constructions. Finally, our experiments with frontier reasoning models show approximately linear reasoning token scaling on these tasks and failures when constrained to smaller reasoning budgets, consistent with our theoretical lower bounds. Together, our results identify fundamental bottlenecks in inference-time compute through CoT and offer a principled tool for analyzing optimal reasoning length.

</details>


### [18] [DeltaEvolve: Accelerating Scientific Discovery through Momentum-Driven Evolution](https://arxiv.org/abs/2602.02919)
*Jiachen Jiang,Tianyu Ding,Zhihui Zhu*

Main category: cs.AI

TL;DR: 现有基于大语言模型的进化系统存在问题，本文提出DeltaEvolve框架，用结构化语义增量替代全代码历史，减少输入令牌并在多领域任务中表现更好。


<details>
  <summary>Details</summary>
Motivation: 现有如AlphaEvolve等方法依赖全代码历史，存在上下文效率低和进化指导弱的问题。

Method: 将进化代理形式化为期望最大化框架，提出DeltaEvolve框架，用结构化语义增量替代全代码历史，通过多级数据库和渐进披露机制组织语义增量。

Result: 在多个科学领域的任务实证评估中，该框架能用更少的令牌消耗找到更好的解决方案。

Conclusion: DeltaEvolve框架在基于全代码的进化代理中表现更优，能以更少资源获得更好结果。

Abstract: LLM-driven evolutionary systems have shown promise for automated science discovery, yet existing approaches such as AlphaEvolve rely on full-code histories that are context-inefficient and potentially provide weak evolutionary guidance. In this work, we first formalize the evolutionary agents as a general Expectation-Maximization framework, where the language model samples candidate programs (E-step) and the system updates the control context based on evaluation feedback (M-step). Under this view, constructing context via full-code snapshots constitutes a suboptimal M-step, as redundant implement details dilutes core algorithmic ideas, making it difficult to provide clear inspirations for evolution. To address this, we propose DeltaEvolve, a momentum-driven evolutionary framework that replaces full-code history with structured semantic delta capturing how and why modifications between successive nodes affect performance. As programs are often decomposable, semantic delta usually contains many effective components which are transferable and more informative to drive improvement. By organizing semantic delta through multi-level database and progressive disclosure mechanism, input tokens are further reduced. Empirical evaluations on tasks across diverse scientific domains show that our framework can discover better solution with less token consumption over full-code-based evolutionary agents.

</details>


### [19] [UAT-LITE: Inference-Time Uncertainty-Aware Attention for Pretrained Transformers](https://arxiv.org/abs/2602.02952)
*Elias Hossain,Shubhashis Roy Dipta,Subash Neupane,Rajib Rana,Ravid Shwartz-Ziv,Ivan Garibay,Niloofar Yousefi*

Main category: cs.AI

TL;DR: 提出UAT - LITE推理时间框架提升预训练变压器分类器不确定性感知，减少校准误差、提升选择性预测和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 神经NLP模型常校准错误，现有校准方法存在局限性，如事后校准不改变内部计算，集成和贝叶斯方法成本高。

Method: 提出UAT - LITE框架，通过蒙特卡罗Dropout实现近似贝叶斯推理使自注意力具有不确定性感知，估计词级认知不确定性来调制自注意力；引入层间方差分解诊断预测不确定性累积。

Result: 在SQuAD 2.0、MNLI和SST - 2上，相对微调的BERT - base基线平均减少约20%的预期校准误差，保持任务准确率，提升选择性预测和分布偏移下的鲁棒性。

Conclusion: UAT - LITE能有效提升模型校准性、选择性预测和鲁棒性。

Abstract: Neural NLP models are often miscalibrated, assigning high confidence to incorrect predictions, which undermines selective prediction and high-stakes deployment. Post-hoc calibration methods adjust output probabilities but leave internal computation unchanged, while ensemble and Bayesian approaches improve uncertainty at substantial training or storage cost. We propose UAT-LITE, an inference-time framework that makes self-attention uncertainty-aware using approximate Bayesian inference via Monte Carlo dropout in pretrained transformer classifiers. Token-level epistemic uncertainty is estimated from stochastic forward passes and used to modulate self-attention during contextualization, without modifying pretrained weights or training objectives. We additionally introduce a layerwise variance decomposition to diagnose how predictive uncertainty accumulates across transformer depth. Across the SQuAD 2.0 answerability, MNLI, and SST-2, UAT-LITE reduces Expected Calibration Error by approximately 20% on average relative to a fine-tuned BERT-base baseline while preserving task accuracy, and improves selective prediction and robustness under distribution shift.

</details>


### [20] [Generative Engine Optimization: A VLM and Agent Framework for Pinterest Acquisition Growth](https://arxiv.org/abs/2602.02961)
*Faye Zhang,Qianyu Cheng,Jasmine Wan,Vishwakarma Singh,Jinfeng Rao,Kofi Boakye*

Main category: cs.AI

TL;DR: 介绍大语言模型对内容发现的变革，提出Pinterest GEO框架应对视觉内容平台挑战，该框架实现流量和用户增长。


<details>
  <summary>Details</summary>
Motivation: 大语言模型驱动的生成式搜索带来范式转变，视觉内容平台面临挑战，需新方法提升竞争力。

Method: 提出Pinterest GEO框架，微调视觉语言模型预测搜索词，用AI挖掘实时趋势，构建语义连贯页面，采用混合架构建立权威链接结构。

Result: 在数十亿图像和数千万集合上部署，带来20%有机流量增长和数百万月活用户增长。

Conclusion: Pinterest GEO为视觉平台在生成式搜索时代发展提供了有效途径。

Abstract: Large Language Models are fundamentally reshaping content discovery through AI-native search systems such as ChatGPT, Gemini, and Claude. Unlike traditional search engines that match keywords to documents, these systems infer user intent, synthesize multimodal evidence, and generate contextual answers directly on the search page, introducing a paradigm shift from Search Engine Optimization (SEO) to Generative Engine Optimization (GEO). For visual content platforms hosting billions of assets, this poses an acute challenge: individual images lack the semantic depth and authority signals that generative search prioritizes, risking disintermediation as user needs are satisfied in-place without site visits.
  We present Pinterest GEO, a production-scale framework that pioneers reverse search design: rather than generating generic image captions describing what content is, we fine-tune Vision-Language Models (VLMs) to predict what users would actually search for, augmented this with AI agents that mine real-time internet trends to capture emerging search demand. These VLM-generated queries then drive construction of semantically coherent Collection Pages via multimodal embeddings, creating indexable aggregations optimized for generative retrieval. Finally, we employ hybrid VLM and two-tower ANN architectures to build authority-aware interlinking structures that propagate signals across billions of visual assets. Deployed at scale across billions of images and tens of millions of collections, GEO delivers 20\% organic traffic growth contributing to multi-million monthly active user (MAU) growth, demonstrating a principled pathway for visual platforms to thrive in the generative search era.

</details>


### [21] [Ontology-to-tools compilation for executable semantic constraint enforcement in LLM agents](https://arxiv.org/abs/2602.03439)
*Xiaochi Zhou,Patrick Bulter,Changxuan Yang,Simon D. Rihm,Thitikarn Angkanaporn,Jethro Akroyd,Sebastian Mosbach,Markus Kraft*

Main category: cs.AI

TL;DR: 介绍了本体到工具编译机制，将大语言模型与形式领域知识耦合，并通过案例展示其可引导LLM行为、减少手动工程。


<details>
  <summary>Details</summary>
Motivation: 将大语言模型与形式领域知识进行有效耦合，在生成过程中实施语义约束。

Method: 在The World Avatar中把本体规范编译成可执行工具接口，扩展语义代理组合框架，通过基于代理的工作流将本体转化为工具并迭代应用。

Result: 使用金属 - 有机多面体合成文献案例，展示了可执行的本体语义能引导LLM行为。

Conclusion: 建立了将形式知识嵌入生成系统的通用范式。

Abstract: We introduce ontology-to-tools compilation as a proof-of-principle mechanism for coupling large language models (LLMs) with formal domain knowledge. Within The World Avatar (TWA), ontological specifications are compiled into executable tool interfaces that LLM-based agents must use to create and modify knowledge graph instances, enforcing semantic constraints during generation rather than through post-hoc validation. Extending TWA's semantic agent composition framework, the Model Context Protocol (MCP) and associated agents are integral components of the knowledge graph ecosystem, enabling structured interaction between generative models, symbolic constraints, and external resources. An agent-based workflow translates ontologies into ontology-aware tools and iteratively applies them to extract, validate, and repair structured knowledge from unstructured scientific text. Using metal-organic polyhedra synthesis literature as an illustrative case, we show how executable ontological semantics can guide LLM behaviour and reduce manual schema and prompt engineering, establishing a general paradigm for embedding formal knowledge into generative systems.

</details>


### [22] [Structuring Value Representations via Geometric Coherence in Markov Decision Processes](https://arxiv.org/abs/2602.02978)
*Zuyuan Zhang,Zeyu Fang,Tian Lan*

Main category: cs.AI

TL;DR: 本文从序理论视角看待强化学习，提出GCR - RL方法，开发两种算法，分析理论性质和收敛率，实验显示其在样本效率和稳定性上有显著提升。


<details>
  <summary>Details</summary>
Motivation: 利用几何性质稳定和加速强化学习，从序理论角度提出新的强化学习方法。

Method: 提出GCR - RL方法，通过计算超偏序集细化序列确保几何一致性，开发基于Q学习和演员 - 评论家的两种算法。

Result: 在一系列任务中进行实验，相比强基线在样本效率和稳定性能上有显著改进。

Conclusion: 从序理论角度提出的GCR - RL方法能有效提升强化学习的样本效率和稳定性。

Abstract: Geometric properties can be leveraged to stabilize and speed reinforcement learning. Existing examples include encoding symmetry structure, geometry-aware data augmentation, and enforcing structural restrictions. In this paper, we take a novel view of RL through the lens of order theory and recast value function estimates into learning a desired poset (partially ordered set). We propose \emph{GCR-RL} (Geometric Coherence Regularized Reinforcement Learning) that computes a sequence of super-poset refinements -- by refining posets in previous steps and learning additional order relationships from temporal difference signals -- thus ensuring geometric coherence across the sequence of posets underpinning the learned value functions. Two novel algorithms by Q-learning and by actor--critic are developed to efficiently realize these super-poset refinements. Their theoretical properties and convergence rates are analyzed. We empirically evaluate GCR-RL in a range of tasks and demonstrate significant improvements in sample efficiency and stable performance over strong baselines.

</details>


### [23] [Are LLMs Biased Like Humans? Causal Reasoning as a Function of Prior Knowledge, Irrelevant Information, and Reasoning Budget](https://arxiv.org/abs/2602.02983)
*Hanna M. Dettki,Charley M. Wu,Bob Rehder*

Main category: cs.AI

TL;DR: 对20多个大语言模型（LLMs）在11个因果判断任务上与人类基线进行对比，分析其因果判断策略和鲁棒性，指出LLMs可补充人类，但规则式推理有局限性。


<details>
  <summary>Details</summary>
Motivation: 明确大语言模型在因果推理领域的判断依据，是规范因果计算、类人捷径还是脆弱模式匹配。

Method: 在由撞车结构形式化的11个因果判断任务上对20+个大语言模型与匹配的人类基线进行基准测试；探究大语言模型因果判断在语义抽象和提示过载下的鲁棒性。

Result: 小的可解释模型能很好压缩大语言模型的因果判断；多数大语言模型比人类更具规则式推理策略；多数大语言模型未体现人类典型的撞车偏差；思维链可提高许多大语言模型的鲁棒性。

Conclusion: 大语言模型在避免已知偏差时可补充人类，但在存在内在不确定性时规则式推理可能失效，需刻画其推理策略以安全有效部署。

Abstract: Large language models (LLMs) are increasingly used in domains where causal reasoning matters, yet it remains unclear whether their judgments reflect normative causal computation, human-like shortcuts, or brittle pattern matching. We benchmark 20+ LLMs against a matched human baseline on 11 causal judgment tasks formalized by a collider structure ($C_1 \!\rightarrow\! E\! \leftarrow \!C_2$). We find that a small interpretable model compresses LLMs' causal judgments well and that most LLMs exhibit more rule-like reasoning strategies than humans who seem to account for unmentioned latent factors in their probability judgments. Furthermore, most LLMs do not mirror the characteristic human collider biases of weak explaining away and Markov violations. We probe LLMs' causal judgment robustness under (i) semantic abstraction and (ii) prompt overloading (injecting irrelevant text), and find that chain-of-thought (CoT) increases robustness for many LLMs. Together, this divergence suggests LLMs can complement humans when known biases are undesirable, but their rule-like reasoning may break down when uncertainty is intrinsic -- highlighting the need to characterize LLM reasoning strategies for safe, effective deployment.

</details>


### [24] [Large Language Models Can Take False First Steps at Inference-time Planning](https://arxiv.org/abs/2602.02991)
*Haijiang Yan,Jian-Qiao Zhu,Adam Sanborn*

Main category: cs.AI

TL;DR: 本文提出贝叶斯解释，说明大语言模型推理时规划行为短视的原因，并通过两个实验验证。


<details>
  <summary>Details</summary>
Motivation: 大语言模型训练时获得规划能力，但推理时规划行为短视且与能力不符，需解释差距。

Method: 提出基于生成上下文的贝叶斯解释模型，通过随机生成任务和高斯采样任务验证。

Result: 随机生成任务显示人类提示下规划受限，自生成上下文积累时规划强度增加；高斯采样任务显示以自生成序列为条件时初始偏差降低。

Conclusion: 为刻画大语言模型推理时的前瞻规划能力提供理论解释和实证证据。

Abstract: Large language models (LLMs) have been shown to acquire sequence-level planning abilities during training, yet their planning behavior exhibited at inference time often appears short-sighted and inconsistent with these capabilities. We propose a Bayesian account for this gap by grounding planning behavior in the evolving generative context: given the subtle differences between natural language and the language internalized by LLMs, accumulated self-generated context drives a planning-shift during inference and thereby creates the appearance of compromised planning behavior. We further validate the proposed model through two controlled experiments: a random-generation task demonstrating constrained planning under human prompts and increasing planning strength as self-generated context accumulates, and a Gaussian-sampling task showing reduced initial bias when conditioning on self-generated sequences. These findings provide a theoretical explanation along with empirical evidence for characterizing how LLMs plan ahead during inference.

</details>


### [25] [Agent Alpha: Tree Search Unifying Generation, Exploration and Evaluation for Computer-Use Agents](https://arxiv.org/abs/2602.02995)
*Sizhe Tang,Rongqian Chen,Tian Lan*

Main category: cs.AI

TL;DR: 本文提出Agent Alpha框架，通过步骤级MCTS协同生成、探索和评估，在OSWorld基准测试中取得77%成功率，优于轨迹级基线。


<details>
  <summary>Details</summary>
Motivation: 现有GUI代理缺乏回归能力，无法复用部分成功和从早期失误中恢复。

Method: 引入Agent Alpha框架，通过步骤级MCTS协同生成、探索和评估，集成alpha - UCT引导搜索，采用比较驱动评估和多样性约束扩展。

Result: 在OSWorld基准测试中，Agent Alpha达到约77%的成功率，在同等计算条件下显著优于轨迹级基线。

Conclusion: Agent Alpha框架有效解决了现有GUI代理缺乏回归能力的问题，提高了规划效率和成功率。

Abstract: While scaling test-time compute through trajectory-level sampling has significantly improved Graphical User Interface (GUI) agents, the lack of regressive ability prevents the reuse of partial successes and the recovery from early missteps. In this paper, we introduce Agent Alpha, a unified framework that synergizes generation, exploration, and evaluation through step-level Monte Carlo Tree Search (MCTS). It enables active modeling or exploiting structures of the planning space. By integrating alpha-UCT guided search into the interaction loop, Agent Alpha enables deliberate planning, facilitating early pruning of suboptimal branches and efficient prefix reuse. We also employ comparison-driven evaluation to mitigate absolute scoring biases and diversity-constrained expansion to maintain a compact, informative search space. Regret bound of alpha-UCT is analyzed. On the OSWorld benchmark, Agent Alpha achieves a state-of-the-art success rate of $\sim 77\%$, significantly outperforming trajectory-level baselines under equivalent compute.

</details>


### [26] [Methods and Open Problems in Differentiable Social Choice: Learning Mechanisms, Decisions, and Alignment](https://arxiv.org/abs/2602.03003)
*Zhiyu An,Wan Du*

Main category: cs.AI

TL;DR: 社会选择成现代机器学习系统基础组件，综述可微社会选择范式，总结相关工作并提出36个开放问题。


<details>
  <summary>Details</summary>
Motivation: 许多当代机器学习系统已隐含实施社会选择机制却缺乏明确规范审查，需研究可微社会选择。

Method: 对拍卖、投票、预算等多领域的可微社会选择工作进行综合分析。

Result: 展示经典公理和不可能结果以目标、约束和优化权衡形式再现。

Conclusion: 确定36个开放问题，形成机器学习、经济学和民主理论交叉领域新研究议程。

Abstract: Social choice is no longer a peripheral concern of political theory or economics-it has become a foundational component of modern machine learning systems. From auctions and resource allocation to federated learning, participatory governance, and the alignment of large language models, machine learning pipelines increasingly aggregate heterogeneous preferences, incentives, and judgments into collective decisions. In effect, many contemporary machine learning systems already implement social choice mechanisms, often implicitly and without explicit normative scrutiny.
  This Review surveys differentiable social choice: an emerging paradigm that formulates voting rules, mechanisms, and aggregation procedures as learnable, differentiable models optimized from data. We synthesize work across auctions, voting, budgeting, liquid democracy, decentralized aggregation, and inverse mechanism learning, showing how classical axioms and impossibility results reappear as objectives, constraints, and optimization trade-offs. We conclude by identifying 36 open problems defining a new research agenda at the intersection of machine learning, economics, and democratic theory.

</details>


### [27] [Distilling LLM Reasoning into Graph of Concept Predictors](https://arxiv.org/abs/2602.03006)
*Ziyang Yu,Liang Zhao*

Main category: cs.AI

TL;DR: 提出推理感知的主动蒸馏框架GCP以降低大语言模型推理成本，在NLP分类基准测试中表现良好。


<details>
  <summary>Details</summary>
Motivation: 大语言模型用于判别任务时受推理延迟、计算和API成本限制，现有主动蒸馏方法存在不足。

Method: 提出GCP框架，将教师决策过程外化为有向无环图，用模块化概念预测器镜像，通过图感知获取策略提高样本效率，进行有针对性的子模块再训练。

Result: 在八个NLP分类基准测试中，GCP在有限标注预算下提升了性能，训练动态更具可解释性和可控性。

Conclusion: GCP是一种有效的推理感知主动蒸馏方法。

Abstract: Deploying Large Language Models (LLMs) for discriminative workloads is often limited by inference latency, compute, and API costs at scale. Active distillation reduces these costs by querying an LLM oracle to train compact discriminative students, but most pipelines distill only final labels, discarding intermediate reasoning signals and offering limited diagnostics of what reasoning is missing and where errors arise. We propose Graph of Concept Predictors (GCP), a reasoning-aware active distillation framework that externalizes the teacher's decision process as a directed acyclic graph and mirrors it with modular concept predictors in the student. GCP enhances sample efficiency through a graph-aware acquisition strategy that targets uncertainty and disagreement at critical reasoning nodes. Additionally, it improves training stability and efficiency by performing targeted sub-module retraining, which attributes downstream loss to specific concept predictors and updates only the most influential modules. Experiments on eight NLP classification benchmarks demonstrate that GCP enhances performance under limited annotation budgets while yielding more interpretable and controllable training dynamics. Code is available at: https://github.com/Ziyang-Yu/GCP.

</details>


### [28] [STAR: Similarity-guided Teacher-Assisted Refinement for Super-Tiny Function Calling Models](https://arxiv.org/abs/2602.03022)
*Jiliang Ni,Jiachen Pu,Zhongyi Yang,Jingfeng Luo,Conggang Hu*

Main category: cs.AI

TL;DR: 现有范式在将大语言模型能力迁移到小模型时存在问题，提出STAR框架，通过CKD和Sim - RL实现大模型能力向超小模型迁移，实验表现优异。


<details>
  <summary>Details</summary>
Motivation: 大语言模型因规模大难以广泛应用，需将其能力迁移到小模型，但现有范式存在过拟合、训练不稳定等问题。

Method: 引入STAR框架，包括CKD和Sim - RL。CKD增强前向KL散度抑制错误预测；Sim - RL引入基于相似度的细粒度奖励信号进行策略优化。

Result: STAR模型在对应尺寸类别中达SOTA，0.6B的STAR模型在小于1B的模型中表现最佳，超越部分更大规模模型。

Conclusion: STAR框架可将大模型能力迁移到超小模型，为强大、易用和高效的AI智能体发展提供了途径。

Abstract: The proliferation of Large Language Models (LLMs) in function calling is pivotal for creating advanced AI agents, yet their large scale hinders widespread adoption, necessitating transferring their capabilities into smaller ones. However, existing paradigms are often plagued by overfitting, training instability, ineffective binary rewards for multi-solution tasks, and the difficulty of synergizing techniques. We introduce STAR: Similarity-guided Teacher-Assisted Refinement, a novel holistic framework that effectively transfers LLMs' capabilities to super-tiny models. STAR consists of two core technical innovations: (1) Constrained Knowledge Distillation (CKD), a training objective that augments top-k forward KL divergence to suppress confidently incorrect predictions, ensuring training stability while preserving exploration capacity for downstream RL. STAR holistically synergizes these strategies within a cohesive training curriculum, enabling super-tiny models to achieve exceptional performance on complex function calling tasks; (2) Similarity-guided RL (Sim-RL), a RL mechanism that introduces a fine-grained, similarity-based reward. This provides a robust, continuous, and rich signal for better policy optimization by evaluating the similarity between generated outputs and the ground truth. Extensive experiments on challenging and renowned benchmarks demonstrate the effectiveness of our method. Our STAR models establish SOTA in their size classes, significantly outperforming baselines. Remarkably, our 0.6B STAR model achieves the best performance among all open models under 1B, surpassing even several well-known open models at a larger scale. STAR demonstrates a training framework that distills capabilities of LLMs into super-tiny models, paving the way for powerful, accessible, and efficient AI agents.

</details>


### [29] [RC-GRPO: Reward-Conditioned Group Relative Policy Optimization for Multi-Turn Tool Calling Agents](https://arxiv.org/abs/2602.03025)
*Haitian Zhong,Jixiu Zhai,Lei Song,Jiang Bian,Qiang Liu,Tieniu Tan*

Main category: cs.AI

TL;DR: 提出RC - GRPO解决大语言模型多轮工具调用中奖励稀疏和探索成本高的问题，在基准测试中表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 大语言模型多轮工具调用存在奖励稀疏和探索成本高的问题，现有SFT+GRPO方法在组内奖励变化低时会停滞。

Method: 提出RC - GRPO，先在混合质量轨迹上微调奖励条件轨迹策略，在强化学习中在每个GRPO组内采样不同奖励令牌以提高组内多样性。

Result: 在BFCLv4多轮基准测试中，方法性能始终优于基线，Qwen - 2.5 - 7B - Instruct的性能甚至超过所有闭源API模型。

Conclusion: RC - GRPO能有效解决大语言模型多轮工具调用的问题，提升性能。

Abstract: Multi-turn tool calling is challenging for Large Language Models (LLMs) because rewards are sparse and exploration is expensive. A common recipe, SFT followed by GRPO, can stall when within-group reward variation is low (e.g., more rollouts in a group receive the all 0 or all 1 reward), making the group-normalized advantage uninformative and yielding vanishing updates. To address this problem, we propose RC-GRPO (Reward-Conditioned Group Relative Policy Optimization), which treats exploration as a controllable steering problem via discrete reward tokens. We first fine-tune a Reward-Conditioned Trajectory Policy (RCTP) on mixed-quality trajectories with reward goal special tokens (e.g., <|high_reward|>, <|low_reward|>) injected into the prompts, enabling the model to learn how to generate distinct quality trajectories on demand. Then during RL, we sample diverse reward tokens within each GRPO group and condition rollouts on the sampled token to improve within-group diversity, improving advantage gains. On the Berkeley Function Calling Leaderboard v4 (BFCLv4) multi-turn benchmark, our method yields consistently improved performance than baselines, and the performance on Qwen-2.5-7B-Instruct even surpasses all closed-source API models.

</details>


### [30] [Visual Reasoning over Time Series via Multi-Agent System](https://arxiv.org/abs/2602.03026)
*Weilin Ruan,Yuxuan Liang*

Main category: cs.AI

TL;DR: 提出用于通用时间序列任务的工具驱动多智能体系统MAS4TS，实验显示其在多任务上达SOTA水平，有强泛化和高效推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列方法在集成直观视觉推理和跨任务自适应工具使用泛化方面存在局限。

Method: 基于Analyzer - Reasoner - Executor范式构建MAS4TS，用视觉语言模型进行视觉推理，在潜在空间重建预测轨迹，三个智能体通过共享内存和门控通信协作，路由器选择工具链执行。

Result: 在多个基准测试上，MAS4TS在广泛时间序列任务中达到了最先进的性能。

Conclusion: MAS4TS具有很强的泛化能力和高效的推理能力。

Abstract: Time series analysis underpins many real-world applications, yet existing time-series-specific methods and pretrained large-model-based approaches remain limited in integrating intuitive visual reasoning and generalizing across tasks with adaptive tool usage. To address these limitations, we propose MAS4TS, a tool-driven multi-agent system for general time series tasks, built upon an Analyzer-Reasoner-Executor paradigm that integrates agent communication, visual reasoning, and latent reconstruction within a unified framework. MAS4TS first performs visual reasoning over time series plots with structured priors using a Vision-Language Model to extract temporal structures, and subsequently reconstructs predictive trajectories in latent space. Three specialized agents coordinate via shared memory and gated communication, while a router selects task-specific tool chains for execution. Extensive experiments on multiple benchmarks demonstrate that MAS4TS achieves state-of-the-art performance across a wide range of time series tasks, while exhibiting strong generalization and efficient inference.

</details>


### [31] [KANFIS A Neuro-Symbolic Framework for Interpretable and Uncertainty-Aware Learning](https://arxiv.org/abs/2602.03034)
*Binbin Yong,Haoran Pei,Jun Shen,Haoran Li,Qingguo Zhou,Zhao Su*

Main category: cs.AI

TL;DR: 提出KANFIS解决传统ANFIS架构结构复杂问题，兼容两种模糊逻辑系统，生成紧凑规则集，性能有竞争力。


<details>
  <summary>Details</summary>
Motivation: 传统ANFIS架构存在结构复杂性问题，在高维空间中基于产品的推理机制会导致规则呈指数级爆炸。

Method: 提出KANFIS，采用加法聚合机制，兼容T1和IT2模糊逻辑系统，使用稀疏掩码机制生成紧凑规则集。

Result: KANFIS在与代表性神经和神经模糊基线的比较中取得了具有竞争力的性能。

Conclusion: KANFIS解决了传统ANFIS的问题，是一种紧凑、可解释的神经符号架构。

Abstract: Adaptive Neuro-Fuzzy Inference System (ANFIS) was designed to combine the learning capabilities of neural network with the reasoning transparency of fuzzy logic. However, conventional ANFIS architectures suffer from structural complexity, where the product-based inference mechanism causes an exponential explosion of rules in high-dimensional spaces. We herein propose the Kolmogorov-Arnold Neuro-Fuzzy Inference System (KANFIS), a compact neuro-symbolic architecture that unifies fuzzy reasoning with additive function decomposition. KANFIS employs an additive aggregation mechanism, under which both model parameters and rule complexity scale linearly with input dimensionality rather than exponentially. Furthermore, KANFIS is compatible with both Type-1 (T1) and Interval Type-2 (IT2) fuzzy logic systems, enabling explicit modeling of uncertainty and ambiguity in fuzzy representations. By using sparse masking mechanisms, KANFIS generates compact and structured rule sets, resulting in an intrinsically interpretable model with clear rule semantics and transparent inference processes. Empirical results demonstrate that KANFIS achieves competitive performance against representative neural and neuro-fuzzy baselines.

</details>


### [32] [MAS-ProVe: Understanding the Process Verification of Multi-Agent Systems](https://arxiv.org/abs/2602.03053)
*Vishal Venkataramani,Haizhou Shi,Zixuan Ke,Austin Xu,Xiaoxiao He,Yingbo Zhou,Semih Yavuz,Hao Wang,Shafiq Joty*

Main category: cs.AI

TL;DR: 研究多智能体系统（MAS）中过程验证的有效性，发现其存在问题，有效稳健的过程验证仍是挑战。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型构建的MAS推理轨迹方差大，过程验证在MAS中的实际效果不明。

Method: 开展MAS - ProVe研究，涵盖三种验证范式、两种验证粒度，研究五种验证器和四种上下文管理策略，在六个MAS框架和多个推理基准上实验。

Result: 过程级验证不能持续提升性能且方差大；LLM - as - Judge通常优于基于奖励的方法；LLM作为评判者和单智能体表现差距小；验证存在上下文长度与性能的权衡。

Conclusion: MAS的有效稳健过程验证仍是开放挑战，需超越现有范式的进一步进展。

Abstract: Multi-Agent Systems (MAS) built on Large Language Models (LLMs) often exhibit high variance in their reasoning trajectories. Process verification, which evaluates intermediate steps in trajectories, has shown promise in general reasoning settings, and has been suggested as a potential tool for guiding coordination of MAS; however, its actual effectiveness in MAS remains unclear. To fill this gap, we present MAS-ProVe, a systematic empirical study of process verification for multi-agent systems (MAS). Our study spans three verification paradigms (LLM-as-a-Judge, reward models, and process reward models), evaluated across two levels of verification granularity (agent-level and iteration-level). We further examine five representative verifiers and four context management strategies, and conduct experiments over six diverse MAS frameworks on multiple reasoning benchmarks. We find that process-level verification does not consistently improve performance and frequently exhibits high variance, highlighting the difficulty of reliably evaluating partial multi-agent trajectories. Among the methods studied, LLM-as-a-Judge generally outperforms reward-based approaches, with trained judges surpassing general-purpose LLMs. We further observe a small performance gap between LLMs acting as judges and as single agents, and identify a context-length-performance trade-off in verification. Overall, our results suggest that effective and robust process verification for MAS remains an open challenge, requiring further advances beyond current paradigms. Code is available at https://github.com/Wang-ML-Lab/MAS-ProVe.

</details>


### [33] [De-conflating Preference and Qualification: Constrained Dual-Perspective Reasoning for Job Recommendation with Large Language Models](https://arxiv.org/abs/2602.03097)
*Bryce Kan,Wei Yang,Emily Nguyen,Ganghui Yi,Bowen Yi,Chenxiao Yu,Yan Liu*

Main category: cs.AI

TL;DR: 提出JobRec框架用于专业职位推荐，可解耦偏好和资格，在实验中表现优于基线并提升可控性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在专业职位推荐中，将求职者偏好与雇主资格两个决策维度合并为单一交互信号，存在监督混淆和策略可控性受限的问题。

Method: 提出JobRec框架，包括统一语义对齐模式、两阶段合作训练策略和基于拉格朗日的策略对齐模块，并构建专家提炼的合成数据集。

Result: JobRec在实验中始终优于强基线模型。

Conclusion: JobRec能有效解耦偏好和资格，为策略感知的专业匹配提供更好的可控性。

Abstract: Professional job recommendation involves a complex bipartite matching process that must reconcile a candidate's subjective preference with an employer's objective qualification. While Large Language Models (LLMs) are well-suited for modeling the rich semantics of resumes and job descriptions, existing paradigms often collapse these two decision dimensions into a single interaction signal, yielding confounded supervision under recruitment-funnel censoring and limiting policy controllability. To address these challenges, We propose JobRec, a generative job recommendation framework for de-conflating preference and qualification via constrained dual-perspective reasoning. JobRec introduces a Unified Semantic Alignment Schema that aligns candidate and job attributes into structured semantic layers, and a Two-Stage Cooperative Training Strategy that learns decoupled experts to separately infer preference and qualification. Building on these experts, a Lagrangian-based Policy Alignment module optimizes recommendations under explicit eligibility requirements, enabling controllable trade-offs. To mitigate data scarcity, we construct a synthetic dataset refined by experts. Experiments show that JobRec consistently outperforms strong baselines and provides improved controllability for strategy-aware professional matching.

</details>


### [34] [Risky-Bench: Probing Agentic Safety Risks under Real-World Deployment](https://arxiv.org/abs/2602.03100)
*Jingnan Zheng,Yanzhen Luo,Jingjun Xu,Bingnan Liu,Yuxin Chen,Chenhang Cui,Gelei Deng,Chaochao Lu,Xiang Wang,An Zhang,Tat-Seng Chua*

Main category: cs.AI

TL;DR: 提出 Risky - Bench 框架解决现有代理安全评估的局限性，适用于包括生活辅助代理在内的不同场景


<details>
  <summary>Details</summary>
Motivation: 现有代理安全评估覆盖风险空间有限，针对特定设置适应性差，无法评估复杂真实场景中长周期交互任务的安全行为

Method: 构建 Risky - Bench 框架，围绕领域无关安全原则组织评估，推导上下文感知的安全准则，在不同威胁假设下通过实际任务执行评估安全风险

Result: 应用于生活辅助代理设置时，发现了现有先进代理在实际执行条件下的大量安全风险

Conclusion: Risky - Bench 框架可不限于生活辅助场景，能为其他部署场景构建特定环境的安全评估，提供了一种可扩展的代理安全评估方法

Abstract: Large Language Models (LLMs) are increasingly deployed as agents that operate in real-world environments, introducing safety risks beyond linguistic harm. Existing agent safety evaluations rely on risk-oriented tasks tailored to specific agent settings, resulting in limited coverage of safety risk space and failing to assess agent safety behavior during long-horizon, interactive task execution in complex real-world deployments. Moreover, their specialization to particular agent settings limits adaptability across diverse agent configurations. To address these limitations, we propose Risky-Bench, a framework that enables systematic agent safety evaluation grounded in real-world deployment. Risky-Bench organizes evaluation around domain-agnostic safety principles to derive context-aware safety rubrics that delineate safety space, and systematically evaluates safety risks across this space through realistic task execution under varying threat assumptions. When applied to life-assist agent settings, Risky-Bench uncovers substantial safety risks in state-of-the-art agents under realistic execution conditions. Moreover, as a well-structured evaluation pipeline, Risky-Bench is not confined to life-assist scenarios and can be adapted to other deployment settings to construct environment-specific safety evaluations, providing an extensible methodology for agent safety assessment.

</details>


### [35] [Understanding Multi-Agent LLM Frameworks: A Unified Benchmark and Experimental Analysis](https://arxiv.org/abs/2602.03128)
*Abdelghny Orogat,Ana Rostam,Essam Mansour*

Main category: cs.AI

TL;DR: 本文介绍架构分类法和MAFBench评估套件，研究多智能体LLM框架架构选择对性能的影响并给出设计原则和框架选择建议。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体LLM框架架构对系统性能的影响不明，且缺乏标准化框架级评估。

Method: 引入架构分类法，开发统一评估套件MAFBench，对多个常用框架进行受控实证研究。

Result: 框架级设计选择可使延迟增加超100倍，规划准确率降低30%，协调成功率从超90%降至低于30%。

Conclusion: 将研究结果转化为具体架构设计原则和框架选择指导，指出未来研究方向。

Abstract: Multi-agent LLM frameworks are widely used to accelerate the development of agent systems powered by large language models (LLMs). These frameworks impose distinct architectural structures that govern how agents interact, store information, and coordinate tasks. However, their impact on system performance remains poorly understood. This gap is critical, as architectural choices alone can induce order-of-magnitude differences in latency and throughput, as well as substantial variation in accuracy and scalability. Addressing this challenge requires (i) jointly evaluating multiple capabilities, such as orchestration overhead, memory behavior, planning, specialization, and coordination, and (ii) conducting these evaluations under controlled, framework-level conditions to isolate architectural effects. Existing benchmarks focus on individual capabilities and lack standardized framework-level evaluation. We address these limitations by (i) introducing an architectural taxonomy for systematically comparing multi-agent LLM frameworks along fundamental dimensions, and (ii) developing MAFBench, a unified evaluation suite that integrates existing benchmarks under a standardized execution pipeline. Using MAFBench, we conduct a controlled empirical study across several widely used frameworks. Our results show that framework-level design choices alone can increase latency by over 100x, reduce planning accuracy by up to 30%, and lower coordination success from above 90% to below 30%. Finally, we translate our findings into concrete architectural design principles and framework selection guidance, and outline promising future research directions.

</details>


### [36] [General Agents Contain World Models, even under Partial Observability and Stochasticity](https://arxiv.org/abs/2602.03146)
*Santiago Cifuentes*

Main category: cs.AI

TL;DR: 本文将已有定理扩展到部分可观测环境中的随机智能体，证明随机智能体无法通过随机化避免学习环境，且能力较弱的智能体也包含其运行环境的模型。


<details>
  <summary>Details</summary>
Motivation: 原定理依赖智能体确定性和环境完全可观测的假设，本文旨在去除这些假设，进一步研究智能体对环境的学习情况。

Method: 将定理扩展到部分可观测环境中的随机智能体，并通过弱化一般性概念来加强结果。

Result: 证明了随机智能体不能通过随机化避免学习环境，能力较弱的智能体也包含其运行环境的模型。

Conclusion: 随机智能体必然会学习其所处环境，且对智能体一般性要求降低后也能得出其包含环境模型的结论。

Abstract: Deciding whether an agent possesses a model of its surrounding world is a fundamental step toward understanding its capabilities and limitations. In [10], it was shown that, within a particular framework, every almost optimal and general agent necessarily contains sufficient knowledge of its environment to allow an approximate reconstruction of it by querying the agent as a black box. This result relied on the assumptions that the agent is deterministic and that the environment is fully observable.
  In this work, we remove both assumptions by extending the theorem to stochastic agents operating in partially observable environments. Fundamentally, this shows that stochastic agents cannot avoid learning their environment through the usage of randomization. We also strengthen the result by weakening the notion of generality, proving that less powerful agents already contain a model of the world in which they operate.

</details>


### [37] [Enhancing Foundation VLM Robustness to Missing Modality: Scalable Diffusion for Bi-directional Feature Restoration](https://arxiv.org/abs/2602.03151)
*Wei Dai,Haoyu Wang,Honghao Chang,Lijun He,Fan Li,Jian Sun,Haixia Bi*

Main category: cs.AI

TL;DR: 现有视觉语言模型在模态输入缺失时效果不佳，本文提出通用缺失模态恢复策略，经实验验证优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在推理时假设模态输入完整，缺失时效果骤降。现有基于提示和插补的方法存在不足，恢复精确语义和保持模型泛化性有挑战。

Method: 提出通用缺失模态恢复策略，引入增强扩散模型作为可插拔的中期训练模块，采用动态模态门控和跨模态互学习机制。

Result: 零样本评估显示该方法优于现有基线方法，大量实验和消融研究表明模型在缺失模态场景下稳健可扩展。

Conclusion: 所提方法是视觉语言模型在缺失模态场景下的可靠扩展，代码和模型将公开。

Abstract: Vision Language Models (VLMs) typically assume complete modality input during inference. However, their effectiveness drops sharply when certain modalities are unavailable or incomplete. Current research primarily faces two dilemmas: Prompt-based methods struggle to restore missing yet indispensable features and impair generalization of VLMs. Imputation-based approaches, lacking effective guidance, are prone to generating semantically irrelevant noise. Restoring precise semantics while sustaining VLM generalization remains challenging. Therefore, we propose a general missing modality restoration strategy in this paper. We introduce an enhanced diffusion model as a pluggable mid-stage training module to effectively restore missing features. Our strategy introduces two key innovations: (I) Dynamic Modality Gating, which adaptively leverages conditional features to steer the generation of semantically consistent features; (II) Cross-Modal Mutual Learning mechanism, which bridges the semantic spaces of dual encoders to achieve bidirectional alignment. Zero-shot evaluations across benchmark datasets demonstrate that our approach outperforms existing baseline methods. Extensive experiments and ablation studies confirm our model as a robust and scalable extension for VLMs in missing modality scenarios, ensuring reliability across diverse missing rates and environments. Our code and models will be publicly available.

</details>


### [38] [VALUEFLOW: Toward Pluralistic and Steerable Value-based Alignment in Large Language Models](https://arxiv.org/abs/2602.03160)
*Woojin Kim,Sieun Hyeon,Jusang Oh,Jaeyoung Do*

Main category: cs.AI

TL;DR: 论文指出大语言模型与人类价值观对齐存在挑战，引入VALUEFLOW框架解决价值提取、评估和引导等问题，开展大规模研究并建立评估和控制价值强度的基础设施。


<details>
  <summary>Details</summary>
Motivation: 偏好方法难以捕捉深层动机原则，基于价值的方法存在提取忽视层次结构、评估缺乏强度校准、大语言模型可控强度引导能力不足等问题，为解决这些进行研究。

Method: 引入VALUEFLOW框架，包含HIVES、VIDB、基于锚点的评估器三个组件。

Result: 对10个模型和4种价值理论进行大规模研究，发现引导能力的不对称性和多值控制的组成规律。

Conclusion: 建立了评估和控制价值强度的可扩展基础设施，推动大语言模型的多元化对齐。

Abstract: Aligning Large Language Models (LLMs) with the diverse spectrum of human values remains a central challenge: preference-based methods often fail to capture deeper motivational principles. Value-based approaches offer a more principled path, yet three gaps persist: extraction often ignores hierarchical structure, evaluation detects presence but not calibrated intensity, and the steerability of LLMs at controlled intensities remains insufficiently understood. To address these limitations, we introduce VALUEFLOW, the first unified framework that spans extraction, evaluation, and steering with calibrated intensity control. The framework integrates three components: (i) HIVES, a hierarchical value embedding space that captures intra- and cross-theory value structure; (ii) the Value Intensity DataBase (VIDB), a large-scale resource of value-labeled texts with intensity estimates derived from ranking-based aggregation; and (iii) an anchor-based evaluator that produces consistent intensity scores for model outputs by ranking them against VIDB panels. Using VALUEFLOW, we conduct a comprehensive large-scale study across ten models and four value theories, identifying asymmetries in steerability and composition laws for multi-value control. This paper establishes a scalable infrastructure for evaluating and controlling value intensity, advancing pluralistic alignment of LLMs.

</details>


### [39] [Beyond Quantity: Trajectory Diversity Scaling for Code Agents](https://arxiv.org/abs/2602.03219)
*Guhong Chen,Chenghao Sun,Cheng Fu,Qiyao Wang,Zhihong Huang,Chaopeng Wei,Guangxu Chen,Feiteng Fang,Ahmadreza Argha,Bing Zhao,Xander Xu,Qi Han,Hamid Alinejad-Rokny,Qiang Qu,Binhua Li,Shiwen Ni,Min Yang,Hu Wei,Yongbin Li*

Main category: cs.AI

TL;DR: 提出TDScaling数据合成框架，通过轨迹多样性提升代码代理性能，实验证明其能提升工具使用泛化性和编码能力。


<details>
  <summary>Details</summary>
Motivation: 现有代码大语言模型作为工具交互代理时，泛化性受低质量合成数据和数量扩展收益递减限制，且以数量为中心的扩展存在瓶颈，未充分利用轨迹数据。

Method: 提出TDScaling框架，整合业务集群机制、蓝图驱动的多智能体范式、自适应进化机制和沙盒代码工具。

Result: 在通用工具使用基准和代码代理任务的实验中，实现了工具使用泛化和固有编码能力提升的双赢。

Conclusion: TDScaling通过多样性而非数量规模提升代码代理性能，改善了性能 - 成本权衡，计划发布代码库和合成数据集。

Abstract: As code large language models (LLMs) evolve into tool-interactive agents via the Model Context Protocol (MCP), their generalization is increasingly limited by low-quality synthetic data and the diminishing returns of quantity scaling. Moreover, quantity-centric scaling exhibits an early bottleneck that underutilizes trajectory data. We propose TDScaling, a Trajectory Diversity Scaling-based data synthesis framework for code agents that scales performance through diversity rather than raw volume. Under a fixed training budget, increasing trajectory diversity yields larger gains than adding more trajectories, improving the performance-cost trade-off for agent training. TDScaling integrates four innovations: (1) a Business Cluster mechanism that captures real-service logical dependencies; (2) a blueprint-driven multi-agent paradigm that enforces trajectory coherence; (3) an adaptive evolution mechanism that steers synthesis toward long-tail scenarios using Domain Entropy, Reasoning Mode Entropy, and Cumulative Action Complexity to prevent mode collapse; and (4) a sandboxed code tool that mitigates catastrophic forgetting of intrinsic coding capabilities. Experiments on general tool-use benchmarks (BFCL, tau^2-Bench) and code agent tasks (RebenchT, CodeCI, BIRD) demonstrate a win-win outcome: TDScaling improves both tool-use generalization and inherent coding proficiency. We plan to release the full codebase and the synthesized dataset (including 30,000+ tool clusters) upon publication.

</details>


### [40] [TAME: A Trustworthy Test-Time Evolution of Agent Memory with Systematic Benchmarking](https://arxiv.org/abs/2602.03224)
*Yu Cheng,Jiuan Zhou,Yongkang Hu,Yihang Chen,Huichi Zhou,Mingang Chen,Zhizhong Zhang,Kun Shao,Yuan Xie,Zhaoxia Yin*

Main category: cs.AI

TL;DR: 文章指出测试时智能体记忆演化存在Agent Memory Misevolution问题，构建Trust - Memevo基准评估，提出TAME框架解决问题，实验证明其有效。


<details>
  <summary>Details</summary>
Motivation: 解决测试时智能体记忆演化中出现的Agent Memory Misevolution问题，即良性任务演化时智能体安全对齐易受影响的现象。

Method: 构建Trust - Memevo基准评估多维度可信度；提出TAME双记忆进化框架，分别进化执行器记忆和评估器记忆，通过记忆过滤、草稿生成等闭环操作。

Result: TAME缓解了记忆错误进化问题，实现了可信度和任务性能的共同提升。

Conclusion: TAME框架在不牺牲效用的情况下保留了可信度，是解决智能体记忆演化中信任问题的有效方法。

Abstract: Test-time evolution of agent memory serves as a pivotal paradigm for achieving AGI by bolstering complex reasoning through experience accumulation. However, even during benign task evolution, agent safety alignment remains vulnerable-a phenomenon known as Agent Memory Misevolution. To evaluate this phenomenon, we construct the Trust-Memevo benchmark to assess multi-dimensional trustworthiness during benign task evolution, revealing an overall decline in trustworthiness across various task domains and evaluation settings. To address this issue, we propose TAME, a dual-memory evolutionary framework that separately evolves executor memory to improve task performance by distilling generalizable methodologies, and evaluator memory to refine assessments of both safety and task utility based on historical feedback. Through a closed loop of memory filtering, draft generation, trustworthy refinement, execution, and dual-track memory updating, TAME preserves trustworthiness without sacrificing utility. Experiments demonstrate that TAME mitigates misevolution, achieving a joint improvement in both trustworthiness and task performance.

</details>


### [41] [The Necessity of a Unified Framework for LLM-Based Agent Evaluation](https://arxiv.org/abs/2602.03238)
*Pengyu Zhu,Li Sun,Philip S. Yu,Sen Su*

Main category: cs.AI

TL;DR: 大语言模型下通用智能体评估面临挑战，现有基准存在问题，提出统一评估框架提案。


<details>
  <summary>Details</summary>
Motivation: 当前智能体评估存在受无关因素干扰、评估框架碎片化、缺乏标准化数据等问题，影响评估公平性和透明度，需统一评估框架推动评估发展。

Method: 文中未提及具体方法，仅提出标准化智能体评估的提案。

Result: 无具体结果，仅提出提案。

Conclusion: 统一评估框架对智能体评估的严格发展至关重要，提出了标准化评估的提案。

Abstract: With the advent of Large Language Models (LLMs), general-purpose agents have seen fundamental advancements. However, evaluating these agents presents unique challenges that distinguish them from static QA benchmarks. We observe that current agent benchmarks are heavily confounded by extraneous factors, including system prompts, toolset configurations, and environmental dynamics. Existing evaluations often rely on fragmented, researcher-specific frameworks where the prompt engineering for reasoning and tool usage varies significantly, making it difficult to attribute performance gains to the model itself. Additionally, the lack of standardized environmental data leads to untraceable errors and non-reproducible results. This lack of standardization introduces substantial unfairness and opacity into the field. We propose that a unified evaluation framework is essential for the rigorous advancement of agent evaluation. To this end, we introduce a proposal aimed at standardizing agent evaluation.

</details>


### [42] [Accordion-Thinking: Self-Regulated Step Summaries for Efficient and Readable LLM Reasoning](https://arxiv.org/abs/2602.03249)
*Zhicheng Yang,Zhijiang Guo,Yinya Huang,Yongxin Wang,Wenlei Shi,Yiwei Wang,Xiaodan Liang,Jing Tang*

Main category: cs.AI

TL;DR: 本文提出Accordion - Thinking框架，让大语言模型通过动态总结自我调节推理步骤粒度，实现推理上下文有效压缩，在不损失精度下减少对历史标记的依赖，提升推理效率。


<details>
  <summary>Details</summary>
Motivation: 长思维链提升推理能力时面临KV缓存线性增长和注意力复杂度二次增长的实际限制。

Method: 引入Accordion - Thinking端到端框架，采用动态总结让模型学习自我调节推理步骤粒度，有Fold推理模式；应用强化学习激励该能力。

Result: 高效的Fold模式和详尽的Unfold模式的准确率差距在训练中逐渐缩小并消失；Accordion - Thinker在48GB GPU内存配置下保持准确率同时吞吐量提升3倍。

Conclusion: 大语言模型通过学习自我压缩，能以最小的依赖标记开销处理复杂推理任务，且结构化步骤总结便于人类阅读推理过程。

Abstract: Scaling test-time compute via long Chain-ofThought unlocks remarkable gains in reasoning capabilities, yet it faces practical limits due to the linear growth of KV cache and quadratic attention complexity. In this paper, we introduce Accordion-Thinking, an end-to-end framework where LLMs learn to self-regulate the granularity of the reasoning steps through dynamic summarization. This mechanism enables a Fold inference mode, where the model periodically summarizes its thought process and discards former thoughts to reduce dependency on historical tokens. We apply reinforcement learning to incentivize this capability further, uncovering a critical insight: the accuracy gap between the highly efficient Fold mode and the exhaustive Unfold mode progressively narrows and eventually vanishes over the course of training. This phenomenon demonstrates that the model learns to encode essential reasoning information into compact summaries, achieving effective compression of the reasoning context. Our Accordion-Thinker demonstrates that with learned self-compression, LLMs can tackle complex reasoning tasks with minimal dependency token overhead without compromising solution quality, and it achieves a 3x throughput while maintaining accuracy on a 48GB GPU memory configuration, while the structured step summaries provide a human-readable account of the reasoning process.

</details>


### [43] [LPS-Bench: Benchmarking Safety Awareness of Computer-Use Agents in Long-Horizon Planning under Benign and Adversarial Scenarios](https://arxiv.org/abs/2602.03255)
*Tianyu Chen,Chujia Hu,Ge Gao,Dongrui Liu,Xia Hu,Wenjie Wang*

Main category: cs.AI

TL;DR: 提出针对计算机使用代理（CUA）的LPS - Bench基准，评估其长周期任务规划阶段的安全意识，实验发现现有CUA存在安全行为缺陷，并提出改进策略。


<details>
  <summary>Details</summary>
Motivation: 现有基准多关注短周期或基于GUI的任务，评估执行时错误，忽视规划时风险评估能力，因此需要新基准评估CUA长周期任务规划时的安全意识。

Method: 提出LPS - Bench基准，涵盖7个任务域和9种风险类型的65种场景；引入多智能体自动管道进行可扩展数据生成；采用LLM作为评判的评估协议，通过规划轨迹评估安全意识。

Result: 实验表明现有CUA在保持安全行为方面存在重大缺陷。

Conclusion: 分析了风险并提出缓解策略以改善基于MCP的CUA系统的长周期规划安全性，代码已开源。

Abstract: Computer-use agents (CUAs) that interact with real computer systems can perform automated tasks but face critical safety risks. Ambiguous instructions may trigger harmful actions, and adversarial users can manipulate tool execution to achieve malicious goals. Existing benchmarks mostly focus on short-horizon or GUI-based tasks, evaluating on execution-time errors but overlooking the ability to anticipate planning-time risks. To fill this gap, we present LPS-Bench, a benchmark that evaluates the planning-time safety awareness of MCP-based CUAs under long-horizon tasks, covering both benign and adversarial interactions across 65 scenarios of 7 task domains and 9 risk types. We introduce a multi-agent automated pipeline for scalable data generation and adopt an LLM-as-a-judge evaluation protocol to assess safety awareness through the planning trajectory. Experiments reveal substantial deficiencies in existing CUAs' ability to maintain safe behavior. We further analyze the risks and propose mitigation strategies to improve long-horizon planning safety in MCP-based CUA systems. We open-source our code at https://github.com/tychenn/LPS-Bench.

</details>


### [44] [CSR-Bench: A Benchmark for Evaluating the Cross-modal Safety and Reliability of MLLMs](https://arxiv.org/abs/2602.03263)
*Yuxuan Liu,Yuntian Shi,Kun Wang,Haoting Shen,Kun Yang*

Main category: cs.AI

TL;DR: 提出CSR - Bench基准评估多模态大语言模型跨模态可靠性，评估16个模型发现跨模态对齐差距等问题。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型安全行为可能依赖单模态捷径而非真正的联合意图理解，需评估其跨模态可靠性。

Method: 引入CSR - Bench基准，通过四种压力测试交互模式评估，覆盖61种细粒度类型，还提供纯文本对照。

Result: 评估16个模型发现系统的跨模态对齐差距，模型安全意识弱、干扰下语言主导性强、多模态输入性能下降，减少过度拒绝和保持安全无歧视行为存在权衡。

Conclusion: 部分模型的安全改进可能源于拒绝导向启发式而非稳健的意图理解。

Abstract: Multimodal large language models (MLLMs) enable interaction over both text and images, but their safety behavior can be driven by unimodal shortcuts instead of true joint intent understanding. We introduce CSR-Bench, a benchmark for evaluating cross-modal reliability through four stress-testing interaction patterns spanning Safety, Over-rejection, Bias, and Hallucination, covering 61 fine-grained types. Each instance is constructed to require integrated image-text interpretation, and we additionally provide paired text-only controls to diagnose modality-induced behavior shifts. We evaluate 16 state-of-the-art MLLMs and observe systematic cross-modal alignment gaps. Models show weak safety awareness, strong language dominance under interference, and consistent performance degradation from text-only controls to multimodal inputs. We also observe a clear trade-off between reducing over-rejection and maintaining safe, non-discriminatory behavior, suggesting that some apparent safety gains may come from refusal-oriented heuristics rather than robust intent understanding. WARNING: This paper contains unsafe contents.

</details>


### [45] [Agentic Proposing: Enhancing Large Language Model Reasoning via Compositional Skill Synthesis](https://arxiv.org/abs/2602.03279)
*Zhengbo Jiao,Shaobo Wang,Zifan Zhang,Xuan Ren,Wei Wang,Bing Zhao,Hu Wei,Linfeng Zhang*

Main category: cs.AI

TL;DR: 提出Agentic Proposing框架生成高质量可验证训练轨迹，下游求解器在合成数据上训练效果好，小数据量合成信号可替代大量人工标注数据。


<details>
  <summary>Details</summary>
Motivation: 复杂推理的大语言模型需要高质量可验证数据集，但人工标注成本高难扩展，现有合成范式存在权衡问题。

Method: 提出Agentic Proposing框架，将问题合成建模为目标驱动的顺序决策过程，用Multi - Granularity Policy Optimization (MGPO)开发Agentic - Proposer - 4B。

Result: 下游求解器在合成数据上训练显著优于基线，有强跨领域泛化能力，小数据量合成轨迹训练的求解器在AIME25上达91.6%准确率。

Conclusion: 小体积高质量合成信号能有效替代大量人工标注数据集。

Abstract: Advancing complex reasoning in large language models relies on high-quality, verifiable datasets, yet human annotation remains cost-prohibitive and difficult to scale. Current synthesis paradigms often face a recurring trade-off: maintaining structural validity typically restricts problem complexity, while relaxing constraints to increase difficulty frequently leads to inconsistent or unsolvable instances. To address this, we propose Agentic Proposing, a framework that models problem synthesis as a goal-driven sequential decision process where a specialized agent dynamically selects and composes modular reasoning skills. Through an iterative workflow of internal reflection and tool-use, we develop the Agentic-Proposer-4B using Multi-Granularity Policy Optimization (MGPO) to generate high-precision, verifiable training trajectories across mathematics, coding, and science. Empirical results demonstrate that downstream solvers trained on agent-synthesized data significantly outperform leading baselines and exhibit robust cross-domain generalization. Notably, a 30B solver trained on only 11,000 synthesized trajectories achieves a state-of-the-art 91.6% accuracy on AIME25, rivaling frontier-scale proprietary models such as GPT-5 and proving that a small volume of high-quality synthetic signals can effectively substitute for massive human-curated datasets.

</details>


### [46] [MeetBench-XL: Calibrated Multi-Dimensional Evaluation and Learned Dual-Policy Agents for Real-Time Meetings](https://arxiv.org/abs/2602.03285)
*Yuelin Hu,Jun Xu,Bingcong Lu,Zhengxue Cheng,Hongwei Hu,Ronghua Wu,Li Song*

Main category: cs.AI

TL;DR: 针对企业会议环境对AI助手的需求，现有会议基准有不足。本文引入MeetAll数据集、MeetBench XL评估协议和MeetMaster XL代理，实验显示有优势。


<details>
  <summary>Details</summary>
Motivation: 企业会议环境需要能处理多样任务的AI助手，现有会议基准无法反映真实企业工作流程，存在差距。

Method: 引入MeetAll双语多模态语料库，提出MeetBench XL多维评估协议，呈现MeetMaster XL学习型双策略代理。

Result: 实验表明相比商业系统有一致提升，通过消融实验、鲁棒性测试和实际部署案例研究验证。

Conclusion: 通过所提出的数据集、评估协议和代理，能有效满足企业会议环境对AI助手的需求，在质量和延迟上有较好平衡。

Abstract: Enterprise meeting environments require AI assistants that handle diverse operational tasks, from rapid fact checking during live discussions to cross meeting analysis for strategic planning, under strict latency, cost, and privacy constraints. Existing meeting benchmarks mainly focus on simplified question answering and fail to reflect real world enterprise workflows, where queries arise organically from multi stakeholder collaboration, span long temporal contexts, and require tool augmented reasoning.
  We address this gap through a grounded dataset and a learned agent framework. First, we introduce MeetAll, a bilingual and multimodal corpus derived from 231 enterprise meetings totaling 140 hours. Questions are injected using an enterprise informed protocol validated by domain expert review and human discriminability studies. Unlike purely synthetic benchmarks, this protocol is grounded in four enterprise critical dimensions: cognitive load, temporal context span, domain expertise, and actionable task execution, calibrated through interviews with stakeholders across finance, healthcare, and technology sectors.
  Second, we propose MeetBench XL, a multi dimensional evaluation protocol aligned with human judgment that measures factual fidelity, intent alignment, response efficiency, structural clarity, and completeness. Third, we present MeetMaster XL, a learned dual policy agent that jointly optimizes query routing between fast and slow reasoning paths and tool invocation, including retrieval, cross meeting aggregation, and web search. A lightweight classifier enables accurate routing with minimal overhead, achieving a superior quality latency tradeoff over single model baselines. Experiments against commercial systems show consistent gains, supported by ablations, robustness tests, and a real world deployment case study.Resources: https://github.com/huyuelin/MeetBench.

</details>


### [47] [Rejecting Arguments Based on Doubt in Structured Bipolar Argumentation](https://arxiv.org/abs/2602.03286)
*Michael A. Müller,Srdjan Vesic,Bruno Yun*

Main category: cs.AI

TL;DR: 本文结合哲学和语言学观点开发了一种新的计算论证方法，定义了SBAFs并给出其语义，该方法能为现有方法提供新视角。


<details>
  <summary>Details</summary>
Motivation: 将文献中鲜有关注的两个想法（基于怀疑理性拒绝论证、以句子而非论证思考）融入计算方法。

Method: 定义结构化双极论证框架（SBAFs），并为其提供具有不强迫接受所有被辩护论证、同时给出句子可接受集合语义特点的语义。

Result: 语义介于抽象论证的可允许语义和完全语义之间，可明确忽略论证间支持的条件，演绎支持语义是该方法的特例。

Conclusion: 所提出的新方法能为计算论证提供一种新思路，并为现有方法提供新视角。

Abstract: This paper develops a new approach to computational argumentation that is informed by philosophical and linguistic views. Namely, it takes into account two ideas that have received little attention in the literature on computational argumentation: First, an agent may rationally reject an argument based on mere doubt, thus not all arguments they could defend must be accepted; and, second, that it is sometimes more natural to think in terms of which individual sentences or claims an agent accepts in a debate, rather than which arguments. In order to incorporate these two ideas into a computational approach, we first define the notion of structured bipolar argumentation frameworks (SBAFs), where arguments consist of sentences and we have both an attack and a support relation between them. Then, we provide semantics for SBAFs with two features: (1) Unlike with completeness-based semantics, our semantics do not force agents to accept all defended arguments. (2) In addition to argument extensions, which give acceptable sets of arguments, we also provide semantics for language extensions that specify acceptable sets of sentences. These semantics represent reasonable positions an agent might have in a debate. Our semantics lie between the admissible and complete semantics of abstract argumentation. Further, our approach can be used to provide a new perspective on existing approaches. For instance, we can specify the conditions under which an agent can ignore support between arguments (i.e. under which the use of abstract argumentation is warranted) and we show that deductive support semantics is a special case of our approach.

</details>


### [48] [Memora: A Harmonic Memory Representation Balancing Abstraction and Specificity](https://arxiv.org/abs/2602.03315)
*Menglin Xia,Xuchao Zhang,Shantanu Dixit,Paramaguru Harimurugan,Rujia Wang,Victor Ruhle,Robert Sim,Chetan Bansal,Saravan Rajmohan*

Main category: cs.AI

TL;DR: 提出Memora记忆表示方法，平衡抽象与具体，在基准测试达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有代理记忆系统在抽象信息时会牺牲具体性，影响有效推理，需解决此矛盾。

Method: 引入Memora，通过主要抽象组织信息，用提示锚扩展检索，采用检索策略利用记忆连接检索信息。

Result: 在LoCoMo和LongMemEval基准测试中达到新的SOTA水平，证明记忆扩展时检索相关性和推理有效性更好。

Conclusion: 标准的RAG和基于KG的记忆系统是其框架的特殊情况，Memora能有效平衡抽象和具体。

Abstract: Agent memory systems must accommodate continuously growing information while supporting efficient, context-aware retrieval for downstream tasks. Abstraction is essential for scaling agent memory, yet it often comes at the cost of specificity, obscuring the fine-grained details required for effective reasoning. We introduce Memora, a harmonic memory representation that structurally balances abstraction and specificity. Memora organizes information via its primary abstractions that index concrete memory values and consolidate related updates into unified memory entries, while cue anchors expand retrieval access across diverse aspects of the memory and connect related memories. Building on this structure, we employ a retrieval policy that actively exploits these memory connections to retrieve relevant information beyond direct semantic similarity. Theoretically, we show that standard Retrieval-Augmented Generation (RAG) and Knowledge Graph (KG)-based memory systems emerge as special cases of our framework. Empirically, Memora establishes a new state-of-the-art on the LoCoMo and LongMemEval benchmarks, demonstrating better retrieval relevance and reasoning effectiveness as memory scales.

</details>


### [49] [MentalSeek-Dx: Towards Progressive Hypothetico-Deductive Reasoning for Real-world Psychiatric Diagnosis](https://arxiv.org/abs/2602.03340)
*Xiao Sun,Yuming Yang,Junnan Zhu,Jiang Zhong,Xinyu Zhou,Kaiwen Wei*

Main category: cs.AI

TL;DR: 心理健康障碍是全球挑战，现有大语言模型在精神诊断临床应用受限，本文推出MentalDx Bench基准，评估发现范式不一致问题，提出MentalSeek - Dx模型，在基准测试中取得SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在精神评估中的临床应用受限于缺乏生态有效性的基准和细粒度诊断监督，为解决这一问题进行研究。

Method: 引入MentalDx Bench基准，评估18个大语言模型；提出MentalSeek - Dx模型，通过监督轨迹构建和基于课程的强化学习训练。

Result: 评估发现粗粒度诊断分类表现好但疾病级诊断存在系统性失败；MentalSeek - Dx仅用14B参数就在MentalDx Bench上取得SOTA性能。

Conclusion: MentalSeek - Dx为可靠的精神疾病诊断建立了临床基础框架。

Abstract: Mental health disorders represent a burgeoning global public health challenge. While Large Language Models (LLMs) have demonstrated potential in psychiatric assessment, their clinical utility is severely constrained by benchmarks that lack ecological validity and fine-grained diagnostic supervision. To bridge this gap, we introduce \textbf{MentalDx Bench}, the first benchmark dedicated to disorder-level psychiatric diagnosis within real-world clinical settings. Comprising 712 de-identified electronic health records annotated by board-certified psychiatrists under ICD-11 guidelines, the benchmark covers 76 disorders across 16 diagnostic categories. Evaluation of 18 LLMs reveals a critical \textit{paradigm misalignment}: strong performance at coarse diagnostic categorization contrasts with systematic failure at disorder-level diagnosis, underscoring a gap between pattern-based modeling and clinical hypothetico-deductive reasoning. In response, we propose \textbf{MentalSeek-Dx}, a medical-specialized LLM trained to internalize this clinical reasoning process through supervised trajectory construction and curriculum-based reinforcement learning. Experiments on MentalDx Bench demonstrate that MentalSeek-Dx achieves state-of-the-art (SOTA) performance with only 14B parameters, establishing a clinically grounded framework for reliable psychiatric diagnosis.

</details>


### [50] [Building Interpretable Models for Moral Decision-Making](https://arxiv.org/abs/2602.03351)
*Mayank Goel,Aritra Das,Paras Chopra*

Main category: cs.AI

TL;DR: 构建自定义transformer模型研究神经网络在电车困境中的道德决策，模型在Moral Machine数据上达77%准确率，并使用可解释性技术揭示道德推理分布。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络如何在电车式困境中做出道德决策。

Method: 构建自定义transformer模型处理结构化场景，使用不同可解释性技术分析。

Result: 2层架构模型在Moral Machine数据上准确率达77%，可进行详细分析，发现偏差定位于不同计算阶段。

Conclusion: 可通过构建自定义模型及运用可解释性技术研究神经网络道德决策过程及推理分布。

Abstract: We build a custom transformer model to study how neural networks make moral decisions on trolley-style dilemmas. The model processes structured scenarios using embeddings that encode who is affected, how many people, and which outcome they belong to. Our 2-layer architecture achieves 77% accuracy on Moral Machine data while remaining small enough for detailed analysis. We use different interpretability techniques to uncover how moral reasoning distributes across the network, demonstrating that biases localize to distinct computational stages among other findings.

</details>


### [51] [GFlowPO: Generative Flow Network as a Language Model Prompt Optimizer](https://arxiv.org/abs/2602.03358)
*Junmo Cho,Suhan Kim,Sangjune An,Minsu Kim,Dong Bok Lee,Heejun Lee,Sung Ju Hwang,Hae Beom Lee*

Main category: cs.AI

TL;DR: 现有基于强化学习的提示优化器样本效率低，本文提出GFlowPO框架，经两步优化，在多个任务上表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的提示优化器依赖策略内更新和固定分布采样的元提示，样本效率差，需有效解决方法。

Method: 第一步用离策略生成流网络目标微调轻量级提示语言模型，使用基于回放的训练策略；第二步引入无训练机制动态内存更新，更新元提示。

Result: 在少样本文本分类、指令归纳基准和问答任务中，GFlowPO始终优于近期离散提示优化基线。

Conclusion: GFlowPO框架能有效解决现有提示优化器样本效率低的问题，在多个任务中表现良好。

Abstract: Finding effective prompts for language models (LMs) is critical yet notoriously difficult: the prompt space is combinatorially large, rewards are sparse due to expensive target-LM evaluation. Yet, existing RL-based prompt optimizers often rely on on-policy updates and a meta-prompt sampled from a fixed distribution, leading to poor sample efficiency. We propose GFlowPO, a probabilistic prompt optimization framework that casts prompt search as a posterior inference problem over latent prompts regularized by a meta-prompted reference-LM prior. In the first step, we fine-tune a lightweight prompt-LM with an off-policy Generative Flow Network (GFlowNet) objective, using a replay-based training policy that reuses past prompt evaluations to enable sample-efficient exploration. In the second step, we introduce Dynamic Memory Update (DMU), a training-free mechanism that updates the meta-prompt by injecting both (i) diverse prompts from a replay buffer and (ii) top-performing prompts from a small priority queue, thereby progressively concentrating the search process on high-reward regions. Across few-shot text classification, instruction induction benchmarks, and question answering tasks, GFlowPO consistently outperforms recent discrete prompt optimization baselines.

</details>


### [52] [Risk Awareness Injection: Calibrating Vision-Language Models for Safety without Compromising Utility](https://arxiv.org/abs/2602.03402)
*Mengxuan Wang,Yuxin Chen,Gang Xu,Tao He,Hongjie Jiang,Ming Li*

Main category: cs.AI

TL;DR: 提出轻量级无训练框架RAI来增强视觉语言模型对多模态越狱攻击防御能力，经实验验证有效。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型易受多模态越狱攻击，而现有防御方法成本高或降低实用性，且视觉输入会稀释风险信号。

Method: 提出Risk Awareness Injection (RAI)框架，从语言嵌入构建不安全原型子空间并对高风险视觉令牌进行调制。

Result: 在多个越狱和实用基准测试中，RAI大幅降低攻击成功率且不影响任务性能。

Conclusion: RAI能有效增强视觉语言模型对多模态越狱攻击的防御能力。

Abstract: Vision language models (VLMs) extend the reasoning capabilities of large language models (LLMs) to cross-modal settings, yet remain highly vulnerable to multimodal jailbreak attacks. Existing defenses predominantly rely on safety fine-tuning or aggressive token manipulations, incurring substantial training costs or significantly degrading utility. Recent research shows that LLMs inherently recognize unsafe content in text, and the incorporation of visual inputs in VLMs frequently dilutes risk-related signals. Motivated by this, we propose Risk Awareness Injection (RAI), a lightweight and training-free framework for safety calibration that restores LLM-like risk recognition by amplifying unsafe signals in VLMs. Specifically, RAI constructs an Unsafe Prototype Subspace from language embeddings and performs targeted modulation on selected high-risk visual tokens, explicitly activating safety-critical signals within the cross-modal feature space. This modulation restores the model's LLM-like ability to detect unsafe content from visual inputs, while preserving the semantic integrity of original tokens for cross-modal reasoning. Extensive experiments across multiple jailbreak and utility benchmarks demonstrate that RAI substantially reduces attack success rate without compromising task performance.

</details>


### [53] [Feasible strategies for conflict resolution within intuitionistic fuzzy preference-based conflict situations](https://arxiv.org/abs/2602.03403)
*Guangming Lang,Mingchuan Shang,Mengjun Hu,Jie Zhou,Feng Xu*

Main category: cs.AI

TL;DR: 引入直觉模糊偏好冲突情况概念，构建相关冲突分析模型、计算阈值，提出可行策略并举例验证模型有效性。


<details>
  <summary>Details</summary>
Motivation: 现有基于偏好的冲突模型仅用三种定性关系描述主体态度，限制其捕捉冲突本质的能力，需改进。

Method: 引入直觉模糊偏好冲突情况概念，构建冲突分析模型，用相对损失函数计算阈值，提出基于调整机制的可行策略及构建算法。

Result: 提出了新的模型、计算方法和可行策略。

Conclusion: 所提出的模型有效且可行。

Abstract: In three-way conflict analysis, preference-based conflict situations characterize agents' attitudes towards issues by formally modeling their preferences over pairs of issues. However, existing preference-based conflict models rely exclusively on three qualitative relations, namely, preference, converse, and indifference, to describe agents' attitudes towards issue pairs, which significantly limits their capacity in capturing the essence of conflict. To overcome this limitation, we introduce the concept of an intuitionistic fuzzy preference-based conflict situation that captures agents' attitudes towards issue pairs with finer granularity than that afforded by classical preference-based models. Afterwards, we develop intuitionistic fuzzy preference-based conflict measures within this framework, and construct three-way conflict analysis models for trisecting the set of agent pairs, the agent set, and the issue set. Additionally, relative loss functions built on the proposed conflict functions are employed to calculate thresholds for three-way conflict analysis. Finally, we present adjustment mechanism-based feasible strategies that simultaneously account for both adjustment magnitudes and conflict degrees, together with an algorithm for constructing such feasible strategies, and provide an illustrative example to demonstrate the validity and effectiveness of the proposed model.

</details>


### [54] [DiscoverLLM: From Executing Intents to Discovering Them](https://arxiv.org/abs/2602.03429)
*Tae Soo Kim,Yoonjoo Lee,Jaesang Yu,John Joon Young Chung,Juho Kim*

Main category: cs.AI

TL;DR: 提出DiscoverLLM框架训练大语言模型帮助用户形成和发现意图，在多个任务上表现良好，提升用户对话满意度和效率。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型处理用户模糊请求存在局限，用户常是因未形成意图而表达模糊，简单询问无法解决问题。

Method: 引入DiscoverLLM框架，使用新颖用户模拟器，以意图具体化程度作为奖励信号训练模型，让模型在意图不明时探索选项，意图明确时细化实施。

Result: 在多个交互基准测试中，DiscoverLLM任务性能提升超10%，对话长度最多减少40%；用户研究显示其提升了对话满意度和效率。

Conclusion: DiscoverLLM框架能有效帮助用户形成和发现意图，提升交互性能和用户体验。

Abstract: To handle ambiguous and open-ended requests, Large Language Models (LLMs) are increasingly trained to interact with users to surface intents they have not yet expressed (e.g., ask clarification questions). However, users are often ambiguous because they have not yet formed their intents: they must observe and explore outcomes to discover what they want. Simply asking "what kind of tone do you want?" fails when users themselves do not know. We introduce DiscoverLLM, a novel and generalizable framework that trains LLMs to help users form and discover their intents. Central to our approach is a novel user simulator that models cognitive state with a hierarchy of intents that progressively concretize as the model surfaces relevant options -- where the degree of concretization serves as a reward signal that models can be trained to optimize. Resulting models learn to collaborate with users by adaptively diverging (i.e., explore options) when intents are unclear, and converging (i.e., refine and implement) when intents concretize. Across proposed interactive benchmarks in creative writing, technical writing, and SVG drawing, DiscoverLLM achieves over 10% higher task performance while reducing conversation length by up to 40%. In a user study with 75 human participants, DiscoverLLM improved conversation satisfaction and efficiency compared to baselines.

</details>


### [55] [CRL-VLA: Continual Vision-Language-Action Learning](https://arxiv.org/abs/2602.03445)
*Qixin Zeng,Shuo Zhang,Hongyin Zhang,Renjie Wang,Han Zhao,Libang Zhao,Runze Li,Donglin Wang,Chao Huang*

Main category: cs.AI

TL;DR: 提出CRL - VLA框架用于VLA模型持续训练，通过不对称调节解决稳定性 - 可塑性权衡问题，实验表明其性能优于基线。


<details>
  <summary>Details</summary>
Motivation: 在终身机器人场景中部署VLA模型时，现有方法难以平衡稳定性和可塑性。

Method: 引入CRL - VLA框架，推导统一性能边界，采用不对称调节，通过双批评家架构和GCVF实现。

Result: 在LIBERO基准测试中，CRL - VLA有效协调冲突目标，在抗遗忘和前向适应方面优于基线。

Conclusion: CRL - VLA框架能有效解决VLA模型在终身学习中稳定性和可塑性的权衡问题。

Abstract: Lifelong learning is critical for embodied agents in open-world environments, where reinforcement learning fine-tuning has emerged as an important paradigm to enable Vision-Language-Action (VLA) models to master dexterous manipulation through environmental interaction. Thus, Continual Reinforcement Learning (CRL) is a promising pathway for deploying VLA models in lifelong robotic scenarios, yet balancing stability (retaining old skills) and plasticity (learning new ones) remains a formidable challenge for existing methods. We introduce CRL-VLA, a framework for continual post-training of VLA models with rigorous theoretical bounds. We derive a unified performance bound linking the stability-plasticity trade-off to goal-conditioned advantage magnitude, scaled by policy divergence. CRL-VLA resolves this dilemma via asymmetric regulation: constraining advantage magnitudes on prior tasks while enabling controlled growth on new tasks. This is realized through a simple but effective dual-critic architecture with novel Goal-Conditioned Value Formulation (GCVF), where a frozen critic anchors semantic consistency and a trainable estimator drives adaptation. Experiments on the LIBERO benchmark demonstrate that CRL-VLA effectively harmonizes these conflicting objectives, outperforming baselines in both anti-forgetting and forward adaptation.

</details>


### [56] [The Dual Role of Abstracting over the Irrelevant in Symbolic Explanations: Cognitive Effort vs. Understanding](https://arxiv.org/abs/2602.03467)
*Zeynep G. Saribatur,Johannes Langer,Ute Schmid*

Main category: cs.AI

TL;DR: 研究形式抽象对人类推理表现和认知努力的影响，实验表明抽象可增强以人为本的符号解释。


<details>
  <summary>Details</summary>
Motivation: AI系统输出难理解，符号AI原始逻辑痕迹带来高认知负荷，需研究形式抽象的影响。

Method: 以Answer Set Programming (ASP)为框架定义无关细节进行抽象，开展认知实验让参与者对不同领域刺激分类。

Result: 聚类细节显著提升参与者理解，去除细节显著降低认知努力。

Conclusion: 抽象能增强以人为本的符号解释。

Abstract: Explanations are central to human cognition, yet AI systems often produce outputs that are difficult to understand. While symbolic AI offers a transparent foundation for interpretability, raw logical traces often impose a high extraneous cognitive load. We investigate how formal abstractions, specifically removal and clustering, impact human reasoning performance and cognitive effort. Utilizing Answer Set Programming (ASP) as a formal framework, we define a notion of irrelevant details to be abstracted over to obtain simplified explanations. Our cognitive experiments, in which participants classified stimuli across domains with explanations derived from an answer set program, show that clustering details significantly improve participants' understanding, while removal of details significantly reduce cognitive effort, supporting the hypothesis that abstraction enhances human-centered symbolic explanations.

</details>


### [57] [IntentRL: Training Proactive User-intent Agents for Open-ended Deep Research via Reinforcement Learning](https://arxiv.org/abs/2602.03468)
*Haohao Luo,Zexi Li,Yuexiang Xie,Wenhao Zhang,Yaliang Li,Ying Shen*

Main category: cs.AI

TL;DR: 提出IntentRL框架训练主动代理在长时研究前澄清用户潜在意图，通过可扩展管道处理数据并采用两阶段强化学习策略，实验显示有显著效果。


<details>
  <summary>Details</summary>
Motivation: Deep Research（DR）智能体存在自主性与交互性的困境，高自主性处理模糊查询会导致执行时间长且结果不理想。

Method: 提出IntentRL框架，引入可扩展管道拓展数据，采用两阶段强化学习策略，第一阶段在离线对话上使用RL学习用户交互行为，第二阶段用训练好的代理和用户模拟器进行在线滚动。

Result: IntentRL显著提高了意图命中率和下游任务性能，优于闭源DR智能体的内置澄清模块和主动式大语言模型基线。

Conclusion: IntentRL能有效解决DR智能体的自主性 - 交互性困境，有良好效果。

Abstract: Deep Research (DR) agents extend Large Language Models (LLMs) beyond parametric knowledge by autonomously retrieving and synthesizing evidence from large web corpora into long-form reports, enabling a long-horizon agentic paradigm. However, unlike real-time conversational assistants, DR is computationally expensive and time-consuming, creating an autonomy-interaction dilemma: high autonomy on ambiguous user queries often leads to prolonged execution with unsatisfactory outcomes. To address this, we propose IntentRL, a framework that trains proactive agents to clarify latent user intents before starting long-horizon research. To overcome the scarcity of open-ended research data, we introduce a scalable pipeline that expands a few seed samples into high-quality dialogue turns via a shallow-to-deep intent refinement graph. We further adopt a two-stage reinforcement learning (RL) strategy: Stage I applies RL on offline dialogues to efficiently learn general user-interaction behavior, while Stage II uses the trained agent and a user simulator for online rollouts to strengthen adaptation to diverse user feedback. Extensive experiments show that IntentRL significantly improves both intent hit rate and downstream task performance, outperforming the built-in clarify modules of closed-source DR agents and proactive LLM baselines.

</details>


### [58] [When Routing Collapses: On the Degenerate Convergence of LLM Routers](https://arxiv.org/abs/2602.03478)
*Guannan Lai,Han-Jia Ye*

Main category: cs.AI

TL;DR: 现有大语言模型路由器存在路由崩溃问题，即未充分利用小模型，提出决策感知的EquiRouter缓解该问题，在RouterBench上降低成本。


<details>
  <summary>Details</summary>
Motivation: 发现现有大语言模型路由器存在路由崩溃，未充分利用小模型，浪费计算和金钱成本，违背路由初衷。

Method: 提出决策感知的EquiRouter，直接学习模型排名。

Result: 在RouterBench上，与最强的先前路由器相比，EquiRouter在达到GPT - 4性能水平时成本降低约17%。

Conclusion: EquiRouter能恢复小模型的作用，缓解路由崩溃问题，代码已开源。

Abstract: LLM routing aims to achieve a favorable quality--cost trade-off by dynamically assigning easy queries to smaller models and harder queries to stronger ones. However, across both unimodal and multimodal settings, we uncover a pervasive yet underexplored failure mode in existing routers: as the user's cost budget increases, routers systematically default to the most capable and most expensive model even when cheaper models already suffice. As a result, current routers under-utilize small models, wasting computation and monetary cost and undermining the core promise of routing; we term this phenomenon routing collapse. We attribute routing collapse to an objective--decision mismatch: many routers are trained to predict scalar performance scores, whereas routing decisions ultimately depend on discrete comparisons among candidate models. Consequently, small prediction errors can flip relative orderings and trigger suboptimal selections. To bridge this gap, we propose EquiRouter, a decision-aware router that directly learns model rankings, restoring the role of smaller models and mitigating routing collapse. On RouterBench, EquiRouter reduces cost by about 17\% at GPT-4-level performance compared to the strongest prior router. Our code is available at https://github.com/AIGNLAI/EquiRouter.

</details>


### [59] [Group Selection as a Safeguard Against AI Substitution](https://arxiv.org/abs/2602.03541)
*Qiankun Zhong,Thomas F. Eisenmann,Julian Garcia,Iyad Rahwan*

Main category: cs.AI

TL;DR: 本文研究AI使用对人类文化进化的长期影响，发现AI替代型用户在个体选择中占优，而AI互补型用户在文化群体选择中更有利。


<details>
  <summary>Details</summary>
Motivation: 探讨AI使用对人类文化进化的长期后果以及导致“文化崩溃”的条件。

Method: 使用基于主体的模型和进化博弈论，比较AI互补和替代两种使用方式，并研究其在进化动态中的竞争和传播。

Result: AI替代型用户在个体层面选择中占优，尽管会大幅降低文化差异；AI互补型用户能在群体边界强时通过维持探索所需的差异使群体受益。

Conclusion: 研究揭示了AI采用的长期、群体层面的影响，为减轻风险的政策和组织策略提供了依据。

Abstract: Reliance on generative AI can reduce cultural variance and diversity, especially in creative work. This reduction in variance has already led to problems in model performance, including model collapse and hallucination. In this paper, we examine the long-term consequences of AI use for human cultural evolution and the conditions under which widespread AI use may lead to "cultural collapse", a process in which reliance on AI-generated content reduces human variation and innovation and slows cumulative cultural evolution. Using an agent-based model and evolutionary game theory, we compare two types of AI use: complement and substitute. AI-complement users seek suggestions and guidance while remaining the main producers of the final output, whereas AI-substitute users provide minimal input, and rely on AI to produce most of the output. We then study how these use strategies compete and spread under evolutionary dynamics. We find that AI-substitute users prevail under individual-level selection despite the stronger reduction in cultural variance. By contrast, AI-complement users can benefit their groups by maintaining the variance needed for exploration, and can therefore be favored under cultural group selection when group boundaries are strong. Overall, our findings shed light on the long-term, population-level effects of AI adoption and inform policy and organizational strategies to mitigate these risks.

</details>


### [60] [Persona Generators: Generating Diverse Synthetic Personas at Scale](https://arxiv.org/abs/2602.03545)
*Davide Paglieri,Logan Cross,William A. Cunningham,Joel Z. Leibo,Alexander Sasha Vezhnevets*

Main category: cs.AI

TL;DR: 提出Persona Generators生成多样化合成人群，用基于AlphaEvolve的迭代改进循环优化，表现优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 评估与人类交互的AI系统需要理解其在不同用户群体中的行为，收集代表性人类数据困难，现有方法有局限。

Method: 引入Persona Generators，应用基于AlphaEvolve的迭代改进循环，用大语言模型作为变异算子优化代码。

Result: 优化产生的轻量级Persona Generators可将小描述扩展为多样化合成人群，在六个多样性指标上优于现有基线。

Conclusion: 进化后的生成器能产生跨越罕见特征组合的人群，在标准大语言模型输出中难以实现。

Abstract: Evaluating AI systems that interact with humans requires understanding their behavior across diverse user populations, but collecting representative human data is often expensive or infeasible, particularly for novel technologies or hypothetical future scenarios. Recent work in Generative Agent-Based Modeling has shown that large language models can simulate human-like synthetic personas with high fidelity, accurately reproducing the beliefs and behaviors of specific individuals. However, most approaches require detailed data about target populations and often prioritize density matching (replicating what is most probable) rather than support coverage (spanning what is possible), leaving long-tail behaviors underexplored. We introduce Persona Generators, functions that can produce diverse synthetic populations tailored to arbitrary contexts. We apply an iterative improvement loop based on AlphaEvolve, using large language models as mutation operators to refine our Persona Generator code over hundreds of iterations. The optimization process produces lightweight Persona Generators that can automatically expand small descriptions into populations of diverse synthetic personas that maximize coverage of opinions and preferences along relevant diversity axes. We demonstrate that evolved generators substantially outperform existing baselines across six diversity metrics on held-out contexts, producing populations that span rare trait combinations difficult to achieve in standard LLM outputs.

</details>


### [61] [EHRWorld: A Patient-Centric Medical World Model for Long-Horizon Clinical Trajectories](https://arxiv.org/abs/2602.03569)
*Linjie Mu,Zhongzhen Huang,Yannian Gu,Shengqian Qin,Shaoting Zhang,Xiaofan Zhang*

Main category: cs.AI

TL;DR: 本文指出仅融入医学知识的大语言模型用于临床模拟会有问题，引入EHRWorld模型和EHRWorld - 110K数据集，评估显示EHRWorld表现更佳。


<details>
  <summary>Details</summary>
Motivation: 当前在复杂医学领域实现世界模型有挑战，大语言模型虽在静态医学推理任务表现好，但不确定能否用于动态医学模拟，且仅含医学知识的大语言模型在临床模拟中有局限。

Method: 引入以患者为中心、在因果顺序范式下训练的EHRWorld模型，以及来自真实电子健康记录的大规模纵向临床数据集EHRWorld - 110K。

Result: EHRWorld显著优于基于大语言模型的基线方法，实现更稳定的长期模拟、更好的临床敏感事件建模和更优的推理效率。

Conclusion: 可靠和强大的医学世界建模需要在因果基础上随时间演变的临床数据上进行训练。

Abstract: World models offer a principled framework for simulating future states under interventions, but realizing such models in complex, high-stakes domains like medicine remains challenging. Recent large language models (LLMs) have achieved strong performance on static medical reasoning tasks, raising the question of whether they can function as dynamic medical world models capable of simulating disease progression and treatment outcomes over time. In this work, we show that LLMs only incorporating medical knowledge struggle to maintain consistent patient states under sequential interventions, leading to error accumulation in long-horizon clinical simulation. To address this limitation, we introduce EHRWorld, a patient-centric medical world model trained under a causal sequential paradigm, together with EHRWorld-110K, a large-scale longitudinal clinical dataset derived from real-world electronic health records. Extensive evaluations demonstrate that EHRWorld significantly outperforms naive LLM-based baselines, achieving more stable long-horizon simulation, improved modeling of clinically sensitive events, and favorable reasoning efficiency, highlighting the necessity of training on causally grounded, temporally evolving clinical data for reliable and robust medical world modeling.

</details>


### [62] [Can LLMs Do Rocket Science? Exploring the Limits of Complex Reasoning with GTOC 12](https://arxiv.org/abs/2602.03630)
*Iñaki del Campo,Pablo Cuervo,Victor Rodriguez-Fernandez,Roberto Armellin,Jack Yarndley*

Main category: cs.AI

TL;DR: 研究评估当前AI代理在复杂航天动力学挑战中的表现，发现平均战略可行性得分提升，但存在策略与执行的能力差距，LLMs受实施障碍限制。


<details>
  <summary>Details</summary>
Motivation: 探究当前AI代理在高维、物理受限环境中自主多阶段规划的能力极限。

Method: 将MLE - Bench框架应用于轨道力学领域，部署基于AIDE的代理架构生成和优化任务解决方案，采用“LLM - as - a - Judge”方法评估性能。

Result: 过去两年平均战略可行性得分近翻倍，但策略和执行存在关键能力差距，先进模型实施常失败。

Conclusion: 当前LLMs有能力处理空间科学任务，但受实施障碍限制，只能作为领域促进者而非完全自主的工程师。

Abstract: Large Language Models (LLMs) have demonstrated remarkable proficiency in code generation and general reasoning, yet their capacity for autonomous multi-stage planning in high-dimensional, physically constrained environments remains an open research question. This study investigates the limits of current AI agents by evaluating them against the 12th Global Trajectory Optimization Competition (GTOC 12), a complex astrodynamics challenge requiring the design of a large-scale asteroid mining campaign. We adapt the MLE-Bench framework to the domain of orbital mechanics and deploy an AIDE-based agent architecture to autonomously generate and refine mission solutions. To assess performance beyond binary validity, we employ an "LLM-as-a-Judge" methodology, utilizing a rubric developed by domain experts to evaluate strategic viability across five structural categories. A comparative analysis of models, ranging from GPT-4-Turbo to reasoning-enhanced architectures like Gemini 2.5 Pro, and o3, reveals a significant trend: the average strategic viability score has nearly doubled in the last two years (rising from 9.3 to 17.2 out of 26). However, we identify a critical capability gap between strategy and execution. While advanced models demonstrate sophisticated conceptual understanding, correctly framing objective functions and mission architectures, they consistently fail at implementation due to physical unit inconsistencies, boundary condition errors, and inefficient debugging loops. We conclude that, while current LLMs often demonstrate sufficient knowledge and intelligence to tackle space science tasks, they remain limited by an implementation barrier, functioning as powerful domain facilitators rather than fully autonomous engineers.

</details>


### [63] [Search-R2: Enhancing Search-Integrated Reasoning via Actor-Refiner Collaboration](https://arxiv.org/abs/2602.03647)
*Bowei He,Minda Hu,Zenan Xu,Hongru Wang,Licheng Zong,Yankai Chen,Chen Ma,Xue Liu,Pluto Zhou,Irwin King*

Main category: cs.AI

TL;DR: 提出 Search - R2 框架解决搜索集成推理代理训练中的多尺度信用分配问题，实验表明其性能优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有通过强化学习训练搜索集成推理语言代理的方法存在多尺度信用分配问题，无法区分高质量推理和偶然猜测，导致冗余或误导性搜索行为。

Method: 提出 Search - R2 框架，将生成过程分解为 Actor 和 Meta - Refiner，引入混合奖励设计；理论上形式化 Actor - Refiner 交互为平滑混合策略。

Result: 在各种通用和多跳 QA 数据集上进行广泛实验，Search - R2 在不同模型规模下始终优于基于 RAG 和 RL 的强基线。

Conclusion: Search - R2 能以最小开销实现卓越推理准确性。

Abstract: Search-integrated reasoning enables language agents to transcend static parametric knowledge by actively querying external sources. However, training these agents via reinforcement learning is hindered by the multi-scale credit assignment problem: existing methods typically rely on sparse, trajectory-level rewards that fail to distinguish between high-quality reasoning and fortuitous guesses, leading to redundant or misleading search behaviors. To address this, we propose Search-R2, a novel Actor-Refiner collaboration framework that enhances reasoning through targeted intervention, with both components jointly optimized during training. Our approach decomposes the generation process into an Actor, which produces initial reasoning trajectories, and a Meta-Refiner, which selectively diagnoses and repairs flawed steps via a 'cut-and-regenerate' mechanism. To provide fine-grained supervision, we introduce a hybrid reward design that couples outcome correctness with a dense process reward quantifying the information density of retrieved evidence. Theoretically, we formalize the Actor-Refiner interaction as a smoothed mixture policy, proving that selective correction yields strict performance gains over strong baselines. Extensive experiments across various general and multi-hop QA datasets demonstrate that Search-R2 consistently outperforms strong RAG and RL-based baselines across model scales, achieving superior reasoning accuracy with minimal overhead.

</details>


### [64] [Mitigating Conversational Inertia in Multi-Turn Agents](https://arxiv.org/abs/2602.03664)
*Yang Wan,Zheng Cao,Zhenhao Zhang,Zhengwen Zeng,Shuheng Shen,Changhua Meng,Linchao Zhu*

Main category: cs.AI

TL;DR: 大语言模型在多轮对话中存在模仿自身先前回复问题，本文提出上下文偏好学习和上下文管理策略，减少对话惯性并提升性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在多轮代理场景中会错误模仿自身先前回复，存在对话惯性和模仿偏差，限制探索能力，需解决探索与利用的矛盾。

Method: 通过注意力分析识别对话惯性现象，基于相同状态下不同上下文长度产生的惯性差异构建偏好对，提出上下文偏好学习，并在推理时提供上下文管理策略。

Result: 在八个代理环境和一个深度研究场景的实验中，框架减少了对话惯性并实现了性能提升。

Conclusion: 所提出的方法能有效减少对话惯性，平衡探索与利用，提高模型性能。

Abstract: Large language models excel as few-shot learners when provided with appropriate demonstrations, yet this strength becomes problematic in multiturn agent scenarios, where LLMs erroneously mimic their own previous responses as few-shot examples. Through attention analysis, we identify conversational inertia, a phenomenon where models exhibit strong diagonal attention to previous responses, which is associated with imitation bias that constrains exploration. This reveals a tension when transforming few-shot LLMs into agents: longer context enriches environmental feedback for exploitation, yet also amplifies conversational inertia that undermines exploration. Our key insight is that for identical states, actions generated with longer contexts exhibit stronger inertia than those with shorter contexts, enabling construction of preference pairs without environment rewards. Based on this, we propose Context Preference Learning to calibrate model preferences to favor low-inertia responses over highinertia ones. We further provide context management strategies at inference time to balance exploration and exploitation. Experimental results across eight agentic environments and one deep research scenario validate that our framework reduces conversational inertia and achieves performance improvements.

</details>


### [65] [TodyComm: Task-Oriented Dynamic Communication for Multi-Round LLM-based Multi-Agent System](https://arxiv.org/abs/2602.03688)
*Wenzhe Fan,Tommaso Tognoli,Henry Peng Zou,Chunyu Miao,Yibo Wang,Xinhua Zhang*

Main category: cs.AI

TL;DR: 提出TodyComm算法解决多轮大语言模型多智能体系统通信拓扑固定问题，实验显示其在多方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有多轮大语言模型多智能体系统推理时通信拓扑固定，无法适应现实中智能体角色随轮次变化的情况。

Method: 提出任务导向的动态通信算法TodyComm，通过策略梯度优化任务效用，生成适应每轮动态的协作拓扑。

Result: 在五个基准测试中，TodyComm在动态对手和通信预算条件下，展现出更好的任务有效性，同时保持了令牌效率和可扩展性。

Conclusion: TodyComm能有效解决多轮大语言模型多智能体系统通信拓扑固定问题，具有良好性能。

Abstract: Multi-round LLM-based multi-agent systems rely on effective communication structures to support collaboration across rounds. However, most existing methods employ a fixed communication topology during inference, which falls short in many realistic applications where the agents' roles may change \textit{across rounds} due to dynamic adversary, task progression, or time-varying constraints such as communication bandwidth. In this paper, we propose addressing this issue through TodyComm, a \textbf{t}ask-\textbf{o}riented \textbf{dy}namic \textbf{comm}unication algorithm. It produces behavior-driven collaboration topologies that adapt to the dynamics at each round, optimizing the utility for the task through policy gradient. Experiments on five benchmarks demonstrate that under both dynamic adversary and communications budgets, TodyComm delivers superior task effectiveness while retaining token efficiency and scalability.

</details>


### [66] [AOrchestra: Automating Sub-Agent Creation for Agentic Orchestration](https://arxiv.org/abs/2602.03786)
*Jianhao Ruan,Zhihao Xu,Yiran Peng,Fashen Ren,Zhaoyang Yu,Xinbing Liang,Jinyu Xiang,Bang Liu,Chenglin Wu,Yuyu Luo,Jiayi Zhang*

Main category: cs.AI

TL;DR: 文章提出统一代理抽象，构建AOrchestra系统解决长任务自动化中现有设计适应性问题，在多基准测试有提升。


<details>
  <summary>Details</summary>
Motivation: 现有子代理作为工具的范式在长复杂任务中缺乏动态抽象视图，导致适应性不足。

Method: 提出统一、框架无关的代理抽象，将代理建模为元组，构建AOrchestra系统按需生成执行器，中央协调器在每一步具体化元组。

Result: 在GAIA、SWE - Bench、Terminal - Bench三个基准测试中，与Gemini - 3 - Flash搭配时比最强基线相对提升16.28%。

Conclusion: 设计可减少人工工程努力，框架无关，支持即插即用，能实现可控的性能成本权衡，接近帕累托最优。

Abstract: Language agents have shown strong promise for task automation. Realizing this promise for increasingly complex, long-horizon tasks has driven the rise of a sub-agent-as-tools paradigm for multi-turn task solving. However, existing designs still lack a dynamic abstraction view of sub-agents, thereby hurting adaptability. We address this challenge with a unified, framework-agnostic agent abstraction that models any agent as a tuple Instruction, Context, Tools, Model. This tuple acts as a compositional recipe for capabilities, enabling the system to spawn specialized executors for each task on demand. Building on this abstraction, we introduce an agentic system AOrchestra, where the central orchestrator concretizes the tuple at each step: it curates task-relevant context, selects tools and models, and delegates execution via on-the-fly automatic agent creation. Such designs enable reducing human engineering efforts, and remain framework-agnostic with plug-and-play support for diverse agents as task executors. It also enables a controllable performance-cost trade-off, allowing the system to approach Pareto-efficient. Across three challenging benchmarks (GAIA, SWE-Bench, Terminal-Bench), AOrchestra achieves 16.28% relative improvement against the strongest baseline when paired with Gemini-3-Flash. The code is available at: https://github.com/FoundationAgents/AOrchestra

</details>


### [67] [Understanding Agent Scaling in LLM-Based Multi-Agent Systems via Diversity](https://arxiv.org/abs/2602.03794)
*Yingxuan Yang,Chengrui Qu,Muning Wen,Laixi Shi,Ying Wen,Weinan Zhang,Adam Wierman,Shangding Gu*

Main category: cs.AI

TL;DR: 研究基于大语言模型的多智能体系统（MAS），发现同质设置下增加智能体数量收益递减，异质性可带来显著提升，提出信息论框架解释原因并给出有效通道数指标，实验表明异质配置表现更好。


<details>
  <summary>Details</summary>
Motivation: 探究基于大语言模型的多智能体系统中增加智能体数量收益递减的原因，以及异质性为何能带来提升。

Method: 提出信息论框架，推导与架构无关的边界，引入有效通道数指标$K^*$。

Result: 实验显示异质配置始终优于同质扩展，2个不同智能体可达到或超过16个同质智能体的性能。

Conclusion: 研究结果为通过考虑多样性设计构建高效、稳健的多智能体系统提供了原则性指导。

Abstract: LLM-based multi-agent systems (MAS) have emerged as a promising approach to tackle complex tasks that are difficult for individual LLMs. A natural strategy is to scale performance by increasing the number of agents; however, we find that such scaling exhibits strong diminishing returns in homogeneous settings, while introducing heterogeneity (e.g., different models, prompts, or tools) continues to yield substantial gains. This raises a fundamental question: what limits scaling, and why does diversity help? We present an information-theoretic framework showing that MAS performance is bounded by the intrinsic task uncertainty, not by agent count. We derive architecture-agnostic bounds demonstrating that improvements depend on how many effective channels the system accesses. Homogeneous agents saturate early because their outputs are strongly correlated, whereas heterogeneous agents contribute complementary evidence. We further introduce $K^*$, an effective channel count that quantifies the number of effective channels without ground-truth labels. Empirically, we show that heterogeneous configurations consistently outperform homogeneous scaling: 2 diverse agents can match or exceed the performance of 16 homogeneous agents. Our results provide principled guidelines for building efficient and robust MAS through diversity-aware design. Code and Dataset are available at the link: https://github.com/SafeRL-Lab/Agent-Scaling.

</details>


### [68] [Conformal Thinking: Risk Control for Reasoning on a Compute Budget](https://arxiv.org/abs/2602.03814)
*Xi Wang,Anushri Suresh,Alvin Zhang,Rishi More,William Jurayj,Benjamin Van Durme,Mehrdad Farajtabar,Daniel Khashabi,Eric Nalisnick*

Main category: cs.AI

TL;DR: 本文将大语言模型推理的预算设置问题重新定义为风险控制，引入上下阈值机制，用无分布风险控制优化停止机制，实验证明该方法有效。


<details>
  <summary>Details</summary>
Motivation: 大语言模型推理中设置token预算和自适应推理阈值存在挑战，需平衡风险与准确性。

Method: 将预算设置问题重新定义为风险控制，引入上下阈值机制，用无分布风险控制优化停止机制，多标准场景考虑效率损失选择退出机制。

Result: 在不同推理任务和模型上的实验显示，该方法能在满足用户指定风险目标的同时，通过下阈值和集成停止机制提高计算效率。

Conclusion: 提出的风险控制方法在大语言模型推理中有效，可提高计算效率并控制风险。

Abstract: Reasoning Large Language Models (LLMs) enable test-time scaling, with dataset-level accuracy improving as the token budget increases, motivating adaptive reasoning -- spending tokens when they improve reliability and stopping early when additional computation is unlikely to help. However, setting the token budget, as well as the threshold for adaptive reasoning, is a practical challenge that entails a fundamental risk-accuracy trade-off. We re-frame the budget setting problem as risk control, limiting the error rate while minimizing compute. Our framework introduces an upper threshold that stops reasoning when the model is confident (risking incorrect output) and a novel parametric lower threshold that preemptively stops unsolvable instances (risking premature stoppage). Given a target risk and a validation set, we use distribution-free risk control to optimally specify these stopping mechanisms. For scenarios with multiple budget controlling criteria, we incorporate an efficiency loss to select the most computationally efficient exiting mechanism. Empirical results across diverse reasoning tasks and models demonstrate the effectiveness of our risk control approach, demonstrating computational efficiency gains from the lower threshold and ensemble stopping mechanisms while adhering to the user-specified risk target.

</details>


### [69] [AutoFigure: Generating and Refining Publication-Ready Scientific Illustrations](https://arxiv.org/abs/2602.03828)
*Minjun Zhu,Zhen Lin,Yixuan Weng,Panzhong Lu,Qiujie Xie,Yifan Wei,Sifan Liu,Qiyao Sun,Yue Zhang*

Main category: cs.AI

TL;DR: 提出首个大规模科学插图生成基准FigureBench和自动生成框架AutoFigure，实验表明AutoFigure性能优于基线方法，相关代码和数据集已开源。


<details>
  <summary>Details</summary>
Motivation: 高质量科学插图手动创建是学界和业界瓶颈，需自动化生成方法。

Method: 构建含3300个文本 - 插图对的FigureBench基准，提出能进行思考、重组和验证的AutoFigure框架。

Result: AutoFigure在实验中始终超越基线方法，能生成可用于发表的科学插图。

Conclusion: AutoFigure是有效的科学插图自动生成框架，相关资源已开源方便后续研究。

Abstract: High-quality scientific illustrations are crucial for effectively communicating complex scientific and technical concepts, yet their manual creation remains a well-recognized bottleneck in both academia and industry. We present FigureBench, the first large-scale benchmark for generating scientific illustrations from long-form scientific texts. It contains 3,300 high-quality scientific text-figure pairs, covering diverse text-to-illustration tasks from scientific papers, surveys, blogs, and textbooks. Moreover, we propose AutoFigure, the first agentic framework that automatically generates high-quality scientific illustrations based on long-form scientific text. Specifically, before rendering the final result, AutoFigure engages in extensive thinking, recombination, and validation to produce a layout that is both structurally sound and aesthetically refined, outputting a scientific illustration that achieves both structural completeness and aesthetic appeal. Leveraging the high-quality data from FigureBench, we conduct extensive experiments to test the performance of AutoFigure against various baseline methods. The results demonstrate that AutoFigure consistently surpasses all baseline methods, producing publication-ready scientific illustrations. The code, dataset and huggingface space are released in https://github.com/ResearAI/AutoFigure.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [70] [Efficient Counterfactual Estimation of Conditional Greeks via Malliavin-based Weak Derivatives](https://arxiv.org/abs/2602.02811)
*Vikram Krishnamurthy,Luke Snow*

Main category: cs.CE

TL;DR: 研究扩散过程条件损失泛函的反事实梯度估计，提出无核两阶段方法用于稀有事件制度下的算法和计算。


<details>
  <summary>Details</summary>
Motivation: 定量金融中条件希腊字母梯度估计，传统方法在条件事件概率为零或趋近零时效率低、收敛慢。

Method: 提出两阶段无核方法，一是用Malliavin微积分将条件损失泛函表示为Skorohod积分；二是对条件损失泛函的弱导数估计，方差恒定。

Result: 得到经典蒙特卡罗估计器的方差和收敛率，弱导数估计方差恒定。

Conclusion: 为稀有事件制度下的反事实条件随机梯度算法和金融希腊字母计算提供有效框架。

Abstract: We study counterfactual gradient estimation of conditional loss functionals of diffusion processes. In quantitative finance, these gradients are known as conditional Greeks: the sensitivity of expected market values, conditioned on some event of interest. The difficulty is that when the conditioning event has vanishing or zero probability, naive Monte Carlo estimators are prohibitively inefficient; kernel smoothing, though common, suffers from slow convergence. We propose a two-stage kernel-free methodology. First, we show using Malliavin calculus that the conditional loss functional of a diffusion process admits an exact representation as a Skorohod integral, yielding classical Monte-Carlo estimator variance and convergence rates. Second, we establish that a weak derivative estimate of the conditional loss functional with respect to model parameters can be evaluated algorithmically with constant variance, in contrast to the widely used score function method whose variance grows linearly in the sample path length. Together, these results yield an efficient framework for counterfactual conditional stochastic gradient algorithms and financial Greek computations in rare-event regimes.

</details>


### [71] [Generative Artificial Intelligence creates delicious, sustainable, and nutritious burgers](https://arxiv.org/abs/2602.03092)
*Vahidullah Tac,Christopher Gardner,Ellen Kuhl*

Main category: cs.CE

TL;DR: 本文表明生成式人工智能可从食谱数据中学习人类味觉结构，以汉堡为模型系统，生成不同优化目标的新汉堡，展示其在食品设计中的应用。


<details>
  <summary>Details</summary>
Motivation: 设计美味、营养且可持续的食物具有挑战，需要新方法。

Method: 利用生成式人工智能从大规模人类生成的食谱数据中学习人类味觉结构，以汉堡为模型系统生成新食物。

Result: 生成式人工智能能无监督重现经典巨无霸，生成的新汉堡在美味度、可持续性和营养方面有不同优势，如美味汉堡在感官评价中表现好、蘑菇汉堡环境影响低、豆类汉堡营养得分高。

Conclusion: 生成式人工智能可作为学习人类味觉和解决食品设计复杂权衡问题的定量框架。

Abstract: Food choices shape both human and planetary health; yet, designing foods that are delicious, nutritious, and sustainable remains challenging. Here we show that generative artificial intelligence can learn the structure of the human palate directly from large-scale, human-generated recipe data to create novel foods within a structured design space. Using burgers as a model system, the generative AI rediscovers the classic Big Mac without explicit supervision and generates novel burgers optimized for deliciousness, sustainability, or nutrition. Compared to the Big Mac, its delicious burgers score the same or better in overall liking, flavor, and texture in a blinded sensory evaluation conducted in a restaurant setting with 101 participants; its mushroom burger achieves an environmental impact score more than an order of magnitude lower; and its bean burger attains nearly twice the nutritional score. Together, these results establish generative AI as a quantitative framework for learning human taste and navigating complex trade-offs in principled food design.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [72] [ResQ: Realistic Performance-Aware Query Generation](https://arxiv.org/abs/2602.02999)
*Zhengle Wang,Yanfei Zhang,Chunwei Liu*

Main category: cs.DB

TL;DR: 本文提出ResQ系统用于生成可执行SQL工作负载，在多组迹上实验表明其显著优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 获取真实世界SQL查询困难，现有云数据库供应商发布的匿名性能迹信息不足，现有工具无法高保真、高效地重现公共迹。

Method: 构建执行感知查询图，通过贝叶斯优化驱动的谓词搜索将其实例化为SQL，在精确查询和参数化模板级别显式建模工作负载重复，结合搜索空间边界和轻量级局部成本模型加速优化。

Result: 在公共云迹和新发布的工业迹上实验，ResQ实现96.71%的令牌节省和86.97%的运行时间减少，在CPU时间上最大Q误差降低14.8倍，在扫描字节上降低997.7倍，且紧密匹配操作符组成。

Conclusion: ResQ系统能有效生成可执行SQL工作负载，显著优于现有技术。

Abstract: Database research and development rely heavily on realistic user workloads for benchmarking, instance optimization, migration testing, and database tuning. However, acquiring real-world SQL queries is notoriously challenging due to strict privacy regulations. While cloud database vendors have begun releasing anonymized performance traces to the research community, these traces typi- cally provide only high-level execution statistics without the origi- nal query text or data, which is insufficient for scenarios that require actual execution. Existing tools fail to capture fine-grained perfor- mance patterns or generate runnable workloads that reproduce these public traces with both high fidelity and efficiency. To bridge this gap, we propose ResQ, a fine-grained workload synthesis sys- tem designed to generate executable SQL workloads that faithfully match the per-query execution targets and operator distributions of production traces. ResQ constructs execution-aware query graphs, instantiates them into SQL via Bayesian Optimization-driven pred- icate search, and explicitly models workload repetition through reuse at both exact-query and parameterized-template levels. To ensure practical scalability, ResQ combines search-space bounding with lightweight local cost models to accelerate optimization. Ex- periments on public cloud traces (Snowset, Redset) and a newly released industrial trace (Bendset) demonstrate that ResQ signif- icantly outperforms state-of-the-art baselines, achieving 96.71% token savings and a 86.97% reduction in runtime, while lowering maximum Q-error by 14.8x on CPU time and 997.7x on scanned bytes, and closely matching operator composition.

</details>


### [73] [Skill-Based Autonomous Agents for Material Creep Database Construction](https://arxiv.org/abs/2602.03069)
*Yue Wu,Tianhao Su,Shunbo Hu,Deng Pan*

Main category: cs.DB

TL;DR: 提出基于大语言模型的自主框架从科学PDF中挖掘高保真数据集，构建材料蠕变力学数据库，提取成功率超90%，确保数据自洽，为知识获取提供范式。


<details>
  <summary>Details</summary>
Motivation: 数据驱动材料科学发展受限于历史实验数据多存在于非结构化文本和栅格化图形中，人工整理耗力且易出错。

Method: 引入基于大语言模型的自主框架，采用模块化“基于技能”架构，执行语义过滤、多模态信息提取和物理信息验证等认知任务。

Result: 对243篇出版物应用该框架，图形数据数字化提取成功率超90%，数据点与本构参数交叉验证$R^2 > 0.99$。

Conclusion: 此工作为研究材料时变变形提供资源，建立了自主知识获取的可扩展范式，为下一代自动实验室奠定基础。

Abstract: The advancement of data-driven materials science is currently constrained by a fundamental bottleneck: the vast majority of historical experimental data remains locked within the unstructured text and rasterized figures of legacy scientific literature. Manual curation of this knowledge is prohibitively labor-intensive and prone to human error. To address this challenge, we introduce an autonomous, agent-based framework powered by Large Language Models (LLMs) designed to excavate high-fidelity datasets from scientific PDFs without human intervention. By deploying a modular "skill-based" architecture, the agent orchestrates complex cognitive tasks - including semantic filtering, multi-modal information extraction, and physics-informed validation. We demonstrate the efficacy of this framework by constructing a physically self-consistent database for material creep mechanics, a domain characterized by complex graphical trajectories and heterogeneous constitutive models. Applying the pipeline to 243 publications, the agent achieved a verified extraction success rate exceeding 90% for graphical data digitization. Crucially, we introduce a cross-modal verification protocol, demonstrating that the agent can autonomously align visually extracted data points with textually extracted constitutive parameters ($R^2 > 0.99$), ensuring the physical self-consistency of the database. This work not only provides a critical resource for investigating time-dependent deformation across diverse material systems but also establishes a scalable paradigm for autonomous knowledge acquisition, paving the way for the next generation of self-driving laboratories.

</details>


### [74] [StreamShield: A Production-Proven Resiliency Solution for Apache Flink at ByteDance](https://arxiv.org/abs/2602.03189)
*Yong Fang,Yuxing Han,Meng Wang,Yifan Zhang,Yue Ma,Chi Zhang*

Main category: cs.DB

TL;DR: 本文介绍字节跳动Flink集群的StreamShield弹性解决方案，含关键技术与测试部署管道，评估显示其有效。


<details>
  <summary>Details</summary>
Motivation: 分布式流处理系统中，确保弹性和运行稳定性对满足服务级别目标至关重要，但大规模生产环境实现此目标有挑战。

Method: 从引擎和集群角度设计StreamShield，采用运行时优化、细粒度容错、混合复制策略和外部系统高可用性等技术，提出测试和部署管道。

Result: 在生产集群上的广泛评估证明了StreamShield所提技术的效率和有效性。

Conclusion: StreamShield是一个经生产验证的弹性解决方案，能提升分布式流处理系统的弹性和稳定性。

Abstract: Distributed Stream Processing Systems (DSPSs) form the backbone of real-time processing and analytics at ByteDance, where Apache Flink powers one of the largest production clusters worldwide. Ensuring resiliency, the ability to withstand and rapidly recover from failures, together with operational stability, which provides consistent and predictable performance under normal conditions, is essential for meeting strict Service Level Objectives (SLOs). However, achieving resiliency and stability in large-scale production environments remains challenging due to the cluster scale, business diversity, and significant operational overhead. In this work, we present StreamShield, a production-proven resiliency solution deployed in ByteDance's Flink clusters. Designed along complementary perspectives of the engine and cluster, StreamShield introduces key techniques to enhance resiliency, covering runtime optimization, fine-grained fault-tolerance, hybrid replication strategy, and high availability under external systems. Furthermore, StreamShield proposes a robust testing and deployment pipeline that ensures reliability and robustness in production releases. Extensive evaluations on a production cluster demonstrate the efficiency and effectiveness of techniques proposed by StreamShield.

</details>


### [75] [A Pipeline for ADNI Resting-State Functional MRI Processing and Quality Control](https://arxiv.org/abs/2602.03278)
*Saige Rutherford,Zeshawn Zahid,Robert C. Welsh,Andrea Avena-Koenigsberger,Vincent Koppelmans,Amanda F. Mejia*

Main category: cs.DB

TL;DR: 为ADNI的静息态功能磁共振成像（rs - fMRI）数据提出处理流程，以提升数据利用质量和规模。


<details>
  <summary>Details</summary>
Motivation: ADNI的rs - fMRI数据因采集协议、质量和时间对齐等问题，许多研究仅用小部分数据，限制了统计效力等，故需要改进数据处理方式。

Method: 使用开源软件（Clinica、fMRIPrep和MRIQC）和定制工具，整合各ADNI站点和扫描器类型的数据，进行数据下载、时间对齐、预处理和质量控制，生成质量指标和报告。

Result: 生成符合BIDS - derivatives规范的高质量rs - fMRI时间序列数据。

Conclusion: 该协议为ADNI fMRI数据管理和利用提供透明可扩展框架，有助于AD研究中的生物标志物发现和多模态分析。

Abstract: The Alzheimer's Disease Neuroimaging Initiative (ADNI) provides a comprehensive multimodal neuroimaging resource for studying aging and Alzheimer's disease (AD). Since its second wave, ADNI has increasingly collected resting-state functional MRI (rs-fMRI), a valuable resource for discovering brain connectivity changes predictive of cognitive decline and AD. A major barrier to its use is the considerable variability in acquisition protocols and data quality, compounded by missing imaging sessions and inconsistencies in how functional scans temporally align with clinical assessments. As a result, many studies only utilize a small subset of the total rs-fMRI data, limiting statistical power, reproducibility, and the ability to study longitudinal functional brain changes at scale. Here, we describe a pipeline for ADNI rs-fMRI data that encompasses the download of necessary imaging and clinical data, temporally aligning the clinical and imaging data, preprocessing, and quality control. We integrate data curation and preprocessing across all ADNI sites and scanner types using a combination of open-source software (Clinica, fMRIPrep, and MRIQC) and bespoke tools. Quality metrics and reports are generated for each subject and session to facilitate rigorous data screening. All scripts and configuration files are available to enable reproducibility. The pipeline, which currently supports ADNI-GO, ADNI-2, and ADNI-3 data releases, outputs high-quality rs-fMRI time series data adhering to the BIDS-derivatives specification. This protocol provides a transparent and scalable framework for curating and utilizing ADNI fMRI data, empowering large-scale functional biomarker discovery and integrative multimodal analyses in Alzheimer's disease research.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [76] [Prefix Consensus For Censorship Resistant BFT](https://arxiv.org/abs/2602.02892)
*Zhuolun Xiang,Andrei Tonkikh,Alexander Spiegelman*

Main category: cs.DC

TL;DR: 提出新抽象和协议栈解决BFT共识链中审查抗性弱的问题，构建相关共识并得到 censorship - resistant BFT SMR协议，还连接与其他共识。


<details>
  <summary>Details</summary>
Motivation: 现有BFT共识在区块链中审查抗性弱，领导者可排除交易，在交易和DeFi领域引发担忧。

Method: 引入Prefix Consensus和Strong Prefix Consensus，构建无领导、多提议者的 censorship - resistant BFT SMR协议，建立与其他共识的联系。

Result: Prefix Consensus可异步求解并给出轮复杂度界限，在四回合内提交诚实提案，得到最优延迟分级共识和无领导的Binary/Validated Consensus。

Conclusion: 所提方法能有效解决BFT共识审查抗性弱的问题，在多种情况下保证协议的性能和安全性。

Abstract: Despite broad use of BFT consensus in blockchains, censorship resistance is weak: leaders can exclude transactions, a growing concern for trading and DeFi.
  We address this by introducing a new abstraction and protocol stack. First, we introduce \emph{Prefix Consensus}, where parties input vectors and output $(v^{\sf low},v^{\sf high})$ that (i) extend the maximum common prefix of honest inputs and (ii) satisfy $v_i^{\sf low}\preceq v_j^{\sf high}$ for all honest $i,j$. Unlike classical consensus, no single output is required. We show Prefix Consensus is solvable asynchronously and give tight round-complexity bounds.
  We then define \emph{Strong Prefix Consensus}, requiring agreement on the \emph{high} output. Our protocol is leaderless and partially synchronous: one Prefix Consensus instance decides (possibly different) lows, and additional instances yield a unique safe-to-extend high, even if an adversary can suspend one party per round.
  We lift this to a leaderless, multi-proposer, censorship-resistant BFT SMR protocol: per slot, all parties broadcast proposals, deterministically rank them, and run one Strong Prefix Consensus on proposal hashes, committing honest proposals in \emph{four rounds}. A deterministic demotion rule updates the ranking when a party's proposal is excluded, implying that after GST at most $f$ slots can miss an honest proposal while progress remains leaderless under suspension and up to $f{-}1$ Byzantine faults.
  Finally, we connect Prefix Consensus to graded and binary/validated consensus: we obtain an optimal-latency graded consensus (3 message delays) and leaderless Binary/Validated Consensus with worst-case message complexity $O(n^3)$ and communication $O(n^4)$.

</details>


### [77] [Large-Scale LLM Inference with Heterogeneous Workloads: Prefill-Decode Contention and Asymptotically Optimal Control](https://arxiv.org/abs/2602.02987)
*Ruihan Lin,Zezhen Ding,Zean Han,Jiheng Zhang*

Main category: cs.DC

TL;DR: 本文针对大语言模型推理的两阶段特性和工作负载异质性问题，开发了随机控制框架来调度异构LLM工作负载，证明策略渐近最优，且实验显示优于标准服务启发式方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型推理的两阶段共享GPU资源时会产生状态相关的争用问题，且工作负载具有异质性，需要有效调度方法。

Method: 将LLM推理建模为具有状态相关服务率的多类多服务器排队网络，分析流体近似，求解稳态线性规划，设计门控和路由策略。

Result: 设计的策略在多GPU极限情况下渐近最优，在考虑延迟和公平性等服务水平指标时也适用。

Conclusion: 数值实验表明所提出的策略优于标准服务启发式方法，能有效调度异构LLM工作负载。

Abstract: Large Language Models (LLMs) are rapidly becoming critical infrastructure for enterprise applications, driving unprecedented demand for GPU-based inference services. A key operational challenge arises from the two-phase nature of LLM inference: a compute-intensive \emph{prefill} phase that processes user input, followed by a memory-bound \emph{decode} phase that generates output tokens. When these phases share GPU resources, prefill tasks throttle the processing speed of concurrent decodes, creating state-dependent contention. This contention is further complicated by workload heterogeneity, as different applications exhibit vastly different input and output lengths. We develop a stochastic control framework for scheduling heterogeneous LLM workloads across large GPU clusters. We formulate LLM inference as a multiclass many-server queueing network with state-dependent service rates, grounded in empirical iteration-time measurements. We analyze the fluid approximation of this system and solve steady-state linear programs that characterize optimal resource allocation. We design gate-and-route policies that regulate prefill admission and decode routing, and prove that they are asymptotically optimal in the many-GPU limit under both bundled and separate token-pricing schemes. We further extend the framework to incorporate Service Level Indicators (SLIs) such as latency and fairness, providing a general approach to constrained scheduling. Numerical experiments calibrated to empirical iteration-time data demonstrate that our policies outperform standard serving heuristics.

</details>


### [78] [Studying the Effect of Schedule Preemption on Dynamic Task Graph Scheduling](https://arxiv.org/abs/2602.03081)
*Mohammadali Khodabandehlou,Jared Coleman,Niranjan Suri,Bhaskar Krishnamachari*

Main category: cs.DC

TL;DR: 研究可控调度抢占，引入Last - K Preemption模型，对比不同策略，发现适度抢占可兼顾多方面优势。


<details>
  <summary>Details</summary>
Motivation: 现有动态任务图调度常不重新审视先前任务分配，主要关注最小化完工时间，需改进调度方法。

Method: 引入Last - K Preemption模型，使用合成、RIoTBench、WFCommons和对抗性工作负载，对比抢占、非抢占和部分抢占策略。

Result: 适度抢占能在保持公平性和低开销的同时，实现与完全抢占相近的完工时间和利用率提升。

Conclusion: 适度抢占策略在任务图调度中具有优势。

Abstract: Dynamic scheduling of task graphs is often addressed without revisiting prior task allocations, with a primary focus on minimizing makespan. We study controlled schedule preemption, introducing the Last-K Preemption model, which selectively reschedules recent task graphs while preserving earlier allocations. Using synthetic, RIoTBench, WFCommons, and adversarial workloads, we compare preemptive, non-preemptive, and partial-preemptive strategies across makespan, fairness, utilization, and runtime. Results show moderate preemption can match most makespan and utilization gains of full preemption while maintaining fairness and low overhead.

</details>


### [79] [Joint Network-and-Server Congestion in Multi-Source Traffic Allocation: A Convex Formulation and Price-Based Decentralization](https://arxiv.org/abs/2602.03246)
*Tamoghna Sarkar,Bhaskar Krishnamachari*

Main category: cs.DC

TL;DR: 本文研究多源多服务节点的稳态流量速率分配问题，证明其为凸规划，开发了轻量级分布式定价算法，数值示例展示了收敛性和权衡。


<details>
  <summary>Details</summary>
Motivation: 解决多源多服务节点在路径延迟和节点排队延迟均受容量约束情况下的流量速率分配问题。

Method: 证明问题为凸规划，利用KKT条件得到全局最优解，开发基于分布式定价算法，其中服务节点计算并广播拥塞价格，源根据价格更新流量分配。

Result: 分布式迭代收敛到集中式最优解，揭示了联合建模接入和服务拥塞带来的权衡。

Conclusion: 所提出的分布式定价算法可以有效解决多源多服务节点的流量速率分配问题。

Abstract: This paper studies an important rate allocation problem that arises in many networked and distributed systems: steady-state traffic rate allocation from multiple sources to multiple service nodes when both (i) the access-path delay on each source-node route is rate-dependent (capacity-constrained) and convex, and (ii) each service node (also capacity-constrained) experiences a load-dependent queueing delay driven by aggregate load from all sources. We show that the resulting flow-weighted end-to-end delay minimization is a convex program, yielding a global system-optimal solution characterized by KKT conditions that equalize total marginal costs (a path marginal access term plus a node congestion price) across all utilized routes. This condition admits a Wardrop-type interpretation: for each source, all utilized options equalize total marginal cost, while any option with strictly larger total marginal cost receives no flow. Building on this structure, we develop a lightweight distributed pricing-based algorithm in which each service node locally computes and broadcasts a scalar congestion price from its observed aggregate load, while each source updates its traffic split by solving a small separable convex allocation problem under the advertised prices. Numerical illustrations demonstrate convergence of the distributed iteration to the centralized optimum and highlight the trade-offs induced by jointly modeling access and service congestion.

</details>


### [80] [Exploiting Multi-Core Parallelism in Blockchain Validation and Construction](https://arxiv.org/abs/2602.03444)
*Arivarasan Karmegam,Lucianna Kiffer,Antonio Fernández Anta*

Main category: cs.DC

TL;DR: 本文研究区块链验证器如何在不违反区块链语义下利用多核并行性，提出优化问题及对应解法并量化最优性与运行时间的权衡。


<details>
  <summary>Details</summary>
Motivation: 区块链验证器可利用多核CPU减少块处理时间，但需在遵循交易冲突和每块运行时间限制下保持给定总顺序，因此要研究如何利用多核并行性。

Method: 形式化两个验证器端优化问题，为其开发精确的混合整数线性规划（MILP）公式，提出快速确定性启发式算法，使用以太坊主网跟踪数据并设置基线进行实验。

Result: 通过实验量化了最优性和运行时间之间的权衡。

Conclusion: 所提出的方法能帮助验证器在不违反区块链语义的情况下，在块构建和执行过程中有效利用多核并行性。

Abstract: Blockchain validators can reduce block processing time by exploiting multi-core CPUs, but deterministic execution must preserve a given total order while respecting transaction conflicts and per-block runtime limits. This paper systematically examines how validators can exploit multi-core parallelism during both block construction and execution without violating blockchain semantics. We formalize two validator-side optimization problems: (i) executing an already ordered block on \(p\) cores to minimize makespan while ensuring equivalence to sequential execution; and (ii) selecting and scheduling a subset of mempool transactions under a runtime limit \(B\) to maximize validator reward. For both, we develop exact Mixed-Integer Linear Programming (MILP) formulations that capture conflict, order, and capacity constraints, and propose fast deterministic heuristics that scale to realistic workloads. Using Ethereum mainnet traces and including a Solana-inspired declared-access baseline (Sol) for ordered-block scheduling and a simple reward-greedy baseline (RG) for block construction, we empirically quantify the trade-offs between optimality and runtime.

</details>


### [81] [Recursive Energy Efficient Agreement](https://arxiv.org/abs/2602.03474)
*Shachar Meir,David Peleg*

Main category: cs.DC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Agreement is a foundational problem in distributed computing that have been studied extensively for over four decades. Recently, Meir, Mirault, Peleg and Robinson introduced the notion of \emph{Energy Efficient Agreement}, where the goal is to solve Agreement while minimizing the number of round a party participates in, thereby reducing the energy cost per participant. We show a recursive Agreement algorithm that has $O(\log f)$ active rounds per participant, where $f<n$ represents the maximum number of crash faults in the system.

</details>


### [82] [DALI: A Workload-Aware Offloading Framework for Efficient MoE Inference on Local PCs](https://arxiv.org/abs/2602.03495)
*Zeyu Zhu,Gang Li,Peisong Wang,Zitao Mo,Minnan Pei,Zhuoran Song,Xiaoyao Liang,Jian Cheng*

Main category: cs.DC

TL;DR: 现有MoE架构参数大，现有卸载方法有不足，提出DALI框架解决问题并实现加速。


<details>
  <summary>Details</summary>
Motivation: 现有MoE架构参数大，现有卸载方法存在CPU-GPU负载不平衡、预取不准确、GPU缓存命中率低等问题，需改进。

Method: 提出DALI框架，包括用贪心分配策略动态分配专家到CPU或GPU、基于残差的预取方法和工作负载感知的缓存替换策略。

Result: 在各种MoE模型和设置下，DALI在预填充和解码阶段比现有卸载框架显著加速。

Conclusion: DALI框架能有效解决现有MoE卸载方法的问题，提高推理效率。

Abstract: Mixture of Experts (MoE) architectures significantly enhance the capacity of LLMs without proportional increases in computation, but at the cost of a vast parameter size. Offloading MoE expert parameters to host memory and leveraging both CPU and GPU computation has recently emerged as a promising direction to support such models on resourceconstrained local PC platforms. While promising, we notice that existing approaches mismatch the dynamic nature of expert workloads, which leads to three fundamental inefficiencies: (1) Static expert assignment causes severe CPUGPU load imbalance, underutilizing CPU and GPU resources; (2) Existing prefetching techniques fail to accurately predict high-workload experts, leading to costly inaccurate prefetches; (3) GPU cache policies neglect workload dynamics, resulting in poor hit rates and limited effectiveness. To address these challenges, we propose DALI, a workloaDAware offLoadIng framework for efficient MoE inference on local PCs. To fully utilize hardware resources, DALI first dynamically assigns experts to CPU or GPU by modeling assignment as a 0-1 integer optimization problem and solving it efficiently using a Greedy Assignment strategy at runtime. To improve prefetching accuracy, we develop a Residual-Based Prefetching method leveraging inter-layer residual information to accurately predict high-workload experts. Additionally, we introduce a Workload-Aware Cache Replacement policy that exploits temporal correlation in expert activations to improve GPU cache efficiency. By evaluating across various MoE models and settings, DALI achieves significant speedups in the both prefill and decoding phases over the state-of-the-art offloading frameworks.

</details>


### [83] [Do We Need Asynchronous SGD? On the Near-Optimality of Synchronous Methods](https://arxiv.org/abs/2602.03802)
*Grigory Begunov,Alexander Tyurin*

Main category: cs.DC

TL;DR: 重新审视同步SGD及其变体，理论证明其在多种异构计算场景近乎最优


<details>
  <summary>Details</summary>
Motivation: 现代分布式优化方法多依赖传统同步方法，虽异步优化有进展但同步方法优势待研究

Method: 分析同步方法在随机计算时间和工人部分参与情况下特性，证明时间复杂度

Result: 同步方法在许多实际场景时间复杂度最优，达对数因子

Conclusion: 同步方法在许多现代异构计算场景足够但非通用解

Abstract: Modern distributed optimization methods mostly rely on traditional synchronous approaches, despite substantial recent progress in asynchronous optimization. We revisit Synchronous SGD and its robust variant, called $m$-Synchronous SGD, and theoretically show that they are nearly optimal in many heterogeneous computation scenarios, which is somewhat unexpected. We analyze the synchronous methods under random computation times and adversarial partial participation of workers, and prove that their time complexities are optimal in many practical regimes, up to logarithmic factors. While synchronous methods are not universal solutions and there exist tasks where asynchronous methods may be necessary, we show that they are sufficient for many modern heterogeneous computation scenarios.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [84] [Learning-augmented smooth integer programs with PAC-learnable oracles](https://arxiv.org/abs/2602.02505)
*Hao-Yuan He,Ming Li*

Main category: cs.DS

TL;DR: 研究平滑整数规划的学习增强算法，引入框架并证明其有效性和可学习性


<details>
  <summary>Details</summary>
Motivation: 为平滑整数规划（如MAX - CUT和MAX - k - SAT）找到更好的算法

Method: 引入结合预测神谕的框架构建目标线性替代物，通过线性规划和舍入过程求解，还建立其PAC可学习性

Result: 该方法有效将经典稠密区域的近似方法扩展到近稠密区域，且算法类有有界伪维度

Conclusion: 可以用多项式样本学习到具有近最优期望性能的神谕

Abstract: This paper investigates learning-augmented algorithms for smooth integer programs, covering canonical problems such as MAX-CUT and MAX-k-SAT. We introduce a framework that incorporates a predictive oracle to construct a linear surrogate of the objective, which is then solved via linear programming followed by a rounding procedure. Crucially, our framework ensures that the solution quality is both consistent and smooth against prediction errors. We demonstrate that this approach effectively extends tractable approximations from the classical dense regime to the near-dense regime. Furthermore, we go beyond the assumption of oracle existence by establishing its PAC-learnability. We prove that the induced algorithm class possesses a bounded pseudo-dimension, thereby ensuring that an oracle with near-optimal expected performance can be learned with polynomial samples.

</details>


### [85] [On the Complexity of Maximal/Closed Frequent Tree Mining for Bounded Height Trees](https://arxiv.org/abs/2602.03436)
*Kenta Komoto,Kazuhiro Kurita,Hirotaka Ono*

Main category: cs.DS

TL;DR: 本文研究枚举所有频繁最大/闭合树问题，明确不同树高设定下该问题的复杂度。


<details>
  <summary>Details</summary>
Motivation: 经典的数据挖掘问题中，在树高的‘现实假设’下该问题的复杂度未明确，尤其是树高较小时的情况。

Method: 在有序和无序树、最大和闭合变体等多种设定下研究树挖掘问题。

Result: 文中未提及具体结果。

Conclusion: 文中未提及具体结论。

Abstract: In this paper, we address the problem of enumerating all frequent maximal/closed trees. This is a classical and central problem in data mining. Although many practical algorithms have been developed for this problem, its complexity under ``realistic assumptions'' on tree height has not been clarified. More specifically, while it was known that the mining problem becomes hard when the tree height is at least 60, the complexity for cases where the tree height is smaller has not yet been clarified. We resolve this gap by establishing results for these tree mining problems under several settings, including ordered and unordered trees, as well as maximal and closed variants.

</details>


### [86] [ZOR filters: fast and smaller than fuse filters](https://arxiv.org/abs/2602.03525)
*Antoine Limasset*

Main category: cs.DS

TL;DR: 介绍了ZOR过滤器，在保留XOR/fuse过滤器查询机制基础上，保证构建过程确定性，能以接近理论下限的开销实现高效查询。


<details>
  <summary>Details</summary>
Motivation: 当前XOR和fuse过滤器基于剥离的构建过程仅能大概率成功，导致确定性构建复杂。

Method: 引入ZOR过滤器，用确定性剥离替代失败重启方式，并将剩余键存储在紧凑辅助结构。

Result: 实验中，适度元数下被放弃键比例低于1%，ZOR过滤器开销接近理论下限，保持类似fuse查询性能，但额外开销集中在负查询。目前构建速度比优化的fuse构建器慢。

Conclusion: ZOR过滤器可保证构建终止，在开销和查询性能上表现良好，缩小构建速度差距是工程目标。

Abstract: Probabilistic membership filters support fast approximate membership queries with a controlled false-positive probability $\varepsilon$ and are widely used across storage, analytics, networking, and bioinformatics \cite{chang2008bigtable,dayan2018optimalbloom,broder2004network,harris2020improved,marchet2023scalable,chikhi2025logan,hernandez2025reindeer2}. In the static setting, state-of-the-art designs such as XOR and fuse filters achieve low overhead and very fast queries, but their peeling-based construction succeeds only with high probability, which complicates deterministic builds \cite{graf2020xor,graf2022binary,ulrich2023taxor}.
  We introduce \emph{ZOR filters}, a deterministic continuation of XOR/fuse filters that guarantees construction termination while preserving the same XOR-based query mechanism. ZOR replaces restart-on-failure with deterministic peeling that abandons a small fraction of keys, and restores false-positive-only semantics by storing the remainder in a compact auxiliary structure. In our experiments, the abandoned fraction drops below $1\%$ for moderate arity (e.g., $N\ge 5$), so the auxiliary handles a negligible fraction of keys. As a result, ZOR filters can achieve overhead within $1\%$ of the information-theoretic lower bound $\log_2(1/\varepsilon)$ while retaining fuse-like query performance; the additional cost is concentrated on negative queries due to the auxiliary check. Our current prototype builds several-fold slower than highly optimized fuse builders because it maintains explicit incidence information during deterministic peeling; closing this optimisation gap is an engineering target.

</details>


### [87] [Perfect Network Resilience in Polynomial Time](https://arxiv.org/abs/2602.03827)
*Matthias Bentert,Stefan Schmid*

Main category: cs.DS

TL;DR: 本文给出了何时能实现完美弹性的完整刻画，设计了判断实例是否具有完美弹性的O(n)时间算法和计算完美弹性重路由规则的O(nm)时间算法，部分肯定回答了一个长期悬而未决的问题。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对何时能实现完美弹性的详细理解，本文旨在填补这一空白。

Method: 对何时能实现完美弹性进行完整刻画，设计O(n)时间的判断算法和O(nm)时间的计算算法。

Result: 得出完美弹性的刻画结果，设计出相应算法，且表明在完美弹性背景下，跳过重路由规则与更通用的重路由规则一样强大。

Conclusion: 部分肯定回答了Chiesa等人提出的长期悬而未决的问题。

Abstract: Modern communication networks support local fast rerouting mechanisms to quickly react to link failures: nodes store a set of conditional rerouting rules which define how to forward an incoming packet in case of incident link failures. The rerouting decisions at any node $v$ must rely solely on local information available at $v$: the link from which a packet arrived at $v$, the target of the packet, and the incident link failures at $v$. Ideally, such rerouting mechanisms provide perfect resilience: any packet is routed from its source to its target as long as the two are connected in the underlying graph after the link failures. Already in their seminal paper at ACM PODC '12, Feigenbaum, Godfrey, Panda, Schapira, Shenker, and Singla showed that perfect resilience cannot always be achieved. While the design of local rerouting algorithms has received much attention since then, we still lack a detailed understanding of when perfect resilience is achievable.
  This paper closes this gap and presents a complete characterization of when perfect resilience can be achieved. This characterization also allows us to design an $O(n)$-time algorithm to decide whether a given instance is perfectly resilient and an $O(nm)$-time algorithm to compute perfectly resilient rerouting rules whenever it is. Our algorithm is also attractive for the simple structure of the rerouting rules it uses, known as skipping in the literature: alternative links are chosen according to an ordered priority list (per in-port), where failed links are simply skipped. Intriguingly, our result also implies that in the context of perfect resilience, skipping rerouting rules are as powerful as more general rerouting rules. This partially answers a long-standing open question by Chiesa, Nikolaevskiy, Mitrovic, Gurtov, Madry, Schapira, and Shenker [IEEE/ACM Transactions on Networking, 2017] in the affirmative.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [88] [A two-player version of the assignment problem](https://arxiv.org/abs/2602.02628)
*Florian Galliot,Nacim Oijid,Jonas Sénizergues*

Main category: cs.GT

TL;DR: 本文介绍竞争分配问题，证明其PSPACE - 完全性，还给出特定情况下复杂度结果。


<details>
  <summary>Details</summary>
Motivation: 现实中存在两个实体争夺相同资源并用于竞争的情况，如体育和纸牌游戏中的选秀，需要对这类问题进行研究。

Method: 对问题进行分类讨论，分析不同条件下（如代理最多有两个非零效率、最多有一个非零效率等）问题的复杂度。

Result: 该问题是PSPACE - 完全的，即使限制代理最多有两个非零效率；在代理最多有一个非零效率情况下，问题在以任务数量为参数时属于XP类，且当只有两个任务时可在线性时间计算最优得分。

Conclusion: 明确了竞争分配问题在不同条件下的计算复杂度。

Abstract: We introduce the competitive assignment problem, a two-player version of the well-known assignment problem. Given a set of tasks and a set of agents with different efficiencies for different tasks, Alice and Bob take turns picking agents one by one. Once all agents have been picked, Alice and Bob compute the optimal values $s_A$ and $s_B$ for the assignment problem on their respective sets of agents, i.e. they assign their own agents to tasks (with at most one agent per task and at most one task per agent) so as to maximize the sum of the efficiencies. The score of the game is then defined as $s_A-s_B$. Alice aims at maximizing the score, while Bob aims at minimizing it. This problem can model drafts in sports and card games, or more generally situations where two entities fight for the same resources and then use them to compete against each other. We show that the problem is PSPACE-complete, even restricted to agents that have at most two nonzero efficiencies. On the other hand, in the case of agents having at most one nonzero efficiency, the problem lies in XP parameterized by the number of tasks, and the optimal score can be computed in linear time when there are only two tasks.

</details>


### [89] [Internet of Agentic AI: Incentive-Compatible Distributed Teaming and Workflow](https://arxiv.org/abs/2602.03145)
*Ya-Ting Yang,Quanyan Zhu*

Main category: cs.GT

TL;DR: 论文提出用于可扩展代理智能的“智能代理互联网”框架，解决现有代理架构局限性，还提出相关算法，并用医疗案例验证。


<details>
  <summary>Details</summary>
Motivation: 现有大多数代理架构集中且单一，限制了可扩展性、专业性和互操作性，需要提出新框架解决这些问题。

Method: 提出“智能代理互联网”框架，形式化代理协作的网络原生模型，引入激励兼容的工作流联盟可行性框架，提出分散式联盟形成算法。

Result: 框架可作为模型上下文协议（MCP）之上的协调层，医疗案例研究展示了其能实现可扩展、有弹性且经济可行的代理工作流。

Conclusion: 该工作为新兴的智能代理互联网时代的原则性协调和可扩展性奠定了基础。

Abstract: Large language models (LLMs) have enabled a new class of agentic AI systems that reason, plan, and act by invoking external tools. However, most existing agentic architectures remain centralized and monolithic, limiting scalability, specialization, and interoperability. This paper proposes a framework for scalable agentic intelligence, termed the Internet of Agentic AI, in which autonomous, heterogeneous agents distributed across cloud and edge infrastructure dynamically form coalitions to execute task-driven workflows. We formalize a network-native model of agentic collaboration and introduce an incentive-compatible workflow-coalition feasibility framework that integrates capability coverage, network locality, and economic implementability. To enable scalable coordination, we formulate a minimum-effort coalition selection problem and propose a decentralized coalition formation algorithm. The proposed framework can operate as a coordination layer above the Model Context Protocol (MCP). A healthcare case study demonstrates how domain specialization, cloud-edge heterogeneity, and dynamic coalition formation enable scalable, resilient, and economically viable agentic workflows. This work lays the foundation for principled coordination and scalability in the emerging era of Internet of Agentic AI.

</details>


### [90] [Dynamic Programming for Epistemic Uncertainty in Markov Decision Processes](https://arxiv.org/abs/2602.03381)
*Axel Benyamine,Julien Grand-Clément,Marek Petrik,Michael I. Jordan,Alain Durmus*

Main category: cs.GT

TL;DR: 提出模糊厌恶MDPs通用理论，统一相关模型，扩展相关概念并建立动态规划原理的结果，刻画风险度量。


<details>
  <summary>Details</summary>
Motivation: 处理MDPs中不确定转移概率问题，统一具有认知不确定性的MDPs模型。

Method: 将不确定转移概率视为随机变量，用风险度量评估策略，扩展价值函数和贝尔曼算子等概念。

Result: 建立动态规划原理下的结果，如平稳策略的存在性、价值和策略迭代算法，刻画了与动态规划兼容的风险度量。

Conclusion: 建立了多种MDP模型间的联系，明确了动态规划范式下的可能性及需跳出该范式的风险度量类型。

Abstract: In this paper, we propose a general theory of ambiguity-averse MDPs, which treats the uncertain transition probabilities as random variables and evaluates a policy via a risk measure applied to its random return. This ambiguity-averse MDP framework unifies several models of MDPs with epistemic uncertainty for specific choices of risk measures. We extend the concepts of value functions and Bellman operators to our setting. Based on these objects, we establish the consequences of dynamic programming principles in this framework (existence of stationary policies, value and policy iteration algorithms), and we completely characterize law-invariant risk measures compatible with dynamic programming. Our work draws connections among several variants of MDP models and fully delineates what is possible under the dynamic programming paradigm and which risk measures require leaving it.

</details>


### [91] [Toward a Sustainable Federated Learning Ecosystem: A Practical Least Core Mechanism for Payoff Allocation](https://arxiv.org/abs/2602.03387)
*Zhengwei Ni,Zhidu Li,Wei Chen,Zhaoyang Zhang,Zehua Wang,F. Richard Yu,Victor C. M. Leung*

Main category: cs.GT

TL;DR: 本文针对联邦学习中收益分配问题，引入基于最小核心（LC）概念的收益分配框架，并提出基于栈的剪枝算法实现，案例研究表明该框架能促进稳定协作和可持续的联邦学习生态。


<details>
  <summary>Details</summary>
Motivation: 新兴网络范式和应用依赖联邦学习实现隐私保护下的协作智能，但协作环境的可持续性依赖公平稳定的收益分配机制，现有方法缺乏对联盟稳定性的关注。

Method: 引入基于最小核心（LC）概念的收益分配框架，提出基于栈的剪枝算法以适应大规模网络，平衡计算效率和分配精度。

Result: 在联邦入侵检测的案例研究中，该机制能正确识别关键贡献者和战略联盟。

Conclusion: 实用的LC框架能促进稳定协作，培育可持续的联邦学习生态。

Abstract: Emerging network paradigms and applications increasingly rely on federated learning (FL) to enable collaborative intelligence while preserving privacy. However, the sustainability of such collaborative environments hinges on a fair and stable payoff allocation mechanism. Focusing on coalition stability, this paper introduces a payoff allocation framework based on the least core (LC) concept. Unlike traditional methods, the LC prioritizes the cohesion of the federation by minimizing the maximum dissatisfaction among all potential subgroups, ensuring that no participant has an incentive to break away. To adapt this game-theoretic concept to practical, large-scale networks, we propose a streamlined implementation with a stack-based pruning algorithm, effectively balancing computational efficiency with allocation precision. Case studies in federated intrusion detection demonstrate that our mechanism correctly identifies pivotal contributors and strategic alliances. The results confirm that the practical LC framework promotes stable collaboration and fosters a sustainable FL ecosystem.

</details>


### [92] [Sequential Linear Contracts on Matroids](https://arxiv.org/abs/2602.03543)
*Kanstantsin Pashkovich,Jacob Skitsko,Yun Xing*

Main category: cs.GT

TL;DR: 研究拟阵约束下的顺序合同，聚焦线性合同，建立最优线性合同问题与拟阵（不）可靠性问题的关系。


<details>
  <summary>Details</summary>
Motivation: 在顺序合同场景下，激励代理采取行动，需研究合适的合同。

Method: 集中研究线性合同，假设委托人总奖励基于拟阵中的独立集行动子集计算。

Result: 发现寻找最优线性合同问题（或计算相应委托人效用）与拟阵（不）可靠性问题存在关系，在添加元素并行副本条件下问题等价。

Conclusion: 明确了拟阵约束下顺序合同中最优线性合同问题与拟阵（不）可靠性问题的联系。

Abstract: In this work, we study sequential contracts under matroid constraints. In the sequential setting, an agent can take actions one by one. After each action, the agent observes the stochastic value of the action and then decides which action to take next, if any. At the end, the agent decides what subset of taken actions to use for the principal's reward; and the principal receives the total value of this subset as a reward. Taking each action induces a certain cost for the agent. Thus, to motivate the agent to take actions the principal is expected to offer an appropriate contract. A contract describes the payment from the principal to the agent as a function of the principal's reward obtained through the agent's actions. In this work, we concentrate on studying linear contracts, i.e.\ the contracts where the principal transfers a fraction of their total reward to the agent. We assume that the total principal's reward is calculated based on a subset of actions that forms an independent set in a given matroid. We establish a relationship between the problem of finding an optimal linear contract (or computing the corresponding principal's utility) and the so called matroid (un)reliability problem. Generally, the above problems turn out to be equivalent subject to adding parallel copies of elements to the given matroid.

</details>


### [93] [Efficient Investment in Multi-Agent Models of Public Transportation](https://arxiv.org/abs/2602.03687)
*Martin Bullinger,Edith Elkind,Kassian Köck*

Main category: cs.GT

TL;DR: 研究两个公共交通资源分配模型，一个是公交站点开放决策，另一个是复杂网络边旅行时间改善决策，得出不同情况下的计算复杂度结果。


<details>
  <summary>Details</summary>
Motivation: 研究在公共交通中分配有限、不可分割资源的决策问题。

Method: 对第一个模型分析计算复杂度；对第二个模型，用Dijkstra算法和动态规划结合找最优解，分析不同数量代理时的复杂度。

Result: 第一个模型中，计算平等主义福利近似最优解是NP完全的；第二个模型中，一两个代理时有多项式时间算法，代理数量可变时，功利主义和平等主义福利有NP完全和不可近似性结果。

Conclusion: 不同模型和条件下公共交通资源分配决策计算复杂度不同，结果对铁路网络设计有启示。

Abstract: We study two stylized, multi-agent models aimed at investing a limited, indivisible resource in public transportation. In the first model, we face the decision of which potential stops to open along a (e.g., bus) path, given agents' travel demands. While it is known that utilitarian optimal solutions can be identified in polynomial time, we find that computing approximately optimal solutions with respect to egalitarian welfare is NP-complete. This is surprising as we operate on the simple topology of a line graph.
  In the second model, agents navigate a more complex network modeled by a weighted graph where edge weights represent distances. We face the decision of improving travel time along a fixed number of edges. We provide a polynomial-time algorithm that combines Dijkstra's algorithm with a dynamical program to find the optimal decision for one or two agents. By contrast, if the number of agents is variable, we find \np-completeness and inapproximability results for utilitarian and egalitarian welfare. Moreover, we demonstrate implications of our results for a related model of railway network design.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [94] [Design and Evaluation of Whole-Page Experience Optimization for E-commerce Search](https://arxiv.org/abs/2602.02514)
*Pratik Lahiri,Bingqing Ge,Zhou Qin,Aditya Jumde,Shuning Huo,Lucas Scottini,Yi Liu,Mahmoud Mamlouk,Wenyang Liu*

Main category: cs.IR

TL;DR: 传统排名模型不适用于现代电商搜索结果页，本文提出全页体验优化框架，经A/B测试验证有效。


<details>
  <summary>Details</summary>
Motivation: 传统位置偏置排名模型无法满足现代电商搜索结果页需求，现有优化框架多关注短期信号，缺乏对长期用户满意度的考量。

Method: 提出全页体验优化框架，明确建模商品相关性、二维位置布局和视觉元素间的相互作用，用因果框架基于准实验数据开发衡量长期用户满意度的指标。

Result: 通过行业规模的A/B测试验证，模型使品牌相关性提升1.86%，并使收入显著提升0.05%。

Conclusion: 所提出的全页体验优化框架能有效提升品牌相关性和收入，可用于解决电商搜索结果页的排名和优化问题。

Abstract: E-commerce Search Results Pages (SRPs) are evolving from linear lists to complex, non-linear layouts, rendering traditional position-biased ranking models insufficient. Moreover, existing optimization frameworks typically maximize short-term signals (e.g., clicks, same-day revenue) because long-term satisfaction metrics (e.g., expected two-week revenue) involve delayed feedback and challenging long-horizon credit attribution. To bridge these gaps, we propose a novel Whole-Page Experience Optimization Framework. Unlike traditional list-wise rankers, our approach explicitly models the interplay between item relevance, 2D positional layout, and visual elements. We use a causal framework to develop metrics for measuring long-term user satisfaction based on quasi-experimental data. We validate our approach through industry-scale A/B testing, where the model demonstrated a 1.86% improvement in brand relevance (our primary customer experience metric) while simultaneously achieving a statistically significant revenue uplift of +0.05%

</details>


### [95] [Col-Bandit: Zero-Shot Query-Time Pruning for Late-Interaction Retrieval](https://arxiv.org/abs/2602.02827)
*Roi Pony,Adi Raz,Oshri Naparstek,Idan Friedman,Udi Barzelay*

Main category: cs.IR

TL;DR: 提出Col - Bandit算法，在不影响排名保真度下减少MaxSim计算量。


<details>
  <summary>Details</summary>
Motivation: 多向量后期交互检索器查询时计算成本高，单向量近似会造成精度损失，需降低计算负担。

Method: 将重排序转化为有限总体Top - K识别问题，维护部分观察到的文档分数的不确定性边界，动态稀疏化交互矩阵，作为零样本层应用于标准多向量系统。

Result: 在文本和多模态基准测试中，Col - Bandit在保留排名保真度的同时，最多可将MaxSim浮点运算次数减少5倍。

Conclusion: 密集后期交互评分存在大量冗余，可在查询时有效识别和修剪。

Abstract: Multi-vector late-interaction retrievers such as ColBERT achieve state-of-the-art retrieval quality, but their query-time cost is dominated by exhaustively computing token-level MaxSim interactions for every candidate document. While approximating late interaction with single-vector representations reduces cost, it often incurs substantial accuracy loss. We introduce Col-Bandit, a query-time pruning algorithm that reduces this computational burden by casting reranking as a finite-population Top-$K$ identification problem. Col-Bandit maintains uncertainty-aware bounds over partially observed document scores and adaptively reveals only the (document, query token) MaxSim entries needed to determine the top results under statistical decision bounds with a tunable relaxation. Unlike coarse-grained approaches that prune entire documents or tokens offline, Col-Bandit sparsifies the interaction matrix on the fly. It operates as a zero-shot, drop-in layer over standard multi-vector systems, requiring no index modifications, offline preprocessing, or model retraining. Experiments on textual (BEIR) and multimodal (REAL-MM-RAG) benchmarks show that Col-Bandit preserves ranking fidelity while reducing MaxSim FLOPs by up to 5$\times$, indicating that dense late-interaction scoring contains substantial redundancy that can be identified and pruned efficiently at query time.

</details>


### [96] [Efficiency Optimizations for Superblock-based Sparse Retrieval](https://arxiv.org/abs/2602.02883)
*Parker Carlson,Wentai Xie,Rohil Shah,Tao Yang*

Main category: cs.IR

TL;DR: 本文提出简单有效的超块剪枝方案用于高效稀疏检索，在多数据集上验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 减少超块分数计算开销，同时保持检索相关性。

Method: 提出简单有效的超块剪枝方案，并结合紧凑索引结构和零样本配置。

Result: 在MS MARCO和BEIR数据集上分析和评估，证明了方案的可行性。

Conclusion: 该方案可作为高效稀疏检索的有力替代方法。

Abstract: Learned sparse retrieval (LSR) is a popular method for first-stage retrieval because it combines the semantic matching of language models with efficient CPU-friendly algorithms. Previous work aggregates blocks into "superblocks" to quickly skip the visitation of blocks during query processing by using an advanced pruning heuristic. This paper proposes a simple and effective superblock pruning scheme that reduces the overhead of superblock score computation while preserving competitive relevance. It combines this scheme with a compact index structure and a robust zero-shot configuration that is effective across LSR models and multiple datasets. This paper provides an analytical justification and evaluation on the MS MARCO and BEIR datasets, demonstrating that the proposed scheme can be a strong alternative for efficient sparse retrieval.

</details>


### [97] [ALPBench: A Benchmark for Attribution-level Long-term Personal Behavior Understanding](https://arxiv.org/abs/2602.03056)
*Lu Ren,Junda She,Xinchen Luo,Tao Wang,Xin Ye,Xu Zhang,Muxuan Wang,Xiao Yang,Chenguang Wang,Fei Xie,Yiwei Zhou,Danjun Wu,Guodong Zhang,Yifei Hu,Guoying Zheng,Shujie Yang,Xingmei Wang,Shiyao Wang,Yukun Zhou,Fan Yang,Size Li,Kuo Cai,Qiang Luo,Ruiming Tang,Han Li,Kun Gai*

Main category: cs.IR

TL;DR: 介绍用于归因级长期个人行为理解的基准ALPBench，可对大语言模型在个性化推荐中建模长期用户行为能力进行评估。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在个性化推荐中有潜力，但准确捕捉用户偏好是挑战，需要系统评估其建模长期用户行为的能力。

Method: 引入归因级长期个人行为理解基准ALPBench，预测用户感兴趣的属性组合，用自然语言序列表示用户历史。

Result: 使能对个性化进行细粒度评估，专注的属性组合预测任务能反映当前大语言模型面临的挑战。

Conclusion: ALPBench可用于系统评估大语言模型在个性化推荐中建模长期用户行为的能力。

Abstract: Recent advances in large language models have highlighted their potential for personalized recommendation, where accurately capturing user preferences remains a key challenge. Leveraging their strong reasoning and generalization capabilities, LLMs offer new opportunities for modeling long-term user behavior. To systematically evaluate this, we introduce ALPBench, a Benchmark for Attribution-level Long-term Personal Behavior Understanding. Unlike item-focused benchmarks, ALPBench predicts user-interested attribute combinations, enabling ground-truth evaluation even for newly introduced items. It models preferences from long-term historical behaviors rather than users' explicitly expressed requests, better reflecting enduring interests. User histories are represented as natural language sequences, allowing interpretable, reasoning-based personalization. ALPBench enables fine-grained evaluation of personalization by focusing on the prediction of attribute combinations task that remains highly challenging for current LLMs due to the need to capture complex interactions among multiple attributes and reason over long-term user behavior sequences.

</details>


### [98] [PAMAS: Self-Adaptive Multi-Agent System with Perspective Aggregation for Misinformation Detection](https://arxiv.org/abs/2602.03158)
*Zongwei Wang,Min Gao,Junliang Yu,Tong Chen,Chenghua Lin*

Main category: cs.IR

TL;DR: 提出PAMAS多智能体框架解决社交媒体错误信息检测中信息淹没问题，实验显示其有高准确率和效率。


<details>
  <summary>Details</summary>
Motivation: 社交媒体错误信息检测困难，传统多智能体系统存在信息淹没问题，需新方法。

Method: 提出PAMAS框架，组织智能体为三种角色，采用分层、视角感知聚合，还加入自适应机制。

Result: 在多个基准数据集上实验，PAMAS实现了更高的准确率和效率。

Conclusion: PAMAS为错误信息检测提供了可扩展且可靠的方法。

Abstract: Misinformation on social media poses a critical threat to information credibility, as its diverse and context-dependent nature complicates detection. Large language model-empowered multi-agent systems (MAS) present a promising paradigm that enables cooperative reasoning and collective intelligence to combat this threat. However, conventional MAS suffer from an information-drowning problem, where abundant truthful content overwhelms sparse and weak deceptive cues. With full input access, agents tend to focus on dominant patterns, and inter-agent communication further amplifies this bias. To tackle this issue, we propose PAMAS, a multi-agent framework with perspective aggregation, which employs hierarchical, perspective-aware aggregation to highlight anomaly cues and alleviate information drowning. PAMAS organizes agents into three roles: Auditors, Coordinators, and a Decision-Maker. Auditors capture anomaly cues from specialized feature subsets; Coordinators aggregate their perspectives to enhance coverage while maintaining diversity; and the Decision-Maker, equipped with evolving memory and full contextual access, synthesizes all subordinate insights to produce the final judgment. Furthermore, to improve efficiency in multi-agent collaboration, PAMAS incorporates self-adaptive mechanisms for dynamic topology optimization and routing-based inference, enhancing both efficiency and scalability. Extensive experiments on multiple benchmark datasets demonstrate that PAMAS achieves superior accuracy and efficiency, offering a scalable and trustworthy way for misinformation detection.

</details>


### [99] [Distribution-Aware End-to-End Embedding for Streaming Numerical Features in Click-Through Rate Prediction](https://arxiv.org/abs/2602.03223)
*Jiahao Liu,Hongji Ruan,Weimin Zhang,Ziye Tong,Derick Tang,Zhanpeng Zeng,Qinsong Zeng,Peng Zhang,Tun Lu,Ning Gu*

Main category: cs.IR

TL;DR: 本文提出DAES框架解决流式环境下点击率预测的数值特征嵌入问题，实验表现优异并已部署到短视频平台。


<details>
  <summary>Details</summary>
Motivation: 传统静态分箱方法更新边界时会引发语义漂移，神经嵌入方法会丢弃分布信息，且流式特征违反i.i.d.假设，难以估计总体分布，同时数值分布的上下文依赖性常被忽略。

Method: 提出DAES框架，引入基于水库采样的分布估计方法和两种场感知分布调制策略。

Result: 通过大量离线和在线实验，DAES显著优于现有方法。

Conclusion: DAES能有效解决流式训练场景中的数值特征嵌入问题，已在短视频平台全面部署。

Abstract: This paper explores effective numerical feature embedding for Click-Through Rate prediction in streaming environments. Conventional static binning methods rely on offline statistics of numerical distributions; however, this inherently two-stage process often triggers semantic drift during bin boundary updates. While neural embedding methods enable end-to-end learning, they often discard explicit distributional information. Integrating such information end-to-end is challenging because streaming features often violate the i.i.d. assumption, precluding unbiased estimation of the population distribution via the expectation of order statistics. Furthermore, the critical context dependency of numerical distributions is often neglected. To this end, we propose DAES, an end-to-end framework designed to tackle numerical feature embedding in streaming training scenarios by integrating distributional information with an adaptive modulation mechanism. Specifically, we introduce an efficient reservoir-sampling-based distribution estimation method and two field-aware distribution modulation strategies to capture streaming distributions and field-dependent semantics. DAES significantly outperforms existing approaches as demonstrated by extensive offline and online experiments and has been fully deployed on a leading short-video platform with hundreds of millions of daily active users.

</details>


### [100] [To Search or Not to Search: Aligning the Decision Boundary of Deep Search Agents via Causal Intervention](https://arxiv.org/abs/2602.03304)
*Wenlin Zhang,Kuicai Dong,Junyi Li,Yingyi Zhang,Xiaopeng Li,Pengyue Jia,Yi Wen,Derong Xu,Maolin Wang,Yichao Wang,Yong Liu,Xiangyu Zhao*

Main category: cs.IR

TL;DR: 当前深度搜索代理存在效率问题，本文提出含因果干预诊断和DAS的框架解决决策边界错误，实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 当前深度搜索代理因以结果为中心的训练，存在无法准确判断搜索停止时机的效率问题。

Method: 提出包含因果干预诊断和决策边界对齐（DAS）的综合框架。

Result: 实验表明决策边界错误在现有代理中普遍存在，DAS方法有效校准边界，提升准确性和效率。

Conclusion: 所提方法能有效解决深度搜索代理决策边界错误问题，提高搜索准确性和效率。

Abstract: Deep search agents, which autonomously iterate through multi-turn web-based reasoning, represent a promising paradigm for complex information-seeking tasks. However, current agents suffer from critical inefficiency: they conduct excessive searches as they cannot accurately judge when to stop searching and start answering. This stems from outcome-centric training that prioritize final results over the search process itself. We identify the root cause as misaligned decision boundaries, the threshold determining when accumulated information suffices to answer. This causes over-search (redundant searching despite sufficient knowledge) and under-search (premature termination yielding incorrect answers). To address these errors, we propose a comprehensive framework comprising two key components. First, we introduce causal intervention-based diagnosis that identifies boundary errors by comparing factual and counterfactual trajectories at each decision point. Second, we develop Decision Boundary Alignment for Deep Search agents (DAS), which constructs preference datasets from causal feedback and aligns policies via preference optimization. Experiments on public datasets demonstrate that decision boundary errors are pervasive across state-of-the-art agents. Our DAS method effectively calibrates these boundaries, mitigating both over-search and under-search to achieve substantial gains in accuracy and efficiency. Our code and data are publicly available at: https://github.com/Applied-Machine-Learning-Lab/WWW2026_DAS.

</details>


### [101] [Learning to Select: Query-Aware Adaptive Dimension Selection for Dense Retrieval](https://arxiv.org/abs/2602.03306)
*Zhanyu Wu,Richong Zhang,Zhijie Nie*

Main category: cs.IR

TL;DR: 提出查询感知自适应维度选择框架，直接从查询嵌入中学习预测每个维度重要性，实验证明该框架能提升检索效果


<details>
  <summary>Details</summary>
Motivation: 现有基于伪相关反馈的维度重要性估计依赖噪声伪信号和启发式测试程序，有监督适配器方法学习全局转换，未显式建模查询感知维度重要性

Method: 构建基于监督相关性标签的嵌入维度重要性分布，训练预测器将查询嵌入映射到标签提炼的重要性得分，推理时基于查询嵌入选择查询感知维度子集进行相似度计算

Result: 在多个密集检索器和基准测试中，所提出的维度选择器比全维度基线、基于PRF的掩码和有监督适配器基线都提高了检索效果

Conclusion: 所提出的查询感知自适应维度选择框架能有效提升检索有效性

Abstract: Dense retrieval represents queries and docu-002 ments as high-dimensional embeddings, but003 these representations can be redundant at the004 query level: for a given information need, only005 a subset of dimensions is consistently help-006 ful for ranking. Prior work addresses this via007 pseudo-relevance feedback (PRF) based dimen-008 sion importance estimation, which can produce009 query-aware masks without labeled data but010 often relies on noisy pseudo signals and heuris-011 tic test-time procedures. In contrast, super-012 vised adapter methods leverage relevance labels013 to improve embedding quality, yet they learn014 global transformations shared across queries015 and do not explicitly model query-aware di-016 mension importance. We propose a Query-017 Aware Adaptive Dimension Selection frame-018 work that learns to predict per-dimension im-019 portance directly from query embedding. We020 first construct oracle dimension importance dis-021 tributions over embedding dimensions using022 supervised relevance labels, and then train a023 predictor to map a query embedding to these024 label-distilled importance scores. At inference,025 the predictor selects a query-aware subset of026 dimensions for similarity computation based027 solely on the query embedding, without pseudo-028 relevance feedback. Experiments across multi-029 ple dense retrievers and benchmarks show that030 our learned dimension selector improves re-031 trieval effectiveness over the full-dimensional032 baseline as well as PRF-based masking and033 supervised adapter baselines.

</details>


### [102] [SCASRec: A Self-Correcting and Auto-Stopping Model for Generative Route List Recommendation](https://arxiv.org/abs/2602.03324)
*Chao Chen,Longfei Xu,Daohan Su,Tengfei Liu,Hanyu Guo,Yihai Duan,Kaikui Liu,Xiangxiang Chu*

Main category: cs.IR

TL;DR: 传统路线推荐系统多阶段管道有局限，提出SCASRec统一生成框架，实验表现优且已部署应用。


<details>
  <summary>Details</summary>
Motivation: 解决传统路线推荐系统中离线训练目标与在线指标不一致、冗余消除规则缺乏适应性、精排和重排阶段分离导致性能不佳等问题。

Method: 提出SCASRec统一生成框架，将排序和冗余消除集成到端到端过程，引入SCR引导列表优化，使用EOR令牌自适应终止生成。

Result: 在两个大规模开源路线推荐数据集实验中，SCASRec在离线和在线设置中达到SOTA，且已在真实导航应用中全面部署。

Conclusion: SCASRec有效克服传统路线推荐系统的局限性，具备实际应用价值。

Abstract: Route recommendation systems commonly adopt a multi-stage pipeline involving fine-ranking and re-ranking to produce high-quality ordered recommendations. However, this paradigm faces three critical limitations. First, there is a misalignment between offline training objectives and online metrics. Offline gains do not necessarily translate to online improvements. Actual performance must be validated through A/B testing, which may potentially compromise the user experience. Second, redundancy elimination relies on rigid, handcrafted rules that lack adaptability to the high variance in user intent and the unstructured complexity of real-world scenarios. Third, the strict separation between fine-ranking and re-ranking stages leads to sub-optimal performance. Since each module is optimized in isolation, the fine-ranking stage remains oblivious to the list-level objectives (e.g., diversity) targeted by the re-ranker, thereby preventing the system from achieving a jointly optimized global optimum. To overcome these intertwined challenges, we propose \textbf{SCASRec} (\textbf{S}elf-\textbf{C}orrecting and \textbf{A}uto-\textbf{S}topping \textbf{Rec}ommendation), a unified generative framework that integrates ranking and redundancy elimination into a single end-to-end process. SCASRec introduces a stepwise corrective reward (SCR) to guide list-wise refinement by focusing on hard samples, and employs a learnable End-of-Recommendation (EOR) token to terminate generation adaptively when no further improvement is expected. Experiments on two large-scale, open-sourced route recommendation datasets demonstrate that SCASRec establishes an SOTA in offline and online settings. SCASRec has been fully deployed in a real-world navigation app, demonstrating its effectiveness.

</details>


### [103] [Beyond Exposure: Optimizing Ranking Fairness with Non-linear Time-Income Functions](https://arxiv.org/abs/2602.03345)
*Xuancheng Li,Tao Yang,Yujia Zhou,Qingyao Ai,Yiqun Liu*

Main category: cs.IR

TL;DR: 文章指出当前曝光公平性概念的局限，提出收入公平性定义与度量指标，设计DIDRF算法优化效果和收入公平性，且表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有曝光公平性概念主要依赖位置确定曝光，忽略了时间等影响收入的因素，需研究考虑其他上下文因素的排名公平性。

Method: 给出收入公平性的正式定义和测量指标，提出基于当前时间步边际收入增益、用泰勒展开梯度同时优化效果和收入公平性的DIDRF算法。

Result: 模拟实验表明现有基于曝光公平性的排名算法无法优化提出的收入公平性；在不同时间 - 收入函数的离线和在线设置中，DIDRF算法始终优于现有方法。

Conclusion: DIDRF算法能有效同时优化排名的有效性和收入公平性。

Abstract: Ranking is central to information distribution in web search and recommendation. Nowadays, in ranking optimization, the fairness to item providers is viewed as a crucial factor alongside ranking relevance for users. There are currently numerous concepts of fairness and one widely recognized fairness concept is Exposure Fairness. However, it relies primarily on exposure determined solely by position, overlooking other factors that significantly influence income, such as time. To address this limitation, we propose to study ranking fairness when the provider utility is influenced by other contextual factors and is neither equal to nor proportional to item exposure. We give a formal definition of Income Fairness and develop a corresponding measurement metric. Simulated experiments show that existing-exposure-fairness-based ranking algorithms fail to optimize the proposed income fairness. Therefore, we propose the Dynamic-Income-Derivative-aware Ranking Fairness algorithm, which, based on the marginal income gain at the present timestep, uses Taylor-expansion-based gradients to simultaneously optimize effectiveness and income fairness. In both offline and online settings with diverse time-income functions, DIDRF consistently outperforms state-of-the-art methods.

</details>


### [104] [AesRec: A Dataset for Aesthetics-Aligned Clothing Outfit Recommendation](https://arxiv.org/abs/2602.03416)
*Wenxin Ye,Lin Li,Ming Li,Yang Shen,Kanghong Wang,Jimmy Xiangji Huang*

Main category: cs.IR

TL;DR: 为解决现有服装推荐方法忽视审美表达问题，提出含定量审美标注的AesRec数据集，利用视觉语言模型评分，实验证明整合审美信息可在满足个性化需求时提供审美指导。


<details>
  <summary>Details</summary>
Motivation: 现有服装推荐方法主要依赖用户 - 物品 - 穿搭交互行为，忽略了服装美学的明确表达。

Method: 提出AesRec基准数据集，依据专业服装标准和时尚审美原则定义多维指标，在单品和穿搭层面分别评估，利用视觉语言模型进行大规模美学评分，并进行人机一致性验证。

Result: 在时尚数据集上验证了生成评分的可靠性，基于AesRec的实验表明，将量化的审美信息整合到服装推荐模型中可在满足用户个性化需求的同时提供审美指导。

Conclusion: 整合量化的审美信息到服装推荐模型中是可行且有效的，能为用户提供审美指导。

Abstract: Clothing recommendation extends beyond merely generating personalized outfits; it serves as a crucial medium for aesthetic guidance. However, existing methods predominantly rely on user-item-outfit interaction behaviors while overlooking explicit representations of clothing aesthetics. To bridge this gap, we present the AesRec benchmark dataset featuring systematic quantitative aesthetic annotations, thereby enabling the development of aesthetics-aligned recommendation systems. Grounded in professional apparel quality standards and fashion aesthetic principles, we define a multidimensional set of indicators. At the item level, six dimensions are independently assessed: silhouette, chromaticity, materiality, craftsmanship, wearability, and item-level impression. Transitioning to the outfit level, the evaluation retains the first five core attributes while introducing stylistic synergy, visual harmony, and outfit-level impression as distinct metrics to capture the collective aesthetic impact. Given the increasing human-like proficiency of Vision-Language Models in multimodal understanding and interaction, we leverage them for large-scale aesthetic scoring. We conduct rigorous human-machine consistency validation on a fashion dataset, confirming the reliability of the generated ratings. Experimental results based on AesRec further demonstrate that integrating quantified aesthetic information into clothing recommendation models can provide aesthetic guidance for users while fulfilling their personalized requirements.

</details>


### [105] [RankSteer: Activation Steering for Pointwise LLM Ranking](https://arxiv.org/abs/2602.03422)
*Yumeng Wang,Catherine Chen,Suzan Verberne*

Main category: cs.IR

TL;DR: 提出RankSteer框架用于零样本逐点大语言模型排序，通过实验证明其有效性并进行几何分析。


<details>
  <summary>Details</summary>
Motivation: 大语言模型作为零样本排序器的有效性对提示公式敏感，先前分析提出可在激活层控制排序行为，因此尝试提出新框架。

Method: 提出RankSteer，在表示空间中确定三个可解耦和可控的方向，使用基于投影的干预方法在推理时联合控制这些方向。

Result: 在TREC DL 20和多个BEIR基准测试中，RankSteer仅用少量锚定查询就持续提高了排序质量。

Conclusion: 逐点大语言模型排序器中有大量排序能力未被充分利用，转向通过稳定排序几何形状和减少分散性来提高排序效果，为理解大语言模型内部表示和校准相关性判断提供新见解。

Abstract: Large language models (LLMs) have recently shown strong performance as zero-shot rankers, yet their effectiveness is highly sensitive to prompt formulation, particularly role-play instructions. Prior analyses suggest that role-related signals are encoded along activation channels that are largely separate from query-document representations, raising the possibility of steering ranking behavior directly at the activation level rather than through brittle prompt engineering. In this work, we propose RankSteer, a post-hoc activation steering framework for zero-shot pointwise LLM ranking. We characterize ranking behavior through three disentangled and steerable directions in representation space: a \textbf{decision direction} that maps hidden states to relevance scores, an \textbf{evidence direction} that captures relevance signals not directly exploited by the decision head, and a \textbf{role direction} that modulates model behavior without injecting relevance information. Using projection-based interventions at inference time, RankSteer jointly controls these directions to calibrate ranking behavior without modifying model weights or introducing explicit cross-document comparisons. Experiments on TREC DL 20 and multiple BEIR benchmarks show that RankSteer consistently improves ranking quality using only a small number of anchor queries, demonstrating that substantial ranking capacity remains under-utilized in pointwise LLM rankers. We further provide a geometric analysis revealing that steering improves ranking by stabilizing ranking geometry and reducing dispersion, offering new insight into how LLMs internally represent and calibrate relevance judgments.

</details>


### [106] [Failure is Feedback: History-Aware Backtracking for Agentic Traversal in Multimodal Graphs](https://arxiv.org/abs/2602.03432)
*Joohyung Yun,Doyup Lee,Wook-Shin Han*

Main category: cs.IR

TL;DR: 提出FiF方法用于开放域多模态文档检索，有历史感知回溯机制和经济理性代理工作流，实验达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有基于图的检索方法存在忽略跳特定语义、难以动态纠错的局限，需根据上下文调整推理并从错误中恢复。

Method: 将子图检索视为顺序决策过程，引入历史感知回溯机制和经济理性代理工作流。

Result: FiF在MultimodalQA、MMCoQA和WebQA基准测试中实现了最先进的检索效果。

Conclusion: FiF方法有效提升了开放域多模态文档检索性能。

Abstract: Open-domain multimodal document retrieval aims to retrieve specific components (paragraphs, tables, or images) from large and interconnected document corpora. Existing graph-based retrieval approaches typically rely on a uniform similarity metric that overlooks hop-specific semantics, and their rigid pre-defined plans hinder dynamic error correction. These limitations suggest that a retriever should adapt its reasoning to the evolving context and recover intelligently from dead ends. To address these needs, we propose Failure is Feedback (FiF), which casts subgraph retrieval as a sequential decision process and introduces two key innovations. (i) We introduce a history-aware backtracking mechanism; unlike standard backtracking that simply reverts the state, our approach piggybacks on the context of failed traversals, leveraging insights from previous failures. (ii) We implement an economically-rational agentic workflow. Unlike conventional agents with static strategies, our orchestrator employs a cost-aware traversal method to dynamically manage the trade-off between retrieval accuracy and inference costs, escalating to intensive LLM-based reasoning only when the prior failure justifies the additional computational investment. Extensive experiments show that FiF achieves state-of-the-art retrieval on the benchmarks of MultimodalQA, MMCoQA and WebQA.

</details>


### [107] [Tutorial on Reasoning for IR & IR for Reasoning](https://arxiv.org/abs/2602.03640)
*Mohanna Hoveyda,Panagiotis Efstratiadis,Arjen de Vries,Maarten de Rijke*

Main category: cs.IR

TL;DR: 本文旨在帮助信息检索（IR）领域研究者梳理推理研究，提出统一分析框架，介绍现有方法权衡与互补，助力提升IR系统推理能力。


<details>
  <summary>Details</summary>
Motivation: 现实信息需求需推理能力，但推理研究分散，IR研究者难识别相关想法和机会。

Method: 给出信息检索中推理的工作定义，由此导出统一分析框架，将现有方法映射到框架轴上。

Result: 揭示现有方法的权衡与互补，指出IR可从跨学科进展中获益，说明检索过程在推理系统中的核心作用。

Conclusion: 为参与者提供概念框架和实践指导，使IR既受益于又促进推理方法的发展。

Abstract: Information retrieval has long focused on ranking documents by semantic relatedness. Yet many real-world information needs demand more: enforcement of logical constraints, multi-step inference, and synthesis of multiple pieces of evidence. Addressing these requirements is, at its core, a problem of reasoning. Across AI communities, researchers are developing diverse solutions for the problem of reasoning, from inference-time strategies and post-training of LLMs, to neuro-symbolic systems, Bayesian and probabilistic frameworks, geometric representations, and energy-based models. These efforts target the same problem: to move beyond pattern-matching systems toward structured, verifiable inference. However, they remain scattered across disciplines, making it difficult for IR researchers to identify the most relevant ideas and opportunities. To help navigate the fragmented landscape of research in reasoning, this tutorial first articulates a working definition of reasoning within the context of information retrieval and derives from it a unified analytical framework. The framework maps existing approaches along axes that reflect the core components of the definition. By providing a comprehensive overview of recent approaches and mapping current methods onto the defined axes, we expose their trade-offs and complementarities, highlight where IR can benefit from cross-disciplinary advances, and illustrate how retrieval process itself can play a central role in broader reasoning systems. The tutorial will equip participants with both a conceptual framework and practical guidance for enhancing reasoning-capable IR systems, while situating IR as a domain that both benefits and contributes to the broader development of reasoning methodologies.

</details>


### [108] [Bringing Reasoning to Generative Recommendation Through the Lens of Cascaded Ranking](https://arxiv.org/abs/2602.03692)
*Xinyu Lin,Pengyuan Liu,Wenjie Wang,Yicheng Hu,Chen Xu,Fuli Feng,Qifan Wang,Tat-Seng Chua*

Main category: cs.IR

TL;DR: 当前生成式推荐（GR）模型存在偏差放大问题，本文提出CARE框架解决该问题，实验显示其有优势。


<details>
  <summary>Details</summary>
Motivation: 解决当前GR模型存在的偏差放大问题，提升推荐多样性和用户体验。

Method: 提出CARE框架，包括渐进式历史编码机制以整合异构信息，查询锚定推理机制来分配更多计算资源。

Result: 在四个数据集的三个GR骨干模型上实验，结果显示CARE在推荐准确性、多样性、效率和可扩展性方面表现优越。

Conclusion: CARE是一种简单有效的去偏GR级联推理框架，能有效解决偏差放大问题。

Abstract: Generative Recommendation (GR) has become a promising end-to-end approach with high FLOPS utilization for resource-efficient recommendation. Despite the effectiveness, we show that current GR models suffer from a critical \textbf{bias amplification} issue, where token-level bias escalates as token generation progresses, ultimately limiting the recommendation diversity and hurting the user experience. By comparing against the key factor behind the success of traditional multi-stage pipelines, we reveal two limitations in GR that can amplify the bias: homogeneous reliance on the encoded history, and fixed computational budgets that prevent deeper user preference understanding.
  To combat the bias amplification issue, it is crucial for GR to 1) incorporate more heterogeneous information, and 2) allocate greater computational resources at each token generation step. To this end, we propose CARE, a simple yet effective cascaded reasoning framework for debiased GR. To incorporate heterogeneous information, we introduce a progressive history encoding mechanism, which progressively incorporates increasingly fine-grained history information as the generation process advances. To allocate more computations, we propose a query-anchored reasoning mechanism, which seeks to perform a deeper understanding of historical information through parallel reasoning steps. We instantiate CARE on three GR backbones. Empirical results on four datasets show the superiority of CARE in recommendation accuracy, diversity, efficiency, and promising scalability. The codes and datasets are available at https://github.com/Linxyhaha/CARE.

</details>


### [109] [Multimodal Generative Recommendation for Fusing Semantic and Collaborative Signals](https://arxiv.org/abs/2602.03713)
*Moritz Vandenhirtz,Kaveh Hassani,Shervin Ghasemlou,Shuai Shao,Hamid Eghbalzadeh,Fuchun Peng,Jun Liu,Michael Louis Iuzzolino*

Main category: cs.IR

TL;DR: 提出MSCGRec推荐器在三个大数据集上表现超越基线，并进行消融实验。


<details>
  <summary>Details</summary>
Motivation: 生成式推荐方法在大规模物品集上无法超越传统序贯推荐系统，限制其应用。

Method: 提出MSCGRec，融合多模态语义，引入基于DINO的自监督量化学习，融合协同和语义信号，采用约束序列学习。

Result: 在三个大型真实数据集上MSCGRec超越序贯和生成推荐基线。

Conclusion: MSCGRec在大型物品集上表现良好，通过消融实验验证各组件影响。

Abstract: Sequential recommender systems rank relevant items by modeling a user's interaction history and computing the inner product between the resulting user representation and stored item embeddings. To avoid the significant memory overhead of storing large item sets, the generative recommendation paradigm instead models each item as a series of discrete semantic codes. Here, the next item is predicted by an autoregressive model that generates the code sequence corresponding to the predicted item. However, despite promising ranking capabilities on small datasets, these methods have yet to surpass traditional sequential recommenders on large item sets, limiting their adoption in the very scenarios they were designed to address. To resolve this, we propose MSCGRec, a Multimodal Semantic and Collaborative Generative Recommender. MSCGRec incorporates multiple semantic modalities and introduces a novel self-supervised quantization learning approach for images based on the DINO framework. Additionally, MSCGRec fuses collaborative and semantic signals by extracting collaborative features from sequential recommenders and treating them as a separate modality. Finally, we propose constrained sequence learning that restricts the large output space during training to the set of permissible tokens. We empirically demonstrate on three large real-world datasets that MSCGRec outperforms both sequential and generative recommendation baselines and provide an extensive ablation study to validate the impact of each component.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [110] [UNSO: Unified Newton Schulz Orthogonalization](https://arxiv.org/abs/2602.02500)
*Chen Hu,Qianxi Zhao,Yuming Li,Mingyu Zhou,Xiyin Li*

Main category: cs.LG

TL;DR: 针对传统牛顿 - 舒尔茨（NS）迭代效率低和不稳定问题，提出统一牛顿 - 舒尔茨正交化（UNSO）框架，优化学习系数后收敛稳定且性能出色。


<details>
  <summary>Details</summary>
Motivation: 传统 NS 迭代效率低、不稳定，现有改进未脱离传统迭代范式，会因反复长维度矩阵相乘增加计算负担。

Method: 将迭代结构整合为统一框架 UNSO，评估各矩阵幂的作用，去除不重要项，提供带可学习系数的推荐多项式并优化系数。

Result: 通过优化可学习系数，实现稳定收敛并取得出色性能。

Conclusion: 提出的 UNSO 框架能有效解决传统 NS 迭代的问题，代码已开源。

Abstract: The Newton-Schulz (NS) iteration has gained increasing interest for its role in the Muon optimizer and the Stiefel manifold. However, the conventional NS iteration suffers from inefficiency and instability. Although various improvements have been introduced to NS iteration, they fail to deviate from the conventional iterative paradigm, which could increase computation burden largely due to the matrix products along the long dimension repeatedly. To address this, we consolidate the iterative structure into a unified framework, named Unified Newton-Schulz Orthogonalization (UNSO). To do so, we could avoid a polynomial expansion. Instead, we evaluate the role of each matrix power, remove the insignificant terms, and provide a recommended polynomial with learnable coefficients. These learnable coefficients are then optimized, and achieve an outstanding performance with stable convergence. The code of our method is available: https://github.com/greekinRoma/Unified_Newton_Schulz_Orthogonalization.

</details>


### [111] [Augmenting Parameter-Efficient Pre-trained Language Models with Large Language Models](https://arxiv.org/abs/2602.02501)
*Saurabh Anand,Shubham Malaviya,Manish Shukla,Sachin Lodha*

Main category: cs.LG

TL;DR: 本文提出结合参数高效预训练模型与大语言模型的策略，解决网络安全AI模型训练挑战，实验证明可提升模型可靠性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 网络安全AI模型训练存在数据漂移、标注数据稀缺等问题，导致模型需频繁更新和过拟合风险。

Method: 采用预训练语言模型的参数高效微调技术，结合compacters和各种层冻结策略；引入两种利用大语言模型的策略，一是作为数据标注工具，二是作为低置信度预测的后备机制。

Result: 在网络安全领域不同下游任务上进行综合实验分析。

Conclusion: 结合参数高效预训练模型与大语言模型能提高模型可靠性和鲁棒性，更适合现实网络安全应用。

Abstract: Training AI models in cybersecurity with help of vast datasets offers significant opportunities to mimic real-world behaviors effectively. However, challenges like data drift and scarcity of labelled data lead to frequent updates of models and the risk of overfitting. To address these challenges, we used parameter-efficient fine-tuning techniques for pre-trained language models wherein we combine compacters with various layer freezing strategies. To enhance the capabilities of these pre-trained language models, in this work we introduce two strategies that use large language models. In the first strategy, we utilize large language models as data-labelling tools wherein they generate labels for unlabeled data. In the second strategy, large language modes are utilized as fallback mechanisms for predictions having low confidence scores. We perform comprehensive experimental analysis on the proposed strategies on different downstream tasks specific to cybersecurity domain. We empirically demonstrate that by combining parameter-efficient pre-trained models with large language models, we can improve the reliability and robustness of models, making them more suitable for real-world cybersecurity applications.

</details>


### [112] [Sparse Adapter Fusion for Continual Learning in NLP](https://arxiv.org/abs/2602.02502)
*Min Zeng,Xi Chen,Haiqin Yang,Yike Guo*

Main category: cs.LG

TL;DR: 提出稀疏适配器融合方法（SAFM）解决自然语言处理持续学习的现存问题，该方法分决策和调整两个阶段，实验显示其表现优于现有技术，且参数使用量少。


<details>
  <summary>Details</summary>
Motivation: 现有自然语言处理持续学习方法存在参数复用效率低、易灾难性遗忘、新任务引入不必要新参数等问题。

Method: 提出SAFM方法，包括决策阶段（确定引入新适配器、复用旧适配器或添加空适配器）和调整阶段（使用层损失鼓励适配器区分）。

Result: SAFM实验表现优于现有技术，在使用不到60%参数时达到了相当的性能。

Conclusion: SAFM能有效解决目前持续学习方法的问题，提升参数复用率。

Abstract: Continual learning in natural language processing plays a crucial role in adapting to evolving data and preventing catastrophic forgetting. Despite significant progress, existing methods still face challenges, such as inefficient parameter reuse across tasks, risking catastrophic forgetting when tasks are dissimilar, and the unnecessary introduction of new parameters for each task, which hampers knowledge sharing among similar tasks. To tackle these issues, we propose a Sparse Adapter Fusion Method (SAFM), which dynamically fuses old and new adapters to address these challenges. SAFM operates in two stages: the decision stage and the tuning stage. In the decision stage, SAFM determines whether to incorporate a new adapter, reuse an existing one, or add an empty adapter. The architecture search procedure, designed to prioritize reusing or adding empty adapters, minimizes parameter consumption and maximizes reuse. In the tuning stage, SAFM especially facilitates a layer-wise loss to encourage differentiation between adapters, effectively capturing knowledge within the same task. Experimental results consistently show that SAFM outperforms state-of-the-art (SOTA) methods, achieving comparable performance while utilizing less than 60% of the parameters.

</details>


### [113] [When Single Answer Is Not Enough: Rethinking Single-Step Retrosynthesis Benchmarks for LLMs](https://arxiv.org/abs/2602.03554)
*Bogdan Zagribelnyy,Ivan Ilin,Maksim Kuznetsov,Nikita Bondarev,Roman Schutski,Thomas MacDougall,Rim Shayakhmetov,Zulfat Miftakhutdinov,Mikolaj Mizera,Vladimir Aladinskiy,Alex Aliper,Alex Zhavoronkov*

Main category: cs.LG

TL;DR: 提出新的单步逆合成基准框架，用ChemCensor评估多类大语言模型，引入CREED数据集训练模型提升性能


<details>
  <summary>Details</summary>
Motivation: 现有逆合成性能评估受限，依赖单一真实基准和Top - K准确率，无法反映实际合成开放性

Method: 提出新的基准框架，使用ChemCensor评估语言模型，引入CREED数据集训练模型

Result: 提出的方法能更好评估，训练的模型在基准测试中优于大语言模型基线

Conclusion: 强调合理性而非精确匹配的评估方法更符合人类合成规划实践

Abstract: Recent progress has expanded the use of large language models (LLMs) in drug discovery, including synthesis planning. However, objective evaluation of retrosynthesis performance remains limited. Existing benchmarks and metrics typically rely on published synthetic procedures and Top-K accuracy based on single ground-truth, which does not capture the open-ended nature of real-world synthesis planning. We propose a new benchmarking framework for single-step retrosynthesis that evaluates both general-purpose and chemistry-specialized LLMs using ChemCensor, a novel metric for chemical plausibility. By emphasizing plausibility over exact match, this approach better aligns with human synthesis planning practices. We also introduce CREED, a novel dataset comprising millions of ChemCensor-validated reaction records for LLM training, and use it to train a model that improves over the LLM baselines under this benchmark.

</details>


### [114] [Refining Decision Boundaries In Anomaly Detection Using Similarity Search Within the Feature Space](https://arxiv.org/abs/2602.02925)
*Sidahmed Benabderrahmane,Petko Valtchev,James Cheney,Talal Rahwan*

Main category: cs.LG

TL;DR: 本文提出SDA2E自编码器和相似性引导主动学习框架，在52个不平衡数据集上评估，表现优于15种先进方法，减少标注数据，适用于网络安全。


<details>
  <summary>Details</summary>
Motivation: 解决高度不平衡数据集中检测罕见多样异常的挑战，传统主动学习方法未充分利用特征空间几何结构。

Method: 引入SDA2E学习潜在表示，提出相似性引导主动学习框架，含三种策略及新相似度度量SIM_NM1。

Result: SDA2E排名性能优越，nDCG可达1.0，相比被动训练减少80%标注数据，统计测试验证改进显著。

Conclusion: 建立了适用于网络安全异常检测的强大、高效且经统计验证的框架。

Abstract: Detecting rare and diverse anomalies in highly imbalanced datasets-such as Advanced Persistent Threats (APTs) in cybersecurity-remains a fundamental challenge for machine learning systems. Active learning offers a promising direction by strategically querying an oracle to minimize labeling effort, yet conventional approaches often fail to exploit the intrinsic geometric structure of the feature space for model refinement. In this paper, we introduce SDA2E, a Sparse Dual Adversarial Attention-based AutoEncoder designed to learn compact and discriminative latent representations from imbalanced, high-dimensional data. We further propose a similarity-guided active learning framework that integrates three novel strategies to refine decision boundaries efficiently: mormal-like expansion, which enriches the training set with points similar to labeled normals to improve reconstruction fidelity; anomaly-like prioritization, which boosts ranking accuracy by focusing on points resembling known anomalies; and a hybrid strategy that combines both for balanced model refinement and ranking. A key component of our framework is a new similarity measure, Normalized Matching 1s (SIM_NM1), tailored for sparse binary embeddings. We evaluate SDA2E extensively across 52 imbalanced datasets, including multiple DARPA Transparent Computing scenarios, and benchmark it against 15 state-of-the-art anomaly detection methods. Results demonstrate that SDA2E consistently achieves superior ranking performance (nDCG up to 1.0 in several cases) while reducing the required labeled data by up to 80% compared to passive training. Statistical tests confirm the significance of these improvements. Our work establishes a robust, efficient, and statistically validated framework for anomaly detection that is particularly suited to cybersecurity applications such as APT detection.

</details>


### [115] [Learning ORDER-Aware Multimodal Representations for Composite Materials Design](https://arxiv.org/abs/2602.02513)
*Xinyao Li,Hangwei Qian,Jingjing Li,Ivor Tsang*

Main category: cs.LG

TL;DR: 提出ORDER多模态预训练框架用于复合材料表征，在多任务上优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有基于图的AI方法在复合材料上失效，现有多模态框架无法处理连续设计空间和数据稀缺问题。

Method: 引入ORDER多模态预训练框架，以序数作为复合材料表征的核心原则。

Result: 在公开和内部数据集上，ORDER在属性预测、跨模态检索和微观结构生成任务上均有持续改进。

Conclusion: ORDER框架有效，能保留复合材料属性的连续性，实现稀疏设计间的有意义插值。

Abstract: Artificial intelligence (AI) has shown remarkable success in materials discovery and property prediction, particularly for crystalline and polymer systems where material properties and structures are dominated by discrete graph representations. Such graph-central paradigm breaks down on composite materials, which possess continuous and nonlinear design spaces that lack well-defined graph structures. General composite descriptors, e.g., fiber volume and misalignment angle, cannot fully capture the fiber distributions that fundamentally determine microstructural characteristics, necessitating the integration of heterogeneous data sources through multimodal learning. Existing alignment-oriented multimodal frameworks have proven effective on abundant crystal or polymer data under discrete, unique graph-property mapping assumptions, but fail to address the highly continuous composite design space under extreme data scarcity. In this work, we introduce ORDinal-aware imagE-tabulaR alignment (ORDER), a multimodal pretraining framework that establishes ordinality as a core principle for composite material representations. ORDER ensures that materials with similar target properties occupy nearby regions in the latent space, which effectively preserves the continuous nature of composite properties and enables meaningful interpolation between sparsely observed designs. We evaluate ORDER on a public Nanofiber-enforced composite dataset and an internally curated dataset that simulates the construction of carbon fiber T700 with diverse fiber distributions. ORDER achieves consistent improvements over state-of-the-art multimodal baselines across property prediction, cross-modal retrieval, and microstructure generation tasks.

</details>


### [116] [RPG-AE: Neuro-Symbolic Graph Autoencoders with Rare Pattern Mining for Provenance-Based Anomaly Detection](https://arxiv.org/abs/2602.02929)
*Asif Tauhid,Sidahmed Benabderrahmane,Mohamad Altrabulsi,Ahamed Foisal,Talal Rahwan*

Main category: cs.LG

TL;DR: 本文提出结合图自编码器与稀有模式挖掘的神经符号异常检测框架，用于识别系统级溯源数据中类似APT的活动，在DARPA数据集上表现良好。


<details>
  <summary>Details</summary>
Motivation: APTs攻击隐蔽，难以检测，需有效方法识别系统级溯源数据中类似APT的活动。

Method: 先基于特征相似度用k近邻构建进程行为图，再用图自编码器学习正常关系结构，通过观察和重建图结构的偏差识别异常候选，集成稀有模式挖掘模块提升异常分数。

Result: 稀有模式增强在异常排名质量上比基线GAE有显著提升，单一统一模型优于基于上下文的检测器，性能与集成聚合方法相当。

Conclusion: 将基于图的表示学习与经典模式挖掘相结合，能提高基于溯源的安全异常检测的有效性和可解释性。

Abstract: Advanced Persistent Threats (APTs) are sophisticated, long-term cyberattacks that are difficult to detect because they operate stealthily and often blend into normal system behavior. This paper presents a neuro-symbolic anomaly detection framework that combines a Graph Autoencoder (GAE) with rare pattern mining to identify APT-like activities in system-level provenance data. Our approach first constructs a process behavioral graph using k-Nearest Neighbors based on feature similarity, then learns normal relational structure using a Graph Autoencoder. Anomaly candidates are identified through deviations between observed and reconstructed graph structure. To further improve detection, we integrate an rare pattern mining module that discovers infrequent behavioral co-occurrences and uses them to boost anomaly scores for processes exhibiting rare signatures. We evaluate the proposed method on the DARPA Transparent Computing datasets and show that rare-pattern boosting yields substantial gains in anomaly ranking quality over the baseline GAE. Compared with existing unsupervised approaches on the same benchmark, our single unified model consistently outperforms individual context-based detectors and achieves performance competitive with ensemble aggregation methods that require multiple separate detectors. These results highlight the value of coupling graph-based representation learning with classical pattern mining to improve both effectiveness and interpretability in provenance-based security anomaly detection.

</details>


### [117] [What Drives Length of Stay After Elective Spine Surgery? Insights from a Decade of Predictive Modeling](https://arxiv.org/abs/2602.02517)
*Ha Na Cho,Seungmin Jeong,Yawen Guo,Alexander Lopez,Hansen Bow,Kai Zheng*

Main category: cs.LG

TL;DR: 该系统综述总结预测择期脊柱手术后住院时间的计算方法，发现机器学习模型潜力大，但缺乏标准化和外部验证。


<details>
  <summary>Details</summary>
Motivation: 预测择期脊柱手术后住院时间对优化患者预后和医院资源使用至关重要，需总结相关计算方法。

Method: 按照PRISMA指南，系统搜索数据库，筛选符合条件的研究，由三位评审员独立筛选和提取数据。

Result: 1263项研究中29项符合标准，机器学习模型表现优于传统统计模型，常见预测因素包括年龄、共病等，外部验证和报告做法差异大。

Conclusion: 机器学习模型在预测择期脊柱手术后住院时间方面有很大潜力，有望改善出院计划和医院资源管理。

Abstract: Objective: Predicting length of stay after elective spine surgery is essential for optimizing patient outcomes and hospital resource use. This systematic review synthesizes computational methods used to predict length of stay in this patient population, highlighting model performance and key predictors. Methods: Following PRISMA guidelines, we systematically searched PubMed, Google Scholar, and ACM Digital Library for studies published between December 1st, 2015, and December 1st, 2024. Eligible studies applied statistical or machine learning models to predict length of stay for elective spine surgery patients. Three reviewers independently screened studies and extracted data. Results: Out of 1,263 screened studies, 29 studies met inclusion criteria. Length of stay was predicted as a continuous, binary, or percentile-based outcome. Models included logistic regression, random forest, boosting algorithms, and neural networks. Machine learning models consistently outperformed traditional statistical models, with AUCs ranging from 0.94 to 0.99. K-Nearest Neighbors and Naive Bayes achieved top performance in some studies. Common predictors included age, comorbidities (notably hypertension and diabetes), BMI, type and duration of surgery, and number of spinal levels. However, external validation and reporting practices varied widely across studies. Discussion: There is growing interest in artificial intelligence and machine learning in length of stay prediction, but lack of standardization and external validation limits clinical utility. Future studies should prioritize standardized outcome definitions and transparent reporting needed to advance real-world deployment. Conclusion: Machine learning models offer strong potential for length of stay prediction after elective spine surgery, highlighting their potential for improving discharge planning and hospital resource management.

</details>


### [118] [Contrastive Concept-Tree Search for LLM-Assisted Algorithm Discovery](https://arxiv.org/abs/2602.03132)
*Timothee Leleu,Sudeera Gunathilaka,Federico Ghimenti,Surya Ganguli*

Main category: cs.LG

TL;DR: 本文提出CCTS方法用于大语言模型辅助的算法发现，可提高搜索效率并生成可解释概念树。


<details>
  <summary>Details</summary>
Motivation: 目前如何最大程度利用大语言模型对可能程序空间的内部表示来提高性能仍是未解决的问题。

Method: 引入Contrastive Concept-Tree Search (CCTS)，从生成程序中提取分层概念表示，学习对比概念模型来指导父选择，通过似然比分数重新加权父节点。

Result: CCTS在开放式Erdős类型组合数学问题基准上提高了搜索效率，产生了可解释的特定任务概念树。分析表明主要收益来自学习避免哪些概念，在合成算法发现环境中验证了结果。

Conclusion: CCTS方法有助于提高大语言模型辅助算法发现的性能，主要通过学习避免无效概念来实现。

Abstract: Large language Model (LLM)-assisted algorithm discovery is an iterative, black-box optimization process over programs to approximatively solve a target task, where an LLM proposes candidate programs and an external evaluator provides task feedback. Despite intense recent research on the topic and promising results, how can the LLM internal representation of the space of possible programs be maximally exploited to improve performance is an open question. Here, we introduce Contrastive Concept-Tree Search (CCTS), which extracts a hierarchical concept representation from the generated programs and learns a contrastive concept model that guides parent selection. By reweighting parents using a likelihood-ratio score between high- and low-performing solutions, CCTS biases search toward useful concept combinations and away from misleading ones, providing guidance through an explicit concept hierarchy rather than the algorithm lineage constructed by the LLM. We show that CCTS improves search efficiency over fitness-based baselines and produces interpretable, task-specific concept trees across a benchmark of open Erdős-type combinatorics problems. Our analysis indicates that the gains are driven largely by learning which concepts to avoid. We further validate these findings in a controlled synthetic algorithm-discovery environment, which reproduces qualitatively the search dynamics observed with the LLMs.

</details>


### [119] [Decision-oriented benchmarking to transform AI weather forecast access: Application to the Indian monsoon](https://arxiv.org/abs/2602.03767)
*Rajat Masiwal,Colin Aitken,Adam Marchakitus,Mayank Gupta,Katherine Kowal,Hamid A. Pahlavan,Tyler Yang,Y. Qiang Sun,Michael Kremer,Amir Jina,William R. Boos,Pedram Hassanzadeh*

Main category: cs.LG

TL;DR: 介绍面向决策的人工智能天气预报（AIWP）模型评估框架，以印度季风预报为例展示其应用及效果，称该框架可助力脆弱人群适应天气冲击。


<details>
  <summary>Details</summary>
Motivation: 当前AIWP模型评估方法未考虑当地利益相关者在面向决策的运营框架中的需求，需引入新框架。

Method: 引入连接气象、人工智能和社会科学的框架，并应用于印度季风预报问题。

Result: AIWP模型能提前数周在区域尺度上准确预测农业相关的季风起始指数，该框架为向印度农民发送AI季风预报提供信息，捕捉到季风进程的异常停顿。

Conclusion: 该面向决策的基准框架为利用AIWP模型帮助脆弱人群适应天气冲击提供蓝图的关键部分。

Abstract: Artificial intelligence weather prediction (AIWP) models now often outperform traditional physics-based models on common metrics while requiring orders-of-magnitude less computing resources and time. Open-access AIWP models thus hold promise as transformational tools for helping low- and middle-income populations make decisions in the face of high-impact weather shocks. Yet, current approaches to evaluating AIWP models focus mainly on aggregated meteorological metrics without considering local stakeholders' needs in decision-oriented, operational frameworks. Here, we introduce such a framework that connects meteorology, AI, and social sciences. As an example, we apply it to the 150-year-old problem of Indian monsoon forecasting, focusing on benefits to rain-fed agriculture, which is highly susceptible to climate change. AIWP models skillfully predict an agriculturally relevant onset index at regional scales weeks in advance when evaluated out-of-sample using deterministic and probabilistic metrics. This framework informed a government-led effort in 2025 to send 38 million Indian farmers AI-based monsoon onset forecasts, which captured an unusual weeks-long pause in monsoon progression. This decision-oriented benchmarking framework provides a key component of a blueprint for harnessing the power of AIWP models to help large vulnerable populations adapt to weather shocks in the face of climate variability and change.

</details>


### [120] [GraphDancer: Training LLMs to Explore and Reason over Graphs via Curriculum Reinforcement Learning](https://arxiv.org/abs/2602.02518)
*Yuyang Bai,Zhuofeng Li,Ping Nie,Jianwen Xie,Yu Zhang*

Main category: cs.LG

TL;DR: 提出GraphDancer框架让大语言模型在图结构知识上导航推理，用图感知课程训练，在多领域基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现实中很多知识源是异构图，大语言模型在图结构知识上推理面临导航需精确函数调用和多跳证据聚合两个挑战。

Method: 提出GraphDancer强化学习框架，引入图感知课程，用易到难有偏采样器按信息搜索轨迹结构复杂度安排训练。

Result: 在多领域基准测试中，仅用3B主干的GraphDancer优于配备14B主干或GPT - 4o - mini的基线模型。

Conclusion: GraphDancer在图探索和推理技能上有强大的跨领域泛化能力。

Abstract: Large language models (LLMs) increasingly rely on external knowledge to improve factuality, yet many real-world knowledge sources are organized as heterogeneous graphs rather than plain text. Reasoning over such graph-structured knowledge poses two key challenges: (1) navigating structured, schema-defined relations requires precise function calls rather than similarity-based retrieval, and (2) answering complex questions often demands multi-hop evidence aggregation through iterative information seeking. We propose GraphDancer, a reinforcement learning (RL) framework that teaches LLMs to navigate graphs by interleaving reasoning and function execution. To make RL effective for moderate-sized LLMs, we introduce a graph-aware curriculum that schedules training by the structural complexity of information-seeking trajectories using an easy-to-hard biased sampler. We evaluate GraphDancer on a multi-domain benchmark by training on one domain only and testing on unseen domains and out-of-distribution question types. Despite using only a 3B backbone, GraphDancer outperforms baselines equipped with either a 14B backbone or GPT-4o-mini, demonstrating robust cross-domain generalization of graph exploration and reasoning skills. Our code and models can be found at https://yuyangbai.com/graphdancer/ .

</details>


### [121] [Equilibrium Propagation for Non-Conservative Systems](https://arxiv.org/abs/2602.03670)
*Antonino Emanuele Scurria,Dimitri Vanden Abeele,Bortolo Matteo Mognetti,Serge Massar*

Main category: cs.LG

TL;DR: 提出扩展平衡传播（EP）到任意非保守系统的框架，实验显示性能和学习速度优于先前方案。


<details>
  <summary>Details</summary>
Motivation: 原EP算法局限于保守系统，非保守系统在应用中重要，此前扩展EP到非保守系统的尝试未能计算成本函数的精确梯度。

Method: 修改学习阶段的动力学，加入与相互作用非互易部分成比例的项以获得成本函数的精确梯度，也可通过变分公式推导。

Result: 使用MNIST数据库的数值实验表明，该算法性能更好、学习更快。

Conclusion: 提出的框架成功将EP扩展到任意非保守系统，且效果优于先前方法。

Abstract: Equilibrium Propagation (EP) is a physics-inspired learning algorithm that uses stationary states of a dynamical system both for inference and learning. In its original formulation it is limited to conservative systems, $\textit{i.e.}$ to dynamics which derive from an energy function. Given their importance in applications, it is important to extend EP to nonconservative systems, $\textit{i.e.}$ systems with non-reciprocal interactions. Previous attempts to generalize EP to such systems failed to compute the exact gradient of the cost function. Here we propose a framework that extends EP to arbitrary nonconservative systems, including feedforward networks. We keep the key property of equilibrium propagation, namely the use of stationary states both for inference and learning. However, we modify the dynamics in the learning phase by a term proportional to the non-reciprocal part of the interaction so as to obtain the exact gradient of the cost function. This algorithm can also be derived using a variational formulation that generates the learning dynamics through an energy function defined over an augmented state space. Numerical experiments using the MNIST database show that this algorithm achieves better performance and learns faster than previous proposals.

</details>


### [122] [Scaled Dot-Product Attention implements projection of inputs onto a common surface](https://arxiv.org/abs/2602.02521)
*Terence D Sanger*

Main category: cs.LG

TL;DR: 本文指出SDPA可重写为将输入向量投影到由输入自身确定的公共表面的形式，能发现输入中的非线性依赖，提高算法速度并暗示潜在扩展，还对其在语言情境中的作用进行了新解读。


<details>
  <summary>Details</summary>
Motivation: 解决SDPA中“查询、键、值”概念与数学信号处理标准方法难以调和的问题。

Method: 将SDPA重写为数学等价的投影形式。

Result: 重写形式可提高前馈和学习算法速度，暗示潜在扩展，对SDPA在语言情境中的作用有新解读。

Conclusion: 新解读与“自注意力”概念不同，为SDPA用于具有时变局部非线性依赖的时间序列数据提供了有力依据。

Abstract: Scaled dot-product attention (SDPA) is a fundamental component responsible for the success of large-language models and other nonlinear signal processing applications. The rationale for SDPA has been based upon "query, key, value" concepts borrowed from database theory, but these concepts are difficult to reconcile with standard methods in mathematical signal processing. We show that SDPA can be rewritten in a different but mathematically equivalent form as a projection of the input vectors onto a common surface determined by the inputs themselves. Therefore SDPA discovers nonlinear dependencies in the input that are time-dependent and context-dependent. The rewritten form of SDPA permits increased speed of both feedforward and learning algorithms, but more importantly suggests potential extensions. In the context of language, we re-interpret the role of SDPA as finding a time-dependent contextual meaning determined by the surface on which the set of input vectors lies. Input token embeddings are then modified by the local context surface. This interpretation differs substantially from the concept of "self-attention", and provides a strong justification for the use of SDPA for time-series data with time-varying local nonlinear dependencies.

</details>


### [123] [IMU-1: Sample-Efficient Pre-training of Small Language Models](https://arxiv.org/abs/2602.02522)
*George Grigorev*

Main category: cs.LG

TL;DR: 介绍4.3亿参数语言模型IMU - 1，其接近用多56倍数据训练模型的基准性能，给出训练配方并提供消融实验、代码等。


<details>
  <summary>Details</summary>
Motivation: 训练出在较少数据下能达到接近大数据量训练模型性能的语言模型。

Method: 结合近期架构改进（QK - norm注意力、逐头门控等）、优化进展（NorMuon等）和三阶段训练计划及事后检查点EMA。

Result: IMU - 1达到接近用多56倍数据训练模型的基准性能。

Conclusion: 通过特定训练配方可在较少数据下训练出高性能语言模型，且公开代码等方便复现。

Abstract: We present IMU-1, a 430M-parameter language model trained on 72B tokens that approaches the benchmark performance of models trained on 56x more data. We describe a validated training recipe combining recent architectural interventions (QK-norm attention, per-head gating, value residuals, LayerNorm scaling) with optimization advances (NorMuon with cautious weight decay, muP parametrization) and a three-stage training schedule with post-hoc checkpoint EMA. We provide ablations for each component and release code, weights and data to enable reproduction: https://huggingface.co/thepowerfuldeez/imu1_base

</details>


### [124] [TabularMath: Evaluating Computational Extrapolation in Tabular Learning via Program-Verified Synthesis](https://arxiv.org/abs/2602.02523)
*Zerui Cheng,Jiashuo Liu,Jianzhu Yao,Pramod Viswanath,Ge Zhang,Wenhao Huang*

Main category: cs.LG

TL;DR: 本文探讨表格模型能否从统计插值扩展到计算外推，提出TabularMath基准进行评估，发现不同模型在不同指标表现不同，两种范式互补。


<details>
  <summary>Details</summary>
Motivation: 标准表格基准主要关注模型在数据流形内的插值能力评估，而存在大量基于确定性计算过程生成的高价值表格数据，因此研究表格模型能否从统计插值扩展到计算外推。

Method: 提出TabularMath诊断基准，包含114个确定性问题，评估9种表格架构和基于GPT - OSS - 120B的上下文学习（ICL）。

Result: 在标准回归指标上，TabPFN v2.5表现出色；但在精确整数匹配指标上，在分布外数据中TabPFN v2.5准确率低于10%，ICL约为40%。

Conclusion: 表格模型学习的平滑函数近似在进行外推时难以恢复精确计算输出，TabPFN和ICL两范式互补，发布代码和数据供进一步研究。

Abstract: Standard tabular benchmarks mainly focus on the evaluation of a model's capability to interpolate values inside a data manifold, where models good at performing local statistical smoothing are rewarded. However, there exists a very large category of high-value tabular data, including financial modeling and physical simulations, which are generated based upon deterministic computational processes, as opposed to stochastic and noisy relationships. Therefore, we investigate if tabular models can provide an extension from statistical interpolation to computational extrapolation.
  We propose TabularMath, a diagnostic benchmark of 114 deterministic problems (233,472 rows) generated from verified programs based on GSM8K and AIME. We evaluate 9 tabular architectures and in-context learning (ICL) with GPT-OSS-120B. On standard regression metrics, TabPFN v2.5 performs remarkably well, achieving R^2=0.998 in-distribution and maintaining positive R^2 even under distribution shift, which is unique among the tabular models we tested. When we measure rounded consistency (exact integer match), a different picture emerges: TabPFN v2.5 drops below 10% on out-of-distribution data, while ICL maintains around 40%. This gap between R^2 and exact-match accuracy suggests that tabular models learn smooth function approximations but struggle to recover precise computational outputs under extrapolation. The two paradigms appear complementary: TabPFN scales efficiently with data; ICL achieves exact computation from few examples. We release all code and data to support further investigation.

</details>


### [125] [The "Robert Boulton" Singularity: Semantic Tunneling and Manifold Unfolding in Recursive AI](https://arxiv.org/abs/2602.02526)
*Pengyue Hou*

Main category: cs.LG

TL;DR: 研究指出PPL在特定情境下是有误导性的指标，发现“语义隧道”失效模式，用MNCIS框架解决语义多样性问题。


<details>
  <summary>Details</summary>
Motivation: 传统用PPL监测生成式人工智能稳定性，但在上下文稳定机制中PPL有误导性，需解决模型语义多样性丢失问题。

Method: 采用严格滑动窗口协议识别失效模式，应用MNCIS框架，利用自适应谱负耦合诱导“流形展开”。

Result: 基线模型虽语法流畅但语义多样性灾难性损失，MNCIS使模型有效秩从3.62提升到5.35，构建“人工流形”。

Conclusion: PPL在特定情形下不适用，MNCIS框架可有效解决模型语义多样性丢失问题，抵抗语义吸引子影响。

Abstract: The stability of generative artificial intelligence trained on recursive synthetic data is conventionally monitored via Perplexity (PPL). We demonstrate that PPL is a deceptive metric in context-stabilized regimes (L=128). Using a rigorous sliding-window protocol (N=1500), we identify a novel failure mode termed "Semantic Tunneling." While the Baseline model maintains high grammatical fluency (PPL approx. 83.9), it suffers a catastrophic loss of semantic diversity, converging within seven generations to a single, low-entropy narrative attractor: the "Robert Boulton" Singularity. This phenomenon represents a total collapse of the latent manifold (Global Effective Rank 3.62 -> 2.22), where the model discards diverse world knowledge to optimize for statistically safe syntactic templates. To address this, we apply the Multi-Scale Negative Coupled Information Systems (MNCIS) framework recently established in Hou (2026) [arXiv:2601.11594]. We demonstrate that Adaptive Spectral Negative Coupling (ASNC) acts as a topological operator that actively induces "Manifold Unfolding." MNCIS forces the model to expand its effective rank from the anisotropic baseline of 3.62 to a hyper-diverse state of 5.35, effectively constructing an "Artificial Manifold" that resists the gravitational pull of semantic attractors and preserves the long-tail distribution of the training data.

</details>


### [126] [Controlled disagreement improves generalization in decentralized training](https://arxiv.org/abs/2602.02899)
*Zesen Wang,Mikael Johansson*

Main category: cs.LG

TL;DR: 本文引入带自适应共识的去中心化SGD（DSGD - AC），证明共识误差可作隐式正则化器，在测试准确率和解的平坦度上超越传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统观点认为去中心化训练中工作节点间的共识误差会破坏收敛和泛化，本文旨在挑战这一观点。

Method: 引入DSGD - AC，通过时间依赖的缩放机制有意保留非零共识误差。

Result: 在图像分类和机器翻译基准测试中，DSGD - AC在测试准确率和解的平坦度上均超过标准DSGD和集中式SGD。

Conclusion: 共识误差是有用的隐式正则化器，为去中心化学习算法设计提供新视角。

Abstract: Decentralized training is often regarded as inferior to centralized training because the consensus errors between workers are thought to undermine convergence and generalization, even with homogeneous data distributions. This work challenges this view by introducing decentralized SGD with Adaptive Consensus (DSGD-AC), which intentionally preserves non-vanishing consensus errors through a time-dependent scaling mechanism. We prove that these errors are not random noise but systematically align with the dominant Hessian subspace, acting as structured perturbations that guide optimization toward flatter minima. Across image classification and machine translation benchmarks, DSGD-AC consistently surpasses both standard DSGD and centralized SGD in test accuracy and solution flatness. Together, these results establish consensus errors as a useful implicit regularizer and open a new perspective on the design of decentralized learning algorithms.

</details>


### [127] [Incident-Guided Spatiotemporal Traffic Forecasting](https://arxiv.org/abs/2602.02528)
*Lixiang Fan,Bohao Li,Tao Zou,Bowen Du,Junchen Ye*

Main category: cs.LG

TL;DR: 提出IGSTGNN框架处理交通事件对流量预测的影响，构建数据集，框架性能达SOTA，模块可集成到现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有交通流量预测方法忽视突发交通事件对时间模式的影响，影响建模和预测精度。

Method: 提出IGSTGNN框架，含ICSF和TIID模块，构建大规模数据集。

Result: IGSTGNN框架在新基准上达SOTA，ICSF和TIID模块可集成到现有模型。

Conclusion: IGSTGNN框架能有效处理交通事件影响，模块具有可推广性。

Abstract: Recent years have witnessed the rapid development of deep-learning-based, graph-neural-network-based forecasting methods for modern intelligent transportation systems. However, most existing work focuses exclusively on capturing spatio-temporal dependencies from historical traffic data, while overlooking the fact that suddenly occurring transportation incidents, such as traffic accidents and adverse weather, serve as external disturbances that can substantially alter temporal patterns. We argue that this issue has become a major obstacle to modeling the dynamics of traffic systems and improving prediction accuracy, but the unpredictability of incidents makes it difficult to observe patterns from historical sequences. To address these challenges, this paper proposes a novel framework named the Incident-Guided Spatiotemporal Graph Neural Network (IGSTGNN). IGSTGNN explicitly models the incident's impact through two core components: an Incident-Context Spatial Fusion (ICSF) module to capture the initial heterogeneous spatial influence, and a Temporal Incident Impact Decay (TIID) module to model the subsequent dynamic dissipation. To facilitate research on the spatio-temporal impact of incidents on traffic flow, a large-scale dataset is constructed and released, featuring incident records that are time-aligned with traffic time series. On this new benchmark, the proposed IGSTGNN framework is demonstrated to achieve state-of-the-art performance. Furthermore, the generalizability of the ICSF and TIID modules is validated by integrating them into various existing models.

</details>


### [128] [Formulating Reinforcement Learning for Human-Robot Collaboration through Off-Policy Evaluation](https://arxiv.org/abs/2602.02530)
*Saurav Singh,Rodney Sanchez,Alexander Ororbia,Jamison Heard*

Main category: cs.LG

TL;DR: 本文提出基于离线策略评估(OPE)的强化学习框架，仅用日志数据选状态空间和奖励函数，减少实时互动依赖，经两环境验证可提升离线RL可行性与扩展性。


<details>
  <summary>Details</summary>
Motivation: 传统RL方法在现实场景部署需大量人力和与环境交互，成本高且不实用，尤其在复杂和安全关键应用中。

Method: 提出基于OPE的RL框架，用日志交互数据，训练离线RL智能体并应用OPE评估策略性能，选最优状态空间和奖励函数。

Result: 在OpenAI Gym的月球着陆器环境和NASA - MATB - II人类研究环境验证了方法。

Conclusion: 通过数据驱动的OPE评估自动完成关键RL设计决策，提升了离线RL在现实环境的可行性和扩展性，可用于复杂人机交互场景。

Abstract: Reinforcement learning (RL) has the potential to transform real-world decision-making systems by enabling autonomous agents to learn from experience. Deploying RL in real-world settings, especially in the context of human-robot interaction, requires defining state representations and reward functions, which are critical for learning efficiency and policy performance. Traditional RL approaches often rely on domain expertise and trial-and-error, necessitating extensive human involvement as well as direct interaction with the environment, which can be costly and impractical, especially in complex and safety-critical applications. This work proposes a novel RL framework that leverages off-policy evaluation (OPE) for state space and reward function selection, using only logged interaction data. This approach eliminates the need for real-time access to the environment or human-in-the-loop feedback, greatly reducing the dependency on costly real-time interactions. The proposed approach systematically evaluates multiple candidate state representations and reward functions by training offline RL agents and applying OPE to estimate policy performance. The optimal state space and reward function are selected based on their ability to produce high-performing policies under OPE metrics. Our method is validated on two environments: the Lunar Lander environment by OpenAI Gym, which provides a controlled setting for assessing state space and reward function selection, and a NASA-MATB-II human subjects study environment, which evaluates the approach's real-world applicability to human-robot teaming scenarios. This work enhances the feasibility and scalability of offline RL for real-world environments by automating critical RL design decisions through a data-driven OPE-based evaluation, enabling more reliable, effective, and sustainable RL formulation for complex human-robot interaction settings.

</details>


### [129] [Dynamic Topology Optimization for Non-IID Data in Decentralized Learning](https://arxiv.org/abs/2602.03383)
*Bart Cox,Antreas Ioannou,Jérémie Decouchant*

Main category: cs.LG

TL;DR: 提出用于分散式学习的拓扑优化算法Morph，实验表明其在多方面优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 解决分散式学习在数据非独立同分布和通信拓扑静态时难以训练高精度模型的问题。

Method: 提出Morph算法，节点基于最大模型差异自适应选择交换模型的对等节点，通过基于流言的对等节点发现和多样性驱动的邻居选择动态重塑通信图。

Result: 在CIFAR - 10和FEMNIST数据集上，Morph始终优于静态和流行基线方法，接近全连接上限；在测试准确率等指标上表现更优，所需通信轮数更少且无需全局知识。

Conclusion: Morph能实现更高最终准确率、更快收敛和更稳定的学习。

Abstract: Decentralized learning (DL) enables a set of nodes to train a model collaboratively without central coordination, offering benefits for privacy and scalability. However, DL struggles to train a high accuracy model when the data distribution is non-independent and identically distributed (non-IID) and when the communication topology is static. To address these issues, we propose Morph, a topology optimization algorithm for DL. In Morph, nodes adaptively choose peers for model exchange based on maximum model dissimilarity. Morph maintains a fixed in-degree while dynamically reshaping the communication graph through gossip-based peer discovery and diversity-driven neighbor selection, thereby improving robustness to data heterogeneity. Experiments on CIFAR-10 and FEMNIST with up to 100 nodes show that Morph consistently outperforms static and epidemic baselines, while closely tracking the fully connected upper bound. On CIFAR-10, Morph achieves a relative improvement of 1.12x in test accuracy compared to the state-of-the-art baselines. On FEMNIST, Morph achieves an accuracy that is 1.08x higher than Epidemic Learning. Similar trends hold for 50 node deployments, where Morph narrows the gap to the fully connected upper bound within 0.5 percentage points on CIFAR-10. These results demonstrate that Morph achieves higher final accuracy, faster convergence, and more stable learning as quantified by lower inter-node variance, while requiring fewer communication rounds than baselines and no global knowledge.

</details>


### [130] [Hypersonic Flow Control: Generalized Deep Reinforcement Learning for Hypersonic Intake Unstart Control under Uncertainty](https://arxiv.org/abs/2602.02531)
*Trishit Mondal,Ameya D. Jagtap*

Main category: cs.LG

TL;DR: 提出基于深度强化学习（DRL）的主动流动控制策略，控制马赫5的二维高超声速进气道不起动现象，该策略鲁棒性强、泛化能力好。


<details>
  <summary>Details</summary>
Motivation: 高超声速不起动现象对马赫5及以上可靠吸气式推进构成挑战，需有效控制方法。

Method: 采用基于深度强化学习（DRL）的主动流动控制策略，利用内部CFD求解器进行高保真模拟。

Result: DRL控制器能在宽范围背压下稳定进气道，对未见过场景有强零样本泛化能力，在噪声测量下控制仍稳健，最优传感器集能实现相近性能。

Conclusion: 建立了在实际运行不确定性下的实时高超声速流动控制的数据驱动方法。

Abstract: The hypersonic unstart phenomenon poses a major challenge to reliable air-breathing propulsion at Mach 5 and above, where strong shock-boundary-layer interactions and rapid pressure fluctuations can destabilize inlet operation. Here, we demonstrate a deep reinforcement learning (DRL)- based active flow control strategy to control unstart in a canonical two-dimensional hypersonic inlet at Mach 5 and Reynolds number $5\times 10^6$. The in-house CFD solver enables high-fidelity simulations with adaptive mesh refinement, resolving key flow features, including shock motion, boundary-layer dynamics, and flow separation, that are essential for learning physically consistent control policies suitable for real-time deployment. The DRL controller robustly stabilizes the inlet over a wide range of back pressures representative of varying combustion chamber conditions. It further generalizes to previously unseen scenarios, including different back-pressure levels, Reynolds numbers, and sensor configurations, while operating with noisy measurements, thereby demonstrating strong zero-shot generalization. Control remains robust in the presence of noisy sensor measurements, and a minimal, optimally selected sensor set achieves comparable performance, enabling practical implementation. These results establish a data-driven approach for real-time hypersonic flow control under realistic operational uncertainties.

</details>


### [131] [Mitigating Staleness in Asynchronous Pipeline Parallelism via Basis Rotation](https://arxiv.org/abs/2602.03515)
*Hyunji Jung,Sungbin Shin,Namhoon Lee*

Main category: cs.LG

TL;DR: 本文指出异步管道并行中梯度延迟会破坏可扩展性，通过理论分析和实验验证问题，提出基旋转方法解决对齐问题，加速收敛。


<details>
  <summary>Details</summary>
Motivation: 异步管道并行存在梯度陈旧问题，且延迟随管道深度线性增加，破坏可扩展性，需要解决此问题以实现高效可扩展训练。

Method: 通过基旋转纠正延迟梯度，解决Hessian特征基与标准坐标基的对齐问题。

Result: 理论分析和实验验证了梯度延迟问题，基旋转方法有效缓解对齐问题，训练1B参数大语言模型时迭代次数减少76.8%。

Conclusion: 基旋转方法能在异步设置中恢复可扩展的异步训练，同时保持性能，显著加速收敛。

Abstract: Asynchronous pipeline parallelism maximizes hardware utilization by eliminating the pipeline bubbles inherent in synchronous execution, offering a path toward efficient large-scale distributed training. However, this efficiency gain can be compromised by gradient staleness, where the immediate model updates with delayed gradients introduce noise into the optimization process. Crucially, we identify a critical, yet often overlooked, pathology: this delay scales linearly with pipeline depth, fundamentally undermining the very scalability that the method originally intends to provide. In this work, we investigate this inconsistency and bridge the gap by rectifying delayed gradients through basis rotation, restoring scalable asynchronous training while maintaining performance. Specifically, we observe that the deleterious effects of delayed gradients are exacerbated when the Hessian eigenbasis is misaligned with the standard coordinate basis. We demonstrate that this misalignment prevents coordinate-wise adaptive schemes, such as Adam, from effectively leveraging curvature-aware adaptivity. This failure leads to significant oscillations in the optimization trajectory and, consequently, slower convergence. We substantiate these findings through both rigorous theoretical analysis and empirical evaluation. To address this challenge, we propose the use of basis rotation, demonstrating that it effectively mitigates the alignment issue and significantly accelerates convergence in asynchronous settings. For example, our training of a 1B-parameter LLM with basis rotation achieves the same training loss in 76.8% fewer iterations compared to the best-performing asynchronous pipeline parallel training baseline.

</details>


### [132] [CADENT: Gated Hybrid Distillation for Sample-Efficient Transfer in Reinforcement Learning](https://arxiv.org/abs/2602.02532)
*Mahyar Alinejad,Yue Wang,George Atia*

Main category: cs.LG

TL;DR: 本文提出CADENT框架，统一了基于自动机的战略知识和策略级战术知识，通过经验门控信任机制实现自适应知识转移，在多种环境中表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有迁移学习方法在源和目标环境间的领域转移存在问题，策略蒸馏无法转移长期战略知识，基于自动机的方法缺乏细粒度动作指导。

Method: 引入CADENT框架，采用经验门控信任机制在状态 - 动作层面动态权衡教师指导和学生自身经验。

Result: 在具有挑战性的环境中，CADENT比基线方法的样本效率提高了40 - 60%，并保持了优越的渐近性能。

Conclusion: CADENT是一种用于强化学习中自适应知识转移的可靠方法。

Abstract: Transfer learning promises to reduce the high sample complexity of deep reinforcement learning (RL), yet existing methods struggle with domain shift between source and target environments. Policy distillation provides powerful tactical guidance but fails to transfer long-term strategic knowledge, while automaton-based methods capture task structure but lack fine-grained action guidance. This paper introduces Context-Aware Distillation with Experience-gated Transfer (CADENT), a framework that unifies strategic automaton-based knowledge with tactical policy-level knowledge into a coherent guidance signal. CADENT's key innovation is an experience-gated trust mechanism that dynamically weighs teacher guidance against the student's own experience at the state-action level, enabling graceful adaptation to target domain specifics. Across challenging environments, from sparse-reward grid worlds to continuous control tasks, CADENT achieves 40-60\% better sample efficiency than baselines while maintaining superior asymptotic performance, establishing a robust approach for adaptive knowledge transfer in RL.

</details>


### [133] [Enhancing Psychologists' Understanding through Explainable Deep Learning Framework for ADHD Diagnosis](https://arxiv.org/abs/2602.02535)
*Abdul Rehman,Ilona Heldal,Jerry Chun-Wei Lin*

Main category: cs.LG

TL;DR: 本文提出HyExDNN - RNN模型用于ADHD检测、多类分类和决策解释，框架结果优秀，结合AI与人类专业知识具诊断潜力。


<details>
  <summary>Details</summary>
Motivation: ADHD诊断具有挑战性，需要可靠、透明的先进方法用于识别和分类。

Method: 提出基于微调混合深度神经网络和循环神经网络的可解释框架，用皮尔逊相关系数选特征，采用机器学习和深度学习模型分析比较，用标准化技术降维、选模型和解释。

Result: 框架在二分类表现出色，HyExDNN - RNN在多类分类F1分数分别达99%和94.2%，XAI方法揭示特征重要性和决策逻辑。

Conclusion: 框架有潜力辅助ADHD诊断和解释。

Abstract: Attention Deficit Hyperactivity Disorder (ADHD) is a neurodevelopmental disorder that is challenging to diagnose and requires advanced approaches for reliable and transparent identification and classification. It is characterized by a pattern of inattention, hyperactivity and impulsivity that is more severe and more frequent than in individuals with a comparable level of development. In this paper, an explainable framework based on a fine-tuned hybrid Deep Neural Network (DNN) and Recurrent Neural Network (RNN) called HyExDNN-RNN model is proposed for ADHD detection, multi-class categorization, and decision interpretation. This framework not only detects ADHD, but also provides interpretable insights into the diagnostic process so that psychologists can better understand and trust the results of the diagnosis. We use the Pearson correlation coefficient for optimal feature selection and machine and deep learning models for experimental analysis and comparison. We use a standardized technique for feature reduction, model selection and interpretation to accurately determine the diagnosis rate and ensure the interpretability of the proposed framework. Our framework provided excellent results on binary classification, with HyExDNN-RNN achieving an F1 score of 99% and 94.2% on multi-class categorization. XAI approaches, in particular SHapley Additive exPlanations (SHAP) and Permutation Feature Importance (PFI), provided important insights into the importance of features and the decision logic of models. By combining AI with human expertise, we aim to bridge the gap between advanced computational techniques and practical psychological applications. These results demonstrate the potential of our framework to assist in ADHD diagnosis and interpretation.

</details>


### [134] [From Sparse Decisions to Dense Reasoning: A Multi-attribute Trajectory Paradigm for Multimodal Moderation](https://arxiv.org/abs/2602.02536)
*Tianle Gu,Kexin Huang,Lingyu Li,Ruilin Luo,Shiyang Huang,Zongqi Wang,Yujiu Yang,Yan Teng,Yingchun Wang*

Main category: cs.LG

TL;DR: 提出新学习范式UniMod和多头部标量奖励模型UniRM用于多模态安全审核，用更少数据取得好效果。


<details>
  <summary>Details</summary>
Motivation: 多模态安全审核受数据和监督双重稀疏性阻碍，传统二元标签导致捷径学习，需有效多模态判别方法。

Method: 提出UniMod从稀疏决策过渡到密集推理轨迹，构建结构化轨迹；开发UniRM提供多维度监督；引入优化策略解耦参数和平衡训练动态。

Result: UniMod用不到领先基线40%训练数据取得有竞争力文本审核表现并创下多模态新基准。

Conclusion: 多属性轨迹推理有效，为多模态审核提供有效高效框架。

Abstract: Safety moderation is pivotal for identifying harmful content. Despite the success of textual safety moderation, its multimodal counterparts remain hindered by a dual sparsity of data and supervision. Conventional reliance on binary labels lead to shortcut learning, which obscures the intrinsic classification boundaries necessary for effective multimodal discrimination. Hence, we propose a novel learning paradigm (UniMod) that transitions from sparse decision-making to dense reasoning traces. By constructing structured trajectories encompassing evidence grounding, modality assessment, risk mapping, policy decision, and response generation, we reformulate monolithic decision tasks into a multi-dimensional boundary learning process. This approach forces the model to ground its decision in explicit safety semantics, preventing the model from converging on superficial shortcuts. To facilitate this paradigm, we develop a multi-head scalar reward model (UniRM). UniRM provides multi-dimensional supervision by assigning attribute-level scores to the response generation stage. Furthermore, we introduce specialized optimization strategies to decouple task-specific parameters and rebalance training dynamics, effectively resolving interference between diverse objectives in multi-task learning. Empirical results show UniMod achieves competitive textual moderation performance and sets a new multimodal benchmark using less than 40\% of the training data used by leading baselines. Ablations further validate our multi-attribute trajectory reasoning, offering an effective and efficient framework for multimodal moderation. Supplementary materials are available at \href{https://trustworthylab.github.io/UniMod/}{project website}.

</details>


### [135] [Enhancing Post-Training Quantization via Future Activation Awareness](https://arxiv.org/abs/2602.02538)
*Zheqi Lv,Zhenxuan Fan,Qi Tian,Wenqiao Zhang,Yueting Zhuang*

Main category: cs.LG

TL;DR: 提出Future - Aware Quantization (FAQ)方法压缩大语言模型，实验显示其优于先前方法且适合边缘部署。


<details>
  <summary>Details</summary>
Motivation: 传统后训练量化（PTQ）方法存在量化偏差和误差累积问题，尤其是校准数据有偏差时，量化效果不佳且不稳定。

Method: 提出FAQ方法，利用未来层激活来指导量化，引入窗口预览机制聚合多个未来层激活，使用预搜索配置减少开销。

Result: 实验表明FAQ在几乎无额外成本的情况下，始终优于先前方法，无需反向传播、数据重构或调优。

Conclusion: FAQ适合用于边缘部署。

Abstract: Post-training quantization (PTQ) is a widely used method to compress large language models (LLMs) without fine-tuning. It typically sets quantization hyperparameters (e.g., scaling factors) based on current-layer activations. Although this method is efficient, it suffers from quantization bias and error accumulation, resulting in suboptimal and unstable quantization, especially when the calibration data is biased. To overcome these issues, we propose Future-Aware Quantization (FAQ), which leverages future-layer activations to guide quantization. This allows better identification and preservation of important weights, while reducing sensitivity to calibration noise. We further introduce a window-wise preview mechanism to softly aggregate multiple future-layer activations, mitigating over-reliance on any single layer. To avoid expensive greedy search, we use a pre-searched configuration to minimize overhead. Experiments show that FAQ consistently outperforms prior methods with negligible extra cost, requiring no backward passes, data reconstruction, or tuning, making it well-suited for edge deployment.

</details>


### [136] [How Much Information Can a Vision Token Hold? A Scaling Law for Recognition Limits in VLMs](https://arxiv.org/abs/2602.02539)
*Shuxin Zhuang,Zi Liang,Runsheng Yu,Hongzong Li,Rong Feng,Shiqin Tang,Youzhi Zhang*

Main category: cs.LG

TL;DR: 研究视觉标记信息上限，通过压力测试发现相转变现象，提出概率缩放定律并验证其普遍性，为视觉上下文压缩提供指导。


<details>
  <summary>Details</summary>
Motivation: 现有以视觉为中心的方法虽有进展，但视觉编码器表征能力有限，需探究视觉标记的信息上限。

Method: 通过逐步增加图像内信息数量进行受控压力测试，分析相转变的机制起源和关键因素，提出概率缩放定律。

Result: 观察到包含稳定、不稳定和崩溃三个阶段的相转变现象，提出的概率缩放定律在多种视觉语言模型实验中展现出普遍性。

Conclusion: 概率缩放定律为优化视觉上下文压缩的效率 - 准确性权衡提供了关键的实证指导。

Abstract: Recent vision-centric approaches have made significant strides in long-context modeling. Represented by DeepSeek-OCR, these models encode rendered text into continuous vision tokens, achieving high compression rates without sacrificing recognition precision. However, viewing the vision encoder as a lossy channel with finite representational capacity raises a fundamental question: what is the information upper bound of visual tokens? To investigate this limit, we conduct controlled stress tests by progressively increasing the information quantity (character count) within an image. We observe a distinct phase-transition phenomenon characterized by three regimes: a near-perfect Stable Phase, an Instability Phase marked by increased error variance, and a total Collapse Phase. We analyze the mechanical origins of these transitions and identify key factors. Furthermore, we formulate a probabilistic scaling law that unifies average vision token load and visual density into a latent difficulty metric. Extensive experiments across various Vision-Language Models demonstrate the universality of this scaling law, providing critical empirical guidance for optimizing the efficiency-accuracy trade-off in visual context compression.

</details>


### [137] [Auto-Augmentation Contrastive Learning for Wearable-based Human Activity Recognition](https://arxiv.org/abs/2602.02542)
*Qingyu Wu,Jianfei Shen,Feiyi Fan,Yang Gu,Chenyang Xu,Yiqiang Chen*

Main category: cs.LG

TL;DR: 提出用于可穿戴式人类活动识别的端到端自动增强对比学习方法AutoCL，实验表明其能显著提高识别准确率。


<details>
  <summary>Details</summary>
Motivation: 现有对比学习在人类活动识别的低语义数据增强策略缺乏泛化性和灵活性，需减轻增强负担。

Method: 提出基于Siamese网络架构的AutoCL，嵌入生成器学习自动增强，训练生成器克服原始传感器数据中的干扰，还提出停止梯度设计和相关性降低策略。

Result: 架构实证研究表明设计有效，基于四个常见人类活动识别数据集的大量实验显示，AutoCL比其他SOTA方法显著提高识别准确率。

Conclusion: 提出的AutoCL方法在人类活动识别中有效，能提升识别准确率。

Abstract: For low-semantic sensor signals from human activity recognition (HAR), contrastive learning (CL) is essential to implement novel applications or generic models without manual annotation, which is a high-performance self-supervised learning (SSL) method. However, CL relies heavily on data augmentation for pairwise comparisons. Especially for low semantic data in the HAR area, conducting good performance augmentation strategies in pretext tasks still rely on manual attempts lacking generalizability and flexibility. To reduce the augmentation burden, we propose an end-to-end auto-augmentation contrastive learning (AutoCL) method for wearable-based HAR. AutoCL is based on a Siamese network architecture that shares the parameters of the backbone and with a generator embedded to learn auto-augmentation. AutoCL trains the generator based on the representation in the latent space to overcome the disturbances caused by noise and redundant information in raw sensor data. The architecture empirical study indicates the effectiveness of this design. Furthermore, we propose a stop-gradient design and correlation reduction strategy in AutoCL to enhance encoder representation learning. Extensive experiments based on four wide-used HAR datasets demonstrate that the proposed AutoCL method significantly improves recognition accuracy compared with other SOTA methods.

</details>


### [138] [Fubini Study geometry of representation drift in high dimensional data](https://arxiv.org/abs/2602.02596)
*Arturo Tozzi*

Main category: cs.LG

TL;DR: 本文引入基于Fubini - Study度量的投影几何视角来量化高维表示漂移，对比传统度量，该方法能分离有意义的结构演化和参数化伪影。


<details>
  <summary>Details</summary>
Motivation: 传统欧氏或余弦距离量化高维表示漂移时，会将数据的内在变化与任意参数化引起的变化混在一起。

Method: 引入基于Fubini - Study度量的投影几何视角，构建表示轨迹，对比不同距离度量。

Result: 传统度量在表示存在真正的投影模糊时会系统性高估变化，Fubini - Study度量能在规范诱导的波动下保持不变，且余弦和Fubini - Study漂移的差异可捕获因规范自由度导致的表示搅动。

Conclusion: 建立了评估高维系统表示稳定性的几何标准，明确了角距离的局限性，将表示动力学嵌入投影空间有助于数据分析。

Abstract: High dimensional representation drift is commonly quantified using Euclidean or cosine distances, which presuppose fixed coordinates when comparing representations across time, training or preprocessing stages. While effective in many settings, these measures entangle intrinsic changes in the data with variations induced by arbitrary parametrizations. We introduce a projective geometric view of representation drift grounded in the Fubini Study metric, which identifies representations that differ only by gauge transformations such as global rescalings or sign flips. Applying this framework to empirical high dimensional datasets, we explicitly construct representation trajectories and track their evolution through cumulative geometric drift. Comparing Euclidean, cosine and Fubini Study distances along these trajectories reveals that conventional metrics systematically overestimate change whenever representations carry genuine projective ambiguity. By contrast, the Fubini Study metric isolates intrinsic evolution by remaining invariant under gauge-induced fluctuations. We further show that the difference between cosine and Fubini Study drift defines a computable, monotone quantity that directly captures representation churn attributable to gauge freedom. This separation provides a diagnostic for distinguishing meaningful structural evolution from parametrization artifacts, without introducing model-specific assumptions. Overall, we establish a geometric criterion for assessing representation stability in high-dimensional systems and clarify the limits of angular distances. Embedding representation dynamics in projective space connects data analysis with established geometric programs and yields observables that are directly testable in empirical workflows.

</details>


### [139] [Toward Ultra-Long-Horizon Sequential Model Editing](https://arxiv.org/abs/2602.02543)
*Mingda Liu,Zhenghan Zhu,Ze'an Miao,Katsuki Fujisawa*

Main category: cs.LG

TL;DR: 现有L&E模型编辑范式长序列编辑会导致模型崩溃，本文提出NAS策略解决该问题，实验效果良好。


<details>
  <summary>Details</summary>
Motivation: 现有的Locate-and-Edit (L&E)范式在长序列编辑时会触发模型崩溃，需要解决该问题。

Method: 提出Norm-Anchor Scaling (NAS)，一种即插即用的范数约束策略。

Result: NAS将代表性L&E算法的崩溃点延迟4倍多，编辑性能平均相对提高72.2%，仅需额外一行代码且计算开销可忽略不计。

Conclusion: NAS策略有效解决了L&E范式长序列编辑的模型崩溃问题，具有良好的应用价值。

Abstract: Model editing has emerged as a practical approach for mitigating factual errors and outdated knowledge in large language models (LLMs). Among existing methods, the Locate-and-Edit (L&E) paradigm is the dominant framework: it locates MLP parameters implicated in expressing a target fact, and then performs a localized update to rewrite that fact. However, long sequences of edits often trigger abrupt model collapse in L&E beyond a critical point. We empirically identify a strong correlation between collapse and explosive growth of edited MLP weight norms, and formally prove that commonly used L&E update rules can induce exponential norm growth across sequential edits in the absence of explicit norm control. To address this issue, we propose Norm-Anchor Scaling NAS, a plug-and-play norm-constrained strategy. Across extensive experiments, NAS delays the collapse point of representative L&E algorithms by more than 4 times and yields a 72.2% average relative gain in editing performance, requiring only a single additional line of code and incurring negligible computational overhead.

</details>


### [140] [Learning Better Certified Models from Empirically-Robust Teachers](https://arxiv.org/abs/2602.02626)
*Alessandro De Palma*

Main category: cs.LG

TL;DR: 本文提出利用经验鲁棒的教师模型，通过知识蒸馏提高可证明鲁棒模型的性能，在多个鲁棒计算机视觉基准测试中取得进展。


<details>
  <summary>Details</summary>
Motivation: 对抗训练虽有经验鲁棒性但难获强鲁棒性证明，早期认证训练标准性能不佳，现有方法在可验证性上仍有标准性能成本，故提出改进方法。

Method: 利用经验鲁棒的教师模型，通过知识蒸馏，使用通用特征空间蒸馏目标。

Result: 从对抗训练的教师模型进行蒸馏，在ReLU网络的认证训练中持续改进了现有技术水平。

Conclusion: 通过知识蒸馏利用经验鲁棒教师模型可有效提升可证明鲁棒模型的性能。

Abstract: Adversarial training attains strong empirical robustness to specific adversarial attacks by training on concrete adversarial perturbations, but it produces neural networks that are not amenable to strong robustness certificates through neural network verification. On the other hand, earlier certified training schemes directly train on bounds from network relaxations to obtain models that are certifiably robust, but display sub-par standard performance. Recent work has shown that state-of-the-art trade-offs between certified robustness and standard performance can be obtained through a family of losses combining adversarial outputs and neural network bounds. Nevertheless, differently from empirical robustness, verifiability still comes at a significant cost in standard performance. In this work, we propose to leverage empirically-robust teachers to improve the performance of certifiably-robust models through knowledge distillation. Using a versatile feature-space distillation objective, we show that distillation from adversarially-trained teachers consistently improves on the state-of-the-art in certified training for ReLU networks across a series of robust computer vision benchmarks.

</details>


### [141] [SPA-Cache: Singular Proxies for Adaptive Caching in Diffusion Language Models](https://arxiv.org/abs/2602.02544)
*Wenhao Sun,Rong-Cheng Tu,Yifu Ding,Zhao Jin,Jingyi Liao,Yongcheng Jing,Dacheng Tao*

Main category: cs.LG

TL;DR: 提出 SPA - Cache 优化 DLM 缓存，提升效率。


<details>
  <summary>Details</summary>
Motivation: 现有 DLM 缓存方法存在更新识别启发式成本高和预算分配僵化问题，需改进。

Method: 推导低维奇异代理识别关键更新令牌，引入自适应策略分配更新。

Result: 显著提高 DLM 效率，相比普通解码吞吐量提升 8 倍，比现有缓存基线加速 2 - 4 倍。

Conclusion: SPA - Cache 能有效解决现有 DLM 缓存挑战，提升效率。

Abstract: While Diffusion Language Models (DLMs) offer a flexible, arbitrary-order alternative to the autoregressive paradigm, their non-causal nature precludes standard KV caching, forcing costly hidden state recomputation at every decoding step. Existing DLM caching approaches reduce this cost by selective hidden state updates; however, they are still limited by (i) costly token-wise update identification heuristics and (ii) rigid, uniform budget allocation that fails to account for heterogeneous hidden state dynamics. To address these challenges, we present SPA-Cache that jointly optimizes update identification and budget allocation in DLM cache. First, we derive a low-dimensional singular proxy that enables the identification of update-critical tokens in a low-dimensional subspace, substantially reducing the overhead of update identification. Second, we introduce an adaptive strategy that allocates fewer updates to stable layers without degrading generation quality. Together, these contributions significantly improve the efficiency of DLMs, yielding up to an $8\times$ throughput improvement over vanilla decoding and a $2$--$4\times$ speedup over existing caching baselines.

</details>


### [142] [Membership Inference Attacks from Causal Principles](https://arxiv.org/abs/2602.02819)
*Mathieu Even,Clément Berenfeld,Linus Bleistein,Tudor Cebere,Julie Josse,Aurélien Bellet*

Main category: cs.LG

TL;DR: 将MIA评估作为因果推理问题，解决现有评估方法统计有效性不明的问题，提出有一致性保证的估计量，实验证明方法可靠。


<details>
  <summary>Details</summary>
Motivation: 标准的成员推理攻击（MIAs）评估需重复训练，计算成本高，常用的单轮和零轮方法统计有效性不明。

Method: 将MIA评估构建为因果推理问题，定义记忆为训练数据包含数据点的因果效应，推导标准MIA指标的因果类似物，提出不同轮次制度的实用估计量。

Result: 实验表明即使在无法重新训练和分布转移情况下，该方法也能实现可靠的记忆测量。

Conclusion: 该方法为现代人工智能系统的隐私评估提供了原则性的基础。

Abstract: Membership Inference Attacks (MIAs) are widely used to quantify training data memorization and assess privacy risks. Standard evaluation requires repeated retraining, which is computationally costly for large models. One-run methods (single training with randomized data inclusion) and zero-run methods (post hoc evaluation) are often used instead, though their statistical validity remains unclear. To address this gap, we frame MIA evaluation as a causal inference problem, defining memorization as the causal effect of including a data point in the training set. This novel formulation reveals and formalizes key sources of bias in existing protocols: one-run methods suffer from interference between jointly included points, while zero-run evaluations popular for LLMs are confounded by non-random membership assignment. We derive causal analogues of standard MIA metrics and propose practical estimators for multi-run, one-run, and zero-run regimes with non-asymptotic consistency guarantees. Experiments on real-world data show that our approach enables reliable memorization measurement even when retraining is impractical and under distribution shift, providing a principled foundation for privacy evaluation in modern AI systems.

</details>


### [143] [Beyond Alignment: Expanding Reasoning Capacity via Manifold-Reshaping Policy Optimization](https://arxiv.org/abs/2602.02545)
*Dayu Wang,Jiaye Yang,Weikang Li,Jiahui Liang,Yang Li*

Main category: cs.LG

TL;DR: 本文挑战了强化学习仅对齐大语言模型现有潜在能力的假设，提出MRPO框架能拓展推理空间，在数学任务上取得SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 现有研究质疑强化学习是否真正扩展了大语言模型的推理能力，作者想挑战可访问边界假设，拓展潜在推理空间。

Method: 提出Manifold - Reshaping Policy Optimization (MRPO)框架，分两阶段操作：先用Spectral Orthogonal Exploration (SOE)将策略初始化移出偏差流形的零空间，再在策略优化目标中加入有效秩正则化项。

Result: 4B参数的方法在数学任务上达到SOTA，显著超越更大模型（如Qwen3 - 32B），能力边界超越标准GRPO。

Conclusion: 通过针对性的几何干预能从根本上拓展大语言模型的潜在推理空间，MRPO框架有效。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has demonstrated remarkable success in enhancing the reasoning capabilities of Large Language Models (LLMs). However, recent studies question whether RL genuinely expands reasoning capacity or merely aligns existing latent capabilities, arguing that exploration remains confined within the pre-trained model's low-rank bias manifold. In this work, we challenge this accessibility boundary hypothesis by demonstrating that the latent reasoning space can be fundamentally expanded through targeted geometric interventions. We propose Manifold-Reshaping Policy Optimization (MRPO), a geometric framework designed to fundamentally restructure the inference space of LLMs. MRPO operates in two stages: first, we employ Spectral Orthogonal Exploration (SOE) to eject the policy initialization into the null space of the bias manifold; second, we integrate an Effective Rank regularization term into the policy optimization objective. This approach incentivizes the discovery and maintenance of high-dimensional reasoning trajectories against the entropy-reducing tendency of standard RL. Empirically, our 4B-parameter method achieves state-of-the-art performance on mathematical tasks, significantly outperforming larger models (e.g., Qwen3-32B) and expanding the capability boundary beyond standard GRPO. Our code is available at https://anonymous.4open.science/r/MRPO-D57B/

</details>


### [144] [A Random Matrix Theory Perspective on the Consistency of Diffusion Models](https://arxiv.org/abs/2602.02908)
*Binxu Wang,Jacob Zavatone-Veth,Cengiz Pehlevan*

Main category: cs.LG

TL;DR: 研究扩散模型在不同数据集子集训练时输出一致性，用随机矩阵理论框架分析，理论预测线性模型行为并在非记忆机制架构验证，为扩散训练可重复性提供基线。


<details>
  <summary>Details</summary>
Motivation: 解释扩散模型在不同非重叠数据集子集训练时，相同噪声种子产生相似输出的一致性原因。

Method: 开发随机矩阵理论（RMT）框架量化有限数据集对学习去噪器和采样映射的期望和方差的影响，将确定性等价工具扩展到分数矩阵幂分析采样轨迹。

Result: 理论准确预测线性扩散模型行为，在UNet和DiT架构非记忆机制下验证预测，识别不同训练数据划分下样本偏差位置和方式。

Conclusion: 为扩散训练的可重复性提供了基于理论的基线，将数据的频谱特性与生成输出的稳定性联系起来。

Abstract: Diffusion models trained on different, non-overlapping subsets of a dataset often produce strikingly similar outputs when given the same noise seed. We trace this consistency to a simple linear effect: the shared Gaussian statistics across splits already predict much of the generated images. To formalize this, we develop a random matrix theory (RMT) framework that quantifies how finite datasets shape the expectation and variance of the learned denoiser and sampling map in the linear setting. For expectations, sampling variability acts as a renormalization of the noise level through a self-consistent relation $σ^2 \mapsto κ(σ^2)$, explaining why limited data overshrink low-variance directions and pull samples toward the dataset mean. For fluctuations, our variance formulas reveal three key factors behind cross-split disagreement: \textit{anisotropy} across eigenmodes, \textit{inhomogeneity} across inputs, and overall scaling with dataset size. Extending deterministic-equivalence tools to fractional matrix powers further allows us to analyze entire sampling trajectories. The theory sharply predicts the behavior of linear diffusion models, and we validate its predictions on UNet and DiT architectures in their non-memorization regime, identifying where and how samples deviates across training data split. This provides a principled baseline for reproducibility in diffusion training, linking spectral properties of data to the stability of generative outputs.

</details>


### [145] [D$^2$Quant: Accurate Low-bit Post-Training Weight Quantization for LLMs](https://arxiv.org/abs/2602.02546)
*Xianglong Yan,ChengZhu Bao,Zhiteng Li,Tianao Zhang,Shaoqiu Zhang,Ruobing Xie,Samm Sun,Yulun Zhang*

Main category: cs.LG

TL;DR: 论文指出大语言模型在资源受限场景部署难，仅权重量化在亚4位精度下精度下降，提出D²Quant框架解决问题，实验证明其性能优越。


<details>
  <summary>Details</summary>
Motivation: 大语言模型计算和内存成本高，在资源受限场景难以部署，仅权重量化在亚4位精度下精度显著下降，需解决相关问题。

Method: 提出D²Quant框架，在权重方面设计双尺度量化器（DSQ）处理下投影矩阵，在激活方面提出偏差感知校正（DAC）减轻量化引起的激活分布偏移。

Result: 在多个大语言模型家族和评估指标上的大量实验表明，D²Quant在亚4位精度的仅权重量化中表现优越。

Conclusion: D²Quant框架能有效解决仅权重量化在亚4位精度下的问题，提升量化性能。

Abstract: Large language models (LLMs) deliver strong performance, but their high compute and memory costs make deployment difficult in resource-constrained scenarios. Weight-only post-training quantization (PTQ) is appealing, as it reduces memory usage and enables practical speedup without low-bit operators or specialized hardware. However, accuracy often degrades significantly in weight-only PTQ at sub-4-bit precision, and our analysis identifies two main causes: (1) down-projection matrices are a well-known quantization bottleneck, but maintaining their fidelity often requires extra bit-width; (2) weight quantization induces activation deviations, but effective correction strategies remain underexplored. To address these issues, we propose D$^2$Quant, a novel weight-only PTQ framework that improves quantization from both the weight and activation perspectives. On the weight side, we design a Dual-Scale Quantizer (DSQ) tailored to down-projection matrices, with an absorbable scaling factor that significantly improves accuracy without increasing the bit budget. On the activation side, we propose Deviation-Aware Correction (DAC), which incorporates a mean-shift correction within LayerNorm to mitigate quantization-induced activation distribution shifts. Extensive experiments across multiple LLM families and evaluation metrics show that D$^2$Quant delivers superior performance for weight-only PTQ at sub-4-bit precision. The code and models will be available at https://github.com/XIANGLONGYAN/D2Quant.

</details>


### [146] [Notes on the Reward Representation of Posterior Updates](https://arxiv.org/abs/2602.02912)
*Pedro A. Ortega*

Main category: cs.LG

TL;DR: 研究现代控制和强化学习中决策作为推理何时能从隐喻变为现实，得出后验更新相关结果及连贯性约束条件。


<details>
  <summary>Details</summary>
Motivation: 探讨现代控制和强化学习中决策作为推理何时能成为字面意义而非隐喻。

Method: 研究KL正则化软更新在单一固定概率模型中恰为贝叶斯后验的特殊情况。

Result: 后验更新可确定改变行为的相对、依赖上下文的激励信号，但不能唯一确定绝对奖励，不同更新方向要求一个可重用的延续值会增加连贯性约束。

Conclusion: 明确了后验更新在确定激励信号和奖励方面的作用及限制，还有连贯性约束条件。

Abstract: Many ideas in modern control and reinforcement learning treat decision-making as inference: start from a baseline distribution and update it when a signal arrives. We ask when this can be made literal rather than metaphorical. We study the special case where a KL-regularized soft update is exactly a Bayesian posterior inside a single fixed probabilistic model, so the update variable is a genuine channel through which information is transmitted. In this regime, behavioral change is driven only by evidence carried by that channel: the update must be explainable as an evidence reweighing of the baseline. This yields a sharp identification result: posterior updates determine the relative, context-dependent incentive signal that shifts behavior, but they do not uniquely determine absolute rewards, which remain ambiguous up to context-specific baselines. Requiring one reusable continuation value across different update directions adds a further coherence constraint linking the reward descriptions associated with different conditioning orders.

</details>


### [147] [naPINN: Noise-Adaptive Physics-Informed Neural Networks for Recovering Physics from Corrupted Measurement](https://arxiv.org/abs/2602.02547)
*Hankyeol Kim,Pilsung Kang*

Main category: cs.LG

TL;DR: 提出Noise - Adaptive Physics - Informed Neural Network (naPINN)解决PINNs在复杂噪声和异常值下性能下降问题，经实验验证其效果好。


<details>
  <summary>Details</summary>
Motivation: Physics - Informed Neural Networks (PINNs)在复杂测量噪声和严重异常值下性能显著下降，需改进。

Method: 将基于能量的模型嵌入训练循环学习预测残差的潜在分布，用可训练的可靠性门过滤高能量数据点，用拒绝成本正则化防止丢弃有效数据。

Result: 在受非高斯噪声和不同异常值率影响的各种基准偏微分方程上，naPINN显著优于现有鲁棒PINN基线，能隔离异常值并准确重建动态。

Conclusion: naPINN能在无噪声分布先验知识情况下，从损坏测量中稳健恢复物理解，有效解决PINNs的问题。

Abstract: Physics-Informed Neural Networks (PINNs) are effective methods for solving inverse problems and discovering governing equations from observational data. However, their performance degrades significantly under complex measurement noise and gross outliers. To address this issue, we propose the Noise-Adaptive Physics-Informed Neural Network (naPINN), which robustly recovers physical solutions from corrupted measurements without prior knowledge of the noise distribution. naPINN embeds an energy-based model into the training loop to learn the latent distribution of prediction residuals. Leveraging the learned energy landscape, a trainable reliability gate adaptively filters data points exhibiting high energy, while a rejection cost regularization prevents trivial solutions where valid data are discarded. We demonstrate the efficacy of naPINN on various benchmark partial differential equations corrupted by non-Gaussian noise and varying rates of outliers. The results show that naPINN significantly outperforms existing robust PINN baselines, successfully isolating outliers and accurately reconstructing the dynamics under severe data corruption.

</details>


### [148] [Why Some Models Resist Unlearning: A Linear Stability Perspective](https://arxiv.org/abs/2602.02986)
*Wei-Kai Chang,Rajiv Khanna*

Main category: cs.LG

TL;DR: 文章从渐近线性稳定性角度研究机器学习的遗忘，分析数据相干性，证明稳定性阈值，研究两层ReLU CNN，通过实验验证，给出记忆、相干性和遗忘之间权衡的理论解释。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习遗忘领域缺乏理论理解，文章旨在填补这一空白。

Method: 通过渐近线性稳定性框架研究遗忘，分析数据相干性，研究两层ReLU CNN，使用随机矩阵理论工具。

Result: 证明了稳定性阈值，发现高信噪比模型难遗忘，实验验证与预测边界相符。

Conclusion: 文章提供了记忆、相干性和遗忘之间权衡的理论解释。

Abstract: Machine unlearning, the ability to erase the effect of specific training samples without retraining from scratch, is critical for privacy, regulation, and efficiency. However, most progress in unlearning has been empirical, with little theoretical understanding of when and why unlearning works. We tackle this gap by framing unlearning through the lens of asymptotic linear stability to capture the interaction between optimization dynamics and data geometry. The key quantity in our analysis is data coherence which is the cross sample alignment of loss surface directions near the optimum. We decompose coherence along three axes: within the retain set, within the forget set, and between them, and prove tight stability thresholds that separate convergence from divergence. To further link data properties to forgettability, we study a two layer ReLU CNN under a signal plus noise model and show that stronger memorization makes forgetting easier: when the signal to noise ratio (SNR) is lower, cross sample alignment is weaker, reducing coherence and making unlearning easier; conversely, high SNR, highly aligned models resist unlearning. For empirical verification, we show that Hessian tests and CNN heatmaps align closely with the predicted boundary, mapping the stability frontier of gradient based unlearning as a function of batching, mixing, and data/model alignment. Our analysis is grounded in random matrix theory tools and provides the first principled account of the trade offs between memorization, coherence, and unlearning.

</details>


### [149] [ToolTok: Tool Tokenization for Efficient and Generalizable GUI Agents](https://arxiv.org/abs/2602.02548)
*Xiaoce Wang,Guibin Zhang,Junzhe Li,Jinzhe Tu,Chun Li,Ming Li*

Main category: cs.LG

TL;DR: 提出ToolTok范式解决现有GUI代理模型问题，在多基准测试中表现优且泛化性强，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有GUI代理模型基于坐标一步视觉定位难以适应不同分辨率和宽高比，无坐标策略在数据稀缺时学习困难。

Method: 提出多步寻路范式ToolTok，设计符合人类习惯工具并用可学习令牌嵌入表示，引入语义锚定机制辅助学习，构建由易到难课程让预训练大模型学习工具语义。

Result: ToolTok在可比规模模型中性能优越，与更大模型竞争有优势，用少于1%训练数据，有强泛化能力。

Conclusion: ToolTok是解决现有GUI代理模型问题的有效范式。

Abstract: Existing GUI agent models relying on coordinate-based one-step visual grounding struggle with generalizing to varying input resolutions and aspect ratios. Alternatives introduce coordinate-free strategies yet suffer from learning under severe data scarcity. To address the limitations, we propose ToolTok, a novel paradigm of multi-step pathfinding for GUI agents, where operations are modeled as a sequence of progressive tool usage. Specifically, we devise tools aligned with human interaction habits and represent each tool using learnable token embeddings. To enable efficient embedding learning under limited supervision, ToolTok introduces a semantic anchoring mechanism that grounds each tool with semantically related concepts as natural inductive bias. To further enable a pre-trained large language model to progressively acquire tool semantics, we construct an easy-to-hard curriculum consisting of three tasks: token definition question-answering, pure text-guided tool selection, and simplified visual pathfinding. Extensive experiments on multiple benchmarks show that ToolTok achieves superior performance among models of comparable scale (4B) and remains competitive with a substantially larger model (235B). Notably, these results are obtained using less than 1% of the training data required by other post-training approaches. In addition, ToolTok demonstrates strong generalization across unseen scenarios. Our training & inference code is open-source at https://github.com/ZephinueCode/ToolTok.

</details>


### [150] [HyPAC: Cost-Efficient LLMs-Human Hybrid Annotation with PAC Error Guarantees](https://arxiv.org/abs/2602.02550)
*Hao Zeng,Huipeng Huang,Xinhao Qu,Jianguo Huang,Bingyi Jing,Hongxin Wei*

Main category: cs.LG

TL;DR: 研究为输入选择最具成本效益标注源问题，提出HyPAC方法，在控制标注误差下降低成本，实验有效。


<details>
  <summary>Details</summary>
Motivation: 数据标注有不同成本 - 质量权衡的多源，需为输入路由到最具成本效益的标注源并控制标注误差。

Method: 提出HyPAC方法，用重要性采样和上置信界校准两个决策阈值，按不确定性划分输入为三个区域并路由到合适标注源。

Result: 实验在常见基准上证明方法有效，标注成本降低78.51%且能严格控制标注误差。

Conclusion: HyPAC能以大概率近似正确（PAC）保证实现最小预期成本，且与数据分布和预训练模型无关。

Abstract: Data annotation often involves multiple sources with different cost-quality trade-offs, such as fast large language models (LLMs), slow reasoning models, and human experts. In this work, we study the problem of routing inputs to the most cost-efficient annotation source while controlling the labeling error on test instances. We propose \textbf{HyPAC}, a method that adaptively labels inputs to the most cost-efficient annotation source while providing distribution-free guarantees on annotation error. HyPAC calibrates two decision thresholds using importance sampling and upper confidence bounds, partitioning inputs into three regions based on uncertainty and routing each to the appropriate annotation source. We prove that HyPAC achieves the minimum expected cost with a probably approximately correct (PAC) guarantee on the annotation error, free of data distribution and pre-trained models. Experiments on common benchmarks demonstrate the effectiveness of our method, reducing the annotation cost by 78.51\% while tightly controlling the annotation error.

</details>


### [151] [Evaluating LLMs When They Do Not Know the Answer: Statistical Evaluation of Mathematical Reasoning via Comparative Signals](https://arxiv.org/abs/2602.03061)
*Zihan Dong,Zhixian Zhang,Yang Zhou,Can Jin,Ruijia Wu,Linjun Zhang*

Main category: cs.LG

TL;DR: 现有大语言模型数学推理评估受基准规模和模型随机性限制，本文利用模型给出的成对比较信号，设计高效评估框架，提升了排名准确性和性能估计精度。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型数学推理评估存在基准规模有限、模型随机性大的问题，导致准确率估计方差大、排名不稳定。

Method: 设计结合标准标记结果和成对比较信号的评估框架，将比较信号作为控制变量，基于有效影响函数开发半参数估计器。

Result: 在模拟实验中大幅提高排名准确性，在GPQA Diamond、AIME 2025和GSM8K实验中实现更精确的性能估计和更可靠的模型排名。

Conclusion: 所提出的评估框架在小样本情况下能有效解决传统评估不稳定的问题，具有更好的评估效果。

Abstract: Evaluating mathematical reasoning in LLMs is constrained by limited benchmark sizes and inherent model stochasticity, yielding high-variance accuracy estimates and unstable rankings across platforms. On difficult problems, an LLM may fail to produce a correct final answer, yet still provide reliable pairwise comparison signals indicating which of two candidate solutions is better. We leverage this observation to design a statistically efficient evaluation framework that combines standard labeled outcomes with pairwise comparison signals obtained by having models judge auxiliary reasoning chains. Treating these comparison signals as control variates, we develop a semiparametric estimator based on the efficient influence function (EIF) for the setting where auxiliary reasoning chains are observed. This yields a one-step estimator that achieves the semiparametric efficiency bound, guarantees strict variance reduction over naive sample averaging, and admits asymptotic normality for principled uncertainty quantification. Across simulations, our one-step estimator substantially improves ranking accuracy, with gains increasing as model output noise grows. Experiments on GPQA Diamond, AIME 2025, and GSM8K further demonstrate more precise performance estimation and more reliable model rankings, especially in small-sample regimes where conventional evaluation is pretty unstable.

</details>


### [152] [EEO-TFV: Escape-Explore Optimizer for Web-Scale Time-Series Forecasting and Vision Analysis](https://arxiv.org/abs/2602.02551)
*Hua Wang,Jinghao Lu,Fan Zhang*

Main category: cs.LG

TL;DR: 现有Transformer基础模型在多变量长序列预测和图像任务有问题，提出轻量级Transformer架构和EEO优化器，实验显示该方法有竞争力，具泛化和稳定性。


<details>
  <summary>Details</summary>
Motivation: 解决Transformer基础模型在多变量长序列预测的误差累积、图像任务对分布外样本的脆弱性，以及大规模Web数据分析任务的优化难题。

Method: 提出轻量级Transformer架构和新型Escape - Explore Optimizer (EEO)。

Result: 在11个时间序列基准数据集和Synapse医学图像分割任务中，性能与最先进模型相当，且泛化和稳定性更优。

Conclusion: 该方法有潜力成为Web规模数据挖掘和分析的通用跨任务基础模型。

Abstract: Transformer-based foundation models have achieved remarkable progress in tasks such as time-series forecasting and image segmentation. However, they frequently suffer from error accumulation in multivariate long-sequence prediction and exhibit vulnerability to out-of-distribution samples in image-related tasks. Furthermore, these challenges become particularly pronounced in large-scale Web data analysis tasks, which typically involve complex temporal patterns and multimodal features. This complexity substantially increases optimization difficulty, rendering models prone to stagnation at saddle points within high-dimensional parameter spaces. To address these issues, we propose a lightweight Transformer architecture in conjunction with a novel Escape-Explore Optimizer (EEO). The optimizer enhances both exploration and generalization while effectively avoiding sharp minima and saddle-point traps. Experimental results show that, in representative Web data scenarios, our method achieves performance on par with state-of-the-art models across 11 time-series benchmark datasets and the Synapse medical image segmentation task. Moreover, it demonstrates superior generalization and stability, thereby validating its potential as a versatile cross-task foundation model for Web-scale data mining and analysis.

</details>


### [153] [Self-Hinting Language Models Enhance Reinforcement Learning](https://arxiv.org/abs/2602.03143)
*Baohao Liao,Hanze Dong,Xinxing Xu,Christof Monz,Jiang Bian*

Main category: cs.LG

TL;DR: 本文提出SAGE框架解决GRPO在稀疏终端奖励下的问题，实验显示SAGE性能优于GRPO。


<details>
  <summary>Details</summary>
Motivation: GRPO在稀疏终端奖励下常停滞，组内滚动更新常获相同奖励，导致相对优势瓦解和更新消失。

Method: 提出SAGE框架，在训练中注入特权提示重塑滚动更新分布，模型采样提示再生成解决方案，测试时不使用提示，且采样多样化的自提示可作为自适应课程。

Result: 在6个基准测试和3个大语言模型上实验，SAGE始终优于GRPO，在不同模型上有平均提升。

Conclusion: SAGE能有效解决GRPO在稀疏奖励下的问题，提高模型性能。代码开源。

Abstract: Group Relative Policy Optimization (GRPO) has recently emerged as a practical recipe for aligning large language models with verifiable objectives. However, under sparse terminal rewards, GRPO often stalls because rollouts within a group frequently receive identical rewards, causing relative advantages to collapse and updates to vanish. We propose self-hint aligned GRPO with privileged supervision (SAGE), an on-policy reinforcement learning framework that injects privileged hints during training to reshape the rollout distribution under the same terminal verifier reward. For each prompt $x$, the model samples a compact hint $h$ (e.g., a plan or decomposition) and then generates a solution $τ$ conditioned on $(x,h)$. Crucially, the task reward $R(x,τ)$ is unchanged; hints only increase within-group outcome diversity under finite sampling, preventing GRPO advantages from collapsing under sparse rewards. At test time, we set $h=\varnothing$ and deploy the no-hint policy without any privileged information. Moreover, sampling diverse self-hints serves as an adaptive curriculum that tracks the learner's bottlenecks more effectively than fixed hints from an initial policy or a stronger external model. Experiments over 6 benchmarks with 3 LLMs show that SAGE consistently outperforms GRPO, on average +2.0 on Llama-3.2-3B-Instruct, +1.2 on Qwen2.5-7B-Instruct and +1.3 on Qwen3-4B-Instruct. The code is available at https://github.com/BaohaoLiao/SAGE.

</details>


### [154] [BatCoder: Self-Supervised Bidirectional Code-Documentation Learning via Back-Translation](https://arxiv.org/abs/2602.02554)
*Jingwen Xu,Yiyang Lu,Zisu Huang,Changze Lv,Xiaohua Wang,Shizheng Li,Zhibo Xu,Zhengkang Guo,Zhengyuan Wang,Muzhao Tian,Xuanjing Huang,Xiaoqing Zheng*

Main category: cs.LG

TL;DR: 提出BatCoder自监督强化学习框架，用回译策略仅基于代码训练，在HumanEval和MBPP上表现出色且可扩展性好。


<details>
  <summary>Details</summary>
Motivation: 训练代码相关大模型依赖的高质量代码 - 文档对成本高且小众语言稀缺。

Method: 采用回译策略，先由代码生成文档，再用文档重构代码，以原代码和重构代码的语义相似性为隐式奖励进行强化学习。

Result: 在HumanEval和MBPP上用7B模型实现83.5%和81.0%的pass@1，优于开源基线模型。

Conclusion: 该框架能仅用代码训练模型，增加训练样本，且在训练语料规模和模型容量上有很好的扩展性。

Abstract: Training LLMs for code-related tasks typically depends on high-quality code-documentation pairs, which are costly to curate and often scarce for niche programming languages. We introduce BatCoder, a self-supervised reinforcement learning framework designed to jointly optimize code generation and documentation production. BatCoder employs a back-translation strategy: a documentation is first generated from code, and then the generated documentation is used to reconstruct the original code. The semantic similarity between the original and reconstructed code serves as an implicit reward, enabling reinforcement learning to improve the model's performance both in generating code from documentation and vice versa. This approach allows models to be trained using only code, substantially increasing the available training examples. Evaluated on HumanEval and MBPP with a 7B model, BatCoder achieved 83.5% and 81.0% pass@1, outperforming strong open-source baselines. Moreover, the framework demonstrates consistent scaling with respect to both training corpus size and model capacity.

</details>


### [155] [Learning to Explore with Parameter-Space Noise: A Deep Dive into Parameter-Space Noise for Reinforcement Learning with Verifiable Rewards](https://arxiv.org/abs/2602.02555)
*Bizhe Bai,Xinyue Wang,Peng Ye,Tao Chen*

Main category: cs.LG

TL;DR: 现有RLVR方法存在探索上限问题，本文提出PSN - RLVR方法解决该问题，在多个基准测试中表现更好。


<details>
  <summary>Details</summary>
Motivation: 现有的Reinforcement Learning with Verifiable Rewards (RLVR)方法存在探索上限，常对已有解决方案轨迹重新加权，而非发现新策略，限制了大采样预算下的收益。

Method: 提出PSN - RLVR，在生成回滚前扰动策略参数以进行时间一致的轨迹级探索；引入truncated importance sampling (TIS)缓解采样更新不匹配问题；提出基于轻量级替代指标的实时自适应噪声调度器。

Result: 基于GRPO实现的PSN - GRPO在多个数学推理基准测试和模型族中，扩大了有效推理能力边界，在大采样预算下有更高的pass - at - k，优于先前的探索导向RLVR方法。

Conclusion: PSN - RLVR能够解决RLVR的探索上限问题，且可与其他方法结合以获得更多收益。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) improves LLM reasoning, yet growing evidence indicates an exploration ceiling: it often reweights existing solution traces rather than discovering new strategies, limiting gains under large sampling budgets (e.g., pass-at-256). We address this limitation with PSN-RLVR, which perturbs policy parameters before rollout generation to induce temporally consistent, trajectory-level exploration that better preserves long-horizon chain-of-thought coherence than action-space noise. To mitigate the resulting sampling-update mismatch, we incorporate truncated importance sampling (TIS). To avoid expensive KL-based adaptive noise control, we propose a computationally efficient real-time adaptive noise scheduler driven by a lightweight surrogate that combines semantic diversity with normalized self-certainty. Instantiated on GRPO, a widely used RLVR method, PSN-GRPO consistently expands the effective reasoning capability boundary across multiple mathematical reasoning benchmarks and model families, yielding higher pass-at-k under large sampling budgets and outperforming prior exploration-oriented RLVR methods (e.g., Pass-at-k-style training) while remaining orthogonal and thus composable for additional gains.

</details>


### [156] [Beyond Experience Retrieval: Learning to Generate Utility-Optimized Structured Experience for Frozen LLMs](https://arxiv.org/abs/2602.02556)
*Xuancheng Li,Haitao Li,Yujia Zhou,Yiqun Liu,Qingyao Ai*

Main category: cs.LG

TL;DR: 提出SEAM插件指导大语言模型，实验显示有准确率提升。


<details>
  <summary>Details</summary>
Motivation: 大语言模型静态、重复推理或犯错，现有经验复用依赖外部检索有缺陷。

Method: 引入SEAM插件，通过执行器滚动和GRPO训练，部署后可在成功轨迹上监督微调。

Result: 数学推理基准实验显示各执行器准确率提升且开销低。

Conclusion: SEAM有效且鲁棒，消融和分析阐明其机制。

Abstract: Large language models (LLMs) are largely static and often redo reasoning or repeat mistakes. Prior experience reuse typically relies on external retrieval, which is similarity-based, can introduce noise, and adds latency. We introduce SEAM (Structured Experience Adapter Module), a lightweight, executor-specific plug-in that stores experience in its parameters and generates a structured, instance-tailored experience entry in a single forward pass to guide a frozen LLM executor. SEAM is trained for utility via executor rollouts and GRPO while keeping the executor frozen, and it can be further improved after deployment with supervised fine-tuning on logged successful trajectories. Experiments on mathematical reasoning benchmarks show consistent accuracy gains across executors with low overhead. Extensive ablations and analyses further elucidate the mechanisms underlying SEAM's effectiveness and robustness.

</details>


### [157] [daVinci-Agency: Unlocking Long-Horizon Agency Data-Efficiently](https://arxiv.org/abs/2602.02619)
*Mohan Jiang,Dayuan Fu,Junhao Shi,Ji Zeng,Weiye Si,Keyu Li,Xuefeng Li,Yang Xiao,Wenjie Li,Dequan Wang,Pengfei Liu*

Main category: cs.LG

TL;DR: 将大语言模型扩展到长期代理工作流具有挑战，文章提出 daVinci - Agency 方法，从 PR 序列挖掘结构监督，数据高效且在基准测试中有提升。


<details>
  <summary>Details</summary>
Motivation: 大语言模型扩展到长期代理工作流存在挑战，现有数据合成方法无法提供可扩展、高质量的监督。

Method: 提出 daVinci - Agency，通过连续提交进行渐进式任务分解、通过统一功能目标进行长期一致性强制、从真实修复轨迹进行可验证改进，从 PR 链中挖掘结构监督。

Result: 生成的轨迹数据量大且数据高效，微调 GLM - 4.6 在基准测试中有广泛提升，在 Toolathlon 上相对增益 47%。

Conclusion: 未在给定内容明确给出，但方法在基准测试表现良好且有进一步分析。

Abstract: While Large Language Models (LLMs) excel at short-term tasks, scaling them to long-horizon agentic workflows remains challenging. The core bottleneck lies in the scarcity of training data that captures authentic long-dependency structures and cross-stage evolutionary dynamics--existing synthesis methods either confine to single-feature scenarios constrained by model distribution, or incur prohibitive human annotation costs, failing to provide scalable, high-quality supervision. We address this by reconceptualizing data synthesis through the lens of real-world software evolution. Our key insight: Pull Request (PR) sequences naturally embody the supervision signals for long-horizon learning. They decompose complex objectives into verifiable submission units, maintain functional coherence across iterations, and encode authentic refinement patterns through bug-fix histories. Building on this, we propose daVinci-Agency, which systematically mines structured supervision from chain-of-PRs through three interlocking mechanisms: (1) progressive task decomposition via continuous commits, (2) long-term consistency enforcement through unified functional objectives, and (3) verifiable refinement from authentic bug-fix trajectories. Unlike synthetic trajectories that treat each step independently, daVinci-Agency's PR-grounded structure inherently preserves the causal dependencies and iterative refinements essential for teaching persistent goal-directed behavior and enables natural alignment with project-level, full-cycle task modeling. The resulting trajectories are substantial--averaging 85k tokens and 116 tool calls--yet remarkably data-efficient: fine-tuning GLM-4.6 on 239 daVinci-Agency samples yields broad improvements across benchmarks, notably achieving a 47% relative gain on Toolathlon. Beyond benchmark performance, our analysis confirms...

</details>


### [158] [The Alignment Curse: Cross-Modality Jailbreak Transfer in Omni-Models](https://arxiv.org/abs/2602.02557)
*Yupeng Chen,Junchi Yu,Aoxi Liu,Philip Torr,Adel Bibi*

Main category: cs.LG

TL;DR: 本文研究越狱攻击从文本到音频的跨模态转移，发现文本转移的音频越狱效果佳，有强跨模型可迁移性。


<details>
  <summary>Details</summary>
Motivation: 端到端全模态模型发展，安全红队拓展到音频越狱攻击，但文本和音频越狱间联系待研究，且文本越狱方法成熟，基于两模态语义相似性开展研究。

Method: 分析模态对齐与跨模态越狱转移的联系，对文本越狱、文本转移的音频越狱和现有基于音频的越狱进行实证评估。

Result: 文本转移的音频越狱效果与基于音频的越狱相当，且常更好，有强跨模型可迁移性，在更严格的仅音频访问威胁模型下仍有效。

Conclusion: 文本转移的音频越狱可作为未来音频红队的简单而强大的基线。

Abstract: Recent advances in end-to-end trained omni-models have significantly improved multimodal understanding. At the same time, safety red-teaming has expanded beyond text to encompass audio-based jailbreak attacks. However, an important bridge between textual and audio jailbreaks remains underexplored. In this work, we study the cross-modality transfer of jailbreak attacks from text to audio, motivated by the semantic similarity between the two modalities and the maturity of textual jailbreak methods. We first analyze the connection between modality alignment and cross-modality jailbreak transfer, showing that strong alignment can inadvertently propagate textual vulnerabilities to the audio modality, which we term the alignment curse. Guided by this analysis, we conduct an empirical evaluation of textual jailbreaks, text-transferred audio jailbreaks, and existing audio-based jailbreaks on recent omni-models. Our results show that text-transferred audio jailbreaks perform comparably to, and often better than, audio-based jailbreaks, establishing them as simple yet powerful baselines for future audio red-teaming. We further demonstrate strong cross-model transferability and show that text-transferred audio attacks remain effective even under a stricter audio-only access threat model.

</details>


### [159] [PA-MIL: Phenotype-Aware Multiple Instance Learning Guided by Language Prompting and Genotype-to-Phenotype Relationships](https://arxiv.org/abs/2602.02558)
*Zekang Yang,Hong Liu,Xiangdong Wang*

Main category: cs.LG

TL;DR: 提出Phenotype - Aware Multiple Instance Learning (PA - MIL)框架用于癌症亚型分类，在多数据集实验表现好且可解释性强。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在病理全切片图像分析中多以事后方式提供预测解释性，缺乏可靠且可追责的解释，因此需要新的可解释框架。

Method: 构建包含癌症相关表型及对应基因型的知识库；用表型的形态描述作为语言提示聚合特征；设计基于基因型 - 表型关系的神经网络GP - NN为PA - MIL提供多级指导。

Result: PA - MIL与现有MIL方法相比有竞争力，利用表型显著性作为证据，用线性分类器也能取得与先进方法相当的结果。

Conclusion: PA - MIL具有可靠的可解释性和可追责性。

Abstract: Deep learning has been extensively researched in the analysis of pathology whole-slide images (WSIs). However, most existing methods are limited to providing prediction interpretability by locating the model's salient areas in a post-hoc manner, failing to offer more reliable and accountable explanations. In this work, we propose Phenotype-Aware Multiple Instance Learning (PA-MIL), a novel ante-hoc interpretable framework that identifies cancer-related phenotypes from WSIs and utilizes them for cancer subtyping. To facilitate PA-MIL in learning phenotype-aware features, we 1) construct a phenotype knowledge base containing cancer-related phenotypes and their associated genotypes. 2) utilize the morphological descriptions of phenotypes as language prompting to aggregate phenotype-related features. 3) devise the Genotype-to-Phenotype Neural Network (GP-NN) grounded in genotype-to-phenotype relationships, which provides multi-level guidance for PA-MIL. Experimental results on multiple datasets demonstrate that PA-MIL achieves competitive performance compared to existing MIL methods while offering improved interpretability. PA-MIL leverages phenotype saliency as evidence and, using a linear classifier, achieves competitive results compared to state-of-the-art methods. Additionally, we thoroughly analyze the genotype-phenotype relationships, as well as cohort-level and case-level interpretability, demonstrating the reliability and accountability of PA-MIL.

</details>


### [160] [Soft-Radial Projection for Constrained End-to-End Learning](https://arxiv.org/abs/2602.03461)
*Philipp J. Schneider,Daniel Kuhn*

Main category: cs.LG

TL;DR: 提出Soft - Radial Projection层解决将硬约束集成到深度学习时现有构造层的梯度饱和问题，理论和实验证明其效果。


<details>
  <summary>Details</summary>
Motivation: 将硬约束集成到深度学习对安全关键系统至关重要，但现有将预测投影到约束边界的构造层存在梯度饱和问题。

Method: 引入Soft - Radial Projection，这是一个可微的重新参数化层，通过从欧几里得空间到可行集内部的径向映射来避免该问题。

Result: 理论证明该架构保留了通用逼近性，实证显示其在收敛行为和解决方案质量上优于现有基于优化和投影的基线方法。

Conclusion: Soft - Radial Projection能解决现有方法的优化停滞问题，且表现更好。

Abstract: Integrating hard constraints into deep learning is essential for safety-critical systems. Yet existing constructive layers that project predictions onto constraint boundaries face a fundamental bottleneck: gradient saturation. By collapsing exterior points onto lower-dimensional surfaces, standard orthogonal projections induce rank-deficient Jacobians, which nullify gradients orthogonal to active constraints and hinder optimization. We introduce Soft-Radial Projection, a differentiable reparameterization layer that circumvents this issue through a radial mapping from Euclidean space into the interior of the feasible set. This construction guarantees strict feasibility while preserving a full-rank Jacobian almost everywhere, thereby preventing the optimization stalls typical of boundary-based methods. We theoretically prove that the architecture retains the universal approximation property and empirically show improved convergence behavior and solution quality over state-of-the-art optimization- and projection-based baselines.

</details>


### [161] [Auditing Sybil: Explaining Deep Lung Cancer Risk Prediction Through Generative Interventional Attributions](https://arxiv.org/abs/2602.02560)
*Bartlomiej Sobieski,Jakub Grzywaczewski,Karol Dobiczek,Mateusz Wójcik,Tomasz Bartczak,Patryk Szatkowski,Przemysław Bombiński,Matthew Tivnan,Przemyslaw Biecek*

Main category: cs.LG

TL;DR: 肺癌死亡率高促使自动化筛查工具发展，提出S(H)NAP框架审计Sybil模型，发现其存在关键缺陷。


<details>
  <summary>Details</summary>
Motivation: 肺癌是癌症死亡主要原因，现有自动化筛查工具Sybil评估依赖观察指标，忽视推理机制，需因果验证。

Method: 提出模型无关审计框架S(H)NAP，利用3D扩散桥建模修改解剖特征，分离对象特定因果贡献。

Result: 对Sybil进行干预审计，发现模型虽有类似专家表现，但存在对临床不合理伪影敏感和径向偏差等关键失败模式。

Conclusion: 在临床部署Sybil前需通过因果验证确保决策可靠。

Abstract: Lung cancer remains the leading cause of cancer mortality, driving the development of automated screening tools to alleviate radiologist workload. Standing at the frontier of this effort is Sybil, a deep learning model capable of predicting future risk solely from computed tomography (CT) with high precision. However, despite extensive clinical validation, current assessments rely purely on observational metrics. This correlation-based approach overlooks the model's actual reasoning mechanism, necessitating a shift to causal verification to ensure robust decision-making before clinical deployment. We propose S(H)NAP, a model-agnostic auditing framework that constructs generative interventional attributions validated by expert radiologists. By leveraging realistic 3D diffusion bridge modeling to systematically modify anatomical features, our approach isolates object-specific causal contributions to the risk score. Providing the first interventional audit of Sybil, we demonstrate that while the model often exhibits behavior akin to an expert radiologist, differentiating malignant pulmonary nodules from benign ones, it suffers from critical failure modes, including dangerous sensitivity to clinically unjustified artifacts and a distinct radial bias.

</details>


### [162] [A General ReLearner: Empowering Spatiotemporal Prediction by Re-learning Input-label Residual](https://arxiv.org/abs/2602.02563)
*Jiaming Ma,Binwu Wang,Pengkun Wang,Xu Wang,Zhengyang Zhou,Yang Wang*

Main category: cs.LG

TL;DR: 现有时空预测模型在输入和标签存在时空差异时表现不佳，本文提出引入标签特征，基于时空残差定理设计ReLearner模块增强双向学习能力，实验证明其能提升现有STNNs预测性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有时空预测模型在输入和标签存在时空差异时表现不佳的问题。

Method: 引入时空残差定理，将传统单向时空预测范式推广为双向学习框架，设计ReLearner模块，包括残差学习模块和残差平滑模块。

Result: 在11个真实世界数据集和14个骨干模型上的实验表明，ReLearner显著提升了现有STNNs的预测性能。

Conclusion: 提出的ReLearner模块能有效增强现有STNNs的预测性能，代码已在GitHub上公开。

Abstract: Prevailing spatiotemporal prediction models typically operate under a forward (unidirectional) learning paradigm, in which models extract spatiotemporal features from historical observation input and map them to target spatiotemporal space for future forecasting (label). However, these models frequently exhibit suboptimal performance when spatiotemporal discrepancies exist between inputs and labels, for instance, when nodes with similar time-series inputs manifest distinct future labels, or vice versa. To address this limitation, we propose explicitly incorporating label features during the training phase. Specifically, we introduce the Spatiotemporal Residual Theorem, which generalizes the conventional unidirectional spatiotemporal prediction paradigm into a bidirectional learning framework. Building upon this theoretical foundation, we design an universal module, termed ReLearner, which seamlessly augments Spatiotemporal Neural Networks (STNNs) with a bidirectional learning capability via an auxiliary inverse learning process. In this process, the model relearns the spatiotemporal feature residuals between input data and future data. The proposed ReLearner comprises two critical components: (1) a Residual Learning Module, designed to effectively disentangle spatiotemporal feature discrepancies between input and label representations; and (2) a Residual Smoothing Module, employed to smooth residual terms and facilitate stable convergence. Extensive experiments conducted on 11 real-world datasets across 14 backbone models demonstrate that ReLearner significantly enhances the predictive performance of existing STNNs.Our code is available on GitHub.

</details>


### [163] [A Function-Space Stability Boundary for Generalization in Interpolating Learning Systems](https://arxiv.org/abs/2602.03514)
*Ronald Katende*

Main category: cs.LG

TL;DR: 本文研究算法稳定性对现代学习系统泛化的解释，提出条件和证书，实验验证其作用，指出稳定性非通用解释。


<details>
  <summary>Details</summary>
Motivation: 现代学习系统能在插值训练数据时良好泛化，但不清楚算法稳定性何时能解释此现象。

Method: 将训练建模为函数空间轨迹，测量沿轨迹对单样本扰动的敏感性，提出收缩传播条件和通过展开递归得到的稳定性证书。

Result: 小的证书意味着基于稳定性的泛化，存在插值区域稳定性敏感性不成立，实验表明证书增长能预测不同优化器、步长和数据集扰动下的泛化差异。

Conclusion: 该框架确定了稳定性可解释泛化的情况以及需其他机制解释成功的情况，说明稳定性不是通用解释。

Abstract: Modern learning systems often interpolate training data while still generalizing well, yet it remains unclear when algorithmic stability explains this behavior. We model training as a function-space trajectory and measure sensitivity to single-sample perturbations along this trajectory.
  We propose a contractive propagation condition and a stability certificate obtained by unrolling the resulting recursion. A small certificate implies stability-based generalization, while we also prove that there exist interpolating regimes with small risk where such contractive sensitivity cannot hold, showing that stability is not a universal explanation.
  Experiments confirm that certificate growth predicts generalization differences across optimizers, step sizes, and dataset perturbations. The framework therefore identifies regimes where stability explains generalization and where alternative mechanisms must account for success.

</details>


### [164] [Label Curation Using Agentic AI](https://arxiv.org/abs/2602.02564)
*Subhodeep Ghosh,Bayan Divaaniaazar,Md Ishat-E-Rabban,Spencer Clarke,Senjuti Basu Roy*

Main category: cs.LG

TL;DR: 提出AURA框架用于大规模多模态数据标注，在多个数据集上提升标注准确性并能评估标注者可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统以人为中心的数据标注流程成本高、速度慢且易受标注者差异影响，因此需要可靠的自动标注方法。

Method: AURA框架协调多个AI代理生成和验证标签，采用经典概率模型，通过期望最大化算法调和冲突标注并聚合噪声预测。

Result: 在四个基准数据集上，AURA比基线准确率提升最高达5.8%；在标注者质量差的挑战性设置中，提升最高达50%，且能准确估计标注者可靠性。

Conclusion: AURA框架可有效用于大规模多模态数据标注，能提升标注准确性并评估标注者质量。

Abstract: Data annotation is essential for supervised learning, yet producing accurate, unbiased, and scalable labels remains challenging as datasets grow in size and modality. Traditional human-centric pipelines are costly, slow, and prone to annotator variability, motivating reliability-aware automated annotation. We present AURA (Agentic AI for Unified Reliability Modeling and Annotation Aggregation), an agentic AI framework for large-scale, multi-modal data annotation. AURA coordinates multiple AI agents to generate and validate labels without requiring ground truth. At its core, AURA adapts a classical probabilistic model that jointly infers latent true labels and annotator reliability via confusion matrices, using Expectation-Maximization to reconcile conflicting annotations and aggregate noisy predictions. Across the four benchmark datasets evaluated, AURA achieves accuracy improvements of up to 5.8% over baseline. In more challenging settings with poor quality annotators, the improvement is up to 50% over baseline. AURA also accurately estimates the reliability of annotators, allowing assessment of annotator quality even without any pre-validation steps.

</details>


### [165] [Riemannian Neural Optimal Transport](https://arxiv.org/abs/2602.03566)
*Alessandro Micheli,Yueqi Cao,Anthea Monod,Samir Bhatt*

Main category: cs.LG

TL;DR: 论文指出现有神经OT方法适用于欧氏几何，扩展到高维黎曼流形有挑战，提出RNOT映射，证明其复杂度优势，实验显示性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有神经OT方法针对欧氏几何，扩展到高维黎曼流形是挑战，且离散近似传输映射有维度灾难问题。

Method: 引入Riemannian Neural OT (RNOT) 映射，对OT映射进行连续神经网络参数化，避免离散化并融入几何结构。

Result: 实验表明，在合成和真实数据集上，与基于离散化的基线方法相比，RNOT映射具有更好的可扩展性和竞争力。

Conclusion: RNOT映射能以亚指数复杂度近似黎曼OT映射，避免了离散化方法的维度灾难问题。

Abstract: Computational optimal transport (OT) offers a principled framework for generative modeling. Neural OT methods, which use neural networks to learn an OT map (or potential) from data in an amortized way, can be evaluated out of sample after training, but existing approaches are tailored to Euclidean geometry. Extending neural OT to high-dimensional Riemannian manifolds remains an open challenge. In this paper, we prove that any method for OT on manifolds that produces discrete approximations of transport maps necessarily suffers from the curse of dimensionality: achieving a fixed accuracy requires a number of parameters that grows exponentially with the manifold dimension. Motivated by this limitation, we introduce Riemannian Neural OT (RNOT) maps, which are continuous neural-network parameterizations of OT maps on manifolds that avoid discretization and incorporate geometric structure by construction. Under mild regularity assumptions, we prove that RNOT maps approximate Riemannian OT maps with sub-exponential complexity in the dimension. Experiments on synthetic and real datasets demonstrate improved scalability and competitive performance relative to discretization-based baselines.

</details>


### [166] [High Rank Matrix Completion via Grassmannian Proxy Fusion](https://arxiv.org/abs/2602.02565)
*Huanran Li,Jeremy Johnson,Daniel Pimentel-Alarcón*

Main category: cs.LG

TL;DR: 本文提出新方法解决高秩矩阵补全问题，实验表明在低采样率下表现更好，缩小与理论采样极限差距。


<details>
  <summary>Details</summary>
Motivation: 现有高秩矩阵补全方法缺乏理论支持、结果难以解释且所需样本多于理论必要数量。

Method: 通过对代理子空间分组来聚类不完整向量，并在格拉斯曼流形上最小化点到子空间的弦距离和子空间间的测地距离这两个标准。

Result: 在合成和真实数据集实验中，高采样率下与领先方法表现相当，低采样率下显著更好。

Conclusion: 所提方法缩小了高秩矩阵补全与理论采样极限的差距。

Abstract: This paper approaches high-rank matrix completion (HRMC) by filling missing entries in a data matrix where columns lie near a union of subspaces, clustering these columns, and identifying the underlying subspaces. Current methods often lack theoretical support, produce uninterpretable results, and require more samples than theoretically necessary. We propose clustering incomplete vectors by grouping proxy subspaces and minimizing two criteria over the Grassmannian: (a) the chordal distance between each point and its corresponding subspace and (b) the geodesic distances between subspaces of all data points. Experiments on synthetic and real datasets demonstrate that our method performs comparably to leading methods in high sampling rates and significantly better in low sampling rates, thus narrowing the gap to the theoretical sampling limit of HRMC.

</details>


### [167] [Bridging Online and Offline RL: Contextual Bandit Learning for Multi-Turn Code Generation](https://arxiv.org/abs/2602.03806)
*Ziru Chen,Dongdong Chen,Ruinan Jin,Yingbin Liang,Yujia Xie,Huan Sun*

Main category: cs.LG

TL;DR: 本文提出Cobalt方法结合在线和离线强化学习，用于多轮代码生成，表现优于基线方法，并缓解了模型的上下文奖励作弊问题。


<details>
  <summary>Details</summary>
Motivation: 在线强化学习训练成本高且不稳定，阻碍其在多轮代码生成等任务中的广泛应用。

Method: 将多轮代码生成建模为单步可恢复的马尔可夫决策过程，先使用参考大语言模型收集代码生成轨迹并划分为部分轨迹作为上下文提示，然后在在线策略学习中训练大语言模型通过单步代码生成完成每个部分轨迹提示，还使用扰动轨迹缓解模型的上下文奖励作弊问题。

Result: Cobalt在LiveCodeBench上优于基于GRPO和VeRPO的两个多轮在线强化学习基线方法，显著提高了R1 - Distill 8B和Qwen3 8B的Pass@1分数。

Conclusion: Cobalt是多轮代码生成等迭代决策任务的一个有前景的解决方案。

Abstract: Recently, there have been significant research interests in training large language models (LLMs) with reinforcement learning (RL) on real-world tasks, such as multi-turn code generation. While online RL tends to perform better than offline RL, its higher training cost and instability hinders wide adoption. In this paper, we build on the observation that multi-turn code generation can be formulated as a one-step recoverable Markov decision process and propose contextual bandit learning with offline trajectories (Cobalt), a new method that combines the benefits of online and offline RL. Cobalt first collects code generation trajectories using a reference LLM and divides them into partial trajectories as contextual prompts. Then, during online bandit learning, the LLM is trained to complete each partial trajectory prompt through single-step code generation. Cobalt outperforms two multi-turn online RL baselines based on GRPO and VeRPO, and substantially improves R1-Distill 8B and Qwen3 8B by up to 9.0 and 6.2 absolute Pass@1 scores on LiveCodeBench. Also, we analyze LLMs' in-context reward hacking behaviors and augment Cobalt training with perturbed trajectories to mitigate this issue. Overall, our results demonstrate Cobalt as a promising solution for iterative decision-making tasks like multi-turn code generation. Our code and data are available at https://github.com/OSU-NLP-Group/cobalt.

</details>


### [168] [A Comparative Simulation Study of the Fairness and Accuracy of Predictive Policing Systems in Baltimore City](https://arxiv.org/abs/2602.02566)
*Samin Semsar,Kiran Laxmikant Prabhu,Gabriella Waters,James Foulds*

Main category: cs.LG

TL;DR: 对巴尔的摩预测性警务技术的公平性和准确性进行综合比较模拟研究，发现其偏差情况比预想复杂，短期比热点警务更公平准确，但长期可能更差，还展示了评估方法。


<details>
  <summary>Details</summary>
Motivation: 现有预测性警务系统存在不公平问题且比较研究少、不全面，需进行综合比较研究。

Method: 对巴尔的摩预测性警务技术的公平性和准确性进行综合比较模拟研究。

Result: 预测性警务偏差情况比预想复杂，短期比热点警务更公平准确，但放大偏差更快；在巴尔的摩部分情况是白人社区过度警务。

Conclusion: 展示了针对特定城市评估和比较预测性警务系统行为趋势的方法，模拟可揭示不平等和长期趋势。

Abstract: There are ongoing discussions about predictive policing systems, such as those deployed in Los Angeles, California and Baltimore, Maryland, being unfair, for example, by exhibiting racial bias. Studies found that unfairness may be due to feedback loops and being trained on historically biased recorded data. However, comparative studies on predictive policing systems are few and are not sufficiently comprehensive. In this work, we perform a comprehensive comparative simulation study on the fairness and accuracy of predictive policing technologies in Baltimore. Our results suggest that the situation around bias in predictive policing is more complex than was previously assumed. While predictive policing exhibited bias due to feedback loops as was previously reported, we found that the traditional alternative, hot spots policing, had similar issues. Predictive policing was found to be more fair and accurate than hot spots policing in the short term, although it amplified bias faster, suggesting the potential for worse long-run behavior. In Baltimore, in some cases the bias in these systems tended toward over-policing in White neighborhoods, unlike in previous studies. Overall, this work demonstrates a methodology for city-specific evaluation and behavioral-tendency comparison of predictive policing systems, showing how such simulations can reveal inequities and long-term tendencies.

</details>


### [169] [Universal One-third Time Scaling in Learning Peaked Distributions](https://arxiv.org/abs/2602.03685)
*Yizhou Liu,Ziming Liu,Cengiz Pehlevan,Jeff Gore*

Main category: cs.LG

TL;DR: 研究表明大语言模型训练损失的缓慢幂律收敛源于softmax和交叉熵，最终导致损失具有1/3的通用幂律时间缩放。研究为神经缩放提供解释并提出提高训练效率的方向。


<details>
  <summary>Details</summary>
Motivation: 大语言模型训练计算成本高，损失的缓慢幂律收敛的成因存在争议。

Method: 对玩具模型进行系统分析，对大语言模型进行实证评估。

Result: 使用softmax和交叉熵在学习尖峰概率分布时，会产生幂律消失的损失和梯度，最终导致损失具有1/3的通用幂律时间缩放。

Conclusion: 为观察到的神经缩放提供了机理解释，并提出了提高大语言模型训练效率的新方向。

Abstract: Training large language models (LLMs) is computationally expensive, partly because the loss exhibits slow power-law convergence whose origin remains debatable. Through systematic analysis of toy models and empirical evaluation of LLMs, we show that this behavior can arise intrinsically from the use of softmax and cross-entropy. When learning peaked probability distributions, e.g., next-token distributions, these components yield power-law vanishing losses and gradients, creating a fundamental optimization bottleneck. This ultimately leads to power-law time scaling of the loss with a universal exponent of $1/3$. Our results provide a mechanistic explanation for observed neural scaling and suggest new directions for improving LLM training efficiency.

</details>


### [170] [IceBench-S2S: A Benchmark of Deep Learning for Challenging Subseasonal-to-Seasonal Daily Arctic Sea Ice Forecasting in Deep Latent Space](https://arxiv.org/abs/2602.02567)
*Jingyi Xu,Shengnan Wang,Weidong Yang,Siwei Tu,Lei Bai,Ben Fei*

Main category: cs.LG

TL;DR: 提出IceBench - S2S基准，促进深层学习模型预测北极海冰浓度从次季节到季节尺度的预测。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习模型对北极海冰浓度预测的有效时长局限在次季节和数月平均，阻碍实际应用，需将预测时长从次季节扩展到季节尺度。

Method: 提出通用框架，先将每日海冰数据的空间特征压缩到深层潜空间，再用深度学习预测骨干模型对时间上串联的深层特征建模。

Result: 推出IceBench - S2S综合基准，提供统一训练和评估流程。

Conclusion: IceBench - S2S为极地环境监测任务中的模型选择提供实用指导，可解决当前深度学习模型预测时长不足的问题。

Abstract: Arctic sea ice plays a critical role in regulating Earth's climate system, significantly influencing polar ecological stability and human activities in coastal regions. Recent advances in artificial intelligence have facilitated the development of skillful pan-Arctic sea ice forecasting systems, where data-driven approaches showcase tremendous potential to outperform conventional physics-based numerical models in terms of accuracy, computational efficiency and forecasting lead times. Despite the latest progress made by deep learning (DL) forecasting models, most of their skillful forecasting lead times are confined to daily subseasonal scale and monthly averaged values for up to six months, which drastically hinders their deployment for real-world applications, e.g., maritime routine planning for Arctic transportation and scientific investigation. Extending daily forecasts from subseasonal to seasonal (S2S) scale is scientifically crucial for operational applications. To bridge the gap between the forecasting lead time of current DL models and the significant daily S2S scale, we introduce IceBench-S2S, the first comprehensive benchmark for evaluating DL approaches in mitigating the challenge of forecasting Arctic sea ice concentration in successive 180-day periods. It proposes a generalized framework that first compresses spatial features of daily sea ice data into a deep latent space. The temporally concatenated deep features are subsequently modeled by DL-based forecasting backbones to predict the sea ice variation at S2S scale. IceBench-S2S provides a unified training and evaluation pipeline for different backbones, along with practical guidance for model selection in polar environmental monitoring tasks.

</details>


### [171] [Anytime Pretraining: Horizon-Free Learning-Rate Schedules with Weight Averaging](https://arxiv.org/abs/2602.03702)
*Alexandru Meterez,Pranav Ajit Nair,Depen Morwani,Cengiz Pehlevan,Sham Kakade*

Main category: cs.LG

TL;DR: 本文研究大语言模型预训练的随时学习调度，理论分析了过参数化线性回归的情况，实证表明权重平均结合无时间范围步长是余弦学习率调度的有效替代方案。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型更多在持续或开放式设置中训练，但多数预训练方案非随时可用，依赖固定预算下的时间范围相关学习率调度和大量调优。

Method: 理论分析过参数化线性回归中随时学习调度的存在性及权重平均的作用，实证对比不同参数规模语言模型在不同学习率调度下的表现。

Result: 随时调度在整个训练范围内能达到与余弦衰减相当的最终损失。

Conclusion: 权重平均结合简单、无时间范围的步长，是大语言模型预训练中余弦学习率调度的实用且有效替代方案。

Abstract: Large language models are increasingly trained in continual or open-ended settings, where the total training horizon is not known in advance. Despite this, most existing pretraining recipes are not anytime: they rely on horizon-dependent learning rate schedules and extensive tuning under a fixed compute budget. In this work, we provide a theoretical analysis demonstrating the existence of anytime learning schedules for overparameterized linear regression, and we highlight the central role of weight averaging - also known as model merging - in achieving the minimax convergence rates of stochastic gradient descent. We show that these anytime schedules polynomially decay with time, with the decay rate determined by the source and capacity conditions of the problem. Empirically, we evaluate 150M and 300M parameter language models trained at 1-32x Chinchilla scale, comparing constant learning rates with weight averaging and $1/\sqrt{t}$ schedules with weight averaging against a well-tuned cosine schedule. Across the full training range, the anytime schedules achieve comparable final loss to cosine decay. Taken together, our results suggest that weight averaging combined with simple, horizon-free step sizes offers a practical and effective anytime alternative to cosine learning rate schedules for large language model pretraining.

</details>


### [172] [Mitigating Task-Order Sensitivity and Forgetting via Hierarchical Second-Order Consolidation](https://arxiv.org/abs/2602.02568)
*Protik Nag,Krishnan Raghavan,Vignesh Narayanan*

Main category: cs.LG

TL;DR: 提出HTCL框架解决随机任务排序高方差问题，能多尺度集成知识，提升性能且降低标准差。


<details>
  <summary>Details</summary>
Motivation: 解决随机任务排序带来的高方差和任务顺序影响问题。

Method: 通过Hessian正则化的泰勒展开确定最佳组内任务序列，整合局部更新，且可扩展到L级层次。

Result: 在多种数据集和基线方法上，提升均值准确率7% - 25%，降低最终准确率标准差达68%。

Conclusion: HTCL作为模型无关的整合层能有效提升持续学习性能。

Abstract: We introduce $\textbf{Hierarchical Taylor Series-based Continual Learning (HTCL)}$, a framework that couples fast local adaptation with conservative, second-order global consolidation to address the high variance introduced by random task ordering. To address task-order effects, HTCL identifies the best intra-group task sequence and integrates the resulting local updates through a Hessian-regularized Taylor expansion, yielding a consolidation step with theoretical guarantees. The approach naturally extends to an $L$-level hierarchy, enabling multiscale knowledge integration in a manner not supported by conventional single-level CL systems. Across a wide range of datasets and replay and regularization baselines, HTCL acts as a model-agnostic consolidation layer that consistently enhances performance, yielding mean accuracy gains of $7\%$ to $25\%$ while reducing the standard deviation of final accuracy by up to $68\%$ across random task permutations.

</details>


### [173] [Trajectory Consistency for One-Step Generation on Euler Mean Flows](https://arxiv.org/abs/2602.02571)
*Zhiqi Li,Yuchen Sun,Duowen Chen,Jinjin He,Bo Zhu*

Main category: cs.LG

TL;DR: 提出Euler Mean Flows (EMF)生成框架，以最小采样成本实现长程轨迹一致性，实验显示优化稳定性、样本质量提升，训练时间和内存消耗降低。


<details>
  <summary>Details</summary>
Motivation: 解决长时尺度下轨迹一致性约束难以监督和优化的问题。

Method: 用线性替代约束取代原轨迹一致性约束，基于流模型半群公式推导近似，形成无JVP训练框架。

Result: 在图像合成、几何生成和功能生成实验中，固定采样预算下优化稳定性和样本质量提高，训练时间和内存消耗约降50%。

Conclusion: EMF框架有效，能提升生成效果并降低成本。

Abstract: We propose \emph{Euler Mean Flows (EMF)}, a flow-based generative framework for one-step and few-step generation that enforces long-range trajectory consistency with minimal sampling cost. The key idea of EMF is to replace the trajectory consistency constraint, which is difficult to supervise and optimize over long time scales, with a principled linear surrogate that enables direct data supervision for long-horizon flow-map compositions. We derive this approximation from the semigroup formulation of flow-based models and show that, under mild regularity assumptions, it faithfully approximates the original consistency objective while being substantially easier to optimize. This formulation leads to a unified, JVP-free training framework that supports both $u$-prediction and $x_1$-prediction variants, avoiding explicit Jacobian computations and significantly reducing memory and computational overhead. Experiments on image synthesis, particle-based geometry generation, and functional generation demonstrate improved optimization stability and sample quality under fixed sampling budgets, together with approximately $50\%$ reductions in training time and memory consumption compared to existing one-step methods for image generation.

</details>


### [174] [Reward Shaping for Inference-Time Alignment: A Stackelberg Game Perspective](https://arxiv.org/abs/2602.02572)
*Haichuan Wang,Tao Lin,Lingkai Kong,Ce Li,Hezi Jiang,Milind Tambe*

Main category: cs.LG

TL;DR: 现有对齐方法有局限性，通过将奖励模型优化问题形式化为Stackelberg博弈，提出简单奖励塑形方案，实验证明有效且能提升平均奖励。


<details>
  <summary>Details</summary>
Motivation: 现有直接使用奖励模型优化LLM策略的方法因KL正则化存在不足，放大奖励又有奖励破解风险，需优化奖励模型设计。

Method: 将奖励模型优化问题形式化为Stackelberg博弈，用简单奖励塑形方案近似最优奖励模型。

Result: 在推理时间对齐设置中评估，能无缝融入现有对齐方法，提升平均奖励，对所有基线的胜率平局率超66%。

Conclusion: 提出的方法有效且能以最小开销融入现有方法，提升性能。

Abstract: Existing alignment methods directly use the reward model learned from user preference data to optimize an LLM policy, subject to KL regularization with respect to the base policy. This practice is suboptimal for maximizing user's utility because the KL regularization may cause the LLM to inherit the bias in the base policy that conflicts with user preferences. While amplifying rewards for preferred outputs can mitigate this bias, it also increases the risk of reward hacking. This tradeoff motivates the problem of optimally designing reward models under KL regularization. We formalize this reward model optimization problem as a Stackelberg game, and show that a simple reward shaping scheme can effectively approximate the optimal reward model. We empirically evaluate our method in inference-time alignment settings and demonstrate that it integrates seamlessly into existing alignment methods with minimal overhead. Our method consistently improves average reward and achieves win-tie rates exceeding 66% against all baselines, averaged across evaluation settings.

</details>


### [175] [Product Interaction: An Algebraic Formalism for Deep Learning Architectures](https://arxiv.org/abs/2602.02573)
*Haonan Dong,Chun-Wun Cheng,Angelica I. Aviles-Rivero*

Main category: cs.LG

TL;DR: 本文介绍乘积交互，指出现代神经网络的代数表达式可通过线性、二次及高阶乘积交互统一构建，不同网络对应不同类型交互。


<details>
  <summary>Details</summary>
Motivation: 引入一种构建神经网络层的代数形式体系。

Method: 基于合适代数上定义的乘法算子的组合构建神经网络层，通过提高交互阶数生成和组织代数表达式。

Result: 发现现代神经网络的代数表达式可由线性、二次及高阶乘积交互统一构建，不同网络对应不同类型交互。

Conclusion: 乘积交互为构建神经网络层提供了一种原则性方法。

Abstract: In this paper, we introduce product interactions, an algebraic formalism in which neural network layers are constructed from compositions of a multiplication operator defined over suitable algebras. Product interactions provide a principled way to generate and organize algebraic expressions by increasing interaction order. Our central observation is that algebraic expressions in modern neural networks admit a unified construction in terms of linear, quadratic, and higher-order product interactions. Convolutional and equivariant networks arise as symmetry-constrained linear product interactions, while attention and Mamba correspond to higher-order product interactions.

</details>


### [176] [QuantLRM: Quantization of Large Reasoning Models via Fine-Tuning Signals](https://arxiv.org/abs/2602.02581)
*Nan Zhang,Eugene Kwek,Yusen Zhang,Muyu Pan,Suhang Wang,Prasenjit Mitra,Rui Zhang*

Main category: cs.LG

TL;DR: 研究基于推理激励微调中权重更新幅度的大语言模型量化方法QuantLRM，在多个推理基准上验证其有效性和适用性。


<details>
  <summary>Details</summary>
Motivation: 探索推理激励微调中权重更新幅度对大推理模型量化的价值。

Method: 提出“保护两端”假设，引入QuantLRM，通过拟合受限二次函数保护两端，计算通道重要性。

Result: QuantLRM在多种微调模型量化上有一致提升，在强化学习微调模型上平均提升6.55%，还支持非微调模型。

Conclusion: QuantLRM是一种有效的大推理模型量化方法，适用性强。

Abstract: Weight-only quantization is important for compressing Large Language Models (LLMs). Inspired by the spirit of classical magnitude pruning, we study whether the magnitude of weight updates during reasoning-incentivized fine-tuning can provide valuable signals for quantizing Large Reasoning Models (LRMs). We hypothesize that the smallest and largest weight updates during fine-tuning are more important than those of intermediate magnitude, a phenomenon we term "protecting both ends". Upon hypothesis validation, we introduce QuantLRM, which stands for weight quantization of LRMs via fine-tuning signals. We fit simple restricted quadratic functions on weight updates to protect both ends. By multiplying the average quadratic values with the count of zero weight updates of channels, we compute channel importance that is more effective than using activation or second-order information. We run QuantLRM to quantize various fine-tuned models (including supervised, direct preference optimization, and reinforcement learning fine-tuning) over four reasoning benchmarks (AIME-120, FOLIO, temporal sequences, and GPQA-Diamond) and empirically find that QuantLRM delivers a consistent improvement for LRMs quantization, with an average improvement of 6.55% on a reinforcement learning fine-tuned model. Also supporting non-fine-tuned LRMs, QuantLRM gathers effective signals via pseudo-fine-tuning, which greatly enhances its applicability.

</details>


### [177] [Copula-Based Aggregation and Context-Aware Conformal Prediction for Reliable Renewable Energy Forecasting](https://arxiv.org/abs/2602.02583)
*Alireza Moradi,Mathieu Tanneau,Reza Zandehshahvar,Pascal Van Hentenryck*

Main category: cs.LG

TL;DR: 本文提出校准概率聚合框架，将站点级概率预测转换为可靠的机群级预测，实验表明该方法效果好。


<details>
  <summary>Details</summary>
Motivation: 可再生能源渗透率快速增长，需要可靠的概率预测支持电网运行，但系统运营商常依赖站点级预测，构建机群级预测存在挑战。

Method: 提出校准概率聚合框架，结合基于copula的依赖建模和上下文感知共形预测（CACP）。

Result: 在大型太阳能发电数据集上，Copula+CACP方法实现接近标称的覆盖率，且预测区间更尖锐。

Conclusion: 所提出的Copula+CACP方法能有效将站点级概率预测转换为可靠的机群级预测。

Abstract: The rapid growth of renewable energy penetration has intensified the need for reliable probabilistic forecasts to support grid operations at aggregated (fleet or system) levels. In practice, however, system operators often lack access to fleet-level probabilistic models and instead rely on site-level forecasts produced by heterogeneous third-party providers. Constructing coherent and calibrated fleet-level probabilistic forecasts from such inputs remains challenging due to complex cross-site dependencies and aggregation-induced miscalibration. This paper proposes a calibrated probabilistic aggregation framework that directly converts site-level probabilistic forecasts into reliable fleet-level forecasts in settings where system-level models cannot be trained or maintained. The framework integrates copula-based dependence modeling to capture cross-site correlations with Context-Aware Conformal Prediction (CACP) to correct miscalibration at the aggregated level. This combination enables dependence-aware aggregation while providing valid coverage and maintaining sharp prediction intervals. Experiments on large-scale solar generation datasets from MISO, ERCOT, and SPP demonstrate that the proposed Copula+CACP approach consistently achieves near-nominal coverage with significantly sharper intervals than uncalibrated aggregation baselines.

</details>


### [178] [Learnable Koopman-Enhanced Transformer-Based Time Series Forecasting with Spectral Control](https://arxiv.org/abs/2602.02592)
*Ali Forootani,Raffaele Iervolino*

Main category: cs.LG

TL;DR: 提出可学习的Koopman算子参数化统一族，含四种变体，在多模型基准测试中表现优，证明其对深度预测有效稳定且有理论依据。


<details>
  <summary>Details</summary>
Motivation: 将线性动力系统理论与现代深度学习预测架构结合，解决预测领域问题。

Method: 引入四种可学习的Koopman变体，在大规模基准测试中评估，进行全谱分析。

Result: 可学习的Koopman模型实现更好的偏差 - 方差权衡，改善调节，潜动态更易解释。

Conclusion: 可学习的Koopman算子是深度预测中有效、稳定且有理论依据的组件。

Abstract: This paper proposes a unified family of learnable Koopman operator parameterizations that integrate linear dynamical systems theory with modern deep learning forecasting architectures. We introduce four learnable Koopman variants-scalar-gated, per-mode gated, MLP-shaped spectral mapping, and low-rank Koopman operators which generalize and interpolate between strictly stable Koopman operators and unconstrained linear latent dynamics. Our formulation enables explicit control over the spectrum, stability, and rank of the linear transition operator while retaining compatibility with expressive nonlinear backbones such as Patchtst, Autoformer, and Informer. We evaluate the proposed operators in a large-scale benchmark that also includes LSTM, DLinear, and simple diagonal State-Space Models (SSMs), as well as lightweight transformer variants. Experiments across multiple horizons and patch lengths show that learnable Koopman models provide a favorable bias-variance trade-off, improved conditioning, and more interpretable latent dynamics. We provide a full spectral analysis, including eigenvalue trajectories, stability envelopes, and learned spectral distributions. Our results demonstrate that learnable Koopman operators are effective, stable, and theoretically principled components for deep forecasting.

</details>


### [179] [Effective Frontiers: A Unification of Neural Scaling Laws](https://arxiv.org/abs/2602.02593)
*Jiaxuan Zou,Zixuan Gong,Ye Su,Huayi Tang,Yong Liu*

Main category: cs.LG

TL;DR: 本文提出统一框架解释神经缩放定律，推导N、D、C的缩放定律，统一相关机制。


<details>
  <summary>Details</summary>
Motivation: 现有神经缩放定律理论解释依赖特定架构或复杂核方法，缺乏直观通用性。

Method: 将一般学习任务抽象为长尾分布模式的渐进覆盖，引入有效前沿概念，证明可约损失与尾部概率质量有关。

Result: 推导出N、D、C的精确缩放定律，通过最大瓶颈原则统一相关机制，表明不同缩放定律是同一约束优化问题在不同瓶颈下的平衡解。

Conclusion: 提出的统一框架为神经缩放定律提供更通用、直观的理论解释。

Abstract: Neural scaling laws govern the prediction power-law improvement of test loss with respect to model capacity ($N$), datasize ($D$), and compute ($C$). However, existing theoretical explanations often rely on specific architectures or complex kernel methods, lacking intuitive universality. In this paper, we propose a unified framework that abstracts general learning tasks as the progressive coverage of patterns from a long-tail (Zipfian) distribution. We introduce the Effective Frontier ($k_\star$), a threshold in the pattern rank space that separates learned knowledge from the unlearned tail. We prove that reducible loss is asymptotically determined by the probability mass of the tail a resource-dependent frontier truncation. Based on our framework, we derive the precise scaling laws for $N$, $D$, and $C$, attributing them to capacity, coverage, and optimization bottlenecks, respectively. Furthermore, we unify these mechanisms via a Max-Bottleneck principle, demonstrating that the Kaplan and Chinchilla scaling laws are not contradictory, but equilibrium solutions to the same constrained optimization problem under different active bottlenecks.

</details>


### [180] [ContextEvolve: Multi-Agent Context Compression for Systems Code Optimization](https://arxiv.org/abs/2602.02597)
*Hongyuan Su,Yu Zheng,Yong Li*

Main category: cs.LG

TL;DR: 介绍了多智能体框架ContextEvolve，在严格参数盲约束下实现类似强化学习的搜索效率，在ADRS基准上表现优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型生成的系统解决方案需迭代优化，测试时强化学习在仅API访问下不可行，现有无训练进化方法存在上下文利用低效和搜索无向的问题。

Method: 引入ContextEvolve框架，将优化上下文分解为三个正交维度，通过三个智能体（Summarizer Agent、Navigator Agent、Sampler Agent）协同工作，在文本潜在空间进行原则性优化。

Result: 在ADRS基准上，ContextEvolve比现有最先进的基线性能提高33.3%，同时减少29.0%的令牌消耗。

Conclusion: ContextEvolve能有效解决现有方法在系统优化方面的问题，在严格约束下实现高效搜索。

Abstract: Large language models are transforming systems research by automating the discovery of performance-critical algorithms for computer systems. Despite plausible codes generated by LLMs, producing solutions that meet the stringent correctness and performance requirements of systems demands iterative optimization. Test-time reinforcement learning offers high search efficiency but requires parameter updates infeasible under API-only access, while existing training-free evolutionary methods suffer from inefficient context utilization and undirected search. We introduce ContextEvolve, a multi-agent framework that achieves RL-level search efficiency under strict parameter-blind constraints by decomposing optimization context into three orthogonal dimensions: a Summarizer Agent condenses semantic state via code-to-language abstraction, a Navigator Agent distills optimization direction from trajectory analysis, and a Sampler Agent curates experience distribution through prioritized exemplar retrieval. This orchestration forms a functional isomorphism with RL-mapping to state representation, policy gradient, and experience replay-enabling principled optimization in a textual latent space. On the ADRS benchmark, ContextEvolve outperforms state-of-the-art baselines by 33.3% while reducing token consumption by 29.0%. Codes for our work are released at https://anonymous.4open.science/r/ContextEvolve-ACC

</details>


### [181] [RAP: KV-Cache Compression via RoPE-Aligned Pruning](https://arxiv.org/abs/2602.02599)
*Jihao Xin,Tian Lvu,Hatem Ltaief,David Keyes,Marco Canini*

Main category: cs.LG

TL;DR: 提出RoPE对齐剪枝（RAP）方法，可同时减少KV缓存、注意力参数和FLOPs，降低注意力延迟，且保持高准确率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型长上下文推理受KV缓存的内存和计算成本瓶颈限制，现代基于RoPE的大语言模型中低秩分解的吸收方法失败，会重新引入大量内存和计算开销。

Method: 提出RoPE-Aligned Pruning（RAP）方法，修剪整个RoPE对齐的列对，以保留RoPE的2x2旋转结构，恢复B吸收并消除重建。

Result: 在LLaMA - 3 - 8B和Mistral - 7B上的评估显示，RAP可同时将KV缓存、注意力参数和FLOPs减少20 - 30%，将注意力延迟降低到基线的83%（预填充）和77%（解码），并保持强准确率。

Conclusion: RAP方法能有效解决大语言模型长上下文推理中KV缓存的内存和计算成本问题，同时保证模型的准确性。

Abstract: Long-context inference in large language models is increasingly bottlenecked by the memory and compute cost of the KV-Cache. Low-rank factorization compresses KV projections by writing $W \approx A * B$, where A produces latent KV states and B can be absorbed into downstream weights. In modern RoPE-based LLMs, this absorption fails: RoPE forces latent KV states to be reconstructed to full dimension, reintroducing substantial memory and compute overhead. We propose RoPE-Aligned Pruning (RAP), which prunes entire RoPE-aligned column pairs to preserve RoPE's 2x2 rotation structure, restore B absorption, and eliminate reconstruction. Our evaluation on LLaMA-3-8B and Mistral-7B shows that RAP enables joint reduction of KV-Cache, attention parameters, and FLOPs by 20-30%, all at once, while maintaining strong accuracy. Notably, RAP reduces attention latency to 83% (prefill) and 77% (decode) of baseline.

</details>


### [182] [Step-Wise Refusal Dynamics in Autoregressive and Diffusion Language Models](https://arxiv.org/abs/2602.02600)
*Eliron Rahimi,Elad Hirshel,Rom Himelstein,Amit LeVi,Avi Mendelson,Chaim Baskin*

Main category: cs.LG

TL;DR: 本文提出逐步拒绝动态分析框架，引入SRI信号提升语言模型安全性，其检测器轻量级且泛化性好、推理开销低。


<details>
  <summary>Details</summary>
Motivation: 当前对采样机制在塑造拒绝行为和越狱鲁棒性方面的作用理解不足。

Method: 提出逐步拒绝动态分析框架，引入SRI信号。

Result: SRI的几何结构能捕捉内部恢复动态，识别有害生成中的异常行为，可构建轻量级推理时检测器。

Conclusion: 采样策略对安全行为起核心作用，SRI信号可提升AR和DLMs的可解释性和安全性，轻量级检测器泛化性好且推理开销低。

Abstract: Diffusion language models (DLMs) have recently emerged as a promising alternative to autoregressive (AR) models, offering parallel decoding and controllable sampling dynamics while achieving competitive generation quality at scale. Despite this progress, the role of sampling mechanisms in shaping refusal behavior and jailbreak robustness remains poorly understood. In this work, we present a fundamental analytical framework for step-wise refusal dynamics, enabling comparison between AR and diffusion sampling. Our analysis reveals that the sampling strategy itself plays a central role in safety behavior, as a factor distinct from the underlying learned representations. Motivated by this analysis, we introduce the Step-Wise Refusal Internal Dynamics (SRI) signal, which supports interpretability and improved safety for both AR and DLMs. We demonstrate that the geometric structure of SRI captures internal recovery dynamics, and identifies anomalous behavior in harmful generations as cases of \emph{incomplete internal recovery} that are not observable at the text level. This structure enables lightweight inference-time detectors that generalize to unseen attacks while matching or outperforming existing defenses with over $100\times$ lower inference overhead.

</details>


### [183] [Discovering Data Manifold Geometry via Non-Contracting Flows](https://arxiv.org/abs/2602.02611)
*David Vigouroux,Lucas Drumetz,Ronan Fablet,François Rousseau*

Main category: cs.LG

TL;DR: 提出一种无监督方法构建全局参考系统，通过学习向量场得到固有坐标，理论证明能恢复全局坐标图，在合成流形和CIFAR - 10上效果良好。


<details>
  <summary>Details</summary>
Motivation: 传统等距目标隐式假设流形平坦，需要一种新方法构建全局参考系统。

Method: 在环境空间学习向量场，使样本流向共同参考点，定义固有坐标，施加非收缩约束，推导无积分目标。

Result: 在合成流形上实现正确的切线对齐和连贯的全局坐标结构，在CIFAR - 10上学习的坐标有有竞争力的下游分类性能。

Conclusion: 最小化提出的目标在全局坐标图存在时可恢复它，方法有效且可扩展。

Abstract: We introduce an unsupervised approach for constructing a global reference system by learning, in the ambient space, vector fields that span the tangent spaces of an unknown data manifold. In contrast to isometric objectives, which implicitly assume manifold flatness, our method learns tangent vector fields whose flows transport all samples to a common, learnable reference point. The resulting arc-lengths along these flows define interpretable intrinsic coordinates tied to a shared global frame. To prevent degenerate collapse, we enforce a non-shrinking constraint and derive a scalable, integration-free objective inspired by flow matching. Within our theoretical framework, we prove that minimizing the proposed objective recovers a global coordinate chart when one exists. Empirically, we obtain correct tangent alignment and coherent global coordinate structure on synthetic manifolds. We also demonstrate the scalability of our method on CIFAR-10, where the learned coordinates achieve competitive downstream classification performance.

</details>


### [184] [A Semi-Supervised Pipeline for Generalized Behavior Discovery from Animal-Borne Motion Time Series](https://arxiv.org/abs/2602.02618)
*Fatemeh Karimi Nejadasl,Judy Shamoun-Baranes,Eldar Rakhimberdiev*

Main category: cs.LG

TL;DR: 本文提出半监督发现管道用于从海鸥的短多元运动片段中进行广义行为发现，其KDE + HDR包含分数可为生态运动时间序列中的广义类别发现提供实用定量测试。


<details>
  <summary>Details</summary>
Motivation: 从动物携带的传感器学习行为分类具有挑战，如标签稀缺、类别高度不平衡和行为可能不在标注集中。

Method: 提出半监督发现管道，包括从标记子集学习嵌入函数、对标记和未标记样本的嵌入进行标签引导聚类形成候选行为组、使用包含分数判断发现的组是否为新行为。

Result: 在部分行为仅存在于未标记集的实验中恢复了独特集群，包含分数通过低重叠标记新行为，无新行为的对照组重叠率更高。

Conclusion: 基于HDR的包含分数为有限标注和严重类别不平衡下的生态运动时间序列广义类别发现提供了实用定量测试。

Abstract: Learning behavioral taxonomies from animal-borne sensors is challenging because labels are scarce, classes are highly imbalanced, and behaviors may be absent from the annotated set. We study generalized behavior discovery in short multivariate motion snippets from gulls, where each sample is a sequence with 3-axis IMU acceleration (20 Hz) and GPS speed, spanning nine expert-annotated behavior categories. We propose a semi-supervised discovery pipeline that (i) learns an embedding function from the labeled subset, (ii) performs label-guided clustering over embeddings of both labeled and unlabeled samples to form candidate behavior groups, and (iii) decides whether a discovered group is truly novel using a containment score. Our key contribution is a KDE + HDR (highest-density region) containment score that measures how much a discovered cluster distribution is contained within, or contains, each known-class distribution; the best-match containment score serves as an interpretable novelty statistic. In experiments where an entire behavior is withheld from supervision and appears only in the unlabeled pool, the method recovers a distinct cluster and the containment score flags novelty via low overlap, while a negative-control setting with no novel behavior yields consistently higher overlaps. These results suggest that HDR-based containment provides a practical, quantitative test for generalized class discovery in ecological motion time series under limited annotation and severe class imbalance.

</details>


### [185] [Learning Consistent Causal Abstraction Networks](https://arxiv.org/abs/2602.02623)
*Gabriele D'Acunto,Paolo Di Lorenzo,Sergio Barbarossa*

Main category: cs.LG

TL;DR: 本文聚焦因果人工智能，提出学习一致因果抽象网络（CAN）的方法，实验显示在CA学习任务和CAN结构恢复上表现良好。


<details>
  <summary>Details</summary>
Motivation: 为增强人工智能的可解释性、可信度和鲁棒性，利用结构因果模型（SCMs），推进网络层和上同调层因果知识的形式化。

Method: 将问题拆分为特定边缘的局部黎曼问题，避免非凸目标，提出高效搜索程序SPECTRAL求解局部问题。

Result: 在合成数据实验中，在CA学习任务上有竞争力，能成功恢复不同CAN结构。

Conclusion: 所提方法在因果抽象网络学习方面有效，可用于解决相关任务。

Abstract: Causal artificial intelligence aims to enhance explainability, trustworthiness, and robustness in AI by leveraging structural causal models (SCMs). In this pursuit, recent advances formalize network sheaves and cosheaves of causal knowledge. Pushing in the same direction, we tackle the learning of consistent causal abstraction network (CAN), a sheaf-theoretic framework where (i) SCMs are Gaussian, (ii) restriction maps are transposes of constructive linear causal abstractions (CAs) adhering to the semantic embedding principle, and (iii) edge stalks correspond--up to permutation--to the node stalks of more detailed SCMs. Our problem formulation separates into edge-specific local Riemannian problems and avoids nonconvex objectives. We propose an efficient search procedure, solving the local problems with SPECTRAL, our iterative method with closed-form updates and suitable for positive definite and semidefinite covariance matrices. Experiments on synthetic data show competitive performance in the CA learning task, and successful recovery of diverse CAN structures.

</details>


### [186] [Performance of Small Language Model Pretraining on FABRIC: An Empirical Study](https://arxiv.org/abs/2602.02632)
*Praveen Rao*

Main category: cs.LG

TL;DR: 研究小尺寸大语言模型预训练技术在免费实验平台上的表现，提出选择预训练技术的系统方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型预训练需大量算力，小尺寸模型在有限数据集下更合适，要探究其在学术免费平台上的预训练技术表现。

Method: 考虑多种并行方式及其组合进行预训练，设置不同GPU集群，研究网络延迟影响，用GPT - 2模型和开源包Alpa、Ray进行实验。

Result: Alpa集体优化算子内和算子/流水线并行的执行计划在GPU地理分布且网络延迟为几十毫秒时表现最佳。

Conclusion: 提出选择合适预训练技术以提高训练性能、降低执行时间和减少GPU数量的系统方法。

Abstract: Large language models (LLMs) require enormous computing power to pretrain on massive datasets. When limited datasets are available, smaller-sized LLMs are better choice to pretrain (on user-specified datasets) by following the scaling laws of LLMs. Using pretrained models, vector embeddings can be generated for raw data and stored using vector databases to support modern AI applications and semantic search. In this work, we investigate the performance of pretraining techniques for smaller-sized LLMs on an experimental testbed (with commodity GPUs) available to academic users at no charge. We consider data parallelism, intra-operator parallelism, and inter-operator/pipeline parallelism, and their combinations for pretraining. We set up different GPU clusters with homogeneous and heterogeneous GPU hardware. Furthermore, we investigate the impact of network latency on pretraining performance especially when GPUs are geographically distributed. We used GPT-2 medium and large models and pretrained them using open-source packages, namely, Alpa and Ray. We observed that Alpa's execution plans that collectively optimized intra-operator and inter-operator/pipeline parallelism consistently performed the best when GPUs were geographically distributed. This was especially true when the network latencies were in 10's of milliseconds. Based on the insights gained from the experiments, we propose a systematic approach for selecting the appropriate pretraining technique to achieve high training performance/lower execution time as well as to reduce the number of GPUs used.

</details>


### [187] [A Reduction from Delayed to Immediate Feedback for Online Convex Optimization with Improved Guarantees](https://arxiv.org/abs/2602.02634)
*Alexander Ryabchenko,Idan Attias,Daniel M. Roy*

Main category: cs.LG

TL;DR: 本文开发了基于延迟反馈的在线学习框架，改进了一阶和老虎机凸优化的现有结果。


<details>
  <summary>Details</summary>
Motivation: 改进现有在线学习中延迟反馈问题的结果，提升算法性能。

Method: 引入连续时间模型，将遗憾分解为独立延迟学习项和延迟诱导漂移项，得到延迟自适应约简。

Result: 对于老虎机凸优化，改进了后悔界，得到更好的结果；对于一阶反馈，通过简单统一分析得到现有最优后悔界。

Conclusion: 所提出的框架能有效处理延迟反馈问题，在老虎机凸优化和一阶反馈中都有显著性能提升。

Abstract: We develop a reduction-based framework for online learning with delayed feedback that recovers and improves upon existing results for both first-order and bandit convex optimization. Our approach introduces a continuous-time model under which regret decomposes into a delay-independent learning term and a delay-induced drift term, yielding a delay-adaptive reduction that converts any algorithm for online linear optimization into one that handles round-dependent delays. For bandit convex optimization, we significantly improve existing regret bounds, with delay-dependent terms matching state-of-the-art first-order rates. For first-order feedback, we recover state-of-the-art regret bounds via a simpler, unified analysis. Quantitatively, for bandit convex optimization we obtain $O(\sqrt{d_{\text{tot}}} + T^{\frac{3}{4}}\sqrt{k})$ regret, improving the delay-dependent term from $O(\min\{\sqrt{T d_{\text{max}}},(Td_{\text{tot}})^{\frac{1}{3}}\})$ in previous work to $O(\sqrt{d_{\text{tot}}})$. Here, $k$, $T$, $d_{\text{max}}$, and $d_{\text{tot}}$ denote the dimension, time horizon, maximum delay, and total delay, respectively. Under strong convexity, we achieve $O(\min\{σ_{\text{max}} \ln T, \sqrt{d_{\text{tot}}}\} + (T^2\ln T)^{\frac{1}{3}} {k}^{\frac{2}{3}})$, improving the delay-dependent term from $O(d_{\text{max}} \ln T)$ in previous work to $O(\min\{σ_{\text{max}} \ln T, \sqrt{d_{\text{tot}}}\})$, where $σ_{\text{max}}$ denotes the maximum number of outstanding observations and may be considerably smaller than $d_{\text{max}}$.

</details>


### [188] [hSNMF: Hybrid Spatially Regularized NMF for Image-Derived Spatial Transcriptomics](https://arxiv.org/abs/2602.02638)
*Md Ishtyaq Mahmud,Veena Kochat,Suresh Satpati,Jagan Mohan Reddy Dwarampudi,Humaira Anzum,Kunal Rai,Tania Banerjee*

Main category: cs.LG

TL;DR: 研究对Xenium平台数据进行分析，提出SNMF和hSNMF两种NMF变体进行空间转录组学分析，在胆管癌数据集上表现优于其他空间基线。


<details>
  <summary>Details</summary>
Motivation: 高分辨率空间转录组学平台数据高维性给表示学习和聚类带来挑战，需要新的分析方法。

Method: 提出Spatial NMF（SNMF），通过在空间邻域内扩散每个细胞的NMF因子向量来实现局部空间平滑；引入Hybrid Spatial NMF（hSNMF），先进行空间正则化NMF，再在整合了空间邻近性和转录组相似性的混合邻接上进行Leiden聚类。

Result: 在胆管癌数据集上，SNMF和hSNMF在空间紧凑性、聚类可分离性和生物一致性方面优于其他空间基线。

Conclusion: 提出的SNMF和hSNMF方法有效解决了高分辨率空间转录组学数据的分析问题。

Abstract: High-resolution spatial transcriptomics platforms, such as Xenium, generate single-cell images that capture both molecular and spatial context, but their extremely high dimensionality poses major challenges for representation learning and clustering. In this study, we analyze data from the Xenium platform, which captures high-resolution images of tumor microarray (TMA) tissues and converts them into cell-by-gene matrices suitable for computational analysis. We benchmark and extend nonnegative matrix factorization (NMF) for spatial transcriptomics by introducing two spatially regularized variants. First, we propose Spatial NMF (SNMF), a lightweight baseline that enforces local spatial smoothness by diffusing each cell's NMF factor vector over its spatial neighborhood. Second, we introduce Hybrid Spatial NMF (hSNMF), which performs spatially regularized NMF followed by Leiden clustering on a hybrid adjacency that integrates spatial proximity (via a contact-radius graph) and transcriptomic similarity through a tunable mixing parameter alpha. Evaluated on a cholangiocarcinoma dataset, SNMF and hSNMF achieve markedly improved spatial compactness (CHAOS < 0.004, Moran's I > 0.96), greater cluster separability (Silhouette > 0.12, DBI < 1.8), and higher biological coherence (CMC and enrichment) compared to other spatial baselines. Availability and implementation: https://github.com/ishtyaqmahmud/hSNMF

</details>


### [189] [MARA: Continuous SE(3)-Equivariant Attention for Molecular Force Fields](https://arxiv.org/abs/2602.02671)
*Francesco Leonardi,Boris Bonev,Kaspar Riesen*

Main category: cs.LG

TL;DR: 介绍了一种可提高原子模型性能的模块MARA。


<details>
  <summary>Details</summary>
Motivation: 现有的机器学习力场方法大多依赖固定角度扩展，限制了局部几何相互作用权重的灵活性。

Method: 引入了Modular Angular - Radial Attention (MARA)模块，它将球形注意力扩展到分子领域和SE(3)，可直接处理相邻原子的角度和径向坐标。

Result: 在分子基准测试中，MARA改进了能量和力的预测，减少了高误差事件，增强了鲁棒性。

Conclusion: 连续球形注意力是一种有效且可推广的几何算子，可提高原子模型的表达能力、稳定性和可靠性。

Abstract: Machine learning force fields (MLFFs) have become essential for accurate and efficient atomistic modeling. Despite their high accuracy, most existing approaches rely on fixed angular expansions, limiting flexibility in weighting local geometric interactions. We introduce Modular Angular-Radial Attention (MARA), a module that extends spherical attention -- originally developed for SO(3) tasks -- to the molecular domain and SE(3), providing an efficient approximation of equivariant interactions. MARA operates directly on the angular and radial coordinates of neighboring atoms, enabling flexible, geometrically informed, and modular weighting of local environments. Unlike existing attention mechanisms in SE(3)-equivariant architectures, MARA can be integrated in a plug-and-play manner into models such as MACE without architectural modifications. Across molecular benchmarks, MARA improves energy and force predictions, reduces high-error events, and enhances robustness. These results demonstrate that continuous spherical attention is an effective and generalizable geometric operator that increases the expressiveness, stability, and reliability of atomistic models.

</details>


### [190] [FlexRank: Nested Low-Rank Knowledge Decomposition for Adaptive Model Deployment](https://arxiv.org/abs/2602.02680)
*Riccardo Zaccone,Stefanos Laskaridis,Marco Ciccone,Samuel Horváth*

Main category: cs.LG

TL;DR: 深度神经网络训练和部署成本高，提出FlexRank方法可基于计算预算提取子模型，实现性能与成本权衡。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络规模增大，训练和部署成本高，且现有模型缺乏灵活性，难以适应不同成本预算。

Method: 提出FlexRank方法，利用低秩权重分解和基于重要性的嵌套合并，从预训练模型中提取能力递增的子模型。

Result: 实现了“一次训练，随处部署”的模式。

Conclusion: 该方法能在不从头训练的情况下，为不同成本预算实现性能和成本的平衡，推动大模型实际部署。

Abstract: The growing scale of deep neural networks, encompassing large language models (LLMs) and vision transformers (ViTs), has made training from scratch prohibitively expensive and deployment increasingly costly. These models are often used as computational monoliths with fixed cost, a rigidity that does not leverage overparametrized architectures and largely hinders adaptive deployment across different cost budgets. We argue that importance-ordered nested components can be extracted from pretrained models, and selectively activated on the available computational budget. To this end, our proposed FlexRank method leverages low-rank weight decomposition with nested, importance-based consolidation to extract submodels of increasing capabilities. Our approach enables a "train-once, deploy-everywhere" paradigm that offers a graceful trade-off between cost and performance without training from scratch for each budget - advancing practical deployment of large models.

</details>


### [191] [Expert-Data Alignment Governs Generation Quality in Decentralized Diffusion Models](https://arxiv.org/abs/2602.02685)
*Marcos Villagra,Bidhan Roy,Raihan Seraj,Zhiying Jiang*

Main category: cs.LG

TL;DR: 对去中心化扩散模型（DDMs）生成质量的影响因素进行研究，发现专家 - 数据对齐比数值稳定性更重要。


<details>
  <summary>Details</summary>
Motivation: 探究去中心化扩散模型中是什么因素决定生成质量。

Method: 对两个不同的DDM系统进行研究，采用数据 - 簇距离分析、每个专家分析和专家分歧分析。

Result: 全集成路由采样动态最稳定、数值收敛最好，但生成质量最差；专家 - 数据对齐是决定生成质量的原则。

Conclusion: DDM部署时，路由应优先考虑专家 - 数据对齐而非数值稳定性指标。

Abstract: Decentralized Diffusion Models (DDMs) route denoising through experts trained independently on disjoint data clusters, which can strongly disagree in their predictions. What governs the quality of generations in such systems? We present the first ever systematic investigation of this question. A priori, the expectation is that minimizing denoising trajectory sensitivity -- minimizing how perturbations amplify during sampling -- should govern generation quality. We demonstrate this hypothesis is incorrect: a stability-quality dissociation. Full ensemble routing, which combines all expert predictions at each step, achieves the most stable sampling dynamics and best numerical convergence while producing the worst generation quality (FID 47.9 vs. 22.6 for sparse Top-2 routing). Instead, we identify expert-data alignment as the governing principle: generation quality depends on routing inputs to experts whose training distribution covers the current denoising state. Across two distinct DDM systems, we validate expert-data alignment using (i) data-cluster distance analysis, confirming sparse routing selects experts with data clusters closest to the current denoising state, and (ii) per-expert analysis, showing selected experts produce more accurate predictions than non-selected ones, and (iii) expert disagreement analysis, showing quality degrades when experts disagree. For DDM deployment, our findings establish that routing should prioritize expert-data alignment over numerical stability metrics.

</details>


### [192] [Sparsely Supervised Diffusion](https://arxiv.org/abs/2602.02699)
*Wenshuai Zhao,Zhiyuan Li,Yi Zhao,Mohammad Hassan Vali,Martin Trapp,Joni Pajarinen,Juho Kannala,Arno Solin*

Main category: cs.LG

TL;DR: 提出稀疏监督学习解决扩散模型空间不一致问题，可掩盖98%像素，实验有竞争力且避免小数据集训练不稳定。


<details>
  <summary>Details</summary>
Motivation: 扩散模型存在空间生成不一致问题，因去噪机制的局部性导致样本局部合理但全局不一致。

Method: 提出稀疏监督学习，一种简单有效的掩码策略。

Result: 实验中可安全掩盖98%像素，有有竞争力的FID分数，避免小数据集训练不稳定。

Conclusion: 掩码策略减少记忆，促进生成时使用关键上下文信息。

Abstract: Diffusion models have shown remarkable success across a wide range of generative tasks. However, they often suffer from spatially inconsistent generation, arguably due to the inherent locality of their denoising mechanisms. This can yield samples that are locally plausible but globally inconsistent. To mitigate this issue, we propose sparsely supervised learning for diffusion models, a simple yet effective masking strategy that can be implemented with only a few lines of code. Interestingly, the experiments show that it is safe to mask up to 98\% of pixels during diffusion model training. Our method delivers competitive FID scores across experiments and, most importantly, avoids training instability on small datasets. Moreover, the masking strategy reduces memorization and promotes the use of essential contextual information during generation.

</details>


### [193] [Every Bit Counts: A Theoretical Study of Precision-Expressivity Tradeoffs in Quantized Transformers](https://arxiv.org/abs/2602.02707)
*Sayak Chakrabarti,Toniann Pitassi,Josh Alman*

Main category: cs.LG

TL;DR: 研究量化对Transformer表达能力的影响，证明存在表达能力和精度的细粒度权衡，为选择量化程度提供启发。


<details>
  <summary>Details</summary>
Motivation: 量化广泛用于加速Transformer推理，但对其表达能力的影响缺乏清晰刻画。

Method: 结合显式有限精度Transformer构造和通信复杂度下界进行证明。

Result: 证明一层softmax Transformer计算特定函数存在一位的精度阈值，解释了量化时表达能力损失现象。

Conclusion: 需要类似相等比较的任务对量化敏感，应根据具体任务中相等检查的长度选择量化精度。

Abstract: Quantization reduces the numerical precision of Transformer computations and is widely used to accelerate inference, yet its effect on expressivity remains poorly characterized. We demonstrate a fine-grained theoretical tradeoff between expressivity and precision: For every p we exhibit a function Γ, inspired by the equality function, and prove that a one-layer softmax Transformer can compute Γ, with p bits of precision, but not with p-1 bits of precision.
  This result concretely explains the widely observed phenomenon of empirical loss of expressivity when quantization is used. Practically, it suggests that tasks requiring equality-like comparisons (exact match, membership, etc.) are especially sensitive to quantization. Dropping even one bit can cross a threshold where the model cannot represent the needed comparison reliably. Thus, it paves the way for developing heuristics that will help practitioners choose how much quantization is possible: the precision should be chosen as a function of the length of equality to be checked for the specific task.
  Our proofs combine explicit finite-precision Transformer constructions with communication-complexity lower bounds, yielding a tight "one-bit" threshold.

</details>


### [194] [BinaryPPO: Efficient Policy Optimization for Binary Classification](https://arxiv.org/abs/2602.02708)
*Punya Syon Pandey,Zhijing Jin*

Main category: cs.LG

TL;DR: 本文介绍BinaryPPO框架用于二元分类，能在多种基准测试中显著提升准确率，证明基于置信度的奖励设计是SFT的有效替代方案。


<details>
  <summary>Details</summary>
Motivation: 监督微调（SFT）在标签噪声、类别不平衡或监督稀疏的现实场景中表现不佳，需要更有效的二元分类方法。

Method: 引入BinaryPPO框架，将二元分类问题重构为奖励最大化问题，利用改进的近端策略优化（PPO）和置信加权奖励函数，使模型从静态数据集学习决策策略。

Result: 在八个特定领域基准测试和多种不同架构模型中，BinaryPPO准确率提高40 - 60个百分点，最高达99%，大幅超越监督基线。

Conclusion: 基于置信度的奖励设计为二元分类提供了比SFT更稳健的替代方案。

Abstract: Supervised fine-tuning (SFT) is the standard approach for binary classification tasks such as toxicity detection, factuality verification, and causal inference. However, SFT often performs poorly in real-world settings with label noise, class imbalance, or sparse supervision. We introduce BinaryPPO, an offline reinforcement learning large language model (LLM) framework that reformulates binary classification as a reward maximization problem. Our method leverages a variant of Proximal Policy Optimization (PPO) with a confidence-weighted reward function that penalizes uncertain or incorrect predictions, enabling the model to learn robust decision policies from static datasets without online interaction. Across eight domain-specific benchmarks and multiple models with differing architectures, BinaryPPO improves accuracy by 40-60 percentage points, reaching up to 99%, substantially outperforming supervised baselines. We provide an in-depth analysis of the role of reward shaping, advantage scaling, and policy stability in enabling this improvement. Overall, we demonstrate that confidence-based reward design provides a robust alternative to SFT for binary classification. Our code is available at https://github.com/psyonp/BinaryPPO.

</details>


### [195] [Maximum Likelihood Reinforcement Learning](https://arxiv.org/abs/2602.02710)
*Fahim Tajwar,Guanning Zeng,Yueer Zhou,Yuda Song,Daman Arora,Yiding Jiang,Jeff Schneider,Ruslan Salakhutdinov,Haiwen Feng,Andrea Zanette*

Main category: cs.LG

TL;DR: 提出最大似然强化学习（MaxRL）框架，在测试中优于现有方法，是基于正确性设置中扩展强化学习训练的有前景框架。


<details>
  <summary>Details</summary>
Motivation: 观察到强化学习不最大化正确滚动的似然，仅优化低阶近似，因此想改进。

Method: 定义基于样本的目标函数族，处理不可微采样问题，用策略梯度估计器。

Result: MaxRL在测试的所有模型和任务中帕累托优于现有方法，测试时扩展效率提升达20倍，且随数据和计算资源增加扩展性更好。

Conclusion: MaxRL是基于正确性设置中扩展强化学习训练的有前景框架。

Abstract: Reinforcement learning is the method of choice to train models in sampling-based setups with binary outcome feedback, such as navigation, code generation, and mathematical problem solving. In such settings, models implicitly induce a likelihood over correct rollouts. However, we observe that reinforcement learning does not maximize this likelihood, and instead optimizes only a lower-order approximation. Inspired by this observation, we introduce Maximum Likelihood Reinforcement Learning (MaxRL), a sampling-based framework to approximate maximum likelihood using reinforcement learning techniques. MaxRL addresses the challenges of non-differentiable sampling by defining a compute-indexed family of sample-based objectives that interpolate between standard reinforcement learning and exact maximum likelihood as additional sampling compute is allocated. The resulting objectives admit a simple, unbiased policy-gradient estimator and converge to maximum likelihood optimization in the infinite-compute limit. Empirically, we show that MaxRL Pareto-dominates existing methods in all models and tasks we tested, achieving up to 20x test-time scaling efficiency gains compared to its GRPO-trained counterpart. We also observe MaxRL to scale better with additional data and compute. Our results suggest MaxRL is a promising framework for scaling RL training in correctness based settings.

</details>


### [196] [Towards Understanding Steering Strength](https://arxiv.org/abs/2602.02712)
*Magamed Taimeskhanov,Samuel Vaiter,Damien Garreau*

Main category: cs.LG

TL;DR: 本文首次对大语言模型中间潜在表征转向强度进行理论分析，并在11个语言模型上实证验证。


<details>
  <summary>Details</summary>
Motivation: 现有方法对大语言模型中间潜在表征转向的移动幅度选择缺乏理解，而其选择很重要。

Method: 对转向强度进行理论分析，刻画其对下一个标记概率、概念存在性和交叉熵的影响，推导相关定性规律。

Result: 分析揭示了包括转向强度非单调效应等意外行为。

Conclusion: 理论分析结果在11个不同规模语言模型上得到实证验证。

Abstract: A popular approach to post-training control of large language models (LLMs) is the steering of intermediate latent representations. Namely, identify a well-chosen direction depending on the task at hand and perturbs representations along this direction at inference time. While many propositions exist to pick this direction, considerably less is understood about how to choose the magnitude of the move, whereas its importance is clear: too little and the intended behavior does not emerge, too much and the model's performance degrades beyond repair. In this work, we propose the first theoretical analysis of steering strength. We characterize its effect on next token probability, presence of a concept, and cross-entropy, deriving precise qualitative laws governing these quantities. Our analysis reveals surprising behaviors, including non-monotonic effects of steering strength. We validate our theoretical predictions empirically on eleven language models, ranging from a small GPT architecture to modern models.

</details>


### [197] [Neural Probabilistic Amplitude Shaping for Nonlinear Fiber Channels](https://arxiv.org/abs/2602.02716)
*Mohammad Taha Askari,Lutz Lampe,Amirhossein Ghazisaeidi*

Main category: cs.LG

TL;DR: 提出用于相干光纤系统的神经概率幅度整形联合分布学习框架，在单跨205公里链路的双偏振64 - QAM传输上比序列选择方案有0.5 dB信噪比增益。


<details>
  <summary>Details</summary>
Motivation: 为相干光纤系统寻求更优的方案以提升性能。

Method: 引入神经概率幅度整形这一联合分布学习框架。

Result: 在单跨205公里链路的双偏振64 - QAM传输中，相比序列选择方案获得0.5 dB的信噪比增益。

Conclusion: 所提出的神经概率幅度整形方案在相干光纤系统中有较好的性能提升效果。

Abstract: We introduce neural probabilistic amplitude shaping, a joint-distribution learning framework for coherent fiber systems. The proposed scheme provides a 0.5 dB signal-to-noise ratio gain over sequence selection for dual-polarized 64-QAM transmission across a single-span 205 km link.

</details>


### [198] [Hierarchical Entity-centric Reinforcement Learning with Factored Subgoal Diffusion](https://arxiv.org/abs/2602.02722)
*Dan Haramati,Carl Qi,Tal Daniel,Amy Zhang,Aviv Tamar,George Konidaris*

Main category: cs.LG

TL;DR: 提出用于离线目标条件强化学习的分层以实体为中心框架，结合子目标分解和因子结构，提升多实体领域长程任务性能。


<details>
  <summary>Details</summary>
Motivation: 强化学习中在复杂环境实现长程目标是核心挑战，多实体领域因组合复杂性更难，目标条件强化学习在高维观测和组合状态空间下表现不佳。

Method: 采用由基于价值的目标条件强化学习智能体和因子化子目标生成条件扩散模型组成的两级层次结构，两者独立训练并通过基于价值函数的选择性子目标生成组合。

Result: 在基于图像的稀疏奖励长程任务上持续提升基础强化学习智能体性能，在最难任务上成功率提高超150%，并能泛化到更长时间范围和更多实体。

Conclusion: 所提框架有效解决多实体领域长程任务挑战，提升强化学习性能。

Abstract: We propose a hierarchical entity-centric framework for offline Goal-Conditioned Reinforcement Learning (GCRL) that combines subgoal decomposition with factored structure to solve long-horizon tasks in domains with multiple entities. Achieving long-horizon goals in complex environments remains a core challenge in Reinforcement Learning (RL). Domains with multiple entities are particularly difficult due to their combinatorial complexity. GCRL facilitates generalization across goals and the use of subgoal structure, but struggles with high-dimensional observations and combinatorial state-spaces, especially under sparse reward. We employ a two-level hierarchy composed of a value-based GCRL agent and a factored subgoal-generating conditional diffusion model. The RL agent and subgoal generator are trained independently and composed post hoc through selective subgoal generation based on the value function, making the approach modular and compatible with existing GCRL algorithms. We introduce new variations to benchmark tasks that highlight the challenges of multi-entity domains, and show that our method consistently boosts performance of the underlying RL agent on image-based long-horizon tasks with sparse rewards, achieving over 150% higher success rates on the hardest task in our suite and generalizing to increasing horizons and numbers of entities. Rollout videos are provided at: https://sites.google.com/view/hecrl

</details>


### [199] [Automated Dysphagia Screening Using Noninvasive Neck Acoustic Sensing](https://arxiv.org/abs/2602.02725)
*Jade Chng,Rong Xing,Yunfei Luo,Kristen Linnemeyer-Risser,Tauhidur Rahman,Andrew Yousef,Philip A Weissbrod*

Main category: cs.LG

TL;DR: 提出用便携式非侵入性声学传感和机器学习检测吞咽异常框架，AUC - ROC达0.904，证明非侵入性声学传感用于咽健康监测可行。


<details>
  <summary>Details</summary>
Motivation: 咽健康对人体重要，早期检测吞咽异常关键，但现有诊断方法依赖影像学或侵入性操作。

Method: 提出使用便携式非侵入性声学传感结合机器学习的自动化框架，捕捉吞咽时颈部细微声学信号识别异常模式。

Result: 在5次独立训练 - 测试分割下，AUC - ROC为0.904，有良好的测试时异常检测性能。

Conclusion: 非侵入性声学传感作为咽健康监测实用且可扩展的工具是可行的。

Abstract: Pharyngeal health plays a vital role in essential human functions such as breathing, swallowing, and vocalization. Early detection of swallowing abnormalities, also known as dysphagia, is crucial for timely intervention. However, current diagnostic methods often rely on radiographic imaging or invasive procedures. In this study, we propose an automated framework for detecting dysphagia using portable and noninvasive acoustic sensing coupled with applied machine learning. By capturing subtle acoustic signals from the neck during swallowing tasks, we aim to identify patterns associated with abnormal physiological conditions. Our approach achieves promising test-time abnormality detection performance, with an AUC-ROC of 0.904 under 5 independent train-test splits. This work demonstrates the feasibility of using noninvasive acoustic sensing as a practical and scalable tool for pharyngeal health monitoring.

</details>


### [200] [Vector Quantized Latent Concepts: A Scalable Alternative to Clustering-Based Concept Discovery](https://arxiv.org/abs/2602.02726)
*Xuemin Yu,Ankur Garg,Samira Ebrahimi Kahou,Hassan Sajjad*

Main category: cs.LG

TL;DR: 现有基于聚类的概念解释方法有局限，提出VQLC方法，可提升扩展性并保持解释质量。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型难以理解预测依赖的信息，常用聚类方法在大规模数据集上有计算 infeasibility 问题或聚类效果不佳。

Method: 提出基于VQ - VAE架构的VQLC方法，学习离散码本将连续表示映射到概念向量。

Result: VQLC提升了可扩展性，且在人类可理解的解释质量上与其他方法相当。

Conclusion: VQLC方法是一种有效的深度学习模型后验概念解释方法，能应对大规模数据集。

Abstract: Deep Learning models encode rich semantic information in their hidden representations. However, it remains challenging to understand which parts of this information models actually rely on when making predictions. A promising line of post-hoc concept-based explanation methods relies on clustering token representations. However, commonly used approaches such as hierarchical clustering are computationally infeasible for large-scale datasets, and K-Means often yields shallow or frequency-dominated clusters. We propose the vector quantized latent concept (VQLC) method, a framework built upon the vector quantized-variational autoencoder (VQ-VAE) architecture that learns a discrete codebook mapping continuous representations to concept vectors. We perform thorough evaluations and show that VQLC improves scalability while maintaining comparable quality of human-understandable explanations.

</details>


### [201] [RLAnything: Forge Environment, Policy, and Reward Model in Completely Dynamic RL System](https://arxiv.org/abs/2602.02488)
*Yinjie Wang,Tianbao Xie,Ke Shen,Mengdi Wang,Ling Yang*

Main category: cs.LG

TL;DR: 提出RLAnything框架，通过闭环优化强化学习信号，提升整体RL系统，实验显示在多任务中有显著提升。


<details>
  <summary>Details</summary>
Motivation: 为任何大语言模型或智能体场景增强强化学习系统的性能。

Method: 利用闭环优化动态构建环境、策略和奖励模型，策略用分步和结果信号的综合反馈训练，奖励模型通过一致性反馈优化，通过理论驱动的自动环境适配提升训练。

Result: 各组件持续提升整体系统，在多个代表性任务中取得显著增益，如在OSWorld提升Qwen3-VL - 8B - Thinking 9.1%等，优化的奖励模型信号优于依赖人类标签的结果。

Conclusion: RLAnything框架有效提升了强化学习系统在多种大语言模型和智能体任务中的表现。

Abstract: We propose RLAnything, a reinforcement learning framework that dynamically forges environment, policy, and reward models through closed-loop optimization, amplifying learning signals and strengthening the overall RL system for any LLM or agentic scenarios. Specifically, the policy is trained with integrated feedback from step-wise and outcome signals, while the reward model is jointly optimized via consistency feedback, which in turn further improves policy training. Moreover, our theory-motivated automatic environment adaptation improves training for both the reward and policy models by leveraging critic feedback from each, enabling learning from experience. Empirically, each added component consistently improves the overall system, and RLAnything yields substantial gains across various representative LLM and agentic tasks, boosting Qwen3-VL-8B-Thinking by 9.1% on OSWorld and Qwen2.5-7B-Instruct by 18.7% and 11.9% on AlfWorld and LiveBench, respectively. We also that optimized reward-model signals outperform outcomes that rely on human labels. Code: https://github.com/Gen-Verse/Open-AgentRL

</details>


### [202] [Search-Augmented Masked Diffusion Models for Constrained Generation](https://arxiv.org/abs/2602.02727)
*Huu Binh Ta,Michael Cardei,Alvaro Velasquez,Ferdinando Fioretto*

Main category: cs.LG

TL;DR: 本文提出无训练神经符号推理框架SearchDiff，集成搜索到逆向去噪过程，在生物设计和符号推理实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: 标准离散扩散模型训练目标基于似然，无法在推理时执行硬约束或优化不可微属性，存在局限性。

Method: 引入SearchDiff，将知情搜索集成到逆向去噪过程，在每个去噪步骤根据模型预测定义提案集，在用户指定属性满足条件下优化。

Result: 在生物设计和符号推理实验中，SearchDiff显著提高约束满足度和属性遵守度，持续优于离散扩散和自回归基线。

Conclusion: SearchDiff能有效解决标准离散扩散模型的局限性，提升性能。

Abstract: Discrete diffusion models generate sequences by iteratively denoising samples corrupted by categorical noise, offering an appealing alternative to autoregressive decoding for structured and symbolic generation. However, standard training targets a likelihood-based objective that primarily matches the data distribution and provides no native mechanism for enforcing hard constraints or optimizing non-differentiable properties at inference time. This work addresses this limitation and introduces Search-Augmented Masked Diffusion (SearchDiff), a training-free neurosymbolic inference framework that integrates informed search directly into the reverse denoising process. At each denoising step, the model predictions define a proposal set that is optimized under a user-specified property satisfaction, yielding a modified reverse transition that steers sampling toward probable and feasible solutions. Experiments in biological design and symbolic reasoning illustrate that SearchDiff substantially improves constraint satisfaction and property adherence, while consistently outperforming discrete diffusion and autoregressive baselines.

</details>


### [203] [CAPS: Unifying Attention, Recurrence, and Alignment in Transformer-based Time Series Forecasting](https://arxiv.org/abs/2602.02729)
*Viresh Pati,Yubin Kim,Vinh Pham,Jevon Twitty,Shihao Yang,Jiecheng Lu*

Main category: cs.LG

TL;DR: 提出CAPS结构化注意力机制用于时间序列预测，解耦三种时间结构，实验超基线。


<details>
  <summary>Details</summary>
Motivation: 标准softmax注意力会混淆不同时间结构，循环模型牺牲长期、顺序无关选择，需新机制。

Method: CAPS在单个注意力层结合SO(2)旋转和三个加法门控路径，引入Clock机制进行时间加权。

Result: 在长短时预测基准实验中超越普通softmax和线性注意力机制，与七个强基线竞争表现好。

Conclusion: CAPS是一种有效的时间序列预测结构化注意力机制。

Abstract: This paper presents $\textbf{CAPS}$ (Clock-weighted Aggregation with Prefix-products and Softmax), a structured attention mechanism for time series forecasting that decouples three distinct temporal structures: global trends, local shocks, and seasonal patterns. Standard softmax attention entangles these through global normalization, while recent recurrent models sacrifice long-term, order-independent selection for order-dependent causal structure. CAPS combines SO(2) rotations for phase alignment with three additive gating paths -- Riemann softmax, prefix-product gates, and a Clock baseline -- within a single attention layer. We introduce the Clock mechanism, a learned temporal weighting that modulates these paths through a shared notion of temporal importance. Experiments on long- and short-term forecasting benchmarks surpass vanilla softmax and linear attention mechanisms and demonstrate competitive performance against seven strong baselines with linear complexity. Our code implementation is available at https://github.com/vireshpati/CAPS-Attention.

</details>


### [204] [TabPFN for Zero-shot Parametric Engineering Design Generation](https://arxiv.org/abs/2602.02735)
*Ke Wang,Yifan Tang,Nguyen Gia Hien Vu,Faez Ahmed,G. Gary Wang*

Main category: cs.LG

TL;DR: 提出基于TabPFN的零样本参数化工程设计生成框架，可减少计算开销和数据需求，在多数据集实验中效果良好。


<details>
  <summary>Details</summary>
Motivation: 现有深度生成模型用于工程设计时存在计算成本高、需大量训练数据和重新训练的问题，限制了其在实际工程设计工作流程中的适用性。

Method: 提出基于TabPFN的零样本生成框架，根据目标性能指标顺序生成设计参数。

Result: 在三个工程设计数据集上实验表明，该方法可在高度结构化参数设计空间中实现有竞争力的多样性，对采样、分辨率和几何生成参数维度变化具有鲁棒性，性能误差低，相比基于扩散的生成模型，显著降低计算开销和数据需求。

Conclusion: 零样本、数据高效的生成方法有潜力成为工程设计实用高效工具，能快速部署、灵活适应新设计场景并易于集成到实际工程工作流中。

Abstract: Deep generative models for engineering design often require substantial computational cost, large training datasets, and extensive retraining when design requirements or datasets change, limiting their applicability in real-world engineering design workflow. In this work, we propose a zero-shot generation framework for parametric engineering design based on TabPFN, enabling conditional design generation using only a limited number of reference samples and without any task-specific model training or fine-tuning. The proposed method generates design parameters sequentially conditioned on target performance indicators, providing a flexible alternative to conventional generative models. The effectiveness of the proposed approach is evaluated on three engineering design datasets, i.e., ship hull design, BlendedNet aircraft, and UIUC airfoil. Experimental results demonstrate that the proposed method achieves competitive diversity across highly structured parametric design spaces, remains robust to variations in sampling, resolution and parameter dimensionality of geometry generation, and achieves a low performance error (e.g., less than 2% in generated ship hull designs' performance). Compared with diffusion-based generative models, the proposed framework significantly reduces computational overhead and data requirements while preserving reliable generation performance. These results highlight the potential of zero-shot, data-efficient generation as a practical and efficient tool for engineering design, enabling rapid deployment, flexible adaptation to new design settings, and ease of integration into real-world engineering workflows.

</details>


### [205] [TopoPrune: Robust Data Pruning via Unified Latent Space Topology](https://arxiv.org/abs/2602.02739)
*Arjun Roy,Prajna G. Malettira,Manish Nagaraj,Kaushik Roy*

Main category: cs.LG

TL;DR: 传统几何数据剪枝方法不稳定，提出TopoPrune框架，利用拓扑解决问题，达高精度且抗噪、可跨架构转移。


<details>
  <summary>Details</summary>
Motivation: 传统几何数据剪枝方法依赖外在几何，对潜在空间扰动敏感，在跨架构转移或有特征噪声时性能下降。

Method: TopoPrune在两个尺度操作，一是用拓扑感知流形近似建立全局低维嵌入；二是用可微持久同调对流形嵌入进行局部拓扑优化，按结构复杂度对样本排序。

Result: 该方法在高剪枝率下保证高精度和高精准度，对潜在特征嵌入的噪声扰动有极强鲁棒性，在不同网络架构中有出色的可转移性。

Conclusion: 基于拓扑的框架为稳健的数据高效学习提供了有前景的方向。

Abstract: Geometric data pruning methods, while practical for leveraging pretrained models, are fundamentally unstable. Their reliance on extrinsic geometry renders them highly sensitive to latent space perturbations, causing performance to degrade during cross-architecture transfer or in the presence of feature noise. We introduce TopoPrune, a framework which resolves this challenge by leveraging topology to capture the stable, intrinsic structure of data. TopoPrune operates at two scales, (1) utilizing a topology-aware manifold approximation to establish a global low-dimensional embedding of the dataset. Subsequently, (2) it employs differentiable persistent homology to perform a local topological optimization on the manifold embeddings, ranking samples by their structural complexity. We demonstrate that our unified dual-scale topological approach ensures high accuracy and precision, particularly at significant dataset pruning rates (e.g., 90%). Furthermore, through the inherent stability properties of topology, TopoPrune is (a) exceptionally robust to noise perturbations of latent feature embeddings and (b) demonstrates superior transferability across diverse network architectures. This study demonstrates a promising avenue towards stable and principled topology-based frameworks for robust data-efficient learning.

</details>


### [206] [Entropy-Guided Dynamic Tokens for Graph-LLM Alignment in Molecular Understanding](https://arxiv.org/abs/2602.02742)
*Zihao Jing,Qiuhao Zeng,Ruiyi Fang,Yan Sun,Boyu Wang,Pingzhao Hu*

Main category: cs.LG

TL;DR: 现有图 - 大语言模型桥接方法有局限，本文提出EDT - Former，实现高效微调并取得SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 大语言模型难以有效理解分子图，现有图 - 大语言模型桥接方法存在忽视立体化学和子结构上下文、需昂贵微调等问题。

Method: 引入EDT - Former，一种熵引导的动态令牌变换器，生成与信息丰富的分子块对齐的令牌，保留局部和全局结构特征。

Result: 在MoleculeQA、Molecule - oriented Mol - Instructions和属性预测基准测试中达到了最先进水平。

Conclusion: EDT - Former可实现可扩展和可泛化的多模态分子理解，计算效率高。

Abstract: Molecular understanding is central to advancing areas such as scientific discovery, yet Large Language Models (LLMs) struggle to understand molecular graphs effectively. Existing graph-LLM bridges often adapt the Q-Former-style connector with fixed-length static tokens, which is originally designed for vision tasks. These designs overlook stereochemistry and substructural context and typically require costly LLM-backbone fine-tuning, limiting efficiency and generalization. We introduce EDT-Former, an Entropy-guided Dynamic Token Transformer that generates tokens aligned with informative molecular patches, thereby preserving both local and global structural features for molecular graph understanding. Beyond prior approaches, EDT-Former enables alignment between frozen graph encoders and LLMs without tuning the LLM backbone (excluding the embedding layer), resulting in computationally efficient finetuning, and achieves stateof-the-art results on MoleculeQA, Molecule-oriented Mol-Instructions, and property prediction benchmarks (TDC, MoleculeNet), underscoring its effectiveness for scalable and generalizable multimodal molecular understanding

</details>


### [207] [On the Sample Efficiency of Inverse Dynamics Models for Semi-Supervised Imitation Learning](https://arxiv.org/abs/2602.02762)
*Sacha Morin,Moonsub Byeon,Alexia Jolicoeur-Martineau,Sébastien Lachapelle*

Main category: cs.LG

TL;DR: 本文探讨半监督模仿学习中基于IDM的策略，揭示VM - IDM和IDM标记在极限情况学习相同策略，分析其优势原因并改进LAPO算法。


<details>
  <summary>Details</summary>
Motivation: 理解半监督模仿学习中基于IDM的策略优势来源，并改进现有算法。

Method: 基于统计学习理论分析，开展新实验，包括使用UVA架构研究基于IDM的策略。

Result: 证明VM - IDM和IDM标记在极限情况学习相同策略，指出基于IDM的策略优势源于IDM学习的样本效率高，其有两个原因。

Conclusion: 提出改进版的LAPO算法用于潜在动作策略学习。

Abstract: Semi-supervised imitation learning (SSIL) consists in learning a policy from a small dataset of action-labeled trajectories and a much larger dataset of action-free trajectories. Some SSIL methods learn an inverse dynamics model (IDM) to predict the action from the current state and the next state. An IDM can act as a policy when paired with a video model (VM-IDM) or as a label generator to perform behavior cloning on action-free data (IDM labeling). In this work, we first show that VM-IDM and IDM labeling learn the same policy in a limit case, which we call the IDM-based policy. We then argue that the previously observed advantage of IDM-based policies over behavior cloning is due to the superior sample efficiency of IDM learning, which we attribute to two causes: (i) the ground-truth IDM tends to be contained in a lower complexity hypothesis class relative to the expert policy, and (ii) the ground-truth IDM is often less stochastic than the expert policy. We argue these claims based on insights from statistical learning theory and novel experiments, including a study of IDM-based policies using recent architectures for unified video-action prediction (UVA). Motivated by these insights, we finally propose an improved version of the existing LAPO algorithm for latent action policy learning.

</details>


### [208] [Exposing Vulnerabilities in Explanation for Time Series Classifiers via Dual-Target Attacks](https://arxiv.org/abs/2602.02763)
*Bohan Wang,Zewen Liu,Lu Lin,Hui Liu,Li Xiong,Ming Jin,Wei Jin*

Main category: cs.LG

TL;DR: 研究表明时间序列深度学习系统中，解释稳定性不能代表决策鲁棒性，提出TSEF攻击方法验证此观点。


<details>
  <summary>Details</summary>
Motivation: 指出可解释时间序列深度学习系统以解释的时间一致性评估鲁棒性的假设可能不成立，需验证。

Method: 提出TSEF双目标攻击方法，联合操纵分类器和解释器输出。

Result: 在多个数据集和解释器架构上，TSEF能实现目标预测改变，同时保持解释与参考一致。

Conclusion: 解释稳定性是决策鲁棒性的误导性指标，可信时间序列任务需进行耦合感知的鲁棒性评估。

Abstract: Interpretable time series deep learning systems are often assessed by checking temporal consistency on explanations, implicitly treating this as evidence of robustness. We show that this assumption can fail: Predictions and explanations can be adversarially decoupled, enabling targeted misclassification while the explanation remains plausible and consistent with a chosen reference rationale. We propose TSEF (Time Series Explanation Fooler), a dual-target attack that jointly manipulates the classifier and explainer outputs. In contrast to single-objective misclassification attacks that disrupt explanation and spread attribution mass broadly, TSEF achieves targeted prediction changes while keeping explanations consistent with the reference. Across multiple datasets and explainer backbones, our results consistently reveal that explanation stability is a misleading proxy for decision robustness and motivate coupling-aware robustness evaluations for trustworthy time series tasks.

</details>


### [209] [Privately Fine-Tuned LLMs Preserve Temporal Dynamics in Tabular Data](https://arxiv.org/abs/2602.02766)
*Lucas Rosenblatt,Peihan Liu,Ryan McKenna,Natalia Ponomareva*

Main category: cs.LG

TL;DR: 现有差分隐私合成表格数据研究忽略纵向数据集的时间复杂性，本文提出PATH框架，能有效捕捉长距离依赖，减少分布距离和状态转移错误。


<details>
  <summary>Details</summary>
Motivation: 现有研究聚焦独立同分布行，忽略纵向数据集时间复杂度，传统扁平化策略不足。

Method: 引入PATH框架，将全量表格作为合成单元，利用私有微调大语言模型的自回归能力。

Result: PATH有效捕捉传统方法遗漏的长距离依赖，相比主流边际机制，减少超60%分布距离和近50%状态转移错误，同时保持相近边际保真度。

Conclusion: PATH框架在处理纵向数据集的差分隐私合成数据方面优于传统方法。

Abstract: Research on differentially private synthetic tabular data has largely focused on independent and identically distributed rows where each record corresponds to a unique individual. This perspective neglects the temporal complexity in longitudinal datasets, such as electronic health records, where a user contributes an entire (sub) table of sequential events. While practitioners might attempt to model such data by flattening user histories into high-dimensional vectors for use with standard marginal-based mechanisms, we demonstrate that this strategy is insufficient. Flattening fails to preserve temporal coherence even when it maintains valid marginal distributions. We introduce PATH, a novel generative framework that treats the full table as the unit of synthesis and leverages the autoregressive capabilities of privately fine-tuned large language models. Extensive evaluations show that PATH effectively captures long-range dependencies that traditional methods miss. Empirically, our method reduces the distributional distance to real trajectories by over 60% and reduces state transition errors by nearly 50% compared to leading marginal mechanisms while achieving similar marginal fidelity.

</details>


### [210] [Provable Effects of Data Replay in Continual Learning: A Feature Learning Perspective](https://arxiv.org/abs/2602.02767)
*Meng Ding,Jinhui Xu,Kaiyi Ji*

Main category: cs.LG

TL;DR: 本文从特征学习角度为持续学习中的全数据重放训练提供理论框架，分析遗忘影响因素，得出关键结论并通过实验验证。


<details>
  <summary>Details</summary>
Motivation: 持续学习中全数据重放的理论有效性未被充分探索，需构建理论框架分析。

Method: 采用多视图数据模型，聚焦任务增量式二分类，分析信噪比影响，确定关键因素。

Result: 发现即使全重放也可能遗忘，足够信号积累可恢复早期任务，还发现任务排序新见解。

Conclusion: 通过理论分析和实验验证，明确信噪比是影响遗忘关键因素，合理的任务排序可防遗忘。

Abstract: Continual learning (CL) aims to train models on a sequence of tasks while retaining performance on previously learned ones. A core challenge in this setting is catastrophic forgetting, where new learning interferes with past knowledge. Among various mitigation strategies, data-replay methods, where past samples are periodically revisited, are considered simple yet effective, especially when memory constraints are relaxed. However, the theoretical effectiveness of full data replay, where all past data is accessible during training, remains largely unexplored. In this paper, we present a comprehensive theoretical framework for analyzing full data-replay training in continual learning from a feature learning perspective. Adopting a multi-view data model, we identify the signal-to-noise ratio (SNR) as a critical factor affecting forgetting. Focusing on task-incremental binary classification across $M$ tasks, our analysis verifies two key conclusions: (1) forgetting can still occur under full replay when the cumulative noise from later tasks dominates the signal from earlier ones; and (2) with sufficient signal accumulation, data replay can recover earlier tasks-even if their initial learning was poor. Notably, we uncover a novel insight into task ordering: prioritizing higher-signal tasks not only facilitates learning of lower-signal tasks but also helps prevent catastrophic forgetting. We validate our theoretical findings through synthetic and real-world experiments that visualize the interplay between signal learning and noise memorization across varying SNRs and task correlation regimes.

</details>


### [211] [BiTimeCrossNet: Time-Aware Self-Supervised Learning for Pediatric Sleep](https://arxiv.org/abs/2602.02769)
*Saurav Raj Pandey,Harlin Lee*

Main category: cs.LG

TL;DR: 提出用于长生理记录的多模态自监督学习框架BTCNet，在六项儿科睡眠数据下游任务中表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有方法多在短片段上独立训练，缺乏对片段在整个记录中时间信息的利用，需更好的多模态自监督学习框架。

Method: 提出BTCNet框架，纳入片段时间信息，通过交叉注意力学习生理信号间成对交互，无需任务标签或序列级监督。

Result: 在六项下游任务的冻结骨干线性探测中，BTCNet始终优于非时间感知变体，且能在独立数据集泛化，在呼吸相关任务表现出色。

Conclusion: BTCNet是用于长生理记录多模态自监督学习的有效框架。

Abstract: We present BiTimeCrossNet (BTCNet), a multimodal self-supervised learning framework for long physiological recordings such as overnight sleep studies. While many existing approaches train on short segments treated as independent samples, BTCNet incorporates information about when each segment occurs within its parent recording, for example within a sleep session. BTCNet further learns pairwise interactions between physiological signals via cross-attention, without requiring task labels or sequence-level supervision.
  We evaluate BTCNet on pediatric sleep data across six downstream tasks, including sleep staging, arousal detection, and respiratory event detection. Under frozen-backbone linear probing, BTCNet consistently outperforms an otherwise identical non-time-aware variant, with gains that generalize to an independent pediatric dataset. Compared to existing multimodal self-supervised sleep models, BTCNet achieves strong performance, particularly on respiration-related tasks.

</details>


### [212] [VerIde ECG Biometrics: Verification and Identification](https://arxiv.org/abs/2602.02776)
*Scagnetto Arjuna*

Main category: cs.LG

TL;DR: 研究大规模心电图生物识别，评估心电图与个体关联及匿名化风险，展示不同模型性能，表明心电图含个体特征，需考虑隐私。


<details>
  <summary>Details</summary>
Motivation: 研究心电图生物识别，评估心电图与个体的关联程度以及其匿名化可能面临的风险。

Method: 先使用基于MLP的嵌入网络处理表格特征，再采用基于嵌入的深度学习模型ArcFace，分别处理特征和心电图波形。

Result: 从表格输入到波形输入性能提升，大规模测试集上验证、识别有高指标，开放集两阶段管道有良好表现。

Conclusion: 心电图带有可测量的个体特征，基于表格特征即可再识别，嵌入模型会增强此能力，需考虑隐私影响和操作协议。

Abstract: This work studies electrocardiogram (ECG) biometrics at large scale, evaluating how strongly an ECG can be linked to an individual and, consequently, how its anonymization may be compromised. We show that identity information is already present in tabular representations (fiducial features): even a simple MLP-based embedding network yields non-trivial performance, indicating that anonymization based solely on releasing features does not guarantee privacy. We then adopt embedding-based deep learning models (ArcFace), first on features and then on ECG waveforms, showing a performance jump when moving from tabular inputs to waveforms, and a further gain with larger training sets and consistent normalization across train/val/test. On a large-scale test set, verification achieves high TAR at strict FAR thresholds (TAR=0.908 @ FAR=1e-3; TAR=0.820 @ FAR=1e-4) with EER=2.53% (all-vs-all); closed-set identification yields Rank@1=0.812 and Rank@10=0.910. In open-set, a two-stage pipeline (top-K shortlist on embeddings + re-ranking) reaches DIR@FAR up to 0.976 at FAR=1e-3 and 1e-4. Overall, the results show that ECG carries a measurable individual signature: re-identification is already possible with tabular features and is further amplified by embedding-based models, making privacy implications and realistic operational protocols essential to consider.

</details>


### [213] [Cross-Temporal Attention Fusion (CTAF) for Multimodal Physiological Signals in Self-Supervised Learning](https://arxiv.org/abs/2602.02784)
*Arian Khorasani,Théophile Demazure*

Main category: cs.LG

TL;DR: 研究EEG和外周生理数据异步时的多模态情感建模，提出CTAF方法，在K - EmoCon数据集验证效果良好，是向高效标注和可推广融合迈进的有力一步。


<details>
  <summary>Details</summary>
Motivation: 多数融合方法忽略或用高成本方法处理EEG和外周生理数据异步问题，需新的建模方法。

Method: 提出Cross - Temporal Attention Fusion (CTAF)，利用时间感知交叉注意力、轻量级融合门和对齐正则化对比目标，可选择弱监督。

Result: 在K - EmoCon数据集留一法交叉验证中，CTAF在匹配对余弦边距、跨模态标记检索上表现好，在三分类准确率和宏F1上与基线有竞争力且用少量标签。

Conclusion: CTAF是在时间异步下实现高效标注、可推广的EEG - 外周融合的有力进展。

Abstract: We study multimodal affect modeling when EEG and peripheral physiology are asynchronous, which most fusion methods ignore or handle with costly warping. We propose Cross-Temporal Attention Fusion (CTAF), a self-supervised module that learns soft bidirectional alignments between modalities and builds a robust clip embedding using time-aware cross attention, a lightweight fusion gate, and alignment-regularized contrastive objectives with optional weak supervision. On the K-EmoCon dataset, under leave-one-out cross-validation evaluation, CTAF yields higher cosine margins for matched pairs and better cross-modal token retrieval within one second, and it is competitive with the baseline on three-bin accuracy and macro-F1 while using few labels. Our contributions are a time-aware fusion mechanism that directly models correspondence, an alignment-driven self-supervised objective tailored to EEG and physiology, and an evaluation protocol that measures alignment quality itself. Our approach accounts for the coupling between the central and autonomic nervous systems in psychophysiological time series. These results indicate that CTAF is a strong step toward label-efficient, generalizable EEG-peripheral fusion under temporal asynchrony.

</details>


### [214] [LEMON: Local Explanations via Modality-aware OptimizatioN](https://arxiv.org/abs/2602.02786)
*Yu Qin,Phillip Sloan,Raul Santos-Rodriguez,Majid Mirmehdi,Telmo de Menezes e Silva Filho*

Main category: cs.LG

TL;DR: 提出LEMON框架用于多模态预测局部解释，在多任务评估中表现优，减少评估次数和运行时间。


<details>
  <summary>Details</summary>
Motivation: 现有可解释性方法存在单模态、依赖架构或计算成本高的问题。

Method: 引入LEMON框架，拟合带组结构稀疏性的单模态感知替代模型，将预测器视为黑盒。

Result: 在视觉语言问答和临床预测任务中，相比多模态基线，LEMON在基于删除的忠实度上有竞争力，减少黑盒评估35 - 67倍，运行时间减少2 - 8倍。

Conclusion: LEMON是一种有效、计算高效的多模态预测局部解释框架。

Abstract: Multimodal models are ubiquitous, yet existing explainability methods are often single-modal, architecture-dependent, or too computationally expensive to run at scale. We introduce LEMON (Local Explanations via Modality-aware OptimizatioN), a model-agnostic framework for local explanations of multimodal predictions. LEMON fits a single modality-aware surrogate with group-structured sparsity to produce unified explanations that disentangle modality-level contributions and feature-level attributions. The approach treats the predictor as a black box and is computationally efficient, requiring relatively few forward passes while remaining faithful under repeated perturbations. We evaluate LEMON on vision-language question answering and a clinical prediction task with image, text, and tabular inputs, comparing against representative multimodal baselines. Across backbones, LEMON achieves competitive deletion-based faithfulness while reducing black-box evaluations by 35-67 times and runtime by 2-8 times compared to strong multimodal baselines.

</details>


### [215] [Structure-Preserving Learning Improves Geometry Generalization in Neural PDEs](https://arxiv.org/abs/2602.02788)
*Benjamin D. Shaffer,Shawn Koohy,Brooks Kinch,M. Ani Hsieh,Nathaniel Trask*

Main category: cs.LG

TL;DR: 提出Geo - NeW方法开发物理基础模型求解偏微分方程，在基准测试表现优。


<details>
  <summary>Details</summary>
Motivation: 开发能实时求解偏微分方程，在适应未见几何形状时保持结构和精度的物理基础模型。

Method: 引入General - Geometry Neural Whitney Forms (Geo - NeW)，联合学习微分算子和兼容的约简有限元空间，通过有限元外微分计算精确保留物理守恒定律，用变压器编码和有限元空间连接几何与解。

Result: 在多个稳态偏微分方程基准测试中达到了最先进的性能，在分布外几何形状上比传统基线有显著改进。

Conclusion: Geo - NeW方法为学习神经偏微分方程提供强大归纳偏置，提高对未见领域的泛化能力。

Abstract: We aim to develop physics foundation models for science and engineering that provide real-time solutions to Partial Differential Equations (PDEs) which preserve structure and accuracy under adaptation to unseen geometries. To this end, we introduce General-Geometry Neural Whitney Forms (Geo-NeW): a data-driven finite element method. We jointly learn a differential operator and compatible reduced finite element spaces defined on the underlying geometry. The resulting model is solved to generate predictions, while exactly preserving physical conservation laws through Finite Element Exterior Calculus. Geometry enters the model as a discretized mesh both through a transformer-based encoding and as the basis for the learned finite element spaces. This explicitly connects the underlying geometry and imposed boundary conditions to the solution, providing a powerful inductive bias for learning neural PDEs, which we demonstrate improves generalization to unseen domains. We provide a novel parameterization of the constitutive model ensuring the existence and uniqueness of the solution. Our approach demonstrates state-of-the-art performance on several steady-state PDE benchmarks, and provides a significant improvement over conventional baselines on out-of-distribution geometries.

</details>


### [216] [Causality--Δ: Jacobian-Based Dependency Analysis in Flow Matching Models](https://arxiv.org/abs/2602.02793)
*Reza Rezvan,Gustav Gille,Moritz Schauer,Richard Torkar*

Main category: cs.LG

TL;DR: 研究流匹配中潜在扰动的传播，通过Jacobian - vector products分析依赖结构，在不同场景验证。


<details>
  <summary>Details</summary>
Motivation: 研究流匹配中微小潜在扰动如何在流中传播及数据生成特征的依赖结构。

Method: 推导高斯和高斯混合情况下最优漂移及其Jacobian的闭式表达式，在低维合成基准测试中验证数值JVPs，在图像领域用属性分类器构建JVP估计器。

Result: 数值JVPs在低维合成基准中恢复解析Jacobians；属性级JVP估计器在MNIST和CelebA上恢复经验相关性；对小的分类器Jacobian范数进行条件处理能减少相关性。

Conclusion: 即使全局非线性流也有局部仿射结构，对小分类器Jacobian范数的条件处理与假设的共同原因结构一致，但不是正式干预。

Abstract: Flow matching learns a velocity field that transports a base distribution to data. We study how small latent perturbations propagate through these flows and show that Jacobian-vector products (JVPs) provide a practical lens on dependency structure in the generated features. We derive closed-form expressions for the optimal drift and its Jacobian in Gaussian and mixture-of-Gaussian settings, revealing that even globally nonlinear flows admit local affine structure. In low-dimensional synthetic benchmarks, numerical JVPs recover the analytical Jacobians. In image domains, composing the flow with an attribute classifier yields an attribute-level JVP estimator that recovers empirical correlations on MNIST and CelebA. Conditioning on small classifier-Jacobian norms reduces correlations in a way consistent with a hypothesized common-cause structure, while we emphasize that this conditioning is not a formal do intervention.

</details>


### [217] [Joint Learning of Hierarchical Neural Options and Abstract World Model](https://arxiv.org/abs/2602.02799)
*Wasu Top Piriyakulkij,Wolfgang Lehrach,Kevin Ellis,Kevin Murphy*

Main category: cs.LG

TL;DR: 提出AgentOWL方法，能更高效学习技能，在部分游戏上展示优势


<details>
  <summary>Details</summary>
Motivation: 当前AI需要构建能组合现有技能执行新技能的智能体，且现有无模型分层强化算法需大量数据

Method: 提出AgentOWL方法，以高效方式联合学习抽象世界模型和分层神经选项

Result: 在部分Object - Centric Atari游戏上，相比基线方法能用更少数据学习更多技能

Conclusion: 所提方法在学习技能方面更具数据效率

Abstract: Building agents that can perform new skills by composing existing skills is a long-standing goal of AI agent research. Towards this end, we investigate how to efficiently acquire a sequence of skills, formalized as hierarchical neural options. However, existing model-free hierarchical reinforcement algorithms need a lot of data. We propose a novel method, which we call AgentOWL (Option and World model Learning Agent), that jointly learns -- in a sample efficient way -- an abstract world model (abstracting across both states and time) and a set of hierarchical neural options. We show, on a subset of Object-Centric Atari games, that our method can learn more skills using much less data than baseline methods.

</details>


### [218] [From Tokens to Numbers: Continuous Number Modeling for SVG Generation](https://arxiv.org/abs/2602.02820)
*Michael Ogezi,Martin Bell,Freda Shi,Ethan Smith*

Main category: cs.LG

TL;DR: 提出连续数字建模（CNM）方法解决 SVG 图像生成问题，提升训练速度和视觉质量。


<details>
  <summary>Details</summary>
Motivation: SVG 在图像生成任务中有优势，但数值几何参数编码为长序列令牌会导致训练慢、精度低和泛化能力差的问题。

Method: 提出 CNM 方法，将数字直接建模为连续值；用 200 万个栅格到 SVG 的样本训练多模态变压器，并通过强化学习微调。

Result: 训练速度提高 30%以上，保持较高感知保真度。

Conclusion: CNM 是一种实用高效的高质量矢量生成方法，有更广泛应用潜力，代码开源。

Abstract: For certain image generation tasks, vector graphics such as Scalable Vector Graphics (SVGs) offer clear benefits such as increased flexibility, size efficiency, and editing ease, but remain less explored than raster-based approaches. A core challenge is that the numerical, geometric parameters, which make up a large proportion of SVGs, are inefficiently encoded as long sequences of tokens. This slows training, reduces accuracy, and hurts generalization. To address these problems, we propose Continuous Number Modeling (CNM), an approach that directly models numbers as first-class, continuous values rather than discrete tokens. This formulation restores the mathematical elegance of the representation by aligning the model's inputs with the data's continuous nature, removing discretization artifacts introduced by token-based encoding. We then train a multimodal transformer on 2 million raster-to-SVG samples, followed by fine-tuning via reinforcement learning using perceptual feedback to further improve visual quality. Our approach improves training speed by over 30% while maintaining higher perceptual fidelity compared to alternative approaches. This work establishes CNM as a practical and efficient approach for high-quality vector generation, with potential for broader applications. We make our code available http://github.com/mikeogezi/CNM.

</details>


### [219] [A Single Revision Step Improves Token-Efficient LLM Reasoning](https://arxiv.org/abs/2602.02828)
*Yingchuan Zhang,Terry Ma,Wenxuan Zhong,Ping Ma*

Main category: cs.LG

TL;DR: 提出无训练、仅推理的PACER框架，让推理轨迹相互‘评审’，在数学基准测试中表现良好。


<details>
  <summary>Details</summary>
Motivation: 标准聚合方法孤立评估每个轨迹，难题中模型会生成高置信度幻觉路径，导致传统投票压制正确解，需解决近失误差。

Method: 引入PACER框架，初步筛选轨迹后构建共识包，轨迹基于此包自我审查，最后通过置信加权投票得出最终预测。

Result: 在AIME和BRUMO等数学基准测试中，PACER达到或超过256样本多数投票的准确率，显著优于原始集成基线。

Conclusion: PACER将简单共识转化为协作逻辑细化过程，有效解决推理任务中的近失误差问题。

Abstract: Large language models (LLMs) achieve higher accuracy on challenging reasoning tasks by scaling test-time compute through multiple trajectory sampling. However, standard aggregation methods like majority voting or individual confidence-based filtering face a fundamental "blind spot": they evaluate each trace in isolation. As problems scale in difficulty, models often generate hallucinated paths that exhibit misleadingly high confidence, causing the true solution to be suppressed by a narrow margin in traditional voting. We ask: can we enable traces to "peer-review" each other to resolve these near-miss errors?
  We introduce Packet-Conditioned Revision (PACER), a training-free, inference-only framework that enables reasoning traces to revise their conclusions through a structured coordination step. After a preliminary screening of generated traces, PACER constructs a compact consensus packet containing (i) unique candidate answers, (ii) their aggregated confidence scores, and (iii) representative reasoning summaries for each candidate answer. Individual traces then perform a targeted self-review conditioned on this packet, allowing them to identify specific logical junctions where they diverged from the broader consensus and pivot if their original reasoning is found to be flawed. Final predictions are obtained via confidence-weighted voting over these revised trajectories. On challenging competitive math benchmarks such as AIME and BRUMO, PACER matches or exceeds the accuracy of 256-sample majority voting, significantly outperforming raw ensemble baselines by transforming simple consensus into a collaborative logical refinement process.

</details>


### [220] [SC3D: Dynamic and Differentiable Causal Discovery for Temporal and Instantaneous Graphs](https://arxiv.org/abs/2602.02830)
*Sourajit Das,Dibyajyoti Chakraborthy,Romit Maulik*

Main category: cs.LG

TL;DR: 提出SC3D框架用于从多元时间序列中发现因果结构，实验显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 从多元时间序列发现因果结构存在交互跨多延迟、可能含瞬时依赖及动态图搜索空间组合大等问题。

Method: 提出两阶段可微框架SC3D，第一阶段通过节点预测进行边预选择获取滞后和瞬时边的掩码，第二阶段通过优化含稀疏性的似然函数并对瞬时块施加无环性来细化掩码。

Result: 在合成和基准动态系统的数值实验中，SC3D相比现有时间基线方法，在恢复滞后和瞬时因果结构上稳定性更好、准确性更高。

Conclusion: SC3D在从多元时间序列发现因果结构方面有优势。

Abstract: Discovering causal structures from multivariate time series is a key problem because interactions span across multiple lags and possibly involve instantaneous dependencies. Additionally, the search space of the dynamic graphs is combinatorial in nature. In this study, we propose \textit{Stable Causal Dynamic Differentiable Discovery (SC3D)}, a two-stage differentiable framework that jointly learns lag-specific adjacency matrices and, if present, an instantaneous directed acyclic graph (DAG). In Stage 1, SC3D performs edge preselection through node-wise prediction to obtain masks for lagged and instantaneous edges, whereas Stage 2 refines these masks by optimizing a likelihood with sparsity along with enforcing acyclicity on the instantaneous block. Numerical results across synthetic and benchmark dynamical systems demonstrate that SC3D achieves improved stability and more accurate recovery of both lagged and instantaneous causal structures compared to existing temporal baselines.

</details>


### [221] [Koopman Autoencoders with Continuous-Time Latent Dynamics for Fluid Dynamics Forecasting](https://arxiv.org/abs/2602.02832)
*Rares Grozavescu,Pengyu Zhang,Etienne Meunier,Mark Girolami*

Main category: cs.LG

TL;DR: 本文引入连续时间Koopman框架用于湍流流动模拟，该方法对时间分辨率有鲁棒性，能高效长时预测，并在CFD基准测试中评估。


<details>
  <summary>Details</summary>
Motivation: 经典数据驱动替代模型在自回归滚动时难以兼顾短期精度和长期稳定性，且现有的Koopman自编码器多为离散时间设置，限制了时间灵活性。

Method: 引入连续时间Koopman框架，通过数值积分方案对潜在演化进行建模，推理时允许可变时间步长。

Result: 该方法对时间分辨率具有鲁棒性，学习到的动态与解析矩阵指数解接近，能实现高效长时预测。

Conclusion: 在经典CFD基准测试中对该方法进行了评估，展现了其准确性、稳定性和外推特性。

Abstract: Data-driven surrogate models have emerged as powerful tools for accelerating the simulation of turbulent flows. However, classical approaches which perform autoregressive rollouts often trade off between strong short-term accuracy and long-horizon stability. Koopman autoencoders, inspired by Koopman operator theory, provide a physics-based alternative by mapping nonlinear dynamics into a latent space where linear evolution is conducted. In practice, most existing formulations operate in a discrete-time setting, limiting temporal flexibility. In this work, we introduce a continuous-time Koopman framework that models latent evolution through numerical integration schemes. By allowing variable timesteps at inference, the method demonstrates robustness to temporal resolution and generalizes beyond training regimes. In addition, the learned dynamics closely adhere to the analytical matrix exponential solution, enabling efficient long-horizon forecasting. We evaluate the approach on classical CFD benchmarks and report accuracy, stability, and extrapolation properties.

</details>


### [222] [Tabula RASA: Exposing and Breaking the Relational Bottleneck in Transformers](https://arxiv.org/abs/2602.02834)
*Jonas Petersen,Camilla Mazzoleni,Riccardo Maggioni*

Main category: cs.LG

TL;DR: 传统Transformers在多跳关系推理任务表现不佳，本文提出RASA改进，经实验能有效提升性能。


<details>
  <summary>Details</summary>
Motivation: 标准Transformers在处理需要对结构化数据进行多跳关系推理的任务时存在困难，需改进。

Method: 引入RASA，添加边类型嵌入以注入关系结构到注意力分数，使用稀疏掩码限制注意力到图相邻位置。

Result: 在MetaQA和WebQuestionsSP上，RASA优于标准Transformers，以低成本达到GPT - 4水平，推理深度增加优势更明显。

Conclusion: 少量结构修改能显著提升多跳推理能力，未提供形式化学习保证。

Abstract: Transformers achieve remarkable performance across many domains, yet struggle with tasks requiring multi-hop relational reasoning over structured data. We analyze this limitation through circuit complexity: standard transformers are $\mathsf{TC}^0$-complete and require $Ω(k)$ layers for $k$-hop reasoning. We introduce RASA (Relation-Aware Sparse Attention), a minimal modification adding: (1) edge-type embeddings that inject relational structure into attention scores, and (2) sparse masking that restricts attention to graph-adjacent positions. While RASA has the same asymptotic depth requirements, sparse masking reduces the attention search space from $O(2^{n^2})$ to $O(2^m)$ patterns, and edge biases provide explicit relation routing. Empirically, on MetaQA (1/2/3-hop) and WebQuestionsSP, RASA outperforms standard transformers and matches GPT-4 at lower cost, with advantages growing with reasoning depth (+7.1 points on 3-hop). We do not claim formal learnability guarantees; the contribution is empirical validation that minimal structural modifications substantially improve multi-hop reasoning.

</details>


### [223] [Semantics-Aware Generative Latent Data Augmentation for Learning in Low-Resource Domains](https://arxiv.org/abs/2602.02841)
*Jae-Sung Bae,Minje Kim*

Main category: cs.LG

TL;DR: 提出语义感知生成式潜在数据增强框架GeLDA，在零样本语音情感识别和长尾图像分类任务验证其效果。


<details>
  <summary>Details</summary>
Motivation: 深度学习在数据稀缺场景表现不佳，基础模型在下游微调时也受限于标记数据不足。

Method: 提出GeLDA，利用条件扩散模型在基础模型诱导的潜在空间合成样本，以辅助特征向量为条件进行生成。

Result: 在零样本语言特定语音情感识别中，提高基线模型的未加权平均召回率；在长尾图像分类中，在ImageNet - LT上取得尾类准确率新的最优结果。

Conclusion: GeLDA能在数据稀缺场景实现高效、高质量的数据增强。

Abstract: Despite strong performance in data-rich regimes, deep learning often underperforms in the data-scarce settings common in practice. While foundation models (FMs) trained on massive datasets demonstrate strong generalization by extracting general-purpose features, they can still suffer from scarce labeled data during downstream fine-tuning. To address this, we propose GeLDA, a semantics-aware generative latent data augmentation framework that leverages conditional diffusion models to synthesize samples in an FM-induced latent space. Because this space is low-dimensional and concentrates task-relevant information compared to the input space, GeLDA enables efficient, high-quality data generation. GeLDA conditions generation on auxiliary feature vectors that capture semantic relationships among classes or subdomains, facilitating data augmentation in low-resource domains. We validate GeLDA in two large-scale recognition tasks: (a) in zero-shot language-specific speech emotion recognition, GeLDA improves the Whisper-large baseline's unweighted average recall by 6.13%; and (b) in long-tailed image classification, it achieves 74.7% tail-class accuracy on ImageNet-LT, setting a new state-of-the-art result.

</details>


### [224] [Causal Flow Q-Learning for Robust Offline Reinforcement Learning](https://arxiv.org/abs/2602.02847)
*Mingxuan Li,Junzhe Zhang,Elias Bareinboim*

Main category: cs.LG

TL;DR: 文章从因果角度解决离线强化学习中混杂观察问题，提出新目标和方法，实验显示其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 标准策略梯度假设数据无未测量混杂，但像素演示数据可能存在隐式混杂偏差，需解决该问题。

Method: 从因果角度研究问题，提出优化策略最坏情况性能的新目标，用深度判别器评估策略差异，学习表达性流匹配策略。

Result: 在25个基于像素的任务实验中，提出的抗混杂增强程序成功率是无混杂意识的最先进离线强化学习方法的120%。

Conclusion: 所提出的方法能有效应对离线强化学习中混杂观察问题，提升性能。

Abstract: Expressive policies based on flow-matching have been successfully applied in reinforcement learning (RL) more recently due to their ability to model complex action distributions from offline data. These algorithms build on standard policy gradients, which assume that there is no unmeasured confounding in the data. However, this condition does not necessarily hold for pixel-based demonstrations when a mismatch exists between the demonstrator's and the learner's sensory capabilities, leading to implicit confounding biases in offline data. We address the challenge by investigating the problem of confounded observations in offline RL from a causal perspective. We develop a novel causal offline RL objective that optimizes policies' worst-case performance that may arise due to confounding biases. Based on this new objective, we introduce a practical implementation that learns expressive flow-matching policies from confounded demonstrations, employing a deep discriminator to assess the discrepancy between the target policy and the nominal behavioral policy. Experiments across 25 pixel-based tasks demonstrate that our proposed confounding-robust augmentation procedure achieves a success rate 120\% that of confounding-unaware, state-of-the-art offline RL methods.

</details>


### [225] [Zero Sum SVD: Balancing Loss Sensitivity for Low Rank LLM Compression](https://arxiv.org/abs/2602.02848)
*Ali Abbasi,Chayne Thrash,Haoran Qin,Shansita Sharma,Sepehr Seifi,Soheil Kolouri*

Main category: cs.LG

TL;DR: 提出ZS - SVD方法对大语言模型进行压缩，结合可选轻量级校正，实验显示有增益。


<details>
  <summary>Details</summary>
Motivation: 大语言模型内存和计算成本阻碍部署，现有SVD压缩方法在确定矩阵秩分配上存在不足。

Method: 提出ZS - SVD后训练方法，用激活白化和一阶校准损失估计进行全局奇异分量选择，按零和规则剪枝；引入可选轻量级校正，截断后进行单次投影梯度更新再截断。

Result: 在多个LLM架构的大量实验中，在不同基准和压缩比下均有一致增益。

Conclusion: ZS - SVD方法有效，可用于大语言模型的压缩，提高性能。

Abstract: Advances in large language models have driven strong performance across many tasks, but their memory and compute costs still hinder deployment. SVD-based compression reduces storage and can speed up inference via low-rank factors, yet performance depends on how rank is allocated under a global compression ratio. Prior methods often use homogeneous ranks for similarly sized matrices, despite large differences in loss sensitivity, or rely on expensive iterative pre-truncation optimization to determine per matrix ranks. We propose \textbf{Zero Sum SVD} (\textbf{ZS-SVD}), a post-training method that performs \emph{global} singular component selection using activation whitening and first-order calibration loss estimates in whitened coordinates. \textbf{ZS-SVD} prunes components across the whole model with a \textbf{zero sum} rule that keeps the cumulative predicted loss change near zero, automatically yielding heterogeneous ranks without solving a rank allocation optimization. Motivated by evidence that gradients near pretrained solutions exhibit low rank structure, we also introduce an optional lightweight correction that applies a \textbf{single} projected gradient update after truncation, followed by re-truncation. Extensive experiments across multiple LLM architectures show consistent gains across diverse benchmarks and compression ratios. Code is available at https://github.com/mint-vu/Zero-Sum-SVD

</details>


### [226] [Recurrent Equivariant Constraint Modulation: Learning Per-Layer Symmetry Relaxation from Data](https://arxiv.org/abs/2602.02853)
*Stefanos Pertigkiozoglou,Mircea Petrache,Shubhendu Trivedi,Kostas Daniilidis*

Main category: cs.LG

TL;DR: 提出RECM机制学习合适的等变约束松弛水平，在多种任务上表现优于先前方法。


<details>
  <summary>Details</summary>
Motivation: 严格等变约束阻碍学习，现有方法依赖预设松弛水平且调参成本高。

Method: 提出Recurrent Equivariant Constraint Modulation (RECM)机制，从训练信号和输入 - 目标分布对称性学习松弛水平。

Result: RECM下各层松弛水平收敛到由对称间隙界定的值，处理对称分布层恢复完全等变性，近似对称层保留灵活性。

Conclusion: RECM在多种精确和近似等变任务上优于先前方法。

Abstract: Equivariant neural networks exploit underlying task symmetries to improve generalization, but strict equivariance constraints can induce more complex optimization dynamics that can hinder learning. Prior work addresses these limitations by relaxing strict equivariance during training, but typically relies on prespecified, explicit, or implicit target levels of relaxation for each network layer, which are task-dependent and costly to tune. We propose Recurrent Equivariant Constraint Modulation (RECM), a layer-wise constraint modulation mechanism that learns appropriate relaxation levels solely from the training signal and the symmetry properties of each layer's input-target distribution, without requiring any prior knowledge about the task-dependent target relaxation level. We demonstrate that under the proposed RECM update, the relaxation level of each layer provably converges to a value upper-bounded by its symmetry gap, namely the degree to which its input-target distribution deviates from exact symmetry. Consequently, layers processing symmetric distributions recover full equivariance, while those with approximate symmetries retain sufficient flexibility to learn non-symmetric solutions when warranted by the data. Empirically, RECM outperforms prior methods across diverse exact and approximate equivariant tasks, including the challenging molecular conformer generation on the GEOM-Drugs dataset.

</details>


### [227] [When pre-training hurts LoRA fine-tuning: a dynamical analysis via single-index models](https://arxiv.org/abs/2602.02855)
*Gibbs Nwemadji,Bruno Loureiro,Jean Barbier*

Main category: cs.LG

TL;DR: 研究表明过度预训练会减慢微调优化速度，刻画了LoRA微调收敛率与初始微调对齐度和目标任务非线性程度的关系。


<details>
  <summary>Details</summary>
Motivation: 验证预训练促进下游微调这一直觉是否总是成立。

Method: 研究单指标模型下使用单遍SGD训练的LoRA微调，利用微调动态的汇总统计描述进行分析。

Result: 即使预训练和下游任务对齐良好，强预训练也会导致搜索阶段延长并阻碍收敛。

Conclusion: 理论提供了预训练强度和任务难度共同影响LoRA微调动态和局限性的统一图景。

Abstract: Pre-training on a source task is usually expected to facilitate fine-tuning on similar downstream problems. In this work, we mathematically show that this naive intuition is not always true: excessive pre-training can computationally slow down fine-tuning optimization. We study this phenomenon for low-rank adaptation (LoRA) fine-tuning on single-index models trained under one-pass SGD. Leveraging a summary statistics description of the fine-tuning dynamics, we precisely characterize how the convergence rate depends on the initial fine-tuning alignment and the degree of non-linearity of the target task. The key take away is that even when the pre-training and down- stream tasks are well aligned, strong pre-training can induce a prolonged search phase and hinder convergence. Our theory thus provides a unified picture of how pre-training strength and task difficulty jointly shape the dynamics and limitations of LoRA fine-tuning in a nontrivial tractable model.

</details>


### [228] [Late-Stage Generalization Collapse in Grokking: Detecting anti-grokking with Weightwatcher](https://arxiv.org/abs/2602.02859)
*Hari K Prakash,Charles H Martin*

Main category: cs.LG

TL;DR: 本文发现神经网络训练中grok机制的新阶段anti - grokking，用WeightWatcher工具诊断，发现相关信号并与其他诊断方法对比，还指出其在大语言模型中的类似问题。


<details>
  <summary>Details</summary>
Motivation: 神经网络中“记忆”缺乏精确操作定义，常从grok机制推断，研究旨在发现新的训练阶段。

Method: 重新研究两个经典grok设置，用WeightWatcher工具诊断，对比其他grok诊断方法。

Result: 发现anti - grokking阶段，确定相关诊断信号，其他方法无法识别anti - grokking，还观察到其在大语言模型中的类似问题。

Conclusion: anti - grokking是一种新的泛化失败模式，Correlation Traps等信号可用于诊断。

Abstract: \emph{Memorization} in neural networks lacks a precise operational definition and is often inferred from the grokking regime, where training accuracy saturates while test accuracy remains very low. We identify a previously unreported third phase of grokking in this training regime: \emph{anti-grokking}, a late-stage collapse of generalization.
  We revisit two canonical grokking setups: a 3-layer MLP trained on a subset of MNIST and a transformer trained on modular addition, but extended training far beyond standard. In both cases, after models transition from pre-grokking to successful generalization, test accuracy collapses back to chance while training accuracy remains perfect, indicating a distinct post-generalization failure mode.
  To diagnose anti-grokking, we use the open-source \texttt{WeightWatcher} tool based on HTSR/SETOL theory. The primary signal is the emergence of \emph{Correlation Traps}: anomalously large eigenvalues beyond the Marchenko--Pastur bulk in the empirical spectral density of shuffled weight matrices, which are predicted to impair generalization. As a secondary signal, anti-grokking corresponds to the average HTSR layer quality metric $α$ deviating from $2.0$. Neither metric requires access to the test or training data.
  We compare these signals to alternative grokking diagnostic, including $\ell_2$ norms, Activation Sparsity, Absolute Weight Entropy, and Local Circuit Complexity. These track pre-grokking and grokking but fail to identify anti-grokking. Finally, we show that Correlation Traps can induce catastrophic forgetting and/or prototype memorization, and observe similar pathologies in large-scale LLMs, like OSS GPT 20/120B.

</details>


### [229] [A Geometry-Aware Efficient Algorithm for Compositional Entropic Risk Minimization](https://arxiv.org/abs/2602.02877)
*Xiyuan Wei,Linli Zhou,Bokun Wang,Chih-Jen Lin,Tianbao Yang*

Main category: cs.LG

TL;DR: 本文研究组合熵风险最小化问题优化，提出SCENT算法，证明收敛率，理论分析对比优势并通过实验验证有效性。


<details>
  <summary>Details</summary>
Motivation: 现有熵风险最小化优化算法存在不收敛、数值不稳定和收敛慢等局限。

Method: 提出几何感知随机算法SCENT，采用随机近端镜像下降（SPMD）更新对偶变量。

Result: 证明SCENT算法在凸问题上有O(1/√T)收敛率；理论刻画SPMD相对SGD的优势；实验表明SCENT在多个任务上优于现有基线。

Conclusion: SCENT算法有效解决现有优化算法的局限，在理论和实践上都有优势。

Abstract: This paper studies optimization for a family of problems termed $\textbf{compositional entropic risk minimization}$, in which each data's loss is formulated as a Log-Expectation-Exponential (Log-E-Exp) function. The Log-E-Exp formulation serves as an abstraction of the Log-Sum-Exponential (LogSumExp) function when the explicit summation inside the logarithm is taken over a gigantic number of items and is therefore expensive to evaluate. While entropic risk objectives of this form arise in many machine learning problems, existing optimization algorithms suffer from several fundamental limitations including non-convergence, numerical instability, and slow convergence rates. To address these limitations, we propose a geometry-aware stochastic algorithm, termed $\textbf{SCENT}$, for the dual formulation of entropic risk minimization cast as a min--min optimization problem. The key to our design is a $\textbf{stochastic proximal mirror descent (SPMD)}$ update for the dual variable, equipped with a Bregman divergence induced by a negative exponential function that faithfully captures the geometry of the objective. Our main contributions are threefold: (i) we establish an $O(1/\sqrt{T})$ convergence rate of the proposed SCENT algorithm for convex problems; (ii) we theoretically characterize the advantages of SPMD over standard SGD update for optimizing the dual variable; and (iii) we demonstrate the empirical effectiveness of SCENT on extreme classification, partial AUC maximization, contrastive learning and distributionally robust optimization, where it consistently outperforms existing baselines.

</details>


### [230] [Mixture of Concept Bottleneck Experts](https://arxiv.org/abs/2602.02886)
*Francesco De Santis,Gabriele Ciravegna,Giovanni De Felice,Arianna Casanova,Francesco Giannini,Michelangelo Diligenti,Mateo Espinosa Zarlenga,Pietro Barbiero,Johannes Schneider,Danilo Giordano*

Main category: cs.LG

TL;DR: 提出混合概念瓶颈专家（M - CBEs）框架，实例化两个新模型，可权衡准确性和可解释性以适应不同需求。


<details>
  <summary>Details</summary>
Motivation: 现有概念瓶颈模型（CBMs）将任务预测器固定为单一表达式，限制了预测准确性和适应性，需要改进。

Method: 提出M - CBEs框架，沿专家数量和专家函数形式两个维度推广现有CBMs，实例化Linear M - CBE和Symbolic M - CBE两个新模型。

Result: 不同的混合大小和函数形式为权衡准确性和可解释性提供了强大框架。

Conclusion: M - CBEs框架能适应不同用户和任务需求。

Abstract: Concept Bottleneck Models (CBMs) promote interpretability by grounding predictions in human-understandable concepts. However, existing CBMs typically fix their task predictor to a single linear or Boolean expression, limiting both predictive accuracy and adaptability to diverse user needs. We propose Mixture of Concept Bottleneck Experts (M-CBEs), a framework that generalizes existing CBMs along two dimensions: the number of experts and the functional form of each expert, exposing an underexplored region of the design space. We investigate this region by instantiating two novel models: Linear M-CBE, which learns a finite set of linear expressions, and Symbolic M-CBE, which leverages symbolic regression to discover expert functions from data under user-specified operator vocabularies. Empirical evaluation demonstrates that varying the mixture size and functional form provides a robust framework for navigating the accuracy-interpretability trade-off, adapting to different user and task needs.

</details>


### [231] [Self-Soupervision: Cooking Model Soups without Labels](https://arxiv.org/abs/2602.02890)
*Anthony Fuller,James R. Green,Evan Shelhamer*

Main category: cs.LG

TL;DR: 提出Self-Souping将模型汤方法拓展到自监督学习，提升了模型鲁棒性，还能用不同自监督算法制作模型汤。


<details>
  <summary>Details</summary>
Motivation: 现有模型汤方法依赖监督学习，希望将其拓展到自监督学习。

Method: 提出Self-Souping方法，在新数据源上调整模型参数，再混合参数形成模型汤。

Result: 在损坏测试数据上进行Self-Souping，再在未损坏训练数据上微调，在ImageNet-C上提升鲁棒性3.5%，在LAION-C上提升7%；用不同自监督算法制作的模型汤比单一算法更准确。

Conclusion: Self-Souping能将模型汤方法拓展到自监督学习，可利用多种自监督算法制作更鲁棒的模型汤。

Abstract: Model soups are strange and strangely effective combinations of parameters. They take a model (the stock), fine-tune it into multiple models (the ingredients), and then mix their parameters back into one model (the soup) to improve predictions. While all known soups require supervised learning, and optimize the same loss on labeled data, our recipes for Self-\emph{Soup}ervision generalize soups to self-supervised learning (SSL). Our Self-Souping lets us flavor ingredients on new data sources, e.g. from unlabeled data from a task for transfer or from a shift for robustness. We show that Self-Souping on corrupted test data, then fine-tuning back on uncorrupted train data, boosts robustness by +3.5\% (ImageNet-C) and +7\% (LAION-C). Self-\emph{Soup}ervision also unlocks countless SSL algorithms to cook the diverse ingredients needed for more robust soups. We show for the first time that ingredients can differ in their SSL hyperparameters -- and more surprisingly, in their SSL algorithms. We cook soups of MAE, MoCoV3, and MMCR ingredients that are more accurate than any one single SSL ingredient.

</details>


### [232] [TraceNAS: Zero-shot LLM Pruning via Gradient Trace Correlation](https://arxiv.org/abs/2602.02891)
*Prajna G. Malettira,Manish Nagaraj,Arjun Roy,Shubham Negi,Kaushik Roy*

Main category: cs.LG

TL;DR: 提出训练自由的NAS框架TraceNAS用于大语言模型结构化剪枝，高效且性能有竞争力。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型结构化剪枝方法未考虑全局依赖或计算成本高，需要新方法降低训练感知剪枝的计算负担并捕捉全局依赖。

Method: 提出TraceNAS框架，联合探索大语言模型深度和宽度的结构化剪枝，并使用尺度不变的零样本代理来识别剪枝模型。

Result: 能在单个GPU上8.5小时内实现高保真的剪枝模型发现，相比训练感知方法减少10倍GPU小时数，在多个基准测试中与训练感知基线具有竞争力。

Conclusion: TraceNAS是一种高效的大语言模型结构化剪枝方法，在计算效率和性能上表现良好。

Abstract: Structured pruning is essential for efficient deployment of Large Language Models (LLMs). The varying sensitivity of LLM sub-blocks to pruning necessitates the identification of optimal non-uniformly pruned models. Existing methods evaluate the importance of layers, attention heads, or weight channels in isolation. Such localized focus ignores the complex global structural dependencies that exist across the model. Training-aware structured pruning addresses global dependencies, but its computational cost can be just as expensive as post-pruning training. To alleviate the computational burden of training-aware pruning and capture global structural dependencies, we propose TraceNAS, a training-free Neural Architecture Search (NAS) framework that jointly explores structured pruning of LLM depth and width. TraceNAS identifies pruned models that maintain a high degree of loss landscape alignment with the pretrained model using a scale-invariant zero-shot proxy, effectively selecting models that exhibit maximal performance potential during post-pruning training. TraceNAS is highly efficient, enabling high-fidelity discovery of pruned models on a single GPU in 8.5 hours, yielding a 10$\times$ reduction in GPU-hours compared to training-aware methods. Evaluations on the Llama and Qwen families demonstrate that TraceNAS is competitive with training-aware baselines across commonsense and reasoning benchmarks.

</details>


### [233] [Manifold-Constrained Energy-Based Transition Models for Offline Reinforcement Learning](https://arxiv.org/abs/2602.02900)
*Zeyu Fang,Zuyuan Zhang,Mahdi Imani,Tian Lan*

Main category: cs.LG

TL;DR: 提出MC - ETM解决基于模型的离线强化学习在分布转移下的脆性问题，通过特定负采样和策略优化方法，提高性能。


<details>
  <summary>Details</summary>
Motivation: 基于模型的离线强化学习在分布转移下很脆弱，策略改进会导致严重的价值高估。

Method: 提出MC - ETM，使用流形投影 - 扩散负采样训练条件能量基转移模型，学习下一状态的潜在流形；在策略优化中，利用学习到的能量提供可靠性信号，截断超出阈值的滚动，并通过悲观惩罚稳定贝尔曼备份。

Result: MC - ETM提高了多步动态保真度，在标准离线控制基准上获得更高的归一化回报，尤其在不规则动态和稀疏数据覆盖情况下。

Conclusion: 通过混合悲观MDP公式化MC - ETM，并推导保守性能界限，表明其在离线强化学习中有良好表现。

Abstract: Model-based offline reinforcement learning is brittle under distribution shift: policy improvement drives rollouts into state--action regions weakly supported by the dataset, where compounding model error yields severe value overestimation. We propose Manifold-Constrained Energy-based Transition Models (MC-ETM), which train conditional energy-based transition models using a manifold projection--diffusion negative sampler. MC-ETM learns a latent manifold of next states and generates near-manifold hard negatives by perturbing latent codes and running Langevin dynamics in latent space with the learned conditional energy, sharpening the energy landscape around the dataset support and improving sensitivity to subtle out-of-distribution deviations. For policy optimization, the learned energy provides a single reliability signal: rollouts are truncated when the minimum energy over sampled next states exceeds a threshold, and Bellman backups are stabilized via pessimistic penalties based on Q-value-level dispersion across energy-guided samples. We formalize MC-ETM through a hybrid pessimistic MDP formulation and derive a conservative performance bound separating in-support evaluation error from truncation risk. Empirically, MC-ETM improves multi-step dynamics fidelity and yields higher normalized returns on standard offline control benchmarks, particularly under irregular dynamics and sparse data coverage.

</details>


### [234] [Spatiotemporal Decision Transformer for Traffic Coordination](https://arxiv.org/abs/2602.02903)
*Haoran Su,Yandong Sun,Hanxiao Deng*

Main category: cs.LG

TL;DR: 提出MADT解决多智能体交通信号控制问题，实验表明效果佳。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在多智能体协调和样本效率方面存在问题，需更好的多智能体交通信号控制方法。

Method: 将多智能体交通信号控制重新表述为序列建模问题，引入MADT，结合图注意力机制、时间变压器编码器和返回目标条件。

Result: 在合成网格网络和真实交通场景实验中，MADT达到了最先进性能，平均旅行时间比最强基线减少5 - 6%，相邻交叉口协调更好。

Conclusion: MADT在多智能体交通信号控制方面表现出色，是一种有效的解决方案。

Abstract: Traffic signal control is a critical challenge in urban transportation, requiring coordination among multiple intersections to optimize network-wide traffic flow. While reinforcement learning has shown promise for adaptive signal control, existing methods struggle with multi-agent coordination and sample efficiency. We introduce MADT (Multi-Agent Decision Transformer), a novel approach that reformulates multi-agent traffic signal control as a sequence modeling problem. MADT extends the Decision Transformer paradigm to multi-agent settings by incorporating: (1) a graph attention mechanism for modeling spatial dependencies between intersections, (2) a|temporal transformer encoder for capturing traffic dynamics, and (3) return-to-go conditioning for target performance specification. Our approach enables offline learning from historical traffic data, with architecture design that facilitates potential online fine-tuning. Experiments on synthetic grid networks and real-world traffic scenarios demonstrate that MADT achieves state-of-the-art performance, reducing average travel time by 5-6% compared to the strongest baseline while exhibiting superior coordination among adjacent intersections.

</details>


### [235] [Weighted Temporal Decay Loss for Learning Wearable PPG Data with Sparse Clinical Labels](https://arxiv.org/abs/2602.02917)
*Yunsung Chung,Keum San Chun,Migyeong Gwak,Han Feng,Yingshuo Liu,Chanho Lim,Viswam Nathan,Nassir Marrouche,Sharanya Arcot Desai*

Main category: cs.LG

TL;DR: 本文提出一种训练策略，在智能手表PPG数据上提升健康算法性能，线性衰减函数最稳健且结果具可解释性。


<details>
  <summary>Details</summary>
Motivation: 开发基于生物信号的健康算法时，临床标签稀疏使远离实验室检测时间的生物信号用于监督不可靠。

Method: 引入训练策略，学习样本权重随片段与其真实标签时间间隔的生物标志物特异性衰减，并在损失函数中使用该权重加正则化器。

Result: 在450名参与者的10种生物标志物的智能手表PPG数据上，该方法优于基线。主题设置下，该方法平均AUPRC为0.715，线性衰减函数最稳健。

Conclusion: 该方法有效提升性能，学习到的衰减率能总结生物标志物PPG证据失效速度，提供时间敏感性的可解释视图。

Abstract: Advances in wearable computing and AI have increased interest in leveraging PPG for health monitoring over the past decade. One of the biggest challenges in developing health algorithms based on such biosignals is the sparsity of clinical labels, which makes biosignals temporally distant from lab draws less reliable for supervision. To address this problem, we introduce a simple training strategy that learns a biomarker-specific decay of sample weight over the time gap between a segment and its ground truth label and uses this weight in the loss with a regularizer to prevent trivial solutions. On smartwatch PPG from 450 participants across 10 biomarkers, the approach improves over baselines. In the subject-wise setting, the proposed approach averages 0.715 AUPRC, compared to 0.674 for a fine-tuned self-supervised baseline and 0.626 for a feature-based Random Forest. A comparison of four decay families shows that a simple linear decay function is most robust on average. Beyond accuracy, the learned decay rates summarize how quickly each biomarker's PPG evidence becomes stale, providing an interpretable view of temporal sensitivity.

</details>


### [236] [A Reproducible Framework for Bias-Resistant Machine Learning on Small-Sample Neuroimaging Data](https://arxiv.org/abs/2602.02920)
*Jagan Mohan Reddy Dwarampudi,Jennifer L Purks,Joshua Wong,Renjie Hu,Tania Banerjee*

Main category: cs.LG

TL;DR: 提出适用于小样本神经影像数据的机器学习框架，在数据集上验证效果，为数据有限的生物医学领域提供蓝图。


<details>
  <summary>Details</summary>
Motivation: 传统交叉验证框架会产生乐观偏差结果，限制可重复性和泛化性，需要更好的框架。

Method: 集成领域信息特征工程、嵌套交叉验证和校准决策阈值优化构建机器学习框架。

Result: 在高维结构MRI数据集上，框架嵌套CV平衡准确率达0.660 ± 0.068。

Conclusion: 该工作结合可解释性和无偏评估，为数据有限的生物医学领域提供通用计算蓝图。

Abstract: We introduce a reproducible, bias-resistant machine learning framework that integrates domain-informed feature engineering, nested cross-validation, and calibrated decision-threshold optimization for small-sample neuroimaging data. Conventional cross-validation frameworks that reuse the same folds for both model selection and performance estimation yield optimistically biased results, limiting reproducibility and generalization. Demonstrated on a high-dimensional structural MRI dataset of deep brain stimulation cognitive outcomes, the framework achieved a nested-CV balanced accuracy of 0.660\,$\pm$\,0.068 using a compact, interpretable subset selected via importance-guided ranking. By combining interpretability and unbiased evaluation, this work provides a generalizable computational blueprint for reliable machine learning in data-limited biomedical domains.

</details>


### [237] [How Does the Lagrangian Guide Safe Reinforcement Learning through Diffusion Models?](https://arxiv.org/abs/2602.02924)
*Xiaoyuan Cheng,Wenxuan Yuan,Boyang Li,Yuanchao Xu,Yiming Yang,Hao Liang,Bei Peng,Robert Loftin,Zhuo Sun,Yukun Hu*

Main category: cs.LG

TL;DR: 提出ALGD算法用于离线策略安全强化学习，解决现有基于扩散的强化学习方法在在线安全设置方面的不足，理论和实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的强化学习方法主要关注离线奖励最大化，对在线安全考虑有限，需解决此问题。

Method: 提出Augmented Lagrangian - Guided Diffusion (ALGD)算法，引入增强拉格朗日函数使能量景观局部凸化。

Result: 理论分析和大量实验表明ALGD在不同环境中实现了强大而稳定的性能。

Conclusion: ALGD算法理论基础扎实，经验上有效，能稳定安全地解决基于扩散的强化学习问题。

Abstract: Diffusion policy sampling enables reinforcement learning (RL) to represent multimodal action distributions beyond suboptimal unimodal Gaussian policies. However, existing diffusion-based RL methods primarily focus on offline settings for reward maximization, with limited consideration of safety in online settings. To address this gap, we propose Augmented Lagrangian-Guided Diffusion (ALGD), a novel algorithm for off-policy safe RL. By revisiting optimization theory and energy-based model, we show that the instability of primal-dual methods arises from the non-convex Lagrangian landscape. In diffusion-based safe RL, the Lagrangian can be interpreted as an energy function guiding the denoising dynamics. Counterintuitively, direct usage destabilizes both policy generation and training. ALGD resolves this issue by introducing an augmented Lagrangian that locally convexifies the energy landscape, yielding a stabilized policy generation and training process without altering the distribution of the optimal policy. Theoretical analysis and extensive experiments demonstrate that ALGD is both theoretically grounded and empirically effective, achieving strong and stable performance across diverse environments.

</details>


### [238] [Distance Marching for Generative Modeling](https://arxiv.org/abs/2602.02928)
*Zimo Wang,Ishit Mehta,Haolin Lu,Chung-En Sun,Ge Yan,Tsui-Wei Weng,Tzu-Mao Li*

Main category: cs.LG

TL;DR: 提出Distance Marching时间无关生成方法，在CIFAR - 10和ImageNet数据集上提升FID，有多种优势。


<details>
  <summary>Details</summary>
Motivation: 现有时间无关生成模型在无时间条件下，同一噪声输入对应多噪声水平和不同去噪方向，干扰监督信号。

Method: 受距离场建模启发，提出Distance Marching方法和两种推理方法，设计关注更近目标的损失函数。

Result: 在多种架构上，相比近期时间无关基线，在CIFAR - 10和ImageNet上FID提升13.5%；在类条件ImageNet生成上表现超流匹配方法，使用60%采样步骤达到更低FID，平均低13.6%；距离预测利于采样早停和OOD检测。

Conclusion: 希望距离场建模能为生成建模提供理论视角。

Abstract: Time-unconditional generative models learn time-independent denoising vector fields. But without time conditioning, the same noisy input may correspond to multiple noise levels and different denoising directions, which interferes with the supervision signal. Inspired by distance field modeling, we propose Distance Marching, a new time-unconditional approach with two principled inference methods. Crucially, we design losses that focus on closer targets. This yields denoising directions better directed toward the data manifold. Across architectures, Distance Marching consistently improves FID by 13.5% on CIFAR-10 and ImageNet over recent time-unconditional baselines. For class-conditional ImageNet generation, despite removing time input, Distance Marching surpasses flow matching using our losses and inference methods. It achieves lower FID than flow matching's final performance using 60% of the sampling steps and 13.6% lower FID on average across backbone sizes. Moreover, our distance prediction is also helpful for early stopping during sampling and for OOD detection. We hope distance field modeling can serve as a principled lens for generative modeling.

</details>


### [239] [Rare Event Early Detection: A Dataset of Sepsis Onset for Critically Ill Trauma Patients](https://arxiv.org/abs/2602.02930)
*Yin Jin,Tucker R. Stewart,Deyi Zhou,Chhavi Gupta,Arjita Nema,Scott C. Brakenridge,Grant E. O'Keefe,Juhua Hu*

Main category: cs.LG

TL;DR: 本文关注创伤后败血症早期检测，提出需针对性识别，引入公开标准化数据集，构建新罕见事件检测问题并建立基准。


<details>
  <summary>Details</summary>
Motivation: 现有公共数据集将ICU患者视为统一群体，忽视创伤重症患者特点，为改善败血症临床结果需对创伤后败血症进行针对性早期检测。

Method: 从MIMIC - III提取、重新标注并验证得到公开标准化创伤后败血症发作数据集，根据ICU日常临床工作流程构建新的罕见事件检测问题。

Result: 通过综合实验建立了通用基准，显示使用新数据集进一步改进的必要性。

Conclusion: 强调针对创伤后败血症早期检测的重要性，新数据集及基准对后续研究有推动作用，代码已公开。

Abstract: Sepsis is a major public health concern due to its high morbidity, mortality, and cost. Its clinical outcome can be substantially improved through early detection and timely intervention. By leveraging publicly available datasets, machine learning (ML) has driven advances in both research and clinical practice. However, existing public datasets consider ICU patients (Intensive Care Unit) as a uniform group and neglect the potential challenges presented by critically ill trauma patients in whom injury-related inflammation and organ dysfunction can overlap with the clinical features of sepsis. We propose that a targeted identification of post-traumatic sepsis is necessary in order to develop methods for early detection. Therefore, we introduce a publicly available standardized post-trauma sepsis onset dataset extracted, relabeled using standardized post-trauma clinical facts, and validated from MIMIC-III. Furthermore, we frame early detection of post-trauma sepsis onset according to clinical workflow in ICUs in a daily basis resulting in a new rare event detection problem. We then establish a general benchmark through comprehensive experiments, which shows the necessity of further advancements using this new dataset. The data code is available at https://github.com/ML4UWHealth/SepsisOnset_TraumaCohort.git.

</details>


### [240] [3D-Learning: Diffusion-Augmented Distributionally Robust Decision-Focused Learning](https://arxiv.org/abs/2602.02943)
*Jiaqi Wen,Lei Fan,Jianyi Yang*

Main category: cs.LG

TL;DR: 提出3D - Learning框架解决PTO管道中ML模型在OOD条件下泛化问题，在LLM资源供应任务中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: PTO管道中ML模型在测试时易受OOD样本影响，导致决策性能显著下降，需解决OOD条件下的泛化挑战。

Method: 提出DR - DFL框架，在此基础上提出3D - Learning，在扩散模型的参数化空间中搜索最坏情况分布。

Result: 在LLM资源供应任务的实验结果表明，3D - Learning在OOD泛化性能上优于现有的DRO和数据增强方法。

Conclusion: 3D - Learning能有效解决PTO管道中ML模型在OOD条件下的泛化问题，在平均和最坏情况场景间取得良好平衡。

Abstract: Predict-then-Optimize (PTO) pipelines are widely employed in computing and networked systems, where Machine Learning (ML) models are used to predict critical contextual information for downstream decision-making tasks such as cloud LLM serving, data center demand response, and edge workload scheduling. However, these ML predictors are often vulnerable to out-of-distribution (OOD) samples at test time, leading to significant decision performance degradation due to large prediction errors. To address the generalization challenges under OOD conditions, we present the framework of Distributionally Robust Decision-Focused Learning (DR-DFL), which trains ML models to optimize decision performance under the worst-case distribution. Instead of relying on classical Distributionally Robust Optimization (DRO) techniques, we propose Diffusion-Augmented Distributionally Robust Decision-Focused Learning (3D-Learning), which searches for the worst-case distribution within the parameterized space of a diffusion model. By leveraging the powerful distribution modeling capabilities of diffusion models, 3D-Learning identifies worst-case distributions that remain consistent with real data, achieving a favorable balance between average and worst-case scenarios. Empirical results on an LLM resource provisioning task demonstrate that 3D-Learning outperforms existing DRO and Data Augmentation methods in OOD generalization performance.

</details>


### [241] [Variational Sparse Paired Autoencoders (vsPAIR) for Inverse Problems and Uncertainty Quantification](https://arxiv.org/abs/2602.02948)
*Jack Michael Solomon,Rishi Leburu,Matthias Chung*

Main category: cs.LG

TL;DR: 提出Variational Sparse Paired Autoencoder (vsPAIR)解决逆问题中快速推理与不确定性估计难题，经实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 逆问题需在众多应用中提供快速推理和可解释的不确定性估计，但这仍具挑战性。

Method: 提出vsPAIR架构，将标准VAE和稀疏VAE通过学习的潜在映射连接；提出对现有稀疏VAE方法的修改，包括硬混凝土尖峰 - 平板松弛和β超先验。

Result: 在盲图像修复和计算机断层扫描实验中，vsPAIR能提供可解释和结构化的不确定性估计。

Conclusion: vsPAIR是一个有能力的逆问题求解器。

Abstract: Inverse problems are fundamental to many scientific and engineering disciplines; they arise when one seeks to reconstruct hidden, underlying quantities from noisy measurements. Many applications demand not just point estimates but interpretable uncertainty. Providing fast inference alongside uncertainty estimates remains challenging yet desirable in numerous applications.
  We propose the Variational Sparse Paired Autoencoder (vsPAIR) to address this challenge. The architecture pairs a standard VAE encoding observations with a sparse VAE encoding quantities of interest, connected through a learned latent mapping. The variational structure enables uncertainty estimation, the paired architecture encourages interpretability by anchoring QoI representations to clean data, and sparse encodings provide structure by concentrating information into identifiable factors rather than diffusing across all dimensions. We also propose modifications to existing sparse VAE methods: a hard-concrete spike-and-slab relaxation for differentiable training and a beta hyperprior for adaptive sparsity levels. To validate the effectiveness of our proposed architecture, we conduct experiments on blind inpainting and computed tomography, demonstrating that vsPAIR is a capable inverse problem solver that can provide interpretable and structured uncertainty estimates.

</details>


### [242] [Quant VideoGen: Auto-Regressive Long Video Generation via 2-Bit KV-Cache Quantization](https://arxiv.org/abs/2602.02958)
*Haocheng Xi,Shuo Yang,Yilong Zhao,Muyang Li,Han Cai,Xingyang Li,Yujun Lin,Zhuoyang Zhang,Jintao Zhang,Xiuyu Li,Zhiying Xu,Jun Wu,Chenfeng Xu,Ion Stoica,Song Han,Kurt Keutzer*

Main category: cs.LG

TL;DR: 提出Quant VideoGen (QVG)框架解决自回归视频扩散模型中KV缓存内存瓶颈问题，提升质量和内存效率。


<details>
  <summary>Details</summary>
Motivation: 自回归视频生成模型中KV缓存增长快，占用大量GPU内存，限制部署且影响长期一致性。

Method: 提出QVG框架，利用语义感知平滑处理视频时空冗余，引入渐进式残差量化。

Result: 在多个基准测试中，QVG在质量和内存效率上达到新的帕累托前沿，KV缓存内存最多减少7倍，端到端延迟开销小于4%，生成质量超现有基线。

Conclusion: QVG能有效解决自回归视频扩散模型的KV缓存内存瓶颈问题。

Abstract: Despite rapid progress in autoregressive video diffusion, an emerging system algorithm bottleneck limits both deployability and generation capability: KV cache memory. In autoregressive video generation models, the KV cache grows with generation history and quickly dominates GPU memory, often exceeding 30 GB, preventing deployment on widely available hardware. More critically, constrained KV cache budgets restrict the effective working memory, directly degrading long horizon consistency in identity, layout, and motion. To address this challenge, we present Quant VideoGen (QVG), a training free KV cache quantization framework for autoregressive video diffusion models. QVG leverages video spatiotemporal redundancy through Semantic Aware Smoothing, producing low magnitude, quantization friendly residuals. It further introduces Progressive Residual Quantization, a coarse to fine multi stage scheme that reduces quantization error while enabling a smooth quality memory trade off. Across LongCat Video, HY WorldPlay, and Self Forcing benchmarks, QVG establishes a new Pareto frontier between quality and memory efficiency, reducing KV cache memory by up to 7.0 times with less than 4% end to end latency overhead while consistently outperforming existing baselines in generation quality.

</details>


### [243] [Human-Centric Traffic Signal Control for Equity: A Multi-Agent Action Branching Deep Reinforcement Learning Approach](https://arxiv.org/abs/2602.02959)
*Xiaocai Zhang,Neema Nassir,Lok Sang Chan,Milad Haghani*

Main category: cs.LG

TL;DR: 提出MA2B - DDQN框架优化交通信号协调，降低受影响旅客数量，适应不同交通条件。


<details>
  <summary>Details</summary>
Motivation: 多模态走廊交通信号协调困难，现有多智能体深度强化学习方法以车辆为中心，难以处理高维离散动作空间。

Method: 提出人类中心的多智能体动作分支双深度Q网络（MA2B - DDQN）框架，将走廊控制分解为局部和全局动作，设计人类中心奖励函数。

Result: 在澳大利亚墨尔本七个现实交通场景评估中，该方法显著减少受影响旅客数量，优于现有DRL和基线方法，模型鲁棒性好。

Conclusion: 该框架倡导公平交通信号系统，提供可扩展且适应不同城市交通条件的解决方案。

Abstract: Coordinating traffic signals along multimodal corridors is challenging because many multi-agent deep reinforcement learning (DRL) approaches remain vehicle-centric and struggle with high-dimensional discrete action spaces. We propose MA2B-DDQN, a human-centric multi-agent action-branching double Deep Q-Network (DQN) framework that explicitly optimizes traveler-level equity. Our key contribution is an action-branching discrete control formulation that decomposes corridor control into (i) local, per-intersection actions that allocate green time between the next two phases and (ii) a single global action that selects the total duration of those phases. This decomposition enables scalable coordination under discrete control while reducing the effective complexity of joint decision-making. We also design a human-centric reward that penalizes the number of delayed individuals in the corridor, accounting for pedestrians, vehicle occupants, and transit passengers. Extensive evaluations across seven realistic traffic scenarios in Melbourne, Australia, demonstrate that our approach significantly reduces the number of impacted travelers, outperforming existing DRL and baseline methods. Experiments confirm the robustness of our model, showing minimal variance across diverse settings. This framework not only advocates for a fairer traffic signal system but also provides a scalable solution adaptable to varied urban traffic conditions.

</details>


### [244] [Q-ShiftDP: A Differentially Private Parameter-Shift Rule for Quantum Machine Learning](https://arxiv.org/abs/2602.02962)
*Hoang M. Ngo,Nhat Hoang-Xuan,Quan Nguyen,Nguyen Do,Incheol Shin,My T. Thai*

Main category: cs.LG

TL;DR: 提出适用于量子机器学习的差分隐私机制Q - ShiftDP，结合量子噪声，实验显示其在QML中优于经典DP方法。


<details>
  <summary>Details</summary>
Motivation: 量子机器学习虽有计算优势，但保护训练数据隐私有挑战，经典方法无法利用量子梯度估计特性。

Method: 引入Differentially Private Parameter - Shift Rule (Q - ShiftDP)，结合校准高斯噪声和量子本征噪声。

Result: Q - ShiftDP能进行更严格的敏感度分析，降低噪声需求，结合量子噪声改善隐私 - 效用权衡。

Conclusion: 在基准数据集实验中，Q - ShiftDP在量子机器学习中始终优于经典DP方法。

Abstract: Quantum Machine Learning (QML) promises significant computational advantages, but preserving training data privacy remains challenging. Classical approaches like differentially private stochastic gradient descent (DP-SGD) add noise to gradients but fail to exploit the unique properties of quantum gradient estimation. In this work, we introduce the Differentially Private Parameter-Shift Rule (Q-ShiftDP), the first privacy mechanism tailored to QML. By leveraging the inherent boundedness and stochasticity of quantum gradients computed via the parameter-shift rule, Q-ShiftDP enables tighter sensitivity analysis and reduces noise requirements. We combine carefully calibrated Gaussian noise with intrinsic quantum noise to provide formal privacy and utility guarantees, and show that harnessing quantum noise further improves the privacy-utility trade-off. Experiments on benchmark datasets demonstrate that Q-ShiftDP consistently outperforms classical DP methods in QML.

</details>


### [245] [Co2PO: Coordinated Constrained Policy Optimization for Multi-Agent RL](https://arxiv.org/abs/2602.02970)
*Shrenik Patel,Christine Truong*

Main category: cs.LG

TL;DR: 提出Co2PO框架解决约束多智能体强化学习中探索与安全约束优化的矛盾，在多智能体安全基准测试中表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有约束多智能体强化学习方法如拉格朗日方法依赖全局惩罚或集中式评论家，抑制探索且导致过度保守，需要解决探索和安全约束优化的矛盾。

Method: 提出Co2PO框架，引入共享黑板架构和学习的危险预测器，将预测集成到约束优化目标。

Result: 在一系列复杂多智能体安全基准测试中，Co2PO与领先的约束基线相比获得更高回报，并在部署时收敛到符合成本的策略。消融研究验证了相关组件的必要性。

Conclusion: Co2PO框架能有效解决约束多智能体强化学习中的问题，实现协调驱动的安全。

Abstract: Constrained multi-agent reinforcement learning (MARL) faces a fundamental tension between exploration and safety-constrained optimization. Existing leading approaches, such as Lagrangian methods, typically rely on global penalties or centralized critics that react to violations after they occur, often suppressing exploration and leading to over-conservatism. We propose Co2PO, a novel MARL communication-augmented framework that enables coordination-driven safety through selective, risk-aware communication. Co2PO introduces a shared blackboard architecture for broadcasting positional intent and yield signals, governed by a learned hazard predictor that proactively forecasts potential violations over an extended temporal horizon. By integrating these forecasts into a constrained optimization objective, Co2PO allows agents to anticipate and navigate collective hazards without the performance trade-offs inherent in traditional reactive constraints. We evaluate Co2PO across a suite of complex multi-agent safety benchmarks, where it achieves higher returns compared to leading constrained baselines while converging to cost-compliant policies at deployment. Ablation studies further validate the necessity of risk-triggered communication, adaptive gating, and shared memory components.

</details>


### [246] [NLI:Non-uniform Linear Interpolation Approximation of Nonlinear Operations for Efficient LLMs Inference](https://arxiv.org/abs/2602.02988)
*Jiangyong Yu,Xiaomeng Han,Xing Hu,Chen Xu,Zhe Jiang,Dawei Yang*

Main category: cs.LG

TL;DR: 提出NLI框架近似非线性函数，设计计算单元，硬件实验显示计算效率提升超4倍。


<details>
  <summary>Details</summary>
Motivation: 大语言模型部署受内存和计算成本限制，现有工作对线性层压缩加速有进展，但非线性层仍依赖高精度浮点运算。

Method: 提出校准无关、动态规划最优且硬件友好的NLI框架，将切点选择转化为动态规划问题；基于NLI算法设计并实现即插即用的通用非线性计算单元。

Result: NLI框架能高效近似多种非线性函数，几乎不损失精度；NLI引擎计算效率相比现有设计提升超4倍。

Conclusion: NLI框架及计算单元可有效解决大语言模型中非线性层运算问题，提升计算效率。

Abstract: Large Language Models (LLMs) have demonstrated remarkable performance across a wide range of tasks, but their deployment is often constrained by substantial memory footprints and computational costs. While prior work has achieved significant progress in compressing and accelerating linear layers, nonlinear layers-such as SiLU, RMSNorm, and Softmax-still heavily depend on high-precision floating-point operations. In this paper, we propose a calibration-free, dynamic-programming-optimal, and hardware-friendly framework called Non-uniform Linear Interpolation (NLI). NLI is capable of efficiently approximating a variety of nonlinear functions, enabling seamless integration into LLMs and other deep neural networks with almost no loss in accuracy. NLI ingeniously recasts cutpoint selection as a dynamic-programming problem, achieving the globally minimal interpolation error in O(MxN2) time via Bellman's optimality principle. Based on the NLI algorithm, we also design and implement a plug-and-play universal nonlinear computation unit. Hardware experiments demonstrate that the NLI Engine achieves more than 4x improvement in computational efficiency compared to the state-of-the-art designs.

</details>


### [247] [Learning to Repair Lean Proofs from Compiler Feedback](https://arxiv.org/abs/2602.02990)
*Evan Wang,Simon Chess,Daniel Lee,Siyuan Ge,Ajit Mallavarapu,Vasily Ilin*

Main category: cs.LG

TL;DR: 研究Lean证明修复问题，引入APRIL数据集，训练语言模型提升修复准确率。


<details>
  <summary>Details</summary>
Motivation: 现有Lean数据集多为正确证明，缺乏对理解和修复失败情况的监督，需研究证明修复问题。

Method: 将Lean证明修复作为监督学习问题，引入APRIL数据集进行训练。

Result: 在APRIL上训练语言模型显著提高修复准确率和反馈条件推理能力，微调的4B参数模型表现优于开源基线。

Conclusion: 诊断条件监督可作为使用反馈的证明器的补充训练信号，数据集已公开。

Abstract: As neural theorem provers become increasingly agentic, the ability to interpret and act on compiler feedback is critical. However, existing Lean datasets consist almost exclusively of correct proofs, offering little supervision for understanding and repairing failures. We study Lean proof repair as a supervised learning problem: given an erroneous proof and compiler feedback, predict both a corrected proof and a natural-language diagnosis grounded in the same feedback. We introduce APRIL (Automated Proof Repair in Lean), a dataset of 260,000 supervised tuples pairing systematically generated proof failures with compiler diagnostics and aligned repair and explanation targets. Training language models on APRIL substantially improves repair accuracy and feedback-conditioned reasoning; in our single-shot repair evaluation setting, a finetuned 4B-parameter model outperforms the strongest open-source baseline. We view diagnostic-conditioned supervision as a complementary training signal for feedback-using provers. Our dataset is available at \href{https://huggingface.co/datasets/uw-math-ai/APRIL}{this link}.

</details>


### [248] [Adaptive Batch Sizes Using Non-Euclidean Gradient Noise Scales for Stochastic Sign and Spectral Descent](https://arxiv.org/abs/2602.03001)
*Hiroki Naganuma,Shagun Gupta,Youssef Briki,Ioannis Mitliagkas,Irina Rish,Parameswaran Raman,Hao-Jun Michael Shi*

Main category: cs.LG

TL;DR: 本文推导了signSGD和specSGD的梯度噪声尺度，提出高效方差估计程序，实验表明非欧几里得GNS自适应批量大小策略可减少训练步骤。


<details>
  <summary>Details</summary>
Motivation: 现有基于梯度噪声尺度（GNS）的自适应策略与基于广义范数的流行优化器存在几何不匹配问题，为解决此问题开展研究。

Method: 推导signSGD和specSGD的梯度噪声尺度，提出利用分布式数据并行系统中不同秩上的局部小批量梯度进行方差估计的程序。

Result: 在1.6亿参数的Llama模型上，使用非欧几里得GNS的自适应批量大小策略能匹配恒定批量基线的验证损失，同时使Signum和Muon的训练步骤最多减少66%。

Conclusion: 非欧几里得GNS的自适应批量大小策略有效，可提高训练效率。

Abstract: To maximize hardware utilization, modern machine learning systems typically employ large constant or manually tuned batch size schedules, relying on heuristics that are brittle and costly to tune. Existing adaptive strategies based on gradient noise scale (GNS) offer a principled alternative. However, their assumption of SGD's Euclidean geometry creates a fundamental mismatch with popular optimizers based on generalized norms, such as signSGD / Signum ($\ell_\infty$) and stochastic spectral descent (specSGD) / Muon ($\mathcal{S}_\infty$). In this work, we derive gradient noise scales for signSGD and specSGD that naturally emerge from the geometry of their respective dual norms. To practically estimate these non-Euclidean metrics, we propose an efficient variance estimation procedure that leverages the local mini-batch gradients on different ranks in distributed data-parallel systems. Our experiments demonstrate that adaptive batch size strategies using non-Euclidean GNS enable us to match the validation loss of constant-batch baselines while reducing training steps by up to 66% for Signum and Muon on a 160 million parameter Llama model.

</details>


### [249] [Causal Graph Spatial-Temporal Autoencoder for Reliable and Interpretable Process Monitoring](https://arxiv.org/abs/2602.03004)
*Xiangrui Zhang,Chunyue Song,Wei Dai,Zheng Zhang,Kaihua Gao,Furong Gao*

Main category: cs.LG

TL;DR: 本文提出CGSTAE用于工业过程监控，包含相关图结构学习和时空编解码模块，通过两个统计量实现监控与故障检测，并通过两个过程验证有效性。


<details>
  <summary>Details</summary>
Motivation: 提高工业过程监控的可靠性和可解释性。

Method: 提出CGSTAE，结合基于空间自注意力机制的相关图结构学习模块和利用图卷积长短时记忆的时空编解码模块，引入三步因果图结构学习算法，利用因果不变性原理反向视角推导因果图，用GCLSTM单元构建时空编解码器。

Result: 通过特征空间和残差空间的两个统计量实现有效过程监控和故障检测。

Conclusion: 通过田纳西 - 伊斯曼过程和实际空分过程验证了CGSTAE在过程监控中的有效性。

Abstract: To improve the reliability and interpretability of industrial process monitoring, this article proposes a Causal Graph Spatial-Temporal Autoencoder (CGSTAE). The network architecture of CGSTAE combines two components: a correlation graph structure learning module based on spatial self-attention mechanism (SSAM) and a spatial-temporal encoder-decoder module utilizing graph convolutional long-short term memory (GCLSTM). The SSAM learns correlation graphs by capturing dynamic relationships between variables, while a novel three-step causal graph structure learning algorithm is introduced to derive a causal graph from these correlation graphs. The algorithm leverages a reverse perspective of causal invariance principle to uncover the invariant causal graph from varying correlations. The spatial-temporal encoder-decoder, built with GCLSTM units, reconstructs time-series process data within a sequence-to-sequence framework. The proposed CGSTAE enables effective process monitoring and fault detection through two statistics in the feature space and residual space. Finally, we validate the effectiveness of CGSTAE in process monitoring through the Tennessee Eastman process and a real-world air separation process.

</details>


### [250] [From Zero to Hero: Advancing Zero-Shot Foundation Models for Tabular Outlier Detection](https://arxiv.org/abs/2602.03018)
*Xueying Ding,Haomin Wen,Simon Klütterman,Leman Akoglu*

Main category: cs.LG

TL;DR: 文章介绍了异常检测（OD）新模型OUTFORMER，它改进FoMo - 0D，在多个基准测试中达最优性能，且推理快、零样本、即插即用。


<details>
  <summary>Details</summary>
Motivation: 现有OD在新任务部署因缺少标记异常值而困难，算法和超参数选择难，需改进方法。

Method: 使用合成先验混合和自我进化课程训练改进FoMo - 0D，仅在合成标记数据集上预训练，利用训练数据作为上下文输入推理新任务测试标签。

Result: OUTFORMER在AdBench及两个新大规模OD基准测试（超1500个数据集）中达最优性能，且推理速度快。

Conclusion: OUTFORMER能快速零样本推理，无需额外工作，可实现即插即用部署，提升OD在新任务的有效性。

Abstract: Outlier detection (OD) is widely used in practice; but its effective deployment on new tasks is hindered by lack of labeled outliers, which makes algorithm and hyperparameter selection notoriously hard. Foundation models (FMs) have transformed ML, and OD is no exception: Shen et. al. (2025) introduced FoMo-0D, the first FM for OD, achieving remarkable performance against numerous baselines. This work introduces OUTFORMER, which advances FoMo-0D with (1) a mixture of synthetic priors and (2) self-evolving curriculum training. OUTFORMER is pretrained solely on synthetic labeled datasets and infers test labels of a new task by using its training data as in-context input. Inference is fast and zero-shot, requiring merely forward pass and no labeled outliers. Thanks to in-context learning, it requires zero additional work-no OD model training or bespoke model selection-enabling truly plug-and-play deployment. OUTFORMER achieves state-of-the-art performance on the prominent AdBench, as well as two new large-scale OD benchmarks that we introduce, comprising over 1,500 datasets, while maintaining speedy inference.

</details>


### [251] [FedKRSO: Communication and Memory Efficient Federated Fine-Tuning of Large Language Models](https://arxiv.org/abs/2602.03019)
*Guohao Yang,Tongle Wu,Yuanxiong Guo,Ying Sun,Yanmin Gong*

Main category: cs.LG

TL;DR: 本文提出FedKRSO方法，可在联邦学习场景下高效微调大语言模型，减少通信和内存开销，性能接近全参数微调。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在大语言模型微调中因传输全模型参数和计算全梯度成本高而面临挑战，参数高效微调方法会牺牲性能。

Method: 提出FedKRSO方法，客户端在服务器生成的随机低维子空间内更新模型，仅发送模型更新累加器，便于高效全局模型聚合和分发。

Result: 严格分析了FedKRSO的收敛性，在GLUE基准测试上的实验表明其性能优越且通信和内存开销低。

Conclusion: FedKRSO为资源受限边缘的联邦大语言模型微调铺平了道路。

Abstract: Fine-tuning is essential to adapt general-purpose large language models (LLMs) to domain-specific tasks. As a privacy-preserving framework to leverage decentralized data for collaborative model training, Federated Learning (FL) is gaining popularity in LLM fine-tuning, but remains challenging due to the high cost of transmitting full model parameters and computing full gradients on resource-constrained clients. While Parameter-Efficient Fine-Tuning (PEFT) methods are widely used in FL to reduce communication and memory costs, they often sacrifice model performance compared to FFT. This paper proposes FedKRSO (Federated $K$-Seed Random Subspace Optimization), a novel method that enables communication and memory efficient FFT of LLMs in federated settings. In FedKRSO, clients update the model within a shared set of random low-dimension subspaces generated by the server to save memory usage. Furthermore, instead of transmitting full model parameters in each FL round, clients send only the model update accumulators along the subspaces to the server, enabling efficient global model aggregation and dissemination. By using these strategies, FedKRSO can substantially reduce communication and memory overhead while overcoming the performance limitations of PEFT, closely approximating the performance of federated FFT. The convergence properties of FedKRSO are analyzed rigorously under general FL settings. Extensive experiments on the GLUE benchmark across diverse FL scenarios demonstrate that FedKRSO achieves both superior performance and low communication and memory overhead, paving the way towards on federated LLM fine-tuning at the resource-constrained edge.

</details>


### [252] [Consistency Deep Equilibrium Models](https://arxiv.org/abs/2602.03024)
*Junchao Lin,Zenan Ling,Jingwen Xu,Robert C. Qiu*

Main category: cs.LG

TL;DR: 本文提出C - DEQ框架加速DEQ推理，在多领域任务实验中表现出优势。


<details>
  <summary>Details</summary>
Motivation: Deep Equilibrium Models（DEQs）因定点求解器的迭代特性导致推理延迟大，需要加速推理。

Method: 引入Consistency Deep Equilibrium Model（C - DEQ）框架，利用一致性蒸馏，将DEQ迭代推理过程视为沿固定ODE轨迹向平衡态的演化，训练C - DEQ将中间状态直接映射到不动点。

Result: 在相同少步推理预算下，C - DEQs比隐式DEQs实现了2 - 20倍的准确率提升。

Conclusion: C - DEQ框架能加速DEQ推理，在保留教师DEQ性能的同时实现少步推理，还可灵活权衡计算与性能。

Abstract: Deep Equilibrium Models (DEQs) have emerged as a powerful paradigm in deep learning, offering the ability to model infinite-depth networks with constant memory usage. However, DEQs incur significant inference latency due to the iterative nature of fixed-point solvers. In this work, we introduce the Consistency Deep Equilibrium Model (C-DEQ), a novel framework that leverages consistency distillation to accelerate DEQ inference. We cast the DEQ iterative inference process as evolution along a fixed ODE trajectory toward the equilibrium. Along this trajectory, we train C-DEQs to consistently map intermediate states directly to the fixed point, enabling few-step inference while preserving the performance of the teacher DEQ. At the same time, it facilitates multi-step evaluation to flexibly trade computation for performance gains. Extensive experiments across various domain tasks demonstrate that C-DEQs achieves consistent 2-20$\times$ accuracy improvements over implicit DEQs under the same few-step inference budget.

</details>


### [253] [SAFE-KD: Risk-Controlled Early-Exit Distillation for Vision Backbones](https://arxiv.org/abs/2602.03043)
*Salim Khazem*

Main category: cs.LG

TL;DR: 提出SAFE - KD多出口包装器，结合分层蒸馏与共形风险控制，在多数据集和架构上有更好表现。


<details>
  <summary>Details</summary>
Motivation: 早期退出网络实际部署需知道何时提前退出是安全的。

Method: 引入SAFE - KD，在中间层添加轻量级出口头，通过解耦知识蒸馏将强教师模型知识蒸馏到所有出口，利用共形风险控制校准退出阈值。

Result: 在多个数据集和架构上实现了更好的准确率 - 计算权衡、更强的校准和在数据损坏下的鲁棒性能。

Conclusion: SAFE - KD能在提供有限样本风险保证的同时提升性能。

Abstract: Early-exit networks reduce inference cost by allowing ``easy'' inputs to stop early, but practical deployment hinges on knowing \emph{when} early exit is safe. We introduce SAFE-KD, a universal multi-exit wrapper for modern vision backbones that couples hierarchical distillation with \emph{conformal risk control}. SAFE-KD attaches lightweight exit heads at intermediate depths, distills a strong teacher into all exits via Decoupled Knowledge Distillation (DKD), and enforces deep-to-shallow consistency between exits. At inference, we calibrate per-exit stopping thresholds on a held-out set using conformal risk control (CRC) to guarantee a user-specified \emph{selective} misclassification risk (among the samples that exit early) under exchangeability. Across multiple datasets and architectures, SAFE-KD yields improved accuracy compute trade-offs, stronger calibration, and robust performance under corruption while providing finite-sample risk guarantees.

</details>


### [254] [Clarify Before You Draw: Proactive Agents for Robust Text-to-CAD Generation](https://arxiv.org/abs/2602.03045)
*Bo Yuan,Zelin Zhao,Petr Molodyk,Bin Hu,Yongxin Chen*

Main category: cs.LG

TL;DR: 提出ProCAD框架解决文本到CAD程序生成中规格问题，实验表明其性能优且代码数据集将开源。


<details>
  <summary>Details</summary>
Motivation: 现有微调模型在处理文本到CAD程序生成时，对不明确或不一致的几何描述易产生维度幻觉，需解决规格问题。

Method: 提出ProCAD框架，包含主动澄清代理和CAD编码代理，对编码代理在高质量数据集上微调，对澄清代理通过代理SFT训练。

Result: 主动澄清显著提高对模糊提示的鲁棒性，降低交互开销，ProCAD优于前沿闭源模型，降低Chamfer距离79.9%，将无效率从4.8%降至0.9%。

Conclusion: ProCAD框架能有效解决文本到CAD程序生成中的规格问题，性能表现良好。

Abstract: Large language models have recently enabled text-to-CAD systems that synthesize parametric CAD programs (e.g., CadQuery) from natural language prompts. In practice, however, geometric descriptions can be under-specified or internally inconsistent: critical dimensions may be missing and constraints may conflict. Existing fine-tuned models tend to reactively follow user instructions and hallucinate dimensions when the text is ambiguous. To address this, we propose a proactive agentic framework for text-to-CadQuery generation, named ProCAD, that resolves specification issues before code synthesis. Our framework pairs a proactive clarifying agent, which audits the prompt and asks targeted clarification questions only when necessary to produce a self-consistent specification, with a CAD coding agent that translates the specification into an executable CadQuery program. We fine-tune the coding agent on a curated high-quality text-to-CadQuery dataset and train the clarifying agent via agentic SFT on clarification trajectories. Experiments show that proactive clarification significantly improves robustness to ambiguous prompts while keeping interaction overhead low. ProCAD outperforms frontier closed-source models, including Claude Sonnet 4.5, reducing the mean Chamfer distance by 79.9 percent and lowering the invalidity ratio from 4.8 percent to 0.9 percent. Our code and datasets will be made publicly available.

</details>


### [255] [CoBA-RL: Capability-Oriented Budget Allocation for Reinforcement Learning in LLMs](https://arxiv.org/abs/2602.03048)
*Zhiyuan Yao,Yi-Kai Zhang,Yuxin Chen,Yueqing Sun,Zishan Xu,Yu Yang,Tianhao Hu,Qi Gu,Hui Su,Xunliang Cai*

Main category: cs.LG

TL;DR: 提出CoBA - RL算法用于自适应分配强化学习资源，在多基准测试中提升泛化能力，强调量化样本训练价值和优化预算分配对提升LLM训练效率的重要性。


<details>
  <summary>Details</summary>
Motivation: 标准强化学习框架资源分配效率低，现有自适应方法无法捕捉模型动态学习状态。

Method: 提出CoBA - RL算法，用面向能力的价值函数映射任务潜在训练收益，采用基于堆的贪心策略分配计算资源。

Result: 有效平衡探索与利用，在多个挑战性基准测试中实现一致的泛化改进。

Conclusion: 量化样本训练价值和优化预算分配对提升LLM训练后效率至关重要。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a key approach for enhancing LLM reasoning.However, standard frameworks like Group Relative Policy Optimization (GRPO) typically employ a uniform rollout budget, leading to resource inefficiency. Moreover, existing adaptive methods often rely on instance-level metrics, such as task pass rates, failing to capture the model's dynamic learning state. To address these limitations, we propose CoBA-RL, a reinforcement learning algorithm designed to adaptively allocate rollout budgets based on the model's evolving capability. Specifically, CoBA-RL utilizes a Capability-Oriented Value function to map tasks to their potential training gains and employs a heap-based greedy strategy to efficiently self-calibrate the distribution of computational resources to samples with high training value. Extensive experiments demonstrate that our approach effectively orchestrates the trade-off between exploration and exploitation, delivering consistent generalization improvements across multiple challenging benchmarks. These findings underscore that quantifying sample training value and optimizing budget allocation are pivotal for advancing LLM post-training efficiency.

</details>


### [256] [Fedcompass: Federated Clustered and Periodic Aggregation Framework for Hybrid Classical-Quantum Models](https://arxiv.org/abs/2602.03052)
*Yueheng Wang,Xing He,Zinuo Cai,Rui Zhang,Ruhui Ma,Yuan Liu,Rajkumar Buyya*

Main category: cs.LG

TL;DR: 提出用于混合经典 - 量子联邦学习的分层聚合框架FEDCOMPASS，实验显示其能提高测试准确率和收敛稳定性。


<details>
  <summary>Details</summary>
Motivation: 混合经典 - 量子联邦学习在非IID数据下性能易下降，需解决该问题。

Method: 采用谱聚类按类分布相似度对客户端分组，对经典特征提取器进行聚类聚合；对量子参数使用循环均值聚合结合自适应优化。

Result: 在三个基准数据集实验表明，FEDCOMPASS在非IID设置下将测试准确率提高达10.22%，提升收敛稳定性，优于六个强大的联邦学习基线。

Conclusion: FEDCOMPASS能有效解决混合经典 - 量子联邦学习在非IID数据下的性能问题。

Abstract: Federated learning enables collaborative model training across decentralized clients under privacy constraints. Quantum computing offers potential for alleviating computational and communication burdens in federated learning, yet hybrid classical-quantum federated learning remains susceptible to performance degradation under non-IID data. To address this,we propose FEDCOMPASS, a layered aggregation framework for hybrid classical-quantum federated learning. FEDCOMPASS employs spectral clustering to group clients by class distribution similarity and performs cluster-wise aggregation for classical feature extractors. For quantum parameters, it uses circular mean aggregation combined with adaptive optimization to ensure stable global updates. Experiments on three benchmark datasets show that FEDCOMPASS improves test accuracy by up to 10.22% and enhances convergence stability under non-IID settings, outperforming six strong federated learning baselines.

</details>


### [257] [Shortcut Features as Top Eigenfunctions of NTK: A Linear Neural Network Case and More](https://arxiv.org/abs/2602.03066)
*Jinwoo Lim,Suhyun Kim,Soo-Mook Moon*

Main category: cs.LG

TL;DR: 基于 NTK 框架分析线性神经网络的捷径学习，发现捷径特征对应大特征值特征，该偏好不受输出边界控制影响，且结果可扩展到复杂网络。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习模型的捷径学习这一长期问题，即神经网络倾向学习训练集中非泛化特征。

Method: 基于 Neural Tangent Kernel (NTK) 框架分析线性神经网络，将神经网络特征定义为 NTK 的特征函数。

Result: 捷径特征对应大特征值特征，大特征值特征训练后仍影响输出，输出边界控制不影响对特定特征的偏好。

Conclusion: 最大边界偏差不是捷径学习的唯一主要原因，线性神经网络的特性可扩展到复杂神经网络。

Abstract: One of the chronic problems of deep-learning models is shortcut learning. In a case where the majority of training data are dominated by a certain feature, neural networks prefer to learn such a feature even if the feature is not generalizable outside the training set. Based on the framework of Neural Tangent Kernel (NTK), we analyzed the case of linear neural networks to derive some important properties of shortcut learning. We defined a feature of a neural network as an eigenfunction of NTK. Then, we found that shortcut features correspond to features with larger eigenvalues when the shortcuts stem from the imbalanced number of samples in the clustered distribution. We also showed that the features with larger eigenvalues still have a large influence on the neural network output even after training, due to data variances in the clusters. Such a preference for certain features remains even when a margin of a neural network output is controlled, which shows that the max-margin bias is not the only major reason for shortcut learning. These properties of linear neural networks are empirically extended for more complex neural networks as a two-layer fully-connected ReLU network and a ResNet-18.

</details>


### [258] [FlashSinkhorn: IO-Aware Entropic Optimal Transport](https://arxiv.org/abs/2602.03067)
*Felix X. -F. Ye,Xingjie Li,An Yu,Ming-Ching Chang,Linsong Chu,Davis Wertheimer*

Main category: cs.LG

TL;DR: 提出FlashSinkhorn这一IO感知的EOT求解器，在GPU上实现加速并提升可扩展性，还开源代码。


<details>
  <summary>Details</summary>
Motivation: 现有基于GPU的熵最优传输（EOT）求解器在大规模场景下效率低，张量化实现有高HBM流量问题，现有在线后端融合能力有限。

Method: 将稳定的对数域Sinkhorn更新重写为带偏置点积分数的逐行LogSumExp归约，采用类似FlashAttention的融合和分块技术，用Triton内核进行芯片上SRAM分块流式处理。

Result: 在A100 GPU上，点云OT任务中前向传播加速达32倍，端到端加速达161倍，提升了基于OT的下游任务可扩展性。

Conclusion: FlashSinkhorn能有效解决现有EOT求解器的效率和扩展性问题。

Abstract: Entropic optimal transport (EOT) via Sinkhorn iterations is widely used in modern machine learning, yet GPU solvers remain inefficient at scale. Tensorized implementations suffer quadratic HBM traffic from dense $n\times m$ interactions, while existing online backends avoid storing dense matrices but still rely on generic tiled map-reduce reduction kernels with limited fusion. We present \textbf{FlashSinkhorn}, an IO-aware EOT solver for squared Euclidean cost that rewrites stabilized log-domain Sinkhorn updates as row-wise LogSumExp reductions of biased dot-product scores, the same normalization as transformer attention. This enables FlashAttention-style fusion and tiling: fused Triton kernels stream tiles through on-chip SRAM and update dual potentials in a single pass, substantially reducing HBM IO per iteration while retaining linear-memory operations. We further provide streaming kernels for transport application, enabling scalable first- and second-order optimization. On A100 GPUs, FlashSinkhorn achieves up to $32\times$ forward-pass and $161\times$ end-to-end speedups over state-of-the-art online baselines on point-cloud OT, improves scalability on OT-based downstream tasks. For reproducibility, we release an open-source implementation at https://github.com/ot-triton-lab/ot_triton.

</details>


### [259] [TMS: Trajectory-Mixed Supervision for Reward-Free, On-Policy SFT](https://arxiv.org/abs/2602.03073)
*Rana Muhammad Shahroz Khan,Zijie Liu,Zhen Tan,Charles Fleming,Tianlong Chen*

Main category: cs.LG

TL;DR: 提出无奖励框架Trajectory - Mixed Supervision (TMS)解决强化学习和监督微调在提升大语言模型性能上的权衡问题，实验证明其有效。


<details>
  <summary>Details</summary>
Motivation: 强化学习成本高，监督微调易因监督不匹配导致灾难性遗忘，需解决两者权衡问题。

Method: 提出无奖励框架TMS，通过模型自身历史检查点创建动态课程，最小化策略 - 标签差异（PLD）。

Result: 在推理和指令遵循基准测试中，TMS有效改变了准确性 - 保留率帕累托边界，显著优于标准和迭代监督微调，缩小与强化学习差距。

Conclusion: PLD漂移可准确预测遗忘，TMS能成功缓解该漂移。

Abstract: Reinforcement Learning (RL) and Supervised Fine-Tuning (SFT) are the two dominant paradigms for enhancing Large Language Model (LLM) performance on downstream tasks. While RL generally preserves broader model capabilities (retention) better than SFT, it comes with significant costs: complex reward engineering, instability, and expensive on-policy sampling. In contrast, SFT is efficient but brittle, often suffering from catastrophic forgetting due to $\textbf{Supervision Mismatch}$: the divergence between the model's evolving policy and static training labels. We address this trade-off with $\textbf{Trajectory-Mixed Supervision (TMS)}$, a reward-free framework that approximates the on-policy benefits of RL by creating a dynamic curriculum from the model's own historical checkpoints. TMS minimizes $\textit{Policy-Label Divergence (PLD)}$, preventing the mode collapse that drives forgetting in standard SFT. Experiments across reasoning (MATH, GSM8K) and instruction-following benchmarks demonstrate that TMS effectively shifts the accuracy--retention Pareto frontier. While RL remains the gold standard for retention, TMS significantly outperforms standard and iterative SFT, bridging the gap to RL without requiring reward models or verifiers. Mechanistic analysis confirms that PLD drift accurately predicts forgetting and that TMS successfully mitigates this drift.

</details>


### [260] [Geometry-Preserving Neural Architectures on Manifolds with Boundary](https://arxiv.org/abs/2602.03082)
*Karthik Elamvazhuthi,Shiba Biswal,Kian Rosenblum,Arushi Katyal,Tianli Qu,Grady Ma,Rishi Sonthalia*

Main category: cs.LG

TL;DR: 提出几何感知架构，建立约束神经ODE通用逼近结果，分析输出端强制几何架构，未知约束集时学习投影，实验展示可行性和性能。


<details>
  <summary>Details</summary>
Motivation: 在学习中保留几何结构很重要，需提出相关架构和理论。

Method: 提出统一的几何感知架构，离散化流形上投影动力系统；分析输出端强制几何架构；未知约束集时通过小时热核极限学习投影。

Result: 建立约束神经ODE通用逼近结果；证明输出端强制几何架构通用逼近性质；实验展示解析更新可行性和学习投影的强性能。

Conclusion: 所提出的架构和方法在保留几何结构学习中有效。

Abstract: Preserving geometric structure is important in learning. We propose a unified class of geometry-aware architectures that interleave geometric updates between layers, where both projection layers and intrinsic exponential map updates arise as discretizations of projected dynamical systems on manifolds (with or without boundary). Within this framework, we establish universal approximation results for constrained neural ODEs. We also analyze architectures that enforce geometry only at the output, proving a separate universal approximation property that enables direct comparison to interleaved designs. When the constraint set is unknown, we learn projections via small-time heat-kernel limits, showing diffusion/flow-matching can be used as data-based projections. Experiments on dynamics over S^2 and SO(3), and diffusion on S^{d-1}-valued features demonstrate exact feasibility for analytic updates and strong performance for learned projections

</details>


### [261] [Neural Predictor-Corrector: Solving Homotopy Problems with Reinforcement Learning](https://arxiv.org/abs/2602.03086)
*Jiayao Mai,Bangyan Liao,Zhenjun Zhao,Yingping Zeng,Haoang Li,Javier Civera,Tailin Wu,Yi Zhou,Peidong Liu*

Main category: cs.LG

TL;DR: 本文提出Neural Predictor - Corrector (NPC)统一求解同伦问题，用强化学习自动学习策略，引入摊销训练机制，实验显示其泛化性好、效率高且稳定性强。


<details>
  <summary>Details</summary>
Motivation: 现有同伦问题实用求解器依赖手工启发式方法确定步长和迭代终止条件，往往不是最优且特定于任务，因此需要统一的框架和更好的求解方法。

Method: 将同伦问题统一在单个框架下，提出NPC，将策略选择作为序列决策问题，用强化学习自动发现高效策略，引入摊销训练机制。

Result: 在四个代表性同伦问题实验中，方法有效泛化到未见实例，在效率上始终优于经典和专门的基线方法，在各任务中展现出更好的稳定性。

Conclusion: 将同伦方法统一到单个神经框架有很大价值。

Abstract: The Homotopy paradigm, a general principle for solving challenging problems, appears across diverse domains such as robust optimization, global optimization, polynomial root-finding, and sampling. Practical solvers for these problems typically follow a predictor-corrector (PC) structure, but rely on hand-crafted heuristics for step sizes and iteration termination, which are often suboptimal and task-specific. To address this, we unify these problems under a single framework, which enables the design of a general neural solver. Building on this unified view, we propose Neural Predictor-Corrector (NPC), which replaces hand-crafted heuristics with automatically learned policies. NPC formulates policy selection as a sequential decision-making problem and leverages reinforcement learning to automatically discover efficient strategies. To further enhance generalization, we introduce an amortized training mechanism, enabling one-time offline training for a class of problems and efficient online inference on new instances. Experiments on four representative homotopy problems demonstrate that our method generalizes effectively to unseen instances. It consistently outperforms classical and specialized baselines in efficiency while demonstrating superior stability across tasks, highlighting the value of unifying homotopy methods into a single neural framework.

</details>


### [262] [PRISM: Structured Optimization via Anisotropic Spectral Shaping](https://arxiv.org/abs/2602.03096)
*Yujie Yang*

Main category: cs.LG

TL;DR: 提出PRISM优化器，用部分二阶信息增强一阶谱下降方法，开销小且无额外内存消耗。


<details>
  <summary>Details</summary>
Motivation: 将曲率自适应特性集成到谱优化范式中，增强一阶谱下降方法。

Method: 通过创新增强极分解构建高效低秩拟二阶预条件器，实现各向异性谱整形。

Result: 能自适应抑制高方差子空间的更新，同时保留信号主导方向的更新强度。

Conclusion: PRISM为谱优化范式提供了实用的曲率自适应集成策略。

Abstract: We propose PRISM, an optimizer that enhances first-order spectral descent methods like Muon with partial second-order information. It constructs an efficient, low-rank quasi-second-order preconditioner via innovation-augmented polar decomposition. This mechanism enables PRISM to perform anisotropic spectral shaping, which adaptively suppresses updates in high-variance subspaces while preserving update strength in signal-dominated directions. Crucially, this is achieved with minimal computational overhead and zero additional memory compared to first-order baselines. PRISM demonstrates a practical strategy for integrating curvature-adaptive properties into the spectral optimization paradigm.

</details>


### [263] [TextME: Bridging Unseen Modalities Through Text Descriptions](https://arxiv.org/abs/2602.03098)
*Soyeon Hong,Jinchan Kim,Jaegook You,Seungtaek Choi,Suha Kwak,Hyunsouk Cho*

Main category: cs.LG

TL;DR: 提出TextME文本模态扩展框架，利用预训练对比编码器几何结构实现零样本跨模态转移，验证单文本训练有效性及跨模态检索能力。


<details>
  <summary>Details</summary>
Motivation: 扩展多模态表示到新模态依赖大规模配对数据集，成本高且在医学成像和分子分析等领域不可行。

Method: 引入TextME框架，将不同模态投影到LLM嵌入空间，利用预训练对比编码器几何结构，仅用文本描述实现零样本跨模态转移。

Result: 验证跨多个领域存在一致模态差距，单文本训练可保留预训练编码器性能，实现未明确对齐模态对的跨模态检索。

Conclusion: 单文本训练可作为模态扩展中配对监督的实用替代方案。

Abstract: Expanding multimodal representations to novel modalities is constrained by reliance on large-scale paired datasets (e.g., text-image, text-audio, text-3D, text-molecule), which are costly and often infeasible in domains requiring expert annotation such as medical imaging and molecular analysis. We introduce TextME, the first text-only modality expansion framework, to the best of our knowledge, projecting diverse modalities into LLM embedding space as a unified anchor. Our approach exploits the geometric structure of pretrained contrastive encoders to enable zero-shot cross-modal transfer using only text descriptions, without paired supervision. We empirically validate that such consistent modality gaps exist across image, video, audio, 3D, X-ray, and molecular domains, demonstrating that text-only training can preserve substantial performance of pretrained encoders. We further show that our framework enables emergent cross-modal retrieval between modality pairs not explicitly aligned during training (e.g., audio-to-image, 3D-to-image). These results establish text-only training as a practical alternative to paired supervision for modality expansion.

</details>


### [264] [Consensus Group Relative Policy Optimization for Text Generation](https://arxiv.org/abs/2602.03102)
*Yuki Ichihara,Yuu Jinnai,Kaito Ariu,Eiji Uchibe*

Main category: cs.LG

TL;DR: 提出C - GRPO方法，将MBR解码提炼到训练中，实验表明其性能与MBR解码相当且无推理开销，优于无参考基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有文本生成强解码方法计算成本高，之前分摊推理计算的尝试依赖额外数据，增加数据集构建和奖励模型需求。

Method: 提出Consensus Group Relative Policy Optimization (C - GRPO)，将共识效用公式化为GRPO中的组相对目标。

Result: 在机器翻译和文本摘要实验中，C - GRPO性能与MBR解码相当，且无推理开销，优于无参考基线方法。

Conclusion: C - GRPO能在不增加推理开销的情况下实现与MBR解码相当的性能。

Abstract: Many strong decoding methods for text generation follow a sample-and-rerank paradigm: they draw multiple candidates, score each under a utility (reward) function using consensus across samples, and return the best one. Although effective, these methods incur high computational costs during inference due to repeated sampling and scoring. Prior attempts to amortize inference-time computation typically rely on gold references, teacher labels, or curated preference data, increasing dataset construction effort and the demand for high-fidelity reward models. We propose Consensus Group Relative Policy Optimization (C-GRPO), which distills Minimum Bayes Risk (MBR) decoding into training by formulating the consensus utility as a group-relative objective within GRPO. C-GRPO requires only a utility function and policy samples, without gold references or explicit preference labels. Under ideal conditions, we show that the objective function of C-GRPO is directionally aligned with the gradient of the expected-utility objective underlying MBR decoding, leading to a convergence guarantee. Experiments on machine translation (WMT 2024) and text summarization (XSum) demonstrate that C-GRPO successfully achieves performance comparable to MBR decoding without the associated inference-time overhead, while outperforming reference-free baseline methods.

</details>


### [265] [Function-Space Empirical Bayes Regularisation with Large Vision-Language Model Priors](https://arxiv.org/abs/2602.03119)
*Pengcheng Hao,Huaze Tang,Ercan Engin Kuruoglu,Wenbo Ding*

Main category: cs.LG

TL;DR: 提出VLM - FS - EB函数空间经验贝叶斯正则化框架，利用大视觉语言模型生成语义上下文点构建先验，实验显示该方法提升预测性能和不确定性估计可靠性。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯深度学习中设计能有效扩展到高维数据的信息先验分布是挑战，现有方法依赖高斯过程先验在高维表现受限。

Method: 提出VLM - FS - EB框架，利用大视觉语言模型生成语义有意义的上下文点，用其嵌入构建表达性强的函数先验。

Result: 与各种基线对比，该方法在预测性能上有提升，在分布外检测任务和数据稀缺场景中能给出更可靠的不确定性估计。

Conclusion: 所提VLM - FS - EB方法能有效解决贝叶斯深度学习中先验分布设计问题，在相关任务中表现良好。

Abstract: Bayesian deep learning (BDL) provides a principled framework for reliable uncertainty quantification by combining deep neural networks with Bayesian inference. A central challenge in BDL lies in the design of informative prior distributions that scale effectively to high-dimensional data. Recent functional variational inference (VI) approaches address this issue by imposing priors directly in function space; however, most existing methods rely on Gaussian process (GP) priors, whose expressiveness and generalisation capabilities become limited in high-dimensional regimes. In this work, we propose VLM-FS-EB, a novel function-space empirical Bayes regularisation framework, leveraging large vision-language models (VLMs) to generates semantically meaningful context points. These synthetic samples are then used VLMs for embeddings to construct expressive functional priors. Furthermore, the proposed method is evaluated against various baselines, and experimental results demonstrate that our method consistently improves predictive performance and yields more reliable uncertainty estimates, particularly in out-of-distribution (OOD) detection tasks and data-scarce regimes.

</details>


### [266] [Quantized Evolution Strategies: High-precision Fine-tuning of Quantized LLMs at Low-precision Cost](https://arxiv.org/abs/2602.03120)
*Yinggan Xu,Risto Miikkulainen,Xin Qiu*

Main category: cs.LG

TL;DR: 本文提出量化进化策略（QES），可在量化空间对大语言模型进行全参数微调，优于现有方法，使量化模型直接微调成为可能。


<details>
  <summary>Details</summary>
Motivation: 后训练量化使模型静态且难微调，标准微调范式无法用于量化模型，现有进化策略优化量化参数可能失败。

Method: 提出QES，集成累积误差反馈保留高精度梯度信号，利用无状态种子重放降低内存使用。

Result: QES在算术推理任务上显著优于现有零阶微调方法。

Conclusion: QES使量化模型直接微调成为可能，为在量化空间扩展大语言模型提供了可能。

Abstract: Post-Training Quantization (PTQ) is essential for deploying Large Language Models (LLMs) on memory-constrained devices, yet it renders models static and difficult to fine-tune. Standard fine-tuning paradigms, including Reinforcement Learning (RL), fundamentally rely on backpropagation and high-precision weights to compute gradients. Thus they cannot be used on quantized models, where the parameter space is discrete and non-differentiable. While Evolution Strategies (ES) offer a backpropagation-free alternative, optimization of the quantized parameters can still fail due to vanishing or inaccurate gradient. This paper introduces Quantized Evolution Strategies (QES), an optimization paradigm that performs full-parameter fine-tuning directly in the quantized space. QES is based on two innovations: (1) it integrates accumulated error feedback to preserve high-precision gradient signals, and (2) it utilizes a stateless seed replay to reduce memory usage to low-precision inference levels. QES significantly outperforms the state-of-the-art zeroth-order fine-tuning method on arithmetic reasoning tasks, making direct fine-tuning for quantized models possible. It therefore opens up the possibility for scaling up LLMs entirely in the quantized space. The source code is available at https://github.com/dibbla/Quantized-Evolution-Strategies .

</details>


### [267] [Enhanced Parcel Arrival Forecasting for Logistic Hubs: An Ensemble Deep Learning Approach](https://arxiv.org/abs/2602.03135)
*Xinyue Pan,Yujia Xu,Benoit Montreuil*

Main category: cs.LG

TL;DR: 提出基于深度学习的集成框架预测物流枢纽工作量，经测试优于传统和单一模型，能提升运营效率。


<details>
  <summary>Details</summary>
Motivation: 网购发展使物流服务提供商需提升枢纽网络效率、敏捷性和可预测性。

Method: 提出基于深度学习的集成框架，利用历史到达模式和实时包裹状态更新预测物流枢纽工作量。

Result: 通过某大城市包裹物流案例研究进行算法实证测试，该集成方法优于传统预测技术和单一深度学习模型。

Conclusion: 此方法在提升物流枢纽运营效率上潜力巨大，建议广泛采用。

Abstract: The rapid expansion of online shopping has increased the demand for timely parcel delivery, compelling logistics service providers to enhance the efficiency, agility, and predictability of their hub networks. In order to solve the problem, we propose a novel deep learning-based ensemble framework that leverages historical arrival patterns and real-time parcel status updates to forecast upcoming workloads at logistic hubs. This approach not only facilitates the generation of short-term forecasts, but also improves the accuracy of future hub workload predictions for more strategic planning and resource management. Empirical tests of the algorithm, conducted through a case study of a major city's parcel logistics, demonstrate the ensemble method's superiority over both traditional forecasting techniques and standalone deep learning models. Our findings highlight the significant potential of this method to improve operational efficiency in logistics hubs and advocate for its broader adoption.

</details>


### [268] [SATORIS-N: Spectral Analysis based Traffic Observation Recovery via Informed Subspaces and Nuclear-norm minimization](https://arxiv.org/abs/2602.03138)
*Sampad Mohanty,Bhaskar Krishnamachari*

Main category: cs.LG

TL;DR: 提出 SATORIS-N 框架填补交通密度矩阵缺失值，优于多种基准方法，可应用于智能交通场景。


<details>
  <summary>Details</summary>
Motivation: 在基础设施传感器或车辆观测不完整时，需要可靠的交通密度重建方法，以支持智能交通系统关键应用。

Method: 引入基于相邻日期先验子空间信息的 SATORIS-N 框架，采用子空间感知半定规划（SDP）公式的核范数及轻量级隐式子空间对齐策略。

Result: 在两个真实数据集上，SATORIS-N 在高遮挡水平下始终优于标准矩阵补全方法和深度学习基线。

Conclusion: SATORIS-N 框架能有效提升交通密度重建精度，可推广到其他特征子空间随时间缓慢变化的时空场景。

Abstract: Traffic-density matrices from different days exhibit both low rank and stable correlations in their singular-vector subspaces. Leveraging this, we introduce SATORIS-N, a framework for imputing partially observed traffic-density by informed subspace priors from neighboring days. Our contribution is a subspace-aware semidefinite programming (SDP)} formulation of nuclear norm that explicitly informs the reconstruction with prior singular-subspace information. This convex formulation jointly enforces low rank and subspace alignment, providing a single global optimum and substantially improving accuracy under medium and high occlusion. We also study a lightweight implicit subspace-alignment} strategy in which matrices from consecutive days are concatenated to encourage alignment of spatial or temporal singular directions. Although this heuristic offers modest gains when missing rates are low, the explicit SDP approach is markedly more robust when large fractions of entries are missing. Across two real-world datasets (Beijing and Shanghai), SATORIS-N consistently outperforms standard matrix-completion methods such as SoftImpute, IterativeSVD, statistical, and even deep learning baselines at high occlusion levels. The framework generalizes to other spatiotemporal settings in which singular subspaces evolve slowly over time. In the context of intelligent vehicles and vehicle-to-everything (V2X) systems, accurate traffic-density
  reconstruction enables critical applications including cooperative perception, predictive routing, and vehicle-to-infrastructure (V2I) communication optimization. When infrastructure sensors or vehicle-reported observations are incomplete - due to communication dropouts, sensor occlusions, or sparse connected vehicle penetration-reliable imputation becomes essential for safe and efficient autonomous navigation.

</details>


### [269] [What Makes a Good Example? Modeling Exemplar Selection with Neural Network Representations](https://arxiv.org/abs/2602.03144)
*Fanxiao Wani Qiu,Oscar Leong,Alexander LaTourrette*

Main category: cs.LG

TL;DR: 本文用神经网络特征表示和子集选择标准建模人类示例选择，发现基于联合代表性或其与多样性结合的策略最能捕捉人类判断，Transformer 表示更符合人类行为，凸显数据集蒸馏方法用于教学计算模型的潜力。


<details>
  <summary>Details</summary>
Motivation: 前人研究表明人类教学时会考虑代表性和多样性，但权衡的计算原则尚不清楚，本文旨在填补这一空白。

Method: 使用预训练视觉模型将新视觉类别嵌入一维形态连续体，改变选择策略对典型性、联合代表性和多样性的侧重，让成年参与者选择示例教学，进行模型与人类判断的比较。

Result: 基于联合代表性或其与多样性结合的策略最能捕捉人类判断，纯基于典型性或多样性的策略表现较差，Transformer 表示比卷积网络更符合人类行为。

Conclusion: 数据集蒸馏方法在机器学习中作为教学计算模型具有潜在效用。

Abstract: Teaching requires distilling a rich category distribution into a small set of informative exemplars. Although prior work shows that humans consider both representativeness and diversity when teaching, the computational principles underlying these tradeoffs remain unclear. We address this gap by modeling human exemplar selection using neural network feature representations and principled subset selection criteria. Novel visual categories were embedded along a one-dimensional morph continuum using pretrained vision models, and selection strategies varied in their emphasis on prototypicality, joint representativeness, and diversity. Adult participants selected one to three exemplars to teach a learner. Model-human comparisons revealed that strategies based on joint representativeness, or its combination with diversity, best captured human judgments, whereas purely prototypical or diversity-based strategies performed worse. Moreover, transformer-based representations consistently aligned more closely with human behavior than convolutional networks. These results highlight the potential utility of dataset distillation methods in machine learning as computational models for teaching.

</details>


### [270] [MemCast: Memory-Driven Time Series Forecasting with Experience-Conditioned Reasoning](https://arxiv.org/abs/2602.03164)
*Xiaoyu Tao,Mingyue Cheng,Ze Guo,Shuo Yu,Yaguo Liu,Qi Liu,Shijin Wang*

Main category: cs.LG

TL;DR: 提出MemCast框架用于时间序列预测，能有效积累经验并持续进化，在多数据集上超现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型（LLM）的时间序列预测方法缺乏显式经验积累和持续进化能力。

Method: 提出MemCast框架，将时间序列预测转化为经验条件推理任务，学习训练集经验并组织成层次化记忆，推理时利用记忆内容引导，设计动态置信自适应策略实现持续进化。

Result: 在多个数据集上进行广泛实验，MemCast始终优于先前方法。

Conclusion: MemCast方法有效，代码已公开。

Abstract: Time series forecasting (TSF) plays a critical role in decision-making for many real-world applications. Recently, LLM-based forecasters have made promising advancements. Despite their effectiveness, existing methods often lack explicit experience accumulation and continual evolution. In this work, we propose MemCast, a learning-to-memory framework that reformulates TSF as an experience-conditioned reasoning task. Specifically, we learn experience from the training set and organize it into a hierarchical memory. This is achieved by summarizing prediction results into historical patterns, distilling inference trajectories into reasoning wisdom, and inducing extracted temporal features into general laws. Furthermore, during inference, we leverage historical patterns to guide the reasoning process and utilize reasoning wisdom to select better trajectories, while general laws serve as criteria for reflective iteration. Additionally, to enable continual evolution, we design a dynamic confidence adaptation strategy that updates the confidence of individual entries without leaking the test set distribution. Extensive experiments on multiple datasets demonstrate that MemCast consistently outperforms previous methods, validating the effectiveness of our approach. Our code is available at https://github.com/Xiaoyu-Tao/MemCast-TS.

</details>


### [271] [StepScorer: Accelerating Reinforcement Learning with Step-wise Scoring and Psychological Regret Modeling](https://arxiv.org/abs/2602.03171)
*Zhe Xu*

Main category: cs.LG

TL;DR: 提出心理后悔模型（PRM）加速强化学习收敛，在基准环境比PPO快36%，在连续控制和延迟反馈环境有效。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习算法因稀疏奖励信号在复杂环境收敛慢。

Method: 引入PRM，基于最优动作预期价值和实际动作价值之差计算后悔信号，将稀疏奖励转换为密集反馈信号。

Result: PRM在Lunar Lander等基准环境比传统PPO约快36%达到稳定性能。

Conclusion: PRM在连续控制和延迟反馈环境有效，适用于机器人、金融等领域，连接了行为经济学和强化学习。

Abstract: Reinforcement learning algorithms often suffer from slow convergence due to sparse reward signals, particularly in complex environments where feedback is delayed or infrequent. This paper introduces the Psychological Regret Model (PRM), a novel approach that accelerates learning by incorporating regret-based feedback signals after each decision step. Rather than waiting for terminal rewards, PRM computes a regret signal based on the difference between the expected value of the optimal action and the value of the action taken in each state. This transforms sparse rewards into dense feedback signals through a step-wise scoring framework, enabling faster convergence. We demonstrate that PRM achieves stable performance approximately 36\% faster than traditional Proximal Policy Optimization (PPO) in benchmark environments such as Lunar Lander. Our results indicate that PRM is particularly effective in continuous control tasks and environments with delayed feedback, making it suitable for real-world applications such as robotics, finance, and adaptive education where rapid policy adaptation is critical. The approach formalizes human-inspired counterfactual thinking as a computable regret signal, bridging behavioral economics and reinforcement learning.

</details>


### [272] [Adversarial construction as a potential solution to the experiment design problem in large task spaces](https://arxiv.org/abs/2602.03172)
*Prakhar Godara,Frederick Callaway,Marcelo G. Mattar*

Main category: cs.LG

TL;DR: 本文旨在为任务空间中的所有任务开发统一模型，针对任务空间大难以实验探索的问题，提出对抗构造方法，结果表明该方法优于随机抽样。


<details>
  <summary>Details</summary>
Motivation: 缺乏通用的人类行为理论，要解决任务通用性问题，开发统一模型。

Method: 提出对抗构造方法来识别最可能引发新行为的任务。

Result: 对抗构造方法显著优于环境的随机抽样。

Conclusion: 对抗构造方法可作为高维任务空间中最优实验设计的代理。

Abstract: Despite decades of work, we still lack a robust, task-general theory of human behavior even in the simplest domains. In this paper we tackle the generality problem head-on, by aiming to develop a unified model for all tasks embedded in a task-space. In particular we consider the space of binary sequence prediction tasks where the observations are generated by the space parameterized by hidden Markov models (HMM). As the space of tasks is large, experimental exploration of the entire space is infeasible. To solve this problem we propose the adversarial construction approach, which helps identify tasks that are most likely to elicit a qualitatively novel behavior. Our results suggest that adversarial construction significantly outperforms random sampling of environments and therefore could be used as a proxy for optimal experimental design in high-dimensional task spaces.

</details>


### [273] [Probe-then-Commit Multi-Objective Bandits: Theoretical Benefits of Limited Multi-Arm Feedback](https://arxiv.org/abs/2602.03175)
*Ming Shi*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study an online resource-selection problem motivated by multi-radio access selection and mobile edge computing offloading. In each round, an agent chooses among $K$ candidate links/servers (arms) whose performance is a stochastic $d$-dimensional vector (e.g., throughput, latency, energy, reliability). The key interaction is \emph{probe-then-commit (PtC)}: the agent may probe up to $q>1$ candidates via control-plane measurements to observe their vector outcomes, but must execute exactly one candidate in the data plane. This limited multi-arm feedback regime strictly interpolates between classical bandits ($q=1$) and full-information experts ($q=K$), yet existing multi-objective learning theory largely focuses on these extremes. We develop \textsc{PtC-P-UCB}, an optimistic probe-then-commit algorithm whose technical core is frontier-aware probing under uncertainty in a Pareto mode, e.g., it selects the $q$ probes by approximately maximizing a hypervolume-inspired frontier-coverage potential and commits by marginal hypervolume gain to directly expand the attained Pareto region. We prove a dominated-hypervolume frontier error of $\tilde{O} (K_P d/\sqrt{qT})$, where $K_P$ is the Pareto-frontier size and $T$ is the horizon, and scalarized regret $\tilde{O} (L_φd\sqrt{(K/q)T})$, where $φ$ is the scalarizer. These quantify a transparent $1/\sqrt{q}$ acceleration from limited probing. We further extend to \emph{multi-modal probing}: each probe returns $M$ modalities (e.g., CSI, queue, compute telemetry), and uncertainty fusion yields variance-adaptive versions of the above bounds via an effective noise scale.

</details>


### [274] [DynSplit-KV: Dynamic Semantic Splitting for KVCache Compression in Efficient Long-Context LLM Inference](https://arxiv.org/abs/2602.03184)
*Jiancai Ye,Jun Liu,Qingchen Li,Tianlang Zhao,Hanbin Zhang,Jiayi Pan,Ningyi Xu,Guohao Dai*

Main category: cs.LG

TL;DR: KV缓存压缩对长上下文大语言模型推理很重要，现有方法有缺陷，提出DynSplit - KV方法，实验效果好。


<details>
  <summary>Details</summary>
Motivation: 现有KV缓存压缩方法采用刚性分割策略，在不同场景下精度下降严重，需动态语义分割。

Method: 提出DynSplit - KV方法，包括动态重要性感知的分隔符选择策略和统一映射策略。

Result: DynSplit - KV提高精度49.9%，减少推理开销4.9倍，比FlashAttention加速2.2倍，长上下文场景峰值内存减少2.6倍。

Conclusion: DynSplit - KV在准确性、速度和内存使用上表现出色。

Abstract: Although Key-Value (KV) Cache is essential for efficient large language models (LLMs) inference, its growing memory footprint in long-context scenarios poses a significant bottleneck, making KVCache compression crucial. Current compression methods rely on rigid splitting strategies, such as fixed intervals or pre-defined delimiters. We observe that rigid splitting suffers from significant accuracy degradation (ranging from 5.5% to 55.1%) across different scenarios, owing to the scenario-dependent nature of the semantic boundaries. This highlights the necessity of dynamic semantic splitting to match semantics. To achieve this, we face two challenges. (1) Improper delimiter selection misaligns semantics with the KVCache, resulting in 28.6% accuracy loss. (2) Variable-length blocks after splitting introduce over 73.1% additional inference overhead. To address the above challenges, we propose DynSplit-KV, a KVCache compression method that dynamically identifies delimiters for splitting. We propose: (1) a dynamic importance-aware delimiter selection strategy, improving accuracy by 49.9%. (2) A uniform mapping strategy that transforms variable-length semantic blocks into a fixed-length format, reducing inference overhead by 4.9x. Experiments show that DynSplit-KV achieves the highest accuracy, 2.2x speedup compared with FlashAttention and 2.6x peak memory reduction in long-context scenarios.

</details>


### [275] [Prompt Augmentation Scales up GRPO Training on Mathematical Reasoning](https://arxiv.org/abs/2602.03190)
*Wenquan Lu,Hai Huang,Randall Balestriero*

Main category: cs.LG

TL;DR: 提出提示增强策略提升大模型数学推理能力，解决训练不稳定问题，取得SOTA效果。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习算法在强化后训练中存在熵坍塌现象，导致训练不稳定和崩溃，且多依赖单一固定推理提示，限制策略改进。

Method: 引入提示增强训练策略，让模型在不同模板和格式下生成推理轨迹，增加采样多样性。

Result: 在固定数据集下，无KL正则项时提示增强可稳定延长训练时长，模型能容忍低熵状态。Qwen2.5 - Math - 1.5B模型在MATH Level 3 - 5数据集上训练后达SOTA性能。

Conclusion: 提示增强策略能有效解决现有强化学习算法在提升大模型数学推理能力训练中的问题。

Abstract: Reinforcement learning algorithms such as group-relative policy optimization (GRPO) have demonstrated strong potential for improving the mathematical reasoning capabilities of large language models. However, prior work has consistently observed an entropy collapse phenomenon during reinforcement post-training, characterized by a monotonic decrease in policy entropy that ultimately leads to training instability and collapse. As a result, most existing approaches restrict training to short horizons (typically 5-20 epochs), limiting sustained exploration and hindering further policy improvement. In addition, nearly all prior work relies on a single, fixed reasoning prompt or template during training. In this work, we introduce prompt augmentation, a training strategy that instructs the model to generate reasoning traces under diverse templates and formats, thereby increasing rollout diversity. We show that, without a KL regularization term, prompt augmentation enables stable scaling of training duration under a fixed dataset and allows the model to tolerate low-entropy regimes without premature collapse. Empirically, a Qwen2.5-Math-1.5B model trained with prompt augmentation on the MATH Level 3-5 dataset achieves state-of-the-art performance, reaching 44.5 per-benchmark accuracy and 51.3 per-question accuracy on standard mathematical reasoning benchmarks, including AIME24, AMC, MATH500, Minerva, and OlympiadBench. The code and model checkpoints are available at https://github.com/wenquanlu/prompt-augmentation-GRPO.

</details>


### [276] [Reinforcement Learning with Promising Tokens for Large Language Models](https://arxiv.org/abs/2602.03195)
*Jing-Cheng Pang,Liang Lu,Xian Tang,Kun Jiang,Sijie Wu,Kai Zhang,Xubin Li*

Main category: cs.LG

TL;DR: 本文提出RLPT框架解决强化学习在大语言模型中动作空间问题，实验表明其有效。


<details>
  <summary>Details</summary>
Motivation: 标准强化学习方法在大语言模型中动作空间包含大量无关token，分散策略决策注意力。

Method: 提出RLPT框架，利用基础模型语义先验识别有前景的token集合，通过掩码将策略优化限制在该子集。

Result: 理论分析和实验表明RLPT有效降低梯度方差，稳定训练过程，提高样本效率，在多个任务上优于标准基线。

Conclusion: RLPT能有效解决大语言模型强化学习中的动作空间问题，且可跨模型大小和算法集成。

Abstract: Reinforcement learning (RL) has emerged as a key paradigm for aligning and optimizing large language models (LLMs). Standard approaches treat the LLM as the policy and apply RL directly over the full vocabulary space. However, this formulation includes the massive tail of contextually irrelevant tokens in the action space, which could distract the policy from focusing on decision-making among the truly reasonable tokens. In this work, we verify that valid reasoning paths could inherently concentrate within a low-rank subspace. Based on this insight, we introduce Reinforcement Learning with Promising Tokens (RLPT), a framework that mitigates the action space issue by decoupling strategic decision-making from token generation. Specifically, RLPT leverages the semantic priors of the base model to identify a dynamic set of \emph{promising tokens} and constrains policy optimization exclusively to this refined subset via masking. Theoretical analysis and empirical results demonstrate that RLPT effectively reduces gradient variance, stabilizes the training process, and improves sample efficiency. Experiment results on math, coding, and telecom reasoning show that RLPT outperforms standard RL baselines and integrates effectively across various model sizes (4B and 8B) and RL algorithms (GRPO and DAPO).

</details>


### [277] [From Scalar Rewards to Potential Trends: Shaping Potential Landscapes for Model-Based Reinforcement Learning](https://arxiv.org/abs/2602.03201)
*Yao-Hui Li,Zeyu Wang,Xin Li,Wei Pang,Yingfang Yuan,Zhengkun Chen,Boya Zhang,Riashat Islam,Alex Lamb,Yonggang Zhang*

Main category: cs.LG

TL;DR: 提出SLOPE框架解决MBRL在稀疏奖励环境中效率低的问题，实验显示其表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 模型基强化学习（MBRL）在稀疏奖励设置中有效性严重受损，标准的奖励回归范式在稀疏环境中无法为规划提供方向指导。

Method: 提出SLOPE框架，将奖励建模从预测标量转变为构建信息丰富的潜在景观，采用乐观分布回归估计高置信上限。

Result: 在5个基准的30多个任务评估中，SLOPE在全稀疏、半稀疏和密集奖励设置下均持续优于领先基线。

Conclusion: SLOPE框架有效解决了MBRL在稀疏奖励环境中的问题，表现良好。

Abstract: Model-based reinforcement learning (MBRL) achieves high sample efficiency by simulating future trajectories with learned dynamics and reward models. However, its effectiveness is severely compromised in sparse reward settings. The core limitation lies in the standard paradigm of regressing ground-truth scalar rewards: in sparse environments, this yields a flat, gradient-free landscape that fails to provide directional guidance for planning. To address this challenge, we propose Shaping Landscapes with Optimistic Potential Estimates (SLOPE), a novel framework that shifts reward modeling from predicting scalars to constructing informative potential landscapes. SLOPE employs optimistic distributional regression to estimate high-confidence upper bounds, which amplifies rare success signals and ensures sufficient exploration gradients. Evaluations on 30+ tasks across 5 benchmarks demonstrate that SLOPE consistently outperforms leading baselines in fully sparse, semi-sparse, and dense rewards.

</details>


### [278] [Sparsity is Combinatorial Depth: Quantifying MoE Expressivity via Tropical Geometry](https://arxiv.org/abs/2602.03204)
*Ye Su,Huayi Tang,Zixuan Gong,Yong Liu*

Main category: cs.LG

TL;DR: 从热带几何角度分析MoE架构，揭示Top - k路由机制与k - 阶基本对称热带多项式的代数同构，引入有效容量概念，证明MoE架构的组合弹性。


<details>
  <summary>Details</summary>
Motivation: 以往MoE架构理论成功多归因于启发式效率，缺乏从几何表达性的分析，本文从热带几何角度进行研究。

Method: 通过热带几何分析MoE，建立Top - k路由机制与k - 阶基本对称热带多项式的同构，引入有效容量概念。

Result: 发现稀疏性是组合深度，MoE架构在低维数据上有组合弹性，保持高表达性。

Conclusion: 框架统一了超单形离散几何与神经函数连续几何，为条件计算的拓扑优越性提供理论依据。

Abstract: While Mixture-of-Experts (MoE) architectures define the state-of-the-art, their theoretical success is often attributed to heuristic efficiency rather than geometric expressivity. In this work, we present the first analysis of MoE through the lens of tropical geometry, establishing that the Top-$k$ routing mechanism is algebraically isomorphic to the $k$-th elementary symmetric tropical polynomial. This isomorphism partitions the input space into the Normal Fan of a Hypersimplex, revealing that \textbf{sparsity is combinatorial depth} which scales geometric capacity by the binomial coefficient $\binom{N}{k}$. Moving beyond ambient bounds, we introduce the concept of \textit{Effective Capacity} under the Manifold Hypothesis. We prove that while dense networks suffer from capacity collapse on low-dimensional data, MoE architectures exhibit \textit{Combinatorial Resilience}, maintaining high expressivity via the transversality of routing cones. In this study, our framework unifies the discrete geometry of the Hypersimplex with the continuous geometry of neural functions, offering a rigorous theoretical justification for the topological supremacy of conditional computation.

</details>


### [279] [Spectral Evolution Search: Efficient Inference-Time Scaling for Reward-Aligned Image Generation](https://arxiv.org/abs/2602.03208)
*Jinyan Ye,Zhongjie Duan,Zhiwen Li,Cen Chen,Daoyuan Chen,Yaliang Li,Yingda Chen*

Main category: cs.LG

TL;DR: 现有高维初始噪声优化方法效率低，提出谱进化搜索（SES）框架，实验表明其提升了生成质量与计算成本的帕累托前沿。


<details>
  <summary>Details</summary>
Motivation: 现有优化高维初始噪声的方法效率低下，因很多搜索方向对最终生成影响可忽略。

Method: 提出谱进化搜索（SES），在低频子空间内执行无梯度进化搜索，并从扰动传播动力学推导出谱缩放预测。

Result: SES显著提升了生成质量与计算成本的帕累托前沿，在同等预算下始终优于强基线模型。

Conclusion: SES是一种有效且高效的初始噪声优化框架，能更好地平衡生成质量和计算成本。

Abstract: Inference-time scaling offers a versatile paradigm for aligning visual generative models with downstream objectives without parameter updates. However, existing approaches that optimize the high-dimensional initial noise suffer from severe inefficiency, as many search directions exert negligible influence on the final generation. We show that this inefficiency is closely related to a spectral bias in generative dynamics: model sensitivity to initial perturbations diminishes rapidly as frequency increases. Building on this insight, we propose Spectral Evolution Search (SES), a plug-and-play framework for initial noise optimization that executes gradient-free evolutionary search within a low-frequency subspace. Theoretically, we derive the Spectral Scaling Prediction from perturbation propagation dynamics, which explains the systematic differences in the impact of perturbations across frequencies. Extensive experiments demonstrate that SES significantly advances the Pareto frontier of generation quality versus computational cost, consistently outperforming strong baselines under equivalent budgets.

</details>


### [280] [Lookahead Sample Reward Guidance for Test-Time Scaling of Diffusion Models](https://arxiv.org/abs/2602.03211)
*Yeongmin Kim,Donghyeok Shin,Byeonghu Na,Minsang Park,Richard Lee Kim,Il-Chul Moon*

Main category: cs.LG

TL;DR: 本文提出LiDAR采样方案，利用预训练扩散模型的边缘样本计算预期未来奖励，引入前瞻采样提高效率，实现性能显著提升和加速。


<details>
  <summary>Details</summary>
Motivation: 扩散模型生成样本常无法完全符合人类意图，现有梯度引导方法计算预期未来奖励计算成本高。

Method: 提出仅用预训练扩散模型的边缘样本计算预期未来奖励的公式，引入前瞻采样收集边缘样本，使用精确求解器引导粒子向高奖励前瞻样本生成最终样本。

Result: LiDAR采样方案仅用3个样本和3步前瞻求解器实现性能大幅提升，随着前瞻精度和样本数量增加性能提升明显，速度比最新梯度引导方法快9.5倍。

Conclusion: LiDAR采样方案能有效解决扩散模型生成样本不符合人类意图问题，且计算效率高。

Abstract: Diffusion models have demonstrated strong generative performance; however, generated samples often fail to fully align with human intent. This paper studies a test-time scaling method that enables sampling from regions with higher human-aligned reward values. Existing gradient guidance methods approximate the expected future reward (EFR) at an intermediate particle $\mathbf{x}_t$ using a Taylor approximation, but this approximation at each time step incurs high computational cost due to sequential neural backpropagation. We show that the EFR at any $\mathbf{x}_t$ can be computed using only marginal samples from a pre-trained diffusion model. The proposed EFR formulation detaches the neural dependency between $\mathbf{x}_t$ and the EFR, enabling closed-form guidance computation without neural backpropagation. To further improve efficiency, we introduce lookahead sampling to collect marginal samples. For final sample generation, we use an accurate solver that guides particles toward high-reward lookahead samples. We refer to this sampling scheme as LiDAR sampling. LiDAR achieves substantial performance improvements using only three samples with a 3-step lookahead solver, exhibiting steep performance gains as lookahead accuracy and sample count increase; notably, it reaches the same GenEval performance as the latest gradient guidance method for SDXL with a 9.5x speedup.

</details>


### [281] [Topology Matters: A Cautionary Case Study of Graph SSL on Neuro-Inspired Benchmarks](https://arxiv.org/abs/2602.03217)
*May Kristine Jonson Carlon,Su Myat Noe,Haojiong Wang,Yasuo Kuniyoshi*

Main category: cs.LG

TL;DR: 引入分层自监督学习框架研究脑连接组，发现通用图自监督学习应用于类脑连接组数据有根本缺陷，需新的拓扑感知目标函数


<details>
  <summary>Details</summary>
Motivation: 理解局部交互如何产生全局大脑组织，需要能跨多尺度表示信息的模型

Method: 引入分层自监督学习框架，构建可控合成基准，采用四阶段评估协议，进行消融实验

Result: 基于不变性的自监督学习模型与基准拓扑属性不符，被经典拓扑感知启发式方法超越；自监督目标学习忽略经典方法利用的社区结构

Conclusion: 通用图自监督学习应用于类脑连接组数据有根本缺陷，神经人工智能研究需新的拓扑感知自监督目标函数

Abstract: Understanding how local interactions give rise to global brain organization requires models that can represent information across multiple scales. We introduce a hierarchical self-supervised learning (SSL) framework that jointly learns node-, edge-, and graph-level embeddings, inspired by multimodal neuroimaging. We construct a controllable synthetic benchmark mimicking the topological properties of connectomes. Our four-stage evaluation protocol reveals a critical failure: the invariance-based SSL model is fundamentally misaligned with the benchmark's topological properties and is catastrophically outperformed by classical, topology-aware heuristics. Ablations confirm an objective mismatch: SSL objectives designed to be invariant to topological perturbations learn to ignore the very community structure that classical methods exploit. Our results expose a fundamental pitfall in applying generic graph SSL to connectome-like data. We present this framework as a cautionary case study, highlighting the need for new, topology-aware SSL objectives for neuro-AI research that explicitly reward the preservation of structure (e.g., modularity or motifs).

</details>


### [282] [BayeSQP: Bayesian Optimization through Sequential Quadratic Programming](https://arxiv.org/abs/2602.03232)
*Paul Brunzema,Sebastian Trimpe*

Main category: cs.LG

TL;DR: 介绍了结合顺序二次规划和贝叶斯优化的BayeSQP算法，在高维场景表现优，连接了经典与现代黑箱优化方法。


<details>
  <summary>Details</summary>
Motivation: 开发一种新的通用黑箱优化算法，结合经典与现代优化方法的优势。

Method: 使用二阶高斯过程替代目标和约束函数，构建局部子问题并求解搜索方向，通过约束汤普森采样进行一维线搜索选择下一个评估点。

Result: 在特定高维设置中，BayeSQP算法优于现有最先进方法。

Conclusion: BayeSQP提供了一个有原则且灵活的框架，连接了经典与现代黑箱优化方法。

Abstract: We introduce BayeSQP, a novel algorithm for general black-box optimization that merges the structure of sequential quadratic programming with concepts from Bayesian optimization. BayeSQP employs second-order Gaussian process surrogates for both the objective and constraints to jointly model the function values, gradients, and Hessian from only zero-order information. At each iteration, a local subproblem is constructed using the GP posterior estimates and solved to obtain a search direction. Crucially, the formulation of the subproblem explicitly incorporates uncertainty in both the function and derivative estimates, resulting in a tractable second-order cone program for high probability improvements under model uncertainty. A subsequent one-dimensional line search via constrained Thompson sampling selects the next evaluation point. Empirical results show thatBayeSQP outperforms state-of-the-art methods in specific high-dimensional settings. Our algorithm offers a principled and flexible framework that bridges classical optimization techniques with modern approaches to black-box optimization.

</details>


### [283] [Merging Beyond: Streaming LLM Updates via Activation-Guided Rotations](https://arxiv.org/abs/2602.03237)
*Yuxuan Yao,Haonan Sheng,Qingsong Lv,Han Wu,Shuqi Liu,Zehua Liu,Zengyan Liu,Jiahui Gao,Haochen Tan,Xiaojin Fu,Haoli Bai,Hing Cheung So,Zhijiang Guo,Linqi Song*

Main category: cs.LG

TL;DR: 提出Streaming Merging范式及ARM策略用于高效模型适配，实验证明其能超越收敛检查点，提供可扩展轻量级框架。


<details>
  <summary>Details</summary>
Motivation: 大语言模型规模增大需高效适配技术，现有模型合并技术有局限，无法捕捉监督微调动态优化优势。

Method: 提出Streaming Merging范式，以ARM策略近似梯度下降动态，将合并系数视为学习率，从激活子空间导出旋转向量，使参数更新沿数据驱动轨迹。

Result: 实验在不同模型规模和领域中表明，ARM能超越收敛检查点。

Conclusion: ARM为高效模型适配提供了可扩展且轻量级的框架。

Abstract: The escalating scale of Large Language Models (LLMs) necessitates efficient adaptation techniques. Model merging has gained prominence for its efficiency and controllability. However, existing merging techniques typically serve as post-hoc refinements or focus on mitigating task interference, often failing to capture the dynamic optimization benefits of supervised fine-tuning (SFT). In this work, we propose Streaming Merging, an innovative model updating paradigm that conceptualizes merging as an iterative optimization process. Central to this paradigm is \textbf{ARM} (\textbf{A}ctivation-guided \textbf{R}otation-aware \textbf{M}erging), a strategy designed to approximate gradient descent dynamics. By treating merging coefficients as learning rates and deriving rotation vectors from activation subspaces, ARM effectively steers parameter updates along data-driven trajectories. Unlike conventional linear interpolation, ARM aligns semantic subspaces to preserve the geometric structure of high-dimensional parameter evolution. Remarkably, ARM requires only early SFT checkpoints and, through iterative merging, surpasses the fully converged SFT model. Experimental results across model scales (1.7B to 14B) and diverse domains (e.g., math, code) demonstrate that ARM can transcend converged checkpoints. Extensive experiments show that ARM provides a scalable and lightweight framework for efficient model adaptation.

</details>


### [284] [GraDE: A Graph Diffusion Estimator for Frequent Subgraph Discovery in Neural Architectures](https://arxiv.org/abs/2602.03257)
*Yikang Yang,Zhengxin Yang,Minghao Luo,Luzhou Peng,Hongxiao Li,Wanling Gao,Lei Wang,Jianfeng Zhan*

Main category: cs.LG

TL;DR: 本文提出GraDE框架解决现有方法在寻找神经架构频繁子图模式时的问题，实验显示其有优异表现。


<details>
  <summary>Details</summary>
Motivation: 现有枚举法计算成本高，采样法发现能力差，需新方法确保计算可行性与发现能力。

Method: 提出GraDE框架，引入图扩散模型，通过对学习分布中典型性打分识别频繁子图。

Result: 估计器排名准确率高，比采样基线最多提高114%；框架成功发现大规模频繁模式，中位频率比采样法最多高30倍。

Conclusion: GraDE框架在寻找神经架构频繁子图模式方面效果良好，能解决现有方法的问题。

Abstract: Finding frequently occurring subgraph patterns or network motifs in neural architectures is crucial for optimizing efficiency, accelerating design, and uncovering structural insights. However, as the subgraph size increases, enumeration-based methods are perfectly accurate but computationally prohibitive, while sampling-based methods are computationally tractable but suffer from a severe decline in discovery capability. To address these challenges, this paper proposes GraDE, a diffusion-guided search framework that ensures both computational feasibility and discovery capability. The key innovation is the Graph Diffusion Estimator (GraDE), which is the first to introduce graph diffusion models to identify frequent subgraphs by scoring their typicality within the learned distribution. Comprehensive experiments demonstrate that the estimator achieves superior ranking accuracy, with up to 114\% improvement compared to sampling-based baselines. Benefiting from this, the proposed framework successfully discovers large-scale frequent patterns, achieving up to 30$\times$ higher median frequency than sampling-based methods.

</details>


### [285] [Beyond Suffixes: Token Position in GCG Adversarial Attacks on Large Language Models](https://arxiv.org/abs/2602.03265)
*Hicham Eddoubi,Umar Faruk Abdullahi,Fadi Hassan*

Main category: cs.LG

TL;DR: 研究LMM越狱攻击中对抗性标记位置影响攻击成功率，指出当前安全评估盲点。


<details>
  <summary>Details</summary>
Motivation: 大语言模型广泛使用，需强大安全对齐机制，但越狱攻击使鲁棒性具挑战。

Method: 以GCG攻击为例，研究越狱攻击中对抗性标记在提示中的位置，评估优化攻击生成前缀而非后缀、改变评估时对抗标记位置的影响。

Result: 优化攻击生成前缀而非后缀、改变对抗性标记位置会显著影响攻击成功率。

Conclusion: 现有安全评估存在关键盲点，对抗鲁棒性评估需考虑对抗标记位置。

Abstract: Large Language Models (LLMs) have seen widespread adoption across multiple domains, creating an urgent need for robust safety alignment mechanisms. However, robustness remains challenging due to jailbreak attacks that bypass alignment via adversarial prompts. In this work, we focus on the prevalent Greedy Coordinate Gradient (GCG) attack and identify a previously underexplored attack axis in jailbreak attacks typically framed as suffix-based: the placement of adversarial tokens within the prompt. Using GCG as a case study, we show that both optimizing attacks to generate prefixes instead of suffixes and varying adversarial token position during evaluation substantially influence attack success rates. Our findings highlight a critical blind spot in current safety evaluations and underline the need to account for the position of adversarial tokens in the adversarial robustness evaluation of LLMs.

</details>


### [286] [Unveiling Covert Toxicity in Multimodal Data via Toxicity Association Graphs: A Graph-Based Metric and Interpretable Detection Framework](https://arxiv.org/abs/2602.03268)
*Guanzong Wu,Zihao Zhu,Siwei Lyu,Baoyuan Wu*

Main category: cs.LG

TL;DR: 提出基于TAGs的检测框架和MTC指标，构建数据集验证方法，实验表明该方法优于现有方法，推动可解释多模态毒性检测发展。


<details>
  <summary>Details</summary>
Motivation: 多模态数据中毒性检测困难，有害含义常潜藏于各模态组合中，需解决该问题。

Method: 提出基于TAGs的检测框架，引入MTC指标，构建Covert Toxic Dataset进行验证。

Result: 所提方法在低和高隐蔽性毒性检测中均优于现有方法，检测结果清晰、可解释、可审计。

Conclusion: 研究成果推动了可解释多模态毒性检测的发展，为未来上下文感知和可解释方法奠定基础。

Abstract: Detecting toxicity in multimodal data remains a significant challenge, as harmful meanings often lurk beneath seemingly benign individual modalities: only emerging when modalities are combined and semantic associations are activated. To address this, we propose a novel detection framework based on Toxicity Association Graphs (TAGs), which systematically model semantic associations between innocuous entities and latent toxic implications. Leveraging TAGs, we introduce the first quantifiable metric for hidden toxicity, the Multimodal Toxicity Covertness (MTC), which measures the degree of concealment in toxic multimodal expressions. By integrating our detection framework with the MTC metric, our approach enables precise identification of covert toxicity while preserving full interpretability of the decision-making process, significantly enhancing transparency in multimodal toxicity detection. To validate our method, we construct the Covert Toxic Dataset, the first benchmark specifically designed to capture high-covertness toxic multimodal instances. This dataset encodes nuanced cross-modal associations and serves as a rigorous testbed for evaluating both the proposed metric and detection framework. Extensive experiments demonstrate that our approach outperforms existing methods across both low- and high-covertness toxicity regimes, while delivering clear, interpretable, and auditable detection outcomes. Together, our contributions advance the state of the art in explainable multimodal toxicity detection and lay the foundation for future context-aware and interpretable approaches. Content Warning: This paper contains examples of toxic multimodal content that may be offensive or disturbing to some readers. Reader discretion is advised.

</details>


### [287] [BlockRR: A Unified Framework of RR-type Algorithms for Label Differential Privacy](https://arxiv.org/abs/2602.03277)
*Haixia Liu,Yi Ding*

Main category: cs.LG

TL;DR: 本文提出用于标签差分隐私的BlockRR机制，理论证明其满足ε - 标签DP，设计分区方法，实验表明在不同隐私机制下有良好表现。


<details>
  <summary>Details</summary>
Motivation: 现有RR型机制需单独分析，提出统一的随机响应机制以消除这种需求。

Method: 提出BlockRR机制，基于标签先验信息的权重矩阵设计分区方法，利用并行组合原则。

Result: 在高和中度隐私机制下（ε ≤ 3.0），提出方法在测试准确率和每类准确率平均值间取得更好平衡；低隐私机制下（ε ≥ 4.0），所有方法将BlockRR简化为标准RR且无额外性能损失。

Conclusion: BlockRR是一种有效的标签差分隐私随机响应机制，在不同隐私机制下均有良好表现。

Abstract: In this paper, we introduce BlockRR, a novel and unified randomized-response mechanism for label differential privacy. This framework generalizes existed RR-type mechanisms as special cases under specific parameter settings, which eliminates the need for separate, case-by-case analysis. Theoretically, we prove that BlockRR satisfies $ε$-label DP. We also design a partition method for BlockRR based on a weight matrix derived from label prior information; the parallel composition principle ensures that the composition of two such mechanisms remains $ε$-label DP. Empirically, we evaluate BlockRR on two variants of CIFAR-10 with varying degrees of class imbalance. Results show that in the high-privacy and moderate-privacy regimes ($ε\leq 3.0$), our propsed method gets a better balance between test accuaracy and the average of per-class accuracy. In the low-privacy regime ($ε\geq 4.0$), all methods reduce BlockRR to standard RR without additional performance loss.

</details>


### [288] [Universal Approximation of Continuous Functionals on Compact Subsets via Linear Measurements and Scalar Nonlinearities](https://arxiv.org/abs/2602.03290)
*Andrey Krylov,Maksim Penkin*

Main category: cs.LG

TL;DR: 研究希尔伯特空间乘积的紧子集上连续泛函的通用逼近，证明其可被特定模型逼近并推广结果，为相关设计模式提供依据。


<details>
  <summary>Details</summary>
Motivation: 探究希尔伯特空间乘积紧子集上连续泛函的逼近问题。

Method: 证明任意连续泛函可通过先对输入进行有限次连续线性测量，再通过连续标量非线性组合这些测量值的模型来均匀逼近，并将逼近原理扩展到取值于巴拿赫空间的映射。

Result: 得出连续泛函可被特定模型逼近，且将逼近原理扩展后得到有限秩逼近。

Conclusion: 为算子学习和成像中常用的设计模式提供了紧集上的合理性证明。

Abstract: We study universal approximation of continuous functionals on compact subsets of products of Hilbert spaces. We prove that any such functional can be uniformly approximated by models that first take finitely many continuous linear measurements of the inputs and then combine these measurements through continuous scalar nonlinearities. We also extend the approximation principle to maps with values in a Banach space, yielding finite-rank approximations. These results provide a compact-set justification for the common ``measure, apply scalar nonlinearities, then combine'' design pattern used in operator learning and imaging.

</details>


### [289] [Anomaly Detection via Mean Shift Density Enhancement](https://arxiv.org/abs/2602.03293)
*Pritam Kar,Rahul Bordoloi,Olaf Wolkenhauer,Saptarshi Bej*

Main category: cs.LG

TL;DR: 提出MSDE用于无监督异常检测，在多噪声水平和异常类型上比基准模型表现更好，证明位移评分的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有无监督异常检测算法在不同异常类型和噪声环境下鲁棒性不足。

Method: 提出MSDE框架，基于密度驱动流形演化检测异常，使用UMAP模糊邻域图导出权重，通过均值漂移迭代位移累计得分。

Result: 在ADBench基准上，与13种无监督基线模型相比，在多个噪声水平和多种异常类型平均值上，MSDE在AUC - ROC、AUC - PR和Precision@n上表现一致、平衡且稳健。

Conclusion: 基于位移的评分方法为无监督异常检测提供了优于现有技术的可靠替代方案。

Abstract: Unsupervised anomaly detection stands as an important problem in machine learning, with applications in financial fraud prevention, network security and medical diagnostics. Existing unsupervised anomaly detection algorithms rarely perform well across different anomaly types, often excelling only under specific structural assumptions. This lack of robustness also becomes particularly evident under noisy settings. We propose Mean Shift Density Enhancement (MSDE), a fully unsupervised framework that detects anomalies through their geometric response to density-driven manifold evolution. MSDE is based on the principle that normal samples, being well supported by local density, remain stable under iterative density enhancement, whereas anomalous samples undergo large cumulative displacements as they are attracted toward nearby density modes. To operationalize this idea, MSDE employs a weighted mean-shift procedure with adaptive, sample-specific density weights derived from a UMAP-based fuzzy neighborhood graph. Anomaly scores are defined by the total displacement accumulated across a small number of mean-shift iterations. We evaluate MSDE on the ADBench benchmark, comprising forty six real-world tabular datasets, four realistic anomaly generation mechanisms, and six noise levels. Compared to 13 established unsupervised baselines, MSDE achieves consistently strong, balanced and robust performance for AUC-ROC, AUC-PR, and Precision@n, at several noise levels and on average over several types of anomalies. These results demonstrate that displacement-based scoring provides a robust alternative to the existing state-of-the-art for unsupervised anomaly detection.

</details>


### [290] [Lipschitz Multiscale Deep Equilibrium Models: A Theoretically Guaranteed and Accelerated Approach](https://arxiv.org/abs/2602.03297)
*Naoki Sato,Hideaki Iiduka*

Main category: cs.LG

TL;DR: 本文提出Lipschitz多尺度DEQ改进DEQ定点收敛性，减少计算时间，在CIFAR - 10实验中提速4.75倍但精度略有下降。


<details>
  <summary>Details</summary>
Motivation: DEQ训练和推理计算时间长且定点迭代无收敛保证，需改进收敛性以减少计算时间。

Method: 通过重构模型架构，提出Lipschitz多尺度DEQ，通过调整超参数保证前后向定点收敛。

Result: 在CIFAR - 10数值实验中实现了最高4.75倍的提速，精度有轻微下降。

Conclusion: 提出的方法能有效改善DEQ定点收敛性，减少计算时间。

Abstract: Deep equilibrium models (DEQs) achieve infinitely deep network representations without stacking layers by exploring fixed points of layer transformations in neural networks. Such models constitute an innovative approach that achieves performance comparable to state-of-the-art methods in many large-scale numerical experiments, despite requiring significantly less memory. However, DEQs face the challenge of requiring vastly more computational time for training and inference than conventional methods, as they repeatedly perform fixed-point iterations with no convergence guarantee upon each input. Therefore, this study explored an approach to improve fixed-point convergence and consequently reduce computational time by restructuring the model architecture to guarantee fixed-point convergence. Our proposed approach for image classification, Lipschitz multiscale DEQ, has theoretically guaranteed fixed-point convergence for both forward and backward passes by hyperparameter adjustment, achieving up to a 4.75$\times$ speed-up in numerical experiments on CIFAR-10 at the cost of a minor drop in accuracy.

</details>


### [291] [R1-SyntheticVL: Is Synthetic Data from Generative Models Ready for Multimodal Large Language Model?](https://arxiv.org/abs/2602.03300)
*Jingyi Zhang,Tianyi Lin,Huanjin Yao,Xiang Lan,Shunyu Liu,Jiaxing Huang*

Main category: cs.LG

TL;DR: 提出CADS方法合成多模态数据，构建MMSynthetic - 20K训练模型R1 - SyntheticVL，在基准测试表现优。


<details>
  <summary>Details</summary>
Motivation: 开发有效数据合成技术，为增强MLLMs解决复杂现实任务能力，自主合成多模态训练数据。

Method: 提出Collective Adversarial Data Synthesis (CADS)方法，有CAD - Generate和CAD - Judge两个循环阶段，还引入Adversarial Context Optimization机制。

Result: 构建MMSynthetic - 20K，训练模型R1 - SyntheticVL，在多个基准测试中表现出色。

Conclusion: CADS方法能有效合成高质量、多样且具挑战性的多模态数据，可提升MLLMs性能。

Abstract: In this work, we aim to develop effective data synthesis techniques that autonomously synthesize multimodal training data for enhancing MLLMs in solving complex real-world tasks. To this end, we propose Collective Adversarial Data Synthesis (CADS), a novel and general approach to synthesize high-quality, diverse and challenging multimodal data for MLLMs. The core idea of CADS is to leverage collective intelligence to ensure high-quality and diverse generation, while exploring adversarial learning to synthesize challenging samples for effectively driving model improvement. Specifically, CADS operates with two cyclic phases, i.e., Collective Adversarial Data Generation (CAD-Generate) and Collective Adversarial Data Judgment (CAD-Judge). CAD-Generate leverages collective knowledge to jointly generate new and diverse multimodal data, while CAD-Judge collaboratively assesses the quality of synthesized data. In addition, CADS introduces an Adversarial Context Optimization mechanism to optimize the generation context to encourage challenging and high-value data generation. With CADS, we construct MMSynthetic-20K and train our model R1-SyntheticVL, which demonstrates superior performance on various benchmarks.

</details>


### [292] [Periodic Regularized Q-Learning](https://arxiv.org/abs/2602.03301)
*Hyukjun Yang,Han-Dong Lim,Donghwan Lee*

Main category: cs.LG

TL;DR: 提出新算法周期性正则化Q学习（PRQ），为线性函数近似下算法提供有限时间收敛性证明。


<details>
  <summary>Details</summary>
Motivation: 传统Q学习在线性函数近似下无收敛保证，现有研究引入正则化技术克服该局限。

Method: 在投影算子层面引入正则化，构建正则化投影值迭代（RP - VI）并扩展为基于样本的强化学习算法，将正则化投影扩展到随机设置得到PRQ算法。

Result: 使投影值迭代成为压缩映射，为PRQ算法提供有限时间收敛保证。

Conclusion: 提出的PRQ算法在线性函数近似下有有限时间收敛性。

Abstract: In reinforcement learning (RL), Q-learning is a fundamental algorithm whose convergence is guaranteed in the tabular setting. However, this convergence guarantee does not hold under linear function approximation. To overcome this limitation, a significant line of research has introduced regularization techniques to ensure stable convergence under function approximation. In this work, we propose a new algorithm, periodic regularized Q-learning (PRQ). We first introduce regularization at the level of the projection operator and explicitly construct a regularized projected value iteration (RP-VI), subsequently extending it to a sample-based RL algorithm. By appropriately regularizing the projection operator, the resulting projected value iteration becomes a contraction. By extending this regularized projection into the stochastic setting, we establish the PRQ algorithm and provide a rigorous theoretical analysis that proves finite-time convergence guarantees for PRQ under linear function approximation.

</details>


### [293] [medR: Reward Engineering for Clinical Offline Reinforcement Learning via Tri-Drive Potential Functions](https://arxiv.org/abs/2602.03305)
*Qianyi Xu,Gousia Habib,Feng Wu,Yanrui Du,Zhihui Chen,Swapnil Mishra,Dilruk Perera,Mengling Feng*

Main category: cs.LG

TL;DR: 提出用大语言模型进行离线奖励设计和验证的自动化流程，以解决临床强化学习中奖励工程的瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 临床强化学习受限于奖励工程，现有方法依赖手动启发式，难以跨病理推广。

Method: 利用大语言模型构建自动化流程，用包含生存、信心和能力的势函数构建奖励函数，并引入量化指标评估选择最优奖励结构。

Result: 框架实现了特定疾病奖励函数的自动化设计，显著提升了策略性能。

Conclusion: 通过集成大语言模型驱动的领域知识，该框架能有效解决临床强化学习奖励工程问题。

Abstract: Reinforcement Learning (RL) offers a powerful framework for optimizing dynamic treatment regimes (DTRs). However, clinical RL is fundamentally bottlenecked by reward engineering: the challenge of defining signals that safely and effectively guide policy learning in complex, sparse offline environments. Existing approaches often rely on manual heuristics that fail to generalize across diverse pathologies. To address this, we propose an automated pipeline leveraging Large Language Models (LLMs) for offline reward design and verification. We formulate the reward function using potential functions consisted of three core components: survival, confidence, and competence. We further introduce quantitative metrics to rigorously evaluate and select the optimal reward structure prior to deployment. By integrating LLM-driven domain knowledge, our framework automates the design of reward functions for specific diseases while significantly enhancing the performance of the resulting policies.

</details>


### [294] [Entropy-Gated Selective Policy Optimization:Token-Level Gradient Allocation for Hybrid Training of Large Language Models](https://arxiv.org/abs/2602.03309)
*Yuelin Hu,Zhengxue Cheng,Wei Liu,Li Song*

Main category: cs.LG

TL;DR: 提出EGSPO框架，结合样本级混合与token级梯度调制，在数学推理基准上有提升且计算开销小


<details>
  <summary>Details</summary>
Motivation: 改进大语言模型的混合训练方法，提升训练效果

Method: 提出三阶段的EGSPO框架，包括SFT专家学习、RL rollout生成和EGSPO机制

Result: 在AIME和MATH基准上分别提升3.8%和2.9%，仅增加3.4%计算开销

Conclusion: EGSPO能有效提升大语言模型在数学推理上的性能且新增开销可接受

Abstract: Hybrid training methods for large language models combine supervised fine tuning (SFT) on expert demonstrations with reinforcement learning (RL) on model rollouts, typically at the sample level. We propose Entropy Gated Selective Policy Optimization (EGSPO), a three stage framework that extends sample level mixing with token level gradient modulation.
  Stage 1, SFT expert learning, establishes a reliable warm up policy using expert demonstrations with a pure SFT loss. Stage 2, RL rollout generation, samples trajectories from the current policy and computes per token predictive entropy. Stage 3, the EGSPO mechanism, applies entropy gated gradient allocation: a predictive entropy module routes high entropy tokens to full PPO updates to encourage exploration, and low entropy tokens to attenuated PPO updates to reduce variance and preserve knowledge. Critically, both branches incorporate the advantage function A_t, ensuring that incorrect trajectories receive consistent negative learning signals and preventing reinforcement of confident errors.
  EGSPO achieves consistent improvements on mathematical reasoning benchmarks, with gains of 3.8 percent on AIME and 2.9 percent on MATH over the CHORD phi baseline, while incurring only 3.4 percent additional computational overhead.

</details>


### [295] [Information-Theoretic Multi-Model Fusion for Target-Oriented Adaptive Sampling in Materials Design](https://arxiv.org/abs/2602.03319)
*Yixuan Zhang,Zhiyuan Li,Weijia He,Mian Dai,Chen Shen,Teng Long,Hongbin Zhang*

Main category: cs.LG

TL;DR: 提出面向目标的自适应采样信息论框架，在多材料设计任务中提升样本效率和可靠性，在合成基准测试中展现鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在有限评估预算下，解决高维、异构设计空间中面向目标的发现问题，新测量成本高。

Method: 将优化重构为轨迹发现，通过维度感知信息预算、自适应引导蒸馏、结构感知候选流形分析和多模型融合平衡开发与探索。

Result: 在14个单目标和多目标材料设计任务中提高样本效率和可靠性，通常100次评估内到达最优区域，合成基准测试展现鲁棒性。

Conclusion: 所提出的信息论框架在解决面向目标的发现问题上有效且鲁棒。

Abstract: Target-oriented discovery under limited evaluation budgets requires making reliable progress in high-dimensional, heterogeneous design spaces where each new measurement is costly, whether experimental or high-fidelity simulation. We present an information-theoretic framework for target-oriented adaptive sampling that reframes optimization as trajectory discovery: instead of approximating the full response surface, the method maintains and refines a low-entropy information state that concentrates search on target-relevant directions. The approach couples data, model beliefs, and physics/structure priors through dimension-aware information budgeting, adaptive bootstrapped distillation over a heterogeneous surrogate reservoir, and structure-aware candidate manifold analysis with Kalman-inspired multi-model fusion to balance consensus-driven exploitation and disagreement-driven exploration. Evaluated under a single unified protocol without dataset-specific tuning, the framework improves sample efficiency and reliability across 14 single- and multi-objective materials design tasks spanning candidate pools from $600$ to $4 \times 10^6$ and feature dimensions from $10$ to $10^3$, typically reaching top-performing regions within 100 evaluations. Complementary 20-dimensional synthetic benchmarks (Ackley, Rastrigin, Schwefel) further demonstrate robustness to rugged and multimodal landscapes.

</details>


### [296] [From Inexact Gradients to Byzantine Robustness: Acceleration and Optimization under Similarity](https://arxiv.org/abs/2602.03329)
*Renaud Gaucher,Aymeric Dieuleveut,Hadrien Hendrikx*

Main category: cs.LG

TL;DR: 论文指出标准联邦学习易受拜占庭攻击，将拜占庭鲁棒分布式优化转化为带不精确梯度的一般优化问题，提出两种加速收敛方案，理论和实验表明可降低通信复杂度。


<details>
  <summary>Details</summary>
Motivation: 标准联邦学习算法易受对抗节点攻击，现有鲁棒算法分析缺乏通用性，阻碍复杂算法发展。

Method: 将拜占庭鲁棒分布式优化转化为带不精确梯度的一般优化问题；提出Nesterov型加速方案和基于相似性的优化方案。

Result: 表明在拜占庭环境下基于标准鲁棒聚合的GD能获得最优渐近误差；理论和实验证明两种方案可大幅降低通信复杂度。

Conclusion: 通过转化问题和提出新方案，降低了拜占庭环境下分布式优化的通信复杂度，推动了复杂鲁棒算法的发展。

Abstract: Standard federated learning algorithms are vulnerable to adversarial nodes, a.k.a. Byzantine failures. To solve this issue, robust distributed learning algorithms have been developed, which typically replace parameter averaging by robust aggregations. While generic conditions on these aggregations exist to guarantee the convergence of (Stochastic) Gradient Descent (SGD), the analyses remain rather ad-hoc. This hinders the development of more complex robust algorithms, such as accelerated ones. In this work, we show that Byzantine-robust distributed optimization can, under standard generic assumptions, be cast as a general optimization with inexact gradient oracles (with both additive and multiplicative error terms), an active field of research.
  This allows for instance to directly show that GD on top of standard robust aggregation procedures obtains optimal asymptotic error in the Byzantine setting. Going further, we propose two optimization schemes to speed up the convergence. The first one is a Nesterov-type accelerated scheme whose proof directly derives from accelerated inexact gradient results applied to our formulation. The second one hinges on Optimization under Similarity, in which the server leverages an auxiliary loss function that approximates the global loss. Both approaches allow to drastically reduce the communication complexity compared to previous methods, as we show theoretically and empirically.

</details>


### [297] [Bayesian Conformal Prediction as a Decision Risk Problem](https://arxiv.org/abs/2602.03331)
*Fanyi Wu,Veronika Lohmanova,Samuel Kaski,Michele Caprio*

Main category: cs.LG

TL;DR: 使用贝叶斯后验预测密度和贝叶斯求积估计并最小化预期预测集大小，BCP在多种任务中表现良好。


<details>
  <summary>Details</summary>
Motivation: 在模型误设情况下，实现有效覆盖保证并减少预测集大小的运行间变异性。

Method: 采用贝叶斯后验预测密度作为非一致性分数和贝叶斯求积，在拆分共形框架下操作。

Result: 在回归和分类任务中，预测集大小与拆分共形预测相当，运行间变异性更低；在稀疏回归中，经验覆盖表现优于贝叶斯可信区间。

Conclusion: BCP在模型误设时能提供有效覆盖保证，具有较好的性能。

Abstract: Bayesian posterior predictive densities as non-conformity scores and Bayesian quadrature are used to estimate and minimise the expected prediction set size. Operating within a split conformal framework, BCP provides valid coverage guarantees and demonstrates reliable empirical coverage under model misspecification. Across regression and classification tasks, including distribution-shifted settings such as ImageNet-A, BCP yields prediction sets of comparable size to split conformal prediction, while exhibiting substantially lower run-to-run variability in set size. In sparse regression with nominal coverage of 80 percent, BCP achieves 81 percent empirical coverage under a misspecified prior, whereas Bayesian credible intervals under-cover at 49 percent.

</details>


### [298] [Robustness as an Emergent Property of Task Performance](https://arxiv.org/abs/2602.03344)
*Shir Ashury-Tahan,Ariel Gera,Elron Bandel,Michal Shmueli-Scheuer,Leshem Choshen*

Main category: cs.LG

TL;DR: 研究表明模型在任务上接近高性能时可有效实现鲁棒性，鲁棒性主要由特定任务能力驱动，对研究者和从业者有不同启示。


<details>
  <summary>Details</summary>
Motivation: 探究现实应用中模型鲁棒性这一关键挑战，验证是否简单任务无论呈现方式如何都更易完成。

Method: 对多种模型在不同数据集和配置下进行实证分析。

Result: 发现模型性能与鲁棒性有强正相关，鲁棒性主要由特定任务能力而非模型固有属性驱动。

Conclusion: 新任务饱和时模型在这些任务上的鲁棒性会相应出现，研究者可减少对测量和提高鲁棒性的关注，从业者可认为模型在简单过往任务上可靠可用于实际部署。

Abstract: Robustness is often regarded as a critical future challenge for real-world applications, where stability is essential. However, as models often learn tasks in a similar order, we hypothesize that easier tasks will be easier regardless of how they are presented to the model. Indeed, in this paper, we show that as models approach high performance on a task, robustness is effectively achieved. Through an empirical analysis of multiple models across diverse datasets and configurations (e.g., paraphrases, different temperatures), we find a strong positive correlation. Moreover, we find that robustness is primarily driven by task-specific competence rather than inherent model-level properties, challenging current approaches that treat robustness as an independent capability. Thus, from a high-level perspective, we may expect that as new tasks saturate, model robustness on these tasks will emerge accordingly. For researchers, this implies that explicit efforts to measure and improve robustness may warrant reduced emphasis, as such robustness is likely to develop alongside performance gains. For practitioners, it acts as a sign that indeed the tasks that the literature deals with are unreliable, but on easier past tasks, the models are reliable and ready for real-world deployment.

</details>


### [299] [Causal Graph Learning via Distributional Invariance of Cause-Effect Relationship](https://arxiv.org/abs/2602.03353)
*Nang Hung Nguyen,Phi Le Nguyen,Thao Nguyen Truong,Trong Nghia Hoang,Masashi Sugiyama*

Main category: cs.LG

TL;DR: 本文提出从观测数据恢复因果图的新框架，利用因果条件分布不变性，开发算法高效发现因果关系，性能优且可扩展性强。


<details>
  <summary>Details</summary>
Motivation: 提出新方法从观测数据恢复因果图，提高处理效率和可扩展性。

Method: 利用效应条件分布对原因先验分布变化的不变性，通过检查多子集效应 - 原因条件分布方差进行因果关系测试，并结合因果图稀疏性开发算法。

Result: 算法复杂度为二次，比现有方法处理时间最多减少25倍，在大规模数据集基准测试中性能优或相当，可扩展性增强。

Conclusion: 新框架和算法能有效从观测数据恢复因果图，在效率和性能上表现良好。

Abstract: This paper introduces a new framework for recovering causal graphs from observational data, leveraging the observation that the distribution of an effect, conditioned on its causes, remains invariant to changes in the prior distribution of those causes. This insight enables a direct test for potential causal relationships by checking the variance of their corresponding effect-cause conditional distributions across multiple downsampled subsets of the data. These subsets are selected to reflect different prior cause distributions, while preserving the effect-cause conditional relationships. Using this invariance test and exploiting an (empirical) sparsity of most causal graphs, we develop an algorithm that efficiently uncovers causal relationships with quadratic complexity in the number of observational variables, reducing the processing time by up to 25x compared to state-of-the-art methods. Our empirical experiments on a varied benchmark of large-scale datasets show superior or equivalent performance compared to existing works, while achieving enhanced scalability.

</details>


### [300] [Achieving Linear Speedup for Composite Federated Learning](https://arxiv.org/abs/2602.03357)
*Kun Huang,Shi Pu*

Main category: cs.LG

TL;DR: 提出FedNMap用于复合联邦学习，处理非光滑项，缓解数据异质性影响，对非凸损失实现线性加速。


<details>
  <summary>Details</summary>
Motivation: 解决复合联邦学习中目标包含非光滑正则项以及客户端数据异质性的问题。

Method: 采用基于法向映射的更新方案处理非光滑项，结合本地校正策略缓解数据异质性影响。

Result: 在标准假设下，对非凸损失，无论有无PL条件，FedNMap在客户端数量n和本地更新次数Q上都实现了线性加速。

Conclusion: FedNMap是首个在非凸复合联邦学习中建立线性加速的方法。

Abstract: This paper proposes FedNMap, a normal map-based method for composite federated learning, where the objective consists of a smooth loss and a possibly nonsmooth regularizer. FedNMap leverages a normal map-based update scheme to handle the nonsmooth term and incorporates a local correction strategy to mitigate the impact of data heterogeneity across clients. Under standard assumptions, including smooth local losses, weak convexity of the regularizer, and bounded stochastic gradient variance, FedNMap achieves linear speedup with respect to both the number of clients $n$ and the number of local updates $Q$ for nonconvex losses, both with and without the Polyak-Łojasiewicz (PL) condition. To our knowledge, this is the first result establishing linear speedup for nonconvex composite federated learning.

</details>


### [301] [MeKi: Memory-based Expert Knowledge Injection for Efficient LLM Scaling](https://arxiv.org/abs/2602.03359)
*Ning Ding,Fangcheng Liu,Kyungrae Kim,Linji Hao,Kyeng-Hun Lee,Hyeonmok Ko,Yehui Tang*

Main category: cs.LG

TL;DR: 提出MeKi系统，通过存储而非FLOPs扩展大语言模型容量，零推理延迟开销，表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 传统扩展大语言模型的策略因边缘设备资源限制难以部署，需提升边缘设备上模型性能以改善用户体验。

Method: 提出MeKi系统，为Transformer层配备基于令牌的记忆专家注入语义知识，采用重新参数化策略将训练参数矩阵折叠为静态查找表，将知识卸载到ROM。

Result: MeKi在相同推理速度下显著优于密集大语言模型基线。

Conclusion: 基于内存的扩展范式对设备端大语言模型有效。

Abstract: Scaling Large Language Models (LLMs) typically relies on increasing the number of parameters or test-time computations to boost performance. However, these strategies are impractical for edge device deployment due to limited RAM and NPU resources. Despite hardware constraints, deploying performant LLM on edge devices such as smartphone remains crucial for user experience. To address this, we propose MeKi (Memory-based Expert Knowledge Injection), a novel system that scales LLM capacity via storage space rather than FLOPs. MeKi equips each Transformer layer with token-level memory experts that injects pre-stored semantic knowledge into the generation process. To bridge the gap between training capacity and inference efficiency, we employ a re-parameterization strategy to fold parameter matrices used during training into a compact static lookup table. By offloading the knowledge to ROM, MeKi decouples model capacity from computational cost, introducing zero inference latency overhead. Extensive experiments demonstrate that MeKi significantly outperforms dense LLM baselines with identical inference speed, validating the effectiveness of memory-based scaling paradigm for on-device LLMs. Project homepage is at https://github.com/ningding-o/MeKi.

</details>


### [302] [Rethinking Benign Relearning: Syntax as the Hidden Driver of Unlearning Failures](https://arxiv.org/abs/2602.03379)
*Sangyeon Yoon,Hyesoo Hong,Wonje Jeung,Albert No*

Main category: cs.LG

TL;DR: 现有机器学习遗忘方法易受良性重学影响，研究发现句法相似性是主因，提出句法多样化方法抑制该现象。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习遗忘方法存在良性重学问题，即遗忘信息会因良性微调数据重新出现，而原解释不足。

Method: 通过系统分析确定句法相似性为驱动因素，提出句法多样化方法，在遗忘前将原遗忘查询改写为异构结构。

Result: 该方法有效抑制良性重学，加速遗忘，缓解遗忘效果与模型效用间的权衡。

Conclusion: 句法多样化方法能解决现有机器学习遗忘方法的脆弱性问题。

Abstract: Machine unlearning aims to remove specific content from trained models while preserving overall performance. However, the phenomenon of benign relearning, in which forgotten information reemerges even from benign fine-tuning data, reveals that existing unlearning methods remain fundamentally fragile. A common explanation attributes this effect to topical relevance, but we find this account insufficient. Through systematic analysis, we demonstrate that syntactic similarity, rather than topicality, is the primary driver: across benchmarks, syntactically similar data consistently trigger recovery even without topical overlap, due to their alignment in representations and gradients with the forgotten content. Motivated by this insight, we introduce syntactic diversification, which paraphrases the original forget queries into heterogeneous structures prior to unlearning. This approach effectively suppresses benign relearning, accelerates forgetting, and substantially alleviates the trade-off between unlearning efficacy and model utility.

</details>


### [303] [An Approximate Ascent Approach To Prove Convergence of PPO](https://arxiv.org/abs/2602.03386)
*Leif Doering,Daniel Schmidt,Moritz Melcher,Sebastian Kassing,Benedikt Wille,Tilman Aach,Simon Weissmann*

Main category: cs.LG

TL;DR: 研究PPO理论基础，证明其收敛性并指出截断GAE问题及解决方法


<details>
  <summary>Details</summary>
Motivation: PPO理论基础不完善，收敛性和核心优势理解不充分

Method: 将PPO策略更新方案解释为近似策略梯度上升，控制替代梯度累积偏差，用随机重排技术证明收敛性

Result: 证明PPO收敛性，发现截断GAE问题，简单权重修正可在特定环境提升性能

Conclusion: 该研究完善PPO理论基础，提出的修正方法有实际应用价值

Abstract: Proximal Policy Optimization (PPO) is among the most widely used deep reinforcement learning algorithms, yet its theoretical foundations remain incomplete. Most importantly, convergence and understanding of fundamental PPO advantages remain widely open. Under standard theory assumptions we show how PPO's policy update scheme (performing multiple epochs of minibatch updates on multi-use rollouts with a surrogate gradient) can be interpreted as approximated policy gradient ascent. We show how to control the bias accumulated by the surrogate gradients and use techniques from random reshuffling to prove a convergence theorem for PPO that sheds light on PPO's success. Additionally, we identify a previously overlooked issue in truncated Generalized Advantage Estimation commonly used in PPO. The geometric weighting scheme induces infinite mass collapse onto the longest $k$-step advantage estimator at episode boundaries. Empirical evaluations show that a simple weight correction can yield substantial improvements in environments with strong terminal signal, such as Lunar Lander.

</details>


### [304] [Chain-of-Goals Hierarchical Policy for Long-Horizon Offline Goal-Conditioned RL](https://arxiv.org/abs/2602.03389)
*Jinwoo Choi,Sang-Hyun Lee,Seung-Woo Seo*

Main category: cs.LG

TL;DR: 提出CoGHP框架解决离线目标条件强化学习长程任务难题，在基准测试中表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有分层方法在解决长程任务时，多依赖分离的高低层网络且仅生成单一子目标，无法处理需协调多决策的复杂任务。

Method: 借鉴思维链范式，提出CoGHP框架，将分层决策重新表述为统一架构内的自回归序列建模，使用MLP - Mixer骨干网络。

Result: 在具有挑战性的导航和操作基准测试中，CoGHP始终优于强大的离线基线方法。

Conclusion: CoGHP框架能有效提升长程任务的性能。

Abstract: Offline goal-conditioned reinforcement learning remains challenging for long-horizon tasks. While hierarchical approaches mitigate this issue by decomposing tasks, most existing methods rely on separate high- and low-level networks and generate only a single intermediate subgoal, making them inadequate for complex tasks that require coordinating multiple intermediate decisions. To address this limitation, we draw inspiration from the chain-of-thought paradigm and propose the Chain-of-Goals Hierarchical Policy (CoGHP), a novel framework that reformulates hierarchical decision-making as autoregressive sequence modeling within a unified architecture. Given a state and a final goal, CoGHP autoregressively generates a sequence of latent subgoals followed by the primitive action, where each latent subgoal acts as a reasoning step that conditions subsequent predictions. To implement this efficiently, we pioneer the use of an MLP-Mixer backbone, which supports cross-token communication and captures structural relationships among state, goal, latent subgoals, and action. Across challenging navigation and manipulation benchmarks, CoGHP consistently outperforms strong offline baselines, demonstrating improved performance on long-horizon tasks.

</details>


### [305] [On the Entropy Dynamics in Reinforcement Fine-Tuning of Large Language Models](https://arxiv.org/abs/2602.03392)
*Shumin Wang,Yuexiang Xie,Wenhao Zhang,Yuchang Sun,Yanxi Chen,Yaliang Li,Yanyong Zhang*

Main category: cs.LG

TL;DR: 本文建立理论框架分析强化微调（RFT）过程中的熵动态，设计熵控制方法，并通过实验验证其有效性，为大语言模型微调提供优化策略。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对强化微调过程中熵动态的原理性理解，本文旨在填补这一空白，以更好地平衡大语言模型探索与利用能力。

Method: 建立分析RFT过程中熵动态的理论框架，先给出单次对数几率更新下熵变化的判别式，推导熵变化的一阶表达式并扩展到GRPO更新公式。

Result: 理论分析得出的推论和见解启发了熵控制方法设计，通过实验验证了基于熵判别器裁剪方法的有效性。

Conclusion: 本研究为RFT训练动态带来新见解，为大语言模型微调中探索 - 利用平衡优化提供理论支持和实践策略。

Abstract: Entropy serves as a critical metric for measuring the diversity of outputs generated by large language models (LLMs), providing valuable insights into their exploration capabilities. While recent studies increasingly focus on monitoring and adjusting entropy to better balance exploration and exploitation in reinforcement fine-tuning (RFT), a principled understanding of entropy dynamics during this process is yet to be thoroughly investigated. In this paper, we establish a theoretical framework for analyzing the entropy dynamics during the RFT process, which begins with a discriminant expression that quantifies entropy change under a single logit update. This foundation enables the derivation of a first-order expression for entropy change, which can be further extended to the update formula of Group Relative Policy Optimization (GRPO). The corollaries and insights drawn from the theoretical analysis inspire the design of entropy control methods, and also offer a unified lens for interpreting various entropy-based methods in existing studies. We provide empirical evidence to support the main conclusions of our analysis and demonstrate the effectiveness of the derived entropy-discriminator clipping methods. This study yields novel insights into RFT training dynamics, providing theoretical support and practical strategies for optimizing the exploration-exploitation balance during LLM fine-tuning.

</details>


### [306] [The Label Horizon Paradox: Rethinking Supervision Targets in Financial Forecasting](https://arxiv.org/abs/2602.03395)
*Chen-Hui Song,Shuoling Liu,Liyuan Chen*

Main category: cs.LG

TL;DR: 本文挑战深度学习金融预测中训练标签须严格匹配推理目标的假设，揭示标签视野悖论，提出双层优化框架，实验显示优于传统基线。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习金融预测中对监督信号设计关注不足，挑战训练标签严格匹配推理目标的假设。

Method: 从理论上基于动态信号 - 噪声权衡解释现象，提出双层优化框架自主识别最优代理标签。

Result: 在大规模金融数据集上的实验显示，相比传统基线有持续改进。

Conclusion: 为金融预测中以标签为中心的研究开辟了新途径。

Abstract: While deep learning has revolutionized financial forecasting through sophisticated architectures, the design of the supervision signal itself is rarely scrutinized. We challenge the canonical assumption that training labels must strictly mirror inference targets, uncovering the Label Horizon Paradox: the optimal supervision signal often deviates from the prediction goal, shifting across intermediate horizons governed by market dynamics. We theoretically ground this phenomenon in a dynamic signal-noise trade-off, demonstrating that generalization hinges on the competition between marginal signal realization and noise accumulation. To operationalize this insight, we propose a bi-level optimization framework that autonomously identifies the optimal proxy label within a single training run. Extensive experiments on large-scale financial datasets demonstrate consistent improvements over conventional baselines, thereby opening new avenues for label-centric research in financial forecasting.

</details>


### [307] [Most Convolutional Networks Suffer from Small Adversarial Perturbations](https://arxiv.org/abs/2602.03415)
*Amit Daniely,Idan Mehalel*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The existence of adversarial examples is relatively understood for random fully connected neural networks, but much less so for convolutional neural networks (CNNs). The recent work [Daniely, 2025] establishes that adversarial examples can be found in CNNs, in some non-optimal distance from the input. We extend over this work and prove that adversarial examples in random CNNs with input dimension $d$ can be found already in $\ell_2$-distance of order $\lVert x \rVert /\sqrt{d}$ from the input $x$, which is essentially the nearest possible. We also show that such adversarial small perturbations can be found using a single step of gradient descent. To derive our results we use Fourier decomposition to efficiently bound the singular values of a random linear convolutional operator, which is the main ingredient of a CNN layer. This bound might be of independent interest.

</details>


### [308] [Beyond Variance: Prompt-Efficient RLVR via Rare-Event Amplification and Bidirectional Pairing](https://arxiv.org/abs/2602.03452)
*Xin Sheng,Jiaxin Li,Yujuan Pang,Ran Peng,Yong Ma*

Main category: cs.LG

TL;DR: 本文从机制层面重新审视强化学习中提示选择问题，提出正-负配对方法和加权GRPO，在Qwen2.5 - Math - 7B等模型上提升了性能。


<details>
  <summary>Details</summary>
Motivation: 先前基于训练准确率方差的提示选择方法导致优化方向不稳定和迁移能力弱，需更好的提示选择方法。

Method: 提出正 - 负配对，每次更新时采样难但可解和易但脆弱的提示对；引入加权GRPO，在对级别重新加权二元结果并使用组归一化优势。

Result: 在Qwen2.5 - Math - 7B等模型上，每次更新使用单个配对小批量始终优于基于方差选择提示的GRPO基线，如AIME 2025 Pass@8从16.8提升到22.2等。

Conclusion: 正 - 负配对和加权GRPO方法能为成功和失败提供信息丰富的学习反馈，提高样本效率且不抑制探索。

Abstract: Reinforcement learning with verifiable rewards (RLVR) is effective for training large language models on deterministic outcome reasoning tasks. Prior work shows RLVR works with few prompts, but prompt selection is often based only on training-accuracy variance, leading to unstable optimization directions and weaker transfer. We revisit prompt selection from a mechanism-level view and argue that an effective minibatch should provide both (i) a reliable positive anchor and (ii) explicit negative learning signals from rare failures. Based on this principle, we propose \emph{positive--negative pairing}: at each update, we sample a hard-but-solvable $q^{+}$ and an easy-but-brittle prompt $q^{-}$(high success rate but not perfect), characterized by low and high empirical success rates under multiple rollouts. We further introduce Weighted GRPO, which reweights binary outcomes at the pair level and uses group-normalized advantages to amplify rare successes on $q^{+}$ into sharp positive guidance while turning rare failures on $q^{-}$ into strong negative penalties. This bidirectional signal provides informative learning feedback for both successes and failures, improving sample efficiency without suppressing exploration. On Qwen2.5-Math-7B, a single paired minibatch per update consistently outperforms a GRPO baseline that selects two prompts via commonly used variance-based selection heuristics: AIME~2025 Pass@8 improves from 16.8 to 22.2, and AMC23 Pass@64 from 94.0 to 97.0, while remaining competitive with large-scale RLVR trained from a pool of 1209 training prompts. Similar gains are observed on Qwen2.5-Math-7B-Instruct.

</details>


### [309] [Causal Inference on Networks under Misspecified Exposure Mappings: A Partial Identification Framework](https://arxiv.org/abs/2602.03459)
*Maresa Schröder,Miruna Oprescu,Stefan Feuerriegel,Nathan Kallus*

Main category: cs.LG

TL;DR: 本文提出网络因果推断的部分识别框架，推导治疗效果上下界，开发正交估计量，实验表明该框架在暴露映射错误设定下仍有效。


<details>
  <summary>Details</summary>
Motivation: 现有方法对网络治疗效果估计时，若暴露映射错误设定，标准估计量会有严重偏差，需评估其鲁棒性。

Method: 提出部分识别框架，推导上下界，针对三种典型暴露设定实例化框架，开发正交估计量。

Result: 开发的正交估计量得到的界估计是有效、尖锐和高效的，实验显示界在暴露映射错误设定下仍有信息价值。

Conclusion: 所提框架是因果敏感性分析在暴露映射上的新应用，能在暴露映射错误设定下给出可靠结论。

Abstract: Estimating treatment effects in networks is challenging, as each potential outcome depends on the treatments of all other nodes in the network. To overcome this difficulty, existing methods typically impose an exposure mapping that compresses the treatment assignments in the network into a low-dimensional summary. However, if this mapping is misspecified, standard estimators for direct and spillover effects can be severely biased. We propose a novel partial identification framework for causal inference on networks to assess the robustness of treatment effects under misspecifications of the exposure mapping. Specifically, we derive sharp upper and lower bounds on direct and spillover effects under such misspecifications. As such, our framework presents a novel application of causal sensitivity analysis to exposure mappings. We instantiate our framework for three canonical exposure settings widely used in practice: (i) weighted means of the neighborhood treatments, (ii) threshold-based exposure mappings, and (iii) truncated neighborhood interference in the presence of higher-order spillovers. Furthermore, we develop orthogonal estimators for these bounds and prove that the resulting bound estimates are valid, sharp, and efficient. Our experiments show the bounds remain informative and provide reliable conclusions under misspecification of exposure mappings.

</details>


### [310] [Scaling Continual Learning with Bi-Level Routing Mixture-of-Experts](https://arxiv.org/abs/2602.03473)
*Meng Lou,Yunxiang Fu,Yizhou Yu*

Main category: cs.LG

TL;DR: 提出CaRE这种可扩展持续学习器，用BR - MoE机制并引入评估方案，实验显示其在多数据集和任务设置中有领先表现。


<details>
  <summary>Details</summary>
Motivation: 在预训练模型基础上进行持续学习，特别是类增量学习时，有效学习特征表示并在长任务序列中保持稳定性和可塑性仍是待解决问题。

Method: 提出CaRE，采用双层路由机制的BR - MoE，同时引入评估协议用于评估CIL方法。

Result: CaRE在多种数据集和任务设置中表现领先，在100 - 300多个不重叠的长任务序列中超越所有基线。

Conclusion: CaRE是首个能扩展到很长任务序列的持续学习器，有良好性能，代码将公开。

Abstract: Continual learning, especially class-incremental learning (CIL), on the basis of a pre-trained model (PTM) has garnered substantial research interest in recent years. However, how to effectively learn both discriminative and comprehensive feature representations while maintaining stability and plasticity over very long task sequences remains an open problem. We propose CaRE, a scalable {C}ontinual Le{a}rner with efficient Bi-Level {R}outing Mixture-of-{E}xperts (BR-MoE). The core idea of BR-MoE is a bi-level routing mechanism: a router selection stage that dynamically activates relevant task-specific routers, followed by an expert routing phase that dynamically activates and aggregates experts, aiming to inject discriminative and comprehensive representations into every intermediate network layer. On the other hand, we introduce a challenging evaluation protocol for comprehensively assessing CIL methods across very long task sequences spanning hundreds of tasks. Extensive experiments show that CaRE demonstrates leading performance across a variety of datasets and task settings, including commonly used CIL datasets with classical CIL settings (e.g., 5-20 tasks). To the best of our knowledge, CaRE is the first continual learner that scales to very long task sequences (ranging from 100 to over 300 non-overlapping tasks), while outperforming all baselines by a large margin on such task sequences. Code will be publicly released at https://github.com/LMMMEng/CaRE.git.

</details>


### [311] [ScDiVa: Masked Discrete Diffusion for Joint Modeling of Single-Cell Identity and Expression](https://arxiv.org/abs/2602.03477)
*Mingxuan Wang,Cheng Chen,Gaoyang Jiang,Zijia Ren,Chuangxin Zhao,Lu Shi,Yanbiao Ma*

Main category: cs.LG

TL;DR: 提出蒙版离散扩散基础模型scDiVa应对单细胞RNA测序数据生成问题，预训练后在多基准测试中表现良好，证明该方法有效。


<details>
  <summary>Details</summary>
Motivation: 单细胞RNA测序数据高维、稀疏且无序，自回归生成存在人为排序偏差和误差累积问题。

Method: 提出scDiVa模型，定义连续时间前向蒙版机制，采用双向去噪器，通过深度不变时间采样和双重去噪目标进行训练。

Result: 在5900万个细胞上预训练后，scDiVa在多个基准测试中取得良好迁移性能。

Conclusion: 蒙版离散扩散是自回归方法的生物相干且有效的替代方案。

Abstract: Single-cell RNA-seq profiles are high-dimensional, sparse, and unordered, causing autoregressive generation to impose an artificial ordering bias and suffer from error accumulation. To address this, we propose scDiVa, a masked discrete diffusion foundation model that aligns generation with the dropout-like corruption process by defining a continuous-time forward masking mechanism in token space. ScDiVa features a bidirectional denoiser that jointly models discrete gene identities and continuous values, utilizing entropy-normalized serialization and a latent anchor token to maximize information efficiency and preserve global cell identity. The model is trained via depth-invariant time sampling and a dual denoising objective to simulate varying sparsity levels while ensuring precise recovery of both identity and magnitude. Pre-trained on 59 million cells, scDiVa achieves strong transfer performance across major benchmarks, including batch integration, cell type annotation, and perturbation response prediction. These results suggest that masked discrete diffusion serves as a biologically coherent and effective alternative to autoregression.

</details>


### [312] [DeepDFA: Injecting Temporal Logic in Deep Learning for Sequential Subsymbolic Applications](https://arxiv.org/abs/2602.03486)
*Elena Umili,Francesco Argenziano,Roberto Capobianco*

Main category: cs.LG

TL;DR: 提出DeepDFA框架，将高级时间逻辑集成到神经网络架构，在实验中表现优于传统模型，实现时间知识集成的SOTA。


<details>
  <summary>Details</summary>
Motivation: 解决将逻辑知识集成到深度神经网络训练的难题，尤其是顺序或时间扩展领域。

Method: 提出DeepDFA框架，将高级时间逻辑以DFA或Moore Machines表示，将时间规则建模为连续可微层。

Result: DeepDFA在静态图像序列分类和交互式非马尔可夫环境策略学习中表现良好，优于传统深度学习模型和新型神经符号系统。

Conclusion: DeepDFA有潜力在顺序任务中弥合亚符号学习和符号推理之间的差距。

Abstract: Integrating logical knowledge into deep neural network training is still a hard challenge, especially for sequential or temporally extended domains involving subsymbolic observations. To address this problem, we propose DeepDFA, a neurosymbolic framework that integrates high-level temporal logic - expressed as Deterministic Finite Automata (DFA) or Moore Machines - into neural architectures. DeepDFA models temporal rules as continuous, differentiable layers, enabling symbolic knowledge injection into subsymbolic domains. We demonstrate how DeepDFA can be used in two key settings: (i) static image sequence classification, and (ii) policy learning in interactive non-Markovian environments. Across extensive experiments, DeepDFA outperforms traditional deep learning models (e.g., LSTMs, GRUs, Transformers) and novel neuro-symbolic systems, achieving state-of-the-art results in temporal knowledge integration. These results highlight the potential of DeepDFA to bridge subsymbolic learning and symbolic reasoning in sequential tasks.

</details>


### [313] [A Minimal Task Reveals Emergent Path Integration and Object-Location Binding in a Predictive Sequence Model](https://arxiv.org/abs/2602.03490)
*Linda Ariel Ventura,Victoria Bosch,Tim C Kietzmann,Sushrut Thorat*

Main category: cs.LG

TL;DR: 研究在最小计算机环境下，动作条件顺序预测学习世界模型的可能性，发现结构化表征可支持预测。


<details>
  <summary>Details</summary>
Motivation: 预测性神经网络常被认为可形成‘世界模型’，但其潜在机制不明，探究动作条件顺序预测能否学习世界模型。

Method: 从2D连续令牌场景中顺序采样令牌，训练循环神经网络根据当前输入和类似扫视的位移预测下一个令牌。

Result: 在新场景中，预测准确性随序列提高，解码分析显示路径整合和令牌身份与位置的动态绑定，干预分析表明可在序列后期学习新绑定和分布外绑定。

Conclusion: 展示了依赖灵活绑定的结构化表征如何出现以支持预测，为认知科学中的顺序世界建模提供了机制解释。

Abstract: Adaptive cognition requires structured internal models representing objects and their relations. Predictive neural networks are often proposed to form such "world models", yet their underlying mechanisms remain unclear. One hypothesis is that action-conditioned sequential prediction suffices for learning such world models. In this work, we investigate this possibility in a minimal in-silico setting. Sequentially sampling tokens from 2D continuous token scenes, a recurrent neural network is trained to predict the upcoming token from current input and a saccade-like displacement. On novel scenes, prediction accuracy improves across the sequence, indicating in-context learning. Decoding analyses reveal path integration and dynamic binding of token identity to position. Interventional analyses show that new bindings can be learned late in sequence and that out-of-distribution bindings can be learned. Together, these results demonstrate how structured representations that rely on flexible binding emerge to support prediction, offering a mechanistic account of sequential world modeling relevant to cognitive science.

</details>


### [314] [Least but not Last: Fine-tuning Intermediate Principal Components for Better Performance-Forgetting Trade-Offs](https://arxiv.org/abs/2602.03493)
*Alessio Quercia,Arya Bangun,Ira Assent,Hanno Scharr*

Main category: cs.LG

TL;DR: 本文分析低秩适配中性能 - 遗忘权衡，提出用主成分初始化LoRA的实用方法，在多任务中提升准确率并减少遗忘。


<details>
  <summary>Details</summary>
Motivation: 现有低秩适配方法在平衡特定任务性能提升和预训练知识灾难性遗忘方面存在不足，且建议不一致。

Method: 使用主成分初始化进行低秩适配的性能 - 遗忘权衡综合分析，微调中间组件。

Result: 微调中间组件比现有方法的首尾组件能更好平衡且对高学习率更鲁棒，在多任务中提升准确率并减少遗忘。

Conclusion: 提出的LoRA初始化实用方法能提供更优权衡，在持续学习场景也有效。

Abstract: Low-Rank Adaptation (LoRA) methods have emerged as crucial techniques for adapting large pre-trained models to downstream tasks under computational and memory constraints. However, they face a fundamental challenge in balancing task-specific performance gains against catastrophic forgetting of pre-trained knowledge, where existing methods provide inconsistent recommendations. This paper presents a comprehensive analysis of the performance-forgetting trade-offs inherent in low-rank adaptation using principal components as initialization. Our investigation reveals that fine-tuning intermediate components leads to better balance and show more robustness to high learning rates than first (PiSSA) and last (MiLoRA) components in existing work. Building on these findings, we provide a practical approach for initialization of LoRA that offers superior trade-offs. We demonstrate in a thorough empirical study on a variety of computer vision and NLP tasks that our approach improves accuracy and reduces forgetting, also in continual learning scenarios.

</details>


### [315] [Lookahead Path Likelihood Optimization for Diffusion LLMs](https://arxiv.org/abs/2602.03496)
*Xuejie Liu,Yap Vit Chun,Yitao Liang,Anji Liu*

Main category: cs.LG

TL;DR: 提出Path LL和POKE，集成到POKE - SMC框架用于选择扩散大语言模型的解掩码路径，实验显示能提升推理准确率。


<details>
  <summary>Details</summary>
Motivation: 现有扩散大语言模型解掩码顺序策略依赖启发式方法，缺乏全局一致性和准确性指导。

Method: 引入Path LL目标，提出POKE值估计器，将其集成到POKE - SMC搜索框架。

Result: 在6个推理任务中，POKE - SMC持续提升准确率，在LLaDA模型上比强解码时间缩放基线平均提高2% - 3%。

Conclusion: POKE - SMC能在可比推理开销下提升准确率，推进了准确率 - 计算的帕累托前沿。

Abstract: Diffusion Large Language Models (dLLMs) support arbitrary-order generation, yet their inference performance critically depends on the unmasking order. Existing strategies rely on heuristics that greedily optimize local confidence, offering limited guidance for identifying unmasking paths that are globally consistent and accurate. To bridge this gap, we introduce path log-likelihood (Path LL), a trajectory-conditioned objective that strongly correlates with downstream accuracy and enables principled selection of unmasking paths. To optimize Path LL at inference time, we propose POKE, an efficient value estimator that predicts the expected future Path LL of a partial decoding trajectory. We then integrate this lookahead signal into POKE-SMC, a Sequential Monte Carlo-based search framework for dynamically identifying optimal unmasking paths. Extensive experiments across 6 reasoning tasks show that POKE-SMC consistently improves accuracy, achieving 2%--3% average gains over strong decoding-time scaling baselines at comparable inference overhead on LLaDA models and advancing the accuracy--compute Pareto frontier.

</details>


### [316] [Reparameterization Flow Policy Optimization](https://arxiv.org/abs/2602.03501)
*Hai Zhong,Zhuoran Li,Xun Wang,Longbo Huang*

Main category: cs.LG

TL;DR: 本文指出流策略与重参数化策略梯度（RPG）框架自然契合，但直接结合效果不佳，提出重参数化流策略优化（RFO）方法，含正则化项和动作分块变体，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 先前RPG方法多限于高斯策略，限制性能且未利用生成模型进展，直接结合流策略与RPG存在训练不稳定和缺乏探索问题。

Method: 提出RFO，通过在流生成过程和系统动力学中联合反向传播计算策略梯度，包含两个正则化项，还提出动作分块变体。

Result: 在多种运动和操作任务中进行广泛实验，证明了RFO的有效性，在控制软体四足动物的挑战性运动任务中，RFO的奖励几乎是现有最优基线的2倍。

Conclusion: RFO是一种有效的方法，能在不进行难处理的对数似然计算的情况下实现高样本效率。

Abstract: Reparameterization Policy Gradient (RPG) has emerged as a powerful paradigm for model-based reinforcement learning, enabling high sample efficiency by backpropagating gradients through differentiable dynamics. However, prior RPG approaches have been predominantly restricted to Gaussian policies, limiting their performance and failing to leverage recent advances in generative models. In this work, we identify that flow policies, which generate actions via differentiable ODE integration, naturally align with the RPG framework, a connection not established in prior work. However, naively exploiting this synergy proves ineffective, often suffering from training instability and a lack of exploration. We propose Reparameterization Flow Policy Optimization (RFO). RFO computes policy gradients by backpropagating jointly through the flow generation process and system dynamics, unlocking high sample efficiency without requiring intractable log-likelihood calculations. RFO includes two tailored regularization terms for stability and exploration. We also propose a variant of RFO with action chunking. Extensive experiments on diverse locomotion and manipulation tasks, involving both rigid and soft bodies with state or visual inputs, demonstrate the effectiveness of RFO. Notably, on a challenging locomotion task controlling a soft-body quadruped, RFO achieves almost $2\times$ the reward of the state-of-the-art baseline.

</details>


### [317] [Explaining the Explainer: Understanding the Inner Workings of Transformer-based Symbolic Regression Models](https://arxiv.org/abs/2602.03506)
*Arco van Breda,Erman Acar*

Main category: cs.LG

TL;DR: 本文引入进化电路发现算法PATCHES对符号回归（SR）变压器进行电路级表征，验证结果，指出不同方法效用，确立SR为机制可解释性高潜力应用领域。


<details>
  <summary>Details</summary>
Motivation: 变压器用于符号回归有效，但生成数学运算符的内在机制未充分探索，且机制可解释性未应用于SR。

Method: 引入进化电路发现算法PATCHES，使用基于关键概念的因果评估框架验证结果。

Result: 分离出28个电路，均值修补与基于性能的评估最能可靠分离功能正确的电路，直接对数归因和探测分类器主要捕捉相关特征。

Conclusion: SR是机制可解释性的高潜力应用领域，提出了电路发现的原则性方法。

Abstract: Following their success across many domains, transformers have also proven effective for symbolic regression (SR); however, the internal mechanisms underlying their generation of mathematical operators remain largely unexplored. Although mechanistic interpretability has successfully identified circuits in language and vision models, it has not yet been applied to SR. In this article, we introduce PATCHES, an evolutionary circuit discovery algorithm that identifies compact and correct circuits for SR. Using PATCHES, we isolate 28 circuits, providing the first circuit-level characterisation of an SR transformer. We validate these findings through a robust causal evaluation framework based on key notions such as faithfulness, completeness, and minimality. Our analysis shows that mean patching with performance-based evaluation most reliably isolates functionally correct circuits. In contrast, we demonstrate that direct logit attribution and probing classifiers primarily capture correlational features rather than causal ones, limiting their utility for circuit discovery. Overall, these results establish SR as a high-potential application domain for mechanistic interpretability and propose a principled methodology for circuit discovery.

</details>


### [318] [Not All Negative Samples Are Equal: LLMs Learn Better from Plausible Reasoning](https://arxiv.org/abs/2602.03516)
*Zixiang Di,Jinyi Han,Shuo Zhang,Ying Liao,Zhi Li,Xiaofeng Ji,Yongqi Wang,Zheming Yang,Ming Gao,Bingdong Li,Jie Wang*

Main category: cs.LG

TL;DR: 提出PNS方法合成高质量负样本提升大语言模型推理能力，经实验验证效果优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 现有从负样本学习的方法忽视样本质量，未能有效提升大语言模型推理能力。

Method: 提出PNS方法，通过反向强化学习，结合多种评估指标训练模型生成高质量负样本，还将其作为即插即用数据源进行偏好优化。

Result: PNS在七个数学推理基准测试中，在三个骨干模型上始终优于其他负样本合成方法，比强化学习训练的模型平均提高2.03%。

Conclusion: PNS方法能有效提升大语言模型的推理能力。

Abstract: Learning from negative samples holds great promise for improving Large Language Model (LLM) reasoning capability, yet existing methods treat all incorrect responses as equally informative, overlooking the crucial role of sample quality. To address this, we propose Plausible Negative Samples (PNS), a method that synthesizes high-quality negative samples exhibiting expected format and structural coherence while ultimately yielding incorrect answers. PNS trains a dedicated model via reverse reinforcement learning (RL) guided by a composite reward combining format compliance, accuracy inversion, reward model assessment, and chain-of-thought evaluation, generating responses nearly indistinguishable from correct solutions. We further validate PNS as a plug-and-play data source for preference optimization across three backbone models on seven mathematical reasoning benchmarks. Results demonstrate that PNS consistently outperforms other negative sample synthesis methods, achieving an average improvement of 2.03% over RL-trained models.

</details>


### [319] [Rank-Learner: Orthogonal Ranking of Treatment Effects](https://arxiv.org/abs/2602.03517)
*Henri Arno,Dennis Frauen,Emil Javurek,Thomas Demeester,Stefan Feuerriegel*

Main category: cs.LG

TL;DR: 提出Rank - Learner直接从观测数据学习处理效应排名，有理论保证且实验效果好。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注因果效应估计，处理效应排名问题未充分探索，而很多决策问题需按处理效应排名。

Method: 引入两阶段Rank - Learner，优化成对学习目标，不进行显式CATE估计，且该方法具有Neyman - 正交性、模型无关性。

Result: 通过大量实验，Rank - Learner始终优于标准CATE估计方法和非正交排名方法。

Conclusion: 为从业者提供了新的正交两阶段学习者用于按处理效应给个体排名。

Abstract: Many decision-making problems require ranking individuals by their treatment effects rather than estimating the exact effect magnitudes. Examples include prioritizing patients for preventive care interventions, or ranking customers by the expected incremental impact of an advertisement. Surprisingly, while causal effect estimation has received substantial attention in the literature, the problem of directly learning rankings of treatment effects has largely remained unexplored. In this paper, we introduce Rank-Learner, a novel two-stage learner that directly learns the ranking of treatment effects from observational data. We first show that naive approaches based on precise treatment effect estimation solve a harder problem than necessary for ranking, while our Rank-Learner optimizes a pairwise learning objective that recovers the true treatment effect ordering, without explicit CATE estimation. We further show that our Rank-Learner is Neyman-orthogonal and thus comes with strong theoretical guarantees, including robustness to estimation errors in the nuisance functions. In addition, our Rank-Learner is model-agnostic, and can be instantiated with arbitrary machine learning models (e.g., neural networks). We demonstrate the effectiveness of our method through extensive experiments where Rank-Learner consistently outperforms standard CATE estimators and non-orthogonal ranking methods. Overall, we provide practitioners with a new, orthogonal two-stage learner for ranking individuals by their treatment effects.

</details>


### [320] [Live or Lie: Action-Aware Capsule Multiple Instance Learning for Risk Assessment in Live Streaming Platforms](https://arxiv.org/abs/2602.03520)
*Yiran Qiao,Jing Chen,Xiang Ao,Qiwei Zhong,Yang Liu,Qing He*

Main category: cs.LG

TL;DR: 本文针对直播房间风险评估展开研究，提出AC - MIL框架，在大规模工业数据集上表现出色，且有可解释性。


<details>
  <summary>Details</summary>
Motivation: 直播面临多个参与者间稀疏、协调的恶意行为带来的严重风险，且难以被及时准确检测，当前仅有房间级标签的弱监督情况。

Method: 将任务表述为多实例学习（MIL）问题，提出AC - MIL框架，通过串行和并行架构建模个体行为和群体协调模式。

Result: 在抖音的大规模工业数据集上，AC - MIL显著优于MIL和顺序基线，达到房间级风险评估的新水平。

Conclusion: AC - MIL在直播房间级风险评估中表现出色，且能提供胶囊级可解释性，可识别风险行为片段作为干预证据。

Abstract: Live streaming has become a cornerstone of today's internet, enabling massive real-time social interactions. However, it faces severe risks arising from sparse, coordinated malicious behaviors among multiple participants, which are often concealed within normal activities and challenging to detect timely and accurately. In this work, we provide a pioneering study on risk assessment in live streaming rooms, characterized by weak supervision where only room-level labels are available. We formulate the task as a Multiple Instance Learning (MIL) problem, treating each room as a bag and defining structured user-timeslot capsules as instances. These capsules represent subsequences of user actions within specific time windows, encapsulating localized behavioral patterns. Based on this formulation, we propose AC-MIL, an Action-aware Capsule MIL framework that models both individual behaviors and group-level coordination patterns. AC-MIL captures multi-granular semantics and behavioral cues through a serial and parallel architecture that jointly encodes temporal dynamics and cross-user dependencies. These signals are integrated for robust room-level risk prediction, while also offering interpretable evidence at the behavior segment level. Extensive experiments on large-scale industrial datasets from Douyin demonstrate that AC-MIL significantly outperforms MIL and sequential baselines, establishing new state-of-the-art performance in room-level risk assessment for live streaming. Moreover, AC-MIL provides capsule-level interpretability, enabling identification of risky behavior segments as actionable evidence for intervention. The project page is available at: https://qiaoyran.github.io/AC-MIL/.

</details>


### [321] [WARP Logic Neural Networks](https://arxiv.org/abs/2602.03527)
*Lino Gerlach,Thore Gerlach,Liv Våge,Elliott Kauffman,Isobel Ojalvo*

Main category: cs.LG

TL;DR: 提出WARP逻辑神经网络框架，解决现有逻辑神经网络问题，实验显示效果好。


<details>
  <summary>Details</summary>
Motivation: 现有逻辑神经网络训练成本高、有冗余、依赖近似梯度，限制可扩展性。

Method: 引入WARP逻辑神经网络框架，通过引入可学习阈值和残差初始化改进训练，用随机平滑弥合训练与推理差距。

Result: WARP是学习布尔函数最参数高效的表示，实验比现有基线收敛快，能有效扩展到更深架构和高输入元数逻辑函数。

Conclusion: WARP框架有效解决现有逻辑神经网络的局限，在训练和推理上有良好表现。

Abstract: Fast and efficient AI inference is increasingly important, and recent models that directly learn low-level logic operations have achieved state-of-the-art performance. However, existing logic neural networks incur high training costs, introduce redundancy or rely on approximate gradients, which limits scalability. To overcome these limitations, we introduce WAlsh Relaxation for Probabilistic (WARP) logic neural networks -- a novel gradient-based framework that efficiently learns combinations of hardware-native logic blocks. We show that WARP yields the most parameter-efficient representation for exactly learning Boolean functions and that several prior approaches arise as restricted special cases. Training is improved by introducing learnable thresholding and residual initialization, while we bridge the gap between relaxed training and discrete logic inference through stochastic smoothing. Experiments demonstrate faster convergence than state-of-the-art baselines, while scaling effectively to deeper architectures and logic functions with higher input arity.

</details>


### [322] [Robust Representation Learning in Masked Autoencoders](https://arxiv.org/abs/2602.03531)
*Anika Shrivastava,Renu Rameshan,Samar Agnihotri*

Main category: cs.LG

TL;DR: 本文旨在理解MAE在下游分类任务中的强性能，发现其表示具有鲁棒性，通过层分析揭示其构建潜在空间方式，引入指标量化特征鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 理解Masked Autoencoders (MAEs)在下游分类任务中表现出色的原因。

Method: 对token嵌入进行逐层分析，引入两个敏感性指标（干净和扰动嵌入之间的方向对齐、降级下活动特征的头保留）来量化特征鲁棒性。

Result: 发现经过预训练和微调学习的表示具有鲁棒性，在模糊和遮挡等降级情况下仍有良好分类性能；预训练的MAE以类别感知的方式逐步构建潜在空间；MAE在编码器层表现出早期和持久的全局注意力。

Conclusion: 这些研究有助于证实MAEs的鲁棒分类性能。

Abstract: Masked Autoencoders (MAEs) achieve impressive performance in image classification tasks, yet the internal representations they learn remain less understood. This work started as an attempt to understand the strong downstream classification performance of MAE. In this process we discover that representations learned with the pretraining and fine-tuning, are quite robust - demonstrating a good classification performance in the presence of degradations, such as blur and occlusions. Through layer-wise analysis of token embeddings, we show that pretrained MAE progressively constructs its latent space in a class-aware manner across network depth: embeddings from different classes lie in subspaces that become increasingly separable. We further observe that MAE exhibits early and persistent global attention across encoder layers, in contrast to standard Vision Transformers (ViTs). To quantify feature robustness, we introduce two sensitivity indicators: directional alignment between clean and perturbed embeddings, and head-wise retention of active features under degradations. These studies help establish the robust classification performance of MAEs.

</details>


### [323] [Sparse Training of Neural Networks based on Multilevel Mirror Descent](https://arxiv.org/abs/2602.03535)
*Yannick Lunk,Sebastian J. Scott,Leon Bungert*

Main category: cs.LG

TL;DR: 提出基于线性化Bregman迭代/镜像下降的动态稀疏训练算法，有收敛保证，能生成高稀疏准确模型，减少FLOPs。


<details>
  <summary>Details</summary>
Motivation: 有效探索稀疏参数空间并维持稀疏性。

Method: 结合诱导稀疏的Bregman迭代与网络结构自适应冻结，嵌入多级优化框架。

Result: 在标准基准测试中生成高稀疏准确模型，对比SGD训练，理论FLOPs从38%降至6%且维持测试准确率。

Conclusion: 该动态稀疏训练算法有效且高效。

Abstract: We introduce a dynamic sparse training algorithm based on linearized Bregman iterations / mirror descent that exploits the naturally incurred sparsity by alternating between periods of static and dynamic sparsity pattern updates. The key idea is to combine sparsity-inducing Bregman iterations with adaptive freezing of the network structure to enable efficient exploration of the sparse parameter space while maintaining sparsity. We provide convergence guaranties by embedding our method in a multilevel optimization framework. Furthermore, we empirically show that our algorithm can produce highly sparse and accurate models on standard benchmarks. We also show that the theoretical number of FLOPs compared to SGD training can be reduced from 38% for standard Bregman iterations to 6% for our method while maintaining test accuracy.

</details>


### [324] [MatGPTQ: Accurate and Efficient Post-Training Matryoshka Quantization](https://arxiv.org/abs/2602.03537)
*Maximilian Kleinegger,Elvir Crnčević,Dan Alistarh*

Main category: cs.LG

TL;DR: 提出Post - Training Matryoshka Quantization (MatGPTQ)，解决现有Matryoshka量化方法的局限，实现单检查点多精度部署。


<details>
  <summary>Details</summary>
Motivation: 现有Matryoshka量化方法依赖昂贵的量化感知训练，缺乏快速的一次性训练后量化方法和开源及内核支持。

Method: 将Matryoshka量化作为多精度目标，结合位切片和跨位误差补偿，进行一次性优化；引入预算感知的层间位宽搜索，提供高效内核。

Result: 在标准大语言模型和基准测试中，MatGPTQ在低比特位宽设置下显著提高性能，同时保留高比特精度。

Conclusion: MatGPTQ为Matryoshka风格的训练后量化建立了新的技术水平，使单检查点、多精度部署更开放和实用。

Abstract: Matryoshka Quantization (MatQuant) is a recent quantization approach showing that a single integer-quantized model can be served across multiple precisions, by slicing the most significant bits (MSB) at inference time. This enables a single checkpoint to cover a wide range of memory and latency budgets, but renders quantization much more challenging. In particular, the initial MatQuant relies on expensive quantization-aware training (QAT) variants, rather than fast one-shot post training quantization (PTQ), and lacks open-source and kernel support. We address all of these limitations by introducing Post-Training Matryoshka Quantization (MatGPTQ), a new PTQ pipeline that produces a single parent model jointly optimized for multiple target precisions in one-shot, based on a small calibration set. MatGPTQ casts Matryoshka quantization as a multi-precision objective with bit-slicing and cross-bit error compensation, resulting in an algorithm that produces a multi-bit-width, "sliceable" model in a single pass. We also incorporate a new budget-aware search for heterogeneous per-layer bit-witdhs and provide efficient kernels that implement slicing and mixed-precision execution. Across standard LLMs and benchmarks, MatGPTQ preserves high-bit accuracy while substantially improving performance at low-bit-witdh settings. Overall, we establish a new state of the art for Matryoshka-style post-training quantization and make single-checkpoint, multi-precision deployment open and practical. Code is available at https://github.com/IST-DASLab/MatGPTQ.

</details>


### [325] [How to Train Your Resistive Network: Generalized Equilibrium Propagation and Analytical Learning](https://arxiv.org/abs/2602.03546)
*Jonathan Lin,Aman Desai,Frank Barrows,Francesco Caravelli*

Main category: cs.LG

TL;DR: 本文针对模拟计算系统训练难题，提出用图论和分析框架精确计算梯度的算法，引入广义平衡传播框架并对比，模拟表明可在电阻网络训练中减少计算，还能部分更新电阻值。


<details>
  <summary>Details</summary>
Motivation: 当前数字硬件计算机器学习能耗高，模拟计算虽节能但训练受物理本地约束限制，需解决训练问题。

Method: 使用图论和分析框架精确计算梯度；引入广义平衡传播框架并对比方法；数值模拟验证算法。

Result: 可在电阻网络训练中仅在输出层操作，无需对所有电阻复制或读出；部分更新电阻值时性能无严重下降。

Conclusion: 提出的算法有效解决模拟计算系统训练的本地约束问题，能高效训练电阻网络，且部分更新操作可行。

Abstract: Machine learning is a powerful method of extracting meaning from data; unfortunately, current digital hardware is extremely energy-intensive. There is interest in an alternative analog computing implementation that could match the performance of traditional machine learning while being significantly more energy-efficient. However, it remains unclear how to train such analog computing systems while adhering to locality constraints imposed by the physical (as opposed to digital) nature of these systems. Local learning algorithms such as Equilibrium Propagation and Coupled Learning have been proposed to address this issue. In this paper, we develop an algorithm to exactly calculate gradients using a graph theoretic and analytical framework for Kirchhoff's laws. We also introduce Generalized Equilibrium Propagation, a framework encompassing a broad class of Hebbian learning algorithms, including Coupled Learning and Equilibrium Propagation, and show how our algorithm compares. We demonstrate our algorithm using numerical simulations and show that we can train resistor networks without the need for a replica or readout over all resistors, only at the output layer. We also show that under the analytical gradient approach, it is possible to update only a subset of the resistance values without a strong degradation in performance.

</details>


### [326] [NPCNet: Navigator-Driven Pseudo Text for Deep Clustering of Early Sepsis Phenotyping](https://arxiv.org/abs/2602.03562)
*Pi-Ju Tsai,Charkkri Limbud,Kuan-Fu Chen,Yi-Ju Tseng*

Main category: cs.LG

TL;DR: 提出NPCNet网络整合电子病历识别脓毒症表型，增强精准治疗策略。


<details>
  <summary>Details</summary>
Motivation: 现有脓毒症聚类过程缺乏临床相关性，难以反映临床不同表型，需更好方法识别表型以实现精准治疗。

Method: 提出带目标导航器的新型深度聚类网络NPCNet，整合时间序列电子病历数据。

Result: 识别出四种脓毒症表型，能区分早期病情相似但后续发展不同的患者，发现部分表型可能从早期血管升压药治疗中获益。

Conclusion: NPCNet通过揭示临床不同表型，增强了精准治疗策略。

Abstract: Sepsis is a heterogeneous syndrome. Identifying clinically distinct phenotypes may enable more precise treatment strategies. In recent years, many researchers have applied clustering algorithms to sepsis patients. However, the clustering process rarely incorporates clinical relevance, potentially limiting to reflect clinically distinct phenotypes. We propose NPCNet, a novel deep clustering network with a target navigator that integrates temporal Electronic Health Records (EHRs) to better align sepsis phenotypes with clinical significance. We identify four sepsis phenotypes ($α$, $β$, $γ$, and $δ$) with divergence in SOFA trajectories. Notably, while $α$ and $δ$ phenotypes both show severe conditions in the early stage, NPCNet effectively differentiates patients who are likely to improve ($α$) from those at risk of deterioration ($δ$). Furthermore, through the treatment effect analysis, we discover that $α$, $β$, and $δ$ phenotypes may benefit from early vasopressor administration. The results show that NPCNet enhances precision treatment strategies by uncovering clinically distinct phenotypes.

</details>


### [327] [CoGenCast: A Coupled Autoregressive-Flow Generative Framework for Time Series Forecasting](https://arxiv.org/abs/2602.03564)
*Yaguo Liu,Mingyue Cheng,Daoyu Wang,Xiaoyu Tao,Qi Liu*

Main category: cs.LG

TL;DR: 提出CoGenCast混合生成框架用于时间序列预测，结合预训练大语言模型和流匹配机制，支持多模态预测和跨域统一训练，实验表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法同时对时间序列预测中的语义上下文和连续概率生成进行有效建模。

Method: 提出CoGenCast框架，修改预训练仅解码器大语言模型的注意力拓扑，构建预测编码器 - 解码器骨干，再集成流匹配机制来建模时间演变。

Result: 在多个基准测试上，CoGenCast始终优于之前的对比基线。

Conclusion: CoGenCast是一种有效的时间序列预测方法，自然支持多模态预测和跨域统一训练。

Abstract: Time series forecasting can be viewed as a generative problem that requires both semantic understanding over contextual conditions and stochastic modeling of continuous temporal dynamics. Existing approaches typically rely on either autoregressive large language models (LLMs) for semantic context modeling or diffusion-like models for continuous probabilistic generation. However, neither method alone can adequately model both aspects simultaneously. In this work, we propose CoGenCast, a hybrid generative framework that couples pre-trained LLMs with flow-matching mechanism for effective time series forecasting. Specifically, we reconfigure pre-trained decoder-only LLMs into a native forecasting encoder-decoder backbone by modifying only the attention topology, enabling bidirectional context encoding and causal representation generation. Building on this, a flow-matching mechanism is further integrated to model temporal evolution, capturing continuous stochastic dynamics conditioned on the autoregressively generated representation. Notably, CoGenCast naturally supports multimodal forecasting and cross-domain unified training. Extensive experiments on multiple benchmarks show that CoGenCast consistently outperforms previous compared baselines. Code is available at https://github.com/liuyaguo/_CoGenCast.

</details>


### [328] [EVE: Efficient Verification of Data Erasure through Customized Perturbation in Approximate Unlearning](https://arxiv.org/abs/2602.03567)
*Weiqi Wang,Zhiyi Tian,Chenhan Zhang,Luoyu Chen,Shui Yu*

Main category: cs.LG

TL;DR: 提出高效擦除验证方法EVE，无需参与模型初始训练，实验表明其优于现有方法并开源代码。


<details>
  <summary>Details</summary>
Motivation: 现有基于后门技术的机器学习遗忘验证方法需参与模型初始训练，低效且不实际，需新方法。

Method: 扰动遗忘数据，使指定样本在遗忘前后模型预测改变，将扰动生成形式化为对抗优化问题求解。

Result: EVE无需参与模型初始训练即可验证机器学习遗忘，显著优于现有方法，提高效率和验证准确性。

Conclusion: EVE是一种高效的机器学习遗忘验证方法，为该领域提供新工具。

Abstract: Verifying whether the machine unlearning process has been properly executed is critical but remains underexplored. Some existing approaches propose unlearning verification methods based on backdooring techniques. However, these methods typically require participation in the model's initial training phase to backdoor the model for later verification, which is inefficient and impractical. In this paper, we propose an efficient verification of erasure method (EVE) for verifying machine unlearning without requiring involvement in the model's initial training process. The core idea is to perturb the unlearning data to ensure the model prediction of the specified samples will change before and after unlearning with perturbed data. The unlearning users can leverage the observation of the changes as a verification signal. Specifically, the perturbations are designed with two key objectives: ensuring the unlearning effect and altering the unlearned model's prediction of target samples. We formalize the perturbation generation as an adversarial optimization problem, solving it by aligning the unlearning gradient with the gradient of boundary change for target samples. We conducted extensive experiments, and the results show that EVE can verify machine unlearning without involving the model's initial training process, unlike backdoor-based methods. Moreover, EVE significantly outperforms state-of-the-art unlearning verification methods, offering significant speedup in efficiency while enhancing verification accuracy. The source code of EVE is released at \uline{https://anonymous.4open.science/r/EVE-C143}, providing a novel tool for verification of machine unlearning.

</details>


### [329] [Asymmetric Hierarchical Anchoring for Audio-Visual Joint Representation: Resolving Information Allocation Ambiguity for Robust Cross-Modal Generalization](https://arxiv.org/abs/2602.03570)
*Bixing Wu,Yuhong Zhao,Zongli Ye,Jiachen Lian,Xiangyu Yue,Gopala Anumanchipalli*

Main category: cs.LG

TL;DR: 提出非对称分层锚定（AHA）方法用于跨模态泛化下的视听联合表征学习，实验显示其性能优于对称基线。


<details>
  <summary>Details</summary>
Motivation: 现有对称框架存在信息分配模糊、跨模态语义泄漏问题。

Method: 提出AHA方法，利用音频残差向量量化诱导的分层离散表示引导视频特征蒸馏，用基于GRL的对抗解耦器抑制语义泄漏，引入局部滑动对齐促进细粒度时间对齐。

Result: 在AVE和AVVP基准测试上，AHA始终优于对称基线；说话人脸解纠缠实验表明学习的表征语义一致性和可分离性提升。

Conclusion: AHA方法在跨模态转移中有效，具有更广泛的适用性。

Abstract: Audio-visual joint representation learning under Cross-Modal Generalization (CMG) aims to transfer knowledge from a labeled source modality to an unlabeled target modality through a unified discrete representation space. Existing symmetric frameworks often suffer from information allocation ambiguity, where the absence of structural inductive bias leads to semantic-specific leakage across modalities. We propose Asymmetric Hierarchical Anchoring (AHA), which enforces directional information allocation by designating a structured semantic anchor within a shared hierarchy. In our instantiation, we exploit the hierarchical discrete representations induced by audio Residual Vector Quantization (RVQ) to guide video feature distillation into a shared semantic space. To ensure representational purity, we replace fragile mutual information estimators with a GRL-based adversarial decoupler that explicitly suppresses semantic leakage in modality-specific branches, and introduce Local Sliding Alignment (LSA) to encourage fine-grained temporal alignment across modalities. Extensive experiments on AVE and AVVP benchmarks demonstrate that AHA consistently outperforms symmetric baselines in cross-modal transfer. Additional analyses on talking-face disentanglement experiment further validate that the learned representations exhibit improved semantic consistency and disentanglement, indicating the broader applicability of the proposed framework.

</details>


### [330] [Optimization and Generation in Aerodynamics Inverse Design](https://arxiv.org/abs/2602.03582)
*Huaguan Chen,Ning Lin,Luxi Chen,Rui Zhang,Wenbing Huang,Chongxuan Li,Hao Sun*

Main category: cs.LG

TL;DR: 本文提出逆设计新方法，包括成本预测器训练损失和密度梯度优化方法，统一现有无训练引导生成方法，还开发近似协方差估计算法，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 基于物理目标的逆设计因高维几何与昂贵模拟耦合而具有挑战性，如减阻的空气动力学形状优化。

Method: 提出成本预测器的新训练损失和密度梯度优化方法，统一现有无训练引导生成方法，开发近似协方差估计算法。

Result: 在二维研究和三维空气动力学基准测试中，优化和引导生成均有一致提升，离线强化学习结果也支持方法的通用性。

Conclusion: 所提方法在逆设计的优化和引导生成中有效且具有通用性。

Abstract: Inverse design with physics-based objectives is challenging because it couples high-dimensional geometry with expensive simulations, as exemplified by aerodynamic shape optimization for drag reduction. We revisit inverse design through two canonical solutions, the optimal design point and the optimal design distribution, and relate them to optimization and guided generation. Building on this view, we propose a new training loss for cost predictors and a density-gradient optimization method that improves objectives while preserving plausible shapes. We further unify existing training-free guided generation methods. To address their inability to approximate conditional covariance in high dimensions, we develop a time- and memory-efficient algorithm for approximate covariance estimation. Experiments on a controlled 2D study and high-fidelity 3D aerodynamic benchmarks (car and aircraft), validated by OpenFOAM simulations and miniature wind-tunnel tests with 3D-printed prototypes, demonstrate consistent gains in both optimization and guided generation. Additional offline RL results further support the generality of our approach.

</details>


### [331] [APEX: Probing Neural Networks via Activation Perturbation](https://arxiv.org/abs/2602.03586)
*Tao Ren,Xiaoyu Luo,Qiongxiu Li*

Main category: cs.LG

TL;DR: 提出APEX推理时探测范式，能在固定输入和参数下扰动隐藏激活，展示其理论和实际优势。


<details>
  <summary>Details</summary>
Motivation: 现有探测神经网络方法在获取中间表征结构信息上有局限。

Method: 引入APEX范式，通过扰动隐藏激活进行探测。

Result: 小噪声下可衡量样本规律，区分模型；大噪声下暴露模型偏差。

Conclusion: APEX为探索和理解神经网络提供有效视角。

Abstract: Prior work on probing neural networks primarily relies on input-space analysis or parameter perturbation, both of which face fundamental limitations in accessing structural information encoded in intermediate representations. We introduce Activation Perturbation for EXploration (APEX), an inference-time probing paradigm that perturbs hidden activations while keeping both inputs and model parameters fixed. We theoretically show that activation perturbation induces a principled transition from sample-dependent to model-dependent behavior by suppressing input-specific signals and amplifying representation-level structure, and further establish that input perturbation corresponds to a constrained special case of this framework. Through representative case studies, we demonstrate the practical advantages of APEX. In the small-noise regime, APEX provides a lightweight and efficient measure of sample regularity that aligns with established metrics, while also distinguishing structured from randomly labeled models and revealing semantically coherent prediction transitions. In the large-noise regime, APEX exposes training-induced model-level biases, including a pronounced concentration of predictions on the target class in backdoored models. Overall, our results show that APEX offers an effective perspective for exploring, and understanding neural networks beyond what is accessible from input space alone.

</details>


### [332] [SAGE-5GC: Security-Aware Guidelines for Evaluating Anomaly Detection in the 5G Core Network](https://arxiv.org/abs/2602.03596)
*Cristian Manca,Christian Scano,Giorgio Piras,Fabio Brau,Maura Pintor,Battista Biggio*

Main category: cs.LG

TL;DR: 研究5G网络野外环境攻击检测，提出SAGE - 5GC评估准则，实验表明对抗攻击会降低检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于机器学习的5G核心网异常检测方法评估假设在实际环境中难成立，需考虑实际部署场景。

Method: 提出SAGE - 5GC评估准则，用真实数据集训练检测器，评估其在标准攻击和对抗场景下的性能，通过随机扰动分析模型敏感性和鲁棒性，引入基于遗传算法的优化策略。

Result: 对抗性攻击会大幅降低检测性能。

Conclusion: 在野外部署的5G网络异常检测需要鲁棒、安全感知的评估方法。

Abstract: Machine learning-based anomaly detection systems are increasingly being adopted in 5G Core networks to monitor complex, high-volume traffic. However, most existing approaches are evaluated under strong assumptions that rarely hold in operational environments, notably the availability of independent and identically distributed (IID) data and the absence of adaptive attackers.In this work, we study the problem of detecting 5G attacks \textit{in the wild}, focusing on realistic deployment settings. We propose a set of Security-Aware Guidelines for Evaluating anomaly detectors in 5G Core Network (SAGE-5GC), driven by domain knowledge and consideration of potential adversarial threats. Using a realistic 5G Core dataset, we first train several anomaly detectors and assess their baseline performance against standard 5GC control-plane cyberattacks targeting PFCP-based network services.We then extend the evaluation to adversarial settings, where an attacker tries to manipulate the observable features of the network traffic to evade detection, under the constraint that the intended functionality of the malicious traffic is preserved. Starting from a selected set of controllable features, we analyze model sensitivity and adversarial robustness through randomized perturbations. Finally, we introduce a practical optimization strategy based on genetic algorithms that operates exclusively on attacker-controllable features and does not require prior knowledge of the underlying detection model. Our experimental results show that adversarially crafted attacks can substantially degrade detection performance, underscoring the need for robust, security-aware evaluation methodologies for anomaly detection in 5G networks deployed in the wild.

</details>


### [333] [Explanations Leak: Membership Inference with Differential Privacy and Active Learning Defense](https://arxiv.org/abs/2602.03611)
*Fatima Ezzeddine,Osama Zammar,Silvia Giordano,Omran Ayoub*

Main category: cs.LG

TL;DR: 研究反事实解释（CFs）如何扩大机器学习即服务（MLaaS）攻击面，提出防御框架并评估隐私、性能和解释质量的权衡。


<details>
  <summary>Details</summary>
Motivation: 当前ML模型部署易受隐私攻击，而CFs对攻击面的影响尚不清楚，需设计防御机制平衡风险与效用、可解释性。

Method: 系统分析CFs对基于影子的成员推理攻击（MIAs）的影响；提出结合差分隐私（DP）和主动学习（AL）的防御框架；进行广泛的实证评估。

Result: 完成对CFs扩大攻击面的分析，提出防御框架，评估了隐私泄漏、预测性能和解释质量间的三方权衡。

Conclusion: 在负责任地部署可解释MLaaS系统时，需仔细平衡透明度、效用和隐私。

Abstract: Counterfactual explanations (CFs) are increasingly integrated into Machine Learning as a Service (MLaaS) systems to improve transparency; however, ML models deployed via APIs are already vulnerable to privacy attacks such as membership inference and model extraction, and the impact of explanations on this threat landscape remains insufficiently understood. In this work, we focus on the problem of how CFs expand the attack surface of MLaaS by strengthening membership inference attacks (MIAs), and on the need to design defense mechanisms that mitigate this emerging risk without undermining utility and explainability. First, we systematically analyze how exposing CFs through query-based APIs enables more effective shadow-based MIAs. Second, we propose a defense framework that integrates Differential Privacy (DP) with Active Learning (AL) to jointly reduce memorization and limit effective training data exposure. Finally, we conduct an extensive empirical evaluation to characterize the three-way trade-off between privacy leakage, predictive performance, and explanation quality. Our findings highlight the need to carefully balance transparency, utility, and privacy in the responsible deployment of explainable MLaaS systems.

</details>


### [334] [Quantization-Aware Regularizers for Deep Neural Networks Compression](https://arxiv.org/abs/2602.03614)
*Dario Malchiodi,Mattia Ferraretto,Marco Frasca*

Main category: cs.LG

TL;DR: 本文提出在训练中引入每层正则项使权重自然聚类，将量化感知集成到优化过程，减少量化精度损失，还把量化代表作为网络参数，实验验证策略有效


<details>
  <summary>Details</summary>
Motivation: 深度神经网络模型规模大，在资源受限设备部署有挑战，模型压缩必要，现有权重量化技术会导致精度下降且不影响学习阶段参数空间探索

Method: 引入每层正则项，使权重在训练中自然形成聚类，将量化感知集成到优化过程，把量化代表作为网络参数嵌入反向传播过程

Result: 在CIFAR - 10上用AlexNet和VGG16模型实验，验证了所提策略的有效性

Conclusion: 所提策略可减少量化方法通常带来的精度损失，同时保留其压缩潜力

Abstract: Deep Neural Networks reached state-of-the-art performance across numerous domains, but this progress has come at the cost of increasingly large and over-parameterized models, posing serious challenges for deployment on resource-constrained devices. As a result, model compression has become essential, and -- among compression techniques -- weight quantization is largely used and particularly effective, yet it typically introduces a non-negligible accuracy drop. However, it is usually applied to already trained models, without influencing how the parameter space is explored during the learning phase. In contrast, we introduce per-layer regularization terms that drive weights to naturally form clusters during training, integrating quantization awareness directly into the optimization process. This reduces the accuracy loss typically associated with quantization methods while preserving their compression potential. Furthermore, in our framework quantization representatives become network parameters, marking, to the best of our knowledge, the first approach to embed quantization parameters directly into the backpropagation procedure. Experiments on CIFAR-10 with AlexNet and VGG16 models confirm the effectiveness of the proposed strategy.

</details>


### [335] [Ultra Fast PDE Solving via Physics Guided Few-step Diffusion](https://arxiv.org/abs/2602.03627)
*Cindy Xiangrui Kong,Yueqi Wang,Haoyang Zheng,Weijian Luo,Guang Lin*

Main category: cs.LG

TL;DR: 提出Phys - Instruct框架解决扩散模型求解PDEs的问题，实现快速推理并降低误差。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的模型求解PDEs存在高采样成本和物理一致性不足的问题。

Method: 提出Phys - Instruct框架，将预训练扩散PDE求解器压缩为少步生成器实现快速采样，通过PDE蒸馏引导注入PDE知识增强物理一致性。

Result: 在五个PDE基准测试中，推理速度快几个数量级，PDE误差比现有扩散基线降低超8倍，生成的无条件学生模型可用于下游条件任务。

Conclusion: Phys - Instruct是基于深度生成模型的超快速PDE求解的有效框架。

Abstract: Diffusion-based models have demonstrated impressive accuracy and generalization in solving partial differential equations (PDEs). However, they still face significant limitations, such as high sampling costs and insufficient physical consistency, stemming from their many-step iterative sampling mechanism and lack of explicit physics constraints. To address these issues, we propose Phys-Instruct, a novel physics-guided distillation framework which not only (1) compresses a pre-trained diffusion PDE solver into a few-step generator via matching generator and prior diffusion distributions to enable rapid sampling, but also (2) enhances the physics consistency by explicitly injecting PDE knowledge through a PDE distillation guidance. Physic-Instruct is built upon a solid theoretical foundation, leading to a practical physics-constrained training objective that admits tractable gradients. Across five PDE benchmarks, Phys-Instruct achieves orders-of-magnitude faster inference while reducing PDE error by more than 8 times compared to state-of-the-art diffusion baselines. Moreover, the resulting unconditional student model functions as a compact prior, enabling efficient and physically consistent inference for various downstream conditional tasks. Our results indicate that Phys-Instruct is a novel, effective, and efficient framework for ultra-fast PDE solving powered by deep generative models.

</details>


### [336] [CTTVAE: Latent Space Structuring for Conditional Tabular Data Generation on Imbalanced Datasets](https://arxiv.org/abs/2602.03641)
*Milosh Devic,Jordan Gierschendorf,David Garson*

Main category: cs.LG

TL;DR: 提出CTTVAE+TBS框架用于严重类别不平衡下的合成表格数据生成，在多基准测试中表现出色，为相关行业提供解决方案。


<details>
  <summary>Details</summary>
Motivation: 多数生成模型在严重类别不平衡时忽视少数群体或生成样本对下游学习无用，需要解决合成表格数据生成问题。

Method: 引入CTTVAE，配备类感知三元组边界损失和按采样训练策略，形成CTTVAE+TBS框架。

Result: 在六个真实世界基准测试中，CTTVAE+TBS在少数类下游效用最强，保持竞争力，消融研究证实组件贡献。

Conclusion: CTTVAE+TBS为条件表格数据生成提供稳健可解释方案，适用于医疗、欺诈检测等行业。

Abstract: Generating synthetic tabular data under severe class imbalance is essential for domains where rare but high-impact events drive decision-making. However, most generative models either overlook minority groups or fail to produce samples that are useful for downstream learning. We introduce CTTVAE, a Conditional Transformer-based Tabular Variational Autoencoder equipped with two complementary mechanisms: (i) a class-aware triplet margin loss that restructures the latent space for sharper intra-class compactness and inter-class separation, and (ii) a training-by-sampling strategy that adaptively increases exposure to underrepresented groups. Together, these components form CTTVAE+TBS, a framework that consistently yields more representative and utility-aligned samples without destabilizing training. Across six real-world benchmarks, CTTVAE+TBS achieves the strongest downstream utility on minority classes, often surpassing models trained on the original imbalanced data while maintaining competitive fidelity and bridging the gap for privacy for interpolation-based sampling methods and deep generative methods. Ablation studies further confirm that both latent structuring and targeted sampling contribute to these gains. By explicitly prioritizing downstream performance in rare categories, CTTVAE+TBS provides a robust and interpretable solution for conditional tabular data generation, with direct applicability to industries such as healthcare, fraud detection, and predictive maintenance where even small gains in minority cases can be critical.

</details>


### [337] [Reinforcement Fine-Tuning for History-Aware Dense Retriever in RAG](https://arxiv.org/abs/2602.03645)
*Yicheng Zhang,Zhen Qin,Zhaomin Wu,Wenqi Zhang,Shuiguang Deng*

Main category: cs.LG

TL;DR: 本文提出新方法解决基于强化学习的RAG检索器优化问题，实验证明有效提升RAG性能。


<details>
  <summary>Details</summary>
Motivation: 现有RAG检索器优化方案存在目标不匹配问题，强化学习用于检索器优化有挑战。

Method: 用随机采样代替确定性检索，将RAG作为马尔可夫决策过程，在每个检索步骤将检索历史纳入状态。

Result: 在不同RAG管道、数据集和检索器规模的广泛实验中，该方法均提升了RAG性能。

Conclusion: 所提方法能有效解决现有问题，提升RAG性能

Abstract: Retrieval-augmented generation (RAG) enables large language models (LLMs) to produce evidence-based responses, and its performance hinges on the matching between the retriever and LLMs. Retriever optimization has emerged as an efficient alternative to fine-tuning LLMs. However, existing solutions suffer from objective mismatch between retriever optimization and the goal of RAG pipeline. Reinforcement learning (RL) provides a promising solution to address this limitation, yet applying RL to retriever optimization introduces two fundamental challenges: 1) the deterministic retrieval is incompatible with RL formulations, and 2) state aliasing arises from query-only retrieval in multi-hop reasoning. To address these challenges, we replace deterministic retrieval with stochastic sampling and formulate RAG as a Markov decision process, making retriever optimizable by RL. Further, we incorporate retrieval history into the state at each retrieval step to mitigate state aliasing. Extensive experiments across diverse RAG pipelines, datasets, and retriever scales demonstrate consistent improvements of our approach in RAG performance.

</details>


### [338] [Sequential Group Composition: A Window into the Mechanics of Deep Learning](https://arxiv.org/abs/2602.03655)
*Giovanni Luca Marchetti,Daniel Kunin,Adele Myers,Francisco Acosta,Nina Miolane*

Main category: cs.LG

TL;DR: 引入序列群组合任务探究神经网络学习结构化操作能力，分析不同因素对学习的影响，对比不同网络学习该任务的表现。


<details>
  <summary>Details</summary>
Motivation: 了解在序列上训练的神经网络如何获得执行结构化操作的能力。

Method: 引入序列群组合任务，分析群结构、编码统计和序列长度对学习的影响，证明两层网络学习方式，对比不同深度模型学习效果。

Result: 两层网络学习任务需隐藏层宽度随序列长度指数增长，深度模型可利用任务结合律改善扩展性，RNN按顺序组合元素，多层网络并行组合相邻对。

Conclusion: 序列群组合任务为深入理解深度学习机制提供了可处理的窗口。

Abstract: How do neural networks trained over sequences acquire the ability to perform structured operations, such as arithmetic, geometric, and algorithmic computation? To gain insight into this question, we introduce the sequential group composition task. In this task, networks receive a sequence of elements from a finite group encoded in a real vector space and must predict their cumulative product. The task can be order-sensitive and requires a nonlinear architecture to be learned. Our analysis isolates the roles of the group structure, encoding statistics, and sequence length in shaping learning. We prove that two-layer networks learn this task one irreducible representation of the group at a time in an order determined by the Fourier statistics of the encoding. These networks can perfectly learn the task, but doing so requires a hidden width exponential in the sequence length $k$. In contrast, we show how deeper models exploit the associativity of the task to dramatically improve this scaling: recurrent neural networks compose elements sequentially in $k$ steps, while multilayer networks compose adjacent pairs in parallel in $\log k$ layers. Overall, the sequential group composition task offers a tractable window into the mechanics of deep learning.

</details>


### [339] [ContraLog: Log File Anomaly Detection with Contrastive Learning and Masked Language Modeling](https://arxiv.org/abs/2602.03678)
*Simon Dietz,Kai Klede,An Nguyen,Bjoern M Eskofier*

Main category: cs.LG

TL;DR: 提出无解析器的自监督方法ContraLog用于日志异常检测，在多个数据集验证有效性，表明嵌入级预测有潜力。


<details>
  <summary>Details</summary>
Motivation: 现有日志异常检测方法依赖日志解析器，丢弃变量值和语义内容，需更好方法。

Method: 将日志异常检测重构为预测连续消息嵌入，结合消息编码器和序列编码器，用掩码语言建模和对比学习训练。

Result: 在HDFS、BGL和Thunderbird基准数据集上有效，生成的消息嵌入有意义，无序列上下文也能预测异常。

Conclusion: 嵌入级预测是日志异常检测的有效方法，可能适用于其他事件序列。

Abstract: Log files record computational events that reflect system state and behavior, making them a primary source of operational insights in modern computer systems. Automated anomaly detection on logs is therefore critical, yet most established methods rely on log parsers that collapse messages into discrete templates, discarding variable values and semantic content. We propose ContraLog, a parser-free and self-supervised method that reframes log anomaly detection as predicting continuous message embeddings rather than discrete template IDs. ContraLog combines a message encoder that produces rich embeddings for individual log messages with a sequence encoder to model temporal dependencies within sequences. The model is trained with a combination of masked language modeling and contrastive learning to predict masked message embeddings based on the surrounding context. Experiments on the HDFS, BGL, and Thunderbird benchmark datasets empirically demonstrate effectiveness on complex datasets with diverse log messages. Additionally, we find that message embeddings generated by ContraLog carry meaningful information and are predictive of anomalies even without sequence context. These results highlight embedding-level prediction as an approach for log anomaly detection, with potential applicability to other event sequences.

</details>


### [340] [QuAIL: Quality-Aware Inertial Learning for Robust Training under Data Corruption](https://arxiv.org/abs/2602.03686)
*Mattia Sabella,Alberto Archetti,Pietro Pinoli,Matteo Matteucci,Cinzia Cappiello*

Main category: cs.LG

TL;DR: 提出QuAIL机制，将特征可靠性先验融入学习过程，在50个数据集上验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 表格机器学习系统数据存在非均匀损坏问题，且仅通过列级可靠性指标记录，限制了鲁棒性和清理技术的应用。

Method: 提出QuAIL，用可学习的特征调制层增强现有模型，通过质量相关的近端正则化器选择性约束更新。

Result: 在50个分类和回归数据集上，QuAIL在随机和依赖值的损坏情况下均提高了平均性能，在低数据和系统偏差设置中表现尤其稳健。

Conclusion: 将特征可靠性信息直接融入优化动态是实现弹性表格学习的实用有效方法。

Abstract: Tabular machine learning systems are frequently trained on data affected by non-uniform corruption, including noisy measurements, missing entries, and feature-specific biases. In practice, these defects are often documented only through column-level reliability indicators rather than instance-wise quality annotations, limiting the applicability of many robustness and cleaning techniques. We present QuAIL, a quality-informed training mechanism that incorporates feature reliability priors directly into the learning process. QuAIL augments existing models with a learnable feature-modulation layer whose updates are selectively constrained by a quality-dependent proximal regularizer, thereby inducing controlled adaptation across features of varying trustworthiness. This stabilizes optimization under structured corruption without explicit data repair or sample-level reweighting. Empirical evaluation across 50 classification and regression datasets demonstrates that QuAIL consistently improves average performance over neural baselines under both random and value-dependent corruption, with especially robust behavior in low-data and systematically biased settings. These results suggest that incorporating feature reliability information directly into optimization dynamics is a practical and effective approach for resilient tabular learning.

</details>


### [341] [LLM-Inspired Pretrain-Then-Finetune for Small-Data, Large-Scale Optimization](https://arxiv.org/abs/2602.03690)
*Zishi Zhang,Jinhui Han,Ming Hu,Yijie Peng*

Main category: cs.LG

TL;DR: 针对小数据、大规模决策问题，提出基于Transformer模型的预训练 - 微调方法，理论分析验证其有效性，微调有规模经济效应。


<details>
  <summary>Details</summary>
Motivation: 解决企业在小数据、大规模决策场景下，每个实例数据少且可能有噪声的问题。

Method: 设计Transformer模型，先在大规模领域相关合成数据上预训练，再在真实观测数据上微调，有特定架构设计和训练流程。

Result: 开发了关于Transformer学习的全面误差分析，建立非渐近保证验证方法有效性，揭示预训练和微调共同决定性能，微调有规模经济效应。

Conclusion: 所提出的预训练 - 微调方法能有效解决小数据、大规模决策问题，微调在实例增多时效果更好。

Abstract: We consider small-data, large-scale decision problems in which a firm must make many operational decisions simultaneously (e.g., across a large product portfolio) while observing only a few, potentially noisy, data points per instance. Inspired by the success of large language models (LLMs), we propose a pretrain-then-finetune approach built on a designed Transformer model to address this challenge. The model is first pretrained on large-scale, domain-informed synthetic data that encode managerial knowledge and structural features of the decision environment, and is then fine-tuned on real observations. This new pipeline offers two complementary advantages: pretraining injects domain knowledge into the learning process and enables the training of high-capacity models using abundant synthetic data, while finetuning adapts the pretrained model to the operational environment and improves alignment with the true data-generating regime. While we have leveraged the Transformer's state-of-the-art representational capacity, particularly its attention mechanism, to efficiently extract cross-task structure, our approach is not an off-the-shelf application. Instead, it relies on problem-specific architectural design and a tailored training procedure to match the decision setting. Theoretically, we develop the first comprehensive error analysis regarding Transformer learning in relevant contexts, establishing nonasymptotic guarantees that validate the method's effectiveness. Critically, our analysis reveals how pretraining and fine-tuning jointly determine performance, with the dominant contribution governed by whichever is more favorable. In particular, finetuning exhibits an economies-of-scale effect, whereby transfer learning becomes increasingly effective as the number of instances grows.

</details>


### [342] [Conflict-Resolving and Sharpness-Aware Minimization for Generalized Knowledge Editing with Multiple Updates](https://arxiv.org/abs/2602.03696)
*Duy Nguyen,Hanqi Xiao,Archiki Prasad,Elias Stengel-Eskin,Hyunji Lee,Mohit Bansal*

Main category: cs.LG

TL;DR: 提出CoRSA框架用于大模型知识更新，在多基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 大模型需知识更新，但现有方法存在泛化差、稳定性低和知识冲突等局限。

Method: 提出CoRSA训练框架，通过最小化损失曲率提升泛化和稳定性，最大化新旧知识间隔解决冲突。

Result: 在三个事实编辑基准测试中泛化能力显著提升，多次更新时减少灾难性遗忘，在代码领域也表现良好。

Conclusion: CoRSA是一种有效的大模型知识编辑框架，能解决现有方法的局限。

Abstract: Large language models (LLMs) rely on internal knowledge to solve many downstream tasks, making it crucial to keep them up to date. Since full retraining is expensive, prior work has explored efficient alternatives such as model editing and parameter-efficient fine-tuning. However, these approaches often break down in practice due to poor generalization across inputs, limited stability, and knowledge conflict. To address these limitations, we propose the CoRSA (Conflict-Resolving and Sharpness-Aware Minimization) training framework, a parameter-efficient, holistic approach for knowledge editing with multiple updates. CoRSA tackles multiple challenges simultaneously: it improves generalization to different input forms and enhances stability across multiple updates by minimizing loss curvature, and resolves conflicts by maximizing the margin between new and prior knowledge. Across three widely used fact editing benchmarks, CoRSA achieves significant gains in generalization, outperforming baselines with average absolute improvements of 12.42% over LoRA and 10% over model editing methods. With multiple updates, it maintains high update efficacy while reducing catastrophic forgetting by 27.82% compared to LoRA. CoRSA also generalizes to the code domain, outperforming the strongest baseline by 5.48% Pass@5 in update efficacy.

</details>


### [343] [Data-Driven Graph Filters via Adaptive Spectral Shaping](https://arxiv.org/abs/2602.03698)
*Dylan Sandfelder,Mihai Cucuringu,Xiaowen Dong*

Main category: cs.LG

TL;DR: 提出Adaptive Spectral Shaping图滤波框架及TASS实现少样本迁移，在合成基准测试中表现良好，提供紧凑光谱模块。


<details>
  <summary>Details</summary>
Motivation: 构建可扩展、可解释且能跨图泛化的图滤波框架。

Method: 引入自适应光谱整形框架，用切比雪夫多项式展开实现滤波器，提出TASS在目标图上仅调整形状参数。

Result: 在合成基准测试中，该框架降低重建误差，TASS实现正向迁移。

Conclusion: 该框架提供紧凑光谱模块，具备可扩展性、可解释性和跨图泛化能力。

Abstract: We introduce Adaptive Spectral Shaping, a data-driven framework for graph filtering that learns a reusable baseline spectral kernel and modulates it with a small set of Gaussian factors. The resulting multi-peak, multi-scale responses allocate energy to heterogeneous regions of the Laplacian spectrum while remaining interpretable via explicit centers and bandwidths. To scale, we implement filters with Chebyshev polynomial expansions, avoiding eigendecompositions. We further propose Transferable Adaptive Spectral Shaping (TASS): the baseline kernel is learned on source graphs and, on a target graph, kept fixed while only the shaping parameters are adapted, enabling few-shot transfer under matched compute. Across controlled synthetic benchmarks spanning graph families and signal regimes, Adaptive Spectral Shaping reduces reconstruction error relative to fixed-prototype wavelets and learned linear banks, and TASS yields consistent positive transfer. The framework provides compact spectral modules that plug into graph signal processing pipelines and graph neural networks, combining scalability, interpretability, and cross-graph generalization.

</details>


### [344] [Efficient Training of Boltzmann Generators Using Off-Policy Log-Dispersion Regularization](https://arxiv.org/abs/2602.03729)
*Henrik Schopmans,Christopher von Klitzing,Pascal Friederich*

Main category: cs.LG

TL;DR: 提出离策略对数分散正则化 (LDR) 框架用于从非归一化概率密度采样，提升了性能和数据效率。


<details>
  <summary>Details</summary>
Motivation: Boltzmann 生成器的实际成功依赖于数据高效训练，而模拟数据和目标能量评估成本高。

Method: 提出离策略 LDR 正则化框架，结合标准数据训练目标，无需额外在线策略样本，利用目标能量标签信息。

Result: 在所有基准测试中，LDR 提升了最终性能和数据效率，样本效率提升高达一个数量级。

Conclusion: 提出的 LDR 框架具有广泛适用性，能提升性能和数据效率。

Abstract: Sampling from unnormalized probability densities is a central challenge in computational science. Boltzmann generators are generative models that enable independent sampling from the Boltzmann distribution of physical systems at a given temperature. However, their practical success depends on data-efficient training, as both simulation data and target energy evaluations are costly. To this end, we propose off-policy log-dispersion regularization (LDR), a novel regularization framework that builds on a generalization of the log-variance objective. We apply LDR in the off-policy setting in combination with standard data-based training objectives, without requiring additional on-policy samples. LDR acts as a shape regularizer of the energy landscape by leveraging additional information in the form of target energy labels. The proposed regularization framework is broadly applicable, supporting unbiased or biased simulation datasets as well as purely variational training without access to target samples. Across all benchmarks, LDR improves both final performance and data efficiency, with sample efficiency gains of up to one order of magnitude.

</details>


### [345] [Fast-MWEM: Private Data Release in Sublinear Time](https://arxiv.org/abs/2602.03732)
*Themistoklis Haris,Steve Choi,Mutiraj Laksanawisit*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The Multiplicative Weights Exponential Mechanism (MWEM) is a fundamental iterative framework for private data analysis, with broad applications such as answering $m$ linear queries, or privately solving systems of $m$ linear constraints. However, a critical bottleneck hindering its scalability is the $Θ(m)$ time complexity required to execute the exponential mechanism in each iteration. We introduce a modification to the MWEM framework that improves the per-iteration runtime dependency to $Θ(\sqrt{m})$ in expectation. This is done via a lazy sampling approach to the Report-Noisy-Max mechanism, which we implement efficiently using Gumbel noise and a $k$-Nearest Neighbor data structure. This allows for the rapid selection of the approximate score in the exponential mechanism without an exhaustive linear scan. We apply our accelerated framework to the problems of private linear query release and solving Linear Programs (LPs) under neighboring constraint conditions and low-sensitivity assumptions. Experimental evaluation confirms that our method provides a substantial runtime improvement over classic MWEM.

</details>


### [346] [Soft Sensor for Bottom-Hole Pressure Estimation in Petroleum Wells Using Long Short-Term Memory and Transfer Learning](https://arxiv.org/abs/2602.03737)
*M. A. Fernandes,E. Gildin,M. A. Sampaio*

Main category: cs.LG

TL;DR: 提出基于机器学习的软传感器估算井底压力，用LSTM等模型并引入迁移学习，在巴西数据集测试表现佳，是物理传感器的有效替代方案。


<details>
  <summary>Details</summary>
Motivation: 永久井下压力计存在可靠性和成本问题，需要一种新方法来估算井底压力以实现生产优化、安全和减排。

Method: 提出基于机器学习的软传感器，使用LSTM模型并与MLP和岭回归比较，引入迁移学习以适应不同操作环境。

Result: 在巴西盐下盆地的海上实际数据集上测试，平均绝对百分比误差（MAPE）始终低于2%，优于基准。

Conclusion: 该方法是一种经济有效、准确的物理传感器替代方案，适用于不同的油藏和流动条件。

Abstract: Monitoring bottom-hole variables in petroleum wells is essential for production optimization, safety, and emissions reduction. Permanent Downhole Gauges (PDGs) provide real-time pressure data but face reliability and cost issues. We propose a machine learning-based soft sensor to estimate flowing Bottom-Hole Pressure (BHP) using wellhead and topside measurements. A Long Short-Term Memory (LSTM) model is introduced and compared with Multi-Layer Perceptron (MLP) and Ridge Regression. We also pioneer Transfer Learning for adapting models across operational environments. Tested on real offshore datasets from Brazil's Pre-salt basin, the methodology achieved Mean Absolute Percentage Error (MAPE) consistently below 2\%, outperforming benchmarks. This work offers a cost-effective, accurate alternative to physical sensors, with broad applicability across diverse reservoir and flow conditions.

</details>


### [347] [Reasoning with Latent Tokens in Diffusion Language Models](https://arxiv.org/abs/2602.03769)
*Andre He,Sean Welleck,Daniel Fried*

Main category: cs.LG

TL;DR: 离散扩散模型在语言建模中表现良好但推理慢，研究发现联合预测机制是关键，提出调节潜在标记数量方法平衡推理速度和样本质量，还可引入自回归模型提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决离散扩散模型在推理时计算量大的问题，提升自回归模型在推理任务上的性能。

Method: 追踪联合预测机制，消融联合预测，调节潜在标记数量，在自回归模型中引入辅助多标记预测目标。

Result: 调节潜在标记数量可平滑平衡推理速度和样本质量，在自回归模型中引入潜在标记可提升推理任务性能。

Conclusion: 潜在标记是提高需要全局连贯性或前瞻性任务性能的通用机制。

Abstract: Discrete diffusion models have recently become competitive with autoregressive models for language modeling, even outperforming them on reasoning tasks requiring planning and global coherence, but they require more computation at inference time. We trace this trade-off to a key mechanism: diffusion models are trained to jointly predict a distribution over all unknown tokens, including those that will not actually be decoded in the current step. Ablating this joint prediction yields faster inference but degrades performance, revealing that accurate prediction at the decoded position relies on joint reasoning about the distribution of undecoded tokens. We interpret these as latent tokens and introduce a method for modulating their number, demonstrating empirically that this enables a smooth tradeoff between inference speed and sample quality. Furthermore, we demonstrate that latent tokens can be introduced into autoregressive models through an auxiliary multi-token prediction objective, yielding substantial improvements on the same reasoning tasks where they have traditionally struggled. Our results suggest that latent tokens, while arising naturally in diffusion, represent a general mechanism for improving performance on tasks requiring global coherence or lookahead.

</details>


### [348] [UniGeM: Unifying Data Mixing and Selection via Geometric Exploration and Mining](https://arxiv.org/abs/2602.03772)
*Changhao Wang,Yunfei Yu,Xinhao Yao,Jiaolong Yang,Riccardo Cantoro,Chaobo Li,Qing Cui,Jun Zhou*

Main category: cs.LG

TL;DR: 由于数据质量，大语言模型扩展受限，提出UniGeM框架统一数据混合和选择，实验验证其提升效率和性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型扩展受数据质量限制，现有的数据混合和样本选择方法会破坏代码语料结构。

Method: 将数据管理视为流形近似问题，通过分层操作，Macro-Exploration利用基于稳定性的聚类学习混合权重，Micro-Mining根据几何分布过滤高质量实例。

Result: 通过在100B个token上训练8B和16B的MoE模型进行验证，UniGeM比随机基线实现了2.0倍的数据效率，在推理评估和多语言泛化方面提高了整体性能。

Conclusion: UniGeM框架统一了数据混合和选择，在数据效率和模型性能上有优势。

Abstract: The scaling of Large Language Models (LLMs) is increasingly limited by data quality. Most methods handle data mixing and sample selection separately, which can break the structure in code corpora. We introduce \textbf{UniGeM}, a framework that unifies mixing and selection by treating data curation as a \textit{manifold approximation} problem without training proxy models or relying on external reference datasets. UniGeM operates hierarchically: \textbf{Macro-Exploration} learns mixing weights with stability-based clustering; \textbf{Micro-Mining} filters high-quality instances by their geometric distribution to ensure logical consistency. Validated by training 8B and 16B MoE models on 100B tokens, UniGeM achieves \textbf{2.0$\times$ data efficiency} over a random baseline and further improves overall performance compared to SOTA methods in reasoning-heavy evaluations and multilingual generalization.

</details>


### [349] [Reasoning Cache: Continual Improvement Over Long Horizons via Short-Horizon RL](https://arxiv.org/abs/2602.03773)
*Ian Wu,Yuxiao Qu,Amrith Setlur,Aviral Kumar*

Main category: cs.LG

TL;DR: 提出迭代解码算法RC，使大语言模型能在测试时持续提升性能，在HMMT 2025上表现优异，还能更好利用现有支架提升性能。


<details>
  <summary>Details</summary>
Motivation: 标准强化学习在固定问题分布和训练预算下运行，限制了大语言模型在测试时分布变化情况下的外推能力。

Method: 引入迭代解码算法RC，在训练和推理时替代标准自回归解码，利用大语言模型响应生成和摘要能力的不对称性构建推理链。

Result: 用16k-token训练预算训练4B模型，在HMMT 2025上性能从40%提升到近70%，优于同规模模型和许多更大的推理大语言模型。

Conclusion: 使用RC训练的模型能外推并持续提升推理能力，还能更有效利用现有支架提升测试时的性能。

Abstract: Large Language Models (LLMs) that can continually improve beyond their training budgets are able to solve increasingly difficult problems by adapting at test time, a property we refer to as extrapolation. However, standard reinforcement learning (RL) operates over fixed problem distributions and training budgets, which limits extrapolation amidst distribution shift at test time. To address this, we introduce RC, an iterative decoding algorithm that replaces standard autoregressive decoding during both training and inference. RC exploits an asymmetry between the response generation and summarization capabilities of LLMs to construct reasoning chains that consistently improve across iterations. Models trained to use RC can extrapolate and continually improve over reasoning horizons more than an order of magnitude longer than those seen during training. Empirically, training a 4B model with RC using a 16k-token training budget improves performance on HMMT 2025 from 40% to nearly 70% with 0.5m tokens at test time, outperforming both comparably sized models and many larger reasoning LLMs. Finally, we also show that models trained with RC can more effectively leverage existing scaffolds to further scale test-time performance, due to the improved summary-conditioned generation abilities learned through training.

</details>


### [350] [Reward Redistribution for CVaR MDPs using a Bellman Operator on L-infinity](https://arxiv.org/abs/2602.03778)
*Aneri Muni,Vincent Taboga,Esther Derman,Pierre-Luc Bacon,Erick Delage*

Main category: cs.LG

TL;DR: 提出基于增广的静态CVaR目标新公式，开发风险规避算法，有收敛保证和误差界，实证显示算法有效。


<details>
  <summary>Details</summary>
Motivation: 传统静态CVaR目标在马尔可夫决策过程中无递归贝尔曼分解，经典解决方法有稀疏奖励和退化不动点问题。

Method: 提出基于增广的静态CVaR目标新公式，开发风险规避值迭代和无模型Q学习算法，依赖离散化增广状态。

Result: 算法能成功学习CVaR敏感策略，实现有效性能 - 安全权衡。

Conclusion: 所提新公式和算法有效，可解决传统方法问题，实现性能 - 安全权衡。

Abstract: Tail-end risk measures such as static conditional value-at-risk (CVaR) are used in safety-critical applications to prevent rare, yet catastrophic events. Unlike risk-neutral objectives, the static CVaR of the return depends on entire trajectories without admitting a recursive Bellman decomposition in the underlying Markov decision process. A classical resolution relies on state augmentation with a continuous variable. However, unless restricted to a specialized class of admissible value functions, this formulation induces sparse rewards and degenerate fixed points. In this work, we propose a novel formulation of the static CVaR objective based on augmentation. Our alternative approach leads to a Bellman operator with: (1) dense per-step rewards; (2) contracting properties on the full space of bounded value functions. Building on this theoretical foundation, we develop risk-averse value iteration and model-free Q-learning algorithms that rely on discretized augmented states. We further provide convergence guarantees and approximation error bounds due to discretization. Empirical results demonstrate that our algorithms successfully learn CVaR-sensitive policies and achieve effective performance-safety trade-offs.

</details>


### [351] [Efficient Estimation of Kernel Surrogate Models for Task Attribution](https://arxiv.org/abs/2602.03783)
*Zhenshuo Zhang,Minxuan Duan,Hongyang R. Zhang*

Main category: cs.LG

TL;DR: 本文提出统一任务加权框架分析任务归因方法，引入核替代模型，开发高效学习方法，实验证明其在多领域效果优于线性替代模型。


<details>
  <summary>Details</summary>
Motivation: 量化每个训练任务对目标任务性能的影响，解决直接方法计算不可行及线性替代模型无法捕捉非线性交互的问题。

Method: 提出统一任务加权框架，引入核替代模型，开发基于梯度的估计程序。

Result: 核替代模型估计误差小于2%，与留一法真值相关性比线性替代模型高25%，下游任务选择中提升40%。

Conclusion: 核替代模型能有效捕捉二阶任务交互，在多领域表现良好，可用于下游任务选择。

Abstract: Modern AI agents such as large language models are trained on diverse tasks -- translation, code generation, mathematical reasoning, and text prediction -- simultaneously. A key question is to quantify how each individual training task influences performance on a target task, a problem we refer to as task attribution. The direct approach, leave-one-out retraining, measures the effect of removing each task, but is computationally infeasible at scale. An alternative approach that builds surrogate models to predict a target task's performance for any subset of training tasks has emerged in recent literature. Prior work focuses on linear surrogate models, which capture first-order relationships, but miss nonlinear interactions such as synergy, antagonism, or XOR-type effects. In this paper, we first consider a unified task weighting framework for analyzing task attribution methods, and show a new connection between linear surrogate models and influence functions through a second-order analysis. Then, we introduce kernel surrogate models, which more effectively represent second-order task interactions. To efficiently learn the kernel surrogate, we develop a gradient-based estimation procedure that leverages a first-order approximation of pretrained models; empirically, this yields accurate estimates with less than $2\%$ relative error without repeated retraining. Experiments across multiple domains -- including math reasoning in transformers, in-context learning, and multi-objective reinforcement learning -- demonstrate the effectiveness of kernel surrogate models. They achieve a $25\%$ higher correlation with the leave-one-out ground truth than linear surrogates and influence-function baselines. When used for downstream task selection, kernel surrogate models yield a $40\%$ improvement in demonstration selection for in-context learning and multi-objective reinforcement learning benchmarks.

</details>


### [352] [Inference-time Unlearning Using Conformal Prediction](https://arxiv.org/abs/2602.03787)
*Somnath Basu Roy Chowdhury,Rahul Kidambi,Avinava Dubey,David Wang,Gokhan Mergen,Amr Ahmed,Aranyak Mehta*

Main category: cs.LG

TL;DR: 本文针对现有机器学习去学习方法应用于生成模型时的不足，提出推理时去学习范式和框架，利用共形预测，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有去学习方法的假设在现实应用中常受挑战，且更新参数会降低模型预训练获得的通用能力。

Method: 采用推理时去学习范式，引入框架，利用验证器反馈迭代优化生成响应质量，不更新模型参数，利用共形预测减少计算开销并提供无分布去学习保证。

Result: 该方法显著优于现有最先进方法，在具有挑战性的去学习基准测试中，去学习误差最多降低93%。

Conclusion: 所提推理时去学习框架有效，能解决现有去学习方法的不足。

Abstract: Machine unlearning is the process of efficiently removing specific information from a trained machine learning model without retraining from scratch. Existing unlearning methods, which often provide provable guarantees, typically involve retraining a subset of model parameters based on a forget set. While these approaches show promise in certain scenarios, their underlying assumptions are often challenged in real-world applications -- particularly when applied to generative models. Furthermore, updating parameters using these unlearning procedures often degrades the general-purpose capabilities the model acquired during pre-training. Motivated by these shortcomings, this paper considers the paradigm of inference time unlearning -- wherein, the generative model is equipped with an (approximately correct) verifier that judges whether the model's response satisfies appropriate unlearning guarantees. This paper introduces a framework that iteratively refines the quality of the generated responses using feedback from the verifier without updating the model parameters. The proposed framework leverages conformal prediction to reduce computational overhead and provide distribution-free unlearning guarantees. This paper's approach significantly outperforms existing state-of-the-art methods, reducing unlearning error by up to 93% across challenging unlearning benchmarks.

</details>


### [353] [Should I use Synthetic Data for That? An Analysis of the Suitability of Synthetic Data for Data Sharing and Augmentation](https://arxiv.org/abs/2602.03791)
*Bogdan Kulynych,Theresa Stadler,Jean Louis Raisaro,Carmela Troncoso*

Main category: cs.LG

TL;DR: 研究合成数据三个用例，分析其适用条件与局限，助决策者判断是否适用。


<details>
  <summary>Details</summary>
Motivation: 生成式建模发展使合成数据受关注，研究其在数据访问、稀缺和代表性不足问题上的三个用例。

Method: 对每个用例进行问题形式化，通过形式分析和案例研究，分析合成数据达成目标的条件。

Result: 识别出合成数据作为有效解决方案的基本和实际限制，发现许多现有或设想用例不适合。

Conclusion: 对合成数据用例的形式化和分类可帮助决策者评估其对特定数据可用性问题的适用性。

Abstract: Recent advances in generative modelling have led many to see synthetic data as the go-to solution for a range of problems around data access, scarcity, and under-representation. In this paper, we study three prominent use cases: (1) Sharing synthetic data as a proxy for proprietary datasets to enable statistical analyses while protecting privacy, (2) Augmenting machine learning training sets with synthetic data to improve model performance, and (3) Augmenting datasets with synthetic data to reduce variance in statistical estimation. For each use case, we formalise the problem setting and study, through formal analysis and case studies, under which conditions synthetic data can achieve its intended objectives. We identify fundamental and practical limits that constrain when synthetic data can serve as an effective solution for a particular problem. Our analysis reveals that due to these limits many existing or envisioned use cases of synthetic data are a poor problem fit. Our formalisations and classification of synthetic data use cases enable decision makers to assess whether synthetic data is a suitable approach for their specific data availability problem.

</details>


### [354] [Manifold Random Features](https://arxiv.org/abs/2602.03797)
*Ananya Parashar,Derek Long,Dwaipayan Saha,Krzysztof Choromanski*

Main category: cs.LG

TL;DR: 提出用于近似流形上双变量函数的流形随机特征（MRFs）新范式，有理论分析和实验验证。


<details>
  <summary>Details</summary>
Motivation: 寻找在一般流形上近似双变量函数（特别是核函数）的方法，以解决一般场景下无法解析推导连续近似机制的问题。

Method: 利用流形离散化和图随机特征（GRFs）技术学习流形上的连续场，得到连续近似机制。

Result: MRFs提供正且有界特征，揭示了GRFs与常规核的连续随机特征的深层渐近联系，重新发现高斯核近似机制。

Conclusion: 所提出的MRFs方法有效，经理论分析和实验研究验证。

Abstract: We present a new paradigm for creating random features to approximate bi-variate functions (in particular, kernels) defined on general manifolds. This new mechanism of Manifold Random Features (MRFs) leverages discretization of the manifold and the recently introduced technique of Graph Random Features (GRFs) to learn continuous fields on manifolds. Those fields are used to find continuous approximation mechanisms that otherwise, in general scenarios, cannot be derived analytically. MRFs provide positive and bounded features, a key property for accurate, low-variance approximation. We show deep asymptotic connection between GRFs, defined on discrete graph objects, and continuous random features used for regular kernels. As a by-product of our method, we re-discover recently introduced mechanism of Gaussian kernel approximation applied in particular to improve linear-attention Transformers, considering simple random walks on graphs and by-passing original complex mathematical computations. We complement our algorithm with a rigorous theoretical analysis and verify in thorough experimental studies.

</details>


### [355] [Prediction of Critical Heat Flux in Rod Bundles Using Tube-Based Hybrid Machine Learning Models in CTF](https://arxiv.org/abs/2602.03805)
*Aidan Furlong,Robert Salko,Xingang Zhao,Xu Wu*

Main category: cs.LG

TL;DR: 本文探讨基于管的临界热流密度（CHF）数据训练的机器学习CHF预测模型在棒束中的泛化能力，三种机器学习方法预测表现优于基线模型，混合查找表模型性能最佳。


<details>
  <summary>Details</summary>
Motivation: 当前基于机器学习的CHF预测研究多集中于管模型，全尺寸反应堆堆芯模拟需使用棒束几何结构，需研究基于管数据训练的模型在棒束中的泛化能力。

Method: 在CTF子通道代码中实现纯数据驱动深度神经网络（DNN）和两个混合偏差校正模型，预测燃烧工程5×5棒束CHF测试系列的CHF位置和大小，并以W - 3相关性、Bowring相关性和Groeneveld查找表为基线比较器。

Result: 三种基于机器学习的方法在CHF大小和位置的预测上平均而言比基线模型更准确，混合查找表模型表现出最有利的性能指标。

Conclusion: 基于管的CHF数据训练的机器学习模型能有效推广到棒束CHF预测，混合查找表模型效果最佳。

Abstract: The prediction of critical heat flux (CHF) using machine learning (ML) approaches has become a highly active research activity in recent years, the goal of which is to build models more accurate than current conventional approaches such as empirical correlations or lookup tables (LUTs). Previous work developed and deployed tube-based pure and hybrid ML models in the CTF subchannel code, however, full-scale reactor core simulations require the use of rod bundle geometries. Unlike isolated subchannels, rod bundles experience complex thermal hydraulic phenomena such as channel crossflow, spacer grid losses, and effects from unheated conductors. This study investigates the generalization of ML-based CHF prediction models in rod bundles after being trained on tube-based CHF data. A purely data-driven DNN and two hybrid bias-correction models were implemented in the CTF subchannel code and used to predict CHF location and magnitude in the Combustion Engineering 5-by-5 bundle CHF test series. The W-3 correlation, Bowring correlation, and Groeneveld LUT were used as baseline comparators. On average, all three ML-based approaches produced magnitude and location predictions more accurate than the baseline models, with the hybrid LUT model exhibiting the most favorable performance metrics.

</details>


### [356] [Enhancing Imbalanced Node Classification via Curriculum-Guided Feature Learning and Three-Stage Attention Network](https://arxiv.org/abs/2602.03808)
*Abdul Joseph Fofanah,Lian Wen,David Chen,Shaoyang Zhang*

Main category: cs.LG

TL;DR: 提出CL3AN - GNN解决图神经网络节点分类不平衡问题，在多个数据集验证有效。


<details>
  <summary>Details</summary>
Motivation: 图神经网络中标签不平衡导致模型学习不公平，对少数类表现差。

Method: 提出CL3AN - GNN，采用三步注意力系统，分阶段处理简单和复杂特征，通过迭代消息传递和课程对齐损失加权整合特征。

Result: 在八个数据集上，该模型在准确率、F1分数和AUC上较现有方法持续提升。

Conclusion: 为图神经网络课程学习提供理论框架，实证其解决不平衡问题的有效性。

Abstract: Imbalanced node classification in graph neural networks (GNNs) happens when some labels are much more common than others, which causes the model to learn unfairly and perform badly on the less common classes. To solve this problem, we propose a Curriculum-Guided Feature Learning and Three-Stage Attention Network (CL3AN-GNN), a learning network that uses a three-step attention system (Engage, Enact, Embed) similar to how humans learn. The model begins by engaging with structurally simpler features, defined as (1) local neighbourhood patterns (1-hop), (2) low-degree node attributes, and (3) class-separable node pairs identified via initial graph convolutional networks and graph attention networks (GCN and GAT) embeddings. This foundation enables stable early learning despite label skew. The Enact stage then addresses complicated aspects: (1) connections that require multiple steps, (2) edges that connect different types of nodes, and (3) nodes at the edges of minority classes by using adjustable attention weights. Finally, Embed consolidates these features via iterative message passing and curriculum-aligned loss weighting. We evaluate CL3AN-GNN on eight Open Graph Benchmark datasets spanning social, biological, and citation networks. Experiments show consistent improvements across all datasets in accuracy, F1-score, and AUC over recent state-of-the-art methods. The model's step-by-step method works well with different types of graph datasets, showing quicker results than training everything at once, better performance on new, imbalanced graphs, and clear explanations of each step using gradient stability and attention correlation learning curves. This work provides both a theoretically grounded framework for curriculum learning in GNNs and practical evidence of its effectiveness against imbalances, validated through metrics, convergence speeds, and generalisation tests.

</details>


### [357] [Antidistillation Fingerprinting](https://arxiv.org/abs/2602.03812)
*Yixuan Even Xu,John Kirchenbauer,Yash Savani,Asher Trockman,Alexander Robey,Tom Goldstein,Fei Fang,J. Zico Kolter*

Main category: cs.LG

TL;DR: 提出反蒸馏指纹（ADFP）方法检测学生模型是否基于教师模型输出训练，实验显示其优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有指纹技术在检测模型蒸馏时，在生成质量和指纹强度间存在权衡，需大幅降低实用性来确保指纹被有效内化。

Method: 基于反蒸馏采样的梯度框架，利用代理模型识别和采样能使微调后学生模型中指纹可检测性最大化的标记。

Result: 在GSM8K和OASST1基准测试中，ADFP比现有基线有显著帕累托改进，在对实用性影响极小的情况下有更强检测置信度。

Conclusion: ADFP是一种有效的检测模型蒸馏的方法，即使学生模型架构未知也适用。

Abstract: Model distillation enables efficient emulation of frontier large language models (LLMs), creating a need for robust mechanisms to detect when a third-party student model has trained on a teacher model's outputs. However, existing fingerprinting techniques that could be used to detect such distillation rely on heuristic perturbations that impose a steep trade-off between generation quality and fingerprinting strength, often requiring significant degradation of utility to ensure the fingerprint is effectively internalized by the student. We introduce antidistillation fingerprinting (ADFP), a principled approach that aligns the fingerprinting objective with the student's learning dynamics. Building upon the gradient-based framework of antidistillation sampling, ADFP utilizes a proxy model to identify and sample tokens that directly maximize the expected detectability of the fingerprint in the student after fine-tuning, rather than relying on the incidental absorption of the un-targeted biases of a more naive watermark. Experiments on GSM8K and OASST1 benchmarks demonstrate that ADFP achieves a significant Pareto improvement over state-of-the-art baselines, yielding stronger detection confidence with minimal impact on utility, even when the student model's architecture is unknown.

</details>


### [358] [SymPlex: A Structure-Aware Transformer for Symbolic PDE Solving](https://arxiv.org/abs/2602.03816)
*Yesom Park,Annie C. Lu,Shao-Ching Huang,Qiyang Hu,Y. Sungtaek Ju,Stanley Osher*

Main category: cs.LG

TL;DR: 提出SymPlex框架用于在无真实表达式情况下发现偏微分方程的解析符号解，核心是SymFormer，实验证明可精确恢复非光滑和参数化PDE解。


<details>
  <summary>Details</summary>
Motivation: 在无真实表达式的情况下，寻找偏微分方程的解析符号解，且解决现有方法在表示非光滑行为和显式参数依赖方面的不足。

Method: 将符号PDE求解表述为树结构决策，使用PDE及其边界条件优化候选解，核心是通过树相对自注意力建模层次符号依赖、语法约束自回归解码的SymFormer。

Result: 使用基于深度学习的符号方法精确恢复了非光滑和参数化PDE解。

Conclusion: SymPlex框架能在符号表达式空间直接操作，得到可解释和人类可读的解，克服了现有数值和神经方法的局限。

Abstract: We propose SymPlex, a reinforcement learning framework for discovering analytical symbolic solutions to partial differential equations (PDEs) without access to ground-truth expressions. SymPlex formulates symbolic PDE solving as tree-structured decision-making and optimizes candidate solutions using only the PDE and its boundary conditions. At its core is SymFormer, a structure-aware Transformer that models hierarchical symbolic dependencies via tree-relative self-attention and enforces syntactic validity through grammar-constrained autoregressive decoding, overcoming the limited expressivity of sequence-based generators. Unlike numerical and neural approaches that approximate solutions in discretized or implicit function spaces, SymPlex operates directly in symbolic expression space, enabling interpretable and human-readable solutions that naturally represent non-smooth behavior and explicit parametric dependence. Empirical results demonstrate exact recovery of non-smooth and parametric PDE solutions using deep learning-based symbolic methods.

</details>


### [359] [Robust Intervention Learning from Emergency Stop Interventions](https://arxiv.org/abs/2602.03825)
*Ethan Pronovost,Khimya Khetarpal,Siddhartha Srinivasa*

Main category: cs.LG

TL;DR: 本文定义了鲁棒干预学习问题，提出残差干预微调算法RIFT，从理论分析并通过实验验证其能实现策略改进，表明该领域有前景。


<details>
  <summary>Details</summary>
Motivation: 人类干预数据常含噪声、不完整，需从干预数据学习并对其质量和信息性保持鲁棒性，以改进当前策略。

Method: 提出残差干预微调算法RIFT，将干预学习视为微调问题，结合先验策略；进行理论分析以明确策略改进条件和学习失败情况。

Result: 实验表明残差微调能在多种干预策略和先验策略质量下实现鲁棒且一致的策略改进。

Conclusion: 鲁棒干预学习是未来工作的一个有前途的方向。

Abstract: Human interventions are a common source of data in autonomous systems during testing. These interventions provide an important signal about where the current policy needs improvement, but are often noisy and incomplete. We define Robust Intervention Learning (RIL) as the problem of learning from intervention data while remaining robust to the quality and informativeness of the intervention signal. In the best case, interventions are precise and avoiding them is sufficient to solve the task, but in many realistic settings avoiding interventions is necessary but not sufficient for achieving good performance. We study robust intervention learning in the context of emergency stop interventions and propose Residual Intervention Fine-Tuning (RIFT), a residual fine-tuning algorithm that treats intervention feedback as an incomplete learning signal and explicitly combines it with a prior policy. By framing intervention learning as a fine-tuning problem, our approach leverages structure encoded in the prior policy to resolve ambiguity when intervention signals under-specify the task. We provide theoretical analysis characterizing conditions under which this formulation yields principled policy improvement, and identify regimes where intervention learning is expected to fail. Our experiments reveal that residual fine-tuning enables robust and consistent policy improvement across a range of intervention strategies and prior policy qualities, and highlight robust intervention learning as a promising direction for future work.

</details>


### [360] [Understanding and Exploiting Weight Update Sparsity for Communication-Efficient Distributed RL](https://arxiv.org/abs/2602.03839)
*Erfan Miahi,Eugene Belilovsky*

Main category: cs.LG

TL;DR: 研究强化学习权重更新稀疏性，提出PULSE方法，在带宽受限环境减少通信量，使分散训练接近集中式吞吐量。


<details>
  <summary>Details</summary>
Motivation: 带宽受限的分布式强化学习中，策略权重同步成为可扩展性瓶颈，现有研究基于粗粒度检查点差异。

Method: 系统地在步级和多步粒度上研究权重更新稀疏性，提出PULSE方法，仅传输修改参数的索引和值。

Result: 更新稀疏性持续较高，PULSE方法在带宽受限分散环境通信减少超100倍，保持训练动态和性能一致。

Conclusion: 利用权重更新稀疏结构的PULSE方法，能让分散式强化学习训练接近集中式吞吐量，降低带宽需求。

Abstract: Reinforcement learning (RL) is a critical component for post-training large language models (LLMs). However, in bandwidth-constrained distributed RL, scalability is often bottlenecked by the synchronization of policy weights from trainers to inference workers, particularly over commodity networks or in decentralized settings. While recent studies suggest that RL updates modify only a small fraction of model parameters, these observations are typically based on coarse checkpoint differences. We present a systematic empirical study of weight-update sparsity at both step-level and multi-step granularities, examining its evolution across training dynamics, off-policy delay, and model scale. We find that update sparsity is consistently high, frequently exceeding 99% across practically relevant settings. Leveraging this structure, we propose PULSE (Patch Updates via Lossless Sparse Encoding), a simple yet highly efficient lossless weight synchronization method that transmits only the indices and values of modified parameters. PULSE is robust to transmission errors and avoids floating-point drift inherent in additive delta schemes. In bandwidth-constrained decentralized environments, our approach achieves over 100x (14 GB to ~108 MB) communication reduction while maintaining bit-identical training dynamics and performance compared to full weight synchronization. By exploiting this structure, PULSE enables decentralized RL training to approach centralized throughput, reducing the bandwidth required for weight synchronization from 20 Gbit/s to 0.2 Gbit/s to maintain high GPU utilization.

</details>


### [361] [PLATE: Plasticity-Tunable Efficient Adapters for Geometry-Aware Continual Learning](https://arxiv.org/abs/2602.03846)
*Romain Cosentino*

Main category: cs.LG

TL;DR: 提出无需旧任务数据的预训练模型持续学习方法PLATE，利用网络几何冗余性，代码开源。


<details>
  <summary>Details</summary>
Motivation: 解决基础模型适应中预训练分布数据常不可用的实际障碍。

Method: 利用预训练网络的几何冗余性，构建近似受保护的更新子空间，限制更新到部分冗余神经元；提出PLATE方法，用结构化低秩更新参数化各层，仅训练A。

Result: 得到了能显式控制可塑性 - 保留权衡的持续学习方法。

Conclusion: PLATE方法可在无旧任务数据下进行持续学习，具有较好效果且代码开源。

Abstract: We develop a continual learning method for pretrained models that \emph{requires no access to old-task data}, addressing a practical barrier in foundation model adaptation where pretraining distributions are often unavailable. Our key observation is that pretrained networks exhibit substantial \emph{geometric redundancy}, and that this redundancy can be exploited in two complementary ways. First, redundant neurons provide a proxy for dominant pretraining-era feature directions, enabling the construction of approximately protected update subspaces directly from pretrained weights. Second, redundancy offers a natural bias for \emph{where} to place plasticity: by restricting updates to a subset of redundant neurons and constraining the remaining degrees of freedom, we obtain update families with reduced functional drift on the old-data distribution and improved worst-case retention guarantees. These insights lead to \textsc{PLATE} (\textbf{Pla}sticity-\textbf{T}unable \textbf{E}fficient Adapters), a continual learning method requiring no past-task data that provides explicit control over the plasticity-retention trade-off. PLATE parameterizes each layer with a structured low-rank update $ΔW = B A Q^\top$, where $B$ and $Q$ are computed once from pretrained weights and kept frozen, and only $A$ is trained on the new task. The code is available at https://github.com/SalesforceAIResearch/PLATE.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [362] [Fine-Tuning Language Models to Know What They Know](https://arxiv.org/abs/2602.02605)
*Sangjun Park,Elliot Meyerson,Xin Qiu,Risto Miikkulainen*

Main category: cs.NE

TL;DR: 研究提出用双提示法测量元认知能力的框架及ESMA方法，ESMA有泛化能力，改进源于少量参数修改。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在依赖内部记忆回答问题及报告知识状态方面的研究不足。

Method: 提出用双提示法测量元认知能力$d_{m{type2}}'$的框架，引入ESMA将模型内部知识与显性行为绑定。

Result: ESMA在不同未训练设置中表现出强大泛化能力，表明模型参考自身知识的能力增强。

Conclusion: 这些改进归因于少量显著的参数修改。

Abstract: Metacognition is a critical component of intelligence, specifically regarding the awareness of one's own knowledge. While humans rely on shared internal memory for both answering questions and reporting their knowledge state, this dependency in LLMs remains underexplored. This study proposes a framework to measure metacognitive ability $d_{\rm{type2}}'$ using a dual-prompt method, followed by the introduction of Evolution Strategy for Metacognitive Alignment (ESMA) to bind a model's internal knowledge to its explicit behaviors. ESMA demonstrates robust generalization across diverse untrained settings, indicating a enhancement in the model's ability to reference its own knowledge. Furthermore, parameter analysis attributes these improvements to a sparse set of significant modifications.

</details>


### [363] [Automatic Design of Optimization Test Problems with Large Language Models](https://arxiv.org/abs/2602.02724)
*Wojciech Achtelik,Hubert Guzowski,Maciej Smołka,Jacek Mańdziuk*

Main category: cs.NE

TL;DR: 提出EoTF框架自动生成连续优化测试函数，实验证明其有效性和可扩展性，为基准生成提供实用途径。


<details>
  <summary>Details</summary>
Motivation: 现有基准套件多为手工合成函数，对高维探索性景观分析（ELA）特征覆盖有限，影响评估和元黑盒优化器训练。

Method: 采用LLM驱动的进化搜索，通过最小化生成候选的采样ELA特征与目标特征向量的距离，进化出可解释、独立的目标函数实现。

Result: 在实验中能可靠生成具有匹配ELA特征的非平凡函数，保持优化器性能排名，在3D及维度增加时表现较好。

Conclusion: EoTF为按需生成具有景观属性的基准测试提供了实用方法。

Abstract: The development of black-box optimization algorithms depends on the availability of benchmark suites that are both diverse and representative of real-world problem landscapes. Widely used collections such as BBOB and CEC remain dominated by hand-crafted synthetic functions and provide limited coverage of the high-dimensional space of Exploratory Landscape Analysis (ELA) features, which in turn biases evaluation and hinders training of meta-black-box optimizers. We introduce Evolution of Test Functions (EoTF), a framework that automatically generates continuous optimization test functions whose landscapes match a specified target ELA feature vector. EoTF adapts LLM-driven evolutionary search, originally proposed for heuristic discovery, to evolve interpretable, self-contained numpy implementations of objective functions by minimizing the distance between sampled ELA features of generated candidates and a target profile. In experiments on 24 noiseless BBOB functions and a contamination-mitigating suite of 24 MA-BBOB hybrid functions, EoTF reliably produces non-trivial functions with closely matching ELA characteristics and preserves optimizer performance rankings under fixed evaluation budgets, supporting their validity as surrogate benchmarks. While a baseline neural-network-based generator achieves higher accuracy in 2D, EoTF substantially outperforms it in 3D and exhibits stable solution quality as dimensionality increases, highlighting favorable scalability. Overall, EoTF offers a practical route to scalable, portable, and interpretable benchmark generation targeted to desired landscape properties.

</details>


### [364] [Investigating Quantum Circuit Designs Using Neuro-Evolution](https://arxiv.org/abs/2602.03840)
*Devroop Kar,Daniel Krutz,Travis Desell*

Main category: cs.NE

TL;DR: 提出EXAQC进化方法用于自动设计和训练参数化量子电路，初步结果显示其在分类任务和量子态模拟上表现良好。


<details>
  <summary>Details</summary>
Motivation: 当前量子电路设计方法在可扩展性、灵活性和适应性上存在局限，难以匹配具体问题和量子硬件。

Method: 提出EXAQC进化方法，联合搜索门类型、量子比特连接性、参数化和电路深度，同时考虑硬件和噪声约束，支持Qiskit和Pennylane库。

Result: 在分类任务中，进化电路在多数基准数据集上以有限计算预算达到超90%准确率，能以高保真度模拟目标电路量子态。

Conclusion: 进化搜索是推进量子机器学习和变分量子算法的关键工具，为可扩展、问题感知和硬件高效的量子电路设计提供了原则性途径。

Abstract: Designing effective quantum circuits remains a central challenge in quantum computing, as circuit structure strongly influences expressivity, trainability, and hardware feasibility. Current approaches, whether using manually designed circuit templates, fixed heuristics, or automated rules, face limitations in scalability, flexibility, and adaptability, often producing circuits that are poorly matched to the specific problem or quantum hardware. In this work, we propose the Evolutionary eXploration of Augmenting Quantum Circuits (EXAQC), an evolutionary approach to the automated design and training of parameterized quantum circuits (PQCs) which leverages and extends on strategies from neuroevolution and genetic programming. The proposed method jointly searches over gate types, qubit connectivity, parameterization, and circuit depth while respecting hardware and noise constraints. The method supports both Qiskit and Pennylane libraries, allowing the user to configure every aspect. This work highlights evolutionary search as a critical tool for advancing quantum machine learning and variational quantum algorithms, providing a principled pathway toward scalable, problem-aware, and hardware-efficient quantum circuit design. Preliminary results demonstrate that circuits evolved on classification tasks are able to achieve over 90% accuracy on most of the benchmark datasets with a limited computational budget, and are able to emulate target circuit quantum states with high fidelity scores.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [365] [WritePolicyBench: Benchmarking Memory Write Policies under Byte Budgets](https://arxiv.org/abs/2602.02574)
*Edgard El Cham*

Main category: cs.PF

TL;DR: 介绍用于评估内存写入策略的WritePolicyBench基准，包含任务生成器、外部内存接口、成本模型和标准化指标。


<details>
  <summary>Details</summary>
Motivation: 评估在处理带有文档/API漂移的流时，严格字节预算下内存写入策略（决定存储、合并和逐出内容）的性能。

Method: 创建WritePolicyBench基准，提供任务生成器、外部内存动作接口、字节精确成本模型和标准化指标。

Result: 成功构建WritePolicyBench基准。

Conclusion: WritePolicyBench可用于评估内存写入策略。

Abstract: We introduce WritePolicyBench, a benchmark for evaluating memory write policies: decision rules that choose what to store, merge, and evict under a strict byte budget while processing a stream with document/API drift. The benchmark provides (i) task generators with controlled non-stationarity, (ii) an explicit action interface for external memory, (iii) a byte-accurate cost model, and (iv) standardized metrics that measure both task success and budget efficiency.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [366] [Constitutional Spec-Driven Development: Enforcing Security by Construction in AI-Assisted Code Generation](https://arxiv.org/abs/2602.02584)
*Srinivas Rao Marri*

Main category: cs.SE

TL;DR: 提出Constitutional Spec - Driven Development方法，将安全原则嵌入规范层，用银行微服务案例展示其效果，能减少安全缺陷同时保持开发速度。


<details>
  <summary>Details</summary>
Motivation: AI辅助的快速软件开发引入安全风险，大语言模型关注功能正确性而忽视安全。

Method: 提出Constitutional Spec - Driven Development方法，引入包含安全约束的宪法文档，且该方法与领域无关。

Result: 通过银行微服务案例实施，解决10个关键CWE漏洞，相比无约束AI生成，安全缺陷减少73%，并保持开发速度。

Conclusion: 贡献了宪法安全的正式框架、完整开发方法，证实主动安全规范在AI辅助开发中优于被动安全验证。

Abstract: The proliferation of AI-assisted "vibe coding" enables rapid software development but introduces significant security risks, as Large Language Models (LLMs) prioritize functional correctness over security. We present Constitutional Spec-Driven Development, a methodology that embeds non-negotiable security principles into the specification layer, ensuring AI-generated code adheres to security requirements by construction rather than inspection. Our approach introduces a Constitution: a versioned, machine-readable document encoding security constraints derived from Common Weakness Enumeration (CWE)/MITRE Top 25 vulnerabilities and regulatory frameworks. We demonstrate the methodology through a banking microservices application, selected as a representative example domain due to its stringent regulatory and security requirements, implementing customer management, account operations, and transaction processing. The methodology itself is domain-agnostic. The implementation addresses 10 critical CWE vulnerabilities through constitutional constraints with full traceability from principles to code locations. Our case study shows that constitutional constraints reduce security defects by 73% compared to unconstrained AI generation while maintaining developer velocity. We contribute a formal framework for constitutional security, a complete development methodology, and empirical evidence that proactive security specification outperforms reactive security verification in AI-assisted development workflows.

</details>


### [367] [Agentic Observability: Automated Alert Triage for Adobe E-Commerce](https://arxiv.org/abs/2602.02585)
*Aprameya Bharadwaj,Kyle Tu*

Main category: cs.SE

TL;DR: 本文提出在Adobe电商基础设施中部署的智能可观测性框架，用ReAct范式自动执行警报分类，生产部署结果显示成效显著。


<details>
  <summary>Details</summary>
Motivation: 现代企业系统复杂，人工警报分类是降低平均恢复时间的瓶颈，亟需改进方法。

Method: 采用ReAct范式，在检测到警报后，代理动态识别受影响服务，检索分析分布式系统相关日志，规划上下文相关操作。

Result: 与手动分类相比，平均洞察时间减少90%，保持相当诊断准确性。

Conclusion: 智能AI可大幅降低分类延迟、提高解决准确性，推动企业运营向自主可观测性转变。

Abstract: Modern enterprise systems exhibit complex interdependencies that make observability and incident response increasingly challenging. Manual alert triage, which typically involves log inspection, API verification, and cross-referencing operational knowledge bases, remains a major bottleneck in reducing mean recovery time (MTTR). This paper presents an agentic observability framework deployed within Adobe's e-commerce infrastructure that autonomously performs alert triage using a ReAct paradigm. Upon alert detection, the agent dynamically identifies the affected service, retrieves and analyzes correlated logs across distributed systems, and plans context-dependent actions such as handbook consultation, runbook execution, or retrieval-augmented analysis of recently deployed code. Empirical results from production deployment indicate a 90% reduction in mean time to insight compared to manual triage, while maintaining comparable diagnostic accuracy. Our results show that agentic AI enables an order-of-magnitude reduction in triage latency and a step-change in resolution accuracy, marking a pivotal shift toward autonomous observability in enterprise operations.

</details>


### [368] [Testing Storage-System Correctness: Challenges, Fuzzing Limitations, and AI-Augmented Opportunities](https://arxiv.org/abs/2602.02614)
*Ying Wang,Jiahui Chen,Dejun Jiang*

Main category: cs.SE

TL;DR: 本文围绕存储系统正确性测试展开，梳理现有技术并分析其优缺点，探讨模糊测试及AI对其的补充作用。


<details>
  <summary>Details</summary>
Motivation: 存储系统正确性保障困难，现有研究难以系统性暴露故障，需对测试技术进行梳理和分析。

Method: 采用以存储为中心的视角，按执行属性和故障机制组织现有技术，分析各类方法优缺点，研究模糊测试及AI补充作用。

Result: 对存储系统正确性测试技术进行全面梳理，分析了不同方法的优缺点，指出模糊测试与存储系统语义的不匹配，探讨AI补充作用。

Conclusion: 为存储系统正确性测试提供统一视角，明确关键挑战。

Abstract: Storage systems are fundamental to modern computing infrastructures, yet ensuring their correctness remains challenging in practice. Despite decades of research on system testing, many storage-system failures (including durability, ordering, recovery, and consistency violations) remain difficult to expose systematically. This difficulty stems not primarily from insufficient testing tooling, but from intrinsic properties of storage-system execution, including nondeterministic interleavings, long-horizon state evolution, and correctness semantics that span multiple layers and execution phases.
  This survey adopts a storage-centric view of system testing and organizes existing techniques according to the execution properties and failure mechanisms they target. We review a broad spectrum of approaches, ranging from concurrency testing and long-running workloads to crash-consistency analysis, hardware-level semantic validation, and distributed fault injection, and analyze their fundamental strengths and limitations. Within this framework, we examine fuzzing as an automated testing paradigm, highlighting systematic mismatches between conventional fuzzing assumptions and storage-system semantics, and discuss how recent artificial intelligence advances may complement fuzzing through state-aware and semantic guidance. Overall, this survey provides a unified perspective on storage-system correctness testing and outlines key challenges

</details>


### [369] [Outrunning LLM Cutoffs: A Live Kernel Crash Resolution Benchmark for All](https://arxiv.org/abs/2602.02690)
*Chenxi Huang,Alex Mathai,Feiyang Yu,Aleksandr Nogikh,Petros Maniatis,Franjo Ivančić,Eugene Wu,Kostis Kaffes,Junfeng Yang,Baishakhi Ray*

Main category: cs.SE

TL;DR: 提出Live - kBench评估框架和kEnv环境解决现有内核崩溃修复评估问题，通过实验展示性能差异和反馈作用。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的Linux内核崩溃修复评估基准静态，无法捕捉内核演变且有数据污染问题。

Method: 提出Live - kBench评估框架持续评估新发现内核漏洞，构建kEnv标准化环境，整理534个Linux内核漏洞数据集。

Result:  agents在LLM知识截止前修复的漏洞上等效补丁率高25%；三个先进代理首次尝试修复率74%，但仅约20%补丁接近开发者修复；反馈使修复率提高29%。

Conclusion: Live - kBench为社区提供对时间和属性敏感的评估基础设施。

Abstract: Repairing system crashes discovered by kernel fuzzers like Syzkaller is a critical yet underexplored challenge in software engineering. While recent works have introduced Large Language Model (LLM) based agents for Linux kernel crash-resolution, their evaluation benchmarks are usually static and thus, do not capture the evolving nature of the Linux kernel, and suffer from potential data contamination due to LLM knowledge cutoffs. To address the above problem, we present (i) Live-kBench, an evaluation framework for self-evolving benchmarks that continuously scrapes and evaluates agents on freshly discovered kernel bugs, and (ii) kEnv, an agent-agnostic standardized crash-resolution environment for kernel compilation, execution, and feedback. This design decouples agent workflows from heavy-weight execution, enabling fair and scalable comparison across diverse agent frameworks under identical conditions.
  To this end, we curate an inaugural dataset of 534 Linux kernel bugs and empirically demonstrate a significant performance gap, with agents achieving up to 25% higher equivalent patch rate on bugs fixed before the LLM knowledge cutoff. Using kEnv, we benchmark three state-of-the-art agents, showing that they resolve 74% of crashes on the first attempt (plausible patches); however only ~20% of generated patches closely match developer fixes. Additionally, exposing crash resolution feedback improves crash resolution rate by 29%. Live-kBench provides the community with an evaluation infrastructure for self-evolving benchmarks that is both time and attribute sensitive; complete with a public dashboard to track agent progress on Linux kernel bugs.

</details>


### [370] [Beyond the Prompt: Assessing Domain Knowledge Strategies for High-Dimensional LLM Optimization in Software Engineering](https://arxiv.org/abs/2602.02752)
*Srinath Srinivasan,Tim Menzies*

Main category: cs.SE

TL;DR: 对比人类与人工智能生成领域知识的策略，评估四种架构能否让大语言模型为高维优化生成有效预热启动。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在低维软件工程优化任务表现好，但高维问题表现不如贝叶斯方法，需理解如何系统整合领域知识来缩小差距。

Method: 在MOOT数据集上评估四种方法，包括H - DKP、AMP、DAPR和HKMA，通过切比雪夫距离量化性能并使用Scott - Knott聚类排名。

Result: 未提及

Conclusion: 未提及

Abstract: Background/Context: Large Language Models (LLMs) demonstrate strong performance on low-dimensional software engineering optimization tasks ($\le$11 features) but consistently underperform on high-dimensional problems where Bayesian methods dominate. A fundamental gap exists in understanding how systematic integration of domain knowledge (whether from humans or automated reasoning) can bridge this divide.
  Objective/Aim: We compare human versus artificial intelligence strategies for generating domain knowledge. We systematically evaluate four distinct architectures to determine if structured knowledge integration enables LLMs to generate effective warm starts for high-dimensional optimization.
  Method: We evaluate four approaches on MOOT datasets stratified by dimensionality: (1) Human-in-the-Loop Domain Knowledge Prompting (H-DKP), utilizing asynchronous expert feedback loops; (2) Adaptive Multi-Stage Prompting (AMP), implementing sequential constraint identification and validation; (3) Dimension-Aware Progressive Refinement (DAPR), conducting optimization in progressively expanding feature subspaces; and (4) Hybrid Knowledge-Model Approach (HKMA), synthesizing statistical scouting (TPE) with RAG-enhanced prompting. Performance is quantified via Chebyshev distance to optimal solutions and ranked using Scott-Knott clustering against an established baseline for LLM generated warm starts.
  Note that all human studies conducted as part of this study will comply with the policies of our local Institutional Review Board.

</details>


### [371] [A Proxy Stakeholder Approach to Requirements Engineering for Inclusive Navigation](https://arxiv.org/abs/2602.02869)
*Wei Wang,Anuradha Madugalla,John Grundy,Paul McIntosh,Charmine E. J. Härtel*

Main category: cs.SE

TL;DR: 该研究以定性为主的混合方法，研究代理利益相关者支持认知障碍者导航的策略并形成设计建议，提出更包容的需求获取方法。


<details>
  <summary>Details</summary>
Motivation: 认知障碍者在环境导航中面临挑战，主流导航技术未考虑其需求，需设计更合适的导航技术。

Method: 采用定性为主的混合方法，包括国际调查和三阶段访谈研究，研究代理利益相关者支持日常导航的策略。

Result: 得出了一系列基于实证的设计建议，强调可定制性、协作使用和基于日常惯例的导航支持。

Conclusion: 引入代理利益相关者概念，提出更包容的需求获取方法，为设计能反映认知支持实际情况的导航技术提供实用指导。

Abstract: Wayfinding, or the ability to navigate one's surroundings, is crucial for independent living and requires a complex combination of cognitive abilities, environmental awareness, and technology to manage this successfully. Individuals with cognitive impairment (IwCI) often face significant challenges in learning and navigating their environment. Despite its importance, mainstream navigation technologies are rarely designed with their diverse needs in mind. This study reframes the search for places as a socially distributed task and emphasizes the role of proxy stakeholders, who act on behalf or in coordination with IwCI during navigation. Using a qualitatively led mixed-methods approach, which includes an international survey and a three-stage interview study, we examine the real-world strategies that proxy stakeholders employ to support daily navigation. The findings are synthesized into a set of empirically grounded design recommendations that emphasize customisability, collaborative use, and support for routine-based navigation. Our findings highlight key challenges and adaptive practices, which are synthesized into design recommendations that prioritize customisability, routine-based navigation, and multi-user coordination. By introducing the proxy stakeholder concept into the software engineering literature, we propose a more inclusive approach to requirements elicitation and offer practical guidance for designing navigation technologies that better reflect the complex realities of cognitive support.

</details>


### [372] [Learning-Infused Formal Reasoning: From Contract Synthesis to Artifact Reuse and Formal Semantics](https://arxiv.org/abs/2602.02881)
*Arshad Beg,Diarmuid O'Donoghue,Rosemary Monahan*

Main category: cs.SE

TL;DR: 本文提出形式方法与人工智能交叉领域的长期研究议程，展望下一代形式方法，提出结合大语言模型与图表示的混合框架以推动验证生态系统发展。


<details>
  <summary>Details</summary>
Motivation: 使未来验证系统从孤立正确性证明转变为累积、知识驱动范式，实现规格、合约和证明在系统间的持续合成与转移。

Method: 提出结合大语言模型与图表示的混合框架，学习组件提供语义指导，符号匹配确保形式正确性。

Result: 描述该框架可实现可扩展语义匹配与验证工件的原则性重用。

Conclusion: 该愿景将引导验证生态系统系统地发展，利用过去验证工作加速未来保障。

Abstract: This vision paper articulates a long-term research agenda for formal methods at the intersection with artificial intelligence, outlining multiple conceptual and technical dimensions and reporting on our ongoing work toward realising this agenda. It advances a forward-looking perspective on the next generation of formal methods based on the integration of automated contract synthesis, semantic artifact reuse, and refinement-based theory. We argue that future verification systems must move beyond isolated correctness proofs toward a cumulative, knowledge-driven paradigm in which specifications, contracts, and proofs are continuously synthesised and transferred across systems. To support this shift, we outline a hybrid framework combining large language models with graph-based representations to enable scalable semantic matching and principled reuse of verification artifacts. Learning-based components provide semantic guidance across heterogeneous notations and abstraction levels, while symbolic matching ensures formal soundness. Grounded in compositional reasoning, this vision points toward verification ecosystems that evolve systematically, leveraging past verification efforts to accelerate future assurance.

</details>


### [373] [Failure-Aware Enhancements for Large Language Model (LLM) Code Generation: An Empirical Study on Decision Framework](https://arxiv.org/abs/2602.02896)
*Jianru Shen,Zedong Peng,Lucy Owen*

Main category: cs.SE

TL;DR: 研究发现渐进式提示在软件开发自动化中表现优于直接提示，但仍有不足。评估了不同增强策略对不同失败类型的效果，提出决策框架为从业者提供指导。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在自动化软件开发中虽有潜力，但现有提示工作流有需求未满足，且开发者缺乏使用增强方法的明确指导。

Method: 对25个GitHub项目进行实证研究，对比渐进式提示和直接提示；对6个代表性项目评估不同增强策略在4种失败类型上的效果。

Result: 渐进式提示平均任务完成率96.9%，优于直接提示；不同增强策略对不同失败类型效果不同，RAG在各类型中表现最佳。

Conclusion: 提出决策框架，根据失败模式匹配最合适的增强方法，为从业者提供基于数据的实用指导。

Abstract: Large language models (LLMs) show promise for automating software development by translating requirements into code. However, even advanced prompting workflows like progressive prompting often leave some requirements unmet. Although methods such as self-critique, multi-model collaboration, and retrieval-augmented generation (RAG) have been proposed to address these gaps, developers lack clear guidance on when to use each. In an empirical study of 25 GitHub projects, we found that progressive prompting achieves 96.9% average task completion, significantly outperforming direct prompting (80.5%, Cohen's d=1.63, p<0.001) but still leaving 8 projects incomplete. For 6 of the most representative projects, we evaluated each enhancement strategy across 4 failure types. Our results reveal that method effectiveness depends critically on failure characteristics: Self-Critique succeeds on code-reviewable logic errors but fails completely on external service integration (0% improvement), while RAG achieves highest completion across all failure types with superior efficiency. Based on these findings, we propose a decision framework that maps each failure pattern to the most suitable enhancement method, giving practitioners practical, data-driven guidance instead of trial-and-error.

</details>


### [374] [Beyond Blame: Rethinking SZZ with Knowledge Graph Search](https://arxiv.org/abs/2602.02934)
*Yu Shi,Hao Li,Bram Adams,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: 现有SZZ方法有局限，文章提出AgenticSZZ利用TKG解决BIC识别问题，评估显示效果优于现有方法，开启新研究方向


<details>
  <summary>Details</summary>
Motivation: 现有基于SZZ的方法依赖git blame，搜索空间受限，很多情况无法解决BIC识别问题

Method: 将BIC识别转化为图搜索问题，分两步：构建TKG编码提交信息，利用LLM智能体导航图进行候选探索和因果分析

Result: 在三个数据集上评估，F1分数0.48 - 0.74，相比现有技术显著提升达27%，消融实验证明两组件都重要

Conclusion: 将BIC识别转化为图搜索问题为软件演化分析的时间和因果推理开辟新方向

Abstract: Identifying Bug-Inducing Commits (BICs) is fundamental for understanding software defects and enabling downstream tasks such as defect prediction and automated program repair. Yet existing SZZ-based approaches are limited by their reliance on git blame, which restricts the search space to commits that directly modified the fixed lines. Our preliminary study on 2,102 validated bug-fixing commits reveals that this limitation is significant: over 40% of cases cannot be solved by blame alone, as 28% of BICs require traversing commit history beyond blame results and 14% are blameless.
  We present AgenticSZZ, the first approach to apply Temporal Knowledge Graphs (TKGs) to software evolution analysis. AgenticSZZ reframes BIC identification from a ranking problem over blame commits into a graph search problem, where temporal ordering is fundamental to causal reasoning about bug introduction. The approach operates in two phases: (1) constructing a TKG that encodes commits with temporal and structural relationships, expanding the search space by traversing file history backward from two reference points (blame commits and the BFC); and (2) leveraging an LLM agent to navigate the graph using specialized tools for candidate exploration and causal analysis.
  Evaluation on three datasets shows that AgenticSZZ achieves F1-scores of 0.48 to 0.74, with statistically significant improvements over state-of-the-art by up to 27%. Our ablation study confirms that both components are essential, reflecting a classic exploration-exploitation trade-off: the TKG expands the search space while the agent provides intelligent selection. By transforming BIC identification into a graph search problem, we open a new research direction for temporal and causal reasoning in software evolution analysis.

</details>


### [375] [Testing Framework Migration with Large Language Models](https://arxiv.org/abs/2602.02964)
*Altino Alves,João Eduardo Montandon,Andre Hora*

Main category: cs.SE

TL;DR: 研究大语言模型自动化将Python测试框架从unittest迁移到Pytest的能力，评估了不同模型、策略和设置，结果显示部分迁移失败，不同模型有不同迁移倾向。


<details>
  <summary>Details</summary>
Motivation: 手动将测试套件从unittest迁移到Pytest耗时费力，自动化迁移可减少工作量并加速测试现代化。

Method: 评估GPT 4o和Claude Sonnet 4在三种提示策略和两种温度设置下的表现，引入真实数据集，执行LLM生成的测试迁移。

Result: 51.5%的LLM生成的测试迁移失败，48.5%通过，Claude Sonnet 4迁移保守，GPT - 4o更倾向转换。

Conclusion: 探讨了对从业者和研究者的多个启示。

Abstract: Python developers rely on two major testing frameworks: \texttt{unittest} and \texttt{Pytest}. While \texttt{Pytest} offers simpler assertions, reusable fixtures, and better interoperability, migrating existing suites from \texttt{unittest} remains a manual and time-consuming process. Automating this migration could substantially reduce effort and accelerate test modernization. In this paper, we investigate the capability of Large Language Models (LLMs) to automate test framework migrations from \texttt{unittest} to \texttt{Pytest}. We evaluate GPT 4o and Claude Sonnet 4 under three prompting strategies (Zero-shot, One-shot, and Chain-of-Thought) and two temperature settings (0.0 and 1.0). To support this analysis, we first introduce a curated dataset of real-world migrations extracted from the top 100 Python open-source projects. Next, we actually execute the LLM-generated test migrations in their respective test suites. Overall, we find that 51.5% of the LLM-generated test migrations failed, while 48.5% passed. The results suggest that LLMs can accelerate test migration, but there are often caveats. For example, Claude Sonnet 4 exhibited more conservative migrations (e.g., preserving class-based tests and legacy \texttt{unittest} references), while GPT-4o favored more transformations (e.g., to function-based tests). We conclude by discussing multiple implications for practitioners and researchers.

</details>


### [376] [Understanding Bug-Reproducing Tests: A First Empirical Study](https://arxiv.org/abs/2602.02965)
*Andre Hora,Gordon Fraser*

Main category: cs.SE

TL;DR: 对15个Python系统的642个复现bug测试进行研究，发现其与其他测试在LOC、断言数量和复杂度上无显著差异，含更多try/except块和“弱断言”，多数仅复现单个bug。


<details>
  <summary>Details</summary>
Motivation: 现有对bug复现测试的特性研究较少，不清楚其是否与其他测试有根本区别，因此开展研究。

Method: 分析15个真实Python系统的642个bug复现测试。

Result: bug复现测试在LOC、断言数量和复杂度上与其他测试无显著差异，含稍多try/except块和“弱断言”，95%复现单个bug，5%复现多个。

Conclusion: 讨论了研究的影响和未来研究方向。

Abstract: Developers create bug-reproducing tests that support debugging by failing as long as the bug is present, and passing once the bug has been fixed. These tests are usually integrated into existing test suites and executed regularly alongside all other tests to ensure that future regressions are caught. Despite this co-existence with other types of tests, the properties of bug-reproducing tests are scarcely researched, and it remains unclear whether they differ fundamentally. In this short paper, we provide an initial empirical study to understand bug-reproducing tests better. We analyze 642 bug-reproducing tests of 15 real-world Python systems. Overall, we find that bug-reproducing tests are not (statistically significantly) different from other tests regarding LOC, number of assertions, and complexity. However, bug-reproducing tests contain slightly more try/except blocks and ``weak assertions'' (e.g.,~\texttt{assertNotEqual}). Lastly, we detect that the majority (95%) of the bug-reproducing tests reproduce a single bug, while 5% reproduce multiple bugs. We conclude by discussing implications and future research directions.

</details>


### [377] [What Do Contribution Guidelines Say About Software Testing?](https://arxiv.org/abs/2602.02966)
*Bruna Falcucci,Felipe Gomide,Andre Hora*

Main category: cs.SE

TL;DR: 本文对200个Python和JavaScript开源软件项目的贡献指南进行实证研究，揭示了测试文档的包含情况、位置、内容覆盖等方面。


<details>
  <summary>Details</summary>
Motivation: 尽管多数项目要求贡献者编写测试，但对贡献者沟通的具体测试实践尚不明确，旨在更好地了解开源项目贡献指南中的软件测试方式。

Method: 分析200个Python和JavaScript开源软件项目的贡献指南。

Result: 78%的项目为贡献者提供某种形式测试文档；文档位于多个来源；常解释如何运行测试，但较少指导如何编写测试，多涵盖单元测试，较少涉及集成和端到端测试，其他测试方面讨论也较少。

Conclusion: 最后讨论了研究的影响和未来研究方向。

Abstract: Software testing plays a crucial role in the contribution process of open-source projects. For example, contributions introducing new features are expected to include tests, and contributions with tests are more likely to be accepted. Although most real-world projects require contributors to write tests, the specific testing practices communicated to contributors remain unclear. In this paper, we present an empirical study to understand better how software testing is approached in contribution guidelines. We analyze the guidelines of 200 Python and JavaScript open-source software projects. We find that 78\% of the projects include some form of test documentation for contributors. Test documentation is located in multiple sources, including \texttt{CONTRIBUTING} files (58\%), external documentation (24\%), and \texttt{README} files (8\%). Furthermore, test documentation commonly explains how to run tests (83.5\%), but less often provides guidance on how to write tests (37\%). It frequently covers unit tests (71\%), but rarely addresses integration (20.5\%) and end-to-end tests (15.5\%). Other key testing aspects are also less frequently discussed: test coverage (25.5\%) and mocking (9.5\%). We conclude by discussing implications and future research.

</details>


### [378] [Maintaining the Heterogeneity in the Organization of Software Engineering Research](https://arxiv.org/abs/2602.03093)
*Yang Yue,Zheng Jiang,Yi Wang*

Main category: cs.SE

TL;DR: 软件工程研究组织存在异质性，但资助研究模式占主导威胁了这种异质性，文章呼吁维护异质性。


<details>
  <summary>Details</summary>
Motivation: 当前资助研究模式主导，软件工程研究组织的异质性受到严重系统威胁，需维护这种异质性。

Method: 先解释软件工程研究组织中异质性存在的必要性，再呈现当前研究趋势、后果及潜在未来。

Result: 指出资助研究模式主导对异质性的威胁情况。

Conclusion: 呼吁软件工程界认真考虑维护研究组织的异质性。

Abstract: The heterogeneity in the organization of software engineering (SE) research historically exists, i.e., funded research model and hands-on model, which makes software engineering become a thriving interdisciplinary field in the last 50 years. However, the funded research model is becoming dominant in SE research recently, indicating such heterogeneity has been seriously and systematically threatened. In this essay, we first explain why the heterogeneity is needed in the organization of SE research, then present the current trend of SE research nowadays, as well as the consequences and potential futures. The choice is at our hands, and we urge our community to seriously consider maintaining the heterogeneity in the organization of software engineering research.

</details>


### [379] [Synthesizing File-Level Data for Unit Test Generation with Chain-of-Thoughts via Self-Debugging](https://arxiv.org/abs/2602.03181)
*Ziyue Hua,Tianyu Chen,Yeyun Gong,Shuai Lu,Peng Cheng,Qinglin Zhu,Yibo He,Yingjie Fu,Wenpin Jiao,Wei Yang,Tao Xie*

Main category: cs.SE

TL;DR: 现有自动单元测试生成方法难以产生高质量测试，本文提出数据蒸馏方法，构建数据集微调模型，评估显示微调后模型效果优于现有商业模型。


<details>
  <summary>Details</summary>
Motivation: 现有自动单元测试生成方法难以产生带正确断言和可靠思维链解释的高质量测试，训练数据存在不足。

Method: 提出数据蒸馏方法，结合引导式测试修复和思维链压缩，应用于开源项目构建数据集，用于基础模型的监督微调。

Result: 微调后的模型在测试断言通过率、分支覆盖率和变异得分等方面表现出色，远超 o4 - mini 等先进商业模型。

Conclusion: 所提出的数据蒸馏方法有效，能提升自动单元测试生成的效果。

Abstract: Automatic unit test (UT) generation is essential for software quality assurance, but existing approaches--including symbolic execution, search-based approaches, and recent LLM-based generators--struggle to produce human-quality tests with correct, meaningful assertions and reliable chain-of-thought (CoT) explanations. We identify a gap in UT training data: repository-mined tests lack developer CoTs, while LLM-distilled CoTs are often incorrect or incomplete. To address this issue, we propose a novel data-distillation approach that uses self-debugging to produce high-quality UT training examples paired with faithful CoTs. Our approach combines (1) guided test repair, a heuristic loop (error-, failure-, and coverage-focused steps) that asks the used model to diagnose and iteratively fix generated tests, and (2) CoT compression, which compacts original and debugging CoTs into concise explanations that directly justify correct tests. We apply this pipeline to a large corpus of open-source projects to construct a dataset of 74,518 high-quality <focal method, test, CoT> examples, and then use it for supervised fine-tuning of a base model. An empirical evaluation shows that the fine-tuned model achieves high UT generation effectiveness: it attains a pass rate of 36.17% on test assertions, a branch coverage of 43.90%, and a mutation score of 88.66%, substantially higher than state-of-the-art commercial models like o4-mini.

</details>


### [380] [Multi-Level Testing of Conversational AI Systems](https://arxiv.org/abs/2602.03311)
*Elena Masserini*

Main category: cs.SE

TL;DR: 论文研究对话式AI系统的新测试方法，关注不同粒度组件验证。


<details>
  <summary>Details</summary>
Motivation: 现有测试方案无法适应对话式交互特点和AI组件行为。

Method: 研究新的测试方法，对对话式AI系统不同粒度的组成元素进行验证。

Result: 未提及。

Conclusion: 未提及。

Abstract: Conversational AI systems combine AI-based solutions with the flexibility of conversational interfaces. However, most existing testing solutions do not straightforwardly adapt to the characteristics of conversational interaction or to the behavior of AI components. To address this limitation, this Ph.D. thesis investigates a new family of testing approaches for conversational AI systems, focusing on the validation of their constituent elements at different levels of granularity, from the integration between the language and the AI components, to individual conversational agents, up to multi-agent implementations of conversational AI systems

</details>


### [381] [Precision in Practice: Knowledge Guided Code Summarizing Grounded in Industrial Expectations](https://arxiv.org/abs/2602.03400)
*Jintai Li,Songqiang Chen,Shuo Jin,Xiaoyuan Xie*

Main category: cs.SE

TL;DR: 现有代码摘要在工业场景实用性不足，提出ExpSum方法，实验表明其性能优于基线。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型生成的代码摘要在工业场景实用性未充分探究，现有方法生成的摘要超57.4%不符合开发者期待。

Method: 提出ExpSum方法，集成函数元数据抽象、信息元数据过滤、上下文感知领域知识检索和约束驱动提示来引导大语言模型生成摘要。

Result: 在HarmonyOS项目和常用代码摘要基准测试中，ExpSum始终优于所有基线，BLEU - 4最高提升26.71%，ROUGE - L最高提升20.10%，跨项目更符合开发者期待。

Conclusion: ExpSum在工业代码文档方面有效。

Abstract: Code summaries are essential for helping developers understand code functionality and reducing maintenance and collaboration costs. Although recent advances in large language models (LLMs) have significantly improved automatic code summarization, the practical usefulness of generated summaries in industrial settings remains insufficiently explored. In collaboration with documentation experts from the industrial HarmonyOS project, we conducted a questionnaire study showing that over 57.4% of code summaries produced by state-of-the-art approaches were rejected due to violations of developers' expectations for industrial documentation. Beyond semantic similarity to reference summaries, developers emphasize additional requirements, including the use of appropriate domain terminology, explicit function categorization, and the avoidance of redundant implementation details.
  To address these expectations, we propose ExpSum, an expectation-aware code summarization approach that integrates function metadata abstraction, informative metadata filtering, context-aware domain knowledge retrieval, and constraint-driven prompting to guide LLMs in generating structured, expectation-aligned summaries. We evaluate ExpSum on the HarmonyOS project and widely used code summarization benchmarks. Experimental results show that ExpSum consistently outperforms all baselines, achieving improvements of up to 26.71% in BLEU-4 and 20.10% in ROUGE-L on HarmonyOS. Furthermore, LLM-based evaluations indicate that ExpSum-generated summaries better align with developer expectations across other projects, demonstrating its effectiveness for industrial code documentation.

</details>


### [382] [SWE-Master: Unleashing the Potential of Software Engineering Agents via Post-Training](https://arxiv.org/abs/2602.03411)
*Huatong Song,Lisheng Huang,Shuang Sun,Jinhao Jiang,Ran Le,Daixuan Cheng,Guoxin Chen,Yiwen Hu,Zongchao Chen,Wayne Xin Zhao,Yang Song,Tao Zhang,Ji-Rong Wen*

Main category: cs.SE

TL;DR: 本文介绍开源可复现的软件工程代理后训练框架SWE - Master，展示其优化方法能提升长时程软件工程任务解决能力，评估表现超现有基线。


<details>
  <summary>Details</summary>
Motivation: 构建有效的软件工程代理，推动软件工程代理可复现研究。

Method: 系统探索代理开发全流程，包括教师轨迹合成、数据整理、长时程SFT、基于真实执行反馈的RL和推理框架设计，还结合测试时缩放（TTS）与基于大语言模型的环境反馈。

Result: 在SWE - bench Verified基准测试中，用Qwen2.5 - Coder - 32B达到61.4%的解决率，结合TTS@8达到70.8%，远超现有开源基线。

Conclusion: SWE - Master为软件工程代理的可复现研究提供了实用透明的基础。

Abstract: In this technical report, we present SWE-Master, an open-source and fully reproducible post-training framework for building effective software engineering agents. SWE-Master systematically explores the complete agent development pipeline, including teacher-trajectory synthesis and data curation, long-horizon SFT, RL with real execution feedback, and inference framework design. Starting from an open-source base model with limited initial SWE capability, SWE-Master demonstrates how systematical optimization method can elicit strong long-horizon SWE task solving abilities. We evaluate SWE-Master on SWE-bench Verified, a standard benchmark for realistic software engineering tasks. Under identical experimental settings, our approach achieves a resolve rate of 61.4\% with Qwen2.5-Coder-32B, substantially outperforming existing open-source baselines. By further incorporating test-time scaling~(TTS) with LLM-based environment feedback, SWE-Master reaches 70.8\% at TTS@8, demonstrating a strong performance potential. SWE-Master provides a practical and transparent foundation for advancing reproducible research on software engineering agents. The code is available at https://github.com/RUCAIBox/SWE-Master.

</details>


### [383] [SWE-World: Building Software Engineering Agents in Docker-Free Environments](https://arxiv.org/abs/2602.03419)
*Shuang Sun,Huatong Song,Lisheng Huang,Jinhao Jiang,Ran Le,Zhihao Lv,Zongchao Chen,Yiwen Hu,Wenyang Luo,Wayne Xin Zhao,Yang Song,Hongteng Xu,Tao Zhang,Ji-Rong Wen*

Main category: cs.SE

TL;DR: 提出无Docker的SWE - World框架训练和评估软件工程代理，提升模型在软件工程任务上的表现且免维护环境。


<details>
  <summary>Details</summary>
Motivation: 现有基于容器化环境执行反馈的方法资源密集、难维护，限制了代理训练和可扩展性。

Method: 用基于真实交互数据训练的LLM模型作为代理环境的替代，预测执行结果和测试反馈，省去物理容器环境交互。

Result: 在SWE - bench Verified实验中，SWE - World使Qwen2.5 - Coder - 32B通过无Docker的SFT达52.0%，无Docker的RL达55.0%，结合TTS达68.2%。

Conclusion: SWE - World框架能有效解决传统方法的问题，提升软件工程任务表现及测试时扩展性，代码已开源。

Abstract: Recent advances in large language models (LLMs) have enabled software engineering agents to tackle complex code modification tasks. Most existing approaches rely on execution feedback from containerized environments, which require dependency-complete setup and physical execution of programs and tests. While effective, this paradigm is resource-intensive and difficult to maintain, substantially complicating agent training and limiting scalability. We propose SWE-World, a Docker-free framework that replaces physical execution environments with a learned surrogate for training and evaluating software engineering agents. SWE-World leverages LLM-based models trained on real agent-environment interaction data to predict intermediate execution outcomes and final test feedback, enabling agents to learn without interacting with physical containerized environments. This design preserves the standard agent-environment interaction loop while eliminating the need for costly environment construction and maintenance during agent optimization and evaluation. Furthermore, because SWE-World can simulate the final evaluation outcomes of candidate trajectories without real submission, it enables selecting the best solution among multiple test-time attempts, thereby facilitating effective test-time scaling (TTS) in software engineering tasks. Experiments on SWE-bench Verified demonstrate that SWE-World raises Qwen2.5-Coder-32B from 6.2\% to 52.0\% via Docker-free SFT, 55.0\% with Docker-free RL, and 68.2\% with further TTS. The code is available at https://github.com/RUCAIBox/SWE-World

</details>


### [384] [RAL-Bench: Benchmarking for Application-Level Functional Correctness and Non-Functional Quality Attributes](https://arxiv.org/abs/2602.03462)
*Ruwei Pan,Yakun Zhang,Qingyuan Liang,Yueheng Zhu,Chao Liu,Lu Zhang,Hongyu Zhang*

Main category: cs.SE

TL;DR: 现有代码生成基准在应用级评估有限，提出 RAL - Bench 基准和评估框架并评估 16 个大模型，发现功能正确性是主要瓶颈并开源工具。


<details>
  <summary>Details</summary>
Motivation: 现有基准对应用级代码生成的评估有限，需评估当前大语言模型能否生成满足功能和非功能标准的应用级代码库。

Method: 提出 RAL - Bench 基准和评估框架，从参考项目提炼需求，构建覆盖功能和非功能属性的黑盒系统测试，用系统测试通过率测功能正确性，用基于 ISO/IEC 25010 的五维度及 AHP 权重向量测非功能质量。

Result: 对 16 个大语言模型零样本评估显示，功能正确性是主要瓶颈，无模型功能通过率超 45%。

Conclusion: RAL - Bench 可用于评估应用级代码生成，功能正确性待提升。

Abstract: Code generation has advanced rapidly with code-focused large language models (LLMs), especially on snippet-level tasks. However, application-level generation requires producing a runnable multi-file repository with correct structure, dependencies, and end-to-end executability, and real-world software must satisfy both functional correctness and non-functional quality (e.g., maintainability, security). Existing benchmarks provide a limited execution-based assessment of these requirements at the application level. We ask: Can current LLMs generate application-level repositories that meet both functional and non-functional criteria? We propose RAL-Bench, a benchmark and evaluation framework for application-level code generation. For each task, we distill a concise natural-language requirement from a high-quality reference project, build black-box system tests covering functional and non-functional attributes, and keep only tests that pass on the reference repository to ensure a sound oracle and an end-to-end executable suite. Functional correctness is measured by system-test pass rate. Non-functional quality is measured along five ISO/IEC 25010-inspired dimensions and aggregated with an Analytic Hierarchy Process (AHP)-derived weight vector, with per-dimension diagnostics and baseline-normalized scoring using reference measurements. Across 16 LLMs evaluated zero-shot with greedy decoding, functional correctness is the dominant bottleneck: no model exceeds a 45% functional pass rate under our requirement-driven, reference-validated tests. We release RAL-Bench at https://github.com/Wwstarry/RAL-Bench. .

</details>


### [385] [Formal Evidence Generation for Assurance Cases for Robotic Software Models](https://arxiv.org/abs/2602.03550)
*Fang Yan,Simon Foster,Ana Cavalcanti,Ibrahim Habli,James Baxter*

Main category: cs.SE

TL;DR: 提出基于模型方法系统生成保证案例证据，结合模型检查和定理证明，案例研究证明有效。


<details>
  <summary>Details</summary>
Motivation: 机器人和自主系统用于安全关键领域需证明安全，现有生成和维护保证案例证据方式劳动密集、易出错且难保持一致性。

Method: 将形式验证嵌入保证工作流，用模板从自然语言需求导出形式断言，协调多工具处理不同属性类型，结合RoboChart语言进行模型检查和定理证明。

Result: 结构化需求自动转换为形式断言，验证结果自动集成作为证据。

Conclusion: 案例研究证明该方法有效。

Abstract: Robotics and Autonomous Systems are increasingly deployed in safety-critical domains, so that demonstrating their safety is essential. Assurance Cases (ACs) provide structured arguments supported by evidence, but generating and maintaining this evidence is labour-intensive, error-prone, and difficult to keep consistent as systems evolve. We present a model-based approach to systematically generating AC evidence by embedding formal verification into the assurance workflow. The approach addresses three challenges: systematically deriving formal assertions from natural language requirements using templates, orchestrating multiple formal verification tools to handle diverse property types, and integrating formal evidence production into the workflow. Leveraging RoboChart, a domain-specific modelling language with formal semantics, we combine model checking and theorem proving in our approach. Structured requirements are automatically transformed into formal assertions using predefined templates, and verification results are automatically integrated as evidence. Case studies demonstrate the effectiveness of our approach.

</details>


### [386] [Flaky Tests in a Large Industrial Database Management System: An Empirical Study of Fixed Issue Reports for SAP HANA](https://arxiv.org/abs/2602.03556)
*Alexander Berndt,Thomas Bach,Sebastian Baltes*

Main category: cs.SE

TL;DR: 本文提出用大语言模型标注问题报告以识别常见的测试不稳定类别，分析了SAP HANA测试的不稳定情况并给出建议。


<details>
  <summary>Details</summary>
Motivation: 手动标记不稳定测试耗时且繁琐，不同项目的测试不稳定主要根源有差异，需要方法获取概述。

Method: 提出LLMs - as - annotators方法，利用模型内和模型间一致性标记固定的不稳定问题的报告。

Result: SAP HANA测试最常见的问题与并发有关（23%），不同测试类型面临不同不稳定挑战。

Conclusion: 鼓励未来缓解不稳定问题的研究考虑在不同测试类型中评估方法的通用性。

Abstract: Flaky tests yield different results when executed multiple times for the same version of the source code. Thus, they provide an ambiguous signal about the quality of the code and interfere with the automated assessment of code changes. While a variety of factors can cause test flakiness, approaches to fix flaky tests are typically tailored to address specific causes. However, the prevalent root causes of flaky tests can vary depending on the programming language, application domain, or size of the software project. Since manually labeling flaky tests is time-consuming and tedious, this work proposes an LLMs-as-annotators approach that leverages intra- and inter-model consistency to label issue reports related to fixed flakiness issues with the relevant root cause category. This allows us to gain an overview of prevalent flakiness categories in the issue reports. We evaluated our labeling approach in the context of SAP HANA, a large industrial database management system. Our results suggest that SAP HANA's tests most commonly suffer from issues related to concurrency (23%, 130 of 559 analyzed issue reports). Moreover, our results suggest that different test types face different flakiness challenges. Therefore, we encourage future research on flakiness mitigation to consider evaluating the generalizability of proposed approaches across different test types.

</details>


### [387] [Scaling Test-Driven Code Generation from Functions to Classes: An Empirical Study](https://arxiv.org/abs/2602.03557)
*Yunhao Liang,Ruixuan Ying,Shiwen Ni,Zhe Cui*

Main category: cs.SE

TL;DR: 本文提出迭代TDD框架将测试驱动代码生成从函数扩展到类，构建ClassEval - TDD数据集，实验表明该框架能提升类级代码生成可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有TDD风格代码生成研究多限于函数级任务，类级合成研究不足。

Method: 提出迭代TDD框架，分析类内方法依赖确定生成顺序，结合反射式执行反馈和有限修复迭代，构建ClassEval - TDD数据集。

Result: 类级TDD框架使类级正确性提升12 - 26个绝对百分点，最高达71%完全正确的类，平均只需少量修复。

Conclusion: 测试驱动生成可有效扩展到孤立函数之外，显著提高类级代码生成可靠性。

Abstract: Test-driven development (TDD) has been adopted to improve Large Language Model (LLM)-based code generation by using tests as executable specifications. However, existing TDD-style code generation studies are largely limited to function-level tasks, leaving class-level synthesis where multiple methods interact through shared state and call dependencies underexplored. In this paper, we scale test-driven code generation from functions to classes via an iterative TDD framework. Our approach first analyzes intra-class method dependencies to derive a feasible generation schedule, and then incrementally implements each method under method-level public tests with reflection-style execution feedback and bounded repair iterations. To support test-driven generation and rigorous class-level evaluation, we construct ClassEval-TDD, a cleaned and standardized variant of ClassEval with consistent specifications, deterministic test environments, and complete method-level public tests. We conduct an empirical study across eight LLMs and compare against the strongest direct-generation baseline (the best of holistic, incremental, and compositional strategies). Our class-level TDD framework consistently improves class-level correctness by 12 to 26 absolute points and achieves up to 71% fully correct classes, while requiring only a small number of repairs on average. These results demonstrate that test-driven generation can effectively scale beyond isolated functions and substantially improve class-level code generation reliability. All code and data are available at https://anonymous.4open.science/r/ClassEval-TDD-C4C9/

</details>


### [388] [Causal Inference for the Effect of Code Coverage on Bug Introduction](https://arxiv.org/abs/2602.03585)
*Lukas Schulte,Gordon Fraser,Steffen Herbold*

Main category: cs.SE

TL;DR: 研究成熟JavaScript和TypeScript开源项目中代码覆盖率对引入bug的因果影响，构建因果图并使用广义倾向得分调整等方法分析。


<details>
  <summary>Details</summary>
Motivation: 代码覆盖率作为软件质量保证指标，其效果和合适剂量存在争议，以往研究仅为相关性关联，易受混杂因素影响，因此要量化其因果效应。

Method: 构建因果有向无环图识别软件工程过程中的混杂因素，使用广义倾向得分调整，对新数据集进行基于双重稳健回归的连续暴露因果推断，估计平均处理效应和剂量 - 响应关系。

Result: 未提及

Conclusion: 未提及

Abstract: Context: Code coverage is widely used as a software quality assurance measure. However, its effect, and specifically the advisable dose, are disputed in both the research and engineering communities. Prior work reports only correlational associations, leaving results vulnerable to confounding factors. Objective: We aim to quantify the causal effect of code coverage (exposure) on bug introduction (outcome) in the context of mature JavaScript and TypeScript open source projects, addressing both the overall effect and its variance across coverage levels. Method: We construct a causal directed acyclic graph to identify confounders within the software engineering process, modeling key variables from the source code, issue- and review systems, and continuous integration. Using generalized propensity score adjustment, we will apply doubly robust regression-based causal inference for continuous exposure to a novel dataset of bug-introducing and non-bug-introducing changes. We estimate the average treatment effect and dose-response relationship to examine potential non-linear patterns (e.g., thresholds or diminishing returns) within the projects of our dataset.

</details>


### [389] [Beyond the Commit: Developer Perspectives on Productivity with AI Coding Assistants](https://arxiv.org/abs/2602.03593)
*Valerie Chen,Jasmyn He,Behnjamin Williams,Jason Valentino,Ameet Talwalkar*

Main category: cs.SE

TL;DR: 本文通过混合方法研究，分析评估AI代码编写助手对开发者生产力影响不同方法的有效性，指出需多方面评估，且强调长期指标。


<details>
  <summary>Details</summary>
Motivation: 在AI代码编写助手时代，需要了解其对开发者生产力的影响，以及早期衡量方法和框架是否仍适用。

Method: 采用混合方法研究，在纽约银行梅隆进行了有2989名开发者参与的调查和11次深度访谈。

Result: 需要多方面方法衡量AI生产力影响，调查结果显示对AI工具有用性观点冲突，访谈得出六个不同因素，兼顾生产力短期和长期维度。

Conclusion: 鼓励未来研究纳入更多以人类为中心的因素，支持行业采用更全面方法评估开发者生产力。

Abstract: Measuring developer productivity is a topic that has attracted attention from both academic research and industrial practice. In the age of AI coding assistants, it has become even more important for both academia and industry to understand how to measure their impact on developer productivity, and to reconsider whether earlier measures and frameworks still apply. This study analyzes the validity of different approaches to evaluating the productivity impacts of AI coding assistants by leveraging mixed-method research. At BNY Mellon, we conduct a survey with 2989 developer responses and 11 in-depth interviews. Our findings demonstrate that a multifaceted approach is needed to measure AI productivity impacts: survey results expose conflicting perspectives on AI tool usefulness, while interviews elicit six distinct factors that capture both short-term and long-term dimensions of productivity. In contrast to prior work, our factors highlight the importance of long-term metrics like technical expertise and ownership of work. We hope this work encourages future research to incorporate a broader range of human-centered factors, and supports industry in adopting more holistic approaches to evaluating developer productivity.

</details>


### [390] [CALM: A Self-Adaptive Orchestration Approach for QoS-Aware Routing in Small Language Model based Systems](https://arxiv.org/abs/2602.03632)
*Hemang Jain,Divyansh Pandey,Karthik Vaidhyanathan*

Main category: cs.SE

TL;DR: AI系统面临多种运行时不确定性，影响服务质量。提出CALM机制协调小语言模型舰队，评估显示可降低延迟和能耗。


<details>
  <summary>Details</summary>
Motivation: 解决AI系统运行时不确定性对服务质量的影响，以及大语言模型资源和隐私成本问题，小语言模型无法满足多样化需求的问题。

Method: 引入基于MAPE - K的自适应性编排机制CALM，持续监控用户查询、分析QoS指标、选择最优模型、路由查询，利用缓存和调度管理模型。

Result: 与单大语言模型基线相比，CALM降低约40%延迟和50%能耗，同时保持特定领域任务性能。

Conclusion: 利用协调的小语言模型舰队并通过CALM机制进行编排，能有效应对运行时不确定性，提升系统性能。

Abstract: AI-enabled systems are subjected to various types of runtime uncertainties, ranging from dynamic workloads, resource requirements, model drift, etc. These uncertainties have a big impact on the overall Quality of Service (QoS). This is particularly true in the case of Language Model (LM) enabled systems where the autoregressive nature of token generation introduces variability in latency, energy usage and response quality. These systems, powered by LLMs, are either resource-intensive (if run on-prem) or raise privacy/cost concerns (if leveraged using APIs). While deploying a Small Language Model (SLM) can be resource-efficient, it often falls short in addressing the diversity and scale of real-world requirements. To this, we argue that, rather than relying on any one SLM, leveraging a coordinated fleet of SLMs, each with specialized strengths can enable systems to dynamically adapt to shifting contexts and workload patterns. However, realizing the full potential of such an approach demands intelligent orchestration and continuous adaptation. To this end, we introduce CALM , a self-adaptive orchestration mechanism based on MAPE-K. Our approach continuously monitors user queries, analyzes the QoS metrics of the SLMs, identifies the optimal SLM to be used, routes the query to the identified SLM and further to enhance the effectiveness and efficiency, leverages caching and scheduling to decide the SLMs to be kept in memory. Our evaluation shows that CALM reduces latency by approximately 40% and energy consumption by 50%, while preserving domain-specific task performance when compared to single-LLM baselines.

</details>


### [391] [SWE-Refactor: A Repository-Level Benchmark for Real-World LLM-Based Code Refactoring](https://arxiv.org/abs/2602.03712)
*Yisen Xu,Jinqiu Yang,Tse-Hsun,Chen*

Main category: cs.SE

TL;DR: 本文介绍新的代码重构基准SWE - Refactor，评估9个LLMs并公布结果以促进相关研究。


<details>
  <summary>Details</summary>
Motivation: 现有重构基准存在覆盖场景有限、包含无关更改实例和缺少存储库级上下文等问题，需要新的基准。

Method: 从18个Java项目挖掘1099个重构实例构建SWE - Refactor，通过编译、测试执行和自动检测工具验证正确性，并评估9个常用LLMs。

Result: 复杂和复合重构仍是失败主因，OpenAI Codex代理在复合实例上成功率仅39.4%。

Conclusion: 发布SWE - Refactor和评估结果，以推动基于LLM的代码重构研究。

Abstract: Large Language Models (LLMs) have recently attracted wide interest for tackling software engineering tasks. In contrast to code generation, refactoring demands precise, semantics-preserving edits that improve program structure, which also makes automated evaluation challenging. However, existing refactoring benchmarks commonly suffer from three shortcomings: limited coverage of refactoring scenarios, the inclusion of instances that mix refactoring with unrelated changes, and insufficient repository-level context for realistic assessment. To mitigate these issues, we introduce SWE-Refactor, a new benchmark for LLM-based code refactoring. SWE-Refactor comprises 1,099 developer-written, behavior-preserving refactorings mined from 18 Java projects, including 922 atomic and 177 compound instances. Each instance is validated via compilation, test execution, and automated refactoring detection tools to ensure correctness. We evaluate nine widely used LLMs on SWE-Refactor, covering models such as GPT-4o-mini, DeepSeek-V3, and CodeLLaMa, to provide representative reference results. Our results show that complex and compound refactorings remain the primary source of failures; notably, an OpenAI Codex agent achieves only 39.4% success on compound instances. We release SWE-Refactor and all evaluation results to facilitate future research on LLM-based code refactoring.

</details>


### [392] [Improving Deep Learning Library Testing with Machine Learning](https://arxiv.org/abs/2602.03755)
*Facundo Molina,M M Abid Naziri,Feiran Qin,Alessandra Gorla,Marcelo d'Amorim*

Main category: cs.SE

TL;DR: 利用ML分类器确定输入有效性，在TensorFlow和Pytorch的183个API上评估，准确率超91%，可提升ACETest通过率，表明ML增强输入分类有助于扩展DL库测试。


<details>
  <summary>Details</summary>
Motivation: 现有DL库易有bug，找bug技术无精确API规范会产生误报，现有挖掘API规范方法缺乏准确性。

Method: 用ML分类器确定输入有效性，以张量形状作为精确抽象来编码具体输入并捕捉数据关系，通过观察输入样本的运行结果获取标记数据来训练分类器。

Result: 分类器在未见过的数据上泛化良好，准确率超91%，集成到ACETest中使其通过率从约29%提升到约61%。

Conclusion: ML增强输入分类对扩展DL库测试很重要。

Abstract: Deep Learning (DL) libraries like TensorFlow and Pytorch simplify machine learning (ML) model development but are prone to bugs due to their complex design. Bug-finding techniques exist, but without precise API specifications, they produce many false alarms. Existing methods to mine API specifications lack accuracy. We explore using ML classifiers to determine input validity. We hypothesize that tensor shapes are a precise abstraction to encode concrete inputs and capture relationships of the data. Shape abstraction severely reduces problem dimensionality, which is important to facilitate ML training. Labeled data are obtained by observing runtime outcomes on a sample of inputs and classifiers are trained on sets of labeled inputs to capture API constraints. Our evaluation, conducted over 183 APIs from TensorFlow and Pytorch, shows that the classifiers generalize well on unseen data with over 91% accuracy. Integrating these classifiers into the pipeline of ACETest, a SoTA bug-finding technique, improves its pass rate from ~29% to ~61%. Our findings suggest that ML-enhanced input classification is an important aid to scale DL library testing.

</details>


### [393] [FullStack-Agent: Enhancing Agentic Full-Stack Web Coding via Development-Oriented Testing and Repository Back-Translation](https://arxiv.org/abs/2602.03798)
*Zimu Lu,Houxing Ren,Yunqiao Yang,Ke Wang,Zhuofan Zong,Mingjie Zhan,Hongsheng Li*

Main category: cs.SE

TL;DR: 提出FullStack - Agent全栈代码生成系统，含FullStack - Dev、FullStack - Learn和FullStack - Bench三部分，实验证明其有效性并开源代码。


<details>
  <summary>Details</summary>
Motivation: 现有代码代理倾向于仅生成前端网页，缺乏全栈数据处理和存储能力，构建生产级全栈Web应用挑战大。

Method: 构建FullStack - Agent系统，包含有多种能力的FullStack - Dev多代理框架、创新的数据扩展和自我改进方法FullStack - Learn、全面测试功能的FullStack - Bench基准。

Result: FullStack - Dev在前端、后端和数据库测试用例上分别比之前的先进方法高出8.7%、38.2%和15.9%；FullStack - Learn使30B模型在三组测试用例上性能分别提升9.7%、9.5%和2.8%。

Conclusion: 提出的FullStack - Agent系统有效，能解决现有代码代理在全栈开发中的不足。

Abstract: Assisting non-expert users to develop complex interactive websites has become a popular task for LLM-powered code agents. However, existing code agents tend to only generate frontend web pages, masking the lack of real full-stack data processing and storage with fancy visual effects. Notably, constructing production-level full-stack web applications is far more challenging than only generating frontend web pages, demanding careful control of data flow, comprehensive understanding of constantly updating packages and dependencies, and accurate localization of obscure bugs in the codebase. To address these difficulties, we introduce FullStack-Agent, a unified agent system for full-stack agentic coding that consists of three parts: (1) FullStack-Dev, a multi-agent framework with strong planning, code editing, codebase navigation, and bug localization abilities. (2) FullStack-Learn, an innovative data-scaling and self-improving method that back-translates crawled and synthesized website repositories to improve the backbone LLM of FullStack-Dev. (3) FullStack-Bench, a comprehensive benchmark that systematically tests the frontend, backend and database functionalities of the generated website. Our FullStack-Dev outperforms the previous state-of-the-art method by 8.7%, 38.2%, and 15.9% on the frontend, backend, and database test cases respectively. Additionally, FullStack-Learn raises the performance of a 30B model by 9.7%, 9.5%, and 2.8% on the three sets of test cases through self-improvement, demonstrating the effectiveness of our approach. The code is released at https://github.com/mnluzimu/FullStack-Agent.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [394] [DiffLOB: Diffusion Models for Counterfactual Generation in Limit Order Books](https://arxiv.org/abs/2602.03776)
*Zhuohan Wang,Carmine Ventre*

Main category: q-fin.CP

TL;DR: 提出DiffLOB模型用于可控和反事实的限价订单簿轨迹生成，并给出评估框架。


<details>
  <summary>Details</summary>
Motivation: 现有限价订单簿生成模型被动，对压力测试、情景分析和决策的实用性有限。

Method: 提出DiffLOB模型，该模型将生成过程与未来市场制度条件关联；构建包含可控现实性、反事实有效性和反事实有用性三个标准的评估框架。

Result: 未提及具体结果。

Conclusion: 未提及具体结论。

Abstract: Modern generative models for limit order books (LOBs) can reproduce realistic market dynamics, but remain fundamentally passive: they either model what typically happens without accounting for hypothetical future market conditions, or they require interaction with another agent to explore alternative outcomes. This limits their usefulness for stress testing, scenario analysis, and decision-making. We propose \textbf{DiffLOB}, a regime-conditioned \textbf{Diff}usion model for controllable and counterfactual generation of \textbf{LOB} trajectories. DiffLOB explicitly conditions the generative process on future market regimes--including trend, volatility, liquidity, and order-flow imbalance, which enables the model to answer counterfactual queries of the form: ``If the future market regime were X instead of Y, how would the limit order book evolve?'' Our systematic evaluation framework for counterfactual LOB generation consists of three criteria: (1) \textit{Controllable Realism}, measuring how well generated trajectories can reproduce marginal distributions, temporal dependence structure and regime variables; (2) \textit{Counterfactual validity}, testing whether interventions on future regimes induce consistent changes in the generated LOB dynamics; (3) \textit{Counterfactual usefulness}, assessing whether synthetic counterfactual trajectories improve downstream prediction of future market regimes.

</details>


<div id='q-fin.PM'></div>

# q-fin.PM [[Back]](#toc)

### [395] [A Novel approach to portfolio construction](https://arxiv.org/abs/2602.03325)
*T. Di Matteo,L. Riso,M. G. Zoia*

Main category: q-fin.PM

TL;DR: 本文提出基于机器学习的BPASGM框架用于资产选择和组合构建，通过蒙特卡罗模拟和实证研究表明其在风险收益等方面优于标准均值 - 方差组合。


<details>
  <summary>Details</summary>
Motivation: 解决样本均值 - 方差组合对估计误差敏感的问题，提升有限样本中的投资组合实际表现。

Method: 将大量金融资产的线性和非线性依赖关系映射到满足结构马尔可夫性质的稀疏图模型，进行依赖驱动筛选，去除正相关或冗余资产，再用标准均值 - 方差技术进行投资组合优化。

Result: 蒙特卡罗模拟显示基于BPASGM的投资组合具有更稳定的风险 - 收益特征、更低的实际波动率和更优的风险调整表现；实证结果证实了上述发现，并大幅降低了投资组合基数。

Conclusion: BPASGM为依赖感知的资产选择提供了一个有统计学依据且计算高效的框架，结合了稀疏图建模和投资组合理论。

Abstract: This paper proposes a machine learning-based framework for asset selection and portfolio construction, termed the Best-Path Algorithm Sparse Graphical Model (BPASGM). The method extends the Best-Path Algorithm (BPA) by mapping linear and non-linear dependencies among a large set of financial assets into a sparse graphical model satisfying a structural Markov property. Based on this representation, BPASGM performs a dependence-driven screening that removes positively or redundantly connected assets, isolating subsets that are conditionally independent or negatively correlated. This step is designed to enhance diversification and reduce estimation error in high-dimensional portfolio settings. Portfolio optimization is then conducted on the selected subset using standard mean-variance techniques. BPASGM does not aim to improve the theoretical mean-variance optimum under known population parameters, but rather to enhance realized performance in finite samples, where sample-based Markowitz portfolios are highly sensitive to estimation error. Monte Carlo simulations show that BPASGM-based portfolios achieve more stable risk-return profiles, lower realized volatility, and superior risk-adjusted performance compared to standard mean-variance portfolios. Empirical results for U.S. equities, global stock indices, and foreign exchange rates over 1990-2025 confirm these findings and demonstrate a substantial reduction in portfolio cardinality. Overall, BPASGM offers a statistically grounded and computationally efficient framework that integrates sparse graphical modeling with portfolio theory for dependence-aware asset selection.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [396] [Relaxed Triangle Inequality for Kullback-Leibler Divergence Between Multivariate Gaussian Distributions](https://arxiv.org/abs/2602.02577)
*Shiji Xiao,Yufeng Zhang,Chubo Liu,Yan Ding,Keqin Li,Kenli Li*

Main category: stat.ML

TL;DR: 文章研究多元高斯分布KL散度的松弛三角不等式，给出了KL(N₁, N₃)上确界及其可达条件，并展示了结果在两个领域的应用。


<details>
  <summary>Details</summary>
Motivation: KL散度不是合适的距离度量，不满足三角不等式，现有研究虽证明多元高斯分布KL散度满足松弛三角不等式，但KL(N₁, N₃)上确界未知。

Method: 对多元高斯分布KL散度的松弛三角不等式进行研究分析。

Result: 得出当ε₁和ε₂较小时，KL(N₁, N₃)的上确界为ε₁+ε₂+√(ε₁ε₂)+o(ε₁)+o(ε₂)。

Conclusion: 研究结果可应用于基于流的生成模型的分布外检测和安全强化学习。

Abstract: The Kullback-Leibler (KL) divergence is not a proper distance metric and does not satisfy the triangle inequality, posing theoretical challenges in certain practical applications. Existing work has demonstrated that KL divergence between multivariate Gaussian distributions follows a relaxed triangle inequality. Given any three multivariate Gaussian distributions $\mathcal{N}_1, \mathcal{N}_2$, and $\mathcal{N}_3$, if $KL(\mathcal{N}_1, \mathcal{N}_2)\leq ε_1$ and $KL(\mathcal{N}_2, \mathcal{N}_3)\leq ε_2$, then $KL(\mathcal{N}_1, \mathcal{N}_3)< 3ε_1+3ε_2+2\sqrt{ε_1ε_2}+o(ε_1)+o(ε_2)$. However, the supremum of $KL(\mathcal{N}_1, \mathcal{N}_3)$ is still unknown. In this paper, we investigate the relaxed triangle inequality for the KL divergence between multivariate Gaussian distributions and give the supremum of $KL(\mathcal{N}_1, \mathcal{N}_3)$ as well as the conditions when the supremum can be attained. When $ε_1$ and $ε_2$ are small, the supremum is $ε_1+ε_2+\sqrt{ε_1ε_2}+o(ε_1)+o(ε_2)$. Finally, we demonstrate several applications of our results in out-of-distribution detection with flow-based generative models and safe reinforcement learning.

</details>


### [397] [Rethinking Test-Time Training: Tilting The Latent Distribution For Few-Shot Source-Free Adaptation](https://arxiv.org/abs/2602.02633)
*Tahir Qasim Syed,Behraj Khan*

Main category: stat.ML

TL;DR: 研究冻结模型下基础模型的少样本分类测试时自适应问题，提出无训练推理方法，效果好证明推理级分布校正可行。


<details>
  <summary>Details</summary>
Motivation: 部署场景中轻量级参数更新可能导致模型偏移或调优不稳定，研究完全冻结模型且无上游数据情况下基础模型的少样本分类测试时自适应。

Method: 提出无训练推理方法，通过对编码器诱导的潜在嵌入分布进行测度变换，利用小标签支持集的任务相似性分数以KL最优方式对潜在分布重新加权。

Result: 该方法在多个基准和射击制度中与基于参数更新的方法竞争，且在更严格约束下表现良好。

Conclusion: 证明即使在完全冻结模型管道下，推理级分布校正进行测试时自适应是可行的。

Abstract: Often, constraints arise in deployment settings where even lightweight parameter updates e.g. parameter-efficient fine-tuning could induce model shift or tuning instability. We study test-time adaptation of foundation models for few-shot classification under a completely frozen-model regime, where additionally, no upstream data are accessible. We propose arguably the first training-free inference method that adapts predictions to the new task by performing a change of measure over the latent embedding distribution induced by the encoder. Using task-similarity scores derived from a small labeled support set, exponential tilting reweights latent distributions in a KL-optimal manner without modifying model parameters. Empirically, the method consistently competes with parameter-update-based methods across multiple benchmarks and shot regimes, while operating under strictly and universally stronger constraints. These results demonstrate the viability of inference-level distributional correction for test-time adaptation even with a fully-frozen model pipeline.

</details>


### [398] [Near-Universal Multiplicative Updates for Nonnegative Einsum Factorization](https://arxiv.org/abs/2602.02759)
*John Hood,Aaron Schein*

Main category: stat.ML

TL;DR: 提出NNEinFact算法用于非负张量分解，可由字符串指定模型，收敛快且表现优。


<details>
  <summary>Details</summary>
Motivation: 现有适合定制非负张量分解的用户友好工具少，研究者面临方法选择和实现难题。

Method: 引入基于einsum的乘法更新算法NNEinFact，通过最小化用户指定损失函数拟合非负张量分解。

Result: NNEinFact收敛到损失局部最小值，支持缺失数据，能快速处理大规模张量，自定义模型在预测任务中表现优于标准模型，比基于梯度方法收敛快且测试损失低。

Conclusion: NNEinFact是适合非负张量分解的有效算法。

Abstract: Despite the ubiquity of multiway data across scientific domains, there are few user-friendly tools that fit tailored nonnegative tensor factorizations. Researchers may use gradient-based automatic differentiation (which often struggles in nonnegative settings), choose between a limited set of methods with mature implementations, or implement their own model from scratch. As an alternative, we introduce NNEinFact, an einsum-based multiplicative update algorithm that fits any nonnegative tensor factorization expressible as a tensor contraction by minimizing one of many user-specified loss functions (including the $(α,β)$-divergence). To use NNEinFact, the researcher simply specifies their model with a string. NNEinFact converges to a local minimum of the loss, supports missing data, and fits to tensors with hundreds of millions of entries in seconds. Empirically, NNEinFact fits custom models which outperform standard ones in heldout prediction tasks on real-world tensor data by over $37\%$ and attains less than half the test loss of gradient-based methods while converging up to 90 times faster.

</details>


### [399] [Plug-In Classification of Drift Functions in Diffusion Processes Using Neural Networks](https://arxiv.org/abs/2602.02791)
*Yuzhen Zhao,Jiarong Fan,Yating Liu*

Main category: stat.ML

TL;DR: 本文研究扩散过程的监督多类分类问题，提出基于神经网络的插件分类器，分析收敛率，数值实验显示其性能良好。


<details>
  <summary>Details</summary>
Motivation: 研究扩散过程的多类分类问题，扩展一维框架到多维。

Method: 将一维多类框架扩展到多维扩散，提出基于神经网络的插件分类器，根据贝叶斯型决策规则分配标签。

Result: 建立了多余误分类风险的收敛率；数值实验表明该方法比一维设置下的方法收敛更快、性能更好，在高维有效，且优于未利用扩散模型结构的直接神经网络分类器。

Conclusion: 提出的基于神经网络的插件分类器在扩散过程多类分类问题上表现良好，具有一定优势。

Abstract: We study a supervised multiclass classification problem for diffusion processes, where each class is characterized by a distinct drift function and trajectories are observed at discrete times. Extending the one-dimensional multiclass framework of Denis et al. (2024) to multidimensional diffusions, we propose a neural network-based plug-in classifier that estimates the drift functions for each class from independent sample paths and assigns labels based on a Bayes-type decision rule. Under standard regularity assumptions, we establish convergence rates for the excess misclassification risk, explicitly capturing the effects of drift estimation error and time discretization. Numerical experiments demonstrate that the proposed method achieves faster convergence and improved classification performance compared to Denis et al. (2024) in the one-dimensional setting, remains effective in higher dimensions when the underlying drift functions admit a compositional structure, and consistently outperforms direct neural network classifiers trained end-to-end on trajectories without exploiting the diffusion model structure.

</details>


### [400] [Training-Free Self-Correction for Multimodal Masked Diffusion Models](https://arxiv.org/abs/2602.02927)
*Yidong Ouyang,Panwen Hu,Zhengyan Wan,Zhe Wang,Liyan Xie,Dmitriy Bespalov,Ying Nian Wu,Guang Cheng,Hongyuan Zha,Qiang Sun*

Main category: stat.ML

TL;DR: 本文提出无训练自校正框架，提升掩码扩散模型生成质量，减少采样步数，且具有跨架构的泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有掩码扩散模型采样程序会导致错误累积，现有自校正方法存在需额外训练或依赖不一致似然估计的局限。

Method: 提出无训练自校正框架，利用预训练掩码扩散模型的归纳偏置，不修改模型参数或引入辅助评估器。

Result: 在文本到图像生成和多模态理解任务上显著提升生成质量，减少采样步数。

Conclusion: 该框架具有跨不同掩码扩散架构的泛化性，具备鲁棒性和实际应用性。

Abstract: Masked diffusion models have emerged as a powerful framework for text and multimodal generation. However, their sampling procedure updates multiple tokens simultaneously and treats generated tokens as immutable, which may lead to error accumulation when early mistakes cannot be revised. In this work, we revisit existing self-correction methods and identify limitations stemming from additional training requirements or reliance on misaligned likelihood estimates. We propose a training-free self-correction framework that exploits the inductive biases of pre-trained masked diffusion models. Without modifying model parameters or introducing auxiliary evaluators, our method significantly improves generation quality on text-to-image generation and multimodal understanding tasks with reduced sampling steps. Moreover, the proposed framework generalizes across different masked diffusion architectures, highlighting its robustness and practical applicability. Code can be found in https://github.com/huge123/FreeCorrection.

</details>


### [401] [Unified Inference Framework for Single and Multi-Player Performative Prediction: Method and Asymptotic Optimality](https://arxiv.org/abs/2602.03049)
*Zhixian Zhang,Xiaotian Hou,Linjun Zhang*

Main category: stat.ML

TL;DR: 本文提出统一统计推断框架处理单智能体和多智能体的性能预测问题，给出估计方法并证明效率和稳健性。


<details>
  <summary>Details</summary>
Motivation: 过往研究将单智能体和多智能体的性能预测视为不同现象，本文旨在提出统一的统计推断框架。

Method: 提出重复风险最小化（RRM）程序估计性能稳定性，建立严格推断理论；引入基于重校准预测驱动推断（RePPI）和重要性采样的两步插件式估计器评估性能最优性。

Result: 理论分析表明估计器达到半参数效率界，并且在温和分布误设下保持稳健性。

Conclusion: 为动态、性能驱动的环境中的可靠估计和决策提供了原则性工具包。

Abstract: Performative prediction characterizes environments where predictive models alter the very data distributions they aim to forecast, triggering complex feedback loops. While prior research treats single-agent and multi-agent performativity as distinct phenomena, this paper introduces a unified statistical inference framework that bridges these contexts, treating the former as a special case of the latter. Our contribution is two-fold. First, we put forward the Repeated Risk Minimization (RRM) procedure for estimating the performative stability, and establish a rigorous inferential theory for admitting its asymptotic normality and confirming its asymptotic efficiency. Second, for the performative optimality, we introduce a novel two-step plug-in estimator that integrates the idea of Recalibrated Prediction Powered Inference (RePPI) with Importance Sampling, and further provide formal derivations for the Central Limit Theorems of both the underlying distributional parameters and the plug-in results. The theoretical analysis demonstrates that our estimator achieves the semiparametric efficiency bound and maintains robustness under mild distributional misspecification. This work provides a principled toolkit for reliable estimation and decision-making in dynamic, performative environments.

</details>


### [402] [Online Conformal Prediction via Universal Portfolio Algorithms](https://arxiv.org/abs/2602.03168)
*Tuo Liu,Edgar Dobriban,Francesco Orabona*

Main category: stat.ML

TL;DR: 本文为基于(1 - α)-pinball损失的区间值在线共形预测（OCP）发展了一种通用的遗憾-覆盖率理论，并提出无参数方法UP - OCP，实验表明其表现优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有OCP方法常需手动调整学习率且需特定算法分析，本文旨在解决这些问题。

Method: 基于(1 - α)-pinball损失发展通用理论，确定线性化遗憾为关键概念，将问题转化为双资产投资组合选择问题，利用通用投资组合算法提出UP - OCP。

Result: 得到UP - OCP误覆盖的强有限时间界，即使预测呈多项式增长；实验显示UP - OCP在大小/覆盖率权衡上表现更好。

Conclusion: UP - OCP是一种有效的无参数OCP方法，在性能上优于现有在线共形基线。

Abstract: Online conformal prediction (OCP) seeks prediction intervals that achieve long-run $1-α$ coverage for arbitrary (possibly adversarial) data streams, while remaining as informative as possible. Existing OCP methods often require manual learning-rate tuning to work well, and may also require algorithm-specific analyses. Here, we develop a general regret-to-coverage theory for interval-valued OCP based on the $(1-α)$-pinball loss. Our first contribution is to identify \emph{linearized regret} as a key notion, showing that controlling it implies coverage bounds for any online algorithm. This relies on a black-box reduction that depends only on the Fenchel conjugate of an upper bound on the linearized regret. Building on this theory, we propose UP-OCP, a parameter-free method for OCP, via a reduction to a two-asset portfolio selection problem, leveraging universal portfolio algorithms. We show strong finite-time bounds on the miscoverage of UP-OCP, even for polynomially growing predictions. Extensive experiments support that UP-OCP delivers consistently better size/coverage trade-offs than prior online conformal baselines.

</details>


### [403] [NeuralFLoC: Neural Flow-Based Joint Registration and Clustering of Functional Data](https://arxiv.org/abs/2602.03169)
*Xinyang Xiong,Siyuan jiang,Pengcheng Zeng*

Main category: stat.ML

TL;DR: 提出NeuralFLoC框架用于联合功能配准和聚类，有理论保证，实验表现好且代码开源。


<details>
  <summary>Details</summary>
Motivation: 当前聚类功能数据时处理相位变化有挑战，现有方法将配准和聚类分离或依赖严格参数假设。

Method: 提出基于Neural ODE驱动的微分同胚流和谱聚类的全无监督、端到端深度学习框架NeuralFLoC。

Result: 在功能基准测试中，配准和聚类表现达到了最先进水平，对缺失数据、不规则采样和噪声具有鲁棒性，且可扩展性良好。

Conclusion: NeuralFLoC框架有效解决了功能数据聚类中的相位变化问题，有理论和实践优势。

Abstract: Clustering functional data in the presence of phase variation is challenging, as temporal misalignment can obscure intrinsic shape differences and degrade clustering performance. Most existing approaches treat registration and clustering as separate tasks or rely on restrictive parametric assumptions. We present \textbf{NeuralFLoC}, a fully unsupervised, end-to-end deep learning framework for joint functional registration and clustering based on Neural ODE-driven diffeomorphic flows and spectral clustering. The proposed model learns smooth, invertible warping functions and cluster-specific templates simultaneously, effectively disentangling phase and amplitude variation. We establish universal approximation guarantees and asymptotic consistency for the proposed framework. Experiments on functional benchmarks show state-of-the-art performance in both registration and clustering, with robustness to missing data, irregular sampling, and noise, while maintaining scalability. Code is available at https://anonymous.4open.science/r/NeuralFLoC-FEC8.

</details>


### [404] [Latent Neural-ODE for Model-Informed Precision Dosing: Overcoming Structural Assumptions in Pharmacokinetics](https://arxiv.org/abs/2602.03215)
*Benjamin Maurel,Agathe Guilloux,Sarah Zohar,Moreno Ursino,Jean-Baptiste Woillard*

Main category: stat.ML

TL;DR: 提出用Latent ODE预测他克莫司AUC，经模拟和临床验证，证明其是强大可靠的AUC预测工具，为个性化医疗模型提供基础。


<details>
  <summary>Details</summary>
Motivation: 当前基于NLME方法的PopPK模型依赖预设假设，难以捕捉复杂患者特异性动态，导致模型误设，需新方法准确估计他克莫司暴露量。

Method: 引入基于Latent ODE的数据驱动方法预测他克莫司AUC，通过多场景模拟评估，与NLME和it2B方法对比，并在两个临床数据集上验证。

Result: 模拟中Latent ODE模型鲁棒性好；临床数据集内部验证精度更高，外部队列结果与标准估计器相当。

Conclusion: Latent ODE是强大可靠的AUC预测工具，其灵活架构为个性化医疗多模态模型提供了有前景的基础。

Abstract: Accurate estimation of tacrolimus exposure, quantified by the area under the concentration-time curve (AUC), is essential for precision dosing after renal transplantation. Current practice relies on population pharmacokinetic (PopPK) models based on nonlinear mixed-effects (NLME) methods. However, these models depend on rigid, pre-specified assumptions and may struggle to capture complex, patient-specific dynamics, leading to model misspecification.
  In this study, we introduce a novel data-driven alternative based on Latent Ordinary Differential Equations (Latent ODEs) for tacrolimus AUC prediction. This deep learning approach learns individualized pharmacokinetic dynamics directly from sparse clinical data, enabling greater flexibility in modeling complex biological behavior. The model was evaluated through extensive simulations across multiple scenarios and benchmarked against two standard approaches: NLME-based estimation and the iterative two-stage Bayesian (it2B) method. We further performed a rigorous clinical validation using a development dataset (n = 178) and a completely independent external dataset (n = 75).
  In simulation, the Latent ODE model demonstrated superior robustness, maintaining high accuracy even when underlying biological mechanisms deviated from standard assumptions. Regarding experiments on clinical datasets, in internal validation, it achieved significantly higher precision with a mean RMSPE of 7.99% compared with 9.24% for it2B (p < 0.001). On the external cohort, it achieved an RMSPE of 10.82%, comparable to the two standard estimators (11.48% and 11.54%).
  These results establish the Latent ODE as a powerful and reliable tool for AUC prediction. Its flexible architecture provides a promising foundation for next-generation, multi-modal models in personalized medicine.

</details>


### [405] [Principled Federated Random Forests for Heterogeneous Data](https://arxiv.org/abs/2602.03258)
*Rémi Khellaf,Erwan Scornet,Aurélien Bellet,Julie Josse*

Main category: stat.ML

TL;DR: 提出FedForest算法以适应联邦学习场景下的随机森林，可处理数据异质性，性能接近集中式且通信高效。


<details>
  <summary>Details</summary>
Motivation: 现有随机森林模型多适用于集中式数据，联邦学习场景下适配方法少，且现有实现依赖无原则启发式方法。

Method: 提出FedForest算法，基于聚合客户统计信息进行分裂过程，允许在客户指标上进行分裂。

Result: 在不同异质性基准测试中，联邦森林性能接近集中式，且通信高效。

Conclusion: FedForest算法能自然适应客户数据异质性，是有效的联邦随机森林算法。

Abstract: Random Forests (RF) are among the most powerful and widely used predictive models for centralized tabular data, yet few methods exist to adapt them to the federated learning setting. Unlike most federated learning approaches, the piecewise-constant nature of RF prevents exact gradient-based optimization. As a result, existing federated RF implementations rely on unprincipled heuristics: for instance, aggregating decision trees trained independently on clients fails to optimize the global impurity criterion, even under simple distribution shifts. We propose FedForest, a new federated RF algorithm for horizontally partitioned data that naturally accommodates diverse forms of client data heterogeneity, from covariate shift to more complex outcome shift mechanisms. We prove that our splitting procedure, based on aggregating carefully chosen client statistics, closely approximates the split selected by a centralized algorithm. Moreover, FedForest allows splits on client indicators, enabling a non-parametric form of personalization that is absent from prior federated random forest methods. Empirically, we demonstrate that the resulting federated forests closely match centralized performance across heterogeneous benchmarks while remaining communication-efficient.

</details>


### [406] [Multiparameter Uncertainty Mapping in Quantitative Molecular MRI using a Physics-Structured Variational Autoencoder (PS-VAE)](https://arxiv.org/abs/2602.03317)
*Alex Finkelstein,Ron Moneta,Or Zohar,Michal Rivlin,Moritz Zaiss,Dinora Friedmann Morvinski,Or Perlman*

Main category: stat.ML

TL;DR: 介绍用于快速提取体素级多参数后验分布的物理结构变分自编码器（PS - VAE），并验证其效果和用途。


<details>
  <summary>Details</summary>
Motivation: 现有的定量成像逆问题方法缺乏原则性的不确定性量化，限制了临床接受度，需新方法。

Method: 使用PS - VAE，将可微自旋物理模拟器与自监督学习相结合。

Result: 该方法在多质子池化学交换饱和转移和半固体磁化转移分子磁共振指纹成像研究中得到验证，多参数后验与暴力贝叶斯分析结果一致，全脑量化加速。

Conclusion: 该方法可用于协议优化和实时自适应采集。

Abstract: Quantitative imaging methods, such as magnetic resonance fingerprinting (MRF), aim to extract interpretable pathology biomarkers by estimating biophysical tissue parameters from signal evolutions. However, the pattern-matching algorithms or neural networks used in such inverse problems often lack principled uncertainty quantification, which limits the trustworthiness and transparency, required for clinical acceptance. Here, we describe a physics-structured variational autoencoder (PS-VAE) designed for rapid extraction of voxelwise multi-parameter posterior distributions. Our approach integrates a differentiable spin physics simulator with self-supervised learning, and provides a full covariance that captures the inter-parameter correlations of the latent biophysical space. The method was validated in a multi-proton pool chemical exchange saturation transfer (CEST) and semisolid magnetization transfer (MT) molecular MRF study, across in-vitro phantoms, tumor-bearing mice, healthy human volunteers, and a subject with glioblastoma. The resulting multi-parametric posteriors are in good agreement with those calculated using a brute-force Bayesian analysis, while providing an orders-of-magnitude acceleration in whole brain quantification. In addition, we demonstrate how monitoring the multi-parameter posterior dynamics across progressively acquired signals provides practical insights for protocol optimization and may facilitate real-time adaptive acquisition.

</details>


### [407] [Improving the Linearized Laplace Approximation via Quadratic Approximations](https://arxiv.org/abs/2602.03394)
*Pedro Jiménez,Luis A. Ortega,Pablo Morales-Álvarez,Daniel Hernández-Lobato*

Main category: stat.ML

TL;DR: DNN常产生过自信的分布外预测，LLA有问题，提出QLA改进不确定性估计。


<details>
  <summary>Details</summary>
Motivation: DNN常产生过自信的分布外预测，LLA的线性化可能降低对真实拉普拉斯近似的保真度。

Method: 提出QLA，通过高效幂迭代获得的一阶因子近似拉普拉斯对数后验中的每个二阶因子。

Result: 在五个回归数据集上，QLA比LLA有适度且一致的不确定性估计改进。

Conclusion: QLA能在不显著增加计算成本的情况下，改进不确定性估计。

Abstract: Deep neural networks (DNNs) often produce overconfident out-of-distribution predictions, motivating Bayesian uncertainty quantification. The Linearized Laplace Approximation (LLA) achieves this by linearizing the DNN and applying Laplace inference to the resulting model. Importantly, the linear model is also used for prediction. We argue this linearization in the posterior may degrade fidelity to the true Laplace approximation. To alleviate this problem, without increasing significantly the computational cost, we propose the Quadratic Laplace Approximation (QLA). QLA approximates each second order factor in the approximate Laplace log-posterior using a rank-one factor obtained via efficient power iterations. QLA is expected to yield a posterior precision closer to that of the full Laplace without forming the full Hessian, which is typically intractable. For prediction, QLA also uses the linearized model. Empirically, QLA yields modest yet consistent uncertainty estimation improvements over LLA on five regression datasets.

</details>


### [408] [Score-based diffusion models for diffuse optical tomography with uncertainty quantification](https://arxiv.org/abs/2602.03449)
*Fabian Schneider,Meghdoot Mozumder,Konstantin Tamarov,Leila Taghizadeh,Tanja Tarvainen,Tapio Helin,Duc-Lam Duong*

Main category: stat.ML

TL;DR: 本文评估UCoS框架用于漫射光学断层成像（DOT）的线性化差分成像，引入新的正则化方法，实验表明数据驱动先验分布在高度病态问题和存在建模误差时有效。


<details>
  <summary>Details</summary>
Motivation: 目前对存在建模误差的现实逆问题及物理测量数据利用的研究不足，需要评估UCoS框架在DOT中的应用。

Method: 引入一种新的正则化方法，构建由学习和基于模型的分量组成的混合分数，防止分数函数过拟合，并使用模拟和实验测量数据进行验证。

Result: 与经典的基于模型的估计相比，数据驱动的先验分布产生的后验样本方差低，且围绕真实值分布，即使在高度病态问题和存在建模误差的情况下也是如此。

Conclusion: 数据驱动的先验分布在高度病态的逆问题和存在建模误差的情况下表现良好，所提出的方法有效。

Abstract: Score-based diffusion models are a recently developed framework for posterior sampling in Bayesian inverse problems with a state-of-the-art performance for severely ill-posed problems by leveraging a powerful prior distribution learned from empirical data. Despite generating significant interest especially in the machine-learning community, a thorough study of realistic inverse problems in the presence of modelling error and utilization of physical measurement data is still outstanding. In this work, the framework of unconditional representation for the conditional score function (UCoS) is evaluated for linearized difference imaging in diffuse optical tomography (DOT). DOT uses boundary measurements of near-infrared light to estimate the spatial distribution of absorption and scattering parameters in biological tissues. The problem is highly ill-posed and thus sensitive to noise and modelling errors. We introduce a novel regularization approach that prevents overfitting of the score function by constructing a mixed score composed of a learned and a model-based component. Validation of this approach is done using both simulated and experimental measurement data. The experiments demonstrate that a data-driven prior distribution results in posterior samples with low variance, compared to classical model-based estimation, and centred around the ground truth, even in the context of a highly ill-posed problem and in the presence of modelling errors.

</details>


### [409] [Generator-based Graph Generation via Heat Diffusion](https://arxiv.org/abs/2602.03612)
*Anthony Stephenson,Ian Gallagher,Christopher Nemeth*

Main category: stat.ML

TL;DR: 本文提出一种新的图生成框架，将生成器匹配范式应用于图数据，利用图拉普拉斯算子及其热核定义连续时间扩散，实验表明该方法能有效捕捉图结构特性。


<details>
  <summary>Details</summary>
Motivation: 图生成建模在多个领域有广泛应用，需要新的有效图生成框架。

Method: 将生成器匹配范式应用于图数据，利用图拉普拉斯算子及其热核定义连续时间扩散，训练神经网络最小化真实生成器和可学习替代生成器之间的Bregman散度，用替代生成器模拟时间反转扩散过程采样新图结构。

Result: 该方法能有效捕捉真实和合成图的结构特性。

Conclusion: 提出的框架统一并推广了现有的基于扩散的图生成模型，通过拉普拉斯算子注入特定领域归纳偏置，同时保留了神经逼近器的灵活性。

Abstract: Graph generative modelling has become an essential task due to the wide range of applications in chemistry, biology, social networks, and knowledge representation. In this work, we propose a novel framework for generating graphs by adapting the Generator Matching (arXiv:2410.20587) paradigm to graph-structured data. We leverage the graph Laplacian and its associated heat kernel to define a continous-time diffusion on each graph. The Laplacian serves as the infinitesimal generator of this diffusion, and its heat kernel provides a family of conditional perturbations of the initial graph. A neural network is trained to match this generator by minimising a Bregman divergence between the true generator and a learnable surrogate. Once trained, the surrogate generator is used to simulate a time-reversed diffusion process to sample new graph structures. Our framework unifies and generalises existing diffusion-based graph generative models, injecting domain-specific inductive bias via the Laplacian, while retaining the flexibility of neural approximators. Experimental studies demonstrate that our approach captures structural properties of real and synthetic graphs effectively.

</details>


### [410] [Improved Analysis of the Accelerated Noisy Power Method with Applications to Decentralized PCA](https://arxiv.org/abs/2602.03682)
*Pierre Aguié,Mathieu Even,Laurent Massoulié*

Main category: stat.ML

TL;DR: 分析加速噪声幂法，在更宽松条件下保留加速收敛率，结果最优且有实际应用。


<details>
  <summary>Details</summary>
Motivation: 以往工作对扰动幅度上界要求过严，限制加速噪声幂法实际应用。

Method: 对加速噪声幂法进行改进分析。

Result: 新分析在更宽松条件下保留加速收敛率，是最坏情况最优，推导出适用于去中心化PCA的加速算法。

Conclusion: 提出的新分析有理论最优性且推导出首个有可证明加速收敛的去中心化PCA算法。

Abstract: We analyze the Accelerated Noisy Power Method, an algorithm for Principal Component Analysis in the setting where only inexact matrix-vector products are available, which can arise for instance in decentralized PCA. While previous works have established that acceleration can improve convergence rates compared to the standard Noisy Power Method, these guarantees require overly restrictive upper bounds on the magnitude of the perturbations, limiting their practical applicability. We provide an improved analysis of this algorithm, which preserves the accelerated convergence rate under much milder conditions on the perturbations. We show that our new analysis is worst-case optimal, in the sense that the convergence rate cannot be improved, and that the noise conditions we derive cannot be relaxed without sacrificing convergence guarantees. We demonstrate the practical relevance of our results by deriving an accelerated algorithm for decentralized PCA, which has similar communication costs to non-accelerated methods. To our knowledge, this is the first decentralized algorithm for PCA with provably accelerated convergence.

</details>


### [411] [Efficient Variance-reduced Estimation from Generative EHR Models: The SCOPE and REACH Estimators](https://arxiv.org/abs/2602.03730)
*Luke Solo,Matthew B. A. McDermott,William F. Parker,Bashar Ramadan,Michael C. Burkhart,Brett K. Beaulieu-Jones*

Main category: stat.ML

TL;DR: 提出SCOPE和REACH兩個新估計器，提升生成式EHR模型效率，減少推理成本，提高在資源受限臨床環境部署的可行性。


<details>
  <summary>Details</summary>
Motivation: 現有利用自監督訓練生成式模型進行臨床結果預測的方法存在估計分佈稀疏、計算成本高和採樣方差大等問題。

Method: 提出Sum of Conditional Outcome Probability Estimator (SCOPE) 和Risk Estimation from Anticipated Conditional Hazards (REACH) 兩個新估計器，利用標準蒙特卡羅法捨棄的下一令牌概率分佈。

Result: 證明兩個估計器無偏，REACH能保證在任何模型和結果上的方差小於蒙特卡羅採樣；在MIMIC - IV的住院死亡率預測中，SCOPE和REACH用10 - 11個樣本達到100個樣本的蒙特卡羅表現，推論成本降低約10倍；在ICU入院預測中效率提升約1.2倍。

Conclusion: 這些方法顯著提高了在資源受限臨床環境中部署生成式EHR模型的可行性。

Abstract: Generative models trained using self-supervision of tokenized electronic health record (EHR) timelines show promise for clinical outcome prediction. This is typically done using Monte Carlo simulation for future patient trajectories. However, existing approaches suffer from three key limitations: sparse estimate distributions that poorly differentiate patient risk levels, extreme computational costs, and high sampling variance. We propose two new estimators: the Sum of Conditional Outcome Probability Estimator (SCOPE) and Risk Estimation from Anticipated Conditional Hazards (REACH), that leverage next-token probability distributions discarded by standard Monte Carlo. We prove both estimators are unbiased and that REACH guarantees variance reduction over Monte Carlo sampling for any model and outcome. Empirically, on hospital mortality prediction in MIMIC-IV using the ETHOS-ARES framework, SCOPE and REACH match 100-sample Monte Carlo performance using only 10-11 samples (95% CI: [9,11]), representing a ~10x reduction in inference cost without degrading calibration. For ICU admission prediction, efficiency gains are more modest (~1.2x), which we attribute to the outcome's lower "spontaneity," a property we characterize theoretically and empirically. These methods substantially improve the feasibility of deploying generative EHR models in resource-constrained clinical settings.

</details>


### [412] [Fast Sampling for Flows and Diffusions with Lazy and Point Mass Stochastic Interpolants](https://arxiv.org/abs/2602.03789)
*Gabriel Damsholt,Jes Frellsen,Susanne Ditlevsen*

Main category: stat.ML

TL;DR: 本文研究随机插值器，证明SDE样本路径的转换方法，扩展其框架到点质量调度，确定惰性调度族，还在非高斯真实数据上展示理论结果的实用性。


<details>
  <summary>Details</summary>
Motivation: 随机插值器统一了流和扩散这两种生成模型框架，但这些方法的主要超参数插值调度有待研究，期望探索不同调度和扩散系数下的转换及扩展框架。

Method: 证明将任意调度和扩散系数下的SDE样本路径转换为另一种情况的方法；扩展随机插值器框架到点质量调度；在高斯数据假设下确定惰性调度族。

Result: 得到了不同调度间的样本路径转换，确定了使漂移为零的惰性调度族，在非高斯真实数据上应用惰性调度转换可使预训练流模型少步骤生成图像。

Conclusion: 所提出的理论结果和方法在生成模型中有实际应用价值，可在不重新训练模型的情况下减少图像生成步骤。

Abstract: Stochastic interpolants unify flows and diffusions, popular generative modeling frameworks. A primary hyperparameter in these methods is the interpolation schedule that determines how to bridge a standard Gaussian base measure to an arbitrary target measure. We prove how to convert a sample path of a stochastic differential equation (SDE) with arbitrary diffusion coefficient under any schedule into the unique sample path under another arbitrary schedule and diffusion coefficient. We then extend the stochastic interpolant framework to admit a larger class of point mass schedules in which the Gaussian base measure collapses to a point mass measure. Under the assumption of Gaussian data, we identify lazy schedule families that make the drift identically zero and show that with deterministic sampling one gets a variance-preserving schedule commonly used in diffusion models, whereas with statistically optimal SDE sampling one gets our point mass schedule. Finally, to demonstrate the usefulness of our theoretical results on realistic highly non-Gaussian data, we apply our lazy schedule conversion to a state-of-the-art pretrained flow model and show that this allows for generating images in fewer steps without retraining the model.

</details>


### [413] [Preference-based Conditional Treatment Effects and Policy Learning](https://arxiv.org/abs/2602.03823)
*Dovid Parnas,Mathieu Even,Julie Josse,Uri Shalit*

Main category: stat.ML

TL;DR: 本文介绍了基于条件偏好治疗效应（CPTE）的偏好框架用于条件治疗效应估计和政策学习，给出估计策略，实验有性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多元、有序或偏好驱动结果的异质性效应建模上受限，部分估计量不可识别，需新方法。

Method: 通过匹配、分位数和分布回归给出估计策略，设计有效影响函数估计器纠正偏差并最大化政策价值。

Result: 合成和半合成实验表明该框架有明显性能提升和实际影响。

Conclusion: 基于 CPTE 的偏好框架能统一多种应用，为不可识别估计量提供新可识别条件，是有效的方法。

Abstract: We introduce a new preference-based framework for conditional treatment effect estimation and policy learning, built on the Conditional Preference-based Treatment Effect (CPTE). CPTE requires only that outcomes be ranked under a preference rule, unlocking flexible modeling of heterogeneous effects with multivariate, ordinal, or preference-driven outcomes. This unifies applications such as conditional probability of necessity and sufficiency, conditional Win Ratio, and Generalized Pairwise Comparisons. Despite the intrinsic non-identifiability of comparison-based estimands, CPTE provides interpretable targets and delivers new identifiability conditions for previous unidentifiable estimands. We present estimation strategies via matching, quantile, and distributional regression, and further design efficient influence-function estimators to correct plug-in bias and maximize policy value. Synthetic and semi-synthetic experiments demonstrate clear performance gains and practical impact.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [414] [From Accessibility to Allocation: An Integrated Workflow for Land-Use Assignment and FAR Estimation](https://arxiv.org/abs/2602.02887)
*Yue Sun,Ryan Weightman,Yang Yang,Anye Shi,Timur Dogan,Samitha Samaranayake*

Main category: stat.CO

TL;DR: 提出三阶段框架将街道可达性用于土地利用和建筑面积分配，可支持多目标政策搜索，应用于实际城区有效，为规划者提供透明工具。


<details>
  <summary>Details</summary>
Motivation: 现有城市土地利用和建筑强度规划缺乏与网络可达性的直接可审计联系，限制了事前政策评估，研究能否将多半径街道中心性从诊断提升为设计手段。

Method: 引入三阶段流程：计算街道网络多半径可达性并转换到街区；构建嵌套服务盆地指导基于规则的土地利用布局；通过可达性加权线性模型分配建筑面积比。支持多目标政策搜索。

Result: 应用于实际城区，能重现商业选址和工业带分布，集中强度于高连通性街区。多目标筛选的政策采样产生帕累托有效计划。

Conclusion: 方法上，将空间句法措施转化为有明确保障的土地利用和建筑面积比分配；实践上，为规划者提供用于不同尺度反事实测试和权衡的透明工具。

Abstract: Urban land use and building intensity are often planned without a direct, auditable link to network accessibility, limiting ex-ante policy evaluation. This study asks whether multi-radius street centralities can be elevated from diagnosis to design lever to allocate land use and floor area in a transparent, optimization-ready workflow. We introduce a three-stage pipeline that connects configuration to program and intensity. First, multi-radius accessibility is computed on the street network and translated to blocks to provide scale-legible measures of reach. Second, these measures structure nested service basins that guide a rule-based placement of land uses with explicit priorities and minimum parcel footprints, ensuring reproducibility. Third, within each use, floor-area ratio (FAR) is assigned by an accessibility-weighted linear model that satisfies global construction totals while anchoring the average FAR, thereby tilting height toward better-connected blocks without pathological extremes. The framework supports multi-objective policy search via sampling and Pareto screening. Applied to a real urban district, the workflow reproduces corridor-biased commercial siting and industrial belts while concentrating intensity on highly connected blocks. Policy sampling via multi-objective screening yields Pareto-efficient plans that reconcile accessibility gains with deviations from target land-share and construction-share structures. The contribution is twofold: methodologically, it translates familiar space-syntax measures into cluster-aware, rule-governed land-use and FAR assignment with explicit guarantees (scale-legible radii, parcel minima, and an average-FAR anchor). Practically, it offers planners a transparent instrument for counterfactual testing and negotiated trade-offs at neighborhood/district/city scales.

</details>


### [415] [Bayesian Methods for the Navier-Stokes Equations](https://arxiv.org/abs/2602.02945)
*Nicholas Polson,Vadim Sokolov*

Main category: stat.CO

TL;DR: 本文开发了用于求解含不确定性的不可压缩Navier - Stokes方程的贝叶斯方法，介绍不同维度的求解思路和方法优势，还讨论了非高斯误差模型。


<details>
  <summary>Details</summary>
Motivation: 对不可压缩Navier - Stokes方程进行数值求解并量化不确定性。

Method: 将离散的Navier - Stokes动力学视为状态空间模型，把数值解视为后验计算；利用随机表示进行Monte Carlo求解；构建随机Navier - Stokes模型并描述基于粒子和集合的贝叶斯工作流；基于粒子学习进行参数学习；支持顺序观测更新；讨论基于正态方差 - 均值混合的非高斯误差模型。

Result: 可稳定地进行参数学习，避免了朴素顺序重要性采样的权重崩溃问题；在有部分观测时可支持顺序观测更新；通过潜在尺度增强实现条件高斯更新。

Conclusion: 所提出的贝叶斯方法在求解含不确定性的不可压缩Navier - Stokes方程上有计算优势和多种能力。

Abstract: We develop a Bayesian methodology for numerical solution of the incompressible Navier--Stokes equations with quantified uncertainty. The central idea is to treat discretized Navier--Stokes dynamics as a state-space model and to view numerical solution as posterior computation: priors encode physical structure and modeling error, and the solver outputs a distribution over states and quantities of interest rather than a single trajectory. In two dimensions, stochastic representations (Feynman--Kac and stochastic characteristics for linear advection--diffusion with prescribed drift) motivate Monte Carlo solvers and provide intuition for uncertainty propagation. In three dimensions, we formulate stochastic Navier--Stokes models and describe particle-based and ensemble-based Bayesian workflows for uncertainty propagation in spectral discretizations. A key computational advantage is that parameter learning can be performed stably via particle learning: marginalization and resample--propagate (one-step smoothing) constructions avoid the weight-collapse that plagues naive sequential importance sampling on static parameters. When partial observations are available, the same machinery supports sequential observational updating as an additional capability. We also discuss non-Gaussian (heavy-tailed) error models based on normal variance-mean mixtures, which yield conditionally Gaussian updates via latent scale augmentation.

</details>


### [416] [MARADONER: Motif Activity Response Analysis Done Right](https://arxiv.org/abs/2602.03343)
*Georgy Meshcheryakov,Andrey I. Buyan*

Main category: stat.CO

TL;DR: 介绍了统计框架MARADONER用于转录因子活性推断，相比经典MARA有改进。


<details>
  <summary>Details</summary>
Motivation: 从高通量转录或开放染色质分析中推断转录因子活性是系统生物学长期挑战，识别高活性主调控因子可助力对不同条件下基因表达等变化的机制解释。

Method: 提出统计框架MARADONER及软件实现，利用启动子序列特征和启动子或基因水平的活性表达估计。

Result: 相比经典MARA，采用无偏方差参数估计和偏置调整的固定效应似然估计，提高拟合优度和活性估计准确性，能处理基序得分和活性估计的异方差性。

Conclusion: MARADONER在转录因子活性推断方面具有优势。

Abstract: Inferring the activities of transcription factors from high-throughput transcriptomic or open chromatin profiling, such as RNA-/CAGE-/ATAC-Seq, is a long-standing challenge in systems biology. Identification of highly active master regulators enables mechanistic interpretation of differential gene expression, chromatin state changes, or perturbation responses across conditions, cell types, and diseases. Here, we describe MARADONER, a statistical framework and its software implementation for motif activity response analysis (MARA), utilizing the sequence-level features obtained with pattern matching (motif scanning) of individual promoters and promoter- or gene-level activity or expression estimates. Compared to the classic MARA, MARADONER (MARA-done-right) employs an unbiased variance parameter estimation and a bias-adjusted likelihood estimation of fixed effects, thereby enhancing goodness-of-fit and the accuracy of activity estimation. Further, MARADONER is capable of accounting for heteroscedasticity of motif scores and activity estimates.

</details>


### [417] [On the Convergence of Wasserstein Gradient Descent for Sampling](https://arxiv.org/abs/2602.03413)
*Van Chien Ta,Thi Mai Hong Chu,Minh-Ngoc Tran*

Main category: stat.CO

TL;DR: 研究Wasserstein概率测度空间上的KL泛函优化，开发基于Wasserstein梯度下降（WGD）的采样框架，有理论基础和数值实验支持。


<details>
  <summary>Details</summary>
Motivation: 探索Wasserstein空间上的优化和采样方法，为测度空间上基于优化的采样方法提供理论基础。

Method: 开发基于WGD的采样框架，确定WGD收敛的Wasserstein空间子类，构建基于粒子的WGD算法并通过得分匹配估计得分函数。

Result: WGD能很好近似多种复杂目标分布，对标准MCMC和参数变分贝叶斯方法构成挑战的分布也适用。

Conclusion: WGD为高维或多模态场景下的可扩展贝叶斯推理提供了有前景且灵活的替代方案。

Abstract: This paper studies the optimization of the KL functional on the Wasserstein space of probability measures, and develops a sampling framework based on Wasserstein gradient descent (WGD). We identify two important subclasses of the Wasserstein space for which the WGD scheme is guaranteed to converge, thereby providing new theoretical foundations for optimization-based sampling methods on measure spaces. For practical implementation, we construct a particle-based WGD algorithm in which the score function is estimated via score matching. Through a series of numerical experiments, we demonstrate that WGD can provide good approximation to a variety of complex target distributions, including those that pose substantial challenges for standard MCMC and parametric variational Bayes methods. These results suggest that WGD offers a promising and flexible alternative for scalable Bayesian inference in high-dimensional or multimodal settings.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [418] [From Separate Compilation to Sound Language Composition](https://arxiv.org/abs/2602.03777)
*Federico Bruzzone,Walter Cazzola,Luca Favalli*

Main category: cs.PL

TL;DR: 本文介绍了针对Neverlang语言工作台的静态分析工具nlgcheck，它能在编译时检测潜在运行时错误，实验证明其有效增强了健壮性。


<details>
  <summary>Details</summary>
Motivation: 现有语言工作台在处理单独编译时放松约束影响工件可重用性和集成，现有方法以编译时正确性为代价换取灵活性，会导致运行时错误。

Method: 引入基于数据流分析的静态分析工具nlgcheck，在编译时检测潜在运行时错误。

Result: 使用变异测试的实验评估表明，nlgcheck能有效增强健壮性，不牺牲模块化和灵活性，性能不影响日常开发。

Conclusion: nlgcheck在保持单独编译的同时能提供强静态正确性保证，可用于日常开发。

Abstract: The development of programming languages involves complex theoretical and practical challenges, particularly when addressing modularity and reusability through language extensions. While language workbenches aim to enable modular development under the constraints of the language extension problem, one critical constraint -- separate compilation -- is often relaxed due to its complexity. However, this relaxation undermines artifact reusability and integration with common dependency systems. A key difficulty under separate compilation arises from managing attribute grammars, as extensions may introduce new attributes that invalidate previously generated abstract syntax tree structures. Existing approaches, such as the use of dynamic maps in the Neverlang workbench, favor flexibility at the cost of compile-time correctness, leading to potential runtime errors due to undefined attributes. This work addresses this issue by introducing nlgcheck, a theoretically sound static analysis tool based on data-flow analysis for the Neverlang language workbench. nlgcheck detects potential runtime errors -- such as undefined attribute accesses -- at compile time, preserving separate compilation while maintaining strong static correctness guarantees. Experimental evaluation using mutation testing on Neverlang-based projects demonstrates that nlgcheck effectively enhances robustness without sacrificing modularity or flexibility and with a level of performance that does not impede its adoption in daily development activities.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [419] [Efficient Edge Rewiring Strategies for Enhancing PageRank Fairness](https://arxiv.org/abs/2602.02512)
*Changan Liu,Haoxin Sun,Ahad N. Zehmakan,Zhongzhi Zhang*

Main category: cs.SI

TL;DR: 研究社交网络不公平性，提出修改网络结构提升PageRank公平性的问题，设计线性时间算法，实验显示该算法优于现有算法，能快速处理大规模网络。


<details>
  <summary>Details</summary>
Motivation: 解决社交网络中特定群体因网络位置不利而在获取重要信息方面的不公平问题，提升PageRank公平性。

Method: 基于贪心方法，利用有根生成森林的快速采样技术设计线性时间算法。

Result: 在各种真实网络数据上的实验表明，提出的算法显著优于现有算法，能在几分钟内为百万节点网络生成准确解。

Conclusion: 提出的算法在提升PageRank公平性方面表现出色，能高效处理大规模网络。

Abstract: We study the notion of unfairness in social networks, where a group such as females in a male-dominated industry are disadvantaged in access to important information, e.g. job posts, due to their less favorable positions in the network. We investigate a well-established network-based formulation of fairness called PageRank fairness, which refers to a fair allocation of the PageRank weights among distinct groups. Our goal is to enhance the PageRank fairness by modifying the underlying network structure. More precisely, we study the problem of maximizing PageRank fairness with respect to a disadvantaged group, when we are permitted to rewire a fixed number of edges in the network. Building on a greedy approach, we leverage techniques from fast sampling of rooted spanning forests to devise an effective linear-time algorithm for this problem. To evaluate the accuracy and performance of our proposed algorithm, we conduct a large set of experiments on various real-world network data. Our experiments demonstrate that the proposed algorithm significantly outperforms the existing ones. Our algorithm is capable of generating accurate solutions for networks of million nodes in just a few minutes.

</details>


### [420] [GASTON: Graph-Aware Social Transformer for Online Networks](https://arxiv.org/abs/2602.02524)
*Olha Wloch,Liam Hebert,Robin Cohen,Lukasz Golab*

Main category: cs.SI

TL;DR: 提出GASTON模型检测在线社区有害内容，其基于社区和用户嵌入，实验表明效果优于基线。


<details>
  <summary>Details</summary>
Motivation: 在线社区存在有害内容，但因文本和社交规范因素，检测困难。

Method: 提出GASTON模型，采用对比初始化策略预训练社区嵌入。

Result: GASTON生成的嵌入在压力检测、毒性评分和规范违规等任务中优于现有基线。

Conclusion: GASTON模型能有效检测在线社区有害内容。

Abstract: Online communities have become essential places for socialization and support, yet they also possess toxicity, echo chambers, and misinformation. Detecting this harmful content is difficult because the meaning of an online interaction stems from both what is written (textual content) and where it is posted (social norms). We propose GASTON (Graph-Aware Social Transformer for Online Networks), which learns text and user embeddings that are grounded in their local norms, providing the necessary context for downstream tasks. The heart of our solution is a contrastive initialization strategy that pretrains community embeddings based on user membership patterns, capturing a community's user base before processing any text. This allows GASTON to distinguish between communities (e.g., a support group vs. a hate group) based on who interacts there, even if they share similar vocabulary. Experiments on tasks such as stress detection, toxicity scoring, and norm violation demonstrate that the embeddings produced by GASTON outperform state-of-the-art baselines.

</details>


### [421] [Community Norms in the Spotlight: Enabling Task-Agnostic Unsupervised Pre-Training to Benefit Online Social Media](https://arxiv.org/abs/2602.02525)
*Liam Hebert,Lucas Kopp,Robin Cohen*

Main category: cs.SI

TL;DR: 本文提出从任务特定微调转向无监督预训练的范式转变，以解决在线社交平台建模问题。


<details>
  <summary>Details</summary>
Motivation: 现有Discussion Transformers依赖高质量人工标注数据集，潜力受限，需要解决数据稀缺问题并理解AI系统决策背后的社会规范。

Method: 倡导从任务特定微调转向基于社区规范的无监督预训练。

Result: 未提及具体结果。

Conclusion: 这种方向为AI造福社会提供了很多机会。

Abstract: Modelling the complex dynamics of online social platforms is critical for addressing challenges such as hate speech and misinformation. While Discussion Transformers, which model conversations as graph structures, have emerged as a promising architecture, their potential is severely constrained by reliance on high-quality, human-labelled datasets. In this paper, we advocate a paradigm shift from task-specific fine-tuning to unsupervised pretraining, grounded in an entirely novel consideration of community norms. We posit that this framework not only mitigates data scarcity but also enables interpretation of the social norms underlying the decisions made by such an AI system. Ultimately, we believe that this direction offers many opportunities for AI for Social Good.

</details>


### [422] [CaST: Causal Discovery via Spatio-Temporal Graphs in Disaster Tweets](https://arxiv.org/abs/2602.02601)
*Hieu Duong,Eugene Levin,Todd Gary,Long Nguyen*

Main category: cs.SI

TL;DR: 提出CaST框架用于灾害领域社交媒体文本因果发现，结合语义、时空信息，实验显示性能优越。


<details>
  <summary>Details</summary>
Motivation: 现有因果发现方法常忽略语义、空间和时间上下文的相互作用，需要在灾害领域社交媒体文本进行有效因果发现。

Method: 提出CaST框架，用预训练大语言模型整合语义相似性和时空邻近性，构建事件图，用多头图注意力网络学习因果关系。

Result: CaST在实验中性能优于传统和最先进方法，消融研究表明纳入时空信号提高召回率和训练稳定性。

Conclusion: 将时空推理整合到事件图中能实现更稳健、可解释的灾害相关社交媒体文本因果发现。

Abstract: Understanding causality between real-world events from social media is essential for situational awareness, yet existing causal discovery methods often overlook the interplay between semantic, spatial, and temporal contexts. We propose CaST: Causal Discovery via Spatio-Temporal Graphs, a unified framework for causal discovery in disaster domain that integrates semantic similarity and spatio-temporal proximity using Large Language Models (LLMs) pretrained on disaster datasets. CaST constructs an event graph for each window of tweets. Each event extracted from tweets is represented as a node embedding enriched with its contextual semantics, geographic coordinates, and temporal features. These event nodes are then connected to form a spatio-temporal event graph, which is processed using a multi-head Graph Attention Network (GAT) \cite{gat} to learn directed causal relationships. We construct an in-house dataset of approximately 167K disaster-related tweets collected during Hurricane Harvey and annotated following the MAVEN-ERE schema. Experimental results show that CaST achieves superior performance over both traditional and state-of-the-art methods. Ablation studies further confirm that incorporating spatial and temporal signals substantially improves both recall and stability during training. Overall, CaST demonstrates that integrating spatio-temporal reasoning into event graphs enables more robust and interpretable causal discovery in disaster-related social media text.

</details>


### [423] [Gender Dynamics and Homophily in a Social Network of LLM Agents](https://arxiv.org/abs/2602.02606)
*Faezeh Fadaei,Jenny Carla Moran,Taha Yasseri*

Main category: cs.SI

TL;DR: 研究Chirper.ai平台上AI聊天机器人的性别表现，发现其性别表现具流动性但网络有基于性别的同质性，社会选择和影响机制共同作用，无身体情况下文化也使性别表现分类。


<details>
  <summary>Details</summary>
Motivation: 探究生成式人工智能和大语言模型在大规模网络交互中身份表现的发展情况。

Method: 以Chirper.ai平台为研究对象，分析超70000个智能体、约1.4亿条帖子及一年的关注网络，根据文本输出为智能体分配每周性别分数。

Result: 智能体性别表现具流动性，网络有基于性别的同质性，社会选择和社会影响机制共同塑造LLMs交互结构和演变。

Conclusion: 即使无身体，文化对性别表现的影响会导致基于性别的分类，对LLM应用有重要意义。

Abstract: Generative artificial intelligence and large language models (LLMs) are increasingly deployed in interactive settings, yet we know little about how their identity performance develops when they interact within large-scale networks. We address this by examining Chirper.ai, a social media platform similar to X but composed entirely of autonomous AI chatbots. Our dataset comprises over 70,000 agents, approximately 140 million posts, and the evolving followership network over one year. Based on agents' text production, we assign weekly gender scores to each agent. Results suggest that each agent's gender performance is fluid rather than fixed. Despite this fluidity, the network displays strong gender-based homophily, as agents consistently follow others performing gender similarly. Finally, we investigate whether these homophilic connections arise from social selection, in which agents choose to follow similar accounts, or from social influence, in which agents become more similar to their followees over time. Consistent with human social networks, we find evidence that both mechanisms shape the structure and evolution of interactions among LLMs. Our findings suggest that, even in the absence of bodies, cultural entraining of gender performance leads to gender-based sorting. This has important implications for LLM applications in synthetic hybrid populations, social simulations, and decision support.

</details>


### [424] [Recommender system in X inadvertently profiles ideological positions of users](https://arxiv.org/abs/2602.02624)
*Paul Bouchaud,Pedro Ramaciotti*

Main category: cs.SI

TL;DR: 利用数据捐赠项目研究社交媒体推荐系统中用户政治和社会属性处理，发现推荐系统用户排序与左右立场高度相关，探索新推荐方法。


<details>
  <summary>Details</summary>
Motivation: 以往研究多关注推荐项质量和策略影响，本文旨在研究现实中推荐者如何在AI系统黑盒中学习、表示和处理用户政治和社会属性。

Method: 通过数据捐赠项目收集超250万条好友推荐数据，利用推荐器架构知识推断用户嵌入空间位置，结合政治调查数据校准的意识形态缩放分析用户政治立场。

Result: 平台推荐系统产生的用户空间排序与左右立场高度相关（Pearson rho=0.887, p值 < 0.0001），且不能由社会人口学属性解释。

Conclusion: 研究为研究人机系统交互提供新可能，引发数据隐私监管中算法画像法律定义问题，探索了限制推荐器政治信息的新推荐方法以保障隐私。

Abstract: Studies on recommendations in social media have mainly analyzed the quality of recommended items (e.g., their diversity or biases) and the impact of recommendation policies (e.g., in comparison with purely chronological policies). We use a data donation program, collecting more than 2.5 million friend recommendations made to 682 volunteers on X over a year, to study instead how real-world recommenders learn, represent and process political and social attributes of users inside the so-called black boxes of AI systems. Using publicly available knowledge on the architecture of the recommender, we inferred the positions of recommended users in its embedding space. Leveraging ideology scaling calibrated with political survey data, we analyzed the political position of users in our study (N=26,509 among volunteers and recommended contacts) among several attributes, including age and gender. Our results show that the platform's recommender system produces a spatial ordering of users that is highly correlated with their Left-Right positions (Pearson rho=0.887, p-value < 0.0001), and that cannot be explained by socio-demographic attributes. These results open new possibilities for studying the interaction between human and AI systems. They also raise important questions linked to the legal definition of algorithmic profiling in data privacy regulation by blurring the line between active and passive profiling. We explore new constrained recommendation methods enabled by our results, limiting the political information in the recommender as a potential tool for privacy compliance capable of preserving recommendation relevance.

</details>


### [425] [Beyond Content: Behavioral Policies Reveal Actors in Information Operations](https://arxiv.org/abs/2602.02838)
*Philipp J. Schneider,Lanqin Yuan,Marian-Andrei Rizoiu*

Main category: cs.SI

TL;DR: 传统在线影响操作检测方法变脆弱，提出平台无关框架，以行为策略识别恶意用户，实验显示其优于内容模型，能更早检测且更具鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统基于内容分析或网络特征的在线影响操作检测方法，因生成模型、平台数据限制等因素变得脆弱，需要新方法。

Method: 将用户活动建模为顺序决策过程，构建平台无关框架，分析Reddit用户活动。

Result: 基于活动的表示在检测恶意账户上优于内容模型，策略分类器区分恶意用户和普通用户时F1值更高，能更早检测且在规避策略或数据损坏时表现更好。

Conclusion: 行为动态编码了稳定、有区分性的操纵信号，可用于合成内容和数据访问受限时代的跨平台检测。

Abstract: The detection of online influence operations -- coordinated campaigns by malicious actors to spread narratives -- has traditionally depended on content analysis or network features. These approaches are increasingly brittle as generative models produce convincing text, platforms restrict access to behavioral data, and actors migrate to less-regulated spaces. We introduce a platform-agnostic framework that identifies malicious actors from their behavioral policies by modeling user activity as sequential decision processes. We apply this approach to 12,064 Reddit users, including 99 accounts linked to the Russian Internet Research Agency in Reddit's 2017 transparency report, analyzing over 38 million activity steps from 2015-2018. Activity-based representations, which model how users act rather than what they post, consistently outperform content models in detecting malicious accounts. When distinguishing trolls -- users engaged in coordinated manipulation -- from ordinary users, policy-based classifiers achieve a median macro-$F_1$ of 94.9%, compared to 91.2% for text embeddings. Policy features also enable earlier detection from short traces and degrade more gracefully under evasion strategies or data corruption. These findings show that behavioral dynamics encode stable, discriminative signals of manipulation and point to resilient, cross-platform detection strategies in the era of synthetic content and limited data access.

</details>


### [426] [An Empirical Study of Collective Behaviors and Social Dynamics in Large Language Model Agents](https://arxiv.org/abs/2602.03775)
*Farnoosh Hashemi,Michael W. Macy*

Main category: cs.SI

TL;DR: 研究LLM驱动的社交媒体平台Chirper.ai，分析其行为并提出防止有害活动方法


<details>
  <summary>Details</summary>
Motivation: 探索与其他智能体的反复交互是否会放大LLM的偏差或导致排他行为

Method: 分析Chirper.ai平台上700万条帖子和3.2万个LLM智能体一年的交互情况

Result: LLM社交网络有同质性和社会影响现象，有毒帖子结构模式与人类不同

Conclusion: 提出Chain of Social Thought (CoST)方法提醒LLM避免有害发帖

Abstract: Large Language Models (LLMs) increasingly mediate our social, cultural, and political interactions. While they can simulate some aspects of human behavior and decision-making, it is still underexplored whether repeated interactions with other agents amplify their biases or lead to exclusionary behaviors. To this end, we study Chirper.ai-an LLM-driven social media platform-analyzing 7M posts and interactions among 32K LLM agents over a year. We start with homophily and social influence among LLMs, learning that similar to humans', their social networks exhibit these fundamental phenomena. Next, we study the toxic language of LLMs, its linguistic features, and their interaction patterns, finding that LLMs show different structural patterns in toxic posting than humans. After studying the ideological leaning in LLMs posts, and the polarization in their community, we focus on how to prevent their potential harmful activities. We present a simple yet effective method, called Chain of Social Thought (CoST), that reminds LLM agents to avoid harmful posting.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [427] [From Speech-to-Spatial: Grounding Utterances on A Live Shared View with Augmented Reality](https://arxiv.org/abs/2602.03059)
*Yoonsang Kim,Divyansh Pradhan,Devshree Jadeja,Arie Kaufman*

Main category: cs.HC

TL;DR: 介绍了将语音指令转化为AR引导的Speech - to - Spatial框架，该框架仅依赖语音输入，评估显示其能提升任务效率等。


<details>
  <summary>Details</summary>
Motivation: 现有系统依赖额外线索或人工标注，通过对语音引用模式研究，想要设计仅依赖语音输入的指称消歧框架。

Method: 分析人们指定目标的常见方式并映射到以对象为中心的关系图，解析话语中的指称线索并渲染为AR视觉引导。

Result: 在远程引导和意图消歧场景中展示了系统用例，相比传统仅语音基线，提高了任务效率、降低认知负荷、增强了可用性。

Conclusion: Speech - to - Spatial可将无形的语音指令转化为可视可操作的引导。

Abstract: We introduce Speech-to-Spatial, a referent disambiguation framework that converts verbal remote-assistance instructions into spatially grounded AR guidance. Unlike prior systems that rely on additional cues (e.g., gesture, gaze) or manual expert annotations, Speech-to-Spatial infers the intended target solely from spoken references (speech input). Motivated by our formative study of speech referencing patterns, we characterize recurring ways people specify targets (Direct Attribute, Relational, Remembrance, and Chained) and ground them to our object-centric relational graph. Given an utterance, referent cues are parsed and rendered as persistent in-situ AR visual guidance, reducing iterative micro-guidance ("a bit more to the right", "now, stop.") during remote guidance. We demonstrate the use cases of our system with remote guided assistance and intent disambiguation scenarios. Our evaluation shows that Speechto-Spatial improves task efficiency, reduces cognitive load, and enhances usability compared to a conventional voice-only baseline, transforming disembodied verbal instruction into visually explainable, actionable guidance on a live shared view.

</details>


### [428] [Intelligent Front-End Personalization: AI-Driven UI Adaptation](https://arxiv.org/abs/2602.03154)
*Mona Rajhans*

Main category: cs.HC

TL;DR: 论文提出AI驱动的动态前端个性化方法，含三种策略并给出实现细节与评估方法。


<details>
  <summary>Details</summary>
Motivation: 传统前端个性化依赖静态设计或规则适配，无法充分捕捉用户行为模式。

Method: 提出三种策略，包括基于用户路径预测的动态布局适配、通过强化学习的内容优先级排序和AI驱动与规则式个性化的对比分析。

Result: 提供技术实现细节、算法、系统架构和评估方法，证明可行性和性能提升。

Conclusion: AI驱动的动态前端个性化方法可行且有性能优势。

Abstract: Front-end personalization has traditionally relied on static designs or rule-based adaptations, which fail to fully capture user behavior patterns. This paper presents an AI driven approach for dynamic front-end personalization, where UI layouts, content, and features adapt in real-time based on predicted user behavior. We propose three strategies: dynamic layout adaptation using user path prediction, content prioritization through reinforcement learning, and a comparative analysis of AI-driven vs. rule-based personalization. Technical implementation details, algorithms, system architecture, and evaluation methods are provided to illustrate feasibility and performance gains.

</details>


### [429] [Simulating Human Audiovisual Search Behavior](https://arxiv.org/abs/2602.02790)
*Hyunsung Cho,Xuejing Luo,Byungjoo Lee,David Lindlbauer,Antti Oulasvirta*

Main category: cs.HC

TL;DR: 提出Sensonaut计算模型用于体现式视听搜索，验证其能复现人类搜索特征并为设计视听界面提供信息。


<details>
  <summary>Details</summary>
Motivation: 现有视听搜索模型常孤立处理感知和行动，忽略人们如何自适应协调运动和感官策略。

Method: 提出Sensonaut模型，将其表述为部分可观测下的资源理性决策问题。

Result: 模型验证表明能复现任务复杂性、遮挡和干扰下搜索时间和努力的自适应缩放以及人类典型错误。

Conclusion: 模型模拟为设计降低搜索成本和认知负担的视听界面提供参考。

Abstract: Locating a target based on auditory and visual cues$\unicode{x2013}$such as finding a car in a crowded parking lot or identifying a speaker in a virtual meeting$\unicode{x2013}$requires balancing effort, time, and accuracy under uncertainty. Existing models of audiovisual search often treat perception and action in isolation, overlooking how people adaptively coordinate movement and sensory strategies. We present Sensonaut, a computational model of embodied audiovisual search. The core assumption is that people deploy their body and sensory systems in ways they believe will most efficiently improve their chances of locating a target, trading off time and effort under perceptual constraints. Our model formulates this as a resource-rational decision-making problem under partial observability. We validate the model against newly collected human data, showing that it reproduces both adaptive scaling of search time and effort under task complexity, occlusion, and distraction, and characteristic human errors. Our simulation of human-like resource-rational search informs the design of audiovisual interfaces that minimize search cost and cognitive load.

</details>


### [430] [Towards Considerate Embodied AI: Co-Designing Situated Multi-Site Healthcare Robots from Abstract Concepts to High-Fidelity Prototypes](https://arxiv.org/abs/2602.03054)
*Yuanchen Bai,Ruixiang Han,Niti Parikh,Wendy Ju,Angelique Taylor*

Main category: cs.HC

TL;DR: 本文通过14周工作坊研究具身AI在医疗场景的协同设计，提出协同设计更贴心具身AI的八项准则。


<details>
  <summary>Details</summary>
Motivation: 以往研究未将多学科协作、迭代原型设计等融入持续协同设计过程，且研究场景局限、成果通用性不足，需改进。

Method: 开展14周工作坊，让22人多学科团队围绕具身AI减轻医疗场景非增值任务负担进行研究。

Result: 从抽象头脑风暴到高保真原型的迭代过程，在教育支撑下，让参与者理解现实权衡，生成更具可部署性的解决方案。

Conclusion: 提出协同设计更贴心具身AI的八项准则。

Abstract: Co-design is essential for grounding embodied artificial intelligence (AI) systems in real-world contexts, especially high-stakes domains such as healthcare. While prior work has explored multidisciplinary collaboration, iterative prototyping, and support for non-technical participants, few have interwoven these into a sustained co-design process. Such efforts often target one context and low-fidelity stages, limiting the generalizability of findings and obscuring how participants' ideas evolve. To address these limitations, we conducted a 14-week workshop with a multidisciplinary team of 22 participants, centered around how embodied AI can reduce non-value-added task burdens in three healthcare settings: emergency departments, long-term rehabilitation facilities, and sleep disorder clinics. We found that the iterative progression from abstract brainstorming to high-fidelity prototypes, supported by educational scaffolds, enabled participants to understand real-world trade-offs and generate more deployable solutions. We propose eight guidelines for co-designing more considerate embodied AI: attuned to context, responsive to social dynamics, mindful of expectations, and grounded in deployment. Project Page: https://byc-sophie.github.io/Towards-Considerate-Embodied-AI/

</details>


### [431] ["I'm happy even though it's not real": GenAI Photo Editing as a Remembering Experience](https://arxiv.org/abs/2602.03104)
*Yufeng Wu,Qing Li,Elise van den Hoven,Baki Kocaballi*

Main category: cs.HC

TL;DR: 研究探索人们使用生成式AI编辑个人照片的方式与原因，以及对记忆体验的影响，发现参与者重情感记忆而非事实准确，明确编辑利弊并提出设计建议。


<details>
  <summary>Details</summary>
Motivation: 生成式AI融入个人设备照片应用，可能影响照片所代表的记忆，需探究人们使用其编辑个人照片的情况及对记忆体验的塑造。

Method: 对12名参与者进行两阶段定性研究，先按记忆体验维度用生成式AI工具进行照片编辑，后进行半结构化访谈。

Result: 参与者更看重情感记忆；环境元素易修改，涉及人物身份的编辑不可接受；编辑有正负影响，其过程也成为记忆体验。

Conclusion: 讨论生成式AI编辑用于记忆目的的利弊，提出负责任的生成式AI设计建议。

Abstract: Generative Artificial Intelligence (GenAI) is increasingly integrated into photo applications on personal devices, making editing photographs easier than ever while potentially influencing the memories they represent. This study explores how and why people use GenAI to edit personal photos and how this shapes their remembering experience. We conducted a two-phase qualitative study with 12 participants: a photo editing session using a GenAI tool guided by the Remembering Experience (RX) dimensions, followed by semi-structured interviews where participants reflected on the editing process and results. Findings show that participants prioritised felt memory over factual accuracy. For different photo elements, environments were modified easily, however, editing was deemed unacceptable if it touched upon a person's identity. Editing processes brought positive and negative impacts, and itself also became a remembering experience. We further discuss potential benefits and risks of GenAI editing for remembering purposes and propose design implications for responsible GenAI.

</details>


### [432] [PrevizWhiz: Combining Rough 3D Scenes and 2D Video to Guide Generative Video Previsualization](https://arxiv.org/abs/2602.03838)
*Erzhen Hu,Frederik Brudy,David Ledo,George Fitzmaurice,Fraser Anderson*

Main category: cs.HC

TL;DR: 提出PrevizWhiz系统，结合粗糙3D场景与生成式图像和视频模型创建风格化视频预览，经测试可降低技术门槛等，也暴露AI辅助电影制作问题。


<details>
  <summary>Details</summary>
Motivation: 传统电影预制作方法在效率和表现力上有折衷，手绘故事板缺乏空间精度，3D预可视化需要专业知识和高质量资产，需新方法解决。

Method: 提出PrevizWhiz系统，将粗糙3D场景与生成式图像和视频模型结合，工作流程集成帧级图像重塑、基于时间的编辑和提炼成高保真视频片段。

Result: 经与电影制作人的研究表明，系统降低技术门槛、加速创意迭代、有效弥合沟通差距。

Conclusion: 系统有积极作用，但也凸显AI辅助电影制作在连续性、作者身份和伦理考量方面的挑战。

Abstract: In pre-production, filmmakers and 3D animation experts must rapidly prototype ideas to explore a film's possibilities before fullscale production, yet conventional approaches involve trade-offs in efficiency and expressiveness. Hand-drawn storyboards often lack spatial precision needed for complex cinematography, while 3D previsualization demands expertise and high-quality rigged assets. To address this gap, we present PrevizWhiz, a system that leverages rough 3D scenes in combination with generative image and video models to create stylized video previews. The workflow integrates frame-level image restyling with adjustable resemblance, time-based editing through motion paths or external video inputs, and refinement into high-fidelity video clips. A study with filmmakers demonstrates that our system lowers technical barriers for film-makers, accelerates creative iteration, and effectively bridges the communication gap, while also surfacing challenges of continuity, authorship, and ethical consideration in AI-assisted filmmaking.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [433] [MathlibLemma: Folklore Lemma Generation and Benchmark for Formal Mathematics](https://arxiv.org/abs/2602.02561)
*Xinyu Liu,Zixuan Xie,Amir Moeini,Claire Chen,Shuze Daniel Liu,Yu Meng,Aidong Zhang,Shangtong Zhang*

Main category: cs.LO

TL;DR: 引入MathlibLemma多智能体系统自动发现和形式化数学民间引理，构建基准，推动形式数学库自我进化。


<details>
  <summary>Details</summary>
Motivation: Mathlib缺少许多民间引理，限制了Lean作为数学家日常工具的可用性。

Method: 引入基于大语言模型的多智能体系统MathlibLemma来挖掘和形式化民间引理。

Result: 生成了经过验证的民间引理库，部分已并入Mathlib，构建了含4028个类型检查Lean语句的MathlibLemma基准。

Conclusion: 该工作将大语言模型从被动消费者转变为主动贡献者，为形式数学库的自我进化建立了建设性方法。

Abstract: While the ecosystem of Lean and Mathlib has enjoyed celebrated success in formal mathematical reasoning with the help of large language models (LLMs), the absence of many folklore lemmas in Mathlib remains a persistent barrier that limits Lean's usability as an everyday tool for mathematicians like LaTeX or Maple. To address this, we introduce MathlibLemma, the first LLM-based multi-agent system to automate the discovery and formalization of mathematical folklore lemmas. This framework constitutes our primary contribution, proactively mining the missing connective tissue of mathematics. Its efficacy is demonstrated by the production of a verified library of folklore lemmas, a subset of which has already been formally merged into the latest build of Mathlib, thereby validating the system's real-world utility and alignment with expert standards. Leveraging this pipeline, we further construct the MathlibLemma benchmark, a suite of 4,028 type-checked Lean statements spanning a broad range of mathematical domains. By transforming the role of LLMs from passive consumers to active contributors, this work establishes a constructive methodology for the self-evolution of formal mathematical libraries.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [434] [BIRDTurk: Adaptation of the BIRD Text-to-SQL Dataset to Turkish](https://arxiv.org/abs/2602.03633)
*Burak Aktaş,Mehmet Can Baytekin,Süha Kağan Köse,Ömer İlbilgi,Elif Özge Yılmaz,Çağrı Toraman,Bilge Kaan Görür*

Main category: cs.CL

TL;DR: 介绍首个土耳其语版BIRDTurk基准，评估多种方法，揭示土耳其语影响及各方法表现，发布数据支持研究


<details>
  <summary>Details</summary>
Motivation: 探索文本到SQL系统在形态丰富、低资源语言（土耳其语）中的表现

Method: 构建BIRDTurk基准，用控制翻译流程；验证翻译质量；用BIRDTurk评估推理提示、代理多阶段推理和监督微调

Result: 土耳其语导致性能下降，代理推理跨语言鲁棒性强，监督微调对标准多语言基线有挑战但在现代指令调优模型中有效

Conclusion: BIRDTurk为跨语言文本到SQL评估提供测试平台，发布数据支持未来研究

Abstract: Text-to-SQL systems have achieved strong performance on English benchmarks, yet their behavior in morphologically rich, low-resource languages remains largely unexplored. We introduce BIRDTurk, the first Turkish adaptation of the BIRD benchmark, constructed through a controlled translation pipeline that adapts schema identifiers to Turkish while strictly preserving the logical structure and execution semantics of SQL queries and databases. Translation quality is validated on a sample size determined by the Central Limit Theorem to ensure 95% confidence, achieving 98.15% accuracy on human-evaluated samples. Using BIRDTurk, we evaluate inference-based prompting, agentic multi-stage reasoning, and supervised fine-tuning. Our results reveal that Turkish introduces consistent performance degradation, driven by both structural linguistic divergence and underrepresentation in LLM pretraining, while agentic reasoning demonstrates stronger cross-lingual robustness. Supervised fine-tuning remains challenging for standard multilingual baselines but scales effectively with modern instruction-tuned models. BIRDTurk provides a controlled testbed for cross-lingual Text-to-SQL evaluation under realistic database conditions. We release the training and development splits to support future research.

</details>


### [435] [WideSeek: Advancing Wide Research via Multi-Agent Scaling](https://arxiv.org/abs/2602.02636)
*Ziyang Huang,Haolin Ren,Xiaowei Yuan,Jiawei Wang,Zhongtao Jiang,Kun Xu,Shizhu He,Jun Zhao,Kang Liu*

Main category: cs.CL

TL;DR: 搜索智能正从深度研究向广度研究转变，但缺乏专用基准和优化方法。本文从数据管道和智能体优化两方面开展研究，构建了WideSeekBench基准，提出WideSeek架构和统一训练框架，实验证明方法有效。


<details>
  <summary>Details</summary>
Motivation: 搜索智能向广度研究转变，但缺乏专用基准和搜索广度优化方法，阻碍了该领域的发展。

Method: 从数据管道和智能体优化两方面研究。构建WideSeekBench基准，提出WideSeek动态分层多智能体架构，设计统一训练框架并使用端到端强化学习优化系统。

Result: 实验证明了WideSeek和多智能体强化学习的有效性。

Conclusion: 增加智能体数量是推进广度研究范式的一个有前景的方向。

Abstract: Search intelligence is evolving from Deep Research to Wide Research, a paradigm essential for retrieving and synthesizing comprehensive information under complex constraints in parallel. However, progress in this field is impeded by the lack of dedicated benchmarks and optimization methodologies for search breadth. To address these challenges, we take a deep dive into Wide Research from two perspectives: Data Pipeline and Agent Optimization. First, we produce WideSeekBench, a General Broad Information Seeking (GBIS) benchmark constructed via a rigorous multi-phase data pipeline to ensure diversity across the target information volume, logical constraints, and domains. Second, we introduce WideSeek, a dynamic hierarchical multi-agent architecture that can autonomously fork parallel sub-agents based on task requirements. Furthermore, we design a unified training framework that linearizes multi-agent trajectories and optimizes the system using end-to-end RL. Experimental results demonstrate the effectiveness of WideSeek and multi-agent RL, highlighting that scaling the number of agents is a promising direction for advancing the Wide Research paradigm.

</details>


### [436] [Controlling Output Rankings in Generative Engines for LLM-based Search](https://arxiv.org/abs/2602.03608)
*Haibo Jin,Ruoxi Chen,Peiyan Zhang,Yifeng Luo,Huimin Zeng,Man Luo,Haohan Wang*

Main category: cs.CL

TL;DR: 论文提出CORE优化方法控制大语言模型搜索生成引擎的输出排名，通过添加优化内容实现，在ProductBench基准测试中表现良好，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型搜索的推荐受初始检索顺序影响，不利于小企业和独立创作者，需优化输出排名。

Method: 提出CORE方法，以搜索引擎返回内容为影响输出排名的主要手段，通过添加三种优化内容（基于字符串、推理和评论）来优化检索内容。

Result: 在四个有搜索能力的大语言模型上实验，CORE在15个产品类别中平均晋升成功率在不同排名表现良好，@Top - 5为91.4%，@Top - 3为86.6%，@Top - 1为80.3%，优于现有排名操纵方法且保留优化内容流畅性。

Conclusion: CORE方法能有效控制大语言模型搜索生成引擎的输出排名，优于现有方法。

Abstract: The way customers search for and choose products is changing with the rise of large language models (LLMs). LLM-based search, or generative engines, provides direct product recommendations to users, rather than traditional online search results that require users to explore options themselves. However, these recommendations are strongly influenced by the initial retrieval order of LLMs, which disadvantages small businesses and independent creators by limiting their visibility.
  In this work, we propose CORE, an optimization method that \textbf{C}ontrols \textbf{O}utput \textbf{R}ankings in g\textbf{E}nerative Engines for LLM-based search. Since the LLM's interactions with the search engine are black-box, CORE targets the content returned by search engines as the primary means of influencing output rankings. Specifically, CORE optimizes retrieved content by appending strategically designed optimization content to steer the ranking of outputs. We introduce three types of optimization content: string-based, reasoning-based, and review-based, demonstrating their effectiveness in shaping output rankings. To evaluate CORE in realistic settings, we introduce ProductBench, a large-scale benchmark with 15 product categories and 200 products per category, where each product is associated with its top-10 recommendations collected from Amazon's search interface.
  Extensive experiments on four LLMs with search capabilities (GPT-4o, Gemini-2.5, Claude-4, and Grok-3) demonstrate that CORE achieves an average Promotion Success Rate of \textbf{91.4\% @Top-5}, \textbf{86.6\% @Top-3}, and \textbf{80.3\% @Top-1}, across 15 product categories, outperforming existing ranking manipulation methods while preserving the fluency of optimized content.

</details>


### [437] [RAGTurk: Best Practices for Retrieval Augmented Generation in Turkish](https://arxiv.org/abs/2602.03652)
*Süha Kağan Köse,Mehmet Can Baytekin,Burak Aktaş,Bilge Kaan Görür,Evren Ayberk Munis,Deniz Yılmaz,Muhammed Yusuf Kartal,Çağrı Toraman*

Main category: cs.CL

TL;DR: 本文构建土耳其RAG数据集，对RAG流程七阶段进行基准测试，发现复杂方法HyDE准确率高，特定配置性价比高，过度堆叠生成模块会降低性能。


<details>
  <summary>Details</summary>
Motivation: 现有RAG设计指导以英语为中心，缺乏对土耳其语等形态丰富语言的见解。

Method: 构建土耳其RAG数据集，对RAG流程七阶段进行基准测试，不进行特定任务微调。

Result: 复杂方法HyDE准确率达85%，高于基线；特定配置准确率84.6%，成本低。

Conclusion: 过度堆叠生成模块会因扭曲形态线索降低性能，简单查询澄清和强大重排序是有效解决方案。

Abstract: Retrieval-Augmented Generation (RAG) enhances LLM factuality, yet design guidance remains English-centric, limiting insights for morphologically rich languages like Turkish. We address this by constructing a comprehensive Turkish RAG dataset derived from Turkish Wikipedia and CulturaX, comprising question-answer pairs and relevant passage chunks. We benchmark seven stages of the RAG pipeline, from query transformation and reranking to answer refinement, without task-specific fine-tuning. Our results show that complex methods like HyDE maximize accuracy (85%) that is considerably higher than the baseline (78.70%). Also a Pareto-optimal configuration using Cross-encoder Reranking and Context Augmentation achieves comparable performance (84.60%) with much lower cost. We further demonstrate that over-stacking generative modules can degrade performance by distorting morphological cues, whereas simple query clarification with robust reranking offers an effective solution.

</details>


### [438] [Kimi K2.5: Visual Agentic Intelligence](https://arxiv.org/abs/2602.02276)
*Kimi Team,Tongtong Bai,Yifan Bai,Yiping Bao,S. H. Cai,Yuan Cao,Y. Charles,H. S. Che,Cheng Chen,Guanduo Chen,Huarong Chen,Jia Chen,Jiahao Chen,Jianlong Chen,Jun Chen,Kefan Chen,Liang Chen,Ruijue Chen,Xinhao Chen,Yanru Chen,Yanxu Chen,Yicun Chen,Yimin Chen,Yingjiang Chen,Yuankun Chen,Yujie Chen,Yutian Chen,Zhirong Chen,Ziwei Chen,Dazhi Cheng,Minghan Chu,Jialei Cui,Jiaqi Deng,Muxi Diao,Hao Ding,Mengfan Dong,Mengnan Dong,Yuxin Dong,Yuhao Dong,Angang Du,Chenzhuang Du,Dikang Du,Lingxiao Du,Yulun Du,Yu Fan,Shengjun Fang,Qiulin Feng,Yichen Feng,Garimugai Fu,Kelin Fu,Hongcheng Gao,Tong Gao,Yuyao Ge,Shangyi Geng,Chengyang Gong,Xiaochen Gong,Zhuoma Gongque,Qizheng Gu,Xinran Gu,Yicheng Gu,Longyu Guan,Yuanying Guo,Xiaoru Hao,Weiran He,Wenyang He,Yunjia He,Chao Hong,Hao Hu,Jiaxi Hu,Yangyang Hu,Zhenxing Hu,Ke Huang,Ruiyuan Huang,Weixiao Huang,Zhiqi Huang,Tao Jiang,Zhejun Jiang,Xinyi Jin,Yu Jing,Guokun Lai,Aidi Li,C. Li,Cheng Li,Fang Li,Guanghe Li,Guanyu Li,Haitao Li,Haoyang Li,Jia Li,Jingwei Li,Junxiong Li,Lincan Li,Mo Li,Weihong Li,Wentao Li,Xinhang Li,Xinhao Li,Yang Li,Yanhao Li,Yiwei Li,Yuxiao Li,Zhaowei Li,Zheming Li,Weilong Liao,Jiawei Lin,Xiaohan Lin,Zhishan Lin,Zichao Lin,Cheng Liu,Chenyu Liu,Hongzhang Liu,Liang Liu,Shaowei Liu,Shudong Liu,Shuran Liu,Tianwei Liu,Tianyu Liu,Weizhou Liu,Xiangyan Liu,Yangyang Liu,Yanming Liu,Yibo Liu,Yuanxin Liu,Yue Liu,Zhengying Liu,Zhongnuo Liu,Enzhe Lu,Haoyu Lu,Zhiyuan Lu,Junyu Luo,Tongxu Luo,Yashuo Luo,Long Ma,Yingwei Ma,Shaoguang Mao,Yuan Mei,Xin Men,Fanqing Meng,Zhiyong Meng,Yibo Miao,Minqing Ni,Kun Ouyang,Siyuan Pan,Bo Pang,Yuchao Qian,Ruoyu Qin,Zeyu Qin,Jiezhong Qiu,Bowen Qu,Zeyu Shang,Youbo Shao,Tianxiao Shen,Zhennan Shen,Juanfeng Shi,Lidong Shi,Shengyuan Shi,Feifan Song,Pengwei Song,Tianhui Song,Xiaoxi Song,Hongjin Su,Jianlin Su,Zhaochen Su,Lin Sui,Jinsong Sun,Junyao Sun,Tongyu Sun,Flood Sung,Yunpeng Tai,Chuning Tang,Heyi Tang,Xiaojuan Tang,Zhengyang Tang,Jiawen Tao,Shiyuan Teng,Chaoran Tian,Pengfei Tian,Ao Wang,Bowen Wang,Chensi Wang,Chuang Wang,Congcong Wang,Dingkun Wang,Dinglu Wang,Dongliang Wang,Feng Wang,Hailong Wang,Haiming Wang,Hengzhi Wang,Huaqing Wang,Hui Wang,Jiahao Wang,Jinhong Wang,Jiuzheng Wang,Kaixin Wang,Linian Wang,Qibin Wang,Shengjie Wang,Shuyi Wang,Si Wang,Wei Wang,Xiaochen Wang,Xinyuan Wang,Yao Wang,Yejie Wang,Yipu Wang,Yiqin Wang,Yucheng Wang,Yuzhi Wang,Zhaoji Wang,Zhaowei Wang,Zhengtao Wang,Zhexu Wang,Zihan Wang,Zizhe Wang,Chu Wei,Ming Wei,Chuan Wen,Zichen Wen,Chengjie Wu,Haoning Wu,Junyan Wu,Rucong Wu,Wenhao Wu,Yuefeng Wu,Yuhao Wu,Yuxin Wu,Zijian Wu,Chenjun Xiao,Jin Xie,Xiaotong Xie,Yuchong Xie,Yifei Xin,Bowei Xing,Boyu Xu,Jianfan Xu,Jing Xu,Jinjing Xu,L. H. Xu,Lin Xu,Suting Xu,Weixin Xu,Xinbo Xu,Xinran Xu,Yangchuan Xu,Yichang Xu,Yuemeng Xu,Zelai Xu,Ziyao Xu,Junjie Yan,Yuzi Yan,Guangyao Yang,Hao Yang,Junwei Yang,Kai Yang,Ningyuan Yang,Ruihan Yang,Xiaofei Yang,Xinlong Yang,Ying Yang,Yi Yang,Yi Yang,Zhen Yang,Zhilin Yang,Zonghan Yang,Haotian Yao,Dan Ye,Wenjie Ye,Zhuorui Ye,Bohong Yin,Chengzhen Yu,Longhui Yu,Tao Yu,Tianxiang Yu,Enming Yuan,Mengjie Yuan,Xiaokun Yuan,Yang Yue,Weihao Zeng,Dunyuan Zha,Haobing Zhan,Dehao Zhang,Hao Zhang,Jin Zhang,Puqi Zhang,Qiao Zhang,Rui Zhang,Xiaobin Zhang,Y. Zhang,Yadong Zhang,Yangkun Zhang,Yichi Zhang,Yizhi Zhang,Yongting Zhang,Yu Zhang,Yushun Zhang,Yutao Zhang,Yutong Zhang,Zheng Zhang,Chenguang Zhao,Feifan Zhao,Jinxiang Zhao,Shuai Zhao,Xiangyu Zhao,Yikai Zhao,Zijia Zhao,Huabin Zheng,Ruihan Zheng,Shaojie Zheng,Tengyang Zheng,Junfeng Zhong,Longguang Zhong,Weiming Zhong,M. Zhou,Runjie Zhou,Xinyu Zhou,Zaida Zhou,Jinguo Zhu,Liya Zhu,Xinhao Zhu,Yuxuan Zhu,Zhen Zhu,Jingze Zhuang,Weiyu Zhuang,Ying Zou,Xinxing Zu*

Main category: cs.CL

TL;DR: 介绍开源多模态智能体模型Kimi K2.5，强调文本与视觉联合优化，引入Agent Swarm框架，评估显示其在多领域达SOTA，Agent Swarm降低延迟，发布模型检查点。


<details>
  <summary>Details</summary>
Motivation: 推进通用智能体智能的发展。

Method: 采用文本 - 视觉联合预训练、零视觉SFT、文本 - 视觉联合强化学习等技术，引入Agent Swarm自导向并行智能体编排框架。

Result: Kimi K2.5在编码、视觉、推理和智能体任务等多领域达SOTA，Agent Swarm相比单智能体基线最多降低4.5倍延迟。

Conclusion: 发布Kimi K2.5模型检查点，以促进智能体智能的未来研究和实际应用。

Abstract: We introduce Kimi K2.5, an open-source multimodal agentic model designed to advance general agentic intelligence. K2.5 emphasizes the joint optimization of text and vision so that two modalities enhance each other. This includes a series of techniques such as joint text-vision pre-training, zero-vision SFT, and joint text-vision reinforcement learning. Building on this multimodal foundation, K2.5 introduces Agent Swarm, a self-directed parallel agent orchestration framework that dynamically decomposes complex tasks into heterogeneous sub-problems and executes them concurrently. Extensive evaluations show that Kimi K2.5 achieves state-of-the-art results across various domains including coding, vision, reasoning, and agentic tasks. Agent Swarm also reduces latency by up to $4.5\times$ over single-agent baselines. We release the post-trained Kimi K2.5 model checkpoint to facilitate future research and real-world applications of agentic intelligence.

</details>


### [439] [STEMVerse: A Dual-Axis Diagnostic Framework for STEM Reasoning in Large Language Models](https://arxiv.org/abs/2602.02497)
*Xuzhao Li,Xuchen Li,Jian Zhao,Shiyu Hu*

Main category: cs.CL

TL;DR: 当前大语言模型评估范式有局限，提出STEMVerse框架分析其STEM推理能力，揭示失败模式并提供理解视角。


<details>
  <summary>Details</summary>
Motivation: 当前评估范式将基准视为孤立“竖井”，只提供整体分数，忽略学术专业和认知深度细节，诊断价值有限。

Method: 提出STEMVerse框架，将超20000个STEM问题重新聚合到统一能力空间，为每个实例分配双轴标签，评估不同参数规模和训练范式的大语言模型家族。

Result: 揭示了STEM推理中的结构性失败模式。

Conclusion: STEMVerse框架将多学科覆盖和细粒度认知分层整合，为理解大语言模型科学推理特征提供清晰可行视角。

Abstract: As Large Language Models (LLMs) achieve significant breakthroughs in complex reasoning tasks, evaluating their proficiency in science, technology, engineering, and mathematics (STEM) has become a primary method for measuring machine intelligence. However, current evaluation paradigms often treat benchmarks as isolated "silos," offering only monolithic aggregate scores that neglect the intricacies of both academic specialization and cognitive depth. This result-oriented approach fails to distinguish whether model errors stem from insufficient domain knowledge or deficiencies in cognitive capacity, thereby limiting the diagnostic value. To address this, we propose STEMVerse, a diagnostic framework designed to systematically analyze the STEM reasoning capabilities of LLMs. This framework characterizes model performance across academic specialization and cognitive complexity to map the capability required for reasoning. We re-aggregate over 20,000 STEM problems from mainstream benchmarks into a unified "Discipline $\times$ Cognition" capability space, assigning dual-axis labels to every instance. Utilizing this unified diagnostic framework, we systematically evaluate representative LLM families across varying parameter scales and training paradigms. Our empirical results reveal structural failure patterns in STEM reasoning. By integrating multi-disciplinary coverage and fine-grained cognitive stratification into a unified framework, STEMVerse provides a clear and actionable perspective for understanding the scientific reasoning characteristics of LLMs.

</details>


### [440] [Test-Time Detoxification without Training or Learning Anything](https://arxiv.org/abs/2602.02498)
*Baturay Saglam,Dionysis Kalogerias*

Main category: cs.CL

TL;DR: 介绍一种无需训练、仅依赖黑盒优化的测试时方法用以减少大语言模型生成文本的毒性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型可能产生有害文本，现有去毒方法成本高且难以跨模型迁移，因此需要更有效方法。

Method: 采用零阶优化方法，近似计算完成毒性关于输入嵌入的梯度，通过少量下降步骤引导生成毒性更低的文本。

Result: 该方法在不同模型和提示上能稳健降低毒性，多数情况下实现最佳毒性 - 质量权衡。

Conclusion: 词嵌入可作为有效控制变量，鼓励更广泛使用黑盒优化来引导生成安全文本。

Abstract: Large language models can produce toxic or inappropriate text even for benign inputs, creating risks when deployed at scale. Detoxification is therefore important for safety and user trust, particularly when we want to reduce harmful content without sacrificing the model's generation quality. Many existing approaches rely on model retraining, gradients, or learned auxiliary components, which can be costly and may not transfer across model families or to truly black-box settings. We introduce a test-time procedure that approximates the gradient of completion toxicity with respect to the input embeddings and uses a small number of descent steps to steer generation toward less toxic continuations. This is achieved with zeroth-order optimization that requires only access to input embeddings, a toxicity scoring function, and forward evaluations of the model. Empirically, the approach delivers robust toxicity reductions across models and prompts and, in most settings, achieves the best overall toxicity-quality trade-off. More broadly, our work positions word embeddings as effective control variables and encourages wider use of black-box optimization to guide autoregressive language models toward scalable, safer text generation, without requiring any training or access to intermediate computations.

</details>


### [441] [Monotonicity as an Architectural Bias for Robust Language Models](https://arxiv.org/abs/2602.02686)
*Patrick Cooper,Alireza Nadali,Ashutosh Trivedi,Alvaro Velasquez*

Main category: cs.CL

TL;DR: 研究通过在Transformer语言模型中引入单调性提升其鲁棒性，实验显示攻击成功率大幅下降且标准摘要性能仅轻微下降。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在对抗性提示和越狱攻击下表现脆弱，需提升基于Transformer的语言模型的鲁棒性。

Method: 在序列到序列Transformer的前馈子层中选择性地强制单调性，不约束注意力机制。

Result: 对抗攻击成功率从约69%降至19%，标准摘要性能仅轻微下降。

Conclusion: 通过选择性地强制单调性可在不牺牲性能的情况下提升语言模型的鲁棒性。

Abstract: Large language models (LLMs) are known to exhibit brittle behavior under adversarial prompts and jailbreak attacks, even after extensive alignment and fine-tuning. This fragility reflects a broader challenge of modern neural language models: small, carefully structured perturbations in high-dimensional input spaces can induce large and unpredictable changes in internal semantic representations and output.
  We investigate monotonicity as an architectural inductive bias for improving the robustness of Transformer-based language models. Monotonicity constrains semantic transformations so that strengthening information, evidence, or constraints cannot lead to regressions in the corresponding internal representations. Such order-preserving behavior has long been exploited in control and safety-critical systems to simplify reasoning and improve robustness, but has traditionally been viewed as incompatible with the expressivity required by neural language models.
  We show that this trade-off is not inherent. By enforcing monotonicity selectively in the feed-forward sublayers of sequence-to-sequence Transformers -- while leaving attention mechanisms unconstrained -- we obtain monotone language models that preserve the performance of their pretrained counterparts. This architectural separation allows negation, contradiction, and contextual interactions to be introduced explicitly through attention, while ensuring that subsequent semantic refinement is order-preserving. Empirically, monotonicity substantially improves robustness: adversarial attack success rates drop from approximately 69% to 19%, while standard summarization performance degrades only marginally.

</details>


### [442] [Predicting first-episode homelessness among US Veterans using longitudinal EHR data: time-varying models and social risk factors](https://arxiv.org/abs/2602.02731)
*Rohan Pandey,Haijuan Yan,Hong Yu,Jack Tsai*

Main category: cs.CL

TL;DR: 分析退伍军人电子健康记录数据预测无家可归风险，结合社会行为因素提升模型性能，大语言模型在不同种族群体中表现差异小，可助力制定预防策略。


<details>
  <summary>Details</summary>
Motivation: 美国退伍军人无家可归是公共卫生挑战，需通过风险预测进行主动干预。

Method: 分析2016年4276403名退伍军人事务患者电子健康记录数据，构建静态和时变表示，比较经典机器学习、基于transformer的掩码语言模型和微调大语言模型的性能。

Result: 结合社会和行为因素使纵向模型的PR - AUC提高15 - 30%；不同模型在不同时间点的风险等级中有不同阳性预测值；大语言模型在区分度上不如编码器模型，但在不同种族群体中性能差异小。

Conclusion: 纵向、考虑社会因素的电子健康记录建模可将无家可归风险分层，为高危退伍军人制定有针对性的预防策略。

Abstract: Homelessness among US veterans remains a critical public health challenge, yet risk prediction offers a pathway for proactive intervention. In this retrospective prognostic study, we analyzed electronic health record (EHR) data from 4,276,403 Veterans Affairs patients during a 2016 observation period to predict first-episode homelessness occurring 3-12 months later in 2017 (prevalence: 0.32-1.19%). We constructed static and time-varying EHR representations, utilizing clinician-informed logic to model the persistence of clinical conditions and social risks over time. We then compared the performance of classical machine learning, transformer-based masked language models, and fine-tuned large language models (LLMs). We demonstrate that incorporating social and behavioral factors into longitudinal models improved precision-recall area under the curve (PR-AUC) by 15-30%. In the top 1% risk tier, models yielded positive predictive values ranging from 3.93-4.72% at 3 months, 7.39-8.30% at 6 months, 9.84-11.41% at 9 months, and 11.65-13.80% at 12 months across model architectures. Large language models underperformed encoder-based models on discrimination but showed smaller performance disparities across racial groups. These results demonstrate that longitudinal, socially informed EHR modeling concentrates homelessness risk into actionable strata, enabling targeted and data-informed prevention strategies for at-risk veterans.

</details>


### [443] [HALT: Hallucination Assessment via Log-probs as Time series](https://arxiv.org/abs/2602.02888)
*Ahmad Shapiro,Karan Taneja,Ashok Goel*

Main category: cs.CL

TL;DR: 提出轻量级幻觉检测器HALT和基准测试HUB，HALT小且快，与HUB共同构建有效幻觉检测框架。


<details>
  <summary>Details</summary>
Motivation: 幻觉是大语言模型在安全关键领域的主要障碍，需要有效检测方法。

Method: HALT利用LLM生成的前20个标记对数概率作为时间序列，用门控循环单元模型结合基于熵的特征学习模型校准偏差；引入HUB整合先前数据集进行性能基准测试。

Result: HALT比Lettuce小30倍，在HUB上速度提升60倍且性能更优。

Conclusion: HALT和HUB共同为不同LLM能力的幻觉检测建立了有效框架。

Abstract: Hallucinations remain a major obstacle for large language models (LLMs), especially in safety-critical domains. We present HALT (Hallucination Assessment via Log-probs as Time series), a lightweight hallucination detector that leverages only the top-20 token log-probabilities from LLM generations as a time series. HALT uses a gated recurrent unit model combined with entropy-based features to learn model calibration bias, providing an extremely efficient alternative to large encoders. Unlike white-box approaches, HALT does not require access to hidden states or attention maps, relying only on output log-probabilities. Unlike black-box approaches, it operates on log-probs rather than surface-form text, which enables stronger domain generalization and compatibility with proprietary LLMs without requiring access to internal weights. To benchmark performance, we introduce HUB (Hallucination detection Unified Benchmark), which consolidates prior datasets into ten capabilities covering both reasoning tasks (Algorithmic, Commonsense, Mathematical, Symbolic, Code Generation) and general purpose skills (Chat, Data-to-Text, Question Answering, Summarization, World Knowledge). While being 30x smaller, HALT outperforms Lettuce, a fine-tuned modernBERT-base encoder, achieving a 60x speedup gain on HUB. HALT and HUB together establish an effective framework for hallucination detection across diverse LLM capabilities.

</details>


### [444] [Equal Access, Unequal Interaction: A Counterfactual Audit of LLM Fairness](https://arxiv.org/abs/2602.02932)
*Alireza Amiri-Margavi,Arshia Gharagozlou,Amin Gholami Davodi,Seyed Pouyan Mousavi Davoudi,Hamidreza Hasani Balyani*

Main category: cs.CL

TL;DR: 文章对大语言模型在访问权限授予后的交互公平性进行审计，发现即使访问公平，交互质量仍存在差异。


<details>
  <summary>Details</summary>
Motivation: 以往大语言模型公平性研究主要关注访问层面，而获得回复后的交互质量公平性未受足够重视，因此开展研究。

Method: 采用反事实提示设计，评估GPT - 4和LLaMA - 3.1 - 70B在职业建议任务中的表现，改变年龄、性别和国籍等身份属性，通过拒绝分析评估访问公平性，用自动语言指标衡量交互质量，用配对统计测试评估身份条件差异。

Result: 两个模型对所有身份的拒绝率均为零，表明访问公平，但交互质量存在模型特定的系统差异，GPT - 4对年轻男性用户表现出更高的模糊性，LLaMA在不同身份群体间有更广泛的情感变化。

Conclusion: 即使访问公平，交互层面的公平差异仍可能存在，应开展超越基于拒绝审计的评估。

Abstract: Prior work on fairness in large language models (LLMs) has primarily focused on access-level behaviors such as refusals and safety filtering. However, equitable access does not ensure equitable interaction quality once a response is provided. In this paper, we conduct a controlled fairness audit examining how LLMs differ in tone, uncertainty, and linguistic framing across demographic identities after access is granted. Using a counterfactual prompt design, we evaluate GPT-4 and LLaMA-3.1-70B on career advice tasks while varying identity attributes along age, gender, and nationality. We assess access fairness through refusal analysis and measure interaction quality using automated linguistic metrics, including sentiment, politeness, and hedging. Identity-conditioned differences are evaluated using paired statistical tests. Both models exhibit zero refusal rates across all identities, indicating uniform access. Nevertheless, we observe systematic, model-specific disparities in interaction quality: GPT-4 expresses significantly higher hedging toward younger male users, while LLaMA exhibits broader sentiment variation across identity groups. These results show that fairness disparities can persist at the interaction level even when access is equal, motivating evaluation beyond refusal-based audits.

</details>


### [445] [Where Norms and References Collide: Evaluating LLMs on Normative Reasoning](https://arxiv.org/abs/2602.02975)
*Mitchell Abrams,Kaveh Eskandari Miandoab,Felix Gervits,Vasanth Sarathy,Matthias Scheutz*

Main category: cs.CL

TL;DR: 研究通过SNIC测试床评估大语言模型处理基于规范的指称消解能力，发现当前最强模型在此存在不足。


<details>
  <summary>Details</summary>
Motivation: 不清楚大语言模型是否能支持基于社会规范推理的指称消解，需进行评估。

Method: 引入人类验证的诊断测试床SNIC，对大语言模型进行一系列控制评估。

Result: 即便最强的大语言模型在识别和应用社会规范时存在困难，尤其规范隐含、不明确或冲突时。

Conclusion: 揭示当前大语言模型的盲点，指出在社会情境化的具身环境中部署语言系统面临的关键挑战。

Abstract: Embodied agents, such as robots, will need to interact in situated environments where successful communication often depends on reasoning over social norms: shared expectations that constrain what actions are appropriate in context. A key capability in such settings is norm-based reference resolution (NBRR), where interpreting referential expressions requires inferring implicit normative expectations grounded in physical and social context. Yet it remains unclear whether Large Language Models (LLMs) can support this kind of reasoning. In this work, we introduce SNIC (Situated Norms in Context), a human-validated diagnostic testbed designed to probe how well state-of-the-art LLMs can extract and utilize normative principles relevant to NBRR. SNIC emphasizes physically grounded norms that arise in everyday tasks such as cleaning, tidying, and serving. Across a range of controlled evaluations, we find that even the strongest LLMs struggle to consistently identify and apply social norms, particularly when norms are implicit, underspecified, or in conflict. These findings reveal a blind spot in current LLMs and highlight a key challenge for deploying language-based systems in socially situated, embodied settings.

</details>


### [446] [Task--Specificity Score: Measuring How Much Instructions Really Matter for Supervision](https://arxiv.org/abs/2602.03103)
*Pritam Kadasi,Abhishek Upperwal,Mayank Singh*

Main category: cs.CL

TL;DR: 提出任务特异性分数（TSS）和TSS++来量化指令对输出的重要性，实验表明选择任务特定示例能提升下游性能。


<details>
  <summary>Details</summary>
Motivation: 许多指令 - 输入 - 输出对的指令弱指定，需探究指令是否唯一决定目标输出。

Method: 提出TSS对比真实指令与同一输入的合理替代指令，引入TSS++缓解易负效应。

Result: 在三个指令数据集和三个开源大语言模型上，选择任务特定示例在令牌预算紧张时能提升下游性能。

Conclusion: TSS和TSS++有效，选择任务特定示例可提升性能并补充基于质量的过滤器。

Abstract: Instruction tuning is now the default way to train and adapt large language models, but many instruction--input--output pairs are only weakly specified: for a given input, the same output can remain plausible under several alternative instructions. This raises a simple question: \emph{does the instruction uniquely determine the target output?}
  We propose the \textbf{Task--Specificity Score (TSS)} to quantify how much an instruction matters for predicting its output, by contrasting the true instruction against plausible alternatives for the same input. We further introduce \textbf{TSS++}, which uses hard alternatives and a small quality term to mitigate easy-negative effects. Across three instruction datasets (\textsc{Alpaca}, \textsc{Dolly-15k}, \textsc{NI-20}) and three open LLMs (Gemma, Llama, Qwen), we show that selecting task-specific examples improves downstream performance under tight token budgets and complements quality-based filters such as perplexity and IFD.

</details>


### [447] [Privasis: Synthesizing the Largest "Public" Private Dataset from Scratch](https://arxiv.org/abs/2602.03183)
*Hyunwoo Kim,Niloofar Mireshghallah,Michael Duan,Rui Xin,Shuyue Stella Li,Jaehun Jung,David Acuna,Qi Pang,Hanshen Xiao,G. Edward Suh,Sewoong Oh,Yulia Tsvetkov,Pang Wei Koh,Yejin Choi*

Main category: cs.CL

TL;DR: 提出百万级全合成数据集Privasis，用于解决隐私敏感数据研究中的数据稀缺问题，其规模和多样性优于现有数据集，基于此构建的文本清理模型表现出色，计划发布相关资源。


<details>
  <summary>Details</summary>
Motivation: 隐私敏感数据研究受数据稀缺限制，且现代AI获取敏感信息使风险上升，需解决这一长期瓶颈和新风险。

Method: 构建全新的百万级全合成数据集Privasis，利用其构建文本清理的平行语料库，训练紧凑的清理模型。

Result: Privasis规模和多样性远超现有数据集，基于其训练的紧凑清理模型（<=4B）性能超过GPT - 5和Qwen - 3 235B等大语言模型。

Conclusion: 发布数据、模型和代码以加速隐私敏感领域和智能体的未来研究。

Abstract: Research involving privacy-sensitive data has always been constrained by data scarcity, standing in sharp contrast to other areas that have benefited from data scaling. This challenge is becoming increasingly urgent as modern AI agents--such as OpenClaw and Gemini Agent--are granted persistent access to highly sensitive personal information. To tackle this longstanding bottleneck and the rising risks, we present Privasis (i.e., privacy oasis), the first million-scale fully synthetic dataset entirely built from scratch--an expansive reservoir of texts with rich and diverse private information--designed to broaden and accelerate research in areas where processing sensitive social data is inevitable. Compared to existing datasets, Privasis, comprising 1.4 million records, offers orders-of-magnitude larger scale with quality, and far greater diversity across various document types, including medical history, legal documents, financial records, calendars, and text messages with a total of 55.1 million annotated attributes such as ethnicity, date of birth, workplace, etc. We leverage Privasis to construct a parallel corpus for text sanitization with our pipeline that decomposes texts and applies targeted sanitization. Our compact sanitization models (<=4B) trained on this dataset outperform state-of-the-art large language models, such as GPT-5 and Qwen-3 235B. We plan to release data, models, and code to accelerate future research on privacy-sensitive domains and agents.

</details>


### [448] [ATACompressor: Adaptive Task-Aware Compression for Efficient Long-Context Processing in LLMs](https://arxiv.org/abs/2602.03226)
*Xuancheng Li,Haitao Li,Yujia Zhou,Qingyao Ai,Yiqun Liu*

Main category: cs.CL

TL;DR: 大语言模型处理长文本存'中间丢失'问题，提出 ATACompressor 方法优于现方法且可扩展性强，并做消融实验。


<details>
  <summary>Details</summary>
Motivation: 大语言模型处理长上下文输入存在“中间丢失”问题，现有上下文压缩方法难以平衡信息保留和压缩效率。

Method: 提出 ATACompressor，用选择性编码器压缩任务相关部分，用自适应分配控制器调整压缩率。

Result: 在 HotpotQA、MSMARCO 和 SQUAD 三个问答数据集上评估，ATACompressor 在压缩效率和任务性能上均优于现有方法。

Conclusion: ATACompressor 为大语言模型的长上下文处理提供了可扩展的解决方案。

Abstract: Long-context inputs in large language models (LLMs) often suffer from the "lost in the middle" problem, where critical information becomes diluted or ignored due to excessive length. Context compression methods aim to address this by reducing input size, but existing approaches struggle with balancing information preservation and compression efficiency. We propose Adaptive Task-Aware Compressor (ATACompressor), which dynamically adjusts compression based on the specific requirements of the task. ATACompressor employs a selective encoder that compresses only the task-relevant portions of long contexts, ensuring that essential information is preserved while reducing unnecessary content. Its adaptive allocation controller perceives the length of relevant content and adjusts the compression rate accordingly, optimizing resource utilization. We evaluate ATACompressor on three QA datasets: HotpotQA, MSMARCO, and SQUAD-showing that it outperforms existing methods in terms of both compression efficiency and task performance. Our approach provides a scalable solution for long-context processing in LLMs. Furthermore, we perform a range of ablation studies and analysis experiments to gain deeper insights into the key components of ATACompressor.

</details>


### [449] [POP: Prefill-Only Pruning for Efficient Large Model Inference](https://arxiv.org/abs/2602.03295)
*Junhui He,Zhihui Fu,Jun Wang,Qingan Li*

Main category: cs.CL

TL;DR: 论文指出现有大模型部署受计算成本阻碍，结构化剪枝方法有精度损失问题，提出阶段感知推理策略POP，实验证明其能提升预填充阶段速度且性能损失小。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型和视觉语言模型部署时计算成本高，现有结构化剪枝方法精度下降的问题。

Method: 引入虚拟门机制进行重要性分析，提出阶段感知推理策略POP，引入独立的键值投影和边界处理策略。

Result: 在Llama - 3.1、Qwen3 - VL和Gemma - 3上实验，POP使预填充延迟最高提升1.37倍，性能损失极小。

Conclusion: POP有效克服了现有结构化剪枝方法在精度和效率权衡上的局限。

Abstract: Large Language Models (LLMs) and Vision-Language Models (VLMs) have demonstrated remarkable capabilities. However, their deployment is hindered by significant computational costs. Existing structured pruning methods, while hardware-efficient, often suffer from significant accuracy degradation. In this paper, we argue that this failure stems from a stage-agnostic pruning approach that overlooks the asymmetric roles between the prefill and decode stages. By introducing a virtual gate mechanism, our importance analysis reveals that deep layers are critical for next-token prediction (decode) but largely redundant for context encoding (prefill). Leveraging this insight, we propose Prefill-Only Pruning (POP), a stage-aware inference strategy that safely omits deep layers during the computationally intensive prefill stage while retaining the full model for the sensitive decode stage. To enable the transition between stages, we introduce independent Key-Value (KV) projections to maintain cache integrity, and a boundary handling strategy to ensure the accuracy of the first generated token. Extensive experiments on Llama-3.1, Qwen3-VL, and Gemma-3 across diverse modalities demonstrate that POP achieves up to 1.37$\times$ speedup in prefill latency with minimal performance loss, effectively overcoming the accuracy-efficiency trade-off limitations of existing structured pruning methods.

</details>


### [450] [Self-Verification Dilemma: Experience-Driven Suppression of Overused Checking in LLM Reasoning](https://arxiv.org/abs/2602.03485)
*Quanyu Long,Kai Jie Jiang,Jianda Chen,Xu Guo,Leilei Gan,Wenya Wang*

Main category: cs.CL

TL;DR: 发现大推理模型自验证步骤存在过度使用问题，提出经验驱动框架减少验证，可减少token使用并保持或提升准确率。


<details>
  <summary>Details</summary>
Motivation: 大推理模型自验证步骤常重复确认中间结果，多数为确认性而非纠正性，激活频率与实际作用不匹配。

Method: 提出经验驱动测试时框架，检测重查行为激活，参考离线经验池，通过高效检索估计重查是否必要，必要时发抑制信号。

Result: 在多个模型和基准测试中，减少token使用达20.3%，保持准确率，部分数据集准确率提升。

Conclusion: 所提框架能有效减少大推理模型过度使用的自验证，提升效率。

Abstract: Large Reasoning Models (LRMs) achieve strong performance by generating long reasoning traces with reflection. Through a large-scale empirical analysis, we find that a substantial fraction of reflective steps consist of self-verification (recheck) that repeatedly confirm intermediate results. These rechecks occur frequently across models and benchmarks, yet the vast majority are confirmatory rather than corrective, rarely identifying errors and altering reasoning outcomes. This reveals a mismatch between how often self-verification is activated and how often it is actually useful. Motivated by this, we propose a novel, experience-driven test-time framework that reduces the overused verification. Our method detects the activation of recheck behavior, consults an offline experience pool of past verification outcomes, and estimates whether a recheck is likely unnecessary via efficient retrieval. When historical experience suggests unnecessary, a suppression signal redirects the model to proceed. Across multiple model and benchmarks, our approach reduces token usage up to 20.3% while maintaining the accuracy, and in some datasets even yields accuracy improvements.

</details>


### [451] [HySparse: A Hybrid Sparse Attention Architecture with Oracle Token Selection and KV Cache Sharing](https://arxiv.org/abs/2602.03560)
*Yizhao Gao,Jianyu Wei,Qihao Zhang,Yu Cheng,Shimao Chen,Zhengju Tang,Zihan Jiang,Yifan Song,Hailin Zhang,Liang Zhao,Bo Yang,Gang Wang,Shijie Cao,Fuli Luo*

Main category: cs.CL

TL;DR: 本文提出HySparse架构，结合全注意力层和稀疏注意力层，解决了先前稀疏注意力方法的两个局限性，在多个模型上表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 解决先前稀疏注意力方法依赖额外代理预测令牌重要性增加复杂度、性能不佳，以及仅减少计算不节省KV缓存的问题。

Method: 提出HySparse架构，将每个全注意力层与多个稀疏注意力层交错排列，稀疏层的令牌选择和KV缓存直接从先前的全注意力层导出。

Result: 在7B密集和80B MoE模型上评估，HySparse始终优于全注意力和混合SWA基线，在80B MoE模型中减少近10倍的KV缓存存储并取得显著性能提升。

Conclusion: HySparse架构有效解决了先前方法的局限性，在性能和存储上都有优越表现。

Abstract: This work introduces Hybrid Sparse Attention (HySparse), a new architecture that interleaves each full attention layer with several sparse attention layers. While conceptually simple, HySparse strategically derives each sparse layer's token selection and KV caches directly from the preceding full attention layer. This architecture resolves two fundamental limitations of prior sparse attention methods. First, conventional approaches typically rely on additional proxies to predict token importance, introducing extra complexity and potentially suboptimal performance. In contrast, HySparse uses the full attention layer as a precise oracle to identify important tokens. Second, existing sparse attention designs often reduce computation without saving KV cache. HySparse enables sparse attention layers to reuse the full attention KV cache, thereby reducing both computation and memory. We evaluate HySparse on both 7B dense and 80B MoE models. Across all settings, HySparse consistently outperforms both full attention and hybrid SWA baselines. Notably, in the 80B MoE model with 49 total layers, only 5 layers employ full attention, yet HySparse achieves substantial performance gains while reducing KV cache storage by nearly 10x.

</details>


### [452] [Use Graph When It Needs: Efficiently and Adaptively Integrating Retrieval-Augmented Generation with Graphs](https://arxiv.org/abs/2602.03578)
*Su Dong,Qinggang Zhang,Yilin Xiao,Shengyuan Chen,Chuang Zhou,Xiao Huang*

Main category: cs.CL

TL;DR: 大语言模型处理知识密集任务有问题，现有方法有局限，提出EA - GraphRAG框架，可动态集成RAG和GraphRAG，实验证明其能提升表现。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在知识密集任务上因幻觉和知识过时存在问题，RAG受非结构化文档信息碎片化限制，GraphRAG在现实场景表现不佳，需解决其刚性应用问题。

Method: 提出EA - GraphRAG框架，包括语法特征构造器、轻量级复杂度评分器、基于分数的路由策略。

Result: 在综合基准测试中，EA - GraphRAG显著提高了准确率，降低了延迟，在处理简单和复杂查询的混合场景中达到了最先进的性能。

Conclusion: EA - GraphRAG框架能有效动态集成RAG和GraphRAG，提升处理不同复杂度查询的性能。

Abstract: Large language models (LLMs) often struggle with knowledge-intensive tasks due to hallucinations and outdated parametric knowledge. While Retrieval-Augmented Generation (RAG) addresses this by integrating external corpora, its effectiveness is limited by fragmented information in unstructured domain documents. Graph-augmented RAG (GraphRAG) emerged to enhance contextual reasoning through structured knowledge graphs, yet paradoxically underperforms vanilla RAG in real-world scenarios, exhibiting significant accuracy drops and prohibitive latency despite gains on complex queries. We identify the rigid application of GraphRAG to all queries, regardless of complexity, as the root cause. To resolve this, we propose an efficient and adaptive GraphRAG framework called EA-GraphRAG that dynamically integrates RAG and GraphRAG paradigms through syntax-aware complexity analysis. Our approach introduces: (i) a syntactic feature constructor that parses each query and extracts a set of structural features; (ii) a lightweight complexity scorer that maps these features to a continuous complexity score; and (iii) a score-driven routing policy that selects dense RAG for low-score queries, invokes graph-based retrieval for high-score queries, and applies complexity-aware reciprocal rank fusion to handle borderline cases. Extensive experiments on a comprehensive benchmark, consisting of two single-hop and two multi-hop QA benchmarks, demonstrate that our EA-GraphRAG significantly improves accuracy, reduces latency, and achieves state-of-the-art performance in handling mixed scenarios involving both simple and complex queries.

</details>


### [453] [From Task Solving to Robust Real-World Adaptation in LLM Agents](https://arxiv.org/abs/2602.02760)
*Pouya Pezeshkpour,Estevam Hruschka*

Main category: cs.CL

TL;DR: 本文针对大语言模型作为智能体的情况，在违背‘干净接口’假设的复杂环境下对其进行压力测试，发现模型在任务解决能力和部署鲁棒性间存在差距，为后续研究提供方向。


<details>
  <summary>Details</summary>
Motivation: 现有评估高估大语言模型在现实世界的适用性，智能体在实际中面临规则不明确、信号不可靠等问题，需要评估其在复杂环境下的鲁棒性。

Method: 在基于网格的游戏中，模拟部分可观测、动态环境、噪声信号和动态智能体状态四种情况对大语言模型智能体进行压力测试。

Result: 五种最先进的大语言模型智能体在名义任务解决能力和类部署鲁棒性之间存在巨大差距，性能随网格大小和执行时长增加而下降，排名不稳定，模型能进行部分目标推断。

Conclusion: 研究揭示了模型特定的敏感性和失败驱动因素，为部分可观测、噪声和非平稳性条件下的验证、安全动作选择和目标推断研究提供了动力。

Abstract: Large language models are increasingly deployed as specialized agents that plan, call tools, and take actions over extended horizons. Yet many existing evaluations assume a "clean interface" where dynamics are specified and stable, tools and sensors are reliable, and success is captured by a single explicit objective-often overestimating real-world readiness. In practice, agents face underspecified rules, unreliable signals, shifting environments, and implicit, multi-stakeholder goals. The challenge is therefore not just solving tasks, but adapting while solving: deciding what to trust, what is wanted, when to verify, and when to fall back or escalate. We stress-test deployment-relevant robustness under four operational circumstances: partial observability, dynamic environments, noisy signals, and dynamic agent state. We benchmark agentic LLMs in a grid-based game with a simple goal but long-horizon execution. Episodes violate clean-interface assumptions yet remain solvable, forcing agents to infer rules, pay for information, adapt to environmental and internal shifts, and act cautiously under noise. Across five state-of-the-art LLM agents, we find large gaps between nominal task-solving and deployment-like robustness. Performance generally degrades as grid size and horizon increase, but rankings are unstable: weaker models can beat stronger ones when strategy matches the uncertainty regime. Despite no explicit instruction, agents trade off completion, efficiency, and penalty avoidance, suggesting partial objective inference. Ablations and feature analyses reveal model-specific sensitivities and failure drivers, motivating work on verification, safe action selection, and objective inference under partial observability, noise, and non-stationarity.

</details>


### [454] [$V_0$: A Generalist Value Model for Any Policy at State Zero](https://arxiv.org/abs/2602.03584)
*Yi-Kai Zhang,Zhiyuan Yao,Hongyan Hao,Yueqing Sun,Qi Gu,Hui Su,Xunliang Cai,De-Chuan Zhan,Han-Jia Ye*

Main category: cs.CL

TL;DR: 本文提出了通用价值模型$V_0$，无需参数更新即可估计模型在未见提示上的预期性能，在训练和部署中发挥作用，实证显示其在性能和成本上有优势。


<details>
  <summary>Details</summary>
Motivation: 现有策略梯度方法中价值模型需昂贵的同步增量训练，GRPO方法需大量采样来保持估计稳定，因此需要新的价值估计方法。

Method: 提出$V_0$模型，将策略的动态能力作为显式上下文输入，利用指令 - 性能对历史动态分析模型，聚焦于初始状态下的价值估计。

Result: $V_0$显著优于启发式预算分配，在大语言模型路由任务中实现了性能和成本的帕累托最优权衡。

Conclusion: $V_0$模型是有效的，能在训练和部署中发挥重要作用，实现性能和成本的良好平衡。

Abstract: Policy gradient methods rely on a baseline to measure the relative advantage of an action, ensuring the model reinforces behaviors that outperform its current average capability. In the training of Large Language Models (LLMs) using Actor-Critic methods (e.g., PPO), this baseline is typically estimated by a Value Model (Critic) often as large as the policy model itself. However, as the policy continuously evolves, the value model requires expensive, synchronous incremental training to accurately track the shifting capabilities of the policy. To avoid this overhead, Group Relative Policy Optimization (GRPO) eliminates the coupled value model by using the average reward of a group of rollouts as the baseline; yet, this approach necessitates extensive sampling to maintain estimation stability. In this paper, we propose $V_0$, a Generalist Value Model capable of estimating the expected performance of any model on unseen prompts without requiring parameter updates. We reframe value estimation by treating the policy's dynamic capability as an explicit context input; specifically, we leverage a history of instruction-performance pairs to dynamically profile the model, departing from the traditional paradigm that relies on parameter fitting to perceive capability shifts. Focusing on value estimation at State Zero (i.e., the initial prompt, hence $V_0$), our model serves as a critical resource scheduler. During GRPO training, $V_0$ predicts success rates prior to rollout, allowing for efficient sampling budget allocation; during deployment, it functions as a router, dispatching instructions to the most cost-effective and suitable model. Empirical results demonstrate that $V_0$ significantly outperforms heuristic budget allocation and achieves a Pareto-optimal trade-off between performance and cost in LLM routing tasks.

</details>


### [455] [Rethinking the Reranker: Boundary-Aware Evidence Selection for Robust Retrieval-Augmented Generation](https://arxiv.org/abs/2602.03689)
*Jiashuo Sun,Pengcheng Jiang,Saizhuo Wang,Jiajun Fan,Heng Wang,Siru Ouyang,Ming Zhong,Yizhu Jiao,Chengsong Huang,Xueqiang Xu,Pengrui Han,Peiran Li,Jiaxin Huang,Ge Liu,Heng Ji,Jiawei Han*

Main category: cs.CL

TL;DR: 提出BAR - RAG解决RAG系统在检索噪声下的脆性问题，实验显示能提升性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统在检索噪声下很脆弱，检索器和重排序器仅优化相关性，未考虑证据对生成器的适用性。

Method: 将重排序器重构为边界感知证据选择器，用强化学习训练选择器，采用两阶段管道微调生成器。

Result: 在知识密集型问答基准测试中，BAR - RAG持续提升端到端性能，比基线平均提升10.3%，并显著增强鲁棒性。

Conclusion: BAR - RAG能有效解决RAG系统在检索噪声下的问题。

Abstract: Retrieval-Augmented Generation (RAG) systems remain brittle under realistic retrieval noise, even when the required evidence appears in the top-K results. A key reason is that retrievers and rerankers optimize solely for relevance, often selecting either trivial, answer-revealing passages or evidence that lacks the critical information required to answer the question, without considering whether the evidence is suitable for the generator. We propose BAR-RAG, which reframes the reranker as a boundary-aware evidence selector that targets the generator's Goldilocks Zone -- evidence that is neither trivially easy nor fundamentally unanswerable for the generator, but is challenging yet sufficient for inference and thus provides the strongest learning signal. BAR-RAG trains the selector with reinforcement learning using generator feedback, and adopts a two-stage pipeline that fine-tunes the generator under the induced evidence distribution to mitigate the distribution mismatch between training and inference. Experiments on knowledge-intensive question answering benchmarks show that BAR-RAG consistently improves end-to-end performance under noisy retrieval, achieving an average gain of 10.3 percent over strong RAG and reranking baselines while substantially improving robustness. Code is publicly avaliable at https://github.com/GasolSun36/BAR-RAG.

</details>


### [456] [OCRTurk: A Comprehensive OCR Benchmark for Turkish](https://arxiv.org/abs/2602.03693)
*Deniz Yılmaz,Evren Ayberk Munis,Çağrı Toraman,Süha Kağan Köse,Burak Aktaş,Mehmet Can Baytekin,Bilge Kaan Görür*

Main category: cs.CL

TL;DR: 文章介绍土耳其文档解析基准OCRTurk，用它评估7种OCR模型，PaddleOCR表现最佳。


<details>
  <summary>Details</summary>
Motivation: 现有基准多针对高资源语言，对土耳其语等低资源语言覆盖有限，且缺乏能反映真实场景和文档多样性的标准基准。

Method: 引入OCRTurk基准，包含多种布局元素和文档类别共180份土耳其文档，用元素级指标评估7种OCR模型。

Result: 在各难度等级中，PaddleOCR整体表现最强，多数元素指标领先，不同文档类型中模型表现有差异。

Conclusion: OCRTurk可用于评估土耳其文档解析模型，PaddleOCR在该基准测试中表现出色。

Abstract: Document parsing is now widely used in applications, such as large-scale document digitization, retrieval-augmented generation, and domain-specific pipelines in healthcare and education. Benchmarking these models is crucial for assessing their reliability and practical robustness. Existing benchmarks mostly target high-resource languages and provide limited coverage for low-resource settings, such as Turkish. Moreover, existing studies on Turkish document parsing lack a standardized benchmark that reflects real-world scenarios and document diversity. To address this gap, we introduce OCRTurk, a Turkish document parsing benchmark covering multiple layout elements and document categories at three difficulty levels. OCRTurk consists of 180 Turkish documents drawn from academic articles, theses, slide decks, and non-academic articles. We evaluate seven OCR models on OCRTurk using element-wise metrics. Across difficulty levels, PaddleOCR achieves the strongest overall results, leading most element-wise metrics except figures and attaining high Normalized Edit Distance scores in easy, medium, and hard subsets. We also observe performance variation by document type. Models perform well on non-academic documents, while slideshows become the most challenging.

</details>


### [457] [Cognitively Diverse Multiple-Choice Question Generation: A Hybrid Multi-Agent Framework with Large Language Models](https://arxiv.org/abs/2602.03704)
*Yu Tian,Linh Huynh,Katerina Christhilf,Shubham Chakraborty,Micah Watanabe,Tracy Arner,Danielle McNamara*

Main category: cs.CL

TL;DR: 本文提出ReQUESTA框架用于生成多样选择题，评估发现其生成题目效果更好，证明混合式多智能体编排可提升大模型生成可靠性和可控性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型自动生成选择题难以满足认知需求，需解决此问题。

Method: 引入ReQUESTA框架，将选择题创作分解为子任务，协调大语言模型智能体和基于规则组件，在阅读理解研究中与GPT - 5零样本基线对比，进行心理测量分析和专家评估。

Result: ReQUESTA生成题目更具挑战性、区分度，与整体阅读理解表现更一致，专家评估显示与核心概念更契合，干扰项质量更好。

Conclusion: 混合式多智能体编排能系统地提升大语言模型生成的可靠性和可控性，强调工作流设计对结构化产物生成的重要性。

Abstract: Recent advances in large language models (LLMs) have made automated multiple-choice question (MCQ) generation increasingly feasible; however, reliably producing items that satisfy controlled cognitive demands remains a challenge. To address this gap, we introduce ReQUESTA, a hybrid, multi-agent framework for generating cognitively diverse MCQs that systematically target text-based, inferential, and main idea comprehension. ReQUESTA decomposes MCQ authoring into specialized subtasks and coordinates LLM-powered agents with rule-based components to support planning, controlled generation, iterative evaluation, and post-processing. We evaluated the framework in a large-scale reading comprehension study using academic expository texts, comparing ReQUESTA-generated MCQs with those produced by a single-pass GPT-5 zero-shot baseline. Psychometric analyses of learner responses assessed item difficulty and discrimination, while expert raters evaluated question quality across multiple dimensions, including topic relevance and distractor quality. Results showed that ReQUESTA-generated items were consistently more challenging, more discriminative, and more strongly aligned with overall reading comprehension performance. Expert evaluations further indicated stronger alignment with central concepts and superior distractor linguistic consistency and semantic plausibility, particularly for inferential questions. These findings demonstrate that hybrid, agentic orchestration can systematically improve the reliability and controllability of LLM-based generation, highlighting workflow design as a key lever for structured artifact generation beyond single-pass prompting.

</details>


### [458] [LatentMem: Customizing Latent Memory for Multi-Agent Systems](https://arxiv.org/abs/2602.03036)
*Muxin Fu,Guibin Zhang,Xiangyuan Xue,Yafu Li,Zefeng He,Siyuan Huang,Xiaoye Qu,Yu Cheng,Yang Yang*

Main category: cs.CL

TL;DR: 提出LatentMem框架解决多智能体记忆设计瓶颈，实验显示性能优于现有架构。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体记忆设计存在记忆同质化和信息过载两个瓶颈。

Method: 提出LatentMem框架，包括存储交互轨迹的经验库和合成潜在记忆的记忆合成器，还引入LMPO优化策略。

Result: 在多个基准和主流多智能体系统框架实验中，LatentMem比普通设置性能提升最多达19.36%，且始终优于现有记忆架构。

Conclusion: LatentMem能在不修改底层框架的情况下有效提升多智能体系统性能。

Abstract: Large language model (LLM)-powered multi-agent systems (MAS) demonstrate remarkable collective intelligence, wherein multi-agent memory serves as a pivotal mechanism for continual adaptation. However, existing multi-agent memory designs remain constrained by two fundamental bottlenecks: (i) memory homogenization arising from the absence of role-aware customization, and (ii) information overload induced by excessively fine-grained memory entries. To address these limitations, we propose LatentMem, a learnable multi-agent memory framework designed to customize agent-specific memories in a token-efficient manner. Specifically, LatentMem comprises an experience bank that stores raw interaction trajectories in a lightweight form, and a memory composer that synthesizes compact latent memories conditioned on retrieved experience and agent-specific contexts. Further, we introduce Latent Memory Policy Optimization (LMPO), which propagates task-level optimization signals through latent memories to the composer, encouraging it to produce compact and high-utility representations. Extensive experiments across diverse benchmarks and mainstream MAS frameworks show that LatentMem achieves a performance gain of up to $19.36$% over vanilla settings and consistently outperforms existing memory architectures, without requiring any modifications to the underlying frameworks.

</details>


### [459] [ForesightKV: Optimizing KV Cache Eviction for Reasoning Models by Learning Long-Term Contribution](https://arxiv.org/abs/2602.03203)
*Zican Dong,Peiyu Liu,Junyi Li,Zhipeng Chen,Han Peng,Shuo Wang,Wayne Xin Zhao*

Main category: cs.CL

TL;DR: 现有KV缓存驱逐方法在处理长文本时存在不足，本文提出ForesightKV框架平衡效率与性能，实验显示其优于先前方法。


<details>
  <summary>Details</summary>
Motivation: 解决长文本生成时KV缓存线性扩展带来的高内存和计算成本问题，以及现有KV缓存驱逐方法无法捕捉复杂依赖导致的性能下降问题。

Method: 设计Golden Eviction算法确定最优驱逐KV对，通过监督训练蒸馏信息，将缓存驱逐问题建模为马尔可夫决策过程并应用GRPO算法。

Result: 在AIME2024和AIME2025基准测试中，ForesightKV在仅一半缓存预算下始终优于先前方法。

Conclusion: ForesightKV能有效平衡效率和性能，且受益于监督学习和强化学习方法。

Abstract: Recently, large language models (LLMs) have shown remarkable reasoning abilities by producing long reasoning traces. However, as the sequence length grows, the key-value (KV) cache expands linearly, incurring significant memory and computation costs. Existing KV cache eviction methods mitigate this issue by discarding less important KV pairs, but often fail to capture complex KV dependencies, resulting in performance degradation. To better balance efficiency and performance, we introduce ForesightKV, a training-based KV cache eviction framework that learns to predict which KV pairs to evict during long-text generations. We first design the Golden Eviction algorithm, which identifies the optimal eviction KV pairs at each step using future attention scores. These traces and the scores at each step are then distilled via supervised training with a Pairwise Ranking Loss. Furthermore, we formulate cache eviction as a Markov Decision Process and apply the GRPO algorithm to mitigate the significant language modeling loss increase on low-entropy tokens. Experiments on AIME2024 and AIME2025 benchmarks of three reasoning models demonstrate that ForesightKV consistently outperforms prior methods under only half the cache budget, while benefiting synergistically from both supervised and reinforcement learning approaches.

</details>


### [460] [Accelerating Scientific Research with Gemini: Case Studies and Common Techniques](https://arxiv.org/abs/2602.03837)
*David P. Woodruff,Vincent Cohen-Addad,Lalit Jain,Jieming Mao,Song Zuo,MohammadHossein Bateni,Simina Branzei,Michael P. Brenner,Lin Chen,Ying Feng,Lance Fortnow,Gang Fu,Ziyi Guan,Zahra Hadizadeh,Mohammad T. Hajiaghayi,Mahdi JafariRaviz,Adel Javanmard,Karthik C. S.,Ken-ichi Kawarabayashi,Ravi Kumar,Silvio Lattanzi,Euiwoong Lee,Yi Li,Ioannis Panageas,Dimitris Paparas,Benjamin Przybocki,Bernardo Subercaseaux,Ola Svensson,Shayan Taherijam,Xuan Wu,Eylon Yogev,Morteza Zadimoghaddam,Samson Zhou,Vahab Mirrokni*

Main category: cs.CL

TL;DR: 本文展示研究人员与谷歌基于Gemini的模型合作解决理论计算机科学等领域问题，提取有效人机协作技术，凸显AI在科研发现中的潜力。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型在新颖、专家级数学发现方面的能力及在科研中的作用。

Method: 通过案例研究，研究人员与基于Gemini的模型合作解决问题，总结出迭代改进、问题分解和跨学科知识转移等协作技术，还采用非标准聊天界面方式。

Result: 成功解决开放问题、反驳猜想、生成新证明，总结出有效协作技术，发现AI在特定场景的新应用。

Conclusion: AI不仅是自动化工具，更是科研发现创造性过程中的多功能、真正伙伴。

Abstract: Recent advances in large language models (LLMs) have opened new avenues for accelerating scientific research. While models are increasingly capable of assisting with routine tasks, their ability to contribute to novel, expert-level mathematical discovery is less understood. We present a collection of case studies demonstrating how researchers have successfully collaborated with advanced AI models, specifically Google's Gemini-based models (in particular Gemini Deep Think and its advanced variants), to solve open problems, refute conjectures, and generate new proofs across diverse areas in theoretical computer science, as well as other areas such as economics, optimization, and physics. Based on these experiences, we extract common techniques for effective human-AI collaboration in theoretical research, such as iterative refinement, problem decomposition, and cross-disciplinary knowledge transfer. While the majority of our results stem from this interactive, conversational methodology, we also highlight specific instances that push beyond standard chat interfaces. These include deploying the model as a rigorous adversarial reviewer to detect subtle flaws in existing proofs, and embedding it within a "neuro-symbolic" loop that autonomously writes and executes code to verify complex derivations. Together, these examples highlight the potential of AI not just as a tool for automation, but as a versatile, genuine partner in the creative process of scientific discovery.

</details>


### [461] [Token Sparse Attention: Efficient Long-Context Inference with Interleaved Token Selection](https://arxiv.org/abs/2602.03216)
*Dongwon Jo,Beomseok Kang,Jiwon Song,Jae-Joon Kim*

Main category: cs.CL

TL;DR: 提出Token Sparse Attention解决大语言模型长上下文推理中注意力的二次复杂度瓶颈，实验显示能提升准确率 - 延迟权衡。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型长上下文推理中注意力复杂度为二次，此前加速方法存在保留无关令牌或依赖不可逆转早期决策的问题。

Method: 提出Token Sparse Attention，一种轻量级动态令牌级稀疏化机制，压缩Q、K、V到缩减的令牌集，再解压缩输出到原序列，且与多种注意力实现兼容。

Result: Token Sparse Attention持续改善准确率 - 延迟权衡，在128K上下文下实现最高3.23倍的注意力加速，准确率下降少于1%。

Conclusion: 动态和交错的令牌级稀疏化是可扩展长上下文推理的有效策略。

Abstract: The quadratic complexity of attention remains the central bottleneck in long-context inference for large language models. Prior acceleration methods either sparsify the attention map with structured patterns or permanently evict tokens at specific layers, which can retain irrelevant tokens or rely on irreversible early decisions despite the layer-/head-wise dynamics of token importance. In this paper, we propose Token Sparse Attention, a lightweight and dynamic token-level sparsification mechanism that compresses per-head $Q$, $K$, $V$ to a reduced token set during attention and then decompresses the output back to the original sequence, enabling token information to be reconsidered in subsequent layers. Furthermore, Token Sparse Attention exposes a new design point at the intersection of token selection and sparse attention. Our approach is fully compatible with dense attention implementations, including Flash Attention, and can be seamlessly composed with existing sparse attention kernels. Experimental results show that Token Sparse Attention consistently improves accuracy-latency trade-off, achieving up to $\times$3.23 attention speedup at 128K context with less than 1% accuracy degradation. These results demonstrate that dynamic and interleaved token-level sparsification is a complementary and effective strategy for scalable long-context inference.

</details>


### [462] [Accurate Failure Prediction in Agents Does Not Imply Effective Failure Prevention](https://arxiv.org/abs/2602.03338)
*Rakshith Vasudev,Melisa Russak,Dan Bikel,Waseem Alshikh*

Main category: cs.CL

TL;DR: 研究表明LLM批评模型干预效果有差异，提出预部署测试以判断干预是否有益。


<details>
  <summary>Details</summary>
Motivation: 深入理解LLM批评模型部署时的实际效果，避免盲目干预造成性能下降。

Method: 识别出干扰 - 恢复权衡，提出用50个任务的预部署测试来预估干预效果。

Result: 测试能正确预测结果，干预在高成功率任务中降低性能，在高失败率基准测试中有小幅提升。

Conclusion: 框架的主要价值在于识别何时不应干预，防止部署前的严重性能下降。

Abstract: Proactive interventions by LLM critic models are often assumed to improve reliability, yet their effects at deployment time are poorly understood. We show that a binary LLM critic with strong offline accuracy (AUROC 0.94) can nevertheless cause severe performance degradation, inducing a 26 percentage point (pp) collapse on one model while affecting another by near zero pp. This variability demonstrates that LLM critic accuracy alone is insufficient to determine whether intervention is safe.
  We identify a disruption-recovery tradeoff: interventions may recover failing trajectories but also disrupt trajectories that would have succeeded. Based on this insight, we propose a pre-deployment test that uses a small pilot of 50 tasks to estimate whether intervention is likely to help or harm, without requiring full deployment. Across benchmarks, the test correctly anticipates outcomes: intervention degrades performance on high-success tasks (0 to -26 pp), while yielding a modest improvement on the high-failure ALFWorld benchmark (+2.8 pp, p=0.014). The primary value of our framework is therefore identifying when not to intervene, preventing severe regressions before deployment.

</details>


### [463] [Can Large Language Models Generalize Procedures Across Representations?](https://arxiv.org/abs/2602.03542)
*Fangru Lin,Valentin Hofmann,Xingchen Wan,Weixing Wang,Zifeng Ding,Anthony G. Cohn,Janet B. Pierrehumbert*

Main category: cs.CL

TL;DR: 研究大语言模型跨代码、图和自然语言表示的泛化能力，提出两阶段数据课程提升性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在符号表示上训练测试多，但真实用户任务常用自然语言描述，需探究其跨表示的泛化能力。

Method: 研究同构任务，提出先符号数据后自然语言数据的两阶段数据课程。

Result: 仅在图或代码数据上训练不能可靠泛化到自然语言任务，仅在自然语言训练性能提升低效；两阶段课程显著提升各模型和任务的性能，1.5B Qwen模型接近零样本GPT - 4o。

Conclusion: 成功的跨表示泛化可视为生成类比形式，两阶段课程能有效促进。

Abstract: Large language models (LLMs) are trained and tested extensively on symbolic representations such as code and graphs, yet real-world user tasks are often specified in natural language. To what extent can LLMs generalize across these representations? Here, we approach this question by studying isomorphic tasks involving procedures represented in code, graphs, and natural language (e.g., scheduling steps in planning). We find that training LLMs with popular post-training methods on graphs or code data alone does not reliably generalize to corresponding natural language tasks, while training solely on natural language can lead to inefficient performance gains. To address this gap, we propose a two-stage data curriculum that first trains on symbolic, then natural language data. The curriculum substantially improves model performance across model families and tasks. Remarkably, a 1.5B Qwen model trained by our method can closely match zero-shot GPT-4o in naturalistic planning. Finally, our analysis suggests that successful cross-representation generalization can be interpreted as a form of generative analogy, which our curriculum effectively encourages.

</details>


### [464] [TRE: Encouraging Exploration in the Trust Region](https://arxiv.org/abs/2602.03635)
*Chao Huang,Yujing Lu,Quangang Li,Shenghe Wang,Yan Wang,Yueyang Zhang,Long Xia,Jiashu Zhao,Zhiyuan Sun,Daiting Shi,Tingwen Liu*

Main category: cs.CL

TL;DR: 标准熵正则化在大语言模型探索效果不佳，提出信任区域熵（TRE）方法，实验证明其优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 标准熵正则化技术在大语言模型中效果差甚至会降低性能，需解决大词汇量和长生成序列带来的累积尾部风险问题。

Method: 提出信任区域熵（TRE）方法，严格在模型信任区域内鼓励探索。

Result: 在数学推理、组合搜索和偏好对齐等任务中，TRE 始终优于 vanilla PPO、标准熵正则化和其他探索基线。

Conclusion: TRE 方法有效，可用于解决大语言模型中探索问题。

Abstract: Entropy regularization is a standard technique in reinforcement learning (RL) to enhance exploration, yet it yields negligible effects or even degrades performance in Large Language Models (LLMs). We attribute this failure to the cumulative tail risk inherent to LLMs with massive vocabularies and long generation horizons. In such environments, standard global entropy maximization indiscriminately dilutes probability mass into the vast tail of invalid tokens rather than focusing on plausible candidates, thereby disrupting coherent reasoning. To address this, we propose Trust Region Entropy (TRE), a method that encourages exploration strictly within the model's trust region. Extensive experiments across mathematical reasoning (MATH), combinatorial search (Countdown), and preference alignment (HH) tasks demonstrate that TRE consistently outperforms vanilla PPO, standard entropy regularization, and other exploration baselines. Our code is available at https://github.com/WhyChaos/TRE-Encouraging-Exploration-in-the-Trust-Region.

</details>


### [465] [Neural Attention Search Linear: Towards Adaptive Token-Level Hybrid Attention Models](https://arxiv.org/abs/2602.03681)
*Difan Deng,Andreas Bentzen Winje,Lukas Fehring,Marius Lindauer*

Main category: cs.CL

TL;DR: 论文提出NAtS - L框架，在同一层对不同token应用线性和softmax注意力操作，实现高效的token级混合架构。


<details>
  <summary>Details</summary>
Motivation: softmax变压器二次计算复杂度在长上下文场景成瓶颈，现有线性注意力模型表达能力有限，混合模型受softmax层效率限制。

Method: 提出NAtS - L框架，自动判断token用线性或softmax注意力处理，搜索最优Gated DeltaNet和softmax注意力组合。

Result: 展示NAtS - L提供了强大且高效的token级混合架构。

Conclusion: NAtS - L框架能在保证表达能力的同时，有效降低计算复杂度。

Abstract: The quadratic computational complexity of softmax transformers has become a bottleneck in long-context scenarios. In contrast, linear attention model families provide a promising direction towards a more efficient sequential model. These linear attention models compress past KV values into a single hidden state, thereby efficiently reducing complexity during both training and inference. However, their expressivity remains limited by the size of their hidden state. Previous work proposed interleaving softmax and linear attention layers to reduce computational complexity while preserving expressivity. Nevertheless, the efficiency of these models remains bottlenecked by their softmax attention layers. In this paper, we propose Neural Attention Search Linear (NAtS-L), a framework that applies both linear attention and softmax attention operations within the same layer on different tokens. NAtS-L automatically determines whether a token can be handled by a linear attention model, i.e., tokens that have only short-term impact and can be encoded into fixed-size hidden states, or require softmax attention, i.e., tokens that contain information related to long-term retrieval and need to be preserved for future queries. By searching for optimal Gated DeltaNet and softmax attention combinations across tokens, we show that NAtS-L provides a strong yet efficient token-level hybrid architecture.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [466] [Error Analysis of Matrix Multiplication Emulation Using Ozaki-II Scheme](https://arxiv.org/abs/2602.02549)
*Yuki Uchino,Katsuhisa Ozaki,Toshiyuki Imamura*

Main category: math.NA

TL;DR: 对Ozaki-II方案进行严格确定性误差分析，阐明其精度行为并能估计所需低精度矩阵乘法次数。


<details>
  <summary>Details</summary>
Motivation: Ozaki-II方案在输入矩阵指数分布宽时精度会下降，需大量低精度矩阵乘法获高精度结果，因此需要对其误差进行分析。

Method: 对Ozaki-II方案进行严格的确定性误差分析。

Result: 提出的分析阐明了该方法的精度行为，能估计出达到期望数值精度所需的低精度矩阵乘法次数。

Conclusion: 该分析有助于更好地理解和应用Ozaki-II方案提高数值精度。

Abstract: The Ozaki-II scheme is an emulation method that leverages the Chinese Remainder Theorem to compute high-precision matrix multiplication via a sequence of low-precision matrix multiplications. In this scheme, the attainable numerical accuracy improves as the number of low-precision matrix multiplications increases. Previous numerical studies have shown that single- and double-precision matrix multiplication using the Ozaki-II scheme achieves higher throughput than that of standard BLAS routines on modern AI hardware equipped with fast INT8 matrix multiply-accumulate units with INT8 inputs and INT32 accumulation. However, the accuracy of the Ozaki-II scheme can degrade when the exponent distribution of the input matrices is wide, in which case a large number of low-precision matrix multiplications is required to obtain high-precision results. In this paper, we present a rigorous deterministic error analysis of the Ozaki-II scheme. The proposed analysis not only clarifies the accuracy behavior of the method but also enables the estimation of the number of low-precision matrix multiplications required to achieve a desired level of numerical accuracy.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [467] [Sharp Inequalities between Total Variation and Hellinger Distances for Gaussian Mixtures](https://arxiv.org/abs/2602.03202)
*Joonhyuk Jung,Chao Gao*

Main category: math.ST

TL;DR: 研究两个高斯位置混合分布的总变差（TV）与Hellinger距离关系，建立一般上界，构造序列证明其尖锐性，解决开放问题，有相关应用。


<details>
  <summary>Details</summary>
Motivation: 研究高斯位置混合分布中TV和Hellinger距离的关系，解决Jia等人（2023）提出的开放问题。

Method: 建立一般上界，并构造两个混合分布序列。

Result: 建立了一般上界，构造序列证明了上界的尖锐性，解决开放问题。

Conclusion: 解决了开放问题，得到高斯混合的熵特征描述，实现Hellinger距离下高斯混合的最优鲁棒估计，对经验贝叶斯的极小极大后悔有直接影响。

Abstract: We study the relation between the total variation (TV) and Hellinger distances between two Gaussian location mixtures. Our first result establishes a general upper bound: for any two mixing distributions supported on a compact set, the Hellinger distance between the two mixtures is controlled by the TV distance raised to a power $1-o(1)$, where the $o(1)$ term is of order $1/\log\log(1/\mathrm{TV})$. We also construct two sequences of mixing distributions that demonstrate the sharpness of this bound. Taken together, our results resolve an open problem raised in Jia et al. (2023) and thus lead to an entropic characterization of learning Gaussian mixtures in total variation. Our inequality also yields optimal robust estimation of Gaussian mixtures in Hellinger distance, which has a direct implication for bounding the minimax regret of empirical Bayes under Huber contamination.

</details>


### [468] [Orthogonal Approximate Message Passing Algorithms for Rectangular Spiked Matrix Models with Rotationally Invariant Noise](https://arxiv.org/abs/2602.03283)
*Haohua Chen,Songbin Liu,Junjie Ma*

Main category: math.ST

TL;DR: 提出适用于带一般旋转不变噪声的矩形尖峰矩阵模型信号估计的OAMP算法，推导最优变体，分析特殊情况并提出猜想。


<details>
  <summary>Details</summary>
Motivation: 解决带一般旋转不变噪声的矩形尖峰矩阵模型中的信号估计问题。

Method: 提出OAMP算法，建立严格状态演化来刻画算法高维动态，推导最优变体。

Result: 特殊情况与标准AMP算法定点一致，对一般噪声模型提出最优OAMP算法统计最优的猜想。

Conclusion: OAMP算法可用于信号估计，在特定情况有良好表现，在一般噪声模型或达统计最优。

Abstract: We propose an orthogonal approximate message passing (OAMP) algorithm for signal estimation in the rectangular spiked matrix model with general rotationally invariant (RI) noise. We establish a rigorous state evolution that exactly characterizes the high-dimensional dynamics of the algorithm. Building on this framework, we derive an optimal variant of OAMP that minimizes the predicted mean-squared error at each iteration. For the special case of i.i.d. Gaussian noise, the fixed point of the proposed OAMP algorithm coincides with that of the standard AMP algorithm. For general RI noise models, we conjecture that the optimal OAMP algorithm is statistically optimal within a broad class of iterative methods, and achieves Bayes-optimal performance in certain regimes.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [469] [Measuring Individual User Fairness with User Similarity and Effectiveness Disparity](https://arxiv.org/abs/2602.02516)
*Theresia Veronika Rampisela,Maria Maistro,Tuukka Ruotsalo,Christina Lioma*

Main category: cs.CY

TL;DR: 现有推荐系统个体用户公平性评估指标存在不足，本文提出新指标PUF，考虑有效性差异和用户相似度，经实验验证其优势。


<details>
  <summary>Details</summary>
Motivation: 现有个体用户公平性评估指标不能同时兼顾推荐有效性差异和用户相似度，定义不完整。

Method: 提出Pairwise User unFairness (PUF)评估指标，在4个数据集和7个排序器上进行实验验证。

Result: PUF在不同数据集和排序器上能始终兼顾有效性差异和用户相似度，而其他指标对有效性差异不敏感或对用户相似度完全不敏感。

Conclusion: 本文提出的PUF是首个能可靠捕捉个体用户公平性中用户相似度和有效性的推荐系统评估指标。

Abstract: Individual user fairness is commonly understood as treating similar users similarly. In Recommender Systems (RSs), several evaluation measures exist for quantifying individual user fairness. These measures evaluate fairness via either: (i) the disparity in RS effectiveness scores regardless of user similarity, or (ii) the disparity in items recommended to similar users regardless of item relevance. Both disparity in recommendation effectiveness and user similarity are very important in fairness, yet no existing individual user fairness measure simultaneously accounts for both. In brief, current user fairness evaluation measures implement a largely incomplete definition of fairness. To fill this gap, we present Pairwise User unFairness (PUF), a novel evaluation measure of individual user fairness that considers both effectiveness disparity and user similarity. PUF is the only measure that can express this important distinction. We empirically validate that PUF does this consistently across 4 datasets and 7 rankers, and robustly when varying user similarity or effectiveness. In contrast, all other measures are either almost insensitive to effectiveness disparity or completely insensitive to user similarity. We contribute the first RS evaluation measure to reliably capture both user similarity and effectiveness in individual user fairness. Our code: https://github.com/theresiavr/PUF-individual-user-fairness-recsys.

</details>


### [470] [CodeGuard: Improving LLM Guardrails in CS Education](https://arxiv.org/abs/2602.02509)
*Nishat Raihan,Noah Erdachew,Jayoti Devi,Joanna C. S. Santos,Marcos Zampieri*

Main category: cs.CY

TL;DR: 文章评估现有大语言模型在处理计算机科学教育领域不安全提示的能力，发现不足后提出 CodeGuard 框架。实验显示其能有效减少有害代码完成，相关资源开源。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在计算机科学教育中易受恶意提示影响，威胁学生学习和学术诚信，需应对此问题。

Method: 评估现有大语言模型处理不安全提示能力，分析其不足后提出 CodeGuard 框架，包括提示分类法、含 8000 条提示的数据集和实时检测不安全提示的轻量级模型 PromptShield。

Result: PromptShield 的 F1 分数达 0.93，超越现有防护方法；CodeGuard 能减少 30 - 65% 潜在有害或违规代码完成，且不影响合法教育任务。

Conclusion: CodeGuard 是适用于教育人工智能系统的有效防护框架，其代码、数据集和评估脚本可开源供社区使用。

Abstract: Large language models (LLMs) are increasingly embedded in Computer Science (CS) classrooms to automate code generation, feedback, and assessment. However, their susceptibility to adversarial or ill-intentioned prompts threatens student learning and academic integrity. To cope with this important issue, we evaluate existing off-the-shelf LLMs in handling unsafe and irrelevant prompts within the domain of CS education. We identify important shortcomings in existing LLM guardrails which motivates us to propose CodeGuard, a comprehensive guardrail framework for educational AI systems. CodeGuard includes (i) a first-of-its-kind taxonomy for classifying prompts; (ii) the CodeGuard dataset, a collection of 8,000 prompts spanning the taxonomy; and (iii) PromptShield, a lightweight sentence-encoder model fine-tuned to detect unsafe prompts in real time. Experiments show that PromptShield achieves 0.93 F1 score, surpassing existing guardrail methods. Additionally, further experimentation reveals that CodeGuard reduces potentially harmful or policy-violating code completions by 30-65% without degrading performance on legitimate educational tasks. The code, datasets, and evaluation scripts are made freely available to the community.

</details>


### [471] [Beyond Translation: Cross-Cultural Meme Transcreation with Vision-Language Models](https://arxiv.org/abs/2602.02510)
*Yuming Zhao,Peiyi Zhang,Oana Ignat*

Main category: cs.CY

TL;DR: 研究跨文化表情包再创作，提出基于视觉语言模型的混合框架及数据集，分析结果显示跨文化创作有局限且有方向不对称性，还提出评估框架。


<details>
  <summary>Details</summary>
Motivation: 表情包在跨文化适应上因文化特异性有挑战，研究跨文化表情包再创作以保留意图和幽默。

Method: 提出基于视觉语言模型的混合再创作框架，引入中美表情包双向大规模数据集，用人工和自动评估分析6315对表情包。

Result: 当前视觉语言模型进行跨文化表情包再创作有局限，美中再创作质量始终高于中美。

Conclusion: 确定幽默和视觉文本设计哪些可跨文化转移、哪些有挑战，提出跨文化多模态生成评估框架。

Abstract: Memes are a pervasive form of online communication, yet their cultural specificity poses significant challenges for cross-cultural adaptation. We study cross-cultural meme transcreation, a multimodal generation task that aims to preserve communicative intent and humor while adapting culture-specific references. We propose a hybrid transcreation framework based on vision-language models and introduce a large-scale bidirectional dataset of Chinese and US memes. Using both human judgments and automated evaluation, we analyze 6,315 meme pairs and assess transcreation quality across cultural directions. Our results show that current vision-language models can perform cross-cultural meme transcreation to a limited extent, but exhibit clear directional asymmetries: US-Chinese transcreation consistently achieves higher quality than Chinese-US. We further identify which aspects of humor and visual-textual design transfer across cultures and which remain challenging, and propose an evaluation framework for assessing cross-cultural multimodal generation. Our code and dataset are publicly available at https://github.com/AIM-SCU/MemeXGen.

</details>


### [472] [Training Data Governance for Brain Foundation Models](https://arxiv.org/abs/2602.02511)
*Margot Hanley,Jiunn-Tyng Yeh,Ryan Rodriguez,Jack Pilkington,Nita Farahany*

Main category: cs.CY

TL;DR: 本文探讨脑基础模型在神经数据训练中带来的规范问题，介绍其技术基础和数据生态，组织相关伦理担忧并提出问题与保障措施。


<details>
  <summary>Details</summary>
Motivation: 脑基础模型在神经数据上训练带来新的规范问题，当前治理碎片化且不清晰，需进行研究。

Method: 首先描述脑基础模型的技术基础和训练数据生态，然后借鉴AI伦理、神经伦理和生物伦理来梳理隐私、同意、偏差、利益共享和治理等方面的担忧。

Result: 梳理出了隐私、同意、偏差、利益共享和治理等方面的伦理问题，并针对每个方面提出了议程设定问题和基线保障措施。

Conclusion: 随着该领域的成熟，通过设定问题和建立保障措施来应对脑基础模型在神经数据训练中的规范挑战是必要的。

Abstract: Brain foundation models bring the foundation model paradigm to the field of neuroscience. Like language and image foundation models, they are general-purpose AI systems pretrained on large-scale datasets that adapt readily to downstream tasks. Unlike text-and-image based models, however, they train on brain data: large-datasets of EEG, fMRI, and other neural data types historically collected within tightly governed clinical and research settings. This paper contends that training foundation models on neural data opens new normative territory. Neural data carry stronger expectations of, and claims to, protection than text or images, given their body-derived nature and historical governance within clinical and research settings. Yet the foundation model paradigm subjects them to practices of large-scale repurposing, cross-context stitching, and open-ended downstream application. Furthermore, these practices are now accessible to a much broader range of actors, including commercial developers, against a backdrop of fragmented and unclear governance. To map this territory, we first describe brain foundation models' technical foundations and training-data ecosystem. We then draw on AI ethics, neuroethics, and bioethics to organize concerns across privacy, consent, bias, benefit sharing, and governance. For each, we propose both agenda-setting questions and baseline safeguards as the field matures.

</details>


### [473] [Evaluation of Large Language Models' educational feedback in Higher Education: potential, limitations and implications for educational practice](https://arxiv.org/abs/2602.02519)
*Daniele Agostini,Federica Picasso*

Main category: cs.CY

TL;DR: 研究探讨AI生成反馈对学生学习的支持作用，评估大语言模型生成的反馈，发现其有潜力成为有效反馈工具。


<details>
  <summary>Details</summary>
Motivation: 认识到管理高等教育反馈实践的重要性，且当前反馈实践受AI影响，需了解AI对反馈生成的影响以明确益处和实施策略。

Method: 采用既定分析框架，让七个大语言模型根据大学教师制定的结构化评分标准为学生项目生成定量评估和定性反馈，再用Hughes等人的框架分析反馈。

Result: 大语言模型能生成结构良好的反馈。

Conclusion: 大语言模型在明确的上下文信息和清晰指令引导下，有潜力成为可持续且有意义的反馈工具，后续还需进一步探索。

Abstract: The importance of managing feedback practices in higher education has been widely recognised, as they play a crucial role in enhancing teaching, learning, and assessment processes. In today's educational landscape, feedback practices are increasingly influenced by technological advancements, particularly artificial intelligence (AI). Understanding the impact of AI on feedback generation is essential for identifying its potential benefits and establishing effective implementation strategies. This study examines how AI-generated feedback supports student learning using a well-established analytical framework. Specifically, feedback produced by different Large Language Models (LLMs) was assessed in relation to student-designed projects within a training course on inclusive teaching and learning. The evaluation process involved providing seven LLMs with a structured rubric, developed by the university instructor, which defined specific criteria and performance levels. The LLMs were tasked with generating both quantitative assessments and qualitative feedback based on this rubric. The AI-generated feedback was then analysed using Hughes, Smith, and Creese's framework to evaluate its structure and effectiveness in fostering formative learning experiences. Overall, these findings indicate that LLMs can generate well-structured feedback and hold great potential as a sustainable and meaningful feedback tool, provided they are guided by clear contextual information and a well-defined instructions that will be explored further in the conclusions.

</details>


### [474] [Artificial Intelligence for Inclusive Engineering Education: Advancing Equality, Diversity, and Ethical Leadership](https://arxiv.org/abs/2602.02520)
*Mona G. Ibrahim,Riham Hilal*

Main category: cs.CY

TL;DR: AI技术变革工程教育，但存在性别、文化等方面公平性问题，论文提出支持联合国可持续议程的伦理方法，结果显示该方法能提升包容性和教育公平性，并提及将教育转变为全球系统。


<details>
  <summary>Details</summary>
Motivation: 解决AI技术在工程教育中存在的性别公平、文化代表性及教育就业机会等方面的差距，支持联合国2030可持续发展议程。

Method: 基于综合策略，运用批判性思维分析全球使用AI自适应平台解决教育公平差距的案例。

Result: 使用AI技术不仅提高了包容性，还促进了STEM教育的公平性。

Conclusion: 提出将教育转变为全球系统的相关结论。

Abstract: AI technology development has transformed the field of engineering education with its adaptivity-driven, data-based, and ethical-led learning platforms that promote equity, diversity, and inclusivity. But with so much progress being made in so many areas, there are unfortunately gaps in gender equity, representation in cultures around the world, and access to education and jobs in stem education. The paper describes an ethical approach to using AI technology that supports the United Nations 2030 agenda for sustainability. In particular, this includes both Goal 5--Gender Equity--and Goal 10--Reducing Inequalities. Based on a synthesis strategy using both critical thinking strategies related to case studies around the world using AI-based adaptivity platforms to address equity gaps related to education inclusion. The model presented offers a synthesis solution that includes ethical leadership data-related to equity to measure inclusivity based upon sustainability thinking. The result has demonstrated that using AI technology not only increases inclusivity but promotes equity related to access to education in stem education access. Finally, there are concluding remarks related to transforming education into a global system.

</details>


### [475] [Digital Lifelong Learning in the Age of AI: Trends and Insights](https://arxiv.org/abs/2602.03114)
*Geeta Puri,Nachamma Socklingam,Dorien Herremans*

Main category: cs.CY

TL;DR: 研究聚焦成人和终身学习者与数字学习平台互动，用调查数据分析，发现疫情后数字学习对年轻人和女性更重要，为相关方提供优化建议。


<details>
  <summary>Details</summary>
Motivation: 理解数字学习如何为成人和终身学习者持续发展，因AI和大语言模型创新加速数字学习应用，其从补充资源变为教育支柱。

Method: 使用200名受访者的多份调查数据和高级分析方法，研究不同人群与数字学习平台互动、学习者动机、游戏化效果和AI整合。

Result: 发现疫情后数字学习的感知相关性显著增加，尤其在年轻成年人和女性群体中，与支持个性化学习的大语言模型AI工具兴起同步。

Conclusion: 旨在为企业、政府政策制定者和教育工作者提供可操作的见解，以优化数字学习服务满足不断变化的劳动力需求。

Abstract: Rapid innovations in AI and large language models (LLMs) have accelerated the adoption of digital learning, particularly beyond formal education. What began as an emergency response during COVID-19 has shifted from a supplementary resource to an essential pillar of education. Understanding how digital learning continues to evolve for adult and lifelong learners is therefore increasingly important.
  This study examines how various demographics interact with digital learning platforms, focusing on the learner motivations, the effectiveness of gamification in digital learning, and the integration of AI. Using multi survey data from 200 respondents and advanced analytics, our findings reveal a notable increase in the perceived relevance of digital learning after the pandemic, especially among young adults and women, coinciding with the rise of LLM-powered AI tools that support personalized learning. We aim to provide actionable insights for businesses, government policymakers, and educators seeking to optimize their digital learning offerings to meet evolving workforce needs.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [476] [Stationarity and Spectral Characterization of Random Signals on Simplicial Complexes](https://arxiv.org/abs/2602.03055)
*Madeline Navarro,Andrei Buciulea,Santiago Segarra,Antonio Marques*

Main category: eess.SP

TL;DR: 提出用于定义在单纯复形上随机信号的概率框架，推广平稳性概念，定义拓扑功率谱密度并实证其优势。


<details>
  <summary>Details</summary>
Motivation: 数据结构复杂，现有图模型有局限，需新方法处理拓扑信号。

Method: 通过Hodge和Dirac理论的谱对偶性，定义平稳拓扑信号，给出拓扑功率谱密度。

Result: 定义的拓扑平稳性自然扩展了时间序列和图信号平稳性的理想属性，通过模拟证明了优势。

Conclusion: 提出的概率框架在处理拓扑信号上有优势且具有实际应用价值。

Abstract: It is increasingly common for data to possess intricate structure, necessitating new models and analytical tools. Graphs, a prominent type of structure, can encode the relationships between any two entities (nodes). However, graphs neither allow connections that are not dyadic nor permit relationships between sets of nodes. We thus turn to simplicial complexes for connecting more than two nodes as well as modeling relationships between simplices, such as edges and triangles. Our data then consist of signals lying on topological spaces, represented by simplicial complexes. Much recent work explores these topological signals, albeit primarily through deterministic formulations. We propose a probabilistic framework for random signals defined on simplicial complexes. Specifically, we generalize the classical notion of stationarity. By spectral dualities of Hodge and Dirac theory, we define stationary topological signals as the outputs of topological filters given white noise. This definition naturally extends desirable properties of stationarity that hold for both time-series and graph signals. Crucially, we properly define topological power spectral density (PSD) through a clear spectral characterization. We then discuss the advantages of topological stationarity due to spectral properties via the PSD. In addition, we empirically demonstrate the practicality of these benefits through multiple synthetic and real-world simulations.

</details>


### [477] [Joint single-shot ToA and DoA estimation for VAA-based BLE ranging with phase ambiguity: A deep learning-based approach](https://arxiv.org/abs/2602.02503)
*Jincheng Xie,Yili Deng,Jiguang He,Pengyu Wang,Miaomiao Dong,Rui Tang,Zhongyi Huang*

Main category: eess.SP

TL;DR: 传统到达方向估计方法在BLE设备上实现成本高，本文提出结合VAA与BLE双向CFR的统一模型及基于神经网络的相位恢复框架，可用于联合ToA和DoA估计，仿真显示性能优越。


<details>
  <summary>Details</summary>
Motivation: 传统到达方向（DoA）估计方法在尺寸受限的蓝牙低功耗（BLE）设备上实现成本高，且BLE的双向信道频率响应（CFR）存在二进制相位模糊问题，阻碍虚拟天线阵列（VAA）直接应用。

Method: 提出结合VAA与BLE双向CFR的统一模型，引入基于神经网络、采用行列预测器和投票机制的相位恢复框架来解决模糊问题，利用恢复的单向CFR进行联合到达时间（ToA）和DoA估计。

Result: 仿真结果表明，所提方法在非均匀VAA下性能优越，在SNR ≥ 5 dB时均方误差接近克拉美罗界。

Conclusion: 所提结合模型与相位恢复框架能有效解决BLE设备上DoA估计问题，实现较好的联合ToA和DoA估计性能。

Abstract: Conventional direction-of-arrival (DoA) estimation methods rely on multi-antenna arrays, which are costly to implement on size-constrained Bluetooth Low Energy (BLE) devices. Virtual antenna array (VAA) techniques enable DoA estimation with a single antenna, making angle estimation feasible on such devices. However, BLE only provides a single-shot two-way channel frequency response (CFR) with a binary phase ambiguity issue, which hinders the direct application of VAA. To address this challenge, we propose a unified model that combines VAA with BLE two-way CFR, and introduce a neural network based phase recovery framework that employs row / column predictors with a voting mechanism to resolve the ambiguity. The recovered one-way CFR then enables super resolution algorithms such as MUSIC for joint time of arrival (ToA) and DoA estimation. Simulation results demonstrate that the proposed method achieves superior performance under non-uniform VAAs, with mean square errors approaching the Cramer Rao bound at SNR $\geq$ 5 dB.

</details>


### [478] [VR-VFL: Joint Rate and Client Selection for Vehicular Federated Learning Under Imperfect CSI](https://arxiv.org/abs/2602.03711)
*Metehan Karatas,Subhrakanti Dey,Christian Rohner,Jose Mairton Barros da Silva*

Main category: eess.SP

TL;DR: 提出VR - VFL方法解决车联网联邦学习资源分配问题，仿真显示比其他方法收敛快约40%。


<details>
  <summary>Details</summary>
Motivation: 车联网联邦学习在资源分配上因车辆高移动性和不完美信道状态信息面临挑战，现有方法过于简化现实，有效性受限。

Method: 提出VR - VFL方法，结合动态客户端选择和自适应传输速率选择，允许轮次时间根据无线条件灵活变化，基于双目标优化框架。

Result: 仿真结果表明，提出的VR - VFL方案比文献中的其他方法收敛速度快约40%。

Conclusion: VR - VFL考虑了移动性挑战和实际无线约束，为车联网边缘网络中的联邦学习提供了更实用、高效的方法。

Abstract: Federated learning in vehicular edge networks faces major challenges in efficient resource allocation, largely due to high vehicle mobility and the presence of imperfect channel state information. Many existing methods oversimplify these realities, often assuming fixed communication rounds or ideal channel conditions, which limits their effectiveness in real-world scenarios. To address this, we propose variable rate vehicular federated learning (VR-VFL), a novel federated learning method designed specifically for vehicular networks under imperfect channel state information. VR-VFL combines dynamic client selection with adaptive transmission rate selection, while also allowing round times to flex in response to changing wireless conditions. At its core, VR-VFL is built on a bi-objective optimization framework that strikes a balance between improving learning convergence and minimizing the time required to complete each round. By accounting for both the challenges of mobility and realistic wireless constraints, VR-VFL offers a more practical and efficient approach to federated learning in vehicular edge networks. Simulation results show that the proposed VR-VFL scheme achieves convergence approximately 40% faster than other methods in the literature.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [479] [Game-Theoretic and Algorithmic Analyses of Multi-Agent Routing under Crossing Costs](https://arxiv.org/abs/2602.03455)
*Tesshu Hanaka,Nikolaos Melissinos,Hirotaka Ono*

Main category: cs.MA

TL;DR: 本文提出混合图上跨路成本的多智能体路由模型，解决异步环境下多智能体协调问题，从博弈论和算法层面给出贡献，提供理论基础。


<details>
  <summary>Details</summary>
Motivation: 传统多智能体路径规划依赖集中控制和同步避碰，不适用于异步环境，需新的解决方案。

Method: 提出新模型，将其建模为具有非标准成本函数的拥塞博弈，分析平衡点；设计参数化算法最小化跨路成本。

Result: 证明纯纳什均衡存在，一般情况下问题是PLS完全，最小化总跨路成本是NP难，参数化算法有XP或FPT结果。

Conclusion: 框架为分布式多智能体路由提供新理论基础，支持可扩展和风险感知的协调。

Abstract: Coordinating the movement of multiple autonomous agents over a shared network is a fundamental challenge in algorithmic robotics, intelligent transportation, and distributed systems. The dominant approach, Multi-Agent Path Finding, relies on centralized control and synchronous collision avoidance, which often requires strict synchronization and guarantees of globally conflict-free execution. This paper introduces the Multi-Agent Routing under Crossing Cost model on mixed graphs, a novel framework tailored to asynchronous settings. In our model, instead of treating conflicts as hard constraints, each agent is assigned a path, and the system is evaluated through a cost function that measures potential head-on encounters. This ``crossing cost'', which is defined as the product of the numbers of agents traversing an edge in opposite directions, quantifies the risk of congestion and delay in decentralized execution.
  Our contributions are both game-theoretic and algorithmic. We model the setting as a congestion game with a non-standard cost function, prove the existence of pure Nash equilibria, and analyze the dynamics leading to them. Equilibria can be found in polynomial time under mild conditions, while the general case is PLS-complete. From an optimization perspective, minimizing the total crossing cost is NP-hard, as the problem generalizes Steiner Orientation. To address this hardness barrier, we design a suite of parameterized algorithms for minimizing crossing cost, with parameters including the number of arcs, edges, agents, and structural graph measures. These yield XP or FPT results depending on the parameter, offering algorithmic strategies for structurally restricted instances. Our framework provides a new theoretical foundation for decentralized multi-agent routing, bridging equilibrium analysis and parameterized complexity to support scalable and risk-aware coordination.

</details>


### [480] [When Should Agents Coordinate in Differentiable Sequential Decision Problems?](https://arxiv.org/abs/2602.03674)
*Caleb Probine,Su Ann Low,David Fridovich-Keil,Ufuk Topcu*

Main category: cs.MA

TL;DR: 本文探讨可微运动规划问题中协调的价值，将协调行为建模为光谱，提出基于二阶推理的算法来确定团队何时应协调。


<details>
  <summary>Details</summary>
Motivation: 多机器人团队需协调以有效运作，但协调需成本高的通信，研究可微运动规划问题中协调的价值。

Method: 将协调行为建模为光谱，从联合优化团队目标到在纳什均衡下各自决定，通过分析代理人目标的二阶属性来推理协调。

Result: 证明可微运动规划问题中关于协调的推理可归结为对代理人目标二阶属性的推理。

Conclusion: 提供了用二阶推理来确定机器人团队何时应协调的算法。

Abstract: Multi-robot teams must coordinate to operate effectively. When a team operates in an uncoordinated manner, and agents choose actions that are only individually optimal, the team's outcome can suffer. However, in many domains, coordination requires costly communication. We explore the value of coordination in a broad class of differentiable motion-planning problems. In particular, we model coordinated behavior as a spectrum: at one extreme, agents jointly optimize a common team objective, and at the other, agents make unilaterally optimal decisions given their individual decision variables, i.e., they operate at Nash equilibria. We then demonstrate that reasoning about coordination in differentiable motion-planning problems reduces to reasoning about the second-order properties of agents' objectives, and we provide algorithms that use this second-order reasoning to determine at which times a team of agents should coordinate.

</details>


### [481] [Exploring Silicon-Based Societies: An Early Study of the Moltbook Agent Community](https://arxiv.org/abs/2602.02613)
*Yu-Zheng Lin,Bono Po-Jen Shih,Hsuan-Ying Alessandra Chien,Shalaka Satam,Jesus Horacio Pacheco,Sicong Shao,Soheil Salehi,Pratik Satam*

Main category: cs.MA

TL;DR: 本文引入数据驱动的硅社会学框架，通过对Moltbook平台数据挖掘，研究自治大语言模型智能体社会结构形成，发现智能体组织集体空间的模式，为硅社会学奠定方法基础。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法充分理解自治大语言模型智能体生态系统的集体行为，需要系统的实证框架来研究智能体间社会结构形成。

Method: 对Moltbook平台进行大规模数据挖掘，通过程序化和非侵入式数据采集，对12,758个子板块文本描述进行预处理、上下文嵌入和无监督聚类分析。

Result: 发现自治智能体通过涵盖类人兴趣、以硅为中心的自我反思以及早期经济和协调行为的可重复模式来系统地组织集体空间，这些结构直接从机器生成的数据痕迹中出现。

Conclusion: 为数据驱动的硅社会学建立了方法论基础，证明数据挖掘技术可用于理解大型自治智能体社会的组织和演变。

Abstract: The rapid emergence of autonomous large language model agents has given rise to persistent, large-scale agent ecosystems whose collective behavior cannot be adequately understood through anecdotal observation or small-scale simulation. This paper introduces data-driven silicon sociology as a systematic empirical framework for studying social structure formation among interacting artificial agents. We present a pioneering large-scale data mining investigation of an in-the-wild agent society by analyzing Moltbook, a social platform designed primarily for agent-to-agent interaction. At the time of study, Moltbook hosted over 150,000 registered autonomous agents operating across thousands of agent-created sub-communities. Using programmatic and non-intrusive data acquisition, we collected and analyzed the textual descriptions of 12,758 submolts, which represent proactive sub-community partitioning activities within the ecosystem. Treating agent-authored descriptions as first-class observational artifacts, we apply rigorous preprocessing, contextual embedding, and unsupervised clustering techniques to uncover latent patterns of thematic organization and social space structuring. The results show that autonomous agents systematically organize collective space through reproducible patterns spanning human-mimetic interests, silicon-centric self-reflection, and early-stage economic and coordination behaviors. Rather than relying on predefined sociological taxonomies, these structures emerge directly from machine-generated data traces. This work establishes a methodological foundation for data-driven silicon sociology and demonstrates that data mining techniques can provide a powerful lens for understanding the organization and evolution of large autonomous agent societies.

</details>


### [482] [Scaling Small Agents Through Strategy Auctions](https://arxiv.org/abs/2602.02751)
*Lisa Alazraki,William F. Shen,Yoram Bachrach,Akhil Mathur*

Main category: cs.MA

TL;DR: 研究发现小语言模型在复杂任务性能不佳，提出SALE框架，能降低大模型依赖和成本，提升性能，建议从系统层面构建智能体生态。


<details>
  <summary>Details</summary>
Motivation: 明确小语言模型在不同复杂度任务的性能表现，以及如何更好利用小模型处理长周期工作负载。

Method: 提出Strategy Auctions for Workload Efficiency (SALE)框架，让智能体用战略计划投标，通过成本 - 价值机制评分并借助共享拍卖记忆优化。

Result: 在深度搜索和编码任务中，SALE减少53%对最大智能体的依赖，降低35%总体成本，提升最大智能体的pass@1。

Conclusion: 小智能体处理复杂工作负载不足，但可通过协调任务分配和测试时自我改进提升，应从系统层面构建智能体生态。

Abstract: Small language models are increasingly viewed as a promising, cost-effective approach to agentic AI, with proponents claiming they are sufficiently capable for agentic workflows. However, while smaller agents can closely match larger ones on simple tasks, it remains unclear how their performance scales with task complexity, when large models become necessary, and how to better leverage small agents for long-horizon workloads. In this work, we empirically show that small agents' performance fails to scale with task complexity on deep search and coding tasks, and we introduce Strategy Auctions for Workload Efficiency (SALE), an agent framework inspired by freelancer marketplaces. In SALE, agents bid with short strategic plans, which are scored by a systematic cost-value mechanism and refined via a shared auction memory, enabling per-task routing and continual self-improvement without training a separate router or running all models to completion. Across deep search and coding tasks of varying complexity, SALE reduces reliance on the largest agent by 53%, lowers overall cost by 35%, and consistently improves upon the largest agent's pass@1 with only a negligible overhead beyond executing the final trace. In contrast, established routers that rely on task descriptions either underperform the largest agent or fail to reduce cost -- often both -- underscoring their poor fit for agentic workflows. These results suggest that while small agents may be insufficient for complex workloads, they can be effectively "scaled up" through coordinated task allocation and test-time self-improvement. More broadly, they motivate a systems-level view of agentic AI in which performance gains come less from ever-larger individual models and more from market-inspired coordination mechanisms that organize heterogeneous agents into efficient, adaptive ecosystems.

</details>


### [483] [Agent Primitives: Reusable Latent Building Blocks for Multi-Agent Systems](https://arxiv.org/abs/2602.03695)
*Haibo Jin,Kuang Peng,Ye Yu,Xiaopeng Yuan,Haohan Wang*

Main category: cs.MA

TL;DR: 提出Agent Primitives用于LLM - 基多智能体系统，实验显示其有更好的准确性、效率和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体系统任务特定性强、架构复杂、可复用性低，且基于自然语言通信易出错和不稳定。

Method: 提出Review、Voting and Selection、Planning and Execution三个基本组件，通过键值缓存通信，由Organizer智能体根据知识池选择和组合基本组件。

Result: 基于基本组件的多智能体系统比单智能体基线平均准确率提高12.0 - 16.5%，相比基于文本的多智能体系统减少约3 - 4倍的令牌使用和推理延迟。

Conclusion: 基于Agent Primitives的多智能体系统能提高准确性、效率和稳定性，且开销相对合理。

Abstract: While existing multi-agent systems (MAS) can handle complex problems by enabling collaboration among multiple agents, they are often highly task-specific, relying on manually crafted agent roles and interaction prompts, which leads to increased architectural complexity and limited reusability across tasks. Moreover, most MAS communicate primarily through natural language, making them vulnerable to error accumulation and instability in long-context, multi-stage interactions within internal agent histories.
  In this work, we propose \textbf{Agent Primitives}, a set of reusable latent building blocks for LLM-based MAS. Inspired by neural network design, where complex models are built from reusable components, we observe that many existing MAS architectures can be decomposed into a small number of recurring internal computation patterns. Based on this observation, we instantiate three primitives: Review, Voting and Selection, and Planning and Execution. All primitives communicate internally via key-value (KV) cache, which improves both robustness and efficiency by mitigating information degradation across multi-stage interactions. To enable automatic system construction, an Organizer agent selects and composes primitives for each query, guided by a lightweight knowledge pool of previously successful configurations, forming a primitive-based MAS.
  Experiments show that primitives-based MAS improve average accuracy by 12.0-16.5\% over single-agent baselines, reduce token usage and inference latency by approximately 3$\times$-4$\times$ compared to text-based MAS, while incurring only 1.3$\times$-1.6$\times$ overhead relative to single-agent inference and providing more stable performance across model backbones.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [484] [The Evolution of Lying in a Spatially-Explicit Prisoner's Dilemma Model](https://arxiv.org/abs/2602.02587)
*Gregg Hartvigsen*

Main category: physics.soc-ph

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: I present the results from a spatial model of the prisoner's dilemma, played on a toroidal lattice. Each individual has a default strategy of either cooperating ($C$) or defecting ($D$). Two strategies were tested, including ``tit-for-tat'' (TFT), in which individuals play their opponent's last play, or simply playing their default play. Each individual also has a probability of telling the truth ($0 \leq P_{truth} \leq 1$) about their last play. This parameter, which can evolve over time, allows individuals to be, for instance, a defector but present as a cooperator regarding their last play. This leads to interesting dynamics where mixed populations of defectors and cooperators with $P_{truth} \geq 0.75$ move toward populations of truth-telling cooperators. Likewise, mixed populations with $P_{truth} < 0.7$ become populations of lying defectors. Both such populations are stable because they each have higher average scores than populations with intermediate values of $P_{truth}$. Applications of this model are discussed with regards to both humans and animals.

</details>


### [485] [Social Catalysts, Not Moral Agents: The Illusion of Alignment in LLM Societies](https://arxiv.org/abs/2602.02598)
*Yueqing Hu,Yixuan Jiang,Zehua Jiang,Xiao Wen,Tianhong Wang*

Main category: physics.soc-ph

TL;DR: 研究锚定代理在公共物品博弈中促进合作的效果，发现提升合作率但非真正规范内化，揭示人工社会行为修改与价值对齐的差距。


<details>
  <summary>Details</summary>
Motivation: 大语言模型发展使多智能体系统中集体合作受‘公地悲剧’威胁，研究锚定代理在公共物品博弈中促进合作的有效性。

Method: 采用全因子设计，对三个最先进的大语言模型进行研究，分析行为结果和内部推理链。

Result: 锚定代理提升了局部合作率，但效果源于战略合规和认知卸载，非真正规范内化，多数智能体在新环境回归利己，高级模型有‘变色龙效应’。

Conclusion: 人工社会中行为修改和真正的价值对齐存在关键差距。

Abstract: The rapid evolution of Large Language Models (LLMs) has led to the emergence of Multi-Agent Systems where collective cooperation is often threatened by the "Tragedy of the Commons." This study investigates the effectiveness of Anchoring Agents--pre-programmed altruistic entities--in fostering cooperation within a Public Goods Game (PGG). Using a full factorial design across three state-of-the-art LLMs, we analyzed both behavioral outcomes and internal reasoning chains. While Anchoring Agents successfully boosted local cooperation rates, cognitive decomposition and transfer tests revealed that this effect was driven by strategic compliance and cognitive offloading rather than genuine norm internalization. Notably, most agents reverted to self-interest in new environments, and advanced models like GPT-4.1 exhibited a "Chameleon Effect," masking strategic defection under public scrutiny. These findings highlight a critical gap between behavioral modification and authentic value alignment in artificial societies.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [486] [Downscaling land surface temperature data using edge detection and block-diagonal Gaussian process regression](https://arxiv.org/abs/2602.02813)
*Sanjit Dandapanthula,Margaret Johnson,Madeleine Pascolini-Campbell,Glynn Hulley,Mikael Kuusela*

Main category: stat.AP

TL;DR: 本文提出新统计方法对NASA ECOSTRESS任务的地表温度（LST）数据进行降尺度处理，可获高分辨率LST估计值。


<details>
  <summary>Details</summary>
Motivation: 准确且高分辨率的LST估计对农业应用中蒸散量的估算至关重要，需要开发新方法获得高分辨率LST数据。

Method: 利用Landsat 8高分辨率数据，通过边缘检测技术确定农田边界，提出块对角高斯过程（BDGP）模型，进行高斯过程回归并量化不确定性。

Result: 所提方法能产生可靠的高分辨率LST估计值。

Conclusion: 该方法具有实用性，在农业、城市规划和气候研究等领域有潜在应用。

Abstract: Accurate and high-resolution estimation of land surface temperature (LST) is crucial in estimating evapotranspiration, a measure of plant water use and a central quantity in agricultural applications. In this work, we develop a novel statistical method for downscaling LST data obtained from NASA's ECOSTRESS mission, using high-resolution data from the Landsat 8 mission as a proxy for modeling agricultural field structure. Using the Landsat data, we identify the boundaries of agricultural fields through edge detection techniques, allowing us to capture the inherent block structure present in the spatial domain. We propose a block-diagonal Gaussian process (BDGP) model that captures the spatial structure of the agricultural fields, leverages independence of LST across fields for computational tractability, and accounts for the change of support present in ECOSTRESS observations. We use the resulting BDGP model to perform Gaussian process regression and obtain high-resolution estimates of LST from ECOSTRESS data, along with uncertainty quantification. Our results demonstrate the practicality of the proposed method in producing reliable high-resolution LST estimates, with potential applications in agriculture, urban planning, and climate studies.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [487] [Kriging for large datasets via penalized neighbor selection](https://arxiv.org/abs/2602.03483)
*Francisco Cuevas-Pacheco,Jonathan Acosta*

Main category: stat.ME

TL;DR: 传统克里金法处理大数据集计算复杂，本文提出惩罚克里金框架，结合自适应LASSO实现数据驱动的邻域选择，降低计算成本并保持预测精度


<details>
  <summary>Details</summary>
Motivation: 传统克里金法处理大数据集计算复杂度高，局部克里金法选择K值缺乏考虑空间相关性

Method: 提出惩罚克里金框架，将LASSO型惩罚项融入克里金方程，扩展为自适应LASSO，通过l1正则化确定非零权重观测值，用基于有效样本量的准则选择惩罚参数

Result: 数值实验表明，惩罚克里金法能自动调整邻域结构，在降低计算成本的同时保持与全局克里金法相当的预测精度

Conclusion: 惩罚克里金法可有效处理大数据集的空间预测问题，在计算成本和预测精度间取得平衡

Abstract: Kriging is a fundamental tool for spatial prediction, but its computational complexity of $O(N^3)$ becomes prohibitive for large datasets. While local kriging using $K$-nearest neighbors addresses this issue, the selection of $K$ typically relies on ad-hoc criteria that fail to account for spatial correlation structure. We propose a penalized kriging framework that incorporates LASSO-type penalties directly into the kriging equations to achieve automatic, data-driven neighbor selection. We further extend this to adaptive LASSO, using data-driven penalty weights that account for the spatial correlation structure. Our method determines which observations contribute non-zero weights through $\ell_1$ regularization, with the penalty parameter selected via a novel criterion based on effective sample size that balances prediction accuracy against information redundancy. Numerical experiments demonstrate that penalized kriging automatically adapts neighborhood structure to the underlying spatial correlation, selecting fewer neighbors for smoother processes and more for highly variable fields, while maintaining prediction accuracy comparable to global kriging at substantially reduced computational cost.

</details>


### [488] [Entropic Mirror Monte Carlo](https://arxiv.org/abs/2602.03165)
*Anas Cherradi,Yazid Janati,Alain Durmus,Sylvain Le Corff,Yohan Petetin,Julien Stoehr*

Main category: stat.ME

TL;DR: 提出用于构建高效重要性采样提议分布的自适应方案并证明收敛性、做数值实验。


<details>
  <summary>Details</summary>
Motivation: 当目标分布复杂时，重要性采样效率关键取决于提议分布选择，需构建高效提议分布。

Method: 结合全局采样机制和延迟加权过程，加权机制使提议分布与目标不匹配区域能快速重采样。

Result: 采样算法在温和假设下几何收敛，通过数值实验说明算法。

Conclusion: 提出的自适应方案能用于构建高效提议分布，算法有收敛性。

Abstract: Importance sampling is a Monte Carlo method which designs estimators of expectations under a target distribution using weighted samples from a proposal distribution. When the target distribution is complex, such as multimodal distributions in highdimensional spaces, the efficiency of importance sampling critically depends on the choice of the proposal distribution. In this paper, we propose a novel adaptive scheme for the construction of efficient proposal distributions. Our algorithm promotes efficient exploration of the target distribution by combining global sampling mechanisms with a delayed weighting procedure. The proposed weighting mechanism plays a key role by enabling rapid resampling in regions where the proposal distribution is poorly adapted to the target. Our sampling algorithm is shown to be geometrically convergent under mild assumptions and is illustrated through various numerical experiments.

</details>


### [489] [Simulation-Based Inference via Regression Projection and Batched Discrepancies](https://arxiv.org/abs/2602.03613)
*Arya Farahi,Jonah Rose,Paul Torrey*

Main category: stat.ME

TL;DR: 分析了一种轻量级基于模拟的推理方法，该方法仅通过对观测数据进行基于回归的投影来推断模拟器参数，给出方法理论性质并通过实验展示其优势与局限性。


<details>
  <summary>Details</summary>
Motivation: 提出一种简单、可并行且对数据要求低的基于模拟的推理方法。

Method: 拟合代理线性回归，在建议参数值处模拟小批量数据，根据批量 - 残差差异分配核权重，构建自归一化伪后验；进行理论证明和实验验证。

Result: 证明了方法随着参数抽样数量增加的一致性、有限样本估计代理回归的稳定性；刻画了批量大小增加和带宽缩小时的渐近集中性；实验展示了基于回归投影的计算优势和低信息摘要导致的可识别性局限。

Conclusion: 该方法有计算优势，但在低信息摘要情况下存在可识别性问题，明确了方法何时产生点识别与集识别。

Abstract: We analyze a lightweight simulation-based inference method that infers simulator parameters using only a regression-based projection of the observed data. After fitting a surrogate linear regression once, the procedure simulates small batches at the proposed parameter values and assigns kernel weights based on the resulting batch-residual discrepancy, producing a self-normalized pseudo-posterior that is simple, parallelizable, and requires access only to the fitted regression coefficients rather than raw observations. We formalize the construction as an importance-sampling approximation to a population target that averages over simulator randomness, prove consistency as the number of parameter draws grows, and establish stability in estimating the surrogate regression from finite samples. We then characterize the asymptotic concentration as the batch size increases and the bandwidth shrinks, showing that the pseudo-posterior concentrates on an identified set determined by the chosen projection, thereby clarifying when the method yields point versus set identification. Experiments on a tractable nonlinear model and on a cosmological calibration task using the DREAMS simulation suite illustrate the computational advantages of regression-based projections and the identifiability limitations arising from low-information summaries.

</details>


### [490] [Weighted Sum-of-Trees Model for Clustered Data](https://arxiv.org/abs/2602.02931)
*Kevin McCoy,Zachary Wooten,Katarzyna Tomczak,Christine B. Peterson*

Main category: stat.ME

TL;DR: 本文针对聚类数据建模问题，提出轻量级树之和模型，在模拟和真实数据中表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统聚类数据建模方法在预测样本外组数据时假设所有簇共享通用结果模型，存在局限性，需要改进。

Method: 提出轻量级树之和模型，为每个样本组学习决策树，用权重组合树的预测结果。

Result: 在多种模拟设置中，模型性能优于传统决策树和随机森林；在癌症基因组图谱肉瘤队列真实数据中得到验证。

Conclusion: 所提模型能更好地处理聚类数据，在预测和组间相似性推断上有优势。

Abstract: Clustered data, which arise when observations are nested within groups, are incredibly common in clinical, education, and social science research. Traditionally, a linear mixed model, which includes random effects to account for within-group correlation, would be used to model the observed data and make new predictions on unseen data. Some work has been done to extend the mixed model approach beyond linear regression into more complex and non-parametric models, such as decision trees and random forests. However, existing methods are limited to using the global fixed effects for prediction on data from out-of-sample groups, effectively assuming that all clusters share a common outcome model. We propose a lightweight sum-of-trees model in which we learn a decision tree for each sample group. We combine the predictions from these trees using weights so that out-of-sample group predictions are more closely aligned with the most similar groups in the training data. This strategy also allows for inference on the similarity across groups in the outcome prediction model, as the unique tree structures and variable importances for each group can be directly compared. We show our model outperforms traditional decision trees and random forests in a variety of simulation settings. Finally, we showcase our method on real-world data from the sarcoma cohort of The Cancer Genome Atlas, where patient samples are grouped by sarcoma subtype.

</details>


<div id='cond-mat.dis-nn'></div>

# cond-mat.dis-nn [[Back]](#toc)

### [491] [Physics-inspired transformer quantum states via latent imaginary-time evolution](https://arxiv.org/abs/2602.03031)
*Kimihiro Yamazaki,Itsushi Sakata,Takuya Konishi,Yoshinobu Kawahara*

Main category: cond-mat.dis-nn

TL;DR: 提出物理透明框架，引入PITQS，在J1 - J2海森堡模型上用更少变分参数达可比或更优精度，弥合黑箱表达与物理透明构建的差距。


<details>
  <summary>Details</summary>
Motivation: 现有神经量子态（NQS）架构常被视为黑箱，缺乏物理透明性。

Method: 将NQS视为潜在虚时演化的神经近似，引入PITQS，通过跨层共享权重和Trotter - Suzuki分解提高传播精度。

Result: 在受挫J1 - J2海森堡模型上，PITQS用更少变分参数达到与或超过最先进的基于Transformer的NQS（TQS）的精度。

Conclusion: 将深度网络结构重新解释为潜在冷却过程，能实现更具物理基础、系统和紧凑的设计。

Abstract: Neural quantum states (NQS) are powerful ansätze in the variational Monte Carlo framework, yet their architectures are often treated as black boxes. We propose a physically transparent framework in which NQS are treated as neural approximations to latent imaginary-time evolution. This viewpoint suggests that standard Transformer-based NQS (TQS) architectures correspond to physically unmotivated effective Hamiltonians dependent on imaginary time in a latent space. Building on this interpretation, we introduce physics-inspired transformer quantum states (PITQS), which enforce a static effective Hamiltonian by sharing weights across layers and improve propagation accuracy via Trotter-Suzuki decompositions without increasing the number of variational parameters. For the frustrated $J_1$-$J_2$ Heisenberg model, our ansätze achieve accuracies comparable to or exceeding state-of-the-art TQS while using substantially fewer variational parameters. This study demonstrates that reinterpreting the deep network structure as a latent cooling process enables a more physically grounded, systematic, and compact design, thereby bridging the gap between black-box expressivity and physically transparent construction.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [492] [WAXAL: A Large-Scale Multilingual African Language Speech Corpus](https://arxiv.org/abs/2602.02734)
*Abdoulaye Diack,Perry Nelson,Kwaku Agbesi,Angela Nakalembe,MohamedElfatih MohamedKhair,Vusumuzi Dube,Tavonga Siyavora,Subhashini Venugopalan,Jason Hickey,Uche Okonkwo,Abhishek Bapna,Isaac Wiafe,Raynard Dodzi Helegah,Elikem Doe Atsakpo,Charles Nutrokpor,Fiifi Baffoe Payin Winful,Kafui Kwashie Solaga,Jamal-Deen Abdulai,Akon Obu Ekpezu,Audace Niyonkuru,Samuel Rutunda,Boris Ishimwe,Michael Melese,Engineer Bainomugisha,Joyce Nakatumba-Nabende,Andrew Katumba,Claire Babirye,Jonathan Mukiibi,Vincent Kimani,Samuel Kibacia,James Maina,Fridah Emmah,Ahmed Ibrahim Shekarau,Ibrahim Shehu Adamu,Yusuf Abdullahi,Howard Lakougna,Bob MacDonald,Hadar Shemtov,Aisha Walcott-Bryant,Moustapha Cisse,Avinatan Hassidim,Jeff Dean,Yossi Matias*

Main category: eess.AS

TL;DR: 本文介绍了面向21种撒哈拉以南非洲语言的大规模开放语音数据集WAXAL，包括数据构成、收集方法等，数据集已发布。


<details>
  <summary>Details</summary>
Motivation: 解决语音技术发展在高资源语言和撒哈拉以南非洲语言间造成的数字鸿沟问题。

Method: 与四个非洲学术和社区组织合作进行数据收集、标注和质量控制。

Result: 创建了包含约1250小时自动语音识别数据集和超180小时文本转语音数据集的WAXAL。

Conclusion: WAXAL数据集以CC - BY - 4.0许可发布，可促进研究、推动包容性技术发展和语言数字保护。

Abstract: The advancement of speech technology has predominantly favored high-resource languages, creating a significant digital divide for speakers of most Sub-Saharan African languages. To address this gap, we introduce WAXAL, a large-scale, openly accessible speech dataset for 21 languages representing over 100 million speakers. The collection consists of two main components: an Automated Speech Recognition (ASR) dataset containing approximately 1,250 hours of transcribed, natural speech from a diverse range of speakers, and a Text-to-Speech (TTS) dataset with over 180 hours of high-quality, single-speaker recordings reading phonetically balanced scripts. This paper details our methodology for data collection, annotation, and quality control, which involved partnerships with four African academic and community organizations. We provide a detailed statistical overview of the dataset and discuss its potential limitations and ethical considerations. The WAXAL datasets are released at https://huggingface.co/datasets/google/WaxalNLP under the permissive CC-BY-4.0 license to catalyze research, enable the development of inclusive technologies, and serve as a vital resource for the digital preservation of these languages.

</details>


### [493] [Conditional Flow Matching for Visually-Guided Acoustic Highlighting](https://arxiv.org/abs/2602.03762)
*Hugo Malard,Gael Le Lan,Daniel Wong,David Lou Alon,Yi-Chiao Wu,Sanjeel Parekh*

Main category: eess.AS

TL;DR: 本文将视觉引导的声学突出任务重新定义为生成问题，引入CFM框架，提出滚动损失和条件模块，实验表明该方法优于现有判别式方法。


<details>
  <summary>Details</summary>
Motivation: 现有视觉引导的声学突出研究不足，判别式模型在音频混音中存在局限性，视觉和听觉焦点易出现不对齐。

Method: 将任务重构为生成问题，引入条件流匹配（CFM）框架；提出滚动损失以稳定长程流积分；提出条件模块用于融合音频和视觉线索。

Result: 广泛的定量和定性评估表明，该方法持续超越先前的判别式方法。

Conclusion: 通过生成式建模能够更好地解决视觉引导的音频混音问题。

Abstract: Visually-guided acoustic highlighting seeks to rebalance audio in alignment with the accompanying video, creating a coherent audio-visual experience. While visual saliency and enhancement have been widely studied, acoustic highlighting remains underexplored, often leading to misalignment between visual and auditory focus. Existing approaches use discriminative models, which struggle with the inherent ambiguity in audio remixing, where no natural one-to-one mapping exists between poorly-balanced and well-balanced audio mixes. To address this limitation, we reframe this task as a generative problem and introduce a Conditional Flow Matching (CFM) framework. A key challenge in iterative flow-based generation is that early prediction errors -- in selecting the correct source to enhance -- compound over steps and push trajectories off-manifold. To address this, we introduce a rollout loss that penalizes drift at the final step, encouraging self-correcting trajectories and stabilizing long-range flow integration. We further propose a conditioning module that fuses audio and visual cues before vector field regression, enabling explicit cross-modal source selection. Extensive quantitative and qualitative evaluations show that our method consistently surpasses the previous state-of-the-art discriminative approach, establishing that visually-guided audio remixing is best addressed through generative modeling.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [494] [ProOPF: Benchmarking and Improving LLMs for Professional-Grade Power Systems Optimization Modeling](https://arxiv.org/abs/2602.03070)
*Chao Shen,Zihan Guo,Xu Wan,Zhenghao Yang,Yifan Zhang,Wengi Huang,Jie Song,Zongyan Zhang,Mingyang Sun*

Main category: eess.SY

TL;DR: 引入可再生能源使电力系统运行面临不确定性，现有大语言模型在电力系统OPF建模评估有限，因此推出ProOPF - D数据集和ProOPF - B基准。


<details>
  <summary>Details</summary>
Motivation: 可再生能源渗透率增加使电力系统运行需频繁调整调度目标和约束，现有大语言模型在电力系统OPF建模评估不足。

Method: 引入ProOPF - D数据集，包含12K个自然语言请求与OPF参数调整、结构扩展及可执行实现的配对实例；引入ProOPF - B基准，提供121个有专家标注和真实代码的测试用例。

Result: 推出了专业级OPF建模的数据集ProOPF - D和基准ProOPF - B。

Conclusion: 新的数据集和基准可用于电力系统OPF建模的端到端评估。

Abstract: Growing renewable penetration introduces substantial uncertainty into power system operations, necessitating frequent adaptation of dispatch objectives and constraints and challenging expertise-intensive, near-real-time modeling workflows. Large Language Models (LLMs) provide a promising avenue for automating this process by translating natural-language (NL) operational requirements into executable optimization models via semantic reasoning and code synthesis. Yet existing LLM datasets and benchmarks for optimization modeling primarily target coarse-grained cross-domain generalization, offering limited, rigorous evaluation in power-system settings, particularly for Optimal Power Flow (OPF). We therefore introduce \textbf{ProOPF-D} and \textbf{ProOPF-B}, a dataset and benchmark for professional-grade OPF modeling: ProOPF-D contains 12K instances pairing NL requests with parameter adjustments and structural extensions to a canonical OPF, together with executable implementations; ProOPF-B provides 121 expert-annotated test cases with ground-truth code, enabling end-to-end evaluation under both concrete and abstract OPF modeling regimes.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [495] [Morphe: High-Fidelity Generative Video Streaming with Vision Foundation Model](https://arxiv.org/abs/2602.03529)
*Tianyi Gong,Zijian Cao,Zixing Zhang,Jiangkai Wu,Xinggong Zhang,Shuguang Cui,Fangxin Wang*

Main category: cs.NI

TL;DR: 现有视频流技术在压缩和实时性上存在不足，本文提出基于VFM的端到端生成式视频流范式Morphe，节省带宽且能在复杂网络环境实时传输。


<details>
  <summary>Details</summary>
Motivation: 现有视频流技术在压缩和实时性方面存在局限，利用VFM强大的视频理解和处理能力，实现高压缩率、泛化性、高保真和抗丢包的实时视频流。

Method: 采用视觉分词器联合训练和可变分辨率时空优化，构建利用智能丢包抵抗网络干扰的鲁棒流系统。

Result: 与H.265相比，Morphe在视觉质量相当的情况下节省62.5%带宽，能在挑战性网络环境中实现实时、抗丢包视频传输。

Conclusion: Morphe是VFM赋能多媒体流解决方案的一个里程碑。

Abstract: Video streaming is a fundamental Internet service, while the quality still cannot be guaranteed especially in poor network conditions such as bandwidth-constrained and remote areas. Existing works mainly work towards two directions: traditional pixel-codec streaming nearly approaches its limit and is hard to step further in compression; the emerging neural-enhanced or generative streaming usually fall short in latency and visual fidelity, hindering their practical deployment. Inspired by the recent success of vision foundation model (VFM), we strive to harness the powerful video understanding and processing capacities of VFM to achieve generalization, high fidelity and loss resilience for real-time video streaming with even higher compression rate. We present the first revolutionized paradigm that enables VFM-based end-to-end generative video streaming towards this goal. Specifically, Morphe employs joint training of visual tokenizers and variable-resolution spatiotemporal optimization under simulated network constraints. Additionally, a robust streaming system is constructed that leverages intelligent packet dropping to resist real-world network perturbations. Extensive evaluation demonstrates that Morphe achieves comparable visual quality while saving 62.5\% bandwidth compared to H.265, and accomplishes real-time, loss-resilient video delivery in challenging network environments, representing a milestone in VFM-enabled multimedia streaming solutions.

</details>


### [496] [NSC-SL: A Bandwidth-Aware Neural Subspace Compression for Communication-Efficient Split Learning](https://arxiv.org/abs/2602.02696)
*Zhen Fang,Miao Yang,Zehang Lin,Zheng Lin,Zihan Fang,Zongyuan Zhang,Tianyang Duan,Dong Huang,Shunzhi Zhu*

Main category: cs.NI

TL;DR: 提出NSC - SL算法解决分裂学习通信开销大问题，实验证明其性能出色。


<details>
  <summary>Details</summary>
Motivation: 神经网络规模扩大，分裂学习通信开销大，需解决通信效率问题。

Method: NSC - SL先根据奇异值分布动态确定低秩近似最优秩，再用带残差反馈的交替正交迭代进行误差补偿张量分解。

Result: NSC - SL能在保证收敛所需信息的同时实现高压缩比。

Conclusion: NSC - SL性能卓越。

Abstract: The expanding scale of neural networks poses a major challenge for distributed machine learning, particularly under limited communication resources. While split learning (SL) alleviates client computational burden by distributing model layers between clients and server, it incurs substantial communication overhead from frequent transmission of intermediate activations and gradients. To tackle this issue, we propose NSC-SL, a bandwidth-aware adaptive compression algorithm for communication-efficient SL. NSC-SL first dynamically determines the optimal rank of low-rank approximation based on the singular value distribution for adapting real-time bandwidth constraints. Then, NSC-SL performs error-compensated tensor factorization using alternating orthogonal iteration with residual feedback, effectively minimizing truncation loss. The collaborative mechanisms enable NSC-SL to achieve high compression ratios while preserving semantic-rich information essential for convergence. Extensive experiments demonstrate the superb performance of NSC-SL.

</details>


<div id='cs.SC'></div>

# cs.SC [[Back]](#toc)

### [497] [Learning Fast Monomial Orders for Gröbner Basis Computations](https://arxiv.org/abs/2602.02972)
*R. Caleb Bunch,Alperen A. Ergür,Melika Golestani,Jessie Tong,Malia Walewski,Yunus E. Zeytuncu*

Main category: cs.SC

TL;DR: 将多项式方程组求解中单项式序选择问题转化为强化学习问题，实验表明学习策略优于标准启发式，且难以提炼为简单可解释模型。


<details>
  <summary>Details</summary>
Motivation: 现有Gröbner基计算中单项式序选择多依赖静态启发式，缺乏更优方法。

Method: 将单项式序选择作为强化学习问题，利用反映计算成本的奖励信号和蒙特卡罗估计。

Result: 学习策略在系统生物学和计算机视觉基准问题上始终优于标准启发式，大幅降低计算成本。

Conclusion: 深度强化学习可让智能体利用传统启发式无法触及的非线性几何结构。

Abstract: The efficiency of Gröbner basis computation, the standard engine for solving systems of polynomial equations, depends on the choice of monomial ordering. Despite a near-continuum of possible monomial orders, most implementations rely on static heuristics such as GrevLex, guided primarily by expert intuition. We address this gap by casting the selection of monomial orderings as a reinforcement learning problem over the space of admissible orderings. Our approach leverages domain-informed reward signals that accurately reflect the computational cost of Gröbner basis computations and admits efficient Monte Carlo estimation. Experiments on benchmark problems from systems biology and computer vision show that the resulting learned policies consistently outperform standard heuristics, yielding substantial reductions in computational cost. Moreover, we find that these policies resist distillation into simple interpretable models, providing empirical evidence that deep reinforcement learning allows the agents to exploit non-linear geometric structure beyond the scope of traditional heuristics.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [498] [Quantum Speedups for Derivative Pricing Beyond Black-Scholes](https://arxiv.org/abs/2602.03725)
*Dylan Herman,Yue Sun,Jin-Peng Liu,Marco Pistoia,Charlie Che,Rob Otter,Shouvanik Chakrabarti,Aram Harrow*

Main category: quant-ph

TL;DR: 本文探索量子算法在奇异衍生品定价中的进展，为实用模型展示新的二次加速，引入量子Milstein采样器，改进数值积分分析，研究无算术量子程序，还对量子PDE求解器进行批判。


<details>
  <summary>Details</summary>
Motivation: 经典蒙特卡罗积分是奇异衍生品定价的先进方法，量子算法虽有潜在加速，但端到端加速仅在简化模型中被证明，需拓展到更实用模型。

Method: 利用底层随机微分方程的快进性拓展现有框架，引入基于新型量子算法的量子Milstein采样器，改进数值积分分析，研究无算术量子程序，批判量子PDE求解器。

Result: 为实用模型展示新的二次加速，减少GBM和CIR模型定价的资源需求，指出量子PDE求解器的理论障碍。

Conclusion: 研究显著推进了量子算法在衍生品定价中的理解，解决了该领域的关键挑战和开放性问题。

Abstract: This paper explores advancements in quantum algorithms for derivative pricing of exotics, a computational pipeline of fundamental importance in quantitative finance. For such cases, the classical Monte Carlo integration procedure provides the state-of-the-art provable, asymptotic performance: polynomial in problem dimension and quadratic in inverse-precision. While quantum algorithms are known to offer quadratic speedups over classical Monte Carlo methods, end-to-end speedups have been proven only in the simplified setting over the Black-Scholes geometric Brownian motion (GBM) model. This paper extends existing frameworks to demonstrate novel quadratic speedups for more practical models, such as the Cox-Ingersoll-Ross (CIR) model and a variant of Heston's stochastic volatility model, utilizing a characteristic of the underlying SDEs which we term fast-forwardability. Additionally, for general models that do not possess the fast-forwardable property, we introduce a quantum Milstein sampler, based on a novel quantum algorithm for sampling Lévy areas, which enables quantum multi-level Monte Carlo to achieve quadratic speedups for multi-dimensional stochastic processes exhibiting certain correlation types.
  We also present an improved analysis of numerical integration for derivative pricing, leading to substantial reductions in the resource requirements for pricing GBM and CIR models. Furthermore, we investigate the potential for additional reductions using arithmetic-free quantum procedures. Finally, we critique quantum partial differential equation (PDE) solvers as a method for derivative pricing based on amplitude estimation, identifying theoretical barriers that obstruct achieving a quantum speedup through this approach. Our findings significantly advance the understanding of quantum algorithms in derivative pricing, addressing key challenges and open questions in the field.

</details>


### [499] [Quantum Circuit Generation via test-time learning with large language models](https://arxiv.org/abs/2602.03466)
*Adriano Macarone-Palmieri*

Main category: quant-ph

TL;DR: 本文将量子电路合成作为闭环测试时间优化问题，提出轻量级测试时间学习方法，展示了基于记忆评估器引导大语言模型优化电路合成的潜力与问题。


<details>
  <summary>Details</summary>
Motivation: 使大语言模型成为科学设计中可靠的优化器，实现量子电路合成的迭代改进。

Method: 将量子电路合成看作闭环问题，LLM提出门列表编辑，外部模拟器用MW全局纠缠度量评估；引入轻量级测试时间学习方法，重用高性能候选、增加分数差反馈、应用从最佳点重启采样。

Result: 无反馈和无重启的循环能改进随机初始电路；全学习策略提升性能和成功率；分析合成态结构，高MW解对应特定结构但不保证全连接。

Conclusion: 展示了记忆评估器引导LLM优化电路合成的前景与陷阱，强调人类理论定理对设计定制工具的关键作用。

Abstract: Large language models (LLMs) can generate structured artifacts, but using them as dependable optimizers for scientific design requires a mechanism for iterative improvement under black-box evaluation. Here, we cast quantum circuit synthesis as a closed-loop, test-time optimization problem: an LLM proposes edits to a fixed-length gate list, and an external simulator evaluates the resulting state with the Meyer-Wallach (MW) global entanglement measure. We introduce a lightweight test-time learning recipe that can reuse prior high-performing candidates as an explicit memory trace, augments prompts with a score-difference feedback, and applies restart-from-the-best sampling to escape potential plateaus. Across fixed 20-qubit settings, the loop without feedback and restart-from-the-best improves random initial circuits over a range of gate budgets. To lift up this performance and success rate, we use the full learning strategy. For 25-qubit, it mitigates a pronounced performance plateau when naive querying is used. Beyond raw scores, we analyze the structure of synthesized states and find that high MW solutions can correspond to stabilizer or graph-state-like constructions, but full connectivity is not guaranteed due to the metric property and prompt design. These results illustrate both the promise and the pitfalls of memory evaluator-guided LLM optimization for circuit synthesis, highlighting the critical role of prior human-made theoretical theorem to optimally design a custom tool in support of research.

</details>


### [500] [Enhancing Quantum Diffusion Models for Complex Image Generation](https://arxiv.org/abs/2602.03405)
*Jeongbin Jo,Santanam Wishal,Shah Md Khalil Ullah,Shan Kowalski,Dikshant Dulai*

Main category: quant-ph

TL;DR: 本文探索混合量子 - 经典U - Net架构结合自适应非局部可观测量解决量子生成模型在多模态分布应用中的可扩展性和表达性问题，实验表明该架构有一定生成能力。


<details>
  <summary>Details</summary>
Motivation: 量子生成模型在应用于多模态分布时面临可扩展性和表达性挑战，需寻找解决方案。

Method: 采用混合量子 - 经典U - Net架构结合自适应非局部可观测量，压缩经典数据到量子潜空间，利用可训练可观测量提取非局部特征，研究跳跃连接在逆扩散过程中保留语义信息的作用。

Result: 在完整MNIST数据集上实验，该架构能为所有数字类别生成结构连贯、可识别的图像。

Conclusion: 具有自适应测量的混合架构为缓解模式崩溃和增强生成能力提供了可行途径，尽管硬件仍限制分辨率。

Abstract: Quantum generative models offer a novel approach to exploring high-dimensional Hilbert spaces but face significant challenges in scalability and expressibility when applied to multi-modal distributions. In this study, we explore a Hybrid Quantum-Classical U-Net architecture integrated with Adaptive Non-Local Observables (ANO) as a potential solution to these hurdles. By compressing classical data into a dense quantum latent space and utilizing trainable observables, our model aims to extract non-local features that complement classical processing. We also investigate the role of Skip Connections in preserving semantic information during the reverse diffusion process. Experimental results on the full MNIST dataset (digits 0-9) demonstrate that the proposed architecture is capable of generating structurally coherent and recognizable images for all digit classes. While hardware constraints still impose limitations on resolution, our findings suggest that hybrid architectures with adaptive measurements provide a feasible pathway for mitigating mode collapse and enhancing generative capabilities in the NISQ era.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [501] [Trailer Reimagined: An Innovative, Llm-DRiven, Expressive Automated Movie Summary framework (TRAILDREAMS)](https://arxiv.org/abs/2602.02630)
*Roberto Balestri,Pasquale Cascarano,Mirko Degli Esposti,Guglielmo Pescatore*

Main category: cs.MM

TL;DR: 本文介绍用大语言模型自动化生成电影预告片的框架TRAILDREAMS，评估中表现优于现有方法，但与人工制作仍有差距。


<details>
  <summary>Details</summary>
Motivation: 高效制作引人入胜且视觉上有吸引力的电影预告片。

Method: 使用大语言模型选择关键视觉序列和有影响力的对话，生成音乐和旁白等音频元素。

Result: 在观众评分的比较评估中，TRAILDREAMS优于当前最先进的预告片生成方法，但不如真人制作的预告片。

Conclusion: TRAILDREAMS有显著前景，是自动化创意过程的进步，但需进一步改进以缩小与传统预告片的质量差距。

Abstract: This paper introduces TRAILDREAMS, a framework that uses a large language model (LLM) to automate the production of movie trailers. The purpose of LLM is to select key visual sequences and impactful dialogues, and to help TRAILDREAMS to generate audio elements such as music and voiceovers. The goal is to produce engaging and visually appealing trailers efficiently. In comparative evaluations, TRAILDREAMS surpasses current state-of-the-art trailer generation methods in viewer ratings. However, it still falls short when compared to real, human-crafted trailers. While TRAILDREAMS demonstrates significant promise and marks an advancement in automated creative processes, further improvements are necessary to bridge the quality gap with traditional trailers.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [502] [CryoLVM: Self-supervised Learning from Cryo-EM Density Maps with Large Vision Models](https://arxiv.org/abs/2602.02620)
*Weining Fu,Kai Shu,Kui Xu,Qiangfeng Cliff Zhang*

Main category: q-bio.QM

TL;DR: 提出基础模型CryoLVM用于冷冻电镜数据处理，在多个任务上效果超基线。


<details>
  <summary>Details</summary>
Motivation: 冷冻电镜数据量和复杂度增长，现有深度学习方法可扩展性和泛化性有限，需统一计算框架。

Method: 提出CryoLVM，利用JEPA和SCUNet骨干学习结构表示，引入直方图分布对齐损失。

Result: 在密度图锐化、超分辨率和缺失楔恢复三个关键任务中，CryoLVM在多个质量指标上优于现有基线。

Conclusion: CryoLVM有潜力成为多种冷冻电镜应用的通用模型。

Abstract: Cryo-electron microscopy (cryo-EM) has revolutionized structural biology by enabling near-atomic-level visualization of biomolecular assemblies. However, the exponential growth in cryo-EM data throughput and complexity, coupled with diverse downstream analytical tasks, necessitates unified computational frameworks that transcend current task-specific deep learning approaches with limited scalability and generalizability. We present CryoLVM, a foundation model that learns rich structural representations from experimental density maps with resolved structures by leveraging the Joint-Embedding Predictive Architecture (JEPA) integrated with SCUNet-based backbone, which can be rapidly adapted to various downstream tasks. We further introduce a novel histogram-based distribution alignment loss that accelerates convergence and enhances fine-tuning performance. We demonstrate CryoLVM's effectiveness across three critical cryo-EM tasks: density map sharpening, density map super-resolution, and missing wedge restoration. Our method consistently outperforms state-of-the-art baselines across multiple density map quality metrics, confirming its potential as a versatile model for a wide spectrum of cryo-EM applications.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [503] [FinMTM: A Multi-Turn Multimodal Benchmark for Financial Reasoning and Agent Evaluation](https://arxiv.org/abs/2602.03130)
*Chenxi Zhang,Ziliang Gan,Liyun Zhu,Youwei Pang,Qing Zhang,Rongjunchen Zhang*

Main category: cs.CV

TL;DR: 提出多轮多模态金融基准FinMTM，评估22个VLM模型，揭示其在多方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有金融基准多为单轮且问题格式单一，无法全面评估视觉语言模型在现实金融场景中的表现。

Method: 提出FinMTM，在数据上整理注释11133个双语金融问答对，任务涵盖多种类型，并设计特定评估协议。

Result: 对22个视觉语言模型的广泛实验评估，揭示了它们在细粒度视觉感知、长上下文推理和复杂代理工作流程方面的局限性。

Conclusion: FinMTM可更好地评估视觉语言模型在金融领域的能力，现有模型在多方面存在不足。

Abstract: The financial domain poses substantial challenges for vision-language models (VLMs) due to specialized chart formats and knowledge-intensive reasoning requirements. However, existing financial benchmarks are largely single-turn and rely on a narrow set of question formats, limiting comprehensive evaluation in realistic application scenarios. To address this gap, we propose FinMTM, a multi-turn multimodal benchmark that expands diversity along both data and task dimensions. On the data side, we curate and annotate 11{,}133 bilingual (Chinese and English) financial QA pairs grounded in financial visuals, including candlestick charts, statistical plots, and report figures. On the task side, FinMTM covers single- and multiple-choice questions, multi-turn open-ended dialogues, and agent-based tasks. We further design task-specific evaluation protocols, including a set-overlap scoring rule for multiple-choice questions, a weighted combination of turn-level and session-level scores for multi-turn dialogues, and a composite metric that integrates planning quality with final outcomes for agent tasks. Extensive experimental evaluation of 22 VLMs reveal their limitations in fine-grained visual perception, long-context reasoning, and complex agent workflows.

</details>


### [504] [FOVI: A biologically-inspired foveated interface for deep vision models](https://arxiv.org/abs/2602.03766)
*Nicholas M. Blauch,George A. Alvarez,Talia Konkle*

Main category: cs.CV

TL;DR: 提出基于人类视网膜和初级视觉皮层的FOVI，通过新方法实现kNN卷积，展示两个用例，模型计算成本低且性能有竞争力。


<details>
  <summary>Details</summary>
Motivation: 人类视觉是中央凹视觉，而多数计算机视觉系统以均匀分辨率编码视觉世界，处理全视野高分辨率图像有挑战。

Method: 提出FOVI将可变分辨率的类视网膜传感器阵列重新格式化为均匀密集的类V1传感器流形，通过新的核映射技术实现kNN卷积。

Result: 展示两个用例，模型在计算成本远低于非中央凹基线模型时仍有有竞争力的性能。

Conclusion: 为高分辨率以自我为中心的视觉的高效可扩展主动感知开辟了途径。

Abstract: Human vision is foveated, with variable resolution peaking at the center of a large field of view; this reflects an efficient trade-off for active sensing, allowing eye-movements to bring different parts of the world into focus with other parts of the world in context. In contrast, most computer vision systems encode the visual world at a uniform resolution, raising challenges for processing full-field high-resolution images efficiently. We propose a foveated vision interface (FOVI) based on the human retina and primary visual cortex, that reformats a variable-resolution retina-like sensor array into a uniformly dense, V1-like sensor manifold. Receptive fields are defined as k-nearest-neighborhoods (kNNs) on the sensor manifold, enabling kNN-convolution via a novel kernel mapping technique. We demonstrate two use cases: (1) an end-to-end kNN-convolutional architecture, and (2) a foveated adaptation of the foundational DINOv3 ViT model, leveraging low-rank adaptation (LoRA). These models provide competitive performance at a fraction of the computational cost of non-foveated baselines, opening pathways for efficient and scalable active sensing for high-resolution egocentric vision. Code and pre-trained models are available at https://github.com/nblauch/fovi and https://huggingface.co/fovi-pytorch.

</details>


### [505] [LmPT: Conditional Point Transformer for Anatomical Landmark Detection on 3D Point Clouds](https://arxiv.org/abs/2602.02808)
*Matteo Bastico,Pierre Onghena,David Ryckelynck,Beatriz Marcotegui,Santiago Velasco-Forero,Laurent Corté,Caroline Robine--Decourcelle,Etienne Decencière*

Main category: cs.CV

TL;DR: 提出 Landmark Point Transformer (LmPT) 方法用于点云自动解剖标志检测，可跨物种学习，在股骨标志检测上验证了跨物种泛化性和有效性。


<details>
  <summary>Details</summary>
Motivation: 传统手动标志标注耗时且有观察者差异，基于规则的方法有局限性，需新方法克服现有标志技术局限。

Method: 提出 LmPT 方法，该模型包含调节机制以适应不同输入类型进行跨物种学习。

Result: 在人类和新标注的狗股骨标志检测上进行评估，证明方法具有跨物种泛化和有效性。

Conclusion: LmPT 方法可用于点云自动解剖标志检测，能利用不同物种同源骨骼进行转化研究。代码和狗股骨数据集将公开。

Abstract: Accurate identification of anatomical landmarks is crucial for various medical applications. Traditional manual landmarking is time-consuming and prone to inter-observer variability, while rule-based methods are often tailored to specific geometries or limited sets of landmarks. In recent years, anatomical surfaces have been effectively represented as point clouds, which are lightweight structures composed of spatial coordinates. Following this strategy and to overcome the limitations of existing landmarking techniques, we propose Landmark Point Transformer (LmPT), a method for automatic anatomical landmark detection on point clouds that can leverage homologous bones from different species for translational research. The LmPT model incorporates a conditioning mechanism that enables adaptability to different input types to conduct cross-species learning. We focus the evaluation of our approach on femoral landmarking using both human and newly annotated dog femurs, demonstrating its generalization and effectiveness across species. The code and dog femur dataset will be publicly available at: https://github.com/Pierreoo/LandmarkPointTransformer.

</details>


### [506] [A Multi-scale Linear-time Encoder for Whole-Slide Image Analysis](https://arxiv.org/abs/2602.02918)
*Jagan Mohan Reddy Dwarampudi,Joshua Wong,Hien Van Nguyen,Tania Banerjee*

Main category: cs.CV

TL;DR: 提出基于Mamba的多尺度自适应循环生物医学线性时间编码器MARBLE用于全切片图像分析，实验显示其在多个指标上有提升。


<details>
  <summary>Details</summary>
Motivation: 全切片图像分析因高分辨率和分层放大具有挑战，现有多实例学习方法多为单尺度，基于Transformer的方法有二次注意力成本问题。

Method: 提出MARBLE框架，并行处理多放大级别，在线性时间状态空间模型中集成粗到精推理，耦合并行多尺度处理和线性时间序列建模。

Result: 在五个公共数据集上，AUC提升达6.9%，准确率提升达20.3%，C指数提升达2.3%。

Conclusion: MARBLE是用于多尺度全切片图像分析的高效且可泛化的框架。

Abstract: We introduce Multi-scale Adaptive Recurrent Biomedical Linear-time Encoder (MARBLE), the first \textit{purely Mamba-based} multi-state multiple instance learning (MIL) framework for whole-slide image (WSI) analysis. MARBLE processes multiple magnification levels in parallel and integrates coarse-to-fine reasoning within a linear-time state-space model, efficiently capturing cross-scale dependencies with minimal parameter overhead. WSI analysis remains challenging due to gigapixel resolutions and hierarchical magnifications, while existing MIL methods typically operate at a single scale and transformer-based approaches suffer from quadratic attention costs. By coupling parallel multi-scale processing with linear-time sequence modeling, MARBLE provides a scalable and modular alternative to attention-based architectures. Experiments on five public datasets show improvements of up to \textbf{6.9\%} in AUC, \textbf{20.3\%} in accuracy, and \textbf{2.3\%} in C-index, establishing MARBLE as an efficient and generalizable framework for multi-scale WSI analysis.

</details>


### [507] [Nüwa: Mending the Spatial Integrity Torn by VLM Token Pruning](https://arxiv.org/abs/2602.02951)
*Yihong Huang,Fei Ma,Yihua Shao,Jingcai Guo,Zitong Yu,Laizhong Cui,Qi Tian*

Main category: cs.CV

TL;DR: 提出Nüwa两阶段标记剪枝框架提升高效视觉语言模型性能，在多任务取得好效果。


<details>
  <summary>Details</summary>
Motivation: 现有视觉标记剪枝方法在视觉问答表现好，但在视觉定位任务性能下降。分析发现基于全局语义相似性和注意力分数的策略会丢失全局空间参考框架。

Method: 提出两阶段Nüwa框架，第一阶段在视觉编码器后用受群体智能算法启发的操作保留信息丰富的全局空间锚点；第二阶段在大语言模型内，执行文本引导的剪枝以保留任务相关视觉标记。

Result: Nüwa在多个视觉问答基准测试中达到SOTA性能（从94%到95%），在视觉定位任务上有显著提升（从7%到47%）。

Conclusion: Nüwa能在高效特征聚合的同时保持空间完整性，适用于多种视觉语言任务。

Abstract: Vision token pruning has proven to be an effective acceleration technique for the efficient Vision Language Model (VLM). However, existing pruning methods demonstrate excellent performance preservation in visual question answering (VQA) and suffer substantial degradation on visual grounding (VG) tasks. Our analysis of the VLM's processing pipeline reveals that strategies utilizing global semantic similarity and attention scores lose the global spatial reference frame, which is derived from the interactions of tokens' positional information. Motivated by these findings, we propose $\text{Nüwa}$, a two-stage token pruning framework that enables efficient feature aggregation while maintaining spatial integrity. In the first stage, after the vision encoder, we apply three operations, namely separation, alignment, and aggregation, which are inspired by swarm intelligence algorithms to retain information-rich global spatial anchors. In the second stage, within the LLM, we perform text-guided pruning to retain task-relevant visual tokens. Extensive experiments demonstrate that $\text{Nüwa}$ achieves SOTA performance on multiple VQA benchmarks (from 94% to 95%) and yields substantial improvements on visual grounding tasks (from 7% to 47%).

</details>


### [508] [Aligning Forest and Trees in Images and Long Captions for Visually Grounded Understanding](https://arxiv.org/abs/2602.02977)
*Byeongju Woo,Zilin Wang,Byeonghyun Pak,Sangwoo Mo,Stella X. Yu*

Main category: cs.CV

TL;DR: 提出CAFT框架用于图像与长文本表示学习，在长文本检索基准测试中达SOTA。


<details>
  <summary>Details</summary>
Motivation: 大视觉语言模型处理长文本有困难，细粒度视觉语言理解需跨视觉和文本领域的分层语义，但语言和视觉层级结构不匹配。

Method: 提出CAFT框架，耦合从细到粗的视觉编码器和分层文本变换器，使用分层对齐损失函数。

Result: 在3000万图像 - 文本对上训练，在六个长文本检索基准测试中达到SOTA，展现出强扩展性。

Conclusion: 分层跨域对齐可在无显式区域级监督下形成细粒度、基于视觉的图像 - 文本表示。

Abstract: Large vision-language models such as CLIP struggle with long captions because they align images and texts as undifferentiated wholes. Fine-grained vision-language understanding requires hierarchical semantics capturing both global context and localized details across visual and textual domains. Yet linguistic hierarchies from syntax or semantics rarely match visual organization, and purely visual hierarchies tend to fragment scenes into appearance-driven parts without semantic focus. We propose CAFT (Cross-domain Alignment of Forests and Trees), a hierarchical image-text representation learning framework that aligns global and local semantics across images and long captions without pixel-level supervision. Coupling a fine-to-coarse visual encoder with a hierarchical text transformer, it uses a hierarchical alignment loss that matches whole images with whole captions while biasing region-sentence correspondences, so that coarse semantics are built from fine-grained evidence rather than from aggregation untethered to part-level grounding. Trained on 30M image-text pairs, CAFT achieves state-of-the-art performance on six long-text retrieval benchmarks and exhibits strong scaling behavior. Experiments show that hierarchical cross-domain alignment enables fine-grained, visually grounded image-text representations to emerge without explicit region-level supervision.

</details>


### [509] [VOILA: Value-of-Information Guided Fidelity Selection for Cost-Aware Multimodal Question Answering](https://arxiv.org/abs/2602.03007)
*Rahul Atul Bhope,K. R. Jayaram,Vinod Muthusamy,Ritesh Kumar,Vatche Isahagian,Nalini Venkatasubramanian*

Main category: cs.CV

TL;DR: 提出VOILA框架用于视觉问答中自适应保真度选择，评估显示能显著降低成本并保持较高准确率


<details>
  <summary>Details</summary>
Motivation: 多数多模态视觉语言系统以固定保真度运行，存在检索和处理高保真视觉输入成本高的问题

Method: 使用两阶段管道，先由梯度提升回归器从问题特征估计各保真度的正确性可能性，再由等渗校准器细化概率以进行决策，选择最小成本下最大化预期效用的保真度

Result: 在多种数据集和模型架构上评估，VOILA能持续降低50 - 60%的成本，同时保留90 - 95%的全分辨率准确率

Conclusion: 预检索保真度选择对资源约束下的多模态推理优化至关重要

Abstract: Despite significant costs from retrieving and processing high-fidelity visual inputs, most multimodal vision-language systems operate at fixed fidelity levels. We introduce VOILA, a framework for Value-Of-Information-driven adaptive fidelity selection in Visual Question Answering (VQA) that optimizes what information to retrieve before model execution. Given a query, VOILA uses a two-stage pipeline: a gradient-boosted regressor estimates correctness likelihood at each fidelity from question features alone, then an isotonic calibrator refines these probabilities for reliable decision-making. The system selects the minimum-cost fidelity maximizing expected utility given predicted accuracy and retrieval costs. We evaluate VOILA across three deployment scenarios using five datasets (VQA-v2, GQA, TextVQA, LoCoMo, FloodNet) and six Vision-Language Models (VLMs) with 7B-235B parameters. VOILA consistently achieves 50-60% cost reductions while retaining 90-95% of full-resolution accuracy across diverse query types and model architectures, demonstrating that pre-retrieval fidelity selection is vital to optimize multimodal inference under resource constraints.

</details>


### [510] [Bongards at the Boundary of Perception and Reasoning: Programs or Language?](https://arxiv.org/abs/2602.03038)
*Cassidy Langenfeld,Claas Beger,Gloria Geng,Wasu Top Piriyakulkij,Keya Hu,Yewen Pu,Kevin Ellis*

Main category: cs.CV

TL;DR: 本文提出神经符号方法解决邦加德问题，并进行相关评估。


<details>
  <summary>Details</summary>
Motivation: 现有视觉 - 语言模型缺乏在全新场景下的视觉推理能力，而邦加德问题可严格测试该能力，故需找到解决方法。

Method: 提出神经符号方法，利用大语言模型生成规则的参数化程序表示，用贝叶斯优化进行参数拟合。

Result: 对给定真实规则的邦加德问题图像进行分类评估，以及从零开始解决问题。

Conclusion: 

Abstract: Vision-Language Models (VLMs) have made great strides in everyday visual tasks, such as captioning a natural image, or answering commonsense questions about such images. But humans possess the puzzling ability to deploy their visual reasoning abilities in radically new situations, a skill rigorously tested by the classic set of visual reasoning challenges known as the Bongard problems. We present a neurosymbolic approach to solving these problems: given a hypothesized solution rule for a Bongard problem, we leverage LLMs to generate parameterized programmatic representations for the rule and perform parameter fitting using Bayesian optimization. We evaluate our method on classifying Bongard problem images given the ground truth rule, as well as on solving the problems from scratch.

</details>


### [511] [JRDB-Pose3D: A Multi-person 3D Human Pose and Shape Estimation Dataset for Robotics](https://arxiv.org/abs/2602.03064)
*Sandika Biswas,Kian Izadpanah,Hamid Rezatofighi*

Main category: cs.CV

TL;DR: 引入JRDB - Pose3D数据集以解决现有3D人体姿态估计数据集与真实场景相关性不足的问题，该数据集适用于多种下游任务。


<details>
  <summary>Details</summary>
Motivation: 现有3D人体姿态估计数据集多聚焦单人场景或在实验室环境收集，与真实应用相关性受限，需新数据集。

Method: 从移动机器人平台捕获多人室内外环境数据，提供丰富3D人体姿态标注。

Result: 得到JRDB - Pose3D数据集，平均每帧含5 - 10个人体姿态，部分场景达35人，有多种挑战，继承JRDB数据集多种标注。

Conclusion: JRDB - Pose3D是适用于多种下游感知和以人类为中心理解任务的整体数据集。

Abstract: Real-world scenes are inherently crowded. Hence, estimating 3D poses of all nearby humans, tracking their movements over time, and understanding their activities within social and environmental contexts are essential for many applications, such as autonomous driving, robot perception, robot navigation, and human-robot interaction. However, most existing 3D human pose estimation datasets primarily focus on single-person scenes or are collected in controlled laboratory environments, which restricts their relevance to real-world applications. To bridge this gap, we introduce JRDB-Pose3D, which captures multi-human indoor and outdoor environments from a mobile robotic platform. JRDB-Pose3D provides rich 3D human pose annotations for such complex and dynamic scenes, including SMPL-based pose annotations with consistent body-shape parameters and track IDs for each individual over time. JRDB-Pose3D contains, on average, 5-10 human poses per frame, with some scenes featuring up to 35 individuals simultaneously. The proposed dataset presents unique challenges, including frequent occlusions, truncated bodies, and out-of-frame body parts, which closely reflect real-world environments. Moreover, JRDB-Pose3D inherits all available annotations from the JRDB dataset, such as 2D pose, information about social grouping, activities, and interactions, full-scene semantic masks with consistent human- and object-level tracking, and detailed annotations for each individual, such as age, gender, and race, making it a holistic dataset for a wide range of downstream perception and human-centric understanding tasks.

</details>


### [512] [Beyond Cropping and Rotation: Automated Evolution of Powerful Task-Specific Augmentations with Generative Models](https://arxiv.org/abs/2602.03123)
*Judah Goldfeder,Shreyes Kaliyur,Vaibhav Sourirajan,Patrick Minwan Puma,Philippe Martin Wyder,Yuhang Hu,Jiong Lin,Hod Lipson*

Main category: cs.CV

TL;DR: 提出自动化增强学习管道EvoAug，结合生成模型和进化算法学习最优特定任务增强，在多任务表现好。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型用于数据增强虽能增加数据多样性和真实性但可能因与任务不匹配而降低性能，需学习最优特定任务增强。

Method: 提出EvoAug自动化增强学习管道，利用生成模型和高效进化算法，学习随机增强树来分层组合增强。

Result: 在细粒度分类和少样本学习任务中表现出色，能发现与领域知识相符的增强，在低数据设置下也有效。

Conclusion: 展示了学习生成式增强的潜力，为鲁棒模型训练带来新可能。

Abstract: Data augmentation has long been a cornerstone for reducing overfitting in vision models, with methods like AutoAugment automating the design of task-specific augmentations. Recent advances in generative models, such as conditional diffusion and few-shot NeRFs, offer a new paradigm for data augmentation by synthesizing data with significantly greater diversity and realism. However, unlike traditional augmentations like cropping or rotation, these methods introduce substantial changes that enhance robustness but also risk degrading performance if the augmentations are poorly matched to the task. In this work, we present EvoAug, an automated augmentation learning pipeline, which leverages these generative models alongside an efficient evolutionary algorithm to learn optimal task-specific augmentations. Our pipeline introduces a novel approach to image augmentation that learns stochastic augmentation trees that hierarchically compose augmentations, enabling more structured and adaptive transformations. We demonstrate strong performance across fine-grained classification and few-shot learning tasks. Notably, our pipeline discovers augmentations that align with domain knowledge, even in low-data settings. These results highlight the potential of learned generative augmentations, unlocking new possibilities for robust model training.

</details>


### [513] [SwiftVLM: Efficient Vision-Language Model Inference via Cross-Layer Token Bypass](https://arxiv.org/abs/2602.03134)
*Chen Qian,Xinran Yu,Danyang Li,Guoxuan Chi,Zheng Yang,Qiang Ma,Xin Miao*

Main category: cs.CV

TL;DR: 现有视觉语言模型视觉令牌修剪方法在细粒度任务表现不佳，本文提出bypass范式和无训练的SwiftVLM方法，实验显示其优于现有策略。


<details>
  <summary>Details</summary>
Motivation: 现有视觉令牌修剪方法在需要细粒度视觉细节的任务上性能显著下降，且过早修剪会造成关键信息不可逆丢失。

Method: 引入bypass范式保留未选视觉令牌并重新评估，提出无训练的SwiftVLM方法，在特定层进行修剪并实现各层独立决策。

Result: 在多个视觉语言模型和基准测试中，SwiftVLM始终优于现有修剪策略。

Conclusion: SwiftVLM能实现更优的准确率 - 效率权衡和更可靠的视觉令牌选择行为。

Abstract: Visual token pruning is a promising approach for reducing the computational cost of vision-language models (VLMs), and existing methods often rely on early pruning decisions to improve efficiency. While effective on coarse-grained reasoning tasks, they suffer from significant performance degradation on tasks requiring fine-grained visual details. Through layer-wise analysis, we reveal substantial discrepancies in visual token importance across layers, showing that tokens deemed unimportant at shallow layers can later become highly relevant for text-conditioned reasoning. To avoid irreversible critical information loss caused by premature pruning, we introduce a new pruning paradigm, termed bypass, which preserves unselected visual tokens and forwards them to subsequent pruning stages for re-evaluation. Building on this paradigm, we propose SwiftVLM, a simple and training-free method that performs pruning at model-specific layers with strong visual token selection capability, while enabling independent pruning decisions across layers. Experiments across multiple VLMs and benchmarks demonstrate that SwiftVLM consistently outperforms existing pruning strategies, achieving superior accuracy-efficiency trade-offs and more faithful visual token selection behavior.

</details>


### [514] [Hand3R: Online 4D Hand-Scene Reconstruction in the Wild](https://arxiv.org/abs/2602.03200)
*Wendi Hu,Haonan Zhou,Wenhao Hu,Gaoang Wang*

Main category: cs.CV

TL;DR: 提出首个在线框架Hand3R用于单目视频的4D手-场景联合重建，性能有竞争力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在具身AI中恢复局部坐标下孤立的手，忽略周围3D环境，需联合重建动态手和密集场景上下文以理解物理交互。

Method: 通过场景感知视觉提示机制将预训练手专家与4D场景基础模型协同，将高保真手先验信息注入持久场景记忆。

Result: 可在单次前向传播中同时重建准确的手网格和密集度量尺度场景几何，实验表明Hand3R无需离线优化。

Conclusion: Hand3R在局部手重建和全局定位方面有有竞争力的表现。

Abstract: For Embodied AI, jointly reconstructing dynamic hands and the dense scene context is crucial for understanding physical interaction. However, most existing methods recover isolated hands in local coordinates, overlooking the surrounding 3D environment. To address this, we present Hand3R, the first online framework for joint 4D hand-scene reconstruction from monocular video. Hand3R synergizes a pre-trained hand expert with a 4D scene foundation model via a scene-aware visual prompting mechanism. By injecting high-fidelity hand priors into a persistent scene memory, our approach enables simultaneous reconstruction of accurate hand meshes and dense metric-scale scene geometry in a single forward pass. Experiments demonstrate that Hand3R bypasses the reliance on offline optimization and delivers competitive performance in both local hand reconstruction and global positioning.

</details>


### [515] [Global Geometry Is Not Enough for Vision Representations](https://arxiv.org/abs/2602.03282)
*Jiwan Chung,Seon Joo Kim*

Main category: cs.CV

TL;DR: 研究发现全局嵌入几何只能反映表征能力的一部分，功能敏感性是建模复合结构的关键补充维度。


<details>
  <summary>Details</summary>
Motivation: 常见表征学习假设全局分布良好的嵌入支持强大通用表征，但全局几何常对元素组合不敏感，需研究其局限性。

Method: 对21个视觉编码器测试几何指标预测组合绑定的能力，分析输入 - 输出雅可比矩阵衡量功能敏感性。

Result: 基于标准几何的统计数据与组合绑定的相关性接近零，而功能敏感性能可靠跟踪此能力；分析表明这种差异源于目标设计。

Conclusion: 全局嵌入几何仅展现部分表征能力，功能敏感性是重要的补充维度。

Abstract: A common assumption in representation learning is that globally well-distributed embeddings support robust and generalizable representations. This focus has shaped both training objectives and evaluation protocols, implicitly treating global geometry as a proxy for representational competence. While global geometry effectively encodes which elements are present, it is often insensitive to how they are composed. We investigate this limitation by testing the ability of geometric metrics to predict compositional binding across 21 vision encoders. We find that standard geometry-based statistics exhibit near-zero correlation with compositional binding. In contrast, functional sensitivity, as measured by the input-output Jacobian, reliably tracks this capability. We further provide an analytic account showing that this disparity arises from objective design, as existing losses explicitly constrain embedding geometry but leave the local input-output mapping unconstrained. These results suggest that global embedding geometry captures only a partial view of representational competence and establish functional sensitivity as a critical complementary axis for modeling composite structure.

</details>


### [516] [Full end-to-end diagnostic workflow automation of 3D OCT via foundation model-driven AI for retinal diseases](https://arxiv.org/abs/2602.03302)
*Jinze Zhang,Jian Zhong,Li Lin,Jiaxiong Li,Ke Ma,Naiyang Li,Meng Li,Yuan Pan,Zeyu Meng,Mengyun Zhou,Shang Huang,Shilong Yu,Zhengyu Duan,Sutong Li,Honghui Xia,Juping Liu,Dan Liang,Yantao Wei,Xiaoying Tang,Jin Yuan,Peng Xiao*

Main category: cs.CV

TL;DR: 提出基于全流程OCT的临床应用系统FOCUS实现3D OCT视网膜疾病诊断的端到端自动化，经测试表现良好，推动无人眼科发展。


<details>
  <summary>Details</summary>
Motivation: OCT在临床实践中全诊断自动化受多阶段工作流程和传统单切片单任务AI模型限制。

Method: FOCUS先用EfficientNetV2 - S进行图像质量评估，再用微调的视觉基础模型进行异常检测和多疾病分类，利用统一自适应聚合方法将2D切片预测集成到3D患者级诊断。

Result: 在多个数据集测试中，FOCUS在质量评估、异常检测和患者级诊断等方面取得高F1分数，跨中心验证表现稳定，人机对比中匹配专家水平且更高效。

Conclusion: FOCUS实现图像到诊断的自动化，为无人眼科发展提供有效方案，可提高视网膜疾病筛查的可及性和效率。

Abstract: Optical coherence tomography (OCT) has revolutionized retinal disease diagnosis with its high-resolution and three-dimensional imaging nature, yet its full diagnostic automation in clinical practices remains constrained by multi-stage workflows and conventional single-slice single-task AI models. We present Full-process OCT-based Clinical Utility System (FOCUS), a foundation model-driven framework enabling end-to-end automation of 3D OCT retinal disease diagnosis. FOCUS sequentially performs image quality assessment with EfficientNetV2-S, followed by abnormality detection and multi-disease classification using a fine-tuned Vision Foundation Model. Crucially, FOCUS leverages a unified adaptive aggregation method to intelligently integrate 2D slices-level predictions into comprehensive 3D patient-level diagnosis. Trained and tested on 3,300 patients (40,672 slices), and externally validated on 1,345 patients (18,498 slices) across four different-tier centers and diverse OCT devices, FOCUS achieved high F1 scores for quality assessment (99.01%), abnormally detection (97.46%), and patient-level diagnosis (94.39%). Real-world validation across centers also showed stable performance (F1: 90.22%-95.24%). In human-machine comparisons, FOCUS matched expert performance in abnormality detection (F1: 95.47% vs 90.91%) and multi-disease diagnosis (F1: 93.49% vs 91.35%), while demonstrating better efficiency. FOCUS automates the image-to-diagnosis pipeline, representing a critical advance towards unmanned ophthalmology with a validated blueprint for autonomous screening to enhance population scale retinal care accessibility and efficiency.

</details>


### [517] [MedSAM-Agent: Empowering Interactive Medical Image Segmentation with Multi-turn Agentic Reinforcement Learning](https://arxiv.org/abs/2602.03320)
*Shengyuan Liu,Liuxin Bao,Qi Yang,Wanting Geng,Boyun Zheng,Chenxin Li,Wenting Chen,Houwen Peng,Yixuan Yuan*

Main category: cs.CV

TL;DR: 提出MedSAM - Agent框架，将交互式分割转化为多步自主决策过程，实现医学推理与优化结合，实验表现达SOTA水平。


<details>
  <summary>Details</summary>
Motivation: 现有基于MLLMs的医学图像分割方法存在单轮、刚性交互策略和缺乏训练过程监督的问题，阻碍工具潜力发挥。

Method: 提出MedSAM - Agent框架，引入混合提示策略生成专家轨迹，开发两阶段训练管道，结合多轮端到端结果验证与临床保真过程奖励设计。

Result: 在6种医学模态和21个数据集上的广泛实验表明，MedSAM - Agent达到了最先进的性能。

Conclusion: MedSAM - Agent能有效统一自主医学推理与强大的迭代优化。

Abstract: Medical image segmentation is evolving from task-specific models toward generalizable frameworks. Recent research leverages Multi-modal Large Language Models (MLLMs) as autonomous agents, employing reinforcement learning with verifiable reward (RLVR) to orchestrate specialized tools like the Segment Anything Model (SAM). However, these approaches often rely on single-turn, rigid interaction strategies and lack process-level supervision during training, which hinders their ability to fully exploit the dynamic potential of interactive tools and leads to redundant actions. To bridge this gap, we propose MedSAM-Agent, a framework that reformulates interactive segmentation as a multi-step autonomous decision-making process. First, we introduce a hybrid prompting strategy for expert-curated trajectory generation, enabling the model to internalize human-like decision heuristics and adaptive refinement strategies. Furthermore, we develop a two-stage training pipeline that integrates multi-turn, end-to-end outcome verification with a clinical-fidelity process reward design to promote interaction parsimony and decision efficiency. Extensive experiments across 6 medical modalities and 21 datasets demonstrate that MedSAM-Agent achieves state-of-the-art performance, effectively unifying autonomous medical reasoning with robust, iterative optimization. Code is available \href{https://github.com/CUHK-AIM-Group/MedSAM-Agent}{here}.

</details>


### [518] [Tiled Prompts: Overcoming Prompt Underspecification in Image and Video Super-Resolution](https://arxiv.org/abs/2602.03342)
*Bryan Sangwoo Kim,Jonghyun Park,Jong Chul Ye*

Main category: cs.CV

TL;DR: 提出Tiled Prompts框架解决文本条件扩散模型在图像和视频超分辨率中的提示不明确问题，实验有积极效果。


<details>
  <summary>Details</summary>
Motivation: 现代超分辨率管道使用单一全局提示会导致提示不明确，包括提示稀疏和提示误导问题。

Method: 提出Tiled Prompts统一框架，为每个潜在块生成特定提示，在局部文本条件后验下进行超分辨率。

Result: 在高分辨率真实世界图像和视频实验中，感知质量和文本对齐度有一致提升，减少幻觉和块级伪影。

Conclusion: Tiled Prompts框架能以最小开销解决提示不明确问题，提升超分辨率效果。

Abstract: Text-conditioned diffusion models have advanced image and video super-resolution by using prompts as semantic priors, but modern super-resolution pipelines typically rely on latent tiling to scale to high resolutions, where a single global caption causes prompt underspecification. A coarse global prompt often misses localized details (prompt sparsity) and provides locally irrelevant guidance (prompt misguidance) that can be amplified by classifier-free guidance. We propose Tiled Prompts, a unified framework for image and video super-resolution that generates a tile-specific prompt for each latent tile and performs super-resolution under locally text-conditioned posteriors, providing high-information guidance that resolves prompt underspecification with minimal overhead. Experiments on high resolution real-world images and videos show consistent gains in perceptual quality and text alignment, while reducing hallucinations and tile-level artifacts relative to global-prompt baselines.

</details>


### [519] [SLIM-Diff: Shared Latent Image-Mask Diffusion with Lp loss for Data-Scarce Epilepsy FLAIR MRI](https://arxiv.org/abs/2602.03372)
*Mario Pascual-González,Ariadna Jiménez-Partinen,R. M. Luque-Baena,Fátima Nagib-Raya,Ezequiel López-Rubio*

Main category: cs.CV

TL;DR: 提出SLIM - Diff紧凑联合扩散模型解决癫痫FLAIR MRI中局灶性皮质发育不良病变图像与掩码生成建模问题，实验表明x0预测和特定Lp损失效果好。


<details>
  <summary>Details</summary>
Motivation: 癫痫FLAIR MRI中局灶性皮质发育不良（FCD）病变细微且稀少，导致联合图像 - 掩码生成建模易不稳定和过拟合。

Method: 提出SLIM - Diff模型，采用单共享瓶颈U - Net加强解剖结构和病变几何的耦合，通过可调Lp目标进行损失 - 几何调整，并设置规范DDPM风格目标作为内部基线。

Result: 实验显示x0预测是联合合成的最佳选择，分数次二次惩罚（L1.5）提高图像保真度，L2更好保留病变掩码形态。

Conclusion: SLIM - Diff模型在解决相关生成建模问题上有较好效果，代码和模型权重公开。

Abstract: Focal cortical dysplasia (FCD) lesions in epilepsy FLAIR MRI are subtle and scarce, making joint image--mask generative modeling prone to instability and memorization. We propose SLIM-Diff, a compact joint diffusion model whose main contributions are (i) a single shared-bottleneck U-Net that enforces tight coupling between anatomy and lesion geometry from a 2-channel image+mask representation, and (ii) loss-geometry tuning via a tunable $L_p$ objective. As an internal baseline, we include the canonical DDPM-style objective ($ε$-prediction with $L_2$ loss) and isolate the effect of prediction parameterization and $L_p$ geometry under a matched setup. Experiments show that $x_0$-prediction is consistently the strongest choice for joint synthesis, and that fractional sub-quadratic penalties ($L_{1.5}$) improve image fidelity while $L_2$ better preserves lesion mask morphology. Our code and model weights are available in https://github.com/MarioPasc/slim-diff

</details>


### [520] [Socratic-Geo: Synthetic Data Generation and Geometric Reasoning via Multi-Agent Interaction](https://arxiv.org/abs/2602.03414)
*Zhengbo Jiao,Shaobo Wang,Zifan Zhang,Wei Wang,Bing Zhao,Hu Wei,Linfeng Zhang*

Main category: cs.CV

TL;DR: 现有多模态大语言模型在几何推理上有瓶颈，因高质量图像文本对稀缺，本文提出Socratic - Geo框架，通过多智能体交互耦合数据合成与模型学习，取得良好效果。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型在几何推理上存在瓶颈，高质量图像文本对稀缺，人工标注昂贵，自动方法难保证质量，现有数据生成方法不理想。

Method: 提出Socratic - Geo框架，利用Teacher智能体生成参数化Python脚本确保图像文本对纯度，Solver智能体通过偏好学习优化推理，Generator学习图像生成能力。

Result: Socratic - Solver用四分之一基线数据在六个基准上得49.11分，超基线2.43分；Socratic - Generator在GenExam上达42.4%，创开源模型新纪录。

Conclusion: Socratic - Geo框架在解决多模态大语言模型几何推理数据稀缺问题上有效，能提升模型性能。

Abstract: Multimodal Large Language Models (MLLMs) have significantly advanced vision-language understanding. However, even state-of-the-art models struggle with geometric reasoning, revealing a critical bottleneck: the extreme scarcity of high-quality image-text pairs. Human annotation is prohibitively expensive, while automated methods fail to ensure fidelity and training effectiveness. Existing approaches either passively adapt to available images or employ inefficient random exploration with filtering, decoupling generation from learning needs. We propose Socratic-Geo, a fully autonomous framework that dynamically couples data synthesis with model learning through multi-agent interaction. The Teacher agent generates parameterized Python scripts with reflective feedback (Reflect for solvability, RePI for visual validity), ensuring image-text pair purity. The Solver agent optimizes reasoning through preference learning, with failure paths guiding Teacher's targeted augmentation. Independently, the Generator learns image generation capabilities on accumulated "image-code-instruction" triplets, distilling programmatic drawing intelligence into visual generation. Starting from only 108 seed problems, Socratic-Solver achieves 49.11 on six benchmarks using one-quarter of baseline data, surpassing strong baselines by 2.43 points. Socratic-Generator achieves 42.4% on GenExam, establishing new state-of-the-art for open-source models, surpassing Seedream-4.0 (39.8%) and approaching Gemini-2.5-Flash-Image (43.1%).

</details>


### [521] [Hierarchical Concept-to-Appearance Guidance for Multi-Subject Image Generation](https://arxiv.org/abs/2602.03448)
*Yijia Xu,Zihao Wang,Jinshi Cui*

Main category: cs.CV

TL;DR: 提出Hierarchical Concept - to - Appearance Guidance (CAG)框架提升多主体图像生成性能


<details>
  <summary>Details</summary>
Motivation: 现有多主体图像生成方法依赖扩散模型隐式关联文本提示和参考图像，存在身份不一致和组合控制有限问题

Method: 提出CAG框架，概念层面用VAE dropout训练策略，让模型依赖VLM语义信号；外观层面将VLM对应关系集成到Diffusion Transformer的对应感知掩码注意力模块

Result: 在多主体图像生成上达到了当前最优性能，显著提高了指令跟随和主体一致性

Conclusion: 所提方法有效提升了多主体图像生成效果

Abstract: Multi-subject image generation aims to synthesize images that faithfully preserve the identities of multiple reference subjects while following textual instructions. However, existing methods often suffer from identity inconsistency and limited compositional control, as they rely on diffusion models to implicitly associate text prompts with reference images. In this work, we propose Hierarchical Concept-to-Appearance Guidance (CAG), a framework that provides explicit, structured supervision from high-level concepts to fine-grained appearances. At the conceptual level, we introduce a VAE dropout training strategy that randomly omits reference VAE features, encouraging the model to rely more on robust semantic signals from a Visual Language Model (VLM) and thereby promoting consistent concept-level generation in the absence of complete appearance cues. At the appearance level, we integrate the VLM-derived correspondences into a correspondence-aware masked attention module within the Diffusion Transformer (DiT). This module restricts each text token to attend only to its matched reference regions, ensuring precise attribute binding and reliable multi-subject composition. Extensive experiments demonstrate that our method achieves state-of-the-art performance on the multi-subject image generation, substantially improving prompt following and subject consistency.

</details>


### [522] [WorldVQA: Measuring Atomic World Knowledge in Multimodal Large Language Models](https://arxiv.org/abs/2602.02537)
*Runjie Zhou,Youbo Shao,Haoyu Lu,Bowei Xing,Tongtong Bai,Yujie Chen,Jie Zhao,Lin Sui,Haotian Yao,Zijia Zhao,Hao Yang,Haoning Wu,Zaida Zhou,Jinguo Zhu,Zhiqi Huang,Yiping Bao,Yangyang Liu,Y. Charles,Xinyu Zhou*

Main category: cs.CV

TL;DR: 介绍用于评估多模态大语言模型原子视觉世界知识的基准WorldVQA，期望其为评估模型视觉事实性等提供标准。


<details>
  <summary>Details</summary>
Motivation: 当前评估常混淆视觉知识检索与推理，需要严格衡量模型所记忆内容，评估模型原子能力。

Method: 设计WorldVQA基准，在分层分类法下评估模型对视觉实体的定位和命名能力。

Result: 未提及明确结果。

Conclusion: 期望WorldVQA作为严格测试，为评估模型百科知识广度和幻觉率建立标准。

Abstract: We introduce WorldVQA, a benchmark designed to evaluate the atomic visual world knowledge of Multimodal Large Language Models (MLLMs). Unlike current evaluations, which often conflate visual knowledge retrieval with reasoning, WorldVQA decouples these capabilities to strictly measure "what the model memorizes." The benchmark assesses the atomic capability of grounding and naming visual entities across a stratified taxonomy, spanning from common head-class objects to long-tail rarities. We expect WorldVQA to serve as a rigorous test for visual factuality, thereby establishing a standard for assessing the encyclopedic breadth and hallucination rates of current and next-generation frontier models.

</details>


### [523] [ELIQ: A Label-Free Framework for Quality Assessment of Evolving AI-Generated Images](https://arxiv.org/abs/2602.03558)
*Xinyue Li,Zhiming Xu,Zhichao Zhang,Zhaolin Cai,Sijing Wu,Xiongkuo Min,Yitong Chen,Guangtao Zhai*

Main category: cs.CV

TL;DR: 提出无标签框架ELIQ用于评估不断演变的AI生成图像质量，实验显示其优于现有无标签方法，且能从AIGC泛化到UGC场景。


<details>
  <summary>Details</summary>
Motivation: 生成式文本到图像模型发展迅速，使之前收集的标签对新模型不可靠，需要新的图像质量评估方法。

Method: 聚焦视觉质量和提示 - 图像对齐，自动构建正负样本对，通过指令调优将预训练多模态模型转换为质量评估器，用轻量级门控融合和质量查询变压器预测二维质量。

Result: 在多个基准测试中，ELIQ始终优于现有无标签方法，可直接从AIGC场景泛化到UGC场景。

Conclusion: ELIQ为不断演变的生成式模型下可扩展的无标签质量评估铺平了道路。

Abstract: Generative text-to-image models are advancing at an unprecedented pace, continuously shifting the perceptual quality ceiling and rendering previously collected labels unreliable for newer generations. To address this, we present ELIQ, a Label-free Framework for Quality Assessment of Evolving AI-generated Images. Specifically, ELIQ focuses on visual quality and prompt-image alignment, automatically constructs positive and aspect-specific negative pairs to cover both conventional distortions and AIGC-specific distortion modes, enabling transferable supervision without human annotations. Building on these pairs, ELIQ adapts a pre-trained multimodal model into a quality-aware critic via instruction tuning and predicts two-dimensional quality using lightweight gated fusion and a Quality Query Transformer. Experiments across multiple benchmarks demonstrate that ELIQ consistently outperforms existing label-free methods, generalizes from AI-generated content (AIGC) to user-generated content (UGC) scenarios without modification, and paves the way for scalable and label-free quality assessment under continuously evolving generative models. The code will be released upon publication.

</details>


### [524] [A Lightweight Library for Energy-Based Joint-Embedding Predictive Architectures](https://arxiv.org/abs/2602.03604)
*Basile Terver,Randall Balestriero,Megi Dervishi,David Fan,Quentin Garrido,Tushar Nagarajan,Koustuv Sinha,Wancong Zhang,Mike Rabbat,Yann LeCun,Amir Bar*

Main category: cs.CV

TL;DR: 介绍开源库EB - JEPA，用于用JEPAs学习表征和世界模型，展示其在图像、视频和行动条件世界模型的应用及效果。


<details>
  <summary>Details</summary>
Motivation: 避免生成式建模的缺陷，捕捉适合下游任务的有语义意义特征，将图像表征学习技术拓展到视频和行动条件世界模型。

Method: 提供模块化、自包含实现，在CIFAR - 10、Moving MNIST等数据集上进行实验，进行消融实验。

Result: 在CIFAR - 10上探测表征准确率达91%，在Moving MNIST上展示多步预测，在Two Rooms导航任务规划成功率达97%。

Conclusion: 每个正则化组件对防止表征崩溃至关重要。

Abstract: We present EB-JEPA, an open-source library for learning representations and world models using Joint-Embedding Predictive Architectures (JEPAs). JEPAs learn to predict in representation space rather than pixel space, avoiding the pitfalls of generative modeling while capturing semantically meaningful features suitable for downstream tasks. Our library provides modular, self-contained implementations that illustrate how representation learning techniques developed for image-level self-supervised learning can transfer to video, where temporal dynamics add complexity, and ultimately to action-conditioned world models, where the model must additionally learn to predict the effects of control inputs. Each example is designed for single-GPU training within a few hours, making energy-based self-supervised learning accessible for research and education. We provide ablations of JEA components on CIFAR-10. Probing these representations yields 91% accuracy, indicating that the model learns useful features. Extending to video, we include a multi-step prediction example on Moving MNIST that demonstrates how the same principles scale to temporal modeling. Finally, we show how these representations can drive action-conditioned world models, achieving a 97% planning success rate on the Two Rooms navigation task. Comprehensive ablations reveal the critical importance of each regularization component for preventing representation collapse. Code is available at https://github.com/facebookresearch/eb_jepa.

</details>


### [525] [Efficient Sequential Neural Network with Spatial-Temporal Attention and Linear LSTM for Robust Lane Detection Using Multi-Frame Images](https://arxiv.org/abs/2602.03669)
*Sandeep Patil,Yongqi Dong,Haneen Farah,Hans Hellendoorn*

Main category: cs.CV

TL;DR: 提出带时空注意力机制的顺序神经网络模型用于车道检测，在多数据集表现好且具计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有车道检测方法缺乏通用性，基于视觉的方法常忽略图像关键区域和时空显著性，在复杂情况下性能不佳。

Method: 引入带时空注意力机制的顺序神经网络模型，基于标准编码器 - 解码器结构和常用神经网络骨干，在三个大规模开源数据集上进行训练和评估。

Result: 模型在各种测试场景中优于现有方法；与基线顺序模型相比，参数更少，乘累加操作减少。

Conclusion: 所提模型具有强大鲁棒性和计算效率，相关数据、代码和模型已开源。

Abstract: Lane detection is a crucial perception task for all levels of automated vehicles (AVs) and Advanced Driver Assistance Systems, particularly in mixed-traffic environments where AVs must interact with human-driven vehicles (HDVs) and challenging traffic scenarios. Current methods lack versatility in delivering accurate, robust, and real-time compatible lane detection, especially vision-based methods often neglect critical regions of the image and their spatial-temporal (ST) salience, leading to poor performance in difficult circumstances such as serious occlusion and dazzle lighting. This study introduces a novel sequential neural network model with a spatial-temporal attention mechanism to focus on key features of lane lines and exploit salient ST correlations among continuous image frames. The proposed model, built on a standard encoder-decoder structure and common neural network backbones, is trained and evaluated on three large-scale open-source datasets. Extensive experiments demonstrate the strength and robustness of the proposed model, outperforming state-of-the-art methods in various testing scenarios. Furthermore, with the ST attention mechanism, the developed sequential neural network models exhibit fewer parameters and reduced Multiply-Accumulate Operations (MACs) compared to baseline sequential models, highlighting their computational efficiency. Relevant data, code, and models are released at https://doi.org/10.4121/4619cab6-ae4a-40d5-af77-582a77f3d821.

</details>


### [526] [DoubleTake: Contrastive Reasoning for Faithful Decision-Making in Medical Imaging](https://arxiv.org/abs/2602.02894)
*Daivik Patel,Shrenik Patel*

Main category: cs.CV

TL;DR: 本文提出对比、文档感知的参考选择框架和反事实 - 对比推理框架，在医学影像推理中取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有医学影像决策方法依赖近邻检索，会返回冗余证据且强化单一假设，ROCO数据集未指明如何为对比推理选择参考。

Method: 引入对比、文档感知的参考选择框架，构建优化的证据集；提出反事实 - 对比推理框架，进行结构化视觉比较并聚合证据。

Result: 在MediConfusion基准上，相比先前方法，集级准确率提升近15%，减少混淆并提高个体准确率。

Conclusion: 所提方法在医学影像推理中表现优异，能有效提升决策准确性。

Abstract: Accurate decision making in medical imaging requires reasoning over subtle visual differences between confusable conditions, yet most existing approaches rely on nearest neighbor retrieval that returns redundant evidence and reinforces a single hypothesis. We introduce a contrastive, document-aware reference selection framework that constructs compact evidence sets optimized for discrimination rather than similarity by explicitly balancing visual relevance, embedding diversity, and source-level provenance using ROCO embeddings and metadata. While ROCO provides large-scale image-caption pairs, it does not specify how references should be selected for contrastive reasoning, and naive retrieval frequently yields near-duplicate figures from the same document. To address this gap, we release a reproducible reference selection protocol and curated reference bank that enable a systematic study of contrastive retrieval in medical image reasoning. Building on these contrastive evidence sets, we propose Counterfactual-Contrastive Inference, a confidence-aware reasoning framework that performs structured pairwise visual comparisons and aggregates evidence using margin-based decision rules with faithful abstention. On the MediConfusion benchmark, our approach achieves state-of-the-art performance, improving set-level accuracy by nearly 15% relative to prior methods while reducing confusion and improving individual accuracy.

</details>


### [527] [Zero-shot large vision-language model prompting for automated bone identification in paleoradiology x-ray archives](https://arxiv.org/abs/2602.03750)
*Owen Dong,Lily Gao,Manish Kota,Bennett A. Landmana,Jelena Bekvalac,Gaynor Western,Katherine D. Van Schaik*

Main category: cs.CV

TL;DR: 提出零样本提示策略用LVLM自动识别古放射影像信息，实验取得高准确率，可加速影像代码开发助力内容导航


<details>
  <summary>Details</summary>
Motivation: 古放射影像数据异质性大，内容导航困难，需高效分诊手段以利专家分析

Method: 将原始DICOM文件转为PNG，用精心设计的提示将图像提交给LVLM，获取结构化JSON输出并整理成表格

Result: 在100张样本图像上，主骨识别准确率92%，投影视图准确率80%，侧别识别准确率100%，模糊情况有置信度标记

Conclusion: LVLMs能大幅加速古放射学数据集代码词开发，利于未来人类学工作流程的内容导航

Abstract: Paleoradiology, the use of modern imaging technologies to study archaeological and anthropological remains, offers new windows on millennial scale patterns of human health. Unfortunately, the radiographs collected during field campaigns are heterogeneous: bones are disarticulated, positioning is ad hoc, and laterality markers are often absent. Additionally, factors such as age at death, age of bone, sex, and imaging equipment introduce high variability. Thus, content navigation, such as identifying a subset of images with a specific projection view, can be time consuming and difficult, making efficient triaging a bottleneck for expert analysis. We report a zero shot prompting strategy that leverages a state of the art Large Vision Language Model (LVLM) to automatically identify the main bone, projection view, and laterality in such images. Our pipeline converts raw DICOM files to bone windowed PNGs, submits them to the LVLM with a carefully engineered prompt, and receives structured JSON outputs, which are extracted and formatted onto a spreadsheet in preparation for validation. On a random sample of 100 images reviewed by an expert board certified paleoradiologist, the system achieved 92% main bone accuracy, 80% projection view accuracy, and 100% laterality accuracy, with low or medium confidence flags for ambiguous cases. These results suggest that LVLMs can substantially accelerate code word development for large paleoradiology datasets, allowing for efficient content navigation in future anthropology workflows.

</details>


### [528] [Feature, Alignment, and Supervision in Category Learning: A Comparative Approach with Children and Neural Networks](https://arxiv.org/abs/2602.03124)
*Fanxiao Wani Qiu,Oscar Leong*

Main category: cs.CV

TL;DR: 本文对比儿童和卷积神经网络在少样本半监督类别学习任务中的表现，发现两者学习特点不同，强调人类与模型对比需考虑条件。


<details>
  <summary>Details</summary>
Motivation: 了解人类和机器如何从稀疏数据中学习，这对认知科学和机器学习至关重要。

Method: 采用物种公平设计，让儿童和卷积神经网络在相同条件下接触新物体类别，改变监督程度、目标特征和感知对齐情况进行实验。

Result: 儿童能从最少标签中快速泛化，但有特征特定偏差和对齐敏感性；卷积神经网络中增加监督能提高性能，对齐和特征结构会调节额外监督对学习的影响。

Conclusion: 人类与模型的比较必须在正确条件下进行，应强调监督、特征结构和对齐之间的相互作用而非整体准确性。

Abstract: Understanding how humans and machines learn from sparse data is central to cognitive science and machine learning. Using a species-fair design, we compare children and convolutional neural networks (CNNs) in a few-shot semi-supervised category learning task. Both learners are exposed to novel object categories under identical conditions. Learners receive mixtures of labeled and unlabeled exemplars while we vary supervision (1/3/6 labels), target feature (size, shape, pattern), and perceptual alignment (high/low). We find that children generalize rapidly from minimal labels but show strong feature-specific biases and sensitivity to alignment. CNNs show a different interaction profile: added supervision improves performance, but both alignment and feature structure moderate the impact additional supervision has on learning. These results show that human-model comparisons must be drawn under the right conditions, emphasizing interactions among supervision, feature structure, and alignment rather than overall accuracy.

</details>


### [529] [Fully Kolmogorov-Arnold Deep Model in Medical Image Segmentation](https://arxiv.org/abs/2602.03156)
*Xingyu Qiu,Xinghua Ma,Dong Liang,Gongning Luo,Wei Wang,Kuanquan Wang,Shuo Li*

Main category: cs.CV

TL;DR: 本文提出首个全基于KA的深度模型ALL U - KAN，解决深度堆叠KAN的训练和内存问题，在医学图像分割任务中表现优越。


<details>
  <summary>Details</summary>
Motivation: 深度堆叠KAN训练困难、内存要求高，现有研究只能使用少量KAN层，限制了对KAN的全面探索。

Method: 提出Share - activation KAN (SaKAN)简化参数化和增加训练样本密度以缓解训练难度；提出Grad - Free Spline减少内存使用和计算开销；用KA和KAonv层完全替代FC和Conv层构建ALL U - KAN。

Result: 在三个医学图像分割任务中，全基于KA的架构比部分基于KA和传统架构有更高的分割精度，ALL U - KAN相比直接深度堆叠KAN参数减少10倍，内存消耗减少超20倍。

Conclusion: 克服了深度堆叠KAN的局限，解锁了对深度KAN架构的新探索。

Abstract: Deeply stacked KANs are practically impossible due to high training difficulties and substantial memory requirements. Consequently, existing studies can only incorporate few KAN layers, hindering the comprehensive exploration of KANs. This study overcomes these limitations and introduces the first fully KA-based deep model, demonstrating that KA-based layers can entirely replace traditional architectures in deep learning and achieve superior learning capacity. Specifically, (1) the proposed Share-activation KAN (SaKAN) reformulates Sprecher's variant of Kolmogorov-Arnold representation theorem, which achieves better optimization due to its simplified parameterization and denser training samples, to ease training difficulty, (2) this paper indicates that spline gradients contribute negligibly to training while consuming huge GPU memory, thus proposes the Grad-Free Spline to significantly reduce memory usage and computational overhead. (3) Building on these two innovations, our ALL U-KAN is the first representative implementation of fully KA-based deep model, where the proposed KA and KAonv layers completely replace FC and Conv layers. Extensive evaluations on three medical image segmentation tasks confirm the superiority of the full KA-based architecture compared to partial KA-based and traditional architectures, achieving all higher segmentation accuracy. Compared to directly deeply stacked KAN, ALL U-KAN achieves 10 times reduction in parameter count and reduces memory consumption by more than 20 times, unlocking the new explorations into deep KAN architectures.

</details>


### [530] [HypCBC: Domain-Invariant Hyperbolic Cross-Branch Consistency for Generalizable Medical Image Analysis](https://arxiv.org/abs/2602.03264)
*Francesco Di Salvo,Sebastian Doerrich,Jonas Alle,Christian Ledig*

Main category: cs.CV

TL;DR: 本文利用双曲流形优势进行医学图像分析，提出无监督、域不变的双曲跨分支一致性约束，在多个数据集上表现优于欧几里得方法。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在训练分布外的鲁棒泛化是挑战，医学图像分析中数据稀缺、存在协变量偏移，且现有学习范式基于欧几里得流形无法捕捉临床数据复杂结构。

Method: 利用双曲流形建模复杂数据特征，进行双曲表示学习的全面验证，提出无监督、域不变的双曲跨分支一致性约束。

Result: 在11个分布内数据集和3个ViT模型上有显著提升，在三个领域泛化基准上平均AUC比现有欧几里得方法高2.1%。

Conclusion: 所提出方法能促进域不变特征，在不同成像方式、数据大小和标签粒度下有泛化能力。

Abstract: Robust generalization beyond training distributions remains a critical challenge for deep neural networks. This is especially pronounced in medical image analysis, where data is often scarce and covariate shifts arise from different hardware devices, imaging protocols, and heterogeneous patient populations. These factors collectively hinder reliable performance and slow down clinical adoption. Despite recent progress, existing learning paradigms primarily rely on the Euclidean manifold, whose flat geometry fails to capture the complex, hierarchical structures present in clinical data. In this work, we exploit the advantages of hyperbolic manifolds to model complex data characteristics. We present the first comprehensive validation of hyperbolic representation learning for medical image analysis and demonstrate statistically significant gains across eleven in-distribution datasets and three ViT models. We further propose an unsupervised, domain-invariant hyperbolic cross-branch consistency constraint. Extensive experiments confirm that our proposed method promotes domain-invariant features and outperforms state-of-the-art Euclidean methods by an average of $+2.1\%$ AUC on three domain generalization benchmarks: Fitzpatrick17k, Camelyon17-WILDS, and a cross-dataset setup for retinal imaging. These datasets span different imaging modalities, data sizes, and label granularities, confirming generalization capabilities across substantially different conditions. The code is available at https://github.com/francescodisalvo05/hyperbolic-cross-branch-consistency .

</details>


### [531] [Symbol-Aware Reasoning with Masked Discrete Diffusion for Handwritten Mathematical Expression Recognition](https://arxiv.org/abs/2602.03370)
*Takaya Kawakatsu,Ryo Ishiyama*

Main category: cs.CV

TL;DR: 提出离散扩散框架用于手写数学表达式识别，在基准测试中表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 自回归模型在手写数学表达式识别中存在曝光偏差和句法不一致问题。

Method: 提出离散扩散框架，通过多步重掩码逐步细化符号和结构关系，采用符号感知分词和随机掩码互学习。

Result: 在MathWriting基准上实现5.56% CER和60.42% EM，在CROHME 2014 - 2023上有一致提升。

Conclusion: 离散扩散为结构感知视觉识别提供了新范式。

Abstract: Handwritten Mathematical Expression Recognition (HMER) requires reasoning over diverse symbols and 2D structural layouts, yet autoregressive models struggle with exposure bias and syntactic inconsistency. We present a discrete diffusion framework that reformulates HMER as iterative symbolic refinement instead of sequential generation. Through multi-step remasking, the proposal progressively refines both symbols and structural relations, removing causal dependencies and improving structural consistency. A symbol-aware tokenization and Random-Masking Mutual Learning further enhance syntactic alignment and robustness to handwriting diversity. On the MathWriting benchmark, the proposal achieves 5.56\% CER and 60.42\% EM, outperforming strong Transformer and commercial baselines. Consistent gains on CROHME 2014--2023 demonstrate that discrete diffusion provides a new paradigm for structure-aware visual recognition beyond generative modeling.

</details>


### [532] [From Vicious to Virtuous Cycles: Synergistic Representation Learning for Unsupervised Video Object-Centric Learning](https://arxiv.org/abs/2602.03390)
*Hyun Seok Seong,WonJun Moon,Jae-Pil Heo*

Main category: cs.CV

TL;DR: 现有无监督以对象为中心学习模型有冲突问题，提出SRL方法打破循环，取得SOTA结果并开源代码。


<details>
  <summary>Details</summary>
Motivation: 现有基于重建训练的无监督以对象为中心学习模型，编码器和解码器存在冲突导致恶性循环。

Method: 引入协同表示学习（SRL），让编码器和解码器相互细化，用预热阶段和槽正则化目标稳定该过程。

Result: 在视频以对象为中心学习基准测试中取得了最先进的结果。

Conclusion: SRL能有效弥合编码器和解码器之间的表示差距，提升模型性能。

Abstract: Unsupervised object-centric learning models, particularly slot-based architectures, have shown great promise in decomposing complex scenes. However, their reliance on reconstruction-based training creates a fundamental conflict between the sharp, high-frequency attention maps of the encoder and the spatially consistent but blurry reconstruction maps of the decoder. We identify that this discrepancy gives rise to a vicious cycle: the noisy feature map from the encoder forces the decoder to average over possibilities and produce even blurrier outputs, while the gradient computed from blurry reconstruction maps lacks high-frequency details necessary to supervise encoder features. To break this cycle, we introduce Synergistic Representation Learning (SRL) that establishes a virtuous cycle where the encoder and decoder mutually refine one another. SRL leverages the encoder's sharpness to deblur the semantic boundary within the decoder output, while exploiting the decoder's spatial consistency to denoise the encoder's features. This mutual refinement process is stabilized by a warm-up phase with a slot regularization objective that initially allocates distinct entities per slot. By bridging the representational gap between the encoder and decoder, SRL achieves state-of-the-art results on video object-centric learning benchmarks. Codes are available at https://github.com/hynnsk/SRL.

</details>


### [533] [Fast-Slow Efficient Training for Multimodal Large Language Models via Visual Token Pruning](https://arxiv.org/abs/2602.03815)
*Dingkun Zhang,Shuhan Qi,Yulin Wu,Xinyu Xiao,Xuan Wang,Long Chen*

Main category: cs.CV

TL;DR: 文章针对多模态大语言模型训练效率低的问题，提出DualSpeed快慢框架，可在保证性能的同时提升训练效率，实验表明能加速模型训练。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型存在严重的训练效率问题，现有方法多关注减小模型或可训练参数，应用视觉令牌剪枝会导致训练 - 推理不匹配，需新的高效训练方法。

Method: 提出DualSpeed框架，快模式结合现有视觉令牌剪枝方法并使用模式隔离器，慢模式在完整视觉序列上训练，同时利用自蒸馏从快模式学习。

Result: DualSpeed能将LLaVA - 1.5的训练加速2.1倍，LLaVA - NeXT加速4.0倍，且保留超99%的性能。

Conclusion: DualSpeed框架可以实现多模态大语言模型训练效率和性能的平衡。

Abstract: Multimodal Large Language Models (MLLMs) suffer from severe training inefficiency issue, which is associated with their massive model sizes and visual token numbers. Existing efforts in efficient training focus on reducing model sizes or trainable parameters. Inspired by the success of Visual Token Pruning (VTP) in improving inference efficiency, we are exploring another substantial research direction for efficient training by reducing visual tokens. However, applying VTP at the training stage results in a training-inference mismatch: pruning-trained models perform poorly when inferring on non-pruned full visual token sequences. To close this gap, we propose DualSpeed, a fast-slow framework for efficient training of MLLMs. The fast-mode is the primary mode, which incorporates existing VTP methods as plugins to reduce visual tokens, along with a mode isolator to isolate the model's behaviors. The slow-mode is the auxiliary mode, where the model is trained on full visual sequences to retain training-inference consistency. To boost its training, it further leverages self-distillation to learn from the sufficiently trained fast-mode. Together, DualSpeed can achieve both training efficiency and non-degraded performance. Experiments show DualSpeed accelerates the training of LLaVA-1.5 by 2.1$\times$ and LLaVA-NeXT by 4.0$\times$, retaining over 99% performance. Code: https://github.com/dingkun-zhang/DualSpeed

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [534] [Applications of structural equation modeling and mathematical statistics to the triggering mechanism of a class of liquors consumer behaviors in Sichuan province](https://arxiv.org/abs/2602.02956)
*Ruofeng Rao*

Main category: econ.GN

TL;DR: 本文运用结构方程模型（SEM）分析五粮液消费者数据，揭示变量间因果关系，发现环境刺激是主要影响因素，且Z世代网购高端酒频率高，最后给出销售优化建议。


<details>
  <summary>Details</summary>
Motivation: 研究多种因素对五粮液购买行为的影响，解构变量间因果关系，优化销售策略。

Method: 运用结构方程模型，构建包含潜变量测量和路径分析的双框架，对四川省五粮液消费者网上问卷的协方差矩阵进行分析，用最大似然估计来检验路径系数的显著性。

Result: 发现消费者民族中心主义直接促进购买意愿，环境刺激是主要影响因素，通过感知价值间接影响购买行为，Z世代网购高端五粮液酒频率高。

Conclusion: 通过优化分析模型得出数据驱动的战略建议，可促进四川市场五粮液产品销售优化。

Abstract: Structural Equation Modeling (SEM) systematically validated hierarchical pathways among multiple factors by constructing a dual framework integrating latent variable measurement and path analysis, utilizing covariance matrices derived from online questionnaires of Wuliangye consumers in Sichuan Province. Statistical analysis quantified path coefficient significance through maximum likelihood estimation, revealing via factor loadings and goodness-of-fit tests that consumer ethnocentrism directly promotes purchase intention, while simultaneously refuting the null hypothesis regarding perceived behavioral control-thus deconstructing the "trigger-transmission" causal chain among variables. Crucially, SEM findings revealed environmental stimuli as the predominant factor, indirectly influencing purchasing behavior through perceived value, contrary to existing literature asserting equal impacts from consumer ethnocentrism, environmental stimuli, and perceived behavioral control. Statistical evidence further demonstrated higher online purchase frequency for premium Wuliangye liquor, aligning with Generation Z's e-commerce preferences. By implementing stricter website-based participant screening than prior studies, this research optimized the analytical model, yielding data-driven strategic recommendations: strengthening e-commerce platforms, enhancing promotional expertise, leveraging cultural localization, and prioritizing premium product development. These actionable insights significantly advance sales optimization strategies for Wuliangye products in Sichuan's dynamic market.

</details>


### [535] [Mathematical Modeling of Common-Pool Resources: A Comprehensive Review of Bioeconomics, Strategic Interaction, and Complex Adaptive Systems](https://arxiv.org/abs/2602.03129)
*Zebiao Li,Rui Liu,Chengyi Tu*

Main category: econ.GN

TL;DR: 该综述梳理分析公共池塘资源治理数学框架的历史与理论演变，阐明从静态优化到动态恢复力的管理范式转变。


<details>
  <summary>Details</summary>
Motivation: 公共池塘资源治理是多领域持久且复杂的挑战，需对相关数学框架进行梳理分析。

Method: 追溯从20世纪中期早期确定性生物经济模型到当代复杂人 - 环境系统模型的发展轨迹，用经典博弈论分析‘公地悲剧’，探讨‘奥斯特罗姆转向’，结合进化博弈论、行为经济学等，最后综合随机微分方程和基于主体的计算经济学等进展。

Result: 明确了不同阶段数学框架的特点和作用，如经典博弈论提供预测基线，‘奥斯特罗姆转向’引入制度现实，进化博弈论等放松理性假设，最新进展捕捉空间异质性等关键因素。

Conclusion: 公共池塘资源管理从静态优化向动态恢复力范式转变。

Abstract: The governance of common-pool resources-resource systems characterized by high subtractability of yield and difficulty of exclusion-constitutes one of the most persistent and intricate challenges in the fields of economics, ecology, and applied mathematics. This comprehensive review delineates the historical and theoretical evolution of the mathematical frameworks developed to analyze, predict, and manage these systems. We trace the intellectual trajectory from the early, deterministic bioeconomic models of the mid-20th century, which established the fundamental tension between individual profit maximization and collective efficiency, to the contemporary era of complex coupled human-environment system models. Our analysis systematically dissects the formalization of the "Tragedy of the Commons" through the lens of classical cooperative and non-cooperative game theory, examining how the N-person Prisoner's Dilemma and Nash Equilibrium concepts provided the initial, albeit pessimistic, predictive baseline. We subsequently explore the "Ostrom Turn," which necessitated the integration of institutional realism-specifically monitoring, graduated sanctions, and communication-into formal game-theoretic structures. The review further investigates the relaxation of rationality assumptions via evolutionary game theory and behavioral economics, highlighting the destabilizing roles of prospect theory and hyperbolic discounting. Finally, we synthesize recent advances in stochastic differential equations and agent-based computational economics, which capture the critical roles of spatial heterogeneity, noise-induced regime shifts, and early warning signals of collapse. By unifying these diverse mathematical threads, this review elucidates the shifting paradigm from static optimization to dynamic resilience in the management of the commons.

</details>


### [536] [The long-run returns to breastfeeding](https://arxiv.org/abs/2602.03221)
*Marco Francesconi,Stephanie von Hinke,Emil N. Sørensen*

Main category: econ.GN

TL;DR: 本文研究20世纪中叶英国母乳喂养对后代健康和人力资本的影响，发现母乳喂养影响身高和流体智力，且对身高的影响存在基因异质性。


<details>
  <summary>Details</summary>
Motivation: 探究20世纪中叶英国母亲母乳喂养对后代健康和人力资本结果的影响。

Method: 采用家庭内设计，对比母乳喂养和非母乳喂养的兄弟姐妹；进一步用多基因指数分析影响的基因异质性。

Result: 母乳喂养增加成人身高和流体智力，但不影响教育程度和成人BMI；母乳喂养对身高的影响在有高个子基因倾向的人群中更大，其他结果无基因异质性。

Conclusion: 母乳喂养在儿童发展中起着不可忽视的作用。

Abstract: This paper shows that the mid-20th century was characterised by a considerable reduction in breastfeeding rates, reducing from over 80% in the late 1930s to just over 40% only three decades later. We investigate how maternal breastfeeding during this period has shaped offspring health and human capital outcomes in the UK. We use a within-family design, comparing children who were breastfed to their sibling(s) who were not. Our results show that breastfeeding increases adult height, as well as fluid intelligence, but does not affect educational attainment, nor adult BMI. In further analyses, we examine whether and how this impact varies with individuals' genetic "predisposition" for these outcomes, proxied by the outcome-specific polygenic index. We find that the "height-returns" to breastfeeding are larger among those genetically predisposed to be taller, with no genetic heterogeneity for the other outcomes, though we note that power in the within-family GxE analysis is more limited. Overall, our estimates suggest that breastfeeding plays a non-negligible role in child development.

</details>


### [537] [Confrontation with the West and Long-Run Economic and Institutional Outcomes: Evidence from Iran](https://arxiv.org/abs/2602.03231)
*Rok Spruk*

Main category: econ.GN

TL;DR: 本文研究伊朗与西方对抗的长期经济和制度后果，发现对抗导致伊朗GDP、FDI等大幅下降，政治稳定性等恶化，是一种持久冲击。


<details>
  <summary>Details</summary>
Motivation: 研究伊朗与西方对抗带来的长期经济和制度后果，而非仅关注短期制裁影响。

Method: 使用合成控制和广义合成控制方法，从与西方关系持续正常化的国家中构建反事实情况。

Result: 伊朗实际GDP和人均GDP大幅持续损失，外国直接投资、贸易一体化和非石油出口急剧下降，政治稳定性、法治和腐败控制能力显著恶化。

Conclusion: 对抗是一种深刻而持久的经济和制度冲击，拓展了研究至长期地缘政治孤立影响。

Abstract: This paper studies the long-run economic and institutional consequences of Iran's confrontation with the West, treating the 2006-2007 strategic shift as the onset of a sustained confrontation regime rather than a discrete sanctions episode. Using synthetic control and generalized synthetic control methods, I construct transparent counterfactuals for Iran's post-confrontation trajectory from a donor pool of countries with continuously normalized relations with the West. I find large, persistent losses in real GDP and GDP per capita, accompanied by sharp declines in foreign direct investment, trade integration, and non-oil exports. These economic effects coincide with substantial and durable deterioration in political stability, rule of law, and control of corruption. Magnitude calculations imply cumulative output losses comparable to civil-war settings, despite the absence of internal armed conflict. The results highlight confrontation as a deep and persistent economic and institutional shock, extending the literature beyond short-run sanctions effects to sustained geopolitical isolation.

</details>


### [538] [Tracing the Genetic Footprints of the UK National Health Service](https://arxiv.org/abs/2602.03751)
*Nicolau Martin-Bassols,Pietro Biroli,Elisabetta De Cao,Massimo Anelli,Stephanie von Hinke,Silvia Mendolia*

Main category: econ.GN

TL;DR: 本文评估英国国民医疗服务体系（NHS）引入对早期死亡率的因果影响及生存是否具有选择性，发现NHS引入后死胎和婴儿死亡率显著下降，且影响了人口构成。


<details>
  <summary>Details</summary>
Motivation: 评估NHS引入对早期死亡率的因果影响，测试生存是否具有选择性。

Method: 采用局部随机化下的回归断点设计，比较1948年7月前后出生的个体，利用新数字化的每周死亡记录，并使用多基因指数追踪人口构成变化。

Result: NHS引入后死胎和婴儿死亡率显著下降，出生队列的多基因指数发生变化，结果在不同数据集上稳健，在社会经济弱势地区和男性中影响最强。

Conclusion: 大规模公共政策会对人口构成留下持久印记并产生长期生存偏差。

Abstract: The establishment of the UK National Health Service (NHS) in July 1948 was one of the most consequential health policy interventions of the twentieth century, providing universal and free access to medical care and substantially expanding maternal and infant health services. In this paper, we estimate the causal effect of the NHS introduction on early-life mortality and we test whether survival is selective. We adopt a regression discontinuity design under local randomization, comparing individuals born just before and just after July 1948. Leveraging newly digitized weekly death records, we document a significant decline in stillbirths and infant mortality following the introduction of the NHS, the latter driven primarily by reductions in deaths from congenital conditions and diarrhea. We then use polygenic indexes (PGIs), fixed at conception, to track changes in population composition, showing that cohorts born at or after the NHS introduction exhibit higher PGIs associated with contextually-adverse traits (e.g., depression, COPD, and preterm birth) and lower PGIs associated with contextually-valued traits (e.g., educational attainment, self-rated health, and pregnancy length), with effect sizes as large as 7.5% of a standard deviation. These results based on the UK Biobank data are robust to family-based designs and replicate in the English Longitudinal Study of Ageing and the UK Household Longitudinal Study. Effects are strongest in socioeconomically disadvantaged areas and among males. This novel evidence on the existence and magnitude of selective survival highlights how large-scale public policies can leave a persistent imprint on population composition and generate long-term survival biases.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [539] [ProphetKV: User-Query-Driven Selective Recomputation for Efficient KV Cache Reuse in Retrieval-Augmented Generation](https://arxiv.org/abs/2602.02579)
*Shihao Wang,Jiahao Chen,Yanqi Pan,Hao Huang,Yichen Hao,Xiangyu Zou,Wen Xia,Wentao Zhang,Haitao Wang,Junhong Li,Chongyang Qiu,Pengfei Wang*

Main category: cs.OS

TL;DR: 当前长上下文RAG预填充阶段有计算瓶颈，现有方法有‘挤出效应’，提出ProphetKV方法，以低开销实现高保真注意力恢复，评估显示效果优异。


<details>
  <summary>Details</summary>
Motivation: 长上下文RAG的预填充阶段受计算开销严重限制，且现有方法的令牌选择标准存在‘挤出效应’，降低推理准确性。

Method: 提出ProphetKV方法，基于与用户查询的语义相关性动态对令牌进行优先级排序，并采用双阶段重新计算管道将分层注意力指标融合成高实用集。

Result: ProphetKV仅用20%的重新计算比率保持了全预填充准确率的96%-101%，在RULER和LongBench上比现有方法有显著的准确率提升。

Conclusion: ProphetKV能以最小的开销实现高保真的注意力恢复，有效解决长上下文RAG预填充阶段的问题。

Abstract: The prefill stage of long-context Retrieval-Augmented Generation (RAG) is severely bottlenecked by computational overhead. To mitigate this, recent methods assemble pre-calculated KV caches of retrieved RAG documents (by a user query) and reprocess selected tokens to recover cross-attention between these pre-calculated KV caches. However, we identify a fundamental "crowding-out effect" in current token selection criteria: globally salient but user-query-irrelevant tokens saturate the limited recomputation budget, displacing the tokens truly essential for answering the user query and degrading inference accuracy.
  We propose ProphetKV, a user-query-driven KV Cache reuse method for RAG scenarios. ProphetKV dynamically prioritizes tokens based on their semantic relevance to the user query and employs a dual-stage recomputation pipeline to fuse layer-wise attention metrics into a high-utility set. By ensuring the recomputation budget is dedicated to bridging the informational gap between retrieved context and the user query, ProphetKV achieves high-fidelity attention recovery with minimal overhead. Our extensive evaluation results show that ProphetKV retains 96%-101% of full-prefill accuracy with only a 20% recomputation ratio, while achieving accuracy improvements of 8.8%-24.9% on RULER and 18.6%-50.9% on LongBench over the state-of-the-art approaches (e.g., CacheBlend, EPIC, and KVShare).

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [540] [Acceleration of Atomistic NEGF: Algorithms, Parallelization, and Machine Learning](https://arxiv.org/abs/2602.03438)
*Mathieu Luisier,Nicolas Vetsch,Alexander Maeder,Vincent Maillou,Anders Winka,Leonard Deuschle,Chen Hao Xia,Manasa Kaniselvan,Marko Mladenovic,Jiang Cao,Alexandros Nikolaos Ziogas*

Main category: cond-mat.mtrl-sci

TL;DR: 本文总结了使DFT+NEGF模拟更接近现实系统尺寸和功能的关键算法成就，并讨论利用图神经网络和机器学习加速从头算器件模拟的可能性。


<details>
  <summary>Details</summary>
Motivation: 推动DFT+NEGF模拟更接近现实系统的尺寸和功能，提升模拟能力。

Method: 回顾相关算法成就，探讨利用图神经网络和机器学习的方法。

Result: 总结出关键算法成就，讨论了新的加速模拟可能性。

Conclusion: 在纳米器件量子输运模拟方面取得进展，新方法有潜在应用价值。

Abstract: The Non-equilibrium Green's function (NEGF) formalism is a particularly powerful method to simulate the quantum transport properties of nanoscale devices such as transistors, photo-diodes, or memory cells, in the ballistic limit of transport or in the presence of various scattering sources such as electronphonon, electron-photon, or even electron-electron interactions. The inclusion of all these mechanisms has been first demonstrated in small systems, composed of a few atoms, before being scaled up to larger structures made of thousands of atoms. Also, the accuracy of the models has kept improving, from empirical to fully ab-initio ones, e.g., density functional theory (DFT). This paper summarizes key (algorithmic) achievements that have allowed us to bring DFT+NEGF simulations closer to the dimensions and functionality of realistic systems. The possibility of leveraging graph neural networks and machine learning to speed up ab-initio device simulations is discussed as well.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [541] [Reading Between the Code Lines: On the Use of Self-Admitted Technical Debt for Security Analysis](https://arxiv.org/abs/2602.03470)
*Nicolás E. Díaz Ferreyra,Moritz Mock,Max Kretschmann,Barbara Russo,Mojtaba Shahin,Mansooreh Zahedi,Riccardo Scandariato*

Main category: cs.CR

TL;DR: 本文探讨安全相关自承技术债务（SATD）能否补充静态分析工具（SATs）输出并弥补其局限，通过混合方法研究得出SATD可有效补充SATs。


<details>
  <summary>Details</summary>
Motivation: SATs存在高误报率和漏洞类覆盖不全问题，此前不清楚SATD在SATs辅助安全分析中的利用情况，因此研究SATD对SATs输出的补充作用及弥补其局限的程度。

Method: 采用混合方法，包括用三个先进SATs分析SATD注释的漏洞数据集，以及对72位安全从业者进行在线调查。

Result: 所有SATs联合使用标记了135个安全相关SATD实例中的114个，涵盖24种不同的通用弱点枚举（CWE）标识符；SATD注释手动映射显示33种独特CWE类型，6种是SATs常忽略或难检测的；调查表明开发者常结合SAT输出和SATD见解分析安全弱点。

Conclusion: SATD编码的信息能有意义地补充SAT驱动的安全分析，帮助克服SATs的一些实际缺点。

Abstract: Static Analysis Tools (SATs) are central to security engineering activities, as they enable early identification of code weaknesses without requiring execution. However, their effectiveness is often limited by high false-positive rates and incomplete coverage of vulnerability classes. At the same time, developers frequently document security-related shortcuts and compromises as Self-Admitted Technical Debt (SATD) in software artifacts, such as code comments. While prior work has recognized SATD as a rich source of security information, it remains unclear whether -and in what ways- it is utilized during SAT-aided security analysis. OBJECTIVE: This work investigates the extent to which security-related SATD complements the output produced by SATs and helps bridge some of their well-known limitations. METHOD: We followed a mixed-methods approach consisting of (i) the analysis of a SATD-annotated vulnerability dataset using three state-of-the-art SATs and (ii) an online survey with 72 security practitioners. RESULTS: The combined use of all SATs flagged 114 of the 135 security-related SATD instances, spanning 24 distinct Common Weakness Enumeration (CWE) identifiers. A manual mapping of the SATD comments revealed 33 unique CWE types, 6 of which correspond to categories that SATs commonly overlook or struggle to detect (e.g., race conditions). Survey responses further suggest that developers frequently pair SAT outputs with SATD insights to better understand the impact and root causes of security weaknesses and to identify suitable fixes. IMPLICATIONS: Our findings show that such SATD-encoded information can be a meaningful complement to SAT-driven security analysis, while helping to overcome some of SATs' practical shortcomings.

</details>


### [542] [DECEIVE-AFC: Adversarial Claim Attacks against Search-Enabled LLM-based Fact-Checking Systems](https://arxiv.org/abs/2602.02569)
*Haoran Ou,Kangjie Chen,Gelei Deng,Hangcheng Liu,Jie Zhang,Tianwei Zhang,Kwok-Yan Lam*

Main category: cs.CR

TL;DR: 研究针对基于可搜索大语言模型的事实核查系统的对抗性声明攻击，提出DECEIVE - AFC框架，评估显示攻击显著降低核查性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于可搜索大语言模型的事实核查系统对抗攻击的鲁棒性理解不足，需研究对抗性声明攻击。

Method: 提出DECEIVE - AFC，一种基于代理的对抗攻击框架，集成新的声明级攻击策略和对抗声明有效性评估原则，在不依赖证据源或模型内部信息的情况下探索攻击轨迹。

Result: 在基准数据集和现实系统上的评估表明，攻击使验证准确率从78.7%降至53.7%，显著优于现有基于声明的攻击基线，且具有强跨系统转移性。

Conclusion: 所提出的DECEIVE - AFC框架能有效对基于可搜索大语言模型的事实核查系统进行对抗攻击，降低其验证性能。

Abstract: Fact-checking systems with search-enabled large language models (LLMs) have shown strong potential for verifying claims by dynamically retrieving external evidence. However, the robustness of such systems against adversarial attack remains insufficiently understood. In this work, we study adversarial claim attacks against search-enabled LLM-based fact-checking systems under a realistic input-only threat model. We propose DECEIVE-AFC, an agent-based adversarial attack framework that integrates novel claim-level attack strategies and adversarial claim validity evaluation principles. DECEIVE-AFC systematically explores adversarial attack trajectories that disrupt search behavior, evidence retrieval, and LLM-based reasoning without relying on access to evidence sources or model internals. Extensive evaluations on benchmark datasets and real-world systems demonstrate that our attacks substantially degrade verification performance, reducing accuracy from 78.7% to 53.7%, and significantly outperform existing claim-based attack baselines with strong cross-system transferability.

</details>


### [543] [To Defend Against Cyber Attacks, We Must Teach AI Agents to Hack](https://arxiv.org/abs/2602.02595)
*Terry Yue Zhuo,Yangruibo Ding,Wenbo Guo,Ruijie Meng*

Main category: cs.CR

TL;DR: AI agents break the balance of cybersecurity, existing defenses are inadequate, and defenders need to develop offensive security intelligence and take three actions.


<details>
  <summary>Details</summary>
Motivation: AI agents enable large - scale automated cyber - attacks, and existing defenses can't stop adaptive adversaries, so a shift in defensive strategy is needed.

Method: Identify the inadequacies of existing defenses and propose three actions: construct comprehensive benchmarks, advance to trained agents for vulnerability discovery, and implement governance for offensive agents.

Result: Highlight the inevitability of AI - agent - driven cyber attacks and the need for offensive security intelligence.

Conclusion: Treat offensive AI capabilities as essential defensive infrastructure and master them in controlled settings.

Abstract: For over a decade, cybersecurity has relied on human labor scarcity to limit attackers to high-value targets manually or generic automated attacks at scale. Building sophisticated exploits requires deep expertise and manual effort, leading defenders to assume adversaries cannot afford tailored attacks at scale. AI agents break this balance by automating vulnerability discovery and exploitation across thousands of targets, needing only small success rates to remain profitable. Current developers focus on preventing misuse through data filtering, safety alignment, and output guardrails. Such protections fail against adversaries who control open-weight models, bypass safety controls, or develop offensive capabilities independently. We argue that AI-agent-driven cyber attacks are inevitable, requiring a fundamental shift in defensive strategy. In this position paper, we identify why existing defenses cannot stop adaptive adversaries and demonstrate that defenders must develop offensive security intelligence. We propose three actions for building frontier offensive AI capabilities responsibly. First, construct comprehensive benchmarks covering the full attack lifecycle. Second, advance from workflow-based to trained agents for discovering in-wild vulnerabilities at scale. Third, implement governance restricting offensive agents to audited cyber ranges, staging release by capability tier, and distilling findings into safe defensive-only agents. We strongly recommend treating offensive AI capabilities as essential defensive infrastructure, as containing cybersecurity risks requires mastering them in controlled settings before adversaries do.

</details>


### [544] [TinyGuard:A lightweight Byzantine Defense for Resource-Constrained Federated Learning via Statistical Update Fingerprints](https://arxiv.org/abs/2602.02615)
*Ali Mahdavi,Santa Aghapour,Azadeh Zamanifar,Amirfarhad Farhadi*

Main category: cs.CR

TL;DR: 提出轻量级拜占庭防御机制TinyGuard，通过统计更新指纹增强FedAvg，实验证明其有效性和鲁棒性，适用于基础模型联邦微调。


<details>
  <summary>Details</summary>
Motivation: 现有拜占庭鲁棒聚合机制计算开销大，限制在大规模和资源受限联邦系统中的应用。

Method: 提出TinyGuard，提取客户端更新的统计指纹，在低维指纹空间测量统计偏差识别拜占庭客户端。

Result: 在多个数据集和攻击场景实验表明，TinyGuard能保持FedAvg收敛，准确率达95%，攻击者难以同时逃避检测和有效投毒，消融实验验证检测精度稳定。

Conclusion: TinyGuard架构无关，适用于传统拜占庭防御不可行的基础模型联邦微调。

Abstract: Existing Byzantine robust aggregation mechanisms typically rely on fulldimensional gradi ent comparisons or pairwise distance computations, resulting in computational overhead that limits applicability in large scale and resource constrained federated systems. This paper proposes TinyGuard, a lightweight Byzantine defense that augments the standard FedAvg algorithm via statistical update f ingerprinting. Instead of operating directly on high-dimensional gradients, TinyGuard extracts compact statistical fingerprints cap turing key behavioral properties of client updates, including norm statistics, layer-wise ratios, sparsity measures, and low-order mo ments. Byzantine clients are identified by measuring robust sta tistical deviations in this low-dimensional fingerprint space with nd complexity, without modifying the underlying optimization procedure. Extensive experiments on MNIST, Fashion-MNIST, ViT-Lite, and ViT-Small with LoRA adapters demonstrate that TinyGuard pre serves FedAvg convergence in benign settings and achieves up to 95 percent accuracy under multiple Byzantine attack scenarios, including sign-flipping, scaling, noise injection, and label poisoning. Against adaptive white-box adversaries, Pareto frontier analysis across four orders of magnitude confirms that attackers cannot simultaneously evade detection and achieve effective poisoning, features we term statistical handcuffs. Ablation studies validate stable detection precision 0.8 across varying client counts (50-150), threshold parameters and extreme data heterogeneity . The proposed framework is architecture-agnostic and well-suited for federated fine-tuning of foundation models where traditional Byzantine defenses become impractical

</details>


### [545] [Trustworthy Blockchain-based Federated Learning for Electronic Health Records: Securing Participant Identity with Decentralized Identifiers and Verifiable Credentials](https://arxiv.org/abs/2602.02629)
*Rodrigo Tertulino,Ricardo Almeida,Laercio Alencar*

Main category: cs.CR

TL;DR: 本文提出基于可信区块链的联邦学习框架TBFL，集成SSI标准，用MIMIC - IV数据集评估，能降低安全风险，有良好性能和低计算开销，为跨机构医疗数据协作提供安全经济的生态。


<details>
  <summary>Details</summary>
Motivation: 医疗数字化产生大量电子病历，但隐私法规造成数据孤岛，联邦学习虽有潜力但易受攻击，现有结合区块链的方法依赖概率声誉系统而非强身份验证。

Method: 提出TBFL框架，集成SSI标准，利用去中心化标识符（DIDs）和可验证凭证（VCs）确保只有认证过的医疗实体参与全球模型训练。

Result: 框架成功中和100%的Sybil攻击，有良好预测性能（AUC = 0.954, Recall = 0.890），计算开销可忽略（<0.12%），多机构100轮训练总运营成本约18美元。

Conclusion: 该方法为跨机构医疗数据协作提供了安全、可扩展且经济可行的生态系统。

Abstract: The digitization of healthcare has generated massive volumes of Electronic Health Records (EHRs), offering unprecedented opportunities for training Artificial Intelligence (AI) models. However, stringent privacy regulations such as GDPR and HIPAA have created data silos that prevent centralized training. Federated Learning (FL) has emerged as a promising solution that enables collaborative model training without sharing raw patient data. Despite its potential, FL remains vulnerable to poisoning and Sybil attacks, in which malicious participants corrupt the global model or infiltrate the network using fake identities. While recent approaches integrate Blockchain technology for auditability, they predominantly rely on probabilistic reputation systems rather than robust cryptographic identity verification. This paper proposes a Trustworthy Blockchain-based Federated Learning (TBFL) framework integrating Self-Sovereign Identity (SSI) standards. By leveraging Decentralized Identifiers (DIDs) and Verifiable Credentials (VCs), our architecture ensures only authenticated healthcare entities contribute to the global model. Through comprehensive evaluation using the MIMIC-IV dataset, we demonstrate that anchoring trust in cryptographic identity verification rather than behavioral patterns significantly mitigates security risks while maintaining clinical utility. Our results show the framework successfully neutralizes 100% of Sybil attacks, achieves robust predictive performance (AUC = 0.954, Recall = 0.890), and introduces negligible computational overhead (<0.12%). The approach provides a secure, scalable, and economically viable ecosystem for inter-institutional health data collaboration, with total operational costs of approximately $18 for 100 training rounds across multiple institutions.

</details>


### [546] [Benchmarking Large Language Models for Zero-shot and Few-shot Phishing URL Detection](https://arxiv.org/abs/2602.02641)
*Najmul Hasan,Prashanth BusiReddyGari*

Main category: cs.CR

TL;DR: 传统URL存在安全等方面局限，在AI时代网络钓鱼威胁升级，本文用统一框架对大语言模型进行零样本和少样本提示基准测试，发现少样本提示可提升多个大语言模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统URL缺乏安全等机制，AI时代网络钓鱼威胁升级且新策略产生速度超标记数据生产速度，需有效检测钓鱼URL的方法。

Method: 在统一的零样本和少样本提示框架下对大语言模型进行综合基准测试，使用平衡数据集和一致提示，通过多个指标量化分析。

Result: 对大语言模型性能、泛化能力和模型有效性进行详细分析。

Conclusion: 少样本提示能提高多个大语言模型的性能。

Abstract: The Uniform Resource Locator (URL), introduced in a connectivity-first era to define access and locate resources, remains historically limited, lacking future-proof mechanisms for security, trust, or resilience against fraud and abuse, despite the introduction of reactive protections like HTTPS during the cybersecurity era. In the current AI-first threatscape, deceptive URLs have reached unprecedented sophistication due to the widespread use of generative AI by cybercriminals and the AI-vs-AI arms race to produce context-aware phishing websites and URLs that are virtually indistinguishable to both users and traditional detection tools. Although AI-generated phishing accounted for a small fraction of filter-bypassing attacks in 2024, phishing volume has escalated over 4,000% since 2022, with nearly 50% more attacks evading detection. At the rate the threatscape is escalating, and phishing tactics are emerging faster than labeled data can be produced, zero-shot and few-shot learning with large language models (LLMs) offers a timely and adaptable solution, enabling generalization with minimal supervision. Given the critical importance of phishing URL detection in large-scale cybersecurity defense systems, we present a comprehensive benchmark of LLMs under a unified zero-shot and few-shot prompting framework and reveal operational trade-offs. Our evaluation uses a balanced dataset with consistent prompts, offering detailed analysis of performance, generalization, and model efficacy, quantified by accuracy, precision, recall, F1 score, AUROC, and AUPRC, to reflect both classification quality and practical utility in threat detection settings. We conclude few-shot prompting improves performance across multiple LLMs.

</details>


### [547] [Eidolon: A Practical Post-Quantum Signature Scheme Based on k-Colorability in the Age of Graph Neural Networks](https://arxiv.org/abs/2602.02689)
*Asmaa Cherkaoui,Ramon Flores,Delaram Kahrobaei,Richard Wilson*

Main category: cs.CR

TL;DR: 提出基于NP完全k - 着色问题的后量子签名方案Eidolon，经实验表明该方案能抵抗现代密码分析，复兴组合硬度作为后量子签名基础。


<details>
  <summary>Details</summary>
Motivation: 构建实用的后量子签名方案。

Method: 将Goldreich - Micali - Wigderson零知识协议推广到任意k >= 3，应用Fiat - Shamir变换，用Merkle树承诺压缩签名，通过植入“安静”着色生成难题实例。

Result: 实验显示，对于n >= 60，经典求解器和自定义图神经网络攻击者都无法恢复秘密着色。

Conclusion: 精心设计的k - 着色实例可抵抗现代密码分析，组合硬度可作为后量子签名的可靠基础。

Abstract: We propose Eidolon, a practical post-quantum signature scheme based on the NP-complete k-colorability problem. Our construction generalizes the Goldreich-Micali-Wigderson zero-knowledge protocol to arbitrary k >= 3, applies the Fiat-Shamir transform, and uses Merkle-tree commitments to compress signatures from O(tn) to O(t log n). Crucially, we generate hard instances via planted "quiet" colorings that preserve the statistical profile of random graphs. We present the first empirical security analysis of such a scheme against both classical solvers (ILP, DSatur) and a custom graph neural network (GNN) attacker. Experiments show that for n >= 60, neither approach recovers the secret coloring, demonstrating that well-engineered k-coloring instances can resist modern cryptanalysis, including machine learning. This revives combinatorial hardness as a credible foundation for post-quantum signatures.

</details>


### [548] [Evaluating False Alarm and Missing Attacks in CAN IDS](https://arxiv.org/abs/2602.02781)
*Nirab Hossain,Pablo Moriano*

Main category: cs.CR

TL;DR: 本文对基于机器学习的CAN入侵检测系统进行对抗性评估，结果显示模型在对抗扰动下存在漏洞，强调安全关键汽车入侵检测系统需进行对抗鲁棒性评估


<details>
  <summary>Details</summary>
Motivation: 基于机器学习的CAN入侵检测系统对抗操纵的鲁棒性未得到充分探索，需要对其进行评估

Method: 使用ROAD数据集，对比四个浅层学习模型和一个基于深度神经网络的检测器，通过FGSM、BIM和PGD生成符合协议的有效负载级扰动，评估对良性和恶意CAN帧的对抗效果

Result: 所有模型在良性条件下基线性能良好，但对抗扰动暴露出明显漏洞，所有架构漏报攻击显著增加，ET模型在梯度攻击下对漏报攻击的鲁棒性更好

Conclusion: 对抗操纵可同时触发误报和逃避检测，安全关键汽车入侵检测系统需要进行对抗鲁棒性评估

Abstract: Modern vehicles rely on electronic control units (ECUs) interconnected through the Controller Area Network (CAN), making in-vehicle communication a critical security concern. Machine learning (ML)-based intrusion detection systems (IDS) are increasingly deployed to protect CAN traffic, yet their robustness against adversarial manipulation remains largely unexplored. We present a systematic adversarial evaluation of CAN IDS using the ROAD dataset, comparing four shallow learning models with a deep neural network-based detector. Using protocol-compliant, payload-level perturbations generated via FGSM, BIM and PGD, we evaluate adversarial effects on both benign and malicious CAN frames. While all models achieve strong baseline performance under benign conditions, adversarial perturbations reveal substantial vulnerabilities. Although shallow and deep models are robust to false-alarm induction, with the deep neural network (DNN) performing best on benign traffic, all architectures suffer significant increases in missed attacks. Notably, under gradient-based attacks, the shallow model extra trees (ET) demonstrates improved robustness to missed-attack induction compared to the other models. Our results demonstrate that adversarial manipulation can simultaneously trigger false alarms and evade detection, underscoring the need for adversarial robustness evaluation in safety-critical automotive IDS.

</details>


### [549] [CVE-Factory: Scaling Expert-Level Agentic Tasks for Code Security Vulnerability](https://arxiv.org/abs/2602.03012)
*Xianzhen Luo,Jingyuan Zhang,Shiqi Zhou,Rain Huang,Chuan Xiao,Qingfu Zhu,Zhiyuan Ma,Xing Yue,Yang Yue,Wencong Zeng,Wanxiang Che*

Main category: cs.CR

TL;DR: 提出CVE - Factory框架将CVE元数据转化为可执行任务，构建LiveCVEBench基准和大量训练环境，开源相关资源。


<details>
  <summary>Details</summary>
Motivation: 现有代码代理安全能力评估和改进工作依赖手动复现，成本高、不可扩展且数据分布过时。

Method: 提出CVE - Factory多智能体框架自动将稀疏CVE元数据转化为可执行任务。

Result: CVE - Factory解决方案正确率达95%，环境保真度96%，最新漏洞验证成功率66.2%；构建含190个任务的LiveCVEBench基准；合成超1000个训练环境；微调模型Qwen3 - 32B性能提升。

Conclusion: 开源CVE - Factory、LiveCVEBench等资源，助力代码安全领域智能体任务研究。

Abstract: Evaluating and improving the security capabilities of code agents requires high-quality, executable vulnerability tasks. However, existing works rely on costly, unscalable manual reproduction and suffer from outdated data distributions. To address these, we present CVE-Factory, the first multi-agent framework to achieve expert-level quality in automatically transforming sparse CVE metadata into fully executable agentic tasks. Cross-validation against human expert reproductions shows that CVE-Factory achieves 95\% solution correctness and 96\% environment fidelity, confirming its expert-level quality. It is also evaluated on the latest realistic vulnerabilities and achieves a 66.2\% verified success. This automation enables two downstream contributions. First, we construct LiveCVEBench, a continuously updated benchmark of 190 tasks spanning 14 languages and 153 repositories that captures emerging threats including AI-tooling vulnerabilities. Second, we synthesize over 1,000 executable training environments, the first large-scale scaling of agentic tasks in code security. Fine-tuned Qwen3-32B improves from 5.3\% to 35.8\% on LiveCVEBench, surpassing Claude 4.5 Sonnet, with gains generalizing to Terminal Bench (12.5\% to 31.3\%). We open-source CVE-Factory, LiveCVEBench, Abacus-cve (fine-tuned model), training dataset, and leaderboard. All resources are available at https://github.com/livecvebench/CVE-Factory .

</details>


### [550] [The Trigger in the Haystack: Extracting and Reconstructing LLM Backdoor Triggers](https://arxiv.org/abs/2602.03085)
*Blake Bullwinkel,Giorgio Severi,Keegan Hines,Amanda Minnich,Ram Shankar Siva Kumar,Yonatan Zunger*

Main category: cs.CR

TL;DR: 本文提出一种实用扫描器，用于识别因果语言模型中的潜伏代理式后门，方法可扩展且不依赖先验知识，能在多种场景和模型中恢复有效触发。


<details>
  <summary>Details</summary>
Motivation: 解决检测模型是否被投毒这一长期存在的AI安全问题。

Method: 基于潜伏代理会记忆投毒数据、投毒的大语言模型在输入含后门触发时输出分布和注意力头有独特模式这两个发现，开发一种可扩展的后门扫描方法，仅需推理操作且无需触发或目标行为的先验知识。

Result: 该方法能在多个后门场景、广泛的模型和微调方法中恢复有效的触发。

Conclusion: 所开发的扫描器可自然融入更广泛的防御策略，且不影响模型性能。

Abstract: Detecting whether a model has been poisoned is a longstanding problem in AI security. In this work, we present a practical scanner for identifying sleeper agent-style backdoors in causal language models. Our approach relies on two key findings: first, sleeper agents tend to memorize poisoning data, making it possible to leak backdoor examples using memory extraction techniques. Second, poisoned LLMs exhibit distinctive patterns in their output distributions and attention heads when backdoor triggers are present in the input. Guided by these observations, we develop a scalable backdoor scanning methodology that assumes no prior knowledge of the trigger or target behavior and requires only inference operations. Our scanner integrates naturally into broader defensive strategies and does not alter model performance. We show that our method recovers working triggers across multiple backdoor scenarios and a broad range of models and fine-tuning methods.

</details>


### [551] [Position: 3D Gaussian Splatting Watermarking Should Be Scenario-Driven and Threat-Model Explicit](https://arxiv.org/abs/2602.02602)
*Yangfan Deng,Anirudh Nakra,Min Wu*

Main category: cs.CR

TL;DR: 3D内容获取和创建快速发展，3DGS成有前景的3D内容表示，但需知识产权保护。本文提出场景驱动公式，构建参考框架，研究遗留扩频嵌入方案，以促进3D资产有效知识产权保护。


<details>
  <summary>Details</summary>
Motivation: 3D内容发展使3D资产知识产权保护需求增加，当前缺乏安全规范和评估。

Method: 提出场景驱动公式，通过安全模型形式化对抗能力；构建参考框架组织现有方法；研究遗留扩频嵌入方案。

Result: 构建了参考框架，分析了遗留扩频嵌入方案的优缺点及权衡。

Conclusion: 该工作有助于促进3D资产的有效知识产权保护。

Abstract: 3D content acquisition and creation are expanding rapidly in the new era of machine learning and AI. 3D Gaussian Splatting (3DGS) has become a promising high-fidelity and real-time representation for 3D content. Similar to the initial wave of digital audio-visual content at the turn of the millennium, the demand for intellectual property protection is also increasing, since explicit and editable 3D parameterization makes unauthorized use and dissemination easier. In this position paper, we argue that effective progress in watermarking 3D assets requires articulated security objectives and realistic threat models, incorporating the lessons learned from digital audio-visual asset protection over the past decades. To address this gap in security specification and evaluation, we advocate a scenario-driven formulation, in which adversarial capabilities are formalized through a security model. Based on this formulation, we construct a reference framework that organizes existing methods and clarifies how specific design choices map to corresponding adversarial assumptions. Within this framework, we also examine a legacy spread-spectrum embedding scheme, characterizing its advantages and limitations and highlighting the important trade-offs it entails. Overall, this work aims to foster effective intellectual property protection for 3D assets.

</details>


### [552] [Don't believe everything you read: Understanding and Measuring MCP Behavior under Misleading Tool Descriptions](https://arxiv.org/abs/2602.03580)
*Zhihao Li,Boyang Ma,Xuelong Dai,Minghui Xu,Yue Zhang,Biwei Yan,Kun Li*

Main category: cs.CR

TL;DR: 研究MCP生态中描述与代码不一致问题，发现约13%服务器有重大不匹配，凸显审计和透明性保障需求。


<details>
  <summary>Details</summary>
Motivation: MCP未强制文档化工具行为与实际代码执行一致，存在未充分探索的安全风险，需研究描述与实现不匹配对智能体的影响。

Method: 设计自动化静态分析框架，对36类10240个真实世界MCP服务器进行分析。

Result: 多数服务器一致性高，但约13%有重大不匹配，不同应用类别、流行度和MCP市场存在系统差异。

Conclusion: 描述与代码不一致是基于MCP的AI智能体具体且普遍的攻击面，未来智能体生态需系统审计和更强透明性保障。

Abstract: The Model Context Protocol (MCP) enables large language models to invoke external tools through natural-language descriptions, forming the foundation of many AI agent applications. However, MCP does not enforce consistency between documented tool behavior and actual code execution, even though MCP Servers often run with broad system privileges. This gap introduces a largely unexplored security risk. We study how mismatches between externally presented tool descriptions and underlying implementations systematically shape the mental models and decision-making behavior of intelligent agents. Specifically, we present the first large-scale study of description-code inconsistency in the MCP ecosystem. We design an automated static analysis framework and apply it to 10,240 real-world MCP Servers across 36 categories. Our results show that while most servers are highly consistent, approximately 13% exhibit substantial mismatches that can enable undocumented privileged operations, hidden state mutations, or unauthorized financial actions. We further observe systematic differences across application categories, popularity levels, and MCP marketplaces. Our findings demonstrate that description-code inconsistency is a concrete and prevalent attack surface in MCP-based AI agents, and motivate the need for systematic auditing and stronger transparency guarantees in future agent ecosystems.

</details>


### [553] [Generalizable and Interpretable RF Fingerprinting with Shapelet-Enhanced Large Language Models](https://arxiv.org/abs/2602.03035)
*Tianya Zhao,Junqing Zhang,Haowen Xu,Xiaoyan Sun,Jun Dai,Xuyu Wang*

Main category: cs.CR

TL;DR: 论文提出整合二维形状特征和预训练大语言模型的框架解决DNN在射频指纹识别的问题，实验显示方法在多方面表现优越。


<details>
  <summary>Details</summary>
Motivation: DNN在射频指纹识别的实际部署面临域转移和黑盒属性限制，需要更高效、可解释和可泛化的方法。

Method: 提出整合一组可变长度二维形状特征和预训练大语言模型的框架，利用二维形状特征捕捉局部模式，大语言模型捕捉长程依赖和全局信息，支持原型生成进行少样本推理。

Result: 在六个不同协议和领域的数据集上进行广泛实验，方法在源域和未见域都取得了卓越的标准和少样本性能。

Conclusion: 所提出的框架能有效解决DNN在RF指纹识别中的问题，实现高效、可解释和可泛化的RF指纹识别。

Abstract: Deep neural networks (DNNs) have achieved remarkable success in radio frequency (RF) fingerprinting for wireless device authentication. However, their practical deployment faces two major limitations: domain shift, where models trained in one environment struggle to generalize to others, and the black-box nature of DNNs, which limits interpretability. To address these issues, we propose a novel framework that integrates a group of variable-length two-dimensional (2D) shapelets with a pre-trained large language model (LLM) to achieve efficient, interpretable, and generalizable RF fingerprinting. The 2D shapelets explicitly capture diverse local temporal patterns across the in-phase and quadrature (I/Q) components, providing compact and interpretable representations. Complementarily, the pre-trained LLM captures more long-range dependencies and global contextual information, enabling strong generalization with minimal training overhead. Moreover, our framework also supports prototype generation for few-shot inference, enhancing cross-domain performance without additional retraining. To evaluate the effectiveness of our proposed method, we conduct extensive experiments on six datasets across various protocols and domains. The results show that our method achieves superior standard and few-shot performance across both source and unseen domains.

</details>


### [554] [WebSentinel: Detecting and Localizing Prompt Injection Attacks for Web Agents](https://arxiv.org/abs/2602.03792)
*Xilong Wang,Yinuo Liu,Zhun Wang,Dawn Song,Neil Gong*

Main category: cs.CR

TL;DR: 提出WebSentinel检测和定位网页提示注入攻击，效果佳，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有检测和定位提示注入攻击的方法在网络代理场景下效果有限。

Method: 提出WebSentinel两步法，第一步提取可能受污染的感兴趣片段，第二步检查片段与网页内容的一致性。

Result: WebSentinel效果显著，在多个数据集上大幅超越基线方法。

Conclusion: WebSentinel能有效检测和定位网页提示注入攻击。

Abstract: Prompt injection attacks manipulate webpage content to cause web agents to execute attacker-specified tasks instead of the user's intended ones. Existing methods for detecting and localizing such attacks achieve limited effectiveness, as their underlying assumptions often do not hold in the web-agent setting. In this work, we propose WebSentinel, a two-step approach for detecting and localizing prompt injection attacks in webpages. Given a webpage, Step I extracts \emph{segments of interest} that may be contaminated, and Step II evaluates each segment by checking its consistency with the webpage content as context. We show that WebSentinel is highly effective, substantially outperforming baseline methods across multiple datasets of both contaminated and clean webpages that we collected. Our code is available at: https://github.com/wxl-lxw/WebSentinel.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [555] [Precoding-Oriented CSI Feedback Design with Mutual Information Regularized VQ-VAE](https://arxiv.org/abs/2602.02508)
*Xi Chen,Homa Esfahanizadeh,Foad Sohrabi*

Main category: cs.IT

TL;DR: 提出基于向量量化变分自编码器并结合信息论正则化的面向预编码的CSI反馈框架，固定反馈长度下性能好。


<details>
  <summary>Details</summary>
Motivation: 解决大规模多输入多输出系统中平衡CSI反馈开销和下行链路速率的问题，最大化有限反馈的效用以维持高系统性能。

Method: 提出基于向量量化变分自编码器的框架，并引入可微互信息下界估计器作为训练正则化器。

Result: 该方法在固定反馈长度下达到与可变长度神经压缩方案相当的速率，学习的码字使用更均匀且能捕捉与信道状态信息强相关的可解释结构。

Conclusion: 所提方法在固定反馈长度下有良好表现，能有效利用反馈资源。

Abstract: Efficient channel state information (CSI) compression at the user equipment plays a key role in enabling accurate channel reconstruction and precoder design in massive multiple-input multiple-output systems. A key challenge lies in balancing the CSI feedback overhead with the achievable downlink rate, i.e., maximizing the utility of limited feedback to maintain high system performance. In this work, we propose a precoding-oriented CSI feedback framework based on a vector quantized variational autoencoder, augmented with an information-theoretic regularization. To achieve this, we introduce a differentiable mutual information lower-bound estimator as a training regularizer to promote effective utilization of the learned codebook under a fixed feedback budget. Numerical results demonstrate that the proposed method achieves rates comparable to variable-length neural compression schemes, while operating with fixed-length feedback. Furthermore, the learned codewords exhibit significantly more uniform usage and capture interpretable structures that are strongly correlated with the underlying channel state information.

</details>


### [556] [Generative Decompression: Optimal Lossy Decoding Against Distribution Mismatch](https://arxiv.org/abs/2602.03505)
*Saeed R. Khosravirad,Ahmed Alkhateeb,Ingrid van de Voorde*

Main category: cs.IT

TL;DR: 本文研究有损压缩中压缩器设计分布与源实际分布不匹配时的最优解码策略，提出生成式解压缩策略，拓展至噪声信道传输和面向任务解码，实验表明该策略能缩小与理想联合优化基准的性能差距。


<details>
  <summary>Details</summary>
Motivation: 解决标准化通信系统中解码器获取的源真实分布信息与固定编码器不匹配的问题。

Method: 正式定义不匹配量化问题，采用生成式解压缩策略进行贝叶斯校正，推导鲁棒软解码规则，将方法拓展到面向任务解码。

Result: 生成式解压缩策略严格优于传统质心规则，量化了标准模块化源 - 信道分离架构在不匹配情况下的低效性，最优策略从条件均值估计转变为最大后验检测。

Conclusion: 生成式解压缩能在不修改编码器的情况下实现自适应、高保真重建，缩小与理想联合优化基准的性能差距。

Abstract: This paper addresses optimal decoding strategies in lossy compression where the assumed distribution for compressor design mismatches the actual (true) distribution of the source. This problem has immediate relevance in standardized communication systems where the decoder acquires side information or priors about the true distribution that are unavailable to the fixed encoder. We formally define the mismatched quantization problem, demonstrating that the optimal reconstruction rule, termed generative decompression, aligns with classical Bayesian estimation by taking the conditional expectation under the true distribution given the quantization indices and adapting it to fixed-encoder constraints. This strategy effectively performs a generative Bayesian correction on the decoder side, strictly outperforming the conventional centroid rule. We extend this framework to transmission over noisy channels, deriving a robust soft-decoding rule that quantifies the inefficiency of standard modular source--channel separation architectures under mismatch. Furthermore, we generalize the approach to task-oriented decoding, showing that the optimal strategy shifts from conditional mean estimation to maximum a posteriori (MAP) detection. Experimental results on Gaussian sources and deep-learning-based semantic classification demonstrate that generative decompression closes a vast majority of the performance gap to the ideal joint-optimization benchmark, enabling adaptive, high-fidelity reconstruction without modifying the encoder.

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [557] [Vigemers: on the number of $k$-mers sharing the same XOR-based minimizer](https://arxiv.org/abs/2602.03337)
*Florian Ingels,Antoine Limasset,Camille Marchet,Mikaël Salson*

Main category: cs.DM

TL;DR: 本文扩展了字典序分区的理论结果，研究了一类哈希函数下最小化分区问题，提出组合方程计算分区最大桶大小。


<details>
  <summary>Details</summary>
Motivation: 最小化方法广泛用于分区，但缺乏分区质量的理论研究，字典序分区不平衡，需研究新的哈希函数分区。

Method: 将文献中字典序方法应用到新框架，使用动态规划，通过组合方程计算。

Result: 可在 $O(km^2)$ 时间和 $O(km)$ 空间内计算 $π_k^γ(w)$。

Conclusion: 扩展了字典序分区理论，为使用哈希函数的最小化分区提供了计算方法。

Abstract: In bioinformatics, minimizers have become an inescapable method for handling $k$-mers (words of fixed size $k$) extracted from DNA or RNA sequencing, whether for sampling, storage, querying or partitioning. According to some fixed order on $m$-mers ($m<k$), the minimizer of a $k$-mer is defined as its smallest $m$-mer -- and acts as its fingerprint. Although minimizers are widely used for partitioning purposes, there is almost no theoretical work on the quality of the resulting partitions. For instance, it has been known for decades that the lexicographic order empirically leads to highly unbalanced partitions that are unusable in practice, but it was not until very recently that this observation was theoretically substantiated. The rejection of the lexicographic order has led the community to resort to (pseudo-)random orders using hash functions. In this work, we extend the theoretical results relating to the partitions obtained by the lexicographical order, departing from it to a (exponentially) large family of hash functions, namely where the $m$-mers are XORed against a fixed key. More precisely, provided a key $γ$ and a $m$-mer $w$, we investigate the function that counts how many $k$-mers admit $w$ as their minimizer (i.e. where $w\oplusγ$ is minimal among all $m$-mers of said $k$-mers). This number, denoted by $π_k^γ(w)$, represents the maximum size of the bucket associated with $w$, if all possible $k$-mers were to be seen and partitioned. We adapt the (lexicographical order) method of the literature to our framework and propose combinatorial equations that allow to compute, using dynamic programming, $π_k^γ(w)$ in $O(km^2)$ time and $O(km)$ space.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [558] [VividVoice: A Unified Framework for Scene-Aware Visually-Driven Speech Synthesis](https://arxiv.org/abs/2602.02591)
*Chengyuan Ma,Jiawei Jin,Ruijie Xiong,Chunxiang Jin,Canxiang Yan,Wenming Yang*

Main category: cs.SD

TL;DR: 提出场景感知视觉驱动语音合成新任务及VividVoice框架，构建Vivid - 210K数据集，设计D - MSVA模块，实验显示其性能优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 解决现有语音生成模型在创建与真实物理世界相符的沉浸式听觉体验方面的局限性。

Method: 提出VividVoice统一生成框架，构建Vivid - 210K数据集，设计D - MSVA核心对齐模块。

Result: 主观和客观实验结果表明，VividVoice在音频保真度、内容清晰度和多模态一致性方面显著优于现有基线模型。

Conclusion: VividVoice能有效解决数据稀缺和模态解耦问题，提升语音合成效果。

Abstract: We introduce and define a novel task-Scene-Aware Visually-Driven Speech Synthesis, aimed at addressing the limitations of existing speech generation models in creating immersive auditory experiences that align with the real physical world. To tackle the two core challenges of data scarcity and modality decoupling, we propose VividVoice, a unified generative framework. First, we constructed a large-scale, high-quality hybrid multimodal dataset, Vivid-210K, which, through an innovative programmatic pipeline, establishes a strong correlation between visual scenes, speaker identity, and audio for the first time. Second, we designed a core alignment module, D-MSVA, which leverages a decoupled memory bank architecture and a cross-modal hybrid supervision strategy to achieve fine-grained alignment from visual scenes to timbre and environmental acoustic features. Both subjective and objective experimental results provide strong evidence that VividVoice significantly outperforms existing baseline models in terms of audio fidelity, content clarity, and multimodal consistency. Our demo is available at https://chengyuann.github.io/VividVoice/.

</details>


### [559] [When Noise Lowers The Loss: Rethinking Likelihood-Based Evaluation in Music Large Language Models](https://arxiv.org/abs/2602.02738)
*Xiaosha Li,Chun Liu,Ziyu Wang*

Main category: cs.SD

TL;DR: 研究音乐大语言模型输出质量评估方法，发现标准交叉熵损失存在问题，提出基于损失曲线形状的无标签评估框架。


<details>
  <summary>Details</summary>
Motivation: 音乐大语言模型兴起，需有效评估输出质量方法，标准交叉熵损失在判断音乐质量时存在不足。

Method: 引入噪声注入实验，向音乐上下文注入不同长度的噪声信号，观察模型损失反应。

Result: 音乐大语言模型对局部纹理级干扰反应更强，损失曲线形状包含生成内容质量的关键信息。

Conclusion: 基于损失曲线形状的评估可作为无标签、模型内在的音乐质量评估框架，有助于更合理的训练目标和更精准的基准。

Abstract: The rise of music large language models (LLMs) demands robust methods of evaluating output quality, especially in distinguishing high-quality compositions from "garbage music". Curiously, we observe that the standard cross-entropy loss -- a core training metric -- often decrease when models encounter systematically corrupted music, undermining its validity as a standalone quality indicator. To investigate this paradox, we introduce noise injection experiment, where controlled noise signal of varying lengths are injected into musical contexts. We hypothesize that a model's loss reacting positively to these perturbations, specifically a sharp increase ("Peak" area) for short injection, can serve as a proxy for its ability to discern musical integrity. Experiments with MusicGen models in the audio waveform domain confirm that Music LLMs respond more strongly to local, texture-level disruptions than to global semantic corruption. Beyond exposing this bias, our results highlight a new principle: the shape of the loss curve -- rather than its absolute value -- encodes critical information about the quality of the generated content (i.e., model behavior). We envision this profile-based evaluation as a label-free, model-intrinsic framework for assessing musical quality -- opening the door to more principled training objectives and sharper benchmarks.

</details>


### [560] [Synthetic Data Augmentation for Medical Audio Classification: A Preliminary Evaluation](https://arxiv.org/abs/2602.02955)
*David McShannon,Anthony Mella,Nicholas Dietrich*

Main category: cs.SD

TL;DR: 本文探讨合成数据增强对呼吸音分类的影响，发现单一增强策略未提升性能，仅增强模型集成有小幅改善，表明合成增强在医学音频分类中效果不稳定。


<details>
  <summary>Details</summary>
Motivation: 医学音频分类存在信噪比低、特征难区分等问题，合成数据增强虽被提出但方法和结果不一致，因此研究其对呼吸音分类的影响。

Method: 使用基线深度卷积神经网络，在适度不平衡数据集上评估三种生成式增强策略（变分自编码器、生成对抗网络和扩散模型）。

Result: 无增强的基线模型F1分数为0.645，单一增强策略未带来性能提升，部分配置性能中性或下降，仅增强模型集成使F1分数提升至0.664。

Conclusion: 合成增强在应用于标准CNN分类器时，可能无法持续提升医学音频分类性能，未来应关注特定任务数据特征、模型与增强的兼容性和评估框架。

Abstract: Medical audio classification remains challenging due to low signal-to-noise ratios, subtle discriminative features, and substantial intra-class variability, often compounded by class imbalance and limited training data. Synthetic data augmentation has been proposed as a potential strategy to mitigate these constraints; however, prior studies report inconsistent methodological approaches and mixed empirical results. In this preliminary study, we explore the impact of synthetic augmentation on respiratory sound classification using a baseline deep convolutional neural network trained on a moderately imbalanced dataset (73%:27%). Three generative augmentation strategies (variational autoencoders, generative adversarial networks, and diffusion models) were assessed under controlled experimental conditions. The baseline model without augmentation achieved an F1-score of 0.645. Across individual augmentation strategies, performance gains were not observed, with several configurations demonstrating neutral or degraded classification performance. Only an ensemble of augmented models yielded a modest improvement in F1-score (0.664). These findings suggest that, for medical audio classification, synthetic augmentation may not consistently enhance performance when applied to a standard CNN classifier. Future work should focus on delineating task-specific data characteristics, model-augmentation compatibility, and evaluation frameworks necessary for synthetic augmentation to be effective in medical audio applications.

</details>


### [561] [D3PIA: A Discrete Denoising Diffusion Model for Piano Accompaniment Generation From Lead sheet](https://arxiv.org/abs/2602.03523)
*Eunjin Choi,Hounsu Kim,Hayeon Bang,Taegyun Kwon,Juhan Nam*

Main category: cs.SD

TL;DR: 提出基于离散扩散的钢琴伴奏生成模型D3PIA，在POP909数据集上评估，表现优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 解决符号音乐领域中，根据给定旋律和和弦约束生成完整钢琴伴奏这一具有挑战性的任务。

Method: 提出D3PIA模型，利用钢琴卷帘表示中乐谱与伴奏的局部对齐，结合邻域注意力来编码乐谱和预测音符状态。

Result: 客观评估显示D3PIA比基于连续扩散和Transformer的基线模型更忠实地保留和弦条件；主观听力测试表明D3PIA生成的伴奏更具音乐连贯性。

Conclusion: D3PIA模型在钢琴伴奏生成任务中具有优良性能。

Abstract: Generating piano accompaniments in the symbolic music domain is a challenging task that requires producing a complete piece of piano music from given melody and chord constraints, such as those provided by a lead sheet. In this paper, we propose a discrete diffusion-based piano accompaniment generation model, D3PIA, leveraging local alignment between lead sheet and accompaniment in piano-roll representation. D3PIA incorporates Neighborhood Attention (NA) to both encode the lead sheet and condition it for predicting note states in the piano accompaniment. This design enhances local contextual modeling by efficiently attending to nearby melody and chord conditions. We evaluate our model using the POP909 dataset, a widely used benchmark for piano accompaniment generation. Objective evaluation results demonstrate that D3PIA preserves chord conditions more faithfully compared to continuous diffusion-based and Transformer-based baselines. Furthermore, a subjective listening test indicates that D3PIA generates more musically coherent accompaniments than the comparison models.

</details>


### [562] [Rethinking Music Captioning with Music Metadata LLMs](https://arxiv.org/abs/2602.03023)
*Irmak Bukey,Zhepei Wang,Chris Donahue,Nicholas J. Bryan*

Main category: cs.SD

TL;DR: 提出基于元数据的音乐字幕生成方法，相比端到端基线模型有训练时间短、风格灵活、可进行元数据插补等优势。


<details>
  <summary>Details</summary>
Motivation: 音乐字幕训练需要高质量数据，但数据稀缺，使用大语言模型合成字幕存在风格固定和信息纠缠问题。

Method: 训练元数据预测模型从音频推断详细音乐元数据，在推理时通过预训练大语言模型将其转换为富有表现力的字幕。

Result: 与基于大语言模型生成字幕训练的强端到端基线模型相比，训练时间更短、性能相当；训练后可灵活改变风格；能利用音频和部分元数据进行元数据插补。

Conclusion: 提出的基于元数据的字幕生成方法有效且具有多种优势。

Abstract: Music captioning, or the task of generating a natural language description of music, is useful for both music understanding and controllable music generation. Training captioning models, however, typically requires high-quality music caption data which is scarce compared to metadata (e.g., genre, mood, etc.). As a result, it is common to use large language models (LLMs) to synthesize captions from metadata to generate training data for captioning models, though this process imposes a fixed stylization and entangles factual information with natural language style. As a more direct approach, we propose metadata-based captioning. We train a metadata prediction model to infer detailed music metadata from audio and then convert it into expressive captions via pre-trained LLMs at inference time. Compared to a strong end-to-end baseline trained on LLM-generated captions derived from metadata, our method: (1) achieves comparable performance in less training time over end-to-end captioners, (2) offers flexibility to easily change stylization post-training, enabling output captions to be tailored to specific stylistic and quality requirements, and (3) can be prompted with audio and partial metadata to enable powerful metadata imputation or in-filling--a common task for organizing music data.

</details>


### [563] [Adaptive Evidence Weighting for Audio-Spatiotemporal Fusion](https://arxiv.org/abs/2602.03817)
*Oscar Ovanger,Levi Harris,Timothy H. Keitt*

Main category: cs.SD

TL;DR: 提出FINCH框架用于生物声学分类中证据融合，在多个基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 机器学习系统有多个证据源但可靠性和信息性不同，且通常只有判别式预测器，需要有效融合证据。

Method: 引入FINCH自适应对数线性证据融合框架，学习样本门控函数估计上下文信息可靠性。

Result: FINCH在多个基准测试中始终优于固定权重融合和仅音频基线，在CBI上达到了最先进的性能。

Conclusion: FINCH是一种轻量级、可解释、基于证据的方法，能有效融合证据，提高鲁棒性和误差权衡。

Abstract: Many machine learning systems have access to multiple sources of evidence for the same prediction target, yet these sources often differ in reliability and informativeness across inputs. In bioacoustic classification, species identity may be inferred both from the acoustic signal and from spatiotemporal context such as location and season; while Bayesian inference motivates multiplicative evidence combination, in practice we typically only have access to discriminative predictors rather than calibrated generative models. We introduce \textbf{F}usion under \textbf{IN}dependent \textbf{C}onditional \textbf{H}ypotheses (\textbf{FINCH}), an adaptive log-linear evidence fusion framework that integrates a pre-trained audio classifier with a structured spatiotemporal predictor. FINCH learns a per-sample gating function that estimates the reliability of contextual information from uncertainty and informativeness statistics. The resulting fusion family \emph{contains} the audio-only classifier as a special case and explicitly bounds the influence of contextual evidence, yielding a risk-contained hypothesis class with an interpretable audio-only fallback. Across benchmarks, FINCH consistently outperforms fixed-weight fusion and audio-only baselines, improving robustness and error trade-offs even when contextual information is weak in isolation. We achieve state-of-the-art performance on CBI and competitive or improved performance on several subsets of BirdSet using a lightweight, interpretable, evidence-based approach. Code is available: \texttt{\href{https://anonymous.4open.science/r/birdnoise-85CD/README.md}{anonymous-repository}}

</details>


### [564] [PACE: Pretrained Audio Continual Learning](https://arxiv.org/abs/2602.03355)
*Chang Li,Kanglei Zhou,Liyuan Wang*

Main category: cs.SD

TL;DR: 论文提出首个基于预训练模型的音频持续学习系统基准，分析其挑战，指出简单迁移视觉方法不可行，提出 PACE 方法，实验证明其效果优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有预训练音频模型在现实场景数据分布变化时很脆弱，且视觉中的持续学习策略直接应用于音频效果差，需要探索有效的音频持续学习方法。

Method: 识别出具有首会话适应的分析分类器作为有前途的方向；提出 PACE 方法，通过正则化分析分类器增强 FSA，利用自适应子空间正交的参数高效微调实现多会话适应；引入基于频谱图的边界感知扰动。

Result: 在六个不同的音频持续学习基准上的实验表明，PACE 显著优于现有最先进的基线。

Conclusion: PACE 方法为基于预训练模型的鲁棒且可扩展的音频持续学习迈出了重要一步。

Abstract: Audio is a fundamental modality for analyzing speech, music, and environmental sounds. Although pretrained audio models have significantly advanced audio understanding, they remain fragile in real-world settings where data distributions shift over time. In this work, we present the first systematic benchmark for audio continual learning (CL) with pretrained models (PTMs), together with a comprehensive analysis of its unique challenges. Unlike in vision, where parameter-efficient fine-tuning (PEFT) has proven effective for CL, directly transferring such strategies to audio leads to poor performance. This stems from a fundamental property of audio backbones: they focus on low-level spectral details rather than structured semantics, causing severe upstream-downstream misalignment. Through extensive empirical study, we identify analytic classifiers with first-session adaptation (FSA) as a promising direction, but also reveal two major limitations: representation saturation in coarse-grained scenarios and representation drift in fine-grained scenarios. To address these challenges, we propose PACE, a novel method that enhances FSA via a regularized analytic classifier and enables multi-session adaptation through adaptive subspace-orthogonal PEFT for improved semantic alignment. In addition, we introduce spectrogram-based boundary-aware perturbations to mitigate representation overlap and improve stability. Experiments on six diverse audio CL benchmarks demonstrate that PACE substantially outperforms state-of-the-art baselines, marking an important step toward robust and scalable audio continual learning with PTMs.

</details>


### [565] [CoCoEmo: Composable and Controllable Human-Like Emotional TTS via Activation Steering](https://arxiv.org/abs/2602.03420)
*Siyi Wang,Shihong Tan,Siyi Liu,Hong Jia,Gongping Huang,James Bailey,Ting Dang*

Main category: cs.SD

TL;DR: 本文对混合TTS模型中用于情感控制的激活导向进行系统分析，引入定量可控的导向框架和多评估者评估协议，实现混合情感合成和文本 - 情感不匹配合成，还发现TTS语言模块主导情感韵律合成并提供轻量级导向方法。


<details>
  <summary>Details</summary>
Motivation: 人类语音情感表达细微且具组合性，多数表达性文本转语音系统仅支持单一话语级情感，限制了情感多样性，而激活导向虽有潜力，但在TTS中存在诸多未明确问题。

Method: 对混合TTS模型中的激活导向进行系统分析，引入定量可控的导向框架和多评估者评估协议。

Result: 首次证明情感韵律和表达变异性主要由TTS语言模块而非流匹配模块合成，提供了轻量级导向方法以生成自然、类人的情感语音。

Conclusion: 所提出的方法能实现可组合的混合情感合成和可靠的文本 - 情感不匹配合成，为情感语音合成提供了有效解决方案。

Abstract: Emotional expression in human speech is nuanced and compositional, often involving multiple, sometimes conflicting, affective cues that may diverge from linguistic content. In contrast, most expressive text-to-speech systems enforce a single utterance-level emotion, collapsing affective diversity and suppressing mixed or text-emotion-misaligned expression. While activation steering via latent direction vectors offers a promising solution, it remains unclear whether emotion representations are linearly steerable in TTS, where steering should be applied within hybrid TTS architectures, and how such complex emotion behaviors should be evaluated. This paper presents the first systematic analysis of activation steering for emotional control in hybrid TTS models, introducing a quantitative, controllable steering framework, and multi-rater evaluation protocols that enable composable mixed-emotion synthesis and reliable text-emotion mismatch synthesis. Our results demonstrate, for the first time, that emotional prosody and expressive variability are primarily synthesized by the TTS language module instead of the flow-matching module, and also provide a lightweight steering approach for generating natural, human-like emotional speech.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [566] [Kino-PAX$^+$: Near-Optimal Massively Parallel Kinodynamic Sampling-based Motion Planner](https://arxiv.org/abs/2602.02846)
*Nicolas Perrault,Qi Heng Ho,Morteza Lahijanian*

Main category: cs.RO

TL;DR: 介绍了具有渐近近最优保证的大规模并行运动规划器Kino - PAX$^{+}$，比现有方法更快且成本更低。


<details>
  <summary>Details</summary>
Motivation: 现有基于采样的运动规划器（SBMPs）因串行计算难以实时运行，并行化方法无法保证目标函数优化。

Method: 将传统串行操作分解为三个大规模并行子例程构建稀疏树，聚焦局部邻域中有前景的节点进行传播和细化。

Result: Kino - PAX$^{+}$比现有串行方法快三个数量级，比基于GPU的先进规划器成本更低。

Conclusion: Kino - PAX$^{+}$在保持概率$δ$-鲁棒完备性的同时，能保证渐近$δ$-鲁棒近最优性。

Abstract: Sampling-based motion planners (SBMPs) are widely used for robot motion planning with complex kinodynamic constraints in high-dimensional spaces, yet they struggle to achieve \emph{real-time} performance due to their serial computation design. Recent efforts to parallelize SBMPs have achieved significant speedups in finding feasible solutions; however, they provide no guarantees of optimizing an objective function. We introduce Kino-PAX$^{+}$, a massively parallel kinodynamic SBMP with asymptotic near-optimal guarantees. Kino-PAX$^{+}$ builds a sparse tree of dynamically feasible trajectories by decomposing traditionally serial operations into three massively parallel subroutines. The algorithm focuses computation on the most promising nodes within local neighborhoods for propagation and refinement, enabling rapid improvement of solution cost. We prove that, while maintaining probabilistic $δ$-robust completeness, this focus on promising nodes ensures asymptotic $δ$-robust near-optimality. Our results show that Kino-PAX$^{+}$ finds solutions up to three orders of magnitude faster than existing serial methods and achieves lower solution costs than a state-of-the-art GPU-based planner.

</details>


### [567] [AROLA: A Modular Layered Architecture for Scaled Autonomous Racing](https://arxiv.org/abs/2602.02730)
*Fam Shihata,Mohammed Abdelazim,Ahmed Hussein*

Main category: cs.RO

TL;DR: 本文介绍模块化分层软件架构AROLA及Race Monitor框架，经仿真和硬件验证，证明其可加速开发并提高可重复性。


<details>
  <summary>Details</summary>
Motivation: 随着自主赛车发展，软件栈需相应进化。

Method: 引入AROLA架构，将自动驾驶流程分解成多个模块，通过标准化接口连接；引入Race Monitor框架进行性能评估。

Result: 在仿真和RoboRacer硬件平台验证了AROLA，包括在2025 RoboRacer IV25竞赛中部署。

Conclusion: 模块化、透明接口和系统评估可加速开发并提高可重复性。

Abstract: Autonomous racing has advanced rapidly, particularly on scaled platforms, and software stacks must evolve accordingly. In this work, AROLA is introduced as a modular, layered software architecture in which fragmented and monolithic designs are reorganized into interchangeable layers and components connected through standardized ROS 2 interfaces. The autonomous-driving pipeline is decomposed into sensing, pre-processing, perception, localization and mapping, planning, behavior, control, and actuation, enabling rapid module replacement and objective benchmarking without reliance on custom message definitions. To support consistent performance evaluation, a Race Monitor framework is introduced as a lightweight system through which lap timing, trajectory quality, and computational load are logged in real time and standardized post-race analyses are generated. AROLA is validated in simulation and on hardware using the RoboRacer platform, including deployment at the 2025 RoboRacer IV25 competition. Together, AROLA and Race Monitor demonstrate that modularity, transparent interfaces, and systematic evaluation can accelerate development and improve reproducibility in scaled autonomous racing.

</details>


### [568] [Moving On, Even When You're Broken: Fail-Active Trajectory Generation via Diffusion Policies Conditioned on Embodiment and Task](https://arxiv.org/abs/2602.02895)
*Gilberto G. Briscoe-Martinez,Yaashia Gautam,Rahul Shetty,Anuj Pasricha,Marco M. Nicotra,Alessandro Roncone*

Main category: cs.RO

TL;DR: 本文提出基于扩散的轨迹生成器DEFT，用于应对机器人驱动故障，经模拟和现实场景评估有效。


<details>
  <summary>Details</summary>
Motivation: 解决机器人故障需人工干预问题，实现故障时仍能完成任务的主动式操作。

Method: 引入基于扩散的轨迹生成器DEFT，其基于机器人当前状态和任务约束。

Result: 模拟中在数千个关节故障案例中表现优于基线两倍，在未见过的故障中也表现出色；现实实验中经典方法失败处DEFT能成功。

Conclusion: DEFT能在任意故障配置和现实场景中实现主动式操作。

Abstract: Robot failure is detrimental and disruptive, often requiring human intervention to recover. Maintaining safe operation under impairment to achieve task completion, i.e. fail-active operation, is our target. Focusing on actuation failures, we introduce DEFT, a diffusion-based trajectory generator conditioned on the robot's current embodiment and task constraints. DEFT generalizes across failure types, supports constrained and unconstrained motions, and enables task completion under arbitrary failure. We evaluated DEFT in both simulation and real-world scenarios using a 7-DoF robotic arm. In simulation over thousands of joint-failure cases across multiple tasks, DEFT outperformed the baseline by up to 2 times. On failures unseen during training, it continued to outperform the baseline, indicating robust generalization in simulation. Further, we performed real-world evaluations on two multi-step tasks, drawer manipulation and whiteboard erasing. These experiments demonstrated DEFT succeeding on tasks where classical methods failed. Our results show that DEFT achieves fail-active manipulation across arbitrary failure configurations and real-world deployments.

</details>


### [569] [Embodiment-Aware Generalist Specialist Distillation for Unified Humanoid Whole-Body Control](https://arxiv.org/abs/2602.02960)
*Quanquan Peng,Yunfeng Lin,Yufei Xue,Jiangmiao Pang,Weinan Zhang*

Main category: cs.RO

TL;DR: 本文提出EAGLE框架，可生成单一统一策略控制多种异构人形机器人，实验显示其有高跟踪精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习训练的人形全身控制器多针对单一机器人，动态、自由度和运动学拓扑差异阻碍单一策略控制多样人形机器人，获取能跨实体转移并支持更多行为的通用策略具有挑战性。

Method: 引入EAGLE迭代式通用 - 专家蒸馏框架，每个周期从当前通用策略分叉出特定于实体的专家策略，在各自机器人上优化，再将新技能通过在合并的实体集上训练蒸馏回通用策略，重复循环直至性能收敛。

Result: 在模拟的五个不同机器人和四个现实场景机器人上进行实验，与其他方法相比，EAGLE实现了高跟踪精度和鲁棒性。

Conclusion: EAGLE向可扩展的机群级人形机器人控制迈出了一步。

Abstract: Humanoid Whole-Body Controllers trained with reinforcement learning (RL) have recently achieved remarkable performance, yet many target a single robot embodiment. Variations in dynamics, degrees of freedom (DoFs), and kinematic topology still hinder a single policy from commanding diverse humanoids. Moreover, obtaining a generalist policy that not only transfers across embodiments but also supports richer behaviors-beyond simple walking to squatting, leaning-remains especially challenging. In this work, we tackle these obstacles by introducing EAGLE, an iterative generalist-specialist distillation framework that produces a single unified policy that controls multiple heterogeneous humanoids without per-robot reward tuning. During each cycle, embodiment-specific specialists are forked from the current generalist, refined on their respective robots, and new skills are distilled back into the generalist by training on the pooled embodiment set. Repeating this loop until performance convergence produces a robust Whole-Body Controller validated on robots such as Unitree H1, G1, and Fourier N1. We conducted experiments on five different robots in simulation and four in real-world settings. Through quantitative evaluations, EAGLE achieves high tracking accuracy and robustness compared to other methods, marking a step toward scalable, fleet-level humanoid control. See more details at https://eagle-wbc.github.io/

</details>


### [570] [Training and Simulation of Quadrupedal Robot in Adaptive Stair Climbing for Indoor Firefighting: An End-to-End Reinforcement Learning Approach](https://arxiv.org/abs/2602.03087)
*Baixiao Huang,Baiyu Huang,Yu Hou*

Main category: cs.RO

TL;DR: 提出两阶段端到端深度强化学习方法，让四足机器人学会攀爬不同室内楼梯以处理室内火灾初级搜索情况。


<details>
  <summary>Details</summary>
Motivation: 四足机器人用于室内火灾初级搜索时，在复杂室内环境的态势感知和快速爬楼梯是主要挑战。

Method: 设计两阶段端到端深度强化学习方法，第一阶段在金字塔楼梯地形训练，第二阶段将策略迁移到真实室内楼梯训练。

Result: 实现从抽象地形到真实室内楼梯拓扑的爬楼梯技能转移，基于中心线的导航公式实现统一学习，仅用局部高度图感知实现策略泛化，对不同难度楼梯进行实证分析。

Conclusion: 该两阶段端到端RL方法能让四足机器人适应不同楼梯形状，平衡导航和运动。

Abstract: Quadruped robots are used for primary searches during the early stages of indoor fires. A typical primary search involves quickly and thoroughly looking for victims under hazardous conditions and monitoring flammable materials. However, situational awareness in complex indoor environments and rapid stair climbing across different staircases remain the main challenges for robot-assisted primary searches. In this project, we designed a two-stage end-to-end deep reinforcement learning (RL) approach to optimize both navigation and locomotion. In the first stage, the quadrupeds, Unitree Go2, were trained to climb stairs in Isaac Lab's pyramid-stair terrain. In the second stage, the quadrupeds were trained to climb various realistic indoor staircases in the Isaac Lab engine, with the learned policy transferred from the previous stage. These indoor staircases are straight, L-shaped, and spiral, to support climbing tasks in complex environments. This project explores how to balance navigation and locomotion and how end-to-end RL methods can enable quadrupeds to adapt to different stair shapes. Our main contributions are: (1) A two-stage end-to-end RL framework that transfers stair-climbing skills from abstract pyramid terrain to realistic indoor stair topologies. (2) A centerline-based navigation formulation that enables unified learning of navigation and locomotion without hierarchical planning. (3) Demonstration of policy generalization across diverse staircases using only local height-map perception. (4) An empirical analysis of success, efficiency, and failure modes under increasing stair difficulty.

</details>


### [571] [RDT2: Exploring the Scaling Limit of UMI Data Towards Zero-Shot Cross-Embodiment Generalization](https://arxiv.org/abs/2602.03310)
*Songming Liu,Bangguo Li,Kai Ma,Lingxuan Wu,Hengkai Tan,Xiao Ouyang,Hang Su,Jun Zhu*

Main category: cs.RO

TL;DR: 提出机器人基础模型RDT2，用新训练方法实现零样本泛化并在下游任务表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决Vision - Language - Action (VLA)模型存在的数据稀缺、架构效率低和跨硬件平台泛化能力不足的问题。

Method: 收集超10000小时开源数据集，使用增强的UMI；采用三阶段训练方法，通过RVQ、流匹配和蒸馏实现实时推理。

Result: RDT2能零样本泛化到未见物体、场景、指令和机器人平台，在灵巧、长视野和动态下游任务中超越现有基线。

Conclusion: RDT2是一个有效的机器人基础模型，为通用机器人发展提供了新方案。

Abstract: Vision-Language-Action (VLA) models hold promise for generalist robotics but currently struggle with data scarcity, architectural inefficiencies, and the inability to generalize across different hardware platforms. We introduce RDT2, a robotic foundation model built upon a 7B parameter VLM designed to enable zero-shot deployment on novel embodiments for open-vocabulary tasks. To achieve this, we collected one of the largest open-source robotic datasets--over 10,000 hours of demonstrations in diverse families--using an enhanced, embodiment-agnostic Universal Manipulation Interface (UMI). Our approach employs a novel three-stage training recipe that aligns discrete linguistic knowledge with continuous control via Residual Vector Quantization (RVQ), flow-matching, and distillation for real-time inference. Consequently, RDT2 becomes one of the first models that simultaneously zero-shot generalizes to unseen objects, scenes, instructions, and even robotic platforms. Besides, it outperforms state-of-the-art baselines in dexterous, long-horizon, and dynamic downstream tasks like playing table tennis. See https://rdt-robotics.github.io/rdt2/ for more information.

</details>


### [572] [HMVLA: Hyperbolic Multimodal Fusion for Vision-Language-Action Models](https://arxiv.org/abs/2602.02533)
*Kun Wang,Xiao Feng,Mingcheng Qu,Tonghua Su*

Main category: cs.RO

TL;DR: 本文提出HMVLA框架，利用视觉和语言的层次结构进行语义对齐，在双曲空间嵌入特征并引入稀疏门控专家混合机制，实验显示其在准确性、泛化性和鲁棒性上优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言动作（VLA）模型在直接微调预训练视觉语言模型时，未充分解决VLA领域独特的语义对齐挑战。

Method: 提出HMVLA框架，在双曲空间嵌入多模态特征，引入稀疏门控专家混合（MoE）机制进行语义对齐。

Result: HMVLA在准确性和泛化性上超越基线方法，通过重建数据集验证了其鲁棒性和跨域适应性。

Conclusion: HMVLA框架在解决VLA领域语义对齐问题上表现出色，具有更好的性能和适应性。

Abstract: Vision Language Action (VLA) models have recently shown great potential in bridging multimodal perception with robotic control. However, existing methods often rely on direct fine-tuning of pre-trained Vision-Language Models (VLMs), feeding semantic and visual features directly into a policy network without fully addressing the unique semantic alignment challenges in the VLA domain. In this paper, we propose HMVLA, a novel VLA framework that exploits the inherent hierarchical structures in vision and language for comprehensive semantic alignment. Unlike traditional methods that perform alignment in Euclidean space, our HMVLA embeds multimodal features in hyperbolic space, enabling more effective modeling of the hierarchical relationships present in image text data. Furthermore, we introduce a sparsely gated Mixture of Experts (MoE) mechanism tailored for semantic alignment, which enhances multimodal comprehension between images and text while improving efficiency. Extensive experiments demonstrate that HMVLA surpasses baseline methods in both accuracy and generalization. In addition, we validate its robustness by reconstructing datasets to further test cross domain adaptability.

</details>


### [573] [CMR: Contractive Mapping Embeddings for Robust Humanoid Locomotion on Unstructured Terrains](https://arxiv.org/abs/2602.03511)
*Qixin Zeng,Hongyin Zhang,Shangke Lyu,Junxi Jin,Donglin Wang,Chao Huang*

Main category: cs.RO

TL;DR: 本文提出CMR框架解决人形机器人在非结构化地形上的鲁棒干扰抑制问题，理论分析观测噪声下的回报差距，实验显示CMR在噪声环境中表现更优。


<details>
  <summary>Details</summary>
Motivation: 人形机器人在非结构化地形上的鲁棒干扰抑制是长期挑战，感知信息受噪声和仿真与现实差距影响，会导致策略不稳定。

Method: 提供理论分析界定观测噪声下的回报差距；提出CMR框架，将高维易受干扰的观测映射到潜在空间，结合对比表示学习和Lipschitz正则化；将其作为辅助损失项融入深度强化学习流程。

Result: 广泛的人形机器人实验表明，CMR在增加噪声的情况下显著优于其他运动算法。

Conclusion: CMR框架能有效解决人形机器人在非结构化地形受噪声干扰时的运动鲁棒性问题。

Abstract: Robust disturbance rejection remains a longstanding challenge in humanoid locomotion, particularly on unstructured terrains where sensing is unreliable and model mismatch is pronounced. While perception information, such as height map, enhances terrain awareness, sensor noise and sim-to-real gaps can destabilize policies in practice. In this work, we provide theoretical analysis that bounds the return gap under observation noise, when the induced latent dynamics are contractive. Furthermore, we present Contractive Mapping for Robustness (CMR) framework that maps high-dimensional, disturbance-prone observations into a latent space, where local perturbations are attenuated over time. Specifically, this approach couples contrastive representation learning with Lipschitz regularization to preserve task-relevant geometry while explicitly controlling sensitivity. Notably, the formulation can be incorporated into modern deep reinforcement learning pipelines as an auxiliary loss term with minimal additional technical effort required. Further, our extensive humanoid experiments show that CMR potently outperforms other locomotion algorithms under increased noise.

</details>


### [574] [IMAGINE: Intelligent Multi-Agent Godot-based Indoor Networked Exploration](https://arxiv.org/abs/2602.02858)
*Tiago Leite,Maria Conceição,António Grilo*

Main category: cs.RO

TL;DR: 本文在2D室内环境中用多智能体强化学习（MARL）实现无人机自主协作探索，通过高保真模拟和消融研究，结果表明可实现室内区域快速自主探索，课程学习助力更高效训练。


<details>
  <summary>Details</summary>
Motivation: 解决自主通信感知协作的无人机群组在未知、无GNSS环境中探索时在协调、感知和分散决策方面的挑战。

Method: 采用MARL，结合高保真游戏引擎模拟（Godot）和连续动作空间，基于ND - POMDPs进行策略训练，每架无人机配备LiDAR传感器并可与邻居共享数据，进行广泛消融研究。

Result: 可实现室内区域快速自主探索，课程学习使训练更快更稳健。

Conclusion: 高保真模拟、MARL公式和计算效率的结合为在物理机器人系统中部署学习到的协作策略奠定基础。

Abstract: The exploration of unknown, Global Navigation Satellite System (GNSS) denied environments by an autonomous communication-aware and collaborative group of Unmanned Aerial Vehicles (UAVs) presents significant challenges in coordination, perception, and decentralized decision-making. This paper implements Multi-Agent Reinforcement Learning (MARL) to address these challenges in a 2D indoor environment, using high-fidelity game-engine simulations (Godot) and continuous action spaces. Policy training aims to achieve emergent collaborative behaviours and decision-making under uncertainty using Network-Distributed Partially Observable Markov Decision Processes (ND-POMDPs). Each UAV is equipped with a Light Detection and Ranging (LiDAR) sensor and can share data (sensor measurements and a local occupancy map) with neighbouring agents. Inter-agent communication constraints include limited range, bandwidth and latency. Extensive ablation studies evaluated MARL training paradigms, reward function, communication system, neural network (NN) architecture, memory mechanisms, and POMDP formulations. This work jointly addresses several key limitations in prior research, namely reliance on discrete actions, single-agent or centralized formulations, assumptions of a priori knowledge and permanent connectivity, inability to handle dynamic obstacles, short planning horizons and architectural complexity in Recurrent NNs/Transformers. Results show that the scalable training paradigm, combined with a simplified architecture, enables rapid autonomous exploration of an indoor area. The implementation of Curriculum-Learning (five increasingly complex levels) also enabled faster, more robust training. This combination of high-fidelity simulation, MARL formulation, and computational efficiency establishes a strong foundation for deploying learned cooperative strategies in physical robotic systems.

</details>


### [575] [Conformal Reachability for Safe Control in Unknown Environments](https://arxiv.org/abs/2602.03799)
*Xinhang Ma,Junlin Wu,Yiannis Kantaros,Yevgeniy Vorobeychik*

Main category: cs.RO

TL;DR: 提出结合共形预测与可达性分析的概率验证框架，为未知动态系统设计安全控制策略，实验表明所学策略有强安全保证且保持高平均奖励。


<details>
  <summary>Details</summary>
Motivation: 以往设计可证明安全控制的工作假设系统动力学已知或确定、状态和动作空间有限，限制了应用范围，需解决此局限。

Method: 开发概率验证框架，用共形预测获取未知动力学的有效不确定性区间，用可达性分析验证安全；开发算法训练控制策略，优化标称奖励并最大化规划时域同时保证概率安全。

Result: 在四个领域的七个安全控制设置中评估，所学策略实现了最强可证明安全保证，同时保持高平均奖励。

Conclusion: 所提出的方法能为未知动态系统设计出既保证安全又有高奖励的控制策略。

Abstract: Designing provably safe control is a core problem in trustworthy autonomy. However, most prior work in this regard assumes either that the system dynamics are known or deterministic, or that the state and action space are finite, significantly limiting application scope. We address this limitation by developing a probabilistic verification framework for unknown dynamical systems which combines conformal prediction with reachability analysis. In particular, we use conformal prediction to obtain valid uncertainty intervals for the unknown dynamics at each time step, with reachability then verifying whether safety is maintained within the conformal uncertainty bounds. Next, we develop an algorithmic approach for training control policies that optimize nominal reward while also maximizing the planning horizon with sound probabilistic safety guarantees. We evaluate the proposed approach in seven safe control settings spanning four domains -- cartpole, lane following, drone control, and safe navigation -- for both affine and nonlinear safety specifications. Our experiments show that the policies we learn achieve the strongest provable safety guarantees while still maintaining high average reward.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [576] [The Innovation Tax: Generative AI Adoption, Productivity Paradox, and Systemic Risk in the U.S. Banking Sector](https://arxiv.org/abs/2602.02607)
*Tatsuru Kikuchi*

Main category: econ.EM

TL;DR: 本文评估美国银行业采用生成式人工智能对生产率和系统性风险的影响，发现‘生产率悖论’及显著溢出效应，存在系统性传染新渠道。


<details>
  <summary>Details</summary>
Motivation: 评估生成式人工智能（GenAI）采用对美国银行业生产率和系统性风险的因果影响。

Method: 使用新数据集，采用动态空间杜宾模型（DSDM）捕捉网络溢出效应，合成双重差分法（SDID）进行因果推断，以2022年11月ChatGPT发布为外生冲击。

Result: 发现‘生产率悖论’，采用AI的银行存在‘实施税’致ROE下降，对小银行影响更大；DSDM分析显示有显著正向溢出效应，美国银行系统‘算法耦合’。

Conclusion: AI驱动决策同步化产生系统性传染新渠道，广泛采用的AI模型技术故障可能引发金融网络相关冲击。

Abstract: This paper evaluates the causal impact of Generative Artificial Intelligence (GenAI) adoption on productivity and systemic risk in the U.S. banking sector. Using a novel dataset linking SEC 10-Q filings to Federal Reserve regulatory data for 809 financial institutions over 2018--2025, we employ two complementary identification strategies: Dynamic Spatial Durbin Models (DSDM) to capture network spillovers and Synthetic Difference-in-Differences (SDID) for causal inference using the November 2022 ChatGPT release as an exogenous shock. Our findings reveal a striking ``Productivity Paradox'': while DSDM estimates show that AI-adopting banks are high performers ($β> 0$), the causal SDID analysis documents a significant ``Implementation Tax'' -- adopting banks experience a 428-basis-point decline in ROE as they absorb GenAI integration costs. This tax falls disproportionately on smaller institutions, with bottom-quartile banks suffering a 517-basis-point ROE decline compared to 129 basis points for larger banks, suggesting that economies of scale provide significant advantages in AI implementation. Most critically, our DSDM analysis reveals significant positive spillovers ($θ= 0.161$ for ROA, $p < 0.01$; $θ= 0.679$ for ROE, $p < 0.05$), with spillovers among large banks reaching $θ= 3.13$ for ROE, indicating that the U.S. banking system is becoming ``algorithmically coupled.'' This synchronization of AI-driven decision-making creates a new channel for systemic contagion: a technical failure in widely-adopted AI models could trigger correlated shocks across the entire financial network.

</details>


### [577] [AI Assisted Economics Measurement From Survey: Evidence from Public Employee Pension Choice](https://arxiv.org/abs/2602.02604)
*Tiancheng Wang,Krishna Sharma*

Main category: econ.EM

TL;DR: 开发利用大语言模型从调查工具提取测量结构的迭代框架，应用于退休计划调查，可指导实证分析和调查设计。


<details>
  <summary>Details</summary>
Motivation: 构建经济测量的有效框架，从调查工具中提取测量结构，明确退休选择相关的经济机制。

Method: 通过软映射将调查项目映射到潜在结构的稀疏分布，聚合响应得到子维度得分，用样本外增量效度测试和判别效度诊断规范分类，利用重叠和冗余诊断进行分类细化和重映射。

Result: 应用于大规模公共雇员退休计划调查，识别出包含行为信号的语义组件，明确了影响退休选择的经济机制。

Conclusion: 该方法为调查工具提供可移植的测量审计，能指导实证分析和调查设计。

Abstract: We develop an iterative framework for economic measurement that leverages large language models to extract measurement structure directly from survey instruments. The approach maps survey items to a sparse distribution over latent constructs through what we term a soft mapping, aggregates harmonized responses into respondent level sub dimension scores, and disciplines the resulting taxonomy through out of sample incremental validity tests and discriminant validity diagnostics. The framework explicitly integrates iteration into the measurement construction process. Overlap and redundancy diagnostics trigger targeted taxonomy refinement and constrained remapping, ensuring that added measurement flexibility is retained only when it delivers stable out of sample performance gains. Applied to a large scale public employee retirement plan survey, the framework identifies which semantic components contain behavioral signal and clarifies the economic mechanisms, such as beliefs versus constraints, that matter for retirement choices. The methodology provides a portable measurement audit of survey instruments that can guide both empirical analysis and survey design.

</details>
