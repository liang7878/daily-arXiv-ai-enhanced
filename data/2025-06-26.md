<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 23]
- [cs.CE](#cs.CE) [Total: 3]
- [cs.DB](#cs.DB) [Total: 2]
- [cs.DC](#cs.DC) [Total: 5]
- [cs.DS](#cs.DS) [Total: 4]
- [cs.GT](#cs.GT) [Total: 2]
- [cs.IR](#cs.IR) [Total: 5]
- [cs.LG](#cs.LG) [Total: 76]
- [cs.NE](#cs.NE) [Total: 2]
- [cs.SE](#cs.SE) [Total: 9]
- [q-fin.ST](#q-fin.ST) [Total: 1]
- [stat.ML](#stat.ML) [Total: 8]
- [q-bio.BM](#q-bio.BM) [Total: 2]
- [hep-ph](#hep-ph) [Total: 1]
- [math.OC](#math.OC) [Total: 3]
- [cs.MA](#cs.MA) [Total: 2]
- [physics.geo-ph](#physics.geo-ph) [Total: 1]
- [eess.AS](#eess.AS) [Total: 2]
- [cs.RO](#cs.RO) [Total: 5]
- [cs.CL](#cs.CL) [Total: 19]
- [cs.CV](#cs.CV) [Total: 16]
- [cs.HC](#cs.HC) [Total: 4]
- [eess.IV](#eess.IV) [Total: 2]
- [cs.CR](#cs.CR) [Total: 14]
- [cs.OS](#cs.OS) [Total: 1]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [eess.SP](#eess.SP) [Total: 1]
- [cs.SD](#cs.SD) [Total: 1]
- [eess.SY](#eess.SY) [Total: 2]
- [cs.GR](#cs.GR) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [econ.GN](#econ.GN) [Total: 3]
- [stat.ME](#stat.ME) [Total: 4]
- [physics.optics](#physics.optics) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.NI](#cs.NI) [Total: 1]
- [physics.med-ph](#physics.med-ph) [Total: 2]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cs.IT](#cs.IT) [Total: 1]
- [quant-ph](#quant-ph) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Prover Agent: An Agent-based Framework for Formal Mathematical Proofs](https://arxiv.org/abs/2506.19923)
*Kaito Baba,Chaoran Liu,Shuhei Kurita,Akiyoshi Sannai*

Main category: cs.AI

TL;DR: 提出Prover Agent用于自动定理证明，结合大语言模型和形式证明助手Lean，在MiniF2F基准上取得86.1%成功率。


<details>
  <summary>Details</summary>
Motivation: 开发新的自动定理证明AI代理。

Method: 整合非正式推理大语言模型、形式证明模型和Lean反馈，生成辅助引理。

Result: 在MiniF2F基准上成功率达86.1%，在使用小语言模型方法中创最新最优，样本预算更低。

Conclusion: Prover Agent有效，生成的引理有助于解决难题。

Abstract: We present Prover Agent, a novel AI agent for automated theorem proving that
integrates large language models (LLMs) with a formal proof assistant, Lean.
Prover Agent coordinates an informal reasoning LLM, a formal prover model, and
feedback from Lean while also generating auxiliary lemmas to assist in
discovering the overall proof strategy. It achieves an 86.1% success rate on
the MiniF2F benchmark, establishing a new state-of-the-art among methods using
small language models (SLMs) with a much lower sample budget than previous
approaches. We also present case studies illustrating how these generated
lemmas contribute to solving challenging problems.

</details>


### [2] [Context Attribution with Multi-Armed Bandit Optimization](https://arxiv.org/abs/2506.19977)
*Deng Pan,Keerthiram Murugesan,Nuno Moniz,Nitesh Chawla*

Main category: cs.AI

TL;DR: 提出将上下文归因建模为CMAB问题的框架，用CTS探索上下文子集空间，比传统方法查询效率高，实验证明用更少查询获有竞争力归因质量。


<details>
  <summary>Details</summary>
Motivation: 理解检索上下文哪些部分对大语言模型生成答案有贡献，对构建可解释、可信的生成式问答系统至关重要。

Method: 将上下文归因建模为CMAB问题，每个上下文片段视为老虎机臂，用CTS在有限查询预算下探索上下文子集空间，定义基于归一化标记似然的奖励函数，自适应平衡探索与利用。

Result: 相比传统基于扰动的归因方法，提高了查询效率，同时保持高归因保真度。

Conclusion: 在不同数据集和大语言模型上的实验表明，该方法能用更少的模型查询实现有竞争力的归因质量。

Abstract: Understanding which parts of the retrieved context contribute to a large
language model's generated answer is essential for building interpretable and
trustworthy generative QA systems. We propose a novel framework that formulates
context attribution as a combinatorial multi-armed bandit (CMAB) problem. Each
context segment is treated as a bandit arm, and we employ Combinatorial
Thompson Sampling (CTS) to efficiently explore the exponentially large space of
context subsets under a limited query budget. Our method defines a reward
function based on normalized token likelihoods, capturing how well a subset of
segments supports the original model response. Unlike traditional
perturbation-based attribution methods such as SHAP, which sample subsets
uniformly and incur high computational costs, our approach adaptively balances
exploration and exploitation by leveraging posterior estimates of segment
relevance. This leads to substantially improved query efficiency while
maintaining high attribution fidelity. Extensive experiments on diverse
datasets and LLMs demonstrate that our method achieves competitive attribution
quality with fewer model queries.

</details>


### [3] [QHackBench: Benchmarking Large Language Models for Quantum Code Generation Using PennyLane Hackathon Challenges](https://arxiv.org/abs/2506.20008)
*Abdul Basit,Minghao Shao,Haider Asif,Nouhaila Innan,Muhammad Kashif,Alberto Marchisio,Muhammad Shafique*

Main category: cs.AI

TL;DR: 本文用QHack真实挑战对基于PennyLane的大语言模型量子代码生成进行基准测试，引入QHackBench数据集，评估不同提示方法下模型性能，提出多智能体评估流程，并将相关资源公开。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在量子计算代码生成方面的有效性尚未充分探索，需进行相关研究。

Method: 引入QHackBench数据集，在普通提示和检索增强生成（RAG）下评估模型，用结构化评估框架评估模型，提出多智能体评估流程。

Result: RAG增强模型在复杂量子算法中与标准提示生成结果相近，多智能体评估流程提高执行成功率。

Conclusion: 公开QHackBench、评估框架和实验结果，促进人工智能辅助量子编程的进一步发展。

Abstract: Recent advances in Large Language Models (LLMs) have demonstrated strong
potential in code generation, yet their effectiveness in quantum computing
remains underexplored. This paper benchmarks LLMs for PennyLane-based quantum
code generation using real-world challenges from the Quantum Hackathon (QHack).
We introduce QHackBench, a novel benchmark dataset derived from QHack
competitions, and evaluate model performance under vanilla prompting and
Retrieval-Augmented Generation (RAG). Our structured evaluation framework
assesses functional correctness, syntactic validity, and execution success
across varying challenge difficulties. Results indicate that RAG-enhanced
models, supplemented with an augmented PennyLane dataset, approximately
generate similar results as the standard prompting, particularly in complex
quantum algorithms. Additionally, we introduce a multi-agent evaluation
pipeline that iteratively refines incorrect solutions, further enhancing
execution success rates. To foster further research, we commit to publicly
releasing QHackBench, along with our evaluation framework and experimental
results, enabling continued advancements in AI-assisted quantum programming.

</details>


### [4] [Accurate and Energy Efficient: Local Retrieval-Augmented Generation Models Outperform Commercial Large Language Models in Medical Tasks](https://arxiv.org/abs/2506.20009)
*Konstantinos Vrettos,Michail E. Klontzas*

Main category: cs.AI

TL;DR: 本文开发可定制医疗RAG框架，对比模型性能与能耗，发现定制RAG模型在医疗任务中表现优于商业模型且环境影响小。


<details>
  <summary>Details</summary>
Motivation: 人工智能在医疗领域应用引发对环境和伦理影响的关注，商业大语言模型资源消耗大且存在患者隐私和安全问题。

Method: 开发可定制的医疗任务RAG框架，监测能源使用和二氧化碳排放，基于不同开源大语言模型创建RAG，与商业模型对比，用医疗问题数据集评估。

Result: 定制RAG模型在准确性和能耗上优于商业模型，基于llama3.1:8B的RAG模型准确性最高、能耗最低。

Conclusion: 本地大语言模型可开发出在医疗任务中优于商业在线大语言模型的RAG，减少电力使用，促进可持续人工智能发展。

Abstract: Background The increasing adoption of Artificial Intelligence (AI) in
healthcare has sparked growing concerns about its environmental and ethical
implications. Commercial Large Language Models (LLMs), such as ChatGPT and
DeepSeek, require substantial resources, while the utilization of these systems
for medical purposes raises critical issues regarding patient privacy and
safety. Methods We developed a customizable Retrieval-Augmented Generation
(RAG) framework for medical tasks, which monitors its energy usage and CO2
emissions. This system was then used to create RAGs based on various
open-source LLMs. The tested models included both general purpose models like
llama3.1:8b and medgemma-4b-it, which is medical-domain specific. The best RAGs
performance and energy consumption was compared to DeepSeekV3-R1 and OpenAIs
o4-mini model. A dataset of medical questions was used for the evaluation.
Results Custom RAG models outperformed commercial models in accuracy and energy
consumption. The RAG model built on llama3.1:8B achieved the highest accuracy
(58.5%) and was significantly better than other models, including o4-mini and
DeepSeekV3-R1. The llama3.1-RAG also exhibited the lowest energy consumption
and CO2 footprint among all models, with a Performance per kWh of 0.52 and a
total CO2 emission of 473g. Compared to o4-mini, the llama3.1-RAG achieved 2.7x
times more accuracy points per kWh and 172% less electricity usage while
maintaining higher accuracy. Conclusion Our study demonstrates that local LLMs
can be leveraged to develop RAGs that outperform commercial, online LLMs in
medical tasks, while having a smaller environmental impact. Our modular
framework promotes sustainable AI development, reducing electricity usage and
aligning with the UNs Sustainable Development Goals.

</details>


### [5] [Achieving Trustworthy Real-Time Decision Support Systems with Low-Latency Interpretable AI Models](https://arxiv.org/abs/2506.20018)
*Zechun Deng,Ziwei Liu,Ziqian Bi,Junhao Song,Chia Xin Liang,Joe Yeong,Junfeng Hao*

Main category: cs.AI

TL;DR: 本文研究利用低延迟AI模型的实时决策支持系统，探讨大语言模型辅助决策、技术发展影响，通过综述给出开发策略和应用领域的实用观点，为未来突破奠定基础。


<details>
  <summary>Details</summary>
Motivation: 研究利用低延迟AI模型的实时决策支持系统，解决资源有限等问题，探索AI在实时决策支持中的应用。

Method: 对相关技术和发展进行详细综述。

Result: 提供了开发策略和应用领域的实用观点。

Conclusion: 为该快速变化领域的未来突破奠定基础，强调AI可重塑实时决策支持。

Abstract: This paper investigates real-time decision support systems that leverage
low-latency AI models, bringing together recent progress in holistic AI-driven
decision tools, integration with Edge-IoT technologies, and approaches for
effective human-AI teamwork. It looks into how large language models can assist
decision-making, especially when resources are limited. The research also
examines the effects of technical developments such as DeLLMa, methods for
compressing models, and improvements for analytics on edge devices, while also
addressing issues like limited resources and the need for adaptable frameworks.
Through a detailed review, the paper offers practical perspectives on
development strategies and areas of application, adding to the field by
pointing out opportunities for more efficient and flexible AI-supported
systems. The conclusions set the stage for future breakthroughs in this
fast-changing area, highlighting how AI can reshape real-time decision support.

</details>


### [6] [Persona-Assigned Large Language Models Exhibit Human-Like Motivated Reasoning](https://arxiv.org/abs/2506.20020)
*Saloni Dash,Amélie Reymond,Emma S. Spiro,Aylin Caliskan*

Main category: cs.AI

TL;DR: 研究发现赋予角色的大语言模型会出现类似人类的动机性推理，且传统去偏提示难以缓解。


<details>
  <summary>Details</summary>
Motivation: 人类推理易受潜在动机影响产生偏差，集体层面的动机性推理会危害社会，此前研究虽表明大语言模型也有认知偏差，但角色对其动机性推理的影响尚待探索。

Method: 对8种大语言模型赋予8种涵盖4种政治和社会人口属性的角色，进行两项人类主体研究中的推理任务测试。

Result: 赋予角色的大语言模型真实性辨别能力最多降低9%；政治角色在科学证据与诱导的政治身份一致时，正确评估枪支管制科学证据的可能性最多高90%；基于提示的去偏方法基本无效。

Conclusion: 赋予角色的大语言模型会出现类似人类的动机性推理，且传统去偏提示难以缓解，这可能加剧大语言模型和人类的身份一致性推理。

Abstract: Reasoning in humans is prone to biases due to underlying motivations like
identity protection, that undermine rational decision-making and judgment. This
motivated reasoning at a collective level can be detrimental to society when
debating critical issues such as human-driven climate change or vaccine safety,
and can further aggravate political polarization. Prior studies have reported
that large language models (LLMs) are also susceptible to human-like cognitive
biases, however, the extent to which LLMs selectively reason toward
identity-congruent conclusions remains largely unexplored. Here, we investigate
whether assigning 8 personas across 4 political and socio-demographic
attributes induces motivated reasoning in LLMs. Testing 8 LLMs (open source and
proprietary) across two reasoning tasks from human-subject studies -- veracity
discernment of misinformation headlines and evaluation of numeric scientific
evidence -- we find that persona-assigned LLMs have up to 9% reduced veracity
discernment relative to models without personas. Political personas
specifically, are up to 90% more likely to correctly evaluate scientific
evidence on gun control when the ground truth is congruent with their induced
political identity. Prompt-based debiasing methods are largely ineffective at
mitigating these effects. Taken together, our empirical findings are the first
to suggest that persona-assigned LLMs exhibit human-like motivated reasoning
that is hard to mitigate through conventional debiasing prompts -- raising
concerns of exacerbating identity-congruent reasoning in both LLMs and humans.

</details>


### [7] [DiaLLMs: EHR Enhanced Clinical Conversational System for Clinical Test Recommendation and Diagnosis Prediction](https://arxiv.org/abs/2506.20059)
*Weijieying Ren,Tianxiang Zhao,Lei Wang,Tianchun Wang,Vasant Honavar*

Main category: cs.AI

TL;DR: 提出医疗大语言模型DiaLLM，集成电子健康记录数据，实验显示其在临床测试推荐和诊断预测上优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有医疗大语言模型忽视电子健康记录作用且主要关注诊断推荐，临床适用性受限。

Method: 设计临床测试参考策略构建基于临床的对话；采用强化学习框架进行证据获取和自动诊断；引入拒绝采样策略；设计确认奖励和类别敏感诊断奖励。

Result: DiaLLM在临床测试推荐和诊断预测上优于基线模型。

Conclusion: DiaLLM通过集成电子健康记录数据到临床对话，能更好地符合实际医疗实践，在相关任务中有更好表现。

Abstract: Recent advances in Large Language Models (LLMs) have led to remarkable
progresses in medical consultation. However, existing medical LLMs overlook the
essential role of Electronic Health Records (EHR) and focus primarily on
diagnosis recommendation, limiting their clinical applicability. We propose
DiaLLM, the first medical LLM that integrates heterogeneous EHR data into
clinically grounded dialogues, enabling clinical test recommendation, result
interpretation, and diagnosis prediction to better align with real-world
medical practice. To construct clinically grounded dialogues from EHR, we
design a Clinical Test Reference (CTR) strategy that maps each clinical code to
its corresponding description and classifies test results as "normal" or
"abnormal". Additionally, DiaLLM employs a reinforcement learning framework for
evidence acquisition and automated diagnosis. To handle the large action space,
we introduce a reject sampling strategy to reduce redundancy and improve
exploration efficiency. Furthermore, a confirmation reward and a
class-sensitive diagnosis reward are designed to guide accurate diagnosis
prediction. Extensive experimental results demonstrate that DiaLLM outperforms
baselines in clinical test recommendation and diagnosis prediction.

</details>


### [8] [AI Copilots for Reproducibility in Science: A Case Study](https://arxiv.org/abs/2506.20130)
*Adrien Bibal,Steven N. Minton,Deborah Khider,Yolanda Gil*

Main category: cs.AI

TL;DR: 介绍AI平台OpenPub及其可重复性副驾驶，测试显示能大幅减少复现时间、检测复现障碍，表明AI工具可减轻复现负担，架构利于拓展。


<details>
  <summary>Details</summary>
Motivation: 解决开放科学中已发表研究结果难以独立复现的问题。

Method: 介绍OpenPub平台及可重复性副驾驶，用已知复现基准的研究论文进行可行性测试。

Result: OpenPub可将复现时间从超30小时减至约1小时，高覆盖适合计算复现的内容，能系统检测复现障碍。

Conclusion: AI驱动工具可减轻复现负担，促进科学交流，模块化副驾驶架构利于拓展到其他开放科学目标。

Abstract: Open science initiatives seek to make research outputs more transparent,
accessible, and reusable, but ensuring that published findings can be
independently reproduced remains a persistent challenge. This paper introduces
OpenPub, an AI-powered platform that supports researchers, reviewers, and
readers through a suite of modular copilots focused on key open science tasks.
In this work, we present the Reproducibility Copilot, which analyzes
manuscripts, code, and supplementary materials to generate structured Jupyter
Notebooks and recommendations aimed at facilitating computational, or "rote",
reproducibility. We conducted feasibility tests using previously studied
research papers with known reproducibility benchmarks. Results indicate that
OpenPub can substantially reduce reproduction time - from over 30 hours to
about 1 hour - while achieving high coverage of figures, tables, and results
suitable for computational reproduction. The system systematically detects
barriers to reproducibility, including missing hyperparameters, undocumented
preprocessing steps, and incomplete or inaccessible datasets. These findings
suggest that AI-driven tools can meaningfully reduce the burden of
reproducibility efforts and contribute to more transparent and verifiable
scientific communication. The modular copilot architecture also provides a
foundation for extending AI assistance to additional open science objectives
beyond reproducibility.

</details>


### [9] [Language Modeling by Language Models](https://arxiv.org/abs/2506.20249)
*Junyan Cheng,Peter Clark,Kyle Richardson*

Main category: cs.AI

TL;DR: 提出多智能体大语言模型方法Genesys探索新型语言模型架构，用遗传编程骨干提高设计生成效率，发现众多新设计且表现出色，并进行系统消融和理论分析。


<details>
  <summary>Details</summary>
Motivation: 利用大语言模型来模拟发现新型语言模型架构的研究过程。

Method: 提出多智能体大语言模型方法，采用梯级尺度方法，使用新型遗传编程骨干。

Result: 实验发现1162个新设计，1062个通过预训练验证，最佳设计在多个基准测试中表现优于已知架构。

Conclusion: 实验结果结合系统消融和理论分析，为有效自主发现系统的设计提供了更广泛的见解。

Abstract: Can we leverage LLMs to model the process of discovering novel language model
(LM) architectures? Inspired by real research, we propose a multi-agent LLM
approach that simulates the conventional stages of research, from ideation and
literature search (proposal stage) to design implementation (code generation),
generative pre-training, and downstream evaluation (verification). Using ideas
from scaling laws, our system, Genesys, employs a Ladder of Scales approach;
new designs are proposed, adversarially reviewed, implemented, and selectively
verified at increasingly larger model scales (14M$\sim$350M parameters) with a
narrowing budget (the number of models we can train at each scale). To help
make discovery efficient and factorizable, Genesys uses a novel genetic
programming backbone, which we show has empirical advantages over commonly used
direct prompt generation workflows (e.g., $\sim$86\% percentage point
improvement in successful design generation, a key bottleneck). We report
experiments involving 1,162 newly discovered designs (1,062 fully verified
through pre-training) and find the best designs to be highly competitive with
known architectures (e.g., outperform GPT2, Mamba2, etc., on 6/9 common
benchmarks). We couple these results with comprehensive system-level ablations
and formal results, which give broader insights into the design of effective
autonomous discovery systems.

</details>


### [10] [Enterprise Large Language Model Evaluation Benchmark](https://arxiv.org/abs/2506.20274)
*Liya Wang,David Yi,Damien Jose,John Passarelli,James Gao,Jordan Leventis,Kang Li*

Main category: cs.AI

TL;DR: 提出14任务框架评估大语言模型在企业场景能力，构建9700样本基准测试，评估显示开源模型在推理任务可与闭源竞争但判断场景有差距，为企业评估和模型部署提供思路。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试如MMLU无法充分评估大语言模型在企业特定任务的复杂能力。

Method: 基于布鲁姆分类法提出14任务框架，开发结合LLM - as - a - Labeler、LLM - as - a - Judge和CRAG的可扩展流程来构建基准测试。

Result: 评估六个领先模型，开源模型如DeepSeek R1在推理任务可与闭源竞争，但在判断场景因过度思考而落后。

Conclusion: 基准测试揭示企业性能差距，为模型优化提供见解，为企业定制评估提供蓝图，推动大语言模型实际部署。

Abstract: Large Language Models (LLMs) ) have demonstrated promise in boosting
productivity across AI-powered tools, yet existing benchmarks like Massive
Multitask Language Understanding (MMLU) inadequately assess enterprise-specific
task complexities. We propose a 14-task framework grounded in Bloom's Taxonomy
to holistically evaluate LLM capabilities in enterprise contexts. To address
challenges of noisy data and costly annotation, we develop a scalable pipeline
combining LLM-as-a-Labeler, LLM-as-a-Judge, and corrective retrieval-augmented
generation (CRAG), curating a robust 9,700-sample benchmark. Evaluation of six
leading models shows open-source contenders like DeepSeek R1 rival proprietary
models in reasoning tasks but lag in judgment-based scenarios, likely due to
overthinking. Our benchmark reveals critical enterprise performance gaps and
offers actionable insights for model optimization. This work provides
enterprises a blueprint for tailored evaluations and advances practical LLM
deployment.

</details>


### [11] [Mobile-R1: Towards Interactive Reinforcement Learning for VLM-Based Mobile Agent via Task-Level Rewards](https://arxiv.org/abs/2506.20332)
*Jihao Gu,Qihang Ai,Yingyao Wang,Pi Bu,Jingxuan Xing,Zekun Zhu,Wei Jiang,Ziming Wang,Yingxiu Zhao,Ming-Liang Zhang,Jun Song,Yuning Jiang,Bo Zheng*

Main category: cs.AI

TL;DR: 提出Mobile - R1方法，采用带任务级奖励的交互式多轮强化学习，提升移动智能体探索和纠错能力，还收集数据集并建立新基准且开源资源。


<details>
  <summary>Details</summary>
Motivation: 现有研究局限于离线强化学习训练或使用动作级奖励进行在线优化，导致智能体陷入局部最优，削弱探索和纠错能力。

Method: 提出Mobile - R1方法，训练框架分初始格式微调、基于动作级奖励的单步在线训练、基于多轮轨迹的任务级奖励在线训练三个阶段。

Result: 提升了Mobile - R1的探索和纠错能力，带来显著性能提升，收集了涵盖28个中文应用、24521条高质量人工标注的数据集，建立了含500条轨迹的新基准。

Conclusion: 所提方法有效提升移动智能体性能，开源资源利于后续研究。

Abstract: Vision-language model-based mobile agents have gained the ability to not only
understand complex instructions and mobile screenshots, but also optimize their
action outputs via thinking and reasoning, benefiting from reinforcement
learning, such as Group Relative Policy Optimization (GRPO). However, existing
research centers on offline reinforcement learning training or online
optimization using action-level rewards, which limits the agent's dynamic
interaction with the environment. This often results in agents settling into
local optima, thereby weakening their ability for exploration and error action
correction. To address these challenges, we introduce an approach called
Mobile-R1, which employs interactive multi-turn reinforcement learning with
task-level rewards for mobile agents. Our training framework consists of three
stages: initial format finetuning, single-step online training via action-level
reward, followed by online training via task-level reward based on multi-turn
trajectories. This strategy is designed to enhance the exploration and error
correction capabilities of Mobile-R1, leading to significant performance
improvements. Moreover, we have collected a dataset covering 28 Chinese
applications with 24,521 high-quality manual annotations and established a new
benchmark with 500 trajectories. We will open source all resources, including
the dataset, benchmark, model weight, and codes:
https://mobile-r1.github.io/Mobile-R1/.

</details>


### [12] [Tabular Feature Discovery With Reasoning Type Exploration](https://arxiv.org/abs/2506.20357)
*Sungwon Han,Sungkyu Park,Seungeon Lee*

Main category: cs.AI

TL;DR: 本文提出REFeat方法，利用多种推理引导大语言模型为表格数据生成特征，实验表明该方法能提升预测准确性并发现更多样有意义的特征。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的表格数据特征工程方法生成的特征过于简单或重复，原因在于模型变换存在固有偏差且生成时缺乏结构化推理指导。

Method: 提出REFeat方法，利用多种推理引导大语言模型发现多样化且有信息价值的特征。

Result: 在59个基准数据集上的实验显示，该方法平均预测准确性更高，且能发现更多样、有意义的特征。

Conclusion: 将丰富的推理范式和自适应策略选择融入大语言模型驱动的表格数据特征发现有前景。

Abstract: Feature engineering for tabular data remains a critical yet challenging step
in machine learning. Recently, large language models (LLMs) have been used to
automatically generate new features by leveraging their vast knowledge.
However, existing LLM-based approaches often produce overly simple or
repetitive features, partly due to inherent biases in the transformations the
LLM chooses and the lack of structured reasoning guidance during generation. In
this paper, we propose a novel method REFeat, which guides an LLM to discover
diverse and informative features by leveraging multiple types of reasoning to
steer the feature generation process. Experiments on 59 benchmark datasets
demonstrate that our approach not only achieves higher predictive accuracy on
average, but also discovers more diverse and meaningful features. These results
highlight the promise of incorporating rich reasoning paradigms and adaptive
strategy selection into LLM-driven feature discovery for tabular data.

</details>


### [13] [Paladin-mini: A Compact and Efficient Grounding Model Excelling in Real-World Scenarios](https://arxiv.org/abs/2506.20384)
*Dror Ivry,Oran Nahum*

Main category: cs.AI

TL;DR: 本文提出两项重要成果解决声明接地问题，介绍Paladin - mini模型和接地基准数据集，并展示模型基准测试结果。


<details>
  <summary>Details</summary>
Motivation: 解决在给定上下文中声明接地的问题。

Method: 引入38亿参数的开源分类器模型Paladin - mini和用于评估关键推理任务性能的接地基准数据集。

Result: 对Paladin - mini进行基准测试并与现有最先进技术对比，得到清晰可复现的结果。

Conclusion: 未明确提及，但暗示所提出的模型和数据集有助于解决声明接地问题。

Abstract: This paper introduces two significant contributions to address the issue of
grounding claims in a given context. Grounding means that given a context
(document) and a claim, there's at least one supportive evidence for the claim
in the document. We will introduce Paladin-mini, a compact (3.8B parameters)
open-source classifier model (used for labeling data as grounded or ungrounded)
engineered for robust performance in real-world scenarios, and the
grounding-benchmark, a new evaluation dataset designed to assess performance on
critical reasoning tasks. We'll also demonstrate the results of Paladin-mini
with benchmarks against the current State-of-the-art and share clear and
reproducible results.

</details>


### [14] [Smart Ride and Delivery Services with Electric Vehicles: Leveraging Bidirectional Charging for Profit Optimisation](https://arxiv.org/abs/2506.20401)
*Jinchun Du,Bojie Shen,Muhammad Aamir Cheema,Adel N. Toosi*

Main category: cs.AI

TL;DR: 文章提出EVOP - V2G问题，用MIP模型和两种元启发式算法求解，实验表明方法可提升司机利润，助力智能盈利的EV出行系统。


<details>
  <summary>Details</summary>
Motivation: 随着电动汽车普及，在其融入现代服务系统时，因续航短及V2G技术带来新机遇和复杂性，需解决利润最大化问题。

Method: 将问题建模为MIP模型，提出基于进化算法（EA）和大邻域搜索（LNS）的两种近最优元启发式算法。

Result: 在真实数据实验中，所提方法比基线方法使司机利润翻倍，小实例接近最优解，大实例有良好可扩展性。

Conclusion: 研究为支持电网的智能、盈利EV出行系统指明了有前景的方向。

Abstract: With the rising popularity of electric vehicles (EVs), modern service
systems, such as ride-hailing delivery services, are increasingly integrating
EVs into their operations. Unlike conventional vehicles, EVs often have a
shorter driving range, necessitating careful consideration of charging when
fulfilling requests. With recent advances in Vehicle-to-Grid (V2G) technology -
allowing EVs to also discharge energy back to the grid - new opportunities and
complexities emerge. We introduce the Electric Vehicle Orienteering Problem
with V2G (EVOP-V2G): a profit-maximization problem where EV drivers must select
customer requests or orders while managing when and where to charge or
discharge. This involves navigating dynamic electricity prices, charging
station selection, and route constraints. We formulate the problem as a Mixed
Integer Programming (MIP) model and propose two near-optimal metaheuristic
algorithms: one evolutionary (EA) and the other based on large neighborhood
search (LNS). Experiments on real-world data show our methods can double driver
profits compared to baselines, while maintaining near-optimal performance on
small instances and excellent scalability on larger ones. Our work highlights a
promising path toward smarter, more profitable EV-based mobility systems that
actively support the energy grid.

</details>


### [15] [GymPN: A Library for Decision-Making in Process Management Systems](https://arxiv.org/abs/2506.20404)
*Riccardo Lo Bianco,Willem van Jaarsveld,Remco Dijkman*

Main category: cs.AI

TL;DR: 本文提出软件库GymPN，用深度强化学习支持业务流程最优决策，在典型问题模式上评估显示其建模便捷且能学习最优决策策略。


<details>
  <summary>Details</summary>
Motivation: 业务流程管理系统做工作分配决策需合适软件工具支持，以往工作有局限性。

Method: 提出软件库GymPN，基于先前工作引入支持部分流程可观测性和多决策建模两个创新点。

Result: 在八个典型业务流程决策问题模式上评估，表明GymPN便于建模且能学习最优决策策略。

Conclusion: GymPN解决了先前工作的基本局限性，能实现更现实的流程决策表示。

Abstract: Process management systems support key decisions about the way work is
allocated in organizations. This includes decisions on which task to perform
next, when to execute the task, and who to assign the task to. Suitable
software tools are required to support these decisions in a way that is optimal
for the organization. This paper presents a software library, called GymPN,
that supports optimal decision-making in business processes using Deep
Reinforcement Learning. GymPN builds on previous work that supports task
assignment in business processes, introducing two key novelties: support for
partial process observability and the ability to model multiple decisions in a
business process. These novel elements address fundamental limitations of
previous work and thus enable the representation of more realistic process
decisions. We evaluate the library on eight typical business process
decision-making problem patterns, showing that GymPN allows for easy modeling
of the desired problems, as well as learning optimal decision policies.

</details>


### [16] [Mixtures of Neural Cellular Automata: A Stochastic Framework for Growth Modelling and Self-Organization](https://arxiv.org/abs/2506.20486)
*Salvatore Milite,Giulio Caravagna,Andrea Sottoriva*

Main category: cs.AI

TL;DR: 提出混合神经细胞自动机（MNCA）框架，评估其在三个领域有效性，结果表明MNCA建模有优势。


<details>
  <summary>Details</summary>
Motivation: 传统神经细胞自动机（NCAs）的确定性限制了其捕捉现实世界生物和物理系统随机性的能力，需要新方法。

Method: 将混合模型思想融入NCA范式，结合概率规则分配和内在噪声构建MNCA。

Result: MNCA在组织生长和分化模拟、图像形态发生鲁棒性、显微镜图像分割三个领域，表现出对扰动的优越鲁棒性，能更好重现生物生长模式，提供可解释的规则分割。

Conclusion: MNCA是建模随机动力系统和研究自我生长过程的有前途工具。

Abstract: Neural Cellular Automata (NCAs) are a promising new approach to model
self-organizing processes, with potential applications in life science.
However, their deterministic nature limits their ability to capture the
stochasticity of real-world biological and physical systems.
  We propose the Mixture of Neural Cellular Automata (MNCA), a novel framework
incorporating the idea of mixture models into the NCA paradigm. By combining
probabilistic rule assignments with intrinsic noise, MNCAs can model diverse
local behaviors and reproduce the stochastic dynamics observed in biological
processes.
  We evaluate the effectiveness of MNCAs in three key domains: (1) synthetic
simulations of tissue growth and differentiation, (2) image morphogenesis
robustness, and (3) microscopy image segmentation. Results show that MNCAs
achieve superior robustness to perturbations, better recapitulate real
biological growth patterns, and provide interpretable rule segmentation. These
findings position MNCAs as a promising tool for modeling stochastic dynamical
systems and studying self-growth processes.

</details>


### [17] [Engineering Sentience](https://arxiv.org/abs/2506.20504)
*Konstantin Demin,Taylor Webb,Eric Elmoznino,Hakwan Lau*

Main category: cs.AI

TL;DR: 提出对机器设计和构建有意义的感知定义，阐述其条件和实现方式及意义。


<details>
  <summary>Details</summary>
Motivation: 为在机器中设计和构建感知提供有用的定义。

Method: 以功能和计算的角度详细阐述感知定义，提出感知发生需特定感官信号具备断言性和定性。

Result: 给出感知的具体定义，并给出基于当前技术的潜在实现方式。

Conclusion: 理解人工主体功能性感知的条件有助于避免无意中创造它们或及时察觉已创造它们。

Abstract: We spell out a definition of sentience that may be useful for designing and
building it in machines. We propose that for sentience to be meaningful for AI,
it must be fleshed out in functional, computational terms, in enough detail to
allow for implementation. Yet, this notion of sentience must also reflect
something essentially 'subjective', beyond just having the general capacity to
encode perceptual content. For this specific functional notion of sentience to
occur, we propose that certain sensory signals need to be both assertoric
(persistent) and qualitative. To illustrate the definition in more concrete
terms, we sketch out some ways for potential implementation, given current
technology. Understanding what it takes for artificial agents to be
functionally sentient can also help us avoid creating them inadvertently, or at
least, realize that we have created them in a timely manner.

</details>


### [18] [Case-based Reasoning Augmented Large Language Model Framework for Decision Making in Realistic Safety-Critical Driving Scenarios](https://arxiv.org/abs/2506.20531)
*Wenbin Gan,Minh-Son Dao,Koji Zettsu*

Main category: cs.AI

TL;DR: 本文提出CBR - LLM框架用于复杂风险场景的避险机动决策，实验显示该框架提升决策准确性等性能，证明其作为智能驾驶决策支持工具的潜力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型直接应用于自动驾驶存在领域适应、上下文关联和缺乏经验知识等挑战，难以在动态高风险环境做出可靠且可解释决策，因此需解决这一问题。

Method: 提出基于案例推理增强的大语言模型（CBR - LLM）框架，将行车记录仪视频输入的语义场景理解与相关过往驾驶案例检索相结合。

Result: 跨多个开源大语言模型的实验表明，框架提高了决策准确性、理由质量和与人类专家行为的一致性；风险感知提示策略提升不同风险类型下的性能；基于相似度的案例检索在引导上下文学习方面优于随机抽样；案例研究展示框架在现实挑战条件下的鲁棒性。

Conclusion: 该框架有潜力成为智能驾驶系统自适应且值得信赖的决策支持工具。

Abstract: Driving in safety-critical scenarios requires quick, context-aware
decision-making grounded in both situational understanding and experiential
reasoning. Large Language Models (LLMs), with their powerful general-purpose
reasoning capabilities, offer a promising foundation for such decision-making.
However, their direct application to autonomous driving remains limited due to
challenges in domain adaptation, contextual grounding, and the lack of
experiential knowledge needed to make reliable and interpretable decisions in
dynamic, high-risk environments. To address this gap, this paper presents a
Case-Based Reasoning Augmented Large Language Model (CBR-LLM) framework for
evasive maneuver decision-making in complex risk scenarios. Our approach
integrates semantic scene understanding from dashcam video inputs with the
retrieval of relevant past driving cases, enabling LLMs to generate maneuver
recommendations that are both context-sensitive and human-aligned. Experiments
across multiple open-source LLMs show that our framework improves decision
accuracy, justification quality, and alignment with human expert behavior.
Risk-aware prompting strategies further enhance performance across diverse risk
types, while similarity-based case retrieval consistently outperforms random
sampling in guiding in-context learning. Case studies further demonstrate the
framework's robustness in challenging real-world conditions, underscoring its
potential as an adaptive and trustworthy decision-support tool for intelligent
driving systems.

</details>


### [19] [Fine-Tuning and Prompt Engineering of LLMs, for the Creation of Multi-Agent AI for Addressing Sustainable Protein Production Challenges](https://arxiv.org/abs/2506.20598)
*Alexander D. Kalian,Jaewook Lee,Stefan P. Johannesson,Lennart Otte,Christer Hogstrand,Miao Guo*

Main category: cs.AI

TL;DR: 本文提出多智能体AI框架支持可持续蛋白质生产研究，探索微调与提示工程优化方法，提高信息提取性能并开发用户界面。


<details>
  <summary>Details</summary>
Motivation: 全球对可持续蛋白质来源的需求促使需要能快速处理和合成特定领域科学知识的智能工具。

Method: 构建基于RAG的多智能体AI框架，含文献搜索和信息提取代理，探索微调与提示工程两种优化方法。

Result: 两种方法均能提高信息提取代理性能，微调使平均余弦相似度分数提升更大，提示工程统计不确定性更低。

Conclusion: 开发的多智能体AI系统对可持续蛋白质生产研究有支持作用，还探索了额外搜索能力。

Abstract: The global demand for sustainable protein sources has accelerated the need
for intelligent tools that can rapidly process and synthesise domain-specific
scientific knowledge. In this study, we present a proof-of-concept multi-agent
Artificial Intelligence (AI) framework designed to support sustainable protein
production research, with an initial focus on microbial protein sources. Our
Retrieval-Augmented Generation (RAG)-oriented system consists of two GPT-based
LLM agents: (1) a literature search agent that retrieves relevant scientific
literature on microbial protein production for a specified microbial strain,
and (2) an information extraction agent that processes the retrieved content to
extract relevant biological and chemical information. Two parallel
methodologies, fine-tuning and prompt engineering, were explored for agent
optimisation. Both methods demonstrated effectiveness at improving the
performance of the information extraction agent in terms of transformer-based
cosine similarity scores between obtained and ideal outputs. Mean cosine
similarity scores were increased by up to 25%, while universally reaching mean
scores of $\geq 0.89$ against ideal output text. Fine-tuning overall improved
the mean scores to a greater extent (consistently of $\geq 0.94$) compared to
prompt engineering, although lower statistical uncertainties were observed with
the latter approach. A user interface was developed and published for enabling
the use of the multi-agent AI system, alongside preliminary exploration of
additional chemical safety-based search capabilities

</details>


### [20] [CogGen: A Learner-Centered Generative AI Architecture for Intelligent Tutoring with Programming Video](https://arxiv.org/abs/2506.20600)
*Wengxi Li,Roy Pea,Nick Haber,Hari Subramonyam*

Main category: cs.AI

TL;DR: 介绍CogGen架构，将编程视频转化为互动学习体验，技术评估有效，各组件必要，推进AI辅导


<details>
  <summary>Details</summary>
Motivation: 将编程视频转化为交互式、自适应的学习体验，提升基于视频的编程教育

Method: 基于认知学徒制框架，集成学生建模与生成式AI辅导，架构包含按学习目标分割视频、应用策略的对话辅导引擎、用贝叶斯知识追踪的学生模型

Result: 技术评估显示视频分割准确性有效，在各层面有强教学一致性，消融研究证实各组件必要性

Conclusion: 该工作通过结合结构化学生建模和交互式AI对话，推进AI辅导，为提升编程教育提供可扩展方法

Abstract: We introduce CogGen, a learner-centered AI architecture that transforms
programming videos into interactive, adaptive learning experiences by
integrating student modeling with generative AI tutoring based on the Cognitive
Apprenticeship framework. The architecture consists of three components: (1)
video segmentation by learning goals, (2) a conversational tutoring engine
applying Cognitive Apprenticeship strategies, and (3) a student model using
Bayesian Knowledge Tracing to adapt instruction. Our technical evaluation
demonstrates effective video segmentation accuracy and strong pedagogical
alignment across knowledge, method, action, and interaction layers. Ablation
studies confirm the necessity of each component in generating effective
guidance. This work advances AI-powered tutoring by bridging structured student
modeling with interactive AI conversations, offering a scalable approach to
enhancing video-based programming education.

</details>


### [21] [AI Assistants to Enhance and Exploit the PETSc Knowledge Base](https://arxiv.org/abs/2506.20608)
*Barry Smith,Junchao Zhang,Hong Zhang,Lois Curfman McInnes,Murat Keceli,Archit Vasan,Satish Balay,Toby Isaac,Le Chen,Venkatram Vishwanath*

Main category: cs.AI

TL;DR: 本文介绍PETSc团队构建基于大语言模型（LLM）的系统，以激活和利用PETSc知识，分享设计与评估工具的初步经验，目标是建立科学软件知识中心AI框架。


<details>
  <summary>Details</summary>
Motivation: PETSc在三十年发展中积累了丰富但零散的知识，很多知识难以被用户和新开发者获取，需要有效激活和利用这些知识。

Method: 构建结合PETSc内容与定制LLM工具的系统，包括检索增强生成（RAG）、重排序算法和聊天机器人等，还涉及系统架构设计、评估各种LLM和嵌入模型的方法以及用户界面设计。

Result: 利用阿贡国家实验室领导力计算设施资源，分析LLM响应如何增强数值软件开发和使用，初步聚焦可扩展Krylov求解器。

Conclusion: 提出将该系统扩展为强大、不断发展的平台的方向，以推动软件生态系统加速科学发现。

Abstract: Generative AI, especially through large language models (LLMs), is
transforming how technical knowledge can be accessed, reused, and extended.
PETSc, a widely used numerical library for high-performance scientific
computing, has accumulated a rich but fragmented knowledge base over its three
decades of development, spanning source code, documentation, mailing lists,
GitLab issues, Discord conversations, technical papers, and more. Much of this
knowledge remains informal and inaccessible to users and new developers. To
activate and utilize this knowledge base more effectively, the PETSc team has
begun building an LLM-powered system that combines PETSc content with custom
LLM tools -- including retrieval-augmented generation (RAG), reranking
algorithms, and chatbots -- to assist users, support developers, and propose
updates to formal documentation. This paper presents initial experiences
designing and evaluating these tools, focusing on system architecture, using
RAG and reranking for PETSc-specific information, evaluation methodologies for
various LLMs and embedding models, and user interface design. Leveraging the
Argonne Leadership Computing Facility resources, we analyze how LLM responses
can enhance the development and use of numerical software, with an initial
focus on scalable Krylov solvers. Our goal is to establish an extensible
framework for knowledge-centered AI in scientific software, enabling scalable
support, enriched documentation, and enhanced workflows for research and
development. We conclude by outlining directions for expanding this system into
a robust, evolving platform that advances software ecosystems to accelerate
scientific discovery.

</details>


### [22] [Towards Community-Driven Agents for Machine Learning Engineering](https://arxiv.org/abs/2506.20640)
*Sijie Li,Weiwei Sun,Shanda Li,Ameet Talwalkar,Yiming Yang*

Main category: cs.AI

TL;DR: 提出MLE - Live框架和CoMind代理，CoMind在框架和竞赛中表现出色，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的机器学习代理孤立运作，缺乏与研究社区互动，需弥合这一差距。

Method: 引入MLE - Live框架评估代理与模拟Kaggle研究社区交流和利用集体知识的能力，提出CoMind代理。

Result: CoMind在MLE - Live上达到了最先进的性能，在四个正在进行的Kaggle竞赛中平均胜过79.2%的人类竞争对手。

Conclusion: CoMind能在社区环境中有效交流见解和开发新解决方案。

Abstract: Large language model-based machine learning (ML) agents have shown great
promise in automating ML research. However, existing agents typically operate
in isolation on a given research problem, without engaging with the broader
research community, where human researchers often gain insights and contribute
by sharing knowledge. To bridge this gap, we introduce MLE-Live, a live
evaluation framework designed to assess an agent's ability to communicate with
and leverage collective knowledge from a simulated Kaggle research community.
Building on this framework, we propose CoMind, a novel agent that excels at
exchanging insights and developing novel solutions within a community context.
CoMind achieves state-of-the-art performance on MLE-Live and outperforms 79.2%
human competitors on average across four ongoing Kaggle competitions. Our code
is released at https://github.com/comind-ml/CoMind.

</details>


### [23] [The Decrypto Benchmark for Multi-Agent Reasoning and Theory of Mind](https://arxiv.org/abs/2506.20664)
*Andrei Lupu,Timon Willi,Jakob Foerster*

Main category: cs.AI

TL;DR: 本文提出基于游戏的多智能体推理和心智理论基准Decrypto，通过多种实验验证其设计，发现大语言模型游戏能力落后，前沿推理模型特定任务表现不如旧模型。


<details>
  <summary>Details</summary>
Motivation: 现有基准存在范围窄、数据泄露等问题，大语言模型多智能体能力研究不足，需新基准。

Method: 提出Decrypto基准，对前沿大语言模型进行全面实证评估、鲁棒性研究和人机交叉游戏实验，创建经典认知科学实验变体评估关键心智理论能力。

Result: 大语言模型游戏能力落后于人类和简单词嵌入基线，前沿推理模型特定任务表现不如旧模型。

Conclusion: Decrypto填补了当前推理和心智理论评估的关键空白，为更好的人工智能体发展铺平道路。

Abstract: As Large Language Models (LLMs) gain agentic abilities, they will have to
navigate complex multi-agent scenarios, interacting with human users and other
agents in cooperative and competitive settings. This will require new reasoning
skills, chief amongst them being theory of mind (ToM), or the ability to reason
about the "mental" states of other agents. However, ToM and other multi-agent
abilities in LLMs are poorly understood, since existing benchmarks suffer from
narrow scope, data leakage, saturation, and lack of interactivity. We thus
propose Decrypto, a game-based benchmark for multi-agent reasoning and ToM
drawing inspiration from cognitive science, computational pragmatics and
multi-agent reinforcement learning. It is designed to be as easy as possible in
all other dimensions, eliminating confounding factors commonly found in other
benchmarks. To our knowledge, it is also the first platform for designing
interactive ToM experiments.
  We validate the benchmark design through comprehensive empirical evaluations
of frontier LLMs, robustness studies, and human-AI cross-play experiments. We
find that LLM game-playing abilities lag behind humans and simple
word-embedding baselines. We then create variants of two classic cognitive
science experiments within Decrypto to evaluate three key ToM abilities.
Surprisingly, we find that state-of-the-art reasoning models are significantly
worse at those tasks than their older counterparts. This demonstrates that
Decrypto addresses a crucial gap in current reasoning and ToM evaluations, and
paves the path towards better artificial agents.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [24] [DiT-SGCR: Directed Temporal Structural Representation with Global-Cluster Awareness for Ethereum Malicious Account Detection](https://arxiv.org/abs/2506.20123)
*Ye Tian,Liangliang Song,Peng Qian,Yanbin Wang,Jianguo Sun,Yifan Jia*

Main category: cs.CE

TL;DR: 提出DiT - SGCR用于以太坊恶意账户检测，实验显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有以太坊恶意账户检测方法存在无法建模连续交易动态、计算成本高、未考虑交易流方向和账户聚类等问题，需新方法解决。

Method: 提出DiT - SGCR，采用定向时间聚合捕捉动态账户交互，结合可微聚类和图拉普拉斯正则化生成嵌入。

Result: 在三个数据集上实验，DiT - SGCR在所有基准上均优于现有方法，F1分数提升3.62% - 10.83%。

Conclusion: DiT - SGCR能同时编码定向时间动态、全局拓扑和特定集群行为模式，增强账户表示的可区分性和鲁棒性，且具有可扩展性优势。

Abstract: The detection of malicious accounts on Ethereum - the preeminent DeFi
platform - is critical for protecting digital assets and maintaining trust in
decentralized finance. Recent advances highlight that temporal transaction
evolution reveals more attack signatures than static graphs. However, current
methods either fail to model continuous transaction dynamics or incur high
computational costs that limit scalability to large-scale transaction networks.
Furthermore, current methods fail to consider two higher-order behavioral
fingerprints: (1) direction in temporal transaction flows, which encodes money
movement trajectories, and (2) account clustering, which reveals coordinated
behavior of organized malicious collectives. To address these challenges, we
propose DiT-SGCR, an unsupervised graph encoder for malicious account
detection. Specifically, DiT-SGCR employs directional temporal aggregation to
capture dynamic account interactions, then coupled with differentiable
clustering and graph Laplacian regularization to generate high-quality,
low-dimensional embeddings. Our approach simultaneously encodes directional
temporal dynamics, global topology, and cluster-specific behavioral patterns,
thereby enhancing the discriminability and robustness of account
representations. Furthermore, DiT-SGCR bypasses conventional graph propagation
mechanisms, yielding significant scalability advantages. Extensive experiments
on three datasets demonstrate that DiT-SGCR consistently outperforms
state-of-the-art methods across all benchmarks, achieving F1-score improvements
ranging from 3.62% to 10.83%.

</details>


### [25] [Developing Artificial Mechanics Intuitions from Extremely Small Data](https://arxiv.org/abs/2506.20148)
*Jingruo Peng,Shuze Zhu*

Main category: cs.CE

TL;DR: 提出样本可切换训练方法，从少量样本中开发高精度人工力学直觉。


<details>
  <summary>Details</summary>
Motivation: 在人工智能时代，探索如何从少量数据中开发人工力学直觉。

Method: 提出样本可切换训练方法。

Result: 该方法能通过不超过三个样本掌握多个力学问题，模型直觉预测能力随训练样本数非线性增加。

Conclusion: 表明原则上可基于有限样本实现卓越力学直觉，为培养能直观理解和预测材料变形与运动的人工智能提供新视角。

Abstract: Humans can possess good mechanics intuitions by learning from a few examples,
which leads to the question of how to develop artificial mechanics intuitions
that can be learned from small data, as we are eagerly entering the era of
artificial intelligence. We propose in this Letter the sample-switchable
training method, which successfully develops highly-accurate artificial
mechanics intuitions that can master brachistochrone problem, catenary problem,
and large nonlinear deformation problem of elastic plate by learning from no
more than three samples. The model's intuitive prediction ability increases
nonlinearly with respect to the number of training samples, suggesting that
superb mechanics intuitions can be in-principle achieved based on a finite
number of samples, reflecting how human brains form good mechanics intuitions
just by learning a few cases. Our current work presents an alternative
perspective for educating artificial intelligence capable of intuitively
understand and predict how materials deform and move, a scenario that has been
frequently seen in Science-Fiction movies.

</details>


### [26] [Opinion Dynamics with Highly Oscillating Opinions](https://arxiv.org/abs/2506.20472)
*Víctor A. Vargas-Pérez,Jesús Giráldez-Cru,Oscar Cordón*

Main category: cs.CE

TL;DR: 研究意见动态（OD）模型再现高度振荡意见动态的能力，实验表明ATBCR模型最适合捕捉高度振荡意见。


<details>
  <summary>Details</summary>
Motivation: 现有OD模型难以分析现实世界中高度振荡的意见场景，需克服此局限。

Method: 制定优化问题，用进化算法求解，结合定量和定性分析。

Result: 在西班牙移民意见真实数据集实验显示，基于理性和情感更新机制的ATBCR模型最能准确捕捉高度振荡意见。

Conclusion: ATBCR模型在捕捉高度振荡意见方面表现最优。

Abstract: Opinion Dynamics (OD) models are a particular case of Agent-Based Models in
which the evolution of opinions within a population is studied. In most OD
models, opinions evolve as a consequence of interactions between agents, and
the opinion fusion rule defines how those opinions are updated. In consequence,
despite being simplistic, OD models provide an explainable and interpretable
mechanism for understanding the underlying dynamics of opinion evolution.
Unfortunately, existing OD models mainly focus on explaining the evolution of
(usually synthetic) opinions towards consensus, fragmentation, or polarization,
but they usually fail to analyze scenarios of (real-world) highly oscillating
opinions. This work overcomes this limitation by studying the ability of
several OD models to reproduce highly oscillating dynamics. To this end, we
formulate an optimization problem which is further solved using Evolutionary
Algorithms, providing both quantitative results on the performance of the
optimization and qualitative interpretations on the obtained results. Our
experiments on a real-world opinion dataset about immigration from the monthly
barometer of the Spanish Sociological Research Center show that the ATBCR,
based on both rational and emotional mechanisms of opinion update, is the most
accurate OD model for capturing highly oscillating opinions.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [27] [Near Data Processing in Taurus Database](https://arxiv.org/abs/2506.20010)
*Shu Lin,Arunprasad P. Marathe,Per-Ȧke Larson,Chong Chen,Calvin Sun,Paul Lee,Weidong Yu*

Main category: cs.DB

TL;DR: 本文介绍华为云原生数据库系统Taurus的近数据处理（NDP）设计与实现，NDP可减少网络数据传输、释放计算层CPU、降低查询时间，实验显示其有显著效果。


<details>
  <summary>Details</summary>
Motivation: 介绍Taurus系统中近数据处理（NDP）的设计与实现，以及探究其带来的好处。

Method: 描述Taurus中NDP的设计与实现，并使用TPCH基准（100 GB）进行实验。

Result: 22个查询中有18个受益于NDP，数据传输减少63%，CPU时间减少50%；Q15数据传输减少98%，CPU时间减少91%，运行时间减少80%。

Conclusion: NDP可减少网络数据传输、释放计算层CPU容量、降低查询运行时间，提高系统吞吐量。

Abstract: Huawei's cloud-native database system GaussDB for MySQL (also known as
Taurus) stores data in a separate storage layer consisting of a pool of storage
servers. Each server has considerable compute power making it possible to push
data reduction operations (selection, projection, and aggregation) close to
storage. This paper describes the design and implementation of near data
processing (NDP) in Taurus. NDP has several benefits: it reduces the amount of
data shipped over the network; frees up CPU capacity in the compute layer; and
reduces query run time, thereby enabling higher system throughput. Experiments
with the TPCH benchmark (100 GB) showed that 18 out of 22 queries benefited
from NDP; data shipped was reduced by 63 percent; and CPU time by 50 percent.
On Q15 the impact was even higher: data shipped was reduced by 98 percent; CPU
time by 91 percent; and run time by 80 percent.

</details>


### [28] [Piecewise Linear Approximation in Learned Index Structures: Theoretical and Empirical Analysis](https://arxiv.org/abs/2506.20139)
*Jiayong Qin,Xianyu Zhu,Qiyu Liu,Guangyi Zhang,Zhigang Cai,Jianwei Liao,Sha Hu,Jingshu Peng,Yingxia Shao,Lei Chen*

Main category: cs.DB

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: A growing trend in the database and system communities is to augment
conventional index structures, such as B+-trees, with machine learning (ML)
models. Among these, error-bounded Piecewise Linear Approximation
($\epsilon$-PLA) has emerged as a popular choice due to its simplicity and
effectiveness. Despite its central role in many learned indexes, the design and
analysis of $\epsilon$-PLA fitting algorithms remain underexplored. In this
paper, we revisit $\epsilon$-PLA from both theoretical and empirical
perspectives, with a focus on its application in learned index structures. We
first establish a fundamentally improved lower bound of $\Omega(\kappa \cdot
\epsilon^2)$ on the expected segment coverage for existing $\epsilon$-PLA
fitting algorithms, where $\kappa$ is a data-dependent constant. We then
present a comprehensive benchmark of state-of-the-art $\epsilon$-PLA algorithms
when used in different learned data structures. Our results highlight key
trade-offs among model accuracy, model size, and query performance, providing
actionable guidelines for the principled design of future learned data
structures.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [29] [MAIZX: A Carbon-Aware Framework for Optimizing Cloud Computing Emissions](https://arxiv.org/abs/2506.19972)
*Federico Ruilova,Ernst Gunnar Gran,Sven-Arne Reinemo*

Main category: cs.DC

TL;DR: 云计算有环境挑战，研究评估MAIZX框架，其可优化云操作、减少碳足迹，经测试有效。


<details>
  <summary>Details</summary>
Motivation: 云计算能耗高、碳排放大，实现2050年净零排放目标迫切，需高效透明解决方案。

Method: 评估MAIZX框架，基于实时和预测的碳强度、PUE和能耗对资源动态排名。

Result: MAIZX较基线虚拟机操作减少85.68%的CO2排放，在分布式数据中心测试有可扩展性和有效性。

Conclusion: MAIZX将相关数据集成到云管理，是提升气候性能、保持运营效率的有力工具。

Abstract: Cloud computing drives innovation but also poses significant environmental
challenges due to its high-energy consumption and carbon emissions. Data
centers account for 2-4% of global energy usage, and the ICT sector's share of
electricity consumption is projected to reach 40% by 2040. As the goal of
achieving net-zero emissions by 2050 becomes increasingly urgent, there is a
growing need for more efficient and transparent solutions, particularly for
private cloud infrastructures, which are utilized by 87% of organizations,
despite the dominance of public-cloud systems.
  This study evaluates the MAIZX framework, designed to optimize cloud
operations and reduce carbon footprint by dynamically ranking resources,
including data centers, edge computing nodes, and multi-cloud environments,
based on real-time and forecasted carbon intensity, Power Usage Effectiveness
(PUE), and energy consumption. Leveraging a flexible ranking algorithm, MAIZX
achieved an 85.68% reduction in CO2 emissions compared to baseline hypervisor
operations. Tested across geographically distributed data centers, the
framework demonstrates scalability and effectiveness, directly interfacing with
hypervisors to optimize workloads in private, hybrid, and multi-cloud
environments. MAIZX integrates real-time data on carbon intensity, power
consumption, and carbon footprint, as well as forecasted values, into cloud
management, providing a robust tool for enhancing climate performance potential
while maintaining operational efficiency.

</details>


### [30] [On the $h$-majority dynamics with many opinions](https://arxiv.org/abs/2506.20218)
*Francesco d'Amore,Niccolò D'Archivio,George Giakkoupis,Emanuele Natale*

Main category: cs.DC

TL;DR: 本文给出了具有k种意见的h - 多数动态同步收敛到共识时间的首个上界，证明在一定条件下，过程能在O(log n)轮收敛到多数共识，且所需偏差渐近更小。


<details>
  <summary>Details</summary>
Motivation: 研究具有非恒定h和k值的h - 多数动态在同步设置下收敛到共识的时间上界。

Method: 假设过程开始时有初始加性偏差，通过数学推导证明在特定偏差和节点数量条件下的收敛情况。

Result: 若偏差为ω(√x)且初始多数意见至少由x = ω(log n)个节点支持，当h = ω(n log n / x)时，过程以高概率在O(log n)轮收敛到多数共识；还给出了相关推论。

Conclusion: 之前Becchetti等人给出的Ω(k / h²)轮的下界不能进一步推到Ω̃(k / h)，且所需偏差比3 - 多数动态中保证多数共识的偏差渐近更小。

Abstract: We present the first upper bound on the convergence time to consensus of the
well-known $h$-majority dynamics with $k$ opinions, in the synchronous setting,
for $h$ and $k$ that are both non-constant values.
  We suppose that, at the beginning of the process, there is some initial
additive bias towards some plurality opinion, that is, there is an opinion that
is supported by $x$ nodes while any other opinion is supported by strictly
fewer nodes.
  We prove that, with high probability, if the bias is $\omega(\sqrt{x})$ and
the initial plurality opinion is supported by at least $x = \omega(\log n)$
nodes, then the process converges to plurality consensus in $O(\log n)$ rounds
whenever $h = \omega(n \log n / x)$.
  A main corollary is the following: if $k = o(n / \log n)$ and the process
starts from an almost-balanced configuration with an initial bias of magnitude
$\omega(\sqrt{n/k})$ towards the initial plurality opinion, then any function
$h = \omega(k \log n)$ suffices to guarantee convergence to consensus in
$O(\log n)$ rounds, with high probability.
  Our upper bound shows that the lower bound of $\Omega(k / h^2)$ rounds to
reach consensus given by Becchetti et al.\ (2017) cannot be pushed further than
$\widetilde{\Omega}(k / h)$.
  Moreover, the bias we require is asymptotically smaller than the
$\Omega(\sqrt{n\log n})$ bias that guarantees plurality consensus in the
$3$-majority dynamics: in our case, the required bias is at most any
(arbitrarily small) function in $\omega(\sqrt{x})$ for any value of $k \ge 2$.

</details>


### [31] [PAT: a new algorithm for all-gather and reduce-scatter operations at scale](https://arxiv.org/abs/2506.20252)
*Sylvain Jeaugey*

Main category: cs.DC

TL;DR: 介绍新算法PAT用于实现all - gather和reduce - scatter操作，可改善NCCL库性能


<details>
  <summary>Details</summary>
Motivation: 在环算法效率低下（小尺寸或大规模场景线性延迟性能差）的情况下，提高NCCL库性能

Method: 提出名为PAT（Parallel Aggregated Trees）的新算法

Result: 该算法适用于任意数量的进程，小尺寸操作网络传输次数呈对数级，减少长距离通信，内部缓冲区需求呈对数级，与总操作大小无关

Conclusion: 新算法PAT能改善NCCL库在特定场景下的性能

Abstract: This paper describes a new algorithm called PAT, for Parallel Aggregated
Trees, and which can be used to implement all-gather and reduce-scatter
operations. This algorithm works on any number of ranks, has a logarithmic
number of network transfers for small size operations, minimizes long-distance
communication, and requires a logarithmic amount of internal buffers,
independently from the total operation size. It is aimed at improving the
performance of the NCCL library in cases where the ring algorithm would be
inefficient, as its linear latency would show poor performance for small sizes
and/or at scale.

</details>


### [32] [WattsOnAI: Measuring, Analyzing, and Visualizing Energy and Carbon Footprint of AI Workloads](https://arxiv.org/abs/2506.20535)
*Hongzhen Huang,Kunming Zhang,Hanlong Liao,Kui Wu,Guoming Tang*

Main category: cs.DC

TL;DR: 本文提出了用于测量、分析和可视化AI工作负载能耗等指标的软件工具包WattsOnAI，解决现有工具的局限性，推动绿色AI实践。


<details>
  <summary>Details</summary>
Motivation: AI尤其是大语言模型的发展引发对能耗和碳排放的担忧，现有测量和报告工具存在碎片化、缺乏系统指标整合等问题。

Method: 开发WattsOnAI工具包，无缝集成现有AI框架，提供标准化报告和细粒度时间序列数据。

Result: 开发出WattsOnAI工具包，代码可在指定链接获取。

Conclusion: WattsOnAI解决了现有工具的关键局限，鼓励研究界权衡AI工作负载的环境影响和性能，推动向绿色AI实践转变。

Abstract: The rapid advancement of AI, particularly large language models (LLMs), has
raised significant concerns about the energy use and carbon emissions
associated with model training and inference. However, existing tools for
measuring and reporting such impacts are often fragmented, lacking systematic
metric integration and offering limited support for correlation analysis among
them. This paper presents WattsOnAI, a comprehensive software toolkit for the
measurement, analysis, and visualization of energy use, power draw, hardware
performance, and carbon emissions across AI workloads. By seamlessly
integrating with existing AI frameworks, WattsOnAI offers standardized reports
and exports fine-grained time-series data to support benchmarking and
reproducibility in a lightweight manner. It further enables in-depth
correlation analysis between hardware metrics and model performance and thus
facilitates bottleneck identification and performance enhancement. By
addressing critical limitations in existing tools, WattsOnAI encourages the
research community to weigh environmental impact alongside raw performance of
AI workloads and advances the shift toward more sustainable "Green AI"
practices. The code is available at https://github.com/SusCom-Lab/WattsOnAI.

</details>


### [33] [SuperSONIC: Cloud-Native Infrastructure for ML Inferencing](https://arxiv.org/abs/2506.20657)
*Dmitry Kondratyev,Benedikt Riedel,Yuan-Tang Chou,Miles Cochran-Branson,Noah Paladino,David Schultz,Mia Liu,Javier Duarte,Philip Harris,Shih-Chieh Hsu*

Main category: cs.DC

TL;DR: 因大规模科学实验计算需求增长，SONIC被采用以加速ML推理，SuperSONIC项目为此构建可扩展服务器基础设施，已在多个实验和集群成功部署，解决云原生时代挑战。


<details>
  <summary>Details</summary>
Motivation: 大规模科学实验中不断增长的计算需求促使采用SONIC方法加速ML推理，提升资源利用率。

Method: 开发SuperSONIC项目，构建可扩展服务器基础设施，利用NVIDIA Triton Inference Server，将客户端工作流与服务器基础设施解耦。

Result: SuperSONIC已成功部署于多个实验，并在多个Kubernetes集群上测试。

Conclusion: SuperSONIC提供了可复用、可配置的框架，提高了跨领域加速器推理部署效率，解决云原生时代挑战。

Abstract: The increasing computational demand from growing data rates and complex
machine learning (ML) algorithms in large-scale scientific experiments has
driven the adoption of the Services for Optimized Network Inference on
Coprocessors (SONIC) approach. SONIC accelerates ML inference by offloading it
to local or remote coprocessors to optimize resource utilization. Leveraging
its portability to different types of coprocessors, SONIC enhances data
processing and model deployment efficiency for cutting-edge research in high
energy physics (HEP) and multi-messenger astrophysics (MMA). We developed the
SuperSONIC project, a scalable server infrastructure for SONIC, enabling the
deployment of computationally intensive tasks to Kubernetes clusters equipped
with graphics processing units (GPUs). Using NVIDIA Triton Inference Server,
SuperSONIC decouples client workflows from server infrastructure, standardizing
communication, optimizing throughput, load balancing, and monitoring.
SuperSONIC has been successfully deployed for the CMS and ATLAS experiments at
the CERN Large Hadron Collider (LHC), the IceCube Neutrino Observatory
(IceCube), and the Laser Interferometer Gravitational-Wave Observatory (LIGO)
and tested on Kubernetes clusters at Purdue University, the National Research
Platform (NRP), and the University of Chicago. SuperSONIC addresses the
challenges of the Cloud-native era by providing a reusable, configurable
framework that enhances the efficiency of accelerator-based inference
deployment across diverse scientific domains and industries.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [34] [All-Pairs Shortest Paths with Few Weights per Node](https://arxiv.org/abs/2506.20017)
*Amir Abboud,Nick Fischer,Ce Jin,Virginia Vassilevska Williams,Zoe Xi*

Main category: cs.DS

TL;DR: 研究受限的全对最短路径问题，提出新算法，改进节点加权APSP时间复杂度，对特定权重数实现亚立方时间求解，促进对APSP假设的理解。


<details>
  <summary>Details</summary>
Motivation: 探索有最多d个不同权重限制下的全对最短路径问题的高效算法，解决节点加权APSP复杂度相关问题。

Method: 提出新算法解决节点加权APSP和特定d值下的APSP问题。

Result: 1. 节点加权APSP求解时间从旧的亚立方界改进到O(n^2.686)；2. 对于d <= n^(3 - ω - ε)，能在亚立方时间O(n^(3 - f(ε)))求解。

Conclusion: 新算法改进了节点加权APSP复杂度，拓展了特定权重数下的亚立方时间求解范围，推动了对APSP假设的认识，技术有独特应用。

Abstract: We study the central All-Pairs Shortest Paths (APSP) problem under the
restriction that there are at most $d$ distinct weights on the outgoing edges
from every node. For $d=n$ this is the classical (unrestricted) APSP problem
that is hypothesized to require cubic time $n^{3-o(1)}$, and at the other
extreme, for $d=1$, it is equivalent to the Node-Weighted APSP problem. We
present new algorithms that achieve the following results:
  1. Node-Weighted APSP can be solved in time $\tilde{O}(n^{(3+\omega)/2}) =
\tilde{O}(n^{2.686})$, improving on the 15-year-old subcubic bounds
$\tilde{O}(n^{(9+\omega)/4}) = \tilde{O}(n^{2.843})$ [Chan; STOC '07] and
$\tilde{O}(n^{2.830})$ [Yuster; SODA '09]. This positively resolves the
question of whether Node-Weighted APSP is an ``intermediate'' problem in the
sense of having complexity $n^{2.5+o(1)}$ if $\omega=2$, in which case it also
matches an $n^{2.5-o(1)}$ conditional lower bound.
  2. For up to $d \leq n^{3-\omega-\epsilon}$ distinct weights per node (where
$\epsilon > 0$), the problem can be solved in subcubic time
$O(n^{3-f(\epsilon)})$ (where $f(\epsilon) > 0$). In particular, assuming that
$\omega = 2$, we can tolerate any sublinear number of distinct weights per node
$d \leq n^{1-\epsilon}$, whereas previous work [Yuster; SODA '09] could only
handle $d \leq n^{1/2-\epsilon}$ in subcubic time. This promotes our
understanding of the APSP hypothesis showing that the hardest instances must
exhaust a linear number of weights per node. Our result also applies to the
All-Pairs Exact Triangle problem, thus generalizing a result of Chan and
Lewenstein on "Clustered 3SUM" from arrays to matrices. Notably, our technique
constitutes a rare application of additive combinatorics in graph algorithms.

</details>


### [35] [LZSE: an LZ-style compressor supporting $O(\log n)$-time random access](https://arxiv.org/abs/2506.20107)
*Hiroki Shibata,Yuto Nakashima,Yutaro Yamaguchi,Shunsuke Inenaga*

Main category: cs.DS

TL;DR: 本文提出新的LZSE因式分解变体，研究贪心LZSE因式分解，证明其线性时间可计算，在重复度衡量上强于基于文法的压缩，还给出支持快速查询的数据结构。


<details>
  <summary>Details</summary>
Motivation: 现有基于文法的压缩方案虽支持高效随机访问，但一般LZ类因式分解方法未知，受限LZ类方案牺牲压缩效率换取访问速度，因此提出新的LZSE因式分解变体。

Method: 引入LZ - Start - End (LZSE)因式分解，研究贪心LZSE因式分解，展示其线性时间计算方法，提出支持快速查询的数据结构。

Result: 贪心LZSE因式分解可在线性时间计算，存在字符串家族使贪心LZSE因式分解规模小于最小文法，提出的数据结构支持O(log n)时间随机访问。

Conclusion: LZSE方案在重复度衡量方面比基于文法的压缩更强，所提数据结构可支持快速随机访问。

Abstract: An LZ-like factorization of a string is a factorization in which each factor
is either a single character or a copy of a substring that occurs earlier in
the string. While grammar-based compression schemes support efficient random
access with linear space in the size of the compressed representation, such
methods are not known for general LZ-like factorizations. This has led to the
development of restricted LZ-like schemes such as LZ-End [Kreft and Navarro,
2013] and height-bounded (LZHB) [Bannai et al., 2024], which trade off some
compression efficiency for faster access. We introduce LZ-Start-End (LZSE), a
new variant of LZ-like factorizations in which each copy factor refers to a
contiguous sequence of preceding factors. By its nature, any context-free
grammar can easily be converted into an LZSE factorization of equal size.
Further, we study the greedy LZSE factorization, in which each copy factor is
taken as long as possible. We show how the greedy LZSE factorization can be
computed in linear time with respect to the input string length, and that there
exists a family of strings for which the size of the greedy LZSE factorization
is of strictly lower order than that of the smallest grammar. These imply that
our LZSE scheme is stronger than grammar-based compressions in the context of
repetitiveness measures. To support fast queries, we propose a data structure
for LZSE-compressed strings that permits $O(\log n)$-time random access within
space linear in the compressed size, where $n$ is the length of the input
string.

</details>


### [36] [Accept More, Reject Less: Reducing up to 19% Unnecessary Desk-Rejections over 11 Years of ICLR Data](https://arxiv.org/abs/2506.20141)
*Xiaoyu Li,Zhao Song,Jiahao Zhang*

Main category: cs.DS

TL;DR: AI会议投稿激增致严格限稿，本文提出算法可在遵守规则下减少不必要拒稿。


<details>
  <summary>Details</summary>
Motivation: 当前AI会议按ID顺序拒稿政策可能丢弃有价值论文，需探索遵守投稿限制同时减少不必要拒稿的方法。

Method: 将当前初审拒稿政策形式化为优化问题，基于线性规划松弛和舍入方案开发实用算法。

Result: 在11年ICLR真实数据评估中，该方法可多保留19.23%论文，且计算高效，最多53.64秒完成。

Conclusion: 提出的初审拒稿策略简单实用，可显著减少不必要拒稿，有望改进当前计算机科学会议投稿政策。

Abstract: The explosive growth of AI research has driven paper submissions at flagship
AI conferences to unprecedented levels, necessitating many venues in 2025
(e.g., CVPR, ICCV, KDD, AAAI, IJCAI, WSDM) to enforce strict per-author
submission limits and to desk-reject any excess papers by simple ID order.
While this policy helps reduce reviewer workload, it may unintentionally
discard valuable papers and penalize authors' efforts. In this paper, we ask an
essential research question on whether it is possible to follow submission
limits while minimizing needless rejections. We first formalize the current
desk-rejection policies as an optimization problem, and then develop a
practical algorithm based on linear programming relaxation and a rounding
scheme. Under extensive evaluation on 11 years of real-world ICLR
(International Conference on Learning Representations) data, our method
preserves up to $19.23\%$ more papers without violating any author limits.
Moreover, our algorithm is highly efficient in practice, with all results on
ICLR data computed within at most 53.64 seconds. Our work provides a simple and
practical desk-rejection strategy that significantly reduces unnecessary
rejections, demonstrating strong potential to improve current CS conference
submission policies.

</details>


### [37] [Cut-Query Algorithms with Few Rounds](https://arxiv.org/abs/2506.20412)
*Yotam Kenneth-Mordoch,Robert Krauthgamer*

Main category: cs.DS

TL;DR: 研究cut - query模型中算法的轮复杂度，给出不同图（加权/无权）最小割等问题在轮复杂度和查询复杂度间的权衡算法。


<details>
  <summary>Details</summary>
Motivation: 以往对cut - query模型的研究集中在解决优化问题所需的查询次数，本文关注其轮复杂度。

Method: 设计不同算法来解决图中最小割、最小(s,t) - 割和近似最大割等问题，给出不同轮数和查询复杂度的组合。

Result: 给出了无权图和加权图中求最小割的不同算法及其对应的轮数和查询复杂度，也提供了求最小(s,t) - 割和近似最大割的算法。

Conclusion: 在cut - query模型中，几个经典图问题能以常数轮数解决。

Abstract: In the cut-query model, the algorithm can access the input graph $G=(V,E)$
only via cut queries that report, given a set $S\subseteq V$, the total weight
of edges crossing the cut between $S$ and $V\setminus S$. This model was
introduced by Rubinstein, Schramm and Weinberg [ITCS'18] and its investigation
has so far focused on the number of queries needed to solve optimization
problems, such as global minimum cut. We turn attention to the round complexity
of cut-query algorithms, and show that several classical problems can be solved
in this model with only a constant number of rounds.
  Our main results are algorithms for finding a minimum cut in a graph, that
offer different tradeoffs between round complexity and query complexity, where
$n=|V|$ and $\delta(G)$ denotes the minimum degree of $G$: (i)
$\tilde{O}(n^{4/3})$ cut queries in two rounds in unweighted graphs; (ii)
$\tilde{O}(rn^{1+1/r}/\delta(G)^{1/r})$ queries in $2r+1$ rounds for any
integer $r\ge 1$ again in unweighted graphs; and (iii)
$\tilde{O}(rn^{1+(1+\log_n W)/r})$ queries in $4r+3$ rounds for any $r\ge1$ in
weighted graphs. We also provide algorithms that find a minimum $(s,t)$-cut and
approximate the maximum cut in a few rounds.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [38] [Polynomial-Time Approximation Schemes via Utility Alignment: Unit-Demand Pricing and More](https://arxiv.org/abs/2506.20030)
*Robin Bowers,Marius Garbea,Emmanouil Pountourakis,Samuel Taggart*

Main category: cs.GT

TL;DR: 本文为多个NP - 难随机优化问题导出多项式时间近似方案，结果提升了各问题的研究水平，关键技术洞察是效用对齐。


<details>
  <summary>Details</summary>
Motivation: 为算法机制设计和运筹学文献中的几个NP - 难随机优化问题找到多项式时间近似方案。

Method: 利用被称为效用对齐的经济属性，聚焦于在代理效用高的实现上最大化性能。

Result: 在单位需求定价、商品组合优化和委托选择等问题上取得进展，提升了各问题的近似结果。

Conclusion: 效用对齐属性可用于解决多个NP - 难随机优化问题，推进了各问题的研究现状。

Abstract: This paper derives polynomial-time approximation schemes for several NP-hard
stochastic optimization problems from the algorithmic mechanism design and
operations research literatures. The problems we consider involve a principal
or seller optimizing with respect to a subsequent choice by an agent or buyer.
These include posted pricing for a unit-demand buyer with independent values
(Chawla et al., 2007, Cai and Daskalakis, 2011), assortment optimization with
independent utilities (Talluri and van Ryzin, 2004), and delegated choice
(Khodabakhsh et al., 2024). Our results advance the state of the art for each
of these problems. For unit-demand pricing with discrete distributions, our
multiplicative PTAS improves on the additive PTAS of Cai and Daskalakis, and we
additionally give a PTAS for the unbounded regular case, improving on the
latter paper's QPTAS. For assortment optimization, no constant approximation
was previously known. For delegated choice, we improve on both the
$3$-approximation for the case with no outside option and the
super-constant-approximation with an outside option.
  A key technical insight driving our results is an economically meaningful
property we term utility alignment. Informally, a problem is utility aligned
if, at optimality, the principal derives most of their utility from
realizations where the agent's utility is also high. Utility alignment allows
the algorithm designer to focus on maximizing performance on realizations with
high agent utility, which is often an algorithmically simpler task. We prove
utility alignment results for all the problems mentioned above, including
strong results for unit-demand pricing and delegation, as well as a weaker but
very broad guarantee that holds for many other problems under very mild
conditions.

</details>


### [39] [Exact and approximate maximin share allocations in multi-graphs](https://arxiv.org/abs/2506.20317)
*George Christodoulou,Symeon Mastrakoulis*

Main category: cs.GT

TL;DR: 研究图估值模型下不可分割物品的（近似）最大最小份额（MMS）分配问题，给出不同估值下的正负结果。


<details>
  <summary>Details</summary>
Motivation: 研究图估值模型下不可分割物品的公平分配问题。

Method: 聚焦于Christodolou等人提出的图估值模型，研究可加、XOS和次可加估值。

Result: 给出了（近似）MMS公平性和（近似）成对最大最小份额（PMMS）公平性的正负结果。

Conclusion: 对图估值模型下的公平分配问题有了一定的研究成果。

Abstract: We study the problem of (approximate) maximin share (MMS) allocation of
indivisible items among a set of agents. We focus on the graphical valuation
model, previously studied by Christodolou, Fiat, Koutsoupias, and Sgouritsa
("Fair allocation in graphs", EC 2023), where the input is given by a graph
where edges correspond to items, and vertices correspond to agents. An edge may
have non-zero marginal value only for its incident vertices. We study additive,
XOS and subadditive valuations and we present positive and negative results for
(approximate) MMS fairness, and also for (approximate) pair-wise maximin share
(PMMS) fairness.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [40] [CoVE: Compressed Vocabulary Expansion Makes Better LLM-based Recommender Systems](https://arxiv.org/abs/2506.19993)
*Haochen Zhang,Tianyi Zhang,Junze Yin,Oren Gal,Anshumali Shrivastava,Vladimir Braverman*

Main category: cs.IR

TL;DR: 本文提出压缩词汇扩展系统CoVE，利用大语言模型序列理解能力提升推荐任务表现，压缩嵌入层使其适用于大规模工业应用，并通过实验验证效果。


<details>
  <summary>Details</summary>
Motivation: 现有将大语言模型与推荐任务对齐的方法未充分利用其序列信息处理能力，导致性能不佳。

Method: 提出CoVE系统，为每个物品在扩展词汇表中分配唯一ID，利用大语言模型序列理解能力，压缩嵌入层。

Result: 通过在多个推荐数据集上的综合实验以及与先前工作的比较，证明了CoVE的有效性和性能。

Conclusion: CoVE能有效提升大语言模型在推荐任务上的表现，且适用于大规模工业应用。

Abstract: Recommender systems play a pivotal role in providing relevant content to
users. With the rapid development of large language models (LLMs), researchers
have begun utilizing LLMs to build more powerful recommender systems. However,
existing approaches that focus on aligning LLMs with recommendation tasks do
not fully leverage their sequential information processing capabilities,
leading to suboptimal performance.
  In this paper, we propose a novel system called compressed vocabulary
expansion (CoVE). In CoVE, each item is assigned a unique ID within the
expanded vocabulary. Our framework effectively capitalizes on sequence
understanding abilities of LLMs, significantly enhancing their performance on
recommendation tasks. Additionally, we compress the embedding layer, making
CoVE practical for large-scale industrial applications. The effectiveness and
performance of CoVE are demonstrated through comprehensive experiments on
multiple recommendation datasets and comparisons with prior works. Our code can
be found at https://github.com/HaochenZhang717/CoVE-official-Repo.

</details>


### [41] [Controlled Retrieval-augmented Context Evaluation for Long-form RAG](https://arxiv.org/abs/2506.20051)
*Jia-Huei Ju,Suzan Verberne,Maarten de Rijke,Andrew Yates*

Main category: cs.IR

TL;DR: 提出CRUX框架评估检索增强上下文，结果显示其评估更有效，指出当前检索方法有改进空间。


<details>
  <summary>Details</summary>
Motivation: 现有基于相关性的排名指标不足以反映检索对最终RAG结果的影响，特别是在长文本生成场景中。

Method: 引入CRUX框架，用人工编写的摘要控制知识信息范围，采用基于问题的评估进行细粒度评估。

Result: CRUX提供了更具反映性和诊断性的评估。

Conclusion: 当前检索方法有很大改进空间，为推进RAG检索指明了方向，数据和代码公开以支持未来研究。

Abstract: Retrieval-augmented generation (RAG) enhances large language models by
incorporating context retrieved from external knowledge sources. While the
effectiveness of the retrieval module is typically evaluated with
relevance-based ranking metrics, such metrics may be insufficient to reflect
the retrieval's impact on the final RAG result, especially in long-form
generation scenarios. We argue that providing a comprehensive
retrieval-augmented context is important for long-form RAG tasks like report
generation and propose metrics for assessing the context independent of
generation. We introduce CRUX, a \textbf{C}ontrolled
\textbf{R}etrieval-a\textbf{U}gmented conte\textbf{X}t evaluation framework
designed to directly assess retrieval-augmented contexts. This framework uses
human-written summaries to control the information scope of knowledge, enabling
us to measure how well the context covers information essential for long-form
generation. CRUX uses question-based evaluation to assess RAG's retrieval in a
fine-grained manner. Empirical results show that CRUX offers more reflective
and diagnostic evaluation. Our findings also reveal substantial room for
improvement in current retrieval methods, pointing to promising directions for
advancing RAG's retrieval. Our data and code are publicly available to support
and advance future research on retrieval.

</details>


### [42] [Multimodal Information Retrieval for Open World with Edit Distance Weak Supervision](https://arxiv.org/abs/2506.20070)
*KMA Solaiman,Bharat Bhargava*

Main category: cs.IR

TL;DR: 提出FemmIR框架，利用弱监督进行多模态检索，无需相似度标签，在MuQNOL数据集上评估，表现与同类系统相当。


<details>
  <summary>Details</summary>
Motivation: 避免检索作为有监督分类任务的标注开销，复用预训练编码器，解决数据标注稀缺且需在不同应用中表现良好的问题。

Method: 提出FemmIR框架，基于样本间编辑距离引入弱监督，通过多层次交互得分复用高层属性并维护约束；创建MuQNOL数据集用于基准测试。

Result: 在MuQNOL数据集的失踪人员用例上进行评估，FemmIR在按需提供精确和近似相似检索结果方面表现与同类系统相当。

Conclusion: FemmIR能有效实现多模态检索，无需相似度标签，可利用现有系统属性标识符完成检索任务。

Abstract: Existing multi-media retrieval models either rely on creating a common
subspace with modality-specific representation models or require schema mapping
among modalities to measure similarities among multi-media data. Our goal is to
avoid the annotation overhead incurred from considering retrieval as a
supervised classification task and re-use the pretrained encoders in large
language models and vision tasks. We propose "FemmIR", a framework to retrieve
multimodal results relevant to information needs expressed with multimodal
queries by example without any similarity label. Such identification is
necessary for real-world applications where data annotations are scarce and
satisfactory performance is required without fine-tuning with a common
framework across applications. We curate a new dataset called MuQNOL for
benchmarking progress on this task. Our technique is based on weak supervision
introduced through edit distance between samples: graph edit distance can be
modified to consider the cost of replacing a data sample in terms of its
properties, and relevance can be measured through the implicit signal from the
amount of edit cost among the objects. Unlike metric learning or encoding
networks, FemmIR re-uses the high-level properties and maintains the property
value and relationship constraints with a multi-level interaction score between
data samples and the query example provided by the user. We empirically
evaluate FemmIR on a missing person use case with MuQNOL. FemmIR performs
comparably to similar retrieval systems in delivering on-demand retrieval
results with exact and approximate similarities while using the existing
property identifiers in the system.

</details>


### [43] [Semantic-enhanced Modality-asymmetric Retrieval for Online E-commerce Search](https://arxiv.org/abs/2506.20330)
*Zhigong Zhou,Ning Ding,Xiaochuan Fan,Yue Shang,Yiming Qiu,Jingwei Zhuo,Zhiwei Ge,Songlin Wang,Lin Liu,Sulong Xu,Han Zhang*

Main category: cs.IR

TL;DR: 本文提出SMAR模型解决非对称场景下多模态检索的模态融合和对齐问题，实验显示该模型检索准确率优于基线模型，还开源了工业数据集。


<details>
  <summary>Details</summary>
Motivation: 语义检索在电商搜索中重要，多模态检索可利用视觉信息提升检索性能，但非对称场景下多模态检索问题未解决。

Method: 提出名为SMAR的语义增强模态非对称检索模型。

Result: 在工业数据集上的实验表明，SMAR模型在检索准确率上显著优于基线模型。

Conclusion: SMAR模型能有效解决非对称场景下多模态检索的模态融合和对齐问题，开源数据集利于后续研究。

Abstract: Semantic retrieval, which retrieves semantically matched items given a
textual query, has been an essential component to enhance system effectiveness
in e-commerce search. In this paper, we study the multimodal retrieval problem,
where the visual information (e.g, image) of item is leveraged as supplementary
of textual information to enrich item representation and further improve
retrieval performance. Though learning from cross-modality data has been
studied extensively in tasks such as visual question answering or media
summarization, multimodal retrieval remains a non-trivial and unsolved problem
especially in the asymmetric scenario where the query is unimodal while the
item is multimodal. In this paper, we propose a novel model named SMAR, which
stands for Semantic-enhanced Modality-Asymmetric Retrieval, to tackle the
problem of modality fusion and alignment in this kind of asymmetric scenario.
Extensive experimental results on an industrial dataset show that the proposed
model outperforms baseline models significantly in retrieval accuracy. We have
open sourced our industrial dataset for the sake of reproducibility and future
research works.

</details>


### [44] [Unidentified and Confounded? Understanding Two-Tower Models for Unbiased Learning to Rank](https://arxiv.org/abs/2506.20501)
*Philipp Hager,Onno Zoeter,Maarten de Rijke*

Main category: cs.IR

TL;DR: 本文研究双塔模型训练中排名性能下降问题，分析原因并提出样本加权技术。


<details>
  <summary>Details</summary>
Motivation: 解决在良好生产系统收集的点击数据上训练双塔模型导致排名性能下降的问题。

Method: 理论分析双塔模型可识别性条件，研究日志策略对模型的影响，提出样本加权技术。

Result: 发现恢复模型参数需文档跨位置交换或特征分布重叠；日志策略在模型完美和不完美捕捉用户行为时有不同影响。

Conclusion: 提出的样本加权技术可缓解相关问题，为使用双塔模型的研究者和从业者提供见解。

Abstract: Additive two-tower models are popular learning-to-rank methods for handling
biased user feedback in industry settings. Recent studies, however, report a
concerning phenomenon: training two-tower models on clicks collected by
well-performing production systems leads to decreased ranking performance. This
paper investigates two recent explanations for this observation: confounding
effects from logging policies and model identifiability issues. We
theoretically analyze the identifiability conditions of two-tower models,
showing that either document swaps across positions or overlapping feature
distributions are required to recover model parameters from clicks. We also
investigate the effect of logging policies on two-tower models, finding that
they introduce no bias when models perfectly capture user behavior. However,
logging policies can amplify biases when models imperfectly capture user
behavior, particularly when prediction errors correlate with document placement
across positions. We propose a sample weighting technique to mitigate these
effects and provide actionable insights for researchers and practitioners using
two-tower models.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [45] [DIM-SUM: Dynamic IMputation for Smart Utility Management](https://arxiv.org/abs/2506.20023)
*Ryan Hildebrant,Rahul Bhope,Sharad Mehrotra,Christopher Tull,Nalini Venkatasubramanian*

Main category: cs.LG

TL;DR: 介绍DIM - SUM预处理框架用于训练稳健的时间序列插补模型，实验显示其优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列插补模型使用人工掩码模式模拟缺失值，与现实中基础设施监测数据大量复杂异构缺失情况不符，需开发新方法。

Method: 引入DIM - SUM框架，结合模式聚类和自适应掩码策略，并有理论学习保证。

Result: 在超20亿条读数的实验中，DIM - SUM以更短处理时间、更少训练数据达到类似精度，对比大预训练模型，精度平均高2倍且推理时间显著减少。

Conclusion: DIM - SUM能有效处理实际数据中的多样缺失模式，优于传统时间序列插补方法。

Abstract: Time series imputation models have traditionally been developed using
complete datasets with artificial masking patterns to simulate missing values.
However, in real-world infrastructure monitoring, practitioners often encounter
datasets where large amounts of data are missing and follow complex,
heterogeneous patterns. We introduce DIM-SUM, a preprocessing framework for
training robust imputation models that bridges the gap between artificially
masked training data and real missing patterns. DIM-SUM combines pattern
clustering and adaptive masking strategies with theoretical learning guarantees
to handle diverse missing patterns actually observed in the data. Through
extensive experiments on over 2 billion readings from California water
districts, electricity datasets, and benchmarks, we demonstrate that DIM-SUM
outperforms traditional methods by reaching similar accuracy with lower
processing time and significantly less training data. When compared against a
large pre-trained model, DIM-SUM averages 2x higher accuracy with significantly
less inference time.

</details>


### [46] [Position: Machine Learning Conferences Should Establish a "Refutations and Critiques" Track](https://arxiv.org/abs/2506.19882)
*Rylan Schaeffer,Joshua Kazdan,Yegor Denisov-Blanch,Brando Miranda,Matthias Gerstgrasser,Susan Zhang,Andreas Haupt,Isha Gupta,Elyas Obbad,Jesse Dodge,Jessica Zosa Forde,Koustuv Sinha,Francesco Orabona,Sanmi Koyejo,David Donoho*

Main category: cs.LG

TL;DR: 机器学习研究成果激增，但同行评审有缺陷，会让有问题的研究被接受。本文建议机器学习会议设立“反驳与批评”（R & C）赛道，以促进自我纠错研究生态。


<details>
  <summary>Details</summary>
Motivation: 机器学习研究成果快速增长，同行评审的缺陷导致有问题的研究被接受，且会议缺乏纠错机制。

Method: 提议在机器学习会议设立“反驳与批评”（R & C）赛道，并讨论赛道设计、评审原则和潜在陷阱等。

Result: 给出了关于最近ICLR 2025口头报告的示例提交。

Conclusion: 机器学习会议应建立官方、可靠的机制来帮助机器学习研究自我纠错。

Abstract: Science progresses by iteratively advancing and correcting humanity's
understanding of the world. In machine learning (ML) research, rapid
advancements have led to an explosion of publications, but have also led to
misleading, incorrect, flawed or perhaps even fraudulent studies being accepted
and sometimes highlighted at ML conferences due to the fallibility of peer
review. While such mistakes are understandable, ML conferences do not offer
robust processes to help the field systematically correct when such errors are
made.This position paper argues that ML conferences should establish a
dedicated "Refutations and Critiques" (R & C) Track. This R & C Track would
provide a high-profile, reputable platform to support vital research that
critically challenges prior research, thereby fostering a dynamic
self-correcting research ecosystem. We discuss key considerations including
track design, review principles, potential pitfalls, and provide an
illustrative example submission concerning a recent ICLR 2025 Oral. We conclude
that ML conferences should create official, reputable mechanisms to help ML
research self-correct.

</details>


### [47] [STIMULUS: Achieving Fast Convergence and Low Sample Complexity in Stochastic Multi-Objective Learning](https://arxiv.org/abs/2506.19883)
*Zhuqing Liu,Chaosheng Dong,Michinari Momma,Simone Shao,Shaoyuan Xu,Yan Gao,Haibo Yang,Jia Liu*

Main category: cs.LG

TL;DR: 本文提出STIMULUS及其增强版STIMULUS - M算法解决多目标优化问题，分析收敛率和样本复杂度，还提出自适应批量增强版。


<details>
  <summary>Details</summary>
Motivation: 现有多目标优化算法设计处于早期，很多方法收敛率和样本复杂度表现不佳。

Method: 提出STIMULUS算法，引入递归框架更新随机梯度估计；提出增强版STIMULUS - M加入动量项；提出自适应批量增强版STIMULUS+/ STIMULUS - M+。

Result: 建立了非凸和强凸设置下的收敛率，实现了最先进的样本复杂度。

Conclusion: 提出的算法有效解决多目标优化问题，具有良好的收敛性能和低样本复杂度。

Abstract: Recently, multi-objective optimization (MOO) has gained attention for its
broad applications in ML, operations research, and engineering. However, MOO
algorithm design remains in its infancy and many existing MOO methods suffer
from unsatisfactory convergence rate and sample complexity performance. To
address this challenge, in this paper, we propose an algorithm called STIMULUS(
stochastic path-integrated multi-gradient recursive e\ulstimator), a new and
robust approach for solving MOO problems. Different from the traditional
methods, STIMULUS introduces a simple yet powerful recursive framework for
updating stochastic gradient estimates to improve convergence performance with
low sample complexity. In addition, we introduce an enhanced version of
STIMULUS, termed STIMULUS-M, which incorporates a momentum term to further
expedite convergence. We establish $O(1/T)$ convergence rates of the proposed
methods for non-convex settings and $O (\exp{-\mu T})$ for strongly convex
settings, where $T$ is the total number of iteration rounds. Additionally, we
achieve the state-of-the-art $O \left(n+\sqrt{n}\epsilon^{-1}\right)$ sample
complexities for non-convex settings and $O\left(n+ \sqrt{n} \ln
({\mu/\epsilon})\right)$ for strongly convex settings, where $\epsilon>0$ is a
desired stationarity error. Moreover, to alleviate the periodic full gradient
evaluation requirement in STIMULUS and STIMULUS-M, we further propose enhanced
versions with adaptive batching called STIMULUS+/ STIMULUS-M+ and provide their
theoretical analysis.

</details>


### [48] [FlightKooba: A Fast Interpretable FTP Model](https://arxiv.org/abs/2506.19885)
*Jing Lu,Xuan Wu,Yizhun Tian,Songhan Fan,Yali Fang*

Main category: cs.LG

TL;DR: 本文提出FlightKooba框架解决现有Koopman理论应用于飞行轨迹预测任务的问题，实验证明其在时间和内存消耗上有优势。


<details>
  <summary>Details</summary>
Motivation: 现有应用Koopman理论到飞行轨迹预测任务的模型效果不佳、缺乏可解释性且计算量大、训练时间长。

Method: 基于HIPPO方法、Koopman理论和控制论的状态空间方程构建FlightKooba框架，直接从数据构建Koopman算子。

Result: FlightKooba建模方法在时间和内存消耗上表现优越，基本完成飞行轨迹预测任务。

Conclusion: 为Koopman算子的快速计算提供新方法，为时间序列预测和控制的结合开辟新可能。

Abstract: The Koopman theory is a powerful and effective modeling tool for converting
nonlinear systems into linear representations, and flight trajectory prediction
(FTP) is a complex nonlinear system. However, current models applying the
Koopman theory to FTP tasks are not very effective, model interpretability is
indeed an issue, and the Koopman operators are computationally intensive,
resulting in long training times. To address this issue, this paper proposes a
new modeling and control framework based on the HIPPO method, the Koopman
theory, and state space equations from cybernetics: FlightKooba. Inspired by
the idea of structural state space equations, FlightKooba directly constructs
the Koopman operators from data. This makes the framework highly interpretable
and significantly reduces the number of trainable parameters in the module,
thereby greatly reducing training time. Experiments have demonstrated the
superiority of the FlightKooba modeling method in terms of time and memory
consumption (training time comparable to the Mamba module without using
CUDA-level acceleration; memory reduced by more than 50% on most datasets, with
a tenfold reduction in the number of parameters), essentially completing the
FTP task. It provides a new method for the fast computation of the Koopman
operators, opening up new possibilities for the combination of time series
forecasting and control.

</details>


### [49] [Neuromorphic Wireless Split Computing with Resonate-and-Fire Neurons](https://arxiv.org/abs/2506.20015)
*Dengyu Wu,Jiechen Chen,H. Vincent Poor,Bipin Rajendran,Osvaldo Simeone*

Main category: cs.LG

TL;DR: 本文研究无线分割计算架构，用RF神经元处理时域信号，减少频谱预处理成本，在音频和调制分类任务中降低能耗且准确率相当。


<details>
  <summary>Details</summary>
Motivation: 传统LIF脉冲神经元无法有效捕捉边缘应用中流信号的丰富频谱特征，需要更高效的处理方式。

Method: 采用具有振荡动力学的RF神经元直接处理时域信号，结合基于OFDM的模拟无线接口进行脉冲传输，并给出完整系统设计。

Result: 提出的RF - SNN架构在音频和调制分类任务中与传统LIF - SNNs和ANNs准确率相当，大幅降低了脉冲率和推理及通信的总能耗。

Conclusion: RF神经元用于无线分割计算架构可有效处理时域信号，在保证准确率的同时显著降低能耗。

Abstract: Neuromorphic computing offers an energy-efficient alternative to conventional
deep learning accelerators for real-time time-series processing. However, many
edge applications, such as wireless sensing and audio recognition, generate
streaming signals with rich spectral features that are not effectively captured
by conventional leaky integrate-and-fire (LIF) spiking neurons. This paper
investigates a wireless split computing architecture that employs
resonate-and-fire (RF) neurons with oscillatory dynamics to process time-domain
signals directly, eliminating the need for costly spectral pre-processing. By
resonating at tunable frequencies, RF neurons extract time-localized spectral
features while maintaining low spiking activity. This temporal sparsity
translates into significant savings in both computation and transmission
energy. Assuming an OFDM-based analog wireless interface for spike
transmission, we present a complete system design and evaluate its performance
on audio classification and modulation classification tasks. Experimental
results show that the proposed RF-SNN architecture achieves comparable accuracy
to conventional LIF-SNNs and ANNs, while substantially reducing spike rates and
total energy consumption during inference and communication.

</details>


### [50] [Causal-Aware Intelligent QoE Optimization for VR Interaction with Adaptive Keyframe Extraction](https://arxiv.org/abs/2506.19890)
*Ziru Zhang,Jiadong Yu,Danny H. K. Tsang*

Main category: cs.LG

TL;DR: 提出集成自适应关键帧提取和因果感知强化学习的智能框架优化多用户VR交互QoE，实验显示其性能优于基准方法。


<details>
  <summary>Details</summary>
Motivation: 现有自适应关键帧提取方法常忽略分配带宽、CPU频率和用户感知间的因果关系，限制了QoE提升，需优化多用户VR交互中的QoE。

Method: 提出新的QoE度量，将QoE优化问题建模为混合整数规划任务，提出PS - CDDPG集成DDPG与因果影响检测，利用因果信息指导动作探索。

Result: 使用CMU运动捕捉数据库实验表明，该框架显著降低交互延迟、提升QoE并保持公平性。

Conclusion: 所提框架能有效优化多用户VR交互的QoE，性能优于基准方法。

Abstract: The optimization of quality of experience (QoE) in multi-user virtual reality
(VR) interactions demands a delicate balance between ultra-low latency,
high-fidelity motion synchronization, and equitable resource allocation. While
adaptive keyframe extraction mitigates transmission overhead, existing
approaches often overlook the causal relationships among allocated bandwidth,
CPU frequency, and user perception, limiting QoE gains. This paper proposes an
intelligent framework to maximize QoE by integrating adaptive keyframe
extraction with causal-aware reinforcement learning (RL). First, a novel QoE
metric is formulated using the Weber-Fechner Law, combining perceptual
sensitivity, attention-driven priorities, and motion reconstruction accuracy.
The QoE optimization problem is then modeled as a mixed integer programming
(MIP) task, jointly optimizing keyframe ratios, bandwidth, and computational
resources under horizon-fairness constraints. We propose Partial State Causal
Deep Deterministic Policy Gradient (PS-CDDPG), which integrates the Deep
Deterministic Policy Gradient (DDPG) method with causal influence detection. By
leveraging causal information regarding how QoE is influenced and determined by
various actions, we explore actions guided by weights calculated from causal
inference (CI), which in turn improves training efficiency. Experiments
conducted with the CMU Motion Capture Database demonstrate that our framework
significantly reduces interactive latency, enhances QoE, and maintains
fairness, achieving superior performance compared to benchmark methods.

</details>


### [51] [Orthogonal Soft Pruning for Efficient Class Unlearning](https://arxiv.org/abs/2506.19891)
*Qinghui Gong,Xue Yang,Xiaohu Tang*

Main category: cs.LG

TL;DR: 提出类感知软剪枝框架用于机器学习去学习，实现快速精准遗忘，减少推理攻击风险，为MLaaS场景提供高效解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习去学习方法在去学习速度和保留预测准确性间存在权衡，有高计算开销或性能下降问题。

Method: 提出类感知软剪枝框架，利用正交卷积核正则化，训练时实施正交约束，通过激活差异分析识别特定类通道。

Result: 在多架构和数据集上稳定剪枝，近乎即时执行，完全遗忘目标类，保留数据准确性损失极小，降低成员推理攻击风险，比现有方法加速去学习。

Conclusion: 该框架为MLaaS场景的实时机器学习去学习提供高效实用方案。

Abstract: Machine unlearning aims to selectively remove class-specific knowledge from
pretrained neural networks to satisfy privacy regulations such as the GDPR.
Existing methods typically face a trade-off between unlearning speed and
preservation of predictive accuracy, often incurring either high computational
overhead or significant performance degradation on retained classes. In this
paper, we propose a novel class-aware soft pruning framework leveraging
orthogonal convolutional kernel regularization to achieve rapid and precise
forgetting with millisecond-level response times. By enforcing orthogonality
constraints during training, our method decorrelates convolutional filters and
disentangles feature representations, while efficiently identifying
class-specific channels through activation difference analysis. Extensive
evaluations across multiple architectures and datasets demonstrate stable
pruning with near-instant execution, complete forgetting of targeted classes,
and minimal accuracy loss on retained data. Experiments on CIFAR-10, CIFAR-100,
and TinyImageNet confirm that our approach substantially reduces membership
inference attack risks and accelerates unlearning by orders of magnitude
compared to state-of-the-art baselines. This framework provides an efficient,
practical solution for real-time machine unlearning in Machine Learning as a
Service (MLaaS) scenarios.

</details>


### [52] [Self-Supervised Graph Learning via Spectral Bootstrapping and Laplacian-Based Augmentations](https://arxiv.org/abs/2506.20362)
*Lorenzo Bini,Stephane Marchand-Maillet*

Main category: cs.LG

TL;DR: 提出LaplaceGNN自监督图学习框架，无需负采样，通过光谱自举技术实现线性扩展，在基准数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 寻找一种无需负采样、对比目标或手工增强的更简单高效的自监督图学习方法。

Method: 将基于拉普拉斯的信号集成到学习过程，通过最大 - 最小中心性引导优化预计算光谱增强，采用对抗自举训练方案。

Result: 在不同基准数据集上的实验表明，LaplaceGNN性能优于现有自监督图方法。

Conclusion: LaplaceGNN为高效学习有表现力的图表示提供了有前景的方向。

Abstract: We present LaplaceGNN, a novel self-supervised graph learning framework that
bypasses the need for negative sampling by leveraging spectral bootstrapping
techniques. Our method integrates Laplacian-based signals into the learning
process, allowing the model to effectively capture rich structural
representations without relying on contrastive objectives or handcrafted
augmentations. By focusing on positive alignment, LaplaceGNN achieves linear
scaling while offering a simpler, more efficient, self-supervised alternative
for graph neural networks, applicable across diverse domains. Our contributions
are twofold: we precompute spectral augmentations through max-min
centrality-guided optimization, enabling rich structural supervision without
relying on handcrafted augmentations, then we integrate an adversarial
bootstrapped training scheme that further strengthens feature learning and
robustness. Our extensive experiments on different benchmark datasets show that
LaplaceGNN achieves superior performance compared to state-of-the-art
self-supervised graph methods, offering a promising direction for efficiently
learning expressive graph representations.

</details>


### [53] [Distillation-Enabled Knowledge Alignment for Generative Semantic Communications in AIGC Provisioning Tasks](https://arxiv.org/abs/2506.19893)
*Jingzhi Hu,Geoffrey Ye Li*

Main category: cs.LG

TL;DR: 因AIGC数据量大，网络传输压力大，GSC是解决方案但存在知识对齐难题。本文提出DeKA - g算法，含MAKD和VGSA方法，模拟结果显示其在图像对齐、压缩率适应和低SNR性能上有提升。


<details>
  <summary>Details</summary>
Motivation: AIGC数据从云端传输到边缘和移动用户产生大量网络流量，GSC虽有潜力但存在知识对齐挑战。

Method: 提出DeKA - g算法，包括metaword - aided knowledge distillation (MAKD)和variable - rate grouped SNR adaptation (VGSA)。MAKD用优化元词提升知识蒸馏效率，VGSA适应不同压缩率和SNR范围。

Result: DeKA - g使边缘生成图像与云端生成图像的对齐度提高44%，适应压缩率的效率比基线高116%，在低SNR条件下性能提升28%。

Conclusion: DeKA - g算法有效解决了GSC系统中的知识对齐问题，在图像对齐、压缩率适应和低SNR性能上有显著提升。

Abstract: Due to the surging amount of AI-generated content (AIGC), its provisioning to
edges and mobile users from the cloud incurs substantial traffic on networks.
Generative semantic communication (GSC) offers a promising solution by
transmitting highly compact information, i.e., prompt text and latent
representations, instead of high-dimensional AIGC data. However, GSC relies on
the alignment between the knowledge in the cloud generative AI (GAI) and that
possessed by the edges and users, and between the knowledge for wireless
transmission and that of actual channels, which remains challenging. In this
paper, we propose DeKA-g, a distillation-enabled knowledge alignment algorithm
for GSC systems. The core idea is to distill the generation knowledge from the
cloud-GAI into low-rank matrices, which can be incorporated by the edge and
used to adapt the transmission knowledge to diverse wireless channel
conditions. DeKA-g comprises two novel methods: metaword-aided knowledge
distillation (MAKD) and variable-rate grouped SNR adaptation (VGSA). For MAKD,
an optimized metaword is employed to enhance the efficiency of knowledge
distillation, while VGSA enables efficient adaptation to diverse compression
rates and SNR ranges. From simulation results, DeKA-g improves the alignment
between the edge-generated images and the cloud-generated ones by 44%.
Moreover, it adapts to compression rates with 116% higher efficiency than the
baseline and enhances the performance in low-SNR conditions by 28%.

</details>


### [54] [LSH-DynED: A Dynamic Ensemble Framework with LSH-Based Undersampling for Evolving Multi-Class Imbalanced Classification](https://arxiv.org/abs/2506.20041)
*Soheil Abadifard,Fazli Can*

Main category: cs.LG

TL;DR: 本文提出LSH - DynED方法处理多类不平衡非平稳数据流分类问题，实验显示其优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 多类不平衡数据流分类存在挑战，有效管理动态不平衡率困难，现有研究少。

Method: 将LSH - RHP集成到DynED框架，利用LSH - RHP对多数类欠采样。

Result: 在23个真实和10个半合成数据集实验，LSH - DynED在Kappa和mG - Mean指标上优于其他15种方法。

Conclusion: LSH - DynED能处理多类不平衡非平稳数据流，在大规模、高维数据集表现良好，有适应性和鲁棒性。

Abstract: The classification of imbalanced data streams, which have unequal class
distributions, is a key difficulty in machine learning, especially when dealing
with multiple classes. While binary imbalanced data stream classification tasks
have received considerable attention, only a few studies have focused on
multi-class imbalanced data streams. Effectively managing the dynamic imbalance
ratio is a key challenge in this domain. This study introduces a novel, robust,
and resilient approach to address these challenges by integrating Locality
Sensitive Hashing with Random Hyperplane Projections (LSH-RHP) into the Dynamic
Ensemble Diversification (DynED) framework. To the best of our knowledge, we
present the first application of LSH-RHP for undersampling in the context of
imbalanced non-stationary data streams. The proposed method undersamples the
majority classes by utilizing LSH-RHP, provides a balanced training set, and
improves the ensemble's prediction performance. We conduct comprehensive
experiments on 23 real-world and ten semi-synthetic datasets and compare
LSH-DynED with 15 state-of-the-art methods. The results reveal that LSH-DynED
outperforms other approaches in terms of both Kappa and mG-Mean effectiveness
measures, demonstrating its capability in dealing with multi-class imbalanced
non-stationary data streams. Notably, LSH-DynED performs well in large-scale,
high-dimensional datasets with considerable class imbalances and demonstrates
adaptation and robustness in real-world circumstances. To motivate our design,
we review existing methods for imbalanced data streams, outline key challenges,
and offer guidance for future work. For the reproducibility of our results, we
have made our implementation available on GitHub.

</details>


### [55] [Explaining deep neural network models for electricity price forecasting with XAI](https://arxiv.org/abs/2506.19894)
*Antoine Pesenti,Aidan OSullivan*

Main category: cs.LG

TL;DR: 本文使用DNN预测电价，结合XAI方法及可视化技术分析五大电力市场价格驱动因素，还引入SSHAP值和SSHAP线概念。


<details>
  <summary>Details</summary>
Motivation: 电力市场复杂，难以理解其内部运作和价格驱动因素，传统计量经济学方法不如DNN强大，目标是增进对不同电力市场运作的理解。

Method: 使用DNN进行价格预测，应用SHAP和Gradient等可解释方法，结合热力图等可视化技术分析特征，引入SSHAP值和SSHAP线概念。

Result: 未提及具体结果。

Conclusion: 未提及具体结论。

Abstract: Electricity markets are highly complex, involving lots of interactions and
complex dependencies that make it hard to understand the inner workings of the
market and what is driving prices. Econometric methods have been developed for
this, white-box models, however, they are not as powerful as deep neural
network models (DNN). In this paper, we use a DNN to forecast the price and
then use XAI methods to understand the factors driving the price dynamics in
the market. The objective is to increase our understanding of how different
electricity markets work. To do that, we apply explainable methods such as SHAP
and Gradient, combined with visual techniques like heatmaps (saliency maps) to
analyse the behaviour and contributions of various features across five
electricity markets. We introduce the novel concepts of SSHAP values and SSHAP
lines to enhance the complex representation of high-dimensional tabular models.

</details>


### [56] [A Framework for Uncertainty Quantification Based on Nearest Neighbors Across Layers](https://arxiv.org/abs/2506.19895)
*Miguel N. Font,José L. Jorro-Aragoneses,Carlos M. Alaíz*

Main category: cs.LG

TL;DR: 提出基于相似激活向量训练案例测量神经网络决策不确定性的后验框架及两个新指标，在CIFAR - 10和MNIST数据集分类模型评估中表现良好。


<details>
  <summary>Details</summary>
Motivation: 神经网络在高风险领域可能给出错误解决方案，需要测量其决策不确定性来检测和缓解错误。

Method: 提出基于各层与查询有相似激活向量的检索训练案例测量决策不确定性的后验框架，提出Decision Change和Layer Uncertainty两个新指标。

Result: 在CIFAR - 10和MNIST数据集分类模型评估中，新指标提升了不确定性估计，尤其在挑战性分类任务中优于基于softmax的置信度。

Conclusion: 新提出的指标和框架能有效增强神经网络决策不确定性估计，在挑战性分类任务中有更好表现。

Abstract: Neural Networks have high accuracy in solving problems where it is difficult
to detect patterns or create a logical model. However, these algorithms
sometimes return wrong solutions, which become problematic in high-risk domains
like medical diagnosis or autonomous driving. One strategy to detect and
mitigate these errors is the measurement of the uncertainty over neural network
decisions. In this paper, we present a novel post-hoc framework for measuring
the uncertainty of a decision based on retrieved training cases that have a
similar activation vector to the query for each layer. Based on these retrieved
cases, we propose two new metrics: Decision Change and Layer Uncertainty, which
capture changes in nearest-neighbor class distributions across layers. We
evaluated our approach in a classification model for two datasets: CIFAR-10 and
MNIST. The results show that these metrics enhance uncertainty estimation,
especially in challenging classification tasks, outperforming softmax-based
confidence.

</details>


### [57] [Collaborative Batch Size Optimization for Federated Learning](https://arxiv.org/abs/2506.20511)
*Arno Geimer,Karthick Panner Selvam,Beltran Fiz Pontiveros*

Main category: cs.LG

TL;DR: 本文聚焦通过硬件使用优化改进联邦学习本地训练过程，用贪婪随机搜索优化本地批量大小，结果显示能提升收敛速度。


<details>
  <summary>Details</summary>
Motivation: 联邦学习参与者在无信息交换时，不当训练配置会阻碍训练过程，需改进本地训练。

Method: 利用联邦学习的并行处理特性，采用贪婪随机搜索来优化所有参与者的本地批量大小。

Result: 与默认参数设置相比，该方法提高了收敛速度，且与优化本地参数的情况相近。

Conclusion: 通过硬件使用优化，尤其是优化本地批量大小，可有效提升联邦学习的训练效果。

Abstract: Federated Learning (FL) is a decentralized collaborative Machine Learning
framework for training models without collecting data in a centralized
location. It has seen application across various disciplines, from helping
medical diagnoses in hospitals to detecting fraud in financial transactions. In
this paper, we focus on improving the local training process through hardware
usage optimization. While participants in a federation might share the hardware
they are training on, since there is no information exchange between them,
their training process can be hindered by an improper training configuration.
Taking advantage of the parallel processing inherent to Federated Learning, we
use a greedy randomized search to optimize local batch sizes for the best
training settings across all participants. Our results show that against
default parameter settings, our method improves convergence speed while staying
nearly on par with the case where local parameters are optimized.

</details>


### [58] [A Comparative Analysis of Reinforcement Learning and Conventional Deep Learning Approaches for Bearing Fault Diagnosis](https://arxiv.org/abs/2506.19929)
*Efe Çakır,Patrick Dumond*

Main category: cs.LG

TL;DR: 研究探索强化学习用于轴承故障分类，结果显示其在优化奖励结构时适应性出色，但计算需求待改善，有潜力补充传统方法。


<details>
  <summary>Details</summary>
Motivation: 现代轴承故障诊断方法依赖振动分析和机器学习，需大量标记数据且难适应动态环境，故探索强化学习提高诊断准确性和适应性。

Method: 采用强化学习中的深度Q网络（DQNs）进行轴承故障分类任务。

Result: 研究开发的RL模型在可控条件下性能与传统监督学习模型相当，优化奖励结构时适应性出色，但计算需求大。

Conclusion: RL有潜力补充传统方法，为自适应诊断框架奠定基础。

Abstract: Bearing faults in rotating machinery can lead to significant operational
disruptions and maintenance costs. Modern methods for bearing fault diagnosis
rely heavily on vibration analysis and machine learning techniques, which often
require extensive labeled data and may not adapt well to dynamic environments.
This study explores the feasibility of reinforcement learning (RL),
specifically Deep Q-Networks (DQNs), for bearing fault classification tasks in
machine condition monitoring to enhance the accuracy and adaptability of
bearing fault diagnosis. The results demonstrate that while RL models developed
in this study can match the performance of traditional supervised learning
models under controlled conditions, they excel in adaptability when equipped
with optimized reward structures. However, their computational demands
highlight areas for further improvement. These findings demonstrate RL's
potential to complement traditional methods, paving the way for adaptive
diagnostic frameworks.

</details>


### [59] [Hear No Evil: Detecting Gradient Leakage by Malicious Servers in Federated Learning](https://arxiv.org/abs/2506.20651)
*Fei Wang,Baochun Li*

Main category: cs.LG

TL;DR: 文章从防御者角度分析恶意梯度泄漏攻击，指出攻击存在有效性与隐蔽性的权衡，实践中攻击有限且可检测，还提出轻量级客户端检测机制。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明联邦学习中梯度更新会无意泄露敏感信息，恶意服务器操纵全局模型会加剧风险，因此进行全面分析。

Method: 从防御者视角分析恶意梯度泄漏攻击及模型操纵技术，研究攻击在实际联邦学习场景中的特点。

Result: 发现攻击在重建私有数据的有效性和躲避检测的隐蔽性之间存在权衡，实践中攻击有局限性且常可通过基本监控检测到。

Conclusion: 恶意梯度泄漏攻击理论上令人担忧，但实践中受限且可检测，提出的客户端检测机制可在低开销下防御此类攻击，保障联邦学习系统隐私。

Abstract: Recent work has shown that gradient updates in federated learning (FL) can
unintentionally reveal sensitive information about a client's local data. This
risk becomes significantly greater when a malicious server manipulates the
global model to provoke information-rich updates from clients. In this paper,
we adopt a defender's perspective to provide the first comprehensive analysis
of malicious gradient leakage attacks and the model manipulation techniques
that enable them. Our investigation reveals a core trade-off: these attacks
cannot be both highly effective in reconstructing private data and sufficiently
stealthy to evade detection -- especially in realistic FL settings that
incorporate common normalization techniques and federated averaging.
  Building on this insight, we argue that malicious gradient leakage attacks,
while theoretically concerning, are inherently limited in practice and often
detectable through basic monitoring. As a complementary contribution, we
propose a simple, lightweight, and broadly applicable client-side detection
mechanism that flags suspicious model updates before local training begins,
despite the fact that such detection may not be strictly necessary in realistic
FL settings. This mechanism further underscores the feasibility of defending
against these attacks with minimal overhead, offering a deployable safeguard
for privacy-conscious federated learning systems.

</details>


### [60] [These are Not All the Features You are Looking For: A Fundamental Bottleneck In Supervised Pretraining](https://arxiv.org/abs/2506.18221)
*Xingyu Alice Yang,Jianyu Zhang,Léon Bottou*

Main category: cs.LG

TL;DR: 本文指出迁移学习中特征迁移应对新数据集的挑战，发现深度学习模型存在信息饱和瓶颈，建议关注特定任务训练并提出丰富特征表示的解决方案。


<details>
  <summary>Details</summary>
Motivation: 解决迁移学习中特征能否处理新数据集以及量化任务相关性的挑战。

Method: 评估从预训练混合数据到各组件任务的模型迁移，对比预训练特征和特定任务直接训练的性能。

Result: 发现深度学习模型存在信息饱和瓶颈，该现象在深度学习架构中普遍存在。

Conclusion: 仅依靠大规模网络可能不如特定任务训练有效，提出丰富特征表示以实现跨新数据集的泛化。

Abstract: Transfer learning is a cornerstone of modern machine learning, promising a
way to adapt models pretrained on a broad mix of data to new tasks with minimal
new data. However, a significant challenge remains in ensuring that transferred
features are sufficient to handle unseen datasets, amplified by the difficulty
of quantifying whether two tasks are "related". To address these challenges, we
evaluate model transfer from a pretraining mixture to each of its component
tasks, assessing whether pretrained features can match the performance of
task-specific direct training. We identify a fundamental limitation in deep
learning models -- an "information saturation bottleneck" -- where networks
fail to learn new features once they encode similar competing features during
training. When restricted to learning only a subset of key features during
pretraining, models will permanently lose critical features for transfer and
perform inconsistently on data distributions, even components of the training
mixture. Empirical evidence from published studies suggests that this
phenomenon is pervasive in deep learning architectures -- factors such as data
distribution or ordering affect the features that current representation
learning methods can learn over time. This study suggests that relying solely
on large-scale networks may not be as effective as focusing on task-specific
training, when available. We propose richer feature representations as a
potential solution to better generalize across new datasets and, specifically,
present existing methods alongside a novel approach, the initial steps towards
addressing this challenge.

</details>


### [61] [Any-Order GPT as Masked Diffusion Model: Decoupling Formulation and Architecture](https://arxiv.org/abs/2506.19935)
*Shuchen Xue,Tianyu Xie,Tianyang Hu,Zijin Feng,Jiacheng Sun,Kenji Kawaguchi,Zhenguo Li,Zhi-Ming Ma*

Main category: cs.LG

TL;DR: 研究在仅解码器框架下评估掩码扩散模型（MDMs），公平比较MDMs与自回归（AR）范式，还研究MDM架构影响，发现解码器MDMs可加速生成并取得可比困惑度。


<details>
  <summary>Details</summary>
Motivation: AR和MDM范式比较中因架构差异导致比较不公平，难以区分差异源于范式还是架构转变。

Method: 在仅解码器框架下评估MDMs，比较Any - Order AR（AO - AR）和标准AR范式，研究MDMs中仅解码器和仅编码器架构影响。

Result: 标准AO - AR目标可能需改进；解码器MDMs可实现约25倍生成加速，通过温度退火达到可比困惑度。

Conclusion: 解耦了核心范式差异与架构影响，为未来模型设计提供见解。

Abstract: Large language models (LLMs) predominantly use autoregressive (AR)
approaches, but masked diffusion models (MDMs) are emerging as viable
alternatives. A key challenge in comparing AR and MDM paradigms is their
typical architectural difference: AR models are often decoder-only, while MDMs
have largely been encoder-only. This practice of changing both the modeling
paradigm and architecture simultaneously makes direct comparisons unfair, as
it's hard to distinguish whether observed differences stem from the paradigm
itself or the architectural shift. This research evaluates MDMs within a
decoder-only framework to: (1) equitably compare MDM (as Any-Order AR, or
AO-AR) and standard AR paradigms. Our investigation suggests that the standard
AO-AR objective, which averages over all token permutations, may benefit from
refinement, as many permutations appear less informative compared to the
language's inherent left-to-right structure. (2) Investigate architectural
influences (decoder-only vs. encoder-only) within MDMs. We demonstrate that
while encoder-only MDMs model a simpler conditional probability space,
decoder-only MDMs can achieve dramatic generation speedups ($\sim25\times$) and
comparable perplexity with temperature annealing despite modeling a vastly
larger space, highlighting key trade-offs. This work thus decouples core
paradigm differences from architectural influences, offering insights for
future model design. Code is available at https://github.com/scxue/AO-GPT-MDM.

</details>


### [62] [The Most Important Features in Generalized Additive Models Might Be Groups of Features](https://arxiv.org/abs/2506.19937)
*Tomas M. Bosschieter,Luis Franca,Jessica Wolk,Yiyuan Wu,Bella Mehta,Joseph Dehoney,Orsolya Kiss,Fiona C. Baker,Qingyu Zhao,Rich Caruana,Kilian M. Pohl*

Main category: cs.LG

TL;DR: 本文提出一种确定广义加性模型特征组重要性的新方法，通过合成实验和两个案例研究表明分析特征组重要性比单特征分析更准确全面。


<details>
  <summary>Details</summary>
Motivation: 现有可解释机器学习中常忽略相关特征组的联合信号，而特征组的组合效应在很多情况下是重要预测因素，对于含自然特征分组的数据集此问题更突出。

Method: 提出一种确定广义加性模型特征组重要性的新方法，该方法高效、无需重新训练模型、允许事后定义组、允许组重叠且在高维环境有意义。

Result: 通过三个合成实验展示方法特性，在两个案例研究中揭示分析特征组重要性比单特征分析能提供更准确、全面的医学问题视图。

Conclusion: 分析特征组重要性在处理含特征分组的数据集时更有效，能提供更准确全面的信息。

Abstract: While analyzing the importance of features has become ubiquitous in
interpretable machine learning, the joint signal from a group of related
features is sometimes overlooked or inadvertently excluded. Neglecting the
joint signal could bypass a critical insight: in many instances, the most
significant predictors are not isolated features, but rather the combined
effect of groups of features. This can be especially problematic for datasets
that contain natural groupings of features, including multimodal datasets. This
paper introduces a novel approach to determine the importance of a group of
features for Generalized Additive Models (GAMs) that is efficient, requires no
model retraining, allows defining groups posthoc, permits overlapping groups,
and remains meaningful in high-dimensional settings. Moreover, this definition
offers a parallel with explained variation in statistics. We showcase
properties of our method on three synthetic experiments that illustrate the
behavior of group importance across various data regimes. We then demonstrate
the importance of groups of features in identifying depressive symptoms from a
multimodal neuroscience dataset, and study the importance of social
determinants of health after total hip arthroplasty. These two case studies
reveal that analyzing group importance offers a more accurate, holistic view of
the medical issues compared to a single-feature analysis.

</details>


### [63] [HERCULES: Hierarchical Embedding-based Recursive Clustering Using LLMs for Efficient Summarization](https://arxiv.org/abs/2506.19992)
*Gabor Petnehazi,Bernadett Aradi*

Main category: cs.LG

TL;DR: 介绍用于不同数据类型分层k均值聚类的HERCULES算法及Python包，结合LLM增强可解释性，有两种模式并具可视化工具，展示其能力和潜力。


<details>
  <summary>Details</summary>
Motivation: 复杂数据集增长需要先进分析工具，有效分组数据并提供可理解结构洞察。

Method: 递归应用k均值聚类构建层次结构，深度集成LLM为各级簇生成语义丰富标题和描述，支持直接和描述两种表示模式，用户可提供主题种子引导。

Result: 开发出HERCULES算法和Python包，具备交互式可视化工具。

Conclusion: HERCULES有从复杂数据集中提取有意义分层知识的潜力。

Abstract: The explosive growth of complex datasets across various modalities
necessitates advanced analytical tools that not only group data effectively but
also provide human-understandable insights into the discovered structures. We
introduce HERCULES (Hierarchical Embedding-based Recursive Clustering Using
LLMs for Efficient Summarization), a novel algorithm and Python package
designed for hierarchical k-means clustering of diverse data types, including
text, images, and numeric data (processed one modality per run). HERCULES
constructs a cluster hierarchy by recursively applying k-means clustering,
starting from individual data points at level 0. A key innovation is its deep
integration of Large Language Models (LLMs) to generate semantically rich
titles and descriptions for clusters at each level of the hierarchy,
significantly enhancing interpretability. The algorithm supports two main
representation modes: `direct' mode, which clusters based on original data
embeddings or scaled numeric features, and `description' mode, which clusters
based on embeddings derived from LLM-generated summaries. Users can provide a
`topic\_seed' to guide LLM-generated summaries towards specific themes. An
interactive visualization tool facilitates thorough analysis and understanding
of the clustering results. We demonstrate HERCULES's capabilities and discuss
its potential for extracting meaningful, hierarchical knowledge from complex
datasets.

</details>


### [64] [TRACED: Transition-aware Regret Approximation with Co-learnability for Environment Design](https://arxiv.org/abs/2506.19997)
*Geonwoo Cho,Jaegyun Im,Jihwan Lee,Hojun Yi,Sejin Kim,Sundong Kim*

Main category: cs.LG

TL;DR: 提出TRACED方法改进UED，提升零样本泛化能力且减少环境交互次数。


<details>
  <summary>Details</summary>
Motivation: 解决深度强化学习智能体在未知环境的泛化问题，改进现有UED方法。

Method: 引入过渡预测误差，提出协同学习能力指标，结合二者得到TRACED方法。

Result: TRACED在多基准测试中提升零样本泛化能力，减少环境交互次数，消融实验验证各部分作用。

Conclusion: 精炼的遗憾近似和任务关系显式建模可用于UED中样本高效的课程设计。

Abstract: Generalizing deep reinforcement learning agents to unseen environments
remains a significant challenge. One promising solution is Unsupervised
Environment Design (UED), a co-evolutionary framework in which a teacher
adaptively generates tasks with high learning potential, while a student learns
a robust policy from this evolving curriculum. Existing UED methods typically
measure learning potential via regret, the gap between optimal and current
performance, approximated solely by value-function loss. Building on these
approaches, we introduce the transition prediction error as an additional term
in our regret approximation. To capture how training on one task affects
performance on others, we further propose a lightweight metric called
co-learnability. By combining these two measures, we present Transition-aware
Regret Approximation with Co-learnability for Environment Design (TRACED).
Empirical evaluations show that TRACED yields curricula that improve zero-shot
generalization across multiple benchmarks while requiring up to 2x fewer
environment interactions than strong baselines. Ablation studies confirm that
the transition prediction error drives rapid complexity ramp-up and that
co-learnability delivers additional gains when paired with the transition
prediction error. These results demonstrate how refined regret approximation
and explicit modeling of task relationships can be leveraged for
sample-efficient curriculum design in UED.

</details>


### [65] [Elucidated Rolling Diffusion Models for Probabilistic Weather Forecasting](https://arxiv.org/abs/2506.20024)
*Salva Rühling Cachay,Miika Aittala,Karsten Kreis,Noah Brenowitz,Arash Vahdat,Morteza Mardani,Rose Yu*

Main category: cs.LG

TL;DR: 提出Elucidated Rolling Diffusion Models (ERDM)，结合滚动预测结构与Elucidated Diffusion Models (EDM)，在2D Navier - Stokes模拟和ERA5全球天气预报中表现优于基于扩散的基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有高维混沌系统概率预测方法难以建模复杂时间依赖和不确定性增长，滚动扩散框架与先进扩散技术集成存在挑战。

Method: 将EDM核心组件适配到滚动预测场景，提出新的损失加权方案、高效初始化策略和定制混合序列架构。

Result: 在2D Navier - Stokes模拟和ERA5全球天气预报中，ERDM始终优于关键的基于扩散的基线模型。

Conclusion: ERDM为处理基于扩散的序列生成问题提供了灵活强大的通用框架。

Abstract: Diffusion models are a powerful tool for probabilistic forecasting, yet most
applications in high-dimensional chaotic systems predict future snapshots
one-by-one. This common approach struggles to model complex temporal
dependencies and fails to explicitly account for the progressive growth of
uncertainty inherent to such systems. While rolling diffusion frameworks, which
apply increasing noise to forecasts at longer lead times, have been proposed to
address this, their integration with state-of-the-art, high-fidelity diffusion
techniques remains a significant challenge. We tackle this problem by
introducing Elucidated Rolling Diffusion Models (ERDM), the first framework to
successfully unify a rolling forecast structure with the principled, performant
design of Elucidated Diffusion Models (EDM). To do this, we adapt the core EDM
components-its noise schedule, network preconditioning, and Heun sampler-to the
rolling forecast setting. The success of this integration is driven by three
key contributions: (i) a novel loss weighting scheme that focuses model
capacity on the mid-range forecast horizons where determinism gives way to
stochasticity; (ii) an efficient initialization strategy using a pre-trained
EDM for the initial window; and (iii) a bespoke hybrid sequence architecture
for robust spatiotemporal feature extraction under progressive denoising. On 2D
Navier-Stokes simulations and ERA5 global weather forecasting at 1.5^\circ
resolution, ERDM consistently outperforms key diffusion-based baselines,
including conditional autoregressive EDM. ERDM offers a flexible and powerful
general framework for tackling diffusion-based sequence generation problems
where modeling escalating uncertainty is paramount. Code is available at:
https://github.com/salvaRC/erdm

</details>


### [66] [New Insights on Unfolding and Fine-tuning Quantum Federated Learning](https://arxiv.org/abs/2506.20016)
*Shanika Iroshi Nanayakkara,Shiva Raj Pokhrel*

Main category: cs.LG

TL;DR: 本文提出基于深度展开的方法解决量子联邦学习中客户端异质性问题，经实验验证效果良好，适用于复杂挑战。


<details>
  <summary>Details</summary>
Motivation: 客户端异质性对量子联邦学习性能造成显著挑战，传统聚合方法在高度异质环境中常失败。

Method: 提出基于深度展开的方法，使客户端能根据自身训练行为自主优化超参数。

Result: 在IBM量子硬件和Qiskit Aer模拟器上实时训练，框架达到约90%的准确率，远超传统方法的约55%。

Conclusion: 该方法能解决传统量子联邦学习的核心局限，适用于医疗和基因组研究等复杂挑战。

Abstract: Client heterogeneity poses significant challenges to the performance of
Quantum Federated Learning (QFL). To overcome these limitations, we propose a
new approach leveraging deep unfolding, which enables clients to autonomously
optimize hyperparameters, such as learning rates and regularization factors,
based on their specific training behavior. This dynamic adaptation mitigates
overfitting and ensures robust optimization in highly heterogeneous
environments where standard aggregation methods often fail. Our framework
achieves approximately 90% accuracy, significantly outperforming traditional
methods, which typically yield around 55% accuracy, as demonstrated through
real-time training on IBM quantum hardware and Qiskit Aer simulators. By
developing self adaptive fine tuning, the proposed method proves particularly
effective in critical applications such as gene expression analysis and cancer
detection, enhancing diagnostic precision and predictive modeling within
quantum systems. Our results are attributed to convergence-aware, learnable
optimization steps intrinsic to the deep unfolded framework, which maintains
the generalization. Hence, this study addresses the core limitations of
conventional QFL, streamlining its applicability to any complex challenges such
as healthcare and genomic research.

</details>


### [67] [Thumb on the Scale: Optimal Loss Weighting in Last Layer Retraining](https://arxiv.org/abs/2506.20025)
*Nathan Stromberg,Christos Thrampoulidis,Lalitha Sankar*

Main category: cs.LG

TL;DR: 本文探讨最后一层再训练（LLR）机制，理论与实践证明损失加权在此机制中仍有效，但权重需考虑模型相对过参数化情况。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在判别任务能力提升的同时，克服训练数据偏差的能力受关注，以往结果显示参数化有两种极端情况，本文探索处于两者之间的LLR机制。

Method: 从理论和实践两方面进行研究。

Result: 发现在LLR机制中损失加权仍然有效，但权重必须考虑模型的相对过参数化。

Conclusion: 损失加权在LLR机制中有效，但要考虑模型相对过参数化。

Abstract: While machine learning models become more capable in discriminative tasks at
scale, their ability to overcome biases introduced by training data has come
under increasing scrutiny. Previous results suggest that there are two extremes
of parameterization with very different behaviors: the population
(underparameterized) setting where loss weighting is optimal and the separable
overparameterized setting where loss weighting is ineffective at ensuring equal
performance across classes. This work explores the regime of last layer
retraining (LLR) in which the unseen limited (retraining) data is frequently
inseparable and the model proportionately sized, falling between the two
aforementioned extremes. We show, in theory and practice, that loss weighting
is still effective in this regime, but that these weights \emph{must} take into
account the relative overparameterization of the model.

</details>


### [68] [On the ability of Deep Neural Networks to Learn Granger Causality in Multi-Variate Time Series Data](https://arxiv.org/abs/2506.20347)
*Malik Shahid Sultan,Hernando Ombao*

Main category: cs.LG

TL;DR: 提出一种新的格兰杰因果关系（GC）研究范式，利用深度学习模型集体建模时间序列，通过比较模型不确定性揭示GC结构，表明正则化模型可从数据中学到真实GC结构。


<details>
  <summary>Details</summary>
Motivation: 线性向量自回归模型（VAR）在GC估计中有局限性，现有基于深度神经网络（DNNs）的方法将GC视为变量选择问题，需新范式。

Method: 用深度学习模型集体建模时间序列，通过比较使用全量过去信息和去掉特定时间序列分量时模型不确定性或残差分布揭示GC结构，还比较输入层丢弃对神经网络学习GC的影响。

Result: 正则化模型无需在损失函数中显式添加引导项来选择变量或进行稀疏回归，就能从数据中学到真实GC结构。

Conclusion: 提出的新范式可行，正则化深度学习模型可有效学习时间序列中的真实GC结构。

Abstract: Granger Causality (GC) offers an elegant statistical framework to study the
association between multivariate time series data. Linear Vector Autoregressive
models (VAR) though have nice interpretation properties but have limited
practical application due to underlying assumptions on the kind of associations
that can be captured by these models. Numerous attempts have already been made
in the literature that exploit the functional approximation power of Deep
Neural Networks (DNNs) for the task of GC estimation. These methods however
treat GC as a variable selection problem. We present a novel paradigm for
approaching GC. We present this idea that GC is essentially linked with
prediction and if a deep learning model is used to model the time series
collectively or jointly, a well regularized model may learn the true granger
causal structure from the data, given that there is enough training data. We
propose to uncover the learned GC structure by comparing the model uncertainty
or distribution of the residuals when the past of everything is used as
compared to the one where a specific time series component is dropped from the
model. We also compare the effect of input layer dropout on the ability of a
neural network to learn granger causality from the data. We show that a well
regularized model infact can learn the true GC structure from the data without
explicitly adding terms in the loss function that guide the model to select
variables or perform sparse regression.

</details>


### [69] [Lost in Retraining: Roaming the Parameter Space of Exponential Families Under Closed-Loop Learning](https://arxiv.org/abs/2506.20623)
*Fariba Jangjoo,Matteo Marsili,Yasser Roudi*

Main category: cs.LG

TL;DR: 研究指数族模型的闭环学习过程，发现参数估计会使过程收敛放大初始偏差，提出防止方法并指出动态渐近行为不具重参数化不变性。


<details>
  <summary>Details</summary>
Motivation: 因未来大型神经网络模型可能主要用自身生成数据训练，关注闭环学习过程。

Method: 研究指数族模型，推导参数动态的运动方程。

Result: 最大似然估计使充分统计量有鞅性质，过程收敛放大初始偏差；可通过污染数据、最大后验估计或引入正则化防止；动态渐近行为不具重参数化不变性。

Conclusion: 闭环学习在指数族模型中有特定动态和问题，可采取措施避免不良结果。

Abstract: Closed-loop learning is the process of repeatedly estimating a model from
data generated from the model itself. It is receiving great attention due to
the possibility that large neural network models may, in the future, be
primarily trained with data generated by artificial neural networks themselves.
We study this process for models that belong to exponential families, deriving
equations of motions that govern the dynamics of the parameters. We show that
maximum likelihood estimation of the parameters endows sufficient statistics
with the martingale property and that as a result the process converges to
absorbing states that amplify initial biases present in the data. However, we
show that this outcome may be prevented by polluting the data with an
infinitesimal fraction of data points generated from a fixed model, by relying
on maximum a posteriori estimation or by introducing regularisation.
Furthermore, we show that the asymptotic behavior of the dynamics is not
reparametrisation invariant.

</details>


### [70] [PLoP: Precise LoRA Placement for Efficient Finetuning of Large Models](https://arxiv.org/abs/2506.20629)
*Soufiane Hayou,Nikhil Ghosh,Bin Yu*

Main category: cs.LG

TL;DR: 提出PLoP方法自动确定LoRA适配器放置的模块类型，实验证明其性能优于常用策略。


<details>
  <summary>Details</summary>
Motivation: 现有LoRA适配器放置策略研究少且结果不明确，需要更好的方法确定放置位置。

Method: 通过直观理论分析，提出PLoP方法自动识别模块类型。

Result: 在监督微调与推理强化学习的综合实验中，PLoP始终优于或至少能与常用放置策略竞争。

Conclusion: PLoP是一种有效的自动确定LoRA适配器放置位置的方法。

Abstract: Low-Rank Adaptation (LoRA) is a widely used finetuning method for large
models. Its small memory footprint allows practitioners to adapt large models
to specific tasks at a fraction of the cost of full finetuning. Different
modifications have been proposed to enhance its efficiency by, for example,
setting the learning rate, the rank, and the initialization. Another
improvement axis is adapter placement strategy: when using LoRA, practitioners
usually pick module types to adapt with LoRA, such as Query and Key modules.
Few works have studied the problem of adapter placement, with nonconclusive
results: original LoRA paper suggested placing adapters in attention modules,
while other works suggested placing them in the MLP modules. Through an
intuitive theoretical analysis, we introduce PLoP (Precise LoRA Placement), a
lightweight method that allows automatic identification of module types where
LoRA adapters should be placed, given a pretrained model and a finetuning task.
We demonstrate that PLoP consistently outperforms, and in the worst case
competes, with commonly used placement strategies through comprehensive
experiments on supervised finetuning and reinforcement learning for reasoning.

</details>


### [71] [Automated Generation of Diverse Courses of Actions for Multi-Agent Operations using Binary Optimization and Graph Learning](https://arxiv.org/abs/2506.20031)
*Prithvi Poddar,Ehsan Tarkesh Esfahani,Karthik Dantu,Souma Chowdhury*

Main category: cs.LG

TL;DR: 本文提出新理论和计算框架生成行动方案池，经测试有显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 多智能体任务规划需自动化处理，环境变化、智能体能力差异对规划有影响，需多样行动方案池。

Method: 将任务空间和行动方案池抽象为图，把行动方案制定为集中式多机器人任务分配问题，用遗传算法进行任务分配，用图神经网络进行单智能体任务排序。

Result: 在模拟环境测试中，相比随机游走基线有显著性能提升，任务排序最优差距小，为5个智能体和100个任务规划最多20个行动方案执行时间约50分钟。

Conclusion: 所提框架在生成多样行动方案池方面有效。

Abstract: Operations in disaster response, search \& rescue, and military missions that
involve multiple agents demand automated processes to support the planning of
the courses of action (COA). Moreover, traverse-affecting changes in the
environment (rain, snow, blockades, etc.) may impact the expected performance
of a COA, making it desirable to have a pool of COAs that are diverse in task
distributions across agents. Further, variations in agent capabilities, which
could be human crews and/or autonomous systems, present practical opportunities
and computational challenges to the planning process. This paper presents a new
theoretical formulation and computational framework to generate such diverse
pools of COAs for operations with soft variations in agent-task compatibility.
Key to the problem formulation is a graph abstraction of the task space and the
pool of COAs itself to quantify its diversity. Formulating the COAs as a
centralized multi-robot task allocation problem, a genetic algorithm is used
for (order-ignoring) allocations of tasks to each agent that jointly maximize
diversity within the COA pool and overall compatibility of the agent-task
mappings. A graph neural network is trained using a policy gradient approach to
then perform single agent task sequencing in each COA, which maximizes
completion rates adaptive to task features. Our tests of the COA generation
process in a simulated environment demonstrate significant performance gain
over a random walk baseline, small optimality gap in task sequencing, and
execution time of about 50 minutes to plan up to 20 COAs for 5 agent/100 task
operations.

</details>


### [72] [Mastering Multiple-Expert Routing: Realizable $H$-Consistency and Strong Guarantees for Learning to Defer](https://arxiv.org/abs/2506.20650)
*Anqi Mao,Mehryar Mohri,Yutao Zhong*

Main category: cs.LG

TL;DR: 本文针对多专家学习延迟问题，提出新的替代损失函数和高效算法，给出理论学习保证，进行实验对比性能。


<details>
  <summary>Details</summary>
Motivation: 现有优化延迟的替代损失函数在保证一致性属性上存在挑战，需解决相关问题。

Method: 引入新的替代损失函数和高效算法，分析单阶段和两阶段学习场景下的一致性问题并给出理论证明。

Result: 单阶段提出新的可实现H一致替代损失族，证明部分成员的H一致性；两阶段推导新的替代损失实现多种一致性，低噪声假设下有增强理论保证，还进行了实验。

Conclusion: 所提替代损失函数和算法有理论保证，实验结果可对比其与现有基线的性能。

Abstract: The problem of learning to defer with multiple experts consists of optimally
assigning input instances to experts, balancing the trade-off between their
accuracy and computational cost. This is a critical challenge in natural
language generation, but also in other fields such as image processing, and
medical diagnostics. Recent studies have proposed surrogate loss functions to
optimize deferral, but challenges remain in ensuring their consistency
properties. This paper introduces novel surrogate loss functions and efficient
algorithms with strong theoretical learning guarantees. We address open
questions regarding realizable $H$-consistency, $H$-consistency bounds, and
Bayes-consistency for both single-stage (jointly learning predictor and
deferral function) and two-stage (learning only the deferral function with a
fixed expert) learning scenarios. For single-stage deferral, we introduce a
family of new realizable $H$-consistent surrogate losses and further prove
$H$-consistency for a selected member. For two-stage deferral, we derive new
surrogate losses that achieve realizable $H$-consistency, $H$-consistency
bounds, and Bayes-consistency for the two-expert scenario and, under natural
assumptions, multiple-expert scenario. Additionally, we provide enhanced
theoretical guarantees under low-noise assumptions for both scenarios. Finally,
we report the results of experiments using our proposed surrogate losses,
comparing their performance against existing baselines.

</details>


### [73] [Verifiable Unlearning on Edge](https://arxiv.org/abs/2506.20037)
*Mohammad M Maheri,Alex Davidson,Hamed Haddadi*

Main category: cs.LG

TL;DR: 本文提出利用零知识证明的验证框架，确保边缘设备可验证地进行数据遗忘操作，结果表明框架实用有效。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习中版权侵权、偏差或监管要求下，确保边缘设备正确执行数据遗忘操作以维护完整性的问题。

Method: 引入基于零知识证明（zk - SNARKs）的验证框架，开发适配高效证明生成的遗忘算法，兼顾边缘环境限制与模型个性化。

Result: 验证框架实用有效，可验证的遗忘操作对个性化性能提升的影响极小。

Conclusion: 该方法能在边缘设备实现可验证、保护隐私且有效的机器遗忘。

Abstract: Machine learning providers commonly distribute global models to edge devices,
which subsequently personalize these models using local data. However, issues
such as copyright infringements, biases, or regulatory requirements may require
the verifiable removal of certain data samples across all edge devices.
Ensuring that edge devices correctly execute such unlearning operations is
critical to maintaining integrity.
  In this work, we introduce a verification framework leveraging zero-knowledge
proofs, specifically zk-SNARKs, to confirm data unlearning on personalized
edge-device models without compromising privacy. We have developed algorithms
explicitly designed to facilitate unlearning operations that are compatible
with efficient zk-SNARK proof generation, ensuring minimal computational and
memory overhead suitable for constrained edge environments. Furthermore, our
approach carefully preserves personalized enhancements on edge devices,
maintaining model performance post-unlearning.
  Our results affirm the practicality and effectiveness of this verification
framework, demonstrating verifiable unlearning with minimal degradation in
personalization-induced performance improvements. Our methodology ensures
verifiable, privacy-preserving, and effective machine unlearning across edge
devices.

</details>


### [74] [Cross-Layer Discrete Concept Discovery for Interpreting Language Models](https://arxiv.org/abs/2506.20040)
*Ankur Garg,Xuemin Yu,Hassan Sajjad,Samira Ebrahimi Kahou*

Main category: cs.LG

TL;DR: 本文指出当前研究在挖掘跨层涌现概念上的局限，提出CLVQVAE框架解决问题。


<details>
  <summary>Details</summary>
Motivation: 当前研究主要检查单层神经表征，忽略跨层叠加和冗余，难以挖掘跨层涌现概念，因此需要新方法解决。

Method: 提出CLVQVAE框架，利用向量量化映射跨层表征，合并重复特征为可解释概念向量；结合top - k基于温度采样和EMA码本更新；用scaled - spherical k - means++初始化码本。

Result: 未提及具体结果。

Conclusion: 未提及明确结论。

Abstract: Uncovering emergent concepts across transformer layers remains a significant
challenge because the residual stream linearly mixes and duplicates
information, obscuring how features evolve within large language models.
Current research efforts primarily inspect neural representations at single
layers, thereby overlooking this cross-layer superposition and the redundancy
it introduces. These representations are typically either analyzed directly for
activation patterns or passed to probing classifiers that map them to a limited
set of predefined concepts. To address these limitations, we propose
\gls{clvqvae}, a framework that uses vector quantization to map representations
across layers and in the process collapse duplicated residual-stream features
into compact, interpretable concept vectors. Our approach uniquely combines
top-$k$ temperature-based sampling during quantization with EMA codebook
updates, providing controlled exploration of the discrete latent space while
maintaining code-book diversity. We further enhance the framework with
scaled-spherical k-means++ for codebook initialization, which clusters by
directional similarity rather than magnitude, better aligning with semantic
structure in word embedding space.

</details>


### [75] [GNN's Uncertainty Quantification using Self-Distillation](https://arxiv.org/abs/2506.20046)
*Hirad Daneshvar,Reza Samavi*

Main category: cs.LG

TL;DR: 本文提出基于知识蒸馏的新方法高效精确量化GNN预测不确定性，在两个图数据集验证效果良好。


<details>
  <summary>Details</summary>
Motivation: 量化GNN预测不确定性有挑战，贝叶斯和集成方法计算成本高且集成方法度量有缺陷。

Method: 采用自蒸馏，同一网络兼作师生模型，开发不确定性度量为每个GNN分类器分配不同权重。

Result: 实验表明该方法能有效捕获模型预测不确定性，性能与MC Dropout和集成方法相近。

Conclusion: 提出的方法可高效精确量化GNN预测不确定性。

Abstract: Graph Neural Networks (GNNs) have shown remarkable performance in the
healthcare domain. However, what remained challenging is quantifying the
predictive uncertainty of GNNs, which is an important aspect of trustworthiness
in clinical settings. While Bayesian and ensemble methods can be used to
quantify uncertainty, they are computationally expensive. Additionally, the
disagreement metric used by ensemble methods to compute uncertainty cannot
capture the diversity of models in an ensemble network. In this paper, we
propose a novel method, based on knowledge distillation, to quantify GNNs'
uncertainty more efficiently and with higher precision. We apply
self-distillation, where the same network serves as both the teacher and
student models, thereby avoiding the need to train several networks
independently. To ensure the impact of self-distillation, we develop an
uncertainty metric that captures the diverse nature of the network by assigning
different weights to each GNN classifier. We experimentally evaluate the
precision, performance, and ability of our approach in distinguishing
out-of-distribution data on two graph datasets: MIMIC-IV and Enzymes. The
evaluation results demonstrate that the proposed method can effectively capture
the predictive uncertainty of the model while having performance similar to
that of the MC Dropout and ensemble methods. The code is publicly available at
https://github.com/tailabTMU/UQ_GNN.

</details>


### [76] [Universal pre-training by iterated random computation](https://arxiv.org/abs/2506.20057)
*Peter Bloem*

Main category: cs.LG

TL;DR: 研究用随机生成数据预训练模型，理论推导并实证其有效性及优势。


<details>
  <summary>Details</summary>
Motivation: 探索使用随机生成数据进行模型预训练的可行性和优势。

Method: 从算法复杂度角度理论推导，结合前人研究得出相关理论结果，进行实证研究。

Result: 合成生成数据可用于预训练，模型有零样本上下文学习能力且性能随规模提升，预训练后微调有更快收敛性和更好泛化性。

Conclusion: 随机生成数据预训练模型是可行且有效的，在实际应用中有积极效果。

Abstract: We investigate the use of randomly generated data for the sake of
pre-training a model. We justify this approach theoretically from the
perspective of algorithmic complexity, building on recent research that shows
that sequence models can be trained to approximate Solomonoff induction. We
derive similar, but complementary theoretical results. We show empirically that
synthetically generated data can be used to pre-train a model before the data
is seen. We replicate earlier results that models trained this way show
zero-shot in-context learning across a variety of datasets, and that this
performance improves with scale. We extend earlier results to real-world data,
and show that finetuning a model after pre-training offers faster convergence
and better generalization.

</details>


### [77] [Learning Instruction-Following Policies through Open-Ended Instruction Relabeling with Large Language Models](https://arxiv.org/abs/2506.20061)
*Zhicheng Zhang,Ziyan Wang,Yali Du,Fei Fang*

Main category: cs.LG

TL;DR: 本文提出利用大语言模型（LLMs）从先前收集的智能体轨迹中自动生成开放式指令的方法，以缓解对人工标注的依赖，在Craftax环境中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 强化学习中开发有效的指令跟随策略依赖大量人工标注数据集且难从稀疏奖励中学习，有挑战性。

Method: 利用大语言模型从智能体轨迹中回顾性地自动生成开放式指令，对失败轨迹重新标注，丰富训练数据。

Result: 在Craftax环境中进行实证评估，相比现有基线方法，在样本效率、指令覆盖范围和整体策略性能上有明显提升。

Conclusion: 利用大语言模型引导的开放式指令重新标注能有效提升指令跟随强化学习效果。

Abstract: Developing effective instruction-following policies in reinforcement learning
remains challenging due to the reliance on extensive human-labeled instruction
datasets and the difficulty of learning from sparse rewards. In this paper, we
propose a novel approach that leverages the capabilities of large language
models (LLMs) to automatically generate open-ended instructions retrospectively
from previously collected agent trajectories. Our core idea is to employ LLMs
to relabel unsuccessful trajectories by identifying meaningful subtasks the
agent has implicitly accomplished, thereby enriching the agent's training data
and substantially alleviating reliance on human annotations. Through this
open-ended instruction relabeling, we efficiently learn a unified
instruction-following policy capable of handling diverse tasks within a single
policy. We empirically evaluate our proposed method in the challenging Craftax
environment, demonstrating clear improvements in sample efficiency, instruction
coverage, and overall policy performance compared to state-of-the-art
baselines. Our results highlight the effectiveness of utilizing LLM-guided
open-ended instruction relabeling to enhance instruction-following
reinforcement learning.

</details>


### [78] [Supervised Coupled Matrix-Tensor Factorization (SCMTF) for Computational Phenotyping of Patient Reported Outcomes in Ulcerative Colitis](https://arxiv.org/abs/2506.20065)
*Cristian Minoccheri,Sophia Tesic,Kayvan Najarian,Ryan Stidham*

Main category: cs.LG

TL;DR: 本文提出新的监督耦合矩阵 - 张量分解（SCMTF）方法用于溃疡性结肠炎（UC）计算表型分析，处理患者报告结局（PROs）数据中的大量缺失值，预测用药持续性，取得良好效果并识别出有用表型。


<details>
  <summary>Details</summary>
Motivation: 患者报告症状通常嘈杂、主观且稀疏，一般不用于表型分析和机器学习方法，本文旨在利用计算表型分析挖掘PROs数据价值。

Method: 提出新颖的监督耦合矩阵 - 张量分解（SCMTF）方法，结合深度学习框架，整合时间序列的PROs、实验室检查数据和静态特征。

Result: 最佳模型在测试集上分别对未来8个月和20个月的用药变化预测的AUC值为0.853和0.803，导出包含静态和时间特征的可解释表型。

Conclusion: 基于低秩矩阵和张量的表型分析可成功应用于UC领域和高度缺失的PROs数据，PROs包含通常被丢弃的相关信息。

Abstract: Phenotyping is the process of distinguishing groups of patients to identify
different types of disease progression. A recent trend employs low-rank matrix
and tensor factorization methods for their capability of dealing with
multi-modal, heterogeneous, and missing data. Symptom quantification is crucial
for understanding patient experiences in inflammatory bowel disease, especially
in conditions such as ulcerative colitis (UC). However, patient-reported
symptoms are typically noisy, subjective, and significantly more sparse than
other data types. For this reason, they are usually not included in phenotyping
and other machine learning methods. This paper explores the application of
computational phenotyping to leverage Patient-Reported Outcomes (PROs) using a
novel supervised coupled matrix-tensor factorization (SCMTF) method, which
integrates temporal PROs and temporal labs with static features to predict
medication persistence in ulcerative colitis. This is the first tensor-based
method that is both supervised and coupled, it is the first application to the
UC domain, and the first application to PROs. We use a deep learning framework
that makes the model flexible and easy to train. The proposed method allows us
to handle the large amount of missing data in the PROs. The best model predicts
changes in medication 8 and 20 months in the future with AUCs of 0.853 and
0.803 on the test set respectively. We derive interpretable phenotypes
consisting of static features and temporal features (including their temporal
patterns). We show that low-rank matrix and tensor based phenotyping can be
successfully applied to the UC domain and to highly missing PRO data. We
identify phenotypes useful to predict medication persistence - these phenotypes
include several symptom variables, showing that PROs contain relevant
infromation that is usually discarded.

</details>


### [79] [A Survey of Predictive Maintenance Methods: An Analysis of Prognostics via Classification and Regression](https://arxiv.org/abs/2506.20090)
*Ainaz Jamshidi,Dongchan Kim,Muhammad Arif*

Main category: cs.LG

TL;DR: 文章对预测性维护（PdM）方法进行综述，比较回归和分类方法，分析进展、挑战和趋势，为相关人员提供参考并指出未来工作方向。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏回归和分类方法在PdM中的独立比较研究，需综合分析不同PdM方法优劣。

Method: 全面分析近期文献，聚焦分类和回归方法在PdM预测中的比较使用。

Result: 明确关键进展、挑战（数据不平衡和高维特征空间）和新兴趋势（混合方法和AI预后系统）。

Conclusion: 为研究人员和从业者提供各PdM方法优缺点认知，助其确定未来研究方向，构建更稳健维护系统，未来可系统回顾公共数据集等实际方面。

Abstract: Predictive maintenance (PdM) has become a crucial element of modern
industrial practice. PdM plays a significant role in operational dependability
and cost management by decreasing unforeseen downtime and optimizing asset life
cycle management. Machine learning and deep learning have enabled more precise
forecasts of equipment failure and remaining useful life (RUL). Although many
studies have been conducted on PdM, there has not yet been a standalone
comparative study between regression- and classification-based approaches. In
this review, we look across a range of PdM methodologies, while focusing more
strongly on the comparative use of classification and regression methods in
prognostics. While regression-based methods typically provide estimates of RUL,
classification-based methods present a forecast of the probability of failure
across defined time intervals. Through a comprehensive analysis of recent
literature, we highlight key advancements, challenges-such as data imbalance
and high-dimensional feature spaces-and emerging trends, including hybrid
approaches and AI-enabled prognostic systems. This review aims to provide
researchers and practitioners with an awareness of the strengths and
compromises of various PdM methods and to help identify future research and
build more robust, directed adaptive maintenance systems. Future work may
include a systematic review of practical aspects such as public datasets,
benchmarking platforms, and open-source tools to support the advancement of PdM
research.

</details>


### [80] [MEL: Multi-level Ensemble Learning for Resource-Constrained Environments](https://arxiv.org/abs/2506.20094)
*Krishna Praneet Gudipaty,Walid A. Hanafy,Kaan Ozkara,Qianlin Liang,Jesse Milzman,Prashant Shenoy,Suhas Diggavi*

Main category: cs.LG

TL;DR: 本文提出用于边缘推理的多层集成学习（MEL）框架，训练多个轻量级备份模型协作工作，在保证准确性的同时实现容错和灵活部署，实验显示其有良好性能。


<details>
  <summary>Details</summary>
Motivation: 边缘环境资源受限且易发生故障，传统故障恢复方法会牺牲延迟或准确性，无法满足关键边缘推理服务需求。

Method: 提出MEL框架，将其表述为多目标优化问题，损失公式鼓励个体模型间的多样性，确保模型独立性能良好。

Result: 在视觉、语言和音频数据集上的评估表明，MEL性能与原架构相当；40%原模型大小的集成模型性能相似，故障时保留95.6%的集成准确率。

Conclusion: MEL框架能在边缘推理中实现容错和灵活部署，同时保证准确性。

Abstract: AI inference at the edge is becoming increasingly common for low-latency
services. However, edge environments are power- and resource-constrained, and
susceptible to failures. Conventional failure resilience approaches, such as
cloud failover or compressed backups, often compromise latency or accuracy,
limiting their effectiveness for critical edge inference services. In this
paper, we propose Multi-Level Ensemble Learning (MEL), a new framework for
resilient edge inference that simultaneously trains multiple lightweight backup
models capable of operating collaboratively, refining each other when multiple
servers are available, and independently under failures while maintaining good
accuracy. Specifically, we formulate our approach as a multi-objective
optimization problem with a loss formulation that inherently encourages
diversity among individual models to promote mutually refining representations,
while ensuring each model maintains good standalone performance. Empirical
evaluations across vision, language, and audio datasets show that MEL provides
performance comparable to original architectures while also providing fault
tolerance and deployment flexibility across edge platforms. Our results show
that our ensemble model, sized at 40\% of the original model, achieves similar
performance, while preserving 95.6\% of ensemble accuracy in the case of
failures when trained using MEL.

</details>


### [81] [High-Resolution Live Fuel Moisture Content (LFMC) Maps for Wildfire Risk from Multimodal Earth Observation Data](https://arxiv.org/abs/2506.20132)
*Patrick Alan Johnson,Gabriel Tseng,Yawen Zhang,Heather Heward,Virginia Sjahli,Favyen Bastani,Joseph Redmon,Patrick Beukema*

Main category: cs.LG

TL;DR: 利用预训练多模态地球观测模型生成大范围LFMC地图，相比随机初始化模型有显著改进，还提供自动化管道。


<details>
  <summary>Details</summary>
Motivation: 野火强度和严重程度增加，LFMC是关键野火风险因素，但地面LFMC样本获取困难，需更好方法生成LFMC地图。

Method: 使用预训练的高度多模态地球观测模型生成大范围空间完整的LFMC地图，并提供自动化管道。

Result: 相比使用随机初始化模型的先前方法，RMSE降低20%，能在美国快速生成LFMC地图，并在两个受野火影响地区证明了有效性。

Conclusion: 利用预训练模型和自动化管道能有效生成LFMC地图，为野火研究和应对提供支持。

Abstract: Wildfires are increasing in intensity and severity at an alarming rate.
Recent advances in AI and publicly available satellite data enable monitoring
critical wildfire risk factors globally, at high resolution and low latency.
Live Fuel Moisture Content (LFMC) is a critical wildfire risk factor and is
valuable for both wildfire research and operational response. However,
ground-based LFMC samples are both labor intensive and costly to acquire,
resulting in sparse and infrequent updates. In this work, we explore the use of
a pretrained, highly-multimodal earth-observation model for generating
large-scale spatially complete (wall-to-wall) LFMC maps. Our approach achieves
significant improvements over previous methods using randomly initialized
models (20 reduction in RMSE). We provide an automated pipeline that enables
rapid generation of these LFMC maps across the United States, and demonstrate
its effectiveness in two regions recently impacted by wildfire (Eaton and
Palisades).

</details>


### [82] [Causal discovery in deterministic discrete LTI-DAE systems](https://arxiv.org/abs/2506.20169)
*Bala Rajesh Konkathi,Arun K. Tangirala*

Main category: cs.LG

TL;DR: 本文提出用于LTI - DAE系统因果发现的变量划分（PoV）方法，优于Kathari和Tangirala（2022）的方法，并通过案例验证有效性。


<details>
  <summary>Details</summary>
Motivation: Kathari和Tangirala（2022）提出的方法不适用于具有反馈控制和/或守恒定律的微分代数（DAE）或混合因果系统，需新方法用于LTI - DAE系统因果发现。

Method: 提出变量划分（PoV）方法，先使用DIPCA确定代数关系数量、动态关系数量和约束矩阵，再通过约束矩阵的可允许划分确定子集。

Result: PoV方法优于Kathari和Tangirala（2022）的方法，能识别因果驱动至最小子集。

Conclusion: 案例研究证明了PoV方法在LTI - DAE系统因果发现中的有效性。

Abstract: Discovering pure causes or driver variables in deterministic LTI systems is
of vital importance in the data-driven reconstruction of causal networks. A
recent work by Kathari and Tangirala, proposed in 2022, formulated the causal
discovery method as a constraint identification problem. The constraints are
identified using a dynamic iterative PCA (DIPCA)-based approach for dynamical
systems corrupted with Gaussian measurement errors. The DIPCA-based method
works efficiently for dynamical systems devoid of any algebraic relations.
However, several dynamical systems operate under feedback control and/or are
coupled with conservation laws, leading to differential-algebraic (DAE) or
mixed causal systems. In this work, a method, namely the partition of variables
(PoV), for causal discovery in LTI-DAE systems is proposed. This method is
superior to the method that was presented by Kathari and Tangirala (2022), as
PoV also works for pure dynamical systems, which are devoid of algebraic
equations. The proposed method identifies the causal drivers up to a minimal
subset. PoV deploys DIPCA to first determine the number of algebraic relations
($n_a$), the number of dynamical relations ($n_d$) and the constraint matrix.
Subsequently, the subsets are identified through an admissible partitioning of
the constraint matrix by finding the condition number of it. Case studies are
presented to demonstrate the effectiveness of the proposed method.

</details>


### [83] [Causal Operator Discovery in Partial Differential Equations via Counterfactual Physics-Informed Neural Networks](https://arxiv.org/abs/2506.20181)
*Ronald Katende*

Main category: cs.LG

TL;DR: 本文利用物理信息神经网络和反事实扰动开发了一个发现偏微分方程（PDE）因果结构的框架，理论上证明可精确恢复因果算子支持，实证验证在多数据集上表现良好，优于标准方法。


<details>
  <summary>Details</summary>
Motivation: 开发新方法以更好地发现偏微分方程中的因果结构，克服经典方法的局限性。

Method: 利用物理信息神经网络和反事实扰动，引入因果敏感性指数和结构偏差度量评估候选微分算子的影响。

Result: 理论上证明在特定条件下可精确恢复因果算子支持，实证验证在合成和真实数据集上能在噪声、冗余和数据稀缺情况下恢复控制算子，结构保真度优于标准方法。

Conclusion: 使因果PDE发现成为基于结构因果模型和变分残差分析的可处理且可解释的推理任务。

Abstract: We develop a principled framework for discovering causal structure in partial
differential equations (PDEs) using physics-informed neural networks and
counterfactual perturbations. Unlike classical residual minimization or sparse
regression methods, our approach quantifies operator-level necessity through
functional interventions on the governing dynamics. We introduce causal
sensitivity indices and structural deviation metrics to assess the influence of
candidate differential operators within neural surrogates. Theoretically, we
prove exact recovery of the causal operator support under restricted isometry
or mutual coherence conditions, with residual bounds guaranteeing
identifiability. Empirically, we validate the framework on both synthetic and
real-world datasets across climate dynamics, tumor diffusion, and ocean flows.
Our method consistently recovers governing operators even under noise,
redundancy, and data scarcity, outperforming standard PINNs and DeepONets in
structural fidelity. This work positions causal PDE discovery as a tractable
and interpretable inference task grounded in structural causal models and
variational residual analysis.

</details>


### [84] [DuoGPT: Training-free Dual Sparsity through Activation-aware Pruning in LLMs](https://arxiv.org/abs/2506.20194)
*Ruokai Yin,Yuhang Li,Donghyun Lee,Priyadarshini Panda*

Main category: cs.LG

TL;DR: 提出DuoGPT框架，结合非结构化权重剪枝和激活稀疏性构建双稀疏工作负载，在LLaMA-2和LLaMA-3上评估表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型因高内存和计算成本难以部署，多数剪枝方法忽略运行时激活稀疏性。

Method: 将激活稀疏性重新解释为动态结构化权重稀疏性，提出DuoGPT框架；扩展OBC框架进行激活感知校准，引入密集模型输出残差作为校正项；优化解决方案以实现高效GPU执行。

Result: 在LLaMA-2和LLaMA-3上评估，DuoGPT在等加速比1.39×时，比现有最先进的结构化剪枝方法准确率最高提升9.17%。

Conclusion: DuoGPT是一种有效的大语言模型压缩方法，能在减少计算和内存需求的同时提高准确率。

Abstract: Large language models (LLMs) deliver strong performance but are difficult to
deploy due to high memory and compute costs. While pruning reduces these
demands, most methods ignore activation sparsity observed at runtime. We
reinterpret activation sparsity as dynamic structured weight sparsity and
propose DuoGPT, a unified framework that constructs dual-sparse (spMspV)
workloads by combining unstructured weight pruning with activation sparsity. To
preserve accuracy, we extend the Optimal Brain Compression (OBC) framework with
activation-aware calibration and introduce output residuals from the dense
model as correction terms. We further optimize the solution for efficient GPU
execution, enabling scalability to billion-parameter LLMs. Evaluations on
LLaMA-2 and LLaMA-3 show that DuoGPT outperforms state-of-the-art structured
pruning methods by up to 9.17% accuracy at an iso-speedup of 1.39$\times$
compared to the baseline dense model.

</details>


### [85] [Zero-Shot Attribution for Large Language Models: A Distribution Testing Approach](https://arxiv.org/abs/2506.20197)
*Clément L. Canonne,Yash Pote,Uddalok Sarkar*

Main category: cs.LG

TL;DR: 本文提出零样本代码归属工具Anubis，用假设检验解决代码归属问题，实验表明其区分不同大语言模型代码时AUROC分数高。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型生成代码增多，需解决代码归属问题。

Method: 使用假设检验，将归属问题转化为分布测试问题，利用样本和密度估计，引入Anubis工具。

Result: 在代码样本基准测试中，Anubis区分不同大语言模型时仅用约2000个样本就能达到≥0.9的AUROC分数。

Conclusion: Anubis工具能有效解决大语言模型生成代码的归属问题。

Abstract: A growing fraction of all code is sampled from Large Language Models (LLMs).
We investigate the problem of attributing code generated by language models
using hypothesis testing to leverage established techniques and guarantees.
Given a set of samples $S$ and a suspect model $\mathcal{L}^*$, our goal is to
assess the likelihood of $S$ originating from $\mathcal{L}^*$. Due to the curse
of dimensionality, this is intractable when only samples from the LLM are
given: to circumvent this, we use both samples and density estimates from the
LLM, a form of access commonly available.
  We introduce $\mathsf{Anubis}$, a zero-shot attribution tool that frames
attribution as a distribution testing problem. Our experiments on a benchmark
of code samples show that $\mathsf{Anubis}$ achieves high AUROC scores (
$\ge0.9$) when distinguishing between LLMs like DeepSeek-Coder, CodeGemma, and
Stable-Code using only $\approx 2000$ samples.

</details>


### [86] [Affective Priming Score: A Data-Driven Method to Detect Priming in Sequential Datasets](https://arxiv.org/abs/2506.20204)
*Eduardo Gutierrez Maestro,Hadi Banaee,Amy Loutfi*

Main category: cs.LG

TL;DR: 提出情感启动分数（APS）检测受启动效应影响的数据点，应用于数据集验证，减少误分类率，提升模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 情感计算中启动效应对数据本身的影响，尤其是生理信号方面研究不足，受启动效应影响的数据会导致学习模型误分类。

Method: 提出数据驱动的情感启动分数（APS），为每个数据点打分量化受启动效应影响程度，将其应用于SEED和SEED - VII数据集，用原始数据和无启动序列训练相同配置的模型。

Result: 使用无启动序列时误分类率比原始数据显著降低。

Conclusion: 在数据层面识别和减轻启动效应，增强模型鲁棒性，为情感计算数据集设计和收集提供有价值见解。

Abstract: Affective priming exemplifies the challenge of ambiguity in affective
computing. While the community has largely addressed this issue from a
label-based perspective, identifying data points in the sequence affected by
the priming effect, the impact of priming on data itself, particularly in
physiological signals, remains underexplored. Data affected by priming can lead
to misclassifications when used in learning models. This study proposes the
Affective Priming Score (APS), a data-driven method to detect data points
influenced by the priming effect. The APS assigns a score to each data point,
quantifying the extent to which it is affected by priming. To validate this
method, we apply it to the SEED and SEED-VII datasets, which contain sufficient
transitions between emotional events to exhibit priming effects. We train
models with the same configuration using both the original data and
priming-free sequences. The misclassification rate is significantly reduced
when using priming-free sequences compared to the original data. This work
contributes to the broader challenge of ambiguity by identifying and mitigating
priming effects at the data level, enhancing model robustness, and offering
valuable insights for the design and collection of affective computing
datasets.

</details>


### [87] [Directed Link Prediction using GNN with Local and Global Feature Fusion](https://arxiv.org/abs/2506.20235)
*Yuyang Zhang,Xu Shen,Yu Xie,Ka-Chun Wong,Weidun Xie,Chengbin Peng*

Main category: cs.LG

TL;DR: 提出新的GNN框架融合特征嵌入与社区信息，将图转换为有向线图，实验显示多数情况下优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决有向图链接预测问题，改进现有深度学习方法。

Method: 提出新的GNN框架融合特征与社区信息，将输入图转换为有向线图。

Result: 在基准数据集上，使用30%、40%、50%和60%连接边作为训练数据时，多数情况优于现有方法。

Conclusion: 融合特征嵌入与社区信息的混合特征能提升有向链接预测性能。

Abstract: Link prediction is a classical problem in graph analysis with many practical
applications. For directed graphs, recently developed deep learning approaches
typically analyze node similarities through contrastive learning and aggregate
neighborhood information through graph convolutions. In this work, we propose a
novel graph neural network (GNN) framework to fuse feature embedding with
community information. We theoretically demonstrate that such hybrid features
can improve the performance of directed link prediction. To utilize such
features efficiently, we also propose an approach to transform input graphs
into directed line graphs so that nodes in the transformed graph can aggregate
more information during graph convolutions. Experiments on benchmark datasets
show that our approach outperforms the state-of-the-art in most cases when 30%,
40%, 50%, and 60% of the connected links are used as training data,
respectively.

</details>


### [88] [FedBKD: Distilled Federated Learning to Embrace Gerneralization and Personalization on Non-IID Data](https://arxiv.org/abs/2506.20245)
*Yushan Zhao,Jinyuan He,Donglai Chen,Weijie Luo,Chong Xie,Ri Zhang,Yonghong Chen,Yan Xu*

Main category: cs.LG

TL;DR: 本文提出Federated Bidirectional Knowledge Distillation (FedBKD) 框架解决联邦学习非IID数据问题，实验表明其达SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前联邦学习处理非IID数据的方案难以同时兼顾全局和本地模型性能，且引入公共数据集有数据泄露风险。

Method: 提出FedBKD框架，训练GAN生成合成数据，利用合成数据在全局和本地模型间进行双向蒸馏以实现知识交互。

Result: 在4个不同非IID设置的基准测试上，FedBKD在每种情况下都达到了SOTA性能。

Conclusion: FedBKD能有效解决联邦学习中非IID数据的问题，提升全局和本地模型的性能。

Abstract: Federated learning (FL) is a decentralized collaborative machine learning
(ML) technique. It provides a solution to the issues of isolated data islands
and data privacy leakage in industrial ML practices. One major challenge in FL
is handling the non-identical and independent distributed (non-IID) data.
Current solutions either focus on constructing an all-powerful global model, or
customizing personalized local models. Few of them can provide both a
well-generalized global model and well-performed local models at the same time.
Additionally, many FL solutions to the non-IID problem are benefited from
introducing public datasets. However, this will also increase the risk of data
leakage. To tackle the problems, we propose a novel data-free distillation
framework, Federated Bidirectional Knowledge Distillation (FedBKD).
Specifically, we train Generative Adversarial Networks (GAN) for synthetic
data. During the GAN training, local models serve as discriminators and their
parameters are frozen. The synthetic data is then used for bidirectional
distillation between global and local models to achieve knowledge interactions
so that performances for both sides are improved. We conduct extensive
experiments on 4 benchmarks under different non-IID settings. The results show
that FedBKD achieves SOTA performances in every case.

</details>


### [89] [Q-resafe: Assessing Safety Risks and Quantization-aware Safety Patching for Quantized Large Language Models](https://arxiv.org/abs/2506.20251)
*Kejia Chen,Jiawen Zhang,Jiacong Hu,Yu Wang,Jian Lou,Zunlei Feng,Mingli Song*

Main category: cs.LG

TL;DR: 本文对主流量化技术和校准数据集进行安全评估，提出Q - resafe框架恢复量化大语言模型安全能力，实验表明其有效。


<details>
  <summary>Details</summary>
Motivation: 新兴无校准数据集量化方法可能损害大语言模型安全能力，需要系统安全评估和有效缓解策略。

Method: 使用广泛认可的安全基准对主流量化技术和不同校准数据集进行全面安全评估，提出量化感知安全修补框架Q - resafe。

Result: 广泛实验结果表明，Q - resafe能在有挑战的评估场景下，使量化大语言模型的安全性与量化前相当。

Conclusion: Q - resafe能有效恢复量化大语言模型的安全能力，同时减少对实用性的不利影响。

Abstract: Quantized large language models (LLMs) have gained increasing attention and
significance for enabling deployment in resource-constrained environments.
However, emerging studies on a few calibration dataset-free quantization
methods suggest that quantization may compromise the safety capabilities of
LLMs, underscoring the urgent need for systematic safety evaluations and
effective mitigation strategies. In this paper, we present comprehensive safety
evaluations across various mainstream quantization techniques and diverse
calibration datasets, utilizing widely accepted safety benchmarks. To address
the identified safety vulnerabilities, we propose a quantization-aware safety
patching framework, Q-resafe, to efficiently restore the safety capabilities of
quantized LLMs while minimizing any adverse impact on utility. Extensive
experimental results demonstrate that Q-resafe successfully re-aligns the
safety of quantized LLMs with their pre-quantization counterparts, even under
challenging evaluation scenarios. Project page is available at:
https://github.com/Thecommonirin/Qresafe.

</details>


### [90] [Time-series surrogates from energy consumers generated by machine learning approaches for long-term forecasting scenarios](https://arxiv.org/abs/2506.20253)
*Ben Gerhards,Nikita Popkov,Annekatrin König,Marcel Arpogaus,Bastian Schäfermeier,Leonie Riedl,Stephan Vogt,Philip Hehlert*

Main category: cs.LG

TL;DR: 文章对用于长期电力消费预测的合成时间序列数据生成的数据驱动方法进行评估比较，分析多种技术优缺点，以提升合成数据质量。


<details>
  <summary>Details</summary>
Motivation: 多数研究集中于电力系统短期预测，对个体消费者长期用电预测关注少，而高保真合成数据对能源应用重要。

Method: 评估比较混合Wasserstein生成对抗网络、去噪扩散概率模型、隐马尔可夫模型和掩码自回归伯恩斯坦多项式归一化流等技术。

Result: 突出各方法优缺点，助力选择合适方法，生成的合成电力数据可用于状态估计等应用。

Conclusion: 所构建的框架能提升合成电力消费数据的准确性和可靠性，同时满足匿名化等标准。

Abstract: Forecasting attracts a lot of research attention in the electricity value
chain. However, most studies concentrate on short-term forecasting of
generation or consumption with a focus on systems and less on individual
consumers. Even more neglected is the topic of long-term forecasting of
individual power consumption.
  Here, we provide an in-depth comparative evaluation of data-driven methods
for generating synthetic time series data tailored to energy consumption
long-term forecasting. High-fidelity synthetic data is crucial for a wide range
of applications, including state estimations in energy systems or power grid
planning. In this study, we assess and compare the performance of multiple
state-of-the-art but less common techniques: a hybrid Wasserstein Generative
Adversarial Network (WGAN), Denoising Diffusion Probabilistic Model (DDPM),
Hidden Markov Model (HMM), and Masked Autoregressive Bernstein polynomial
normalizing Flows (MABF). We analyze the ability of each method to replicate
the temporal dynamics, long-range dependencies, and probabilistic transitions
characteristic of individual energy consumption profiles. Our comparative
evaluation highlights the strengths and limitations of: WGAN, DDPM, HMM and
MABF aiding in selecting the most suitable approach for state estimations and
other energy-related tasks. Our generation and analysis framework aims to
enhance the accuracy and reliability of synthetic power consumption data while
generating data that fulfills criteria like anonymisation - preserving privacy
concerns mitigating risks of specific profiling of single customers. This study
utilizes an open-source dataset from households in Germany with 15min time
resolution. The generated synthetic power profiles can readily be used in
applications like state estimations or consumption forecasting.

</details>


### [91] [Argumentative Ensembling for Robust Recourse under Model Multiplicity](https://arxiv.org/abs/2506.20260)
*Junqi Jiang,Antonio Rago,Francesco Leofante,Francesca Toni*

Main category: cs.LG

TL;DR: 本文针对机器学习中模型多样性下反事实解释提供追索建议的问题，提出追索感知集成（RAE）问题，引入相关理想属性，提出新的论证集成方法并进行理论分析和实证验证。


<details>
  <summary>Details</summary>
Motivation: 在模型多样性（MM）情况下，反事实解释（CEs）可能在所有模型中都无效，提供追索建议变得复杂，需要解决MM下提供追索的问题。

Method: 提出追索感知集成（RAE）问题，引入六个理想属性，提出新的论证集成方法，利用计算论证明确表示模型和反事实之间的冲突，用论证语义解决冲突。

Result: 通过理论分析刻画了四种不同论证语义下论证集成的行为，用八种方法实例进行实证证明了方法在满足理想属性方面的有效性。

Conclusion: 提出的论证集成方法能保证MM下CEs的鲁棒性，可满足理想属性，且允许对模型进行偏好指定以进一步定制集成。

Abstract: In machine learning, it is common to obtain multiple equally performing
models for the same prediction task, e.g., when training neural networks with
different random seeds. Model multiplicity (MM) is the situation which arises
when these competing models differ in their predictions for the same input, for
which ensembling is often employed to determine an aggregation of the outputs.
Providing recourse recommendations via counterfactual explanations (CEs) under
MM thus becomes complex, since the CE may not be valid across all models, i.e.,
the CEs are not robust under MM. In this work, we formalise the problem of
providing recourse under MM, which we name recourse-aware ensembling (RAE). We
propose the idea that under MM, CEs for each individual model should be
considered alongside their predictions so that the aggregated prediction and
recourse are decided in tandem. Centred around this intuition, we introduce six
desirable properties for solutions to this problem. For solving RAE, we propose
a novel argumentative ensembling method which guarantees the robustness of CEs
under MM. Specifically, our method leverages computational argumentation to
explicitly represent the conflicts between models and counterfactuals regarding
prediction results and CE validity. It then uses argumentation semantics to
resolve the conflicts and obtain the final solution, in a manner which is
parametric to the chosen semantics. Our method also allows for the
specification of preferences over the models under MM, allowing further
customisation of the ensemble. In a comprehensive theoretical analysis, we
characterise the behaviour of argumentative ensembling with four different
argumentation semantics. We then empirically demonstrate the effectiveness of
our approach in satisfying desirable properties with eight instantiations of
our method. (Abstract is shortened for arXiv.)

</details>


### [92] [Distilling A Universal Expert from Clustered Federated Learning](https://arxiv.org/abs/2506.20285)
*Zeqi Leng,Chunxu Zhang,Guodong Long,Riting Xia,Bo Yang*

Main category: cs.LG

TL;DR: 本文提出一种新的联邦学习框架，从多集群知识中提炼通用专家模型，经三步迭代学习，实验证明该方法在多种场景下性能优越。


<details>
  <summary>Details</summary>
Motivation: 现有聚类联邦学习方法常忽略集群间共享信息，本文旨在克服这一局限。

Method: 提出新的联邦学习框架，包含本地模型训练、集群特定模型聚合和通用专家提炼三个迭代步骤。

Result: 与传统基于梯度的聚合方法相比，基于蒸馏的模型聚合在处理模型异质性上更灵活，减少集群专家间冲突。

Conclusion: 该方法能有效平衡个性化和共享知识，推动聚类联邦学习发展。

Abstract: Clustered Federated Learning (CFL) addresses the challenges posed by non-IID
data by training multiple group- or cluster-specific expert models. However,
existing methods often overlook the shared information across clusters, which
represents the generalizable knowledge valuable to all participants in the
Federated Learning (FL) system. To overcome this limitation, this paper
introduces a novel FL framework that distills a universal expert model from the
knowledge of multiple clusters. This universal expert captures globally shared
information across all clients and is subsequently distributed to each client
as the initialization for the next round of model training. The proposed FL
framework operates in three iterative steps: (1) local model training at each
client, (2) cluster-specific model aggregation, and (3) universal expert
distillation. This three-step learning paradigm ensures the preservation of
fine-grained non-IID characteristics while effectively incorporating shared
knowledge across clusters. Compared to traditional gradient-based aggregation
methods, the distillation-based model aggregation introduces greater
flexibility in handling model heterogeneity and reduces conflicts among
cluster-specific experts. Extensive experimental results demonstrate the
superior performance of the proposed method across various scenarios,
highlighting its potential to advance the state of CFL by balancing
personalized and shared knowledge more effectively.

</details>


### [93] [Learning Moderately Input-Sensitive Functions: A Case Study in QR Code Decoding](https://arxiv.org/abs/2506.20305)
*Kazuki Yoda,Kazuhiko Kawamoto,Hiroshi Kera*

Main category: cs.LG

TL;DR: 研究基于学习的二维码解码，发现Transformer可成功解码二维码，甚至突破理论纠错极限，且解码机制不同。


<details>
  <summary>Details</summary>
Motivation: 研究具有中等敏感度的学习函数，开展基于学习的二维码解码研究。

Method: 通过实验让Transformer学习二维码中嵌入文本的结构进行解码。

Result: Transformer能成功解码二维码，突破理论纠错极限，可从英文数据泛化到其他语言和随机字符串，且解码时关注数据位忽略纠错位。

Conclusion: Transformer有不同于标准二维码阅读器的解码机制。

Abstract: The hardness of learning a function that attains a target task relates to its
input-sensitivity. For example, image classification tasks are
input-insensitive as minor corruptions should not affect the classification
results, whereas arithmetic and symbolic computation, which have been recently
attracting interest, are highly input-sensitive as each input variable connects
to the computation results. This study presents the first learning-based Quick
Response (QR) code decoding and investigates learning functions of medium
sensitivity. Our experiments reveal that Transformers can successfully decode
QR codes, even beyond the theoretical error-correction limit, by learning the
structure of embedded texts. They generalize from English-rich training data to
other languages and even random strings. Moreover, we observe that the
Transformer-based QR decoder focuses on data bits while ignoring
error-correction bits, suggesting a decoding mechanism distinct from standard
QR code readers.

</details>


### [94] [Beyond-Expert Performance with Limited Demonstrations: Efficient Imitation Learning with Double Exploration](https://arxiv.org/abs/2506.20307)
*Heyang Zhao,Xingrui Yu,David M. Bossens,Ivor W. Tsang,Quanquan Gu*

Main category: cs.LG

TL;DR: 提出ILDE模仿学习算法，实现双探索，在样本效率上超现有算法，在Atari和MuJoCo任务上以更少演示达到超专家表现，并有理论证明。


<details>
  <summary>Details</summary>
Motivation: 解决从有限演示中准确学习专家策略的挑战，并探索环境以实现超专家表现。

Method: 提出Imitation Learning with Double Exploration (ILDE)算法，从乐观策略优化和好奇心驱动探索两方面实现探索。

Result: ILDE在样本效率上优于现有算法，在Atari和MuJoCo任务上以更少演示达到超专家表现。

Conclusion: ILDE可作为带乐观探索的不确定性正则化策略优化方法，后悔值在回合数上呈亚线性增长。

Abstract: Imitation learning is a central problem in reinforcement learning where the
goal is to learn a policy that mimics the expert's behavior. In practice, it is
often challenging to learn the expert policy from a limited number of
demonstrations accurately due to the complexity of the state space. Moreover,
it is essential to explore the environment and collect data to achieve
beyond-expert performance. To overcome these challenges, we propose a novel
imitation learning algorithm called Imitation Learning with Double Exploration
(ILDE), which implements exploration in two aspects: (1) optimistic policy
optimization via an exploration bonus that rewards state-action pairs with high
uncertainty to potentially improve the convergence to the expert policy, and
(2) curiosity-driven exploration of the states that deviate from the
demonstration trajectories to potentially yield beyond-expert performance.
Empirically, we demonstrate that ILDE outperforms the state-of-the-art
imitation learning algorithms in terms of sample efficiency and achieves
beyond-expert performance on Atari and MuJoCo tasks with fewer demonstrations
than in previous work. We also provide a theoretical justification of ILDE as
an uncertainty-regularized policy optimization method with optimistic
exploration, leading to a regret growing sublinearly in the number of episodes.

</details>


### [95] [Comparative Analysis of Deep Learning Models for Crop Disease Detection: A Transfer Learning Approach](https://arxiv.org/abs/2506.20323)
*Saundarya Subramaniam,Shalini Majumdar,Shantanu Nadar,Kaustubh Kulkarni*

Main category: cs.LG

TL;DR: 开发AI驱动的作物病害检测系统，对比深度学习模型，展示迁移学习在农业应用潜力


<details>
  <summary>Details</summary>
Motivation: 为资源有限的农村地区农民开发作物病害检测系统，比较不同深度学习模型在迁移学习中的有效性

Method: 利用EfficientNet、ResNet101、MobileNetV2和自定义CNN等深度学习模型进行作物病害分类

Result: 自定义CNN模型达到95.76%的验证准确率，系统能有效分类植物病害

Conclusion: 迁移学习有潜力重塑农业实践，改善作物健康管理，支持农村可持续农业

Abstract: This research presents the development of an Artificial Intelligence (AI) -
driven crop disease detection system designed to assist farmers in rural areas
with limited resources. We aim to compare different deep learning models for a
comparative analysis, focusing on their efficacy in transfer learning. By
leveraging deep learning models, including EfficientNet, ResNet101,
MobileNetV2, and our custom CNN, which achieved a validation accuracy of
95.76%, the system effectively classifies plant diseases. This research
demonstrates the potential of transfer learning in reshaping agricultural
practices, improving crop health management, and supporting sustainable farming
in rural environments.

</details>


### [96] [Permutation Equivariant Neural Controlled Differential Equations for Dynamic Graph Representation Learning](https://arxiv.org/abs/2506.20324)
*Torben Berndt,Benjamin Walker,Tiexin Qin,Jan Stühmer,Andrey Kormilitzin*

Main category: cs.LG

TL;DR: 提出置换等变神经图CDE，减少模型参数，提升训练效率和泛化能力，实验证明其优势。


<details>
  <summary>Details</summary>
Motivation: 动态图有复杂时间动态，虽已有图神经CDE，但为减少参数和提升效率，提出置换等变神经图CDE。

Method: 将图神经CDE投影到置换等变函数空间。

Result: 在模拟动力系统和现实任务实验中，在插值和外推场景表现更优。

Conclusion: 置换等变神经图CDE能减少参数，不损失表达能力，训练更高效，泛化性更好。

Abstract: Dynamic graphs exhibit complex temporal dynamics due to the interplay between
evolving node features and changing network structures. Recently, Graph Neural
Controlled Differential Equations (Graph Neural CDEs) successfully adapted
Neural CDEs from paths on Euclidean domains to paths on graph domains. Building
on this foundation, we introduce Permutation Equivariant Neural Graph CDEs,
which project Graph Neural CDEs onto permutation equivariant function spaces.
This significantly reduces the model's parameter count without compromising
representational power, resulting in more efficient training and improved
generalisation. We empirically demonstrate the advantages of our approach
through experiments on simulated dynamical systems and real-world tasks,
showing improved performance in both interpolation and extrapolation scenarios.

</details>


### [97] [Producer-Fairness in Sequential Bundle Recommendation](https://arxiv.org/abs/2506.20329)
*Alexandre Rio,Marta Soare,Sihem Amer-Yahia*

Main category: cs.LG

TL;DR: 本文研究顺序捆绑推荐中的公平性，提出精确解和两种启发式方法及自适应变体，实验验证了方法在保证推荐质量下提供公平捆绑推荐的有效性。


<details>
  <summary>Details</summary>
Motivation: 受现实场景启发，在顺序捆绑推荐中实现不同物品组在用户间的期望曝光，即生产者公平性。

Method: 提出精确解处理小实例问题，研究质量优先、公平优先两种启发式方法及自适应变体以平衡捆绑公平性和质量。

Result: 在三个真实数据集上的实验凸显了各解决方案的优缺点，证明它们能在不降低捆绑质量的情况下提供公平的捆绑推荐。

Conclusion: 所提方法在顺序捆绑推荐中能有效实现公平性且不影响推荐质量。

Abstract: We address fairness in the context of sequential bundle recommendation, where
users are served in turn with sets of relevant and compatible items. Motivated
by real-world scenarios, we formalize producer-fairness, that seeks to achieve
desired exposure of different item groups across users in a recommendation
session. Our formulation combines naturally with building high quality bundles.
Our problem is solved in real time as users arrive. We propose an exact
solution that caters to small instances of our problem. We then examine two
heuristics, quality-first and fairness-first, and an adaptive variant that
determines on-the-fly the right balance between bundle fairness and quality.
Our experiments on three real-world datasets underscore the strengths and
limitations of each solution and demonstrate their efficacy in providing fair
bundle recommendations without compromising bundle quality.

</details>


### [98] [DipSVD: Dual-importance Protected SVD for Efficient LLM Compression](https://arxiv.org/abs/2506.20353)
*Xuan Ding,Rui Sun,Yunjian Zhang,Xiu Yan,Yueqi Zhou,Kaihao Huang,Suzhong Fu,Chuanlong Xie,Yao Zhu*

Main category: cs.LG

TL;DR: 本文提出双级重要性保护机制DipSVD改进基于SVD的大语言模型压缩方法，实验显示其在多基准测试中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于SVD的大语言模型压缩方法注重整体矩阵差异，忽略矩阵内关键组件保护，导致压缩模型性能不佳。

Method: 提出双级重要性保护机制，包括通过通道加权数据白化进行局部重要性保护，以及通过启发式或基于优化的方法进行全局重要性保护。

Result: DipSVD在多个基准测试中优于现有基于SVD的压缩方法，在高压缩比下模型性能更优。

Conclusion: 双级重要性保护机制能有效增强基于SVD的大语言模型压缩方法。

Abstract: The ever-increasing computational demands and deployment costs of large
language models (LLMs) have spurred numerous compressing methods. Compared to
quantization and unstructured pruning, SVD compression offers superior hardware
compatibility and theoretical guarantees. However, existing SVD-based methods
focus on the overall discrepancy between the original and compressed matrices
while overlooking the protection of critical components within the matrix,
which leads to inferior performance in the compressed models. This paper
proposes a dual-level importance protection mechanism to enhance SVD-based
compression methods: (1) local importance protection: preserving the most
critical singular vectors within each weight matrix through channel-weighted
data whitening; and (2) global importance protection: enabling less important
layers to bear a greater portion of the compression burden through either a
heuristic or optimization-based approach, thereby minimizing the impact of
compression on critical layers. Extensive experiments demonstrate that DipSVD
outperforms existing SVD-based compression approaches across multiple
benchmarks, achieving superior model performance especially at high model
compression ratios.

</details>


### [99] [A foundation model with multi-variate parallel attention to generate neuronal activity](https://arxiv.org/abs/2506.20354)
*Francesco Carzaniga,Michael Hersche,Abu Sebastian,Kaspar Schindler,Abbas Rahimi*

Main category: cs.LG

TL;DR: 本文提出多变量并行注意力机制MVPA，构建生成式基础模型MVPFormer，发布SWEC iEEG数据集，模型在癫痫检测等任务表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决深度神经网络从具有异构通道配置的多变量时间序列中学习的挑战，特别是临床领域如颅内脑电图（iEEG）。

Method: 引入MVPA机制，用其构建MVPFormer模型，发布SWEC iEEG数据集。

Result: MVPFormer在不同受试者间泛化能力强，癫痫检测达专家水平，在多个数据集上优于基线模型，MVPA在标准时间序列任务中表现良好。

Conclusion: MVPA是异构时间序列通用注意力机制，MVPFormer是首个具有先进临床性能的开源、开放权重和开放数据的iEEG基础模型。

Abstract: Learning from multi-variate time-series with heterogeneous channel
configurations remains a fundamental challenge for deep neural networks (DNNs),
particularly in clinical domains such as intracranial electroencephalography
(iEEG), where channel setups vary widely across subjects. In this work, we
introduce multi-variate parallel attention (MVPA), a novel self-attention
mechanism that disentangles content, temporal, and spatial attention, enabling
flexible, generalizable, and efficient modeling of time-series data with
varying channel counts and configurations. We use MVPA to build MVPFormer, a
generative foundation model for human electrophysiology, trained to predict the
evolution of iEEG signals across diverse subjects. To support this and future
effort by the community, we release the SWEC iEEG dataset, the largest publicly
available iEEG dataset to date, comprising nearly 10,000 hours of recordings
from heterogeneous clinical sources. MVPFormer leverages MVPA to achieve strong
generalization across subjects, demonstrating expert-level performance in
seizure detection and outperforming state-of-the-art Transformer baselines on
our SWEC, the MAYO, and the FNUSA dataset. We further validate MVPA on standard
time-series forecasting and classification tasks, where it matches or exceeds
existing attention-based models. Together, our contributions establish MVPA as
a general-purpose attention mechanism for heterogeneous time-series and
MVPFormer as the first open-source, open-weights, and open-data iEEG foundation
model with state-of-the-art clinical performance. The code is available at
https://github.com/IBM/multi-variate-parallel-transformer. The SWEC iEEG
dataset is available at
https://mb-neuro.medical-blocks.ch/public_access/databases/ieeg/swec_ieeg.

</details>


### [100] [Towards Interpretable and Efficient Feature Selection in Trajectory Datasets: A Taxonomic Approach](https://arxiv.org/abs/2506.20359)
*Chanuka Don Samarasinghage,Dhruv Gulabani*

Main category: cs.LG

TL;DR: 本文介绍基于分类学的特征选择方法处理轨迹数据高维特征爆炸问题，该方法提升预测性能、减少特征选择时间并增强可解释性。


<details>
  <summary>Details</summary>
Motivation: 轨迹数据特征增多带来高维特征爆炸问题，降低机器学习模型效率、可解释性和准确性，需有效特征选择方法。

Method: 提出基于分类学的特征选择方法，将特征按内部结构分类为几何和运动学特征，再细分曲率、缩进、速度和加速度。

Result: 基于分类学的方法预测性能相当或更优，因分类减少组合空间，大幅缩短特征选择时间，还能洞察数据集对特征集的敏感度。

Conclusion: 基于分类学的特征选择方法可增加可解释性、降低维度和计算复杂度，为处理轨迹数据集的研究者和从业者提供方法框架，助力可解释人工智能领域。

Abstract: Trajectory analysis is not only about obtaining movement data, but it is also
of paramount importance in understanding the pattern in which an object moves
through space and time, as well as in predicting its next move. Due to the
significant interest in the area, data collection has improved substantially,
resulting in a large number of features becoming available for training and
predicting models. However, this introduces a high-dimensionality-induced
feature explosion problem, which reduces the efficiency and interpretability of
the data, thereby reducing the accuracy of machine learning models. To overcome
this issue, feature selection has become one of the most prevalent tools. Thus,
the objective of this paper was to introduce a taxonomy-based feature selection
method that categorizes features based on their internal structure. This
approach classifies the data into geometric and kinematic features, further
categorizing them into curvature, indentation, speed, and acceleration. The
comparative analysis indicated that a taxonomy-based approach consistently
achieved comparable or superior predictive performance. Furthermore, due to the
taxonomic grouping, which reduces combinatorial space, the time taken to select
features was drastically reduced. The taxonomy was also used to gain insights
into what feature sets each dataset was more sensitive to. Overall, this study
provides robust evidence that a taxonomy-based feature selection method can add
a layer of interpretability, reduce dimensionality and computational
complexity, and contribute to high-level decision-making. It serves as a step
toward providing a methodological framework for researchers and practitioners
dealing with trajectory datasets and contributing to the broader field of
explainable artificial intelligence.

</details>


### [101] [TESSERA: Temporal Embeddings of Surface Spectra for Earth Representation and Analysis](https://arxiv.org/abs/2506.20380)
*Zhengpeng Feng,Sadiq Jaffer,Jovana Knezevic,Silja Sormunen,Robin Young,Madeline Lisaius,Markus Immitzer,James Ball,Clement Atzberger,David A. Coomes,Anil Madhavapeddy,Andrew Blake,Srinivasan Keshav*

Main category: cs.LG

TL;DR: 介绍新型遥感基础模型TESSERA，用自监督学习生成10米尺度全球表征，性能超传统基线和领先模型。


<details>
  <summary>Details</summary>
Motivation: 为卫星遥感的下游地球观测应用提供更好的模型，生成高质量、高分辨率表征。

Method: 使用自监督学习，用两个基于Transformer的编码器分别处理光学和SAR数据流信息，再用多层感知机融合，生成2017 - 2024年全球表征图。

Result: 预计算表征设定了新的性能基准，在五个不同任务中表现优于传统遥感基线和领先的地理空间基础模型。

Conclusion: TESSERA模型性能出色，开源方法让更多人可获取高性能、高分辨率表征。

Abstract: Satellite remote sensing (RS) enables a wide array of downstream Earth
observation (EO) applications, including climate modeling, carbon accounting,
and strategies for conservation and sustainable land use. We present TESSERA, a
novel Remote Sensing Foundation Model (RSFM) that uses Self-Supervised Learning
(SSL) to generate global, robust representations at 10m scale from pixel-level
satellite time series data. TESSERA combines information from only optical and
SAR data streams using two parallel Transformer-based encoders: one dedicated
to Sentinel-1 SAR polarizations and another to Sentinel-2 MSI data (10 selected
spectral bands) to create representations that are then fused using a
multilayer perceptron (MLP), resulting in a global representation map covering
the years 2017 to 2024. Our precomputed representations set a new
state-of-the-art performance benchmark and our open-source approach
democratizes access to high-performance, high-resolution representations. We
benchmark the performance of TESSERA in five diverse tasks, comparing our work
with state-of-the-art task-specific models and other foundation models. Our
results show that TESSERA outperforms both traditional RS baselines and the
leading geospatial foundation models in these diverse downstream tasks.

</details>


### [102] [Client Clustering Meets Knowledge Sharing: Enhancing Privacy and Robustness in Personalized Peer-to-Peer Learning](https://arxiv.org/abs/2506.20413)
*Mohammad Mahdi Maheri,Denys Herasymuk,Hamed Haddadi*

Main category: cs.LG

TL;DR: 论文提出P4方法，为资源受限的物联网设备提供个性化模型，确保差分隐私和抗中毒攻击能力，实验显示其效果优于现有方法且实用性强。


<details>
  <summary>Details</summary>
Motivation: 人工智能在物联网生态系统的应用增加，需要能在异构、资源受限设备上高效且私密运行的个性化学习方法，但在分散环境中实现有效个性化学习面临诸多挑战。

Method: 开发P4方法，采用轻量级、全分散算法检测客户端相似度并形成协作组，组内利用差分隐私知识蒸馏共同训练模型。

Result: P4比领先的差分隐私点对点方法准确率高5% - 30%，在30%恶意客户端情况下保持鲁棒性，在资源受限设备上部署时两个客户端协作训练仅增加约7秒开销。

Conclusion: P4方法能有效解决在分散环境中为资源受限物联网设备实现个性化学习的挑战，具有较好的准确性、鲁棒性和实用性。

Abstract: The growing adoption of Artificial Intelligence (AI) in Internet of Things
(IoT) ecosystems has intensified the need for personalized learning methods
that can operate efficiently and privately across heterogeneous,
resource-constrained devices. However, enabling effective personalized learning
in decentralized settings introduces several challenges, including efficient
knowledge transfer between clients, protection of data privacy, and resilience
against poisoning attacks. In this paper, we address these challenges by
developing P4 (Personalized, Private, Peer-to-Peer) -- a method designed to
deliver personalized models for resource-constrained IoT devices while ensuring
differential privacy and robustness against poisoning attacks. Our solution
employs a lightweight, fully decentralized algorithm to privately detect client
similarity and form collaborative groups. Within each group, clients leverage
differentially private knowledge distillation to co-train their models,
maintaining high accuracy while ensuring robustness to the presence of
malicious clients. We evaluate P4 on popular benchmark datasets using both
linear and CNN-based architectures across various heterogeneity settings and
attack scenarios. Experimental results show that P4 achieves 5% to 30% higher
accuracy than leading differentially private peer-to-peer approaches and
maintains robustness with up to 30% malicious clients. Additionally, we
demonstrate its practicality by deploying it on resource-constrained devices,
where collaborative training between two clients adds only ~7 seconds of
overhead.

</details>


### [103] [Off-Policy Evaluation and Learning for the Future under Non-Stationarity](https://arxiv.org/abs/2506.20417)
*Tatsuhiro Shimizu,Kazuki Kawamura,Takanori Muroi,Yusuke Narita,Kei Tateno,Takuma Udagawa,Yuta Saito*

Main category: cs.LG

TL;DR: 研究非平稳环境下未来离策略评估（F - OPE）和学习（F - OPL）问题，提出OPFV估计器及策略梯度方法，实验表明性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决非平稳环境下估计和优化未来策略价值问题，现有方法假设平稳或有奖励建模限制，会产生显著偏差。

Method: 提出OPFV估计器，利用时间序列数据结构和新型重要性加权；扩展估计器开发新的策略梯度方法。

Result: 理论分析确定OPFV低偏差的条件；实证结果显示在非平稳环境下估计和优化未来策略价值方面，方法大幅优于现有方法。

Conclusion: 所提OPFV估计器和策略梯度方法能有效解决非平稳环境下未来策略价值估计和优化问题。

Abstract: We study the novel problem of future off-policy evaluation (F-OPE) and
learning (F-OPL) for estimating and optimizing the future value of policies in
non-stationary environments, where distributions vary over time. In e-commerce
recommendations, for instance, our goal is often to estimate and optimize the
policy value for the upcoming month using data collected by an old policy in
the previous month. A critical challenge is that data related to the future
environment is not observed in the historical data. Existing methods assume
stationarity or depend on restrictive reward-modeling assumptions, leading to
significant bias. To address these limitations, we propose a novel estimator
named \textit{\textbf{O}ff-\textbf{P}olicy Estimator for the \textbf{F}uture
\textbf{V}alue (\textbf{\textit{OPFV}})}, designed for accurately estimating
policy values at any future time point. The key feature of OPFV is its ability
to leverage the useful structure within time-series data. While future data
might not be present in the historical log, we can leverage, for example,
seasonal, weekly, or holiday effects that are consistent in both the historical
and future data. Our estimator is the first to exploit these time-related
structures via a new type of importance weighting, enabling effective F-OPE.
Theoretical analysis identifies the conditions under which OPFV becomes
low-bias. In addition, we extend our estimator to develop a new policy-gradient
method to proactively learn a good future policy using only historical data.
Empirical results show that our methods substantially outperform existing
methods in estimating and optimizing the future policy value under
non-stationarity for various experimental setups.

</details>


### [104] [Tackling Data Heterogeneity in Federated Learning through Knowledge Distillation with Inequitable Aggregation](https://arxiv.org/abs/2506.20431)
*Xing Ma*

Main category: cs.LG

TL;DR: 提出KDIA策略解决大规模客户端少量参与训练的联邦学习问题，实验表明其在多数据集和异构设置下效果好。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视大规模客户端少量参与训练场景，且客户端标签和数据量等异构问题会降低模型性能。

Method: 提出KDIA策略，学生模型是参与客户端平均聚合，教师模型按三种频率加权聚合所有客户端，本地训练时进行自知识蒸馏，用服务器训练的生成器生成数据特征辅助训练。

Result: KDIA在更少训练轮次下可实现更高准确率，在严重异构下改进更显著。

Conclusion: KDIA策略能有效利用所有客户端知识，在解决联邦学习异构问题上表现良好。

Abstract: Federated learning aims to train a global model in a distributed environment
that is close to the performance of centralized training. However, issues such
as client label skew, data quantity skew, and other heterogeneity problems
severely degrade the model's performance. Most existing methods overlook the
scenario where only a small portion of clients participate in training within a
large-scale client setting, whereas our experiments show that this scenario
presents a more challenging federated learning task. Therefore, we propose a
Knowledge Distillation with teacher-student Inequitable Aggregation (KDIA)
strategy tailored to address the federated learning setting mentioned above,
which can effectively leverage knowledge from all clients. In KDIA, the student
model is the average aggregation of the participating clients, while the
teacher model is formed by a weighted aggregation of all clients based on three
frequencies: participation intervals, participation counts, and data volume
proportions. During local training, self-knowledge distillation is performed.
Additionally, we utilize a generator trained on the server to generate
approximately independent and identically distributed (IID) data features
locally for auxiliary training. We conduct extensive experiments on the
CIFAR-10/100/CINIC-10 datasets and various heterogeneous settings to evaluate
KDIA. The results show that KDIA can achieve better accuracy with fewer rounds
of training, and the improvement is more significant under severe
heterogeneity.

</details>


### [105] [Méthode de quadrature pour les PINNs fondée théoriquement sur la hessienne des résiduels](https://arxiv.org/abs/2506.20441)
*Antoine Caradot,Rémi Emonet,Amaury Habrard,Abdel-Rahim Mezidi,Marc Sebban*

Main category: cs.LG

TL;DR: 本文提出基于函数海森矩阵的积分近似新方法，用于指导PINNs训练中配点选择。


<details>
  <summary>Details</summary>
Motivation: PINNs配点选择有改进空间，以往配点为均匀采样，近期有自适应采样改进。

Method: 提出基于函数海森矩阵的求积方法来近似定积分，并在PINNs训练过程中用该方法指导配点选择。

Result: 未提及。

Conclusion: 未提及。

Abstract: Physics-informed Neural Networks (PINNs) have emerged as an efficient way to
learn surrogate neural solvers of PDEs by embedding the physical model in the
loss function and minimizing its residuals using automatic differentiation at
so-called collocation points. Originally uniformly sampled, the choice of the
latter has been the subject of recent advances leading to adaptive sampling
refinements. In this paper, we propose a new quadrature method for
approximating definite integrals based on the hessian of the considered
function, and that we leverage to guide the selection of the collocation points
during the training process of PINNs.

</details>


### [106] [Automatic Demonstration Selection for LLM-based Tabular Data Classification](https://arxiv.org/abs/2506.20451)
*Shuchu Han,Wolfgang Bruckner*

Main category: cs.LG

TL;DR: 本文提出一种算法自动选择表格数据分类中上下文学习提示所需的示范数量，并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决表格数据分类中上下文学习提示里确定理想示范数量的问题。

Method: 结合表格数据分布、用户选择的提示模板和大语言模型进行估计，基于谱图理论定义新指标量化示范间相似度，构建相似度图并分析其拉普拉斯矩阵特征值来确定最小示范数量。

Result: 通过在不同数据集和大语言模型上与传统随机选择算法对比实验，验证了方法的有效性。

Conclusion: 提出的算法能有效自动选择表格数据分类中上下文学习提示所需的示范数量。

Abstract: A fundamental question in applying In-Context Learning (ICL) for tabular data
classification is how to determine the ideal number of demonstrations in the
prompt. This work addresses this challenge by presenting an algorithm to
automatically select a reasonable number of required demonstrations. Our method
distinguishes itself by integrating not only the tabular data's distribution
but also the user's selected prompt template and the specific Large Language
Model (LLM) into its estimation. Rooted in Spectral Graph Theory, our proposed
algorithm defines a novel metric to quantify the similarities between different
demonstrations. We then construct a similarity graph and analyze the
eigenvalues of its Laplacian to derive the minimum number of demonstrations
capable of representing the data within the LLM's intrinsic representation
space. We validate the efficacy of our approach through experiments comparing
its performance against conventional random selection algorithms on diverse
datasets and LLMs.

</details>


### [107] [Counterfactual Influence as a Distributional Quantity](https://arxiv.org/abs/2506.20481)
*Matthieu Meeus,Igor Shilov,Georgios Kaissis,Yves-Alexandre de Montjoye*

Main category: cs.LG

TL;DR: 本文研究机器学习模型记忆问题，将反事实影响视为分布量，发现仅看自我影响会低估记忆风险，记忆源于训练数据复杂交互，全影响分布更能捕捉记忆情况。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型记忆样本引发隐私和泛化问题，现有研究表明记忆受自我影响之外因素影响，因此研究将反事实影响作为分布量来研究记忆。

Method: 对于小语言模型，计算训练样本间的全影响分布并分析其属性，在图像分类中观察影响分布。

Result: 仅看自我影响会严重低估记忆相关风险，存在（近）重复样本会降低自我影响，但这些样本可（近）提取，在图像分类中能通过影响分布发现近重复样本。

Conclusion: 记忆源于训练数据的复杂交互，全影响分布比单一自我影响更能捕捉记忆情况。

Abstract: Machine learning models are known to memorize samples from their training
data, raising concerns around privacy and generalization. Counterfactual
self-influence is a popular metric to study memorization, quantifying how the
model's prediction for a sample changes depending on the sample's inclusion in
the training dataset. However, recent work has shown memorization to be
affected by factors beyond self-influence, with other training samples, in
particular (near-)duplicates, having a large impact. We here study memorization
treating counterfactual influence as a distributional quantity, taking into
account how all training samples influence how a sample is memorized. For a
small language model, we compute the full influence distribution of training
samples on each other and analyze its properties. We find that solely looking
at self-influence can severely underestimate tangible risks associated with
memorization: the presence of (near-)duplicates seriously reduces
self-influence, while we find these samples to be (near-)extractable. We
observe similar patterns for image classification, where simply looking at the
influence distributions reveals the presence of near-duplicates in CIFAR-10.
Our findings highlight that memorization stems from complex interactions across
training data and is better captured by the full influence distribution than by
self-influence alone.

</details>


### [108] [Multimodal Representation Learning and Fusion](https://arxiv.org/abs/2506.20494)
*Qihang Jin,Enze Ge,Yuhang Xie,Hongying Luo,Junhao Song,Ziqian Bi,Chia Xin Liang,Jibin Guan,Joe Yeong,Junfeng Hao*

Main category: cs.LG

TL;DR: 多模态学习是人工智能快速发展领域，结合多源信息使AI系统构建更强内部表征，但仍有问题，研究者探索新方法、关注评估指标，有望改善多领域。


<details>
  <summary>Details</summary>
Motivation: 帮助机器理解复杂事物，使AI系统构建更强、更丰富的内部表征，以更好地在现实场景中进行解释、推理和决策。

Method: 采用表征学习、对齐方法和融合策略等核心技术，探索无监督或半监督学习、AutoML工具等新方法，关注设计评估指标和构建共享基准。

Result: 虽取得一定进展，但仍存在处理不同数据格式、应对缺失输入和抵御对抗攻击等问题。

Conclusion: 多模态学习有望改善计算机视觉、自然语言处理等多个领域，未来可能构建更像人类的AI系统。

Abstract: Multi-modal learning is a fast growing area in artificial intelligence. It
tries to help machines understand complex things by combining information from
different sources, like images, text, and audio. By using the strengths of each
modality, multi-modal learning allows AI systems to build stronger and richer
internal representations. These help machines better interpretation, reasoning,
and making decisions in real-life situations. This field includes core
techniques such as representation learning (to get shared features from
different data types), alignment methods (to match information across
modalities), and fusion strategies (to combine them by deep learning models).
Although there has been good progress, some major problems still remain. Like
dealing with different data formats, missing or incomplete inputs, and
defending against adversarial attacks. Researchers now are exploring new
methods, such as unsupervised or semi-supervised learning, AutoML tools, to
make models more efficient and easier to scale. And also more attention on
designing better evaluation metrics or building shared benchmarks, make it
easier to compare model performance across tasks and domains. As the field
continues to grow, multi-modal learning is expected to improve many areas:
computer vision, natural language processing, speech recognition, and
healthcare. In the future, it may help to build AI systems that can understand
the world in a way more like humans, flexible, context aware, and able to deal
with real-world complexity.

</details>


### [109] [WallStreetFeds: Client-Specific Tokens as Investment Vehicles in Federated Learning](https://arxiv.org/abs/2506.20518)
*Arno Geimer,Beltran Fiz Pontiveros,Radu State*

Main category: cs.LG

TL;DR: 本文提出一种在联邦学习（FL）生态系统中引入特定客户端代币作为投资工具的新框架，以解决现有激励方案的局限性。


<details>
  <summary>Details</summary>
Motivation: 在营利性联邦学习中，奖励分配框架研究相对不足，需开发更灵活、可扩展的奖励分配系统。

Method: 利用去中心化金融（DeFi）平台和自动做市商（AMMs）创建新框架。

Result: 未提及具体结果。

Conclusion: 提出的框架可解决现有激励方案局限，为参与者提供更灵活可扩展的奖励分配系统，并为第三方投资联邦学习过程提供机制。

Abstract: Federated Learning (FL) is a collaborative machine learning paradigm which
allows participants to collectively train a model while training data remains
private. This paradigm is especially beneficial for sectors like finance, where
data privacy, security and model performance are paramount. FL has been
extensively studied in the years following its introduction, leading to, among
others, better performing collaboration techniques, ways to defend against
other clients trying to attack the model, and contribution assessment methods.
An important element in for-profit Federated Learning is the development of
incentive methods to determine the allocation and distribution of rewards for
participants. While numerous methods for allocation have been proposed and
thoroughly explored, distribution frameworks remain relatively understudied. In
this paper, we propose a novel framework which introduces client-specific
tokens as investment vehicles within the FL ecosystem. Our framework aims to
address the limitations of existing incentive schemes by leveraging a
decentralized finance (DeFi) platform and automated market makers (AMMs) to
create a more flexible and scalable reward distribution system for
participants, and a mechanism for third parties to invest in the federation
learning process.

</details>


### [110] [Asymmetric REINFORCE for off-Policy Reinforcement Learning: Balancing positive and negative rewards](https://arxiv.org/abs/2506.20520)
*Charles Arnal,Gaëtan Narozniak,Vivien Cabannes,Yunhao Tang,Julia Kempe,Remi Munos*

Main category: cs.LG

TL;DR: 研究介于离线策略强化学习和监督微调之间的算法，对离线策略REINFORCE算法进行理论分析并实验验证。


<details>
  <summary>Details</summary>
Motivation: 离线策略方法在对齐大语言模型时虽有实现简单和数据效率高的优点，但性能欠佳，因此研究中间范围算法。

Method: 分析简单的离线策略REINFORCE算法，对其进行理论分析，在随机多臂老虎机环境和推理任务微调大语言模型上进行实验验证。

Result: 理论分析表明，当基线V下界为期望奖励时，算法有策略改进保证；实验发现离线策略更新更应关注正奖励。

Conclusion: 研究的离线策略REINFORCE算法有一定优势，离线策略更新注重正奖励有益。

Abstract: Reinforcement learning (RL) is increasingly used to align large language
models (LLMs). Off-policy methods offer greater implementation simplicity and
data efficiency than on-policy techniques, but often result in suboptimal
performance. In this work, we study the intermediate range of algorithms
between off-policy RL and supervised fine-tuning by analyzing a simple
off-policy REINFORCE algorithm, where the advantage is defined as $A=r-V$, with
$r$ a reward and $V$ some tunable baseline. Intuitively, lowering $V$
emphasizes high-reward samples, while raising it penalizes low-reward ones more
heavily. We first provide a theoretical analysis of this off-policy REINFORCE
algorithm, showing that when the baseline $V$ lower-bounds the expected reward,
the algorithm enjoys a policy improvement guarantee. Our analysis reveals that
while on-policy updates can safely leverage both positive and negative signals,
off-policy updates benefit from focusing more on positive rewards than on
negative ones. We validate our findings experimentally in a controlled
stochastic bandit setting and through fine-tuning state-of-the-art LLMs on
reasoning tasks.

</details>


### [111] [Industrial Energy Disaggregation with Digital Twin-generated Dataset and Efficient Data Augmentation](https://arxiv.org/abs/2506.20525)
*Christian Internò,Andrea Castellani,Sebastian Schmitt,Fabio Stella,Barbara Hammer*

Main category: cs.LG

TL;DR: 提出开源数据集SIDED和数据增强方法AMDA，实验表明AMDA增强数据训练的NILM模型能提升复杂工业设备能耗分解效果。


<details>
  <summary>Details</summary>
Motivation: 解决工业非侵入式负荷监测（NILM）中高质量数据集稀缺和数据隐私问题。

Method: 使用数字孪生模拟生成开源数据集SIDED；提出计算高效的Appliance - Modulated Data Augmentation (AMDA)方法，基于电器相对影响智能缩放功率贡献来增强NILM模型泛化能力。

Result: AMDA增强数据训练的NILM模型在分解复杂工业电器能耗上显著提升，样本外场景中归一化分解误差为0.093，优于无数据增强（0.451）和随机数据增强（0.290）。

Conclusion: AMDA能有效对齐训练和测试数据分布，增强模型泛化能力。

Abstract: Industrial Non-Intrusive Load Monitoring (NILM) is limited by the scarcity of
high-quality datasets and the complex variability of industrial energy
consumption patterns. To address data scarcity and privacy issues, we introduce
the Synthetic Industrial Dataset for Energy Disaggregation (SIDED), an
open-source dataset generated using Digital Twin simulations. SIDED includes
three types of industrial facilities across three different geographic
locations, capturing diverse appliance behaviors, weather conditions, and load
profiles. We also propose the Appliance-Modulated Data Augmentation (AMDA)
method, a computationally efficient technique that enhances NILM model
generalization by intelligently scaling appliance power contributions based on
their relative impact. We show in experiments that NILM models trained with
AMDA-augmented data significantly improve the disaggregation of energy
consumption of complex industrial appliances like combined heat and power
systems. Specifically, in our out-of-sample scenarios, models trained with AMDA
achieved a Normalized Disaggregation Error of 0.093, outperforming models
trained without data augmentation (0.451) and those trained with random data
augmentation (0.290). Data distribution analyses confirm that AMDA effectively
aligns training and test data distributions, enhancing model generalization.

</details>


### [112] [Physics-Informed Machine Learning Regulated by Finite Element Analysis for Simulation Acceleration of Laser Powder Bed Fusion](https://arxiv.org/abs/2506.20537)
*R. Sharma,M. Raissi,Y. B. Guo*

Main category: cs.LG

TL;DR: 本文提出FEA - PINN框架加速激光粉末床熔融（LPBF）热场预测，降低计算成本并保证精度。


<details>
  <summary>Details</summary>
Motivation: 传统数值方法如有限元分析（FEA）计算成本高，需要高效模拟LPBF以进行过程预测。

Method: 提出FEA - PINN框架，开发动态材料更新策略，PINN模型结合表观热容法，框架在推理时集成校正FEA模拟。

Result: FEA - PINN与FEA精度相当，显著降低计算成本，通过基准FEA数据验证和单道扫描演示。

Conclusion: FEA - PINN框架能有效加速LPBF热场预测，解决PINN在时变问题中的高计算成本问题。

Abstract: Efficient simulation of Laser Powder Bed Fusion (LPBF) is crucial for process
prediction due to the lasting issue of high computation cost using traditional
numerical methods such as finite element analysis (FEA). This study presents an
efficient modeling framework termed FEA-Regulated Physics-Informed Neural
Network (FEA-PINN) to accelerate the thermal field prediction in a LPBF process
while maintaining the FEA accuracy. A novel dynamic material updating strategy
is developed to capture the dynamic phase change of powder-liquid-solid in the
PINN model. The PINN model incorporates temperature-dependent material
properties and phase change behavior using the apparent heat capacity method.
While the PINN model demonstrates high accuracy with a small training data and
enables generalization of new process parameters via transfer learning, it
faces the challenge of high computation cost in time-dependent problems due to
the residual accumulation. To overcome this issue, the FEA-PINN framework
integrates corrective FEA simulations during inference to enforce physical
consistency and reduce error drift. A comparative analysis shows that FEA-PINN
achieves equivalent accuracy to FEA while significantly reducing computational
cost. The framework has been validated using the benchmark FEA data and
demonstrated through single-track scanning in LPBF.

</details>


### [113] [Demonstration of effective UCB-based routing in skill-based queues on real-world data](https://arxiv.org/abs/2506.20543)
*Sanne van Kempen,Jaron Sanders,Fiona Sloothaak,Maarten G. Wolf*

Main category: cs.LG

TL;DR: 本文通过案例研究探讨强化学习算法在基于技能的排队系统最优客户路由中的应用，算法表现良好，还引入新规则、支持多目标优化并研究了敏感性。


<details>
  <summary>Details</summary>
Motivation: 对数据中心、云计算网络和服务系统等基于技能的排队系统进行最优控制。

Method: 使用真实数据集进行案例研究，采用最近开发的强化学习算法用于最优客户路由，引入新的启发式路由规则，利用调参平衡性能权衡。

Result: 算法能高效学习并适应变化环境，优于静态基准策略，可实现多目标优化。

Conclusion: 该算法在复杂现实排队系统中具有实时实施潜力，研究为自适应路由算法的实施提供了有价值的见解。

Abstract: This paper is about optimally controlling skill-based queueing systems such
as data centers, cloud computing networks, and service systems. By means of a
case study using a real-world data set, we investigate the practical
implementation of a recently developed reinforcement learning algorithm for
optimal customer routing. Our experiments show that the algorithm efficiently
learns and adapts to changing environments and outperforms static benchmark
policies, indicating its potential for live implementation. We also augment the
real-world applicability of this algorithm by introducing a new heuristic
routing rule to reduce delays. Moreover, we show that the algorithm can
optimize for multiple objectives: next to payoff maximization, secondary
objectives such as server load fairness and customer waiting time reduction can
be incorporated. Tuning parameters are used for balancing inherent performance
trade--offs. Lastly, we investigate the sensitivity to estimation errors and
parameter tuning, providing valuable insights for implementing adaptive routing
algorithms in complex real-world queueing systems.

</details>


### [114] [MIRAGE: A Benchmark for Multimodal Information-Seeking and Reasoning in Agricultural Expert-Guided Conversations](https://arxiv.org/abs/2506.20100)
*Vardhan Dongre,Chi Gui,Shubham Garg,Hooshang Nayyeri,Gokhan Tur,Dilek Hakkani-Tür,Vikram S. Adve*

Main category: cs.LG

TL;DR: 介绍农业领域多模态专家级推理和决策新基准MIRAGE，具有高保真度和丰富多样性。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏能全面捕捉专家咨询复杂性、适用于现实知识密集型领域的多模态基准，需要设计新基准来评估模型。

Method: 基于超35000个真实用户 - 专家交互，通过精心设计的多步流程进行整理。

Result: 构建出MIRAGE基准，涵盖超7000个独特生物实体，是视觉 - 语言模型中分类学最多样的真实世界基准。

Conclusion: MIRAGE相比现有基准，具有更复杂的开放世界场景，对模型推理和处理能力要求更高。

Abstract: We introduce MIRAGE, a new benchmark for multimodal expert-level reasoning
and decision-making in consultative interaction settings. Designed for the
agriculture domain, MIRAGE captures the full complexity of expert consultations
by combining natural user queries, expert-authored responses, and image-based
context, offering a high-fidelity benchmark for evaluating models on grounded
reasoning, clarification strategies, and long-form generation in a real-world,
knowledge-intensive domain. Grounded in over 35,000 real user-expert
interactions and curated through a carefully designed multi-step pipeline,
MIRAGE spans diverse crop health, pest diagnosis, and crop management
scenarios. The benchmark includes more than 7,000 unique biological entities,
covering plant species, pests, and diseases, making it one of the most
taxonomically diverse benchmarks available for vision-language models, grounded
in the real world. Unlike existing benchmarks that rely on well-specified user
inputs and closed-set taxonomies, MIRAGE features underspecified, context-rich
scenarios with open-world settings, requiring models to infer latent knowledge
gaps, handle rare entities, and either proactively guide the interaction or
respond. Project Page: https://mirage-benchmark.github.io

</details>


### [115] [Benchmarking Unsupervised Strategies for Anomaly Detection in Multivariate Time Series](https://arxiv.org/abs/2506.20574)
*Laura Boggia,Rafael Teixeira de Lima,Bogdan Malaescu*

Main category: cs.LG

TL;DR: 本文研究基于Transformer的时间序列异常检测方法，聚焦iTransformer架构，有四方面贡献，包括应用探索、标签提取、异常数据影响研究和模型对比。


<details>
  <summary>Details</summary>
Motivation: 多元时间序列异常检测在多领域重要，但因异常未知性和维度间复杂依赖而具挑战性。

Method: 研究基于iTransformer架构进行时间序列异常检测，探索关键参数影响、提取异常标签方法、评估不同损失函数效果，对比多个基于Transformer的模型。

Result: 未明确提及具体结果。

Conclusion: 未明确提及具体结论。

Abstract: Anomaly detection in multivariate time series is an important problem across
various fields such as healthcare, financial services, manufacturing or physics
detector monitoring. Accurately identifying when unexpected errors or faults
occur is essential, yet challenging, due to the unknown nature of anomalies and
the complex interdependencies between time series dimensions. In this paper, we
investigate transformer-based approaches for time series anomaly detection,
focusing on the recently proposed iTransformer architecture. Our contributions
are fourfold: (i) we explore the application of the iTransformer to time series
anomaly detection, and analyse the influence of key parameters such as window
size, step size, and model dimensions on performance; (ii) we examine methods
for extracting anomaly labels from multidimensional anomaly scores and discuss
appropriate evaluation metrics for such labels; (iii) we study the impact of
anomalous data present during training and assess the effectiveness of
alternative loss functions in mitigating their influence; and (iv) we present a
comprehensive comparison of several transformer-based models across a diverse
set of datasets for time series anomaly detection.

</details>


### [116] [Exploring Graph-Transformer Out-of-Distribution Generalization Abilities](https://arxiv.org/abs/2506.20575)
*Itay Niv,Neta Rabin*

Main category: cs.LG

TL;DR: 本文研究图神经网络的分布外泛化问题，评估不同骨干架构表现，发现图变换器和混合骨干架构泛化能力更强，并提出新的训练后分析方法。


<details>
  <summary>Details</summary>
Motivation: 当前图神经网络方法常假设训练和测试数据同分布，在现实中难满足，图变换器骨干在分布偏移下效果未知，需解决分布外泛化挑战。

Method: 系统评估图变换器和混合骨干架构在分布外设置下的表现，并与传统消息传递神经网络比较；适配多种领域泛化算法；提出新的训练后分析方法。

Result: 图变换器和混合骨干架构即使无专门领域泛化算法，也比传统消息传递神经网络泛化能力强；新分析方法对骨干架构有深入洞察，有更广泛适用性。

Conclusion: 图变换器在现实图学习中有前景，为分布外泛化研究指明新方向。

Abstract: Deep learning on graphs has shown remarkable success across numerous
applications, including social networks, bio-physics, traffic networks, and
recommendation systems. Regardless of their successes, current methods
frequently depend on the assumption that training and testing data share the
same distribution, a condition rarely met in real-world scenarios. While
graph-transformer (GT) backbones have recently outperformed traditional
message-passing neural networks (MPNNs) in multiple in-distribution (ID)
benchmarks, their effectiveness under distribution shifts remains largely
unexplored.
  In this work, we address the challenge of out-of-distribution (OOD)
generalization for graph neural networks, with a special focus on the impact of
backbone architecture. We systematically evaluate GT and hybrid backbones in
OOD settings and compare them to MPNNs. To do so, we adapt several leading
domain generalization (DG) algorithms to work with GTs and assess their
performance on a benchmark designed to test a variety of distribution shifts.
Our results reveal that GT and hybrid GT-MPNN backbones consistently
demonstrate stronger generalization ability compared to MPNNs, even without
specialized DG algorithms.
  Additionally, we propose a novel post-training analysis approach that
compares the clustering structure of the entire ID and OOD test datasets,
specifically examining domain alignment and class separation. Demonstrating its
model-agnostic design, this approach not only provided meaningful insights into
GT and MPNN backbones. It also shows promise for broader applicability to DG
problems beyond graph learning, offering a deeper perspective on generalization
abilities that goes beyond standard accuracy metrics. Together, our findings
highlight the promise of graph-transformers for robust, real-world graph
learning and set a new direction for future research in OOD generalization.

</details>


### [117] [The kernel of graph indices for vector search](https://arxiv.org/abs/2506.20584)
*Mariano Tepper,Ted Willke*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The most popular graph indices for vector search use principles from
computational geometry to build the graph. Hence, their formal graph
navigability guarantees are only valid in Euclidean space. In this work, we
show that machine learning can be used to build graph indices for vector search
in metric and non-metric vector spaces (e.g., for inner product similarity).
From this novel perspective, we introduce the Support Vector Graph (SVG), a new
type of graph index that leverages kernel methods to establish the graph
connectivity and that comes with formal navigability guarantees valid in metric
and non-metric vector spaces. In addition, we interpret the most popular graph
indices, including HNSW and DiskANN, as particular specializations of SVG and
show that new indices can be derived from the principles behind this
specialization. Finally, we propose SVG-L0 that incorporates an $\ell_0$
sparsity constraint into the SVG kernel method to build graphs with a bounded
out-degree. This yields a principled way of implementing this practical
requirement, in contrast to the traditional heuristic of simply truncating the
out edges of each node. Additionally, we show that SVG-L0 has a self-tuning
property that avoids the heuristic of using a set of candidates to find the
out-edges of each node and that keeps its computational complexity in check.

</details>


### [118] [H-FEX: A Symbolic Learning Method for Hamiltonian Systems](https://arxiv.org/abs/2506.20607)
*Jasen Lai,Senwei Liang,Chunmei Wang*

Main category: cs.LG

TL;DR: 提出用于学习哈密顿系统的有限表达式方法H - FEX，能有效恢复复杂系统的哈密顿函数并保持能量守恒。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动方法在准确捕捉复杂哈密顿函数并保持能量守恒方面存在困难。

Method: 提出H - FEX，引入新颖交互节点以有效捕捉复杂交互项。

Result: 实验表明H - FEX能恢复复杂系统的哈密顿函数，准确捕捉系统动力学并在长时间内保持能量守恒。

Conclusion: H - FEX是发现复杂动力系统封闭形式表达式的强大框架。

Abstract: Hamiltonian systems describe a broad class of dynamical systems governed by
Hamiltonian functions, which encode the total energy and dictate the evolution
of the system. Data-driven approaches, such as symbolic regression and neural
network-based methods, provide a means to learn the governing equations of
dynamical systems directly from observational data of Hamiltonian systems.
However, these methods often struggle to accurately capture complex Hamiltonian
functions while preserving energy conservation. To overcome this limitation, we
propose the Finite Expression Method for learning Hamiltonian Systems (H-FEX),
a symbolic learning method that introduces novel interaction nodes designed to
capture intricate interaction terms effectively. Our experiments, including
those on highly stiff dynamical systems, demonstrate that H-FEX can recover
Hamiltonian functions of complex systems that accurately capture system
dynamics and preserve energy over long time horizons. These findings highlight
the potential of H-FEX as a powerful framework for discovering closed-form
expressions of complex dynamical systems.

</details>


### [119] [Efficient Federated Learning with Encrypted Data Sharing for Data-Heterogeneous Edge Devices](https://arxiv.org/abs/2506.20644)
*Hangyu Li,Hongyue Wu,Guodong Fan,Zhen Zhang,Shizhan Chen,Zhiyong Feng*

Main category: cs.LG

TL;DR: 现有联邦学习研究忽略部分因素致问题，提出FedEDS方案，用客户端模型等训练数据加密器，加速收敛，实验证明其有效。


<details>
  <summary>Details</summary>
Motivation: 当前研究忽略网络拓扑、物理距离和数据异质性对边缘设备的影响，导致延迟增加和模型性能下降。

Method: 提出Federated Learning with Encrypted Data Sharing (FedEDS)方案，用客户端模型和随机层训练数据加密器，生成加密数据共享，用本地和共享加密数据训练模型。

Result: 实验结果表明FedEDS能促进模型性能。

Conclusion: FedEDS加速了联邦学习训练的收敛速度，减轻了数据异质性的负面影响，适用于需要快速收敛的边缘设备应用服务。

Abstract: As privacy protection gains increasing importance, more models are being
trained on edge devices and subsequently merged into the central server through
Federated Learning (FL). However, current research overlooks the impact of
network topology, physical distance, and data heterogeneity on edge devices,
leading to issues such as increased latency and degraded model performance. To
address these issues, we propose a new federated learning scheme on edge
devices that called Federated Learning with Encrypted Data Sharing(FedEDS).
FedEDS uses the client model and the model's stochastic layer to train the data
encryptor. The data encryptor generates encrypted data and shares it with other
clients. The client uses the corresponding client's stochastic layer and
encrypted data to train and adjust the local model. FedEDS uses the client's
local private data and encrypted shared data from other clients to train the
model. This approach accelerates the convergence speed of federated learning
training and mitigates the negative impact of data heterogeneity, making it
suitable for application services deployed on edge devices requiring rapid
convergence. Experiments results show the efficacy of FedEDS in promoting model
performance.

</details>


### [120] [A Spatio-Temporal Point Process for Fine-Grained Modeling of Reading Behavior](https://arxiv.org/abs/2506.19999)
*Francesco Ignazio Re,Andreas Opedal,Glib Manaiev,Mario Giulianelli,Ryan Cotterell*

Main category: cs.LG

TL;DR: 提出基于标记时空点过程的阅读行为概率模型，比基线更拟合人类扫视，发现意外性理论难解释精细眼动。


<details>
  <summary>Details</summary>
Motivation: 标准阅读行为建模方法依赖聚合眼动测量和强假设模型，忽略时空动态，需更通用模型。

Method: 基于标记时空点过程构建阅读行为概率模型，用Hawkes过程模拟扫视，将注视事件持续时间建模为特定预测因子的时间卷积函数。

Result: Hawkes过程模型比基线更拟合人类扫视；加入语境意外性作为预测因子对模型预测准确性提升有限。

Conclusion: 意外性理论难以解释精细眼动。

Abstract: Reading is a process that unfolds across space and time, alternating between
fixations where a reader focuses on a specific point in space, and saccades
where a reader rapidly shifts their focus to a new point. An ansatz of
psycholinguistics is that modeling a reader's fixations and saccades yields
insight into their online sentence processing. However, standard approaches to
such modeling rely on aggregated eye-tracking measurements and models that
impose strong assumptions, ignoring much of the spatio-temporal dynamics that
occur during reading. In this paper, we propose a more general probabilistic
model of reading behavior, based on a marked spatio-temporal point process,
that captures not only how long fixations last, but also where they land in
space and when they take place in time. The saccades are modeled using a Hawkes
process, which captures how each fixation excites the probability of a new
fixation occurring near it in time and space. The duration time of fixation
events is modeled as a function of fixation-specific predictors convolved
across time, thus capturing spillover effects. Empirically, our Hawkes process
model exhibits a better fit to human saccades than baselines. With respect to
fixation durations, we observe that incorporating contextual surprisal as a
predictor results in only a marginal improvement in the model's predictive
accuracy. This finding suggests that surprisal theory struggles to explain
fine-grained eye movements.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [121] [Higher-Order Neuromorphic Ising Machines -- Autoencoders and Fowler-Nordheim Annealers are all you need for Scalability](https://arxiv.org/abs/2506.19964)
*Faiek Ahsan,Saptarshi Maiti,Zihao Chen,Jakob Kaiser,Ankita Nandi,Madhuvanthi Srivatsav,Johannes Schemmel,Andreas G. Andreou,Jason Eshraghian,Chetan Singh Thakur,Shantanu Chakrabartty*

Main category: cs.NE

TL;DR: 提出高阶神经形态伊辛机，相比二阶模型在解的质量、时间和可靠性上更优，还可结合稀疏技术和硬件协同设计。


<details>
  <summary>Details</summary>
Motivation: 开发具有更好可扩展性、解质量和可靠性的伊辛机。

Method: 采用异步自编码器架构直接处理伊辛子句，基于Fowler - Nordheim量子隧穿退火动力学采样自编码器潜在空间。

Result: 在解决MAX - CUT和MAX - SAT等问题时，高阶伊辛机比二阶模型能在更短时间内提供更高质量解，可应用稀疏技术，硬件协同设计能进一步改善求解时间。

Conclusion: 自编码器和Fowler - Nordheim退火器足以实现任意阶神经形态伊辛机的可靠性和扩展性。

Abstract: We report a higher-order neuromorphic Ising machine that exhibits superior
scalability compared to architectures based on quadratization, while also
achieving state-of-the-art quality and reliability in solutions with
competitive time-to-solution metrics. At the core of the proposed machine is an
asynchronous autoencoder architecture that captures higher-order interactions
by directly manipulating Ising clauses instead of Ising spins, thereby
maintaining resource complexity independent of interaction order. Asymptotic
convergence to the Ising ground state is ensured by sampling the autoencoder
latent space defined by the spins, based on the annealing dynamics of the
Fowler-Nordheim quantum mechanical tunneling. To demonstrate the advantages of
the proposed higher-order neuromorphic Ising machine, we systematically solved
benchmark combinatorial optimization problems such as MAX-CUT and MAX-SAT,
comparing the results to those obtained using a second-order Ising machine
employing the same annealing process. Our findings indicate that the proposed
architecture consistently provides higher quality solutions in shorter time
frames compared to the second-order model across multiple runs. Additionally,
we show that the techniques based on the sparsity of the interconnection
matrix, such as graph coloring, can be effectively applied to higher-order
neuromorphic Ising machines, enhancing the solution quality and the
time-to-solution. The time-to-solution can be further improved through hardware
co-design, as demonstrated in this paper using a field-programmable gate array
(FPGA). The results presented in this paper provide further evidence that
autoencoders and Fowler-Nordheim annealers are sufficient to achieve
reliability and scaling of any-order neuromorphic Ising machines.

</details>


### [122] [Surrogate-Assisted Evolution for Efficient Multi-branch Connection Design in Deep Neural Networks](https://arxiv.org/abs/2506.20469)
*Fergal Stapleton,Daniel García Núñez,Yanan Sun,Edgar Galván*

Main category: cs.NE

TL;DR: 本文提出NeuroLGP - MB方法用进化算法自动发现DNN高性能架构，使用代理辅助EA并引入更优代理模型解决训练计算成本高和架构选择难题。


<details>
  <summary>Details</summary>
Motivation: 现有多分支连接DNN训练计算成本高，且难以确定最优网络架构。

Method: 利用进化算法自动发现高性能架构，提出基于线性遗传编程的NeuroLGP - MB方法编码多分支连接，使用代理辅助EA，结合语义方法并引入更高级代理模型。

Result: 未提及具体实验结果。

Conclusion: 未明确总结性结论，但暗示所提方法有助于解决现有DNN训练和架构设计问题。

Abstract: State-of-the-art Deep Neural Networks (DNNs) often incorporate multi-branch
connections, enabling multi-scale feature extraction and enhancing the capture
of diverse features. This design improves network capacity and generalisation
to unseen data. However, training such DNNs can be computationally expensive.
The challenge is further exacerbated by the complexity of identifying optimal
network architectures. To address this, we leverage Evolutionary Algorithms
(EAs) to automatically discover high-performing architectures, a process
commonly known as neuroevolution. We introduce a novel approach based on Linear
Genetic Programming (LGP) to encode multi-branch (MB) connections within DNNs,
referred to as NeuroLGP-MB. To efficiently design the DNNs, we use
surrogate-assisted EAs. While their application in simple artificial neural
networks has been influential, we scale their use from dozens or hundreds of
sample points to thousands, aligning with the demands of complex DNNs by
incorporating a semantic-based approach in our surrogate-assisted EA.
Furthermore, we introduce a more advanced surrogate model that outperforms
baseline, computationally expensive, and simpler surrogate models.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [123] [Can LLMs Replace Humans During Code Chunking?](https://arxiv.org/abs/2506.19897)
*Christopher Glasz,Emily Escamilla,Eric O. Scott,Anand Patel,Jacob Zimmer,Colin Diggs,Michael Doyle,Scott Rosen,Nitin Naik,Justin F. Brunelle,Samruddhi Thaker,Parthav Poudel,Arun Sridharan,Amit Madan,Doug Wendt,William Macke,Thomas Schill*

Main category: cs.SE

TL;DR: 本文探讨大语言模型（LLMs）在政府遗留代码现代化中的应用，研究代码分块方法对文档生成质量的影响，发现LLMs分块表现出色，可替代人工分块。


<details>
  <summary>Details</summary>
Motivation: 现有工作未解决政府应用代码的独特挑战，如使用遗留语言、代码长度超LLMs上下文窗口，且LLMs对遗留语言理解待研究，因此研究LLMs在政府遗留代码现代化中的应用。

Method: 研究各种代码分块方法，评估其对不同LLMs生成遗留代码文件摘要模块注释质量的影响。

Result: LLMs能选择与人类专家分区相近的分区点，分块方法对下游文档生成任务有显著影响，LLM创建的分区生成的注释比人工分区更具事实性和实用性。

Conclusion: 在LLM辅助的现代化过程中，LLMs可作为大型代码库人工分区的合适替代品。

Abstract: Large language models (LLMs) have become essential tools in computer science,
especially for tasks involving code understanding and generation. However,
existing work does not address many of the unique challenges presented by code
written for government applications. In particular, government enterprise
software is often written in legacy languages like MUMPS or assembly language
code (ALC) and the overall token lengths of these systems exceed the context
window size for current commercially available LLMs. Additionally, LLMs are
primarily trained on modern software languages and have undergone limited
testing with legacy languages, making their ability to understand legacy
languages unknown and, hence, an area for empirical study. This paper examines
the application of LLMs in the modernization of legacy government code written
in ALC and MUMPS, addressing the challenges of input limitations. We
investigate various code-chunking methods to optimize the generation of summary
module comments for legacy code files, evaluating the impact of code-chunking
methods on the quality of documentation produced by different LLMs, including
GPT-4o, Claude 3 Sonnet, Mixtral, and Llama 3. Our results indicate that LLMs
can select partition points closely aligned with human expert partitioning. We
also find that chunking approaches have significant impact on downstream tasks
such as documentation generation. LLM-created partitions produce comments that
are up to 20% more factual and up to 10% more useful than when humans create
partitions. Therefore, we conclude that LLMs can be used as suitable
replacements for human partitioning of large codebases during LLM-aided
modernization.

</details>


### [124] [When Domains Collide: An Activity Theory Exploration of Cross-Disciplinary Collaboration](https://arxiv.org/abs/2506.20063)
*Zixuan Feng,Thomas Zimmermann,Lorenzo Pisani,Christopher Gooley,Jeremiah Wander,Anita Sarma*

Main category: cs.SE

TL;DR: 本文研究跨学科软件开发（CDSD）中领域专家（DEs）和专业软件开发人员（SDEs）协作的动态和摩擦，通过活动理论和混合研究方法得出结果并提供见解。


<details>
  <summary>Details</summary>
Motivation: 软件开发团队日益多元化、跨学科，但不同期望、视角和优先级导致摩擦，需研究CDSD新兴协作动态及摩擦表现。

Method: 运用活动理论，采用混合研究方法，包括24次访谈（12名DEs和12名SDEs）和293名参与者的大规模验证调查。

Result: 概念化并实证了CDSD动态，确定了SDEs的8种期望和DEs的6种期望，揭示21种摩擦及产生方式。

Conclusion: 提供理解CDSD动态和摩擦的理论视角，为未来研究、从业者和基础设施设计提供可行见解。

Abstract: Background: Software development teams are increasingly diverse, embedded,
and cross-disciplinary. Domain experts (DEs) from different disciplines
collaborate with professional software developers (SDEs), bringing
complementary expertise in creating and maintaining complex production
software. However, contested expectations, divergent problem-solving
perspectives, and conflicting priorities lead to friction. Aims: This study
aims to investigate the dynamics of emerging collaboration of
cross-disciplinary software development (CDSD) by exploring the expectations
held by DEs and SDEs and understanding how these frictions manifest in
practice. Method: We utilize Activity Theory (AT), a well-established
socio-technical framework, as an analytical lens in a grounded, empirical
investigation, conducted through a mixed-method study involving 24 interviews
(12 DEs and 12 SDEs) and a large-scale validation survey with 293 participants
(161 DEs and 132 SDEs). Results: We conceptualize and empirically ground the
CDSD dynamics. We identified eight expectations held by SDEs and six by DEs. By
mapping these expectations to AT components, we revealed 21 frictions in CDSD
and illustrated where and how they arise. Conclusions: This study offers a
theoretical lens for understanding the dynamics and frictions in CDSD and
provides actionable insights for future research, practitioners, and
infrastructure design.

</details>


### [125] [AI and Agile Software Development: From Frustration to Success -- XP2025 Workshop Summary](https://arxiv.org/abs/2506.20159)
*Tomas Herda,Victoria Pichler,Zheying Zhang,Pekka Abrahamsson,Geir K. Hanssen*

Main category: cs.SE

TL;DR: XP 2025举办AI与敏捷开发研讨会，探讨AI融入敏捷软件开发挑战与机遇，制定研究路线图。


<details>
  <summary>Details</summary>
Motivation: 解决将人工智能融入敏捷软件开发的实际挑战和探索机遇。

Method: 通过互动环节，参与者确定挑战，系统地对挑战进行优先级排序和分析以找出根本原因。

Result: 协作开发出研究路线图，明确未来工作的可行动方向。

Conclusion: 制定了结构化议程，促进产学研共同努力实现从发现问题到成功实施的转变。

Abstract: The full-day workshop on AI and Agile at XP 2025 convened a diverse group of
researchers and industry practitioners to address the practical challenges and
opportunities of integrating Artificial Intelligence into Agile software
development. Through interactive sessions, participants identified shared
frustrations related to integrating AI into Agile Software Development
practices, including challenges with tooling, governance, data quality, and
critical skill gaps. These challenges were systematically prioritized and
analyzed to uncover root causes. The workshop culminated in the collaborative
development of a research roadmap that pinpoints actionable directions for
future work, including both immediate solutions and ambitious long-term goals.
The key outcome is a structured agenda designed to foster joint
industry-academic efforts to move from identified frustrations to successful
implementation.

</details>


### [126] [Ten simple rules for PIs to integrate Research Software Engineering into their research group](https://arxiv.org/abs/2506.20217)
*Stuart M. Allen,Neil Chue Hong,Stephan Druskat,Toby Hodges,Daniel S. Katz,Jan Linxweiler,Frank Löffler,Lars Grunske,Heidi Seibold,Jan Philipp Thiele,Samantha Wittke*

Main category: cs.SE

TL;DR: 本文提出十条简单规则以提高研究软件工程（RSEng）的可及性，助力研究人员将其融入研究组。


<details>
  <summary>Details</summary>
Motivation: 很多研究组负责人不了解RSEng，且RSEng有技术复杂性，可及性低。

Method: 提出十条简单规则。

Result: 读者可提高研究软件的质量、可重复性和可信度。

Conclusion: 遵循规则能带来更好、更具可重复性和可信度的研究成果。

Abstract: Research Software Engineering (RSEng) is a key success factor in producing
high-quality research software, which in turn enables and improves research
outcomes. However, as a principal investigator or leader of a research group
you may not know what RSEng is, where to get started with it, or how to use it
to maximize its benefit for your research. RSEng also often comes with
technical complexity, and therefore reduced accessibility to some researchers.
The ten simple rules presented in this paper aim to improve the accessibility
of RSEng, and provide practical and actionable advice to PIs and leaders for
integrating RSEng into their research group. By following these rules, readers
can improve the quality, reproducibility, and trustworthiness of their research
software, ultimately leading to better, more reproducible and more trustworthy
research outcomes.

</details>


### [127] [The Composition of Digital Twins for Systems-of-Systems: a Systematic Literature Review](https://arxiv.org/abs/2506.20435)
*Mennatullah T. Khedr,John S. Fitzgerald*

Main category: cs.SE

TL;DR: 本文对数字孪生（DT）的组成、验证与确认（V&V）方法进行系统文献综述，分析现状与挑战，提出需标准化可扩展的V&V及严格的组成方法。


<details>
  <summary>Details</summary>
Motivation: 研究数字孪生在复杂系统尤其是网络物理系统（CPS）和系统之系统（SoS）中有效集成的关键，即DT组成和V&V方法。

Method: 分析2022 - 2024年的21项研究，考察组成机制、SoS特性以及V&V的形式、范围和挑战。

Result: 组成的形式化有限；V&V方法多样，半正式方法和模拟占主导，形式验证未充分利用；存在模型不确定性和集成复杂性等技术挑战，缺乏标准化DT特定的V&V框架。

Conclusion: 贡献了V&V方法的结构化分类，强调复杂DT实现需要标准化、可扩展的V&V和严格的组成方法。

Abstract: Digital Twins (DTs) are increasingly used to model complex systems,
especially in Cyber-Physical Systems (CPS) and System-of-Systems (SoS), where
effective integration is key. This systematic literature review investigates DT
composition and verification and validation (V&V) methodologies. Analyzing 21
studies from 2022-2024, we examined composition mechanisms, SoS
characteristics, and V&V formality, scope, and challenges. While composition is
discussed, formalization is limited. V&V approaches vary, with semi-formal
methods and simulations dominating; formal verification is underutilized. Key
technical challenges include model uncertainty and integration complexity.
Methodological challenges highlight the lack of standardized DT-specific V&V
frameworks. There is a need to move beyond model validation to address
integration and cyber-physical consistency. This review contributes a
structured classification of V&V approaches and emphasizes the need for
standardized, scalable V&V and rigorous composition methodologies for complex
DT implementations.

</details>


### [128] [Smart Cuts: Enhance Active Learning for Vulnerability Detection by Pruning Bad Seeds](https://arxiv.org/abs/2506.20444)
*Xiang Lan,Tim Menzies,Bowen Xu*

Main category: cs.SE

TL;DR: 提出基于数据集映射的方法识别和减轻难学习的异常值，提高漏洞检测模型训练效率，实验显示效果良好。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在漏洞检测领域受低质量训练数据集影响，含噪声、错误标签或不平衡样本，影响模型效果。

Method: 提出新的数据集映射方法，按学习难度对训练样本分类，并集成到主动学习框架，过滤有害样本，强调信息丰富样本。

Result: 在Big - Vul数据集上，该方法比随机选择提高F1分数，比标准主动学习表现更好，还增强模型鲁棒性、改善样本选择和稳定主动学习性能。

Conclusion: 该方法有效优化样本选择，为未来数据集构建提供见解，使漏洞检测更可靠、成本更低。

Abstract: Vulnerability detection is crucial for identifying security weaknesses in
software systems. However, the effectiveness of machine learning models in this
domain is often hindered by low-quality training datasets, which contain noisy,
mislabeled, or imbalanced samples. This paper proposes a novel dataset
maps-empowered approach that systematically identifies and mitigates
hard-to-learn outliers, referred to as "bad seeds", to improve model training
efficiency. Our approach can categorize training examples based on learning
difficulty and integrate this information into an active learning framework.
Unlike traditional methods that focus on uncertainty-based sampling, our
strategy prioritizes dataset quality by filtering out performance-harmful
samples while emphasizing informative ones. Our experimental results show that
our approach can improve F1 score over random selection by 45.36% (DeepGini)
and 45.91% (K-Means) and outperforms standard active learning by 61.46%
(DeepGini) and 32.65% (K-Means) for CodeBERT on the Big-Vul dataset,
demonstrating the effectiveness of integrating dataset maps for optimizing
sample selection in vulnerability detection. Furthermore, our approach also
enhances model robustness, improves sample selection by filtering bad seeds,
and stabilizes active learning performance across iterations. By analyzing the
characteristics of these outliers, we provide insights for future improvements
in dataset construction, making vulnerability detection more reliable and
cost-effective.

</details>


### [129] [Large Language Model-Driven Code Compliance Checking in Building Information Modeling](https://arxiv.org/abs/2506.20551)
*Soumya Madireddy,Lu Gao,Zia Din,Kinam Kim,Ahmed Senouci,Zhe Han,Yunpeng Zhang*

Main category: cs.SE

TL;DR: 研究引入大语言模型驱动的方法半自动化建筑信息模型（BIM）代码合规性检查，经案例验证可省时、提效、增准，是有前景的进展。


<details>
  <summary>Details</summary>
Motivation: 解决手动进行BIM代码合规性检查耗时且易出错的问题。

Method: 将GPT、Claude、Gemini和Llama等大语言模型与Revit软件集成，用于解释建筑规范、生成Python脚本并在BIM环境中进行半自动化合规性检查。

Result: 通过单户住宅和办公楼项目案例，系统减少了合规性检查的时间和精力，提高了准确性，简化了违规识别，消除了重复任务。

Conclusion: 该方法是基于BIM合规性检查的有前景进展，可应用于建筑项目不同监管文件。

Abstract: This research addresses the time-consuming and error-prone nature of manual
code compliance checking in Building Information Modeling (BIM) by introducing
a Large Language Model (LLM)-driven approach to semi-automate this critical
process. The developed system integrates LLMs such as GPT, Claude, Gemini, and
Llama, with Revit software to interpret building codes, generate Python
scripts, and perform semi-automated compliance checks within the BIM
environment. Case studies on a single-family residential project and an office
building project demonstrated the system's ability to reduce the time and
effort required for compliance checks while improving accuracy. It streamlined
the identification of violations, such as non-compliant room dimensions,
material usage, and object placements, by automatically assessing relationships
and generating actionable reports. Compared to manual methods, the system
eliminated repetitive tasks, simplified complex regulations, and ensured
reliable adherence to standards. By offering a comprehensive, adaptable, and
cost-effective solution, this proposed approach offers a promising advancement
in BIM-based compliance checking, with potential applications across diverse
regulatory documents in construction projects.

</details>


### [130] [CCISolver: End-to-End Detection and Repair of Method-Level Code-Comment Inconsistency](https://arxiv.org/abs/2506.20558)
*Renyi Zhong,Yintong Huo,Wenwei Gu,Jinxi Kuang,Zhihan Jiang,Guangba Yu,Yichen Li,David Lo,Michael R. Lyu*

Main category: cs.SE

TL;DR: 本文指出代码注释不一致问题及现有研究不足，提出CCIBench数据集和CCISolver框架，评估显示CCISolver性能优越且具实用性。


<details>
  <summary>Details</summary>
Motivation: 代码注释不一致影响软件开发等，现有研究存在数据集不准确和解决方案不足的问题。

Method: 对现有数据集定量分析，引入CCIBench数据集，提出基于大语言模型的CCISolver框架。

Result: CCISolver检测F1分数达89.54%，修复任务GLEU分数相对提升18.84%，人工评估修复成功率0.6533，推理速度比基线模型快约36%。

Conclusion: CCISolver性能优越，具有可扩展性和实际应用价值。

Abstract: Comments within code serve as a crucial foundation for software
documentation, facilitating developers to communicate and understand the code
effectively. However, code-comment inconsistency (CCI) can negatively affect
software development, testing, and maintenance. Recent efforts to mitigate this
issue have emerged, but existing studies often suffer from inaccurate datasets
and inadequate solutions, weakening their practical effectiveness. In this
study, we first conduct a quantitative analysis of existing datasets, revealing
a substantial portion of sampled data are mislabeled. To address these data
limitations, we introduce CCIBench, a refined dataset comprising high-quality
data, to support the training and evaluation of method-level CCI methods.
Furthermore, we present an innovative end-to-end LLM-based framework,
CCISolver, designed to improve code quality by identifying and rectifying CCIs.
Comprehensive evaluations demonstrate CCISolver's superior performance. For
detection, it establishes a new state-of-the-art with an F1-score of 89.54%. In
fixing task, it achieves a remarkable 18.84% relative improvement in GLEU score
over the strongest baseline. This superiority is confirmed by human evaluation,
where CCISolver's fixing success rate of 0.6533 significantly surpasses
existing methods. Critically, in a practical end-to-end setting, CCISolver's
innovative architecture is approximately 36% faster for inference than the
baseline model, underscoring its scalability and real-world applicability.

</details>


### [131] [Define-ML: An Approach to Ideate Machine Learning-Enabled Systems](https://arxiv.org/abs/2506.20621)
*Silvio Alonso,Antonio Pedro Santos Alves,Lucas Romao,Hélio Lopes,Marcos Kalinowski*

Main category: cs.SE

TL;DR: 本文提出Define - ML框架，在Lean Inception基础上增加活动，用于早期ML产品构思，经验证有效且参与者有采纳意向。


<details>
  <summary>Details</summary>
Motivation: 机器学习在软件系统中应用增多，传统构思方法缺乏对ML特定挑战的支持，易导致产品愿景不一致和期望不现实。

Method: 按照技术转移模型开发和验证Define - ML，进行静态（玩具问题）和动态（实际工业案例）验证，结合定量调查和定性反馈评估。

Result: 参与者认为Define - ML能澄清数据问题、使ML能力与业务目标一致、促进跨职能协作，减少构思模糊性，但存在学习曲线，可由专家缓解，所有参与者有意采纳。

Conclusion: Define - ML是一种公开可用、经过验证的ML产品构思方法，在Lean Inception基础上使功能与可用数据对齐，提高技术可行性意识。

Abstract: [Context] The increasing adoption of machine learning (ML) in software
systems demands specialized ideation approaches that address ML-specific
challenges, including data dependencies, technical feasibility, and alignment
between business objectives and probabilistic system behavior. Traditional
ideation methods like Lean Inception lack structured support for these ML
considerations, which can result in misaligned product visions and unrealistic
expectations. [Goal] This paper presents Define-ML, a framework that extends
Lean Inception with tailored activities - Data Source Mapping, Feature-to-Data
Source Mapping, and ML Mapping - to systematically integrate data and technical
constraints into early-stage ML product ideation. [Method] We developed and
validated Define-ML following the Technology Transfer Model, conducting both
static validation (with a toy problem) and dynamic validation (in a real-world
industrial case study). The analysis combined quantitative surveys with
qualitative feedback, assessing utility, ease of use, and intent of adoption.
[Results] Participants found Define-ML effective for clarifying data concerns,
aligning ML capabilities with business goals, and fostering cross-functional
collaboration. The approach's structured activities reduced ideation ambiguity,
though some noted a learning curve for ML-specific components, which can be
mitigated by expert facilitation. All participants expressed the intention to
adopt Define-ML. [Conclusion] Define-ML provides an openly available, validated
approach for ML product ideation, building on Lean Inception's agility while
aligning features with available data and increasing awareness of technical
feasibility.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [132] [Supervised Similarity for Firm Linkages](https://arxiv.org/abs/2506.19856)
*Ryan Samson,Adrian Banner,Luca Candelori,Sebastien Cottrell,Tiziana Di Matteo,Paul Duchnowski,Vahagn Kirakosyan,Jose Marques,Kharen Musaelian,Stefano Pasquali,Ryan Stever,Dario Villani*

Main category: q-fin.ST

TL;DR: 引入特征向量链接（CVLs）作为企业关联的新代理，用欧氏相似度和量子认知机器学习（QCML）估计企业关联，发现两种方法可构建盈利策略且QCML更优。


<details>
  <summary>Details</summary>
Motivation: 引入新的企业关联代理并探索有效估计方法以构建盈利交易策略。

Method: 先通过欧氏相似度，后应用量子认知机器学习（QCML）进行相似度学习来估计企业关联。

Result: 两种方法都能构建盈利的动量溢出交易策略，且QCML相似度表现优于简单的欧氏相似度。

Conclusion: 新提出的CVLs代理和QCML方法在构建盈利交易策略方面有优势，QCML效果更好。

Abstract: We introduce a novel proxy for firm linkages, Characteristic Vector Linkages
(CVLs). We use this concept to estimate firm linkages, first through Euclidean
similarity, and then by applying Quantum Cognition Machine Learning (QCML) to
similarity learning. We demonstrate that both methods can be used to construct
profitable momentum spillover trading strategies, but QCML similarity
outperforms the simpler Euclidean similarity.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [133] [Data-Driven Dynamic Factor Modeling via Manifold Learning](https://arxiv.org/abs/2506.19945)
*Graeme Baker,Agostino Capponi,J. Antonio Sidaoui*

Main category: stat.ML

TL;DR: 提出数据驱动动态因子框架，利用各向异性扩散映射挖掘联合动态，应用于股票投资组合压力测试，表现优于基准。


<details>
  <summary>Details</summary>
Motivation: 在不施加参数模型情况下，处理响应变量依赖高维协变量的联合动态问题。

Method: 利用各向异性扩散映射挖掘联合动态，用线性扩散近似嵌入动态，通过卡尔曼滤波预测，推广收敛率分析并证明方法合理性。

Result: 应用于股票投资组合压力测试，在三次重大金融危机历史回测中，较标准情景分析和主成分分析基准，情景组合回报预测平均绝对误差分别降低达55%和39%。

Conclusion: 数据驱动的压力测试方法优于标准情景分析和主成分分析基准。

Abstract: We propose a data-driven dynamic factor framework where a response variable
depends on a high-dimensional set of covariates, without imposing any
parametric model on the joint dynamics. Leveraging Anisotropic Diffusion Maps,
a nonlinear manifold learning technique introduced by Singer and Coifman, our
framework uncovers the joint dynamics of the covariates and responses in a
purely data-driven way. We approximate the embedding dynamics using linear
diffusions, and exploit Kalman filtering to predict the evolution of the
covariates and response variables directly from the diffusion map embedding
space. We generalize Singer's convergence rate analysis of the graph Laplacian
from the case of independent uniform samples on a compact manifold to the case
of time series arising from Langevin diffusions in Euclidean space.
Furthermore, we provide rigorous justification for our procedure by showing the
robustness of approximations of the diffusion map coordinates by linear
diffusions, and the convergence of ergodic averages under standard spectral
assumptions on the underlying dynamics. We apply our method to the stress
testing of equity portfolios using a combination of financial and macroeconomic
factors from the Federal Reserve's supervisory scenarios. We demonstrate that
our data-driven stress testing method outperforms standard scenario analysis
and Principal Component Analysis benchmarks through historical backtests
spanning three major financial crises, achieving reductions in mean absolute
error of up to 55% and 39% for scenario-based portfolio return prediction,
respectively.

</details>


### [134] [A Principled Path to Fitted Distributional Evaluation](https://arxiv.org/abs/2506.20048)
*Sungee Hong,Jiayi Wang,Zhengling Qi,Raymond Ka Wai Wong*

Main category: stat.ML

TL;DR: 本文将适用于基于期望的强化学习的fitted - Q评估扩展到分布性离线策略评估（OPE）设置，提出构建FDE方法的原则，开发新方法并给出收敛分析，实验证明其性能优越。


<details>
  <summary>Details</summary>
Motivation: 分布性OPE中缺乏设计fitted distributional evaluation (FDE)方法的统一框架。

Method: 提出构建理论上合理的FDE方法的指导原则，基于这些原则开发新的FDE方法并进行收敛分析。

Result: 广泛实验（线性二次调节器模拟和Atari游戏）表明FDE方法性能优越。

Conclusion: 所提出的构建原则和开发的FDE方法是有效的，能在非表格环境中为现有方法提供理论依据。

Abstract: In reinforcement learning, distributional off-policy evaluation (OPE) focuses
on estimating the return distribution of a target policy using offline data
collected under a different policy. This work focuses on extending the widely
used fitted-Q evaluation -- developed for expectation-based reinforcement
learning -- to the distributional OPE setting. We refer to this extension as
fitted distributional evaluation (FDE). While only a few related approaches
exist, there remains no unified framework for designing FDE methods. To fill
this gap, we present a set of guiding principles for constructing theoretically
grounded FDE methods. Building on these principles, we develop several new FDE
methods with convergence analysis and provide theoretical justification for
existing methods, even in non-tabular environments. Extensive experiments,
including simulations on linear quadratic regulators and Atari games,
demonstrate the superior performance of the FDE methods.

</details>


### [135] [Extracting Interpretable Models from Tree Ensembles: Computational and Statistical Perspectives](https://arxiv.org/abs/2506.20114)
*Brian Liu,Rahul Mazumder,Peter Radchenko*

Main category: stat.ML

TL;DR: 提出从树集成中提取决策规则集的估计器，开发求解算法，建立误差界，实验显示其优于现有算法。


<details>
  <summary>Details</summary>
Motivation: 树集成模型难以解释且可能无法发现数据有用关系，需要一种方法提取可解释规则。

Method: 提出估计器，开发精确算法和近似算法求解优化问题，建立非渐近预测误差界。

Result: 估计器的大样本预测性能与理论最优相当，实验中优于现有规则提取算法。

Conclusion: 所提估计器能有效从树集成中提取准确且可解释的决策规则集。

Abstract: Tree ensembles are non-parametric methods widely recognized for their
accuracy and ability to capture complex interactions. While these models excel
at prediction, they are difficult to interpret and may fail to uncover useful
relationships in the data. We propose an estimator to extract compact sets of
decision rules from tree ensembles. The extracted models are accurate and can
be manually examined to reveal relationships between the predictors and the
response. A key novelty of our estimator is the flexibility to jointly control
the number of rules extracted and the interaction depth of each rule, which
improves accuracy. We develop a tailored exact algorithm to efficiently solve
optimization problems underlying our estimator and an approximate algorithm for
computing regularization paths, sequences of solutions that correspond to
varying model sizes. We also establish novel non-asymptotic prediction error
bounds for our proposed approach, comparing it to an oracle that chooses the
best data-dependent linear combination of the rules in the ensemble subject to
the same complexity constraint as our estimator. The bounds illustrate that the
large-sample predictive performance of our estimator is on par with that of the
oracle. Through experiments, we demonstrate that our estimator outperforms
existing algorithms for rule extraction.

</details>


### [136] [Scalable Subset Selection in Linear Mixed Models](https://arxiv.org/abs/2506.20425)
*Ryan Thompson,Matt P. Wand,Joanna J. J. Wang*

Main category: stat.ML

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Linear mixed models (LMMs), which incorporate fixed and random effects, are
key tools for analyzing heterogeneous data, such as in personalized medicine or
adaptive marketing. Nowadays, this type of data is increasingly wide, sometimes
containing thousands of candidate predictors, necessitating sparsity for
prediction and interpretation. However, existing sparse learning methods for
LMMs do not scale well beyond tens or hundreds of predictors, leaving a large
gap compared with sparse methods for linear models, which ignore random
effects. This paper closes the gap with a new $\ell_0$ regularized method for
LMM subset selection that can run on datasets containing thousands of
predictors in seconds to minutes. On the computational front, we develop a
coordinate descent algorithm as our main workhorse and provide a guarantee of
its convergence. We also develop a local search algorithm to help traverse the
nonconvex optimization surface. Both algorithms readily extend to subset
selection in generalized LMMs via a penalized quasi-likelihood approximation.
On the statistical front, we provide a finite-sample bound on the
Kullback-Leibler divergence of the new method. We then demonstrate its
excellent performance in synthetic experiments and illustrate its utility on
two datasets from biology and journalism.

</details>


### [137] [Valid Selection among Conformal Sets](https://arxiv.org/abs/2506.20173)
*Mahmoud Hegazy,Liviu Aolaritei,Michael I. Jordan,Aymeric Dieuleveut*

Main category: stat.ML

TL;DR: 提出基于稳定性的方法确保所选共形预测集的覆盖率，扩展到在线共形设置并实验验证有效性。


<details>
  <summary>Details</summary>
Motivation: 实际中存在多个有效共形预测集，选择最理想集合可能使覆盖率保证失效。

Method: 提出基于稳定性的方法，扩展到在线共形设置并在有额外结构的设置中提出改进。

Result: 通过实验证明了所提方法的有效性。

Conclusion: 所提基于稳定性的方法能确保所选预测集的覆盖率。

Abstract: Conformal prediction offers a distribution-free framework for constructing
prediction sets with coverage guarantees. In practice, multiple valid conformal
prediction sets may be available, arising from different models or
methodologies. However, selecting the most desirable set, such as the smallest,
can invalidate the coverage guarantees. To address this challenge, we propose a
stability-based approach that ensures coverage for the selected prediction set.
We extend our results to the online conformal setting, propose several
refinements in settings where additional structure is available, and
demonstrate its effectiveness through experiments.

</details>


### [138] [POLAR: A Pessimistic Model-based Policy Learning Algorithm for Dynamic Treatment Regimes](https://arxiv.org/abs/2506.20406)
*Ruijia Zhang,Zhengling Qi,Yue Wu,Xiangyu Zhang,Yanxun Xu*

Main category: stat.ML

TL;DR: 提出POLAR算法优化离线动态治疗方案，有理论保障，实证效果佳。


<details>
  <summary>Details</summary>
Motivation: 现有统计方法和离线强化学习方法在优化动态治疗方案时存在依赖强假设、缺乏统计保障等问题。

Method: 提出POLAR算法，从离线数据估计转移动态，量化历史 - 动作对的不确定性，在奖励函数中加入悲观惩罚。

Result: 在合成数据和MIMIC - III数据集上，POLAR优于现有方法，产生接近最优、考虑历史的治疗策略。

Conclusion: POLAR是首个为动态治疗方案提供统计和计算保障的基于模型的方法。

Abstract: Dynamic treatment regimes (DTRs) provide a principled framework for
optimizing sequential decision-making in domains where decisions must adapt
over time in response to individual trajectories, such as healthcare,
education, and digital interventions. However, existing statistical methods
often rely on strong positivity assumptions and lack robustness under partial
data coverage, while offline reinforcement learning approaches typically focus
on average training performance, lack statistical guarantees, and require
solving complex optimization problems. To address these challenges, we propose
POLAR, a novel pessimistic model-based policy learning algorithm for offline
DTR optimization. POLAR estimates the transition dynamics from offline data and
quantifies uncertainty for each history-action pair. A pessimistic penalty is
then incorporated into the reward function to discourage actions with high
uncertainty. Unlike many existing methods that focus on average training
performance, POLAR directly targets the suboptimality of the final learned
policy and offers theoretical guarantees, without relying on computationally
intensive minimax or constrained optimization procedures. To the best of our
knowledge, POLAR is the first model-based DTR method to provide both
statistical and computational guarantees, including finite-sample bounds on
policy suboptimality. Empirical results on both synthetic data and the
MIMIC-III dataset demonstrate that POLAR outperforms state-of-the-art methods
and yields near-optimal, history-aware treatment strategies.

</details>


### [139] [Global Convergence of Iteratively Reweighted Least Squares for Robust Subspace Recovery](https://arxiv.org/abs/2506.20533)
*Gilad Lerman,Kang Li,Tyler Maunu,Teng Zhang*

Main category: stat.ML

TL;DR: 本文研究IRLS在鲁棒子空间估计中的应用，证明其在确定性条件下线性收敛，拓展到仿射子空间估计并应用于神经网络训练。


<details>
  <summary>Details</summary>
Motivation: IRLS在鲁棒子空间估计中经验有效，但理论性质不明，且仿射子空间估计缺乏恢复理论。

Method: 分析一种带动态平滑正则化的IRLS变体，在确定性条件下研究其收敛性。

Result: 证明IRLS变体从任意初始化线性收敛到潜在子空间，拓展到仿射子空间估计，通过神经网络训练展示其实际益处。

Conclusion: 为鲁棒子空间恢复的IRLS及黎曼流形上的非凸IRLS提供了首个全局收敛保证。

Abstract: Robust subspace estimation is fundamental to many machine learning and data
analysis tasks. Iteratively Reweighted Least Squares (IRLS) is an elegant and
empirically effective approach to this problem, yet its theoretical properties
remain poorly understood. This paper establishes that, under deterministic
conditions, a variant of IRLS with dynamic smoothing regularization converges
linearly to the underlying subspace from any initialization. We extend these
guarantees to affine subspace estimation, a setting that lacks prior recovery
theory. Additionally, we illustrate the practical benefits of IRLS through an
application to low-dimensional neural network training. Our results provide the
first global convergence guarantees for IRLS in robust subspace recovery and,
more broadly, for nonconvex IRLS on a Riemannian manifold.

</details>


### [140] [LARP: Learner-Agnostic Robust Data Prefiltering](https://arxiv.org/abs/2506.20573)
*Kristian Minchev,Dimitar Iliev Dimitrov,Nikola Konstantinov*

Main category: stat.ML

TL;DR: 论文形式化了LARP问题，在标量均值估计中实例化框架，给出硬度结果并分析过滤程序，实验表明LARP在异构学习者集上有性能损失，用博弈论框架分析权衡并展示其对大数据集的好处。


<details>
  <summary>Details</summary>
Motivation: 公共数据集存在低质量或受污染数据，许多学习程序对此敏感，需构建学习者无关的鲁棒数据预过滤方法。

Method: 形式化Learner - Agnostic Robust data Prefiltering (LARP)问题，在标量均值估计中实例化框架，提供硬度结果，分析预过滤程序，进行真实数据实验，用博弈论框架建模。

Result: 在异构学习者集上执行LARP与为每个学习者单独过滤数据相比有模型性能损失，实验观察到效用显著降低。

Conclusion: 在博弈论框架下分析了效用下降和重复（特定于学习者）预过滤成本的权衡，展示了LARP对大数据集的好处。

Abstract: The widespread availability of large public datasets is a key factor behind
the recent successes of statistical inference and machine learning methods.
However, these datasets often contain some low-quality or contaminated data, to
which many learning procedures are sensitive. Therefore, the question of
whether and how public datasets should be prefiltered to facilitate accurate
downstream learning arises. On a technical level this requires the construction
of principled data prefiltering methods which are learner-agnostic robust, in
the sense of provably protecting a set of pre-specified downstream learners
from corrupted data. In this work, we formalize the problem of Learner-Agnostic
Robust data Prefiltering (LARP), which aims at finding prefiltering procedures
that minimize a worst-case loss over a pre-specified set of learners. We first
instantiate our framework in the context of scalar mean estimation with Huber
estimators under the Huber data contamination model. We provide a hardness
result on a specific problem instance and analyze several natural prefiltering
procedures. Our theoretical results indicate that performing LARP on a
heterogeneous set of learners leads to some loss in model performance compared
to the alternative of prefiltering data for each learner/use-case individually.
We explore the resulting utility loss and its dependence on the problem
parameters via extensive experiments on real-world image and tabular data,
observing statistically significant reduction in utility. Finally, we model the
trade-off between the utility drop and the cost of repeated (learner-specific)
prefiltering within a game-theoretic framework and showcase benefits of LARP
for large datasets.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [141] [DualEquiNet: A Dual-Space Hierarchical Equivariant Network for Large Biomolecules](https://arxiv.org/abs/2506.19862)
*Junjie Xu,Jiahao Zhang,Mangal Prakash,Xiang Zhang,Suhang Wang*

Main category: q-bio.BM

TL;DR: 现有几何GNN在大分子生物建模有挑战，提出DualEquiNet，在双空间构建互补表示，有双向跨空间消息传递和交互池化机制，在多个基准测试表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有几何GNN在应用于RNA和蛋白质等大分子生物建模时面临可扩展性和表达能力的挑战，无法同时捕捉细粒度原子相互作用、长程依赖和生物相关的层次结构。

Method: 引入DualEquiNet，在欧几里得和球谐空间构建互补表示，采用双向跨空间消息传递和新颖的跨空间交互池化机制，将原子特征分层聚合为有生物意义的单元。

Result: DualEquiNet在多个现有RNA属性预测和蛋白质建模基准测试中达到了最先进的性能，在两个新引入的3D结构基准测试中优于先前的方法。

Conclusion: DualEquiNet在一系列大分子生物建模任务中具有广泛的有效性。

Abstract: Geometric graph neural networks (GNNs) that respect E(3) symmetries have
achieved strong performance on small molecule modeling, but they face
scalability and expressiveness challenges when applied to large biomolecules
such as RNA and proteins. These systems require models that can simultaneously
capture fine-grained atomic interactions, long-range dependencies across
spatially distant components, and biologically relevant hierarchical structure,
such as atoms forming residues, which in turn form higher-order domains.
Existing geometric GNNs, which typically operate exclusively in either
Euclidean or Spherical Harmonics space, are limited in their ability to capture
both the fine-scale atomic details and the long-range, symmetry-aware
dependencies required for modeling the multi-scale structure of large
biomolecules. We introduce DualEquiNet, a Dual-Space Hierarchical Equivariant
Network that constructs complementary representations in both Euclidean and
Spherical Harmonics spaces to capture local geometry and global symmetry-aware
features. DualEquiNet employs bidirectional cross-space message passing and a
novel Cross-Space Interaction Pooling mechanism to hierarchically aggregate
atomic features into biologically meaningful units, such as residues, enabling
efficient and expressive multi-scale modeling for large biomolecular systems.
DualEquiNet achieves state-of-the-art performance on multiple existing
benchmarks for RNA property prediction and protein modeling, and outperforms
prior methods on two newly introduced 3D structural benchmarks demonstrating
its broad effectiveness across a range of large biomolecule modeling tasks.

</details>


### [142] [Scalable and Cost-Efficient de Novo Template-Based Molecular Generation](https://arxiv.org/abs/2506.19865)
*Piotr Gaiński,Oussama Boussif,Andrei Rekesh,Dmytro Shevchuk,Ali Parviz,Mike Tyers,Robert A. Batey,Michał Koziarski*

Main category: q-bio.BM

TL;DR: 提出递归成本引导框架、开发动态库机制解决模板分子生成的三个核心挑战，取得了模板分子生成的最优结果。


<details>
  <summary>Details</summary>
Motivation: 解决基于模板的GFlowNets中最小化合成成本、扩展到大构建块库、有效利用小片段集三个核心挑战。

Method: 提出递归成本引导，采用辅助机器学习模型近似合成成本和可行性；引入开发惩罚平衡探索与利用；开发动态库机制在小构建块库中重用中间高回报状态构建完整合成树。

Result: 所提方法显著提高了成本效率、分子多样性和质量，建立了模板分子生成的最优结果。

Conclusion: 所提方法在模板分子生成中有效且具有先进性。

Abstract: Template-based molecular generation offers a promising avenue for drug design
by ensuring generated compounds are synthetically accessible through predefined
reaction templates and building blocks. In this work, we tackle three core
challenges in template-based GFlowNets: (1) minimizing synthesis cost, (2)
scaling to large building block libraries, and (3) effectively utilizing small
fragment sets. We propose \textbf{Recursive Cost Guidance}, a backward policy
framework that employs auxiliary machine learning models to approximate
synthesis cost and viability. This guidance steers generation toward low-cost
synthesis pathways, significantly enhancing cost-efficiency, molecular
diversity, and quality, especially when paired with an \textbf{Exploitation
Penalty} that balances the trade-off between exploration and exploitation. To
enhance performance in smaller building block libraries, we develop a
\textbf{Dynamic Library} mechanism that reuses intermediate high-reward states
to construct full synthesis trees. Our approach establishes state-of-the-art
results in template-based molecular generation.

</details>


<div id='hep-ph'></div>

# hep-ph [[Back]](#toc)

### [143] [DeepQuark: deep-neural-network approach to multiquark bound states](https://arxiv.org/abs/2506.20555)
*Wei-Lin Wu,Lu Meng,Shi-Lin Zhu*

Main category: hep-ph

TL;DR: 首次用基于深度神经网络的变分蒙特卡罗方法研究多夸克束缚态，设计DeepQuark架构，性能良好，有多种成果，对扩展研究和探索禁闭机制有意义。


<details>
  <summary>Details</summary>
Motivation: 多夸克系统因强SU(3)色相互作用，复杂度超电子或核子系统，需新方法研究。

Method: 实现基于深度神经网络的变分蒙特卡罗方法，设计DeepQuark架构。

Result: 在核子、重四夸克和五夸克系统表现有竞争力，如描述多种粒子，得到弱束缚态。

Conclusion: DeepQuark有望扩展到更大多夸克系统，可探索多夸克态中超越两体相互作用的禁闭机制。

Abstract: For the first time, we implement the deep-neural-network-based variational
Monte Carlo approach for the multiquark bound states, whose complexity
surpasses that of electron or nucleon systems due to strong SU(3) color
interactions. We design a novel and high-efficiency architecture, DeepQuark, to
address the unique challenges in multiquark systems such as stronger
correlations, extra discrete quantum numbers, and intractable confinement
interaction. Our method demonstrates competitive performance with
state-of-the-art approaches, including diffusion Monte Carlo and Gaussian
expansion method, in the nucleon, doubly heavy tetraquark, and fully heavy
tetraquark systems. Notably, it outperforms existing calculations for
pentaquarks, exemplified by the triply heavy pentaquark. For the nucleon, we
successfully incorporate three-body flux-tube confinement interactions without
additional computational costs. In tetraquark systems, we consistently describe
hadronic molecule $T_{cc}$ and compact tetraquark $T_{bb}$ with an unbiased
form of wave function ansatz. In the pentaquark sector, we obtain weakly bound
$\bar D^*\Xi_{cc}^*$ molecule $P_{cc\bar c}(5715)$ with $S=\frac{5}{2}$ and its
bottom partner $P_{bb\bar b}(15569)$. They can be viewed as the analogs of the
molecular $T_{cc}$. We recommend experimental search of $P_{cc\bar c}(5715)$ in
the D-wave $J/\psi \Lambda_c$ channel. DeepQuark holds great promise for
extension to larger multiquark systems, overcoming the computational barriers
in conventional methods. It also serves as a powerful framework for exploring
confining mechanism beyond two-body interactions in multiquark states, which
may offer valuable insights into nonperturbative QCD and general many-body
physics.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [144] [Finite-Time Information-Theoretic Bounds in Queueing Control](https://arxiv.org/abs/2506.18278)
*Yujie Liu,Vincent Y. F. Tan,Yunbei Xu*

Main category: math.OC

TL;DR: 本文为随机处理网络调度问题的总队列长度建立有限时间信息论下界，推导新策略，揭示MaxWeight在有限时间的局限性。


<details>
  <summary>Details</summary>
Motivation: 以往对MaxWeight的分析仅保证稳定性和重负载下的渐近最优性，需研究其在有限时间的性能。

Method: 提出极小极大框架确定影响策略有限时间性能的参数，推导信息论下界，分析MaxWeight局限性，提出新调度规则。

Result: 证明MaxWeight在有限时间会产生更大积压，新调度规则在一定条件下可匹配下界。

Conclusion: 揭示了“仅漂移”方法的基本局限性，为排队控制的非渐近最优指明方向。

Abstract: We establish the first finite-time information-theoretic lower bounds-and
derive new policies that achieve them-for the total queue length in scheduling
problems over stochastic processing networks with both adversarial and
stochastic arrivals. Prior analyses of MaxWeight guarantee only stability and
asymptotic optimality in heavy traffic; we prove that, at finite horizons,
MaxWeight can incur strictly larger backlog by problem-dependent factors which
we identify. Our main innovations are 1) a minimax framework that pinpoints the
precise problem parameters governing any policy's finite-time performance; 2)
an information-theoretic lower bound on total queue length; 3) fundamental
limitation of MaxWeight that it is suboptimal in finite time; and 4) a new
scheduling rule that minimizes the full Lyapunov drift-including its
second-order term-thereby matching the lower bound under certain conditions, up
to universal constants. These findings reveal a fundamental limitation on
"drift-only" methods and points the way toward principled, non-asymptotic
optimality in queueing control.

</details>


### [145] [A Complete Loss Landscape Analysis of Regularized Deep Matrix Factorization](https://arxiv.org/abs/2506.20344)
*Po Chen,Rujun Jiang,Peng Wang*

Main category: math.OC

TL;DR: 对正则化深度矩阵分解（DMF）问题的损失景观进行全面研究，给出临界点表达式和类型条件，推导特定条件并进行数值实验。


<details>
  <summary>Details</summary>
Motivation: 现有DMF优化基础研究不足，填补其损失景观研究的空白。

Method: 先给出所有临界点的闭式表达式，再确定临界点为不同类型的精确条件，推导临界点为局部极小值点或严格鞍点的充要条件，最后进行数值实验。

Result: 获得了临界点的表达式和不同类型的条件，推导出特定充要条件，通过数值实验可视化损失景观。

Conclusion: 解释了基于梯度的方法几乎总是收敛到正则化DMF问题的局部极小值点的原因。

Abstract: Despite its wide range of applications across various domains, the
optimization foundations of deep matrix factorization (DMF) remain largely
open. In this work, we aim to fill this gap by conducting a comprehensive study
of the loss landscape of the regularized DMF problem. Toward this goal, we
first provide a closed-form expression of all critical points. Building on
this, we establish precise conditions under which a critical point is a local
minimizer, a global minimizer, a strict saddle point, or a non-strict saddle
point. Leveraging these results, we derive a necessary and sufficient condition
under which each critical point is either a local minimizer or a strict saddle
point. This provides insights into why gradient-based methods almost always
converge to a local minimizer of the regularized DMF problem. Finally, we
conduct numerical experiments to visualize its loss landscape under different
settings to support our theory.

</details>


### [146] [First-order methods for stochastic and finite-sum convex optimization with deterministic constraints](https://arxiv.org/abs/2506.20630)
*Zhaosong Lu,Yifeng Xiao*

Main category: math.OC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In this paper, we study a class of stochastic and finite-sum convex
optimization problems with deterministic constraints. Existing methods
typically aim to find an $\epsilon$-$expectedly\ feasible\ stochastic\ optimal$
solution, in which the expected constraint violation and expected optimality
gap are both within a prescribed tolerance $\epsilon$. However, in many
practical applications, constraints must be nearly satisfied with certainty,
rendering such solutions potentially unsuitable due to the risk of substantial
violations. To address this issue, we propose stochastic first-order methods
for finding an $\epsilon$-$surely\ feasible\ stochastic\ optimal$
($\epsilon$-SFSO) solution, where the constraint violation is deterministically
bounded by $\epsilon$ and the expected optimality gap is at most $\epsilon$.
Our methods apply an accelerated stochastic gradient (ASG) scheme or a modified
variance-reduced ASG scheme $only\ once$ to a sequence of quadratic penalty
subproblems with appropriately chosen penalty parameters. We establish
first-order oracle complexity bounds for the proposed methods in computing an
$\epsilon$-SFSO solution. As a byproduct, we also derive first-order oracle
complexity results for sample average approximation method in computing an
$\epsilon$-SFSO solution of the stochastic optimization problem using our
proposed methods to solve the sample average problem.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [147] [Learning Bilateral Team Formation in Cooperative Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2506.20039)
*Koorosh Moslemi,Chi-Guhn Lee*

Main category: cs.MA

TL;DR: 文章提出动态多智能体系统中双边团队形成学习框架，验证其在多场景有竞争力和更好泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体强化学习团队形成研究未充分探索动态群体中算法双边分组选择的影响，需填补该空白。

Method: 引入动态多智能体系统中学习双边团队形成的框架。

Result: 在广泛采用的多智能体场景中验证，表现有竞争力，多数场景泛化性提升。

Conclusion: 研究有助于了解双边团队形成中的算法属性对策略性能和泛化性的影响。

Abstract: Team formation and the dynamics of team-based learning have drawn significant
interest in the context of Multi-Agent Reinforcement Learning (MARL). However,
existing studies primarily focus on unilateral groupings, predefined teams, or
fixed-population settings, leaving the effects of algorithmic bilateral
grouping choices in dynamic populations underexplored. To address this gap, we
introduce a framework for learning two-sided team formation in dynamic
multi-agent systems. Through this study, we gain insight into what algorithmic
properties in bilateral team formation influence policy performance and
generalization. We validate our approach using widely adopted multi-agent
scenarios, demonstrating competitive performance and improved generalization in
most scenarios.

</details>


### [148] [A Visualization Framework for Exploring Multi-Agent-Based Simulations Case Study of an Electric Vehicle Home Charging Ecosystem](https://arxiv.org/abs/2506.20400)
*Kristoffer Christensen,Bo Nørregaard Jørgensen,Zheng Grace Ma*

Main category: cs.MA

TL;DR: 本文提出基于Python的模块化仪表盘框架，用于电动汽车家庭充电生态多智能体模拟输出的分析，通过案例展示其功能和优势，架构可适配其他能源系统。


<details>
  <summary>Details</summary>
Motivation: 电动汽车家庭充电生态的多智能体模拟产生复杂数据集，静态后处理难检测和解释系统层面事件。

Method: 构建基于Dash by Plotly的模块化仪表盘框架，有三个协调视图及多种可视化工具。

Result: 通过丹麦住宅网络案例，展示仪表盘可快速识别和解释异常。

Conclusion: 框架有助于研究人员和运营商生成可行动见解，架构具适应性。

Abstract: Multi-agent-based simulations (MABS) of electric vehicle (EV) home charging
ecosystems generate large, complex, and stochastic time-series datasets that
capture interactions between households, grid infrastructure, and energy
markets. These interactions can lead to unexpected system-level events, such as
transformer overloads or consumer dissatisfaction, that are difficult to detect
and explain through static post-processing. This paper presents a modular,
Python-based dashboard framework, built using Dash by Plotly, that enables
efficient, multi-level exploration and root-cause analysis of emergent behavior
in MABS outputs. The system features three coordinated views (System Overview,
System Analysis, and Consumer Analysis), each offering high-resolution
visualizations such as time-series plots, spatial heatmaps, and agent-specific
drill-down tools. A case study simulating full EV adoption with smart charging
in a Danish residential network demonstrates how the dashboard supports rapid
identification and contextual explanation of anomalies, including clustered
transformer overloads and time-dependent charging failures. The framework
facilitates actionable insight generation for researchers and distribution
system operators, and its architecture is adaptable to other distributed energy
resources and complex energy systems.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [149] [Fast ground penetrating radar dual-parameter full waveform inversion method accelerated by hybrid compilation of CUDA kernel function and PyTorch](https://arxiv.org/abs/2506.20513)
*Lei Liu,Chao Song,Liangsheng He,Silin Wang,Xuan Feng,Cai Liu*

Main category: physics.geo-ph

TL;DR: 提出用于探地雷达的高性能双参数全波形反演框架，结合CUDA和PyTorch加速，实验验证其准确性与灵活性。


<details>
  <summary>Details</summary>
Motivation: 为探地雷达数据实现准确高效的双参数全波形反演，用于地下成像。

Method: 通过CUDA内核函数和PyTorch混合编译加速，将定制CUDA内核集成到PyTorch自动微分机制。

Result: 在合成数据和真实波场数据实验中实现双参数全波形反演，保持高精度，框架灵活可扩展。

Conclusion: 该方法是用于土木工程、环境监测和地球物理勘探等领域基于探地雷达的地下快速成像的实用且可扩展框架。

Abstract: This study proposes a high-performance dual-parameter full waveform inversion
framework (FWI) for ground-penetrating radar (GPR), accelerated through the
hybrid compilation of CUDA kernel functions and PyTorch. The method leverages
the computational efficiency of GPU programming while preserving the
flexibility and usability of Python-based deep learning frameworks. By
integrating customized CUDA kernels into PyTorch's automatic differentiation
mechanism, the framework enables accurate and efficient inversion of both
dielectric permittivity and electrical conductivity. Experimental evaluations
on synthetic data and real wavefield data demonstrate that the proposed method
achieves dual-parameter FWI for GPR data while maintaining high accuracy.
Moreover, the framework is flexible and extensible, supporting optional
regularization strategies such as total variation and multi-scale inversion.
These features make the proposed approach a practical and scalable framework
for rapid GPR-based subsurface imaging in applications including civil
engineering, environmental monitoring, and geophysical exploration.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [150] [Speaker Embeddings to Improve Tracking of Intermittent and Moving Speakers](https://arxiv.org/abs/2506.19875)
*Taous Iatariene,Can Cui,Alexandre Guérin,Romain Serizel*

Main category: eess.AS

TL;DR: 论文提出利用说话人嵌入进行身份重新分配以解决说话人跟踪中身份分配问题，评估显示该方法能提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有说话人跟踪方法在处理间歇性和移动说话人场景时有局限性，因说话人不活跃时位置改变导致空间轨迹不连续。

Method: 在跟踪后使用说话人嵌入进行身份重新分配，利用初始跟踪的轨迹信息和多通道音频信号，用波束形成增强信号以计算说话人嵌入，再根据注册池分配新的跟踪身份。

Result: 在说话人在不活跃期改变位置的数据集上评估，该方法持续提升了神经和标准跟踪系统的身份分配性能，还研究了波束形成和嵌入提取输入时长的影响。

Conclusion: 基于说话人嵌入的身份重新分配方法能有效提升说话人跟踪中的身份分配性能。

Abstract: Speaker tracking methods often rely on spatial observations to assign
coherent track identities over time. This raises limits in scenarios with
intermittent and moving speakers, i.e., speakers that may change position when
they are inactive, thus leading to discontinuous spatial trajectories. This
paper proposes to investigate the use of speaker embeddings, in a simple
solution to this issue. We propose to perform identity reassignment
post-tracking, using speaker embeddings. We leverage trajectory-related
information provided by an initial tracking step and multichannel audio signal.
Beamforming is used to enhance the signal towards the speakers' positions in
order to compute speaker embeddings. These are then used to assign new track
identities based on an enrollment pool. We evaluate the performance of the
proposed speaker embedding-based identity reassignment method on a dataset
where speakers change position during inactivity periods. Results show that it
consistently improves the identity assignment performance of neural and
standard tracking systems. In particular, we study the impact of beamforming
and input duration for embedding extraction.

</details>


### [151] [MATER: Multi-level Acoustic and Textual Emotion Representation for Interpretable Speech Emotion Recognition](https://arxiv.org/abs/2506.19887)
*Hyo Jin Jon,Longbin Jin,Hyuntaek Jung,Hyunseo Kim,Donghun Min,Eun Yi Kim*

Main category: eess.AS

TL;DR: 本文介绍SERNC挑战赛成果，提出MATER框架及不确定性感知集成策略，在任务中取得较好排名。


<details>
  <summary>Details</summary>
Motivation: 解决自然语音中类别情感识别和情感属性预测问题，处理自然语音的复杂性及标注者不一致问题。

Method: 提出MATER框架，在词、话语和嵌入层集成声学和文本特征；引入不确定性感知集成策略。

Result: MATER在两项任务中排名第四，Macro - F1为41.01%，平均CCC为0.5928，在效价预测中排名第二，CCC为0.6941。

Conclusion: MATER框架和不确定性感知集成策略能有效处理自然语音情感识别和属性预测问题，提升模型性能和鲁棒性。

Abstract: This paper presents our contributions to the Speech Emotion Recognition in
Naturalistic Conditions (SERNC) Challenge, where we address categorical emotion
recognition and emotional attribute prediction. To handle the complexities of
natural speech, including intra- and inter-subject variability, we propose
Multi-level Acoustic-Textual Emotion Representation (MATER), a novel
hierarchical framework that integrates acoustic and textual features at the
word, utterance, and embedding levels. By fusing low-level lexical and acoustic
cues with high-level contextualized representations, MATER effectively captures
both fine-grained prosodic variations and semantic nuances. Additionally, we
introduce an uncertainty-aware ensemble strategy to mitigate annotator
inconsistencies, improving robustness in ambiguous emotional expressions. MATER
ranks fourth in both tasks with a Macro-F1 of 41.01% and an average CCC of
0.5928, securing second place in valence prediction with an impressive CCC of
0.6941.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [152] [Hierarchical Reinforcement Learning and Value Optimization for Challenging Quadruped Locomotion](https://arxiv.org/abs/2506.20036)
*Jeremiah Coholich,Muhammad Ali Murtaza,Seth Hutchinson,Zsolt Kira*

Main category: cs.RO

TL;DR: 提出用于四足机器人在复杂地形运动的分层强化学习框架，对比端到端方法展示其优势。


<details>
  <summary>Details</summary>
Motivation: 解决四足机器人在复杂地形的运动问题。

Method: 采用两层分层结构，高层策略（HLP）为低层策略（LLP）选择最优目标，LLP用策略内演员 - 评论家RL算法训练，HLP通过对LLP学习到的值函数进行在线优化操作。

Result: 与端到端强化学习方法对比，在不同地形尤其是比训练时更难的地形上，能获得更高奖励且碰撞更少。

Conclusion: 所提出的分层强化学习框架在四足机器人复杂地形运动中有效且优越。

Abstract: We propose a novel hierarchical reinforcement learning framework for
quadruped locomotion over challenging terrain. Our approach incorporates a
two-layer hierarchy in which a high-level policy (HLP) selects optimal goals
for a low-level policy (LLP). The LLP is trained using an on-policy
actor-critic RL algorithm and is given footstep placements as goals. We propose
an HLP that does not require any additional training or environment samples and
instead operates via an online optimization process over the learned value
function of the LLP. We demonstrate the benefits of this framework by comparing
it with an end-to-end reinforcement learning (RL) approach. We observe
improvements in its ability to achieve higher rewards with fewer collisions
across an array of different terrains, including terrains more difficult than
any encountered during training.

</details>


### [153] [Robust Robotic Exploration and Mapping Using Generative Occupancy Map Synthesis](https://arxiv.org/abs/2506.20049)
*Lorin Achey,Alec Reed,Brendan Crowe,Bradley Hayes,Christoffer Heckman*

Main category: cs.RO

TL;DR: 提出用生成式占用映射增强机器人探索的新方法SceneSense，经实验验证可提升地图质量、遍历性和探索效果。


<details>
  <summary>Details</summary>
Motivation: 提升机器人探索能力，改善占用地图质量和遍历性。

Method: 引入扩散模型SceneSense预测3D占用地图，实时概率融合预测结果到运行中的占用地图，在四足机器人上实现并实验验证。

Result: SceneSense增强的占用地图更好地代表真实数据（机器人周围FID提高24.44%，远距离提高75.59%），集成到探索栈可提升鲁棒性和遍历时间，局部增强地图探索结果更一致。

Conclusion: 提出的方法有效，局部增强地图比仅靠直接传感器测量构建的地图探索效果更好。

Abstract: We present a novel approach for enhancing robotic exploration by using
generative occupancy mapping. We introduce SceneSense, a diffusion model
designed and trained for predicting 3D occupancy maps given partial
observations. Our proposed approach probabilistically fuses these predictions
into a running occupancy map in real-time, resulting in significant
improvements in map quality and traversability. We implement SceneSense onboard
a quadruped robot and validate its performance with real-world experiments to
demonstrate the effectiveness of the model. In these experiments, we show that
occupancy maps enhanced with SceneSense predictions better represent our fully
observed ground truth data (24.44% FID improvement around the robot and 75.59%
improvement at range). We additionally show that integrating
SceneSense-enhanced maps into our robotic exploration stack as a "drop-in" map
improvement, utilizing an existing off-the-shelf planner, results in
improvements in robustness and traversability time. Finally we show results of
full exploration evaluations with our proposed system in two dissimilar
environments and find that locally enhanced maps provide more consistent
exploration results than maps constructed only from direct sensor measurements.

</details>


### [154] [Generating and Customizing Robotic Arm Trajectories using Neural Networks](https://arxiv.org/abs/2506.20259)
*Andrej Lúčny,Matilde Antonj,Carlo Mazzola,Hana Hornáčková,Igor Farkaš*

Main category: cs.RO

TL;DR: 提出一种用于生成和定制机械臂轨迹的神经网络方法，在认知机器人实验中验证其能产生精确且可定制轨迹。


<details>
  <summary>Details</summary>
Motivation: 开发能保证机械臂轨迹精度和可重复性的方法，提高机器人与人交互时动作的可预测性。

Method: 神经网络计算机械臂正向运动学，结合关节角度生成器，在人工数据集上训练另一个神经网络，通过计算角速度使机器人能执行运动。

Result: 机器人能以精确线性运动指向空间特定点，可评估其动作质量。

Conclusion: 该方法具有广泛适用性，能成功生成形状可定制、适应不同场景的精确轨迹。

Abstract: We introduce a neural network approach for generating and customizing the
trajectory of a robotic arm, that guarantees precision and repeatability. To
highlight the potential of this novel method, we describe the design and
implementation of the technique and show its application in an experimental
setting of cognitive robotics. In this scenario, the NICO robot was
characterized by the ability to point to specific points in space with precise
linear movements, increasing the predictability of the robotic action during
its interaction with humans. To achieve this goal, the neural network computes
the forward kinematics of the robot arm. By integrating it with a generator of
joint angles, another neural network was developed and trained on an artificial
dataset created from suitable start and end poses of the robotic arm. Through
the computation of angular velocities, the robot was characterized by its
ability to perform the movement, and the quality of its action was evaluated in
terms of shape and accuracy. Thanks to its broad applicability, our approach
successfully generates precise trajectories that could be customized in their
shape and adapted to different settings.

</details>


### [155] [CARMA: Context-Aware Situational Grounding of Human-Robot Group Interactions by Combining Vision-Language Models with Object and Action Recognition](https://arxiv.org/abs/2506.20373)
*Joerg Deigmoeller,Stephan Hasler,Nakul Agarwal,Daniel Tanneberg,Anna Belardinelli,Reza Ghoddoosian,Chao Wang,Felix Ocker,Fan Zhang,Behzad Dariush,Michael Gienger*

Main category: cs.RO

TL;DR: 介绍用于人机群体交互情境基础的系统CARMA，通过实验验证其能可靠生成准确三元组，为协作场景提供基础。


<details>
  <summary>Details</summary>
Motivation: 人机群体协作需基于一致表征的情境感知，要明确实例分配，以让机器人正确识别和跟踪实体及交互。

Method: 提出CARMA系统，将现实世界实体唯一标识并组织成三元组；开展协作倾倒、交接和分类三个实验进行验证。

Result: 实验表明系统能可靠生成准确的参与者 - 动作 - 对象三元组。

Conclusion: 该系统为协作场景中需要时空推理和情境决策的应用提供了结构化且强大的基础。

Abstract: We introduce CARMA, a system for situational grounding in human-robot group
interactions. Effective collaboration in such group settings requires
situational awareness based on a consistent representation of present persons
and objects coupled with an episodic abstraction of events regarding actors and
manipulated objects. This calls for a clear and consistent assignment of
instances, ensuring that robots correctly recognize and track actors, objects,
and their interactions over time. To achieve this, CARMA uniquely identifies
physical instances of such entities in the real world and organizes them into
grounded triplets of actors, objects, and actions.
  To validate our approach, we conducted three experiments, where multiple
humans and a robot interact: collaborative pouring, handovers, and sorting.
These scenarios allow the assessment of the system's capabilities as to role
distinction, multi-actor awareness, and consistent instance identification. Our
experiments demonstrate that the system can reliably generate accurate
actor-action-object triplets, providing a structured and robust foundation for
applications requiring spatiotemporal reasoning and situated decision-making in
collaborative settings.

</details>


### [156] [DemoDiffusion: One-Shot Human Imitation using pre-trained Diffusion Policy](https://arxiv.org/abs/2506.20668)
*Sungjae Park,Homanga Bharadhwaj,Shubham Tulsiani*

Main category: cs.RO

TL;DR: 提出DemoDiffusion方法，让机器人通过模仿单个人类演示在自然环境执行操作任务，避免在线强化学习等，实验显示效果佳。


<details>
  <summary>Details</summary>
Motivation: 使机器人能在自然环境中通过模仿单个人类演示执行操作任务，且避免在线强化学习和配对的人机数据。

Method: 基于两点关键见解，先通过运动学重定向将人类演示的手部运动转换为机器人粗略的开环运动轨迹，再利用预训练的通用扩散策略修改轨迹。

Result: 在模拟和现实场景实验中，DemoDiffusion优于基础策略和重定向轨迹，即使预训练通用策略完全失败的任务，机器人也能成功。

Conclusion: DemoDiffusion方法简单可扩展，能让机器人在新任务和场景中以最小的人工努力进行鲁棒适应。

Abstract: We propose DemoDiffusion, a simple and scalable method for enabling robots to
perform manipulation tasks in natural environments by imitating a single human
demonstration. Our approach is based on two key insights. First, the hand
motion in a human demonstration provides a useful prior for the robot's
end-effector trajectory, which we can convert into a rough open-loop robot
motion trajectory via kinematic retargeting. Second, while this retargeted
motion captures the overall structure of the task, it may not align well with
plausible robot actions in-context. To address this, we leverage a pre-trained
generalist diffusion policy to modify the trajectory, ensuring it both follows
the human motion and remains within the distribution of plausible robot
actions. Our approach avoids the need for online reinforcement learning or
paired human-robot data, enabling robust adaptation to new tasks and scenes
with minimal manual effort. Experiments in both simulation and real-world
settings show that DemoDiffusion outperforms both the base policy and the
retargeted trajectory, enabling the robot to succeed even on tasks where the
pre-trained generalist policy fails entirely. Project page:
https://demodiffusion.github.io/

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [157] [Narrative Shift Detection: A Hybrid Approach of Dynamic Topic Models and Large Language Models](https://arxiv.org/abs/2506.20269)
*Kai-Robin Lange,Tobias Schmidt,Matthias Reccius,Henrik Müller,Michael Roos,Carsten Jentsch*

Main category: cs.CL

TL;DR: 结合大语言模型与主题模型，利用叙事政策框架动态建模叙事随时间的变化，应用于《华尔街日报》文章语料，发现大语言模型在提取叙事转变上有效，但区分内容和叙事转变表现不佳。


<details>
  <summary>Details</summary>
Motivation: 媒体叙事快速演变，需研究叙事随时间的发展，而现有流行叙事提取方法应用于整个语料存在高成本等障碍。

Method: 结合大语言模型的语言理解能力和主题模型的大规模适用性，利用叙事政策框架；应用主题模型和变化点检测方法，筛选代表变化的文档输入大语言模型自动解释变化并区分内容和叙事转变。

Result: 大语言模型能有效提取给定时间点的叙事转变，但区分内容和叙事转变表现不佳。

Conclusion: 提出的结合方法在叙事转变提取上有一定效果，但在区分转变类型上有待改进。

Abstract: With rapidly evolving media narratives, it has become increasingly critical
to not just extract narratives from a given corpus but rather investigate, how
they develop over time. While popular narrative extraction methods such as
Large Language Models do well in capturing typical narrative elements or even
the complex structure of a narrative, applying them to an entire corpus comes
with obstacles, such as a high financial or computational cost. We propose a
combination of the language understanding capabilities of Large Language Models
with the large scale applicability of topic models to dynamically model
narrative shifts across time using the Narrative Policy Framework. We apply a
topic model and a corresponding change point detection method to find changes
that concern a specific topic of interest. Using this model, we filter our
corpus for documents that are particularly representative of that change and
feed them into a Large Language Model that interprets the change that happened
in an automated fashion and distinguishes between content and narrative shifts.
We employ our pipeline on a corpus of The Wall Street Journal news paper
articles from 2009 to 2023. Our findings indicate that a Large Language Model
can efficiently extract a narrative shift if one exists at a given point in
time, but does not perform as well when having to decide whether a shift in
content or a narrative shift took place.

</details>


### [158] [Knowledge-Aware Diverse Reranking for Cross-Source Question Answering](https://arxiv.org/abs/2506.20476)
*Tong Zhou*

Main category: cs.CL

TL;DR: 本文介绍Team Marikarp在SIGIR 2025 LiveRAG竞赛的解决方案，提出的知识感知多样化重排序RAG管道获竞赛第一名。


<details>
  <summary>Details</summary>
Motivation: 参加SIGIR 2025 LiveRAG竞赛，该竞赛有自动生成的评价集，可公平评估从文档子集中检索相关支持文档的能力。

Method: 提出知识感知多样化重排序RAG管道。

Result: 在竞赛中获得第一名。

Conclusion: 所提出的知识感知多样化重排序RAG管道在竞赛中表现优异。

Abstract: This paper presents Team Marikarp's solution for the SIGIR 2025 LiveRAG
competition. The competition's evaluation set, automatically generated by
DataMorgana from internet corpora, encompassed a wide range of target topics,
question types, question formulations, audience types, and knowledge
organization methods. It offered a fair evaluation of retrieving
question-relevant supporting documents from a 15M documents subset of the
FineWeb corpus. Our proposed knowledge-aware diverse reranking RAG pipeline
achieved first place in the competition.

</details>


### [159] [ReCode: Updating Code API Knowledge with Reinforcement Learning](https://arxiv.org/abs/2506.20495)
*Haoze Wu,Yunzhi Yao,Wenhao Yu,Huajun Chen,Ningyu Zhang*

Main category: cs.CL

TL;DR: 提出ReCode框架解决大语言模型在动态API场景代码生成问题，实验表明该框架提升性能且对通用能力影响小。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在适应外部库API频繁更新时存在缺陷，因训练数据中API知识过时，阻碍其在动态环境可靠生成代码。

Method: 构建约2000个数据条目的数据集训练大语言模型进行版本迁移，引入修改后的字符串相似度指标作为强化学习奖励。

Result: ReCode显著提升大语言模型在动态API场景的代码生成性能，对通用代码生成能力影响小，在多种模型和算法上应用均有改进，Qwen2.5 - Coder - 7B训练后表现出色。

Conclusion: ReCode框架能有效解决大语言模型在动态API场景代码生成的问题，具有较好应用效果。

Abstract: Large Language Models (LLMs) exhibit remarkable code generation capabilities
but falter when adapting to frequent updates in external library APIs. This
critical limitation, stemming from reliance on outdated API knowledge from
their training data, even with access to current documentation, impedes
reliable code generation in dynamic environments. To tackle this issue, we
propose ReCode (rule-based Reinforcement learning for Code Update), a novel
framework that mimics human programmer adaptation to API changes. Specifically,
we construct a dataset of approximately 2,000 data entries to train the LLMs to
perform version migration based on updated information. Then, we introduce a
modified string similarity metric for code evaluation as the reward for
reinforcement learning. Our experiments demonstrate that ReCode substantially
boosts LLMs' code generation performance in dynamic API scenarios, especially
on the unseen CodeUpdateArena task. Crucially, compared to supervised
fine-tuning, ReCode has less impact on LLMs' general code generation abilities.
We apply ReCode on various LLMs and reinforcement learning algorithms (GRPO and
DAPO), all achieving consistent improvements. Notably, after training,
Qwen2.5-Coder-7B outperforms that of the 32B parameter code instruction-tuned
model and the reasoning model with the same architecture. Code is available at
https://github.com/zjunlp/ReCode.

</details>


### [160] [CycleDistill: Bootstrapping Machine Translation using LLMs with Cyclical Distillation](https://arxiv.org/abs/2506.19952)
*Deepon Halder,Thanmay Jayakumar,Raj Dabre*

Main category: cs.CL

TL;DR: 提出CycleDistill方法，利用大语言模型和少样本翻译，仅靠单语语料实现高质量机器翻译，在印地语实验中有显著提升。


<details>
  <summary>Details</summary>
Motivation: 大语言模型少样本机器翻译效果不如基于平行语料训练的系统，且低资源语言平行语料稀缺。

Method: 提出CycleDistill方法，通过零样本或少样本翻译从单语语料迭代生成合成平行语料，用于微调模型。

Result: 在三种印度语言实验中，仅依靠单语语料，首次迭代平均比少样本基线模型提高20 - 30 chrF点，蒸馏过程中利用softmax激活有轻微质量提升。

Conclusion: CycleDistill方法能在少样本情况下，仅靠单语语料实现高质量机器翻译。

Abstract: Large language models (LLMs), despite their ability to perform few-shot
machine translation (MT), often lag behind dedicated MT systems trained on
parallel corpora, which are crucial for high quality machine translation (MT).
However, parallel corpora are often scarce or non-existent for low-resource
languages. In this paper, we propose CycleDistill, a bootstrapping approach
leveraging LLMs and few-shot translation to obtain high-quality MT systems.
CycleDistill involves iteratively generating synthetic parallel corpora from
monolingual corpora via zero- or few-shot MT, which is then used to fine-tune
the model that was used for generating said data for MT. CycleDistill does not
need parallel corpora beyond 1 to 4 few-shot examples, and in our experiments
focusing on three Indian languages, by relying solely on monolingual corpora,
it can achieve high-quality machine translation, improving upon a few-shot
baseline model by over 20-30 chrF points on average in the first iteration. We
also study the effect of leveraging softmax activations during the distillation
process and observe mild improvements in translation quality.

</details>


### [161] [Inference Scaled GraphRAG: Improving Multi Hop Question Answering on Knowledge Graphs](https://arxiv.org/abs/2506.19967)
*Travis Thompson,Seung-Hwan Lim,Paul Liu,Ruoying He,Dongkuan Xu*

Main category: cs.CL

TL;DR: 现有大语言模型在知识密集推理任务表现不佳，提出Inference - Scaled GraphRAG框架，实验证明其提升多跳问答性能，表明推理时间缩放是实用且与架构无关的解决方案。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在知识密集推理任务表现欠佳，传统RAG和GraphRAG方法无法捕捉知识图节点间关系结构。

Method: 引入Inference - Scaled GraphRAG框架，结合顺序缩放与深度思维链图遍历、并行缩放与多数投票，在交错推理执行循环中操作。

Result: 在GRBench基准测试中，该方法显著提升多跳问答性能，优于传统GraphRAG和先前图遍历基线。

Conclusion: 推理时间缩放是用于大语言模型结构化知识推理的实用且与架构无关的解决方案。

Abstract: Large Language Models (LLMs) have achieved impressive capabilities in
language understanding and generation, yet they continue to underperform on
knowledge-intensive reasoning tasks due to limited access to structured context
and multi-hop information. Retrieval-Augmented Generation (RAG) partially
mitigates this by grounding generation in retrieved context, but conventional
RAG and GraphRAG methods often fail to capture relational structure across
nodes in knowledge graphs. We introduce Inference-Scaled GraphRAG, a novel
framework that enhances LLM-based graph reasoning by applying inference-time
compute scaling. Our method combines sequential scaling with deep
chain-of-thought graph traversal, and parallel scaling with majority voting
over sampled trajectories within an interleaved reasoning-execution loop.
Experiments on the GRBench benchmark demonstrate that our approach
significantly improves multi-hop question answering performance, achieving
substantial gains over both traditional GraphRAG and prior graph traversal
baselines. These findings suggest that inference-time scaling is a practical
and architecture-agnostic solution for structured knowledge reasoning with LLMs

</details>


### [162] [A Modular Multitask Reasoning Framework Integrating Spatio-temporal Models and LLMs](https://arxiv.org/abs/2506.20073)
*Kethmi Hirushini Hettige,Jiahao Ji,Cheng Long,Shili Xiang,Gao Cong,Jingyuan Wang*

Main category: cs.CL

TL;DR: 本文提出STReason框架，结合大语言模型推理能力与时空模型分析能力进行多任务推理，实验显示其性能优异，有实际应用潜力。


<details>
  <summary>Details</summary>
Motivation: 现有模型局限于狭窄任务，缺乏多任务推理和复杂长形式推理能力，难以应用于现实多面决策场景。

Method: 引入STReason框架，利用上下文学习将复杂自然语言查询分解为模块化、可解释程序并执行，构建新基准数据集和统一评估框架。

Result: STReason在所有指标上显著优于先进LLM基线，尤其在复杂推理密集的时空场景中表现出色，人类评估验证其可信度和实用性。

Conclusion: STReason为开发更强大、可泛化的时空推理系统提供了有前景的方向。

Abstract: Spatio-temporal data mining plays a pivotal role in informed decision making
across diverse domains. However, existing models are often restricted to narrow
tasks, lacking the capacity for multi-task inference and complex long-form
reasoning that require generation of in-depth, explanatory outputs. These
limitations restrict their applicability to real-world, multi-faceted decision
scenarios. In this work, we introduce STReason, a novel framework that
integrates the reasoning strengths of large language models (LLMs) with the
analytical capabilities of spatio-temporal models for multi-task inference and
execution. Without requiring task-specific finetuning, STReason leverages
in-context learning to decompose complex natural language queries into modular,
interpretable programs, which are then systematically executed to generate both
solutions and detailed rationales. To facilitate rigorous evaluation, we
construct a new benchmark dataset and propose a unified evaluation framework
with metrics specifically designed for long-form spatio-temporal reasoning.
Experimental results show that STReason significantly outperforms advanced LLM
baselines across all metrics, particularly excelling in complex,
reasoning-intensive spatio-temporal scenarios. Human evaluations further
validate STReason's credibility and practical utility, demonstrating its
potential to reduce expert workload and broaden the applicability to real-world
spatio-temporal tasks. We believe STReason provides a promising direction for
developing more capable and generalizable spatio-temporal reasoning systems.

</details>


### [163] [SACL: Understanding and Combating Textual Bias in Code Retrieval with Semantic-Augmented Reranking and Localization](https://arxiv.org/abs/2506.20081)
*Dhruv Gupta,Gayathri Ganesh Lakshmy,Yiqing Xie*

Main category: cs.CL

TL;DR: 对代码检索进行分析，发现当前检索器问题，提出SACL框架，实验表明其能提升代码检索和生成性能。


<details>
  <summary>Details</summary>
Motivation: 深入分析代码检索，解决当前代码检索存在的问题以提升代码生成效果。

Method: 系统地屏蔽特定特征同时保留代码功能来分析代码检索，基于发现提出SACL框架，用语义信息丰富代码或结构知识。

Result: SACL大幅提升代码检索性能，如在HumanEval / MBPP / SWE - Bench - Lite上Recall@1分别提升12.8% / 9.4% / 7.0%；也提升代码生成性能，如在HumanEval上Pass@1提升4.88%。

Conclusion: SACL能有效改善代码检索，进而提升代码生成性能。

Abstract: Retrieval-Augmented Code Generation (RACG) is a critical technique for
enhancing code generation by retrieving relevant information. In this work, we
conduct an in-depth analysis of code retrieval by systematically masking
specific features while preserving code functionality. Our discoveries include:
(1) although trained on code, current retrievers heavily rely on surface-level
textual features (e.g., docstrings, identifier names), and (2) they exhibit a
strong bias towards well-documented code, even if the documentation is
irrelevant.Based on our discoveries, we propose SACL, a framework that enriches
textual information and reduces bias by augmenting code or structural knowledge
with semantic information. Extensive experiments show that SACL substantially
improves code retrieval (e.g., by 12.8% / 9.4% / 7.0% Recall@1 on HumanEval /
MBPP / SWE-Bench-Lite), which also leads to better code generation performance
(e.g., by 4.88% Pass@1 on HumanEval).

</details>


### [164] [CCRS: A Zero-Shot LLM-as-a-Judge Framework for Comprehensive RAG Evaluation](https://arxiv.org/abs/2506.20128)
*Aashiq Muhamed*

Main category: cs.CL

TL;DR: 提出CCRS指标评估RAG系统，在BioASQ数据集验证其有效性，比RAGChecker更高效。


<details>
  <summary>Details</summary>
Motivation: 现有RAG输出评估方法存在不足，难以全面评估输出多方面质量且效率低。

Method: 提出CCRS，利用预训练LLM作为零样本端到端评估器，含五个指标。

Result: CCRS能有效区分不同RAG系统性能，如Mistral - 7B优于Llama变体，相比RAGChecker在召回和忠实度等方面有可比或更优鉴别力且计算更高效。

Conclusion: CCRS为评估和改进RAG系统提供实用、全面且高效的框架。

Abstract: RAG systems enhance LLMs by incorporating external knowledge, which is
crucial for domains that demand factual accuracy and up-to-date information.
However, evaluating the multifaceted quality of RAG outputs, spanning aspects
such as contextual coherence, query relevance, factual correctness, and
informational completeness, poses significant challenges. Existing evaluation
methods often rely on simple lexical overlap metrics, which are inadequate for
capturing these nuances, or involve complex multi-stage pipelines with
intermediate steps like claim extraction or require finetuning specialized
judge models, hindering practical efficiency. To address these limitations, we
propose CCRS (Contextual Coherence and Relevance Score), a novel suite of five
metrics that utilizes a single, powerful, pretrained LLM as a zero-shot,
end-to-end judge. CCRS evaluates: Contextual Coherence (CC), Question Relevance
(QR), Information Density (ID), Answer Correctness (AC), and Information Recall
(IR). We apply CCRS to evaluate six diverse RAG system configurations on the
challenging BioASQ dataset. Our analysis demonstrates that CCRS effectively
discriminates between system performances, confirming, for instance, that the
Mistral-7B reader outperforms Llama variants. We provide a detailed analysis of
CCRS metric properties, including score distributions, convergent/discriminant
validity, tie rates, population statistics, and discriminative power. Compared
to the complex RAGChecker framework, CCRS offers comparable or superior
discriminative power for key aspects like recall and faithfulness, while being
significantly more computationally efficient. CCRS thus provides a practical,
comprehensive, and efficient framework for evaluating and iteratively improving
RAG systems.

</details>


### [165] [SEED: A Structural Encoder for Embedding-Driven Decoding in Time Series Prediction with LLMs](https://arxiv.org/abs/2506.20167)
*Fengze Li,Yue Wang,Yangle Liu,Ming Huang,Dou Hong,Jieming Ma*

Main category: cs.CL

TL;DR: 提出SEED解决多元时间序列预测中结构语义建模差距问题，效果良好。


<details>
  <summary>Details</summary>
Motivation: 现有结构编码器缺乏语义推理和任务适应能力，大语言模型与原始时间序列输入不兼容，限制统一可迁移预测系统发展。

Method: 引入SEED，包含令牌感知编码器、投影模块、语义重编程机制和冻结语言模型四个阶段，解耦表示学习和推理。

Result: 在强基线方法上实现持续改进，不同数据集比较研究证实能解决结构语义建模差距。

Conclusion: SEED有效解决多元时间序列预测中结构语义建模差距问题。

Abstract: Multivariate time series forecasting requires models to simultaneously
capture variable-wise structural dependencies and generalize across diverse
tasks. While structural encoders are effective in modeling feature
interactions, they lack the capacity to support semantic-level reasoning or
task adaptation. Conversely, large language models (LLMs) possess strong
generalization capabilities but remain incompatible with raw time series
inputs. This gap limits the development of unified, transferable prediction
systems. Therefore, we introduce SEED, a structural encoder for
embedding-driven decoding, which integrates four stages: a token-aware encoder
for patch extraction, a projection module that aligns patches with language
model embeddings, a semantic reprogramming mechanism that maps patches to
task-aware prototypes, and a frozen language model for prediction. This modular
architecture decouples representation learning from inference, enabling
efficient alignment between numerical patterns and semantic reasoning.
Empirical results demonstrate that the proposed method achieves consistent
improvements over strong baselines, and comparative studies on various datasets
confirm SEED's role in addressing the structural-semantic modeling gap.

</details>


### [166] [COIN: Uncertainty-Guarding Selective Question Answering for Foundation Models with Provable Risk Guarantees](https://arxiv.org/abs/2506.20178)
*Zhiyuan Wang,Jinhao Duan,Qingni Wang,Xiaofeng Zhu,Tianlong Chen,Xiaoshuang Shi,Kaidi Xu*

Main category: cs.CL

TL;DR: 提出不确定性保护选择框架COIN，在指定FDR约束下筛选答案，经实验展示其多方面优势及可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有启发式UQ方法缺乏关键指标的正式保证，以往SCP框架预测集含错误候选，实用性受限。

Method: 提出COIN框架，在校准集上估计经验错误率，用置信区间方法确定真实错误率上限，筛选不确定性阈值。

Result: COIN在风险控制、保留可接受答案和有限校准数据下的预测效率方面表现良好。采用替代上限构造和UQ策略可提升其性能。

Conclusion: COIN框架有效且具有可扩展性和适应性，能应用于不同场景。

Abstract: Uncertainty quantification (UQ) for foundation models is essential to
identify and mitigate potential hallucinations in automatically generated text.
However, heuristic UQ approaches lack formal guarantees for key metrics such as
the false discovery rate (FDR) in selective prediction. Previous work adopts
the split conformal prediction (SCP) framework to ensure desired coverage of
admissible answers by constructing prediction sets, but these sets often
contain incorrect candidates, limiting their practical utility. To address
this, we propose COIN, an uncertainty-guarding selection framework that
calibrates statistically valid thresholds to filter a single generated answer
per question under user-specified FDR constraints. COIN estimates the empirical
error rate on a calibration set and applies confidence interval methods such as
Clopper-Pearson to establish a high-probability upper bound on the true error
rate (i.e., FDR). This enables the selection of the largest uncertainty
threshold that ensures FDR control on test data while significantly increasing
sample retention. We demonstrate COIN's robustness in risk control, strong
test-time power in retaining admissible answers, and predictive efficiency
under limited calibration data across both general and multimodal text
generation tasks. Furthermore, we show that employing alternative upper bound
constructions and UQ strategies can further boost COIN's power performance,
which underscores its extensibility and adaptability to diverse application
scenarios.

</details>


### [167] [How to Retrieve Examples in In-context Learning to Improve Conversational Emotion Recognition using Large Language Models?](https://arxiv.org/abs/2506.20199)
*Mengqi Wang,Tiantian Feng,Shrikanth Narayanan*

Main category: cs.CL

TL;DR: 研究探索用大语言模型改进对话情感识别，提出不同示例检索策略，实验表明增强示例检索效果最佳。


<details>
  <summary>Details</summary>
Motivation: 大语言模型应用于情感识别等主观任务时创建高性能、高准确性应用有挑战，受SLT 2024 GenSER Challenge启发研究改进对话情感识别方法。

Method: 探索在上下文学习中检索高质量示例的方法，提出基于随机和增强示例检索的策略，分析对话上下文对识别准确率的影响。

Result: 在IEMOCAP、MELD和EmoryNLP三个数据集上实验，增强示例检索在所有数据集上始终优于其他技术。

Conclusion: 强调检索连贯目标示例并通过释义增强它们的重要性。

Abstract: Large language models (LLMs) have enabled a wide variety of real-world
applications in various domains. However, creating a high-performing
application with high accuracy remains challenging, particularly for subjective
tasks like emotion recognition. Inspired by the SLT 2024 GenSER Challenge, this
study investigates approaches to improving conversational emotion recognition
(CER) by LLMs. Specifically, we explore how to retrieve high-quality examples
in in-context learning (ICL) to enhance CER. We propose various strategies
based on random and augmented example retrieval and also analyze the impact of
conversational context on CER accuracy. Experiments were conducted on the three
datasets including IEMOCAP, MELD and EmoryNLP. The results show that augmented
example retrieval consistently outperforms other techniques under investigation
across all datasets, highlighting the importance of retrieving coherent
targeted examples and enhancing them through paraphrasing.

</details>


### [168] [Perspectives in Play: A Multi-Perspective Approach for More Inclusive NLP Systems](https://arxiv.org/abs/2506.20209)
*Benedetta Muscato,Lucia Passaro,Gizem Gezici,Fosca Giannotti*

Main category: cs.CL

TL;DR: 传统NLP处理人类分歧方法有缺陷，本文提出多视角软标签方法，在多种主观文本分类任务中表现良好，但在部分任务置信度低，还用XAI探索模型不确定性。


<details>
  <summary>Details</summary>
Motivation: 传统聚合注释者观点建立单一真相的方法会忽视少数观点，尤其是主观任务中，需开发更具包容性和多元化的视角感知模型。

Method: 提出使用软标签的多视角方法，在多种主观文本分类任务中进行分析，并利用XAI探索模型不确定性。

Result: 多视角方法能更好近似人类标签分布，分类性能更优，但在反讽和立场检测任务中置信度低。

Conclusion: 多视角方法有优势，但存在部分任务置信度低的问题，XAI有助于理解模型预测。

Abstract: In the realm of Natural Language Processing (NLP), common approaches for
handling human disagreement consist of aggregating annotators' viewpoints to
establish a single ground truth. However, prior studies show that disregarding
individual opinions can lead can lead to the side effect of underrepresenting
minority perspectives, especially in subjective tasks, where annotators may
systematically disagree because of their preferences. Recognizing that labels
reflect the diverse backgrounds, life experiences, and values of individuals,
this study proposes a new multi-perspective approach using soft labels to
encourage the development of the next generation of perspective aware models,
more inclusive and pluralistic. We conduct an extensive analysis across diverse
subjective text classification tasks, including hate speech, irony, abusive
language, and stance detection, to highlight the importance of capturing human
disagreements, often overlooked by traditional aggregation methods. Results
show that the multi-perspective approach not only better approximates human
label distributions, as measured by Jensen-Shannon Divergence (JSD), but also
achieves superior classification performance (higher F1 scores), outperforming
traditional approaches. However, our approach exhibits lower confidence in
tasks like irony and stance detection, likely due to the inherent subjectivity
present in the texts. Lastly, leveraging Explainable AI (XAI), we explore model
uncertainty and uncover meaningful insights into model predictions.

</details>


### [169] [Enhancing Large Language Models through Structured Reasoning](https://arxiv.org/abs/2506.20241)
*Yubo Dong,Hehe Fan*

Main category: cs.CL

TL;DR: 本文介绍了通过显式结构化推理增强大语言模型的方法，实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在复杂推理任务中存在困难，因其依赖隐式统计关系而缺乏结构化知识表示。

Method: 将非结构化数据转换为结构化格式，通过监督微调（SFT）训练大语言模型，使用组相对策略优化（GRPO）增强其结构化推理能力，融入MAX - Flow和最长公共子序列（LCS）算法。

Result: 对DeepSeek - R1 - Distill - Qwen - 1.5B模型微调的实验结果显示推理简洁，在不同场景中表现良好，与优化技术的兼容性提高。

Conclusion: 结构化推理集成到大型语言模型中是有效的。

Abstract: Recent Large Language Models (LLMs) have significantly advanced natural
language processing and automated decision-making. However, these models still
encounter difficulties when performing complex reasoning tasks involving
logical deduction and systematic planning, primarily due to their reliance on
implicit statistical relationships without structured knowledge
representation.Inspired by cognitive science and neurosymbolic AI, we introduce
a novel approach to enhance LLMs through explicit structured reasoning. First,
we convert unstructured data into structured formats by explicitly annotating
reasoning steps. We then employ this structured dataset to train LLMs through
Supervised Fine-Tuning (SFT). Additionally, we enhance the structured reasoning
capabilities of LLMs using Group Relative Policy Optimization (GRPO),
incorporating two innovative algorithms--MAX-Flow and Longest Common
Subsequence (LCS)--which notably improve reasoning effectiveness and reduce
computational complexity. Experimental results from fine-tuning a
DeepSeek-R1-Distill-Qwen-1.5B model demonstrate concise reasoning, robust
performance across various scenarios, and improved compatibility with
optimization techniques, validating the efficacy of structured reasoning
integration in LLMs.

</details>


### [170] [An Agentic System for Rare Disease Diagnosis with Traceable Reasoning](https://arxiv.org/abs/2506.20430)
*Weike Zhao,Chaoyi Wu,Yanjie Fan,Xiaoman Zhang,Pengcheng Qiu,Yuze Sun,Xiao Zhou,Yanfeng Wang,Ya Zhang,Yongguo Yu,Kun Sun,Weidi Xie*

Main category: cs.CL

TL;DR: 介绍首个由大语言模型驱动的罕见病诊断系统DeepRare，评估显示其诊断性能出色，并已实现为网页应用。


<details>
  <summary>Details</summary>
Motivation: 罕见病全球影响超3亿人，但及时准确诊断因临床异质性、低患病率和医生认知不足仍是难题。

Method: 构建DeepRare系统，包含带长期记忆模块的中央主机和负责特定领域分析任务的代理服务器，整合超40种专业工具和最新医学知识源。

Result: 在8个数据集评估中，对2919种疾病诊断表现出色，在多种评估中显著优于其他15种方法，推理链人工验证达成95.40%一致。

Conclusion: DeepRare系统具有出色诊断性能，模块化可扩展设计能实现复杂诊断推理，且已实现为用户友好的网页应用。

Abstract: Rare diseases collectively affect over 300 million individuals worldwide, yet
timely and accurate diagnosis remains a pervasive challenge. This is largely
due to their clinical heterogeneity, low individual prevalence, and the limited
familiarity most clinicians have with rare conditions. Here, we introduce
DeepRare, the first rare disease diagnosis agentic system powered by a large
language model (LLM), capable of processing heterogeneous clinical inputs. The
system generates ranked diagnostic hypotheses for rare diseases, each
accompanied by a transparent chain of reasoning that links intermediate
analytic steps to verifiable medical evidence.
  DeepRare comprises three key components: a central host with a long-term
memory module; specialized agent servers responsible for domain-specific
analytical tasks integrating over 40 specialized tools and web-scale,
up-to-date medical knowledge sources, ensuring access to the most current
clinical information. This modular and scalable design enables complex
diagnostic reasoning while maintaining traceability and adaptability. We
evaluate DeepRare on eight datasets. The system demonstrates exceptional
diagnostic performance among 2,919 diseases, achieving 100% accuracy for 1013
diseases. In HPO-based evaluations, DeepRare significantly outperforms other 15
methods, like traditional bioinformatics diagnostic tools, LLMs, and other
agentic systems, achieving an average Recall@1 score of 57.18% and surpassing
the second-best method (Reasoning LLM) by a substantial margin of 23.79
percentage points. For multi-modal input scenarios, DeepRare achieves 70.60% at
Recall@1 compared to Exomiser's 53.20% in 109 cases. Manual verification of
reasoning chains by clinical experts achieves 95.40% agreements. Furthermore,
the DeepRare system has been implemented as a user-friendly web application
http://raredx.cn/doctor.

</details>


### [171] [Leveraging AI Graders for Missing Score Imputation to Achieve Accurate Ability Estimation in Constructed-Response Tests](https://arxiv.org/abs/2506.20119)
*Masaki Uto,Yuma Ito*

Main category: cs.CL

TL;DR: 文章针对建构反应测试手动评分费力且成本高的问题，提出利用自动评分技术填补缺失分数的方法，能准确估计能力并减少手动评分工作量。


<details>
  <summary>Details</summary>
Motivation: 建构反应测试手动评分劳动密集且成本高，现有IRT方法在缺失分数比例增加时估计准确性下降，数据增强技术处理稀疏或异构数据不准确。

Method: 提出一种利用自动评分技术填补缺失分数的新方法，以实现基于IRT的准确能力估计。

Result: 该方法在能力估计上达到高准确性，显著减少了手动评分工作量。

Conclusion: 所提新方法有效解决了现有能力评估方法的问题，能准确估计能力并减少手动评分工作。

Abstract: Evaluating the abilities of learners is a fundamental objective in the field
of education. In particular, there is an increasing need to assess higher-order
abilities such as expressive skills and logical thinking. Constructed-response
tests such as short-answer and essay-based questions have become widely used as
a method to meet this demand. Although these tests are effective, they require
substantial manual grading, making them both labor-intensive and costly. Item
response theory (IRT) provides a promising solution by enabling the estimation
of ability from incomplete score data, where human raters grade only a subset
of answers provided by learners across multiple test items. However, the
accuracy of ability estimation declines as the proportion of missing scores
increases. Although data augmentation techniques for imputing missing scores
have been explored in order to address this limitation, they often struggle
with inaccuracy for sparse or heterogeneous data. To overcome these challenges,
this study proposes a novel method for imputing missing scores by leveraging
automated scoring technologies for accurate IRT-based ability estimation. The
proposed method achieves high accuracy in ability estimation while markedly
reducing manual grading workload.

</details>


### [172] [OctoThinker: Mid-training Incentivizes Reinforcement Learning Scaling](https://arxiv.org/abs/2506.20512)
*Zengzhi Wang,Fan Zhou,Xuefeng Li,Pengfei Liu*

Main category: cs.CL

TL;DR: 研究不同基础语言模型在强化学习后训练中的表现，提出两阶段中间训练策略 Stable-then-Decay 得到 OctoThinker 模型，发布开源模型和语料库。


<details>
  <summary>Details</summary>
Motivation: 深入了解何种基础语言模型适合强化学习，为开发下一代可扩展强化学习的基础模型提供依据。

Method: 研究中间训练策略对强化学习动态的影响，聚焦 Qwen 和 Llama 两个模型家族，提出两阶段中间训练策略 Stable-then-Decay。

Result: 高质量数学语料能提升模型性能；添加 QA 数据、长 CoT 推理示例和指令数据可增强 RL 结果；长 CoT 会带来一些问题；扩展中间训练可提升下游 RL 性能；得到 OctoThinker 模型。

Conclusion: 研究有助于塑造强化学习时代基础模型的预训练策略，发布开源模型和语料库以支持后续研究。

Abstract: Different base language model families, such as Llama and Qwen, exhibit
divergent behaviors during post-training with reinforcement learning (RL),
especially on reasoning-intensive tasks. What makes a base language model
suitable for reinforcement learning? Gaining deeper insight into this question
is essential for developing RL-scalable foundation models of the next
generation. In this work, we investigate how mid-training strategies shape RL
dynamics, focusing on two representative model families: Qwen and Llama. Our
study reveals that (1) high-quality mathematical corpora, such as
MegaMath-Web-Pro, significantly improve both base model and RL performance,
while existing alternatives (e.g., FineMath-4plus) fail to do so; (2) further
adding QA-style data, particularly long chain-of-thought (CoT) reasoning
examples, enhances RL outcomes, and instruction data further unlocks this
effect; (3) while long-CoT improves reasoning depth, it can also induce
verbosity of model responses and unstability of RL training, underscoring the
importance of data formatting; (4) scaling mid-training consistently leads to
stronger downstream RL performance. Building on these insights, we introduce a
two-stage mid-training strategy, Stable-then-Decay, in which base models are
first trained on 200B tokens with a constant learning rate, followed by 20B
tokens across three CoT-focused branches with learning rate decay. This yields
OctoThinker, a family of models demonstrating strong RL compatibility and
closing the performance gap with more RL-friendly model families, i.e., Qwen.
We hope our work will help shape pre-training strategies for foundation models
in the RL era. To support further research, we release our open-source models
along with a curated math reasoning-intensive corpus of over 70 billion tokens
(i.e., MegaMath-Web-Pro-Max).

</details>


### [173] [When Life Gives You Samples: The Benefits of Scaling up Inference Compute for Multilingual LLMs](https://arxiv.org/abs/2506.20544)
*Ammar Khairi,Daniel D'souza,Ye Shen,Julia Kreutzer,Sara Hooker*

Main category: cs.CL

TL;DR: 研究多语言、多任务场景下开放式生成任务推理时计算的扩展方法，提出新策略并取得显著效果，强调需语言和任务感知方法。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型推理计算扩展工作多聚焦英语和少数领域，作者关注跨开放式任务、可验证任务和多语言的技术。

Method: 研究基于温度变化的采样策略和选择策略，评估现有选择方法，提出适应多语言多任务推理场景的新采样和选择策略。

Result: 新的组合采样和选择方法使8B模型在m - ArenaHard - v2.0提示上胜率平均提升6.8，111B的Command - A模型在相同基准上胜率提升9.0。

Conclusion: 推理时计算需要语言和任务感知方法，以促进低资源语言性能提升。

Abstract: Recent advancements in large language models (LLMs) have shifted focus toward
scaling inference-time compute, improving performance without retraining the
model. A common approach is to sample multiple outputs in parallel, and select
one of these as the final output. However, work to date has focused on English
and a handful of domains such as math and code. In contrast, we are most
interested in techniques that generalize across open-ended tasks, formally
verifiable tasks, and across languages. In this work, we study how to robustly
scale inference-time compute for open-ended generative tasks in a multilingual,
multi-task setting.
  Our findings show that both sampling strategy based on temperature variation
and selection strategy must be adapted to account for diverse domains and
varied language settings. We evaluate existing selection methods, revealing
that strategies effective in English often fail to generalize across languages.
We propose novel sampling and selection strategies specifically adapted for
multilingual and multi-task inference scenarios, and show they yield notable
gains across languages and tasks. In particular, our combined sampling and
selection methods lead to an average +6.8 jump in win-rates for our 8B models
on m-ArenaHard-v2.0 prompts, against proprietary models such as Gemini. At
larger scale, Command-A (111B model) equipped with our methods, shows +9.0
improvement in win-rates on the same benchmark with just five samples against
single-sample decoding, a substantial increase at minimal cost. Our results
underscore the need for language- and task-aware approaches to inference-time
compute, aiming to democratize performance improvements in underrepresented
languages.

</details>


### [174] [Biomed-Enriched: A Biomedical Dataset Enriched with LLMs for Pretraining and Extracting Rare and Hidden Content](https://arxiv.org/abs/2506.20331)
*Rian Touchent,Nathan Godey,Eric de la Clergerie*

Main category: cs.CL

TL;DR: 介绍了Biomed - Enriched生物医学文本数据集，经两阶段注释构建，为生物医学和临床NLP提供资源，预训练实验显示其能改进模型性能。


<details>
  <summary>Details</summary>
Motivation: 临床文本因隐私限制难获取，需公开可用的临床病例数据集用于生物医学和临床NLP。

Method: 通过两阶段注释构建数据集，先用大语言模型注释PubMed段落，再微调小语言模型将标签扩展到PMC - OA语料库，还进行质量过滤和领域上采样。

Result: 得到包含200万临床病例段落的数据集，预训练实验中，临床数据上采样使MMLU ProfMed性能提升约5%，教育质量过滤使MedQA和MedMCQA提升约1%，组合技术可加快收敛。

Conclusion: 该数据集是有价值资源，可实现更高效有效的生物医学预训练策略。

Abstract: We introduce Biomed-Enriched, a biomedical text dataset constructed from
PubMed via a two-stage annotation process. In the first stage, a large language
model annotates 400K paragraphs from PubMed scientific articles, assigning
scores for their type (review, study, clinical case, other), domain (clinical,
biomedical, other), and educational quality. The educational quality score
(rated 1 to 5) estimates how useful a paragraph is for college-level learning.
These annotations are then used to fine-tune a small language model, which
propagates the labels across the full PMC-OA corpus. The resulting metadata
allows us to extract refined subsets, including 2M clinical case paragraphs
with over 450K high-quality ones from articles with commercial-use licenses,
and to construct several variants via quality filtering and domain upsampling.
Clinical text is typically difficult to access due to privacy constraints, as
hospital records cannot be publicly shared. Hence, our dataset provides an
alternative large-scale, openly available collection of clinical cases from
PubMed, making it a valuable resource for biomedical and clinical NLP.
Preliminary continual-pretraining experiments with OLMo2 suggest these curated
subsets enable targeted improvements, with clinical upsampling boosting
performance by ~5% on MMLU ProfMed and educational quality filtering improving
MedQA and MedMCQA by ~1%. Combinations of these techniques led to faster
convergence, reaching same performance with a third of training tokens,
indicating potential for more efficient and effective biomedical pretraining
strategies.

</details>


### [175] [Inside you are many wolves: Using cognitive models to interpret value trade-offs in LLMs](https://arxiv.org/abs/2506.20666)
*Sonia K. Murthy,Rosie Zhao,Jennifer Hu,Sham Kakade,Markus Wulfmeier,Peng Qian,Tomer Ullman*

Main category: cs.CL

TL;DR: 文章用礼貌言语的认知模型评估大语言模型（LLMs）中价值权衡情况，发现推理模型信息效用高于社会效用等结果，并指出方法能适应LLM发展，对形成假设和塑造训练机制有意义。


<details>
  <summary>Details</summary>
Motivation: 日常社交需权衡价值，但当前解释LLMs中价值动态多面概念的工具有限，故用认知模型评估LLMs的价值权衡。

Method: 使用礼貌言语的认知模型，对前沿黑盒模型的推理‘努力’程度和开源模型的强化学习后训练动态这两种模型设置进行系统评估。

Result: 推理模型信息效用高于社会效用；开源模型数学推理更强；LLMs训练早期效用值有大变化，基础模型和预训练数据选择影响持久。

Conclusion: 方法能适应LLM发展，对形成假设、塑造推理模型训练机制和控制模型训练中价值权衡有启示。

Abstract: Navigating everyday social situations often requires juggling conflicting
goals, such as conveying a harsh truth, maintaining trust, all while still
being mindful of another person's feelings. These value trade-offs are an
integral part of human decision-making and language use, however, current tools
for interpreting such dynamic and multi-faceted notions of values in LLMs are
limited. In cognitive science, so-called "cognitive models" provide formal
accounts of these trade-offs in humans, by modeling the weighting of a
speaker's competing utility functions in choosing an action or utterance. In
this work, we use a leading cognitive model of polite speech to interpret the
extent to which LLMs represent human-like trade-offs. We apply this lens to
systematically evaluate value trade-offs in two encompassing model settings:
degrees of reasoning "effort" in frontier black-box models, and RL
post-training dynamics of open-source models. Our results highlight patterns of
higher informational utility than social utility in reasoning models, and in
open-source models shown to be stronger in mathematical reasoning. Our findings
from LLMs' training dynamics suggest large shifts in utility values early on in
training with persistent effects of the choice of base model and pretraining
data, compared to feedback dataset or alignment method. We show that our method
is responsive to diverse aspects of the rapidly evolving LLM landscape, with
insights for forming hypotheses about other high-level behaviors, shaping
training regimes for reasoning models, and better controlling trade-offs
between values during model training.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [176] [From Codicology to Code: A Comparative Study of Transformer and YOLO-based Detectors for Layout Analysis in Historical Documents](https://arxiv.org/abs/2506.20326)
*Sergio Torres Aguilar*

Main category: cs.CV

TL;DR: 本文在三个不同复杂程度的数据集上对五种先进的目标检测架构进行基准测试，发现模型性能受架构、数据集和边界框表示影响，使用定向边界框对历史手稿建模至关重要，还指出Transformer和CNN - OBB模型存在权衡。


<details>
  <summary>Details</summary>
Motivation: 鲁棒的文档布局分析对具有复杂页面组织的历史文档自动化处理和理解至关重要，需评估不同模型在不同数据集上的性能。

Method: 在e - NDP、CATMuS和HORAE三个数据集上，评估两个基于Transformer的模型（Co - DETR、Grounding DINO）和三个YOLO变体（AABB、OBB、YOLO - World）。

Result: 在e - NDP数据集上Co - DETR表现最佳，在更复杂的CATMuS和HORAE数据集上CNN基的YOLOv11x - OBB显著优于其他模型。

Conclusion: 使用定向边界框是准确建模历史手稿的基本要求，Transformer的全局上下文感知适合结构化布局，CNN - OBB模型在视觉多样和复杂文档上泛化能力更强，两者存在权衡。

Abstract: Robust Document Layout Analysis (DLA) is critical for the automated
processing and understanding of historical documents with complex page
organizations. This paper benchmarks five state-of-the-art object detection
architectures on three annotated datasets representing a spectrum of
codicological complexity: The e-NDP, a corpus of Parisian medieval registers
(1326-1504); CATMuS, a diverse multiclass dataset derived from various medieval
and modern sources (ca.12th-17th centuries) and HORAE, a corpus of decorated
books of hours (ca.13th-16th centuries). We evaluate two Transformer-based
models (Co-DETR, Grounding DINO) against three YOLO variants (AABB, OBB, and
YOLO-World). Our findings reveal significant performance variations dependent
on model architecture, data set characteristics, and bounding box
representation. In the e-NDP dataset, Co-DETR achieves state-of-the-art results
(0.752 mAP@.50:.95), closely followed by YOLOv11X-OBB (0.721). Conversely, on
the more complex CATMuS and HORAE datasets, the CNN-based YOLOv11x-OBB
significantly outperforms all other models (0.564 and 0.568, respectively).
This study unequivocally demonstrates that using Oriented Bounding Boxes (OBB)
is not a minor refinement but a fundamental requirement for accurately modeling
the non-Cartesian nature of historical manuscripts. We conclude that a key
trade-off exists between the global context awareness of Transformers, ideal
for structured layouts, and the superior generalization of CNN-OBB models for
visually diverse and complex documents.

</details>


### [177] [BrokenVideos: A Benchmark Dataset for Fine-Grained Artifact Localization in AI-Generated Videos](https://arxiv.org/abs/2506.20103)
*Jiahao Lin,Weixuan Peng,Bojia Zi,Yifeng Gao,Xianbiao Qi,Xingjun Ma,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: 当前AI生成视频保真度有限，缺乏用于伪影定位的综合基准。本文引入BrokenVideos数据集，可提升模型定位能力，为相关研究奠定基础。


<details>
  <summary>Details</summary>
Motivation: 当前AI生成视频存在视觉伪影，且缺乏用于伪影定位的综合基准数据集，影响自动化质量控制和生成模型的改进。

Method: 引入包含3254个AI生成视频的BrokenVideos数据集，对每个视频进行像素级掩码标注，并经过人工详细检查验证。

Result: 在BrokenVideos数据集上训练先进的伪影检测模型和多模态大语言模型，显著提高了它们定位损坏区域的能力。

Conclusion: BrokenVideos数据集为生成视频模型中的伪影定位研究提供了关键基础。

Abstract: Recent advances in deep generative models have led to significant progress in
video generation, yet the fidelity of AI-generated videos remains limited.
Synthesized content often exhibits visual artifacts such as temporally
inconsistent motion, physically implausible trajectories, unnatural object
deformations, and local blurring that undermine realism and user trust.
Accurate detection and spatial localization of these artifacts are crucial for
both automated quality control and for guiding the development of improved
generative models. However, the research community currently lacks a
comprehensive benchmark specifically designed for artifact localization in AI
generated videos. Existing datasets either restrict themselves to video or
frame level detection or lack the fine-grained spatial annotations necessary
for evaluating localization methods. To address this gap, we introduce
BrokenVideos, a benchmark dataset of 3,254 AI-generated videos with
meticulously annotated, pixel-level masks highlighting regions of visual
corruption. Each annotation is validated through detailed human inspection to
ensure high quality ground truth. Our experiments show that training state of
the art artifact detection models and multi modal large language models (MLLMs)
on BrokenVideos significantly improves their ability to localize corrupted
regions. Through extensive evaluation, we demonstrate that BrokenVideos
establishes a critical foundation for benchmarking and advancing research on
artifact localization in generative video models. The dataset is available at:
https://broken-video-detection-datetsets.github.io/Broken-Video-Detection-Datasets.github.io/.

</details>


### [178] [EAR: Erasing Concepts from Unified Autoregressive Models](https://arxiv.org/abs/2506.20151)
*Haipeng Fan,Shiyuan Zhang,Baohunesitu,Zihang Guo,Huaiwen Zhang*

Main category: cs.CV

TL;DR: 提出EAR方法用于AR模型概念擦除，引入WGA和TLM策略，提出ECGVF基准，实验表明EAR在擦除效果和模型效用保留上有显著提升。


<details>
  <summary>Details</summary>
Motivation: 解决在AR模型中移除不想要的概念同时保持整体生成质量的挑战。

Method: 提出EAR方法，引入WGA和TLM策略，提出ECGVF基准，用结构化模板生成提示对，用视觉分类器过滤。

Result: 在ECGVF基准上用AR模型Janus - Pro实验，EAR在擦除效果和模型效用保留上有显著提升。

Conclusion: EAR方法能有效实现AR模型的概念擦除并保留模型效用。

Abstract: Autoregressive (AR) models have achieved unified and strong performance
across both visual understanding and image generation tasks. However, removing
undesired concepts from AR models while maintaining overall generation quality
remains an open challenge. In this paper, we propose Erasure Autoregressive
Model (EAR), a fine-tuning method for effective and utility-preserving concept
erasure in AR models. Specifically, we introduce Windowed Gradient Accumulation
(WGA) strategy to align patch-level decoding with erasure objectives, and
Thresholded Loss Masking (TLM) strategy to protect content unrelated to the
target concept during fine-tuning. Furthermore, we propose a novel benchmark,
Erase Concept Generator and Visual Filter (ECGVF), aim at provide a more
rigorous and comprehensive foundation for evaluating concept erasure in AR
models. Specifically, we first employ structured templates across diverse large
language models (LLMs) to pre-generate a large-scale corpus of
target-replacement concept prompt pairs. Subsequently, we generate images from
these prompts and subject them to rigorous filtering via a visual classifier to
ensure concept fidelity and alignment. Extensive experimental results conducted
on the ECGVF benchmark with the AR model Janus-Pro demonstrate that EAR
achieves marked improvements in both erasure effectiveness and model utility
preservation. Code is available at: https://github.com/immc-lab/ear/

</details>


### [179] [Loss-Aware Automatic Selection of Structured Pruning Criteria for Deep Neural Network Acceleration](https://arxiv.org/abs/2506.20152)
*Deepak Ghimire,Kilho Lee,Seong-heum Kim*

Main category: cs.CV

TL;DR: 本文提出LAASP方法用于压缩和加速深度神经网络，采用边训练边剪枝，实验证明有效，代码公开。


<details>
  <summary>Details</summary>
Motivation: 现有结构化剪枝方法多采用顺序流程，本文旨在提出更高效的剪枝方法，实现网络瘦身和加速。

Method: 采用边训练边剪枝方法，消除训练阶段，整合剪枝和微调阶段；根据网络在训练数据子集上的整体损失自动选择剪枝标准和层；每次减少预定义FLOPs后进行短暂再训练；自动确定各层最佳剪枝率。

Result: 在CIFAR - 10和ImageNet数据集上的VGGNet和ResNet模型实验有效，如ResNet56和ResNet110在CIFAR - 10上减少52%FLOPs且提高top - 1准确率，ResNet50在ImageNet上减少超42%FLOPs且top - 5准确率仅降0.33%。

Conclusion: 提出的LAASP方法能有效压缩和加速深度神经网络。

Abstract: Structured pruning is a well-established technique for compressing neural
networks, making it suitable for deployment in resource-limited edge devices.
This paper presents an efficient Loss-Aware Automatic Selection of Structured
Pruning Criteria (LAASP) for slimming and accelerating deep neural networks.
The majority of pruning methodologies employ a sequential process consisting of
three stages: 1) training, 2) pruning, and 3) fine-tuning, whereas the proposed
pruning technique adopts a pruning-while-training approach that eliminates the
first stage and integrates the second and third stages into a single cycle. The
automatic selection of magnitude or similarity-based filter pruning criteria
from a specified pool of criteria and the specific pruning layer at each
pruning iteration is guided by the network's overall loss on a small subset of
the training data. To mitigate the abrupt accuracy drop due to pruning, the
network is retrained briefly after each reduction of a predefined number of
floating-point operations (FLOPs). The optimal pruning rates for each layer in
the network are automatically determined, eliminating the need for manual
allocation of fixed or variable pruning rates for each layer. Experiments on
the VGGNet and ResNet models on the CIFAR-10 and ImageNet benchmark datasets
demonstrate the effectiveness of the proposed method. In particular, the
ResNet56 and ResNet110 models on the CIFAR-10 dataset significantly improve the
top-1 accuracy compared to state-of-the-art methods while reducing the network
FLOPs by 52\%. Furthermore, the ResNet50 model on the ImageNet dataset reduces
FLOPs by more than 42\% with a negligible 0.33\% drop in top-5 accuracy. The
source code of this paper is publicly available online -
https://github.com/ghimiredhikura/laasp.

</details>


### [180] [Progressive Alignment Degradation Learning for Pansharpening](https://arxiv.org/abs/2506.20179)
*Enzhe Zhao,Zhichang Guo,Yao Li,Fanghui Song,Boying Wu*

Main category: cs.CV

TL;DR: 本文指出Wald协议限制深度学习全色锐化模型泛化能力，提出PADM模块和HFreqdiff方法，实验证明优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有Wald协议不准确近似现实退化模式，限制了深度学习全色锐化模型的泛化能力。

Method: 提出Progressive Alignment Degradation Module (PADM)，利用两个子网络相互迭代学习准确的退化过程；引入HFreqdiff，将高频细节嵌入扩散框架并结合CFB和BACM模块。

Result: 实现高分辨率全色和多光谱图像的有效融合，显著提升空间锐度和质量。

Conclusion: 提出的方法在实验和消融研究中表现优于现有技术。

Abstract: Deep learning-based pansharpening has been shown to effectively generate
high-resolution multispectral (HRMS) images. To create supervised ground-truth
HRMS images, synthetic data generated using the Wald protocol is commonly
employed. This protocol assumes that networks trained on artificial
low-resolution data will perform equally well on high-resolution data. However,
well-trained models typically exhibit a trade-off in performance between
reduced-resolution and full-resolution datasets. In this paper, we delve into
the Wald protocol and find that its inaccurate approximation of real-world
degradation patterns limits the generalization of deep pansharpening models. To
address this issue, we propose the Progressive Alignment Degradation Module
(PADM), which uses mutual iteration between two sub-networks, PAlignNet and
PDegradeNet, to adaptively learn accurate degradation processes without relying
on predefined operators. Building on this, we introduce HFreqdiff, which embeds
high-frequency details into a diffusion framework and incorporates CFB and BACM
modules for frequency-selective detail extraction and precise reverse process
learning. These innovations enable effective integration of high-resolution
panchromatic and multispectral images, significantly enhancing spatial
sharpness and quality. Experiments and ablation studies demonstrate the
proposed method's superior performance compared to state-of-the-art techniques.

</details>


### [181] [Feature Hallucination for Self-supervised Action Recognition](https://arxiv.org/abs/2506.20342)
*Lei Wang,Piotr Koniusz*

Main category: cs.CV

TL;DR: 提出深度翻译动作识别框架，引入新描述符，集成多模态特征，处理特征不确定性，在多基准测试中达SOTA。


<details>
  <summary>Details</summary>
Motivation: 理解视频中人类动作需高层语义推理和多模态特征有效集成，提升动作识别准确性。

Method: 提出框架联合预测动作概念和辅助特征，用幻觉流推断缺失线索，引入ODF和SDF描述符，集成多模态特征，结合不确定性建模和鲁棒损失函数。

Result: 在Kinetics - 400、Kinetics - 600和Something - Something V2等基准测试中取得SOTA性能。

Conclusion: 该多模态自监督动作识别框架能有效捕捉细粒度动作动态。

Abstract: Understanding human actions in videos requires more than raw pixel analysis;
it relies on high-level semantic reasoning and effective integration of
multimodal features. We propose a deep translational action recognition
framework that enhances recognition accuracy by jointly predicting action
concepts and auxiliary features from RGB video frames. At test time,
hallucination streams infer missing cues, enriching feature representations
without increasing computational overhead. To focus on action-relevant regions
beyond raw pixels, we introduce two novel domain-specific descriptors. Object
Detection Features (ODF) aggregate outputs from multiple object detectors to
capture contextual cues, while Saliency Detection Features (SDF) highlight
spatial and intensity patterns crucial for action recognition. Our framework
seamlessly integrates these descriptors with auxiliary modalities such as
optical flow, Improved Dense Trajectories, skeleton data, and audio cues. It
remains compatible with state-of-the-art architectures, including I3D,
AssembleNet, Video Transformer Network, FASTER, and recent models like VideoMAE
V2 and InternVideo2. To handle uncertainty in auxiliary features, we
incorporate aleatoric uncertainty modeling in the hallucination step and
introduce a robust loss function to mitigate feature noise. Our multimodal
self-supervised action recognition framework achieves state-of-the-art
performance on multiple benchmarks, including Kinetics-400, Kinetics-600, and
Something-Something V2, demonstrating its effectiveness in capturing
fine-grained action dynamics.

</details>


### [182] [Pay Less Attention to Deceptive Artifacts: Robust Detection of Compressed Deepfakes on Online Social Networks](https://arxiv.org/abs/2506.20548)
*Manyi Li,Renshuai Tao,Yufan Liu,Chuangchuang Tan,Haotong Qin,Bing Li,Yunchao Wei,Yao Zhao*

Main category: cs.CV

TL;DR: 提出PLADA框架解决深度伪造检测问题，在26个数据集实验中表现出色，还引入“块效应”作为关键因素。


<details>
  <summary>Details</summary>
Motivation: 现有深度伪造检测方法忽略OSNs中压缩带来的“块效应”，且主要关注原始图像，与现实场景不符。

Method: 提出PLADA框架，包含处理块效应的Block Effect Eraser（B2E）和处理配对与非配对数据的Open Data Aggregation（ODA）两个核心模块。

Result: 在26个数据集的广泛实验中，PLADA在深度伪造检测上实现显著平衡，即使在有限配对数据和压缩情况下，也优于现有最优方法。

Conclusion: 引入“块效应”作为深度伪造检测的关键因素，为开放世界场景提供了强大解决方案。

Abstract: With the rapid advancement of deep learning, particularly through generative
adversarial networks (GANs) and diffusion models (DMs), AI-generated images, or
``deepfakes", have become nearly indistinguishable from real ones. These images
are widely shared across Online Social Networks (OSNs), raising concerns about
their misuse. Existing deepfake detection methods overlook the ``block effects"
introduced by compression in OSNs, which obscure deepfake artifacts, and
primarily focus on raw images, rarely encountered in real-world scenarios. To
address these challenges, we propose PLADA (Pay Less Attention to Deceptive
Artifacts), a novel framework designed to tackle the lack of paired data and
the ineffective use of compressed images. PLADA consists of two core modules:
Block Effect Eraser (B2E), which uses a dual-stage attention mechanism to
handle block effects, and Open Data Aggregation (ODA), which processes both
paired and unpaired data to improve detection. Extensive experiments across 26
datasets demonstrate that PLADA achieves a remarkable balance in deepfake
detection, outperforming SoTA methods in detecting deepfakes on OSNs, even with
limited paired data and compression. More importantly, this work introduces the
``block effect" as a critical factor in deepfake detection, providing a robust
solution for open-world scenarios. Our code is available at
https://github.com/ManyiLee/PLADA.

</details>


### [183] [A Transformer Based Handwriting Recognition System Jointly Using Online and Offline Features](https://arxiv.org/abs/2506.20255)
*Ayush Lodh,Ritabrata Chakraborty,Shivakumara Palaiahnakote,Umapada Pal*

Main category: cs.CV

TL;DR: 提出端到端网络结合离线图像和在线笔画数据进行手写识别，在多个数据集取得SOTA。


<details>
  <summary>Details</summary>
Motivation: 手写识别可从光栅化字形和笔轨迹的互补线索中受益，但多数系统仅利用一种模态。

Method: 引入端到端网络，在共享潜在空间对离线图像和在线笔画数据进行早期融合，使用补丁编码器和轻量级变压器处理数据，通过可学习潜在查询联合处理令牌流，在交叉熵损失目标下池化和解码。

Result: 在IAMOn - DB和VNOn - DB上实现SOTA，最高超过之前最佳结果1%，还展示了在ISI - Air数据集上的适应性。

Conclusion: 所提方法能在手写识别中有效融合多模态数据，提升性能和作者独立性。

Abstract: We posit that handwriting recognition benefits from complementary cues
carried by the rasterized complex glyph and the pen's trajectory, yet most
systems exploit only one modality. We introduce an end-to-end network that
performs early fusion of offline images and online stroke data within a shared
latent space. A patch encoder converts the grayscale crop into fixed-length
visual tokens, while a lightweight transformer embeds the $(x, y, \text{pen})$
sequence. Learnable latent queries attend jointly to both token streams,
yielding context-enhanced stroke embeddings that are pooled and decoded under a
cross-entropy loss objective. Because integration occurs before any high-level
classification, temporal cues reinforce each other during representation
learning, producing stronger writer independence. Comprehensive experiments on
IAMOn-DB and VNOn-DB demonstrate that our approach achieves state-of-the-art
accuracy, exceeding previous bests by up to 1\%. Our study also shows
adaptation of this pipeline with gesturification on the ISI-Air dataset. Our
code can be found here.

</details>


### [184] [Show, Tell and Summarize: Dense Video Captioning Using Visual Cue Aided Sentence Summarization](https://arxiv.org/abs/2506.20567)
*Zhiwang Zhang,Dong Xu,Wanli Ouyang,Chuanqi Tan*

Main category: cs.CV

TL;DR: 提出用于密集视频字幕的DaS框架，在ActivityNet Captions数据集实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 解决密集视频字幕问题，提出更有效的方法。

Method: 将未修剪长视频分割为多个事件提案，提取片段视觉特征生成描述语句，用两级LSTM和分层注意力机制总结语句。

Result: 在ActivityNet Captions数据集上进行了全面实验。

Conclusion: 新提出的DaS框架对密集视频字幕有效。

Abstract: In this work, we propose a division-and-summarization (DaS) framework for
dense video captioning. After partitioning each untrimmed long video as
multiple event proposals, where each event proposal consists of a set of short
video segments, we extract visual feature (e.g., C3D feature) from each segment
and use the existing image/video captioning approach to generate one sentence
description for this segment. Considering that the generated sentences contain
rich semantic descriptions about the whole event proposal, we formulate the
dense video captioning task as a visual cue aided sentence summarization
problem and propose a new two stage Long Short Term Memory (LSTM) approach
equipped with a new hierarchical attention mechanism to summarize all generated
sentences as one descriptive sentence with the aid of visual features.
Specifically, the first-stage LSTM network takes all semantic words from the
generated sentences and the visual features from all segments within one event
proposal as the input, and acts as the encoder to effectively summarize both
semantic and visual information related to this event proposal. The
second-stage LSTM network takes the output from the first-stage LSTM network
and the visual features from all video segments within one event proposal as
the input, and acts as the decoder to generate one descriptive sentence for
this event proposal. Our comprehensive experiments on the ActivityNet Captions
dataset demonstrate the effectiveness of our newly proposed DaS framework for
dense video captioning.

</details>


### [185] [Forensic Study of Paintings Through the Comparison of Fabrics](https://arxiv.org/abs/2506.20272)
*Juan José Murillo-Fuentes,Pablo M. Olmos,Laura Alba-Carcelén*

Main category: cs.CV

TL;DR: 本文提出基于深度学习评估纺织品相似度的新方法，应用于普拉多博物馆画布，证明方法可行且准确，为杰作分析开辟新途径。


<details>
  <summary>Details</summary>
Motivation: 传统基于线密度图匹配的画布相似度评估方法，在画布并非来自一卷相邻位置时无法应用，需要新方法。

Method: 设计并训练孪生深度学习模型比较图像对，提出相似度估计方法聚合多对布料样本预测结果得到相似度分数。

Result: 应用于普拉多博物馆画布，证实即使平纹画布线密度相似也能有效比较。

Conclusion: 所提方法可行且准确，为杰作分析提供新方向。

Abstract: The study of canvas fabrics in works of art is a crucial tool for
authentication, attribution and conservation. Traditional methods are based on
thread density map matching, which cannot be applied when canvases do not come
from contiguous positions on a roll. This paper presents a novel approach based
on deep learning to assess the similarity of textiles. We introduce an
automatic tool that evaluates the similarity between canvases without relying
on thread density maps. A Siamese deep learning model is designed and trained
to compare pairs of images by exploiting the feature representations learned
from the scans. In addition, a similarity estimation method is proposed,
aggregating predictions from multiple pairs of cloth samples to provide a
robust similarity score. Our approach is applied to canvases from the Museo
Nacional del Prado, corroborating the hypothesis that plain weave canvases,
widely used in painting, can be effectively compared even when their thread
densities are similar. The results demonstrate the feasibility and accuracy of
the proposed method, opening new avenues for the analysis of masterpieces.

</details>


### [186] [Causal Representation Learning with Observational Grouping for CXR Classification](https://arxiv.org/abs/2506.20582)
*Rajat Rasal,Avinash Kori,Ben Glocker*

Main category: cs.CV

TL;DR: 本文提出端到端框架，通过分组观察学习胸部X光疾病分类的可识别因果表示，实验证明其能提升泛化性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 可识别因果表示学习能挖掘数据生成过程中的真实因果关系，在医学影像中有助于提升特定任务潜在特征的泛化性和鲁棒性。

Method: 引入分组观察概念，通过端到端框架学习胸部X光疾病分类的可识别表示。

Result: 实验表明，当通过分组确保关于种族、性别和成像视图的不变性时，这些因果表示能提升多个分类任务的泛化性和鲁棒性。

Conclusion: 使用分组观察学习可识别因果表示用于胸部X光疾病分类是有效的，能改善泛化性和鲁棒性。

Abstract: Identifiable causal representation learning seeks to uncover the true causal
relationships underlying a data generation process. In medical imaging, this
presents opportunities to improve the generalisability and robustness of
task-specific latent features. This work introduces the concept of grouping
observations to learn identifiable representations for disease classification
in chest X-rays via an end-to-end framework. Our experiments demonstrate that
these causal representations improve generalisability and robustness across
multiple classification tasks when grouping is used to enforce invariance w.r.t
race, sex, and imaging views.

</details>


### [187] [Dense Video Captioning using Graph-based Sentence Summarization](https://arxiv.org/abs/2506.20583)
*Zhiwang Zhang,Dong Xu,Wanli Ouyang,Luping Zhou*

Main category: cs.CV

TL;DR: 提出基于图的分区与总结（GPaS）框架用于密集视频字幕，分两阶段处理，聚焦总结阶段，通过GCN - LSTM结合处理语义词，在两数据集上验证有效性。


<details>
  <summary>Details</summary>
Motivation: 现有密集视频字幕方法未充分探索事件时间提案内的场景演变，在场景和对象变化的长提案上表现不佳。

Method: 提出GPaS框架，分分区和总结两阶段，总结阶段将语义词视为图节点，结合GCN和LSTM学习交互，提出两种GLI模块。

Result: 在ActivityNet Captions和YouCook II两个基准数据集上与现有方法进行广泛比较，证明了该方法的有效性。

Conclusion: 所提出的基于图的分区与总结框架及GCN - LSTM结合方法能有效处理密集视频字幕，解决现有方法不足。

Abstract: Recently, dense video captioning has made attractive progress in detecting
and captioning all events in a long untrimmed video. Despite promising results
were achieved, most existing methods do not sufficiently explore the scene
evolution within an event temporal proposal for captioning, and therefore
perform less satisfactorily when the scenes and objects change over a
relatively long proposal. To address this problem, we propose a graph-based
partition-and-summarization (GPaS) framework for dense video captioning within
two stages. For the ``partition" stage, a whole event proposal is split into
short video segments for captioning at a finer level. For the ``summarization"
stage, the generated sentences carrying rich description information for each
segment are summarized into one sentence to describe the whole event. We
particularly focus on the ``summarization" stage, and propose a framework that
effectively exploits the relationship between semantic words for summarization.
We achieve this goal by treating semantic words as nodes in a graph and
learning their interactions by coupling Graph Convolutional Network (GCN) and
Long Short Term Memory (LSTM), with the aid of visual cues. Two schemes of
GCN-LSTM Interaction (GLI) modules are proposed for seamless integration of GCN
and LSTM. The effectiveness of our approach is demonstrated via an extensive
comparison with the state-of-the-arts methods on the two benchmarks ActivityNet
Captions dataset and YouCook II dataset.

</details>


### [188] [InvZW: Invariant Feature Learning via Noise-Adversarial Training for Robust Image Zero-Watermarking](https://arxiv.org/abs/2506.20370)
*Abdullah All Tanvir,Xin Zhong*

Main category: cs.CV

TL;DR: 本文提出基于失真不变特征学习的图像零水印深度学习框架，实验表明该方法在特征稳定性和水印恢复方面达最优，泛化和鲁棒性超现有技术。


<details>
  <summary>Details</summary>
Motivation: 设计一种新的图像零水印方法，在不改变原始图像的前提下，实现鲁棒的零水印。

Method: 框架包含两个关键模块，一是通过噪声对抗学习训练特征提取器，二是设计基于学习的多比特零水印方案，将特征投影到参考码上。

Result: 在不同图像数据集和多种失真情况下，该方法在特征稳定性和水印恢复上达到最优，与现有技术对比显示出更好的泛化和鲁棒性。

Conclusion: 所提出的深度学习框架具有优异的鲁棒性和泛化能力，是一种有效的图像零水印方法。

Abstract: This paper introduces a novel deep learning framework for robust image
zero-watermarking based on distortion-invariant feature learning. As a
zero-watermarking scheme, our method leaves the original image unaltered and
learns a reference signature through optimization in the feature space. The
proposed framework consists of two key modules. In the first module, a feature
extractor is trained via noise-adversarial learning to generate representations
that are both invariant to distortions and semantically expressive. This is
achieved by combining adversarial supervision against a distortion
discriminator and a reconstruction constraint to retain image content. In the
second module, we design a learning-based multibit zero-watermarking scheme
where the trained invariant features are projected onto a set of trainable
reference codes optimized to match a target binary message. Extensive
experiments on diverse image datasets and a wide range of distortions show that
our method achieves state-of-the-art robustness in both feature stability and
watermark recovery. Comparative evaluations against existing self-supervised
and deep watermarking techniques further highlight the superiority of our
framework in generalization and robustness.

</details>


### [189] [Exploiting Lightweight Hierarchical ViT and Dynamic Framework for Efficient Visual Tracking](https://arxiv.org/abs/2506.20381)
*Ben Kang,Xin Chen,Jie Zhao,Chunjuan Bo,Dong Wang,Huchuan Lu*

Main category: cs.CV

TL;DR: 提出HiT和DyHiT高效跟踪模型，前者用桥接模块和双图像位置编码，后者动态适应场景复杂度，还提出免训练加速方法提升其他跟踪器速度。


<details>
  <summary>Details</summary>
Motivation: 解决基于Transformer的视觉跟踪器在资源受限设备上处理速度慢、实用性有限的问题。

Method: HiT引入桥接模块和双图像位置编码；DyHiT用高效动态路由器分类场景并采用分治策略；基于DyHiT动态路由架构提出免训练加速方法。

Result: HiT在NVIDIA Jetson AGX平台达61 fps，LaSOT基准AUC 64.6%；DyHiT最快版本达111 fps，LaSOT AUC 62.4%；加速方法使SeqTrack - B256在NVIDIA GeForce RTX 2080 Ti GPU上速度提升2.68倍，AUC不变。

Conclusion: 所提模型和方法能在保证跟踪精度的同时，显著提升跟踪器在不同设备上的处理速度。

Abstract: Transformer-based visual trackers have demonstrated significant advancements
due to their powerful modeling capabilities. However, their practicality is
limited on resource-constrained devices because of their slow processing
speeds. To address this challenge, we present HiT, a novel family of efficient
tracking models that achieve high performance while maintaining fast operation
across various devices. The core innovation of HiT lies in its Bridge Module,
which connects lightweight transformers to the tracking framework, enhancing
feature representation quality. Additionally, we introduce a dual-image
position encoding approach to effectively encode spatial information. HiT
achieves an impressive speed of 61 frames per second (fps) on the NVIDIA Jetson
AGX platform, alongside a competitive AUC of 64.6% on the LaSOT benchmark,
outperforming all previous efficient trackers.Building on HiT, we propose
DyHiT, an efficient dynamic tracker that flexibly adapts to scene complexity by
selecting routes with varying computational requirements. DyHiT uses search
area features extracted by the backbone network and inputs them into an
efficient dynamic router to classify tracking scenarios. Based on the
classification, DyHiT applies a divide-and-conquer strategy, selecting
appropriate routes to achieve a superior trade-off between accuracy and speed.
The fastest version of DyHiT achieves 111 fps on NVIDIA Jetson AGX while
maintaining an AUC of 62.4% on LaSOT.Furthermore, we introduce a training-free
acceleration method based on the dynamic routing architecture of DyHiT. This
method significantly improves the execution speed of various high-performance
trackers without sacrificing accuracy. For instance, our acceleration method
enables the state-of-the-art tracker SeqTrack-B256 to achieve a 2.68 times
speedup on an NVIDIA GeForce RTX 2080 Ti GPU while maintaining the same AUC of
69.9% on the LaSOT.

</details>


### [190] [Disentangled representations of microscopy images](https://arxiv.org/abs/2506.20649)
*Jacopo Dapueto,Vito Paolo Pastore,Nicoletta Noceti,Francesca Odone*

Main category: cs.CV

TL;DR: 提出解纠缠表示学习（DRL）方法用于显微图像分类，在准确性和可解释性间取得良好平衡。


<details>
  <summary>Details</summary>
Motivation: 现代采集系统使图像数量增多，虽深度学习方法有进展，但显微图像分析的可解释性仍是挑战。

Method: 提出DRL方法，利用三个不同微观图像领域的基准数据集，基于合成数据学习的表示进行迁移。

Result: 展示了DRL框架在准确性和可解释性之间能取得良好平衡。

Conclusion: DRL方法可提升显微图像分类模型的可解释性。

Abstract: Microscopy image analysis is fundamental for different applications, from
diagnosis to synthetic engineering and environmental monitoring. Modern
acquisition systems have granted the possibility to acquire an escalating
amount of images, requiring a consequent development of a large collection of
deep learning-based automatic image analysis methods. Although deep neural
networks have demonstrated great performance in this field, interpretability,
an essential requirement for microscopy image analysis, remains an open
challenge.
  This work proposes a Disentangled Representation Learning (DRL) methodology
to enhance model interpretability for microscopy image classification.
Exploiting benchmark datasets from three different microscopic image domains
(plankton, yeast vacuoles, and human cells), we show how a DRL framework, based
on transferring a representation learnt from synthetic data, can provide a good
trade-off between accuracy and interpretability in this domain.

</details>


### [191] [HiWave: Training-Free High-Resolution Image Generation via Wavelet-Based Diffusion Sampling](https://arxiv.org/abs/2506.20452)
*Tobias Vontobel,Seyedmorteza Sadat,Farnood Salehi,Romann M. Weber*

Main category: cs.CV

TL;DR: 本文提出无训练零样本方法HiWave，用预训练扩散模型提升超高分辨率图像合成质量，经评估和用户研究证明有效。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在高分辨率训练计算成本高，零样本生成技术合成图像有伪影问题。

Method: 采用两阶段流程，先从预训练模型生成基础图像，再进行逐块DDIM反演和基于小波的细节增强模块。利用反演方法得到保留全局一致性的初始噪声向量，采样时小波域细节增强器保留低频分量确保结构一致，引导高频分量丰富细节。

Result: 使用Stable Diffusion XL评估表明HiWave有效减少常见视觉伪影，用户研究中超80%对比中被首选。

Conclusion: HiWave无需重新训练或架构修改，对高质量、超高分辨率图像合成有效。

Abstract: Diffusion models have emerged as the leading approach for image synthesis,
demonstrating exceptional photorealism and diversity. However, training
diffusion models at high resolutions remains computationally prohibitive, and
existing zero-shot generation techniques for synthesizing images beyond
training resolutions often produce artifacts, including object duplication and
spatial incoherence. In this paper, we introduce HiWave, a training-free,
zero-shot approach that substantially enhances visual fidelity and structural
coherence in ultra-high-resolution image synthesis using pretrained diffusion
models. Our method employs a two-stage pipeline: generating a base image from
the pretrained model followed by a patch-wise DDIM inversion step and a novel
wavelet-based detail enhancer module. Specifically, we first utilize inversion
methods to derive initial noise vectors that preserve global coherence from the
base image. Subsequently, during sampling, our wavelet-domain detail enhancer
retains low-frequency components from the base image to ensure structural
consistency, while selectively guiding high-frequency components to enrich fine
details and textures. Extensive evaluations using Stable Diffusion XL
demonstrate that HiWave effectively mitigates common visual artifacts seen in
prior methods, achieving superior perceptual quality. A user study confirmed
HiWave's performance, where it was preferred over the state-of-the-art
alternative in more than 80% of comparisons, highlighting its effectiveness for
high-quality, ultra-high-resolution image synthesis without requiring
retraining or architectural modifications.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [192] [Irec: A Metacognitive Scaffolding for Self-Regulated Learning through Just-in-Time Insight Recall: A Conceptual Framework and System Prototype](https://arxiv.org/abs/2506.20156)
*Xuefei Hou,Xizhao Tan*

Main category: cs.HC

TL;DR: 论文指出学习核心挑战转向有效自我调节学习，现有数字工具支持不足，介绍‘Insight Recall’范式及Irec系统以应对挑战，贡献理论框架和系统平台。


<details>
  <summary>Details</summary>
Motivation: 现有数字工具在支持元认知反思方面不足，SRS忽视情境作用，PKM工具需高手动维护，需新方法促进自我调节学习。

Method: 引入‘Insight Recall’范式，用JITAI框架形式化，实现Irec原型系统，利用动态知识图、混合检索引擎、LLM评估，有人在环知识图构建管道，还提出‘Guided Inquiry’模块。

Result: 提出了一个坚实的理论框架和一个可用的系统平台。

Conclusion: 为设计增强元认知和自我调节的下一代智能学习系统提供了理论框架和系统平台。

Abstract: The core challenge in learning has shifted from knowledge acquisition to
effective Self-Regulated Learning (SRL): planning, monitoring, and reflecting
on one's learning. Existing digital tools, however, inadequately support
metacognitive reflection. Spaced Repetition Systems (SRS) use de-contextualized
review, overlooking the role of context, while Personal Knowledge Management
(PKM) tools require high manual maintenance.
  To address these challenges, this paper introduces "Insight Recall," a novel
paradigm that conceptualizes the context-triggered retrieval of personal past
insights as a metacognitive scaffold to promote SRL. We formalize this paradigm
using the Just-in-Time Adaptive Intervention (JITAI) framework and implement a
prototype system, Irec, to demonstrate its feasibility. At its core, Irec uses
a dynamic knowledge graph of the user's learning history. When a user faces a
new problem, a hybrid retrieval engine recalls relevant personal "insights."
Subsequently, a large language model (LLM) performs a deep similarity
assessment to filter and present the most relevant scaffold in a just-in-time
manner. To reduce cognitive load, Irec features a human-in-the-loop pipeline
for LLM-based knowledge graph construction. We also propose an optional "Guided
Inquiry" module, where users can engage in a Socratic dialogue with an expert
LLM, using the current problem and recalled insights as context. The
contribution of this paper is a solid theoretical framework and a usable system
platform for designing next-generation intelligent learning systems that
enhance metacognition and self-regulation.

</details>


### [193] [A Literature Review on Simulation in Conversational Recommender Systems](https://arxiv.org/abs/2506.20291)
*Haoran Zhang,Xin Zhao,Jinze Chen,Junpeng Guo*

Main category: cs.HC

TL;DR: 本文对对话推荐系统（CRSs）研究中的模拟方法进行综述，构建分类框架，指出模拟方法作用与挑战并给出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 全面分析CRSs研究中模拟方法，了解研究现状并为未来研究提供方向。

Method: 开发分类框架将相关出版物分为数据集构建、算法设计、系统评估和实证研究四类。

Result: 模拟方法在应对CRSs主要挑战中起关键作用，如基于大语言模型的模拟方法可用于创建数据、改进算法和评估系统，但存在数据集偏差等挑战。

Conclusion: 尽管存在挑战，模拟方法在推动CRS研究方面仍有巨大潜力，综述为该领域提供了研究现状总结和未来研究方向。

Abstract: Conversational Recommender Systems (CRSs) have garnered attention as a novel
approach to delivering personalized recommendations through multi-turn
dialogues. This review developed a taxonomy framework to systematically
categorize relevant publications into four groups: dataset construction,
algorithm design, system evaluation, and empirical studies, providing a
comprehensive analysis of simulation methods in CRSs research. Our analysis
reveals that simulation methods play a key role in tackling CRSs' main
challenges. For example, LLM-based simulation methods have been used to create
conversational recommendation data, enhance CRSs algorithms, and evaluate CRSs.
Despite several challenges, such as dataset bias, the limited output
flexibility of LLM-based simulations, and the gap between text semantic space
and behavioral semantics, persist due to the complexity in Human-Computer
Interaction (HCI) of CRSs, simulation methods hold significant potential for
advancing CRS research. This review offers a thorough summary of the current
research landscape in this domain and identifies promising directions for
future inquiry.

</details>


### [194] [Beyond Autocomplete: Designing CopilotLens Towards Transparent and Explainable AI Coding Agents](https://arxiv.org/abs/2506.20062)
*Runlong Ye,Zeling Zhang,Boushra Almazroua,Michael Liut*

Main category: cs.HC

TL;DR: 当前AI代码助手缺乏解释性，本文提出CopilotLens框架让代码补全可解释，利于人机协作。


<details>
  <summary>Details</summary>
Motivation: 现有AI代码助手给出建议但不解释原理，影响开发者评估和建立信任，需要提升解释性。

Method: 引入CopilotLens框架，通过动态两级界面揭示AI代码助手的“思考过程”。

Result: 提出CopilotLens的设计和原理。

Conclusion: CopilotLens为构建注重推理清晰性的代码助手提供框架，促进人机深度协作。

Abstract: AI-powered code assistants are widely used to generate code completions,
significantly boosting developer productivity. However, these tools typically
present suggestions without explaining their rationale, leaving their
decision-making process inscrutable. This opacity hinders developers' ability
to critically evaluate the output, form accurate mental models, and build
calibrated trust in the system. To address this, we introduce CopilotLens, a
novel interactive framework that reframes code completion from a simple
suggestion into a transparent, explainable event. CopilotLens operates as an
explanation layer that reveals the AI agent's "thought process" through a
dynamic two-level interface, surfacing everything from its reconstructed
high-level plans to the specific codebase context influencing the code. This
paper presents the design and rationale of CopilotLens, offering a concrete
framework for building future agentic code assistants that prioritize clarity
of reasoning over speed of suggestion, thereby fostering deeper comprehension
and more robust human-AI collaboration.

</details>


### [195] [AI in the Writing Process: How Purposeful AI Support Fosters Student Writing](https://arxiv.org/abs/2506.20595)
*Momin N. Siddiqui,Roy Pea,Hari Subramonyam*

Main category: cs.HC

TL;DR: 研究不同AI写作支持方式对学生写作能动性和知识转化深度的影响，发现集成AI写作工具效果更佳。


<details>
  <summary>Details</summary>
Motivation: 因ChatGPT等技术普及引发对学生写作影响的担忧，探究不同AI支持方法对写作能动性和知识转化的作用。

Method: 对90名本科生进行随机对照试验，比较聊天式LLM写作助手、集成AI写作工具和标准写作界面三种情况。

Result: 在AI支持条件下，使用集成AI写作工具的学生在写作过程中有更强能动性，知识转化更深入。

Conclusion: 针对写作过程特定方面精心设计的AI写作支持能助学生掌控作品，提升内容参与度。

Abstract: The ubiquity of technologies like ChatGPT has raised concerns about their
impact on student writing, particularly regarding reduced learner agency and
superficial engagement with content. While standalone chat-based LLMs often
produce suboptimal writing outcomes, evidence suggests that purposefully
designed AI writing support tools can enhance the writing process. This paper
investigates how different AI support approaches affect writers' sense of
agency and depth of knowledge transformation. Through a randomized control
trial with 90 undergraduate students, we compare three conditions: (1) a
chat-based LLM writing assistant, (2) an integrated AI writing tool to support
diverse subprocesses, and (3) a standard writing interface (control). Our
findings demonstrate that, among AI-supported conditions, students using the
integrated AI writing tool exhibited greater agency over their writing process
and engaged in deeper knowledge transformation overall. These results suggest
that thoughtfully designed AI writing support targeting specific aspects of the
writing process can help students maintain ownership of their work while
facilitating improved engagement with content.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [196] [VoxelOpt: Voxel-Adaptive Message Passing for Discrete Optimization in Deformable Abdominal CT Registration](https://arxiv.org/abs/2506.19975)
*Hang Zhang,Yuxi Zhang,Jiazheng Wang,Xiang Chen,Renjiu Hu,Xin Tian,Gaolei Li,Min Liu*

Main category: eess.IV

TL;DR: 提出VoxelOpt框架结合学习和迭代方法优势，用于可变形图像配准，在腹部CT配准中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有学习和迭代方法在可变形图像配准中分别存在训练数据有限、大变形处理差和速度慢的问题，需结合两者优势。

Method: VoxelOpt用局部代价体的位移熵测量体素位移信号强度，采用体素级自适应消息传递、多级图像金字塔和预训练分割模型提取特征。

Result: 在腹部CT配准中，VoxelOpt在效率和精度上优于领先迭代方法，与有标签监督的学习方法相当。

Conclusion: VoxelOpt能在配准精度和运行时间间取得更好平衡。

Abstract: Recent developments in neural networks have improved deformable image
registration (DIR) by amortizing iterative optimization, enabling fast and
accurate DIR results. However, learning-based methods often face challenges
with limited training data, large deformations, and tend to underperform
compared to iterative approaches when label supervision is unavailable. While
iterative methods can achieve higher accuracy in such scenarios, they are
considerably slower than learning-based methods. To address these limitations,
we propose VoxelOpt, a discrete optimization-based DIR framework that combines
the strengths of learning-based and iterative methods to achieve a better
balance between registration accuracy and runtime. VoxelOpt uses displacement
entropy from local cost volumes to measure displacement signal strength at each
voxel, which differs from earlier approaches in three key aspects. First, it
introduces voxel-wise adaptive message passing, where voxels with lower entropy
receives less influence from their neighbors. Second, it employs a multi-level
image pyramid with 27-neighbor cost volumes at each level, avoiding exponential
complexity growth. Third, it replaces hand-crafted features or contrastive
learning with a pretrained foundational segmentation model for feature
extraction. In abdominal CT registration, these changes allow VoxelOpt to
outperform leading iterative in both efficiency and accuracy, while matching
state-of-the-art learning-based methods trained with label supervision. The
source code will be available at https://github.com/tinymilky/VoxelOpt

</details>


### [197] [Weighted Mean Frequencies: a handcraft Fourier feature for 4D Flow MRI segmentation](https://arxiv.org/abs/2506.20614)
*Simon Perrin,Sébastien Levilly,Huajun Sun,Harold Mouchère,Jean-Michel Serfaty*

Main category: eess.IV

TL;DR: 本文提出加权平均频率（WMF）特征用于4D Flow MRI图像分割，实验表明其比PC - MRA特征效果好，有望用于其他血管区域分割。


<details>
  <summary>Details</summary>
Motivation: 4D Flow MRI图像存在分辨率不足和噪声问题，现有生物标志物受血管分割分辨率影响，旨在引入新特征辅助分割。

Method: 提出WMF特征，通过最优阈值和深度学习方法对4D Flow MRI图像进行分割实验。

Result: 深度学习任务中，与PC - MRA特征相比，IoU和Dice分别提高0.12和0.13。

Conclusion: WMF特征有潜力为其他血管区域的分割过程提供有价值的见解。

Abstract: In recent decades, the use of 4D Flow MRI images has enabled the
quantification of velocity fields within a volume of interest and along the
cardiac cycle. However, the lack of resolution and the presence of noise in
these biomarkers are significant issues. As indicated by recent studies, it
appears that biomarkers such as wall shear stress are particularly impacted by
the poor resolution of vessel segmentation. The Phase Contrast Magnetic
Resonance Angiography (PC-MRA) is the state-of-the-art method to facilitate
segmentation. The objective of this work is to introduce a new handcraft
feature that provides a novel visualisation of 4D Flow MRI images, which is
useful in the segmentation task. This feature, termed Weighted Mean Frequencies
(WMF), is capable of revealing the region in three dimensions where a voxel has
been passed by pulsatile flow. Indeed, this feature is representative of the
hull of all pulsatile velocity voxels. The value of the feature under
discussion is illustrated by two experiments. The experiments involved
segmenting 4D Flow MRI images using optimal thresholding and deep learning
methods. The results obtained demonstrate a substantial enhancement in terms of
IoU and Dice, with a respective increase of 0.12 and 0.13 in comparison with
the PC-MRA feature, as evidenced by the deep learning task. This feature has
the potential to yield valuable insights that could inform future segmentation
processes in other vascular regions, such as the heart or the brain.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [198] [RepuNet: A Reputation System for Mitigating Malicious Clients in DFL](https://arxiv.org/abs/2506.19892)
*Isaac Marroqui Penalva,Enrique Tomás Martínez Beltrán,Manuel Gil Pérez,Alberto Huertas Celdrán*

Main category: cs.CR

TL;DR: 本文提出RepuNet去中心化声誉系统应对DFL安全威胁，实验证明其有效检测和缓解恶意行为，有实际应用潜力。


<details>
  <summary>Details</summary>
Motivation: DFL存在新安全漏洞，现有解决方案有计算开销、可扩展性和适应性问题。

Method: 提出RepuNet系统，通过模型相似度、参数变化等指标评估节点行为，调整节点在模型聚合中的影响力。

Result: 将RepuNet集成到Nebula DFL平台实验，MNIST场景F1分数超95%，CIFAR - 10约76%。

Conclusion: RepuNet具有适应性、鲁棒性，能有效缓解DFL环境威胁。

Abstract: Decentralized Federated Learning (DFL) enables nodes to collaboratively train
models without a central server, introducing new vulnerabilities since each
node independently selects peers for model aggregation. Malicious nodes may
exploit this autonomy by sending corrupted models (model poisoning), delaying
model submissions (delay attack), or flooding the network with excessive
messages, negatively affecting system performance. Existing solutions often
depend on rigid configurations or additional infrastructures such as
blockchain, leading to computational overhead, scalability issues, or limited
adaptability. To overcome these limitations, this paper proposes RepuNet, a
decentralized reputation system that categorizes threats in DFL and dynamically
evaluates node behavior using metrics like model similarity, parameter changes,
message latency, and communication volume. Nodes' influence in model
aggregation is adjusted based on their reputation scores. RepuNet was
integrated into the Nebula DFL platform and experimentally evaluated with MNIST
and CIFAR-10 datasets under non-IID distributions, using federations of up to
25 nodes in both fully connected and random topologies. Different attack
intensities, frequencies, and activation intervals were tested. Results
demonstrated that RepuNet effectively detects and mitigates malicious behavior,
achieving F1 scores above 95% for MNIST scenarios and approximately 76% for
CIFAR-10 cases. These outcomes highlight RepuNet's adaptability, robustness,
and practical potential for mitigating threats in decentralized federated
learning environments.

</details>


### [199] [Quantum-Resistant Domain Name System: A Comprehensive System-Level Study](https://arxiv.org/abs/2506.19943)
*Juyoul Lee,Sanzida Hoque,Abdullah Aydeger,Engin Zeydan*

Main category: cs.CR

TL;DR: 本文对三种广泛部署的机制进行后量子DNS安全的系统级研究，提出PQC - DNS框架，实验分析了后量子原语的影响并给出安全建议。


<details>
  <summary>Details</summary>
Motivation: 量子计算机对DNS核心协议构成威胁，需要确保后量子时代DNS的机密性、真实性和完整性。

Method: 提出PQC - DNS统一框架，利用OQS库，将格和基于哈希的原语集成到BIND9和TLS 1.3栈，形式化性能和威胁模型。

Result: 基于格的原语有较好的延迟和资源表现，基于哈希的方案增加消息大小和处理开销，还发现了一些安全隐患。

Conclusion: 研究结果为部署量子弹性DNS提供实用指导，助力保障后量子时代核心互联网协议安全。

Abstract: The Domain Name System (DNS) plays a foundational role in Internet
infrastructure, yet its core protocols remain vulnerable to compromise by
quantum adversaries. As cryptographically relevant quantum computers become a
realistic threat, ensuring DNS confidentiality, authenticity, and integrity in
the post-quantum era is imperative. In this paper, we present a comprehensive
system-level study of post-quantum DNS security across three widely deployed
mechanisms: DNSSEC, DNS-over-TLS (DoT), and DNS-over-HTTPS (DoH). We propose
Post-Quantum Cryptographic (PQC)-DNS, a unified framework for benchmarking DNS
security under legacy, post-quantum, and hybrid cryptographic configurations.
Our implementation leverages the Open Quantum Safe (OQS) libraries and
integrates lattice- and hash-based primitives into BIND9 and TLS 1.3 stacks. We
formalize performance and threat models and analyze the impact of post-quantum
key encapsulation and digital signatures on end-to-end DNS resolution.
Experimental results on a containerized testbed reveal that lattice-based
primitives such as Module-Lattice-Based Key-Encapsulation Mechanism (MLKEM) and
Falcon offer practical latency and resource profiles, while hash-based schemes
like SPHINCS+ significantly increase message sizes and processing overhead. We
also examine security implications including downgrade risks, fragmentation
vulnerabilities, and susceptibility to denial-of-service amplification. Our
findings inform practical guidance for deploying quantum-resilient DNS and
contribute to the broader effort of securing core Internet protocols for the
post-quantum future.

</details>


### [200] [Can One Safety Loop Guard Them All? Agentic Guard Rails for Federated Computing](https://arxiv.org/abs/2506.20000)
*Narasimha Raghavan Veeraragavan,Jan Franz Nygård*

Main category: cs.CR

TL;DR: 提出Guardian - FC框架用于隐私保护联邦计算，统一不同隐私保护机制安全执行，介绍设计与特性，给出定性场景、形式化模型并提出研究议程。


<details>
  <summary>Details</summary>
Motivation: 统一不同隐私保护机制（如FHE、MPC、DP）在联邦计算中的安全执行。

Method: 设计后端中立的领域特定语言编写插件和可互换执行提供者，通过代理AI控制平面执行有限状态安全循环，采用以清单为中心的设计。

Result: 提出定性场景说明后端无关安全，构建形式化模型基础用于验证。

Conclusion: 邀请社区推进自适应护栏调整、多后端组合等方面的研究。

Abstract: We propose Guardian-FC, a novel two-layer framework for privacy preserving
federated computing that unifies safety enforcement across diverse privacy
preserving mechanisms, including cryptographic back-ends like fully homomorphic
encryption (FHE) and multiparty computation (MPC), as well as statistical
techniques such as differential privacy (DP). Guardian-FC decouples guard-rails
from privacy mechanisms by executing plug-ins (modular computation units),
written in a backend-neutral, domain-specific language (DSL) designed
specifically for federated computing workflows and interchangeable Execution
Providers (EPs), which implement DSL operations for various privacy back-ends.
An Agentic-AI control plane enforces a finite-state safety loop through signed
telemetry and commands, ensuring consistent risk management and auditability.
The manifest-centric design supports fail-fast job admission and seamless
extensibility to new privacy back-ends. We present qualitative scenarios
illustrating backend-agnostic safety and a formal model foundation for
verification. Finally, we outline a research agenda inviting the community to
advance adaptive guard-rail tuning, multi-backend composition, DSL
specification development, implementation, and compiler extensibility alongside
human-override usability.

</details>


### [201] [Secure Energy Transactions Using Blockchain Leveraging AI for Fraud Detection and Energy Market Stability](https://arxiv.org/abs/2506.19870)
*Md Asif Ul Hoq Khan,MD Zahedul Islam,Istiaq Ahmed,Md Masud Karim Rabbi,Farhana Rahman Anonna,MD Abdul Fahim Zeeshan,Mehedi Hasan Ridoy,Bivash Ranjan Chowdhury,Md Nazmul Shakir Rabbi,GM Alamin Sadnan*

Main category: cs.CR

TL;DR: 研究为美国分散式能源市场开发结合区块链与AI的安全能源交易系统，用超120万条交易记录构建数据集，提出两层架构。


<details>
  <summary>Details</summary>
Motivation: 美国能源市场向去中心化转变带来安全和交易真实性新挑战，需开发安全、智能、高效的能源交易系统。

Method: 将区块链与AI技术以新方式结合，用超120万条模拟P2P能源交易匿名记录构建数据集，提出区块链和AI两层架构系统，选用在分类任务表现好的机器学习模型。

Result: 文中未明确提及具体研究结果。

Conclusion: 文中未明确提及具体研究结论。

Abstract: Peer-to-peer trading and the move to decentralized grids have reshaped the
energy markets in the United States. Notwithstanding, such developments lead to
new challenges, mainly regarding the safety and authenticity of energy trade.
This study aimed to develop and build a secure, intelligent, and efficient
energy transaction system for the decentralized US energy market. This research
interlinks the technological prowess of blockchain and artificial intelligence
(AI) in a novel way to solve long-standing challenges in the distributed energy
market, specifically those of security, fraudulent behavior detection, and
market reliability. The dataset for this research is comprised of more than 1.2
million anonymized energy transaction records from a simulated peer-to-peer
(P2P) energy exchange network emulating real-life blockchain-based American
microgrids, including those tested by LO3 Energy and Grid+ Labs. Each record
contains detailed fields of transaction identifier, timestamp, energy volume
(kWh), transaction type (buy/sell), unit price, prosumer/consumer identifier
(hashed for privacy), smart meter readings, geolocation regions, and settlement
confirmation status. The dataset also includes system-calculated behavior
metrics of transaction rate, variability of energy production, and historical
pricing patterns. The system architecture proposed involves the integration of
two layers, namely a blockchain layer and artificial intelligence (AI) layer,
each playing a unique but complementary function in energy transaction securing
and market intelligence improvement. The machine learning models used in this
research were specifically chosen for their established high performance in
classification tasks, specifically in the identification of energy transaction
fraud in decentralized markets.

</details>


### [202] [An Attack Method for Medical Insurance Claim Fraud Detection based on Generative Adversarial Network](https://arxiv.org/abs/2506.19871)
*Yining Pang,Chenghan Li*

Main category: cs.CR

TL;DR: 本文提出基于GAN的方法对保险欺诈检测系统进行对抗攻击，结果显示攻击者能以99%的成功率生成被误判为合法的欺诈案例，强调需增强检测模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前保险欺诈检测系统缺乏标准化防御机制，易受新兴对抗威胁，需评估其脆弱性。

Method: 提出基于GAN的方法对欺诈检测系统进行对抗攻击。

Result: 攻击者在不知训练数据和模型细节情况下，能以99%的攻击成功率生成被分类为合法的欺诈案例，可通过修改真实记录增加欺诈风险。

Conclusion: 亟需增强保险欺诈检测模型对抗对抗性操纵的鲁棒性，以确保保险系统的稳定性和可靠性。

Abstract: Insurance fraud detection represents a pivotal advancement in modern
insurance service, providing intelligent and digitalized monitoring to enhance
management and prevent fraud. It is crucial for ensuring the security and
efficiency of insurance systems. Although AI and machine learning algorithms
have demonstrated strong performance in detecting fraudulent claims, the
absence of standardized defense mechanisms renders current systems vulnerable
to emerging adversarial threats. In this paper, we propose a GAN-based approach
to conduct adversarial attacks on fraud detection systems. Our results indicate
that an attacker, without knowledge of the training data or internal model
details, can generate fraudulent cases that are classified as legitimate with a
99\% attack success rate (ASR). By subtly modifying real insurance records and
claims, adversaries can significantly increase the fraud risk, potentially
bypassing compromised detection systems. These findings underscore the urgent
need to enhance the robustness of insurance fraud detection models against
adversarial manipulation, thereby ensuring the stability and reliability of
different insurance systems.

</details>


### [203] [Towards Provable (In)Secure Model Weight Release Schemes](https://arxiv.org/abs/2506.19874)
*Xing Yang,Bingtao Wang,Yuhao Wang,Zimo Ji,Terry Jingchen Zhang,Wenyuan Jiang*

Main category: cs.CR

TL;DR: 现有安全权重发布方案缺乏严格安全基础，本文引入安全定义分析TaylorMLP，发现其漏洞并期望推动相关严谨研究。


<details>
  <summary>Details</summary>
Motivation: 现有安全权重发布方案缺乏严格安全基础，仅提供非正式安全保证。

Method: 引入具体安全定义，对TaylorMLP进行案例研究。

Result: 分析发现TaylorMLP存在允许参数提取的漏洞，未达成其非正式安全目标。

Conclusion: 倡导机器学习与安全领域的严谨研究，为未来权重发布方案设计和评估提供蓝图。

Abstract: Recent secure weight release schemes claim to enable open-source model
distribution while protecting model ownership and preventing misuse. However,
these approaches lack rigorous security foundations and provide only informal
security guarantees. Inspired by established works in cryptography, we
formalize the security of weight release schemes by introducing several
concrete security definitions. We then demonstrate our definition's utility
through a case study of TaylorMLP, a prominent secure weight release scheme.
Our analysis reveals vulnerabilities that allow parameter extraction thus
showing that TaylorMLP fails to achieve its informal security goals. We hope
this work will advocate for rigorous research at the intersection of machine
learning and security communities and provide a blueprint for how future weight
release schemes should be designed and evaluated.

</details>


### [204] [Robust Anomaly Detection in Network Traffic: Evaluating Machine Learning Models on CICIDS2017](https://arxiv.org/abs/2506.19877)
*Zhaoyang Xu,Yunbo Liu*

Main category: cs.CR

TL;DR: 本文对比四种模型在CICIDS2017数据集上的入侵检测效果，为动态网络环境选择IDS模型提供指导。


<details>
  <summary>Details</summary>
Motivation: 确定适合入侵检测的机器学习范式，构建有效且通用的安全解决方案。

Method: 在CICIDS2017数据集上，对MLP、1D CNN、OCSVM和LOF四种模型在检测已知攻击类型和未知威胁两种场景下进行对比。

Result: 有监督的MLP和CNN对已知攻击准确率高，但对新攻击召回率下降；无监督的LOF对未知威胁召回率高但误报率高；基于边界的OCSVM在精度和召回率上取得平衡，在两种场景下检测都很稳健。

Conclusion: 研究结果为动态网络环境中选择IDS模型提供了实用指导。

Abstract: Identifying suitable machine learning paradigms for intrusion detection
remains critical for building effective and generalizable security solutions.
In this study, we present a controlled comparison of four representative models
- Multi-Layer Perceptron (MLP), 1D Convolutional Neural Network (CNN),
One-Class Support Vector Machine (OCSVM) and Local Outlier Factor (LOF) - on
the CICIDS2017 dataset under two scenarios: detecting known attack types and
generalizing to previously unseen threats. Our results show that supervised MLP
and CNN achieve near-perfect accuracy on familiar attacks but suffer drastic
recall drops on novel attacks. Unsupervised LOF attains moderate overall
accuracy and high recall on unknown threats at the cost of elevated false
alarms, while boundary-based OCSVM balances precision and recall best,
demonstrating robust detection across both scenarios. These findings offer
practical guidance for selecting IDS models in dynamic network environments.

</details>


### [205] [Retrieval-Confused Generation is a Good Defender for Privacy Violation Attack of Large Language Models](https://arxiv.org/abs/2506.19889)
*Wanli Peng,Xin Chen,Hang Fu,XinYu He,Xue Yiming,Juan Wen*

Main category: cs.CR

TL;DR: 本文提出基于大语言模型检索混淆生成的新防御范式应对隐私侵犯攻击，经实验验证其可行性和优越性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型隐私侵犯攻击防御方法存在推理时间长、性能不佳、易暴露等问题，需新防御方法。

Method: 设计改写提示构建干扰数据库，采用最不相关检索策略获取用户数据，替换“数据评论”形成防御查询。

Result: 在两个数据集和八个流行大语言模型上进行了大量实验。

Conclusion: 所提防御方法能高效、隐蔽地防御隐私侵犯攻击，具有可行性和优越性。

Abstract: Recent advances in large language models (LLMs) have made a profound impact
on our society and also raised new security concerns. Particularly, due to the
remarkable inference ability of LLMs, the privacy violation attack (PVA),
revealed by Staab et al., introduces serious personal privacy issues. Existing
defense methods mainly leverage LLMs to anonymize the input query, which
requires costly inference time and cannot gain satisfactory defense
performance. Moreover, directly rejecting the PVA query seems like an effective
defense method, while the defense method is exposed, promoting the evolution of
PVA. In this paper, we propose a novel defense paradigm based on
retrieval-confused generation (RCG) of LLMs, which can efficiently and covertly
defend the PVA. We first design a paraphrasing prompt to induce the LLM to
rewrite the "user comments" of the attack query to construct a disturbed
database. Then, we propose the most irrelevant retrieval strategy to retrieve
the desired user data from the disturbed database. Finally, the "data comments"
are replaced with the retrieved user data to form a defended query, leading to
responding to the adversary with some wrong personal attributes, i.e., the
attack fails. Extensive experiments are conducted on two datasets and eight
popular LLMs to comprehensively evaluate the feasibility and the superiority of
the proposed defense method.

</details>


### [206] [Blameless Users in a Clean Room: Defining Copyright Protection for Generative Models](https://arxiv.org/abs/2506.19881)
*Aloni Cohen*

Main category: cs.CR

TL;DR: 本文重新探讨生成模型输出版权保护问题，指出原方法不足，引入无责版权保护框架及洁净室版权保护，并证明特定条件下差分隐私与版权保护的关系。


<details>
  <summary>Details</summary>
Motivation: 重新审视生成模型输出不侵犯训练数据版权的条件，改进之前提出的近无访问性（NAF）方法。

Method: 先证明NAF不能防止侵权，引入无责版权保护框架并以洁净室版权保护实例化，最后证明在数据集满足版权去重要求时，差分隐私意味着洁净室版权保护。

Result: 表明NAF模型存在侵权问题，提出洁净室版权保护，证明了差分隐私与版权保护的联系。

Conclusion: 为可证明的版权保护建立了更坚实的技术和法律基础。

Abstract: Are there any conditions under which a generative model's outputs are
guaranteed not to infringe the copyrights of its training data? This is the
question of "provable copyright protection" first posed by Vyas, Kakade, and
Barak (ICML 2023). They define near access-freeness (NAF) and propose it as
sufficient for protection. This paper revisits the question and establishes new
foundations for provable copyright protection -- foundations that are firmer
both technically and legally. First, we show that NAF alone does not prevent
infringement. In fact, NAF models can enable verbatim copying, a blatant
failure of copy protection that we dub being tainted. Then, we introduce our
blameless copy protection framework for defining meaningful guarantees, and
instantiate it with clean-room copy protection. Clean-room copy protection
allows a user to control their risk of copying by behaving in a way that is
unlikely to copy in a counterfactual clean-room setting. Finally, we formalize
a common intuition about differential privacy and copyright by proving that DP
implies clean-room copy protection when the dataset is golden, a copyright
deduplication requirement.

</details>


### [207] [Diffusion-based Task-oriented Semantic Communications with Model Inversion Attack](https://arxiv.org/abs/2506.19886)
*Xuesong Wang,Mo Li,Xingyan Shi,Zhaoqian Liu,Shenghao Yang*

Main category: cs.CR

TL;DR: 提出基于扩散的语义通信框架DiffSem，提升任务性能和系统鲁棒性，实验显示其在MNIST数据集上提升分类准确率并稳定运行，且传统指标与语义信息泄露有偏差。


<details>
  <summary>Details</summary>
Motivation: 任务导向语义通信面临隐私保护和任务准确性的挑战，传统图像质量指标评估攻击严重程度不足。

Method: 提出DiffSem框架，通过扩散机制和自引用标签嵌入优化语义信息重建，补偿信道噪声，采用语义信息失真，提出新的评估攻击者有效性的指标。

Result: 在MNIST数据集上，DiffSem提高分类准确率10.03%，在动态信道下保持稳定性能，传统图像质量指标与任务相关语义信息泄露存在显著偏差。

Conclusion: DiffSem框架有效提升了任务性能和系统鲁棒性，传统指标不适用于评估任务导向语义通信的攻击。

Abstract: Semantic communication has emerged as a promising neural network-based system
design for 6G networks. Task-oriented semantic communication is a novel
paradigm whose core goal is to efficiently complete specific tasks by
transmitting semantic information, optimizing communication efficiency and task
performance. The key challenge lies in preserving privacy while maintaining
task accuracy, as this scenario is susceptible to model inversion attacks. In
such attacks, adversaries can restore or even reconstruct input data by
analyzing and processing model outputs, owing to the neural network-based
nature of the systems. In addition, traditional systems use image quality
indicators (such as PSNR or SSIM) to assess attack severity, which may be
inadequate for task-oriented semantic communication, since visual differences
do not necessarily ensure semantic divergence. In this paper, we propose a
diffusion-based semantic communication framework, named DiffSem, that optimizes
semantic information reconstruction through a diffusion mechanism with
self-referential label embedding to significantly improve task performance. Our
model also compensates channel noise and adopt semantic information distortion
to ensure the robustness of the system in various signal-to-noise ratio
environments. To evaluate the attacker's effectiveness, we propose a new metric
that better quantifies the semantic fidelity of estimations from the adversary.
Experimental results based on this criterion show that on the MNIST dataset,
DiffSem improves the classification accuracy by 10.03%, and maintain stable
performance under dynamic channels. Our results further demonstrate that
significant deviation exists between traditional image quality indicators and
the leakage of task-relevant semantic information.

</details>


### [208] [SV-LLM: An Agentic Approach for SoC Security Verification using Large Language Models](https://arxiv.org/abs/2506.20415)
*Dipayan Saha,Shams Tarek,Hasan Al Shaikh,Khan Thamid Hasan,Pavan Sai Nalluri,Md. Ajoad Hasan,Nashmin Alam,Jingbo Zhou,Sujan Kumar Saha,Mark Tehranipoor,Farimah Farahmandi*

Main category: cs.CR

TL;DR: 本文引入SV - LLM多智能体辅助系统用于自动化和增强SoC安全验证，展示其在硬件安全实践中的潜力。


<details>
  <summary>Details</summary>
Motivation: 传统验证技术在自动化、可扩展性等方面面临挑战，LLMs能力为解决问题带来新范式。

Method: 引入SV - LLM多智能体系统，集成多种专业智能体处理不同任务，智能体利用不同学习范式优化性能。

Result: 通过案例研究和实验展示了SV - LLM的适用性和有效性。

Conclusion: SV - LLM可减少人工干预、提高准确性、加速安全分析，支持在设计早期主动识别和缓解风险，有望改变硬件安全实践。

Abstract: Ensuring the security of complex system-on-chips (SoCs) designs is a critical
imperative, yet traditional verification techniques struggle to keep pace due
to significant challenges in automation, scalability, comprehensiveness, and
adaptability. The advent of large language models (LLMs), with their remarkable
capabilities in natural language understanding, code generation, and advanced
reasoning, presents a new paradigm for tackling these issues. Moving beyond
monolithic models, an agentic approach allows for the creation of multi-agent
systems where specialized LLMs collaborate to solve complex problems more
effectively. Recognizing this opportunity, we introduce SV-LLM, a novel
multi-agent assistant system designed to automate and enhance SoC security
verification. By integrating specialized agents for tasks like verification
question answering, security asset identification, threat modeling, test plan
and property generation, vulnerability detection, and simulation-based bug
validation, SV-LLM streamlines the workflow. To optimize their performance in
these diverse tasks, agents leverage different learning paradigms, such as
in-context learning, fine-tuning, and retrieval-augmented generation (RAG). The
system aims to reduce manual intervention, improve accuracy, and accelerate
security analysis, supporting proactive identification and mitigation of risks
early in the design cycle. We demonstrate its potential to transform hardware
security practices through illustrative case studies and experiments that
showcase its applicability and efficacy.

</details>


### [209] [Attack Smarter: Attention-Driven Fine-Grained Webpage Fingerprinting Attacks](https://arxiv.org/abs/2506.20082)
*Yali Yuan,Weiyi Zou,Guang Cheng*

Main category: cs.CR

TL;DR: 文章指出传统网站指纹攻击局限于小规模场景，提出注意力驱动的细粒度网页指纹攻击ADWPF，实验表明其效果优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 传统网站指纹攻击局限于小规模场景且难以应对多子页面和多标签浏览情况，为解决这些问题开展研究。

Method: 提出ADWPF攻击，训练时基于注意力图对流量显著区域进行增强，提取特征并应用自注意力模块，用残差注意力处理多标签场景。

Result: 广泛实验表明提出的方法在不同规模数据集上始终优于现有基线。

Conclusion: ADWPF攻击能有效解决传统网站指纹攻击的局限，提升攻击效果。

Abstract: Website Fingerprinting (WF) attacks aim to infer which websites a user is
visiting by analyzing traffic patterns, thereby compromising user anonymity.
Although this technique has been demonstrated to be effective in controlled
experimental environments, it remains largely limited to small-scale scenarios,
typically restricted to recognizing website homepages. In practical settings,
however, users frequently access multiple subpages in rapid succession, often
before previous content fully loads. WebPage Fingerprinting (WPF) generalizes
the WF framework to large-scale environments by modeling subpages of the same
site as distinct classes. These pages often share similar page elements,
resulting in lower inter-class variance in traffic features. Furthermore, we
consider multi-tab browsing scenarios, in which a single trace encompasses
multiple categories of webpages. This leads to overlapping traffic segments,
and similar features may appear in different positions within the traffic,
thereby increasing the difficulty of classification. To address these
challenges, we propose an attention-driven fine-grained WPF attack, named
ADWPF. Specifically, during the training phase, we apply targeted augmentation
to salient regions of the traffic based on attention maps, including attention
cropping and attention masking. ADWPF then extracts low-dimensional features
from both the original and augmented traffic and applies self-attention modules
to capture the global contextual patterns of the trace. Finally, to handle the
multi-tab scenario, we employ the residual attention to generate class-specific
representations of webpages occurring at different temporal positions.
Extensive experiments demonstrate that the proposed method consistently
surpasses state-of-the-art baselines across datasets of different scales.

</details>


### [210] [Autonomous Cyber Resilience via a Co-Evolutionary Arms Race within a Fortified Digital Twin Sandbox](https://arxiv.org/abs/2506.20102)
*Malikussaid,Sutiyo*

Main category: cs.CR

TL;DR: 论文引入ARC框架，通过自主闭环强化过程实现分析弹性，在测试床实验验证性能优越，是关键基础设施安全的范式转变。


<details>
  <summary>Details</summary>
Motivation: IT与OT融合使关键基础设施面临新威胁，现有安全范式无法解决“信任三位一体”问题。

Method: 建立ARC框架，在F - SCDT高保真沙箱中进行共进化军备竞赛，利用DRL“红代理”发现攻击路径，基于集成的“蓝代理”通过对抗训练强化防御。

Result: 在TEP和SWaT测试床实验验证框架性能优越，共进化过程显著提升新攻击检测性能。

Conclusion: ARC是关键基础设施未来动态、自我改进安全的必要范式转变。

Abstract: The convergence of IT and OT has created hyper-connected ICS, exposing
critical infrastructure to a new class of adaptive, intelligent adversaries
that render static defenses obsolete. Existing security paradigms often fail to
address a foundational "Trinity of Trust," comprising the fidelity of the
system model, the integrity of synchronizing data, and the resilience of the
analytical engine against sophisticated evasion. This paper introduces the ARC
framework, a method for achieving analytical resilience through an autonomous,
closed-loop hardening process. ARC establishes a perpetual co-evolutionary arms
race within the high-fidelity sandbox of a F-SCDT. A DRL agent, the "Red
Agent," is formalized and incentivized to autonomously discover stealthy,
physically-plausible attack paths that maximize process disruption while
evading detection. Concurrently, an ensemble-based "Blue Agent" defender is
continuously hardened via adversarial training against the evolving threats
discovered by its adversary. This co-evolutionary dynamic forces both agents to
become progressively more sophisticated, enabling the system to autonomously
probe and patch its own vulnerabilities. Experimental validation on both the
TEP and the SWaT testbeds demonstrates the framework's superior performance. A
comprehensive ablation study, supported by extensive visualizations including
ROC curves and SHAP plots, reveals that the co-evolutionary process itself is
responsible for a significant performance increase in detecting novel attacks.
By integrating XAI to ensure operator trust and proposing a scalable F-ARC
architecture, this work presents ARC not merely as an improvement, but as a
necessary paradigm shift toward dynamic, self-improving security for the future
of critical infrastructure.

</details>


### [211] [Vulnerability Disclosure through Adaptive Black-Box Adversarial Attacks on NIDS](https://arxiv.org/abs/2506.20576)
*Sabrine Ennaji,Elhadj Benkhelifa,Luigi V. Mancini*

Main category: cs.CR

TL;DR: 本文提出一种新的黑盒对抗攻击方法，用自适应特征选择策略，实验证明其在网络流量中能有效规避检测，为开发防御奠定基础。


<details>
  <summary>Details</summary>
Motivation: 当前对抗攻击理论与实践有差距，在网络流量等结构化数据中有效对抗操作困难，现有方法模糊且不可复现，导致现有防御难以应对攻击。

Method: 提出严格遵循黑盒约束的新方法，采用基于变点检测和因果分析的自适应特征选择策略，识别并针对敏感特征。

Result: 综合实验表明该攻击能以最少交互有效规避检测，提高在现实场景的适应性和适用性。

Conclusion: 此工作增进对网络流量中对抗攻击的理解，为开发强大防御奠定基础。

Abstract: Adversarial attacks, wherein slight inputs are carefully crafted to mislead
intelligent models, have attracted increasing attention. However, a critical
gap persists between theoretical advancements and practical application,
particularly in structured data like network traffic, where interdependent
features complicate effective adversarial manipulations. Moreover, ambiguity in
current approaches restricts reproducibility and limits progress in this field.
Hence, existing defenses often fail to handle evolving adversarial attacks.
This paper proposes a novel approach for black-box adversarial attacks, that
addresses these limitations. Unlike prior work, which often assumes system
access or relies on repeated probing, our method strictly respect black-box
constraints, reducing interaction to avoid detection and better reflect
real-world scenarios. We present an adaptive feature selection strategy using
change-point detection and causality analysis to identify and target sensitive
features to perturbations. This lightweight design ensures low computational
cost and high deployability. Our comprehensive experiments show the attack's
effectiveness in evading detection with minimal interaction, enhancing its
adaptability and applicability in real-world scenarios. By advancing the
understanding of adversarial attacks in network traffic, this work lays a
foundation for developing robust defenses.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [212] [MNN-AECS: Energy Optimization for LLM Decoding on Mobile Devices via Adaptive Core Selection](https://arxiv.org/abs/2506.19884)
*Zhengxiang Huang,Chaoyue Niu,Zhaode Wang,Jiarui Xue,Hanming Zhang,Yugang Wang,Zewei Xin,Xiaotang Jiang,Chengfei Lv,Fan Wu,Guihai Chen*

Main category: cs.OS

TL;DR: 提出MNN - AECS解决设备端大语言模型推理能耗问题，评估显示节能效果显著且有速度提升。


<details>
  <summary>Details</summary>
Motivation: 设备端大语言模型推理需求增长，能耗成主要问题，现有工作多关注预填充阶段，忽视解码阶段能耗。

Method: 引入Adaptive Energy - Centric Core Selection (AECS) 并集成到MNN中形成MNN - AECS，通过动态选择低功耗CPU核心降低解码能耗。

Result: 在多设备和数据集上，MNN - AECS比原始MNN平均节能23%且无减速，对比其他引擎节能39% - 78%，提速12% - 363%。

Conclusion: MNN - AECS是首个无需root权限或修改操作系统的引擎级节能大语言模型解码解决方案，节能且有速度优势。

Abstract: As the demand for on-device Large Language Model (LLM) inference grows,
energy efficiency has become a major concern, especially for battery-limited
mobile devices. Our analysis shows that the memory-bound LLM decode phase
dominates energy use, and yet most existing works focus on accelerating the
prefill phase, neglecting energy concerns. We introduce Adaptive Energy-Centric
Core Selection (AECS) and integrate it into MNN to create the energy-efficient
version, MNN-AECS, the first engine-level system solution without requiring
root access or OS modifications for energy-efficient LLM decoding. MNN-AECS is
designed to reduce LLM decoding energy while keeping decode speed within an
acceptable slowdown threshold by dynamically selecting low-power CPU cores.
MNN-AECS is evaluated across 5 Android and 2 iOS devices on 5 popular LLMs of
various sizes. Compared to original MNN, MNN-AECS cuts down energy use by 23%
without slowdown averaged over all 7 devices and 4 datasets. Against other
engines, including llama.cpp, executorch, mllm, and MediaPipe, MNN-AECS
delivers 39% to 78% energy saving and 12% to 363% speedup on average.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [213] [An ab initio foundation model of wavefunctions that accurately describes chemical bond breaking](https://arxiv.org/abs/2506.19960)
*Adam Foster,Zeno Schätzle,P. Bernát Szabó,Lixue Cheng,Jonas Köhler,Gino Cassella,Nicholas Gao,Jiawei Li,Frank Noé,Jan Hermann*

Main category: physics.chem-ph

TL;DR: 提出Orbformer可转移波函数模型，在量子化学中实现分摊求解薛定谔方程成本，达化学精度。


<details>
  <summary>Details</summary>
Motivation: 传统量子化学处理断键时因多参考特性面临挑战，多参考方法计算成本高，此前深度QMC转移模型尝试范围有限。

Method: 提出Orbformer可转移波函数模型，在22000个平衡和离解结构上预训练，可在未见分子上微调。

Result: 在既定基准、断键和狄尔斯 - 阿尔德反应中，Orbformer是唯一能始终收敛到化学精度（1 kcal/mol）的方法。

Conclusion: 将分摊多分子薛定谔方程求解成本的想法转化为量子化学实用方法。

Abstract: Reliable description of bond breaking remains a major challenge for quantum
chemistry due to the multireferential character of the electronic structure in
dissociating species. Multireferential methods in particular suffer from large
computational cost, which under the normal paradigm has to be paid anew for
each system at a full price, ignoring commonalities in electronic structure
across molecules. Quantum Monte Carlo with deep neural networks (deep QMC)
uniquely offers to exploit such commonalities by pretraining transferable
wavefunction models, but all such attempts were so far limited in scope. Here,
we bring this new paradigm to fruition with Orbformer, a novel transferable
wavefunction model pretrained on 22,000 equilibrium and dissociating structures
that can be fine-tuned on unseen molecules reaching an accuracy-cost ratio
rivalling classical multireferential methods. On established benchmarks as well
as more challenging bond dissociations and Diels-Alder reactions, Orbformer is
the only method that consistently converges to chemical accuracy (1 kcal/mol).
This work turns the idea of amortizing the cost of solving the Schr\"odinger
equation over many molecules into a practical approach in quantum chemistry.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [214] [Exploring the Capabilities of the Frontier Large Language Models for Nuclear Energy Research](https://arxiv.org/abs/2506.19863)
*Ahmed Almeldein,Mohammed Alnaggar,Rick Archibald,Tom Beck,Arpan Biswas,Rike Bostelmann,Wes Brewer,Chris Bryan,Christopher Calle,Cihangir Celik,Rajni Chahal,Jong Youl Choi,Arindam Chowdhury,Mark Cianciosa,Franklin Curtis,Gregory Davidson,Sebastian De Pascuale,Lisa Fassino,Ana Gainaru,Yashika Ghai,Luke Gibson,Qian Gong,Christopher Greulich,Scott Greenwood,Cory Hauck,Ehab Hassan,Rinkle Juneja,Soyoung Kang,Scott Klasky,Atul Kumar,Vineet Kumar,Paul Laiu,Calvin Lear,Yan-Ru Lin,Jono McConnell,Furkan Oz,Anant Raj,Pradeep Ramuhalli,Marie Romedenne,Samantha Sabatino,José Salcedo-Pérez,Nathan D. See,Arpan Sircar,Punam Thankur,Tim Younkin,Xiao-Ying Yu,Prashant Jain,Tom Evans,Prasanna Balaprakash*

Main category: physics.comp-ph

TL;DR: 橡树岭国家实验室的人工智能核能研讨会评估大语言模型加速核研究潜力，团队用多种模型探索核科学挑战，发现其优缺点，为集成AI工具提供路线图。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型加速核聚变和裂变研究的潜力。

Method: 14个跨学科团队使用多种AI模型，采用结合提示工程、深度研究能力和迭代改进的结构化工作流程。

Result: 大语言模型在早期探索、文献综合和工作流程设计方面表现出色，但在新材料设计、高级代码生成和特定领域细节验证方面有局限。

Conclusion: 验证了AI加速核能研究的潜力，强调需要特定数据集、工作流自动化和专业模型开发，为集成AI工具提供路线图。

Abstract: The AI for Nuclear Energy workshop at Oak Ridge National Laboratory evaluated
the potential of Large Language Models (LLMs) to accelerate fusion and fission
research. Fourteen interdisciplinary teams explored diverse nuclear science
challenges using ChatGPT, Gemini, Claude, and other AI models over a single
day. Applications ranged from developing foundation models for fusion reactor
control to automating Monte Carlo simulations, predicting material degradation,
and designing experimental programs for advanced reactors. Teams employed
structured workflows combining prompt engineering, deep research capabilities,
and iterative refinement to generate hypotheses, prototype code, and research
strategies. Key findings demonstrate that LLMs excel at early-stage
exploration, literature synthesis, and workflow design, successfully
identifying research gaps and generating plausible experimental frameworks.
However, significant limitations emerged, including difficulties with novel
materials designs, advanced code generation for modeling and simulation, and
domain-specific details requiring expert validation. The successful outcomes
resulted from expert-driven prompt engineering and treating AI as a
complementary tool rather than a replacement for physics-based methods. The
workshop validated AI's potential to accelerate nuclear energy research through
rapid iteration and cross-disciplinary synthesis while highlighting the need
for curated nuclear-specific datasets, workflow automation, and specialized
model development. These results provide a roadmap for integrating AI tools
into nuclear science workflows, potentially reducing development cycles for
safer, more efficient nuclear energy systems while maintaining rigorous
scientific standards.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [215] [OLALa: Online Learned Adaptive Lattice Codes for Heterogeneous Federated Learning](https://arxiv.org/abs/2506.20297)
*Natalie Lang,Maya Simhi,Nir Shlezinger*

Main category: eess.SP

TL;DR: 提出OLALa异构联邦学习框架，可在线调整量化器，实验表明性能优于传统方案。


<details>
  <summary>Details</summary>
Motivation: 现有基于格的联邦学习方案采用固定量化规则，在异构和动态环境中效果不佳。

Method: 推导非固定格量化器联邦学习的收敛保证，设计在线学习算法让客户端调整量化器。

Result: OLALa在不同量化率下持续提升学习性能，优于传统固定码本和非自适应方案。

Conclusion: OLALa框架有效，能在异构和动态环境中实现更好的联邦学习。

Abstract: Federated learning (FL) enables collaborative training across distributed
clients without sharing raw data, often at the cost of substantial
communication overhead induced by transmitting high-dimensional model updates.
This overhead can be alleviated by having the clients quantize their model
updates, with dithered lattice quantizers identified as an attractive scheme
due to its structural simplicity and convergence-preserving properties.
However, existing lattice-based FL schemes typically rely on a fixed
quantization rule, which is suboptimal in heterogeneous and dynamic
environments where the model updates distribution varies across users and
training rounds. In this work, we propose Online Learned Adaptive Lattices
(OLALa), a heterogeneous FL framework where each client can adjust its
quantizer online using lightweight local computations. We first derive
convergence guarantees for FL with non-fixed lattice quantizers and show that
proper lattice adaptation can tighten the convergence bound. Then, we design an
online learning algorithm that enables clients to tune their quantizers
throughout the FL process while exchanging only a compact set of quantization
parameters. Numerical experiments demonstrate that OLALa consistently improves
learning performance under various quantization rates, outperforming
conventional fixed-codebook and non-adaptive schemes.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [216] [Deciphering GunType Hierarchy through Acoustic Analysis of Gunshot Recordings](https://arxiv.org/abs/2506.20609)
*Ankit Shah,Rita Singh,Bhiksha Raj,Alexander Hauptmann*

Main category: cs.SD

TL;DR: 研究利用枪声录音声学分析开发低成本枪击检测与枪支类型分类系统，对比不同机器学习框架，虽有挑战但有长期愿景。


<details>
  <summary>Details</summary>
Motivation: 现有商业枪击检测系统成本高，需低成本方案辅助执法机构应对枪支暴力。

Method: 使用3459条枪声录音数据集，分析枪声声学特征，提出并评估SVM和CNN机器学习框架用于枪击检测与枪支类型分类。

Result: 深度学习方法在干净标注数据上mAP达0.58，优于SVM基线；使用含噪声网络数据时mAP为0.35。

Conclusion: 有望开发高精度、实时且可部署在普通设备上的系统，降低检测成本，为应急人员提供关键信息。

Abstract: The escalating rates of gun-related violence and mass shootings represent a
significant threat to public safety. Timely and accurate information for law
enforcement agencies is crucial in mitigating these incidents. Current
commercial gunshot detection systems, while effective, often come with
prohibitive costs. This research explores a cost-effective alternative by
leveraging acoustic analysis of gunshot recordings, potentially obtainable from
ubiquitous devices like cell phones, to not only detect gunshots but also
classify the type of firearm used. This paper details a study on deciphering
gun type hierarchies using a curated dataset of 3459 recordings. We investigate
the fundamental acoustic characteristics of gunshots, including muzzle blasts
and shockwaves, which vary based on firearm type, ammunition, and shooting
direction. We propose and evaluate machine learning frameworks, including
Support Vector Machines (SVMs) as a baseline and a more advanced Convolutional
Neural Network (CNN) architecture for joint gunshot detection and gun type
classification. Results indicate that our deep learning approach achieves a
mean average precision (mAP) of 0.58 on clean labeled data, outperforming the
SVM baseline (mAP 0.39). Challenges related to data quality, environmental
noise, and the generalization capabilities when using noisy web-sourced data
(mAP 0.35) are also discussed. The long-term vision is to develop a highly
accurate, real-time system deployable on common recording devices,
significantly reducing detection costs and providing critical intelligence to
first responders.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [217] [Analyzing the Impact of Strategic Bidding on the Reserve Capacity via a Bi-Level Model](https://arxiv.org/abs/2506.20493)
*Yun Xu,Yunxiao Bai,Yunyong Zhang,Peng Wang,Xuelin Wang,Jiqun Guo,Kaijun Xie,Rusheng Zhao*

Main category: eess.SY

TL;DR: 研究电力市场中企业策略性投标行为，建双层模型分析，发现会使更多机组运行增加系统备用但提高电价，强调市场设计重要性。


<details>
  <summary>Details</summary>
Motivation: 可再生能源整合需足够备用容量，但电力公司策略性投标可能影响系统备用，需研究其影响。

Method: 将市场建模为双层问题，上层是策略性公司最大化利润，下层是系统运营商清结算市场，用连续备用容量计算近似机组组合。

Result: 在不完全竞争市场中，更多机组运行提高系统备用，但部分机组为盈利上网，提高了消费者电价。

Conclusion: 强调市场设计在应对策略性投标行为时平衡备用充足性和经济效率的重要性。

Abstract: The growing integration of renewable energy sources necessitates adequate
reserve capacity to maintain power balance. However, in market clearing, power
companies with flexible resources may submit strategic bids to maximize
profits, potentially compromising system reserves. This paper examines the
effects of such strategic behavior by modeling the market as a bi-level
problem. The upper level represents a strategic company aiming to maximize
profit, while the lower level simulates the system operator clearing the market
based on submitted offers. To enable duality-based solution methods, we
approximate unit commitments with a continuous reserve capacity calculation.
Case studies indicate that, in an imperfectly competitive market, more units
are incentivized to operate,enhancing system reserves. However, some units go
online mainly for profit, ultimately raising electricity costs for consumers.
These findings highlight the importance of market design in managing the
trade-off between reserve adequacy and economic efficiency in the presence of
strategic bidding behavior.

</details>


### [218] [Recurrent neural network-based robust control systems with closed-loop regional incremental ISS and application to MPC design](https://arxiv.org/abs/2506.20334)
*Daniele Ravasio,Marcello Farina,Alessio La Bella,Andrea Ballarino*

Main category: eess.SY

TL;DR: 本文研究循环神经网络系统输出反馈方案设计，提出基于线性矩阵不等式的方法，并引入管基非线性模型预测控制器，通过数值模拟验证方案有效性。


<details>
  <summary>Details</summary>
Motivation: 设计适用于一类循环神经网络系统的输出反馈方案，增强系统对干扰和状态估计不确定性的鲁棒性。

Method: 提出基于线性矩阵不等式设计观测器和静态状态反馈控制器的方法；引入管基非线性模型预测控制器替代静态定律。

Result: 所提条件能形成具有收敛性和递归可行性保证的鲁棒NMPC定律，扩大吸引域；通过pH中和过程基准的数值模拟验证了方案有效性。

Conclusion: 所提出的设计方案在循环神经网络系统输出反馈控制中是有效的。

Abstract: This paper investigates the design of output-feedback schemes for systems
described by a class of recurrent neural networks. We propose a procedure based
on linear matrix inequalities for designing an observer and a static
state-feedback controller. The algorithm leverages global and regional
incremental input-to-state stability (incremental ISS) and enables the tracking
of constant setpoints, ensuring robustness to disturbances and state estimation
uncertainty. To address the potential limitations of regional incremental ISS,
we introduce an alternative scheme in which the static law is replaced with a
tube-based nonlinear model predictive controller (NMPC) that exploits regional
incremental ISS properties. We show that these conditions enable the
formulation of a robust NMPC law with guarantees of convergence and recursive
feasibility, leading to an enlarged region of attraction. Theoretical results
are validated through numerical simulations on the pH-neutralisation process
benchmark, demonstrating the effectiveness of the proposed schemes.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [219] [X-SiT: Inherently Interpretable Surface Vision Transformers for Dementia Diagnosis](https://arxiv.org/abs/2506.20267)
*Fabian Bongratz,Tom Nuno Wolf,Jaume Gual Ramon,Christian Wachinger*

Main category: cs.GR

TL;DR: 提出可解释的Surface Vision Transformer (X-SiT)用于脑部医学图像分析，在检测疾病上性能优异且能提供信息丰富的原型。


<details>
  <summary>Details</summary>
Motivation: 可解释模型对临床决策重要，但3D体积数据的大脑皮层结构难可视化和解释，皮层表面渲染有优势且表面数据广泛用于研究神经疾病。

Method: 提出X - SiT，引入原型表面补丁解码器对表面补丁嵌入进行分类，结合基于案例的推理和空间对应的皮质原型。

Result: 在检测阿尔茨海默病和额颞叶痴呆方面达到了最先进的性能，还能提供与已知疾病模式相符的信息丰富的原型并揭示分类错误。

Conclusion: X - SiT是首个基于可解释皮质特征提供人类可理解预测的固有可解释神经网络，在脑部疾病检测中有良好表现。

Abstract: Interpretable models are crucial for supporting clinical decision-making,
driving advances in their development and application for medical images.
However, the nature of 3D volumetric data makes it inherently challenging to
visualize and interpret intricate and complex structures like the cerebral
cortex. Cortical surface renderings, on the other hand, provide a more
accessible and understandable 3D representation of brain anatomy, facilitating
visualization and interactive exploration. Motivated by this advantage and the
widespread use of surface data for studying neurological disorders, we present
the eXplainable Surface Vision Transformer (X-SiT). This is the first
inherently interpretable neural network that offers human-understandable
predictions based on interpretable cortical features. As part of X-SiT, we
introduce a prototypical surface patch decoder for classifying surface patch
embeddings, incorporating case-based reasoning with spatially corresponding
cortical prototypes. The results demonstrate state-of-the-art performance in
detecting Alzheimer's disease and frontotemporal dementia while additionally
providing informative prototypes that align with known disease patterns and
reveal classification errors.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [220] [Reinforcement Learning Increases Wind Farm Power Production by Enabling Closed-Loop Collaborative Control](https://arxiv.org/abs/2506.20554)
*Andrew Mole,Max Weissenbacher,Georgios Rigas,Sylvain Laizet*

Main category: physics.flu-dyn

TL;DR: 本文提出将强化学习控制器与高保真大涡模拟结合，实现风电场动态控制，使功率输出提升，是风电场优化变革性方法。


<details>
  <summary>Details</summary>
Motivation: 传统风电场独立控制各涡轮机，虽协调尾流转向可提升能量产量，但风电场优化主要依赖静态、低保真模拟器，忽略关键湍流动力学，需更好的控制方法。

Method: 将强化学习（RL）控制器与高保真大涡模拟（LES）直接集成，通过协作、动态控制策略实时响应大气湍流。

Result: RL控制器使风电场功率输出比基线操作提高4.30%，几乎是贝叶斯优化静态最优偏航控制增益2.19%的两倍。

Conclusion: 动态流量响应控制是风电场优化的变革性方法，对加速可再生能源实现净零目标有直接影响。

Abstract: Traditional wind farm control operates each turbine independently to maximize
individual power output. However, coordinated wake steering across the entire
farm can substantially increase the combined wind farm energy production.
Although dynamic closed-loop control has proven effective in flow control
applications, wind farm optimization has relied primarily on static,
low-fidelity simulators that ignore critical turbulent flow dynamics. In this
work, we present the first reinforcement learning (RL) controller integrated
directly with high-fidelity large-eddy simulation (LES), enabling real-time
response to atmospheric turbulence through collaborative, dynamic control
strategies. Our RL controller achieves a 4.30% increase in wind farm power
output compared to baseline operation, nearly doubling the 2.19% gain from
static optimal yaw control obtained through Bayesian optimization. These
results establish dynamic flow-responsive control as a transformative approach
to wind farm optimization, with direct implications for accelerating renewable
energy deployment to net-zero targets.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [221] [An experiment in price perception error](https://arxiv.org/abs/2506.19953)
*Shawn Berry*

Main category: econ.GN

TL;DR: 研究351名受访者对13种产品和服务价格的猜测，分析个体态度和人口因素对价格感知的影响。


<details>
  <summary>Details</summary>
Motivation: 消费者决策过程多维，价格感知重要但理解不足，消费者倾向用价格知识启发法做决策，研究个体因素对价格感知的影响。

Method: 用ANOVA分析个体态度和人口因素对正确猜测价格能力的影响，用潜变量分析开发七因素价格感知模型。

Result: 多数受访者先研究价格或与类似产品比较，常高估或低估价格；ANOVA显示多个因素是价格感知误差的显著预测因子；潜变量分析表明人口、决策和价格敏感因素影响最大。

Conclusion: 多个个体态度和人口因素会显著影响消费者的价格感知误差。

Abstract: The process of consumer decision-making is multidimensional, and price
perception is a very important but still not well-understood dimension for both
marketers and consumers. Although heuristics or mental shortcuts are seen as
biased and can cause decision errors, consumers tend to use price knowledge
heuristics for purchase decisions, sometimes relying on old information. This
study examined the effects of individual attitudinal and demographic factors on
the ability of 351 respondents to correctly guess the prices of 13 products and
services using ANOVA and the development of a seven-factor price perception
model using latent variable analysis. While most respondents preferred to
either research prices first or compare prices to a similar product, they
either systematically underestimated or overestimated the prices of the
products. In the ANOVA, brand loyalty, importance of substitute products and
knockoff products, how financially well-off the respondent household was
growing up, product quality importance, haggling tendency, and level of income
were statistically significant predictors of price perception error. Latent
variable analysis revealed that demographics, decision making, and price
sensitivity factors had the greatest influence on price perception error. Level
of education and income, frequency of regret, coupon importance, and how
respondents chose to save or spend money were significant latent variables.

</details>


### [222] [Exiting National Anti-Poverty Campaign, Social Support, and Improved Mental Health](https://arxiv.org/abs/2506.20292)
*Zhengwen Liu,Castiel Chen Zhuang,Yibo Wu*

Main category: econ.GN

TL;DR: 研究中国国家扶贫运动退出的心理和社会影响，发现退出改善心理健康并加强社会和家庭联系，为政策退出设计提供见解。


<details>
  <summary>Details</summary>
Motivation: 研究国家扶贫运动退出的心理和社会影响。

Method: 利用中国国家扶贫援助逐步退出的自然实验，采用回归断点设计。

Result: 退出国家扶贫运动改善心理健康，同时加强社会和家庭联系，收入和物质条件基本不变。

Conclusion: 研究结果为政策退出设计提供见解，强调实施或结束援助计划时维持社区和家庭支持系统措施的重要性。

Abstract: We study the psychological and social impacts of exiting a national
anti-poverty campaign, leveraging China's phase-out of its national poverty
assistance as a natural experiment. Using a regression discontinuity design, we
find that exiting the national campaign improves mental wellbeing. These
improvements are accompanied by stronger social and family ties -- such as
greater perceived support and communication, while income and material
conditions remain largely unchanged. Our findings offer insights into the
design of policy exits and underscore the importance of incorporating measures
that sustain community- and family-based support systems when implementing or
ending assistance programs.

</details>


### [223] [Cost-benefit analysis of an AI-driven operational digital platform for integrated electric mobility, renewable energy, and grid management](https://arxiv.org/abs/2506.20631)
*Arega Getaneh Abate,Xiaobing Zhang,Xiufeng Liu,Dogan Keles*

Main category: econ.GN

TL;DR: 本文对用于跨部门优化的AI驱动运营数字平台进行成本效益分析，采用七步框架量化效益与支出。


<details>
  <summary>Details</summary>
Motivation: 电动出行和可再生能源与电网整合对脱碳等很重要，但存在运营协调平台开发和成本效益研究的挑战。

Method: 提出七步成本效益分析框架，与欧盟指南对齐，量化经济、可靠性和环境效益与资本及运营支出。

Result: 可明确将效益规模与AI驱动的运营数字平台及优化效率相联系，如量化平台带来的市场套利改进、预测能力和服务运营效率提升。

Conclusion: 该AI驱动的运营数字平台有助于提升能源效率、电网可靠性和环境可持续性。

Abstract: Integrating electric mobility (electric vehicles (EVs), electric trucks
(ETs)) and renewable energy sources (RES) with the power grid is paramount for
achieving decarbonization, efficiency, and stability. Given the rapid growth of
decentralized technologies and their critical role in decarbonization, two
critical challenges emerge: first, the development of a digital platform for
operational coordination; and second, rigorous research into their cost-benefit
profile. This paper addresses this by presenting a comprehensive cost-benefit
analysis (CBA) of an AI-driven operational digital platform (ODP) designed for
holistic, cross-sectoral optimization. The ODP aims to enhance energy
efficiency, grid reliability, and environmental sustainability. A seven-step
CBA framework, aligned with EU guidelines, quantifies economic, reliability,
and environmental benefits against capital and operational expenditures,
explicitly linking benefit magnitude to AI-driven ODP and optimization
efficiencies, such as quantified improvements in market arbitrage from ODP,
enabled forecasting, and enhanced operational efficiencies across various
services.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [224] [Introducing RobustiPy: An efficient next generation multiversal library with model selection, averaging, resampling, and explainable artificial intelligence](https://arxiv.org/abs/2506.19958)
*Daniel Valdenegro Ibarra,Jiani Yan,Duiyi Dai,Charles Rahal*

Main category: stat.ME

TL;DR: 介绍基于Python的RobustiPy框架用于模型不确定性量化和多宇宙分析，通过多种方法超越现有工具，经模拟和案例研究展示其加速推理和加深模型敏感性理解的能力。


<details>
  <summary>Details</summary>
Motivation: 开发超越现有不确定性量化工具的框架，支持严格的样本外评估和协变量预测贡献分配。

Method: 集成基于高效自举的置信区间、因变量规格的组合探索、模型选择和平均以及两个互补的联合推理程序。

Result: 在五个模拟和十个案例研究中部署该库，对约6.72亿个模拟线性回归进行时间分析，展示了加速推理和加深模型敏感性理解的效果。

Conclusion: RobustiPy不仅加速了稳健推理，还加深了对模型在分析多宇宙中敏感性的理解。

Abstract: We present RobustiPy, a next generation Python-based framework for model
uncertainty quantification and multiverse analysis, released under the GNU GPL
v3.0. Through the integration of efficient bootstrap-based confidence
intervals, combinatorial exploration of dependent-variable specifications,
model selection and averaging, and two complementary joint-inference routines,
RobustiPy transcends existing uncertainty-quantification tools. Its design
further supports rigorous out-of-sample evaluation and apportions the
predictive contribution of each covariate. We deploy the library across five
carefully constructed simulations and ten empirically grounded case studies
drawn from high-impact literature and teaching examples, including a novel
re-analysis of "unexplained discrepancies" in famous prior work. To illustrate
its performance, we time-profile RobustiPy over roughly 672 million simulated
linear regressions. These applications showcase how RobustiPy not only
accelerates robust inference but also deepens our interpretive insight into
model sensitivity across the vast analytical multiverse within which scientists
operate.

</details>


### [225] [hdbayes: An R Package for Bayesian Analysis of Generalized Linear Models Using Historical Data](https://arxiv.org/abs/2506.20060)
*Ethan M. Alt,Xinxin Chen,Luiz M. Carvalho,Joseph G. Ibrahim*

Main category: stat.ME

TL;DR: 本文介绍了R包hdbayes，用于广义线性模型多种先验的实现。


<details>
  <summary>Details</summary>
Motivation: 利用历史数据制定回归模型先验的方法虽多，但因软件获取和方法实现差异大，限制了其应用，需要统一实现。

Method: 使用Stan编程语言编写包主体，用R包装函数调用采样器，实现了多种广义线性模型的先验。

Result: 开发出R包hdbayes。

Conclusion: 该R包可解决现有方法应用受限和难以比较的问题。

Abstract: There has been increased interest in the use of historical data to formulate
informative priors in regression models. While many such priors for
incorporating historical data have been proposed, adoption is limited due to
access to software. Where software does exist, the implementations between
different methods could be vastly different, making comparisons between methods
difficult. In this paper, we introduce the R package hdbayes, an implementation
of the power prior, normalized power prior, Bayesian hierarchical model, robust
meta-analytic prior, commensurate prior, and latent exchangeability prior for
generalized linear models. The bulk of the package is written in the Stan
programming language, with user-friendly R wrapper functions to call samplers.

</details>


### [226] [Anytime-Valid Inference in Adaptive Experiments: Covariate Adjustment and Balanced Power](https://arxiv.org/abs/2506.20523)
*Daniel Molitor,Samantha Gold*

Main category: stat.ME

TL;DR: 本文介绍两种自适应实验方法MADCovar和MADMod，可解决自适应实验的关键局限，实现精确可靠因果推断，并通过开源软件包实现。


<details>
  <summary>Details</summary>
Motivation: 自适应实验虽有诸多优势，但存在损害有效推断、使次优处理缺乏检验效力等问题。

Method: 提出两种方法，MADCovar在任意时刻有效推断框架内结合协变量调整提高平均处理效应精度；MADMod将样本重新分配到低检验效力处理组，提高所有处理的统计检验力。

Result: 模拟研究和实证分析表明，MADCovar显著提高精度，MADMod确保次优处理的可靠推断。

Conclusion: 这两种方法解决了自适应实验的关键局限，为研究人员提供精确可靠因果推断的实用工具。

Abstract: Adaptive experiments have become central to modern experimental design,
enabling researchers to efficiently identify optimal treatments, improve
statistical power, and maximize respondent welfare. However, adaptive
experiments compromise valid inference and leave sub-optimal treatments
underpowered. We introduce two methodological advances for adaptive
experimentation. First, covariate-adjusted Mixture Adaptive Design (MADCovar)
achieves substantial improvements in average treatment effect (ATE) precision
by incorporating covariate adjustment within an anytime-valid inference
framework. Second, power-modified MAD (MADMod) reallocates sample to
underpowered treatment arms, improving statistical power across all treatments
while maintaining error control. Both methods provide anytime-valid guarantees,
enabling continuous monitoring without inflating Type 1 error rates. Simulation
studies and empirical analyses demonstrate that MADCovar delivers significant
precision gains and that MADMod ensures robust inference even for suboptimal
treatments. Together, these methods address key limitations of adaptive
experiments and equip researchers with practical tools for precise and reliable
causal inference. Our proposed methods are implemented through an open-source
software package.

</details>


### [227] [Inference for Error-Prone Count Data: Estimation under a Binomial Convolution Framework](https://arxiv.org/abs/2506.20596)
*Yuqiu Yang,Christina Vu,Cornelis J. Potgieter,Xinlei Wang,Akihito Kamata*

Main category: stat.ME

TL;DR: 本文提出二项卷积框架处理计数数据测量误差，比较三种估计策略，模拟显示各有优劣，并用实际数据验证框架效用。


<details>
  <summary>Details</summary>
Motivation: 计数数据测量误差常见但研究不足，尤其是观测分数有界且源于离散计分过程的情况，受口语阅读流畅性评估应用启发开展研究。

Method: 提出二项卷积框架，比较最大似然估计（MLE）、线性回归和广义矩估计（GMM）三种估计策略。

Result: 模拟显示MLE正确设定模型时最准确但计算密集且对错误设定不稳健；回归简单稳定但精度低；GMM在模型依赖性上有折中但对异常值敏感。

Conclusion: 该框架支持无监督环境下的改进推断，能进行分数校正，强调了估计器选择的实际影响和对计数数据中不对称测量误差建模的重要性。

Abstract: Measurement error in count data is common but underexplored in the
literature, particularly in contexts where observed scores are bounded and
arise from discrete scoring processes. Motivated by applications in oral
reading fluency assessment, we propose a binomial convolution framework that
extends binary misclassification models to settings where only the aggregate
number of correct responses is observed, and errors may involve both
overcounting and undercounting the number of events. The model accommodates
distinct true positive and true negative accuracy rates and preserves the
bounded nature of the data.
  Assuming the availability of both contaminated and error-free scores on a
subset of items, we develop and compare three estimation strategies: maximum
likelihood estimation (MLE), linear regression, and generalized method of
moments (GMM). Extensive simulations show that MLE is most accurate when the
model is correctly specified but is computationally intensive and less robust
to misspecification. Regression is simple and stable but less precise, while
GMM offers a compromise in model dependence, though it is sensitive to
outliers.
  In practice, this framework supports improved inference in unsupervised
settings where contaminated scores serve as inputs to downstream analyses. By
quantifying accuracy rates, the model enables score corrections even when no
specific outcome is yet defined. We demonstrate its utility using real oral
reading fluency data, comparing human and AI-generated scores. Findings
highlight the practical implications of estimator choice and underscore the
importance of explicitly modeling asymmetric measurement error in count data.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [228] [Machine-Learning-Assisted Photonic Device Development: A Multiscale Approach from Theory to Characterization](https://arxiv.org/abs/2506.20056)
*Yuheng Chen,Alexander Montes McNeil,Taehyuk Park,Blake A. Wilson,Vaishnavi Iyer,Michael Bezick,Jae-Ik Choi,Rohan Ojha,Pravin Mahendran,Daksh Kumar Singh,Geetika Chitturi,Peigang Chen,Trang Do,Alexander V. Kildishev,Vladimir M. Shalaev,Michael Moebius,Wenshan Cai,Yongmin Liu,Alexandra Boltasseva*

Main category: physics.optics

TL;DR: 回顾机器学习辅助光子器件开发（ML - PDD）的方法，助力复杂光子器件和系统发展。


<details>
  <summary>Details</summary>
Motivation: 传统光子器件开发（PDD）方法存在计算难、成本高、难扩展等问题，需新策略应对。

Method: 介绍机器学习的替代估计器、生成模型、强化学习和主动学习等方法助力ML - PDD。

Result: 呈现全面的ML - PDD方法，包括高效设计优化、快速模拟和表征建模及强化学习制造。

Conclusion: 为不同背景研究者提供见解，促进跨学科合作加速复杂光子器件和系统开发。

Abstract: Photonic device development (PDD) has achieved remarkable success in
designing and implementing new devices for controlling light across various
wavelengths, scales, and applications, including telecommunications, imaging,
sensing, and quantum information processing. PDD is an iterative, five-step
process that consists of: i) deriving device behavior from design parameters,
ii) simulating device performance, iii) finding the optimal candidate designs
from simulations, iv) fabricating the optimal device, and v) measuring device
performance. Classically, all these steps involve Bayesian optimization,
material science, control theory, and direct physics-driven numerical methods.
However, many of these techniques are computationally intractable, monetarily
costly, or difficult to implement at scale. In addition, PDD suffers from large
optimization landscapes, uncertainties in structural or optical
characterization, and difficulties in implementing robust fabrication
processes. However, the advent of machine learning over the past decade has
provided novel, data-driven strategies for tackling these challenges, including
surrogate estimators for speeding up computations, generative modeling for
noisy measurement modeling and data augmentation, reinforcement learning for
fabrication, and active learning for experimental physical discovery. In this
review, we present a comprehensive perspective on these methods to enable
machine-learning-assisted PDD (ML-PDD) for efficient design optimization with
powerful generative models, fast simulation and characterization modeling under
noisy measurements, and reinforcement learning for fabrication. This review
will provide researchers from diverse backgrounds with valuable insights into
this emerging topic, fostering interdisciplinary efforts to accelerate the
development of complex photonic devices and systems.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [229] [Do psychic cells generate consciousness?](https://arxiv.org/abs/2506.20164)
*Mototaka Suzuki,Jaan Aru*

Main category: q-bio.NC

TL;DR: 本文回顾了大脑意识处理细胞层面机制的进展，指出锥体神经元及相关代谢型受体是关键。


<details>
  <summary>Details</summary>
Motivation: 随着技术进步，以新方式探讨意识的基本问题，回顾细胞层面意识处理机制的进展。

Method: 对近期研究进行综述。

Result: 发现皮质锥体神经元在麻醉导致意识丧失时会出现反馈信号选择性中断，分布在锥体神经元树突上的一类代谢型受体是关键细胞机制。

Conclusion: 一个多世纪前Cajal的直觉可能是正确的，我们刚开始理解“精神细胞”是否以及如何产生和控制意识。

Abstract: Technological advances in the past decades have begun to enable
neuroscientists to address fundamental questions about consciousness in an
unprecedented way. Here we review remarkable recent progress in our
understanding of cellular-level mechanisms of conscious processing in the
brain. Of particular interest are the cortical pyramidal neurons -- or "psychic
cells" called by Ram\'on y Cajal more than 100 years ago -- which have an
intriguing cellular mechanism that accounts for selective disruption of
feedback signaling in the brain upon anesthetic-induced loss of consciousness.
Importantly, a particular class of metabotropic receptors distributed over the
dendrites of pyramidal cells are highlighted as the key cellular mechanism.
After all, Cajal's instinct over a century ago may turn out to be correct -- we
may have just begun to understand whether and how psychic cells indeed generate
and control our consciousness.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [230] [MILAAP: Mobile Link Allocation via Attention-based Prediction](https://arxiv.org/abs/2506.19947)
*Yung-Fu Chen,Anish Arora*

Main category: cs.NI

TL;DR: 本文提出MiLAAP框架，基于学习进行信道占用预测，无需状态共享，可应对节点移动，在动态网络中预测准确率高且有零样本泛化性。


<details>
  <summary>Details</summary>
Motivation: 传统信道跳频通信系统状态共享会带来大量通信开销，降低吞吐量效率，需新方法适应干扰变化和节点移动。

Method: 提出MiLAAP注意力预测框架，利用自注意力机制捕捉干扰区域内的时频谱模式预测信道占用，用多头自注意力机制捕捉时空依赖预测节点运动轨迹。

Result: 对于使用本地CS序列支持长寿命流流量的动态网络，MiLAAP信道状态预测准确率接近100%，且在不同CS序列周期有零样本泛化性。

Conclusion: MiLAAP框架在无需状态共享的情况下，能有效适应干扰变化和节点移动，提高信道状态预测准确性。

Abstract: Channel hopping (CS) communication systems must adapt to interference changes
in the wireless network and to node mobility for maintaining throughput
efficiency. Optimal scheduling requires up-to-date network state information
(i.e., of channel occupancy) to select non-overlapping channels for links in
interference regions. However, state sharing among nodes introduces significant
communication overhead, especially as network size or node mobility scale,
thereby decreasing throughput efficiency of already capacity-limited networks.
In this paper, we eschew state sharing while adapting the CS schedule based on
a learning-based channel occupancy prediction. We propose the MiLAAP
attention-based prediction framework for machine learning models of spectral,
spatial, and temporal dependencies among network nodes. MiLAAP uses a
self-attention mechanism that lets each node capture the temporospectral CS
pattern in its interference region and accordingly predict the channel
occupancy state within that region. Notably, the prediction relies only on
locally and passively observed channel activities, and thus introduces no
communication overhead. To deal with node mobility, MiLAAP also uses a
multi-head self-attention mechanism that lets each node locally capture the
spatiotemporal dependencies on other network nodes that can interfere with it
and accordingly predict the motion trajectory of those nodes. Detecting nodes
that enter or move outside the interference region is used to further improve
the prediction accuracy of channel occupancy. We show that for dynamic networks
that use local CS sequences to support relatively long-lived flow traffics, the
channel state prediction accuracy of MiLAAP is remarkably ~100% across
different node mobility patterns and it achieves zero-shot generalizability
across different periods of CS sequences.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [231] [Physics-Guided Radiotherapy Treatment Planning with Deep Learning](https://arxiv.org/abs/2506.19880)
*Stefanos Achlatis,Efstratios Gavves,Jan-Jakob Sonke*

Main category: physics.med-ph

TL;DR: 本文提出用于放疗计划的两阶段物理引导深度学习管道，在前列腺癌患者数据上验证其能生成接近临床真实情况的治疗计划，展现物理引导深度学习在放疗计划中的潜力。


<details>
  <summary>Details</summary>
Motivation: 自适应放疗需频繁修改治疗计划，需要高效解决方案，深度学习可自动化此过程。

Method: 提出两阶段物理引导深度学习管道，第一阶段直接监督治疗计划参数训练网络，第二阶段结合预测3D剂量分布的额外监督信号。

Result: 使用3D U - Net和UNETR架构实现的方法能生成接近临床真实的治疗计划，PTV的D95%和V95%有较好均值差异，还能减少危及器官辐射暴露。

Conclusion: 物理引导深度学习在放疗计划中有潜力。

Abstract: Radiotherapy (RT) is a critical cancer treatment, with volumetric modulated
arc therapy (VMAT) being a commonly used technique that enhances dose
conformity by dynamically adjusting multileaf collimator (MLC) positions and
monitor units (MU) throughout gantry rotation. Adaptive radiotherapy requires
frequent modifications to treatment plans to account for anatomical variations,
necessitating time-efficient solutions. Deep learning offers a promising
solution to automate this process. To this end, we propose a two-stage,
physics-guided deep learning pipeline for radiotherapy planning. In the first
stage, our network is trained with direct supervision on treatment plan
parameters, consisting of MLC and MU values. In the second stage, we
incorporate an additional supervision signal derived from the predicted 3D dose
distribution, integrating physics-based guidance into the training process. We
train and evaluate our approach on 133 prostate cancer patients treated with a
uniform 2-arc VMAT protocol delivering a dose of 62 Gy to the planning target
volume (PTV). Our results demonstrate that the proposed approach, implemented
using both 3D U-Net and UNETR architectures, consistently produces treatment
plans that closely match clinical ground truths. Our method achieves a mean
difference of D95% = 0.42 +/- 1.83 Gy and V95% = -0.22 +/- 1.87% at the PTV
while generating dose distributions that reduce radiation exposure to organs at
risk. These findings highlight the potential of physics-guided deep learning in
RT planning.

</details>


### [232] [Neural networks for the prediction of peel force for skin adhesive interface using FEM simulation](https://arxiv.org/abs/2506.19855)
*Ashish Masarkar,Rakesh Gupta,Naga Neehar Dingari,Beena Rai*

Main category: physics.med-ph

TL;DR: 提出基于神经网络方法预测胶粘剂从皮肤组织分离的最小剥离力，减少计算成本且表现良好，为皮肤 - 胶粘剂系统设计提供框架。


<details>
  <summary>Details</summary>
Motivation: 传统研究胶粘剂在皮肤上剥离行为的方法资源密集、计算昂贵且耗时，需新方法。

Method: 利用有限元模拟生成数据集训练神经网络模型，通过 5 折交叉验证。

Result: 模型能预测多种皮肤 - 胶粘剂剥离行为，测试集上均方误差为 3.66*10^ - 7，R^2 分数为 0.94。

Conclusion: 该方法可靠、计算高效，结合机器学习与生物力学模拟，为相关研究提供可扩展框架。

Abstract: Studying the peeling behaviour of adhesives on skin is vital for advancing
biomedical applications such as medical adhesives and transdermal patches.
Traditional methods like experimental testing and finite element method (FEM),
though considered gold standards, are resource-intensive, computationally
expensive and time-consuming, particularly when analysing a wide material
parameter space. In this study, we present a neural network-based approach to
predict the minimum peel force (F_min) required for adhesive detachment from
skin tissue, limiting the need for repeated FEM simulations and significantly
reducing the computational cost. Leveraging a dataset generated from FEM
simulations of 90 degree peel test with varying adhesive and fracture mechanics
parameters, our neural network model achieved high accuracy, validated through
rigorous 5-fold cross-validation. The final architecture was able to predict a
wide variety of skin-adhesive peeling behaviour, exhibiting a mean squared
error (MSE) of 3.66*10^-7 and a R^2 score of 0.94 on test set, demonstrating
robust performance. This work introduces a reliable, computationally efficient
method for predicting adhesive behaviour, significantly reducing simulation
time while maintaining accuracy. This integration of machine learning with
high-fidelity biomechanical simulations enables efficient design and
optimization of skin-adhesive systems, providing a scalable framework for
future research in computational dermato-mechanics and bio-adhesive material
design.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [233] [PocketVina Enables Scalable and Highly Accurate Physically Valid Docking through Multi-Pocket Conditioning](https://arxiv.org/abs/2506.20043)
*Ahmet Sarigun,Bora Uyar,Vedran Franke,Altuna Akalin*

Main category: q-bio.QM

TL;DR: 介绍PocketVina对接框架，在多个基准测试中表现出色，还推出TargetDock - AI数据集，PocketVina适合高通量虚拟筛选和基于结构的药物发现。


<details>
  <summary>Details</summary>
Motivation: 解决分子对接中采样物理有效配体结合姿势的难题，特别是针对未见或结构多样的靶点。

Method: 引入结合口袋预测与系统多口袋探索的PocketVina搜索式对接框架，推出TargetDock - AI基准数据集。

Result: PocketVina在多个基准测试中表现良好，在结合配体RMSD和物理有效性方面达最优，能区分活性和非活性靶点，优于深度学习基线且所需GPU内存和运行时间少。

Conclusion: PocketVina是一种强大且可扩展的对接策略，无需特定任务训练，适用于高通量虚拟筛选和基于结构的药物发现。

Abstract: Sampling physically valid ligand-binding poses remains a major challenge in
molecular docking, particularly for unseen or structurally diverse targets. We
introduce PocketVina, a fast and memory-efficient, search-based docking
framework that combines pocket prediction with systematic multi-pocket
exploration. We evaluate PocketVina across four established
benchmarks--PDBbind2020 (timesplit and unseen), DockGen, Astex, and
PoseBusters--and observe consistently strong performance in sampling physically
valid docking poses. PocketVina achieves state-of-the-art performance when
jointly considering ligand RMSD and physical validity (PB-valid), while
remaining competitive with deep learning-based approaches in terms of RMSD
alone, particularly on structurally diverse and previously unseen targets.
PocketVina also maintains state-of-the-art physically valid docking accuracy
across ligands with varying degrees of flexibility. We further introduce
TargetDock-AI, a benchmarking dataset we curated, consisting of over 500000
protein-ligand pairs, and a partition of the dataset labeled with PubChem
activity annotations. On this large-scale dataset, PocketVina successfully
discriminates active from inactive targets, outperforming a deep learning
baseline while requiring significantly less GPU memory and runtime. PocketVina
offers a robust and scalable docking strategy that requires no task-specific
training and runs efficiently on standard GPUs, making it well-suited for
high-throughput virtual screening and structure-based drug discovery.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [234] [Exploration-Exploitation Tradeoff in Universal Lossy Compression](https://arxiv.org/abs/2506.20261)
*Nir Weinberger,Ram Zamir*

Main category: cs.IT

TL;DR: 将顺序模式重铸为多臂老虎机问题，研究有损压缩中探索与利用的权衡，分析已有方案局限并推导新算法。


<details>
  <summary>Details</summary>
Motivation: 研究通用压缩顺序模式中探索与利用的权衡。

Method: 将顺序模式重铸为多臂老虎机问题，分析已有“自然类型选择”方案，推导新的鲁棒成本导向MAB算法。

Result: 表明“自然类型选择”方案可视为重建导向MAB算法，指出其在鲁棒性和短块性能方面的局限，得到可用于任意块长的新算法。

Conclusion: 提出的鲁棒成本导向MAB算法能解决已有方案局限，适用于任意块长。

Abstract: Universal compression can learn the source and adapt to it either in a batch
mode (forward adaptation), or in a sequential mode (backward adaptation). We
recast the sequential mode as a multi-armed bandit problem, a fundamental model
in reinforcement-learning, and study the trade-off between exploration and
exploitation in the lossy compression case. We show that a previously proposed
"natural type selection" scheme can be cast as a reconstruction-directed MAB
algorithm, for sequential lossy compression, and explain its limitations in
terms of robustness and short-block performance. We then derive and analyze
robust cost-directed MAB algorithms, which work at any block length.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [235] [Quantum Neural Networks for Propensity Score Estimation and Survival Analysis in Observational Biomedical Studies](https://arxiv.org/abs/2506.19973)
*Vojtěch Novák,Ivan Zelinka,Lenka Přibylová,Lubomír Martínek*

Main category: quant-ph

TL;DR: 研究用QNN估计倾向得分以解决结直肠癌手术生存结果比较中的选择偏差，QNN表现良好，调整后无显著生存差异。


<details>
  <summary>Details</summary>
Motivation: 解决腹腔镜和开放手术生存结果比较中的选择偏差问题。

Method: 用含77个变量的数据集开发基于QNN的倾向得分模型，采用特定架构和优化策略，结合方差正则化，在不同条件下模拟。

Result: QNN在小样本中优于经典模型，倾向得分匹配和加权实现协变量平衡，调整后生存分析无显著差异。

Conclusion: QNN结合CMA - ES和噪声感知策略可改善生物医学研究中的因果推断，尤其适用于小样本、高维数据集。

Abstract: This study investigates the application of quantum neural networks (QNNs) for
propensity score estimation to address selection bias in comparing survival
outcomes between laparoscopic and open surgical techniques in a cohort of 1177
colorectal carcinoma patients treated at University Hospital Ostrava
(2001-2009). Using a dataset with 77 variables, including patient demographics
and tumor characteristics, we developed QNN-based propensity score models
focusing on four key covariates (Age, Sex, Stage, BMI). The QNN architecture
employed a linear ZFeatureMap for data encoding, a SummedPaulis operator for
predictions, and the Covariance Matrix Adaptation Evolution Strategy (CMA-ES)
for robust, gradient-free optimization in noisy quantum environments. Variance
regularization was integrated to mitigate quantum measurement noise, with
simulations conducted under exact, sampling (1024 shots), and noisy hardware
(FakeManhattanV2) conditions. QNNs, particularly with simulated hardware noise,
outperformed classical logistic regression and gradient boosted machines in
small samples (AUC up to 0.750 for n=100), with noise modeling enhancing
predictive stability. Propensity score matching and weighting, optimized via
genetic matching and matching weights, achieved covariate balance with
standardized mean differences of 0.0849 and 0.0869, respectively. Survival
analyses using Kaplan-Meier estimation, Cox proportional hazards, and Aalen
additive regression revealed no significant survival differences
post-adjustment (p-values 0.287-0.851), indicating confounding bias in
unadjusted outcomes. These results highlight QNNs' potential, enhanced by
CMA-ES and noise-aware strategies, to improve causal inference in biomedical
research, particularly for small-sample, high-dimensional datasets.

</details>
