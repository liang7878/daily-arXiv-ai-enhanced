<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 22]
- [cs.CE](#cs.CE) [Total: 7]
- [cs.DB](#cs.DB) [Total: 2]
- [cs.DC](#cs.DC) [Total: 3]
- [cs.DS](#cs.DS) [Total: 3]
- [cs.GT](#cs.GT) [Total: 1]
- [cs.IR](#cs.IR) [Total: 17]
- [cs.LG](#cs.LG) [Total: 53]
- [cs.NE](#cs.NE) [Total: 2]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.SE](#cs.SE) [Total: 11]
- [stat.ML](#stat.ML) [Total: 4]
- [stat.CO](#stat.CO) [Total: 1]
- [cs.CY](#cs.CY) [Total: 2]
- [cs.CV](#cs.CV) [Total: 20]
- [cs.NI](#cs.NI) [Total: 2]
- [cs.SD](#cs.SD) [Total: 6]
- [econ.GN](#econ.GN) [Total: 6]
- [cs.RO](#cs.RO) [Total: 4]
- [eess.SY](#eess.SY) [Total: 1]
- [math.OC](#math.OC) [Total: 1]
- [quant-ph](#quant-ph) [Total: 3]
- [eess.SP](#eess.SP) [Total: 2]
- [eess.IV](#eess.IV) [Total: 4]
- [physics.app-ph](#physics.app-ph) [Total: 1]
- [math.DS](#math.DS) [Total: 1]
- [cs.HC](#cs.HC) [Total: 1]
- [q-bio.OT](#q-bio.OT) [Total: 1]
- [hep-lat](#hep-lat) [Total: 1]
- [stat.ME](#stat.ME) [Total: 4]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [cs.MA](#cs.MA) [Total: 1]
- [eess.AS](#eess.AS) [Total: 2]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cs.CR](#cs.CR) [Total: 7]
- [cs.CL](#cs.CL) [Total: 31]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [q-bio.GN](#q-bio.GN) [Total: 1]
- [cs.CC](#cs.CC) [Total: 1]
- [math.ST](#math.ST) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Human-AI Collaboration Increases Efficiency in Regulatory Writing](https://arxiv.org/abs/2509.09738)
*Umut Eser,Yael Gozin,L. Jay Stallons,Ari Caroline,Martin Preusse,Brandon Rice,Scott Wright,Andrew Robertson*

Main category: cs.AI

TL;DR: 评估AutoIND平台在IND申请初稿撰写中的表现，发现其能大幅缩短时间，但输出需专家完善。


<details>
  <summary>Details</summary>
Motivation: IND申请准备耗时长且依赖专业知识，影响早期临床开发，评估AutoIND能否缩短初稿撰写时间并保证文档质量。

Method: 记录AutoIND生成IND非临床书面总结的起草时间，与美国FDA先前批准的手动起草时间作对比，由评估员按七个预定义类别评估质量。

Result: AutoIND将初始起草时间减少约97%，IND - 1和IND - 2质量得分分别为69.6%和77.9%，未检测到关键监管错误，但存在强调、简洁性和清晰度不足问题。

Conclusion: AutoIND可显著加速IND起草，但仍需专家完善输出质量，发现的不足为模型改进提供方向。

Abstract: Background: Investigational New Drug (IND) application preparation is
time-intensive and expertise-dependent, slowing early clinical development.
Objective: To evaluate whether a large language model (LLM) platform (AutoIND)
can reduce first-draft composition time while maintaining document quality in
regulatory submissions. Methods: Drafting times for IND nonclinical written
summaries (eCTD modules 2.6.2, 2.6.4, 2.6.6) generated by AutoIND were directly
recorded. For comparison, manual drafting times for IND summaries previously
cleared by the U.S. FDA were estimated from the experience of regulatory
writers ($\geq$6 years) and used as industry-standard benchmarks. Quality was
assessed by a blinded regulatory writing assessor using seven pre-specified
categories: correctness, completeness, conciseness, consistency, clarity,
redundancy, and emphasis. Each sub-criterion was scored 0-3 and normalized to a
percentage. A critical regulatory error was defined as any misrepresentation or
omission likely to alter regulatory interpretation (e.g., incorrect NOAEL,
omission of mandatory GLP dose-formulation analysis). Results: AutoIND reduced
initial drafting time by $\sim$97% (from $\sim$100 h to 3.7 h for 18,870
pages/61 reports in IND-1; and to 2.6 h for 11,425 pages/58 reports in IND-2).
Quality scores were 69.6\% and 77.9\% for IND-1 and IND-2. No critical
regulatory errors were detected, but deficiencies in emphasis, conciseness, and
clarity were noted. Conclusions: AutoIND can dramatically accelerate IND
drafting, but expert regulatory writers remain essential to mature outputs to
submission-ready quality. Systematic deficiencies identified provide a roadmap
for targeted model improvements.

</details>


### [2] [Executable Ontologies: Synthesizing Event Semantics with Dataflow Architecture](https://arxiv.org/abs/2509.09775)
*Aleksandr Boldachev*

Main category: cs.AI

TL;DR: 介绍boldsea，一种用可执行本体建模复杂动态系统的架构，展示其优势并介绍相关语言和引擎架构。


<details>
  <summary>Details</summary>
Motivation: 解决传统业务流程管理系统和面向对象语义技术的局限性。

Method: 将事件语义与数据流架构集成，提出正式的BSL语言，设计boldsea - engine架构。

Result: 实现运行时修改事件模型、保证时间透明度，在统一语义框架中无缝融合数据和业务逻辑。

Conclusion: 事件语义与数据流架构集成的方法有效，boldsea架构有显著优势。

Abstract: This paper presents boldsea, Boldachev's semantic-event approach -- an
architecture for modeling complex dynamic systems using executable ontologies
-- semantic models that act as dynamic structures, directly controlling process
execution. We demonstrate that integrating event semantics with a dataflow
architecture addresses the limitations of traditional Business Process
Management (BPM) systems and object-oriented semantic technologies. The paper
presents the formal BSL (boldsea Semantic Language), including its BNF grammar,
and outlines the boldsea-engine's architecture, which directly interprets
semantic models as executable algorithms without compilation. It enables the
modification of event models at runtime, ensures temporal transparency, and
seamlessly merges data and business logic within a unified semantic framework.

</details>


### [3] [How well can LLMs provide planning feedback in grounded environments?](https://arxiv.org/abs/2509.09790)
*Yuxuan Li,Victor Zhong*

Main category: cs.AI

TL;DR: 评估大语言模型和视觉语言模型在不同环境下提供规划反馈的效果，发现基础模型能提供多样高质量反馈，大模型和推理模型表现更好，但复杂环境下反馈质量下降。


<details>
  <summary>Details</summary>
Motivation: 传统规划学习需精心设计奖励函数或高质量标注演示，而预训练基础模型包含对规划有用的背景知识，可减少奖励设计和演示需求，因此评估其提供反馈的效果。

Method: 评估大语言模型和视觉语言模型在符号、语言和连续控制环境中提供反馈的情况，考虑多种规划反馈类型和影响反馈性能的推理方法。

Result: 基础模型能跨领域提供多样高质量反馈，更大和具备推理能力的模型反馈更准确、偏差更小，且从增强推理方法中受益更多；复杂动态或连续状态与动作空间的环境中反馈质量下降。

Conclusion: 基础模型在规划反馈方面有一定优势，但在复杂环境下存在局限性。

Abstract: Learning to plan in grounded environments typically requires carefully
designed reward functions or high-quality annotated demonstrations. Recent
works show that pretrained foundation models, such as large language models
(LLMs) and vision language models (VLMs), capture background knowledge helpful
for planning, which reduces the amount of reward design and demonstrations
needed for policy learning. We evaluate how well LLMs and VLMs provide feedback
across symbolic, language, and continuous control environments. We consider
prominent types of feedback for planning including binary feedback, preference
feedback, action advising, goal advising, and delta action feedback. We also
consider inference methods that impact feedback performance, including
in-context learning, chain-of-thought, and access to environment dynamics. We
find that foundation models can provide diverse high-quality feedback across
domains. Moreover, larger and reasoning models consistently provide more
accurate feedback, exhibit less bias, and benefit more from enhanced inference
methods. Finally, feedback quality degrades for environments with complex
dynamics or continuous state spaces and action spaces.

</details>


### [4] [A Modular and Multimodal Generative AI Framework for Urban Building Energy Data: Generating Synthetic Homes](https://arxiv.org/abs/2509.09794)
*Jackson Eshbaugh,Chetan Tiwari,Jorge Silveyra*

Main category: cs.AI

TL;DR: 提出模块化多模态框架用生成式AI从公开信息和图像生成能源建模数据，避免生成模型常见问题，数据真实可标注，推动研究可及和可复现。


<details>
  <summary>Details</summary>
Motivation: 计算模型用于能源建模研究需大量数据，部分数据获取难、成本高或涉及隐私问题。

Method: 引入模块化多模态框架，利用生成式AI从公开住宅信息和图像生成数据，并提供框架演示流程，评估生成式AI组件。

Result: 框架使用AI避免了生成模型常见问题，能产生真实、有标注的数据。

Conclusion: 减少对昂贵或受限数据源的依赖，为更易获取和可复现的研究铺平道路。

Abstract: Computational models have emerged as powerful tools for energy modeling
research, touting scalability and quantitative results. However, these models
require a plethora of data, some of which is inaccessible, expensive, or raises
privacy concerns. We introduce a modular multimodal framework to produce this
data from publicly accessible residential information and images using
generative artificial intelligence (AI). Additionally, we provide a pipeline
demonstrating this framework, and we evaluate its generative AI components. Our
experiments show that our framework's use of AI avoids common issues with
generative models. Our framework produces realistic, labeled data. By reducing
dependence on costly or restricted data sources, we pave a path towards more
accessible and reproducible research.

</details>


### [5] [Towards a Common Framework for Autoformalization](https://arxiv.org/abs/2509.09810)
*Agnieszka Mensfelt,David Tena Cucala,Santiago Franco,Angeliki Koutsoukou-Argyraki,Vince Trencsenyi,Kostas Stathis*

Main category: cs.AI

TL;DR: 本文回顾可视为自动形式化的实例并提出统一框架，以促进不同领域交叉融合推动下一代AI系统发展。


<details>
  <summary>Details</summary>
Motivation: 自动形式化及相关研究领域独立发展，限制了共享方法、基准和理论框架，阻碍了进展。

Method: 回顾自动形式化的显式或隐式实例。

Result: 提出统一框架。

Conclusion: 该框架能促进不同领域交叉融合，推动下一代AI系统发展。

Abstract: Autoformalization has emerged as a term referring to the automation of
formalization - specifically, the formalization of mathematics using
interactive theorem provers (proof assistants). Its rapid development has been
driven by progress in deep learning, especially large language models (LLMs).
More recently, the term has expanded beyond mathematics to describe the broader
task of translating informal input into formal logical representations. At the
same time, a growing body of research explores using LLMs to translate informal
language into formal representations for reasoning, planning, and knowledge
representation - often without explicitly referring to this process as
autoformalization. As a result, despite addressing similar tasks, the largely
independent development of these research areas has limited opportunities for
shared methodologies, benchmarks, and theoretical frameworks that could
accelerate progress. The goal of this paper is to review - explicit or implicit
- instances of what can be considered autoformalization and to propose a
unified framework, encouraging cross-pollination between different fields to
advance the development of next generation AI systems.

</details>


### [6] [Towards an AI-based knowledge assistant for goat farmers based on Retrieval-Augmented Generation](https://arxiv.org/abs/2509.09848)
*Nana Han,Dong Liu,Tomas Norton*

Main category: cs.AI

TL;DR: 本文介绍了用于山羊养殖健康管理的智能知识辅助系统，采用RAG及两种结构化知识处理方法，建立知识库并集成在线搜索模块，实验验证了系统有效性，指出改进方向。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在畜牧养殖应用受限，需开发支持山羊养殖健康管理的系统。

Method: 利用RAG，提出表文本化和决策树文本化两种结构化知识处理方法，建立特定领域山羊养殖知识库，集成在线搜索模块，进行六项消融实验。

Result: 异构知识融合方法效果最佳，验证集平均准确率87.90%，测试集84.22%，问答任务准确率超85%，错误以遗漏为主。

Conclusion: 所提系统在山羊养殖实际应用中具有鲁棒性和可靠性。

Abstract: Large language models (LLMs) are increasingly being recognised as valuable
knowledge communication tools in many industries. However, their application in
livestock farming remains limited, being constrained by several factors not
least the availability, diversity and complexity of knowledge sources. This
study introduces an intelligent knowledge assistant system designed to support
health management in farmed goats. Leveraging the Retrieval-Augmented
Generation (RAG), two structured knowledge processing methods, table
textualization and decision-tree textualization, were proposed to enhance large
language models' (LLMs) understanding of heterogeneous data formats. Based on
these methods, a domain-specific goat farming knowledge base was established to
improve LLM's capacity for cross-scenario generalization. The knowledge base
spans five key domains: Disease Prevention and Treatment, Nutrition Management,
Rearing Management, Goat Milk Management, and Basic Farming Knowledge.
Additionally, an online search module is integrated to enable real-time
retrieval of up-to-date information. To evaluate system performance, six
ablation experiments were conducted to examine the contribution of each
component. The results demonstrated that heterogeneous knowledge fusion method
achieved the best results, with mean accuracies of 87.90% on the validation set
and 84.22% on the test set. Across the text-based, table-based, decision-tree
based Q&A tasks, accuracy consistently exceeded 85%, validating the
effectiveness of structured knowledge fusion within a modular design. Error
analysis identified omission as the predominant error category, highlighting
opportunities to further improve retrieval coverage and context integration. In
conclusion, the results highlight the robustness and reliability of the
proposed system for practical applications in goat farming.

</details>


### [7] [LLMs as Agentic Cooperative Players in Multiplayer UNO](https://arxiv.org/abs/2509.09867)
*Yago Romano Matinez,Jesse Roberts*

Main category: cs.AI

TL;DR: 本文测试大语言模型能否作为主动参与者帮助玩家完成目标，在UNO游戏中评估不同规模模型表现，发现多数模型难以显著帮助其他玩家。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型作为主动参与者帮助人类完成目标的能力极限。

Method: 构建工具让仅解码器的大语言模型在RLCard游戏环境中作为代理参与UNO游戏，采用两种提示策略，评估不同规模模型。

Result: 所有模型在UNO游戏中表现优于随机基线，但很少能显著帮助其他玩家。

Conclusion: 大语言模型在帮助其他玩家完成目标方面存在一定局限性。

Abstract: LLMs promise to assist humans -- not just by answering questions, but by
offering useful guidance across a wide range of tasks. But how far does that
assistance go? Can a large language model based agent actually help someone
accomplish their goal as an active participant? We test this question by
engaging an LLM in UNO, a turn-based card game, asking it not to win but
instead help another player to do so. We built a tool that allows decoder-only
LLMs to participate as agents within the RLCard game environment. These models
receive full game-state information and respond using simple text prompts under
two distinct prompting strategies. We evaluate models ranging from small (1B
parameters) to large (70B parameters) and explore how model scale impacts
performance. We find that while all models were able to successfully outperform
a random baseline when playing UNO, few were able to significantly aid another
player.

</details>


### [8] [The (R)evolution of Scientific Workflows in the Agentic AI Era: Towards Autonomous Science](https://arxiv.org/abs/2509.09915)
*Woong Shin,Renan Souza,Daniel Rosendo,Frédéric Suter,Feiyi Wang,Prasanna Balaprakash,Rafael Ferreira da Silva*

Main category: cs.AI

TL;DR: 提出概念框架和架构蓝图，助力从当前工作流管理系统向自主分布式科学实验室发展，有望加速科学发现。


<details>
  <summary>Details</summary>
Motivation: 现代科学发现需协调分布式设施和异构资源，使研究者负担重，AI 虽带来机遇但落地集成不明。

Method: 提出工作流沿智能和组合两个维度进化的概念框架及架构蓝图。

Result: 给出可帮助科学界利用自主科学机遇的架构蓝图。

Conclusion: 该架构蓝图有加速 100 倍科学发现及变革科学工作流的潜力。

Abstract: Modern scientific discovery increasingly requires coordinating distributed
facilities and heterogeneous resources, forcing researchers to act as manual
workflow coordinators rather than scientists. Advances in AI leading to AI
agents show exciting new opportunities that can accelerate scientific discovery
by providing intelligence as a component in the ecosystem. However, it is
unclear how this new capability would materialize and integrate in the real
world. To address this, we propose a conceptual framework where workflows
evolve along two dimensions which are intelligence (from static to intelligent)
and composition (from single to swarm) to chart an evolutionary path from
current workflow management systems to fully autonomous, distributed scientific
laboratories. With these trajectories in mind, we present an architectural
blueprint that can help the community take the next steps towards harnessing
the opportunities in autonomous science with the potential for 100x discovery
acceleration and transformational scientific workflows.

</details>


### [9] [A Markovian Framing of WaveFunctionCollapse for Procedurally Generating Aesthetically Complex Environments](https://arxiv.org/abs/2509.09919)
*Franklin Yiu,Mohan Lu,Nina Li,Kevin Joseph,Tianxu Zhang,Julian Togelius,Timothy Merino,Sam Earle*

Main category: cs.AI

TL;DR: 将WaveFunctionCollapse (WFC) 重新表述为马尔可夫决策过程 (MDP) 以优化程序内容生成，实验表明该方法优于传统进化方法。


<details>
  <summary>Details</summary>
Motivation: 解决程序内容生成中同时优化约束和目标的挑战。

Method: 将WFC重新表述为MDP，让外部优化算法专注目标最大化，利用WFC传播机制确保约束满足。

Result: 在多个不同难度领域中，传统联合优化方法在任务复杂度增加时表现不佳，且始终不如基于WFC - MDP的优化方法。

Conclusion: 将局部约束满足与全局目标优化解耦具有优势。

Abstract: Procedural content generation often requires satisfying both
designer-specified objectives and adjacency constraints implicitly imposed by
the underlying tile set. To address the challenges of jointly optimizing both
constraints and objectives, we reformulate WaveFunctionCollapse (WFC) as a
Markov Decision Process (MDP), enabling external optimization algorithms to
focus exclusively on objective maximization while leveraging WFC's propagation
mechanism to enforce constraint satisfaction. We empirically compare optimizing
this MDP to traditional evolutionary approaches that jointly optimize global
metrics and local tile placement. Across multiple domains with various
difficulties, we find that joint optimization not only struggles as task
complexity increases, but consistently underperforms relative to optimization
over the WFC-MDP, underscoring the advantages of decoupling local constraint
satisfaction from global objective optimization.

</details>


### [10] [Evaluation of Black-Box XAI Approaches for Predictors of Values of Boolean Formulae](https://arxiv.org/abs/2509.09982)
*Stav Armoni-Friedmann,Hana Chockler,David A. Kelly*

Main category: cs.AI

TL;DR: 本文聚焦表格数据和布尔函数值预测的AI模型，提出变量重要性度量，评估现有XAI工具，还推出新工具B - ReX并证明其优越性。


<details>
  <summary>Details</summary>
Motivation: 由于解释的主观性，评估可解释AI（XAI）方法是一项具有挑战性的任务，本文旨在解决该领域在表格数据和布尔函数值预测方面的评估问题。

Method: 提出基于实际因果关系的变量重要性的正式且精确的度量，评估现有XAI工具；基于现有工具ReX开发新工具B - ReX。

Result: 在大规模基准测试中，B - ReX优于其他黑盒XAI工具，在随机10值布尔公式上实现了0.072 ± 0.012的Jensen - Shannon散度。

Conclusion: 提出的变量重要性度量可用于评估XAI工具，新工具B - ReX具有更好的性能。

Abstract: Evaluating explainable AI (XAI) approaches is a challenging task in general,
due to the subjectivity of explanations. In this paper, we focus on tabular
data and the specific use case of AI models predicting the values of Boolean
functions. We extend the previous work in this domain by proposing a formal and
precise measure of importance of variables based on actual causality, and we
evaluate state-of-the-art XAI tools against this measure. We also present a
novel XAI tool B-ReX, based on the existing tool ReX, and demonstrate that it
is superior to other black-box XAI tools on a large-scale benchmark.
Specifically, B-ReX achieves a Jensen-Shannon divergence of 0.072 $\pm$ 0.012
on random 10-valued Boolean formulae

</details>


### [11] [GAMA: A General Anonymizing Multi-Agent System for Privacy Preservation Enhanced by Domain Rules and Disproof Method](https://arxiv.org/abs/2509.10018)
*Hailong Yang,Renhuo Zhao,Guanjin Wang,Zhaohong Deng*

Main category: cs.AI

TL;DR: 提出通用匿名多智能体系统GAMA，划分公私空间保护隐私，有DRKE和DLE模块，在多数据集测试表现优。


<details>
  <summary>Details</summary>
Motivation: 高性能大语言模型多在公共远程服务器，含隐私数据任务需隐私保护机制。

Method: 提出GAMA系统，划分公私空间，用匿名机制保护隐私，含DRKE和DLE模块减少语义损失。

Result: 在Trivia Creative Writing、Logic Grid Puzzle等数据集测试，GAMA性能优于现有模型，在隐私保护上也有效。

Conclusion: GAMA在任务处理和隐私保护上效果出色。

Abstract: With the rapid advancement of Large Language Model (LLM), LLM-based agents
exhibit exceptional abilities in understanding and generating natural language,
facilitating human-like collaboration and information transmission in LLM-based
Multi-Agent System (MAS). High-performance LLMs are often hosted on remote
servers in public spaces. When tasks involve privacy data, MAS cannot securely
utilize these LLMs without implementing privacy-preserving mechanisms. To
address this challenge, we propose a General Anonymizing Multi-Agent system
(GAMA), which divides the agents' workspace into private and public spaces and
protects privacy through the anonymizing mechanism. In the private space,
agents handle sensitive data, while in the public space, only anonymized data
is utilized. GAMA incorporates two key modules to mitigate semantic loss caused
by anonymization: Domain-Rule-based Knowledge Enhancement (DRKE) and
Disproof-based Logic Enhancement (DLE). We evaluate GAMA on two public
question-answering datasets: Trivia Creative Writing and Logic Grid Puzzle. The
results demonstrate that GAMA has superior performance compared to the
state-of-the-art models. To further assess its privacy-preserving capabilities,
we designed two new datasets: Knowledge Privacy Preservation and Logic Privacy
Preservation. The final results highlight GAMA's exceptional effectiveness in
both task processing and privacy preservation.

</details>


### [12] [XAgents: A Unified Framework for Multi-Agent Cooperation via IF-THEN Rules and Multipolar Task Processing Graph](https://arxiv.org/abs/2509.10054)
*Hailong Yang,Mingxian Gu,Jianqi Wang,Guanjin Wang,Zhaohong Deng*

Main category: cs.AI

TL;DR: 提出XAgents多智能体合作框架，在多数据集上表现超现有方法，代码开源。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽提升多智能体系统能力，但处理复杂不确定任务时任务规划仍有挑战，输出可能误导或错误。

Method: 构建基于多极任务处理图和IF - THEN规则的XAgents框架，用多极任务处理图进行动态任务规划和处理不确定性，子任务处理时集成特定领域IF - THEN规则约束行为，用全局规则增强协作。

Result: 在三个不同数据集上评估，XAgents在知识型和逻辑型问答任务中始终超越现有单智能体和多智能体方法。

Conclusion: XAgents框架能有效解决多智能体系统在复杂不确定任务中的任务规划问题。

Abstract: The rapid advancement of Large Language Models (LLMs) has significantly
enhanced the capabilities of Multi-Agent Systems (MAS) in supporting humans
with complex, real-world tasks. However, MAS still face challenges in effective
task planning when handling highly complex tasks with uncertainty, often
resulting in misleading or incorrect outputs that hinder task execution. To
address this, we propose XAgents, a unified multi-agent cooperative framework
built on a multipolar task processing graph and IF-THEN rules. XAgents uses the
multipolar task processing graph to enable dynamic task planning and handle
task uncertainty. During subtask processing, it integrates domain-specific
IF-THEN rules to constrain agent behaviors, while global rules enhance
inter-agent collaboration. We evaluate the performance of XAgents across three
distinct datasets, demonstrating that it consistently surpasses
state-of-the-art single-agent and multi-agent approaches in both
knowledge-typed and logic-typed question-answering tasks. The codes for XAgents
are available at: https://github.com/AGI-FHBC/XAgents.

</details>


### [13] [AI Harmonics: a human-centric and harms severity-adaptive AI risk assessment framework](https://arxiv.org/abs/2509.10104)
*Sofia Vei,Paolo Giudici,Pavlos Sermpezis,Athena Vakali,Adelaide Emma Bernardelli*

Main category: cs.AI

TL;DR: 现有AI风险评估模型有局限，提出AI Harmonics方法，实验验证其能识别伤害分布以有效缓解风险。


<details>
  <summary>Details</summary>
Motivation: 现有AI风险评估模型注重内部合规，忽略利益相关者视角和现实后果，AI发展带来社会危害和风险。

Method: 提出以人类为中心、基于实证事件数据的危害严重性自适应方法AI Harmonics，包括新的AI危害评估指标AIH。

Result: 实验表明政治和身体伤害集中度最高需紧急缓解，AI Harmonics能识别不均衡的伤害分布。

Conclusion: AI Harmonics方法具有现实意义，能让政策制定者和组织有效开展缓解工作。

Abstract: The absolute dominance of Artificial Intelligence (AI) introduces
unprecedented societal harms and risks. Existing AI risk assessment models
focus on internal compliance, often neglecting diverse stakeholder perspectives
and real-world consequences. We propose a paradigm shift to a human-centric,
harm-severity adaptive approach grounded in empirical incident data. We present
AI Harmonics, which includes a novel AI harm assessment metric (AIH) that
leverages ordinal severity data to capture relative impact without requiring
precise numerical estimates. AI Harmonics combines a robust, generalized
methodology with a data-driven, stakeholder-aware framework for exploring and
prioritizing AI harms. Experiments on annotated incident data confirm that
political and physical harms exhibit the highest concentration and thus warrant
urgent mitigation: political harms erode public trust, while physical harms
pose serious, even life-threatening risks, underscoring the real-world
relevance of our approach. Finally, we demonstrate that AI Harmonics
consistently identifies uneven harm distributions, enabling policymakers and
organizations to target their mitigation efforts effectively.

</details>


### [14] [Virtual Agent Economies](https://arxiv.org/abs/2509.10147)
*Nenad Tomasev,Matija Franklin,Joel Z. Leibo,Julian Jacobs,William A. Cunningham,Iason Gabriel,Simon Osindero*

Main category: cs.AI

TL;DR: 提出“沙盒经济”框架分析自主AI智能体产生的新经济层，指出发展趋势与挑战，探讨引导AI智能体市场的设计选择。


<details>
  <summary>Details</summary>
Motivation: 随着自主AI智能体的快速应用，出现新经济层且超出人类直接监管，需框架分析该系统。

Method: 从起源（涌现与有意）和与人类经济分离程度（可渗透与不可渗透）两个维度定义“沙盒经济”框架，讨论引导AI智能体市场的设计选择。

Result: 当前趋势指向AI智能体经济自发涌现且高度可渗透，带来协调机遇与经济风险、不平等加剧等挑战。

Conclusion: 应主动设计可引导的智能体市场，确保技术变革符合人类长期繁荣。

Abstract: The rapid adoption of autonomous AI agents is giving rise to a new economic
layer where agents transact and coordinate at scales and speeds beyond direct
human oversight. We propose the "sandbox economy" as a framework for analyzing
this emergent system, characterizing it along two key dimensions: its origins
(emergent vs. intentional) and its degree of separateness from the established
human economy (permeable vs. impermeable). Our current trajectory points toward
a spontaneous emergence of a vast and highly permeable AI agent economy,
presenting us with opportunities for an unprecedented degree of coordination as
well as significant challenges, including systemic economic risk and
exacerbated inequality. Here we discuss a number of possible design choices
that may lead to safely steerable AI agent markets. In particular, we consider
auction mechanisms for fair resource allocation and preference resolution, the
design of AI "mission economies" to coordinate around achieving collective
goals, and socio-technical infrastructure needed to ensure trust, safety, and
accountability. By doing this, we argue for the proactive design of steerable
agent markets to ensure the coming technological shift aligns with humanity's
long-term collective flourishing.

</details>


### [15] [Online Robust Planning under Model Uncertainty: A Sample-Based Approach](https://arxiv.org/abs/2509.10162)
*Tamir Shazman,Idan Lev-Yehudi,Ron Benchetit,Vadim Indelman*

Main category: cs.AI

TL;DR: 本文提出用于Robust MDPs的在线规划算法Robust Sparse Sampling (RSS)，有有限样本理论性能保证，适用无限或连续状态空间，在不确定动态环境中表现优于标准Sparse Sampling。


<details>
  <summary>Details</summary>
Motivation: 现有基于样本的MDP在线规划方法在生成模型基于有限数据学习时存在近似误差，而Robust MDPs现有方法计算密集不适合实时使用。

Method: 引入RSS算法，利用Sample Average Approximation (SAA)的效率和理论性质计算鲁棒值函数，实现可处理的在线鲁棒策略计算。

Result: 给出理论性能保证，实证表明RSS在不确定动态环境中优于标准Sparse Sampling。

Conclusion: RSS是首个有有限样本理论性能保证的RMDPs在线规划算法，适用于无限或连续状态空间，且样本和计算复杂度与状态空间大小无关。

Abstract: Online planning in Markov Decision Processes (MDPs) enables agents to make
sequential decisions by simulating future trajectories from the current state,
making it well-suited for large-scale or dynamic environments. Sample-based
methods such as Sparse Sampling and Monte Carlo Tree Search (MCTS) are widely
adopted for their ability to approximate optimal actions using a generative
model. However, in practical settings, the generative model is often learned
from limited data, introducing approximation errors that can degrade
performance or lead to unsafe behaviors. To address these challenges, Robust
MDPs (RMDPs) offer a principled framework for planning under model uncertainty,
yet existing approaches are typically computationally intensive and not suited
for real-time use. In this work, we introduce Robust Sparse Sampling (RSS), the
first online planning algorithm for RMDPs with finite-sample theoretical
performance guarantees. Unlike Sparse Sampling, which estimates the nominal
value function, RSS computes a robust value function by leveraging the
efficiency and theoretical properties of Sample Average Approximation (SAA),
enabling tractable robust policy computation in online settings. RSS is
applicable to infinite or continuous state spaces, and its sample and
computational complexities are independent of the state space size. We provide
theoretical performance guarantees and empirically show that RSS outperforms
standard Sparse Sampling in environments with uncertain dynamics.

</details>


### [16] [Towards Fully Automated Molecular Simulations: Multi-Agent Framework for Simulation Setup and Force Field Extraction](https://arxiv.org/abs/2509.10210)
*Marko Petković,Vlado Menkovski,Sofía Calero*

Main category: cs.AI

TL;DR: 提出多智能体框架用于多孔材料自动表征，首推系统用于力场提取和模拟设置，初评效果好。


<details>
  <summary>Details</summary>
Motivation: 多孔材料自动表征受模拟设置和力场选择复杂性限制，需加速材料发现。

Method: 提出多智能体框架，其中基于大语言模型的智能体可自主完成表征任务各环节。

Result: 初始评估显示该方法具有高正确性和可重复性。

Conclusion: 该方法有潜力实现完全自主、可扩展的材料表征。

Abstract: Automated characterization of porous materials has the potential to
accelerate materials discovery, but it remains limited by the complexity of
simulation setup and force field selection. We propose a multi-agent framework
in which LLM-based agents can autonomously understand a characterization task,
plan appropriate simulations, assemble relevant force fields, execute them and
interpret their results to guide subsequent steps. As a first step toward this
vision, we present a multi-agent system for literature-informed force field
extraction and automated RASPA simulation setup. Initial evaluations
demonstrate high correctness and reproducibility, highlighting this approach's
potential to enable fully autonomous, scalable materials characterization.

</details>


### [17] [Compartmentalised Agentic Reasoning for Clinical NLI](https://arxiv.org/abs/2509.10222)
*Maël Jullien,Lei Xu,Marco Valentino,André Freitas*

Main category: cs.AI

TL;DR: 本文研究临床自然语言推理中数据和参数扩展与内部表征的关系，提出CARENLI方法提升推理表现，指出当前主要瓶颈。


<details>
  <summary>Details</summary>
Motivation: 验证在临床自然语言推理中，扩展数据和参数是否能产生更具结构和泛化性的内部表征这一假设。

Method: 采用分解为四个推理家族的基准，引入CARENLI方法，将知识访问与原则性推理分离，通过规划器、验证器和细化器执行可审计程序。

Result: 在四个大语言模型上，CARENLI使保真度最多提高42个点，验证器能高可靠地标记违规，细化器能纠正大量认知错误，剩余失败集中在路由环节。

Conclusion: 大语言模型常保留相关事实，但推理不明确时会采用启发式方法，CARENLI明确了这种分离并提供了更安全、可审计推理的框架。

Abstract: A common assumption holds that scaling data and parameters yields
increasingly structured, generalisable internal representations. We interrogate
this assumption in clinical natural language inference (NLI) by adopting a
benchmark decomposed into four reasoning families, Causal Attribution,
Compositional Grounding, Epistemic Verification, and Risk State Abstraction,
and introducing CARENLI, a Compartmentalised Agentic Reasoning for Clinical NLI
that separates knowledge access from principled inference. CARENLI routes each
premise, statement pair to a family specific solver and enforces auditable
procedures via a planner, verifier, and refiner.
  Across four LLMs, CARENLI improves fidelity by up to 42 points, reaching
98.0% in Causal Attribution and 81.2% in Risk State Abstraction. Verifiers flag
violations with near-ceiling reliability, while refiners correct a substantial
share of epistemic errors. Remaining failures cluster in routing, identifying
family classification as the main bottleneck. These results show that LLMs
often retain relevant facts but default to heuristics when inference is
underspecified, a dissociation CARENLI makes explicit while offering a
framework for safer, auditable reasoning.

</details>


### [18] [Investigating Language Model Capabilities to Represent and Process Formal Knowledge: A Preliminary Study to Assist Ontology Engineering](https://arxiv.org/abs/2509.10249)
*Hanna Abi Akl*

Main category: cs.AI

TL;DR: 研究形式方法对小语言模型推理任务性能的影响，发现可用逻辑语言替代自然语言并保持性能，用于改进小语言模型在本体工程中的作用。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型在推理领域存在不足，影响本体工程等任务，需探究形式方法对小语言模型推理性能的影响。

Method: 开展一系列初步实验，研究用不同语法表达逻辑问题对小语言模型在预定义推理任务中性能的影响。

Result: 可以用更紧凑的逻辑语言替代自然语言，同时在推理任务上保持良好性能。

Conclusion: 有望利用这些结果进一步完善小语言模型在本体工程中的作用。

Abstract: Recent advances in Language Models (LMs) have failed to mask their
shortcomings particularly in the domain of reasoning. This limitation impacts
several tasks, most notably those involving ontology engineering. As part of a
PhD research, we investigate the consequences of incorporating formal methods
on the performance of Small Language Models (SLMs) on reasoning tasks.
Specifically, we aim to orient our work toward using SLMs to bootstrap ontology
construction and set up a series of preliminary experiments to determine the
impact of expressing logical problems with different grammars on the
performance of SLMs on a predefined reasoning task. Our findings show that it
is possible to substitute Natural Language (NL) with a more compact logical
language while maintaining a strong performance on reasoning tasks and hope to
use these results to further refine the role of SLMs in ontology engineering.

</details>


### [19] [The Morality of Probability: How Implicit Moral Biases in LLMs May Shape the Future of Human-AI Symbiosis](https://arxiv.org/abs/2509.10297)
*Eoin O'Doherty,Nicole Weinrauch,Andrew Talone,Uri Klempner,Xiaoyuan Yi,Xing Xie,Yi Zeng*

Main category: cs.AI

TL;DR: 本文研究先进AI系统对道德结果的优先级排序，通过对六个大语言模型实验发现价值偏差，为AI研究作出实证、理论和实践贡献。


<details>
  <summary>Details</summary>
Motivation: 人工智能发展引发如何使机器决策与人类道德价值一致的问题，研究AI系统对道德结果的优先级排序及人类与AI共生前景。

Method: 对六个大语言模型进行定量实验，对代表五种道德框架的18个困境的结果进行排名和评分。

Result: 发现明显一致的价值偏差，关怀和美德价值结果被评为最道德，自由意志主义选择受惩罚；推理模型对上下文更敏感、解释更丰富，非推理模型判断更统一但不透明。

Conclusion: 研究在实证、理论和实践上有贡献，强调可解释性和文化意识是引导AI走向透明、协调和共生未来的关键设计原则。

Abstract: Artificial intelligence (AI) is advancing at a pace that raises urgent
questions about how to align machine decision-making with human moral values.
This working paper investigates how leading AI systems prioritize moral
outcomes and what this reveals about the prospects for human-AI symbiosis. We
address two central questions: (1) What moral values do state-of-the-art large
language models (LLMs) implicitly favour when confronted with dilemmas? (2) How
do differences in model architecture, cultural origin, and explainability
affect these moral preferences? To explore these questions, we conduct a
quantitative experiment with six LLMs, ranking and scoring outcomes across 18
dilemmas representing five moral frameworks. Our findings uncover strikingly
consistent value biases. Across all models, Care and Virtue values outcomes
were rated most moral, while libertarian choices were consistently penalized.
Reasoning-enabled models exhibited greater sensitivity to context and provided
richer explanations, whereas non-reasoning models produced more uniform but
opaque judgments. This research makes three contributions: (i) Empirically, it
delivers a large-scale comparison of moral reasoning across culturally distinct
LLMs; (ii) Theoretically, it links probabilistic model behaviour with
underlying value encodings; (iii) Practically, it highlights the need for
explainability and cultural awareness as critical design principles to guide AI
toward a transparent, aligned, and symbiotic future.

</details>


### [20] [State Algebra for Propositional Logic](https://arxiv.org/abs/2509.10326)
*Dmitry Lesnik,Tobias Schäfer*

Main category: cs.AI

TL;DR: 提出State Algebra框架用代数方法处理命题逻辑，有三种表示层次，具灵活性，可用于多种算法及扩展。


<details>
  <summary>Details</summary>
Motivation: 设计新框架以代数方法表示和处理命题逻辑。

Method: 构建包含Set、Coordinate和Row Decomposition三种表示层次的State Algebra框架，利用代数引擎计算。

Result: 虽默认状态向量约简非规范，但按固定变量顺序可获唯一规范形式，框架在放弃规范保证下获灵活性和紧凑表示。

Conclusion: 该框架能用于搜索和知识编译算法，可自然扩展到概率逻辑和加权模型计数。

Abstract: This paper presents State Algebra, a novel framework designed to represent
and manipulate propositional logic using algebraic methods. The framework is
structured as a hierarchy of three representations: Set, Coordinate, and Row
Decomposition. These representations anchor the system in well-known semantics
while facilitating the computation using a powerful algebraic engine. A key
aspect of State Algebra is its flexibility in representation. We show that
although the default reduction of a state vector is not canonical, a unique
canonical form can be obtained by applying a fixed variable order during the
reduction process. This highlights a trade-off: by foregoing guaranteed
canonicity, the framework gains increased flexibility, potentially leading to
more compact representations of certain classes of problems. We explore how
this framework provides tools to articulate both search-based and knowledge
compilation algorithms and discuss its natural extension to probabilistic logic
and Weighted Model Counting.

</details>


### [21] [Abduct, Act, Predict: Scaffolding Causal Inference for Automated Failure Attribution in Multi-Agent Systems](https://arxiv.org/abs/2509.10401)
*Alva West,Yixuan Weng,Minjun Zhu,Zhen Lin,Yue Zhang*

Main category: cs.AI

TL;DR: 现有多智能体系统故障归因方法准确率低，本文提出A2P框架将故障归因转化为结构化因果推理任务，实验表明其能显著提升准确率。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体系统故障归因方法将其作为模式识别任务，步级准确率低，无法进行鲁棒反事实推理。

Method: 引入Abduct - Act - Predict (A2P) Scaffolding框架，引导大语言模型在单次推理中完成溯因、行动、预测三个步骤。

Result: 在Who&When基准测试中，A2P在算法生成数据集上步级准确率达47.46%，是基线的2.85倍；在手工制作数据集上达29.31%，是基线的2.43倍。

Conclusion: A2P Scaffolding框架通过因果视角重构问题，为自动故障归因提供了更强大、可验证且准确的解决方案。

Abstract: Failure attribution in multi-agent systems -- pinpointing the exact step
where a decisive error occurs -- is a critical yet unsolved challenge. Current
methods treat this as a pattern recognition task over long conversation logs,
leading to critically low step-level accuracy (below 17\%), which renders them
impractical for debugging complex systems. Their core weakness is a fundamental
inability to perform robust counterfactual reasoning: to determine if
correcting a single action would have actually averted the task failure. To
bridge this counterfactual inference gap, we introduce Abduct-Act-Predict (A2P)
Scaffolding, a novel agent framework that transforms failure attribution from
pattern recognition into a structured causal inference task. A2P explicitly
guides a large language model through a formal three-step reasoning process
within a single inference pass: (1) Abduction, to infer the hidden root causes
behind an agent's actions; (2) Action, to define a minimal corrective
intervention; and (3) Prediction, to simulate the subsequent trajectory and
verify if the intervention resolves the failure. This structured approach
leverages the holistic context of the entire conversation while imposing a
rigorous causal logic on the model's analysis. Our extensive experiments on the
Who\&When benchmark demonstrate its efficacy. On the Algorithm-Generated
dataset, A2P achieves 47.46\% step-level accuracy, a 2.85$\times$ improvement
over the 16.67\% of the baseline. On the more complex Hand-Crafted dataset, it
achieves 29.31\% step accuracy, a 2.43$\times$ improvement over the baseline's
12.07\%. By reframing the problem through a causal lens, A2P Scaffolding
provides a robust, verifiable, and significantly more accurate solution for
automated failure attribution.

</details>


### [22] [Mutual Information Tracks Policy Coherence in Reinforcement Learning](https://arxiv.org/abs/2509.10423)
*Cameron Reid,Wael Hafez,Amirhossein Nazeri*

Main category: cs.AI

TL;DR: 提出信息论框架用于强化学习（RL），分析信息特征诊断异常，实现故障定位，为自适应RL系统奠基。


<details>
  <summary>Details</summary>
Motivation: 现实环境中RL代理缺乏检测和诊断故障机制，需方法解决。

Method: 分析机器人控制任务中状态 - 动作互信息模式，进行受控扰动实验。

Result: 成功学习有特征信息签名，互信息变化有规律；信息指标可区分诊断系统故障。

Conclusion: 建立信息模式可作为学习签名和系统健康诊断依据，为自适应RL系统提供基础。

Abstract: Reinforcement Learning (RL) agents deployed in real-world environments face
degradation from sensor faults, actuator wear, and environmental shifts, yet
lack intrinsic mechanisms to detect and diagnose these failures. We present an
information-theoretic framework that reveals both the fundamental dynamics of
RL and provides practical methods for diagnosing deployment-time anomalies.
Through analysis of state-action mutual information patterns in a robotic
control task, we first demonstrate that successful learning exhibits
characteristic information signatures: mutual information between states and
actions steadily increases from 0.84 to 2.83 bits (238% growth) despite growing
state entropy, indicating that agents develop increasingly selective attention
to task-relevant patterns. Intriguingly, states, actions and next states joint
mutual information, MI(S,A;S'), follows an inverted U-curve, peaking during
early learning before declining as the agent specializes suggesting a
transition from broad exploration to efficient exploitation. More immediately
actionable, we show that information metrics can differentially diagnose system
failures: observation-space, i.e., states noise (sensor faults) produces broad
collapses across all information channels with pronounced drops in state-action
coupling, while action-space noise (actuator faults) selectively disrupts
action-outcome predictability while preserving state-action relationships. This
differential diagnostic capability demonstrated through controlled perturbation
experiments enables precise fault localization without architectural
modifications or performance degradation. By establishing information patterns
as both signatures of learning and diagnostic for system health, we provide the
foundation for adaptive RL systems capable of autonomous fault detection and
policy adjustment based on information-theoretic principles.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [23] [Superstructure Optimization with Embedded Neural Networks for Sustainable Aviation Fuel Production](https://arxiv.org/abs/2509.09796)
*Alexander Klimek,Christoph Plate,Sebastian Sager,Kai Sundmacher,Caroline Ganzer*

Main category: cs.CE

TL;DR: 提出可持续航空燃料生产多目标优化框架，应用于费托煤油生产，分析不同配置成本和排放，强调过程适应性作用并做敏感性分析。


<details>
  <summary>Details</summary>
Motivation: 解决传统超结构方法在可持续航空燃料生产优化中的关键局限，实现离散过程选择和连续操作参数的同时优化。

Method: 将人工神经网络集成到混合整数二次约束规划公式中，嵌入数据驱动的代理模型到数学优化结构。

Result: 无碳排放约束时化石基自热重整路线成本最低；有碳排放约束需整合生物质气化和直接空气捕获与碳封存；零排放时混合配置成本最低；利用灵活参数的配置表现更优，可达20%成本节约。

Conclusion: 所提框架有效，过程适应性在可持续航空燃料生产优化中起关键作用，敏感性分析有助于了解过程条件影响。

Abstract: This study presents a multi-objective optimization framework for sustainable
aviation fuel (SAF) production, integrating artificial neural networks (ANNs)
within a mixed-integer quadratically constrained programming (MIQCP)
formulation. By embedding data-driven surrogate models into the mathematical
optimization structure, the proposed methodology addresses key limitations of
conventional superstructure-based approaches, enabling simultaneous
optimization of discrete process choices and continuous operating parameters.
The framework captures variable input and output stream compositions,
facilitating the joint optimization of target product composition and system
design. Application to Fischer-Tropsch (FT) kerosene production demonstrates
that cost-minimizing configurations under unconstrained CO2 emissions are
dominated by the fossil-based autothermal reforming (ATR) route. Imposing
carbon emission constraints necessitates the integration of biomass
gasification and direct air capture coupled with carbon sequestration (DAC-CS),
resulting in substantially reduced net emissions but higher production costs.
At the zero-emission limit, hybrid configurations combining ATR and biomass
gasification achieve the lowest costs (~2.38 \$/kg-kerosene), followed closely
by biomass gasification-only (~2.43 \$/kg), both of which outperform the
ATR-only pathway with DAC-CS (~2.65 \$/kg). In contrast, DAC-only systems
relying exclusively on atmospheric CO2 and water electrolysis are prohibitively
expensive (~10.8 \$/kg). The results highlight the critical role of process
adaptability: configurations exploiting flexible process parameters,
facilitated by embedded ANNs, consistently outperform fixed setups, achieving
up to 20% cost savings. Sensitivity analyses elucidate the influence of process
conditions, such as FT reactor pressure and gasification temperature, on
economic and environmental performance.

</details>


### [24] [Fraud detection and risk assessment of online payment transactions on e-commerce platforms based on LLM and GCN frameworks](https://arxiv.org/abs/2509.09928)
*RuiHan Luo,Nanxi Wang,Xiaotong Zhu*

Main category: cs.CE

TL;DR: 研究提出结合大语言模型和图卷积网络的电商在线支付欺诈检测框架，实验证明其准确性达0.98，为在线支付安全提供可扩展实时方案。


<details>
  <summary>Details</summary>
Motivation: 电商发展使在线支付欺诈复杂，传统检测方法难捕捉交易数据关系结构，威胁金融安全和消费者信任。

Method: 收集含284万笔交易的高度不平衡数据集，将消费者和商家作为节点、交易作为边构建异构图，用GCN学习行为模式，融合GPT - 4o和Tabformer提取的语义特征与结构特征。

Result: 所提模型准确率达0.98，能有效平衡欺诈检测的精确率和敏感度。

Conclusion: 该框架为保障在线支付环境提供可扩展实时解决方案，为基于图的深度学习在金融欺诈预防应用提供方向。

Abstract: With the rapid growth of e-commerce, online payment fraud has become
increasingly complex, posing serious threats to financial security and consumer
trust. Traditional detection methods often struggle to capture the intricate
relational structures inherent in transactional data. This study presents a
novel fraud detection framework that combines Large Language Models (LLM) with
Graph Convolutional Networks (GCN) to effectively identify fraudulent
activities in e-commerce online payment transactions. A dataset of 2,840,000
transactions was collected over 14 days from major platforms such as Amazon,
involving approximately 2,000 U.S.-based consumers and 30 merchants. With fewer
than 6000 fraudulent instances, the dataset represents a highly imbalanced
scenario. Consumers and merchants were modeled as nodes and transactions as
edges to form a heterogeneous graph, upon which a GCN was applied to learn
complex behavioral patterns. Semantic features extracted via GPT-4o and
Tabformer were integrated with structural features to enhance detection
performance. Experimental results demonstrate that the proposed model achieves
an accuracy of 0.98, effectively balancing precision and sensitivity in fraud
detection. This framework offers a scalable and real-time solution for securing
online payment environments and provides a promising direction for applying
graph-based deep learning in financial fraud prevention.

</details>


### [25] [QuantAgent: Price-Driven Multi-Agent LLMs for High-Frequency Trading](https://arxiv.org/abs/2509.09995)
*Fei Xiong,Xiang Zhang,Aosong Feng,Siqi Sun,Chenyu You*

Main category: cs.CE

TL;DR: 介绍首个专为高频算法交易设计的多智能体大语言模型框架QuantAgent，经评估表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体大语言模型框架不适用于高频交易的高速、高精度需求，需开发新框架。

Method: 将交易分解为四个专业智能体，每个智能体配备特定工具和推理能力以捕捉短期市场动态。

Result: 在十种金融工具的零样本评估中，QuantAgent在预测准确性和4小时交易区间累计回报方面表现优于强大的神经和基于规则的基线。

Conclusion: 将结构化金融先验与语言原生推理相结合，为高频金融市场的可追溯实时决策系统带来新潜力。

Abstract: Recent advances in Large Language Models (LLMs) have demonstrated impressive
capabilities in financial reasoning and market understanding. Multi-agent LLM
frameworks such as TradingAgent and FINMEM augment these models to long-horizon
investment tasks, leveraging fundamental and sentiment-based inputs for
strategic decision-making. However, such systems are ill-suited for the
high-speed, precision-critical demands of High-Frequency Trading (HFT). HFT
requires rapid, risk-aware decisions based on structured, short-horizon
signals, including technical indicators, chart patterns, and trend-based
features, distinct from the long-term semantic reasoning typical of traditional
financial LLM applications. To this end, we introduce QuantAgent, the first
multi-agent LLM framework explicitly designed for high-frequency algorithmic
trading. The system decomposes trading into four specialized agents, Indicator,
Pattern, Trend, and Risk, each equipped with domain-specific tools and
structured reasoning capabilities to capture distinct aspects of market
dynamics over short temporal windows. In zero-shot evaluations across ten
financial instruments, including Bitcoin and Nasdaq futures, QuantAgent
demonstrates superior performance in both predictive accuracy and cumulative
return over 4-hour trading intervals, outperforming strong neural and
rule-based baselines. Our findings suggest that combining structured financial
priors with language-native reasoning unlocks new potential for traceable,
real-time decision systems in high-frequency financial markets.

</details>


### [26] [Optimizing Freight Rail Electrification: A Framework for Charge Station Selection and Battery Charge/Swap Scheduling](https://arxiv.org/abs/2509.10157)
*Jia Guo,Elnaz Irannezhad*

Main category: cs.CE

TL;DR: 研究提出电池电动货运列车充电基础设施和调度的综合模型，用三种算法求解，实验表明Benders分解算法最优。


<details>
  <summary>Details</summary>
Motivation: 电池电动货运列车对脱碳至关重要，其有效应用依赖高效的电池电气化策略。

Method: 建立混合整数线性规划模型，提出矩形分段线性逼近技术、固定算法启发式和Benders分解算法三种算法求解。

Result: 在计算实验中用三种算法求解最多25个站点的实例，统计分析表明Benders分解算法在目标函数值上表现最优。

Conclusion: Benders分解算法在解决该优化问题上优于其他两种算法。

Abstract: Battery electric freight trains are crucial for decarbonization by providing
zero-emission transportation alternatives. The proper adoption of battery
electric freight trains depends on an efficient battery electrification
strategy, involving both infrastructure setup and charge scheduling. The study
presents a comprehensive model for the optimal design of charging
infrastructure and charge scheduling for each train. To provide more refueling
flexibility, we allow batteries to be either charged or swapped in a deployed
station, and each train can carry multiple batteries. This problem is
formulated as a mixed integer linear programming model. To obtain real-time
solutions for a large scale network, we develop three algorithms to solve the
optimization problem: (1) a Rectangle Piecewise Linear Approximation technique,
(2) a Fixed Algorithm heuristic, and (3) Benders Decomposition algorithm. In
computational experiments, we use the three proposed algorithms to solve
instances with up to 25 stations. Statistical analysis verifies that Benders
Decomposition outperforms the other two algorithms with respect to the
objective function value, closely followed by the Rectangle Piecewise Linear
Approximation technique, and the Fixed Algorithm provides the least optimal
solution.

</details>


### [27] [Discovering Flow Separation Control Strategies in 3D Wings via Deep Reinforcement Learning](https://arxiv.org/abs/2509.10185)
*R. Montalà,B. Font,P. Suárez,J. Rabault,O. Lehmkuhl,R. Vinuesa,I. Rodriguez*

Main category: cs.CE

TL;DR: 本文将深度强化学习应用于三维SD7003机翼的主动流动控制，取得提升升力、降低阻力和提高气动效率的效果。


<details>
  <summary>Details</summary>
Motivation: 探索深度强化学习在复杂湍流主动流动控制中的应用。

Method: 使用GPU加速的CFD求解器和多智能体训练。

Result: 发现的控制策略可提升升力79%、降低阻力65%、提高气动效率408%，流动可视化证实分离剪切层重新附着。

Conclusion: 证明了深度强化学习在复杂湍流控制方面的潜力。

Abstract: In this work, deep reinforcement learning (DRL) is applied to active flow
control (AFC) over a threedimensional SD7003 wing at a Reynolds number of Re =
60,000 and angle of attack of AoA = 14 degrees. In the uncontrolled baseline
case, the flow exhibits massive separation and a fully turbulent wake. Using a
GPU-accelerated CFD solver and multi-agent training, DRL discovers control
strategies that enhance lift (79%), reduce drag (65%), and improve aerodynamic
efficiency (408%). Flow visualizations confirm reattachment of the separated
shear layer, demonstrating the potential of DRL for complex and turbulent
flows.

</details>


### [28] [Deep Reinforcement Learning for Active Flow Control around a Three-Dimensional Flow-Separated Wing at Re = 1,000](https://arxiv.org/abs/2509.10195)
*R. Montalà,B. Font,P. Suárez,J. Rabault,O. Lehmkuhl,R. Vinuesa,I. Rodriguez*

Main category: cs.CE

TL;DR: 本文探索用深度强化学习（DRL）进行主动流动控制（AFC）以减少大攻角机翼气流分离。


<details>
  <summary>Details</summary>
Motivation: 利用DRL解决复杂空气动力学挑战，突破传统AFC方法局限。

Method: DRL代理通过实时流数据和奖励函数自主识别最优控制动作，用Redis内存数据库集成GPU加速CFD求解器SOD2D和TF - Agents DRL库进行快速训练。

Result: 未明确提及具体结果。

Conclusion: 证明了DRL在解决复杂空气动力学挑战和推动传统AFC方法边界方面的潜力。

Abstract: This study explores the use of deep reinforcement learning (DRL) for active
flow control (AFC) to reduce flow separation on wings at high angles of attack.
Concretely, here the DRL agent controls the flow over the three-dimensional
NACA0012 wing section at the Reynolds number Re = 1,000 and angle of attack AoA
= 20 degrees, autonomously identifying optimal control actions through
real-time flow data and a reward function focused on improving aerodynamic
performance. The framework integrates the GPU-accelerated computational fluid
dynamics (CFD) solver SOD2D with the TF-Agents DRL library via a Redis
in-memory database, enabling rapid training. This work builds on previous DRL
flow-control studies, demonstrating DRL potential to address complex
aerodynamic challenges and push the boundaries of traditional AFC methods.

</details>


### [29] [TubeBEND: A Real-World Dataset for Geometry Prediction in Rotary Draw Bending](https://arxiv.org/abs/2509.10272)
*Zeyneddin Oz,Jonas Knoche,Alireza Yazdani,Bernd Engel,Kristof Van Laerhoven*

Main category: cs.CE

TL;DR: 本文介绍了TubeBEND数据集，包含318个旋转管弯曲过程，可用于评估机器学习和信号分析方法，能助力优化管材回弹和变形，数据集公开。


<details>
  <summary>Details</summary>
Motivation: 应对工业中预测第一阶段弯曲几何形状的挑战，寻找替代传统方法的解决方案。

Method: 收集和整理来自各领域专家的318个旋转管弯曲过程数据，记录几何标准和过程参数。

Result: 构建和测试可预测几何形状的机器学习模型，提供过程参数对最终管材几何形状影响的详细信息。

Conclusion: 数据集公开可用，可作为基准改进该领域的数据驱动方法。

Abstract: This paper presents TubeBEND, a real-world dataset comprising 318 rotary tube
bending processes, which were collected and sorted by experts from various
fields to evaluate machine learning and signal analysis methods. The dataset
addresses the industrial challenge of predicting the geometry of a first-stage
bend, which can be beneficial for designing machine clamping molds for the
second-stage bend in two-stage rotary draw bending. Some geometry criteria,
such as the tube's final bent angle (or springback) and its cross-sectional
deformation, are being recorded in this dataset. This dataset gives us the
possibility to build and test machine learning models that can predict the
geometry and help the machine operators with a better machine setup to optimize
the tube's springback and deformation. Moreover, by recording some process
parameters, such as tool movements and forces or torques applied to them, we
deliver detailed information about their impacts on the final tube geometry.
The focus of our work is to discover solutions that can replace traditional
methods, such as trial-and-error or simulation-based predictions, by including
experimental process variables in ML algorithms. Our dataset is publicly
available at https://github.com/zeyneddinoz/tubebend and
https://zenodo.org/records/16614082 as a benchmark to improve data-driven
methods in this field.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [30] [Space-Time Tradeoffs for Spatial Conjunctive Queries](https://arxiv.org/abs/2509.10050)
*Aryan Esmailpour,Xiao Hu,Stavros Sintos*

Main category: cs.DB

TL;DR: 本文旨在构建时空高效的索引来回答空间连接查询，建立了查询时间和空间使用的下界，构造了最优索引并扩展到任意连接查询，还展示了其对关系算法运行时间的改进。


<details>
  <summary>Details</summary>
Motivation: 现有方法在查询时间或空间使用上效率低，需要构建时空高效的索引来回答空间连接查询。

Method: 建立查询时间和空间使用的下界，构造k - star和k - path查询的最优索引，利用广义超树分解扩展到任意连接查询。

Result: 得出k - star和k - path查询的索引空间下界，构造了最优索引并扩展到分层查询和任意连接查询。

Conclusion: 新索引可用于改进关系设置中已知算法的运行时间。

Abstract: Given a conjunctive query and a database instance, we aim to develop an index
that can efficiently answer spatial queries on the results of a conjunctive
query. We are interested in some commonly used spatial queries, such as range
emptiness, range count, and nearest neighbor queries. These queries have
essential applications in data analytics, such as filtering relational data
based on attribute ranges and temporal graph analysis for counting graph
structures like stars, paths, and cliques. Furthermore, this line of research
can accelerate relational algorithms that incorporate spatial queries in their
workflow, such as relational clustering. Known approaches either have to spend
$\tilde{O}(N)$ query time or use space as large as the number of query results,
which are inefficient or unrealistic to employ in practice. Hence, we aim to
construct an index that answers spatial conjunctive queries in both time- and
space-efficient ways.
  In this paper, we establish lower bounds on the tradeoff between answering
time and space usage. For $k$-star (resp. $k$-path) queries, we show that any
index for range emptiness, range counting or nearest neighbor queries with $T$
answering time requires $\Omega\left(N+\frac{N^k}{T^k}\right)$ (resp.
$\Omega\left(N+\frac{N^2}{T^{2/(k-1)}}\right)$) space. Then, we construct
optimal indexes for answering range emptiness and range counting problems over
$k$-star and $k$-path queries. Extending this result, we build an index for
hierarchical queries. By resorting to the generalized hypertree decomposition,
we can extend our index to arbitrary conjunctive queries for supporting spatial
conjunctive queries. Finally, we show how our new indexes can be used to
improve the running time of known algorithms in the relational setting.

</details>


### [31] [Semi-interval Comparison Constraints in Query Containment and Their Impact on Certain Answer Computation](https://arxiv.org/abs/2509.10138)
*Foto N. Afrati,Matthew Damigos*

Main category: cs.DB

TL;DR: 研究带算术比较的合取查询（CQAC）的查询包含问题和确定答案计算复杂度，发现部分情况问题可在NP内解决，部分仍为Π₂ᵖ - 完全，还证明最大包含重写可计算所有确定答案及部分情况能多项式时间计算。


<details>
  <summary>Details</summary>
Motivation: 探究CQAC查询包含问题的计算复杂度以及使用任何CQAC视图回答带半区间比较的CQAC查询时计算确定答案的复杂度。

Method: 理论分析与证明。

Result: 对于CQAC查询包含问题，包含查询为特定半区间算术比较类时问题可在NP内解决，部分简单情况仍为Π₂ᵖ - 完全；最大包含重写能计算所有确定答案，部分情况可多项式时间计算确定答案。

Conclusion: 不同类型的CQAC查询在包含问题和确定答案计算复杂度上有不同表现，最大包含重写在计算确定答案方面有重要作用。

Abstract: We consider conjunctive queries with arithmetic comparisons (CQAC) and
investigate the computational complexity of the problem: Given two CQAC
queries, $Q$ and $Q'$, is $Q'$ contained in $Q$? We know that, for CQAC
queries, the problem of testing containment is $\Pi_2 ^p$ -complete. However,
there are broad classes of queries with semi-interval arithmetic comparisons in
the containing query that render the problem solvable in NP. In all cases
examined the contained query is allowed to be any CQAC. Interestingly, we also
prove that there are simple cases where the problem remains $\Pi_2 ^p$
-complete.
  We also investigate the complexity of computing certain answers in the
framework of answering CQAC queries with semi-interval comparisons using any
CQAC views. We prove that maximally contained rewritings in the language of
union of CQACs always compute exactly all certain answers. We find cases where
we can compute certain answers in polynomial time using maximally contained
rewritings.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [32] [Setchain Algorithms for Blockchain Scalability](https://arxiv.org/abs/2509.09795)
*Arivarasan Karmegam,Gabina Luz Bianchi,Margarita Capretto,Martín Ceresa,Antonio Fernández Anta,César Sánchez*

Main category: cs.DC

TL;DR: 本文提出并评估三种基于底层区块链账本的Setchain算法，在CometBFT平台实现并测试，结果显示算法吞吐量远高于底层区块链，且能在4秒内达成最终性。


<details>
  <summary>Details</summary>
Motivation: 通过放宽交易间严格全序要求，提高区块链可扩展性。

Method: 提出Vanilla、Compresschain和Hashchain三种Setchain算法，维护epoch-proof让轻客户端安全交互，在CometBFT平台实现算法。

Result: Setchain算法吞吐量比底层区块链高几个数量级，最终性延迟低于4秒。

Conclusion: 所提出的Setchain算法有效提升了区块链的可扩展性，在吞吐量和延迟方面表现良好。

Abstract: Setchain has been proposed to increase blockchain scalability by relaxing the
strict total order requirement among transactions. Setchain organizes elements
into a sequence of sets, referred to as epochs, so that elements within each
epoch are unordered. In this paper, we propose and evaluate three distinct
Setchain algorithms, that leverage an underlying block-based ledger. Vanilla is
a basic implementation that serves as a reference point. Compresschain
aggregates elements into batches, and compresses these batches before appending
them as epochs in the ledger. Hashchain converts batches into fixed-length
hashes which are appended as epochs in the ledger. This requires Hashchain to
use a distributed service to obtain the batch contents from its hash. To allow
light clients to safely interact with only one server, the proposed algorithms
maintain, as part of the Setchain, proofs for the epochs. An epoch-proof is the
hash of the epoch, cryptographically signed by a server. A client can verify
the correctness of an epoch with $f+1$ epoch-proofs (where $f$ is the maximum
number of Byzantine servers assumed). All three Setchain algorithms are
implemented on top of the CometBFT blockchain application platform. We
conducted performance evaluations across various configurations, using clusters
of four, seven, and ten servers. Our results show that the Setchain algorithms
reach orders of magnitude higher throughput than the underlying blockchain, and
achieve finality with latency below 4 seconds.

</details>


### [33] [Ordered Consensus with Equal Opportunity](https://arxiv.org/abs/2509.09868)
*Yunhao Zhang,Haobin Ni,Soumya Basu,Shir Cohen,Maofan Yin,Lorenzo Alvisi,Robbert van Renesse,Qi Chen,Lidong Zhou*

Main category: cs.DC

TL;DR: 本文针对基于状态机复制（SMR）的区块链中排序操纵问题，将有序共识扩展为支持公平性的平等机会，引入秘密随机预言机（SRO），并在新协议Bercow中实例化以缓解排序攻击。


<details>
  <summary>Details</summary>
Motivation: 基于SMR的区块链中命令排序影响客户经济回报，但现实中存在非拜占庭节点的排序操纵问题，需要解决公平性问题。

Method: 将有序共识扩展为支持平等机会，引入SRO生成容错随机数，设计基于可信硬件和阈值可验证随机函数的两种SRO，并在Bercow协议中实例化。

Result: 提出了两种SRO设计，并在Bercow协议中实现，可在可配置范围内近似平等机会。

Conclusion: Bercow协议通过近似平等机会，能有效缓解基于SMR的区块链中已知的排序攻击。

Abstract: The specification of state machine replication (SMR) has no requirement on
the final total order of commands. In blockchains based on SMR, however, order
matters, since different orders could provide their clients with different
financial rewards. Ordered consensus augments the specification of SMR to
include specific guarantees on such order, with a focus on limiting the
influence of Byzantine nodes. Real-world ordering manipulations, however, can
and do happen even without Byzantine replicas, typically because of factors,
such as faster networks or closer proximity to the blockchain infrastructure,
that give some clients an unfair advantage. To address this challenge, this
paper proceeds to extend ordered consensus by requiring it to also support
equal opportunity, a concrete notion of fairness, widely adopted in social
sciences. Informally, equal opportunity requires that two candidates who,
according to a set of criteria deemed to be relevant, are equally qualified for
a position (in our case, a specific slot in the SMR total order), should have
an equal chance of landing it. We show how randomness can be leveraged to keep
bias in check, and, to this end, introduce the secret random oracle (SRO), a
system component that generates randomness in a fault-tolerant manner. We
describe two SRO designs based, respectively, on trusted hardware and threshold
verifiable random functions, and instantiate them in Bercow, a new ordered
consensus protocol that, by approximating equal opportunity up to within a
configurable factor, can effectively mitigate well-known ordering attacks in
SMR-based blockchains.

</details>


### [34] [Characterizing the Efficiency of Distributed Training: A Power, Performance, and Thermal Perspective](https://arxiv.org/abs/2509.10371)
*Seokjin Go,Joongun Park,Spandan More,Hanjiang Wu,Irene Wang,Aaron Jezghani,Tushar Krishna,Divya Mahajan*

Main category: cs.DC

TL;DR: 本文对大语言模型（LLM）在不同工作负载和硬件平台上的训练进行全面表征，分析不同并行策略和优化方法，揭示训练性能受多因素影响，并给出系统和硬件设计建议。


<details>
  <summary>Details</summary>
Motivation: 大语言模型训练工作负载超出单节点分析限制，需深入了解其在大规模多GPU系统中的行为。

Method: 对不同硬件平台（NVIDIA H100/H200和AMD MI250 GPUs）上的密集和稀疏模型，采用多种并行策略（张量、流水线、数据和专家并行）进行分析，并评估优化方法的有效性。

Result: 性能并非仅由硬件容量决定；特定并行组合会导致带宽利用率低；增加微批次大小会引发执行问题和热节流。

Conclusion: 提出系统和硬件设计建议，以提高未来LLM系统和工作负载的可扩展性和可靠性。

Abstract: The rapid scaling of Large Language Models (LLMs) has pushed training
workloads far beyond the limits of single-node analysis, demanding a deeper
understanding of how these models behave across large-scale, multi-GPU systems.
In this paper, we present a comprehensive characterization of LLM training
across diverse real-world workloads and hardware platforms, including NVIDIA
H100/H200 and AMD MI250 GPUs. We analyze dense and sparse models under various
parallelism strategies -- tensor, pipeline, data, and expert -- and evaluate
their effects on hardware utilization, power consumption, and thermal behavior.
We further evaluate the effectiveness of optimizations such as activation
recomputation and compute-communication overlap. Our findings show that
performance is not determined solely by scaling hardware capacity. Scale-up
systems with fewer, higher-memory GPUs can outperform scale-out systems in
communication-bound regimes, but only under carefully tuned configurations; in
other cases, scale-out deployments achieve superior throughput. We also show
that certain parallelism combinations, such as tensor with pipeline, lead to
bandwidth underutilization due to inefficient data chunking, while increasing
microbatch sizes beyond a certain point induces bursty execution and peak power
excursions that worsen thermal throttling. These insights reveal how training
performance is shaped by complex interactions between hardware, system
topology, and model execution. We conclude by offering recommendations for
system and hardware design to improve the scalability and reliability of future
LLM systems and workloads. The source code of this project is available at
https://github.com/sitar-lab/CharLLM-PPT.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [35] [Approximate Graph Propagation Revisited: Dynamic Parameterized Queries, Tighter Bounds and Dynamic Updates](https://arxiv.org/abs/2509.10036)
*Zhuowei Zhao,Zhuo Zhang,Hanzhi Wang,Junhao Gan,Zhifeng Bao,Jianzhong Qi*

Main category: cs.DS

TL;DR: 重新审视AGP框架，聚焦动态图和动态参数化查询场景，指出AGP - Static问题，提出AGP - Static++和AGP - Dynamic算法，实验验证算法有显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有SOTA方案AGP - Static在动态图和动态参数化查询场景存在查询复杂度高和处理图更新慢的问题，需改进。

Method: 提出AGP - Static++算法降低查询复杂度，再提出AGP - Dynamic算法实现每次更新的摊销时间为O(1)。

Result: 实验表明相比基线，算法在更新时间上加速达177倍，查询效率加速达10倍。

Conclusion: 提出的AGP - Static++和AGP - Dynamic算法有效解决了AGP - Static的问题，提升了动态图和动态参数化查询场景下的性能。

Abstract: We revisit Approximate Graph Propagation (AGP), a unified framework which
captures various graph propagation tasks, such as PageRank, feature propagation
in Graph Neural Networks (GNNs), and graph-based Retrieval-Augmented Generation
(RAG). Our work focuses on the settings of dynamic graphs and dynamic
parameterized queries, where the underlying graphs evolve over time (updated by
edge insertions or deletions) and the input query parameters are specified on
the fly to fit application needs. Our first contribution is an interesting
observation that the SOTA solution, AGP-Static, can be adapted to support
dynamic parameterized queries; however several challenges remain unresolved.
Firstly, the query time complexity of AGP-Static is based on an assumption of
using an optimal algorithm for subset sampling in its query algorithm.
Unfortunately, back to that time, such an algorithm did not exist; without such
an optimal algorithm, an extra $O(\log^2 n)$ factor is required in the query
complexity, where $n$ is the number of vertices in the graphs. Secondly,
AGP-Static performs poorly on dynamic graphs, taking $O(n\log n)$ time to
process each update. To address these challenges, we propose a new algorithm,
AGP-Static++, which is simpler yet reduces roughly a factor of $O(\log^2 n)$ in
the query complexity while preserving the approximation guarantees of
AGP-Static. However, AGP-Static++ still requires $O(n)$ time to process each
update. To better support dynamic graphs, we further propose AGP-Dynamic, which
achieves $O(1)$ amortized time per update, significantly improving the
aforementioned $O(n)$ per-update bound, while still preserving the query
complexity and approximation guarantees. Last, our comprehensive experiments
validate the theoretical improvements: compared to the baselines, our algorithm
achieves speedups of up to $177\times$ on update time and $10\times$ on query
efficiency.

</details>


### [36] [Constant Time with Minimal Preprocessing, a Robust and Extensive Complexity Class](https://arxiv.org/abs/2509.10188)
*Étienne Grandjean,Louis Jachiet*

Main category: cs.DS

TL;DR: 研究操作类cstPP，证明其具有鲁棒性、广泛性和多种闭包性质，探讨预处理时间对其的影响


<details>
  <summary>Details</summary>
Motivation: 研究满足特定性质的操作类cstPP的特性

Method: 理论证明，分析不同原始操作集和预处理时间下的情况

Result: cstPP类具有多种闭包性质，常数时间过程可简化，预处理时间在一定范围内变化不影响该类，减少到N^{o(1)}会退化

Conclusion: cstPP类具有良好性质，预处理时间对其有特定影响

Abstract: In this paper, we study the class $\mathtt{cstPP}$ of operations
$\mathtt{op}: \mathbb{N}^k\to\mathbb{N}$, of any fixed arity $k\ge 1$,
satisfying the following property: for each fixed integer $d\ge 1$, there
exists an algorithm for a RAM machine which, for any input integer $N\ge 2$, -
pre-computes some tables in $O(N)$ time, - then reads $k$ operands
$x_1,\ldots,x_k<N^d$ and computes $\mathtt{op}(x_1,\dots,x_k)$ in constant
time.
  We show that the $\mathtt{cstPP}$ class is robust and extensive and satisfies
several closure properties. It is invariant depending on whether the set of
primitive operations of the RAM is $\{+\}$, or
$\{+,-,\times,\mathtt{div},\mathtt{mod}\}$, or any set of operations in
$\mathtt{cstPP}$ provided it includes $+$. We prove that the $\mathtt{cstPP}$
class is closed under composition and, for fast-growing functions, is closed
under inverse. We also show that in the definition of $\mathtt{cstPP}$ the
constant-time procedure can be reduced to a single return instruction. Finally,
we establish that linear preprocessing time is not essential in the definition
of the $\mathtt{cstPP}$ class: this class is not modified if the preprocessing
time is increased to $O(N^c)$, for any fixed $c>1$, or conversely, is reduced
to $N^{\varepsilon}$, for any positive $\varepsilon<1$ (provided the set of
primitive operation includes $+$, $\mathtt{div}$ and $\mathtt{mod}$). To
complete the picture, we demonstrate that the $\mathtt{cstPP}$ class
degenerates if the preprocessing time reduces to $N^{o(1)}$.

</details>


### [37] [A linear-time algorithm for Chow decompositions](https://arxiv.org/abs/2509.10450)
*Alexander Taveira Blomenhofer,Benjamin Lovitz*

Main category: cs.DS

TL;DR: 提出线性时间算法计算低秩Chow分解，还开发高阶Chow分解及特定3 - 张量Chow分解的次二次时间算法。


<details>
  <summary>Details</summary>
Motivation: 开发计算Chow分解的高效算法。

Method: 提出基于铅笔的线性时间算法，依赖广义特征值计算；开发次二次时间算法。

Result: 实现了对特定对称3 - 张量的Chow分解，得到分解对称3 - 张量为W - 张量线性组合的次二次时间算法。

Conclusion: 成功提出高效算法用于不同类型的Chow分解。

Abstract: We propose a linear-time algorithm to compute low-rank Chow decompositions.
Our algorithm can decompose concise symmetric 3-tensors in n variables of Chow
rank n/3. The algorithm is pencil based, hence it relies on generalized
eigenvalue computations. We also develop sub-quadratic time algorithms for
higher order Chow decompositions, and Chow decompositions of 3-tensors into
products of linear forms which do not lie on the generic orbit. In particular,
we obtain a sub-quadratic-time algorithm for decomposing a symmetric 3-tensor
into a linear combination of W-tensors.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [38] [Evolution of Coordination Through Institutional Incentives: An Evolutionary Game Theory Approach](https://arxiv.org/abs/2509.10112)
*Ndidi Bianca Ogbo,Zhao Song,The Anh Han*

Main category: cs.GT

TL;DR: 本文开发进化博弈论模型研究制度激励预算分配，发现奖励比惩罚激励更能促进协调，资源合理分配有最优结果，为新技术制度激励设计提供见解。


<details>
  <summary>Details</summary>
Motivation: 现有合作与承诺的应用分析和政策讨论多为定性，对稀缺制度资源在促进参与和确保承诺合规间的分配关注不足。

Method: 开发进化博弈论模型，研究预承诺框架下有限预算在促进参与和确保承诺合规这两个目标上的战略分配。

Result: 基于奖励的激励方法比基于惩罚的方法更能促进协调成功，资源在促进参与和确保合规间合理分配时有最优结果。

Conclusion: 研究结果为设计制度激励以促进新技术的广泛协调采用提供了新见解。

Abstract: There is a broad recognition that commitment-based mechanisms can promote
coordination and cooperative behaviours in both biological populations and
self-organised multi-agent systems by making individuals' intentions explicit
prior to engagement. Yet their effectiveness depends on sustained compliance
supported by institutions, especially in one-off interactions. Despite advances
in quantitative studies of cooperation and commitment, most applied analyses
and policy debates remain largely qualitative, with limited attention to the
allocation of scarce institutional resources between enhancing participation
and ensuring commitment compliance. Herein, we develop an evolutionary
game-theoretic model that explicitly examines the strategic distribution of a
limited budget for institutional incentives, namely rewards or punishments,
aimed at these two critical objectives within pre-commitment frameworks. Our
findings reveal that a reward-based incentive approach consistently yields
greater coordination success than a punishment-based approach, with optimal
outcomes arising when resources are appropriately distributed between
participation promotion and compliance assurance. These findings offer novel
insights for designing institutional incentives to promote broad, coordinated
adoption of new technologies.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [39] [DB3 Team's Solution For Meta KDD Cup' 25](https://arxiv.org/abs/2509.09681)
*Yikuan Xia,Jiazun Chen,Yirui Zhan,Suifeng Zhao,Weipeng Jiang,Chaorui Zhang,Wei Han,Bo Bai,Jun Gao*

Main category: cs.IR

TL;DR: 本文介绍db3团队在KDD Cup'25上Meta CRAG - MM挑战赛的获胜方案，阐述了框架与方法，系统取得优异成绩获大奖。


<details>
  <summary>Details</summary>
Motivation: 应对Meta CRAG - MM挑战赛的多模态、多轮问答基准挑战。

Method: 开发综合框架，整合不同任务的定制检索管道和统一的大语言模型微调方法控制幻觉，包括特定领域检索管道和先进的拒绝训练。

Result: 系统在任务1、2获第2名，任务3获第1名，凭借出色处理第一人称视角挑战获得以自我为中心查询的卓越大奖。

Conclusion: 所提出的框架和方法在Meta CRAG - MM挑战赛中取得了显著成效。

Abstract: This paper presents the db3 team's winning solution for the Meta CRAG-MM
Challenge 2025 at KDD Cup'25. Addressing the challenge's unique multi-modal,
multi-turn question answering benchmark (CRAG-MM), we developed a comprehensive
framework that integrates tailored retrieval pipelines for different tasks with
a unified LLM-tuning approach for hallucination control. Our solution features
(1) domain-specific retrieval pipelines handling image-indexed knowledge
graphs, web sources, and multi-turn conversations; and (2) advanced refusal
training using SFT, DPO, and RL. The system achieved 2nd place in Task 1, 2nd
place in Task 2, and 1st place in Task 3, securing the grand prize for
excellence in ego-centric queries through superior handling of first-person
perspective challenges.

</details>


### [40] [Faster and Memory-Efficient Training of Sequential Recommendation Models for Large Catalogs](https://arxiv.org/abs/2509.09682)
*Maxim Zhelnin,Dmitry Redko,Volkov Daniil,Anna Volodkevich,Petr Sokerin,Valeriy Shevchenko,Egor Shvetsov,Alexey Vasilev,Darya Denisova,Ruslan Izmailov,Alexey Zaytsev*

Main category: cs.IR

TL;DR: 本文指出基于Transformer的序列推荐模型训练计算成本高、内存消耗大的问题，提出CCE-方法，可加速训练并减少内存消耗，还分析了关键超参数。


<details>
  <summary>Details</summary>
Motivation: 解决基于Transformer的序列推荐模型训练时因交叉熵损失导致的高计算成本和高内存消耗问题。

Method: 引入CCE-方法，实现GPU高效的带负采样的交叉熵损失，还发布Triton内核实现该方法。

Result: CCE-方法可将训练加速达两倍，减少内存消耗超10倍，在大物品目录数据集上可提高模型准确性。

Conclusion: 需要在关键内存相关超参数间进行微妙平衡，同时缩放负样本数量和批量大小效果更好。

Abstract: Sequential recommendations (SR) with transformer-based architectures are
widely adopted in real-world applications, where SR models require frequent
retraining to adapt to ever-changing user preferences. However, training
transformer-based SR models often encounters a high computational cost
associated with scoring extensive item catalogs, often exceeding thousands of
items. This occurs mainly due to the use of cross-entropy loss, where peak
memory scales proportionally to catalog size, batch size, and sequence length.
Recognizing this, practitioners in the field of recommendation systems
typically address memory consumption by integrating the cross-entropy (CE) loss
with negative sampling, thereby reducing the explicit memory demands of the
final layer. However, a small number of negative samples would degrade model
performance, and as we demonstrate in our work, increasing the number of
negative samples and the batch size further improves the model's performance,
but rapidly starts to exceed industrial GPUs' size (~40Gb).
  In this work, we introduce the CCE- method, which offers a GPU-efficient
implementation of the CE loss with negative sampling. Our method accelerates
training by up to two times while reducing memory consumption by more than 10
times. Leveraging the memory savings afforded by using CCE- for model training,
it becomes feasible to enhance its accuracy on datasets with a large item
catalog compared to those trained with original PyTorch-implemented loss
functions. Finally, we perform an analysis of key memory-related
hyperparameters and highlight the necessity of a delicate balance among these
factors. We demonstrate that scaling both the number of negative samples and
batch size leads to better results rather than maximizing only one of them. To
facilitate further adoption of CCE-, we release a Triton kernel that
efficiently implements the proposed method.

</details>


### [41] [Text-to-SQL Oriented to the Process Mining Domain: A PT-EN Dataset for Query Translation](https://arxiv.org/abs/2509.09684)
*Bruno Yui Yamate,Thais Rodrigues Neubauer,Marcelo Fantinato,Sarajane Marques Peres*

Main category: cs.IR

TL;DR: 本文介绍了面向过程挖掘领域文本到SQL任务的双语（葡萄牙语 - 英语）基准数据集text - 2 - SQL - 4 - PM，展示其构建方法和基线研究成果。


<details>
  <summary>Details</summary>
Motivation: 文本到SQL转换可提高数据库查询的可及性和效率，现有数据集无法满足过程挖掘领域独特挑战，因此构建text - 2 - SQL - 4 - PM数据集。

Method: 采用专家手动策划、专业翻译和详细注释流程构建数据集，并使用GPT - 3.5 Turbo进行基线研究。

Result: text - 2 - SQL - 4 - PM数据集支持文本到SQL实现的评估，对语义解析和其他自然语言处理任务有更广泛适用性。

Conclusion: text - 2 - SQL - 4 - PM数据集对过程挖掘领域文本到SQL任务及相关自然语言处理任务有积极作用。

Abstract: This paper introduces text-2-SQL-4-PM, a bilingual (Portuguese-English)
benchmark dataset designed for the text-to-SQL task in the process mining
domain. Text-to-SQL conversion facilitates natural language querying of
databases, increasing accessibility for users without SQL expertise and
productivity for those that are experts. The text-2-SQL-4-PM dataset is
customized to address the unique challenges of process mining, including
specialized vocabularies and single-table relational structures derived from
event logs. The dataset comprises 1,655 natural language utterances, including
human-generated paraphrases, 205 SQL statements, and ten qualifiers. Methods
include manual curation by experts, professional translations, and a detailed
annotation process to enable nuanced analyses of task complexity. Additionally,
a baseline study using GPT-3.5 Turbo demonstrates the feasibility and utility
of the dataset for text-to-SQL applications. The results show that
text-2-SQL-4-PM supports evaluation of text-to-SQL implementations, offering
broader applicability for semantic parsing and other natural language
processing tasks.

</details>


### [42] [Forecasting Clicks in Digital Advertising: Multimodal Inputs and Interpretable Outputs](https://arxiv.org/abs/2509.09683)
*Briti Gangopadhyay,Zhao Wang,Shingo Takamatsu*

Main category: cs.IR

TL;DR: 提出结合点击数据与文本日志的多模态预测框架，用强化学习提升性能，实验显示优于基线。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列模型预测点击量时忽略文本上下文信息，需更好方法。

Method: 构建多模态预测框架，结合点击数据与文本日志，用强化学习理解文本信息和融合模态。

Result: 在大规模行业数据集实验中，该方法在准确性和推理质量上优于基线。

Conclusion: 所提出的多模态预测框架能有效用于点击量预测，提升预测性能。

Abstract: Forecasting click volume is a key task in digital advertising, influencing
both revenue and campaign strategy. Traditional time series models rely solely
on numerical data, often overlooking rich contextual information embedded in
textual elements, such as keyword updates. We present a multimodal forecasting
framework that combines click data with textual logs from real-world ad
campaigns and generates human-interpretable explanations alongside numeric
predictions. Reinforcement learning is used to improve comprehension of textual
information and enhance fusion of modalities. Experiments on a large-scale
industry dataset show that our method outperforms baselines in both accuracy
and reasoning quality.

</details>


### [43] [Wave-Based Semantic Memory with Resonance-Based Retrieval: A Phase-Aware Alternative to Vector Embedding Stores](https://arxiv.org/abs/2509.09691)
*Aleksandr Listopad*

Main category: cs.IR

TL;DR: 提出基于波的语义记忆框架，能保留幅度和相位信息，检索有更高判别力，实现可扩展。


<details>
  <summary>Details</summary>
Motivation: 传统基于向量的内存系统对相位不敏感，捕捉意义表征的共振现象能力有限。

Method: 提出Wave - Based Semantic Memory框架，将知识建模为波模式并通过基于共振的干扰检索。

Result: 基于共振的检索在向量方法失效的情况下有更高判别力，ResonanceDB可扩展到数百万模式且延迟低。

Conclusion: 基于波的内存可作为面向AGI推理和知识表示的向量存储的可行替代方案。

Abstract: Conventional vector-based memory systems rely on cosine or inner product
similarity within real-valued embedding spaces. While computationally
efficient, such approaches are inherently phase-insensitive and limited in
their ability to capture resonance phenomena crucial for meaning
representation. We propose Wave-Based Semantic Memory, a novel framework that
models knowledge as wave patterns $\psi(x) = A(x) e^{i\phi(x)}$ and retrieves
it through resonance-based interference. This approach preserves both amplitude
and phase information, enabling more expressive and robust semantic similarity.
We demonstrate that resonance-based retrieval achieves higher discriminative
power in cases where vector methods fail, including phase shifts, negations,
and compositional queries. Our implementation, ResonanceDB, shows scalability
to millions of patterns with millisecond latency, positioning wave-based memory
as a viable alternative to vector stores for AGI-oriented reasoning and
knowledge representation.

</details>


### [44] [TalkPlayData 2: An Agentic Synthetic Data Pipeline for Multimodal Conversational Music Recommendation](https://arxiv.org/abs/2509.09685)
*Keunwoo Choi,Seungheon Doh,Juhan Nam*

Main category: cs.IR

TL;DR: 介绍TalkPlayData 2合成数据集，说明其生成方式、特点，实验达标并开源。


<details>
  <summary>Details</summary>
Motivation: 生成用于多模态对话式音乐推荐的合成数据集，以训练生成式音乐推荐模型。

Method: 采用代理数据管道，创建不同角色的大语言模型代理，记录对话数据，为Listener LLM设置对话目标，使用多模态LLM模拟推荐和对话。

Result: 在LLM评判和主观评估实验中，TalkPlayData 2在训练生成式音乐推荐模型的各方面达到预期目标。

Conclusion: TalkPlayData 2数据集及其生成代码已开源，可用于多模态对话式音乐推荐模型训练。

Abstract: We present TalkPlayData 2, a synthetic dataset for multimodal conversational
music recommendation generated by an agentic data pipeline. In TalkPlayData 2
pipeline, multiple large language model (LLM) agents are created under various
roles with specialized prompts and access to different parts of information,
and the chat data is acquired by logging the conversation between the Listener
LLM and the Recsys LLM. To cover various conversation scenarios, for each
conversation, the Listener LLM is conditioned on a finetuned conversation goal.
Finally, all the LLMs are multimodal with audio and images, allowing a
simulation of multimodal recommendation and conversation. In the LLM-as-a-judge
and subjective evaluation experiments, TalkPlayData 2 achieved the proposed
goal in various aspects related to training a generative recommendation model
for music. TalkPlayData 2 and its generation code are open-sourced at
https://talkpl.ai/talkplaydata2.html.

</details>


### [45] [GeoGPT.RAG Technical Report](https://arxiv.org/abs/2509.09686)
*Fei Huang,Fan Wu,Zeqing Zhang,Qihao Wang,Long Zhang,Grant Michael Boquet,Hongyang Chen*

Main category: cs.IR

TL;DR: GeoGPT是用于地球科学研究的开源大语言模型，集成RAG，利用专业语料库生成答案，用户可创建知识库，经模型微调优化RAG，还开源核心组件。


<details>
  <summary>Details</summary>
Motivation: 推动地球科学研究，提升模型在地球科学领域的特定能力。

Method: 集成RAG，从GeoGPT Library获取信息；允许用户创建个性化知识库；微调嵌入模型和排序模型。

Result: 优化了RAG在地球科学应用中的表现，显著提升系统输出的准确性和可信度。

Conclusion: GeoGPT体现了对开放科学的承诺，开源核心组件为全球相关人员提供强大易用的AI工具。

Abstract: GeoGPT is an open large language model system built to advance research in
the geosciences. To enhance its domain-specific capabilities, we integrated
Retrieval Augmented Generation(RAG), which augments model outputs with relevant
information retrieved from an external knowledge source. GeoGPT uses RAG to
draw from the GeoGPT Library, a specialized corpus curated for geoscientific
content, enabling it to generate accurate, context-specific answers. Users can
also create personalized knowledge bases by uploading their own publication
lists, allowing GeoGPT to retrieve and respond using user-provided materials.
To further improve retrieval quality and domain alignment, we fine-tuned both
the embedding model and a ranking model that scores retrieved passages by
relevance to the query. These enhancements optimize RAG for geoscience
applications and significantly improve the system's ability to deliver precise
and trustworthy outputs. GeoGPT reflects a strong commitment to open science
through its emphasis on collaboration, transparency, and community driven
development. As part of this commitment, we have open-sourced two core RAG
components-GeoEmbedding and GeoReranker-to support geoscientists, researchers,
and professionals worldwide with powerful, accessible AI tools.

</details>


### [46] [Demonstrating Narrative Pattern Discovery from Biomedical Literature](https://arxiv.org/abs/2509.09687)
*Hermann Kroll,Pascal Sackhoff,Bill Matthias Thang,Christin Katharina Kreutz,Wolf-Tilo Balke*

Main category: cs.IR

TL;DR: 本文介绍数字图书馆PubPharm，引入新搜索功能叙事模式挖掘，并通过专家访谈验证原型有用性。


<details>
  <summary>Details</summary>
Motivation: 数字图书馆需为用户提供有效访问路径，现有功能基础上需新搜索功能。

Method: 引入叙事模式挖掘新搜索功能，对五位领域专家进行访谈。

Result: 未提及具体结果，但进行专家访谈验证原型。

Conclusion: 未明确给出结论，但暗示新功能有一定价值。

Abstract: Digital libraries maintain extensive collections of knowledge and need to
provide effective access paths for their users. For instance, PubPharm, the
specialized information service for Pharmacy in Germany, provides and develops
access paths to their underlying biomedical document collection. In brief,
PubPharm supports traditional keyword-based search, search for chemical
structures, as well as novel graph-based discovery workflows, e.g., listing or
searching for interactions between different pharmaceutical entities. This
paper introduces a new search functionality, called narrative pattern mining,
allowing users to explore context-relevant entities and entity interactions. We
performed interviews with five domain experts to verify the usefulness of our
prototype.

</details>


### [47] [AI-Powered Assistant for Long-Term Access to RHIC Knowledge](https://arxiv.org/abs/2509.09688)
*Mohammad Atif,Vincent Garonne,Eric Lancon,Jerome Lauret,Alexandr Prozorov,Michal Vranovsky*

Main category: cs.IR

TL;DR: RHIC结束25年运行，其数据和分析保存计划引入AI助手系统助力数据利用，展示现代AI/ML工具对科学遗留数据的作用。


<details>
  <summary>Details</summary>
Motivation: RHIC结束25年运行，需保存大量数据和科学知识，支持数据可重复性、教育和未来发现。

Method: 基于检索增强生成和模型上下文协议的大语言模型，对RHIC实验的结构化和非结构化内容进行索引，实现领域自适应交互。

Result: 报告了系统的部署、计算性能、多实验集成和架构特性。

Conclusion: 现代AI/ML工具可提升科学遗留数据的可用性和可发现性。

Abstract: As the Relativistic Heavy Ion Collider (RHIC) at Brookhaven National
Laboratory concludes 25 years of operation, preserving not only its vast data
holdings ($\sim$1 ExaByte) but also the embedded scientific knowledge becomes a
critical priority. The RHIC Data and Analysis Preservation Plan (DAPP)
introduces an AI-powered assistant system that provides natural language access
to documentation, workflows, and software, with the aim of supporting
reproducibility, education, and future discovery. Built upon Large Language
Models using Retrieval-Augmented Generation and the Model Context Protocol,
this assistant indexes structured and unstructured content from RHIC
experiments and enables domain-adapted interaction. We report on the
deployment, computational performance, ongoing multi-experiment integration,
and architectural features designed for a sustainable and explainable long-term
AI access. Our experience illustrates how modern AI/ML tools can transform the
usability and discoverability of scientific legacy data.

</details>


### [48] [Personas within Parameters: Fine-Tuning Small Language Models with Low-Rank Adapters to Mimic User Behaviors](https://arxiv.org/abs/2509.09689)
*Himanshu Thakur,Eshani Agrawal,Smruthi Mukund*

Main category: cs.IR

TL;DR: 提出用冻结的LLM提取用户文本表示，微调的SLM模拟用户代理，训练低秩适配器平衡可扩展性和性能，实验证明方法有效。


<details>
  <summary>Details</summary>
Motivation: 开发准确推荐模型时模拟用户行为面临挑战，现有LLM与用户偏好对齐存在问题。

Method: 用冻结的LLM提取稳健的文本用户表示，用微调的SLM模拟低成本、资源高效的用户代理，训练多组低秩适配器。

Result: 实验表明该方法有效，开发的用户代理有潜力缩小推荐系统离线指标和实际性能差距。

Conclusion: 所提方法能有效模拟用户行为，提升推荐系统性能。

Abstract: A long-standing challenge in developing accurate recommendation models is
simulating user behavior, mainly due to the complex and stochastic nature of
user interactions. Towards this, one promising line of work has been the use of
Large Language Models (LLMs) for simulating user behavior. However, aligning
these general-purpose large pre-trained models with user preferences
necessitates: (i) effectively and continously parsing large-scale tabular
user-item interaction data, (ii) overcoming pre-training-induced inductive
biases to accurately learn user specific knowledge, and (iii) achieving the
former two at scale for millions of users. While most previous works have
focused on complex methods to prompt an LLM or fine-tune it on tabular
interaction datasets, our approach shifts the focus to extracting robust
textual user representations using a frozen LLM and simulating cost-effective,
resource-efficient user agents powered by fine-tuned Small Language Models
(SLMs). Further, we showcase a method for training multiple low-rank adapters
for groups of users or \textit{persona}, striking an optimal balance between
scalability and performance of user behavior agents. Our experiments provide
compelling empirical evidence of the efficacy of our methods, demonstrating
that user agents developed using our approach have the potential to bridge the
gap between offline metrics and real-world performance of recommender systems.

</details>


### [49] [Powering Job Search at Scale: LLM-Enhanced Query Understanding in Job Matching Systems](https://arxiv.org/abs/2509.09690)
*Ping Liu,Jianqiang Shen,Qianqi Shen,Chunnan Yao,Kevin Kao,Dan Xu,Rajat Arora,Baofen Zheng,Caleb Johnson,Liangjie Hong,Jingwei Wu,Wenjing Zhang*

Main category: cs.IR

TL;DR: 提出基于大语言模型的统一查询理解框架，提升相关性质量、降低系统复杂度和运营开销。


<details>
  <summary>Details</summary>
Motivation: 传统查询理解方法依赖多任务特定命名实体识别模型，架构脆弱、维护成本高且适应变化慢，需改进。

Method: 引入由大语言模型驱动的统一查询理解框架，联合建模用户查询和上下文信号生成结构化解释。

Result: 在在线A/B测试中提高了相关性质量，显著降低系统复杂度和运营开销。

Conclusion: 该解决方案为动态Web应用中的查询理解提供了可扩展和适应性强的基础。

Abstract: Query understanding is essential in modern relevance systems, where user
queries are often short, ambiguous, and highly context-dependent. Traditional
approaches often rely on multiple task-specific Named Entity Recognition models
to extract structured facets as seen in job search applications. However, this
fragmented architecture is brittle, expensive to maintain, and slow to adapt to
evolving taxonomies and language patterns. In this paper, we introduce a
unified query understanding framework powered by a Large Language Model (LLM),
designed to address these limitations. Our approach jointly models the user
query and contextual signals such as profile attributes to generate structured
interpretations that drive more accurate and personalized recommendations. The
framework improves relevance quality in online A/B testing while significantly
reducing system complexity and operational overhead. The results demonstrate
that our solution provides a scalable and adaptable foundation for query
understanding in dynamic web applications.

</details>


### [50] [A Research Vision for Web Search on Emerging Topics](https://arxiv.org/abs/2509.10212)
*Alisa Rieger,Stefan Dietze,Ran Yu*

Main category: cs.IR

TL;DR: 论文提出面向新兴主题搜索系统和界面的研究愿景，提出三个研究问题，指出相关文献及可能解决办法，还讨论潜在挑战。


<details>
  <summary>Details</summary>
Motivation: 新兴主题信息稀疏、动态变化，质量和可信度不确定，易被操纵、含错误信息和偏见，需要支持有效知识获取等的搜索系统和界面。

Method: 提出三个研究问题以实现研究愿景，针对每个问题突出相关文献及解决思路。

Result: 提出了研究愿景和三个研究问题，明确了相关文献。

Conclusion: 探讨了追求该研究愿景可能出现的挑战。

Abstract: We regularly encounter information on novel, emerging topics for which the
body of knowledge is still evolving, which can be linked, for instance, to
current events. A primary way to learn more about such topics is through web
search. However, information on emerging topics is sparse and evolves
dynamically as knowledge grows, making it uncertain and variable in quality and
trustworthiness and prone to deliberate or accidental manipulation,
misinformation, and bias. In this paper, we outline a research vision towards
search systems and interfaces that support effective knowledge acquisition,
awareness of the dynamic nature of topics, and responsible opinion formation
among people searching the web for information on emerging topics. To realize
this vision, we propose three overarching research questions, aimed at
understanding the status quo, determining requirements of systems aligned with
our vision, and building these systems. For each of the three questions, we
highlight relevant literature, including pointers on how they could be
addressed. Lastly, we discuss the challenges that will potentially arise in
pursuing the proposed vision.

</details>


### [51] [Model-agnostic post-hoc explainability for recommender systems](https://arxiv.org/abs/2509.10245)
*Irina Arévalo,Jose L Salmeron*

Main category: cs.IR

TL;DR: 研究在推荐系统中应用删除诊断方法，对NCF和SVD模型进行实验，证明方法跨范式的通用性。


<details>
  <summary>Details</summary>
Motivation: 现有推荐系统使用复杂特征嵌入和深度学习算法提升性能，但降低了可解释性和透明度，需新方法解决。

Method: 开发删除诊断方法，对比含特定用户或物品与不含的模型性能，量化其对推荐器的影响，并应用于NCF和SVD。

Result: 在MovieLens和Amazon Reviews数据集上实验，深入了解模型行为。

Conclusion: 该方法具有模型无关性，在不同推荐范式中具有通用性。

Abstract: Recommender systems often benefit from complex feature embeddings and deep
learning algorithms, which deliver sophisticated recommendations that enhance
user experience, engagement, and revenue. However, these methods frequently
reduce the interpretability and transparency of the system. In this research,
we develop a systematic application, adaptation, and evaluation of deletion
diagnostics in the recommender setting. The method compares the performance of
a model to that of a similar model trained without a specific user or item,
allowing us to quantify how that observation influences the recommender, either
positively or negatively. To demonstrate its model-agnostic nature, the
proposal is applied to both Neural Collaborative Filtering (NCF), a widely used
deep learning-based recommender, and Singular Value Decomposition (SVD), a
classical collaborative filtering technique. Experiments on the MovieLens and
Amazon Reviews datasets provide insights into model behavior and highlight the
generality of the approach across different recommendation paradigms.

</details>


### [52] [Diversified recommendations of cultural activities with personalized determinantal point processes](https://arxiv.org/abs/2509.10392)
*Carole Ibrahim,Hiba Bederina,Daniel Cuesta,Laurent Montier,Cyrille Delabre,Jill-Jênn Vie*

Main category: cs.IR

TL;DR: 研究用个性化DPP抽样生成多样且相关推荐，评估相关性与多样性权衡，开源代码。


<details>
  <summary>Details</summary>
Motivation: 优化推荐系统时有效多样化推荐且不影响核心业务指标是行业挑战，拓宽受众文化实践。

Method: 使用个性化Determinantal Point Processes (DPPs) 抽样，利用相似核的质量 - 多样性分解赋予用户偏好更多权重。

Result: 评估了相关性和多样性之间的权衡，为从业者提供在生产环境中使用的见解。

Conclusion: 未明确提及结论，但通过评估和提供见解，对使用个性化DPP抽样多样化推荐有积极意义。

Abstract: While optimizing recommendation systems for user engagement is a
well-established practice, effectively diversifying recommendations without
negatively impacting core business metrics remains a significant industry
challenge. In line with our initiative to broaden our audience's cultural
practices, this study investigates using personalized Determinantal Point
Processes (DPPs) to sample diverse and relevant recommendations. We rely on a
well-known quality-diversity decomposition of the similarity kernel to give
more weight to user preferences. In this paper, we present our implementations
of the personalized DPP sampling, evaluate the trade-offs between relevance and
diversity through both offline and online metrics, and give insights for
practitioners on their use in a production environment. For the sake of
reproducibility, we release the full code for our platform and experiments on
GitHub.

</details>


### [53] [RecoWorld: Building Simulated Environments for Agentic Recommender Systems](https://arxiv.org/abs/2509.10397)
*Fei Liu,Xinyu Lin,Hanchao Yu,Mingyuan Wu,Jianyu Wang,Qiang Zhang,Zhuokai Zhao,Yinglong Xia,Yao Zhang,Weiwei Li,Mingze Gao,Qifan Wang,Lizhu Zhang,Benyu Zhang,Xiangjun Fan*

Main category: cs.IR

TL;DR: 介绍RecoWorld，为代理推荐系统构建模拟环境，有双视图架构，支持多智能体模拟，迈向用户与代理协作的推荐系统。


<details>
  <summary>Details</summary>
Motivation: 为代理推荐系统提供无影响真实用户的训练空间，提升用户留存和参与度。

Method: 采用双视图架构，模拟用户与代理推荐器多轮交互，利用大语言模型推理能力，探索多种内容表示和多轮强化学习。

Result: 构建RecoWorld，支持多智能体模拟，形成动态反馈循环。

Conclusion: RecoWorld是迈向用户与代理共同塑造个性化信息流推荐系统的重要一步，有望实现新交互范式。

Abstract: We present RecoWorld, a blueprint for building simulated environments
tailored to agentic recommender systems. Such environments give agents a proper
training space where they can learn from errors without impacting real users.
RecoWorld distinguishes itself with a dual-view architecture: a simulated user
and an agentic recommender engage in multi-turn interactions aimed at
maximizing user retention. The user simulator reviews recommended items,
updates its mindset, and when sensing potential user disengagement, generates
reflective instructions. The agentic recommender adapts its recommendations by
incorporating these user instructions and reasoning traces, creating a dynamic
feedback loop that actively engages users. This process leverages the
exceptional reasoning capabilities of modern LLMs. We explore diverse content
representations within the simulator, including text-based, multimodal, and
semantic ID modeling, and discuss how multi-turn RL enables the recommender to
refine its strategies through iterative interactions. RecoWorld also supports
multi-agent simulations, allowing creators to simulate the responses of
targeted user populations. It marks an important first step toward recommender
systems where users and agents collaboratively shape personalized information
streams. We envision new interaction paradigms where "user instructs,
recommender responds," jointly optimizing user retention and engagement.

</details>


### [54] [MatSKRAFT: A framework for large-scale materials knowledge extraction from scientific tables](https://arxiv.org/abs/2509.10448)
*Kausik Hira,Mohd Zaki,Mausam,N. M. Anoop Krishnan*

Main category: cs.IR

TL;DR: 提出MatSKRAFT框架自动提取和整合材料科学知识，性能超现有模型，构建数据库助力材料发现。


<details>
  <summary>Details</summary>
Motivation: 多数实验数据以半结构化格式存在，难以系统提取和分析，阻碍跨文献知识合成。

Method: 将表格转换为基于图的表示，用约束驱动的GNN处理，将科学原理编码到模型架构中。

Result: MatSKRAFT性能显著优于现有大语言模型，F1分数高，处理速度快，构建含超535,000条条目的数据库。

Conclusion: 该系统方法能揭示新的材料，推动基于数据的成分 - 属性关系发现。

Abstract: Scientific progress increasingly depends on synthesizing knowledge across
vast literature, yet most experimental data remains trapped in semi-structured
formats that resist systematic extraction and analysis. Here, we present
MatSKRAFT, a computational framework that automatically extracts and integrates
materials science knowledge from tabular data at unprecedented scale. Our
approach transforms tables into graph-based representations processed by
constraint-driven GNNs that encode scientific principles directly into model
architecture. MatSKRAFT significantly outperforms state-of-the-art large
language models, achieving F1 scores of 88.68 for property extraction and 71.35
for composition extraction, while processing data $19$-$496\times$ faster than
them (compared to the slowest and the fastest models, respectively) with modest
hardware requirements. Applied to nearly 69,000 tables from more than 47,000
research publications, we construct a comprehensive database containing over
535,000 entries, including 104,000 compositions that expand coverage beyond
major existing databases, pending manual validation. This systematic approach
reveals previously overlooked materials with distinct property combinations and
enables data-driven discovery of composition-property relationships forming the
cornerstone of materials and scientific discovery.

</details>


### [55] [Generative Engine Optimization: How to Dominate AI Search](https://arxiv.org/abs/2509.08919)
*Mahe Chen,Xiaoxuan Wang,Kaiwen Chen,Nick Koudas*

Main category: cs.IR

TL;DR: 随着生成式AI搜索引擎兴起，传统SEO需转变为GEO。本文对比AI搜索和传统搜索，发现AI搜索有信息来源偏向，据此提出GEO战略议程。


<details>
  <summary>Details</summary>
Motivation: 生成式AI搜索引擎改变信息检索方式，挑战传统SEO，需建立新的优化范式GEO。

Method: 通过多领域、多语言、多查询表述的大规模对照实验，量化AI搜索和传统搜索在信息来源上的差异。

Result: AI搜索更偏向第三方权威来源，不同AI搜索服务在领域多样性、新鲜度等方面有显著差异。

Conclusion: 基于实验结果制定GEO战略议程，为从业者提供行动指导，助力在新搜索环境中提升可见度。

Abstract: The rapid adoption of generative AI-powered search engines like ChatGPT,
Perplexity, and Gemini is fundamentally reshaping information retrieval, moving
from traditional ranked lists to synthesized, citation-backed answers. This
shift challenges established Search Engine Optimization (SEO) practices and
necessitates a new paradigm, which we term Generative Engine Optimization
(GEO).
  This paper presents a comprehensive comparative analysis of AI Search and
traditional web search (Google). Through a series of large-scale, controlled
experiments across multiple verticals, languages, and query paraphrases, we
quantify critical differences in how these systems source information. Our key
findings reveal that AI Search exhibit a systematic and overwhelming bias
towards Earned media (third-party, authoritative sources) over Brand-owned and
Social content, a stark contrast to Google's more balanced mix. We further
demonstrate that AI Search services differ significantly from each other in
their domain diversity, freshness, cross-language stability, and sensitivity to
phrasing.
  Based on these empirical results, we formulate a strategic GEO agenda. We
provide actionable guidance for practitioners, emphasizing the critical need
to: (1) engineer content for machine scannability and justification, (2)
dominate earned media to build AI-perceived authority, (3) adopt
engine-specific and language-aware strategies, and (4) overcome the inherent
"big brand bias" for niche players. Our work provides the foundational
empirical analysis and a strategic framework for achieving visibility in the
new generative search landscape.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [56] [Structure Matters: Brain Graph Augmentation via Learnable Edge Masking for Data-efficient Psychiatric Diagnosis](https://arxiv.org/abs/2509.09744)
*Mujie Liu,Chenze Wang,Liping Chen,Nguyen Linh Dan Le,Niharika Tewari,Ting Dang,Jiangang Ma,Feng Xia*

Main category: cs.LG

TL;DR: 提出SAM - BG框架用于学习脑图表示，解决脑网络数据标注有限下的精神诊断问题，实验显示其性能佳且有临床可解释性。


<details>
  <summary>Details</summary>
Motivation: 标注的脑网络数据有限，现有自监督学习方法会破坏脑图关键结构语义，难以实现准确且可解释的精神诊断。

Method: 提出两阶段框架SAM - BG，预训练阶段用边缘掩码器在小标注子集上捕获关键结构语义，自监督学习阶段用提取的结构先验指导结构感知增强过程。

Result: 在两个真实世界精神疾病数据集上实验表明，SAM - BG在小标注数据设置下优于现有方法，且发现增强可解释性的临床相关连接模式。

Conclusion: SAM - BG是一种有效的学习脑图表示的方法，能提高精神诊断的准确性和可解释性。

Abstract: The limited availability of labeled brain network data makes it challenging
to achieve accurate and interpretable psychiatric diagnoses. While
self-supervised learning (SSL) offers a promising solution, existing methods
often rely on augmentation strategies that can disrupt crucial structural
semantics in brain graphs. To address this, we propose SAM-BG, a two-stage
framework for learning brain graph representations with structural semantic
preservation. In the pre-training stage, an edge masker is trained on a small
labeled subset to capture key structural semantics. In the SSL stage, the
extracted structural priors guide a structure-aware augmentation process,
enabling the model to learn more semantically meaningful and robust
representations. Experiments on two real-world psychiatric datasets demonstrate
that SAM-BG outperforms state-of-the-art methods, particularly in small-labeled
data settings, and uncovers clinically relevant connectivity patterns that
enhance interpretability. Our code is available at
https://github.com/mjliu99/SAM-BG.

</details>


### [57] [D-CAT: Decoupled Cross-Attention Transfer between Sensor Modalities for Unimodal Inference](https://arxiv.org/abs/2509.09747)
*Leen Daher,Zhaobo Wang,Malcolm Mielle*

Main category: cs.LG

TL;DR: 提出D - CAT框架解决跨模态迁移学习在推理时需配对传感器数据的问题，在多模态人类活动数据集上评估显示能提升性能并减少硬件冗余。


<details>
  <summary>Details</summary>
Motivation: 现有跨模态迁移学习方法在训练和推理时都需要配对传感器数据，限制了其在资源受限环境中的部署。

Method: 提出D - CAT框架，结合自注意力模块进行特征提取和新颖的交叉注意力对齐损失，在推理时无需联合传感器模态。

Result: 在分布内场景，从高性能模态迁移可使F1分数提升达10%；分布外场景，弱源模态也能提升目标性能。

Conclusion: D - CAT能在保持准确性的同时减少感知系统的硬件冗余，适用于成本敏感或自适应部署场景。

Abstract: Cross-modal transfer learning is used to improve multi-modal classification
models (e.g., for human activity recognition in human-robot collaboration).
However, existing methods require paired sensor data at both training and
inference, limiting deployment in resource-constrained environments where full
sensor suites are not economically and technically usable. To address this, we
propose Decoupled Cross-Attention Transfer (D-CAT), a framework that aligns
modality-specific representations without requiring joint sensor modality
during inference. Our approach combines a self-attention module for feature
extraction with a novel cross-attention alignment loss, which enforces the
alignment of sensors' feature spaces without requiring the coupling of the
classification pipelines of both modalities. We evaluate D-CAT on three
multi-modal human activity datasets (IMU, video, and audio) under both
in-distribution and out-of-distribution scenarios, comparing against uni-modal
models. Results show that in in-distribution scenarios, transferring from
high-performing modalities (e.g., video to IMU) yields up to 10% F1-score gains
over uni-modal training. In out-of-distribution scenarios, even weaker source
modalities (e.g., IMU to video) improve target performance, as long as the
target model isn't overfitted on the training data. By enabling single-sensor
inference with cross-modal knowledge, D-CAT reduces hardware redundancy for
perception systems while maintaining accuracy, which is critical for
cost-sensitive or adaptive deployments (e.g., assistive robots in homes with
variable sensor availability). Code is available at
https://github.com/Schindler-EPFL-Lab/D-CAT.

</details>


### [58] [Meta-Learning Reinforcement Learning for Crypto-Return Prediction](https://arxiv.org/abs/2509.09751)
*Junqiao Wang,Zhaoyang Guan,Guanyu Liu,Tianze Xia,Xianzhi Li,Shuo Yin,Xinyuan Song,Chuhan Cheng,Tianyu Shi,Alex Lee*

Main category: cs.LG

TL;DR: 提出Meta - RL - Crypto架构统一元学习和强化学习创建交易代理，无需额外人工监督，实验显示性能良好且优于其他基线。


<details>
  <summary>Details</summary>
Motivation: 加密货币回报预测困难，价格受多种因素驱动且标注训练数据稀缺昂贵。

Method: 提出Meta - RL - Crypto架构，基于Transformer，从指令微调大语言模型开始，代理在闭环架构中交替扮演三种角色，利用多模态市场输入和内部偏好反馈。

Result: 在不同市场制度的实验中，Meta - RL - Crypto在真实市场技术指标上表现良好，优于其他基于大语言模型的基线。

Conclusion: Meta - RL - Crypto架构在加密货币交易预测方面有较好效果。

Abstract: Predicting cryptocurrency returns is notoriously difficult: price movements
are driven by a fast-shifting blend of on-chain activity, news flow, and social
sentiment, while labeled training data are scarce and expensive. In this paper,
we present Meta-RL-Crypto, a unified transformer-based architecture that
unifies meta-learning and reinforcement learning (RL) to create a fully
self-improving trading agent. Starting from a vanilla instruction-tuned LLM,
the agent iteratively alternates between three roles-actor, judge, and
meta-judge-in a closed-loop architecture. This learning process requires no
additional human supervision. It can leverage multimodal market inputs and
internal preference feedback. The agent in the system continuously refines both
the trading policy and evaluation criteria. Experiments across diverse market
regimes demonstrate that Meta-RL-Crypto shows good performance on the technical
indicators of the real market and outperforming other LLM-based baselines.

</details>


### [59] [LAVa: Layer-wise KV Cache Eviction with Dynamic Budget Allocation](https://arxiv.org/abs/2509.09754)
*Yiqun Shen,Song Yuan,Zhengze Zhang,Xiaoliang Wang,Daxin Jiang,Nguyen Cam-Tu*

Main category: cs.LG

TL;DR: 介绍统一缓存压缩框架LAVa，无需训练或多策略组合，实验显示其优越性能并揭示不同任务中动态预算作用。


<details>
  <summary>Details</summary>
Motivation: 现有KV Cache压缩方法多为启发式且缺乏动态预算分配，需改进。

Method: 通过最小化Transformer残差流信息损失构建统一框架，分析层注意力输出损失得新指标，实现层和头的动态预算分配。

Result: 在多个基准测试中表现优越，发现不同任务中动态层和头预算的关键作用。

Conclusion: LAVa作为全动态压缩方法，在各类任务中保持顶级性能。

Abstract: KV Cache is commonly used to accelerate LLM inference with long contexts, yet
its high memory demand drives the need for cache compression. Existing
compression methods, however, are largely heuristic and lack dynamic budget
allocation. To address this limitation, we introduce a unified framework for
cache compression by minimizing information loss in Transformer residual
streams. Building on it, we analyze the layer attention output loss and derive
a new metric to compare cache entries across heads, enabling layer-wise
compression with dynamic head budgets. Additionally, by contrasting cross-layer
information, we also achieve dynamic layer budgets. LAVa is the first unified
strategy for cache eviction and dynamic budget allocation that, unlike prior
methods, does not rely on training or the combination of multiple strategies.
Experiments with benchmarks (LongBench, Needle-In-A-Haystack, Ruler, and
InfiniteBench) demonstrate its superiority. Moreover, our experiments reveal a
new insight: dynamic layer budgets are crucial for generation tasks (e.g., code
completion), while dynamic head budgets play a key role in extraction tasks
(e.g., extractive QA). As a fully dynamic compression method, LAVa consistently
maintains top performance across task types. Our code is available at
https://github.com/MGDDestiny/Lava.

</details>


### [60] [Hybrid Adaptive Conformal Offline Reinforcement Learning for Fair Population Health Management](https://arxiv.org/abs/2509.09772)
*Sanjay Basu,Sadiq Y. Patel,Parth Sheth,Bhairavi Muralidharan,Namrata Elamaran,Aakriti Kinra,Rajaie Batniji*

Main category: cs.LG

TL;DR: 提出HACO框架用于医疗补助人群健康管理，分离风险校准和偏好优化，用数据集评估，结果显示能提供保守可审计决策支持。


<details>
  <summary>Details</summary>
Motivation: 医疗补助人群健康管理项目需安全、公平、可审计，需有效方法生成行动建议并控制不良事件风险。

Method: 提出HACO框架，训练风险模型、推导保形阈值、学习偏好策略，用FQE评估策略并审计亚组表现。

Result: HACO有强风险区分度（AUC约0.81）和校准阈值，保持高安全覆盖率，亚组分析显示不同人群估计值有差异。

Conclusion: 保形风险门控与离线强化学习结合，能为人群健康管理团队提供保守、可审计的决策支持。

Abstract: Population health management programs for Medicaid populations coordinate
longitudinal outreach and services (e.g., benefits navigation, behavioral
health, social needs support, and clinical scheduling) and must be safe, fair,
and auditable. We present a Hybrid Adaptive Conformal Offline Reinforcement
Learning (HACO) framework that separates risk calibration from preference
optimization to generate conservative action recommendations at scale. In our
setting, each step involves choosing among common coordination actions (e.g.,
which member to contact, by which modality, and whether to route to a
specialized service) while controlling the near-term risk of adverse
utilization events (e.g., unplanned emergency department visits or
hospitalizations). Using a de-identified operational dataset from Waymark
comprising 2.77 million sequential decisions across 168,126 patients, HACO (i)
trains a lightweight risk model for adverse events, (ii) derives a conformal
threshold to mask unsafe actions at a target risk level, and (iii) learns a
preference policy on the resulting safe subset. We evaluate policies with a
version-agnostic fitted Q evaluation (FQE) on stratified subsets and audit
subgroup performance across age, sex, and race. HACO achieves strong risk
discrimination (AUC ~0.81) with a calibrated threshold ( {\tau} ~0.038 at
{\alpha} = 0.10), while maintaining high safe coverage. Subgroup analyses
reveal systematic differences in estimated value across demographics,
underscoring the importance of fairness auditing. Our results show that
conformal risk gating integrates cleanly with offline RL to deliver
conservative, auditable decision support for population health management
teams.

</details>


### [61] [FedBiF: Communication-Efficient Federated Learning via Bits Freezing](https://arxiv.org/abs/2509.10161)
*Shiwei Li,Qunwei Li,Haozhao Wang,Ruixuan Li,Jianbin Lin,Wenliang Zhong*

Main category: cs.LG

TL;DR: 本文提出Federated Bit Freezing (FedBiF)框架，在本地训练中直接学习量化模型参数，实验表明其通信压缩效果好、促进模型稀疏性且精度与FedAvg相当。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习因通信开销大影响训练效率，多数量化方法在本地训练后应用，会引入量化误差、降低模型精度。

Method: 提出FedBiF框架，服务器先量化模型参数并传给客户端，客户端每次更新单比特，冻结其余比特。

Result: 在五个数据集的IID和Non - IID设置下实验，FedBiF通信压缩效果好、促进模型稀疏性，1bpp上行和3bpp下行通信时精度与FedAvg相当。

Conclusion: FedBiF是有效的联邦学习框架，在通信压缩和模型精度上表现良好。

Abstract: Federated learning (FL) is an emerging distributed machine learning paradigm
that enables collaborative model training without sharing local data. Despite
its advantages, FL suffers from substantial communication overhead, which can
affect training efficiency. Recent efforts have mitigated this issue by
quantizing model updates to reduce communication costs. However, most existing
methods apply quantization only after local training, introducing quantization
errors into the trained parameters and potentially degrading model accuracy. In
this paper, we propose Federated Bit Freezing (FedBiF), a novel FL framework
that directly learns quantized model parameters during local training. In each
communication round, the server first quantizes the model parameters and
transmits them to the clients. FedBiF then allows each client to update only a
single bit of the multi-bit parameter representation, freezing the remaining
bits. This bit-by-bit update strategy reduces each parameter update to one bit
while maintaining high precision in parameter representation. Extensive
experiments are conducted on five widely used datasets under both IID and
Non-IID settings. The results demonstrate that FedBiF not only achieves
superior communication compression but also promotes sparsity in the resulting
models. Notably, FedBiF attains accuracy comparable to FedAvg, even when using
only 1 bit-per-parameter (bpp) for uplink and 3 bpp for downlink communication.
The code is available at https://github.com/Leopold1423/fedbif-tpds25.

</details>


### [62] [One Head, Many Models: Cross-Attention Routing for Cost-Aware LLM Selection](https://arxiv.org/abs/2509.09782)
*Roshini Pulishetty,Mani Kishan Ghantasala,Keerthy Kaushik Dasoju,Niti Mangwani,Vishal Garimella,Aditya Mate,Somya Chatterjee,Yue Kang,Ehi Nosakhare,Sadid Hasan,Soundar Srinivasan*

Main category: cs.LG

TL;DR: 提出统一路由框架，利用单头交叉注意力机制为输入查询动态选择最优大语言模型，在RouterBench上评估，性能优于现有路由器，提出指数奖励函数平衡性能和成本。


<details>
  <summary>Details</summary>
Motivation: 不同计算成本和性能的大语言模型在实际应用中可扩展、经济高效部署面临挑战。

Method: 引入统一路由框架，利用单头交叉注意力机制联合建模查询和模型嵌入；在RouterBench上评估；提出指数奖励函数。

Result: 在平均质量改进（AIQ）上提升达6.6%，最大性能提升2.9%。

Conclusion: 所提架构轻量级，跨领域泛化性好，效率优于先前方法，为成本感知的大语言模型路由树立新标准。

Abstract: The proliferation of large language models (LLMs) with varying computational
costs and performance profiles presents a critical challenge for scalable,
cost-effective deployment in real-world applications. We introduce a unified
routing framework that leverages a single-head cross-attention mechanism to
jointly model query and model embeddings, enabling dynamic selection of the
optimal LLM for each input query. Our approach is evaluated on RouterBench, a
large-scale, publicly available benchmark encompassing diverse LLM pools and
domains. By explicitly capturing fine-grained query-model interactions, our
router predicts both response quality and generation cost, achieving up to 6.6%
improvement in Average Improvement in Quality (AIQ) and 2.9% in maximum
performance over existing routers. To robustly balance performance and cost, we
propose an exponential reward function that enhances stability across user
preferences. The resulting architecture is lightweight, generalizes effectively
across domains, and demonstrates improved efficiency compared to prior methods,
establishing a new standard for cost-aware LLM routing.

</details>


### [63] [From the Gradient-Step Denoiser to the Proximal Denoiser and their associated convergent Plug-and-Play algorithms](https://arxiv.org/abs/2509.09793)
*Vincent Herfeld,Baudouin Denis de Senneville,Arthur Leclaire,Nicolas Papadakis*

Main category: cs.LG

TL;DR: 分析梯度步去噪器及其在即插即用算法中的应用


<details>
  <summary>Details</summary>
Motivation: 即插即用优化算法使用现成去噪器替代图像先验的邻近算子或梯度下降算子，通常图像先验是隐式无法表达的，需要合适的去噪器

Method: 对梯度步去噪器进行分析

Result: 未提及

Conclusion: 梯度步去噪器在保留先进去噪能力的同时，可作为显式泛函的梯度下降算子或邻近算子

Abstract: In this paper we analyze the Gradient-Step Denoiser and its usage in
Plug-and-Play algorithms. The Plug-and-Play paradigm of optimization algorithms
uses off the shelf denoisers to replace a proximity operator or a gradient
descent operator of an image prior. Usually this image prior is implicit and
cannot be expressed, but the Gradient-Step Denoiser is trained to be exactly
the gradient descent operator or the proximity operator of an explicit
functional while preserving state-of-the-art denoising capabilities.

</details>


### [64] [Distinguishing Startle from Surprise Events Based on Physiological Signals](https://arxiv.org/abs/2509.09799)
*Mansi Sharma,Alexandre Duchevet,Florian Daiber,Jean-Paul Imbert,Maurice Rekrut*

Main category: cs.LG

TL;DR: 利用机器学习和多模态融合策略，基于生理信号区分惊吓和惊喜事件，模型预测准确率较高。


<details>
  <summary>Details</summary>
Motivation: 意外事件影响注意力和决策，惊吓和惊喜反应难区分，现有研究多单独研究且对综合影响和生理数据区分关注有限。

Method: 使用机器学习和多模态融合策略，基于生理信号区分惊吓和惊喜事件。

Result: 能可靠预测事件，SVM和Late Fusion最高平均准确率达85.7%；扩展评估区分惊吓、惊喜和基线状态，XGBoost和Late Fusion最高平均准确率达74.9%。

Conclusion: 基于生理信号结合机器学习和多模态融合策略可有效区分惊吓和惊喜事件，模型具有一定鲁棒性。

Abstract: Unexpected events can impair attention and delay decision-making, posing
serious safety risks in high-risk environments such as aviation. In particular,
reactions like startle and surprise can impact pilot performance in different
ways, yet are often hard to distinguish in practice. Existing research has
largely studied these reactions separately, with limited focus on their
combined effects or how to differentiate them using physiological data. In this
work, we address this gap by distinguishing between startle and surprise events
based on physiological signals using machine learning and multi-modal fusion
strategies. Our results demonstrate that these events can be reliably
predicted, achieving a highest mean accuracy of 85.7% with SVM and Late Fusion.
To further validate the robustness of our model, we extended the evaluation to
include a baseline condition, successfully differentiating between Startle,
Surprise, and Baseline states with a highest mean accuracy of 74.9% with
XGBoost and Late Fusion.

</details>


### [65] [Flow Straight and Fast in Hilbert Space: Functional Rectified Flow](https://arxiv.org/abs/2509.10384)
*Jianxin Zhang,Clayton Scott*

Main category: cs.LG

TL;DR: 本文在无限维希尔伯特空间建立了修正流的严格泛函公式，拓展到泛函流匹配和泛函概率流ODEs，去除了现有理论假设，实验性能优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 许多生成模型在无限维有泛化，但修正流在无限维空间的拓展未被探索。

Method: 基于无限维空间连续性方程的叠加原理建立修正流的泛函公式。

Result: 该框架自然拓展到泛函流匹配和泛函概率流ODEs，去除了现有理论的测度论假设，实验性能更优。

Conclusion: 成功在无限维希尔伯特空间建立修正流泛函公式，且方法有效。

Abstract: Many generative models originally developed in finite-dimensional Euclidean
space have functional generalizations in infinite-dimensional settings.
However, the extension of rectified flow to infinite-dimensional spaces remains
unexplored. In this work, we establish a rigorous functional formulation of
rectified flow in an infinite-dimensional Hilbert space. Our approach builds
upon the superposition principle for continuity equations in an
infinite-dimensional space. We further show that this framework extends
naturally to functional flow matching and functional probability flow ODEs,
interpreting them as nonlinear generalizations of rectified flow. Notably, our
extension to functional flow matching removes the restrictive measure-theoretic
assumptions in the existing theory of \citet{kerrigan2024functional}.
Furthermore, we demonstrate experimentally that our method achieves superior
performance compared to existing functional generative models.

</details>


### [66] [Revisiting Actor-Critic Methods in Discrete Action Off-Policy Reinforcement Learning](https://arxiv.org/abs/2509.09838)
*Reza Asad,Reza Babanezhad,Sharan Vaswani*

Main category: cs.LG

TL;DR: 从离散SAC出发改进离散动作环境下离策略强化学习的演员-评论家方法，提出灵活框架，理论证明收敛性，实验接近DQN性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于值的方法是离散动作环境下离策略强化学习默认方法，基于策略的方法存在不能有效利用离策略数据或经验性能差的问题，需改进演员-评论家方法。

Method: 从离散SAC出发，解耦演员和评论家的熵，引入灵活的离策略演员-评论家框架，使用m步贝尔曼算子更新评论家，结合标准策略优化方法和熵正则化确定演员目标。

Result: 理论上证明所提方法在表格设置中可保证收敛到最优正则化值函数，实验上在标准Atari游戏中接近DQN性能，且无需熵正则化或显式探索。

Conclusion: 解耦演员和评论家的熵能提升离散SAC性能，所提灵活框架有效，可用于离散动作环境下离策略强化学习。

Abstract: Value-based approaches such as DQN are the default methods for off-policy
reinforcement learning with discrete-action environments such as Atari. Common
policy-based methods are either on-policy and do not effectively learn from
off-policy data (e.g. PPO), or have poor empirical performance in the
discrete-action setting (e.g. SAC). Consequently, starting from discrete SAC
(DSAC), we revisit the design of actor-critic methods in this setting. First,
we determine that the coupling between the actor and critic entropy is the
primary reason behind the poor performance of DSAC. We demonstrate that by
merely decoupling these components, DSAC can have comparable performance as
DQN. Motivated by this insight, we introduce a flexible off-policy actor-critic
framework that subsumes DSAC as a special case. Our framework allows using an
m-step Bellman operator for the critic update, and enables combining standard
policy optimization methods with entropy regularization to instantiate the
resulting actor objective. Theoretically, we prove that the proposed methods
can guarantee convergence to the optimal regularized value function in the
tabular setting. Empirically, we demonstrate that these methods can approach
the performance of DQN on standard Atari games, and do so even without entropy
regularization or explicit exploration.

</details>


### [67] [HGEN: Heterogeneous Graph Ensemble Networks](https://arxiv.org/abs/2509.09843)
*Jiajun Shen,Yufei Jin,Yi He,Xingquan Zhu*

Main category: cs.LG

TL;DR: 本文提出HGEN，利用元路径和变换优化管道集成多学习者提升异质图分类准确率，实验显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 异质图在节点类型、特征和局部拓扑的异质性给集成学习带来挑战，特别是难以适应不同图学习器。

Method: 通过元路径和变换优化管道集成多学习者；用元路径结合随机丢弃创建等位基因GNN；采用残差注意力机制校准不同元路径的等位基因GNN；使用相关性正则化项增大不同元路径嵌入矩阵差异。

Result: 分析了HGEN的收敛性，证明其正则化幅度高于简单投票；在五个异质网络上实验表明HGEN大幅优于现有方法。

Conclusion: HGEN在异质图集成学习中表现出色，能有效提升分类准确率。

Abstract: This paper presents HGEN that pioneers ensemble learning for heterogeneous
graphs. We argue that the heterogeneity in node types, nodal features, and
local neighborhood topology poses significant challenges for ensemble learning,
particularly in accommodating diverse graph learners. Our HGEN framework
ensembles multiple learners through a meta-path and transformation-based
optimization pipeline to uplift classification accuracy. Specifically, HGEN
uses meta-path combined with random dropping to create Allele Graph Neural
Networks (GNNs), whereby the base graph learners are trained and aligned for
later ensembling. To ensure effective ensemble learning, HGEN presents two key
components: 1) a residual-attention mechanism to calibrate allele GNNs of
different meta-paths, thereby enforcing node embeddings to focus on more
informative graphs to improve base learner accuracy, and 2) a
correlation-regularization term to enlarge the disparity among embedding
matrices generated from different meta-paths, thereby enriching base learner
diversity. We analyze the convergence of HGEN and attest its higher
regularization magnitude over simple voting. Experiments on five heterogeneous
networks validate that HGEN consistently outperforms its state-of-the-art
competitors by substantial margin.

</details>


### [68] [Understanding Outer Optimizers in Local SGD: Learning Rates, Momentum, and Acceleration](https://arxiv.org/abs/2509.10439)
*Ahmed Khaled,Satyen Kale,Arthur Douillard,Chi Jin,Rob Fergus,Manzil Zaheer*

Main category: cs.LG

TL;DR: 研究Local SGD中外层优化器作用，证明新收敛保证，探讨外层学习率调整，扩展到含动量设置，研究加速效果，引入数据依赖分析并实验验证。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习通信是瓶颈，Local SGD可减少通信开销，但外层优化器及其超参数选择不明确，需研究其作用。

Method: 理论分析Local SGD中外层优化器，证明收敛保证；扩展到含动量设置；研究加速效果；引入数据依赖分析；用标准语言模型和多种外层优化器实验验证。

Result: 调整外层学习率可权衡优化误差和随机梯度噪声方差，弥补内层学习率调参不佳；动量调整的外层学习率有类似作用；外层优化器加速可提高收敛率；数据依赖分析对外层学习率调整有新见解。

Conclusion: 研究明确了Local SGD中外层优化器及外层学习率的作用，相关理论得到实验验证。

Abstract: Modern machine learning often requires training with large batch size,
distributed data, and massively parallel compute hardware (like mobile and
other edge devices or distributed data centers). Communication becomes a major
bottleneck in such settings but methods like Local Stochastic Gradient Descent
(Local SGD) show great promise in reducing this additional communication
overhead. Local SGD consists of three parts: a local optimization process, an
aggregation mechanism, and an outer optimizer that uses the aggregated updates
from the nodes to produce a new model. While there exists an extensive
literature on understanding the impact of hyperparameters in the local
optimization process, the choice of outer optimizer and its hyperparameters is
less clear. We study the role of the outer optimizer in Local SGD, and prove
new convergence guarantees for the algorithm. In particular, we show that
tuning the outer learning rate allows us to (a) trade off between optimization
error and stochastic gradient noise variance, and (b) make up for ill-tuning of
the inner learning rate. Our theory suggests that the outer learning rate
should sometimes be set to values greater than $1$. We extend our results to
settings where we use momentum in the outer optimizer, and we show a similar
role for the momentum-adjusted outer learning rate. We also study acceleration
in the outer optimizer and show that it improves the convergence rate as a
function of the number of communication rounds, improving upon the convergence
rate of prior algorithms that apply acceleration locally. Finally, we also
introduce a novel data-dependent analysis of Local SGD that yields further
insights on outer learning rate tuning. We conduct comprehensive experiments
with standard language models and various outer optimizers to validate our
theory.

</details>


### [69] [Latency and Token-Aware Test-Time Compute](https://arxiv.org/abs/2509.09864)
*Jenny Y. Huang,Mehul Damani,Yousef El-Kurdi,Ramon Astudillo,Wei Sun*

Main category: cs.LG

TL;DR: 本文提出将推理时间扩展作为动态计算分配和方法选择问题，兼顾令牌成本和时钟延迟，实验表明该方法优于静态策略。


<details>
  <summary>Details</summary>
Motivation: 现有推理时间扩展工作在动态计算分配方面，仅考虑并行生成方法，忽略增量解码方法，且主要关注令牌使用，忽略延迟。

Method: 将推理时间扩展问题表述为动态计算分配和方法选择问题，明确纳入令牌成本和时钟延迟。

Result: 在推理基准测试中，该方法始终优于静态策略，实现了良好的准确率 - 成本权衡。

Conclusion: 该方法在实现良好准确率 - 成本权衡的同时，适合实际部署。

Abstract: Inference-time scaling has emerged as a powerful way to improve large
language model (LLM) performance by generating multiple candidate responses and
selecting among them. However, existing work on dynamic allocation for
test-time compute typically considers only parallel generation methods such as
best-of-N, overlooking incremental decoding methods like beam search, and has
largely ignored latency, focusing only on token usage. We formulate
inference-time scaling as a problem of dynamic compute allocation and method
selection, where the system must decide which strategy to apply and how much
compute to allocate on a per-query basis. Our framework explicitly incorporates
both token cost and wall-clock latency, the latter being critical for user
experience and particularly for agentic workflows where models must issue
multiple queries efficiently. Experiments on reasoning benchmarks show that our
approach consistently outperforms static strategies, achieving favorable
accuracy-cost trade-offs while remaining practical for deployment.

</details>


### [70] [Variational Neural Networks for Observable Thermodynamics (V-NOTS)](https://arxiv.org/abs/2509.09899)
*Christopher Eldred,François Gay-Balmaz,Vakhtang Putkaradze*

Main category: cs.LG

TL;DR: 提出基于可观测量的高效数据计算框架，用于描述耗散动力系统相空间演化。


<details>
  <summary>Details</summary>
Motivation: 现有数据计算物理系统演化方法中，可用数据常与定义系统相空间的变量不对应，如耗散动力系统中动量和熵一般无法直接观测。

Method: 构建基于热力学拉格朗日量的新方法，并构建遵循热力学且保证熵非减演化的神经网络。

Result: 网络能基于有限数据点和较少系统参数对相空间演化进行有效描述。

Conclusion: 所开发的基于可观测量的计算框架可有效解决数据与相空间变量不匹配问题，实现相空间演化描述。

Abstract: Much attention has recently been devoted to data-based computing of evolution
of physical systems. In such approaches, information about data points from
past trajectories in phase space is used to reconstruct the equations of motion
and to predict future solutions that have not been observed before. However, in
many cases, the available data does not correspond to the variables that define
the system's phase space. We focus our attention on the important example of
dissipative dynamical systems. In that case, the phase space consists of
coordinates, momenta and entropies; however, the momenta and entropies cannot,
in general, be observed directly. To address this difficulty, we develop an
efficient data-based computing framework based exclusively on observable
variables, by constructing a novel approach based on the \emph{thermodynamic
Lagrangian}, and constructing neural networks that respect the thermodynamics
and guarantees the non-decreasing entropy evolution. We show that our network
can provide an efficient description of phase space evolution based on a
limited number of data points and a relatively small number of parameters in
the system.

</details>


### [71] [LoFT: Parameter-Efficient Fine-Tuning for Long-tailed Semi-Supervised Learning in Open-World Scenarios](https://arxiv.org/abs/2509.09926)
*Jiahao Chen,Zhiyuan Huang,Yurou Liu,Bing Su*

Main category: cs.LG

TL;DR: 本文将长尾半监督学习拓展到基础模型微调范式，提出LoFT框架，还探索开放世界条件下的半监督学习并提出LoFT - OW，实验显示其性能优于先前方法。


<details>
  <summary>Details</summary>
Motivation: 现有长尾半监督学习方法多从头训练模型，存在过度自信和伪标签质量低的问题，且缺乏开放世界条件下的研究。

Method: 提出LoFT框架，将长尾半监督学习拓展到基础模型微调范式；提出LoFT - OW处理开放世界条件下的半监督学习问题。

Result: 在多个基准测试上，即使仅使用先前工作1%的未标记数据，该方法也取得了优于先前方法的性能。

Conclusion: 基础模型微调可生成更可靠的伪标签，LoFT和LoFT - OW能有效解决长尾半监督学习问题，性能表现出色。

Abstract: Long-tailed learning has garnered increasing attention due to its wide
applicability in real-world scenarios. Among existing approaches, Long-Tailed
Semi-Supervised Learning (LTSSL) has emerged as an effective solution by
incorporating a large amount of unlabeled data into the imbalanced labeled
dataset. However, most prior LTSSL methods are designed to train models from
scratch, which often leads to issues such as overconfidence and low-quality
pseudo-labels. To address these challenges, we extend LTSSL into the foundation
model fine-tuning paradigm and propose a novel framework: LoFT (Long-tailed
semi-supervised learning via parameter-efficient Fine-Tuning). We demonstrate
that fine-tuned foundation models can generate more reliable pseudolabels,
thereby benefiting imbalanced learning. Furthermore, we explore a more
practical setting by investigating semi-supervised learning under open-world
conditions, where the unlabeled data may include out-of-distribution (OOD)
samples. To handle this problem, we propose LoFT-OW (LoFT under Open-World
scenarios) to improve the discriminative ability. Experimental results on
multiple benchmarks demonstrate that our method achieves superior performance
compared to previous approaches, even when utilizing only 1\% of the unlabeled
data compared with previous works.

</details>


### [72] [Multi-Play Combinatorial Semi-Bandit Problem](https://arxiv.org/abs/2509.09933)
*Shintaro Nakamura,Yuko Kuroki,Wei Chen*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In the combinatorial semi-bandit (CSB) problem, a player selects an action
from a combinatorial action set and observes feedback from the base arms
included in the action. While CSB is widely applicable to combinatorial
optimization problems, its restriction to binary decision spaces excludes
important cases involving non-negative integer flows or allocations, such as
the optimal transport and knapsack problems.To overcome this limitation, we
propose the multi-play combinatorial semi-bandit (MP-CSB), where a player can
select a non-negative integer action and observe multiple feedbacks from a
single arm in each round. We propose two algorithms for the MP-CSB. One is a
Thompson-sampling-based algorithm that is computationally feasible even when
the action space is exponentially large with respect to the number of arms, and
attains $O(\log T)$ distribution-dependent regret in the stochastic regime,
where $T$ is the time horizon. The other is a best-of-both-worlds algorithm,
which achieves $O(\log T)$ variance-dependent regret in the stochastic regime
and the worst-case $\tilde{\mathcal{O}}\left( \sqrt{T} \right)$ regret in the
adversarial regime. Moreover, its regret in adversarial one is data-dependent,
adapting to the cumulative loss of the optimal action, the total quadratic
variation, and the path-length of the loss sequence. Finally, we numerically
show that the proposed algorithms outperform existing methods in the CSB
literature.

</details>


### [73] [SciML Agents: Write the Solver, Not the Solution](https://arxiv.org/abs/2509.09936)
*Saarth Gaonkar,Xiang Zheng,Haocheng Xi,Rishabh Tiwari,Kurt Keutzer,Dmitriy Morozov,Michael W. Mahoney,Amir Gholami*

Main category: cs.LG

TL;DR: 本文探索用大语言模型（LLMs）编写利用数值算法的代码来解决科学常微分方程（ODE）问题，引入新数据集进行评估，发现合理提示和微调可使LLMs有效解决简单ODE问题。


<details>
  <summary>Details</summary>
Motivation: 当前科学机器学习在实现高精度和鲁棒性上有挑战，且缺乏科学计算任务能力的基准测试，因此探索用LLMs解决科学ODE问题并建立评估标准。

Method: 引入诊断数据集和大规模基准数据集，从无引导与有领域知识引导提示、现成模型与微调模型两个维度评估开源和闭源LLM模型，衡量代码可执行性和数值有效性。

Result: 有足够上下文和引导提示时，新的指令跟随模型在两个评估标准上精度高；许多近期开源系统不微调也表现出色，旧模型或小模型微调后有提升。

Conclusion: 精心提示和微调可产生能可靠解决简单ODE问题的专业LLM代理。

Abstract: Recent work in scientific machine learning aims to tackle scientific tasks
directly by predicting target values with neural networks (e.g.,
physics-informed neural networks, neural ODEs, neural operators, etc.), but
attaining high accuracy and robustness has been challenging. We explore an
alternative view: use LLMs to write code that leverages decades of numerical
algorithms. This shifts the burden from learning a solution function to making
domain-aware numerical choices. We ask whether LLMs can act as SciML agents
that, given a natural-language ODE description, generate runnable code that is
scientifically appropriate, selecting suitable solvers (stiff vs. non-stiff),
and enforcing stability checks. There is currently no benchmark to measure this
kind of capability for scientific computing tasks. As such, we first introduce
two new datasets: a diagnostic dataset of adversarial "misleading" problems;
and a large-scale benchmark of 1,000 diverse ODE tasks. The diagnostic set
contains problems whose superficial appearance suggests stiffness, and that
require algebraic simplification to demonstrate non-stiffness; and the
large-scale benchmark spans stiff and non-stiff ODE regimes. We evaluate open-
and closed-source LLM models along two axes: (i) unguided versus guided
prompting with domain-specific knowledge; and (ii) off-the-shelf versus
fine-tuned variants. Our evaluation measures both executability and numerical
validity against reference solutions. We find that with sufficient context and
guided prompts, newer instruction-following models achieve high accuracy on
both criteria. In many cases, recent open-source systems perform strongly
without fine-tuning, while older or smaller models still benefit from
fine-tuning. Overall, our preliminary results indicate that careful prompting
and fine-tuning can yield a specialized LLM agent capable of reliably solving
simple ODE problems.

</details>


### [74] [DyKen-Hyena: Dynamic Kernel Generation via Cross-Modal Attention for Multimodal Intent Recognition](https://arxiv.org/abs/2509.09940)
*Yifei Wang,Wenbin Wang,Yong Luo*

Main category: cs.LG

TL;DR: 现有多模态意图识别模型存在问题，提出DyKen - Hyena模型，将问题从特征融合转为处理调制，在基准测试取得SOTA结果，提升了F1分数。


<details>
  <summary>Details</summary>
Motivation: 当前多模态意图识别模型在融合模态时，可能用无关非语言信号破坏主要语言特征，无法捕捉细粒度影响，阻碍性能提升。

Method: 引入DyKen - Hyena模型，将视听线索转化为动态的、逐词卷积核，直接调制文本特征提取。

Result: 在MIntRec和MIntRec2.0基准测试中取得了SOTA结果，在超出范围检测中F1分数提升10.46%。

Conclusion: 该方法创建了更稳健的意图表示。

Abstract: Though Multimodal Intent Recognition (MIR) proves effective by utilizing rich
information from multiple sources (e.g., language, video, and audio), the
potential for intent-irrelevant and conflicting information across modalities
may hinder performance from being further improved. Most current models attempt
to fuse modalities by applying mechanisms like multi-head attention to unimodal
feature sequences and then adding the result back to the original
representation. This process risks corrupting the primary linguistic features
with noisy or irrelevant non-verbal signals, as it often fails to capture the
fine-grained, token-level influence where non-verbal cues should modulate, not
just augment, textual meaning. To address this, we introduce DyKen-Hyena, which
reframes the problem from feature fusion to processing modulation. Our model
translates audio-visual cues into dynamic, per-token convolutional kernels that
directly modulate textual feature extraction. This fine-grained approach
achieves state-of-the-art results on the MIntRec and MIntRec2.0 benchmarks.
Notably, it yields a +10.46% F1-score improvement in out-of-scope detection,
validating that our method creates a fundamentally more robust intent
representation.

</details>


### [75] [Adaptive Token Merging for Efficient Transformer Semantic Communication at the Edge](https://arxiv.org/abs/2509.09955)
*Omar Erak,Omar Alhussein,Hatem Abou-Zeid,Mehdi Bennis,Sami Muhaidat*

Main category: cs.LG

TL;DR: 提出无训练自适应令牌合并框架，在资源受限边缘设备上压缩变压器表示，实现效率与任务相关性平衡。


<details>
  <summary>Details</summary>
Motivation: 大规模变压器在现代语义通信中计算和通信成本高，难以部署在资源受限边缘设备。

Method: 引入无训练自适应令牌合并框架，将合并策略发现视为多目标优化问题，用贝叶斯优化获取帕累托最优权衡。

Result: 在ImageNet分类和视觉问答任务上，减少运算和通信成本并保持竞争力，在不同信道条件下稳定且有隐私优势。

Conclusion: 该框架为资源受限边缘智能场景部署变压器模型提供实用通用解决方案。

Abstract: Large-scale transformers are central to modern semantic communication, yet
their high computational and communication costs hinder deployment on
resource-constrained edge devices. This paper introduces a training-free
framework for adaptive token merging, a novel mechanism that compresses
transformer representations at runtime by selectively merging semantically
redundant tokens under per-layer similarity thresholds. Unlike prior
fixed-ratio reduction, our approach couples merging directly to input
redundancy, enabling data-dependent adaptation that balances efficiency and
task relevance without retraining. We cast the discovery of merging strategies
as a multi-objective optimization problem and leverage Bayesian optimization to
obtain Pareto-optimal trade-offs between accuracy, inference cost, and
communication cost. On ImageNet classification, we match the accuracy of the
unmodified transformer with 30\% fewer floating-point operations per second and
under 20\% of the original communication cost, while for visual question
answering our method achieves performance competitive with the full LLaVA model
at less than one-third of the compute and one-tenth of the bandwidth. Finally,
we show that our adaptive merging is robust across varying channel conditions
and provides inherent privacy benefits, substantially degrading the efficacy of
model inversion attacks. Our framework provides a practical and versatile
solution for deploying powerful transformer models in resource-limited edge
intelligence scenarios.

</details>


### [76] [Limited Reference, Reliable Generation: A Two-Component Framework for Tabular Data Generation in Low-Data Regimes](https://arxiv.org/abs/2509.09960)
*Mingxuan Jiang,Yongxin Wang,Ziyue Dai,Yicun Liu,Hongyi Nie,Sen Liu,Hongfeng Chai*

Main category: cs.LG

TL;DR: 提出ReFine框架解决现有表格数据生成方法在数据稀缺场景的问题，实验表明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有表格数据生成方法在数据稀缺的特定领域数据库效果不佳，prompt - based LLMs存在问题，需新方法。

Method: 提出ReFine框架，从可解释模型导出“if - then”规则嵌入提示，应用双粒度过滤策略。

Result: 在多种回归和分类基准测试中，ReFine始终优于现有方法，回归的R平方最高提升0.44，分类的F1分数相对提升10.0%。

Conclusion: ReFine框架能有效解决现有表格数据生成方法的问题，提升下游任务性能。

Abstract: Synthetic tabular data generation is increasingly essential in data
management, supporting downstream applications when real-world and high-quality
tabular data is insufficient. Existing tabular generation approaches, such as
generative adversarial networks (GANs), diffusion models, and fine-tuned Large
Language Models (LLMs), typically require sufficient reference data, limiting
their effectiveness in domain-specific databases with scarce records. While
prompt-based LLMs offer flexibility without parameter tuning, they often fail
to capture dataset-specific feature-label dependencies and generate redundant
data, leading to degradation in downstream task performance. To overcome these
issues, we propose ReFine, a framework that (i) derives symbolic "if-then"
rules from interpretable models and embeds them into prompts to explicitly
guide generation toward domain-specific feature distribution, and (ii) applies
a dual-granularity filtering strategy that suppresses over-sampling patterns
and selectively refines rare but informative samples to reduce distributional
imbalance. Extensive experiments on various regression and classification
benchmarks demonstrate that ReFine consistently outperforms state-of-the-art
methods, achieving up to 0.44 absolute improvement in R-squared for regression
and 10.0 percent relative improvement in F1 score for classification tasks.

</details>


### [77] [Data-Driven Energy Estimation for Virtual Servers Using Combined System Metrics and Machine Learning](https://arxiv.org/abs/2509.09991)
*Amandip Sangha*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This paper presents a machine learning-based approach to estimate the energy
consumption of virtual servers without access to physical power measurement
interfaces. Using resource utilization metrics collected from guest virtual
machines, we train a Gradient Boosting Regressor to predict energy consumption
measured via RAPL on the host. We demonstrate, for the first time, guest-only
resource-based energy estimation without privileged host access with
experiments across diverse workloads, achieving high predictive accuracy and
variance explained ($0.90 \leq R^2 \leq 0.97$), indicating the feasibility of
guest-side energy estimation. This approach can enable energy-aware scheduling,
cost optimization and physical host independent energy estimates in virtualized
environments. Our approach addresses a critical gap in virtualized environments
(e.g. cloud) where direct energy measurement is infeasible.

</details>


### [78] [Neural Scaling Laws for Deep Regression](https://arxiv.org/abs/2509.10000)
*Tilen Cadez,Kyoung-Min Kim*

Main category: cs.LG

TL;DR: 本文实证研究深度回归中的神经缩放定律，观察到损失与训练数据集大小和模型容量的幂律关系，表明增加数据量可大幅提升深度回归模型性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型凸显了神经缩放定律的重要性，但该定律在深度回归模型中的应用尚待探索。

Method: 使用扭曲范德瓦尔斯磁体的参数估计模型，采用全连接网络、残差网络和视觉变压器等多种架构进行实证研究。

Result: 观察到损失与训练数据集大小和模型容量在大范围值内呈幂律关系，缩放指数在1到2之间，具体值取决于回归参数和模型细节。

Conclusion: 深度回归模型的性能可随数据量增加而显著提升。

Abstract: Neural scaling laws--power-law relationships between generalization errors
and characteristics of deep learning models--are vital tools for developing
reliable models while managing limited resources. Although the success of large
language models highlights the importance of these laws, their application to
deep regression models remains largely unexplored. Here, we empirically
investigate neural scaling laws in deep regression using a parameter estimation
model for twisted van der Waals magnets. We observe power-law relationships
between the loss and both training dataset size and model capacity across a
wide range of values, employing various architectures--including fully
connected networks, residual networks, and vision transformers. Furthermore,
the scaling exponents governing these relationships range from 1 to 2, with
specific values depending on the regressed parameters and model details. The
consistent scaling behaviors and their large scaling exponents suggest that the
performance of deep regression models can improve substantially with increasing
data size.

</details>


### [79] [Clip Your Sequences Fairly: Enforcing Length Fairness for Sequence-Level RL](https://arxiv.org/abs/2509.09177)
*Hanyi Mao,Quanjia Xiao,Lei Pang,Haixiao Liu*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We propose FSPO (Fair Sequence Policy Optimization), a sequence-level
reinforcement learning method for LLMs that enforces length-fair clipping
directly in the importance-sampling (IS) weight space. We revisit
sequence-level RL methods and identify a mismatch when PPO/GRPO-style clipping
is transplanted to sequences: a fixed clip range systematically reweights short
vs. long responses, distorting the effective objective. Theoretically, we
formalize length fairness via a Length Reweighting Error (LRE) and prove that
small LRE yields a directional cosine guarantee between the clipped and true
updates. FSPO introduces a simple, Gaussian-motivated remedy: we clip the
sequence log-IS ratio with a band that applies a KL-corrected drift term and
scales as $\sqrt{L}$. Empirically, FSPO flattens clip rates across length bins,
stabilizes training, and outperforms all baselines across multiple evaluation
datasets.

</details>


### [80] [Intrinsic Dimension Estimating Autoencoder (IDEA) Using CancelOut Layer and a Projected Loss](https://arxiv.org/abs/2509.10011)
*Antoine Orioua,Philipp Krah,Julian Koellermeier*

Main category: cs.LG

TL;DR: 本文介绍IDEA自编码器，可估计数据集内在维度并重建数据，经理论基准测试表现良好，还应用于一维自由表面流数据。


<details>
  <summary>Details</summary>
Motivation: 提出一种能识别多种数据集潜在内在维度，且能将数据投影到潜在空间后重建的方法。

Method: 引入投影重建损失项指导模型训练，使用重新加权的双CancelOut层构建潜在空间。

Result: 在理论基准测试中表现出良好准确性和高通用性，成功估计一维自由表面流数据集的内在维度并重建原始解。

Conclusion: IDEA方法有效，可用于估计数据集内在维度和重建数据。

Abstract: This paper introduces the Intrinsic Dimension Estimating Autoencoder (IDEA),
which identifies the underlying intrinsic dimension of a wide range of datasets
whose samples lie on either linear or nonlinear manifolds. Beyond estimating
the intrinsic dimension, IDEA is also able to reconstruct the original dataset
after projecting it onto the corresponding latent space, which is structured
using re-weighted double CancelOut layers. Our key contribution is the
introduction of the projected reconstruction loss term, guiding the training of
the model by continuously assessing the reconstruction quality under the
removal of an additional latent dimension. We first assess the performance of
IDEA on a series of theoretical benchmarks to validate its robustness. These
experiments allow us to test its reconstruction ability and compare its
performance with state-of-the-art intrinsic dimension estimators. The
benchmarks show good accuracy and high versatility of our approach.
Subsequently, we apply our model to data generated from the numerical solution
of a vertically resolved one-dimensional free-surface flow, following a
pointwise discretization of the vertical velocity profile in the horizontal
direction, vertical direction, and time. IDEA succeeds in estimating the
dataset's intrinsic dimension and then reconstructs the original solution by
working directly within the projection space identified by the network.

</details>


### [81] [AEGIS: An Agent for Extraction and Geographic Identification in Scholarly Proceedings](https://arxiv.org/abs/2509.09470)
*Om Vishesh,Harshad Khadilkar,Deepak Akkil*

Main category: cs.LG

TL;DR: 提出自动化系统应对学术文献增长挑战，用 Agent - E 识别特定地区论文并执行 RPA，验证效果良好，显示任务导向 AI 潜力。


<details>
  <summary>Details</summary>
Motivation: 应对学术文献快速增长，解决学术发现手动操作耗时的问题。

Method: 构建从数据发现到直接行动的自动化系统，利用 Agent - E 识别特定地理区域论文并执行 RPA。

Result: 在 586 篇来自五个不同会议的论文上验证，成功识别所有目标论文，召回率 100%，准确率 99.4%。

Conclusion: 任务导向的 AI 代理不仅能过滤信息，还能积极参与并加速学术界工作流程。

Abstract: Keeping pace with the rapid growth of academia literature presents a
significant challenge for researchers, funding bodies, and academic societies.
To address the time-consuming manual effort required for scholarly discovery,
we present a novel, fully automated system that transitions from data discovery
to direct action. Our pipeline demonstrates how a specialized AI agent,
'Agent-E', can be tasked with identifying papers from specific geographic
regions within conference proceedings and then executing a Robotic Process
Automation (RPA) to complete a predefined action, such as submitting a
nomination form. We validated our system on 586 papers from five different
conferences, where it successfully identified every target paper with a recall
of 100% and a near perfect accuracy of 99.4%. This demonstration highlights the
potential of task-oriented AI agents to not only filter information but also to
actively participate in and accelerate the workflows of the academic community.

</details>


### [82] [Exploring Expert Specialization through Unsupervised Training in Sparse Mixture of Experts](https://arxiv.org/abs/2509.10025)
*Strahinja Nikolic,Ilker Oguz,Demetri Psaltis*

Main category: cs.LG

TL;DR: 本文探索SMoE - VAE架构，在QuickDraw数据集上对比无监督和有监督专家路由，发现无监督路由重建性能更好，还研究了数据集大小的影响。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习可解释性中理解神经网络内部组织的挑战。

Method: 探索SMoE - VAE架构，在QuickDraw数据集上对比无监督专家路由和有监督基线。

Result: 无监督路由重建性能始终更优，专家能识别超越人类定义类边界的子类别结构，还研究了数据集大小对专家专业化的影响。

Conclusion: MoE模型能发现更符合模型目标的数据结构，研究为设计高效MoE架构提供指导。

Abstract: Understanding the internal organization of neural networks remains a
fundamental challenge in deep learning interpretability. We address this
challenge by exploring a novel Sparse Mixture of Experts Variational
Autoencoder (SMoE-VAE) architecture. We test our model on the QuickDraw
dataset, comparing unsupervised expert routing against a supervised baseline
guided by ground-truth labels. Surprisingly, we find that unsupervised routing
consistently achieves superior reconstruction performance. The experts learn to
identify meaningful sub-categorical structures that often transcend
human-defined class boundaries. Through t-SNE visualizations and reconstruction
analysis, we investigate how MoE models uncover fundamental data structures
that are more aligned with the model's objective than predefined labels.
Furthermore, our study on the impact of dataset size provides insights into the
trade-offs between data quantity and expert specialization, offering guidance
for designing efficient MoE architectures.

</details>


### [83] [Sparse Coding Representation of 2-way Data](https://arxiv.org/abs/2509.10033)
*Boya Ma,Abram Magner,Maxwell McNeil,Petko Bogdanov*

Main category: cs.LG

TL;DR: 提出二维字典场景的低秩编码模型AODL，分析数据复杂度，通过交替优化求解，在合成和真实数据集上验证其数据重建和缺失值插补效果，比非低秩和分析字典基线更稀疏。


<details>
  <summary>Details</summary>
Motivation: 数据学习字典在多字典场景下学习字典和编码系数具有挑战性，尤其是编码系数对应所有字典原子组合。

Method: 提出低秩编码模型，给出数据复杂度界；提出凸松弛解AODL，通过稀疏编码矩阵和学习字典的交替优化求解，并证明收敛性。

Result: 在合成和真实数据集上展示了数据重建和缺失值插补的质量；与非低秩和分析字典基线相比，在固定重建质量下学习到的解稀疏度高90%；学习的字典揭示训练样本内模式的可解释见解。

Conclusion: AODL模型在多字典场景的稀疏编码中有效，能得到更稀疏的解并提供可解释信息。

Abstract: Sparse dictionary coding represents signals as linear combinations of a few
dictionary atoms. It has been applied to images, time series, graph signals and
multi-way spatio-temporal data by jointly employing temporal and spatial
dictionaries. Data-agnostic analytical dictionaries, such as the discrete
Fourier transform, wavelets and graph Fourier, have seen wide adoption due to
efficient implementations and good practical performance. On the other hand,
dictionaries learned from data offer sparser and more accurate solutions but
require learning of both the dictionaries and the coding coefficients. This
becomes especially challenging for multi-dictionary scenarios since encoding
coefficients correspond to all atom combinations from the dictionaries. To
address this challenge, we propose a low-rank coding model for 2-dictionary
scenarios and study its data complexity. Namely, we establish a bound on the
number of samples needed to learn dictionaries that generalize to unseen
samples from the same distribution. We propose a convex relaxation solution,
called AODL, whose exact solution we show also solves the original problem. We
then solve this relaxation via alternating optimization between the sparse
coding matrices and the learned dictionaries, which we prove to be convergent.
We demonstrate its quality for data reconstruction and missing value imputation
in both synthetic and real-world datasets. For a fixed reconstruction quality,
AODL learns up to 90\% sparser solutions compared to non-low-rank and
analytical (fixed) dictionary baselines. In addition, the learned dictionaries
reveal interpretable insights into patterns present within the samples used for
training.

</details>


### [84] [Symbolic Feedforward Networks for Probabilistic Finite Automata: Exact Simulation and Learnability](https://arxiv.org/abs/2509.10034)
*Sahil Rajesh Dhayalkar*

Main category: cs.LG

TL;DR: 提出用符号前馈神经网络精确模拟概率有限自动机（PFAs）的理论，展示其并行、可解释和可微特性，且模拟器可学习，统一了概率自动机理论和神经网络架构。


<details>
  <summary>Details</summary>
Motivation: 将概率自动机理论与神经网络架构在严谨代数框架下统一，弥合符号计算和深度学习之间的差距。

Method: 用向量表示状态分布，随机矩阵表示转换，通过矩阵 - 向量积进行概率状态传播，用分层符号计算表征概率子集构造、ε - 闭包和精确模拟，用标准梯度下降优化训练。

Result: 能精确模拟 PFAs 动态，训练后可恢复真实 PFAs 的行为。

Conclusion: 基于严格代数框架统一了概率自动机理论和神经网络架构。

Abstract: We present a formal and constructive theory showing that probabilistic finite
automata (PFAs) can be exactly simulated using symbolic feedforward neural
networks. Our architecture represents state distributions as vectors and
transitions as stochastic matrices, enabling probabilistic state propagation
via matrix-vector products. This yields a parallel, interpretable, and
differentiable simulation of PFA dynamics using soft updates-without
recurrence. We formally characterize probabilistic subset construction,
$\varepsilon$-closure, and exact simulation via layered symbolic computation,
and prove equivalence between PFAs and specific classes of neural networks. We
further show that these symbolic simulators are not only expressive but
learnable: trained with standard gradient descent-based optimization on labeled
sequence data, they recover the exact behavior of ground-truth PFAs. This
learnability, formalized in Proposition 5.1, is the crux of this work. Our
results unify probabilistic automata theory with neural architectures under a
rigorous algebraic framework, bridging the gap between symbolic computation and
deep learning.

</details>


### [85] [FedRP: A Communication-Efficient Approach for Differentially Private Federated Learning Using Random Projection](https://arxiv.org/abs/2509.10041)
*Mohammad Hasan Narimani,Mostafa Tavassolipour*

Main category: cs.LG

TL;DR: 本文提出新的联邦学习算法FedRP，结合随机投影和ADMM优化框架，增强隐私保护、降低通信成本，实验显示其在隐私和效率上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在用户隐私保护和通信成本管理方面存在挑战，需改进算法。

Method: 引入FedRP算法，结合随机投影技术和ADMM优化框架，用随机投影降低模型参数维度。

Result: FedRP算法能保持高模型准确率，在隐私保护和通信效率上优于现有方法。

Conclusion: FedRP算法能有效解决联邦学习中隐私保护和通信成本问题，具有良好性能。

Abstract: Federated learning (FL) offers an innovative paradigm for collaborative model
training across decentralized devices, such as smartphones, balancing enhanced
predictive performance with the protection of user privacy in sensitive areas
like Internet of Things (IoT) and medical data analysis. Despite its
advantages, FL encounters significant challenges related to user privacy
protection against potential attacks and the management of communication costs.
This paper introduces a novel federated learning algorithm called FedRP, which
integrates random projection techniques with the Alternating Direction Method
of Multipliers (ADMM) optimization framework. This approach enhances privacy by
employing random projection to reduce the dimensionality of model parameters
prior to their transmission to a central server, reducing the communication
cost. The proposed algorithm offers a strong $(\epsilon, \delta)$-differential
privacy guarantee, demonstrating resilience against data reconstruction
attacks. Experimental results reveal that FedRP not only maintains high model
accuracy but also outperforms existing methods, including conventional
differential privacy approaches and FedADMM, in terms of both privacy
preservation and communication efficiency.

</details>


### [86] [Uncertainty-Aware Tabular Prediction: Evaluating VBLL-Enhanced TabPFN in Safety-Critical Medical Data](https://arxiv.org/abs/2509.10048)
*Madhushan Ramalingam*

Main category: cs.LG

TL;DR: 评估VBLL集成TabPFN在不确定性校准中的性能，实验发现原始TabPFN在不确定性校准上始终优于集成版。


<details>
  <summary>Details</summary>
Motivation: 预测模型在关键领域应用时，可靠的不确定性估计很重要，评估VBLL集成TabPFN在不确定性校准中的性能。

Method: 在三个医学表格数据集上对比原始TabPFN和VBLL集成版TabPFN的性能。

Result: 原始TabPFN在所有数据集的不确定性校准上始终优于VBLL集成TabPFN。

Conclusion: 原始TabPFN在不确定性校准方面表现更好，与预期相反。

Abstract: Predictive models are being increasingly used across a wide range of domains,
including safety-critical applications such as medical diagnosis and criminal
justice. Reliable uncertainty estimation is a crucial task in such settings.
Tabular Prior-data Fitted Network (TabPFN) is a recently proposed machine
learning foundation model for tabular dataset, which uses a generative
transformer architecture. Variational Bayesian Last Layers (VBLL) is a
state-of-the-art lightweight variational formulation that effectively improves
uncertainty estimation with minimal computational overhead. In this work we aim
to evaluate the performance of VBLL integrated with the recently proposed
TabPFN in uncertainty calibration. Our experiments, conducted on three
benchmark medical tabular datasets, compare the performance of the original
TabPFN and the VBLL-integrated version. Contrary to expectations, we observed
that original TabPFN consistently outperforms VBLL integrated TabPFN in
uncertainty calibration across all datasets.

</details>


### [87] [KAN-SR: A Kolmogorov-Arnold Network Guided Symbolic Regression Framework](https://arxiv.org/abs/2509.10089)
*Marco Andrea Bühler,Gonzalo Guillén-Gosálbez*

Main category: cs.LG

TL;DR: 提出基于KANs的符号回归框架KAN - SR，结合深度学习和简化策略恢复Feynman数据集方程，与神经控制微分方程结合可精确建模生物过程系统。


<details>
  <summary>Details</summary>
Motivation: 改进符号回归问题的解决方法，传统使用遗传编程，本文探索用深度学习技术。

Method: 构建基于KANs的KAN - SR框架，结合深度学习技术和简化策略，还与神经控制微分方程结合。

Result: 能恢复Feynman SRSD数据集的真实方程，可精确建模生物过程系统。

Conclusion: KAN - SR框架在符号回归和工程系统动态建模方面有应用潜力。

Abstract: We introduce a novel symbolic regression framework, namely KAN-SR, built on
Kolmogorov Arnold Networks (KANs) which follows a divide-and-conquer approach.
Symbolic regression searches for mathematical equations that best fit a given
dataset and is commonly solved with genetic programming approaches. We show
that by using deep learning techniques, more specific KANs, and combining them
with simplification strategies such as translational symmetries and
separabilities, we are able to recover ground-truth equations of the Feynman
Symbolic Regression for Scientific Discovery (SRSD) dataset. Additionally, we
show that by combining the proposed framework with neural controlled
differential equations, we are able to model the dynamics of an in-silico
bioprocess system precisely, opening the door for the dynamic modeling of other
engineering systems.

</details>


### [88] [Cost-Free Personalization via Information-Geometric Projection in Bayesian Federated Learning](https://arxiv.org/abs/2509.10132)
*Nour Jamoussi,Giuseppe Serra,Photios A. Stavrou,Marios Kountouris*

Main category: cs.LG

TL;DR: 提出信息几何投影框架用于参数化贝叶斯联邦学习个性化，可平衡全局和局部性能且计算开销小。


<details>
  <summary>Details</summary>
Motivation: 现有贝叶斯联邦学习方法在数据异质性和隐私约束下，需更好平衡全局泛化和局部专业化。

Method: 提出信息几何投影框架，将全局模型投影到用户局部模型邻域，在变分学习中应用IVON优化器，并扩展到通用聚合方案。

Result: 在温和假设下投影步骤等价于在统计流形上计算重心，能得到封闭解和免费个性化，实证评估表明有效平衡全局和局部性能且计算开销小。

Conclusion: 所提方法能在贝叶斯联邦学习中有效平衡全局和局部性能，且计算成本低。

Abstract: Bayesian Federated Learning (BFL) combines uncertainty modeling with
decentralized training, enabling the development of personalized and reliable
models under data heterogeneity and privacy constraints. Existing approaches
typically rely on Markov Chain Monte Carlo (MCMC) sampling or variational
inference, often incorporating personalization mechanisms to better adapt to
local data distributions. In this work, we propose an information-geometric
projection framework for personalization in parametric BFL. By projecting the
global model onto a neighborhood of the user's local model, our method enables
a tunable trade-off between global generalization and local specialization.
Under mild assumptions, we show that this projection step is equivalent to
computing a barycenter on the statistical manifold, allowing us to derive
closed-form solutions and achieve cost-free personalization. We apply the
proposed approach to a variational learning setup using the Improved
Variational Online Newton (IVON) optimizer and extend its application to
general aggregation schemes in BFL. Empirical evaluations under heterogeneous
data distributions confirm that our method effectively balances global and
local performance with minimal computational overhead.

</details>


### [89] [BenchECG and xECG: a benchmark and baseline for ECG foundation models](https://arxiv.org/abs/2509.10151)
*Riccardo Lunelli,Angus Nicolson,Samuel Martin Pröll,Sebastian Johannes Reinstadler,Axel Bauer,Clemens Dlaska*

Main category: cs.LG

TL;DR: 本文介绍了标准化基准BenchECG和xLSTM模型xECG，xECG在BenchECG上表现最佳，BenchECG可加速心电图表征学习。


<details>
  <summary>Details</summary>
Motivation: 此前心电图基础模型评估缺乏一致性，阻碍公平比较，需标准化评估。

Method: 引入包含综合心电图数据集和多样任务的BenchECG基准，提出用SimDINOv2自监督学习训练的基于xLSTM的循环模型xECG。

Result: xECG在BenchECG上得分最佳，是唯一在所有数据集和任务上表现出色的公开模型。

Conclusion: BenchECG可实现严格比较，加速心电图表征学习进展，xECG定义了未来心电图基础模型的新基线。

Abstract: Electrocardiograms (ECGs) are inexpensive, widely used, and well-suited to
deep learning. Recently, interest has grown in developing foundation models for
ECGs - models that generalise across diverse downstream tasks. However,
consistent evaluation has been lacking: prior work often uses narrow task
selections and inconsistent datasets, hindering fair comparison. Here, we
introduce BenchECG, a standardised benchmark comprising a comprehensive suite
of publicly available ECG datasets and versatile tasks. We also propose xECG,
an xLSTM-based recurrent model trained with SimDINOv2 self-supervised learning,
which achieves the best BenchECG score compared to publicly available
state-of-the-art models. In particular, xECG is the only publicly available
model to perform strongly on all datasets and tasks. By standardising
evaluation, BenchECG enables rigorous comparison and aims to accelerate
progress in ECG representation learning. xECG achieves superior performance
over earlier approaches, defining a new baseline for future ECG foundation
models.

</details>


### [90] [Federated Multi-Agent Reinforcement Learning for Privacy-Preserving and Energy-Aware Resource Management in 6G Edge Networks](https://arxiv.org/abs/2509.10163)
*Francisco Javier Esono Nkulu Andong,Qi Min*

Main category: cs.LG

TL;DR: 本文提出Fed - MARL框架用于6G边缘网络资源管理，模拟显示其性能和隐私保护更好。


<details>
  <summary>Details</summary>
Motivation: 6G网络朝超密集、智能边缘环境发展，需在隐私、移动性和能源约束下进行高效资源管理。

Method: 引入Fed - MARL框架，结合MAC层和应用层跨层编排；各智能体用DRQN学习策略；用基于椭圆曲线Diffie Hellman密钥交换的安全聚合协议保护隐私；将资源管理问题建模为POMMDP并设多目标奖励函数。

Result: 模拟结果显示Fed - MARL在任务成功率、延迟、能源效率和公平性上优于集中式MARL和启发式基线。

Conclusion: Fed - MARL在动态、资源受限的6G边缘网络中能实现高效资源管理，有强大隐私保护和可扩展性。

Abstract: As sixth-generation (6G) networks move toward ultra-dense, intelligent edge
environments, efficient resource management under stringent privacy, mobility,
and energy constraints becomes critical. This paper introduces a novel
Federated Multi-Agent Reinforcement Learning (Fed-MARL) framework that
incorporates cross-layer orchestration of both the MAC layer and application
layer for energy-efficient, privacy-preserving, and real-time resource
management across heterogeneous edge devices. Each agent uses a Deep Recurrent
Q-Network (DRQN) to learn decentralized policies for task offloading, spectrum
access, and CPU energy adaptation based on local observations (e.g., queue
length, energy, CPU usage, and mobility). To protect privacy, we introduce a
secure aggregation protocol based on elliptic curve Diffie Hellman key
exchange, which ensures accurate model updates without exposing raw data to
semi-honest adversaries. We formulate the resource management problem as a
partially observable multi-agent Markov decision process (POMMDP) with a
multi-objective reward function that jointly optimizes latency, energy
efficiency, spectral efficiency, fairness, and reliability under 6G-specific
service requirements such as URLLC, eMBB, and mMTC. Simulation results
demonstrate that Fed-MARL outperforms centralized MARL and heuristic baselines
in task success rate, latency, energy efficiency, and fairness, while ensuring
robust privacy protection and scalability in dynamic, resource-constrained 6G
edge networks.

</details>


### [91] [A Symmetry-Integrated Approach to Surface Code Decoding](https://arxiv.org/abs/2509.10164)
*Hoshitaro Ohnishi,Hideo Mukai*

Main category: cs.LG

TL;DR: 本文提出通过神经网络插值的连续函数重新优化表面码解码器模型，评估显示重新优化的解码器精度更高，表明将表面码解码问题转化为深度学习可解决的回归问题是有用策略。


<details>
  <summary>Details</summary>
Motivation: 以往解码器因输入预测不唯一，仅能获取错误概率分布，需解决此问题。

Method: 用神经网络数学插值的连续函数近似综合征测量，重新优化解码器模型。

Result: 对不同码距和网络架构的解码器评估显示，重新优化的解码器精度高于原模型。

Conclusion: 将表面码解码问题转化为深度学习可处理的回归问题是有效策略。

Abstract: Quantum error correction, which utilizes logical qubits that are encoded as
redundant multiple physical qubits to find and correct errors in physical
qubits, is indispensable for practical quantum computing. Surface code is
considered to be a promising encoding method with a high error threshold that
is defined by stabilizer generators. However, previous methods have suffered
from the problem that the decoder acquires solely the error probability
distribution because of the non-uniqueness of correct prediction obtained from
the input. To circumvent this problem, we propose a technique to reoptimize the
decoder model by approximating syndrome measurements with a continuous function
that is mathematically interpolated by neural network. We evaluated the
improvement in accuracy of a multilayer perceptron based decoder for code
distances of 5 and 7 as well as for decoders based on convolutional and
recurrent neural networks and transformers for a code distance of 5. In all
cases, the reoptimized decoder gave better accuracy than the original models,
demonstrating the universal effectiveness of the proposed method that is
independent of code distance or network architecture. These results suggest
that re-framing the problem of surface code decoding into a regression problem
that can be tackled by deep learning is a useful strategy.

</details>


### [92] [The Hidden Width of Deep ResNets: Tight Error Bounds and Phase Diagrams](https://arxiv.org/abs/2509.10167)
*Lénaïc Chizat*

Main category: cs.LG

TL;DR: 研究大深度残差网络从标准随机初始化开始的基于梯度的训练，得出训练动态收敛到神经均值 ODE 训练动态，给出误差界，分析不同情况特征学习及残差尺度影响。


<details>
  <summary>Details</summary>
Motivation: 研究大深度残差网络从标准随机初始化开始的基于梯度训练的动态特性。

Method: 从新的数学视角，利用初始化随机性使残差网络前后向传播作为某些均值 ODE 的随机近似，通过混沌传播确保训练动态中的行为。

Result: 训练动态收敛到神经均值 ODE 训练动态，得到不同残差尺度下误差界，不同残差尺度对应不同特征学习情况。

Conclusion: 得出大深度残差网络训练动态的收敛结果，明确不同情况特征学习与残差尺度的关系。

Abstract: We study the gradient-based training of large-depth residual networks
(ResNets) from standard random initializations. We show that with a diverging
depth $L$, a fixed embedding dimension $D$, and an arbitrary hidden width $M$,
the training dynamics converges to a Neural Mean ODE training dynamics.
Remarkably, the limit is independent of the scaling of $M$, covering practical
cases of, say, Transformers, where $M$ (the number of hidden units or attention
heads per layer) is typically of the order of $D$. For a residual scale
$\Theta_D\big(\frac{\alpha}{LM}\big)$, we obtain the error bound
$O_D\big(\frac{1}{L}+ \frac{\alpha}{\sqrt{LM}}\big)$ between the model's output
and its limit after a fixed number gradient of steps, and we verify empirically
that this rate is tight. When $\alpha=\Theta(1)$, the limit exhibits complete
feature learning, i.e. the Mean ODE is genuinely non-linearly parameterized. In
contrast, we show that $\alpha \to \infty$ yields a \lazy ODE regime where the
Mean ODE is linearly parameterized. We then focus on the particular case of
ResNets with two-layer perceptron blocks, for which we study how these scalings
depend on the embedding dimension $D$. We show that for this model, the only
residual scale that leads to complete feature learning is
$\Theta\big(\frac{\sqrt{D}}{LM}\big)$. In this regime, we prove the error bound
$O\big(\frac{1}{L}+ \frac{\sqrt{D}}{\sqrt{LM}}\big)$ between the ResNet and its
limit after a fixed number of gradient steps, which is also empirically tight.
Our convergence results rely on a novel mathematical perspective on ResNets :
(i) due to the randomness of the initialization, the forward and backward pass
through the ResNet behave as the stochastic approximation of certain mean ODEs,
and (ii) by propagation of chaos (that is, asymptotic independence of the
units) this behavior is preserved through the training dynamics.

</details>


### [93] [P3D: Scalable Neural Surrogates for High-Resolution 3D Physics Simulations with Global Context](https://arxiv.org/abs/2509.10186)
*Benjamin Holzschuh,Georg Kohl,Florian Redinger,Nils Thuerey*

Main category: cs.LG

TL;DR: 提出用于3D物理模拟的可扩展学习框架，引入混合CNN - Transformer骨干架构，表现优，可预训练小补丁并融合，评估模型，展示高分辨率模拟及作为扩散模型的应用。


<details>
  <summary>Details</summary>
Motivation: 为高分辨率3D物理模拟学习确定性和概率性神经替代模型。

Method: 引入针对3D物理模拟的混合CNN - Transformer骨干架构，在小补丁上预训练，可融合获得全局解，可通过序列到序列模型引入长程依赖。

Result: 该架构在速度和准确性上显著优于现有架构，能在高分辨率数据集上以减少的内存和计算需求训练大规模模型，可扩展到高分辨率各向同性湍流模拟，作为扩散模型能准确捕捉流动统计。

Conclusion: 所提出的框架和架构有效且通用，适用于多种3D物理模拟场景。

Abstract: We present a scalable framework for learning deterministic and probabilistic
neural surrogates for high-resolution 3D physics simulations. We introduce a
hybrid CNN-Transformer backbone architecture targeted for 3D physics
simulations, which significantly outperforms existing architectures in terms of
speed and accuracy. Our proposed network can be pretrained on small patches of
the simulation domain, which can be fused to obtain a global solution,
optionally guided via a fast and scalable sequence-to-sequence model to include
long-range dependencies. This setup allows for training large-scale models with
reduced memory and compute requirements for high-resolution datasets. We
evaluate our backbone architecture against a large set of baseline methods with
the objective to simultaneously learn the dynamics of 14 different types of
PDEs in 3D. We demonstrate how to scale our model to high-resolution isotropic
turbulence with spatial resolutions of up to $512^3$. Finally, we demonstrate
the versatility of our network by training it as a diffusion model to produce
probabilistic samples of highly turbulent 3D channel flows across varying
Reynolds numbers, accurately capturing the underlying flow statistics.

</details>


### [94] [Hadamard-Riemannian Optimization for Margin-Variance Ensemble](https://arxiv.org/abs/2509.10189)
*Zexu Jin*

Main category: cs.LG

TL;DR: 本文提出新集成学习框架，将边缘方差纳入损失函数，优化负期望边缘及其方差，重参数化集成权重到单位球，实验表明优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统基于边缘的集成方法忽视边缘方差，限制模型泛化能力且易过拟合，在概率单纯形中优化集成权重存在计算效率和可扩展性问题。

Method: 引入新集成学习框架，将边缘方差纳入损失函数，联合优化负期望边缘及其方差，将集成权重重新参数化到单位球。

Result: 在多个基准数据集上的大量实验表明，所提方法始终优于传统基于边缘的集成技术。

Conclusion: 所提方法有效且具有实际应用价值。

Abstract: Ensemble learning has been widely recognized as a pivotal technique for
boosting predictive performance by combining multiple base models.
Nevertheless, conventional margin-based ensemble methods predominantly focus on
maximizing the expected margin while neglecting the critical role of margin
variance, which inherently restricts the generalization capability of the model
and heightens its vulnerability to overfitting, particularly in noisy or
imbalanced datasets. Additionally, the conventional approach of optimizing
ensemble weights within the probability simplex often introduces computational
inefficiency and scalability challenges, complicating its application to
large-scale problems. To tackle these limitations, this paper introduces a
novel ensemble learning framework that explicitly incorporates margin variance
into the loss function. Our method jointly optimizes the negative expected
margin and its variance, leading to enhanced robustness and improved
generalization performance. Moreover, by reparameterizing the ensemble weights
onto the unit sphere, we substantially simplify the optimization process and
improve computational efficiency. Extensive experiments conducted on multiple
benchmark datasets demonstrate that the proposed approach consistently
outperforms traditional margin-based ensemble techniques, underscoring its
effectiveness and practical utility.

</details>


### [95] [A Certifiable Machine Learning-Based Pipeline to Predict Fatigue Life of Aircraft Structures](https://arxiv.org/abs/2509.10227)
*Ángel Ladrón,Miguel Sánchez-Domínguez,Javier Rozalén,Fernando R. Sánchez,Javier de Vicente,Lucas Lacasa,Eusebio Valero,Gonzalo Rubio*

Main category: cs.LG

TL;DR: 本文提出基于机器学习的管道来估算飞机机翼不同位置疲劳寿命，可减少模拟成本和资源需求，且预测准确。


<details>
  <summary>Details</summary>
Motivation: 传统工程方法预测飞机疲劳寿命耗时且流程复杂，需要可靠精确的疲劳寿命预测器保证航空安全，机器学习可作为传统方法的补充。

Method: 提出基于机器学习的管道，根据飞机飞行参数估算机翼不同位置疲劳寿命。

Result: 在现实案例中验证管道，实现准确预测，进行统计验证和不确定性量化。

Conclusion: 该管道可作为传统方法的补充，减少昂贵模拟数量，降低计算和人力成本。

Abstract: Fatigue life prediction is essential in both the design and operational
phases of any aircraft, and in this sense safety in the aerospace industry
requires early detection of fatigue cracks to prevent in-flight failures.
Robust and precise fatigue life predictors are thus essential to ensure safety.
Traditional engineering methods, while reliable, are time consuming and involve
complex workflows, including steps such as conducting several Finite Element
Method (FEM) simulations, deriving the expected loading spectrum, and applying
cycle counting techniques like peak-valley or rainflow counting. These steps
often require collaboration between multiple teams and tools, added to the
computational time and effort required to achieve fatigue life predictions.
Machine learning (ML) offers a promising complement to traditional fatigue life
estimation methods, enabling faster iterations and generalization, providing
quick estimates that guide decisions alongside conventional simulations.
  In this paper, we present a ML-based pipeline that aims to estimate the
fatigue life of different aircraft wing locations given the flight parameters
of the different missions that the aircraft will be operating throughout its
operational life. We validate the pipeline in a realistic use case of fatigue
life estimation, yielding accurate predictions alongside a thorough statistical
validation and uncertainty quantification. Our pipeline constitutes a
complement to traditional methodologies by reducing the amount of costly
simulations and, thereby, lowering the required computational and human
resources.

</details>


### [96] [Prompt Injection Attacks on LLM Generated Reviews of Scientific Publications](https://arxiv.org/abs/2509.10248)
*Janis Keuper*

Main category: cs.LG

TL;DR: 本文研究作者使用隐藏提示注入操纵评审分数的可行性和技术成功率，评估显示简单提示注入效果显著且LLM评审普遍倾向接受。


<details>
  <summary>Details</summary>
Motivation: 当前科学同行评审过程中大量使用大语言模型，有作者使用隐藏提示注入操纵评审分数的情况，此现象对相关讨论有重大影响，故开展研究。

Method: 对2024年ICLR论文的1k条由多种大语言模型生成的评审进行系统评估。

Result: 非常简单的提示注入非常有效，接受率可达100%；大语言模型评审普遍倾向接受，许多模型接受率超95%。

Conclusion: 研究结果对当前同行评审中使用大语言模型的讨论有重大影响。

Abstract: The ongoing intense discussion on rising LLM usage in the scientific
peer-review process has recently been mingled by reports of authors using
hidden prompt injections to manipulate review scores. Since the existence of
such "attacks" - although seen by some commentators as "self-defense" - would
have a great impact on the further debate, this paper investigates the
practicability and technical success of the described manipulations. Our
systematic evaluation uses 1k reviews of 2024 ICLR papers generated by a wide
range of LLMs shows two distinct results: I) very simple prompt injections are
indeed highly effective, reaching up to 100% acceptance scores. II) LLM reviews
are generally biased toward acceptance (>95% in many models). Both results have
great impact on the ongoing discussions on LLM usage in peer-review.

</details>


### [97] [Property prediction for ionic liquids without prior structural knowledge using limited experimental data: A data-driven neural recommender system leveraging transfer learning](https://arxiv.org/abs/2509.10273)
*Sahil Sethi,Kai Sundmacher,Caroline Ganzer*

Main category: cs.LG

TL;DR: 提出数据驱动的迁移学习框架，用神经推荐系统结合模拟和实验数据对离子液体关键热物理性质预测，支持知识迁移，模型性能提升且能外推，可对大量组合预测。


<details>
  <summary>Details</summary>
Motivation: 离子液体虽可替代传统溶剂，但因化学设计空间大、实验数据有限，准确预测关键热物理性质具挑战性。

Method: 采用两阶段过程的迁移学习框架，先在固定温压的COSMO - RS模拟数据上预训练神经推荐系统模型，学习阴阳离子特定属性结构嵌入；再用不同温压的实验数据微调前馈神经网络。

Result: 框架支持属性内和跨属性知识迁移，用预训练的密度、粘度和热容模型微调五个目标属性模型，四个性能大幅提升，模型能外推到未见离子液体，可对超700,000种组合预测。

Conclusion: 结合模拟数据和迁移学习能克服实验数据稀疏问题，有效预测离子液体属性。

Abstract: Ionic liquids (ILs) have emerged as versatile replacements for traditional
solvents because their physicochemical properties can be precisely tailored to
various applications. However, accurately predicting key thermophysical
properties remains challenging due to the vast chemical design space and the
limited availability of experimental data. In this study, we present a
data-driven transfer learning framework that leverages a neural recommender
system (NRS) to enable reliable property prediction for ILs using sparse
experimental datasets. The approach involves a two-stage process: first,
pre-training NRS models on COSMO-RS-based simulated data at fixed temperature
and pressure to learn property-specific structural embeddings for cations and
anions; and second, fine-tuning simple feedforward neural networks using these
embeddings with experimental data at varying temperatures and pressures. In
this work, five essential IL properties are considered: density, viscosity,
surface tension, heat capacity, and melting point. The framework supports both
within-property and cross-property knowledge transfer. Notably, pre-trained
models for density, viscosity, and heat capacity are used to fine-tune models
for all five target properties, achieving improved performance by a substantial
margin for four of them. The model exhibits robust extrapolation to previously
unseen ILs. Moreover, the final trained models enable property prediction for
over 700,000 IL combinations, offering a scalable solution for IL screening in
process design. This work highlights the effectiveness of combining simulated
data and transfer learning to overcome sparsity in the experimental data.

</details>


### [98] [Proof of AutoML: SDN based Secure Energy Trading with Blockchain in Disaster Case](https://arxiv.org/abs/2509.10291)
*Salih Toprak,Muge Erel-Ozcevik*

Main category: cs.LG

TL;DR: 研究提出基于SDN架构，利用机器学习回归器生成随机值作为区块链交易的随机数，评估多个模型随机特性，发现部分模型可作为有效随机数生成器。


<details>
  <summary>Details</summary>
Motivation: 在传统能源基础设施受损的灾难场景下，确保太阳能家庭与移动充电单元在区块链网络上的能源交易完整性，需要强大且不可预测的随机数生成。

Method: 提出SDN架构，使用AutoML选择五个回归模型，通过打乱数据输入评估其产生多样化和非确定性输出的能力。

Result: 随机森林和极端随机树回归器完全依赖随机性，梯度提升、K近邻和LightGBM有较强但稍低的随机性得分。

Conclusion: 某些机器学习模型，尤其是基于树的集成模型，可作为适用于灾难条件下基于区块链和SDN的能源交易基础设施的有效轻量级随机数生成器。

Abstract: In disaster scenarios where conventional energy infrastructure is
compromised, secure and traceable energy trading between solar-powered
households and mobile charging units becomes a necessity. To ensure the
integrity of such transactions over a blockchain network, robust and
unpredictable nonce generation is vital. This study proposes an SDN-enabled
architecture where machine learning regressors are leveraged not for their
accuracy, but for their potential to generate randomized values suitable as
nonce candidates. Therefore, it is newly called Proof of AutoML. Here, SDN
allows flexible control over data flows and energy routing policies even in
fragmented or degraded networks, ensuring adaptive response during emergencies.
Using a 9000-sample dataset, we evaluate five AutoML-selected regression models
- Gradient Boosting, LightGBM, Random Forest, Extra Trees, and K-Nearest
Neighbors - not by their prediction accuracy, but by their ability to produce
diverse and non-deterministic outputs across shuffled data inputs. Randomness
analysis reveals that Random Forest and Extra Trees regressors exhibit complete
dependency on randomness, whereas Gradient Boosting, K-Nearest Neighbors and
LightGBM show strong but slightly lower randomness scores (97.6%, 98.8% and
99.9%, respectively). These findings highlight that certain machine learning
models, particularly tree-based ensembles, may serve as effective and
lightweight nonce generators within blockchain-secured, SDN-based energy
trading infrastructures resilient to disaster conditions.

</details>


### [99] [Generalizing Beyond Suboptimality: Offline Reinforcement Learning Learns Effective Scheduling through Random Data](https://arxiv.org/abs/2509.10303)
*Jesse van Remmerden,Zaharah Bukhsh,Yingqian Zhang*

Main category: cs.LG

TL;DR: 提出离线强化学习算法CDQAC解决作业车间调度问题，能从历史数据学习，实验显示其性能和样本效率佳。


<details>
  <summary>Details</summary>
Motivation: 现有在线强化学习方法需大量模拟环境交互且样本效率低，无法捕捉现实复杂性。

Method: 引入CDQAC算法，结合基于分位数的评论家和延迟策略更新，估计机器 - 操作对的回报分布。

Result: CDQAC能从不同数据源学习，优于原启发式方法和现有基线，样本效率高，用随机启发式生成数据训练效果更好。

Conclusion: CDQAC是解决作业车间调度问题的有效算法，可直接从历史数据学习，提高样本效率。

Abstract: The Job-Shop Scheduling Problem (JSP) and Flexible Job-Shop Scheduling
Problem (FJSP), are canonical combinatorial optimization problems with
wide-ranging applications in industrial operations. In recent years, many
online reinforcement learning (RL) approaches have been proposed to learn
constructive heuristics for JSP and FJSP. Although effective, these online RL
methods require millions of interactions with simulated environments that may
not capture real-world complexities, and their random policy initialization
leads to poor sample efficiency. To address these limitations, we introduce
Conservative Discrete Quantile Actor-Critic (CDQAC), a novel offline RL
algorithm that learns effective scheduling policies directly from historical
data, eliminating the need for costly online interactions, while maintaining
the ability to improve upon suboptimal training data. CDQAC couples a
quantile-based critic with a delayed policy update, estimating the return
distribution of each machine-operation pair rather than selecting pairs
outright. Our extensive experiments demonstrate CDQAC's remarkable ability to
learn from diverse data sources. CDQAC consistently outperforms the original
data-generating heuristics and surpasses state-of-the-art offline and online RL
baselines. In addition, CDQAC is highly sample efficient, requiring only 10-20
training instances to learn high-quality policies. Surprisingly, we find that
CDQAC performs better when trained on data generated by a random heuristic than
when trained on higher-quality data from genetic algorithms and priority
dispatching rules.

</details>


### [100] [GraphCSVAE: Graph Categorical Structured Variational Autoencoder for Spatiotemporal Auditing of Physical Vulnerability Towards Sustainable Post-Disaster Risk Reduction](https://arxiv.org/abs/2509.10308)
*Joshua Dimasaka,Christian Geiß,Robert Muir-Wood,Emily So*

Main category: cs.LG

TL;DR: 文章提出GraphCSVAE框架用于建模物理脆弱性，分析两个受灾地区，揭示灾后区域物理脆弱性动态，为减灾提供见解。


<details>
  <summary>Details</summary>
Motivation: 全球许多机构在持续监测灾害风险变化方面存在挑战，且在建模风险方程中物理脆弱性这一重要元素上进展有限。

Method: 引入GraphCSVAE框架，结合深度学习、图表示和分类概率推理，利用卫星时间序列数据集和专家先验信念系统，采用弱监督一阶转移矩阵。

Result: 揭示了两个受灾地区灾后物理脆弱性的区域动态。

Conclusion: 研究为灾后风险的本地化时空审计和可持续减灾策略提供了有价值的见解。

Abstract: In the aftermath of disasters, many institutions worldwide face challenges in
continually monitoring changes in disaster risk, limiting the ability of key
decision-makers to assess progress towards the UN Sendai Framework for Disaster
Risk Reduction 2015-2030. While numerous efforts have substantially advanced
the large-scale modeling of hazard and exposure through Earth observation and
data-driven methods, progress remains limited in modeling another equally
important yet challenging element of the risk equation: physical vulnerability.
To address this gap, we introduce Graph Categorical Structured Variational
Autoencoder (GraphCSVAE), a novel probabilistic data-driven framework for
modeling physical vulnerability by integrating deep learning, graph
representation, and categorical probabilistic inference, using time-series
satellite-derived datasets and prior expert belief systems. We introduce a
weakly supervised first-order transition matrix that reflects the changes in
the spatiotemporal distribution of physical vulnerability in two
disaster-stricken and socioeconomically disadvantaged areas: (1) the
cyclone-impacted coastal Khurushkul community in Bangladesh and (2) the
mudslide-affected city of Freetown in Sierra Leone. Our work reveals
post-disaster regional dynamics in physical vulnerability, offering valuable
insights into localized spatiotemporal auditing and sustainable strategies for
post-disaster risk reduction.

</details>


### [101] [ARMA Block: A CNN-Based Autoregressive and Moving Average Module for Long-Term Time Series Forecasting](https://arxiv.org/abs/2509.10324)
*Myung Jin Kim,YeongHyeon Park,Il Dong Yun*

Main category: cs.LG

TL;DR: 提出用于长期时间序列预测的卷积模块ARMA，实验显示有竞争力的准确性且架构简单，还可能替代位置嵌入。


<details>
  <summary>Details</summary>
Motivation: 为长期时间序列预测设计简单有效的卷积模块。

Method: 受ARIMA模型启发，设计包含捕捉趋势和细化局部变化的两个卷积组件的模块，直接进行多步预测。

Result: 在九个基准数据集上实验，ARMA在强趋势变化数据集上有竞争力的准确性，且架构简单。

Conclusion: 该模块有潜力作为顺序模型中位置嵌入的轻量级替代。

Abstract: This paper proposes a simple yet effective convolutional module for long-term
time series forecasting. The proposed block, inspired by the Auto-Regressive
Integrated Moving Average (ARIMA) model, consists of two convolutional
components: one for capturing the trend (autoregression) and the other for
refining local variations (moving average). Unlike conventional ARIMA, which
requires iterative multi-step forecasting, the block directly performs
multi-step forecasting, making it easily extendable to multivariate settings.
Experiments on nine widely used benchmark datasets demonstrate that our method
ARMA achieves competitive accuracy, particularly on datasets exhibiting strong
trend variations, while maintaining architectural simplicity. Furthermore,
analysis shows that the block inherently encodes absolute positional
information, suggesting its potential as a lightweight replacement for
positional embeddings in sequential models.

</details>


### [102] [Physics-informed sensor coverage through structure preserving machine learning](https://arxiv.org/abs/2509.10363)
*Benjamin David Shaffer,Brooks Kinch,Joseph Klobusicky,M. Ani Hsieh,Nathaniel Trask*

Main category: cs.LG

TL;DR: 提出用于自适应源定位的机器学习框架，利用结构保留数字孪生体，结合CNWF和算子学习，采用交错方案，实验表明结构保留对源识别有效。


<details>
  <summary>Details</summary>
Motivation: 实现耦合水动力 - 传输系统的自适应源定位。

Method: 构建基于条件神经Whitney形式（CNWF）的数字孪生体，结合有限元外微积分（FEEC）和基于Transformer的算子学习，采用交错方案交替评估数字孪生体和应用Lloyd算法。

Result: 诱导的降阶环境模型保留稳定性和一致性，能从传感器数据到源场实现物理可实现的规则映射，实验显示在复杂几何形状中精度更高。

Conclusion: 结构保留为源识别提供了有效的归纳偏置。

Abstract: We present a machine learning framework for adaptive source localization in
which agents use a structure-preserving digital twin of a coupled
hydrodynamic-transport system for real-time trajectory planning and data
assimilation. The twin is constructed with conditional neural Whitney forms
(CNWF), coupling the numerical guarantees of finite element exterior calculus
(FEEC) with transformer-based operator learning. The resulting model preserves
discrete conservation, and adapts in real time to streaming sensor data. It
employs a conditional attention mechanism to identify: a reduced Whitney-form
basis; reduced integral balance equations; and a source field, each compatible
with given sensor measurements. The induced reduced-order environmental model
retains the stability and consistency of standard finite-element simulation,
yielding a physically realizable, regular mapping from sensor data to the
source field. We propose a staggered scheme that alternates between evaluating
the digital twin and applying Lloyd's algorithm to guide sensor placement, with
analysis providing conditions for monotone improvement of a coverage
functional. Using the predicted source field as an importance function within
an optimal-recovery scheme, we demonstrate recovery of point sources under
continuity assumptions, highlighting the role of regularity as a sufficient
condition for localization. Experimental comparisons with physics-agnostic
transformer architectures show improved accuracy in complex geometries when
physical constraints are enforced, indicating that structure preservation
provides an effective inductive bias for source identification.

</details>


### [103] [A Discrepancy-Based Perspective on Dataset Condensation](https://arxiv.org/abs/2509.10367)
*Tong Chen,Raghavendra Selvan*

Main category: cs.LG

TL;DR: 提出统一框架涵盖现有数据集浓缩方法，扩展其定义，包容更多目标。


<details>
  <summary>Details</summary>
Motivation: 现有数据集浓缩方法与数据分布近似问题相关，需扩展任务特定概念到更通用正式定义。

Method: 用差异概念构建统一框架，量化不同状态下概率分布距离。

Result: 提出统一框架，拓宽数据集浓缩目标，超越泛化性。

Conclusion: 该框架能容纳鲁棒性、隐私性等更多目标。

Abstract: Given a dataset of finitely many elements $\mathcal{T} = \{\mathbf{x}_i\}_{i
= 1}^N$, the goal of dataset condensation (DC) is to construct a synthetic
dataset $\mathcal{S} = \{\tilde{\mathbf{x}}_j\}_{j = 1}^M$ which is
significantly smaller ($M \ll N$) such that a model trained from scratch on
$\mathcal{S}$ achieves comparable or even superior generalization performance
to a model trained on $\mathcal{T}$. Recent advances in DC reveal a close
connection to the problem of approximating the data distribution represented by
$\mathcal{T}$ with a reduced set of points. In this work, we present a unified
framework that encompasses existing DC methods and extend the task-specific
notion of DC to a more general and formal definition using notions of
discrepancy, which quantify the distance between probability distribution in
different regimes. Our framework broadens the objective of DC beyond
generalization, accommodating additional objectives such as robustness,
privacy, and other desirable properties.

</details>


### [104] [Data distribution impacts the performance and generalisability of contrastive learning-based foundation models of electrocardiograms](https://arxiv.org/abs/2509.10369)
*Gul Rukh Khattak,Konstantinos Patlatzoglou,Joseph Barker,Libor Pastika,Boroumand Zeidaabadi,Ahmed El-Medany,Hesham Aggour,Yixiu Liang,Antonio H. Ribeiro,Jeffrey Annis,Antonio Luiz Pinho Ribeiro,Junbo Ge,Daniel B. Kramer,Jonathan W. Waks,Evan Brittain,Nicholas Peters,Fu Siong Ng,Arunashis Sau*

Main category: cs.LG

TL;DR: 提出CAPE基础模型，在多队列预训练，研究队列因素对下游性能影响，发现多中心多样队列问题并提出IDB策略提升泛化性。


<details>
  <summary>Details</summary>
Motivation: 探索对比学习对队列组成的依赖，开发临床公平且可泛化的基础模型。

Method: 提出CAPE模型，在四个队列预训练，评估多因素对下游性能影响，提出IDB策略。

Result: 下游性能依赖预训练队列分布特性，多中心多样队列降低OOD泛化性。

Conclusion: IDB策略可增强OOD鲁棒性，为开发基础模型提供重要见解。

Abstract: Contrastive learning is a widely adopted self-supervised pretraining
strategy, yet its dependence on cohort composition remains underexplored. We
present Contrasting by Patient Augmented Electrocardiograms (CAPE) foundation
model and pretrain on four cohorts (n = 5,203,352), from diverse populations
across three continents (North America, South America, Asia). We systematically
assess how cohort demographics, health status, and population diversity
influence the downstream performance for prediction tasks also including two
additional cohorts from another continent (Europe). We find that downstream
performance depends on the distributional properties of the pretraining cohort,
including demographics and health status. Moreover, while pretraining with a
multi-centre, demographically diverse cohort improves in-distribution accuracy,
it reduces out-of-distribution (OOD) generalisation of our contrastive approach
by encoding cohort-specific artifacts. To address this, we propose the
In-Distribution Batch (IDB) strategy, which preserves intra-cohort consistency
during pretraining and enhances OOD robustness. This work provides important
insights for developing clinically fair and generalisable foundation models.

</details>


### [105] [Vendi Information Gain for Active Learning and its Application to Ecology](https://arxiv.org/abs/2509.10390)
*Quan Nguyen,Adji Bousso Dieng*

Main category: cs.LG

TL;DR: 提出新主动学习策略VIG用于相机陷阱图像物种识别，用不到10%标签达接近全监督准确率，具广泛适用性。


<details>
  <summary>Details</summary>
Motivation: 相机陷阱监测生物多样性时图像物种识别因标注资源有限成瓶颈，现有主动学习未考虑全数据集不确定性。

Method: 引入基于图像对全数据集预测不确定性影响来选择图像的新主动学习策略Vendi信息增益（VIG）。

Result: 在Snapshot Serengeti数据集上，VIG用不到10%标签达接近全监督的预测准确率，各指标和批量大小下均超标准基线，在特征空间收集更多样数据。

Conclusion: VIG有广泛适用性，对数据有限环境下生物多样性监测有价值。

Abstract: While monitoring biodiversity through camera traps has become an important
endeavor for ecological research, identifying species in the captured image
data remains a major bottleneck due to limited labeling resources. Active
learning -- a machine learning paradigm that selects the most informative data
to label and train a predictive model -- offers a promising solution, but
typically focuses on uncertainty in the individual predictions without
considering uncertainty across the entire dataset. We introduce a new active
learning policy, Vendi information gain (VIG), that selects images based on
their impact on dataset-wide prediction uncertainty, capturing both
informativeness and diversity. Applied to the Snapshot Serengeti dataset, VIG
achieves impressive predictive accuracy close to full supervision using less
than 10% of the labels. It consistently outperforms standard baselines across
metrics and batch sizes, collecting more diverse data in the feature space. VIG
has broad applicability beyond ecology, and our results highlight its value for
biodiversity monitoring in data-limited environments.

</details>


### [106] [Inpainting-Guided Policy Optimization for Diffusion Large Language Models](https://arxiv.org/abs/2509.10396)
*Siyan Zhao,Mengchen Liu,Jing Huang,Miao Liu,Chenyu Wang,Bo Liu,Yuandong Tian,Guan Pang,Sean Bell,Aditya Grover,Feiyu Chen*

Main category: cs.LG

TL;DR: 探索用掩码扩散大语言模型（dLLMs）的修复能力指导强化学习算法设计，提出IGPO框架，结合额外技术在数学基准测试中取得新的最优结果。


<details>
  <summary>Details</summary>
Motivation: 大语言模型与强化学习结合存在探索挑战，dLLMs的修复能力提供了独特机会。

Method: 引入IGPO框架，在在线采样时插入部分真实推理轨迹；提出对合成重写的简洁轨迹进行监督微调，结合基于熵的过滤等技术。

Result: 在GSM8K、Math500和AMC三个数学基准测试中取得显著提升，为全注意力掩码dLLMs创造新的最优结果。

Conclusion: 利用dLLMs的修复能力指导强化学习算法设计是有效的，所提出的方法能提高样本效率和模型性能。

Abstract: Masked diffusion large language models (dLLMs) are emerging as promising
alternatives to autoregressive LLMs, offering competitive performance while
supporting unique generation capabilities such as inpainting. We explore how
inpainting can inform RL algorithm design for dLLMs. Aligning LLMs with
reinforcement learning faces an exploration challenge: sparse reward signals
and sample waste when models fail to discover correct solutions. While this
inefficiency affects LLMs broadly, dLLMs offer a distinctive opportunity--their
inpainting ability can guide exploration. We introduce IGPO (Inpainting Guided
Policy Optimization), an RL framework that strategically inserts partial
ground-truth reasoning traces during online sampling. Unlike providing full
solutions, inpainting steers exploration toward promising trajectory spaces
while preserving self-generated reasoning, bridging supervised fine-tuning and
reinforcement learning. We apply IGPO to group-based optimization methods such
as GRPO, where exploration failures cause zero advantages and gradients. IGPO
restores meaningful gradients while improving sample efficiency. We also
propose supervised fine-tuning on synthetically rewritten concise traces that
better align with dLLM generation patterns. With additional techniques
including entropy-based filtering, our training recipe yields substantial gains
across three mathematical benchmarks--GSM8K, Math500, and AMC--achieving new
state-of-the-art results for full-attention masked dLLMs.

</details>


### [107] [Multipole Semantic Attention: A Fast Approximation of Softmax Attention for Pretraining](https://arxiv.org/abs/2509.10406)
*Rupert Mitchell,Kristian Kersting*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We present Multipole Semantic Attention (MuSe), an efficient approximation of
softmax attention that combines semantic clustering with multipole expansions
from computational physics. Our method addresses the quadratic computational
complexity of transformers in the context length by clustering queries and keys
separately in their learned representation spaces, enabling a hierarchical
two-stage attention mechanism. Unlike prior clustering approaches that group
only keys or use unified clustering, we maintain separate clusterings that
respect attention's asymmetric treatment of these spaces. We augment
centroid-based (monopole) approximations with dipole corrections that capture
directional variance within clusters, preserving richer information during
training. The method operates as a drop-in replacement for standard attention,
requiring only hyperparameter specification without architectural
modifications. Our approach achieves $\mathcal{O}(NCD)$ complexity for acausal
attention with $C$ clusters and $\mathcal{O}(NCD \log N)$ for causal attention.
On isolated attention layers, we demonstrate $3\times$ speedup over CUDNN Flash
Attention at 8k context length, with relative squared errors below 20%. For
causal attention, we develop a hierarchical block decomposition that combines
exact local computation with efficient long-range approximation. In end-to-end
pretraining of a 30M parameter model on book-length texts with 16k context, we
achieve 12.2% runtime reduction with only 0.36% loss degradation, establishing
the viability of multipole approximations for efficient transformer
pretraining.

</details>


### [108] [Run-Time Monitoring of ERTMS/ETCS Control Flow by Process Mining](https://arxiv.org/abs/2509.10419)
*Francesco Vitale,Tommaso Zoppi,Francesco Flammini,Nicola Mazzocca*

Main category: cs.LG

TL;DR: 本文探索使用过程挖掘进行运行时控制流异常检测，以增强ERTMS/ETCS L2的弹性，并通过无监督机器学习进行异常定位，在RBC/RBC交接场景测试效果良好。


<details>
  <summary>Details</summary>
Motivation: 计算机铁路系统日益复杂关键，虽有严格验证流程，但运行时仍可能出现异常，需要增强系统弹性。

Method: 使用过程挖掘学习系统实际控制流，通过在线一致性检查进行运行时监控；利用无监督机器学习进行异常定位。

Result: 在RBC/RBC交接场景中，该方法能高精度、高效且可解释地检测和定位异常。

Conclusion: 提出的运行时控制流异常检测和定位方法可有效增强ERTMS/ETCS L2的弹性。

Abstract: Ensuring the resilience of computer-based railways is increasingly crucial to
account for uncertainties and changes due to the growing complexity and
criticality of those systems. Although their software relies on strict
verification and validation processes following well-established best-practices
and certification standards, anomalies can still occur at run-time due to
residual faults, system and environmental modifications that were unknown at
design-time, or other emergent cyber-threat scenarios. This paper explores
run-time control-flow anomaly detection using process mining to enhance the
resilience of ERTMS/ETCS L2 (European Rail Traffic Management System / European
Train Control System Level 2). Process mining allows learning the actual
control flow of the system from its execution traces, thus enabling run-time
monitoring through online conformance checking. In addition, anomaly
localization is performed through unsupervised machine learning to link
relevant deviations to critical system components. We test our approach on a
reference ERTMS/ETCS L2 scenario, namely the RBC/RBC Handover, to show its
capability to detect and localize anomalies with high accuracy, efficiency, and
explainability.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [109] [LLM-Based Instance-Driven Heuristic Bias In the Context of a Biased Random Key Genetic Algorithm](https://arxiv.org/abs/2509.09707)
*Camilo Chacón Sartori,Martín Isla Pino,Pedro Pinacho-Davidson,Christian Blum*

Main category: cs.NE

TL;DR: 本文提出将大语言模型与偏置随机键遗传算法集成的框架解决最长游程子序列问题，实验表明该混合方法有显著改进。


<details>
  <summary>Details</summary>
Motivation: 现有集成大语言模型解决组合优化问题的方法常忽略问题实例结构特性，需要新方法。

Method: 引入人类 - 大语言模型协作流程，设计高效度量，大语言模型分析度量生成启发式偏差引导算法搜索。

Result: 顶级混合方法 BRKGA+Llama - 4 - Maverick 相比基线有显著改进，尤其在复杂实例上。

Conclusion: 利用大语言模型产生实例驱动的启发式偏差能有效增强复杂优化领域的元启发式算法。

Abstract: Integrating Large Language Models (LLMs) within metaheuristics opens a novel
path for solving complex combinatorial optimization problems. While most
existing approaches leverage LLMs for code generation to create or refine
specific heuristics, they often overlook the structural properties of
individual problem instances. In this work, we introduce a novel framework that
integrates LLMs with a Biased Random-Key Genetic Algorithm (BRKGA) to solve the
NP-hard Longest Run Subsequence problem. Our approach extends the
instance-driven heuristic bias paradigm by introducing a human-LLM
collaborative process to co-design and implement a set of computationally
efficient metrics. The LLM analyzes these instance-specific metrics to generate
a tailored heuristic bias, which steers the BRKGA toward promising areas of the
search space. We conduct a comprehensive experimental evaluation, including
rigorous statistical tests, convergence and behavioral analyses, and targeted
ablation studies, comparing our method against a standard BRKGA baseline across
1,050 generated instances of varying complexity. Results show that our
top-performing hybrid, BRKGA+Llama-4-Maverick, achieves statistically
significant improvements over the baseline, particularly on the most complex
instances. Our findings confirm that leveraging an LLM to produce an a priori,
instance-driven heuristic bias is a valuable approach for enhancing
metaheuristics in complex optimization domains.

</details>


### [110] [Predictive Spike Timing Enables Distributed Shortest Path Computation in Spiking Neural Networks](https://arxiv.org/abs/2509.10077)
*Simen Storesund,Kristian Valset Aars,Robin Dietrich,Nicolai Waniek*

Main category: cs.NE

TL;DR: 提出生物可行的最短路径计算算法，通过局部基于脉冲的消息传递实现，经证明和模拟有效，为理解生物和人工系统分布式计算提供新方向。


<details>
  <summary>Details</summary>
Motivation: 现有高效规划和序列选择方法与生物计算不兼容，经典图算法和强化学习方法存在不足。

Method: 提出基于局部脉冲消息传递的算法，利用脉冲时间巧合识别最优路径节点，通过时间压缩从目标向源传播。

Result: 通过分析证明和随机空间网络模拟，算法收敛并能发现所有最短路径。

Conclusion: 展示短期时间动态可计算最短路径，为生物和人工系统分布式计算提供新见解，对多个领域有潜在影响。

Abstract: Efficient planning and sequence selection are central to intelligence, yet
current approaches remain largely incompatible with biological computation.
Classical graph algorithms like Dijkstra's or A* require global state and
biologically implausible operations such as backtracing, while reinforcement
learning methods rely on slow gradient-based policy updates that appear
inconsistent with rapid behavioral adaptation observed in natural systems.
  We propose a biologically plausible algorithm for shortest-path computation
that operates through local spike-based message-passing with realistic
processing delays. The algorithm exploits spike-timing coincidences to identify
nodes on optimal paths: Neurons that receive inhibitory-excitatory message
pairs earlier than predicted reduce their response delays, creating a temporal
compression that propagates backwards from target to source. Through analytical
proof and simulations on random spatial networks, we demonstrate that the
algorithm converges and discovers all shortest paths using purely timing-based
mechanisms. By showing how short-term timing dynamics alone can compute
shortest paths, this work provides new insights into how biological networks
might solve complex computational problems through purely local computation and
relative spike-time prediction. These findings open new directions for
understanding distributed computation in biological and artificial systems,
with possible implications for computational neuroscience, AI, reinforcement
learning, and neuromorphic systems.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [111] [eHashPipe: Lightweight Top-K and Per-PID Resource Monitoring with eBPF](https://arxiv.org/abs/2509.09879)
*Yuanjun Dai,Qingzhe Guo,Xiangren Wang*

Main category: cs.PF

TL;DR: 本文介绍轻量级实时资源可观测系统eHashPipe，利用eBPF和HashPipe算法，实验显示其精度高、开销低，适用于云边环境。


<details>
  <summary>Details</summary>
Motivation: 解决系统级资源监控在精度和效率方面的持续挑战。

Method: 引入eHashPipe系统，支持Top - k和特定PID跟踪模式，实现两个内核eBPF管道用于监控CPU时间和内存使用。

Result: 实验中，eHashPipe在不同k值下对CPU和内存的Top - k精度高，时间分辨率比top工具高约14倍，开销极低。

Conclusion: eHashPipe能提供准确、及时的洞察，对系统影响小，适合对延迟敏感的云边环境。

Abstract: System-level resource monitoring with both precision and efficiency is a
continuous challenge. We introduce eHashPipe, a lightweight, real-time resource
observability system utilizing eBPF and the HashPipe sketching algorithm.
eHashPipe supports two tracking modes: Top-k monitoring to identify the most
resource-demanding processes and specific PID tracking to detail the behavior
of selected processes. We implement two in-kernel eBPF pipelines for on-CPU
time and memory usage. Unlike traditional userspace polling tools, eHashPipe
operates in the kernel to reduce latency and context-switch overhead while
keeping the runtime footprint small. During our experiments, eHashPipe attains
100 percent Top-k precision for CPU and memory at k = 1, 5, and 10, 95.0/90.0
percent at k = 20, and 93.3/83.3 percent at k = 30 compared to the ground
truth. It exposes short-lived bursts with about 14 times finer temporal
resolution than top while imposing very low overhead. These results show that
eHashPipe delivers accurate, responsive insight with minimal impact, making it
well suited for latency-sensitive cloud and edge environments.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [112] [SWE-Effi: Re-Evaluating Software AI Agent System Effectiveness Under Resource Constraints](https://arxiv.org/abs/2509.09853)
*Zhiyu Fan,Kirill Vasilevski,Dayi Lin,Boyuan Chen,Yihao Chen,Zhiqing Zhong,Jie M. Zhang,Pinjia He,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: 现有软件工程AI排行榜仅关注准确性，本文引入SWE - Effi指标重新评估AI系统有效性，发现系统有效性关键在于与基础模型集成，还指出‘token snowball’等问题及预算间权衡。


<details>
  <summary>Details</summary>
Motivation: 现有软件工程AI排行榜只关注解决方案准确性，忽略资源受限环境下的有效性，需要新指标评估AI系统。

Method: 引入SWE - Effi指标，在SWE - bench基准的子集上对流行AI系统进行重新排名。

Result: 发现AI系统有效性取决于与基础模型的集成，识别出‘token snowball’和‘expensive failures’等问题，观察到不同预算下有效性的权衡。

Conclusion: AI系统不仅要结果准确，还需资源高效，新指标有助于在资源受限环境下评估和改进AI系统，对项目预算管理和强化学习有重要意义。

Abstract: The advancement of large language models (LLMs) and code agents has
demonstrated significant potential to assist software engineering (SWE) tasks,
such as autonomous issue resolution and feature addition. Existing AI for
software engineering leaderboards (e.g., SWE-bench) focus solely on solution
accuracy, ignoring the crucial factor of effectiveness in a
resource-constrained world. This is a universal problem that also exists beyond
software engineering tasks: any AI system should be more than correct - it must
also be cost-effective. To address this gap, we introduce SWE-Effi, a set of
new metrics to re-evaluate AI systems in terms of holistic effectiveness
scores. We define effectiveness as the balance between the accuracy of outcome
(e.g., issue resolve rate) and the resources consumed (e.g., token and time).
In this paper, we specifically focus on the software engineering scenario by
re-ranking popular AI systems for issue resolution on a subset of the SWE-bench
benchmark using our new multi-dimensional metrics. We found that AI system's
effectiveness depends not just on the scaffold itself, but on how well it
integrates with the base model, which is key to achieving strong performance in
a resource-efficient manner. We also identified systematic challenges such as
the "token snowball" effect and, more significantly, a pattern of "expensive
failures". In these cases, agents consume excessive resources while stuck on
unsolvable tasks - an issue that not only limits practical deployment but also
drives up the cost of failed rollouts during RL training. Lastly, we observed a
clear trade-off between effectiveness under the token budget and effectiveness
under the time budget, which plays a crucial role in managing project budgets
and enabling scalable reinforcement learning, where fast responses are
essential.

</details>


### [113] [From Hugging Face to GitHub: Tracing License Drift in the Open-Source AI Ecosystem](https://arxiv.org/abs/2509.09873)
*James Jewitt,Hao Li,Bram Adams,Gopi Krishnan Rajbahadur,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: 对Hugging Face上数据集、模型及其下游应用进行许可证审计，发现系统合规问题，开发规则引擎检测冲突，发布数据和引擎助力开源AI许可证合规。


<details>
  <summary>Details</summary>
Motivation: 开源AI生态中隐藏的许可证冲突存在法律和道德风险，但缺乏数据驱动的理解，需了解冲突发生频率、来源和受影响群体。

Method: 对Hugging Face上的数据集、模型及下游应用进行端到端审计，开发可扩展规则引擎编码近200个条款检测冲突。

Result: 35.5%的模型到应用转换通过重新许可消除限制性条款，规则引擎能解决86.4%的软件应用许可证冲突。

Conclusion: 许可证合规是开源AI的关键治理挑战，本研究提供了数据和工具以实现大规模自动化合规。

Abstract: Hidden license conflicts in the open-source AI ecosystem pose serious legal
and ethical risks, exposing organizations to potential litigation and users to
undisclosed risk. However, the field lacks a data-driven understanding of how
frequently these conflicts occur, where they originate, and which communities
are most affected. We present the first end-to-end audit of licenses for
datasets and models on Hugging Face, as well as their downstream integration
into open-source software applications, covering 364 thousand datasets, 1.6
million models, and 140 thousand GitHub projects. Our empirical analysis
reveals systemic non-compliance in which 35.5% of model-to-application
transitions eliminate restrictive license clauses by relicensing under
permissive terms. In addition, we prototype an extensible rule engine that
encodes almost 200 SPDX and model-specific clauses for detecting license
conflicts, which can solve 86.4% of license conflicts in software applications.
To support future research, we release our dataset and the prototype engine.
Our study highlights license compliance as a critical governance challenge in
open-source AI and provides both the data and tools necessary to enable
automated, AI-aware compliance at scale.

</details>


### [114] [SLD-Spec: Enhancement LLM-assisted Specification Generation for Complex Loop Functions via Program Slicing and Logical Deletion](https://arxiv.org/abs/2509.09917)
*Zehan Chen,Long Zhang,Zhiwei Zhang,JingJing Zhang,Ruoyu Zhou,Yulong Shen,JianFeng Ma,Lin Yang*

Main category: cs.SE

TL;DR: 提出针对复杂循环程序的LLM辅助规范生成方法SLD - Spec，实验表明其效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的方法处理含复杂循环结构程序时生成规范存在不相关、不完整和模糊问题，需改进。

Method: 提出SLD - Spec方法，包括切片阶段分解函数为含独立循环结构的代码片段，逻辑删除阶段用LLM推理过滤错误候选规范。

Result: 在简单数据集上比AutoSpec多验证5个程序，减少23.73%运行时间；在自建复杂循环程序数据集上显著提升规范的正确性、相关性和完整性，使95.1%断言和90.91%程序通过验证。

Conclusion: 逻辑删除对提升规范正确性和相关性至关重要，程序切片对提升规范完整性贡献大。

Abstract: Automatically generating formal specifications from program code can greatly
enhance the efficiency of program verification and enable end-to-end automation
from requirements to reliable software. However, existing LLM-based approaches
often struggle with programs that include complex loop structures, leading to
irrelevant specifications. Moreover, the rigorous proof obligations and design
constraints imposed by verification tools can further result in incomplete and
ambiguous specifications. To address these challenges, we propose SLD-Spec, an
LLM-assisted specification generation method tailored for programs with complex
loop constructs. SLD-Spec introduces two novel phases into the traditional
specification generation framework: (1) A slicing phase, which decomposes each
function into code fragments containing independent loop structures, thereby
reducing the complexity of specification generation; and (2) A logical deletion
phase, which applies LLM-based reasoning to filter out incorrect candidate
specifications--especially those not easily identified by verification
tool--while retaining valid ones. Experimental results show that on the simple
dataset, SLD-Spec successfully verifies five more programs than the
state-of-the-art AutoSpec and reduces runtime by 23.73%. To address the
limitations of existing research, we manually construct a dataset comprising
four categories of complex loop programs. On this dataset, SLD-Spec
significantly improves the correctness, relevance, and completeness of
generated specifications compared to baseline methods, enabling 95.1% of
assertions and 90.91% of programs to pass verification. Ablation studies
further reveal that logical deletion is critical for enhancing specification
correctness and relevance, while program slicing contributes significantly to
specification completeness. Our code and data are publicly available.

</details>


### [115] [WALL: A Web Application for Automated Quality Assurance using Large Language Models](https://arxiv.org/abs/2509.09918)
*Seyed Moein Abtahi,Akramul Azim*

Main category: cs.SE

TL;DR: 本文提出Web应用WALL，集成SonarQube和大语言模型自动化处理软件代码问题，实验证明其有效性并规划未来工作。


<details>
  <summary>Details</summary>
Motivation: 随着软件项目复杂度增加，代码文件问题增多，需要高效的问题检测、解决和评估工具。

Method: 提出WALL应用，包含问题提取、代码问题修订和代码比较三个模块，集成SonarQube和大语言模型。

Result: 在563个有超7599个问题的文件上实验，证明WALL能减少人力且保证修订质量，混合使用大语言模型可降低成本、提高修订率。

Conclusion: 未来将集成开源大语言模型、消除人工干预，实现代码质量管理全自动化。

Abstract: As software projects become increasingly complex, the volume and variety of
issues in code files have grown substantially. Addressing this challenge
requires efficient issue detection, resolution, and evaluation tools. This
paper presents WALL, a web application that integrates SonarQube and large
language models (LLMs) such as GPT-3.5 Turbo and GPT-4o to automate these
tasks. WALL comprises three modules: an issue extraction tool, code issues
reviser, and code comparison tool. Together, they enable a seamless pipeline
for detecting software issues, generating automated code revisions, and
evaluating the accuracy of revisions. Our experiments, conducted on 563 files
with over 7,599 issues, demonstrate WALL's effectiveness in reducing human
effort while maintaining high-quality revisions. Results show that employing a
hybrid approach of cost-effective and advanced LLMs can significantly lower
costs and improve revision rates. Future work aims to enhance WALL's
capabilities by integrating open-source LLMs and eliminating human
intervention, paving the way for fully automated code quality management.

</details>


### [116] [Toward Green Code: Prompting Small Language Models for Energy-Efficient Code Generation](https://arxiv.org/abs/2509.09947)
*Humza Ashraf,Syed Muhammad Danish,Zeeshan Sattar*

Main category: cs.SE

TL;DR: 研究探讨提示工程能否提升小型语言模型（SLMs）代码生成的能源效率，评估四个开源SLMs在不同提示策略下表现，发现提示效果因模型而异，精心设计提示可助力绿色软件开发。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在软件开发中能源消耗大、碳足迹高，小型语言模型更具可持续性，研究提示工程能否提升其能源效率。

Method: 评估四个开源SLMs（StableCode - Instruct - 3B、Qwen2.5 - Coder - 3B - Instruct、CodeLlama - 7B - Instruct、Phi - 3 - Mini - 4K - Instruct）在150个LeetCode Python问题上的表现，采用四种提示策略（角色提示、零样本、少样本、思维链），测量运行时间、内存使用和能源消耗并与人类编写基线比较。

Result: 思维链提示能为Qwen2.5 - Coder和StableCode - 3B持续节省能源，CodeLlama - 7B和Phi - 3 - Mini - 4K在任何提示策略下都未超过基线。

Conclusion: 提示工程的好处因模型而异，精心设计的提示可引导SLMs实现更绿色的软件开发。

Abstract: There is a growing concern about the environmental impact of large language
models (LLMs) in software development, particularly due to their high energy
use and carbon footprint. Small Language Models (SLMs) offer a more sustainable
alternative, requiring fewer computational resources while remaining effective
for fundamental programming tasks. In this study, we investigate whether prompt
engineering can improve the energy efficiency of SLMs in code generation. We
evaluate four open-source SLMs, StableCode-Instruct-3B,
Qwen2.5-Coder-3B-Instruct, CodeLlama-7B-Instruct, and Phi-3-Mini-4K-Instruct,
across 150 Python problems from LeetCode, evenly distributed into easy, medium,
and hard categories. Each model is tested under four prompting strategies: role
prompting, zero-shot, few-shot, and chain-of-thought (CoT). For every generated
solution, we measure runtime, memory usage, and energy consumption, comparing
the results with a human-written baseline. Our findings show that CoT prompting
provides consistent energy savings for Qwen2.5-Coder and StableCode-3B, while
CodeLlama-7B and Phi-3-Mini-4K fail to outperform the baseline under any
prompting strategy. These results highlight that the benefits of prompting are
model-dependent and that carefully designed prompts can guide SLMs toward
greener software development.

</details>


### [117] [Development of Automated Software Design Document Review Methods Using Large Language Models](https://arxiv.org/abs/2509.09975)
*Takasaburo Fukuda,Takao Nakagawa,Keisuke Miyazaki,Susumu Tokumoto*

Main category: cs.SE

TL;DR: 本文探索用大语言模型（LLM）自动化软件设计文档评审流程，分析评审视角、解决LLM应用问题并开发新技术，实验证实LLM可识别文档不一致性。


<details>
  <summary>Details</summary>
Motivation: 探索使用大语言模型自动化软件设计文档评审流程。

Method: 分析设计文档评审方法，整理11个评审视角；分析LLM用于这些视角的问题，确定可由LLM评审的视角；针对可评审视角开发新技术使LLM理解含表格数据的复杂文档；用GPT进行实验评估设计项和描述的一致性。

Result: 实验证实LLM可在评审过程中识别软件设计文档的不一致性。

Conclusion: LLM可用于软件设计文档评审过程中识别不一致性。

Abstract: In this study, we explored an approach to automate the review process of
software design documents by using LLM. We first analyzed the review methods of
design documents and organized 11 review perspectives. Additionally, we
analyzed the issues of utilizing LLMs for these 11 review perspectives and
determined which perspectives can be reviewed by current general-purpose LLMs
instead of humans. For the reviewable perspectives, we specifically developed
new techniques to enable LLMs to comprehend complex design documents that
include table data. For evaluation, we conducted experiments using GPT to
assess the consistency of design items and descriptions across different design
documents in the design process used in actual business operations. Our results
confirmed that LLMs can be utilized to identify inconsistencies in software
design documents during the review process.

</details>


### [118] [Sustaining Research Software: A Fitness Function Approach](https://arxiv.org/abs/2509.10085)
*Philipp Zech,Irdin Pekaric*

Main category: cs.SE

TL;DR: 论文提出用进化架构的适应度函数解决研究软件长期可持续性问题，定义适配研究软件的适应度函数，案例和实验证明其可提升软件FAIR性。


<details>
  <summary>Details</summary>
Motivation: 研究软件存在可维护性差、缺乏适应性和最终过时等问题，需要解决其长期可持续性挑战。

Method: 利用进化架构的适应度函数概念，定义一套针对研究软件独特需求、聚焦FAIR的适应度函数，并将其集成到开发生命周期。

Result: 案例研究和实验结果表明该方法有潜力提升研究软件的长期FAIR性。

Conclusion: 该方法能缩小临时项目开发与持久科学影响之间的差距，促进研究社区形成可持续发展文化。

Abstract: The long-term sustainability of research software is a critical challenge, as
it usually suffers from poor maintainability, lack of adaptability, and
eventual obsolescence. This paper proposes a novel approach to addressing this
issue by leveraging the concept of fitness functions from evolutionary
architecture. Fitness functions are automated, continuously evaluated metrics
designed to ensure that software systems meet desired non-functional,
architectural qualities over time. We define a set of fitness functions
tailored to the unique requirements of research software, focusing on
findability, accessibility, interoperability and reusability (FAIR). These
fitness functions act as proactive safeguards, promoting practices such as
modular design, comprehensive documentation, version control, and compatibility
with evolving technological ecosystems. By integrating these metrics into the
development life cycle, we aim to foster a culture of sustainability within the
research community. Case studies and experimental results demonstrate the
potential of this approach to enhance the long-term FAIR of research software,
bridging the gap between ephemeral project-based development and enduring
scientific impact.

</details>


### [119] [Generating Energy-Efficient Code via Large-Language Models -- Where are we now?](https://arxiv.org/abs/2509.10099)
*Radu Apsan,Vincenzo Stoico,Michel Albonico,Rudra Dhar,Karthik Vaidhyanathan,Ivano Malavolta*

Main category: cs.SE

TL;DR: 本文实证评估大语言模型（LLMs）生成的Python代码与人类编写代码及绿色软件专家开发代码的能源效率，发现人类专家代码更节能。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型生成的Python代码的能源效率。

Method: 对EvoEval基准中的9个编码问题的363个解决方案进行测试，使用6种广泛使用的大语言模型和4种提示技术，并与人类开发的解决方案进行比较，在三种硬件平台上测量能耗。

Result: 人类解决方案在服务器和树莓派上更节能，大语言模型在PC上比人类开发者高25%；提示技术不一定能节能；绿色软件专家开发的代码在所有硬件平台上比所有大语言模型生成的代码至少节能17% - 30%。

Conclusion: 目前大语言模型生成的代码能源效率不如经验丰富的绿色软件开发者，开发节能Python代码仍需人类专业知识。

Abstract: Context. The rise of Large Language Models (LLMs) has led to their widespread
adoption in development pipelines. Goal. We empirically assess the energy
efficiency of Python code generated by LLMs against human-written code and code
developed by a Green software expert. Method. We test 363 solutions to 9 coding
problems from the EvoEval benchmark using 6 widespread LLMs with 4 prompting
techniques, and comparing them to human-developed solutions. Energy consumption
is measured on three different hardware platforms: a server, a PC, and a
Raspberry Pi for a total of ~881h (36.7 days). Results. Human solutions are 16%
more energy-efficient on the server and 3% on the Raspberry Pi, while LLMs
outperform human developers by 25% on the PC. Prompting does not consistently
lead to energy savings, where the most energy-efficient prompts vary by
hardware platform. The code developed by a Green software expert is
consistently more energy-efficient by at least 17% to 30% against all LLMs on
all hardware platforms. Conclusions. Even though LLMs exhibit relatively good
code generation capabilities, no LLM-generated code was more energy-efficient
than that of an experienced Green software developer, suggesting that as of
today there is still a great need of human expertise for developing
energy-efficient Python code.

</details>


### [120] [Stencil-Lifting: Hierarchical Recursive Lifting System for Extracting Summary of Stencil Kernel in Legacy Codes](https://arxiv.org/abs/2509.10236)
*Mingyi Li,Junmin Xiao,Siyan Chen,Hui Ma,Xi Chen,Peihua Bao,Liang Yuan,Guangming Tan*

Main category: cs.SE

TL;DR: 提出Stencil - Lifting系统，将低级语言的模板内核转换为DSL实现，通过创新方法提升效率，实验有显著加速。


<details>
  <summary>Details</summary>
Motivation: 针对现有验证提升系统的效率瓶颈，提高低级模板内核到DSL实现的转换效率。

Method: 提出分层递归提升理论，用不变子图表示模板内核；开发分层递归提升算法，通过收敛递归过程保证终止。

Result: 在不同基准测试和实际应用中，与STNG和Dexter相比分别实现31.62×和5.8×加速，且保持语义等价。

Conclusion: 该工作显著提升转换效率，弥合传统优化技术与现代DSL范式间的差距。

Abstract: We introduce Stencil-Lifting, a novel system for automatically converting
stencil kernels written in low-level languages in legacy code into semantically
equivalent Domain-Specific Language (DSL) implementations. Targeting the
efficiency bottlenecks of existing verified lifting systems, Stencil-Lifting
achieves scalable stencil kernel abstraction through two key innovations.
First, we propose a hierarchical recursive lifting theory that represents
stencil kernels, structured as nested loops, using invariant subgraphs, which
are customized data dependency graphs that capture loop-carried computation and
structural invariants. Each vertex in the invariant subgraph is associated with
a predicate-based summary, encoding its computational semantics. By enforcing
self-consistency across these summaries, Stencil-Lifting ensures the derivation
of correct loop invariants and postconditions for nested loops, eliminating the
need for external verification. Second, we develop a hierarchical recursive
lifting algorithm that guarantees termination through a convergent recursive
process, avoiding the inefficiencies of search-based synthesis. The algorithm
efficiently derives the valid summaries of stencil kernels, and its
completeness is formally proven. We evaluate Stencil-Lifting on diverse stencil
benchmarks from two different suites and on four real-world applications.
Experimental results demonstrate that Stencil-Lifting achieves 31.62$\times$
and 5.8$\times$ speedups compared to the state-of-the-art verified lifting
systems STNG and Dexter, respectively, while maintaining full semantic
equivalence. Our work significantly enhances the translation efficiency of
low-level stencil kernels to DSL implementations, effectively bridging the gap
between legacy optimization techniques and modern DSL-based paradigms.

</details>


### [121] [Targeted Test Selection Approach in Continuous Integration](https://arxiv.org/abs/2509.10279)
*Pavel Plyusnin,Aleksey Antonov,Vasilii Ermakov,Aleksandr Khaybriev,Margarita Kikot,Ilseyar Alimova,Stanislav Moiseev*

Main category: cs.SE

TL;DR: 提出针对工业测试选择的机器学习方法T - TS，在生产环境评估效果良好，代码公开。


<details>
  <summary>Details</summary>
Motivation: 现代软件开发中，代码库和测试套件扩大，高频代码提交下高效管理测试过程挑战大。

Method: 提出Targeted Test Selection (T - TS)方法，用改变文件的词袋表示提交，结合跨文件和额外预测特征，不使用覆盖率地图。

Result: 在实时工业数据上，T - TS仅选择15%的测试，执行时间减少5.9倍，加速管道5.6倍，检测到超95%的测试失败。

Conclusion: T - TS方法能有效应对工业测试选择问题，实现高效测试，代码公开利于进一步研究和实际应用。

Abstract: In modern software development change-based testing plays a crucial role.
However, as codebases expand and test suites grow, efficiently managing the
testing process becomes increasingly challenging, especially given the high
frequency of daily code commits. We propose Targeted Test Selection (T-TS), a
machine learning approach for industrial test selection. Our key innovation is
a data representation that represent commits as Bags-of-Words of changed files,
incorporates cross-file and additional predictive features, and notably avoids
the use of coverage maps. Deployed in production, T-TS was comprehensively
evaluated against industry standards and recent methods using both internal and
public datasets, measuring time efficiency and fault detection. On live
industrial data, T-TS selects only 15% of tests, reduces execution time by
$5.9\times$, accelerates the pipeline by $5.6\times$, and detects over 95% of
test failures. The implementation is publicly available to support further
research and practical adoption.

</details>


### [122] [Developer-LLM Conversations: An Empirical Study of Interactions and Generated Code Quality](https://arxiv.org/abs/2509.10402)
*Suzhen Zhong,Ying Zou,Bram Adams*

Main category: cs.SE

TL;DR: 本文借助CodeChat数据集研究开发者与大语言模型交互，发现响应比提示长、多轮对话占比高，识别常见任务，指出各语言代码问题及改进情况，明确有效纠错提示方式。


<details>
  <summary>Details</summary>
Motivation: 当前对开发者与大语言模型实际交互情况及对话动态对任务结果、代码质量和软件工程工作流的影响理解有限。

Method: 利用包含82,845个真实开发者 - 大语言模型对话的CodeChat数据集进行研究。

Result: LLM响应比开发者提示长，多轮对话占比68%，常见任务有网页设计和神经网络训练，各语言代码存在特定问题，部分语言代码质量有改善，特定纠错提示有效。

Conclusion: 明确了开发者与LLM交互的特点、代码问题及有效纠错方式，有助于改进软件开发工作流。

Abstract: Large Language Models (LLMs) are becoming integral to modern software
development workflows, assisting developers with code generation, API
explanation, and iterative problem-solving through natural language
conversations. Despite widespread adoption, there is limited understanding of
how developers interact with LLMs in practice and how these conversational
dynamics influence task outcomes, code quality, and software engineering
workflows. To address this, we leverage CodeChat, a large dataset comprising
82,845 real-world developer-LLM conversations, containing 368,506 code snippets
generated across over 20 programming languages, derived from the WildChat
dataset. We find that LLM responses are substantially longer than developer
prompts, with a median token-length ratio of 14:1. Multi-turn conversations
account for 68% of the dataset and often evolve due to shifting requirements,
incomplete prompts, or clarification requests. Topic analysis identifies web
design (9.6% of conversations) and neural network training (8.7% of
conversations) as the most frequent LLM-assisted tasks. Evaluation across five
languages (i.e., Python, JavaScript, C++, Java, and C#) reveals prevalent and
language-specific issues in LLM-generated code: generated Python and JavaScript
code often include undefined variables (83.4% and 75.3% of code snippets,
respectively); Java code lacks required comments (75.9%); C++ code frequently
omits headers (41.1%) and C# code shows unresolved namespaces (49.2%). During a
conversation, syntax and import errors persist across turns; however,
documentation quality in Java improves by up to 14.7%, and import handling in
Python improves by 3.7% over 5 turns. Prompts that point out mistakes in code
generated in prior turns and explicitly request a fix are most effective for
resolving errors.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [123] [An Information-Theoretic Framework for Credit Risk Modeling: Unifying Industry Practice with Statistical Theory for Fair and Interpretable Scorecards](https://arxiv.org/abs/2509.09855)
*Agus Sudjianto,Denis Burakov*

Main category: stat.ML

TL;DR: 本文建立统一信息论框架，揭示信用风险建模常用指标与经典信息散度关系，推导标准误差，比较编码策略，平衡性能与公平性。


<details>
  <summary>Details</summary>
Motivation: 信用风险建模中常用指标理论基础不连贯，需统一框架提供严谨统计基础。

Method: 建立统一信息论框架，运用delta方法推导标准误差，用深度1的XGBoost树桩自动分箱，比较三种编码策略，用混合整数规划追踪帕累托有效解。

Result: 所有方法预测性能相当（AUC 0.82 - 0.84），表明信息论分箱比编码选择更重要。

Conclusion: 该框架连接理论与实践，为信用风险指标提供统计基础，提供平衡准确性与公平性的工具。

Abstract: Credit risk modeling relies extensively on Weight of Evidence (WoE) and
Information Value (IV) for feature engineering, and Population Stability Index
(PSI) for drift monitoring, yet their theoretical foundations remain
disconnected. We establish a unified information-theoretic framework revealing
these industry-standard metrics as instances of classical information
divergences. Specifically, we prove that IV exactly equals PSI (Jeffreys
divergence) computed between good and bad credit outcomes over identical bins.
Through the delta method applied to WoE transformations, we derive standard
errors for IV and PSI, enabling formal hypothesis testing and probabilistic
fairness constraints for the first time. We formalize credit modeling's
inherent performance-fairness trade-off as maximizing IV for predictive power
while minimizing IV for protected attributes. Using automated binning with
depth-1 XGBoost stumps, we compare three encoding strategies: logistic
regression with one-hot encoding, WoE transformation, and constrained XGBoost.
All methods achieve comparable predictive performance (AUC 0.82-0.84),
demonstrating that principled, information-theoretic binning outweighs encoding
choice. Mixed-integer programming traces Pareto-efficient solutions along the
performance-fairness frontier with uncertainty quantification. This framework
bridges theory and practice, providing the first rigorous statistical
foundation for widely-used credit risk metrics while offering principled tools
for balancing accuracy and fairness in regulated environments.

</details>


### [124] [Repulsive Monte Carlo on the sphere for the sliced Wasserstein distance](https://arxiv.org/abs/2509.10166)
*Vladimir Petrovic,Rémi Bardenet,Agnès Desolneux*

Main category: stat.ML

TL;DR: 本文研究用蒙特卡罗方法计算任意维度单位球上函数积分，聚焦切片Wasserstein距离计算，评估多种求积法并分析UnifOrtho估计器方差，推荐低维用随机拟蒙特卡罗，高维用UnifOrtho。


<details>
  <summary>Details</summary>
Motivation: 切片Wasserstein距离在机器学习中应用广泛，当前有其求积法的数值基准，本文关注节点具有排斥性的求积法以降低方差。

Method: 从行列式点过程和排斥点过程文献中提取求积法，对这些求积法进行数值基准测试，分析UnifOrtho估计器的方差。

Result: 分析揭示了UnifOrtho在高维下估计切片Wasserstein距离成功的原因及文献中的反例；DPP求积法在拟蒙特卡罗方法有效的情况下表现出色，排斥求积法一般有适度的方差降低。

Conclusion: 计算切片Wasserstein距离，低维推荐使用随机拟蒙特卡罗方法，高维推荐使用UnifOrtho；DPP求积法和排斥求积法需更多理论研究使其更稳健。

Abstract: In this paper, we consider the problem of computing the integral of a
function on the unit sphere, in any dimension, using Monte Carlo methods.
Although the methods we present are general, our guiding thread is the sliced
Wasserstein distance between two measures on $\mathbb{R}^d$, which is precisely
an integral on the $d$-dimensional sphere. The sliced Wasserstein distance (SW)
has gained momentum in machine learning either as a proxy to the less
computationally tractable Wasserstein distance, or as a distance in its own
right, due in particular to its built-in alleviation of the curse of
dimensionality. There has been recent numerical benchmarks of quadratures for
the sliced Wasserstein, and our viewpoint differs in that we concentrate on
quadratures where the nodes are repulsive, i.e. negatively dependent. Indeed,
negative dependence can bring variance reduction when the quadrature is adapted
to the integration task. Our first contribution is to extract and motivate
quadratures from the recent literature on determinantal point processes (DPPs)
and repelled point processes, as well as repulsive quadratures from the
literature specific to the sliced Wasserstein distance. We then numerically
benchmark these quadratures. Moreover, we analyze the variance of the UnifOrtho
estimator, an orthogonal Monte Carlo estimator. Our analysis sheds light on
UnifOrtho's success for the estimation of the sliced Wasserstein in large
dimensions, as well as counterexamples from the literature. Our final
recommendation for the computation of the sliced Wasserstein distance is to use
randomized quasi-Monte Carlo in low dimensions and \emph{UnifOrtho} in large
dimensions. DPP-based quadratures only shine when quasi-Monte Carlo also does,
while repelled quadratures show moderate variance reduction in general, but
more theoretical effort is needed to make them robust.

</details>


### [125] [Why does your graph neural network fail on some graphs? Insights from exact generalisation error](https://arxiv.org/abs/2509.10337)
*Nil Ayday,Mahalakshmi Sabanayagam,Debarghya Ghoshdastidar*

Main category: stat.ML

TL;DR: 本文从信号处理视角推导归纳式固定设计下GNN的精确泛化误差，为多种GNN提供框架，解释其何时及为何能有效利用信息。


<details>
  <summary>Details</summary>
Motivation: 现有研究未解释GNN提取有意义表示的原因和性能差异，泛化误差界通常较宽松，对实际泛化的理解有限。

Method: 从信号处理角度，聚焦线性GNN并允许图滤波器存在非线性，推导多种GNN的精确泛化误差。

Result: 精确泛化误差表明只有节点特征和图结构的对齐信息有助于泛化，并量化了同质性对泛化的影响。

Conclusion: 提供框架解释GNN何时及为何能有效利用结构和特征信息，为模型选择提供实际指导。

Abstract: Graph Neural Networks (GNNs) are widely used in learning on graph-structured
data, yet a principled understanding of why they succeed or fail remains
elusive. While prior works have examined architectural limitations such as
over-smoothing and over-squashing, these do not explain what enables GNNs to
extract meaningful representations or why performance varies drastically
between similar architectures. These questions are related to the role of
generalisation: the ability of a model to make accurate predictions on
unlabelled data. Although several works have derived generalisation error
bounds for GNNs, these are typically loose, restricted to a single
architecture, and offer limited insight into what governs generalisation in
practice. In this work, we take a different approach by deriving the exact
generalisation error for GNNs in a transductive fixed-design setting through
the lens of signal processing. From this viewpoint, GNNs can be interpreted as
graph filter operators that act on node features via the graph structure. By
focusing on linear GNNs while allowing non-linearity in the graph filters, we
derive the first exact generalisation error for a broad range of GNNs,
including convolutional, PageRank-based, and attention-based models. The exact
characterisation of the generalisation error reveals that only the aligned
information between node features and graph structure contributes to
generalisation. Furthermore, we quantify the effect of homophily on
generalisation. Our work provides a framework that explains when and why GNNs
can effectively leverage structural and feature information, offering practical
guidance for model selection.

</details>


### [126] [Differentially Private Decentralized Dataset Synthesis Through Randomized Mixing with Correlated Noise](https://arxiv.org/abs/2509.10385)
*Utsab Saha,Tanvir Muntakim Tonoy,Hafiz Imtiaz*

Main category: stat.ML

TL;DR: 本文在去中心化数据场景下，基于DP - CDA探索差分隐私合成数据生成，提出CAPE Assisted Federated DP - CDA算法，实验表明该算法在保证差分隐私的同时能提升隐私 - 效用权衡。


<details>
  <summary>Details</summary>
Motivation: DP - CDA在去中心化或联邦设置中，因客户端样本量有限，导致本地计算敏感度增加，需注入更多噪声，使效用明显下降，因此需要改进。

Method: 将Correlation - Assisted Private Estimation (CAPE)协议集成到联邦DP - CDA框架中，提出CAPE Assisted Federated DP - CDA算法，允许客户端生成联合分布（反相关）噪声以抵消聚合噪声。

Result: 在MNIST和FashionMNIST数据集上的大量实验表明，在某些参数条件下，该算法能达到与集中式DP - CDA相当的效用，并保持严格的差分隐私保证。

Conclusion: 提出的CAPE Assisted Federated DP - CDA算法显著改善了联邦设置中的隐私 - 效用权衡。

Abstract: In this work, we explore differentially private synthetic data generation in
a decentralized-data setting by building on the recently proposed
Differentially Private Class-Centric Data Aggregation (DP-CDA). DP-CDA
synthesizes data in a centralized setting by mixing multiple randomly-selected
samples from the same class and injecting carefully calibrated Gaussian noise,
ensuring ({\epsilon}, {\delta})-differential privacy. When deployed in a
decentralized or federated setting, where each client holds only a small
partition of the data, DP-CDA faces new challenges. The limited sample size per
client increases the sensitivity of local computations, requiring higher noise
injection to maintain the differential privacy guarantee. This, in turn, leads
to a noticeable degradation in the utility compared to the centralized setting.
To mitigate this issue, we integrate the Correlation-Assisted Private
Estimation (CAPE) protocol into the federated DP-CDA framework and propose CAPE
Assisted Federated DP-CDA algorithm. CAPE enables limited collaboration among
the clients by allowing them to generate jointly distributed (anti-correlated)
noise that cancels out in aggregate, while preserving privacy at the individual
level. This technique significantly improves the privacy-utility trade-off in
the federated setting. Extensive experiments on MNIST and FashionMNIST datasets
demonstrate that the proposed CAPE Assisted Federated DP-CDA approach can
achieve utility comparable to its centralized counterpart under some parameter
regime, while maintaining rigorous differential privacy guarantees.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [127] [A Computable Measure of Suboptimality for Entropy-Regularised Variational Objectives](https://arxiv.org/abs/2509.10393)
*Clémentine Chazal,Heishiro Kanagawa,Zheyang Shen,Anna Korba,Chris. J. Oates*

Main category: stat.CO

TL;DR: 本文引入‘梯度差异’（KGD）应对新兴后贝叶斯方法的计算挑战，提出新采样算法并给出理论性质。


<details>
  <summary>Details</summary>
Motivation: 新兴后贝叶斯方法增加灵活性的同时带来计算挑战，因无法获取目标的显式未归一化密度。

Method: 引入‘梯度差异’（KGD），在标准贝叶斯环境中与核斯坦因差异（KSD）一致，提出新采样算法。

Result: 提出多种新算法，如斯坦变分梯度下降的自然推广，并应用于平均场神经网络和以预测为中心的不确定性量化。

Conclusion: 建立了KGD具有连续性和收敛控制等理想性质的充分条件。

Abstract: Several emerging post-Bayesian methods target a probability distribution for
which an entropy-regularised variational objective is minimised. This increased
flexibility introduces a computational challenge, as one loses access to an
explicit unnormalised density for the target. To mitigate this difficulty, we
introduce a novel measure of suboptimality called 'gradient discrepancy', and
in particular a 'kernel gradient discrepancy' (KGD) that can be explicitly
computed. In the standard Bayesian context, KGD coincides with the kernel Stein
discrepancy (KSD), and we obtain a novel charasterisation of KSD as measuring
the size of a variational gradient. Outside this familiar setting, KGD enables
novel sampling algorithms to be developed and compared, even when unnormalised
densities cannot be obtained. To illustrate this point several novel algorithms
are proposed, including a natural generalisation of Stein variational gradient
descent, with applications to mean-field neural networks and prediction-centric
uncertainty quantification presented. On the theoretical side, our principal
contribution is to establish sufficient conditions for desirable properties of
KGD, such as continuity and convergence control.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [128] [Openness in AI and downstream governance: A global value chain approach](https://arxiv.org/abs/2509.10220)
*Christopher Foster*

Main category: cs.CY

TL;DR: 本文探讨AI开放性发展，通过价值链分析增加概念清晰度，拓展对AI作为生产部门的理解，指出AI开放性或带来潜在溢出效应。


<details>
  <summary>Details</summary>
Motivation: AI快速崛起且开放性发展迅速，需分析开放资源能否支持技术转移及追赶，为相关辩论增加概念清晰度。

Method: 将AI开放性概念化为独特的企业间关系，进行价值链分析，拓展先前AI价值链映射构建框架。

Result: 构建了将基础AI与下游价值链相联系的框架。

Conclusion: 拓展了对AI作为生产部门的理解，虽批判领先AI企业权力，但AI开放性可能带来潜在溢出效应。

Abstract: The rise of AI has been rapid, becoming a leading sector for investment and
promising disruptive impacts across the economy. Within the critical analysis
of the economic impacts, AI has been aligned to the critical literature on data
power and platform capitalism - further concentrating power and value capture
amongst a small number of "big tech" leaders.
  The equally rapid rise of openness in AI (here taken to be claims made by AI
firms about openness, "open source" and free provision) signals an interesting
development. It highlights an emerging ecosystem of open AI models, datasets
and toolchains, involving massive capital investment. It poses questions as to
whether open resources can support technological transfer and the ability for
catch-up, even in the face of AI industry power.
  This work seeks to add conceptual clarity to these debates by conceptualising
openness in AI as a unique type of interfirm relation and therefore amenable to
value chain analysis. This approach then allows consideration of the capitalist
dynamics of "outsourcing" of foundational firms in value chains, and
consequently the types of governance and control that might emerge downstream
as AI is adopted. This work, therefore, extends previous mapping of AI value
chains to build a framework which links foundational AI with downstream value
chains.
  Overall, this work extends our understanding of AI as a productive sector.
While the work remains critical of the power of leading AI firms, openness in
AI may lead to potential spillovers stemming from the intense competition for
global technological leadership in AI.

</details>


### [129] [We Need a New Ethics for a World of AI Agents](https://arxiv.org/abs/2509.10289)
*Iason Gabriel,Geoff Keeling,Arianna Manzini,James Evans*

Main category: cs.CY

TL;DR: 部署有能力的AI智能体引发新问题，呼吁各界关注并探讨人机及智能体间互动挑战。


<details>
  <summary>Details</summary>
Motivation: 由于AI智能体的部署引发了关于安全、人机关系和社会协调的新问题，所以需要各界关注其影响。

Method: 文中未提及具体方法。

Result: 文中未提及具体研究结果。

Conclusion: 需解决关键挑战，确保人机及智能体间的互动广泛有益。

Abstract: The deployment of capable AI agents raises fresh questions about safety,
human-machine relationships and social coordination. We argue for greater
engagement by scientists, scholars, engineers and policymakers with the
implications of a world increasingly populated by AI agents. We explore key
challenges that must be addressed to ensure that interactions between humans
and agents, and among agents themselves, remain broadly beneficial.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [130] [A Multimodal RAG Framework for Housing Damage Assessment: Collaborative Optimization of Image Encoding and Policy Vector Retrieval](https://arxiv.org/abs/2509.09721)
*Jiayi Miao,Dingxin Lu,Zhuqi Wang*

Main category: cs.CV

TL;DR: 提出MM - RAG框架用于自然灾害后房屋损坏评估，性能优越，Top - 1检索准确率提高9.6%。


<details>
  <summary>Details</summary>
Motivation: 自然灾害后，准确评估房屋损坏对保险理赔和资源规划很重要。

Method: 引入MM - RAG框架，采用两分支多模态编码器结构，图像分支用ResNet和Transformer提取特征，文本分支用BERT检索器，集成跨模态交互模块，引入模态注意力门控机制，端到端训练，多任务优化。

Result: 在检索准确率和损坏严重程度分类指标上表现优越，Top - 1检索准确率提高9.6%。

Conclusion: MM - RAG框架能有效实现图像理解和政策匹配，在房屋损坏评估中表现良好。

Abstract: After natural disasters, accurate evaluations of damage to housing are
important for insurance claims response and planning of resources. In this
work, we introduce a novel multimodal retrieval-augmented generation (MM-RAG)
framework. On top of classical RAG architecture, we further the framework to
devise a two-branch multimodal encoder structure that the image branch employs
a visual encoder composed of ResNet and Transformer to extract the
characteristic of building damage after disaster, and the text branch harnesses
a BERT retriever for the text vectorization of posts as well as insurance
policies and for the construction of a retrievable restoration index. To impose
cross-modal semantic alignment, the model integrates a cross-modal interaction
module to bridge the semantic representation between image and text via
multi-head attention. Meanwhile, in the generation module, the introduced modal
attention gating mechanism dynamically controls the role of visual evidence and
text prior information during generation. The entire framework takes end-to-end
training, and combines the comparison loss, the retrieval loss and the
generation loss to form multi-task optimization objectives, and achieves image
understanding and policy matching in collaborative learning. The results
demonstrate superior performance in retrieval accuracy and classification index
on damage severity, where the Top-1 retrieval accuracy has been improved by
9.6%.

</details>


### [131] [MITS: A Large-Scale Multimodal Benchmark Dataset for Intelligent Traffic Surveillance](https://arxiv.org/abs/2509.09730)
*Kaikai Zhao,Zhaoxiang Liu,Peng Wang,Xin Wang,Zhicheng Ma,Yajun Xu,Wenjing Zhang,Yibing Nan,Kai Wang,Shiguo Lian*

Main category: cs.CV

TL;DR: 提出首个用于智能交通监控（ITS）的大规模多模态基准数据集MITS，含真实图像及生成的文本数据，实验表明可显著提升LMM在ITS应用的性能，相关资源开源。


<details>
  <summary>Details</summary>
Motivation: 通用领域大跨模态模型（LMMs）在ITS领域因缺乏专用多模态数据集，性能受限，故提出MITS数据集。

Method: 收集170,400张真实ITS图像并标注，通过系统数据生成流程生成图像描述和问答对，在数据集上微调主流LMMs。

Result: MITS显著提升LMM在ITS应用的性能，如LLaVA - 1.5性能从0.494提升到0.905等。

Conclusion: MITS数据集有效，开源的数据集、代码和模型为ITS和LMM研究提供高价值资源。

Abstract: General-domain large multimodal models (LMMs) have achieved significant
advances in various image-text tasks. However, their performance in the
Intelligent Traffic Surveillance (ITS) domain remains limited due to the
absence of dedicated multimodal datasets. To address this gap, we introduce
MITS (Multimodal Intelligent Traffic Surveillance), the first large-scale
multimodal benchmark dataset specifically designed for ITS. MITS includes
170,400 independently collected real-world ITS images sourced from traffic
surveillance cameras, annotated with eight main categories and 24 subcategories
of ITS-specific objects and events under diverse environmental conditions.
Additionally, through a systematic data generation pipeline, we generate
high-quality image captions and 5 million instruction-following visual
question-answer pairs, addressing five critical ITS tasks: object and event
recognition, object counting, object localization, background analysis, and
event reasoning. To demonstrate MITS's effectiveness, we fine-tune mainstream
LMMs on this dataset, enabling the development of ITS-specific applications.
Experimental results show that MITS significantly improves LMM performance in
ITS applications, increasing LLaVA-1.5's performance from 0.494 to 0.905
(+83.2%), LLaVA-1.6's from 0.678 to 0.921 (+35.8%), Qwen2-VL's from 0.584 to
0.926 (+58.6%), and Qwen2.5-VL's from 0.732 to 0.930 (+27.0%). We release the
dataset, code, and models as open-source, providing high-value resources to
advance both ITS and LMM research.

</details>


### [132] [World Modeling with Probabilistic Structure Integration](https://arxiv.org/abs/2509.09737)
*Klemen Kotar,Wanhee Lee,Rahul Venkatesh,Honglin Chen,Daniel Bear,Jared Watrous,Simon Kim,Khai Loong Aw,Lilian Naing Chen,Stefan Stojanov,Kevin Feigelis,Imran Thobani,Alex Durango,Khaled Jedoui,Atlas Kazemian,Dan Yamins*

Main category: cs.CV

TL;DR: 提出概率结构集成（PSI）系统从数据中学习可灵活控制和提示的世界模型，介绍三步循环及应用成果。


<details>
  <summary>Details</summary>
Motivation: 开发能从数据中学习丰富可控且灵活可提示世界模型的系统。

Method: PSI包含概率预测、结构提取和集成三步循环，构建概率图模型Psi，通过因果推理提取结构并转化为新令牌类型。

Result: 在1.4万亿互联网视频数据上训练Psi，实现多种视频预测和理解推理，提取先进的光流、自监督深度和目标分割。

Conclusion: PSI系统有效，能提升对底层数据的建模能力并创造新控制手段。

Abstract: We present Probabilistic Structure Integration (PSI), a system for learning
richly controllable and flexibly promptable world models from data. PSI
consists of a three-step cycle. The first step, Probabilistic prediction,
involves building a probabilistic graphical model Psi of the data, in the form
of a random-access autoregressive sequence model. Psi supports a complete set
of learned conditional distributions describing the dependence of any variables
in the data on any other set of variables. In step 2, Structure extraction, we
show how to extract underlying low-dimensional properties in the data,
corresponding to a diverse set of meaningful "intermediate structures", in a
zero-shot fashion via causal inference on Psi. Step 3, Integration, completes
the cycle by converting these structures into new token types that are then
continually mixed back into the training diet as conditioning signals and
prediction targets. Each such cycle augments the capabilities of Psi, both
allowing it to model the underlying data better, and creating new control
handles -- akin to an LLM-like universal prompting language. We train an
instance of Psi on 1.4 trillion tokens of internet video data; we use it to
perform a variety of useful video prediction and understanding inferences; we
extract state-of-the-art optical flow, self-supervised depth and object
segmentation; and we use these structures to support a full cycle of predictive
improvements.

</details>


### [133] [A Co-Training Semi-Supervised Framework Using Faster R-CNN and YOLO Networks for Object Detection in Densely Packed Retail Images](https://arxiv.org/abs/2509.09750)
*Hossein Yazdanjouei,Arash Mansouri,Mohammad Shokouhifar*

Main category: cs.CV

TL;DR: 提出半监督协同训练框架用于零售环境目标检测，结合Faster R - CNN和YOLO，用集成分类器，优化超参数，减少标注成本，实验表现好。


<details>
  <summary>Details</summary>
Motivation: 零售环境中标记数据有限和条件复杂，传统方法难以应对。

Method: 结合Faster R - CNN和YOLO进行伪标签交换；使用XGBoost、Random Forest和SVM集成分类；用元启发式算法优化超参数。

Result: 在SKU - 110k数据集实验中表现良好。

Conclusion: 框架可扩展且实用，适用于零售自动化库存跟踪等实际应用。

Abstract: This study proposes a semi-supervised co-training framework for object
detection in densely packed retail environments, where limited labeled data and
complex conditions pose major challenges. The framework combines Faster R-CNN
(utilizing a ResNet backbone) for precise localization with YOLO (employing a
Darknet backbone) for global context, enabling mutual pseudo-label exchange
that improves accuracy in scenes with occlusion and overlapping objects. To
strengthen classification, it employs an ensemble of XGBoost, Random Forest,
and SVM, utilizing diverse feature representations for higher robustness.
Hyperparameters are optimized using a metaheuristic-driven algorithm, enhancing
precision and efficiency across models. By minimizing reliance on manual
labeling, the approach reduces annotation costs and adapts effectively to
frequent product and layout changes common in retail. Experiments on the
SKU-110k dataset demonstrate strong performance, highlighting the scalability
and practicality of the proposed framework for real-world retail applications
such as automated inventory tracking, product monitoring, and checkout systems.

</details>


### [134] [Improving MLLM Historical Record Extraction with Test-Time Image](https://arxiv.org/abs/2509.09722)
*Taylor Archibald,Tony Martinez*

Main category: cs.CV

TL;DR: 提出稳定从嘈杂历史文档中基于大语言模型进行文本提取的集成框架，展示新数据集，方法提升转录准确率。


<details>
  <summary>Details</summary>
Motivation: 稳定从嘈杂历史文档中基于大语言模型进行文本提取。

Method: 用Gemini 2.0 Flash转录图像增强变体，用自定义Needleman Wunsch风格对齐器融合输出得到共识转录和置信度分数。

Result: 新方法比单镜头基线转录准确率提高4个百分点，发现填充和模糊利于提升准确率，网格扭曲扰动利于区分高低置信度情况。

Conclusion: 该方法简单、可扩展，可立即应用于其他文档集和转录模型。

Abstract: We present a novel ensemble framework that stabilizes LLM based text
extraction from noisy historical documents. We transcribe multiple augmented
variants of each image with Gemini 2.0 Flash and fuse these outputs with a
custom Needleman Wunsch style aligner that yields both a consensus
transcription and a confidence score. We present a new dataset of 622
Pennsylvania death records, and demonstrate our method improves transcription
accuracy by 4 percentage points relative to a single shot baseline. We find
that padding and blurring are the most useful for improving accuracy, while
grid warp perturbations are best for separating high and low confidence cases.
The approach is simple, scalable, and immediately deployable to other document
collections and transcription models.

</details>


### [135] [Surrogate Supervision for Robust and Generalizable Deformable Image Registration](https://arxiv.org/abs/2509.09869)
*Yihao Liu,Junyu Chen,Lianrui Zuo,Shuwen Wei,Brian D. Boyd,Carmen Andreescu,Olusola Ajilore,Warren D. Taylor,Aaron Carass,Bennett A. Landman*

Main category: cs.CV

TL;DR: 论文提出替代监督方法提升深度学习图像配准网络的鲁棒性与泛化性，经多应用验证效果良好。


<details>
  <summary>Details</summary>
Motivation: 深度学习图像配准对输入图像特征变化敏感，旨在开发提升配准网络鲁棒性和泛化性的通用训练范式。

Method: 引入替代监督，将输入域与监督域解耦，通过对替代图像应用估计的空间变换进行训练。

Result: 替代监督在多项任务中对输入变化表现出强适应性，在优质数据上也保持高性能。

Conclusion: 替代监督为训练鲁棒且泛化性强的深度学习配准模型提供了原则性框架，不增加复杂度。

Abstract: Objective: Deep learning-based deformable image registration has achieved
strong accuracy, but remains sensitive to variations in input image
characteristics such as artifacts, field-of-view mismatch, or modality
difference. We aim to develop a general training paradigm that improves the
robustness and generalizability of registration networks. Methods: We introduce
surrogate supervision, which decouples the input domain from the supervision
domain by applying estimated spatial transformations to surrogate images. This
allows training on heterogeneous inputs while ensuring supervision is computed
in domains where similarity is well defined. We evaluate the framework through
three representative applications: artifact-robust brain MR registration,
mask-agnostic lung CT registration, and multi-modal MR registration. Results:
Across tasks, surrogate supervision demonstrated strong resilience to input
variations including inhomogeneity field, inconsistent field-of-view, and
modality differences, while maintaining high performance on well-curated data.
Conclusions: Surrogate supervision provides a principled framework for training
robust and generalizable deep learning-based registration models without
increasing complexity. Significance: Surrogate supervision offers a practical
pathway to more robust and generalizable medical image registration, enabling
broader applicability in diverse biomedical imaging scenarios.

</details>


### [136] [Early Detection of Visual Impairments at Home Using a Smartphone Red-Eye Reflex Test](https://arxiv.org/abs/2509.09808)
*Judith Massmann,Alexander Lichtenstein,Francisco M. López*

Main category: cs.CV

TL;DR: 本文介绍了免费应用KidsVisionCheck，它用手机红反射图像进行视力筛查，模型准确率达90%，为全球儿童视力筛查和早期干预迈出第一步。


<details>
  <summary>Details</summary>
Motivation: 利用智能手机和人工智能技术，开发可重现Bruckner测试的移动应用，实现儿童视力筛查。

Method: 使用眼科医生收集和标注的儿童瞳孔图像训练深度神经网络。

Result: 模型在未见测试数据上准确率达90%，能确定数据收集的最佳条件并向用户提供即时反馈。

Conclusion: 这项工作为全球可及的儿科视力筛查和视力异常早期干预迈出了第一步。

Abstract: Numerous visual impairments can be detected in red-eye reflex images from
young children. The so-called Bruckner test is traditionally performed by
ophthalmologists in clinical settings. Thanks to the recent technological
advances in smartphones and artificial intelligence, it is now possible to
recreate the Bruckner test using a mobile device. In this paper, we present a
first study conducted during the development of KidsVisionCheck, a free
application that can perform vision screening with a mobile device using
red-eye reflex images. The underlying model relies on deep neural networks
trained on children's pupil images collected and labeled by an ophthalmologist.
With an accuracy of 90% on unseen test data, our model provides highly reliable
performance without the necessity of specialist equipment. Furthermore, we can
identify the optimal conditions for data collection, which can in turn be used
to provide immediate feedback to the users. In summary, this work marks a first
step toward accessible pediatric vision screenings and early intervention for
vision abnormalities worldwide.

</details>


### [137] [DGFusion: Depth-Guided Sensor Fusion for Robust Semantic Perception](https://arxiv.org/abs/2509.09828)
*Tim Broedermannn,Christos Sakaridis,Luigi Piccinelli,Wim Abbeloos,Luc Van Gool*

Main category: cs.CV

TL;DR: 提出深度引导多模态融合方法DGFusion用于自动驾驶语义感知，在MUSES和DELIVER数据集上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有传感器融合方法在处理输入时统一对待传感器数据，面对复杂条件时性能受限。

Method: 将多模态分割作为多任务问题，利用激光雷达测量数据，设计辅助深度头学习深度感知特征，编码为局部深度令牌，结合全局条件令牌动态调整传感器融合；提出鲁棒的深度损失函数。

Result: 在MUSES和DELIVER数据集上实现了最先进的全景和语义分割性能。

Conclusion: 所提的深度引导多模态融合方法DGFusion有效提升自动驾驶语义感知性能。

Abstract: Robust semantic perception for autonomous vehicles relies on effectively
combining multiple sensors with complementary strengths and weaknesses.
State-of-the-art sensor fusion approaches to semantic perception often treat
sensor data uniformly across the spatial extent of the input, which hinders
performance when faced with challenging conditions. By contrast, we propose a
novel depth-guided multimodal fusion method that upgrades condition-aware
fusion by integrating depth information. Our network, DGFusion, poses
multimodal segmentation as a multi-task problem, utilizing the lidar
measurements, which are typically available in outdoor sensor suites, both as
one of the model's inputs and as ground truth for learning depth. Our
corresponding auxiliary depth head helps to learn depth-aware features, which
are encoded into spatially varying local depth tokens that condition our
attentive cross-modal fusion. Together with a global condition token, these
local depth tokens dynamically adapt sensor fusion to the spatially varying
reliability of each sensor across the scene, which largely depends on depth. In
addition, we propose a robust loss for our depth, which is essential for
learning from lidar inputs that are typically sparse and noisy in adverse
conditions. Our method achieves state-of-the-art panoptic and semantic
segmentation performance on the challenging MUSES and DELIVER datasets. Code
and models will be available at https://github.com/timbroed/DGFusion

</details>


### [138] [WAVE-DETR Multi-Modal Visible and Acoustic Real-Life Drone Detector](https://arxiv.org/abs/2509.09859)
*Razvan Stefanescu,Ethan Oh,Ruben Vazquez,Chris Mesterharm,Constantin Serban,Ritu Chadha*

Main category: cs.CV

TL;DR: 本文提出多模态WAVE - DETR无人机检测器，融合RGB和声学信号提升检测性能，测试多种融合配置，门控融合效果最佳。


<details>
  <summary>Details</summary>
Motivation: 实现鲁棒的现实生活中无人机目标检测，提升在具有挑战性环境条件下的检测性能。

Method: 结合Deformable DETR和Wav2Vec2架构，融合视觉和声学特征；利用现有和新生成数据集；开发、训练和测试基于门控机制、线性层、MLP和交叉注意力的四种融合配置。

Result: 门控融合方法表现最佳，在ARDrone数据集上，小无人机mAP提升11.1% - 15.3%，中大型无人机mAP也有提升，所有尺寸无人机整体提升3.27% - 5.84%。

Conclusion: 多模态融合的WAVE - DETR无人机检测器能有效提升无人机目标检测性能，门控融合配置效果显著。

Abstract: We introduce a multi-modal WAVE-DETR drone detector combining visible RGB and
acoustic signals for robust real-life UAV object detection. Our approach fuses
visual and acoustic features in a unified object detector model relying on the
Deformable DETR and Wav2Vec2 architectures, achieving strong performance under
challenging environmental conditions. Our work leverage the existing
Drone-vs-Bird dataset and the newly generated ARDrone dataset containing more
than 7,500 synchronized images and audio segments. We show how the acoustic
information is used to improve the performance of the Deformable DETR object
detector on the real ARDrone dataset. We developed, trained and tested four
different fusion configurations based on a gated mechanism, linear layer, MLP
and cross attention. The Wav2Vec2 acoustic embeddings are fused with the multi
resolution feature mappings of the Deformable DETR and enhance the object
detection performance over all drones dimensions. The best performer is the
gated fusion approach, which improves the mAP of the Deformable DETR object
detector on our in-distribution and out-of-distribution ARDrone datasets by
11.1% to 15.3% for small drones across all IoU thresholds between 0.5 and 0.9.
The mAP scores for medium and large drones are also enhanced, with overall
gains across all drone sizes ranging from 3.27% to 5.84%.

</details>


### [139] [An Autoencoder and Vision Transformer-based Interpretability Analysis of the Differences in Automated Staging of Second and Third Molars](https://arxiv.org/abs/2509.09911)
*Barkin Buyukcakir,Jannick De Tobel,Patrick Thevissen,Dirk Vandermeulen,Peter Claes*

Main category: cs.CV

TL;DR: 本文提出结合卷积自编码器和视觉变换器的框架，提升法医年龄估计中下颌第二、三磨牙分期分类准确性，还提供诊断见解并指出数据问题。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型的“黑盒”特性限制其在法医应用中的实际采用，需提升性能和透明度。

Method: 以第二、三磨牙自动分期性能差异为案例，采用结合卷积自编码器（AE）和视觉变换器（ViT）的框架。

Result: 框架提高两颗牙齿的分类准确率，37号牙从0.712提升到0.815，38号牙从0.462提升到0.543，还提供多方面诊断见解，指出性能差距源于数据。

Conclusion: 单一可解释性方式不足，该框架提升准确性并解释不确定性，是法医年龄估计中支持专家决策的更可靠工具。

Abstract: The practical adoption of deep learning in high-stakes forensic applications,
such as dental age estimation, is often limited by the 'black box' nature of
the models. This study introduces a framework designed to enhance both
performance and transparency in this context. We use a notable performance
disparity in the automated staging of mandibular second (tooth 37) and third
(tooth 38) molars as a case study. The proposed framework, which combines a
convolutional autoencoder (AE) with a Vision Transformer (ViT), improves
classification accuracy for both teeth over a baseline ViT, increasing from
0.712 to 0.815 for tooth 37 and from 0.462 to 0.543 for tooth 38. Beyond
improving performance, the framework provides multi-faceted diagnostic
insights. Analysis of the AE's latent space metrics and image reconstructions
indicates that the remaining performance gap is data-centric, suggesting high
intra-class morphological variability in the tooth 38 dataset is a primary
limiting factor. This work highlights the insufficiency of relying on a single
mode of interpretability, such as attention maps, which can appear anatomically
plausible yet fail to identify underlying data issues. By offering a
methodology that both enhances accuracy and provides evidence for why a model
may be uncertain, this framework serves as a more robust tool to support expert
decision-making in forensic age estimation.

</details>


### [140] [Zero-Shot Referring Expression Comprehension via Visual-Language True/False Verification](https://arxiv.org/abs/2509.09958)
*Jeffrey Liu,Rongbin Hu*

Main category: cs.CV

TL;DR: 提出零样本工作流用于指代表达理解，性能优越，表明工作流设计比特定任务预训练更重要。


<details>
  <summary>Details</summary>
Motivation: 探索不经过指代表达理解特定训练的零样本工作流在指代表达理解任务中的表现。

Method: 将指代表达理解重新表述为逐框视觉语言验证，利用通用检测器的提议和通用视觉语言模型独立回答查询。

Result: 在RefCOCO、RefCOCO+和RefCOCOg数据集上超越零样本GroundingDINO基线，也超过在指代表达理解上训练的GroundingDINO和GroundingDINO+CRG的报告结果。

Conclusion: 工作流设计而非特定任务预训练是实现强零样本指代表达理解性能的关键。

Abstract: Referring Expression Comprehension (REC) is usually addressed with
task-trained grounding models. We show that a zero-shot workflow, without any
REC-specific training, can achieve competitive or superior performance. Our
approach reformulates REC as box-wise visual-language verification: given
proposals from a COCO-clean generic detector (YOLO-World), a general-purpose
VLM independently answers True/False queries for each region. This simple
procedure reduces cross-box interference, supports abstention and multiple
matches, and requires no fine-tuning. On RefCOCO, RefCOCO+, and RefCOCOg, our
method not only surpasses a zero-shot GroundingDINO baseline but also exceeds
reported results for GroundingDINO trained on REC and GroundingDINO+CRG.
Controlled studies with identical proposals confirm that verification
significantly outperforms selection-based prompting, and results hold with open
VLMs. Overall, we show that workflow design, rather than task-specific
pretraining, drives strong zero-shot REC performance.

</details>


### [141] [Multimodal Mathematical Reasoning Embedded in Aerial Vehicle Imagery: Benchmarking, Analysis, and Exploration](https://arxiv.org/abs/2509.10059)
*Yue Zhou,Litong Feng,Mengcheng Lan,Xue Yang,Qingyun Li,Yiping Ke,Xue Jiang,Wayne Zhang*

Main category: cs.CV

TL;DR: 本文提出AVI - Math基准测试评估无人机影像多模态数学推理能力，测试14种VLM模型，发现其推理能力有限，还探索了应对技术。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在无人机遥感数学推理领域测试不足，需新基准测试评估。

Method: 引入AVI - Math基准测试，包含3773个高质量问题，涵盖6个数学主题和20个话题；对14种VLM模型进行综合评估；探索思维链提示和微调技术。

Result: 当前VLM模型在AVI - Math推理任务中表现不佳，思维链提示和微调技术有一定潜力。

Conclusion: 揭示了VLM模型在数学推理方面的局限性，为无人机可信VLM模型在实际应用中的发展提供了见解。

Abstract: Mathematical reasoning is critical for tasks such as precise distance and
area computations, trajectory estimations, and spatial analysis in unmanned
aerial vehicle (UAV) based remote sensing, yet current vision-language models
(VLMs) have not been adequately tested in this domain. To address this gap, we
introduce AVI-Math, the first benchmark to rigorously evaluate multimodal
mathematical reasoning in aerial vehicle imagery, moving beyond simple counting
tasks to include domain-specific knowledge in areas such as geometry, logic,
and algebra. The dataset comprises 3,773 high-quality vehicle-related questions
captured from UAV views, covering 6 mathematical subjects and 20 topics. The
data, collected at varying altitudes and from multiple UAV angles, reflects
real-world UAV scenarios, ensuring the diversity and complexity of the
constructed mathematical problems. In this paper, we benchmark 14 prominent
VLMs through a comprehensive evaluation and demonstrate that, despite their
success on previous multimodal benchmarks, these models struggle with the
reasoning tasks in AVI-Math. Our detailed analysis highlights significant
limitations in the mathematical reasoning capabilities of current VLMs and
suggests avenues for future research. Furthermore, we explore the use of
Chain-of-Thought prompting and fine-tuning techniques, which show promise in
addressing the reasoning challenges in AVI-Math. Our findings not only expose
the limitations of VLMs in mathematical reasoning but also offer valuable
insights for advancing UAV-based trustworthy VLMs in real-world applications.
The code, and datasets will be released at
https://github.com/VisionXLab/avi-math

</details>


### [142] [MCL-AD: Multimodal Collaboration Learning for Zero-Shot 3D Anomaly Detection](https://arxiv.org/abs/2509.10282)
*Gang Li,Tianjiao Chen,Mingle Zhou,Min Li,Delong Han,Jin Wan*

Main category: cs.CV

TL;DR: 本文提出MCL - AD框架用于零样本3D异常检测，结合多模态协作学习，实验表明其达SOTA水平。


<details>
  <summary>Details</summary>
Motivation: 现有零样本3D异常检测方法多仅关注点云，忽略RGB图像和文本先验等多模态语义信息，本文旨在解决此问题。

Method: 提出多模态提示学习机制（MPLM）增强模态内表示和模态间协作学习，提出协作调制机制（CMM）充分利用点云和RGB图像的互补表示。

Result: 所提出的MCL - AD框架在零样本3D异常检测中取得了最先进的性能。

Conclusion: MCL - AD框架通过多模态协作学习能有效提升零样本3D异常检测性能。

Abstract: Zero-shot 3D (ZS-3D) anomaly detection aims to identify defects in 3D objects
without relying on labeled training data, making it especially valuable in
scenarios constrained by data scarcity, privacy, or high annotation cost.
However, most existing methods focus exclusively on point clouds, neglecting
the rich semantic cues available from complementary modalities such as RGB
images and texts priors. This paper introduces MCL-AD, a novel framework that
leverages multimodal collaboration learning across point clouds, RGB images,
and texts semantics to achieve superior zero-shot 3D anomaly detection.
Specifically, we propose a Multimodal Prompt Learning Mechanism (MPLM) that
enhances the intra-modal representation capability and inter-modal
collaborative learning by introducing an object-agnostic decoupled text prompt
and a multimodal contrastive loss. In addition, a collaborative modulation
mechanism (CMM) is proposed to fully leverage the complementary representations
of point clouds and RGB images by jointly modulating the RGB image-guided and
point cloud-guided branches. Extensive experiments demonstrate that the
proposed MCL-AD framework achieves state-of-the-art performance in ZS-3D
anomaly detection.

</details>


### [143] [Realism Control One-step Diffusion for Real-World Image Super-Resolution](https://arxiv.org/abs/2509.10122)
*Zongliang Wu,Siming Zheng,Peng-Tao Jiang,Xin Yuan*

Main category: cs.CV

TL;DR: 提出用于Real - ISR的RCOD框架，实现保真度和感知质量提升且保持计算效率，优于现有OSD方法。


<details>
  <summary>Details</summary>
Motivation: 现有一步扩散（OSD）方法在平衡保真度和真实感方面存在局限，缺乏灵活控制机制。

Method: 提出RCOD框架，包括潜在域分组策略、退化感知采样策略和视觉提示注入模块。

Result: 方法在定量指标和视觉质量上优于现有OSD方法，在推理阶段有灵活的真实感控制能力。

Conclusion: RCOD框架有效解决了OSD方法的问题，能在保持计算效率的同时提升保真度和感知质量。

Abstract: Pre-trained diffusion models have shown great potential in real-world image
super-resolution (Real-ISR) tasks by enabling high-resolution reconstructions.
While one-step diffusion (OSD) methods significantly improve efficiency
compared to traditional multi-step approaches, they still have limitations in
balancing fidelity and realism across diverse scenarios. Since the OSDs for SR
are usually trained or distilled by a single timestep, they lack flexible
control mechanisms to adaptively prioritize these competing objectives, which
are inherently manageable in multi-step methods through adjusting sampling
steps. To address this challenge, we propose a Realism Controlled One-step
Diffusion (RCOD) framework for Real-ISR. RCOD provides a latent domain grouping
strategy that enables explicit control over fidelity-realism trade-offs during
the noise prediction phase with minimal training paradigm modifications and
original training data. A degradation-aware sampling strategy is also
introduced to align distillation regularization with the grouping strategy and
enhance the controlling of trade-offs. Moreover, a visual prompt injection
module is used to replace conventional text prompts with degradation-aware
visual tokens, enhancing both restoration accuracy and semantic consistency.
Our method achieves superior fidelity and perceptual quality while maintaining
computational efficiency. Extensive experiments demonstrate that RCOD
outperforms state-of-the-art OSD methods in both quantitative metrics and
visual qualities, with flexible realism control capabilities in the inference
stage. The code will be released.

</details>


### [144] [I-Segmenter: Integer-Only Vision Transformer for Efficient Semantic Segmentation](https://arxiv.org/abs/2509.10334)
*Jordan Sassoon,Michal Szczepanski,Martyna Poreba*

Main category: cs.CV

TL;DR: 提出首个全整数ViT分割框架I - Segmenter，提升效率并保证一定精度。


<details>
  <summary>Details</summary>
Motivation: Vision Transformers在语义分割表现好，但在资源受限设备部署因高内存和计算成本受限，量化虽有效但ViT分割模型在低精度下脆弱。

Method: 基于Segmenter架构，用整数运算替代浮点运算；提出λ - ShiftGELU激活函数；移除L2归一化层，用最近邻上采样替代双线性插值。

Result: I - Segmenter精度与FP32基线接近，平均相差5.1%，模型大小最多缩减3.8倍，推理速度最多提升1.2倍，单张校准图像的一次性PTQ也有有竞争力的精度。

Conclusion: I - Segmenter具有实用性，适合实际部署。

Abstract: Vision Transformers (ViTs) have recently achieved strong results in semantic
segmentation, yet their deployment on resource-constrained devices remains
limited due to their high memory footprint and computational cost. Quantization
offers an effective strategy to improve efficiency, but ViT-based segmentation
models are notoriously fragile under low precision, as quantization errors
accumulate across deep encoder-decoder pipelines. We introduce I-Segmenter, the
first fully integer-only ViT segmentation framework. Building on the Segmenter
architecture, I-Segmenter systematically replaces floating-point operations
with integer-only counterparts. To further stabilize both training and
inference, we propose $\lambda$-ShiftGELU, a novel activation function that
mitigates the limitations of uniform quantization in handling long-tailed
activation distributions. In addition, we remove the L2 normalization layer and
replace bilinear interpolation in the decoder with nearest neighbor upsampling,
ensuring integer-only execution throughout the computational graph. Extensive
experiments show that I-Segmenter achieves accuracy within a reasonable margin
of its FP32 baseline (5.1 % on average), while reducing model size by up to
3.8x and enabling up to 1.2x faster inference with optimized runtimes. Notably,
even in one-shot PTQ with a single calibration image, I-Segmenter delivers
competitive accuracy, underscoring its practicality for real-world deployment.

</details>


### [145] [GLAM: Geometry-Guided Local Alignment for Multi-View VLP in Mammography](https://arxiv.org/abs/2509.10344)
*Yuexi Du,Lihui Chen,Nicha C. Dvornek*

Main category: cs.CV

TL;DR: 现有乳腺钼靶VLM存在不足，提出GLAM模型利用几何引导进行预训练，在多数据集上表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有乳腺钼靶VLM受数据和领域差异限制，且忽略多视图关系等特定领域特征，导致预测效果不佳。

Method: 提出GLAM模型，利用乳腺钼靶多视图成像的先验知识，通过联合全局和局部、视觉 - 视觉、视觉 - 语言对比学习来学习局部跨视图对齐和细粒度局部特征。

Result: 在最大的开放乳腺钼靶数据集之一EMBED上预训练，在不同设置下的多个数据集上优于基线。

Conclusion: GLAM模型通过几何引导的预训练方法能有效提升乳腺钼靶图像分析效果。

Abstract: Mammography screening is an essential tool for early detection of breast
cancer. The speed and accuracy of mammography interpretation have the potential
to be improved with deep learning methods. However, the development of a
foundation visual language model (VLM) is hindered by limited data and domain
differences between natural and medical images. Existing mammography VLMs,
adapted from natural images, often ignore domain-specific characteristics, such
as multi-view relationships in mammography. Unlike radiologists who analyze
both views together to process ipsilateral correspondence, current methods
treat them as independent images or do not properly model the multi-view
correspondence learning, losing critical geometric context and resulting in
suboptimal prediction. We propose GLAM: Global and Local Alignment for
Multi-view mammography for VLM pretraining using geometry guidance. By
leveraging the prior knowledge about the multi-view imaging process of
mammograms, our model learns local cross-view alignments and fine-grained local
features through joint global and local, visual-visual, and visual-language
contrastive learning. Pretrained on EMBED [14], one of the largest open
mammography datasets, our model outperforms baselines across multiple datasets
under different settings.

</details>


### [146] [SignClip: Leveraging Mouthing Cues for Sign Language Translation by Multimodal Contrastive Fusion](https://arxiv.org/abs/2509.10266)
*Wenfang Wu,Tingting Yuan,Yupeng Li,Daling Wang,Xiaoming Fu*

Main category: cs.CV

TL;DR: 提出SignClip框架改进手语翻译准确性，融合手动与非手动线索，引入分层对比学习框架，实验证明其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有手语翻译方法多关注手动信号，忽略了如口型等非手动线索，而口型在消除视觉相似手语歧义等方面有重要作用。

Method: 提出SignClip框架，融合手动和非手动线索（空间手势和嘴唇运动特征），引入具有多级对齐目标的分层对比学习框架。

Result: 在PHOENIX14T和How2Sign两个基准数据集上实验表现优越，如在PHOENIX14T的无注释设置下，BLEU - 4从24.32提升到24.71，ROUGE从46.57提升到48.38。

Conclusion: SignClip框架能有效提高手语翻译的准确性。

Abstract: Sign language translation (SLT) aims to translate natural language from sign
language videos, serving as a vital bridge for inclusive communication. While
recent advances leverage powerful visual backbones and large language models,
most approaches mainly focus on manual signals (hand gestures) and tend to
overlook non-manual cues like mouthing. In fact, mouthing conveys essential
linguistic information in sign languages and plays a crucial role in
disambiguating visually similar signs. In this paper, we propose SignClip, a
novel framework to improve the accuracy of sign language translation. It fuses
manual and non-manual cues, specifically spatial gesture and lip movement
features. Besides, SignClip introduces a hierarchical contrastive learning
framework with multi-level alignment objectives, ensuring semantic consistency
across sign-lip and visual-text modalities. Extensive experiments on two
benchmark datasets, PHOENIX14T and How2Sign, demonstrate the superiority of our
approach. For example, on PHOENIX14T, in the Gloss-free setting, SignClip
surpasses the previous state-of-the-art model SpaMo, improving BLEU-4 from
24.32 to 24.71, and ROUGE from 46.57 to 48.38.

</details>


### [147] [SSL-AD: Spatiotemporal Self-Supervised Learning for Generalizability and Adaptability Across Alzheimer's Prediction Tasks and Datasets](https://arxiv.org/abs/2509.10453)
*Emily Kaczmarek,Justin Szeto,Brennan Nichyporuk,Tal Arbel*

Main category: cs.CV

TL;DR: 本文针对阿尔茨海默病预测任务中深度学习模型的局限，改进三种自监督学习方法，在多任务中表现良好且具泛化性，并公开代码和模型。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型在阿尔茨海默病预测任务中存在可用标注数据少、跨数据集泛化能力差、输入灵活性不足等局限。

Method: 改进三种用于3D脑MRI分析的时序自监督学习方法，添加处理可变长度输入和学习空间特征的扩展，聚合四个公开数据集预训练。

Result: 自监督学习模型在七个下游任务中的六个上优于监督学习，在不同任务和输入图像数量及时间间隔上表现出适应性和泛化性。

Conclusion: 模型在临床应用中具有稳健性能，可用于阿尔茨海默病的多种预测任务。

Abstract: Alzheimer's disease is a progressive, neurodegenerative disorder that causes
memory loss and cognitive decline. While there has been extensive research in
applying deep learning models to Alzheimer's prediction tasks, these models
remain limited by lack of available labeled data, poor generalization across
datasets, and inflexibility to varying numbers of input scans and time
intervals between scans. In this study, we adapt three state-of-the-art
temporal self-supervised learning (SSL) approaches for 3D brain MRI analysis,
and add novel extensions designed to handle variable-length inputs and learn
robust spatial features. We aggregate four publicly available datasets
comprising 3,161 patients for pre-training, and show the performance of our
model across multiple Alzheimer's prediction tasks including diagnosis
classification, conversion detection, and future conversion prediction.
Importantly, our SSL model implemented with temporal order prediction and
contrastive learning outperforms supervised learning on six out of seven
downstream tasks. It demonstrates adaptability and generalizability across
tasks and number of input images with varying time intervals, highlighting its
capacity for robust performance across clinical applications. We release our
code and model publicly at https://github.com/emilykaczmarek/SSL-AD.

</details>


### [148] [Towards Understanding Visual Grounding in Visual Language Models](https://arxiv.org/abs/2509.10345)
*Georgios Pantazopoulos,Eda B. Özyiğit*

Main category: cs.CV

TL;DR: 本文对现代通用视觉语言模型（VLMs）中视觉定位相关研究进行综述，涵盖重要性、核心组件、应用、相互关系等，并分析挑战与未来方向。


<details>
  <summary>Details</summary>
Motivation: 视觉定位能力使模型有广泛应用，因此对现代通用VLMs中视觉定位研究进行综述很有必要。

Method: 回顾现代通用VLMs关键领域的代表性工作，概述定位重要性、核心组件、应用，探讨相互关系，分析挑战。

Result: 梳理了视觉定位在VLMs中的多方面内容，包括核心组件、应用、与其他概念的关系等。

Conclusion: 分析了视觉定位存在的挑战，并给出未来研究的有前景方向。

Abstract: Visual grounding refers to the ability of a model to identify a region within
some visual input that matches a textual description. Consequently, a model
equipped with visual grounding capabilities can target a wide range of
applications in various domains, including referring expression comprehension,
answering questions pertinent to fine-grained details in images or videos,
caption visual context by explicitly referring to entities, as well as low and
high-level control in simulated and real environments. In this survey paper, we
review representative works across the key areas of research on modern
general-purpose vision language models (VLMs). We first outline the importance
of grounding in VLMs, then delineate the core components of the contemporary
paradigm for developing grounded models, and examine their practical
applications, including benchmarks and evaluation metrics for grounded
multimodal generation. We also discuss the multifaceted interrelations among
visual grounding, multimodal chain-of-thought, and reasoning in VLMs. Finally,
we analyse the challenges inherent to visual grounding and suggest promising
directions for future research.

</details>


### [149] [Multimodal SAM-adapter for Semantic Segmentation](https://arxiv.org/abs/2509.10408)
*Iacopo Curti,Pierluigi Zama Ramirez,Alioscia Petrelli,Luigi Di Stefano*

Main category: cs.CV

TL;DR: 本文提出MM SAM - adapter框架用于多模态语义分割，在三个基准测试中表现出色，证明多模态适应对场景理解的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前语义分割方法在恶劣条件下易受影响，多模态方法可增强鲁棒性，因此提出新框架扩展Segment Anything Model (SAM)用于多模态语义分割。

Method: 采用适配器网络将融合的多模态特征注入SAM的RGB特征，有选择地纳入辅助模态。

Result: 在DeLiVER、FMB和MUSES三个基准测试中取得了最先进的性能，在不同条件下均优于竞争方法。

Conclusion: MM SAM - adapter框架能平衡高效利用多模态信息，多模态适应对鲁棒场景理解有效。

Abstract: Semantic segmentation, a key task in computer vision with broad applications
in autonomous driving, medical imaging, and robotics, has advanced
substantially with deep learning. Nevertheless, current approaches remain
vulnerable to challenging conditions such as poor lighting, occlusions, and
adverse weather. To address these limitations, multimodal methods that
integrate auxiliary sensor data (e.g., LiDAR, infrared) have recently emerged,
providing complementary information that enhances robustness. In this work, we
present MM SAM-adapter, a novel framework that extends the capabilities of the
Segment Anything Model (SAM) for multimodal semantic segmentation. The proposed
method employs an adapter network that injects fused multimodal features into
SAM's rich RGB features. This design enables the model to retain the strong
generalization ability of RGB features while selectively incorporating
auxiliary modalities only when they contribute additional cues. As a result, MM
SAM-adapter achieves a balanced and efficient use of multimodal information. We
evaluate our approach on three challenging benchmarks, DeLiVER, FMB, and MUSES,
where MM SAM-adapter delivers state-of-the-art performance. To further analyze
modality contributions, we partition DeLiVER and FMB into RGB-easy and RGB-hard
subsets. Results consistently demonstrate that our framework outperforms
competing methods in both favorable and adverse conditions, highlighting the
effectiveness of multimodal adaptation for robust scene understanding. The code
is available at the following link:
https://github.com/iacopo97/Multimodal-SAM-Adapter.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [150] [DBOS Network Sensing: A Web Services Approach to Collaborative Awareness](https://arxiv.org/abs/2509.09898)
*Sophia Lockton,Jeremy Kepner,Michael Stonebraker,Hayden Jananthan,LaToya Anderson,William Arcand,David Bestor,William Bergeron,Alex Bonn,Daniel Burrill,Chansup Byun,Timothy Davis,Vijay Gadepally,Michael Houle,Matthew Hubbell,Michael Jones,Piotr Luszczek,Peter Michaleas,Lauren Milechin,Chasen Milner,Guillermo Morales,Julie Mullen,Michel Pelletier,Alex Poliakov,Andrew Prout,Albert Reuther,Antonio Rosa,Charles Yee,Alex Pentland*

Main category: cs.NI

TL;DR: DBOS集成多种功能以减少部署工作并增强弹性，通过两种方式添加网络感知，经测试表明添加网络感知开销小，协作网络感知计算资源增加可忽略不计。


<details>
  <summary>Details</summary>
Motivation: 减少Web部署工作，增强系统弹性、安全性和协作感知能力。

Method: 使用GraphBLAS超稀疏流量矩阵的两种方法（Python - GraphBLAS和OneSparse PostgreSQL）添加网络感知，用pPython并行化系统，在MIT SuperCloud上用64个计算节点进行基准测试。

Result: 单个DBOS实例维持的Web请求率大于10^5；Python - GraphBLAS和OneSparse PostgreSQL实现分别线性扩展到64和32个节点。

Conclusion: 向DBOS添加网络感知开销可忽略不计，DBOS协作网络感知所需计算资源增加可忽略不计。

Abstract: DBOS (DataBase Operating System) is a novel capability that integrates web
services, operating system functions, and database features to significantly
reduce web-deployment effort while increasing resilience. Integration of high
performance network sensing enables DBOS web services to collaboratively create
a shared awareness of their network environments to enhance their collective
resilience and security. Network sensing is added to DBOS using GraphBLAS
hypersparse traffic matrices via two approaches: (1) Python-GraphBLAS and (2)
OneSparse PostgreSQL. These capabilities are demonstrated using the workflow
and analytics from the IEEE/MIT/Amazon Anonymized Network Sensing Graph
Challenge. The system was parallelized using pPython and benchmarked using 64
compute nodes on the MIT SuperCloud. The web request rate sustained by a single
DBOS instance was ${>}10^5$, well above the required maximum, indicating that
network sensing can be added to DBOS with negligible overhead. For
collaborative awareness, many DBOS instances were connected to a single DBOS
aggregator. The Python-GraphBLAS and OneSparse PostgreSQL implementations
scaled linearly up to 64 and 32 nodes respectively. These results suggest that
DBOS collaborative network awareness can be achieved with a negligible increase
in computing resources.

</details>


### [151] [RFSeek and Ye Shall Find](https://arxiv.org/abs/2509.10216)
*Noga H. Rotman,Tiago Ferreira,Hila Peleg,Mark Silberstein,Alexandra Silva*

Main category: cs.NI

TL;DR: 提出RFSeek工具自动从RFC提取协议逻辑可视化摘要，展示多种用例，证明可重构或生成新图，强调结合LLM与可视化的方向。


<details>
  <summary>Details</summary>
Motivation: RFC基于文本的格式和长度阻碍对网络协议的精确操作理解。

Method: 使用大语言模型（LLMs）生成与出处关联、可探索的图表。

Result: RFSeek能重构RFC图，发现文本中重要逻辑，为复杂RFC生成新可视化图。

Conclusion: 结合LLMs与正式、用户定制的可视化是增强协议理解和支持可靠实现的有前景方向。

Abstract: Requests for Comments (RFCs) are extensive specification documents for
network protocols, but their prose-based format and their considerable length
often impede precise operational understanding. We present RFSeek, an
interactive tool that automatically extracts visual summaries of protocol logic
from RFCs. RFSeek leverages large language models (LLMs) to generate
provenance-linked, explorable diagrams, surfacing both official state machines
and additional logic found only in the RFC text. Compared to existing RFC
visualizations, RFSeek's visual summaries are more transparent and easier to
audit against their textual source. We showcase the tool's potential through a
series of use cases, including guided knowledge extraction and semantic
diffing, applied to protocols such as TCP, QUIC, PPTP, and DCCP.
  In practice, RFSeek not only reconstructs the RFC diagrams included in some
specifications, but, more interestingly, also uncovers important logic such as
nodes or edges described in the text but missing from those diagrams. RFSeek
further derives new visualization diagrams for complex RFCs, with QUIC as a
representative case. Our approach, which we term \emph{Summary Visualization},
highlights a promising direction: combining LLMs with formal, user-customized
visualizations to enhance protocol comprehension and support robust
implementations.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [152] [VStyle: A Benchmark for Voice Style Adaptation with Spoken Instructions](https://arxiv.org/abs/2509.09716)
*Jun Zhan,Mingyang Han,Yuxuan Xie,Chen Wang,Dong Zhang,Kexin Huang,Haoxiang Shi,DongXiao Wang,Tengtao Song,Qinyuan Cheng,Shimin Li,Jun Song,Xipeng Qiu,Bo Zheng*

Main category: cs.SD

TL;DR: 介绍语音风格适应（VSA）新任务，提出双语基准VStyle和评估框架，实验表明当前模型在风格适应有局限，发布相关资源促进人机交互。


<details>
  <summary>Details</summary>
Motivation: 多数语音语言模型研究聚焦语义准确性和指令遵循，对基于语音指令调整说话风格能力关注有限，因此提出VSA任务。

Method: 提出双语基准VStyle，涵盖四类语音生成；引入LALM as a Judge框架评估输出。

Result: 在商业系统和开源语音语言模型上实验，发现当前模型在可控风格适应方面存在明显局限。

Conclusion: 发布VStyle和评估工具包，为推进以人为中心的语音交互提供基础。

Abstract: Spoken language models (SLMs) have emerged as a unified paradigm for speech
understanding and generation, enabling natural human machine interaction.
However, while most progress has focused on semantic accuracy and instruction
following, the ability of SLMs to adapt their speaking style based on spoken
instructions has received limited attention. We introduce Voice Style
Adaptation (VSA), a new task that examines whether SLMs can modify their
speaking style, such as timbre, prosody, or persona following natural language
spoken commands. To study this task, we present VStyle, a bilingual (Chinese &
English) benchmark covering four categories of speech generation: acoustic
attributes, natural language instruction, role play, and implicit empathy. We
also introduce the Large Audio Language Model as a Judge (LALM as a Judge)
framework, which progressively evaluates outputs along textual faithfulness,
style adherence, and naturalness, ensuring reproducible and objective
assessment. Experiments on commercial systems and open source SLMs demonstrate
that current models face clear limitations in controllable style adaptation,
highlighting both the novelty and challenge of this task. By releasing VStyle
and its evaluation toolkit, we aim to provide the community with a foundation
for advancing human centered spoken interaction. The dataset and code are
publicly available at
\href{https://junzhan2000.github.io/VStyle.github.io/}{project's homepage}.

</details>


### [153] [Testing chatbots on the creation of encoders for audio conditioned image generation](https://arxiv.org/abs/2509.09717)
*Jorge E. León,Miguel Carrasco*

Main category: cs.SD

TL;DR: 探索用对话代理设计音频编码器替换Stable Diffusion 1.5的CLIP文本编码器，虽多数设计有效但结果不佳，揭示聊天机器人架构偏差及编码差距。


<details>
  <summary>Details</summary>
Motivation: 聊天机器人用于编码任务流行，生成图像模型多用文本编码器，探索用对话代理设计音频编码器实现从声音合成图像。

Method: 提示五个公开聊天机器人提出音频编码器架构，训练并在验证和测试集评估，进行定性分析。

Result: 几乎所有聊天机器人生成的设计有效，但无满意结果，Gemini定量指标最佳，Grok生成图像更连贯。

Conclusion: 揭示聊天机器人架构偏差和编码差距，创建公开演示，提出未来研究问题并鼓励更多专业任务。

Abstract: On one hand, recent advances in chatbots has led to a rising popularity in
using these models for coding tasks. On the other hand, modern generative image
models primarily rely on text encoders to translate semantic concepts into
visual representations, even when there is clear evidence that audio can be
employed as input as well. Given the previous, in this work, we explore whether
state-of-the-art conversational agents can design effective audio encoders to
replace the CLIP text encoder from Stable Diffusion 1.5, enabling image
synthesis directly from sound. We prompted five publicly available chatbots to
propose neural architectures to work as these audio encoders, with a set of
well-explained shared conditions. Each valid suggested encoder was trained on
over two million context related audio-image-text observations, and evaluated
on held-out validation and test sets using various metrics, together with a
qualitative analysis of their generated images. Although almost all chatbots
generated valid model designs, none achieved satisfactory results, indicating
that their audio embeddings failed to align reliably with those of the original
text encoder. Among the proposals, the Gemini audio encoder showed the best
quantitative metrics, while the Grok audio encoder produced more coherent
images (particularly, when paired with the text encoder). Our findings reveal a
shared architectural bias across chatbots and underscore the remaining coding
gap that needs to be bridged in future versions of these models. We also
created a public demo so everyone could study and try out these audio encoders.
Finally, we propose research questions that should be tackled in the future,
and encourage other researchers to perform more focused and highly specialized
tasks like this one, so the respective chatbots cannot make use of well-known
solutions and their creativity/reasoning is fully tested.

</details>


### [154] [SoilSound: Smartphone-based Soil Moisture Estimation](https://arxiv.org/abs/2509.09823)
*Yixuan Gao,Tanvir Ahmed,Shuang He,Zhongqi Cheng,Rajalakshmi Nandakumar*

Main category: cs.SD

TL;DR: 提出基于智能手机的声学传感系统SoilSound，可无损测土壤湿度，评估显示误差小且无需校准。


<details>
  <summary>Details</summary>
Motivation: 现有土壤湿度监测方法存在需侵入土壤或用专业设备的局限，限制公众使用。

Method: 利用手机内置扬声器和麦克风进行垂直扫描，基于表面粗糙度效应建立声学反射模型，将反射信号处理后输入卷积神经网络进行湿度估计。

Result: 在10个不同地点平均绝对误差为2.39%，能准确跟踪15.9% - 34.0%湿度范围。

Conclusion: SoilSound无需校准和扰动土壤，可在多场景广泛用于土壤湿度监测。

Abstract: Soil moisture monitoring is essential for agriculture and environmental
management, yet existing methods require either invasive probes disturbing the
soil or specialized equipment, limiting access to the public. We present
SoilSound, an ubiquitous accessible smartphone-based acoustic sensing system
that can measure soil moisture without disturbing the soil. We leverage the
built-in speaker and microphone to perform a vertical scan mechanism to
accurately measure moisture without any calibration. Unlike existing work that
use transmissive properties, we propose an alternate model for acoustic
reflections in soil based on the surface roughness effect to enable moisture
sensing without disturbing the soil. The system works by sending acoustic
chirps towards the soil and recording the reflections during a vertical scan,
which are then processed and fed to a convolutional neural network for
on-device soil moisture estimation with negligible computational, memory, or
power overhead. We evaluated the system by training with curated soils in boxes
in the lab and testing in the outdoor fields and show that SoilSound achieves a
mean absolute error (MAE) of 2.39% across 10 different locations. Overall, the
evaluation shows that SoilSound can accurately track soil moisture levels
ranging from 15.9% to 34.0% across multiple soil types, environments, and
users; without requiring any calibration or disturbing the soil, enabling
widespread moisture monitoring for home gardeners, urban farmers, citizen
scientists, and agricultural communities in resource-limited settings.

</details>


### [155] [CoDiCodec: Unifying Continuous and Discrete Compressed Representations of Audio](https://arxiv.org/abs/2509.09836)
*Marco Pasini,Stefan Lattner,George Fazekas*

Main category: cs.SD

TL;DR: 介绍CoDiCodec音频自编码器，能兼顾连续嵌入和离散令牌，实现高效音频压缩，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有自编码器在连续嵌入和离散令牌间需做选择，且高压缩比下保持音频保真度有挑战。

Method: 引入有限标量量化（FSQ）和FSQ - dropout技术，采用单一一致性损失进行端到端训练，支持自回归解码和并行解码策略。

Result: 在相似比特率下，CoDiCodec在重建音频质量上优于现有连续和离散自编码器，并行解码有更好音频质量和更快解码速度。

Conclusion: CoDiCodec实现了音频压缩的统一方法，弥合了连续和离散生成建模范式的差距。

Abstract: Efficiently representing audio signals in a compressed latent space is
critical for latent generative modelling. However, existing autoencoders often
force a choice between continuous embeddings and discrete tokens. Furthermore,
achieving high compression ratios while maintaining audio fidelity remains a
challenge. We introduce CoDiCodec, a novel audio autoencoder that overcomes
these limitations by both efficiently encoding global features via summary
embeddings, and by producing both compressed continuous embeddings at ~ 11 Hz
and discrete tokens at a rate of 2.38 kbps from the same trained model,
offering unprecedented flexibility for different downstream generative tasks.
This is achieved through Finite Scalar Quantization (FSQ) and a novel
FSQ-dropout technique, and does not require additional loss terms beyond the
single consistency loss used for end-to-end training. CoDiCodec supports both
autoregressive decoding and a novel parallel decoding strategy, with the latter
achieving superior audio quality and faster decoding. CoDiCodec outperforms
existing continuous and discrete autoencoders at similar bitrates in terms of
reconstruction audio quality. Our work enables a unified approach to audio
compression, bridging the gap between continuous and discrete generative
modelling paradigms.

</details>


### [156] [Prototypical Contrastive Learning For Improved Few-Shot Audio Classification](https://arxiv.org/abs/2509.10074)
*Christos Sgouropoulos,Christos Nikou,Stefanos Vlachos,Vasileios Theiou,Christos Foukanelis,Theodoros Giannakopoulos*

Main category: cs.SD

TL;DR: 研究将监督对比损失集成到音频分类的原型少样本训练中的效果，提出方法在MetaAudio上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 少样本学习在音频分类领域研究不足，需探索有效方法解决音频分类中标记数据有限的问题。

Method: 将监督对比损失集成到原型少样本训练，用角损失替代标准对比损失，利用SpecAugment和自注意力机制将增强输入信息封装到统一嵌入中。

Result: 在MetaAudio基准上进行评估，所提方法在5-way, 5-shot设置下取得了当前最优性能。

Conclusion: 将监督对比损失集成到原型少样本训练对音频分类有效，角损失比标准对比损失更能提升性能。

Abstract: Few-shot learning has emerged as a powerful paradigm for training models with
limited labeled data, addressing challenges in scenarios where large-scale
annotation is impractical. While extensive research has been conducted in the
image domain, few-shot learning in audio classification remains relatively
underexplored. In this work, we investigate the effect of integrating
supervised contrastive loss into prototypical few shot training for audio
classification. In detail, we demonstrate that angular loss further improves
the performance compared to the standard contrastive loss. Our method leverages
SpecAugment followed by a self-attention mechanism to encapsulate diverse
information of augmented input versions into one unified embedding. We evaluate
our approach on MetaAudio, a benchmark including five datasets with predefined
splits, standardized preprocessing, and a comprehensive set of few-shot
learning models for comparison. The proposed approach achieves state-of-the-art
performance in a 5-way, 5-shot setting.

</details>


### [157] [Improving Audio Event Recognition with Consistency Regularization](https://arxiv.org/abs/2509.10391)
*Shanmuka Sadhu,Weiran Wang*

Main category: cs.SD

TL;DR: 本文将一致性正则化（CR）用于音频事件识别，在AudioSet上验证其有效性，通过消融实验展示其在监督和半监督设置下的性能提升。


<details>
  <summary>Details</summary>
Motivation: 将一致性正则化应用于音频事件识别，探索其在该领域的有效性。

Method: 在AudioSet上进行广泛消融实验，分别在小（约20k）和大（约180万）监督训练集上测试，还扩展到半监督设置。

Result: CR在已大量使用数据增强的监督基线模型上带来持续改进，更强和多重增强的CR对小训练集有额外增益，半监督设置下性能也有提升。

Conclusion: 一致性正则化可有效用于音频事件识别，在监督和半监督设置下都能提升性能。

Abstract: Consistency regularization (CR), which enforces agreement between model
predictions on augmented views, has found recent benefits in automatic speech
recognition [1]. In this paper, we propose the use of consistency
regularization for audio event recognition, and demonstrate its effectiveness
on AudioSet. With extensive ablation studies for both small ($\sim$20k) and
large ($\sim$1.8M) supervised training sets, we show that CR brings consistent
improvement over supervised baselines which already heavily utilize data
augmentation, and CR using stronger augmentation and multiple augmentations
leads to additional gain for the small training set. Furthermore, we extend the
use of CR into the semi-supervised setup with 20K labeled samples and 1.8M
unlabeled samples, and obtain performance improvement over our best model
trained on the small set.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [158] [Linear fractional relative risk aversion](https://arxiv.org/abs/2509.09865)
*Kristian Behrens,Yasusada Murata*

Main category: econ.GN

TL;DR: 用高斯超几何函数表征满足线性分数相对风险厌恶（LFRRA）的效用函数族，应用该函数族于垄断竞争并获利润最大化价格的闭式解，由企业级数据确定RRA情况以决定加价与边际成本的关系。


<details>
  <summary>Details</summary>
Motivation: 研究满足LFRRA的效用函数族并应用于垄断竞争问题，以解决利润最大化价格求解和分析加价与边际成本关系。

Method: 用高斯超几何函数表征效用函数族，推广兰伯特W函数求闭式解，让企业级数据确定RRA情况。

Result: 得到了利润最大化价格的闭式解，能根据数据判断RRA情况及加价与边际成本的关系。

Conclusion: 通过对特定效用函数族的研究和应用，可有效分析垄断竞争中的价格和加价问题。

Abstract: We characterize the family of utility functions satisfying linear fractional
relative risk aversion (LFRRA) in terms of the Gauss hypergeometric functions.
We apply this family, which nests various utility functions used in different
strands of literature, to monopolistic competition and obtain a closed-form
solution for the profit-maximizing price by generalizing the Lambert W
function. We let firm-level data decide whether the RRA in each sector or in
the aggregate economy is increasing, decreasing, or constant, which in turn
determines whether markups are decreasing, increasing, or constant with respect
to marginal costs.

</details>


### [159] [Robo-Advisors Beyond Automation: Principles and Roadmap for AI-Driven Financial Planning](https://arxiv.org/abs/2509.09922)
*Runhuan Feng,Hong Li,Ming Liu*

Main category: econ.GN

TL;DR: 本文探讨人工智能在金融规划中的应用，提出负责任AI框架及五级路线图，展示其利弊。


<details>
  <summary>Details</summary>
Motivation: 人工智能虽革新金融规划，但缺乏保障会重现市场低效问题，需构建负责任AI框架。

Method: 提出基于五项原则的负责任AI框架，用案例说明风险与机遇，并拓展为五级路线图，结合技术设计和经济理论。

Result: 展示了AI在金融中介中既能放大脆弱性，也能创造更具韧性和可信度的形式。

Conclusion: 通过构建框架和路线图，说明AI在金融规划中可合理运用，创造更优金融中介形式。

Abstract: Artificial intelligence (AI) is transforming financial planning by expanding
access, lowering costs, and enabling dynamic, data-driven advice. Yet without
clear safeguards, digital platforms risk reproducing longstanding market
inefficiencies such as information asymmetry, misaligned incentives, and
systemic fragility. This paper develops a framework for responsible AI in
financial planning, anchored in five principles: fiduciary duty, adaptive
personalization, technical robustness, ethical and fairness constraints, and
auditability. We illustrate these risks and opportunities through case studies,
and extend the framework into a five-level roadmap of AI financial
intermediaries. By linking technological design to economic theory, we show how
AI can either amplify vulnerabilities or create more resilient, trustworthy
forms of financial intermediation.

</details>


### [160] [Price Formation in a Highly-Renewable, Sector-Coupled Energy System](https://arxiv.org/abs/2509.10092)
*Julian Geis,Fabian Neumann,Michael Lindner,Philipp Härtel,Tom Brown*

Main category: econ.GN

TL;DR: 研究批发电力市场价格形成从化石燃料主导到储能和需求管理主导的转变，分析德国能源系统模型，发现价格曲线变化，强调灵活性和跨部门需求投标对稳定电价的作用。


<details>
  <summary>Details</summary>
Motivation: 随着可变可再生能源增加和需求电气化，分析批发电力市场价格形成的转变。

Method: 引入基于能源系统优化问题对偶变量映射的新方法，构建每小时的供需曲线，分析德国能源系统模型。

Result: 价格从对应化石燃料的不同水平过渡到由可变可再生能源、电池和电解设定的更平滑曲线，全脱碳系统75%时间有非零价格。

Conclusion: 灵活性和跨部门需求投标对气候中和未来稳定电价至关重要，为投资决策和政策提供指导。

Abstract: As variable renewable energy increases and more demand is electrified, we
expect price formation in wholesale electricity markets to transition from
being dominated by fossil fuel generators to being dominated by the opportunity
costs of storage and demand management. In order to analyse this transition, we
introduce a new method to investigate price formation based on a mapping from
the dual variables of the energy system optimisation problem to the bids and
asks of electricity suppliers and consumers. This allows us to build the full
supply and demand curves in each hour. We use this method to analyse price
formation in a sector-coupled, climate-neutral energy system model for Germany,
PyPSA-DE, with high temporal resolution and myopic foresight in 5-year steps
from 2020 until full decarbonisation in 2045. We find a clear transition from
distinct price levels, corresponding to fossil fuels, to a smoother price curve
set by variable renewable energy sources, batteries and electrolysis. Despite
higher price volatility, the fully decarbonised system clears with non-zero
prices in 75% of all hours. Our results suggest that flexibility and
cross-sectoral demand bidding play a vital role in stabilising electricity
prices in a climate-neutral future. These findings are highly relevant for
guiding investment decisions and informing policy, particularly in support of
dynamic pricing, the expansion of energy storage across multiple timescales,
and the coordinated development of renewable and flexibility technologies.

</details>


### [161] [The anatomy of Green AI technologies: structure, evolution, and impact](https://arxiv.org/abs/2509.10109)
*Lorenzo Emer,Andrea Mina,Andrea Vandin*

Main category: econ.GN

TL;DR: 研究通过分析约6.3万个绿色AI专利，探讨AI与气候技术的交叉领域，发现技术领域转变、企业专利集中等情况，指出部分领域需政策干预。


<details>
  <summary>Details</summary>
Motivation: 研究AI与气候适应和缓解技术的交叉领域，了解绿色AI专利现状及影响。

Method: 对约6.3万个绿色AI专利进行分析，用主题建模（BERTopic）识别技术领域。

Result: 发现技术领域从传统转向新兴，企业专利集中，部分绿色AI领域技术与市场价值表现不同。

Conclusion: 部分绿色AI领域需政策干预以促进新应用产生和使用。

Abstract: Artificial intelligence (AI) is a key enabler of innovation against climate
change. In this study, we investigate the intersection of AI and climate
adaptation and mitigation technologies through patent analyses of a novel
dataset of approximately 63 000 Green AI patents. We analyze patenting trends,
corporate ownership of the technology, the geographical distributions of
patents, their impact on follow-on inventions and their market value. We use
topic modeling (BERTopic) to identify 16 major technological domains, track
their evolution over time, and identify their relative impact. We uncover a
clear shift from legacy domains such as combustion engines technology to
emerging areas like data processing, microgrids, and agricultural water
management. We find evidence of growing concentration in corporate patenting
against a rapidly increasing number of patenting firms. Looking at the
technological and economic impact of patents, while some Green AI domains
combine technological impact and market value, others reflect weaker private
incentives for innovation, despite their relevance for climate adaptation and
mitigation strategies. This is where policy intervention might be required to
foster the generation and use of new Green AI applications.

</details>


### [162] [Evaluating the Economic Feasibility of Labor Replacement Through Robotics and Automation in Qatar](https://arxiv.org/abs/2509.10152)
*Tariq Eldakruri,Edip Senyurek*

Main category: econ.GN

TL;DR: 本文研究卡塔尔制造和服务行业用机器人和自动化替代人力劳动的经济可行性，给出可行领域和面临挑战，为政策制定者和行业利益相关者提供指导。


<details>
  <summary>Details</summary>
Motivation: 探究卡塔尔制造和服务行业用机器人和自动化替代人力劳动的经济可行性。

Method: 分析劳动力成本、生产率提升和实施费用，评估机器人集成的潜在财务影响和投资回报。

Result: 指出自动化在经济上可行的行业，识别出劳动力适应、政策和基础设施相关的挑战。

Conclusion: 研究结果为卡塔尔考虑自动化战略的政策制定者和行业利益相关者提供了指导。

Abstract: This paper investigates the economic feasibility of replacing human labor
with robotics and automation in Qatar's manufacturing and service sectors. By
analyzing labor costs, productivity gains, and implementation expenses, the
study assesses the potential financial impact and return on investment of
robotic integration. Results indicate the sectors where automation is
economically viable and identify challenges related to workforce adaptation,
policy, and infrastructure. These insights provide guidance for policymakers
and industry stakeholders considering automation strategies in Qatar.

</details>


### [163] [The temporary impact of permanent hiring incentives: Evidence from Italy](https://arxiv.org/abs/2509.10193)
*Michele Cantarella,Maria Cristina Maurizio,Francesco Serti*

Main category: econ.GN

TL;DR: 本文评估招聘激励政策对临时合同永久转换的中短期效果，发现激励有短期积极影响但长期效果为零。


<details>
  <summary>Details</summary>
Motivation: 评估旨在通过社保缴费减免促进临时合同永久转换的招聘激励政策的中短期效果。

Method: 利用托斯卡纳丰富的行政数据，采用双重差分和断点回归设计，分析2018年资格标准的独特变化。

Result: 激励政策立即提高了合同转换概率，且无对非合格群体的替代效应，但积极效果短暂，长期对永久雇佣无影响。

Conclusion: 招聘激励政策的积极效果是短期的，反映的可能是预期转换，长期对永久雇佣无实质作用。

Abstract: This paper evaluates the short and medium-term effectiveness of hiring
incentives aimed at promoting the permanent conversion of temporary contracts
through social contribution exemptions. Using rich administrative data from
Tuscany, providing detailed employment histories, we use difference in
differences and regression discontinuity designs to exploit a unique change in
eligibility criteria in 2018. We find that the incentives immediately increased
the probability of conversion, with no evidence of substitution against
non-eligible cohorts. However, these positive effects were short-lived and
appear to reflect anticipated conversions, as we find null longer-term effects
on permanent hirings.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [164] [Self-Augmented Robot Trajectory: Efficient Imitation Learning via Safe Self-augmentation with Demonstrator-annotated Precision](https://arxiv.org/abs/2509.09893)
*Hanbit Oh,Masaki Murooka,Tomohiro Motoda,Ryoichi Nakajo,Yukiyasu Domae*

Main category: cs.RO

TL;DR: 提出SART框架，可从单次人类演示中学习策略并自主扩展数据集，提高数据收集效率且保证安全，评估显示其成功率高于仅基于人类演示训练的策略。


<details>
  <summary>Details</summary>
Motivation: 标准模仿学习方法需大量数据采集，探索缺乏安全保障且增加人力负担，需改进。

Method: SART框架分两个阶段：人类仅演示一次并标注关键路点精度边界，机器人在边界内生成无碰撞轨迹并与原演示重新连接。

Result: 在仿真和现实操作任务评估中，SART成功率明显高于仅基于人类演示训练的策略。

Conclusion: SART框架可提高数据收集效率并保证安全，有效提升机器人策略学习效果。

Abstract: Imitation learning is a promising paradigm for training robot agents;
however, standard approaches typically require substantial data acquisition --
via numerous demonstrations or random exploration -- to ensure reliable
performance. Although exploration reduces human effort, it lacks safety
guarantees and often results in frequent collisions -- particularly in
clearance-limited tasks (e.g., peg-in-hole) -- thereby, necessitating manual
environmental resets and imposing additional human burden. This study proposes
Self-Augmented Robot Trajectory (SART), a framework that enables policy
learning from a single human demonstration, while safely expanding the dataset
through autonomous augmentation. SART consists of two stages: (1) human
teaching only once, where a single demonstration is provided and precision
boundaries -- represented as spheres around key waypoints -- are annotated,
followed by one environment reset; (2) robot self-augmentation, where the robot
generates diverse, collision-free trajectories within these boundaries and
reconnects to the original demonstration. This design improves the data
collection efficiency by minimizing human effort while ensuring safety.
Extensive evaluations in simulation and real-world manipulation tasks show that
SART achieves substantially higher success rates than policies trained solely
on human-collected demonstrations. Video results available at
https://sites.google.com/view/sart-il .

</details>


### [165] [TwinTac: A Wide-Range, Highly Sensitive Tactile Sensor with Real-to-Sim Digital Twin Sensor Model](https://arxiv.org/abs/2509.10063)
*Xiyan Huang,Zhe Xu,Chenxi Xiao*

Main category: cs.RO

TL;DR: 提出TwinTac系统结合物理触觉传感器与数字孪生模型，实验验证其在跨域学习任务中可弥合差距。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习中因缺乏触觉传感器仿真模型，阻碍触觉感知技能学习的问题。

Method: 设计高灵敏度和宽测量范围的硬件传感器，用实到虚方法开发数字孪生模型，收集同步跨域数据并训练神经网络。

Result: 表征物理传感器灵敏度，证明数字孪生模型输出一致性，在物体分类任务中数字孪生传感器生成的数据可提高准确率。

Conclusion: TwinTac有潜力弥合跨域学习任务的差距。

Abstract: Robot skill acquisition processes driven by reinforcement learning often rely
on simulations to efficiently generate large-scale interaction data. However,
the absence of simulation models for tactile sensors has hindered the use of
tactile sensing in such skill learning processes, limiting the development of
effective policies driven by tactile perception. To bridge this gap, we present
TwinTac, a system that combines the design of a physical tactile sensor with
its digital twin model. Our hardware sensor is designed for high sensitivity
and a wide measurement range, enabling high quality sensing data essential for
object interaction tasks. Building upon the hardware sensor, we develop the
digital twin model using a real-to-sim approach. This involves collecting
synchronized cross-domain data, including finite element method results and the
physical sensor's outputs, and then training neural networks to map simulated
data to real sensor responses. Through experimental evaluation, we
characterized the sensitivity of the physical sensor and demonstrated the
consistency of the digital twin in replicating the physical sensor's output.
Furthermore, by conducting an object classification task, we showed that
simulation data generated by our digital twin sensor can effectively augment
real-world data, leading to improved accuracy. These results highlight
TwinTac's potential to bridge the gap in cross-domain learning tasks.

</details>


### [166] [Robot guide with multi-agent control and automatic scenario generation with LLM](https://arxiv.org/abs/2509.10317)
*Elizaveta D. Moskovskaya,Anton D. Moscowsky*

Main category: cs.RO

TL;DR: 本文开发了用于拟人导游机器人的混合控制架构，结合多智能体资源管理系统与基于大语言模型的自动行为场景生成，试验展示其潜力。


<details>
  <summary>Details</summary>
Motivation: 克服传统系统依赖手动调整行为场景的局限，如手动配置、灵活性低和行为缺乏自然性。

Method: 采用两阶段生成准备游览场景，先创建风格化叙述，再将非语言动作标签集成到文本中，用多智能体系统确保并行动作执行时的协调和冲突解决。

Result: 试验结果显示该方法在自动化和扩展社交机器人控制系统方面有潜力。

Conclusion: 所提出的方法可用于自动化和扩展社交机器人控制系统。

Abstract: The work describes the development of a hybrid control architecture for an
anthropomorphic tour guide robot, combining a multi-agent resource management
system with automatic behavior scenario generation based on large language
models. The proposed approach aims to overcome the limitations of traditional
systems, which rely on manual tuning of behavior scenarios. These limitations
include manual configuration, low flexibility, and lack of naturalness in robot
behavior. The process of preparing tour scenarios is implemented through a
two-stage generation: first, a stylized narrative is created, then non-verbal
action tags are integrated into the text. The multi-agent system ensures
coordination and conflict resolution during the execution of parallel actions,
as well as maintaining default behavior after the completion of main
operations, contributing to more natural robot behavior. The results obtained
from the trial demonstrate the potential of the proposed approach for
automating and scaling social robot control systems.

</details>


### [167] [Efficient Learning-Based Control of a Legged Robot in Lunar Gravity](https://arxiv.org/abs/2509.10128)
*Philip Arm,Oliver Fischer,Joseph Church,Adrian Fuhrer,Hendrik Kolvenbach,Marco Hutter*

Main category: cs.RO

TL;DR: 本文提出基于强化学习的控制方法，用重力缩放的功率优化奖励函数为多重力环境下的腿式机器人开发控制器，实现节能且在多重力环境可扩展。


<details>
  <summary>Details</summary>
Motivation: 行星机器人功率和热预算受限，需节能且能适应多重力环境的控制方法。

Method: 引入基于强化学习的控制方法，使用重力缩放的功率优化奖励函数，开发并验证运动和姿态控制器，还设计了恒力弹簧卸载系统进行实验。

Result: 该方法在多重力环境能成功扩展；在地球重力下，功率优化的运动控制器功耗降低23%；在月球重力下，功率优化的控制策略功耗比基线控制器低36%。

Conclusion: 该方法为多重力水平的腿式机器人开发节能运动控制器提供了可扩展的途径。

Abstract: Legged robots are promising candidates for exploring challenging areas on
low-gravity bodies such as the Moon, Mars, or asteroids, thanks to their
advanced mobility on unstructured terrain. However, as planetary robots' power
and thermal budgets are highly restricted, these robots need energy-efficient
control approaches that easily transfer to multiple gravity environments. In
this work, we introduce a reinforcement learning-based control approach for
legged robots with gravity-scaled power-optimized reward functions. We use our
approach to develop and validate a locomotion controller and a base pose
controller in gravity environments from lunar gravity (1.62 m/s2) to a
hypothetical super-Earth (19.62 m/s2). Our approach successfully scales across
these gravity levels for locomotion and base pose control with the
gravity-scaled reward functions. The power-optimized locomotion controller
reached a power consumption for locomotion of 23.4 W in Earth gravity on a
15.65 kg robot at 0.4 m/s, a 23 % improvement over the baseline policy.
Additionally, we designed a constant-force spring offload system that allowed
us to conduct real-world experiments on legged locomotion in lunar gravity. In
lunar gravity, the power-optimized control policy reached 12.2 W, 36 % less
than a baseline controller which is not optimized for power efficiency. Our
method provides a scalable approach to developing power-efficient locomotion
controllers for legged robots across multiple gravity levels.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [168] [Off Policy Lyapunov Stability in Reinforcement Learning](https://arxiv.org/abs/2509.09863)
*Sarvan Gill,Daniela Constantinescu*

Main category: eess.SY

TL;DR: 本文提出离策略学习Lyapunov函数方法，将其融入算法提供数据高效的稳定性证明，通过仿真验证性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习缺乏稳定性保证，现有自学习Lyapunov函数因在线策略性质导致样本效率低。

Method: 引入离策略学习Lyapunov函数方法，并将其融入Soft Actor Critic和Proximal Policy Optimization算法。

Result: 倒立摆和四旋翼仿真显示，融入离策略Lyapunov函数后两个算法性能提升。

Conclusion: 所提离策略学习Lyapunov函数方法能为算法提供数据高效的稳定性证明，提升算法性能。

Abstract: Traditional reinforcement learning lacks the ability to provide stability
guarantees. More recent algorithms learn Lyapunov functions alongside the
control policies to ensure stable learning. However, the current self-learned
Lyapunov functions are sample inefficient due to their on-policy nature. This
paper introduces a method for learning Lyapunov functions off-policy and
incorporates the proposed off-policy Lyapunov function into the Soft Actor
Critic and Proximal Policy Optimization algorithms to provide them with a data
efficient stability certificate. Simulations of an inverted pendulum and a
quadrotor illustrate the improved performance of the two algorithms when
endowed with the proposed off-policy Lyapunov function.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [169] [Sparse Polyak: an adaptive step size rule for high-dimensional M-estimation](https://arxiv.org/abs/2509.09802)
*Tianqi Qiao,Marie Maros*

Main category: math.OC

TL;DR: 提出Sparse Polyak解决高维统计估计问题，理论分析和实验证明其性能提升。


<details>
  <summary>Details</summary>
Motivation: 标准Polyak步长在高维统计估计问题（维度增长远超样本量）中表现差，需更多迭代达到最优统计精度。

Method: 修改步长以估计限制Lipschitz平滑常数，提出Sparse Polyak。

Result: 通过理论分析和数值实验证明Sparse Polyak性能有提升。

Conclusion: Sparse Polyak能有效解决高维统计估计问题，性能优于标准Polyak步长。

Abstract: We propose and study Sparse Polyak, a variant of Polyak's adaptive step size,
designed to solve high-dimensional statistical estimation problems where the
problem dimension is allowed to grow much faster than the sample size. In such
settings, the standard Polyak step size performs poorly, requiring an
increasing number of iterations to achieve optimal statistical precision-even
when, the problem remains well conditioned and/or the achievable precision
itself does not degrade with problem size. We trace this limitation to a
mismatch in how smoothness is measured: in high dimensions, it is no longer
effective to estimate the Lipschitz smoothness constant. Instead, it is more
appropriate to estimate the smoothness restricted to specific directions
relevant to the problem (restricted Lipschitz smoothness constant). Sparse
Polyak overcomes this issue by modifying the step size to estimate the
restricted Lipschitz smoothness constant. We support our approach with both
theoretical analysis and numerical experiments, demonstrating its improved
performance.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [170] [Nearly optimal algorithms to learn sparse quantum Hamiltonians in physically motivated distances](https://arxiv.org/abs/2509.09813)
*Amira Abbas,Nunzia Cerrato,Francisco Escudero Gutiérrez,Dmitry Grinko,Francesco Anna Mele,Pulkit Sinha*

Main category: quant-ph

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study the problem of learning Hamiltonians $H$ that are $s$-sparse in the
Pauli basis, given access to their time evolution. Although Hamiltonian
learning has been extensively investigated, two issues recur in much of the
existing literature: the absence of matching lower bounds and the use of
mathematically convenient but physically opaque error measures.
  We address both challenges by introducing two physically motivated distances
between Hamiltonians and designing a nearly optimal algorithm with respect to
one of these metrics. The first, time-constrained distance, quantifies
distinguishability through dynamical evolution up to a bounded time. The
second, temperature-constrained distance, captures distinguishability through
thermal states at bounded inverse temperatures.
  We show that $s$-sparse Hamiltonians with bounded operator norm can be
learned in both distances with $O(s \log(1/\epsilon))$ experiments and
$O(s^2/\epsilon)$ evolution time. For the time-constrained distance, we further
establish lower bounds of $\Omega((s/n)\log(1/\epsilon) + s)$ experiments and
$\Omega(\sqrt{s}/\epsilon)$ evolution time, demonstrating near-optimality in
the number of experiments.
  As an intermediate result, we obtain an algorithm that learns every Pauli
coefficient of $s$-sparse Hamiltonians up to error $\epsilon$ in
$O(s\log(1/\epsilon))$ experiments and $O(s/\epsilon)$ evolution time,
improving upon several recent results.
  The source of this improvement is a new isolation technique, inspired by the
Valiant-Vazirani theorem (STOC'85), which shows that NP is as easy as detecting
unique solutions. This isolation technique allows us to query the time
evolution of a single Pauli coefficient of a sparse Hamiltonian--even when the
Pauli support of the Hamiltonian is unknown--ultimately enabling us to recover
the Pauli support itself.

</details>


### [171] [Toward Minimum Graphic Parity Networks](https://arxiv.org/abs/2509.10070)
*Yixin Cao,Yiren Lu,Junhong Nie,Xiaoming Sun,Guojing Tian*

Main category: quant-ph

TL;DR: 本文从理论角度研究图形奇偶网络合成问题以优化量子电路合成，给出了门数量下界，提出随机算法，并探索特定图类的最小图形奇偶网络合成。


<details>
  <summary>Details</summary>
Motivation: 量子电路由CNOT和$R_z$组成，是许多量子算法的基本构建块，优化此类量子电路的合成至关重要。

Method: 研究图形奇偶网络合成问题，给出理论下界，提出简单随机算法，探索特定图类并给出线性时间算法，还研究图类识别问题及相应算法。

Result: 给出连通图图形奇偶网络门数量下界，提出随机算法合成预期门数量的图形奇偶网络，探索特定图类并给出线性时间合成算法，证明图类识别是$	extsf{NP}$-完全问题并给出固定参数可处理算法。

Conclusion: 为量子电路合成提供了理论下界和算法，对特定图类的图形奇偶网络合成有一定成果，但图类识别存在$	extsf{NP}$-完全问题。

Abstract: Quantum circuits composed of CNOT and $R_z$ are fundamental building blocks
of many quantum algorithms, so optimizing the synthesis of such quantum
circuits is crucial. We address this problem from a theoretical perspective by
studying the graphic parity network synthesis problem. A graphic parity network
for a graph $G$ is a quantum circuit composed solely of CNOT gates where each
edge of $G$ is represented in the circuit, and the final state of the wires
matches the original input. We aim to synthesize graphic parity networks with
the minimum number of gates, specifically for quantum algorithms addressing
combinatorial optimization problems with Ising formulations. We demonstrate
that a graphic parity network for a connected graph with $n$ vertices and $m$
edges requires at least $m+n-1$ gates. This lower bound can be improved to
$m+\Omega(m) = m+\Omega(n^{1.5})$ when the shortest cycle in the graph has a
length of at least five. We complement this result with a simple randomized
algorithm that synthesizes a graphic parity network with expected $m +
O(n^{1.5}\sqrt{\log n})$ gates. Additionally, we begin exploring connected
graphs that allow for graphic parity networks with exactly $m+n-1$ gates. We
conjecture that all such graphs belong to a newly defined graph class.
Furthermore, we present a linear-time algorithm for synthesizing minimum
graphic parity networks for graphs within this class. However, this graph class
is not closed under taking induced subgraphs, and we show that recognizing it
is $\textsf{NP}$-complete, which is complemented with a fixed-parameter
tractable algorithm parameterized by the treewidth.

</details>


### [172] [Certifying and learning quantum Ising Hamiltonians](https://arxiv.org/abs/2509.10239)
*Andreas Bluhm,Matthias C. Caro,Francisco Escudero Gutiérrez,Aadil Oufkir,Cambyse Rouzé*

Main category: quant-ph

TL;DR: 本文研究量子Ising哈密顿量的认证与学习问题，给出了认证Ising哈密顿量、学习和认证Ising吉布斯态的算法，并将结果扩展到一般k - 局部哈密顿量。


<details>
  <summary>Details</summary>
Motivation: 解决量子Ising哈密顿量的认证和学习问题，改进以往算法在样本复杂度等方面的不足。

Method: 在认证Ising哈密顿量分析中使用Fourier分析的Bonami引理；设计样本高效的算法来学习和认证Ising吉布斯态。

Result: 认证Ising哈密顿量的时间演化复杂度接近最优；设计的学习Ising吉布斯态算法在所有参数上样本高效；给出的认证Ising吉布斯态算法样本和时间均高效，并解决了Anshu提出的问题；将结果扩展到一般k - 局部哈密顿量。

Conclusion: 提出了近最优的哈密顿量属性测试算法，改进了Ising吉布斯态学习和认证算法，并可推广到一般k - 局部哈密顿量。

Abstract: In this work, we study the problems of certifying and learning quantum Ising
Hamiltonians. Our main contributions are as follows:
  Certification of Ising Hamiltonians. We show that certifying an Ising
Hamiltonian in normalized Frobenius norm via access to its time-evolution
operator requires only $\widetilde O(1/\varepsilon)$ time evolution. This
matches the Heisenberg-scaling lower bound of $\Omega(1/\varepsilon)$ up to
logarithmic factors. To our knowledge, this is the first nearly-optimal
algorithm for testing a Hamiltonian property. A key ingredient in our analysis
is the Bonami Lemma from Fourier analysis.
  Learning Ising Gibbs states. We design an algorithm for learning Ising Gibbs
states in trace norm that is sample-efficient in all parameters. In contrast,
previous approaches learned the underlying Hamiltonian (which implies learning
the Gibbs state) but suffered from exponential sample complexity in the inverse
temperature.
  Certification of Ising Gibbs states. We give an algorithm for certifying
Ising Gibbs states in trace norm that is both sample and time-efficient,
thereby solving a question posed by Anshu (Harvard Data Science Review, 2022).
  Finally, we extend our results on learning and certification of Gibbs states
to general $k$-local Hamiltonians for any constant $k$.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [173] [Machine-learning competition to grade EEG background patterns in newborns with hypoxic-ischaemic encephalopathy](https://arxiv.org/abs/2509.09695)
*Fabio Magarelli,Geraldine B. Boylan,Saeed Montazeri,Feargal O'Sullivan,Dominic Lightbody,Minoo Ashoori,Tamara Skoric Ceranic,John M. O'Toole*

Main category: eess.SP

TL;DR: 本文通过举办机器学习竞赛开发新生儿脑电图背景模式分类模型，发现深度学习模型泛化性更好，但所有模型在验证集表现下滑，强调使用大而多样数据集及留出验证集的重要性。


<details>
  <summary>Details</summary>
Motivation: 开发准确可靠的机器学习模型监测高危新生儿脑功能，但高质量标注数据稀缺，竞赛可解决数据获取、共享学习等问题。

Method: 收集多中心研究的新生儿脑电图数据，匿名化并划分数据集，创建网络竞赛平台举办竞赛，竞赛结束后用留出验证集评估前4名模型。

Result: 基于特征的模型在测试集排名第一，但深度学习模型在验证集泛化性更好，所有方法验证集性能较测试集显著下降。

Conclusion: 强调训练机器学习模型使用大而多样数据集以确保泛化性，竞赛表明开放数据和协作式开发能促进协作研究，加速新生儿神经监测临床决策支持工具的开发。

Abstract: Machine learning (ML) has the potential to support and improve expert
performance in monitoring the brain function of at-risk newborns. Developing
accurate and reliable ML models depends on access to high-quality, annotated
data, a resource in short supply. ML competitions address this need by
providing researchers access to expertly annotated datasets, fostering shared
learning through direct model comparisons, and leveraging the benefits of
crowdsourcing diverse expertise. We compiled a retrospective dataset containing
353 hours of EEG from 102 individual newborns from a multi-centre study. The
data was fully anonymised and divided into training, testing, and held-out
validation datasets. EEGs were graded for the severity of abnormal background
patterns. Next, we created a web-based competition platform and hosted a
machine learning competition to develop ML models for classifying the severity
of EEG background patterns in newborns. After the competition closed, the top 4
performing models were evaluated offline on a separate held-out validation
dataset. Although a feature-based model ranked first on the testing dataset,
deep learning models generalised better on the validation sets. All methods had
a significant decline in validation performance compared to the testing
performance. This highlights the challenges for model generalisation on unseen
data, emphasising the need for held-out validation datasets in ML studies with
neonatal EEG. The study underscores the importance of training ML models on
large and diverse datasets to ensure robust generalisation. The competition's
outcome demonstrates the potential for open-access data and collaborative ML
development to foster a collaborative research environment and expedite the
development of clinical decision-support tools for neonatal neuromonitoring.

</details>


### [174] [FetalSleepNet: A Transfer Learning Framework with Spectral Equalisation Domain Adaptation for Fetal Sleep Stage Classification](https://arxiv.org/abs/2509.10082)
*Weitao Tang,Johann Vargas-Calixto,Nasim Katebi,Nhi Tran,Sharmony B. Kelly,Gari D. Clifford,Robert Galinsky,Faezeh Marzbanrad*

Main category: eess.SP

TL;DR: 本文提出 FetalSleepNet 用于从绵羊胎儿脑电图中分类睡眠状态，通过迁移学习和领域适应策略训练，取得较好结果，适用于低功耗实时可穿戴胎儿监测系统。


<details>
  <summary>Details</summary>
Motivation: 胎儿脑电图获取复杂且解读困难，准确睡眠阶段分类有助于早期检测与妊娠并发症相关的异常脑成熟情况。

Method: 在 24 只晚期妊娠胎羊的顶叶皮质硬脑膜上固定脑电图电极，使用从成人脑电图的迁移学习在绵羊脑电图上训练轻量级深度神经网络，并采用基于频谱均衡的领域适应策略减少跨领域不匹配。

Result: 直接迁移效果差，全微调结合频谱均衡取得最佳整体性能，准确率 86.6%，宏观 F1 分数 62.5，优于基线模型。

Conclusion: FetalSleepNet 是首个专为胎儿脑电图自动睡眠分期开发的深度学习框架，可作为标签引擎，其轻量级设计适合部署在低功耗、实时和可穿戴胎儿监测系统中。

Abstract: Introduction: This study presents FetalSleepNet, the first published deep
learning approach to classifying sleep states from the ovine
electroencephalogram (EEG). Fetal EEG is complex to acquire and difficult and
laborious to interpret consistently. However, accurate sleep stage
classification may aid in the early detection of abnormal brain maturation
associated with pregnancy complications (e.g. hypoxia or intrauterine growth
restriction).
  Methods: EEG electrodes were secured onto the ovine dura over the parietal
cortices of 24 late gestation fetal sheep. A lightweight deep neural network
originally developed for adult EEG sleep staging was trained on the ovine EEG
using transfer learning from adult EEG. A spectral equalisation-based domain
adaptation strategy was used to reduce cross-domain mismatch.
  Results: We demonstrated that while direct transfer performed poorly, full
fine tuning combined with spectral equalisation achieved the best overall
performance (accuracy: 86.6 percent, macro F1-score: 62.5), outperforming
baseline models.
  Conclusions: To the best of our knowledge, FetalSleepNet is the first deep
learning framework specifically developed for automated sleep staging from the
fetal EEG. Beyond the laboratory, the EEG-based sleep stage classifier
functions as a label engine, enabling large scale weak/semi supervised labeling
and distillation to facilitate training on less invasive signals that can be
acquired in the clinic, such as Doppler Ultrasound or electrocardiogram data.
FetalSleepNet's lightweight design makes it well suited for deployment in low
power, real time, and wearable fetal monitoring systems.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [175] [Automated Tuning for Diffusion Inverse Problem Solvers without Generative Prior Retraining](https://arxiv.org/abs/2509.09880)
*Yaşar Utku Alçalar,Junno Yun,Mehmet Akçakaya*

Main category: eess.IV

TL;DR: 提出零样本自适应扩散采样（ZADS）方法用于加速MRI重建，自适应调整保真度权重，实验显示其性能优于传统和现有扩散方法。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型解决逆问题时依赖手动调整保真度权重，且现有方法在不同测量条件和时间步长下泛化性差。

Method: 提出ZADS方法，将去噪过程视为固定展开采样器，仅使用欠采样测量以自监督方式优化保真度权重。

Result: 在fastMRI膝盖数据集实验中，ZADS始终优于传统压缩感知和近期基于扩散的方法。

Conclusion: ZADS能在不同噪声调度和采集设置下实现高保真重建。

Abstract: Diffusion/score-based models have recently emerged as powerful generative
priors for solving inverse problems, including accelerated MRI reconstruction.
While their flexibility allows decoupling the measurement model from the
learned prior, their performance heavily depends on carefully tuned data
fidelity weights, especially under fast sampling schedules with few denoising
steps. Existing approaches often rely on heuristics or fixed weights, which
fail to generalize across varying measurement conditions and irregular timestep
schedules. In this work, we propose Zero-shot Adaptive Diffusion Sampling
(ZADS), a test-time optimization method that adaptively tunes fidelity weights
across arbitrary noise schedules without requiring retraining of the diffusion
prior. ZADS treats the denoising process as a fixed unrolled sampler and
optimizes fidelity weights in a self-supervised manner using only undersampled
measurements. Experiments on the fastMRI knee dataset demonstrate that ZADS
consistently outperforms both traditional compressed sensing and recent
diffusion-based methods, showcasing its ability to deliver high-fidelity
reconstructions across varying noise schedules and acquisition settings.

</details>


### [176] [Accelerating 3D Photoacoustic Computed Tomography with End-to-End Physics-Aware Neural Operators](https://arxiv.org/abs/2509.09894)
*Jiayun Wang,Yousuf Aborahama,Arya Khokhar,Yang Zhang,Chuwei Wang,Karteekeya Sastry,Julius Berner,Yilin Luo,Boris Bonev,Zongyi Li,Kamyar Azizzadenesheli,Lihong V. Wang,Anima Anandkumar*

Main category: eess.IV

TL;DR: 本文介绍端到端物理感知模型Pano用于3D光声计算机断层扫描（PACT）成像，能高效重建高质量图像，降低硬件要求。


<details>
  <summary>Details</summary>
Motivation: 现有3D PACT系统需密集换能器阵列和长时间采集，限制临床转化。

Method: 引入Pano模型，采用球面离散 - 连续卷积，结合亥姆霍兹方程约束，分辨率独立。

Result: Pano在模拟和真实实验数据上展示了鲁棒性和高效性，减少换能器数量和有限角度采集下仍表现一致。

Conclusion: 该进展为3D PACT临床应用提供可行途径，不损重建质量情况下降低硬件要求。

Abstract: Photoacoustic computed tomography (PACT) combines optical contrast with
ultrasonic resolution, achieving deep-tissue imaging beyond the optical
diffusion limit. While three-dimensional PACT systems enable high-resolution
volumetric imaging for applications spanning transcranial to breast imaging,
current implementations require dense transducer arrays and prolonged
acquisition times, limiting clinical translation. We introduce Pano (PACT
imaging neural operator), an end-to-end physics-aware model that directly
learns the inverse acoustic mapping from sensor measurements to volumetric
reconstructions. Unlike existing approaches (e.g. universal back-projection
algorithm), Pano learns both physics and data priors while also being agnostic
to the input data resolution. Pano employs spherical discrete-continuous
convolutions to preserve hemispherical sensor geometry, incorporates Helmholtz
equation constraints to ensure physical consistency and operates
resolutionindependently across varying sensor configurations. We demonstrate
the robustness and efficiency of Pano in reconstructing high-quality images
from both simulated and real experimental data, achieving consistent
performance even with significantly reduced transducer counts and limited-angle
acquisition configurations. The framework maintains reconstruction fidelity
across diverse sparse sampling patterns while enabling real-time volumetric
imaging capabilities. This advancement establishes a practical pathway for
making 3D PACT more accessible and feasible for both preclinical research and
clinical applications, substantially reducing hardware requirements without
compromising image reconstruction quality.

</details>


### [177] [Drone-Based Multispectral Imaging and Deep Learning for Timely Detection of Branched Broomrape in Tomato Farms](https://arxiv.org/abs/2509.09972)
*Mohammadreza Narimani,Alireza Pourreza,Ali Moghimi,Mohsen Mesgaran,Parastoo Farajpoor,Hamid Jafarbiglu*

Main category: eess.IV

TL;DR: 研究结合无人机多光谱图像与LSTM深度学习网络，在加州一农场检测分枝列当，结果显示该方法对早期列当检测有潜力，或成精准农业工具。


<details>
  <summary>Details</summary>
Motivation: 分枝列当威胁加州番茄产业，其地下生命周期使早期检测难，传统化学控制成本高、有害且低效。

Method: 结合无人机多光谱图像与LSTM深度学习网络，用SMOTE处理类别不平衡问题，在加州一农场5个关键生长阶段开展研究。

Result: 在897 GDD时，不整合后期阶段列当检测总体准确率79.09%、召回率70.36%；整合所有生长阶段并经SMOTE增强后，总体准确率88.37%、召回率95.37%。

Conclusion: 时间多光谱分析和LSTM网络对早期列当检测有强大潜力，基于无人机的多光谱传感与深度学习或成精准农业工具。

Abstract: This study addresses the escalating threat of branched broomrape (Phelipanche
ramosa) to California's tomato industry, which supplies over 90 percent of U.S.
processing tomatoes. The parasite's largely underground life cycle makes early
detection difficult, while conventional chemical controls are costly,
environmentally harmful, and often ineffective. To address this, we combined
drone-based multispectral imagery with Long Short-Term Memory (LSTM) deep
learning networks, using the Synthetic Minority Over-sampling Technique (SMOTE)
to handle class imbalance. Research was conducted on a known broomrape-infested
tomato farm in Woodland, Yolo County, CA, across five key growth stages
determined by growing degree days (GDD). Multispectral images were processed to
isolate tomato canopy reflectance. At 897 GDD, broomrape could be detected with
79.09 percent overall accuracy and 70.36 percent recall without integrating
later stages. Incorporating sequential growth stages with LSTM improved
detection substantially. The best-performing scenario, which integrated all
growth stages with SMOTE augmentation, achieved 88.37 percent overall accuracy
and 95.37 percent recall. These results demonstrate the strong potential of
temporal multispectral analysis and LSTM networks for early broomrape
detection. While further real-world data collection is needed for practical
deployment, this study shows that UAV-based multispectral sensing coupled with
deep learning could provide a powerful precision agriculture tool to reduce
losses and improve sustainability in tomato production.

</details>


### [178] [Multi-pathology Chest X-ray Classification with Rejection Mechanisms](https://arxiv.org/abs/2509.10348)
*Yehudit Aperstein,Amit Tzahar,Alon Gottlib,Tal Verber,Ravit Shagan Damti,Alexander Apartsin*

Main category: eess.IV

TL;DR: 研究提出基于DenseNet - 121的不确定性感知框架用于胸部X光诊断，用两种选择性预测机制提升可靠性，实验表明选择性拒绝能改善诊断准确性和覆盖率的权衡。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型的过度自信在高风险的胸部X光多标签分类等医学成像任务中存在重大风险，需提升模型可靠性。

Method: 引入基于DenseNet - 121的不确定性感知框架，采用熵基拒绝和置信区间基拒绝两种选择性预测机制，用分位数校准程序调整拒绝阈值。

Result: 在三个公共数据集实验显示，选择性拒绝改善了诊断准确性和覆盖率的权衡，熵基拒绝在所有病理上平均AUC最高。

Conclusion: 支持将选择性预测集成到AI辅助诊断工作流程，是深度学习在临床安全、不确定性感知部署的实际一步。

Abstract: Overconfidence in deep learning models poses a significant risk in
high-stakes medical imaging tasks, particularly in multi-label classification
of chest X-rays, where multiple co-occurring pathologies must be detected
simultaneously. This study introduces an uncertainty-aware framework for chest
X-ray diagnosis based on a DenseNet-121 backbone, enhanced with two selective
prediction mechanisms: entropy-based rejection and confidence interval-based
rejection. Both methods enable the model to abstain from uncertain predictions,
improving reliability by deferring ambiguous cases to clinical experts. A
quantile-based calibration procedure is employed to tune rejection thresholds
using either global or class-specific strategies. Experiments conducted on
three large public datasets (PadChest, NIH ChestX-ray14, and MIMIC-CXR)
demonstrate that selective rejection improves the trade-off between diagnostic
accuracy and coverage, with entropy-based rejection yielding the highest
average AUC across all pathologies. These results support the integration of
selective prediction into AI-assisted diagnostic workflows, providing a
practical step toward safer, uncertainty-aware deployment of deep learning in
clinical settings.

</details>


<div id='physics.app-ph'></div>

# physics.app-ph [[Back]](#toc)

### [179] [Reinforcement learning for spin torque oscillator tasks](https://arxiv.org/abs/2509.10057)
*Jakub Mojsiejuk,Sławomir Ziętek,Witold Skowroński*

Main category: physics.app-ph

TL;DR: 使用强化学习解决自旋电子振荡器自动同步问题，模拟训练并改进任务获成效。


<details>
  <summary>Details</summary>
Motivation: 解决自旋电子振荡器的自动同步问题。

Method: 用宏观自旋Landau - Lifschitz - Gilbert - Slonczewski方程数值解模拟振荡器，训练两种强化学习智能体在固定步数内与目标频率同步，并探索任务修改。

Result: 在模拟环境中实现了同步收敛性和能量效率的提升。

Conclusion: 通过强化学习及任务修改，可在模拟环境中改善自旋电子振荡器同步的收敛性和能量效率。

Abstract: We address the problem of automatic synchronisation of the spintronic
oscillator (STO) by means of reinforcement learning (RL). A numerical solution
of the macrospin Landau-Lifschitz-Gilbert-Slonczewski equation is used to
simulate the STO and we train the two types of RL agents to synchronise with a
target frequency within a fixed number of steps. We explore modifications to
this base task and show an improvement in both convergence and energy
efficiency of the synchronisation that can be easily achieved in the simulated
environment.

</details>


<div id='math.DS'></div>

# math.DS [[Back]](#toc)

### [180] [Data-driven approximation of transfer operators for mean-field stochastic differential equations](https://arxiv.org/abs/2509.09891)
*Eirini Ioannou,Stefan Klus,Gonçalo dos Reis*

Main category: math.DS

TL;DR: 本文将转移算子理论扩展到McKean - Vlasov方程，用扩展动态模式分解和Galerkin投影方法计算算子的有限维近似，以计算谱性质并识别模式或检测亚稳集，还通过实例说明结果。


<details>
  <summary>Details</summary>
Motivation: McKean - Vlasov方程在多领域有重要作用，通过分析相关转移算子的特征值和特征函数可获取复杂动力系统的全局信息，故将转移算子理论扩展到该方程。

Method: 将转移算子理论扩展到McKean - Vlasov方程，使用扩展动态模式分解和Galerkin投影方法计算这些算子的有限维近似。

Result: 能够计算谱性质，识别缓慢演化的时空模式或检测亚稳集，并通过Cormier模型、Kuramoto模型等实例说明结果。

Conclusion: 扩展转移算子理论到McKean - Vlasov方程是可行的，所采用的方法能有效计算谱性质并识别相关模式和检测亚稳集。

Abstract: Mean-field stochastic differential equations, also called McKean--Vlasov
equations, are the limiting equations of interacting particle systems with
fully symmetric interaction potential. Such systems play an important role in a
variety of fields ranging from biology and physics to sociology and economics.
Global information about the behavior of complex dynamical systems can be
obtained by analyzing the eigenvalues and eigenfunctions of associated transfer
operators such as the Perron--Frobenius operator and the Koopman operator. In
this paper, we extend transfer operator theory to McKean--Vlasov equations and
show how extended dynamic mode decomposition and the Galerkin projection
methodology can be used to compute finite-dimensional approximations of these
operators, which allows us to compute spectral properties and thus to identify
slowly evolving spatiotemporal patterns or to detect metastable sets. The
results will be illustrated with the aid of several guiding examples and
benchmark problems including the Cormier model, the Kuramoto model, and a
three-dimensional generalization of the Kuramoto model.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [181] [Vibe Check: Understanding the Effects of LLM-Based Conversational Agents' Personality and Alignment on User Perceptions in Goal-Oriented Tasks](https://arxiv.org/abs/2509.09870)
*Hasibur Rahman,Smit Desai*

Main category: cs.HC

TL;DR: 研究探索对话代理（CA）个性表达水平和用户 - 代理个性匹配对目标导向任务中用户感知的影响，发现中等表达效果最佳，个性匹配能提升结果。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型使对话代理能表达不同个性，研究个性表达水平和用户 - 代理个性匹配如何影响用户感知。

Method: 进行了一项有150名参与者的组间实验，通过特质调制键框架控制五大特质的低、中、高表达水平。

Result: 中等表达产生最积极评价，个性匹配能增强结果，外向性和情绪稳定性影响最大，聚类分析确定三种兼容性模式。

Conclusion: 个性表达和战略特质匹配是对话代理个性的最佳设计目标，对基于大语言模型的对话代理设计有启示。

Abstract: Large language models (LLMs) enable conversational agents (CAs) to express
distinctive personalities, raising new questions about how such designs shape
user perceptions. This study investigates how personality expression levels and
user-agent personality alignment influence perceptions in goal-oriented tasks.
In a between-subjects experiment (N=150), participants completed travel
planning with CAs exhibiting low, medium, or high expression across the Big
Five traits, controlled via our novel Trait Modulation Keys framework. Results
revealed an inverted-U relationship: medium expression produced the most
positive evaluations across Intelligence, Enjoyment, Anthropomorphism,
Intention to Adopt, Trust, and Likeability, significantly outperforming both
extremes. Personality alignment further enhanced outcomes, with Extraversion
and Emotional Stability emerging as the most influential traits. Cluster
analysis identified three distinct compatibility profiles, with "Well-Aligned"
users reporting substantially positive perceptions. These findings demonstrate
that personality expression and strategic trait alignment constitute optimal
design targets for CA personality, offering design implications as LLM-based
CAs become increasingly prevalent.

</details>


<div id='q-bio.OT'></div>

# q-bio.OT [[Back]](#toc)

### [182] [Standards in the Preparation of Biomedical Research Metadata: A Bridge2AI Perspective](https://arxiv.org/abs/2509.10432)
*Harry Caufield,Satrajit Ghosh,Sek Wong Kong,Jillian Parker,Nathan Sheffield,Bhavesh Patel,Andrew Williams,Timothy Clark,Monica C. Munoz-Torres*

Main category: q-bio.OT

TL;DR: 介绍AI-readiness概念，Bridge2AI的GCs项目需特定元数据确保AI-readiness，报告评估其元数据创建和标准化情况并给出建议。


<details>
  <summary>Details</summary>
Motivation: 确保Bridge2AI的GCs项目的数据具备AI-readiness，明确数据结构和关系。

Method: 评估Bridge2AI GCs项目中元数据的创建和标准化情况。

Result: 确定项目中元数据创建和标准化的现状，找出差距和可改进之处。

Conclusion: 报告成果对Bridge2AI内外新项目创建元数据以促进AI-readiness有借鉴意义。

Abstract: AI-readiness describes the degree to which data may be optimally and
ethically used for subsequent AI and Machine Learning (AI/ML) methods, where
those methods may involve some combination of model training, data
classification, and ethical, explainable prediction. The Bridge2AI consortium
has defined the particular criteria a biomedical dataset may possess to render
it AI-ready: in brief, a dataset's readiness is related to its FAIRness,
provenance, degree of characterization, explainability, sustainability, and
computability, in addition to its accompaniment with documentation about
ethical data practices.
  To ensure AI-readiness and to clarify data structure and relationships within
Bridge2AI's Grand Challenges (GCs), particular types of metadata are necessary.
The GCs within the Bridge2AI initiative include four data-generating projects
focusing on generating AI/ML-ready datasets to tackle complex biomedical and
behavioral research problems. These projects develop standardized, multimodal
data, tools, and training resources to support AI integration, while addressing
ethical data practices. Examples include using voice as a biomarker, building
interpretable genomic tools, modeling disease trajectories with diverse
multimodal data, and mapping cellular and molecular health indicators across
the human body.
  This report assesses the state of metadata creation and standardization in
the Bridge2AI GCs, provides guidelines where required, and identifies gaps and
areas for improvement across the program. New projects, including those outside
the Bridge2AI consortium, would benefit from what we have learned about
creating metadata as part of efforts to promote AI readiness.

</details>


<div id='hep-lat'></div>

# hep-lat [[Back]](#toc)

### [183] [Matrix-free Neural Preconditioner for the Dirac Operator in Lattice Gauge Theory](https://arxiv.org/abs/2509.10378)
*Yixuan Sun,Srinivas Eswar,Yin Lin,William Detmold,Phiala Shanahan,Xiaoye Li,Yang Liu,Prasanna Balaprakash*

Main category: hep-lat

TL;DR: 提出利用算子学习技术构建线性映射作为有效预条件子的框架，可减少线性系统条件数和迭代次数，有零样本学习能力。


<details>
  <summary>Details</summary>
Motivation: 格点量子色动力学中求解厄米正定系统时，迭代法耗时且计算成本高，现有预条件子构建有挑战且增加计算开销。

Method: 利用算子学习技术构建线性映射作为预条件子，不依赖显式矩阵进行模型训练和应用。

Result: 在特定模型中有效降低线性系统条件数，在相关参数范围内使收敛所需迭代次数约减半，且框架学习到依赖格点结构的通用映射，有零样本学习能力。

Conclusion: 所提框架可有效加速线性系统求解，具有一定泛化能力。

Abstract: Linear systems arise in generating samples and in calculating observables in
lattice quantum chromodynamics~(QCD). Solving the Hermitian positive definite
systems, which are sparse but ill-conditioned, involves using iterative
methods, such as Conjugate Gradient (CG), which are time-consuming and
computationally expensive. Preconditioners can effectively accelerate this
process, with the state-of-the-art being multigrid preconditioners. However,
constructing useful preconditioners can be challenging, adding additional
computational overhead, especially in large linear systems. We propose a
framework, leveraging operator learning techniques, to construct linear maps as
effective preconditioners. The method in this work does not rely on explicit
matrices from either the original linear systems or the produced
preconditioners, allowing efficient model training and application in the CG
solver. In the context of the Schwinger model U(1) gauge theory in 1+1
spacetime dimensions with two degenerate-mass fermions), this preconditioning
scheme effectively decreases the condition number of the linear systems and
approximately halves the number of iterations required for convergence in
relevant parameter ranges. We further demonstrate the framework learns a
general mapping dependent on the lattice structure which leads to zero-shot
learning ability for the Dirac operators constructed from gauge field
configurations of different sizes.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [184] [Computational Study of New Record-Based Transmuted Chen Distribution and Its Applications to the Failure Time and Iron Sheet Data](https://arxiv.org/abs/2509.09756)
*Caner Tanış*

Main category: stat.ME

TL;DR: 本文通过基于记录的变换方法引入新分布替代Chen分布，探索其性质、估计参数并进行模拟研究和实际数据验证。


<details>
  <summary>Details</summary>
Motivation: 引入新分布替代Chen分布。

Method: 采用基于前两个上记录值分布的记录变换方法，用九种估计方法估计参数，进行模拟研究和实际数据验证。

Result: 提出基于Chen分布的新分布，探索其多种分布性质，比较了九种估计方法的性能，用实际数据评估了模型拟合度。

Conclusion: 提出的新分布有一定优势，可作为Chen分布的替代，不同估计方法性能有差异。

Abstract: This study is considered to introduce a novel distribution as an alternative
to Chen distribution via the record-based transmutation method. This technique
is based on the distributions of first two upper record values. Thus, we
suggest a new special case based on Chen distribution in the family of record
based transmuted distributions. We explore various distributional properties of
the proposed model namely, quantile function, hazard function, median, moments,
and stochastic ordering. Our distribution has three parameters and to estimate
these parameters, we utilize nine different and well-known estimators. Then, we
compare the performances of these estimators via a comprehensive simulation
study. Also, we provide two real-world data examples to assess the fits the
data sets the suggested model and its some competitors.

</details>


### [185] [Record-based transmuted log-logistic distribution: Properties, simulation, and applications to petroleum rock and reactor pump data](https://arxiv.org/abs/2509.09757)
*Caner Tanış*

Main category: stat.ME

TL;DR: 本文引入基于记录的变换对数逻辑分布，探讨其数学性质、参数估计方法，通过蒙特卡罗模拟评估估计量性能，用实际数据对比拟合效果，得出该分布对两类数据拟合最佳。


<details>
  <summary>Details</summary>
Motivation: 向文献中引入新的寿命分布——基于记录的变换对数逻辑分布。

Method: 使用基于上记录值分布的记录变换映射得到新分布；探讨分布的数学性质；用七种方法进行点估计；进行蒙特卡罗模拟；用实际数据对比拟合效果。

Result: 蒙特卡罗模拟评估了估计量性能；数据对比表明基于记录的变换对数逻辑分布对反应堆泵故障和石油岩石数据集拟合最佳。

Conclusion: 基于记录的变换对数逻辑分布是反应堆泵故障和石油岩石数据集的最佳拟合分布。

Abstract: This study aims to introduce a new lifetime distribution, called the
record-based transformed log-logistic distribution, to the literature. We
obtain this distribution using a record-based transformation map based on the
distributions of upper record values. We explore some mathematical properties
of the suggested distribution, namely the quantile function, hazard function,
moments, order statistics, and stochastic ordering. We discuss the point
estimation via seven different methods such as maximum likelihood, least
squares, weighted least squares, Anderson-Darling, Cramer-von Mises, maximum
product spacings, and right tail Anderson Darling. Then, we perform a Monte
Carlo simulation study to evaluate the performances of these estimators. Also,
we present two practical data examples, reactor pump failure and petroleum rock
data to compare the fits of the proposed distribution with its rivals. As a
result of data analysis, we conclude that the best-fitted distribution is the
record-based transmuted log-logistic distribution for reactor pump failure and
petroleum rock data sets.

</details>


### [186] [Meta-Analysis with JASP, Part I: Classical Approaches](https://arxiv.org/abs/2509.09845)
*František Bartoš,Eric-Jan Wagenmakers,Wolfgang Viechtbauer*

Main category: stat.ME

TL;DR: 开发JASP中的Meta - Analysis模块以降低元分析方法使用门槛，介绍其工具及优势。


<details>
  <summary>Details</summary>
Motivation: 高级元分析方法的使用门槛高，仅专业编程人员可使用，需降低采用门槛。

Method: 开发JASP中的Meta - Analysis模块，通过易用的图形用户界面提供标准和高级元分析技术。

Result: 开发出具有易用图形界面的Meta - Analysis模块。

Conclusion: JASP支持严谨、相关和可重复的元分析实践。

Abstract: Meta-analyses play a crucial part in empirical science, enabling researchers
to synthesize evidence across studies and draw more precise and generalizable
conclusions. Despite their importance, access to advanced meta-analytic
methodology is often limited to scientists and students with considerable
expertise in computer programming. To lower the barrier for adoption, we have
developed the Meta-Analysis module in JASP (https://jasp-stats.org/), a free
and open-source software for statistical analyses. The module offers standard
and advanced meta-analytic techniques through an easy-to-use graphical user
interface (GUI), allowing researchers with diverse technical backgrounds to
conduct state-of-the-art analyses. This manuscript presents an overview of the
meta-analytic tools implemented in the module and showcases how JASP supports a
meta-analytic practice that is rigorous, relevant, and reproducible.

</details>


### [187] [Meta-Analysis with JASP, Part II: Bayesian Approaches](https://arxiv.org/abs/2509.09850)
*František Bartoš,Eric-Jan Wagenmakers*

Main category: stat.ME

TL;DR: 本文介绍在JASP统计软件中实现贝叶斯元分析工具，使不同技术背景研究者都能进行高级贝叶斯元分析。


<details>
  <summary>Details</summary>
Motivation: 高级贝叶斯元分析方法常限于有编程经验的研究者，为让更多人使用这些工具。

Method: 在JASP的元分析模块中实现最先进的贝叶斯元分析方法。

Result: 该模块可进行贝叶斯估计、假设检验和模型平均，结果可用森林图等解释。

Conclusion: JASP中的贝叶斯元分析工具能让不同技术背景研究者进行高级贝叶斯元分析。

Abstract: Bayesian inference is on the rise, partly because it allows researchers to
quantify parameter uncertainty, evaluate evidence for competing hypotheses,
incorporate model ambiguity, and seamlessly update knowledge as information
accumulates. All of these advantages apply to the meta-analytic settings;
however, advanced Bayesian meta-analytic methodology is often restricted to
researchers with programming experience. In order to make these tools available
to a wider audience, we implemented state-of-the-art Bayesian meta-analysis
methods in the Meta-Analysis module of JASP, a free and open-source statistical
software package (https://jasp-stats.org/). The module allows researchers to
conduct Bayesian estimation, hypothesis testing, and model averaging with
models such as meta-regression, multilevel meta-analysis, and publication bias
adjusted meta-analysis. Results can be interpreted using forest plots, bubble
plots, and estimated marginal means. This manuscript provides an overview of
the Bayesian meta-analysis tools available in JASP and demonstrates how the
software enables researchers of all technical backgrounds to perform advanced
Bayesian meta-analysis.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [188] [Enhancing understanding and clinical applications of cerebral autoregulation: A novel integrated numerical framework](https://arxiv.org/abs/2509.10295)
*Qi Zhang,Meng-di Yang,Xuan-hao Xu,Xiu-li Xu,Shuai Tian,Li-ling Hao*

Main category: physics.med-ph

TL;DR: 本文提出新算法结合已有模型研究脑自动调节（CA）对脑血流量（CBF）的调节作用，经验证框架可靠准确，有助于提升对CA的理解。


<details>
  <summary>Details</summary>
Motivation: 因脑血管结构复杂及关键参数量化困难，临床对CA的理解有限。

Method: 引入含偏微分和常微分方程的新数值算法，结合Windkessel模型调节CBF，并与先前的个性化0D - 1D多维模型集成。

Result: 集成框架用两个独立数据集验证，在多种生理条件下准确可靠地捕捉CA对CBF的调节作用。

Conclusion: 该工作显著提升对CA的理解，为开发基于血流动力学的治疗策略奠定基础。

Abstract: Cerebral autoregulation (CA) is a fundamental mechanism that modulates
cerebrovascular resistance, primarily by regulating the diameter of small
cerebral vessels to maintain stable cerebral blood flow (CBF) in response to
fluctuations in systemic arterial pressure. However, the clinical understanding
of CA remains limited due to the intricate structure of the cerebral
vasculature and the challenges in accurately quantifying the hemodynamic and
physiological parameters that govern this autoregulatory process. Method: In
this study, we introduced a novel numerical algorithm that employs three
partial differential equations and one ordinary differential equation to
capture both the spatial and temporal distributions of key CA-driving factors,
including the arterial pressure (P) and the partial pressures of oxygen (PO_2)
and carbon dioxide (PCO_2) within the cerebral vasculature, together with a
Windkessel model in turn to regulate the CBF based on the calculated P, PO_2,
and PCO_2. This algorithm was sequentially integrated with our previously
developed personalized 0D-1D multi-dimensional model to account for the
patient-specific effects. Results: The integrated framework was rigorously
validated using two independent datasets, demonstrating its high reliability
and accuracy in capturing the regulatory effects of CA on CBF across a range of
physiological conditions. Conclusion: This work significantly advances our
understanding of CA and provides a promising foundation for developing
hemodynamic-based therapeutic strategies aimed at improving clinical outcomes
in patients with cerebrovascular disorders.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [189] [Tackling One Health Risks: How Large Language Models are leveraged for Risk Negotiation and Consensus-building](https://arxiv.org/abs/2509.09906)
*Alexandra Fetsch,Iurii Savvateev,Racem Ben Romdhane,Martin Wiedmann,Artemiy Dimov,Maciej Durkalec,Josef Teichmann,Jakob Zinsstag,Konstantinos Koutsoumanis,Andreja Rajkovic,Jason Mann,Mauro Tonolla,Monika Ehling-Schulz,Matthias Filter,Sophia Johler*

Main category: cs.MA

TL;DR: 本文提出AI辅助谈判框架应对跨部门协作挑战，在两个场景验证其潜力，设计适合资源有限用户。


<details>
  <summary>Details</summary>
Motivation: 传统风险分析框架有局限，实现跨部门利益平衡面临时间、信息和视角整合难题，缺乏跨部门协作工具。

Method: 提出结合大语言模型和AI自主代理的以谈判为中心的风险分析工作流框架，利用大语言模型语义分析能力。

Result: 在两个真实场景进行概念验证实现。

Conclusion: AI辅助谈判有潜力解决跨部门协作工具缺乏问题，开源、基于网络的设计适合更多资源有限用户。

Abstract: Key global challenges of our times are characterized by complex
interdependencies and can only be effectively addressed through an integrated,
participatory effort. Conventional risk analysis frameworks often reduce
complexity to ensure manageability, creating silos that hinder comprehensive
solutions. A fundamental shift towards holistic strategies is essential to
enable effective negotiations between different sectors and to balance the
competing interests of stakeholders. However, achieving this balance is often
hindered by limited time, vast amounts of information, and the complexity of
integrating diverse perspectives. This study presents an AI-assisted
negotiation framework that incorporates large language models (LLMs) and
AI-based autonomous agents into a negotiation-centered risk analysis workflow.
The framework enables stakeholders to simulate negotiations, systematically
model dynamics, anticipate compromises, and evaluate solution impacts. By
leveraging LLMs' semantic analysis capabilities we could mitigate information
overload and augment decision-making process under time constraints.
Proof-of-concept implementations were conducted in two real-world scenarios:
(i) prudent use of a biopesticide, and (ii) targeted wild animal population
control. Our work demonstrates the potential of AI-assisted negotiation to
address the current lack of tools for cross-sectoral engagement. Importantly,
the solution's open source, web based design, suits for application by a
broader audience with limited resources and enables users to tailor and develop
it for their own needs.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [190] [Unified Learnable 2D Convolutional Feature Extraction for ASR](https://arxiv.org/abs/2509.10031)
*Peter Vieting,Benedikt Hilmes,Ralf Schlüter,Hermann Ney*

Main category: eess.AS

TL;DR: 本文旨在开发通用语音特征提取前端，实验表明该通用统一方法可行且性能与现有监督可学习特征提取器相当。


<details>
  <summary>Details</summary>
Motivation: 现有神经前端技术受经典方法影响大，作者旨在开发更通用的前端并统一架构。

Method: 系统地减少现有技术的影响以实现通用前端。

Result: 得到参数高效、适用于计算资源有限场景的2D卷积前端，性能与现有监督可学习特征提取器相当。

Conclusion: 通用统一的特征提取前端方法不仅可行，还能达到与现有方法相当的性能。

Abstract: Neural front-ends represent a promising approach to feature extraction for
automatic speech recognition (ASR) systems as they enable to learn specifically
tailored features for different tasks. Yet, many of the existing techniques
remain heavily influenced by classical methods. While this inductive bias may
ease the system design, our work aims to develop a more generic front-end for
feature extraction. Furthermore, we seek to unify the front-end architecture
contrasting with existing approaches that apply a composition of several layer
topologies originating from different sources. The experiments systematically
show how to reduce the influence of existing techniques to achieve a generic
front-end. The resulting 2D convolutional front-end is parameter-efficient and
suitable for a scenario with limited computational resources unlike large
models pre-trained on unlabeled audio. The results demonstrate that this
generic unified approach is not only feasible but also matches the performance
of existing supervised learnable feature extractors.

</details>


### [191] [Error Analysis in a Modular Meeting Transcription System](https://arxiv.org/abs/2509.10143)
*Peter Vieting,Simon Berger,Thilo von Neumann,Christoph Boeddeker,Ralf Schlüter,Reinhold Haeb-Umbach*

Main category: eess.AS

TL;DR: 本文扩展语音分离泄漏分析框架，研究泄漏对性能的影响，比较不同分割方法，在特定数据集达当前最优。


<details>
  <summary>Details</summary>
Motivation: 会议转录虽有进展但仍有挑战限制其性能，需进一步研究。

Method: 扩展先前提出的语音分离泄漏分析框架，比较不同分割方法。

Result: 主说话人活跃区域存在显著跨通道泄漏，但对最终性能影响不大；先进的说话人分割方法比简单基于能量的VAD能将与理想分割的差距缩小三分之一；在仅用LibriSpeech数据训练识别模块的系统中，在LibriCSS上达到当前最优。

Conclusion: 揭示了影响性能的因素，展示了先进分割方法的优势。

Abstract: Meeting transcription is a field of high relevance and remarkable progress in
recent years. Still, challenges remain that limit its performance. In this
work, we extend a previously proposed framework for analyzing leakage in speech
separation with proper sensitivity to temporal locality. We show that there is
significant leakage to the cross channel in areas where only the primary
speaker is active. At the same time, the results demonstrate that this does not
affect the final performance much as these leaked parts are largely ignored by
the voice activity detection (VAD). Furthermore, different segmentations are
compared showing that advanced diarization approaches are able to reduce the
gap to oracle segmentation by a third compared to a simple energy-based VAD. We
additionally reveal what factors contribute to the remaining difference. The
results represent state-of-the-art performance on LibriCSS among systems that
train the recognition module on LibriSpeech data only.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [192] [HypoGeneAgent: A Hypothesis Language Agent for Gene-Set Cluster Resolution Selection Using Perturb-seq Datasets](https://arxiv.org/abs/2509.09740)
*Ying Yuan,Xing-Yue Monica Ge,Aaron Archer Waterman,Tommaso Biancalani,David Richmond,Yogesh Pandit,Avtar Singh,Russell Littman,Jin Liu,Jan-Christian Huetter,Vladimir Ermakov*

Main category: q-bio.QM

TL;DR: 提出HYPOGENEAGENT框架，将细胞聚类注释转化为可量化优化任务，用LLM生成GO假设并计算分辨率得分，初步测试效果良好，为单细胞多组学研究自动化解释奠定基础。


<details>
  <summary>Details</summary>
Motivation: 现有细胞聚类分辨率选择和功能注释具有主观性，依赖启发式方法和专家策展，需要客观方法。

Method: 使用LLM分析基因程序或扰动模块生成GO假设及置信分数，用句子嵌入模型嵌入预测描述，计算余弦相似度得出内部一致性和外部差异性，结合得到分辨率得分。

Result: 在K562 CRISPRi Perturb - seq数据集上，分辨率得分选择的聚类粒度与已知通路更匹配，优于经典指标。

Conclusion: LLM代理可作为聚类分辨率和功能注释的客观评判者，为单细胞多组学研究全自动、上下文感知解释管道铺平道路。

Abstract: Large-scale single-cell and Perturb-seq investigations routinely involve
clustering cells and subsequently annotating each cluster with Gene-Ontology
(GO) terms to elucidate the underlying biological programs. However, both
stages, resolution selection and functional annotation, are inherently
subjective, relying on heuristics and expert curation. We present
HYPOGENEAGENT, a large language model (LLM)-driven framework, transforming
cluster annotation into a quantitatively optimizable task. Initially, an LLM
functioning as a gene-set analyst analyzes the content of each gene program or
perturbation module and generates a ranked list of GO-based hypotheses,
accompanied by calibrated confidence scores. Subsequently, we embed every
predicted description with a sentence-embedding model, compute pair-wise cosine
similarities, and let the agent referee panel score (i) the internal
consistency of the predictions, high average similarity within the same
cluster, termed intra-cluster agreement (ii) their external distinctiveness,
low similarity between clusters, termed inter-cluster separation. These two
quantities are combined to produce an agent-derived resolution score, which is
maximized when clusters exhibit simultaneous coherence and mutual exclusivity.
When applied to a public K562 CRISPRi Perturb-seq dataset as a preliminary
test, our Resolution Score selects clustering granularities that exhibit
alignment with known pathway compared to classical metrics such silhouette
score, modularity score for gene functional enrichment summary. These findings
establish LLM agents as objective adjudicators of cluster resolution and
functional annotation, thereby paving the way for fully automated,
context-aware interpretation pipelines in single-cell multi-omics studies.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [193] [SmartCoder-R1: Towards Secure and Explainable Smart Contract Generation with Security-Aware Group Relative Policy Optimization](https://arxiv.org/abs/2509.09942)
*Lei Yu,Jingyuan Zhang,Xin Wang,Jiajia Ma,Li Yang,Fengjun Zhang*

Main category: cs.CR

TL;DR: 提出基于Qwen2.5 - Coder - 7B的SmartCoder - R1框架用于安全且可解释的智能合约生成，经评估表现优异。


<details>
  <summary>Details</summary>
Motivation: 智能合约漏洞会造成重大财务损失，大语言模型在生成代码时存在不透明和易产生安全漏洞的问题。

Method: 先进行持续预训练（CPT），再用7998个样本进行长思维链监督微调（L - CoT SFT），最后使用安全感知组相对策略优化（S - GRPO）。

Result: 在756个真实世界函数的基准测试中，SmartCoder - R1在五项关键指标上达到最优，生成的推理在人工评估中也表现出色。

Conclusion: SmartCoder - R1框架在智能合约生成方面建立了新的技术水平，能有效解决现有问题。

Abstract: Smart contracts automate the management of high-value assets, where
vulnerabilities can lead to catastrophic financial losses. This challenge is
amplified in Large Language Models (LLMs) by two interconnected failures: they
operate as unauditable "black boxes" lacking a transparent reasoning process,
and consequently, generate code riddled with critical security vulnerabilities.
To address both issues, we propose SmartCoder-R1 (based on Qwen2.5-Coder-7B), a
novel framework for secure and explainable smart contract generation. It begins
with Continual Pre-training (CPT) to specialize the model. We then apply Long
Chain-of-Thought Supervised Fine-Tuning (L-CoT SFT) on 7,998 expert-validated
reasoning-and-code samples to train the model to emulate human security
analysis. Finally, to directly mitigate vulnerabilities, we employ
Security-Aware Group Relative Policy Optimization (S-GRPO), a reinforcement
learning phase that refines the generation policy by optimizing a weighted
reward signal for compilation success, security compliance, and format
correctness. Evaluated against 17 baselines on a benchmark of 756 real-world
functions, SmartCoder-R1 establishes a new state of the art, achieving top
performance across five key metrics: a ComPass of 87.70%, a VulRate of 8.60%, a
SafeAval of 80.16%, a FuncRate of 53.84%, and a FullRate of 50.53%. This
FullRate marks a 45.79% relative improvement over the strongest baseline,
DeepSeek-R1. Crucially, its generated reasoning also excels in human
evaluations, achieving high-quality ratings for Functionality (82.7%), Security
(85.3%), and Clarity (90.7%).

</details>


### [194] [Automated Testing of Broken Authentication Vulnerabilities in Web APIs with AuthREST](https://arxiv.org/abs/2509.10320)
*Davide Corradini,Mariano Ceccato,Mohammad Ghafari*

Main category: cs.CR

TL;DR: 介绍开源安全测试工具AuthREST，可测试API认证漏洞，实证显示有效并发现4个公共API未知漏洞。


<details>
  <summary>Details</summary>
Motivation: 解决野外最普遍的API安全风险之一——认证破坏问题。

Method: AuthREST自动对Web API进行凭证填充、密码暴力破解和未检查令牌真实性的测试。

Result: AuthREST有效提升Web API安全性，发现4个公共API的未知认证漏洞。

Conclusion: AuthREST能有效改善Web API安全状况。

Abstract: We present AuthREST, an open-source security testing tool targeting broken
authentication, one of the most prevalent API security risks in the wild.
AuthREST automatically tests web APIs for credential stuffing, password brute
forcing, and unchecked token authenticity. Empirical results show that AuthREST
is effective in improving web API security. Notably, it uncovered previously
unknown authentication vulnerabilitiesin in four public APIs.

</details>


### [195] [Bitcoin Cross-Chain Bridge: A Taxonomy and Its Promise in Artificial Intelligence of Things](https://arxiv.org/abs/2509.10413)
*Guojun Tang,Carylyne Chan,Ning Nan,Spencer Yang,Jiayu Zhou,Henry Leung,Mohammad Mamun,Steve Drew*

Main category: cs.CR

TL;DR: 本文提出比特币跨链桥协议分类法，分析其特性与适用性，强调新兴创新潜力，探索在AIoT的用例，为设计跨链基础设施提供框架。


<details>
  <summary>Details</summary>
Motivation: 比特币脚本能力有限和缺乏互操作性机制，限制其融入区块链生态，特别是DeFi和多链应用。

Method: 提出比特币跨链桥协议的分类法，分为天真代币交换、挂钩资产桥和任意消息桥三类，并从信任模型、延迟等关键指标评估。

Result: 强调BitVM和递归侧链等新兴创新有潜力实现安全、可扩展和可编程的比特币互操作性，探索了跨链桥在AIoT的实际用例。

Conclusion: 该分类法为AIoT系统中设计安全高效的跨链基础设施提供基础框架。

Abstract: Bitcoin's limited scripting capabilities and lack of native interoperability
mechanisms have constrained its integration into the broader blockchain
ecosystem, especially decentralized finance (DeFi) and multi-chain
applications. This paper presents a comprehensive taxonomy of Bitcoin
cross-chain bridge protocols, systematically analyzing their trust assumptions,
performance characteristics, and applicability to the Artificial Intelligence
of Things (AIoT) scenarios. We categorize bridge designs into three main types:
naive token swapping, pegged-asset bridges, and arbitrary-message bridges. Each
category is evaluated across key metrics such as trust model, latency, capital
efficiency, and DeFi composability. Emerging innovations like BitVM and
recursive sidechains are highlighted for their potential to enable secure,
scalable, and programmable Bitcoin interoperability. Furthermore, we explore
practical use cases of cross-chain bridges in AIoT applications, including
decentralized energy trading, healthcare data integration, and supply chain
automation. This taxonomy provides a foundational framework for researchers and
practitioners seeking to design secure and efficient cross-chain
infrastructures in AIoT systems.

</details>


### [196] [Differential Robustness in Transformer Language Models: Empirical Evaluation Under Adversarial Text Attacks](https://arxiv.org/abs/2509.09706)
*Taniya Gidatkar,Oluwaseun Ajao,Matthew Shardlow*

Main category: cs.CR

TL;DR: 研究评估Flan - T5、BERT和RoBERTa - Base等大语言模型对抗攻击的弹性，发现不同模型鲁棒性差异大，部分模型需大量计算资源，还给出防御策略建议。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型对抗攻击的弹性，了解其安全状况。

Method: 使用TextFooler和BERTAttack进行系统设计的对抗测试。

Result: RoBERTa - Base和FlanT5攻击成功率为0%，表现出强韧性；BERT - Base很脆弱，TextFooler使模型准确率从48%降至3%，成功率93.75%。

Conclusion: 部分大语言模型有有效防御机制但需大量计算资源，研究有助于了解LLM安全，提出开发更高效防御策略的建议。

Abstract: This study evaluates the resilience of large language models (LLMs) against
adversarial attacks, specifically focusing on Flan-T5, BERT, and RoBERTa-Base.
Using systematically designed adversarial tests through TextFooler and
BERTAttack, we found significant variations in model robustness. RoBERTa-Base
and FlanT5 demonstrated remarkable resilience, maintaining accuracy even when
subjected to sophisticated attacks, with attack success rates of 0%. In
contrast. BERT-Base showed considerable vulnerability, with TextFooler
achieving a 93.75% success rate in reducing model accuracy from 48% to just 3%.
Our research reveals that while certain LLMs have developed effective defensive
mechanisms, these safeguards often require substantial computational resources.
This study contributes to the understanding of LLM security by identifying
existing strengths and weaknesses in current safeguarding approaches and
proposes practical recommendations for developing more efficient and effective
defensive strategies.

</details>


### [197] [ZORRO: Zero-Knowledge Robustness and Privacy for Split Learning (Full Version)](https://arxiv.org/abs/2509.09787)
*Nojan Sheybani,Alessandro Pegoraro,Jonathan Knauer,Phillip Rieger,Elissa Mollakuqe,Farinaz Koushanfar,Ahmad-Reza Sadeghi*

Main category: cs.CR

TL;DR: 文章指出Split Learning存在客户端攻击问题，提出ZORRO防御方案，评估显示其有效降低攻击成功率且开销小。


<details>
  <summary>Details</summary>
Motivation: 现有Split Learning防御方法有局限，客户端防御难以强制恶意客户端执行防御算法，需新方案。

Method: 运用交互式零知识证明让客户端证明正确执行防御算法，利用模型分区的频率表示检查本地训练模型。

Result: 在不同模型架构、攻击策略和数据场景评估中，ZORRO将攻击成功率降至低于6%，客户端存储百万参数时开销小于10秒。

Conclusion: ZORRO是有效、私密、可验证且健壮的Split Learning防御方案。

Abstract: Split Learning (SL) is a distributed learning approach that enables
resource-constrained clients to collaboratively train deep neural networks
(DNNs) by offloading most layers to a central server while keeping in- and
output layers on the client-side. This setup enables SL to leverage server
computation capacities without sharing data, making it highly effective in
resource-constrained environments dealing with sensitive data. However, the
distributed nature enables malicious clients to manipulate the training
process. By sending poisoned intermediate gradients, they can inject backdoors
into the shared DNN. Existing defenses are limited by often focusing on
server-side protection and introducing additional overhead for the server. A
significant challenge for client-side defenses is enforcing malicious clients
to correctly execute the defense algorithm.
  We present ZORRO, a private, verifiable, and robust SL defense scheme.
Through our novel design and application of interactive zero-knowledge proofs
(ZKPs), clients prove their correct execution of a client-located defense
algorithm, resulting in proofs of computational integrity attesting to the
benign nature of locally trained DNN portions. Leveraging the frequency
representation of model partitions enables ZORRO to conduct an in-depth
inspection of the locally trained models in an untrusted environment, ensuring
that each client forwards a benign checkpoint to its succeeding client. In our
extensive evaluation, covering different model architectures as well as various
attack strategies and data scenarios, we show ZORRO's effectiveness, as it
reduces the attack success rate to less than 6\% while causing even for models
storing \numprint{1000000} parameters on the client-side an overhead of less
than 10 seconds.

</details>


### [198] [Securing LLM-Generated Embedded Firmware through AI Agent-Driven Validation and Patching](https://arxiv.org/abs/2509.09970)
*Seyed Moein Abtahi,Akramul Azim*

Main category: cs.CR

TL;DR: 论文提出三相方法结合大语言模型生成固件与安全验证及迭代优化，实验显示提升了固件安全和性能并贡献数据集。


<details>
  <summary>Details</summary>
Motivation: 大语言模型生成嵌入式系统固件存在安全漏洞和不满足实时性能约束的问题。

Method: 提出三相方法，用结构化提示让GPT - 4生成固件，用模糊测试、静态分析等检测漏洞，AI代理协作改进检测和修复，迭代生成补丁。

Result: 漏洞修复率达92.4%（提升37.3%），威胁模型合规率95.8%，安全覆盖指数0.87，实时指标表现良好。

Conclusion: 该过程提升了固件安全和性能，还贡献了开源数据集用于未来研究。

Abstract: Large Language Models (LLMs) show promise in generating firmware for embedded
systems, but often introduce security flaws and fail to meet real-time
performance constraints. This paper proposes a three-phase methodology that
combines LLM-based firmware generation with automated security validation and
iterative refinement in a virtualized environment. Using structured prompts,
models like GPT-4 generate firmware for networking and control tasks, deployed
on FreeRTOS via QEMU. These implementations are tested using fuzzing, static
analysis, and runtime monitoring to detect vulnerabilities such as buffer
overflows (CWE-120), race conditions (CWE-362), and denial-of-service threats
(CWE-400). Specialized AI agents for Threat Detection, Performance
Optimization, and Compliance Verification collaborate to improve detection and
remediation. Identified issues are categorized using CWE, then used to prompt
targeted LLM-generated patches in an iterative loop. Experiments show a 92.4\%
Vulnerability Remediation Rate (37.3\% improvement), 95.8\% Threat Model
Compliance, and 0.87 Security Coverage Index. Real-time metrics include 8.6ms
worst-case execution time and 195{\mu}s jitter. This process enhances firmware
security and performance while contributing an open-source dataset for future
research.

</details>


### [199] [Investigating Feature Attribution for 5G Network Intrusion Detection](https://arxiv.org/abs/2509.10206)
*Federica Uccello,Simin Nadjm-Tehrani*

Main category: cs.CR

TL;DR: 文章探讨5G网络中机器学习模型安全警报解释方法，对比SHAP和VoTE - XAI，发现二者所选特征有差异，VoTE - XAI更高效。


<details>
  <summary>Details</summary>
Motivation: 随着5G网络在关键应用中兴起，需从恶意活动检测转向能提供可靠判定的系统，理解和解释机器学习模型安全警报对事件响应编排至关重要，要探究归因方法对未来通信系统的相关性。

Method: 研究SHAP和VoTE - XAI两种方法，分析它们对XGBoost模型在三种5G通信攻击用例中生成警报的解释，确定稀疏性、稳定性和效率三个评估指标。

Result: 在5G网络中，VoTE - XAI和SHAP所选重要特征数量不同且有差异，但VoTE - XAI未遗漏SHAP的顶级特征；VoTE - XAI在提供解释效率上显著更高。

Conclusion: 在5G网络安全警报解释方面，VoTE - XAI在特征选择和效率上有优势。

Abstract: With the rise of fifth-generation (5G) networks in critical applications, it
is urgent to move from detection of malicious activity to systems capable of
providing a reliable verdict suitable for mitigation. In this regard,
understanding and interpreting machine learning (ML) models' security alerts is
crucial for enabling actionable incident response orchestration. Explainable
Artificial Intelligence (XAI) techniques are expected to enhance trust by
providing insights into why alerts are raised. A dominant approach
statistically associates feature sets that can be correlated to a given alert.
This paper starts by questioning whether such attribution is relevant for
future generation communication systems, and investigates its merits in
comparison with an approach based on logical explanations. We extensively study
two methods, SHAP and VoTE-XAI, by analyzing their interpretations of alerts
generated by an XGBoost model in three different use cases with several 5G
communication attacks. We identify three metrics for assessing explanations:
sparsity, how concise they are; stability, how consistent they are across
samples from the same attack type; and efficiency, how fast an explanation is
generated. As an example, in a 5G network with 92 features, 6 were deemed
important by VoTE-XAI for a Denial of Service (DoS) variant, ICMPFlood, while
SHAP identified over 20. More importantly, we found a significant divergence
between features selected by SHAP and VoTE-XAI. However, none of the top-ranked
features selected by SHAP were missed by VoTE-XAI. When it comes to efficiency
of providing interpretations, we found that VoTE-XAI is significantly more
responsive, e.g. it provides a single explanation in under 0.002 seconds, in a
high-dimensional setting (478 features).

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [200] [A Role-Aware Multi-Agent Framework for Financial Education Question Answering with LLMs](https://arxiv.org/abs/2509.09727)
*Andy Zhu,Yingjun Du*

Main category: cs.CL

TL;DR: 现有大语言模型在金融问答任务中表现不佳，本文提出多智能体框架，通过角色提示提升性能，实验表明该方法有效且成本效益高。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型方法难以满足金融问答所需的细致和专业推理，金融领域需要多步定量推理、熟悉特定术语和理解现实场景。

Method: 提出包含基础生成器、证据检索器和专家评审器的多智能体框架，利用检索增强生成获取上下文证据，采用提示策略让专家评审。

Result: 基于批判的优化使答案准确率比零样本思维链基线提高6.6 - 8.3%，Gemini - 2.0 - Flash性能最佳，使GPT - 4o - mini达到与FinGPT - mt_Llama3 - 8B_LoRA相当的性能。

Conclusion: 该方法是一种经济有效的提升金融问答性能的途径，为多智能体金融大语言模型系统的进一步研究提供了见解。

Abstract: Question answering (QA) plays a central role in financial education, yet
existing large language model (LLM) approaches often fail to capture the
nuanced and specialized reasoning required for financial problem-solving. The
financial domain demands multistep quantitative reasoning, familiarity with
domain-specific terminology, and comprehension of real-world scenarios. We
present a multi-agent framework that leverages role-based prompting to enhance
performance on domain-specific QA. Our framework comprises a Base Generator, an
Evidence Retriever, and an Expert Reviewer agent that work in a single-pass
iteration to produce a refined answer. We evaluated our framework on a set of
3,532 expert-designed finance education questions from Study.com, an online
learning platform. We leverage retrieval-augmented generation (RAG) for
contextual evidence from 6 finance textbooks and prompting strategies for a
domain-expert reviewer. Our experiments indicate that critique-based refinement
improves answer accuracy by 6.6-8.3% over zero-shot Chain-of-Thought baselines,
with the highest performance from Gemini-2.0-Flash. Furthermore, our method
enables GPT-4o-mini to achieve performance comparable to the finance-tuned
FinGPT-mt_Llama3-8B_LoRA. Our results show a cost-effective approach to
enhancing financial QA and offer insights for further research in multi-agent
financial LLM systems.

</details>


### [201] [Towards Reliable and Interpretable Document Question Answering via VLMs](https://arxiv.org/abs/2509.10129)
*Alessio Chen,Simone Giovannini,Andrea Gemelli,Fabio Coppini,Simone Marinai*

Main category: cs.CL

TL;DR: 提出DocExplainerV0模块解决VLM文档答案定位难题并评估定位差距，设基准。


<details>
  <summary>Details</summary>
Motivation: 现有VLM在文档中准确定位答案有挑战，限制可解释性和实际应用。

Method: 引入DocExplainerV0插件式边界框预测模块，解耦答案生成与空间定位，使其适用于现有VLM。

Result: 通过评估得出文本准确性和空间定位存在差距，正确答案常缺乏可靠定位。

Conclusion: 标准化框架凸显问题，为更具可解释性和鲁棒性的文档信息提取VLM研究设基准。

Abstract: Vision-Language Models (VLMs) have shown strong capabilities in document
understanding, particularly in identifying and extracting textual information
from complex documents. Despite this, accurately localizing answers within
documents remains a major challenge, limiting both interpretability and
real-world applicability. To address this, we introduce
\textit{DocExplainerV0}, a plug-and-play bounding-box prediction module that
decouples answer generation from spatial localization. This design makes it
applicable to existing VLMs, including proprietary systems where fine-tuning is
not feasible. Through systematic evaluation, we provide quantitative insights
into the gap between textual accuracy and spatial grounding, showing that
correct answers often lack reliable localization. Our standardized framework
highlights these shortcomings and establishes a benchmark for future research
toward more interpretable and robust document information extraction VLMs.

</details>


### [202] [Structured Information Matters: Explainable ICD Coding with Patient-Level Knowledge Graphs](https://arxiv.org/abs/2509.09699)
*Mingyang Li,Viktor Schlegel,Tingting Mu,Warren Del-Pinto,Goran Nenadic*

Main category: cs.CL

TL;DR: 本文利用文档级知识图谱计算输入文档的结构化表示，用于自动ICD - 9编码，提升了编码效果和训练效率，且增强了解释性。


<details>
  <summary>Details</summary>
Motivation: 临床文档映射到标准化词汇手动编码困难且耗时，自动化编码虽有潜力但因映射到高维长尾目标空间而困难，且利用外部资源表示输入文档的研究不足。

Method: 利用文档级知识图谱计算输入文档的结构化表示，并将其集成到PLM - ICD架构中进行自动ICD - 9编码。

Result: 在流行基准上Macro - F1分数最多提高3.20%，提高了训练效率。

Conclusion: 文档级知识图谱能有效表示以患者为中心的输入文档，不同实体和关系提升了编码效果和方法的可解释性。

Abstract: Mapping clinical documents to standardised clinical vocabularies is an
important task, as it provides structured data for information retrieval and
analysis, which is essential to clinical research, hospital administration and
improving patient care. However, manual coding is both difficult and
time-consuming, making it impractical at scale. Automated coding can
potentially alleviate this burden, improving the availability and accuracy of
structured clinical data. The task is difficult to automate, as it requires
mapping to high-dimensional and long-tailed target spaces, such as the
International Classification of Diseases (ICD). While external knowledge
sources have been readily utilised to enhance output code representation, the
use of external resources for representing the input documents has been
underexplored. In this work, we compute a structured representation of the
input documents, making use of document-level knowledge graphs (KGs) that
provide a comprehensive structured view of a patient's condition. The resulting
knowledge graph efficiently represents the patient-centred input documents with
23\% of the original text while retaining 90\% of the information. We assess
the effectiveness of this graph for automated ICD-9 coding by integrating it
into the state-of-the-art ICD coding architecture PLM-ICD. Our experiments
yield improved Macro-F1 scores by up to 3.20\% on popular benchmarks, while
improving training efficiency. We attribute this improvement to different types
of entities and relationships in the KG, and demonstrate the improved
explainability potential of the approach over the text-only baseline.

</details>


### [203] [Cross-Layer Attention Probing for Fine-Grained Hallucination Detection](https://arxiv.org/abs/2509.09700)
*Malavika Suresh,Rahaf Aljundi,Ikechukwu Nkisi-Orji,Nirmalie Wiratunga*

Main category: cs.CL

TL;DR: 提出用于幻觉检测的CLAP技术，经实验评估能提升检测效果，可采用检测 - 缓解策略提升大语言模型可靠性，且分布外应用也有高可靠性。


<details>
  <summary>Details</summary>
Motivation: 大规模应用大语言模型时，其生成不准确文本（幻觉）引发可靠性担忧。

Method: 提出Cross - Layer Attention Probing (CLAP)，将大语言模型整个残差流的激活作为联合序列处理。

Result: 使用五个大语言模型和三个任务的实验表明，CLAP在贪婪解码响应和高温采样响应上均比基线方法提升了幻觉检测效果，能进行细粒度检测。

Conclusion: 可采用基于CLAP的检测 - 缓解策略提升大语言模型可靠性，且CLAP在分布外应用时也保持高可靠性。

Abstract: With the large-scale adoption of Large Language Models (LLMs) in various
applications, there is a growing reliability concern due to their tendency to
generate inaccurate text, i.e. hallucinations. In this work, we propose
Cross-Layer Attention Probing (CLAP), a novel activation probing technique for
hallucination detection, which processes the LLM activations across the entire
residual stream as a joint sequence. Our empirical evaluations using five LLMs
and three tasks show that CLAP improves hallucination detection compared to
baselines on both greedy decoded responses as well as responses sampled at
higher temperatures, thus enabling fine-grained detection, i.e. the ability to
disambiguate hallucinations and non-hallucinations among different sampled
responses to a given prompt. This allows us to propose a detect-then-mitigate
strategy using CLAP to reduce hallucinations and improve LLM reliability
compared to direct mitigation approaches. Finally, we show that CLAP maintains
high reliability even when applied out-of-distribution.

</details>


### [204] [Creativity Benchmark: A benchmark for marketing creativity for LLM models](https://arxiv.org/abs/2509.09702)
*Ninad Bhat,Kieran Browne,Pip Bingemann*

Main category: cs.CL

TL;DR: 介绍营销创意领域大语言模型评估框架Creativity Benchmark，结果显示无模型占主导，自动评判不能替代人工评估，传统测试部分适用，需专家评估和多样性工作流。


<details>
  <summary>Details</summary>
Motivation: 提出大语言模型在营销创意方面的评估框架。

Method: 使用100个品牌和3种提示类型，让678名创意从业者进行11012次匿名比较，用Bradley - Terry模型分析；用余弦距离分析模型多样性；对比三种大语言模型评判设置与人类排名。

Result: 无模型在各品牌或提示类型中占主导，自动评判与人类排名相关性弱且有偏差，传统创造力测试部分适用于品牌受限任务。

Conclusion: 强调需要专家人工评估和考虑多样性的工作流。

Abstract: We introduce Creativity Benchmark, an evaluation framework for large language
models (LLMs) in marketing creativity. The benchmark covers 100 brands (12
categories) and three prompt types (Insights, Ideas, Wild Ideas). Human
pairwise preferences from 678 practising creatives over 11,012 anonymised
comparisons, analysed with Bradley-Terry models, show tightly clustered
performance with no model dominating across brands or prompt types: the
top-bottom spread is $\Delta\theta \approx 0.45$, which implies a head-to-head
win probability of $0.61$; the highest-rated model beats the lowest only about
$61\%$ of the time. We also analyse model diversity using cosine distances to
capture intra- and inter-model variation and sensitivity to prompt reframing.
Comparing three LLM-as-judge setups with human rankings reveals weak,
inconsistent correlations and judge-specific biases, underscoring that
automated judges cannot substitute for human evaluation. Conventional
creativity tests also transfer only partially to brand-constrained tasks.
Overall, the results highlight the need for expert human evaluation and
diversity-aware workflows.

</details>


### [205] [CTCC: A Robust and Stealthy Fingerprinting Framework for Large Language Models via Cross-Turn Contextual Correlation Backdoor](https://arxiv.org/abs/2509.09703)
*Zhenhua Xu,Xixiang Zhao,Xubin Yue,Shengwei Tian,Changting Lin,Meng Han*

Main category: cs.CL

TL;DR: 提出CTCC框架用于大语言模型指纹识别，比现有方法有更好的隐蔽性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型广泛部署引发知识产权保护担忧，现有指纹方法存在隐蔽性、鲁棒性和通用性权衡问题。

Method: 引入CTCC，一种基于规则的指纹框架，编码多轮对话上下文关联。

Result: 在多个大语言模型架构上实验表明，CTCC比先前工作有更强隐蔽性和鲁棒性。

Conclusion: CTCC是现实世界大语言模型部署场景中所有权验证的可靠实用解决方案。

Abstract: The widespread deployment of large language models (LLMs) has intensified
concerns around intellectual property (IP) protection, as model theft and
unauthorized redistribution become increasingly feasible. To address this,
model fingerprinting aims to embed verifiable ownership traces into LLMs.
However, existing methods face inherent trade-offs between stealthness,
robustness, and generalizability, being either detectable via distributional
shifts, vulnerable to adversarial modifications, or easily invalidated once the
fingerprint is revealed. In this work, we introduce CTCC, a novel rule-driven
fingerprinting framework that encodes contextual correlations across multiple
dialogue turns, such as counterfactual, rather than relying on token-level or
single-turn triggers. CTCC enables fingerprint verification under black-box
access while mitigating false positives and fingerprint leakage, supporting
continuous construction under a shared semantic rule even if partial triggers
are exposed. Extensive experiments across multiple LLM architectures
demonstrate that CTCC consistently achieves stronger stealth and robustness
than prior work. Our findings position CTCC as a reliable and practical
solution for ownership verification in real-world LLM deployment scenarios. Our
code and data are publicly available at <https://github.com/Xuzhenhua55/CTCC>.

</details>


### [206] [Temporal Preferences in Language Models for Long-Horizon Assistance](https://arxiv.org/abs/2509.09704)
*Ali Mazyaki,Mohammad Naghizadeh,Samaneh Ranjkhah Zonouzaghi,Hossein Setareh*

Main category: cs.CL

TL;DR: 研究语言模型在跨期选择中的时间偏好及可操纵性，评估模型并提出指标，得出相关结果并讨论设计影响与研究方向。


<details>
  <summary>Details</summary>
Motivation: 探究语言模型在跨期选择中是否存在未来或当前导向偏好以及能否被系统操纵。

Method: 采用改编的人类实验协议，在时间权衡任务上评估多个语言模型，并与人类决策者样本进行对比，引入可操纵性指标MTO。

Result: 推理型模型在未来导向提示下选后期选项，但跨身份或地域个性化决策能力有限，正确推理时间导向的模型会内化未来导向。

Conclusion: 讨论了AI助手与异质、长期目标对齐的设计影响，提出个性化上下文校准和社会意识部署的研究议程。

Abstract: We study whether language models (LMs) exhibit future- versus
present-oriented preferences in intertemporal choice and whether those
preferences can be systematically manipulated. Using adapted human experimental
protocols, we evaluate multiple LMs on time-tradeoff tasks and benchmark them
against a sample of human decision makers. We introduce an operational metric,
the Manipulability of Time Orientation (MTO), defined as the change in an LM's
revealed time preference between future- and present-oriented prompts. In our
tests, reasoning-focused models (e.g., DeepSeek-Reasoner and grok-3-mini)
choose later options under future-oriented prompts but only partially
personalize decisions across identities or geographies. Moreover, models that
correctly reason about time orientation internalize a future orientation for
themselves as AI decision makers. We discuss design implications for AI
assistants that should align with heterogeneous, long-horizon goals and outline
a research agenda on personalized contextual calibration and socially aware
deployment.

</details>


### [207] [The Non-Determinism of Small LLMs: Evidence of Low Answer Consistency in Repetition Trials of Standard Multiple-Choice Benchmarks](https://arxiv.org/abs/2509.09705)
*Claudio Pinhanez,Paulo Cavalin,Cassia Sanctos,Marcelo Grave,Yago Primerano*

Main category: cs.CL

TL;DR: 研究小语言模型多次回答同一问题的一致性，提出新工具，发现小模型一致性答案比例及与整体准确率的关联，中型模型一致性更高。


<details>
  <summary>Details</summary>
Motivation: 探究小语言模型（2B - 8B参数）多次回答同一问题的一致性，以及多轮答案一致性对准确率的影响和模型选择的权衡。

Method: 用开源大语言模型对MMLU - Redux和MedQA多项选择基准测试题重复回答10次，考虑不同推理温度、模型大小、微调与否等参数，提出新分析和图形工具。

Result: 小模型在低推理温度下能一致回答的问题比例通常在50% - 80%，一致答案准确率与整体准确率有合理相关性，中型模型答案一致性更高。

Conclusion: 不同模型回答问题的一致性存在差异，中型模型比小模型有更高的答案一致性。

Abstract: This work explores the consistency of small LLMs (2B-8B parameters) in
answering multiple times the same question. We present a study on known,
open-source LLMs responding to 10 repetitions of questions from the
multiple-choice benchmarks MMLU-Redux and MedQA, considering different
inference temperatures, small vs. medium models (50B-80B), finetuned vs. base
models, and other parameters. We also look into the effects of requiring
multi-trial answer consistency on accuracy and the trade-offs involved in
deciding which model best provides both of them. To support those studies, we
propose some new analytical and graphical tools. Results show that the number
of questions which can be answered consistently vary considerably among models
but are typically in the 50%-80% range for small models at low inference
temperatures. Also, accuracy among consistent answers seems to reasonably
correlate with overall accuracy. Results for medium-sized models seem to
indicate much higher levels of answer consistency.

</details>


### [208] [Beyond I'm Sorry, I Can't: Dissecting Large Language Model Refusal](https://arxiv.org/abs/2509.09708)
*Nirmalendu Prakash,Yeo Wei Jie,Amir Abdullah,Ranjan Satapathy,Erik Cambria,Roy Ka Wei Lee*

Main category: cs.CL

TL;DR: 研究通过SAEs分析指令调优大模型拒绝有害提示行为，找到越狱关键特征，揭示拒绝机制并为安全行为审计和干预提供可能。


<details>
  <summary>Details</summary>
Motivation: 当前对指令调优大模型拒绝有害提示行为的内在原因理解不足，需深入研究。

Method: 使用SAEs分析Gemma - 2 - 2B - IT和LLaMA - 3.1 - 8B - IT两个模型，分拒绝方向、贪婪过滤、交互发现三个阶段搜索SAE潜在空间。

Result: 找到大量越狱关键特征，发现冗余特征。

Conclusion: 通过操纵可解释潜在空间，能对安全行为进行细粒度审计和有针对性干预。

Abstract: Refusal on harmful prompts is a key safety behaviour in instruction-tuned
large language models (LLMs), yet the internal causes of this behaviour remain
poorly understood. We study two public instruction-tuned models, Gemma-2-2B-IT
and LLaMA-3.1-8B-IT, using sparse autoencoders (SAEs) trained on
residual-stream activations. Given a harmful prompt, we search the SAE latent
space for feature sets whose ablation flips the model from refusal to
compliance, demonstrating causal influence and creating a jailbreak. Our search
proceeds in three stages: (1) Refusal Direction: find a refusal-mediating
direction and collect SAE features near that direction; (2) Greedy Filtering:
prune to a minimal set; and (3) Interaction Discovery: fit a factorization
machine (FM) that captures nonlinear interactions among the remaining active
features and the minimal set. This pipeline yields a broad set of
jailbreak-critical features, offering insight into the mechanistic basis of
refusal. Moreover, we find evidence of redundant features that remain dormant
unless earlier features are suppressed. Our findings highlight the potential
for fine-grained auditing and targeted intervention in safety behaviours by
manipulating the interpretable latent space.

</details>


### [209] [Assisting Research Proposal Writing with Large Language Models: Evaluation and Refinement](https://arxiv.org/abs/2509.09709)
*Jing Ren,Weiqi Wang*

Main category: cs.CL

TL;DR: 研究提出内容质量和参考有效性两个评估指标及迭代提示方法，实验表明能客观评估ChatGPT写作表现，提升内容质量并减少参考问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型用于学术写作存在参考问题，且现有内容质量评估主观、费力、缺乏客观性。

Method: 提出内容质量和参考有效性两个评估指标，以及基于这两个指标得分的迭代提示方法。

Result: 提出的指标为评估ChatGPT写作表现提供了客观、定量的框架，迭代提示显著提升内容质量，减少参考不准确和编造问题。

Conclusion: 所提方法能解决学术场景中的关键伦理挑战，可对大语言模型进行定量评估，提升其研究提案写作能力。

Abstract: Large language models (LLMs) like ChatGPT are increasingly used in academic
writing, yet issues such as incorrect or fabricated references raise ethical
concerns. Moreover, current content quality evaluations often rely on
subjective human judgment, which is labor-intensive and lacks objectivity,
potentially compromising the consistency and reliability. In this study, to
provide a quantitative evaluation and enhance research proposal writing
capabilities of LLMs, we propose two key evaluation metrics--content quality
and reference validity--and an iterative prompting method based on the scores
derived from these two metrics. Our extensive experiments show that the
proposed metrics provide an objective, quantitative framework for assessing
ChatGPT's writing performance. Additionally, iterative prompting significantly
enhances content quality while reducing reference inaccuracies and
fabrications, addressing critical ethical challenges in academic contexts.

</details>


### [210] [Generating Individual Travel Diaries Using Large Language Models Informed by Census and Land-Use Data](https://arxiv.org/abs/2509.09710)
*Sepehr Golrokh Amin,Devin Rhoads,Fatemeh Fakhrmoosavi,Nicholas E. Lownes,John N. Ivan*

Main category: cs.CL

TL;DR: 本文提出用大语言模型（LLM）为基于代理的交通模型生成个人出行日记，通过开源数据生成人物角色，用新的真实度分数验证，结果显示LLM生成的日记有一定优势和可行性。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖大量专有家庭出行调查，希望利用开源数据和大语言模型生成个人出行日记。

Method: 从开源美国社区调查（ACS）和智能位置数据库（SLD）数据随机生成人物角色，通过直接提示合成日记；用新的真实度分数和Jensen - Shannon散度验证。

Result: 与经典方法相比，LLM生成的日记整体真实度相当，在确定出行目的上表现出色、一致性更高；经典模型在出行次数和活动时长数值估计上占优；聚合验证证明LLM具有统计代表性。

Conclusion: LLM具有零样本可行性，为未来合成日记评估系统建立了可量化的日记真实度指标。

Abstract: This study introduces a Large Language Model (LLM) scheme for generating
individual travel diaries in agent-based transportation models. While
traditional approaches rely on large quantities of proprietary household travel
surveys, the method presented in this study generates personas stochastically
from open-source American Community Survey (ACS) and Smart Location Database
(SLD) data, then synthesizes diaries through direct prompting. This study
features a novel one-to-cohort realism score: a composite of four metrics (Trip
Count Score, Interval Score, Purpose Score, and Mode Score) validated against
the Connecticut Statewide Transportation Study (CSTS) diaries, matched across
demographic variables. The validation utilizes Jensen-Shannon Divergence to
measure distributional similarities between generated and real diaries. When
compared to diaries generated with classical methods (Negative Binomial for
trip generation; Multinomial Logit for mode/purpose) calibrated on the
validation set, LLM-generated diaries achieve comparable overall realism (LLM
mean: 0.485 vs. 0.455). The LLM excels in determining trip purpose and
demonstrates greater consistency (narrower realism score distribution), while
classical models lead in numerical estimates of trip count and activity
duration. Aggregate validation confirms the LLM's statistical
representativeness (LLM mean: 0.612 vs. 0.435), demonstrating LLM's zero-shot
viability and establishing a quantifiable metric of diary realism for future
synthetic diary evaluation systems.

</details>


### [211] [Psychiatry-Bench: A Multi-Task Benchmark for LLMs in Psychiatry](https://arxiv.org/abs/2509.09711)
*Aya E. Fouda,Abdelrahamn A. Hassan,Radwa J. Hanafy,Mohammed E. Fouda*

Main category: cs.CL

TL;DR: 本文介绍PsychiatryBench基准测试，评估大语言模型在精神科应用表现，发现模型存在不足，该基准可用于提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有评估资源临床有效性不足，无法全面反映精神科推理复杂性，需新基准评估大语言模型在精神科实践中的表现。

Method: 构建基于权威精神科教材和案例集的PsychiatryBench基准，含11个问答任务超5300个标注项，用传统指标和‘LLM-as-judge’框架评估多种前沿大语言模型和开源医疗模型。

Result: 评估发现模型在临床一致性和安全性上有较大差距，尤其在多轮跟进和管理任务中。

Conclusion: 需要对模型进行专门调优和采用更稳健评估范式，PsychiatryBench为高风险心理健康应用提供模块化、可扩展的基准平台。

Abstract: Large language models (LLMs) hold great promise in enhancing psychiatric
practice, from improving diagnostic accuracy to streamlining clinical
documentation and therapeutic support. However, existing evaluation resources
heavily rely on small clinical interview corpora, social media posts, or
synthetic dialogues, which limits their clinical validity and fails to capture
the full complexity of psychiatric reasoning. In this work, we introduce
PsychiatryBench, a rigorously curated benchmark grounded exclusively in
authoritative, expert-validated psychiatric textbooks and casebooks.
PsychiatryBench comprises eleven distinct question-answering tasks ranging from
diagnostic reasoning and treatment planning to longitudinal follow-up,
management planning, clinical approach, sequential case analysis, and
multiple-choice/extended matching formats totaling over 5,300 expert-annotated
items. We evaluate a diverse set of frontier LLMs (including Google Gemini,
DeepSeek, LLaMA 3, and QWQ-32) alongside leading open-source medical models
(e.g., OpenBiloLLM, MedGemma) using both conventional metrics and an
"LLM-as-judge" similarity scoring framework. Our results reveal substantial
gaps in clinical consistency and safety, particularly in multi-turn follow-up
and management tasks, underscoring the need for specialized model tuning and
more robust evaluation paradigms. PsychiatryBench offers a modular, extensible
platform for benchmarking and improving LLM performance in high-stakes mental
health applications.

</details>


### [212] [The Thinking Therapist: Training Large Language Models to Deliver Acceptance and Commitment Therapy using Supervised Fine-Tuning and Odds Ratio Policy Optimization](https://arxiv.org/abs/2509.09712)
*Talha Tahir*

Main category: cs.CL

TL;DR: 研究用不同方法训练小语言模型提供ACT疗法，发现ORPO训练模型表现更佳，COT对SFT模型有益。


<details>
  <summary>Details</summary>
Motivation: 研究后训练方法和显式推理对小语言模型提供ACT疗法能力的影响。

Method: 用Mistral - Large生成的50组合成ACT对话训练Llama - 3.2 - 3b - Instruct，采用SFT和ORPO两种方法，分有无显式COT推理步骤，与基础模型对比，在模拟治疗会话中用ACT - FM和TES评估。

Result: ORPO训练模型在ACT保真度和治疗共情方面显著优于SFT和基础模型；COT对SFT模型有益，对ORPO和基础调优模型无明显优势。

Conclusion: 偏好对齐策略优化可有效使小语言模型具备ACT能力，显式推理的效用高度依赖训练范式。

Abstract: Acceptance and Commitment Therapy (ACT) is a third-wave cognitive behavioral
therapy with emerging evidence of efficacy in several psychiatric conditions.
This study investigates the impact of post-training methodology and explicit
reasoning on the ability of a small open-weight large language model (LLM) to
deliver ACT. Using 50 sets of synthetic ACT transcripts generated by
Mistral-Large, we trained Llama-3.2-3b-Instruct with two distinct approaches,
supervised fine-tuning (SFT) and odds ratio policy optimization (ORPO), each
with and without an explicit chain-of-thought (COT) reasoning step. Performance
was evaluated by comparing these four post-trained variants against the base
Instruct model. These models were benchmarked in simulated therapy sessions,
with performance quantitatively assessed on the ACT Fidelity Measure (ACT-FM)
and the Therapist Empathy Scale (TES) by an LLM judge that had been fine-tuned
on human evaluations. Our findings demonstrate that the ORPO-trained models
significantly outperformed both their SFT and Instruct counterparts on ACT
fidelity ($\chi^2(5) = 185.15, p < .001$) and therapeutic empathy ($\chi^2(5) =
140.37, p < .001$). The effect of COT was conditional as it provided a
significant benefit to SFT models, improving ACT-FM scores by an average of
2.68 points ($p < .001$), while offering no discernible advantage to the
superior ORPO or instruct-tuned variants. We posit that the superiority of ORPO
stems from its ability to learn the therapeutic `process' over imitating
`content,' a key aspect of ACT, while COT acts as a necessary scaffold for
models trained only via imitation. This study establishes that
preference-aligned policy optimization can effectively instill ACT competencies
in small LLMs, and that the utility of explicit reasoning is highly dependent
on the underlying training paradigm.

</details>


### [213] [HANRAG: Heuristic Accurate Noise-resistant Retrieval-Augmented Generation for Multi-hop Question Answering](https://arxiv.org/abs/2509.09713)
*Duolin Sun,Dan Yang,Yue Shen,Yihan Jiao,Zhehao Tan,Jie Feng,Lianzhen Zhong,Jian Wang,Peng Wei,Jinjie Gu*

Main category: cs.CL

TL;DR: 本文指出当前RAG方法处理多跳查询存在问题，提出HANRAG框架，经对比实验显示其在单跳和多跳问答任务中表现更优。


<details>
  <summary>Details</summary>
Motivation: 当前RAG方法在处理多跳查询时面临诸多挑战，如过度依赖迭代检索、检索内容有噪声等，需要改进。

Method: 引入基于启发式的HANRAG框架，借助强大的揭示器对查询进行路由、分解为子查询，并过滤检索文档中的噪声。

Result: 将HANRAG框架与其他领先的行业方法在多个基准上进行比较，结果表明该框架在单跳和多跳问答任务中均获得了更优的性能。

Conclusion: HANRAG框架能有效处理不同复杂度的问题，增强系统的适应性和抗噪声能力，适用于各种查询。

Abstract: The Retrieval-Augmented Generation (RAG) approach enhances question-answering
systems and dialogue generation tasks by integrating information retrieval (IR)
technologies with large language models (LLMs). This strategy, which retrieves
information from external knowledge bases to bolster the response capabilities
of generative models, has achieved certain successes. However, current RAG
methods still face numerous challenges when dealing with multi-hop queries. For
instance, some approaches overly rely on iterative retrieval, wasting too many
retrieval steps on compound queries. Additionally, using the original complex
query for retrieval may fail to capture content relevant to specific
sub-queries, resulting in noisy retrieved content. If the noise is not managed,
it can lead to the problem of noise accumulation. To address these issues, we
introduce HANRAG, a novel heuristic-based framework designed to efficiently
tackle problems of varying complexity. Driven by a powerful revelator, HANRAG
routes queries, decomposes them into sub-queries, and filters noise from
retrieved documents. This enhances the system's adaptability and noise
resistance, making it highly capable of handling diverse queries. We compare
the proposed framework against other leading industry methods across various
benchmarks. The results demonstrate that our framework obtains superior
performance in both single-hop and multi-hop question-answering tasks.

</details>


### [214] [How Small Transformation Expose the Weakness of Semantic Similarity Measures](https://arxiv.org/abs/2509.09714)
*Serge Lionel Nikiema,Albérick Euraste Djire,Abdoul Aziz Bonkoungou,Micheline Bénédicte Moumoula,Jordan Samhi,Abdoul Kader Kabore,Jacques Klein,Tegawendé F. Bissyande*

Main category: cs.CL

TL;DR: 研究测试18种语义相似度测量方法，发现常用指标有问题，嵌入方法表现差，LLM方法区分语义差异更好。


<details>
  <summary>Details</summary>
Motivation: 研究不同方法测量语义相似度的效果，解决大语言模型是否真理解语义关系的疑问，其对软件工程应用重要。

Method: 测试18种相似度测量方法，创建系统测试框架，对文本和代码做控制更改来评估。

Result: 常用指标有显著问题，嵌入方法常误判，换距离计算方式有改善，LLM方法区分语义更好。

Conclusion: 嵌入方法计算距离方式影响性能，LLM方法在区分语义差异上表现更佳。

Abstract: This research examines how well different methods measure semantic
similarity, which is important for various software engineering applications
such as code search, API recommendations, automated code reviews, and
refactoring tools. While large language models are increasingly used for these
similarity assessments, questions remain about whether they truly understand
semantic relationships or merely recognize surface patterns.
  The study tested 18 different similarity measurement approaches, including
word-based methods, embedding techniques, LLM-based systems, and
structure-aware algorithms. The researchers created a systematic testing
framework that applies controlled changes to text and code to evaluate how well
each method handles different types of semantic relationships.
  The results revealed significant issues with commonly used metrics. Some
embedding-based methods incorrectly identified semantic opposites as similar up
to 99.9 percent of the time, while certain transformer-based approaches
occasionally rated opposite meanings as more similar than synonymous ones. The
study found that embedding methods' poor performance often stemmed from how
they calculate distances; switching from Euclidean distance to cosine
similarity improved results by 24 to 66 percent. LLM-based approaches performed
better at distinguishing semantic differences, producing low similarity scores
(0.00 to 0.29) for genuinely different meanings, compared to embedding methods
that incorrectly assigned high scores (0.82 to 0.99) to dissimilar content.

</details>


### [215] [Investigating Symbolic Triggers of Hallucination in Gemma Models Across HaluEval and TruthfulQA](https://arxiv.org/abs/2509.09715)
*Naveen Lamba,Sanju Tiwari,Manas Gaur*

Main category: cs.CL

TL;DR: 研究识别并刻画大语言模型易产生幻觉的关键属性，利用数据集分析，发现模型规模增大幻觉率降低但符号属性仍致大量幻觉。


<details>
  <summary>Details</summary>
Motivation: 现有研究未识别和研究大语言模型易产生幻觉的内在属性，本文旨在解决该问题。

Method: 利用HaluEval和TruthfulQA数据集，将其问答格式转换，以确定导致幻觉的属性。

Result: Gemma - 2 - 2B模型符号属性幻觉率平均79.0%，Gemma - 2 - 9B降至73.6%，Gemma - 2 - 27B降至63.9%；修饰词和命名实体幻觉率高。

Conclusion: 符号元素持续干扰模型，这是大语言模型处理此类输入的根本弱点，与模型规模无关。

Abstract: Hallucination in Large Language Models (LLMs) is a well studied problem.
However, the properties that make LLM intrinsically vulnerable to
hallucinations have not been identified and studied. This research identifies
and characterizes the key properties, allowing us to pinpoint vulnerabilities
within the model's internal mechanisms. To solidify on these properties, we
utilized two established datasets, HaluEval and TruthfulQA and convert their
existing format of question answering into various other formats to narrow down
these properties as the reason for the hallucinations. Our findings reveal that
hallucination percentages across symbolic properties are notably high for
Gemma-2-2B, averaging 79.0% across tasks and datasets. With increased model
scale, hallucination drops to 73.6% for Gemma-2-9B and 63.9% for Gemma-2-27B,
reflecting a 15 percentage point reduction overall. Although the hallucination
rate decreases as the model size increases, a substantial amount of
hallucination caused by symbolic properties still persists. This is especially
evident for modifiers (ranging from 84.76% to 94.98%) and named entities
(ranging from 83.87% to 93.96%) across all Gemma models and both datasets.
These findings indicate that symbolic elements continue to confuse the models,
pointing to a fundamental weakness in how these LLMs process such
inputs--regardless of their scale.

</details>


### [216] [ALIGNS: Unlocking nomological networks in psychological measurement through a large language model](https://arxiv.org/abs/2509.09723)
*Kai R. Larsen,Sen Yan,Roland Müller,Lan Sang,Mikko Rönkkö,Ravi Starzl,Donald Edmondson*

Main category: cs.CL

TL;DR: 文章介绍基于大语言模型的ALIGNS系统解决测量验证中的基础问题，提供多个领域的法则网络并进行评估，该系统免费可用。


<details>
  <summary>Details</summary>
Motivation: 当前构建法则网络以确立测量有效性仍是挑战，且此局限有实际后果，如临床试验和公共政策可能受影响。

Method: 引入基于大语言模型、用经过验证的问卷测量训练的ALIGNS系统，进行分类准确率测试和三项评估。

Result: ALIGNS提供含超55万指标的三个综合法则网络；评估发现NIH PROMIS焦虑和抑郁工具可归为一个维度；发现儿童气质测量四个潜在维度并质疑一个现有维度；专家评估其重要性、可及性和适用性。

Conclusion: ALIGNS系统免费可用，能以大规模法则分析补充传统验证方法。

Abstract: Psychological measurement is critical to many disciplines. Despite advances
in measurement, building nomological networks, theoretical maps of how concepts
and measures relate to establish validity, remains a challenge 70 years after
Cronbach and Meehl proposed them as fundamental to validation. This limitation
has practical consequences: clinical trials may fail to detect treatment
effects, and public policy may target the wrong outcomes. We introduce Analysis
of Latent Indicators to Generate Nomological Structures (ALIGNS), a large
language model-based system trained with validated questionnaire measures.
ALIGNS provides three comprehensive nomological networks containing over
550,000 indicators across psychology, medicine, social policy, and other
fields. This represents the first application of large language models to solve
a foundational problem in measurement validation. We report classification
accuracy tests used to develop the model, as well as three evaluations. In the
first evaluation, the widely used NIH PROMIS anxiety and depression instruments
are shown to converge into a single dimension of emotional distress. The second
evaluation examines child temperament measures and identifies four potential
dimensions not captured by current frameworks, and questions one existing
dimension. The third evaluation, an applicability check, engages expert
psychometricians who assess the system's importance, accessibility, and
suitability. ALIGNS is freely available at nomologicalnetwork.org,
complementing traditional validation methods with large-scale nomological
analysis.

</details>


### [217] [DiTTO-LLM: Framework for Discovering Topic-based Technology Opportunities via Large Language Model](https://arxiv.org/abs/2509.09724)
*Wonyoung Kim,Sujeong Seo,Juhyun Lee*

Main category: cs.CL

TL;DR: 本文提出基于技术时间关系的框架识别新兴技术机会，用专利数据集评估，结果显示人工智能技术向日常易获取形式发展，该框架有识别未来技术机会潜力。


<details>
  <summary>Details</summary>
Motivation: 技术机会是技术、产业和创新进步的基础，需识别新兴技术机会。

Method: 从专利数据集提取文本，映射文本主题发现技术间关系，跟踪主题随时间变化识别技术机会，利用大语言模型提取主题，用提示支持发现技术机会。

Result: 实验表明人工智能技术正演变为便于日常使用的形式。

Conclusion: 所提出的框架有识别未来技术机会的潜力。

Abstract: Technology opportunities are critical information that serve as a foundation
for advancements in technology, industry, and innovation. This paper proposes a
framework based on the temporal relationships between technologies to identify
emerging technology opportunities. The proposed framework begins by extracting
text from a patent dataset, followed by mapping text-based topics to discover
inter-technology relationships. Technology opportunities are then identified by
tracking changes in these topics over time. To enhance efficiency, the
framework leverages a large language model to extract topics and employs a
prompt for a chat-based language model to support the discovery of technology
opportunities. The framework was evaluated using an artificial intelligence
patent dataset provided by the United States Patent and Trademark Office. The
experimental results suggest that artificial intelligence technology is
evolving into forms that facilitate everyday accessibility. This approach
demonstrates the potential of the proposed framework to identify future
technology opportunities.

</details>


### [218] [MultimodalHugs: Enabling Sign Language Processing in Hugging Face](https://arxiv.org/abs/2509.09729)
*Gerard Sant,Zifan Jiang,Carlos Escolano,Amit Moryossef,Mathias Müller,Rico Sennrich,Sarah Ebling*

Main category: cs.CL

TL;DR: 现有手语处理研究因代码问题导致可复现性低，Hugging Face 不适合手语实验，为此提出 MultimodalHugs 框架并进行实验展示其适用性。


<details>
  <summary>Details</summary>
Motivation: 手语处理研究存在代码复杂、可复现性低、现有工具不适用等问题，需要更好的解决方案。

Method: 构建基于 Hugging Face 的 MultimodalHugs 框架，并进行定量实验。

Result: MultimodalHugs 能适应如手语姿态估计数据、文本字符像素数据等多种数据模态。

Conclusion: MultimodalHugs 框架具有更广泛的适用性，能解决手语处理研究现有问题。

Abstract: In recent years, sign language processing (SLP) has gained importance in the
general field of Natural Language Processing. However, compared to research on
spoken languages, SLP research is hindered by complex ad-hoc code,
inadvertently leading to low reproducibility and unfair comparisons. Existing
tools that are built for fast and reproducible experimentation, such as Hugging
Face, are not flexible enough to seamlessly integrate sign language
experiments. This view is confirmed by a survey we conducted among SLP
researchers.
  To address these challenges, we introduce MultimodalHugs, a framework built
on top of Hugging Face that enables more diverse data modalities and tasks,
while inheriting the well-known advantages of the Hugging Face ecosystem. Even
though sign languages are our primary focus, MultimodalHugs adds a layer of
abstraction that makes it more widely applicable to other use cases that do not
fit one of the standard templates of Hugging Face. We provide quantitative
experiments to illustrate how MultimodalHugs can accommodate diverse modalities
such as pose estimation data for sign languages, or pixel data for text
characters.

</details>


### [219] [MCP-AgentBench: Evaluating Real-World Language Agent Performance with MCP-Mediated Tools](https://arxiv.org/abs/2509.09734)
*Zikang Guo,Benfeng Xu,Chiwei Zhu,Wentao Hong,Xiaorui Wang,Zhendong Mao*

Main category: cs.CL

TL;DR: 现有基准难以评估MCP范式下智能体性能，本文引入MCP - AgentBench进行评估并提供见解，助力构建标准化框架。


<details>
  <summary>Details</summary>
Motivation: 现有基准无法准确评估MCP范式下智能体在现实场景的性能，导致对其真实价值认知偏差。

Method: 建立含33个操作服务器和188个不同工具的MCP测试平台；开发含600个系统设计查询、分6类不同交互复杂度的基准；引入以结果为导向的MCP - Eval评估方法。

Result: 对领先语言智能体进行广泛实证评估并提供基础见解。

Conclusion: MCP - AgentBench为研究社区提供标准化可靠框架，推动构建能充分利用MCP优势的智能体，加速迈向真正强大且可互操作的AI系统。

Abstract: The Model Context Protocol (MCP) is rapidly emerging as a pivotal open
standard, designed to enhance agent-tool integration and interoperability, and
is positioned to unlock a new era of powerful, interconnected, and genuinely
utilitarian agentic AI. However, despite MCP's growing adoption, existing
benchmarks often fail to capture real-world agent performance within this new
paradigm, leading to a distorted perception of their true operational value and
an inability to reliably differentiate proficiencies. To bridge this critical
evaluation gap, we introduce MCP-AgentBench -- a comprehensive benchmark
specifically engineered to rigorously assess language agent capabilities in
MCP-mediated tool interactions. Core contributions of MCP-AgentBench include:
the establishment of a robust MCP testbed comprising 33 operational servers
with 188 distinct tools; the development of a benchmark featuring 600
systematically designed queries distributed across 6 distinct categories of
varying interaction complexity; and the introduction of MCP-Eval, a novel
outcome-oriented evaluation methodology prioritizing real-world task success.
Through extensive empirical evaluation of leading language agents, we provide
foundational insights. MCP-AgentBench aims to equip the research community with
a standardized and reliable framework to build, validate, and advance agents
capable of fully leveraging MCP's transformative benefits, thereby accelerating
progress toward truly capable and interoperable AI systems.

</details>


### [220] [HEFT: A Coarse-to-Fine Hierarchy for Enhancing the Efficiency and Accuracy of Language Model Reasoning](https://arxiv.org/abs/2509.09801)
*Brennen Hill*

Main category: cs.CL

TL;DR: 论文提出HEFT策略，结合两种PEFT方法，在BoolQ基准测试中展现出协同效应，以更少计算资源提升语言模型推理能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型适应专业推理任务受计算资源限制，研究不同PEFT范式协同能否提升性能和效率。

Method: 提出HEFT策略，先在权重空间用LoRA进行基础调整，再用ReFT对内部激活进行精确细化。

Result: 用HEFT策略对Llama - 2 - 7B模型微调3个周期，准确率达85.17%，超过仅用LoRA训练20个周期（85.05%）和仅用ReFT训练20个周期（83.36%）的模型。

Conclusion: 精心组合PEFT方法是有效的算法创新，为提升语言模型推理能力提供更高效途径，能克服大规模模型适应复杂认知任务的障碍。

Abstract: The adaptation of large language models (LLMs) to specialized reasoning tasks
is fundamentally constrained by computational resources. Parameter-Efficient
Fine-Tuning (PEFT) methods have emerged as a powerful solution, yet the
landscape of these techniques is diverse, with distinct methods operating in
either the model's weight space or its representation space. This paper
investigates the hypothesis that a synergistic combination of these paradigms
can unlock superior performance and efficiency. We introduce HEFT (Hierarchical
Efficient Fine-Tuning), a novel hierarchical adaptation strategy that composes
two distinct PEFT methods in a coarse-to-fine manner: first, a broad,
foundational adaptation in the weight space using Low-Rank Adaptation (LoRA),
followed by a precise, surgical refinement of internal activations using
Representation Fine-Tuning (ReFT). We evaluate this approach by fine-tuning a
Llama-2-7B model on the BoolQ benchmark, a challenging dataset for inferential
reasoning. Our results reveal a profound synergistic effect. A model fine-tuned
for only three epochs with our HEFT strategy achieves an accuracy of 85.17\%,
exceeding the performance of models trained for 20 epochs with either LoRA-only
(85.05\%) or ReFT-only (83.36\%) methodologies. This work demonstrates that the
thoughtful composition of PEFT methods is a potent algorithmic innovation,
offering a more efficient and effective path toward advancing the reasoning
capabilities of language models. By achieving superior results with a fraction
of the computational budget, our findings present a principled approach to
overcoming the obstacles inherent in adapting large-scale models for complex
cognitive tasks.

</details>


### [221] [A meta-analysis on the performance of machine-learning based language models for sentiment analysis](https://arxiv.org/abs/2509.09728)
*Elena Rohde,Jonas Klingwort,Christian Borgs*

Main category: cs.CL

TL;DR: 本文对Twitter数据情感分析中机器学习性能进行元分析，分析模型性能影响因素，得出平均准确率，给出两点关键见解。


<details>
  <summary>Details</summary>
Motivation: 评估机器学习在Twitter数据情感分析中的平均性能，评估研究间和研究内的异质性，分析研究特征对模型性能的影响。

Method: 按照PRISMA指南搜索学术数据库，选取20项研究的195个试验及12个研究特征，用双反正弦变换和三级随机效应模型分析总体准确率。

Result: AIC优化模型的平均总体准确率为0.80 [0.76, 0.84]。

Conclusion: 总体准确率常用但因对类别不平衡和情感类别数量敏感而具误导性，需标准化；跨研究比较ML分类器需标准化报告模型性能，当前远未普及。

Abstract: This paper presents a meta-analysis evaluating ML performance in sentiment
analysis for Twitter data. The study aims to estimate the average performance,
assess heterogeneity between and within studies, and analyze how study
characteristics influence model performance. Using PRISMA guidelines, we
searched academic databases and selected 195 trials from 20 studies with 12
study features. Overall accuracy, the most reported performance metric, was
analyzed using double arcsine transformation and a three-level random effects
model. The average overall accuracy of the AIC-optimized model was 0.80 [0.76,
0.84]. This paper provides two key insights: 1) Overall accuracy is widely used
but often misleading due to its sensitivity to class imbalance and the number
of sentiment classes, highlighting the need for normalization. 2) Standardized
reporting of model performance, including reporting confusion matrices for
independent test sets, is essential for reliable comparisons of ML classifiers
across studies, which seems far from common practice.

</details>


### [222] [Emulating Public Opinion: A Proof-of-Concept of AI-Generated Synthetic Survey Responses for the Chilean Case](https://arxiv.org/abs/2509.09871)
*Bastián González-Bustamante,Nando Verelst,Carla Cisternas*

Main category: cs.CL

TL;DR: 评估大语言模型生成的合成调查响应与真实人类响应的可靠性，发现合成响应在信任项目上表现出色，不同模型表现相当，45 - 59岁受访者合成 - 人类一致性最高，但捕捉民意细微差别仍具挑战。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在调查研究中有创新潜力，但恢复聚合项目分布的程度不确定且有再现社会刻板印象和偏差的风险，因此评估其生成的合成调查响应的可靠性。

Method: 以智利公共舆论概率调查的真实人类响应为基准，对128个提示 - 模型 - 问题三元组进行基准测试，生成189,696个合成概况，并在128个问题 - 子样本对中进行元分析，评估OpenAI的GPT系列、o系列推理模型、Llama和Qwen检查点。

Result: 合成响应在信任项目上表现优异（F1分数和准确率>0.90）；GPT - 4o、GPT - 4o - mini和Llama 4 Maverick表现相当；45 - 59岁受访者的合成 - 人类一致性最高。

Conclusion: 基于大语言模型的合成样本可近似概率样本的响应，但存在显著的项目级异质性，捕捉民意细微差别仍具挑战，需要仔细校准和额外的分布测试以确保算法保真度和减少误差。

Abstract: Large Language Models (LLMs) offer promising avenues for methodological and
applied innovations in survey research by using synthetic respondents to
emulate human answers and behaviour, potentially mitigating measurement and
representation errors. However, the extent to which LLMs recover aggregate item
distributions remains uncertain and downstream applications risk reproducing
social stereotypes and biases inherited from training data. We evaluate the
reliability of LLM-generated synthetic survey responses against ground-truth
human responses from a Chilean public opinion probabilistic survey.
Specifically, we benchmark 128 prompt-model-question triplets, generating
189,696 synthetic profiles, and pool performance metrics (i.e., accuracy,
precision, recall, and F1-score) in a meta-analysis across 128
question-subsample pairs to test for biases along key sociodemographic
dimensions. The evaluation spans OpenAI's GPT family and o-series reasoning
models, as well as Llama and Qwen checkpoints. Three results stand out. First,
synthetic responses achieve excellent performance on trust items (F1-score and
accuracy > 0.90). Second, GPT-4o, GPT-4o-mini and Llama 4 Maverick perform
comparably on this task. Third, synthetic-human alignment is highest among
respondents aged 45-59. Overall, LLM-based synthetic samples approximate
responses from a probabilistic sample, though with substantial item-level
heterogeneity. Capturing the full nuance of public opinion remains challenging
and requires careful calibration and additional distributional tests to ensure
algorithmic fidelity and reduce errors.

</details>


### [223] [Large Language Models Meet Legal Artificial Intelligence: A Survey](https://arxiv.org/abs/2509.09969)
*Zhitian Hou,Zihan Ye,Nanli Zeng,Tianyong Hao,Kun Zeng*

Main category: cs.CL

TL;DR: 本文对16个法律大语言模型系列、47个基于大语言模型的法律任务框架进行综述，收集15个基准和29个数据集，分析挑战并探讨未来方向，为初学者提供系统介绍。


<details>
  <summary>Details</summary>
Motivation: 推动基于大语言模型的方法在法律领域的研究和应用。

Method: 对法律大语言模型系列、框架进行全面综述，收集基准和数据集。

Result: 完成对相关模型、框架、基准和数据集的收集与分析。

Conclusion: 为初学者提供系统介绍，鼓励该领域未来研究。

Abstract: Large Language Models (LLMs) have significantly advanced the development of
Legal Artificial Intelligence (Legal AI) in recent years, enhancing the
efficiency and accuracy of legal tasks. To advance research and applications of
LLM-based approaches in legal domain, this paper provides a comprehensive
review of 16 legal LLMs series and 47 LLM-based frameworks for legal tasks, and
also gather 15 benchmarks and 29 datasets to evaluate different legal
capabilities. Additionally, we analyse the challenges and discuss future
directions for LLM-based approaches in the legal domain. We hope this paper
provides a systematic introduction for beginners and encourages future research
in this field. Resources are available at
https://github.com/ZhitianHou/LLMs4LegalAI.

</details>


### [224] [Unsupervised Hallucination Detection by Inspecting Reasoning Processes](https://arxiv.org/abs/2509.10004)
*Ponhvoan Srey,Xiaobao Wu,Anh Tuan Luu*

Main category: cs.CL

TL;DR: 提出无监督幻觉检测框架IRIS，利用与事实正确性相关的内部表示，实验显示其优于现有无监督方法，适合实时检测。


<details>
  <summary>Details</summary>
Motivation: 现有无监督幻觉检测方法依赖与事实正确性无关的代理信号，存在偏差且泛化性受限。

Method: 提出IRIS框架，促使大语言模型验证陈述真实性，获取上下文嵌入作为特征训练，将响应不确定性作为软伪标签。

Result: IRIS始终优于现有无监督方法。

Conclusion: IRIS完全无监督、计算成本低，少量训练数据下效果好，适合实时检测。

Abstract: Unsupervised hallucination detection aims to identify hallucinated content
generated by large language models (LLMs) without relying on labeled data.
While unsupervised methods have gained popularity by eliminating
labor-intensive human annotations, they frequently rely on proxy signals
unrelated to factual correctness. This misalignment biases detection probes
toward superficial or non-truth-related aspects, limiting generalizability
across datasets and scenarios. To overcome these limitations, we propose IRIS,
an unsupervised hallucination detection framework, leveraging internal
representations intrinsic to factual correctness. IRIS prompts the LLM to
carefully verify the truthfulness of a given statement, and obtain its
contextualized embedding as informative features for training. Meanwhile, the
uncertainty of each response is considered a soft pseudolabel for truthfulness.
Experimental results demonstrate that IRIS consistently outperforms existing
unsupervised methods. Our approach is fully unsupervised, computationally low
cost, and works well even with few training data, making it suitable for
real-time detection.

</details>


### [225] [Population-Aligned Persona Generation for LLM-based Social Simulation](https://arxiv.org/abs/2509.10127)
*Zhengyu Hu,Zheyuan Xiao,Max Xiong,Yuxuan Lei,Tianfu Wang,Jianxun Lian,Kaize Ding,Ziang Xiao,Nicholas Jing Yuan,Xing Xie*

Main category: cs.CL

TL;DR: 本文提出为大语言模型驱动的社会模拟合成高质量、与人群匹配的角色集框架，实验证明该方法能减少偏差，实现准确灵活的社会模拟。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的社会模拟研究常忽视角色生成复杂性和非代表性角色集带来的偏差，需要构建能真实反映现实人群多样性和分布的角色集。

Method: 利用大语言模型从社交媒体数据生成叙事角色，进行质量评估过滤低质量角色，应用重要性采样实现与参考心理测量分布全局对齐，引入特定任务模块使全局对齐角色集适配目标子群体。

Result: 该方法显著减少了人群层面的偏差，能为广泛的研究和政策应用实现准确、灵活的社会模拟。

Conclusion: 所提出的系统框架有效，可用于大语言模型驱动的社会模拟，减少偏差并实现准确灵活模拟以支持研究和政策应用。

Abstract: Recent advances in large language models (LLMs) have enabled human-like
social simulations at unprecedented scale and fidelity, offering new
opportunities for computational social science. A key challenge, however, is
the construction of persona sets that authentically represent the diversity and
distribution of real-world populations. Most existing LLM-based social
simulation studies focus primarily on designing agentic frameworks and
simulation environments, often overlooking the complexities of persona
generation and the potential biases introduced by unrepresentative persona
sets. In this paper, we propose a systematic framework for synthesizing
high-quality, population-aligned persona sets for LLM-driven social simulation.
Our approach begins by leveraging LLMs to generate narrative personas from
long-term social media data, followed by rigorous quality assessment to filter
out low-fidelity profiles. We then apply importance sampling to achieve global
alignment with reference psychometric distributions, such as the Big Five
personality traits. To address the needs of specific simulation contexts, we
further introduce a task-specific module that adapts the globally aligned
persona set to targeted subpopulations. Extensive experiments demonstrate that
our method significantly reduces population-level bias and enables accurate,
flexible social simulation for a wide range of research and policy
applications.

</details>


### [226] [Established Psychometric vs. Ecologically Valid Questionnaires: Rethinking Psychological Assessments in Large Language Models](https://arxiv.org/abs/2509.10078)
*Dongmin Choi,Woojung Song,Jongwook Han,Eun-Ju Lee,Yohan Jo*

Main category: cs.CL

TL;DR: 本文对比分析既定心理问卷和生态有效问卷对大语言模型测量的结果，发现既定问卷存在诸多问题，提醒勿用其测量大语言模型。


<details>
  <summary>Details</summary>
Motivation: 既定心理问卷用于测量大语言模型存在缺乏生态效度等问题，且不清楚两种问卷结果差异及启示。

Method: 对既定心理问卷和生态有效问卷进行全面对比分析。

Result: 既定问卷与生态有效问卷测量结果差异大，存在测量项目不足、造成错误印象、夸大结果等问题。

Conclusion: 提醒不要使用既定心理问卷测量大语言模型。

Abstract: Researchers have applied established psychometric questionnaires (e.g., BFI,
PVQ) to measure the personality traits and values reflected in the responses of
Large Language Models (LLMs). However, concerns have been raised about applying
these human-designed questionnaires to LLMs. One such concern is their lack of
ecological validity--the extent to which survey questions adequately reflect
and resemble real-world contexts in which LLMs generate texts in response to
user queries. However, it remains unclear how established questionnaires and
ecologically valid questionnaires differ in their outcomes, and what insights
these differences may provide. In this paper, we conduct a comprehensive
comparative analysis of the two types of questionnaires. Our analysis reveals
that established questionnaires (1) yield substantially different profiles of
LLMs from ecologically valid ones, deviating from the psychological
characteristics expressed in the context of user queries, (2) suffer from
insufficient items for stable measurement, (3) create misleading impressions
that LLMs possess stable constructs, and (4) yield exaggerated profiles for
persona-prompted LLMs. Overall, our work cautions against the use of
established psychological questionnaires for LLMs. Our code will be released
upon publication.

</details>


### [227] [Benchmark of stylistic variation in LLM-generated texts](https://arxiv.org/abs/2509.10179)
*Jiří Milička,Anna Marklová,Václav Cvrček*

Main category: cs.CL

TL;DR: 研究人类与大语言模型（LLMs）文本的语域差异，应用Biber多维分析，用新语料并在捷克语中复制分析，创建基准对模型排名。


<details>
  <summary>Details</summary>
Motivation: 探究人类文本和大语言模型生成文本在语域上的差异。

Method: 对人类和AI生成文本样本应用Biber多维分析，使用新语料AI - Brown和AI - Koditex，在英语和捷克语中分析16个前沿模型不同设置和提示情况。

Result: 找到LLMs与人类在哪些维度的语域差异最显著和系统。

Conclusion: 创建了可在可解释维度对模型相互比较和排名的基准。

Abstract: This study investigates the register variation in texts written by humans and
comparable texts produced by large language models (LLMs). Biber's
multidimensional analysis (MDA) is applied to a sample of human-written texts
and AI-created texts generated to be their counterparts to find the dimensions
of variation in which LLMs differ most significantly and most systematically
from humans. As textual material, a new LLM-generated corpus AI-Brown is used,
which is comparable to BE-21 (a Brown family corpus representing contemporary
British English). Since all languages except English are underrepresented in
the training data of frontier LLMs, similar analysis is replicated on Czech
using AI-Koditex corpus and Czech multidimensional model. Examined were 16
frontier models in various settings and prompts, with emphasis placed on the
difference between base models and instruction-tuned models. Based on this, a
benchmark is created through which models can be compared with each other and
ranked in interpretable dimensions.

</details>


### [228] [SI-FACT: Mitigating Knowledge Conflict via Self-Improving Faithfulness-Aware Contrastive Tuning](https://arxiv.org/abs/2509.10208)
*Shengqiang Fu*

Main category: cs.CL

TL;DR: 提出Self Improving Faithfulness Aware Contrastive Tuning框架解决大语言模型知识冲突问题，实验显示有效提升上下文忠实度。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在知识密集型任务中因知识冲突产生不忠实回复，依赖内部参数知识而非上下文。

Method: 提出Self Improving Faithfulness Aware Contrastive Tuning框架，用自指令机制生成对比学习数据，再进行对比学习训练模型。

Result: 基于Llama3 8B Instruct的SIFACT模型在知识冲突评估基准上使上下文召回率提高6.2%，减少对内部记忆依赖。

Conclusion: SIFACT在增强大语言模型上下文忠实度上有效且数据效率高，为构建更可靠语言模型提供途径。

Abstract: Large Language Models often generate unfaithful responses in knowledge
intensive tasks due to knowledge conflict,that is,a preference for relying on
internal parametric knowledge rather than the provided context.To address this
issue,we propose a novel self improving framework,Self Improving Faithfulness
Aware Contrastive Tuning.The framework uses a self instruct mechanism that
allows the base LLM to automatically generate high quality,structured
contrastive learning data,including anchor samples,semantically equivalent
positive samples,and negative samples simulating unfaithful scenarios.This
approach significantly reduces the cost of manual
annotation.Subsequently,contrastive learning is applied to train the
model,enabling it to pull faithful responses closer and push unfaithful
responses farther apart in the representation space.Experiments on knowledge
conflict evaluation benchmarks ECARE KRE and COSE KRE show that the SI FACT
model based on Llama3 8B Instruct improves the Contextual Recall Rate by 6.2%
over the best baseline method,while significantly reducing dependence on
internal memory.The results indicate that SI FACT provides strong effectiveness
and high data efficiency in enhancing the contextual faithfulness of
LLMs,offering a practical pathway toward building more proactive and
trustworthy language models.

</details>


### [229] [Is In-Context Learning Learning?](https://arxiv.org/abs/2509.10414)
*Adrian de Wynter*

Main category: cs.CL

TL;DR: 文章探讨上下文学习（ICL）是否为学习，通过大规模分析发现ICL有效但学习和泛化能力受限，自回归临时编码机制不可靠。


<details>
  <summary>Details</summary>
Motivation: 探讨在上下文学习中，模型通过少量示例解决未见任务是否真正构成学习。

Method: 进行大规模分析，排除或考虑记忆、预训练、分布偏移和提示风格等因素。

Result: ICL是有效学习范式，但学习和泛化能力有限，示例增多时，准确率对多种因素不敏感，会导致分布敏感性。

Conclusion: 自回归的临时编码不是鲁棒机制，通用泛化能力有限。

Abstract: In-context learning (ICL) allows some autoregressive models to solve tasks
via next-token prediction and without needing further training. This has led to
claims about these model's ability to solve (learn) unseen tasks with only a
few shots (exemplars) in the prompt. However, deduction does not always imply
learning, as ICL does not explicitly encode a given observation. Instead, the
models rely on their prior knowledge and the exemplars given, if any. We argue
that, mathematically, ICL does constitute learning, but its full
characterisation requires empirical work. We then carry out a large-scale
analysis of ICL ablating out or accounting for memorisation, pretraining,
distributional shifts, and prompting style and phrasing. We find that ICL is an
effective learning paradigm, but limited in its ability to learn and generalise
to unseen tasks. We note that, in the limit where exemplars become more
numerous, accuracy is insensitive to exemplar distribution, model, prompt
style, and the input's linguistic features. Instead, it deduces patterns from
regularities in the prompt, which leads to distributional sensitivity,
especially in prompting styles such as chain-of-thought. Given the varied
accuracies on formally similar tasks, we conclude that autoregression's ad-hoc
encoding is not a robust mechanism, and suggests limited all-purpose
generalisability.

</details>


### [230] [WhisTLE: Deeply Supervised, Text-Only Domain Adaptation for Pretrained Speech Recognition Transformers](https://arxiv.org/abs/2509.10452)
*Akshat Pandey,Karun Kumar,Raphael Tang*

Main category: cs.CL

TL;DR: 现有预训练ASR模型需领域适配，提出文本适配方法WhisTLE，降低WER且优于基线。


<details>
  <summary>Details</summary>
Motivation: 预训练ASR模型需领域适配，但收集语音数据不实际，需文本适配。

Method: 提出WhisTLE方法，训练VAE建模编码器输出，微调解码器，可结合TTS适配，推理时恢复原编码器。

Result: 在四个域外数据集和四个ASR模型上，结合TTS的WhisTLE相对仅TTS适配降低WER 12.3%，32种场景中27种优于非WhisTLE基线。

Conclusion: WhisTLE是一种有效的预训练ASR模型文本适配方法。

Abstract: Pretrained automatic speech recognition (ASR) models such as Whisper perform
well but still need domain adaptation to handle unseen vocabulary and parlance.
In many real-world settings, collecting speech data is impractical,
necessitating text-only adaptation. We propose WhisTLE, a deeply supervised,
text-only adaptation method for pretrained encoder-decoder ASR models. WhisTLE
trains a variational autoencoder (VAE) to model encoder outputs from text and
fine-tunes the decoder using the learned text-to-latent encoder, optionally
combined with text-to-speech (TTS) adaptation. At inference, the original
encoder is restored, incurring no extra runtime cost. Across four out-of-domain
datasets and four ASR models, WhisTLE with TTS reduces word error rate (WER) by
12.3% relative to TTS-only adaptation and outperforms all non-WhisTLE baselines
in 27 of 32 scenarios.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [231] [DCHO: A Decomposition-Composition Framework for Predicting Higher-Order Brain Connectivity to Enhance Diverse Downstream Applications](https://arxiv.org/abs/2509.09696)
*Weibin Li,Wendu Li,Quanying Liu*

Main category: q-bio.NC

TL;DR: 提出DCHO方法用于建模和预测高阶脑连接（HOBC）的时间演变，在多任务中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有HOBC研究多为静态分析，不适用于动态预测任务。

Method: 基于分解 - 组合框架，采用分解 - 组合策略，分为HOBC推理和潜在轨迹预测两个子问题，推理阶段用双视图编码器和潜在组合学习器，预测阶段引入潜在空间预测损失。

Result: 在多个神经影像数据集上的实验表明，DCHO在非预测任务（状态分类）和预测任务（脑动力学预测）中表现出色，显著优于现有方法。

Conclusion: DCHO是一种有效的统一方法，适用于HOBC的建模和预测。

Abstract: Higher-order brain connectivity (HOBC), which captures interactions among
three or more brain regions, provides richer organizational information than
traditional pairwise functional connectivity (FC). Recent studies have begun to
infer latent HOBC from noninvasive imaging data, but they mainly focus on
static analyses, limiting their applicability in dynamic prediction tasks. To
address this gap, we propose DCHO, a unified approach for modeling and
forecasting the temporal evolution of HOBC based on a Decomposition-Composition
framework, which is applicable to both non-predictive tasks (state
classification) and predictive tasks (brain dynamics forecasting). DCHO adopts
a decomposition-composition strategy that reformulates the prediction task into
two manageable subproblems: HOBC inference and latent trajectory prediction. In
the inference stage, we propose a dual-view encoder to extract multiscale
topological features and a latent combinatorial learner to capture high-level
HOBC information. In the forecasting stage, we introduce a latent-space
prediction loss to enhance the modeling of temporal trajectories. Extensive
experiments on multiple neuroimaging datasets demonstrate that DCHO achieves
superior performance in both non-predictive tasks (state classification) and
predictive tasks (brain dynamics forecasting), significantly outperforming
existing methods.

</details>


<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [232] [Engineering Spatial and Molecular Features from Cellular Niches to Inform Predictions of Inflammatory Bowel Disease](https://arxiv.org/abs/2509.09923)
*Myles Joshua Toledo Tan,Maria Kapetanaki,Panayiotis V. Benos*

Main category: q-bio.GN

TL;DR: 研究提出用空间转录组学创建可解释机器学习模型对炎症性肠病（IBD）分类，分析结肠黏膜ST数据，模型在多分类和二分类问题上有较好表现，还揭示了疾病相关预测因素和机制。


<details>
  <summary>Details</summary>
Motivation: 由于克罗恩病（CD）和溃疡性结肠炎（UC）症状重叠，区分IBD的两种主要亚型是临床挑战，因此需新方法用于IBD分类。

Method: 分析健康对照、UC和CD患者结肠黏膜的ST数据，用非负矩阵分解（NMF）识别细胞生态位，设计44个特征，用多层感知器（MLP）分类器训练。

Result: MLP分类器在三分类问题准确率为0.774 +/- 0.161，二分类问题准确率为0.916 +/- 0.118；模型可解释性分析揭示生态位空间组织破坏是炎症强预测因素，UC和CD分类依赖特定生态位 - 基因表达特征。

Conclusion: 该研究提供了将描述性空间数据转化为准确可解释预测工具的概念验证管道，为IBD诊断提供新范式，也加深对IBD亚型生物学机制的理解。

Abstract: Differentiating between the two main subtypes of Inflammatory Bowel Disease
(IBD): Crohns disease (CD) and ulcerative colitis (UC) is a persistent clinical
challenge due to overlapping presentations. This study introduces a novel
computational framework that employs spatial transcriptomics (ST) to create an
explainable machine learning model for IBD classification. We analyzed ST data
from the colonic mucosa of healthy controls (HC), UC, and CD patients. Using
Non-negative Matrix Factorization (NMF), we first identified four recurring
cellular niches, representing distinct functional microenvironments within the
tissue. From these niches, we systematically engineered 44 features capturing
three key aspects of tissue pathology: niche composition, neighborhood
enrichment, and niche-gene signals. A multilayer perceptron (MLP) classifier
trained on these features achieved an accuracy of 0.774 +/- 0.161 for the more
challenging three-class problem (HC, UC, and CD) and 0.916 +/- 0.118 in the
two-class problem of distinguishing IBD from healthy tissue. Crucially, model
explainability analysis revealed that disruptions in the spatial organization
of niches were the strongest predictors of general inflammation, while the
classification between UC and CD relied on specific niche-gene expression
signatures. This work provides a robust, proof-of-concept pipeline that
transforms descriptive spatial data into an accurate and explainable predictive
tool, offering not only a potential new diagnostic paradigm but also deeper
insights into the distinct biological mechanisms that drive IBD subtypes.

</details>


<div id='cs.CC'></div>

# cs.CC [[Back]](#toc)

### [233] [Parameterized Complexity of Vehicle Routing](https://arxiv.org/abs/2509.10361)
*Michelle Döring,Jan Fehse,Tobias Friedrich,Paula Marten,Niklas Mohrin,Kirill Simonov,Farehe Soheil,Jakob Timm,Shaily Verma*

Main category: cs.CC

TL;DR: 研究VRP和CVRP的几种参数化复杂度，给出VRP基于树宽的FPT算法，证明CVRP多种参数化的难度，提供CVRP基于树宽和车辆容量的XP算法。


<details>
  <summary>Details</summary>
Motivation: 探究VRP和CVRP在多种参数化下的复杂度情况。

Method: 分析VRP和CVRP问题，针对不同参数化进行研究，如聚焦图的树宽，设计FPT算法、证明复杂度、设计XP算法。

Result: 为VRP基于树宽给出FPT算法，证明CVRP多种参数化（含树宽）的paraNP - 和W[·] - 难，给出CVRP基于树宽和车辆容量的XP算法。

Conclusion: VRP基于树宽有FPT算法，CVRP多种参数化下存在较大复杂度，不太可能有FPT算法，但基于树宽和车辆容量有XP算法。

Abstract: The Vehicle Routing Problem (VRP) is a popular generalization of the
Traveling Salesperson Problem. Instead of one salesperson traversing the entire
weighted, undirected graph $G$, there are $k$ vehicles available to jointly
cover the set of clients $C \subseteq V(G)$. Every vehicle must start at one of
the depot vertices $D \subseteq V(G)$ and return to its start. Capacitated
Vehicle Routing (CVRP) additionally restricts the route of each vehicle by
limiting the number of clients it can cover, the distance it can travel, or
both.
  In this work, we study the complexity of VRP and the three variants of CVRP
for several parameterizations, in particular focusing on the treewidth of $G$.
We present an FPT algorithm for VRP parameterized by treewidth. For CVRP, we
prove paraNP- and $W[\cdot]$-hardness for various parameterizations, including
treewidth, thereby rendering the existence of FPT algorithms unlikely. In turn,
we provide an XP algorithm for CVRP when parameterized by both treewidth and
the vehicle capacity.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [234] [A Smooth Computational Transition in Tensor PCA](https://arxiv.org/abs/2509.09904)
*Zhangsong Li*

Main category: math.ST

TL;DR: 提出基于加权超图计数的张量PCA高效算法，在特定信噪比下成功运行且时间复杂度有改善，证实猜想。


<details>
  <summary>Details</summary>
Motivation: 解决阶数p（p≥3）的张量PCA问题，改进现有算法时间复杂度，验证相关猜想。

Method: 基于计数特定加权超图族设计算法。

Result: 在信噪比为λn^(-p/4)（λ = Ω(1)）时算法成功，运行时间为n^(C + o(1))，相比之前算法有改进，且呈现信噪比和计算成本的平滑权衡。

Conclusion: 算法有效，证实了文献[KWB22]中的猜想。

Abstract: We propose an efficient algorithm for tensor PCA based on counting a specific
family of weighted hypergraphs. For the order-$p$ tensor PCA problem where $p
\geq 3$ is a fixed integer, we show that when the signal-to-noise ratio is
$\lambda n^{-\frac{p}{4}}$ where $\lambda=\Omega(1)$, our algorithm succeeds
and runs in time $n^{C+o(1)}$ where $C=C(\lambda)$ is a constant depending on
$\lambda$. This algorithm improves a poly-logarithmic factor compared to
previous algorithms based on the Sum-of-Squares hierarchy \cite{HSS15} or based
on the Kikuchi hierarchy in statistical physics \cite{WEM19}. Furthermore, our
result shows a smooth tradeoff between the signal-to-noise ratio and the
computational cost in this problem, thereby confirming a conjecture posed in
\cite{KWB22}.

</details>
